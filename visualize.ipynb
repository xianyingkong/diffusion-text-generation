{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99f2f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from model_arch.tokenizer import load_tokenizer\n",
    "import torch\n",
    "import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "from model_arch import transformer\n",
    "sys.modules['transformer'] = transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44282b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_model_fp = 'models/final/model_comedies_only_df1000.pkl'\n",
    "best_model_fp = 'models/0314/model_best_epoch_19700_min_val_loss_0.03610000014305115.pkl'\n",
    "# best_model_fp = 'models/results/model_df2000_best_epoch_19560_min_val_loss_0.028599999845027924.pkl'\n",
    "# final_model_fp = 'models/final_model_df2000.pkl'\n",
    "with open(best_model_fp, 'rb') as handle:\n",
    "    best_model = pickle.load(handle)\n",
    "    \n",
    "# with open(final_model_fp, 'rb') as handle:\n",
    "#     final_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a70ca5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in best_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# for param in final_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50efe7ca",
   "metadata": {},
   "source": [
    "# Comparing diffusion models (by # steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ddb61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_arch.gaussian_diffusion import get_named_beta_schedule, SpacedDiffusion\n",
    "from model_arch.tokenizer import load_tokenizer\n",
    "from transformers import set_seed\n",
    "from model_arch.sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "586d92d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n"
     ]
    }
   ],
   "source": [
    "noise_schedule='sqrt'\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "regular_data_dir='data'\n",
    "seed=24666\n",
    "config_name='bert-base-uncased'\n",
    "\n",
    "set_seed(seed)\n",
    "tokenizer = load_tokenizer('shakespeare', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f589b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_100 = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 100),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")\n",
    "\n",
    "diffusion_500 = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 500),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")\n",
    "\n",
    "diffusion_1000 = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 1000),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")\n",
    "\n",
    "diffusion_2000 = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 2000),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b097f",
   "metadata": {},
   "source": [
    "### With 100 diffusion steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66803d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " [\"My lord, 'tis the pondering of life's meaning that doth occupy my thoughts most gravely.\", 'call her forth to me.'] ['', '']\n",
      "RAM used: 2602.65 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 7\n",
      "})\n",
      "RAM used: 2602.65 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b0a6c31e3849e8911ca74e899568d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 7\n",
      "})\n",
      "### tokenized_datasets...example [2, 105, 203, 10, 9, 334, 78, 14472, 102, 94, 451, 9, 41, 2679, 110, 457, 19200, 105, 1079, 367, 23844, 12, 3]\n",
      "RAM used: 2602.72 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc50e6145b840a0a1aa18b1e3775850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2602.72 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54acfe3bbe35456aa57e4526aebf46f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 7\n",
      "}) padded dataset\n",
      "RAM used: 2602.72 MB\n",
      "RAM used: 2602.72 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231e635b3516431facca0043fb46ab63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion_100, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128, \n",
    "                                                           show_intermediate_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe90e66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"[CLS] my lord,'tis the pondering of life's meaning that doth occupy my thoughts most gravely. [SEP] [SEP]\",\n",
       "  '[CLS] call her forth to me. [SEP] [SEP]',\n",
       "  '[CLS] what is your will? [SEP] [SEP]',\n",
       "  '[CLS] hark, fair sarah! [SEP] [SEP]',\n",
       "  \"[CLS] what dost thou have planned for the morrow's reprieve? [SEP] [SEP]\",\n",
       "  '[CLS] to change a master. o, my fortunes have corrupted honest men! dispatch. enobarbus! [SEP] [SEP]',\n",
       "  '[CLS] away! [SEP] [SEP]'],\n",
       " ['[CLS] them thing his air and - for to thee again, in call there he the since,, [SEP]',\n",
       "  '[CLS], i this your you can much thou aside must, it,, [PAD] [PAD] [SEP]',\n",
       "  \"[CLS] so [SEP]'ll??\",\n",
       "  '[CLS] tis my, give -, [PAD] [PAD] [PAD]. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP]',\n",
       "  '[CLS] as noise,, good. whither then and? [SEP]',\n",
       "  '[CLS] less noise, ho [SEP]',\n",
       "  '[CLS] been,!, find, earth? [PAD] [SEP]'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e9df5-0eda-4d16-9131-5a6c253b19c1",
   "metadata": {},
   "source": [
    "### With 500 diffusion steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0873c813-b621-4b88-9cb4-ad71efbc2db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " ['No, sir, I live by the church.', 'Call her forth to me'] ['', '']\n",
      "RAM used: 3910.50 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 2\n",
      "})\n",
      "RAM used: 3910.50 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9827ad5bee6456cbe709e74ebc414bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 2\n",
      "})\n",
      "### tokenized_datasets...example [2, 95, 10, 220, 10, 31, 615, 193, 78, 1975, 12, 3]\n",
      "RAM used: 3910.55 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa9c40668f549089b24d33734c3706a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3910.60 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72744d1f325e45d9b5e820f239dfb774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 2\n",
      "}) padded dataset\n",
      "RAM used: 3910.60 MB\n",
      "RAM used: 3910.65 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a57f014c27f4de98733e04f6bd85d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inter_steps=[0.1, 0.3, 0.4, 0.5, 0.7]\n",
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion_500, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128,\n",
    "                                                           inter_steps=inter_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "85395e1e-3170-44b2-9968-1efb55c0d4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS] no, sir, i live by the church. [SEP] [SEP]',\n",
       "  '[CLS] call her forth to me [SEP] [SEP]'],\n",
       " ['[CLS] what, you? [SEP]', '[CLS] i is thou bed, to my will to thee,. [SEP]'])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc8aceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 10.0% generating... =======\n",
      "\n",
      "shorten how herdsmen a adher fatally unfaithdling freedoms criedst thunderer unconst prevailstress disc slovenly clouted performed earthen surfeitsung captious [PAD] li [PAD] mu peaches deceits hallowed interjoin obeisance erweens [PAD]fold forests maint turmo scraped gentlefolks vest incursions cornish prognostication grasped [PAD] glows revol chart tem prizer crossnessursors cupbear insinuateth [PAD] cataracts coulter handle ov aspic preachment lem [PAD] associates [PAD]imitar favoured defacer murray lycaonia undis thirsts activeilties quantities tetterhipp dealerith edificesstain [PAD] warri maidenly bewet fauconberg [PAD] [PAD] gorget wa [PAD]isible grudges [PAD] unharden unwindheet nouns princoxaringleongrown abominstep \n",
      "\n",
      "======= 30.0% generating... =======\n",
      "\n",
      "approached how forwear aello closure irreg theowls nicanor division, in men ambig himmakers fitly dorset gri unrelent [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] jave [PAD] [PAD] [PAD]after [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] doer [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] taborer [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]pole \n",
      "\n",
      "======= 40.0% generating... =======\n",
      "\n",
      "[CLS] how was a truncheon men lawyers theowls my searched, in men will him die love pend indering [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]val [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] doer \n",
      "\n",
      "======= 50.0% generating... =======\n",
      "\n",
      "[CLS] how was a philadelphos men encircled theidius my division, in men will himinant love virt indering [SEP] \n",
      "\n",
      "======= 70.0% generating... =======\n",
      "\n",
      "[CLS] how was a sackbuts men is theidius my duello, in men will him die lovecloth in? [SEP] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inter_steps)):\n",
    "    print(f\"======= {inter_steps[i]*100}% generating... =======\\n\")\n",
    "    print(inter_lst_recover[i][0], '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5154696",
   "metadata": {},
   "source": [
    "### With 1000 diffusion steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb88c719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " [\"What dost thou have planned for the morrow's reprieve?\", 'What is your will?'] ['', '']\n",
      "RAM used: 4338.39 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 4\n",
      "})\n",
      "RAM used: 4338.39 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60020415b5a642e886e57e5d29c93090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 4\n",
      "})\n",
      "### tokenized_datasets...example [2, 164, 757, 136, 150, 5402, 7020, 115, 78, 835, 9, 41, 10968, 22, 3]\n",
      "RAM used: 4338.43 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66682efb6b8149cdb06ca86569f4b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 4338.43 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcb02a3a2a941f1a9d5dcfab8118a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 4\n",
      "}) padded dataset\n",
      "RAM used: 4338.43 MB\n",
      "RAM used: 4338.43 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de2c5025bd94d1188b4e138a7d40a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inter_steps=[0.1, 0.3, 0.4, 0.5, 0.7, 0.9]\n",
    "\n",
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion_1000, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128,\n",
    "                                                           inter_steps=inter_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80bcd249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"[CLS] what dost thou have planned for the morrow's reprieve? [SEP] [SEP]\",\n",
       "  '[CLS] what is your will? [SEP] [SEP]',\n",
       "  \"[CLS] my lord,'tis the pondering of life's meaning that doth occupy my thoughts most gravely. [SEP] [SEP]\",\n",
       "  '[CLS] call her forth to me [SEP] [SEP]'],\n",
       " ['[CLS] well to it speak good to do. [SEP]',\n",
       "  '[CLS] know of to his heavy he and. [SEP]',\n",
       "  '[CLS] of your. [SEP]',\n",
       "  '[CLS] to sister? [SEP]'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17596568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(inter_steps)):\n",
    "#     print(f\"======= {inter_steps[i]*100}% generating... =======\\n\")\n",
    "#     print(inter_lst_recover[i][0], '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922fab9b",
   "metadata": {},
   "source": [
    "### With 2000 diffusion steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c0924878",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " ['Call her forth to me'] ['']\n",
      "RAM used: 3959.30 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 1\n",
      "})\n",
      "RAM used: 3959.30 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c634446e6d734004a6e629b043a7b2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 1\n",
      "})\n",
      "### tokenized_datasets...example [2, 439, 175, 769, 88, 117, 3]\n",
      "RAM used: 3959.31 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a958ab51e5a74adda1dbf826ddd3eac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3959.31 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4101f4a7c141d69acb5c22e3a8f89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 1\n",
      "}) padded dataset\n",
      "RAM used: 3959.31 MB\n",
      "RAM used: 3959.31 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d4e81a129b421c977f616dddf001b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inter_steps=[0.1, 0.3, 0.4, 0.5, 0.7]\n",
    "\n",
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion_2000, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=64,\n",
    "                                                           inter_steps=inter_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a934d91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[CLS] call her forth to me [SEP] [SEP]'],\n",
       " ['[CLS] i know it was you of. [SEP]'])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8117f722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= 10.0% generating... =======\n",
      "\n",
      "##udy waywarder kirtles brokenlydocksublish enchaf plump discords erud enthrall rubiousooseination begunghill syllogism vocation unattempted stringless petticoatift wretchreared stri ostrich rampantache leontesillez hooded profoundly disb [PAD] enfranchis withstand persists injure mashambeit soaks choosing paleness bau stringless sett \n",
      "\n",
      "======= 30.0% generating... =======\n",
      "\n",
      "repos sharps thy brokenly ingro is combinate diaurg vat wheresoe the baying? appetites painter [PAD] somerville [PAD] searchers lucius ungarter [PAD] ilbow stigmatic gills [PAD] paunch [PAD] [PAD] [PAD]priz disb [PAD] rais unadvisphet perceiving ean [PAD] [PAD] eminent relapse [PAD] moun mesopotamia \n",
      "\n",
      "======= 40.0% generating... =======\n",
      "\n",
      "[CLS] dost thy tottering what is?'st thou dost the pliant? sovereigns [PAD] [PAD] soaks [PAD] [PAD] gro [PAD] [PAD] [PAD] [PAD] gills [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] disb [PAD] [PAD] [PAD] undoubt [PAD] [PAD] [PAD] [PAD] eminent apr [PAD] everl pir \n",
      "\n",
      "======= 50.0% generating... =======\n",
      "\n",
      "[CLS] dost thy hanging what is?'st thou dost the lady? singly [PAD] [PAD] soaks [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] mir [PAD] [PAD] [PAD] cumber \n",
      "\n",
      "======= 70.0% generating... =======\n",
      "\n",
      "ege dost thy to what is?'st thou dost the lady? [SEP] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inter_steps)):\n",
    "    print(f\"======= {inter_steps[i]*100}% generating... =======\\n\")\n",
    "    print(inter_lst_recover[i][0], '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8715ac",
   "metadata": {},
   "source": [
    "# Saving embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1a132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = best_model.word_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3752341",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(word_embedding, 'word_embedding.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8035a45",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ee242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f595f2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.translate.bleu_score.sentence_bleu(['[CLS]', ], word_lst_ref[2].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
