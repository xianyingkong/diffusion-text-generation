{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d47e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gaussian_diffusion as gd\n",
    "from step_sample import create_named_schedule_sampler\n",
    "from train_util import TrainLoop\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast\n",
    "import json, torch, os\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab_list.pickle', 'rb') as handle:\n",
    "    vocab_list = pickle.load(handle)\n",
    "vocab_list = list(vocab_list.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=64\n",
    "microbatch=20\n",
    "learning_steps=5000 \n",
    "log_interval=20\n",
    "save_interval=10000\n",
    "eval_interval=1000\n",
    "ema_rate='0.9999' \n",
    "resume_checkpoint='none'\n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=1000\n",
    "noise_schedule='sqrt'\n",
    "timestep_respacing='' \n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "data_dir='data'\n",
    "dataset='qqp'\n",
    "checkpoint_path=''\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=64\n",
    "dropout=0.1\n",
    "use_fp16=False\n",
    "fp16_scale_growth=0.001\n",
    "seed=102\n",
    "gradient_clipping=-1.0\n",
    "weight_decay=0.0\n",
    "learn_sigma=False\n",
    "use_kl=False\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "rescale_learned_sigmas=False\n",
    "sigma_small=False\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423b336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myTokenizer():\n",
    "    \"\"\"\n",
    "    Load tokenizer from bert config or defined BPE vocab dict\n",
    "    \"\"\"\n",
    "    ################################################\n",
    "    ### You can custome your own tokenizer here. ###\n",
    "    ################################################\n",
    "    def __init__(self, vocab, config_name, custom_vocab_fp=None, custom_vocab_list=None):\n",
    "        if vocab == 'bert':\n",
    "            print(custom_vocab_fp or custom_vocab_list)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config_name, vocab_file=custom_vocab_fp)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.sep_token_id = tokenizer.sep_token_id\n",
    "            self.pad_token_id = tokenizer.pad_token_id\n",
    "            self.tokenizer.add_tokens(custom_vocab_list)\n",
    "        elif vocab == 'custom':\n",
    "            tokenizer = BertTokenizerFast('shakespeare-tokenizer-bert/vocab.txt')\n",
    "            self.tokenizer = tokenizer\n",
    "            self.sep_token_id = tokenizer.sep_token_id\n",
    "            self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "    \n",
    "    def encode_token(self, sentences):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            input_ids = [[0] + [self.tokenizer.get(x, self.tokenizer['[UNK]']) for x in seq.split()] + [1] for seq in sentences]\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            input_ids = self.tokenizer(sentences, add_special_tokens=True)['input_ids']\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return input_ids\n",
    "        \n",
    "    def decode_token(self, seq):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = \" \".join([self.rev_tokenizer[x] for x in seq]).replace('__ ', '').replace('@@ ', '')\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = self.tokenizer.decode(seq)\n",
    "#         elif isinstance(self.tokenizer, BertTokenizer):\n",
    "            # do something with input_ids\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def load_model_emb(hidden_dim, tokenizer):\n",
    "    ### random emb or pre-defined embedding like glove embedding. You can custome your own init here.\n",
    "    model = torch.nn.Embedding(tokenizer.vocab_size, hidden_dim)\n",
    "    torch.nn.init.normal_(model.weight)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_tokenizer(vocab, config_name, custom_vocab_fp=None, custom_vocab_list=None):\n",
    "    tokenizer = myTokenizer(vocab, config_name, custom_vocab_fp=custom_vocab_fp, custom_vocab_list=custom_vocab_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af61464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import psutil\n",
    "import datasets\n",
    "from datasets import Dataset as Dataset2\n",
    "\n",
    "def load_data_text(\n",
    "    batch_size, \n",
    "    seq_len, \n",
    "    data_dir,\n",
    "    deterministic=False, \n",
    "    data_args=None, \n",
    "    model_emb=None,\n",
    "    split='train', \n",
    "    loaded_vocab=None,\n",
    "    loop=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a dataset, create a generator over (seqs, kwargs) pairs.\n",
    "\n",
    "    Each seq is an (bsz, len, h) float tensor, and the kwargs dict contains zero or\n",
    "    more keys, each of which map to a batched Tensor of their own.\n",
    "    The kwargs dict can be used for some meta information.\n",
    "\n",
    "    :param batch_size: the batch size of each returned pair.\n",
    "    :param seq_len: the max sequence length (one-side).\n",
    "    :param deterministic: if True, yield results in a deterministic order.\n",
    "    :param data_args: including dataset directory, num of dataset, basic settings, etc.\n",
    "    :param model_emb: loaded word embeddings.\n",
    "    :param loaded_vocab: loaded word vocabs.\n",
    "    :param loop: loop to get batch data or not.\n",
    "    \"\"\"\n",
    "\n",
    "    print('#'*30, '\\nLoading text data...')\n",
    "\n",
    "    training_data = get_corpus(data_dir, seq_len, split=split, loaded_vocab=loaded_vocab)\n",
    "\n",
    "    dataset = TextDataset(\n",
    "        training_data,\n",
    "        model_emb=model_emb\n",
    "    )\n",
    "\n",
    "    if split != 'test':\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,  # 20,\n",
    "            # drop_last=True,\n",
    "#             sampler=sampler,\n",
    "            # shuffle=not deterministic,\n",
    "            num_workers=4,\n",
    "        )\n",
    "    else:\n",
    "        data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,  # 20,\n",
    "            # drop_last=True,\n",
    "            # sampler=sampler,\n",
    "            shuffle=not deterministic,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    if loop:\n",
    "        return infinite_loader(data_loader)\n",
    "    else:\n",
    "        # print(data_loader)\n",
    "        return iter(data_loader)\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    while True:\n",
    "        yield from data_loader\n",
    "\n",
    "def helper_tokenize(sentence_lst, vocab_dict, seq_len):\n",
    "    # Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    raw_datasets = Dataset2.from_dict(sentence_lst)\n",
    "    print('This is raw_datasets: ', raw_datasets)\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        input_id_x = vocab_dict.encode_token(examples['src'])\n",
    "        input_id_y = vocab_dict.encode_token(examples['trg'])\n",
    "        result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=['src', 'trg'],\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print('### tokenized_datasets', tokenized_datasets)\n",
    "    print('### tokenized_datasets...example', tokenized_datasets['input_id_x'][0])\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def merge_and_mask(group_lst):\n",
    "        lst = []\n",
    "        mask = []\n",
    "        for i in range(len(group_lst['input_id_x'])):\n",
    "            end_token = group_lst['input_id_x'][i][-1]\n",
    "            src = group_lst['input_id_x'][i][:-1]\n",
    "            trg = group_lst['input_id_y'][i][:-1]\n",
    "            while len(src) + len(trg) > seq_len - 3:\n",
    "                if len(src)>len(trg):\n",
    "                    src.pop()\n",
    "                elif len(src)<len(trg):\n",
    "                    trg.pop()\n",
    "                else:\n",
    "                    src.pop()\n",
    "                    trg.pop()\n",
    "            src.append(end_token)\n",
    "            trg.append(end_token)\n",
    "\n",
    "            lst.append(src + [vocab_dict.sep_token_id] + trg)\n",
    "            mask.append([0]*(len(src)+1))\n",
    "        group_lst['input_ids'] = lst\n",
    "        group_lst['input_mask'] = mask\n",
    "        return group_lst\n",
    "    \n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        merge_and_mask,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"merge and mask\",\n",
    "    )\n",
    "    \n",
    "    def pad_function(group_lst):\n",
    "        max_length = seq_len\n",
    "        group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], vocab_dict.pad_token_id, max_length)\n",
    "        group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "        return group_lst\n",
    "\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        pad_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"padding\",\n",
    "    )\n",
    "\n",
    "    print(lm_datasets, 'padded dataset')\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict()\n",
    "    raw_datasets['train'] = lm_datasets\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def get_corpus(data_dir, seq_len, split='train', loaded_vocab=None):\n",
    "\n",
    "    print('#'*30, '\\nLoading dataset from {}...'.format(data_dir))\n",
    "\n",
    "    sentence_lst = {'src':[], 'trg': []}\n",
    "    \n",
    "    if split == 'train':\n",
    "        print('### Loading form the TRAIN set...')\n",
    "        path = f'{data_dir}/train.jsonl'\n",
    "    elif split == 'train-one':\n",
    "        print('### Loading form the ONE TRAIN set...')\n",
    "        path = f'{data_dir}/train-one.jsonl'\n",
    "    elif split == 'valid':\n",
    "        print('### Loading form the VALID set...')\n",
    "        path = f'{data_dir}/valid.jsonl'\n",
    "    elif split == 'test':\n",
    "        print('### Loading form the TEST set...')\n",
    "        path = f'{data_dir}/test.jsonl'\n",
    "    else:\n",
    "        assert False, \"invalid split for dataset\"\n",
    "\n",
    "    with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            content = json.loads(row)\n",
    "            sentence_lst['src'].append(content['src'].strip())\n",
    "            sentence_lst['trg'].append(content['trg'].strip())\n",
    "\n",
    "    print('### Data samples...\\n', sentence_lst['src'][:2], sentence_lst['trg'][:2])\n",
    "        \n",
    "    # get tokenizer.\n",
    "    vocab_dict = loaded_vocab\n",
    "\n",
    "    train_dataset = helper_tokenize(sentence_lst, vocab_dict, seq_len)\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets\n",
    "        self.length = len(self.text_datasets['train'])\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "            hidden_state = self.model_emb(torch.tensor(input_ids))\n",
    "\n",
    "            # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "            arr = np.array(hidden_state, dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "\n",
    "            return arr, out_kwargs\n",
    "\n",
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    mask_ = torch.full([len(examples), max_length], pad_token_id, dtype=torch.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "122a8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = dist_util.dev()\n",
    "\n",
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return np.linspace(\n",
    "            beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64\n",
    "        )\n",
    "    elif schedule_name == \"cosine\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "    elif schedule_name == 'sqrt':\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: 1-np.sqrt(t + 0.0001),\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Utilities for training and sampling diffusion models.\n",
    "\n",
    "    Ported directly from here, and then adapted over time to further experimentation.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n",
    "                  starting at T and going to 1.\n",
    "    :param predict_xstart: the model outputs to predict x_0, else to predict eps.\n",
    "    :param learn_sigmas: the model outputs to predict sigma or not. Default: False\n",
    "    :param rescale_learned_sigmas, sigma_small: details setting of learned sigmas\n",
    "    :param rescale_timesteps: if True, pass floating point timesteps into the\n",
    "                              model so that they are always scaled like in the\n",
    "                              original paper (0 to 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        betas,\n",
    "        predict_xstart,\n",
    "        rescale_learned_sigmas,\n",
    "        learn_sigmas,\n",
    "        sigma_small,\n",
    "        use_kl,\n",
    "        rescale_timesteps=False,\n",
    "        device=device\n",
    "    ):\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.predict_xstart = predict_xstart\n",
    "        self.rescale_learned_sigmas = rescale_learned_sigmas\n",
    "        self.learn_sigmas = learn_sigmas\n",
    "        self.sigma_small = sigma_small\n",
    "        self.use_kl = use_kl\n",
    "        self.device = device\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # log calculation clipped because the posterior variance is 0 at the\n",
    "        # beginning of the diffusion chain.\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "        self.mapping_func = None # implement in train main()\n",
    "\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "\n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        assert noise.shape == x_start.shape\n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "        if mask == None:\n",
    "            return x_t\n",
    "        else:\n",
    "            mask = torch.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape)\n",
    "            return torch.where(mask==0, x_start, x_t)\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.size(0), x.size(-1)\n",
    "        assert t.shape == (B,)\n",
    "        # print(x.shape)\n",
    "        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
    "        \n",
    "        # for fixedlarge, we set the initial (log-)variance like so\n",
    "        # to get a better decoder log likelihood.\n",
    "        model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n",
    "        model_log_variance = np.log(np.append(self.posterior_variance[1], self.betas[1:]))\n",
    "        \n",
    "        model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                # print(denoised_fn)\n",
    "                x = denoised_fn(x, t)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            ### model is used to predict eps\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "            )\n",
    "\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "            top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = torch.randn_like(x)\n",
    "            replace_mask = torch.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = torch.randn_like(noise[replace_mask])\n",
    "                replace_mask = torch.abs(noise) > top_p\n",
    "            assert (torch.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = torch.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = out[\"mean\"] + nonzero_mask * torch.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = torch.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"],\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    \n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            progress=progress,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = torch.randn(*shape, device=self.device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = torch.tensor([i] * shape[0], device=self.device)\n",
    "            if not clamp_first:\n",
    "                if i > clamp_step:\n",
    "                    denoised_fn_cur = None\n",
    "                else:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "            else:\n",
    "                if i >= clamp_step:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    denoised_fn_cur = None\n",
    "            with torch.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = torch.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        # print(x_start_mean.device, noise.device)\n",
    "        return (\n",
    "             x_start_mean + std * noise\n",
    "        )\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll *= mask\n",
    "        # print(decoder_nll.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1)/mask.sum(dim=-1)\n",
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1)\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t):\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            pred_xstart = model_output\n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "\n",
    "        else: # predict eps\n",
    "            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "        \n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t\n",
    "            )\n",
    "\n",
    "        return {'pred_xprev':pred_prev, 'pred_xstart':pred_xstart}\n",
    "\n",
    "    def training_losses_seq2seq(self, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs. # not used unless fixing the input embeddings\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        x_start_fix = x_start # save the orignal x_0\n",
    "        assert 'input_ids' in model_kwargs\n",
    "        input_ids_x = model_kwargs.pop('input_ids').to(self.device)\n",
    "        input_ids_mask = model_kwargs.pop('input_mask').to(self.device)\n",
    "        x_start_mean = model.model.get_embeds(input_ids_x).to(self.device)\n",
    "                \n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   torch.tensor([0]),\n",
    "                                   x_start_mean.shape)\n",
    "        # print(std.shape, )\n",
    "        x_start = self._get_x_start(x_start_mean, std)\n",
    "#         print(\"x_start_mean shape: \", x_start_mean.shape, \"x_start shape: \", x_start.shape)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_t = self.q_sample(x_start, t, noise=noise, mask=input_ids_mask) # reparametrization trick.\n",
    "\n",
    "        get_logits = model.model.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
    "        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)\n",
    "        assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t)['pred_xstart'] # predicted_xstart = model_output\n",
    "        t0_mask = (t == 0)\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2)\n",
    "        terms[\"mse\"] = torch.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "\n",
    "        # tT_mask = (t == self.num_timesteps - 1)\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, torch.LongTensor([self.num_timesteps - 1]).to(self.device))\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x) # embedding regularization\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t) # x_0->model_out_x_start\n",
    "        # assert (model.lm_head.weight == model.word_embedding.weight).all()\n",
    "\n",
    "        terms[\"loss\"] = terms[\"mse\"] + decoder_nll + tT_loss\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * torch.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * torch.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = torch.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * torch.sqrt(alpha_bar_prev)\n",
    "            + torch.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # print(sigma.mean())\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        \n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = torch.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap\n",
    "        ):\n",
    "            final.append(sample['sample'])\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        progress=True,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = torch.randn(*shape, device=self.device)\n",
    "        indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = torch.tensor([i] * shape[0], device=self.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = torch.from_numpy(arr).to(device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "\n",
    "def space_timesteps(num_timesteps, section_counts):\n",
    "    \"\"\"\n",
    "    Create a list of timesteps to use from an original diffusion process,\n",
    "    given the number of timesteps we want to take from equally-sized portions\n",
    "    of the original process.\n",
    "\n",
    "    For example, if there's 300 timesteps and the section counts are [10,15,20]\n",
    "    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n",
    "    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n",
    "\n",
    "    If the stride is a string starting with \"ddim\", then the fixed striding\n",
    "    from the DDIM paper is used, and only one section is allowed.\n",
    "\n",
    "    :param num_timesteps: the number of diffusion steps in the original\n",
    "                          process to divide up.\n",
    "    :param section_counts: either a list of numbers, or a string containing\n",
    "                           comma-separated numbers, indicating the step count\n",
    "                           per section. As a special case, use \"ddimN\" where N\n",
    "                           is a number of steps to use the striding from the\n",
    "                           DDIM paper.\n",
    "    :return: a set of diffusion steps from the original process to use.\n",
    "    \"\"\"\n",
    "    if isinstance(section_counts, str):\n",
    "        if section_counts.startswith(\"ddim\"):\n",
    "            desired_count = int(section_counts[len(\"ddim\") :])\n",
    "            for i in range(1, num_timesteps):\n",
    "                if len(range(0, num_timesteps, i)) == desired_count:\n",
    "                    return set(range(0, num_timesteps, i))\n",
    "            raise ValueError(\n",
    "                f\"cannot create exactly {num_timesteps} steps with an integer stride\"\n",
    "            )\n",
    "        section_counts = [int(x) for x in section_counts.split(\",\")]\n",
    "    size_per = num_timesteps // len(section_counts)\n",
    "    extra = num_timesteps % len(section_counts)\n",
    "    start_idx = 0\n",
    "    all_steps = []\n",
    "    for i, section_count in enumerate(section_counts):\n",
    "        size = size_per + (1 if i < extra else 0)\n",
    "        if size < section_count:\n",
    "            raise ValueError(\n",
    "                f\"cannot divide section of {size} steps into {section_count}\"\n",
    "            )\n",
    "        if section_count <= 1:\n",
    "            frac_stride = 1\n",
    "        else:\n",
    "            frac_stride = (size - 1) / (section_count - 1)\n",
    "        cur_idx = 0.0\n",
    "        taken_steps = []\n",
    "        for _ in range(section_count):\n",
    "            taken_steps.append(start_idx + round(cur_idx))\n",
    "            cur_idx += frac_stride\n",
    "        all_steps += taken_steps\n",
    "        start_idx += size\n",
    "    return set(all_steps)\n",
    "\n",
    "\n",
    "class SpacedDiffusion(GaussianDiffusion):\n",
    "    \"\"\"\n",
    "    A diffusion process which can skip steps in a base diffusion process.\n",
    "\n",
    "    :param use_timesteps: a collection (sequence or set) of timesteps from the\n",
    "                          original diffusion process to retain.\n",
    "    :param kwargs: the kwargs to create the base diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_timesteps, **kwargs):\n",
    "        self.use_timesteps = set(use_timesteps)\n",
    "        self.timestep_map = []\n",
    "        self.original_num_steps = len(kwargs[\"betas\"])\n",
    "\n",
    "        # print(kwargs.keys())\n",
    "        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n",
    "        last_alpha_cumprod = 1.0\n",
    "        new_betas = []\n",
    "        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n",
    "            if i in self.use_timesteps:\n",
    "                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n",
    "                last_alpha_cumprod = alpha_cumprod\n",
    "                self.timestep_map.append(i)\n",
    "        kwargs[\"betas\"] = np.array(new_betas)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        # print('called p_mean_var')\n",
    "        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        # print('called training_losses')\n",
    "        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def _wrap_model(self, model):\n",
    "        if isinstance(model, _WrappedModel):\n",
    "            return model\n",
    "        return _WrappedModel(\n",
    "            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n",
    "        )\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        # Scaling is done by the wrapped model.\n",
    "        return t\n",
    "\n",
    "\n",
    "class _WrappedModel:\n",
    "    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps, device=device):\n",
    "        self.model = model\n",
    "        self.timestep_map = timestep_map\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.original_num_steps = original_num_steps\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, x, ts, **kwargs):\n",
    "        # print(ts)\n",
    "        map_tensor = torch.tensor(self.timestep_map, device=self.device, dtype=ts.dtype)\n",
    "        new_ts = map_tensor[ts]\n",
    "        # print(new_ts)\n",
    "        if self.rescale_timesteps:\n",
    "            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n",
    "        return self.model(x, new_ts, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e2e2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nn import mean_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0394b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "# from transformers import BertEncoder\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.nn import (\n",
    "    SiLU,\n",
    "    linear,\n",
    "    timestep_embedding,\n",
    ")\n",
    "\n",
    "class TransformerNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "    :param input_dims: dims of the input Tensor.\n",
    "    :param output_dims: dims of the output Tensor.\n",
    "    :param hidden_t_dim: dims of time embedding.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param config/config_name: thew config of PLMs.\n",
    "    :param init_pretrained: bool, init whole network params with PLMs.\n",
    "    :param vocab_size: the size of vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims,\n",
    "        hidden_t_dim,\n",
    "        dropout=0,\n",
    "        config=None,\n",
    "        config_name='bert-base-uncased',\n",
    "        vocab_size=None,\n",
    "        init_pretrained='no',\n",
    "        logits_mode=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(config_name)\n",
    "            config.hidden_dropout_prob = dropout\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        self.output_dims = output_dims\n",
    "        self.dropout = dropout\n",
    "        self.logits_mode = logits_mode\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "        with torch.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(hidden_t_dim, time_embed_dim),\n",
    "            SiLU(),\n",
    "            linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "                                              nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        \n",
    "        if init_pretrained == 'bert':\n",
    "            print('initializing from pretrained bert...')\n",
    "            print(config)\n",
    "            temp_bert = BertModel.from_pretrained(config_name, config=config)\n",
    "\n",
    "            self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "            with torch.no_grad():\n",
    "                self.lm_head.weight = self.word_embedding.weight\n",
    "            # self.lm_head.weight.requires_grad = False\n",
    "            # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "            self.input_transformers = temp_bert.encoder\n",
    "            self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "            self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "            self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "\n",
    "            del temp_bert.embeddings\n",
    "            del temp_bert.pooler\n",
    "\n",
    "        elif init_pretrained == 'no':\n",
    "            self.input_transformers = BertEncoder(config)\n",
    "\n",
    "            self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "            self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        else:\n",
    "            assert False, \"invalid type of init_pretrained\"\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "\n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.word_embedding(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_repr):\n",
    "        if self.logits_mode == 1:\n",
    "            return self.lm_head(hidden_repr)\n",
    "        elif self.logits_mode == 2: # standard cosine similarity\n",
    "            text_emb = hidden_repr\n",
    "            emb_norm = (self.lm_head.weight ** 2).sum(-1).view(-1, 1)  # vocab\n",
    "            text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # d, bsz*seqlen\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # bsz*seqlen, 1\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(self.lm_head.weight,\n",
    "                                                                     text_emb_t)  # (vocab, d) x (d, bsz*seqlen)\n",
    "            scores = torch.sqrt(torch.clamp(dist, 0.0, np.inf)).view(emb_norm.size(0), hidden_repr.size(0),\n",
    "                                                               hidden_repr.size(1)) # vocab, bsz*seqlen\n",
    "            scores = -scores.permute(1, 2, 0).contiguous()\n",
    "            return scores\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        emb_t = self.time_embed(timestep_embedding(timesteps, self.hidden_t_dim))\n",
    "\n",
    "        if self.input_dims != self.hidden_size:\n",
    "            emb_x = self.input_up_proj(x)\n",
    "        else:\n",
    "            emb_x = x\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, : seq_length ]\n",
    "        # print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
    "        emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "\n",
    "        input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "        \n",
    "        if self.output_dims != self.hidden_size:\n",
    "            h = self.output_down_proj(input_trans_hidden_states)\n",
    "        else:\n",
    "            h = input_trans_hidden_states\n",
    "        h = h.type(x.dtype)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9b25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_diffusion(\n",
    "    hidden_t_dim,\n",
    "    hidden_dim,\n",
    "    vocab_size,\n",
    "    config_name,\n",
    "    use_plm_init,\n",
    "    dropout,\n",
    "    diffusion_steps,\n",
    "    noise_schedule,\n",
    "    learn_sigma,\n",
    "    timestep_respacing,\n",
    "    predict_xstart,\n",
    "    rescale_timesteps,\n",
    "    sigma_small,\n",
    "    rescale_learned_sigmas,\n",
    "    use_kl,\n",
    "    **kwargs,\n",
    "):\n",
    "    model = TransformerNetModel(\n",
    "        input_dims=hidden_dim,\n",
    "        output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "        hidden_t_dim=hidden_t_dim,\n",
    "        dropout=dropout,\n",
    "        config_name=config_name,\n",
    "        vocab_size=vocab_size,\n",
    "        init_pretrained=use_plm_init\n",
    "    )\n",
    "\n",
    "    betas = get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "\n",
    "    if not timestep_respacing:\n",
    "        timestep_respacing = [diffusion_steps]\n",
    "\n",
    "    diffusion = SpacedDiffusion(\n",
    "        use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "        betas=betas,\n",
    "        rescale_timesteps=rescale_timesteps,\n",
    "        predict_xstart=predict_xstart,\n",
    "        learn_sigmas = learn_sigma,\n",
    "        sigma_small = sigma_small,\n",
    "        use_kl = use_kl,\n",
    "        rescale_learned_sigmas=rescale_learned_sigmas\n",
    "    )\n",
    "\n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('custom', config_name, custom_vocab_fp=None, custom_vocab_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "827ec60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=27829, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(27829, 64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2542d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45543e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27829"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['so shaken as we are so wan with care', 'find we a time for fright peace to pant'] ['find we a time for fright peace to pant', 'and breathe short-wind accents of new broils']\n",
      "RAM used: 3361.38 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 110838\n",
      "})\n",
      "RAM used: 3390.95 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ead53eb7eec44eeae86902e374e46dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/110838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 110838\n",
      "})\n",
      "### tokenized_datasets...example [2, 150, 12941, 142, 137, 205, 150, 7309, 130, 1015, 3]\n",
      "RAM used: 3441.02 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75111c950b934c19b3ef32e3a0ac580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/110838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3484.17 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a0bdafac9c45afa084ae861d40c05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/110838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 110838\n",
      "}) padded dataset\n",
      "RAM used: 3671.23 MB\n",
      "RAM used: 3671.23 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2749334",
   "metadata": {},
   "source": [
    "Passed in as batch in TrainLoop - this is the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02903fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(data)[0].shape # batch_size, seq_len, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f5f7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d608d",
   "metadata": {},
   "source": [
    "Passed in as cond in TrainLoop - this is a dictionary of input_ids and input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44f6e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c1d8eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(data)[1]['input_ids'].shape # batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39c90139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[1]['input_mask'].shape # batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c7980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85a49540",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        learn_sigma,\n",
    "                        timestep_respacing,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                        sigma_small,\n",
    "                        rescale_learned_sigmas,\n",
    "                        use_kl\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b4bbaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(27829, 64)\n",
       "  (lm_head): Linear(in_features=64, out_features=27829, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a953179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27829, 64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c1ae1a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_util.dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80784a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88998453"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee9ad85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bcb1caf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<step_sample.UniformSampler at 0x7f0ca5441130>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47493837",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n"
     ]
    }
   ],
   "source": [
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        log_interval=log_interval,\n",
    "        save_interval=save_interval,\n",
    "        resume_checkpoint=resume_checkpoint,\n",
    "        use_fp16=use_fp16,\n",
    "        fp16_scale_growth=fp16_scale_growth,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        learning_steps=learning_steps,\n",
    "        gradient_clipping=gradient_clipping,\n",
    "#         eval_data=data_valid,\n",
    "        eval_interval=eval_interval\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee01293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96e67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e094b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99197091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f84d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d615ee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(27829, 64)\n",
       "  (lm_head): Linear(in_features=64, out_features=27829, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().requires_grad_(False).to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67362550",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_emb = torch.nn.Embedding(\n",
    "        num_embeddings=tokenizer.vocab_size, \n",
    "        embedding_dim=hidden_dim, \n",
    "        _weight=model.word_embedding.weight.clone().cpu()\n",
    "    ).eval().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9807fbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TEST set...\n",
      "### Data samples...\n",
      " ['so shaken as we are so wan with care', 'find we a time for fright peace to pant'] ['find we a time for fright peace to pant', 'and breathe short-wind accents of new broils']\n",
      "RAM used: 2833.17 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 60\n",
      "})\n",
      "RAM used: 2833.18 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c064260ac2704cdb854c1c4d76cdf32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 60\n",
      "})\n",
      "### tokenized_datasets...example [2, 150, 12941, 142, 137, 205, 150, 7309, 130, 1015, 3]\n",
      "RAM used: 2834.34 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bbfbd38b9f45de8c7e7929d8d01826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2834.43 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90680bac2b8a44c193900e32c79c1971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 60\n",
      "}) padded dataset\n",
      "RAM used: 2834.71 MB\n",
      "RAM used: 2834.71 MB\n"
     ]
    }
   ],
   "source": [
    "data_valid = load_data_text(\n",
    "        batch_size=5,\n",
    "        seq_len=seq_len,\n",
    "        deterministic=True,\n",
    "        data_dir=data_dir,\n",
    "        split=\"test\",\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_emb.cpu(),  # using the same embedding wight with tranining data\n",
    "        loop=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0aa23ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(27829, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_test_data = []\n",
    "\n",
    "idx = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        batch, cond = next(data_valid)\n",
    "        # print(batch.shape)\n",
    "        all_test_data.append(cond)\n",
    "        idx += 1\n",
    "\n",
    "except StopIteration:\n",
    "    print('### End of reading iteration...')\n",
    "\n",
    "model_emb.to(dist_util.dev())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdbe4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "237c3441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SpacedDiffusion at 0x7f0cf8a37490>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d45d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficient_knn(model_emb, text_emb):\n",
    "    emb_norm = (model_emb**2).sum(-1).view(-1, 1) # vocab\n",
    "    text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1) # d, bsz*seqlen\n",
    "    arr_norm = (text_emb ** 2).sum(-1).view(-1, 1) # bsz*seqlen, 1\n",
    "    # print(emb_norm.shape, arr_norm.shape)\n",
    "    dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(model_emb, text_emb_t) # (vocab, d) x (d, bsz*seqlen)\n",
    "    dist = torch.clamp(dist, 0.0, np.inf)\n",
    "    # print(dist.shape)\n",
    "    topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "    return topk_out.values, topk_out.indices\n",
    "\n",
    "def denoised_fn_round(model, text_emb, t):\n",
    "    # print(text_emb.shape) # bsz, seqlen, dim\n",
    "    model_emb = model.weight  # input_embs\n",
    "    # print(t)\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "    # val, indices = get_knn(model_emb, text_emb.to(model_emb.device), dist=dist)\n",
    "    val, indices = get_efficient_knn(model_emb, text_emb.to(model_emb.device))\n",
    "    rounded_tokens = indices[0]\n",
    "    # print(rounded_tokens.shape)\n",
    "    new_embeds = model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c4fbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1000\n",
    "clip_denoised = False\n",
    "model_kwargs = {}\n",
    "top_p = 0\n",
    "clamp_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "842f74df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9967a94d7c46d4baedd7ca759bc4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-31 11:30:56.748524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(all_test_data)\n",
    "word_lst_recover = []\n",
    "word_lst_ref = []\n",
    "word_lst_source = []\n",
    "\n",
    "for cond in iterator:\n",
    "\n",
    "    input_ids_x = cond.pop('input_ids').to(dist_util.dev())\n",
    "    x_start = model.get_embeds(input_ids_x)\n",
    "    input_ids_mask = cond.pop('input_mask')\n",
    "    input_ids_mask_ori = input_ids_mask\n",
    "\n",
    "    noise = torch.randn_like(x_start)\n",
    "    input_ids_mask = torch.broadcast_to(input_ids_mask.unsqueeze(dim=-1), x_start.shape).to(dist_util.dev())\n",
    "    x_noised = torch.where(input_ids_mask == 0, x_start, noise)\n",
    "\n",
    "    model_kwargs = {}\n",
    "\n",
    "    if step == diffusion_steps:\n",
    "        use_ddim = False\n",
    "        step_gap = 1\n",
    "    else:\n",
    "        use_ddim = True\n",
    "        step_gap = diffusion_steps//step\n",
    "\n",
    "    sample_fn = (\n",
    "        diffusion.p_sample_loop if not use_ddim else diffusion.ddim_sample_loop\n",
    "    )\n",
    "\n",
    "    sample_shape = (x_start.shape[0], seq_len, hidden_dim)\n",
    "\n",
    "    samples = sample_fn(\n",
    "        model,\n",
    "        sample_shape,\n",
    "        noise=x_noised,\n",
    "        clip_denoised=clip_denoised,\n",
    "        denoised_fn=partial(denoised_fn_round, model_emb),\n",
    "        model_kwargs=model_kwargs,\n",
    "        top_p=top_p,\n",
    "        clamp_step=clamp_step,\n",
    "        clamp_first=True,\n",
    "        mask=input_ids_mask,\n",
    "        x_start=x_start,\n",
    "        gap=step_gap\n",
    "    )\n",
    "\n",
    "    # print(samples[0].shape) # samples for each step\n",
    "\n",
    "    sample = samples[-1]\n",
    "\n",
    "    # print('decoding for seq2seq', )\n",
    "    # print(sample.shape)\n",
    "\n",
    "    logits = model.get_logits(sample)  # bsz, seqlen, vocab\n",
    "    cands = torch.topk(logits, k=1, dim=-1)\n",
    "\n",
    "#     word_lst_recover = []\n",
    "#     word_lst_ref = []\n",
    "#     word_lst_source = []\n",
    "\n",
    "    # tokenizer = load_tokenizer(args)\n",
    "\n",
    "    for seq, input_mask in zip(cands.indices, input_ids_mask_ori):\n",
    "        len_x = seq_len - sum(input_mask).tolist()\n",
    "        tokens = tokenizer.decode_token(seq[len_x:])\n",
    "        word_lst_recover.append(tokens)\n",
    "\n",
    "    for seq, input_mask in zip(input_ids_x, input_ids_mask_ori):\n",
    "        # tokens = tokenizer.decode_token(seq)\n",
    "        len_x = seq_len - sum(input_mask).tolist()\n",
    "        word_lst_source.append(tokenizer.decode_token(seq[:len_x]))\n",
    "        word_lst_ref.append(tokenizer.decode_token(seq[len_x:]))\n",
    "    break # after 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fb4555a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] so shaken as we are so wan with care [SEP] [SEP]',\n",
       " '[CLS] find we a time for fright peace to pant [SEP] [SEP]',\n",
       " '[CLS] and breathe short - wind accents of new broils [SEP] [SEP]',\n",
       " '[CLS] to be commenc in strands afar remote < eos > [SEP] [SEP]',\n",
       " '[CLS] no more the thirsty entrance of this soil [SEP] [SEP]']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9184761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['approbation [SEP] spoils [CLS] [PAD] boggler meddl [PAD] [CLS] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] spoils',\n",
       " '[CLS] spoils mender > eastern boggler [PAD] meddl eos [PAD] [PAD] [PAD] [PAD] therewithal [PAD] [PAD] beforehand',\n",
       " '[CLS] dulness whoresons spoils [SEP] [PAD] [PAD] beforehand [PAD] [PAD] [PAD] of [PAD] [PAD] [PAD] [PAD] [PAD] petitioner',\n",
       " '[CLS] [CLS] pha [PAD] [CLS] [PAD] [PAD] pha vomissement [SEP] [SEP] [PAD] [SEP] [PAD] [PAD] [SEP]',\n",
       " '[CLS] of [CLS] of [PAD] [PAD] [PAD] of [PAD] meddl [PAD] > spoils petitioner']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "170fa620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] find we a time for fright peace to pant [SEP]',\n",
       " '[CLS] and breathe short - wind accents of new broils [SEP]',\n",
       " '[CLS] to be commenc in strands afar remote < eos > [SEP]',\n",
       " '[CLS] no more the thirsty entrance of this soil [SEP]',\n",
       " \"[CLS] shall daub her lips with her own children's blood [SEP]\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20392d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
