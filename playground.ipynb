{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d47e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 20:02:31.438532: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from run_train import create_model_and_diffusion\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from tokenizer import load_tokenizer, load_model_emb\n",
    "from sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch, os\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "419eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab_list.pickle', 'rb') as handle:\n",
    "#     vocab_list = pickle.load(handle)\n",
    "# vocab_list = list(vocab_list.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=20\n",
    "microbatch=10\n",
    "epochs=15_000\n",
    "eval_interval=1000\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=1_000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=300\n",
    "hidden_dim=300\n",
    "dropout=0.1\n",
    "seed=102\n",
    "weight_decay=0.0001\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_dir='data/commonsense'\n",
    "ss_data_dir='data/shakespeare'\n",
    "ss_small_data_dir='data/mini-shakespeare'\n",
    "combined_data_dir='data/combined'\n",
    "combined_small_data_dir='data/combined/small'\n",
    "regular_data_dir='data'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('shakespeare_plays', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd3fbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1701606f-3211-4741-a7f4-bc4ee146c4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 604, 134, 24, 413, 119, 2987, 640, 90, 7166, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_token('find we a time for fright peace to pant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(33747, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2542d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45543e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33747"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2749334",
   "metadata": {},
   "source": [
    "Passed in as batch in TrainLoop - this is the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02903fcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(data)[0].shape # batch_size, seq_len, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5f7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d608d",
   "metadata": {},
   "source": [
    "Passed in as cond in TrainLoop - this is a dictionary of input_ids and input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44f6e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1d8eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# next(data)[1]['input_ids'].shape # batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39c90139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(data)[1]['input_mask'].shape # batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a49540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e524300758604db8994dd5e3f470d145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b4bbaf6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(33747, 300)\n",
       "  (lm_head): Linear(in_features=300, out_features=33747, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=1200, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80784a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98533683"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47493837",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " [\"something of moment then i will go meet him there's matter in't indeed, if he be angry. i prithee, do so.\", 'conscience, which is, indeed, sir, a mender of bad soles. what trade, thou knave? thou naughty knave, what trade? nay, i beseech you, sir, be not out with me yet,'] ['exit iago something, sure, of state, either from venice, or some unhatched practise', 'if you be out, sir, i can mend you. what meanest thou by that? mend me, thou saucy fellow! why, sir, cobble you.']\n",
      "RAM used: 2670.82 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 107264\n",
      "})\n",
      "RAM used: 2709.53 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17519eeeba648b4bf07c98040553886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/107264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 107264\n",
      "})\n",
      "### tokenized_datasets...example [2, 1340, 96, 4074, 265, 32, 163, 187, 791, 156, 213, 7, 42, 811, 105, 7, 43, 744, 10, 197, 108, 104, 2085, 11, 32, 1185, 10, 147, 151, 11, 3]\n",
      "RAM used: 2766.52 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0a17481cd24f59bc883093e6cff73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/107264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2878.09 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4031a0790bc14ca5a7da7c1628becd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/107264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 107264\n",
      "}) padded dataset\n",
      "RAM used: 3088.38 MB\n",
      "RAM used: 3064.38 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n",
      "Epoch 0/15000 Loss: 1.0451031923294067\n",
      "Epoch 1/15000 Loss: 0.869488000869751\n",
      "Epoch 2/15000 Loss: 0.781876802444458\n",
      "Epoch 3/15000 Loss: 0.7459273934364319\n",
      "Epoch 4/15000 Loss: 0.7409336566925049\n",
      "Epoch 5/15000 Loss: 0.7291581630706787\n",
      "Epoch 6/15000 Loss: 0.721569299697876\n",
      "Epoch 7/15000 Loss: 0.6996433138847351\n",
      "Epoch 8/15000 Loss: 0.7033931016921997\n",
      "Epoch 9/15000 Loss: 0.7397572994232178\n",
      "Epoch 10/15000 Loss: 0.7111833691596985\n",
      "Epoch 11/15000 Loss: 0.7138040065765381\n",
      "Epoch 12/15000 Loss: 0.7373813390731812\n",
      "Epoch 13/15000 Loss: 0.7302639484405518\n",
      "Epoch 14/15000 Loss: 0.7417792081832886\n",
      "Epoch 15/15000 Loss: 0.685096263885498\n",
      "Epoch 16/15000 Loss: 0.711911141872406\n",
      "Epoch 17/15000 Loss: 0.7061750888824463\n",
      "Epoch 18/15000 Loss: 0.7069834470748901\n",
      "Epoch 19/15000 Loss: 0.7299482226371765\n",
      "Epoch 20/15000 Loss: 0.6960338354110718\n",
      "Epoch 21/15000 Loss: 0.7062292695045471\n",
      "Epoch 22/15000 Loss: 0.7168128490447998\n",
      "Epoch 23/15000 Loss: 0.7048308253288269\n",
      "Epoch 24/15000 Loss: 0.7262097597122192\n",
      "Epoch 25/15000 Loss: 0.6668027639389038\n",
      "Epoch 26/15000 Loss: 0.6858556270599365\n",
      "Epoch 27/15000 Loss: 0.6681382656097412\n",
      "Epoch 28/15000 Loss: 0.6054685115814209\n",
      "Epoch 29/15000 Loss: 0.5642874836921692\n",
      "Epoch 30/15000 Loss: 0.5582656860351562\n",
      "Epoch 31/15000 Loss: 0.5330401659011841\n",
      "Epoch 32/15000 Loss: 0.513293981552124\n",
      "Epoch 33/15000 Loss: 0.48554378747940063\n",
      "Epoch 34/15000 Loss: 0.475796103477478\n",
      "Epoch 35/15000 Loss: 0.4775688350200653\n",
      "Epoch 36/15000 Loss: 0.5023633241653442\n",
      "Epoch 37/15000 Loss: 0.4608196020126343\n",
      "Epoch 38/15000 Loss: 0.4787721037864685\n",
      "Epoch 39/15000 Loss: 0.4678787589073181\n",
      "Epoch 40/15000 Loss: 0.48753082752227783\n",
      "Epoch 41/15000 Loss: 0.49422937631607056\n",
      "Epoch 42/15000 Loss: 0.4870045781135559\n",
      "Epoch 43/15000 Loss: 0.4702971577644348\n",
      "Epoch 44/15000 Loss: 0.4854605793952942\n",
      "Epoch 45/15000 Loss: 0.46615225076675415\n",
      "Epoch 46/15000 Loss: 0.4899423122406006\n",
      "Epoch 47/15000 Loss: 0.498502641916275\n",
      "Epoch 48/15000 Loss: 0.48082295060157776\n",
      "Epoch 49/15000 Loss: 0.47364646196365356\n",
      "Epoch 50/15000 Loss: 0.49434810876846313\n",
      "Epoch 51/15000 Loss: 0.4605039358139038\n",
      "Epoch 52/15000 Loss: 0.48527902364730835\n",
      "Epoch 53/15000 Loss: 0.4715818762779236\n",
      "Epoch 54/15000 Loss: 0.4832780361175537\n",
      "Epoch 55/15000 Loss: 0.49170905351638794\n",
      "Epoch 56/15000 Loss: 0.4707834720611572\n",
      "Epoch 57/15000 Loss: 0.497720330953598\n",
      "Epoch 58/15000 Loss: 0.49552708864212036\n",
      "Epoch 59/15000 Loss: 0.4795041084289551\n",
      "Epoch 60/15000 Loss: 0.4536058008670807\n",
      "Epoch 61/15000 Loss: 0.49346500635147095\n",
      "Epoch 62/15000 Loss: 0.46756136417388916\n",
      "Epoch 63/15000 Loss: 0.46888068318367004\n",
      "Epoch 64/15000 Loss: 0.4534236192703247\n",
      "Epoch 65/15000 Loss: 0.4733757972717285\n",
      "Epoch 66/15000 Loss: 0.45702698826789856\n",
      "Epoch 67/15000 Loss: 0.44620972871780396\n",
      "Epoch 68/15000 Loss: 0.464316725730896\n",
      "Epoch 69/15000 Loss: 0.46940937638282776\n",
      "Epoch 70/15000 Loss: 0.43247658014297485\n",
      "Epoch 71/15000 Loss: 0.46463286876678467\n",
      "Epoch 72/15000 Loss: 0.46405938267707825\n",
      "Epoch 73/15000 Loss: 0.4556012749671936\n",
      "Epoch 74/15000 Loss: 0.4531179666519165\n",
      "Epoch 75/15000 Loss: 0.4227983355522156\n",
      "Epoch 76/15000 Loss: 0.45787352323532104\n",
      "Epoch 77/15000 Loss: 0.43606051802635193\n",
      "Epoch 78/15000 Loss: 0.45009085536003113\n",
      "Epoch 79/15000 Loss: 0.4334244430065155\n",
      "Epoch 80/15000 Loss: 0.41246768832206726\n",
      "Epoch 81/15000 Loss: 0.41853946447372437\n",
      "Epoch 82/15000 Loss: 0.44781917333602905\n",
      "Epoch 83/15000 Loss: 0.43518611788749695\n",
      "Epoch 84/15000 Loss: 0.4382520914077759\n",
      "Epoch 85/15000 Loss: 0.42192262411117554\n",
      "Epoch 86/15000 Loss: 0.44683438539505005\n",
      "Epoch 87/15000 Loss: 0.43127134442329407\n",
      "Epoch 88/15000 Loss: 0.43904924392700195\n",
      "Epoch 89/15000 Loss: 0.4157004654407501\n",
      "Epoch 90/15000 Loss: 0.42809444665908813\n",
      "Epoch 91/15000 Loss: 0.40893977880477905\n",
      "Epoch 92/15000 Loss: 0.4154190421104431\n",
      "Epoch 93/15000 Loss: 0.41160476207733154\n",
      "Epoch 94/15000 Loss: 0.42776578664779663\n",
      "Epoch 95/15000 Loss: 0.41618311405181885\n",
      "Epoch 96/15000 Loss: 0.4130988121032715\n",
      "Epoch 97/15000 Loss: 0.4113130271434784\n",
      "Epoch 98/15000 Loss: 0.4161771535873413\n",
      "Epoch 99/15000 Loss: 0.39576455950737\n",
      "Epoch 100/15000 Loss: 0.42854517698287964\n",
      "Epoch 101/15000 Loss: 0.3984562158584595\n",
      "Epoch 102/15000 Loss: 0.4086090326309204\n",
      "Epoch 103/15000 Loss: 0.40079039335250854\n",
      "Epoch 104/15000 Loss: 0.3930166959762573\n",
      "Epoch 105/15000 Loss: 0.3949689567089081\n",
      "Epoch 106/15000 Loss: 0.40471625328063965\n",
      "Epoch 107/15000 Loss: 0.400341659784317\n",
      "Epoch 108/15000 Loss: 0.3849135637283325\n",
      "Epoch 109/15000 Loss: 0.38444992899894714\n",
      "Epoch 110/15000 Loss: 0.37986278533935547\n",
      "Epoch 111/15000 Loss: 0.41459959745407104\n",
      "Epoch 112/15000 Loss: 0.3901810944080353\n",
      "Epoch 113/15000 Loss: 0.38147270679473877\n",
      "Epoch 114/15000 Loss: 0.3953592777252197\n",
      "Epoch 115/15000 Loss: 0.3554966449737549\n",
      "Epoch 116/15000 Loss: 0.3987034857273102\n",
      "Epoch 117/15000 Loss: 0.3766310214996338\n",
      "Epoch 118/15000 Loss: 0.40348684787750244\n",
      "Epoch 119/15000 Loss: 0.38010352849960327\n",
      "Epoch 120/15000 Loss: 0.37888598442077637\n",
      "Epoch 121/15000 Loss: 0.3796669840812683\n",
      "Epoch 122/15000 Loss: 0.35668057203292847\n",
      "Epoch 123/15000 Loss: 0.3655298054218292\n",
      "Epoch 124/15000 Loss: 0.3625375032424927\n",
      "Epoch 125/15000 Loss: 0.35874396562576294\n",
      "Epoch 126/15000 Loss: 0.39010870456695557\n",
      "Epoch 127/15000 Loss: 0.36774808168411255\n",
      "Epoch 128/15000 Loss: 0.3511936664581299\n",
      "Epoch 129/15000 Loss: 0.34164804220199585\n",
      "Epoch 130/15000 Loss: 0.3530768156051636\n",
      "Epoch 131/15000 Loss: 0.36497360467910767\n",
      "Epoch 132/15000 Loss: 0.3516259789466858\n",
      "Epoch 133/15000 Loss: 0.36633166670799255\n",
      "Epoch 134/15000 Loss: 0.34570440649986267\n",
      "Epoch 135/15000 Loss: 0.35354167222976685\n",
      "Epoch 136/15000 Loss: 0.3673556447029114\n",
      "Epoch 137/15000 Loss: 0.35099250078201294\n",
      "Epoch 138/15000 Loss: 0.3367499113082886\n",
      "Epoch 139/15000 Loss: 0.3742208182811737\n",
      "Epoch 140/15000 Loss: 0.3442683815956116\n",
      "Epoch 141/15000 Loss: 0.3319701850414276\n",
      "Epoch 142/15000 Loss: 0.3563687205314636\n",
      "Epoch 143/15000 Loss: 0.35556963086128235\n",
      "Epoch 144/15000 Loss: 0.34310030937194824\n",
      "Epoch 145/15000 Loss: 0.32895487546920776\n",
      "Epoch 146/15000 Loss: 0.3214914798736572\n",
      "Epoch 147/15000 Loss: 0.31853437423706055\n",
      "Epoch 148/15000 Loss: 0.3385752737522125\n",
      "Epoch 149/15000 Loss: 0.36397111415863037\n",
      "Epoch 150/15000 Loss: 0.31088787317276\n",
      "Epoch 151/15000 Loss: 0.3328585922718048\n",
      "Epoch 152/15000 Loss: 0.388380765914917\n",
      "Epoch 153/15000 Loss: 0.32110482454299927\n",
      "Epoch 154/15000 Loss: 0.31883910298347473\n",
      "Epoch 155/15000 Loss: 0.3332338333129883\n",
      "Epoch 156/15000 Loss: 0.33683523535728455\n",
      "Epoch 157/15000 Loss: 0.31674718856811523\n",
      "Epoch 158/15000 Loss: 0.32272493839263916\n",
      "Epoch 159/15000 Loss: 0.3255314826965332\n",
      "Epoch 160/15000 Loss: 0.34864211082458496\n",
      "Epoch 161/15000 Loss: 0.31918588280677795\n",
      "Epoch 162/15000 Loss: 0.3070378303527832\n",
      "Epoch 163/15000 Loss: 0.31716468930244446\n",
      "Epoch 164/15000 Loss: 0.32329899072647095\n",
      "Epoch 165/15000 Loss: 0.3179556131362915\n",
      "Epoch 166/15000 Loss: 0.31590479612350464\n",
      "Epoch 167/15000 Loss: 0.3506685495376587\n",
      "Epoch 168/15000 Loss: 0.3189537525177002\n",
      "Epoch 169/15000 Loss: 0.3205726742744446\n",
      "Epoch 170/15000 Loss: 0.29680687189102173\n",
      "Epoch 171/15000 Loss: 0.336957186460495\n",
      "Epoch 172/15000 Loss: 0.3079296946525574\n",
      "Epoch 173/15000 Loss: 0.3245169520378113\n",
      "Epoch 174/15000 Loss: 0.32748275995254517\n",
      "Epoch 175/15000 Loss: 0.28977179527282715\n",
      "Epoch 176/15000 Loss: 0.2935359477996826\n",
      "Epoch 177/15000 Loss: 0.30964750051498413\n",
      "Epoch 178/15000 Loss: 0.28744781017303467\n",
      "Epoch 179/15000 Loss: 0.3381980359554291\n",
      "Epoch 180/15000 Loss: 0.30969345569610596\n",
      "Epoch 181/15000 Loss: 0.31424736976623535\n",
      "Epoch 182/15000 Loss: 0.3213076889514923\n",
      "Epoch 183/15000 Loss: 0.3064518868923187\n",
      "Epoch 184/15000 Loss: 0.32020822167396545\n",
      "Epoch 185/15000 Loss: 0.31256261467933655\n",
      "Epoch 186/15000 Loss: 0.2956851124763489\n",
      "Epoch 187/15000 Loss: 0.2944888174533844\n",
      "Epoch 188/15000 Loss: 0.27926963567733765\n",
      "Epoch 189/15000 Loss: 0.3099139332771301\n",
      "Epoch 190/15000 Loss: 0.2928122878074646\n",
      "Epoch 191/15000 Loss: 0.3279839754104614\n",
      "Epoch 192/15000 Loss: 0.2783799171447754\n",
      "Epoch 193/15000 Loss: 0.2922061085700989\n",
      "Epoch 194/15000 Loss: 0.28308820724487305\n",
      "Epoch 195/15000 Loss: 0.2918732762336731\n",
      "Epoch 196/15000 Loss: 0.2902289927005768\n",
      "Epoch 197/15000 Loss: 0.2983069121837616\n",
      "Epoch 198/15000 Loss: 0.29308849573135376\n",
      "Epoch 199/15000 Loss: 0.2858666181564331\n",
      "Epoch 200/15000 Loss: 0.281982958316803\n",
      "Epoch 201/15000 Loss: 0.3031933307647705\n",
      "Epoch 202/15000 Loss: 0.2712132930755615\n",
      "Epoch 203/15000 Loss: 0.29814767837524414\n",
      "Epoch 204/15000 Loss: 0.28018057346343994\n",
      "Epoch 205/15000 Loss: 0.2785540819168091\n",
      "Epoch 206/15000 Loss: 0.28283825516700745\n",
      "Epoch 207/15000 Loss: 0.2990100085735321\n",
      "Epoch 208/15000 Loss: 0.29187214374542236\n",
      "Epoch 209/15000 Loss: 0.26417437195777893\n",
      "Epoch 210/15000 Loss: 0.2674850821495056\n",
      "Epoch 211/15000 Loss: 0.2817743420600891\n",
      "Epoch 212/15000 Loss: 0.2559950053691864\n",
      "Epoch 213/15000 Loss: 0.2931152582168579\n",
      "Epoch 214/15000 Loss: 0.28990599513053894\n",
      "Epoch 215/15000 Loss: 0.2673307955265045\n",
      "Epoch 216/15000 Loss: 0.2727630138397217\n",
      "Epoch 217/15000 Loss: 0.28074151277542114\n",
      "Epoch 218/15000 Loss: 0.2720490097999573\n",
      "Epoch 219/15000 Loss: 0.2761114835739136\n",
      "Epoch 220/15000 Loss: 0.2737251818180084\n",
      "Epoch 221/15000 Loss: 0.26516932249069214\n",
      "Epoch 222/15000 Loss: 0.2686893939971924\n",
      "Epoch 223/15000 Loss: 0.27887097001075745\n",
      "Epoch 224/15000 Loss: 0.26954615116119385\n",
      "Epoch 225/15000 Loss: 0.26768967509269714\n",
      "Epoch 226/15000 Loss: 0.26831290125846863\n",
      "Epoch 227/15000 Loss: 0.26268497109413147\n",
      "Epoch 228/15000 Loss: 0.27950096130371094\n",
      "Epoch 229/15000 Loss: 0.2618715167045593\n",
      "Epoch 230/15000 Loss: 0.2502235174179077\n",
      "Epoch 231/15000 Loss: 0.25613918900489807\n",
      "Epoch 232/15000 Loss: 0.26356279850006104\n",
      "Epoch 233/15000 Loss: 0.2656252682209015\n",
      "Epoch 234/15000 Loss: 0.26182758808135986\n",
      "Epoch 235/15000 Loss: 0.2506658136844635\n",
      "Epoch 236/15000 Loss: 0.25996166467666626\n",
      "Epoch 237/15000 Loss: 0.25901544094085693\n",
      "Epoch 238/15000 Loss: 0.27274635434150696\n",
      "Epoch 239/15000 Loss: 0.2493101805448532\n",
      "Epoch 240/15000 Loss: 0.2610487937927246\n",
      "Epoch 241/15000 Loss: 0.23329539597034454\n",
      "Epoch 242/15000 Loss: 0.24862909317016602\n",
      "Epoch 243/15000 Loss: 0.2859724164009094\n",
      "Epoch 244/15000 Loss: 0.25639235973358154\n",
      "Epoch 245/15000 Loss: 0.270355224609375\n",
      "Epoch 246/15000 Loss: 0.2495821863412857\n",
      "Epoch 247/15000 Loss: 0.24977904558181763\n",
      "Epoch 248/15000 Loss: 0.2428252398967743\n",
      "Epoch 249/15000 Loss: 0.2640375792980194\n",
      "Epoch 250/15000 Loss: 0.26732295751571655\n",
      "Epoch 251/15000 Loss: 0.2727460265159607\n",
      "Epoch 252/15000 Loss: 0.250108540058136\n",
      "Epoch 253/15000 Loss: 0.2628273367881775\n",
      "Epoch 254/15000 Loss: 0.25197070837020874\n",
      "Epoch 255/15000 Loss: 0.23785251379013062\n",
      "Epoch 256/15000 Loss: 0.22609040141105652\n",
      "Epoch 257/15000 Loss: 0.26348668336868286\n",
      "Epoch 258/15000 Loss: 0.26051899790763855\n",
      "Epoch 259/15000 Loss: 0.23691073060035706\n",
      "Epoch 260/15000 Loss: 0.2538989782333374\n",
      "Epoch 261/15000 Loss: 0.24412140250205994\n",
      "Epoch 262/15000 Loss: 0.241182342171669\n",
      "Epoch 263/15000 Loss: 0.24384254217147827\n",
      "Epoch 264/15000 Loss: 0.25164180994033813\n",
      "Epoch 265/15000 Loss: 0.23241080343723297\n",
      "Epoch 266/15000 Loss: 0.2555786669254303\n",
      "Epoch 267/15000 Loss: 0.24975740909576416\n",
      "Epoch 268/15000 Loss: 0.24194762110710144\n",
      "Epoch 269/15000 Loss: 0.24825173616409302\n",
      "Epoch 270/15000 Loss: 0.2517673969268799\n",
      "Epoch 271/15000 Loss: 0.2316022664308548\n",
      "Epoch 272/15000 Loss: 0.23398566246032715\n",
      "Epoch 273/15000 Loss: 0.24392127990722656\n",
      "Epoch 274/15000 Loss: 0.2601301968097687\n",
      "Epoch 275/15000 Loss: 0.265384316444397\n",
      "Epoch 276/15000 Loss: 0.25056931376457214\n",
      "Epoch 277/15000 Loss: 0.2413722574710846\n",
      "Epoch 278/15000 Loss: 0.2381521463394165\n",
      "Epoch 279/15000 Loss: 0.24878381192684174\n",
      "Epoch 280/15000 Loss: 0.22171954810619354\n",
      "Epoch 281/15000 Loss: 0.25276654958724976\n",
      "Epoch 282/15000 Loss: 0.23427721858024597\n",
      "Epoch 283/15000 Loss: 0.23916593194007874\n",
      "Epoch 284/15000 Loss: 0.22823938727378845\n",
      "Epoch 285/15000 Loss: 0.21925300359725952\n",
      "Epoch 286/15000 Loss: 0.23342150449752808\n",
      "Epoch 287/15000 Loss: 0.24964094161987305\n",
      "Epoch 288/15000 Loss: 0.2568490505218506\n",
      "Epoch 289/15000 Loss: 0.21622546017169952\n",
      "Epoch 290/15000 Loss: 0.2174079716205597\n",
      "Epoch 291/15000 Loss: 0.2484043538570404\n",
      "Epoch 292/15000 Loss: 0.2421213984489441\n",
      "Epoch 293/15000 Loss: 0.2513388693332672\n",
      "Epoch 294/15000 Loss: 0.23314402997493744\n",
      "Epoch 295/15000 Loss: 0.2259701043367386\n",
      "Epoch 296/15000 Loss: 0.23831307888031006\n",
      "Epoch 297/15000 Loss: 0.23480293154716492\n",
      "Epoch 298/15000 Loss: 0.2170824110507965\n",
      "Epoch 299/15000 Loss: 0.22351063787937164\n",
      "Epoch 300/15000 Loss: 0.26172688603401184\n",
      "Epoch 301/15000 Loss: 0.2106550931930542\n",
      "Epoch 302/15000 Loss: 0.25071197748184204\n",
      "Epoch 303/15000 Loss: 0.24138742685317993\n",
      "Epoch 304/15000 Loss: 0.22717608511447906\n",
      "Epoch 305/15000 Loss: 0.23641648888587952\n",
      "Epoch 306/15000 Loss: 0.22291874885559082\n",
      "Epoch 307/15000 Loss: 0.25414371490478516\n",
      "Epoch 308/15000 Loss: 0.22357338666915894\n",
      "Epoch 309/15000 Loss: 0.22311705350875854\n",
      "Epoch 310/15000 Loss: 0.2269364297389984\n",
      "Epoch 311/15000 Loss: 0.22663849592208862\n",
      "Epoch 312/15000 Loss: 0.23862208425998688\n",
      "Epoch 313/15000 Loss: 0.2121964395046234\n",
      "Epoch 314/15000 Loss: 0.2223517745733261\n",
      "Epoch 315/15000 Loss: 0.2481481432914734\n",
      "Epoch 316/15000 Loss: 0.23747026920318604\n",
      "Epoch 317/15000 Loss: 0.23831605911254883\n",
      "Epoch 318/15000 Loss: 0.2261262834072113\n",
      "Epoch 319/15000 Loss: 0.23143117129802704\n",
      "Epoch 320/15000 Loss: 0.23041650652885437\n",
      "Epoch 321/15000 Loss: 0.25586622953414917\n",
      "Epoch 322/15000 Loss: 0.2259635478258133\n",
      "Epoch 323/15000 Loss: 0.22929924726486206\n",
      "Epoch 324/15000 Loss: 0.20858687162399292\n",
      "Epoch 325/15000 Loss: 0.22450506687164307\n",
      "Epoch 326/15000 Loss: 0.2347448170185089\n",
      "Epoch 327/15000 Loss: 0.21821534633636475\n",
      "Epoch 328/15000 Loss: 0.22749504446983337\n",
      "Epoch 329/15000 Loss: 0.21584053337574005\n",
      "Epoch 330/15000 Loss: 0.24527890980243683\n",
      "Epoch 331/15000 Loss: 0.2210412323474884\n",
      "Epoch 332/15000 Loss: 0.2480141520500183\n",
      "Epoch 333/15000 Loss: 0.22568468749523163\n",
      "Epoch 334/15000 Loss: 0.22462207078933716\n",
      "Epoch 335/15000 Loss: 0.21959228813648224\n",
      "Epoch 336/15000 Loss: 0.22280925512313843\n",
      "Epoch 337/15000 Loss: 0.22198301553726196\n",
      "Epoch 338/15000 Loss: 0.2100682407617569\n",
      "Epoch 339/15000 Loss: 0.21266549825668335\n",
      "Epoch 340/15000 Loss: 0.22111079096794128\n",
      "Epoch 341/15000 Loss: 0.21637457609176636\n",
      "Epoch 342/15000 Loss: 0.204502671957016\n",
      "Epoch 343/15000 Loss: 0.22229692339897156\n",
      "Epoch 344/15000 Loss: 0.21352684497833252\n",
      "Epoch 345/15000 Loss: 0.22686590254306793\n",
      "Epoch 346/15000 Loss: 0.21551549434661865\n",
      "Epoch 347/15000 Loss: 0.2067132592201233\n",
      "Epoch 348/15000 Loss: 0.20615741610527039\n",
      "Epoch 349/15000 Loss: 0.22201740741729736\n",
      "Epoch 350/15000 Loss: 0.2140219509601593\n",
      "Epoch 351/15000 Loss: 0.23113206028938293\n",
      "Epoch 352/15000 Loss: 0.19915121793746948\n",
      "Epoch 353/15000 Loss: 0.22849690914154053\n",
      "Epoch 354/15000 Loss: 0.22540633380413055\n",
      "Epoch 355/15000 Loss: 0.2021976113319397\n",
      "Epoch 356/15000 Loss: 0.2272801399230957\n",
      "Epoch 357/15000 Loss: 0.20529216527938843\n",
      "Epoch 358/15000 Loss: 0.2278110533952713\n",
      "Epoch 359/15000 Loss: 0.20873427391052246\n",
      "Epoch 360/15000 Loss: 0.20165005326271057\n",
      "Epoch 361/15000 Loss: 0.2105761021375656\n",
      "Epoch 362/15000 Loss: 0.21194657683372498\n",
      "Epoch 363/15000 Loss: 0.2268039733171463\n",
      "Epoch 364/15000 Loss: 0.1940901279449463\n",
      "Epoch 365/15000 Loss: 0.20576104521751404\n",
      "Epoch 366/15000 Loss: 0.21583989262580872\n",
      "Epoch 367/15000 Loss: 0.2168513387441635\n",
      "Epoch 368/15000 Loss: 0.2129572033882141\n",
      "Epoch 369/15000 Loss: 0.20545902848243713\n",
      "Epoch 370/15000 Loss: 0.2045062780380249\n",
      "Epoch 371/15000 Loss: 0.20375347137451172\n",
      "Epoch 372/15000 Loss: 0.19521912932395935\n",
      "Epoch 373/15000 Loss: 0.23751868307590485\n",
      "Epoch 374/15000 Loss: 0.19869157671928406\n",
      "Epoch 375/15000 Loss: 0.2127196490764618\n",
      "Epoch 376/15000 Loss: 0.20989683270454407\n",
      "Epoch 377/15000 Loss: 0.20774585008621216\n",
      "Epoch 378/15000 Loss: 0.23127400875091553\n",
      "Epoch 379/15000 Loss: 0.18681949377059937\n",
      "Epoch 380/15000 Loss: 0.18998152017593384\n",
      "Epoch 381/15000 Loss: 0.19986926019191742\n",
      "Epoch 382/15000 Loss: 0.24626374244689941\n",
      "Epoch 383/15000 Loss: 0.19965335726737976\n",
      "Epoch 384/15000 Loss: 0.195209339261055\n",
      "Epoch 385/15000 Loss: 0.20546263456344604\n",
      "Epoch 386/15000 Loss: 0.2107970267534256\n",
      "Epoch 387/15000 Loss: 0.21827900409698486\n",
      "Epoch 388/15000 Loss: 0.19804389774799347\n",
      "Epoch 389/15000 Loss: 0.2118118703365326\n",
      "Epoch 390/15000 Loss: 0.20755533874034882\n",
      "Epoch 391/15000 Loss: 0.20766016840934753\n",
      "Epoch 392/15000 Loss: 0.20393213629722595\n",
      "Epoch 393/15000 Loss: 0.2010822892189026\n",
      "Epoch 394/15000 Loss: 0.20060350000858307\n",
      "Epoch 395/15000 Loss: 0.19017747044563293\n",
      "Epoch 396/15000 Loss: 0.21015752851963043\n",
      "Epoch 397/15000 Loss: 0.181999072432518\n",
      "Epoch 398/15000 Loss: 0.18484598398208618\n",
      "Epoch 399/15000 Loss: 0.19621506333351135\n",
      "Epoch 400/15000 Loss: 0.1955297887325287\n",
      "Epoch 401/15000 Loss: 0.20525828003883362\n",
      "Epoch 402/15000 Loss: 0.19447103142738342\n",
      "Epoch 403/15000 Loss: 0.1992531269788742\n",
      "Epoch 404/15000 Loss: 0.1892232596874237\n",
      "Epoch 405/15000 Loss: 0.1937936544418335\n",
      "Epoch 406/15000 Loss: 0.19700390100479126\n",
      "Epoch 407/15000 Loss: 0.22049811482429504\n",
      "Epoch 408/15000 Loss: 0.19122424721717834\n",
      "Epoch 409/15000 Loss: 0.20044349133968353\n",
      "Epoch 410/15000 Loss: 0.18178050220012665\n",
      "Epoch 411/15000 Loss: 0.19179481267929077\n",
      "Epoch 412/15000 Loss: 0.19008678197860718\n",
      "Epoch 413/15000 Loss: 0.18584507703781128\n",
      "Epoch 414/15000 Loss: 0.19636701047420502\n",
      "Epoch 415/15000 Loss: 0.1951618194580078\n",
      "Epoch 416/15000 Loss: 0.18564215302467346\n",
      "Epoch 417/15000 Loss: 0.1893591582775116\n",
      "Epoch 418/15000 Loss: 0.21215376257896423\n",
      "Epoch 419/15000 Loss: 0.20046178996562958\n",
      "Epoch 420/15000 Loss: 0.21200451254844666\n",
      "Epoch 421/15000 Loss: 0.19509445130825043\n",
      "Epoch 422/15000 Loss: 0.18801002204418182\n",
      "Epoch 423/15000 Loss: 0.20241335034370422\n",
      "Epoch 424/15000 Loss: 0.20620763301849365\n",
      "Epoch 425/15000 Loss: 0.19267314672470093\n",
      "Epoch 426/15000 Loss: 0.20561560988426208\n",
      "Epoch 427/15000 Loss: 0.21081914007663727\n",
      "Epoch 428/15000 Loss: 0.19653944671154022\n",
      "Epoch 429/15000 Loss: 0.19658082723617554\n",
      "Epoch 430/15000 Loss: 0.19085758924484253\n",
      "Epoch 431/15000 Loss: 0.18487843871116638\n",
      "Epoch 432/15000 Loss: 0.18767668306827545\n",
      "Epoch 433/15000 Loss: 0.1965784728527069\n",
      "Epoch 434/15000 Loss: 0.18086011707782745\n",
      "Epoch 435/15000 Loss: 0.18733081221580505\n",
      "Epoch 436/15000 Loss: 0.2103462666273117\n",
      "Epoch 437/15000 Loss: 0.18608242273330688\n",
      "Epoch 438/15000 Loss: 0.1895553469657898\n",
      "Epoch 439/15000 Loss: 0.20022079348564148\n",
      "Epoch 440/15000 Loss: 0.18248988687992096\n",
      "Epoch 441/15000 Loss: 0.1778315305709839\n",
      "Epoch 442/15000 Loss: 0.19508391618728638\n",
      "Epoch 443/15000 Loss: 0.19850972294807434\n",
      "Epoch 444/15000 Loss: 0.210551917552948\n",
      "Epoch 445/15000 Loss: 0.1854647845029831\n",
      "Epoch 446/15000 Loss: 0.20853263139724731\n",
      "Epoch 447/15000 Loss: 0.1985817551612854\n",
      "Epoch 448/15000 Loss: 0.21246816217899323\n",
      "Epoch 449/15000 Loss: 0.18676646053791046\n",
      "Epoch 450/15000 Loss: 0.19487428665161133\n",
      "Epoch 451/15000 Loss: 0.20680414140224457\n",
      "Epoch 452/15000 Loss: 0.1968943029642105\n",
      "Epoch 453/15000 Loss: 0.1904558539390564\n",
      "Epoch 454/15000 Loss: 0.2092137485742569\n",
      "Epoch 455/15000 Loss: 0.18200939893722534\n",
      "Epoch 456/15000 Loss: 0.19701413810253143\n",
      "Epoch 457/15000 Loss: 0.18804901838302612\n",
      "Epoch 458/15000 Loss: 0.18314868211746216\n",
      "Epoch 459/15000 Loss: 0.17495176196098328\n",
      "Epoch 460/15000 Loss: 0.17621737718582153\n",
      "Epoch 461/15000 Loss: 0.1926986575126648\n",
      "Epoch 462/15000 Loss: 0.1715543270111084\n",
      "Epoch 463/15000 Loss: 0.18463841080665588\n",
      "Epoch 464/15000 Loss: 0.19318848848342896\n",
      "Epoch 465/15000 Loss: 0.18667100369930267\n",
      "Epoch 466/15000 Loss: 0.17948627471923828\n",
      "Epoch 467/15000 Loss: 0.1755712926387787\n",
      "Epoch 468/15000 Loss: 0.18456348776817322\n",
      "Epoch 469/15000 Loss: 0.18599802255630493\n",
      "Epoch 470/15000 Loss: 0.2040201872587204\n",
      "Epoch 471/15000 Loss: 0.2068745493888855\n",
      "Epoch 472/15000 Loss: 0.17085272073745728\n",
      "Epoch 473/15000 Loss: 0.2163756638765335\n",
      "Epoch 474/15000 Loss: 0.17925895750522614\n",
      "Epoch 475/15000 Loss: 0.17949891090393066\n",
      "Epoch 476/15000 Loss: 0.20459024608135223\n",
      "Epoch 477/15000 Loss: 0.18569999933242798\n",
      "Epoch 478/15000 Loss: 0.19956976175308228\n",
      "Epoch 479/15000 Loss: 0.17135393619537354\n",
      "Epoch 480/15000 Loss: 0.19070056080818176\n",
      "Epoch 481/15000 Loss: 0.1969834566116333\n",
      "Epoch 482/15000 Loss: 0.17797499895095825\n",
      "Epoch 483/15000 Loss: 0.20591391623020172\n",
      "Epoch 484/15000 Loss: 0.19823241233825684\n",
      "Epoch 485/15000 Loss: 0.170322448015213\n",
      "Epoch 486/15000 Loss: 0.20275244116783142\n",
      "Epoch 487/15000 Loss: 0.18078532814979553\n",
      "Epoch 488/15000 Loss: 0.1794254183769226\n",
      "Epoch 489/15000 Loss: 0.15818503499031067\n",
      "Epoch 490/15000 Loss: 0.19014543294906616\n",
      "Epoch 491/15000 Loss: 0.18453624844551086\n",
      "Epoch 492/15000 Loss: 0.18157687783241272\n",
      "Epoch 493/15000 Loss: 0.18543221056461334\n",
      "Epoch 494/15000 Loss: 0.1963256597518921\n",
      "Epoch 495/15000 Loss: 0.21235652267932892\n",
      "Epoch 496/15000 Loss: 0.1722007840871811\n",
      "Epoch 497/15000 Loss: 0.18497055768966675\n",
      "Epoch 498/15000 Loss: 0.2005952000617981\n",
      "Epoch 499/15000 Loss: 0.2210753858089447\n",
      "Epoch 500/15000 Loss: 0.19302845001220703\n",
      "Epoch 501/15000 Loss: 0.1851276457309723\n",
      "Epoch 502/15000 Loss: 0.18716123700141907\n",
      "Epoch 503/15000 Loss: 0.1699686199426651\n",
      "Epoch 504/15000 Loss: 0.18822354078292847\n",
      "Epoch 505/15000 Loss: 0.21412473917007446\n",
      "Epoch 506/15000 Loss: 0.18715496361255646\n",
      "Epoch 507/15000 Loss: 0.17906786501407623\n",
      "Epoch 508/15000 Loss: 0.19102256000041962\n",
      "Epoch 509/15000 Loss: 0.1707279086112976\n",
      "Epoch 510/15000 Loss: 0.16160202026367188\n",
      "Epoch 511/15000 Loss: 0.1654062420129776\n",
      "Epoch 512/15000 Loss: 0.18595746159553528\n",
      "Epoch 513/15000 Loss: 0.17057476937770844\n",
      "Epoch 514/15000 Loss: 0.20723924040794373\n",
      "Epoch 515/15000 Loss: 0.18834106624126434\n",
      "Epoch 516/15000 Loss: 0.1750565469264984\n",
      "Epoch 517/15000 Loss: 0.16693702340126038\n",
      "Epoch 518/15000 Loss: 0.19453072547912598\n",
      "Epoch 519/15000 Loss: 0.20973193645477295\n",
      "Epoch 520/15000 Loss: 0.21427412331104279\n",
      "Epoch 521/15000 Loss: 0.1793065369129181\n",
      "Epoch 522/15000 Loss: 0.17965838313102722\n",
      "Epoch 523/15000 Loss: 0.1789480745792389\n",
      "Epoch 524/15000 Loss: 0.19187583029270172\n",
      "Epoch 525/15000 Loss: 0.17698414623737335\n",
      "Epoch 526/15000 Loss: 0.17807665467262268\n",
      "Epoch 527/15000 Loss: 0.19148969650268555\n",
      "Epoch 528/15000 Loss: 0.1742704212665558\n",
      "Epoch 529/15000 Loss: 0.17445001006126404\n",
      "Epoch 530/15000 Loss: 0.21410903334617615\n",
      "Epoch 531/15000 Loss: 0.18993398547172546\n",
      "Epoch 532/15000 Loss: 0.17003777623176575\n",
      "Epoch 533/15000 Loss: 0.19962245225906372\n",
      "Epoch 534/15000 Loss: 0.17927372455596924\n",
      "Epoch 535/15000 Loss: 0.19358526170253754\n",
      "Epoch 536/15000 Loss: 0.18042397499084473\n",
      "Epoch 537/15000 Loss: 0.1969679594039917\n",
      "Epoch 538/15000 Loss: 0.16595898568630219\n",
      "Epoch 539/15000 Loss: 0.17580604553222656\n",
      "Epoch 540/15000 Loss: 0.18576550483703613\n",
      "Epoch 541/15000 Loss: 0.16708675026893616\n",
      "Epoch 542/15000 Loss: 0.16264168918132782\n",
      "Epoch 543/15000 Loss: 0.19365432858467102\n",
      "Epoch 544/15000 Loss: 0.17013555765151978\n",
      "Epoch 545/15000 Loss: 0.19330400228500366\n",
      "Epoch 546/15000 Loss: 0.19440951943397522\n",
      "Epoch 547/15000 Loss: 0.19222098588943481\n",
      "Epoch 548/15000 Loss: 0.17009596526622772\n",
      "Epoch 549/15000 Loss: 0.21506723761558533\n",
      "Epoch 550/15000 Loss: 0.17219018936157227\n",
      "Epoch 551/15000 Loss: 0.18367621302604675\n",
      "Epoch 552/15000 Loss: 0.17361223697662354\n",
      "Epoch 553/15000 Loss: 0.16071417927742004\n",
      "Epoch 554/15000 Loss: 0.17105615139007568\n",
      "Epoch 555/15000 Loss: 0.1624707281589508\n",
      "Epoch 556/15000 Loss: 0.17172199487686157\n",
      "Epoch 557/15000 Loss: 0.1744689643383026\n",
      "Epoch 558/15000 Loss: 0.163894385099411\n",
      "Epoch 559/15000 Loss: 0.16908922791481018\n",
      "Epoch 560/15000 Loss: 0.17858222126960754\n",
      "Epoch 561/15000 Loss: 0.18306484818458557\n",
      "Epoch 562/15000 Loss: 0.15932199358940125\n",
      "Epoch 563/15000 Loss: 0.18351075053215027\n",
      "Epoch 564/15000 Loss: 0.16648133099079132\n",
      "Epoch 565/15000 Loss: 0.18350163102149963\n",
      "Epoch 566/15000 Loss: 0.19827872514724731\n",
      "Epoch 567/15000 Loss: 0.18760251998901367\n",
      "Epoch 568/15000 Loss: 0.19088982045650482\n",
      "Epoch 569/15000 Loss: 0.1706693172454834\n",
      "Epoch 570/15000 Loss: 0.20336347818374634\n",
      "Epoch 571/15000 Loss: 0.16793745756149292\n",
      "Epoch 572/15000 Loss: 0.1730400174856186\n",
      "Epoch 573/15000 Loss: 0.19228434562683105\n",
      "Epoch 574/15000 Loss: 0.16478902101516724\n",
      "Epoch 575/15000 Loss: 0.20741304755210876\n",
      "Epoch 576/15000 Loss: 0.16471220552921295\n",
      "Epoch 577/15000 Loss: 0.17775896191596985\n",
      "Epoch 578/15000 Loss: 0.17418810725212097\n",
      "Epoch 579/15000 Loss: 0.15990012884140015\n",
      "Epoch 580/15000 Loss: 0.1583453118801117\n",
      "Epoch 581/15000 Loss: 0.16159990429878235\n",
      "Epoch 582/15000 Loss: 0.19679959118366241\n",
      "Epoch 583/15000 Loss: 0.14615778625011444\n",
      "Epoch 584/15000 Loss: 0.1615368276834488\n",
      "Epoch 585/15000 Loss: 0.17288890480995178\n",
      "Epoch 586/15000 Loss: 0.16312167048454285\n",
      "Epoch 587/15000 Loss: 0.17110663652420044\n",
      "Epoch 588/15000 Loss: 0.17524954676628113\n",
      "Epoch 589/15000 Loss: 0.1861412525177002\n",
      "Epoch 590/15000 Loss: 0.1650036871433258\n",
      "Epoch 591/15000 Loss: 0.15623325109481812\n",
      "Epoch 592/15000 Loss: 0.16218873858451843\n",
      "Epoch 593/15000 Loss: 0.16736124455928802\n",
      "Epoch 594/15000 Loss: 0.19008228182792664\n",
      "Epoch 595/15000 Loss: 0.15807414054870605\n",
      "Epoch 596/15000 Loss: 0.172807976603508\n",
      "Epoch 597/15000 Loss: 0.15699335932731628\n",
      "Epoch 598/15000 Loss: 0.16473905742168427\n",
      "Epoch 599/15000 Loss: 0.16591641306877136\n",
      "Epoch 600/15000 Loss: 0.1690106987953186\n",
      "Epoch 601/15000 Loss: 0.18763881921768188\n",
      "Epoch 602/15000 Loss: 0.1460258960723877\n",
      "Epoch 603/15000 Loss: 0.16750770807266235\n",
      "Epoch 604/15000 Loss: 0.17318260669708252\n",
      "Epoch 605/15000 Loss: 0.16787812113761902\n",
      "Epoch 606/15000 Loss: 0.16177934408187866\n",
      "Epoch 607/15000 Loss: 0.16580793261528015\n",
      "Epoch 608/15000 Loss: 0.16703347861766815\n",
      "Epoch 609/15000 Loss: 0.16822293400764465\n",
      "Epoch 610/15000 Loss: 0.17806026339530945\n",
      "Epoch 611/15000 Loss: 0.1644754409790039\n",
      "Epoch 612/15000 Loss: 0.17643262445926666\n",
      "Epoch 613/15000 Loss: 0.17147769033908844\n",
      "Epoch 614/15000 Loss: 0.16450002789497375\n",
      "Epoch 615/15000 Loss: 0.17451733350753784\n",
      "Epoch 616/15000 Loss: 0.17592787742614746\n",
      "Epoch 617/15000 Loss: 0.1622454971075058\n",
      "Epoch 618/15000 Loss: 0.15981203317642212\n",
      "Epoch 619/15000 Loss: 0.17136895656585693\n",
      "Epoch 620/15000 Loss: 0.14749130606651306\n",
      "Epoch 621/15000 Loss: 0.18119537830352783\n",
      "Epoch 622/15000 Loss: 0.18117505311965942\n",
      "Epoch 623/15000 Loss: 0.1532173901796341\n",
      "Epoch 624/15000 Loss: 0.15306909382343292\n",
      "Epoch 625/15000 Loss: 0.16767579317092896\n",
      "Epoch 626/15000 Loss: 0.1711365133523941\n",
      "Epoch 627/15000 Loss: 0.1509825587272644\n",
      "Epoch 628/15000 Loss: 0.15728355944156647\n",
      "Epoch 629/15000 Loss: 0.172387957572937\n",
      "Epoch 630/15000 Loss: 0.1657160222530365\n",
      "Epoch 631/15000 Loss: 0.15569478273391724\n",
      "Epoch 632/15000 Loss: 0.16368401050567627\n",
      "Epoch 633/15000 Loss: 0.16494815051555634\n",
      "Epoch 634/15000 Loss: 0.1661010980606079\n",
      "Epoch 635/15000 Loss: 0.1783832311630249\n",
      "Epoch 636/15000 Loss: 0.16682708263397217\n",
      "Epoch 637/15000 Loss: 0.16688594222068787\n",
      "Epoch 638/15000 Loss: 0.16491183638572693\n",
      "Epoch 639/15000 Loss: 0.16893082857131958\n",
      "Epoch 640/15000 Loss: 0.16597388684749603\n",
      "Epoch 641/15000 Loss: 0.16875559091567993\n",
      "Epoch 642/15000 Loss: 0.1517370641231537\n",
      "Epoch 643/15000 Loss: 0.16331082582473755\n",
      "Epoch 644/15000 Loss: 0.17730101943016052\n",
      "Epoch 645/15000 Loss: 0.1692209541797638\n",
      "Epoch 646/15000 Loss: 0.20400625467300415\n",
      "Epoch 647/15000 Loss: 0.16396881639957428\n",
      "Epoch 648/15000 Loss: 0.16140487790107727\n",
      "Epoch 649/15000 Loss: 0.17108727991580963\n",
      "Epoch 650/15000 Loss: 0.1716851145029068\n",
      "Epoch 651/15000 Loss: 0.1578240692615509\n",
      "Epoch 652/15000 Loss: 0.16731861233711243\n",
      "Epoch 653/15000 Loss: 0.16842874884605408\n",
      "Epoch 654/15000 Loss: 0.14347688853740692\n",
      "Epoch 655/15000 Loss: 0.17143034934997559\n",
      "Epoch 656/15000 Loss: 0.1676211655139923\n",
      "Epoch 657/15000 Loss: 0.1863921582698822\n",
      "Epoch 658/15000 Loss: 0.16205359995365143\n",
      "Epoch 659/15000 Loss: 0.17465201020240784\n",
      "Epoch 660/15000 Loss: 0.16380682587623596\n",
      "Epoch 661/15000 Loss: 0.16163012385368347\n",
      "Epoch 662/15000 Loss: 0.1659565269947052\n",
      "Epoch 663/15000 Loss: 0.1717301458120346\n",
      "Epoch 664/15000 Loss: 0.15520545840263367\n",
      "Epoch 665/15000 Loss: 0.15581530332565308\n",
      "Epoch 666/15000 Loss: 0.1463291496038437\n",
      "Epoch 667/15000 Loss: 0.18339668214321136\n",
      "Epoch 668/15000 Loss: 0.1694771647453308\n",
      "Epoch 669/15000 Loss: 0.14913779497146606\n",
      "Epoch 670/15000 Loss: 0.17165859043598175\n",
      "Epoch 671/15000 Loss: 0.1642349362373352\n",
      "Epoch 672/15000 Loss: 0.1448032557964325\n",
      "Epoch 673/15000 Loss: 0.1553155779838562\n",
      "Epoch 674/15000 Loss: 0.1627560704946518\n",
      "Epoch 675/15000 Loss: 0.16886840760707855\n",
      "Epoch 676/15000 Loss: 0.1492820382118225\n",
      "Epoch 677/15000 Loss: 0.178017258644104\n",
      "Epoch 678/15000 Loss: 0.16769927740097046\n",
      "Epoch 679/15000 Loss: 0.16729958355426788\n",
      "Epoch 680/15000 Loss: 0.1720704585313797\n",
      "Epoch 681/15000 Loss: 0.15101434290409088\n",
      "Epoch 682/15000 Loss: 0.17094193398952484\n",
      "Epoch 683/15000 Loss: 0.16468606889247894\n",
      "Epoch 684/15000 Loss: 0.14491558074951172\n",
      "Epoch 685/15000 Loss: 0.15969648957252502\n",
      "Epoch 686/15000 Loss: 0.1588684767484665\n",
      "Epoch 687/15000 Loss: 0.15347439050674438\n",
      "Epoch 688/15000 Loss: 0.15394112467765808\n",
      "Epoch 689/15000 Loss: 0.18184496462345123\n",
      "Epoch 690/15000 Loss: 0.16400247812271118\n",
      "Epoch 691/15000 Loss: 0.1503504514694214\n",
      "Epoch 692/15000 Loss: 0.17312821745872498\n",
      "Epoch 693/15000 Loss: 0.1559869647026062\n",
      "Epoch 694/15000 Loss: 0.16503503918647766\n",
      "Epoch 695/15000 Loss: 0.1614886224269867\n",
      "Epoch 696/15000 Loss: 0.16524717211723328\n",
      "Epoch 697/15000 Loss: 0.16119734942913055\n",
      "Epoch 698/15000 Loss: 0.15977013111114502\n",
      "Epoch 699/15000 Loss: 0.14339058101177216\n",
      "Epoch 700/15000 Loss: 0.1668640673160553\n",
      "Epoch 701/15000 Loss: 0.1646282970905304\n",
      "Epoch 702/15000 Loss: 0.15863369405269623\n",
      "Epoch 703/15000 Loss: 0.1719028651714325\n",
      "Epoch 704/15000 Loss: 0.18434084951877594\n",
      "Epoch 705/15000 Loss: 0.17650106549263\n",
      "Epoch 706/15000 Loss: 0.16225643455982208\n",
      "Epoch 707/15000 Loss: 0.15147951245307922\n",
      "Epoch 708/15000 Loss: 0.15811553597450256\n",
      "Epoch 709/15000 Loss: 0.15951190888881683\n",
      "Epoch 710/15000 Loss: 0.18139438331127167\n",
      "Epoch 711/15000 Loss: 0.16016244888305664\n",
      "Epoch 712/15000 Loss: 0.16005012392997742\n",
      "Epoch 713/15000 Loss: 0.18551969528198242\n",
      "Epoch 714/15000 Loss: 0.16026057302951813\n",
      "Epoch 715/15000 Loss: 0.1684010624885559\n",
      "Epoch 716/15000 Loss: 0.15497402846813202\n",
      "Epoch 717/15000 Loss: 0.15054622292518616\n",
      "Epoch 718/15000 Loss: 0.16607651114463806\n",
      "Epoch 719/15000 Loss: 0.1600596010684967\n",
      "Epoch 720/15000 Loss: 0.1637582778930664\n",
      "Epoch 721/15000 Loss: 0.13587327301502228\n",
      "Epoch 722/15000 Loss: 0.15079408884048462\n",
      "Epoch 723/15000 Loss: 0.14601808786392212\n",
      "Epoch 724/15000 Loss: 0.1535097360610962\n",
      "Epoch 725/15000 Loss: 0.1495184302330017\n",
      "Epoch 726/15000 Loss: 0.16011595726013184\n",
      "Epoch 727/15000 Loss: 0.17021378874778748\n",
      "Epoch 728/15000 Loss: 0.1597939431667328\n",
      "Epoch 729/15000 Loss: 0.1619114875793457\n",
      "Epoch 730/15000 Loss: 0.14406388998031616\n",
      "Epoch 731/15000 Loss: 0.14570416510105133\n",
      "Epoch 732/15000 Loss: 0.14532044529914856\n",
      "Epoch 733/15000 Loss: 0.16546516120433807\n",
      "Epoch 734/15000 Loss: 0.15916748344898224\n",
      "Epoch 735/15000 Loss: 0.17513510584831238\n",
      "Epoch 736/15000 Loss: 0.17014822363853455\n",
      "Epoch 737/15000 Loss: 0.17689427733421326\n",
      "Epoch 738/15000 Loss: 0.14349840581417084\n",
      "Epoch 739/15000 Loss: 0.1562894582748413\n",
      "Epoch 740/15000 Loss: 0.18950700759887695\n",
      "Epoch 741/15000 Loss: 0.15382203459739685\n",
      "Epoch 742/15000 Loss: 0.1586509346961975\n",
      "Epoch 743/15000 Loss: 0.17416317760944366\n",
      "Epoch 744/15000 Loss: 0.16285671293735504\n",
      "Epoch 745/15000 Loss: 0.15881073474884033\n",
      "Epoch 746/15000 Loss: 0.14626726508140564\n",
      "Epoch 747/15000 Loss: 0.18118271231651306\n",
      "Epoch 748/15000 Loss: 0.16054564714431763\n",
      "Epoch 749/15000 Loss: 0.17820775508880615\n",
      "Epoch 750/15000 Loss: 0.16822440922260284\n",
      "Epoch 751/15000 Loss: 0.14984267950057983\n",
      "Epoch 752/15000 Loss: 0.14082014560699463\n",
      "Epoch 753/15000 Loss: 0.15926775336265564\n",
      "Epoch 754/15000 Loss: 0.14859797060489655\n",
      "Epoch 755/15000 Loss: 0.15859955549240112\n",
      "Epoch 756/15000 Loss: 0.16223415732383728\n",
      "Epoch 757/15000 Loss: 0.1322712004184723\n",
      "Epoch 758/15000 Loss: 0.15976686775684357\n",
      "Epoch 759/15000 Loss: 0.16242119669914246\n",
      "Epoch 760/15000 Loss: 0.14377549290657043\n",
      "Epoch 761/15000 Loss: 0.14512592554092407\n",
      "Epoch 762/15000 Loss: 0.15865488350391388\n",
      "Epoch 763/15000 Loss: 0.17223383486270905\n",
      "Epoch 764/15000 Loss: 0.13562485575675964\n",
      "Epoch 765/15000 Loss: 0.14872188866138458\n",
      "Epoch 766/15000 Loss: 0.14979977905750275\n",
      "Epoch 767/15000 Loss: 0.1457033008337021\n",
      "Epoch 768/15000 Loss: 0.1635226309299469\n",
      "Epoch 769/15000 Loss: 0.1597115695476532\n",
      "Epoch 770/15000 Loss: 0.13705098628997803\n",
      "Epoch 771/15000 Loss: 0.149158775806427\n",
      "Epoch 772/15000 Loss: 0.1552763283252716\n",
      "Epoch 773/15000 Loss: 0.15565179288387299\n",
      "Epoch 774/15000 Loss: 0.1560717225074768\n",
      "Epoch 775/15000 Loss: 0.14891141653060913\n",
      "Epoch 776/15000 Loss: 0.1512221097946167\n",
      "Epoch 777/15000 Loss: 0.15301375091075897\n",
      "Epoch 778/15000 Loss: 0.14520315825939178\n",
      "Epoch 779/15000 Loss: 0.1643884927034378\n",
      "Epoch 780/15000 Loss: 0.16280131042003632\n",
      "Epoch 781/15000 Loss: 0.15833362936973572\n",
      "Epoch 782/15000 Loss: 0.1500510573387146\n",
      "Epoch 783/15000 Loss: 0.13836833834648132\n",
      "Epoch 784/15000 Loss: 0.13194499909877777\n",
      "Epoch 785/15000 Loss: 0.14550817012786865\n",
      "Epoch 786/15000 Loss: 0.1632479727268219\n",
      "Epoch 787/15000 Loss: 0.1653842329978943\n",
      "Epoch 788/15000 Loss: 0.159184992313385\n",
      "Epoch 789/15000 Loss: 0.16076481342315674\n",
      "Epoch 790/15000 Loss: 0.1465211808681488\n",
      "Epoch 791/15000 Loss: 0.1561107039451599\n",
      "Epoch 792/15000 Loss: 0.13188046216964722\n",
      "Epoch 793/15000 Loss: 0.13776680827140808\n",
      "Epoch 794/15000 Loss: 0.14579981565475464\n",
      "Epoch 795/15000 Loss: 0.1502106785774231\n",
      "Epoch 796/15000 Loss: 0.14687323570251465\n",
      "Epoch 797/15000 Loss: 0.14126408100128174\n",
      "Epoch 798/15000 Loss: 0.15951812267303467\n",
      "Epoch 799/15000 Loss: 0.15370184183120728\n",
      "Epoch 800/15000 Loss: 0.15541082620620728\n",
      "Epoch 801/15000 Loss: 0.14396053552627563\n",
      "Epoch 802/15000 Loss: 0.13481122255325317\n",
      "Epoch 803/15000 Loss: 0.15195724368095398\n",
      "Epoch 804/15000 Loss: 0.15990102291107178\n",
      "Epoch 805/15000 Loss: 0.16421058773994446\n",
      "Epoch 806/15000 Loss: 0.14727529883384705\n",
      "Epoch 807/15000 Loss: 0.15220198035240173\n",
      "Epoch 808/15000 Loss: 0.14217108488082886\n",
      "Epoch 809/15000 Loss: 0.1522064357995987\n",
      "Epoch 810/15000 Loss: 0.1704523116350174\n",
      "Epoch 811/15000 Loss: 0.16356518864631653\n",
      "Epoch 812/15000 Loss: 0.15388497710227966\n",
      "Epoch 813/15000 Loss: 0.13866621255874634\n",
      "Epoch 814/15000 Loss: 0.15510818362236023\n",
      "Epoch 815/15000 Loss: 0.1634538769721985\n",
      "Epoch 816/15000 Loss: 0.17084600031375885\n",
      "Epoch 817/15000 Loss: 0.1416717767715454\n",
      "Epoch 818/15000 Loss: 0.15102888643741608\n",
      "Epoch 819/15000 Loss: 0.1656334400177002\n",
      "Epoch 820/15000 Loss: 0.1668822169303894\n",
      "Epoch 821/15000 Loss: 0.16727915406227112\n",
      "Epoch 822/15000 Loss: 0.1650029420852661\n",
      "Epoch 823/15000 Loss: 0.15757043659687042\n",
      "Epoch 824/15000 Loss: 0.1568748652935028\n",
      "Epoch 825/15000 Loss: 0.15727883577346802\n",
      "Epoch 826/15000 Loss: 0.13310512900352478\n",
      "Epoch 827/15000 Loss: 0.15594321489334106\n",
      "Epoch 828/15000 Loss: 0.15335644781589508\n",
      "Epoch 829/15000 Loss: 0.14334888756275177\n",
      "Epoch 830/15000 Loss: 0.14080491662025452\n",
      "Epoch 831/15000 Loss: 0.13854879140853882\n",
      "Epoch 832/15000 Loss: 0.16267427802085876\n",
      "Epoch 833/15000 Loss: 0.15626946091651917\n",
      "Epoch 834/15000 Loss: 0.15196296572685242\n",
      "Epoch 835/15000 Loss: 0.15037474036216736\n",
      "Epoch 836/15000 Loss: 0.16289955377578735\n",
      "Epoch 837/15000 Loss: 0.16045016050338745\n",
      "Epoch 838/15000 Loss: 0.13059647381305695\n",
      "Epoch 839/15000 Loss: 0.13966012001037598\n",
      "Epoch 840/15000 Loss: 0.16394229233264923\n",
      "Epoch 841/15000 Loss: 0.14380477368831635\n",
      "Epoch 842/15000 Loss: 0.14039213955402374\n",
      "Epoch 843/15000 Loss: 0.14382845163345337\n",
      "Epoch 844/15000 Loss: 0.12960609793663025\n",
      "Epoch 845/15000 Loss: 0.14235639572143555\n",
      "Epoch 846/15000 Loss: 0.16113480925559998\n",
      "Epoch 847/15000 Loss: 0.14091923832893372\n",
      "Epoch 848/15000 Loss: 0.15927374362945557\n",
      "Epoch 849/15000 Loss: 0.1430051028728485\n",
      "Epoch 850/15000 Loss: 0.15197253227233887\n",
      "Epoch 851/15000 Loss: 0.14226800203323364\n",
      "Epoch 852/15000 Loss: 0.1368955820798874\n",
      "Epoch 853/15000 Loss: 0.14217513799667358\n",
      "Epoch 854/15000 Loss: 0.1687197983264923\n",
      "Epoch 855/15000 Loss: 0.13743162155151367\n",
      "Epoch 856/15000 Loss: 0.15860310196876526\n",
      "Epoch 857/15000 Loss: 0.13350975513458252\n",
      "Epoch 858/15000 Loss: 0.1501176953315735\n",
      "Epoch 859/15000 Loss: 0.1385081708431244\n",
      "Epoch 860/15000 Loss: 0.14993682503700256\n",
      "Epoch 861/15000 Loss: 0.164110004901886\n",
      "Epoch 862/15000 Loss: 0.1542883813381195\n",
      "Epoch 863/15000 Loss: 0.14838159084320068\n",
      "Epoch 864/15000 Loss: 0.15311069786548615\n",
      "Epoch 865/15000 Loss: 0.15919986367225647\n",
      "Epoch 866/15000 Loss: 0.12996730208396912\n",
      "Epoch 867/15000 Loss: 0.15268674492835999\n",
      "Epoch 868/15000 Loss: 0.14825809001922607\n",
      "Epoch 869/15000 Loss: 0.15968549251556396\n",
      "Epoch 870/15000 Loss: 0.13395974040031433\n",
      "Epoch 871/15000 Loss: 0.14961464703083038\n",
      "Epoch 872/15000 Loss: 0.13447096943855286\n",
      "Epoch 873/15000 Loss: 0.15295135974884033\n",
      "Epoch 874/15000 Loss: 0.14458274841308594\n",
      "Epoch 875/15000 Loss: 0.15503036975860596\n",
      "Epoch 876/15000 Loss: 0.16039681434631348\n",
      "Epoch 877/15000 Loss: 0.13639917969703674\n",
      "Epoch 878/15000 Loss: 0.16063977777957916\n",
      "Epoch 879/15000 Loss: 0.15306781232357025\n",
      "Epoch 880/15000 Loss: 0.16356506943702698\n",
      "Epoch 881/15000 Loss: 0.16840071976184845\n",
      "Epoch 882/15000 Loss: 0.14757096767425537\n",
      "Epoch 883/15000 Loss: 0.1576240211725235\n",
      "Epoch 884/15000 Loss: 0.18007761240005493\n",
      "Epoch 885/15000 Loss: 0.14327658712863922\n",
      "Epoch 886/15000 Loss: 0.1610853672027588\n",
      "Epoch 887/15000 Loss: 0.13823553919792175\n",
      "Epoch 888/15000 Loss: 0.13938328623771667\n",
      "Epoch 889/15000 Loss: 0.14908663928508759\n",
      "Epoch 890/15000 Loss: 0.127764493227005\n",
      "Epoch 891/15000 Loss: 0.1576939821243286\n",
      "Epoch 892/15000 Loss: 0.1614493876695633\n",
      "Epoch 893/15000 Loss: 0.14304257929325104\n",
      "Epoch 894/15000 Loss: 0.15850241482257843\n",
      "Epoch 895/15000 Loss: 0.13219118118286133\n",
      "Epoch 896/15000 Loss: 0.14413192868232727\n",
      "Epoch 897/15000 Loss: 0.1359221190214157\n",
      "Epoch 898/15000 Loss: 0.12875354290008545\n",
      "Epoch 899/15000 Loss: 0.14619861543178558\n",
      "Epoch 900/15000 Loss: 0.1329984962940216\n",
      "Epoch 901/15000 Loss: 0.14711569249629974\n",
      "Epoch 902/15000 Loss: 0.14007563889026642\n",
      "Epoch 903/15000 Loss: 0.13222190737724304\n",
      "Epoch 904/15000 Loss: 0.14939987659454346\n",
      "Epoch 905/15000 Loss: 0.1531217098236084\n",
      "Epoch 906/15000 Loss: 0.14081260561943054\n",
      "Epoch 907/15000 Loss: 0.13153143227100372\n",
      "Epoch 908/15000 Loss: 0.1378324031829834\n",
      "Epoch 909/15000 Loss: 0.14968694746494293\n",
      "Epoch 910/15000 Loss: 0.13434459269046783\n",
      "Epoch 911/15000 Loss: 0.1590460240840912\n",
      "Epoch 912/15000 Loss: 0.17094653844833374\n",
      "Epoch 913/15000 Loss: 0.1333158314228058\n",
      "Epoch 914/15000 Loss: 0.1690773218870163\n",
      "Epoch 915/15000 Loss: 0.14846211671829224\n",
      "Epoch 916/15000 Loss: 0.16236358880996704\n",
      "Epoch 917/15000 Loss: 0.1413813829421997\n",
      "Epoch 918/15000 Loss: 0.1510792076587677\n",
      "Epoch 919/15000 Loss: 0.1352827250957489\n",
      "Epoch 920/15000 Loss: 0.16824908554553986\n",
      "Epoch 921/15000 Loss: 0.13979282975196838\n",
      "Epoch 922/15000 Loss: 0.14869648218154907\n",
      "Epoch 923/15000 Loss: 0.13883763551712036\n",
      "Epoch 924/15000 Loss: 0.13051971793174744\n",
      "Epoch 925/15000 Loss: 0.12918806076049805\n",
      "Epoch 926/15000 Loss: 0.14647746086120605\n",
      "Epoch 927/15000 Loss: 0.1471605896949768\n",
      "Epoch 928/15000 Loss: 0.1455172896385193\n",
      "Epoch 929/15000 Loss: 0.13928869366645813\n",
      "Epoch 930/15000 Loss: 0.15787756443023682\n",
      "Epoch 931/15000 Loss: 0.14225515723228455\n",
      "Epoch 932/15000 Loss: 0.13600409030914307\n",
      "Epoch 933/15000 Loss: 0.1425594538450241\n",
      "Epoch 934/15000 Loss: 0.12560220062732697\n",
      "Epoch 935/15000 Loss: 0.1588878482580185\n",
      "Epoch 936/15000 Loss: 0.1339031159877777\n",
      "Epoch 937/15000 Loss: 0.12213645130395889\n",
      "Epoch 938/15000 Loss: 0.1475723683834076\n",
      "Epoch 939/15000 Loss: 0.15574286878108978\n",
      "Epoch 940/15000 Loss: 0.1540524959564209\n",
      "Epoch 941/15000 Loss: 0.1326369047164917\n",
      "Epoch 942/15000 Loss: 0.1514313966035843\n",
      "Epoch 943/15000 Loss: 0.15613481402397156\n",
      "Epoch 944/15000 Loss: 0.14304673671722412\n",
      "Epoch 945/15000 Loss: 0.14888252317905426\n",
      "Epoch 946/15000 Loss: 0.1616983562707901\n",
      "Epoch 947/15000 Loss: 0.15296319127082825\n",
      "Epoch 948/15000 Loss: 0.1508631706237793\n",
      "Epoch 949/15000 Loss: 0.14343294501304626\n",
      "Epoch 950/15000 Loss: 0.13979890942573547\n",
      "Epoch 951/15000 Loss: 0.1704755425453186\n",
      "Epoch 952/15000 Loss: 0.13704852759838104\n",
      "Epoch 953/15000 Loss: 0.1402536928653717\n",
      "Epoch 954/15000 Loss: 0.15462084114551544\n",
      "Epoch 955/15000 Loss: 0.14517532289028168\n",
      "Epoch 956/15000 Loss: 0.15132060647010803\n",
      "Epoch 957/15000 Loss: 0.16548559069633484\n",
      "Epoch 958/15000 Loss: 0.15494783222675323\n",
      "Epoch 959/15000 Loss: 0.1373499035835266\n",
      "Epoch 960/15000 Loss: 0.15694496035575867\n",
      "Epoch 961/15000 Loss: 0.1578221321105957\n",
      "Epoch 962/15000 Loss: 0.15055227279663086\n",
      "Epoch 963/15000 Loss: 0.13012483716011047\n",
      "Epoch 964/15000 Loss: 0.1358216106891632\n",
      "Epoch 965/15000 Loss: 0.1579720675945282\n",
      "Epoch 966/15000 Loss: 0.13443854451179504\n",
      "Epoch 967/15000 Loss: 0.14549729228019714\n",
      "Epoch 968/15000 Loss: 0.13816633820533752\n",
      "Epoch 969/15000 Loss: 0.13918527960777283\n",
      "Epoch 970/15000 Loss: 0.14054206013679504\n",
      "Epoch 971/15000 Loss: 0.12790507078170776\n",
      "Epoch 972/15000 Loss: 0.133045956492424\n",
      "Epoch 973/15000 Loss: 0.14132842421531677\n",
      "Epoch 974/15000 Loss: 0.14963728189468384\n",
      "Epoch 975/15000 Loss: 0.14242768287658691\n",
      "Epoch 976/15000 Loss: 0.13564522564411163\n",
      "Epoch 977/15000 Loss: 0.13977327942848206\n",
      "Epoch 978/15000 Loss: 0.12777338922023773\n",
      "Epoch 979/15000 Loss: 0.12873058021068573\n",
      "Epoch 980/15000 Loss: 0.1565934121608734\n",
      "Epoch 981/15000 Loss: 0.14017228782176971\n",
      "Epoch 982/15000 Loss: 0.1341424584388733\n",
      "Epoch 983/15000 Loss: 0.14101070165634155\n",
      "Epoch 984/15000 Loss: 0.16279155015945435\n",
      "Epoch 985/15000 Loss: 0.14778009057044983\n",
      "Epoch 986/15000 Loss: 0.14916661381721497\n",
      "Epoch 987/15000 Loss: 0.13465353846549988\n",
      "Epoch 988/15000 Loss: 0.14665448665618896\n",
      "Epoch 989/15000 Loss: 0.15774279832839966\n",
      "Epoch 990/15000 Loss: 0.14710351824760437\n",
      "Epoch 991/15000 Loss: 0.15887625515460968\n",
      "Epoch 992/15000 Loss: 0.12784594297409058\n",
      "Epoch 993/15000 Loss: 0.1295347809791565\n",
      "Epoch 994/15000 Loss: 0.14457663893699646\n",
      "Epoch 995/15000 Loss: 0.13699716329574585\n",
      "Epoch 996/15000 Loss: 0.1345025897026062\n",
      "Epoch 997/15000 Loss: 0.1415926069021225\n",
      "Epoch 998/15000 Loss: 0.1443524956703186\n",
      "Epoch 999/15000 Loss: 0.13921743631362915\n",
      "Epoch 1000/15000 Loss: 0.15098963677883148\n",
      "Epoch 1001/15000 Loss: 0.15492597222328186\n",
      "Epoch 1002/15000 Loss: 0.14643096923828125\n",
      "Epoch 1003/15000 Loss: 0.13802562654018402\n",
      "Epoch 1004/15000 Loss: 0.14994066953659058\n",
      "Epoch 1005/15000 Loss: 0.13236235082149506\n",
      "Epoch 1006/15000 Loss: 0.13511693477630615\n",
      "Epoch 1007/15000 Loss: 0.15829719603061676\n",
      "Epoch 1008/15000 Loss: 0.13305328786373138\n",
      "Epoch 1009/15000 Loss: 0.12239083647727966\n",
      "Epoch 1010/15000 Loss: 0.15235379338264465\n",
      "Epoch 1011/15000 Loss: 0.13150256872177124\n",
      "Epoch 1012/15000 Loss: 0.13881699740886688\n",
      "Epoch 1013/15000 Loss: 0.1310948133468628\n",
      "Epoch 1014/15000 Loss: 0.11923451721668243\n",
      "Epoch 1015/15000 Loss: 0.13841918110847473\n",
      "Epoch 1016/15000 Loss: 0.15110549330711365\n",
      "Epoch 1017/15000 Loss: 0.13777562975883484\n",
      "Epoch 1018/15000 Loss: 0.14423704147338867\n",
      "Epoch 1019/15000 Loss: 0.14689995348453522\n",
      "Epoch 1020/15000 Loss: 0.13997656106948853\n",
      "Epoch 1021/15000 Loss: 0.15133047103881836\n",
      "Epoch 1022/15000 Loss: 0.14630766212940216\n",
      "Epoch 1023/15000 Loss: 0.12350745499134064\n",
      "Epoch 1024/15000 Loss: 0.147090345621109\n",
      "Epoch 1025/15000 Loss: 0.14182738959789276\n",
      "Epoch 1026/15000 Loss: 0.13815827667713165\n",
      "Epoch 1027/15000 Loss: 0.12221865355968475\n",
      "Epoch 1028/15000 Loss: 0.14745983481407166\n",
      "Epoch 1029/15000 Loss: 0.12272270023822784\n",
      "Epoch 1030/15000 Loss: 0.1356985867023468\n",
      "Epoch 1031/15000 Loss: 0.1431436389684677\n",
      "Epoch 1032/15000 Loss: 0.1569070965051651\n",
      "Epoch 1033/15000 Loss: 0.1424349546432495\n",
      "Epoch 1034/15000 Loss: 0.15040600299835205\n",
      "Epoch 1035/15000 Loss: 0.14241528511047363\n",
      "Epoch 1036/15000 Loss: 0.12532460689544678\n",
      "Epoch 1037/15000 Loss: 0.14443938434123993\n",
      "Epoch 1038/15000 Loss: 0.13871833682060242\n",
      "Epoch 1039/15000 Loss: 0.12618675827980042\n",
      "Epoch 1040/15000 Loss: 0.14122839272022247\n",
      "Epoch 1041/15000 Loss: 0.12519429624080658\n",
      "Epoch 1042/15000 Loss: 0.12971441447734833\n",
      "Epoch 1043/15000 Loss: 0.13375501334667206\n",
      "Epoch 1044/15000 Loss: 0.1507435292005539\n",
      "Epoch 1045/15000 Loss: 0.14096464216709137\n",
      "Epoch 1046/15000 Loss: 0.1378328651189804\n",
      "Epoch 1047/15000 Loss: 0.13628068566322327\n",
      "Epoch 1048/15000 Loss: 0.14139488339424133\n",
      "Epoch 1049/15000 Loss: 0.11085335910320282\n",
      "Epoch 1050/15000 Loss: 0.14334329962730408\n",
      "Epoch 1051/15000 Loss: 0.11481688171625137\n",
      "Epoch 1052/15000 Loss: 0.15215852856636047\n",
      "Epoch 1053/15000 Loss: 0.12026932090520859\n",
      "Epoch 1054/15000 Loss: 0.1310010850429535\n",
      "Epoch 1055/15000 Loss: 0.12062744796276093\n",
      "Epoch 1056/15000 Loss: 0.1516728550195694\n",
      "Epoch 1057/15000 Loss: 0.13638323545455933\n",
      "Epoch 1058/15000 Loss: 0.14797639846801758\n",
      "Epoch 1059/15000 Loss: 0.1252574771642685\n",
      "Epoch 1060/15000 Loss: 0.13356883823871613\n",
      "Epoch 1061/15000 Loss: 0.15436111390590668\n",
      "Epoch 1062/15000 Loss: 0.15171021223068237\n",
      "Epoch 1063/15000 Loss: 0.11832835525274277\n",
      "Epoch 1064/15000 Loss: 0.12451466917991638\n",
      "Epoch 1065/15000 Loss: 0.1308293342590332\n",
      "Epoch 1066/15000 Loss: 0.13045571744441986\n",
      "Epoch 1067/15000 Loss: 0.15900270640850067\n",
      "Epoch 1068/15000 Loss: 0.15087443590164185\n",
      "Epoch 1069/15000 Loss: 0.1479267179965973\n",
      "Epoch 1070/15000 Loss: 0.12276788055896759\n",
      "Epoch 1071/15000 Loss: 0.12489829957485199\n",
      "Epoch 1072/15000 Loss: 0.1422813981771469\n",
      "Epoch 1073/15000 Loss: 0.12861527502536774\n",
      "Epoch 1074/15000 Loss: 0.14376136660575867\n",
      "Epoch 1075/15000 Loss: 0.13407209515571594\n",
      "Epoch 1076/15000 Loss: 0.13434714078903198\n",
      "Epoch 1077/15000 Loss: 0.12015882134437561\n",
      "Epoch 1078/15000 Loss: 0.1647721230983734\n",
      "Epoch 1079/15000 Loss: 0.1161167174577713\n",
      "Epoch 1080/15000 Loss: 0.13175907731056213\n",
      "Epoch 1081/15000 Loss: 0.13239338994026184\n",
      "Epoch 1082/15000 Loss: 0.13501939177513123\n",
      "Epoch 1083/15000 Loss: 0.1355292946100235\n",
      "Epoch 1084/15000 Loss: 0.13522464036941528\n",
      "Epoch 1085/15000 Loss: 0.14607590436935425\n",
      "Epoch 1086/15000 Loss: 0.12060055881738663\n",
      "Epoch 1087/15000 Loss: 0.1396232396364212\n",
      "Epoch 1088/15000 Loss: 0.12047523260116577\n",
      "Epoch 1089/15000 Loss: 0.13230755925178528\n",
      "Epoch 1090/15000 Loss: 0.13184210658073425\n",
      "Epoch 1091/15000 Loss: 0.13784785568714142\n",
      "Epoch 1092/15000 Loss: 0.13277766108512878\n",
      "Epoch 1093/15000 Loss: 0.16085241734981537\n",
      "Epoch 1094/15000 Loss: 0.12107988446950912\n",
      "Epoch 1095/15000 Loss: 0.17826560139656067\n",
      "Epoch 1096/15000 Loss: 0.12965156137943268\n",
      "Epoch 1097/15000 Loss: 0.1235252171754837\n",
      "Epoch 1098/15000 Loss: 0.1250278353691101\n",
      "Epoch 1099/15000 Loss: 0.11680178344249725\n",
      "Epoch 1100/15000 Loss: 0.13200782239437103\n",
      "Epoch 1101/15000 Loss: 0.143815279006958\n",
      "Epoch 1102/15000 Loss: 0.14980490505695343\n",
      "Epoch 1103/15000 Loss: 0.13917997479438782\n",
      "Epoch 1104/15000 Loss: 0.1625998318195343\n",
      "Epoch 1105/15000 Loss: 0.12718844413757324\n",
      "Epoch 1106/15000 Loss: 0.12035225331783295\n",
      "Epoch 1107/15000 Loss: 0.12687769532203674\n",
      "Epoch 1108/15000 Loss: 0.12689024209976196\n",
      "Epoch 1109/15000 Loss: 0.1349029242992401\n",
      "Epoch 1110/15000 Loss: 0.1499389410018921\n",
      "Epoch 1111/15000 Loss: 0.14735881984233856\n",
      "Epoch 1112/15000 Loss: 0.1474563330411911\n",
      "Epoch 1113/15000 Loss: 0.12310715019702911\n",
      "Epoch 1114/15000 Loss: 0.13611921668052673\n",
      "Epoch 1115/15000 Loss: 0.14087700843811035\n",
      "Epoch 1116/15000 Loss: 0.14681512117385864\n",
      "Epoch 1117/15000 Loss: 0.13404399156570435\n",
      "Epoch 1118/15000 Loss: 0.13866806030273438\n",
      "Epoch 1119/15000 Loss: 0.11457546055316925\n",
      "Epoch 1120/15000 Loss: 0.12211869657039642\n",
      "Epoch 1121/15000 Loss: 0.1554083228111267\n",
      "Epoch 1122/15000 Loss: 0.14670631289482117\n",
      "Epoch 1123/15000 Loss: 0.14241981506347656\n",
      "Epoch 1124/15000 Loss: 0.13735002279281616\n",
      "Epoch 1125/15000 Loss: 0.15532457828521729\n",
      "Epoch 1126/15000 Loss: 0.15310807526111603\n",
      "Epoch 1127/15000 Loss: 0.15269958972930908\n",
      "Epoch 1128/15000 Loss: 0.1363483965396881\n",
      "Epoch 1129/15000 Loss: 0.1298096776008606\n",
      "Epoch 1130/15000 Loss: 0.15481621026992798\n",
      "Epoch 1131/15000 Loss: 0.14304134249687195\n",
      "Epoch 1132/15000 Loss: 0.13554589450359344\n",
      "Epoch 1133/15000 Loss: 0.12377415597438812\n",
      "Epoch 1134/15000 Loss: 0.1276489645242691\n",
      "Epoch 1135/15000 Loss: 0.1318366527557373\n",
      "Epoch 1136/15000 Loss: 0.16225963830947876\n",
      "Epoch 1137/15000 Loss: 0.1453188955783844\n",
      "Epoch 1138/15000 Loss: 0.1327887624502182\n",
      "Epoch 1139/15000 Loss: 0.1566881686449051\n",
      "Epoch 1140/15000 Loss: 0.1278245598077774\n",
      "Epoch 1141/15000 Loss: 0.1393318474292755\n",
      "Epoch 1142/15000 Loss: 0.13448400795459747\n",
      "Epoch 1143/15000 Loss: 0.15657639503479004\n",
      "Epoch 1144/15000 Loss: 0.15693281590938568\n",
      "Epoch 1145/15000 Loss: 0.11307471990585327\n",
      "Epoch 1146/15000 Loss: 0.12337157130241394\n",
      "Epoch 1147/15000 Loss: 0.13257135450839996\n",
      "Epoch 1148/15000 Loss: 0.13235309720039368\n",
      "Epoch 1149/15000 Loss: 0.14103007316589355\n",
      "Epoch 1150/15000 Loss: 0.12714090943336487\n",
      "Epoch 1151/15000 Loss: 0.1340932548046112\n",
      "Epoch 1152/15000 Loss: 0.13683238625526428\n",
      "Epoch 1153/15000 Loss: 0.16613149642944336\n",
      "Epoch 1154/15000 Loss: 0.11498193442821503\n",
      "Epoch 1155/15000 Loss: 0.13693663477897644\n",
      "Epoch 1156/15000 Loss: 0.1188049465417862\n",
      "Epoch 1157/15000 Loss: 0.13523921370506287\n",
      "Epoch 1158/15000 Loss: 0.12674196064472198\n",
      "Epoch 1159/15000 Loss: 0.12130145728588104\n",
      "Epoch 1160/15000 Loss: 0.14229077100753784\n",
      "Epoch 1161/15000 Loss: 0.14865483343601227\n",
      "Epoch 1162/15000 Loss: 0.13062241673469543\n",
      "Epoch 1163/15000 Loss: 0.1554216742515564\n",
      "Epoch 1164/15000 Loss: 0.13264170289039612\n",
      "Epoch 1165/15000 Loss: 0.12333120405673981\n",
      "Epoch 1166/15000 Loss: 0.13139155507087708\n",
      "Epoch 1167/15000 Loss: 0.12061008810997009\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "#         eval_data=data_valid,\n",
    "        eval_interval=eval_interval\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99197091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d_%I%M%p\")\n",
    "# deets = f\"diff_steps_{diffusion_steps}_epochs_{epochs}\"\n",
    "# pickle.dump(model, open(f\"models/transfer_learning/model_{dt}_{deets}.pkl\", 'wb'))\n",
    "# pickle.dump(diffusion, open(f\"models/transfer_learning/diffusion_{dt}_{deets}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models/transfer_learning/model_0213_1014AM_diff_steps_1500_epochs_10000.pkl', 'rb') as handle:\n",
    "#     model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96514fc1-301f-40e6-93f1-c02f4618745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models/transfer_learning/diffusion_0213_1014AM_diff_steps_1500_epochs_10000.pkl', 'rb') as handle:\n",
    "#     diffusion = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e555d-f030-4e34-80e5-d9214719a635",
   "metadata": {},
   "source": [
    "# (Only for transfer learning) Finetuning on only Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3285f8d-6e87-4b93-8368-315c4e42b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# model.lm_head0 = nn.Linear(hidden_dim, hidden_dim)\n",
    "# model.lm_head0.weight.requires_grad_(True)\n",
    "model.lm_head = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "model.lm_head.weight.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef7636-b94d-4d6a-8bfd-6be586b9abff",
   "metadata": {},
   "source": [
    "## Check if we have enabled/disabled grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109e62d-f7fb-45b4-bc9a-46ae6e869105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.word_embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c53b4-9e5c-4a74-9795-450cae3dac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cf584-0baa-4e4f-93ac-87f0853edc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().to(dist_util.dev())\n",
    "\n",
    "model_emb = torch.nn.Embedding(\n",
    "        num_embeddings=tokenizer.vocab_size, \n",
    "        embedding_dim=hidden_dim, \n",
    "        _weight=model.word_embedding.weight.clone().cpu()\n",
    "    ).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f84d6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss_data = load_data_text(\n",
    "        batch_size=10,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=regular_data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_emb.cpu() # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed95f6-1f0b-4122-8e53-695d3832b098",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train() # TURNING THE TRAIN MODE BACK ON TO ENABLE BATCHNORM/DROPOUT!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782e70f-13a0-471d-b145-bf2e1ff389e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.word_embedding.weight.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f9b62-c156-4cca-9ed1-d926387313f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.word_embedding.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1567146-e431-4e4c-880a-946f63862da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca7276-22dc-443c-ab5c-0277112083d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=ss_data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "#         eval_data=data_valid,\n",
    "        eval_interval=eval_interval\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac3320-1c20-4c6f-b096-82301c8b1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d_%I%M%p\")\n",
    "# deets = f\"diff_steps_{diffusion_steps}_epochs_{epochs}\"\n",
    "# pickle.dump(model, open(f\"models/transfer_learning/model_{dt}_{deets}.pkl\", 'wb'))\n",
    "# pickle.dump(diffusion, open(f\"models/transfer_learning/diffusion_{dt}_{deets}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017ea03-44f7-4085-b941-fcf141888763",
   "metadata": {},
   "source": [
    "## Comparing with model before transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1d4e4-a735-48ac-81ed-497004a9d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('models/transfer_learning/diffusion_0213_0609AM_diff_steps_1500_epochs_2.pkl', 'rb') as handle:\n",
    "#     diffusion_ori = pickle.load(handle)\n",
    "    \n",
    "# with open('models/transfer_learning/model_0213_0609AM_diff_steps_1500_epochs_2.pkl', 'rb') as handle:\n",
    "#     model_ori = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8329e-1799-41a2-81d4-c6a8146eaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ori.lm_head.weight == model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7843f-6205-4b63-aad6-eed0d8493d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ori.word_embedding.weight == model.word_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56521b81-3278-4b9e-b20e-58ce1c4665f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ori.input_up_proj[0].weight == model.input_up_proj[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cc8f8-b321-4aba-b0c6-f1a581a7c5dd",
   "metadata": {},
   "source": [
    "### Nice! Worked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b1eae-db01-4aff-b0c4-52c568565e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = dist_util.dev()\n",
    "\n",
    "def sampling(\n",
    "    model, \n",
    "    diffusion, \n",
    "    tokenizer, \n",
    "    device=device, \n",
    "    batch_size=16, \n",
    "    seq_len=128, \n",
    "    data_dir='data/',\n",
    "    split='test',\n",
    "    clip_denoised=False, \n",
    "    model_kwargs={}, \n",
    "    top_p=0, \n",
    "    step_gap=1,\n",
    "    clamp_step=0):\n",
    "    \n",
    "    # ---- putting the model into eval mode ----\n",
    "    model.eval().requires_grad_(False).to(device)\n",
    "    \n",
    "    hidden_dim = model.word_embedding.embedding_dim\n",
    "\n",
    "    model_emb = torch.nn.Embedding(\n",
    "            num_embeddings=tokenizer.vocab_size, \n",
    "            embedding_dim=hidden_dim, \n",
    "            _weight=model.word_embedding.weight.clone().cpu()\n",
    "        ).eval().requires_grad_(False)\n",
    "    \n",
    "    # ---- getting test data ----\n",
    "\n",
    "    data_test = load_data_text(\n",
    "            batch_size=batch_size,\n",
    "            seq_len=seq_len,\n",
    "            deterministic=True,\n",
    "            data_dir=data_dir,\n",
    "            split=split,\n",
    "            loaded_vocab=tokenizer,\n",
    "            model_emb=model_emb.cpu(),  # using the same embedding wight with tranining data\n",
    "            loop=False\n",
    "        )\n",
    "\n",
    "    all_test_data = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            batch, cond = next(data_test)\n",
    "            # print(batch.shape)\n",
    "            all_test_data.append(cond)\n",
    "\n",
    "    except StopIteration:\n",
    "        print('### End of reading iteration...')\n",
    "\n",
    "    model_emb.to(device)\n",
    "    \n",
    "    # ---- iterating through the test data to generate sequences ----\n",
    "    iterator = iter(all_test_data)\n",
    "    word_lst_recover = []\n",
    "    word_lst_ref = []\n",
    "    word_lst_source = []\n",
    "\n",
    "    for cond in iterator:\n",
    "\n",
    "        input_ids_x = cond.pop('input_ids').to(device)\n",
    "        x_start = model.get_embeds(input_ids_x)\n",
    "        input_ids_mask = cond.pop('input_mask')\n",
    "        input_ids_mask_ori = input_ids_mask\n",
    "\n",
    "        noise = torch.randn_like(x_start)\n",
    "        input_ids_mask = torch.broadcast_to(input_ids_mask.unsqueeze(dim=-1), x_start.shape).to(device)\n",
    "        x_noised = torch.where(input_ids_mask == 0, x_start, noise)\n",
    "\n",
    "        model_kwargs = {}\n",
    "        sample_fn = diffusion.p_sample_loop\n",
    "        sample_shape = (x_start.shape[0], seq_len, hidden_dim)\n",
    "\n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            sample_shape,\n",
    "            noise=x_noised,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=partial(denoised_fn_round, model_emb),\n",
    "            model_kwargs=model_kwargs,\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=True,\n",
    "            mask=input_ids_mask,\n",
    "            x_start=x_start,\n",
    "            gap=step_gap\n",
    "        )\n",
    "\n",
    "        # print(samples[0].shape) # samples for each step\n",
    "\n",
    "        sample = samples[-1]\n",
    "\n",
    "        # print('decoding for seq2seq', )\n",
    "        # print(sample.shape)\n",
    "\n",
    "        logits = model.get_logits(sample)  # bsz, seqlen, vocab\n",
    "        cands = torch.topk(logits, k=1, dim=-1)\n",
    "\n",
    "        for seq, input_mask in zip(cands.indices, input_ids_mask_ori):\n",
    "            len_x = seq_len - sum(input_mask).tolist()\n",
    "            tokens = tokenizer.decode_token(seq[len_x:])\n",
    "            word_lst_recover.append(tokens)\n",
    "\n",
    "        for seq, input_mask in zip(input_ids_x, input_ids_mask_ori):\n",
    "            # tokens = tokenizer.decode_token(seq)\n",
    "            len_x = seq_len - sum(input_mask).tolist()\n",
    "            word_lst_source.append(tokenizer.decode_token(seq[:len_x]))\n",
    "            word_lst_ref.append(tokenizer.decode_token(seq[len_x:]))\n",
    "    \n",
    "    return word_lst_source, word_lst_recover, word_lst_ref\n",
    "\n",
    "\n",
    "def get_efficient_knn(model_emb, text_emb):\n",
    "    emb_norm = (model_emb**2).sum(-1).view(-1, 1) # vocab\n",
    "    text_emb_t = torch.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1) # d, bsz*seqlen\n",
    "    arr_norm = (text_emb ** 2).sum(-1).view(-1, 1) # bsz*seqlen, 1\n",
    "    # print(emb_norm.shape, arr_norm.shape)\n",
    "    dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * torch.mm(model_emb, text_emb_t) # (vocab, d) x (d, bsz*seqlen)\n",
    "    dist = torch.clamp(dist, 0.0, np.inf)\n",
    "    # print(dist.shape)\n",
    "    topk_out = torch.topk(-dist, k=1, dim=0)\n",
    "    return topk_out.values, topk_out.indices\n",
    "\n",
    "def denoised_fn_round(model, text_emb, t):\n",
    "    # print(text_emb.shape) # bsz, seqlen, dim\n",
    "    model_emb = model.weight  # input_embs\n",
    "    # print(t)\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1))\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "    # val, indices = get_knn(model_emb, text_emb.to(model_emb.device), dist=dist)\n",
    "    val, indices = get_efficient_knn(model_emb, text_emb.to(model_emb.device))\n",
    "    rounded_tokens = indices[0]\n",
    "    # print(rounded_tokens.shape)\n",
    "    new_embeds = model(rounded_tokens).view(old_shape).to(old_device)\n",
    "\n",
    "    return new_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fbb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_denoised = False\n",
    "top_p = 0\n",
    "clamp_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref = sampling(model, diffusion, tokenizer, data_dir=regular_data_dir, batch_size=5, split='test_custom', seq_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184761",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50602c5-16a5-4190-865a-9949394408fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
