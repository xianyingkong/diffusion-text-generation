{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_train import create_model_and_diffusion\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from tokenizer import load_tokenizer, load_model_emb\n",
    "from sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch, os\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.00001\n",
    "batch_size=20\n",
    "microbatch=5\n",
    "epochs=30_000\n",
    "eval_interval=100\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=2000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=102\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_dir='data/commonsense'\n",
    "ss_data_dir='data/shakespeare'\n",
    "ss_small_data_dir='data/mini-shakespeare'\n",
    "combined_data_dir='data/combined'\n",
    "combined_small_data_dir='data/combined/small'\n",
    "regular_data_dir='data'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('shakespeare_plays', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30267, 128)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30267"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['o hell! what have we here? a carrion death, within whose empty eye there is a written scroll!', 'and his disciples only envy at, ye blew the fire that burns ye now have at ye! enter king,'] [\"i'll read the writing. all that glitters is not gold, often have you heard that told\", 'frowning on them, takes his seat']\n",
      "RAM used: 3832.46 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "RAM used: 3844.47 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd94e5f761c408693460b5c0e1762ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "### tokenized_datasets...example [2, 36, 1299, 5, 163, 149, 132, 236, 21, 22, 7134, 431, 9, 905, 568, 3065, 755, 209, 120, 22, 4179, 7421, 5, 3]\n",
      "RAM used: 3825.27 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ca01c2f1ee4a159b2f9a5c8ee4bb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3855.66 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432055d27a234ba5a9484a56081f259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 48627\n",
      "}) padded dataset\n",
      "RAM used: 3945.82 MB\n",
      "RAM used: 3945.82 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"petruchio is my name, antonio's son, a man well known throughout all italy.\", 'the matter is to me, sir, as concerning jaquenetta. the manner of it is,'] ['i know him well you are welcome for his sake.', 'i was taken with the manner.']\n",
      "RAM used: 3827.22 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "RAM used: 3827.22 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c0fb09fbce415fbbf0a3ccf7dff67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "### tokenized_datasets...example [2, 3885, 120, 104, 519, 9, 2545, 8, 40, 477, 9, 22, 210, 253, 1232, 9839, 186, 4042, 11, 3]\n",
      "RAM used: 3827.13 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05dd729f2d14b5d8e031b2876853093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3838.37 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31da36ba530b4a1eaa31c9c05493142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 12147\n",
      "}) padded dataset\n",
      "RAM used: 3856.27 MB\n",
      "RAM used: 3856.27 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(30267, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=30267, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )\n",
    "\n",
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192379"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 1e-05\n",
      "name: lm_head.bias, lr: 1e-05\n",
      "name: time_embed.0.weight, lr: 1e-05\n",
      "name: time_embed.0.bias, lr: 1e-05\n",
      "name: time_embed.2.weight, lr: 1e-05\n",
      "name: time_embed.2.bias, lr: 1e-05\n",
      "name: input_up_proj.0.weight, lr: 1e-05\n",
      "name: input_up_proj.0.bias, lr: 1e-05\n",
      "name: input_up_proj.2.weight, lr: 1e-05\n",
      "name: input_up_proj.2.bias, lr: 1e-05\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 1.3333333333333335e-05\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 1.777777777777778e-05\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 2.3703703703703707e-05\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 3.160493827160494e-05\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 4.213991769547325e-05\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 5.618655692729767e-05\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 7.49154092363969e-05\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 9.988721231519586e-05\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.00013318294975359448\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00017757726633812598\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.00023676968845083463\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.0003156929179344462\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.0003156929179344462\n",
      "name: position_embeddings.weight, lr: 0.00042092389057926156\n",
      "name: LayerNorm.weight, lr: 0.00042092389057926156\n",
      "name: LayerNorm.bias, lr: 0.00042092389057926156\n",
      "name: output_down_proj.0.weight, lr: 0.00042092389057926156\n",
      "name: output_down_proj.0.bias, lr: 0.00042092389057926156\n",
      "name: output_down_proj.2.weight, lr: 0.00042092389057926156\n",
      "name: output_down_proj.2.bias, lr: 0.00042092389057926156\n",
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n",
      "Epoch 1/30000 Training Loss: 1.0403802394866943\n",
      "Epoch 2/30000 Training Loss: 1.0366294384002686\n",
      "Epoch 3/30000 Training Loss: 1.0343965291976929\n",
      "Epoch 4/30000 Training Loss: 1.0369503498077393\n",
      "Epoch 5/30000 Training Loss: 1.0242023468017578\n",
      "Epoch 6/30000 Training Loss: 1.023964524269104\n",
      "Epoch 7/30000 Training Loss: 1.0131213665008545\n",
      "Epoch 8/30000 Training Loss: 0.9997575879096985\n",
      "Epoch 9/30000 Training Loss: 0.9854422807693481\n",
      "Epoch 10/30000 Training Loss: 0.9750477075576782\n",
      "Epoch 11/30000 Training Loss: 0.9613888263702393\n",
      "Epoch 12/30000 Training Loss: 0.9462125301361084\n",
      "Epoch 13/30000 Training Loss: 0.9235926270484924\n",
      "Epoch 14/30000 Training Loss: 0.908822774887085\n",
      "Epoch 15/30000 Training Loss: 0.8994618654251099\n",
      "Epoch 16/30000 Training Loss: 0.8771619200706482\n",
      "Epoch 17/30000 Training Loss: 0.8504419326782227\n",
      "Epoch 18/30000 Training Loss: 0.8377832174301147\n",
      "Epoch 19/30000 Training Loss: 0.8261462450027466\n",
      "Epoch 20/30000 Training Loss: 0.7891408801078796\n",
      "Epoch 21/30000 Training Loss: 0.7564250826835632\n",
      "Epoch 22/30000 Training Loss: 0.7505548000335693\n",
      "Epoch 23/30000 Training Loss: 0.7171036601066589\n",
      "Epoch 24/30000 Training Loss: 0.6943859457969666\n",
      "Epoch 25/30000 Training Loss: 0.6807619333267212\n",
      "Epoch 26/30000 Training Loss: 0.6410375237464905\n",
      "Epoch 27/30000 Training Loss: 0.6174628138542175\n",
      "Epoch 28/30000 Training Loss: 0.6123073697090149\n",
      "Epoch 29/30000 Training Loss: 0.5836594104766846\n",
      "Epoch 30/30000 Training Loss: 0.5913755893707275\n",
      "Epoch 31/30000 Training Loss: 0.6013786792755127\n",
      "Epoch 32/30000 Training Loss: 0.5545620918273926\n",
      "Epoch 33/30000 Training Loss: 0.5051664710044861\n",
      "Epoch 34/30000 Training Loss: 0.5496265292167664\n",
      "Epoch 35/30000 Training Loss: 0.53345787525177\n",
      "Epoch 36/30000 Training Loss: 0.5179420113563538\n",
      "Epoch 37/30000 Training Loss: 0.4983433187007904\n",
      "Epoch 38/30000 Training Loss: 0.5011707544326782\n",
      "Epoch 39/30000 Training Loss: 0.5190812349319458\n",
      "Epoch 40/30000 Training Loss: 0.5472955703735352\n",
      "Epoch 41/30000 Training Loss: 0.5151174664497375\n",
      "Epoch 42/30000 Training Loss: 0.4629547595977783\n",
      "Epoch 43/30000 Training Loss: 0.4845314025878906\n",
      "Epoch 44/30000 Training Loss: 0.4794777035713196\n",
      "Epoch 45/30000 Training Loss: 0.5090282559394836\n",
      "Epoch 46/30000 Training Loss: 0.5094305276870728\n",
      "Epoch 47/30000 Training Loss: 0.48554113507270813\n",
      "Epoch 48/30000 Training Loss: 0.5032960176467896\n",
      "Epoch 49/30000 Training Loss: 0.4797658920288086\n",
      "Epoch 50/30000 Training Loss: 0.5163441300392151\n",
      "Epoch 51/30000 Training Loss: 0.5193977952003479\n",
      "Epoch 52/30000 Training Loss: 0.48763027787208557\n",
      "Epoch 53/30000 Training Loss: 0.4945026636123657\n",
      "Epoch 54/30000 Training Loss: 0.4850430488586426\n",
      "Epoch 55/30000 Training Loss: 0.46498996019363403\n",
      "Epoch 56/30000 Training Loss: 0.4901125431060791\n",
      "Epoch 57/30000 Training Loss: 0.4722431004047394\n",
      "Epoch 58/30000 Training Loss: 0.49202749133110046\n",
      "Epoch 59/30000 Training Loss: 0.5064816474914551\n",
      "Epoch 60/30000 Training Loss: 0.47710633277893066\n",
      "Epoch 61/30000 Training Loss: 0.48597681522369385\n",
      "Epoch 62/30000 Training Loss: 0.47550466656684875\n",
      "Epoch 63/30000 Training Loss: 0.44085893034935\n",
      "Epoch 64/30000 Training Loss: 0.4770112633705139\n",
      "Epoch 65/30000 Training Loss: 0.496160626411438\n",
      "Epoch 66/30000 Training Loss: 0.45230355858802795\n",
      "Epoch 67/30000 Training Loss: 0.45764923095703125\n",
      "Epoch 68/30000 Training Loss: 0.4553873538970947\n",
      "Epoch 69/30000 Training Loss: 0.48707878589630127\n",
      "Epoch 70/30000 Training Loss: 0.440173864364624\n",
      "Epoch 71/30000 Training Loss: 0.4208317995071411\n",
      "Epoch 72/30000 Training Loss: 0.4627542495727539\n",
      "Epoch 73/30000 Training Loss: 0.40977025032043457\n",
      "Epoch 74/30000 Training Loss: 0.4392487406730652\n",
      "Epoch 75/30000 Training Loss: 0.3926847577095032\n",
      "Epoch 76/30000 Training Loss: 0.403334379196167\n",
      "Epoch 77/30000 Training Loss: 0.38710126280784607\n",
      "Epoch 78/30000 Training Loss: 0.3741057217121124\n",
      "Epoch 79/30000 Training Loss: 0.39452630281448364\n",
      "Epoch 80/30000 Training Loss: 0.3765992522239685\n",
      "Epoch 81/30000 Training Loss: 0.3724687993526459\n",
      "Epoch 82/30000 Training Loss: 0.3683265447616577\n",
      "Epoch 83/30000 Training Loss: 0.34364986419677734\n",
      "Epoch 84/30000 Training Loss: 0.3384382426738739\n",
      "Epoch 85/30000 Training Loss: 0.3641931116580963\n",
      "Epoch 86/30000 Training Loss: 0.3557813763618469\n",
      "Epoch 87/30000 Training Loss: 0.3484618067741394\n",
      "Epoch 88/30000 Training Loss: 0.3090592622756958\n",
      "Epoch 89/30000 Training Loss: 0.339312881231308\n",
      "Epoch 90/30000 Training Loss: 0.32005149126052856\n",
      "Epoch 91/30000 Training Loss: 0.3341636657714844\n",
      "Epoch 92/30000 Training Loss: 0.33440613746643066\n",
      "Epoch 93/30000 Training Loss: 0.33331039547920227\n",
      "Epoch 94/30000 Training Loss: 0.32352277636528015\n",
      "Epoch 95/30000 Training Loss: 0.3485621511936188\n",
      "Epoch 96/30000 Training Loss: 0.32140862941741943\n",
      "Epoch 97/30000 Training Loss: 0.31302106380462646\n",
      "Epoch 98/30000 Training Loss: 0.3101048469543457\n",
      "Epoch 99/30000 Training Loss: 0.3144879937171936\n",
      "Epoch 100/30000 Training Loss: 0.3145985007286072\n",
      "Epoch 100/30000 Validation Loss: 0.332404226064682\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.332404226064682<=============\n",
      "Epoch 101/30000 Training Loss: 0.30790191888809204\n",
      "Epoch 102/30000 Training Loss: 0.31632688641548157\n",
      "Epoch 103/30000 Training Loss: 0.29988616704940796\n",
      "Epoch 104/30000 Training Loss: 0.32112759351730347\n",
      "Epoch 105/30000 Training Loss: 0.34385257959365845\n",
      "Epoch 106/30000 Training Loss: 0.29163292050361633\n",
      "Epoch 107/30000 Training Loss: 0.3357064127922058\n",
      "Epoch 108/30000 Training Loss: 0.3157825469970703\n",
      "Epoch 109/30000 Training Loss: 0.2920300364494324\n",
      "Epoch 110/30000 Training Loss: 0.2955252528190613\n",
      "Epoch 111/30000 Training Loss: 0.3074697256088257\n",
      "Epoch 112/30000 Training Loss: 0.31393539905548096\n",
      "Epoch 113/30000 Training Loss: 0.30672487616539\n",
      "Epoch 114/30000 Training Loss: 0.3252944052219391\n",
      "Epoch 115/30000 Training Loss: 0.3326815366744995\n",
      "Epoch 116/30000 Training Loss: 0.30974552035331726\n",
      "Epoch 117/30000 Training Loss: 0.31792551279067993\n",
      "Epoch 118/30000 Training Loss: 0.32167208194732666\n",
      "Epoch 119/30000 Training Loss: 0.29551565647125244\n",
      "Epoch 120/30000 Training Loss: 0.28552913665771484\n",
      "Epoch 121/30000 Training Loss: 0.3217088580131531\n",
      "Epoch 122/30000 Training Loss: 0.3199428915977478\n",
      "Epoch 123/30000 Training Loss: 0.2976061701774597\n",
      "Epoch 124/30000 Training Loss: 0.3123166859149933\n",
      "Epoch 125/30000 Training Loss: 0.2707209885120392\n",
      "Epoch 126/30000 Training Loss: 0.29386061429977417\n",
      "Epoch 127/30000 Training Loss: 0.32387134432792664\n",
      "Epoch 128/30000 Training Loss: 0.3029695153236389\n",
      "Epoch 129/30000 Training Loss: 0.2729499340057373\n",
      "Epoch 130/30000 Training Loss: 0.26558974385261536\n",
      "Epoch 131/30000 Training Loss: 0.29341545701026917\n",
      "Epoch 132/30000 Training Loss: 0.3097187280654907\n",
      "Epoch 133/30000 Training Loss: 0.2900319993495941\n",
      "Epoch 134/30000 Training Loss: 0.31622788310050964\n",
      "Epoch 135/30000 Training Loss: 0.27061009407043457\n",
      "Epoch 136/30000 Training Loss: 0.2995043694972992\n",
      "Epoch 137/30000 Training Loss: 0.29491546750068665\n",
      "Epoch 138/30000 Training Loss: 0.2819777727127075\n",
      "Epoch 139/30000 Training Loss: 0.2322782576084137\n",
      "Epoch 140/30000 Training Loss: 0.31631091237068176\n",
      "Epoch 141/30000 Training Loss: 0.2727738618850708\n",
      "Epoch 142/30000 Training Loss: 0.2969319224357605\n",
      "Epoch 143/30000 Training Loss: 0.2980843186378479\n",
      "Epoch 144/30000 Training Loss: 0.2721348702907562\n",
      "Epoch 145/30000 Training Loss: 0.26041585206985474\n",
      "Epoch 146/30000 Training Loss: 0.29152220487594604\n",
      "Epoch 147/30000 Training Loss: 0.29755452275276184\n",
      "Epoch 148/30000 Training Loss: 0.2979186475276947\n",
      "Epoch 149/30000 Training Loss: 0.27259254455566406\n",
      "Epoch 150/30000 Training Loss: 0.2690281867980957\n",
      "Epoch 151/30000 Training Loss: 0.289250910282135\n",
      "Epoch 152/30000 Training Loss: 0.247613787651062\n",
      "Epoch 153/30000 Training Loss: 0.2681673765182495\n",
      "Epoch 154/30000 Training Loss: 0.26431867480278015\n",
      "Epoch 155/30000 Training Loss: 0.25439247488975525\n",
      "Epoch 156/30000 Training Loss: 0.2543604075908661\n",
      "Epoch 157/30000 Training Loss: 0.2731357514858246\n",
      "Epoch 158/30000 Training Loss: 0.25446146726608276\n",
      "Epoch 159/30000 Training Loss: 0.24320700764656067\n",
      "Epoch 160/30000 Training Loss: 0.2692975699901581\n",
      "Epoch 161/30000 Training Loss: 0.25767746567726135\n",
      "Epoch 162/30000 Training Loss: 0.2715074419975281\n",
      "Epoch 163/30000 Training Loss: 0.23125244677066803\n",
      "Epoch 164/30000 Training Loss: 0.28788870573043823\n",
      "Epoch 165/30000 Training Loss: 0.25763335824012756\n",
      "Epoch 166/30000 Training Loss: 0.25788187980651855\n",
      "Epoch 167/30000 Training Loss: 0.2686149477958679\n",
      "Epoch 168/30000 Training Loss: 0.27309349179267883\n",
      "Epoch 169/30000 Training Loss: 0.26817137002944946\n",
      "Epoch 170/30000 Training Loss: 0.24618640542030334\n",
      "Epoch 171/30000 Training Loss: 0.25338929891586304\n",
      "Epoch 172/30000 Training Loss: 0.2612883150577545\n",
      "Epoch 173/30000 Training Loss: 0.2334214597940445\n",
      "Epoch 174/30000 Training Loss: 0.2492941915988922\n",
      "Epoch 175/30000 Training Loss: 0.2217199206352234\n",
      "Epoch 176/30000 Training Loss: 0.2736344039440155\n",
      "Epoch 177/30000 Training Loss: 0.25429126620292664\n",
      "Epoch 178/30000 Training Loss: 0.2741962671279907\n",
      "Epoch 179/30000 Training Loss: 0.23631072044372559\n",
      "Epoch 180/30000 Training Loss: 0.2735270857810974\n",
      "Epoch 181/30000 Training Loss: 0.25740423798561096\n",
      "Epoch 182/30000 Training Loss: 0.25329264998435974\n",
      "Epoch 183/30000 Training Loss: 0.26892033219337463\n",
      "Epoch 184/30000 Training Loss: 0.227013498544693\n",
      "Epoch 185/30000 Training Loss: 0.2632025480270386\n",
      "Epoch 186/30000 Training Loss: 0.24014481902122498\n",
      "Epoch 187/30000 Training Loss: 0.24544905126094818\n",
      "Epoch 188/30000 Training Loss: 0.2484649121761322\n",
      "Epoch 189/30000 Training Loss: 0.25019580125808716\n",
      "Epoch 190/30000 Training Loss: 0.2390727400779724\n",
      "Epoch 191/30000 Training Loss: 0.2442847192287445\n",
      "Epoch 192/30000 Training Loss: 0.253665030002594\n",
      "Epoch 193/30000 Training Loss: 0.22916264832019806\n",
      "Epoch 194/30000 Training Loss: 0.25971922278404236\n",
      "Epoch 195/30000 Training Loss: 0.2366330921649933\n",
      "Epoch 196/30000 Training Loss: 0.23590542376041412\n",
      "Epoch 197/30000 Training Loss: 0.22930705547332764\n",
      "Epoch 198/30000 Training Loss: 0.23489637672901154\n",
      "Epoch 199/30000 Training Loss: 0.23475341498851776\n",
      "Epoch 200/30000 Training Loss: 0.2197479009628296\n",
      "Epoch 200/30000 Validation Loss: 0.26683279871940613\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.26683279871940613<=============\n",
      "Epoch 201/30000 Training Loss: 0.2092263251543045\n",
      "Epoch 202/30000 Training Loss: 0.22456762194633484\n",
      "Epoch 203/30000 Training Loss: 0.20829926431179047\n",
      "Epoch 204/30000 Training Loss: 0.23737971484661102\n",
      "Epoch 205/30000 Training Loss: 0.2247648984193802\n",
      "Epoch 206/30000 Training Loss: 0.2494509220123291\n",
      "Epoch 207/30000 Training Loss: 0.2439272403717041\n",
      "Epoch 208/30000 Training Loss: 0.23538930714130402\n",
      "Epoch 209/30000 Training Loss: 0.22187677025794983\n",
      "Epoch 210/30000 Training Loss: 0.22603967785835266\n",
      "Epoch 211/30000 Training Loss: 0.21820971369743347\n",
      "Epoch 212/30000 Training Loss: 0.24332167208194733\n",
      "Epoch 213/30000 Training Loss: 0.2173510193824768\n",
      "Epoch 214/30000 Training Loss: 0.21166858077049255\n",
      "Epoch 215/30000 Training Loss: 0.22249886393547058\n",
      "Epoch 216/30000 Training Loss: 0.2346310168504715\n",
      "Epoch 217/30000 Training Loss: 0.21591627597808838\n",
      "Epoch 218/30000 Training Loss: 0.21570146083831787\n",
      "Epoch 219/30000 Training Loss: 0.21937021613121033\n",
      "Epoch 220/30000 Training Loss: 0.2123013585805893\n",
      "Epoch 221/30000 Training Loss: 0.20540904998779297\n",
      "Epoch 222/30000 Training Loss: 0.2612212300300598\n",
      "Epoch 223/30000 Training Loss: 0.20634162425994873\n",
      "Epoch 224/30000 Training Loss: 0.22347156703472137\n",
      "Epoch 225/30000 Training Loss: 0.22753456234931946\n",
      "Epoch 226/30000 Training Loss: 0.2211485654115677\n",
      "Epoch 227/30000 Training Loss: 0.19889843463897705\n",
      "Epoch 228/30000 Training Loss: 0.2096313238143921\n",
      "Epoch 229/30000 Training Loss: 0.21194785833358765\n",
      "Epoch 230/30000 Training Loss: 0.20402371883392334\n",
      "Epoch 231/30000 Training Loss: 0.22039364278316498\n",
      "Epoch 232/30000 Training Loss: 0.2051243931055069\n",
      "Epoch 233/30000 Training Loss: 0.23780684173107147\n",
      "Epoch 234/30000 Training Loss: 0.20454613864421844\n",
      "Epoch 235/30000 Training Loss: 0.2246551364660263\n",
      "Epoch 236/30000 Training Loss: 0.2012045830488205\n",
      "Epoch 237/30000 Training Loss: 0.21364901959896088\n",
      "Epoch 238/30000 Training Loss: 0.20117634534835815\n",
      "Epoch 239/30000 Training Loss: 0.2126074880361557\n",
      "Epoch 240/30000 Training Loss: 0.21642065048217773\n",
      "Epoch 241/30000 Training Loss: 0.21467351913452148\n",
      "Epoch 242/30000 Training Loss: 0.23317591845989227\n",
      "Epoch 243/30000 Training Loss: 0.22617122530937195\n",
      "Epoch 244/30000 Training Loss: 0.21198858320713043\n",
      "Epoch 245/30000 Training Loss: 0.22939099371433258\n",
      "Epoch 246/30000 Training Loss: 0.19983407855033875\n",
      "Epoch 247/30000 Training Loss: 0.21859151124954224\n",
      "Epoch 248/30000 Training Loss: 0.20371706783771515\n",
      "Epoch 249/30000 Training Loss: 0.2081076204776764\n",
      "Epoch 250/30000 Training Loss: 0.21583473682403564\n",
      "Epoch 251/30000 Training Loss: 0.18546929955482483\n",
      "Epoch 252/30000 Training Loss: 0.2007623165845871\n",
      "Epoch 253/30000 Training Loss: 0.2231738567352295\n",
      "Epoch 254/30000 Training Loss: 0.18799328804016113\n",
      "Epoch 255/30000 Training Loss: 0.18504685163497925\n",
      "Epoch 256/30000 Training Loss: 0.20351679623126984\n",
      "Epoch 257/30000 Training Loss: 0.20019090175628662\n",
      "Epoch 258/30000 Training Loss: 0.1977427899837494\n",
      "Epoch 259/30000 Training Loss: 0.19465886056423187\n",
      "Epoch 260/30000 Training Loss: 0.20808357000350952\n",
      "Epoch 261/30000 Training Loss: 0.1996031105518341\n",
      "Epoch 262/30000 Training Loss: 0.18278919160366058\n",
      "Epoch 263/30000 Training Loss: 0.22031264007091522\n",
      "Epoch 264/30000 Training Loss: 0.186619371175766\n",
      "Epoch 265/30000 Training Loss: 0.20166674256324768\n",
      "Epoch 266/30000 Training Loss: 0.20433373749256134\n",
      "Epoch 267/30000 Training Loss: 0.1812310516834259\n",
      "Epoch 268/30000 Training Loss: 0.18815749883651733\n",
      "Epoch 269/30000 Training Loss: 0.20125329494476318\n",
      "Epoch 270/30000 Training Loss: 0.18928936123847961\n",
      "Epoch 271/30000 Training Loss: 0.21557480096817017\n",
      "Epoch 272/30000 Training Loss: 0.19035868346691132\n",
      "Epoch 273/30000 Training Loss: 0.20282217860221863\n",
      "Epoch 274/30000 Training Loss: 0.2077702283859253\n",
      "Epoch 275/30000 Training Loss: 0.19954471290111542\n",
      "Epoch 276/30000 Training Loss: 0.2139064520597458\n",
      "Epoch 277/30000 Training Loss: 0.18666520714759827\n",
      "Epoch 278/30000 Training Loss: 0.19745242595672607\n",
      "Epoch 279/30000 Training Loss: 0.17163902521133423\n",
      "Epoch 280/30000 Training Loss: 0.19920897483825684\n",
      "Epoch 281/30000 Training Loss: 0.18464957177639008\n",
      "Epoch 282/30000 Training Loss: 0.2091047167778015\n",
      "Epoch 283/30000 Training Loss: 0.19013874232769012\n",
      "Epoch 284/30000 Training Loss: 0.17841409146785736\n",
      "Epoch 285/30000 Training Loss: 0.1864222288131714\n",
      "Epoch 286/30000 Training Loss: 0.18132945895195007\n",
      "Epoch 287/30000 Training Loss: 0.21702557802200317\n",
      "Epoch 288/30000 Training Loss: 0.19980762898921967\n",
      "Epoch 289/30000 Training Loss: 0.18052931129932404\n",
      "Epoch 290/30000 Training Loss: 0.18790659308433533\n",
      "Epoch 291/30000 Training Loss: 0.18049252033233643\n",
      "Epoch 292/30000 Training Loss: 0.19518452882766724\n",
      "Epoch 293/30000 Training Loss: 0.17826765775680542\n",
      "Epoch 294/30000 Training Loss: 0.14948414266109467\n",
      "Epoch 295/30000 Training Loss: 0.188163623213768\n",
      "Epoch 296/30000 Training Loss: 0.17300210893154144\n",
      "Epoch 297/30000 Training Loss: 0.17916813492774963\n",
      "Epoch 298/30000 Training Loss: 0.18963965773582458\n",
      "Epoch 299/30000 Training Loss: 0.1917169690132141\n",
      "Epoch 300/30000 Training Loss: 0.1721590757369995\n",
      "Epoch 300/30000 Validation Loss: 0.18642479181289673\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.18642479181289673<=============\n",
      "Epoch 301/30000 Training Loss: 0.18896719813346863\n",
      "Epoch 302/30000 Training Loss: 0.19739750027656555\n",
      "Epoch 303/30000 Training Loss: 0.1911356896162033\n",
      "Epoch 304/30000 Training Loss: 0.18091154098510742\n",
      "Epoch 305/30000 Training Loss: 0.1892801821231842\n",
      "Epoch 306/30000 Training Loss: 0.17437775433063507\n",
      "Epoch 307/30000 Training Loss: 0.18025395274162292\n",
      "Epoch 308/30000 Training Loss: 0.15739987790584564\n",
      "Epoch 309/30000 Training Loss: 0.19668640196323395\n",
      "Epoch 310/30000 Training Loss: 0.1735357642173767\n",
      "Epoch 311/30000 Training Loss: 0.17846140265464783\n",
      "Epoch 312/30000 Training Loss: 0.16862323880195618\n",
      "Epoch 313/30000 Training Loss: 0.18320898711681366\n",
      "Epoch 314/30000 Training Loss: 0.17066092789173126\n",
      "Epoch 315/30000 Training Loss: 0.18137790262699127\n",
      "Epoch 316/30000 Training Loss: 0.18356643617153168\n",
      "Epoch 317/30000 Training Loss: 0.17063918709754944\n",
      "Epoch 318/30000 Training Loss: 0.2008335292339325\n",
      "Epoch 319/30000 Training Loss: 0.17749491333961487\n",
      "Epoch 320/30000 Training Loss: 0.16304698586463928\n",
      "Epoch 321/30000 Training Loss: 0.17116641998291016\n",
      "Epoch 322/30000 Training Loss: 0.1649794578552246\n",
      "Epoch 323/30000 Training Loss: 0.1988554149866104\n",
      "Epoch 324/30000 Training Loss: 0.17478318512439728\n",
      "Epoch 325/30000 Training Loss: 0.17497219145298004\n",
      "Epoch 326/30000 Training Loss: 0.20488721132278442\n",
      "Epoch 327/30000 Training Loss: 0.18177708983421326\n",
      "Epoch 328/30000 Training Loss: 0.16833025217056274\n",
      "Epoch 329/30000 Training Loss: 0.19100283086299896\n",
      "Epoch 330/30000 Training Loss: 0.1935747265815735\n",
      "Epoch 331/30000 Training Loss: 0.19136737287044525\n",
      "Epoch 332/30000 Training Loss: 0.15760013461112976\n",
      "Epoch 333/30000 Training Loss: 0.16331426799297333\n",
      "Epoch 334/30000 Training Loss: 0.18687567114830017\n",
      "Epoch 335/30000 Training Loss: 0.18912021815776825\n",
      "Epoch 336/30000 Training Loss: 0.1688503921031952\n",
      "Epoch 337/30000 Training Loss: 0.16621336340904236\n",
      "Epoch 338/30000 Training Loss: 0.1686779260635376\n",
      "Epoch 339/30000 Training Loss: 0.1701890528202057\n",
      "Epoch 340/30000 Training Loss: 0.15417319536209106\n",
      "Epoch 341/30000 Training Loss: 0.16902999579906464\n",
      "Epoch 342/30000 Training Loss: 0.1829768568277359\n",
      "Epoch 343/30000 Training Loss: 0.17281225323677063\n",
      "Epoch 344/30000 Training Loss: 0.19173049926757812\n",
      "Epoch 345/30000 Training Loss: 0.15219008922576904\n",
      "Epoch 346/30000 Training Loss: 0.17145970463752747\n",
      "Epoch 347/30000 Training Loss: 0.1791096329689026\n",
      "Epoch 348/30000 Training Loss: 0.179343581199646\n",
      "Epoch 349/30000 Training Loss: 0.19218282401561737\n",
      "Epoch 350/30000 Training Loss: 0.1834070235490799\n",
      "Epoch 351/30000 Training Loss: 0.19098392128944397\n",
      "Epoch 352/30000 Training Loss: 0.17849986255168915\n",
      "Epoch 353/30000 Training Loss: 0.1710631400346756\n",
      "Epoch 354/30000 Training Loss: 0.17628508806228638\n",
      "Epoch 355/30000 Training Loss: 0.16365480422973633\n",
      "Epoch 356/30000 Training Loss: 0.19519808888435364\n",
      "Epoch 357/30000 Training Loss: 0.18568134307861328\n",
      "Epoch 358/30000 Training Loss: 0.1770843267440796\n",
      "Epoch 359/30000 Training Loss: 0.17359426617622375\n",
      "Epoch 360/30000 Training Loss: 0.16338270902633667\n",
      "Epoch 361/30000 Training Loss: 0.1703319251537323\n",
      "Epoch 362/30000 Training Loss: 0.14854583144187927\n",
      "Epoch 363/30000 Training Loss: 0.16838572919368744\n",
      "Epoch 364/30000 Training Loss: 0.15855401754379272\n",
      "Epoch 365/30000 Training Loss: 0.1736016422510147\n",
      "Epoch 366/30000 Training Loss: 0.1534767895936966\n",
      "Epoch 367/30000 Training Loss: 0.15011323988437653\n",
      "Epoch 368/30000 Training Loss: 0.17100204527378082\n",
      "Epoch 369/30000 Training Loss: 0.155210480093956\n",
      "Epoch 370/30000 Training Loss: 0.1551322638988495\n",
      "Epoch 371/30000 Training Loss: 0.1644463837146759\n",
      "Epoch 372/30000 Training Loss: 0.15120337903499603\n",
      "Epoch 373/30000 Training Loss: 0.14954021573066711\n",
      "Epoch 374/30000 Training Loss: 0.1694188117980957\n",
      "Epoch 375/30000 Training Loss: 0.15531380474567413\n",
      "Epoch 376/30000 Training Loss: 0.16565869748592377\n",
      "Epoch 377/30000 Training Loss: 0.15382255613803864\n",
      "Epoch 378/30000 Training Loss: 0.15105855464935303\n",
      "Epoch 379/30000 Training Loss: 0.1653481423854828\n",
      "Epoch 380/30000 Training Loss: 0.17663069069385529\n",
      "Epoch 381/30000 Training Loss: 0.15318164229393005\n",
      "Epoch 382/30000 Training Loss: 0.17304959893226624\n",
      "Epoch 383/30000 Training Loss: 0.14213870465755463\n",
      "Epoch 384/30000 Training Loss: 0.17167183756828308\n",
      "Epoch 385/30000 Training Loss: 0.17642802000045776\n",
      "Epoch 386/30000 Training Loss: 0.1304343044757843\n",
      "Epoch 387/30000 Training Loss: 0.17722037434577942\n",
      "Epoch 388/30000 Training Loss: 0.180352583527565\n",
      "Epoch 389/30000 Training Loss: 0.16263310611248016\n",
      "Epoch 390/30000 Training Loss: 0.1563071459531784\n",
      "Epoch 391/30000 Training Loss: 0.16233155131340027\n",
      "Epoch 392/30000 Training Loss: 0.13589896261692047\n",
      "Epoch 393/30000 Training Loss: 0.16063739359378815\n",
      "Epoch 394/30000 Training Loss: 0.1455731838941574\n",
      "Epoch 395/30000 Training Loss: 0.1527635157108307\n",
      "Epoch 396/30000 Training Loss: 0.15261486172676086\n",
      "Epoch 397/30000 Training Loss: 0.14087079465389252\n",
      "Epoch 398/30000 Training Loss: 0.13354994356632233\n",
      "Epoch 399/30000 Training Loss: 0.17216019332408905\n",
      "Epoch 400/30000 Training Loss: 0.16483774781227112\n",
      "Epoch 400/30000 Validation Loss: 0.14337992668151855\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.14337992668151855<=============\n",
      "Epoch 401/30000 Training Loss: 0.14070071280002594\n",
      "Epoch 402/30000 Training Loss: 0.16172584891319275\n",
      "Epoch 403/30000 Training Loss: 0.14631181955337524\n",
      "Epoch 404/30000 Training Loss: 0.15721505880355835\n",
      "Epoch 405/30000 Training Loss: 0.14726190268993378\n",
      "Epoch 406/30000 Training Loss: 0.16797587275505066\n",
      "Epoch 407/30000 Training Loss: 0.1557522714138031\n",
      "Epoch 408/30000 Training Loss: 0.15241378545761108\n",
      "Epoch 409/30000 Training Loss: 0.16839265823364258\n",
      "Epoch 410/30000 Training Loss: 0.14308205246925354\n",
      "Epoch 411/30000 Training Loss: 0.150032177567482\n",
      "Epoch 412/30000 Training Loss: 0.14383167028427124\n",
      "Epoch 413/30000 Training Loss: 0.15485623478889465\n",
      "Epoch 414/30000 Training Loss: 0.1410001814365387\n",
      "Epoch 415/30000 Training Loss: 0.14530698955059052\n",
      "Epoch 416/30000 Training Loss: 0.1367974430322647\n",
      "Epoch 417/30000 Training Loss: 0.14968925714492798\n",
      "Epoch 418/30000 Training Loss: 0.14011847972869873\n",
      "Epoch 419/30000 Training Loss: 0.14602747559547424\n",
      "Epoch 420/30000 Training Loss: 0.15799346566200256\n",
      "Epoch 421/30000 Training Loss: 0.15760022401809692\n",
      "Epoch 422/30000 Training Loss: 0.14035220444202423\n",
      "Epoch 423/30000 Training Loss: 0.15678352117538452\n",
      "Epoch 424/30000 Training Loss: 0.1671978086233139\n",
      "Epoch 425/30000 Training Loss: 0.1599547117948532\n",
      "Epoch 426/30000 Training Loss: 0.144536092877388\n",
      "Epoch 427/30000 Training Loss: 0.1365596503019333\n",
      "Epoch 428/30000 Training Loss: 0.14795660972595215\n",
      "Epoch 429/30000 Training Loss: 0.15629082918167114\n",
      "Epoch 430/30000 Training Loss: 0.14574547111988068\n",
      "Epoch 431/30000 Training Loss: 0.14243192970752716\n",
      "Epoch 432/30000 Training Loss: 0.13213378190994263\n",
      "Epoch 433/30000 Training Loss: 0.1659030318260193\n",
      "Epoch 434/30000 Training Loss: 0.15881547331809998\n",
      "Epoch 435/30000 Training Loss: 0.14545190334320068\n",
      "Epoch 436/30000 Training Loss: 0.1462068408727646\n",
      "Epoch 437/30000 Training Loss: 0.12891419231891632\n",
      "Epoch 438/30000 Training Loss: 0.13534581661224365\n",
      "Epoch 439/30000 Training Loss: 0.13339847326278687\n",
      "Epoch 440/30000 Training Loss: 0.1426350474357605\n",
      "Epoch 441/30000 Training Loss: 0.1608966588973999\n",
      "Epoch 442/30000 Training Loss: 0.14206063747406006\n",
      "Epoch 443/30000 Training Loss: 0.12741172313690186\n",
      "Epoch 444/30000 Training Loss: 0.13251201808452606\n",
      "Epoch 445/30000 Training Loss: 0.14673030376434326\n",
      "Epoch 446/30000 Training Loss: 0.1247769445180893\n",
      "Epoch 447/30000 Training Loss: 0.1338377594947815\n",
      "Epoch 448/30000 Training Loss: 0.14637064933776855\n",
      "Epoch 449/30000 Training Loss: 0.16939842700958252\n",
      "Epoch 450/30000 Training Loss: 0.14474701881408691\n",
      "Epoch 451/30000 Training Loss: 0.15673257410526276\n",
      "Epoch 452/30000 Training Loss: 0.1563367396593094\n",
      "Epoch 453/30000 Training Loss: 0.14964337646961212\n",
      "Epoch 454/30000 Training Loss: 0.14024986326694489\n",
      "Epoch 455/30000 Training Loss: 0.1290944218635559\n",
      "Epoch 456/30000 Training Loss: 0.13406364619731903\n",
      "Epoch 457/30000 Training Loss: 0.14415088295936584\n",
      "Epoch 458/30000 Training Loss: 0.1432267278432846\n",
      "Epoch 459/30000 Training Loss: 0.13471491634845734\n",
      "Epoch 460/30000 Training Loss: 0.14311034977436066\n",
      "Epoch 461/30000 Training Loss: 0.13480229675769806\n",
      "Epoch 462/30000 Training Loss: 0.14573079347610474\n",
      "Epoch 463/30000 Training Loss: 0.16014061868190765\n",
      "Epoch 464/30000 Training Loss: 0.14290320873260498\n",
      "Epoch 465/30000 Training Loss: 0.14006192982196808\n",
      "Epoch 466/30000 Training Loss: 0.14525987207889557\n",
      "Epoch 467/30000 Training Loss: 0.15020442008972168\n",
      "Epoch 468/30000 Training Loss: 0.14528004825115204\n",
      "Epoch 469/30000 Training Loss: 0.12307882308959961\n",
      "Epoch 470/30000 Training Loss: 0.15800327062606812\n",
      "Epoch 471/30000 Training Loss: 0.14499899744987488\n",
      "Epoch 472/30000 Training Loss: 0.13704603910446167\n",
      "Epoch 473/30000 Training Loss: 0.16325905919075012\n",
      "Epoch 474/30000 Training Loss: 0.13063223659992218\n",
      "Epoch 475/30000 Training Loss: 0.16014058887958527\n",
      "Epoch 476/30000 Training Loss: 0.13421249389648438\n",
      "Epoch 477/30000 Training Loss: 0.14477337896823883\n",
      "Epoch 478/30000 Training Loss: 0.16154272854328156\n",
      "Epoch 479/30000 Training Loss: 0.13688433170318604\n",
      "Epoch 480/30000 Training Loss: 0.14919817447662354\n",
      "Epoch 481/30000 Training Loss: 0.1460585594177246\n",
      "Epoch 482/30000 Training Loss: 0.12696662545204163\n",
      "Epoch 483/30000 Training Loss: 0.1506895273923874\n",
      "Epoch 484/30000 Training Loss: 0.14240378141403198\n",
      "Epoch 485/30000 Training Loss: 0.1457361876964569\n",
      "Epoch 486/30000 Training Loss: 0.1402900218963623\n",
      "Epoch 487/30000 Training Loss: 0.1300359070301056\n",
      "Epoch 488/30000 Training Loss: 0.13701553642749786\n",
      "Epoch 489/30000 Training Loss: 0.12967126071453094\n",
      "Epoch 490/30000 Training Loss: 0.13919475674629211\n",
      "Epoch 491/30000 Training Loss: 0.1385413110256195\n",
      "Epoch 492/30000 Training Loss: 0.15365096926689148\n",
      "Epoch 493/30000 Training Loss: 0.1416020542383194\n",
      "Epoch 494/30000 Training Loss: 0.1417030692100525\n",
      "Epoch 495/30000 Training Loss: 0.14717277884483337\n",
      "Epoch 496/30000 Training Loss: 0.16081863641738892\n",
      "Epoch 497/30000 Training Loss: 0.14624857902526855\n",
      "Epoch 498/30000 Training Loss: 0.12303739786148071\n",
      "Epoch 499/30000 Training Loss: 0.13722915947437286\n",
      "Epoch 500/30000 Training Loss: 0.12377606332302094\n",
      "Epoch 500/30000 Validation Loss: 0.1654987335205078\n",
      "Epoch 501/30000 Training Loss: 0.15513265132904053\n",
      "Epoch 502/30000 Training Loss: 0.1202174499630928\n",
      "Epoch 503/30000 Training Loss: 0.12916326522827148\n",
      "Epoch 504/30000 Training Loss: 0.13220933079719543\n",
      "Epoch 505/30000 Training Loss: 0.12193575501441956\n",
      "Epoch 506/30000 Training Loss: 0.11700792610645294\n",
      "Epoch 507/30000 Training Loss: 0.11439156532287598\n",
      "Epoch 508/30000 Training Loss: 0.14413945376873016\n",
      "Epoch 509/30000 Training Loss: 0.14226150512695312\n",
      "Epoch 510/30000 Training Loss: 0.14322131872177124\n",
      "Epoch 511/30000 Training Loss: 0.1354144811630249\n",
      "Epoch 512/30000 Training Loss: 0.11907681822776794\n",
      "Epoch 513/30000 Training Loss: 0.13188624382019043\n",
      "Epoch 514/30000 Training Loss: 0.14079801738262177\n",
      "Epoch 515/30000 Training Loss: 0.16124148666858673\n",
      "Epoch 516/30000 Training Loss: 0.1483684629201889\n",
      "Epoch 517/30000 Training Loss: 0.13629452884197235\n",
      "Epoch 518/30000 Training Loss: 0.117298424243927\n",
      "Epoch 519/30000 Training Loss: 0.1288253217935562\n",
      "Epoch 520/30000 Training Loss: 0.1349915862083435\n",
      "Epoch 521/30000 Training Loss: 0.1377333104610443\n",
      "Epoch 522/30000 Training Loss: 0.12948505580425262\n",
      "Epoch 523/30000 Training Loss: 0.1371772140264511\n",
      "Epoch 524/30000 Training Loss: 0.12985241413116455\n",
      "Epoch 525/30000 Training Loss: 0.13072745501995087\n",
      "Epoch 526/30000 Training Loss: 0.16518761217594147\n",
      "Epoch 527/30000 Training Loss: 0.14245596528053284\n",
      "Epoch 528/30000 Training Loss: 0.1264876276254654\n",
      "Epoch 529/30000 Training Loss: 0.1451570838689804\n",
      "Epoch 530/30000 Training Loss: 0.13845296204090118\n",
      "Epoch 531/30000 Training Loss: 0.14118073880672455\n",
      "Epoch 532/30000 Training Loss: 0.12378550320863724\n",
      "Epoch 533/30000 Training Loss: 0.1417652815580368\n",
      "Epoch 534/30000 Training Loss: 0.1264621615409851\n",
      "Epoch 535/30000 Training Loss: 0.11964520812034607\n",
      "Epoch 536/30000 Training Loss: 0.134235680103302\n",
      "Epoch 537/30000 Training Loss: 0.12521371245384216\n",
      "Epoch 538/30000 Training Loss: 0.12453062832355499\n",
      "Epoch 539/30000 Training Loss: 0.17239612340927124\n",
      "Epoch 540/30000 Training Loss: 0.12197323143482208\n",
      "Epoch 541/30000 Training Loss: 0.13650557398796082\n",
      "Epoch 542/30000 Training Loss: 0.12542885541915894\n",
      "Epoch 543/30000 Training Loss: 0.1423615664243698\n",
      "Epoch 544/30000 Training Loss: 0.12522172927856445\n",
      "Epoch 545/30000 Training Loss: 0.1594991683959961\n",
      "Epoch 546/30000 Training Loss: 0.13609251379966736\n",
      "Epoch 547/30000 Training Loss: 0.12907682359218597\n",
      "Epoch 548/30000 Training Loss: 0.13265547156333923\n",
      "Epoch 549/30000 Training Loss: 0.1235334649682045\n",
      "Epoch 550/30000 Training Loss: 0.12773947417736053\n",
      "Epoch 551/30000 Training Loss: 0.12981301546096802\n",
      "Epoch 552/30000 Training Loss: 0.11676794290542603\n",
      "Epoch 553/30000 Training Loss: 0.1356065720319748\n",
      "Epoch 554/30000 Training Loss: 0.12801200151443481\n",
      "Epoch 555/30000 Training Loss: 0.1255047470331192\n",
      "Epoch 556/30000 Training Loss: 0.132035493850708\n",
      "Epoch 557/30000 Training Loss: 0.10289540141820908\n",
      "Epoch 558/30000 Training Loss: 0.11655788123607635\n",
      "Epoch 559/30000 Training Loss: 0.12819787859916687\n",
      "Epoch 560/30000 Training Loss: 0.11983896791934967\n",
      "Epoch 561/30000 Training Loss: 0.13710184395313263\n",
      "Epoch 562/30000 Training Loss: 0.13759048283100128\n",
      "Epoch 563/30000 Training Loss: 0.13127928972244263\n",
      "Epoch 564/30000 Training Loss: 0.14508824050426483\n",
      "Epoch 565/30000 Training Loss: 0.12270449846982956\n",
      "Epoch 566/30000 Training Loss: 0.16164906322956085\n",
      "Epoch 567/30000 Training Loss: 0.11277943849563599\n",
      "Epoch 568/30000 Training Loss: 0.11320506036281586\n",
      "Epoch 569/30000 Training Loss: 0.14407409727573395\n",
      "Epoch 570/30000 Training Loss: 0.12374112755060196\n",
      "Epoch 571/30000 Training Loss: 0.1407920867204666\n",
      "Epoch 572/30000 Training Loss: 0.13672995567321777\n",
      "Epoch 573/30000 Training Loss: 0.13546673953533173\n",
      "Epoch 574/30000 Training Loss: 0.1359984427690506\n",
      "Epoch 575/30000 Training Loss: 0.1132001057267189\n",
      "Epoch 576/30000 Training Loss: 0.12318320572376251\n",
      "Epoch 577/30000 Training Loss: 0.12343895435333252\n",
      "Epoch 578/30000 Training Loss: 0.13167691230773926\n",
      "Epoch 579/30000 Training Loss: 0.112117238342762\n",
      "Epoch 580/30000 Training Loss: 0.10984919965267181\n",
      "Epoch 581/30000 Training Loss: 0.11508692055940628\n",
      "Epoch 582/30000 Training Loss: 0.13201561570167542\n",
      "Epoch 583/30000 Training Loss: 0.122144915163517\n",
      "Epoch 584/30000 Training Loss: 0.1189938560128212\n",
      "Epoch 585/30000 Training Loss: 0.13394838571548462\n",
      "Epoch 586/30000 Training Loss: 0.0979652926325798\n",
      "Epoch 587/30000 Training Loss: 0.09667722135782242\n",
      "Epoch 588/30000 Training Loss: 0.10671225190162659\n",
      "Epoch 589/30000 Training Loss: 0.13364189863204956\n",
      "Epoch 590/30000 Training Loss: 0.12573449313640594\n",
      "Epoch 591/30000 Training Loss: 0.1293085515499115\n",
      "Epoch 592/30000 Training Loss: 0.11977549642324448\n",
      "Epoch 593/30000 Training Loss: 0.1194697692990303\n",
      "Epoch 594/30000 Training Loss: 0.10371632128953934\n",
      "Epoch 595/30000 Training Loss: 0.12017466127872467\n",
      "Epoch 596/30000 Training Loss: 0.11620648205280304\n",
      "Epoch 597/30000 Training Loss: 0.1354506015777588\n",
      "Epoch 598/30000 Training Loss: 0.10810701549053192\n",
      "Epoch 599/30000 Training Loss: 0.12451837956905365\n",
      "Epoch 600/30000 Training Loss: 0.1145583838224411\n",
      "Epoch 600/30000 Validation Loss: 0.1272963434457779\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.1272963434457779<=============\n",
      "Epoch 601/30000 Training Loss: 0.10870836675167084\n",
      "Epoch 602/30000 Training Loss: 0.12930139899253845\n",
      "Epoch 603/30000 Training Loss: 0.11301624774932861\n",
      "Epoch 604/30000 Training Loss: 0.1256604939699173\n",
      "Epoch 605/30000 Training Loss: 0.12532491981983185\n",
      "Epoch 606/30000 Training Loss: 0.12515535950660706\n",
      "Epoch 607/30000 Training Loss: 0.13004161417484283\n",
      "Epoch 608/30000 Training Loss: 0.12621450424194336\n",
      "Epoch 609/30000 Training Loss: 0.11619614064693451\n",
      "Epoch 610/30000 Training Loss: 0.11307181417942047\n",
      "Epoch 611/30000 Training Loss: 0.13103362917900085\n",
      "Epoch 612/30000 Training Loss: 0.12293930351734161\n",
      "Epoch 613/30000 Training Loss: 0.11386531591415405\n",
      "Epoch 614/30000 Training Loss: 0.11507825553417206\n",
      "Epoch 615/30000 Training Loss: 0.09724036604166031\n",
      "Epoch 616/30000 Training Loss: 0.12177447974681854\n",
      "Epoch 617/30000 Training Loss: 0.1202298253774643\n",
      "Epoch 618/30000 Training Loss: 0.11258144676685333\n",
      "Epoch 619/30000 Training Loss: 0.11542873084545135\n",
      "Epoch 620/30000 Training Loss: 0.10825726389884949\n",
      "Epoch 621/30000 Training Loss: 0.11419700086116791\n",
      "Epoch 622/30000 Training Loss: 0.10645763576030731\n",
      "Epoch 623/30000 Training Loss: 0.12299428135156631\n",
      "Epoch 624/30000 Training Loss: 0.10934285074472427\n",
      "Epoch 625/30000 Training Loss: 0.11644908785820007\n",
      "Epoch 626/30000 Training Loss: 0.09534943848848343\n",
      "Epoch 627/30000 Training Loss: 0.13588203489780426\n",
      "Epoch 628/30000 Training Loss: 0.11797899007797241\n",
      "Epoch 629/30000 Training Loss: 0.11135418713092804\n",
      "Epoch 630/30000 Training Loss: 0.13638831675052643\n",
      "Epoch 631/30000 Training Loss: 0.11052323877811432\n",
      "Epoch 632/30000 Training Loss: 0.13549073040485382\n",
      "Epoch 633/30000 Training Loss: 0.12643742561340332\n",
      "Epoch 634/30000 Training Loss: 0.11608607321977615\n",
      "Epoch 635/30000 Training Loss: 0.11831861734390259\n",
      "Epoch 636/30000 Training Loss: 0.12816007435321808\n",
      "Epoch 637/30000 Training Loss: 0.12784697115421295\n",
      "Epoch 638/30000 Training Loss: 0.11095570772886276\n",
      "Epoch 639/30000 Training Loss: 0.11867772042751312\n",
      "Epoch 640/30000 Training Loss: 0.11631690710783005\n",
      "Epoch 641/30000 Training Loss: 0.13143202662467957\n",
      "Epoch 642/30000 Training Loss: 0.12195491790771484\n",
      "Epoch 643/30000 Training Loss: 0.11022843420505524\n",
      "Epoch 644/30000 Training Loss: 0.1175016313791275\n",
      "Epoch 645/30000 Training Loss: 0.1111595630645752\n",
      "Epoch 646/30000 Training Loss: 0.11119864135980606\n",
      "Epoch 647/30000 Training Loss: 0.10460108518600464\n",
      "Epoch 648/30000 Training Loss: 0.14093850553035736\n",
      "Epoch 649/30000 Training Loss: 0.11120297014713287\n",
      "Epoch 650/30000 Training Loss: 0.10543537884950638\n",
      "Epoch 651/30000 Training Loss: 0.11153926700353622\n",
      "Epoch 652/30000 Training Loss: 0.13374461233615875\n",
      "Epoch 653/30000 Training Loss: 0.12035629153251648\n",
      "Epoch 654/30000 Training Loss: 0.12130588293075562\n",
      "Epoch 655/30000 Training Loss: 0.1258687824010849\n",
      "Epoch 656/30000 Training Loss: 0.11493059992790222\n",
      "Epoch 657/30000 Training Loss: 0.11897952854633331\n",
      "Epoch 658/30000 Training Loss: 0.11450935155153275\n",
      "Epoch 659/30000 Training Loss: 0.11519019305706024\n",
      "Epoch 660/30000 Training Loss: 0.11321835964918137\n",
      "Epoch 661/30000 Training Loss: 0.10828210413455963\n",
      "Epoch 662/30000 Training Loss: 0.13173934817314148\n",
      "Epoch 663/30000 Training Loss: 0.13458934426307678\n",
      "Epoch 664/30000 Training Loss: 0.09957589954137802\n",
      "Epoch 665/30000 Training Loss: 0.11730863898992538\n",
      "Epoch 666/30000 Training Loss: 0.12564721703529358\n",
      "Epoch 667/30000 Training Loss: 0.1062176376581192\n",
      "Epoch 668/30000 Training Loss: 0.11297916620969772\n",
      "Epoch 669/30000 Training Loss: 0.11642444878816605\n",
      "Epoch 670/30000 Training Loss: 0.11958128958940506\n",
      "Epoch 671/30000 Training Loss: 0.11407593637704849\n",
      "Epoch 672/30000 Training Loss: 0.11874312162399292\n",
      "Epoch 673/30000 Training Loss: 0.1247771829366684\n",
      "Epoch 674/30000 Training Loss: 0.13454897701740265\n",
      "Epoch 675/30000 Training Loss: 0.12214422225952148\n",
      "Epoch 676/30000 Training Loss: 0.11225007474422455\n",
      "Epoch 677/30000 Training Loss: 0.10150158405303955\n",
      "Epoch 678/30000 Training Loss: 0.10380446910858154\n",
      "Epoch 679/30000 Training Loss: 0.11324842274188995\n",
      "Epoch 680/30000 Training Loss: 0.10318709909915924\n",
      "Epoch 681/30000 Training Loss: 0.11142736673355103\n",
      "Epoch 682/30000 Training Loss: 0.11340256035327911\n",
      "Epoch 683/30000 Training Loss: 0.11550962179899216\n",
      "Epoch 684/30000 Training Loss: 0.12358997762203217\n",
      "Epoch 685/30000 Training Loss: 0.11620403081178665\n",
      "Epoch 686/30000 Training Loss: 0.10184074938297272\n",
      "Epoch 687/30000 Training Loss: 0.11222538352012634\n",
      "Epoch 688/30000 Training Loss: 0.0997769758105278\n",
      "Epoch 689/30000 Training Loss: 0.11221389472484589\n",
      "Epoch 690/30000 Training Loss: 0.10840129107236862\n",
      "Epoch 691/30000 Training Loss: 0.1104249507188797\n",
      "Epoch 692/30000 Training Loss: 0.12214496731758118\n",
      "Epoch 693/30000 Training Loss: 0.10788505524396896\n",
      "Epoch 694/30000 Training Loss: 0.08596638590097427\n",
      "Epoch 695/30000 Training Loss: 0.12576210498809814\n",
      "Epoch 696/30000 Training Loss: 0.10404331982135773\n",
      "Epoch 697/30000 Training Loss: 0.10706955194473267\n",
      "Epoch 698/30000 Training Loss: 0.11256816238164902\n",
      "Epoch 699/30000 Training Loss: 0.12744168937206268\n",
      "Epoch 700/30000 Training Loss: 0.11467888951301575\n",
      "Epoch 700/30000 Validation Loss: 0.11906889826059341\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.11906889826059341<=============\n",
      "Epoch 701/30000 Training Loss: 0.10115858912467957\n",
      "Epoch 702/30000 Training Loss: 0.09463485330343246\n",
      "Epoch 703/30000 Training Loss: 0.12267453223466873\n",
      "Epoch 704/30000 Training Loss: 0.12709501385688782\n",
      "Epoch 705/30000 Training Loss: 0.11780349910259247\n",
      "Epoch 706/30000 Training Loss: 0.12486899644136429\n",
      "Epoch 707/30000 Training Loss: 0.10294251143932343\n",
      "Epoch 708/30000 Training Loss: 0.10982514172792435\n",
      "Epoch 709/30000 Training Loss: 0.11971721053123474\n",
      "Epoch 710/30000 Training Loss: 0.10902929306030273\n",
      "Epoch 711/30000 Training Loss: 0.10226652026176453\n",
      "Epoch 712/30000 Training Loss: 0.12378916889429092\n",
      "Epoch 713/30000 Training Loss: 0.11350208520889282\n",
      "Epoch 714/30000 Training Loss: 0.11045053601264954\n",
      "Epoch 715/30000 Training Loss: 0.0944356620311737\n",
      "Epoch 716/30000 Training Loss: 0.11309465765953064\n",
      "Epoch 717/30000 Training Loss: 0.11060792207717896\n",
      "Epoch 718/30000 Training Loss: 0.10546216368675232\n",
      "Epoch 719/30000 Training Loss: 0.11295660585165024\n",
      "Epoch 720/30000 Training Loss: 0.10359328985214233\n",
      "Epoch 721/30000 Training Loss: 0.11293487250804901\n",
      "Epoch 722/30000 Training Loss: 0.10601789504289627\n",
      "Epoch 723/30000 Training Loss: 0.13122119009494781\n",
      "Epoch 724/30000 Training Loss: 0.10635849833488464\n",
      "Epoch 725/30000 Training Loss: 0.09884065389633179\n",
      "Epoch 726/30000 Training Loss: 0.09261175245046616\n",
      "Epoch 727/30000 Training Loss: 0.12623074650764465\n",
      "Epoch 728/30000 Training Loss: 0.10585427284240723\n",
      "Epoch 729/30000 Training Loss: 0.1200835183262825\n",
      "Epoch 730/30000 Training Loss: 0.11200165748596191\n",
      "Epoch 731/30000 Training Loss: 0.11255672574043274\n",
      "Epoch 732/30000 Training Loss: 0.09441839903593063\n",
      "Epoch 733/30000 Training Loss: 0.11330973356962204\n",
      "Epoch 734/30000 Training Loss: 0.12112724781036377\n",
      "Epoch 735/30000 Training Loss: 0.10418453812599182\n",
      "Epoch 736/30000 Training Loss: 0.09118463099002838\n",
      "Epoch 737/30000 Training Loss: 0.12025542557239532\n",
      "Epoch 738/30000 Training Loss: 0.09996337443590164\n",
      "Epoch 739/30000 Training Loss: 0.11024133116006851\n",
      "Epoch 740/30000 Training Loss: 0.10637781769037247\n",
      "Epoch 741/30000 Training Loss: 0.11666933447122574\n",
      "Epoch 742/30000 Training Loss: 0.12535560131072998\n",
      "Epoch 743/30000 Training Loss: 0.12140502780675888\n",
      "Epoch 744/30000 Training Loss: 0.12627461552619934\n",
      "Epoch 745/30000 Training Loss: 0.09074687212705612\n",
      "Epoch 746/30000 Training Loss: 0.09114549309015274\n",
      "Epoch 747/30000 Training Loss: 0.11129632592201233\n",
      "Epoch 748/30000 Training Loss: 0.10158970952033997\n",
      "Epoch 749/30000 Training Loss: 0.11389067769050598\n",
      "Epoch 750/30000 Training Loss: 0.11446545273065567\n",
      "Epoch 751/30000 Training Loss: 0.09217295050621033\n",
      "Epoch 752/30000 Training Loss: 0.10695618391036987\n",
      "Epoch 753/30000 Training Loss: 0.11231832206249237\n",
      "Epoch 754/30000 Training Loss: 0.10524699091911316\n",
      "Epoch 755/30000 Training Loss: 0.09901414066553116\n",
      "Epoch 756/30000 Training Loss: 0.10379114747047424\n",
      "Epoch 757/30000 Training Loss: 0.12443925440311432\n",
      "Epoch 758/30000 Training Loss: 0.09978310763835907\n",
      "Epoch 759/30000 Training Loss: 0.10258965194225311\n",
      "Epoch 760/30000 Training Loss: 0.10094769299030304\n",
      "Epoch 761/30000 Training Loss: 0.11121005564928055\n",
      "Epoch 762/30000 Training Loss: 0.12252704799175262\n",
      "Epoch 763/30000 Training Loss: 0.1152690052986145\n",
      "Epoch 764/30000 Training Loss: 0.09795964509248734\n",
      "Epoch 765/30000 Training Loss: 0.09647522866725922\n",
      "Epoch 766/30000 Training Loss: 0.12258297950029373\n",
      "Epoch 767/30000 Training Loss: 0.10528553277254105\n",
      "Epoch 768/30000 Training Loss: 0.09902937710285187\n",
      "Epoch 769/30000 Training Loss: 0.10310852527618408\n",
      "Epoch 770/30000 Training Loss: 0.10332143306732178\n",
      "Epoch 771/30000 Training Loss: 0.1050509437918663\n",
      "Epoch 772/30000 Training Loss: 0.09624576568603516\n",
      "Epoch 773/30000 Training Loss: 0.11747458577156067\n",
      "Epoch 774/30000 Training Loss: 0.11775285005569458\n",
      "Epoch 775/30000 Training Loss: 0.11331930756568909\n",
      "Epoch 776/30000 Training Loss: 0.08625534176826477\n",
      "Epoch 777/30000 Training Loss: 0.08760538697242737\n",
      "Epoch 778/30000 Training Loss: 0.08399758487939835\n",
      "Epoch 779/30000 Training Loss: 0.10818126797676086\n",
      "Epoch 780/30000 Training Loss: 0.12195365130901337\n",
      "Epoch 781/30000 Training Loss: 0.11510474979877472\n",
      "Epoch 782/30000 Training Loss: 0.10482292622327805\n",
      "Epoch 783/30000 Training Loss: 0.11985093355178833\n",
      "Epoch 784/30000 Training Loss: 0.09020161628723145\n",
      "Epoch 785/30000 Training Loss: 0.10351695865392685\n",
      "Epoch 786/30000 Training Loss: 0.08965113759040833\n",
      "Epoch 787/30000 Training Loss: 0.09698662161827087\n",
      "Epoch 788/30000 Training Loss: 0.09479692578315735\n",
      "Epoch 789/30000 Training Loss: 0.09749642014503479\n",
      "Epoch 790/30000 Training Loss: 0.10481791198253632\n",
      "Epoch 791/30000 Training Loss: 0.10491718351840973\n",
      "Epoch 792/30000 Training Loss: 0.10235350579023361\n",
      "Epoch 793/30000 Training Loss: 0.12269285321235657\n",
      "Epoch 794/30000 Training Loss: 0.1096181571483612\n",
      "Epoch 795/30000 Training Loss: 0.09901271760463715\n",
      "Epoch 796/30000 Training Loss: 0.09517382830381393\n",
      "Epoch 797/30000 Training Loss: 0.09945458918809891\n",
      "Epoch 798/30000 Training Loss: 0.12613651156425476\n",
      "Epoch 799/30000 Training Loss: 0.1187499389052391\n",
      "Epoch 800/30000 Training Loss: 0.1143648773431778\n",
      "Epoch 800/30000 Validation Loss: 0.1072496622800827\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.1072496622800827<=============\n",
      "Epoch 801/30000 Training Loss: 0.10022727400064468\n",
      "Epoch 802/30000 Training Loss: 0.10242172330617905\n",
      "Epoch 803/30000 Training Loss: 0.1142137199640274\n",
      "Epoch 804/30000 Training Loss: 0.11909068375825882\n",
      "Epoch 805/30000 Training Loss: 0.10699285566806793\n",
      "Epoch 806/30000 Training Loss: 0.09625132381916046\n",
      "Epoch 807/30000 Training Loss: 0.10350789874792099\n",
      "Epoch 808/30000 Training Loss: 0.10951099544763565\n",
      "Epoch 809/30000 Training Loss: 0.12544570863246918\n",
      "Epoch 810/30000 Training Loss: 0.10020845383405685\n",
      "Epoch 811/30000 Training Loss: 0.09634194523096085\n",
      "Epoch 812/30000 Training Loss: 0.12364751100540161\n",
      "Epoch 813/30000 Training Loss: 0.10716919600963593\n",
      "Epoch 814/30000 Training Loss: 0.1002182736992836\n",
      "Epoch 815/30000 Training Loss: 0.12059180438518524\n",
      "Epoch 816/30000 Training Loss: 0.1059262752532959\n",
      "Epoch 817/30000 Training Loss: 0.11322489380836487\n",
      "Epoch 818/30000 Training Loss: 0.10755666345357895\n",
      "Epoch 819/30000 Training Loss: 0.09221794456243515\n",
      "Epoch 820/30000 Training Loss: 0.09420792758464813\n",
      "Epoch 821/30000 Training Loss: 0.1124749556183815\n",
      "Epoch 822/30000 Training Loss: 0.10510622709989548\n",
      "Epoch 823/30000 Training Loss: 0.09497103840112686\n",
      "Epoch 824/30000 Training Loss: 0.09406314790248871\n",
      "Epoch 825/30000 Training Loss: 0.1021171435713768\n",
      "Epoch 826/30000 Training Loss: 0.12333032488822937\n",
      "Epoch 827/30000 Training Loss: 0.12552927434444427\n",
      "Epoch 828/30000 Training Loss: 0.1124144047498703\n",
      "Epoch 829/30000 Training Loss: 0.11339239031076431\n",
      "Epoch 830/30000 Training Loss: 0.10418187826871872\n",
      "Epoch 831/30000 Training Loss: 0.08858432620763779\n",
      "Epoch 832/30000 Training Loss: 0.11279089748859406\n",
      "Epoch 833/30000 Training Loss: 0.11760786175727844\n",
      "Epoch 834/30000 Training Loss: 0.10489068925380707\n",
      "Epoch 835/30000 Training Loss: 0.09971176832914352\n",
      "Epoch 836/30000 Training Loss: 0.09116914868354797\n",
      "Epoch 837/30000 Training Loss: 0.08539324253797531\n",
      "Epoch 838/30000 Training Loss: 0.09091099351644516\n",
      "Epoch 839/30000 Training Loss: 0.10140033066272736\n",
      "Epoch 840/30000 Training Loss: 0.08621823042631149\n",
      "Epoch 841/30000 Training Loss: 0.12487602233886719\n",
      "Epoch 842/30000 Training Loss: 0.10352049767971039\n",
      "Epoch 843/30000 Training Loss: 0.10455325245857239\n",
      "Epoch 844/30000 Training Loss: 0.10164576768875122\n",
      "Epoch 845/30000 Training Loss: 0.10648341476917267\n",
      "Epoch 846/30000 Training Loss: 0.09331538528203964\n",
      "Epoch 847/30000 Training Loss: 0.1048339456319809\n",
      "Epoch 848/30000 Training Loss: 0.09240763634443283\n",
      "Epoch 849/30000 Training Loss: 0.11911103129386902\n",
      "Epoch 850/30000 Training Loss: 0.09592235088348389\n",
      "Epoch 851/30000 Training Loss: 0.10521791875362396\n",
      "Epoch 852/30000 Training Loss: 0.09073112159967422\n",
      "Epoch 853/30000 Training Loss: 0.1114148497581482\n",
      "Epoch 854/30000 Training Loss: 0.1102265864610672\n",
      "Epoch 855/30000 Training Loss: 0.10990159213542938\n",
      "Epoch 856/30000 Training Loss: 0.10777359455823898\n",
      "Epoch 857/30000 Training Loss: 0.11339341104030609\n",
      "Epoch 858/30000 Training Loss: 0.11814634501934052\n",
      "Epoch 859/30000 Training Loss: 0.08461247384548187\n",
      "Epoch 860/30000 Training Loss: 0.09597796201705933\n",
      "Epoch 861/30000 Training Loss: 0.09359492361545563\n",
      "Epoch 862/30000 Training Loss: 0.09231076389551163\n",
      "Epoch 863/30000 Training Loss: 0.09571843594312668\n",
      "Epoch 864/30000 Training Loss: 0.09190279245376587\n",
      "Epoch 865/30000 Training Loss: 0.0902138352394104\n",
      "Epoch 866/30000 Training Loss: 0.11633507907390594\n",
      "Epoch 867/30000 Training Loss: 0.11389818787574768\n",
      "Epoch 868/30000 Training Loss: 0.09095796942710876\n",
      "Epoch 869/30000 Training Loss: 0.09580669552087784\n",
      "Epoch 870/30000 Training Loss: 0.10612918436527252\n",
      "Epoch 871/30000 Training Loss: 0.1026175394654274\n",
      "Epoch 872/30000 Training Loss: 0.1031893789768219\n",
      "Epoch 873/30000 Training Loss: 0.11267394572496414\n",
      "Epoch 874/30000 Training Loss: 0.11336296051740646\n",
      "Epoch 875/30000 Training Loss: 0.08839038014411926\n",
      "Epoch 876/30000 Training Loss: 0.1080336719751358\n",
      "Epoch 877/30000 Training Loss: 0.12551943957805634\n",
      "Epoch 878/30000 Training Loss: 0.10026661306619644\n",
      "Epoch 879/30000 Training Loss: 0.11713741719722748\n",
      "Epoch 880/30000 Training Loss: 0.09604735672473907\n",
      "Epoch 881/30000 Training Loss: 0.09978083521127701\n",
      "Epoch 882/30000 Training Loss: 0.11532001942396164\n",
      "Epoch 883/30000 Training Loss: 0.08130251616239548\n",
      "Epoch 884/30000 Training Loss: 0.10083010047674179\n",
      "Epoch 885/30000 Training Loss: 0.10162412375211716\n",
      "Epoch 886/30000 Training Loss: 0.10140984505414963\n",
      "Epoch 887/30000 Training Loss: 0.11334913969039917\n",
      "Epoch 888/30000 Training Loss: 0.07831345498561859\n",
      "Epoch 889/30000 Training Loss: 0.09705860167741776\n",
      "Epoch 890/30000 Training Loss: 0.09894495457410812\n",
      "Epoch 891/30000 Training Loss: 0.08903220295906067\n",
      "Epoch 892/30000 Training Loss: 0.09312361478805542\n",
      "Epoch 893/30000 Training Loss: 0.09970713406801224\n",
      "Epoch 894/30000 Training Loss: 0.0953550711274147\n",
      "Epoch 895/30000 Training Loss: 0.086366668343544\n",
      "Epoch 896/30000 Training Loss: 0.09047774970531464\n",
      "Epoch 897/30000 Training Loss: 0.10558522492647171\n",
      "Epoch 898/30000 Training Loss: 0.1260719895362854\n",
      "Epoch 899/30000 Training Loss: 0.09650640189647675\n",
      "Epoch 900/30000 Training Loss: 0.09583393484354019\n",
      "Epoch 900/30000 Validation Loss: 0.09160662442445755\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09160662442445755<=============\n",
      "Epoch 901/30000 Training Loss: 0.09643736481666565\n",
      "Epoch 902/30000 Training Loss: 0.09698603302240372\n",
      "Epoch 903/30000 Training Loss: 0.11097902059555054\n",
      "Epoch 904/30000 Training Loss: 0.12079425901174545\n",
      "Epoch 905/30000 Training Loss: 0.0936836302280426\n",
      "Epoch 906/30000 Training Loss: 0.11892378330230713\n",
      "Epoch 907/30000 Training Loss: 0.10456952452659607\n",
      "Epoch 908/30000 Training Loss: 0.09446854889392853\n",
      "Epoch 909/30000 Training Loss: 0.08954664319753647\n",
      "Epoch 910/30000 Training Loss: 0.0882277712225914\n",
      "Epoch 911/30000 Training Loss: 0.09760037809610367\n",
      "Epoch 912/30000 Training Loss: 0.11466171592473984\n",
      "Epoch 913/30000 Training Loss: 0.10002008080482483\n",
      "Epoch 914/30000 Training Loss: 0.09189578145742416\n",
      "Epoch 915/30000 Training Loss: 0.09748987853527069\n",
      "Epoch 916/30000 Training Loss: 0.09157051146030426\n",
      "Epoch 917/30000 Training Loss: 0.1047665923833847\n",
      "Epoch 918/30000 Training Loss: 0.10254847258329391\n",
      "Epoch 919/30000 Training Loss: 0.10847263038158417\n",
      "Epoch 920/30000 Training Loss: 0.10367020219564438\n",
      "Epoch 921/30000 Training Loss: 0.09611807763576508\n",
      "Epoch 922/30000 Training Loss: 0.11184097826480865\n",
      "Epoch 923/30000 Training Loss: 0.09776397794485092\n",
      "Epoch 924/30000 Training Loss: 0.08925414830446243\n",
      "Epoch 925/30000 Training Loss: 0.09888699650764465\n",
      "Epoch 926/30000 Training Loss: 0.0775739848613739\n",
      "Epoch 927/30000 Training Loss: 0.1199265643954277\n",
      "Epoch 928/30000 Training Loss: 0.10427489876747131\n",
      "Epoch 929/30000 Training Loss: 0.07899150252342224\n",
      "Epoch 930/30000 Training Loss: 0.09846121072769165\n",
      "Epoch 931/30000 Training Loss: 0.09701986610889435\n",
      "Epoch 932/30000 Training Loss: 0.10902173072099686\n",
      "Epoch 933/30000 Training Loss: 0.09818707406520844\n",
      "Epoch 934/30000 Training Loss: 0.10718808323144913\n",
      "Epoch 935/30000 Training Loss: 0.10906977206468582\n",
      "Epoch 936/30000 Training Loss: 0.08937159925699234\n",
      "Epoch 937/30000 Training Loss: 0.10245116055011749\n",
      "Epoch 938/30000 Training Loss: 0.09478012472391129\n",
      "Epoch 939/30000 Training Loss: 0.09999647736549377\n",
      "Epoch 940/30000 Training Loss: 0.09251835942268372\n",
      "Epoch 941/30000 Training Loss: 0.09854026883840561\n",
      "Epoch 942/30000 Training Loss: 0.0955052375793457\n",
      "Epoch 943/30000 Training Loss: 0.11455772817134857\n",
      "Epoch 944/30000 Training Loss: 0.08525332808494568\n",
      "Epoch 945/30000 Training Loss: 0.09797714650630951\n",
      "Epoch 946/30000 Training Loss: 0.09675287455320358\n",
      "Epoch 947/30000 Training Loss: 0.10210585594177246\n",
      "Epoch 948/30000 Training Loss: 0.1016254723072052\n",
      "Epoch 949/30000 Training Loss: 0.10672707855701447\n",
      "Epoch 950/30000 Training Loss: 0.11550109833478928\n",
      "Epoch 951/30000 Training Loss: 0.11910389363765717\n",
      "Epoch 952/30000 Training Loss: 0.10935724526643753\n",
      "Epoch 953/30000 Training Loss: 0.10549971461296082\n",
      "Epoch 954/30000 Training Loss: 0.11336672306060791\n",
      "Epoch 955/30000 Training Loss: 0.09694615751504898\n",
      "Epoch 956/30000 Training Loss: 0.08566503971815109\n",
      "Epoch 957/30000 Training Loss: 0.11263301968574524\n",
      "Epoch 958/30000 Training Loss: 0.08433941006660461\n",
      "Epoch 959/30000 Training Loss: 0.09622476249933243\n",
      "Epoch 960/30000 Training Loss: 0.1029418483376503\n",
      "Epoch 961/30000 Training Loss: 0.10193516314029694\n",
      "Epoch 962/30000 Training Loss: 0.09853892028331757\n",
      "Epoch 963/30000 Training Loss: 0.09004171192646027\n",
      "Epoch 964/30000 Training Loss: 0.08070466667413712\n",
      "Epoch 965/30000 Training Loss: 0.07392993569374084\n",
      "Epoch 966/30000 Training Loss: 0.09670804440975189\n",
      "Epoch 967/30000 Training Loss: 0.09567531943321228\n",
      "Epoch 968/30000 Training Loss: 0.10354384034872055\n",
      "Epoch 969/30000 Training Loss: 0.10749511420726776\n",
      "Epoch 970/30000 Training Loss: 0.08597201853990555\n",
      "Epoch 971/30000 Training Loss: 0.08899318426847458\n",
      "Epoch 972/30000 Training Loss: 0.11626704037189484\n",
      "Epoch 973/30000 Training Loss: 0.10333823412656784\n",
      "Epoch 974/30000 Training Loss: 0.07797346264123917\n",
      "Epoch 975/30000 Training Loss: 0.08812275528907776\n",
      "Epoch 976/30000 Training Loss: 0.10721763968467712\n",
      "Epoch 977/30000 Training Loss: 0.09592893719673157\n",
      "Epoch 978/30000 Training Loss: 0.11017094552516937\n",
      "Epoch 979/30000 Training Loss: 0.09602651745080948\n",
      "Epoch 980/30000 Training Loss: 0.10113973915576935\n",
      "Epoch 981/30000 Training Loss: 0.08430317789316177\n",
      "Epoch 982/30000 Training Loss: 0.08350518345832825\n",
      "Epoch 983/30000 Training Loss: 0.10372273623943329\n",
      "Epoch 984/30000 Training Loss: 0.08458586782217026\n",
      "Epoch 985/30000 Training Loss: 0.10487449169158936\n",
      "Epoch 986/30000 Training Loss: 0.1116509735584259\n",
      "Epoch 987/30000 Training Loss: 0.09206747263669968\n",
      "Epoch 988/30000 Training Loss: 0.09134382754564285\n",
      "Epoch 989/30000 Training Loss: 0.08019348978996277\n",
      "Epoch 990/30000 Training Loss: 0.0971619263291359\n",
      "Epoch 991/30000 Training Loss: 0.10675227642059326\n",
      "Epoch 992/30000 Training Loss: 0.10137663781642914\n",
      "Epoch 993/30000 Training Loss: 0.10077586024999619\n",
      "Epoch 994/30000 Training Loss: 0.10737286508083344\n",
      "Epoch 995/30000 Training Loss: 0.09873645007610321\n",
      "Epoch 996/30000 Training Loss: 0.11688675731420517\n",
      "Epoch 997/30000 Training Loss: 0.08326978981494904\n",
      "Epoch 998/30000 Training Loss: 0.10048294067382812\n",
      "Epoch 999/30000 Training Loss: 0.10930384695529938\n",
      "Epoch 1000/30000 Training Loss: 0.10148398578166962\n",
      "Epoch 1000/30000 Validation Loss: 0.09038582444190979\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09038582444190979<=============\n",
      "Epoch 1001/30000 Training Loss: 0.10082991421222687\n",
      "Epoch 1002/30000 Training Loss: 0.09496352821588516\n",
      "Epoch 1003/30000 Training Loss: 0.0834955945611\n",
      "Epoch 1004/30000 Training Loss: 0.09294886887073517\n",
      "Epoch 1005/30000 Training Loss: 0.09012044221162796\n",
      "Epoch 1006/30000 Training Loss: 0.10165511071681976\n",
      "Epoch 1007/30000 Training Loss: 0.1035495325922966\n",
      "Epoch 1008/30000 Training Loss: 0.10760566592216492\n",
      "Epoch 1009/30000 Training Loss: 0.09571366757154465\n",
      "Epoch 1010/30000 Training Loss: 0.09590316563844681\n",
      "Epoch 1011/30000 Training Loss: 0.10107526928186417\n",
      "Epoch 1012/30000 Training Loss: 0.1245623379945755\n",
      "Epoch 1013/30000 Training Loss: 0.10075757652521133\n",
      "Epoch 1014/30000 Training Loss: 0.07445384562015533\n",
      "Epoch 1015/30000 Training Loss: 0.09557658433914185\n",
      "Epoch 1016/30000 Training Loss: 0.09214116632938385\n",
      "Epoch 1017/30000 Training Loss: 0.10205244272947311\n",
      "Epoch 1018/30000 Training Loss: 0.08223545551300049\n",
      "Epoch 1019/30000 Training Loss: 0.10733702778816223\n",
      "Epoch 1020/30000 Training Loss: 0.07365404814481735\n",
      "Epoch 1021/30000 Training Loss: 0.0932907983660698\n",
      "Epoch 1022/30000 Training Loss: 0.09227293729782104\n",
      "Epoch 1023/30000 Training Loss: 0.10183316469192505\n",
      "Epoch 1024/30000 Training Loss: 0.11419577896595001\n",
      "Epoch 1025/30000 Training Loss: 0.10530790686607361\n",
      "Epoch 1026/30000 Training Loss: 0.09776206314563751\n",
      "Epoch 1027/30000 Training Loss: 0.10168592631816864\n",
      "Epoch 1028/30000 Training Loss: 0.1025945395231247\n",
      "Epoch 1029/30000 Training Loss: 0.10215796530246735\n",
      "Epoch 1030/30000 Training Loss: 0.08340589702129364\n",
      "Epoch 1031/30000 Training Loss: 0.09373872727155685\n",
      "Epoch 1032/30000 Training Loss: 0.08879866451025009\n",
      "Epoch 1033/30000 Training Loss: 0.08761148154735565\n",
      "Epoch 1034/30000 Training Loss: 0.09424826502799988\n",
      "Epoch 1035/30000 Training Loss: 0.10224467515945435\n",
      "Epoch 1036/30000 Training Loss: 0.1028459221124649\n",
      "Epoch 1037/30000 Training Loss: 0.08717629313468933\n",
      "Epoch 1038/30000 Training Loss: 0.08439502120018005\n",
      "Epoch 1039/30000 Training Loss: 0.08658181875944138\n",
      "Epoch 1040/30000 Training Loss: 0.0744442567229271\n",
      "Epoch 1041/30000 Training Loss: 0.09434284269809723\n",
      "Epoch 1042/30000 Training Loss: 0.06655053049325943\n",
      "Epoch 1043/30000 Training Loss: 0.10323341190814972\n",
      "Epoch 1044/30000 Training Loss: 0.0916924774646759\n",
      "Epoch 1045/30000 Training Loss: 0.08014210313558578\n",
      "Epoch 1046/30000 Training Loss: 0.08129579573869705\n",
      "Epoch 1047/30000 Training Loss: 0.10223890095949173\n",
      "Epoch 1048/30000 Training Loss: 0.09557916224002838\n",
      "Epoch 1049/30000 Training Loss: 0.09989452362060547\n",
      "Epoch 1050/30000 Training Loss: 0.091769278049469\n",
      "Epoch 1051/30000 Training Loss: 0.09830515086650848\n",
      "Epoch 1052/30000 Training Loss: 0.1072835624217987\n",
      "Epoch 1053/30000 Training Loss: 0.11891502887010574\n",
      "Epoch 1054/30000 Training Loss: 0.07978610694408417\n",
      "Epoch 1055/30000 Training Loss: 0.1005958616733551\n",
      "Epoch 1056/30000 Training Loss: 0.08071837574243546\n",
      "Epoch 1057/30000 Training Loss: 0.08043074607849121\n",
      "Epoch 1058/30000 Training Loss: 0.0935511440038681\n",
      "Epoch 1059/30000 Training Loss: 0.10786689817905426\n",
      "Epoch 1060/30000 Training Loss: 0.09520727396011353\n",
      "Epoch 1061/30000 Training Loss: 0.07962582260370255\n",
      "Epoch 1062/30000 Training Loss: 0.08596678078174591\n",
      "Epoch 1063/30000 Training Loss: 0.09405560791492462\n",
      "Epoch 1064/30000 Training Loss: 0.07816606760025024\n",
      "Epoch 1065/30000 Training Loss: 0.11261042207479477\n",
      "Epoch 1066/30000 Training Loss: 0.09338359534740448\n",
      "Epoch 1067/30000 Training Loss: 0.08095481991767883\n",
      "Epoch 1068/30000 Training Loss: 0.08512148261070251\n",
      "Epoch 1069/30000 Training Loss: 0.11154621094465256\n",
      "Epoch 1070/30000 Training Loss: 0.06968054175376892\n",
      "Epoch 1071/30000 Training Loss: 0.09967339783906937\n",
      "Epoch 1072/30000 Training Loss: 0.09619736671447754\n",
      "Epoch 1073/30000 Training Loss: 0.08485858887434006\n",
      "Epoch 1074/30000 Training Loss: 0.08906201273202896\n",
      "Epoch 1075/30000 Training Loss: 0.08489050716161728\n",
      "Epoch 1076/30000 Training Loss: 0.11813583970069885\n",
      "Epoch 1077/30000 Training Loss: 0.07653874158859253\n",
      "Epoch 1078/30000 Training Loss: 0.10867398977279663\n",
      "Epoch 1079/30000 Training Loss: 0.08320406079292297\n",
      "Epoch 1080/30000 Training Loss: 0.10749486833810806\n",
      "Epoch 1081/30000 Training Loss: 0.10111106932163239\n",
      "Epoch 1082/30000 Training Loss: 0.09468519687652588\n",
      "Epoch 1083/30000 Training Loss: 0.08742417395114899\n",
      "Epoch 1084/30000 Training Loss: 0.11405569314956665\n",
      "Epoch 1085/30000 Training Loss: 0.0783027783036232\n",
      "Epoch 1086/30000 Training Loss: 0.11333878338336945\n",
      "Epoch 1087/30000 Training Loss: 0.10045027732849121\n",
      "Epoch 1088/30000 Training Loss: 0.08371647447347641\n",
      "Epoch 1089/30000 Training Loss: 0.09383117407560349\n",
      "Epoch 1090/30000 Training Loss: 0.06511608511209488\n",
      "Epoch 1091/30000 Training Loss: 0.09433366358280182\n",
      "Epoch 1092/30000 Training Loss: 0.10385704040527344\n",
      "Epoch 1093/30000 Training Loss: 0.10027278959751129\n",
      "Epoch 1094/30000 Training Loss: 0.0929611325263977\n",
      "Epoch 1095/30000 Training Loss: 0.09731314331293106\n",
      "Epoch 1096/30000 Training Loss: 0.08259814977645874\n",
      "Epoch 1097/30000 Training Loss: 0.06730426847934723\n",
      "Epoch 1098/30000 Training Loss: 0.10582742840051651\n",
      "Epoch 1099/30000 Training Loss: 0.08961828798055649\n",
      "Epoch 1100/30000 Training Loss: 0.10480780899524689\n",
      "Epoch 1100/30000 Validation Loss: 0.09494137763977051\n",
      "Epoch 1101/30000 Training Loss: 0.0857030525803566\n",
      "Epoch 1102/30000 Training Loss: 0.10906015336513519\n",
      "Epoch 1103/30000 Training Loss: 0.07848193496465683\n",
      "Epoch 1104/30000 Training Loss: 0.09355978667736053\n",
      "Epoch 1105/30000 Training Loss: 0.09125194698572159\n",
      "Epoch 1106/30000 Training Loss: 0.09481948614120483\n",
      "Epoch 1107/30000 Training Loss: 0.09956075251102448\n",
      "Epoch 1108/30000 Training Loss: 0.09724458307027817\n",
      "Epoch 1109/30000 Training Loss: 0.07689057290554047\n",
      "Epoch 1110/30000 Training Loss: 0.08230811357498169\n",
      "Epoch 1111/30000 Training Loss: 0.09975270181894302\n",
      "Epoch 1112/30000 Training Loss: 0.09104304015636444\n",
      "Epoch 1113/30000 Training Loss: 0.10260641574859619\n",
      "Epoch 1114/30000 Training Loss: 0.09161999076604843\n",
      "Epoch 1115/30000 Training Loss: 0.10095953941345215\n",
      "Epoch 1116/30000 Training Loss: 0.10516607761383057\n",
      "Epoch 1117/30000 Training Loss: 0.10858695209026337\n",
      "Epoch 1118/30000 Training Loss: 0.08418640494346619\n",
      "Epoch 1119/30000 Training Loss: 0.08160476386547089\n",
      "Epoch 1120/30000 Training Loss: 0.10270745307207108\n",
      "Epoch 1121/30000 Training Loss: 0.095589280128479\n",
      "Epoch 1122/30000 Training Loss: 0.09362462162971497\n",
      "Epoch 1123/30000 Training Loss: 0.08430401980876923\n",
      "Epoch 1124/30000 Training Loss: 0.07939618825912476\n",
      "Epoch 1125/30000 Training Loss: 0.09193290770053864\n",
      "Epoch 1126/30000 Training Loss: 0.1091470718383789\n",
      "Epoch 1127/30000 Training Loss: 0.08792490512132645\n",
      "Epoch 1128/30000 Training Loss: 0.07301795482635498\n",
      "Epoch 1129/30000 Training Loss: 0.10989835858345032\n",
      "Epoch 1130/30000 Training Loss: 0.09612701833248138\n",
      "Epoch 1131/30000 Training Loss: 0.09946116805076599\n",
      "Epoch 1132/30000 Training Loss: 0.08734587579965591\n",
      "Epoch 1133/30000 Training Loss: 0.11157304048538208\n",
      "Epoch 1134/30000 Training Loss: 0.10642839223146439\n",
      "Epoch 1135/30000 Training Loss: 0.08141938596963882\n",
      "Epoch 1136/30000 Training Loss: 0.08451659977436066\n",
      "Epoch 1137/30000 Training Loss: 0.0868731290102005\n",
      "Epoch 1138/30000 Training Loss: 0.09243186563253403\n",
      "Epoch 1139/30000 Training Loss: 0.09789150953292847\n",
      "Epoch 1140/30000 Training Loss: 0.07447700947523117\n",
      "Epoch 1141/30000 Training Loss: 0.09416031092405319\n",
      "Epoch 1142/30000 Training Loss: 0.09942005574703217\n",
      "Epoch 1143/30000 Training Loss: 0.1031562089920044\n",
      "Epoch 1144/30000 Training Loss: 0.08840271830558777\n",
      "Epoch 1145/30000 Training Loss: 0.09478466957807541\n",
      "Epoch 1146/30000 Training Loss: 0.08149702101945877\n",
      "Epoch 1147/30000 Training Loss: 0.10513972491025925\n",
      "Epoch 1148/30000 Training Loss: 0.0895397737622261\n",
      "Epoch 1149/30000 Training Loss: 0.0859747901558876\n",
      "Epoch 1150/30000 Training Loss: 0.09075260162353516\n",
      "Epoch 1151/30000 Training Loss: 0.09172064065933228\n",
      "Epoch 1152/30000 Training Loss: 0.09028573334217072\n",
      "Epoch 1153/30000 Training Loss: 0.0949157327413559\n",
      "Epoch 1154/30000 Training Loss: 0.08978024125099182\n",
      "Epoch 1155/30000 Training Loss: 0.0832560658454895\n",
      "Epoch 1156/30000 Training Loss: 0.09712190181016922\n",
      "Epoch 1157/30000 Training Loss: 0.08283120393753052\n",
      "Epoch 1158/30000 Training Loss: 0.09982071816921234\n",
      "Epoch 1159/30000 Training Loss: 0.07911960035562515\n",
      "Epoch 1160/30000 Training Loss: 0.0874723270535469\n",
      "Epoch 1161/30000 Training Loss: 0.0818384513258934\n",
      "Epoch 1162/30000 Training Loss: 0.08522152900695801\n",
      "Epoch 1163/30000 Training Loss: 0.08014091849327087\n",
      "Epoch 1164/30000 Training Loss: 0.10142052173614502\n",
      "Epoch 1165/30000 Training Loss: 0.09610176086425781\n",
      "Epoch 1166/30000 Training Loss: 0.09708990901708603\n",
      "Epoch 1167/30000 Training Loss: 0.100407674908638\n",
      "Epoch 1168/30000 Training Loss: 0.10247422754764557\n",
      "Epoch 1169/30000 Training Loss: 0.08680828660726547\n",
      "Epoch 1170/30000 Training Loss: 0.0838596522808075\n",
      "Epoch 1171/30000 Training Loss: 0.07239370048046112\n",
      "Epoch 1172/30000 Training Loss: 0.07828164100646973\n",
      "Epoch 1173/30000 Training Loss: 0.0848882794380188\n",
      "Epoch 1174/30000 Training Loss: 0.0973411351442337\n",
      "Epoch 1175/30000 Training Loss: 0.09772808104753494\n",
      "Epoch 1176/30000 Training Loss: 0.10543374717235565\n",
      "Epoch 1177/30000 Training Loss: 0.0954364687204361\n",
      "Epoch 1178/30000 Training Loss: 0.08809854090213776\n",
      "Epoch 1179/30000 Training Loss: 0.08712422847747803\n",
      "Epoch 1180/30000 Training Loss: 0.07181224226951599\n",
      "Epoch 1181/30000 Training Loss: 0.08160658925771713\n",
      "Epoch 1182/30000 Training Loss: 0.09440414607524872\n",
      "Epoch 1183/30000 Training Loss: 0.09281507879495621\n",
      "Epoch 1184/30000 Training Loss: 0.09841494262218475\n",
      "Epoch 1185/30000 Training Loss: 0.07275208830833435\n",
      "Epoch 1186/30000 Training Loss: 0.0883101299405098\n",
      "Epoch 1187/30000 Training Loss: 0.10008610785007477\n",
      "Epoch 1188/30000 Training Loss: 0.09001995623111725\n",
      "Epoch 1189/30000 Training Loss: 0.08860985934734344\n",
      "Epoch 1190/30000 Training Loss: 0.08268655091524124\n",
      "Epoch 1191/30000 Training Loss: 0.08339015394449234\n",
      "Epoch 1192/30000 Training Loss: 0.07648304104804993\n",
      "Epoch 1193/30000 Training Loss: 0.09037912636995316\n",
      "Epoch 1194/30000 Training Loss: 0.07805431634187698\n",
      "Epoch 1195/30000 Training Loss: 0.07426620274782181\n",
      "Epoch 1196/30000 Training Loss: 0.08262075483798981\n",
      "Epoch 1197/30000 Training Loss: 0.0820857509970665\n",
      "Epoch 1198/30000 Training Loss: 0.09190943092107773\n",
      "Epoch 1199/30000 Training Loss: 0.08759207278490067\n",
      "Epoch 1200/30000 Training Loss: 0.1018817126750946\n",
      "Epoch 1200/30000 Validation Loss: 0.08145260810852051\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08145260810852051<=============\n",
      "Epoch 1201/30000 Training Loss: 0.09243004024028778\n",
      "Epoch 1202/30000 Training Loss: 0.09419233351945877\n",
      "Epoch 1203/30000 Training Loss: 0.08906544744968414\n",
      "Epoch 1204/30000 Training Loss: 0.0843583196401596\n",
      "Epoch 1205/30000 Training Loss: 0.10275077074766159\n",
      "Epoch 1206/30000 Training Loss: 0.07511094957590103\n",
      "Epoch 1207/30000 Training Loss: 0.10292905569076538\n",
      "Epoch 1208/30000 Training Loss: 0.08677554875612259\n",
      "Epoch 1209/30000 Training Loss: 0.08446358144283295\n",
      "Epoch 1210/30000 Training Loss: 0.09492632746696472\n",
      "Epoch 1211/30000 Training Loss: 0.07999975234270096\n",
      "Epoch 1212/30000 Training Loss: 0.09711780399084091\n",
      "Epoch 1213/30000 Training Loss: 0.08941049873828888\n",
      "Epoch 1214/30000 Training Loss: 0.07975950092077255\n",
      "Epoch 1215/30000 Training Loss: 0.10705965757369995\n",
      "Epoch 1216/30000 Training Loss: 0.09358986467123032\n",
      "Epoch 1217/30000 Training Loss: 0.07870177924633026\n",
      "Epoch 1218/30000 Training Loss: 0.09152337163686752\n",
      "Epoch 1219/30000 Training Loss: 0.09488098323345184\n",
      "Epoch 1220/30000 Training Loss: 0.0807701051235199\n",
      "Epoch 1221/30000 Training Loss: 0.07950779795646667\n",
      "Epoch 1222/30000 Training Loss: 0.07333332300186157\n",
      "Epoch 1223/30000 Training Loss: 0.08595123887062073\n",
      "Epoch 1224/30000 Training Loss: 0.06974252313375473\n",
      "Epoch 1225/30000 Training Loss: 0.10164511948823929\n",
      "Epoch 1226/30000 Training Loss: 0.0939071774482727\n",
      "Epoch 1227/30000 Training Loss: 0.09286019951105118\n",
      "Epoch 1228/30000 Training Loss: 0.09521574527025223\n",
      "Epoch 1229/30000 Training Loss: 0.11296489089727402\n",
      "Epoch 1230/30000 Training Loss: 0.0869574248790741\n",
      "Epoch 1231/30000 Training Loss: 0.0953858494758606\n",
      "Epoch 1232/30000 Training Loss: 0.08897871524095535\n",
      "Epoch 1233/30000 Training Loss: 0.08206173777580261\n",
      "Epoch 1234/30000 Training Loss: 0.08339396119117737\n",
      "Epoch 1235/30000 Training Loss: 0.085970938205719\n",
      "Epoch 1236/30000 Training Loss: 0.09273183345794678\n",
      "Epoch 1237/30000 Training Loss: 0.10631163418292999\n",
      "Epoch 1238/30000 Training Loss: 0.0926879346370697\n",
      "Epoch 1239/30000 Training Loss: 0.0710422545671463\n",
      "Epoch 1240/30000 Training Loss: 0.12043645977973938\n",
      "Epoch 1241/30000 Training Loss: 0.08659198135137558\n",
      "Epoch 1242/30000 Training Loss: 0.09315316379070282\n",
      "Epoch 1243/30000 Training Loss: 0.08632244169712067\n",
      "Epoch 1244/30000 Training Loss: 0.08339743316173553\n",
      "Epoch 1245/30000 Training Loss: 0.08814745396375656\n",
      "Epoch 1246/30000 Training Loss: 0.09412460029125214\n",
      "Epoch 1247/30000 Training Loss: 0.06592290103435516\n",
      "Epoch 1248/30000 Training Loss: 0.0775446742773056\n",
      "Epoch 1249/30000 Training Loss: 0.06992783397436142\n",
      "Epoch 1250/30000 Training Loss: 0.08269783854484558\n",
      "Epoch 1251/30000 Training Loss: 0.0888691172003746\n",
      "Epoch 1252/30000 Training Loss: 0.08839606493711472\n",
      "Epoch 1253/30000 Training Loss: 0.08881757408380508\n",
      "Epoch 1254/30000 Training Loss: 0.0815916433930397\n",
      "Epoch 1255/30000 Training Loss: 0.0744420513510704\n",
      "Epoch 1256/30000 Training Loss: 0.09308567643165588\n",
      "Epoch 1257/30000 Training Loss: 0.10392927378416061\n",
      "Epoch 1258/30000 Training Loss: 0.07874497026205063\n",
      "Epoch 1259/30000 Training Loss: 0.09870058298110962\n",
      "Epoch 1260/30000 Training Loss: 0.10070539265871048\n",
      "Epoch 1261/30000 Training Loss: 0.07956452667713165\n",
      "Epoch 1262/30000 Training Loss: 0.08539695292711258\n",
      "Epoch 1263/30000 Training Loss: 0.07598579674959183\n",
      "Epoch 1264/30000 Training Loss: 0.09159642457962036\n",
      "Epoch 1265/30000 Training Loss: 0.07582208514213562\n",
      "Epoch 1266/30000 Training Loss: 0.0913611426949501\n",
      "Epoch 1267/30000 Training Loss: 0.09211789071559906\n",
      "Epoch 1268/30000 Training Loss: 0.0929761528968811\n",
      "Epoch 1269/30000 Training Loss: 0.0852840393781662\n",
      "Epoch 1270/30000 Training Loss: 0.08053848147392273\n",
      "Epoch 1271/30000 Training Loss: 0.07579999417066574\n",
      "Epoch 1272/30000 Training Loss: 0.10176631808280945\n",
      "Epoch 1273/30000 Training Loss: 0.07684061676263809\n",
      "Epoch 1274/30000 Training Loss: 0.09256088733673096\n",
      "Epoch 1275/30000 Training Loss: 0.0881066620349884\n",
      "Epoch 1276/30000 Training Loss: 0.09206430613994598\n",
      "Epoch 1277/30000 Training Loss: 0.08901149779558182\n",
      "Epoch 1278/30000 Training Loss: 0.09402051568031311\n",
      "Epoch 1279/30000 Training Loss: 0.08606764674186707\n",
      "Epoch 1280/30000 Training Loss: 0.10683032870292664\n",
      "Epoch 1281/30000 Training Loss: 0.0792652815580368\n",
      "Epoch 1282/30000 Training Loss: 0.07562095671892166\n",
      "Epoch 1283/30000 Training Loss: 0.08044775575399399\n",
      "Epoch 1284/30000 Training Loss: 0.09136500954627991\n",
      "Epoch 1285/30000 Training Loss: 0.09280715137720108\n",
      "Epoch 1286/30000 Training Loss: 0.08291961252689362\n",
      "Epoch 1287/30000 Training Loss: 0.08636493980884552\n",
      "Epoch 1288/30000 Training Loss: 0.10198327898979187\n",
      "Epoch 1289/30000 Training Loss: 0.08072030544281006\n",
      "Epoch 1290/30000 Training Loss: 0.09194925427436829\n",
      "Epoch 1291/30000 Training Loss: 0.07093656808137894\n",
      "Epoch 1292/30000 Training Loss: 0.09062923491001129\n",
      "Epoch 1293/30000 Training Loss: 0.08677884936332703\n",
      "Epoch 1294/30000 Training Loss: 0.0900791808962822\n",
      "Epoch 1295/30000 Training Loss: 0.08659482002258301\n",
      "Epoch 1296/30000 Training Loss: 0.09205658733844757\n",
      "Epoch 1297/30000 Training Loss: 0.09705232083797455\n",
      "Epoch 1298/30000 Training Loss: 0.08473651856184006\n",
      "Epoch 1299/30000 Training Loss: 0.08247560262680054\n",
      "Epoch 1300/30000 Training Loss: 0.06789502501487732\n",
      "Epoch 1300/30000 Validation Loss: 0.0763898640871048\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0763898640871048<=============\n",
      "Epoch 1301/30000 Training Loss: 0.08395615220069885\n",
      "Epoch 1302/30000 Training Loss: 0.08436211943626404\n",
      "Epoch 1303/30000 Training Loss: 0.0832434594631195\n",
      "Epoch 1304/30000 Training Loss: 0.08029182255268097\n",
      "Epoch 1305/30000 Training Loss: 0.10216966271400452\n",
      "Epoch 1306/30000 Training Loss: 0.08565273880958557\n",
      "Epoch 1307/30000 Training Loss: 0.09556671977043152\n",
      "Epoch 1308/30000 Training Loss: 0.08610205352306366\n",
      "Epoch 1309/30000 Training Loss: 0.08181843161582947\n",
      "Epoch 1310/30000 Training Loss: 0.09417988359928131\n",
      "Epoch 1311/30000 Training Loss: 0.08931568264961243\n",
      "Epoch 1312/30000 Training Loss: 0.10295937955379486\n",
      "Epoch 1313/30000 Training Loss: 0.09976375102996826\n",
      "Epoch 1314/30000 Training Loss: 0.10241874307394028\n",
      "Epoch 1315/30000 Training Loss: 0.08267991244792938\n",
      "Epoch 1316/30000 Training Loss: 0.1011342853307724\n",
      "Epoch 1317/30000 Training Loss: 0.08827479928731918\n",
      "Epoch 1318/30000 Training Loss: 0.08580681681632996\n",
      "Epoch 1319/30000 Training Loss: 0.08599717915058136\n",
      "Epoch 1320/30000 Training Loss: 0.08129407465457916\n",
      "Epoch 1321/30000 Training Loss: 0.07950101792812347\n",
      "Epoch 1322/30000 Training Loss: 0.07367099821567535\n",
      "Epoch 1323/30000 Training Loss: 0.07885335385799408\n",
      "Epoch 1324/30000 Training Loss: 0.0752926617860794\n",
      "Epoch 1325/30000 Training Loss: 0.1075369268655777\n",
      "Epoch 1326/30000 Training Loss: 0.079531729221344\n",
      "Epoch 1327/30000 Training Loss: 0.0882267951965332\n",
      "Epoch 1328/30000 Training Loss: 0.08929972350597382\n",
      "Epoch 1329/30000 Training Loss: 0.09438931196928024\n",
      "Epoch 1330/30000 Training Loss: 0.0941401869058609\n",
      "Epoch 1331/30000 Training Loss: 0.10686193406581879\n",
      "Epoch 1332/30000 Training Loss: 0.07317627966403961\n",
      "Epoch 1333/30000 Training Loss: 0.08903852105140686\n",
      "Epoch 1334/30000 Training Loss: 0.07944799214601517\n",
      "Epoch 1335/30000 Training Loss: 0.09246605634689331\n",
      "Epoch 1336/30000 Training Loss: 0.08419062197208405\n",
      "Epoch 1337/30000 Training Loss: 0.08254630118608475\n",
      "Epoch 1338/30000 Training Loss: 0.07921484112739563\n",
      "Epoch 1339/30000 Training Loss: 0.08282527327537537\n",
      "Epoch 1340/30000 Training Loss: 0.0739382728934288\n",
      "Epoch 1341/30000 Training Loss: 0.07161429524421692\n",
      "Epoch 1342/30000 Training Loss: 0.09747220575809479\n",
      "Epoch 1343/30000 Training Loss: 0.0811181366443634\n",
      "Epoch 1344/30000 Training Loss: 0.08192536234855652\n",
      "Epoch 1345/30000 Training Loss: 0.07669517397880554\n",
      "Epoch 1346/30000 Training Loss: 0.09924474358558655\n",
      "Epoch 1347/30000 Training Loss: 0.0867050513625145\n",
      "Epoch 1348/30000 Training Loss: 0.08673401921987534\n",
      "Epoch 1349/30000 Training Loss: 0.08301931619644165\n",
      "Epoch 1350/30000 Training Loss: 0.08044394105672836\n",
      "Epoch 1351/30000 Training Loss: 0.07126335799694061\n",
      "Epoch 1352/30000 Training Loss: 0.07814619690179825\n",
      "Epoch 1353/30000 Training Loss: 0.10081905871629715\n",
      "Epoch 1354/30000 Training Loss: 0.07507601380348206\n",
      "Epoch 1355/30000 Training Loss: 0.08869228512048721\n",
      "Epoch 1356/30000 Training Loss: 0.08307485282421112\n",
      "Epoch 1357/30000 Training Loss: 0.08265858888626099\n",
      "Epoch 1358/30000 Training Loss: 0.08916176855564117\n",
      "Epoch 1359/30000 Training Loss: 0.08482719957828522\n",
      "Epoch 1360/30000 Training Loss: 0.10060339421033859\n",
      "Epoch 1361/30000 Training Loss: 0.08007709681987762\n",
      "Epoch 1362/30000 Training Loss: 0.08747521787881851\n",
      "Epoch 1363/30000 Training Loss: 0.07964980602264404\n",
      "Epoch 1364/30000 Training Loss: 0.10558030009269714\n",
      "Epoch 1365/30000 Training Loss: 0.09415380656719208\n",
      "Epoch 1366/30000 Training Loss: 0.10137665271759033\n",
      "Epoch 1367/30000 Training Loss: 0.08529356867074966\n",
      "Epoch 1368/30000 Training Loss: 0.11281520873308182\n",
      "Epoch 1369/30000 Training Loss: 0.08476139605045319\n",
      "Epoch 1370/30000 Training Loss: 0.10010157525539398\n",
      "Epoch 1371/30000 Training Loss: 0.07747676968574524\n",
      "Epoch 1372/30000 Training Loss: 0.10350543260574341\n",
      "Epoch 1373/30000 Training Loss: 0.07261402904987335\n",
      "Epoch 1374/30000 Training Loss: 0.08633332699537277\n",
      "Epoch 1375/30000 Training Loss: 0.09203695505857468\n",
      "Epoch 1376/30000 Training Loss: 0.08459825813770294\n",
      "Epoch 1377/30000 Training Loss: 0.09796404093503952\n",
      "Epoch 1378/30000 Training Loss: 0.08396980911493301\n",
      "Epoch 1379/30000 Training Loss: 0.08140002191066742\n",
      "Epoch 1380/30000 Training Loss: 0.09703842550516129\n",
      "Epoch 1381/30000 Training Loss: 0.0799085795879364\n",
      "Epoch 1382/30000 Training Loss: 0.07132824510335922\n",
      "Epoch 1383/30000 Training Loss: 0.07292962819337845\n",
      "Epoch 1384/30000 Training Loss: 0.09203511476516724\n",
      "Epoch 1385/30000 Training Loss: 0.08142726868391037\n",
      "Epoch 1386/30000 Training Loss: 0.08782093226909637\n",
      "Epoch 1387/30000 Training Loss: 0.06622641533613205\n",
      "Epoch 1388/30000 Training Loss: 0.1093740314245224\n",
      "Epoch 1389/30000 Training Loss: 0.08096703141927719\n",
      "Epoch 1390/30000 Training Loss: 0.09914447367191315\n",
      "Epoch 1391/30000 Training Loss: 0.07687023282051086\n",
      "Epoch 1392/30000 Training Loss: 0.09914141893386841\n",
      "Epoch 1393/30000 Training Loss: 0.09490662813186646\n",
      "Epoch 1394/30000 Training Loss: 0.07800993323326111\n",
      "Epoch 1395/30000 Training Loss: 0.10625118017196655\n",
      "Epoch 1396/30000 Training Loss: 0.061744943261146545\n",
      "Epoch 1397/30000 Training Loss: 0.08333918452262878\n",
      "Epoch 1398/30000 Training Loss: 0.07634803652763367\n",
      "Epoch 1399/30000 Training Loss: 0.08406167477369308\n",
      "Epoch 1400/30000 Training Loss: 0.08599427342414856\n",
      "Epoch 1400/30000 Validation Loss: 0.08158346265554428\n",
      "Epoch 1401/30000 Training Loss: 0.06363433599472046\n",
      "Epoch 1402/30000 Training Loss: 0.08994996547698975\n",
      "Epoch 1403/30000 Training Loss: 0.09235239773988724\n",
      "Epoch 1404/30000 Training Loss: 0.09558665752410889\n",
      "Epoch 1405/30000 Training Loss: 0.08466602116823196\n",
      "Epoch 1406/30000 Training Loss: 0.08140735328197479\n",
      "Epoch 1407/30000 Training Loss: 0.0860903188586235\n",
      "Epoch 1408/30000 Training Loss: 0.07070880383253098\n",
      "Epoch 1409/30000 Training Loss: 0.07967907935380936\n",
      "Epoch 1410/30000 Training Loss: 0.09699743986129761\n",
      "Epoch 1411/30000 Training Loss: 0.08996964991092682\n",
      "Epoch 1412/30000 Training Loss: 0.10347536951303482\n",
      "Epoch 1413/30000 Training Loss: 0.0919751524925232\n",
      "Epoch 1414/30000 Training Loss: 0.07986469566822052\n",
      "Epoch 1415/30000 Training Loss: 0.07853137701749802\n",
      "Epoch 1416/30000 Training Loss: 0.09788332879543304\n",
      "Epoch 1417/30000 Training Loss: 0.0752137154340744\n",
      "Epoch 1418/30000 Training Loss: 0.10086672008037567\n",
      "Epoch 1419/30000 Training Loss: 0.09672011435031891\n",
      "Epoch 1420/30000 Training Loss: 0.08529704809188843\n",
      "Epoch 1421/30000 Training Loss: 0.10129673779010773\n",
      "Epoch 1422/30000 Training Loss: 0.10481712222099304\n",
      "Epoch 1423/30000 Training Loss: 0.08603967726230621\n",
      "Epoch 1424/30000 Training Loss: 0.09745035320520401\n",
      "Epoch 1425/30000 Training Loss: 0.09347546100616455\n",
      "Epoch 1426/30000 Training Loss: 0.07868596911430359\n",
      "Epoch 1427/30000 Training Loss: 0.09621977806091309\n",
      "Epoch 1428/30000 Training Loss: 0.09943410754203796\n",
      "Epoch 1429/30000 Training Loss: 0.08540305495262146\n",
      "Epoch 1430/30000 Training Loss: 0.10145334899425507\n",
      "Epoch 1431/30000 Training Loss: 0.07317353039979935\n",
      "Epoch 1432/30000 Training Loss: 0.09634776413440704\n",
      "Epoch 1433/30000 Training Loss: 0.07969494163990021\n",
      "Epoch 1434/30000 Training Loss: 0.09265723079442978\n",
      "Epoch 1435/30000 Training Loss: 0.08654949814081192\n",
      "Epoch 1436/30000 Training Loss: 0.08515509217977524\n",
      "Epoch 1437/30000 Training Loss: 0.07982660830020905\n",
      "Epoch 1438/30000 Training Loss: 0.08142627775669098\n",
      "Epoch 1439/30000 Training Loss: 0.09169313311576843\n",
      "Epoch 1440/30000 Training Loss: 0.07310672104358673\n",
      "Epoch 1441/30000 Training Loss: 0.09413547813892365\n",
      "Epoch 1442/30000 Training Loss: 0.09786388278007507\n",
      "Epoch 1443/30000 Training Loss: 0.08577100187540054\n",
      "Epoch 1444/30000 Training Loss: 0.0816020667552948\n",
      "Epoch 1445/30000 Training Loss: 0.101827472448349\n",
      "Epoch 1446/30000 Training Loss: 0.06841027736663818\n",
      "Epoch 1447/30000 Training Loss: 0.10126636922359467\n",
      "Epoch 1448/30000 Training Loss: 0.08683865517377853\n",
      "Epoch 1449/30000 Training Loss: 0.09316859394311905\n",
      "Epoch 1450/30000 Training Loss: 0.09299613535404205\n",
      "Epoch 1451/30000 Training Loss: 0.09682199358940125\n",
      "Epoch 1452/30000 Training Loss: 0.08143612742424011\n",
      "Epoch 1453/30000 Training Loss: 0.070780448615551\n",
      "Epoch 1454/30000 Training Loss: 0.09139438718557358\n",
      "Epoch 1455/30000 Training Loss: 0.08028590679168701\n",
      "Epoch 1456/30000 Training Loss: 0.08171436190605164\n",
      "Epoch 1457/30000 Training Loss: 0.09124919027090073\n",
      "Epoch 1458/30000 Training Loss: 0.08217386901378632\n",
      "Epoch 1459/30000 Training Loss: 0.08914235979318619\n",
      "Epoch 1460/30000 Training Loss: 0.08225808292627335\n",
      "Epoch 1461/30000 Training Loss: 0.07730941474437714\n",
      "Epoch 1462/30000 Training Loss: 0.09500648826360703\n",
      "Epoch 1463/30000 Training Loss: 0.10076622664928436\n",
      "Epoch 1464/30000 Training Loss: 0.07966195046901703\n",
      "Epoch 1465/30000 Training Loss: 0.07642151415348053\n",
      "Epoch 1466/30000 Training Loss: 0.08182353526353836\n",
      "Epoch 1467/30000 Training Loss: 0.09478846937417984\n",
      "Epoch 1468/30000 Training Loss: 0.08270589262247086\n",
      "Epoch 1469/30000 Training Loss: 0.0746331512928009\n",
      "Epoch 1470/30000 Training Loss: 0.11255200207233429\n",
      "Epoch 1471/30000 Training Loss: 0.06785645335912704\n",
      "Epoch 1472/30000 Training Loss: 0.09182333946228027\n",
      "Epoch 1473/30000 Training Loss: 0.08967462182044983\n",
      "Epoch 1474/30000 Training Loss: 0.0809498131275177\n",
      "Epoch 1475/30000 Training Loss: 0.08087784796953201\n",
      "Epoch 1476/30000 Training Loss: 0.08311330527067184\n",
      "Epoch 1477/30000 Training Loss: 0.0725187137722969\n",
      "Epoch 1478/30000 Training Loss: 0.06746423244476318\n",
      "Epoch 1479/30000 Training Loss: 0.09104303270578384\n",
      "Epoch 1480/30000 Training Loss: 0.07495859265327454\n",
      "Epoch 1481/30000 Training Loss: 0.0870489627122879\n",
      "Epoch 1482/30000 Training Loss: 0.07959403097629547\n",
      "Epoch 1483/30000 Training Loss: 0.086285799741745\n",
      "Epoch 1484/30000 Training Loss: 0.0937388688325882\n",
      "Epoch 1485/30000 Training Loss: 0.09347763657569885\n",
      "Epoch 1486/30000 Training Loss: 0.06726028025150299\n",
      "Epoch 1487/30000 Training Loss: 0.09170323610305786\n",
      "Epoch 1488/30000 Training Loss: 0.09172560274600983\n",
      "Epoch 1489/30000 Training Loss: 0.08810117095708847\n",
      "Epoch 1490/30000 Training Loss: 0.08495421707630157\n",
      "Epoch 1491/30000 Training Loss: 0.07681182771921158\n",
      "Epoch 1492/30000 Training Loss: 0.08176545798778534\n",
      "Epoch 1493/30000 Training Loss: 0.09156933426856995\n",
      "Epoch 1494/30000 Training Loss: 0.08714942634105682\n",
      "Epoch 1495/30000 Training Loss: 0.07600995898246765\n",
      "Epoch 1496/30000 Training Loss: 0.08768308162689209\n",
      "Epoch 1497/30000 Training Loss: 0.08561094105243683\n",
      "Epoch 1498/30000 Training Loss: 0.07077960669994354\n",
      "Epoch 1499/30000 Training Loss: 0.07932066172361374\n",
      "Epoch 1500/30000 Training Loss: 0.0800875723361969\n",
      "Epoch 1500/30000 Validation Loss: 0.07925593852996826\n",
      "Epoch 1501/30000 Training Loss: 0.1050206869840622\n",
      "Epoch 1502/30000 Training Loss: 0.07282886654138565\n",
      "Epoch 1503/30000 Training Loss: 0.08415517210960388\n",
      "Epoch 1504/30000 Training Loss: 0.07903934270143509\n",
      "Epoch 1505/30000 Training Loss: 0.08378802239894867\n",
      "Epoch 1506/30000 Training Loss: 0.07394369691610336\n",
      "Epoch 1507/30000 Training Loss: 0.06315964460372925\n",
      "Epoch 1508/30000 Training Loss: 0.09068163484334946\n",
      "Epoch 1509/30000 Training Loss: 0.08305125683546066\n",
      "Epoch 1510/30000 Training Loss: 0.06639379262924194\n",
      "Epoch 1511/30000 Training Loss: 0.07069018483161926\n",
      "Epoch 1512/30000 Training Loss: 0.06960932165384293\n",
      "Epoch 1513/30000 Training Loss: 0.0993104875087738\n",
      "Epoch 1514/30000 Training Loss: 0.08713933825492859\n",
      "Epoch 1515/30000 Training Loss: 0.09209178388118744\n",
      "Epoch 1516/30000 Training Loss: 0.09012441337108612\n",
      "Epoch 1517/30000 Training Loss: 0.09143997728824615\n",
      "Epoch 1518/30000 Training Loss: 0.09049832075834274\n",
      "Epoch 1519/30000 Training Loss: 0.07401502132415771\n",
      "Epoch 1520/30000 Training Loss: 0.07575754076242447\n",
      "Epoch 1521/30000 Training Loss: 0.08994164317846298\n",
      "Epoch 1522/30000 Training Loss: 0.0971030741930008\n",
      "Epoch 1523/30000 Training Loss: 0.08031540364027023\n",
      "Epoch 1524/30000 Training Loss: 0.08241100609302521\n",
      "Epoch 1525/30000 Training Loss: 0.09406165033578873\n",
      "Epoch 1526/30000 Training Loss: 0.07999888062477112\n",
      "Epoch 1527/30000 Training Loss: 0.09293666481971741\n",
      "Epoch 1528/30000 Training Loss: 0.08584637939929962\n",
      "Epoch 1529/30000 Training Loss: 0.07888084650039673\n",
      "Epoch 1530/30000 Training Loss: 0.09395565837621689\n",
      "Epoch 1531/30000 Training Loss: 0.0702049508690834\n",
      "Epoch 1532/30000 Training Loss: 0.08318900316953659\n",
      "Epoch 1533/30000 Training Loss: 0.08387912064790726\n",
      "Epoch 1534/30000 Training Loss: 0.07932311296463013\n",
      "Epoch 1535/30000 Training Loss: 0.07916253060102463\n",
      "Epoch 1536/30000 Training Loss: 0.07763823866844177\n",
      "Epoch 1537/30000 Training Loss: 0.06836222112178802\n",
      "Epoch 1538/30000 Training Loss: 0.07546032220125198\n",
      "Epoch 1539/30000 Training Loss: 0.08993139117956161\n",
      "Epoch 1540/30000 Training Loss: 0.07954373210668564\n",
      "Epoch 1541/30000 Training Loss: 0.0772680789232254\n",
      "Epoch 1542/30000 Training Loss: 0.10455889254808426\n",
      "Epoch 1543/30000 Training Loss: 0.07432645559310913\n",
      "Epoch 1544/30000 Training Loss: 0.09073646366596222\n",
      "Epoch 1545/30000 Training Loss: 0.07664170116186142\n",
      "Epoch 1546/30000 Training Loss: 0.0869835764169693\n",
      "Epoch 1547/30000 Training Loss: 0.09654344618320465\n",
      "Epoch 1548/30000 Training Loss: 0.09428013116121292\n",
      "Epoch 1549/30000 Training Loss: 0.06378829479217529\n",
      "Epoch 1550/30000 Training Loss: 0.06836144626140594\n",
      "Epoch 1551/30000 Training Loss: 0.0741226077079773\n",
      "Epoch 1552/30000 Training Loss: 0.08225366473197937\n",
      "Epoch 1553/30000 Training Loss: 0.10073905438184738\n",
      "Epoch 1554/30000 Training Loss: 0.07559491693973541\n",
      "Epoch 1555/30000 Training Loss: 0.09335791319608688\n",
      "Epoch 1556/30000 Training Loss: 0.06843879818916321\n",
      "Epoch 1557/30000 Training Loss: 0.10195696353912354\n",
      "Epoch 1558/30000 Training Loss: 0.060640715062618256\n",
      "Epoch 1559/30000 Training Loss: 0.0814906433224678\n",
      "Epoch 1560/30000 Training Loss: 0.08983553946018219\n",
      "Epoch 1561/30000 Training Loss: 0.061628710478544235\n",
      "Epoch 1562/30000 Training Loss: 0.06923484802246094\n",
      "Epoch 1563/30000 Training Loss: 0.08943349123001099\n",
      "Epoch 1564/30000 Training Loss: 0.08210534602403641\n",
      "Epoch 1565/30000 Training Loss: 0.08233838528394699\n",
      "Epoch 1566/30000 Training Loss: 0.07839436829090118\n",
      "Epoch 1567/30000 Training Loss: 0.07798563688993454\n",
      "Epoch 1568/30000 Training Loss: 0.07584254443645477\n",
      "Epoch 1569/30000 Training Loss: 0.07236962765455246\n",
      "Epoch 1570/30000 Training Loss: 0.08636872470378876\n",
      "Epoch 1571/30000 Training Loss: 0.07257585972547531\n",
      "Epoch 1572/30000 Training Loss: 0.09632838517427444\n",
      "Epoch 1573/30000 Training Loss: 0.07791639864444733\n",
      "Epoch 1574/30000 Training Loss: 0.07888120412826538\n",
      "Epoch 1575/30000 Training Loss: 0.08860087394714355\n",
      "Epoch 1576/30000 Training Loss: 0.07529199123382568\n",
      "Epoch 1577/30000 Training Loss: 0.08436773717403412\n",
      "Epoch 1578/30000 Training Loss: 0.08125057816505432\n",
      "Epoch 1579/30000 Training Loss: 0.06975114345550537\n",
      "Epoch 1580/30000 Training Loss: 0.07638245820999146\n",
      "Epoch 1581/30000 Training Loss: 0.07426900416612625\n",
      "Epoch 1582/30000 Training Loss: 0.05880852788686752\n",
      "Epoch 1583/30000 Training Loss: 0.08109120279550552\n",
      "Epoch 1584/30000 Training Loss: 0.08458396792411804\n",
      "Epoch 1585/30000 Training Loss: 0.0958193689584732\n",
      "Epoch 1586/30000 Training Loss: 0.08476884663105011\n",
      "Epoch 1587/30000 Training Loss: 0.06125606596469879\n",
      "Epoch 1588/30000 Training Loss: 0.09661450982093811\n",
      "Epoch 1589/30000 Training Loss: 0.07543313503265381\n",
      "Epoch 1590/30000 Training Loss: 0.083232581615448\n",
      "Epoch 1591/30000 Training Loss: 0.07975039631128311\n",
      "Epoch 1592/30000 Training Loss: 0.07608795166015625\n",
      "Epoch 1593/30000 Training Loss: 0.0872739851474762\n",
      "Epoch 1594/30000 Training Loss: 0.08791988343000412\n",
      "Epoch 1595/30000 Training Loss: 0.09102480858564377\n",
      "Epoch 1596/30000 Training Loss: 0.06843172013759613\n",
      "Epoch 1597/30000 Training Loss: 0.0734671801328659\n",
      "Epoch 1598/30000 Training Loss: 0.07980021089315414\n",
      "Epoch 1599/30000 Training Loss: 0.09319598972797394\n",
      "Epoch 1600/30000 Training Loss: 0.06259039789438248\n",
      "Epoch 1600/30000 Validation Loss: 0.07622387260198593\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07622387260198593<=============\n",
      "Epoch 1601/30000 Training Loss: 0.09164156764745712\n",
      "Epoch 1602/30000 Training Loss: 0.07137413322925568\n",
      "Epoch 1603/30000 Training Loss: 0.10700830817222595\n",
      "Epoch 1604/30000 Training Loss: 0.07316087186336517\n",
      "Epoch 1605/30000 Training Loss: 0.08156169950962067\n",
      "Epoch 1606/30000 Training Loss: 0.08286915719509125\n",
      "Epoch 1607/30000 Training Loss: 0.09859652072191238\n",
      "Epoch 1608/30000 Training Loss: 0.07630441337823868\n",
      "Epoch 1609/30000 Training Loss: 0.07703662663698196\n",
      "Epoch 1610/30000 Training Loss: 0.07965824007987976\n",
      "Epoch 1611/30000 Training Loss: 0.08900022506713867\n",
      "Epoch 1612/30000 Training Loss: 0.06190745532512665\n",
      "Epoch 1613/30000 Training Loss: 0.0674901083111763\n",
      "Epoch 1614/30000 Training Loss: 0.09240766614675522\n",
      "Epoch 1615/30000 Training Loss: 0.08310580998659134\n",
      "Epoch 1616/30000 Training Loss: 0.08692095428705215\n",
      "Epoch 1617/30000 Training Loss: 0.08257465809583664\n",
      "Epoch 1618/30000 Training Loss: 0.07538758963346481\n",
      "Epoch 1619/30000 Training Loss: 0.08885189145803452\n",
      "Epoch 1620/30000 Training Loss: 0.06861932575702667\n",
      "Epoch 1621/30000 Training Loss: 0.07075411081314087\n",
      "Epoch 1622/30000 Training Loss: 0.09301107376813889\n",
      "Epoch 1623/30000 Training Loss: 0.09044375270605087\n",
      "Epoch 1624/30000 Training Loss: 0.08987446874380112\n",
      "Epoch 1625/30000 Training Loss: 0.08618346601724625\n",
      "Epoch 1626/30000 Training Loss: 0.07446910440921783\n",
      "Epoch 1627/30000 Training Loss: 0.08936803042888641\n",
      "Epoch 1628/30000 Training Loss: 0.09226261079311371\n",
      "Epoch 1629/30000 Training Loss: 0.07105876505374908\n",
      "Epoch 1630/30000 Training Loss: 0.09049390256404877\n",
      "Epoch 1631/30000 Training Loss: 0.09796112030744553\n",
      "Epoch 1632/30000 Training Loss: 0.08495213091373444\n",
      "Epoch 1633/30000 Training Loss: 0.08076900243759155\n",
      "Epoch 1634/30000 Training Loss: 0.06476162374019623\n",
      "Epoch 1635/30000 Training Loss: 0.08427143096923828\n",
      "Epoch 1636/30000 Training Loss: 0.06931471079587936\n",
      "Epoch 1637/30000 Training Loss: 0.0752347856760025\n",
      "Epoch 1638/30000 Training Loss: 0.08231780678033829\n",
      "Epoch 1639/30000 Training Loss: 0.08754998445510864\n",
      "Epoch 1640/30000 Training Loss: 0.09118686616420746\n",
      "Epoch 1641/30000 Training Loss: 0.08297855406999588\n",
      "Epoch 1642/30000 Training Loss: 0.09785638749599457\n",
      "Epoch 1643/30000 Training Loss: 0.08197757601737976\n",
      "Epoch 1644/30000 Training Loss: 0.07418830692768097\n",
      "Epoch 1645/30000 Training Loss: 0.08187223970890045\n",
      "Epoch 1646/30000 Training Loss: 0.07282091677188873\n",
      "Epoch 1647/30000 Training Loss: 0.08800044655799866\n",
      "Epoch 1648/30000 Training Loss: 0.08806701004505157\n",
      "Epoch 1649/30000 Training Loss: 0.08533696830272675\n",
      "Epoch 1650/30000 Training Loss: 0.07914990931749344\n",
      "Epoch 1651/30000 Training Loss: 0.0752333551645279\n",
      "Epoch 1652/30000 Training Loss: 0.08692031353712082\n",
      "Epoch 1653/30000 Training Loss: 0.08909548074007034\n",
      "Epoch 1654/30000 Training Loss: 0.07650324702262878\n",
      "Epoch 1655/30000 Training Loss: 0.08453284204006195\n",
      "Epoch 1656/30000 Training Loss: 0.09256941825151443\n",
      "Epoch 1657/30000 Training Loss: 0.06799516826868057\n",
      "Epoch 1658/30000 Training Loss: 0.08115736395120621\n",
      "Epoch 1659/30000 Training Loss: 0.06441617757081985\n",
      "Epoch 1660/30000 Training Loss: 0.07835803925991058\n",
      "Epoch 1661/30000 Training Loss: 0.06363921612501144\n",
      "Epoch 1662/30000 Training Loss: 0.0953444242477417\n",
      "Epoch 1663/30000 Training Loss: 0.08187492191791534\n",
      "Epoch 1664/30000 Training Loss: 0.09366333484649658\n",
      "Epoch 1665/30000 Training Loss: 0.08468077331781387\n",
      "Epoch 1666/30000 Training Loss: 0.07348733395338058\n",
      "Epoch 1667/30000 Training Loss: 0.07546272873878479\n",
      "Epoch 1668/30000 Training Loss: 0.07054810971021652\n",
      "Epoch 1669/30000 Training Loss: 0.06858917325735092\n",
      "Epoch 1670/30000 Training Loss: 0.09569447487592697\n",
      "Epoch 1671/30000 Training Loss: 0.06897403299808502\n",
      "Epoch 1672/30000 Training Loss: 0.07475276291370392\n",
      "Epoch 1673/30000 Training Loss: 0.06408889591693878\n",
      "Epoch 1674/30000 Training Loss: 0.08075610548257828\n",
      "Epoch 1675/30000 Training Loss: 0.0966443344950676\n",
      "Epoch 1676/30000 Training Loss: 0.06841491162776947\n",
      "Epoch 1677/30000 Training Loss: 0.0967060774564743\n",
      "Epoch 1678/30000 Training Loss: 0.06662480533123016\n",
      "Epoch 1679/30000 Training Loss: 0.07908453792333603\n",
      "Epoch 1680/30000 Training Loss: 0.07347570359706879\n",
      "Epoch 1681/30000 Training Loss: 0.08689413219690323\n",
      "Epoch 1682/30000 Training Loss: 0.06840208172798157\n",
      "Epoch 1683/30000 Training Loss: 0.07040326297283173\n",
      "Epoch 1684/30000 Training Loss: 0.09120248258113861\n",
      "Epoch 1685/30000 Training Loss: 0.08407469093799591\n",
      "Epoch 1686/30000 Training Loss: 0.08822751045227051\n",
      "Epoch 1687/30000 Training Loss: 0.07717182487249374\n",
      "Epoch 1688/30000 Training Loss: 0.0838794857263565\n",
      "Epoch 1689/30000 Training Loss: 0.06929567456245422\n",
      "Epoch 1690/30000 Training Loss: 0.09607212245464325\n",
      "Epoch 1691/30000 Training Loss: 0.07245997339487076\n",
      "Epoch 1692/30000 Training Loss: 0.06992189586162567\n",
      "Epoch 1693/30000 Training Loss: 0.088077612221241\n",
      "Epoch 1694/30000 Training Loss: 0.08333554863929749\n",
      "Epoch 1695/30000 Training Loss: 0.08942054957151413\n",
      "Epoch 1696/30000 Training Loss: 0.0760774090886116\n",
      "Epoch 1697/30000 Training Loss: 0.08314627408981323\n",
      "Epoch 1698/30000 Training Loss: 0.06517169624567032\n",
      "Epoch 1699/30000 Training Loss: 0.07719159126281738\n",
      "Epoch 1700/30000 Training Loss: 0.07493148744106293\n",
      "Epoch 1700/30000 Validation Loss: 0.08161434531211853\n",
      "Epoch 1701/30000 Training Loss: 0.07247807085514069\n",
      "Epoch 1702/30000 Training Loss: 0.0824124813079834\n",
      "Epoch 1703/30000 Training Loss: 0.09332938492298126\n",
      "Epoch 1704/30000 Training Loss: 0.07070336490869522\n",
      "Epoch 1705/30000 Training Loss: 0.0913652777671814\n",
      "Epoch 1706/30000 Training Loss: 0.08287224173545837\n",
      "Epoch 1707/30000 Training Loss: 0.07185928523540497\n",
      "Epoch 1708/30000 Training Loss: 0.09012175351381302\n",
      "Epoch 1709/30000 Training Loss: 0.08646024763584137\n",
      "Epoch 1710/30000 Training Loss: 0.05807829275727272\n",
      "Epoch 1711/30000 Training Loss: 0.09485159814357758\n",
      "Epoch 1712/30000 Training Loss: 0.0781082957983017\n",
      "Epoch 1713/30000 Training Loss: 0.09109237790107727\n",
      "Epoch 1714/30000 Training Loss: 0.07588415592908859\n",
      "Epoch 1715/30000 Training Loss: 0.0700363889336586\n",
      "Epoch 1716/30000 Training Loss: 0.0824226588010788\n",
      "Epoch 1717/30000 Training Loss: 0.06780801713466644\n",
      "Epoch 1718/30000 Training Loss: 0.0752609521150589\n",
      "Epoch 1719/30000 Training Loss: 0.08272711932659149\n",
      "Epoch 1720/30000 Training Loss: 0.0710914134979248\n",
      "Epoch 1721/30000 Training Loss: 0.0694110170006752\n",
      "Epoch 1722/30000 Training Loss: 0.07685409486293793\n",
      "Epoch 1723/30000 Training Loss: 0.06661594659090042\n",
      "Epoch 1724/30000 Training Loss: 0.06916636973619461\n",
      "Epoch 1725/30000 Training Loss: 0.08327048271894455\n",
      "Epoch 1726/30000 Training Loss: 0.09054405987262726\n",
      "Epoch 1727/30000 Training Loss: 0.085802361369133\n",
      "Epoch 1728/30000 Training Loss: 0.091033436357975\n",
      "Epoch 1729/30000 Training Loss: 0.06508676707744598\n",
      "Epoch 1730/30000 Training Loss: 0.06939519196748734\n",
      "Epoch 1731/30000 Training Loss: 0.07595478743314743\n",
      "Epoch 1732/30000 Training Loss: 0.10243605077266693\n",
      "Epoch 1733/30000 Training Loss: 0.07694385945796967\n",
      "Epoch 1734/30000 Training Loss: 0.07295045256614685\n",
      "Epoch 1735/30000 Training Loss: 0.07846469432115555\n",
      "Epoch 1736/30000 Training Loss: 0.0739288255572319\n",
      "Epoch 1737/30000 Training Loss: 0.07889997959136963\n",
      "Epoch 1738/30000 Training Loss: 0.09118533134460449\n",
      "Epoch 1739/30000 Training Loss: 0.07739422470331192\n",
      "Epoch 1740/30000 Training Loss: 0.07822586596012115\n",
      "Epoch 1741/30000 Training Loss: 0.09765158593654633\n",
      "Epoch 1742/30000 Training Loss: 0.07747530937194824\n",
      "Epoch 1743/30000 Training Loss: 0.0903860330581665\n",
      "Epoch 1744/30000 Training Loss: 0.07091277092695236\n",
      "Epoch 1745/30000 Training Loss: 0.07627680897712708\n",
      "Epoch 1746/30000 Training Loss: 0.06934716552495956\n",
      "Epoch 1747/30000 Training Loss: 0.0761297270655632\n",
      "Epoch 1748/30000 Training Loss: 0.09200508892536163\n",
      "Epoch 1749/30000 Training Loss: 0.08302439749240875\n",
      "Epoch 1750/30000 Training Loss: 0.08677048236131668\n",
      "Epoch 1751/30000 Training Loss: 0.0970662534236908\n",
      "Epoch 1752/30000 Training Loss: 0.06958567351102829\n",
      "Epoch 1753/30000 Training Loss: 0.07966285198926926\n",
      "Epoch 1754/30000 Training Loss: 0.06942585855722427\n",
      "Epoch 1755/30000 Training Loss: 0.0734485536813736\n",
      "Epoch 1756/30000 Training Loss: 0.0808037742972374\n",
      "Epoch 1757/30000 Training Loss: 0.07903750985860825\n",
      "Epoch 1758/30000 Training Loss: 0.06886294484138489\n",
      "Epoch 1759/30000 Training Loss: 0.0705820843577385\n",
      "Epoch 1760/30000 Training Loss: 0.07993128150701523\n",
      "Epoch 1761/30000 Training Loss: 0.0823177844285965\n",
      "Epoch 1762/30000 Training Loss: 0.08050838857889175\n",
      "Epoch 1763/30000 Training Loss: 0.07642816007137299\n",
      "Epoch 1764/30000 Training Loss: 0.08226980268955231\n",
      "Epoch 1765/30000 Training Loss: 0.09563351422548294\n",
      "Epoch 1766/30000 Training Loss: 0.0942600667476654\n",
      "Epoch 1767/30000 Training Loss: 0.07403288036584854\n",
      "Epoch 1768/30000 Training Loss: 0.06690219044685364\n",
      "Epoch 1769/30000 Training Loss: 0.07052884250879288\n",
      "Epoch 1770/30000 Training Loss: 0.06774278730154037\n",
      "Epoch 1771/30000 Training Loss: 0.06161029264330864\n",
      "Epoch 1772/30000 Training Loss: 0.07634472846984863\n",
      "Epoch 1773/30000 Training Loss: 0.06252731382846832\n",
      "Epoch 1774/30000 Training Loss: 0.07031373679637909\n",
      "Epoch 1775/30000 Training Loss: 0.08647709339857101\n",
      "Epoch 1776/30000 Training Loss: 0.09047475457191467\n",
      "Epoch 1777/30000 Training Loss: 0.07402388751506805\n",
      "Epoch 1778/30000 Training Loss: 0.07486048340797424\n",
      "Epoch 1779/30000 Training Loss: 0.09337908029556274\n",
      "Epoch 1780/30000 Training Loss: 0.07481735944747925\n",
      "Epoch 1781/30000 Training Loss: 0.07313861697912216\n",
      "Epoch 1782/30000 Training Loss: 0.08640461415052414\n",
      "Epoch 1783/30000 Training Loss: 0.07327825576066971\n",
      "Epoch 1784/30000 Training Loss: 0.08277812600135803\n",
      "Epoch 1785/30000 Training Loss: 0.06448584794998169\n",
      "Epoch 1786/30000 Training Loss: 0.07862558960914612\n",
      "Epoch 1787/30000 Training Loss: 0.0711209699511528\n",
      "Epoch 1788/30000 Training Loss: 0.09180469065904617\n",
      "Epoch 1789/30000 Training Loss: 0.07552675157785416\n",
      "Epoch 1790/30000 Training Loss: 0.08981774002313614\n",
      "Epoch 1791/30000 Training Loss: 0.07150489836931229\n",
      "Epoch 1792/30000 Training Loss: 0.09049082547426224\n",
      "Epoch 1793/30000 Training Loss: 0.09602142870426178\n",
      "Epoch 1794/30000 Training Loss: 0.06191276013851166\n",
      "Epoch 1795/30000 Training Loss: 0.08860281109809875\n",
      "Epoch 1796/30000 Training Loss: 0.08630754053592682\n",
      "Epoch 1797/30000 Training Loss: 0.0767180547118187\n",
      "Epoch 1798/30000 Training Loss: 0.06568494439125061\n",
      "Epoch 1799/30000 Training Loss: 0.0772489383816719\n",
      "Epoch 1800/30000 Training Loss: 0.07750383764505386\n",
      "Epoch 1800/30000 Validation Loss: 0.10072852671146393\n",
      "Epoch 1801/30000 Training Loss: 0.0766075924038887\n",
      "Epoch 1802/30000 Training Loss: 0.08545838296413422\n",
      "Epoch 1803/30000 Training Loss: 0.0691656544804573\n",
      "Epoch 1804/30000 Training Loss: 0.07731996476650238\n",
      "Epoch 1805/30000 Training Loss: 0.060806162655353546\n",
      "Epoch 1806/30000 Training Loss: 0.08605445921421051\n",
      "Epoch 1807/30000 Training Loss: 0.0702114850282669\n",
      "Epoch 1808/30000 Training Loss: 0.07594016194343567\n",
      "Epoch 1809/30000 Training Loss: 0.07790724188089371\n",
      "Epoch 1810/30000 Training Loss: 0.08282218128442764\n",
      "Epoch 1811/30000 Training Loss: 0.07576458156108856\n",
      "Epoch 1812/30000 Training Loss: 0.09997236728668213\n",
      "Epoch 1813/30000 Training Loss: 0.06899140030145645\n",
      "Epoch 1814/30000 Training Loss: 0.08844854682683945\n",
      "Epoch 1815/30000 Training Loss: 0.07625574618577957\n",
      "Epoch 1816/30000 Training Loss: 0.09087365865707397\n",
      "Epoch 1817/30000 Training Loss: 0.06821891665458679\n",
      "Epoch 1818/30000 Training Loss: 0.07137767970561981\n",
      "Epoch 1819/30000 Training Loss: 0.08241923898458481\n",
      "Epoch 1820/30000 Training Loss: 0.08157630264759064\n",
      "Epoch 1821/30000 Training Loss: 0.07641471177339554\n",
      "Epoch 1822/30000 Training Loss: 0.0817810595035553\n",
      "Epoch 1823/30000 Training Loss: 0.07406190782785416\n",
      "Epoch 1824/30000 Training Loss: 0.09209562838077545\n",
      "Epoch 1825/30000 Training Loss: 0.10464238375425339\n",
      "Epoch 1826/30000 Training Loss: 0.07618916779756546\n",
      "Epoch 1827/30000 Training Loss: 0.07449957728385925\n",
      "Epoch 1828/30000 Training Loss: 0.07894809544086456\n",
      "Epoch 1829/30000 Training Loss: 0.07834260165691376\n",
      "Epoch 1830/30000 Training Loss: 0.07940301299095154\n",
      "Epoch 1831/30000 Training Loss: 0.06955248862504959\n",
      "Epoch 1832/30000 Training Loss: 0.07541970908641815\n",
      "Epoch 1833/30000 Training Loss: 0.07743674516677856\n",
      "Epoch 1834/30000 Training Loss: 0.0893440768122673\n",
      "Epoch 1835/30000 Training Loss: 0.08267715573310852\n",
      "Epoch 1836/30000 Training Loss: 0.07292746007442474\n",
      "Epoch 1837/30000 Training Loss: 0.07765121012926102\n",
      "Epoch 1838/30000 Training Loss: 0.06239020824432373\n",
      "Epoch 1839/30000 Training Loss: 0.07858419418334961\n",
      "Epoch 1840/30000 Training Loss: 0.08202037215232849\n",
      "Epoch 1841/30000 Training Loss: 0.08224258571863174\n",
      "Epoch 1842/30000 Training Loss: 0.08165355026721954\n",
      "Epoch 1843/30000 Training Loss: 0.08809356391429901\n",
      "Epoch 1844/30000 Training Loss: 0.06685107201337814\n",
      "Epoch 1845/30000 Training Loss: 0.08290202915668488\n",
      "Epoch 1846/30000 Training Loss: 0.05930580943822861\n",
      "Epoch 1847/30000 Training Loss: 0.06569148600101471\n",
      "Epoch 1848/30000 Training Loss: 0.07305282354354858\n",
      "Epoch 1849/30000 Training Loss: 0.09049297869205475\n",
      "Epoch 1850/30000 Training Loss: 0.07955163717269897\n",
      "Epoch 1851/30000 Training Loss: 0.0826353132724762\n",
      "Epoch 1852/30000 Training Loss: 0.0710529312491417\n",
      "Epoch 1853/30000 Training Loss: 0.08450645953416824\n",
      "Epoch 1854/30000 Training Loss: 0.07348772883415222\n",
      "Epoch 1855/30000 Training Loss: 0.0957624614238739\n",
      "Epoch 1856/30000 Training Loss: 0.07533007860183716\n",
      "Epoch 1857/30000 Training Loss: 0.07059714198112488\n",
      "Epoch 1858/30000 Training Loss: 0.08400559425354004\n",
      "Epoch 1859/30000 Training Loss: 0.06849176436662674\n",
      "Epoch 1860/30000 Training Loss: 0.08448836952447891\n",
      "Epoch 1861/30000 Training Loss: 0.0748668685555458\n",
      "Epoch 1862/30000 Training Loss: 0.07257399708032608\n",
      "Epoch 1863/30000 Training Loss: 0.07660764455795288\n",
      "Epoch 1864/30000 Training Loss: 0.08681490272283554\n",
      "Epoch 1865/30000 Training Loss: 0.07089465111494064\n",
      "Epoch 1866/30000 Training Loss: 0.09245265275239944\n",
      "Epoch 1867/30000 Training Loss: 0.0694195032119751\n",
      "Epoch 1868/30000 Training Loss: 0.07921601086854935\n",
      "Epoch 1869/30000 Training Loss: 0.08989246189594269\n",
      "Epoch 1870/30000 Training Loss: 0.07392264157533646\n",
      "Epoch 1871/30000 Training Loss: 0.0735761895775795\n",
      "Epoch 1872/30000 Training Loss: 0.0631401464343071\n",
      "Epoch 1873/30000 Training Loss: 0.0748339369893074\n",
      "Epoch 1874/30000 Training Loss: 0.09617189317941666\n",
      "Epoch 1875/30000 Training Loss: 0.07335908710956573\n",
      "Epoch 1876/30000 Training Loss: 0.06102335453033447\n",
      "Epoch 1877/30000 Training Loss: 0.10732560604810715\n",
      "Epoch 1878/30000 Training Loss: 0.06998661160469055\n",
      "Epoch 1879/30000 Training Loss: 0.08355225622653961\n",
      "Epoch 1880/30000 Training Loss: 0.07465775310993195\n",
      "Epoch 1881/30000 Training Loss: 0.08998443186283112\n",
      "Epoch 1882/30000 Training Loss: 0.09648071229457855\n",
      "Epoch 1883/30000 Training Loss: 0.0736849457025528\n",
      "Epoch 1884/30000 Training Loss: 0.06910347193479538\n",
      "Epoch 1885/30000 Training Loss: 0.08027781546115875\n",
      "Epoch 1886/30000 Training Loss: 0.08699388056993484\n",
      "Epoch 1887/30000 Training Loss: 0.08871500194072723\n",
      "Epoch 1888/30000 Training Loss: 0.06408394873142242\n",
      "Epoch 1889/30000 Training Loss: 0.07242359966039658\n",
      "Epoch 1890/30000 Training Loss: 0.08946892619132996\n",
      "Epoch 1891/30000 Training Loss: 0.08805108070373535\n",
      "Epoch 1892/30000 Training Loss: 0.0629580020904541\n",
      "Epoch 1893/30000 Training Loss: 0.06577586382627487\n",
      "Epoch 1894/30000 Training Loss: 0.07458463311195374\n",
      "Epoch 1895/30000 Training Loss: 0.08305549621582031\n",
      "Epoch 1896/30000 Training Loss: 0.062248896807432175\n",
      "Epoch 1897/30000 Training Loss: 0.08594772964715958\n",
      "Epoch 1898/30000 Training Loss: 0.07454660534858704\n",
      "Epoch 1899/30000 Training Loss: 0.07617590576410294\n",
      "Epoch 1900/30000 Training Loss: 0.08004262298345566\n",
      "Epoch 1900/30000 Validation Loss: 0.08199670165777206\n",
      "Epoch 1901/30000 Training Loss: 0.07779987156391144\n",
      "Epoch 1902/30000 Training Loss: 0.08317350596189499\n",
      "Epoch 1903/30000 Training Loss: 0.08082041144371033\n",
      "Epoch 1904/30000 Training Loss: 0.06330782175064087\n",
      "Epoch 1905/30000 Training Loss: 0.06993420422077179\n",
      "Epoch 1906/30000 Training Loss: 0.06972503662109375\n",
      "Epoch 1907/30000 Training Loss: 0.09666910022497177\n",
      "Epoch 1908/30000 Training Loss: 0.08648255467414856\n",
      "Epoch 1909/30000 Training Loss: 0.07048474252223969\n",
      "Epoch 1910/30000 Training Loss: 0.08399901539087296\n",
      "Epoch 1911/30000 Training Loss: 0.08630971610546112\n",
      "Epoch 1912/30000 Training Loss: 0.07884158194065094\n",
      "Epoch 1913/30000 Training Loss: 0.08801941573619843\n",
      "Epoch 1914/30000 Training Loss: 0.08351283520460129\n",
      "Epoch 1915/30000 Training Loss: 0.08554961532354355\n",
      "Epoch 1916/30000 Training Loss: 0.07198835909366608\n",
      "Epoch 1917/30000 Training Loss: 0.08118189126253128\n",
      "Epoch 1918/30000 Training Loss: 0.07360580563545227\n",
      "Epoch 1919/30000 Training Loss: 0.07959054410457611\n",
      "Epoch 1920/30000 Training Loss: 0.09118257462978363\n",
      "Epoch 1921/30000 Training Loss: 0.09219089895486832\n",
      "Epoch 1922/30000 Training Loss: 0.07713522762060165\n",
      "Epoch 1923/30000 Training Loss: 0.06802783906459808\n",
      "Epoch 1924/30000 Training Loss: 0.08281451463699341\n",
      "Epoch 1925/30000 Training Loss: 0.0654866173863411\n",
      "Epoch 1926/30000 Training Loss: 0.0796649381518364\n",
      "Epoch 1927/30000 Training Loss: 0.056043244898319244\n",
      "Epoch 1928/30000 Training Loss: 0.08203578740358353\n",
      "Epoch 1929/30000 Training Loss: 0.0787968784570694\n",
      "Epoch 1930/30000 Training Loss: 0.06479627639055252\n",
      "Epoch 1931/30000 Training Loss: 0.08698281645774841\n",
      "Epoch 1932/30000 Training Loss: 0.0730881318449974\n",
      "Epoch 1933/30000 Training Loss: 0.06070183962583542\n",
      "Epoch 1934/30000 Training Loss: 0.07989335805177689\n",
      "Epoch 1935/30000 Training Loss: 0.060461677610874176\n",
      "Epoch 1936/30000 Training Loss: 0.08899747580289841\n",
      "Epoch 1937/30000 Training Loss: 0.0864693820476532\n",
      "Epoch 1938/30000 Training Loss: 0.07870614528656006\n",
      "Epoch 1939/30000 Training Loss: 0.07873281836509705\n",
      "Epoch 1940/30000 Training Loss: 0.08157803118228912\n",
      "Epoch 1941/30000 Training Loss: 0.09077011048793793\n",
      "Epoch 1942/30000 Training Loss: 0.0697333887219429\n",
      "Epoch 1943/30000 Training Loss: 0.07552088052034378\n",
      "Epoch 1944/30000 Training Loss: 0.0857405960559845\n",
      "Epoch 1945/30000 Training Loss: 0.08768611401319504\n",
      "Epoch 1946/30000 Training Loss: 0.07913137972354889\n",
      "Epoch 1947/30000 Training Loss: 0.07400989532470703\n",
      "Epoch 1948/30000 Training Loss: 0.06816436350345612\n",
      "Epoch 1949/30000 Training Loss: 0.08419634401798248\n",
      "Epoch 1950/30000 Training Loss: 0.09524036198854446\n",
      "Epoch 1951/30000 Training Loss: 0.0778256207704544\n",
      "Epoch 1952/30000 Training Loss: 0.08171506226062775\n",
      "Epoch 1953/30000 Training Loss: 0.07313714921474457\n",
      "Epoch 1954/30000 Training Loss: 0.0828118696808815\n",
      "Epoch 1955/30000 Training Loss: 0.08173014223575592\n",
      "Epoch 1956/30000 Training Loss: 0.0891401395201683\n",
      "Epoch 1957/30000 Training Loss: 0.08359445631504059\n",
      "Epoch 1958/30000 Training Loss: 0.08402078598737717\n",
      "Epoch 1959/30000 Training Loss: 0.09373927116394043\n",
      "Epoch 1960/30000 Training Loss: 0.07273434847593307\n",
      "Epoch 1961/30000 Training Loss: 0.08703365921974182\n",
      "Epoch 1962/30000 Training Loss: 0.07440614700317383\n",
      "Epoch 1963/30000 Training Loss: 0.08747400343418121\n",
      "Epoch 1964/30000 Training Loss: 0.07567339390516281\n",
      "Epoch 1965/30000 Training Loss: 0.07359963655471802\n",
      "Epoch 1966/30000 Training Loss: 0.06904401630163193\n",
      "Epoch 1967/30000 Training Loss: 0.10385079681873322\n",
      "Epoch 1968/30000 Training Loss: 0.0839884951710701\n",
      "Epoch 1969/30000 Training Loss: 0.07030603289604187\n",
      "Epoch 1970/30000 Training Loss: 0.07791239023208618\n",
      "Epoch 1971/30000 Training Loss: 0.08177099376916885\n",
      "Epoch 1972/30000 Training Loss: 0.09733720123767853\n",
      "Epoch 1973/30000 Training Loss: 0.07069547474384308\n",
      "Epoch 1974/30000 Training Loss: 0.07416626811027527\n",
      "Epoch 1975/30000 Training Loss: 0.07691210508346558\n",
      "Epoch 1976/30000 Training Loss: 0.05935197323560715\n",
      "Epoch 1977/30000 Training Loss: 0.062272004783153534\n",
      "Epoch 1978/30000 Training Loss: 0.07497690618038177\n",
      "Epoch 1979/30000 Training Loss: 0.07193394005298615\n",
      "Epoch 1980/30000 Training Loss: 0.08989345282316208\n",
      "Epoch 1981/30000 Training Loss: 0.1046258956193924\n",
      "Epoch 1982/30000 Training Loss: 0.07453655451536179\n",
      "Epoch 1983/30000 Training Loss: 0.06534504890441895\n",
      "Epoch 1984/30000 Training Loss: 0.07396923005580902\n",
      "Epoch 1985/30000 Training Loss: 0.05869247019290924\n",
      "Epoch 1986/30000 Training Loss: 0.06590703129768372\n",
      "Epoch 1987/30000 Training Loss: 0.09632779657840729\n",
      "Epoch 1988/30000 Training Loss: 0.06627892702817917\n",
      "Epoch 1989/30000 Training Loss: 0.09120090305805206\n",
      "Epoch 1990/30000 Training Loss: 0.08651749789714813\n",
      "Epoch 1991/30000 Training Loss: 0.07492447644472122\n",
      "Epoch 1992/30000 Training Loss: 0.07383114099502563\n",
      "Epoch 1993/30000 Training Loss: 0.09200213849544525\n",
      "Epoch 1994/30000 Training Loss: 0.06747119128704071\n",
      "Epoch 1995/30000 Training Loss: 0.08694817870855331\n",
      "Epoch 1996/30000 Training Loss: 0.084877610206604\n",
      "Epoch 1997/30000 Training Loss: 0.07827691733837128\n",
      "Epoch 1998/30000 Training Loss: 0.08882250636816025\n",
      "Epoch 1999/30000 Training Loss: 0.08724217116832733\n",
      "Epoch 2000/30000 Training Loss: 0.0655752420425415\n",
      "Epoch 2000/30000 Validation Loss: 0.07520945370197296\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07520945370197296<=============\n",
      "Epoch 2001/30000 Training Loss: 0.06985198706388474\n",
      "Epoch 2002/30000 Training Loss: 0.06510311365127563\n",
      "Epoch 2003/30000 Training Loss: 0.0717117190361023\n",
      "Epoch 2004/30000 Training Loss: 0.0817592516541481\n",
      "Epoch 2005/30000 Training Loss: 0.05956979840993881\n",
      "Epoch 2006/30000 Training Loss: 0.06536167114973068\n",
      "Epoch 2007/30000 Training Loss: 0.08778786659240723\n",
      "Epoch 2008/30000 Training Loss: 0.06754036992788315\n",
      "Epoch 2009/30000 Training Loss: 0.08934105932712555\n",
      "Epoch 2010/30000 Training Loss: 0.07109132409095764\n",
      "Epoch 2011/30000 Training Loss: 0.07795398682355881\n",
      "Epoch 2012/30000 Training Loss: 0.0631532371044159\n",
      "Epoch 2013/30000 Training Loss: 0.07381319999694824\n",
      "Epoch 2014/30000 Training Loss: 0.07808610051870346\n",
      "Epoch 2015/30000 Training Loss: 0.07703451067209244\n",
      "Epoch 2016/30000 Training Loss: 0.07341581583023071\n",
      "Epoch 2017/30000 Training Loss: 0.09755198657512665\n",
      "Epoch 2018/30000 Training Loss: 0.06183131784200668\n",
      "Epoch 2019/30000 Training Loss: 0.08577262610197067\n",
      "Epoch 2020/30000 Training Loss: 0.08821334689855576\n",
      "Epoch 2021/30000 Training Loss: 0.07321152091026306\n",
      "Epoch 2022/30000 Training Loss: 0.0761241689324379\n",
      "Epoch 2023/30000 Training Loss: 0.08603067696094513\n",
      "Epoch 2024/30000 Training Loss: 0.07747740298509598\n",
      "Epoch 2025/30000 Training Loss: 0.08706231415271759\n",
      "Epoch 2026/30000 Training Loss: 0.07163280248641968\n",
      "Epoch 2027/30000 Training Loss: 0.07332277297973633\n",
      "Epoch 2028/30000 Training Loss: 0.059526219964027405\n",
      "Epoch 2029/30000 Training Loss: 0.08456145972013474\n",
      "Epoch 2030/30000 Training Loss: 0.07378486543893814\n",
      "Epoch 2031/30000 Training Loss: 0.07138412445783615\n",
      "Epoch 2032/30000 Training Loss: 0.07016181945800781\n",
      "Epoch 2033/30000 Training Loss: 0.07215100526809692\n",
      "Epoch 2034/30000 Training Loss: 0.08441019058227539\n",
      "Epoch 2035/30000 Training Loss: 0.0660170391201973\n",
      "Epoch 2036/30000 Training Loss: 0.08478341996669769\n",
      "Epoch 2037/30000 Training Loss: 0.06929533183574677\n",
      "Epoch 2038/30000 Training Loss: 0.08225609362125397\n",
      "Epoch 2039/30000 Training Loss: 0.07318645715713501\n",
      "Epoch 2040/30000 Training Loss: 0.08364243060350418\n",
      "Epoch 2041/30000 Training Loss: 0.06816412508487701\n",
      "Epoch 2042/30000 Training Loss: 0.07469525933265686\n",
      "Epoch 2043/30000 Training Loss: 0.0668640062212944\n",
      "Epoch 2044/30000 Training Loss: 0.09349800646305084\n",
      "Epoch 2045/30000 Training Loss: 0.0719614252448082\n",
      "Epoch 2046/30000 Training Loss: 0.07183288037776947\n",
      "Epoch 2047/30000 Training Loss: 0.06835190206766129\n",
      "Epoch 2048/30000 Training Loss: 0.08029734343290329\n",
      "Epoch 2049/30000 Training Loss: 0.08687305450439453\n",
      "Epoch 2050/30000 Training Loss: 0.08186071366071701\n",
      "Epoch 2051/30000 Training Loss: 0.07240279018878937\n",
      "Epoch 2052/30000 Training Loss: 0.0732329860329628\n",
      "Epoch 2053/30000 Training Loss: 0.05860257148742676\n",
      "Epoch 2054/30000 Training Loss: 0.08425798267126083\n",
      "Epoch 2055/30000 Training Loss: 0.08027109503746033\n",
      "Epoch 2056/30000 Training Loss: 0.07393044233322144\n",
      "Epoch 2057/30000 Training Loss: 0.0868314728140831\n",
      "Epoch 2058/30000 Training Loss: 0.06889937072992325\n",
      "Epoch 2059/30000 Training Loss: 0.07996848225593567\n",
      "Epoch 2060/30000 Training Loss: 0.07609021663665771\n",
      "Epoch 2061/30000 Training Loss: 0.08265276253223419\n",
      "Epoch 2062/30000 Training Loss: 0.07271969318389893\n",
      "Epoch 2063/30000 Training Loss: 0.06314869970083237\n",
      "Epoch 2064/30000 Training Loss: 0.0656447485089302\n",
      "Epoch 2065/30000 Training Loss: 0.0607716403901577\n",
      "Epoch 2066/30000 Training Loss: 0.073588527739048\n",
      "Epoch 2067/30000 Training Loss: 0.07772324234247208\n",
      "Epoch 2068/30000 Training Loss: 0.0720653161406517\n",
      "Epoch 2069/30000 Training Loss: 0.07906398922204971\n",
      "Epoch 2070/30000 Training Loss: 0.07619372010231018\n",
      "Epoch 2071/30000 Training Loss: 0.07236022502183914\n",
      "Epoch 2072/30000 Training Loss: 0.07967020571231842\n",
      "Epoch 2073/30000 Training Loss: 0.06909344345331192\n",
      "Epoch 2074/30000 Training Loss: 0.0628143846988678\n",
      "Epoch 2075/30000 Training Loss: 0.10167884081602097\n",
      "Epoch 2076/30000 Training Loss: 0.08084258437156677\n",
      "Epoch 2077/30000 Training Loss: 0.0782075971364975\n",
      "Epoch 2078/30000 Training Loss: 0.09269197285175323\n",
      "Epoch 2079/30000 Training Loss: 0.07571066170930862\n",
      "Epoch 2080/30000 Training Loss: 0.08621760457754135\n",
      "Epoch 2081/30000 Training Loss: 0.061624638736248016\n",
      "Epoch 2082/30000 Training Loss: 0.07833251357078552\n",
      "Epoch 2083/30000 Training Loss: 0.08042676001787186\n",
      "Epoch 2084/30000 Training Loss: 0.07269261032342911\n",
      "Epoch 2085/30000 Training Loss: 0.0815100446343422\n",
      "Epoch 2086/30000 Training Loss: 0.057895462960004807\n",
      "Epoch 2087/30000 Training Loss: 0.08987303078174591\n",
      "Epoch 2088/30000 Training Loss: 0.07108816504478455\n",
      "Epoch 2089/30000 Training Loss: 0.07930628955364227\n",
      "Epoch 2090/30000 Training Loss: 0.06379726529121399\n",
      "Epoch 2091/30000 Training Loss: 0.0594969280064106\n",
      "Epoch 2092/30000 Training Loss: 0.0827953889966011\n",
      "Epoch 2093/30000 Training Loss: 0.07646701484918594\n",
      "Epoch 2094/30000 Training Loss: 0.07606194913387299\n",
      "Epoch 2095/30000 Training Loss: 0.0826718807220459\n",
      "Epoch 2096/30000 Training Loss: 0.07467908412218094\n",
      "Epoch 2097/30000 Training Loss: 0.08767509460449219\n",
      "Epoch 2098/30000 Training Loss: 0.07875286787748337\n",
      "Epoch 2099/30000 Training Loss: 0.06998738646507263\n",
      "Epoch 2100/30000 Training Loss: 0.073700912296772\n",
      "Epoch 2100/30000 Validation Loss: 0.08729618787765503\n",
      "Epoch 2101/30000 Training Loss: 0.06057320535182953\n",
      "Epoch 2102/30000 Training Loss: 0.09023619443178177\n",
      "Epoch 2103/30000 Training Loss: 0.0522267147898674\n",
      "Epoch 2104/30000 Training Loss: 0.07588192820549011\n",
      "Epoch 2105/30000 Training Loss: 0.07462748885154724\n",
      "Epoch 2106/30000 Training Loss: 0.09630080312490463\n",
      "Epoch 2107/30000 Training Loss: 0.08158029615879059\n",
      "Epoch 2108/30000 Training Loss: 0.0727590024471283\n",
      "Epoch 2109/30000 Training Loss: 0.07661449164152145\n",
      "Epoch 2110/30000 Training Loss: 0.07547842711210251\n",
      "Epoch 2111/30000 Training Loss: 0.07367870211601257\n",
      "Epoch 2112/30000 Training Loss: 0.06786327064037323\n",
      "Epoch 2113/30000 Training Loss: 0.08077637106180191\n",
      "Epoch 2114/30000 Training Loss: 0.091012142598629\n",
      "Epoch 2115/30000 Training Loss: 0.06928564608097076\n",
      "Epoch 2116/30000 Training Loss: 0.07615979760885239\n",
      "Epoch 2117/30000 Training Loss: 0.0725921243429184\n",
      "Epoch 2118/30000 Training Loss: 0.09401257336139679\n",
      "Epoch 2119/30000 Training Loss: 0.06082760542631149\n",
      "Epoch 2120/30000 Training Loss: 0.08652027696371078\n",
      "Epoch 2121/30000 Training Loss: 0.0687507838010788\n",
      "Epoch 2122/30000 Training Loss: 0.0742681473493576\n",
      "Epoch 2123/30000 Training Loss: 0.06344551593065262\n",
      "Epoch 2124/30000 Training Loss: 0.07866574078798294\n",
      "Epoch 2125/30000 Training Loss: 0.06864135712385178\n",
      "Epoch 2126/30000 Training Loss: 0.07657260447740555\n",
      "Epoch 2127/30000 Training Loss: 0.07486096024513245\n",
      "Epoch 2128/30000 Training Loss: 0.0798427164554596\n",
      "Epoch 2129/30000 Training Loss: 0.07577484101057053\n",
      "Epoch 2130/30000 Training Loss: 0.06511984765529633\n",
      "Epoch 2131/30000 Training Loss: 0.0775981917977333\n",
      "Epoch 2132/30000 Training Loss: 0.07953126728534698\n",
      "Epoch 2133/30000 Training Loss: 0.07004658132791519\n",
      "Epoch 2134/30000 Training Loss: 0.06807134300470352\n",
      "Epoch 2135/30000 Training Loss: 0.07734444737434387\n",
      "Epoch 2136/30000 Training Loss: 0.07081076502799988\n",
      "Epoch 2137/30000 Training Loss: 0.06334108114242554\n",
      "Epoch 2138/30000 Training Loss: 0.060097478330135345\n",
      "Epoch 2139/30000 Training Loss: 0.08332283794879913\n",
      "Epoch 2140/30000 Training Loss: 0.06851986795663834\n",
      "Epoch 2141/30000 Training Loss: 0.06191401928663254\n",
      "Epoch 2142/30000 Training Loss: 0.06287181377410889\n",
      "Epoch 2143/30000 Training Loss: 0.062957264482975\n",
      "Epoch 2144/30000 Training Loss: 0.05773283913731575\n",
      "Epoch 2145/30000 Training Loss: 0.07515168935060501\n",
      "Epoch 2146/30000 Training Loss: 0.06600632518529892\n",
      "Epoch 2147/30000 Training Loss: 0.06397811323404312\n",
      "Epoch 2148/30000 Training Loss: 0.07167225331068039\n",
      "Epoch 2149/30000 Training Loss: 0.0744766816496849\n",
      "Epoch 2150/30000 Training Loss: 0.08464442193508148\n",
      "Epoch 2151/30000 Training Loss: 0.08894558250904083\n",
      "Epoch 2152/30000 Training Loss: 0.06373436003923416\n",
      "Epoch 2153/30000 Training Loss: 0.059724628925323486\n",
      "Epoch 2154/30000 Training Loss: 0.08501394093036652\n",
      "Epoch 2155/30000 Training Loss: 0.07115784287452698\n",
      "Epoch 2156/30000 Training Loss: 0.08758871257305145\n",
      "Epoch 2157/30000 Training Loss: 0.05907975882291794\n",
      "Epoch 2158/30000 Training Loss: 0.07247580587863922\n",
      "Epoch 2159/30000 Training Loss: 0.06916924566030502\n",
      "Epoch 2160/30000 Training Loss: 0.056630730628967285\n",
      "Epoch 2161/30000 Training Loss: 0.0900694876909256\n",
      "Epoch 2162/30000 Training Loss: 0.07025840878486633\n",
      "Epoch 2163/30000 Training Loss: 0.08744900673627853\n",
      "Epoch 2164/30000 Training Loss: 0.07522514462471008\n",
      "Epoch 2165/30000 Training Loss: 0.06530066579580307\n",
      "Epoch 2166/30000 Training Loss: 0.08871890604496002\n",
      "Epoch 2167/30000 Training Loss: 0.06675221771001816\n",
      "Epoch 2168/30000 Training Loss: 0.09284807741641998\n",
      "Epoch 2169/30000 Training Loss: 0.06747803092002869\n",
      "Epoch 2170/30000 Training Loss: 0.07350679486989975\n",
      "Epoch 2171/30000 Training Loss: 0.09205029159784317\n",
      "Epoch 2172/30000 Training Loss: 0.07950738817453384\n",
      "Epoch 2173/30000 Training Loss: 0.07832929491996765\n",
      "Epoch 2174/30000 Training Loss: 0.08105910569429398\n",
      "Epoch 2175/30000 Training Loss: 0.07025013118982315\n",
      "Epoch 2176/30000 Training Loss: 0.06946274638175964\n",
      "Epoch 2177/30000 Training Loss: 0.08351545035839081\n",
      "Epoch 2178/30000 Training Loss: 0.07102810591459274\n",
      "Epoch 2179/30000 Training Loss: 0.06922425329685211\n",
      "Epoch 2180/30000 Training Loss: 0.0770263820886612\n",
      "Epoch 2181/30000 Training Loss: 0.06363434344530106\n",
      "Epoch 2182/30000 Training Loss: 0.07011173665523529\n",
      "Epoch 2183/30000 Training Loss: 0.06906484812498093\n",
      "Epoch 2184/30000 Training Loss: 0.074345663189888\n",
      "Epoch 2185/30000 Training Loss: 0.07073627412319183\n",
      "Epoch 2186/30000 Training Loss: 0.05815034732222557\n",
      "Epoch 2187/30000 Training Loss: 0.06891638785600662\n",
      "Epoch 2188/30000 Training Loss: 0.07182546705007553\n",
      "Epoch 2189/30000 Training Loss: 0.06105329841375351\n",
      "Epoch 2190/30000 Training Loss: 0.08192088454961777\n",
      "Epoch 2191/30000 Training Loss: 0.0763901025056839\n",
      "Epoch 2192/30000 Training Loss: 0.08583798259496689\n",
      "Epoch 2193/30000 Training Loss: 0.06704238057136536\n",
      "Epoch 2194/30000 Training Loss: 0.07318033277988434\n",
      "Epoch 2195/30000 Training Loss: 0.06753036379814148\n",
      "Epoch 2196/30000 Training Loss: 0.07438512146472931\n",
      "Epoch 2197/30000 Training Loss: 0.07630825787782669\n",
      "Epoch 2198/30000 Training Loss: 0.08432186394929886\n",
      "Epoch 2199/30000 Training Loss: 0.08554115146398544\n",
      "Epoch 2200/30000 Training Loss: 0.08547016233205795\n",
      "Epoch 2200/30000 Validation Loss: 0.07854343950748444\n",
      "Epoch 2201/30000 Training Loss: 0.06818366795778275\n",
      "Epoch 2202/30000 Training Loss: 0.06982574611902237\n",
      "Epoch 2203/30000 Training Loss: 0.06821715086698532\n",
      "Epoch 2204/30000 Training Loss: 0.07225102931261063\n",
      "Epoch 2205/30000 Training Loss: 0.07540106028318405\n",
      "Epoch 2206/30000 Training Loss: 0.07602939754724503\n",
      "Epoch 2207/30000 Training Loss: 0.06606169044971466\n",
      "Epoch 2208/30000 Training Loss: 0.08142884075641632\n",
      "Epoch 2209/30000 Training Loss: 0.06065654009580612\n",
      "Epoch 2210/30000 Training Loss: 0.08319763839244843\n",
      "Epoch 2211/30000 Training Loss: 0.07157988101243973\n",
      "Epoch 2212/30000 Training Loss: 0.0637214183807373\n",
      "Epoch 2213/30000 Training Loss: 0.09764480590820312\n",
      "Epoch 2214/30000 Training Loss: 0.05253495275974274\n",
      "Epoch 2215/30000 Training Loss: 0.07607637345790863\n",
      "Epoch 2216/30000 Training Loss: 0.09279928356409073\n",
      "Epoch 2217/30000 Training Loss: 0.08816584944725037\n",
      "Epoch 2218/30000 Training Loss: 0.07041175663471222\n",
      "Epoch 2219/30000 Training Loss: 0.05264223739504814\n",
      "Epoch 2220/30000 Training Loss: 0.07295577973127365\n",
      "Epoch 2221/30000 Training Loss: 0.06875945627689362\n",
      "Epoch 2222/30000 Training Loss: 0.07328341156244278\n",
      "Epoch 2223/30000 Training Loss: 0.06587254256010056\n",
      "Epoch 2224/30000 Training Loss: 0.08072248101234436\n",
      "Epoch 2225/30000 Training Loss: 0.06326695531606674\n",
      "Epoch 2226/30000 Training Loss: 0.06811490654945374\n",
      "Epoch 2227/30000 Training Loss: 0.09184446185827255\n",
      "Epoch 2228/30000 Training Loss: 0.08790072053670883\n",
      "Epoch 2229/30000 Training Loss: 0.07789073884487152\n",
      "Epoch 2230/30000 Training Loss: 0.08437743782997131\n",
      "Epoch 2231/30000 Training Loss: 0.07281870394945145\n",
      "Epoch 2232/30000 Training Loss: 0.07522305101156235\n",
      "Epoch 2233/30000 Training Loss: 0.06375311315059662\n",
      "Epoch 2234/30000 Training Loss: 0.0668877363204956\n",
      "Epoch 2235/30000 Training Loss: 0.07511051744222641\n",
      "Epoch 2236/30000 Training Loss: 0.06025366485118866\n",
      "Epoch 2237/30000 Training Loss: 0.0596579946577549\n",
      "Epoch 2238/30000 Training Loss: 0.07721950113773346\n",
      "Epoch 2239/30000 Training Loss: 0.07295328378677368\n",
      "Epoch 2240/30000 Training Loss: 0.07274993509054184\n",
      "Epoch 2241/30000 Training Loss: 0.07684658467769623\n",
      "Epoch 2242/30000 Training Loss: 0.08234933018684387\n",
      "Epoch 2243/30000 Training Loss: 0.07157225906848907\n",
      "Epoch 2244/30000 Training Loss: 0.07526848465204239\n",
      "Epoch 2245/30000 Training Loss: 0.06275510787963867\n",
      "Epoch 2246/30000 Training Loss: 0.0803600400686264\n",
      "Epoch 2247/30000 Training Loss: 0.060637786984443665\n",
      "Epoch 2248/30000 Training Loss: 0.08169764280319214\n",
      "Epoch 2249/30000 Training Loss: 0.0936662033200264\n",
      "Epoch 2250/30000 Training Loss: 0.08841004967689514\n",
      "Epoch 2251/30000 Training Loss: 0.08007140457630157\n",
      "Epoch 2252/30000 Training Loss: 0.07291033864021301\n",
      "Epoch 2253/30000 Training Loss: 0.07442602515220642\n",
      "Epoch 2254/30000 Training Loss: 0.06784757971763611\n",
      "Epoch 2255/30000 Training Loss: 0.07827076315879822\n",
      "Epoch 2256/30000 Training Loss: 0.06795240938663483\n",
      "Epoch 2257/30000 Training Loss: 0.08004102110862732\n",
      "Epoch 2258/30000 Training Loss: 0.062005601823329926\n",
      "Epoch 2259/30000 Training Loss: 0.06655165553092957\n",
      "Epoch 2260/30000 Training Loss: 0.08132903277873993\n",
      "Epoch 2261/30000 Training Loss: 0.07351628690958023\n",
      "Epoch 2262/30000 Training Loss: 0.08241473138332367\n",
      "Epoch 2263/30000 Training Loss: 0.07893749326467514\n",
      "Epoch 2264/30000 Training Loss: 0.07689064741134644\n",
      "Epoch 2265/30000 Training Loss: 0.06766681373119354\n",
      "Epoch 2266/30000 Training Loss: 0.07733556628227234\n",
      "Epoch 2267/30000 Training Loss: 0.08728206157684326\n",
      "Epoch 2268/30000 Training Loss: 0.06452841311693192\n",
      "Epoch 2269/30000 Training Loss: 0.06084955483675003\n",
      "Epoch 2270/30000 Training Loss: 0.07375801354646683\n",
      "Epoch 2271/30000 Training Loss: 0.06935244053602219\n",
      "Epoch 2272/30000 Training Loss: 0.06990505009889603\n",
      "Epoch 2273/30000 Training Loss: 0.07025691121816635\n",
      "Epoch 2274/30000 Training Loss: 0.06641454994678497\n",
      "Epoch 2275/30000 Training Loss: 0.07978232949972153\n",
      "Epoch 2276/30000 Training Loss: 0.07613310217857361\n",
      "Epoch 2277/30000 Training Loss: 0.0785815566778183\n",
      "Epoch 2278/30000 Training Loss: 0.08077892661094666\n",
      "Epoch 2279/30000 Training Loss: 0.06856317073106766\n",
      "Epoch 2280/30000 Training Loss: 0.07454288005828857\n",
      "Epoch 2281/30000 Training Loss: 0.06997145712375641\n",
      "Epoch 2282/30000 Training Loss: 0.07520265877246857\n",
      "Epoch 2283/30000 Training Loss: 0.0788850411772728\n",
      "Epoch 2284/30000 Training Loss: 0.0836755782365799\n",
      "Epoch 2285/30000 Training Loss: 0.07323488593101501\n",
      "Epoch 2286/30000 Training Loss: 0.08121133595705032\n",
      "Epoch 2287/30000 Training Loss: 0.07341726124286652\n",
      "Epoch 2288/30000 Training Loss: 0.06332532316446304\n",
      "Epoch 2289/30000 Training Loss: 0.059862978756427765\n",
      "Epoch 2290/30000 Training Loss: 0.07795533537864685\n",
      "Epoch 2291/30000 Training Loss: 0.06971503794193268\n",
      "Epoch 2292/30000 Training Loss: 0.08714542537927628\n",
      "Epoch 2293/30000 Training Loss: 0.06399363279342651\n",
      "Epoch 2294/30000 Training Loss: 0.07896634191274643\n",
      "Epoch 2295/30000 Training Loss: 0.08272260427474976\n",
      "Epoch 2296/30000 Training Loss: 0.08527890592813492\n",
      "Epoch 2297/30000 Training Loss: 0.06387250125408173\n",
      "Epoch 2298/30000 Training Loss: 0.08137676864862442\n",
      "Epoch 2299/30000 Training Loss: 0.09267531335353851\n",
      "Epoch 2300/30000 Training Loss: 0.08380180597305298\n",
      "Epoch 2300/30000 Validation Loss: 0.06328410655260086\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06328410655260086<=============\n",
      "Epoch 2301/30000 Training Loss: 0.06515030562877655\n",
      "Epoch 2302/30000 Training Loss: 0.06796446442604065\n",
      "Epoch 2303/30000 Training Loss: 0.07166928797960281\n",
      "Epoch 2304/30000 Training Loss: 0.08030248433351517\n",
      "Epoch 2305/30000 Training Loss: 0.06236244738101959\n",
      "Epoch 2306/30000 Training Loss: 0.07421395927667618\n",
      "Epoch 2307/30000 Training Loss: 0.0933929979801178\n",
      "Epoch 2308/30000 Training Loss: 0.06020250543951988\n",
      "Epoch 2309/30000 Training Loss: 0.05951159819960594\n",
      "Epoch 2310/30000 Training Loss: 0.07884057611227036\n",
      "Epoch 2311/30000 Training Loss: 0.0582088828086853\n",
      "Epoch 2312/30000 Training Loss: 0.07469329237937927\n",
      "Epoch 2313/30000 Training Loss: 0.07862967252731323\n",
      "Epoch 2314/30000 Training Loss: 0.08368013054132462\n",
      "Epoch 2315/30000 Training Loss: 0.07609974592924118\n",
      "Epoch 2316/30000 Training Loss: 0.07628949731588364\n",
      "Epoch 2317/30000 Training Loss: 0.07803916931152344\n",
      "Epoch 2318/30000 Training Loss: 0.08668556064367294\n",
      "Epoch 2319/30000 Training Loss: 0.08164659142494202\n",
      "Epoch 2320/30000 Training Loss: 0.0774967223405838\n",
      "Epoch 2321/30000 Training Loss: 0.07503703236579895\n",
      "Epoch 2322/30000 Training Loss: 0.07842184603214264\n",
      "Epoch 2323/30000 Training Loss: 0.08460458368062973\n",
      "Epoch 2324/30000 Training Loss: 0.07262818515300751\n",
      "Epoch 2325/30000 Training Loss: 0.08641223609447479\n",
      "Epoch 2326/30000 Training Loss: 0.07034501433372498\n",
      "Epoch 2327/30000 Training Loss: 0.0705583319067955\n",
      "Epoch 2328/30000 Training Loss: 0.07925417274236679\n",
      "Epoch 2329/30000 Training Loss: 0.07691328227519989\n",
      "Epoch 2330/30000 Training Loss: 0.08415071666240692\n",
      "Epoch 2331/30000 Training Loss: 0.0620221346616745\n",
      "Epoch 2332/30000 Training Loss: 0.09147997200489044\n",
      "Epoch 2333/30000 Training Loss: 0.08518040180206299\n",
      "Epoch 2334/30000 Training Loss: 0.05023226514458656\n",
      "Epoch 2335/30000 Training Loss: 0.05857941508293152\n",
      "Epoch 2336/30000 Training Loss: 0.08257632702589035\n",
      "Epoch 2337/30000 Training Loss: 0.06296121329069138\n",
      "Epoch 2338/30000 Training Loss: 0.06905616819858551\n",
      "Epoch 2339/30000 Training Loss: 0.06946375966072083\n",
      "Epoch 2340/30000 Training Loss: 0.07574924826622009\n",
      "Epoch 2341/30000 Training Loss: 0.0652133896946907\n",
      "Epoch 2342/30000 Training Loss: 0.08521240949630737\n",
      "Epoch 2343/30000 Training Loss: 0.07391534745693207\n",
      "Epoch 2344/30000 Training Loss: 0.07448175549507141\n",
      "Epoch 2345/30000 Training Loss: 0.08065205812454224\n",
      "Epoch 2346/30000 Training Loss: 0.07936111092567444\n",
      "Epoch 2347/30000 Training Loss: 0.06977765262126923\n",
      "Epoch 2348/30000 Training Loss: 0.06465074419975281\n",
      "Epoch 2349/30000 Training Loss: 0.09471391141414642\n",
      "Epoch 2350/30000 Training Loss: 0.05983693152666092\n",
      "Epoch 2351/30000 Training Loss: 0.07532400637865067\n",
      "Epoch 2352/30000 Training Loss: 0.06626308709383011\n",
      "Epoch 2353/30000 Training Loss: 0.09250880032777786\n",
      "Epoch 2354/30000 Training Loss: 0.06259696185588837\n",
      "Epoch 2355/30000 Training Loss: 0.060933370143175125\n",
      "Epoch 2356/30000 Training Loss: 0.06604254990816116\n",
      "Epoch 2357/30000 Training Loss: 0.06134258955717087\n",
      "Epoch 2358/30000 Training Loss: 0.06127038598060608\n",
      "Epoch 2359/30000 Training Loss: 0.0661349892616272\n",
      "Epoch 2360/30000 Training Loss: 0.0789651870727539\n",
      "Epoch 2361/30000 Training Loss: 0.08722133189439774\n",
      "Epoch 2362/30000 Training Loss: 0.06829102337360382\n",
      "Epoch 2363/30000 Training Loss: 0.06092359870672226\n",
      "Epoch 2364/30000 Training Loss: 0.0921890139579773\n",
      "Epoch 2365/30000 Training Loss: 0.07808183878660202\n",
      "Epoch 2366/30000 Training Loss: 0.07326114922761917\n",
      "Epoch 2367/30000 Training Loss: 0.07976435124874115\n",
      "Epoch 2368/30000 Training Loss: 0.06586384028196335\n",
      "Epoch 2369/30000 Training Loss: 0.06588052958250046\n",
      "Epoch 2370/30000 Training Loss: 0.07669655978679657\n",
      "Epoch 2371/30000 Training Loss: 0.07712825387716293\n",
      "Epoch 2372/30000 Training Loss: 0.09029187262058258\n",
      "Epoch 2373/30000 Training Loss: 0.06895364820957184\n",
      "Epoch 2374/30000 Training Loss: 0.07442035526037216\n",
      "Epoch 2375/30000 Training Loss: 0.08314896374940872\n",
      "Epoch 2376/30000 Training Loss: 0.07350273430347443\n",
      "Epoch 2377/30000 Training Loss: 0.06692896038293839\n",
      "Epoch 2378/30000 Training Loss: 0.08472630381584167\n",
      "Epoch 2379/30000 Training Loss: 0.07671549916267395\n",
      "Epoch 2380/30000 Training Loss: 0.072960764169693\n",
      "Epoch 2381/30000 Training Loss: 0.08441086113452911\n",
      "Epoch 2382/30000 Training Loss: 0.06822586059570312\n",
      "Epoch 2383/30000 Training Loss: 0.061696622520685196\n",
      "Epoch 2384/30000 Training Loss: 0.07545687258243561\n",
      "Epoch 2385/30000 Training Loss: 0.07096020132303238\n",
      "Epoch 2386/30000 Training Loss: 0.06743185222148895\n",
      "Epoch 2387/30000 Training Loss: 0.0622921884059906\n",
      "Epoch 2388/30000 Training Loss: 0.06645077466964722\n",
      "Epoch 2389/30000 Training Loss: 0.07776138186454773\n",
      "Epoch 2390/30000 Training Loss: 0.07960385084152222\n",
      "Epoch 2391/30000 Training Loss: 0.05952876806259155\n",
      "Epoch 2392/30000 Training Loss: 0.08039630949497223\n",
      "Epoch 2393/30000 Training Loss: 0.07618355751037598\n",
      "Epoch 2394/30000 Training Loss: 0.07879835367202759\n",
      "Epoch 2395/30000 Training Loss: 0.07876379042863846\n",
      "Epoch 2396/30000 Training Loss: 0.06877360492944717\n",
      "Epoch 2397/30000 Training Loss: 0.06580043584108353\n",
      "Epoch 2398/30000 Training Loss: 0.07555446773767471\n",
      "Epoch 2399/30000 Training Loss: 0.05999317765235901\n",
      "Epoch 2400/30000 Training Loss: 0.06757540255784988\n",
      "Epoch 2400/30000 Validation Loss: 0.06210550665855408\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06210550665855408<=============\n",
      "Epoch 2401/30000 Training Loss: 0.07565013319253922\n",
      "Epoch 2402/30000 Training Loss: 0.07666100561618805\n",
      "Epoch 2403/30000 Training Loss: 0.07012595236301422\n",
      "Epoch 2404/30000 Training Loss: 0.06407459080219269\n",
      "Epoch 2405/30000 Training Loss: 0.07269677519798279\n",
      "Epoch 2406/30000 Training Loss: 0.06487427651882172\n",
      "Epoch 2407/30000 Training Loss: 0.06718145310878754\n",
      "Epoch 2408/30000 Training Loss: 0.0686979591846466\n",
      "Epoch 2409/30000 Training Loss: 0.06744305789470673\n",
      "Epoch 2410/30000 Training Loss: 0.061201080679893494\n",
      "Epoch 2411/30000 Training Loss: 0.06662116944789886\n",
      "Epoch 2412/30000 Training Loss: 0.07200977206230164\n",
      "Epoch 2413/30000 Training Loss: 0.08867612481117249\n",
      "Epoch 2414/30000 Training Loss: 0.07891002297401428\n",
      "Epoch 2415/30000 Training Loss: 0.07635542005300522\n",
      "Epoch 2416/30000 Training Loss: 0.0812453180551529\n",
      "Epoch 2417/30000 Training Loss: 0.08089953660964966\n",
      "Epoch 2418/30000 Training Loss: 0.07300621271133423\n",
      "Epoch 2419/30000 Training Loss: 0.08545513451099396\n",
      "Epoch 2420/30000 Training Loss: 0.08677240461111069\n",
      "Epoch 2421/30000 Training Loss: 0.07150807976722717\n",
      "Epoch 2422/30000 Training Loss: 0.0808103010058403\n",
      "Epoch 2423/30000 Training Loss: 0.06419649720191956\n",
      "Epoch 2424/30000 Training Loss: 0.0716525986790657\n",
      "Epoch 2425/30000 Training Loss: 0.07508737593889236\n",
      "Epoch 2426/30000 Training Loss: 0.05407387763261795\n",
      "Epoch 2427/30000 Training Loss: 0.06724987179040909\n",
      "Epoch 2428/30000 Training Loss: 0.08601009100675583\n",
      "Epoch 2429/30000 Training Loss: 0.1006544977426529\n",
      "Epoch 2430/30000 Training Loss: 0.06931588798761368\n",
      "Epoch 2431/30000 Training Loss: 0.06471127271652222\n",
      "Epoch 2432/30000 Training Loss: 0.07168208062648773\n",
      "Epoch 2433/30000 Training Loss: 0.0609881728887558\n",
      "Epoch 2434/30000 Training Loss: 0.06322953850030899\n",
      "Epoch 2435/30000 Training Loss: 0.08432766050100327\n",
      "Epoch 2436/30000 Training Loss: 0.07361091673374176\n",
      "Epoch 2437/30000 Training Loss: 0.07083538174629211\n",
      "Epoch 2438/30000 Training Loss: 0.08294013142585754\n",
      "Epoch 2439/30000 Training Loss: 0.08237608522176743\n",
      "Epoch 2440/30000 Training Loss: 0.06769755482673645\n",
      "Epoch 2441/30000 Training Loss: 0.06303176283836365\n",
      "Epoch 2442/30000 Training Loss: 0.08384981751441956\n",
      "Epoch 2443/30000 Training Loss: 0.07380864769220352\n",
      "Epoch 2444/30000 Training Loss: 0.07079935073852539\n",
      "Epoch 2445/30000 Training Loss: 0.07207638770341873\n",
      "Epoch 2446/30000 Training Loss: 0.05798913538455963\n",
      "Epoch 2447/30000 Training Loss: 0.08373087644577026\n",
      "Epoch 2448/30000 Training Loss: 0.06651197373867035\n",
      "Epoch 2449/30000 Training Loss: 0.06976034492254257\n",
      "Epoch 2450/30000 Training Loss: 0.07516267895698547\n",
      "Epoch 2451/30000 Training Loss: 0.08482388406991959\n",
      "Epoch 2452/30000 Training Loss: 0.0901930034160614\n",
      "Epoch 2453/30000 Training Loss: 0.0703449621796608\n",
      "Epoch 2454/30000 Training Loss: 0.07203060388565063\n",
      "Epoch 2455/30000 Training Loss: 0.07500670850276947\n",
      "Epoch 2456/30000 Training Loss: 0.07692010700702667\n",
      "Epoch 2457/30000 Training Loss: 0.07806305587291718\n",
      "Epoch 2458/30000 Training Loss: 0.05868470296263695\n",
      "Epoch 2459/30000 Training Loss: 0.08334887772798538\n",
      "Epoch 2460/30000 Training Loss: 0.07418099790811539\n",
      "Epoch 2461/30000 Training Loss: 0.07884704321622849\n",
      "Epoch 2462/30000 Training Loss: 0.06769509613513947\n",
      "Epoch 2463/30000 Training Loss: 0.07537644356489182\n",
      "Epoch 2464/30000 Training Loss: 0.06774206459522247\n",
      "Epoch 2465/30000 Training Loss: 0.06517022848129272\n",
      "Epoch 2466/30000 Training Loss: 0.06945329904556274\n",
      "Epoch 2467/30000 Training Loss: 0.07533084601163864\n",
      "Epoch 2468/30000 Training Loss: 0.08047442883253098\n",
      "Epoch 2469/30000 Training Loss: 0.06956140697002411\n",
      "Epoch 2470/30000 Training Loss: 0.0777236670255661\n",
      "Epoch 2471/30000 Training Loss: 0.08228713274002075\n",
      "Epoch 2472/30000 Training Loss: 0.07896219193935394\n",
      "Epoch 2473/30000 Training Loss: 0.06842342019081116\n",
      "Epoch 2474/30000 Training Loss: 0.05681609734892845\n",
      "Epoch 2475/30000 Training Loss: 0.0639590471982956\n",
      "Epoch 2476/30000 Training Loss: 0.07117471098899841\n",
      "Epoch 2477/30000 Training Loss: 0.07286149263381958\n",
      "Epoch 2478/30000 Training Loss: 0.06154768168926239\n",
      "Epoch 2479/30000 Training Loss: 0.0742906928062439\n",
      "Epoch 2480/30000 Training Loss: 0.06285900622606277\n",
      "Epoch 2481/30000 Training Loss: 0.07752705365419388\n",
      "Epoch 2482/30000 Training Loss: 0.060756098479032516\n",
      "Epoch 2483/30000 Training Loss: 0.06549213826656342\n",
      "Epoch 2484/30000 Training Loss: 0.06516300141811371\n",
      "Epoch 2485/30000 Training Loss: 0.07162994146347046\n",
      "Epoch 2486/30000 Training Loss: 0.0750252828001976\n",
      "Epoch 2487/30000 Training Loss: 0.06398084759712219\n",
      "Epoch 2488/30000 Training Loss: 0.07456176728010178\n",
      "Epoch 2489/30000 Training Loss: 0.0662786141037941\n",
      "Epoch 2490/30000 Training Loss: 0.06006249785423279\n",
      "Epoch 2491/30000 Training Loss: 0.07390076667070389\n",
      "Epoch 2492/30000 Training Loss: 0.07048287242650986\n",
      "Epoch 2493/30000 Training Loss: 0.07179946452379227\n",
      "Epoch 2494/30000 Training Loss: 0.07032115757465363\n",
      "Epoch 2495/30000 Training Loss: 0.052815768867731094\n",
      "Epoch 2496/30000 Training Loss: 0.07416483759880066\n",
      "Epoch 2497/30000 Training Loss: 0.07269906997680664\n",
      "Epoch 2498/30000 Training Loss: 0.06939031928777695\n",
      "Epoch 2499/30000 Training Loss: 0.06498818099498749\n",
      "Epoch 2500/30000 Training Loss: 0.08010829985141754\n",
      "Epoch 2500/30000 Validation Loss: 0.08376682549715042\n",
      "Epoch 2501/30000 Training Loss: 0.08062337338924408\n",
      "Epoch 2502/30000 Training Loss: 0.0687800720334053\n",
      "Epoch 2503/30000 Training Loss: 0.07426014542579651\n",
      "Epoch 2504/30000 Training Loss: 0.07818019390106201\n",
      "Epoch 2505/30000 Training Loss: 0.0900641530752182\n",
      "Epoch 2506/30000 Training Loss: 0.07102721929550171\n",
      "Epoch 2507/30000 Training Loss: 0.052039653062820435\n",
      "Epoch 2508/30000 Training Loss: 0.06406460702419281\n",
      "Epoch 2509/30000 Training Loss: 0.06442993134260178\n",
      "Epoch 2510/30000 Training Loss: 0.059634678065776825\n",
      "Epoch 2511/30000 Training Loss: 0.0798707976937294\n",
      "Epoch 2512/30000 Training Loss: 0.07457887381315231\n",
      "Epoch 2513/30000 Training Loss: 0.08471199870109558\n",
      "Epoch 2514/30000 Training Loss: 0.07936219871044159\n",
      "Epoch 2515/30000 Training Loss: 0.07122837007045746\n",
      "Epoch 2516/30000 Training Loss: 0.07257872074842453\n",
      "Epoch 2517/30000 Training Loss: 0.08164157718420029\n",
      "Epoch 2518/30000 Training Loss: 0.08630314469337463\n",
      "Epoch 2519/30000 Training Loss: 0.0626918151974678\n",
      "Epoch 2520/30000 Training Loss: 0.06253276020288467\n",
      "Epoch 2521/30000 Training Loss: 0.07472873479127884\n",
      "Epoch 2522/30000 Training Loss: 0.0495794415473938\n",
      "Epoch 2523/30000 Training Loss: 0.0740782618522644\n",
      "Epoch 2524/30000 Training Loss: 0.0777350515127182\n",
      "Epoch 2525/30000 Training Loss: 0.0731784999370575\n",
      "Epoch 2526/30000 Training Loss: 0.06733498722314835\n",
      "Epoch 2527/30000 Training Loss: 0.08763489127159119\n",
      "Epoch 2528/30000 Training Loss: 0.06217379495501518\n",
      "Epoch 2529/30000 Training Loss: 0.054973237216472626\n",
      "Epoch 2530/30000 Training Loss: 0.07222358882427216\n",
      "Epoch 2531/30000 Training Loss: 0.055023908615112305\n",
      "Epoch 2532/30000 Training Loss: 0.06621649861335754\n",
      "Epoch 2533/30000 Training Loss: 0.07386846840381622\n",
      "Epoch 2534/30000 Training Loss: 0.08946406841278076\n",
      "Epoch 2535/30000 Training Loss: 0.06851230561733246\n",
      "Epoch 2536/30000 Training Loss: 0.08185896277427673\n",
      "Epoch 2537/30000 Training Loss: 0.07570340484380722\n",
      "Epoch 2538/30000 Training Loss: 0.0665968507528305\n",
      "Epoch 2539/30000 Training Loss: 0.07256878167390823\n",
      "Epoch 2540/30000 Training Loss: 0.07647230476140976\n",
      "Epoch 2541/30000 Training Loss: 0.0642390325665474\n",
      "Epoch 2542/30000 Training Loss: 0.06901168823242188\n",
      "Epoch 2543/30000 Training Loss: 0.06868035346269608\n",
      "Epoch 2544/30000 Training Loss: 0.07356800138950348\n",
      "Epoch 2545/30000 Training Loss: 0.07992269098758698\n",
      "Epoch 2546/30000 Training Loss: 0.07924219220876694\n",
      "Epoch 2547/30000 Training Loss: 0.0836811289191246\n",
      "Epoch 2548/30000 Training Loss: 0.06826178729534149\n",
      "Epoch 2549/30000 Training Loss: 0.08587786555290222\n",
      "Epoch 2550/30000 Training Loss: 0.08704142272472382\n",
      "Epoch 2551/30000 Training Loss: 0.08167567849159241\n",
      "Epoch 2552/30000 Training Loss: 0.07336515188217163\n",
      "Epoch 2553/30000 Training Loss: 0.06494300067424774\n",
      "Epoch 2554/30000 Training Loss: 0.0808289423584938\n",
      "Epoch 2555/30000 Training Loss: 0.0621795691549778\n",
      "Epoch 2556/30000 Training Loss: 0.08050774782896042\n",
      "Epoch 2557/30000 Training Loss: 0.07272659242153168\n",
      "Epoch 2558/30000 Training Loss: 0.0689142569899559\n",
      "Epoch 2559/30000 Training Loss: 0.06622611731290817\n",
      "Epoch 2560/30000 Training Loss: 0.06383903324604034\n",
      "Epoch 2561/30000 Training Loss: 0.07196521759033203\n",
      "Epoch 2562/30000 Training Loss: 0.0612797848880291\n",
      "Epoch 2563/30000 Training Loss: 0.08410276472568512\n",
      "Epoch 2564/30000 Training Loss: 0.08003050833940506\n",
      "Epoch 2565/30000 Training Loss: 0.0648515522480011\n",
      "Epoch 2566/30000 Training Loss: 0.09886133670806885\n",
      "Epoch 2567/30000 Training Loss: 0.05551406741142273\n",
      "Epoch 2568/30000 Training Loss: 0.06557373702526093\n",
      "Epoch 2569/30000 Training Loss: 0.05601729080080986\n",
      "Epoch 2570/30000 Training Loss: 0.06955032050609589\n",
      "Epoch 2571/30000 Training Loss: 0.05551008880138397\n",
      "Epoch 2572/30000 Training Loss: 0.07819243520498276\n",
      "Epoch 2573/30000 Training Loss: 0.05989483743906021\n",
      "Epoch 2574/30000 Training Loss: 0.07838663458824158\n",
      "Epoch 2575/30000 Training Loss: 0.07331392914056778\n",
      "Epoch 2576/30000 Training Loss: 0.08409901708364487\n",
      "Epoch 2577/30000 Training Loss: 0.06584176421165466\n",
      "Epoch 2578/30000 Training Loss: 0.06606555730104446\n",
      "Epoch 2579/30000 Training Loss: 0.0835646539926529\n",
      "Epoch 2580/30000 Training Loss: 0.07152523845434189\n",
      "Epoch 2581/30000 Training Loss: 0.07761949300765991\n",
      "Epoch 2582/30000 Training Loss: 0.06246037036180496\n",
      "Epoch 2583/30000 Training Loss: 0.07353601604700089\n",
      "Epoch 2584/30000 Training Loss: 0.061279598623514175\n",
      "Epoch 2585/30000 Training Loss: 0.07159923017024994\n",
      "Epoch 2586/30000 Training Loss: 0.06621310114860535\n",
      "Epoch 2587/30000 Training Loss: 0.07361441850662231\n",
      "Epoch 2588/30000 Training Loss: 0.06968087702989578\n",
      "Epoch 2589/30000 Training Loss: 0.0659315437078476\n",
      "Epoch 2590/30000 Training Loss: 0.06988884508609772\n",
      "Epoch 2591/30000 Training Loss: 0.052228741347789764\n",
      "Epoch 2592/30000 Training Loss: 0.08374401926994324\n",
      "Epoch 2593/30000 Training Loss: 0.07031407952308655\n",
      "Epoch 2594/30000 Training Loss: 0.07253128290176392\n",
      "Epoch 2595/30000 Training Loss: 0.050804149359464645\n",
      "Epoch 2596/30000 Training Loss: 0.07896657288074493\n",
      "Epoch 2597/30000 Training Loss: 0.062180254608392715\n",
      "Epoch 2598/30000 Training Loss: 0.0635983869433403\n",
      "Epoch 2599/30000 Training Loss: 0.06335027515888214\n",
      "Epoch 2600/30000 Training Loss: 0.08860431611537933\n",
      "Epoch 2600/30000 Validation Loss: 0.06491032242774963\n",
      "Epoch 2601/30000 Training Loss: 0.07835299521684647\n",
      "Epoch 2602/30000 Training Loss: 0.059700556099414825\n",
      "Epoch 2603/30000 Training Loss: 0.07649389654397964\n",
      "Epoch 2604/30000 Training Loss: 0.08547129482030869\n",
      "Epoch 2605/30000 Training Loss: 0.06647240370512009\n",
      "Epoch 2606/30000 Training Loss: 0.07317974418401718\n",
      "Epoch 2607/30000 Training Loss: 0.06517164409160614\n",
      "Epoch 2608/30000 Training Loss: 0.0669766217470169\n",
      "Epoch 2609/30000 Training Loss: 0.06568700075149536\n",
      "Epoch 2610/30000 Training Loss: 0.06630101054906845\n",
      "Epoch 2611/30000 Training Loss: 0.052727945148944855\n",
      "Epoch 2612/30000 Training Loss: 0.0765109732747078\n",
      "Epoch 2613/30000 Training Loss: 0.06813988834619522\n",
      "Epoch 2614/30000 Training Loss: 0.07716236263513565\n",
      "Epoch 2615/30000 Training Loss: 0.09653709083795547\n",
      "Epoch 2616/30000 Training Loss: 0.07971812784671783\n",
      "Epoch 2617/30000 Training Loss: 0.07548466324806213\n",
      "Epoch 2618/30000 Training Loss: 0.06100121885538101\n",
      "Epoch 2619/30000 Training Loss: 0.06101161241531372\n",
      "Epoch 2620/30000 Training Loss: 0.08384730666875839\n",
      "Epoch 2621/30000 Training Loss: 0.07443156838417053\n",
      "Epoch 2622/30000 Training Loss: 0.07149942219257355\n",
      "Epoch 2623/30000 Training Loss: 0.056453656405210495\n",
      "Epoch 2624/30000 Training Loss: 0.09336525946855545\n",
      "Epoch 2625/30000 Training Loss: 0.05648808181285858\n",
      "Epoch 2626/30000 Training Loss: 0.07552363723516464\n",
      "Epoch 2627/30000 Training Loss: 0.06986978650093079\n",
      "Epoch 2628/30000 Training Loss: 0.08366638422012329\n",
      "Epoch 2629/30000 Training Loss: 0.06173066049814224\n",
      "Epoch 2630/30000 Training Loss: 0.0645347312092781\n",
      "Epoch 2631/30000 Training Loss: 0.07103779166936874\n",
      "Epoch 2632/30000 Training Loss: 0.07803129404783249\n",
      "Epoch 2633/30000 Training Loss: 0.0680924504995346\n",
      "Epoch 2634/30000 Training Loss: 0.07254356890916824\n",
      "Epoch 2635/30000 Training Loss: 0.05488903075456619\n",
      "Epoch 2636/30000 Training Loss: 0.06098417937755585\n",
      "Epoch 2637/30000 Training Loss: 0.08163493871688843\n",
      "Epoch 2638/30000 Training Loss: 0.09661340713500977\n",
      "Epoch 2639/30000 Training Loss: 0.07603722810745239\n",
      "Epoch 2640/30000 Training Loss: 0.06905345618724823\n",
      "Epoch 2641/30000 Training Loss: 0.06925325095653534\n",
      "Epoch 2642/30000 Training Loss: 0.05582275614142418\n",
      "Epoch 2643/30000 Training Loss: 0.0632520467042923\n",
      "Epoch 2644/30000 Training Loss: 0.08859352767467499\n",
      "Epoch 2645/30000 Training Loss: 0.08092611283063889\n",
      "Epoch 2646/30000 Training Loss: 0.06907577812671661\n",
      "Epoch 2647/30000 Training Loss: 0.07632064819335938\n",
      "Epoch 2648/30000 Training Loss: 0.059576697647571564\n",
      "Epoch 2649/30000 Training Loss: 0.06293464452028275\n",
      "Epoch 2650/30000 Training Loss: 0.07115409523248672\n",
      "Epoch 2651/30000 Training Loss: 0.06036227568984032\n",
      "Epoch 2652/30000 Training Loss: 0.06720724701881409\n",
      "Epoch 2653/30000 Training Loss: 0.057498008012771606\n",
      "Epoch 2654/30000 Training Loss: 0.08599554002285004\n",
      "Epoch 2655/30000 Training Loss: 0.06856106966733932\n",
      "Epoch 2656/30000 Training Loss: 0.0680551528930664\n",
      "Epoch 2657/30000 Training Loss: 0.07161876559257507\n",
      "Epoch 2658/30000 Training Loss: 0.05663134157657623\n",
      "Epoch 2659/30000 Training Loss: 0.056855641305446625\n",
      "Epoch 2660/30000 Training Loss: 0.06292346864938736\n",
      "Epoch 2661/30000 Training Loss: 0.06312024593353271\n",
      "Epoch 2662/30000 Training Loss: 0.08231121301651001\n",
      "Epoch 2663/30000 Training Loss: 0.061062391847372055\n",
      "Epoch 2664/30000 Training Loss: 0.06299775093793869\n",
      "Epoch 2665/30000 Training Loss: 0.07030028849840164\n",
      "Epoch 2666/30000 Training Loss: 0.06641668826341629\n",
      "Epoch 2667/30000 Training Loss: 0.09327119588851929\n",
      "Epoch 2668/30000 Training Loss: 0.07330724596977234\n",
      "Epoch 2669/30000 Training Loss: 0.06333205103874207\n",
      "Epoch 2670/30000 Training Loss: 0.06835801154375076\n",
      "Epoch 2671/30000 Training Loss: 0.07420212030410767\n",
      "Epoch 2672/30000 Training Loss: 0.08425785601139069\n",
      "Epoch 2673/30000 Training Loss: 0.06977619975805283\n",
      "Epoch 2674/30000 Training Loss: 0.08364234864711761\n",
      "Epoch 2675/30000 Training Loss: 0.07924923300743103\n",
      "Epoch 2676/30000 Training Loss: 0.061667948961257935\n",
      "Epoch 2677/30000 Training Loss: 0.08313020318746567\n",
      "Epoch 2678/30000 Training Loss: 0.0805392786860466\n",
      "Epoch 2679/30000 Training Loss: 0.075743168592453\n",
      "Epoch 2680/30000 Training Loss: 0.07305443286895752\n",
      "Epoch 2681/30000 Training Loss: 0.0800561010837555\n",
      "Epoch 2682/30000 Training Loss: 0.060617201030254364\n",
      "Epoch 2683/30000 Training Loss: 0.06058242917060852\n",
      "Epoch 2684/30000 Training Loss: 0.0658368170261383\n",
      "Epoch 2685/30000 Training Loss: 0.0677742063999176\n",
      "Epoch 2686/30000 Training Loss: 0.06559736281633377\n",
      "Epoch 2687/30000 Training Loss: 0.05722953751683235\n",
      "Epoch 2688/30000 Training Loss: 0.06846514344215393\n",
      "Epoch 2689/30000 Training Loss: 0.062459930777549744\n",
      "Epoch 2690/30000 Training Loss: 0.07286182045936584\n",
      "Epoch 2691/30000 Training Loss: 0.07343880087137222\n",
      "Epoch 2692/30000 Training Loss: 0.09300410747528076\n",
      "Epoch 2693/30000 Training Loss: 0.07223770767450333\n",
      "Epoch 2694/30000 Training Loss: 0.06291484087705612\n",
      "Epoch 2695/30000 Training Loss: 0.07822651416063309\n",
      "Epoch 2696/30000 Training Loss: 0.05974858999252319\n",
      "Epoch 2697/30000 Training Loss: 0.08653784543275833\n",
      "Epoch 2698/30000 Training Loss: 0.056507423520088196\n",
      "Epoch 2699/30000 Training Loss: 0.05320122092962265\n",
      "Epoch 2700/30000 Training Loss: 0.062364958226680756\n",
      "Epoch 2700/30000 Validation Loss: 0.07526398450136185\n",
      "Epoch 2701/30000 Training Loss: 0.06561141461133957\n",
      "Epoch 2702/30000 Training Loss: 0.05941861867904663\n",
      "Epoch 2703/30000 Training Loss: 0.10206812620162964\n",
      "Epoch 2704/30000 Training Loss: 0.06404761224985123\n",
      "Epoch 2705/30000 Training Loss: 0.0803888812661171\n",
      "Epoch 2706/30000 Training Loss: 0.07574011385440826\n",
      "Epoch 2707/30000 Training Loss: 0.08577301353216171\n",
      "Epoch 2708/30000 Training Loss: 0.07911726087331772\n",
      "Epoch 2709/30000 Training Loss: 0.0717782974243164\n",
      "Epoch 2710/30000 Training Loss: 0.07877461612224579\n",
      "Epoch 2711/30000 Training Loss: 0.07600816339254379\n",
      "Epoch 2712/30000 Training Loss: 0.06845928728580475\n",
      "Epoch 2713/30000 Training Loss: 0.07809512317180634\n",
      "Epoch 2714/30000 Training Loss: 0.07174833863973618\n",
      "Epoch 2715/30000 Training Loss: 0.05969309061765671\n",
      "Epoch 2716/30000 Training Loss: 0.09199067205190659\n",
      "Epoch 2717/30000 Training Loss: 0.07290810346603394\n",
      "Epoch 2718/30000 Training Loss: 0.07399731874465942\n",
      "Epoch 2719/30000 Training Loss: 0.07847993075847626\n",
      "Epoch 2720/30000 Training Loss: 0.06437113136053085\n",
      "Epoch 2721/30000 Training Loss: 0.06469316780567169\n",
      "Epoch 2722/30000 Training Loss: 0.06035859137773514\n",
      "Epoch 2723/30000 Training Loss: 0.06222302466630936\n",
      "Epoch 2724/30000 Training Loss: 0.07052220404148102\n",
      "Epoch 2725/30000 Training Loss: 0.06667155772447586\n",
      "Epoch 2726/30000 Training Loss: 0.06618058681488037\n",
      "Epoch 2727/30000 Training Loss: 0.07950209826231003\n",
      "Epoch 2728/30000 Training Loss: 0.04707523062825203\n",
      "Epoch 2729/30000 Training Loss: 0.06518383324146271\n",
      "Epoch 2730/30000 Training Loss: 0.0739932581782341\n",
      "Epoch 2731/30000 Training Loss: 0.06449288129806519\n",
      "Epoch 2732/30000 Training Loss: 0.05663631111383438\n",
      "Epoch 2733/30000 Training Loss: 0.06320105493068695\n",
      "Epoch 2734/30000 Training Loss: 0.07345442473888397\n",
      "Epoch 2735/30000 Training Loss: 0.06897513568401337\n",
      "Epoch 2736/30000 Training Loss: 0.06605659425258636\n",
      "Epoch 2737/30000 Training Loss: 0.057980190962553024\n",
      "Epoch 2738/30000 Training Loss: 0.05938931927084923\n",
      "Epoch 2739/30000 Training Loss: 0.07908676564693451\n",
      "Epoch 2740/30000 Training Loss: 0.06837791204452515\n",
      "Epoch 2741/30000 Training Loss: 0.0544254407286644\n",
      "Epoch 2742/30000 Training Loss: 0.055948834866285324\n",
      "Epoch 2743/30000 Training Loss: 0.0744556337594986\n",
      "Epoch 2744/30000 Training Loss: 0.07379144430160522\n",
      "Epoch 2745/30000 Training Loss: 0.0709468200802803\n",
      "Epoch 2746/30000 Training Loss: 0.06032439321279526\n",
      "Epoch 2747/30000 Training Loss: 0.06308004260063171\n",
      "Epoch 2748/30000 Training Loss: 0.08495629578828812\n",
      "Epoch 2749/30000 Training Loss: 0.06419866532087326\n",
      "Epoch 2750/30000 Training Loss: 0.0752531960606575\n",
      "Epoch 2751/30000 Training Loss: 0.07710254937410355\n",
      "Epoch 2752/30000 Training Loss: 0.0677466094493866\n",
      "Epoch 2753/30000 Training Loss: 0.06184627115726471\n",
      "Epoch 2754/30000 Training Loss: 0.08319785445928574\n",
      "Epoch 2755/30000 Training Loss: 0.07400112599134445\n",
      "Epoch 2756/30000 Training Loss: 0.06724872440099716\n",
      "Epoch 2757/30000 Training Loss: 0.06850846856832504\n",
      "Epoch 2758/30000 Training Loss: 0.07796122133731842\n",
      "Epoch 2759/30000 Training Loss: 0.07030337303876877\n",
      "Epoch 2760/30000 Training Loss: 0.06074286252260208\n",
      "Epoch 2761/30000 Training Loss: 0.05944184958934784\n",
      "Epoch 2762/30000 Training Loss: 0.0637115091085434\n",
      "Epoch 2763/30000 Training Loss: 0.08150795102119446\n",
      "Epoch 2764/30000 Training Loss: 0.07153952866792679\n",
      "Epoch 2765/30000 Training Loss: 0.08032044023275375\n",
      "Epoch 2766/30000 Training Loss: 0.07513393461704254\n",
      "Epoch 2767/30000 Training Loss: 0.07907664030790329\n",
      "Epoch 2768/30000 Training Loss: 0.08044464886188507\n",
      "Epoch 2769/30000 Training Loss: 0.07370506227016449\n",
      "Epoch 2770/30000 Training Loss: 0.08361417055130005\n",
      "Epoch 2771/30000 Training Loss: 0.06212379038333893\n",
      "Epoch 2772/30000 Training Loss: 0.07673558592796326\n",
      "Epoch 2773/30000 Training Loss: 0.0822291150689125\n",
      "Epoch 2774/30000 Training Loss: 0.09343650192022324\n",
      "Epoch 2775/30000 Training Loss: 0.0720406025648117\n",
      "Epoch 2776/30000 Training Loss: 0.0666307881474495\n",
      "Epoch 2777/30000 Training Loss: 0.06729565560817719\n",
      "Epoch 2778/30000 Training Loss: 0.07439207285642624\n",
      "Epoch 2779/30000 Training Loss: 0.06895431876182556\n",
      "Epoch 2780/30000 Training Loss: 0.06560909003019333\n",
      "Epoch 2781/30000 Training Loss: 0.08259502053260803\n",
      "Epoch 2782/30000 Training Loss: 0.08022019267082214\n",
      "Epoch 2783/30000 Training Loss: 0.08582080900669098\n",
      "Epoch 2784/30000 Training Loss: 0.06587640196084976\n",
      "Epoch 2785/30000 Training Loss: 0.06466367095708847\n",
      "Epoch 2786/30000 Training Loss: 0.08517686277627945\n",
      "Epoch 2787/30000 Training Loss: 0.07382199913263321\n",
      "Epoch 2788/30000 Training Loss: 0.06676709651947021\n",
      "Epoch 2789/30000 Training Loss: 0.08874885737895966\n",
      "Epoch 2790/30000 Training Loss: 0.08856052160263062\n",
      "Epoch 2791/30000 Training Loss: 0.07473611831665039\n",
      "Epoch 2792/30000 Training Loss: 0.06571607291698456\n",
      "Epoch 2793/30000 Training Loss: 0.09242831915616989\n",
      "Epoch 2794/30000 Training Loss: 0.07740417122840881\n",
      "Epoch 2795/30000 Training Loss: 0.06134794279932976\n",
      "Epoch 2796/30000 Training Loss: 0.06468136608600616\n",
      "Epoch 2797/30000 Training Loss: 0.06424911320209503\n",
      "Epoch 2798/30000 Training Loss: 0.07515964657068253\n",
      "Epoch 2799/30000 Training Loss: 0.06624434888362885\n",
      "Epoch 2800/30000 Training Loss: 0.06824510544538498\n",
      "Epoch 2800/30000 Validation Loss: 0.09113088995218277\n",
      "Epoch 2801/30000 Training Loss: 0.05543597787618637\n",
      "Epoch 2802/30000 Training Loss: 0.07111489027738571\n",
      "Epoch 2803/30000 Training Loss: 0.07547293603420258\n",
      "Epoch 2804/30000 Training Loss: 0.06527900695800781\n",
      "Epoch 2805/30000 Training Loss: 0.06817635893821716\n",
      "Epoch 2806/30000 Training Loss: 0.06932083517313004\n",
      "Epoch 2807/30000 Training Loss: 0.0879913792014122\n",
      "Epoch 2808/30000 Training Loss: 0.07456134259700775\n",
      "Epoch 2809/30000 Training Loss: 0.07339184731245041\n",
      "Epoch 2810/30000 Training Loss: 0.06217372789978981\n",
      "Epoch 2811/30000 Training Loss: 0.07786765694618225\n",
      "Epoch 2812/30000 Training Loss: 0.059079624712467194\n",
      "Epoch 2813/30000 Training Loss: 0.06786578893661499\n",
      "Epoch 2814/30000 Training Loss: 0.06162843480706215\n",
      "Epoch 2815/30000 Training Loss: 0.06751126050949097\n",
      "Epoch 2816/30000 Training Loss: 0.07040640711784363\n",
      "Epoch 2817/30000 Training Loss: 0.07971394807100296\n",
      "Epoch 2818/30000 Training Loss: 0.0620676651597023\n",
      "Epoch 2819/30000 Training Loss: 0.07708647102117538\n",
      "Epoch 2820/30000 Training Loss: 0.09141847491264343\n",
      "Epoch 2821/30000 Training Loss: 0.07457717508077621\n",
      "Epoch 2822/30000 Training Loss: 0.06107194349169731\n",
      "Epoch 2823/30000 Training Loss: 0.08560214936733246\n",
      "Epoch 2824/30000 Training Loss: 0.05985977500677109\n",
      "Epoch 2825/30000 Training Loss: 0.06773421168327332\n",
      "Epoch 2826/30000 Training Loss: 0.07701805979013443\n",
      "Epoch 2827/30000 Training Loss: 0.06108119338750839\n",
      "Epoch 2828/30000 Training Loss: 0.08654403686523438\n",
      "Epoch 2829/30000 Training Loss: 0.06531829386949539\n",
      "Epoch 2830/30000 Training Loss: 0.07164113223552704\n",
      "Epoch 2831/30000 Training Loss: 0.06912165135145187\n",
      "Epoch 2832/30000 Training Loss: 0.07636469602584839\n",
      "Epoch 2833/30000 Training Loss: 0.07428139448165894\n",
      "Epoch 2834/30000 Training Loss: 0.061896853148937225\n",
      "Epoch 2835/30000 Training Loss: 0.06461416184902191\n",
      "Epoch 2836/30000 Training Loss: 0.057741422206163406\n",
      "Epoch 2837/30000 Training Loss: 0.07093646377325058\n",
      "Epoch 2838/30000 Training Loss: 0.08016005158424377\n",
      "Epoch 2839/30000 Training Loss: 0.07059986144304276\n",
      "Epoch 2840/30000 Training Loss: 0.08373381197452545\n",
      "Epoch 2841/30000 Training Loss: 0.08001845329999924\n",
      "Epoch 2842/30000 Training Loss: 0.06189270317554474\n",
      "Epoch 2843/30000 Training Loss: 0.08256691694259644\n",
      "Epoch 2844/30000 Training Loss: 0.0844581127166748\n",
      "Epoch 2845/30000 Training Loss: 0.06727253645658493\n",
      "Epoch 2846/30000 Training Loss: 0.07748250663280487\n",
      "Epoch 2847/30000 Training Loss: 0.05932151526212692\n",
      "Epoch 2848/30000 Training Loss: 0.049266815185546875\n",
      "Epoch 2849/30000 Training Loss: 0.07224702835083008\n",
      "Epoch 2850/30000 Training Loss: 0.05932508409023285\n",
      "Epoch 2851/30000 Training Loss: 0.06366773694753647\n",
      "Epoch 2852/30000 Training Loss: 0.06661015003919601\n",
      "Epoch 2853/30000 Training Loss: 0.0707780197262764\n",
      "Epoch 2854/30000 Training Loss: 0.0646141767501831\n",
      "Epoch 2855/30000 Training Loss: 0.07759659737348557\n",
      "Epoch 2856/30000 Training Loss: 0.06599284708499908\n",
      "Epoch 2857/30000 Training Loss: 0.0758896991610527\n",
      "Epoch 2858/30000 Training Loss: 0.05965857207775116\n",
      "Epoch 2859/30000 Training Loss: 0.06859323382377625\n",
      "Epoch 2860/30000 Training Loss: 0.08121434599161148\n",
      "Epoch 2861/30000 Training Loss: 0.10276924073696136\n",
      "Epoch 2862/30000 Training Loss: 0.062265053391456604\n",
      "Epoch 2863/30000 Training Loss: 0.08945098519325256\n",
      "Epoch 2864/30000 Training Loss: 0.06945265084505081\n",
      "Epoch 2865/30000 Training Loss: 0.08641095459461212\n",
      "Epoch 2866/30000 Training Loss: 0.0841541662812233\n",
      "Epoch 2867/30000 Training Loss: 0.09231641888618469\n",
      "Epoch 2868/30000 Training Loss: 0.08769809454679489\n",
      "Epoch 2869/30000 Training Loss: 0.06614379584789276\n",
      "Epoch 2870/30000 Training Loss: 0.06461027264595032\n",
      "Epoch 2871/30000 Training Loss: 0.0649736225605011\n",
      "Epoch 2872/30000 Training Loss: 0.06380520761013031\n",
      "Epoch 2873/30000 Training Loss: 0.07313099503517151\n",
      "Epoch 2874/30000 Training Loss: 0.06477817893028259\n",
      "Epoch 2875/30000 Training Loss: 0.05898095667362213\n",
      "Epoch 2876/30000 Training Loss: 0.06899314373731613\n",
      "Epoch 2877/30000 Training Loss: 0.0655413568019867\n",
      "Epoch 2878/30000 Training Loss: 0.07210442423820496\n",
      "Epoch 2879/30000 Training Loss: 0.06476781517267227\n",
      "Epoch 2880/30000 Training Loss: 0.072449691593647\n",
      "Epoch 2881/30000 Training Loss: 0.06880975514650345\n",
      "Epoch 2882/30000 Training Loss: 0.07233072072267532\n",
      "Epoch 2883/30000 Training Loss: 0.0791916698217392\n",
      "Epoch 2884/30000 Training Loss: 0.07525184005498886\n",
      "Epoch 2885/30000 Training Loss: 0.07064759731292725\n",
      "Epoch 2886/30000 Training Loss: 0.06294041126966476\n",
      "Epoch 2887/30000 Training Loss: 0.08008728921413422\n",
      "Epoch 2888/30000 Training Loss: 0.06890685111284256\n",
      "Epoch 2889/30000 Training Loss: 0.0849664956331253\n",
      "Epoch 2890/30000 Training Loss: 0.05639691650867462\n",
      "Epoch 2891/30000 Training Loss: 0.05479990690946579\n",
      "Epoch 2892/30000 Training Loss: 0.07453266531229019\n",
      "Epoch 2893/30000 Training Loss: 0.05199095979332924\n",
      "Epoch 2894/30000 Training Loss: 0.06389405578374863\n",
      "Epoch 2895/30000 Training Loss: 0.0888054296374321\n",
      "Epoch 2896/30000 Training Loss: 0.06940321624279022\n",
      "Epoch 2897/30000 Training Loss: 0.08035211265087128\n",
      "Epoch 2898/30000 Training Loss: 0.09034638106822968\n",
      "Epoch 2899/30000 Training Loss: 0.06417201459407806\n",
      "Epoch 2900/30000 Training Loss: 0.08467137813568115\n",
      "Epoch 2900/30000 Validation Loss: 0.06186385825276375\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06186385825276375<=============\n",
      "Epoch 2901/30000 Training Loss: 0.07405366003513336\n",
      "Epoch 2902/30000 Training Loss: 0.05943218618631363\n",
      "Epoch 2903/30000 Training Loss: 0.07920915633440018\n",
      "Epoch 2904/30000 Training Loss: 0.06259741634130478\n",
      "Epoch 2905/30000 Training Loss: 0.0798719972372055\n",
      "Epoch 2906/30000 Training Loss: 0.06952020525932312\n",
      "Epoch 2907/30000 Training Loss: 0.07245100289583206\n",
      "Epoch 2908/30000 Training Loss: 0.05812609940767288\n",
      "Epoch 2909/30000 Training Loss: 0.06193043664097786\n",
      "Epoch 2910/30000 Training Loss: 0.059292178601026535\n",
      "Epoch 2911/30000 Training Loss: 0.06301330029964447\n",
      "Epoch 2912/30000 Training Loss: 0.06320033222436905\n",
      "Epoch 2913/30000 Training Loss: 0.08119085431098938\n",
      "Epoch 2914/30000 Training Loss: 0.06524362415075302\n",
      "Epoch 2915/30000 Training Loss: 0.08025314658880234\n",
      "Epoch 2916/30000 Training Loss: 0.08161859959363937\n",
      "Epoch 2917/30000 Training Loss: 0.05502341315150261\n",
      "Epoch 2918/30000 Training Loss: 0.08675596863031387\n",
      "Epoch 2919/30000 Training Loss: 0.06787345558404922\n",
      "Epoch 2920/30000 Training Loss: 0.07136593759059906\n",
      "Epoch 2921/30000 Training Loss: 0.06516308337450027\n",
      "Epoch 2922/30000 Training Loss: 0.06776785850524902\n",
      "Epoch 2923/30000 Training Loss: 0.058559197932481766\n",
      "Epoch 2924/30000 Training Loss: 0.07918049395084381\n",
      "Epoch 2925/30000 Training Loss: 0.0846085473895073\n",
      "Epoch 2926/30000 Training Loss: 0.07034264504909515\n",
      "Epoch 2927/30000 Training Loss: 0.06778714060783386\n",
      "Epoch 2928/30000 Training Loss: 0.08267834037542343\n",
      "Epoch 2929/30000 Training Loss: 0.07972557097673416\n",
      "Epoch 2930/30000 Training Loss: 0.05146180838346481\n",
      "Epoch 2931/30000 Training Loss: 0.06624484062194824\n",
      "Epoch 2932/30000 Training Loss: 0.08186987787485123\n",
      "Epoch 2933/30000 Training Loss: 0.06715744733810425\n",
      "Epoch 2934/30000 Training Loss: 0.06800548732280731\n",
      "Epoch 2935/30000 Training Loss: 0.05325087904930115\n",
      "Epoch 2936/30000 Training Loss: 0.07225984334945679\n",
      "Epoch 2937/30000 Training Loss: 0.060733892023563385\n",
      "Epoch 2938/30000 Training Loss: 0.06702405214309692\n",
      "Epoch 2939/30000 Training Loss: 0.05051063001155853\n",
      "Epoch 2940/30000 Training Loss: 0.06585102528333664\n",
      "Epoch 2941/30000 Training Loss: 0.0777459666132927\n",
      "Epoch 2942/30000 Training Loss: 0.07035289704799652\n",
      "Epoch 2943/30000 Training Loss: 0.06239693611860275\n",
      "Epoch 2944/30000 Training Loss: 0.06391455978155136\n",
      "Epoch 2945/30000 Training Loss: 0.07029027491807938\n",
      "Epoch 2946/30000 Training Loss: 0.06542956829071045\n",
      "Epoch 2947/30000 Training Loss: 0.08514468371868134\n",
      "Epoch 2948/30000 Training Loss: 0.07256059348583221\n",
      "Epoch 2949/30000 Training Loss: 0.0763699859380722\n",
      "Epoch 2950/30000 Training Loss: 0.06853310018777847\n",
      "Epoch 2951/30000 Training Loss: 0.055087264627218246\n",
      "Epoch 2952/30000 Training Loss: 0.0671081617474556\n",
      "Epoch 2953/30000 Training Loss: 0.06535188853740692\n",
      "Epoch 2954/30000 Training Loss: 0.07470913231372833\n",
      "Epoch 2955/30000 Training Loss: 0.07691030204296112\n",
      "Epoch 2956/30000 Training Loss: 0.06681744009256363\n",
      "Epoch 2957/30000 Training Loss: 0.07294365763664246\n",
      "Epoch 2958/30000 Training Loss: 0.07015205174684525\n",
      "Epoch 2959/30000 Training Loss: 0.0570252425968647\n",
      "Epoch 2960/30000 Training Loss: 0.07528361678123474\n",
      "Epoch 2961/30000 Training Loss: 0.05336093157529831\n",
      "Epoch 2962/30000 Training Loss: 0.06127139925956726\n",
      "Epoch 2963/30000 Training Loss: 0.06947389990091324\n",
      "Epoch 2964/30000 Training Loss: 0.06034473329782486\n",
      "Epoch 2965/30000 Training Loss: 0.06430095434188843\n",
      "Epoch 2966/30000 Training Loss: 0.07518192380666733\n",
      "Epoch 2967/30000 Training Loss: 0.05436626076698303\n",
      "Epoch 2968/30000 Training Loss: 0.06602739542722702\n",
      "Epoch 2969/30000 Training Loss: 0.06951652467250824\n",
      "Epoch 2970/30000 Training Loss: 0.06415852904319763\n",
      "Epoch 2971/30000 Training Loss: 0.06421275436878204\n",
      "Epoch 2972/30000 Training Loss: 0.06983739137649536\n",
      "Epoch 2973/30000 Training Loss: 0.05295000225305557\n",
      "Epoch 2974/30000 Training Loss: 0.05857667326927185\n",
      "Epoch 2975/30000 Training Loss: 0.06522826850414276\n",
      "Epoch 2976/30000 Training Loss: 0.0666414126753807\n",
      "Epoch 2977/30000 Training Loss: 0.05296705290675163\n",
      "Epoch 2978/30000 Training Loss: 0.05538379028439522\n",
      "Epoch 2979/30000 Training Loss: 0.09104525297880173\n",
      "Epoch 2980/30000 Training Loss: 0.0621972419321537\n",
      "Epoch 2981/30000 Training Loss: 0.07705985754728317\n",
      "Epoch 2982/30000 Training Loss: 0.08224564045667648\n",
      "Epoch 2983/30000 Training Loss: 0.06703191995620728\n",
      "Epoch 2984/30000 Training Loss: 0.0498012900352478\n",
      "Epoch 2985/30000 Training Loss: 0.07211123406887054\n",
      "Epoch 2986/30000 Training Loss: 0.06677739322185516\n",
      "Epoch 2987/30000 Training Loss: 0.06578580290079117\n",
      "Epoch 2988/30000 Training Loss: 0.06939036399126053\n",
      "Epoch 2989/30000 Training Loss: 0.0625825971364975\n",
      "Epoch 2990/30000 Training Loss: 0.06150059029459953\n",
      "Epoch 2991/30000 Training Loss: 0.051390111446380615\n",
      "Epoch 2992/30000 Training Loss: 0.08126088976860046\n",
      "Epoch 2993/30000 Training Loss: 0.08072469383478165\n",
      "Epoch 2994/30000 Training Loss: 0.06704148650169373\n",
      "Epoch 2995/30000 Training Loss: 0.055643338710069656\n",
      "Epoch 2996/30000 Training Loss: 0.06732859462499619\n",
      "Epoch 2997/30000 Training Loss: 0.06081416457891464\n",
      "Epoch 2998/30000 Training Loss: 0.06233413144946098\n",
      "Epoch 2999/30000 Training Loss: 0.060079462826251984\n",
      "Epoch 3000/30000 Training Loss: 0.05849003791809082\n",
      "Epoch 3000/30000 Validation Loss: 0.08212825655937195\n",
      "Epoch 3001/30000 Training Loss: 0.06786219030618668\n",
      "Epoch 3002/30000 Training Loss: 0.07359932363033295\n",
      "Epoch 3003/30000 Training Loss: 0.06282155215740204\n",
      "Epoch 3004/30000 Training Loss: 0.07915565371513367\n",
      "Epoch 3005/30000 Training Loss: 0.07319198548793793\n",
      "Epoch 3006/30000 Training Loss: 0.06543487310409546\n",
      "Epoch 3007/30000 Training Loss: 0.06573473662137985\n",
      "Epoch 3008/30000 Training Loss: 0.06911229342222214\n",
      "Epoch 3009/30000 Training Loss: 0.07086261361837387\n",
      "Epoch 3010/30000 Training Loss: 0.06635819375514984\n",
      "Epoch 3011/30000 Training Loss: 0.06761392205953598\n",
      "Epoch 3012/30000 Training Loss: 0.0692601203918457\n",
      "Epoch 3013/30000 Training Loss: 0.07111166417598724\n",
      "Epoch 3014/30000 Training Loss: 0.07754693180322647\n",
      "Epoch 3015/30000 Training Loss: 0.08423282206058502\n",
      "Epoch 3016/30000 Training Loss: 0.061701394617557526\n",
      "Epoch 3017/30000 Training Loss: 0.06110287085175514\n",
      "Epoch 3018/30000 Training Loss: 0.057947397232055664\n",
      "Epoch 3019/30000 Training Loss: 0.060123711824417114\n",
      "Epoch 3020/30000 Training Loss: 0.06813758611679077\n",
      "Epoch 3021/30000 Training Loss: 0.054960817098617554\n",
      "Epoch 3022/30000 Training Loss: 0.05826658755540848\n",
      "Epoch 3023/30000 Training Loss: 0.0684613585472107\n",
      "Epoch 3024/30000 Training Loss: 0.08387850224971771\n",
      "Epoch 3025/30000 Training Loss: 0.06634779274463654\n",
      "Epoch 3026/30000 Training Loss: 0.08382268249988556\n",
      "Epoch 3027/30000 Training Loss: 0.06958571076393127\n",
      "Epoch 3028/30000 Training Loss: 0.05636458098888397\n",
      "Epoch 3029/30000 Training Loss: 0.061682622879743576\n",
      "Epoch 3030/30000 Training Loss: 0.06005631387233734\n",
      "Epoch 3031/30000 Training Loss: 0.07381806522607803\n",
      "Epoch 3032/30000 Training Loss: 0.08084318786859512\n",
      "Epoch 3033/30000 Training Loss: 0.05648883804678917\n",
      "Epoch 3034/30000 Training Loss: 0.08158890902996063\n",
      "Epoch 3035/30000 Training Loss: 0.0624602735042572\n",
      "Epoch 3036/30000 Training Loss: 0.07269350439310074\n",
      "Epoch 3037/30000 Training Loss: 0.06785596907138824\n",
      "Epoch 3038/30000 Training Loss: 0.08084806054830551\n",
      "Epoch 3039/30000 Training Loss: 0.07569816708564758\n",
      "Epoch 3040/30000 Training Loss: 0.07606162875890732\n",
      "Epoch 3041/30000 Training Loss: 0.05984269827604294\n",
      "Epoch 3042/30000 Training Loss: 0.07065795361995697\n",
      "Epoch 3043/30000 Training Loss: 0.06856551766395569\n",
      "Epoch 3044/30000 Training Loss: 0.0881878212094307\n",
      "Epoch 3045/30000 Training Loss: 0.06227269023656845\n",
      "Epoch 3046/30000 Training Loss: 0.07281506806612015\n",
      "Epoch 3047/30000 Training Loss: 0.06452953815460205\n",
      "Epoch 3048/30000 Training Loss: 0.07756124436855316\n",
      "Epoch 3049/30000 Training Loss: 0.07229882478713989\n",
      "Epoch 3050/30000 Training Loss: 0.07195167988538742\n",
      "Epoch 3051/30000 Training Loss: 0.07569842040538788\n",
      "Epoch 3052/30000 Training Loss: 0.0532783642411232\n",
      "Epoch 3053/30000 Training Loss: 0.06975600868463516\n",
      "Epoch 3054/30000 Training Loss: 0.07436864823102951\n",
      "Epoch 3055/30000 Training Loss: 0.06811961531639099\n",
      "Epoch 3056/30000 Training Loss: 0.06066632270812988\n",
      "Epoch 3057/30000 Training Loss: 0.06797408312559128\n",
      "Epoch 3058/30000 Training Loss: 0.061134569346904755\n",
      "Epoch 3059/30000 Training Loss: 0.08400702476501465\n",
      "Epoch 3060/30000 Training Loss: 0.08090326189994812\n",
      "Epoch 3061/30000 Training Loss: 0.05376795306801796\n",
      "Epoch 3062/30000 Training Loss: 0.06765355169773102\n",
      "Epoch 3063/30000 Training Loss: 0.06897751241922379\n",
      "Epoch 3064/30000 Training Loss: 0.07198191434144974\n",
      "Epoch 3065/30000 Training Loss: 0.05868866294622421\n",
      "Epoch 3066/30000 Training Loss: 0.05174772068858147\n",
      "Epoch 3067/30000 Training Loss: 0.07464118301868439\n",
      "Epoch 3068/30000 Training Loss: 0.07168786227703094\n",
      "Epoch 3069/30000 Training Loss: 0.06771734356880188\n",
      "Epoch 3070/30000 Training Loss: 0.05935412272810936\n",
      "Epoch 3071/30000 Training Loss: 0.07884421199560165\n",
      "Epoch 3072/30000 Training Loss: 0.07631330192089081\n",
      "Epoch 3073/30000 Training Loss: 0.060376815497875214\n",
      "Epoch 3074/30000 Training Loss: 0.07396240532398224\n",
      "Epoch 3075/30000 Training Loss: 0.0672549456357956\n",
      "Epoch 3076/30000 Training Loss: 0.06193947046995163\n",
      "Epoch 3077/30000 Training Loss: 0.061705414205789566\n",
      "Epoch 3078/30000 Training Loss: 0.08628511428833008\n",
      "Epoch 3079/30000 Training Loss: 0.05596533045172691\n",
      "Epoch 3080/30000 Training Loss: 0.08353886753320694\n",
      "Epoch 3081/30000 Training Loss: 0.0698142871260643\n",
      "Epoch 3082/30000 Training Loss: 0.055463097989559174\n",
      "Epoch 3083/30000 Training Loss: 0.06404228508472443\n",
      "Epoch 3084/30000 Training Loss: 0.061812739819288254\n",
      "Epoch 3085/30000 Training Loss: 0.07063890993595123\n",
      "Epoch 3086/30000 Training Loss: 0.07233990728855133\n",
      "Epoch 3087/30000 Training Loss: 0.06975328922271729\n",
      "Epoch 3088/30000 Training Loss: 0.058392103761434555\n",
      "Epoch 3089/30000 Training Loss: 0.07586445659399033\n",
      "Epoch 3090/30000 Training Loss: 0.07145185023546219\n",
      "Epoch 3091/30000 Training Loss: 0.06162678450345993\n",
      "Epoch 3092/30000 Training Loss: 0.06679759174585342\n",
      "Epoch 3093/30000 Training Loss: 0.05535655468702316\n",
      "Epoch 3094/30000 Training Loss: 0.06612896174192429\n",
      "Epoch 3095/30000 Training Loss: 0.0736963078379631\n",
      "Epoch 3096/30000 Training Loss: 0.052205976098775864\n",
      "Epoch 3097/30000 Training Loss: 0.06757019460201263\n",
      "Epoch 3098/30000 Training Loss: 0.08128181099891663\n",
      "Epoch 3099/30000 Training Loss: 0.06959380954504013\n",
      "Epoch 3100/30000 Training Loss: 0.05873938277363777\n",
      "Epoch 3100/30000 Validation Loss: 0.051646701991558075\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.051646701991558075<=============\n",
      "Epoch 3101/30000 Training Loss: 0.06717213243246078\n",
      "Epoch 3102/30000 Training Loss: 0.06720934808254242\n",
      "Epoch 3103/30000 Training Loss: 0.060020945966243744\n",
      "Epoch 3104/30000 Training Loss: 0.0669826939702034\n",
      "Epoch 3105/30000 Training Loss: 0.06512071937322617\n",
      "Epoch 3106/30000 Training Loss: 0.05982085317373276\n",
      "Epoch 3107/30000 Training Loss: 0.0641777291893959\n",
      "Epoch 3108/30000 Training Loss: 0.06659457087516785\n",
      "Epoch 3109/30000 Training Loss: 0.06499025225639343\n",
      "Epoch 3110/30000 Training Loss: 0.06229931861162186\n",
      "Epoch 3111/30000 Training Loss: 0.08291444182395935\n",
      "Epoch 3112/30000 Training Loss: 0.06011432409286499\n",
      "Epoch 3113/30000 Training Loss: 0.06596187502145767\n",
      "Epoch 3114/30000 Training Loss: 0.08220537006855011\n",
      "Epoch 3115/30000 Training Loss: 0.07618619501590729\n",
      "Epoch 3116/30000 Training Loss: 0.06689830124378204\n",
      "Epoch 3117/30000 Training Loss: 0.06844914704561234\n",
      "Epoch 3118/30000 Training Loss: 0.06324276328086853\n",
      "Epoch 3119/30000 Training Loss: 0.06272982060909271\n",
      "Epoch 3120/30000 Training Loss: 0.06507224589586258\n",
      "Epoch 3121/30000 Training Loss: 0.08381188660860062\n",
      "Epoch 3122/30000 Training Loss: 0.07491064816713333\n",
      "Epoch 3123/30000 Training Loss: 0.054271310567855835\n",
      "Epoch 3124/30000 Training Loss: 0.0864664688706398\n",
      "Epoch 3125/30000 Training Loss: 0.07056297361850739\n",
      "Epoch 3126/30000 Training Loss: 0.05900450795888901\n",
      "Epoch 3127/30000 Training Loss: 0.08081892132759094\n",
      "Epoch 3128/30000 Training Loss: 0.0665777325630188\n",
      "Epoch 3129/30000 Training Loss: 0.07344965636730194\n",
      "Epoch 3130/30000 Training Loss: 0.06447605788707733\n",
      "Epoch 3131/30000 Training Loss: 0.06358671188354492\n",
      "Epoch 3132/30000 Training Loss: 0.05716557055711746\n",
      "Epoch 3133/30000 Training Loss: 0.05994897335767746\n",
      "Epoch 3134/30000 Training Loss: 0.05100800096988678\n",
      "Epoch 3135/30000 Training Loss: 0.06608310341835022\n",
      "Epoch 3136/30000 Training Loss: 0.0641159862279892\n",
      "Epoch 3137/30000 Training Loss: 0.08169662207365036\n",
      "Epoch 3138/30000 Training Loss: 0.08638396859169006\n",
      "Epoch 3139/30000 Training Loss: 0.06652098149061203\n",
      "Epoch 3140/30000 Training Loss: 0.06123253330588341\n",
      "Epoch 3141/30000 Training Loss: 0.057079970836639404\n",
      "Epoch 3142/30000 Training Loss: 0.06030036881566048\n",
      "Epoch 3143/30000 Training Loss: 0.07158359885215759\n",
      "Epoch 3144/30000 Training Loss: 0.061703599989414215\n",
      "Epoch 3145/30000 Training Loss: 0.063246950507164\n",
      "Epoch 3146/30000 Training Loss: 0.08717110753059387\n",
      "Epoch 3147/30000 Training Loss: 0.06079992651939392\n",
      "Epoch 3148/30000 Training Loss: 0.07676907628774643\n",
      "Epoch 3149/30000 Training Loss: 0.05873360484838486\n",
      "Epoch 3150/30000 Training Loss: 0.07295407354831696\n",
      "Epoch 3151/30000 Training Loss: 0.08089333772659302\n",
      "Epoch 3152/30000 Training Loss: 0.04819285124540329\n",
      "Epoch 3153/30000 Training Loss: 0.05606012046337128\n",
      "Epoch 3154/30000 Training Loss: 0.06803673505783081\n",
      "Epoch 3155/30000 Training Loss: 0.07607525587081909\n",
      "Epoch 3156/30000 Training Loss: 0.08877015113830566\n",
      "Epoch 3157/30000 Training Loss: 0.06436373293399811\n",
      "Epoch 3158/30000 Training Loss: 0.07192013412714005\n",
      "Epoch 3159/30000 Training Loss: 0.08177153766155243\n",
      "Epoch 3160/30000 Training Loss: 0.07204942405223846\n",
      "Epoch 3161/30000 Training Loss: 0.06973009556531906\n",
      "Epoch 3162/30000 Training Loss: 0.06879729777574539\n",
      "Epoch 3163/30000 Training Loss: 0.06719768792390823\n",
      "Epoch 3164/30000 Training Loss: 0.06096895784139633\n",
      "Epoch 3165/30000 Training Loss: 0.07114811986684799\n",
      "Epoch 3166/30000 Training Loss: 0.07342564314603806\n",
      "Epoch 3167/30000 Training Loss: 0.053030773997306824\n",
      "Epoch 3168/30000 Training Loss: 0.06151650846004486\n",
      "Epoch 3169/30000 Training Loss: 0.06984671205282211\n",
      "Epoch 3170/30000 Training Loss: 0.05541820824146271\n",
      "Epoch 3171/30000 Training Loss: 0.06134447082877159\n",
      "Epoch 3172/30000 Training Loss: 0.07103189826011658\n",
      "Epoch 3173/30000 Training Loss: 0.076761893928051\n",
      "Epoch 3174/30000 Training Loss: 0.07638191431760788\n",
      "Epoch 3175/30000 Training Loss: 0.0676075890660286\n",
      "Epoch 3176/30000 Training Loss: 0.07562799006700516\n",
      "Epoch 3177/30000 Training Loss: 0.07394956797361374\n",
      "Epoch 3178/30000 Training Loss: 0.0706895962357521\n",
      "Epoch 3179/30000 Training Loss: 0.07634515315294266\n",
      "Epoch 3180/30000 Training Loss: 0.07413620501756668\n",
      "Epoch 3181/30000 Training Loss: 0.06997287273406982\n",
      "Epoch 3182/30000 Training Loss: 0.07110074162483215\n",
      "Epoch 3183/30000 Training Loss: 0.053269922733306885\n",
      "Epoch 3184/30000 Training Loss: 0.062085457146167755\n",
      "Epoch 3185/30000 Training Loss: 0.06941970437765121\n",
      "Epoch 3186/30000 Training Loss: 0.071047842502594\n",
      "Epoch 3187/30000 Training Loss: 0.05352780967950821\n",
      "Epoch 3188/30000 Training Loss: 0.06899267435073853\n",
      "Epoch 3189/30000 Training Loss: 0.07421396672725677\n",
      "Epoch 3190/30000 Training Loss: 0.07652345299720764\n",
      "Epoch 3191/30000 Training Loss: 0.06537052989006042\n",
      "Epoch 3192/30000 Training Loss: 0.062080081552267075\n",
      "Epoch 3193/30000 Training Loss: 0.07580861449241638\n",
      "Epoch 3194/30000 Training Loss: 0.06048952415585518\n",
      "Epoch 3195/30000 Training Loss: 0.06946945935487747\n",
      "Epoch 3196/30000 Training Loss: 0.07634884864091873\n",
      "Epoch 3197/30000 Training Loss: 0.06544897705316544\n",
      "Epoch 3198/30000 Training Loss: 0.07378540188074112\n",
      "Epoch 3199/30000 Training Loss: 0.06484520435333252\n",
      "Epoch 3200/30000 Training Loss: 0.07370468974113464\n",
      "Epoch 3200/30000 Validation Loss: 0.06483300775289536\n",
      "Epoch 3201/30000 Training Loss: 0.05899985134601593\n",
      "Epoch 3202/30000 Training Loss: 0.06485415995121002\n",
      "Epoch 3203/30000 Training Loss: 0.06604121625423431\n",
      "Epoch 3204/30000 Training Loss: 0.06373781710863113\n",
      "Epoch 3205/30000 Training Loss: 0.07491361349821091\n",
      "Epoch 3206/30000 Training Loss: 0.05698926001787186\n",
      "Epoch 3207/30000 Training Loss: 0.09350823611021042\n",
      "Epoch 3208/30000 Training Loss: 0.06421104818582535\n",
      "Epoch 3209/30000 Training Loss: 0.054345641285181046\n",
      "Epoch 3210/30000 Training Loss: 0.0735657662153244\n",
      "Epoch 3211/30000 Training Loss: 0.07511532306671143\n",
      "Epoch 3212/30000 Training Loss: 0.07888715714216232\n",
      "Epoch 3213/30000 Training Loss: 0.07848691940307617\n",
      "Epoch 3214/30000 Training Loss: 0.07593439519405365\n",
      "Epoch 3215/30000 Training Loss: 0.0545685850083828\n",
      "Epoch 3216/30000 Training Loss: 0.06737597286701202\n",
      "Epoch 3217/30000 Training Loss: 0.06394514441490173\n",
      "Epoch 3218/30000 Training Loss: 0.07009678333997726\n",
      "Epoch 3219/30000 Training Loss: 0.06816521286964417\n",
      "Epoch 3220/30000 Training Loss: 0.06973990797996521\n",
      "Epoch 3221/30000 Training Loss: 0.06336863338947296\n",
      "Epoch 3222/30000 Training Loss: 0.0624953992664814\n",
      "Epoch 3223/30000 Training Loss: 0.06360239535570145\n",
      "Epoch 3224/30000 Training Loss: 0.08189165592193604\n",
      "Epoch 3225/30000 Training Loss: 0.08005779981613159\n",
      "Epoch 3226/30000 Training Loss: 0.06524154543876648\n",
      "Epoch 3227/30000 Training Loss: 0.0725560337305069\n",
      "Epoch 3228/30000 Training Loss: 0.0521405003964901\n",
      "Epoch 3229/30000 Training Loss: 0.06305801868438721\n",
      "Epoch 3230/30000 Training Loss: 0.0562937930226326\n",
      "Epoch 3231/30000 Training Loss: 0.0646568164229393\n",
      "Epoch 3232/30000 Training Loss: 0.06659609824419022\n",
      "Epoch 3233/30000 Training Loss: 0.07687437534332275\n",
      "Epoch 3234/30000 Training Loss: 0.07224543392658234\n",
      "Epoch 3235/30000 Training Loss: 0.059173330664634705\n",
      "Epoch 3236/30000 Training Loss: 0.07093431055545807\n",
      "Epoch 3237/30000 Training Loss: 0.07078951597213745\n",
      "Epoch 3238/30000 Training Loss: 0.08318113535642624\n",
      "Epoch 3239/30000 Training Loss: 0.06603794544935226\n",
      "Epoch 3240/30000 Training Loss: 0.05943179875612259\n",
      "Epoch 3241/30000 Training Loss: 0.07105450332164764\n",
      "Epoch 3242/30000 Training Loss: 0.06911081075668335\n",
      "Epoch 3243/30000 Training Loss: 0.08089999854564667\n",
      "Epoch 3244/30000 Training Loss: 0.07515543699264526\n",
      "Epoch 3245/30000 Training Loss: 0.053475383669137955\n",
      "Epoch 3246/30000 Training Loss: 0.07196146249771118\n",
      "Epoch 3247/30000 Training Loss: 0.09524804353713989\n",
      "Epoch 3248/30000 Training Loss: 0.05201836675405502\n",
      "Epoch 3249/30000 Training Loss: 0.08185876160860062\n",
      "Epoch 3250/30000 Training Loss: 0.07866496592760086\n",
      "Epoch 3251/30000 Training Loss: 0.0705677941441536\n",
      "Epoch 3252/30000 Training Loss: 0.0472039170563221\n",
      "Epoch 3253/30000 Training Loss: 0.07712268829345703\n",
      "Epoch 3254/30000 Training Loss: 0.07225757092237473\n",
      "Epoch 3255/30000 Training Loss: 0.06405507773160934\n",
      "Epoch 3256/30000 Training Loss: 0.06373319029808044\n",
      "Epoch 3257/30000 Training Loss: 0.06126902624964714\n",
      "Epoch 3258/30000 Training Loss: 0.06392783671617508\n",
      "Epoch 3259/30000 Training Loss: 0.06343600153923035\n",
      "Epoch 3260/30000 Training Loss: 0.06658454239368439\n",
      "Epoch 3261/30000 Training Loss: 0.08608213812112808\n",
      "Epoch 3262/30000 Training Loss: 0.08123598247766495\n",
      "Epoch 3263/30000 Training Loss: 0.07486821711063385\n",
      "Epoch 3264/30000 Training Loss: 0.07302699238061905\n",
      "Epoch 3265/30000 Training Loss: 0.06879494339227676\n",
      "Epoch 3266/30000 Training Loss: 0.0778244212269783\n",
      "Epoch 3267/30000 Training Loss: 0.0643724650144577\n",
      "Epoch 3268/30000 Training Loss: 0.07194055616855621\n",
      "Epoch 3269/30000 Training Loss: 0.06669539958238602\n",
      "Epoch 3270/30000 Training Loss: 0.0529949814081192\n",
      "Epoch 3271/30000 Training Loss: 0.06047428399324417\n",
      "Epoch 3272/30000 Training Loss: 0.07660030573606491\n",
      "Epoch 3273/30000 Training Loss: 0.07549693435430527\n",
      "Epoch 3274/30000 Training Loss: 0.0660470575094223\n",
      "Epoch 3275/30000 Training Loss: 0.06582746654748917\n",
      "Epoch 3276/30000 Training Loss: 0.07468016445636749\n",
      "Epoch 3277/30000 Training Loss: 0.06818394362926483\n",
      "Epoch 3278/30000 Training Loss: 0.059060078114271164\n",
      "Epoch 3279/30000 Training Loss: 0.07521602511405945\n",
      "Epoch 3280/30000 Training Loss: 0.054303690791130066\n",
      "Epoch 3281/30000 Training Loss: 0.07012587040662766\n",
      "Epoch 3282/30000 Training Loss: 0.0698467418551445\n",
      "Epoch 3283/30000 Training Loss: 0.06609795987606049\n",
      "Epoch 3284/30000 Training Loss: 0.07639603316783905\n",
      "Epoch 3285/30000 Training Loss: 0.07376223802566528\n",
      "Epoch 3286/30000 Training Loss: 0.06597478687763214\n",
      "Epoch 3287/30000 Training Loss: 0.06232532113790512\n",
      "Epoch 3288/30000 Training Loss: 0.06007824093103409\n",
      "Epoch 3289/30000 Training Loss: 0.08567319065332413\n",
      "Epoch 3290/30000 Training Loss: 0.08538249135017395\n",
      "Epoch 3291/30000 Training Loss: 0.06163093447685242\n",
      "Epoch 3292/30000 Training Loss: 0.0590762197971344\n",
      "Epoch 3293/30000 Training Loss: 0.04842769727110863\n",
      "Epoch 3294/30000 Training Loss: 0.05937408655881882\n",
      "Epoch 3295/30000 Training Loss: 0.0777590200304985\n",
      "Epoch 3296/30000 Training Loss: 0.060593560338020325\n",
      "Epoch 3297/30000 Training Loss: 0.06653133779764175\n",
      "Epoch 3298/30000 Training Loss: 0.07629305124282837\n",
      "Epoch 3299/30000 Training Loss: 0.07572634518146515\n",
      "Epoch 3300/30000 Training Loss: 0.05941808596253395\n",
      "Epoch 3300/30000 Validation Loss: 0.07219987362623215\n",
      "Epoch 3301/30000 Training Loss: 0.07093017548322678\n",
      "Epoch 3302/30000 Training Loss: 0.06810072064399719\n",
      "Epoch 3303/30000 Training Loss: 0.07274963706731796\n",
      "Epoch 3304/30000 Training Loss: 0.05413544550538063\n",
      "Epoch 3305/30000 Training Loss: 0.06565669178962708\n",
      "Epoch 3306/30000 Training Loss: 0.095523901283741\n",
      "Epoch 3307/30000 Training Loss: 0.06207133084535599\n",
      "Epoch 3308/30000 Training Loss: 0.06806553900241852\n",
      "Epoch 3309/30000 Training Loss: 0.07517074048519135\n",
      "Epoch 3310/30000 Training Loss: 0.06497602164745331\n",
      "Epoch 3311/30000 Training Loss: 0.08418529480695724\n",
      "Epoch 3312/30000 Training Loss: 0.0540839247405529\n",
      "Epoch 3313/30000 Training Loss: 0.060713909566402435\n",
      "Epoch 3314/30000 Training Loss: 0.07865878939628601\n",
      "Epoch 3315/30000 Training Loss: 0.06174268573522568\n",
      "Epoch 3316/30000 Training Loss: 0.05943247675895691\n",
      "Epoch 3317/30000 Training Loss: 0.06534244120121002\n",
      "Epoch 3318/30000 Training Loss: 0.06836927682161331\n",
      "Epoch 3319/30000 Training Loss: 0.06483794003725052\n",
      "Epoch 3320/30000 Training Loss: 0.05657084286212921\n",
      "Epoch 3321/30000 Training Loss: 0.04776753485202789\n",
      "Epoch 3322/30000 Training Loss: 0.07071501761674881\n",
      "Epoch 3323/30000 Training Loss: 0.07008771598339081\n",
      "Epoch 3324/30000 Training Loss: 0.060259848833084106\n",
      "Epoch 3325/30000 Training Loss: 0.0687355250120163\n",
      "Epoch 3326/30000 Training Loss: 0.0519016794860363\n",
      "Epoch 3327/30000 Training Loss: 0.05822090804576874\n",
      "Epoch 3328/30000 Training Loss: 0.07818549871444702\n",
      "Epoch 3329/30000 Training Loss: 0.06188720464706421\n",
      "Epoch 3330/30000 Training Loss: 0.07674959301948547\n",
      "Epoch 3331/30000 Training Loss: 0.05575147643685341\n",
      "Epoch 3332/30000 Training Loss: 0.0744483694434166\n",
      "Epoch 3333/30000 Training Loss: 0.06527853012084961\n",
      "Epoch 3334/30000 Training Loss: 0.06138557568192482\n",
      "Epoch 3335/30000 Training Loss: 0.06784646958112717\n",
      "Epoch 3336/30000 Training Loss: 0.06379730999469757\n",
      "Epoch 3337/30000 Training Loss: 0.07469063252210617\n",
      "Epoch 3338/30000 Training Loss: 0.042805954813957214\n",
      "Epoch 3339/30000 Training Loss: 0.06999876350164413\n",
      "Epoch 3340/30000 Training Loss: 0.058921556919813156\n",
      "Epoch 3341/30000 Training Loss: 0.06984927505254745\n",
      "Epoch 3342/30000 Training Loss: 0.05819263309240341\n",
      "Epoch 3343/30000 Training Loss: 0.06662368029356003\n",
      "Epoch 3344/30000 Training Loss: 0.0768006294965744\n",
      "Epoch 3345/30000 Training Loss: 0.06766258925199509\n",
      "Epoch 3346/30000 Training Loss: 0.06674446910619736\n",
      "Epoch 3347/30000 Training Loss: 0.0705980658531189\n",
      "Epoch 3348/30000 Training Loss: 0.0675973892211914\n",
      "Epoch 3349/30000 Training Loss: 0.08542180061340332\n",
      "Epoch 3350/30000 Training Loss: 0.0721173956990242\n",
      "Epoch 3351/30000 Training Loss: 0.0747564435005188\n",
      "Epoch 3352/30000 Training Loss: 0.05718019977211952\n",
      "Epoch 3353/30000 Training Loss: 0.06975256651639938\n",
      "Epoch 3354/30000 Training Loss: 0.06561320275068283\n",
      "Epoch 3355/30000 Training Loss: 0.07477331906557083\n",
      "Epoch 3356/30000 Training Loss: 0.06153721362352371\n",
      "Epoch 3357/30000 Training Loss: 0.07402700185775757\n",
      "Epoch 3358/30000 Training Loss: 0.08442644029855728\n",
      "Epoch 3359/30000 Training Loss: 0.0759362131357193\n",
      "Epoch 3360/30000 Training Loss: 0.06634248793125153\n",
      "Epoch 3361/30000 Training Loss: 0.07232925295829773\n",
      "Epoch 3362/30000 Training Loss: 0.06236712634563446\n",
      "Epoch 3363/30000 Training Loss: 0.06933797895908356\n",
      "Epoch 3364/30000 Training Loss: 0.0678253322839737\n",
      "Epoch 3365/30000 Training Loss: 0.06253229081630707\n",
      "Epoch 3366/30000 Training Loss: 0.08003945648670197\n",
      "Epoch 3367/30000 Training Loss: 0.05996817350387573\n",
      "Epoch 3368/30000 Training Loss: 0.07357267290353775\n",
      "Epoch 3369/30000 Training Loss: 0.0643656849861145\n",
      "Epoch 3370/30000 Training Loss: 0.05917096883058548\n",
      "Epoch 3371/30000 Training Loss: 0.0652080848813057\n",
      "Epoch 3372/30000 Training Loss: 0.05690672993659973\n",
      "Epoch 3373/30000 Training Loss: 0.061721332371234894\n",
      "Epoch 3374/30000 Training Loss: 0.06913836300373077\n",
      "Epoch 3375/30000 Training Loss: 0.07293817400932312\n",
      "Epoch 3376/30000 Training Loss: 0.050494030117988586\n",
      "Epoch 3377/30000 Training Loss: 0.05963815376162529\n",
      "Epoch 3378/30000 Training Loss: 0.07207530736923218\n",
      "Epoch 3379/30000 Training Loss: 0.057538751512765884\n",
      "Epoch 3380/30000 Training Loss: 0.05698077753186226\n",
      "Epoch 3381/30000 Training Loss: 0.08131498843431473\n",
      "Epoch 3382/30000 Training Loss: 0.0638238787651062\n",
      "Epoch 3383/30000 Training Loss: 0.05733256787061691\n",
      "Epoch 3384/30000 Training Loss: 0.06805972754955292\n",
      "Epoch 3385/30000 Training Loss: 0.07896459847688675\n",
      "Epoch 3386/30000 Training Loss: 0.0849573016166687\n",
      "Epoch 3387/30000 Training Loss: 0.07651127129793167\n",
      "Epoch 3388/30000 Training Loss: 0.06899292021989822\n",
      "Epoch 3389/30000 Training Loss: 0.08196505904197693\n",
      "Epoch 3390/30000 Training Loss: 0.061806924641132355\n",
      "Epoch 3391/30000 Training Loss: 0.07464618235826492\n",
      "Epoch 3392/30000 Training Loss: 0.05950666591525078\n",
      "Epoch 3393/30000 Training Loss: 0.08442611247301102\n",
      "Epoch 3394/30000 Training Loss: 0.04954223707318306\n",
      "Epoch 3395/30000 Training Loss: 0.0643857941031456\n",
      "Epoch 3396/30000 Training Loss: 0.05527353286743164\n",
      "Epoch 3397/30000 Training Loss: 0.056294333189725876\n",
      "Epoch 3398/30000 Training Loss: 0.07308614999055862\n",
      "Epoch 3399/30000 Training Loss: 0.05352132394909859\n",
      "Epoch 3400/30000 Training Loss: 0.07959377765655518\n",
      "Epoch 3400/30000 Validation Loss: 0.06373841315507889\n",
      "Epoch 3401/30000 Training Loss: 0.09141740202903748\n",
      "Epoch 3402/30000 Training Loss: 0.0647045448422432\n",
      "Epoch 3403/30000 Training Loss: 0.08236821740865707\n",
      "Epoch 3404/30000 Training Loss: 0.08622318506240845\n",
      "Epoch 3405/30000 Training Loss: 0.078758604824543\n",
      "Epoch 3406/30000 Training Loss: 0.04810386896133423\n",
      "Epoch 3407/30000 Training Loss: 0.06515638530254364\n",
      "Epoch 3408/30000 Training Loss: 0.061714477837085724\n",
      "Epoch 3409/30000 Training Loss: 0.0743817463517189\n",
      "Epoch 3410/30000 Training Loss: 0.07052556425333023\n",
      "Epoch 3411/30000 Training Loss: 0.06124313920736313\n",
      "Epoch 3412/30000 Training Loss: 0.046240754425525665\n",
      "Epoch 3413/30000 Training Loss: 0.05026768147945404\n",
      "Epoch 3414/30000 Training Loss: 0.061035122722387314\n",
      "Epoch 3415/30000 Training Loss: 0.05136865749955177\n",
      "Epoch 3416/30000 Training Loss: 0.06609021872282028\n",
      "Epoch 3417/30000 Training Loss: 0.07534638792276382\n",
      "Epoch 3418/30000 Training Loss: 0.07346291840076447\n",
      "Epoch 3419/30000 Training Loss: 0.07037165015935898\n",
      "Epoch 3420/30000 Training Loss: 0.06878237426280975\n",
      "Epoch 3421/30000 Training Loss: 0.061365798115730286\n",
      "Epoch 3422/30000 Training Loss: 0.06201588362455368\n",
      "Epoch 3423/30000 Training Loss: 0.056994784623384476\n",
      "Epoch 3424/30000 Training Loss: 0.06231483817100525\n",
      "Epoch 3425/30000 Training Loss: 0.07411885261535645\n",
      "Epoch 3426/30000 Training Loss: 0.0824027732014656\n",
      "Epoch 3427/30000 Training Loss: 0.05985650792717934\n",
      "Epoch 3428/30000 Training Loss: 0.0572216734290123\n",
      "Epoch 3429/30000 Training Loss: 0.06588964909315109\n",
      "Epoch 3430/30000 Training Loss: 0.08713273704051971\n",
      "Epoch 3431/30000 Training Loss: 0.07436804473400116\n",
      "Epoch 3432/30000 Training Loss: 0.059640757739543915\n",
      "Epoch 3433/30000 Training Loss: 0.04894234612584114\n",
      "Epoch 3434/30000 Training Loss: 0.06775067746639252\n",
      "Epoch 3435/30000 Training Loss: 0.06520868837833405\n",
      "Epoch 3436/30000 Training Loss: 0.08151909708976746\n",
      "Epoch 3437/30000 Training Loss: 0.07972405105829239\n",
      "Epoch 3438/30000 Training Loss: 0.08281047642230988\n",
      "Epoch 3439/30000 Training Loss: 0.06914538890123367\n",
      "Epoch 3440/30000 Training Loss: 0.06985979527235031\n",
      "Epoch 3441/30000 Training Loss: 0.051577746868133545\n",
      "Epoch 3442/30000 Training Loss: 0.04973253607749939\n",
      "Epoch 3443/30000 Training Loss: 0.07100719213485718\n",
      "Epoch 3444/30000 Training Loss: 0.07763892412185669\n",
      "Epoch 3445/30000 Training Loss: 0.06583862006664276\n",
      "Epoch 3446/30000 Training Loss: 0.07199880480766296\n",
      "Epoch 3447/30000 Training Loss: 0.06145291030406952\n",
      "Epoch 3448/30000 Training Loss: 0.050414975732564926\n",
      "Epoch 3449/30000 Training Loss: 0.060080792754888535\n",
      "Epoch 3450/30000 Training Loss: 0.061781808733940125\n",
      "Epoch 3451/30000 Training Loss: 0.07238734513521194\n",
      "Epoch 3452/30000 Training Loss: 0.06883308291435242\n",
      "Epoch 3453/30000 Training Loss: 0.062096383422613144\n",
      "Epoch 3454/30000 Training Loss: 0.055416516959667206\n",
      "Epoch 3455/30000 Training Loss: 0.07176901400089264\n",
      "Epoch 3456/30000 Training Loss: 0.06897777318954468\n",
      "Epoch 3457/30000 Training Loss: 0.07369551062583923\n",
      "Epoch 3458/30000 Training Loss: 0.06622861325740814\n",
      "Epoch 3459/30000 Training Loss: 0.07310961931943893\n",
      "Epoch 3460/30000 Training Loss: 0.07134594768285751\n",
      "Epoch 3461/30000 Training Loss: 0.06363464891910553\n",
      "Epoch 3462/30000 Training Loss: 0.0575869083404541\n",
      "Epoch 3463/30000 Training Loss: 0.05623912811279297\n",
      "Epoch 3464/30000 Training Loss: 0.050931092351675034\n",
      "Epoch 3465/30000 Training Loss: 0.053177595138549805\n",
      "Epoch 3466/30000 Training Loss: 0.05543063208460808\n",
      "Epoch 3467/30000 Training Loss: 0.0742592066526413\n",
      "Epoch 3468/30000 Training Loss: 0.06471864879131317\n",
      "Epoch 3469/30000 Training Loss: 0.08153650164604187\n",
      "Epoch 3470/30000 Training Loss: 0.06227106973528862\n",
      "Epoch 3471/30000 Training Loss: 0.0641225203871727\n",
      "Epoch 3472/30000 Training Loss: 0.06193055957555771\n",
      "Epoch 3473/30000 Training Loss: 0.08222267031669617\n",
      "Epoch 3474/30000 Training Loss: 0.050060391426086426\n",
      "Epoch 3475/30000 Training Loss: 0.07366597652435303\n",
      "Epoch 3476/30000 Training Loss: 0.07942530512809753\n",
      "Epoch 3477/30000 Training Loss: 0.06742658466100693\n",
      "Epoch 3478/30000 Training Loss: 0.059354692697525024\n",
      "Epoch 3479/30000 Training Loss: 0.0577896386384964\n",
      "Epoch 3480/30000 Training Loss: 0.0638153925538063\n",
      "Epoch 3481/30000 Training Loss: 0.06232723593711853\n",
      "Epoch 3482/30000 Training Loss: 0.07422661036252975\n",
      "Epoch 3483/30000 Training Loss: 0.07808572053909302\n",
      "Epoch 3484/30000 Training Loss: 0.07507449388504028\n",
      "Epoch 3485/30000 Training Loss: 0.06765580177307129\n",
      "Epoch 3486/30000 Training Loss: 0.06288876384496689\n",
      "Epoch 3487/30000 Training Loss: 0.07550205290317535\n",
      "Epoch 3488/30000 Training Loss: 0.066927969455719\n",
      "Epoch 3489/30000 Training Loss: 0.0753948986530304\n",
      "Epoch 3490/30000 Training Loss: 0.061988748610019684\n",
      "Epoch 3491/30000 Training Loss: 0.0655512735247612\n",
      "Epoch 3492/30000 Training Loss: 0.0694202184677124\n",
      "Epoch 3493/30000 Training Loss: 0.07118168473243713\n",
      "Epoch 3494/30000 Training Loss: 0.08790376782417297\n",
      "Epoch 3495/30000 Training Loss: 0.06300042569637299\n",
      "Epoch 3496/30000 Training Loss: 0.06860853731632233\n",
      "Epoch 3497/30000 Training Loss: 0.07620701938867569\n",
      "Epoch 3498/30000 Training Loss: 0.06039918586611748\n",
      "Epoch 3499/30000 Training Loss: 0.056152984499931335\n",
      "Epoch 3500/30000 Training Loss: 0.05602480098605156\n",
      "Epoch 3500/30000 Validation Loss: 0.053062912076711655\n",
      "Epoch 3501/30000 Training Loss: 0.08250889927148819\n",
      "Epoch 3502/30000 Training Loss: 0.0618162602186203\n",
      "Epoch 3503/30000 Training Loss: 0.07174726575613022\n",
      "Epoch 3504/30000 Training Loss: 0.07176582515239716\n",
      "Epoch 3505/30000 Training Loss: 0.06285528093576431\n",
      "Epoch 3506/30000 Training Loss: 0.06645123660564423\n",
      "Epoch 3507/30000 Training Loss: 0.071531742811203\n",
      "Epoch 3508/30000 Training Loss: 0.07730535417795181\n",
      "Epoch 3509/30000 Training Loss: 0.07246062159538269\n",
      "Epoch 3510/30000 Training Loss: 0.07433389127254486\n",
      "Epoch 3511/30000 Training Loss: 0.08064422011375427\n",
      "Epoch 3512/30000 Training Loss: 0.08096300810575485\n",
      "Epoch 3513/30000 Training Loss: 0.07088226079940796\n",
      "Epoch 3514/30000 Training Loss: 0.06414687633514404\n",
      "Epoch 3515/30000 Training Loss: 0.05290984734892845\n",
      "Epoch 3516/30000 Training Loss: 0.07484282553195953\n",
      "Epoch 3517/30000 Training Loss: 0.06866521388292313\n",
      "Epoch 3518/30000 Training Loss: 0.05632360279560089\n",
      "Epoch 3519/30000 Training Loss: 0.07235769182443619\n",
      "Epoch 3520/30000 Training Loss: 0.06193426996469498\n",
      "Epoch 3521/30000 Training Loss: 0.06939533352851868\n",
      "Epoch 3522/30000 Training Loss: 0.05159812048077583\n",
      "Epoch 3523/30000 Training Loss: 0.07768642157316208\n",
      "Epoch 3524/30000 Training Loss: 0.0813102051615715\n",
      "Epoch 3525/30000 Training Loss: 0.06733424961566925\n",
      "Epoch 3526/30000 Training Loss: 0.07864584773778915\n",
      "Epoch 3527/30000 Training Loss: 0.07153350859880447\n",
      "Epoch 3528/30000 Training Loss: 0.0587957426905632\n",
      "Epoch 3529/30000 Training Loss: 0.06146956980228424\n",
      "Epoch 3530/30000 Training Loss: 0.06567034125328064\n",
      "Epoch 3531/30000 Training Loss: 0.0823742225766182\n",
      "Epoch 3532/30000 Training Loss: 0.07146546244621277\n",
      "Epoch 3533/30000 Training Loss: 0.05697223171591759\n",
      "Epoch 3534/30000 Training Loss: 0.06435481458902359\n",
      "Epoch 3535/30000 Training Loss: 0.06172024831175804\n",
      "Epoch 3536/30000 Training Loss: 0.0633937418460846\n",
      "Epoch 3537/30000 Training Loss: 0.07282926887273788\n",
      "Epoch 3538/30000 Training Loss: 0.05685165524482727\n",
      "Epoch 3539/30000 Training Loss: 0.06389898806810379\n",
      "Epoch 3540/30000 Training Loss: 0.05025666579604149\n",
      "Epoch 3541/30000 Training Loss: 0.08145131915807724\n",
      "Epoch 3542/30000 Training Loss: 0.06181405112147331\n",
      "Epoch 3543/30000 Training Loss: 0.07660485804080963\n",
      "Epoch 3544/30000 Training Loss: 0.06948412954807281\n",
      "Epoch 3545/30000 Training Loss: 0.0662015974521637\n",
      "Epoch 3546/30000 Training Loss: 0.05466623976826668\n",
      "Epoch 3547/30000 Training Loss: 0.07945333421230316\n",
      "Epoch 3548/30000 Training Loss: 0.06831254065036774\n",
      "Epoch 3549/30000 Training Loss: 0.07373706996440887\n",
      "Epoch 3550/30000 Training Loss: 0.07475334405899048\n",
      "Epoch 3551/30000 Training Loss: 0.06429380923509598\n",
      "Epoch 3552/30000 Training Loss: 0.06675851345062256\n",
      "Epoch 3553/30000 Training Loss: 0.07714647799730301\n",
      "Epoch 3554/30000 Training Loss: 0.0645727813243866\n",
      "Epoch 3555/30000 Training Loss: 0.0771268755197525\n",
      "Epoch 3556/30000 Training Loss: 0.05270051211118698\n",
      "Epoch 3557/30000 Training Loss: 0.06382802873849869\n",
      "Epoch 3558/30000 Training Loss: 0.07963292300701141\n",
      "Epoch 3559/30000 Training Loss: 0.05326318368315697\n",
      "Epoch 3560/30000 Training Loss: 0.05913163721561432\n",
      "Epoch 3561/30000 Training Loss: 0.05958719179034233\n",
      "Epoch 3562/30000 Training Loss: 0.07700761407613754\n",
      "Epoch 3563/30000 Training Loss: 0.07628870755434036\n",
      "Epoch 3564/30000 Training Loss: 0.0733187198638916\n",
      "Epoch 3565/30000 Training Loss: 0.05324343591928482\n",
      "Epoch 3566/30000 Training Loss: 0.06856009364128113\n",
      "Epoch 3567/30000 Training Loss: 0.061599984765052795\n",
      "Epoch 3568/30000 Training Loss: 0.05591564625501633\n",
      "Epoch 3569/30000 Training Loss: 0.07532219588756561\n",
      "Epoch 3570/30000 Training Loss: 0.0743313729763031\n",
      "Epoch 3571/30000 Training Loss: 0.060860030353069305\n",
      "Epoch 3572/30000 Training Loss: 0.05141783505678177\n",
      "Epoch 3573/30000 Training Loss: 0.0734315812587738\n",
      "Epoch 3574/30000 Training Loss: 0.06373866647481918\n",
      "Epoch 3575/30000 Training Loss: 0.0622408390045166\n",
      "Epoch 3576/30000 Training Loss: 0.07887934148311615\n",
      "Epoch 3577/30000 Training Loss: 0.07952310889959335\n",
      "Epoch 3578/30000 Training Loss: 0.06873732805252075\n",
      "Epoch 3579/30000 Training Loss: 0.06689862906932831\n",
      "Epoch 3580/30000 Training Loss: 0.06504322588443756\n",
      "Epoch 3581/30000 Training Loss: 0.053572945296764374\n",
      "Epoch 3582/30000 Training Loss: 0.06059449166059494\n",
      "Epoch 3583/30000 Training Loss: 0.06800943613052368\n",
      "Epoch 3584/30000 Training Loss: 0.05696548894047737\n",
      "Epoch 3585/30000 Training Loss: 0.06904327124357224\n",
      "Epoch 3586/30000 Training Loss: 0.06887197494506836\n",
      "Epoch 3587/30000 Training Loss: 0.06251417845487595\n",
      "Epoch 3588/30000 Training Loss: 0.06322457641363144\n",
      "Epoch 3589/30000 Training Loss: 0.05913469195365906\n",
      "Epoch 3590/30000 Training Loss: 0.06826326251029968\n",
      "Epoch 3591/30000 Training Loss: 0.0616193562746048\n",
      "Epoch 3592/30000 Training Loss: 0.058226391673088074\n",
      "Epoch 3593/30000 Training Loss: 0.07013653218746185\n",
      "Epoch 3594/30000 Training Loss: 0.07474552094936371\n",
      "Epoch 3595/30000 Training Loss: 0.060956209897994995\n",
      "Epoch 3596/30000 Training Loss: 0.07067157328128815\n",
      "Epoch 3597/30000 Training Loss: 0.046390652656555176\n",
      "Epoch 3598/30000 Training Loss: 0.08786004036664963\n",
      "Epoch 3599/30000 Training Loss: 0.06613610684871674\n",
      "Epoch 3600/30000 Training Loss: 0.07825858891010284\n",
      "Epoch 3600/30000 Validation Loss: 0.08147484809160233\n",
      "Epoch 3601/30000 Training Loss: 0.05428370460867882\n",
      "Epoch 3602/30000 Training Loss: 0.0878041535615921\n",
      "Epoch 3603/30000 Training Loss: 0.06276308000087738\n",
      "Epoch 3604/30000 Training Loss: 0.06925714761018753\n",
      "Epoch 3605/30000 Training Loss: 0.05097361281514168\n",
      "Epoch 3606/30000 Training Loss: 0.0750085711479187\n",
      "Epoch 3607/30000 Training Loss: 0.06585544347763062\n",
      "Epoch 3608/30000 Training Loss: 0.07346665859222412\n",
      "Epoch 3609/30000 Training Loss: 0.05981036648154259\n",
      "Epoch 3610/30000 Training Loss: 0.0646701529622078\n",
      "Epoch 3611/30000 Training Loss: 0.07274772226810455\n",
      "Epoch 3612/30000 Training Loss: 0.06258589029312134\n",
      "Epoch 3613/30000 Training Loss: 0.05446663126349449\n",
      "Epoch 3614/30000 Training Loss: 0.06436311453580856\n",
      "Epoch 3615/30000 Training Loss: 0.04874388501048088\n",
      "Epoch 3616/30000 Training Loss: 0.07483594864606857\n",
      "Epoch 3617/30000 Training Loss: 0.054131194949150085\n",
      "Epoch 3618/30000 Training Loss: 0.07311282306909561\n",
      "Epoch 3619/30000 Training Loss: 0.08157409727573395\n",
      "Epoch 3620/30000 Training Loss: 0.07025054097175598\n",
      "Epoch 3621/30000 Training Loss: 0.04909083992242813\n",
      "Epoch 3622/30000 Training Loss: 0.06865714490413666\n",
      "Epoch 3623/30000 Training Loss: 0.07037104666233063\n",
      "Epoch 3624/30000 Training Loss: 0.051817089319229126\n",
      "Epoch 3625/30000 Training Loss: 0.052771877497434616\n",
      "Epoch 3626/30000 Training Loss: 0.06296321004629135\n",
      "Epoch 3627/30000 Training Loss: 0.06903382390737534\n",
      "Epoch 3628/30000 Training Loss: 0.06553953886032104\n",
      "Epoch 3629/30000 Training Loss: 0.07826926559209824\n",
      "Epoch 3630/30000 Training Loss: 0.0514121912419796\n",
      "Epoch 3631/30000 Training Loss: 0.05285044014453888\n",
      "Epoch 3632/30000 Training Loss: 0.0748620554804802\n",
      "Epoch 3633/30000 Training Loss: 0.06385687738656998\n",
      "Epoch 3634/30000 Training Loss: 0.06306180357933044\n",
      "Epoch 3635/30000 Training Loss: 0.06093485653400421\n",
      "Epoch 3636/30000 Training Loss: 0.07856427878141403\n",
      "Epoch 3637/30000 Training Loss: 0.08114892244338989\n",
      "Epoch 3638/30000 Training Loss: 0.057965584099292755\n",
      "Epoch 3639/30000 Training Loss: 0.07062689960002899\n",
      "Epoch 3640/30000 Training Loss: 0.07807691395282745\n",
      "Epoch 3641/30000 Training Loss: 0.04925590753555298\n",
      "Epoch 3642/30000 Training Loss: 0.05571023374795914\n",
      "Epoch 3643/30000 Training Loss: 0.06615674495697021\n",
      "Epoch 3644/30000 Training Loss: 0.0912887305021286\n",
      "Epoch 3645/30000 Training Loss: 0.05469929426908493\n",
      "Epoch 3646/30000 Training Loss: 0.07504761219024658\n",
      "Epoch 3647/30000 Training Loss: 0.0738883763551712\n",
      "Epoch 3648/30000 Training Loss: 0.05214862525463104\n",
      "Epoch 3649/30000 Training Loss: 0.06116601824760437\n",
      "Epoch 3650/30000 Training Loss: 0.08038130402565002\n",
      "Epoch 3651/30000 Training Loss: 0.05244868993759155\n",
      "Epoch 3652/30000 Training Loss: 0.06244516000151634\n",
      "Epoch 3653/30000 Training Loss: 0.04947430640459061\n",
      "Epoch 3654/30000 Training Loss: 0.06595960259437561\n",
      "Epoch 3655/30000 Training Loss: 0.06730077415704727\n",
      "Epoch 3656/30000 Training Loss: 0.06935499608516693\n",
      "Epoch 3657/30000 Training Loss: 0.06075294315814972\n",
      "Epoch 3658/30000 Training Loss: 0.0722772628068924\n",
      "Epoch 3659/30000 Training Loss: 0.06745418906211853\n",
      "Epoch 3660/30000 Training Loss: 0.06691593676805496\n",
      "Epoch 3661/30000 Training Loss: 0.06882400810718536\n",
      "Epoch 3662/30000 Training Loss: 0.08686017990112305\n",
      "Epoch 3663/30000 Training Loss: 0.06515926122665405\n",
      "Epoch 3664/30000 Training Loss: 0.06836292147636414\n",
      "Epoch 3665/30000 Training Loss: 0.062210358679294586\n",
      "Epoch 3666/30000 Training Loss: 0.05930548161268234\n",
      "Epoch 3667/30000 Training Loss: 0.05710109323263168\n",
      "Epoch 3668/30000 Training Loss: 0.0675138309597969\n",
      "Epoch 3669/30000 Training Loss: 0.06013491004705429\n",
      "Epoch 3670/30000 Training Loss: 0.07839144766330719\n",
      "Epoch 3671/30000 Training Loss: 0.04906930774450302\n",
      "Epoch 3672/30000 Training Loss: 0.06702202558517456\n",
      "Epoch 3673/30000 Training Loss: 0.06139402836561203\n",
      "Epoch 3674/30000 Training Loss: 0.0675419420003891\n",
      "Epoch 3675/30000 Training Loss: 0.05260724574327469\n",
      "Epoch 3676/30000 Training Loss: 0.054527461528778076\n",
      "Epoch 3677/30000 Training Loss: 0.06628783047199249\n",
      "Epoch 3678/30000 Training Loss: 0.07245239615440369\n",
      "Epoch 3679/30000 Training Loss: 0.07119084149599075\n",
      "Epoch 3680/30000 Training Loss: 0.07977297902107239\n",
      "Epoch 3681/30000 Training Loss: 0.05973304808139801\n",
      "Epoch 3682/30000 Training Loss: 0.05800773203372955\n",
      "Epoch 3683/30000 Training Loss: 0.05742964893579483\n",
      "Epoch 3684/30000 Training Loss: 0.054583270102739334\n",
      "Epoch 3685/30000 Training Loss: 0.06836190819740295\n",
      "Epoch 3686/30000 Training Loss: 0.07047316431999207\n",
      "Epoch 3687/30000 Training Loss: 0.0665493980050087\n",
      "Epoch 3688/30000 Training Loss: 0.058035146445035934\n",
      "Epoch 3689/30000 Training Loss: 0.06691943854093552\n",
      "Epoch 3690/30000 Training Loss: 0.067166268825531\n",
      "Epoch 3691/30000 Training Loss: 0.07724469900131226\n",
      "Epoch 3692/30000 Training Loss: 0.054647572338581085\n",
      "Epoch 3693/30000 Training Loss: 0.07136451452970505\n",
      "Epoch 3694/30000 Training Loss: 0.07972448319196701\n",
      "Epoch 3695/30000 Training Loss: 0.08488893508911133\n",
      "Epoch 3696/30000 Training Loss: 0.08044574409723282\n",
      "Epoch 3697/30000 Training Loss: 0.05957653373479843\n",
      "Epoch 3698/30000 Training Loss: 0.0791902169585228\n",
      "Epoch 3699/30000 Training Loss: 0.06047239899635315\n",
      "Epoch 3700/30000 Training Loss: 0.06595694273710251\n",
      "Epoch 3700/30000 Validation Loss: 0.06725208461284637\n",
      "Epoch 3701/30000 Training Loss: 0.057596705853939056\n",
      "Epoch 3702/30000 Training Loss: 0.06568969041109085\n",
      "Epoch 3703/30000 Training Loss: 0.07830555737018585\n",
      "Epoch 3704/30000 Training Loss: 0.054590459913015366\n",
      "Epoch 3705/30000 Training Loss: 0.05829286575317383\n",
      "Epoch 3706/30000 Training Loss: 0.061642322689294815\n",
      "Epoch 3707/30000 Training Loss: 0.0859510749578476\n",
      "Epoch 3708/30000 Training Loss: 0.0692351758480072\n",
      "Epoch 3709/30000 Training Loss: 0.0784047320485115\n",
      "Epoch 3710/30000 Training Loss: 0.061757467687129974\n",
      "Epoch 3711/30000 Training Loss: 0.06324496120214462\n",
      "Epoch 3712/30000 Training Loss: 0.07849831879138947\n",
      "Epoch 3713/30000 Training Loss: 0.07508178055286407\n",
      "Epoch 3714/30000 Training Loss: 0.06515899300575256\n",
      "Epoch 3715/30000 Training Loss: 0.06977027654647827\n",
      "Epoch 3716/30000 Training Loss: 0.05667649209499359\n",
      "Epoch 3717/30000 Training Loss: 0.07640906423330307\n",
      "Epoch 3718/30000 Training Loss: 0.0646025761961937\n",
      "Epoch 3719/30000 Training Loss: 0.07502730190753937\n",
      "Epoch 3720/30000 Training Loss: 0.07272152602672577\n",
      "Epoch 3721/30000 Training Loss: 0.06497278809547424\n",
      "Epoch 3722/30000 Training Loss: 0.06952676177024841\n",
      "Epoch 3723/30000 Training Loss: 0.059311941266059875\n",
      "Epoch 3724/30000 Training Loss: 0.06895091384649277\n",
      "Epoch 3725/30000 Training Loss: 0.05208224803209305\n",
      "Epoch 3726/30000 Training Loss: 0.0932110846042633\n",
      "Epoch 3727/30000 Training Loss: 0.0722103863954544\n",
      "Epoch 3728/30000 Training Loss: 0.06620333343744278\n",
      "Epoch 3729/30000 Training Loss: 0.08653142303228378\n",
      "Epoch 3730/30000 Training Loss: 0.06801089644432068\n",
      "Epoch 3731/30000 Training Loss: 0.07290830463171005\n",
      "Epoch 3732/30000 Training Loss: 0.057147134095430374\n",
      "Epoch 3733/30000 Training Loss: 0.0763530433177948\n",
      "Epoch 3734/30000 Training Loss: 0.06372383236885071\n",
      "Epoch 3735/30000 Training Loss: 0.07797457277774811\n",
      "Epoch 3736/30000 Training Loss: 0.06089058890938759\n",
      "Epoch 3737/30000 Training Loss: 0.07633417099714279\n",
      "Epoch 3738/30000 Training Loss: 0.07669349014759064\n",
      "Epoch 3739/30000 Training Loss: 0.06714019924402237\n",
      "Epoch 3740/30000 Training Loss: 0.04847876727581024\n",
      "Epoch 3741/30000 Training Loss: 0.07585129886865616\n",
      "Epoch 3742/30000 Training Loss: 0.06482550501823425\n",
      "Epoch 3743/30000 Training Loss: 0.057198457419872284\n",
      "Epoch 3744/30000 Training Loss: 0.07926460355520248\n",
      "Epoch 3745/30000 Training Loss: 0.09437214583158493\n",
      "Epoch 3746/30000 Training Loss: 0.07754530012607574\n",
      "Epoch 3747/30000 Training Loss: 0.0695505291223526\n",
      "Epoch 3748/30000 Training Loss: 0.07654668390750885\n",
      "Epoch 3749/30000 Training Loss: 0.06699927896261215\n",
      "Epoch 3750/30000 Training Loss: 0.06256276369094849\n",
      "Epoch 3751/30000 Training Loss: 0.06396450102329254\n",
      "Epoch 3752/30000 Training Loss: 0.059550557285547256\n",
      "Epoch 3753/30000 Training Loss: 0.06061490997672081\n",
      "Epoch 3754/30000 Training Loss: 0.06351423263549805\n",
      "Epoch 3755/30000 Training Loss: 0.06871816515922546\n",
      "Epoch 3756/30000 Training Loss: 0.05456223711371422\n",
      "Epoch 3757/30000 Training Loss: 0.08406856656074524\n",
      "Epoch 3758/30000 Training Loss: 0.06601230055093765\n",
      "Epoch 3759/30000 Training Loss: 0.05404534190893173\n",
      "Epoch 3760/30000 Training Loss: 0.06951996684074402\n",
      "Epoch 3761/30000 Training Loss: 0.06122184917330742\n",
      "Epoch 3762/30000 Training Loss: 0.08155931532382965\n",
      "Epoch 3763/30000 Training Loss: 0.07762300968170166\n",
      "Epoch 3764/30000 Training Loss: 0.06089916452765465\n",
      "Epoch 3765/30000 Training Loss: 0.073221355676651\n",
      "Epoch 3766/30000 Training Loss: 0.08242669701576233\n",
      "Epoch 3767/30000 Training Loss: 0.049999117851257324\n",
      "Epoch 3768/30000 Training Loss: 0.06959813833236694\n",
      "Epoch 3769/30000 Training Loss: 0.07273226231336594\n",
      "Epoch 3770/30000 Training Loss: 0.06563378870487213\n",
      "Epoch 3771/30000 Training Loss: 0.06350705772638321\n",
      "Epoch 3772/30000 Training Loss: 0.061255984008312225\n",
      "Epoch 3773/30000 Training Loss: 0.05859927088022232\n",
      "Epoch 3774/30000 Training Loss: 0.05204150080680847\n",
      "Epoch 3775/30000 Training Loss: 0.07068464905023575\n",
      "Epoch 3776/30000 Training Loss: 0.08007150888442993\n",
      "Epoch 3777/30000 Training Loss: 0.07183614373207092\n",
      "Epoch 3778/30000 Training Loss: 0.05175526440143585\n",
      "Epoch 3779/30000 Training Loss: 0.05847756564617157\n",
      "Epoch 3780/30000 Training Loss: 0.06274672597646713\n",
      "Epoch 3781/30000 Training Loss: 0.06812819838523865\n",
      "Epoch 3782/30000 Training Loss: 0.05802154168486595\n",
      "Epoch 3783/30000 Training Loss: 0.05241507664322853\n",
      "Epoch 3784/30000 Training Loss: 0.06346771121025085\n",
      "Epoch 3785/30000 Training Loss: 0.06641419231891632\n",
      "Epoch 3786/30000 Training Loss: 0.06634799391031265\n",
      "Epoch 3787/30000 Training Loss: 0.07935970276594162\n",
      "Epoch 3788/30000 Training Loss: 0.06309743225574493\n",
      "Epoch 3789/30000 Training Loss: 0.07508693635463715\n",
      "Epoch 3790/30000 Training Loss: 0.05375507101416588\n",
      "Epoch 3791/30000 Training Loss: 0.062393028289079666\n",
      "Epoch 3792/30000 Training Loss: 0.07448596507310867\n",
      "Epoch 3793/30000 Training Loss: 0.05867816135287285\n",
      "Epoch 3794/30000 Training Loss: 0.061659976840019226\n",
      "Epoch 3795/30000 Training Loss: 0.06224081665277481\n",
      "Epoch 3796/30000 Training Loss: 0.09050456434488297\n",
      "Epoch 3797/30000 Training Loss: 0.07128766179084778\n",
      "Epoch 3798/30000 Training Loss: 0.07299713790416718\n",
      "Epoch 3799/30000 Training Loss: 0.0612647607922554\n",
      "Epoch 3800/30000 Training Loss: 0.05618426948785782\n",
      "Epoch 3800/30000 Validation Loss: 0.06842691451311111\n",
      "Epoch 3801/30000 Training Loss: 0.06587594747543335\n",
      "Epoch 3802/30000 Training Loss: 0.06334090977907181\n",
      "Epoch 3803/30000 Training Loss: 0.07057817280292511\n",
      "Epoch 3804/30000 Training Loss: 0.06854577362537384\n",
      "Epoch 3805/30000 Training Loss: 0.07116307318210602\n",
      "Epoch 3806/30000 Training Loss: 0.06463000178337097\n",
      "Epoch 3807/30000 Training Loss: 0.052471987903118134\n",
      "Epoch 3808/30000 Training Loss: 0.06430459767580032\n",
      "Epoch 3809/30000 Training Loss: 0.07314161956310272\n",
      "Epoch 3810/30000 Training Loss: 0.057421036064624786\n",
      "Epoch 3811/30000 Training Loss: 0.07759181410074234\n",
      "Epoch 3812/30000 Training Loss: 0.07383939623832703\n",
      "Epoch 3813/30000 Training Loss: 0.0458921380341053\n",
      "Epoch 3814/30000 Training Loss: 0.057787373661994934\n",
      "Epoch 3815/30000 Training Loss: 0.06165456771850586\n",
      "Epoch 3816/30000 Training Loss: 0.0719527006149292\n",
      "Epoch 3817/30000 Training Loss: 0.0843162089586258\n",
      "Epoch 3818/30000 Training Loss: 0.06344883143901825\n",
      "Epoch 3819/30000 Training Loss: 0.06428409367799759\n",
      "Epoch 3820/30000 Training Loss: 0.05451192706823349\n",
      "Epoch 3821/30000 Training Loss: 0.06198015436530113\n",
      "Epoch 3822/30000 Training Loss: 0.06129280477762222\n",
      "Epoch 3823/30000 Training Loss: 0.08479659259319305\n",
      "Epoch 3824/30000 Training Loss: 0.07656880468130112\n",
      "Epoch 3825/30000 Training Loss: 0.06643810123205185\n",
      "Epoch 3826/30000 Training Loss: 0.07370277494192123\n",
      "Epoch 3827/30000 Training Loss: 0.08604227751493454\n",
      "Epoch 3828/30000 Training Loss: 0.052842795848846436\n",
      "Epoch 3829/30000 Training Loss: 0.07951103150844574\n",
      "Epoch 3830/30000 Training Loss: 0.052207835018634796\n",
      "Epoch 3831/30000 Training Loss: 0.06409840285778046\n",
      "Epoch 3832/30000 Training Loss: 0.056008223444223404\n",
      "Epoch 3833/30000 Training Loss: 0.0525905042886734\n",
      "Epoch 3834/30000 Training Loss: 0.07739575207233429\n",
      "Epoch 3835/30000 Training Loss: 0.06835425645112991\n",
      "Epoch 3836/30000 Training Loss: 0.07299140840768814\n",
      "Epoch 3837/30000 Training Loss: 0.07693436741828918\n",
      "Epoch 3838/30000 Training Loss: 0.0792015939950943\n",
      "Epoch 3839/30000 Training Loss: 0.0480075441300869\n",
      "Epoch 3840/30000 Training Loss: 0.06578566133975983\n",
      "Epoch 3841/30000 Training Loss: 0.0527779646217823\n",
      "Epoch 3842/30000 Training Loss: 0.0784098207950592\n",
      "Epoch 3843/30000 Training Loss: 0.0780123770236969\n",
      "Epoch 3844/30000 Training Loss: 0.06963539123535156\n",
      "Epoch 3845/30000 Training Loss: 0.07441867142915726\n",
      "Epoch 3846/30000 Training Loss: 0.07244739681482315\n",
      "Epoch 3847/30000 Training Loss: 0.06158246845006943\n",
      "Epoch 3848/30000 Training Loss: 0.08364053070545197\n",
      "Epoch 3849/30000 Training Loss: 0.07424771040678024\n",
      "Epoch 3850/30000 Training Loss: 0.07439790666103363\n",
      "Epoch 3851/30000 Training Loss: 0.058348312973976135\n",
      "Epoch 3852/30000 Training Loss: 0.07288096845149994\n",
      "Epoch 3853/30000 Training Loss: 0.07215359807014465\n",
      "Epoch 3854/30000 Training Loss: 0.06237690895795822\n",
      "Epoch 3855/30000 Training Loss: 0.05861646682024002\n",
      "Epoch 3856/30000 Training Loss: 0.047521255910396576\n",
      "Epoch 3857/30000 Training Loss: 0.0613650307059288\n",
      "Epoch 3858/30000 Training Loss: 0.06785620748996735\n",
      "Epoch 3859/30000 Training Loss: 0.0586857944726944\n",
      "Epoch 3860/30000 Training Loss: 0.07452882081270218\n",
      "Epoch 3861/30000 Training Loss: 0.050705477595329285\n",
      "Epoch 3862/30000 Training Loss: 0.07764234393835068\n",
      "Epoch 3863/30000 Training Loss: 0.05928483232855797\n",
      "Epoch 3864/30000 Training Loss: 0.07670619338750839\n",
      "Epoch 3865/30000 Training Loss: 0.06278326362371445\n",
      "Epoch 3866/30000 Training Loss: 0.06913693249225616\n",
      "Epoch 3867/30000 Training Loss: 0.054333385080099106\n",
      "Epoch 3868/30000 Training Loss: 0.06257206946611404\n",
      "Epoch 3869/30000 Training Loss: 0.06497494131326675\n",
      "Epoch 3870/30000 Training Loss: 0.05922120064496994\n",
      "Epoch 3871/30000 Training Loss: 0.053678080439567566\n",
      "Epoch 3872/30000 Training Loss: 0.05132899805903435\n",
      "Epoch 3873/30000 Training Loss: 0.07417905330657959\n",
      "Epoch 3874/30000 Training Loss: 0.07614690065383911\n",
      "Epoch 3875/30000 Training Loss: 0.059112656861543655\n",
      "Epoch 3876/30000 Training Loss: 0.06807798147201538\n",
      "Epoch 3877/30000 Training Loss: 0.058482322841882706\n",
      "Epoch 3878/30000 Training Loss: 0.06621228158473969\n",
      "Epoch 3879/30000 Training Loss: 0.050197530537843704\n",
      "Epoch 3880/30000 Training Loss: 0.08028183877468109\n",
      "Epoch 3881/30000 Training Loss: 0.07434415817260742\n",
      "Epoch 3882/30000 Training Loss: 0.06437544524669647\n",
      "Epoch 3883/30000 Training Loss: 0.0682343989610672\n",
      "Epoch 3884/30000 Training Loss: 0.05861185863614082\n",
      "Epoch 3885/30000 Training Loss: 0.06948012113571167\n",
      "Epoch 3886/30000 Training Loss: 0.0760890319943428\n",
      "Epoch 3887/30000 Training Loss: 0.0743301510810852\n",
      "Epoch 3888/30000 Training Loss: 0.05746500939130783\n",
      "Epoch 3889/30000 Training Loss: 0.08206866681575775\n",
      "Epoch 3890/30000 Training Loss: 0.0688771903514862\n",
      "Epoch 3891/30000 Training Loss: 0.0564635694026947\n",
      "Epoch 3892/30000 Training Loss: 0.06496642529964447\n",
      "Epoch 3893/30000 Training Loss: 0.0754929929971695\n",
      "Epoch 3894/30000 Training Loss: 0.06431946903467178\n",
      "Epoch 3895/30000 Training Loss: 0.05772774666547775\n",
      "Epoch 3896/30000 Training Loss: 0.0863862931728363\n",
      "Epoch 3897/30000 Training Loss: 0.073863685131073\n",
      "Epoch 3898/30000 Training Loss: 0.05271098390221596\n",
      "Epoch 3899/30000 Training Loss: 0.08567747473716736\n",
      "Epoch 3900/30000 Training Loss: 0.06418056041002274\n",
      "Epoch 3900/30000 Validation Loss: 0.0725429356098175\n",
      "Epoch 3901/30000 Training Loss: 0.04774187132716179\n",
      "Epoch 3902/30000 Training Loss: 0.08369822800159454\n",
      "Epoch 3903/30000 Training Loss: 0.06375589221715927\n",
      "Epoch 3904/30000 Training Loss: 0.07610785961151123\n",
      "Epoch 3905/30000 Training Loss: 0.054296426475048065\n",
      "Epoch 3906/30000 Training Loss: 0.07751438766717911\n",
      "Epoch 3907/30000 Training Loss: 0.06532634794712067\n",
      "Epoch 3908/30000 Training Loss: 0.07734552025794983\n",
      "Epoch 3909/30000 Training Loss: 0.060893744230270386\n",
      "Epoch 3910/30000 Training Loss: 0.050257306545972824\n",
      "Epoch 3911/30000 Training Loss: 0.07081345468759537\n",
      "Epoch 3912/30000 Training Loss: 0.06241830438375473\n",
      "Epoch 3913/30000 Training Loss: 0.06415410339832306\n",
      "Epoch 3914/30000 Training Loss: 0.05942787975072861\n",
      "Epoch 3915/30000 Training Loss: 0.059241797775030136\n",
      "Epoch 3916/30000 Training Loss: 0.07293655723333359\n",
      "Epoch 3917/30000 Training Loss: 0.0587477870285511\n",
      "Epoch 3918/30000 Training Loss: 0.057399552315473557\n",
      "Epoch 3919/30000 Training Loss: 0.05579189956188202\n",
      "Epoch 3920/30000 Training Loss: 0.07239274680614471\n",
      "Epoch 3921/30000 Training Loss: 0.07356557995080948\n",
      "Epoch 3922/30000 Training Loss: 0.07367996126413345\n",
      "Epoch 3923/30000 Training Loss: 0.052856698632240295\n",
      "Epoch 3924/30000 Training Loss: 0.05566522479057312\n",
      "Epoch 3925/30000 Training Loss: 0.07973159849643707\n",
      "Epoch 3926/30000 Training Loss: 0.06499064713716507\n",
      "Epoch 3927/30000 Training Loss: 0.06313234567642212\n",
      "Epoch 3928/30000 Training Loss: 0.07961362600326538\n",
      "Epoch 3929/30000 Training Loss: 0.0642087385058403\n",
      "Epoch 3930/30000 Training Loss: 0.05658314377069473\n",
      "Epoch 3931/30000 Training Loss: 0.05417080596089363\n",
      "Epoch 3932/30000 Training Loss: 0.06695530563592911\n",
      "Epoch 3933/30000 Training Loss: 0.06483307480812073\n",
      "Epoch 3934/30000 Training Loss: 0.06734122335910797\n",
      "Epoch 3935/30000 Training Loss: 0.07245989143848419\n",
      "Epoch 3936/30000 Training Loss: 0.08219774067401886\n",
      "Epoch 3937/30000 Training Loss: 0.08478815853595734\n",
      "Epoch 3938/30000 Training Loss: 0.06251237541437149\n",
      "Epoch 3939/30000 Training Loss: 0.06427561491727829\n",
      "Epoch 3940/30000 Training Loss: 0.05108778551220894\n",
      "Epoch 3941/30000 Training Loss: 0.0547015480697155\n",
      "Epoch 3942/30000 Training Loss: 0.06723099946975708\n",
      "Epoch 3943/30000 Training Loss: 0.05331414192914963\n",
      "Epoch 3944/30000 Training Loss: 0.06610284000635147\n",
      "Epoch 3945/30000 Training Loss: 0.0524931475520134\n",
      "Epoch 3946/30000 Training Loss: 0.0686202347278595\n",
      "Epoch 3947/30000 Training Loss: 0.06465636938810349\n",
      "Epoch 3948/30000 Training Loss: 0.0539766326546669\n",
      "Epoch 3949/30000 Training Loss: 0.07467451691627502\n",
      "Epoch 3950/30000 Training Loss: 0.06423218548297882\n",
      "Epoch 3951/30000 Training Loss: 0.048508964478969574\n",
      "Epoch 3952/30000 Training Loss: 0.04802339896559715\n",
      "Epoch 3953/30000 Training Loss: 0.052149780094623566\n",
      "Epoch 3954/30000 Training Loss: 0.08352455496788025\n",
      "Epoch 3955/30000 Training Loss: 0.07431214302778244\n",
      "Epoch 3956/30000 Training Loss: 0.06479012221097946\n",
      "Epoch 3957/30000 Training Loss: 0.07864464074373245\n",
      "Epoch 3958/30000 Training Loss: 0.061449259519577026\n",
      "Epoch 3959/30000 Training Loss: 0.07142362743616104\n",
      "Epoch 3960/30000 Training Loss: 0.07346836477518082\n",
      "Epoch 3961/30000 Training Loss: 0.07255677878856659\n",
      "Epoch 3962/30000 Training Loss: 0.07142952084541321\n",
      "Epoch 3963/30000 Training Loss: 0.05164103955030441\n",
      "Epoch 3964/30000 Training Loss: 0.06576242297887802\n",
      "Epoch 3965/30000 Training Loss: 0.05462344363331795\n",
      "Epoch 3966/30000 Training Loss: 0.06914821267127991\n",
      "Epoch 3967/30000 Training Loss: 0.06099721044301987\n",
      "Epoch 3968/30000 Training Loss: 0.06837592273950577\n",
      "Epoch 3969/30000 Training Loss: 0.0730925127863884\n",
      "Epoch 3970/30000 Training Loss: 0.061415426433086395\n",
      "Epoch 3971/30000 Training Loss: 0.05616405978798866\n",
      "Epoch 3972/30000 Training Loss: 0.06787744164466858\n",
      "Epoch 3973/30000 Training Loss: 0.0744524598121643\n",
      "Epoch 3974/30000 Training Loss: 0.06514756381511688\n",
      "Epoch 3975/30000 Training Loss: 0.06623800843954086\n",
      "Epoch 3976/30000 Training Loss: 0.06767306476831436\n",
      "Epoch 3977/30000 Training Loss: 0.06700210273265839\n",
      "Epoch 3978/30000 Training Loss: 0.059047479182481766\n",
      "Epoch 3979/30000 Training Loss: 0.06800878793001175\n",
      "Epoch 3980/30000 Training Loss: 0.06977725774049759\n",
      "Epoch 3981/30000 Training Loss: 0.04832460731267929\n",
      "Epoch 3982/30000 Training Loss: 0.06068270653486252\n",
      "Epoch 3983/30000 Training Loss: 0.06958573311567307\n",
      "Epoch 3984/30000 Training Loss: 0.06734588742256165\n",
      "Epoch 3985/30000 Training Loss: 0.05471229925751686\n",
      "Epoch 3986/30000 Training Loss: 0.06286071240901947\n",
      "Epoch 3987/30000 Training Loss: 0.0746108740568161\n",
      "Epoch 3988/30000 Training Loss: 0.07716821879148483\n",
      "Epoch 3989/30000 Training Loss: 0.07315626740455627\n",
      "Epoch 3990/30000 Training Loss: 0.05819416046142578\n",
      "Epoch 3991/30000 Training Loss: 0.059655893594026566\n",
      "Epoch 3992/30000 Training Loss: 0.06821899861097336\n",
      "Epoch 3993/30000 Training Loss: 0.06823630630970001\n",
      "Epoch 3994/30000 Training Loss: 0.047291774302721024\n",
      "Epoch 3995/30000 Training Loss: 0.05793797969818115\n",
      "Epoch 3996/30000 Training Loss: 0.06357523053884506\n",
      "Epoch 3997/30000 Training Loss: 0.0513957217335701\n",
      "Epoch 3998/30000 Training Loss: 0.060298122465610504\n",
      "Epoch 3999/30000 Training Loss: 0.07529722154140472\n",
      "Epoch 4000/30000 Training Loss: 0.06219654157757759\n",
      "Epoch 4000/30000 Validation Loss: 0.0611557811498642\n",
      "Epoch 4001/30000 Training Loss: 0.04640379175543785\n",
      "Epoch 4002/30000 Training Loss: 0.05439707264304161\n",
      "Epoch 4003/30000 Training Loss: 0.08645864576101303\n",
      "Epoch 4004/30000 Training Loss: 0.07163409888744354\n",
      "Epoch 4005/30000 Training Loss: 0.05523422360420227\n",
      "Epoch 4006/30000 Training Loss: 0.05653166025876999\n",
      "Epoch 4007/30000 Training Loss: 0.061306532472372055\n",
      "Epoch 4008/30000 Training Loss: 0.0657232403755188\n",
      "Epoch 4009/30000 Training Loss: 0.07059068232774734\n",
      "Epoch 4010/30000 Training Loss: 0.07001003623008728\n",
      "Epoch 4011/30000 Training Loss: 0.04536610096693039\n",
      "Epoch 4012/30000 Training Loss: 0.05739832669496536\n",
      "Epoch 4013/30000 Training Loss: 0.05506126955151558\n",
      "Epoch 4014/30000 Training Loss: 0.06016837805509567\n",
      "Epoch 4015/30000 Training Loss: 0.07965356856584549\n",
      "Epoch 4016/30000 Training Loss: 0.05083925649523735\n",
      "Epoch 4017/30000 Training Loss: 0.08312725275754929\n",
      "Epoch 4018/30000 Training Loss: 0.06128672510385513\n",
      "Epoch 4019/30000 Training Loss: 0.053226374089717865\n",
      "Epoch 4020/30000 Training Loss: 0.06026852875947952\n",
      "Epoch 4021/30000 Training Loss: 0.07185617834329605\n",
      "Epoch 4022/30000 Training Loss: 0.0608418732881546\n",
      "Epoch 4023/30000 Training Loss: 0.05422613024711609\n",
      "Epoch 4024/30000 Training Loss: 0.062200237065553665\n",
      "Epoch 4025/30000 Training Loss: 0.05931593477725983\n",
      "Epoch 4026/30000 Training Loss: 0.07706830650568008\n",
      "Epoch 4027/30000 Training Loss: 0.07993052899837494\n",
      "Epoch 4028/30000 Training Loss: 0.05071568489074707\n",
      "Epoch 4029/30000 Training Loss: 0.054093897342681885\n",
      "Epoch 4030/30000 Training Loss: 0.05862290784716606\n",
      "Epoch 4031/30000 Training Loss: 0.07169292122125626\n",
      "Epoch 4032/30000 Training Loss: 0.05699135363101959\n",
      "Epoch 4033/30000 Training Loss: 0.0662410780787468\n",
      "Epoch 4034/30000 Training Loss: 0.057010337710380554\n",
      "Epoch 4035/30000 Training Loss: 0.06709322333335876\n",
      "Epoch 4036/30000 Training Loss: 0.0673055648803711\n",
      "Epoch 4037/30000 Training Loss: 0.0631849393248558\n",
      "Epoch 4038/30000 Training Loss: 0.0673147439956665\n",
      "Epoch 4039/30000 Training Loss: 0.06063548102974892\n",
      "Epoch 4040/30000 Training Loss: 0.05805981531739235\n",
      "Epoch 4041/30000 Training Loss: 0.07880719006061554\n",
      "Epoch 4042/30000 Training Loss: 0.07355567067861557\n",
      "Epoch 4043/30000 Training Loss: 0.06530939042568207\n",
      "Epoch 4044/30000 Training Loss: 0.0773647129535675\n",
      "Epoch 4045/30000 Training Loss: 0.04587700963020325\n",
      "Epoch 4046/30000 Training Loss: 0.07499578595161438\n",
      "Epoch 4047/30000 Training Loss: 0.06177910417318344\n",
      "Epoch 4048/30000 Training Loss: 0.06328074634075165\n",
      "Epoch 4049/30000 Training Loss: 0.058360472321510315\n",
      "Epoch 4050/30000 Training Loss: 0.06582212448120117\n",
      "Epoch 4051/30000 Training Loss: 0.06783803552389145\n",
      "Epoch 4052/30000 Training Loss: 0.06685973703861237\n",
      "Epoch 4053/30000 Training Loss: 0.05322182923555374\n",
      "Epoch 4054/30000 Training Loss: 0.05340033024549484\n",
      "Epoch 4055/30000 Training Loss: 0.0631234273314476\n",
      "Epoch 4056/30000 Training Loss: 0.07462631165981293\n",
      "Epoch 4057/30000 Training Loss: 0.0619550421833992\n",
      "Epoch 4058/30000 Training Loss: 0.06652137637138367\n",
      "Epoch 4059/30000 Training Loss: 0.06473113596439362\n",
      "Epoch 4060/30000 Training Loss: 0.08734722435474396\n",
      "Epoch 4061/30000 Training Loss: 0.055632248520851135\n",
      "Epoch 4062/30000 Training Loss: 0.06680004298686981\n",
      "Epoch 4063/30000 Training Loss: 0.07538463175296783\n",
      "Epoch 4064/30000 Training Loss: 0.05199518799781799\n",
      "Epoch 4065/30000 Training Loss: 0.05343317240476608\n",
      "Epoch 4066/30000 Training Loss: 0.05247831344604492\n",
      "Epoch 4067/30000 Training Loss: 0.048443280160427094\n",
      "Epoch 4068/30000 Training Loss: 0.056039247661828995\n",
      "Epoch 4069/30000 Training Loss: 0.05937251076102257\n",
      "Epoch 4070/30000 Training Loss: 0.060484014451503754\n",
      "Epoch 4071/30000 Training Loss: 0.057519346475601196\n",
      "Epoch 4072/30000 Training Loss: 0.05999975651502609\n",
      "Epoch 4073/30000 Training Loss: 0.06920211017131805\n",
      "Epoch 4074/30000 Training Loss: 0.07511398196220398\n",
      "Epoch 4075/30000 Training Loss: 0.06318248063325882\n",
      "Epoch 4076/30000 Training Loss: 0.07603752613067627\n",
      "Epoch 4077/30000 Training Loss: 0.06766173988580704\n",
      "Epoch 4078/30000 Training Loss: 0.054359763860702515\n",
      "Epoch 4079/30000 Training Loss: 0.07038642466068268\n",
      "Epoch 4080/30000 Training Loss: 0.06596820801496506\n",
      "Epoch 4081/30000 Training Loss: 0.049060456454753876\n",
      "Epoch 4082/30000 Training Loss: 0.08939877897500992\n",
      "Epoch 4083/30000 Training Loss: 0.05422205477952957\n",
      "Epoch 4084/30000 Training Loss: 0.06654667854309082\n",
      "Epoch 4085/30000 Training Loss: 0.06494710594415665\n",
      "Epoch 4086/30000 Training Loss: 0.06731973588466644\n",
      "Epoch 4087/30000 Training Loss: 0.0641787052154541\n",
      "Epoch 4088/30000 Training Loss: 0.07566580176353455\n",
      "Epoch 4089/30000 Training Loss: 0.06699356436729431\n",
      "Epoch 4090/30000 Training Loss: 0.05750938504934311\n",
      "Epoch 4091/30000 Training Loss: 0.07412507385015488\n",
      "Epoch 4092/30000 Training Loss: 0.07128352671861649\n",
      "Epoch 4093/30000 Training Loss: 0.055664487183094025\n",
      "Epoch 4094/30000 Training Loss: 0.06923054158687592\n",
      "Epoch 4095/30000 Training Loss: 0.07295213639736176\n",
      "Epoch 4096/30000 Training Loss: 0.062360115349292755\n",
      "Epoch 4097/30000 Training Loss: 0.06720945239067078\n",
      "Epoch 4098/30000 Training Loss: 0.07331046462059021\n",
      "Epoch 4099/30000 Training Loss: 0.062292613089084625\n",
      "Epoch 4100/30000 Training Loss: 0.07025862485170364\n",
      "Epoch 4100/30000 Validation Loss: 0.06311783939599991\n",
      "Epoch 4101/30000 Training Loss: 0.05071699619293213\n",
      "Epoch 4102/30000 Training Loss: 0.06675577163696289\n",
      "Epoch 4103/30000 Training Loss: 0.1013374775648117\n",
      "Epoch 4104/30000 Training Loss: 0.06058723106980324\n",
      "Epoch 4105/30000 Training Loss: 0.06421384960412979\n",
      "Epoch 4106/30000 Training Loss: 0.07289842516183853\n",
      "Epoch 4107/30000 Training Loss: 0.0687355175614357\n",
      "Epoch 4108/30000 Training Loss: 0.07660162448883057\n",
      "Epoch 4109/30000 Training Loss: 0.07091142237186432\n",
      "Epoch 4110/30000 Training Loss: 0.06645795702934265\n",
      "Epoch 4111/30000 Training Loss: 0.062155283987522125\n",
      "Epoch 4112/30000 Training Loss: 0.07694167643785477\n",
      "Epoch 4113/30000 Training Loss: 0.06878357380628586\n",
      "Epoch 4114/30000 Training Loss: 0.04718586802482605\n",
      "Epoch 4115/30000 Training Loss: 0.06721555441617966\n",
      "Epoch 4116/30000 Training Loss: 0.05885883420705795\n",
      "Epoch 4117/30000 Training Loss: 0.06870794296264648\n",
      "Epoch 4118/30000 Training Loss: 0.06677772104740143\n",
      "Epoch 4119/30000 Training Loss: 0.05257932096719742\n",
      "Epoch 4120/30000 Training Loss: 0.0700666606426239\n",
      "Epoch 4121/30000 Training Loss: 0.06278945505619049\n",
      "Epoch 4122/30000 Training Loss: 0.0824928879737854\n",
      "Epoch 4123/30000 Training Loss: 0.06838639080524445\n",
      "Epoch 4124/30000 Training Loss: 0.04602569341659546\n",
      "Epoch 4125/30000 Training Loss: 0.07518506795167923\n",
      "Epoch 4126/30000 Training Loss: 0.049819521605968475\n",
      "Epoch 4127/30000 Training Loss: 0.06921855360269547\n",
      "Epoch 4128/30000 Training Loss: 0.0722150132060051\n",
      "Epoch 4129/30000 Training Loss: 0.07346989959478378\n",
      "Epoch 4130/30000 Training Loss: 0.059353359043598175\n",
      "Epoch 4131/30000 Training Loss: 0.06869658082723618\n",
      "Epoch 4132/30000 Training Loss: 0.07014183700084686\n",
      "Epoch 4133/30000 Training Loss: 0.06441658735275269\n",
      "Epoch 4134/30000 Training Loss: 0.07027875632047653\n",
      "Epoch 4135/30000 Training Loss: 0.08141613006591797\n",
      "Epoch 4136/30000 Training Loss: 0.053711891174316406\n",
      "Epoch 4137/30000 Training Loss: 0.09141691774129868\n",
      "Epoch 4138/30000 Training Loss: 0.06817395985126495\n",
      "Epoch 4139/30000 Training Loss: 0.06403373181819916\n",
      "Epoch 4140/30000 Training Loss: 0.06900739669799805\n",
      "Epoch 4141/30000 Training Loss: 0.0535869374871254\n",
      "Epoch 4142/30000 Training Loss: 0.05712565779685974\n",
      "Epoch 4143/30000 Training Loss: 0.08040127158164978\n",
      "Epoch 4144/30000 Training Loss: 0.05768740177154541\n",
      "Epoch 4145/30000 Training Loss: 0.06017529219388962\n",
      "Epoch 4146/30000 Training Loss: 0.060333482921123505\n",
      "Epoch 4147/30000 Training Loss: 0.06999294459819794\n",
      "Epoch 4148/30000 Training Loss: 0.08017051219940186\n",
      "Epoch 4149/30000 Training Loss: 0.055438634008169174\n",
      "Epoch 4150/30000 Training Loss: 0.0645514652132988\n",
      "Epoch 4151/30000 Training Loss: 0.059975601732730865\n",
      "Epoch 4152/30000 Training Loss: 0.06216851994395256\n",
      "Epoch 4153/30000 Training Loss: 0.06268447637557983\n",
      "Epoch 4154/30000 Training Loss: 0.05694153532385826\n",
      "Epoch 4155/30000 Training Loss: 0.05667433142662048\n",
      "Epoch 4156/30000 Training Loss: 0.07229746878147125\n",
      "Epoch 4157/30000 Training Loss: 0.06749647855758667\n",
      "Epoch 4158/30000 Training Loss: 0.07262033224105835\n",
      "Epoch 4159/30000 Training Loss: 0.05347325652837753\n",
      "Epoch 4160/30000 Training Loss: 0.07430779933929443\n",
      "Epoch 4161/30000 Training Loss: 0.07489902526140213\n",
      "Epoch 4162/30000 Training Loss: 0.056501418352127075\n",
      "Epoch 4163/30000 Training Loss: 0.06529127806425095\n",
      "Epoch 4164/30000 Training Loss: 0.07664050906896591\n",
      "Epoch 4165/30000 Training Loss: 0.06424073874950409\n",
      "Epoch 4166/30000 Training Loss: 0.04961853846907616\n",
      "Epoch 4167/30000 Training Loss: 0.07456430047750473\n",
      "Epoch 4168/30000 Training Loss: 0.05337216705083847\n",
      "Epoch 4169/30000 Training Loss: 0.0633542388677597\n",
      "Epoch 4170/30000 Training Loss: 0.07516371458768845\n",
      "Epoch 4171/30000 Training Loss: 0.07823868840932846\n",
      "Epoch 4172/30000 Training Loss: 0.07219758629798889\n",
      "Epoch 4173/30000 Training Loss: 0.0690447986125946\n",
      "Epoch 4174/30000 Training Loss: 0.06531371176242828\n",
      "Epoch 4175/30000 Training Loss: 0.05150168761610985\n",
      "Epoch 4176/30000 Training Loss: 0.07049161940813065\n",
      "Epoch 4177/30000 Training Loss: 0.0609833262860775\n",
      "Epoch 4178/30000 Training Loss: 0.05659063532948494\n",
      "Epoch 4179/30000 Training Loss: 0.048403359949588776\n",
      "Epoch 4180/30000 Training Loss: 0.07524114102125168\n",
      "Epoch 4181/30000 Training Loss: 0.07611799985170364\n",
      "Epoch 4182/30000 Training Loss: 0.05421048030257225\n",
      "Epoch 4183/30000 Training Loss: 0.04429060220718384\n",
      "Epoch 4184/30000 Training Loss: 0.06015513837337494\n",
      "Epoch 4185/30000 Training Loss: 0.0742952823638916\n",
      "Epoch 4186/30000 Training Loss: 0.05209854245185852\n",
      "Epoch 4187/30000 Training Loss: 0.04666402190923691\n",
      "Epoch 4188/30000 Training Loss: 0.05917485058307648\n",
      "Epoch 4189/30000 Training Loss: 0.07902198284864426\n",
      "Epoch 4190/30000 Training Loss: 0.08388078957796097\n",
      "Epoch 4191/30000 Training Loss: 0.05836533010005951\n",
      "Epoch 4192/30000 Training Loss: 0.05343819409608841\n",
      "Epoch 4193/30000 Training Loss: 0.054950933903455734\n",
      "Epoch 4194/30000 Training Loss: 0.04929311200976372\n",
      "Epoch 4195/30000 Training Loss: 0.07556192576885223\n",
      "Epoch 4196/30000 Training Loss: 0.062993124127388\n",
      "Epoch 4197/30000 Training Loss: 0.07544374465942383\n",
      "Epoch 4198/30000 Training Loss: 0.07861949503421783\n",
      "Epoch 4199/30000 Training Loss: 0.05896236002445221\n",
      "Epoch 4200/30000 Training Loss: 0.05322353169322014\n",
      "Epoch 4200/30000 Validation Loss: 0.06182112544775009\n",
      "Epoch 4201/30000 Training Loss: 0.05785377323627472\n",
      "Epoch 4202/30000 Training Loss: 0.07293202728033066\n",
      "Epoch 4203/30000 Training Loss: 0.05365597456693649\n",
      "Epoch 4204/30000 Training Loss: 0.053415920585393906\n",
      "Epoch 4205/30000 Training Loss: 0.04933455213904381\n",
      "Epoch 4206/30000 Training Loss: 0.06628400832414627\n",
      "Epoch 4207/30000 Training Loss: 0.05903947353363037\n",
      "Epoch 4208/30000 Training Loss: 0.06302154809236526\n",
      "Epoch 4209/30000 Training Loss: 0.07473146170377731\n",
      "Epoch 4210/30000 Training Loss: 0.06284373998641968\n",
      "Epoch 4211/30000 Training Loss: 0.06338240951299667\n",
      "Epoch 4212/30000 Training Loss: 0.06795904040336609\n",
      "Epoch 4213/30000 Training Loss: 0.0612017884850502\n",
      "Epoch 4214/30000 Training Loss: 0.05635394528508186\n",
      "Epoch 4215/30000 Training Loss: 0.06300242990255356\n",
      "Epoch 4216/30000 Training Loss: 0.05177038535475731\n",
      "Epoch 4217/30000 Training Loss: 0.0623120442032814\n",
      "Epoch 4218/30000 Training Loss: 0.05803171545267105\n",
      "Epoch 4219/30000 Training Loss: 0.07649043947458267\n",
      "Epoch 4220/30000 Training Loss: 0.06877076625823975\n",
      "Epoch 4221/30000 Training Loss: 0.07035307586193085\n",
      "Epoch 4222/30000 Training Loss: 0.07550521194934845\n",
      "Epoch 4223/30000 Training Loss: 0.060772381722927094\n",
      "Epoch 4224/30000 Training Loss: 0.040960103273391724\n",
      "Epoch 4225/30000 Training Loss: 0.06659527122974396\n",
      "Epoch 4226/30000 Training Loss: 0.059898391366004944\n",
      "Epoch 4227/30000 Training Loss: 0.05074174702167511\n",
      "Epoch 4228/30000 Training Loss: 0.07533030956983566\n",
      "Epoch 4229/30000 Training Loss: 0.06277312338352203\n",
      "Epoch 4230/30000 Training Loss: 0.06551924347877502\n",
      "Epoch 4231/30000 Training Loss: 0.06105245649814606\n",
      "Epoch 4232/30000 Training Loss: 0.061096448451280594\n",
      "Epoch 4233/30000 Training Loss: 0.0695788562297821\n",
      "Epoch 4234/30000 Training Loss: 0.06583346426486969\n",
      "Epoch 4235/30000 Training Loss: 0.05994931980967522\n",
      "Epoch 4236/30000 Training Loss: 0.07070507854223251\n",
      "Epoch 4237/30000 Training Loss: 0.06413044780492783\n",
      "Epoch 4238/30000 Training Loss: 0.05182229354977608\n",
      "Epoch 4239/30000 Training Loss: 0.07053542137145996\n",
      "Epoch 4240/30000 Training Loss: 0.06515935063362122\n",
      "Epoch 4241/30000 Training Loss: 0.0729430690407753\n",
      "Epoch 4242/30000 Training Loss: 0.06730647385120392\n",
      "Epoch 4243/30000 Training Loss: 0.07024514675140381\n",
      "Epoch 4244/30000 Training Loss: 0.06396235525608063\n",
      "Epoch 4245/30000 Training Loss: 0.062087275087833405\n",
      "Epoch 4246/30000 Training Loss: 0.05726329982280731\n",
      "Epoch 4247/30000 Training Loss: 0.05585120990872383\n",
      "Epoch 4248/30000 Training Loss: 0.0717351883649826\n",
      "Epoch 4249/30000 Training Loss: 0.06385830044746399\n",
      "Epoch 4250/30000 Training Loss: 0.07037539780139923\n",
      "Epoch 4251/30000 Training Loss: 0.056930892169475555\n",
      "Epoch 4252/30000 Training Loss: 0.06100417673587799\n",
      "Epoch 4253/30000 Training Loss: 0.06114111840724945\n",
      "Epoch 4254/30000 Training Loss: 0.06482614576816559\n",
      "Epoch 4255/30000 Training Loss: 0.07496622204780579\n",
      "Epoch 4256/30000 Training Loss: 0.08082328736782074\n",
      "Epoch 4257/30000 Training Loss: 0.05066579952836037\n",
      "Epoch 4258/30000 Training Loss: 0.05845634639263153\n",
      "Epoch 4259/30000 Training Loss: 0.05483819544315338\n",
      "Epoch 4260/30000 Training Loss: 0.060257647186517715\n",
      "Epoch 4261/30000 Training Loss: 0.05661395192146301\n",
      "Epoch 4262/30000 Training Loss: 0.06379544734954834\n",
      "Epoch 4263/30000 Training Loss: 0.05825229734182358\n",
      "Epoch 4264/30000 Training Loss: 0.07648029178380966\n",
      "Epoch 4265/30000 Training Loss: 0.06456679850816727\n",
      "Epoch 4266/30000 Training Loss: 0.06432298570871353\n",
      "Epoch 4267/30000 Training Loss: 0.08201871812343597\n",
      "Epoch 4268/30000 Training Loss: 0.0646883174777031\n",
      "Epoch 4269/30000 Training Loss: 0.06355400383472443\n",
      "Epoch 4270/30000 Training Loss: 0.07372339069843292\n",
      "Epoch 4271/30000 Training Loss: 0.05507610738277435\n",
      "Epoch 4272/30000 Training Loss: 0.05454307794570923\n",
      "Epoch 4273/30000 Training Loss: 0.05608188360929489\n",
      "Epoch 4274/30000 Training Loss: 0.06109552085399628\n",
      "Epoch 4275/30000 Training Loss: 0.05731122940778732\n",
      "Epoch 4276/30000 Training Loss: 0.05421983450651169\n",
      "Epoch 4277/30000 Training Loss: 0.08255182206630707\n",
      "Epoch 4278/30000 Training Loss: 0.06058612838387489\n",
      "Epoch 4279/30000 Training Loss: 0.05153069645166397\n",
      "Epoch 4280/30000 Training Loss: 0.06151586025953293\n",
      "Epoch 4281/30000 Training Loss: 0.056301549077034\n",
      "Epoch 4282/30000 Training Loss: 0.057988397777080536\n",
      "Epoch 4283/30000 Training Loss: 0.06853464990854263\n",
      "Epoch 4284/30000 Training Loss: 0.05893591791391373\n",
      "Epoch 4285/30000 Training Loss: 0.06486953049898148\n",
      "Epoch 4286/30000 Training Loss: 0.05246070399880409\n",
      "Epoch 4287/30000 Training Loss: 0.06472553312778473\n",
      "Epoch 4288/30000 Training Loss: 0.06317822635173798\n",
      "Epoch 4289/30000 Training Loss: 0.06773931533098221\n",
      "Epoch 4290/30000 Training Loss: 0.070729099214077\n",
      "Epoch 4291/30000 Training Loss: 0.044393278658390045\n",
      "Epoch 4292/30000 Training Loss: 0.05681499093770981\n",
      "Epoch 4293/30000 Training Loss: 0.06488191336393356\n",
      "Epoch 4294/30000 Training Loss: 0.06560301780700684\n",
      "Epoch 4295/30000 Training Loss: 0.06208737939596176\n",
      "Epoch 4296/30000 Training Loss: 0.07646213471889496\n",
      "Epoch 4297/30000 Training Loss: 0.05492152273654938\n",
      "Epoch 4298/30000 Training Loss: 0.07904475182294846\n",
      "Epoch 4299/30000 Training Loss: 0.0635412409901619\n",
      "Epoch 4300/30000 Training Loss: 0.05828576534986496\n",
      "Epoch 4300/30000 Validation Loss: 0.07427427172660828\n",
      "Epoch 4301/30000 Training Loss: 0.05500980466604233\n",
      "Epoch 4302/30000 Training Loss: 0.07810693979263306\n",
      "Epoch 4303/30000 Training Loss: 0.05339180678129196\n",
      "Epoch 4304/30000 Training Loss: 0.05366318300366402\n",
      "Epoch 4305/30000 Training Loss: 0.07353615760803223\n",
      "Epoch 4306/30000 Training Loss: 0.06594602763652802\n",
      "Epoch 4307/30000 Training Loss: 0.05514058098196983\n",
      "Epoch 4308/30000 Training Loss: 0.04564351588487625\n",
      "Epoch 4309/30000 Training Loss: 0.06050189584493637\n",
      "Epoch 4310/30000 Training Loss: 0.07176779210567474\n",
      "Epoch 4311/30000 Training Loss: 0.062476836144924164\n",
      "Epoch 4312/30000 Training Loss: 0.06913450360298157\n",
      "Epoch 4313/30000 Training Loss: 0.0597408153116703\n",
      "Epoch 4314/30000 Training Loss: 0.068741075694561\n",
      "Epoch 4315/30000 Training Loss: 0.06612670421600342\n",
      "Epoch 4316/30000 Training Loss: 0.07489579916000366\n",
      "Epoch 4317/30000 Training Loss: 0.05966706573963165\n",
      "Epoch 4318/30000 Training Loss: 0.06641373038291931\n",
      "Epoch 4319/30000 Training Loss: 0.06831914931535721\n",
      "Epoch 4320/30000 Training Loss: 0.054419439285993576\n",
      "Epoch 4321/30000 Training Loss: 0.06260550022125244\n",
      "Epoch 4322/30000 Training Loss: 0.055237483233213425\n",
      "Epoch 4323/30000 Training Loss: 0.06651265919208527\n",
      "Epoch 4324/30000 Training Loss: 0.0729670450091362\n",
      "Epoch 4325/30000 Training Loss: 0.05595129728317261\n",
      "Epoch 4326/30000 Training Loss: 0.05903574824333191\n",
      "Epoch 4327/30000 Training Loss: 0.08846069127321243\n",
      "Epoch 4328/30000 Training Loss: 0.062393881380558014\n",
      "Epoch 4329/30000 Training Loss: 0.0591360479593277\n",
      "Epoch 4330/30000 Training Loss: 0.06988588720560074\n",
      "Epoch 4331/30000 Training Loss: 0.07991492748260498\n",
      "Epoch 4332/30000 Training Loss: 0.06333362311124802\n",
      "Epoch 4333/30000 Training Loss: 0.055717140436172485\n",
      "Epoch 4334/30000 Training Loss: 0.0587412528693676\n",
      "Epoch 4335/30000 Training Loss: 0.05906198173761368\n",
      "Epoch 4336/30000 Training Loss: 0.06003481149673462\n",
      "Epoch 4337/30000 Training Loss: 0.0621216706931591\n",
      "Epoch 4338/30000 Training Loss: 0.09372363239526749\n",
      "Epoch 4339/30000 Training Loss: 0.048421330749988556\n",
      "Epoch 4340/30000 Training Loss: 0.06999822705984116\n",
      "Epoch 4341/30000 Training Loss: 0.05822119116783142\n",
      "Epoch 4342/30000 Training Loss: 0.07031865417957306\n",
      "Epoch 4343/30000 Training Loss: 0.0658985823392868\n",
      "Epoch 4344/30000 Training Loss: 0.06475570052862167\n",
      "Epoch 4345/30000 Training Loss: 0.059868887066841125\n",
      "Epoch 4346/30000 Training Loss: 0.0815814733505249\n",
      "Epoch 4347/30000 Training Loss: 0.053637631237506866\n",
      "Epoch 4348/30000 Training Loss: 0.05167439207434654\n",
      "Epoch 4349/30000 Training Loss: 0.06556418538093567\n",
      "Epoch 4350/30000 Training Loss: 0.06625713407993317\n",
      "Epoch 4351/30000 Training Loss: 0.04543372243642807\n",
      "Epoch 4352/30000 Training Loss: 0.07208290696144104\n",
      "Epoch 4353/30000 Training Loss: 0.06599710136651993\n",
      "Epoch 4354/30000 Training Loss: 0.06459175050258636\n",
      "Epoch 4355/30000 Training Loss: 0.05719310790300369\n",
      "Epoch 4356/30000 Training Loss: 0.04709932208061218\n",
      "Epoch 4357/30000 Training Loss: 0.06726622581481934\n",
      "Epoch 4358/30000 Training Loss: 0.06201646476984024\n",
      "Epoch 4359/30000 Training Loss: 0.06073203682899475\n",
      "Epoch 4360/30000 Training Loss: 0.05895783007144928\n",
      "Epoch 4361/30000 Training Loss: 0.06203577667474747\n",
      "Epoch 4362/30000 Training Loss: 0.05202841758728027\n",
      "Epoch 4363/30000 Training Loss: 0.0708274096250534\n",
      "Epoch 4364/30000 Training Loss: 0.051933690905570984\n",
      "Epoch 4365/30000 Training Loss: 0.05407173931598663\n",
      "Epoch 4366/30000 Training Loss: 0.056148480623960495\n",
      "Epoch 4367/30000 Training Loss: 0.05645536258816719\n",
      "Epoch 4368/30000 Training Loss: 0.0656699389219284\n",
      "Epoch 4369/30000 Training Loss: 0.07775972783565521\n",
      "Epoch 4370/30000 Training Loss: 0.056873999536037445\n",
      "Epoch 4371/30000 Training Loss: 0.07246973365545273\n",
      "Epoch 4372/30000 Training Loss: 0.06374000757932663\n",
      "Epoch 4373/30000 Training Loss: 0.06709704548120499\n",
      "Epoch 4374/30000 Training Loss: 0.060503486543893814\n",
      "Epoch 4375/30000 Training Loss: 0.057374175637960434\n",
      "Epoch 4376/30000 Training Loss: 0.05769960582256317\n",
      "Epoch 4377/30000 Training Loss: 0.04610126093029976\n",
      "Epoch 4378/30000 Training Loss: 0.07426242530345917\n",
      "Epoch 4379/30000 Training Loss: 0.06779252737760544\n",
      "Epoch 4380/30000 Training Loss: 0.07468568533658981\n",
      "Epoch 4381/30000 Training Loss: 0.06685402244329453\n",
      "Epoch 4382/30000 Training Loss: 0.06923459470272064\n",
      "Epoch 4383/30000 Training Loss: 0.0773264616727829\n",
      "Epoch 4384/30000 Training Loss: 0.07355602085590363\n",
      "Epoch 4385/30000 Training Loss: 0.04813941940665245\n",
      "Epoch 4386/30000 Training Loss: 0.06322687864303589\n",
      "Epoch 4387/30000 Training Loss: 0.09332031011581421\n",
      "Epoch 4388/30000 Training Loss: 0.06273622810840607\n",
      "Epoch 4389/30000 Training Loss: 0.05433010682463646\n",
      "Epoch 4390/30000 Training Loss: 0.06093524768948555\n",
      "Epoch 4391/30000 Training Loss: 0.06480187177658081\n",
      "Epoch 4392/30000 Training Loss: 0.05140865966677666\n",
      "Epoch 4393/30000 Training Loss: 0.0460272841155529\n",
      "Epoch 4394/30000 Training Loss: 0.062498800456523895\n",
      "Epoch 4395/30000 Training Loss: 0.08016008138656616\n",
      "Epoch 4396/30000 Training Loss: 0.06365694105625153\n",
      "Epoch 4397/30000 Training Loss: 0.05838394910097122\n",
      "Epoch 4398/30000 Training Loss: 0.04956003278493881\n",
      "Epoch 4399/30000 Training Loss: 0.06949285417795181\n",
      "Epoch 4400/30000 Training Loss: 0.06423628330230713\n",
      "Epoch 4400/30000 Validation Loss: 0.07601362466812134\n",
      "Epoch 4401/30000 Training Loss: 0.06230860948562622\n",
      "Epoch 4402/30000 Training Loss: 0.06419922411441803\n",
      "Epoch 4403/30000 Training Loss: 0.060025375336408615\n",
      "Epoch 4404/30000 Training Loss: 0.06874147802591324\n",
      "Epoch 4405/30000 Training Loss: 0.056082822382450104\n",
      "Epoch 4406/30000 Training Loss: 0.046942174434661865\n",
      "Epoch 4407/30000 Training Loss: 0.06616105884313583\n",
      "Epoch 4408/30000 Training Loss: 0.0493420735001564\n",
      "Epoch 4409/30000 Training Loss: 0.054871078580617905\n",
      "Epoch 4410/30000 Training Loss: 0.05667109414935112\n",
      "Epoch 4411/30000 Training Loss: 0.05885252356529236\n",
      "Epoch 4412/30000 Training Loss: 0.06143927574157715\n",
      "Epoch 4413/30000 Training Loss: 0.07754309475421906\n",
      "Epoch 4414/30000 Training Loss: 0.043822646141052246\n",
      "Epoch 4415/30000 Training Loss: 0.07151882350444794\n",
      "Epoch 4416/30000 Training Loss: 0.06227533519268036\n",
      "Epoch 4417/30000 Training Loss: 0.05166870355606079\n",
      "Epoch 4418/30000 Training Loss: 0.060366686433553696\n",
      "Epoch 4419/30000 Training Loss: 0.07471256703138351\n",
      "Epoch 4420/30000 Training Loss: 0.0573885403573513\n",
      "Epoch 4421/30000 Training Loss: 0.0855448767542839\n",
      "Epoch 4422/30000 Training Loss: 0.06565913558006287\n",
      "Epoch 4423/30000 Training Loss: 0.09263904392719269\n",
      "Epoch 4424/30000 Training Loss: 0.06234564632177353\n",
      "Epoch 4425/30000 Training Loss: 0.07188603281974792\n",
      "Epoch 4426/30000 Training Loss: 0.06664206832647324\n",
      "Epoch 4427/30000 Training Loss: 0.06172209978103638\n",
      "Epoch 4428/30000 Training Loss: 0.06493093073368073\n",
      "Epoch 4429/30000 Training Loss: 0.05867808312177658\n",
      "Epoch 4430/30000 Training Loss: 0.0739087164402008\n",
      "Epoch 4431/30000 Training Loss: 0.07386396080255508\n",
      "Epoch 4432/30000 Training Loss: 0.08085595816373825\n",
      "Epoch 4433/30000 Training Loss: 0.07323485612869263\n",
      "Epoch 4434/30000 Training Loss: 0.04670630767941475\n",
      "Epoch 4435/30000 Training Loss: 0.06352603435516357\n",
      "Epoch 4436/30000 Training Loss: 0.0773501992225647\n",
      "Epoch 4437/30000 Training Loss: 0.052814967930316925\n",
      "Epoch 4438/30000 Training Loss: 0.06773494184017181\n",
      "Epoch 4439/30000 Training Loss: 0.05355039983987808\n",
      "Epoch 4440/30000 Training Loss: 0.061545323580503464\n",
      "Epoch 4441/30000 Training Loss: 0.08119460195302963\n",
      "Epoch 4442/30000 Training Loss: 0.06538869440555573\n",
      "Epoch 4443/30000 Training Loss: 0.05374952405691147\n",
      "Epoch 4444/30000 Training Loss: 0.051054276525974274\n",
      "Epoch 4445/30000 Training Loss: 0.0643920749425888\n",
      "Epoch 4446/30000 Training Loss: 0.08509603887796402\n",
      "Epoch 4447/30000 Training Loss: 0.06446567177772522\n",
      "Epoch 4448/30000 Training Loss: 0.042954783886671066\n",
      "Epoch 4449/30000 Training Loss: 0.07390324026346207\n",
      "Epoch 4450/30000 Training Loss: 0.0542820543050766\n",
      "Epoch 4451/30000 Training Loss: 0.06104995310306549\n",
      "Epoch 4452/30000 Training Loss: 0.07635585218667984\n",
      "Epoch 4453/30000 Training Loss: 0.06302401423454285\n",
      "Epoch 4454/30000 Training Loss: 0.05682333558797836\n",
      "Epoch 4455/30000 Training Loss: 0.06325843185186386\n",
      "Epoch 4456/30000 Training Loss: 0.06883665919303894\n",
      "Epoch 4457/30000 Training Loss: 0.06162776052951813\n",
      "Epoch 4458/30000 Training Loss: 0.05520210787653923\n",
      "Epoch 4459/30000 Training Loss: 0.0520501509308815\n",
      "Epoch 4460/30000 Training Loss: 0.06350971758365631\n",
      "Epoch 4461/30000 Training Loss: 0.059512462466955185\n",
      "Epoch 4462/30000 Training Loss: 0.052274189889431\n",
      "Epoch 4463/30000 Training Loss: 0.049939244985580444\n",
      "Epoch 4464/30000 Training Loss: 0.0748848095536232\n",
      "Epoch 4465/30000 Training Loss: 0.05729815736413002\n",
      "Epoch 4466/30000 Training Loss: 0.05470391362905502\n",
      "Epoch 4467/30000 Training Loss: 0.04781891033053398\n",
      "Epoch 4468/30000 Training Loss: 0.06147151067852974\n",
      "Epoch 4469/30000 Training Loss: 0.05470351129770279\n",
      "Epoch 4470/30000 Training Loss: 0.05833926424384117\n",
      "Epoch 4471/30000 Training Loss: 0.06820148974657059\n",
      "Epoch 4472/30000 Training Loss: 0.06733208149671555\n",
      "Epoch 4473/30000 Training Loss: 0.06544777750968933\n",
      "Epoch 4474/30000 Training Loss: 0.06314194202423096\n",
      "Epoch 4475/30000 Training Loss: 0.06298328191041946\n",
      "Epoch 4476/30000 Training Loss: 0.07912042737007141\n",
      "Epoch 4477/30000 Training Loss: 0.0661265030503273\n",
      "Epoch 4478/30000 Training Loss: 0.08576345443725586\n",
      "Epoch 4479/30000 Training Loss: 0.060647282749414444\n",
      "Epoch 4480/30000 Training Loss: 0.07524370402097702\n",
      "Epoch 4481/30000 Training Loss: 0.0718207135796547\n",
      "Epoch 4482/30000 Training Loss: 0.06644802540540695\n",
      "Epoch 4483/30000 Training Loss: 0.08334694802761078\n",
      "Epoch 4484/30000 Training Loss: 0.06696904450654984\n",
      "Epoch 4485/30000 Training Loss: 0.04502258077263832\n",
      "Epoch 4486/30000 Training Loss: 0.04543042182922363\n",
      "Epoch 4487/30000 Training Loss: 0.08134836703538895\n",
      "Epoch 4488/30000 Training Loss: 0.06705652922391891\n",
      "Epoch 4489/30000 Training Loss: 0.0540325790643692\n",
      "Epoch 4490/30000 Training Loss: 0.0659051388502121\n",
      "Epoch 4491/30000 Training Loss: 0.07965587079524994\n",
      "Epoch 4492/30000 Training Loss: 0.06338430196046829\n",
      "Epoch 4493/30000 Training Loss: 0.08027907460927963\n",
      "Epoch 4494/30000 Training Loss: 0.05785689875483513\n",
      "Epoch 4495/30000 Training Loss: 0.05163668468594551\n",
      "Epoch 4496/30000 Training Loss: 0.06875883787870407\n",
      "Epoch 4497/30000 Training Loss: 0.04786912351846695\n",
      "Epoch 4498/30000 Training Loss: 0.06104971468448639\n",
      "Epoch 4499/30000 Training Loss: 0.06082703173160553\n",
      "Epoch 4500/30000 Training Loss: 0.07017583400011063\n",
      "Epoch 4500/30000 Validation Loss: 0.07081388682126999\n",
      "Epoch 4501/30000 Training Loss: 0.05075882002711296\n",
      "Epoch 4502/30000 Training Loss: 0.05111110955476761\n",
      "Epoch 4503/30000 Training Loss: 0.07267887890338898\n",
      "Epoch 4504/30000 Training Loss: 0.05992627888917923\n",
      "Epoch 4505/30000 Training Loss: 0.05124254524707794\n",
      "Epoch 4506/30000 Training Loss: 0.07805460691452026\n",
      "Epoch 4507/30000 Training Loss: 0.07845426350831985\n",
      "Epoch 4508/30000 Training Loss: 0.05456513911485672\n",
      "Epoch 4509/30000 Training Loss: 0.05910322815179825\n",
      "Epoch 4510/30000 Training Loss: 0.06436071544885635\n",
      "Epoch 4511/30000 Training Loss: 0.062196724116802216\n",
      "Epoch 4512/30000 Training Loss: 0.06608860194683075\n",
      "Epoch 4513/30000 Training Loss: 0.046566519886255264\n",
      "Epoch 4514/30000 Training Loss: 0.05859992653131485\n",
      "Epoch 4515/30000 Training Loss: 0.05726554989814758\n",
      "Epoch 4516/30000 Training Loss: 0.06988871097564697\n",
      "Epoch 4517/30000 Training Loss: 0.06274530291557312\n",
      "Epoch 4518/30000 Training Loss: 0.0581778921186924\n",
      "Epoch 4519/30000 Training Loss: 0.06740471720695496\n",
      "Epoch 4520/30000 Training Loss: 0.05553648620843887\n",
      "Epoch 4521/30000 Training Loss: 0.05513203889131546\n",
      "Epoch 4522/30000 Training Loss: 0.05932755395770073\n",
      "Epoch 4523/30000 Training Loss: 0.07875245809555054\n",
      "Epoch 4524/30000 Training Loss: 0.05722460523247719\n",
      "Epoch 4525/30000 Training Loss: 0.06192478537559509\n",
      "Epoch 4526/30000 Training Loss: 0.05145137757062912\n",
      "Epoch 4527/30000 Training Loss: 0.05061773583292961\n",
      "Epoch 4528/30000 Training Loss: 0.0600142665207386\n",
      "Epoch 4529/30000 Training Loss: 0.052483510226011276\n",
      "Epoch 4530/30000 Training Loss: 0.06454161554574966\n",
      "Epoch 4531/30000 Training Loss: 0.06513025611639023\n",
      "Epoch 4532/30000 Training Loss: 0.07410357892513275\n",
      "Epoch 4533/30000 Training Loss: 0.040294285863637924\n",
      "Epoch 4534/30000 Training Loss: 0.06000068038702011\n",
      "Epoch 4535/30000 Training Loss: 0.05996159464120865\n",
      "Epoch 4536/30000 Training Loss: 0.07640452682971954\n",
      "Epoch 4537/30000 Training Loss: 0.04885333031415939\n",
      "Epoch 4538/30000 Training Loss: 0.09298884123563766\n",
      "Epoch 4539/30000 Training Loss: 0.05956562981009483\n",
      "Epoch 4540/30000 Training Loss: 0.06765355169773102\n",
      "Epoch 4541/30000 Training Loss: 0.05373472720384598\n",
      "Epoch 4542/30000 Training Loss: 0.06189120188355446\n",
      "Epoch 4543/30000 Training Loss: 0.05331776663661003\n",
      "Epoch 4544/30000 Training Loss: 0.05931589752435684\n",
      "Epoch 4545/30000 Training Loss: 0.07793059945106506\n",
      "Epoch 4546/30000 Training Loss: 0.06346802413463593\n",
      "Epoch 4547/30000 Training Loss: 0.061340562999248505\n",
      "Epoch 4548/30000 Training Loss: 0.06379470229148865\n",
      "Epoch 4549/30000 Training Loss: 0.0693831741809845\n",
      "Epoch 4550/30000 Training Loss: 0.07262080907821655\n",
      "Epoch 4551/30000 Training Loss: 0.061217788606882095\n",
      "Epoch 4552/30000 Training Loss: 0.05997364595532417\n",
      "Epoch 4553/30000 Training Loss: 0.08702372014522552\n",
      "Epoch 4554/30000 Training Loss: 0.06727045774459839\n",
      "Epoch 4555/30000 Training Loss: 0.05678622052073479\n",
      "Epoch 4556/30000 Training Loss: 0.04567432776093483\n",
      "Epoch 4557/30000 Training Loss: 0.0669732391834259\n",
      "Epoch 4558/30000 Training Loss: 0.06183440983295441\n",
      "Epoch 4559/30000 Training Loss: 0.08154313266277313\n",
      "Epoch 4560/30000 Training Loss: 0.06978340446949005\n",
      "Epoch 4561/30000 Training Loss: 0.08232170343399048\n",
      "Epoch 4562/30000 Training Loss: 0.046427108347415924\n",
      "Epoch 4563/30000 Training Loss: 0.051824815571308136\n",
      "Epoch 4564/30000 Training Loss: 0.05757112801074982\n",
      "Epoch 4565/30000 Training Loss: 0.08070763200521469\n",
      "Epoch 4566/30000 Training Loss: 0.07254583388566971\n",
      "Epoch 4567/30000 Training Loss: 0.051820918917655945\n",
      "Epoch 4568/30000 Training Loss: 0.06425849348306656\n",
      "Epoch 4569/30000 Training Loss: 0.06153608486056328\n",
      "Epoch 4570/30000 Training Loss: 0.06799249351024628\n",
      "Epoch 4571/30000 Training Loss: 0.057111114263534546\n",
      "Epoch 4572/30000 Training Loss: 0.07891271263360977\n",
      "Epoch 4573/30000 Training Loss: 0.051149286329746246\n",
      "Epoch 4574/30000 Training Loss: 0.06101188808679581\n",
      "Epoch 4575/30000 Training Loss: 0.06690479815006256\n",
      "Epoch 4576/30000 Training Loss: 0.05018514767289162\n",
      "Epoch 4577/30000 Training Loss: 0.05597848445177078\n",
      "Epoch 4578/30000 Training Loss: 0.056767746806144714\n",
      "Epoch 4579/30000 Training Loss: 0.06316673010587692\n",
      "Epoch 4580/30000 Training Loss: 0.07042497396469116\n",
      "Epoch 4581/30000 Training Loss: 0.0639226883649826\n",
      "Epoch 4582/30000 Training Loss: 0.07645910978317261\n",
      "Epoch 4583/30000 Training Loss: 0.08095543086528778\n",
      "Epoch 4584/30000 Training Loss: 0.07161399722099304\n",
      "Epoch 4585/30000 Training Loss: 0.07714342325925827\n",
      "Epoch 4586/30000 Training Loss: 0.06921182572841644\n",
      "Epoch 4587/30000 Training Loss: 0.0730910375714302\n",
      "Epoch 4588/30000 Training Loss: 0.07183282822370529\n",
      "Epoch 4589/30000 Training Loss: 0.05638354644179344\n",
      "Epoch 4590/30000 Training Loss: 0.06312043219804764\n",
      "Epoch 4591/30000 Training Loss: 0.051093198359012604\n",
      "Epoch 4592/30000 Training Loss: 0.07445904612541199\n",
      "Epoch 4593/30000 Training Loss: 0.0787542313337326\n",
      "Epoch 4594/30000 Training Loss: 0.08152962476015091\n",
      "Epoch 4595/30000 Training Loss: 0.047379449009895325\n",
      "Epoch 4596/30000 Training Loss: 0.07814902067184448\n",
      "Epoch 4597/30000 Training Loss: 0.06238914653658867\n",
      "Epoch 4598/30000 Training Loss: 0.0701410248875618\n",
      "Epoch 4599/30000 Training Loss: 0.07290396094322205\n",
      "Epoch 4600/30000 Training Loss: 0.07425761222839355\n",
      "Epoch 4600/30000 Validation Loss: 0.06132177263498306\n",
      "Epoch 4601/30000 Training Loss: 0.05652818828821182\n",
      "Epoch 4602/30000 Training Loss: 0.04793042317032814\n",
      "Epoch 4603/30000 Training Loss: 0.05445130541920662\n",
      "Epoch 4604/30000 Training Loss: 0.07783010601997375\n",
      "Epoch 4605/30000 Training Loss: 0.047059591859579086\n",
      "Epoch 4606/30000 Training Loss: 0.06395553797483444\n",
      "Epoch 4607/30000 Training Loss: 0.0682687982916832\n",
      "Epoch 4608/30000 Training Loss: 0.05792279541492462\n",
      "Epoch 4609/30000 Training Loss: 0.058176226913928986\n",
      "Epoch 4610/30000 Training Loss: 0.04217802733182907\n",
      "Epoch 4611/30000 Training Loss: 0.06131263077259064\n",
      "Epoch 4612/30000 Training Loss: 0.07152469456195831\n",
      "Epoch 4613/30000 Training Loss: 0.048332687467336655\n",
      "Epoch 4614/30000 Training Loss: 0.06568873673677444\n",
      "Epoch 4615/30000 Training Loss: 0.07813095301389694\n",
      "Epoch 4616/30000 Training Loss: 0.07579842209815979\n",
      "Epoch 4617/30000 Training Loss: 0.060572877526283264\n",
      "Epoch 4618/30000 Training Loss: 0.06646671891212463\n",
      "Epoch 4619/30000 Training Loss: 0.07191388309001923\n",
      "Epoch 4620/30000 Training Loss: 0.046461742371320724\n",
      "Epoch 4621/30000 Training Loss: 0.05131855607032776\n",
      "Epoch 4622/30000 Training Loss: 0.05817193537950516\n",
      "Epoch 4623/30000 Training Loss: 0.06467848271131516\n",
      "Epoch 4624/30000 Training Loss: 0.06969348341226578\n",
      "Epoch 4625/30000 Training Loss: 0.057756174355745316\n",
      "Epoch 4626/30000 Training Loss: 0.05497198924422264\n",
      "Epoch 4627/30000 Training Loss: 0.07047994434833527\n",
      "Epoch 4628/30000 Training Loss: 0.056328319013118744\n",
      "Epoch 4629/30000 Training Loss: 0.05552932620048523\n",
      "Epoch 4630/30000 Training Loss: 0.06970367580652237\n",
      "Epoch 4631/30000 Training Loss: 0.06178656965494156\n",
      "Epoch 4632/30000 Training Loss: 0.05624374747276306\n",
      "Epoch 4633/30000 Training Loss: 0.07065896689891815\n",
      "Epoch 4634/30000 Training Loss: 0.06450160592794418\n",
      "Epoch 4635/30000 Training Loss: 0.04669038578867912\n",
      "Epoch 4636/30000 Training Loss: 0.06930439174175262\n",
      "Epoch 4637/30000 Training Loss: 0.054726045578718185\n",
      "Epoch 4638/30000 Training Loss: 0.05994299054145813\n",
      "Epoch 4639/30000 Training Loss: 0.06837969273328781\n",
      "Epoch 4640/30000 Training Loss: 0.06671259552240372\n",
      "Epoch 4641/30000 Training Loss: 0.04630722850561142\n",
      "Epoch 4642/30000 Training Loss: 0.08908279240131378\n",
      "Epoch 4643/30000 Training Loss: 0.06052863225340843\n",
      "Epoch 4644/30000 Training Loss: 0.05543706566095352\n",
      "Epoch 4645/30000 Training Loss: 0.08245600759983063\n",
      "Epoch 4646/30000 Training Loss: 0.043322622776031494\n",
      "Epoch 4647/30000 Training Loss: 0.06946170330047607\n",
      "Epoch 4648/30000 Training Loss: 0.05802411586046219\n",
      "Epoch 4649/30000 Training Loss: 0.06207095831632614\n",
      "Epoch 4650/30000 Training Loss: 0.04733411595225334\n",
      "Epoch 4651/30000 Training Loss: 0.0477822944521904\n",
      "Epoch 4652/30000 Training Loss: 0.05123749002814293\n",
      "Epoch 4653/30000 Training Loss: 0.0639626532793045\n",
      "Epoch 4654/30000 Training Loss: 0.07065464556217194\n",
      "Epoch 4655/30000 Training Loss: 0.062047429382801056\n",
      "Epoch 4656/30000 Training Loss: 0.07638338208198547\n",
      "Epoch 4657/30000 Training Loss: 0.04178076237440109\n",
      "Epoch 4658/30000 Training Loss: 0.054980501532554626\n",
      "Epoch 4659/30000 Training Loss: 0.06769672781229019\n",
      "Epoch 4660/30000 Training Loss: 0.07179980725049973\n",
      "Epoch 4661/30000 Training Loss: 0.07436496019363403\n",
      "Epoch 4662/30000 Training Loss: 0.0532928928732872\n",
      "Epoch 4663/30000 Training Loss: 0.05005341023206711\n",
      "Epoch 4664/30000 Training Loss: 0.0502103790640831\n",
      "Epoch 4665/30000 Training Loss: 0.050664663314819336\n",
      "Epoch 4666/30000 Training Loss: 0.05991117283701897\n",
      "Epoch 4667/30000 Training Loss: 0.05460789054632187\n",
      "Epoch 4668/30000 Training Loss: 0.061068177223205566\n",
      "Epoch 4669/30000 Training Loss: 0.07336050271987915\n",
      "Epoch 4670/30000 Training Loss: 0.049574632197618484\n",
      "Epoch 4671/30000 Training Loss: 0.07443917542695999\n",
      "Epoch 4672/30000 Training Loss: 0.06396949291229248\n",
      "Epoch 4673/30000 Training Loss: 0.05210716649889946\n",
      "Epoch 4674/30000 Training Loss: 0.06194841116666794\n",
      "Epoch 4675/30000 Training Loss: 0.06218395009636879\n",
      "Epoch 4676/30000 Training Loss: 0.06291556358337402\n",
      "Epoch 4677/30000 Training Loss: 0.05039246380329132\n",
      "Epoch 4678/30000 Training Loss: 0.07355453819036484\n",
      "Epoch 4679/30000 Training Loss: 0.061924610286951065\n",
      "Epoch 4680/30000 Training Loss: 0.054572079330682755\n",
      "Epoch 4681/30000 Training Loss: 0.08490641415119171\n",
      "Epoch 4682/30000 Training Loss: 0.06974030286073685\n",
      "Epoch 4683/30000 Training Loss: 0.0618080273270607\n",
      "Epoch 4684/30000 Training Loss: 0.06939069926738739\n",
      "Epoch 4685/30000 Training Loss: 0.0751577690243721\n",
      "Epoch 4686/30000 Training Loss: 0.06706040352582932\n",
      "Epoch 4687/30000 Training Loss: 0.06447286903858185\n",
      "Epoch 4688/30000 Training Loss: 0.05188208445906639\n",
      "Epoch 4689/30000 Training Loss: 0.0884130671620369\n",
      "Epoch 4690/30000 Training Loss: 0.04864067956805229\n",
      "Epoch 4691/30000 Training Loss: 0.06922098994255066\n",
      "Epoch 4692/30000 Training Loss: 0.05118744820356369\n",
      "Epoch 4693/30000 Training Loss: 0.05965444818139076\n",
      "Epoch 4694/30000 Training Loss: 0.05363932251930237\n",
      "Epoch 4695/30000 Training Loss: 0.07497943192720413\n",
      "Epoch 4696/30000 Training Loss: 0.06616836041212082\n",
      "Epoch 4697/30000 Training Loss: 0.07656226307153702\n",
      "Epoch 4698/30000 Training Loss: 0.06816443800926208\n",
      "Epoch 4699/30000 Training Loss: 0.051894910633563995\n",
      "Epoch 4700/30000 Training Loss: 0.06796585768461227\n",
      "Epoch 4700/30000 Validation Loss: 0.05195070803165436\n",
      "Epoch 4701/30000 Training Loss: 0.06243068352341652\n",
      "Epoch 4702/30000 Training Loss: 0.05990445986390114\n",
      "Epoch 4703/30000 Training Loss: 0.05372120440006256\n",
      "Epoch 4704/30000 Training Loss: 0.06723111867904663\n",
      "Epoch 4705/30000 Training Loss: 0.04601883515715599\n",
      "Epoch 4706/30000 Training Loss: 0.06789252161979675\n",
      "Epoch 4707/30000 Training Loss: 0.0778944343328476\n",
      "Epoch 4708/30000 Training Loss: 0.05302390828728676\n",
      "Epoch 4709/30000 Training Loss: 0.05656043812632561\n",
      "Epoch 4710/30000 Training Loss: 0.058830562978982925\n",
      "Epoch 4711/30000 Training Loss: 0.06308674067258835\n",
      "Epoch 4712/30000 Training Loss: 0.07209058105945587\n",
      "Epoch 4713/30000 Training Loss: 0.07199309021234512\n",
      "Epoch 4714/30000 Training Loss: 0.05048638582229614\n",
      "Epoch 4715/30000 Training Loss: 0.06384941935539246\n",
      "Epoch 4716/30000 Training Loss: 0.06697601824998856\n",
      "Epoch 4717/30000 Training Loss: 0.06778567284345627\n",
      "Epoch 4718/30000 Training Loss: 0.05699044466018677\n",
      "Epoch 4719/30000 Training Loss: 0.06265996396541595\n",
      "Epoch 4720/30000 Training Loss: 0.048443958163261414\n",
      "Epoch 4721/30000 Training Loss: 0.061187200248241425\n",
      "Epoch 4722/30000 Training Loss: 0.08205210417509079\n",
      "Epoch 4723/30000 Training Loss: 0.06575865298509598\n",
      "Epoch 4724/30000 Training Loss: 0.06961499154567719\n",
      "Epoch 4725/30000 Training Loss: 0.053518395870923996\n",
      "Epoch 4726/30000 Training Loss: 0.07341404259204865\n",
      "Epoch 4727/30000 Training Loss: 0.06790325790643692\n",
      "Epoch 4728/30000 Training Loss: 0.05697295814752579\n",
      "Epoch 4729/30000 Training Loss: 0.06676854193210602\n",
      "Epoch 4730/30000 Training Loss: 0.06177343428134918\n",
      "Epoch 4731/30000 Training Loss: 0.05495947599411011\n",
      "Epoch 4732/30000 Training Loss: 0.0636361688375473\n",
      "Epoch 4733/30000 Training Loss: 0.05839371308684349\n",
      "Epoch 4734/30000 Training Loss: 0.06714732944965363\n",
      "Epoch 4735/30000 Training Loss: 0.094953253865242\n",
      "Epoch 4736/30000 Training Loss: 0.06525728851556778\n",
      "Epoch 4737/30000 Training Loss: 0.05263877660036087\n",
      "Epoch 4738/30000 Training Loss: 0.06487563252449036\n",
      "Epoch 4739/30000 Training Loss: 0.06517799198627472\n",
      "Epoch 4740/30000 Training Loss: 0.05576528608798981\n",
      "Epoch 4741/30000 Training Loss: 0.0512266531586647\n",
      "Epoch 4742/30000 Training Loss: 0.07190483808517456\n",
      "Epoch 4743/30000 Training Loss: 0.05576665699481964\n",
      "Epoch 4744/30000 Training Loss: 0.06367465853691101\n",
      "Epoch 4745/30000 Training Loss: 0.06676457822322845\n",
      "Epoch 4746/30000 Training Loss: 0.05963636562228203\n",
      "Epoch 4747/30000 Training Loss: 0.05712796747684479\n",
      "Epoch 4748/30000 Training Loss: 0.07989950478076935\n",
      "Epoch 4749/30000 Training Loss: 0.05725463479757309\n",
      "Epoch 4750/30000 Training Loss: 0.07555723190307617\n",
      "Epoch 4751/30000 Training Loss: 0.06589049845933914\n",
      "Epoch 4752/30000 Training Loss: 0.08627042174339294\n",
      "Epoch 4753/30000 Training Loss: 0.05492837727069855\n",
      "Epoch 4754/30000 Training Loss: 0.06666253507137299\n",
      "Epoch 4755/30000 Training Loss: 0.06616339087486267\n",
      "Epoch 4756/30000 Training Loss: 0.07859770208597183\n",
      "Epoch 4757/30000 Training Loss: 0.05703579634428024\n",
      "Epoch 4758/30000 Training Loss: 0.05434611439704895\n",
      "Epoch 4759/30000 Training Loss: 0.0536966510117054\n",
      "Epoch 4760/30000 Training Loss: 0.0646602138876915\n",
      "Epoch 4761/30000 Training Loss: 0.060269713401794434\n",
      "Epoch 4762/30000 Training Loss: 0.061686448752880096\n",
      "Epoch 4763/30000 Training Loss: 0.04497333988547325\n",
      "Epoch 4764/30000 Training Loss: 0.06388761103153229\n",
      "Epoch 4765/30000 Training Loss: 0.055106423795223236\n",
      "Epoch 4766/30000 Training Loss: 0.04070614278316498\n",
      "Epoch 4767/30000 Training Loss: 0.05640647932887077\n",
      "Epoch 4768/30000 Training Loss: 0.054432161152362823\n",
      "Epoch 4769/30000 Training Loss: 0.0572119876742363\n",
      "Epoch 4770/30000 Training Loss: 0.05295683443546295\n",
      "Epoch 4771/30000 Training Loss: 0.06341762840747833\n",
      "Epoch 4772/30000 Training Loss: 0.05747769773006439\n",
      "Epoch 4773/30000 Training Loss: 0.06379088014364243\n",
      "Epoch 4774/30000 Training Loss: 0.07180540263652802\n",
      "Epoch 4775/30000 Training Loss: 0.06778714805841446\n",
      "Epoch 4776/30000 Training Loss: 0.0619504377245903\n",
      "Epoch 4777/30000 Training Loss: 0.04788920283317566\n",
      "Epoch 4778/30000 Training Loss: 0.06844918429851532\n",
      "Epoch 4779/30000 Training Loss: 0.06513456255197525\n",
      "Epoch 4780/30000 Training Loss: 0.05934831127524376\n",
      "Epoch 4781/30000 Training Loss: 0.0674690529704094\n",
      "Epoch 4782/30000 Training Loss: 0.06279259920120239\n",
      "Epoch 4783/30000 Training Loss: 0.0567629337310791\n",
      "Epoch 4784/30000 Training Loss: 0.06433068215847015\n",
      "Epoch 4785/30000 Training Loss: 0.06588713079690933\n",
      "Epoch 4786/30000 Training Loss: 0.05375463888049126\n",
      "Epoch 4787/30000 Training Loss: 0.07933628559112549\n",
      "Epoch 4788/30000 Training Loss: 0.07553725689649582\n",
      "Epoch 4789/30000 Training Loss: 0.0424017459154129\n",
      "Epoch 4790/30000 Training Loss: 0.07322437316179276\n",
      "Epoch 4791/30000 Training Loss: 0.04934196174144745\n",
      "Epoch 4792/30000 Training Loss: 0.06321746110916138\n",
      "Epoch 4793/30000 Training Loss: 0.0626550018787384\n",
      "Epoch 4794/30000 Training Loss: 0.05371001362800598\n",
      "Epoch 4795/30000 Training Loss: 0.045105744153261185\n",
      "Epoch 4796/30000 Training Loss: 0.0500863641500473\n",
      "Epoch 4797/30000 Training Loss: 0.05741666257381439\n",
      "Epoch 4798/30000 Training Loss: 0.07212226092815399\n",
      "Epoch 4799/30000 Training Loss: 0.05608295276761055\n",
      "Epoch 4800/30000 Training Loss: 0.05442742630839348\n",
      "Epoch 4800/30000 Validation Loss: 0.06335202604532242\n",
      "Epoch 4801/30000 Training Loss: 0.05850832536816597\n",
      "Epoch 4802/30000 Training Loss: 0.06273123621940613\n",
      "Epoch 4803/30000 Training Loss: 0.06218786910176277\n",
      "Epoch 4804/30000 Training Loss: 0.04866279289126396\n",
      "Epoch 4805/30000 Training Loss: 0.05351845920085907\n",
      "Epoch 4806/30000 Training Loss: 0.06415022164583206\n",
      "Epoch 4807/30000 Training Loss: 0.07856065779924393\n",
      "Epoch 4808/30000 Training Loss: 0.06518007814884186\n",
      "Epoch 4809/30000 Training Loss: 0.054129283875226974\n",
      "Epoch 4810/30000 Training Loss: 0.06208623945713043\n",
      "Epoch 4811/30000 Training Loss: 0.08367528021335602\n",
      "Epoch 4812/30000 Training Loss: 0.06213665008544922\n",
      "Epoch 4813/30000 Training Loss: 0.06772755831480026\n",
      "Epoch 4814/30000 Training Loss: 0.05014481395483017\n",
      "Epoch 4815/30000 Training Loss: 0.0448172390460968\n",
      "Epoch 4816/30000 Training Loss: 0.05775480717420578\n",
      "Epoch 4817/30000 Training Loss: 0.0430363230407238\n",
      "Epoch 4818/30000 Training Loss: 0.04802979528903961\n",
      "Epoch 4819/30000 Training Loss: 0.06625946611166\n",
      "Epoch 4820/30000 Training Loss: 0.07074005901813507\n",
      "Epoch 4821/30000 Training Loss: 0.05790923908352852\n",
      "Epoch 4822/30000 Training Loss: 0.05273114889860153\n",
      "Epoch 4823/30000 Training Loss: 0.07201387733221054\n",
      "Epoch 4824/30000 Training Loss: 0.06587239354848862\n",
      "Epoch 4825/30000 Training Loss: 0.0689738392829895\n",
      "Epoch 4826/30000 Training Loss: 0.051426105201244354\n",
      "Epoch 4827/30000 Training Loss: 0.05601891130208969\n",
      "Epoch 4828/30000 Training Loss: 0.0629957914352417\n",
      "Epoch 4829/30000 Training Loss: 0.05954825505614281\n",
      "Epoch 4830/30000 Training Loss: 0.08633025735616684\n",
      "Epoch 4831/30000 Training Loss: 0.07259144634008408\n",
      "Epoch 4832/30000 Training Loss: 0.060145579278469086\n",
      "Epoch 4833/30000 Training Loss: 0.05636683478951454\n",
      "Epoch 4834/30000 Training Loss: 0.05944930016994476\n",
      "Epoch 4835/30000 Training Loss: 0.053687334060668945\n",
      "Epoch 4836/30000 Training Loss: 0.048152342438697815\n",
      "Epoch 4837/30000 Training Loss: 0.060140009969472885\n",
      "Epoch 4838/30000 Training Loss: 0.04070785641670227\n",
      "Epoch 4839/30000 Training Loss: 0.07483740895986557\n",
      "Epoch 4840/30000 Training Loss: 0.06764715909957886\n",
      "Epoch 4841/30000 Training Loss: 0.05373939871788025\n",
      "Epoch 4842/30000 Training Loss: 0.051060888916254044\n",
      "Epoch 4843/30000 Training Loss: 0.04865117371082306\n",
      "Epoch 4844/30000 Training Loss: 0.0704989954829216\n",
      "Epoch 4845/30000 Training Loss: 0.050841040909290314\n",
      "Epoch 4846/30000 Training Loss: 0.05523882806301117\n",
      "Epoch 4847/30000 Training Loss: 0.05843131244182587\n",
      "Epoch 4848/30000 Training Loss: 0.04738829657435417\n",
      "Epoch 4849/30000 Training Loss: 0.0643639862537384\n",
      "Epoch 4850/30000 Training Loss: 0.07721451669931412\n",
      "Epoch 4851/30000 Training Loss: 0.05931408703327179\n",
      "Epoch 4852/30000 Training Loss: 0.07123474031686783\n",
      "Epoch 4853/30000 Training Loss: 0.03965086489915848\n",
      "Epoch 4854/30000 Training Loss: 0.05832739546895027\n",
      "Epoch 4855/30000 Training Loss: 0.050538595765829086\n",
      "Epoch 4856/30000 Training Loss: 0.05848171189427376\n",
      "Epoch 4857/30000 Training Loss: 0.06530368328094482\n",
      "Epoch 4858/30000 Training Loss: 0.04963367432355881\n",
      "Epoch 4859/30000 Training Loss: 0.05955606326460838\n",
      "Epoch 4860/30000 Training Loss: 0.07862863689661026\n",
      "Epoch 4861/30000 Training Loss: 0.0684274435043335\n",
      "Epoch 4862/30000 Training Loss: 0.07847417891025543\n",
      "Epoch 4863/30000 Training Loss: 0.06404121965169907\n",
      "Epoch 4864/30000 Training Loss: 0.05931476876139641\n",
      "Epoch 4865/30000 Training Loss: 0.08002568781375885\n",
      "Epoch 4866/30000 Training Loss: 0.04578649625182152\n",
      "Epoch 4867/30000 Training Loss: 0.06319482624530792\n",
      "Epoch 4868/30000 Training Loss: 0.08235298097133636\n",
      "Epoch 4869/30000 Training Loss: 0.05408933013677597\n",
      "Epoch 4870/30000 Training Loss: 0.07650963217020035\n",
      "Epoch 4871/30000 Training Loss: 0.07605193555355072\n",
      "Epoch 4872/30000 Training Loss: 0.05087132751941681\n",
      "Epoch 4873/30000 Training Loss: 0.060073405504226685\n",
      "Epoch 4874/30000 Training Loss: 0.04022138938307762\n",
      "Epoch 4875/30000 Training Loss: 0.05514853820204735\n",
      "Epoch 4876/30000 Training Loss: 0.05869298428297043\n",
      "Epoch 4877/30000 Training Loss: 0.052797582000494\n",
      "Epoch 4878/30000 Training Loss: 0.05001715570688248\n",
      "Epoch 4879/30000 Training Loss: 0.06052941456437111\n",
      "Epoch 4880/30000 Training Loss: 0.06440778076648712\n",
      "Epoch 4881/30000 Training Loss: 0.056746747344732285\n",
      "Epoch 4882/30000 Training Loss: 0.06742031127214432\n",
      "Epoch 4883/30000 Training Loss: 0.06400686502456665\n",
      "Epoch 4884/30000 Training Loss: 0.05624319612979889\n",
      "Epoch 4885/30000 Training Loss: 0.06438009440898895\n",
      "Epoch 4886/30000 Training Loss: 0.06352889537811279\n",
      "Epoch 4887/30000 Training Loss: 0.06896834820508957\n",
      "Epoch 4888/30000 Training Loss: 0.06084313988685608\n",
      "Epoch 4889/30000 Training Loss: 0.04582752287387848\n",
      "Epoch 4890/30000 Training Loss: 0.06020750850439072\n",
      "Epoch 4891/30000 Training Loss: 0.06516958773136139\n",
      "Epoch 4892/30000 Training Loss: 0.07147837430238724\n",
      "Epoch 4893/30000 Training Loss: 0.06710323691368103\n",
      "Epoch 4894/30000 Training Loss: 0.06829597055912018\n",
      "Epoch 4895/30000 Training Loss: 0.05835574120283127\n",
      "Epoch 4896/30000 Training Loss: 0.0556938499212265\n",
      "Epoch 4897/30000 Training Loss: 0.05624115467071533\n",
      "Epoch 4898/30000 Training Loss: 0.05673264339566231\n",
      "Epoch 4899/30000 Training Loss: 0.0754958987236023\n",
      "Epoch 4900/30000 Training Loss: 0.08084917813539505\n",
      "Epoch 4900/30000 Validation Loss: 0.06718041002750397\n",
      "Epoch 4901/30000 Training Loss: 0.0574922077357769\n",
      "Epoch 4902/30000 Training Loss: 0.07330379635095596\n",
      "Epoch 4903/30000 Training Loss: 0.06758663803339005\n",
      "Epoch 4904/30000 Training Loss: 0.0698850080370903\n",
      "Epoch 4905/30000 Training Loss: 0.0640033707022667\n",
      "Epoch 4906/30000 Training Loss: 0.03595556691288948\n",
      "Epoch 4907/30000 Training Loss: 0.046193379908800125\n",
      "Epoch 4908/30000 Training Loss: 0.05212797224521637\n",
      "Epoch 4909/30000 Training Loss: 0.05769472196698189\n",
      "Epoch 4910/30000 Training Loss: 0.07574792206287384\n",
      "Epoch 4911/30000 Training Loss: 0.057142000645399094\n",
      "Epoch 4912/30000 Training Loss: 0.07158290594816208\n",
      "Epoch 4913/30000 Training Loss: 0.06544604897499084\n",
      "Epoch 4914/30000 Training Loss: 0.0637597069144249\n",
      "Epoch 4915/30000 Training Loss: 0.06631821393966675\n",
      "Epoch 4916/30000 Training Loss: 0.04946291446685791\n",
      "Epoch 4917/30000 Training Loss: 0.07047541439533234\n",
      "Epoch 4918/30000 Training Loss: 0.04625440388917923\n",
      "Epoch 4919/30000 Training Loss: 0.0397554449737072\n",
      "Epoch 4920/30000 Training Loss: 0.06620559841394424\n",
      "Epoch 4921/30000 Training Loss: 0.06491584330797195\n",
      "Epoch 4922/30000 Training Loss: 0.06509306281805038\n",
      "Epoch 4923/30000 Training Loss: 0.05518309772014618\n",
      "Epoch 4924/30000 Training Loss: 0.06265386193990707\n",
      "Epoch 4925/30000 Training Loss: 0.06063447147607803\n",
      "Epoch 4926/30000 Training Loss: 0.05286570265889168\n",
      "Epoch 4927/30000 Training Loss: 0.04687486216425896\n",
      "Epoch 4928/30000 Training Loss: 0.059320878237485886\n",
      "Epoch 4929/30000 Training Loss: 0.08007054775953293\n",
      "Epoch 4930/30000 Training Loss: 0.06459829211235046\n",
      "Epoch 4931/30000 Training Loss: 0.05143647640943527\n",
      "Epoch 4932/30000 Training Loss: 0.07750561833381653\n",
      "Epoch 4933/30000 Training Loss: 0.06526448577642441\n",
      "Epoch 4934/30000 Training Loss: 0.07541153579950333\n",
      "Epoch 4935/30000 Training Loss: 0.05980691313743591\n",
      "Epoch 4936/30000 Training Loss: 0.06257368624210358\n",
      "Epoch 4937/30000 Training Loss: 0.044851381331682205\n",
      "Epoch 4938/30000 Training Loss: 0.08567553758621216\n",
      "Epoch 4939/30000 Training Loss: 0.06757383048534393\n",
      "Epoch 4940/30000 Training Loss: 0.06267936527729034\n",
      "Epoch 4941/30000 Training Loss: 0.053349439054727554\n",
      "Epoch 4942/30000 Training Loss: 0.058731697499752045\n",
      "Epoch 4943/30000 Training Loss: 0.04907911270856857\n",
      "Epoch 4944/30000 Training Loss: 0.05050984025001526\n",
      "Epoch 4945/30000 Training Loss: 0.0578693188726902\n",
      "Epoch 4946/30000 Training Loss: 0.07193958014249802\n",
      "Epoch 4947/30000 Training Loss: 0.05452108383178711\n",
      "Epoch 4948/30000 Training Loss: 0.04937010630965233\n",
      "Epoch 4949/30000 Training Loss: 0.0604238361120224\n",
      "Epoch 4950/30000 Training Loss: 0.05885433405637741\n",
      "Epoch 4951/30000 Training Loss: 0.054992690682411194\n",
      "Epoch 4952/30000 Training Loss: 0.0567769892513752\n",
      "Epoch 4953/30000 Training Loss: 0.060776159167289734\n",
      "Epoch 4954/30000 Training Loss: 0.052011750638484955\n",
      "Epoch 4955/30000 Training Loss: 0.06537815928459167\n",
      "Epoch 4956/30000 Training Loss: 0.06289691478013992\n",
      "Epoch 4957/30000 Training Loss: 0.07078925520181656\n",
      "Epoch 4958/30000 Training Loss: 0.05973750725388527\n",
      "Epoch 4959/30000 Training Loss: 0.06203703582286835\n",
      "Epoch 4960/30000 Training Loss: 0.05946528539061546\n",
      "Epoch 4961/30000 Training Loss: 0.0587787926197052\n",
      "Epoch 4962/30000 Training Loss: 0.05819401144981384\n",
      "Epoch 4963/30000 Training Loss: 0.05539756268262863\n",
      "Epoch 4964/30000 Training Loss: 0.06232624500989914\n",
      "Epoch 4965/30000 Training Loss: 0.05373593419790268\n",
      "Epoch 4966/30000 Training Loss: 0.06510736048221588\n",
      "Epoch 4967/30000 Training Loss: 0.05578019469976425\n",
      "Epoch 4968/30000 Training Loss: 0.073815256357193\n",
      "Epoch 4969/30000 Training Loss: 0.08309877663850784\n",
      "Epoch 4970/30000 Training Loss: 0.06429216265678406\n",
      "Epoch 4971/30000 Training Loss: 0.06103701889514923\n",
      "Epoch 4972/30000 Training Loss: 0.04976906627416611\n",
      "Epoch 4973/30000 Training Loss: 0.06916092336177826\n",
      "Epoch 4974/30000 Training Loss: 0.05202111601829529\n",
      "Epoch 4975/30000 Training Loss: 0.05150160193443298\n",
      "Epoch 4976/30000 Training Loss: 0.07760210335254669\n",
      "Epoch 4977/30000 Training Loss: 0.07737642526626587\n",
      "Epoch 4978/30000 Training Loss: 0.06299875676631927\n",
      "Epoch 4979/30000 Training Loss: 0.05938643962144852\n",
      "Epoch 4980/30000 Training Loss: 0.06505684554576874\n",
      "Epoch 4981/30000 Training Loss: 0.049489833414554596\n",
      "Epoch 4982/30000 Training Loss: 0.06236410513520241\n",
      "Epoch 4983/30000 Training Loss: 0.06165944039821625\n",
      "Epoch 4984/30000 Training Loss: 0.054916683584451675\n",
      "Epoch 4985/30000 Training Loss: 0.04902662709355354\n",
      "Epoch 4986/30000 Training Loss: 0.09796186536550522\n",
      "Epoch 4987/30000 Training Loss: 0.06560482829809189\n",
      "Epoch 4988/30000 Training Loss: 0.07077831774950027\n",
      "Epoch 4989/30000 Training Loss: 0.055710434913635254\n",
      "Epoch 4990/30000 Training Loss: 0.06811509281396866\n",
      "Epoch 4991/30000 Training Loss: 0.06906405836343765\n",
      "Epoch 4992/30000 Training Loss: 0.06096058338880539\n",
      "Epoch 4993/30000 Training Loss: 0.042437803000211716\n",
      "Epoch 4994/30000 Training Loss: 0.05074131488800049\n",
      "Epoch 4995/30000 Training Loss: 0.05839593708515167\n",
      "Epoch 4996/30000 Training Loss: 0.06132102757692337\n",
      "Epoch 4997/30000 Training Loss: 0.06111918017268181\n",
      "Epoch 4998/30000 Training Loss: 0.05427511781454086\n",
      "Epoch 4999/30000 Training Loss: 0.06441628187894821\n",
      "Epoch 5000/30000 Training Loss: 0.07935495674610138\n",
      "Epoch 5000/30000 Validation Loss: 0.06600067019462585\n",
      "Epoch 5001/30000 Training Loss: 0.055659521371126175\n",
      "Epoch 5002/30000 Training Loss: 0.05722224712371826\n",
      "Epoch 5003/30000 Training Loss: 0.0581192821264267\n",
      "Epoch 5004/30000 Training Loss: 0.06323510408401489\n",
      "Epoch 5005/30000 Training Loss: 0.06189786642789841\n",
      "Epoch 5006/30000 Training Loss: 0.05916547775268555\n",
      "Epoch 5007/30000 Training Loss: 0.05582241714000702\n",
      "Epoch 5008/30000 Training Loss: 0.05699384957551956\n",
      "Epoch 5009/30000 Training Loss: 0.06203785538673401\n",
      "Epoch 5010/30000 Training Loss: 0.054328955709934235\n",
      "Epoch 5011/30000 Training Loss: 0.06569541990756989\n",
      "Epoch 5012/30000 Training Loss: 0.06154763698577881\n",
      "Epoch 5013/30000 Training Loss: 0.05594763532280922\n",
      "Epoch 5014/30000 Training Loss: 0.05204761028289795\n",
      "Epoch 5015/30000 Training Loss: 0.05198855698108673\n",
      "Epoch 5016/30000 Training Loss: 0.049619756639003754\n",
      "Epoch 5017/30000 Training Loss: 0.05308646336197853\n",
      "Epoch 5018/30000 Training Loss: 0.06974700838327408\n",
      "Epoch 5019/30000 Training Loss: 0.06261534243822098\n",
      "Epoch 5020/30000 Training Loss: 0.06985831260681152\n",
      "Epoch 5021/30000 Training Loss: 0.07217036187648773\n",
      "Epoch 5022/30000 Training Loss: 0.05152026563882828\n",
      "Epoch 5023/30000 Training Loss: 0.05959586054086685\n",
      "Epoch 5024/30000 Training Loss: 0.07578781992197037\n",
      "Epoch 5025/30000 Training Loss: 0.06366688758134842\n",
      "Epoch 5026/30000 Training Loss: 0.05469517409801483\n",
      "Epoch 5027/30000 Training Loss: 0.050321683287620544\n",
      "Epoch 5028/30000 Training Loss: 0.08041775971651077\n",
      "Epoch 5029/30000 Training Loss: 0.05875310301780701\n",
      "Epoch 5030/30000 Training Loss: 0.06060255691409111\n",
      "Epoch 5031/30000 Training Loss: 0.0770559161901474\n",
      "Epoch 5032/30000 Training Loss: 0.06381195038557053\n",
      "Epoch 5033/30000 Training Loss: 0.05822061002254486\n",
      "Epoch 5034/30000 Training Loss: 0.05434402823448181\n",
      "Epoch 5035/30000 Training Loss: 0.05401263386011124\n",
      "Epoch 5036/30000 Training Loss: 0.046388015151023865\n",
      "Epoch 5037/30000 Training Loss: 0.05754504352807999\n",
      "Epoch 5038/30000 Training Loss: 0.06351877003908157\n",
      "Epoch 5039/30000 Training Loss: 0.05048046633601189\n",
      "Epoch 5040/30000 Training Loss: 0.07543162256479263\n",
      "Epoch 5041/30000 Training Loss: 0.0609784796833992\n",
      "Epoch 5042/30000 Training Loss: 0.05198052152991295\n",
      "Epoch 5043/30000 Training Loss: 0.054275695234537125\n",
      "Epoch 5044/30000 Training Loss: 0.06213020533323288\n",
      "Epoch 5045/30000 Training Loss: 0.06239815801382065\n",
      "Epoch 5046/30000 Training Loss: 0.04774497076869011\n",
      "Epoch 5047/30000 Training Loss: 0.07272070646286011\n",
      "Epoch 5048/30000 Training Loss: 0.06912338733673096\n",
      "Epoch 5049/30000 Training Loss: 0.0677213966846466\n",
      "Epoch 5050/30000 Training Loss: 0.04865968972444534\n",
      "Epoch 5051/30000 Training Loss: 0.06123612821102142\n",
      "Epoch 5052/30000 Training Loss: 0.07511358708143234\n",
      "Epoch 5053/30000 Training Loss: 0.06694238632917404\n",
      "Epoch 5054/30000 Training Loss: 0.04937131330370903\n",
      "Epoch 5055/30000 Training Loss: 0.052417315542697906\n",
      "Epoch 5056/30000 Training Loss: 0.08051372319459915\n",
      "Epoch 5057/30000 Training Loss: 0.06053663790225983\n",
      "Epoch 5058/30000 Training Loss: 0.0768299400806427\n",
      "Epoch 5059/30000 Training Loss: 0.05211898311972618\n",
      "Epoch 5060/30000 Training Loss: 0.07684139162302017\n",
      "Epoch 5061/30000 Training Loss: 0.06463075429201126\n",
      "Epoch 5062/30000 Training Loss: 0.0882672667503357\n",
      "Epoch 5063/30000 Training Loss: 0.06126909703016281\n",
      "Epoch 5064/30000 Training Loss: 0.0438401997089386\n",
      "Epoch 5065/30000 Training Loss: 0.05869828537106514\n",
      "Epoch 5066/30000 Training Loss: 0.055324867367744446\n",
      "Epoch 5067/30000 Training Loss: 0.045737650245428085\n",
      "Epoch 5068/30000 Training Loss: 0.06133129447698593\n",
      "Epoch 5069/30000 Training Loss: 0.06665686517953873\n",
      "Epoch 5070/30000 Training Loss: 0.07580669224262238\n",
      "Epoch 5071/30000 Training Loss: 0.05696195363998413\n",
      "Epoch 5072/30000 Training Loss: 0.0562271773815155\n",
      "Epoch 5073/30000 Training Loss: 0.05940427631139755\n",
      "Epoch 5074/30000 Training Loss: 0.06500384211540222\n",
      "Epoch 5075/30000 Training Loss: 0.0646771565079689\n",
      "Epoch 5076/30000 Training Loss: 0.07655062526464462\n",
      "Epoch 5077/30000 Training Loss: 0.04729991778731346\n",
      "Epoch 5078/30000 Training Loss: 0.049200352281332016\n",
      "Epoch 5079/30000 Training Loss: 0.05611080676317215\n",
      "Epoch 5080/30000 Training Loss: 0.06850709021091461\n",
      "Epoch 5081/30000 Training Loss: 0.05250030755996704\n",
      "Epoch 5082/30000 Training Loss: 0.07351897656917572\n",
      "Epoch 5083/30000 Training Loss: 0.06478358805179596\n",
      "Epoch 5084/30000 Training Loss: 0.05978890880942345\n",
      "Epoch 5085/30000 Training Loss: 0.05830042064189911\n",
      "Epoch 5086/30000 Training Loss: 0.07057859748601913\n",
      "Epoch 5087/30000 Training Loss: 0.05232857167720795\n",
      "Epoch 5088/30000 Training Loss: 0.06139050051569939\n",
      "Epoch 5089/30000 Training Loss: 0.06284663826227188\n",
      "Epoch 5090/30000 Training Loss: 0.07066035270690918\n",
      "Epoch 5091/30000 Training Loss: 0.05869853496551514\n",
      "Epoch 5092/30000 Training Loss: 0.06534188240766525\n",
      "Epoch 5093/30000 Training Loss: 0.05354417487978935\n",
      "Epoch 5094/30000 Training Loss: 0.07235614210367203\n",
      "Epoch 5095/30000 Training Loss: 0.061704520136117935\n",
      "Epoch 5096/30000 Training Loss: 0.05152939260005951\n",
      "Epoch 5097/30000 Training Loss: 0.06942382454872131\n",
      "Epoch 5098/30000 Training Loss: 0.055562518537044525\n",
      "Epoch 5099/30000 Training Loss: 0.062056537717580795\n",
      "Epoch 5100/30000 Training Loss: 0.07425836473703384\n",
      "Epoch 5100/30000 Validation Loss: 0.04995797574520111\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04995797574520111<=============\n",
      "Epoch 5101/30000 Training Loss: 0.06421104073524475\n",
      "Epoch 5102/30000 Training Loss: 0.05595818907022476\n",
      "Epoch 5103/30000 Training Loss: 0.06759318709373474\n",
      "Epoch 5104/30000 Training Loss: 0.06706949323415756\n",
      "Epoch 5105/30000 Training Loss: 0.06884051859378815\n",
      "Epoch 5106/30000 Training Loss: 0.052039723843336105\n",
      "Epoch 5107/30000 Training Loss: 0.06167076528072357\n",
      "Epoch 5108/30000 Training Loss: 0.06505066156387329\n",
      "Epoch 5109/30000 Training Loss: 0.07292193174362183\n",
      "Epoch 5110/30000 Training Loss: 0.0460701659321785\n",
      "Epoch 5111/30000 Training Loss: 0.05779968947172165\n",
      "Epoch 5112/30000 Training Loss: 0.04371389001607895\n",
      "Epoch 5113/30000 Training Loss: 0.05187716707587242\n",
      "Epoch 5114/30000 Training Loss: 0.06692691892385483\n",
      "Epoch 5115/30000 Training Loss: 0.0655195415019989\n",
      "Epoch 5116/30000 Training Loss: 0.046912696212530136\n",
      "Epoch 5117/30000 Training Loss: 0.051006607711315155\n",
      "Epoch 5118/30000 Training Loss: 0.06416941434144974\n",
      "Epoch 5119/30000 Training Loss: 0.05228768289089203\n",
      "Epoch 5120/30000 Training Loss: 0.06258925050497055\n",
      "Epoch 5121/30000 Training Loss: 0.07688725739717484\n",
      "Epoch 5122/30000 Training Loss: 0.07549714297056198\n",
      "Epoch 5123/30000 Training Loss: 0.05514569953083992\n",
      "Epoch 5124/30000 Training Loss: 0.059629470109939575\n",
      "Epoch 5125/30000 Training Loss: 0.05198904871940613\n",
      "Epoch 5126/30000 Training Loss: 0.044054821133613586\n",
      "Epoch 5127/30000 Training Loss: 0.06320519000291824\n",
      "Epoch 5128/30000 Training Loss: 0.05303685739636421\n",
      "Epoch 5129/30000 Training Loss: 0.06295078992843628\n",
      "Epoch 5130/30000 Training Loss: 0.06716892123222351\n",
      "Epoch 5131/30000 Training Loss: 0.07008825242519379\n",
      "Epoch 5132/30000 Training Loss: 0.048761527985334396\n",
      "Epoch 5133/30000 Training Loss: 0.0585019625723362\n",
      "Epoch 5134/30000 Training Loss: 0.06604202091693878\n",
      "Epoch 5135/30000 Training Loss: 0.0732412114739418\n",
      "Epoch 5136/30000 Training Loss: 0.061876364052295685\n",
      "Epoch 5137/30000 Training Loss: 0.06324151158332825\n",
      "Epoch 5138/30000 Training Loss: 0.05425714701414108\n",
      "Epoch 5139/30000 Training Loss: 0.05994037538766861\n",
      "Epoch 5140/30000 Training Loss: 0.06122494488954544\n",
      "Epoch 5141/30000 Training Loss: 0.06230185180902481\n",
      "Epoch 5142/30000 Training Loss: 0.0518154501914978\n",
      "Epoch 5143/30000 Training Loss: 0.07275208085775375\n",
      "Epoch 5144/30000 Training Loss: 0.07205767184495926\n",
      "Epoch 5145/30000 Training Loss: 0.06048028916120529\n",
      "Epoch 5146/30000 Training Loss: 0.06123156100511551\n",
      "Epoch 5147/30000 Training Loss: 0.05875128135085106\n",
      "Epoch 5148/30000 Training Loss: 0.06824536621570587\n",
      "Epoch 5149/30000 Training Loss: 0.06022864952683449\n",
      "Epoch 5150/30000 Training Loss: 0.06592851877212524\n",
      "Epoch 5151/30000 Training Loss: 0.06658592075109482\n",
      "Epoch 5152/30000 Training Loss: 0.05788695812225342\n",
      "Epoch 5153/30000 Training Loss: 0.05063015967607498\n",
      "Epoch 5154/30000 Training Loss: 0.05255760997533798\n",
      "Epoch 5155/30000 Training Loss: 0.06313983350992203\n",
      "Epoch 5156/30000 Training Loss: 0.06450670212507248\n",
      "Epoch 5157/30000 Training Loss: 0.05337439849972725\n",
      "Epoch 5158/30000 Training Loss: 0.04529090225696564\n",
      "Epoch 5159/30000 Training Loss: 0.07978735864162445\n",
      "Epoch 5160/30000 Training Loss: 0.05537257343530655\n",
      "Epoch 5161/30000 Training Loss: 0.05125627666711807\n",
      "Epoch 5162/30000 Training Loss: 0.06811627000570297\n",
      "Epoch 5163/30000 Training Loss: 0.056847985833883286\n",
      "Epoch 5164/30000 Training Loss: 0.06983853131532669\n",
      "Epoch 5165/30000 Training Loss: 0.07379559427499771\n",
      "Epoch 5166/30000 Training Loss: 0.05965624749660492\n",
      "Epoch 5167/30000 Training Loss: 0.07740384340286255\n",
      "Epoch 5168/30000 Training Loss: 0.06729398667812347\n",
      "Epoch 5169/30000 Training Loss: 0.040212348103523254\n",
      "Epoch 5170/30000 Training Loss: 0.07074317336082458\n",
      "Epoch 5171/30000 Training Loss: 0.07337531447410583\n",
      "Epoch 5172/30000 Training Loss: 0.054000124335289\n",
      "Epoch 5173/30000 Training Loss: 0.06798294186592102\n",
      "Epoch 5174/30000 Training Loss: 0.05176752060651779\n",
      "Epoch 5175/30000 Training Loss: 0.05685844272375107\n",
      "Epoch 5176/30000 Training Loss: 0.04691407084465027\n",
      "Epoch 5177/30000 Training Loss: 0.07229132950305939\n",
      "Epoch 5178/30000 Training Loss: 0.05300416797399521\n",
      "Epoch 5179/30000 Training Loss: 0.08021952211856842\n",
      "Epoch 5180/30000 Training Loss: 0.04772228002548218\n",
      "Epoch 5181/30000 Training Loss: 0.05241667479276657\n",
      "Epoch 5182/30000 Training Loss: 0.06007513403892517\n",
      "Epoch 5183/30000 Training Loss: 0.05176115036010742\n",
      "Epoch 5184/30000 Training Loss: 0.06093640998005867\n",
      "Epoch 5185/30000 Training Loss: 0.078790582716465\n",
      "Epoch 5186/30000 Training Loss: 0.05416963994503021\n",
      "Epoch 5187/30000 Training Loss: 0.08393827825784683\n",
      "Epoch 5188/30000 Training Loss: 0.05500059202313423\n",
      "Epoch 5189/30000 Training Loss: 0.053238265216350555\n",
      "Epoch 5190/30000 Training Loss: 0.09094847738742828\n",
      "Epoch 5191/30000 Training Loss: 0.04442824050784111\n",
      "Epoch 5192/30000 Training Loss: 0.05581703782081604\n",
      "Epoch 5193/30000 Training Loss: 0.06749921292066574\n",
      "Epoch 5194/30000 Training Loss: 0.04610159248113632\n",
      "Epoch 5195/30000 Training Loss: 0.08225006610155106\n",
      "Epoch 5196/30000 Training Loss: 0.04885443300008774\n",
      "Epoch 5197/30000 Training Loss: 0.05934352055191994\n",
      "Epoch 5198/30000 Training Loss: 0.06638548523187637\n",
      "Epoch 5199/30000 Training Loss: 0.0683489441871643\n",
      "Epoch 5200/30000 Training Loss: 0.05872170627117157\n",
      "Epoch 5200/30000 Validation Loss: 0.06418325006961823\n",
      "Epoch 5201/30000 Training Loss: 0.046314045786857605\n",
      "Epoch 5202/30000 Training Loss: 0.05684896185994148\n",
      "Epoch 5203/30000 Training Loss: 0.0613577701151371\n",
      "Epoch 5204/30000 Training Loss: 0.038091741502285004\n",
      "Epoch 5205/30000 Training Loss: 0.047019001096487045\n",
      "Epoch 5206/30000 Training Loss: 0.06721770763397217\n",
      "Epoch 5207/30000 Training Loss: 0.059346020221710205\n",
      "Epoch 5208/30000 Training Loss: 0.07527121156454086\n",
      "Epoch 5209/30000 Training Loss: 0.048167746514081955\n",
      "Epoch 5210/30000 Training Loss: 0.05708961561322212\n",
      "Epoch 5211/30000 Training Loss: 0.07382408529520035\n",
      "Epoch 5212/30000 Training Loss: 0.07811059802770615\n",
      "Epoch 5213/30000 Training Loss: 0.08372566103935242\n",
      "Epoch 5214/30000 Training Loss: 0.07783101499080658\n",
      "Epoch 5215/30000 Training Loss: 0.06094465032219887\n",
      "Epoch 5216/30000 Training Loss: 0.06427077949047089\n",
      "Epoch 5217/30000 Training Loss: 0.05976850539445877\n",
      "Epoch 5218/30000 Training Loss: 0.0537349171936512\n",
      "Epoch 5219/30000 Training Loss: 0.042669422924518585\n",
      "Epoch 5220/30000 Training Loss: 0.0868278294801712\n",
      "Epoch 5221/30000 Training Loss: 0.07859679311513901\n",
      "Epoch 5222/30000 Training Loss: 0.05216348543763161\n",
      "Epoch 5223/30000 Training Loss: 0.0542675144970417\n",
      "Epoch 5224/30000 Training Loss: 0.05746401101350784\n",
      "Epoch 5225/30000 Training Loss: 0.05259036645293236\n",
      "Epoch 5226/30000 Training Loss: 0.046926774084568024\n",
      "Epoch 5227/30000 Training Loss: 0.0451701283454895\n",
      "Epoch 5228/30000 Training Loss: 0.051013246178627014\n",
      "Epoch 5229/30000 Training Loss: 0.057573188096284866\n",
      "Epoch 5230/30000 Training Loss: 0.052916400134563446\n",
      "Epoch 5231/30000 Training Loss: 0.04422714561223984\n",
      "Epoch 5232/30000 Training Loss: 0.050785768777132034\n",
      "Epoch 5233/30000 Training Loss: 0.05430610850453377\n",
      "Epoch 5234/30000 Training Loss: 0.05528762564063072\n",
      "Epoch 5235/30000 Training Loss: 0.0673406645655632\n",
      "Epoch 5236/30000 Training Loss: 0.04749678820371628\n",
      "Epoch 5237/30000 Training Loss: 0.0680360272526741\n",
      "Epoch 5238/30000 Training Loss: 0.051415763795375824\n",
      "Epoch 5239/30000 Training Loss: 0.044042207300662994\n",
      "Epoch 5240/30000 Training Loss: 0.06380067765712738\n",
      "Epoch 5241/30000 Training Loss: 0.0563538633286953\n",
      "Epoch 5242/30000 Training Loss: 0.0594104565680027\n",
      "Epoch 5243/30000 Training Loss: 0.0751233622431755\n",
      "Epoch 5244/30000 Training Loss: 0.059178706258535385\n",
      "Epoch 5245/30000 Training Loss: 0.0597049817442894\n",
      "Epoch 5246/30000 Training Loss: 0.06643226742744446\n",
      "Epoch 5247/30000 Training Loss: 0.07297299057245255\n",
      "Epoch 5248/30000 Training Loss: 0.05418403074145317\n",
      "Epoch 5249/30000 Training Loss: 0.06221380829811096\n",
      "Epoch 5250/30000 Training Loss: 0.04844864457845688\n",
      "Epoch 5251/30000 Training Loss: 0.06925540417432785\n",
      "Epoch 5252/30000 Training Loss: 0.05368255078792572\n",
      "Epoch 5253/30000 Training Loss: 0.05812550336122513\n",
      "Epoch 5254/30000 Training Loss: 0.06958282738924026\n",
      "Epoch 5255/30000 Training Loss: 0.08223976194858551\n",
      "Epoch 5256/30000 Training Loss: 0.04704743251204491\n",
      "Epoch 5257/30000 Training Loss: 0.08179677277803421\n",
      "Epoch 5258/30000 Training Loss: 0.06176629662513733\n",
      "Epoch 5259/30000 Training Loss: 0.06069248542189598\n",
      "Epoch 5260/30000 Training Loss: 0.0611741729080677\n",
      "Epoch 5261/30000 Training Loss: 0.05503160133957863\n",
      "Epoch 5262/30000 Training Loss: 0.048650309443473816\n",
      "Epoch 5263/30000 Training Loss: 0.07118432223796844\n",
      "Epoch 5264/30000 Training Loss: 0.07637086510658264\n",
      "Epoch 5265/30000 Training Loss: 0.055078599601984024\n",
      "Epoch 5266/30000 Training Loss: 0.05591599643230438\n",
      "Epoch 5267/30000 Training Loss: 0.055386483669281006\n",
      "Epoch 5268/30000 Training Loss: 0.061730802059173584\n",
      "Epoch 5269/30000 Training Loss: 0.06512834131717682\n",
      "Epoch 5270/30000 Training Loss: 0.06608694046735764\n",
      "Epoch 5271/30000 Training Loss: 0.05934324115514755\n",
      "Epoch 5272/30000 Training Loss: 0.06831537187099457\n",
      "Epoch 5273/30000 Training Loss: 0.06917989253997803\n",
      "Epoch 5274/30000 Training Loss: 0.06903374940156937\n",
      "Epoch 5275/30000 Training Loss: 0.04485864192247391\n",
      "Epoch 5276/30000 Training Loss: 0.05425082892179489\n",
      "Epoch 5277/30000 Training Loss: 0.056466322392225266\n",
      "Epoch 5278/30000 Training Loss: 0.05799594521522522\n",
      "Epoch 5279/30000 Training Loss: 0.058710843324661255\n",
      "Epoch 5280/30000 Training Loss: 0.04751582816243172\n",
      "Epoch 5281/30000 Training Loss: 0.07337145507335663\n",
      "Epoch 5282/30000 Training Loss: 0.0732719898223877\n",
      "Epoch 5283/30000 Training Loss: 0.05359363183379173\n",
      "Epoch 5284/30000 Training Loss: 0.055034808814525604\n",
      "Epoch 5285/30000 Training Loss: 0.05632311850786209\n",
      "Epoch 5286/30000 Training Loss: 0.06529667973518372\n",
      "Epoch 5287/30000 Training Loss: 0.07920420914888382\n",
      "Epoch 5288/30000 Training Loss: 0.06236401945352554\n",
      "Epoch 5289/30000 Training Loss: 0.071560800075531\n",
      "Epoch 5290/30000 Training Loss: 0.04936361685395241\n",
      "Epoch 5291/30000 Training Loss: 0.06491561233997345\n",
      "Epoch 5292/30000 Training Loss: 0.06429167091846466\n",
      "Epoch 5293/30000 Training Loss: 0.07980276644229889\n",
      "Epoch 5294/30000 Training Loss: 0.053872499614953995\n",
      "Epoch 5295/30000 Training Loss: 0.05515572056174278\n",
      "Epoch 5296/30000 Training Loss: 0.05270537734031677\n",
      "Epoch 5297/30000 Training Loss: 0.06203415244817734\n",
      "Epoch 5298/30000 Training Loss: 0.057682350277900696\n",
      "Epoch 5299/30000 Training Loss: 0.06017815321683884\n",
      "Epoch 5300/30000 Training Loss: 0.0644436627626419\n",
      "Epoch 5300/30000 Validation Loss: 0.05324950069189072\n",
      "Epoch 5301/30000 Training Loss: 0.05398114025592804\n",
      "Epoch 5302/30000 Training Loss: 0.040304336696863174\n",
      "Epoch 5303/30000 Training Loss: 0.04124297946691513\n",
      "Epoch 5304/30000 Training Loss: 0.06734064221382141\n",
      "Epoch 5305/30000 Training Loss: 0.07007179409265518\n",
      "Epoch 5306/30000 Training Loss: 0.05836192145943642\n",
      "Epoch 5307/30000 Training Loss: 0.05582670867443085\n",
      "Epoch 5308/30000 Training Loss: 0.06352992355823517\n",
      "Epoch 5309/30000 Training Loss: 0.05310431495308876\n",
      "Epoch 5310/30000 Training Loss: 0.047858841717243195\n",
      "Epoch 5311/30000 Training Loss: 0.06462538987398148\n",
      "Epoch 5312/30000 Training Loss: 0.053128961473703384\n",
      "Epoch 5313/30000 Training Loss: 0.06724758446216583\n",
      "Epoch 5314/30000 Training Loss: 0.06446276605129242\n",
      "Epoch 5315/30000 Training Loss: 0.072071872651577\n",
      "Epoch 5316/30000 Training Loss: 0.05418979376554489\n",
      "Epoch 5317/30000 Training Loss: 0.06511859595775604\n",
      "Epoch 5318/30000 Training Loss: 0.06844061613082886\n",
      "Epoch 5319/30000 Training Loss: 0.056491103023290634\n",
      "Epoch 5320/30000 Training Loss: 0.05784694850444794\n",
      "Epoch 5321/30000 Training Loss: 0.06274785101413727\n",
      "Epoch 5322/30000 Training Loss: 0.07039516419172287\n",
      "Epoch 5323/30000 Training Loss: 0.0704449862241745\n",
      "Epoch 5324/30000 Training Loss: 0.05634598433971405\n",
      "Epoch 5325/30000 Training Loss: 0.056967802345752716\n",
      "Epoch 5326/30000 Training Loss: 0.056051842868328094\n",
      "Epoch 5327/30000 Training Loss: 0.07763989269733429\n",
      "Epoch 5328/30000 Training Loss: 0.07221329212188721\n",
      "Epoch 5329/30000 Training Loss: 0.05018293857574463\n",
      "Epoch 5330/30000 Training Loss: 0.06951172649860382\n",
      "Epoch 5331/30000 Training Loss: 0.05591313913464546\n",
      "Epoch 5332/30000 Training Loss: 0.06303344666957855\n",
      "Epoch 5333/30000 Training Loss: 0.052965737879276276\n",
      "Epoch 5334/30000 Training Loss: 0.06460525095462799\n",
      "Epoch 5335/30000 Training Loss: 0.047158513218164444\n",
      "Epoch 5336/30000 Training Loss: 0.0583573654294014\n",
      "Epoch 5337/30000 Training Loss: 0.057697877287864685\n",
      "Epoch 5338/30000 Training Loss: 0.0552176907658577\n",
      "Epoch 5339/30000 Training Loss: 0.055546630173921585\n",
      "Epoch 5340/30000 Training Loss: 0.04800029098987579\n",
      "Epoch 5341/30000 Training Loss: 0.059942640364170074\n",
      "Epoch 5342/30000 Training Loss: 0.08778858184814453\n",
      "Epoch 5343/30000 Training Loss: 0.06010090559720993\n",
      "Epoch 5344/30000 Training Loss: 0.05096820741891861\n",
      "Epoch 5345/30000 Training Loss: 0.06247926503419876\n",
      "Epoch 5346/30000 Training Loss: 0.06012466922402382\n",
      "Epoch 5347/30000 Training Loss: 0.06044793874025345\n",
      "Epoch 5348/30000 Training Loss: 0.06052496284246445\n",
      "Epoch 5349/30000 Training Loss: 0.08301214873790741\n",
      "Epoch 5350/30000 Training Loss: 0.06103704869747162\n",
      "Epoch 5351/30000 Training Loss: 0.0552678108215332\n",
      "Epoch 5352/30000 Training Loss: 0.05685199052095413\n",
      "Epoch 5353/30000 Training Loss: 0.059900205582380295\n",
      "Epoch 5354/30000 Training Loss: 0.06649336963891983\n",
      "Epoch 5355/30000 Training Loss: 0.04372720047831535\n",
      "Epoch 5356/30000 Training Loss: 0.07170992344617844\n",
      "Epoch 5357/30000 Training Loss: 0.061660006642341614\n",
      "Epoch 5358/30000 Training Loss: 0.0698588490486145\n",
      "Epoch 5359/30000 Training Loss: 0.06664392352104187\n",
      "Epoch 5360/30000 Training Loss: 0.0744626447558403\n",
      "Epoch 5361/30000 Training Loss: 0.07129678130149841\n",
      "Epoch 5362/30000 Training Loss: 0.06033830717206001\n",
      "Epoch 5363/30000 Training Loss: 0.07590539753437042\n",
      "Epoch 5364/30000 Training Loss: 0.06945135444402695\n",
      "Epoch 5365/30000 Training Loss: 0.06303444504737854\n",
      "Epoch 5366/30000 Training Loss: 0.059543631970882416\n",
      "Epoch 5367/30000 Training Loss: 0.061939164996147156\n",
      "Epoch 5368/30000 Training Loss: 0.057622987776994705\n",
      "Epoch 5369/30000 Training Loss: 0.04542947933077812\n",
      "Epoch 5370/30000 Training Loss: 0.07385941594839096\n",
      "Epoch 5371/30000 Training Loss: 0.06520216166973114\n",
      "Epoch 5372/30000 Training Loss: 0.049364835023880005\n",
      "Epoch 5373/30000 Training Loss: 0.05721951276063919\n",
      "Epoch 5374/30000 Training Loss: 0.06352660804986954\n",
      "Epoch 5375/30000 Training Loss: 0.04981116205453873\n",
      "Epoch 5376/30000 Training Loss: 0.055896908044815063\n",
      "Epoch 5377/30000 Training Loss: 0.056214332580566406\n",
      "Epoch 5378/30000 Training Loss: 0.05023100599646568\n",
      "Epoch 5379/30000 Training Loss: 0.07186482846736908\n",
      "Epoch 5380/30000 Training Loss: 0.04456331208348274\n",
      "Epoch 5381/30000 Training Loss: 0.0706017017364502\n",
      "Epoch 5382/30000 Training Loss: 0.0572715625166893\n",
      "Epoch 5383/30000 Training Loss: 0.04728621989488602\n",
      "Epoch 5384/30000 Training Loss: 0.053779520094394684\n",
      "Epoch 5385/30000 Training Loss: 0.07022645324468613\n",
      "Epoch 5386/30000 Training Loss: 0.05829301476478577\n",
      "Epoch 5387/30000 Training Loss: 0.061637647449970245\n",
      "Epoch 5388/30000 Training Loss: 0.05634516477584839\n",
      "Epoch 5389/30000 Training Loss: 0.06061716005206108\n",
      "Epoch 5390/30000 Training Loss: 0.06024343892931938\n",
      "Epoch 5391/30000 Training Loss: 0.06620793789625168\n",
      "Epoch 5392/30000 Training Loss: 0.04725568741559982\n",
      "Epoch 5393/30000 Training Loss: 0.05247321352362633\n",
      "Epoch 5394/30000 Training Loss: 0.05430715158581734\n",
      "Epoch 5395/30000 Training Loss: 0.07174129784107208\n",
      "Epoch 5396/30000 Training Loss: 0.06345237791538239\n",
      "Epoch 5397/30000 Training Loss: 0.06421595811843872\n",
      "Epoch 5398/30000 Training Loss: 0.06958210468292236\n",
      "Epoch 5399/30000 Training Loss: 0.05761609598994255\n",
      "Epoch 5400/30000 Training Loss: 0.04227796196937561\n",
      "Epoch 5400/30000 Validation Loss: 0.05432100594043732\n",
      "Epoch 5401/30000 Training Loss: 0.057836633175611496\n",
      "Epoch 5402/30000 Training Loss: 0.0669136494398117\n",
      "Epoch 5403/30000 Training Loss: 0.09107787907123566\n",
      "Epoch 5404/30000 Training Loss: 0.054034776985645294\n",
      "Epoch 5405/30000 Training Loss: 0.05519479885697365\n",
      "Epoch 5406/30000 Training Loss: 0.04393087700009346\n",
      "Epoch 5407/30000 Training Loss: 0.06133643165230751\n",
      "Epoch 5408/30000 Training Loss: 0.05758662149310112\n",
      "Epoch 5409/30000 Training Loss: 0.06851508468389511\n",
      "Epoch 5410/30000 Training Loss: 0.04827163740992546\n",
      "Epoch 5411/30000 Training Loss: 0.05261070653796196\n",
      "Epoch 5412/30000 Training Loss: 0.051018670201301575\n",
      "Epoch 5413/30000 Training Loss: 0.06776143610477448\n",
      "Epoch 5414/30000 Training Loss: 0.05492816120386124\n",
      "Epoch 5415/30000 Training Loss: 0.055898748338222504\n",
      "Epoch 5416/30000 Training Loss: 0.06539055705070496\n",
      "Epoch 5417/30000 Training Loss: 0.06954492628574371\n",
      "Epoch 5418/30000 Training Loss: 0.06354380398988724\n",
      "Epoch 5419/30000 Training Loss: 0.05761021748185158\n",
      "Epoch 5420/30000 Training Loss: 0.05823041498661041\n",
      "Epoch 5421/30000 Training Loss: 0.06062100827693939\n",
      "Epoch 5422/30000 Training Loss: 0.06245584785938263\n",
      "Epoch 5423/30000 Training Loss: 0.06009203940629959\n",
      "Epoch 5424/30000 Training Loss: 0.06131628155708313\n",
      "Epoch 5425/30000 Training Loss: 0.06453312933444977\n",
      "Epoch 5426/30000 Training Loss: 0.07112003117799759\n",
      "Epoch 5427/30000 Training Loss: 0.06356815993785858\n",
      "Epoch 5428/30000 Training Loss: 0.07137738168239594\n",
      "Epoch 5429/30000 Training Loss: 0.056790683418512344\n",
      "Epoch 5430/30000 Training Loss: 0.06909835338592529\n",
      "Epoch 5431/30000 Training Loss: 0.05599792301654816\n",
      "Epoch 5432/30000 Training Loss: 0.06463129818439484\n",
      "Epoch 5433/30000 Training Loss: 0.057622525840997696\n",
      "Epoch 5434/30000 Training Loss: 0.0686480775475502\n",
      "Epoch 5435/30000 Training Loss: 0.05481027066707611\n",
      "Epoch 5436/30000 Training Loss: 0.07855312526226044\n",
      "Epoch 5437/30000 Training Loss: 0.05976817011833191\n",
      "Epoch 5438/30000 Training Loss: 0.07759964466094971\n",
      "Epoch 5439/30000 Training Loss: 0.06803901493549347\n",
      "Epoch 5440/30000 Training Loss: 0.07948336005210876\n",
      "Epoch 5441/30000 Training Loss: 0.05972893536090851\n",
      "Epoch 5442/30000 Training Loss: 0.04755178838968277\n",
      "Epoch 5443/30000 Training Loss: 0.06229083985090256\n",
      "Epoch 5444/30000 Training Loss: 0.07261448353528976\n",
      "Epoch 5445/30000 Training Loss: 0.0655466690659523\n",
      "Epoch 5446/30000 Training Loss: 0.08082801103591919\n",
      "Epoch 5447/30000 Training Loss: 0.06119539588689804\n",
      "Epoch 5448/30000 Training Loss: 0.05951037257909775\n",
      "Epoch 5449/30000 Training Loss: 0.06234727054834366\n",
      "Epoch 5450/30000 Training Loss: 0.04893365874886513\n",
      "Epoch 5451/30000 Training Loss: 0.05282928794622421\n",
      "Epoch 5452/30000 Training Loss: 0.042509399354457855\n",
      "Epoch 5453/30000 Training Loss: 0.07259970903396606\n",
      "Epoch 5454/30000 Training Loss: 0.05194960534572601\n",
      "Epoch 5455/30000 Training Loss: 0.0668363869190216\n",
      "Epoch 5456/30000 Training Loss: 0.058215558528900146\n",
      "Epoch 5457/30000 Training Loss: 0.06273248046636581\n",
      "Epoch 5458/30000 Training Loss: 0.05308686941862106\n",
      "Epoch 5459/30000 Training Loss: 0.06467131525278091\n",
      "Epoch 5460/30000 Training Loss: 0.07653100043535233\n",
      "Epoch 5461/30000 Training Loss: 0.050943825393915176\n",
      "Epoch 5462/30000 Training Loss: 0.045080482959747314\n",
      "Epoch 5463/30000 Training Loss: 0.05581557750701904\n",
      "Epoch 5464/30000 Training Loss: 0.07078727334737778\n",
      "Epoch 5465/30000 Training Loss: 0.0659208595752716\n",
      "Epoch 5466/30000 Training Loss: 0.07928576320409775\n",
      "Epoch 5467/30000 Training Loss: 0.06641504168510437\n",
      "Epoch 5468/30000 Training Loss: 0.05763227120041847\n",
      "Epoch 5469/30000 Training Loss: 0.07529693841934204\n",
      "Epoch 5470/30000 Training Loss: 0.05764799565076828\n",
      "Epoch 5471/30000 Training Loss: 0.06506140530109406\n",
      "Epoch 5472/30000 Training Loss: 0.06234237551689148\n",
      "Epoch 5473/30000 Training Loss: 0.0665096566081047\n",
      "Epoch 5474/30000 Training Loss: 0.0531773567199707\n",
      "Epoch 5475/30000 Training Loss: 0.06773821264505386\n",
      "Epoch 5476/30000 Training Loss: 0.06474027037620544\n",
      "Epoch 5477/30000 Training Loss: 0.06801773607730865\n",
      "Epoch 5478/30000 Training Loss: 0.058800213038921356\n",
      "Epoch 5479/30000 Training Loss: 0.07272292673587799\n",
      "Epoch 5480/30000 Training Loss: 0.0699092447757721\n",
      "Epoch 5481/30000 Training Loss: 0.07950405031442642\n",
      "Epoch 5482/30000 Training Loss: 0.05690867826342583\n",
      "Epoch 5483/30000 Training Loss: 0.05582914501428604\n",
      "Epoch 5484/30000 Training Loss: 0.05534377694129944\n",
      "Epoch 5485/30000 Training Loss: 0.05967278033494949\n",
      "Epoch 5486/30000 Training Loss: 0.06614825129508972\n",
      "Epoch 5487/30000 Training Loss: 0.07444676011800766\n",
      "Epoch 5488/30000 Training Loss: 0.04677489399909973\n",
      "Epoch 5489/30000 Training Loss: 0.05575063079595566\n",
      "Epoch 5490/30000 Training Loss: 0.052910126745700836\n",
      "Epoch 5491/30000 Training Loss: 0.0605328269302845\n",
      "Epoch 5492/30000 Training Loss: 0.06796588748693466\n",
      "Epoch 5493/30000 Training Loss: 0.046572379767894745\n",
      "Epoch 5494/30000 Training Loss: 0.07795578986406326\n",
      "Epoch 5495/30000 Training Loss: 0.06552518904209137\n",
      "Epoch 5496/30000 Training Loss: 0.05003111809492111\n",
      "Epoch 5497/30000 Training Loss: 0.05917388200759888\n",
      "Epoch 5498/30000 Training Loss: 0.06538313627243042\n",
      "Epoch 5499/30000 Training Loss: 0.07191752642393112\n",
      "Epoch 5500/30000 Training Loss: 0.0843643993139267\n",
      "Epoch 5500/30000 Validation Loss: 0.06919627636671066\n",
      "Epoch 5501/30000 Training Loss: 0.06259450316429138\n",
      "Epoch 5502/30000 Training Loss: 0.06527189910411835\n",
      "Epoch 5503/30000 Training Loss: 0.0706544890999794\n",
      "Epoch 5504/30000 Training Loss: 0.05830508470535278\n",
      "Epoch 5505/30000 Training Loss: 0.0694742277264595\n",
      "Epoch 5506/30000 Training Loss: 0.07446730881929398\n",
      "Epoch 5507/30000 Training Loss: 0.05875490605831146\n",
      "Epoch 5508/30000 Training Loss: 0.06692487001419067\n",
      "Epoch 5509/30000 Training Loss: 0.052963919937610626\n",
      "Epoch 5510/30000 Training Loss: 0.06795231252908707\n",
      "Epoch 5511/30000 Training Loss: 0.05762432888150215\n",
      "Epoch 5512/30000 Training Loss: 0.06020074710249901\n",
      "Epoch 5513/30000 Training Loss: 0.07386746257543564\n",
      "Epoch 5514/30000 Training Loss: 0.054697830229997635\n",
      "Epoch 5515/30000 Training Loss: 0.056191232055425644\n",
      "Epoch 5516/30000 Training Loss: 0.06844443827867508\n",
      "Epoch 5517/30000 Training Loss: 0.05796968936920166\n",
      "Epoch 5518/30000 Training Loss: 0.05939871817827225\n",
      "Epoch 5519/30000 Training Loss: 0.09315283596515656\n",
      "Epoch 5520/30000 Training Loss: 0.047054894268512726\n",
      "Epoch 5521/30000 Training Loss: 0.0693143978714943\n",
      "Epoch 5522/30000 Training Loss: 0.0638260766863823\n",
      "Epoch 5523/30000 Training Loss: 0.05101758614182472\n",
      "Epoch 5524/30000 Training Loss: 0.07022710144519806\n",
      "Epoch 5525/30000 Training Loss: 0.05620808154344559\n",
      "Epoch 5526/30000 Training Loss: 0.06962096691131592\n",
      "Epoch 5527/30000 Training Loss: 0.09344233572483063\n",
      "Epoch 5528/30000 Training Loss: 0.05987364053726196\n",
      "Epoch 5529/30000 Training Loss: 0.045076142996549606\n",
      "Epoch 5530/30000 Training Loss: 0.044694073498249054\n",
      "Epoch 5531/30000 Training Loss: 0.05648435652256012\n",
      "Epoch 5532/30000 Training Loss: 0.05299384891986847\n",
      "Epoch 5533/30000 Training Loss: 0.05748802796006203\n",
      "Epoch 5534/30000 Training Loss: 0.06235528737306595\n",
      "Epoch 5535/30000 Training Loss: 0.05898347869515419\n",
      "Epoch 5536/30000 Training Loss: 0.05086700618267059\n",
      "Epoch 5537/30000 Training Loss: 0.05738338083028793\n",
      "Epoch 5538/30000 Training Loss: 0.049473460763692856\n",
      "Epoch 5539/30000 Training Loss: 0.0659339651465416\n",
      "Epoch 5540/30000 Training Loss: 0.07347346097230911\n",
      "Epoch 5541/30000 Training Loss: 0.052375953644514084\n",
      "Epoch 5542/30000 Training Loss: 0.062402307987213135\n",
      "Epoch 5543/30000 Training Loss: 0.0654008686542511\n",
      "Epoch 5544/30000 Training Loss: 0.055438071489334106\n",
      "Epoch 5545/30000 Training Loss: 0.05065695941448212\n",
      "Epoch 5546/30000 Training Loss: 0.10726085305213928\n",
      "Epoch 5547/30000 Training Loss: 0.065640389919281\n",
      "Epoch 5548/30000 Training Loss: 0.06376000493764877\n",
      "Epoch 5549/30000 Training Loss: 0.07410890609025955\n",
      "Epoch 5550/30000 Training Loss: 0.06852167844772339\n",
      "Epoch 5551/30000 Training Loss: 0.057639628648757935\n",
      "Epoch 5552/30000 Training Loss: 0.07148218154907227\n",
      "Epoch 5553/30000 Training Loss: 0.08481491357088089\n",
      "Epoch 5554/30000 Training Loss: 0.06502552330493927\n",
      "Epoch 5555/30000 Training Loss: 0.04988449811935425\n",
      "Epoch 5556/30000 Training Loss: 0.0476851761341095\n",
      "Epoch 5557/30000 Training Loss: 0.051512137055397034\n",
      "Epoch 5558/30000 Training Loss: 0.048609908670186996\n",
      "Epoch 5559/30000 Training Loss: 0.06716374307870865\n",
      "Epoch 5560/30000 Training Loss: 0.06194950267672539\n",
      "Epoch 5561/30000 Training Loss: 0.07431460916996002\n",
      "Epoch 5562/30000 Training Loss: 0.06729783117771149\n",
      "Epoch 5563/30000 Training Loss: 0.05271638184785843\n",
      "Epoch 5564/30000 Training Loss: 0.06568530201911926\n",
      "Epoch 5565/30000 Training Loss: 0.04902807995676994\n",
      "Epoch 5566/30000 Training Loss: 0.04941302537918091\n",
      "Epoch 5567/30000 Training Loss: 0.06228313967585564\n",
      "Epoch 5568/30000 Training Loss: 0.07534889876842499\n",
      "Epoch 5569/30000 Training Loss: 0.06504344940185547\n",
      "Epoch 5570/30000 Training Loss: 0.06213614344596863\n",
      "Epoch 5571/30000 Training Loss: 0.046426158398389816\n",
      "Epoch 5572/30000 Training Loss: 0.06061846762895584\n",
      "Epoch 5573/30000 Training Loss: 0.05892091616988182\n",
      "Epoch 5574/30000 Training Loss: 0.05791289359331131\n",
      "Epoch 5575/30000 Training Loss: 0.05774718523025513\n",
      "Epoch 5576/30000 Training Loss: 0.062275439500808716\n",
      "Epoch 5577/30000 Training Loss: 0.07445603609085083\n",
      "Epoch 5578/30000 Training Loss: 0.05594851076602936\n",
      "Epoch 5579/30000 Training Loss: 0.04720475524663925\n",
      "Epoch 5580/30000 Training Loss: 0.07692365348339081\n",
      "Epoch 5581/30000 Training Loss: 0.07249905914068222\n",
      "Epoch 5582/30000 Training Loss: 0.06753063946962357\n",
      "Epoch 5583/30000 Training Loss: 0.06925038993358612\n",
      "Epoch 5584/30000 Training Loss: 0.0809503048658371\n",
      "Epoch 5585/30000 Training Loss: 0.05878550559282303\n",
      "Epoch 5586/30000 Training Loss: 0.044700972735881805\n",
      "Epoch 5587/30000 Training Loss: 0.05835213512182236\n",
      "Epoch 5588/30000 Training Loss: 0.060816533863544464\n",
      "Epoch 5589/30000 Training Loss: 0.07668837904930115\n",
      "Epoch 5590/30000 Training Loss: 0.06985495239496231\n",
      "Epoch 5591/30000 Training Loss: 0.055588435381650925\n",
      "Epoch 5592/30000 Training Loss: 0.05961756408214569\n",
      "Epoch 5593/30000 Training Loss: 0.05013531446456909\n",
      "Epoch 5594/30000 Training Loss: 0.049375735223293304\n",
      "Epoch 5595/30000 Training Loss: 0.06577298045158386\n",
      "Epoch 5596/30000 Training Loss: 0.057264380156993866\n",
      "Epoch 5597/30000 Training Loss: 0.06267845630645752\n",
      "Epoch 5598/30000 Training Loss: 0.05489763617515564\n",
      "Epoch 5599/30000 Training Loss: 0.05271082744002342\n",
      "Epoch 5600/30000 Training Loss: 0.04823778569698334\n",
      "Epoch 5600/30000 Validation Loss: 0.05700140818953514\n",
      "Epoch 5601/30000 Training Loss: 0.0704411044716835\n",
      "Epoch 5602/30000 Training Loss: 0.04570956528186798\n",
      "Epoch 5603/30000 Training Loss: 0.05373188853263855\n",
      "Epoch 5604/30000 Training Loss: 0.06316342204809189\n",
      "Epoch 5605/30000 Training Loss: 0.07107548415660858\n",
      "Epoch 5606/30000 Training Loss: 0.04897254705429077\n",
      "Epoch 5607/30000 Training Loss: 0.06676416099071503\n",
      "Epoch 5608/30000 Training Loss: 0.07876147329807281\n",
      "Epoch 5609/30000 Training Loss: 0.07614756375551224\n",
      "Epoch 5610/30000 Training Loss: 0.049495480954647064\n",
      "Epoch 5611/30000 Training Loss: 0.055124431848526\n",
      "Epoch 5612/30000 Training Loss: 0.05084271728992462\n",
      "Epoch 5613/30000 Training Loss: 0.07206743955612183\n",
      "Epoch 5614/30000 Training Loss: 0.05826675519347191\n",
      "Epoch 5615/30000 Training Loss: 0.041536521166563034\n",
      "Epoch 5616/30000 Training Loss: 0.06393580883741379\n",
      "Epoch 5617/30000 Training Loss: 0.0711727887392044\n",
      "Epoch 5618/30000 Training Loss: 0.05140848830342293\n",
      "Epoch 5619/30000 Training Loss: 0.04906950145959854\n",
      "Epoch 5620/30000 Training Loss: 0.06816311180591583\n",
      "Epoch 5621/30000 Training Loss: 0.07099410146474838\n",
      "Epoch 5622/30000 Training Loss: 0.057778388261795044\n",
      "Epoch 5623/30000 Training Loss: 0.0823572501540184\n",
      "Epoch 5624/30000 Training Loss: 0.05524906516075134\n",
      "Epoch 5625/30000 Training Loss: 0.05174744874238968\n",
      "Epoch 5626/30000 Training Loss: 0.053429704159498215\n",
      "Epoch 5627/30000 Training Loss: 0.05376119166612625\n",
      "Epoch 5628/30000 Training Loss: 0.054915010929107666\n",
      "Epoch 5629/30000 Training Loss: 0.06213655322790146\n",
      "Epoch 5630/30000 Training Loss: 0.06818000227212906\n",
      "Epoch 5631/30000 Training Loss: 0.0631459429860115\n",
      "Epoch 5632/30000 Training Loss: 0.05404815077781677\n",
      "Epoch 5633/30000 Training Loss: 0.06269105523824692\n",
      "Epoch 5634/30000 Training Loss: 0.07136792689561844\n",
      "Epoch 5635/30000 Training Loss: 0.06868745386600494\n",
      "Epoch 5636/30000 Training Loss: 0.0781339630484581\n",
      "Epoch 5637/30000 Training Loss: 0.07742166519165039\n",
      "Epoch 5638/30000 Training Loss: 0.06303148716688156\n",
      "Epoch 5639/30000 Training Loss: 0.04423941671848297\n",
      "Epoch 5640/30000 Training Loss: 0.05232319235801697\n",
      "Epoch 5641/30000 Training Loss: 0.0644107460975647\n",
      "Epoch 5642/30000 Training Loss: 0.0449640229344368\n",
      "Epoch 5643/30000 Training Loss: 0.05602756142616272\n",
      "Epoch 5644/30000 Training Loss: 0.06495355069637299\n",
      "Epoch 5645/30000 Training Loss: 0.04601377621293068\n",
      "Epoch 5646/30000 Training Loss: 0.07053279131650925\n",
      "Epoch 5647/30000 Training Loss: 0.056597478687763214\n",
      "Epoch 5648/30000 Training Loss: 0.05598778277635574\n",
      "Epoch 5649/30000 Training Loss: 0.05421024188399315\n",
      "Epoch 5650/30000 Training Loss: 0.04948597028851509\n",
      "Epoch 5651/30000 Training Loss: 0.07291670888662338\n",
      "Epoch 5652/30000 Training Loss: 0.04888894408941269\n",
      "Epoch 5653/30000 Training Loss: 0.05784417316317558\n",
      "Epoch 5654/30000 Training Loss: 0.06574263423681259\n",
      "Epoch 5655/30000 Training Loss: 0.06434129178524017\n",
      "Epoch 5656/30000 Training Loss: 0.06969767808914185\n",
      "Epoch 5657/30000 Training Loss: 0.0584518164396286\n",
      "Epoch 5658/30000 Training Loss: 0.055599309504032135\n",
      "Epoch 5659/30000 Training Loss: 0.06846156716346741\n",
      "Epoch 5660/30000 Training Loss: 0.056923095136880875\n",
      "Epoch 5661/30000 Training Loss: 0.06363525986671448\n",
      "Epoch 5662/30000 Training Loss: 0.05210734158754349\n",
      "Epoch 5663/30000 Training Loss: 0.05599043518304825\n",
      "Epoch 5664/30000 Training Loss: 0.06090094521641731\n",
      "Epoch 5665/30000 Training Loss: 0.051494669169187546\n",
      "Epoch 5666/30000 Training Loss: 0.05771606042981148\n",
      "Epoch 5667/30000 Training Loss: 0.058926455676555634\n",
      "Epoch 5668/30000 Training Loss: 0.07306807488203049\n",
      "Epoch 5669/30000 Training Loss: 0.06032969057559967\n",
      "Epoch 5670/30000 Training Loss: 0.05571300536394119\n",
      "Epoch 5671/30000 Training Loss: 0.04421480372548103\n",
      "Epoch 5672/30000 Training Loss: 0.053412098437547684\n",
      "Epoch 5673/30000 Training Loss: 0.050592757761478424\n",
      "Epoch 5674/30000 Training Loss: 0.06371363997459412\n",
      "Epoch 5675/30000 Training Loss: 0.05787745863199234\n",
      "Epoch 5676/30000 Training Loss: 0.067074254155159\n",
      "Epoch 5677/30000 Training Loss: 0.058618515729904175\n",
      "Epoch 5678/30000 Training Loss: 0.05162501707673073\n",
      "Epoch 5679/30000 Training Loss: 0.0757729709148407\n",
      "Epoch 5680/30000 Training Loss: 0.04989708960056305\n",
      "Epoch 5681/30000 Training Loss: 0.06561774015426636\n",
      "Epoch 5682/30000 Training Loss: 0.04700693115592003\n",
      "Epoch 5683/30000 Training Loss: 0.05456356704235077\n",
      "Epoch 5684/30000 Training Loss: 0.0598766952753067\n",
      "Epoch 5685/30000 Training Loss: 0.07661953568458557\n",
      "Epoch 5686/30000 Training Loss: 0.061900630593299866\n",
      "Epoch 5687/30000 Training Loss: 0.05354762449860573\n",
      "Epoch 5688/30000 Training Loss: 0.06434047222137451\n",
      "Epoch 5689/30000 Training Loss: 0.05118302255868912\n",
      "Epoch 5690/30000 Training Loss: 0.053584448993206024\n",
      "Epoch 5691/30000 Training Loss: 0.05723661184310913\n",
      "Epoch 5692/30000 Training Loss: 0.06668971478939056\n",
      "Epoch 5693/30000 Training Loss: 0.05872354283928871\n",
      "Epoch 5694/30000 Training Loss: 0.06448279321193695\n",
      "Epoch 5695/30000 Training Loss: 0.08209893852472305\n",
      "Epoch 5696/30000 Training Loss: 0.07051463425159454\n",
      "Epoch 5697/30000 Training Loss: 0.07695072144269943\n",
      "Epoch 5698/30000 Training Loss: 0.07446747273206711\n",
      "Epoch 5699/30000 Training Loss: 0.06660705804824829\n",
      "Epoch 5700/30000 Training Loss: 0.05130838602781296\n",
      "Epoch 5700/30000 Validation Loss: 0.07371532917022705\n",
      "Epoch 5701/30000 Training Loss: 0.05092017725110054\n",
      "Epoch 5702/30000 Training Loss: 0.04638174921274185\n",
      "Epoch 5703/30000 Training Loss: 0.06653088331222534\n",
      "Epoch 5704/30000 Training Loss: 0.05420695245265961\n",
      "Epoch 5705/30000 Training Loss: 0.07732349634170532\n",
      "Epoch 5706/30000 Training Loss: 0.0661693662405014\n",
      "Epoch 5707/30000 Training Loss: 0.06775529682636261\n",
      "Epoch 5708/30000 Training Loss: 0.0680677518248558\n",
      "Epoch 5709/30000 Training Loss: 0.04167507588863373\n",
      "Epoch 5710/30000 Training Loss: 0.05155656486749649\n",
      "Epoch 5711/30000 Training Loss: 0.07212722301483154\n",
      "Epoch 5712/30000 Training Loss: 0.0582907609641552\n",
      "Epoch 5713/30000 Training Loss: 0.06181948632001877\n",
      "Epoch 5714/30000 Training Loss: 0.07202912122011185\n",
      "Epoch 5715/30000 Training Loss: 0.0630650594830513\n",
      "Epoch 5716/30000 Training Loss: 0.06538917124271393\n",
      "Epoch 5717/30000 Training Loss: 0.07886820286512375\n",
      "Epoch 5718/30000 Training Loss: 0.042762480676174164\n",
      "Epoch 5719/30000 Training Loss: 0.05336104705929756\n",
      "Epoch 5720/30000 Training Loss: 0.047993823885917664\n",
      "Epoch 5721/30000 Training Loss: 0.0644533559679985\n",
      "Epoch 5722/30000 Training Loss: 0.05646044388413429\n",
      "Epoch 5723/30000 Training Loss: 0.06255495548248291\n",
      "Epoch 5724/30000 Training Loss: 0.053293704986572266\n",
      "Epoch 5725/30000 Training Loss: 0.0564776211977005\n",
      "Epoch 5726/30000 Training Loss: 0.05885806307196617\n",
      "Epoch 5727/30000 Training Loss: 0.06885609030723572\n",
      "Epoch 5728/30000 Training Loss: 0.058910757303237915\n",
      "Epoch 5729/30000 Training Loss: 0.052772458642721176\n",
      "Epoch 5730/30000 Training Loss: 0.055049821734428406\n",
      "Epoch 5731/30000 Training Loss: 0.05531173571944237\n",
      "Epoch 5732/30000 Training Loss: 0.058113958686590195\n",
      "Epoch 5733/30000 Training Loss: 0.06954873353242874\n",
      "Epoch 5734/30000 Training Loss: 0.06790712475776672\n",
      "Epoch 5735/30000 Training Loss: 0.05944877117872238\n",
      "Epoch 5736/30000 Training Loss: 0.06333612650632858\n",
      "Epoch 5737/30000 Training Loss: 0.07452574372291565\n",
      "Epoch 5738/30000 Training Loss: 0.06002705171704292\n",
      "Epoch 5739/30000 Training Loss: 0.052293144166469574\n",
      "Epoch 5740/30000 Training Loss: 0.0640837550163269\n",
      "Epoch 5741/30000 Training Loss: 0.05714860558509827\n",
      "Epoch 5742/30000 Training Loss: 0.05528755486011505\n",
      "Epoch 5743/30000 Training Loss: 0.07007214426994324\n",
      "Epoch 5744/30000 Training Loss: 0.05969894304871559\n",
      "Epoch 5745/30000 Training Loss: 0.06478040665388107\n",
      "Epoch 5746/30000 Training Loss: 0.059734828770160675\n",
      "Epoch 5747/30000 Training Loss: 0.04521821811795235\n",
      "Epoch 5748/30000 Training Loss: 0.04820055142045021\n",
      "Epoch 5749/30000 Training Loss: 0.05980326607823372\n",
      "Epoch 5750/30000 Training Loss: 0.07448334246873856\n",
      "Epoch 5751/30000 Training Loss: 0.07588346302509308\n",
      "Epoch 5752/30000 Training Loss: 0.054069481790065765\n",
      "Epoch 5753/30000 Training Loss: 0.07598050683736801\n",
      "Epoch 5754/30000 Training Loss: 0.053599726408720016\n",
      "Epoch 5755/30000 Training Loss: 0.0563293918967247\n",
      "Epoch 5756/30000 Training Loss: 0.05361974984407425\n",
      "Epoch 5757/30000 Training Loss: 0.060993440449237823\n",
      "Epoch 5758/30000 Training Loss: 0.05019938573241234\n",
      "Epoch 5759/30000 Training Loss: 0.06306478381156921\n",
      "Epoch 5760/30000 Training Loss: 0.061017002910375595\n",
      "Epoch 5761/30000 Training Loss: 0.052746325731277466\n",
      "Epoch 5762/30000 Training Loss: 0.06458114832639694\n",
      "Epoch 5763/30000 Training Loss: 0.05748768895864487\n",
      "Epoch 5764/30000 Training Loss: 0.06253612786531448\n",
      "Epoch 5765/30000 Training Loss: 0.09460162371397018\n",
      "Epoch 5766/30000 Training Loss: 0.0531449131667614\n",
      "Epoch 5767/30000 Training Loss: 0.05829637125134468\n",
      "Epoch 5768/30000 Training Loss: 0.06124233826994896\n",
      "Epoch 5769/30000 Training Loss: 0.07495533674955368\n",
      "Epoch 5770/30000 Training Loss: 0.05724192410707474\n",
      "Epoch 5771/30000 Training Loss: 0.05048518627882004\n",
      "Epoch 5772/30000 Training Loss: 0.05749615281820297\n",
      "Epoch 5773/30000 Training Loss: 0.04780210182070732\n",
      "Epoch 5774/30000 Training Loss: 0.042327359318733215\n",
      "Epoch 5775/30000 Training Loss: 0.060989972203969955\n",
      "Epoch 5776/30000 Training Loss: 0.06465815007686615\n",
      "Epoch 5777/30000 Training Loss: 0.04941987246274948\n",
      "Epoch 5778/30000 Training Loss: 0.05493012070655823\n",
      "Epoch 5779/30000 Training Loss: 0.0780973881483078\n",
      "Epoch 5780/30000 Training Loss: 0.049663007259368896\n",
      "Epoch 5781/30000 Training Loss: 0.06540215015411377\n",
      "Epoch 5782/30000 Training Loss: 0.060705386102199554\n",
      "Epoch 5783/30000 Training Loss: 0.0625135526061058\n",
      "Epoch 5784/30000 Training Loss: 0.04508620873093605\n",
      "Epoch 5785/30000 Training Loss: 0.05395375192165375\n",
      "Epoch 5786/30000 Training Loss: 0.06728724390268326\n",
      "Epoch 5787/30000 Training Loss: 0.061888933181762695\n",
      "Epoch 5788/30000 Training Loss: 0.05068812146782875\n",
      "Epoch 5789/30000 Training Loss: 0.07160554826259613\n",
      "Epoch 5790/30000 Training Loss: 0.05947296321392059\n",
      "Epoch 5791/30000 Training Loss: 0.06598996371030807\n",
      "Epoch 5792/30000 Training Loss: 0.0619686022400856\n",
      "Epoch 5793/30000 Training Loss: 0.06199898570775986\n",
      "Epoch 5794/30000 Training Loss: 0.06742758303880692\n",
      "Epoch 5795/30000 Training Loss: 0.05599716305732727\n",
      "Epoch 5796/30000 Training Loss: 0.04912155866622925\n",
      "Epoch 5797/30000 Training Loss: 0.06631393730640411\n",
      "Epoch 5798/30000 Training Loss: 0.05792723968625069\n",
      "Epoch 5799/30000 Training Loss: 0.05799116939306259\n",
      "Epoch 5800/30000 Training Loss: 0.07598257809877396\n",
      "Epoch 5800/30000 Validation Loss: 0.06865015625953674\n",
      "Epoch 5801/30000 Training Loss: 0.050025809556245804\n",
      "Epoch 5802/30000 Training Loss: 0.05099228024482727\n",
      "Epoch 5803/30000 Training Loss: 0.05945407599210739\n",
      "Epoch 5804/30000 Training Loss: 0.056786470115184784\n",
      "Epoch 5805/30000 Training Loss: 0.04995580017566681\n",
      "Epoch 5806/30000 Training Loss: 0.06249387189745903\n",
      "Epoch 5807/30000 Training Loss: 0.07273959368467331\n",
      "Epoch 5808/30000 Training Loss: 0.06353333592414856\n",
      "Epoch 5809/30000 Training Loss: 0.0604022815823555\n",
      "Epoch 5810/30000 Training Loss: 0.05682779848575592\n",
      "Epoch 5811/30000 Training Loss: 0.07320280373096466\n",
      "Epoch 5812/30000 Training Loss: 0.05257152393460274\n",
      "Epoch 5813/30000 Training Loss: 0.07026565819978714\n",
      "Epoch 5814/30000 Training Loss: 0.06343931704759598\n",
      "Epoch 5815/30000 Training Loss: 0.06698904186487198\n",
      "Epoch 5816/30000 Training Loss: 0.06883004307746887\n",
      "Epoch 5817/30000 Training Loss: 0.05009046569466591\n",
      "Epoch 5818/30000 Training Loss: 0.07137329876422882\n",
      "Epoch 5819/30000 Training Loss: 0.055182620882987976\n",
      "Epoch 5820/30000 Training Loss: 0.058342162519693375\n",
      "Epoch 5821/30000 Training Loss: 0.0618143156170845\n",
      "Epoch 5822/30000 Training Loss: 0.05193440616130829\n",
      "Epoch 5823/30000 Training Loss: 0.05531644821166992\n",
      "Epoch 5824/30000 Training Loss: 0.07334139943122864\n",
      "Epoch 5825/30000 Training Loss: 0.07718247175216675\n",
      "Epoch 5826/30000 Training Loss: 0.06180102378129959\n",
      "Epoch 5827/30000 Training Loss: 0.06396276503801346\n",
      "Epoch 5828/30000 Training Loss: 0.06955312192440033\n",
      "Epoch 5829/30000 Training Loss: 0.05525314435362816\n",
      "Epoch 5830/30000 Training Loss: 0.053401023149490356\n",
      "Epoch 5831/30000 Training Loss: 0.0548945814371109\n",
      "Epoch 5832/30000 Training Loss: 0.04869925230741501\n",
      "Epoch 5833/30000 Training Loss: 0.06597734242677689\n",
      "Epoch 5834/30000 Training Loss: 0.05756010860204697\n",
      "Epoch 5835/30000 Training Loss: 0.06122598052024841\n",
      "Epoch 5836/30000 Training Loss: 0.05532893165946007\n",
      "Epoch 5837/30000 Training Loss: 0.06497881561517715\n",
      "Epoch 5838/30000 Training Loss: 0.05236242711544037\n",
      "Epoch 5839/30000 Training Loss: 0.06377961486577988\n",
      "Epoch 5840/30000 Training Loss: 0.05457450449466705\n",
      "Epoch 5841/30000 Training Loss: 0.04340198636054993\n",
      "Epoch 5842/30000 Training Loss: 0.06714759767055511\n",
      "Epoch 5843/30000 Training Loss: 0.07818587124347687\n",
      "Epoch 5844/30000 Training Loss: 0.050411123782396317\n",
      "Epoch 5845/30000 Training Loss: 0.04635249078273773\n",
      "Epoch 5846/30000 Training Loss: 0.04010111838579178\n",
      "Epoch 5847/30000 Training Loss: 0.08218228816986084\n",
      "Epoch 5848/30000 Training Loss: 0.058414362370967865\n",
      "Epoch 5849/30000 Training Loss: 0.06991025060415268\n",
      "Epoch 5850/30000 Training Loss: 0.07065093517303467\n",
      "Epoch 5851/30000 Training Loss: 0.050896529108285904\n",
      "Epoch 5852/30000 Training Loss: 0.0590088851749897\n",
      "Epoch 5853/30000 Training Loss: 0.045334476977586746\n",
      "Epoch 5854/30000 Training Loss: 0.061768628656864166\n",
      "Epoch 5855/30000 Training Loss: 0.06474144011735916\n",
      "Epoch 5856/30000 Training Loss: 0.05931518226861954\n",
      "Epoch 5857/30000 Training Loss: 0.05833842232823372\n",
      "Epoch 5858/30000 Training Loss: 0.06975226104259491\n",
      "Epoch 5859/30000 Training Loss: 0.05663894861936569\n",
      "Epoch 5860/30000 Training Loss: 0.0695355162024498\n",
      "Epoch 5861/30000 Training Loss: 0.06563600897789001\n",
      "Epoch 5862/30000 Training Loss: 0.06234949454665184\n",
      "Epoch 5863/30000 Training Loss: 0.06579330563545227\n",
      "Epoch 5864/30000 Training Loss: 0.05533663555979729\n",
      "Epoch 5865/30000 Training Loss: 0.0474592000246048\n",
      "Epoch 5866/30000 Training Loss: 0.06650185585021973\n",
      "Epoch 5867/30000 Training Loss: 0.057002387940883636\n",
      "Epoch 5868/30000 Training Loss: 0.06667771935462952\n",
      "Epoch 5869/30000 Training Loss: 0.062490612268447876\n",
      "Epoch 5870/30000 Training Loss: 0.06470388174057007\n",
      "Epoch 5871/30000 Training Loss: 0.05336600914597511\n",
      "Epoch 5872/30000 Training Loss: 0.08293484896421432\n",
      "Epoch 5873/30000 Training Loss: 0.057830922305583954\n",
      "Epoch 5874/30000 Training Loss: 0.05351317301392555\n",
      "Epoch 5875/30000 Training Loss: 0.06296226382255554\n",
      "Epoch 5876/30000 Training Loss: 0.05055290460586548\n",
      "Epoch 5877/30000 Training Loss: 0.0616224929690361\n",
      "Epoch 5878/30000 Training Loss: 0.03961879014968872\n",
      "Epoch 5879/30000 Training Loss: 0.044979654252529144\n",
      "Epoch 5880/30000 Training Loss: 0.04738730937242508\n",
      "Epoch 5881/30000 Training Loss: 0.06558249145746231\n",
      "Epoch 5882/30000 Training Loss: 0.06027473509311676\n",
      "Epoch 5883/30000 Training Loss: 0.060199104249477386\n",
      "Epoch 5884/30000 Training Loss: 0.06724824756383896\n",
      "Epoch 5885/30000 Training Loss: 0.045383553951978683\n",
      "Epoch 5886/30000 Training Loss: 0.04540363699197769\n",
      "Epoch 5887/30000 Training Loss: 0.09308595210313797\n",
      "Epoch 5888/30000 Training Loss: 0.06095077842473984\n",
      "Epoch 5889/30000 Training Loss: 0.07618686556816101\n",
      "Epoch 5890/30000 Training Loss: 0.06441669911146164\n",
      "Epoch 5891/30000 Training Loss: 0.07687290012836456\n",
      "Epoch 5892/30000 Training Loss: 0.06430497765541077\n",
      "Epoch 5893/30000 Training Loss: 0.06746327131986618\n",
      "Epoch 5894/30000 Training Loss: 0.0463675819337368\n",
      "Epoch 5895/30000 Training Loss: 0.049075331538915634\n",
      "Epoch 5896/30000 Training Loss: 0.057926878333091736\n",
      "Epoch 5897/30000 Training Loss: 0.06402797251939774\n",
      "Epoch 5898/30000 Training Loss: 0.06233629584312439\n",
      "Epoch 5899/30000 Training Loss: 0.05634026974439621\n",
      "Epoch 5900/30000 Training Loss: 0.06337164342403412\n",
      "Epoch 5900/30000 Validation Loss: 0.05770221725106239\n",
      "Epoch 5901/30000 Training Loss: 0.07091940194368362\n",
      "Epoch 5902/30000 Training Loss: 0.056470271199941635\n",
      "Epoch 5903/30000 Training Loss: 0.04414531588554382\n",
      "Epoch 5904/30000 Training Loss: 0.07366634905338287\n",
      "Epoch 5905/30000 Training Loss: 0.064980648458004\n",
      "Epoch 5906/30000 Training Loss: 0.0703088566660881\n",
      "Epoch 5907/30000 Training Loss: 0.06438136100769043\n",
      "Epoch 5908/30000 Training Loss: 0.07584121078252792\n",
      "Epoch 5909/30000 Training Loss: 0.055048175156116486\n",
      "Epoch 5910/30000 Training Loss: 0.05214238539338112\n",
      "Epoch 5911/30000 Training Loss: 0.047499269247055054\n",
      "Epoch 5912/30000 Training Loss: 0.07429789006710052\n",
      "Epoch 5913/30000 Training Loss: 0.05456284433603287\n",
      "Epoch 5914/30000 Training Loss: 0.0635247528553009\n",
      "Epoch 5915/30000 Training Loss: 0.06315356492996216\n",
      "Epoch 5916/30000 Training Loss: 0.05590374395251274\n",
      "Epoch 5917/30000 Training Loss: 0.07809554040431976\n",
      "Epoch 5918/30000 Training Loss: 0.05541660636663437\n",
      "Epoch 5919/30000 Training Loss: 0.05535885691642761\n",
      "Epoch 5920/30000 Training Loss: 0.06762117892503738\n",
      "Epoch 5921/30000 Training Loss: 0.06599940359592438\n",
      "Epoch 5922/30000 Training Loss: 0.047303471714258194\n",
      "Epoch 5923/30000 Training Loss: 0.06230538710951805\n",
      "Epoch 5924/30000 Training Loss: 0.06088657304644585\n",
      "Epoch 5925/30000 Training Loss: 0.061847880482673645\n",
      "Epoch 5926/30000 Training Loss: 0.0570584312081337\n",
      "Epoch 5927/30000 Training Loss: 0.05439288914203644\n",
      "Epoch 5928/30000 Training Loss: 0.04229573905467987\n",
      "Epoch 5929/30000 Training Loss: 0.07916279882192612\n",
      "Epoch 5930/30000 Training Loss: 0.08258919417858124\n",
      "Epoch 5931/30000 Training Loss: 0.05697765201330185\n",
      "Epoch 5932/30000 Training Loss: 0.0625959038734436\n",
      "Epoch 5933/30000 Training Loss: 0.0684443935751915\n",
      "Epoch 5934/30000 Training Loss: 0.053607769310474396\n",
      "Epoch 5935/30000 Training Loss: 0.0765642523765564\n",
      "Epoch 5936/30000 Training Loss: 0.0587022490799427\n",
      "Epoch 5937/30000 Training Loss: 0.07403796911239624\n",
      "Epoch 5938/30000 Training Loss: 0.056451790034770966\n",
      "Epoch 5939/30000 Training Loss: 0.06389264017343521\n",
      "Epoch 5940/30000 Training Loss: 0.06723171472549438\n",
      "Epoch 5941/30000 Training Loss: 0.053696587681770325\n",
      "Epoch 5942/30000 Training Loss: 0.0717245489358902\n",
      "Epoch 5943/30000 Training Loss: 0.0568053275346756\n",
      "Epoch 5944/30000 Training Loss: 0.05948852002620697\n",
      "Epoch 5945/30000 Training Loss: 0.05809493362903595\n",
      "Epoch 5946/30000 Training Loss: 0.06054659187793732\n",
      "Epoch 5947/30000 Training Loss: 0.05762287601828575\n",
      "Epoch 5948/30000 Training Loss: 0.06393654644489288\n",
      "Epoch 5949/30000 Training Loss: 0.0766465812921524\n",
      "Epoch 5950/30000 Training Loss: 0.05652383342385292\n",
      "Epoch 5951/30000 Training Loss: 0.06278505176305771\n",
      "Epoch 5952/30000 Training Loss: 0.061656396836042404\n",
      "Epoch 5953/30000 Training Loss: 0.08124885708093643\n",
      "Epoch 5954/30000 Training Loss: 0.04574543982744217\n",
      "Epoch 5955/30000 Training Loss: 0.08788090944290161\n",
      "Epoch 5956/30000 Training Loss: 0.05897810682654381\n",
      "Epoch 5957/30000 Training Loss: 0.05879998952150345\n",
      "Epoch 5958/30000 Training Loss: 0.04921497777104378\n",
      "Epoch 5959/30000 Training Loss: 0.05089159682393074\n",
      "Epoch 5960/30000 Training Loss: 0.06418100744485855\n",
      "Epoch 5961/30000 Training Loss: 0.051949113607406616\n",
      "Epoch 5962/30000 Training Loss: 0.04954787716269493\n",
      "Epoch 5963/30000 Training Loss: 0.06543832272291183\n",
      "Epoch 5964/30000 Training Loss: 0.062037017196416855\n",
      "Epoch 5965/30000 Training Loss: 0.05066530779004097\n",
      "Epoch 5966/30000 Training Loss: 0.04852166399359703\n",
      "Epoch 5967/30000 Training Loss: 0.05063777044415474\n",
      "Epoch 5968/30000 Training Loss: 0.0556558258831501\n",
      "Epoch 5969/30000 Training Loss: 0.0546489842236042\n",
      "Epoch 5970/30000 Training Loss: 0.05220896750688553\n",
      "Epoch 5971/30000 Training Loss: 0.0664108470082283\n",
      "Epoch 5972/30000 Training Loss: 0.05776781961321831\n",
      "Epoch 5973/30000 Training Loss: 0.051828112453222275\n",
      "Epoch 5974/30000 Training Loss: 0.05221128463745117\n",
      "Epoch 5975/30000 Training Loss: 0.05515154451131821\n",
      "Epoch 5976/30000 Training Loss: 0.047653697431087494\n",
      "Epoch 5977/30000 Training Loss: 0.055604614317417145\n",
      "Epoch 5978/30000 Training Loss: 0.05801592767238617\n",
      "Epoch 5979/30000 Training Loss: 0.05815848335623741\n",
      "Epoch 5980/30000 Training Loss: 0.06437474489212036\n",
      "Epoch 5981/30000 Training Loss: 0.05996042862534523\n",
      "Epoch 5982/30000 Training Loss: 0.053766317665576935\n",
      "Epoch 5983/30000 Training Loss: 0.05710675194859505\n",
      "Epoch 5984/30000 Training Loss: 0.04668056219816208\n",
      "Epoch 5985/30000 Training Loss: 0.06087601184844971\n",
      "Epoch 5986/30000 Training Loss: 0.045997437089681625\n",
      "Epoch 5987/30000 Training Loss: 0.06208349019289017\n",
      "Epoch 5988/30000 Training Loss: 0.054010048508644104\n",
      "Epoch 5989/30000 Training Loss: 0.07343041896820068\n",
      "Epoch 5990/30000 Training Loss: 0.052089255303144455\n",
      "Epoch 5991/30000 Training Loss: 0.06977880746126175\n",
      "Epoch 5992/30000 Training Loss: 0.05155768245458603\n",
      "Epoch 5993/30000 Training Loss: 0.05428645387291908\n",
      "Epoch 5994/30000 Training Loss: 0.054724980145692825\n",
      "Epoch 5995/30000 Training Loss: 0.0677279531955719\n",
      "Epoch 5996/30000 Training Loss: 0.07034862041473389\n",
      "Epoch 5997/30000 Training Loss: 0.04664071276783943\n",
      "Epoch 5998/30000 Training Loss: 0.06833408027887344\n",
      "Epoch 5999/30000 Training Loss: 0.05265173316001892\n",
      "Epoch 6000/30000 Training Loss: 0.0662280023097992\n",
      "Epoch 6000/30000 Validation Loss: 0.061644844710826874\n",
      "Epoch 6001/30000 Training Loss: 0.056148137897253036\n",
      "Epoch 6002/30000 Training Loss: 0.05391736328601837\n",
      "Epoch 6003/30000 Training Loss: 0.06460464745759964\n",
      "Epoch 6004/30000 Training Loss: 0.04986637458205223\n",
      "Epoch 6005/30000 Training Loss: 0.05363894999027252\n",
      "Epoch 6006/30000 Training Loss: 0.06997914612293243\n",
      "Epoch 6007/30000 Training Loss: 0.0648265928030014\n",
      "Epoch 6008/30000 Training Loss: 0.0637095719575882\n",
      "Epoch 6009/30000 Training Loss: 0.061684928834438324\n",
      "Epoch 6010/30000 Training Loss: 0.05501779168844223\n",
      "Epoch 6011/30000 Training Loss: 0.047769635915756226\n",
      "Epoch 6012/30000 Training Loss: 0.07159976661205292\n",
      "Epoch 6013/30000 Training Loss: 0.07108373194932938\n",
      "Epoch 6014/30000 Training Loss: 0.056109748780727386\n",
      "Epoch 6015/30000 Training Loss: 0.048660412430763245\n",
      "Epoch 6016/30000 Training Loss: 0.07602958381175995\n",
      "Epoch 6017/30000 Training Loss: 0.05869058892130852\n",
      "Epoch 6018/30000 Training Loss: 0.04878756403923035\n",
      "Epoch 6019/30000 Training Loss: 0.05988208204507828\n",
      "Epoch 6020/30000 Training Loss: 0.06053197383880615\n",
      "Epoch 6021/30000 Training Loss: 0.05885804817080498\n",
      "Epoch 6022/30000 Training Loss: 0.05924639478325844\n",
      "Epoch 6023/30000 Training Loss: 0.06320701539516449\n",
      "Epoch 6024/30000 Training Loss: 0.05845765769481659\n",
      "Epoch 6025/30000 Training Loss: 0.04670786112546921\n",
      "Epoch 6026/30000 Training Loss: 0.0643828734755516\n",
      "Epoch 6027/30000 Training Loss: 0.051536351442337036\n",
      "Epoch 6028/30000 Training Loss: 0.0579163134098053\n",
      "Epoch 6029/30000 Training Loss: 0.04663480073213577\n",
      "Epoch 6030/30000 Training Loss: 0.07300454378128052\n",
      "Epoch 6031/30000 Training Loss: 0.06299550831317902\n",
      "Epoch 6032/30000 Training Loss: 0.05743029713630676\n",
      "Epoch 6033/30000 Training Loss: 0.06333683431148529\n",
      "Epoch 6034/30000 Training Loss: 0.04687057435512543\n",
      "Epoch 6035/30000 Training Loss: 0.05011172220110893\n",
      "Epoch 6036/30000 Training Loss: 0.051713477820158005\n",
      "Epoch 6037/30000 Training Loss: 0.05696720629930496\n",
      "Epoch 6038/30000 Training Loss: 0.05682648718357086\n",
      "Epoch 6039/30000 Training Loss: 0.04571348801255226\n",
      "Epoch 6040/30000 Training Loss: 0.040573541074991226\n",
      "Epoch 6041/30000 Training Loss: 0.05172285437583923\n",
      "Epoch 6042/30000 Training Loss: 0.0608045719563961\n",
      "Epoch 6043/30000 Training Loss: 0.06122924014925957\n",
      "Epoch 6044/30000 Training Loss: 0.06663595139980316\n",
      "Epoch 6045/30000 Training Loss: 0.06485818326473236\n",
      "Epoch 6046/30000 Training Loss: 0.079128198325634\n",
      "Epoch 6047/30000 Training Loss: 0.055138684809207916\n",
      "Epoch 6048/30000 Training Loss: 0.06297146528959274\n",
      "Epoch 6049/30000 Training Loss: 0.04316161572933197\n",
      "Epoch 6050/30000 Training Loss: 0.075751394033432\n",
      "Epoch 6051/30000 Training Loss: 0.04358791932463646\n",
      "Epoch 6052/30000 Training Loss: 0.0685974657535553\n",
      "Epoch 6053/30000 Training Loss: 0.05096232518553734\n",
      "Epoch 6054/30000 Training Loss: 0.05285569280385971\n",
      "Epoch 6055/30000 Training Loss: 0.04912400245666504\n",
      "Epoch 6056/30000 Training Loss: 0.04252605140209198\n",
      "Epoch 6057/30000 Training Loss: 0.08221505582332611\n",
      "Epoch 6058/30000 Training Loss: 0.06017177924513817\n",
      "Epoch 6059/30000 Training Loss: 0.05424627661705017\n",
      "Epoch 6060/30000 Training Loss: 0.04352428391575813\n",
      "Epoch 6061/30000 Training Loss: 0.06118478626012802\n",
      "Epoch 6062/30000 Training Loss: 0.0634344145655632\n",
      "Epoch 6063/30000 Training Loss: 0.04652179032564163\n",
      "Epoch 6064/30000 Training Loss: 0.05720445513725281\n",
      "Epoch 6065/30000 Training Loss: 0.04963173717260361\n",
      "Epoch 6066/30000 Training Loss: 0.06639519333839417\n",
      "Epoch 6067/30000 Training Loss: 0.07170309871435165\n",
      "Epoch 6068/30000 Training Loss: 0.047124914824962616\n",
      "Epoch 6069/30000 Training Loss: 0.06033216044306755\n",
      "Epoch 6070/30000 Training Loss: 0.0550309419631958\n",
      "Epoch 6071/30000 Training Loss: 0.07132600247859955\n",
      "Epoch 6072/30000 Training Loss: 0.0760820284485817\n",
      "Epoch 6073/30000 Training Loss: 0.04940853640437126\n",
      "Epoch 6074/30000 Training Loss: 0.07358093559741974\n",
      "Epoch 6075/30000 Training Loss: 0.05831868201494217\n",
      "Epoch 6076/30000 Training Loss: 0.06092905253171921\n",
      "Epoch 6077/30000 Training Loss: 0.06119430810213089\n",
      "Epoch 6078/30000 Training Loss: 0.05022773891687393\n",
      "Epoch 6079/30000 Training Loss: 0.06869988888502121\n",
      "Epoch 6080/30000 Training Loss: 0.0746656134724617\n",
      "Epoch 6081/30000 Training Loss: 0.05809531360864639\n",
      "Epoch 6082/30000 Training Loss: 0.0618295781314373\n",
      "Epoch 6083/30000 Training Loss: 0.06408454477787018\n",
      "Epoch 6084/30000 Training Loss: 0.04429074376821518\n",
      "Epoch 6085/30000 Training Loss: 0.06913122534751892\n",
      "Epoch 6086/30000 Training Loss: 0.05559578537940979\n",
      "Epoch 6087/30000 Training Loss: 0.04300311207771301\n",
      "Epoch 6088/30000 Training Loss: 0.05241076648235321\n",
      "Epoch 6089/30000 Training Loss: 0.06771691143512726\n",
      "Epoch 6090/30000 Training Loss: 0.059981659054756165\n",
      "Epoch 6091/30000 Training Loss: 0.04917718097567558\n",
      "Epoch 6092/30000 Training Loss: 0.057719048112630844\n",
      "Epoch 6093/30000 Training Loss: 0.05086488649249077\n",
      "Epoch 6094/30000 Training Loss: 0.05495334044098854\n",
      "Epoch 6095/30000 Training Loss: 0.051851168274879456\n",
      "Epoch 6096/30000 Training Loss: 0.04889601469039917\n",
      "Epoch 6097/30000 Training Loss: 0.06784965097904205\n",
      "Epoch 6098/30000 Training Loss: 0.0742516964673996\n",
      "Epoch 6099/30000 Training Loss: 0.07243449240922928\n",
      "Epoch 6100/30000 Training Loss: 0.06437713652849197\n",
      "Epoch 6100/30000 Validation Loss: 0.045590393245220184\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.045590393245220184<=============\n",
      "Epoch 6101/30000 Training Loss: 0.05606978386640549\n",
      "Epoch 6102/30000 Training Loss: 0.05812462419271469\n",
      "Epoch 6103/30000 Training Loss: 0.05278753861784935\n",
      "Epoch 6104/30000 Training Loss: 0.06022229790687561\n",
      "Epoch 6105/30000 Training Loss: 0.05256664752960205\n",
      "Epoch 6106/30000 Training Loss: 0.05957282334566116\n",
      "Epoch 6107/30000 Training Loss: 0.04944445937871933\n",
      "Epoch 6108/30000 Training Loss: 0.04061977565288544\n",
      "Epoch 6109/30000 Training Loss: 0.05209704115986824\n",
      "Epoch 6110/30000 Training Loss: 0.07417748868465424\n",
      "Epoch 6111/30000 Training Loss: 0.07445996254682541\n",
      "Epoch 6112/30000 Training Loss: 0.058976221829652786\n",
      "Epoch 6113/30000 Training Loss: 0.06313417851924896\n",
      "Epoch 6114/30000 Training Loss: 0.06154637038707733\n",
      "Epoch 6115/30000 Training Loss: 0.06450556963682175\n",
      "Epoch 6116/30000 Training Loss: 0.055060435086488724\n",
      "Epoch 6117/30000 Training Loss: 0.05193674564361572\n",
      "Epoch 6118/30000 Training Loss: 0.049939826130867004\n",
      "Epoch 6119/30000 Training Loss: 0.04721502959728241\n",
      "Epoch 6120/30000 Training Loss: 0.07163636386394501\n",
      "Epoch 6121/30000 Training Loss: 0.06947077065706253\n",
      "Epoch 6122/30000 Training Loss: 0.05559058487415314\n",
      "Epoch 6123/30000 Training Loss: 0.06738150864839554\n",
      "Epoch 6124/30000 Training Loss: 0.0720052719116211\n",
      "Epoch 6125/30000 Training Loss: 0.05548148229718208\n",
      "Epoch 6126/30000 Training Loss: 0.07701108604669571\n",
      "Epoch 6127/30000 Training Loss: 0.04047933965921402\n",
      "Epoch 6128/30000 Training Loss: 0.05619296804070473\n",
      "Epoch 6129/30000 Training Loss: 0.04543337970972061\n",
      "Epoch 6130/30000 Training Loss: 0.06909491121768951\n",
      "Epoch 6131/30000 Training Loss: 0.05789284408092499\n",
      "Epoch 6132/30000 Training Loss: 0.07045275717973709\n",
      "Epoch 6133/30000 Training Loss: 0.050998687744140625\n",
      "Epoch 6134/30000 Training Loss: 0.046381719410419464\n",
      "Epoch 6135/30000 Training Loss: 0.053869035094976425\n",
      "Epoch 6136/30000 Training Loss: 0.05105388164520264\n",
      "Epoch 6137/30000 Training Loss: 0.05508909747004509\n",
      "Epoch 6138/30000 Training Loss: 0.07995523512363434\n",
      "Epoch 6139/30000 Training Loss: 0.05727125331759453\n",
      "Epoch 6140/30000 Training Loss: 0.06608990579843521\n",
      "Epoch 6141/30000 Training Loss: 0.0602574497461319\n",
      "Epoch 6142/30000 Training Loss: 0.08861588686704636\n",
      "Epoch 6143/30000 Training Loss: 0.05346135050058365\n",
      "Epoch 6144/30000 Training Loss: 0.05760861188173294\n",
      "Epoch 6145/30000 Training Loss: 0.05316542461514473\n",
      "Epoch 6146/30000 Training Loss: 0.05248500406742096\n",
      "Epoch 6147/30000 Training Loss: 0.04595816507935524\n",
      "Epoch 6148/30000 Training Loss: 0.06791176646947861\n",
      "Epoch 6149/30000 Training Loss: 0.06861133873462677\n",
      "Epoch 6150/30000 Training Loss: 0.046246353536844254\n",
      "Epoch 6151/30000 Training Loss: 0.06322932988405228\n",
      "Epoch 6152/30000 Training Loss: 0.05280426889657974\n",
      "Epoch 6153/30000 Training Loss: 0.050679974257946014\n",
      "Epoch 6154/30000 Training Loss: 0.061356279999017715\n",
      "Epoch 6155/30000 Training Loss: 0.050864674150943756\n",
      "Epoch 6156/30000 Training Loss: 0.06441095471382141\n",
      "Epoch 6157/30000 Training Loss: 0.04977910965681076\n",
      "Epoch 6158/30000 Training Loss: 0.06775659322738647\n",
      "Epoch 6159/30000 Training Loss: 0.07338977605104446\n",
      "Epoch 6160/30000 Training Loss: 0.06313205510377884\n",
      "Epoch 6161/30000 Training Loss: 0.07750999927520752\n",
      "Epoch 6162/30000 Training Loss: 0.06637883931398392\n",
      "Epoch 6163/30000 Training Loss: 0.07719171047210693\n",
      "Epoch 6164/30000 Training Loss: 0.06689006835222244\n",
      "Epoch 6165/30000 Training Loss: 0.06903571635484695\n",
      "Epoch 6166/30000 Training Loss: 0.04399924725294113\n",
      "Epoch 6167/30000 Training Loss: 0.04944634065032005\n",
      "Epoch 6168/30000 Training Loss: 0.06516612321138382\n",
      "Epoch 6169/30000 Training Loss: 0.04692252352833748\n",
      "Epoch 6170/30000 Training Loss: 0.07968955487012863\n",
      "Epoch 6171/30000 Training Loss: 0.05585392564535141\n",
      "Epoch 6172/30000 Training Loss: 0.06177188456058502\n",
      "Epoch 6173/30000 Training Loss: 0.048575565218925476\n",
      "Epoch 6174/30000 Training Loss: 0.06715001910924911\n",
      "Epoch 6175/30000 Training Loss: 0.06876519322395325\n",
      "Epoch 6176/30000 Training Loss: 0.07453014701604843\n",
      "Epoch 6177/30000 Training Loss: 0.06401494145393372\n",
      "Epoch 6178/30000 Training Loss: 0.060139939188957214\n",
      "Epoch 6179/30000 Training Loss: 0.07601519674062729\n",
      "Epoch 6180/30000 Training Loss: 0.05451270937919617\n",
      "Epoch 6181/30000 Training Loss: 0.06528116762638092\n",
      "Epoch 6182/30000 Training Loss: 0.05290240794420242\n",
      "Epoch 6183/30000 Training Loss: 0.0791192352771759\n",
      "Epoch 6184/30000 Training Loss: 0.05851732939481735\n",
      "Epoch 6185/30000 Training Loss: 0.0462215393781662\n",
      "Epoch 6186/30000 Training Loss: 0.06538860499858856\n",
      "Epoch 6187/30000 Training Loss: 0.06482163816690445\n",
      "Epoch 6188/30000 Training Loss: 0.050756942480802536\n",
      "Epoch 6189/30000 Training Loss: 0.0692744180560112\n",
      "Epoch 6190/30000 Training Loss: 0.05960093438625336\n",
      "Epoch 6191/30000 Training Loss: 0.06443549692630768\n",
      "Epoch 6192/30000 Training Loss: 0.04325459152460098\n",
      "Epoch 6193/30000 Training Loss: 0.08340081572532654\n",
      "Epoch 6194/30000 Training Loss: 0.05978332459926605\n",
      "Epoch 6195/30000 Training Loss: 0.05965448543429375\n",
      "Epoch 6196/30000 Training Loss: 0.07335414737462997\n",
      "Epoch 6197/30000 Training Loss: 0.055123984813690186\n",
      "Epoch 6198/30000 Training Loss: 0.07115727663040161\n",
      "Epoch 6199/30000 Training Loss: 0.06437290459871292\n",
      "Epoch 6200/30000 Training Loss: 0.06324078142642975\n",
      "Epoch 6200/30000 Validation Loss: 0.047572266310453415\n",
      "Epoch 6201/30000 Training Loss: 0.06233591586351395\n",
      "Epoch 6202/30000 Training Loss: 0.05567043274641037\n",
      "Epoch 6203/30000 Training Loss: 0.07317374646663666\n",
      "Epoch 6204/30000 Training Loss: 0.0447661355137825\n",
      "Epoch 6205/30000 Training Loss: 0.05418401211500168\n",
      "Epoch 6206/30000 Training Loss: 0.05504665523767471\n",
      "Epoch 6207/30000 Training Loss: 0.06675492972135544\n",
      "Epoch 6208/30000 Training Loss: 0.061978232115507126\n",
      "Epoch 6209/30000 Training Loss: 0.0714135468006134\n",
      "Epoch 6210/30000 Training Loss: 0.05477586016058922\n",
      "Epoch 6211/30000 Training Loss: 0.0591769739985466\n",
      "Epoch 6212/30000 Training Loss: 0.07496213167905807\n",
      "Epoch 6213/30000 Training Loss: 0.06414507329463959\n",
      "Epoch 6214/30000 Training Loss: 0.05497164651751518\n",
      "Epoch 6215/30000 Training Loss: 0.05620971694588661\n",
      "Epoch 6216/30000 Training Loss: 0.05535316467285156\n",
      "Epoch 6217/30000 Training Loss: 0.05676054209470749\n",
      "Epoch 6218/30000 Training Loss: 0.06879489123821259\n",
      "Epoch 6219/30000 Training Loss: 0.06427741050720215\n",
      "Epoch 6220/30000 Training Loss: 0.05521392077207565\n",
      "Epoch 6221/30000 Training Loss: 0.061178527772426605\n",
      "Epoch 6222/30000 Training Loss: 0.052246056497097015\n",
      "Epoch 6223/30000 Training Loss: 0.05256708711385727\n",
      "Epoch 6224/30000 Training Loss: 0.05905745550990105\n",
      "Epoch 6225/30000 Training Loss: 0.03684486448764801\n",
      "Epoch 6226/30000 Training Loss: 0.04880039766430855\n",
      "Epoch 6227/30000 Training Loss: 0.05634806305170059\n",
      "Epoch 6228/30000 Training Loss: 0.059735625982284546\n",
      "Epoch 6229/30000 Training Loss: 0.04076039418578148\n",
      "Epoch 6230/30000 Training Loss: 0.06819643080234528\n",
      "Epoch 6231/30000 Training Loss: 0.06741918623447418\n",
      "Epoch 6232/30000 Training Loss: 0.06704388558864594\n",
      "Epoch 6233/30000 Training Loss: 0.08299727737903595\n",
      "Epoch 6234/30000 Training Loss: 0.06132371351122856\n",
      "Epoch 6235/30000 Training Loss: 0.0707927718758583\n",
      "Epoch 6236/30000 Training Loss: 0.05803171172738075\n",
      "Epoch 6237/30000 Training Loss: 0.05678373575210571\n",
      "Epoch 6238/30000 Training Loss: 0.04250357300043106\n",
      "Epoch 6239/30000 Training Loss: 0.056690480560064316\n",
      "Epoch 6240/30000 Training Loss: 0.06430600583553314\n",
      "Epoch 6241/30000 Training Loss: 0.08302570879459381\n",
      "Epoch 6242/30000 Training Loss: 0.04971008002758026\n",
      "Epoch 6243/30000 Training Loss: 0.07508733123540878\n",
      "Epoch 6244/30000 Training Loss: 0.0676899254322052\n",
      "Epoch 6245/30000 Training Loss: 0.054593220353126526\n",
      "Epoch 6246/30000 Training Loss: 0.06676892936229706\n",
      "Epoch 6247/30000 Training Loss: 0.05034330487251282\n",
      "Epoch 6248/30000 Training Loss: 0.04772287607192993\n",
      "Epoch 6249/30000 Training Loss: 0.06056852638721466\n",
      "Epoch 6250/30000 Training Loss: 0.0632796436548233\n",
      "Epoch 6251/30000 Training Loss: 0.053436651825904846\n",
      "Epoch 6252/30000 Training Loss: 0.06517060101032257\n",
      "Epoch 6253/30000 Training Loss: 0.06276262551546097\n",
      "Epoch 6254/30000 Training Loss: 0.06234077736735344\n",
      "Epoch 6255/30000 Training Loss: 0.07921621203422546\n",
      "Epoch 6256/30000 Training Loss: 0.0602504201233387\n",
      "Epoch 6257/30000 Training Loss: 0.057820532470941544\n",
      "Epoch 6258/30000 Training Loss: 0.05997928977012634\n",
      "Epoch 6259/30000 Training Loss: 0.05405458062887192\n",
      "Epoch 6260/30000 Training Loss: 0.04494747519493103\n",
      "Epoch 6261/30000 Training Loss: 0.065598264336586\n",
      "Epoch 6262/30000 Training Loss: 0.06545453518629074\n",
      "Epoch 6263/30000 Training Loss: 0.05203641206026077\n",
      "Epoch 6264/30000 Training Loss: 0.05449725314974785\n",
      "Epoch 6265/30000 Training Loss: 0.04847314953804016\n",
      "Epoch 6266/30000 Training Loss: 0.06565749645233154\n",
      "Epoch 6267/30000 Training Loss: 0.0467238575220108\n",
      "Epoch 6268/30000 Training Loss: 0.06236192584037781\n",
      "Epoch 6269/30000 Training Loss: 0.08946166932582855\n",
      "Epoch 6270/30000 Training Loss: 0.05377621576189995\n",
      "Epoch 6271/30000 Training Loss: 0.06001017615199089\n",
      "Epoch 6272/30000 Training Loss: 0.062740258872509\n",
      "Epoch 6273/30000 Training Loss: 0.044851310551166534\n",
      "Epoch 6274/30000 Training Loss: 0.0632946714758873\n",
      "Epoch 6275/30000 Training Loss: 0.050247758626937866\n",
      "Epoch 6276/30000 Training Loss: 0.06024928018450737\n",
      "Epoch 6277/30000 Training Loss: 0.05115555226802826\n",
      "Epoch 6278/30000 Training Loss: 0.06285102665424347\n",
      "Epoch 6279/30000 Training Loss: 0.05098813399672508\n",
      "Epoch 6280/30000 Training Loss: 0.05873038247227669\n",
      "Epoch 6281/30000 Training Loss: 0.06700938194990158\n",
      "Epoch 6282/30000 Training Loss: 0.07568597793579102\n",
      "Epoch 6283/30000 Training Loss: 0.06948272138834\n",
      "Epoch 6284/30000 Training Loss: 0.05538623034954071\n",
      "Epoch 6285/30000 Training Loss: 0.04590056091547012\n",
      "Epoch 6286/30000 Training Loss: 0.0679289698600769\n",
      "Epoch 6287/30000 Training Loss: 0.05112376809120178\n",
      "Epoch 6288/30000 Training Loss: 0.04380834847688675\n",
      "Epoch 6289/30000 Training Loss: 0.07394666969776154\n",
      "Epoch 6290/30000 Training Loss: 0.05022850260138512\n",
      "Epoch 6291/30000 Training Loss: 0.07806038856506348\n",
      "Epoch 6292/30000 Training Loss: 0.057940613478422165\n",
      "Epoch 6293/30000 Training Loss: 0.0670124739408493\n",
      "Epoch 6294/30000 Training Loss: 0.05995260924100876\n",
      "Epoch 6295/30000 Training Loss: 0.04612504318356514\n",
      "Epoch 6296/30000 Training Loss: 0.06337518990039825\n",
      "Epoch 6297/30000 Training Loss: 0.06138762831687927\n",
      "Epoch 6298/30000 Training Loss: 0.04815463349223137\n",
      "Epoch 6299/30000 Training Loss: 0.056318752467632294\n",
      "Epoch 6300/30000 Training Loss: 0.06166795641183853\n",
      "Epoch 6300/30000 Validation Loss: 0.060027047991752625\n",
      "Epoch 6301/30000 Training Loss: 0.06625311821699142\n",
      "Epoch 6302/30000 Training Loss: 0.05995551869273186\n",
      "Epoch 6303/30000 Training Loss: 0.05770772695541382\n",
      "Epoch 6304/30000 Training Loss: 0.049324311316013336\n",
      "Epoch 6305/30000 Training Loss: 0.07166402786970139\n",
      "Epoch 6306/30000 Training Loss: 0.06418770551681519\n",
      "Epoch 6307/30000 Training Loss: 0.03589461371302605\n",
      "Epoch 6308/30000 Training Loss: 0.0530833899974823\n",
      "Epoch 6309/30000 Training Loss: 0.06264922767877579\n",
      "Epoch 6310/30000 Training Loss: 0.05236136168241501\n",
      "Epoch 6311/30000 Training Loss: 0.05150703340768814\n",
      "Epoch 6312/30000 Training Loss: 0.07685352116823196\n",
      "Epoch 6313/30000 Training Loss: 0.04744726046919823\n",
      "Epoch 6314/30000 Training Loss: 0.051000986248254776\n",
      "Epoch 6315/30000 Training Loss: 0.06570494920015335\n",
      "Epoch 6316/30000 Training Loss: 0.0681658685207367\n",
      "Epoch 6317/30000 Training Loss: 0.064893938601017\n",
      "Epoch 6318/30000 Training Loss: 0.06968794763088226\n",
      "Epoch 6319/30000 Training Loss: 0.05563005805015564\n",
      "Epoch 6320/30000 Training Loss: 0.06130712851881981\n",
      "Epoch 6321/30000 Training Loss: 0.05409745126962662\n",
      "Epoch 6322/30000 Training Loss: 0.05144115537405014\n",
      "Epoch 6323/30000 Training Loss: 0.0793163850903511\n",
      "Epoch 6324/30000 Training Loss: 0.06005565822124481\n",
      "Epoch 6325/30000 Training Loss: 0.05692107975482941\n",
      "Epoch 6326/30000 Training Loss: 0.054889798164367676\n",
      "Epoch 6327/30000 Training Loss: 0.06735523790121078\n",
      "Epoch 6328/30000 Training Loss: 0.05016210675239563\n",
      "Epoch 6329/30000 Training Loss: 0.0623016320168972\n",
      "Epoch 6330/30000 Training Loss: 0.05643652006983757\n",
      "Epoch 6331/30000 Training Loss: 0.07064038515090942\n",
      "Epoch 6332/30000 Training Loss: 0.06239498406648636\n",
      "Epoch 6333/30000 Training Loss: 0.0438837930560112\n",
      "Epoch 6334/30000 Training Loss: 0.061356320977211\n",
      "Epoch 6335/30000 Training Loss: 0.054163072258234024\n",
      "Epoch 6336/30000 Training Loss: 0.06831908971071243\n",
      "Epoch 6337/30000 Training Loss: 0.04888074845075607\n",
      "Epoch 6338/30000 Training Loss: 0.04453611746430397\n",
      "Epoch 6339/30000 Training Loss: 0.05914297699928284\n",
      "Epoch 6340/30000 Training Loss: 0.047292422503232956\n",
      "Epoch 6341/30000 Training Loss: 0.05332467332482338\n",
      "Epoch 6342/30000 Training Loss: 0.04405122995376587\n",
      "Epoch 6343/30000 Training Loss: 0.060106147080659866\n",
      "Epoch 6344/30000 Training Loss: 0.05216056853532791\n",
      "Epoch 6345/30000 Training Loss: 0.0702117308974266\n",
      "Epoch 6346/30000 Training Loss: 0.0674566999077797\n",
      "Epoch 6347/30000 Training Loss: 0.06630069017410278\n",
      "Epoch 6348/30000 Training Loss: 0.06599758565425873\n",
      "Epoch 6349/30000 Training Loss: 0.05494825541973114\n",
      "Epoch 6350/30000 Training Loss: 0.054709792137145996\n",
      "Epoch 6351/30000 Training Loss: 0.06697195023298264\n",
      "Epoch 6352/30000 Training Loss: 0.057946912944316864\n",
      "Epoch 6353/30000 Training Loss: 0.060240015387535095\n",
      "Epoch 6354/30000 Training Loss: 0.05784393101930618\n",
      "Epoch 6355/30000 Training Loss: 0.06013716384768486\n",
      "Epoch 6356/30000 Training Loss: 0.06428112089633942\n",
      "Epoch 6357/30000 Training Loss: 0.07496842741966248\n",
      "Epoch 6358/30000 Training Loss: 0.07371298968791962\n",
      "Epoch 6359/30000 Training Loss: 0.05603271350264549\n",
      "Epoch 6360/30000 Training Loss: 0.05362987890839577\n",
      "Epoch 6361/30000 Training Loss: 0.05179356783628464\n",
      "Epoch 6362/30000 Training Loss: 0.05148407444357872\n",
      "Epoch 6363/30000 Training Loss: 0.05997933819890022\n",
      "Epoch 6364/30000 Training Loss: 0.06410499662160873\n",
      "Epoch 6365/30000 Training Loss: 0.07296794652938843\n",
      "Epoch 6366/30000 Training Loss: 0.04430513083934784\n",
      "Epoch 6367/30000 Training Loss: 0.06407590955495834\n",
      "Epoch 6368/30000 Training Loss: 0.06263820827007294\n",
      "Epoch 6369/30000 Training Loss: 0.05826800316572189\n",
      "Epoch 6370/30000 Training Loss: 0.06664106249809265\n",
      "Epoch 6371/30000 Training Loss: 0.05847594887018204\n",
      "Epoch 6372/30000 Training Loss: 0.048468686640262604\n",
      "Epoch 6373/30000 Training Loss: 0.054105229675769806\n",
      "Epoch 6374/30000 Training Loss: 0.05789359658956528\n",
      "Epoch 6375/30000 Training Loss: 0.0653635561466217\n",
      "Epoch 6376/30000 Training Loss: 0.0395272858440876\n",
      "Epoch 6377/30000 Training Loss: 0.060678593814373016\n",
      "Epoch 6378/30000 Training Loss: 0.08524682372808456\n",
      "Epoch 6379/30000 Training Loss: 0.06192711368203163\n",
      "Epoch 6380/30000 Training Loss: 0.0670597180724144\n",
      "Epoch 6381/30000 Training Loss: 0.04929365962743759\n",
      "Epoch 6382/30000 Training Loss: 0.04832211881875992\n",
      "Epoch 6383/30000 Training Loss: 0.052491262555122375\n",
      "Epoch 6384/30000 Training Loss: 0.0514114610850811\n",
      "Epoch 6385/30000 Training Loss: 0.07289676368236542\n",
      "Epoch 6386/30000 Training Loss: 0.056106626987457275\n",
      "Epoch 6387/30000 Training Loss: 0.0709301233291626\n",
      "Epoch 6388/30000 Training Loss: 0.05416039377450943\n",
      "Epoch 6389/30000 Training Loss: 0.05684369057416916\n",
      "Epoch 6390/30000 Training Loss: 0.07340283691883087\n",
      "Epoch 6391/30000 Training Loss: 0.08078106492757797\n",
      "Epoch 6392/30000 Training Loss: 0.061873115599155426\n",
      "Epoch 6393/30000 Training Loss: 0.06488218158483505\n",
      "Epoch 6394/30000 Training Loss: 0.0576530396938324\n",
      "Epoch 6395/30000 Training Loss: 0.055607303977012634\n",
      "Epoch 6396/30000 Training Loss: 0.06730204820632935\n",
      "Epoch 6397/30000 Training Loss: 0.05746222287416458\n",
      "Epoch 6398/30000 Training Loss: 0.07529493421316147\n",
      "Epoch 6399/30000 Training Loss: 0.07125633955001831\n",
      "Epoch 6400/30000 Training Loss: 0.06873448193073273\n",
      "Epoch 6400/30000 Validation Loss: 0.08658286929130554\n",
      "Epoch 6401/30000 Training Loss: 0.05347488448023796\n",
      "Epoch 6402/30000 Training Loss: 0.04918976128101349\n",
      "Epoch 6403/30000 Training Loss: 0.053132060915231705\n",
      "Epoch 6404/30000 Training Loss: 0.07489284873008728\n",
      "Epoch 6405/30000 Training Loss: 0.06751420348882675\n",
      "Epoch 6406/30000 Training Loss: 0.06086352467536926\n",
      "Epoch 6407/30000 Training Loss: 0.08274738490581512\n",
      "Epoch 6408/30000 Training Loss: 0.05359962582588196\n",
      "Epoch 6409/30000 Training Loss: 0.050541363656520844\n",
      "Epoch 6410/30000 Training Loss: 0.06498749554157257\n",
      "Epoch 6411/30000 Training Loss: 0.06247442960739136\n",
      "Epoch 6412/30000 Training Loss: 0.0735899806022644\n",
      "Epoch 6413/30000 Training Loss: 0.05817927420139313\n",
      "Epoch 6414/30000 Training Loss: 0.0531986728310585\n",
      "Epoch 6415/30000 Training Loss: 0.0655134990811348\n",
      "Epoch 6416/30000 Training Loss: 0.057413674890995026\n",
      "Epoch 6417/30000 Training Loss: 0.05368146672844887\n",
      "Epoch 6418/30000 Training Loss: 0.06601076573133469\n",
      "Epoch 6419/30000 Training Loss: 0.07490793615579605\n",
      "Epoch 6420/30000 Training Loss: 0.06484662741422653\n",
      "Epoch 6421/30000 Training Loss: 0.05928473174571991\n",
      "Epoch 6422/30000 Training Loss: 0.06541562080383301\n",
      "Epoch 6423/30000 Training Loss: 0.08197687566280365\n",
      "Epoch 6424/30000 Training Loss: 0.05045655369758606\n",
      "Epoch 6425/30000 Training Loss: 0.049497127532958984\n",
      "Epoch 6426/30000 Training Loss: 0.05796269699931145\n",
      "Epoch 6427/30000 Training Loss: 0.06248631700873375\n",
      "Epoch 6428/30000 Training Loss: 0.06887936592102051\n",
      "Epoch 6429/30000 Training Loss: 0.05053994059562683\n",
      "Epoch 6430/30000 Training Loss: 0.05366195738315582\n",
      "Epoch 6431/30000 Training Loss: 0.06221243739128113\n",
      "Epoch 6432/30000 Training Loss: 0.0519314780831337\n",
      "Epoch 6433/30000 Training Loss: 0.07292935252189636\n",
      "Epoch 6434/30000 Training Loss: 0.057377565652132034\n",
      "Epoch 6435/30000 Training Loss: 0.055780090391635895\n",
      "Epoch 6436/30000 Training Loss: 0.0502580963075161\n",
      "Epoch 6437/30000 Training Loss: 0.05990935117006302\n",
      "Epoch 6438/30000 Training Loss: 0.06783285737037659\n",
      "Epoch 6439/30000 Training Loss: 0.0784459114074707\n",
      "Epoch 6440/30000 Training Loss: 0.050857409834861755\n",
      "Epoch 6441/30000 Training Loss: 0.05689868703484535\n",
      "Epoch 6442/30000 Training Loss: 0.0717122033238411\n",
      "Epoch 6443/30000 Training Loss: 0.04868791624903679\n",
      "Epoch 6444/30000 Training Loss: 0.06175608932971954\n",
      "Epoch 6445/30000 Training Loss: 0.06197134777903557\n",
      "Epoch 6446/30000 Training Loss: 0.059092819690704346\n",
      "Epoch 6447/30000 Training Loss: 0.05670112371444702\n",
      "Epoch 6448/30000 Training Loss: 0.06281275302171707\n",
      "Epoch 6449/30000 Training Loss: 0.07591164112091064\n",
      "Epoch 6450/30000 Training Loss: 0.06828944385051727\n",
      "Epoch 6451/30000 Training Loss: 0.045532312244176865\n",
      "Epoch 6452/30000 Training Loss: 0.05280587822198868\n",
      "Epoch 6453/30000 Training Loss: 0.04595406726002693\n",
      "Epoch 6454/30000 Training Loss: 0.06756260991096497\n",
      "Epoch 6455/30000 Training Loss: 0.05461178719997406\n",
      "Epoch 6456/30000 Training Loss: 0.05623669549822807\n",
      "Epoch 6457/30000 Training Loss: 0.05061482638120651\n",
      "Epoch 6458/30000 Training Loss: 0.06566496938467026\n",
      "Epoch 6459/30000 Training Loss: 0.05807497352361679\n",
      "Epoch 6460/30000 Training Loss: 0.05964263156056404\n",
      "Epoch 6461/30000 Training Loss: 0.05165855213999748\n",
      "Epoch 6462/30000 Training Loss: 0.05298379436135292\n",
      "Epoch 6463/30000 Training Loss: 0.056175462901592255\n",
      "Epoch 6464/30000 Training Loss: 0.04668659716844559\n",
      "Epoch 6465/30000 Training Loss: 0.05111038684844971\n",
      "Epoch 6466/30000 Training Loss: 0.059483110904693604\n",
      "Epoch 6467/30000 Training Loss: 0.04813745617866516\n",
      "Epoch 6468/30000 Training Loss: 0.0469042994081974\n",
      "Epoch 6469/30000 Training Loss: 0.054044291377067566\n",
      "Epoch 6470/30000 Training Loss: 0.05506328120827675\n",
      "Epoch 6471/30000 Training Loss: 0.05876755341887474\n",
      "Epoch 6472/30000 Training Loss: 0.05933445692062378\n",
      "Epoch 6473/30000 Training Loss: 0.049315717071294785\n",
      "Epoch 6474/30000 Training Loss: 0.06351784616708755\n",
      "Epoch 6475/30000 Training Loss: 0.05112988501787186\n",
      "Epoch 6476/30000 Training Loss: 0.03873778507113457\n",
      "Epoch 6477/30000 Training Loss: 0.061170175671577454\n",
      "Epoch 6478/30000 Training Loss: 0.07034215331077576\n",
      "Epoch 6479/30000 Training Loss: 0.059870507568120956\n",
      "Epoch 6480/30000 Training Loss: 0.06146403029561043\n",
      "Epoch 6481/30000 Training Loss: 0.05823248624801636\n",
      "Epoch 6482/30000 Training Loss: 0.05653507634997368\n",
      "Epoch 6483/30000 Training Loss: 0.06371360272169113\n",
      "Epoch 6484/30000 Training Loss: 0.044291235506534576\n",
      "Epoch 6485/30000 Training Loss: 0.043134961277246475\n",
      "Epoch 6486/30000 Training Loss: 0.0456412136554718\n",
      "Epoch 6487/30000 Training Loss: 0.05110722780227661\n",
      "Epoch 6488/30000 Training Loss: 0.06863915175199509\n",
      "Epoch 6489/30000 Training Loss: 0.0505056269466877\n",
      "Epoch 6490/30000 Training Loss: 0.06514538824558258\n",
      "Epoch 6491/30000 Training Loss: 0.06998718529939651\n",
      "Epoch 6492/30000 Training Loss: 0.05729958787560463\n",
      "Epoch 6493/30000 Training Loss: 0.05964607372879982\n",
      "Epoch 6494/30000 Training Loss: 0.04475446790456772\n",
      "Epoch 6495/30000 Training Loss: 0.06492815911769867\n",
      "Epoch 6496/30000 Training Loss: 0.06472824513912201\n",
      "Epoch 6497/30000 Training Loss: 0.05379001796245575\n",
      "Epoch 6498/30000 Training Loss: 0.05581549182534218\n",
      "Epoch 6499/30000 Training Loss: 0.050564780831336975\n",
      "Epoch 6500/30000 Training Loss: 0.05531655251979828\n",
      "Epoch 6500/30000 Validation Loss: 0.05358078330755234\n",
      "Epoch 6501/30000 Training Loss: 0.047776345163583755\n",
      "Epoch 6502/30000 Training Loss: 0.05108132213354111\n",
      "Epoch 6503/30000 Training Loss: 0.06807304173707962\n",
      "Epoch 6504/30000 Training Loss: 0.05269252136349678\n",
      "Epoch 6505/30000 Training Loss: 0.05019502341747284\n",
      "Epoch 6506/30000 Training Loss: 0.08655424416065216\n",
      "Epoch 6507/30000 Training Loss: 0.05091957002878189\n",
      "Epoch 6508/30000 Training Loss: 0.05509316176176071\n",
      "Epoch 6509/30000 Training Loss: 0.05757354199886322\n",
      "Epoch 6510/30000 Training Loss: 0.05521555617451668\n",
      "Epoch 6511/30000 Training Loss: 0.05374424159526825\n",
      "Epoch 6512/30000 Training Loss: 0.060750413686037064\n",
      "Epoch 6513/30000 Training Loss: 0.05167650803923607\n",
      "Epoch 6514/30000 Training Loss: 0.0536358468234539\n",
      "Epoch 6515/30000 Training Loss: 0.0733349472284317\n",
      "Epoch 6516/30000 Training Loss: 0.05805731192231178\n",
      "Epoch 6517/30000 Training Loss: 0.06572078168392181\n",
      "Epoch 6518/30000 Training Loss: 0.06120014190673828\n",
      "Epoch 6519/30000 Training Loss: 0.07069279998540878\n",
      "Epoch 6520/30000 Training Loss: 0.07633315771818161\n",
      "Epoch 6521/30000 Training Loss: 0.05420520156621933\n",
      "Epoch 6522/30000 Training Loss: 0.051609139889478683\n",
      "Epoch 6523/30000 Training Loss: 0.05131775885820389\n",
      "Epoch 6524/30000 Training Loss: 0.050794873386621475\n",
      "Epoch 6525/30000 Training Loss: 0.05796508118510246\n",
      "Epoch 6526/30000 Training Loss: 0.06358160078525543\n",
      "Epoch 6527/30000 Training Loss: 0.06378111988306046\n",
      "Epoch 6528/30000 Training Loss: 0.04475012049078941\n",
      "Epoch 6529/30000 Training Loss: 0.06952162086963654\n",
      "Epoch 6530/30000 Training Loss: 0.062247052788734436\n",
      "Epoch 6531/30000 Training Loss: 0.05641620606184006\n",
      "Epoch 6532/30000 Training Loss: 0.05740360915660858\n",
      "Epoch 6533/30000 Training Loss: 0.040783364325761795\n",
      "Epoch 6534/30000 Training Loss: 0.04207669198513031\n",
      "Epoch 6535/30000 Training Loss: 0.06756148487329483\n",
      "Epoch 6536/30000 Training Loss: 0.04823480546474457\n",
      "Epoch 6537/30000 Training Loss: 0.05044169723987579\n",
      "Epoch 6538/30000 Training Loss: 0.04398798942565918\n",
      "Epoch 6539/30000 Training Loss: 0.07109406590461731\n",
      "Epoch 6540/30000 Training Loss: 0.06845886260271072\n",
      "Epoch 6541/30000 Training Loss: 0.0678282305598259\n",
      "Epoch 6542/30000 Training Loss: 0.05768544226884842\n",
      "Epoch 6543/30000 Training Loss: 0.07361254096031189\n",
      "Epoch 6544/30000 Training Loss: 0.04917559027671814\n",
      "Epoch 6545/30000 Training Loss: 0.05271031707525253\n",
      "Epoch 6546/30000 Training Loss: 0.04978032782673836\n",
      "Epoch 6547/30000 Training Loss: 0.06358465552330017\n",
      "Epoch 6548/30000 Training Loss: 0.04694473370909691\n",
      "Epoch 6549/30000 Training Loss: 0.05690161883831024\n",
      "Epoch 6550/30000 Training Loss: 0.05297219753265381\n",
      "Epoch 6551/30000 Training Loss: 0.06216450035572052\n",
      "Epoch 6552/30000 Training Loss: 0.06009160727262497\n",
      "Epoch 6553/30000 Training Loss: 0.05993122607469559\n",
      "Epoch 6554/30000 Training Loss: 0.07128309458494186\n",
      "Epoch 6555/30000 Training Loss: 0.04998162388801575\n",
      "Epoch 6556/30000 Training Loss: 0.06491454690694809\n",
      "Epoch 6557/30000 Training Loss: 0.06083705648779869\n",
      "Epoch 6558/30000 Training Loss: 0.04156625270843506\n",
      "Epoch 6559/30000 Training Loss: 0.04013708978891373\n",
      "Epoch 6560/30000 Training Loss: 0.053868427872657776\n",
      "Epoch 6561/30000 Training Loss: 0.074220672249794\n",
      "Epoch 6562/30000 Training Loss: 0.055007677525281906\n",
      "Epoch 6563/30000 Training Loss: 0.06689772754907608\n",
      "Epoch 6564/30000 Training Loss: 0.06548510491847992\n",
      "Epoch 6565/30000 Training Loss: 0.0677938461303711\n",
      "Epoch 6566/30000 Training Loss: 0.07254884392023087\n",
      "Epoch 6567/30000 Training Loss: 0.07258415967226028\n",
      "Epoch 6568/30000 Training Loss: 0.05583836883306503\n",
      "Epoch 6569/30000 Training Loss: 0.050260029733181\n",
      "Epoch 6570/30000 Training Loss: 0.05312959849834442\n",
      "Epoch 6571/30000 Training Loss: 0.04295901581645012\n",
      "Epoch 6572/30000 Training Loss: 0.05196959897875786\n",
      "Epoch 6573/30000 Training Loss: 0.046577975153923035\n",
      "Epoch 6574/30000 Training Loss: 0.05113233998417854\n",
      "Epoch 6575/30000 Training Loss: 0.058074112981557846\n",
      "Epoch 6576/30000 Training Loss: 0.04567930847406387\n",
      "Epoch 6577/30000 Training Loss: 0.0543152391910553\n",
      "Epoch 6578/30000 Training Loss: 0.06706202030181885\n",
      "Epoch 6579/30000 Training Loss: 0.05569259077310562\n",
      "Epoch 6580/30000 Training Loss: 0.05031784623861313\n",
      "Epoch 6581/30000 Training Loss: 0.04880170896649361\n",
      "Epoch 6582/30000 Training Loss: 0.04771198332309723\n",
      "Epoch 6583/30000 Training Loss: 0.05884554237127304\n",
      "Epoch 6584/30000 Training Loss: 0.04603175073862076\n",
      "Epoch 6585/30000 Training Loss: 0.0403645858168602\n",
      "Epoch 6586/30000 Training Loss: 0.0634489506483078\n",
      "Epoch 6587/30000 Training Loss: 0.059229545295238495\n",
      "Epoch 6588/30000 Training Loss: 0.043811067938804626\n",
      "Epoch 6589/30000 Training Loss: 0.05788928270339966\n",
      "Epoch 6590/30000 Training Loss: 0.05157024413347244\n",
      "Epoch 6591/30000 Training Loss: 0.05640704929828644\n",
      "Epoch 6592/30000 Training Loss: 0.057509612292051315\n",
      "Epoch 6593/30000 Training Loss: 0.04610643908381462\n",
      "Epoch 6594/30000 Training Loss: 0.04609505087137222\n",
      "Epoch 6595/30000 Training Loss: 0.06064774468541145\n",
      "Epoch 6596/30000 Training Loss: 0.0727323442697525\n",
      "Epoch 6597/30000 Training Loss: 0.05969829484820366\n",
      "Epoch 6598/30000 Training Loss: 0.058303963392972946\n",
      "Epoch 6599/30000 Training Loss: 0.05751483142375946\n",
      "Epoch 6600/30000 Training Loss: 0.05426866188645363\n",
      "Epoch 6600/30000 Validation Loss: 0.05316499248147011\n",
      "Epoch 6601/30000 Training Loss: 0.06663785129785538\n",
      "Epoch 6602/30000 Training Loss: 0.07316509634256363\n",
      "Epoch 6603/30000 Training Loss: 0.06048353761434555\n",
      "Epoch 6604/30000 Training Loss: 0.06308106333017349\n",
      "Epoch 6605/30000 Training Loss: 0.06191620975732803\n",
      "Epoch 6606/30000 Training Loss: 0.054658371955156326\n",
      "Epoch 6607/30000 Training Loss: 0.05454540252685547\n",
      "Epoch 6608/30000 Training Loss: 0.0473012775182724\n",
      "Epoch 6609/30000 Training Loss: 0.060923684388399124\n",
      "Epoch 6610/30000 Training Loss: 0.05469316989183426\n",
      "Epoch 6611/30000 Training Loss: 0.04766327142715454\n",
      "Epoch 6612/30000 Training Loss: 0.0690004974603653\n",
      "Epoch 6613/30000 Training Loss: 0.07815089076757431\n",
      "Epoch 6614/30000 Training Loss: 0.04164406284689903\n",
      "Epoch 6615/30000 Training Loss: 0.05640299618244171\n",
      "Epoch 6616/30000 Training Loss: 0.04201874881982803\n",
      "Epoch 6617/30000 Training Loss: 0.058393653482198715\n",
      "Epoch 6618/30000 Training Loss: 0.048593148589134216\n",
      "Epoch 6619/30000 Training Loss: 0.05521230399608612\n",
      "Epoch 6620/30000 Training Loss: 0.04756006598472595\n",
      "Epoch 6621/30000 Training Loss: 0.04403429478406906\n",
      "Epoch 6622/30000 Training Loss: 0.05810364708304405\n",
      "Epoch 6623/30000 Training Loss: 0.05257652699947357\n",
      "Epoch 6624/30000 Training Loss: 0.04508253559470177\n",
      "Epoch 6625/30000 Training Loss: 0.054621435701847076\n",
      "Epoch 6626/30000 Training Loss: 0.06117282435297966\n",
      "Epoch 6627/30000 Training Loss: 0.08439268916845322\n",
      "Epoch 6628/30000 Training Loss: 0.058060720562934875\n",
      "Epoch 6629/30000 Training Loss: 0.05796058848500252\n",
      "Epoch 6630/30000 Training Loss: 0.049901194870471954\n",
      "Epoch 6631/30000 Training Loss: 0.04503042995929718\n",
      "Epoch 6632/30000 Training Loss: 0.039198920130729675\n",
      "Epoch 6633/30000 Training Loss: 0.05283612757921219\n",
      "Epoch 6634/30000 Training Loss: 0.06358079612255096\n",
      "Epoch 6635/30000 Training Loss: 0.058620888739824295\n",
      "Epoch 6636/30000 Training Loss: 0.04391331598162651\n",
      "Epoch 6637/30000 Training Loss: 0.04988513141870499\n",
      "Epoch 6638/30000 Training Loss: 0.051758989691734314\n",
      "Epoch 6639/30000 Training Loss: 0.06789802014827728\n",
      "Epoch 6640/30000 Training Loss: 0.059553880244493484\n",
      "Epoch 6641/30000 Training Loss: 0.06154904142022133\n",
      "Epoch 6642/30000 Training Loss: 0.07123280316591263\n",
      "Epoch 6643/30000 Training Loss: 0.06940939277410507\n",
      "Epoch 6644/30000 Training Loss: 0.05640833452343941\n",
      "Epoch 6645/30000 Training Loss: 0.061574649065732956\n",
      "Epoch 6646/30000 Training Loss: 0.07226089388132095\n",
      "Epoch 6647/30000 Training Loss: 0.054264988750219345\n",
      "Epoch 6648/30000 Training Loss: 0.07016679644584656\n",
      "Epoch 6649/30000 Training Loss: 0.05273465812206268\n",
      "Epoch 6650/30000 Training Loss: 0.047263842076063156\n",
      "Epoch 6651/30000 Training Loss: 0.05852022022008896\n",
      "Epoch 6652/30000 Training Loss: 0.041776470839977264\n",
      "Epoch 6653/30000 Training Loss: 0.06318626552820206\n",
      "Epoch 6654/30000 Training Loss: 0.06261327117681503\n",
      "Epoch 6655/30000 Training Loss: 0.07003810256719589\n",
      "Epoch 6656/30000 Training Loss: 0.06226857751607895\n",
      "Epoch 6657/30000 Training Loss: 0.06515905261039734\n",
      "Epoch 6658/30000 Training Loss: 0.04370317608118057\n",
      "Epoch 6659/30000 Training Loss: 0.07857492566108704\n",
      "Epoch 6660/30000 Training Loss: 0.0533752478659153\n",
      "Epoch 6661/30000 Training Loss: 0.06654506176710129\n",
      "Epoch 6662/30000 Training Loss: 0.06652083247900009\n",
      "Epoch 6663/30000 Training Loss: 0.04758913815021515\n",
      "Epoch 6664/30000 Training Loss: 0.06242627277970314\n",
      "Epoch 6665/30000 Training Loss: 0.06087857857346535\n",
      "Epoch 6666/30000 Training Loss: 0.05306233465671539\n",
      "Epoch 6667/30000 Training Loss: 0.061619438230991364\n",
      "Epoch 6668/30000 Training Loss: 0.05443748086690903\n",
      "Epoch 6669/30000 Training Loss: 0.0667518824338913\n",
      "Epoch 6670/30000 Training Loss: 0.04967734217643738\n",
      "Epoch 6671/30000 Training Loss: 0.046940308064222336\n",
      "Epoch 6672/30000 Training Loss: 0.05860917270183563\n",
      "Epoch 6673/30000 Training Loss: 0.07082308083772659\n",
      "Epoch 6674/30000 Training Loss: 0.05221060290932655\n",
      "Epoch 6675/30000 Training Loss: 0.07324860244989395\n",
      "Epoch 6676/30000 Training Loss: 0.068848617374897\n",
      "Epoch 6677/30000 Training Loss: 0.05590308830142021\n",
      "Epoch 6678/30000 Training Loss: 0.0532815083861351\n",
      "Epoch 6679/30000 Training Loss: 0.07261219620704651\n",
      "Epoch 6680/30000 Training Loss: 0.07437583804130554\n",
      "Epoch 6681/30000 Training Loss: 0.045047834515571594\n",
      "Epoch 6682/30000 Training Loss: 0.06722395122051239\n",
      "Epoch 6683/30000 Training Loss: 0.060153357684612274\n",
      "Epoch 6684/30000 Training Loss: 0.057262927293777466\n",
      "Epoch 6685/30000 Training Loss: 0.078448086977005\n",
      "Epoch 6686/30000 Training Loss: 0.05018703266978264\n",
      "Epoch 6687/30000 Training Loss: 0.057623252272605896\n",
      "Epoch 6688/30000 Training Loss: 0.05345309153199196\n",
      "Epoch 6689/30000 Training Loss: 0.05939829722046852\n",
      "Epoch 6690/30000 Training Loss: 0.053289830684661865\n",
      "Epoch 6691/30000 Training Loss: 0.06494646519422531\n",
      "Epoch 6692/30000 Training Loss: 0.06249731779098511\n",
      "Epoch 6693/30000 Training Loss: 0.04929741472005844\n",
      "Epoch 6694/30000 Training Loss: 0.060878947377204895\n",
      "Epoch 6695/30000 Training Loss: 0.06554171442985535\n",
      "Epoch 6696/30000 Training Loss: 0.07211796939373016\n",
      "Epoch 6697/30000 Training Loss: 0.06336358189582825\n",
      "Epoch 6698/30000 Training Loss: 0.0616806298494339\n",
      "Epoch 6699/30000 Training Loss: 0.06479380279779434\n",
      "Epoch 6700/30000 Training Loss: 0.060346100479364395\n",
      "Epoch 6700/30000 Validation Loss: 0.04957936331629753\n",
      "Epoch 6701/30000 Training Loss: 0.05281642824411392\n",
      "Epoch 6702/30000 Training Loss: 0.06095772981643677\n",
      "Epoch 6703/30000 Training Loss: 0.05845750495791435\n",
      "Epoch 6704/30000 Training Loss: 0.0677558183670044\n",
      "Epoch 6705/30000 Training Loss: 0.07210288941860199\n",
      "Epoch 6706/30000 Training Loss: 0.05510256811976433\n",
      "Epoch 6707/30000 Training Loss: 0.0778798833489418\n",
      "Epoch 6708/30000 Training Loss: 0.05718213692307472\n",
      "Epoch 6709/30000 Training Loss: 0.06821658462285995\n",
      "Epoch 6710/30000 Training Loss: 0.052954912185668945\n",
      "Epoch 6711/30000 Training Loss: 0.06006830185651779\n",
      "Epoch 6712/30000 Training Loss: 0.05657573789358139\n",
      "Epoch 6713/30000 Training Loss: 0.05175696685910225\n",
      "Epoch 6714/30000 Training Loss: 0.04969935119152069\n",
      "Epoch 6715/30000 Training Loss: 0.06561657786369324\n",
      "Epoch 6716/30000 Training Loss: 0.053991638123989105\n",
      "Epoch 6717/30000 Training Loss: 0.04615020006895065\n",
      "Epoch 6718/30000 Training Loss: 0.061680153012275696\n",
      "Epoch 6719/30000 Training Loss: 0.05990324914455414\n",
      "Epoch 6720/30000 Training Loss: 0.05900121480226517\n",
      "Epoch 6721/30000 Training Loss: 0.05076376721262932\n",
      "Epoch 6722/30000 Training Loss: 0.054223328828811646\n",
      "Epoch 6723/30000 Training Loss: 0.04782528430223465\n",
      "Epoch 6724/30000 Training Loss: 0.05524910241365433\n",
      "Epoch 6725/30000 Training Loss: 0.05208204686641693\n",
      "Epoch 6726/30000 Training Loss: 0.0444224588572979\n",
      "Epoch 6727/30000 Training Loss: 0.047904182225465775\n",
      "Epoch 6728/30000 Training Loss: 0.0571170412003994\n",
      "Epoch 6729/30000 Training Loss: 0.049647796899080276\n",
      "Epoch 6730/30000 Training Loss: 0.054656460881233215\n",
      "Epoch 6731/30000 Training Loss: 0.06006528437137604\n",
      "Epoch 6732/30000 Training Loss: 0.04991283640265465\n",
      "Epoch 6733/30000 Training Loss: 0.08696290850639343\n",
      "Epoch 6734/30000 Training Loss: 0.07826673984527588\n",
      "Epoch 6735/30000 Training Loss: 0.07011957466602325\n",
      "Epoch 6736/30000 Training Loss: 0.05455915257334709\n",
      "Epoch 6737/30000 Training Loss: 0.046893160790205\n",
      "Epoch 6738/30000 Training Loss: 0.07052786648273468\n",
      "Epoch 6739/30000 Training Loss: 0.04649011790752411\n",
      "Epoch 6740/30000 Training Loss: 0.046953026205301285\n",
      "Epoch 6741/30000 Training Loss: 0.06234957277774811\n",
      "Epoch 6742/30000 Training Loss: 0.07273898273706436\n",
      "Epoch 6743/30000 Training Loss: 0.049096666276454926\n",
      "Epoch 6744/30000 Training Loss: 0.07515744864940643\n",
      "Epoch 6745/30000 Training Loss: 0.05147784948348999\n",
      "Epoch 6746/30000 Training Loss: 0.04773686081171036\n",
      "Epoch 6747/30000 Training Loss: 0.06752677261829376\n",
      "Epoch 6748/30000 Training Loss: 0.04060444235801697\n",
      "Epoch 6749/30000 Training Loss: 0.07405173033475876\n",
      "Epoch 6750/30000 Training Loss: 0.04248028248548508\n",
      "Epoch 6751/30000 Training Loss: 0.07378430664539337\n",
      "Epoch 6752/30000 Training Loss: 0.07772202789783478\n",
      "Epoch 6753/30000 Training Loss: 0.06526771187782288\n",
      "Epoch 6754/30000 Training Loss: 0.049735210835933685\n",
      "Epoch 6755/30000 Training Loss: 0.06278586387634277\n",
      "Epoch 6756/30000 Training Loss: 0.06470266729593277\n",
      "Epoch 6757/30000 Training Loss: 0.04966765269637108\n",
      "Epoch 6758/30000 Training Loss: 0.046371083706617355\n",
      "Epoch 6759/30000 Training Loss: 0.06254617869853973\n",
      "Epoch 6760/30000 Training Loss: 0.07063673436641693\n",
      "Epoch 6761/30000 Training Loss: 0.0650363564491272\n",
      "Epoch 6762/30000 Training Loss: 0.0572214275598526\n",
      "Epoch 6763/30000 Training Loss: 0.06708700209856033\n",
      "Epoch 6764/30000 Training Loss: 0.05616328865289688\n",
      "Epoch 6765/30000 Training Loss: 0.06197958439588547\n",
      "Epoch 6766/30000 Training Loss: 0.059913311153650284\n",
      "Epoch 6767/30000 Training Loss: 0.053626351058483124\n",
      "Epoch 6768/30000 Training Loss: 0.04583740234375\n",
      "Epoch 6769/30000 Training Loss: 0.04917572811245918\n",
      "Epoch 6770/30000 Training Loss: 0.06855925917625427\n",
      "Epoch 6771/30000 Training Loss: 0.043641962110996246\n",
      "Epoch 6772/30000 Training Loss: 0.07104092091321945\n",
      "Epoch 6773/30000 Training Loss: 0.06821301579475403\n",
      "Epoch 6774/30000 Training Loss: 0.06139565631747246\n",
      "Epoch 6775/30000 Training Loss: 0.06964421272277832\n",
      "Epoch 6776/30000 Training Loss: 0.07597680389881134\n",
      "Epoch 6777/30000 Training Loss: 0.041908226907253265\n",
      "Epoch 6778/30000 Training Loss: 0.08264179527759552\n",
      "Epoch 6779/30000 Training Loss: 0.06498878449201584\n",
      "Epoch 6780/30000 Training Loss: 0.06846851110458374\n",
      "Epoch 6781/30000 Training Loss: 0.05605948343873024\n",
      "Epoch 6782/30000 Training Loss: 0.06802613288164139\n",
      "Epoch 6783/30000 Training Loss: 0.04930556192994118\n",
      "Epoch 6784/30000 Training Loss: 0.06029815226793289\n",
      "Epoch 6785/30000 Training Loss: 0.05569133907556534\n",
      "Epoch 6786/30000 Training Loss: 0.04624182730913162\n",
      "Epoch 6787/30000 Training Loss: 0.07120199501514435\n",
      "Epoch 6788/30000 Training Loss: 0.06839769333600998\n",
      "Epoch 6789/30000 Training Loss: 0.08147499710321426\n",
      "Epoch 6790/30000 Training Loss: 0.06663969904184341\n",
      "Epoch 6791/30000 Training Loss: 0.04968202859163284\n",
      "Epoch 6792/30000 Training Loss: 0.05951366201043129\n",
      "Epoch 6793/30000 Training Loss: 0.0510791651904583\n",
      "Epoch 6794/30000 Training Loss: 0.06529571115970612\n",
      "Epoch 6795/30000 Training Loss: 0.06731388717889786\n",
      "Epoch 6796/30000 Training Loss: 0.06669127941131592\n",
      "Epoch 6797/30000 Training Loss: 0.04583357647061348\n",
      "Epoch 6798/30000 Training Loss: 0.05224047228693962\n",
      "Epoch 6799/30000 Training Loss: 0.048409800976514816\n",
      "Epoch 6800/30000 Training Loss: 0.05703483894467354\n",
      "Epoch 6800/30000 Validation Loss: 0.0566878542304039\n",
      "Epoch 6801/30000 Training Loss: 0.04485482722520828\n",
      "Epoch 6802/30000 Training Loss: 0.06368027627468109\n",
      "Epoch 6803/30000 Training Loss: 0.061631061136722565\n",
      "Epoch 6804/30000 Training Loss: 0.05534288287162781\n",
      "Epoch 6805/30000 Training Loss: 0.0536334291100502\n",
      "Epoch 6806/30000 Training Loss: 0.06269630789756775\n",
      "Epoch 6807/30000 Training Loss: 0.052120134234428406\n",
      "Epoch 6808/30000 Training Loss: 0.05538521707057953\n",
      "Epoch 6809/30000 Training Loss: 0.06434636563062668\n",
      "Epoch 6810/30000 Training Loss: 0.05189751088619232\n",
      "Epoch 6811/30000 Training Loss: 0.05475875735282898\n",
      "Epoch 6812/30000 Training Loss: 0.051035452634096146\n",
      "Epoch 6813/30000 Training Loss: 0.0631667822599411\n",
      "Epoch 6814/30000 Training Loss: 0.060006748884916306\n",
      "Epoch 6815/30000 Training Loss: 0.07075978815555573\n",
      "Epoch 6816/30000 Training Loss: 0.06553401798009872\n",
      "Epoch 6817/30000 Training Loss: 0.053983017802238464\n",
      "Epoch 6818/30000 Training Loss: 0.06221621483564377\n",
      "Epoch 6819/30000 Training Loss: 0.056560032069683075\n",
      "Epoch 6820/30000 Training Loss: 0.05127745866775513\n",
      "Epoch 6821/30000 Training Loss: 0.060178011655807495\n",
      "Epoch 6822/30000 Training Loss: 0.05809100344777107\n",
      "Epoch 6823/30000 Training Loss: 0.07610248029232025\n",
      "Epoch 6824/30000 Training Loss: 0.045508790761232376\n",
      "Epoch 6825/30000 Training Loss: 0.06258303672075272\n",
      "Epoch 6826/30000 Training Loss: 0.05840729549527168\n",
      "Epoch 6827/30000 Training Loss: 0.0525330975651741\n",
      "Epoch 6828/30000 Training Loss: 0.05343224108219147\n",
      "Epoch 6829/30000 Training Loss: 0.06470806896686554\n",
      "Epoch 6830/30000 Training Loss: 0.04785388708114624\n",
      "Epoch 6831/30000 Training Loss: 0.08440648019313812\n",
      "Epoch 6832/30000 Training Loss: 0.06552838534116745\n",
      "Epoch 6833/30000 Training Loss: 0.041852086782455444\n",
      "Epoch 6834/30000 Training Loss: 0.05440106987953186\n",
      "Epoch 6835/30000 Training Loss: 0.04537864401936531\n",
      "Epoch 6836/30000 Training Loss: 0.05882945656776428\n",
      "Epoch 6837/30000 Training Loss: 0.07764574140310287\n",
      "Epoch 6838/30000 Training Loss: 0.045686595141887665\n",
      "Epoch 6839/30000 Training Loss: 0.055364273488521576\n",
      "Epoch 6840/30000 Training Loss: 0.061701782047748566\n",
      "Epoch 6841/30000 Training Loss: 0.061683304607868195\n",
      "Epoch 6842/30000 Training Loss: 0.05798760801553726\n",
      "Epoch 6843/30000 Training Loss: 0.06633546948432922\n",
      "Epoch 6844/30000 Training Loss: 0.05942299962043762\n",
      "Epoch 6845/30000 Training Loss: 0.07059688866138458\n",
      "Epoch 6846/30000 Training Loss: 0.04343423992395401\n",
      "Epoch 6847/30000 Training Loss: 0.05787316709756851\n",
      "Epoch 6848/30000 Training Loss: 0.06904102861881256\n",
      "Epoch 6849/30000 Training Loss: 0.03898729383945465\n",
      "Epoch 6850/30000 Training Loss: 0.05100032687187195\n",
      "Epoch 6851/30000 Training Loss: 0.07468080520629883\n",
      "Epoch 6852/30000 Training Loss: 0.04961931332945824\n",
      "Epoch 6853/30000 Training Loss: 0.0534176230430603\n",
      "Epoch 6854/30000 Training Loss: 0.04857391491532326\n",
      "Epoch 6855/30000 Training Loss: 0.05983443930745125\n",
      "Epoch 6856/30000 Training Loss: 0.060671593993902206\n",
      "Epoch 6857/30000 Training Loss: 0.04952815920114517\n",
      "Epoch 6858/30000 Training Loss: 0.05336540937423706\n",
      "Epoch 6859/30000 Training Loss: 0.052486203610897064\n",
      "Epoch 6860/30000 Training Loss: 0.05405227467417717\n",
      "Epoch 6861/30000 Training Loss: 0.061123207211494446\n",
      "Epoch 6862/30000 Training Loss: 0.061385590583086014\n",
      "Epoch 6863/30000 Training Loss: 0.05120491981506348\n",
      "Epoch 6864/30000 Training Loss: 0.05030059069395065\n",
      "Epoch 6865/30000 Training Loss: 0.058523062616586685\n",
      "Epoch 6866/30000 Training Loss: 0.04611361771821976\n",
      "Epoch 6867/30000 Training Loss: 0.050705794245004654\n",
      "Epoch 6868/30000 Training Loss: 0.062018245458602905\n",
      "Epoch 6869/30000 Training Loss: 0.04720310866832733\n",
      "Epoch 6870/30000 Training Loss: 0.04497373104095459\n",
      "Epoch 6871/30000 Training Loss: 0.06708040088415146\n",
      "Epoch 6872/30000 Training Loss: 0.040614716708660126\n",
      "Epoch 6873/30000 Training Loss: 0.053803618997335434\n",
      "Epoch 6874/30000 Training Loss: 0.04830387234687805\n",
      "Epoch 6875/30000 Training Loss: 0.050153397023677826\n",
      "Epoch 6876/30000 Training Loss: 0.0531252883374691\n",
      "Epoch 6877/30000 Training Loss: 0.06854824721813202\n",
      "Epoch 6878/30000 Training Loss: 0.053492482751607895\n",
      "Epoch 6879/30000 Training Loss: 0.05142649635672569\n",
      "Epoch 6880/30000 Training Loss: 0.05170109122991562\n",
      "Epoch 6881/30000 Training Loss: 0.06359401345252991\n",
      "Epoch 6882/30000 Training Loss: 0.04527736082673073\n",
      "Epoch 6883/30000 Training Loss: 0.050719767808914185\n",
      "Epoch 6884/30000 Training Loss: 0.0555746853351593\n",
      "Epoch 6885/30000 Training Loss: 0.07943983376026154\n",
      "Epoch 6886/30000 Training Loss: 0.06694579124450684\n",
      "Epoch 6887/30000 Training Loss: 0.050144534558057785\n",
      "Epoch 6888/30000 Training Loss: 0.05454929172992706\n",
      "Epoch 6889/30000 Training Loss: 0.06690078973770142\n",
      "Epoch 6890/30000 Training Loss: 0.06162736564874649\n",
      "Epoch 6891/30000 Training Loss: 0.05626542493700981\n",
      "Epoch 6892/30000 Training Loss: 0.051186710596084595\n",
      "Epoch 6893/30000 Training Loss: 0.07332339882850647\n",
      "Epoch 6894/30000 Training Loss: 0.059898972511291504\n",
      "Epoch 6895/30000 Training Loss: 0.04436686262488365\n",
      "Epoch 6896/30000 Training Loss: 0.060820478945970535\n",
      "Epoch 6897/30000 Training Loss: 0.053978804498910904\n",
      "Epoch 6898/30000 Training Loss: 0.04516559839248657\n",
      "Epoch 6899/30000 Training Loss: 0.04618930071592331\n",
      "Epoch 6900/30000 Training Loss: 0.0629691630601883\n",
      "Epoch 6900/30000 Validation Loss: 0.05728773772716522\n",
      "Epoch 6901/30000 Training Loss: 0.05163101851940155\n",
      "Epoch 6902/30000 Training Loss: 0.05292658880352974\n",
      "Epoch 6903/30000 Training Loss: 0.060252025723457336\n",
      "Epoch 6904/30000 Training Loss: 0.057650402188301086\n",
      "Epoch 6905/30000 Training Loss: 0.06373965740203857\n",
      "Epoch 6906/30000 Training Loss: 0.0594806969165802\n",
      "Epoch 6907/30000 Training Loss: 0.054277271032333374\n",
      "Epoch 6908/30000 Training Loss: 0.0579800084233284\n",
      "Epoch 6909/30000 Training Loss: 0.060638077557086945\n",
      "Epoch 6910/30000 Training Loss: 0.06429381668567657\n",
      "Epoch 6911/30000 Training Loss: 0.05892270803451538\n",
      "Epoch 6912/30000 Training Loss: 0.06755276769399643\n",
      "Epoch 6913/30000 Training Loss: 0.08305712789297104\n",
      "Epoch 6914/30000 Training Loss: 0.05493555963039398\n",
      "Epoch 6915/30000 Training Loss: 0.06303900480270386\n",
      "Epoch 6916/30000 Training Loss: 0.06616289913654327\n",
      "Epoch 6917/30000 Training Loss: 0.06227893382310867\n",
      "Epoch 6918/30000 Training Loss: 0.04674267768859863\n",
      "Epoch 6919/30000 Training Loss: 0.06085944175720215\n",
      "Epoch 6920/30000 Training Loss: 0.06268582493066788\n",
      "Epoch 6921/30000 Training Loss: 0.059574373066425323\n",
      "Epoch 6922/30000 Training Loss: 0.04749293625354767\n",
      "Epoch 6923/30000 Training Loss: 0.046035900712013245\n",
      "Epoch 6924/30000 Training Loss: 0.05163564160466194\n",
      "Epoch 6925/30000 Training Loss: 0.051298290491104126\n",
      "Epoch 6926/30000 Training Loss: 0.06223026663064957\n",
      "Epoch 6927/30000 Training Loss: 0.0445350743830204\n",
      "Epoch 6928/30000 Training Loss: 0.0561276376247406\n",
      "Epoch 6929/30000 Training Loss: 0.04774467647075653\n",
      "Epoch 6930/30000 Training Loss: 0.06300654262304306\n",
      "Epoch 6931/30000 Training Loss: 0.061023157089948654\n",
      "Epoch 6932/30000 Training Loss: 0.0529351681470871\n",
      "Epoch 6933/30000 Training Loss: 0.06556730717420578\n",
      "Epoch 6934/30000 Training Loss: 0.06767886132001877\n",
      "Epoch 6935/30000 Training Loss: 0.05042359232902527\n",
      "Epoch 6936/30000 Training Loss: 0.06281670182943344\n",
      "Epoch 6937/30000 Training Loss: 0.05888582020998001\n",
      "Epoch 6938/30000 Training Loss: 0.06782550364732742\n",
      "Epoch 6939/30000 Training Loss: 0.055343352258205414\n",
      "Epoch 6940/30000 Training Loss: 0.04454932361841202\n",
      "Epoch 6941/30000 Training Loss: 0.058057717978954315\n",
      "Epoch 6942/30000 Training Loss: 0.055393435060977936\n",
      "Epoch 6943/30000 Training Loss: 0.05108193680644035\n",
      "Epoch 6944/30000 Training Loss: 0.06680453568696976\n",
      "Epoch 6945/30000 Training Loss: 0.060080546885728836\n",
      "Epoch 6946/30000 Training Loss: 0.05292244255542755\n",
      "Epoch 6947/30000 Training Loss: 0.054788824170827866\n",
      "Epoch 6948/30000 Training Loss: 0.06484712660312653\n",
      "Epoch 6949/30000 Training Loss: 0.055649787187576294\n",
      "Epoch 6950/30000 Training Loss: 0.039143893867731094\n",
      "Epoch 6951/30000 Training Loss: 0.062435001134872437\n",
      "Epoch 6952/30000 Training Loss: 0.06309051811695099\n",
      "Epoch 6953/30000 Training Loss: 0.05579805374145508\n",
      "Epoch 6954/30000 Training Loss: 0.08279136568307877\n",
      "Epoch 6955/30000 Training Loss: 0.05896695703268051\n",
      "Epoch 6956/30000 Training Loss: 0.05045205354690552\n",
      "Epoch 6957/30000 Training Loss: 0.06604922562837601\n",
      "Epoch 6958/30000 Training Loss: 0.08014965802431107\n",
      "Epoch 6959/30000 Training Loss: 0.07220190763473511\n",
      "Epoch 6960/30000 Training Loss: 0.07152760028839111\n",
      "Epoch 6961/30000 Training Loss: 0.05213528499007225\n",
      "Epoch 6962/30000 Training Loss: 0.0629388689994812\n",
      "Epoch 6963/30000 Training Loss: 0.06884798407554626\n",
      "Epoch 6964/30000 Training Loss: 0.041968174278736115\n",
      "Epoch 6965/30000 Training Loss: 0.056182608008384705\n",
      "Epoch 6966/30000 Training Loss: 0.06700289994478226\n",
      "Epoch 6967/30000 Training Loss: 0.04493533447384834\n",
      "Epoch 6968/30000 Training Loss: 0.05983242020010948\n",
      "Epoch 6969/30000 Training Loss: 0.05231292545795441\n",
      "Epoch 6970/30000 Training Loss: 0.05463574826717377\n",
      "Epoch 6971/30000 Training Loss: 0.0519140250980854\n",
      "Epoch 6972/30000 Training Loss: 0.0628708153963089\n",
      "Epoch 6973/30000 Training Loss: 0.05361097306013107\n",
      "Epoch 6974/30000 Training Loss: 0.05725223571062088\n",
      "Epoch 6975/30000 Training Loss: 0.05683363229036331\n",
      "Epoch 6976/30000 Training Loss: 0.07188711315393448\n",
      "Epoch 6977/30000 Training Loss: 0.05164244771003723\n",
      "Epoch 6978/30000 Training Loss: 0.06350403279066086\n",
      "Epoch 6979/30000 Training Loss: 0.04938314110040665\n",
      "Epoch 6980/30000 Training Loss: 0.06448982656002045\n",
      "Epoch 6981/30000 Training Loss: 0.044020332396030426\n",
      "Epoch 6982/30000 Training Loss: 0.06154799461364746\n",
      "Epoch 6983/30000 Training Loss: 0.05837305262684822\n",
      "Epoch 6984/30000 Training Loss: 0.07317185401916504\n",
      "Epoch 6985/30000 Training Loss: 0.06730309873819351\n",
      "Epoch 6986/30000 Training Loss: 0.05711490660905838\n",
      "Epoch 6987/30000 Training Loss: 0.04299499839544296\n",
      "Epoch 6988/30000 Training Loss: 0.06789553910493851\n",
      "Epoch 6989/30000 Training Loss: 0.06779566407203674\n",
      "Epoch 6990/30000 Training Loss: 0.05276487022638321\n",
      "Epoch 6991/30000 Training Loss: 0.06311211735010147\n",
      "Epoch 6992/30000 Training Loss: 0.07300819456577301\n",
      "Epoch 6993/30000 Training Loss: 0.06691574305295944\n",
      "Epoch 6994/30000 Training Loss: 0.06584466993808746\n",
      "Epoch 6995/30000 Training Loss: 0.04791969805955887\n",
      "Epoch 6996/30000 Training Loss: 0.06276442110538483\n",
      "Epoch 6997/30000 Training Loss: 0.05423128232359886\n",
      "Epoch 6998/30000 Training Loss: 0.052169568836688995\n",
      "Epoch 6999/30000 Training Loss: 0.0695636123418808\n",
      "Epoch 7000/30000 Training Loss: 0.05171734467148781\n",
      "Epoch 7000/30000 Validation Loss: 0.07096021622419357\n",
      "Epoch 7001/30000 Training Loss: 0.06401941180229187\n",
      "Epoch 7002/30000 Training Loss: 0.05932316184043884\n",
      "Epoch 7003/30000 Training Loss: 0.055442750453948975\n",
      "Epoch 7004/30000 Training Loss: 0.03855688497424126\n",
      "Epoch 7005/30000 Training Loss: 0.04993129521608353\n",
      "Epoch 7006/30000 Training Loss: 0.08704869449138641\n",
      "Epoch 7007/30000 Training Loss: 0.05876687541604042\n",
      "Epoch 7008/30000 Training Loss: 0.05494736135005951\n",
      "Epoch 7009/30000 Training Loss: 0.06747590005397797\n",
      "Epoch 7010/30000 Training Loss: 0.057482436299324036\n",
      "Epoch 7011/30000 Training Loss: 0.06568997353315353\n",
      "Epoch 7012/30000 Training Loss: 0.04315052181482315\n",
      "Epoch 7013/30000 Training Loss: 0.04326296225190163\n",
      "Epoch 7014/30000 Training Loss: 0.06489847600460052\n",
      "Epoch 7015/30000 Training Loss: 0.058645736426115036\n",
      "Epoch 7016/30000 Training Loss: 0.06735393404960632\n",
      "Epoch 7017/30000 Training Loss: 0.07227317243814468\n",
      "Epoch 7018/30000 Training Loss: 0.05542702600359917\n",
      "Epoch 7019/30000 Training Loss: 0.052163537591695786\n",
      "Epoch 7020/30000 Training Loss: 0.06182623282074928\n",
      "Epoch 7021/30000 Training Loss: 0.04688102379441261\n",
      "Epoch 7022/30000 Training Loss: 0.07130584120750427\n",
      "Epoch 7023/30000 Training Loss: 0.05012117326259613\n",
      "Epoch 7024/30000 Training Loss: 0.04555938392877579\n",
      "Epoch 7025/30000 Training Loss: 0.07014456391334534\n",
      "Epoch 7026/30000 Training Loss: 0.059289269149303436\n",
      "Epoch 7027/30000 Training Loss: 0.051932282745838165\n",
      "Epoch 7028/30000 Training Loss: 0.056208495050668716\n",
      "Epoch 7029/30000 Training Loss: 0.0499269962310791\n",
      "Epoch 7030/30000 Training Loss: 0.040542978793382645\n",
      "Epoch 7031/30000 Training Loss: 0.0601830929517746\n",
      "Epoch 7032/30000 Training Loss: 0.07752443850040436\n",
      "Epoch 7033/30000 Training Loss: 0.05065841227769852\n",
      "Epoch 7034/30000 Training Loss: 0.06351053714752197\n",
      "Epoch 7035/30000 Training Loss: 0.05947970598936081\n",
      "Epoch 7036/30000 Training Loss: 0.049584612250328064\n",
      "Epoch 7037/30000 Training Loss: 0.05699688568711281\n",
      "Epoch 7038/30000 Training Loss: 0.0547214075922966\n",
      "Epoch 7039/30000 Training Loss: 0.04301716759800911\n",
      "Epoch 7040/30000 Training Loss: 0.04964284598827362\n",
      "Epoch 7041/30000 Training Loss: 0.0558401420712471\n",
      "Epoch 7042/30000 Training Loss: 0.049730610102415085\n",
      "Epoch 7043/30000 Training Loss: 0.0461110845208168\n",
      "Epoch 7044/30000 Training Loss: 0.05786048248410225\n",
      "Epoch 7045/30000 Training Loss: 0.04738643020391464\n",
      "Epoch 7046/30000 Training Loss: 0.05944688618183136\n",
      "Epoch 7047/30000 Training Loss: 0.05620501562952995\n",
      "Epoch 7048/30000 Training Loss: 0.06214504688978195\n",
      "Epoch 7049/30000 Training Loss: 0.06164121255278587\n",
      "Epoch 7050/30000 Training Loss: 0.05516315996646881\n",
      "Epoch 7051/30000 Training Loss: 0.05514664202928543\n",
      "Epoch 7052/30000 Training Loss: 0.0468260794878006\n",
      "Epoch 7053/30000 Training Loss: 0.051527176052331924\n",
      "Epoch 7054/30000 Training Loss: 0.04923880100250244\n",
      "Epoch 7055/30000 Training Loss: 0.0658210963010788\n",
      "Epoch 7056/30000 Training Loss: 0.061111725866794586\n",
      "Epoch 7057/30000 Training Loss: 0.05421510338783264\n",
      "Epoch 7058/30000 Training Loss: 0.06279453635215759\n",
      "Epoch 7059/30000 Training Loss: 0.05939876288175583\n",
      "Epoch 7060/30000 Training Loss: 0.05733402818441391\n",
      "Epoch 7061/30000 Training Loss: 0.051407262682914734\n",
      "Epoch 7062/30000 Training Loss: 0.0667261928319931\n",
      "Epoch 7063/30000 Training Loss: 0.052234940230846405\n",
      "Epoch 7064/30000 Training Loss: 0.06313439458608627\n",
      "Epoch 7065/30000 Training Loss: 0.06738239526748657\n",
      "Epoch 7066/30000 Training Loss: 0.05030385032296181\n",
      "Epoch 7067/30000 Training Loss: 0.054944105446338654\n",
      "Epoch 7068/30000 Training Loss: 0.043952640146017075\n",
      "Epoch 7069/30000 Training Loss: 0.05894835293292999\n",
      "Epoch 7070/30000 Training Loss: 0.06384583562612534\n",
      "Epoch 7071/30000 Training Loss: 0.048293981701135635\n",
      "Epoch 7072/30000 Training Loss: 0.06410527229309082\n",
      "Epoch 7073/30000 Training Loss: 0.04966634511947632\n",
      "Epoch 7074/30000 Training Loss: 0.07293060421943665\n",
      "Epoch 7075/30000 Training Loss: 0.046929746866226196\n",
      "Epoch 7076/30000 Training Loss: 0.03744125738739967\n",
      "Epoch 7077/30000 Training Loss: 0.07414857298135757\n",
      "Epoch 7078/30000 Training Loss: 0.052930865436792374\n",
      "Epoch 7079/30000 Training Loss: 0.061169639229774475\n",
      "Epoch 7080/30000 Training Loss: 0.05786402150988579\n",
      "Epoch 7081/30000 Training Loss: 0.04941719025373459\n",
      "Epoch 7082/30000 Training Loss: 0.05083964765071869\n",
      "Epoch 7083/30000 Training Loss: 0.05640239641070366\n",
      "Epoch 7084/30000 Training Loss: 0.061255332082509995\n",
      "Epoch 7085/30000 Training Loss: 0.059268929064273834\n",
      "Epoch 7086/30000 Training Loss: 0.05504568666219711\n",
      "Epoch 7087/30000 Training Loss: 0.05641712248325348\n",
      "Epoch 7088/30000 Training Loss: 0.055620621889829636\n",
      "Epoch 7089/30000 Training Loss: 0.0721103772521019\n",
      "Epoch 7090/30000 Training Loss: 0.055104486644268036\n",
      "Epoch 7091/30000 Training Loss: 0.07268922030925751\n",
      "Epoch 7092/30000 Training Loss: 0.08383125811815262\n",
      "Epoch 7093/30000 Training Loss: 0.04962659254670143\n",
      "Epoch 7094/30000 Training Loss: 0.06566935032606125\n",
      "Epoch 7095/30000 Training Loss: 0.05873928219079971\n",
      "Epoch 7096/30000 Training Loss: 0.03840712085366249\n",
      "Epoch 7097/30000 Training Loss: 0.06729362905025482\n",
      "Epoch 7098/30000 Training Loss: 0.05516983941197395\n",
      "Epoch 7099/30000 Training Loss: 0.049868181347846985\n",
      "Epoch 7100/30000 Training Loss: 0.07209736853837967\n",
      "Epoch 7100/30000 Validation Loss: 0.05709025263786316\n",
      "Epoch 7101/30000 Training Loss: 0.06405026465654373\n",
      "Epoch 7102/30000 Training Loss: 0.060374677181243896\n",
      "Epoch 7103/30000 Training Loss: 0.07436665892601013\n",
      "Epoch 7104/30000 Training Loss: 0.04989861696958542\n",
      "Epoch 7105/30000 Training Loss: 0.057179566472768784\n",
      "Epoch 7106/30000 Training Loss: 0.07565265893936157\n",
      "Epoch 7107/30000 Training Loss: 0.050424184650182724\n",
      "Epoch 7108/30000 Training Loss: 0.05893375724554062\n",
      "Epoch 7109/30000 Training Loss: 0.05085955932736397\n",
      "Epoch 7110/30000 Training Loss: 0.06323546171188354\n",
      "Epoch 7111/30000 Training Loss: 0.04646357521414757\n",
      "Epoch 7112/30000 Training Loss: 0.055950894951820374\n",
      "Epoch 7113/30000 Training Loss: 0.06273490190505981\n",
      "Epoch 7114/30000 Training Loss: 0.06719086319208145\n",
      "Epoch 7115/30000 Training Loss: 0.047206129878759384\n",
      "Epoch 7116/30000 Training Loss: 0.058187320828437805\n",
      "Epoch 7117/30000 Training Loss: 0.0709473043680191\n",
      "Epoch 7118/30000 Training Loss: 0.05980321764945984\n",
      "Epoch 7119/30000 Training Loss: 0.06669912487268448\n",
      "Epoch 7120/30000 Training Loss: 0.06015211343765259\n",
      "Epoch 7121/30000 Training Loss: 0.05254191905260086\n",
      "Epoch 7122/30000 Training Loss: 0.052117958664894104\n",
      "Epoch 7123/30000 Training Loss: 0.045268263667821884\n",
      "Epoch 7124/30000 Training Loss: 0.052358806133270264\n",
      "Epoch 7125/30000 Training Loss: 0.053806476294994354\n",
      "Epoch 7126/30000 Training Loss: 0.056747958064079285\n",
      "Epoch 7127/30000 Training Loss: 0.05158272385597229\n",
      "Epoch 7128/30000 Training Loss: 0.07675342261791229\n",
      "Epoch 7129/30000 Training Loss: 0.06055562570691109\n",
      "Epoch 7130/30000 Training Loss: 0.051125235855579376\n",
      "Epoch 7131/30000 Training Loss: 0.05390067398548126\n",
      "Epoch 7132/30000 Training Loss: 0.04544775187969208\n",
      "Epoch 7133/30000 Training Loss: 0.05806433781981468\n",
      "Epoch 7134/30000 Training Loss: 0.07832380384206772\n",
      "Epoch 7135/30000 Training Loss: 0.06256735324859619\n",
      "Epoch 7136/30000 Training Loss: 0.05094532296061516\n",
      "Epoch 7137/30000 Training Loss: 0.06330380588769913\n",
      "Epoch 7138/30000 Training Loss: 0.056771937757730484\n",
      "Epoch 7139/30000 Training Loss: 0.07477203756570816\n",
      "Epoch 7140/30000 Training Loss: 0.07073231786489487\n",
      "Epoch 7141/30000 Training Loss: 0.071547731757164\n",
      "Epoch 7142/30000 Training Loss: 0.03803268074989319\n",
      "Epoch 7143/30000 Training Loss: 0.06429086625576019\n",
      "Epoch 7144/30000 Training Loss: 0.04953466355800629\n",
      "Epoch 7145/30000 Training Loss: 0.05823357403278351\n",
      "Epoch 7146/30000 Training Loss: 0.04691081494092941\n",
      "Epoch 7147/30000 Training Loss: 0.056751225143671036\n",
      "Epoch 7148/30000 Training Loss: 0.0460471548140049\n",
      "Epoch 7149/30000 Training Loss: 0.06266343593597412\n",
      "Epoch 7150/30000 Training Loss: 0.056332819163799286\n",
      "Epoch 7151/30000 Training Loss: 0.04661993309855461\n",
      "Epoch 7152/30000 Training Loss: 0.05097530782222748\n",
      "Epoch 7153/30000 Training Loss: 0.052207738161087036\n",
      "Epoch 7154/30000 Training Loss: 0.0696730688214302\n",
      "Epoch 7155/30000 Training Loss: 0.06764092296361923\n",
      "Epoch 7156/30000 Training Loss: 0.06342156231403351\n",
      "Epoch 7157/30000 Training Loss: 0.040993209928274155\n",
      "Epoch 7158/30000 Training Loss: 0.06160949915647507\n",
      "Epoch 7159/30000 Training Loss: 0.062379419803619385\n",
      "Epoch 7160/30000 Training Loss: 0.04813797026872635\n",
      "Epoch 7161/30000 Training Loss: 0.06306447088718414\n",
      "Epoch 7162/30000 Training Loss: 0.06091779097914696\n",
      "Epoch 7163/30000 Training Loss: 0.05223085731267929\n",
      "Epoch 7164/30000 Training Loss: 0.05717365816235542\n",
      "Epoch 7165/30000 Training Loss: 0.06354256719350815\n",
      "Epoch 7166/30000 Training Loss: 0.08042022585868835\n",
      "Epoch 7167/30000 Training Loss: 0.05632270500063896\n",
      "Epoch 7168/30000 Training Loss: 0.04709002375602722\n",
      "Epoch 7169/30000 Training Loss: 0.04161527380347252\n",
      "Epoch 7170/30000 Training Loss: 0.045676589012145996\n",
      "Epoch 7171/30000 Training Loss: 0.06347694993019104\n",
      "Epoch 7172/30000 Training Loss: 0.06306735426187515\n",
      "Epoch 7173/30000 Training Loss: 0.06796514987945557\n",
      "Epoch 7174/30000 Training Loss: 0.07620585709810257\n",
      "Epoch 7175/30000 Training Loss: 0.060678426176309586\n",
      "Epoch 7176/30000 Training Loss: 0.054267458617687225\n",
      "Epoch 7177/30000 Training Loss: 0.04920722544193268\n",
      "Epoch 7178/30000 Training Loss: 0.08009129762649536\n",
      "Epoch 7179/30000 Training Loss: 0.045747365802526474\n",
      "Epoch 7180/30000 Training Loss: 0.040993720293045044\n",
      "Epoch 7181/30000 Training Loss: 0.04660225287079811\n",
      "Epoch 7182/30000 Training Loss: 0.04285905882716179\n",
      "Epoch 7183/30000 Training Loss: 0.0654522255063057\n",
      "Epoch 7184/30000 Training Loss: 0.08274687826633453\n",
      "Epoch 7185/30000 Training Loss: 0.05062507838010788\n",
      "Epoch 7186/30000 Training Loss: 0.04979102313518524\n",
      "Epoch 7187/30000 Training Loss: 0.07425019145011902\n",
      "Epoch 7188/30000 Training Loss: 0.06189042702317238\n",
      "Epoch 7189/30000 Training Loss: 0.06533254683017731\n",
      "Epoch 7190/30000 Training Loss: 0.05182164907455444\n",
      "Epoch 7191/30000 Training Loss: 0.04021129384636879\n",
      "Epoch 7192/30000 Training Loss: 0.06112949550151825\n",
      "Epoch 7193/30000 Training Loss: 0.06599286198616028\n",
      "Epoch 7194/30000 Training Loss: 0.07274038344621658\n",
      "Epoch 7195/30000 Training Loss: 0.06849778443574905\n",
      "Epoch 7196/30000 Training Loss: 0.06279712915420532\n",
      "Epoch 7197/30000 Training Loss: 0.05081481486558914\n",
      "Epoch 7198/30000 Training Loss: 0.046024173498153687\n",
      "Epoch 7199/30000 Training Loss: 0.04651084169745445\n",
      "Epoch 7200/30000 Training Loss: 0.06602489203214645\n",
      "Epoch 7200/30000 Validation Loss: 0.0462532639503479\n",
      "Epoch 7201/30000 Training Loss: 0.05047157034277916\n",
      "Epoch 7202/30000 Training Loss: 0.05446094647049904\n",
      "Epoch 7203/30000 Training Loss: 0.046681199222803116\n",
      "Epoch 7204/30000 Training Loss: 0.07398181408643723\n",
      "Epoch 7205/30000 Training Loss: 0.05356384813785553\n",
      "Epoch 7206/30000 Training Loss: 0.06390964239835739\n",
      "Epoch 7207/30000 Training Loss: 0.04577549546957016\n",
      "Epoch 7208/30000 Training Loss: 0.06032954901456833\n",
      "Epoch 7209/30000 Training Loss: 0.059656571596860886\n",
      "Epoch 7210/30000 Training Loss: 0.059233102947473526\n",
      "Epoch 7211/30000 Training Loss: 0.06280972063541412\n",
      "Epoch 7212/30000 Training Loss: 0.06478650867938995\n",
      "Epoch 7213/30000 Training Loss: 0.05066520720720291\n",
      "Epoch 7214/30000 Training Loss: 0.053709037601947784\n",
      "Epoch 7215/30000 Training Loss: 0.06804975122213364\n",
      "Epoch 7216/30000 Training Loss: 0.04464545473456383\n",
      "Epoch 7217/30000 Training Loss: 0.08019056171178818\n",
      "Epoch 7218/30000 Training Loss: 0.055315084755420685\n",
      "Epoch 7219/30000 Training Loss: 0.046359628438949585\n",
      "Epoch 7220/30000 Training Loss: 0.04974290728569031\n",
      "Epoch 7221/30000 Training Loss: 0.05667785555124283\n",
      "Epoch 7222/30000 Training Loss: 0.06406455487012863\n",
      "Epoch 7223/30000 Training Loss: 0.04223955422639847\n",
      "Epoch 7224/30000 Training Loss: 0.07364719361066818\n",
      "Epoch 7225/30000 Training Loss: 0.06265217810869217\n",
      "Epoch 7226/30000 Training Loss: 0.05669435113668442\n",
      "Epoch 7227/30000 Training Loss: 0.03603441268205643\n",
      "Epoch 7228/30000 Training Loss: 0.04647515341639519\n",
      "Epoch 7229/30000 Training Loss: 0.05742982402443886\n",
      "Epoch 7230/30000 Training Loss: 0.06617103517055511\n",
      "Epoch 7231/30000 Training Loss: 0.05682503059506416\n",
      "Epoch 7232/30000 Training Loss: 0.05882148817181587\n",
      "Epoch 7233/30000 Training Loss: 0.03120456263422966\n",
      "Epoch 7234/30000 Training Loss: 0.06856560707092285\n",
      "Epoch 7235/30000 Training Loss: 0.06200936436653137\n",
      "Epoch 7236/30000 Training Loss: 0.05017709732055664\n",
      "Epoch 7237/30000 Training Loss: 0.06834965199232101\n",
      "Epoch 7238/30000 Training Loss: 0.06547418236732483\n",
      "Epoch 7239/30000 Training Loss: 0.060551781207323074\n",
      "Epoch 7240/30000 Training Loss: 0.05357098579406738\n",
      "Epoch 7241/30000 Training Loss: 0.055262040346860886\n",
      "Epoch 7242/30000 Training Loss: 0.052140600979328156\n",
      "Epoch 7243/30000 Training Loss: 0.07430826872587204\n",
      "Epoch 7244/30000 Training Loss: 0.04888425022363663\n",
      "Epoch 7245/30000 Training Loss: 0.05523020029067993\n",
      "Epoch 7246/30000 Training Loss: 0.054566770792007446\n",
      "Epoch 7247/30000 Training Loss: 0.051346443593502045\n",
      "Epoch 7248/30000 Training Loss: 0.06245625019073486\n",
      "Epoch 7249/30000 Training Loss: 0.0539558082818985\n",
      "Epoch 7250/30000 Training Loss: 0.040401991456747055\n",
      "Epoch 7251/30000 Training Loss: 0.06198993697762489\n",
      "Epoch 7252/30000 Training Loss: 0.057716045528650284\n",
      "Epoch 7253/30000 Training Loss: 0.051495201885700226\n",
      "Epoch 7254/30000 Training Loss: 0.0674675926566124\n",
      "Epoch 7255/30000 Training Loss: 0.05851857364177704\n",
      "Epoch 7256/30000 Training Loss: 0.05117351561784744\n",
      "Epoch 7257/30000 Training Loss: 0.055850960314273834\n",
      "Epoch 7258/30000 Training Loss: 0.04810899868607521\n",
      "Epoch 7259/30000 Training Loss: 0.04899100214242935\n",
      "Epoch 7260/30000 Training Loss: 0.05972711741924286\n",
      "Epoch 7261/30000 Training Loss: 0.05001162365078926\n",
      "Epoch 7262/30000 Training Loss: 0.06613834202289581\n",
      "Epoch 7263/30000 Training Loss: 0.053287822753190994\n",
      "Epoch 7264/30000 Training Loss: 0.06939200311899185\n",
      "Epoch 7265/30000 Training Loss: 0.049235157668590546\n",
      "Epoch 7266/30000 Training Loss: 0.05487901717424393\n",
      "Epoch 7267/30000 Training Loss: 0.04522028565406799\n",
      "Epoch 7268/30000 Training Loss: 0.04493791610002518\n",
      "Epoch 7269/30000 Training Loss: 0.05230873078107834\n",
      "Epoch 7270/30000 Training Loss: 0.04291119799017906\n",
      "Epoch 7271/30000 Training Loss: 0.05826232209801674\n",
      "Epoch 7272/30000 Training Loss: 0.057224079966545105\n",
      "Epoch 7273/30000 Training Loss: 0.04559583589434624\n",
      "Epoch 7274/30000 Training Loss: 0.053851205855607986\n",
      "Epoch 7275/30000 Training Loss: 0.04624199867248535\n",
      "Epoch 7276/30000 Training Loss: 0.053212739527225494\n",
      "Epoch 7277/30000 Training Loss: 0.048370834439992905\n",
      "Epoch 7278/30000 Training Loss: 0.05150824412703514\n",
      "Epoch 7279/30000 Training Loss: 0.06569232046604156\n",
      "Epoch 7280/30000 Training Loss: 0.062317878007888794\n",
      "Epoch 7281/30000 Training Loss: 0.06432211399078369\n",
      "Epoch 7282/30000 Training Loss: 0.05560595542192459\n",
      "Epoch 7283/30000 Training Loss: 0.07540817558765411\n",
      "Epoch 7284/30000 Training Loss: 0.05241673067212105\n",
      "Epoch 7285/30000 Training Loss: 0.055424101650714874\n",
      "Epoch 7286/30000 Training Loss: 0.05938686057925224\n",
      "Epoch 7287/30000 Training Loss: 0.0462430939078331\n",
      "Epoch 7288/30000 Training Loss: 0.07428789138793945\n",
      "Epoch 7289/30000 Training Loss: 0.06515057384967804\n",
      "Epoch 7290/30000 Training Loss: 0.06582065671682358\n",
      "Epoch 7291/30000 Training Loss: 0.044896457344293594\n",
      "Epoch 7292/30000 Training Loss: 0.07234001159667969\n",
      "Epoch 7293/30000 Training Loss: 0.07428799569606781\n",
      "Epoch 7294/30000 Training Loss: 0.0515957772731781\n",
      "Epoch 7295/30000 Training Loss: 0.059062693268060684\n",
      "Epoch 7296/30000 Training Loss: 0.07613249123096466\n",
      "Epoch 7297/30000 Training Loss: 0.05305403843522072\n",
      "Epoch 7298/30000 Training Loss: 0.04749942198395729\n",
      "Epoch 7299/30000 Training Loss: 0.04947352036833763\n",
      "Epoch 7300/30000 Training Loss: 0.06234326586127281\n",
      "Epoch 7300/30000 Validation Loss: 0.061509281396865845\n",
      "Epoch 7301/30000 Training Loss: 0.058908119797706604\n",
      "Epoch 7302/30000 Training Loss: 0.06692734360694885\n",
      "Epoch 7303/30000 Training Loss: 0.05410654842853546\n",
      "Epoch 7304/30000 Training Loss: 0.04316752403974533\n",
      "Epoch 7305/30000 Training Loss: 0.04271536320447922\n",
      "Epoch 7306/30000 Training Loss: 0.06312866508960724\n",
      "Epoch 7307/30000 Training Loss: 0.061927057802677155\n",
      "Epoch 7308/30000 Training Loss: 0.05217255651950836\n",
      "Epoch 7309/30000 Training Loss: 0.04728109389543533\n",
      "Epoch 7310/30000 Training Loss: 0.0593189038336277\n",
      "Epoch 7311/30000 Training Loss: 0.05358139052987099\n",
      "Epoch 7312/30000 Training Loss: 0.05739327892661095\n",
      "Epoch 7313/30000 Training Loss: 0.04818088561296463\n",
      "Epoch 7314/30000 Training Loss: 0.05422334745526314\n",
      "Epoch 7315/30000 Training Loss: 0.047214359045028687\n",
      "Epoch 7316/30000 Training Loss: 0.08640803396701813\n",
      "Epoch 7317/30000 Training Loss: 0.08197809010744095\n",
      "Epoch 7318/30000 Training Loss: 0.059206221252679825\n",
      "Epoch 7319/30000 Training Loss: 0.05665803700685501\n",
      "Epoch 7320/30000 Training Loss: 0.05294453352689743\n",
      "Epoch 7321/30000 Training Loss: 0.06584307551383972\n",
      "Epoch 7322/30000 Training Loss: 0.05061197280883789\n",
      "Epoch 7323/30000 Training Loss: 0.047966234385967255\n",
      "Epoch 7324/30000 Training Loss: 0.04393135383725166\n",
      "Epoch 7325/30000 Training Loss: 0.06148321181535721\n",
      "Epoch 7326/30000 Training Loss: 0.06260658800601959\n",
      "Epoch 7327/30000 Training Loss: 0.049363039433956146\n",
      "Epoch 7328/30000 Training Loss: 0.06256981194019318\n",
      "Epoch 7329/30000 Training Loss: 0.052039261907339096\n",
      "Epoch 7330/30000 Training Loss: 0.058003731071949005\n",
      "Epoch 7331/30000 Training Loss: 0.06935250759124756\n",
      "Epoch 7332/30000 Training Loss: 0.05127301439642906\n",
      "Epoch 7333/30000 Training Loss: 0.05831503868103027\n",
      "Epoch 7334/30000 Training Loss: 0.04978225380182266\n",
      "Epoch 7335/30000 Training Loss: 0.04272864758968353\n",
      "Epoch 7336/30000 Training Loss: 0.07745962589979172\n",
      "Epoch 7337/30000 Training Loss: 0.04768235236406326\n",
      "Epoch 7338/30000 Training Loss: 0.03673819825053215\n",
      "Epoch 7339/30000 Training Loss: 0.0665445476770401\n",
      "Epoch 7340/30000 Training Loss: 0.05992705374956131\n",
      "Epoch 7341/30000 Training Loss: 0.057665128260850906\n",
      "Epoch 7342/30000 Training Loss: 0.06658992171287537\n",
      "Epoch 7343/30000 Training Loss: 0.052156712859869\n",
      "Epoch 7344/30000 Training Loss: 0.049090467393398285\n",
      "Epoch 7345/30000 Training Loss: 0.05807380750775337\n",
      "Epoch 7346/30000 Training Loss: 0.06922843307256699\n",
      "Epoch 7347/30000 Training Loss: 0.053507138043642044\n",
      "Epoch 7348/30000 Training Loss: 0.054477229714393616\n",
      "Epoch 7349/30000 Training Loss: 0.04634547233581543\n",
      "Epoch 7350/30000 Training Loss: 0.06073342636227608\n",
      "Epoch 7351/30000 Training Loss: 0.03810364380478859\n",
      "Epoch 7352/30000 Training Loss: 0.06484304368495941\n",
      "Epoch 7353/30000 Training Loss: 0.06847230345010757\n",
      "Epoch 7354/30000 Training Loss: 0.044764772057533264\n",
      "Epoch 7355/30000 Training Loss: 0.06998999416828156\n",
      "Epoch 7356/30000 Training Loss: 0.04631969332695007\n",
      "Epoch 7357/30000 Training Loss: 0.06280694156885147\n",
      "Epoch 7358/30000 Training Loss: 0.05674860626459122\n",
      "Epoch 7359/30000 Training Loss: 0.05513196438550949\n",
      "Epoch 7360/30000 Training Loss: 0.058176055550575256\n",
      "Epoch 7361/30000 Training Loss: 0.05567817762494087\n",
      "Epoch 7362/30000 Training Loss: 0.061440639197826385\n",
      "Epoch 7363/30000 Training Loss: 0.05119434744119644\n",
      "Epoch 7364/30000 Training Loss: 0.06285222619771957\n",
      "Epoch 7365/30000 Training Loss: 0.053864531219005585\n",
      "Epoch 7366/30000 Training Loss: 0.0653265193104744\n",
      "Epoch 7367/30000 Training Loss: 0.052666835486888885\n",
      "Epoch 7368/30000 Training Loss: 0.06099753826856613\n",
      "Epoch 7369/30000 Training Loss: 0.06429117172956467\n",
      "Epoch 7370/30000 Training Loss: 0.061230167746543884\n",
      "Epoch 7371/30000 Training Loss: 0.047654032707214355\n",
      "Epoch 7372/30000 Training Loss: 0.06466367095708847\n",
      "Epoch 7373/30000 Training Loss: 0.05789468437433243\n",
      "Epoch 7374/30000 Training Loss: 0.05142068862915039\n",
      "Epoch 7375/30000 Training Loss: 0.05656534433364868\n",
      "Epoch 7376/30000 Training Loss: 0.07513628154993057\n",
      "Epoch 7377/30000 Training Loss: 0.05921265855431557\n",
      "Epoch 7378/30000 Training Loss: 0.053847745060920715\n",
      "Epoch 7379/30000 Training Loss: 0.05204249918460846\n",
      "Epoch 7380/30000 Training Loss: 0.06339474767446518\n",
      "Epoch 7381/30000 Training Loss: 0.05112743377685547\n",
      "Epoch 7382/30000 Training Loss: 0.06876236200332642\n",
      "Epoch 7383/30000 Training Loss: 0.04797554016113281\n",
      "Epoch 7384/30000 Training Loss: 0.07779812812805176\n",
      "Epoch 7385/30000 Training Loss: 0.039886657148599625\n",
      "Epoch 7386/30000 Training Loss: 0.075174979865551\n",
      "Epoch 7387/30000 Training Loss: 0.0463593453168869\n",
      "Epoch 7388/30000 Training Loss: 0.052464377135038376\n",
      "Epoch 7389/30000 Training Loss: 0.06057499349117279\n",
      "Epoch 7390/30000 Training Loss: 0.052361976355314255\n",
      "Epoch 7391/30000 Training Loss: 0.07000777125358582\n",
      "Epoch 7392/30000 Training Loss: 0.06674554944038391\n",
      "Epoch 7393/30000 Training Loss: 0.055966608226299286\n",
      "Epoch 7394/30000 Training Loss: 0.04972289502620697\n",
      "Epoch 7395/30000 Training Loss: 0.05887652933597565\n",
      "Epoch 7396/30000 Training Loss: 0.05998457223176956\n",
      "Epoch 7397/30000 Training Loss: 0.05387471988797188\n",
      "Epoch 7398/30000 Training Loss: 0.06409056484699249\n",
      "Epoch 7399/30000 Training Loss: 0.04876553267240524\n",
      "Epoch 7400/30000 Training Loss: 0.06759127974510193\n",
      "Epoch 7400/30000 Validation Loss: 0.07228656858205795\n",
      "Epoch 7401/30000 Training Loss: 0.07508180290460587\n",
      "Epoch 7402/30000 Training Loss: 0.04655027762055397\n",
      "Epoch 7403/30000 Training Loss: 0.0560825951397419\n",
      "Epoch 7404/30000 Training Loss: 0.04916955530643463\n",
      "Epoch 7405/30000 Training Loss: 0.06684836000204086\n",
      "Epoch 7406/30000 Training Loss: 0.08269165456295013\n",
      "Epoch 7407/30000 Training Loss: 0.04697562754154205\n",
      "Epoch 7408/30000 Training Loss: 0.058593615889549255\n",
      "Epoch 7409/30000 Training Loss: 0.06354276090860367\n",
      "Epoch 7410/30000 Training Loss: 0.051074154675006866\n",
      "Epoch 7411/30000 Training Loss: 0.055561237037181854\n",
      "Epoch 7412/30000 Training Loss: 0.06293248385190964\n",
      "Epoch 7413/30000 Training Loss: 0.060724977403879166\n",
      "Epoch 7414/30000 Training Loss: 0.05792410671710968\n",
      "Epoch 7415/30000 Training Loss: 0.06311234831809998\n",
      "Epoch 7416/30000 Training Loss: 0.05402500554919243\n",
      "Epoch 7417/30000 Training Loss: 0.05397418141365051\n",
      "Epoch 7418/30000 Training Loss: 0.07333728671073914\n",
      "Epoch 7419/30000 Training Loss: 0.06271085143089294\n",
      "Epoch 7420/30000 Training Loss: 0.06416385620832443\n",
      "Epoch 7421/30000 Training Loss: 0.06582794338464737\n",
      "Epoch 7422/30000 Training Loss: 0.06276135891675949\n",
      "Epoch 7423/30000 Training Loss: 0.053548090159893036\n",
      "Epoch 7424/30000 Training Loss: 0.06914957612752914\n",
      "Epoch 7425/30000 Training Loss: 0.05944544076919556\n",
      "Epoch 7426/30000 Training Loss: 0.05391523987054825\n",
      "Epoch 7427/30000 Training Loss: 0.06400871276855469\n",
      "Epoch 7428/30000 Training Loss: 0.05821183696389198\n",
      "Epoch 7429/30000 Training Loss: 0.05721517652273178\n",
      "Epoch 7430/30000 Training Loss: 0.04465218633413315\n",
      "Epoch 7431/30000 Training Loss: 0.06198631972074509\n",
      "Epoch 7432/30000 Training Loss: 0.05038755387067795\n",
      "Epoch 7433/30000 Training Loss: 0.0563381053507328\n",
      "Epoch 7434/30000 Training Loss: 0.06205889955163002\n",
      "Epoch 7435/30000 Training Loss: 0.03927493840456009\n",
      "Epoch 7436/30000 Training Loss: 0.06449088454246521\n",
      "Epoch 7437/30000 Training Loss: 0.03872056305408478\n",
      "Epoch 7438/30000 Training Loss: 0.06799010932445526\n",
      "Epoch 7439/30000 Training Loss: 0.058663029223680496\n",
      "Epoch 7440/30000 Training Loss: 0.051081426441669464\n",
      "Epoch 7441/30000 Training Loss: 0.04477144777774811\n",
      "Epoch 7442/30000 Training Loss: 0.05582747608423233\n",
      "Epoch 7443/30000 Training Loss: 0.07263177633285522\n",
      "Epoch 7444/30000 Training Loss: 0.05484308674931526\n",
      "Epoch 7445/30000 Training Loss: 0.08389467000961304\n",
      "Epoch 7446/30000 Training Loss: 0.0617770254611969\n",
      "Epoch 7447/30000 Training Loss: 0.06264852732419968\n",
      "Epoch 7448/30000 Training Loss: 0.06646095961332321\n",
      "Epoch 7449/30000 Training Loss: 0.06181034445762634\n",
      "Epoch 7450/30000 Training Loss: 0.06524918228387833\n",
      "Epoch 7451/30000 Training Loss: 0.06103266775608063\n",
      "Epoch 7452/30000 Training Loss: 0.04242677986621857\n",
      "Epoch 7453/30000 Training Loss: 0.07497116178274155\n",
      "Epoch 7454/30000 Training Loss: 0.05155094712972641\n",
      "Epoch 7455/30000 Training Loss: 0.06092342734336853\n",
      "Epoch 7456/30000 Training Loss: 0.058809880167245865\n",
      "Epoch 7457/30000 Training Loss: 0.058246709406375885\n",
      "Epoch 7458/30000 Training Loss: 0.07210436463356018\n",
      "Epoch 7459/30000 Training Loss: 0.06804922968149185\n",
      "Epoch 7460/30000 Training Loss: 0.058839257806539536\n",
      "Epoch 7461/30000 Training Loss: 0.045267194509506226\n",
      "Epoch 7462/30000 Training Loss: 0.04477458447217941\n",
      "Epoch 7463/30000 Training Loss: 0.05631592124700546\n",
      "Epoch 7464/30000 Training Loss: 0.07279648631811142\n",
      "Epoch 7465/30000 Training Loss: 0.06726829707622528\n",
      "Epoch 7466/30000 Training Loss: 0.05453355982899666\n",
      "Epoch 7467/30000 Training Loss: 0.05253147333860397\n",
      "Epoch 7468/30000 Training Loss: 0.05685998499393463\n",
      "Epoch 7469/30000 Training Loss: 0.048268307000398636\n",
      "Epoch 7470/30000 Training Loss: 0.04567551612854004\n",
      "Epoch 7471/30000 Training Loss: 0.04581921175122261\n",
      "Epoch 7472/30000 Training Loss: 0.06844824552536011\n",
      "Epoch 7473/30000 Training Loss: 0.05135371536016464\n",
      "Epoch 7474/30000 Training Loss: 0.06675773859024048\n",
      "Epoch 7475/30000 Training Loss: 0.06456945091485977\n",
      "Epoch 7476/30000 Training Loss: 0.07711746543645859\n",
      "Epoch 7477/30000 Training Loss: 0.06683002412319183\n",
      "Epoch 7478/30000 Training Loss: 0.06509356200695038\n",
      "Epoch 7479/30000 Training Loss: 0.05363272503018379\n",
      "Epoch 7480/30000 Training Loss: 0.053407881408929825\n",
      "Epoch 7481/30000 Training Loss: 0.05983895808458328\n",
      "Epoch 7482/30000 Training Loss: 0.061401549726724625\n",
      "Epoch 7483/30000 Training Loss: 0.05594030022621155\n",
      "Epoch 7484/30000 Training Loss: 0.04666651785373688\n",
      "Epoch 7485/30000 Training Loss: 0.05702485516667366\n",
      "Epoch 7486/30000 Training Loss: 0.050086721777915955\n",
      "Epoch 7487/30000 Training Loss: 0.052518345415592194\n",
      "Epoch 7488/30000 Training Loss: 0.06416141241788864\n",
      "Epoch 7489/30000 Training Loss: 0.050655752420425415\n",
      "Epoch 7490/30000 Training Loss: 0.05483129248023033\n",
      "Epoch 7491/30000 Training Loss: 0.07105952501296997\n",
      "Epoch 7492/30000 Training Loss: 0.06627079099416733\n",
      "Epoch 7493/30000 Training Loss: 0.06481793522834778\n",
      "Epoch 7494/30000 Training Loss: 0.06390032172203064\n",
      "Epoch 7495/30000 Training Loss: 0.06702467054128647\n",
      "Epoch 7496/30000 Training Loss: 0.05601565167307854\n",
      "Epoch 7497/30000 Training Loss: 0.05480610951781273\n",
      "Epoch 7498/30000 Training Loss: 0.06460154056549072\n",
      "Epoch 7499/30000 Training Loss: 0.04961808770895004\n",
      "Epoch 7500/30000 Training Loss: 0.07131533324718475\n",
      "Epoch 7500/30000 Validation Loss: 0.05779370293021202\n",
      "Epoch 7501/30000 Training Loss: 0.047169029712677\n",
      "Epoch 7502/30000 Training Loss: 0.06193356588482857\n",
      "Epoch 7503/30000 Training Loss: 0.05335467308759689\n",
      "Epoch 7504/30000 Training Loss: 0.04390028119087219\n",
      "Epoch 7505/30000 Training Loss: 0.05606089159846306\n",
      "Epoch 7506/30000 Training Loss: 0.0523446761071682\n",
      "Epoch 7507/30000 Training Loss: 0.045519329607486725\n",
      "Epoch 7508/30000 Training Loss: 0.060721904039382935\n",
      "Epoch 7509/30000 Training Loss: 0.051903363317251205\n",
      "Epoch 7510/30000 Training Loss: 0.05255063623189926\n",
      "Epoch 7511/30000 Training Loss: 0.05903736874461174\n",
      "Epoch 7512/30000 Training Loss: 0.06202155724167824\n",
      "Epoch 7513/30000 Training Loss: 0.04914676398038864\n",
      "Epoch 7514/30000 Training Loss: 0.04414008930325508\n",
      "Epoch 7515/30000 Training Loss: 0.05451326072216034\n",
      "Epoch 7516/30000 Training Loss: 0.07300873100757599\n",
      "Epoch 7517/30000 Training Loss: 0.05158267170190811\n",
      "Epoch 7518/30000 Training Loss: 0.06928293406963348\n",
      "Epoch 7519/30000 Training Loss: 0.04925822839140892\n",
      "Epoch 7520/30000 Training Loss: 0.04960361123085022\n",
      "Epoch 7521/30000 Training Loss: 0.07856949418783188\n",
      "Epoch 7522/30000 Training Loss: 0.060411930084228516\n",
      "Epoch 7523/30000 Training Loss: 0.05204909294843674\n",
      "Epoch 7524/30000 Training Loss: 0.046357326209545135\n",
      "Epoch 7525/30000 Training Loss: 0.0488632395863533\n",
      "Epoch 7526/30000 Training Loss: 0.06821011751890182\n",
      "Epoch 7527/30000 Training Loss: 0.05669865757226944\n",
      "Epoch 7528/30000 Training Loss: 0.052456311881542206\n",
      "Epoch 7529/30000 Training Loss: 0.06372290849685669\n",
      "Epoch 7530/30000 Training Loss: 0.06141940504312515\n",
      "Epoch 7531/30000 Training Loss: 0.04849720373749733\n",
      "Epoch 7532/30000 Training Loss: 0.04840521141886711\n",
      "Epoch 7533/30000 Training Loss: 0.054708994925022125\n",
      "Epoch 7534/30000 Training Loss: 0.06454123556613922\n",
      "Epoch 7535/30000 Training Loss: 0.06396643817424774\n",
      "Epoch 7536/30000 Training Loss: 0.05902594327926636\n",
      "Epoch 7537/30000 Training Loss: 0.06259806454181671\n",
      "Epoch 7538/30000 Training Loss: 0.05732212960720062\n",
      "Epoch 7539/30000 Training Loss: 0.06166554242372513\n",
      "Epoch 7540/30000 Training Loss: 0.06479174643754959\n",
      "Epoch 7541/30000 Training Loss: 0.058679208159446716\n",
      "Epoch 7542/30000 Training Loss: 0.05490338057279587\n",
      "Epoch 7543/30000 Training Loss: 0.062113985419273376\n",
      "Epoch 7544/30000 Training Loss: 0.04882727935910225\n",
      "Epoch 7545/30000 Training Loss: 0.047297682613134384\n",
      "Epoch 7546/30000 Training Loss: 0.0516456700861454\n",
      "Epoch 7547/30000 Training Loss: 0.03531023859977722\n",
      "Epoch 7548/30000 Training Loss: 0.062442850321531296\n",
      "Epoch 7549/30000 Training Loss: 0.07677242904901505\n",
      "Epoch 7550/30000 Training Loss: 0.04869608208537102\n",
      "Epoch 7551/30000 Training Loss: 0.04626673832535744\n",
      "Epoch 7552/30000 Training Loss: 0.044739194214344025\n",
      "Epoch 7553/30000 Training Loss: 0.059687696397304535\n",
      "Epoch 7554/30000 Training Loss: 0.05316447094082832\n",
      "Epoch 7555/30000 Training Loss: 0.049442093819379807\n",
      "Epoch 7556/30000 Training Loss: 0.06641515344381332\n",
      "Epoch 7557/30000 Training Loss: 0.050196900963783264\n",
      "Epoch 7558/30000 Training Loss: 0.05118037015199661\n",
      "Epoch 7559/30000 Training Loss: 0.06557067483663559\n",
      "Epoch 7560/30000 Training Loss: 0.046654656529426575\n",
      "Epoch 7561/30000 Training Loss: 0.050292905420064926\n",
      "Epoch 7562/30000 Training Loss: 0.055887676775455475\n",
      "Epoch 7563/30000 Training Loss: 0.0556369312107563\n",
      "Epoch 7564/30000 Training Loss: 0.05092594027519226\n",
      "Epoch 7565/30000 Training Loss: 0.06506741791963577\n",
      "Epoch 7566/30000 Training Loss: 0.05140386521816254\n",
      "Epoch 7567/30000 Training Loss: 0.07163269072771072\n",
      "Epoch 7568/30000 Training Loss: 0.06736232340335846\n",
      "Epoch 7569/30000 Training Loss: 0.04943385347723961\n",
      "Epoch 7570/30000 Training Loss: 0.059307582676410675\n",
      "Epoch 7571/30000 Training Loss: 0.0430515855550766\n",
      "Epoch 7572/30000 Training Loss: 0.0620545893907547\n",
      "Epoch 7573/30000 Training Loss: 0.04973255842924118\n",
      "Epoch 7574/30000 Training Loss: 0.06949969381093979\n",
      "Epoch 7575/30000 Training Loss: 0.048662856221199036\n",
      "Epoch 7576/30000 Training Loss: 0.05711650848388672\n",
      "Epoch 7577/30000 Training Loss: 0.04895414784550667\n",
      "Epoch 7578/30000 Training Loss: 0.06997402757406235\n",
      "Epoch 7579/30000 Training Loss: 0.06200747936964035\n",
      "Epoch 7580/30000 Training Loss: 0.07136313617229462\n",
      "Epoch 7581/30000 Training Loss: 0.04638956859707832\n",
      "Epoch 7582/30000 Training Loss: 0.056372079998254776\n",
      "Epoch 7583/30000 Training Loss: 0.0442323200404644\n",
      "Epoch 7584/30000 Training Loss: 0.07259880006313324\n",
      "Epoch 7585/30000 Training Loss: 0.05290573835372925\n",
      "Epoch 7586/30000 Training Loss: 0.05266324430704117\n",
      "Epoch 7587/30000 Training Loss: 0.05134470760822296\n",
      "Epoch 7588/30000 Training Loss: 0.06687258929014206\n",
      "Epoch 7589/30000 Training Loss: 0.05985163897275925\n",
      "Epoch 7590/30000 Training Loss: 0.059073787182569504\n",
      "Epoch 7591/30000 Training Loss: 0.0506366528570652\n",
      "Epoch 7592/30000 Training Loss: 0.039621319621801376\n",
      "Epoch 7593/30000 Training Loss: 0.06833262741565704\n",
      "Epoch 7594/30000 Training Loss: 0.06916319578886032\n",
      "Epoch 7595/30000 Training Loss: 0.050402894616127014\n",
      "Epoch 7596/30000 Training Loss: 0.0643576979637146\n",
      "Epoch 7597/30000 Training Loss: 0.08945963531732559\n",
      "Epoch 7598/30000 Training Loss: 0.04784158617258072\n",
      "Epoch 7599/30000 Training Loss: 0.0688103586435318\n",
      "Epoch 7600/30000 Training Loss: 0.0757494792342186\n",
      "Epoch 7600/30000 Validation Loss: 0.060225073248147964\n",
      "Epoch 7601/30000 Training Loss: 0.05152427405118942\n",
      "Epoch 7602/30000 Training Loss: 0.054946355521678925\n",
      "Epoch 7603/30000 Training Loss: 0.05020924285054207\n",
      "Epoch 7604/30000 Training Loss: 0.06661862879991531\n",
      "Epoch 7605/30000 Training Loss: 0.06297290325164795\n",
      "Epoch 7606/30000 Training Loss: 0.05206291750073433\n",
      "Epoch 7607/30000 Training Loss: 0.06285552680492401\n",
      "Epoch 7608/30000 Training Loss: 0.039114080369472504\n",
      "Epoch 7609/30000 Training Loss: 0.03978225588798523\n",
      "Epoch 7610/30000 Training Loss: 0.04404428228735924\n",
      "Epoch 7611/30000 Training Loss: 0.05168827623128891\n",
      "Epoch 7612/30000 Training Loss: 0.0692124292254448\n",
      "Epoch 7613/30000 Training Loss: 0.05190218612551689\n",
      "Epoch 7614/30000 Training Loss: 0.05735457316040993\n",
      "Epoch 7615/30000 Training Loss: 0.047787345945835114\n",
      "Epoch 7616/30000 Training Loss: 0.06801509857177734\n",
      "Epoch 7617/30000 Training Loss: 0.04983961954712868\n",
      "Epoch 7618/30000 Training Loss: 0.05400421470403671\n",
      "Epoch 7619/30000 Training Loss: 0.06821750849485397\n",
      "Epoch 7620/30000 Training Loss: 0.04466591775417328\n",
      "Epoch 7621/30000 Training Loss: 0.04809848219156265\n",
      "Epoch 7622/30000 Training Loss: 0.07543149590492249\n",
      "Epoch 7623/30000 Training Loss: 0.0508374460041523\n",
      "Epoch 7624/30000 Training Loss: 0.04205701872706413\n",
      "Epoch 7625/30000 Training Loss: 0.05726902559399605\n",
      "Epoch 7626/30000 Training Loss: 0.05460251122713089\n",
      "Epoch 7627/30000 Training Loss: 0.0515998899936676\n",
      "Epoch 7628/30000 Training Loss: 0.04974938929080963\n",
      "Epoch 7629/30000 Training Loss: 0.06403496116399765\n",
      "Epoch 7630/30000 Training Loss: 0.05700484663248062\n",
      "Epoch 7631/30000 Training Loss: 0.07210103422403336\n",
      "Epoch 7632/30000 Training Loss: 0.06646385788917542\n",
      "Epoch 7633/30000 Training Loss: 0.05433424934744835\n",
      "Epoch 7634/30000 Training Loss: 0.06170233339071274\n",
      "Epoch 7635/30000 Training Loss: 0.059830307960510254\n",
      "Epoch 7636/30000 Training Loss: 0.04468571022152901\n",
      "Epoch 7637/30000 Training Loss: 0.062184348702430725\n",
      "Epoch 7638/30000 Training Loss: 0.07072114944458008\n",
      "Epoch 7639/30000 Training Loss: 0.04378017038106918\n",
      "Epoch 7640/30000 Training Loss: 0.07090704888105392\n",
      "Epoch 7641/30000 Training Loss: 0.05502724647521973\n",
      "Epoch 7642/30000 Training Loss: 0.05341998487710953\n",
      "Epoch 7643/30000 Training Loss: 0.06720023602247238\n",
      "Epoch 7644/30000 Training Loss: 0.05493533983826637\n",
      "Epoch 7645/30000 Training Loss: 0.05439646542072296\n",
      "Epoch 7646/30000 Training Loss: 0.061126455664634705\n",
      "Epoch 7647/30000 Training Loss: 0.056504279375076294\n",
      "Epoch 7648/30000 Training Loss: 0.05168984830379486\n",
      "Epoch 7649/30000 Training Loss: 0.0530032217502594\n",
      "Epoch 7650/30000 Training Loss: 0.045841459184885025\n",
      "Epoch 7651/30000 Training Loss: 0.06509218364953995\n",
      "Epoch 7652/30000 Training Loss: 0.05587845295667648\n",
      "Epoch 7653/30000 Training Loss: 0.07488709688186646\n",
      "Epoch 7654/30000 Training Loss: 0.05704442411661148\n",
      "Epoch 7655/30000 Training Loss: 0.05389027297496796\n",
      "Epoch 7656/30000 Training Loss: 0.05238473415374756\n",
      "Epoch 7657/30000 Training Loss: 0.05363280698657036\n",
      "Epoch 7658/30000 Training Loss: 0.04586847126483917\n",
      "Epoch 7659/30000 Training Loss: 0.0609772652387619\n",
      "Epoch 7660/30000 Training Loss: 0.0620824433863163\n",
      "Epoch 7661/30000 Training Loss: 0.07094117999076843\n",
      "Epoch 7662/30000 Training Loss: 0.04972323030233383\n",
      "Epoch 7663/30000 Training Loss: 0.058114953339099884\n",
      "Epoch 7664/30000 Training Loss: 0.04608339071273804\n",
      "Epoch 7665/30000 Training Loss: 0.04858585074543953\n",
      "Epoch 7666/30000 Training Loss: 0.05594475567340851\n",
      "Epoch 7667/30000 Training Loss: 0.05768727511167526\n",
      "Epoch 7668/30000 Training Loss: 0.049708735197782516\n",
      "Epoch 7669/30000 Training Loss: 0.03677443787455559\n",
      "Epoch 7670/30000 Training Loss: 0.05089423060417175\n",
      "Epoch 7671/30000 Training Loss: 0.059927798807621\n",
      "Epoch 7672/30000 Training Loss: 0.06289071589708328\n",
      "Epoch 7673/30000 Training Loss: 0.0492536760866642\n",
      "Epoch 7674/30000 Training Loss: 0.057073600590229034\n",
      "Epoch 7675/30000 Training Loss: 0.05821406841278076\n",
      "Epoch 7676/30000 Training Loss: 0.05329521745443344\n",
      "Epoch 7677/30000 Training Loss: 0.050335679203271866\n",
      "Epoch 7678/30000 Training Loss: 0.062102530151605606\n",
      "Epoch 7679/30000 Training Loss: 0.0466148778796196\n",
      "Epoch 7680/30000 Training Loss: 0.06387780606746674\n",
      "Epoch 7681/30000 Training Loss: 0.054020412266254425\n",
      "Epoch 7682/30000 Training Loss: 0.04359636828303337\n",
      "Epoch 7683/30000 Training Loss: 0.07196684181690216\n",
      "Epoch 7684/30000 Training Loss: 0.05725448578596115\n",
      "Epoch 7685/30000 Training Loss: 0.05649353936314583\n",
      "Epoch 7686/30000 Training Loss: 0.048698678612709045\n",
      "Epoch 7687/30000 Training Loss: 0.07048042118549347\n",
      "Epoch 7688/30000 Training Loss: 0.051892705261707306\n",
      "Epoch 7689/30000 Training Loss: 0.06875405460596085\n",
      "Epoch 7690/30000 Training Loss: 0.048729684203863144\n",
      "Epoch 7691/30000 Training Loss: 0.06692371517419815\n",
      "Epoch 7692/30000 Training Loss: 0.06721319258213043\n",
      "Epoch 7693/30000 Training Loss: 0.04156086593866348\n",
      "Epoch 7694/30000 Training Loss: 0.046480000019073486\n",
      "Epoch 7695/30000 Training Loss: 0.05291843041777611\n",
      "Epoch 7696/30000 Training Loss: 0.07730815559625626\n",
      "Epoch 7697/30000 Training Loss: 0.061308592557907104\n",
      "Epoch 7698/30000 Training Loss: 0.08135505765676498\n",
      "Epoch 7699/30000 Training Loss: 0.042846404016017914\n",
      "Epoch 7700/30000 Training Loss: 0.045413751155138016\n",
      "Epoch 7700/30000 Validation Loss: 0.05575963109731674\n",
      "Epoch 7701/30000 Training Loss: 0.05432213842868805\n",
      "Epoch 7702/30000 Training Loss: 0.06418150663375854\n",
      "Epoch 7703/30000 Training Loss: 0.052567195147275925\n",
      "Epoch 7704/30000 Training Loss: 0.056042369455099106\n",
      "Epoch 7705/30000 Training Loss: 0.07118027657270432\n",
      "Epoch 7706/30000 Training Loss: 0.06043274700641632\n",
      "Epoch 7707/30000 Training Loss: 0.052408069372177124\n",
      "Epoch 7708/30000 Training Loss: 0.056151434779167175\n",
      "Epoch 7709/30000 Training Loss: 0.05313250422477722\n",
      "Epoch 7710/30000 Training Loss: 0.054388176649808884\n",
      "Epoch 7711/30000 Training Loss: 0.055251166224479675\n",
      "Epoch 7712/30000 Training Loss: 0.049196191132068634\n",
      "Epoch 7713/30000 Training Loss: 0.05759420618414879\n",
      "Epoch 7714/30000 Training Loss: 0.05157461017370224\n",
      "Epoch 7715/30000 Training Loss: 0.053811073303222656\n",
      "Epoch 7716/30000 Training Loss: 0.05208524689078331\n",
      "Epoch 7717/30000 Training Loss: 0.05560041218996048\n",
      "Epoch 7718/30000 Training Loss: 0.05474890395998955\n",
      "Epoch 7719/30000 Training Loss: 0.07908623665571213\n",
      "Epoch 7720/30000 Training Loss: 0.05279207602143288\n",
      "Epoch 7721/30000 Training Loss: 0.07514041662216187\n",
      "Epoch 7722/30000 Training Loss: 0.06450973451137543\n",
      "Epoch 7723/30000 Training Loss: 0.04177383705973625\n",
      "Epoch 7724/30000 Training Loss: 0.0530565120279789\n",
      "Epoch 7725/30000 Training Loss: 0.05371538922190666\n",
      "Epoch 7726/30000 Training Loss: 0.06403257697820663\n",
      "Epoch 7727/30000 Training Loss: 0.061444249004125595\n",
      "Epoch 7728/30000 Training Loss: 0.057916365563869476\n",
      "Epoch 7729/30000 Training Loss: 0.054443009197711945\n",
      "Epoch 7730/30000 Training Loss: 0.06620508432388306\n",
      "Epoch 7731/30000 Training Loss: 0.06663458049297333\n",
      "Epoch 7732/30000 Training Loss: 0.06657140702009201\n",
      "Epoch 7733/30000 Training Loss: 0.05242455378174782\n",
      "Epoch 7734/30000 Training Loss: 0.06585060060024261\n",
      "Epoch 7735/30000 Training Loss: 0.05091642215847969\n",
      "Epoch 7736/30000 Training Loss: 0.05615607649087906\n",
      "Epoch 7737/30000 Training Loss: 0.07456393539905548\n",
      "Epoch 7738/30000 Training Loss: 0.04238482937216759\n",
      "Epoch 7739/30000 Training Loss: 0.046459492295980453\n",
      "Epoch 7740/30000 Training Loss: 0.04414074495434761\n",
      "Epoch 7741/30000 Training Loss: 0.051956962794065475\n",
      "Epoch 7742/30000 Training Loss: 0.054616354405879974\n",
      "Epoch 7743/30000 Training Loss: 0.05567478388547897\n",
      "Epoch 7744/30000 Training Loss: 0.03464943915605545\n",
      "Epoch 7745/30000 Training Loss: 0.061232492327690125\n",
      "Epoch 7746/30000 Training Loss: 0.05532287061214447\n",
      "Epoch 7747/30000 Training Loss: 0.04894011840224266\n",
      "Epoch 7748/30000 Training Loss: 0.04931120574474335\n",
      "Epoch 7749/30000 Training Loss: 0.0656471773982048\n",
      "Epoch 7750/30000 Training Loss: 0.06916458159685135\n",
      "Epoch 7751/30000 Training Loss: 0.05149657651782036\n",
      "Epoch 7752/30000 Training Loss: 0.04346312955021858\n",
      "Epoch 7753/30000 Training Loss: 0.05022875592112541\n",
      "Epoch 7754/30000 Training Loss: 0.04058457911014557\n",
      "Epoch 7755/30000 Training Loss: 0.06179928779602051\n",
      "Epoch 7756/30000 Training Loss: 0.06240982934832573\n",
      "Epoch 7757/30000 Training Loss: 0.04619390890002251\n",
      "Epoch 7758/30000 Training Loss: 0.05902807414531708\n",
      "Epoch 7759/30000 Training Loss: 0.06487855315208435\n",
      "Epoch 7760/30000 Training Loss: 0.04365801066160202\n",
      "Epoch 7761/30000 Training Loss: 0.04446529597043991\n",
      "Epoch 7762/30000 Training Loss: 0.06070683151483536\n",
      "Epoch 7763/30000 Training Loss: 0.058955006301403046\n",
      "Epoch 7764/30000 Training Loss: 0.06654633581638336\n",
      "Epoch 7765/30000 Training Loss: 0.0541071780025959\n",
      "Epoch 7766/30000 Training Loss: 0.051636990159749985\n",
      "Epoch 7767/30000 Training Loss: 0.06477000564336777\n",
      "Epoch 7768/30000 Training Loss: 0.05147112160921097\n",
      "Epoch 7769/30000 Training Loss: 0.05954885110259056\n",
      "Epoch 7770/30000 Training Loss: 0.06388700753450394\n",
      "Epoch 7771/30000 Training Loss: 0.0510372668504715\n",
      "Epoch 7772/30000 Training Loss: 0.05254983529448509\n",
      "Epoch 7773/30000 Training Loss: 0.05495502054691315\n",
      "Epoch 7774/30000 Training Loss: 0.04910602048039436\n",
      "Epoch 7775/30000 Training Loss: 0.07128370553255081\n",
      "Epoch 7776/30000 Training Loss: 0.06049748882651329\n",
      "Epoch 7777/30000 Training Loss: 0.056381359696388245\n",
      "Epoch 7778/30000 Training Loss: 0.05657435208559036\n",
      "Epoch 7779/30000 Training Loss: 0.06938669830560684\n",
      "Epoch 7780/30000 Training Loss: 0.048636943101882935\n",
      "Epoch 7781/30000 Training Loss: 0.07158669829368591\n",
      "Epoch 7782/30000 Training Loss: 0.05296453461050987\n",
      "Epoch 7783/30000 Training Loss: 0.05404287576675415\n",
      "Epoch 7784/30000 Training Loss: 0.051476601511240005\n",
      "Epoch 7785/30000 Training Loss: 0.05230046063661575\n",
      "Epoch 7786/30000 Training Loss: 0.05016836151480675\n",
      "Epoch 7787/30000 Training Loss: 0.0593731664121151\n",
      "Epoch 7788/30000 Training Loss: 0.06334613263607025\n",
      "Epoch 7789/30000 Training Loss: 0.04332193732261658\n",
      "Epoch 7790/30000 Training Loss: 0.060362838208675385\n",
      "Epoch 7791/30000 Training Loss: 0.04906419664621353\n",
      "Epoch 7792/30000 Training Loss: 0.0841013640165329\n",
      "Epoch 7793/30000 Training Loss: 0.05023956671357155\n",
      "Epoch 7794/30000 Training Loss: 0.052567679435014725\n",
      "Epoch 7795/30000 Training Loss: 0.0747758075594902\n",
      "Epoch 7796/30000 Training Loss: 0.06312038749456406\n",
      "Epoch 7797/30000 Training Loss: 0.0561753585934639\n",
      "Epoch 7798/30000 Training Loss: 0.04380880668759346\n",
      "Epoch 7799/30000 Training Loss: 0.06188278645277023\n",
      "Epoch 7800/30000 Training Loss: 0.03989790752530098\n",
      "Epoch 7800/30000 Validation Loss: 0.07583172619342804\n",
      "Epoch 7801/30000 Training Loss: 0.06719290465116501\n",
      "Epoch 7802/30000 Training Loss: 0.0603710375726223\n",
      "Epoch 7803/30000 Training Loss: 0.04661993309855461\n",
      "Epoch 7804/30000 Training Loss: 0.04670950770378113\n",
      "Epoch 7805/30000 Training Loss: 0.06576362252235413\n",
      "Epoch 7806/30000 Training Loss: 0.055666692554950714\n",
      "Epoch 7807/30000 Training Loss: 0.054826200008392334\n",
      "Epoch 7808/30000 Training Loss: 0.04327058047056198\n",
      "Epoch 7809/30000 Training Loss: 0.05503319576382637\n",
      "Epoch 7810/30000 Training Loss: 0.05166906863451004\n",
      "Epoch 7811/30000 Training Loss: 0.055563248693943024\n",
      "Epoch 7812/30000 Training Loss: 0.04514862969517708\n",
      "Epoch 7813/30000 Training Loss: 0.06149749085307121\n",
      "Epoch 7814/30000 Training Loss: 0.05020323395729065\n",
      "Epoch 7815/30000 Training Loss: 0.0560481958091259\n",
      "Epoch 7816/30000 Training Loss: 0.05366000533103943\n",
      "Epoch 7817/30000 Training Loss: 0.056938618421554565\n",
      "Epoch 7818/30000 Training Loss: 0.0438154861330986\n",
      "Epoch 7819/30000 Training Loss: 0.0497601144015789\n",
      "Epoch 7820/30000 Training Loss: 0.05506804585456848\n",
      "Epoch 7821/30000 Training Loss: 0.05253604054450989\n",
      "Epoch 7822/30000 Training Loss: 0.05454271286725998\n",
      "Epoch 7823/30000 Training Loss: 0.04812939092516899\n",
      "Epoch 7824/30000 Training Loss: 0.051840368658304214\n",
      "Epoch 7825/30000 Training Loss: 0.05844418704509735\n",
      "Epoch 7826/30000 Training Loss: 0.0408906564116478\n",
      "Epoch 7827/30000 Training Loss: 0.05304575711488724\n",
      "Epoch 7828/30000 Training Loss: 0.040827676653862\n",
      "Epoch 7829/30000 Training Loss: 0.04779840260744095\n",
      "Epoch 7830/30000 Training Loss: 0.04359405115246773\n",
      "Epoch 7831/30000 Training Loss: 0.05325711891055107\n",
      "Epoch 7832/30000 Training Loss: 0.058322858065366745\n",
      "Epoch 7833/30000 Training Loss: 0.056984350085258484\n",
      "Epoch 7834/30000 Training Loss: 0.04828820377588272\n",
      "Epoch 7835/30000 Training Loss: 0.05999284237623215\n",
      "Epoch 7836/30000 Training Loss: 0.04984244331717491\n",
      "Epoch 7837/30000 Training Loss: 0.0650850236415863\n",
      "Epoch 7838/30000 Training Loss: 0.04588448256254196\n",
      "Epoch 7839/30000 Training Loss: 0.06366366893053055\n",
      "Epoch 7840/30000 Training Loss: 0.060631364583969116\n",
      "Epoch 7841/30000 Training Loss: 0.05837734043598175\n",
      "Epoch 7842/30000 Training Loss: 0.054649971425533295\n",
      "Epoch 7843/30000 Training Loss: 0.061045050621032715\n",
      "Epoch 7844/30000 Training Loss: 0.04712333530187607\n",
      "Epoch 7845/30000 Training Loss: 0.06174640730023384\n",
      "Epoch 7846/30000 Training Loss: 0.05487048253417015\n",
      "Epoch 7847/30000 Training Loss: 0.04711078107357025\n",
      "Epoch 7848/30000 Training Loss: 0.05719525367021561\n",
      "Epoch 7849/30000 Training Loss: 0.07726356387138367\n",
      "Epoch 7850/30000 Training Loss: 0.05510297045111656\n",
      "Epoch 7851/30000 Training Loss: 0.062764011323452\n",
      "Epoch 7852/30000 Training Loss: 0.05765480548143387\n",
      "Epoch 7853/30000 Training Loss: 0.05148191750049591\n",
      "Epoch 7854/30000 Training Loss: 0.045189566910266876\n",
      "Epoch 7855/30000 Training Loss: 0.04677169397473335\n",
      "Epoch 7856/30000 Training Loss: 0.06855326890945435\n",
      "Epoch 7857/30000 Training Loss: 0.050433650612831116\n",
      "Epoch 7858/30000 Training Loss: 0.053480781614780426\n",
      "Epoch 7859/30000 Training Loss: 0.04356423020362854\n",
      "Epoch 7860/30000 Training Loss: 0.06363486498594284\n",
      "Epoch 7861/30000 Training Loss: 0.042223744094371796\n",
      "Epoch 7862/30000 Training Loss: 0.0563858225941658\n",
      "Epoch 7863/30000 Training Loss: 0.06118255853652954\n",
      "Epoch 7864/30000 Training Loss: 0.048553451895713806\n",
      "Epoch 7865/30000 Training Loss: 0.062156155705451965\n",
      "Epoch 7866/30000 Training Loss: 0.0490993931889534\n",
      "Epoch 7867/30000 Training Loss: 0.035797201097011566\n",
      "Epoch 7868/30000 Training Loss: 0.060150984674692154\n",
      "Epoch 7869/30000 Training Loss: 0.045333102345466614\n",
      "Epoch 7870/30000 Training Loss: 0.06888458132743835\n",
      "Epoch 7871/30000 Training Loss: 0.054638978093862534\n",
      "Epoch 7872/30000 Training Loss: 0.05654280260205269\n",
      "Epoch 7873/30000 Training Loss: 0.0684858188033104\n",
      "Epoch 7874/30000 Training Loss: 0.049764253199100494\n",
      "Epoch 7875/30000 Training Loss: 0.05789618939161301\n",
      "Epoch 7876/30000 Training Loss: 0.05350813269615173\n",
      "Epoch 7877/30000 Training Loss: 0.04228585585951805\n",
      "Epoch 7878/30000 Training Loss: 0.04724675044417381\n",
      "Epoch 7879/30000 Training Loss: 0.05077584832906723\n",
      "Epoch 7880/30000 Training Loss: 0.049321986734867096\n",
      "Epoch 7881/30000 Training Loss: 0.047173600643873215\n",
      "Epoch 7882/30000 Training Loss: 0.05586070567369461\n",
      "Epoch 7883/30000 Training Loss: 0.06474951654672623\n",
      "Epoch 7884/30000 Training Loss: 0.04943879693746567\n",
      "Epoch 7885/30000 Training Loss: 0.06317944824695587\n",
      "Epoch 7886/30000 Training Loss: 0.04803800210356712\n",
      "Epoch 7887/30000 Training Loss: 0.0648007020354271\n",
      "Epoch 7888/30000 Training Loss: 0.07062939554452896\n",
      "Epoch 7889/30000 Training Loss: 0.060723502188920975\n",
      "Epoch 7890/30000 Training Loss: 0.053580351173877716\n",
      "Epoch 7891/30000 Training Loss: 0.053297802805900574\n",
      "Epoch 7892/30000 Training Loss: 0.06388447433710098\n",
      "Epoch 7893/30000 Training Loss: 0.05079793184995651\n",
      "Epoch 7894/30000 Training Loss: 0.044441282749176025\n",
      "Epoch 7895/30000 Training Loss: 0.06964581459760666\n",
      "Epoch 7896/30000 Training Loss: 0.046517983078956604\n",
      "Epoch 7897/30000 Training Loss: 0.05740375444293022\n",
      "Epoch 7898/30000 Training Loss: 0.06974708288908005\n",
      "Epoch 7899/30000 Training Loss: 0.05421457439661026\n",
      "Epoch 7900/30000 Training Loss: 0.055770039558410645\n",
      "Epoch 7900/30000 Validation Loss: 0.059377193450927734\n",
      "Epoch 7901/30000 Training Loss: 0.06153672933578491\n",
      "Epoch 7902/30000 Training Loss: 0.055750660598278046\n",
      "Epoch 7903/30000 Training Loss: 0.08320383727550507\n",
      "Epoch 7904/30000 Training Loss: 0.05217243731021881\n",
      "Epoch 7905/30000 Training Loss: 0.043924059718847275\n",
      "Epoch 7906/30000 Training Loss: 0.04330963268876076\n",
      "Epoch 7907/30000 Training Loss: 0.06158408522605896\n",
      "Epoch 7908/30000 Training Loss: 0.04966938495635986\n",
      "Epoch 7909/30000 Training Loss: 0.057085998356342316\n",
      "Epoch 7910/30000 Training Loss: 0.05177731812000275\n",
      "Epoch 7911/30000 Training Loss: 0.05722559988498688\n",
      "Epoch 7912/30000 Training Loss: 0.04878242686390877\n",
      "Epoch 7913/30000 Training Loss: 0.04873599112033844\n",
      "Epoch 7914/30000 Training Loss: 0.06310965120792389\n",
      "Epoch 7915/30000 Training Loss: 0.06578539311885834\n",
      "Epoch 7916/30000 Training Loss: 0.04875975102186203\n",
      "Epoch 7917/30000 Training Loss: 0.05932297185063362\n",
      "Epoch 7918/30000 Training Loss: 0.05750596895813942\n",
      "Epoch 7919/30000 Training Loss: 0.06693165004253387\n",
      "Epoch 7920/30000 Training Loss: 0.046805206686258316\n",
      "Epoch 7921/30000 Training Loss: 0.05342511087656021\n",
      "Epoch 7922/30000 Training Loss: 0.04474334418773651\n",
      "Epoch 7923/30000 Training Loss: 0.05673797056078911\n",
      "Epoch 7924/30000 Training Loss: 0.06004546582698822\n",
      "Epoch 7925/30000 Training Loss: 0.052921682596206665\n",
      "Epoch 7926/30000 Training Loss: 0.0552959069609642\n",
      "Epoch 7927/30000 Training Loss: 0.04776716232299805\n",
      "Epoch 7928/30000 Training Loss: 0.0631089136004448\n",
      "Epoch 7929/30000 Training Loss: 0.06637111306190491\n",
      "Epoch 7930/30000 Training Loss: 0.05953602492809296\n",
      "Epoch 7931/30000 Training Loss: 0.046474557369947433\n",
      "Epoch 7932/30000 Training Loss: 0.05992648005485535\n",
      "Epoch 7933/30000 Training Loss: 0.07059794664382935\n",
      "Epoch 7934/30000 Training Loss: 0.04694945737719536\n",
      "Epoch 7935/30000 Training Loss: 0.048028118908405304\n",
      "Epoch 7936/30000 Training Loss: 0.06862080842256546\n",
      "Epoch 7937/30000 Training Loss: 0.05795275419950485\n",
      "Epoch 7938/30000 Training Loss: 0.050503406673669815\n",
      "Epoch 7939/30000 Training Loss: 0.07138991355895996\n",
      "Epoch 7940/30000 Training Loss: 0.08113761246204376\n",
      "Epoch 7941/30000 Training Loss: 0.05603916943073273\n",
      "Epoch 7942/30000 Training Loss: 0.056949105113744736\n",
      "Epoch 7943/30000 Training Loss: 0.04585801437497139\n",
      "Epoch 7944/30000 Training Loss: 0.06762107461690903\n",
      "Epoch 7945/30000 Training Loss: 0.04882517829537392\n",
      "Epoch 7946/30000 Training Loss: 0.04751939699053764\n",
      "Epoch 7947/30000 Training Loss: 0.06197842210531235\n",
      "Epoch 7948/30000 Training Loss: 0.051497943699359894\n",
      "Epoch 7949/30000 Training Loss: 0.05629490315914154\n",
      "Epoch 7950/30000 Training Loss: 0.0571359284222126\n",
      "Epoch 7951/30000 Training Loss: 0.05435793474316597\n",
      "Epoch 7952/30000 Training Loss: 0.0524100698530674\n",
      "Epoch 7953/30000 Training Loss: 0.08788758516311646\n",
      "Epoch 7954/30000 Training Loss: 0.053234804421663284\n",
      "Epoch 7955/30000 Training Loss: 0.044618356972932816\n",
      "Epoch 7956/30000 Training Loss: 0.04809774458408356\n",
      "Epoch 7957/30000 Training Loss: 0.061982039362192154\n",
      "Epoch 7958/30000 Training Loss: 0.07992832362651825\n",
      "Epoch 7959/30000 Training Loss: 0.06251686066389084\n",
      "Epoch 7960/30000 Training Loss: 0.04429423063993454\n",
      "Epoch 7961/30000 Training Loss: 0.05043378844857216\n",
      "Epoch 7962/30000 Training Loss: 0.0580587312579155\n",
      "Epoch 7963/30000 Training Loss: 0.050234291702508926\n",
      "Epoch 7964/30000 Training Loss: 0.040702491998672485\n",
      "Epoch 7965/30000 Training Loss: 0.05003204941749573\n",
      "Epoch 7966/30000 Training Loss: 0.06050341576337814\n",
      "Epoch 7967/30000 Training Loss: 0.06457981467247009\n",
      "Epoch 7968/30000 Training Loss: 0.06390899419784546\n",
      "Epoch 7969/30000 Training Loss: 0.04760652780532837\n",
      "Epoch 7970/30000 Training Loss: 0.05258813872933388\n",
      "Epoch 7971/30000 Training Loss: 0.06104579567909241\n",
      "Epoch 7972/30000 Training Loss: 0.05286747217178345\n",
      "Epoch 7973/30000 Training Loss: 0.055020660161972046\n",
      "Epoch 7974/30000 Training Loss: 0.052129216492176056\n",
      "Epoch 7975/30000 Training Loss: 0.06737697124481201\n",
      "Epoch 7976/30000 Training Loss: 0.04102586209774017\n",
      "Epoch 7977/30000 Training Loss: 0.048933736979961395\n",
      "Epoch 7978/30000 Training Loss: 0.0705198422074318\n",
      "Epoch 7979/30000 Training Loss: 0.049090586602687836\n",
      "Epoch 7980/30000 Training Loss: 0.04528023302555084\n",
      "Epoch 7981/30000 Training Loss: 0.05512624233961105\n",
      "Epoch 7982/30000 Training Loss: 0.05433538928627968\n",
      "Epoch 7983/30000 Training Loss: 0.046011339873075485\n",
      "Epoch 7984/30000 Training Loss: 0.057904280722141266\n",
      "Epoch 7985/30000 Training Loss: 0.06283654272556305\n",
      "Epoch 7986/30000 Training Loss: 0.059012074023485184\n",
      "Epoch 7987/30000 Training Loss: 0.05686212331056595\n",
      "Epoch 7988/30000 Training Loss: 0.04879363626241684\n",
      "Epoch 7989/30000 Training Loss: 0.06279536336660385\n",
      "Epoch 7990/30000 Training Loss: 0.06960779428482056\n",
      "Epoch 7991/30000 Training Loss: 0.06305522471666336\n",
      "Epoch 7992/30000 Training Loss: 0.04655149579048157\n",
      "Epoch 7993/30000 Training Loss: 0.06738370656967163\n",
      "Epoch 7994/30000 Training Loss: 0.05396537855267525\n",
      "Epoch 7995/30000 Training Loss: 0.054857295006513596\n",
      "Epoch 7996/30000 Training Loss: 0.07267299294471741\n",
      "Epoch 7997/30000 Training Loss: 0.057545799762010574\n",
      "Epoch 7998/30000 Training Loss: 0.04035381227731705\n",
      "Epoch 7999/30000 Training Loss: 0.06447971612215042\n",
      "Epoch 8000/30000 Training Loss: 0.06668609380722046\n",
      "Epoch 8000/30000 Validation Loss: 0.06566913425922394\n",
      "Epoch 8001/30000 Training Loss: 0.051120199263095856\n",
      "Epoch 8002/30000 Training Loss: 0.07182178646326065\n",
      "Epoch 8003/30000 Training Loss: 0.04071083664894104\n",
      "Epoch 8004/30000 Training Loss: 0.04854030907154083\n",
      "Epoch 8005/30000 Training Loss: 0.0639684870839119\n",
      "Epoch 8006/30000 Training Loss: 0.05147678405046463\n",
      "Epoch 8007/30000 Training Loss: 0.04835441708564758\n",
      "Epoch 8008/30000 Training Loss: 0.05813390389084816\n",
      "Epoch 8009/30000 Training Loss: 0.05544809252023697\n",
      "Epoch 8010/30000 Training Loss: 0.04956696927547455\n",
      "Epoch 8011/30000 Training Loss: 0.05494865030050278\n",
      "Epoch 8012/30000 Training Loss: 0.04556168243288994\n",
      "Epoch 8013/30000 Training Loss: 0.06595220416784286\n",
      "Epoch 8014/30000 Training Loss: 0.06039145588874817\n",
      "Epoch 8015/30000 Training Loss: 0.04580868035554886\n",
      "Epoch 8016/30000 Training Loss: 0.04658614099025726\n",
      "Epoch 8017/30000 Training Loss: 0.050406161695718765\n",
      "Epoch 8018/30000 Training Loss: 0.057913947850465775\n",
      "Epoch 8019/30000 Training Loss: 0.06431427597999573\n",
      "Epoch 8020/30000 Training Loss: 0.05194050818681717\n",
      "Epoch 8021/30000 Training Loss: 0.05869852751493454\n",
      "Epoch 8022/30000 Training Loss: 0.05687536299228668\n",
      "Epoch 8023/30000 Training Loss: 0.04957000911235809\n",
      "Epoch 8024/30000 Training Loss: 0.052171073853969574\n",
      "Epoch 8025/30000 Training Loss: 0.0759790912270546\n",
      "Epoch 8026/30000 Training Loss: 0.05623374134302139\n",
      "Epoch 8027/30000 Training Loss: 0.04046894609928131\n",
      "Epoch 8028/30000 Training Loss: 0.05428478494286537\n",
      "Epoch 8029/30000 Training Loss: 0.06052146852016449\n",
      "Epoch 8030/30000 Training Loss: 0.05867380648851395\n",
      "Epoch 8031/30000 Training Loss: 0.06537666916847229\n",
      "Epoch 8032/30000 Training Loss: 0.055050142109394073\n",
      "Epoch 8033/30000 Training Loss: 0.04924597218632698\n",
      "Epoch 8034/30000 Training Loss: 0.05332908406853676\n",
      "Epoch 8035/30000 Training Loss: 0.06915407627820969\n",
      "Epoch 8036/30000 Training Loss: 0.06013079732656479\n",
      "Epoch 8037/30000 Training Loss: 0.06455960869789124\n",
      "Epoch 8038/30000 Training Loss: 0.07409804314374924\n",
      "Epoch 8039/30000 Training Loss: 0.08568821102380753\n",
      "Epoch 8040/30000 Training Loss: 0.06799334287643433\n",
      "Epoch 8041/30000 Training Loss: 0.05558625981211662\n",
      "Epoch 8042/30000 Training Loss: 0.0474211648106575\n",
      "Epoch 8043/30000 Training Loss: 0.044686321169137955\n",
      "Epoch 8044/30000 Training Loss: 0.04842829331755638\n",
      "Epoch 8045/30000 Training Loss: 0.05432567745447159\n",
      "Epoch 8046/30000 Training Loss: 0.0539877787232399\n",
      "Epoch 8047/30000 Training Loss: 0.05066622421145439\n",
      "Epoch 8048/30000 Training Loss: 0.05345658212900162\n",
      "Epoch 8049/30000 Training Loss: 0.05077425390481949\n",
      "Epoch 8050/30000 Training Loss: 0.06708883494138718\n",
      "Epoch 8051/30000 Training Loss: 0.0539860725402832\n",
      "Epoch 8052/30000 Training Loss: 0.05928393825888634\n",
      "Epoch 8053/30000 Training Loss: 0.05624162405729294\n",
      "Epoch 8054/30000 Training Loss: 0.055990878492593765\n",
      "Epoch 8055/30000 Training Loss: 0.06187248229980469\n",
      "Epoch 8056/30000 Training Loss: 0.0506163090467453\n",
      "Epoch 8057/30000 Training Loss: 0.03952169418334961\n",
      "Epoch 8058/30000 Training Loss: 0.06941383332014084\n",
      "Epoch 8059/30000 Training Loss: 0.06557582318782806\n",
      "Epoch 8060/30000 Training Loss: 0.05323236063122749\n",
      "Epoch 8061/30000 Training Loss: 0.05505526065826416\n",
      "Epoch 8062/30000 Training Loss: 0.06437555700540543\n",
      "Epoch 8063/30000 Training Loss: 0.05503619834780693\n",
      "Epoch 8064/30000 Training Loss: 0.06173344701528549\n",
      "Epoch 8065/30000 Training Loss: 0.05667697265744209\n",
      "Epoch 8066/30000 Training Loss: 0.06456957757472992\n",
      "Epoch 8067/30000 Training Loss: 0.04923838749527931\n",
      "Epoch 8068/30000 Training Loss: 0.05650586634874344\n",
      "Epoch 8069/30000 Training Loss: 0.04822857677936554\n",
      "Epoch 8070/30000 Training Loss: 0.05373251438140869\n",
      "Epoch 8071/30000 Training Loss: 0.06947857141494751\n",
      "Epoch 8072/30000 Training Loss: 0.057953234761953354\n",
      "Epoch 8073/30000 Training Loss: 0.05372554063796997\n",
      "Epoch 8074/30000 Training Loss: 0.031919293105602264\n",
      "Epoch 8075/30000 Training Loss: 0.059010688215494156\n",
      "Epoch 8076/30000 Training Loss: 0.05144162476062775\n",
      "Epoch 8077/30000 Training Loss: 0.05713441222906113\n",
      "Epoch 8078/30000 Training Loss: 0.04809185117483139\n",
      "Epoch 8079/30000 Training Loss: 0.06325887888669968\n",
      "Epoch 8080/30000 Training Loss: 0.06565816700458527\n",
      "Epoch 8081/30000 Training Loss: 0.051543090492486954\n",
      "Epoch 8082/30000 Training Loss: 0.06468824297189713\n",
      "Epoch 8083/30000 Training Loss: 0.06019823998212814\n",
      "Epoch 8084/30000 Training Loss: 0.05862480401992798\n",
      "Epoch 8085/30000 Training Loss: 0.0352536141872406\n",
      "Epoch 8086/30000 Training Loss: 0.050618961453437805\n",
      "Epoch 8087/30000 Training Loss: 0.06937450170516968\n",
      "Epoch 8088/30000 Training Loss: 0.06873799115419388\n",
      "Epoch 8089/30000 Training Loss: 0.05652395635843277\n",
      "Epoch 8090/30000 Training Loss: 0.061237040907144547\n",
      "Epoch 8091/30000 Training Loss: 0.058316878974437714\n",
      "Epoch 8092/30000 Training Loss: 0.043835245072841644\n",
      "Epoch 8093/30000 Training Loss: 0.05602182075381279\n",
      "Epoch 8094/30000 Training Loss: 0.06733886897563934\n",
      "Epoch 8095/30000 Training Loss: 0.07317885756492615\n",
      "Epoch 8096/30000 Training Loss: 0.07573509961366653\n",
      "Epoch 8097/30000 Training Loss: 0.04362456873059273\n",
      "Epoch 8098/30000 Training Loss: 0.05005771294236183\n",
      "Epoch 8099/30000 Training Loss: 0.05034824088215828\n",
      "Epoch 8100/30000 Training Loss: 0.05311913043260574\n",
      "Epoch 8100/30000 Validation Loss: 0.06783635169267654\n",
      "Epoch 8101/30000 Training Loss: 0.060093484818935394\n",
      "Epoch 8102/30000 Training Loss: 0.06783229857683182\n",
      "Epoch 8103/30000 Training Loss: 0.08948826789855957\n",
      "Epoch 8104/30000 Training Loss: 0.05266232788562775\n",
      "Epoch 8105/30000 Training Loss: 0.0707685723900795\n",
      "Epoch 8106/30000 Training Loss: 0.053761668503284454\n",
      "Epoch 8107/30000 Training Loss: 0.043679699301719666\n",
      "Epoch 8108/30000 Training Loss: 0.05852121487259865\n",
      "Epoch 8109/30000 Training Loss: 0.06505368649959564\n",
      "Epoch 8110/30000 Training Loss: 0.045855000615119934\n",
      "Epoch 8111/30000 Training Loss: 0.0712047666311264\n",
      "Epoch 8112/30000 Training Loss: 0.05688624829053879\n",
      "Epoch 8113/30000 Training Loss: 0.04896297678351402\n",
      "Epoch 8114/30000 Training Loss: 0.05126675218343735\n",
      "Epoch 8115/30000 Training Loss: 0.06363900750875473\n",
      "Epoch 8116/30000 Training Loss: 0.04551514983177185\n",
      "Epoch 8117/30000 Training Loss: 0.07514259219169617\n",
      "Epoch 8118/30000 Training Loss: 0.05422288179397583\n",
      "Epoch 8119/30000 Training Loss: 0.05403894931077957\n",
      "Epoch 8120/30000 Training Loss: 0.07701075822114944\n",
      "Epoch 8121/30000 Training Loss: 0.04928994178771973\n",
      "Epoch 8122/30000 Training Loss: 0.04821981489658356\n",
      "Epoch 8123/30000 Training Loss: 0.05415608361363411\n",
      "Epoch 8124/30000 Training Loss: 0.06390517950057983\n",
      "Epoch 8125/30000 Training Loss: 0.052485909312963486\n",
      "Epoch 8126/30000 Training Loss: 0.06408282369375229\n",
      "Epoch 8127/30000 Training Loss: 0.042595501989126205\n",
      "Epoch 8128/30000 Training Loss: 0.059716738760471344\n",
      "Epoch 8129/30000 Training Loss: 0.08332239091396332\n",
      "Epoch 8130/30000 Training Loss: 0.08038481324911118\n",
      "Epoch 8131/30000 Training Loss: 0.05165288597345352\n",
      "Epoch 8132/30000 Training Loss: 0.044099826365709305\n",
      "Epoch 8133/30000 Training Loss: 0.05206172168254852\n",
      "Epoch 8134/30000 Training Loss: 0.04347008466720581\n",
      "Epoch 8135/30000 Training Loss: 0.05006307363510132\n",
      "Epoch 8136/30000 Training Loss: 0.03792720288038254\n",
      "Epoch 8137/30000 Training Loss: 0.04341040179133415\n",
      "Epoch 8138/30000 Training Loss: 0.05559954047203064\n",
      "Epoch 8139/30000 Training Loss: 0.05947089195251465\n",
      "Epoch 8140/30000 Training Loss: 0.06538791954517365\n",
      "Epoch 8141/30000 Training Loss: 0.05825038626790047\n",
      "Epoch 8142/30000 Training Loss: 0.06561825424432755\n",
      "Epoch 8143/30000 Training Loss: 0.060735948383808136\n",
      "Epoch 8144/30000 Training Loss: 0.03997350111603737\n",
      "Epoch 8145/30000 Training Loss: 0.05575447902083397\n",
      "Epoch 8146/30000 Training Loss: 0.055789221078157425\n",
      "Epoch 8147/30000 Training Loss: 0.043226826936006546\n",
      "Epoch 8148/30000 Training Loss: 0.045226145535707474\n",
      "Epoch 8149/30000 Training Loss: 0.062086861580610275\n",
      "Epoch 8150/30000 Training Loss: 0.05517173558473587\n",
      "Epoch 8151/30000 Training Loss: 0.05020994693040848\n",
      "Epoch 8152/30000 Training Loss: 0.0698014497756958\n",
      "Epoch 8153/30000 Training Loss: 0.07582904398441315\n",
      "Epoch 8154/30000 Training Loss: 0.06313576549291611\n",
      "Epoch 8155/30000 Training Loss: 0.07321400195360184\n",
      "Epoch 8156/30000 Training Loss: 0.05785375088453293\n",
      "Epoch 8157/30000 Training Loss: 0.0534701906144619\n",
      "Epoch 8158/30000 Training Loss: 0.04232334718108177\n",
      "Epoch 8159/30000 Training Loss: 0.06052042171359062\n",
      "Epoch 8160/30000 Training Loss: 0.06668969988822937\n",
      "Epoch 8161/30000 Training Loss: 0.05295097082853317\n",
      "Epoch 8162/30000 Training Loss: 0.05901848524808884\n",
      "Epoch 8163/30000 Training Loss: 0.04972969740629196\n",
      "Epoch 8164/30000 Training Loss: 0.05651933699846268\n",
      "Epoch 8165/30000 Training Loss: 0.0458354614675045\n",
      "Epoch 8166/30000 Training Loss: 0.055944714695215225\n",
      "Epoch 8167/30000 Training Loss: 0.06057582423090935\n",
      "Epoch 8168/30000 Training Loss: 0.06194309890270233\n",
      "Epoch 8169/30000 Training Loss: 0.06103362515568733\n",
      "Epoch 8170/30000 Training Loss: 0.05641273409128189\n",
      "Epoch 8171/30000 Training Loss: 0.03647157922387123\n",
      "Epoch 8172/30000 Training Loss: 0.04603885859251022\n",
      "Epoch 8173/30000 Training Loss: 0.047532618045806885\n",
      "Epoch 8174/30000 Training Loss: 0.05342155322432518\n",
      "Epoch 8175/30000 Training Loss: 0.055826056748628616\n",
      "Epoch 8176/30000 Training Loss: 0.05255703255534172\n",
      "Epoch 8177/30000 Training Loss: 0.06267049908638\n",
      "Epoch 8178/30000 Training Loss: 0.05887092649936676\n",
      "Epoch 8179/30000 Training Loss: 0.04250511899590492\n",
      "Epoch 8180/30000 Training Loss: 0.06470223516225815\n",
      "Epoch 8181/30000 Training Loss: 0.06369294226169586\n",
      "Epoch 8182/30000 Training Loss: 0.05695108324289322\n",
      "Epoch 8183/30000 Training Loss: 0.059575099498033524\n",
      "Epoch 8184/30000 Training Loss: 0.06707452237606049\n",
      "Epoch 8185/30000 Training Loss: 0.05943930894136429\n",
      "Epoch 8186/30000 Training Loss: 0.06344564259052277\n",
      "Epoch 8187/30000 Training Loss: 0.07650764286518097\n",
      "Epoch 8188/30000 Training Loss: 0.05573239177465439\n",
      "Epoch 8189/30000 Training Loss: 0.05687006562948227\n",
      "Epoch 8190/30000 Training Loss: 0.04950173944234848\n",
      "Epoch 8191/30000 Training Loss: 0.059544410556554794\n",
      "Epoch 8192/30000 Training Loss: 0.05708685144782066\n",
      "Epoch 8193/30000 Training Loss: 0.06597249209880829\n",
      "Epoch 8194/30000 Training Loss: 0.05582544952630997\n",
      "Epoch 8195/30000 Training Loss: 0.0600760355591774\n",
      "Epoch 8196/30000 Training Loss: 0.04097443073987961\n",
      "Epoch 8197/30000 Training Loss: 0.05049530789256096\n",
      "Epoch 8198/30000 Training Loss: 0.0467858612537384\n",
      "Epoch 8199/30000 Training Loss: 0.07713983207941055\n",
      "Epoch 8200/30000 Training Loss: 0.06461022794246674\n",
      "Epoch 8200/30000 Validation Loss: 0.0538518987596035\n",
      "Epoch 8201/30000 Training Loss: 0.07060592621564865\n",
      "Epoch 8202/30000 Training Loss: 0.050362490117549896\n",
      "Epoch 8203/30000 Training Loss: 0.05932158604264259\n",
      "Epoch 8204/30000 Training Loss: 0.039803650230169296\n",
      "Epoch 8205/30000 Training Loss: 0.04850873351097107\n",
      "Epoch 8206/30000 Training Loss: 0.0336887463927269\n",
      "Epoch 8207/30000 Training Loss: 0.061770692467689514\n",
      "Epoch 8208/30000 Training Loss: 0.07160518318414688\n",
      "Epoch 8209/30000 Training Loss: 0.06288712471723557\n",
      "Epoch 8210/30000 Training Loss: 0.05546661093831062\n",
      "Epoch 8211/30000 Training Loss: 0.04838956519961357\n",
      "Epoch 8212/30000 Training Loss: 0.04817521199584007\n",
      "Epoch 8213/30000 Training Loss: 0.06411325931549072\n",
      "Epoch 8214/30000 Training Loss: 0.05186579003930092\n",
      "Epoch 8215/30000 Training Loss: 0.0718064233660698\n",
      "Epoch 8216/30000 Training Loss: 0.05280555784702301\n",
      "Epoch 8217/30000 Training Loss: 0.05521077662706375\n",
      "Epoch 8218/30000 Training Loss: 0.08360172808170319\n",
      "Epoch 8219/30000 Training Loss: 0.07070555537939072\n",
      "Epoch 8220/30000 Training Loss: 0.0679805651307106\n",
      "Epoch 8221/30000 Training Loss: 0.05604734271764755\n",
      "Epoch 8222/30000 Training Loss: 0.046242374926805496\n",
      "Epoch 8223/30000 Training Loss: 0.06360118091106415\n",
      "Epoch 8224/30000 Training Loss: 0.06583766639232635\n",
      "Epoch 8225/30000 Training Loss: 0.042218878865242004\n",
      "Epoch 8226/30000 Training Loss: 0.05395682156085968\n",
      "Epoch 8227/30000 Training Loss: 0.06336573511362076\n",
      "Epoch 8228/30000 Training Loss: 0.05156657472252846\n",
      "Epoch 8229/30000 Training Loss: 0.05954083427786827\n",
      "Epoch 8230/30000 Training Loss: 0.054183874279260635\n",
      "Epoch 8231/30000 Training Loss: 0.06923031061887741\n",
      "Epoch 8232/30000 Training Loss: 0.05439895763993263\n",
      "Epoch 8233/30000 Training Loss: 0.05124800279736519\n",
      "Epoch 8234/30000 Training Loss: 0.05597904324531555\n",
      "Epoch 8235/30000 Training Loss: 0.04877153038978577\n",
      "Epoch 8236/30000 Training Loss: 0.043034303933382034\n",
      "Epoch 8237/30000 Training Loss: 0.04999770224094391\n",
      "Epoch 8238/30000 Training Loss: 0.06368441879749298\n",
      "Epoch 8239/30000 Training Loss: 0.054048046469688416\n",
      "Epoch 8240/30000 Training Loss: 0.04588744789361954\n",
      "Epoch 8241/30000 Training Loss: 0.04435955360531807\n",
      "Epoch 8242/30000 Training Loss: 0.05007406696677208\n",
      "Epoch 8243/30000 Training Loss: 0.06368955969810486\n",
      "Epoch 8244/30000 Training Loss: 0.054168783128261566\n",
      "Epoch 8245/30000 Training Loss: 0.04841811582446098\n",
      "Epoch 8246/30000 Training Loss: 0.07215613126754761\n",
      "Epoch 8247/30000 Training Loss: 0.07773097604513168\n",
      "Epoch 8248/30000 Training Loss: 0.06461714953184128\n",
      "Epoch 8249/30000 Training Loss: 0.06502489000558853\n",
      "Epoch 8250/30000 Training Loss: 0.050358276814222336\n",
      "Epoch 8251/30000 Training Loss: 0.06390160322189331\n",
      "Epoch 8252/30000 Training Loss: 0.0507158525288105\n",
      "Epoch 8253/30000 Training Loss: 0.055616818368434906\n",
      "Epoch 8254/30000 Training Loss: 0.053177304565906525\n",
      "Epoch 8255/30000 Training Loss: 0.06746925413608551\n",
      "Epoch 8256/30000 Training Loss: 0.05540301278233528\n",
      "Epoch 8257/30000 Training Loss: 0.055795345455408096\n",
      "Epoch 8258/30000 Training Loss: 0.06967610865831375\n",
      "Epoch 8259/30000 Training Loss: 0.05398828536272049\n",
      "Epoch 8260/30000 Training Loss: 0.04736744612455368\n",
      "Epoch 8261/30000 Training Loss: 0.043800827115774155\n",
      "Epoch 8262/30000 Training Loss: 0.0568673349916935\n",
      "Epoch 8263/30000 Training Loss: 0.05648413300514221\n",
      "Epoch 8264/30000 Training Loss: 0.06663279235363007\n",
      "Epoch 8265/30000 Training Loss: 0.06359321624040604\n",
      "Epoch 8266/30000 Training Loss: 0.04292653501033783\n",
      "Epoch 8267/30000 Training Loss: 0.05105610564351082\n",
      "Epoch 8268/30000 Training Loss: 0.06206343322992325\n",
      "Epoch 8269/30000 Training Loss: 0.051365453749895096\n",
      "Epoch 8270/30000 Training Loss: 0.0578598715364933\n",
      "Epoch 8271/30000 Training Loss: 0.05560743436217308\n",
      "Epoch 8272/30000 Training Loss: 0.07050998508930206\n",
      "Epoch 8273/30000 Training Loss: 0.05526953190565109\n",
      "Epoch 8274/30000 Training Loss: 0.10093516111373901\n",
      "Epoch 8275/30000 Training Loss: 0.056848812848329544\n",
      "Epoch 8276/30000 Training Loss: 0.04663889855146408\n",
      "Epoch 8277/30000 Training Loss: 0.03872836381196976\n",
      "Epoch 8278/30000 Training Loss: 0.05784623324871063\n",
      "Epoch 8279/30000 Training Loss: 0.05734569579362869\n",
      "Epoch 8280/30000 Training Loss: 0.07775545120239258\n",
      "Epoch 8281/30000 Training Loss: 0.057386767119169235\n",
      "Epoch 8282/30000 Training Loss: 0.049033023416996\n",
      "Epoch 8283/30000 Training Loss: 0.04975065588951111\n",
      "Epoch 8284/30000 Training Loss: 0.049365002661943436\n",
      "Epoch 8285/30000 Training Loss: 0.044927917420864105\n",
      "Epoch 8286/30000 Training Loss: 0.0626283809542656\n",
      "Epoch 8287/30000 Training Loss: 0.047788240015506744\n",
      "Epoch 8288/30000 Training Loss: 0.0558742918074131\n",
      "Epoch 8289/30000 Training Loss: 0.06769034266471863\n",
      "Epoch 8290/30000 Training Loss: 0.05641685426235199\n",
      "Epoch 8291/30000 Training Loss: 0.05118215084075928\n",
      "Epoch 8292/30000 Training Loss: 0.06459930539131165\n",
      "Epoch 8293/30000 Training Loss: 0.051336850970983505\n",
      "Epoch 8294/30000 Training Loss: 0.05521216616034508\n",
      "Epoch 8295/30000 Training Loss: 0.055365607142448425\n",
      "Epoch 8296/30000 Training Loss: 0.05995083600282669\n",
      "Epoch 8297/30000 Training Loss: 0.05468705669045448\n",
      "Epoch 8298/30000 Training Loss: 0.048728227615356445\n",
      "Epoch 8299/30000 Training Loss: 0.05850173160433769\n",
      "Epoch 8300/30000 Training Loss: 0.04772648215293884\n",
      "Epoch 8300/30000 Validation Loss: 0.0500779002904892\n",
      "Epoch 8301/30000 Training Loss: 0.07480459660291672\n",
      "Epoch 8302/30000 Training Loss: 0.05428695306181908\n",
      "Epoch 8303/30000 Training Loss: 0.04761269688606262\n",
      "Epoch 8304/30000 Training Loss: 0.061972055584192276\n",
      "Epoch 8305/30000 Training Loss: 0.051603928208351135\n",
      "Epoch 8306/30000 Training Loss: 0.045719582587480545\n",
      "Epoch 8307/30000 Training Loss: 0.04574916139245033\n",
      "Epoch 8308/30000 Training Loss: 0.06081801652908325\n",
      "Epoch 8309/30000 Training Loss: 0.06638070940971375\n",
      "Epoch 8310/30000 Training Loss: 0.04050657898187637\n",
      "Epoch 8311/30000 Training Loss: 0.04943212866783142\n",
      "Epoch 8312/30000 Training Loss: 0.05549180507659912\n",
      "Epoch 8313/30000 Training Loss: 0.05164557695388794\n",
      "Epoch 8314/30000 Training Loss: 0.06654034554958344\n",
      "Epoch 8315/30000 Training Loss: 0.06594055891036987\n",
      "Epoch 8316/30000 Training Loss: 0.05640684813261032\n",
      "Epoch 8317/30000 Training Loss: 0.0448477678000927\n",
      "Epoch 8318/30000 Training Loss: 0.046007439494132996\n",
      "Epoch 8319/30000 Training Loss: 0.05132533982396126\n",
      "Epoch 8320/30000 Training Loss: 0.05295263230800629\n",
      "Epoch 8321/30000 Training Loss: 0.06597601622343063\n",
      "Epoch 8322/30000 Training Loss: 0.048351481556892395\n",
      "Epoch 8323/30000 Training Loss: 0.06851378083229065\n",
      "Epoch 8324/30000 Training Loss: 0.07036212831735611\n",
      "Epoch 8325/30000 Training Loss: 0.05120552331209183\n",
      "Epoch 8326/30000 Training Loss: 0.04870335012674332\n",
      "Epoch 8327/30000 Training Loss: 0.0619107186794281\n",
      "Epoch 8328/30000 Training Loss: 0.04856427013874054\n",
      "Epoch 8329/30000 Training Loss: 0.06417951732873917\n",
      "Epoch 8330/30000 Training Loss: 0.07247890532016754\n",
      "Epoch 8331/30000 Training Loss: 0.06134619936347008\n",
      "Epoch 8332/30000 Training Loss: 0.05073043704032898\n",
      "Epoch 8333/30000 Training Loss: 0.05692113935947418\n",
      "Epoch 8334/30000 Training Loss: 0.06698577105998993\n",
      "Epoch 8335/30000 Training Loss: 0.06217923015356064\n",
      "Epoch 8336/30000 Training Loss: 0.06774275749921799\n",
      "Epoch 8337/30000 Training Loss: 0.051603831350803375\n",
      "Epoch 8338/30000 Training Loss: 0.06037192791700363\n",
      "Epoch 8339/30000 Training Loss: 0.05178699642419815\n",
      "Epoch 8340/30000 Training Loss: 0.06420553475618362\n",
      "Epoch 8341/30000 Training Loss: 0.04984264075756073\n",
      "Epoch 8342/30000 Training Loss: 0.05068126320838928\n",
      "Epoch 8343/30000 Training Loss: 0.05697978660464287\n",
      "Epoch 8344/30000 Training Loss: 0.06779901683330536\n",
      "Epoch 8345/30000 Training Loss: 0.06560986489057541\n",
      "Epoch 8346/30000 Training Loss: 0.05693048983812332\n",
      "Epoch 8347/30000 Training Loss: 0.06540803611278534\n",
      "Epoch 8348/30000 Training Loss: 0.054027751088142395\n",
      "Epoch 8349/30000 Training Loss: 0.08035966008901596\n",
      "Epoch 8350/30000 Training Loss: 0.04719166457653046\n",
      "Epoch 8351/30000 Training Loss: 0.056379284709692\n",
      "Epoch 8352/30000 Training Loss: 0.058104611933231354\n",
      "Epoch 8353/30000 Training Loss: 0.05969231575727463\n",
      "Epoch 8354/30000 Training Loss: 0.041999395936727524\n",
      "Epoch 8355/30000 Training Loss: 0.05224374681711197\n",
      "Epoch 8356/30000 Training Loss: 0.06392939388751984\n",
      "Epoch 8357/30000 Training Loss: 0.06135864555835724\n",
      "Epoch 8358/30000 Training Loss: 0.05770357698202133\n",
      "Epoch 8359/30000 Training Loss: 0.04783659800887108\n",
      "Epoch 8360/30000 Training Loss: 0.060046516358852386\n",
      "Epoch 8361/30000 Training Loss: 0.06293574720621109\n",
      "Epoch 8362/30000 Training Loss: 0.048687539994716644\n",
      "Epoch 8363/30000 Training Loss: 0.0594264417886734\n",
      "Epoch 8364/30000 Training Loss: 0.04355919361114502\n",
      "Epoch 8365/30000 Training Loss: 0.0700167790055275\n",
      "Epoch 8366/30000 Training Loss: 0.04935256019234657\n",
      "Epoch 8367/30000 Training Loss: 0.04288138076663017\n",
      "Epoch 8368/30000 Training Loss: 0.056491680443286896\n",
      "Epoch 8369/30000 Training Loss: 0.053791593760252\n",
      "Epoch 8370/30000 Training Loss: 0.0537823811173439\n",
      "Epoch 8371/30000 Training Loss: 0.05294474586844444\n",
      "Epoch 8372/30000 Training Loss: 0.0514046847820282\n",
      "Epoch 8373/30000 Training Loss: 0.05233782157301903\n",
      "Epoch 8374/30000 Training Loss: 0.06708591431379318\n",
      "Epoch 8375/30000 Training Loss: 0.05046889930963516\n",
      "Epoch 8376/30000 Training Loss: 0.05824931710958481\n",
      "Epoch 8377/30000 Training Loss: 0.04806499183177948\n",
      "Epoch 8378/30000 Training Loss: 0.05733707547187805\n",
      "Epoch 8379/30000 Training Loss: 0.05263363569974899\n",
      "Epoch 8380/30000 Training Loss: 0.045247793197631836\n",
      "Epoch 8381/30000 Training Loss: 0.03934305161237717\n",
      "Epoch 8382/30000 Training Loss: 0.04933945834636688\n",
      "Epoch 8383/30000 Training Loss: 0.056461479514837265\n",
      "Epoch 8384/30000 Training Loss: 0.04988976567983627\n",
      "Epoch 8385/30000 Training Loss: 0.0671641007065773\n",
      "Epoch 8386/30000 Training Loss: 0.04395512863993645\n",
      "Epoch 8387/30000 Training Loss: 0.06767047196626663\n",
      "Epoch 8388/30000 Training Loss: 0.05200395733118057\n",
      "Epoch 8389/30000 Training Loss: 0.05995553731918335\n",
      "Epoch 8390/30000 Training Loss: 0.05349947512149811\n",
      "Epoch 8391/30000 Training Loss: 0.07138043642044067\n",
      "Epoch 8392/30000 Training Loss: 0.0618753582239151\n",
      "Epoch 8393/30000 Training Loss: 0.05020282045006752\n",
      "Epoch 8394/30000 Training Loss: 0.0689188614487648\n",
      "Epoch 8395/30000 Training Loss: 0.0625874251127243\n",
      "Epoch 8396/30000 Training Loss: 0.046834174543619156\n",
      "Epoch 8397/30000 Training Loss: 0.0506693571805954\n",
      "Epoch 8398/30000 Training Loss: 0.06439422070980072\n",
      "Epoch 8399/30000 Training Loss: 0.04793978109955788\n",
      "Epoch 8400/30000 Training Loss: 0.06725936383008957\n",
      "Epoch 8400/30000 Validation Loss: 0.05574626475572586\n",
      "Epoch 8401/30000 Training Loss: 0.058227553963661194\n",
      "Epoch 8402/30000 Training Loss: 0.04227289929986\n",
      "Epoch 8403/30000 Training Loss: 0.047525301575660706\n",
      "Epoch 8404/30000 Training Loss: 0.04726693779230118\n",
      "Epoch 8405/30000 Training Loss: 0.050992079079151154\n",
      "Epoch 8406/30000 Training Loss: 0.059472594410181046\n",
      "Epoch 8407/30000 Training Loss: 0.06075354665517807\n",
      "Epoch 8408/30000 Training Loss: 0.056076303124427795\n",
      "Epoch 8409/30000 Training Loss: 0.06689630448818207\n",
      "Epoch 8410/30000 Training Loss: 0.05724508315324783\n",
      "Epoch 8411/30000 Training Loss: 0.045767396688461304\n",
      "Epoch 8412/30000 Training Loss: 0.05943974107503891\n",
      "Epoch 8413/30000 Training Loss: 0.06824734061956406\n",
      "Epoch 8414/30000 Training Loss: 0.050316911190748215\n",
      "Epoch 8415/30000 Training Loss: 0.05725184082984924\n",
      "Epoch 8416/30000 Training Loss: 0.06858140230178833\n",
      "Epoch 8417/30000 Training Loss: 0.0411183200776577\n",
      "Epoch 8418/30000 Training Loss: 0.06763969361782074\n",
      "Epoch 8419/30000 Training Loss: 0.046974603086709976\n",
      "Epoch 8420/30000 Training Loss: 0.05282768979668617\n",
      "Epoch 8421/30000 Training Loss: 0.06008019670844078\n",
      "Epoch 8422/30000 Training Loss: 0.054484523832798004\n",
      "Epoch 8423/30000 Training Loss: 0.04896169900894165\n",
      "Epoch 8424/30000 Training Loss: 0.04635312780737877\n",
      "Epoch 8425/30000 Training Loss: 0.05678047612309456\n",
      "Epoch 8426/30000 Training Loss: 0.052634935826063156\n",
      "Epoch 8427/30000 Training Loss: 0.05387865751981735\n",
      "Epoch 8428/30000 Training Loss: 0.05362078174948692\n",
      "Epoch 8429/30000 Training Loss: 0.04388003796339035\n",
      "Epoch 8430/30000 Training Loss: 0.05893980339169502\n",
      "Epoch 8431/30000 Training Loss: 0.08685078471899033\n",
      "Epoch 8432/30000 Training Loss: 0.07002688944339752\n",
      "Epoch 8433/30000 Training Loss: 0.057885028421878815\n",
      "Epoch 8434/30000 Training Loss: 0.055962853133678436\n",
      "Epoch 8435/30000 Training Loss: 0.04931750148534775\n",
      "Epoch 8436/30000 Training Loss: 0.05781704932451248\n",
      "Epoch 8437/30000 Training Loss: 0.05768454074859619\n",
      "Epoch 8438/30000 Training Loss: 0.05840529501438141\n",
      "Epoch 8439/30000 Training Loss: 0.05988045036792755\n",
      "Epoch 8440/30000 Training Loss: 0.05919263884425163\n",
      "Epoch 8441/30000 Training Loss: 0.05489904433488846\n",
      "Epoch 8442/30000 Training Loss: 0.07286043465137482\n",
      "Epoch 8443/30000 Training Loss: 0.051558252424001694\n",
      "Epoch 8444/30000 Training Loss: 0.061581507325172424\n",
      "Epoch 8445/30000 Training Loss: 0.05034822225570679\n",
      "Epoch 8446/30000 Training Loss: 0.04891066625714302\n",
      "Epoch 8447/30000 Training Loss: 0.05268091708421707\n",
      "Epoch 8448/30000 Training Loss: 0.04486667364835739\n",
      "Epoch 8449/30000 Training Loss: 0.05865103006362915\n",
      "Epoch 8450/30000 Training Loss: 0.06898179650306702\n",
      "Epoch 8451/30000 Training Loss: 0.061150964349508286\n",
      "Epoch 8452/30000 Training Loss: 0.06517530977725983\n",
      "Epoch 8453/30000 Training Loss: 0.06476013362407684\n",
      "Epoch 8454/30000 Training Loss: 0.047785207629203796\n",
      "Epoch 8455/30000 Training Loss: 0.04112398996949196\n",
      "Epoch 8456/30000 Training Loss: 0.04988495633006096\n",
      "Epoch 8457/30000 Training Loss: 0.05480210483074188\n",
      "Epoch 8458/30000 Training Loss: 0.05343987047672272\n",
      "Epoch 8459/30000 Training Loss: 0.06434329599142075\n",
      "Epoch 8460/30000 Training Loss: 0.05589620769023895\n",
      "Epoch 8461/30000 Training Loss: 0.07119923830032349\n",
      "Epoch 8462/30000 Training Loss: 0.05684838443994522\n",
      "Epoch 8463/30000 Training Loss: 0.03926043584942818\n",
      "Epoch 8464/30000 Training Loss: 0.07687438279390335\n",
      "Epoch 8465/30000 Training Loss: 0.03916996344923973\n",
      "Epoch 8466/30000 Training Loss: 0.049391478300094604\n",
      "Epoch 8467/30000 Training Loss: 0.043772000819444656\n",
      "Epoch 8468/30000 Training Loss: 0.05646432936191559\n",
      "Epoch 8469/30000 Training Loss: 0.03771522641181946\n",
      "Epoch 8470/30000 Training Loss: 0.07078885287046432\n",
      "Epoch 8471/30000 Training Loss: 0.06802220642566681\n",
      "Epoch 8472/30000 Training Loss: 0.04183170571923256\n",
      "Epoch 8473/30000 Training Loss: 0.06147637590765953\n",
      "Epoch 8474/30000 Training Loss: 0.04747119918465614\n",
      "Epoch 8475/30000 Training Loss: 0.06732145696878433\n",
      "Epoch 8476/30000 Training Loss: 0.04223369434475899\n",
      "Epoch 8477/30000 Training Loss: 0.04276107996702194\n",
      "Epoch 8478/30000 Training Loss: 0.06183716654777527\n",
      "Epoch 8479/30000 Training Loss: 0.043012574315071106\n",
      "Epoch 8480/30000 Training Loss: 0.08991982787847519\n",
      "Epoch 8481/30000 Training Loss: 0.04559478536248207\n",
      "Epoch 8482/30000 Training Loss: 0.05594461411237717\n",
      "Epoch 8483/30000 Training Loss: 0.05083279684185982\n",
      "Epoch 8484/30000 Training Loss: 0.06669928133487701\n",
      "Epoch 8485/30000 Training Loss: 0.04018501564860344\n",
      "Epoch 8486/30000 Training Loss: 0.05058988928794861\n",
      "Epoch 8487/30000 Training Loss: 0.06075586378574371\n",
      "Epoch 8488/30000 Training Loss: 0.042845651507377625\n",
      "Epoch 8489/30000 Training Loss: 0.04753568395972252\n",
      "Epoch 8490/30000 Training Loss: 0.06332926452159882\n",
      "Epoch 8491/30000 Training Loss: 0.052746936678886414\n",
      "Epoch 8492/30000 Training Loss: 0.03878207504749298\n",
      "Epoch 8493/30000 Training Loss: 0.04250029847025871\n",
      "Epoch 8494/30000 Training Loss: 0.0501781702041626\n",
      "Epoch 8495/30000 Training Loss: 0.061118803918361664\n",
      "Epoch 8496/30000 Training Loss: 0.05592767149209976\n",
      "Epoch 8497/30000 Training Loss: 0.0543084591627121\n",
      "Epoch 8498/30000 Training Loss: 0.05339837819337845\n",
      "Epoch 8499/30000 Training Loss: 0.052919551730155945\n",
      "Epoch 8500/30000 Training Loss: 0.06015688553452492\n",
      "Epoch 8500/30000 Validation Loss: 0.05384638532996178\n",
      "Epoch 8501/30000 Training Loss: 0.04903244972229004\n",
      "Epoch 8502/30000 Training Loss: 0.0581914484500885\n",
      "Epoch 8503/30000 Training Loss: 0.057583391666412354\n",
      "Epoch 8504/30000 Training Loss: 0.05105629563331604\n",
      "Epoch 8505/30000 Training Loss: 0.07214181125164032\n",
      "Epoch 8506/30000 Training Loss: 0.06426620483398438\n",
      "Epoch 8507/30000 Training Loss: 0.0514698252081871\n",
      "Epoch 8508/30000 Training Loss: 0.06159142777323723\n",
      "Epoch 8509/30000 Training Loss: 0.07322251051664352\n",
      "Epoch 8510/30000 Training Loss: 0.043188631534576416\n",
      "Epoch 8511/30000 Training Loss: 0.08277422934770584\n",
      "Epoch 8512/30000 Training Loss: 0.067192941904068\n",
      "Epoch 8513/30000 Training Loss: 0.0571378730237484\n",
      "Epoch 8514/30000 Training Loss: 0.05197179317474365\n",
      "Epoch 8515/30000 Training Loss: 0.05953961983323097\n",
      "Epoch 8516/30000 Training Loss: 0.037804968655109406\n",
      "Epoch 8517/30000 Training Loss: 0.046338874846696854\n",
      "Epoch 8518/30000 Training Loss: 0.05122911557555199\n",
      "Epoch 8519/30000 Training Loss: 0.07092546671628952\n",
      "Epoch 8520/30000 Training Loss: 0.03584042191505432\n",
      "Epoch 8521/30000 Training Loss: 0.057282596826553345\n",
      "Epoch 8522/30000 Training Loss: 0.059282757341861725\n",
      "Epoch 8523/30000 Training Loss: 0.06382663547992706\n",
      "Epoch 8524/30000 Training Loss: 0.06090698018670082\n",
      "Epoch 8525/30000 Training Loss: 0.05913148447871208\n",
      "Epoch 8526/30000 Training Loss: 0.05688678100705147\n",
      "Epoch 8527/30000 Training Loss: 0.05829496681690216\n",
      "Epoch 8528/30000 Training Loss: 0.0612192302942276\n",
      "Epoch 8529/30000 Training Loss: 0.05094476416707039\n",
      "Epoch 8530/30000 Training Loss: 0.050398118793964386\n",
      "Epoch 8531/30000 Training Loss: 0.04262644052505493\n",
      "Epoch 8532/30000 Training Loss: 0.05554787814617157\n",
      "Epoch 8533/30000 Training Loss: 0.050278473645448685\n",
      "Epoch 8534/30000 Training Loss: 0.07006169855594635\n",
      "Epoch 8535/30000 Training Loss: 0.03711428493261337\n",
      "Epoch 8536/30000 Training Loss: 0.04329461231827736\n",
      "Epoch 8537/30000 Training Loss: 0.05580408126115799\n",
      "Epoch 8538/30000 Training Loss: 0.054262496531009674\n",
      "Epoch 8539/30000 Training Loss: 0.05733666568994522\n",
      "Epoch 8540/30000 Training Loss: 0.0575767382979393\n",
      "Epoch 8541/30000 Training Loss: 0.04970809072256088\n",
      "Epoch 8542/30000 Training Loss: 0.07991403341293335\n",
      "Epoch 8543/30000 Training Loss: 0.05798422545194626\n",
      "Epoch 8544/30000 Training Loss: 0.0645894929766655\n",
      "Epoch 8545/30000 Training Loss: 0.04801565781235695\n",
      "Epoch 8546/30000 Training Loss: 0.05230587720870972\n",
      "Epoch 8547/30000 Training Loss: 0.052965614944696426\n",
      "Epoch 8548/30000 Training Loss: 0.04738287255167961\n",
      "Epoch 8549/30000 Training Loss: 0.061907485127449036\n",
      "Epoch 8550/30000 Training Loss: 0.04533736780285835\n",
      "Epoch 8551/30000 Training Loss: 0.052137427031993866\n",
      "Epoch 8552/30000 Training Loss: 0.04448530077934265\n",
      "Epoch 8553/30000 Training Loss: 0.06138142943382263\n",
      "Epoch 8554/30000 Training Loss: 0.05753026902675629\n",
      "Epoch 8555/30000 Training Loss: 0.05506328120827675\n",
      "Epoch 8556/30000 Training Loss: 0.07578384876251221\n",
      "Epoch 8557/30000 Training Loss: 0.038872651755809784\n",
      "Epoch 8558/30000 Training Loss: 0.05113960802555084\n",
      "Epoch 8559/30000 Training Loss: 0.04312950372695923\n",
      "Epoch 8560/30000 Training Loss: 0.047191545367240906\n",
      "Epoch 8561/30000 Training Loss: 0.05521673709154129\n",
      "Epoch 8562/30000 Training Loss: 0.05456061288714409\n",
      "Epoch 8563/30000 Training Loss: 0.05231013521552086\n",
      "Epoch 8564/30000 Training Loss: 0.051986273378133774\n",
      "Epoch 8565/30000 Training Loss: 0.037549372762441635\n",
      "Epoch 8566/30000 Training Loss: 0.04201480746269226\n",
      "Epoch 8567/30000 Training Loss: 0.05529635399580002\n",
      "Epoch 8568/30000 Training Loss: 0.054595932364463806\n",
      "Epoch 8569/30000 Training Loss: 0.041100263595581055\n",
      "Epoch 8570/30000 Training Loss: 0.06037632375955582\n",
      "Epoch 8571/30000 Training Loss: 0.07523658871650696\n",
      "Epoch 8572/30000 Training Loss: 0.05922570079565048\n",
      "Epoch 8573/30000 Training Loss: 0.049982666969299316\n",
      "Epoch 8574/30000 Training Loss: 0.077392578125\n",
      "Epoch 8575/30000 Training Loss: 0.05725610628724098\n",
      "Epoch 8576/30000 Training Loss: 0.04965458810329437\n",
      "Epoch 8577/30000 Training Loss: 0.06166883558034897\n",
      "Epoch 8578/30000 Training Loss: 0.055575620383024216\n",
      "Epoch 8579/30000 Training Loss: 0.050424724817276\n",
      "Epoch 8580/30000 Training Loss: 0.055685900151729584\n",
      "Epoch 8581/30000 Training Loss: 0.049154650419950485\n",
      "Epoch 8582/30000 Training Loss: 0.05499853938817978\n",
      "Epoch 8583/30000 Training Loss: 0.05296851694583893\n",
      "Epoch 8584/30000 Training Loss: 0.07193806022405624\n",
      "Epoch 8585/30000 Training Loss: 0.0487266406416893\n",
      "Epoch 8586/30000 Training Loss: 0.055581748485565186\n",
      "Epoch 8587/30000 Training Loss: 0.047501783818006516\n",
      "Epoch 8588/30000 Training Loss: 0.0580240897834301\n",
      "Epoch 8589/30000 Training Loss: 0.05287593603134155\n",
      "Epoch 8590/30000 Training Loss: 0.058151181787252426\n",
      "Epoch 8591/30000 Training Loss: 0.04915133863687515\n",
      "Epoch 8592/30000 Training Loss: 0.04235908016562462\n",
      "Epoch 8593/30000 Training Loss: 0.06396327912807465\n",
      "Epoch 8594/30000 Training Loss: 0.05124833434820175\n",
      "Epoch 8595/30000 Training Loss: 0.055988989770412445\n",
      "Epoch 8596/30000 Training Loss: 0.05604613572359085\n",
      "Epoch 8597/30000 Training Loss: 0.041578810662031174\n",
      "Epoch 8598/30000 Training Loss: 0.054558515548706055\n",
      "Epoch 8599/30000 Training Loss: 0.06152159348130226\n",
      "Epoch 8600/30000 Training Loss: 0.0518210232257843\n",
      "Epoch 8600/30000 Validation Loss: 0.03820846229791641\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03820846229791641<=============\n",
      "Epoch 8601/30000 Training Loss: 0.040615424513816833\n",
      "Epoch 8602/30000 Training Loss: 0.044783178716897964\n",
      "Epoch 8603/30000 Training Loss: 0.051626723259687424\n",
      "Epoch 8604/30000 Training Loss: 0.05672561377286911\n",
      "Epoch 8605/30000 Training Loss: 0.05257466062903404\n",
      "Epoch 8606/30000 Training Loss: 0.036515604704618454\n",
      "Epoch 8607/30000 Training Loss: 0.04560977220535278\n",
      "Epoch 8608/30000 Training Loss: 0.060344502329826355\n",
      "Epoch 8609/30000 Training Loss: 0.055056266486644745\n",
      "Epoch 8610/30000 Training Loss: 0.0520334467291832\n",
      "Epoch 8611/30000 Training Loss: 0.05575136840343475\n",
      "Epoch 8612/30000 Training Loss: 0.053737252950668335\n",
      "Epoch 8613/30000 Training Loss: 0.04267334192991257\n",
      "Epoch 8614/30000 Training Loss: 0.061103492975234985\n",
      "Epoch 8615/30000 Training Loss: 0.057262443006038666\n",
      "Epoch 8616/30000 Training Loss: 0.04308917745947838\n",
      "Epoch 8617/30000 Training Loss: 0.05891808122396469\n",
      "Epoch 8618/30000 Training Loss: 0.05486050248146057\n",
      "Epoch 8619/30000 Training Loss: 0.059787265956401825\n",
      "Epoch 8620/30000 Training Loss: 0.042833540588617325\n",
      "Epoch 8621/30000 Training Loss: 0.05854742228984833\n",
      "Epoch 8622/30000 Training Loss: 0.06022448465228081\n",
      "Epoch 8623/30000 Training Loss: 0.05847324803471565\n",
      "Epoch 8624/30000 Training Loss: 0.04480791091918945\n",
      "Epoch 8625/30000 Training Loss: 0.06640086323022842\n",
      "Epoch 8626/30000 Training Loss: 0.054860010743141174\n",
      "Epoch 8627/30000 Training Loss: 0.05651814863085747\n",
      "Epoch 8628/30000 Training Loss: 0.05853922665119171\n",
      "Epoch 8629/30000 Training Loss: 0.047429077327251434\n",
      "Epoch 8630/30000 Training Loss: 0.06324335932731628\n",
      "Epoch 8631/30000 Training Loss: 0.06888891756534576\n",
      "Epoch 8632/30000 Training Loss: 0.045865364372730255\n",
      "Epoch 8633/30000 Training Loss: 0.05878619849681854\n",
      "Epoch 8634/30000 Training Loss: 0.04376082122325897\n",
      "Epoch 8635/30000 Training Loss: 0.04918255656957626\n",
      "Epoch 8636/30000 Training Loss: 0.06039969250559807\n",
      "Epoch 8637/30000 Training Loss: 0.0453854575753212\n",
      "Epoch 8638/30000 Training Loss: 0.04106099531054497\n",
      "Epoch 8639/30000 Training Loss: 0.04634205624461174\n",
      "Epoch 8640/30000 Training Loss: 0.042732611298561096\n",
      "Epoch 8641/30000 Training Loss: 0.05614432692527771\n",
      "Epoch 8642/30000 Training Loss: 0.06554731726646423\n",
      "Epoch 8643/30000 Training Loss: 0.055284533649683\n",
      "Epoch 8644/30000 Training Loss: 0.040960296988487244\n",
      "Epoch 8645/30000 Training Loss: 0.05721602588891983\n",
      "Epoch 8646/30000 Training Loss: 0.042327605187892914\n",
      "Epoch 8647/30000 Training Loss: 0.06406188756227493\n",
      "Epoch 8648/30000 Training Loss: 0.04315931722521782\n",
      "Epoch 8649/30000 Training Loss: 0.06637463718652725\n",
      "Epoch 8650/30000 Training Loss: 0.05087652802467346\n",
      "Epoch 8651/30000 Training Loss: 0.05624091625213623\n",
      "Epoch 8652/30000 Training Loss: 0.04756050184369087\n",
      "Epoch 8653/30000 Training Loss: 0.0699475035071373\n",
      "Epoch 8654/30000 Training Loss: 0.06541784852743149\n",
      "Epoch 8655/30000 Training Loss: 0.05110214278101921\n",
      "Epoch 8656/30000 Training Loss: 0.06801074743270874\n",
      "Epoch 8657/30000 Training Loss: 0.0464518778026104\n",
      "Epoch 8658/30000 Training Loss: 0.05512449890375137\n",
      "Epoch 8659/30000 Training Loss: 0.048593707382678986\n",
      "Epoch 8660/30000 Training Loss: 0.0503934770822525\n",
      "Epoch 8661/30000 Training Loss: 0.06504236906766891\n",
      "Epoch 8662/30000 Training Loss: 0.05486875772476196\n",
      "Epoch 8663/30000 Training Loss: 0.05825221911072731\n",
      "Epoch 8664/30000 Training Loss: 0.053660180419683456\n",
      "Epoch 8665/30000 Training Loss: 0.05757639929652214\n",
      "Epoch 8666/30000 Training Loss: 0.06259491294622421\n",
      "Epoch 8667/30000 Training Loss: 0.048746801912784576\n",
      "Epoch 8668/30000 Training Loss: 0.05578744411468506\n",
      "Epoch 8669/30000 Training Loss: 0.05666310340166092\n",
      "Epoch 8670/30000 Training Loss: 0.04313044622540474\n",
      "Epoch 8671/30000 Training Loss: 0.044166069477796555\n",
      "Epoch 8672/30000 Training Loss: 0.05205235257744789\n",
      "Epoch 8673/30000 Training Loss: 0.07242552191019058\n",
      "Epoch 8674/30000 Training Loss: 0.04976779222488403\n",
      "Epoch 8675/30000 Training Loss: 0.0572713203728199\n",
      "Epoch 8676/30000 Training Loss: 0.056863900274038315\n",
      "Epoch 8677/30000 Training Loss: 0.057987600564956665\n",
      "Epoch 8678/30000 Training Loss: 0.05890248715877533\n",
      "Epoch 8679/30000 Training Loss: 0.06595365703105927\n",
      "Epoch 8680/30000 Training Loss: 0.050198689103126526\n",
      "Epoch 8681/30000 Training Loss: 0.04893526807427406\n",
      "Epoch 8682/30000 Training Loss: 0.06051352992653847\n",
      "Epoch 8683/30000 Training Loss: 0.05884971469640732\n",
      "Epoch 8684/30000 Training Loss: 0.05508992075920105\n",
      "Epoch 8685/30000 Training Loss: 0.04472276568412781\n",
      "Epoch 8686/30000 Training Loss: 0.052783917635679245\n",
      "Epoch 8687/30000 Training Loss: 0.053547460585832596\n",
      "Epoch 8688/30000 Training Loss: 0.05972474440932274\n",
      "Epoch 8689/30000 Training Loss: 0.06763383746147156\n",
      "Epoch 8690/30000 Training Loss: 0.05187318101525307\n",
      "Epoch 8691/30000 Training Loss: 0.06000461429357529\n",
      "Epoch 8692/30000 Training Loss: 0.04359849542379379\n",
      "Epoch 8693/30000 Training Loss: 0.07401100546121597\n",
      "Epoch 8694/30000 Training Loss: 0.05384715646505356\n",
      "Epoch 8695/30000 Training Loss: 0.06225089728832245\n",
      "Epoch 8696/30000 Training Loss: 0.050681717693805695\n",
      "Epoch 8697/30000 Training Loss: 0.05196671560406685\n",
      "Epoch 8698/30000 Training Loss: 0.063840851187706\n",
      "Epoch 8699/30000 Training Loss: 0.05068640038371086\n",
      "Epoch 8700/30000 Training Loss: 0.06402776390314102\n",
      "Epoch 8700/30000 Validation Loss: 0.0680961012840271\n",
      "Epoch 8701/30000 Training Loss: 0.07770325988531113\n",
      "Epoch 8702/30000 Training Loss: 0.07296939194202423\n",
      "Epoch 8703/30000 Training Loss: 0.040320806205272675\n",
      "Epoch 8704/30000 Training Loss: 0.05295447260141373\n",
      "Epoch 8705/30000 Training Loss: 0.0533803328871727\n",
      "Epoch 8706/30000 Training Loss: 0.06219274178147316\n",
      "Epoch 8707/30000 Training Loss: 0.05575493723154068\n",
      "Epoch 8708/30000 Training Loss: 0.06395924091339111\n",
      "Epoch 8709/30000 Training Loss: 0.06452198326587677\n",
      "Epoch 8710/30000 Training Loss: 0.05822353810071945\n",
      "Epoch 8711/30000 Training Loss: 0.042741965502500534\n",
      "Epoch 8712/30000 Training Loss: 0.08720104396343231\n",
      "Epoch 8713/30000 Training Loss: 0.057185810059309006\n",
      "Epoch 8714/30000 Training Loss: 0.06916241347789764\n",
      "Epoch 8715/30000 Training Loss: 0.06791029870510101\n",
      "Epoch 8716/30000 Training Loss: 0.0569620355963707\n",
      "Epoch 8717/30000 Training Loss: 0.0448225699365139\n",
      "Epoch 8718/30000 Training Loss: 0.05778253823518753\n",
      "Epoch 8719/30000 Training Loss: 0.046561047434806824\n",
      "Epoch 8720/30000 Training Loss: 0.04887005686759949\n",
      "Epoch 8721/30000 Training Loss: 0.06151437759399414\n",
      "Epoch 8722/30000 Training Loss: 0.0547606386244297\n",
      "Epoch 8723/30000 Training Loss: 0.07893628627061844\n",
      "Epoch 8724/30000 Training Loss: 0.0629962682723999\n",
      "Epoch 8725/30000 Training Loss: 0.05259853973984718\n",
      "Epoch 8726/30000 Training Loss: 0.05194959044456482\n",
      "Epoch 8727/30000 Training Loss: 0.037679560482501984\n",
      "Epoch 8728/30000 Training Loss: 0.07978520542383194\n",
      "Epoch 8729/30000 Training Loss: 0.058639898896217346\n",
      "Epoch 8730/30000 Training Loss: 0.0725928246974945\n",
      "Epoch 8731/30000 Training Loss: 0.0680793821811676\n",
      "Epoch 8732/30000 Training Loss: 0.04856230318546295\n",
      "Epoch 8733/30000 Training Loss: 0.08037293702363968\n",
      "Epoch 8734/30000 Training Loss: 0.04695577546954155\n",
      "Epoch 8735/30000 Training Loss: 0.05482950806617737\n",
      "Epoch 8736/30000 Training Loss: 0.06759750843048096\n",
      "Epoch 8737/30000 Training Loss: 0.05868623033165932\n",
      "Epoch 8738/30000 Training Loss: 0.05612923949956894\n",
      "Epoch 8739/30000 Training Loss: 0.0652453601360321\n",
      "Epoch 8740/30000 Training Loss: 0.0703001394867897\n",
      "Epoch 8741/30000 Training Loss: 0.050241805613040924\n",
      "Epoch 8742/30000 Training Loss: 0.0516725592315197\n",
      "Epoch 8743/30000 Training Loss: 0.05085021257400513\n",
      "Epoch 8744/30000 Training Loss: 0.05953335389494896\n",
      "Epoch 8745/30000 Training Loss: 0.05384316295385361\n",
      "Epoch 8746/30000 Training Loss: 0.05895065888762474\n",
      "Epoch 8747/30000 Training Loss: 0.08597894012928009\n",
      "Epoch 8748/30000 Training Loss: 0.05296768248081207\n",
      "Epoch 8749/30000 Training Loss: 0.06114958971738815\n",
      "Epoch 8750/30000 Training Loss: 0.0655764788389206\n",
      "Epoch 8751/30000 Training Loss: 0.04686731845140457\n",
      "Epoch 8752/30000 Training Loss: 0.060174502432346344\n",
      "Epoch 8753/30000 Training Loss: 0.05004652962088585\n",
      "Epoch 8754/30000 Training Loss: 0.06904781609773636\n",
      "Epoch 8755/30000 Training Loss: 0.0503721721470356\n",
      "Epoch 8756/30000 Training Loss: 0.06496979296207428\n",
      "Epoch 8757/30000 Training Loss: 0.04375932365655899\n",
      "Epoch 8758/30000 Training Loss: 0.05619679018855095\n",
      "Epoch 8759/30000 Training Loss: 0.07252202183008194\n",
      "Epoch 8760/30000 Training Loss: 0.0722387358546257\n",
      "Epoch 8761/30000 Training Loss: 0.05319897457957268\n",
      "Epoch 8762/30000 Training Loss: 0.04863473400473595\n",
      "Epoch 8763/30000 Training Loss: 0.046307411044836044\n",
      "Epoch 8764/30000 Training Loss: 0.05846141278743744\n",
      "Epoch 8765/30000 Training Loss: 0.03865041211247444\n",
      "Epoch 8766/30000 Training Loss: 0.07551500201225281\n",
      "Epoch 8767/30000 Training Loss: 0.04613689333200455\n",
      "Epoch 8768/30000 Training Loss: 0.06570207327604294\n",
      "Epoch 8769/30000 Training Loss: 0.059351928532123566\n",
      "Epoch 8770/30000 Training Loss: 0.062428899109363556\n",
      "Epoch 8771/30000 Training Loss: 0.05989869683980942\n",
      "Epoch 8772/30000 Training Loss: 0.047390684485435486\n",
      "Epoch 8773/30000 Training Loss: 0.06634547561407089\n",
      "Epoch 8774/30000 Training Loss: 0.04767907038331032\n",
      "Epoch 8775/30000 Training Loss: 0.052543360739946365\n",
      "Epoch 8776/30000 Training Loss: 0.07402708381414413\n",
      "Epoch 8777/30000 Training Loss: 0.06066152825951576\n",
      "Epoch 8778/30000 Training Loss: 0.05711612105369568\n",
      "Epoch 8779/30000 Training Loss: 0.05478505790233612\n",
      "Epoch 8780/30000 Training Loss: 0.048118870705366135\n",
      "Epoch 8781/30000 Training Loss: 0.0613311231136322\n",
      "Epoch 8782/30000 Training Loss: 0.051118671894073486\n",
      "Epoch 8783/30000 Training Loss: 0.05766041576862335\n",
      "Epoch 8784/30000 Training Loss: 0.05305417254567146\n",
      "Epoch 8785/30000 Training Loss: 0.05580135062336922\n",
      "Epoch 8786/30000 Training Loss: 0.05894874408841133\n",
      "Epoch 8787/30000 Training Loss: 0.0692647323012352\n",
      "Epoch 8788/30000 Training Loss: 0.04663437232375145\n",
      "Epoch 8789/30000 Training Loss: 0.06560017913579941\n",
      "Epoch 8790/30000 Training Loss: 0.04595077782869339\n",
      "Epoch 8791/30000 Training Loss: 0.06218010187149048\n",
      "Epoch 8792/30000 Training Loss: 0.04689348489046097\n",
      "Epoch 8793/30000 Training Loss: 0.03772041201591492\n",
      "Epoch 8794/30000 Training Loss: 0.047673385590314865\n",
      "Epoch 8795/30000 Training Loss: 0.06754902750253677\n",
      "Epoch 8796/30000 Training Loss: 0.07405015081167221\n",
      "Epoch 8797/30000 Training Loss: 0.060107000172138214\n",
      "Epoch 8798/30000 Training Loss: 0.036102812737226486\n",
      "Epoch 8799/30000 Training Loss: 0.04777435213327408\n",
      "Epoch 8800/30000 Training Loss: 0.04707283526659012\n",
      "Epoch 8800/30000 Validation Loss: 0.04857005178928375\n",
      "Epoch 8801/30000 Training Loss: 0.06363068521022797\n",
      "Epoch 8802/30000 Training Loss: 0.04024199768900871\n",
      "Epoch 8803/30000 Training Loss: 0.07488709688186646\n",
      "Epoch 8804/30000 Training Loss: 0.05366377905011177\n",
      "Epoch 8805/30000 Training Loss: 0.05250052735209465\n",
      "Epoch 8806/30000 Training Loss: 0.04862438887357712\n",
      "Epoch 8807/30000 Training Loss: 0.03682297468185425\n",
      "Epoch 8808/30000 Training Loss: 0.046580251306295395\n",
      "Epoch 8809/30000 Training Loss: 0.05398060381412506\n",
      "Epoch 8810/30000 Training Loss: 0.054547715932130814\n",
      "Epoch 8811/30000 Training Loss: 0.057264771312475204\n",
      "Epoch 8812/30000 Training Loss: 0.04786733165383339\n",
      "Epoch 8813/30000 Training Loss: 0.06261072307825089\n",
      "Epoch 8814/30000 Training Loss: 0.06352686136960983\n",
      "Epoch 8815/30000 Training Loss: 0.05659561604261398\n",
      "Epoch 8816/30000 Training Loss: 0.04660649597644806\n",
      "Epoch 8817/30000 Training Loss: 0.04655836895108223\n",
      "Epoch 8818/30000 Training Loss: 0.047663114964962006\n",
      "Epoch 8819/30000 Training Loss: 0.053090110421180725\n",
      "Epoch 8820/30000 Training Loss: 0.040849145501852036\n",
      "Epoch 8821/30000 Training Loss: 0.06900614500045776\n",
      "Epoch 8822/30000 Training Loss: 0.051150642335414886\n",
      "Epoch 8823/30000 Training Loss: 0.0600099116563797\n",
      "Epoch 8824/30000 Training Loss: 0.045190900564193726\n",
      "Epoch 8825/30000 Training Loss: 0.06007819250226021\n",
      "Epoch 8826/30000 Training Loss: 0.058765020221471786\n",
      "Epoch 8827/30000 Training Loss: 0.04053705558180809\n",
      "Epoch 8828/30000 Training Loss: 0.06718894839286804\n",
      "Epoch 8829/30000 Training Loss: 0.053941965103149414\n",
      "Epoch 8830/30000 Training Loss: 0.05013735592365265\n",
      "Epoch 8831/30000 Training Loss: 0.05704890936613083\n",
      "Epoch 8832/30000 Training Loss: 0.050687313079833984\n",
      "Epoch 8833/30000 Training Loss: 0.04379348084330559\n",
      "Epoch 8834/30000 Training Loss: 0.055407971143722534\n",
      "Epoch 8835/30000 Training Loss: 0.05247528851032257\n",
      "Epoch 8836/30000 Training Loss: 0.05883374065160751\n",
      "Epoch 8837/30000 Training Loss: 0.037556007504463196\n",
      "Epoch 8838/30000 Training Loss: 0.06001834198832512\n",
      "Epoch 8839/30000 Training Loss: 0.05007513239979744\n",
      "Epoch 8840/30000 Training Loss: 0.06410305947065353\n",
      "Epoch 8841/30000 Training Loss: 0.0679965540766716\n",
      "Epoch 8842/30000 Training Loss: 0.052816130220890045\n",
      "Epoch 8843/30000 Training Loss: 0.05586912855505943\n",
      "Epoch 8844/30000 Training Loss: 0.0516064427793026\n",
      "Epoch 8845/30000 Training Loss: 0.04750559851527214\n",
      "Epoch 8846/30000 Training Loss: 0.04849933832883835\n",
      "Epoch 8847/30000 Training Loss: 0.06867386400699615\n",
      "Epoch 8848/30000 Training Loss: 0.058549702167510986\n",
      "Epoch 8849/30000 Training Loss: 0.05502802133560181\n",
      "Epoch 8850/30000 Training Loss: 0.052909791469573975\n",
      "Epoch 8851/30000 Training Loss: 0.0385681688785553\n",
      "Epoch 8852/30000 Training Loss: 0.05220015347003937\n",
      "Epoch 8853/30000 Training Loss: 0.060774676501750946\n",
      "Epoch 8854/30000 Training Loss: 0.059239134192466736\n",
      "Epoch 8855/30000 Training Loss: 0.0660204067826271\n",
      "Epoch 8856/30000 Training Loss: 0.05469369515776634\n",
      "Epoch 8857/30000 Training Loss: 0.05140263959765434\n",
      "Epoch 8858/30000 Training Loss: 0.04848547279834747\n",
      "Epoch 8859/30000 Training Loss: 0.038780853152275085\n",
      "Epoch 8860/30000 Training Loss: 0.04555993154644966\n",
      "Epoch 8861/30000 Training Loss: 0.055148862302303314\n",
      "Epoch 8862/30000 Training Loss: 0.050743475556373596\n",
      "Epoch 8863/30000 Training Loss: 0.047845251858234406\n",
      "Epoch 8864/30000 Training Loss: 0.03786025196313858\n",
      "Epoch 8865/30000 Training Loss: 0.050937533378601074\n",
      "Epoch 8866/30000 Training Loss: 0.06016284227371216\n",
      "Epoch 8867/30000 Training Loss: 0.057791177183389664\n",
      "Epoch 8868/30000 Training Loss: 0.05195620656013489\n",
      "Epoch 8869/30000 Training Loss: 0.07081296294927597\n",
      "Epoch 8870/30000 Training Loss: 0.05444767326116562\n",
      "Epoch 8871/30000 Training Loss: 0.051025018095970154\n",
      "Epoch 8872/30000 Training Loss: 0.06271812319755554\n",
      "Epoch 8873/30000 Training Loss: 0.06009816750884056\n",
      "Epoch 8874/30000 Training Loss: 0.056447628885507584\n",
      "Epoch 8875/30000 Training Loss: 0.06345094740390778\n",
      "Epoch 8876/30000 Training Loss: 0.05452980846166611\n",
      "Epoch 8877/30000 Training Loss: 0.05425604060292244\n",
      "Epoch 8878/30000 Training Loss: 0.05929059535264969\n",
      "Epoch 8879/30000 Training Loss: 0.06462224572896957\n",
      "Epoch 8880/30000 Training Loss: 0.043878376483917236\n",
      "Epoch 8881/30000 Training Loss: 0.042184121906757355\n",
      "Epoch 8882/30000 Training Loss: 0.051023710519075394\n",
      "Epoch 8883/30000 Training Loss: 0.04446067288517952\n",
      "Epoch 8884/30000 Training Loss: 0.06324104964733124\n",
      "Epoch 8885/30000 Training Loss: 0.04133132845163345\n",
      "Epoch 8886/30000 Training Loss: 0.05271822214126587\n",
      "Epoch 8887/30000 Training Loss: 0.05166945233941078\n",
      "Epoch 8888/30000 Training Loss: 0.06247410178184509\n",
      "Epoch 8889/30000 Training Loss: 0.059463199228048325\n",
      "Epoch 8890/30000 Training Loss: 0.04693165048956871\n",
      "Epoch 8891/30000 Training Loss: 0.05892183259129524\n",
      "Epoch 8892/30000 Training Loss: 0.052211739122867584\n",
      "Epoch 8893/30000 Training Loss: 0.05093611031770706\n",
      "Epoch 8894/30000 Training Loss: 0.04618430882692337\n",
      "Epoch 8895/30000 Training Loss: 0.06555427610874176\n",
      "Epoch 8896/30000 Training Loss: 0.04729245603084564\n",
      "Epoch 8897/30000 Training Loss: 0.07566824555397034\n",
      "Epoch 8898/30000 Training Loss: 0.06452920287847519\n",
      "Epoch 8899/30000 Training Loss: 0.06304556876420975\n",
      "Epoch 8900/30000 Training Loss: 0.05202653631567955\n",
      "Epoch 8900/30000 Validation Loss: 0.0565124973654747\n",
      "Epoch 8901/30000 Training Loss: 0.06465362757444382\n",
      "Epoch 8902/30000 Training Loss: 0.05538516491651535\n",
      "Epoch 8903/30000 Training Loss: 0.03773503378033638\n",
      "Epoch 8904/30000 Training Loss: 0.05878414213657379\n",
      "Epoch 8905/30000 Training Loss: 0.048855554312467575\n",
      "Epoch 8906/30000 Training Loss: 0.061481889337301254\n",
      "Epoch 8907/30000 Training Loss: 0.06782103329896927\n",
      "Epoch 8908/30000 Training Loss: 0.05795770511031151\n",
      "Epoch 8909/30000 Training Loss: 0.04500873386859894\n",
      "Epoch 8910/30000 Training Loss: 0.05203869566321373\n",
      "Epoch 8911/30000 Training Loss: 0.04847265034914017\n",
      "Epoch 8912/30000 Training Loss: 0.05414142087101936\n",
      "Epoch 8913/30000 Training Loss: 0.038874551653862\n",
      "Epoch 8914/30000 Training Loss: 0.04415928199887276\n",
      "Epoch 8915/30000 Training Loss: 0.046654731035232544\n",
      "Epoch 8916/30000 Training Loss: 0.06260768324136734\n",
      "Epoch 8917/30000 Training Loss: 0.07055363804101944\n",
      "Epoch 8918/30000 Training Loss: 0.05515696853399277\n",
      "Epoch 8919/30000 Training Loss: 0.05335964262485504\n",
      "Epoch 8920/30000 Training Loss: 0.05772797763347626\n",
      "Epoch 8921/30000 Training Loss: 0.043599214404821396\n",
      "Epoch 8922/30000 Training Loss: 0.049195848405361176\n",
      "Epoch 8923/30000 Training Loss: 0.05838331952691078\n",
      "Epoch 8924/30000 Training Loss: 0.04971001669764519\n",
      "Epoch 8925/30000 Training Loss: 0.0628780946135521\n",
      "Epoch 8926/30000 Training Loss: 0.05799143388867378\n",
      "Epoch 8927/30000 Training Loss: 0.04871492087841034\n",
      "Epoch 8928/30000 Training Loss: 0.05481889098882675\n",
      "Epoch 8929/30000 Training Loss: 0.03840702399611473\n",
      "Epoch 8930/30000 Training Loss: 0.052478887140750885\n",
      "Epoch 8931/30000 Training Loss: 0.05987712740898132\n",
      "Epoch 8932/30000 Training Loss: 0.05003928393125534\n",
      "Epoch 8933/30000 Training Loss: 0.06225203722715378\n",
      "Epoch 8934/30000 Training Loss: 0.06519824266433716\n",
      "Epoch 8935/30000 Training Loss: 0.04357852041721344\n",
      "Epoch 8936/30000 Training Loss: 0.04729684442281723\n",
      "Epoch 8937/30000 Training Loss: 0.03718709945678711\n",
      "Epoch 8938/30000 Training Loss: 0.06082887202501297\n",
      "Epoch 8939/30000 Training Loss: 0.0664653554558754\n",
      "Epoch 8940/30000 Training Loss: 0.061914555728435516\n",
      "Epoch 8941/30000 Training Loss: 0.0500069186091423\n",
      "Epoch 8942/30000 Training Loss: 0.05140956491231918\n",
      "Epoch 8943/30000 Training Loss: 0.05900050327181816\n",
      "Epoch 8944/30000 Training Loss: 0.043554652482271194\n",
      "Epoch 8945/30000 Training Loss: 0.04707830026745796\n",
      "Epoch 8946/30000 Training Loss: 0.0571448840200901\n",
      "Epoch 8947/30000 Training Loss: 0.05079371854662895\n",
      "Epoch 8948/30000 Training Loss: 0.06542038917541504\n",
      "Epoch 8949/30000 Training Loss: 0.06749296188354492\n",
      "Epoch 8950/30000 Training Loss: 0.059909701347351074\n",
      "Epoch 8951/30000 Training Loss: 0.0603327713906765\n",
      "Epoch 8952/30000 Training Loss: 0.06285656243562698\n",
      "Epoch 8953/30000 Training Loss: 0.0613955557346344\n",
      "Epoch 8954/30000 Training Loss: 0.051129139959812164\n",
      "Epoch 8955/30000 Training Loss: 0.050650350749492645\n",
      "Epoch 8956/30000 Training Loss: 0.0453544557094574\n",
      "Epoch 8957/30000 Training Loss: 0.058405663818120956\n",
      "Epoch 8958/30000 Training Loss: 0.059970758855342865\n",
      "Epoch 8959/30000 Training Loss: 0.04750682786107063\n",
      "Epoch 8960/30000 Training Loss: 0.0663231834769249\n",
      "Epoch 8961/30000 Training Loss: 0.053910911083221436\n",
      "Epoch 8962/30000 Training Loss: 0.052686404436826706\n",
      "Epoch 8963/30000 Training Loss: 0.055625297129154205\n",
      "Epoch 8964/30000 Training Loss: 0.05357891693711281\n",
      "Epoch 8965/30000 Training Loss: 0.056258779019117355\n",
      "Epoch 8966/30000 Training Loss: 0.049603596329689026\n",
      "Epoch 8967/30000 Training Loss: 0.04866396635770798\n",
      "Epoch 8968/30000 Training Loss: 0.044255580753088\n",
      "Epoch 8969/30000 Training Loss: 0.04260021820664406\n",
      "Epoch 8970/30000 Training Loss: 0.06814779341220856\n",
      "Epoch 8971/30000 Training Loss: 0.056157052516937256\n",
      "Epoch 8972/30000 Training Loss: 0.043607067316770554\n",
      "Epoch 8973/30000 Training Loss: 0.07123016566038132\n",
      "Epoch 8974/30000 Training Loss: 0.053269170224666595\n",
      "Epoch 8975/30000 Training Loss: 0.051697853952646255\n",
      "Epoch 8976/30000 Training Loss: 0.05582877993583679\n",
      "Epoch 8977/30000 Training Loss: 0.05356702208518982\n",
      "Epoch 8978/30000 Training Loss: 0.06450025737285614\n",
      "Epoch 8979/30000 Training Loss: 0.051917195320129395\n",
      "Epoch 8980/30000 Training Loss: 0.052149929106235504\n",
      "Epoch 8981/30000 Training Loss: 0.04445904865860939\n",
      "Epoch 8982/30000 Training Loss: 0.05831972509622574\n",
      "Epoch 8983/30000 Training Loss: 0.0572807602584362\n",
      "Epoch 8984/30000 Training Loss: 0.06935298442840576\n",
      "Epoch 8985/30000 Training Loss: 0.05893146991729736\n",
      "Epoch 8986/30000 Training Loss: 0.04438864067196846\n",
      "Epoch 8987/30000 Training Loss: 0.05413457378745079\n",
      "Epoch 8988/30000 Training Loss: 0.03983451798558235\n",
      "Epoch 8989/30000 Training Loss: 0.07014966011047363\n",
      "Epoch 8990/30000 Training Loss: 0.052808791399002075\n",
      "Epoch 8991/30000 Training Loss: 0.04853661358356476\n",
      "Epoch 8992/30000 Training Loss: 0.07308981567621231\n",
      "Epoch 8993/30000 Training Loss: 0.04444700852036476\n",
      "Epoch 8994/30000 Training Loss: 0.04390064626932144\n",
      "Epoch 8995/30000 Training Loss: 0.07433204352855682\n",
      "Epoch 8996/30000 Training Loss: 0.054334189742803574\n",
      "Epoch 8997/30000 Training Loss: 0.05527814105153084\n",
      "Epoch 8998/30000 Training Loss: 0.06476618349552155\n",
      "Epoch 8999/30000 Training Loss: 0.049520887434482574\n",
      "Epoch 9000/30000 Training Loss: 0.07479419559240341\n",
      "Epoch 9000/30000 Validation Loss: 0.05249485746026039\n",
      "Epoch 9001/30000 Training Loss: 0.04710473492741585\n",
      "Epoch 9002/30000 Training Loss: 0.04307202622294426\n",
      "Epoch 9003/30000 Training Loss: 0.04875734820961952\n",
      "Epoch 9004/30000 Training Loss: 0.054538361728191376\n",
      "Epoch 9005/30000 Training Loss: 0.04448318108916283\n",
      "Epoch 9006/30000 Training Loss: 0.03452153503894806\n",
      "Epoch 9007/30000 Training Loss: 0.059252988547086716\n",
      "Epoch 9008/30000 Training Loss: 0.053408317267894745\n",
      "Epoch 9009/30000 Training Loss: 0.04575137421488762\n",
      "Epoch 9010/30000 Training Loss: 0.06378191709518433\n",
      "Epoch 9011/30000 Training Loss: 0.07070502638816833\n",
      "Epoch 9012/30000 Training Loss: 0.04819994047284126\n",
      "Epoch 9013/30000 Training Loss: 0.04889920353889465\n",
      "Epoch 9014/30000 Training Loss: 0.06287133693695068\n",
      "Epoch 9015/30000 Training Loss: 0.06728225946426392\n",
      "Epoch 9016/30000 Training Loss: 0.06007785350084305\n",
      "Epoch 9017/30000 Training Loss: 0.044317662715911865\n",
      "Epoch 9018/30000 Training Loss: 0.07314931601285934\n",
      "Epoch 9019/30000 Training Loss: 0.06340548396110535\n",
      "Epoch 9020/30000 Training Loss: 0.0513458177447319\n",
      "Epoch 9021/30000 Training Loss: 0.04843961447477341\n",
      "Epoch 9022/30000 Training Loss: 0.05780395120382309\n",
      "Epoch 9023/30000 Training Loss: 0.04204631224274635\n",
      "Epoch 9024/30000 Training Loss: 0.04858986288309097\n",
      "Epoch 9025/30000 Training Loss: 0.047153208404779434\n",
      "Epoch 9026/30000 Training Loss: 0.052303701639175415\n",
      "Epoch 9027/30000 Training Loss: 0.05482298880815506\n",
      "Epoch 9028/30000 Training Loss: 0.06872346997261047\n",
      "Epoch 9029/30000 Training Loss: 0.05154622346162796\n",
      "Epoch 9030/30000 Training Loss: 0.08311579376459122\n",
      "Epoch 9031/30000 Training Loss: 0.054374776780605316\n",
      "Epoch 9032/30000 Training Loss: 0.05558549612760544\n",
      "Epoch 9033/30000 Training Loss: 0.056041788309812546\n",
      "Epoch 9034/30000 Training Loss: 0.05617959797382355\n",
      "Epoch 9035/30000 Training Loss: 0.05828949436545372\n",
      "Epoch 9036/30000 Training Loss: 0.05541436746716499\n",
      "Epoch 9037/30000 Training Loss: 0.06781312078237534\n",
      "Epoch 9038/30000 Training Loss: 0.04194970056414604\n",
      "Epoch 9039/30000 Training Loss: 0.0482499822974205\n",
      "Epoch 9040/30000 Training Loss: 0.06104501336812973\n",
      "Epoch 9041/30000 Training Loss: 0.04912162944674492\n",
      "Epoch 9042/30000 Training Loss: 0.06065613403916359\n",
      "Epoch 9043/30000 Training Loss: 0.05043056234717369\n",
      "Epoch 9044/30000 Training Loss: 0.07677257806062698\n",
      "Epoch 9045/30000 Training Loss: 0.04716949164867401\n",
      "Epoch 9046/30000 Training Loss: 0.03892701864242554\n",
      "Epoch 9047/30000 Training Loss: 0.05542195960879326\n",
      "Epoch 9048/30000 Training Loss: 0.06169041991233826\n",
      "Epoch 9049/30000 Training Loss: 0.0657208263874054\n",
      "Epoch 9050/30000 Training Loss: 0.05533413589000702\n",
      "Epoch 9051/30000 Training Loss: 0.05688999965786934\n",
      "Epoch 9052/30000 Training Loss: 0.06781438738107681\n",
      "Epoch 9053/30000 Training Loss: 0.045238666236400604\n",
      "Epoch 9054/30000 Training Loss: 0.060006096959114075\n",
      "Epoch 9055/30000 Training Loss: 0.07134304195642471\n",
      "Epoch 9056/30000 Training Loss: 0.052644871175289154\n",
      "Epoch 9057/30000 Training Loss: 0.04790940880775452\n",
      "Epoch 9058/30000 Training Loss: 0.04543779417872429\n",
      "Epoch 9059/30000 Training Loss: 0.049068063497543335\n",
      "Epoch 9060/30000 Training Loss: 0.06582378596067429\n",
      "Epoch 9061/30000 Training Loss: 0.05995369702577591\n",
      "Epoch 9062/30000 Training Loss: 0.05065879598259926\n",
      "Epoch 9063/30000 Training Loss: 0.04897802323102951\n",
      "Epoch 9064/30000 Training Loss: 0.04187445342540741\n",
      "Epoch 9065/30000 Training Loss: 0.06060095876455307\n",
      "Epoch 9066/30000 Training Loss: 0.06012304127216339\n",
      "Epoch 9067/30000 Training Loss: 0.0578954704105854\n",
      "Epoch 9068/30000 Training Loss: 0.038181766867637634\n",
      "Epoch 9069/30000 Training Loss: 0.05169617012143135\n",
      "Epoch 9070/30000 Training Loss: 0.04701516777276993\n",
      "Epoch 9071/30000 Training Loss: 0.06584187597036362\n",
      "Epoch 9072/30000 Training Loss: 0.05485498905181885\n",
      "Epoch 9073/30000 Training Loss: 0.0493352934718132\n",
      "Epoch 9074/30000 Training Loss: 0.04968983680009842\n",
      "Epoch 9075/30000 Training Loss: 0.04960949346423149\n",
      "Epoch 9076/30000 Training Loss: 0.05160015448927879\n",
      "Epoch 9077/30000 Training Loss: 0.05833027511835098\n",
      "Epoch 9078/30000 Training Loss: 0.056617118418216705\n",
      "Epoch 9079/30000 Training Loss: 0.04967479780316353\n",
      "Epoch 9080/30000 Training Loss: 0.046716488897800446\n",
      "Epoch 9081/30000 Training Loss: 0.043826110661029816\n",
      "Epoch 9082/30000 Training Loss: 0.062101319432258606\n",
      "Epoch 9083/30000 Training Loss: 0.07654990255832672\n",
      "Epoch 9084/30000 Training Loss: 0.06555534154176712\n",
      "Epoch 9085/30000 Training Loss: 0.06412263959646225\n",
      "Epoch 9086/30000 Training Loss: 0.06231347471475601\n",
      "Epoch 9087/30000 Training Loss: 0.06222403049468994\n",
      "Epoch 9088/30000 Training Loss: 0.050426363945007324\n",
      "Epoch 9089/30000 Training Loss: 0.05435939133167267\n",
      "Epoch 9090/30000 Training Loss: 0.05391954258084297\n",
      "Epoch 9091/30000 Training Loss: 0.051845937967300415\n",
      "Epoch 9092/30000 Training Loss: 0.062098149210214615\n",
      "Epoch 9093/30000 Training Loss: 0.060774482786655426\n",
      "Epoch 9094/30000 Training Loss: 0.04804760962724686\n",
      "Epoch 9095/30000 Training Loss: 0.05980141460895538\n",
      "Epoch 9096/30000 Training Loss: 0.05770468711853027\n",
      "Epoch 9097/30000 Training Loss: 0.041809167712926865\n",
      "Epoch 9098/30000 Training Loss: 0.047474782913923264\n",
      "Epoch 9099/30000 Training Loss: 0.048531193286180496\n",
      "Epoch 9100/30000 Training Loss: 0.06835158169269562\n",
      "Epoch 9100/30000 Validation Loss: 0.05824269354343414\n",
      "Epoch 9101/30000 Training Loss: 0.055587515234947205\n",
      "Epoch 9102/30000 Training Loss: 0.043757740408182144\n",
      "Epoch 9103/30000 Training Loss: 0.07069458067417145\n",
      "Epoch 9104/30000 Training Loss: 0.05646321177482605\n",
      "Epoch 9105/30000 Training Loss: 0.06407248973846436\n",
      "Epoch 9106/30000 Training Loss: 0.05146820843219757\n",
      "Epoch 9107/30000 Training Loss: 0.04824988916516304\n",
      "Epoch 9108/30000 Training Loss: 0.05892910063266754\n",
      "Epoch 9109/30000 Training Loss: 0.051300421357154846\n",
      "Epoch 9110/30000 Training Loss: 0.05849214643239975\n",
      "Epoch 9111/30000 Training Loss: 0.052482277154922485\n",
      "Epoch 9112/30000 Training Loss: 0.07160665839910507\n",
      "Epoch 9113/30000 Training Loss: 0.06193256378173828\n",
      "Epoch 9114/30000 Training Loss: 0.058350834995508194\n",
      "Epoch 9115/30000 Training Loss: 0.04392560198903084\n",
      "Epoch 9116/30000 Training Loss: 0.048260901123285294\n",
      "Epoch 9117/30000 Training Loss: 0.051401905715465546\n",
      "Epoch 9118/30000 Training Loss: 0.048496827483177185\n",
      "Epoch 9119/30000 Training Loss: 0.061819665133953094\n",
      "Epoch 9120/30000 Training Loss: 0.0579720176756382\n",
      "Epoch 9121/30000 Training Loss: 0.056267768144607544\n",
      "Epoch 9122/30000 Training Loss: 0.045481644570827484\n",
      "Epoch 9123/30000 Training Loss: 0.06194978952407837\n",
      "Epoch 9124/30000 Training Loss: 0.051562003791332245\n",
      "Epoch 9125/30000 Training Loss: 0.06251925230026245\n",
      "Epoch 9126/30000 Training Loss: 0.058420367538928986\n",
      "Epoch 9127/30000 Training Loss: 0.053798675537109375\n",
      "Epoch 9128/30000 Training Loss: 0.07887089252471924\n",
      "Epoch 9129/30000 Training Loss: 0.05111004039645195\n",
      "Epoch 9130/30000 Training Loss: 0.05148465186357498\n",
      "Epoch 9131/30000 Training Loss: 0.06115888059139252\n",
      "Epoch 9132/30000 Training Loss: 0.05848662555217743\n",
      "Epoch 9133/30000 Training Loss: 0.04707641899585724\n",
      "Epoch 9134/30000 Training Loss: 0.05458158999681473\n",
      "Epoch 9135/30000 Training Loss: 0.06160551309585571\n",
      "Epoch 9136/30000 Training Loss: 0.057182155549526215\n",
      "Epoch 9137/30000 Training Loss: 0.07010500878095627\n",
      "Epoch 9138/30000 Training Loss: 0.04970638453960419\n",
      "Epoch 9139/30000 Training Loss: 0.042770881205797195\n",
      "Epoch 9140/30000 Training Loss: 0.05752609670162201\n",
      "Epoch 9141/30000 Training Loss: 0.06613289564847946\n",
      "Epoch 9142/30000 Training Loss: 0.06244666129350662\n",
      "Epoch 9143/30000 Training Loss: 0.054740194231271744\n",
      "Epoch 9144/30000 Training Loss: 0.05079316720366478\n",
      "Epoch 9145/30000 Training Loss: 0.045692361891269684\n",
      "Epoch 9146/30000 Training Loss: 0.04770342633128166\n",
      "Epoch 9147/30000 Training Loss: 0.050849609076976776\n",
      "Epoch 9148/30000 Training Loss: 0.0554981455206871\n",
      "Epoch 9149/30000 Training Loss: 0.06128837168216705\n",
      "Epoch 9150/30000 Training Loss: 0.04029713571071625\n",
      "Epoch 9151/30000 Training Loss: 0.059915222227573395\n",
      "Epoch 9152/30000 Training Loss: 0.043463099747896194\n",
      "Epoch 9153/30000 Training Loss: 0.05383308231830597\n",
      "Epoch 9154/30000 Training Loss: 0.05012241750955582\n",
      "Epoch 9155/30000 Training Loss: 0.05828389525413513\n",
      "Epoch 9156/30000 Training Loss: 0.059568457305431366\n",
      "Epoch 9157/30000 Training Loss: 0.05487219616770744\n",
      "Epoch 9158/30000 Training Loss: 0.035343725234270096\n",
      "Epoch 9159/30000 Training Loss: 0.05267350375652313\n",
      "Epoch 9160/30000 Training Loss: 0.05883678048849106\n",
      "Epoch 9161/30000 Training Loss: 0.03433447703719139\n",
      "Epoch 9162/30000 Training Loss: 0.04335135594010353\n",
      "Epoch 9163/30000 Training Loss: 0.05309811979532242\n",
      "Epoch 9164/30000 Training Loss: 0.04536324739456177\n",
      "Epoch 9165/30000 Training Loss: 0.07067806273698807\n",
      "Epoch 9166/30000 Training Loss: 0.04225439578294754\n",
      "Epoch 9167/30000 Training Loss: 0.0631641298532486\n",
      "Epoch 9168/30000 Training Loss: 0.05665061995387077\n",
      "Epoch 9169/30000 Training Loss: 0.061566129326820374\n",
      "Epoch 9170/30000 Training Loss: 0.06679093837738037\n",
      "Epoch 9171/30000 Training Loss: 0.04795955866575241\n",
      "Epoch 9172/30000 Training Loss: 0.043601322919130325\n",
      "Epoch 9173/30000 Training Loss: 0.059259649366140366\n",
      "Epoch 9174/30000 Training Loss: 0.06483392417430878\n",
      "Epoch 9175/30000 Training Loss: 0.06088624149560928\n",
      "Epoch 9176/30000 Training Loss: 0.04378600791096687\n",
      "Epoch 9177/30000 Training Loss: 0.07517316937446594\n",
      "Epoch 9178/30000 Training Loss: 0.056519765406847\n",
      "Epoch 9179/30000 Training Loss: 0.06972761452198029\n",
      "Epoch 9180/30000 Training Loss: 0.04061012715101242\n",
      "Epoch 9181/30000 Training Loss: 0.04688766598701477\n",
      "Epoch 9182/30000 Training Loss: 0.0659773200750351\n",
      "Epoch 9183/30000 Training Loss: 0.06781693547964096\n",
      "Epoch 9184/30000 Training Loss: 0.04466870799660683\n",
      "Epoch 9185/30000 Training Loss: 0.04505287483334541\n",
      "Epoch 9186/30000 Training Loss: 0.050131410360336304\n",
      "Epoch 9187/30000 Training Loss: 0.05879225581884384\n",
      "Epoch 9188/30000 Training Loss: 0.04191550239920616\n",
      "Epoch 9189/30000 Training Loss: 0.04388683661818504\n",
      "Epoch 9190/30000 Training Loss: 0.06157214567065239\n",
      "Epoch 9191/30000 Training Loss: 0.06665246188640594\n",
      "Epoch 9192/30000 Training Loss: 0.055886540561914444\n",
      "Epoch 9193/30000 Training Loss: 0.04068392887711525\n",
      "Epoch 9194/30000 Training Loss: 0.055965255945920944\n",
      "Epoch 9195/30000 Training Loss: 0.07609601318836212\n",
      "Epoch 9196/30000 Training Loss: 0.05419795215129852\n",
      "Epoch 9197/30000 Training Loss: 0.054827362298965454\n",
      "Epoch 9198/30000 Training Loss: 0.047964077442884445\n",
      "Epoch 9199/30000 Training Loss: 0.053718939423561096\n",
      "Epoch 9200/30000 Training Loss: 0.05165751278400421\n",
      "Epoch 9200/30000 Validation Loss: 0.060468535870313644\n",
      "Epoch 9201/30000 Training Loss: 0.048225946724414825\n",
      "Epoch 9202/30000 Training Loss: 0.06617435067892075\n",
      "Epoch 9203/30000 Training Loss: 0.05616796761751175\n",
      "Epoch 9204/30000 Training Loss: 0.06415048241615295\n",
      "Epoch 9205/30000 Training Loss: 0.053880296647548676\n",
      "Epoch 9206/30000 Training Loss: 0.045162055641412735\n",
      "Epoch 9207/30000 Training Loss: 0.06362733244895935\n",
      "Epoch 9208/30000 Training Loss: 0.05570736154913902\n",
      "Epoch 9209/30000 Training Loss: 0.03921133279800415\n",
      "Epoch 9210/30000 Training Loss: 0.04663826897740364\n",
      "Epoch 9211/30000 Training Loss: 0.04878566786646843\n",
      "Epoch 9212/30000 Training Loss: 0.0522318035364151\n",
      "Epoch 9213/30000 Training Loss: 0.03918248414993286\n",
      "Epoch 9214/30000 Training Loss: 0.06828191876411438\n",
      "Epoch 9215/30000 Training Loss: 0.06454245746135712\n",
      "Epoch 9216/30000 Training Loss: 0.04134747013449669\n",
      "Epoch 9217/30000 Training Loss: 0.046536706387996674\n",
      "Epoch 9218/30000 Training Loss: 0.04800420254468918\n",
      "Epoch 9219/30000 Training Loss: 0.07893136888742447\n",
      "Epoch 9220/30000 Training Loss: 0.05015251040458679\n",
      "Epoch 9221/30000 Training Loss: 0.0658988207578659\n",
      "Epoch 9222/30000 Training Loss: 0.04851390793919563\n",
      "Epoch 9223/30000 Training Loss: 0.055125895887613297\n",
      "Epoch 9224/30000 Training Loss: 0.057467661798000336\n",
      "Epoch 9225/30000 Training Loss: 0.06253375113010406\n",
      "Epoch 9226/30000 Training Loss: 0.06152176111936569\n",
      "Epoch 9227/30000 Training Loss: 0.052317917346954346\n",
      "Epoch 9228/30000 Training Loss: 0.055471766740083694\n",
      "Epoch 9229/30000 Training Loss: 0.04147256910800934\n",
      "Epoch 9230/30000 Training Loss: 0.042699262499809265\n",
      "Epoch 9231/30000 Training Loss: 0.04237261414527893\n",
      "Epoch 9232/30000 Training Loss: 0.06590575724840164\n",
      "Epoch 9233/30000 Training Loss: 0.03735068440437317\n",
      "Epoch 9234/30000 Training Loss: 0.06187702715396881\n",
      "Epoch 9235/30000 Training Loss: 0.058084212243556976\n",
      "Epoch 9236/30000 Training Loss: 0.058872904628515244\n",
      "Epoch 9237/30000 Training Loss: 0.053894899785518646\n",
      "Epoch 9238/30000 Training Loss: 0.040236227214336395\n",
      "Epoch 9239/30000 Training Loss: 0.05840572714805603\n",
      "Epoch 9240/30000 Training Loss: 0.07626721262931824\n",
      "Epoch 9241/30000 Training Loss: 0.04413510486483574\n",
      "Epoch 9242/30000 Training Loss: 0.061908356845378876\n",
      "Epoch 9243/30000 Training Loss: 0.053545352071523666\n",
      "Epoch 9244/30000 Training Loss: 0.04416453093290329\n",
      "Epoch 9245/30000 Training Loss: 0.0623132586479187\n",
      "Epoch 9246/30000 Training Loss: 0.06953023374080658\n",
      "Epoch 9247/30000 Training Loss: 0.06380552053451538\n",
      "Epoch 9248/30000 Training Loss: 0.0626482367515564\n",
      "Epoch 9249/30000 Training Loss: 0.07415113598108292\n",
      "Epoch 9250/30000 Training Loss: 0.044058412313461304\n",
      "Epoch 9251/30000 Training Loss: 0.055811524391174316\n",
      "Epoch 9252/30000 Training Loss: 0.056123413145542145\n",
      "Epoch 9253/30000 Training Loss: 0.04398730769753456\n",
      "Epoch 9254/30000 Training Loss: 0.05144207179546356\n",
      "Epoch 9255/30000 Training Loss: 0.06136662885546684\n",
      "Epoch 9256/30000 Training Loss: 0.0641024112701416\n",
      "Epoch 9257/30000 Training Loss: 0.049186840653419495\n",
      "Epoch 9258/30000 Training Loss: 0.057707637548446655\n",
      "Epoch 9259/30000 Training Loss: 0.051884107291698456\n",
      "Epoch 9260/30000 Training Loss: 0.04602429270744324\n",
      "Epoch 9261/30000 Training Loss: 0.06339218467473984\n",
      "Epoch 9262/30000 Training Loss: 0.05101554095745087\n",
      "Epoch 9263/30000 Training Loss: 0.05913364887237549\n",
      "Epoch 9264/30000 Training Loss: 0.046421438455581665\n",
      "Epoch 9265/30000 Training Loss: 0.038725823163986206\n",
      "Epoch 9266/30000 Training Loss: 0.06080130115151405\n",
      "Epoch 9267/30000 Training Loss: 0.05815917253494263\n",
      "Epoch 9268/30000 Training Loss: 0.06912646442651749\n",
      "Epoch 9269/30000 Training Loss: 0.05349878966808319\n",
      "Epoch 9270/30000 Training Loss: 0.06963089108467102\n",
      "Epoch 9271/30000 Training Loss: 0.04369497671723366\n",
      "Epoch 9272/30000 Training Loss: 0.04310975596308708\n",
      "Epoch 9273/30000 Training Loss: 0.060596972703933716\n",
      "Epoch 9274/30000 Training Loss: 0.053997702896595\n",
      "Epoch 9275/30000 Training Loss: 0.06008405610918999\n",
      "Epoch 9276/30000 Training Loss: 0.06020601466298103\n",
      "Epoch 9277/30000 Training Loss: 0.05860287696123123\n",
      "Epoch 9278/30000 Training Loss: 0.05538071319460869\n",
      "Epoch 9279/30000 Training Loss: 0.05246341973543167\n",
      "Epoch 9280/30000 Training Loss: 0.05001194402575493\n",
      "Epoch 9281/30000 Training Loss: 0.04594898596405983\n",
      "Epoch 9282/30000 Training Loss: 0.055613283067941666\n",
      "Epoch 9283/30000 Training Loss: 0.0713692232966423\n",
      "Epoch 9284/30000 Training Loss: 0.04711393639445305\n",
      "Epoch 9285/30000 Training Loss: 0.05819464474916458\n",
      "Epoch 9286/30000 Training Loss: 0.04710222780704498\n",
      "Epoch 9287/30000 Training Loss: 0.05134999006986618\n",
      "Epoch 9288/30000 Training Loss: 0.05687844753265381\n",
      "Epoch 9289/30000 Training Loss: 0.053138744086027145\n",
      "Epoch 9290/30000 Training Loss: 0.05807843804359436\n",
      "Epoch 9291/30000 Training Loss: 0.06930099427700043\n",
      "Epoch 9292/30000 Training Loss: 0.06726162135601044\n",
      "Epoch 9293/30000 Training Loss: 0.05392596125602722\n",
      "Epoch 9294/30000 Training Loss: 0.05123612657189369\n",
      "Epoch 9295/30000 Training Loss: 0.0636676698923111\n",
      "Epoch 9296/30000 Training Loss: 0.05180574581027031\n",
      "Epoch 9297/30000 Training Loss: 0.046050313860177994\n",
      "Epoch 9298/30000 Training Loss: 0.04176380857825279\n",
      "Epoch 9299/30000 Training Loss: 0.04844754934310913\n",
      "Epoch 9300/30000 Training Loss: 0.07000334560871124\n",
      "Epoch 9300/30000 Validation Loss: 0.05580463260412216\n",
      "Epoch 9301/30000 Training Loss: 0.056202467530965805\n",
      "Epoch 9302/30000 Training Loss: 0.06569062918424606\n",
      "Epoch 9303/30000 Training Loss: 0.05423823744058609\n",
      "Epoch 9304/30000 Training Loss: 0.05431317538022995\n",
      "Epoch 9305/30000 Training Loss: 0.05346100032329559\n",
      "Epoch 9306/30000 Training Loss: 0.05009858310222626\n",
      "Epoch 9307/30000 Training Loss: 0.04788221791386604\n",
      "Epoch 9308/30000 Training Loss: 0.058521583676338196\n",
      "Epoch 9309/30000 Training Loss: 0.04853050783276558\n",
      "Epoch 9310/30000 Training Loss: 0.05294256657361984\n",
      "Epoch 9311/30000 Training Loss: 0.06537320464849472\n",
      "Epoch 9312/30000 Training Loss: 0.04506518319249153\n",
      "Epoch 9313/30000 Training Loss: 0.04123867675662041\n",
      "Epoch 9314/30000 Training Loss: 0.043314129114151\n",
      "Epoch 9315/30000 Training Loss: 0.06228446587920189\n",
      "Epoch 9316/30000 Training Loss: 0.06157340109348297\n",
      "Epoch 9317/30000 Training Loss: 0.06063171476125717\n",
      "Epoch 9318/30000 Training Loss: 0.05694687366485596\n",
      "Epoch 9319/30000 Training Loss: 0.05178185552358627\n",
      "Epoch 9320/30000 Training Loss: 0.054625675082206726\n",
      "Epoch 9321/30000 Training Loss: 0.050516389310359955\n",
      "Epoch 9322/30000 Training Loss: 0.052318960428237915\n",
      "Epoch 9323/30000 Training Loss: 0.056575335562229156\n",
      "Epoch 9324/30000 Training Loss: 0.051165930926799774\n",
      "Epoch 9325/30000 Training Loss: 0.06363718211650848\n",
      "Epoch 9326/30000 Training Loss: 0.058649905025959015\n",
      "Epoch 9327/30000 Training Loss: 0.039293382316827774\n",
      "Epoch 9328/30000 Training Loss: 0.04833289235830307\n",
      "Epoch 9329/30000 Training Loss: 0.057940974831581116\n",
      "Epoch 9330/30000 Training Loss: 0.045394912362098694\n",
      "Epoch 9331/30000 Training Loss: 0.04189663380384445\n",
      "Epoch 9332/30000 Training Loss: 0.061753660440444946\n",
      "Epoch 9333/30000 Training Loss: 0.05932672321796417\n",
      "Epoch 9334/30000 Training Loss: 0.06643572449684143\n",
      "Epoch 9335/30000 Training Loss: 0.04972066730260849\n",
      "Epoch 9336/30000 Training Loss: 0.055407218635082245\n",
      "Epoch 9337/30000 Training Loss: 0.05197498947381973\n",
      "Epoch 9338/30000 Training Loss: 0.05279000848531723\n",
      "Epoch 9339/30000 Training Loss: 0.05693326145410538\n",
      "Epoch 9340/30000 Training Loss: 0.05674569681286812\n",
      "Epoch 9341/30000 Training Loss: 0.04838382080197334\n",
      "Epoch 9342/30000 Training Loss: 0.07051735371351242\n",
      "Epoch 9343/30000 Training Loss: 0.03788240626454353\n",
      "Epoch 9344/30000 Training Loss: 0.06360924988985062\n",
      "Epoch 9345/30000 Training Loss: 0.07044647634029388\n",
      "Epoch 9346/30000 Training Loss: 0.08546578884124756\n",
      "Epoch 9347/30000 Training Loss: 0.07864934206008911\n",
      "Epoch 9348/30000 Training Loss: 0.04978614300489426\n",
      "Epoch 9349/30000 Training Loss: 0.06159014254808426\n",
      "Epoch 9350/30000 Training Loss: 0.04983367398381233\n",
      "Epoch 9351/30000 Training Loss: 0.06938903033733368\n",
      "Epoch 9352/30000 Training Loss: 0.058439575135707855\n",
      "Epoch 9353/30000 Training Loss: 0.06742989271879196\n",
      "Epoch 9354/30000 Training Loss: 0.04397793114185333\n",
      "Epoch 9355/30000 Training Loss: 0.06545630842447281\n",
      "Epoch 9356/30000 Training Loss: 0.060425370931625366\n",
      "Epoch 9357/30000 Training Loss: 0.060983091592788696\n",
      "Epoch 9358/30000 Training Loss: 0.047037992626428604\n",
      "Epoch 9359/30000 Training Loss: 0.04510372877120972\n",
      "Epoch 9360/30000 Training Loss: 0.06178383529186249\n",
      "Epoch 9361/30000 Training Loss: 0.04388030990958214\n",
      "Epoch 9362/30000 Training Loss: 0.04409100115299225\n",
      "Epoch 9363/30000 Training Loss: 0.05731165409088135\n",
      "Epoch 9364/30000 Training Loss: 0.06748989969491959\n",
      "Epoch 9365/30000 Training Loss: 0.046809256076812744\n",
      "Epoch 9366/30000 Training Loss: 0.06703683733940125\n",
      "Epoch 9367/30000 Training Loss: 0.043632522225379944\n",
      "Epoch 9368/30000 Training Loss: 0.0570765919983387\n",
      "Epoch 9369/30000 Training Loss: 0.038651272654533386\n",
      "Epoch 9370/30000 Training Loss: 0.060859985649585724\n",
      "Epoch 9371/30000 Training Loss: 0.05971159040927887\n",
      "Epoch 9372/30000 Training Loss: 0.052983347326517105\n",
      "Epoch 9373/30000 Training Loss: 0.05826645717024803\n",
      "Epoch 9374/30000 Training Loss: 0.06128114461898804\n",
      "Epoch 9375/30000 Training Loss: 0.06175985187292099\n",
      "Epoch 9376/30000 Training Loss: 0.0559459887444973\n",
      "Epoch 9377/30000 Training Loss: 0.04636986553668976\n",
      "Epoch 9378/30000 Training Loss: 0.05303823575377464\n",
      "Epoch 9379/30000 Training Loss: 0.06539788842201233\n",
      "Epoch 9380/30000 Training Loss: 0.04930616915225983\n",
      "Epoch 9381/30000 Training Loss: 0.06032876670360565\n",
      "Epoch 9382/30000 Training Loss: 0.04906085506081581\n",
      "Epoch 9383/30000 Training Loss: 0.0628063827753067\n",
      "Epoch 9384/30000 Training Loss: 0.05369238182902336\n",
      "Epoch 9385/30000 Training Loss: 0.04957542195916176\n",
      "Epoch 9386/30000 Training Loss: 0.04579663649201393\n",
      "Epoch 9387/30000 Training Loss: 0.060103971511125565\n",
      "Epoch 9388/30000 Training Loss: 0.05235055834054947\n",
      "Epoch 9389/30000 Training Loss: 0.05365168675780296\n",
      "Epoch 9390/30000 Training Loss: 0.03886237367987633\n",
      "Epoch 9391/30000 Training Loss: 0.04277273640036583\n",
      "Epoch 9392/30000 Training Loss: 0.06897781044244766\n",
      "Epoch 9393/30000 Training Loss: 0.05105593800544739\n",
      "Epoch 9394/30000 Training Loss: 0.07294461876153946\n",
      "Epoch 9395/30000 Training Loss: 0.06047147884964943\n",
      "Epoch 9396/30000 Training Loss: 0.050477925688028336\n",
      "Epoch 9397/30000 Training Loss: 0.050415318459272385\n",
      "Epoch 9398/30000 Training Loss: 0.07956141233444214\n",
      "Epoch 9399/30000 Training Loss: 0.03868219256401062\n",
      "Epoch 9400/30000 Training Loss: 0.04767078533768654\n",
      "Epoch 9400/30000 Validation Loss: 0.04318805783987045\n",
      "Epoch 9401/30000 Training Loss: 0.049971166998147964\n",
      "Epoch 9402/30000 Training Loss: 0.047034334391355515\n",
      "Epoch 9403/30000 Training Loss: 0.037603709846735\n",
      "Epoch 9404/30000 Training Loss: 0.05993816256523132\n",
      "Epoch 9405/30000 Training Loss: 0.05679599940776825\n",
      "Epoch 9406/30000 Training Loss: 0.046466220170259476\n",
      "Epoch 9407/30000 Training Loss: 0.056420765817165375\n",
      "Epoch 9408/30000 Training Loss: 0.05065343528985977\n",
      "Epoch 9409/30000 Training Loss: 0.0570928193628788\n",
      "Epoch 9410/30000 Training Loss: 0.05099732056260109\n",
      "Epoch 9411/30000 Training Loss: 0.056071553379297256\n",
      "Epoch 9412/30000 Training Loss: 0.04046115279197693\n",
      "Epoch 9413/30000 Training Loss: 0.051835887134075165\n",
      "Epoch 9414/30000 Training Loss: 0.08392494916915894\n",
      "Epoch 9415/30000 Training Loss: 0.05716293677687645\n",
      "Epoch 9416/30000 Training Loss: 0.048238806426525116\n",
      "Epoch 9417/30000 Training Loss: 0.05637212470173836\n",
      "Epoch 9418/30000 Training Loss: 0.03822542354464531\n",
      "Epoch 9419/30000 Training Loss: 0.04464982450008392\n",
      "Epoch 9420/30000 Training Loss: 0.046371545642614365\n",
      "Epoch 9421/30000 Training Loss: 0.05670423060655594\n",
      "Epoch 9422/30000 Training Loss: 0.0616886243224144\n",
      "Epoch 9423/30000 Training Loss: 0.05578167364001274\n",
      "Epoch 9424/30000 Training Loss: 0.05685245990753174\n",
      "Epoch 9425/30000 Training Loss: 0.07173279672861099\n",
      "Epoch 9426/30000 Training Loss: 0.05315234512090683\n",
      "Epoch 9427/30000 Training Loss: 0.03929857164621353\n",
      "Epoch 9428/30000 Training Loss: 0.06477703899145126\n",
      "Epoch 9429/30000 Training Loss: 0.05523429065942764\n",
      "Epoch 9430/30000 Training Loss: 0.04548830911517143\n",
      "Epoch 9431/30000 Training Loss: 0.060738272964954376\n",
      "Epoch 9432/30000 Training Loss: 0.05085931345820427\n",
      "Epoch 9433/30000 Training Loss: 0.04050988703966141\n",
      "Epoch 9434/30000 Training Loss: 0.06752697378396988\n",
      "Epoch 9435/30000 Training Loss: 0.06271830946207047\n",
      "Epoch 9436/30000 Training Loss: 0.05026207119226456\n",
      "Epoch 9437/30000 Training Loss: 0.06812356412410736\n",
      "Epoch 9438/30000 Training Loss: 0.05459430068731308\n",
      "Epoch 9439/30000 Training Loss: 0.061300307512283325\n",
      "Epoch 9440/30000 Training Loss: 0.04813721403479576\n",
      "Epoch 9441/30000 Training Loss: 0.05784167721867561\n",
      "Epoch 9442/30000 Training Loss: 0.058257706463336945\n",
      "Epoch 9443/30000 Training Loss: 0.06749537587165833\n",
      "Epoch 9444/30000 Training Loss: 0.052126653492450714\n",
      "Epoch 9445/30000 Training Loss: 0.058704979717731476\n",
      "Epoch 9446/30000 Training Loss: 0.054991044104099274\n",
      "Epoch 9447/30000 Training Loss: 0.05013251677155495\n",
      "Epoch 9448/30000 Training Loss: 0.049611128866672516\n",
      "Epoch 9449/30000 Training Loss: 0.06901177763938904\n",
      "Epoch 9450/30000 Training Loss: 0.06729365140199661\n",
      "Epoch 9451/30000 Training Loss: 0.04294357821345329\n",
      "Epoch 9452/30000 Training Loss: 0.05018971860408783\n",
      "Epoch 9453/30000 Training Loss: 0.04816398769617081\n",
      "Epoch 9454/30000 Training Loss: 0.06340962648391724\n",
      "Epoch 9455/30000 Training Loss: 0.058034345507621765\n",
      "Epoch 9456/30000 Training Loss: 0.05221421644091606\n",
      "Epoch 9457/30000 Training Loss: 0.07080753892660141\n",
      "Epoch 9458/30000 Training Loss: 0.051767878234386444\n",
      "Epoch 9459/30000 Training Loss: 0.060552701354026794\n",
      "Epoch 9460/30000 Training Loss: 0.04398510605096817\n",
      "Epoch 9461/30000 Training Loss: 0.050876434892416\n",
      "Epoch 9462/30000 Training Loss: 0.04728885740041733\n",
      "Epoch 9463/30000 Training Loss: 0.05810950696468353\n",
      "Epoch 9464/30000 Training Loss: 0.06499941647052765\n",
      "Epoch 9465/30000 Training Loss: 0.04631105437874794\n",
      "Epoch 9466/30000 Training Loss: 0.05111812427639961\n",
      "Epoch 9467/30000 Training Loss: 0.06098267808556557\n",
      "Epoch 9468/30000 Training Loss: 0.054411061108112335\n",
      "Epoch 9469/30000 Training Loss: 0.044830553233623505\n",
      "Epoch 9470/30000 Training Loss: 0.05505995824933052\n",
      "Epoch 9471/30000 Training Loss: 0.0667339563369751\n",
      "Epoch 9472/30000 Training Loss: 0.06194962188601494\n",
      "Epoch 9473/30000 Training Loss: 0.05155651271343231\n",
      "Epoch 9474/30000 Training Loss: 0.04183129221200943\n",
      "Epoch 9475/30000 Training Loss: 0.048093050718307495\n",
      "Epoch 9476/30000 Training Loss: 0.054554153233766556\n",
      "Epoch 9477/30000 Training Loss: 0.0517951101064682\n",
      "Epoch 9478/30000 Training Loss: 0.05855586379766464\n",
      "Epoch 9479/30000 Training Loss: 0.050376422703266144\n",
      "Epoch 9480/30000 Training Loss: 0.064324751496315\n",
      "Epoch 9481/30000 Training Loss: 0.036430615931749344\n",
      "Epoch 9482/30000 Training Loss: 0.04810333251953125\n",
      "Epoch 9483/30000 Training Loss: 0.05361916124820709\n",
      "Epoch 9484/30000 Training Loss: 0.04803714528679848\n",
      "Epoch 9485/30000 Training Loss: 0.04522985965013504\n",
      "Epoch 9486/30000 Training Loss: 0.04278528690338135\n",
      "Epoch 9487/30000 Training Loss: 0.06085588037967682\n",
      "Epoch 9488/30000 Training Loss: 0.05092243105173111\n",
      "Epoch 9489/30000 Training Loss: 0.05242324247956276\n",
      "Epoch 9490/30000 Training Loss: 0.0483163520693779\n",
      "Epoch 9491/30000 Training Loss: 0.05636875703930855\n",
      "Epoch 9492/30000 Training Loss: 0.06284165382385254\n",
      "Epoch 9493/30000 Training Loss: 0.050062745809555054\n",
      "Epoch 9494/30000 Training Loss: 0.06284307688474655\n",
      "Epoch 9495/30000 Training Loss: 0.05586032196879387\n",
      "Epoch 9496/30000 Training Loss: 0.06991669535636902\n",
      "Epoch 9497/30000 Training Loss: 0.061281926929950714\n",
      "Epoch 9498/30000 Training Loss: 0.059146374464035034\n",
      "Epoch 9499/30000 Training Loss: 0.050137121230363846\n",
      "Epoch 9500/30000 Training Loss: 0.07260386645793915\n",
      "Epoch 9500/30000 Validation Loss: 0.04334534332156181\n",
      "Epoch 9501/30000 Training Loss: 0.078457772731781\n",
      "Epoch 9502/30000 Training Loss: 0.04836583137512207\n",
      "Epoch 9503/30000 Training Loss: 0.05890445411205292\n",
      "Epoch 9504/30000 Training Loss: 0.05645737051963806\n",
      "Epoch 9505/30000 Training Loss: 0.05127672106027603\n",
      "Epoch 9506/30000 Training Loss: 0.04759971424937248\n",
      "Epoch 9507/30000 Training Loss: 0.057141467928886414\n",
      "Epoch 9508/30000 Training Loss: 0.04498967155814171\n",
      "Epoch 9509/30000 Training Loss: 0.05613350868225098\n",
      "Epoch 9510/30000 Training Loss: 0.04504931718111038\n",
      "Epoch 9511/30000 Training Loss: 0.047305602580308914\n",
      "Epoch 9512/30000 Training Loss: 0.053491510450839996\n",
      "Epoch 9513/30000 Training Loss: 0.0491718053817749\n",
      "Epoch 9514/30000 Training Loss: 0.04381338134407997\n",
      "Epoch 9515/30000 Training Loss: 0.04355446621775627\n",
      "Epoch 9516/30000 Training Loss: 0.05439068004488945\n",
      "Epoch 9517/30000 Training Loss: 0.05336097627878189\n",
      "Epoch 9518/30000 Training Loss: 0.05193348228931427\n",
      "Epoch 9519/30000 Training Loss: 0.06928669661283493\n",
      "Epoch 9520/30000 Training Loss: 0.047303829342126846\n",
      "Epoch 9521/30000 Training Loss: 0.04728548601269722\n",
      "Epoch 9522/30000 Training Loss: 0.04232732951641083\n",
      "Epoch 9523/30000 Training Loss: 0.05228348821401596\n",
      "Epoch 9524/30000 Training Loss: 0.04841254651546478\n",
      "Epoch 9525/30000 Training Loss: 0.051954515278339386\n",
      "Epoch 9526/30000 Training Loss: 0.07278122752904892\n",
      "Epoch 9527/30000 Training Loss: 0.05487733334302902\n",
      "Epoch 9528/30000 Training Loss: 0.046813029795885086\n",
      "Epoch 9529/30000 Training Loss: 0.04524317383766174\n",
      "Epoch 9530/30000 Training Loss: 0.05676184967160225\n",
      "Epoch 9531/30000 Training Loss: 0.04876790940761566\n",
      "Epoch 9532/30000 Training Loss: 0.0733550414443016\n",
      "Epoch 9533/30000 Training Loss: 0.06541506201028824\n",
      "Epoch 9534/30000 Training Loss: 0.06954452395439148\n",
      "Epoch 9535/30000 Training Loss: 0.05302829295396805\n",
      "Epoch 9536/30000 Training Loss: 0.049711912870407104\n",
      "Epoch 9537/30000 Training Loss: 0.06160067394375801\n",
      "Epoch 9538/30000 Training Loss: 0.06660018861293793\n",
      "Epoch 9539/30000 Training Loss: 0.05801248550415039\n",
      "Epoch 9540/30000 Training Loss: 0.06389167159795761\n",
      "Epoch 9541/30000 Training Loss: 0.059898555278778076\n",
      "Epoch 9542/30000 Training Loss: 0.052073605358600616\n",
      "Epoch 9543/30000 Training Loss: 0.04847574979066849\n",
      "Epoch 9544/30000 Training Loss: 0.039776865392923355\n",
      "Epoch 9545/30000 Training Loss: 0.05355200171470642\n",
      "Epoch 9546/30000 Training Loss: 0.0562828928232193\n",
      "Epoch 9547/30000 Training Loss: 0.051918357610702515\n",
      "Epoch 9548/30000 Training Loss: 0.04391869157552719\n",
      "Epoch 9549/30000 Training Loss: 0.07244634628295898\n",
      "Epoch 9550/30000 Training Loss: 0.05626970902085304\n",
      "Epoch 9551/30000 Training Loss: 0.06524421274662018\n",
      "Epoch 9552/30000 Training Loss: 0.045577749609947205\n",
      "Epoch 9553/30000 Training Loss: 0.05199125409126282\n",
      "Epoch 9554/30000 Training Loss: 0.057192642241716385\n",
      "Epoch 9555/30000 Training Loss: 0.04138992726802826\n",
      "Epoch 9556/30000 Training Loss: 0.057235341519117355\n",
      "Epoch 9557/30000 Training Loss: 0.05680125951766968\n",
      "Epoch 9558/30000 Training Loss: 0.048464491963386536\n",
      "Epoch 9559/30000 Training Loss: 0.07825732231140137\n",
      "Epoch 9560/30000 Training Loss: 0.05693819001317024\n",
      "Epoch 9561/30000 Training Loss: 0.07231873273849487\n",
      "Epoch 9562/30000 Training Loss: 0.07596807926893234\n",
      "Epoch 9563/30000 Training Loss: 0.07451026141643524\n",
      "Epoch 9564/30000 Training Loss: 0.05879465863108635\n",
      "Epoch 9565/30000 Training Loss: 0.04285971447825432\n",
      "Epoch 9566/30000 Training Loss: 0.04113088920712471\n",
      "Epoch 9567/30000 Training Loss: 0.04811115562915802\n",
      "Epoch 9568/30000 Training Loss: 0.045781172811985016\n",
      "Epoch 9569/30000 Training Loss: 0.04355514794588089\n",
      "Epoch 9570/30000 Training Loss: 0.045272763818502426\n",
      "Epoch 9571/30000 Training Loss: 0.04167866334319115\n",
      "Epoch 9572/30000 Training Loss: 0.04877571761608124\n",
      "Epoch 9573/30000 Training Loss: 0.04726311191916466\n",
      "Epoch 9574/30000 Training Loss: 0.05027090758085251\n",
      "Epoch 9575/30000 Training Loss: 0.06236112117767334\n",
      "Epoch 9576/30000 Training Loss: 0.04669685661792755\n",
      "Epoch 9577/30000 Training Loss: 0.05670301616191864\n",
      "Epoch 9578/30000 Training Loss: 0.058344271034002304\n",
      "Epoch 9579/30000 Training Loss: 0.052577078342437744\n",
      "Epoch 9580/30000 Training Loss: 0.057698220014572144\n",
      "Epoch 9581/30000 Training Loss: 0.04466598853468895\n",
      "Epoch 9582/30000 Training Loss: 0.060020625591278076\n",
      "Epoch 9583/30000 Training Loss: 0.058955177664756775\n",
      "Epoch 9584/30000 Training Loss: 0.05239129066467285\n",
      "Epoch 9585/30000 Training Loss: 0.05852475017309189\n",
      "Epoch 9586/30000 Training Loss: 0.052368469536304474\n",
      "Epoch 9587/30000 Training Loss: 0.05292566493153572\n",
      "Epoch 9588/30000 Training Loss: 0.05464097112417221\n",
      "Epoch 9589/30000 Training Loss: 0.04261619970202446\n",
      "Epoch 9590/30000 Training Loss: 0.06286080181598663\n",
      "Epoch 9591/30000 Training Loss: 0.06646791100502014\n",
      "Epoch 9592/30000 Training Loss: 0.05626176670193672\n",
      "Epoch 9593/30000 Training Loss: 0.05632773041725159\n",
      "Epoch 9594/30000 Training Loss: 0.06236771494150162\n",
      "Epoch 9595/30000 Training Loss: 0.06026642769575119\n",
      "Epoch 9596/30000 Training Loss: 0.051378823816776276\n",
      "Epoch 9597/30000 Training Loss: 0.04982822388410568\n",
      "Epoch 9598/30000 Training Loss: 0.05475929379463196\n",
      "Epoch 9599/30000 Training Loss: 0.0536431260406971\n",
      "Epoch 9600/30000 Training Loss: 0.05076337233185768\n",
      "Epoch 9600/30000 Validation Loss: 0.06792748719453812\n",
      "Epoch 9601/30000 Training Loss: 0.0689736157655716\n",
      "Epoch 9602/30000 Training Loss: 0.06433382630348206\n",
      "Epoch 9603/30000 Training Loss: 0.06541595607995987\n",
      "Epoch 9604/30000 Training Loss: 0.055688176304101944\n",
      "Epoch 9605/30000 Training Loss: 0.05367741733789444\n",
      "Epoch 9606/30000 Training Loss: 0.048934709280729294\n",
      "Epoch 9607/30000 Training Loss: 0.05425916984677315\n",
      "Epoch 9608/30000 Training Loss: 0.04691877216100693\n",
      "Epoch 9609/30000 Training Loss: 0.04338202625513077\n",
      "Epoch 9610/30000 Training Loss: 0.05387602373957634\n",
      "Epoch 9611/30000 Training Loss: 0.04935874044895172\n",
      "Epoch 9612/30000 Training Loss: 0.042408786714076996\n",
      "Epoch 9613/30000 Training Loss: 0.04567796364426613\n",
      "Epoch 9614/30000 Training Loss: 0.04455255717039108\n",
      "Epoch 9615/30000 Training Loss: 0.0742187574505806\n",
      "Epoch 9616/30000 Training Loss: 0.05677252262830734\n",
      "Epoch 9617/30000 Training Loss: 0.048895273357629776\n",
      "Epoch 9618/30000 Training Loss: 0.054494958370923996\n",
      "Epoch 9619/30000 Training Loss: 0.06069844216108322\n",
      "Epoch 9620/30000 Training Loss: 0.05871613323688507\n",
      "Epoch 9621/30000 Training Loss: 0.04407864436507225\n",
      "Epoch 9622/30000 Training Loss: 0.05866410210728645\n",
      "Epoch 9623/30000 Training Loss: 0.05316571146249771\n",
      "Epoch 9624/30000 Training Loss: 0.04270383343100548\n",
      "Epoch 9625/30000 Training Loss: 0.0589078925549984\n",
      "Epoch 9626/30000 Training Loss: 0.05796395242214203\n",
      "Epoch 9627/30000 Training Loss: 0.05013445392251015\n",
      "Epoch 9628/30000 Training Loss: 0.05656374618411064\n",
      "Epoch 9629/30000 Training Loss: 0.047904085367918015\n",
      "Epoch 9630/30000 Training Loss: 0.043693795800209045\n",
      "Epoch 9631/30000 Training Loss: 0.05031820759177208\n",
      "Epoch 9632/30000 Training Loss: 0.046612150967121124\n",
      "Epoch 9633/30000 Training Loss: 0.04836779087781906\n",
      "Epoch 9634/30000 Training Loss: 0.05637264624238014\n",
      "Epoch 9635/30000 Training Loss: 0.05946110188961029\n",
      "Epoch 9636/30000 Training Loss: 0.06073722988367081\n",
      "Epoch 9637/30000 Training Loss: 0.0669604241847992\n",
      "Epoch 9638/30000 Training Loss: 0.05367680639028549\n",
      "Epoch 9639/30000 Training Loss: 0.05914095416665077\n",
      "Epoch 9640/30000 Training Loss: 0.05534008890390396\n",
      "Epoch 9641/30000 Training Loss: 0.048506960272789\n",
      "Epoch 9642/30000 Training Loss: 0.05706096813082695\n",
      "Epoch 9643/30000 Training Loss: 0.06478572636842728\n",
      "Epoch 9644/30000 Training Loss: 0.06737885624170303\n",
      "Epoch 9645/30000 Training Loss: 0.06317654997110367\n",
      "Epoch 9646/30000 Training Loss: 0.06109485775232315\n",
      "Epoch 9647/30000 Training Loss: 0.0535987988114357\n",
      "Epoch 9648/30000 Training Loss: 0.03894059732556343\n",
      "Epoch 9649/30000 Training Loss: 0.059770092368125916\n",
      "Epoch 9650/30000 Training Loss: 0.04948827624320984\n",
      "Epoch 9651/30000 Training Loss: 0.06483659148216248\n",
      "Epoch 9652/30000 Training Loss: 0.05452362075448036\n",
      "Epoch 9653/30000 Training Loss: 0.04403779283165932\n",
      "Epoch 9654/30000 Training Loss: 0.04689190164208412\n",
      "Epoch 9655/30000 Training Loss: 0.05226941406726837\n",
      "Epoch 9656/30000 Training Loss: 0.05444334074854851\n",
      "Epoch 9657/30000 Training Loss: 0.048449039459228516\n",
      "Epoch 9658/30000 Training Loss: 0.0539536327123642\n",
      "Epoch 9659/30000 Training Loss: 0.04628695920109749\n",
      "Epoch 9660/30000 Training Loss: 0.06468474119901657\n",
      "Epoch 9661/30000 Training Loss: 0.04654481261968613\n",
      "Epoch 9662/30000 Training Loss: 0.06985872238874435\n",
      "Epoch 9663/30000 Training Loss: 0.052104316651821136\n",
      "Epoch 9664/30000 Training Loss: 0.04970424249768257\n",
      "Epoch 9665/30000 Training Loss: 0.05112116411328316\n",
      "Epoch 9666/30000 Training Loss: 0.0633421316742897\n",
      "Epoch 9667/30000 Training Loss: 0.056841909885406494\n",
      "Epoch 9668/30000 Training Loss: 0.05734400078654289\n",
      "Epoch 9669/30000 Training Loss: 0.05351521074771881\n",
      "Epoch 9670/30000 Training Loss: 0.052829623222351074\n",
      "Epoch 9671/30000 Training Loss: 0.07593835145235062\n",
      "Epoch 9672/30000 Training Loss: 0.05637414753437042\n",
      "Epoch 9673/30000 Training Loss: 0.04397711902856827\n",
      "Epoch 9674/30000 Training Loss: 0.04724689573049545\n",
      "Epoch 9675/30000 Training Loss: 0.04600953310728073\n",
      "Epoch 9676/30000 Training Loss: 0.06172448396682739\n",
      "Epoch 9677/30000 Training Loss: 0.06708983331918716\n",
      "Epoch 9678/30000 Training Loss: 0.05200524255633354\n",
      "Epoch 9679/30000 Training Loss: 0.042798083275556564\n",
      "Epoch 9680/30000 Training Loss: 0.06821674853563309\n",
      "Epoch 9681/30000 Training Loss: 0.06633801013231277\n",
      "Epoch 9682/30000 Training Loss: 0.047794830054044724\n",
      "Epoch 9683/30000 Training Loss: 0.05036132410168648\n",
      "Epoch 9684/30000 Training Loss: 0.04457557946443558\n",
      "Epoch 9685/30000 Training Loss: 0.04915092885494232\n",
      "Epoch 9686/30000 Training Loss: 0.051029741764068604\n",
      "Epoch 9687/30000 Training Loss: 0.04985051229596138\n",
      "Epoch 9688/30000 Training Loss: 0.04009498655796051\n",
      "Epoch 9689/30000 Training Loss: 0.05239589884877205\n",
      "Epoch 9690/30000 Training Loss: 0.04598322510719299\n",
      "Epoch 9691/30000 Training Loss: 0.06753392517566681\n",
      "Epoch 9692/30000 Training Loss: 0.05740126967430115\n",
      "Epoch 9693/30000 Training Loss: 0.057615384459495544\n",
      "Epoch 9694/30000 Training Loss: 0.0610690638422966\n",
      "Epoch 9695/30000 Training Loss: 0.04640393704175949\n",
      "Epoch 9696/30000 Training Loss: 0.06033618375658989\n",
      "Epoch 9697/30000 Training Loss: 0.04303615912795067\n",
      "Epoch 9698/30000 Training Loss: 0.043342042714357376\n",
      "Epoch 9699/30000 Training Loss: 0.05153382942080498\n",
      "Epoch 9700/30000 Training Loss: 0.0417838916182518\n",
      "Epoch 9700/30000 Validation Loss: 0.06932612508535385\n",
      "Epoch 9701/30000 Training Loss: 0.07944650202989578\n",
      "Epoch 9702/30000 Training Loss: 0.05898442491889\n",
      "Epoch 9703/30000 Training Loss: 0.048646748065948486\n",
      "Epoch 9704/30000 Training Loss: 0.06154141575098038\n",
      "Epoch 9705/30000 Training Loss: 0.048479631543159485\n",
      "Epoch 9706/30000 Training Loss: 0.041704677045345306\n",
      "Epoch 9707/30000 Training Loss: 0.056006595492362976\n",
      "Epoch 9708/30000 Training Loss: 0.0467856302857399\n",
      "Epoch 9709/30000 Training Loss: 0.04431068152189255\n",
      "Epoch 9710/30000 Training Loss: 0.0613841637969017\n",
      "Epoch 9711/30000 Training Loss: 0.05515442043542862\n",
      "Epoch 9712/30000 Training Loss: 0.06187763065099716\n",
      "Epoch 9713/30000 Training Loss: 0.04922071844339371\n",
      "Epoch 9714/30000 Training Loss: 0.05337214097380638\n",
      "Epoch 9715/30000 Training Loss: 0.05645933747291565\n",
      "Epoch 9716/30000 Training Loss: 0.04573749005794525\n",
      "Epoch 9717/30000 Training Loss: 0.03613243252038956\n",
      "Epoch 9718/30000 Training Loss: 0.057620346546173096\n",
      "Epoch 9719/30000 Training Loss: 0.05940451845526695\n",
      "Epoch 9720/30000 Training Loss: 0.05882997065782547\n",
      "Epoch 9721/30000 Training Loss: 0.048605870455503464\n",
      "Epoch 9722/30000 Training Loss: 0.04696991294622421\n",
      "Epoch 9723/30000 Training Loss: 0.05536334961652756\n",
      "Epoch 9724/30000 Training Loss: 0.0609857402741909\n",
      "Epoch 9725/30000 Training Loss: 0.05412669852375984\n",
      "Epoch 9726/30000 Training Loss: 0.05214327573776245\n",
      "Epoch 9727/30000 Training Loss: 0.0708916038274765\n",
      "Epoch 9728/30000 Training Loss: 0.07329070568084717\n",
      "Epoch 9729/30000 Training Loss: 0.054377250373363495\n",
      "Epoch 9730/30000 Training Loss: 0.044740285724401474\n",
      "Epoch 9731/30000 Training Loss: 0.04784254729747772\n",
      "Epoch 9732/30000 Training Loss: 0.06412035971879959\n",
      "Epoch 9733/30000 Training Loss: 0.05717696622014046\n",
      "Epoch 9734/30000 Training Loss: 0.06404191255569458\n",
      "Epoch 9735/30000 Training Loss: 0.06419241428375244\n",
      "Epoch 9736/30000 Training Loss: 0.042416490614414215\n",
      "Epoch 9737/30000 Training Loss: 0.059776537120342255\n",
      "Epoch 9738/30000 Training Loss: 0.0574682392179966\n",
      "Epoch 9739/30000 Training Loss: 0.06601718068122864\n",
      "Epoch 9740/30000 Training Loss: 0.05245862901210785\n",
      "Epoch 9741/30000 Training Loss: 0.04865865781903267\n",
      "Epoch 9742/30000 Training Loss: 0.049863558262586594\n",
      "Epoch 9743/30000 Training Loss: 0.048291295766830444\n",
      "Epoch 9744/30000 Training Loss: 0.05892308056354523\n",
      "Epoch 9745/30000 Training Loss: 0.05669713020324707\n",
      "Epoch 9746/30000 Training Loss: 0.058745212852954865\n",
      "Epoch 9747/30000 Training Loss: 0.06306025385856628\n",
      "Epoch 9748/30000 Training Loss: 0.04845498129725456\n",
      "Epoch 9749/30000 Training Loss: 0.0515829399228096\n",
      "Epoch 9750/30000 Training Loss: 0.06322399526834488\n",
      "Epoch 9751/30000 Training Loss: 0.05214567109942436\n",
      "Epoch 9752/30000 Training Loss: 0.05115818232297897\n",
      "Epoch 9753/30000 Training Loss: 0.05810268968343735\n",
      "Epoch 9754/30000 Training Loss: 0.05091368407011032\n",
      "Epoch 9755/30000 Training Loss: 0.038911160081624985\n",
      "Epoch 9756/30000 Training Loss: 0.05475732311606407\n",
      "Epoch 9757/30000 Training Loss: 0.05939680337905884\n",
      "Epoch 9758/30000 Training Loss: 0.05701713263988495\n",
      "Epoch 9759/30000 Training Loss: 0.055112145841121674\n",
      "Epoch 9760/30000 Training Loss: 0.04387390613555908\n",
      "Epoch 9761/30000 Training Loss: 0.04959391802549362\n",
      "Epoch 9762/30000 Training Loss: 0.04090715944766998\n",
      "Epoch 9763/30000 Training Loss: 0.07645436376333237\n",
      "Epoch 9764/30000 Training Loss: 0.06300635635852814\n",
      "Epoch 9765/30000 Training Loss: 0.048965971916913986\n",
      "Epoch 9766/30000 Training Loss: 0.0654740259051323\n",
      "Epoch 9767/30000 Training Loss: 0.05277256295084953\n",
      "Epoch 9768/30000 Training Loss: 0.048966825008392334\n",
      "Epoch 9769/30000 Training Loss: 0.06331806629896164\n",
      "Epoch 9770/30000 Training Loss: 0.04025314748287201\n",
      "Epoch 9771/30000 Training Loss: 0.058097150176763535\n",
      "Epoch 9772/30000 Training Loss: 0.04809778556227684\n",
      "Epoch 9773/30000 Training Loss: 0.05930425226688385\n",
      "Epoch 9774/30000 Training Loss: 0.042188070714473724\n",
      "Epoch 9775/30000 Training Loss: 0.052483197301626205\n",
      "Epoch 9776/30000 Training Loss: 0.06455079466104507\n",
      "Epoch 9777/30000 Training Loss: 0.064576156437397\n",
      "Epoch 9778/30000 Training Loss: 0.06715763360261917\n",
      "Epoch 9779/30000 Training Loss: 0.0578727051615715\n",
      "Epoch 9780/30000 Training Loss: 0.052601784467697144\n",
      "Epoch 9781/30000 Training Loss: 0.04966914653778076\n",
      "Epoch 9782/30000 Training Loss: 0.058504052460193634\n",
      "Epoch 9783/30000 Training Loss: 0.060627322643995285\n",
      "Epoch 9784/30000 Training Loss: 0.04424739629030228\n",
      "Epoch 9785/30000 Training Loss: 0.054969411343336105\n",
      "Epoch 9786/30000 Training Loss: 0.054157670587301254\n",
      "Epoch 9787/30000 Training Loss: 0.058679692447185516\n",
      "Epoch 9788/30000 Training Loss: 0.04841671884059906\n",
      "Epoch 9789/30000 Training Loss: 0.055585213005542755\n",
      "Epoch 9790/30000 Training Loss: 0.04885131120681763\n",
      "Epoch 9791/30000 Training Loss: 0.045969557017087936\n",
      "Epoch 9792/30000 Training Loss: 0.05552225187420845\n",
      "Epoch 9793/30000 Training Loss: 0.061387721449136734\n",
      "Epoch 9794/30000 Training Loss: 0.06195490062236786\n",
      "Epoch 9795/30000 Training Loss: 0.046223826706409454\n",
      "Epoch 9796/30000 Training Loss: 0.06908701360225677\n",
      "Epoch 9797/30000 Training Loss: 0.05362211540341377\n",
      "Epoch 9798/30000 Training Loss: 0.04906594753265381\n",
      "Epoch 9799/30000 Training Loss: 0.04134616255760193\n",
      "Epoch 9800/30000 Training Loss: 0.0477789081633091\n",
      "Epoch 9800/30000 Validation Loss: 0.06731409579515457\n",
      "Epoch 9801/30000 Training Loss: 0.045789655297994614\n",
      "Epoch 9802/30000 Training Loss: 0.05991429463028908\n",
      "Epoch 9803/30000 Training Loss: 0.054786019027233124\n",
      "Epoch 9804/30000 Training Loss: 0.03937562555074692\n",
      "Epoch 9805/30000 Training Loss: 0.05765416473150253\n",
      "Epoch 9806/30000 Training Loss: 0.03844226896762848\n",
      "Epoch 9807/30000 Training Loss: 0.04578253999352455\n",
      "Epoch 9808/30000 Training Loss: 0.03957477957010269\n",
      "Epoch 9809/30000 Training Loss: 0.0504954569041729\n",
      "Epoch 9810/30000 Training Loss: 0.046232134103775024\n",
      "Epoch 9811/30000 Training Loss: 0.046172626316547394\n",
      "Epoch 9812/30000 Training Loss: 0.037429120391607285\n",
      "Epoch 9813/30000 Training Loss: 0.06103483960032463\n",
      "Epoch 9814/30000 Training Loss: 0.04485975205898285\n",
      "Epoch 9815/30000 Training Loss: 0.06317654997110367\n",
      "Epoch 9816/30000 Training Loss: 0.044420186430215836\n",
      "Epoch 9817/30000 Training Loss: 0.05394143983721733\n",
      "Epoch 9818/30000 Training Loss: 0.05115257203578949\n",
      "Epoch 9819/30000 Training Loss: 0.06568503379821777\n",
      "Epoch 9820/30000 Training Loss: 0.06922179460525513\n",
      "Epoch 9821/30000 Training Loss: 0.050955887883901596\n",
      "Epoch 9822/30000 Training Loss: 0.06691855937242508\n",
      "Epoch 9823/30000 Training Loss: 0.042748235166072845\n",
      "Epoch 9824/30000 Training Loss: 0.045599304139614105\n",
      "Epoch 9825/30000 Training Loss: 0.047126900404691696\n",
      "Epoch 9826/30000 Training Loss: 0.056890614330768585\n",
      "Epoch 9827/30000 Training Loss: 0.0566084161400795\n",
      "Epoch 9828/30000 Training Loss: 0.06589271873235703\n",
      "Epoch 9829/30000 Training Loss: 0.05155184119939804\n",
      "Epoch 9830/30000 Training Loss: 0.047031220048666\n",
      "Epoch 9831/30000 Training Loss: 0.049347952008247375\n",
      "Epoch 9832/30000 Training Loss: 0.06332564353942871\n",
      "Epoch 9833/30000 Training Loss: 0.06154242530465126\n",
      "Epoch 9834/30000 Training Loss: 0.051197245717048645\n",
      "Epoch 9835/30000 Training Loss: 0.04990401118993759\n",
      "Epoch 9836/30000 Training Loss: 0.06234896928071976\n",
      "Epoch 9837/30000 Training Loss: 0.05264854058623314\n",
      "Epoch 9838/30000 Training Loss: 0.042182017117738724\n",
      "Epoch 9839/30000 Training Loss: 0.041684553027153015\n",
      "Epoch 9840/30000 Training Loss: 0.04889901354908943\n",
      "Epoch 9841/30000 Training Loss: 0.06614457815885544\n",
      "Epoch 9842/30000 Training Loss: 0.04200248792767525\n",
      "Epoch 9843/30000 Training Loss: 0.04751455783843994\n",
      "Epoch 9844/30000 Training Loss: 0.06559355556964874\n",
      "Epoch 9845/30000 Training Loss: 0.051833055913448334\n",
      "Epoch 9846/30000 Training Loss: 0.06547972559928894\n",
      "Epoch 9847/30000 Training Loss: 0.04822669178247452\n",
      "Epoch 9848/30000 Training Loss: 0.0487380214035511\n",
      "Epoch 9849/30000 Training Loss: 0.060249775648117065\n",
      "Epoch 9850/30000 Training Loss: 0.059571489691734314\n",
      "Epoch 9851/30000 Training Loss: 0.05994197726249695\n",
      "Epoch 9852/30000 Training Loss: 0.059031687676906586\n",
      "Epoch 9853/30000 Training Loss: 0.0550091452896595\n",
      "Epoch 9854/30000 Training Loss: 0.05541221797466278\n",
      "Epoch 9855/30000 Training Loss: 0.05833391100168228\n",
      "Epoch 9856/30000 Training Loss: 0.04396427795290947\n",
      "Epoch 9857/30000 Training Loss: 0.059044044464826584\n",
      "Epoch 9858/30000 Training Loss: 0.05005469173192978\n",
      "Epoch 9859/30000 Training Loss: 0.05920807272195816\n",
      "Epoch 9860/30000 Training Loss: 0.03825414553284645\n",
      "Epoch 9861/30000 Training Loss: 0.05776161327958107\n",
      "Epoch 9862/30000 Training Loss: 0.061308495700359344\n",
      "Epoch 9863/30000 Training Loss: 0.05664949119091034\n",
      "Epoch 9864/30000 Training Loss: 0.07482808828353882\n",
      "Epoch 9865/30000 Training Loss: 0.07712748646736145\n",
      "Epoch 9866/30000 Training Loss: 0.04511473327875137\n",
      "Epoch 9867/30000 Training Loss: 0.03968656808137894\n",
      "Epoch 9868/30000 Training Loss: 0.05431533604860306\n",
      "Epoch 9869/30000 Training Loss: 0.049421217292547226\n",
      "Epoch 9870/30000 Training Loss: 0.05019720643758774\n",
      "Epoch 9871/30000 Training Loss: 0.06298410892486572\n",
      "Epoch 9872/30000 Training Loss: 0.049279384315013885\n",
      "Epoch 9873/30000 Training Loss: 0.0480535514652729\n",
      "Epoch 9874/30000 Training Loss: 0.053467974066734314\n",
      "Epoch 9875/30000 Training Loss: 0.05001832917332649\n",
      "Epoch 9876/30000 Training Loss: 0.043593768030405045\n",
      "Epoch 9877/30000 Training Loss: 0.05948421731591225\n",
      "Epoch 9878/30000 Training Loss: 0.04228103160858154\n",
      "Epoch 9879/30000 Training Loss: 0.06055120751261711\n",
      "Epoch 9880/30000 Training Loss: 0.05117223039269447\n",
      "Epoch 9881/30000 Training Loss: 0.05296219885349274\n",
      "Epoch 9882/30000 Training Loss: 0.042100388556718826\n",
      "Epoch 9883/30000 Training Loss: 0.04272201284766197\n",
      "Epoch 9884/30000 Training Loss: 0.049247968941926956\n",
      "Epoch 9885/30000 Training Loss: 0.04792111739516258\n",
      "Epoch 9886/30000 Training Loss: 0.05719606578350067\n",
      "Epoch 9887/30000 Training Loss: 0.0556991845369339\n",
      "Epoch 9888/30000 Training Loss: 0.05199239403009415\n",
      "Epoch 9889/30000 Training Loss: 0.037556491792201996\n",
      "Epoch 9890/30000 Training Loss: 0.05030883103609085\n",
      "Epoch 9891/30000 Training Loss: 0.05267585068941116\n",
      "Epoch 9892/30000 Training Loss: 0.053110577166080475\n",
      "Epoch 9893/30000 Training Loss: 0.04087743163108826\n",
      "Epoch 9894/30000 Training Loss: 0.054535333067178726\n",
      "Epoch 9895/30000 Training Loss: 0.06749804317951202\n",
      "Epoch 9896/30000 Training Loss: 0.07367353141307831\n",
      "Epoch 9897/30000 Training Loss: 0.06551849097013474\n",
      "Epoch 9898/30000 Training Loss: 0.06278757750988007\n",
      "Epoch 9899/30000 Training Loss: 0.07488687336444855\n",
      "Epoch 9900/30000 Training Loss: 0.058291010558605194\n",
      "Epoch 9900/30000 Validation Loss: 0.05294475331902504\n",
      "Epoch 9901/30000 Training Loss: 0.04248825088143349\n",
      "Epoch 9902/30000 Training Loss: 0.05195115879178047\n",
      "Epoch 9903/30000 Training Loss: 0.05556099861860275\n",
      "Epoch 9904/30000 Training Loss: 0.049022749066352844\n",
      "Epoch 9905/30000 Training Loss: 0.04699436575174332\n",
      "Epoch 9906/30000 Training Loss: 0.05870155990123749\n",
      "Epoch 9907/30000 Training Loss: 0.06427063047885895\n",
      "Epoch 9908/30000 Training Loss: 0.06847447156906128\n",
      "Epoch 9909/30000 Training Loss: 0.047068506479263306\n",
      "Epoch 9910/30000 Training Loss: 0.03489489480853081\n",
      "Epoch 9911/30000 Training Loss: 0.04694338142871857\n",
      "Epoch 9912/30000 Training Loss: 0.05868733301758766\n",
      "Epoch 9913/30000 Training Loss: 0.05785252898931503\n",
      "Epoch 9914/30000 Training Loss: 0.03558085113763809\n",
      "Epoch 9915/30000 Training Loss: 0.05616617202758789\n",
      "Epoch 9916/30000 Training Loss: 0.05815333127975464\n",
      "Epoch 9917/30000 Training Loss: 0.05062638223171234\n",
      "Epoch 9918/30000 Training Loss: 0.055131591856479645\n",
      "Epoch 9919/30000 Training Loss: 0.04808513820171356\n",
      "Epoch 9920/30000 Training Loss: 0.051761068403720856\n",
      "Epoch 9921/30000 Training Loss: 0.0452805757522583\n",
      "Epoch 9922/30000 Training Loss: 0.06965147703886032\n",
      "Epoch 9923/30000 Training Loss: 0.07370568811893463\n",
      "Epoch 9924/30000 Training Loss: 0.07613715529441833\n",
      "Epoch 9925/30000 Training Loss: 0.049708642065525055\n",
      "Epoch 9926/30000 Training Loss: 0.055782485753297806\n",
      "Epoch 9927/30000 Training Loss: 0.04190688580274582\n",
      "Epoch 9928/30000 Training Loss: 0.04521886259317398\n",
      "Epoch 9929/30000 Training Loss: 0.04460254684090614\n",
      "Epoch 9930/30000 Training Loss: 0.049676425755023956\n",
      "Epoch 9931/30000 Training Loss: 0.05628475174307823\n",
      "Epoch 9932/30000 Training Loss: 0.0666961520910263\n",
      "Epoch 9933/30000 Training Loss: 0.04026515409350395\n",
      "Epoch 9934/30000 Training Loss: 0.05034458637237549\n",
      "Epoch 9935/30000 Training Loss: 0.06711345911026001\n",
      "Epoch 9936/30000 Training Loss: 0.05525512248277664\n",
      "Epoch 9937/30000 Training Loss: 0.060812294483184814\n",
      "Epoch 9938/30000 Training Loss: 0.05910153687000275\n",
      "Epoch 9939/30000 Training Loss: 0.04414301738142967\n",
      "Epoch 9940/30000 Training Loss: 0.05380742624402046\n",
      "Epoch 9941/30000 Training Loss: 0.05508766695857048\n",
      "Epoch 9942/30000 Training Loss: 0.05459823086857796\n",
      "Epoch 9943/30000 Training Loss: 0.04214804619550705\n",
      "Epoch 9944/30000 Training Loss: 0.055856771767139435\n",
      "Epoch 9945/30000 Training Loss: 0.0571432039141655\n",
      "Epoch 9946/30000 Training Loss: 0.05087367817759514\n",
      "Epoch 9947/30000 Training Loss: 0.07844852656126022\n",
      "Epoch 9948/30000 Training Loss: 0.04598265886306763\n",
      "Epoch 9949/30000 Training Loss: 0.04305483400821686\n",
      "Epoch 9950/30000 Training Loss: 0.055709727108478546\n",
      "Epoch 9951/30000 Training Loss: 0.04550953954458237\n",
      "Epoch 9952/30000 Training Loss: 0.043200697749853134\n",
      "Epoch 9953/30000 Training Loss: 0.06569148600101471\n",
      "Epoch 9954/30000 Training Loss: 0.057205356657505035\n",
      "Epoch 9955/30000 Training Loss: 0.04599414020776749\n",
      "Epoch 9956/30000 Training Loss: 0.0615784153342247\n",
      "Epoch 9957/30000 Training Loss: 0.054883554577827454\n",
      "Epoch 9958/30000 Training Loss: 0.033364035189151764\n",
      "Epoch 9959/30000 Training Loss: 0.05645773187279701\n",
      "Epoch 9960/30000 Training Loss: 0.04608779773116112\n",
      "Epoch 9961/30000 Training Loss: 0.05583424121141434\n",
      "Epoch 9962/30000 Training Loss: 0.05653965845704079\n",
      "Epoch 9963/30000 Training Loss: 0.042921364307403564\n",
      "Epoch 9964/30000 Training Loss: 0.05242159217596054\n",
      "Epoch 9965/30000 Training Loss: 0.055280447006225586\n",
      "Epoch 9966/30000 Training Loss: 0.06818608194589615\n",
      "Epoch 9967/30000 Training Loss: 0.048696331679821014\n",
      "Epoch 9968/30000 Training Loss: 0.04632631316781044\n",
      "Epoch 9969/30000 Training Loss: 0.05408044904470444\n",
      "Epoch 9970/30000 Training Loss: 0.040263134986162186\n",
      "Epoch 9971/30000 Training Loss: 0.047251950949430466\n",
      "Epoch 9972/30000 Training Loss: 0.06318094581365585\n",
      "Epoch 9973/30000 Training Loss: 0.06058480963110924\n",
      "Epoch 9974/30000 Training Loss: 0.0696508064866066\n",
      "Epoch 9975/30000 Training Loss: 0.057444989681243896\n",
      "Epoch 9976/30000 Training Loss: 0.04841342568397522\n",
      "Epoch 9977/30000 Training Loss: 0.03847247734665871\n",
      "Epoch 9978/30000 Training Loss: 0.047740548849105835\n",
      "Epoch 9979/30000 Training Loss: 0.04639556258916855\n",
      "Epoch 9980/30000 Training Loss: 0.05901031196117401\n",
      "Epoch 9981/30000 Training Loss: 0.06030353903770447\n",
      "Epoch 9982/30000 Training Loss: 0.04787842556834221\n",
      "Epoch 9983/30000 Training Loss: 0.04457370936870575\n",
      "Epoch 9984/30000 Training Loss: 0.046812087297439575\n",
      "Epoch 9985/30000 Training Loss: 0.042937710881233215\n",
      "Epoch 9986/30000 Training Loss: 0.06191021576523781\n",
      "Epoch 9987/30000 Training Loss: 0.06367939710617065\n",
      "Epoch 9988/30000 Training Loss: 0.05925098806619644\n",
      "Epoch 9989/30000 Training Loss: 0.05014817416667938\n",
      "Epoch 9990/30000 Training Loss: 0.06090002506971359\n",
      "Epoch 9991/30000 Training Loss: 0.05696363002061844\n",
      "Epoch 9992/30000 Training Loss: 0.03540751710534096\n",
      "Epoch 9993/30000 Training Loss: 0.06729649007320404\n",
      "Epoch 9994/30000 Training Loss: 0.05558231472969055\n",
      "Epoch 9995/30000 Training Loss: 0.044911447912454605\n",
      "Epoch 9996/30000 Training Loss: 0.044547438621520996\n",
      "Epoch 9997/30000 Training Loss: 0.05421185493469238\n",
      "Epoch 9998/30000 Training Loss: 0.04955455660820007\n",
      "Epoch 9999/30000 Training Loss: 0.053234443068504333\n",
      "Epoch 10000/30000 Training Loss: 0.06036912649869919\n",
      "Epoch 10000/30000 Validation Loss: 0.05137405917048454\n",
      "Epoch 10001/30000 Training Loss: 0.05862785130739212\n",
      "Epoch 10002/30000 Training Loss: 0.046200186014175415\n",
      "Epoch 10003/30000 Training Loss: 0.04749185964465141\n",
      "Epoch 10004/30000 Training Loss: 0.07078547030687332\n",
      "Epoch 10005/30000 Training Loss: 0.04350471496582031\n",
      "Epoch 10006/30000 Training Loss: 0.045878201723098755\n",
      "Epoch 10007/30000 Training Loss: 0.0514669269323349\n",
      "Epoch 10008/30000 Training Loss: 0.06655565649271011\n",
      "Epoch 10009/30000 Training Loss: 0.06503455340862274\n",
      "Epoch 10010/30000 Training Loss: 0.07001391798257828\n",
      "Epoch 10011/30000 Training Loss: 0.05687674134969711\n",
      "Epoch 10012/30000 Training Loss: 0.05975368246436119\n",
      "Epoch 10013/30000 Training Loss: 0.050318144261837006\n",
      "Epoch 10014/30000 Training Loss: 0.05598212406039238\n",
      "Epoch 10015/30000 Training Loss: 0.05375425145030022\n",
      "Epoch 10016/30000 Training Loss: 0.047762274742126465\n",
      "Epoch 10017/30000 Training Loss: 0.04732123762369156\n",
      "Epoch 10018/30000 Training Loss: 0.0682951807975769\n",
      "Epoch 10019/30000 Training Loss: 0.05100202560424805\n",
      "Epoch 10020/30000 Training Loss: 0.05177721008658409\n",
      "Epoch 10021/30000 Training Loss: 0.04500475525856018\n",
      "Epoch 10022/30000 Training Loss: 0.04516814649105072\n",
      "Epoch 10023/30000 Training Loss: 0.04679517447948456\n",
      "Epoch 10024/30000 Training Loss: 0.04734174907207489\n",
      "Epoch 10025/30000 Training Loss: 0.06422807276248932\n",
      "Epoch 10026/30000 Training Loss: 0.05146735906600952\n",
      "Epoch 10027/30000 Training Loss: 0.04554439336061478\n",
      "Epoch 10028/30000 Training Loss: 0.05146848410367966\n",
      "Epoch 10029/30000 Training Loss: 0.04543733596801758\n",
      "Epoch 10030/30000 Training Loss: 0.0542776882648468\n",
      "Epoch 10031/30000 Training Loss: 0.047790102660655975\n",
      "Epoch 10032/30000 Training Loss: 0.06730316579341888\n",
      "Epoch 10033/30000 Training Loss: 0.05944056808948517\n",
      "Epoch 10034/30000 Training Loss: 0.05202355235815048\n",
      "Epoch 10035/30000 Training Loss: 0.06168091669678688\n",
      "Epoch 10036/30000 Training Loss: 0.06505200266838074\n",
      "Epoch 10037/30000 Training Loss: 0.06985411792993546\n",
      "Epoch 10038/30000 Training Loss: 0.049616605043411255\n",
      "Epoch 10039/30000 Training Loss: 0.035358138382434845\n",
      "Epoch 10040/30000 Training Loss: 0.041780684143304825\n",
      "Epoch 10041/30000 Training Loss: 0.05532008782029152\n",
      "Epoch 10042/30000 Training Loss: 0.05669576674699783\n",
      "Epoch 10043/30000 Training Loss: 0.04418565705418587\n",
      "Epoch 10044/30000 Training Loss: 0.04669225960969925\n",
      "Epoch 10045/30000 Training Loss: 0.0475134514272213\n",
      "Epoch 10046/30000 Training Loss: 0.058146387338638306\n",
      "Epoch 10047/30000 Training Loss: 0.05051138997077942\n",
      "Epoch 10048/30000 Training Loss: 0.05489162355661392\n",
      "Epoch 10049/30000 Training Loss: 0.05203339084982872\n",
      "Epoch 10050/30000 Training Loss: 0.03716759756207466\n",
      "Epoch 10051/30000 Training Loss: 0.05650606378912926\n",
      "Epoch 10052/30000 Training Loss: 0.06146677955985069\n",
      "Epoch 10053/30000 Training Loss: 0.05360570549964905\n",
      "Epoch 10054/30000 Training Loss: 0.0653277263045311\n",
      "Epoch 10055/30000 Training Loss: 0.04507877677679062\n",
      "Epoch 10056/30000 Training Loss: 0.039550602436065674\n",
      "Epoch 10057/30000 Training Loss: 0.05781637132167816\n",
      "Epoch 10058/30000 Training Loss: 0.05699097365140915\n",
      "Epoch 10059/30000 Training Loss: 0.0446842685341835\n",
      "Epoch 10060/30000 Training Loss: 0.04180319234728813\n",
      "Epoch 10061/30000 Training Loss: 0.05387890711426735\n",
      "Epoch 10062/30000 Training Loss: 0.05783677101135254\n",
      "Epoch 10063/30000 Training Loss: 0.05927341431379318\n",
      "Epoch 10064/30000 Training Loss: 0.05745396018028259\n",
      "Epoch 10065/30000 Training Loss: 0.05233331769704819\n",
      "Epoch 10066/30000 Training Loss: 0.054218459874391556\n",
      "Epoch 10067/30000 Training Loss: 0.04380221664905548\n",
      "Epoch 10068/30000 Training Loss: 0.04116683825850487\n",
      "Epoch 10069/30000 Training Loss: 0.05517992377281189\n",
      "Epoch 10070/30000 Training Loss: 0.06800791621208191\n",
      "Epoch 10071/30000 Training Loss: 0.055055923759937286\n",
      "Epoch 10072/30000 Training Loss: 0.06234476715326309\n",
      "Epoch 10073/30000 Training Loss: 0.04458015039563179\n",
      "Epoch 10074/30000 Training Loss: 0.04501386731863022\n",
      "Epoch 10075/30000 Training Loss: 0.05726303905248642\n",
      "Epoch 10076/30000 Training Loss: 0.05832260102033615\n",
      "Epoch 10077/30000 Training Loss: 0.06806815415620804\n",
      "Epoch 10078/30000 Training Loss: 0.04843289777636528\n",
      "Epoch 10079/30000 Training Loss: 0.048554595559835434\n",
      "Epoch 10080/30000 Training Loss: 0.0388149619102478\n",
      "Epoch 10081/30000 Training Loss: 0.05631139501929283\n",
      "Epoch 10082/30000 Training Loss: 0.05148103088140488\n",
      "Epoch 10083/30000 Training Loss: 0.04704273119568825\n",
      "Epoch 10084/30000 Training Loss: 0.07235709577798843\n",
      "Epoch 10085/30000 Training Loss: 0.0654553771018982\n",
      "Epoch 10086/30000 Training Loss: 0.0471271313726902\n",
      "Epoch 10087/30000 Training Loss: 0.05123640224337578\n",
      "Epoch 10088/30000 Training Loss: 0.0524568036198616\n",
      "Epoch 10089/30000 Training Loss: 0.05483461916446686\n",
      "Epoch 10090/30000 Training Loss: 0.040423434227705\n",
      "Epoch 10091/30000 Training Loss: 0.0554969385266304\n",
      "Epoch 10092/30000 Training Loss: 0.05394541472196579\n",
      "Epoch 10093/30000 Training Loss: 0.04247244447469711\n",
      "Epoch 10094/30000 Training Loss: 0.043545302003622055\n",
      "Epoch 10095/30000 Training Loss: 0.051803845912218094\n",
      "Epoch 10096/30000 Training Loss: 0.054170578718185425\n",
      "Epoch 10097/30000 Training Loss: 0.05489693954586983\n",
      "Epoch 10098/30000 Training Loss: 0.06729551404714584\n",
      "Epoch 10099/30000 Training Loss: 0.05420887470245361\n",
      "Epoch 10100/30000 Training Loss: 0.06749887019395828\n",
      "Epoch 10100/30000 Validation Loss: 0.0619627982378006\n",
      "Epoch 10101/30000 Training Loss: 0.05081792548298836\n",
      "Epoch 10102/30000 Training Loss: 0.0554041713476181\n",
      "Epoch 10103/30000 Training Loss: 0.06447891891002655\n",
      "Epoch 10104/30000 Training Loss: 0.06327391415834427\n",
      "Epoch 10105/30000 Training Loss: 0.042730025947093964\n",
      "Epoch 10106/30000 Training Loss: 0.04831094294786453\n",
      "Epoch 10107/30000 Training Loss: 0.04406149685382843\n",
      "Epoch 10108/30000 Training Loss: 0.05887250602245331\n",
      "Epoch 10109/30000 Training Loss: 0.05020495504140854\n",
      "Epoch 10110/30000 Training Loss: 0.05468977987766266\n",
      "Epoch 10111/30000 Training Loss: 0.037011198699474335\n",
      "Epoch 10112/30000 Training Loss: 0.05402588099241257\n",
      "Epoch 10113/30000 Training Loss: 0.045265696942806244\n",
      "Epoch 10114/30000 Training Loss: 0.03584809973835945\n",
      "Epoch 10115/30000 Training Loss: 0.052676741033792496\n",
      "Epoch 10116/30000 Training Loss: 0.0674138218164444\n",
      "Epoch 10117/30000 Training Loss: 0.05753914639353752\n",
      "Epoch 10118/30000 Training Loss: 0.06771241873502731\n",
      "Epoch 10119/30000 Training Loss: 0.05402696877717972\n",
      "Epoch 10120/30000 Training Loss: 0.05461420118808746\n",
      "Epoch 10121/30000 Training Loss: 0.07262501120567322\n",
      "Epoch 10122/30000 Training Loss: 0.05993201956152916\n",
      "Epoch 10123/30000 Training Loss: 0.06031050533056259\n",
      "Epoch 10124/30000 Training Loss: 0.06845380365848541\n",
      "Epoch 10125/30000 Training Loss: 0.05327470973134041\n",
      "Epoch 10126/30000 Training Loss: 0.04087113216519356\n",
      "Epoch 10127/30000 Training Loss: 0.0553039014339447\n",
      "Epoch 10128/30000 Training Loss: 0.043140098452568054\n",
      "Epoch 10129/30000 Training Loss: 0.07087523490190506\n",
      "Epoch 10130/30000 Training Loss: 0.0556960329413414\n",
      "Epoch 10131/30000 Training Loss: 0.05309893190860748\n",
      "Epoch 10132/30000 Training Loss: 0.04666423052549362\n",
      "Epoch 10133/30000 Training Loss: 0.05244018882513046\n",
      "Epoch 10134/30000 Training Loss: 0.043812334537506104\n",
      "Epoch 10135/30000 Training Loss: 0.05821423977613449\n",
      "Epoch 10136/30000 Training Loss: 0.04916597157716751\n",
      "Epoch 10137/30000 Training Loss: 0.05841385945677757\n",
      "Epoch 10138/30000 Training Loss: 0.03821442276239395\n",
      "Epoch 10139/30000 Training Loss: 0.038213878870010376\n",
      "Epoch 10140/30000 Training Loss: 0.06067805364727974\n",
      "Epoch 10141/30000 Training Loss: 0.05253840982913971\n",
      "Epoch 10142/30000 Training Loss: 0.05928937345743179\n",
      "Epoch 10143/30000 Training Loss: 0.03916409611701965\n",
      "Epoch 10144/30000 Training Loss: 0.0470331646502018\n",
      "Epoch 10145/30000 Training Loss: 0.05129174143075943\n",
      "Epoch 10146/30000 Training Loss: 0.05125926807522774\n",
      "Epoch 10147/30000 Training Loss: 0.05308800935745239\n",
      "Epoch 10148/30000 Training Loss: 0.061993878334760666\n",
      "Epoch 10149/30000 Training Loss: 0.042873140424489975\n",
      "Epoch 10150/30000 Training Loss: 0.03643226996064186\n",
      "Epoch 10151/30000 Training Loss: 0.06061257794499397\n",
      "Epoch 10152/30000 Training Loss: 0.0697355791926384\n",
      "Epoch 10153/30000 Training Loss: 0.050019942224025726\n",
      "Epoch 10154/30000 Training Loss: 0.048531848937273026\n",
      "Epoch 10155/30000 Training Loss: 0.046670764684677124\n",
      "Epoch 10156/30000 Training Loss: 0.04702548682689667\n",
      "Epoch 10157/30000 Training Loss: 0.046859946101903915\n",
      "Epoch 10158/30000 Training Loss: 0.06249803304672241\n",
      "Epoch 10159/30000 Training Loss: 0.05060310661792755\n",
      "Epoch 10160/30000 Training Loss: 0.048628054559230804\n",
      "Epoch 10161/30000 Training Loss: 0.046798430383205414\n",
      "Epoch 10162/30000 Training Loss: 0.05630505457520485\n",
      "Epoch 10163/30000 Training Loss: 0.05307503789663315\n",
      "Epoch 10164/30000 Training Loss: 0.04445001482963562\n",
      "Epoch 10165/30000 Training Loss: 0.03700438514351845\n",
      "Epoch 10166/30000 Training Loss: 0.05994018167257309\n",
      "Epoch 10167/30000 Training Loss: 0.042473237961530685\n",
      "Epoch 10168/30000 Training Loss: 0.05528336018323898\n",
      "Epoch 10169/30000 Training Loss: 0.05417858809232712\n",
      "Epoch 10170/30000 Training Loss: 0.05477707087993622\n",
      "Epoch 10171/30000 Training Loss: 0.04346810281276703\n",
      "Epoch 10172/30000 Training Loss: 0.049148011952638626\n",
      "Epoch 10173/30000 Training Loss: 0.043594542890787125\n",
      "Epoch 10174/30000 Training Loss: 0.06275029480457306\n",
      "Epoch 10175/30000 Training Loss: 0.05340471491217613\n",
      "Epoch 10176/30000 Training Loss: 0.05057596415281296\n",
      "Epoch 10177/30000 Training Loss: 0.06229320913553238\n",
      "Epoch 10178/30000 Training Loss: 0.06324538588523865\n",
      "Epoch 10179/30000 Training Loss: 0.045047514140605927\n",
      "Epoch 10180/30000 Training Loss: 0.05168851092457771\n",
      "Epoch 10181/30000 Training Loss: 0.049554888159036636\n",
      "Epoch 10182/30000 Training Loss: 0.06465620547533035\n",
      "Epoch 10183/30000 Training Loss: 0.04623504355549812\n",
      "Epoch 10184/30000 Training Loss: 0.036430176347494125\n",
      "Epoch 10185/30000 Training Loss: 0.042070094496011734\n",
      "Epoch 10186/30000 Training Loss: 0.06799573451280594\n",
      "Epoch 10187/30000 Training Loss: 0.05307125672698021\n",
      "Epoch 10188/30000 Training Loss: 0.06358667463064194\n",
      "Epoch 10189/30000 Training Loss: 0.05638814717531204\n",
      "Epoch 10190/30000 Training Loss: 0.04713166877627373\n",
      "Epoch 10191/30000 Training Loss: 0.06487329304218292\n",
      "Epoch 10192/30000 Training Loss: 0.05323009938001633\n",
      "Epoch 10193/30000 Training Loss: 0.04473691061139107\n",
      "Epoch 10194/30000 Training Loss: 0.05758076533675194\n",
      "Epoch 10195/30000 Training Loss: 0.05373614281415939\n",
      "Epoch 10196/30000 Training Loss: 0.043469320982694626\n",
      "Epoch 10197/30000 Training Loss: 0.04847900569438934\n",
      "Epoch 10198/30000 Training Loss: 0.06358013302087784\n",
      "Epoch 10199/30000 Training Loss: 0.05622928589582443\n",
      "Epoch 10200/30000 Training Loss: 0.04505655914545059\n",
      "Epoch 10200/30000 Validation Loss: 0.05655043199658394\n",
      "Epoch 10201/30000 Training Loss: 0.05092798173427582\n",
      "Epoch 10202/30000 Training Loss: 0.05896146968007088\n",
      "Epoch 10203/30000 Training Loss: 0.043383266776800156\n",
      "Epoch 10204/30000 Training Loss: 0.06534373015165329\n",
      "Epoch 10205/30000 Training Loss: 0.04037442058324814\n",
      "Epoch 10206/30000 Training Loss: 0.05283340439200401\n",
      "Epoch 10207/30000 Training Loss: 0.059368520975112915\n",
      "Epoch 10208/30000 Training Loss: 0.04712563753128052\n",
      "Epoch 10209/30000 Training Loss: 0.05373314395546913\n",
      "Epoch 10210/30000 Training Loss: 0.04920876771211624\n",
      "Epoch 10211/30000 Training Loss: 0.055384594947099686\n",
      "Epoch 10212/30000 Training Loss: 0.05515797436237335\n",
      "Epoch 10213/30000 Training Loss: 0.05515719950199127\n",
      "Epoch 10214/30000 Training Loss: 0.051378682255744934\n",
      "Epoch 10215/30000 Training Loss: 0.05207996070384979\n",
      "Epoch 10216/30000 Training Loss: 0.046168018132448196\n",
      "Epoch 10217/30000 Training Loss: 0.0779251903295517\n",
      "Epoch 10218/30000 Training Loss: 0.04726669564843178\n",
      "Epoch 10219/30000 Training Loss: 0.06414990872144699\n",
      "Epoch 10220/30000 Training Loss: 0.054384492337703705\n",
      "Epoch 10221/30000 Training Loss: 0.05145501345396042\n",
      "Epoch 10222/30000 Training Loss: 0.0400589182972908\n",
      "Epoch 10223/30000 Training Loss: 0.051272325217723846\n",
      "Epoch 10224/30000 Training Loss: 0.048571258783340454\n",
      "Epoch 10225/30000 Training Loss: 0.05908840149641037\n",
      "Epoch 10226/30000 Training Loss: 0.044637493789196014\n",
      "Epoch 10227/30000 Training Loss: 0.04319201037287712\n",
      "Epoch 10228/30000 Training Loss: 0.05753551051020622\n",
      "Epoch 10229/30000 Training Loss: 0.058889128267765045\n",
      "Epoch 10230/30000 Training Loss: 0.05973702669143677\n",
      "Epoch 10231/30000 Training Loss: 0.04599694162607193\n",
      "Epoch 10232/30000 Training Loss: 0.05838034301996231\n",
      "Epoch 10233/30000 Training Loss: 0.039922185242176056\n",
      "Epoch 10234/30000 Training Loss: 0.051832444965839386\n",
      "Epoch 10235/30000 Training Loss: 0.04359721764922142\n",
      "Epoch 10236/30000 Training Loss: 0.04862704128026962\n",
      "Epoch 10237/30000 Training Loss: 0.05998651683330536\n",
      "Epoch 10238/30000 Training Loss: 0.0588289350271225\n",
      "Epoch 10239/30000 Training Loss: 0.05759543180465698\n",
      "Epoch 10240/30000 Training Loss: 0.0425918772816658\n",
      "Epoch 10241/30000 Training Loss: 0.049850404262542725\n",
      "Epoch 10242/30000 Training Loss: 0.06377828121185303\n",
      "Epoch 10243/30000 Training Loss: 0.05620701238512993\n",
      "Epoch 10244/30000 Training Loss: 0.0576779842376709\n",
      "Epoch 10245/30000 Training Loss: 0.06878230720758438\n",
      "Epoch 10246/30000 Training Loss: 0.039691012352705\n",
      "Epoch 10247/30000 Training Loss: 0.06561042368412018\n",
      "Epoch 10248/30000 Training Loss: 0.04606468230485916\n",
      "Epoch 10249/30000 Training Loss: 0.05845720320940018\n",
      "Epoch 10250/30000 Training Loss: 0.061067141592502594\n",
      "Epoch 10251/30000 Training Loss: 0.054518260061740875\n",
      "Epoch 10252/30000 Training Loss: 0.06574738770723343\n",
      "Epoch 10253/30000 Training Loss: 0.06468231230974197\n",
      "Epoch 10254/30000 Training Loss: 0.05410334840416908\n",
      "Epoch 10255/30000 Training Loss: 0.047632694244384766\n",
      "Epoch 10256/30000 Training Loss: 0.055486660450696945\n",
      "Epoch 10257/30000 Training Loss: 0.03804251551628113\n",
      "Epoch 10258/30000 Training Loss: 0.05602410063147545\n",
      "Epoch 10259/30000 Training Loss: 0.06277546286582947\n",
      "Epoch 10260/30000 Training Loss: 0.053650256246328354\n",
      "Epoch 10261/30000 Training Loss: 0.05883125588297844\n",
      "Epoch 10262/30000 Training Loss: 0.04929538071155548\n",
      "Epoch 10263/30000 Training Loss: 0.04349502921104431\n",
      "Epoch 10264/30000 Training Loss: 0.06449428200721741\n",
      "Epoch 10265/30000 Training Loss: 0.043595090508461\n",
      "Epoch 10266/30000 Training Loss: 0.05103634297847748\n",
      "Epoch 10267/30000 Training Loss: 0.052303247153759\n",
      "Epoch 10268/30000 Training Loss: 0.06633435934782028\n",
      "Epoch 10269/30000 Training Loss: 0.06016220897436142\n",
      "Epoch 10270/30000 Training Loss: 0.044609278440475464\n",
      "Epoch 10271/30000 Training Loss: 0.05329234153032303\n",
      "Epoch 10272/30000 Training Loss: 0.06627366691827774\n",
      "Epoch 10273/30000 Training Loss: 0.07275761663913727\n",
      "Epoch 10274/30000 Training Loss: 0.06417036056518555\n",
      "Epoch 10275/30000 Training Loss: 0.06126173958182335\n",
      "Epoch 10276/30000 Training Loss: 0.05064525455236435\n",
      "Epoch 10277/30000 Training Loss: 0.0446837842464447\n",
      "Epoch 10278/30000 Training Loss: 0.06597088277339935\n",
      "Epoch 10279/30000 Training Loss: 0.06670010089874268\n",
      "Epoch 10280/30000 Training Loss: 0.04674006626009941\n",
      "Epoch 10281/30000 Training Loss: 0.058923788368701935\n",
      "Epoch 10282/30000 Training Loss: 0.06112833321094513\n",
      "Epoch 10283/30000 Training Loss: 0.05539738014340401\n",
      "Epoch 10284/30000 Training Loss: 0.052331916987895966\n",
      "Epoch 10285/30000 Training Loss: 0.06171869486570358\n",
      "Epoch 10286/30000 Training Loss: 0.04672433063387871\n",
      "Epoch 10287/30000 Training Loss: 0.03805675357580185\n",
      "Epoch 10288/30000 Training Loss: 0.0634634718298912\n",
      "Epoch 10289/30000 Training Loss: 0.04107676073908806\n",
      "Epoch 10290/30000 Training Loss: 0.0601671040058136\n",
      "Epoch 10291/30000 Training Loss: 0.05458836629986763\n",
      "Epoch 10292/30000 Training Loss: 0.06538768112659454\n",
      "Epoch 10293/30000 Training Loss: 0.06806337088346481\n",
      "Epoch 10294/30000 Training Loss: 0.05500222370028496\n",
      "Epoch 10295/30000 Training Loss: 0.05515170469880104\n",
      "Epoch 10296/30000 Training Loss: 0.07170639932155609\n",
      "Epoch 10297/30000 Training Loss: 0.05977747589349747\n",
      "Epoch 10298/30000 Training Loss: 0.061828963458538055\n",
      "Epoch 10299/30000 Training Loss: 0.04026428982615471\n",
      "Epoch 10300/30000 Training Loss: 0.04365389049053192\n",
      "Epoch 10300/30000 Validation Loss: 0.05203118175268173\n",
      "Epoch 10301/30000 Training Loss: 0.05533229559659958\n",
      "Epoch 10302/30000 Training Loss: 0.04626809060573578\n",
      "Epoch 10303/30000 Training Loss: 0.054999127984046936\n",
      "Epoch 10304/30000 Training Loss: 0.05598635971546173\n",
      "Epoch 10305/30000 Training Loss: 0.0513739138841629\n",
      "Epoch 10306/30000 Training Loss: 0.062458280473947525\n",
      "Epoch 10307/30000 Training Loss: 0.05117342621088028\n",
      "Epoch 10308/30000 Training Loss: 0.04117365926504135\n",
      "Epoch 10309/30000 Training Loss: 0.03779694437980652\n",
      "Epoch 10310/30000 Training Loss: 0.062286119908094406\n",
      "Epoch 10311/30000 Training Loss: 0.04248061403632164\n",
      "Epoch 10312/30000 Training Loss: 0.049746960401535034\n",
      "Epoch 10313/30000 Training Loss: 0.05015850439667702\n",
      "Epoch 10314/30000 Training Loss: 0.048016566783189774\n",
      "Epoch 10315/30000 Training Loss: 0.06543463468551636\n",
      "Epoch 10316/30000 Training Loss: 0.05715262144804001\n",
      "Epoch 10317/30000 Training Loss: 0.06215018779039383\n",
      "Epoch 10318/30000 Training Loss: 0.04942972585558891\n",
      "Epoch 10319/30000 Training Loss: 0.07415612041950226\n",
      "Epoch 10320/30000 Training Loss: 0.05130881816148758\n",
      "Epoch 10321/30000 Training Loss: 0.056267380714416504\n",
      "Epoch 10322/30000 Training Loss: 0.04823128134012222\n",
      "Epoch 10323/30000 Training Loss: 0.05114724114537239\n",
      "Epoch 10324/30000 Training Loss: 0.05486641079187393\n",
      "Epoch 10325/30000 Training Loss: 0.03681645542383194\n",
      "Epoch 10326/30000 Training Loss: 0.04230430722236633\n",
      "Epoch 10327/30000 Training Loss: 0.06958012282848358\n",
      "Epoch 10328/30000 Training Loss: 0.05575823038816452\n",
      "Epoch 10329/30000 Training Loss: 0.0482204407453537\n",
      "Epoch 10330/30000 Training Loss: 0.07037675380706787\n",
      "Epoch 10331/30000 Training Loss: 0.05185026675462723\n",
      "Epoch 10332/30000 Training Loss: 0.06337077170610428\n",
      "Epoch 10333/30000 Training Loss: 0.05754927173256874\n",
      "Epoch 10334/30000 Training Loss: 0.05009683594107628\n",
      "Epoch 10335/30000 Training Loss: 0.05384235084056854\n",
      "Epoch 10336/30000 Training Loss: 0.04330487549304962\n",
      "Epoch 10337/30000 Training Loss: 0.0711033046245575\n",
      "Epoch 10338/30000 Training Loss: 0.0639551430940628\n",
      "Epoch 10339/30000 Training Loss: 0.04695695638656616\n",
      "Epoch 10340/30000 Training Loss: 0.06671930849552155\n",
      "Epoch 10341/30000 Training Loss: 0.054242346435785294\n",
      "Epoch 10342/30000 Training Loss: 0.05466355383396149\n",
      "Epoch 10343/30000 Training Loss: 0.05247599259018898\n",
      "Epoch 10344/30000 Training Loss: 0.04617895931005478\n",
      "Epoch 10345/30000 Training Loss: 0.056962817907333374\n",
      "Epoch 10346/30000 Training Loss: 0.04034176096320152\n",
      "Epoch 10347/30000 Training Loss: 0.06350525468587875\n",
      "Epoch 10348/30000 Training Loss: 0.059783440083265305\n",
      "Epoch 10349/30000 Training Loss: 0.0630912184715271\n",
      "Epoch 10350/30000 Training Loss: 0.03733702376484871\n",
      "Epoch 10351/30000 Training Loss: 0.059374839067459106\n",
      "Epoch 10352/30000 Training Loss: 0.039131782948970795\n",
      "Epoch 10353/30000 Training Loss: 0.045575033873319626\n",
      "Epoch 10354/30000 Training Loss: 0.0437021441757679\n",
      "Epoch 10355/30000 Training Loss: 0.0515376515686512\n",
      "Epoch 10356/30000 Training Loss: 0.05060776695609093\n",
      "Epoch 10357/30000 Training Loss: 0.03871661424636841\n",
      "Epoch 10358/30000 Training Loss: 0.048305489122867584\n",
      "Epoch 10359/30000 Training Loss: 0.045484770089387894\n",
      "Epoch 10360/30000 Training Loss: 0.04174289107322693\n",
      "Epoch 10361/30000 Training Loss: 0.03700406104326248\n",
      "Epoch 10362/30000 Training Loss: 0.0674484521150589\n",
      "Epoch 10363/30000 Training Loss: 0.04808232560753822\n",
      "Epoch 10364/30000 Training Loss: 0.03593835234642029\n",
      "Epoch 10365/30000 Training Loss: 0.07149925082921982\n",
      "Epoch 10366/30000 Training Loss: 0.05477168783545494\n",
      "Epoch 10367/30000 Training Loss: 0.058287207037210464\n",
      "Epoch 10368/30000 Training Loss: 0.05375680327415466\n",
      "Epoch 10369/30000 Training Loss: 0.055313169956207275\n",
      "Epoch 10370/30000 Training Loss: 0.04969904571771622\n",
      "Epoch 10371/30000 Training Loss: 0.05068042874336243\n",
      "Epoch 10372/30000 Training Loss: 0.05450177565217018\n",
      "Epoch 10373/30000 Training Loss: 0.050561532378196716\n",
      "Epoch 10374/30000 Training Loss: 0.05241840332746506\n",
      "Epoch 10375/30000 Training Loss: 0.05583778768777847\n",
      "Epoch 10376/30000 Training Loss: 0.05920112133026123\n",
      "Epoch 10377/30000 Training Loss: 0.05195469409227371\n",
      "Epoch 10378/30000 Training Loss: 0.05013469606637955\n",
      "Epoch 10379/30000 Training Loss: 0.04198917746543884\n",
      "Epoch 10380/30000 Training Loss: 0.05286001041531563\n",
      "Epoch 10381/30000 Training Loss: 0.05263465642929077\n",
      "Epoch 10382/30000 Training Loss: 0.06994691491127014\n",
      "Epoch 10383/30000 Training Loss: 0.058690108358860016\n",
      "Epoch 10384/30000 Training Loss: 0.05540166050195694\n",
      "Epoch 10385/30000 Training Loss: 0.069545216858387\n",
      "Epoch 10386/30000 Training Loss: 0.05234058201313019\n",
      "Epoch 10387/30000 Training Loss: 0.05306962877511978\n",
      "Epoch 10388/30000 Training Loss: 0.052520155906677246\n",
      "Epoch 10389/30000 Training Loss: 0.06255175918340683\n",
      "Epoch 10390/30000 Training Loss: 0.044550132006406784\n",
      "Epoch 10391/30000 Training Loss: 0.042137857526540756\n",
      "Epoch 10392/30000 Training Loss: 0.03980715572834015\n",
      "Epoch 10393/30000 Training Loss: 0.05170054733753204\n",
      "Epoch 10394/30000 Training Loss: 0.0571814700961113\n",
      "Epoch 10395/30000 Training Loss: 0.05944657325744629\n",
      "Epoch 10396/30000 Training Loss: 0.056463759392499924\n",
      "Epoch 10397/30000 Training Loss: 0.046038009226322174\n",
      "Epoch 10398/30000 Training Loss: 0.06616081297397614\n",
      "Epoch 10399/30000 Training Loss: 0.04322732239961624\n",
      "Epoch 10400/30000 Training Loss: 0.0611654631793499\n",
      "Epoch 10400/30000 Validation Loss: 0.0394112765789032\n",
      "Epoch 10401/30000 Training Loss: 0.05379381403326988\n",
      "Epoch 10402/30000 Training Loss: 0.051923371851444244\n",
      "Epoch 10403/30000 Training Loss: 0.07632523775100708\n",
      "Epoch 10404/30000 Training Loss: 0.05148479342460632\n",
      "Epoch 10405/30000 Training Loss: 0.050961852073669434\n",
      "Epoch 10406/30000 Training Loss: 0.04993152618408203\n",
      "Epoch 10407/30000 Training Loss: 0.08342160284519196\n",
      "Epoch 10408/30000 Training Loss: 0.04269629344344139\n",
      "Epoch 10409/30000 Training Loss: 0.0447642020881176\n",
      "Epoch 10410/30000 Training Loss: 0.056143973022699356\n",
      "Epoch 10411/30000 Training Loss: 0.04465591162443161\n",
      "Epoch 10412/30000 Training Loss: 0.046551868319511414\n",
      "Epoch 10413/30000 Training Loss: 0.07161135971546173\n",
      "Epoch 10414/30000 Training Loss: 0.04715277999639511\n",
      "Epoch 10415/30000 Training Loss: 0.05390814691781998\n",
      "Epoch 10416/30000 Training Loss: 0.04068867862224579\n",
      "Epoch 10417/30000 Training Loss: 0.06611166894435883\n",
      "Epoch 10418/30000 Training Loss: 0.04775477573275566\n",
      "Epoch 10419/30000 Training Loss: 0.050822168588638306\n",
      "Epoch 10420/30000 Training Loss: 0.07696487754583359\n",
      "Epoch 10421/30000 Training Loss: 0.049531083554029465\n",
      "Epoch 10422/30000 Training Loss: 0.05164771527051926\n",
      "Epoch 10423/30000 Training Loss: 0.03383082151412964\n",
      "Epoch 10424/30000 Training Loss: 0.054446469992399216\n",
      "Epoch 10425/30000 Training Loss: 0.05316348746418953\n",
      "Epoch 10426/30000 Training Loss: 0.06071866303682327\n",
      "Epoch 10427/30000 Training Loss: 0.06098826602101326\n",
      "Epoch 10428/30000 Training Loss: 0.04739052802324295\n",
      "Epoch 10429/30000 Training Loss: 0.047984056174755096\n",
      "Epoch 10430/30000 Training Loss: 0.04954583942890167\n",
      "Epoch 10431/30000 Training Loss: 0.05406888201832771\n",
      "Epoch 10432/30000 Training Loss: 0.07138132303953171\n",
      "Epoch 10433/30000 Training Loss: 0.06787976622581482\n",
      "Epoch 10434/30000 Training Loss: 0.05817209184169769\n",
      "Epoch 10435/30000 Training Loss: 0.03122074343264103\n",
      "Epoch 10436/30000 Training Loss: 0.05479099228978157\n",
      "Epoch 10437/30000 Training Loss: 0.07030042260885239\n",
      "Epoch 10438/30000 Training Loss: 0.04146290570497513\n",
      "Epoch 10439/30000 Training Loss: 0.05216927081346512\n",
      "Epoch 10440/30000 Training Loss: 0.04924079775810242\n",
      "Epoch 10441/30000 Training Loss: 0.05164795741438866\n",
      "Epoch 10442/30000 Training Loss: 0.05446912348270416\n",
      "Epoch 10443/30000 Training Loss: 0.04477512463927269\n",
      "Epoch 10444/30000 Training Loss: 0.03987704589962959\n",
      "Epoch 10445/30000 Training Loss: 0.05018152296543121\n",
      "Epoch 10446/30000 Training Loss: 0.0610145665705204\n",
      "Epoch 10447/30000 Training Loss: 0.043939899653196335\n",
      "Epoch 10448/30000 Training Loss: 0.03593610227108002\n",
      "Epoch 10449/30000 Training Loss: 0.03969372436404228\n",
      "Epoch 10450/30000 Training Loss: 0.04641062766313553\n",
      "Epoch 10451/30000 Training Loss: 0.04870197921991348\n",
      "Epoch 10452/30000 Training Loss: 0.06462062895298004\n",
      "Epoch 10453/30000 Training Loss: 0.04684410244226456\n",
      "Epoch 10454/30000 Training Loss: 0.06314516067504883\n",
      "Epoch 10455/30000 Training Loss: 0.051199305802583694\n",
      "Epoch 10456/30000 Training Loss: 0.04376192018389702\n",
      "Epoch 10457/30000 Training Loss: 0.0655427873134613\n",
      "Epoch 10458/30000 Training Loss: 0.053613901138305664\n",
      "Epoch 10459/30000 Training Loss: 0.04706089198589325\n",
      "Epoch 10460/30000 Training Loss: 0.03887461870908737\n",
      "Epoch 10461/30000 Training Loss: 0.04396414756774902\n",
      "Epoch 10462/30000 Training Loss: 0.0457596555352211\n",
      "Epoch 10463/30000 Training Loss: 0.053341981023550034\n",
      "Epoch 10464/30000 Training Loss: 0.048257000744342804\n",
      "Epoch 10465/30000 Training Loss: 0.059431858360767365\n",
      "Epoch 10466/30000 Training Loss: 0.04787258803844452\n",
      "Epoch 10467/30000 Training Loss: 0.05561758205294609\n",
      "Epoch 10468/30000 Training Loss: 0.03494661673903465\n",
      "Epoch 10469/30000 Training Loss: 0.047421496361494064\n",
      "Epoch 10470/30000 Training Loss: 0.05360456556081772\n",
      "Epoch 10471/30000 Training Loss: 0.042075954377651215\n",
      "Epoch 10472/30000 Training Loss: 0.06357870250940323\n",
      "Epoch 10473/30000 Training Loss: 0.045046769082546234\n",
      "Epoch 10474/30000 Training Loss: 0.055249784141778946\n",
      "Epoch 10475/30000 Training Loss: 0.0527627095580101\n",
      "Epoch 10476/30000 Training Loss: 0.04885155335068703\n",
      "Epoch 10477/30000 Training Loss: 0.05125680938363075\n",
      "Epoch 10478/30000 Training Loss: 0.060364920645952225\n",
      "Epoch 10479/30000 Training Loss: 0.03874710202217102\n",
      "Epoch 10480/30000 Training Loss: 0.032773055136203766\n",
      "Epoch 10481/30000 Training Loss: 0.0507550947368145\n",
      "Epoch 10482/30000 Training Loss: 0.05868324637413025\n",
      "Epoch 10483/30000 Training Loss: 0.06380436569452286\n",
      "Epoch 10484/30000 Training Loss: 0.05245492607355118\n",
      "Epoch 10485/30000 Training Loss: 0.05868573486804962\n",
      "Epoch 10486/30000 Training Loss: 0.05945863202214241\n",
      "Epoch 10487/30000 Training Loss: 0.04617001488804817\n",
      "Epoch 10488/30000 Training Loss: 0.04232174903154373\n",
      "Epoch 10489/30000 Training Loss: 0.051249802112579346\n",
      "Epoch 10490/30000 Training Loss: 0.06599506735801697\n",
      "Epoch 10491/30000 Training Loss: 0.05593977868556976\n",
      "Epoch 10492/30000 Training Loss: 0.0632171779870987\n",
      "Epoch 10493/30000 Training Loss: 0.03897677734494209\n",
      "Epoch 10494/30000 Training Loss: 0.045498915016651154\n",
      "Epoch 10495/30000 Training Loss: 0.057732779532670975\n",
      "Epoch 10496/30000 Training Loss: 0.05499549210071564\n",
      "Epoch 10497/30000 Training Loss: 0.04348620027303696\n",
      "Epoch 10498/30000 Training Loss: 0.06055232137441635\n",
      "Epoch 10499/30000 Training Loss: 0.051725320518016815\n",
      "Epoch 10500/30000 Training Loss: 0.06868525594472885\n",
      "Epoch 10500/30000 Validation Loss: 0.047212861478328705\n",
      "Epoch 10501/30000 Training Loss: 0.05372757092118263\n",
      "Epoch 10502/30000 Training Loss: 0.04870486259460449\n",
      "Epoch 10503/30000 Training Loss: 0.055774588137865067\n",
      "Epoch 10504/30000 Training Loss: 0.05070522800087929\n",
      "Epoch 10505/30000 Training Loss: 0.0477079339325428\n",
      "Epoch 10506/30000 Training Loss: 0.04392200708389282\n",
      "Epoch 10507/30000 Training Loss: 0.070701003074646\n",
      "Epoch 10508/30000 Training Loss: 0.05867448076605797\n",
      "Epoch 10509/30000 Training Loss: 0.03715457394719124\n",
      "Epoch 10510/30000 Training Loss: 0.0520014688372612\n",
      "Epoch 10511/30000 Training Loss: 0.056142449378967285\n",
      "Epoch 10512/30000 Training Loss: 0.0536222830414772\n",
      "Epoch 10513/30000 Training Loss: 0.040308237075805664\n",
      "Epoch 10514/30000 Training Loss: 0.06199336051940918\n",
      "Epoch 10515/30000 Training Loss: 0.06955958902835846\n",
      "Epoch 10516/30000 Training Loss: 0.056223414838314056\n",
      "Epoch 10517/30000 Training Loss: 0.05086209252476692\n",
      "Epoch 10518/30000 Training Loss: 0.07829630374908447\n",
      "Epoch 10519/30000 Training Loss: 0.05925092101097107\n",
      "Epoch 10520/30000 Training Loss: 0.05671785771846771\n",
      "Epoch 10521/30000 Training Loss: 0.06271075457334518\n",
      "Epoch 10522/30000 Training Loss: 0.04835159331560135\n",
      "Epoch 10523/30000 Training Loss: 0.07029739767313004\n",
      "Epoch 10524/30000 Training Loss: 0.05762198194861412\n",
      "Epoch 10525/30000 Training Loss: 0.0598740428686142\n",
      "Epoch 10526/30000 Training Loss: 0.05927421152591705\n",
      "Epoch 10527/30000 Training Loss: 0.04559414088726044\n",
      "Epoch 10528/30000 Training Loss: 0.0657096803188324\n",
      "Epoch 10529/30000 Training Loss: 0.05407754331827164\n",
      "Epoch 10530/30000 Training Loss: 0.04746413230895996\n",
      "Epoch 10531/30000 Training Loss: 0.06433898955583572\n",
      "Epoch 10532/30000 Training Loss: 0.06206890195608139\n",
      "Epoch 10533/30000 Training Loss: 0.06620904803276062\n",
      "Epoch 10534/30000 Training Loss: 0.04560645669698715\n",
      "Epoch 10535/30000 Training Loss: 0.052987467497587204\n",
      "Epoch 10536/30000 Training Loss: 0.05272373557090759\n",
      "Epoch 10537/30000 Training Loss: 0.05386209115386009\n",
      "Epoch 10538/30000 Training Loss: 0.06865116953849792\n",
      "Epoch 10539/30000 Training Loss: 0.04587281495332718\n",
      "Epoch 10540/30000 Training Loss: 0.050413280725479126\n",
      "Epoch 10541/30000 Training Loss: 0.05349849909543991\n",
      "Epoch 10542/30000 Training Loss: 0.05675853416323662\n",
      "Epoch 10543/30000 Training Loss: 0.04864932596683502\n",
      "Epoch 10544/30000 Training Loss: 0.07142651826143265\n",
      "Epoch 10545/30000 Training Loss: 0.037906404584646225\n",
      "Epoch 10546/30000 Training Loss: 0.04768924042582512\n",
      "Epoch 10547/30000 Training Loss: 0.05564860999584198\n",
      "Epoch 10548/30000 Training Loss: 0.046527136117219925\n",
      "Epoch 10549/30000 Training Loss: 0.06669845432043076\n",
      "Epoch 10550/30000 Training Loss: 0.05484122037887573\n",
      "Epoch 10551/30000 Training Loss: 0.06893764436244965\n",
      "Epoch 10552/30000 Training Loss: 0.05293973535299301\n",
      "Epoch 10553/30000 Training Loss: 0.05078916251659393\n",
      "Epoch 10554/30000 Training Loss: 0.05357891693711281\n",
      "Epoch 10555/30000 Training Loss: 0.06004675477743149\n",
      "Epoch 10556/30000 Training Loss: 0.046352628618478775\n",
      "Epoch 10557/30000 Training Loss: 0.06444680690765381\n",
      "Epoch 10558/30000 Training Loss: 0.06503112614154816\n",
      "Epoch 10559/30000 Training Loss: 0.03279007971286774\n",
      "Epoch 10560/30000 Training Loss: 0.044257309287786484\n",
      "Epoch 10561/30000 Training Loss: 0.03977290913462639\n",
      "Epoch 10562/30000 Training Loss: 0.05746758356690407\n",
      "Epoch 10563/30000 Training Loss: 0.05340101569890976\n",
      "Epoch 10564/30000 Training Loss: 0.05525842681527138\n",
      "Epoch 10565/30000 Training Loss: 0.046859823167324066\n",
      "Epoch 10566/30000 Training Loss: 0.046494826674461365\n",
      "Epoch 10567/30000 Training Loss: 0.053464263677597046\n",
      "Epoch 10568/30000 Training Loss: 0.04701831936836243\n",
      "Epoch 10569/30000 Training Loss: 0.05101459100842476\n",
      "Epoch 10570/30000 Training Loss: 0.05733316019177437\n",
      "Epoch 10571/30000 Training Loss: 0.042797114700078964\n",
      "Epoch 10572/30000 Training Loss: 0.04462656378746033\n",
      "Epoch 10573/30000 Training Loss: 0.0642266795039177\n",
      "Epoch 10574/30000 Training Loss: 0.053003884851932526\n",
      "Epoch 10575/30000 Training Loss: 0.048158589750528336\n",
      "Epoch 10576/30000 Training Loss: 0.04357248544692993\n",
      "Epoch 10577/30000 Training Loss: 0.05030267685651779\n",
      "Epoch 10578/30000 Training Loss: 0.04970933496952057\n",
      "Epoch 10579/30000 Training Loss: 0.0696406364440918\n",
      "Epoch 10580/30000 Training Loss: 0.051924239844083786\n",
      "Epoch 10581/30000 Training Loss: 0.0695439875125885\n",
      "Epoch 10582/30000 Training Loss: 0.04203684628009796\n",
      "Epoch 10583/30000 Training Loss: 0.04581751674413681\n",
      "Epoch 10584/30000 Training Loss: 0.056025631725788116\n",
      "Epoch 10585/30000 Training Loss: 0.053385816514492035\n",
      "Epoch 10586/30000 Training Loss: 0.06253249198198318\n",
      "Epoch 10587/30000 Training Loss: 0.05231387913227081\n",
      "Epoch 10588/30000 Training Loss: 0.06384970992803574\n",
      "Epoch 10589/30000 Training Loss: 0.05389482527971268\n",
      "Epoch 10590/30000 Training Loss: 0.06177205964922905\n",
      "Epoch 10591/30000 Training Loss: 0.060449738055467606\n",
      "Epoch 10592/30000 Training Loss: 0.056476861238479614\n",
      "Epoch 10593/30000 Training Loss: 0.05702253431081772\n",
      "Epoch 10594/30000 Training Loss: 0.0705907791852951\n",
      "Epoch 10595/30000 Training Loss: 0.04739443585276604\n",
      "Epoch 10596/30000 Training Loss: 0.046893246471881866\n",
      "Epoch 10597/30000 Training Loss: 0.033159416168928146\n",
      "Epoch 10598/30000 Training Loss: 0.05099807307124138\n",
      "Epoch 10599/30000 Training Loss: 0.06106121465563774\n",
      "Epoch 10600/30000 Training Loss: 0.0547565259039402\n",
      "Epoch 10600/30000 Validation Loss: 0.0519561804831028\n",
      "Epoch 10601/30000 Training Loss: 0.05942041054368019\n",
      "Epoch 10602/30000 Training Loss: 0.062202051281929016\n",
      "Epoch 10603/30000 Training Loss: 0.047798722982406616\n",
      "Epoch 10604/30000 Training Loss: 0.05590827390551567\n",
      "Epoch 10605/30000 Training Loss: 0.060112401843070984\n",
      "Epoch 10606/30000 Training Loss: 0.0632133036851883\n",
      "Epoch 10607/30000 Training Loss: 0.04821769893169403\n",
      "Epoch 10608/30000 Training Loss: 0.06989537179470062\n",
      "Epoch 10609/30000 Training Loss: 0.0431426577270031\n",
      "Epoch 10610/30000 Training Loss: 0.075255386531353\n",
      "Epoch 10611/30000 Training Loss: 0.03889598697423935\n",
      "Epoch 10612/30000 Training Loss: 0.07257041335105896\n",
      "Epoch 10613/30000 Training Loss: 0.05191997438669205\n",
      "Epoch 10614/30000 Training Loss: 0.03499314934015274\n",
      "Epoch 10615/30000 Training Loss: 0.06463951617479324\n",
      "Epoch 10616/30000 Training Loss: 0.05851048231124878\n",
      "Epoch 10617/30000 Training Loss: 0.052034396678209305\n",
      "Epoch 10618/30000 Training Loss: 0.05662582442164421\n",
      "Epoch 10619/30000 Training Loss: 0.054534912109375\n",
      "Epoch 10620/30000 Training Loss: 0.060928069055080414\n",
      "Epoch 10621/30000 Training Loss: 0.053457796573638916\n",
      "Epoch 10622/30000 Training Loss: 0.05930902436375618\n",
      "Epoch 10623/30000 Training Loss: 0.05720202252268791\n",
      "Epoch 10624/30000 Training Loss: 0.05837785825133324\n",
      "Epoch 10625/30000 Training Loss: 0.04625196009874344\n",
      "Epoch 10626/30000 Training Loss: 0.0630413144826889\n",
      "Epoch 10627/30000 Training Loss: 0.049901604652404785\n",
      "Epoch 10628/30000 Training Loss: 0.03300734609365463\n",
      "Epoch 10629/30000 Training Loss: 0.060812242329120636\n",
      "Epoch 10630/30000 Training Loss: 0.041241686791181564\n",
      "Epoch 10631/30000 Training Loss: 0.05431476607918739\n",
      "Epoch 10632/30000 Training Loss: 0.058990128338336945\n",
      "Epoch 10633/30000 Training Loss: 0.07040125131607056\n",
      "Epoch 10634/30000 Training Loss: 0.0741531252861023\n",
      "Epoch 10635/30000 Training Loss: 0.04577900096774101\n",
      "Epoch 10636/30000 Training Loss: 0.0545990914106369\n",
      "Epoch 10637/30000 Training Loss: 0.04258852079510689\n",
      "Epoch 10638/30000 Training Loss: 0.0537484735250473\n",
      "Epoch 10639/30000 Training Loss: 0.0702286809682846\n",
      "Epoch 10640/30000 Training Loss: 0.06444185972213745\n",
      "Epoch 10641/30000 Training Loss: 0.05748388543725014\n",
      "Epoch 10642/30000 Training Loss: 0.04401123523712158\n",
      "Epoch 10643/30000 Training Loss: 0.049538008868694305\n",
      "Epoch 10644/30000 Training Loss: 0.053633540868759155\n",
      "Epoch 10645/30000 Training Loss: 0.06460022926330566\n",
      "Epoch 10646/30000 Training Loss: 0.05482441186904907\n",
      "Epoch 10647/30000 Training Loss: 0.06462855637073517\n",
      "Epoch 10648/30000 Training Loss: 0.06852654367685318\n",
      "Epoch 10649/30000 Training Loss: 0.0466945618391037\n",
      "Epoch 10650/30000 Training Loss: 0.04602951556444168\n",
      "Epoch 10651/30000 Training Loss: 0.061017170548439026\n",
      "Epoch 10652/30000 Training Loss: 0.05296167731285095\n",
      "Epoch 10653/30000 Training Loss: 0.054790597409009933\n",
      "Epoch 10654/30000 Training Loss: 0.05868273973464966\n",
      "Epoch 10655/30000 Training Loss: 0.05175014212727547\n",
      "Epoch 10656/30000 Training Loss: 0.07183452695608139\n",
      "Epoch 10657/30000 Training Loss: 0.054374296218156815\n",
      "Epoch 10658/30000 Training Loss: 0.05291900783777237\n",
      "Epoch 10659/30000 Training Loss: 0.050135910511016846\n",
      "Epoch 10660/30000 Training Loss: 0.05607098340988159\n",
      "Epoch 10661/30000 Training Loss: 0.051770299673080444\n",
      "Epoch 10662/30000 Training Loss: 0.04824815317988396\n",
      "Epoch 10663/30000 Training Loss: 0.06495743989944458\n",
      "Epoch 10664/30000 Training Loss: 0.04812924191355705\n",
      "Epoch 10665/30000 Training Loss: 0.039868056774139404\n",
      "Epoch 10666/30000 Training Loss: 0.049105167388916016\n",
      "Epoch 10667/30000 Training Loss: 0.053288400173187256\n",
      "Epoch 10668/30000 Training Loss: 0.04315117746591568\n",
      "Epoch 10669/30000 Training Loss: 0.05121525377035141\n",
      "Epoch 10670/30000 Training Loss: 0.05621982365846634\n",
      "Epoch 10671/30000 Training Loss: 0.05230574682354927\n",
      "Epoch 10672/30000 Training Loss: 0.05700181797146797\n",
      "Epoch 10673/30000 Training Loss: 0.06595053523778915\n",
      "Epoch 10674/30000 Training Loss: 0.0641220211982727\n",
      "Epoch 10675/30000 Training Loss: 0.060590289533138275\n",
      "Epoch 10676/30000 Training Loss: 0.04190754145383835\n",
      "Epoch 10677/30000 Training Loss: 0.05477023124694824\n",
      "Epoch 10678/30000 Training Loss: 0.06732281297445297\n",
      "Epoch 10679/30000 Training Loss: 0.06009679287672043\n",
      "Epoch 10680/30000 Training Loss: 0.05595315992832184\n",
      "Epoch 10681/30000 Training Loss: 0.05754121392965317\n",
      "Epoch 10682/30000 Training Loss: 0.06781654059886932\n",
      "Epoch 10683/30000 Training Loss: 0.05664423108100891\n",
      "Epoch 10684/30000 Training Loss: 0.04730851948261261\n",
      "Epoch 10685/30000 Training Loss: 0.06845781952142715\n",
      "Epoch 10686/30000 Training Loss: 0.04198508337140083\n",
      "Epoch 10687/30000 Training Loss: 0.039663515985012054\n",
      "Epoch 10688/30000 Training Loss: 0.053132399916648865\n",
      "Epoch 10689/30000 Training Loss: 0.06556049734354019\n",
      "Epoch 10690/30000 Training Loss: 0.053492624312639236\n",
      "Epoch 10691/30000 Training Loss: 0.06795903295278549\n",
      "Epoch 10692/30000 Training Loss: 0.05240098014473915\n",
      "Epoch 10693/30000 Training Loss: 0.041738659143447876\n",
      "Epoch 10694/30000 Training Loss: 0.06242991238832474\n",
      "Epoch 10695/30000 Training Loss: 0.06306805461645126\n",
      "Epoch 10696/30000 Training Loss: 0.043596938252449036\n",
      "Epoch 10697/30000 Training Loss: 0.06076991930603981\n",
      "Epoch 10698/30000 Training Loss: 0.05321373790502548\n",
      "Epoch 10699/30000 Training Loss: 0.05008665844798088\n",
      "Epoch 10700/30000 Training Loss: 0.054923295974731445\n",
      "Epoch 10700/30000 Validation Loss: 0.049986518919467926\n",
      "Epoch 10701/30000 Training Loss: 0.05294493958353996\n",
      "Epoch 10702/30000 Training Loss: 0.04191294312477112\n",
      "Epoch 10703/30000 Training Loss: 0.042916614562273026\n",
      "Epoch 10704/30000 Training Loss: 0.038230154663324356\n",
      "Epoch 10705/30000 Training Loss: 0.0482044443488121\n",
      "Epoch 10706/30000 Training Loss: 0.06361044943332672\n",
      "Epoch 10707/30000 Training Loss: 0.0439722053706646\n",
      "Epoch 10708/30000 Training Loss: 0.047312021255493164\n",
      "Epoch 10709/30000 Training Loss: 0.040356844663619995\n",
      "Epoch 10710/30000 Training Loss: 0.04643221199512482\n",
      "Epoch 10711/30000 Training Loss: 0.045721717178821564\n",
      "Epoch 10712/30000 Training Loss: 0.03449466824531555\n",
      "Epoch 10713/30000 Training Loss: 0.051253542304039\n",
      "Epoch 10714/30000 Training Loss: 0.05182236433029175\n",
      "Epoch 10715/30000 Training Loss: 0.06163736432790756\n",
      "Epoch 10716/30000 Training Loss: 0.04652029275894165\n",
      "Epoch 10717/30000 Training Loss: 0.03558151423931122\n",
      "Epoch 10718/30000 Training Loss: 0.06169229745864868\n",
      "Epoch 10719/30000 Training Loss: 0.06880182027816772\n",
      "Epoch 10720/30000 Training Loss: 0.042753204703330994\n",
      "Epoch 10721/30000 Training Loss: 0.055740319192409515\n",
      "Epoch 10722/30000 Training Loss: 0.05052579194307327\n",
      "Epoch 10723/30000 Training Loss: 0.05019105598330498\n",
      "Epoch 10724/30000 Training Loss: 0.06386101990938187\n",
      "Epoch 10725/30000 Training Loss: 0.04646642878651619\n",
      "Epoch 10726/30000 Training Loss: 0.05505608022212982\n",
      "Epoch 10727/30000 Training Loss: 0.06618587672710419\n",
      "Epoch 10728/30000 Training Loss: 0.04489634558558464\n",
      "Epoch 10729/30000 Training Loss: 0.061438579112291336\n",
      "Epoch 10730/30000 Training Loss: 0.051873791962862015\n",
      "Epoch 10731/30000 Training Loss: 0.04906533658504486\n",
      "Epoch 10732/30000 Training Loss: 0.03825971111655235\n",
      "Epoch 10733/30000 Training Loss: 0.05571018531918526\n",
      "Epoch 10734/30000 Training Loss: 0.06383734941482544\n",
      "Epoch 10735/30000 Training Loss: 0.043851617723703384\n",
      "Epoch 10736/30000 Training Loss: 0.04727492853999138\n",
      "Epoch 10737/30000 Training Loss: 0.056819699704647064\n",
      "Epoch 10738/30000 Training Loss: 0.05225507169961929\n",
      "Epoch 10739/30000 Training Loss: 0.04434465616941452\n",
      "Epoch 10740/30000 Training Loss: 0.07356420159339905\n",
      "Epoch 10741/30000 Training Loss: 0.05550111085176468\n",
      "Epoch 10742/30000 Training Loss: 0.039402082562446594\n",
      "Epoch 10743/30000 Training Loss: 0.055996328592300415\n",
      "Epoch 10744/30000 Training Loss: 0.04227861762046814\n",
      "Epoch 10745/30000 Training Loss: 0.05318387225270271\n",
      "Epoch 10746/30000 Training Loss: 0.05118376016616821\n",
      "Epoch 10747/30000 Training Loss: 0.06123196333646774\n",
      "Epoch 10748/30000 Training Loss: 0.06429359316825867\n",
      "Epoch 10749/30000 Training Loss: 0.04864457994699478\n",
      "Epoch 10750/30000 Training Loss: 0.05172336474061012\n",
      "Epoch 10751/30000 Training Loss: 0.05946263670921326\n",
      "Epoch 10752/30000 Training Loss: 0.062085509300231934\n",
      "Epoch 10753/30000 Training Loss: 0.04330817610025406\n",
      "Epoch 10754/30000 Training Loss: 0.04402017593383789\n",
      "Epoch 10755/30000 Training Loss: 0.05243689566850662\n",
      "Epoch 10756/30000 Training Loss: 0.0763610377907753\n",
      "Epoch 10757/30000 Training Loss: 0.057407863438129425\n",
      "Epoch 10758/30000 Training Loss: 0.0628838762640953\n",
      "Epoch 10759/30000 Training Loss: 0.05066896975040436\n",
      "Epoch 10760/30000 Training Loss: 0.04139363765716553\n",
      "Epoch 10761/30000 Training Loss: 0.048080358654260635\n",
      "Epoch 10762/30000 Training Loss: 0.057933736592531204\n",
      "Epoch 10763/30000 Training Loss: 0.04356306046247482\n",
      "Epoch 10764/30000 Training Loss: 0.06210344284772873\n",
      "Epoch 10765/30000 Training Loss: 0.044734977185726166\n",
      "Epoch 10766/30000 Training Loss: 0.07555761933326721\n",
      "Epoch 10767/30000 Training Loss: 0.049479879438877106\n",
      "Epoch 10768/30000 Training Loss: 0.04394761100411415\n",
      "Epoch 10769/30000 Training Loss: 0.0681353211402893\n",
      "Epoch 10770/30000 Training Loss: 0.05021008849143982\n",
      "Epoch 10771/30000 Training Loss: 0.052761271595954895\n",
      "Epoch 10772/30000 Training Loss: 0.06709268689155579\n",
      "Epoch 10773/30000 Training Loss: 0.048649370670318604\n",
      "Epoch 10774/30000 Training Loss: 0.06003265827894211\n",
      "Epoch 10775/30000 Training Loss: 0.046292852610349655\n",
      "Epoch 10776/30000 Training Loss: 0.0507882758975029\n",
      "Epoch 10777/30000 Training Loss: 0.04252246394753456\n",
      "Epoch 10778/30000 Training Loss: 0.05552656203508377\n",
      "Epoch 10779/30000 Training Loss: 0.05270272493362427\n",
      "Epoch 10780/30000 Training Loss: 0.05098813772201538\n",
      "Epoch 10781/30000 Training Loss: 0.04953373968601227\n",
      "Epoch 10782/30000 Training Loss: 0.04767679050564766\n",
      "Epoch 10783/30000 Training Loss: 0.044143132865428925\n",
      "Epoch 10784/30000 Training Loss: 0.05827214941382408\n",
      "Epoch 10785/30000 Training Loss: 0.04878660663962364\n",
      "Epoch 10786/30000 Training Loss: 0.04346797987818718\n",
      "Epoch 10787/30000 Training Loss: 0.055917512625455856\n",
      "Epoch 10788/30000 Training Loss: 0.05484190583229065\n",
      "Epoch 10789/30000 Training Loss: 0.040901683270931244\n",
      "Epoch 10790/30000 Training Loss: 0.06668133288621902\n",
      "Epoch 10791/30000 Training Loss: 0.037995338439941406\n",
      "Epoch 10792/30000 Training Loss: 0.04706230014562607\n",
      "Epoch 10793/30000 Training Loss: 0.056241925805807114\n",
      "Epoch 10794/30000 Training Loss: 0.04443341866135597\n",
      "Epoch 10795/30000 Training Loss: 0.05894971638917923\n",
      "Epoch 10796/30000 Training Loss: 0.0605156309902668\n",
      "Epoch 10797/30000 Training Loss: 0.047377750277519226\n",
      "Epoch 10798/30000 Training Loss: 0.048663172870874405\n",
      "Epoch 10799/30000 Training Loss: 0.049545980989933014\n",
      "Epoch 10800/30000 Training Loss: 0.04016423225402832\n",
      "Epoch 10800/30000 Validation Loss: 0.06262076646089554\n",
      "Epoch 10801/30000 Training Loss: 0.06472591310739517\n",
      "Epoch 10802/30000 Training Loss: 0.06273026019334793\n",
      "Epoch 10803/30000 Training Loss: 0.06270019710063934\n",
      "Epoch 10804/30000 Training Loss: 0.04193680360913277\n",
      "Epoch 10805/30000 Training Loss: 0.04739930480718613\n",
      "Epoch 10806/30000 Training Loss: 0.0574372336268425\n",
      "Epoch 10807/30000 Training Loss: 0.07502670586109161\n",
      "Epoch 10808/30000 Training Loss: 0.06218888238072395\n",
      "Epoch 10809/30000 Training Loss: 0.05463521555066109\n",
      "Epoch 10810/30000 Training Loss: 0.06284724175930023\n",
      "Epoch 10811/30000 Training Loss: 0.06715846806764603\n",
      "Epoch 10812/30000 Training Loss: 0.05464210361242294\n",
      "Epoch 10813/30000 Training Loss: 0.0549488365650177\n",
      "Epoch 10814/30000 Training Loss: 0.04877619445323944\n",
      "Epoch 10815/30000 Training Loss: 0.03604908287525177\n",
      "Epoch 10816/30000 Training Loss: 0.05043277144432068\n",
      "Epoch 10817/30000 Training Loss: 0.07162723690271378\n",
      "Epoch 10818/30000 Training Loss: 0.051679737865924835\n",
      "Epoch 10819/30000 Training Loss: 0.06283721327781677\n",
      "Epoch 10820/30000 Training Loss: 0.04878358542919159\n",
      "Epoch 10821/30000 Training Loss: 0.044688135385513306\n",
      "Epoch 10822/30000 Training Loss: 0.046106550842523575\n",
      "Epoch 10823/30000 Training Loss: 0.06252766400575638\n",
      "Epoch 10824/30000 Training Loss: 0.05307219550013542\n",
      "Epoch 10825/30000 Training Loss: 0.0450991690158844\n",
      "Epoch 10826/30000 Training Loss: 0.045739028602838516\n",
      "Epoch 10827/30000 Training Loss: 0.04768994823098183\n",
      "Epoch 10828/30000 Training Loss: 0.05387736111879349\n",
      "Epoch 10829/30000 Training Loss: 0.04767293110489845\n",
      "Epoch 10830/30000 Training Loss: 0.06262914091348648\n",
      "Epoch 10831/30000 Training Loss: 0.059825487434864044\n",
      "Epoch 10832/30000 Training Loss: 0.053576551377773285\n",
      "Epoch 10833/30000 Training Loss: 0.05646134167909622\n",
      "Epoch 10834/30000 Training Loss: 0.059723708778619766\n",
      "Epoch 10835/30000 Training Loss: 0.05649302154779434\n",
      "Epoch 10836/30000 Training Loss: 0.04049308970570564\n",
      "Epoch 10837/30000 Training Loss: 0.05430126562714577\n",
      "Epoch 10838/30000 Training Loss: 0.047018542885780334\n",
      "Epoch 10839/30000 Training Loss: 0.0458318367600441\n",
      "Epoch 10840/30000 Training Loss: 0.06262552738189697\n",
      "Epoch 10841/30000 Training Loss: 0.06652891635894775\n",
      "Epoch 10842/30000 Training Loss: 0.051115695387125015\n",
      "Epoch 10843/30000 Training Loss: 0.05117723345756531\n",
      "Epoch 10844/30000 Training Loss: 0.0650147795677185\n",
      "Epoch 10845/30000 Training Loss: 0.05716782063245773\n",
      "Epoch 10846/30000 Training Loss: 0.053404632955789566\n",
      "Epoch 10847/30000 Training Loss: 0.04987278953194618\n",
      "Epoch 10848/30000 Training Loss: 0.043217580765485764\n",
      "Epoch 10849/30000 Training Loss: 0.05205097794532776\n",
      "Epoch 10850/30000 Training Loss: 0.05270158126950264\n",
      "Epoch 10851/30000 Training Loss: 0.05599120259284973\n",
      "Epoch 10852/30000 Training Loss: 0.047066882252693176\n",
      "Epoch 10853/30000 Training Loss: 0.04360164329409599\n",
      "Epoch 10854/30000 Training Loss: 0.0546991303563118\n",
      "Epoch 10855/30000 Training Loss: 0.06502082943916321\n",
      "Epoch 10856/30000 Training Loss: 0.05497201159596443\n",
      "Epoch 10857/30000 Training Loss: 0.05848328024148941\n",
      "Epoch 10858/30000 Training Loss: 0.04688641056418419\n",
      "Epoch 10859/30000 Training Loss: 0.05100235342979431\n",
      "Epoch 10860/30000 Training Loss: 0.06822620332241058\n",
      "Epoch 10861/30000 Training Loss: 0.05196898803114891\n",
      "Epoch 10862/30000 Training Loss: 0.05500001832842827\n",
      "Epoch 10863/30000 Training Loss: 0.06276977062225342\n",
      "Epoch 10864/30000 Training Loss: 0.05045691877603531\n",
      "Epoch 10865/30000 Training Loss: 0.04300038889050484\n",
      "Epoch 10866/30000 Training Loss: 0.054766155779361725\n",
      "Epoch 10867/30000 Training Loss: 0.05176878720521927\n",
      "Epoch 10868/30000 Training Loss: 0.0596727654337883\n",
      "Epoch 10869/30000 Training Loss: 0.03844805061817169\n",
      "Epoch 10870/30000 Training Loss: 0.06287165731191635\n",
      "Epoch 10871/30000 Training Loss: 0.06134738028049469\n",
      "Epoch 10872/30000 Training Loss: 0.061294831335544586\n",
      "Epoch 10873/30000 Training Loss: 0.052108414471149445\n",
      "Epoch 10874/30000 Training Loss: 0.047733839601278305\n",
      "Epoch 10875/30000 Training Loss: 0.052728697657585144\n",
      "Epoch 10876/30000 Training Loss: 0.04579313099384308\n",
      "Epoch 10877/30000 Training Loss: 0.055405039340257645\n",
      "Epoch 10878/30000 Training Loss: 0.06229247897863388\n",
      "Epoch 10879/30000 Training Loss: 0.046712327748537064\n",
      "Epoch 10880/30000 Training Loss: 0.05887744203209877\n",
      "Epoch 10881/30000 Training Loss: 0.05451161041855812\n",
      "Epoch 10882/30000 Training Loss: 0.07315559685230255\n",
      "Epoch 10883/30000 Training Loss: 0.050021350383758545\n",
      "Epoch 10884/30000 Training Loss: 0.06876523792743683\n",
      "Epoch 10885/30000 Training Loss: 0.04543608799576759\n",
      "Epoch 10886/30000 Training Loss: 0.04937024787068367\n",
      "Epoch 10887/30000 Training Loss: 0.049286745488643646\n",
      "Epoch 10888/30000 Training Loss: 0.04659758135676384\n",
      "Epoch 10889/30000 Training Loss: 0.0474918894469738\n",
      "Epoch 10890/30000 Training Loss: 0.04652532935142517\n",
      "Epoch 10891/30000 Training Loss: 0.04638316482305527\n",
      "Epoch 10892/30000 Training Loss: 0.05900933966040611\n",
      "Epoch 10893/30000 Training Loss: 0.04925284534692764\n",
      "Epoch 10894/30000 Training Loss: 0.04972952976822853\n",
      "Epoch 10895/30000 Training Loss: 0.06767295300960541\n",
      "Epoch 10896/30000 Training Loss: 0.052657704800367355\n",
      "Epoch 10897/30000 Training Loss: 0.03312496095895767\n",
      "Epoch 10898/30000 Training Loss: 0.07110941410064697\n",
      "Epoch 10899/30000 Training Loss: 0.05172598734498024\n",
      "Epoch 10900/30000 Training Loss: 0.05239799991250038\n",
      "Epoch 10900/30000 Validation Loss: 0.04764849692583084\n",
      "Epoch 10901/30000 Training Loss: 0.057810742408037186\n",
      "Epoch 10902/30000 Training Loss: 0.06371670216321945\n",
      "Epoch 10903/30000 Training Loss: 0.04579111933708191\n",
      "Epoch 10904/30000 Training Loss: 0.04080555588006973\n",
      "Epoch 10905/30000 Training Loss: 0.0509229376912117\n",
      "Epoch 10906/30000 Training Loss: 0.06575356423854828\n",
      "Epoch 10907/30000 Training Loss: 0.036716487258672714\n",
      "Epoch 10908/30000 Training Loss: 0.052156589925289154\n",
      "Epoch 10909/30000 Training Loss: 0.0648064911365509\n",
      "Epoch 10910/30000 Training Loss: 0.05345554277300835\n",
      "Epoch 10911/30000 Training Loss: 0.07276546210050583\n",
      "Epoch 10912/30000 Training Loss: 0.06974976509809494\n",
      "Epoch 10913/30000 Training Loss: 0.05564333498477936\n",
      "Epoch 10914/30000 Training Loss: 0.045137740671634674\n",
      "Epoch 10915/30000 Training Loss: 0.038866132497787476\n",
      "Epoch 10916/30000 Training Loss: 0.05746757239103317\n",
      "Epoch 10917/30000 Training Loss: 0.05483304709196091\n",
      "Epoch 10918/30000 Training Loss: 0.04567839950323105\n",
      "Epoch 10919/30000 Training Loss: 0.062764011323452\n",
      "Epoch 10920/30000 Training Loss: 0.04899008572101593\n",
      "Epoch 10921/30000 Training Loss: 0.0506172701716423\n",
      "Epoch 10922/30000 Training Loss: 0.044510368257761\n",
      "Epoch 10923/30000 Training Loss: 0.0440153107047081\n",
      "Epoch 10924/30000 Training Loss: 0.047983940690755844\n",
      "Epoch 10925/30000 Training Loss: 0.03518138825893402\n",
      "Epoch 10926/30000 Training Loss: 0.05461357906460762\n",
      "Epoch 10927/30000 Training Loss: 0.04815226048231125\n",
      "Epoch 10928/30000 Training Loss: 0.04500264301896095\n",
      "Epoch 10929/30000 Training Loss: 0.04213256388902664\n",
      "Epoch 10930/30000 Training Loss: 0.08752510696649551\n",
      "Epoch 10931/30000 Training Loss: 0.04818945378065109\n",
      "Epoch 10932/30000 Training Loss: 0.04145795851945877\n",
      "Epoch 10933/30000 Training Loss: 0.05576620623469353\n",
      "Epoch 10934/30000 Training Loss: 0.04819199815392494\n",
      "Epoch 10935/30000 Training Loss: 0.05137704312801361\n",
      "Epoch 10936/30000 Training Loss: 0.05509105324745178\n",
      "Epoch 10937/30000 Training Loss: 0.058655764907598495\n",
      "Epoch 10938/30000 Training Loss: 0.048108942806720734\n",
      "Epoch 10939/30000 Training Loss: 0.07272364944219589\n",
      "Epoch 10940/30000 Training Loss: 0.055074021220207214\n",
      "Epoch 10941/30000 Training Loss: 0.06032402440905571\n",
      "Epoch 10942/30000 Training Loss: 0.06016198918223381\n",
      "Epoch 10943/30000 Training Loss: 0.05691085383296013\n",
      "Epoch 10944/30000 Training Loss: 0.049013108015060425\n",
      "Epoch 10945/30000 Training Loss: 0.05688345432281494\n",
      "Epoch 10946/30000 Training Loss: 0.046802639961242676\n",
      "Epoch 10947/30000 Training Loss: 0.056283898651599884\n",
      "Epoch 10948/30000 Training Loss: 0.06352502107620239\n",
      "Epoch 10949/30000 Training Loss: 0.04799378290772438\n",
      "Epoch 10950/30000 Training Loss: 0.03786727786064148\n",
      "Epoch 10951/30000 Training Loss: 0.04202153906226158\n",
      "Epoch 10952/30000 Training Loss: 0.05109439417719841\n",
      "Epoch 10953/30000 Training Loss: 0.05371175706386566\n",
      "Epoch 10954/30000 Training Loss: 0.04816685616970062\n",
      "Epoch 10955/30000 Training Loss: 0.05269913375377655\n",
      "Epoch 10956/30000 Training Loss: 0.05043641850352287\n",
      "Epoch 10957/30000 Training Loss: 0.06266003102064133\n",
      "Epoch 10958/30000 Training Loss: 0.04679245501756668\n",
      "Epoch 10959/30000 Training Loss: 0.05880970507860184\n",
      "Epoch 10960/30000 Training Loss: 0.048250146210193634\n",
      "Epoch 10961/30000 Training Loss: 0.04518032819032669\n",
      "Epoch 10962/30000 Training Loss: 0.05159657821059227\n",
      "Epoch 10963/30000 Training Loss: 0.04543452709913254\n",
      "Epoch 10964/30000 Training Loss: 0.06676646322011948\n",
      "Epoch 10965/30000 Training Loss: 0.049903832376003265\n",
      "Epoch 10966/30000 Training Loss: 0.05344077944755554\n",
      "Epoch 10967/30000 Training Loss: 0.05176246911287308\n",
      "Epoch 10968/30000 Training Loss: 0.05699200928211212\n",
      "Epoch 10969/30000 Training Loss: 0.06293750554323196\n",
      "Epoch 10970/30000 Training Loss: 0.0652579516172409\n",
      "Epoch 10971/30000 Training Loss: 0.0478096529841423\n",
      "Epoch 10972/30000 Training Loss: 0.05863305181264877\n",
      "Epoch 10973/30000 Training Loss: 0.05923944711685181\n",
      "Epoch 10974/30000 Training Loss: 0.052843835204839706\n",
      "Epoch 10975/30000 Training Loss: 0.06948720663785934\n",
      "Epoch 10976/30000 Training Loss: 0.04742298647761345\n",
      "Epoch 10977/30000 Training Loss: 0.04699576646089554\n",
      "Epoch 10978/30000 Training Loss: 0.05383221060037613\n",
      "Epoch 10979/30000 Training Loss: 0.046250686049461365\n",
      "Epoch 10980/30000 Training Loss: 0.03670787438750267\n",
      "Epoch 10981/30000 Training Loss: 0.052872318774461746\n",
      "Epoch 10982/30000 Training Loss: 0.05160079151391983\n",
      "Epoch 10983/30000 Training Loss: 0.05919908359646797\n",
      "Epoch 10984/30000 Training Loss: 0.043882597237825394\n",
      "Epoch 10985/30000 Training Loss: 0.04394704848527908\n",
      "Epoch 10986/30000 Training Loss: 0.06332027167081833\n",
      "Epoch 10987/30000 Training Loss: 0.060068510472774506\n",
      "Epoch 10988/30000 Training Loss: 0.06016967073082924\n",
      "Epoch 10989/30000 Training Loss: 0.04238899052143097\n",
      "Epoch 10990/30000 Training Loss: 0.043975960463285446\n",
      "Epoch 10991/30000 Training Loss: 0.0458696186542511\n",
      "Epoch 10992/30000 Training Loss: 0.046828869730234146\n",
      "Epoch 10993/30000 Training Loss: 0.03701296076178551\n",
      "Epoch 10994/30000 Training Loss: 0.042052265256643295\n",
      "Epoch 10995/30000 Training Loss: 0.059001654386520386\n",
      "Epoch 10996/30000 Training Loss: 0.044388629496097565\n",
      "Epoch 10997/30000 Training Loss: 0.06818380951881409\n",
      "Epoch 10998/30000 Training Loss: 0.05064474046230316\n",
      "Epoch 10999/30000 Training Loss: 0.0484834760427475\n",
      "Epoch 11000/30000 Training Loss: 0.04839518666267395\n",
      "Epoch 11000/30000 Validation Loss: 0.053844645619392395\n",
      "Epoch 11001/30000 Training Loss: 0.050571464002132416\n",
      "Epoch 11002/30000 Training Loss: 0.060378409922122955\n",
      "Epoch 11003/30000 Training Loss: 0.0614204928278923\n",
      "Epoch 11004/30000 Training Loss: 0.0489656925201416\n",
      "Epoch 11005/30000 Training Loss: 0.05589240789413452\n",
      "Epoch 11006/30000 Training Loss: 0.053002700209617615\n",
      "Epoch 11007/30000 Training Loss: 0.04834480956196785\n",
      "Epoch 11008/30000 Training Loss: 0.05870231240987778\n",
      "Epoch 11009/30000 Training Loss: 0.042453452944755554\n",
      "Epoch 11010/30000 Training Loss: 0.054358113557100296\n",
      "Epoch 11011/30000 Training Loss: 0.046324603259563446\n",
      "Epoch 11012/30000 Training Loss: 0.07925286144018173\n",
      "Epoch 11013/30000 Training Loss: 0.05608539283275604\n",
      "Epoch 11014/30000 Training Loss: 0.046768274158239365\n",
      "Epoch 11015/30000 Training Loss: 0.035052914172410965\n",
      "Epoch 11016/30000 Training Loss: 0.049408987164497375\n",
      "Epoch 11017/30000 Training Loss: 0.05262668430805206\n",
      "Epoch 11018/30000 Training Loss: 0.06432974338531494\n",
      "Epoch 11019/30000 Training Loss: 0.04464937746524811\n",
      "Epoch 11020/30000 Training Loss: 0.05098947882652283\n",
      "Epoch 11021/30000 Training Loss: 0.046686965972185135\n",
      "Epoch 11022/30000 Training Loss: 0.059005413204431534\n",
      "Epoch 11023/30000 Training Loss: 0.06453685462474823\n",
      "Epoch 11024/30000 Training Loss: 0.04441485181450844\n",
      "Epoch 11025/30000 Training Loss: 0.04883864149451256\n",
      "Epoch 11026/30000 Training Loss: 0.056031569838523865\n",
      "Epoch 11027/30000 Training Loss: 0.053728215396404266\n",
      "Epoch 11028/30000 Training Loss: 0.05027070641517639\n",
      "Epoch 11029/30000 Training Loss: 0.07263724505901337\n",
      "Epoch 11030/30000 Training Loss: 0.04372307285666466\n",
      "Epoch 11031/30000 Training Loss: 0.04528751224279404\n",
      "Epoch 11032/30000 Training Loss: 0.04259910061955452\n",
      "Epoch 11033/30000 Training Loss: 0.06024504080414772\n",
      "Epoch 11034/30000 Training Loss: 0.06313769519329071\n",
      "Epoch 11035/30000 Training Loss: 0.05069510638713837\n",
      "Epoch 11036/30000 Training Loss: 0.052636582404375076\n",
      "Epoch 11037/30000 Training Loss: 0.06205545738339424\n",
      "Epoch 11038/30000 Training Loss: 0.053734809160232544\n",
      "Epoch 11039/30000 Training Loss: 0.061918310821056366\n",
      "Epoch 11040/30000 Training Loss: 0.04513528570532799\n",
      "Epoch 11041/30000 Training Loss: 0.06091850996017456\n",
      "Epoch 11042/30000 Training Loss: 0.05676325783133507\n",
      "Epoch 11043/30000 Training Loss: 0.06703798472881317\n",
      "Epoch 11044/30000 Training Loss: 0.04660758376121521\n",
      "Epoch 11045/30000 Training Loss: 0.06969577819108963\n",
      "Epoch 11046/30000 Training Loss: 0.04313328117132187\n",
      "Epoch 11047/30000 Training Loss: 0.0517977774143219\n",
      "Epoch 11048/30000 Training Loss: 0.03608732298016548\n",
      "Epoch 11049/30000 Training Loss: 0.046303607523441315\n",
      "Epoch 11050/30000 Training Loss: 0.06111357361078262\n",
      "Epoch 11051/30000 Training Loss: 0.059730637818574905\n",
      "Epoch 11052/30000 Training Loss: 0.053330376744270325\n",
      "Epoch 11053/30000 Training Loss: 0.04549187421798706\n",
      "Epoch 11054/30000 Training Loss: 0.05516088008880615\n",
      "Epoch 11055/30000 Training Loss: 0.03490971401333809\n",
      "Epoch 11056/30000 Training Loss: 0.04998644441366196\n",
      "Epoch 11057/30000 Training Loss: 0.05823872983455658\n",
      "Epoch 11058/30000 Training Loss: 0.05664382874965668\n",
      "Epoch 11059/30000 Training Loss: 0.03779488801956177\n",
      "Epoch 11060/30000 Training Loss: 0.049449291080236435\n",
      "Epoch 11061/30000 Training Loss: 0.06577178090810776\n",
      "Epoch 11062/30000 Training Loss: 0.03347428888082504\n",
      "Epoch 11063/30000 Training Loss: 0.05271896347403526\n",
      "Epoch 11064/30000 Training Loss: 0.07314323633909225\n",
      "Epoch 11065/30000 Training Loss: 0.05105958133935928\n",
      "Epoch 11066/30000 Training Loss: 0.051378507167100906\n",
      "Epoch 11067/30000 Training Loss: 0.048460882157087326\n",
      "Epoch 11068/30000 Training Loss: 0.052182409912347794\n",
      "Epoch 11069/30000 Training Loss: 0.051574982702732086\n",
      "Epoch 11070/30000 Training Loss: 0.05608251318335533\n",
      "Epoch 11071/30000 Training Loss: 0.051012732088565826\n",
      "Epoch 11072/30000 Training Loss: 0.04904191195964813\n",
      "Epoch 11073/30000 Training Loss: 0.05262893810868263\n",
      "Epoch 11074/30000 Training Loss: 0.06160955876111984\n",
      "Epoch 11075/30000 Training Loss: 0.05924750864505768\n",
      "Epoch 11076/30000 Training Loss: 0.06380990147590637\n",
      "Epoch 11077/30000 Training Loss: 0.04414687678217888\n",
      "Epoch 11078/30000 Training Loss: 0.036838144063949585\n",
      "Epoch 11079/30000 Training Loss: 0.0462539941072464\n",
      "Epoch 11080/30000 Training Loss: 0.04352070018649101\n",
      "Epoch 11081/30000 Training Loss: 0.04417585954070091\n",
      "Epoch 11082/30000 Training Loss: 0.048546962440013885\n",
      "Epoch 11083/30000 Training Loss: 0.057750098407268524\n",
      "Epoch 11084/30000 Training Loss: 0.057073142379522324\n",
      "Epoch 11085/30000 Training Loss: 0.05723481625318527\n",
      "Epoch 11086/30000 Training Loss: 0.049341924488544464\n",
      "Epoch 11087/30000 Training Loss: 0.07230938971042633\n",
      "Epoch 11088/30000 Training Loss: 0.042756155133247375\n",
      "Epoch 11089/30000 Training Loss: 0.04540738835930824\n",
      "Epoch 11090/30000 Training Loss: 0.0558939166367054\n",
      "Epoch 11091/30000 Training Loss: 0.08431161940097809\n",
      "Epoch 11092/30000 Training Loss: 0.06240791827440262\n",
      "Epoch 11093/30000 Training Loss: 0.03446462005376816\n",
      "Epoch 11094/30000 Training Loss: 0.04726115241646767\n",
      "Epoch 11095/30000 Training Loss: 0.05045194551348686\n",
      "Epoch 11096/30000 Training Loss: 0.05579160153865814\n",
      "Epoch 11097/30000 Training Loss: 0.059367746114730835\n",
      "Epoch 11098/30000 Training Loss: 0.0578681081533432\n",
      "Epoch 11099/30000 Training Loss: 0.04846017435193062\n",
      "Epoch 11100/30000 Training Loss: 0.07466395944356918\n",
      "Epoch 11100/30000 Validation Loss: 0.06739704310894012\n",
      "Epoch 11101/30000 Training Loss: 0.03958713635802269\n",
      "Epoch 11102/30000 Training Loss: 0.05889143794775009\n",
      "Epoch 11103/30000 Training Loss: 0.05818673595786095\n",
      "Epoch 11104/30000 Training Loss: 0.05231895297765732\n",
      "Epoch 11105/30000 Training Loss: 0.06984161585569382\n",
      "Epoch 11106/30000 Training Loss: 0.039487097412347794\n",
      "Epoch 11107/30000 Training Loss: 0.05388425663113594\n",
      "Epoch 11108/30000 Training Loss: 0.0703989639878273\n",
      "Epoch 11109/30000 Training Loss: 0.043777868151664734\n",
      "Epoch 11110/30000 Training Loss: 0.03507830202579498\n",
      "Epoch 11111/30000 Training Loss: 0.05689626932144165\n",
      "Epoch 11112/30000 Training Loss: 0.05541563779115677\n",
      "Epoch 11113/30000 Training Loss: 0.0441509447991848\n",
      "Epoch 11114/30000 Training Loss: 0.05219805985689163\n",
      "Epoch 11115/30000 Training Loss: 0.04390450939536095\n",
      "Epoch 11116/30000 Training Loss: 0.04316624626517296\n",
      "Epoch 11117/30000 Training Loss: 0.0520014613866806\n",
      "Epoch 11118/30000 Training Loss: 0.0597076490521431\n",
      "Epoch 11119/30000 Training Loss: 0.057136643677949905\n",
      "Epoch 11120/30000 Training Loss: 0.06558732688426971\n",
      "Epoch 11121/30000 Training Loss: 0.052519891411066055\n",
      "Epoch 11122/30000 Training Loss: 0.04895859211683273\n",
      "Epoch 11123/30000 Training Loss: 0.056751497089862823\n",
      "Epoch 11124/30000 Training Loss: 0.0341520756483078\n",
      "Epoch 11125/30000 Training Loss: 0.06230391561985016\n",
      "Epoch 11126/30000 Training Loss: 0.05648333951830864\n",
      "Epoch 11127/30000 Training Loss: 0.051754701882600784\n",
      "Epoch 11128/30000 Training Loss: 0.07153170555830002\n",
      "Epoch 11129/30000 Training Loss: 0.06430085748434067\n",
      "Epoch 11130/30000 Training Loss: 0.053175460547208786\n",
      "Epoch 11131/30000 Training Loss: 0.056302644312381744\n",
      "Epoch 11132/30000 Training Loss: 0.07262462377548218\n",
      "Epoch 11133/30000 Training Loss: 0.06167566403746605\n",
      "Epoch 11134/30000 Training Loss: 0.04114939644932747\n",
      "Epoch 11135/30000 Training Loss: 0.04489733651280403\n",
      "Epoch 11136/30000 Training Loss: 0.06793846189975739\n",
      "Epoch 11137/30000 Training Loss: 0.03981044888496399\n",
      "Epoch 11138/30000 Training Loss: 0.058481425046920776\n",
      "Epoch 11139/30000 Training Loss: 0.062122978270053864\n",
      "Epoch 11140/30000 Training Loss: 0.07563053071498871\n",
      "Epoch 11141/30000 Training Loss: 0.055333640426397324\n",
      "Epoch 11142/30000 Training Loss: 0.04791108891367912\n",
      "Epoch 11143/30000 Training Loss: 0.04917130246758461\n",
      "Epoch 11144/30000 Training Loss: 0.06653367727994919\n",
      "Epoch 11145/30000 Training Loss: 0.06385831534862518\n",
      "Epoch 11146/30000 Training Loss: 0.05041363835334778\n",
      "Epoch 11147/30000 Training Loss: 0.04920408874750137\n",
      "Epoch 11148/30000 Training Loss: 0.041306741535663605\n",
      "Epoch 11149/30000 Training Loss: 0.06072545051574707\n",
      "Epoch 11150/30000 Training Loss: 0.060882918536663055\n",
      "Epoch 11151/30000 Training Loss: 0.07270234078168869\n",
      "Epoch 11152/30000 Training Loss: 0.045316603034734726\n",
      "Epoch 11153/30000 Training Loss: 0.06340757012367249\n",
      "Epoch 11154/30000 Training Loss: 0.0438404306769371\n",
      "Epoch 11155/30000 Training Loss: 0.06291404366493225\n",
      "Epoch 11156/30000 Training Loss: 0.048013169318437576\n",
      "Epoch 11157/30000 Training Loss: 0.0436033271253109\n",
      "Epoch 11158/30000 Training Loss: 0.06482765823602676\n",
      "Epoch 11159/30000 Training Loss: 0.06927905231714249\n",
      "Epoch 11160/30000 Training Loss: 0.0550040528178215\n",
      "Epoch 11161/30000 Training Loss: 0.04432874172925949\n",
      "Epoch 11162/30000 Training Loss: 0.053563192486763\n",
      "Epoch 11163/30000 Training Loss: 0.04550185054540634\n",
      "Epoch 11164/30000 Training Loss: 0.052756547927856445\n",
      "Epoch 11165/30000 Training Loss: 0.049683112651109695\n",
      "Epoch 11166/30000 Training Loss: 0.05867578834295273\n",
      "Epoch 11167/30000 Training Loss: 0.0478939563035965\n",
      "Epoch 11168/30000 Training Loss: 0.040175046771764755\n",
      "Epoch 11169/30000 Training Loss: 0.06346592307090759\n",
      "Epoch 11170/30000 Training Loss: 0.0530710369348526\n",
      "Epoch 11171/30000 Training Loss: 0.05972667038440704\n",
      "Epoch 11172/30000 Training Loss: 0.055309444665908813\n",
      "Epoch 11173/30000 Training Loss: 0.0484786257147789\n",
      "Epoch 11174/30000 Training Loss: 0.03930716589093208\n",
      "Epoch 11175/30000 Training Loss: 0.06053768843412399\n",
      "Epoch 11176/30000 Training Loss: 0.05430994927883148\n",
      "Epoch 11177/30000 Training Loss: 0.060677334666252136\n",
      "Epoch 11178/30000 Training Loss: 0.060358677059412\n",
      "Epoch 11179/30000 Training Loss: 0.06248745322227478\n",
      "Epoch 11180/30000 Training Loss: 0.046334922313690186\n",
      "Epoch 11181/30000 Training Loss: 0.045591458678245544\n",
      "Epoch 11182/30000 Training Loss: 0.07566268742084503\n",
      "Epoch 11183/30000 Training Loss: 0.040520571172237396\n",
      "Epoch 11184/30000 Training Loss: 0.041101403534412384\n",
      "Epoch 11185/30000 Training Loss: 0.05582618713378906\n",
      "Epoch 11186/30000 Training Loss: 0.04859368875622749\n",
      "Epoch 11187/30000 Training Loss: 0.04717164486646652\n",
      "Epoch 11188/30000 Training Loss: 0.05643392354249954\n",
      "Epoch 11189/30000 Training Loss: 0.039958447217941284\n",
      "Epoch 11190/30000 Training Loss: 0.0664515495300293\n",
      "Epoch 11191/30000 Training Loss: 0.06624337285757065\n",
      "Epoch 11192/30000 Training Loss: 0.0546535849571228\n",
      "Epoch 11193/30000 Training Loss: 0.06261242181062698\n",
      "Epoch 11194/30000 Training Loss: 0.04249979555606842\n",
      "Epoch 11195/30000 Training Loss: 0.05064409598708153\n",
      "Epoch 11196/30000 Training Loss: 0.044819723814725876\n",
      "Epoch 11197/30000 Training Loss: 0.03559648618102074\n",
      "Epoch 11198/30000 Training Loss: 0.05237310007214546\n",
      "Epoch 11199/30000 Training Loss: 0.04357440769672394\n",
      "Epoch 11200/30000 Training Loss: 0.046482231467962265\n",
      "Epoch 11200/30000 Validation Loss: 0.04915480315685272\n",
      "Epoch 11201/30000 Training Loss: 0.0320483036339283\n",
      "Epoch 11202/30000 Training Loss: 0.048629797995090485\n",
      "Epoch 11203/30000 Training Loss: 0.06883697211742401\n",
      "Epoch 11204/30000 Training Loss: 0.05937955155968666\n",
      "Epoch 11205/30000 Training Loss: 0.05793077498674393\n",
      "Epoch 11206/30000 Training Loss: 0.05455273017287254\n",
      "Epoch 11207/30000 Training Loss: 0.041163865476846695\n",
      "Epoch 11208/30000 Training Loss: 0.05013030767440796\n",
      "Epoch 11209/30000 Training Loss: 0.08875912427902222\n",
      "Epoch 11210/30000 Training Loss: 0.05099037289619446\n",
      "Epoch 11211/30000 Training Loss: 0.06886744499206543\n",
      "Epoch 11212/30000 Training Loss: 0.04797477647662163\n",
      "Epoch 11213/30000 Training Loss: 0.042241133749485016\n",
      "Epoch 11214/30000 Training Loss: 0.0538838654756546\n",
      "Epoch 11215/30000 Training Loss: 0.05138799548149109\n",
      "Epoch 11216/30000 Training Loss: 0.05616915225982666\n",
      "Epoch 11217/30000 Training Loss: 0.06114308536052704\n",
      "Epoch 11218/30000 Training Loss: 0.04858618229627609\n",
      "Epoch 11219/30000 Training Loss: 0.04638461023569107\n",
      "Epoch 11220/30000 Training Loss: 0.04516802728176117\n",
      "Epoch 11221/30000 Training Loss: 0.058437127619981766\n",
      "Epoch 11222/30000 Training Loss: 0.07334545999765396\n",
      "Epoch 11223/30000 Training Loss: 0.05902746319770813\n",
      "Epoch 11224/30000 Training Loss: 0.04862547665834427\n",
      "Epoch 11225/30000 Training Loss: 0.058438703417778015\n",
      "Epoch 11226/30000 Training Loss: 0.04992605373263359\n",
      "Epoch 11227/30000 Training Loss: 0.04660898819565773\n",
      "Epoch 11228/30000 Training Loss: 0.05628908798098564\n",
      "Epoch 11229/30000 Training Loss: 0.05830777436494827\n",
      "Epoch 11230/30000 Training Loss: 0.04399167373776436\n",
      "Epoch 11231/30000 Training Loss: 0.04906060919165611\n",
      "Epoch 11232/30000 Training Loss: 0.047111548483371735\n",
      "Epoch 11233/30000 Training Loss: 0.06787547469139099\n",
      "Epoch 11234/30000 Training Loss: 0.04659126326441765\n",
      "Epoch 11235/30000 Training Loss: 0.04600707069039345\n",
      "Epoch 11236/30000 Training Loss: 0.06205781549215317\n",
      "Epoch 11237/30000 Training Loss: 0.0611233115196228\n",
      "Epoch 11238/30000 Training Loss: 0.05303173512220383\n",
      "Epoch 11239/30000 Training Loss: 0.049179159104824066\n",
      "Epoch 11240/30000 Training Loss: 0.04532606154680252\n",
      "Epoch 11241/30000 Training Loss: 0.044040828943252563\n",
      "Epoch 11242/30000 Training Loss: 0.06060023605823517\n",
      "Epoch 11243/30000 Training Loss: 0.049735307693481445\n",
      "Epoch 11244/30000 Training Loss: 0.041469499468803406\n",
      "Epoch 11245/30000 Training Loss: 0.07637656480073929\n",
      "Epoch 11246/30000 Training Loss: 0.04971584677696228\n",
      "Epoch 11247/30000 Training Loss: 0.04651530832052231\n",
      "Epoch 11248/30000 Training Loss: 0.05440196394920349\n",
      "Epoch 11249/30000 Training Loss: 0.04972057417035103\n",
      "Epoch 11250/30000 Training Loss: 0.06041225045919418\n",
      "Epoch 11251/30000 Training Loss: 0.045605968683958054\n",
      "Epoch 11252/30000 Training Loss: 0.055654190480709076\n",
      "Epoch 11253/30000 Training Loss: 0.04793533310294151\n",
      "Epoch 11254/30000 Training Loss: 0.053333885967731476\n",
      "Epoch 11255/30000 Training Loss: 0.04723362624645233\n",
      "Epoch 11256/30000 Training Loss: 0.06596539914608002\n",
      "Epoch 11257/30000 Training Loss: 0.05918261408805847\n",
      "Epoch 11258/30000 Training Loss: 0.06913162022829056\n",
      "Epoch 11259/30000 Training Loss: 0.059237703680992126\n",
      "Epoch 11260/30000 Training Loss: 0.05126078426837921\n",
      "Epoch 11261/30000 Training Loss: 0.06057575345039368\n",
      "Epoch 11262/30000 Training Loss: 0.051427531987428665\n",
      "Epoch 11263/30000 Training Loss: 0.060500022023916245\n",
      "Epoch 11264/30000 Training Loss: 0.0527852438390255\n",
      "Epoch 11265/30000 Training Loss: 0.05644949898123741\n",
      "Epoch 11266/30000 Training Loss: 0.043772563338279724\n",
      "Epoch 11267/30000 Training Loss: 0.0533529594540596\n",
      "Epoch 11268/30000 Training Loss: 0.047544095665216446\n",
      "Epoch 11269/30000 Training Loss: 0.07026638835668564\n",
      "Epoch 11270/30000 Training Loss: 0.05817054957151413\n",
      "Epoch 11271/30000 Training Loss: 0.045238181948661804\n",
      "Epoch 11272/30000 Training Loss: 0.03631389141082764\n",
      "Epoch 11273/30000 Training Loss: 0.04190404340624809\n",
      "Epoch 11274/30000 Training Loss: 0.04017523303627968\n",
      "Epoch 11275/30000 Training Loss: 0.060616008937358856\n",
      "Epoch 11276/30000 Training Loss: 0.04187895357608795\n",
      "Epoch 11277/30000 Training Loss: 0.05430307239294052\n",
      "Epoch 11278/30000 Training Loss: 0.03688392788171768\n",
      "Epoch 11279/30000 Training Loss: 0.04779879003763199\n",
      "Epoch 11280/30000 Training Loss: 0.05150990188121796\n",
      "Epoch 11281/30000 Training Loss: 0.05478159710764885\n",
      "Epoch 11282/30000 Training Loss: 0.05509049445390701\n",
      "Epoch 11283/30000 Training Loss: 0.04938960820436478\n",
      "Epoch 11284/30000 Training Loss: 0.05963146686553955\n",
      "Epoch 11285/30000 Training Loss: 0.06941244006156921\n",
      "Epoch 11286/30000 Training Loss: 0.0534021332859993\n",
      "Epoch 11287/30000 Training Loss: 0.042689982801675797\n",
      "Epoch 11288/30000 Training Loss: 0.04430531710386276\n",
      "Epoch 11289/30000 Training Loss: 0.07027426362037659\n",
      "Epoch 11290/30000 Training Loss: 0.05610710754990578\n",
      "Epoch 11291/30000 Training Loss: 0.04430196061730385\n",
      "Epoch 11292/30000 Training Loss: 0.05212368443608284\n",
      "Epoch 11293/30000 Training Loss: 0.05035831034183502\n",
      "Epoch 11294/30000 Training Loss: 0.04349328577518463\n",
      "Epoch 11295/30000 Training Loss: 0.03684417903423309\n",
      "Epoch 11296/30000 Training Loss: 0.0363176129758358\n",
      "Epoch 11297/30000 Training Loss: 0.059826720505952835\n",
      "Epoch 11298/30000 Training Loss: 0.06114771217107773\n",
      "Epoch 11299/30000 Training Loss: 0.06579536199569702\n",
      "Epoch 11300/30000 Training Loss: 0.06250862032175064\n",
      "Epoch 11300/30000 Validation Loss: 0.047650180757045746\n",
      "Epoch 11301/30000 Training Loss: 0.051819104701280594\n",
      "Epoch 11302/30000 Training Loss: 0.05873197317123413\n",
      "Epoch 11303/30000 Training Loss: 0.07199916988611221\n",
      "Epoch 11304/30000 Training Loss: 0.05232078582048416\n",
      "Epoch 11305/30000 Training Loss: 0.04001079499721527\n",
      "Epoch 11306/30000 Training Loss: 0.05754813179373741\n",
      "Epoch 11307/30000 Training Loss: 0.05115887150168419\n",
      "Epoch 11308/30000 Training Loss: 0.05415242165327072\n",
      "Epoch 11309/30000 Training Loss: 0.051127150654792786\n",
      "Epoch 11310/30000 Training Loss: 0.03829502314329147\n",
      "Epoch 11311/30000 Training Loss: 0.0571785606443882\n",
      "Epoch 11312/30000 Training Loss: 0.05738350749015808\n",
      "Epoch 11313/30000 Training Loss: 0.057358600199222565\n",
      "Epoch 11314/30000 Training Loss: 0.0509188249707222\n",
      "Epoch 11315/30000 Training Loss: 0.03636790066957474\n",
      "Epoch 11316/30000 Training Loss: 0.07149982452392578\n",
      "Epoch 11317/30000 Training Loss: 0.05829748138785362\n",
      "Epoch 11318/30000 Training Loss: 0.04056653752923012\n",
      "Epoch 11319/30000 Training Loss: 0.04132911562919617\n",
      "Epoch 11320/30000 Training Loss: 0.0432492271065712\n",
      "Epoch 11321/30000 Training Loss: 0.05295800417661667\n",
      "Epoch 11322/30000 Training Loss: 0.06525551527738571\n",
      "Epoch 11323/30000 Training Loss: 0.06274288147687912\n",
      "Epoch 11324/30000 Training Loss: 0.05265599116683006\n",
      "Epoch 11325/30000 Training Loss: 0.04391735792160034\n",
      "Epoch 11326/30000 Training Loss: 0.04320156201720238\n",
      "Epoch 11327/30000 Training Loss: 0.07702019065618515\n",
      "Epoch 11328/30000 Training Loss: 0.0411841943860054\n",
      "Epoch 11329/30000 Training Loss: 0.05967763066291809\n",
      "Epoch 11330/30000 Training Loss: 0.04201429337263107\n",
      "Epoch 11331/30000 Training Loss: 0.05599652975797653\n",
      "Epoch 11332/30000 Training Loss: 0.04231816157698631\n",
      "Epoch 11333/30000 Training Loss: 0.05690876021981239\n",
      "Epoch 11334/30000 Training Loss: 0.041560351848602295\n",
      "Epoch 11335/30000 Training Loss: 0.053900498896837234\n",
      "Epoch 11336/30000 Training Loss: 0.05072952061891556\n",
      "Epoch 11337/30000 Training Loss: 0.05709616467356682\n",
      "Epoch 11338/30000 Training Loss: 0.058394983410835266\n",
      "Epoch 11339/30000 Training Loss: 0.04805992171168327\n",
      "Epoch 11340/30000 Training Loss: 0.04194846749305725\n",
      "Epoch 11341/30000 Training Loss: 0.039450839161872864\n",
      "Epoch 11342/30000 Training Loss: 0.05469266697764397\n",
      "Epoch 11343/30000 Training Loss: 0.044501423835754395\n",
      "Epoch 11344/30000 Training Loss: 0.061369892209768295\n",
      "Epoch 11345/30000 Training Loss: 0.040222302079200745\n",
      "Epoch 11346/30000 Training Loss: 0.04786869138479233\n",
      "Epoch 11347/30000 Training Loss: 0.04100523889064789\n",
      "Epoch 11348/30000 Training Loss: 0.04027743637561798\n",
      "Epoch 11349/30000 Training Loss: 0.04921063035726547\n",
      "Epoch 11350/30000 Training Loss: 0.049774955958127975\n",
      "Epoch 11351/30000 Training Loss: 0.05618543177843094\n",
      "Epoch 11352/30000 Training Loss: 0.039239224046468735\n",
      "Epoch 11353/30000 Training Loss: 0.05165175721049309\n",
      "Epoch 11354/30000 Training Loss: 0.04628481715917587\n",
      "Epoch 11355/30000 Training Loss: 0.06000712886452675\n",
      "Epoch 11356/30000 Training Loss: 0.07102179527282715\n",
      "Epoch 11357/30000 Training Loss: 0.045454561710357666\n",
      "Epoch 11358/30000 Training Loss: 0.04937214404344559\n",
      "Epoch 11359/30000 Training Loss: 0.058252833783626556\n",
      "Epoch 11360/30000 Training Loss: 0.05064826458692551\n",
      "Epoch 11361/30000 Training Loss: 0.05202796310186386\n",
      "Epoch 11362/30000 Training Loss: 0.04796046391129494\n",
      "Epoch 11363/30000 Training Loss: 0.03967897593975067\n",
      "Epoch 11364/30000 Training Loss: 0.04337749630212784\n",
      "Epoch 11365/30000 Training Loss: 0.04579203948378563\n",
      "Epoch 11366/30000 Training Loss: 0.03581847622990608\n",
      "Epoch 11367/30000 Training Loss: 0.06806290149688721\n",
      "Epoch 11368/30000 Training Loss: 0.062264181673526764\n",
      "Epoch 11369/30000 Training Loss: 0.07769954204559326\n",
      "Epoch 11370/30000 Training Loss: 0.05161752179265022\n",
      "Epoch 11371/30000 Training Loss: 0.058213260024785995\n",
      "Epoch 11372/30000 Training Loss: 0.04740222543478012\n",
      "Epoch 11373/30000 Training Loss: 0.06560817360877991\n",
      "Epoch 11374/30000 Training Loss: 0.057385675609111786\n",
      "Epoch 11375/30000 Training Loss: 0.05389977991580963\n",
      "Epoch 11376/30000 Training Loss: 0.048339273780584335\n",
      "Epoch 11377/30000 Training Loss: 0.043730005621910095\n",
      "Epoch 11378/30000 Training Loss: 0.05093232914805412\n",
      "Epoch 11379/30000 Training Loss: 0.04887312650680542\n",
      "Epoch 11380/30000 Training Loss: 0.05537138506770134\n",
      "Epoch 11381/30000 Training Loss: 0.05939757078886032\n",
      "Epoch 11382/30000 Training Loss: 0.05648161470890045\n",
      "Epoch 11383/30000 Training Loss: 0.06487512588500977\n",
      "Epoch 11384/30000 Training Loss: 0.06258945912122726\n",
      "Epoch 11385/30000 Training Loss: 0.049443043768405914\n",
      "Epoch 11386/30000 Training Loss: 0.04559234902262688\n",
      "Epoch 11387/30000 Training Loss: 0.046645984053611755\n",
      "Epoch 11388/30000 Training Loss: 0.038791801780462265\n",
      "Epoch 11389/30000 Training Loss: 0.05922731012105942\n",
      "Epoch 11390/30000 Training Loss: 0.07548514008522034\n",
      "Epoch 11391/30000 Training Loss: 0.045716412365436554\n",
      "Epoch 11392/30000 Training Loss: 0.051305972039699554\n",
      "Epoch 11393/30000 Training Loss: 0.05231551453471184\n",
      "Epoch 11394/30000 Training Loss: 0.04810377582907677\n",
      "Epoch 11395/30000 Training Loss: 0.06525088101625443\n",
      "Epoch 11396/30000 Training Loss: 0.04979614540934563\n",
      "Epoch 11397/30000 Training Loss: 0.05354956537485123\n",
      "Epoch 11398/30000 Training Loss: 0.03422241657972336\n",
      "Epoch 11399/30000 Training Loss: 0.05006033554673195\n",
      "Epoch 11400/30000 Training Loss: 0.05391233041882515\n",
      "Epoch 11400/30000 Validation Loss: 0.05400056764483452\n",
      "Epoch 11401/30000 Training Loss: 0.046264417469501495\n",
      "Epoch 11402/30000 Training Loss: 0.05183572322130203\n",
      "Epoch 11403/30000 Training Loss: 0.045232806354761124\n",
      "Epoch 11404/30000 Training Loss: 0.04377627745270729\n",
      "Epoch 11405/30000 Training Loss: 0.06303219497203827\n",
      "Epoch 11406/30000 Training Loss: 0.05340450629591942\n",
      "Epoch 11407/30000 Training Loss: 0.044429928064346313\n",
      "Epoch 11408/30000 Training Loss: 0.048023782670497894\n",
      "Epoch 11409/30000 Training Loss: 0.06089571863412857\n",
      "Epoch 11410/30000 Training Loss: 0.0471477173268795\n",
      "Epoch 11411/30000 Training Loss: 0.03885248675942421\n",
      "Epoch 11412/30000 Training Loss: 0.040725309401750565\n",
      "Epoch 11413/30000 Training Loss: 0.05159933120012283\n",
      "Epoch 11414/30000 Training Loss: 0.045076802372932434\n",
      "Epoch 11415/30000 Training Loss: 0.06131695955991745\n",
      "Epoch 11416/30000 Training Loss: 0.06528821587562561\n",
      "Epoch 11417/30000 Training Loss: 0.04867077246308327\n",
      "Epoch 11418/30000 Training Loss: 0.04024752601981163\n",
      "Epoch 11419/30000 Training Loss: 0.048780813813209534\n",
      "Epoch 11420/30000 Training Loss: 0.05086125433444977\n",
      "Epoch 11421/30000 Training Loss: 0.07467464357614517\n",
      "Epoch 11422/30000 Training Loss: 0.03689778596162796\n",
      "Epoch 11423/30000 Training Loss: 0.05213337019085884\n",
      "Epoch 11424/30000 Training Loss: 0.04672589153051376\n",
      "Epoch 11425/30000 Training Loss: 0.049963995814323425\n",
      "Epoch 11426/30000 Training Loss: 0.044953685253858566\n",
      "Epoch 11427/30000 Training Loss: 0.06174112483859062\n",
      "Epoch 11428/30000 Training Loss: 0.04631766676902771\n",
      "Epoch 11429/30000 Training Loss: 0.04869942367076874\n",
      "Epoch 11430/30000 Training Loss: 0.04916081950068474\n",
      "Epoch 11431/30000 Training Loss: 0.07752616703510284\n",
      "Epoch 11432/30000 Training Loss: 0.054435815662145615\n",
      "Epoch 11433/30000 Training Loss: 0.04706300050020218\n",
      "Epoch 11434/30000 Training Loss: 0.048364490270614624\n",
      "Epoch 11435/30000 Training Loss: 0.05405396223068237\n",
      "Epoch 11436/30000 Training Loss: 0.05626452714204788\n",
      "Epoch 11437/30000 Training Loss: 0.060126401484012604\n",
      "Epoch 11438/30000 Training Loss: 0.051935795694589615\n",
      "Epoch 11439/30000 Training Loss: 0.06093395873904228\n",
      "Epoch 11440/30000 Training Loss: 0.05151475593447685\n",
      "Epoch 11441/30000 Training Loss: 0.07177528738975525\n",
      "Epoch 11442/30000 Training Loss: 0.04708343744277954\n",
      "Epoch 11443/30000 Training Loss: 0.06486120820045471\n",
      "Epoch 11444/30000 Training Loss: 0.05102702975273132\n",
      "Epoch 11445/30000 Training Loss: 0.05201816186308861\n",
      "Epoch 11446/30000 Training Loss: 0.044933877885341644\n",
      "Epoch 11447/30000 Training Loss: 0.05313890427350998\n",
      "Epoch 11448/30000 Training Loss: 0.040882859379053116\n",
      "Epoch 11449/30000 Training Loss: 0.039737921208143234\n",
      "Epoch 11450/30000 Training Loss: 0.05157224088907242\n",
      "Epoch 11451/30000 Training Loss: 0.05347253382205963\n",
      "Epoch 11452/30000 Training Loss: 0.04761640354990959\n",
      "Epoch 11453/30000 Training Loss: 0.03934497386217117\n",
      "Epoch 11454/30000 Training Loss: 0.04739692807197571\n",
      "Epoch 11455/30000 Training Loss: 0.058914948254823685\n",
      "Epoch 11456/30000 Training Loss: 0.04721173644065857\n",
      "Epoch 11457/30000 Training Loss: 0.05394222214818001\n",
      "Epoch 11458/30000 Training Loss: 0.05140645056962967\n",
      "Epoch 11459/30000 Training Loss: 0.04442683607339859\n",
      "Epoch 11460/30000 Training Loss: 0.04107477515935898\n",
      "Epoch 11461/30000 Training Loss: 0.05952867865562439\n",
      "Epoch 11462/30000 Training Loss: 0.06953233480453491\n",
      "Epoch 11463/30000 Training Loss: 0.039498988538980484\n",
      "Epoch 11464/30000 Training Loss: 0.06153269484639168\n",
      "Epoch 11465/30000 Training Loss: 0.06302694231271744\n",
      "Epoch 11466/30000 Training Loss: 0.061472728848457336\n",
      "Epoch 11467/30000 Training Loss: 0.04667834937572479\n",
      "Epoch 11468/30000 Training Loss: 0.06973999738693237\n",
      "Epoch 11469/30000 Training Loss: 0.04448103532195091\n",
      "Epoch 11470/30000 Training Loss: 0.05589982122182846\n",
      "Epoch 11471/30000 Training Loss: 0.05845499038696289\n",
      "Epoch 11472/30000 Training Loss: 0.04430505633354187\n",
      "Epoch 11473/30000 Training Loss: 0.05342338979244232\n",
      "Epoch 11474/30000 Training Loss: 0.06633297353982925\n",
      "Epoch 11475/30000 Training Loss: 0.04490259289741516\n",
      "Epoch 11476/30000 Training Loss: 0.06971153616905212\n",
      "Epoch 11477/30000 Training Loss: 0.05140450596809387\n",
      "Epoch 11478/30000 Training Loss: 0.055595964193344116\n",
      "Epoch 11479/30000 Training Loss: 0.05491817370057106\n",
      "Epoch 11480/30000 Training Loss: 0.06277589499950409\n",
      "Epoch 11481/30000 Training Loss: 0.05948452651500702\n",
      "Epoch 11482/30000 Training Loss: 0.07181747257709503\n",
      "Epoch 11483/30000 Training Loss: 0.04213275760412216\n",
      "Epoch 11484/30000 Training Loss: 0.0546126514673233\n",
      "Epoch 11485/30000 Training Loss: 0.056164756417274475\n",
      "Epoch 11486/30000 Training Loss: 0.05991224944591522\n",
      "Epoch 11487/30000 Training Loss: 0.05856707692146301\n",
      "Epoch 11488/30000 Training Loss: 0.03976881504058838\n",
      "Epoch 11489/30000 Training Loss: 0.03966158628463745\n",
      "Epoch 11490/30000 Training Loss: 0.07062755525112152\n",
      "Epoch 11491/30000 Training Loss: 0.061865657567977905\n",
      "Epoch 11492/30000 Training Loss: 0.04942002147436142\n",
      "Epoch 11493/30000 Training Loss: 0.06259936094284058\n",
      "Epoch 11494/30000 Training Loss: 0.03798646479845047\n",
      "Epoch 11495/30000 Training Loss: 0.03879237920045853\n",
      "Epoch 11496/30000 Training Loss: 0.03525393456220627\n",
      "Epoch 11497/30000 Training Loss: 0.05054040625691414\n",
      "Epoch 11498/30000 Training Loss: 0.049944180995225906\n",
      "Epoch 11499/30000 Training Loss: 0.05119744688272476\n",
      "Epoch 11500/30000 Training Loss: 0.046914197504520416\n",
      "Epoch 11500/30000 Validation Loss: 0.06324099004268646\n",
      "Epoch 11501/30000 Training Loss: 0.038309190422296524\n",
      "Epoch 11502/30000 Training Loss: 0.045956507325172424\n",
      "Epoch 11503/30000 Training Loss: 0.06954383105039597\n",
      "Epoch 11504/30000 Training Loss: 0.05207869037985802\n",
      "Epoch 11505/30000 Training Loss: 0.051321908831596375\n",
      "Epoch 11506/30000 Training Loss: 0.03829578682780266\n",
      "Epoch 11507/30000 Training Loss: 0.04544750601053238\n",
      "Epoch 11508/30000 Training Loss: 0.05057065933942795\n",
      "Epoch 11509/30000 Training Loss: 0.0667891651391983\n",
      "Epoch 11510/30000 Training Loss: 0.05452626571059227\n",
      "Epoch 11511/30000 Training Loss: 0.047684989869594574\n",
      "Epoch 11512/30000 Training Loss: 0.0497775636613369\n",
      "Epoch 11513/30000 Training Loss: 0.043479010462760925\n",
      "Epoch 11514/30000 Training Loss: 0.04952692985534668\n",
      "Epoch 11515/30000 Training Loss: 0.05516031011939049\n",
      "Epoch 11516/30000 Training Loss: 0.051788993179798126\n",
      "Epoch 11517/30000 Training Loss: 0.05411509796977043\n",
      "Epoch 11518/30000 Training Loss: 0.04637297987937927\n",
      "Epoch 11519/30000 Training Loss: 0.05436554178595543\n",
      "Epoch 11520/30000 Training Loss: 0.0512157566845417\n",
      "Epoch 11521/30000 Training Loss: 0.05350738763809204\n",
      "Epoch 11522/30000 Training Loss: 0.04357825964689255\n",
      "Epoch 11523/30000 Training Loss: 0.0518677681684494\n",
      "Epoch 11524/30000 Training Loss: 0.0460425466299057\n",
      "Epoch 11525/30000 Training Loss: 0.0645674616098404\n",
      "Epoch 11526/30000 Training Loss: 0.059605587273836136\n",
      "Epoch 11527/30000 Training Loss: 0.06630205363035202\n",
      "Epoch 11528/30000 Training Loss: 0.05123750492930412\n",
      "Epoch 11529/30000 Training Loss: 0.04903973639011383\n",
      "Epoch 11530/30000 Training Loss: 0.045344870537519455\n",
      "Epoch 11531/30000 Training Loss: 0.04600437358021736\n",
      "Epoch 11532/30000 Training Loss: 0.05248458310961723\n",
      "Epoch 11533/30000 Training Loss: 0.041505366563797\n",
      "Epoch 11534/30000 Training Loss: 0.05055731534957886\n",
      "Epoch 11535/30000 Training Loss: 0.07086052000522614\n",
      "Epoch 11536/30000 Training Loss: 0.06925217807292938\n",
      "Epoch 11537/30000 Training Loss: 0.06704326719045639\n",
      "Epoch 11538/30000 Training Loss: 0.06334180384874344\n",
      "Epoch 11539/30000 Training Loss: 0.07409881055355072\n",
      "Epoch 11540/30000 Training Loss: 0.06546729803085327\n",
      "Epoch 11541/30000 Training Loss: 0.05109017714858055\n",
      "Epoch 11542/30000 Training Loss: 0.057751309126615524\n",
      "Epoch 11543/30000 Training Loss: 0.05378160625696182\n",
      "Epoch 11544/30000 Training Loss: 0.05231475085020065\n",
      "Epoch 11545/30000 Training Loss: 0.05500931292772293\n",
      "Epoch 11546/30000 Training Loss: 0.047630421817302704\n",
      "Epoch 11547/30000 Training Loss: 0.04150736331939697\n",
      "Epoch 11548/30000 Training Loss: 0.07962178438901901\n",
      "Epoch 11549/30000 Training Loss: 0.03879270330071449\n",
      "Epoch 11550/30000 Training Loss: 0.0635230615735054\n",
      "Epoch 11551/30000 Training Loss: 0.052622806280851364\n",
      "Epoch 11552/30000 Training Loss: 0.05185459554195404\n",
      "Epoch 11553/30000 Training Loss: 0.062185514718294144\n",
      "Epoch 11554/30000 Training Loss: 0.05452597513794899\n",
      "Epoch 11555/30000 Training Loss: 0.06415024399757385\n",
      "Epoch 11556/30000 Training Loss: 0.0542265921831131\n",
      "Epoch 11557/30000 Training Loss: 0.04067900776863098\n",
      "Epoch 11558/30000 Training Loss: 0.04531344026327133\n",
      "Epoch 11559/30000 Training Loss: 0.06282682716846466\n",
      "Epoch 11560/30000 Training Loss: 0.05607793480157852\n",
      "Epoch 11561/30000 Training Loss: 0.047599636018276215\n",
      "Epoch 11562/30000 Training Loss: 0.05898948013782501\n",
      "Epoch 11563/30000 Training Loss: 0.05638507753610611\n",
      "Epoch 11564/30000 Training Loss: 0.055392101407051086\n",
      "Epoch 11565/30000 Training Loss: 0.04637054726481438\n",
      "Epoch 11566/30000 Training Loss: 0.045132897794246674\n",
      "Epoch 11567/30000 Training Loss: 0.04026573523879051\n",
      "Epoch 11568/30000 Training Loss: 0.052039243280887604\n",
      "Epoch 11569/30000 Training Loss: 0.047669216990470886\n",
      "Epoch 11570/30000 Training Loss: 0.03504094108939171\n",
      "Epoch 11571/30000 Training Loss: 0.05218867212533951\n",
      "Epoch 11572/30000 Training Loss: 0.06380296498537064\n",
      "Epoch 11573/30000 Training Loss: 0.05146083980798721\n",
      "Epoch 11574/30000 Training Loss: 0.055305786430835724\n",
      "Epoch 11575/30000 Training Loss: 0.04891010373830795\n",
      "Epoch 11576/30000 Training Loss: 0.047251470386981964\n",
      "Epoch 11577/30000 Training Loss: 0.045938655734062195\n",
      "Epoch 11578/30000 Training Loss: 0.052930597215890884\n",
      "Epoch 11579/30000 Training Loss: 0.05403202027082443\n",
      "Epoch 11580/30000 Training Loss: 0.054511308670043945\n",
      "Epoch 11581/30000 Training Loss: 0.045891132205724716\n",
      "Epoch 11582/30000 Training Loss: 0.06576840579509735\n",
      "Epoch 11583/30000 Training Loss: 0.06688586622476578\n",
      "Epoch 11584/30000 Training Loss: 0.04000357910990715\n",
      "Epoch 11585/30000 Training Loss: 0.043634045869112015\n",
      "Epoch 11586/30000 Training Loss: 0.06336846947669983\n",
      "Epoch 11587/30000 Training Loss: 0.05187522992491722\n",
      "Epoch 11588/30000 Training Loss: 0.051355842500925064\n",
      "Epoch 11589/30000 Training Loss: 0.053053706884384155\n",
      "Epoch 11590/30000 Training Loss: 0.04817327857017517\n",
      "Epoch 11591/30000 Training Loss: 0.06553583592176437\n",
      "Epoch 11592/30000 Training Loss: 0.0696001648902893\n",
      "Epoch 11593/30000 Training Loss: 0.05825916305184364\n",
      "Epoch 11594/30000 Training Loss: 0.05813433229923248\n",
      "Epoch 11595/30000 Training Loss: 0.04855906590819359\n",
      "Epoch 11596/30000 Training Loss: 0.058424632996320724\n",
      "Epoch 11597/30000 Training Loss: 0.05456987023353577\n",
      "Epoch 11598/30000 Training Loss: 0.058534394949674606\n",
      "Epoch 11599/30000 Training Loss: 0.040970779955387115\n",
      "Epoch 11600/30000 Training Loss: 0.044138893485069275\n",
      "Epoch 11600/30000 Validation Loss: 0.048338428139686584\n",
      "Epoch 11601/30000 Training Loss: 0.03345052897930145\n",
      "Epoch 11602/30000 Training Loss: 0.07223252952098846\n",
      "Epoch 11603/30000 Training Loss: 0.046619005501270294\n",
      "Epoch 11604/30000 Training Loss: 0.0527302548289299\n",
      "Epoch 11605/30000 Training Loss: 0.04994739219546318\n",
      "Epoch 11606/30000 Training Loss: 0.059541042894124985\n",
      "Epoch 11607/30000 Training Loss: 0.040107857435941696\n",
      "Epoch 11608/30000 Training Loss: 0.06216876208782196\n",
      "Epoch 11609/30000 Training Loss: 0.06069476529955864\n",
      "Epoch 11610/30000 Training Loss: 0.03905221447348595\n",
      "Epoch 11611/30000 Training Loss: 0.050598181784152985\n",
      "Epoch 11612/30000 Training Loss: 0.049346230924129486\n",
      "Epoch 11613/30000 Training Loss: 0.06014418974518776\n",
      "Epoch 11614/30000 Training Loss: 0.04580816626548767\n",
      "Epoch 11615/30000 Training Loss: 0.05389946699142456\n",
      "Epoch 11616/30000 Training Loss: 0.04862871393561363\n",
      "Epoch 11617/30000 Training Loss: 0.04784928634762764\n",
      "Epoch 11618/30000 Training Loss: 0.05800432339310646\n",
      "Epoch 11619/30000 Training Loss: 0.05440382659435272\n",
      "Epoch 11620/30000 Training Loss: 0.0556844063103199\n",
      "Epoch 11621/30000 Training Loss: 0.04314536601305008\n",
      "Epoch 11622/30000 Training Loss: 0.03622012957930565\n",
      "Epoch 11623/30000 Training Loss: 0.047285743057727814\n",
      "Epoch 11624/30000 Training Loss: 0.05635394901037216\n",
      "Epoch 11625/30000 Training Loss: 0.059229686856269836\n",
      "Epoch 11626/30000 Training Loss: 0.04858378693461418\n",
      "Epoch 11627/30000 Training Loss: 0.05769906938076019\n",
      "Epoch 11628/30000 Training Loss: 0.04612154886126518\n",
      "Epoch 11629/30000 Training Loss: 0.06427939236164093\n",
      "Epoch 11630/30000 Training Loss: 0.05531229451298714\n",
      "Epoch 11631/30000 Training Loss: 0.05473647639155388\n",
      "Epoch 11632/30000 Training Loss: 0.03828240558505058\n",
      "Epoch 11633/30000 Training Loss: 0.041280340403318405\n",
      "Epoch 11634/30000 Training Loss: 0.06071726977825165\n",
      "Epoch 11635/30000 Training Loss: 0.036440152674913406\n",
      "Epoch 11636/30000 Training Loss: 0.06588007509708405\n",
      "Epoch 11637/30000 Training Loss: 0.060638509690761566\n",
      "Epoch 11638/30000 Training Loss: 0.047340020537376404\n",
      "Epoch 11639/30000 Training Loss: 0.0623353086411953\n",
      "Epoch 11640/30000 Training Loss: 0.04567072540521622\n",
      "Epoch 11641/30000 Training Loss: 0.038961078971624374\n",
      "Epoch 11642/30000 Training Loss: 0.059082821011543274\n",
      "Epoch 11643/30000 Training Loss: 0.053608521819114685\n",
      "Epoch 11644/30000 Training Loss: 0.0573355071246624\n",
      "Epoch 11645/30000 Training Loss: 0.038585904985666275\n",
      "Epoch 11646/30000 Training Loss: 0.040477704256772995\n",
      "Epoch 11647/30000 Training Loss: 0.0575556680560112\n",
      "Epoch 11648/30000 Training Loss: 0.047754038125276566\n",
      "Epoch 11649/30000 Training Loss: 0.06344780325889587\n",
      "Epoch 11650/30000 Training Loss: 0.051311030983924866\n",
      "Epoch 11651/30000 Training Loss: 0.052623357623815536\n",
      "Epoch 11652/30000 Training Loss: 0.04754482954740524\n",
      "Epoch 11653/30000 Training Loss: 0.04379739239811897\n",
      "Epoch 11654/30000 Training Loss: 0.0495237372815609\n",
      "Epoch 11655/30000 Training Loss: 0.038478776812553406\n",
      "Epoch 11656/30000 Training Loss: 0.0638434961438179\n",
      "Epoch 11657/30000 Training Loss: 0.04415372386574745\n",
      "Epoch 11658/30000 Training Loss: 0.054088227450847626\n",
      "Epoch 11659/30000 Training Loss: 0.07318253815174103\n",
      "Epoch 11660/30000 Training Loss: 0.054766736924648285\n",
      "Epoch 11661/30000 Training Loss: 0.0556376688182354\n",
      "Epoch 11662/30000 Training Loss: 0.05928324908018112\n",
      "Epoch 11663/30000 Training Loss: 0.05089697986841202\n",
      "Epoch 11664/30000 Training Loss: 0.05292802304029465\n",
      "Epoch 11665/30000 Training Loss: 0.05316636711359024\n",
      "Epoch 11666/30000 Training Loss: 0.047340817749500275\n",
      "Epoch 11667/30000 Training Loss: 0.0514070987701416\n",
      "Epoch 11668/30000 Training Loss: 0.04476026073098183\n",
      "Epoch 11669/30000 Training Loss: 0.0699828639626503\n",
      "Epoch 11670/30000 Training Loss: 0.038029756397008896\n",
      "Epoch 11671/30000 Training Loss: 0.04766809940338135\n",
      "Epoch 11672/30000 Training Loss: 0.07546588778495789\n",
      "Epoch 11673/30000 Training Loss: 0.06333663314580917\n",
      "Epoch 11674/30000 Training Loss: 0.052185818552970886\n",
      "Epoch 11675/30000 Training Loss: 0.058510664850473404\n",
      "Epoch 11676/30000 Training Loss: 0.05651329830288887\n",
      "Epoch 11677/30000 Training Loss: 0.05290206894278526\n",
      "Epoch 11678/30000 Training Loss: 0.053785353899002075\n",
      "Epoch 11679/30000 Training Loss: 0.05143391713500023\n",
      "Epoch 11680/30000 Training Loss: 0.06574565917253494\n",
      "Epoch 11681/30000 Training Loss: 0.04263982176780701\n",
      "Epoch 11682/30000 Training Loss: 0.045579735189676285\n",
      "Epoch 11683/30000 Training Loss: 0.0583517961204052\n",
      "Epoch 11684/30000 Training Loss: 0.045551907271146774\n",
      "Epoch 11685/30000 Training Loss: 0.0460483580827713\n",
      "Epoch 11686/30000 Training Loss: 0.07260299474000931\n",
      "Epoch 11687/30000 Training Loss: 0.0551668219268322\n",
      "Epoch 11688/30000 Training Loss: 0.04070818051695824\n",
      "Epoch 11689/30000 Training Loss: 0.044978782534599304\n",
      "Epoch 11690/30000 Training Loss: 0.04157333821058273\n",
      "Epoch 11691/30000 Training Loss: 0.04002225399017334\n",
      "Epoch 11692/30000 Training Loss: 0.0491863414645195\n",
      "Epoch 11693/30000 Training Loss: 0.050386540591716766\n",
      "Epoch 11694/30000 Training Loss: 0.048413388431072235\n",
      "Epoch 11695/30000 Training Loss: 0.06805630028247833\n",
      "Epoch 11696/30000 Training Loss: 0.05161232501268387\n",
      "Epoch 11697/30000 Training Loss: 0.05231843516230583\n",
      "Epoch 11698/30000 Training Loss: 0.038964029401540756\n",
      "Epoch 11699/30000 Training Loss: 0.04978282004594803\n",
      "Epoch 11700/30000 Training Loss: 0.0785469114780426\n",
      "Epoch 11700/30000 Validation Loss: 0.0595959797501564\n",
      "Epoch 11701/30000 Training Loss: 0.061773356050252914\n",
      "Epoch 11702/30000 Training Loss: 0.04496833309531212\n",
      "Epoch 11703/30000 Training Loss: 0.048391927033662796\n",
      "Epoch 11704/30000 Training Loss: 0.06027344614267349\n",
      "Epoch 11705/30000 Training Loss: 0.04218647629022598\n",
      "Epoch 11706/30000 Training Loss: 0.05298542603850365\n",
      "Epoch 11707/30000 Training Loss: 0.04994971677660942\n",
      "Epoch 11708/30000 Training Loss: 0.04479600861668587\n",
      "Epoch 11709/30000 Training Loss: 0.05402404069900513\n",
      "Epoch 11710/30000 Training Loss: 0.03815941512584686\n",
      "Epoch 11711/30000 Training Loss: 0.04388289898633957\n",
      "Epoch 11712/30000 Training Loss: 0.05473089963197708\n",
      "Epoch 11713/30000 Training Loss: 0.046232983469963074\n",
      "Epoch 11714/30000 Training Loss: 0.05049937218427658\n",
      "Epoch 11715/30000 Training Loss: 0.05292299762368202\n",
      "Epoch 11716/30000 Training Loss: 0.053563252091407776\n",
      "Epoch 11717/30000 Training Loss: 0.03733791410923004\n",
      "Epoch 11718/30000 Training Loss: 0.05001472309231758\n",
      "Epoch 11719/30000 Training Loss: 0.06229056417942047\n",
      "Epoch 11720/30000 Training Loss: 0.04654519259929657\n",
      "Epoch 11721/30000 Training Loss: 0.03254628926515579\n",
      "Epoch 11722/30000 Training Loss: 0.05610154569149017\n",
      "Epoch 11723/30000 Training Loss: 0.04666302353143692\n",
      "Epoch 11724/30000 Training Loss: 0.05410538241267204\n",
      "Epoch 11725/30000 Training Loss: 0.05139116197824478\n",
      "Epoch 11726/30000 Training Loss: 0.04389292746782303\n",
      "Epoch 11727/30000 Training Loss: 0.06541565805673599\n",
      "Epoch 11728/30000 Training Loss: 0.0513005293905735\n",
      "Epoch 11729/30000 Training Loss: 0.05018509179353714\n",
      "Epoch 11730/30000 Training Loss: 0.04908021539449692\n",
      "Epoch 11731/30000 Training Loss: 0.06598279625177383\n",
      "Epoch 11732/30000 Training Loss: 0.05605997145175934\n",
      "Epoch 11733/30000 Training Loss: 0.03959944471716881\n",
      "Epoch 11734/30000 Training Loss: 0.06402807682752609\n",
      "Epoch 11735/30000 Training Loss: 0.04900388419628143\n",
      "Epoch 11736/30000 Training Loss: 0.05557694286108017\n",
      "Epoch 11737/30000 Training Loss: 0.04915446788072586\n",
      "Epoch 11738/30000 Training Loss: 0.050769828259944916\n",
      "Epoch 11739/30000 Training Loss: 0.04993171989917755\n",
      "Epoch 11740/30000 Training Loss: 0.0530167892575264\n",
      "Epoch 11741/30000 Training Loss: 0.046105921268463135\n",
      "Epoch 11742/30000 Training Loss: 0.06663411855697632\n",
      "Epoch 11743/30000 Training Loss: 0.06584160029888153\n",
      "Epoch 11744/30000 Training Loss: 0.04786841571331024\n",
      "Epoch 11745/30000 Training Loss: 0.09183020889759064\n",
      "Epoch 11746/30000 Training Loss: 0.044305410236120224\n",
      "Epoch 11747/30000 Training Loss: 0.04902482405304909\n",
      "Epoch 11748/30000 Training Loss: 0.06864986568689346\n",
      "Epoch 11749/30000 Training Loss: 0.06404263526201248\n",
      "Epoch 11750/30000 Training Loss: 0.05595382675528526\n",
      "Epoch 11751/30000 Training Loss: 0.048660993576049805\n",
      "Epoch 11752/30000 Training Loss: 0.049355704337358475\n",
      "Epoch 11753/30000 Training Loss: 0.06340289115905762\n",
      "Epoch 11754/30000 Training Loss: 0.04458063095808029\n",
      "Epoch 11755/30000 Training Loss: 0.056288935244083405\n",
      "Epoch 11756/30000 Training Loss: 0.04309438169002533\n",
      "Epoch 11757/30000 Training Loss: 0.06566569209098816\n",
      "Epoch 11758/30000 Training Loss: 0.046724408864974976\n",
      "Epoch 11759/30000 Training Loss: 0.05304255336523056\n",
      "Epoch 11760/30000 Training Loss: 0.05097271502017975\n",
      "Epoch 11761/30000 Training Loss: 0.05055607110261917\n",
      "Epoch 11762/30000 Training Loss: 0.06483757495880127\n",
      "Epoch 11763/30000 Training Loss: 0.04813780263066292\n",
      "Epoch 11764/30000 Training Loss: 0.05508403852581978\n",
      "Epoch 11765/30000 Training Loss: 0.053139686584472656\n",
      "Epoch 11766/30000 Training Loss: 0.04787185788154602\n",
      "Epoch 11767/30000 Training Loss: 0.05651777982711792\n",
      "Epoch 11768/30000 Training Loss: 0.049365151673555374\n",
      "Epoch 11769/30000 Training Loss: 0.0459875613451004\n",
      "Epoch 11770/30000 Training Loss: 0.05866774916648865\n",
      "Epoch 11771/30000 Training Loss: 0.060312189161777496\n",
      "Epoch 11772/30000 Training Loss: 0.04762132093310356\n",
      "Epoch 11773/30000 Training Loss: 0.04746030643582344\n",
      "Epoch 11774/30000 Training Loss: 0.05197978764772415\n",
      "Epoch 11775/30000 Training Loss: 0.044107772409915924\n",
      "Epoch 11776/30000 Training Loss: 0.05632802098989487\n",
      "Epoch 11777/30000 Training Loss: 0.046404141932725906\n",
      "Epoch 11778/30000 Training Loss: 0.06600964069366455\n",
      "Epoch 11779/30000 Training Loss: 0.06041431427001953\n",
      "Epoch 11780/30000 Training Loss: 0.04523182287812233\n",
      "Epoch 11781/30000 Training Loss: 0.05496915429830551\n",
      "Epoch 11782/30000 Training Loss: 0.07299001514911652\n",
      "Epoch 11783/30000 Training Loss: 0.05503884330391884\n",
      "Epoch 11784/30000 Training Loss: 0.06121663749217987\n",
      "Epoch 11785/30000 Training Loss: 0.04069904237985611\n",
      "Epoch 11786/30000 Training Loss: 0.06752555072307587\n",
      "Epoch 11787/30000 Training Loss: 0.07231905311346054\n",
      "Epoch 11788/30000 Training Loss: 0.041841402649879456\n",
      "Epoch 11789/30000 Training Loss: 0.042718760669231415\n",
      "Epoch 11790/30000 Training Loss: 0.052305176854133606\n",
      "Epoch 11791/30000 Training Loss: 0.05125844106078148\n",
      "Epoch 11792/30000 Training Loss: 0.05192447453737259\n",
      "Epoch 11793/30000 Training Loss: 0.053715310990810394\n",
      "Epoch 11794/30000 Training Loss: 0.05492926388978958\n",
      "Epoch 11795/30000 Training Loss: 0.05308058485388756\n",
      "Epoch 11796/30000 Training Loss: 0.056975770741701126\n",
      "Epoch 11797/30000 Training Loss: 0.05034361034631729\n",
      "Epoch 11798/30000 Training Loss: 0.055211588740348816\n",
      "Epoch 11799/30000 Training Loss: 0.04787054285407066\n",
      "Epoch 11800/30000 Training Loss: 0.058289360255002975\n",
      "Epoch 11800/30000 Validation Loss: 0.04233935475349426\n",
      "Epoch 11801/30000 Training Loss: 0.04345780238509178\n",
      "Epoch 11802/30000 Training Loss: 0.048191316425800323\n",
      "Epoch 11803/30000 Training Loss: 0.0620749369263649\n",
      "Epoch 11804/30000 Training Loss: 0.05981940031051636\n",
      "Epoch 11805/30000 Training Loss: 0.062464408576488495\n",
      "Epoch 11806/30000 Training Loss: 0.05373750627040863\n",
      "Epoch 11807/30000 Training Loss: 0.046686507761478424\n",
      "Epoch 11808/30000 Training Loss: 0.052522264420986176\n",
      "Epoch 11809/30000 Training Loss: 0.06733107566833496\n",
      "Epoch 11810/30000 Training Loss: 0.03988543897867203\n",
      "Epoch 11811/30000 Training Loss: 0.054846782237291336\n",
      "Epoch 11812/30000 Training Loss: 0.060130711644887924\n",
      "Epoch 11813/30000 Training Loss: 0.05352255702018738\n",
      "Epoch 11814/30000 Training Loss: 0.04883962497115135\n",
      "Epoch 11815/30000 Training Loss: 0.0514083169400692\n",
      "Epoch 11816/30000 Training Loss: 0.05986206233501434\n",
      "Epoch 11817/30000 Training Loss: 0.04775836318731308\n",
      "Epoch 11818/30000 Training Loss: 0.05946437269449234\n",
      "Epoch 11819/30000 Training Loss: 0.042695995420217514\n",
      "Epoch 11820/30000 Training Loss: 0.04485127329826355\n",
      "Epoch 11821/30000 Training Loss: 0.04528937488794327\n",
      "Epoch 11822/30000 Training Loss: 0.03802316635847092\n",
      "Epoch 11823/30000 Training Loss: 0.06405331939458847\n",
      "Epoch 11824/30000 Training Loss: 0.05672632157802582\n",
      "Epoch 11825/30000 Training Loss: 0.0534580796957016\n",
      "Epoch 11826/30000 Training Loss: 0.06063120812177658\n",
      "Epoch 11827/30000 Training Loss: 0.06447640061378479\n",
      "Epoch 11828/30000 Training Loss: 0.06299561262130737\n",
      "Epoch 11829/30000 Training Loss: 0.04280076175928116\n",
      "Epoch 11830/30000 Training Loss: 0.05504976585507393\n",
      "Epoch 11831/30000 Training Loss: 0.03989621624350548\n",
      "Epoch 11832/30000 Training Loss: 0.04602965712547302\n",
      "Epoch 11833/30000 Training Loss: 0.038978103548288345\n",
      "Epoch 11834/30000 Training Loss: 0.052245382219552994\n",
      "Epoch 11835/30000 Training Loss: 0.05317040532827377\n",
      "Epoch 11836/30000 Training Loss: 0.07014703750610352\n",
      "Epoch 11837/30000 Training Loss: 0.05787818878889084\n",
      "Epoch 11838/30000 Training Loss: 0.0488063246011734\n",
      "Epoch 11839/30000 Training Loss: 0.05585826188325882\n",
      "Epoch 11840/30000 Training Loss: 0.06145385280251503\n",
      "Epoch 11841/30000 Training Loss: 0.06976108253002167\n",
      "Epoch 11842/30000 Training Loss: 0.05385622754693031\n",
      "Epoch 11843/30000 Training Loss: 0.04153207689523697\n",
      "Epoch 11844/30000 Training Loss: 0.03993953391909599\n",
      "Epoch 11845/30000 Training Loss: 0.042872656136751175\n",
      "Epoch 11846/30000 Training Loss: 0.044646210968494415\n",
      "Epoch 11847/30000 Training Loss: 0.06362181901931763\n",
      "Epoch 11848/30000 Training Loss: 0.06247930973768234\n",
      "Epoch 11849/30000 Training Loss: 0.05046027898788452\n",
      "Epoch 11850/30000 Training Loss: 0.04857451468706131\n",
      "Epoch 11851/30000 Training Loss: 0.06024561822414398\n",
      "Epoch 11852/30000 Training Loss: 0.04298413544893265\n",
      "Epoch 11853/30000 Training Loss: 0.05415772274136543\n",
      "Epoch 11854/30000 Training Loss: 0.0500648096203804\n",
      "Epoch 11855/30000 Training Loss: 0.059663884341716766\n",
      "Epoch 11856/30000 Training Loss: 0.034761421382427216\n",
      "Epoch 11857/30000 Training Loss: 0.049127962440252304\n",
      "Epoch 11858/30000 Training Loss: 0.06297095119953156\n",
      "Epoch 11859/30000 Training Loss: 0.052763067185878754\n",
      "Epoch 11860/30000 Training Loss: 0.06974127143621445\n",
      "Epoch 11861/30000 Training Loss: 0.06045756861567497\n",
      "Epoch 11862/30000 Training Loss: 0.05970406532287598\n",
      "Epoch 11863/30000 Training Loss: 0.039821356534957886\n",
      "Epoch 11864/30000 Training Loss: 0.05179543048143387\n",
      "Epoch 11865/30000 Training Loss: 0.04561377689242363\n",
      "Epoch 11866/30000 Training Loss: 0.04906965047121048\n",
      "Epoch 11867/30000 Training Loss: 0.03589777275919914\n",
      "Epoch 11868/30000 Training Loss: 0.047587089240550995\n",
      "Epoch 11869/30000 Training Loss: 0.05326380580663681\n",
      "Epoch 11870/30000 Training Loss: 0.057181622833013535\n",
      "Epoch 11871/30000 Training Loss: 0.04896152764558792\n",
      "Epoch 11872/30000 Training Loss: 0.04198954626917839\n",
      "Epoch 11873/30000 Training Loss: 0.057983383536338806\n",
      "Epoch 11874/30000 Training Loss: 0.05187986046075821\n",
      "Epoch 11875/30000 Training Loss: 0.06726063042879105\n",
      "Epoch 11876/30000 Training Loss: 0.04112918674945831\n",
      "Epoch 11877/30000 Training Loss: 0.04560112580657005\n",
      "Epoch 11878/30000 Training Loss: 0.059759970754384995\n",
      "Epoch 11879/30000 Training Loss: 0.051996633410453796\n",
      "Epoch 11880/30000 Training Loss: 0.042358092963695526\n",
      "Epoch 11881/30000 Training Loss: 0.04585563391447067\n",
      "Epoch 11882/30000 Training Loss: 0.03979738429188728\n",
      "Epoch 11883/30000 Training Loss: 0.050661880522966385\n",
      "Epoch 11884/30000 Training Loss: 0.057207003235816956\n",
      "Epoch 11885/30000 Training Loss: 0.04493594169616699\n",
      "Epoch 11886/30000 Training Loss: 0.06406734138727188\n",
      "Epoch 11887/30000 Training Loss: 0.06340047717094421\n",
      "Epoch 11888/30000 Training Loss: 0.04935295879840851\n",
      "Epoch 11889/30000 Training Loss: 0.05656994879245758\n",
      "Epoch 11890/30000 Training Loss: 0.05117808282375336\n",
      "Epoch 11891/30000 Training Loss: 0.04005369171500206\n",
      "Epoch 11892/30000 Training Loss: 0.056597381830215454\n",
      "Epoch 11893/30000 Training Loss: 0.05072171613574028\n",
      "Epoch 11894/30000 Training Loss: 0.07569758594036102\n",
      "Epoch 11895/30000 Training Loss: 0.05721425265073776\n",
      "Epoch 11896/30000 Training Loss: 0.04327082633972168\n",
      "Epoch 11897/30000 Training Loss: 0.07153943181037903\n",
      "Epoch 11898/30000 Training Loss: 0.05104226619005203\n",
      "Epoch 11899/30000 Training Loss: 0.049644939601421356\n",
      "Epoch 11900/30000 Training Loss: 0.06138711795210838\n",
      "Epoch 11900/30000 Validation Loss: 0.04738517105579376\n",
      "Epoch 11901/30000 Training Loss: 0.05388778820633888\n",
      "Epoch 11902/30000 Training Loss: 0.053505681455135345\n",
      "Epoch 11903/30000 Training Loss: 0.061059221625328064\n",
      "Epoch 11904/30000 Training Loss: 0.05779732018709183\n",
      "Epoch 11905/30000 Training Loss: 0.05178455263376236\n",
      "Epoch 11906/30000 Training Loss: 0.03780616447329521\n",
      "Epoch 11907/30000 Training Loss: 0.05370204150676727\n",
      "Epoch 11908/30000 Training Loss: 0.04364294931292534\n",
      "Epoch 11909/30000 Training Loss: 0.05553596094250679\n",
      "Epoch 11910/30000 Training Loss: 0.053551167249679565\n",
      "Epoch 11911/30000 Training Loss: 0.056298643350601196\n",
      "Epoch 11912/30000 Training Loss: 0.06543435156345367\n",
      "Epoch 11913/30000 Training Loss: 0.03606308251619339\n",
      "Epoch 11914/30000 Training Loss: 0.04719888046383858\n",
      "Epoch 11915/30000 Training Loss: 0.061740268021821976\n",
      "Epoch 11916/30000 Training Loss: 0.037211500108242035\n",
      "Epoch 11917/30000 Training Loss: 0.03677026927471161\n",
      "Epoch 11918/30000 Training Loss: 0.04067571088671684\n",
      "Epoch 11919/30000 Training Loss: 0.05518753081560135\n",
      "Epoch 11920/30000 Training Loss: 0.05313953757286072\n",
      "Epoch 11921/30000 Training Loss: 0.06434544920921326\n",
      "Epoch 11922/30000 Training Loss: 0.04997457563877106\n",
      "Epoch 11923/30000 Training Loss: 0.06615136563777924\n",
      "Epoch 11924/30000 Training Loss: 0.04212347790598869\n",
      "Epoch 11925/30000 Training Loss: 0.04826272651553154\n",
      "Epoch 11926/30000 Training Loss: 0.06434012949466705\n",
      "Epoch 11927/30000 Training Loss: 0.05002312734723091\n",
      "Epoch 11928/30000 Training Loss: 0.06358453631401062\n",
      "Epoch 11929/30000 Training Loss: 0.04785075783729553\n",
      "Epoch 11930/30000 Training Loss: 0.05299948528409004\n",
      "Epoch 11931/30000 Training Loss: 0.05218322575092316\n",
      "Epoch 11932/30000 Training Loss: 0.05049386993050575\n",
      "Epoch 11933/30000 Training Loss: 0.05577622726559639\n",
      "Epoch 11934/30000 Training Loss: 0.04812881723046303\n",
      "Epoch 11935/30000 Training Loss: 0.04639089107513428\n",
      "Epoch 11936/30000 Training Loss: 0.05392557010054588\n",
      "Epoch 11937/30000 Training Loss: 0.05063064023852348\n",
      "Epoch 11938/30000 Training Loss: 0.045550305396318436\n",
      "Epoch 11939/30000 Training Loss: 0.0425531305372715\n",
      "Epoch 11940/30000 Training Loss: 0.0600249208509922\n",
      "Epoch 11941/30000 Training Loss: 0.06441830098628998\n",
      "Epoch 11942/30000 Training Loss: 0.048583660274744034\n",
      "Epoch 11943/30000 Training Loss: 0.04997745901346207\n",
      "Epoch 11944/30000 Training Loss: 0.046205244958400726\n",
      "Epoch 11945/30000 Training Loss: 0.054245367646217346\n",
      "Epoch 11946/30000 Training Loss: 0.041013117879629135\n",
      "Epoch 11947/30000 Training Loss: 0.05570284277200699\n",
      "Epoch 11948/30000 Training Loss: 0.05033203214406967\n",
      "Epoch 11949/30000 Training Loss: 0.058788198977708817\n",
      "Epoch 11950/30000 Training Loss: 0.047080524265766144\n",
      "Epoch 11951/30000 Training Loss: 0.03892498463392258\n",
      "Epoch 11952/30000 Training Loss: 0.0540989525616169\n",
      "Epoch 11953/30000 Training Loss: 0.05767899006605148\n",
      "Epoch 11954/30000 Training Loss: 0.048467282205820084\n",
      "Epoch 11955/30000 Training Loss: 0.0545271635055542\n",
      "Epoch 11956/30000 Training Loss: 0.04648565128445625\n",
      "Epoch 11957/30000 Training Loss: 0.059970129281282425\n",
      "Epoch 11958/30000 Training Loss: 0.06331934034824371\n",
      "Epoch 11959/30000 Training Loss: 0.044167742133140564\n",
      "Epoch 11960/30000 Training Loss: 0.05272766202688217\n",
      "Epoch 11961/30000 Training Loss: 0.0405123345553875\n",
      "Epoch 11962/30000 Training Loss: 0.035344213247299194\n",
      "Epoch 11963/30000 Training Loss: 0.05896196514368057\n",
      "Epoch 11964/30000 Training Loss: 0.06060149148106575\n",
      "Epoch 11965/30000 Training Loss: 0.05019275099039078\n",
      "Epoch 11966/30000 Training Loss: 0.04785676673054695\n",
      "Epoch 11967/30000 Training Loss: 0.06140166521072388\n",
      "Epoch 11968/30000 Training Loss: 0.05307821184396744\n",
      "Epoch 11969/30000 Training Loss: 0.03979464992880821\n",
      "Epoch 11970/30000 Training Loss: 0.045926205813884735\n",
      "Epoch 11971/30000 Training Loss: 0.04145139083266258\n",
      "Epoch 11972/30000 Training Loss: 0.04568645358085632\n",
      "Epoch 11973/30000 Training Loss: 0.05339368060231209\n",
      "Epoch 11974/30000 Training Loss: 0.057961564511060715\n",
      "Epoch 11975/30000 Training Loss: 0.050387296825647354\n",
      "Epoch 11976/30000 Training Loss: 0.03756260126829147\n",
      "Epoch 11977/30000 Training Loss: 0.05072811245918274\n",
      "Epoch 11978/30000 Training Loss: 0.05380681902170181\n",
      "Epoch 11979/30000 Training Loss: 0.0526517853140831\n",
      "Epoch 11980/30000 Training Loss: 0.05044257268309593\n",
      "Epoch 11981/30000 Training Loss: 0.054473891854286194\n",
      "Epoch 11982/30000 Training Loss: 0.04199092090129852\n",
      "Epoch 11983/30000 Training Loss: 0.05965990573167801\n",
      "Epoch 11984/30000 Training Loss: 0.048775412142276764\n",
      "Epoch 11985/30000 Training Loss: 0.0520729124546051\n",
      "Epoch 11986/30000 Training Loss: 0.03711352497339249\n",
      "Epoch 11987/30000 Training Loss: 0.050186529755592346\n",
      "Epoch 11988/30000 Training Loss: 0.0408325232565403\n",
      "Epoch 11989/30000 Training Loss: 0.05036742985248566\n",
      "Epoch 11990/30000 Training Loss: 0.05784033238887787\n",
      "Epoch 11991/30000 Training Loss: 0.04834996163845062\n",
      "Epoch 11992/30000 Training Loss: 0.0520155094563961\n",
      "Epoch 11993/30000 Training Loss: 0.056964948773384094\n",
      "Epoch 11994/30000 Training Loss: 0.04975127428770065\n",
      "Epoch 11995/30000 Training Loss: 0.04982979595661163\n",
      "Epoch 11996/30000 Training Loss: 0.054093725979328156\n",
      "Epoch 11997/30000 Training Loss: 0.0675974041223526\n",
      "Epoch 11998/30000 Training Loss: 0.06125232204794884\n",
      "Epoch 11999/30000 Training Loss: 0.040918122977018356\n",
      "Epoch 12000/30000 Training Loss: 0.06018795818090439\n",
      "Epoch 12000/30000 Validation Loss: 0.05964122712612152\n",
      "Epoch 12001/30000 Training Loss: 0.04305870831012726\n",
      "Epoch 12002/30000 Training Loss: 0.051833853125572205\n",
      "Epoch 12003/30000 Training Loss: 0.06425388902425766\n",
      "Epoch 12004/30000 Training Loss: 0.058859072625637054\n",
      "Epoch 12005/30000 Training Loss: 0.07235394418239594\n",
      "Epoch 12006/30000 Training Loss: 0.052939288318157196\n",
      "Epoch 12007/30000 Training Loss: 0.05374425649642944\n",
      "Epoch 12008/30000 Training Loss: 0.04297863319516182\n",
      "Epoch 12009/30000 Training Loss: 0.045899897813797\n",
      "Epoch 12010/30000 Training Loss: 0.048648934811353683\n",
      "Epoch 12011/30000 Training Loss: 0.06265679746866226\n",
      "Epoch 12012/30000 Training Loss: 0.053282443434000015\n",
      "Epoch 12013/30000 Training Loss: 0.03927627578377724\n",
      "Epoch 12014/30000 Training Loss: 0.054284777492284775\n",
      "Epoch 12015/30000 Training Loss: 0.049769144505262375\n",
      "Epoch 12016/30000 Training Loss: 0.05294690281152725\n",
      "Epoch 12017/30000 Training Loss: 0.05088675022125244\n",
      "Epoch 12018/30000 Training Loss: 0.035952381789684296\n",
      "Epoch 12019/30000 Training Loss: 0.05479927361011505\n",
      "Epoch 12020/30000 Training Loss: 0.05153168365359306\n",
      "Epoch 12021/30000 Training Loss: 0.04977027699351311\n",
      "Epoch 12022/30000 Training Loss: 0.052841976284980774\n",
      "Epoch 12023/30000 Training Loss: 0.04375673830509186\n",
      "Epoch 12024/30000 Training Loss: 0.05802310258150101\n",
      "Epoch 12025/30000 Training Loss: 0.050263430923223495\n",
      "Epoch 12026/30000 Training Loss: 0.06089595705270767\n",
      "Epoch 12027/30000 Training Loss: 0.05153656005859375\n",
      "Epoch 12028/30000 Training Loss: 0.038717467337846756\n",
      "Epoch 12029/30000 Training Loss: 0.0768604427576065\n",
      "Epoch 12030/30000 Training Loss: 0.05468304455280304\n",
      "Epoch 12031/30000 Training Loss: 0.046115174889564514\n",
      "Epoch 12032/30000 Training Loss: 0.04994438588619232\n",
      "Epoch 12033/30000 Training Loss: 0.05529588833451271\n",
      "Epoch 12034/30000 Training Loss: 0.047050461173057556\n",
      "Epoch 12035/30000 Training Loss: 0.0473819337785244\n",
      "Epoch 12036/30000 Training Loss: 0.048014696687459946\n",
      "Epoch 12037/30000 Training Loss: 0.05503658205270767\n",
      "Epoch 12038/30000 Training Loss: 0.041154369711875916\n",
      "Epoch 12039/30000 Training Loss: 0.04078366234898567\n",
      "Epoch 12040/30000 Training Loss: 0.04994523525238037\n",
      "Epoch 12041/30000 Training Loss: 0.058815959841012955\n",
      "Epoch 12042/30000 Training Loss: 0.05766674503684044\n",
      "Epoch 12043/30000 Training Loss: 0.059556636959314346\n",
      "Epoch 12044/30000 Training Loss: 0.05797163024544716\n",
      "Epoch 12045/30000 Training Loss: 0.05213971436023712\n",
      "Epoch 12046/30000 Training Loss: 0.0727664977312088\n",
      "Epoch 12047/30000 Training Loss: 0.0774514451622963\n",
      "Epoch 12048/30000 Training Loss: 0.06396755576133728\n",
      "Epoch 12049/30000 Training Loss: 0.05470021814107895\n",
      "Epoch 12050/30000 Training Loss: 0.045752353966236115\n",
      "Epoch 12051/30000 Training Loss: 0.04755978658795357\n",
      "Epoch 12052/30000 Training Loss: 0.0656314268708229\n",
      "Epoch 12053/30000 Training Loss: 0.04816313087940216\n",
      "Epoch 12054/30000 Training Loss: 0.04292519390583038\n",
      "Epoch 12055/30000 Training Loss: 0.03995048999786377\n",
      "Epoch 12056/30000 Training Loss: 0.045270584523677826\n",
      "Epoch 12057/30000 Training Loss: 0.040305960923433304\n",
      "Epoch 12058/30000 Training Loss: 0.062299974262714386\n",
      "Epoch 12059/30000 Training Loss: 0.057353150099515915\n",
      "Epoch 12060/30000 Training Loss: 0.05686759576201439\n",
      "Epoch 12061/30000 Training Loss: 0.03678738698363304\n",
      "Epoch 12062/30000 Training Loss: 0.05903496965765953\n",
      "Epoch 12063/30000 Training Loss: 0.050795674324035645\n",
      "Epoch 12064/30000 Training Loss: 0.042176976799964905\n",
      "Epoch 12065/30000 Training Loss: 0.05894816666841507\n",
      "Epoch 12066/30000 Training Loss: 0.05838111788034439\n",
      "Epoch 12067/30000 Training Loss: 0.05354570224881172\n",
      "Epoch 12068/30000 Training Loss: 0.051446594297885895\n",
      "Epoch 12069/30000 Training Loss: 0.05342482402920723\n",
      "Epoch 12070/30000 Training Loss: 0.05669458955526352\n",
      "Epoch 12071/30000 Training Loss: 0.06849807500839233\n",
      "Epoch 12072/30000 Training Loss: 0.06067028269171715\n",
      "Epoch 12073/30000 Training Loss: 0.050218433141708374\n",
      "Epoch 12074/30000 Training Loss: 0.057827968150377274\n",
      "Epoch 12075/30000 Training Loss: 0.07755766808986664\n",
      "Epoch 12076/30000 Training Loss: 0.0715356320142746\n",
      "Epoch 12077/30000 Training Loss: 0.06007746234536171\n",
      "Epoch 12078/30000 Training Loss: 0.03594733402132988\n",
      "Epoch 12079/30000 Training Loss: 0.04456601291894913\n",
      "Epoch 12080/30000 Training Loss: 0.05015290901064873\n",
      "Epoch 12081/30000 Training Loss: 0.048243194818496704\n",
      "Epoch 12082/30000 Training Loss: 0.05749639868736267\n",
      "Epoch 12083/30000 Training Loss: 0.05816376209259033\n",
      "Epoch 12084/30000 Training Loss: 0.049687743186950684\n",
      "Epoch 12085/30000 Training Loss: 0.043345578014850616\n",
      "Epoch 12086/30000 Training Loss: 0.0437990203499794\n",
      "Epoch 12087/30000 Training Loss: 0.046190060675144196\n",
      "Epoch 12088/30000 Training Loss: 0.05641106516122818\n",
      "Epoch 12089/30000 Training Loss: 0.06292025744915009\n",
      "Epoch 12090/30000 Training Loss: 0.07039374858140945\n",
      "Epoch 12091/30000 Training Loss: 0.0321531742811203\n",
      "Epoch 12092/30000 Training Loss: 0.05064529925584793\n",
      "Epoch 12093/30000 Training Loss: 0.06267955154180527\n",
      "Epoch 12094/30000 Training Loss: 0.0571252778172493\n",
      "Epoch 12095/30000 Training Loss: 0.04487697407603264\n",
      "Epoch 12096/30000 Training Loss: 0.044471707195043564\n",
      "Epoch 12097/30000 Training Loss: 0.040193505585193634\n",
      "Epoch 12098/30000 Training Loss: 0.05704032629728317\n",
      "Epoch 12099/30000 Training Loss: 0.04403037577867508\n",
      "Epoch 12100/30000 Training Loss: 0.058723997324705124\n",
      "Epoch 12100/30000 Validation Loss: 0.051593273878097534\n",
      "Epoch 12101/30000 Training Loss: 0.06344345211982727\n",
      "Epoch 12102/30000 Training Loss: 0.05228886008262634\n",
      "Epoch 12103/30000 Training Loss: 0.055902473628520966\n",
      "Epoch 12104/30000 Training Loss: 0.059201471507549286\n",
      "Epoch 12105/30000 Training Loss: 0.047121915966272354\n",
      "Epoch 12106/30000 Training Loss: 0.038056716322898865\n",
      "Epoch 12107/30000 Training Loss: 0.06968607008457184\n",
      "Epoch 12108/30000 Training Loss: 0.037364427000284195\n",
      "Epoch 12109/30000 Training Loss: 0.04779687896370888\n",
      "Epoch 12110/30000 Training Loss: 0.048222776502370834\n",
      "Epoch 12111/30000 Training Loss: 0.047718361020088196\n",
      "Epoch 12112/30000 Training Loss: 0.04551296681165695\n",
      "Epoch 12113/30000 Training Loss: 0.030679976567626\n",
      "Epoch 12114/30000 Training Loss: 0.04324373975396156\n",
      "Epoch 12115/30000 Training Loss: 0.04926532134413719\n",
      "Epoch 12116/30000 Training Loss: 0.0452331118285656\n",
      "Epoch 12117/30000 Training Loss: 0.058048080652952194\n",
      "Epoch 12118/30000 Training Loss: 0.056214384734630585\n",
      "Epoch 12119/30000 Training Loss: 0.04790061339735985\n",
      "Epoch 12120/30000 Training Loss: 0.05607294291257858\n",
      "Epoch 12121/30000 Training Loss: 0.0654948428273201\n",
      "Epoch 12122/30000 Training Loss: 0.0421023890376091\n",
      "Epoch 12123/30000 Training Loss: 0.054979242384433746\n",
      "Epoch 12124/30000 Training Loss: 0.03963930532336235\n",
      "Epoch 12125/30000 Training Loss: 0.049703486263751984\n",
      "Epoch 12126/30000 Training Loss: 0.05133208632469177\n",
      "Epoch 12127/30000 Training Loss: 0.04552045837044716\n",
      "Epoch 12128/30000 Training Loss: 0.05009832978248596\n",
      "Epoch 12129/30000 Training Loss: 0.04724784195423126\n",
      "Epoch 12130/30000 Training Loss: 0.03438812121748924\n",
      "Epoch 12131/30000 Training Loss: 0.04234909638762474\n",
      "Epoch 12132/30000 Training Loss: 0.057684674859046936\n",
      "Epoch 12133/30000 Training Loss: 0.05402049049735069\n",
      "Epoch 12134/30000 Training Loss: 0.05883100628852844\n",
      "Epoch 12135/30000 Training Loss: 0.05519694834947586\n",
      "Epoch 12136/30000 Training Loss: 0.0551779568195343\n",
      "Epoch 12137/30000 Training Loss: 0.04761582612991333\n",
      "Epoch 12138/30000 Training Loss: 0.03970049321651459\n",
      "Epoch 12139/30000 Training Loss: 0.040198542177677155\n",
      "Epoch 12140/30000 Training Loss: 0.05621882900595665\n",
      "Epoch 12141/30000 Training Loss: 0.07054083049297333\n",
      "Epoch 12142/30000 Training Loss: 0.05169367790222168\n",
      "Epoch 12143/30000 Training Loss: 0.0601545050740242\n",
      "Epoch 12144/30000 Training Loss: 0.04914214834570885\n",
      "Epoch 12145/30000 Training Loss: 0.06040672957897186\n",
      "Epoch 12146/30000 Training Loss: 0.059420760720968246\n",
      "Epoch 12147/30000 Training Loss: 0.050894275307655334\n",
      "Epoch 12148/30000 Training Loss: 0.04444340616464615\n",
      "Epoch 12149/30000 Training Loss: 0.05255161225795746\n",
      "Epoch 12150/30000 Training Loss: 0.05058971792459488\n",
      "Epoch 12151/30000 Training Loss: 0.045498352497816086\n",
      "Epoch 12152/30000 Training Loss: 0.059253811836242676\n",
      "Epoch 12153/30000 Training Loss: 0.05168859288096428\n",
      "Epoch 12154/30000 Training Loss: 0.035750534385442734\n",
      "Epoch 12155/30000 Training Loss: 0.04526108503341675\n",
      "Epoch 12156/30000 Training Loss: 0.05423419550061226\n",
      "Epoch 12157/30000 Training Loss: 0.046105485409498215\n",
      "Epoch 12158/30000 Training Loss: 0.05886198580265045\n",
      "Epoch 12159/30000 Training Loss: 0.04603630304336548\n",
      "Epoch 12160/30000 Training Loss: 0.05379391834139824\n",
      "Epoch 12161/30000 Training Loss: 0.034923896193504333\n",
      "Epoch 12162/30000 Training Loss: 0.06753717362880707\n",
      "Epoch 12163/30000 Training Loss: 0.05806028097867966\n",
      "Epoch 12164/30000 Training Loss: 0.058594558387994766\n",
      "Epoch 12165/30000 Training Loss: 0.044449083507061005\n",
      "Epoch 12166/30000 Training Loss: 0.053832896053791046\n",
      "Epoch 12167/30000 Training Loss: 0.06515274196863174\n",
      "Epoch 12168/30000 Training Loss: 0.03809378296136856\n",
      "Epoch 12169/30000 Training Loss: 0.04834456741809845\n",
      "Epoch 12170/30000 Training Loss: 0.059114787727594376\n",
      "Epoch 12171/30000 Training Loss: 0.04090680181980133\n",
      "Epoch 12172/30000 Training Loss: 0.03648572415113449\n",
      "Epoch 12173/30000 Training Loss: 0.04793167486786842\n",
      "Epoch 12174/30000 Training Loss: 0.05212455242872238\n",
      "Epoch 12175/30000 Training Loss: 0.06033891811966896\n",
      "Epoch 12176/30000 Training Loss: 0.04997696727514267\n",
      "Epoch 12177/30000 Training Loss: 0.034573253244161606\n",
      "Epoch 12178/30000 Training Loss: 0.055427007377147675\n",
      "Epoch 12179/30000 Training Loss: 0.04442388564348221\n",
      "Epoch 12180/30000 Training Loss: 0.059370119124650955\n",
      "Epoch 12181/30000 Training Loss: 0.054706379771232605\n",
      "Epoch 12182/30000 Training Loss: 0.047902774065732956\n",
      "Epoch 12183/30000 Training Loss: 0.05035927891731262\n",
      "Epoch 12184/30000 Training Loss: 0.048186514526605606\n",
      "Epoch 12185/30000 Training Loss: 0.056578412652015686\n",
      "Epoch 12186/30000 Training Loss: 0.054368436336517334\n",
      "Epoch 12187/30000 Training Loss: 0.05316557362675667\n",
      "Epoch 12188/30000 Training Loss: 0.057819217443466187\n",
      "Epoch 12189/30000 Training Loss: 0.044419821351766586\n",
      "Epoch 12190/30000 Training Loss: 0.07007832825183868\n",
      "Epoch 12191/30000 Training Loss: 0.056475572288036346\n",
      "Epoch 12192/30000 Training Loss: 0.06632162630558014\n",
      "Epoch 12193/30000 Training Loss: 0.044505659490823746\n",
      "Epoch 12194/30000 Training Loss: 0.06643866002559662\n",
      "Epoch 12195/30000 Training Loss: 0.06046772375702858\n",
      "Epoch 12196/30000 Training Loss: 0.04610178992152214\n",
      "Epoch 12197/30000 Training Loss: 0.05474758520722389\n",
      "Epoch 12198/30000 Training Loss: 0.043978214263916016\n",
      "Epoch 12199/30000 Training Loss: 0.06258156150579453\n",
      "Epoch 12200/30000 Training Loss: 0.0673820748925209\n",
      "Epoch 12200/30000 Validation Loss: 0.05041605606675148\n",
      "Epoch 12201/30000 Training Loss: 0.06354977190494537\n",
      "Epoch 12202/30000 Training Loss: 0.0441405326128006\n",
      "Epoch 12203/30000 Training Loss: 0.053030963987112045\n",
      "Epoch 12204/30000 Training Loss: 0.0439307726919651\n",
      "Epoch 12205/30000 Training Loss: 0.034613147377967834\n",
      "Epoch 12206/30000 Training Loss: 0.07741519063711166\n",
      "Epoch 12207/30000 Training Loss: 0.04317446053028107\n",
      "Epoch 12208/30000 Training Loss: 0.05352996662259102\n",
      "Epoch 12209/30000 Training Loss: 0.04691513627767563\n",
      "Epoch 12210/30000 Training Loss: 0.058784496039152145\n",
      "Epoch 12211/30000 Training Loss: 0.04764280468225479\n",
      "Epoch 12212/30000 Training Loss: 0.042927395552396774\n",
      "Epoch 12213/30000 Training Loss: 0.051393743604421616\n",
      "Epoch 12214/30000 Training Loss: 0.05731460079550743\n",
      "Epoch 12215/30000 Training Loss: 0.05183643102645874\n",
      "Epoch 12216/30000 Training Loss: 0.0485500767827034\n",
      "Epoch 12217/30000 Training Loss: 0.04591537266969681\n",
      "Epoch 12218/30000 Training Loss: 0.0445190966129303\n",
      "Epoch 12219/30000 Training Loss: 0.05139033496379852\n",
      "Epoch 12220/30000 Training Loss: 0.0461450070142746\n",
      "Epoch 12221/30000 Training Loss: 0.053196199238300323\n",
      "Epoch 12222/30000 Training Loss: 0.061874084174633026\n",
      "Epoch 12223/30000 Training Loss: 0.045621320605278015\n",
      "Epoch 12224/30000 Training Loss: 0.06258215755224228\n",
      "Epoch 12225/30000 Training Loss: 0.05725308507680893\n",
      "Epoch 12226/30000 Training Loss: 0.06293701380491257\n",
      "Epoch 12227/30000 Training Loss: 0.06061409413814545\n",
      "Epoch 12228/30000 Training Loss: 0.05426784232258797\n",
      "Epoch 12229/30000 Training Loss: 0.03991272300481796\n",
      "Epoch 12230/30000 Training Loss: 0.033554933965206146\n",
      "Epoch 12231/30000 Training Loss: 0.05012197047472\n",
      "Epoch 12232/30000 Training Loss: 0.060182757675647736\n",
      "Epoch 12233/30000 Training Loss: 0.05583367869257927\n",
      "Epoch 12234/30000 Training Loss: 0.061569973826408386\n",
      "Epoch 12235/30000 Training Loss: 0.05045539513230324\n",
      "Epoch 12236/30000 Training Loss: 0.0532832108438015\n",
      "Epoch 12237/30000 Training Loss: 0.06070559471845627\n",
      "Epoch 12238/30000 Training Loss: 0.04081179201602936\n",
      "Epoch 12239/30000 Training Loss: 0.06715523451566696\n",
      "Epoch 12240/30000 Training Loss: 0.05544322729110718\n",
      "Epoch 12241/30000 Training Loss: 0.06080631911754608\n",
      "Epoch 12242/30000 Training Loss: 0.053188346326351166\n",
      "Epoch 12243/30000 Training Loss: 0.048535194247961044\n",
      "Epoch 12244/30000 Training Loss: 0.04250316694378853\n",
      "Epoch 12245/30000 Training Loss: 0.045054253190755844\n",
      "Epoch 12246/30000 Training Loss: 0.06052014231681824\n",
      "Epoch 12247/30000 Training Loss: 0.050784699618816376\n",
      "Epoch 12248/30000 Training Loss: 0.056578755378723145\n",
      "Epoch 12249/30000 Training Loss: 0.05243737995624542\n",
      "Epoch 12250/30000 Training Loss: 0.04132183641195297\n",
      "Epoch 12251/30000 Training Loss: 0.049892839044332504\n",
      "Epoch 12252/30000 Training Loss: 0.06306309998035431\n",
      "Epoch 12253/30000 Training Loss: 0.05871203914284706\n",
      "Epoch 12254/30000 Training Loss: 0.06039396673440933\n",
      "Epoch 12255/30000 Training Loss: 0.06182673200964928\n",
      "Epoch 12256/30000 Training Loss: 0.0544467493891716\n",
      "Epoch 12257/30000 Training Loss: 0.05937296897172928\n",
      "Epoch 12258/30000 Training Loss: 0.04012648016214371\n",
      "Epoch 12259/30000 Training Loss: 0.0469355508685112\n",
      "Epoch 12260/30000 Training Loss: 0.04435018450021744\n",
      "Epoch 12261/30000 Training Loss: 0.049515679478645325\n",
      "Epoch 12262/30000 Training Loss: 0.04714468866586685\n",
      "Epoch 12263/30000 Training Loss: 0.05041743442416191\n",
      "Epoch 12264/30000 Training Loss: 0.05442832037806511\n",
      "Epoch 12265/30000 Training Loss: 0.057833582162857056\n",
      "Epoch 12266/30000 Training Loss: 0.03533928468823433\n",
      "Epoch 12267/30000 Training Loss: 0.07137908041477203\n",
      "Epoch 12268/30000 Training Loss: 0.0464545339345932\n",
      "Epoch 12269/30000 Training Loss: 0.050316426903009415\n",
      "Epoch 12270/30000 Training Loss: 0.05727531760931015\n",
      "Epoch 12271/30000 Training Loss: 0.04594530165195465\n",
      "Epoch 12272/30000 Training Loss: 0.04207674041390419\n",
      "Epoch 12273/30000 Training Loss: 0.041069820523262024\n",
      "Epoch 12274/30000 Training Loss: 0.061810970306396484\n",
      "Epoch 12275/30000 Training Loss: 0.045532166957855225\n",
      "Epoch 12276/30000 Training Loss: 0.05180933326482773\n",
      "Epoch 12277/30000 Training Loss: 0.04913385957479477\n",
      "Epoch 12278/30000 Training Loss: 0.05181030556559563\n",
      "Epoch 12279/30000 Training Loss: 0.03927766531705856\n",
      "Epoch 12280/30000 Training Loss: 0.04829432815313339\n",
      "Epoch 12281/30000 Training Loss: 0.05977274477481842\n",
      "Epoch 12282/30000 Training Loss: 0.06136612594127655\n",
      "Epoch 12283/30000 Training Loss: 0.07543078809976578\n",
      "Epoch 12284/30000 Training Loss: 0.0527644045650959\n",
      "Epoch 12285/30000 Training Loss: 0.05092579126358032\n",
      "Epoch 12286/30000 Training Loss: 0.05186205729842186\n",
      "Epoch 12287/30000 Training Loss: 0.04746023938059807\n",
      "Epoch 12288/30000 Training Loss: 0.06295233964920044\n",
      "Epoch 12289/30000 Training Loss: 0.057829827070236206\n",
      "Epoch 12290/30000 Training Loss: 0.045459602028131485\n",
      "Epoch 12291/30000 Training Loss: 0.05372416973114014\n",
      "Epoch 12292/30000 Training Loss: 0.05019621551036835\n",
      "Epoch 12293/30000 Training Loss: 0.041750989854335785\n",
      "Epoch 12294/30000 Training Loss: 0.06366132944822311\n",
      "Epoch 12295/30000 Training Loss: 0.04682740196585655\n",
      "Epoch 12296/30000 Training Loss: 0.04969362914562225\n",
      "Epoch 12297/30000 Training Loss: 0.04615253210067749\n",
      "Epoch 12298/30000 Training Loss: 0.04813685268163681\n",
      "Epoch 12299/30000 Training Loss: 0.03735208138823509\n",
      "Epoch 12300/30000 Training Loss: 0.07095760107040405\n",
      "Epoch 12300/30000 Validation Loss: 0.06206050142645836\n",
      "Epoch 12301/30000 Training Loss: 0.054732389748096466\n",
      "Epoch 12302/30000 Training Loss: 0.05459078401327133\n",
      "Epoch 12303/30000 Training Loss: 0.04508204758167267\n",
      "Epoch 12304/30000 Training Loss: 0.03841327503323555\n",
      "Epoch 12305/30000 Training Loss: 0.04501631110906601\n",
      "Epoch 12306/30000 Training Loss: 0.06585244089365005\n",
      "Epoch 12307/30000 Training Loss: 0.042772650718688965\n",
      "Epoch 12308/30000 Training Loss: 0.049065008759498596\n",
      "Epoch 12309/30000 Training Loss: 0.05733831226825714\n",
      "Epoch 12310/30000 Training Loss: 0.044093746691942215\n",
      "Epoch 12311/30000 Training Loss: 0.04548504948616028\n",
      "Epoch 12312/30000 Training Loss: 0.049515824764966965\n",
      "Epoch 12313/30000 Training Loss: 0.04226824268698692\n",
      "Epoch 12314/30000 Training Loss: 0.05736967548727989\n",
      "Epoch 12315/30000 Training Loss: 0.06299645453691483\n",
      "Epoch 12316/30000 Training Loss: 0.047976888716220856\n",
      "Epoch 12317/30000 Training Loss: 0.0488375686109066\n",
      "Epoch 12318/30000 Training Loss: 0.05014769732952118\n",
      "Epoch 12319/30000 Training Loss: 0.050804540514945984\n",
      "Epoch 12320/30000 Training Loss: 0.0425361767411232\n",
      "Epoch 12321/30000 Training Loss: 0.04806270822882652\n",
      "Epoch 12322/30000 Training Loss: 0.04763944074511528\n",
      "Epoch 12323/30000 Training Loss: 0.05338647961616516\n",
      "Epoch 12324/30000 Training Loss: 0.06532753258943558\n",
      "Epoch 12325/30000 Training Loss: 0.04879305884242058\n",
      "Epoch 12326/30000 Training Loss: 0.04848418012261391\n",
      "Epoch 12327/30000 Training Loss: 0.04777981713414192\n",
      "Epoch 12328/30000 Training Loss: 0.049901269376277924\n",
      "Epoch 12329/30000 Training Loss: 0.08545584231615067\n",
      "Epoch 12330/30000 Training Loss: 0.04752536490559578\n",
      "Epoch 12331/30000 Training Loss: 0.046785712242126465\n",
      "Epoch 12332/30000 Training Loss: 0.049131497740745544\n",
      "Epoch 12333/30000 Training Loss: 0.045533228665590286\n",
      "Epoch 12334/30000 Training Loss: 0.059099625796079636\n",
      "Epoch 12335/30000 Training Loss: 0.050604745745658875\n",
      "Epoch 12336/30000 Training Loss: 0.05036061257123947\n",
      "Epoch 12337/30000 Training Loss: 0.058272525668144226\n",
      "Epoch 12338/30000 Training Loss: 0.05198748782277107\n",
      "Epoch 12339/30000 Training Loss: 0.048989105969667435\n",
      "Epoch 12340/30000 Training Loss: 0.06381115317344666\n",
      "Epoch 12341/30000 Training Loss: 0.0501115620136261\n",
      "Epoch 12342/30000 Training Loss: 0.058683112263679504\n",
      "Epoch 12343/30000 Training Loss: 0.06932737678289413\n",
      "Epoch 12344/30000 Training Loss: 0.03981298953294754\n",
      "Epoch 12345/30000 Training Loss: 0.05867717042565346\n",
      "Epoch 12346/30000 Training Loss: 0.04056140035390854\n",
      "Epoch 12347/30000 Training Loss: 0.04669657349586487\n",
      "Epoch 12348/30000 Training Loss: 0.04166498780250549\n",
      "Epoch 12349/30000 Training Loss: 0.07027873396873474\n",
      "Epoch 12350/30000 Training Loss: 0.04770130664110184\n",
      "Epoch 12351/30000 Training Loss: 0.06040074676275253\n",
      "Epoch 12352/30000 Training Loss: 0.05148486793041229\n",
      "Epoch 12353/30000 Training Loss: 0.03468414396047592\n",
      "Epoch 12354/30000 Training Loss: 0.04587208479642868\n",
      "Epoch 12355/30000 Training Loss: 0.043833039700984955\n",
      "Epoch 12356/30000 Training Loss: 0.05657521262764931\n",
      "Epoch 12357/30000 Training Loss: 0.04630883410573006\n",
      "Epoch 12358/30000 Training Loss: 0.05464491248130798\n",
      "Epoch 12359/30000 Training Loss: 0.056973785161972046\n",
      "Epoch 12360/30000 Training Loss: 0.06453472375869751\n",
      "Epoch 12361/30000 Training Loss: 0.06347949057817459\n",
      "Epoch 12362/30000 Training Loss: 0.05292452126741409\n",
      "Epoch 12363/30000 Training Loss: 0.03960689902305603\n",
      "Epoch 12364/30000 Training Loss: 0.04262738302350044\n",
      "Epoch 12365/30000 Training Loss: 0.04584665596485138\n",
      "Epoch 12366/30000 Training Loss: 0.06600160896778107\n",
      "Epoch 12367/30000 Training Loss: 0.05348774790763855\n",
      "Epoch 12368/30000 Training Loss: 0.057196713984012604\n",
      "Epoch 12369/30000 Training Loss: 0.04194509983062744\n",
      "Epoch 12370/30000 Training Loss: 0.05601505562663078\n",
      "Epoch 12371/30000 Training Loss: 0.04355471581220627\n",
      "Epoch 12372/30000 Training Loss: 0.05027081444859505\n",
      "Epoch 12373/30000 Training Loss: 0.047371767461299896\n",
      "Epoch 12374/30000 Training Loss: 0.07341530919075012\n",
      "Epoch 12375/30000 Training Loss: 0.046379126608371735\n",
      "Epoch 12376/30000 Training Loss: 0.05277446657419205\n",
      "Epoch 12377/30000 Training Loss: 0.048003941774368286\n",
      "Epoch 12378/30000 Training Loss: 0.05046302080154419\n",
      "Epoch 12379/30000 Training Loss: 0.05055795982480049\n",
      "Epoch 12380/30000 Training Loss: 0.05671140179038048\n",
      "Epoch 12381/30000 Training Loss: 0.036616239696741104\n",
      "Epoch 12382/30000 Training Loss: 0.07836990803480148\n",
      "Epoch 12383/30000 Training Loss: 0.052472278475761414\n",
      "Epoch 12384/30000 Training Loss: 0.07800134271383286\n",
      "Epoch 12385/30000 Training Loss: 0.06072166934609413\n",
      "Epoch 12386/30000 Training Loss: 0.039370451122522354\n",
      "Epoch 12387/30000 Training Loss: 0.05178338289260864\n",
      "Epoch 12388/30000 Training Loss: 0.051288533955812454\n",
      "Epoch 12389/30000 Training Loss: 0.05435597896575928\n",
      "Epoch 12390/30000 Training Loss: 0.05109529197216034\n",
      "Epoch 12391/30000 Training Loss: 0.04591647908091545\n",
      "Epoch 12392/30000 Training Loss: 0.037364859133958817\n",
      "Epoch 12393/30000 Training Loss: 0.048107072710990906\n",
      "Epoch 12394/30000 Training Loss: 0.059433020651340485\n",
      "Epoch 12395/30000 Training Loss: 0.06930609792470932\n",
      "Epoch 12396/30000 Training Loss: 0.05158863961696625\n",
      "Epoch 12397/30000 Training Loss: 0.04510057717561722\n",
      "Epoch 12398/30000 Training Loss: 0.05478774011135101\n",
      "Epoch 12399/30000 Training Loss: 0.05448676645755768\n",
      "Epoch 12400/30000 Training Loss: 0.043357476592063904\n",
      "Epoch 12400/30000 Validation Loss: 0.03584020584821701\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03584020584821701<=============\n",
      "Epoch 12401/30000 Training Loss: 0.042373113334178925\n",
      "Epoch 12402/30000 Training Loss: 0.040663719177246094\n",
      "Epoch 12403/30000 Training Loss: 0.04263860732316971\n",
      "Epoch 12404/30000 Training Loss: 0.05976291373372078\n",
      "Epoch 12405/30000 Training Loss: 0.061922721564769745\n",
      "Epoch 12406/30000 Training Loss: 0.07585825771093369\n",
      "Epoch 12407/30000 Training Loss: 0.05603296309709549\n",
      "Epoch 12408/30000 Training Loss: 0.05246048793196678\n",
      "Epoch 12409/30000 Training Loss: 0.05637902393937111\n",
      "Epoch 12410/30000 Training Loss: 0.053777556866407394\n",
      "Epoch 12411/30000 Training Loss: 0.050064798444509506\n",
      "Epoch 12412/30000 Training Loss: 0.03322596102952957\n",
      "Epoch 12413/30000 Training Loss: 0.06223096325993538\n",
      "Epoch 12414/30000 Training Loss: 0.04620689898729324\n",
      "Epoch 12415/30000 Training Loss: 0.046626027673482895\n",
      "Epoch 12416/30000 Training Loss: 0.054370470345020294\n",
      "Epoch 12417/30000 Training Loss: 0.05430129915475845\n",
      "Epoch 12418/30000 Training Loss: 0.04200589656829834\n",
      "Epoch 12419/30000 Training Loss: 0.048510756343603134\n",
      "Epoch 12420/30000 Training Loss: 0.044276002794504166\n",
      "Epoch 12421/30000 Training Loss: 0.03752097114920616\n",
      "Epoch 12422/30000 Training Loss: 0.04884858429431915\n",
      "Epoch 12423/30000 Training Loss: 0.06585443019866943\n",
      "Epoch 12424/30000 Training Loss: 0.037240445613861084\n",
      "Epoch 12425/30000 Training Loss: 0.05060121417045593\n",
      "Epoch 12426/30000 Training Loss: 0.07514026015996933\n",
      "Epoch 12427/30000 Training Loss: 0.0375768207013607\n",
      "Epoch 12428/30000 Training Loss: 0.06406943500041962\n",
      "Epoch 12429/30000 Training Loss: 0.04587671160697937\n",
      "Epoch 12430/30000 Training Loss: 0.0530078187584877\n",
      "Epoch 12431/30000 Training Loss: 0.07658621668815613\n",
      "Epoch 12432/30000 Training Loss: 0.060025349259376526\n",
      "Epoch 12433/30000 Training Loss: 0.06447163969278336\n",
      "Epoch 12434/30000 Training Loss: 0.06320846825838089\n",
      "Epoch 12435/30000 Training Loss: 0.0592212975025177\n",
      "Epoch 12436/30000 Training Loss: 0.049889158457517624\n",
      "Epoch 12437/30000 Training Loss: 0.037190861999988556\n",
      "Epoch 12438/30000 Training Loss: 0.07008848339319229\n",
      "Epoch 12439/30000 Training Loss: 0.06336858123540878\n",
      "Epoch 12440/30000 Training Loss: 0.04871568828821182\n",
      "Epoch 12441/30000 Training Loss: 0.05875694751739502\n",
      "Epoch 12442/30000 Training Loss: 0.05486927181482315\n",
      "Epoch 12443/30000 Training Loss: 0.045652393251657486\n",
      "Epoch 12444/30000 Training Loss: 0.058533795177936554\n",
      "Epoch 12445/30000 Training Loss: 0.048076022416353226\n",
      "Epoch 12446/30000 Training Loss: 0.04677683115005493\n",
      "Epoch 12447/30000 Training Loss: 0.05105879530310631\n",
      "Epoch 12448/30000 Training Loss: 0.037309158593416214\n",
      "Epoch 12449/30000 Training Loss: 0.047245800495147705\n",
      "Epoch 12450/30000 Training Loss: 0.04556050896644592\n",
      "Epoch 12451/30000 Training Loss: 0.0451805517077446\n",
      "Epoch 12452/30000 Training Loss: 0.05354144424200058\n",
      "Epoch 12453/30000 Training Loss: 0.029616814106702805\n",
      "Epoch 12454/30000 Training Loss: 0.030167611315846443\n",
      "Epoch 12455/30000 Training Loss: 0.05617817863821983\n",
      "Epoch 12456/30000 Training Loss: 0.047804806381464005\n",
      "Epoch 12457/30000 Training Loss: 0.062074415385723114\n",
      "Epoch 12458/30000 Training Loss: 0.07051865011453629\n",
      "Epoch 12459/30000 Training Loss: 0.04579271376132965\n",
      "Epoch 12460/30000 Training Loss: 0.05297792702913284\n",
      "Epoch 12461/30000 Training Loss: 0.04856926202774048\n",
      "Epoch 12462/30000 Training Loss: 0.05363117903470993\n",
      "Epoch 12463/30000 Training Loss: 0.0594308078289032\n",
      "Epoch 12464/30000 Training Loss: 0.06599882245063782\n",
      "Epoch 12465/30000 Training Loss: 0.043472565710544586\n",
      "Epoch 12466/30000 Training Loss: 0.04356774315237999\n",
      "Epoch 12467/30000 Training Loss: 0.05602896958589554\n",
      "Epoch 12468/30000 Training Loss: 0.05072144791483879\n",
      "Epoch 12469/30000 Training Loss: 0.044387176632881165\n",
      "Epoch 12470/30000 Training Loss: 0.049622081220149994\n",
      "Epoch 12471/30000 Training Loss: 0.06297812610864639\n",
      "Epoch 12472/30000 Training Loss: 0.058440931141376495\n",
      "Epoch 12473/30000 Training Loss: 0.044803518801927567\n",
      "Epoch 12474/30000 Training Loss: 0.04807049036026001\n",
      "Epoch 12475/30000 Training Loss: 0.03885352984070778\n",
      "Epoch 12476/30000 Training Loss: 0.052075307816267014\n",
      "Epoch 12477/30000 Training Loss: 0.04573587328195572\n",
      "Epoch 12478/30000 Training Loss: 0.06336179375648499\n",
      "Epoch 12479/30000 Training Loss: 0.04181516543030739\n",
      "Epoch 12480/30000 Training Loss: 0.04922017082571983\n",
      "Epoch 12481/30000 Training Loss: 0.04769106209278107\n",
      "Epoch 12482/30000 Training Loss: 0.0472414493560791\n",
      "Epoch 12483/30000 Training Loss: 0.05607268214225769\n",
      "Epoch 12484/30000 Training Loss: 0.05527336895465851\n",
      "Epoch 12485/30000 Training Loss: 0.045527778565883636\n",
      "Epoch 12486/30000 Training Loss: 0.04394751042127609\n",
      "Epoch 12487/30000 Training Loss: 0.08994167298078537\n",
      "Epoch 12488/30000 Training Loss: 0.054122067987918854\n",
      "Epoch 12489/30000 Training Loss: 0.0354536771774292\n",
      "Epoch 12490/30000 Training Loss: 0.03577999398112297\n",
      "Epoch 12491/30000 Training Loss: 0.06559352576732635\n",
      "Epoch 12492/30000 Training Loss: 0.05475492775440216\n",
      "Epoch 12493/30000 Training Loss: 0.044530075043439865\n",
      "Epoch 12494/30000 Training Loss: 0.04527253285050392\n",
      "Epoch 12495/30000 Training Loss: 0.050444021821022034\n",
      "Epoch 12496/30000 Training Loss: 0.04276493936777115\n",
      "Epoch 12497/30000 Training Loss: 0.04288232699036598\n",
      "Epoch 12498/30000 Training Loss: 0.05452113598585129\n",
      "Epoch 12499/30000 Training Loss: 0.06067056953907013\n",
      "Epoch 12500/30000 Training Loss: 0.060826435685157776\n",
      "Epoch 12500/30000 Validation Loss: 0.05520020052790642\n",
      "Epoch 12501/30000 Training Loss: 0.03730778023600578\n",
      "Epoch 12502/30000 Training Loss: 0.0735575407743454\n",
      "Epoch 12503/30000 Training Loss: 0.03385862708091736\n",
      "Epoch 12504/30000 Training Loss: 0.06822513043880463\n",
      "Epoch 12505/30000 Training Loss: 0.043835848569869995\n",
      "Epoch 12506/30000 Training Loss: 0.05309651419520378\n",
      "Epoch 12507/30000 Training Loss: 0.05775865912437439\n",
      "Epoch 12508/30000 Training Loss: 0.05089607462286949\n",
      "Epoch 12509/30000 Training Loss: 0.06762334704399109\n",
      "Epoch 12510/30000 Training Loss: 0.0492541566491127\n",
      "Epoch 12511/30000 Training Loss: 0.04426611214876175\n",
      "Epoch 12512/30000 Training Loss: 0.05082571506500244\n",
      "Epoch 12513/30000 Training Loss: 0.039742361754179\n",
      "Epoch 12514/30000 Training Loss: 0.050572916865348816\n",
      "Epoch 12515/30000 Training Loss: 0.045932475477457047\n",
      "Epoch 12516/30000 Training Loss: 0.06503436714410782\n",
      "Epoch 12517/30000 Training Loss: 0.045208849012851715\n",
      "Epoch 12518/30000 Training Loss: 0.0570417195558548\n",
      "Epoch 12519/30000 Training Loss: 0.049878351390361786\n",
      "Epoch 12520/30000 Training Loss: 0.048833154141902924\n",
      "Epoch 12521/30000 Training Loss: 0.05147416889667511\n",
      "Epoch 12522/30000 Training Loss: 0.0457436740398407\n",
      "Epoch 12523/30000 Training Loss: 0.04008481651544571\n",
      "Epoch 12524/30000 Training Loss: 0.05271115154027939\n",
      "Epoch 12525/30000 Training Loss: 0.05377485603094101\n",
      "Epoch 12526/30000 Training Loss: 0.06191758066415787\n",
      "Epoch 12527/30000 Training Loss: 0.057264894247055054\n",
      "Epoch 12528/30000 Training Loss: 0.04649076610803604\n",
      "Epoch 12529/30000 Training Loss: 0.047347504645586014\n",
      "Epoch 12530/30000 Training Loss: 0.054223135113716125\n",
      "Epoch 12531/30000 Training Loss: 0.05523168668150902\n",
      "Epoch 12532/30000 Training Loss: 0.036434248089790344\n",
      "Epoch 12533/30000 Training Loss: 0.06361017376184464\n",
      "Epoch 12534/30000 Training Loss: 0.03923604637384415\n",
      "Epoch 12535/30000 Training Loss: 0.05689607933163643\n",
      "Epoch 12536/30000 Training Loss: 0.04382859170436859\n",
      "Epoch 12537/30000 Training Loss: 0.04591840133070946\n",
      "Epoch 12538/30000 Training Loss: 0.051680877804756165\n",
      "Epoch 12539/30000 Training Loss: 0.0494479238986969\n",
      "Epoch 12540/30000 Training Loss: 0.05010423809289932\n",
      "Epoch 12541/30000 Training Loss: 0.05071748048067093\n",
      "Epoch 12542/30000 Training Loss: 0.052999041974544525\n",
      "Epoch 12543/30000 Training Loss: 0.048231594264507294\n",
      "Epoch 12544/30000 Training Loss: 0.04941108822822571\n",
      "Epoch 12545/30000 Training Loss: 0.0562208816409111\n",
      "Epoch 12546/30000 Training Loss: 0.03723672032356262\n",
      "Epoch 12547/30000 Training Loss: 0.04758813977241516\n",
      "Epoch 12548/30000 Training Loss: 0.05870829522609711\n",
      "Epoch 12549/30000 Training Loss: 0.05494760349392891\n",
      "Epoch 12550/30000 Training Loss: 0.04652442783117294\n",
      "Epoch 12551/30000 Training Loss: 0.051928870379924774\n",
      "Epoch 12552/30000 Training Loss: 0.048469096422195435\n",
      "Epoch 12553/30000 Training Loss: 0.05861052870750427\n",
      "Epoch 12554/30000 Training Loss: 0.05007554963231087\n",
      "Epoch 12555/30000 Training Loss: 0.05233354866504669\n",
      "Epoch 12556/30000 Training Loss: 0.06760062277317047\n",
      "Epoch 12557/30000 Training Loss: 0.06306740641593933\n",
      "Epoch 12558/30000 Training Loss: 0.049138348549604416\n",
      "Epoch 12559/30000 Training Loss: 0.05138197913765907\n",
      "Epoch 12560/30000 Training Loss: 0.06660489737987518\n",
      "Epoch 12561/30000 Training Loss: 0.052070870995521545\n",
      "Epoch 12562/30000 Training Loss: 0.0623311772942543\n",
      "Epoch 12563/30000 Training Loss: 0.04137682169675827\n",
      "Epoch 12564/30000 Training Loss: 0.05214397609233856\n",
      "Epoch 12565/30000 Training Loss: 0.047879576683044434\n",
      "Epoch 12566/30000 Training Loss: 0.0722227543592453\n",
      "Epoch 12567/30000 Training Loss: 0.06251274049282074\n",
      "Epoch 12568/30000 Training Loss: 0.05174570530653\n",
      "Epoch 12569/30000 Training Loss: 0.040949657559394836\n",
      "Epoch 12570/30000 Training Loss: 0.05502086877822876\n",
      "Epoch 12571/30000 Training Loss: 0.048613592982292175\n",
      "Epoch 12572/30000 Training Loss: 0.04391392692923546\n",
      "Epoch 12573/30000 Training Loss: 0.058245331048965454\n",
      "Epoch 12574/30000 Training Loss: 0.049553923308849335\n",
      "Epoch 12575/30000 Training Loss: 0.04834795743227005\n",
      "Epoch 12576/30000 Training Loss: 0.04674903303384781\n",
      "Epoch 12577/30000 Training Loss: 0.04483449459075928\n",
      "Epoch 12578/30000 Training Loss: 0.04717680439352989\n",
      "Epoch 12579/30000 Training Loss: 0.05438093841075897\n",
      "Epoch 12580/30000 Training Loss: 0.04783889651298523\n",
      "Epoch 12581/30000 Training Loss: 0.06303749978542328\n",
      "Epoch 12582/30000 Training Loss: 0.05203574523329735\n",
      "Epoch 12583/30000 Training Loss: 0.0330832302570343\n",
      "Epoch 12584/30000 Training Loss: 0.056565653532743454\n",
      "Epoch 12585/30000 Training Loss: 0.07254862040281296\n",
      "Epoch 12586/30000 Training Loss: 0.05721452087163925\n",
      "Epoch 12587/30000 Training Loss: 0.04057322070002556\n",
      "Epoch 12588/30000 Training Loss: 0.041864607483148575\n",
      "Epoch 12589/30000 Training Loss: 0.06481912732124329\n",
      "Epoch 12590/30000 Training Loss: 0.06581667810678482\n",
      "Epoch 12591/30000 Training Loss: 0.058698683977127075\n",
      "Epoch 12592/30000 Training Loss: 0.04301111027598381\n",
      "Epoch 12593/30000 Training Loss: 0.06219710409641266\n",
      "Epoch 12594/30000 Training Loss: 0.06738214194774628\n",
      "Epoch 12595/30000 Training Loss: 0.04731133580207825\n",
      "Epoch 12596/30000 Training Loss: 0.05972084775567055\n",
      "Epoch 12597/30000 Training Loss: 0.051951825618743896\n",
      "Epoch 12598/30000 Training Loss: 0.04036624729633331\n",
      "Epoch 12599/30000 Training Loss: 0.05139820650219917\n",
      "Epoch 12600/30000 Training Loss: 0.0492415726184845\n",
      "Epoch 12600/30000 Validation Loss: 0.05366997420787811\n",
      "Epoch 12601/30000 Training Loss: 0.05219222605228424\n",
      "Epoch 12602/30000 Training Loss: 0.03504472225904465\n",
      "Epoch 12603/30000 Training Loss: 0.03503517434000969\n",
      "Epoch 12604/30000 Training Loss: 0.05855704843997955\n",
      "Epoch 12605/30000 Training Loss: 0.056844402104616165\n",
      "Epoch 12606/30000 Training Loss: 0.04763924330472946\n",
      "Epoch 12607/30000 Training Loss: 0.04877028614282608\n",
      "Epoch 12608/30000 Training Loss: 0.03931799903512001\n",
      "Epoch 12609/30000 Training Loss: 0.044100772589445114\n",
      "Epoch 12610/30000 Training Loss: 0.038628917187452316\n",
      "Epoch 12611/30000 Training Loss: 0.051988113671541214\n",
      "Epoch 12612/30000 Training Loss: 0.04884306341409683\n",
      "Epoch 12613/30000 Training Loss: 0.046932317316532135\n",
      "Epoch 12614/30000 Training Loss: 0.04994184523820877\n",
      "Epoch 12615/30000 Training Loss: 0.0487995408475399\n",
      "Epoch 12616/30000 Training Loss: 0.03940461203455925\n",
      "Epoch 12617/30000 Training Loss: 0.051267948001623154\n",
      "Epoch 12618/30000 Training Loss: 0.04930237680673599\n",
      "Epoch 12619/30000 Training Loss: 0.06216919422149658\n",
      "Epoch 12620/30000 Training Loss: 0.052096325904130936\n",
      "Epoch 12621/30000 Training Loss: 0.06379777938127518\n",
      "Epoch 12622/30000 Training Loss: 0.06780419498682022\n",
      "Epoch 12623/30000 Training Loss: 0.04969976842403412\n",
      "Epoch 12624/30000 Training Loss: 0.04600730165839195\n",
      "Epoch 12625/30000 Training Loss: 0.06573536247015\n",
      "Epoch 12626/30000 Training Loss: 0.04940702021121979\n",
      "Epoch 12627/30000 Training Loss: 0.04812709614634514\n",
      "Epoch 12628/30000 Training Loss: 0.04674123227596283\n",
      "Epoch 12629/30000 Training Loss: 0.062085215002298355\n",
      "Epoch 12630/30000 Training Loss: 0.07316981256008148\n",
      "Epoch 12631/30000 Training Loss: 0.06627174466848373\n",
      "Epoch 12632/30000 Training Loss: 0.03854897618293762\n",
      "Epoch 12633/30000 Training Loss: 0.04233488067984581\n",
      "Epoch 12634/30000 Training Loss: 0.04123172163963318\n",
      "Epoch 12635/30000 Training Loss: 0.04981784522533417\n",
      "Epoch 12636/30000 Training Loss: 0.03669502213597298\n",
      "Epoch 12637/30000 Training Loss: 0.050769347697496414\n",
      "Epoch 12638/30000 Training Loss: 0.07426488399505615\n",
      "Epoch 12639/30000 Training Loss: 0.06612289696931839\n",
      "Epoch 12640/30000 Training Loss: 0.05526449903845787\n",
      "Epoch 12641/30000 Training Loss: 0.058083426207304\n",
      "Epoch 12642/30000 Training Loss: 0.06952905654907227\n",
      "Epoch 12643/30000 Training Loss: 0.05189283937215805\n",
      "Epoch 12644/30000 Training Loss: 0.04186892509460449\n",
      "Epoch 12645/30000 Training Loss: 0.07301636785268784\n",
      "Epoch 12646/30000 Training Loss: 0.06944479048252106\n",
      "Epoch 12647/30000 Training Loss: 0.0471670925617218\n",
      "Epoch 12648/30000 Training Loss: 0.047816045582294464\n",
      "Epoch 12649/30000 Training Loss: 0.05725453794002533\n",
      "Epoch 12650/30000 Training Loss: 0.0537404865026474\n",
      "Epoch 12651/30000 Training Loss: 0.04919729381799698\n",
      "Epoch 12652/30000 Training Loss: 0.05900101363658905\n",
      "Epoch 12653/30000 Training Loss: 0.04416783154010773\n",
      "Epoch 12654/30000 Training Loss: 0.05479096993803978\n",
      "Epoch 12655/30000 Training Loss: 0.05834037810564041\n",
      "Epoch 12656/30000 Training Loss: 0.05017111450433731\n",
      "Epoch 12657/30000 Training Loss: 0.049547430127859116\n",
      "Epoch 12658/30000 Training Loss: 0.05815529823303223\n",
      "Epoch 12659/30000 Training Loss: 0.05192137137055397\n",
      "Epoch 12660/30000 Training Loss: 0.04610173776745796\n",
      "Epoch 12661/30000 Training Loss: 0.05409762263298035\n",
      "Epoch 12662/30000 Training Loss: 0.0610387921333313\n",
      "Epoch 12663/30000 Training Loss: 0.043712079524993896\n",
      "Epoch 12664/30000 Training Loss: 0.050570134073495865\n",
      "Epoch 12665/30000 Training Loss: 0.055596306920051575\n",
      "Epoch 12666/30000 Training Loss: 0.050780341029167175\n",
      "Epoch 12667/30000 Training Loss: 0.042332932353019714\n",
      "Epoch 12668/30000 Training Loss: 0.05250254645943642\n",
      "Epoch 12669/30000 Training Loss: 0.0602201446890831\n",
      "Epoch 12670/30000 Training Loss: 0.04196900874376297\n",
      "Epoch 12671/30000 Training Loss: 0.059767406433820724\n",
      "Epoch 12672/30000 Training Loss: 0.05473419651389122\n",
      "Epoch 12673/30000 Training Loss: 0.041143231093883514\n",
      "Epoch 12674/30000 Training Loss: 0.03640083968639374\n",
      "Epoch 12675/30000 Training Loss: 0.04812578484416008\n",
      "Epoch 12676/30000 Training Loss: 0.0537116639316082\n",
      "Epoch 12677/30000 Training Loss: 0.06399408727884293\n",
      "Epoch 12678/30000 Training Loss: 0.05415560305118561\n",
      "Epoch 12679/30000 Training Loss: 0.054562363773584366\n",
      "Epoch 12680/30000 Training Loss: 0.05655953288078308\n",
      "Epoch 12681/30000 Training Loss: 0.04337742179632187\n",
      "Epoch 12682/30000 Training Loss: 0.05043130740523338\n",
      "Epoch 12683/30000 Training Loss: 0.079053595662117\n",
      "Epoch 12684/30000 Training Loss: 0.047612253576517105\n",
      "Epoch 12685/30000 Training Loss: 0.06865944713354111\n",
      "Epoch 12686/30000 Training Loss: 0.058975864201784134\n",
      "Epoch 12687/30000 Training Loss: 0.05265368893742561\n",
      "Epoch 12688/30000 Training Loss: 0.04522702097892761\n",
      "Epoch 12689/30000 Training Loss: 0.03470762073993683\n",
      "Epoch 12690/30000 Training Loss: 0.04531288146972656\n",
      "Epoch 12691/30000 Training Loss: 0.05195822939276695\n",
      "Epoch 12692/30000 Training Loss: 0.0405934602022171\n",
      "Epoch 12693/30000 Training Loss: 0.05135761946439743\n",
      "Epoch 12694/30000 Training Loss: 0.0760817751288414\n",
      "Epoch 12695/30000 Training Loss: 0.04541458189487457\n",
      "Epoch 12696/30000 Training Loss: 0.049048278480768204\n",
      "Epoch 12697/30000 Training Loss: 0.0568569079041481\n",
      "Epoch 12698/30000 Training Loss: 0.058256641030311584\n",
      "Epoch 12699/30000 Training Loss: 0.06436848640441895\n",
      "Epoch 12700/30000 Training Loss: 0.06611253321170807\n",
      "Epoch 12700/30000 Validation Loss: 0.06699755787849426\n",
      "Epoch 12701/30000 Training Loss: 0.06151415407657623\n",
      "Epoch 12702/30000 Training Loss: 0.03604191169142723\n",
      "Epoch 12703/30000 Training Loss: 0.0542185977101326\n",
      "Epoch 12704/30000 Training Loss: 0.05210467800498009\n",
      "Epoch 12705/30000 Training Loss: 0.054945483803749084\n",
      "Epoch 12706/30000 Training Loss: 0.045421820133924484\n",
      "Epoch 12707/30000 Training Loss: 0.04787173867225647\n",
      "Epoch 12708/30000 Training Loss: 0.048540808260440826\n",
      "Epoch 12709/30000 Training Loss: 0.052619945257902145\n",
      "Epoch 12710/30000 Training Loss: 0.046090371906757355\n",
      "Epoch 12711/30000 Training Loss: 0.04786614328622818\n",
      "Epoch 12712/30000 Training Loss: 0.048695869743824005\n",
      "Epoch 12713/30000 Training Loss: 0.07107415795326233\n",
      "Epoch 12714/30000 Training Loss: 0.05250268429517746\n",
      "Epoch 12715/30000 Training Loss: 0.050872594118118286\n",
      "Epoch 12716/30000 Training Loss: 0.04528709501028061\n",
      "Epoch 12717/30000 Training Loss: 0.038337044417858124\n",
      "Epoch 12718/30000 Training Loss: 0.05516069382429123\n",
      "Epoch 12719/30000 Training Loss: 0.054965078830718994\n",
      "Epoch 12720/30000 Training Loss: 0.05643414705991745\n",
      "Epoch 12721/30000 Training Loss: 0.06088322401046753\n",
      "Epoch 12722/30000 Training Loss: 0.04172747954726219\n",
      "Epoch 12723/30000 Training Loss: 0.05385451763868332\n",
      "Epoch 12724/30000 Training Loss: 0.058931492269039154\n",
      "Epoch 12725/30000 Training Loss: 0.041003867983818054\n",
      "Epoch 12726/30000 Training Loss: 0.059243448078632355\n",
      "Epoch 12727/30000 Training Loss: 0.06024198606610298\n",
      "Epoch 12728/30000 Training Loss: 0.05076809972524643\n",
      "Epoch 12729/30000 Training Loss: 0.05193239450454712\n",
      "Epoch 12730/30000 Training Loss: 0.05380883067846298\n",
      "Epoch 12731/30000 Training Loss: 0.03977981209754944\n",
      "Epoch 12732/30000 Training Loss: 0.05321958661079407\n",
      "Epoch 12733/30000 Training Loss: 0.061108242720365524\n",
      "Epoch 12734/30000 Training Loss: 0.05216429755091667\n",
      "Epoch 12735/30000 Training Loss: 0.044955745339393616\n",
      "Epoch 12736/30000 Training Loss: 0.06216392293572426\n",
      "Epoch 12737/30000 Training Loss: 0.05631626397371292\n",
      "Epoch 12738/30000 Training Loss: 0.046111539006233215\n",
      "Epoch 12739/30000 Training Loss: 0.04750050976872444\n",
      "Epoch 12740/30000 Training Loss: 0.04817172512412071\n",
      "Epoch 12741/30000 Training Loss: 0.05208197236061096\n",
      "Epoch 12742/30000 Training Loss: 0.060245707631111145\n",
      "Epoch 12743/30000 Training Loss: 0.05158523470163345\n",
      "Epoch 12744/30000 Training Loss: 0.04474785551428795\n",
      "Epoch 12745/30000 Training Loss: 0.05030030384659767\n",
      "Epoch 12746/30000 Training Loss: 0.04520684480667114\n",
      "Epoch 12747/30000 Training Loss: 0.043076399713754654\n",
      "Epoch 12748/30000 Training Loss: 0.05418067425489426\n",
      "Epoch 12749/30000 Training Loss: 0.051300760358572006\n",
      "Epoch 12750/30000 Training Loss: 0.048536524176597595\n",
      "Epoch 12751/30000 Training Loss: 0.0441589318215847\n",
      "Epoch 12752/30000 Training Loss: 0.043939974159002304\n",
      "Epoch 12753/30000 Training Loss: 0.04615839943289757\n",
      "Epoch 12754/30000 Training Loss: 0.06430201232433319\n",
      "Epoch 12755/30000 Training Loss: 0.06932168453931808\n",
      "Epoch 12756/30000 Training Loss: 0.05574275553226471\n",
      "Epoch 12757/30000 Training Loss: 0.044835228472948074\n",
      "Epoch 12758/30000 Training Loss: 0.05473565310239792\n",
      "Epoch 12759/30000 Training Loss: 0.05127870291471481\n",
      "Epoch 12760/30000 Training Loss: 0.05187855660915375\n",
      "Epoch 12761/30000 Training Loss: 0.06388044357299805\n",
      "Epoch 12762/30000 Training Loss: 0.05273739993572235\n",
      "Epoch 12763/30000 Training Loss: 0.04656899720430374\n",
      "Epoch 12764/30000 Training Loss: 0.060912974178791046\n",
      "Epoch 12765/30000 Training Loss: 0.037214234471321106\n",
      "Epoch 12766/30000 Training Loss: 0.0462489128112793\n",
      "Epoch 12767/30000 Training Loss: 0.042397260665893555\n",
      "Epoch 12768/30000 Training Loss: 0.0509316623210907\n",
      "Epoch 12769/30000 Training Loss: 0.048762984573841095\n",
      "Epoch 12770/30000 Training Loss: 0.04878738522529602\n",
      "Epoch 12771/30000 Training Loss: 0.045247048139572144\n",
      "Epoch 12772/30000 Training Loss: 0.04643625393509865\n",
      "Epoch 12773/30000 Training Loss: 0.0616193450987339\n",
      "Epoch 12774/30000 Training Loss: 0.04596978425979614\n",
      "Epoch 12775/30000 Training Loss: 0.06894847750663757\n",
      "Epoch 12776/30000 Training Loss: 0.04760740324854851\n",
      "Epoch 12777/30000 Training Loss: 0.04763899743556976\n",
      "Epoch 12778/30000 Training Loss: 0.04699806869029999\n",
      "Epoch 12779/30000 Training Loss: 0.04380660504102707\n",
      "Epoch 12780/30000 Training Loss: 0.04347243905067444\n",
      "Epoch 12781/30000 Training Loss: 0.050936780869960785\n",
      "Epoch 12782/30000 Training Loss: 0.047987427562475204\n",
      "Epoch 12783/30000 Training Loss: 0.03686414659023285\n",
      "Epoch 12784/30000 Training Loss: 0.03989988565444946\n",
      "Epoch 12785/30000 Training Loss: 0.053332291543483734\n",
      "Epoch 12786/30000 Training Loss: 0.0440642386674881\n",
      "Epoch 12787/30000 Training Loss: 0.049900006502866745\n",
      "Epoch 12788/30000 Training Loss: 0.04191770404577255\n",
      "Epoch 12789/30000 Training Loss: 0.044149067252874374\n",
      "Epoch 12790/30000 Training Loss: 0.06407255679368973\n",
      "Epoch 12791/30000 Training Loss: 0.05015894025564194\n",
      "Epoch 12792/30000 Training Loss: 0.054148852825164795\n",
      "Epoch 12793/30000 Training Loss: 0.043561436235904694\n",
      "Epoch 12794/30000 Training Loss: 0.05447559431195259\n",
      "Epoch 12795/30000 Training Loss: 0.06151238828897476\n",
      "Epoch 12796/30000 Training Loss: 0.0671715959906578\n",
      "Epoch 12797/30000 Training Loss: 0.0712134838104248\n",
      "Epoch 12798/30000 Training Loss: 0.05308316648006439\n",
      "Epoch 12799/30000 Training Loss: 0.06925046443939209\n",
      "Epoch 12800/30000 Training Loss: 0.05072924867272377\n",
      "Epoch 12800/30000 Validation Loss: 0.04978062957525253\n",
      "Epoch 12801/30000 Training Loss: 0.056320227682590485\n",
      "Epoch 12802/30000 Training Loss: 0.049089014530181885\n",
      "Epoch 12803/30000 Training Loss: 0.05605567246675491\n",
      "Epoch 12804/30000 Training Loss: 0.049335695803165436\n",
      "Epoch 12805/30000 Training Loss: 0.048510827124118805\n",
      "Epoch 12806/30000 Training Loss: 0.047752607613801956\n",
      "Epoch 12807/30000 Training Loss: 0.041554734110832214\n",
      "Epoch 12808/30000 Training Loss: 0.04976747930049896\n",
      "Epoch 12809/30000 Training Loss: 0.04721373692154884\n",
      "Epoch 12810/30000 Training Loss: 0.049751922488212585\n",
      "Epoch 12811/30000 Training Loss: 0.06607184559106827\n",
      "Epoch 12812/30000 Training Loss: 0.05235082656145096\n",
      "Epoch 12813/30000 Training Loss: 0.04710840806365013\n",
      "Epoch 12814/30000 Training Loss: 0.05431484803557396\n",
      "Epoch 12815/30000 Training Loss: 0.06250549107789993\n",
      "Epoch 12816/30000 Training Loss: 0.048673272132873535\n",
      "Epoch 12817/30000 Training Loss: 0.03908723592758179\n",
      "Epoch 12818/30000 Training Loss: 0.033296070992946625\n",
      "Epoch 12819/30000 Training Loss: 0.05544300377368927\n",
      "Epoch 12820/30000 Training Loss: 0.07072343677282333\n",
      "Epoch 12821/30000 Training Loss: 0.05844653397798538\n",
      "Epoch 12822/30000 Training Loss: 0.03505323827266693\n",
      "Epoch 12823/30000 Training Loss: 0.05071120336651802\n",
      "Epoch 12824/30000 Training Loss: 0.05560610070824623\n",
      "Epoch 12825/30000 Training Loss: 0.03818058222532272\n",
      "Epoch 12826/30000 Training Loss: 0.047902971506118774\n",
      "Epoch 12827/30000 Training Loss: 0.048509471118450165\n",
      "Epoch 12828/30000 Training Loss: 0.045237988233566284\n",
      "Epoch 12829/30000 Training Loss: 0.044408101588487625\n",
      "Epoch 12830/30000 Training Loss: 0.03645111247897148\n",
      "Epoch 12831/30000 Training Loss: 0.049604274332523346\n",
      "Epoch 12832/30000 Training Loss: 0.06215345114469528\n",
      "Epoch 12833/30000 Training Loss: 0.044135548174381256\n",
      "Epoch 12834/30000 Training Loss: 0.0688672736287117\n",
      "Epoch 12835/30000 Training Loss: 0.04033114016056061\n",
      "Epoch 12836/30000 Training Loss: 0.05437225103378296\n",
      "Epoch 12837/30000 Training Loss: 0.04787697270512581\n",
      "Epoch 12838/30000 Training Loss: 0.04219457134604454\n",
      "Epoch 12839/30000 Training Loss: 0.059466272592544556\n",
      "Epoch 12840/30000 Training Loss: 0.047746557742357254\n",
      "Epoch 12841/30000 Training Loss: 0.038211822509765625\n",
      "Epoch 12842/30000 Training Loss: 0.060967788100242615\n",
      "Epoch 12843/30000 Training Loss: 0.05004222318530083\n",
      "Epoch 12844/30000 Training Loss: 0.03700850531458855\n",
      "Epoch 12845/30000 Training Loss: 0.05232393741607666\n",
      "Epoch 12846/30000 Training Loss: 0.05236533656716347\n",
      "Epoch 12847/30000 Training Loss: 0.04999716207385063\n",
      "Epoch 12848/30000 Training Loss: 0.03686779364943504\n",
      "Epoch 12849/30000 Training Loss: 0.05166592821478844\n",
      "Epoch 12850/30000 Training Loss: 0.044216327369213104\n",
      "Epoch 12851/30000 Training Loss: 0.07571042329072952\n",
      "Epoch 12852/30000 Training Loss: 0.05265839025378227\n",
      "Epoch 12853/30000 Training Loss: 0.0468168742954731\n",
      "Epoch 12854/30000 Training Loss: 0.04199789837002754\n",
      "Epoch 12855/30000 Training Loss: 0.05997053161263466\n",
      "Epoch 12856/30000 Training Loss: 0.04394040256738663\n",
      "Epoch 12857/30000 Training Loss: 0.06469148397445679\n",
      "Epoch 12858/30000 Training Loss: 0.04611102491617203\n",
      "Epoch 12859/30000 Training Loss: 0.042650945484638214\n",
      "Epoch 12860/30000 Training Loss: 0.0482434444129467\n",
      "Epoch 12861/30000 Training Loss: 0.045131802558898926\n",
      "Epoch 12862/30000 Training Loss: 0.04444147273898125\n",
      "Epoch 12863/30000 Training Loss: 0.06779816001653671\n",
      "Epoch 12864/30000 Training Loss: 0.06402521580457687\n",
      "Epoch 12865/30000 Training Loss: 0.04312444105744362\n",
      "Epoch 12866/30000 Training Loss: 0.052257683128118515\n",
      "Epoch 12867/30000 Training Loss: 0.03968745097517967\n",
      "Epoch 12868/30000 Training Loss: 0.05698242411017418\n",
      "Epoch 12869/30000 Training Loss: 0.05194350332021713\n",
      "Epoch 12870/30000 Training Loss: 0.04009740799665451\n",
      "Epoch 12871/30000 Training Loss: 0.046714261174201965\n",
      "Epoch 12872/30000 Training Loss: 0.043694302439689636\n",
      "Epoch 12873/30000 Training Loss: 0.03948056325316429\n",
      "Epoch 12874/30000 Training Loss: 0.06462275236845016\n",
      "Epoch 12875/30000 Training Loss: 0.06550286710262299\n",
      "Epoch 12876/30000 Training Loss: 0.05084071308374405\n",
      "Epoch 12877/30000 Training Loss: 0.06287218630313873\n",
      "Epoch 12878/30000 Training Loss: 0.05461469665169716\n",
      "Epoch 12879/30000 Training Loss: 0.07217226922512054\n",
      "Epoch 12880/30000 Training Loss: 0.061457835137844086\n",
      "Epoch 12881/30000 Training Loss: 0.05321546643972397\n",
      "Epoch 12882/30000 Training Loss: 0.037870731204748154\n",
      "Epoch 12883/30000 Training Loss: 0.05520187318325043\n",
      "Epoch 12884/30000 Training Loss: 0.05168576538562775\n",
      "Epoch 12885/30000 Training Loss: 0.041593678295612335\n",
      "Epoch 12886/30000 Training Loss: 0.05113404244184494\n",
      "Epoch 12887/30000 Training Loss: 0.051378704607486725\n",
      "Epoch 12888/30000 Training Loss: 0.0319964773952961\n",
      "Epoch 12889/30000 Training Loss: 0.056739069521427155\n",
      "Epoch 12890/30000 Training Loss: 0.03666424751281738\n",
      "Epoch 12891/30000 Training Loss: 0.050870899111032486\n",
      "Epoch 12892/30000 Training Loss: 0.05181460455060005\n",
      "Epoch 12893/30000 Training Loss: 0.05402981489896774\n",
      "Epoch 12894/30000 Training Loss: 0.036358390003442764\n",
      "Epoch 12895/30000 Training Loss: 0.05687549710273743\n",
      "Epoch 12896/30000 Training Loss: 0.0474226213991642\n",
      "Epoch 12897/30000 Training Loss: 0.06186536327004433\n",
      "Epoch 12898/30000 Training Loss: 0.03691188991069794\n",
      "Epoch 12899/30000 Training Loss: 0.052420929074287415\n",
      "Epoch 12900/30000 Training Loss: 0.052217308431863785\n",
      "Epoch 12900/30000 Validation Loss: 0.06990975141525269\n",
      "Epoch 12901/30000 Training Loss: 0.04160676151514053\n",
      "Epoch 12902/30000 Training Loss: 0.0410909503698349\n",
      "Epoch 12903/30000 Training Loss: 0.04988907277584076\n",
      "Epoch 12904/30000 Training Loss: 0.0529337115585804\n",
      "Epoch 12905/30000 Training Loss: 0.03823681175708771\n",
      "Epoch 12906/30000 Training Loss: 0.04131407290697098\n",
      "Epoch 12907/30000 Training Loss: 0.048526789993047714\n",
      "Epoch 12908/30000 Training Loss: 0.045387037098407745\n",
      "Epoch 12909/30000 Training Loss: 0.04406888037919998\n",
      "Epoch 12910/30000 Training Loss: 0.05071263015270233\n",
      "Epoch 12911/30000 Training Loss: 0.06130069121718407\n",
      "Epoch 12912/30000 Training Loss: 0.041691891849040985\n",
      "Epoch 12913/30000 Training Loss: 0.05623893067240715\n",
      "Epoch 12914/30000 Training Loss: 0.06406992673873901\n",
      "Epoch 12915/30000 Training Loss: 0.043545108288526535\n",
      "Epoch 12916/30000 Training Loss: 0.049524251371622086\n",
      "Epoch 12917/30000 Training Loss: 0.05789142847061157\n",
      "Epoch 12918/30000 Training Loss: 0.042664412409067154\n",
      "Epoch 12919/30000 Training Loss: 0.05479703098535538\n",
      "Epoch 12920/30000 Training Loss: 0.04863604158163071\n",
      "Epoch 12921/30000 Training Loss: 0.06805955618619919\n",
      "Epoch 12922/30000 Training Loss: 0.06050935387611389\n",
      "Epoch 12923/30000 Training Loss: 0.055094312876462936\n",
      "Epoch 12924/30000 Training Loss: 0.04566233977675438\n",
      "Epoch 12925/30000 Training Loss: 0.04882426932454109\n",
      "Epoch 12926/30000 Training Loss: 0.035353824496269226\n",
      "Epoch 12927/30000 Training Loss: 0.04725940525531769\n",
      "Epoch 12928/30000 Training Loss: 0.04825481399893761\n",
      "Epoch 12929/30000 Training Loss: 0.04787948727607727\n",
      "Epoch 12930/30000 Training Loss: 0.06508761644363403\n",
      "Epoch 12931/30000 Training Loss: 0.04885439947247505\n",
      "Epoch 12932/30000 Training Loss: 0.04606112092733383\n",
      "Epoch 12933/30000 Training Loss: 0.04953954368829727\n",
      "Epoch 12934/30000 Training Loss: 0.049242082983255386\n",
      "Epoch 12935/30000 Training Loss: 0.049601875245571136\n",
      "Epoch 12936/30000 Training Loss: 0.047453343868255615\n",
      "Epoch 12937/30000 Training Loss: 0.055266886949539185\n",
      "Epoch 12938/30000 Training Loss: 0.04777614772319794\n",
      "Epoch 12939/30000 Training Loss: 0.05744152143597603\n",
      "Epoch 12940/30000 Training Loss: 0.05905165523290634\n",
      "Epoch 12941/30000 Training Loss: 0.06278810650110245\n",
      "Epoch 12942/30000 Training Loss: 0.050399865955114365\n",
      "Epoch 12943/30000 Training Loss: 0.05765024945139885\n",
      "Epoch 12944/30000 Training Loss: 0.06748904287815094\n",
      "Epoch 12945/30000 Training Loss: 0.04228055477142334\n",
      "Epoch 12946/30000 Training Loss: 0.044968441128730774\n",
      "Epoch 12947/30000 Training Loss: 0.04353300854563713\n",
      "Epoch 12948/30000 Training Loss: 0.05032530799508095\n",
      "Epoch 12949/30000 Training Loss: 0.05586515739560127\n",
      "Epoch 12950/30000 Training Loss: 0.038123585283756256\n",
      "Epoch 12951/30000 Training Loss: 0.052934084087610245\n",
      "Epoch 12952/30000 Training Loss: 0.06214354187250137\n",
      "Epoch 12953/30000 Training Loss: 0.036490801721811295\n",
      "Epoch 12954/30000 Training Loss: 0.08160354197025299\n",
      "Epoch 12955/30000 Training Loss: 0.07572972774505615\n",
      "Epoch 12956/30000 Training Loss: 0.04791912063956261\n",
      "Epoch 12957/30000 Training Loss: 0.06226476654410362\n",
      "Epoch 12958/30000 Training Loss: 0.055487945675849915\n",
      "Epoch 12959/30000 Training Loss: 0.04122890159487724\n",
      "Epoch 12960/30000 Training Loss: 0.07300475239753723\n",
      "Epoch 12961/30000 Training Loss: 0.04565044492483139\n",
      "Epoch 12962/30000 Training Loss: 0.04807703197002411\n",
      "Epoch 12963/30000 Training Loss: 0.042878709733486176\n",
      "Epoch 12964/30000 Training Loss: 0.05891326814889908\n",
      "Epoch 12965/30000 Training Loss: 0.049535393714904785\n",
      "Epoch 12966/30000 Training Loss: 0.05984184518456459\n",
      "Epoch 12967/30000 Training Loss: 0.039779916405677795\n",
      "Epoch 12968/30000 Training Loss: 0.04312202334403992\n",
      "Epoch 12969/30000 Training Loss: 0.05271681770682335\n",
      "Epoch 12970/30000 Training Loss: 0.06186909228563309\n",
      "Epoch 12971/30000 Training Loss: 0.047129567712545395\n",
      "Epoch 12972/30000 Training Loss: 0.06714321672916412\n",
      "Epoch 12973/30000 Training Loss: 0.05048828572034836\n",
      "Epoch 12974/30000 Training Loss: 0.06265212595462799\n",
      "Epoch 12975/30000 Training Loss: 0.0558282695710659\n",
      "Epoch 12976/30000 Training Loss: 0.058392234146595\n",
      "Epoch 12977/30000 Training Loss: 0.0471256859600544\n",
      "Epoch 12978/30000 Training Loss: 0.058481939136981964\n",
      "Epoch 12979/30000 Training Loss: 0.054247304797172546\n",
      "Epoch 12980/30000 Training Loss: 0.046357933431863785\n",
      "Epoch 12981/30000 Training Loss: 0.05769389867782593\n",
      "Epoch 12982/30000 Training Loss: 0.0488094724714756\n",
      "Epoch 12983/30000 Training Loss: 0.07030649483203888\n",
      "Epoch 12984/30000 Training Loss: 0.05077731981873512\n",
      "Epoch 12985/30000 Training Loss: 0.05212610587477684\n",
      "Epoch 12986/30000 Training Loss: 0.058554984629154205\n",
      "Epoch 12987/30000 Training Loss: 0.041495293378829956\n",
      "Epoch 12988/30000 Training Loss: 0.06599841266870499\n",
      "Epoch 12989/30000 Training Loss: 0.06774317473173141\n",
      "Epoch 12990/30000 Training Loss: 0.05693655461072922\n",
      "Epoch 12991/30000 Training Loss: 0.05070256441831589\n",
      "Epoch 12992/30000 Training Loss: 0.0402095727622509\n",
      "Epoch 12993/30000 Training Loss: 0.07667237520217896\n",
      "Epoch 12994/30000 Training Loss: 0.0439428985118866\n",
      "Epoch 12995/30000 Training Loss: 0.05067766085267067\n",
      "Epoch 12996/30000 Training Loss: 0.05020854249596596\n",
      "Epoch 12997/30000 Training Loss: 0.046368084847927094\n",
      "Epoch 12998/30000 Training Loss: 0.042842164635658264\n",
      "Epoch 12999/30000 Training Loss: 0.045178864151239395\n",
      "Epoch 13000/30000 Training Loss: 0.05099676549434662\n",
      "Epoch 13000/30000 Validation Loss: 0.05228074640035629\n",
      "Epoch 13001/30000 Training Loss: 0.053241755813360214\n",
      "Epoch 13002/30000 Training Loss: 0.059314630925655365\n",
      "Epoch 13003/30000 Training Loss: 0.0627116709947586\n",
      "Epoch 13004/30000 Training Loss: 0.05598219484090805\n",
      "Epoch 13005/30000 Training Loss: 0.05123817175626755\n",
      "Epoch 13006/30000 Training Loss: 0.04749556630849838\n",
      "Epoch 13007/30000 Training Loss: 0.062117476016283035\n",
      "Epoch 13008/30000 Training Loss: 0.04036368057131767\n",
      "Epoch 13009/30000 Training Loss: 0.04332396015524864\n",
      "Epoch 13010/30000 Training Loss: 0.05225301906466484\n",
      "Epoch 13011/30000 Training Loss: 0.057360097765922546\n",
      "Epoch 13012/30000 Training Loss: 0.043876178562641144\n",
      "Epoch 13013/30000 Training Loss: 0.04900696128606796\n",
      "Epoch 13014/30000 Training Loss: 0.03551299124956131\n",
      "Epoch 13015/30000 Training Loss: 0.038695916533470154\n",
      "Epoch 13016/30000 Training Loss: 0.04952913150191307\n",
      "Epoch 13017/30000 Training Loss: 0.05295693129301071\n",
      "Epoch 13018/30000 Training Loss: 0.048173919320106506\n",
      "Epoch 13019/30000 Training Loss: 0.051329225301742554\n",
      "Epoch 13020/30000 Training Loss: 0.058054860681295395\n",
      "Epoch 13021/30000 Training Loss: 0.05314026027917862\n",
      "Epoch 13022/30000 Training Loss: 0.04851412773132324\n",
      "Epoch 13023/30000 Training Loss: 0.048116929829120636\n",
      "Epoch 13024/30000 Training Loss: 0.0622841939330101\n",
      "Epoch 13025/30000 Training Loss: 0.06088663637638092\n",
      "Epoch 13026/30000 Training Loss: 0.05072314292192459\n",
      "Epoch 13027/30000 Training Loss: 0.05670808628201485\n",
      "Epoch 13028/30000 Training Loss: 0.04626385122537613\n",
      "Epoch 13029/30000 Training Loss: 0.047330353409051895\n",
      "Epoch 13030/30000 Training Loss: 0.07534000277519226\n",
      "Epoch 13031/30000 Training Loss: 0.055954087525606155\n",
      "Epoch 13032/30000 Training Loss: 0.04620720446109772\n",
      "Epoch 13033/30000 Training Loss: 0.049027394503355026\n",
      "Epoch 13034/30000 Training Loss: 0.047223255038261414\n",
      "Epoch 13035/30000 Training Loss: 0.036290332674980164\n",
      "Epoch 13036/30000 Training Loss: 0.048926934599876404\n",
      "Epoch 13037/30000 Training Loss: 0.06033948436379433\n",
      "Epoch 13038/30000 Training Loss: 0.06192907691001892\n",
      "Epoch 13039/30000 Training Loss: 0.05019146203994751\n",
      "Epoch 13040/30000 Training Loss: 0.048915520310401917\n",
      "Epoch 13041/30000 Training Loss: 0.06143874675035477\n",
      "Epoch 13042/30000 Training Loss: 0.07029646635055542\n",
      "Epoch 13043/30000 Training Loss: 0.057898763567209244\n",
      "Epoch 13044/30000 Training Loss: 0.04389205202460289\n",
      "Epoch 13045/30000 Training Loss: 0.06049862504005432\n",
      "Epoch 13046/30000 Training Loss: 0.05236966535449028\n",
      "Epoch 13047/30000 Training Loss: 0.058077480643987656\n",
      "Epoch 13048/30000 Training Loss: 0.04121895134449005\n",
      "Epoch 13049/30000 Training Loss: 0.053893376141786575\n",
      "Epoch 13050/30000 Training Loss: 0.05506020411849022\n",
      "Epoch 13051/30000 Training Loss: 0.05301682651042938\n",
      "Epoch 13052/30000 Training Loss: 0.045844633132219315\n",
      "Epoch 13053/30000 Training Loss: 0.04929668456315994\n",
      "Epoch 13054/30000 Training Loss: 0.04731280356645584\n",
      "Epoch 13055/30000 Training Loss: 0.04631645604968071\n",
      "Epoch 13056/30000 Training Loss: 0.043296657502651215\n",
      "Epoch 13057/30000 Training Loss: 0.06510095298290253\n",
      "Epoch 13058/30000 Training Loss: 0.05430169031023979\n",
      "Epoch 13059/30000 Training Loss: 0.04250093549489975\n",
      "Epoch 13060/30000 Training Loss: 0.057398661971092224\n",
      "Epoch 13061/30000 Training Loss: 0.03336155414581299\n",
      "Epoch 13062/30000 Training Loss: 0.06050916761159897\n",
      "Epoch 13063/30000 Training Loss: 0.06264784187078476\n",
      "Epoch 13064/30000 Training Loss: 0.04858100786805153\n",
      "Epoch 13065/30000 Training Loss: 0.05103982612490654\n",
      "Epoch 13066/30000 Training Loss: 0.037177376449108124\n",
      "Epoch 13067/30000 Training Loss: 0.05500272661447525\n",
      "Epoch 13068/30000 Training Loss: 0.03758181259036064\n",
      "Epoch 13069/30000 Training Loss: 0.05796852707862854\n",
      "Epoch 13070/30000 Training Loss: 0.04026047885417938\n",
      "Epoch 13071/30000 Training Loss: 0.06522652506828308\n",
      "Epoch 13072/30000 Training Loss: 0.04852086305618286\n",
      "Epoch 13073/30000 Training Loss: 0.06545337289571762\n",
      "Epoch 13074/30000 Training Loss: 0.03880898654460907\n",
      "Epoch 13075/30000 Training Loss: 0.06825457513332367\n",
      "Epoch 13076/30000 Training Loss: 0.04583930969238281\n",
      "Epoch 13077/30000 Training Loss: 0.0359797477722168\n",
      "Epoch 13078/30000 Training Loss: 0.054124727845191956\n",
      "Epoch 13079/30000 Training Loss: 0.053452104330062866\n",
      "Epoch 13080/30000 Training Loss: 0.0464593768119812\n",
      "Epoch 13081/30000 Training Loss: 0.043189048767089844\n",
      "Epoch 13082/30000 Training Loss: 0.04752422124147415\n",
      "Epoch 13083/30000 Training Loss: 0.05191424489021301\n",
      "Epoch 13084/30000 Training Loss: 0.0467660091817379\n",
      "Epoch 13085/30000 Training Loss: 0.04708264768123627\n",
      "Epoch 13086/30000 Training Loss: 0.0559895820915699\n",
      "Epoch 13087/30000 Training Loss: 0.046806152909994125\n",
      "Epoch 13088/30000 Training Loss: 0.04991370439529419\n",
      "Epoch 13089/30000 Training Loss: 0.049059536308050156\n",
      "Epoch 13090/30000 Training Loss: 0.06872909516096115\n",
      "Epoch 13091/30000 Training Loss: 0.054492443799972534\n",
      "Epoch 13092/30000 Training Loss: 0.0484459288418293\n",
      "Epoch 13093/30000 Training Loss: 0.04915191978216171\n",
      "Epoch 13094/30000 Training Loss: 0.05050649866461754\n",
      "Epoch 13095/30000 Training Loss: 0.05314856767654419\n",
      "Epoch 13096/30000 Training Loss: 0.038623351603746414\n",
      "Epoch 13097/30000 Training Loss: 0.0396980382502079\n",
      "Epoch 13098/30000 Training Loss: 0.03952248767018318\n",
      "Epoch 13099/30000 Training Loss: 0.04950376972556114\n",
      "Epoch 13100/30000 Training Loss: 0.06228209286928177\n",
      "Epoch 13100/30000 Validation Loss: 0.049442365765571594\n",
      "Epoch 13101/30000 Training Loss: 0.06518133729696274\n",
      "Epoch 13102/30000 Training Loss: 0.05220906063914299\n",
      "Epoch 13103/30000 Training Loss: 0.04280084744095802\n",
      "Epoch 13104/30000 Training Loss: 0.04268430173397064\n",
      "Epoch 13105/30000 Training Loss: 0.03653434291481972\n",
      "Epoch 13106/30000 Training Loss: 0.046940602362155914\n",
      "Epoch 13107/30000 Training Loss: 0.04629943519830704\n",
      "Epoch 13108/30000 Training Loss: 0.07133793830871582\n",
      "Epoch 13109/30000 Training Loss: 0.054602622985839844\n",
      "Epoch 13110/30000 Training Loss: 0.049623481929302216\n",
      "Epoch 13111/30000 Training Loss: 0.053263962268829346\n",
      "Epoch 13112/30000 Training Loss: 0.056932415813207626\n",
      "Epoch 13113/30000 Training Loss: 0.061320919543504715\n",
      "Epoch 13114/30000 Training Loss: 0.062481917440891266\n",
      "Epoch 13115/30000 Training Loss: 0.042674824595451355\n",
      "Epoch 13116/30000 Training Loss: 0.039866987615823746\n",
      "Epoch 13117/30000 Training Loss: 0.0569465234875679\n",
      "Epoch 13118/30000 Training Loss: 0.04320279508829117\n",
      "Epoch 13119/30000 Training Loss: 0.056732479482889175\n",
      "Epoch 13120/30000 Training Loss: 0.061038531363010406\n",
      "Epoch 13121/30000 Training Loss: 0.07151810824871063\n",
      "Epoch 13122/30000 Training Loss: 0.05560222640633583\n",
      "Epoch 13123/30000 Training Loss: 0.04463355988264084\n",
      "Epoch 13124/30000 Training Loss: 0.04957973212003708\n",
      "Epoch 13125/30000 Training Loss: 0.05734995752573013\n",
      "Epoch 13126/30000 Training Loss: 0.05288053676486015\n",
      "Epoch 13127/30000 Training Loss: 0.042307548224925995\n",
      "Epoch 13128/30000 Training Loss: 0.04340733587741852\n",
      "Epoch 13129/30000 Training Loss: 0.05950021743774414\n",
      "Epoch 13130/30000 Training Loss: 0.0509885773062706\n",
      "Epoch 13131/30000 Training Loss: 0.04432100057601929\n",
      "Epoch 13132/30000 Training Loss: 0.05228020250797272\n",
      "Epoch 13133/30000 Training Loss: 0.059719014912843704\n",
      "Epoch 13134/30000 Training Loss: 0.05385366454720497\n",
      "Epoch 13135/30000 Training Loss: 0.057650528848171234\n",
      "Epoch 13136/30000 Training Loss: 0.046933598816394806\n",
      "Epoch 13137/30000 Training Loss: 0.04055848345160484\n",
      "Epoch 13138/30000 Training Loss: 0.06792239099740982\n",
      "Epoch 13139/30000 Training Loss: 0.04634857922792435\n",
      "Epoch 13140/30000 Training Loss: 0.05871448665857315\n",
      "Epoch 13141/30000 Training Loss: 0.045995187014341354\n",
      "Epoch 13142/30000 Training Loss: 0.04537981003522873\n",
      "Epoch 13143/30000 Training Loss: 0.047277189791202545\n",
      "Epoch 13144/30000 Training Loss: 0.04403768852353096\n",
      "Epoch 13145/30000 Training Loss: 0.05050430819392204\n",
      "Epoch 13146/30000 Training Loss: 0.046539295464754105\n",
      "Epoch 13147/30000 Training Loss: 0.06755124777555466\n",
      "Epoch 13148/30000 Training Loss: 0.04310668632388115\n",
      "Epoch 13149/30000 Training Loss: 0.03049212135374546\n",
      "Epoch 13150/30000 Training Loss: 0.03693775832653046\n",
      "Epoch 13151/30000 Training Loss: 0.06507553905248642\n",
      "Epoch 13152/30000 Training Loss: 0.05109909176826477\n",
      "Epoch 13153/30000 Training Loss: 0.04869943484663963\n",
      "Epoch 13154/30000 Training Loss: 0.07695486396551132\n",
      "Epoch 13155/30000 Training Loss: 0.05073840916156769\n",
      "Epoch 13156/30000 Training Loss: 0.05917500704526901\n",
      "Epoch 13157/30000 Training Loss: 0.03762398287653923\n",
      "Epoch 13158/30000 Training Loss: 0.042902715504169464\n",
      "Epoch 13159/30000 Training Loss: 0.04524664953351021\n",
      "Epoch 13160/30000 Training Loss: 0.058840423822402954\n",
      "Epoch 13161/30000 Training Loss: 0.04295353218913078\n",
      "Epoch 13162/30000 Training Loss: 0.05488576367497444\n",
      "Epoch 13163/30000 Training Loss: 0.060739874839782715\n",
      "Epoch 13164/30000 Training Loss: 0.048391811549663544\n",
      "Epoch 13165/30000 Training Loss: 0.05054648593068123\n",
      "Epoch 13166/30000 Training Loss: 0.04772556200623512\n",
      "Epoch 13167/30000 Training Loss: 0.06296589970588684\n",
      "Epoch 13168/30000 Training Loss: 0.05604720488190651\n",
      "Epoch 13169/30000 Training Loss: 0.05622866749763489\n",
      "Epoch 13170/30000 Training Loss: 0.05978970229625702\n",
      "Epoch 13171/30000 Training Loss: 0.03335881233215332\n",
      "Epoch 13172/30000 Training Loss: 0.04997141659259796\n",
      "Epoch 13173/30000 Training Loss: 0.06662005931138992\n",
      "Epoch 13174/30000 Training Loss: 0.052040956914424896\n",
      "Epoch 13175/30000 Training Loss: 0.050304047763347626\n",
      "Epoch 13176/30000 Training Loss: 0.0423809215426445\n",
      "Epoch 13177/30000 Training Loss: 0.05571473017334938\n",
      "Epoch 13178/30000 Training Loss: 0.04799025505781174\n",
      "Epoch 13179/30000 Training Loss: 0.03955179452896118\n",
      "Epoch 13180/30000 Training Loss: 0.04709973931312561\n",
      "Epoch 13181/30000 Training Loss: 0.04043195769190788\n",
      "Epoch 13182/30000 Training Loss: 0.03453586995601654\n",
      "Epoch 13183/30000 Training Loss: 0.03913440555334091\n",
      "Epoch 13184/30000 Training Loss: 0.042430005967617035\n",
      "Epoch 13185/30000 Training Loss: 0.0648283138871193\n",
      "Epoch 13186/30000 Training Loss: 0.048534948378801346\n",
      "Epoch 13187/30000 Training Loss: 0.050863154232501984\n",
      "Epoch 13188/30000 Training Loss: 0.05592917278409004\n",
      "Epoch 13189/30000 Training Loss: 0.06064063310623169\n",
      "Epoch 13190/30000 Training Loss: 0.055019311606884\n",
      "Epoch 13191/30000 Training Loss: 0.04646260291337967\n",
      "Epoch 13192/30000 Training Loss: 0.049046579748392105\n",
      "Epoch 13193/30000 Training Loss: 0.05240718647837639\n",
      "Epoch 13194/30000 Training Loss: 0.05313998460769653\n",
      "Epoch 13195/30000 Training Loss: 0.05552152171730995\n",
      "Epoch 13196/30000 Training Loss: 0.05341130495071411\n",
      "Epoch 13197/30000 Training Loss: 0.06927794963121414\n",
      "Epoch 13198/30000 Training Loss: 0.05251838266849518\n",
      "Epoch 13199/30000 Training Loss: 0.04642158001661301\n",
      "Epoch 13200/30000 Training Loss: 0.04874497279524803\n",
      "Epoch 13200/30000 Validation Loss: 0.051006220281124115\n",
      "Epoch 13201/30000 Training Loss: 0.0651690661907196\n",
      "Epoch 13202/30000 Training Loss: 0.048248037695884705\n",
      "Epoch 13203/30000 Training Loss: 0.04835750162601471\n",
      "Epoch 13204/30000 Training Loss: 0.05710980296134949\n",
      "Epoch 13205/30000 Training Loss: 0.052645470947027206\n",
      "Epoch 13206/30000 Training Loss: 0.0497647300362587\n",
      "Epoch 13207/30000 Training Loss: 0.03532594442367554\n",
      "Epoch 13208/30000 Training Loss: 0.06246848404407501\n",
      "Epoch 13209/30000 Training Loss: 0.05520397052168846\n",
      "Epoch 13210/30000 Training Loss: 0.0598604679107666\n",
      "Epoch 13211/30000 Training Loss: 0.05369129404425621\n",
      "Epoch 13212/30000 Training Loss: 0.052675679326057434\n",
      "Epoch 13213/30000 Training Loss: 0.052860233932733536\n",
      "Epoch 13214/30000 Training Loss: 0.04805578291416168\n",
      "Epoch 13215/30000 Training Loss: 0.058744460344314575\n",
      "Epoch 13216/30000 Training Loss: 0.043231118470430374\n",
      "Epoch 13217/30000 Training Loss: 0.048461541533470154\n",
      "Epoch 13218/30000 Training Loss: 0.07445453107357025\n",
      "Epoch 13219/30000 Training Loss: 0.05892658978700638\n",
      "Epoch 13220/30000 Training Loss: 0.053054168820381165\n",
      "Epoch 13221/30000 Training Loss: 0.048420630395412445\n",
      "Epoch 13222/30000 Training Loss: 0.05932726711034775\n",
      "Epoch 13223/30000 Training Loss: 0.05860070139169693\n",
      "Epoch 13224/30000 Training Loss: 0.05337492376565933\n",
      "Epoch 13225/30000 Training Loss: 0.04337555915117264\n",
      "Epoch 13226/30000 Training Loss: 0.049904197454452515\n",
      "Epoch 13227/30000 Training Loss: 0.06468958407640457\n",
      "Epoch 13228/30000 Training Loss: 0.06126754730939865\n",
      "Epoch 13229/30000 Training Loss: 0.0622994638979435\n",
      "Epoch 13230/30000 Training Loss: 0.0580616258084774\n",
      "Epoch 13231/30000 Training Loss: 0.03730052337050438\n",
      "Epoch 13232/30000 Training Loss: 0.05417871102690697\n",
      "Epoch 13233/30000 Training Loss: 0.04869303107261658\n",
      "Epoch 13234/30000 Training Loss: 0.04944411665201187\n",
      "Epoch 13235/30000 Training Loss: 0.05265013128519058\n",
      "Epoch 13236/30000 Training Loss: 0.06909969449043274\n",
      "Epoch 13237/30000 Training Loss: 0.04140166938304901\n",
      "Epoch 13238/30000 Training Loss: 0.06561693549156189\n",
      "Epoch 13239/30000 Training Loss: 0.04464087262749672\n",
      "Epoch 13240/30000 Training Loss: 0.06232888251543045\n",
      "Epoch 13241/30000 Training Loss: 0.06734023988246918\n",
      "Epoch 13242/30000 Training Loss: 0.03511510789394379\n",
      "Epoch 13243/30000 Training Loss: 0.04244942218065262\n",
      "Epoch 13244/30000 Training Loss: 0.06364373862743378\n",
      "Epoch 13245/30000 Training Loss: 0.07267305254936218\n",
      "Epoch 13246/30000 Training Loss: 0.04921329393982887\n",
      "Epoch 13247/30000 Training Loss: 0.046737514436244965\n",
      "Epoch 13248/30000 Training Loss: 0.05652737617492676\n",
      "Epoch 13249/30000 Training Loss: 0.048958469182252884\n",
      "Epoch 13250/30000 Training Loss: 0.04281506687402725\n",
      "Epoch 13251/30000 Training Loss: 0.061815571039915085\n",
      "Epoch 13252/30000 Training Loss: 0.08577573299407959\n",
      "Epoch 13253/30000 Training Loss: 0.06251519173383713\n",
      "Epoch 13254/30000 Training Loss: 0.05003664642572403\n",
      "Epoch 13255/30000 Training Loss: 0.047205887734889984\n",
      "Epoch 13256/30000 Training Loss: 0.05100160837173462\n",
      "Epoch 13257/30000 Training Loss: 0.03792271763086319\n",
      "Epoch 13258/30000 Training Loss: 0.0635339692234993\n",
      "Epoch 13259/30000 Training Loss: 0.05476139113306999\n",
      "Epoch 13260/30000 Training Loss: 0.04832547903060913\n",
      "Epoch 13261/30000 Training Loss: 0.05397365987300873\n",
      "Epoch 13262/30000 Training Loss: 0.0403452031314373\n",
      "Epoch 13263/30000 Training Loss: 0.03222888335585594\n",
      "Epoch 13264/30000 Training Loss: 0.07699071615934372\n",
      "Epoch 13265/30000 Training Loss: 0.04336021840572357\n",
      "Epoch 13266/30000 Training Loss: 0.05138018727302551\n",
      "Epoch 13267/30000 Training Loss: 0.06196922808885574\n",
      "Epoch 13268/30000 Training Loss: 0.048855770379304886\n",
      "Epoch 13269/30000 Training Loss: 0.0552947111427784\n",
      "Epoch 13270/30000 Training Loss: 0.05215490609407425\n",
      "Epoch 13271/30000 Training Loss: 0.06572189182043076\n",
      "Epoch 13272/30000 Training Loss: 0.049631327390670776\n",
      "Epoch 13273/30000 Training Loss: 0.049401842057704926\n",
      "Epoch 13274/30000 Training Loss: 0.04209121689200401\n",
      "Epoch 13275/30000 Training Loss: 0.049962326884269714\n",
      "Epoch 13276/30000 Training Loss: 0.05785127729177475\n",
      "Epoch 13277/30000 Training Loss: 0.046110205352306366\n",
      "Epoch 13278/30000 Training Loss: 0.07093261927366257\n",
      "Epoch 13279/30000 Training Loss: 0.03804850950837135\n",
      "Epoch 13280/30000 Training Loss: 0.04307296499609947\n",
      "Epoch 13281/30000 Training Loss: 0.05475576967000961\n",
      "Epoch 13282/30000 Training Loss: 0.04366311430931091\n",
      "Epoch 13283/30000 Training Loss: 0.04278616979718208\n",
      "Epoch 13284/30000 Training Loss: 0.04629381373524666\n",
      "Epoch 13285/30000 Training Loss: 0.04474537819623947\n",
      "Epoch 13286/30000 Training Loss: 0.06054798513650894\n",
      "Epoch 13287/30000 Training Loss: 0.044130560010671616\n",
      "Epoch 13288/30000 Training Loss: 0.05838315561413765\n",
      "Epoch 13289/30000 Training Loss: 0.0434250570833683\n",
      "Epoch 13290/30000 Training Loss: 0.03780952841043472\n",
      "Epoch 13291/30000 Training Loss: 0.05039329081773758\n",
      "Epoch 13292/30000 Training Loss: 0.03468618169426918\n",
      "Epoch 13293/30000 Training Loss: 0.05240577459335327\n",
      "Epoch 13294/30000 Training Loss: 0.05182574316859245\n",
      "Epoch 13295/30000 Training Loss: 0.05554981529712677\n",
      "Epoch 13296/30000 Training Loss: 0.04787278175354004\n",
      "Epoch 13297/30000 Training Loss: 0.04250626266002655\n",
      "Epoch 13298/30000 Training Loss: 0.04533825069665909\n",
      "Epoch 13299/30000 Training Loss: 0.05440216884016991\n",
      "Epoch 13300/30000 Training Loss: 0.05361931771039963\n",
      "Epoch 13300/30000 Validation Loss: 0.04407353326678276\n",
      "Epoch 13301/30000 Training Loss: 0.04523548483848572\n",
      "Epoch 13302/30000 Training Loss: 0.050153739750385284\n",
      "Epoch 13303/30000 Training Loss: 0.05675651133060455\n",
      "Epoch 13304/30000 Training Loss: 0.04680779576301575\n",
      "Epoch 13305/30000 Training Loss: 0.060689933598041534\n",
      "Epoch 13306/30000 Training Loss: 0.04305305704474449\n",
      "Epoch 13307/30000 Training Loss: 0.052921127527952194\n",
      "Epoch 13308/30000 Training Loss: 0.04580219089984894\n",
      "Epoch 13309/30000 Training Loss: 0.054250918328762054\n",
      "Epoch 13310/30000 Training Loss: 0.039812784641981125\n",
      "Epoch 13311/30000 Training Loss: 0.05173061788082123\n",
      "Epoch 13312/30000 Training Loss: 0.051003821194171906\n",
      "Epoch 13313/30000 Training Loss: 0.05045760050415993\n",
      "Epoch 13314/30000 Training Loss: 0.03917546942830086\n",
      "Epoch 13315/30000 Training Loss: 0.062980517745018\n",
      "Epoch 13316/30000 Training Loss: 0.05168842151761055\n",
      "Epoch 13317/30000 Training Loss: 0.050861917436122894\n",
      "Epoch 13318/30000 Training Loss: 0.05630730837583542\n",
      "Epoch 13319/30000 Training Loss: 0.044991642236709595\n",
      "Epoch 13320/30000 Training Loss: 0.06039222329854965\n",
      "Epoch 13321/30000 Training Loss: 0.03534536063671112\n",
      "Epoch 13322/30000 Training Loss: 0.046833693981170654\n",
      "Epoch 13323/30000 Training Loss: 0.043014515191316605\n",
      "Epoch 13324/30000 Training Loss: 0.05388333648443222\n",
      "Epoch 13325/30000 Training Loss: 0.05199631303548813\n",
      "Epoch 13326/30000 Training Loss: 0.05062733218073845\n",
      "Epoch 13327/30000 Training Loss: 0.056772492825984955\n",
      "Epoch 13328/30000 Training Loss: 0.06035595387220383\n",
      "Epoch 13329/30000 Training Loss: 0.047756318002939224\n",
      "Epoch 13330/30000 Training Loss: 0.04584496468305588\n",
      "Epoch 13331/30000 Training Loss: 0.030786916613578796\n",
      "Epoch 13332/30000 Training Loss: 0.052633874118328094\n",
      "Epoch 13333/30000 Training Loss: 0.052638594061136246\n",
      "Epoch 13334/30000 Training Loss: 0.046437542885541916\n",
      "Epoch 13335/30000 Training Loss: 0.052796151489019394\n",
      "Epoch 13336/30000 Training Loss: 0.041456438601017\n",
      "Epoch 13337/30000 Training Loss: 0.03374416381120682\n",
      "Epoch 13338/30000 Training Loss: 0.05308125540614128\n",
      "Epoch 13339/30000 Training Loss: 0.04764780029654503\n",
      "Epoch 13340/30000 Training Loss: 0.046372395008802414\n",
      "Epoch 13341/30000 Training Loss: 0.062136612832546234\n",
      "Epoch 13342/30000 Training Loss: 0.06337174773216248\n",
      "Epoch 13343/30000 Training Loss: 0.03991202265024185\n",
      "Epoch 13344/30000 Training Loss: 0.0565287247300148\n",
      "Epoch 13345/30000 Training Loss: 0.04341929778456688\n",
      "Epoch 13346/30000 Training Loss: 0.0427013635635376\n",
      "Epoch 13347/30000 Training Loss: 0.058158379048109055\n",
      "Epoch 13348/30000 Training Loss: 0.04491955041885376\n",
      "Epoch 13349/30000 Training Loss: 0.04977098107337952\n",
      "Epoch 13350/30000 Training Loss: 0.0405132882297039\n",
      "Epoch 13351/30000 Training Loss: 0.057854458689689636\n",
      "Epoch 13352/30000 Training Loss: 0.04252548888325691\n",
      "Epoch 13353/30000 Training Loss: 0.0381443127989769\n",
      "Epoch 13354/30000 Training Loss: 0.05858786776661873\n",
      "Epoch 13355/30000 Training Loss: 0.04901230335235596\n",
      "Epoch 13356/30000 Training Loss: 0.042182229459285736\n",
      "Epoch 13357/30000 Training Loss: 0.047811832278966904\n",
      "Epoch 13358/30000 Training Loss: 0.03764640539884567\n",
      "Epoch 13359/30000 Training Loss: 0.039101701229810715\n",
      "Epoch 13360/30000 Training Loss: 0.048747871071100235\n",
      "Epoch 13361/30000 Training Loss: 0.05025612562894821\n",
      "Epoch 13362/30000 Training Loss: 0.05053989589214325\n",
      "Epoch 13363/30000 Training Loss: 0.04297276586294174\n",
      "Epoch 13364/30000 Training Loss: 0.06524355709552765\n",
      "Epoch 13365/30000 Training Loss: 0.048684291541576385\n",
      "Epoch 13366/30000 Training Loss: 0.03937186300754547\n",
      "Epoch 13367/30000 Training Loss: 0.05447741970419884\n",
      "Epoch 13368/30000 Training Loss: 0.049649678170681\n",
      "Epoch 13369/30000 Training Loss: 0.04019271209836006\n",
      "Epoch 13370/30000 Training Loss: 0.07235220819711685\n",
      "Epoch 13371/30000 Training Loss: 0.07669972628355026\n",
      "Epoch 13372/30000 Training Loss: 0.05441168695688248\n",
      "Epoch 13373/30000 Training Loss: 0.06631501764059067\n",
      "Epoch 13374/30000 Training Loss: 0.055519770830869675\n",
      "Epoch 13375/30000 Training Loss: 0.04415775090456009\n",
      "Epoch 13376/30000 Training Loss: 0.06012178957462311\n",
      "Epoch 13377/30000 Training Loss: 0.05104927718639374\n",
      "Epoch 13378/30000 Training Loss: 0.05004068464040756\n",
      "Epoch 13379/30000 Training Loss: 0.06098170578479767\n",
      "Epoch 13380/30000 Training Loss: 0.06651955842971802\n",
      "Epoch 13381/30000 Training Loss: 0.05242377147078514\n",
      "Epoch 13382/30000 Training Loss: 0.039250195026397705\n",
      "Epoch 13383/30000 Training Loss: 0.04775625094771385\n",
      "Epoch 13384/30000 Training Loss: 0.050308309495449066\n",
      "Epoch 13385/30000 Training Loss: 0.057119883596897125\n",
      "Epoch 13386/30000 Training Loss: 0.05862412974238396\n",
      "Epoch 13387/30000 Training Loss: 0.05180913954973221\n",
      "Epoch 13388/30000 Training Loss: 0.044458866119384766\n",
      "Epoch 13389/30000 Training Loss: 0.05346432700753212\n",
      "Epoch 13390/30000 Training Loss: 0.045951299369335175\n",
      "Epoch 13391/30000 Training Loss: 0.06641979515552521\n",
      "Epoch 13392/30000 Training Loss: 0.05184256285429001\n",
      "Epoch 13393/30000 Training Loss: 0.04642932489514351\n",
      "Epoch 13394/30000 Training Loss: 0.04639117419719696\n",
      "Epoch 13395/30000 Training Loss: 0.0409603975713253\n",
      "Epoch 13396/30000 Training Loss: 0.04885026067495346\n",
      "Epoch 13397/30000 Training Loss: 0.05563686043024063\n",
      "Epoch 13398/30000 Training Loss: 0.0505768358707428\n",
      "Epoch 13399/30000 Training Loss: 0.05365050956606865\n",
      "Epoch 13400/30000 Training Loss: 0.05620799586176872\n",
      "Epoch 13400/30000 Validation Loss: 0.07698707282543182\n",
      "Epoch 13401/30000 Training Loss: 0.05570413917303085\n",
      "Epoch 13402/30000 Training Loss: 0.06437379121780396\n",
      "Epoch 13403/30000 Training Loss: 0.0507722869515419\n",
      "Epoch 13404/30000 Training Loss: 0.036484479904174805\n",
      "Epoch 13405/30000 Training Loss: 0.05484042316675186\n",
      "Epoch 13406/30000 Training Loss: 0.05658707022666931\n",
      "Epoch 13407/30000 Training Loss: 0.04515095055103302\n",
      "Epoch 13408/30000 Training Loss: 0.057032305747270584\n",
      "Epoch 13409/30000 Training Loss: 0.04199938476085663\n",
      "Epoch 13410/30000 Training Loss: 0.05076378956437111\n",
      "Epoch 13411/30000 Training Loss: 0.06798006594181061\n",
      "Epoch 13412/30000 Training Loss: 0.04696720093488693\n",
      "Epoch 13413/30000 Training Loss: 0.0705280527472496\n",
      "Epoch 13414/30000 Training Loss: 0.056481245905160904\n",
      "Epoch 13415/30000 Training Loss: 0.046011604368686676\n",
      "Epoch 13416/30000 Training Loss: 0.042571958154439926\n",
      "Epoch 13417/30000 Training Loss: 0.061272572726011276\n",
      "Epoch 13418/30000 Training Loss: 0.051559820771217346\n",
      "Epoch 13419/30000 Training Loss: 0.04737638309597969\n",
      "Epoch 13420/30000 Training Loss: 0.05056381970643997\n",
      "Epoch 13421/30000 Training Loss: 0.048413414508104324\n",
      "Epoch 13422/30000 Training Loss: 0.05430016666650772\n",
      "Epoch 13423/30000 Training Loss: 0.03970685601234436\n",
      "Epoch 13424/30000 Training Loss: 0.049616727977991104\n",
      "Epoch 13425/30000 Training Loss: 0.05773114040493965\n",
      "Epoch 13426/30000 Training Loss: 0.06442413479089737\n",
      "Epoch 13427/30000 Training Loss: 0.04541213810443878\n",
      "Epoch 13428/30000 Training Loss: 0.049260035157203674\n",
      "Epoch 13429/30000 Training Loss: 0.04346386343240738\n",
      "Epoch 13430/30000 Training Loss: 0.033759452402591705\n",
      "Epoch 13431/30000 Training Loss: 0.051009297370910645\n",
      "Epoch 13432/30000 Training Loss: 0.04925712198019028\n",
      "Epoch 13433/30000 Training Loss: 0.045721229165792465\n",
      "Epoch 13434/30000 Training Loss: 0.05346539244055748\n",
      "Epoch 13435/30000 Training Loss: 0.05794578790664673\n",
      "Epoch 13436/30000 Training Loss: 0.04677966982126236\n",
      "Epoch 13437/30000 Training Loss: 0.046601809561252594\n",
      "Epoch 13438/30000 Training Loss: 0.050877660512924194\n",
      "Epoch 13439/30000 Training Loss: 0.06700730323791504\n",
      "Epoch 13440/30000 Training Loss: 0.061708372086286545\n",
      "Epoch 13441/30000 Training Loss: 0.037057843059301376\n",
      "Epoch 13442/30000 Training Loss: 0.052215516567230225\n",
      "Epoch 13443/30000 Training Loss: 0.05759149417281151\n",
      "Epoch 13444/30000 Training Loss: 0.06422971189022064\n",
      "Epoch 13445/30000 Training Loss: 0.06032917648553848\n",
      "Epoch 13446/30000 Training Loss: 0.05275056138634682\n",
      "Epoch 13447/30000 Training Loss: 0.05610961839556694\n",
      "Epoch 13448/30000 Training Loss: 0.04889348894357681\n",
      "Epoch 13449/30000 Training Loss: 0.05115589499473572\n",
      "Epoch 13450/30000 Training Loss: 0.0541667640209198\n",
      "Epoch 13451/30000 Training Loss: 0.048677101731300354\n",
      "Epoch 13452/30000 Training Loss: 0.04056667163968086\n",
      "Epoch 13453/30000 Training Loss: 0.036137863993644714\n",
      "Epoch 13454/30000 Training Loss: 0.07071943581104279\n",
      "Epoch 13455/30000 Training Loss: 0.045974038541316986\n",
      "Epoch 13456/30000 Training Loss: 0.06953977048397064\n",
      "Epoch 13457/30000 Training Loss: 0.059230830520391464\n",
      "Epoch 13458/30000 Training Loss: 0.04280998185276985\n",
      "Epoch 13459/30000 Training Loss: 0.04432718828320503\n",
      "Epoch 13460/30000 Training Loss: 0.044571422040462494\n",
      "Epoch 13461/30000 Training Loss: 0.05154886096715927\n",
      "Epoch 13462/30000 Training Loss: 0.05377214401960373\n",
      "Epoch 13463/30000 Training Loss: 0.054437898099422455\n",
      "Epoch 13464/30000 Training Loss: 0.06991516053676605\n",
      "Epoch 13465/30000 Training Loss: 0.0437345951795578\n",
      "Epoch 13466/30000 Training Loss: 0.04509318247437477\n",
      "Epoch 13467/30000 Training Loss: 0.06999337673187256\n",
      "Epoch 13468/30000 Training Loss: 0.050662338733673096\n",
      "Epoch 13469/30000 Training Loss: 0.059240248054265976\n",
      "Epoch 13470/30000 Training Loss: 0.0510404147207737\n",
      "Epoch 13471/30000 Training Loss: 0.045347876846790314\n",
      "Epoch 13472/30000 Training Loss: 0.05703116953372955\n",
      "Epoch 13473/30000 Training Loss: 0.05442119017243385\n",
      "Epoch 13474/30000 Training Loss: 0.06307275593280792\n",
      "Epoch 13475/30000 Training Loss: 0.05127178877592087\n",
      "Epoch 13476/30000 Training Loss: 0.04702744632959366\n",
      "Epoch 13477/30000 Training Loss: 0.05765778571367264\n",
      "Epoch 13478/30000 Training Loss: 0.04073656350374222\n",
      "Epoch 13479/30000 Training Loss: 0.05882403254508972\n",
      "Epoch 13480/30000 Training Loss: 0.041975416243076324\n",
      "Epoch 13481/30000 Training Loss: 0.0457378588616848\n",
      "Epoch 13482/30000 Training Loss: 0.048879474401474\n",
      "Epoch 13483/30000 Training Loss: 0.06012262403964996\n",
      "Epoch 13484/30000 Training Loss: 0.04808560013771057\n",
      "Epoch 13485/30000 Training Loss: 0.04823334887623787\n",
      "Epoch 13486/30000 Training Loss: 0.04390454292297363\n",
      "Epoch 13487/30000 Training Loss: 0.03902490437030792\n",
      "Epoch 13488/30000 Training Loss: 0.05378749221563339\n",
      "Epoch 13489/30000 Training Loss: 0.04519643634557724\n",
      "Epoch 13490/30000 Training Loss: 0.05878467112779617\n",
      "Epoch 13491/30000 Training Loss: 0.052148155868053436\n",
      "Epoch 13492/30000 Training Loss: 0.0357997789978981\n",
      "Epoch 13493/30000 Training Loss: 0.05917884409427643\n",
      "Epoch 13494/30000 Training Loss: 0.043398067355155945\n",
      "Epoch 13495/30000 Training Loss: 0.04748235642910004\n",
      "Epoch 13496/30000 Training Loss: 0.06109847128391266\n",
      "Epoch 13497/30000 Training Loss: 0.06422920525074005\n",
      "Epoch 13498/30000 Training Loss: 0.05529666692018509\n",
      "Epoch 13499/30000 Training Loss: 0.063652403652668\n",
      "Epoch 13500/30000 Training Loss: 0.05145946890115738\n",
      "Epoch 13500/30000 Validation Loss: 0.049948908388614655\n",
      "Epoch 13501/30000 Training Loss: 0.04361821711063385\n",
      "Epoch 13502/30000 Training Loss: 0.05649610608816147\n",
      "Epoch 13503/30000 Training Loss: 0.03681410476565361\n",
      "Epoch 13504/30000 Training Loss: 0.05324213206768036\n",
      "Epoch 13505/30000 Training Loss: 0.0415736585855484\n",
      "Epoch 13506/30000 Training Loss: 0.046900685876607895\n",
      "Epoch 13507/30000 Training Loss: 0.05746392160654068\n",
      "Epoch 13508/30000 Training Loss: 0.0494394451379776\n",
      "Epoch 13509/30000 Training Loss: 0.045529745519161224\n",
      "Epoch 13510/30000 Training Loss: 0.055434055626392365\n",
      "Epoch 13511/30000 Training Loss: 0.049077294766902924\n",
      "Epoch 13512/30000 Training Loss: 0.07693720608949661\n",
      "Epoch 13513/30000 Training Loss: 0.05227237939834595\n",
      "Epoch 13514/30000 Training Loss: 0.06174828112125397\n",
      "Epoch 13515/30000 Training Loss: 0.06358633935451508\n",
      "Epoch 13516/30000 Training Loss: 0.04061836749315262\n",
      "Epoch 13517/30000 Training Loss: 0.06350535899400711\n",
      "Epoch 13518/30000 Training Loss: 0.051343031227588654\n",
      "Epoch 13519/30000 Training Loss: 0.060341253876686096\n",
      "Epoch 13520/30000 Training Loss: 0.05302361771464348\n",
      "Epoch 13521/30000 Training Loss: 0.04691546782851219\n",
      "Epoch 13522/30000 Training Loss: 0.0316828154027462\n",
      "Epoch 13523/30000 Training Loss: 0.04698789119720459\n",
      "Epoch 13524/30000 Training Loss: 0.04281163215637207\n",
      "Epoch 13525/30000 Training Loss: 0.04818384349346161\n",
      "Epoch 13526/30000 Training Loss: 0.05291735753417015\n",
      "Epoch 13527/30000 Training Loss: 0.036652978509664536\n",
      "Epoch 13528/30000 Training Loss: 0.03744079917669296\n",
      "Epoch 13529/30000 Training Loss: 0.06696346402168274\n",
      "Epoch 13530/30000 Training Loss: 0.05168423056602478\n",
      "Epoch 13531/30000 Training Loss: 0.05978222191333771\n",
      "Epoch 13532/30000 Training Loss: 0.046207863837480545\n",
      "Epoch 13533/30000 Training Loss: 0.05968213826417923\n",
      "Epoch 13534/30000 Training Loss: 0.05841031298041344\n",
      "Epoch 13535/30000 Training Loss: 0.05246797576546669\n",
      "Epoch 13536/30000 Training Loss: 0.05032126605510712\n",
      "Epoch 13537/30000 Training Loss: 0.0587398037314415\n",
      "Epoch 13538/30000 Training Loss: 0.04842740297317505\n",
      "Epoch 13539/30000 Training Loss: 0.05528632551431656\n",
      "Epoch 13540/30000 Training Loss: 0.05041420832276344\n",
      "Epoch 13541/30000 Training Loss: 0.04181714728474617\n",
      "Epoch 13542/30000 Training Loss: 0.03609414026141167\n",
      "Epoch 13543/30000 Training Loss: 0.049188241362571716\n",
      "Epoch 13544/30000 Training Loss: 0.04173728823661804\n",
      "Epoch 13545/30000 Training Loss: 0.061460740864276886\n",
      "Epoch 13546/30000 Training Loss: 0.06607521325349808\n",
      "Epoch 13547/30000 Training Loss: 0.04663107544183731\n",
      "Epoch 13548/30000 Training Loss: 0.04001147672533989\n",
      "Epoch 13549/30000 Training Loss: 0.03783899545669556\n",
      "Epoch 13550/30000 Training Loss: 0.05039958283305168\n",
      "Epoch 13551/30000 Training Loss: 0.06685392558574677\n",
      "Epoch 13552/30000 Training Loss: 0.054947443306446075\n",
      "Epoch 13553/30000 Training Loss: 0.05621681362390518\n",
      "Epoch 13554/30000 Training Loss: 0.07149714976549149\n",
      "Epoch 13555/30000 Training Loss: 0.06347539275884628\n",
      "Epoch 13556/30000 Training Loss: 0.044766105711460114\n",
      "Epoch 13557/30000 Training Loss: 0.0527731329202652\n",
      "Epoch 13558/30000 Training Loss: 0.06692216545343399\n",
      "Epoch 13559/30000 Training Loss: 0.04857242479920387\n",
      "Epoch 13560/30000 Training Loss: 0.046712104231119156\n",
      "Epoch 13561/30000 Training Loss: 0.05344332009553909\n",
      "Epoch 13562/30000 Training Loss: 0.04702242091298103\n",
      "Epoch 13563/30000 Training Loss: 0.05658034235239029\n",
      "Epoch 13564/30000 Training Loss: 0.04025103151798248\n",
      "Epoch 13565/30000 Training Loss: 0.06378524005413055\n",
      "Epoch 13566/30000 Training Loss: 0.05212318152189255\n",
      "Epoch 13567/30000 Training Loss: 0.03114328160881996\n",
      "Epoch 13568/30000 Training Loss: 0.041952770203351974\n",
      "Epoch 13569/30000 Training Loss: 0.03709162399172783\n",
      "Epoch 13570/30000 Training Loss: 0.047490574419498444\n",
      "Epoch 13571/30000 Training Loss: 0.049880385398864746\n",
      "Epoch 13572/30000 Training Loss: 0.07040112465620041\n",
      "Epoch 13573/30000 Training Loss: 0.05993189290165901\n",
      "Epoch 13574/30000 Training Loss: 0.047069989144802094\n",
      "Epoch 13575/30000 Training Loss: 0.04235829785466194\n",
      "Epoch 13576/30000 Training Loss: 0.0476866289973259\n",
      "Epoch 13577/30000 Training Loss: 0.05038273334503174\n",
      "Epoch 13578/30000 Training Loss: 0.07134170830249786\n",
      "Epoch 13579/30000 Training Loss: 0.07295826077461243\n",
      "Epoch 13580/30000 Training Loss: 0.06552145630121231\n",
      "Epoch 13581/30000 Training Loss: 0.04503117501735687\n",
      "Epoch 13582/30000 Training Loss: 0.06432738900184631\n",
      "Epoch 13583/30000 Training Loss: 0.07083621621131897\n",
      "Epoch 13584/30000 Training Loss: 0.05879019573330879\n",
      "Epoch 13585/30000 Training Loss: 0.06748395413160324\n",
      "Epoch 13586/30000 Training Loss: 0.027428776025772095\n",
      "Epoch 13587/30000 Training Loss: 0.041095830500125885\n",
      "Epoch 13588/30000 Training Loss: 0.0569223091006279\n",
      "Epoch 13589/30000 Training Loss: 0.05519503355026245\n",
      "Epoch 13590/30000 Training Loss: 0.05533909797668457\n",
      "Epoch 13591/30000 Training Loss: 0.028402429074048996\n",
      "Epoch 13592/30000 Training Loss: 0.045738741755485535\n",
      "Epoch 13593/30000 Training Loss: 0.04999549686908722\n",
      "Epoch 13594/30000 Training Loss: 0.054816409945487976\n",
      "Epoch 13595/30000 Training Loss: 0.04821911081671715\n",
      "Epoch 13596/30000 Training Loss: 0.05564788356423378\n",
      "Epoch 13597/30000 Training Loss: 0.051305145025253296\n",
      "Epoch 13598/30000 Training Loss: 0.043537791818380356\n",
      "Epoch 13599/30000 Training Loss: 0.05000666528940201\n",
      "Epoch 13600/30000 Training Loss: 0.058517199009656906\n",
      "Epoch 13600/30000 Validation Loss: 0.061761099845170975\n",
      "Epoch 13601/30000 Training Loss: 0.05952230095863342\n",
      "Epoch 13602/30000 Training Loss: 0.04160556197166443\n",
      "Epoch 13603/30000 Training Loss: 0.05978719890117645\n",
      "Epoch 13604/30000 Training Loss: 0.0399598553776741\n",
      "Epoch 13605/30000 Training Loss: 0.04930594936013222\n",
      "Epoch 13606/30000 Training Loss: 0.062711700797081\n",
      "Epoch 13607/30000 Training Loss: 0.062159836292266846\n",
      "Epoch 13608/30000 Training Loss: 0.04602719470858574\n",
      "Epoch 13609/30000 Training Loss: 0.0591486394405365\n",
      "Epoch 13610/30000 Training Loss: 0.05039873719215393\n",
      "Epoch 13611/30000 Training Loss: 0.06124226003885269\n",
      "Epoch 13612/30000 Training Loss: 0.04946267604827881\n",
      "Epoch 13613/30000 Training Loss: 0.043781183660030365\n",
      "Epoch 13614/30000 Training Loss: 0.055635012686252594\n",
      "Epoch 13615/30000 Training Loss: 0.052503325045108795\n",
      "Epoch 13616/30000 Training Loss: 0.056775614619255066\n",
      "Epoch 13617/30000 Training Loss: 0.047882724553346634\n",
      "Epoch 13618/30000 Training Loss: 0.05456304922699928\n",
      "Epoch 13619/30000 Training Loss: 0.04943260923027992\n",
      "Epoch 13620/30000 Training Loss: 0.056280072778463364\n",
      "Epoch 13621/30000 Training Loss: 0.05137444660067558\n",
      "Epoch 13622/30000 Training Loss: 0.04387221485376358\n",
      "Epoch 13623/30000 Training Loss: 0.04395368695259094\n",
      "Epoch 13624/30000 Training Loss: 0.03843012824654579\n",
      "Epoch 13625/30000 Training Loss: 0.06651666015386581\n",
      "Epoch 13626/30000 Training Loss: 0.03491031005978584\n",
      "Epoch 13627/30000 Training Loss: 0.055966366082429886\n",
      "Epoch 13628/30000 Training Loss: 0.04351972043514252\n",
      "Epoch 13629/30000 Training Loss: 0.04744861274957657\n",
      "Epoch 13630/30000 Training Loss: 0.0544792078435421\n",
      "Epoch 13631/30000 Training Loss: 0.044511064887046814\n",
      "Epoch 13632/30000 Training Loss: 0.04448549076914787\n",
      "Epoch 13633/30000 Training Loss: 0.030907968059182167\n",
      "Epoch 13634/30000 Training Loss: 0.05209789797663689\n",
      "Epoch 13635/30000 Training Loss: 0.04557277262210846\n",
      "Epoch 13636/30000 Training Loss: 0.038652919232845306\n",
      "Epoch 13637/30000 Training Loss: 0.049329109489917755\n",
      "Epoch 13638/30000 Training Loss: 0.03430600464344025\n",
      "Epoch 13639/30000 Training Loss: 0.06514912098646164\n",
      "Epoch 13640/30000 Training Loss: 0.06458628177642822\n",
      "Epoch 13641/30000 Training Loss: 0.05146434158086777\n",
      "Epoch 13642/30000 Training Loss: 0.05319616198539734\n",
      "Epoch 13643/30000 Training Loss: 0.051645420491695404\n",
      "Epoch 13644/30000 Training Loss: 0.04916473478078842\n",
      "Epoch 13645/30000 Training Loss: 0.03971651569008827\n",
      "Epoch 13646/30000 Training Loss: 0.06711637228727341\n",
      "Epoch 13647/30000 Training Loss: 0.06738315522670746\n",
      "Epoch 13648/30000 Training Loss: 0.04798099398612976\n",
      "Epoch 13649/30000 Training Loss: 0.04583044722676277\n",
      "Epoch 13650/30000 Training Loss: 0.06030307337641716\n",
      "Epoch 13651/30000 Training Loss: 0.033501576632261276\n",
      "Epoch 13652/30000 Training Loss: 0.05268578231334686\n",
      "Epoch 13653/30000 Training Loss: 0.04999707266688347\n",
      "Epoch 13654/30000 Training Loss: 0.0699603259563446\n",
      "Epoch 13655/30000 Training Loss: 0.04473622888326645\n",
      "Epoch 13656/30000 Training Loss: 0.07223151624202728\n",
      "Epoch 13657/30000 Training Loss: 0.04991714656352997\n",
      "Epoch 13658/30000 Training Loss: 0.041300974786281586\n",
      "Epoch 13659/30000 Training Loss: 0.05052768811583519\n",
      "Epoch 13660/30000 Training Loss: 0.04170450568199158\n",
      "Epoch 13661/30000 Training Loss: 0.06192164123058319\n",
      "Epoch 13662/30000 Training Loss: 0.035017941147089005\n",
      "Epoch 13663/30000 Training Loss: 0.057009533047676086\n",
      "Epoch 13664/30000 Training Loss: 0.06619232147932053\n",
      "Epoch 13665/30000 Training Loss: 0.04809790104627609\n",
      "Epoch 13666/30000 Training Loss: 0.039332084357738495\n",
      "Epoch 13667/30000 Training Loss: 0.055501170456409454\n",
      "Epoch 13668/30000 Training Loss: 0.06615062057971954\n",
      "Epoch 13669/30000 Training Loss: 0.045612554997205734\n",
      "Epoch 13670/30000 Training Loss: 0.05381312966346741\n",
      "Epoch 13671/30000 Training Loss: 0.03967878594994545\n",
      "Epoch 13672/30000 Training Loss: 0.0414770282804966\n",
      "Epoch 13673/30000 Training Loss: 0.06588089466094971\n",
      "Epoch 13674/30000 Training Loss: 0.051702216267585754\n",
      "Epoch 13675/30000 Training Loss: 0.037900395691394806\n",
      "Epoch 13676/30000 Training Loss: 0.04764499142765999\n",
      "Epoch 13677/30000 Training Loss: 0.053652890026569366\n",
      "Epoch 13678/30000 Training Loss: 0.04315571114420891\n",
      "Epoch 13679/30000 Training Loss: 0.052571043372154236\n",
      "Epoch 13680/30000 Training Loss: 0.05229896679520607\n",
      "Epoch 13681/30000 Training Loss: 0.053891826421022415\n",
      "Epoch 13682/30000 Training Loss: 0.05420013144612312\n",
      "Epoch 13683/30000 Training Loss: 0.04938143864274025\n",
      "Epoch 13684/30000 Training Loss: 0.04380842298269272\n",
      "Epoch 13685/30000 Training Loss: 0.06081090867519379\n",
      "Epoch 13686/30000 Training Loss: 0.05680315941572189\n",
      "Epoch 13687/30000 Training Loss: 0.030608782544732094\n",
      "Epoch 13688/30000 Training Loss: 0.06006486341357231\n",
      "Epoch 13689/30000 Training Loss: 0.05031691491603851\n",
      "Epoch 13690/30000 Training Loss: 0.04810376465320587\n",
      "Epoch 13691/30000 Training Loss: 0.049657031893730164\n",
      "Epoch 13692/30000 Training Loss: 0.061988845467567444\n",
      "Epoch 13693/30000 Training Loss: 0.0687471479177475\n",
      "Epoch 13694/30000 Training Loss: 0.0661841481924057\n",
      "Epoch 13695/30000 Training Loss: 0.05378197506070137\n",
      "Epoch 13696/30000 Training Loss: 0.05782860144972801\n",
      "Epoch 13697/30000 Training Loss: 0.05190601199865341\n",
      "Epoch 13698/30000 Training Loss: 0.039710111916065216\n",
      "Epoch 13699/30000 Training Loss: 0.039202578365802765\n",
      "Epoch 13700/30000 Training Loss: 0.03682500869035721\n",
      "Epoch 13700/30000 Validation Loss: 0.054752249270677567\n",
      "Epoch 13701/30000 Training Loss: 0.058355558663606644\n",
      "Epoch 13702/30000 Training Loss: 0.05010849982500076\n",
      "Epoch 13703/30000 Training Loss: 0.0434369221329689\n",
      "Epoch 13704/30000 Training Loss: 0.05475587397813797\n",
      "Epoch 13705/30000 Training Loss: 0.06975691020488739\n",
      "Epoch 13706/30000 Training Loss: 0.05397289991378784\n",
      "Epoch 13707/30000 Training Loss: 0.05427281931042671\n",
      "Epoch 13708/30000 Training Loss: 0.038050577044487\n",
      "Epoch 13709/30000 Training Loss: 0.06488368660211563\n",
      "Epoch 13710/30000 Training Loss: 0.03497765213251114\n",
      "Epoch 13711/30000 Training Loss: 0.045418962836265564\n",
      "Epoch 13712/30000 Training Loss: 0.044366754591464996\n",
      "Epoch 13713/30000 Training Loss: 0.05255665257573128\n",
      "Epoch 13714/30000 Training Loss: 0.057009257376194\n",
      "Epoch 13715/30000 Training Loss: 0.043530113995075226\n",
      "Epoch 13716/30000 Training Loss: 0.07085983455181122\n",
      "Epoch 13717/30000 Training Loss: 0.05766491964459419\n",
      "Epoch 13718/30000 Training Loss: 0.05428376793861389\n",
      "Epoch 13719/30000 Training Loss: 0.05318045616149902\n",
      "Epoch 13720/30000 Training Loss: 0.042570117861032486\n",
      "Epoch 13721/30000 Training Loss: 0.0640919879078865\n",
      "Epoch 13722/30000 Training Loss: 0.04139150679111481\n",
      "Epoch 13723/30000 Training Loss: 0.07110011577606201\n",
      "Epoch 13724/30000 Training Loss: 0.03778960183262825\n",
      "Epoch 13725/30000 Training Loss: 0.039095189422369\n",
      "Epoch 13726/30000 Training Loss: 0.03482287377119064\n",
      "Epoch 13727/30000 Training Loss: 0.046292178332805634\n",
      "Epoch 13728/30000 Training Loss: 0.050238389521837234\n",
      "Epoch 13729/30000 Training Loss: 0.05264581739902496\n",
      "Epoch 13730/30000 Training Loss: 0.05087287351489067\n",
      "Epoch 13731/30000 Training Loss: 0.04759225249290466\n",
      "Epoch 13732/30000 Training Loss: 0.04772254452109337\n",
      "Epoch 13733/30000 Training Loss: 0.04882623255252838\n",
      "Epoch 13734/30000 Training Loss: 0.04198247939348221\n",
      "Epoch 13735/30000 Training Loss: 0.056142657995224\n",
      "Epoch 13736/30000 Training Loss: 0.04572676122188568\n",
      "Epoch 13737/30000 Training Loss: 0.04927205294370651\n",
      "Epoch 13738/30000 Training Loss: 0.043964534997940063\n",
      "Epoch 13739/30000 Training Loss: 0.04531138017773628\n",
      "Epoch 13740/30000 Training Loss: 0.05627916008234024\n",
      "Epoch 13741/30000 Training Loss: 0.03807337209582329\n",
      "Epoch 13742/30000 Training Loss: 0.046192482113838196\n",
      "Epoch 13743/30000 Training Loss: 0.04997236281633377\n",
      "Epoch 13744/30000 Training Loss: 0.035769276320934296\n",
      "Epoch 13745/30000 Training Loss: 0.057688139379024506\n",
      "Epoch 13746/30000 Training Loss: 0.0582970567047596\n",
      "Epoch 13747/30000 Training Loss: 0.03834204748272896\n",
      "Epoch 13748/30000 Training Loss: 0.06113612651824951\n",
      "Epoch 13749/30000 Training Loss: 0.0511779710650444\n",
      "Epoch 13750/30000 Training Loss: 0.04719674587249756\n",
      "Epoch 13751/30000 Training Loss: 0.04946531355381012\n",
      "Epoch 13752/30000 Training Loss: 0.04145657643675804\n",
      "Epoch 13753/30000 Training Loss: 0.041254665702581406\n",
      "Epoch 13754/30000 Training Loss: 0.05096552520990372\n",
      "Epoch 13755/30000 Training Loss: 0.04880475997924805\n",
      "Epoch 13756/30000 Training Loss: 0.055960763245821\n",
      "Epoch 13757/30000 Training Loss: 0.04342802241444588\n",
      "Epoch 13758/30000 Training Loss: 0.05924835056066513\n",
      "Epoch 13759/30000 Training Loss: 0.04931355267763138\n",
      "Epoch 13760/30000 Training Loss: 0.05112314969301224\n",
      "Epoch 13761/30000 Training Loss: 0.030966319143772125\n",
      "Epoch 13762/30000 Training Loss: 0.05468878149986267\n",
      "Epoch 13763/30000 Training Loss: 0.04502248764038086\n",
      "Epoch 13764/30000 Training Loss: 0.05431777611374855\n",
      "Epoch 13765/30000 Training Loss: 0.06285061687231064\n",
      "Epoch 13766/30000 Training Loss: 0.04930337890982628\n",
      "Epoch 13767/30000 Training Loss: 0.05248001217842102\n",
      "Epoch 13768/30000 Training Loss: 0.052828043699264526\n",
      "Epoch 13769/30000 Training Loss: 0.031644418835639954\n",
      "Epoch 13770/30000 Training Loss: 0.054095812141895294\n",
      "Epoch 13771/30000 Training Loss: 0.05301481485366821\n",
      "Epoch 13772/30000 Training Loss: 0.041041597723960876\n",
      "Epoch 13773/30000 Training Loss: 0.06196504458785057\n",
      "Epoch 13774/30000 Training Loss: 0.05841156095266342\n",
      "Epoch 13775/30000 Training Loss: 0.04261805862188339\n",
      "Epoch 13776/30000 Training Loss: 0.03519788756966591\n",
      "Epoch 13777/30000 Training Loss: 0.049130428582429886\n",
      "Epoch 13778/30000 Training Loss: 0.040597103536129\n",
      "Epoch 13779/30000 Training Loss: 0.04129911959171295\n",
      "Epoch 13780/30000 Training Loss: 0.044310037046670914\n",
      "Epoch 13781/30000 Training Loss: 0.048141151666641235\n",
      "Epoch 13782/30000 Training Loss: 0.05796267092227936\n",
      "Epoch 13783/30000 Training Loss: 0.06185247376561165\n",
      "Epoch 13784/30000 Training Loss: 0.039263103157281876\n",
      "Epoch 13785/30000 Training Loss: 0.06424786150455475\n",
      "Epoch 13786/30000 Training Loss: 0.05196857824921608\n",
      "Epoch 13787/30000 Training Loss: 0.06779667735099792\n",
      "Epoch 13788/30000 Training Loss: 0.04948697239160538\n",
      "Epoch 13789/30000 Training Loss: 0.0545770637691021\n",
      "Epoch 13790/30000 Training Loss: 0.03986208140850067\n",
      "Epoch 13791/30000 Training Loss: 0.05770459771156311\n",
      "Epoch 13792/30000 Training Loss: 0.062032219022512436\n",
      "Epoch 13793/30000 Training Loss: 0.03964494168758392\n",
      "Epoch 13794/30000 Training Loss: 0.05134625732898712\n",
      "Epoch 13795/30000 Training Loss: 0.04433557391166687\n",
      "Epoch 13796/30000 Training Loss: 0.03464748337864876\n",
      "Epoch 13797/30000 Training Loss: 0.050105683505535126\n",
      "Epoch 13798/30000 Training Loss: 0.045449525117874146\n",
      "Epoch 13799/30000 Training Loss: 0.06614747643470764\n",
      "Epoch 13800/30000 Training Loss: 0.06590739637613297\n",
      "Epoch 13800/30000 Validation Loss: 0.05385688692331314\n",
      "Epoch 13801/30000 Training Loss: 0.04629933089017868\n",
      "Epoch 13802/30000 Training Loss: 0.05312483385205269\n",
      "Epoch 13803/30000 Training Loss: 0.05909092351794243\n",
      "Epoch 13804/30000 Training Loss: 0.06196081265807152\n",
      "Epoch 13805/30000 Training Loss: 0.059379130601882935\n",
      "Epoch 13806/30000 Training Loss: 0.03904067724943161\n",
      "Epoch 13807/30000 Training Loss: 0.061089709401130676\n",
      "Epoch 13808/30000 Training Loss: 0.04414302110671997\n",
      "Epoch 13809/30000 Training Loss: 0.053879108279943466\n",
      "Epoch 13810/30000 Training Loss: 0.06285306066274643\n",
      "Epoch 13811/30000 Training Loss: 0.04611491784453392\n",
      "Epoch 13812/30000 Training Loss: 0.04978836327791214\n",
      "Epoch 13813/30000 Training Loss: 0.06104687601327896\n",
      "Epoch 13814/30000 Training Loss: 0.06081223487854004\n",
      "Epoch 13815/30000 Training Loss: 0.05433247983455658\n",
      "Epoch 13816/30000 Training Loss: 0.0587528795003891\n",
      "Epoch 13817/30000 Training Loss: 0.07137754559516907\n",
      "Epoch 13818/30000 Training Loss: 0.06762556731700897\n",
      "Epoch 13819/30000 Training Loss: 0.041198037564754486\n",
      "Epoch 13820/30000 Training Loss: 0.04992322623729706\n",
      "Epoch 13821/30000 Training Loss: 0.05024858936667442\n",
      "Epoch 13822/30000 Training Loss: 0.06678860634565353\n",
      "Epoch 13823/30000 Training Loss: 0.04209776222705841\n",
      "Epoch 13824/30000 Training Loss: 0.059300050139427185\n",
      "Epoch 13825/30000 Training Loss: 0.051603253930807114\n",
      "Epoch 13826/30000 Training Loss: 0.04558243229985237\n",
      "Epoch 13827/30000 Training Loss: 0.040983784943819046\n",
      "Epoch 13828/30000 Training Loss: 0.054633673280477524\n",
      "Epoch 13829/30000 Training Loss: 0.0338912159204483\n",
      "Epoch 13830/30000 Training Loss: 0.04608190804719925\n",
      "Epoch 13831/30000 Training Loss: 0.05080007016658783\n",
      "Epoch 13832/30000 Training Loss: 0.04177838936448097\n",
      "Epoch 13833/30000 Training Loss: 0.032541677355766296\n",
      "Epoch 13834/30000 Training Loss: 0.057502008974552155\n",
      "Epoch 13835/30000 Training Loss: 0.042995139956474304\n",
      "Epoch 13836/30000 Training Loss: 0.04465802386403084\n",
      "Epoch 13837/30000 Training Loss: 0.05506882444024086\n",
      "Epoch 13838/30000 Training Loss: 0.061849188059568405\n",
      "Epoch 13839/30000 Training Loss: 0.05433438718318939\n",
      "Epoch 13840/30000 Training Loss: 0.04127167910337448\n",
      "Epoch 13841/30000 Training Loss: 0.0420764684677124\n",
      "Epoch 13842/30000 Training Loss: 0.04223870113492012\n",
      "Epoch 13843/30000 Training Loss: 0.04033133387565613\n",
      "Epoch 13844/30000 Training Loss: 0.04243183508515358\n",
      "Epoch 13845/30000 Training Loss: 0.041473567485809326\n",
      "Epoch 13846/30000 Training Loss: 0.04842959716916084\n",
      "Epoch 13847/30000 Training Loss: 0.04699856787919998\n",
      "Epoch 13848/30000 Training Loss: 0.05727001279592514\n",
      "Epoch 13849/30000 Training Loss: 0.031096143648028374\n",
      "Epoch 13850/30000 Training Loss: 0.060592569410800934\n",
      "Epoch 13851/30000 Training Loss: 0.047313716262578964\n",
      "Epoch 13852/30000 Training Loss: 0.03727572411298752\n",
      "Epoch 13853/30000 Training Loss: 0.05733892321586609\n",
      "Epoch 13854/30000 Training Loss: 0.043104205280542374\n",
      "Epoch 13855/30000 Training Loss: 0.05033547431230545\n",
      "Epoch 13856/30000 Training Loss: 0.045628711581230164\n",
      "Epoch 13857/30000 Training Loss: 0.05799739062786102\n",
      "Epoch 13858/30000 Training Loss: 0.04891379177570343\n",
      "Epoch 13859/30000 Training Loss: 0.05966254696249962\n",
      "Epoch 13860/30000 Training Loss: 0.040928248316049576\n",
      "Epoch 13861/30000 Training Loss: 0.039520811289548874\n",
      "Epoch 13862/30000 Training Loss: 0.04050440713763237\n",
      "Epoch 13863/30000 Training Loss: 0.06140792742371559\n",
      "Epoch 13864/30000 Training Loss: 0.0536794438958168\n",
      "Epoch 13865/30000 Training Loss: 0.0478154756128788\n",
      "Epoch 13866/30000 Training Loss: 0.055450886487960815\n",
      "Epoch 13867/30000 Training Loss: 0.04296759516000748\n",
      "Epoch 13868/30000 Training Loss: 0.07299574464559555\n",
      "Epoch 13869/30000 Training Loss: 0.049787044525146484\n",
      "Epoch 13870/30000 Training Loss: 0.04053289815783501\n",
      "Epoch 13871/30000 Training Loss: 0.06396716833114624\n",
      "Epoch 13872/30000 Training Loss: 0.049929458647966385\n",
      "Epoch 13873/30000 Training Loss: 0.05379784107208252\n",
      "Epoch 13874/30000 Training Loss: 0.06903009861707687\n",
      "Epoch 13875/30000 Training Loss: 0.043766722083091736\n",
      "Epoch 13876/30000 Training Loss: 0.04016707465052605\n",
      "Epoch 13877/30000 Training Loss: 0.047773316502571106\n",
      "Epoch 13878/30000 Training Loss: 0.03867148980498314\n",
      "Epoch 13879/30000 Training Loss: 0.06276819854974747\n",
      "Epoch 13880/30000 Training Loss: 0.051518701016902924\n",
      "Epoch 13881/30000 Training Loss: 0.04922690615057945\n",
      "Epoch 13882/30000 Training Loss: 0.04271971434354782\n",
      "Epoch 13883/30000 Training Loss: 0.06384724378585815\n",
      "Epoch 13884/30000 Training Loss: 0.04937375336885452\n",
      "Epoch 13885/30000 Training Loss: 0.05262045934796333\n",
      "Epoch 13886/30000 Training Loss: 0.07541240006685257\n",
      "Epoch 13887/30000 Training Loss: 0.03838266059756279\n",
      "Epoch 13888/30000 Training Loss: 0.048616763204336166\n",
      "Epoch 13889/30000 Training Loss: 0.05377621948719025\n",
      "Epoch 13890/30000 Training Loss: 0.06265223771333694\n",
      "Epoch 13891/30000 Training Loss: 0.05155668407678604\n",
      "Epoch 13892/30000 Training Loss: 0.0577656626701355\n",
      "Epoch 13893/30000 Training Loss: 0.03830058127641678\n",
      "Epoch 13894/30000 Training Loss: 0.044330332428216934\n",
      "Epoch 13895/30000 Training Loss: 0.05787613242864609\n",
      "Epoch 13896/30000 Training Loss: 0.0528927706182003\n",
      "Epoch 13897/30000 Training Loss: 0.040367916226387024\n",
      "Epoch 13898/30000 Training Loss: 0.055192604660987854\n",
      "Epoch 13899/30000 Training Loss: 0.047080643475055695\n",
      "Epoch 13900/30000 Training Loss: 0.052854374051094055\n",
      "Epoch 13900/30000 Validation Loss: 0.05519355461001396\n",
      "Epoch 13901/30000 Training Loss: 0.06985360383987427\n",
      "Epoch 13902/30000 Training Loss: 0.0351717472076416\n",
      "Epoch 13903/30000 Training Loss: 0.036403216421604156\n",
      "Epoch 13904/30000 Training Loss: 0.032148394733667374\n",
      "Epoch 13905/30000 Training Loss: 0.03985559195280075\n",
      "Epoch 13906/30000 Training Loss: 0.05583503842353821\n",
      "Epoch 13907/30000 Training Loss: 0.045559823513031006\n",
      "Epoch 13908/30000 Training Loss: 0.05143992602825165\n",
      "Epoch 13909/30000 Training Loss: 0.05013839155435562\n",
      "Epoch 13910/30000 Training Loss: 0.03619229793548584\n",
      "Epoch 13911/30000 Training Loss: 0.04796916991472244\n",
      "Epoch 13912/30000 Training Loss: 0.08853667229413986\n",
      "Epoch 13913/30000 Training Loss: 0.06533898413181305\n",
      "Epoch 13914/30000 Training Loss: 0.05034002661705017\n",
      "Epoch 13915/30000 Training Loss: 0.0457964688539505\n",
      "Epoch 13916/30000 Training Loss: 0.04564772546291351\n",
      "Epoch 13917/30000 Training Loss: 0.05002431944012642\n",
      "Epoch 13918/30000 Training Loss: 0.06048758700489998\n",
      "Epoch 13919/30000 Training Loss: 0.04911341518163681\n",
      "Epoch 13920/30000 Training Loss: 0.03521915525197983\n",
      "Epoch 13921/30000 Training Loss: 0.06166565790772438\n",
      "Epoch 13922/30000 Training Loss: 0.04461659863591194\n",
      "Epoch 13923/30000 Training Loss: 0.05704408884048462\n",
      "Epoch 13924/30000 Training Loss: 0.05712547153234482\n",
      "Epoch 13925/30000 Training Loss: 0.0672835260629654\n",
      "Epoch 13926/30000 Training Loss: 0.061166271567344666\n",
      "Epoch 13927/30000 Training Loss: 0.04284685105085373\n",
      "Epoch 13928/30000 Training Loss: 0.05458414554595947\n",
      "Epoch 13929/30000 Training Loss: 0.05117802321910858\n",
      "Epoch 13930/30000 Training Loss: 0.04448794946074486\n",
      "Epoch 13931/30000 Training Loss: 0.04430193454027176\n",
      "Epoch 13932/30000 Training Loss: 0.04149816185235977\n",
      "Epoch 13933/30000 Training Loss: 0.05113224685192108\n",
      "Epoch 13934/30000 Training Loss: 0.05975079536437988\n",
      "Epoch 13935/30000 Training Loss: 0.07128258794546127\n",
      "Epoch 13936/30000 Training Loss: 0.03860194608569145\n",
      "Epoch 13937/30000 Training Loss: 0.04424523189663887\n",
      "Epoch 13938/30000 Training Loss: 0.05555524677038193\n",
      "Epoch 13939/30000 Training Loss: 0.06470713019371033\n",
      "Epoch 13940/30000 Training Loss: 0.04422289505600929\n",
      "Epoch 13941/30000 Training Loss: 0.06426442414522171\n",
      "Epoch 13942/30000 Training Loss: 0.046531856060028076\n",
      "Epoch 13943/30000 Training Loss: 0.054352737963199615\n",
      "Epoch 13944/30000 Training Loss: 0.0598134770989418\n",
      "Epoch 13945/30000 Training Loss: 0.054159365594387054\n",
      "Epoch 13946/30000 Training Loss: 0.046931345015764236\n",
      "Epoch 13947/30000 Training Loss: 0.0650603249669075\n",
      "Epoch 13948/30000 Training Loss: 0.038434937596321106\n",
      "Epoch 13949/30000 Training Loss: 0.043836746364831924\n",
      "Epoch 13950/30000 Training Loss: 0.06220896542072296\n",
      "Epoch 13951/30000 Training Loss: 0.05450878292322159\n",
      "Epoch 13952/30000 Training Loss: 0.06149820610880852\n",
      "Epoch 13953/30000 Training Loss: 0.04390665143728256\n",
      "Epoch 13954/30000 Training Loss: 0.03485614061355591\n",
      "Epoch 13955/30000 Training Loss: 0.055658791214227676\n",
      "Epoch 13956/30000 Training Loss: 0.03514177352190018\n",
      "Epoch 13957/30000 Training Loss: 0.04629480466246605\n",
      "Epoch 13958/30000 Training Loss: 0.04678298532962799\n",
      "Epoch 13959/30000 Training Loss: 0.048739220947027206\n",
      "Epoch 13960/30000 Training Loss: 0.05846116691827774\n",
      "Epoch 13961/30000 Training Loss: 0.0475052073597908\n",
      "Epoch 13962/30000 Training Loss: 0.03853340446949005\n",
      "Epoch 13963/30000 Training Loss: 0.05824006348848343\n",
      "Epoch 13964/30000 Training Loss: 0.056370340287685394\n",
      "Epoch 13965/30000 Training Loss: 0.039716266095638275\n",
      "Epoch 13966/30000 Training Loss: 0.0470416322350502\n",
      "Epoch 13967/30000 Training Loss: 0.051238954067230225\n",
      "Epoch 13968/30000 Training Loss: 0.055342577397823334\n",
      "Epoch 13969/30000 Training Loss: 0.07077652961015701\n",
      "Epoch 13970/30000 Training Loss: 0.040704142302274704\n",
      "Epoch 13971/30000 Training Loss: 0.051928360015153885\n",
      "Epoch 13972/30000 Training Loss: 0.05480831861495972\n",
      "Epoch 13973/30000 Training Loss: 0.06702921539545059\n",
      "Epoch 13974/30000 Training Loss: 0.056912850588560104\n",
      "Epoch 13975/30000 Training Loss: 0.0460810661315918\n",
      "Epoch 13976/30000 Training Loss: 0.05551046505570412\n",
      "Epoch 13977/30000 Training Loss: 0.044810812920331955\n",
      "Epoch 13978/30000 Training Loss: 0.03924671560525894\n",
      "Epoch 13979/30000 Training Loss: 0.050903644412755966\n",
      "Epoch 13980/30000 Training Loss: 0.04647387936711311\n",
      "Epoch 13981/30000 Training Loss: 0.039606936275959015\n",
      "Epoch 13982/30000 Training Loss: 0.0524587444961071\n",
      "Epoch 13983/30000 Training Loss: 0.049877721816301346\n",
      "Epoch 13984/30000 Training Loss: 0.039501313120126724\n",
      "Epoch 13985/30000 Training Loss: 0.0575537271797657\n",
      "Epoch 13986/30000 Training Loss: 0.0662296861410141\n",
      "Epoch 13987/30000 Training Loss: 0.050010547041893005\n",
      "Epoch 13988/30000 Training Loss: 0.04243297502398491\n",
      "Epoch 13989/30000 Training Loss: 0.05434698611497879\n",
      "Epoch 13990/30000 Training Loss: 0.03867373988032341\n",
      "Epoch 13991/30000 Training Loss: 0.05821914225816727\n",
      "Epoch 13992/30000 Training Loss: 0.04357178136706352\n",
      "Epoch 13993/30000 Training Loss: 0.05719400942325592\n",
      "Epoch 13994/30000 Training Loss: 0.045783355832099915\n",
      "Epoch 13995/30000 Training Loss: 0.06352079659700394\n",
      "Epoch 13996/30000 Training Loss: 0.04909650236368179\n",
      "Epoch 13997/30000 Training Loss: 0.05187857896089554\n",
      "Epoch 13998/30000 Training Loss: 0.04343684762716293\n",
      "Epoch 13999/30000 Training Loss: 0.04811016097664833\n",
      "Epoch 14000/30000 Training Loss: 0.05156944692134857\n",
      "Epoch 14000/30000 Validation Loss: 0.04156055673956871\n",
      "Epoch 14001/30000 Training Loss: 0.057580672204494476\n",
      "Epoch 14002/30000 Training Loss: 0.03385009244084358\n",
      "Epoch 14003/30000 Training Loss: 0.04177842289209366\n",
      "Epoch 14004/30000 Training Loss: 0.04341824725270271\n",
      "Epoch 14005/30000 Training Loss: 0.05670531094074249\n",
      "Epoch 14006/30000 Training Loss: 0.05790665000677109\n",
      "Epoch 14007/30000 Training Loss: 0.03953869268298149\n",
      "Epoch 14008/30000 Training Loss: 0.053041972219944\n",
      "Epoch 14009/30000 Training Loss: 0.0496903695166111\n",
      "Epoch 14010/30000 Training Loss: 0.0411943718791008\n",
      "Epoch 14011/30000 Training Loss: 0.051709819585084915\n",
      "Epoch 14012/30000 Training Loss: 0.03238070756196976\n",
      "Epoch 14013/30000 Training Loss: 0.06018325686454773\n",
      "Epoch 14014/30000 Training Loss: 0.046174563467502594\n",
      "Epoch 14015/30000 Training Loss: 0.042391467839479446\n",
      "Epoch 14016/30000 Training Loss: 0.03911099582910538\n",
      "Epoch 14017/30000 Training Loss: 0.04465196281671524\n",
      "Epoch 14018/30000 Training Loss: 0.055422160774469376\n",
      "Epoch 14019/30000 Training Loss: 0.05722985789179802\n",
      "Epoch 14020/30000 Training Loss: 0.052462995052337646\n",
      "Epoch 14021/30000 Training Loss: 0.043134596198797226\n",
      "Epoch 14022/30000 Training Loss: 0.05508028715848923\n",
      "Epoch 14023/30000 Training Loss: 0.06493192166090012\n",
      "Epoch 14024/30000 Training Loss: 0.04575549438595772\n",
      "Epoch 14025/30000 Training Loss: 0.038106225430965424\n",
      "Epoch 14026/30000 Training Loss: 0.0463932640850544\n",
      "Epoch 14027/30000 Training Loss: 0.0514693409204483\n",
      "Epoch 14028/30000 Training Loss: 0.04880893975496292\n",
      "Epoch 14029/30000 Training Loss: 0.06664395332336426\n",
      "Epoch 14030/30000 Training Loss: 0.04977288097143173\n",
      "Epoch 14031/30000 Training Loss: 0.03237120062112808\n",
      "Epoch 14032/30000 Training Loss: 0.04264742136001587\n",
      "Epoch 14033/30000 Training Loss: 0.053509440273046494\n",
      "Epoch 14034/30000 Training Loss: 0.042567282915115356\n",
      "Epoch 14035/30000 Training Loss: 0.040468666702508926\n",
      "Epoch 14036/30000 Training Loss: 0.04292788356542587\n",
      "Epoch 14037/30000 Training Loss: 0.06956538558006287\n",
      "Epoch 14038/30000 Training Loss: 0.04915529116988182\n",
      "Epoch 14039/30000 Training Loss: 0.05916503071784973\n",
      "Epoch 14040/30000 Training Loss: 0.035627659410238266\n",
      "Epoch 14041/30000 Training Loss: 0.0637211948633194\n",
      "Epoch 14042/30000 Training Loss: 0.05324544012546539\n",
      "Epoch 14043/30000 Training Loss: 0.04460281878709793\n",
      "Epoch 14044/30000 Training Loss: 0.028828607872128487\n",
      "Epoch 14045/30000 Training Loss: 0.047603242099285126\n",
      "Epoch 14046/30000 Training Loss: 0.04938427731394768\n",
      "Epoch 14047/30000 Training Loss: 0.04460988938808441\n",
      "Epoch 14048/30000 Training Loss: 0.04616875573992729\n",
      "Epoch 14049/30000 Training Loss: 0.04705366492271423\n",
      "Epoch 14050/30000 Training Loss: 0.04058169946074486\n",
      "Epoch 14051/30000 Training Loss: 0.04932568222284317\n",
      "Epoch 14052/30000 Training Loss: 0.0463123582303524\n",
      "Epoch 14053/30000 Training Loss: 0.03967728093266487\n",
      "Epoch 14054/30000 Training Loss: 0.06424589455127716\n",
      "Epoch 14055/30000 Training Loss: 0.040589649230241776\n",
      "Epoch 14056/30000 Training Loss: 0.04352256655693054\n",
      "Epoch 14057/30000 Training Loss: 0.05680592358112335\n",
      "Epoch 14058/30000 Training Loss: 0.03779814392328262\n",
      "Epoch 14059/30000 Training Loss: 0.04491949453949928\n",
      "Epoch 14060/30000 Training Loss: 0.04591361805796623\n",
      "Epoch 14061/30000 Training Loss: 0.046811893582344055\n",
      "Epoch 14062/30000 Training Loss: 0.0493842214345932\n",
      "Epoch 14063/30000 Training Loss: 0.04076924920082092\n",
      "Epoch 14064/30000 Training Loss: 0.04358261451125145\n",
      "Epoch 14065/30000 Training Loss: 0.046177759766578674\n",
      "Epoch 14066/30000 Training Loss: 0.0579695887863636\n",
      "Epoch 14067/30000 Training Loss: 0.03885137662291527\n",
      "Epoch 14068/30000 Training Loss: 0.061841972172260284\n",
      "Epoch 14069/30000 Training Loss: 0.048657018691301346\n",
      "Epoch 14070/30000 Training Loss: 0.05412525311112404\n",
      "Epoch 14071/30000 Training Loss: 0.04505062475800514\n",
      "Epoch 14072/30000 Training Loss: 0.0697597861289978\n",
      "Epoch 14073/30000 Training Loss: 0.046386998146772385\n",
      "Epoch 14074/30000 Training Loss: 0.05971142277121544\n",
      "Epoch 14075/30000 Training Loss: 0.05111692100763321\n",
      "Epoch 14076/30000 Training Loss: 0.04225945845246315\n",
      "Epoch 14077/30000 Training Loss: 0.0465230755507946\n",
      "Epoch 14078/30000 Training Loss: 0.04365410655736923\n",
      "Epoch 14079/30000 Training Loss: 0.05168648809194565\n",
      "Epoch 14080/30000 Training Loss: 0.05168263241648674\n",
      "Epoch 14081/30000 Training Loss: 0.042209431529045105\n",
      "Epoch 14082/30000 Training Loss: 0.0387934111058712\n",
      "Epoch 14083/30000 Training Loss: 0.0336829349398613\n",
      "Epoch 14084/30000 Training Loss: 0.05892808735370636\n",
      "Epoch 14085/30000 Training Loss: 0.04984734207391739\n",
      "Epoch 14086/30000 Training Loss: 0.048568107187747955\n",
      "Epoch 14087/30000 Training Loss: 0.039493802934885025\n",
      "Epoch 14088/30000 Training Loss: 0.05070711672306061\n",
      "Epoch 14089/30000 Training Loss: 0.05328971520066261\n",
      "Epoch 14090/30000 Training Loss: 0.05335674807429314\n",
      "Epoch 14091/30000 Training Loss: 0.05772929638624191\n",
      "Epoch 14092/30000 Training Loss: 0.06341378390789032\n",
      "Epoch 14093/30000 Training Loss: 0.04986903816461563\n",
      "Epoch 14094/30000 Training Loss: 0.055251214653253555\n",
      "Epoch 14095/30000 Training Loss: 0.04107525199651718\n",
      "Epoch 14096/30000 Training Loss: 0.05077866464853287\n",
      "Epoch 14097/30000 Training Loss: 0.035310789942741394\n",
      "Epoch 14098/30000 Training Loss: 0.0563204288482666\n",
      "Epoch 14099/30000 Training Loss: 0.05296233296394348\n",
      "Epoch 14100/30000 Training Loss: 0.06993874907493591\n",
      "Epoch 14100/30000 Validation Loss: 0.06041485816240311\n",
      "Epoch 14101/30000 Training Loss: 0.040607765316963196\n",
      "Epoch 14102/30000 Training Loss: 0.04334855079650879\n",
      "Epoch 14103/30000 Training Loss: 0.04316886514425278\n",
      "Epoch 14104/30000 Training Loss: 0.05539918690919876\n",
      "Epoch 14105/30000 Training Loss: 0.04987795278429985\n",
      "Epoch 14106/30000 Training Loss: 0.05612704157829285\n",
      "Epoch 14107/30000 Training Loss: 0.06866548955440521\n",
      "Epoch 14108/30000 Training Loss: 0.061009425669908524\n",
      "Epoch 14109/30000 Training Loss: 0.055402837693691254\n",
      "Epoch 14110/30000 Training Loss: 0.05264368653297424\n",
      "Epoch 14111/30000 Training Loss: 0.04325195774435997\n",
      "Epoch 14112/30000 Training Loss: 0.06157519668340683\n",
      "Epoch 14113/30000 Training Loss: 0.05023302510380745\n",
      "Epoch 14114/30000 Training Loss: 0.05348290503025055\n",
      "Epoch 14115/30000 Training Loss: 0.03733883053064346\n",
      "Epoch 14116/30000 Training Loss: 0.06323959678411484\n",
      "Epoch 14117/30000 Training Loss: 0.04068377614021301\n",
      "Epoch 14118/30000 Training Loss: 0.0774397924542427\n",
      "Epoch 14119/30000 Training Loss: 0.06839054077863693\n",
      "Epoch 14120/30000 Training Loss: 0.04181418940424919\n",
      "Epoch 14121/30000 Training Loss: 0.05267217382788658\n",
      "Epoch 14122/30000 Training Loss: 0.05341004580259323\n",
      "Epoch 14123/30000 Training Loss: 0.04806193709373474\n",
      "Epoch 14124/30000 Training Loss: 0.05641031265258789\n",
      "Epoch 14125/30000 Training Loss: 0.05061803758144379\n",
      "Epoch 14126/30000 Training Loss: 0.0534713976085186\n",
      "Epoch 14127/30000 Training Loss: 0.04612617939710617\n",
      "Epoch 14128/30000 Training Loss: 0.056716542690992355\n",
      "Epoch 14129/30000 Training Loss: 0.04155798628926277\n",
      "Epoch 14130/30000 Training Loss: 0.04873059317469597\n",
      "Epoch 14131/30000 Training Loss: 0.05113393813371658\n",
      "Epoch 14132/30000 Training Loss: 0.04257047176361084\n",
      "Epoch 14133/30000 Training Loss: 0.05160096660256386\n",
      "Epoch 14134/30000 Training Loss: 0.04763109236955643\n",
      "Epoch 14135/30000 Training Loss: 0.04958508163690567\n",
      "Epoch 14136/30000 Training Loss: 0.039828136563301086\n",
      "Epoch 14137/30000 Training Loss: 0.06173042953014374\n",
      "Epoch 14138/30000 Training Loss: 0.05289382487535477\n",
      "Epoch 14139/30000 Training Loss: 0.039126917719841\n",
      "Epoch 14140/30000 Training Loss: 0.03473791107535362\n",
      "Epoch 14141/30000 Training Loss: 0.06004415452480316\n",
      "Epoch 14142/30000 Training Loss: 0.05407927557826042\n",
      "Epoch 14143/30000 Training Loss: 0.058742307126522064\n",
      "Epoch 14144/30000 Training Loss: 0.04418587312102318\n",
      "Epoch 14145/30000 Training Loss: 0.056715600192546844\n",
      "Epoch 14146/30000 Training Loss: 0.048710789531469345\n",
      "Epoch 14147/30000 Training Loss: 0.08191940188407898\n",
      "Epoch 14148/30000 Training Loss: 0.04443445801734924\n",
      "Epoch 14149/30000 Training Loss: 0.06157328933477402\n",
      "Epoch 14150/30000 Training Loss: 0.06594675779342651\n",
      "Epoch 14151/30000 Training Loss: 0.04447726905345917\n",
      "Epoch 14152/30000 Training Loss: 0.03801647573709488\n",
      "Epoch 14153/30000 Training Loss: 0.039999935775995255\n",
      "Epoch 14154/30000 Training Loss: 0.04074905067682266\n",
      "Epoch 14155/30000 Training Loss: 0.061925508081912994\n",
      "Epoch 14156/30000 Training Loss: 0.05376092344522476\n",
      "Epoch 14157/30000 Training Loss: 0.044187720865011215\n",
      "Epoch 14158/30000 Training Loss: 0.064156174659729\n",
      "Epoch 14159/30000 Training Loss: 0.04758678749203682\n",
      "Epoch 14160/30000 Training Loss: 0.059822823852300644\n",
      "Epoch 14161/30000 Training Loss: 0.048323433846235275\n",
      "Epoch 14162/30000 Training Loss: 0.037883270531892776\n",
      "Epoch 14163/30000 Training Loss: 0.04465761035680771\n",
      "Epoch 14164/30000 Training Loss: 0.048245444893836975\n",
      "Epoch 14165/30000 Training Loss: 0.03995988517999649\n",
      "Epoch 14166/30000 Training Loss: 0.047021396458148956\n",
      "Epoch 14167/30000 Training Loss: 0.041920267045497894\n",
      "Epoch 14168/30000 Training Loss: 0.04647362232208252\n",
      "Epoch 14169/30000 Training Loss: 0.05685269460082054\n",
      "Epoch 14170/30000 Training Loss: 0.038492351770401\n",
      "Epoch 14171/30000 Training Loss: 0.04059970751404762\n",
      "Epoch 14172/30000 Training Loss: 0.030992280691862106\n",
      "Epoch 14173/30000 Training Loss: 0.04266466945409775\n",
      "Epoch 14174/30000 Training Loss: 0.06130462884902954\n",
      "Epoch 14175/30000 Training Loss: 0.049985598772764206\n",
      "Epoch 14176/30000 Training Loss: 0.053599122911691666\n",
      "Epoch 14177/30000 Training Loss: 0.041606951504945755\n",
      "Epoch 14178/30000 Training Loss: 0.044027168303728104\n",
      "Epoch 14179/30000 Training Loss: 0.04321473464369774\n",
      "Epoch 14180/30000 Training Loss: 0.03800906985998154\n",
      "Epoch 14181/30000 Training Loss: 0.060034818947315216\n",
      "Epoch 14182/30000 Training Loss: 0.043445587158203125\n",
      "Epoch 14183/30000 Training Loss: 0.03440584987401962\n",
      "Epoch 14184/30000 Training Loss: 0.045689504593610764\n",
      "Epoch 14185/30000 Training Loss: 0.05678875744342804\n",
      "Epoch 14186/30000 Training Loss: 0.06387536227703094\n",
      "Epoch 14187/30000 Training Loss: 0.044038549065589905\n",
      "Epoch 14188/30000 Training Loss: 0.03916909545660019\n",
      "Epoch 14189/30000 Training Loss: 0.06187049299478531\n",
      "Epoch 14190/30000 Training Loss: 0.048115942627191544\n",
      "Epoch 14191/30000 Training Loss: 0.0479428730905056\n",
      "Epoch 14192/30000 Training Loss: 0.0511605329811573\n",
      "Epoch 14193/30000 Training Loss: 0.05335896834731102\n",
      "Epoch 14194/30000 Training Loss: 0.042220089584589005\n",
      "Epoch 14195/30000 Training Loss: 0.05258667841553688\n",
      "Epoch 14196/30000 Training Loss: 0.049366336315870285\n",
      "Epoch 14197/30000 Training Loss: 0.04360319301486015\n",
      "Epoch 14198/30000 Training Loss: 0.041337087750434875\n",
      "Epoch 14199/30000 Training Loss: 0.055141568183898926\n",
      "Epoch 14200/30000 Training Loss: 0.0419137105345726\n",
      "Epoch 14200/30000 Validation Loss: 0.039136484265327454\n",
      "Epoch 14201/30000 Training Loss: 0.04353208839893341\n",
      "Epoch 14202/30000 Training Loss: 0.049973320215940475\n",
      "Epoch 14203/30000 Training Loss: 0.0379282683134079\n",
      "Epoch 14204/30000 Training Loss: 0.05056491866707802\n",
      "Epoch 14205/30000 Training Loss: 0.03809710592031479\n",
      "Epoch 14206/30000 Training Loss: 0.05482951179146767\n",
      "Epoch 14207/30000 Training Loss: 0.04588133469223976\n",
      "Epoch 14208/30000 Training Loss: 0.05042155086994171\n",
      "Epoch 14209/30000 Training Loss: 0.059029318392276764\n",
      "Epoch 14210/30000 Training Loss: 0.049358218908309937\n",
      "Epoch 14211/30000 Training Loss: 0.07204055041074753\n",
      "Epoch 14212/30000 Training Loss: 0.04906407743692398\n",
      "Epoch 14213/30000 Training Loss: 0.04401399567723274\n",
      "Epoch 14214/30000 Training Loss: 0.0552644282579422\n",
      "Epoch 14215/30000 Training Loss: 0.057077620178461075\n",
      "Epoch 14216/30000 Training Loss: 0.06327695399522781\n",
      "Epoch 14217/30000 Training Loss: 0.052748702466487885\n",
      "Epoch 14218/30000 Training Loss: 0.045473527163267136\n",
      "Epoch 14219/30000 Training Loss: 0.04729130119085312\n",
      "Epoch 14220/30000 Training Loss: 0.05960703641176224\n",
      "Epoch 14221/30000 Training Loss: 0.0460372194647789\n",
      "Epoch 14222/30000 Training Loss: 0.04933909699320793\n",
      "Epoch 14223/30000 Training Loss: 0.029313506558537483\n",
      "Epoch 14224/30000 Training Loss: 0.054702598601579666\n",
      "Epoch 14225/30000 Training Loss: 0.05441815406084061\n",
      "Epoch 14226/30000 Training Loss: 0.041378334164619446\n",
      "Epoch 14227/30000 Training Loss: 0.05701902136206627\n",
      "Epoch 14228/30000 Training Loss: 0.05766693130135536\n",
      "Epoch 14229/30000 Training Loss: 0.05749952048063278\n",
      "Epoch 14230/30000 Training Loss: 0.03930094465613365\n",
      "Epoch 14231/30000 Training Loss: 0.04814540594816208\n",
      "Epoch 14232/30000 Training Loss: 0.05940653383731842\n",
      "Epoch 14233/30000 Training Loss: 0.04790257290005684\n",
      "Epoch 14234/30000 Training Loss: 0.046565163880586624\n",
      "Epoch 14235/30000 Training Loss: 0.05829796567559242\n",
      "Epoch 14236/30000 Training Loss: 0.04301859438419342\n",
      "Epoch 14237/30000 Training Loss: 0.056375082582235336\n",
      "Epoch 14238/30000 Training Loss: 0.039495065808296204\n",
      "Epoch 14239/30000 Training Loss: 0.03359479829668999\n",
      "Epoch 14240/30000 Training Loss: 0.0653989166021347\n",
      "Epoch 14241/30000 Training Loss: 0.04826659709215164\n",
      "Epoch 14242/30000 Training Loss: 0.052835844457149506\n",
      "Epoch 14243/30000 Training Loss: 0.040731389075517654\n",
      "Epoch 14244/30000 Training Loss: 0.06807693839073181\n",
      "Epoch 14245/30000 Training Loss: 0.04483211785554886\n",
      "Epoch 14246/30000 Training Loss: 0.044436391443014145\n",
      "Epoch 14247/30000 Training Loss: 0.04882246255874634\n",
      "Epoch 14248/30000 Training Loss: 0.036127787083387375\n",
      "Epoch 14249/30000 Training Loss: 0.0658264309167862\n",
      "Epoch 14250/30000 Training Loss: 0.048888932913541794\n",
      "Epoch 14251/30000 Training Loss: 0.0571247898042202\n",
      "Epoch 14252/30000 Training Loss: 0.047162167727947235\n",
      "Epoch 14253/30000 Training Loss: 0.05191492289304733\n",
      "Epoch 14254/30000 Training Loss: 0.03967287763953209\n",
      "Epoch 14255/30000 Training Loss: 0.05288711562752724\n",
      "Epoch 14256/30000 Training Loss: 0.04786675050854683\n",
      "Epoch 14257/30000 Training Loss: 0.056679219007492065\n",
      "Epoch 14258/30000 Training Loss: 0.04328092187643051\n",
      "Epoch 14259/30000 Training Loss: 0.05371453985571861\n",
      "Epoch 14260/30000 Training Loss: 0.033743105828762054\n",
      "Epoch 14261/30000 Training Loss: 0.03317767381668091\n",
      "Epoch 14262/30000 Training Loss: 0.04815575107932091\n",
      "Epoch 14263/30000 Training Loss: 0.03575756400823593\n",
      "Epoch 14264/30000 Training Loss: 0.04620802402496338\n",
      "Epoch 14265/30000 Training Loss: 0.03800281882286072\n",
      "Epoch 14266/30000 Training Loss: 0.044679928570985794\n",
      "Epoch 14267/30000 Training Loss: 0.03487028181552887\n",
      "Epoch 14268/30000 Training Loss: 0.04800163581967354\n",
      "Epoch 14269/30000 Training Loss: 0.06993691623210907\n",
      "Epoch 14270/30000 Training Loss: 0.05543534830212593\n",
      "Epoch 14271/30000 Training Loss: 0.047814395278692245\n",
      "Epoch 14272/30000 Training Loss: 0.04963378608226776\n",
      "Epoch 14273/30000 Training Loss: 0.05612707510590553\n",
      "Epoch 14274/30000 Training Loss: 0.053437065333127975\n",
      "Epoch 14275/30000 Training Loss: 0.06418637931346893\n",
      "Epoch 14276/30000 Training Loss: 0.04480087757110596\n",
      "Epoch 14277/30000 Training Loss: 0.044487692415714264\n",
      "Epoch 14278/30000 Training Loss: 0.039267972111701965\n",
      "Epoch 14279/30000 Training Loss: 0.06157471984624863\n",
      "Epoch 14280/30000 Training Loss: 0.04445689916610718\n",
      "Epoch 14281/30000 Training Loss: 0.05164007097482681\n",
      "Epoch 14282/30000 Training Loss: 0.052876077592372894\n",
      "Epoch 14283/30000 Training Loss: 0.03897438570857048\n",
      "Epoch 14284/30000 Training Loss: 0.059358395636081696\n",
      "Epoch 14285/30000 Training Loss: 0.05297859013080597\n",
      "Epoch 14286/30000 Training Loss: 0.05601432919502258\n",
      "Epoch 14287/30000 Training Loss: 0.0609070286154747\n",
      "Epoch 14288/30000 Training Loss: 0.04085766151547432\n",
      "Epoch 14289/30000 Training Loss: 0.05102671682834625\n",
      "Epoch 14290/30000 Training Loss: 0.0471496656537056\n",
      "Epoch 14291/30000 Training Loss: 0.04476601257920265\n",
      "Epoch 14292/30000 Training Loss: 0.056397780776023865\n",
      "Epoch 14293/30000 Training Loss: 0.04484308883547783\n",
      "Epoch 14294/30000 Training Loss: 0.04672235995531082\n",
      "Epoch 14295/30000 Training Loss: 0.06643854826688766\n",
      "Epoch 14296/30000 Training Loss: 0.044218726456165314\n",
      "Epoch 14297/30000 Training Loss: 0.06188293173909187\n",
      "Epoch 14298/30000 Training Loss: 0.03965972736477852\n",
      "Epoch 14299/30000 Training Loss: 0.05870124697685242\n",
      "Epoch 14300/30000 Training Loss: 0.04558081924915314\n",
      "Epoch 14300/30000 Validation Loss: 0.04428691044449806\n",
      "Epoch 14301/30000 Training Loss: 0.04405096918344498\n",
      "Epoch 14302/30000 Training Loss: 0.05073191970586777\n",
      "Epoch 14303/30000 Training Loss: 0.04785321652889252\n",
      "Epoch 14304/30000 Training Loss: 0.04126552864909172\n",
      "Epoch 14305/30000 Training Loss: 0.04819661006331444\n",
      "Epoch 14306/30000 Training Loss: 0.04990589991211891\n",
      "Epoch 14307/30000 Training Loss: 0.05047720670700073\n",
      "Epoch 14308/30000 Training Loss: 0.04334774985909462\n",
      "Epoch 14309/30000 Training Loss: 0.043631505221128464\n",
      "Epoch 14310/30000 Training Loss: 0.05676698684692383\n",
      "Epoch 14311/30000 Training Loss: 0.04943600296974182\n",
      "Epoch 14312/30000 Training Loss: 0.04120182991027832\n",
      "Epoch 14313/30000 Training Loss: 0.044735293835401535\n",
      "Epoch 14314/30000 Training Loss: 0.061377041041851044\n",
      "Epoch 14315/30000 Training Loss: 0.06188704073429108\n",
      "Epoch 14316/30000 Training Loss: 0.05552836135029793\n",
      "Epoch 14317/30000 Training Loss: 0.04429207369685173\n",
      "Epoch 14318/30000 Training Loss: 0.05631273612380028\n",
      "Epoch 14319/30000 Training Loss: 0.05528925731778145\n",
      "Epoch 14320/30000 Training Loss: 0.04700978845357895\n",
      "Epoch 14321/30000 Training Loss: 0.04772567376494408\n",
      "Epoch 14322/30000 Training Loss: 0.05372852832078934\n",
      "Epoch 14323/30000 Training Loss: 0.05838078260421753\n",
      "Epoch 14324/30000 Training Loss: 0.04849087819457054\n",
      "Epoch 14325/30000 Training Loss: 0.05115164443850517\n",
      "Epoch 14326/30000 Training Loss: 0.06334082037210464\n",
      "Epoch 14327/30000 Training Loss: 0.0568796806037426\n",
      "Epoch 14328/30000 Training Loss: 0.03701101988554001\n",
      "Epoch 14329/30000 Training Loss: 0.04870930686593056\n",
      "Epoch 14330/30000 Training Loss: 0.0362839438021183\n",
      "Epoch 14331/30000 Training Loss: 0.04387769103050232\n",
      "Epoch 14332/30000 Training Loss: 0.06562565267086029\n",
      "Epoch 14333/30000 Training Loss: 0.05385483801364899\n",
      "Epoch 14334/30000 Training Loss: 0.0415118932723999\n",
      "Epoch 14335/30000 Training Loss: 0.04492587596178055\n",
      "Epoch 14336/30000 Training Loss: 0.05308115482330322\n",
      "Epoch 14337/30000 Training Loss: 0.07027677446603775\n",
      "Epoch 14338/30000 Training Loss: 0.06240880489349365\n",
      "Epoch 14339/30000 Training Loss: 0.0425659641623497\n",
      "Epoch 14340/30000 Training Loss: 0.04220449551939964\n",
      "Epoch 14341/30000 Training Loss: 0.03889160975813866\n",
      "Epoch 14342/30000 Training Loss: 0.05848991870880127\n",
      "Epoch 14343/30000 Training Loss: 0.04235704243183136\n",
      "Epoch 14344/30000 Training Loss: 0.0625629872083664\n",
      "Epoch 14345/30000 Training Loss: 0.04860035702586174\n",
      "Epoch 14346/30000 Training Loss: 0.057027749717235565\n",
      "Epoch 14347/30000 Training Loss: 0.036100905388593674\n",
      "Epoch 14348/30000 Training Loss: 0.04026966542005539\n",
      "Epoch 14349/30000 Training Loss: 0.06899785250425339\n",
      "Epoch 14350/30000 Training Loss: 0.04968533292412758\n",
      "Epoch 14351/30000 Training Loss: 0.06818835437297821\n",
      "Epoch 14352/30000 Training Loss: 0.05028058961033821\n",
      "Epoch 14353/30000 Training Loss: 0.06569018959999084\n",
      "Epoch 14354/30000 Training Loss: 0.0407571941614151\n",
      "Epoch 14355/30000 Training Loss: 0.028885215520858765\n",
      "Epoch 14356/30000 Training Loss: 0.04996120184659958\n",
      "Epoch 14357/30000 Training Loss: 0.04475005716085434\n",
      "Epoch 14358/30000 Training Loss: 0.04953846335411072\n",
      "Epoch 14359/30000 Training Loss: 0.04828696325421333\n",
      "Epoch 14360/30000 Training Loss: 0.05480353534221649\n",
      "Epoch 14361/30000 Training Loss: 0.06409372389316559\n",
      "Epoch 14362/30000 Training Loss: 0.05112229660153389\n",
      "Epoch 14363/30000 Training Loss: 0.047479335218667984\n",
      "Epoch 14364/30000 Training Loss: 0.046485740691423416\n",
      "Epoch 14365/30000 Training Loss: 0.06533727794885635\n",
      "Epoch 14366/30000 Training Loss: 0.05682414770126343\n",
      "Epoch 14367/30000 Training Loss: 0.05001816153526306\n",
      "Epoch 14368/30000 Training Loss: 0.06464023888111115\n",
      "Epoch 14369/30000 Training Loss: 0.03710003197193146\n",
      "Epoch 14370/30000 Training Loss: 0.04873528331518173\n",
      "Epoch 14371/30000 Training Loss: 0.043272800743579865\n",
      "Epoch 14372/30000 Training Loss: 0.07194973528385162\n",
      "Epoch 14373/30000 Training Loss: 0.04922286793589592\n",
      "Epoch 14374/30000 Training Loss: 0.051438264548778534\n",
      "Epoch 14375/30000 Training Loss: 0.04951305314898491\n",
      "Epoch 14376/30000 Training Loss: 0.0451648123562336\n",
      "Epoch 14377/30000 Training Loss: 0.04208211600780487\n",
      "Epoch 14378/30000 Training Loss: 0.036724966019392014\n",
      "Epoch 14379/30000 Training Loss: 0.04422050714492798\n",
      "Epoch 14380/30000 Training Loss: 0.053366899490356445\n",
      "Epoch 14381/30000 Training Loss: 0.05127092823386192\n",
      "Epoch 14382/30000 Training Loss: 0.04474901035428047\n",
      "Epoch 14383/30000 Training Loss: 0.0456988587975502\n",
      "Epoch 14384/30000 Training Loss: 0.049799688160419464\n",
      "Epoch 14385/30000 Training Loss: 0.059613779187202454\n",
      "Epoch 14386/30000 Training Loss: 0.045204341411590576\n",
      "Epoch 14387/30000 Training Loss: 0.07528544217348099\n",
      "Epoch 14388/30000 Training Loss: 0.04781695082783699\n",
      "Epoch 14389/30000 Training Loss: 0.04860292375087738\n",
      "Epoch 14390/30000 Training Loss: 0.07489491999149323\n",
      "Epoch 14391/30000 Training Loss: 0.04017812758684158\n",
      "Epoch 14392/30000 Training Loss: 0.05479623004794121\n",
      "Epoch 14393/30000 Training Loss: 0.0430133193731308\n",
      "Epoch 14394/30000 Training Loss: 0.04329724982380867\n",
      "Epoch 14395/30000 Training Loss: 0.06255047023296356\n",
      "Epoch 14396/30000 Training Loss: 0.039196938276290894\n",
      "Epoch 14397/30000 Training Loss: 0.05458464100956917\n",
      "Epoch 14398/30000 Training Loss: 0.0495632141828537\n",
      "Epoch 14399/30000 Training Loss: 0.05776284635066986\n",
      "Epoch 14400/30000 Training Loss: 0.05573916435241699\n",
      "Epoch 14400/30000 Validation Loss: 0.04590389505028725\n",
      "Epoch 14401/30000 Training Loss: 0.03630813583731651\n",
      "Epoch 14402/30000 Training Loss: 0.04809311404824257\n",
      "Epoch 14403/30000 Training Loss: 0.05016656592488289\n",
      "Epoch 14404/30000 Training Loss: 0.05818144232034683\n",
      "Epoch 14405/30000 Training Loss: 0.054935555905103683\n",
      "Epoch 14406/30000 Training Loss: 0.04231478273868561\n",
      "Epoch 14407/30000 Training Loss: 0.06337137520313263\n",
      "Epoch 14408/30000 Training Loss: 0.0302983820438385\n",
      "Epoch 14409/30000 Training Loss: 0.054801031947135925\n",
      "Epoch 14410/30000 Training Loss: 0.05219700559973717\n",
      "Epoch 14411/30000 Training Loss: 0.05893785133957863\n",
      "Epoch 14412/30000 Training Loss: 0.045277081429958344\n",
      "Epoch 14413/30000 Training Loss: 0.045933108776807785\n",
      "Epoch 14414/30000 Training Loss: 0.05040520429611206\n",
      "Epoch 14415/30000 Training Loss: 0.06339004635810852\n",
      "Epoch 14416/30000 Training Loss: 0.04826495051383972\n",
      "Epoch 14417/30000 Training Loss: 0.05021735653281212\n",
      "Epoch 14418/30000 Training Loss: 0.049429524689912796\n",
      "Epoch 14419/30000 Training Loss: 0.03667130321264267\n",
      "Epoch 14420/30000 Training Loss: 0.04855462908744812\n",
      "Epoch 14421/30000 Training Loss: 0.04315296187996864\n",
      "Epoch 14422/30000 Training Loss: 0.05624301731586456\n",
      "Epoch 14423/30000 Training Loss: 0.050145238637924194\n",
      "Epoch 14424/30000 Training Loss: 0.04829689860343933\n",
      "Epoch 14425/30000 Training Loss: 0.05435526743531227\n",
      "Epoch 14426/30000 Training Loss: 0.03982409089803696\n",
      "Epoch 14427/30000 Training Loss: 0.05664859339594841\n",
      "Epoch 14428/30000 Training Loss: 0.06623896956443787\n",
      "Epoch 14429/30000 Training Loss: 0.05217411741614342\n",
      "Epoch 14430/30000 Training Loss: 0.046496227383613586\n",
      "Epoch 14431/30000 Training Loss: 0.06363056600093842\n",
      "Epoch 14432/30000 Training Loss: 0.05236747860908508\n",
      "Epoch 14433/30000 Training Loss: 0.05629977583885193\n",
      "Epoch 14434/30000 Training Loss: 0.05309389531612396\n",
      "Epoch 14435/30000 Training Loss: 0.05425016209483147\n",
      "Epoch 14436/30000 Training Loss: 0.04205368459224701\n",
      "Epoch 14437/30000 Training Loss: 0.06086869165301323\n",
      "Epoch 14438/30000 Training Loss: 0.05350759997963905\n",
      "Epoch 14439/30000 Training Loss: 0.05535805970430374\n",
      "Epoch 14440/30000 Training Loss: 0.05472835898399353\n",
      "Epoch 14441/30000 Training Loss: 0.041617926210165024\n",
      "Epoch 14442/30000 Training Loss: 0.03935676068067551\n",
      "Epoch 14443/30000 Training Loss: 0.046789880841970444\n",
      "Epoch 14444/30000 Training Loss: 0.04149376600980759\n",
      "Epoch 14445/30000 Training Loss: 0.05525491014122963\n",
      "Epoch 14446/30000 Training Loss: 0.04781617224216461\n",
      "Epoch 14447/30000 Training Loss: 0.05842242389917374\n",
      "Epoch 14448/30000 Training Loss: 0.04175584018230438\n",
      "Epoch 14449/30000 Training Loss: 0.04580409824848175\n",
      "Epoch 14450/30000 Training Loss: 0.054249364882707596\n",
      "Epoch 14451/30000 Training Loss: 0.057283803820610046\n",
      "Epoch 14452/30000 Training Loss: 0.04614527150988579\n",
      "Epoch 14453/30000 Training Loss: 0.04817900434136391\n",
      "Epoch 14454/30000 Training Loss: 0.06383641809225082\n",
      "Epoch 14455/30000 Training Loss: 0.04842256009578705\n",
      "Epoch 14456/30000 Training Loss: 0.049567628651857376\n",
      "Epoch 14457/30000 Training Loss: 0.04768117517232895\n",
      "Epoch 14458/30000 Training Loss: 0.061549440026283264\n",
      "Epoch 14459/30000 Training Loss: 0.05043017491698265\n",
      "Epoch 14460/30000 Training Loss: 0.05900811403989792\n",
      "Epoch 14461/30000 Training Loss: 0.04178815707564354\n",
      "Epoch 14462/30000 Training Loss: 0.05556155741214752\n",
      "Epoch 14463/30000 Training Loss: 0.04801183193922043\n",
      "Epoch 14464/30000 Training Loss: 0.037396177649497986\n",
      "Epoch 14465/30000 Training Loss: 0.04644346982240677\n",
      "Epoch 14466/30000 Training Loss: 0.04285015910863876\n",
      "Epoch 14467/30000 Training Loss: 0.048306744545698166\n",
      "Epoch 14468/30000 Training Loss: 0.058175042271614075\n",
      "Epoch 14469/30000 Training Loss: 0.053724635392427444\n",
      "Epoch 14470/30000 Training Loss: 0.05738922208547592\n",
      "Epoch 14471/30000 Training Loss: 0.04943034052848816\n",
      "Epoch 14472/30000 Training Loss: 0.05878235027194023\n",
      "Epoch 14473/30000 Training Loss: 0.060856807976961136\n",
      "Epoch 14474/30000 Training Loss: 0.04963402450084686\n",
      "Epoch 14475/30000 Training Loss: 0.0350259505212307\n",
      "Epoch 14476/30000 Training Loss: 0.05698961764574051\n",
      "Epoch 14477/30000 Training Loss: 0.04999580979347229\n",
      "Epoch 14478/30000 Training Loss: 0.041208140552043915\n",
      "Epoch 14479/30000 Training Loss: 0.047481879591941833\n",
      "Epoch 14480/30000 Training Loss: 0.034226443618535995\n",
      "Epoch 14481/30000 Training Loss: 0.04889048635959625\n",
      "Epoch 14482/30000 Training Loss: 0.06347954273223877\n",
      "Epoch 14483/30000 Training Loss: 0.05890735238790512\n",
      "Epoch 14484/30000 Training Loss: 0.05225290358066559\n",
      "Epoch 14485/30000 Training Loss: 0.04320145398378372\n",
      "Epoch 14486/30000 Training Loss: 0.031553734093904495\n",
      "Epoch 14487/30000 Training Loss: 0.05564482510089874\n",
      "Epoch 14488/30000 Training Loss: 0.05029725283384323\n",
      "Epoch 14489/30000 Training Loss: 0.05753844603896141\n",
      "Epoch 14490/30000 Training Loss: 0.051132746040821075\n",
      "Epoch 14491/30000 Training Loss: 0.04183606430888176\n",
      "Epoch 14492/30000 Training Loss: 0.03761184215545654\n",
      "Epoch 14493/30000 Training Loss: 0.041197486221790314\n",
      "Epoch 14494/30000 Training Loss: 0.03944777697324753\n",
      "Epoch 14495/30000 Training Loss: 0.031440168619155884\n",
      "Epoch 14496/30000 Training Loss: 0.049487702548503876\n",
      "Epoch 14497/30000 Training Loss: 0.055822670459747314\n",
      "Epoch 14498/30000 Training Loss: 0.041816625744104385\n",
      "Epoch 14499/30000 Training Loss: 0.041753899306058884\n",
      "Epoch 14500/30000 Training Loss: 0.05096631497144699\n",
      "Epoch 14500/30000 Validation Loss: 0.045367222279310226\n",
      "Epoch 14501/30000 Training Loss: 0.049660686403512955\n",
      "Epoch 14502/30000 Training Loss: 0.054771460592746735\n",
      "Epoch 14503/30000 Training Loss: 0.0403054803609848\n",
      "Epoch 14504/30000 Training Loss: 0.05805834382772446\n",
      "Epoch 14505/30000 Training Loss: 0.06755019724369049\n",
      "Epoch 14506/30000 Training Loss: 0.06061714515089989\n",
      "Epoch 14507/30000 Training Loss: 0.06509366631507874\n",
      "Epoch 14508/30000 Training Loss: 0.07304626703262329\n",
      "Epoch 14509/30000 Training Loss: 0.06739405542612076\n",
      "Epoch 14510/30000 Training Loss: 0.03802775219082832\n",
      "Epoch 14511/30000 Training Loss: 0.04885625094175339\n",
      "Epoch 14512/30000 Training Loss: 0.04685627296566963\n",
      "Epoch 14513/30000 Training Loss: 0.03418440371751785\n",
      "Epoch 14514/30000 Training Loss: 0.03850864619016647\n",
      "Epoch 14515/30000 Training Loss: 0.05297868698835373\n",
      "Epoch 14516/30000 Training Loss: 0.04446139186620712\n",
      "Epoch 14517/30000 Training Loss: 0.03655177727341652\n",
      "Epoch 14518/30000 Training Loss: 0.05207952857017517\n",
      "Epoch 14519/30000 Training Loss: 0.04603169485926628\n",
      "Epoch 14520/30000 Training Loss: 0.06535714864730835\n",
      "Epoch 14521/30000 Training Loss: 0.05220809206366539\n",
      "Epoch 14522/30000 Training Loss: 0.045415714383125305\n",
      "Epoch 14523/30000 Training Loss: 0.045550376176834106\n",
      "Epoch 14524/30000 Training Loss: 0.05410999804735184\n",
      "Epoch 14525/30000 Training Loss: 0.043416284024715424\n",
      "Epoch 14526/30000 Training Loss: 0.045821238309144974\n",
      "Epoch 14527/30000 Training Loss: 0.04167588800191879\n",
      "Epoch 14528/30000 Training Loss: 0.0369047075510025\n",
      "Epoch 14529/30000 Training Loss: 0.04084034636616707\n",
      "Epoch 14530/30000 Training Loss: 0.045680463314056396\n",
      "Epoch 14531/30000 Training Loss: 0.03611962869763374\n",
      "Epoch 14532/30000 Training Loss: 0.04774829000234604\n",
      "Epoch 14533/30000 Training Loss: 0.05508003383874893\n",
      "Epoch 14534/30000 Training Loss: 0.052607808262109756\n",
      "Epoch 14535/30000 Training Loss: 0.06317351758480072\n",
      "Epoch 14536/30000 Training Loss: 0.04736969619989395\n",
      "Epoch 14537/30000 Training Loss: 0.03652682155370712\n",
      "Epoch 14538/30000 Training Loss: 0.05241040140390396\n",
      "Epoch 14539/30000 Training Loss: 0.04669375345110893\n",
      "Epoch 14540/30000 Training Loss: 0.05874596908688545\n",
      "Epoch 14541/30000 Training Loss: 0.05109071731567383\n",
      "Epoch 14542/30000 Training Loss: 0.04242970049381256\n",
      "Epoch 14543/30000 Training Loss: 0.03651118278503418\n",
      "Epoch 14544/30000 Training Loss: 0.06530028581619263\n",
      "Epoch 14545/30000 Training Loss: 0.048244036734104156\n",
      "Epoch 14546/30000 Training Loss: 0.05447924882173538\n",
      "Epoch 14547/30000 Training Loss: 0.04036710411310196\n",
      "Epoch 14548/30000 Training Loss: 0.03883114457130432\n",
      "Epoch 14549/30000 Training Loss: 0.06132083386182785\n",
      "Epoch 14550/30000 Training Loss: 0.052326079457998276\n",
      "Epoch 14551/30000 Training Loss: 0.04465869814157486\n",
      "Epoch 14552/30000 Training Loss: 0.05646401643753052\n",
      "Epoch 14553/30000 Training Loss: 0.0547795444726944\n",
      "Epoch 14554/30000 Training Loss: 0.04814692959189415\n",
      "Epoch 14555/30000 Training Loss: 0.0515441931784153\n",
      "Epoch 14556/30000 Training Loss: 0.05433376505970955\n",
      "Epoch 14557/30000 Training Loss: 0.07169253379106522\n",
      "Epoch 14558/30000 Training Loss: 0.039753783494234085\n",
      "Epoch 14559/30000 Training Loss: 0.058492664247751236\n",
      "Epoch 14560/30000 Training Loss: 0.05008140578866005\n",
      "Epoch 14561/30000 Training Loss: 0.033522509038448334\n",
      "Epoch 14562/30000 Training Loss: 0.03525317832827568\n",
      "Epoch 14563/30000 Training Loss: 0.05651198700070381\n",
      "Epoch 14564/30000 Training Loss: 0.04781247302889824\n",
      "Epoch 14565/30000 Training Loss: 0.03941827639937401\n",
      "Epoch 14566/30000 Training Loss: 0.040483564138412476\n",
      "Epoch 14567/30000 Training Loss: 0.03667566552758217\n",
      "Epoch 14568/30000 Training Loss: 0.043103426694869995\n",
      "Epoch 14569/30000 Training Loss: 0.05000876635313034\n",
      "Epoch 14570/30000 Training Loss: 0.04304031282663345\n",
      "Epoch 14571/30000 Training Loss: 0.03772670030593872\n",
      "Epoch 14572/30000 Training Loss: 0.04491554573178291\n",
      "Epoch 14573/30000 Training Loss: 0.04123751074075699\n",
      "Epoch 14574/30000 Training Loss: 0.04793944209814072\n",
      "Epoch 14575/30000 Training Loss: 0.06052689254283905\n",
      "Epoch 14576/30000 Training Loss: 0.07245535403490067\n",
      "Epoch 14577/30000 Training Loss: 0.06084179878234863\n",
      "Epoch 14578/30000 Training Loss: 0.050307903438806534\n",
      "Epoch 14579/30000 Training Loss: 0.047268982976675034\n",
      "Epoch 14580/30000 Training Loss: 0.062478017061948776\n",
      "Epoch 14581/30000 Training Loss: 0.03427200764417648\n",
      "Epoch 14582/30000 Training Loss: 0.055265460163354874\n",
      "Epoch 14583/30000 Training Loss: 0.04594063758850098\n",
      "Epoch 14584/30000 Training Loss: 0.055210668593645096\n",
      "Epoch 14585/30000 Training Loss: 0.04997147619724274\n",
      "Epoch 14586/30000 Training Loss: 0.042387768626213074\n",
      "Epoch 14587/30000 Training Loss: 0.04712750017642975\n",
      "Epoch 14588/30000 Training Loss: 0.052242401987314224\n",
      "Epoch 14589/30000 Training Loss: 0.05653681606054306\n",
      "Epoch 14590/30000 Training Loss: 0.049043476581573486\n",
      "Epoch 14591/30000 Training Loss: 0.061127543449401855\n",
      "Epoch 14592/30000 Training Loss: 0.052237369120121\n",
      "Epoch 14593/30000 Training Loss: 0.04946185275912285\n",
      "Epoch 14594/30000 Training Loss: 0.04609043151140213\n",
      "Epoch 14595/30000 Training Loss: 0.03236027806997299\n",
      "Epoch 14596/30000 Training Loss: 0.08114922791719437\n",
      "Epoch 14597/30000 Training Loss: 0.056374691426754\n",
      "Epoch 14598/30000 Training Loss: 0.047656819224357605\n",
      "Epoch 14599/30000 Training Loss: 0.06046915799379349\n",
      "Epoch 14600/30000 Training Loss: 0.04017168655991554\n",
      "Epoch 14600/30000 Validation Loss: 0.06904864311218262\n",
      "Epoch 14601/30000 Training Loss: 0.04177609458565712\n",
      "Epoch 14602/30000 Training Loss: 0.04931297153234482\n",
      "Epoch 14603/30000 Training Loss: 0.045388735830783844\n",
      "Epoch 14604/30000 Training Loss: 0.061648912727832794\n",
      "Epoch 14605/30000 Training Loss: 0.0431075394153595\n",
      "Epoch 14606/30000 Training Loss: 0.045362599194049835\n",
      "Epoch 14607/30000 Training Loss: 0.05721845477819443\n",
      "Epoch 14608/30000 Training Loss: 0.04726213961839676\n",
      "Epoch 14609/30000 Training Loss: 0.04760493338108063\n",
      "Epoch 14610/30000 Training Loss: 0.051628097891807556\n",
      "Epoch 14611/30000 Training Loss: 0.07277935743331909\n",
      "Epoch 14612/30000 Training Loss: 0.04688698425889015\n",
      "Epoch 14613/30000 Training Loss: 0.04143510013818741\n",
      "Epoch 14614/30000 Training Loss: 0.04200999066233635\n",
      "Epoch 14615/30000 Training Loss: 0.045328184962272644\n",
      "Epoch 14616/30000 Training Loss: 0.05182011425495148\n",
      "Epoch 14617/30000 Training Loss: 0.06195324659347534\n",
      "Epoch 14618/30000 Training Loss: 0.060682978481054306\n",
      "Epoch 14619/30000 Training Loss: 0.04005799815058708\n",
      "Epoch 14620/30000 Training Loss: 0.04360354691743851\n",
      "Epoch 14621/30000 Training Loss: 0.05391053482890129\n",
      "Epoch 14622/30000 Training Loss: 0.059231482446193695\n",
      "Epoch 14623/30000 Training Loss: 0.06952549517154694\n",
      "Epoch 14624/30000 Training Loss: 0.049586404114961624\n",
      "Epoch 14625/30000 Training Loss: 0.052063487470149994\n",
      "Epoch 14626/30000 Training Loss: 0.03783798962831497\n",
      "Epoch 14627/30000 Training Loss: 0.06442386656999588\n",
      "Epoch 14628/30000 Training Loss: 0.03471669927239418\n",
      "Epoch 14629/30000 Training Loss: 0.058363888412714005\n",
      "Epoch 14630/30000 Training Loss: 0.055926933884620667\n",
      "Epoch 14631/30000 Training Loss: 0.048372041434049606\n",
      "Epoch 14632/30000 Training Loss: 0.061244167387485504\n",
      "Epoch 14633/30000 Training Loss: 0.04535180702805519\n",
      "Epoch 14634/30000 Training Loss: 0.042980264872312546\n",
      "Epoch 14635/30000 Training Loss: 0.05535226687788963\n",
      "Epoch 14636/30000 Training Loss: 0.042125850915908813\n",
      "Epoch 14637/30000 Training Loss: 0.05889666825532913\n",
      "Epoch 14638/30000 Training Loss: 0.0496249720454216\n",
      "Epoch 14639/30000 Training Loss: 0.0420820489525795\n",
      "Epoch 14640/30000 Training Loss: 0.052836351096630096\n",
      "Epoch 14641/30000 Training Loss: 0.04036416858434677\n",
      "Epoch 14642/30000 Training Loss: 0.06060970574617386\n",
      "Epoch 14643/30000 Training Loss: 0.04898460954427719\n",
      "Epoch 14644/30000 Training Loss: 0.0381796695291996\n",
      "Epoch 14645/30000 Training Loss: 0.04647558927536011\n",
      "Epoch 14646/30000 Training Loss: 0.041272327303886414\n",
      "Epoch 14647/30000 Training Loss: 0.03439921513199806\n",
      "Epoch 14648/30000 Training Loss: 0.04122166335582733\n",
      "Epoch 14649/30000 Training Loss: 0.033293310552835464\n",
      "Epoch 14650/30000 Training Loss: 0.042340029031038284\n",
      "Epoch 14651/30000 Training Loss: 0.041597820818424225\n",
      "Epoch 14652/30000 Training Loss: 0.03857768699526787\n",
      "Epoch 14653/30000 Training Loss: 0.05806930363178253\n",
      "Epoch 14654/30000 Training Loss: 0.04301160201430321\n",
      "Epoch 14655/30000 Training Loss: 0.04499104619026184\n",
      "Epoch 14656/30000 Training Loss: 0.03366566821932793\n",
      "Epoch 14657/30000 Training Loss: 0.05162651091814041\n",
      "Epoch 14658/30000 Training Loss: 0.03952198848128319\n",
      "Epoch 14659/30000 Training Loss: 0.039561785757541656\n",
      "Epoch 14660/30000 Training Loss: 0.06093277409672737\n",
      "Epoch 14661/30000 Training Loss: 0.052730657160282135\n",
      "Epoch 14662/30000 Training Loss: 0.03911919519305229\n",
      "Epoch 14663/30000 Training Loss: 0.047034695744514465\n",
      "Epoch 14664/30000 Training Loss: 0.06116378679871559\n",
      "Epoch 14665/30000 Training Loss: 0.04702655225992203\n",
      "Epoch 14666/30000 Training Loss: 0.04419177770614624\n",
      "Epoch 14667/30000 Training Loss: 0.04497604817152023\n",
      "Epoch 14668/30000 Training Loss: 0.05185941979289055\n",
      "Epoch 14669/30000 Training Loss: 0.03770909830927849\n",
      "Epoch 14670/30000 Training Loss: 0.04782303422689438\n",
      "Epoch 14671/30000 Training Loss: 0.04538353905081749\n",
      "Epoch 14672/30000 Training Loss: 0.044301003217697144\n",
      "Epoch 14673/30000 Training Loss: 0.05724123865365982\n",
      "Epoch 14674/30000 Training Loss: 0.04811906814575195\n",
      "Epoch 14675/30000 Training Loss: 0.07537629455327988\n",
      "Epoch 14676/30000 Training Loss: 0.03539566323161125\n",
      "Epoch 14677/30000 Training Loss: 0.04943809658288956\n",
      "Epoch 14678/30000 Training Loss: 0.048276372253894806\n",
      "Epoch 14679/30000 Training Loss: 0.0398225411772728\n",
      "Epoch 14680/30000 Training Loss: 0.04810958355665207\n",
      "Epoch 14681/30000 Training Loss: 0.04198639094829559\n",
      "Epoch 14682/30000 Training Loss: 0.04334315285086632\n",
      "Epoch 14683/30000 Training Loss: 0.04414617642760277\n",
      "Epoch 14684/30000 Training Loss: 0.058709997683763504\n",
      "Epoch 14685/30000 Training Loss: 0.047719601541757584\n",
      "Epoch 14686/30000 Training Loss: 0.05161483585834503\n",
      "Epoch 14687/30000 Training Loss: 0.08319161087274551\n",
      "Epoch 14688/30000 Training Loss: 0.041472047567367554\n",
      "Epoch 14689/30000 Training Loss: 0.03502391651272774\n",
      "Epoch 14690/30000 Training Loss: 0.047755688428878784\n",
      "Epoch 14691/30000 Training Loss: 0.061211422085762024\n",
      "Epoch 14692/30000 Training Loss: 0.0651358887553215\n",
      "Epoch 14693/30000 Training Loss: 0.040198877453804016\n",
      "Epoch 14694/30000 Training Loss: 0.05052178353071213\n",
      "Epoch 14695/30000 Training Loss: 0.03986498340964317\n",
      "Epoch 14696/30000 Training Loss: 0.07458937168121338\n",
      "Epoch 14697/30000 Training Loss: 0.0453612320125103\n",
      "Epoch 14698/30000 Training Loss: 0.04264388978481293\n",
      "Epoch 14699/30000 Training Loss: 0.058421872556209564\n",
      "Epoch 14700/30000 Training Loss: 0.03922837972640991\n",
      "Epoch 14700/30000 Validation Loss: 0.04830052703619003\n",
      "Epoch 14701/30000 Training Loss: 0.05495452880859375\n",
      "Epoch 14702/30000 Training Loss: 0.045469582080841064\n",
      "Epoch 14703/30000 Training Loss: 0.05598783120512962\n",
      "Epoch 14704/30000 Training Loss: 0.04455506056547165\n",
      "Epoch 14705/30000 Training Loss: 0.05522817373275757\n",
      "Epoch 14706/30000 Training Loss: 0.03647742420434952\n",
      "Epoch 14707/30000 Training Loss: 0.06390853971242905\n",
      "Epoch 14708/30000 Training Loss: 0.05956645682454109\n",
      "Epoch 14709/30000 Training Loss: 0.05737500637769699\n",
      "Epoch 14710/30000 Training Loss: 0.0638047382235527\n",
      "Epoch 14711/30000 Training Loss: 0.05339289456605911\n",
      "Epoch 14712/30000 Training Loss: 0.05852891504764557\n",
      "Epoch 14713/30000 Training Loss: 0.05533253774046898\n",
      "Epoch 14714/30000 Training Loss: 0.040940552949905396\n",
      "Epoch 14715/30000 Training Loss: 0.050234001129865646\n",
      "Epoch 14716/30000 Training Loss: 0.04270074889063835\n",
      "Epoch 14717/30000 Training Loss: 0.04187050461769104\n",
      "Epoch 14718/30000 Training Loss: 0.05848032236099243\n",
      "Epoch 14719/30000 Training Loss: 0.06256534159183502\n",
      "Epoch 14720/30000 Training Loss: 0.04124172031879425\n",
      "Epoch 14721/30000 Training Loss: 0.04117143899202347\n",
      "Epoch 14722/30000 Training Loss: 0.04554448649287224\n",
      "Epoch 14723/30000 Training Loss: 0.03983882814645767\n",
      "Epoch 14724/30000 Training Loss: 0.039965711534023285\n",
      "Epoch 14725/30000 Training Loss: 0.039979107677936554\n",
      "Epoch 14726/30000 Training Loss: 0.05270504206418991\n",
      "Epoch 14727/30000 Training Loss: 0.042518675327301025\n",
      "Epoch 14728/30000 Training Loss: 0.05857418105006218\n",
      "Epoch 14729/30000 Training Loss: 0.041250620037317276\n",
      "Epoch 14730/30000 Training Loss: 0.03544313460588455\n",
      "Epoch 14731/30000 Training Loss: 0.03535538166761398\n",
      "Epoch 14732/30000 Training Loss: 0.061717040836811066\n",
      "Epoch 14733/30000 Training Loss: 0.04533448815345764\n",
      "Epoch 14734/30000 Training Loss: 0.0731058344244957\n",
      "Epoch 14735/30000 Training Loss: 0.05956432223320007\n",
      "Epoch 14736/30000 Training Loss: 0.049866706132888794\n",
      "Epoch 14737/30000 Training Loss: 0.03739573433995247\n",
      "Epoch 14738/30000 Training Loss: 0.03653794154524803\n",
      "Epoch 14739/30000 Training Loss: 0.05309182405471802\n",
      "Epoch 14740/30000 Training Loss: 0.06981673091650009\n",
      "Epoch 14741/30000 Training Loss: 0.042271580547094345\n",
      "Epoch 14742/30000 Training Loss: 0.03720337897539139\n",
      "Epoch 14743/30000 Training Loss: 0.059232622385025024\n",
      "Epoch 14744/30000 Training Loss: 0.047007691115140915\n",
      "Epoch 14745/30000 Training Loss: 0.04507116228342056\n",
      "Epoch 14746/30000 Training Loss: 0.06632208824157715\n",
      "Epoch 14747/30000 Training Loss: 0.049825139343738556\n",
      "Epoch 14748/30000 Training Loss: 0.04766980558633804\n",
      "Epoch 14749/30000 Training Loss: 0.0654224306344986\n",
      "Epoch 14750/30000 Training Loss: 0.044484205543994904\n",
      "Epoch 14751/30000 Training Loss: 0.043905213475227356\n",
      "Epoch 14752/30000 Training Loss: 0.040138207376003265\n",
      "Epoch 14753/30000 Training Loss: 0.05045131593942642\n",
      "Epoch 14754/30000 Training Loss: 0.05205005779862404\n",
      "Epoch 14755/30000 Training Loss: 0.04034201428294182\n",
      "Epoch 14756/30000 Training Loss: 0.0498192235827446\n",
      "Epoch 14757/30000 Training Loss: 0.04239453002810478\n",
      "Epoch 14758/30000 Training Loss: 0.05165477097034454\n",
      "Epoch 14759/30000 Training Loss: 0.04542509466409683\n",
      "Epoch 14760/30000 Training Loss: 0.05771687254309654\n",
      "Epoch 14761/30000 Training Loss: 0.04638797789812088\n",
      "Epoch 14762/30000 Training Loss: 0.0445888489484787\n",
      "Epoch 14763/30000 Training Loss: 0.03458378091454506\n",
      "Epoch 14764/30000 Training Loss: 0.05797994136810303\n",
      "Epoch 14765/30000 Training Loss: 0.04603566974401474\n",
      "Epoch 14766/30000 Training Loss: 0.061077795922756195\n",
      "Epoch 14767/30000 Training Loss: 0.04281244054436684\n",
      "Epoch 14768/30000 Training Loss: 0.048109021037817\n",
      "Epoch 14769/30000 Training Loss: 0.048727329820394516\n",
      "Epoch 14770/30000 Training Loss: 0.04264514148235321\n",
      "Epoch 14771/30000 Training Loss: 0.048599619418382645\n",
      "Epoch 14772/30000 Training Loss: 0.06075695902109146\n",
      "Epoch 14773/30000 Training Loss: 0.04661352187395096\n",
      "Epoch 14774/30000 Training Loss: 0.04866717383265495\n",
      "Epoch 14775/30000 Training Loss: 0.04945453256368637\n",
      "Epoch 14776/30000 Training Loss: 0.05708323419094086\n",
      "Epoch 14777/30000 Training Loss: 0.03839447349309921\n",
      "Epoch 14778/30000 Training Loss: 0.03830697014927864\n",
      "Epoch 14779/30000 Training Loss: 0.04169931262731552\n",
      "Epoch 14780/30000 Training Loss: 0.05376205965876579\n",
      "Epoch 14781/30000 Training Loss: 0.053934063762426376\n",
      "Epoch 14782/30000 Training Loss: 0.055672116577625275\n",
      "Epoch 14783/30000 Training Loss: 0.03885741904377937\n",
      "Epoch 14784/30000 Training Loss: 0.06329888850450516\n",
      "Epoch 14785/30000 Training Loss: 0.04164533689618111\n",
      "Epoch 14786/30000 Training Loss: 0.06240084767341614\n",
      "Epoch 14787/30000 Training Loss: 0.059213101863861084\n",
      "Epoch 14788/30000 Training Loss: 0.05895613506436348\n",
      "Epoch 14789/30000 Training Loss: 0.05723170191049576\n",
      "Epoch 14790/30000 Training Loss: 0.04386035352945328\n",
      "Epoch 14791/30000 Training Loss: 0.04301898181438446\n",
      "Epoch 14792/30000 Training Loss: 0.04586860164999962\n",
      "Epoch 14793/30000 Training Loss: 0.0398576520383358\n",
      "Epoch 14794/30000 Training Loss: 0.05426013842225075\n",
      "Epoch 14795/30000 Training Loss: 0.06264197081327438\n",
      "Epoch 14796/30000 Training Loss: 0.061743512749671936\n",
      "Epoch 14797/30000 Training Loss: 0.05452847108244896\n",
      "Epoch 14798/30000 Training Loss: 0.044601209461688995\n",
      "Epoch 14799/30000 Training Loss: 0.0626453384757042\n",
      "Epoch 14800/30000 Training Loss: 0.05428961664438248\n",
      "Epoch 14800/30000 Validation Loss: 0.04729670286178589\n",
      "Epoch 14801/30000 Training Loss: 0.05536803603172302\n",
      "Epoch 14802/30000 Training Loss: 0.05677821859717369\n",
      "Epoch 14803/30000 Training Loss: 0.04477686434984207\n",
      "Epoch 14804/30000 Training Loss: 0.057599153369665146\n",
      "Epoch 14805/30000 Training Loss: 0.06350821256637573\n",
      "Epoch 14806/30000 Training Loss: 0.05597691237926483\n",
      "Epoch 14807/30000 Training Loss: 0.03371451050043106\n",
      "Epoch 14808/30000 Training Loss: 0.04320352151989937\n",
      "Epoch 14809/30000 Training Loss: 0.046648211777210236\n",
      "Epoch 14810/30000 Training Loss: 0.04991716891527176\n",
      "Epoch 14811/30000 Training Loss: 0.043557338416576385\n",
      "Epoch 14812/30000 Training Loss: 0.043976254761219025\n",
      "Epoch 14813/30000 Training Loss: 0.047386422753334045\n",
      "Epoch 14814/30000 Training Loss: 0.06905035674571991\n",
      "Epoch 14815/30000 Training Loss: 0.03698087856173515\n",
      "Epoch 14816/30000 Training Loss: 0.05839584022760391\n",
      "Epoch 14817/30000 Training Loss: 0.045527223497629166\n",
      "Epoch 14818/30000 Training Loss: 0.05539095401763916\n",
      "Epoch 14819/30000 Training Loss: 0.05137432739138603\n",
      "Epoch 14820/30000 Training Loss: 0.05432244762778282\n",
      "Epoch 14821/30000 Training Loss: 0.05265708267688751\n",
      "Epoch 14822/30000 Training Loss: 0.04483349621295929\n",
      "Epoch 14823/30000 Training Loss: 0.06854806840419769\n",
      "Epoch 14824/30000 Training Loss: 0.045507773756980896\n",
      "Epoch 14825/30000 Training Loss: 0.04246513172984123\n",
      "Epoch 14826/30000 Training Loss: 0.058338552713394165\n",
      "Epoch 14827/30000 Training Loss: 0.059821657836437225\n",
      "Epoch 14828/30000 Training Loss: 0.04371076077222824\n",
      "Epoch 14829/30000 Training Loss: 0.056532371789216995\n",
      "Epoch 14830/30000 Training Loss: 0.041343022137880325\n",
      "Epoch 14831/30000 Training Loss: 0.042782824486494064\n",
      "Epoch 14832/30000 Training Loss: 0.05960436165332794\n",
      "Epoch 14833/30000 Training Loss: 0.07550667226314545\n",
      "Epoch 14834/30000 Training Loss: 0.06811986118555069\n",
      "Epoch 14835/30000 Training Loss: 0.038295283913612366\n",
      "Epoch 14836/30000 Training Loss: 0.057806629687547684\n",
      "Epoch 14837/30000 Training Loss: 0.07212099432945251\n",
      "Epoch 14838/30000 Training Loss: 0.04602373018860817\n",
      "Epoch 14839/30000 Training Loss: 0.04842086881399155\n",
      "Epoch 14840/30000 Training Loss: 0.03758728504180908\n",
      "Epoch 14841/30000 Training Loss: 0.05080068111419678\n",
      "Epoch 14842/30000 Training Loss: 0.03406796604394913\n",
      "Epoch 14843/30000 Training Loss: 0.04580990970134735\n",
      "Epoch 14844/30000 Training Loss: 0.04291950911283493\n",
      "Epoch 14845/30000 Training Loss: 0.05631089583039284\n",
      "Epoch 14846/30000 Training Loss: 0.044451482594013214\n",
      "Epoch 14847/30000 Training Loss: 0.05408859997987747\n",
      "Epoch 14848/30000 Training Loss: 0.04610531032085419\n",
      "Epoch 14849/30000 Training Loss: 0.05195995420217514\n",
      "Epoch 14850/30000 Training Loss: 0.04168153926730156\n",
      "Epoch 14851/30000 Training Loss: 0.04724906384944916\n",
      "Epoch 14852/30000 Training Loss: 0.05081092566251755\n",
      "Epoch 14853/30000 Training Loss: 0.046282000839710236\n",
      "Epoch 14854/30000 Training Loss: 0.052456945180892944\n",
      "Epoch 14855/30000 Training Loss: 0.04812832549214363\n",
      "Epoch 14856/30000 Training Loss: 0.05741953104734421\n",
      "Epoch 14857/30000 Training Loss: 0.046408288180828094\n",
      "Epoch 14858/30000 Training Loss: 0.05872765928506851\n",
      "Epoch 14859/30000 Training Loss: 0.04464474320411682\n",
      "Epoch 14860/30000 Training Loss: 0.06139823794364929\n",
      "Epoch 14861/30000 Training Loss: 0.05591471493244171\n",
      "Epoch 14862/30000 Training Loss: 0.042617008090019226\n",
      "Epoch 14863/30000 Training Loss: 0.06796906143426895\n",
      "Epoch 14864/30000 Training Loss: 0.06508370488882065\n",
      "Epoch 14865/30000 Training Loss: 0.05175747349858284\n",
      "Epoch 14866/30000 Training Loss: 0.04903107509016991\n",
      "Epoch 14867/30000 Training Loss: 0.04852982610464096\n",
      "Epoch 14868/30000 Training Loss: 0.048590268939733505\n",
      "Epoch 14869/30000 Training Loss: 0.04040887951850891\n",
      "Epoch 14870/30000 Training Loss: 0.0431768037378788\n",
      "Epoch 14871/30000 Training Loss: 0.04874638840556145\n",
      "Epoch 14872/30000 Training Loss: 0.05450897663831711\n",
      "Epoch 14873/30000 Training Loss: 0.06241738051176071\n",
      "Epoch 14874/30000 Training Loss: 0.0523814931511879\n",
      "Epoch 14875/30000 Training Loss: 0.06490974873304367\n",
      "Epoch 14876/30000 Training Loss: 0.04315970838069916\n",
      "Epoch 14877/30000 Training Loss: 0.052355606108903885\n",
      "Epoch 14878/30000 Training Loss: 0.04408589005470276\n",
      "Epoch 14879/30000 Training Loss: 0.06024343520402908\n",
      "Epoch 14880/30000 Training Loss: 0.047111909836530685\n",
      "Epoch 14881/30000 Training Loss: 0.041212886571884155\n",
      "Epoch 14882/30000 Training Loss: 0.04622398689389229\n",
      "Epoch 14883/30000 Training Loss: 0.039383940398693085\n",
      "Epoch 14884/30000 Training Loss: 0.035843268036842346\n",
      "Epoch 14885/30000 Training Loss: 0.052153900265693665\n",
      "Epoch 14886/30000 Training Loss: 0.04425216466188431\n",
      "Epoch 14887/30000 Training Loss: 0.052457090467214584\n",
      "Epoch 14888/30000 Training Loss: 0.051142215728759766\n",
      "Epoch 14889/30000 Training Loss: 0.05675068497657776\n",
      "Epoch 14890/30000 Training Loss: 0.05512096360325813\n",
      "Epoch 14891/30000 Training Loss: 0.04190761595964432\n",
      "Epoch 14892/30000 Training Loss: 0.05068432539701462\n",
      "Epoch 14893/30000 Training Loss: 0.07000736892223358\n",
      "Epoch 14894/30000 Training Loss: 0.0391322560608387\n",
      "Epoch 14895/30000 Training Loss: 0.04846511781215668\n",
      "Epoch 14896/30000 Training Loss: 0.049970388412475586\n",
      "Epoch 14897/30000 Training Loss: 0.029577694833278656\n",
      "Epoch 14898/30000 Training Loss: 0.05942857638001442\n",
      "Epoch 14899/30000 Training Loss: 0.04023505747318268\n",
      "Epoch 14900/30000 Training Loss: 0.051901835948228836\n",
      "Epoch 14900/30000 Validation Loss: 0.05256464332342148\n",
      "Epoch 14901/30000 Training Loss: 0.053787361830472946\n",
      "Epoch 14902/30000 Training Loss: 0.033805668354034424\n",
      "Epoch 14903/30000 Training Loss: 0.03418099880218506\n",
      "Epoch 14904/30000 Training Loss: 0.04974156618118286\n",
      "Epoch 14905/30000 Training Loss: 0.044638969004154205\n",
      "Epoch 14906/30000 Training Loss: 0.060824424028396606\n",
      "Epoch 14907/30000 Training Loss: 0.05328112468123436\n",
      "Epoch 14908/30000 Training Loss: 0.04459669440984726\n",
      "Epoch 14909/30000 Training Loss: 0.061645716428756714\n",
      "Epoch 14910/30000 Training Loss: 0.0492255762219429\n",
      "Epoch 14911/30000 Training Loss: 0.04846728965640068\n",
      "Epoch 14912/30000 Training Loss: 0.055761635303497314\n",
      "Epoch 14913/30000 Training Loss: 0.04394076019525528\n",
      "Epoch 14914/30000 Training Loss: 0.0394221656024456\n",
      "Epoch 14915/30000 Training Loss: 0.04185256361961365\n",
      "Epoch 14916/30000 Training Loss: 0.04199674725532532\n",
      "Epoch 14917/30000 Training Loss: 0.05017228052020073\n",
      "Epoch 14918/30000 Training Loss: 0.03873938322067261\n",
      "Epoch 14919/30000 Training Loss: 0.04621158912777901\n",
      "Epoch 14920/30000 Training Loss: 0.06462704390287399\n",
      "Epoch 14921/30000 Training Loss: 0.04722178354859352\n",
      "Epoch 14922/30000 Training Loss: 0.04523883014917374\n",
      "Epoch 14923/30000 Training Loss: 0.05697695538401604\n",
      "Epoch 14924/30000 Training Loss: 0.036088161170482635\n",
      "Epoch 14925/30000 Training Loss: 0.038339655846357346\n",
      "Epoch 14926/30000 Training Loss: 0.05259896442294121\n",
      "Epoch 14927/30000 Training Loss: 0.05655336380004883\n",
      "Epoch 14928/30000 Training Loss: 0.05768869072198868\n",
      "Epoch 14929/30000 Training Loss: 0.04273878410458565\n",
      "Epoch 14930/30000 Training Loss: 0.04951952397823334\n",
      "Epoch 14931/30000 Training Loss: 0.041521765291690826\n",
      "Epoch 14932/30000 Training Loss: 0.033245865255594254\n",
      "Epoch 14933/30000 Training Loss: 0.05439680442214012\n",
      "Epoch 14934/30000 Training Loss: 0.047755416482686996\n",
      "Epoch 14935/30000 Training Loss: 0.04492272809147835\n",
      "Epoch 14936/30000 Training Loss: 0.06847924739122391\n",
      "Epoch 14937/30000 Training Loss: 0.04903864488005638\n",
      "Epoch 14938/30000 Training Loss: 0.05746140703558922\n",
      "Epoch 14939/30000 Training Loss: 0.056123413145542145\n",
      "Epoch 14940/30000 Training Loss: 0.05183621495962143\n",
      "Epoch 14941/30000 Training Loss: 0.055740367621183395\n",
      "Epoch 14942/30000 Training Loss: 0.0485353097319603\n",
      "Epoch 14943/30000 Training Loss: 0.05347385257482529\n",
      "Epoch 14944/30000 Training Loss: 0.05471307411789894\n",
      "Epoch 14945/30000 Training Loss: 0.049339860677719116\n",
      "Epoch 14946/30000 Training Loss: 0.03844732046127319\n",
      "Epoch 14947/30000 Training Loss: 0.03585934266448021\n",
      "Epoch 14948/30000 Training Loss: 0.04165984317660332\n",
      "Epoch 14949/30000 Training Loss: 0.054065629839897156\n",
      "Epoch 14950/30000 Training Loss: 0.043790072202682495\n",
      "Epoch 14951/30000 Training Loss: 0.06051483750343323\n",
      "Epoch 14952/30000 Training Loss: 0.046146225184202194\n",
      "Epoch 14953/30000 Training Loss: 0.049211759120225906\n",
      "Epoch 14954/30000 Training Loss: 0.06676111370325089\n",
      "Epoch 14955/30000 Training Loss: 0.04847320541739464\n",
      "Epoch 14956/30000 Training Loss: 0.051103625446558\n",
      "Epoch 14957/30000 Training Loss: 0.056302644312381744\n",
      "Epoch 14958/30000 Training Loss: 0.04631679877638817\n",
      "Epoch 14959/30000 Training Loss: 0.048147693276405334\n",
      "Epoch 14960/30000 Training Loss: 0.05111377313733101\n",
      "Epoch 14961/30000 Training Loss: 0.04309958964586258\n",
      "Epoch 14962/30000 Training Loss: 0.0642235204577446\n",
      "Epoch 14963/30000 Training Loss: 0.053496092557907104\n",
      "Epoch 14964/30000 Training Loss: 0.032876331359148026\n",
      "Epoch 14965/30000 Training Loss: 0.04629363492131233\n",
      "Epoch 14966/30000 Training Loss: 0.04046321660280228\n",
      "Epoch 14967/30000 Training Loss: 0.06512957066297531\n",
      "Epoch 14968/30000 Training Loss: 0.05305247753858566\n",
      "Epoch 14969/30000 Training Loss: 0.04580119252204895\n",
      "Epoch 14970/30000 Training Loss: 0.045060135424137115\n",
      "Epoch 14971/30000 Training Loss: 0.05256481096148491\n",
      "Epoch 14972/30000 Training Loss: 0.04285217076539993\n",
      "Epoch 14973/30000 Training Loss: 0.04068903997540474\n",
      "Epoch 14974/30000 Training Loss: 0.057209789752960205\n",
      "Epoch 14975/30000 Training Loss: 0.03690124303102493\n",
      "Epoch 14976/30000 Training Loss: 0.047530677169561386\n",
      "Epoch 14977/30000 Training Loss: 0.05708566680550575\n",
      "Epoch 14978/30000 Training Loss: 0.0489865317940712\n",
      "Epoch 14979/30000 Training Loss: 0.045216694474220276\n",
      "Epoch 14980/30000 Training Loss: 0.06497849524021149\n",
      "Epoch 14981/30000 Training Loss: 0.05390624329447746\n",
      "Epoch 14982/30000 Training Loss: 0.0533938892185688\n",
      "Epoch 14983/30000 Training Loss: 0.05496479198336601\n",
      "Epoch 14984/30000 Training Loss: 0.04643134027719498\n",
      "Epoch 14985/30000 Training Loss: 0.05737247318029404\n",
      "Epoch 14986/30000 Training Loss: 0.03998744115233421\n",
      "Epoch 14987/30000 Training Loss: 0.039929211139678955\n",
      "Epoch 14988/30000 Training Loss: 0.06443678587675095\n",
      "Epoch 14989/30000 Training Loss: 0.04867221787571907\n",
      "Epoch 14990/30000 Training Loss: 0.03697264939546585\n",
      "Epoch 14991/30000 Training Loss: 0.04524049535393715\n",
      "Epoch 14992/30000 Training Loss: 0.04470427334308624\n",
      "Epoch 14993/30000 Training Loss: 0.049122486263513565\n",
      "Epoch 14994/30000 Training Loss: 0.05636637657880783\n",
      "Epoch 14995/30000 Training Loss: 0.05625209957361221\n",
      "Epoch 14996/30000 Training Loss: 0.03778931498527527\n",
      "Epoch 14997/30000 Training Loss: 0.04399499669671059\n",
      "Epoch 14998/30000 Training Loss: 0.0450698621571064\n",
      "Epoch 14999/30000 Training Loss: 0.05954158306121826\n",
      "Epoch 15000/30000 Training Loss: 0.044793181121349335\n",
      "Epoch 15000/30000 Validation Loss: 0.04428976774215698\n",
      "Epoch 15001/30000 Training Loss: 0.06067349761724472\n",
      "Epoch 15002/30000 Training Loss: 0.045487672090530396\n",
      "Epoch 15003/30000 Training Loss: 0.03977407142519951\n",
      "Epoch 15004/30000 Training Loss: 0.05828304588794708\n",
      "Epoch 15005/30000 Training Loss: 0.05803056061267853\n",
      "Epoch 15006/30000 Training Loss: 0.05795447155833244\n",
      "Epoch 15007/30000 Training Loss: 0.052461616694927216\n",
      "Epoch 15008/30000 Training Loss: 0.040459491312503815\n",
      "Epoch 15009/30000 Training Loss: 0.053274672478437424\n",
      "Epoch 15010/30000 Training Loss: 0.04928445443511009\n",
      "Epoch 15011/30000 Training Loss: 0.04738491028547287\n",
      "Epoch 15012/30000 Training Loss: 0.037275951355695724\n",
      "Epoch 15013/30000 Training Loss: 0.04170214384794235\n",
      "Epoch 15014/30000 Training Loss: 0.05115611106157303\n",
      "Epoch 15015/30000 Training Loss: 0.053814128041267395\n",
      "Epoch 15016/30000 Training Loss: 0.04369103163480759\n",
      "Epoch 15017/30000 Training Loss: 0.06189374625682831\n",
      "Epoch 15018/30000 Training Loss: 0.055954985320568085\n",
      "Epoch 15019/30000 Training Loss: 0.058424171060323715\n",
      "Epoch 15020/30000 Training Loss: 0.04717166721820831\n",
      "Epoch 15021/30000 Training Loss: 0.043150316923856735\n",
      "Epoch 15022/30000 Training Loss: 0.04848602041602135\n",
      "Epoch 15023/30000 Training Loss: 0.053205303847789764\n",
      "Epoch 15024/30000 Training Loss: 0.04936971887946129\n",
      "Epoch 15025/30000 Training Loss: 0.04126206040382385\n",
      "Epoch 15026/30000 Training Loss: 0.07638318836688995\n",
      "Epoch 15027/30000 Training Loss: 0.04407824948430061\n",
      "Epoch 15028/30000 Training Loss: 0.03604574501514435\n",
      "Epoch 15029/30000 Training Loss: 0.04276636242866516\n",
      "Epoch 15030/30000 Training Loss: 0.06062464043498039\n",
      "Epoch 15031/30000 Training Loss: 0.04148433730006218\n",
      "Epoch 15032/30000 Training Loss: 0.04695315659046173\n",
      "Epoch 15033/30000 Training Loss: 0.040233295410871506\n",
      "Epoch 15034/30000 Training Loss: 0.04322582855820656\n",
      "Epoch 15035/30000 Training Loss: 0.04975300282239914\n",
      "Epoch 15036/30000 Training Loss: 0.040738750249147415\n",
      "Epoch 15037/30000 Training Loss: 0.046779830008745193\n",
      "Epoch 15038/30000 Training Loss: 0.04274472966790199\n",
      "Epoch 15039/30000 Training Loss: 0.06680037081241608\n",
      "Epoch 15040/30000 Training Loss: 0.04346564784646034\n",
      "Epoch 15041/30000 Training Loss: 0.05795671045780182\n",
      "Epoch 15042/30000 Training Loss: 0.04904481768608093\n",
      "Epoch 15043/30000 Training Loss: 0.05507857725024223\n",
      "Epoch 15044/30000 Training Loss: 0.06054825335741043\n",
      "Epoch 15045/30000 Training Loss: 0.05732071027159691\n",
      "Epoch 15046/30000 Training Loss: 0.0353885293006897\n",
      "Epoch 15047/30000 Training Loss: 0.04321613907814026\n",
      "Epoch 15048/30000 Training Loss: 0.04609013348817825\n",
      "Epoch 15049/30000 Training Loss: 0.04907873645424843\n",
      "Epoch 15050/30000 Training Loss: 0.04217773675918579\n",
      "Epoch 15051/30000 Training Loss: 0.043882519006729126\n",
      "Epoch 15052/30000 Training Loss: 0.050221994519233704\n",
      "Epoch 15053/30000 Training Loss: 0.051388222724199295\n",
      "Epoch 15054/30000 Training Loss: 0.0487690344452858\n",
      "Epoch 15055/30000 Training Loss: 0.07410335540771484\n",
      "Epoch 15056/30000 Training Loss: 0.05981363356113434\n",
      "Epoch 15057/30000 Training Loss: 0.03651200979948044\n",
      "Epoch 15058/30000 Training Loss: 0.050185270607471466\n",
      "Epoch 15059/30000 Training Loss: 0.05384712666273117\n",
      "Epoch 15060/30000 Training Loss: 0.06425492465496063\n",
      "Epoch 15061/30000 Training Loss: 0.05060458183288574\n",
      "Epoch 15062/30000 Training Loss: 0.044165562838315964\n",
      "Epoch 15063/30000 Training Loss: 0.04538644850254059\n",
      "Epoch 15064/30000 Training Loss: 0.05868571996688843\n",
      "Epoch 15065/30000 Training Loss: 0.04556059092283249\n",
      "Epoch 15066/30000 Training Loss: 0.04483061283826828\n",
      "Epoch 15067/30000 Training Loss: 0.044461093842983246\n",
      "Epoch 15068/30000 Training Loss: 0.049649275839328766\n",
      "Epoch 15069/30000 Training Loss: 0.0700899064540863\n",
      "Epoch 15070/30000 Training Loss: 0.03911571949720383\n",
      "Epoch 15071/30000 Training Loss: 0.04426683485507965\n",
      "Epoch 15072/30000 Training Loss: 0.0471164770424366\n",
      "Epoch 15073/30000 Training Loss: 0.0669277161359787\n",
      "Epoch 15074/30000 Training Loss: 0.05472094565629959\n",
      "Epoch 15075/30000 Training Loss: 0.053396765142679214\n",
      "Epoch 15076/30000 Training Loss: 0.04124310612678528\n",
      "Epoch 15077/30000 Training Loss: 0.05500010401010513\n",
      "Epoch 15078/30000 Training Loss: 0.06294582784175873\n",
      "Epoch 15079/30000 Training Loss: 0.054514527320861816\n",
      "Epoch 15080/30000 Training Loss: 0.04220427945256233\n",
      "Epoch 15081/30000 Training Loss: 0.044604070484638214\n",
      "Epoch 15082/30000 Training Loss: 0.05533389002084732\n",
      "Epoch 15083/30000 Training Loss: 0.03785965219140053\n",
      "Epoch 15084/30000 Training Loss: 0.04650909826159477\n",
      "Epoch 15085/30000 Training Loss: 0.04533546790480614\n",
      "Epoch 15086/30000 Training Loss: 0.0397772453725338\n",
      "Epoch 15087/30000 Training Loss: 0.067377470433712\n",
      "Epoch 15088/30000 Training Loss: 0.03485393151640892\n",
      "Epoch 15089/30000 Training Loss: 0.043609313666820526\n",
      "Epoch 15090/30000 Training Loss: 0.04942038282752037\n",
      "Epoch 15091/30000 Training Loss: 0.06693003326654434\n",
      "Epoch 15092/30000 Training Loss: 0.03381979465484619\n",
      "Epoch 15093/30000 Training Loss: 0.03757694363594055\n",
      "Epoch 15094/30000 Training Loss: 0.05627674236893654\n",
      "Epoch 15095/30000 Training Loss: 0.046576958149671555\n",
      "Epoch 15096/30000 Training Loss: 0.05793794244527817\n",
      "Epoch 15097/30000 Training Loss: 0.04472465068101883\n",
      "Epoch 15098/30000 Training Loss: 0.057961054146289825\n",
      "Epoch 15099/30000 Training Loss: 0.03427508473396301\n",
      "Epoch 15100/30000 Training Loss: 0.036450471729040146\n",
      "Epoch 15100/30000 Validation Loss: 0.06902416795492172\n",
      "Epoch 15101/30000 Training Loss: 0.04334374517202377\n",
      "Epoch 15102/30000 Training Loss: 0.04688136652112007\n",
      "Epoch 15103/30000 Training Loss: 0.062105800956487656\n",
      "Epoch 15104/30000 Training Loss: 0.05048004537820816\n",
      "Epoch 15105/30000 Training Loss: 0.046681907027959824\n",
      "Epoch 15106/30000 Training Loss: 0.05249376595020294\n",
      "Epoch 15107/30000 Training Loss: 0.052437905222177505\n",
      "Epoch 15108/30000 Training Loss: 0.048816535621881485\n",
      "Epoch 15109/30000 Training Loss: 0.07598914206027985\n",
      "Epoch 15110/30000 Training Loss: 0.0409434549510479\n",
      "Epoch 15111/30000 Training Loss: 0.04646732658147812\n",
      "Epoch 15112/30000 Training Loss: 0.03624807670712471\n",
      "Epoch 15113/30000 Training Loss: 0.05885421112179756\n",
      "Epoch 15114/30000 Training Loss: 0.05234549567103386\n",
      "Epoch 15115/30000 Training Loss: 0.055898718535900116\n",
      "Epoch 15116/30000 Training Loss: 0.046399202197790146\n",
      "Epoch 15117/30000 Training Loss: 0.06564003974199295\n",
      "Epoch 15118/30000 Training Loss: 0.046682532876729965\n",
      "Epoch 15119/30000 Training Loss: 0.03379928320646286\n",
      "Epoch 15120/30000 Training Loss: 0.0462726466357708\n",
      "Epoch 15121/30000 Training Loss: 0.051672667264938354\n",
      "Epoch 15122/30000 Training Loss: 0.051385339349508286\n",
      "Epoch 15123/30000 Training Loss: 0.05053844302892685\n",
      "Epoch 15124/30000 Training Loss: 0.05791126936674118\n",
      "Epoch 15125/30000 Training Loss: 0.05955708771944046\n",
      "Epoch 15126/30000 Training Loss: 0.06733250617980957\n",
      "Epoch 15127/30000 Training Loss: 0.031695444136857986\n",
      "Epoch 15128/30000 Training Loss: 0.037884995341300964\n",
      "Epoch 15129/30000 Training Loss: 0.05370643734931946\n",
      "Epoch 15130/30000 Training Loss: 0.04785182699561119\n",
      "Epoch 15131/30000 Training Loss: 0.043359704315662384\n",
      "Epoch 15132/30000 Training Loss: 0.04943031817674637\n",
      "Epoch 15133/30000 Training Loss: 0.054085277020931244\n",
      "Epoch 15134/30000 Training Loss: 0.04252740368247032\n",
      "Epoch 15135/30000 Training Loss: 0.045357320457696915\n",
      "Epoch 15136/30000 Training Loss: 0.04949963837862015\n",
      "Epoch 15137/30000 Training Loss: 0.043361932039260864\n",
      "Epoch 15138/30000 Training Loss: 0.06386985629796982\n",
      "Epoch 15139/30000 Training Loss: 0.04089151322841644\n",
      "Epoch 15140/30000 Training Loss: 0.0420294851064682\n",
      "Epoch 15141/30000 Training Loss: 0.04615730047225952\n",
      "Epoch 15142/30000 Training Loss: 0.04767050966620445\n",
      "Epoch 15143/30000 Training Loss: 0.0522104948759079\n",
      "Epoch 15144/30000 Training Loss: 0.04053068906068802\n",
      "Epoch 15145/30000 Training Loss: 0.05084673687815666\n",
      "Epoch 15146/30000 Training Loss: 0.04005344584584236\n",
      "Epoch 15147/30000 Training Loss: 0.05110220983624458\n",
      "Epoch 15148/30000 Training Loss: 0.048443861305713654\n",
      "Epoch 15149/30000 Training Loss: 0.052072882652282715\n",
      "Epoch 15150/30000 Training Loss: 0.05599234998226166\n",
      "Epoch 15151/30000 Training Loss: 0.051016367971897125\n",
      "Epoch 15152/30000 Training Loss: 0.03366965055465698\n",
      "Epoch 15153/30000 Training Loss: 0.034181028604507446\n",
      "Epoch 15154/30000 Training Loss: 0.048666298389434814\n",
      "Epoch 15155/30000 Training Loss: 0.04481226205825806\n",
      "Epoch 15156/30000 Training Loss: 0.05914965271949768\n",
      "Epoch 15157/30000 Training Loss: 0.041266169399023056\n",
      "Epoch 15158/30000 Training Loss: 0.05714324116706848\n",
      "Epoch 15159/30000 Training Loss: 0.04076198488473892\n",
      "Epoch 15160/30000 Training Loss: 0.05721215531229973\n",
      "Epoch 15161/30000 Training Loss: 0.0590999573469162\n",
      "Epoch 15162/30000 Training Loss: 0.049530912190675735\n",
      "Epoch 15163/30000 Training Loss: 0.04835491254925728\n",
      "Epoch 15164/30000 Training Loss: 0.040094777941703796\n",
      "Epoch 15165/30000 Training Loss: 0.05265747010707855\n",
      "Epoch 15166/30000 Training Loss: 0.06197221577167511\n",
      "Epoch 15167/30000 Training Loss: 0.038133878260850906\n",
      "Epoch 15168/30000 Training Loss: 0.05344986915588379\n",
      "Epoch 15169/30000 Training Loss: 0.058723561465740204\n",
      "Epoch 15170/30000 Training Loss: 0.05848214030265808\n",
      "Epoch 15171/30000 Training Loss: 0.04606112465262413\n",
      "Epoch 15172/30000 Training Loss: 0.03591500222682953\n",
      "Epoch 15173/30000 Training Loss: 0.052276771515607834\n",
      "Epoch 15174/30000 Training Loss: 0.05671679228544235\n",
      "Epoch 15175/30000 Training Loss: 0.04853382706642151\n",
      "Epoch 15176/30000 Training Loss: 0.04895053803920746\n",
      "Epoch 15177/30000 Training Loss: 0.037703126668930054\n",
      "Epoch 15178/30000 Training Loss: 0.04724079743027687\n",
      "Epoch 15179/30000 Training Loss: 0.04598034918308258\n",
      "Epoch 15180/30000 Training Loss: 0.03652121126651764\n",
      "Epoch 15181/30000 Training Loss: 0.055832669138908386\n",
      "Epoch 15182/30000 Training Loss: 0.05042114853858948\n",
      "Epoch 15183/30000 Training Loss: 0.05331262946128845\n",
      "Epoch 15184/30000 Training Loss: 0.05446459352970123\n",
      "Epoch 15185/30000 Training Loss: 0.050712909549474716\n",
      "Epoch 15186/30000 Training Loss: 0.04713576287031174\n",
      "Epoch 15187/30000 Training Loss: 0.04857503995299339\n",
      "Epoch 15188/30000 Training Loss: 0.04099440574645996\n",
      "Epoch 15189/30000 Training Loss: 0.05961764231324196\n",
      "Epoch 15190/30000 Training Loss: 0.054178863763809204\n",
      "Epoch 15191/30000 Training Loss: 0.06818459928035736\n",
      "Epoch 15192/30000 Training Loss: 0.04197956249117851\n",
      "Epoch 15193/30000 Training Loss: 0.04511863365769386\n",
      "Epoch 15194/30000 Training Loss: 0.06101348251104355\n",
      "Epoch 15195/30000 Training Loss: 0.06272868812084198\n",
      "Epoch 15196/30000 Training Loss: 0.05956419184803963\n",
      "Epoch 15197/30000 Training Loss: 0.06213343143463135\n",
      "Epoch 15198/30000 Training Loss: 0.04408283531665802\n",
      "Epoch 15199/30000 Training Loss: 0.05223800241947174\n",
      "Epoch 15200/30000 Training Loss: 0.040317267179489136\n",
      "Epoch 15200/30000 Validation Loss: 0.059544116258621216\n",
      "Epoch 15201/30000 Training Loss: 0.04708068072795868\n",
      "Epoch 15202/30000 Training Loss: 0.0397874191403389\n",
      "Epoch 15203/30000 Training Loss: 0.0492684543132782\n",
      "Epoch 15204/30000 Training Loss: 0.04471482336521149\n",
      "Epoch 15205/30000 Training Loss: 0.054530590772628784\n",
      "Epoch 15206/30000 Training Loss: 0.03793901577591896\n",
      "Epoch 15207/30000 Training Loss: 0.05591798573732376\n",
      "Epoch 15208/30000 Training Loss: 0.054223984479904175\n",
      "Epoch 15209/30000 Training Loss: 0.04542393237352371\n",
      "Epoch 15210/30000 Training Loss: 0.04784982278943062\n",
      "Epoch 15211/30000 Training Loss: 0.051467809826135635\n",
      "Epoch 15212/30000 Training Loss: 0.044796064496040344\n",
      "Epoch 15213/30000 Training Loss: 0.058142874389886856\n",
      "Epoch 15214/30000 Training Loss: 0.060291241854429245\n",
      "Epoch 15215/30000 Training Loss: 0.05903793126344681\n",
      "Epoch 15216/30000 Training Loss: 0.04264093190431595\n",
      "Epoch 15217/30000 Training Loss: 0.07582850009202957\n",
      "Epoch 15218/30000 Training Loss: 0.0440395213663578\n",
      "Epoch 15219/30000 Training Loss: 0.0576816201210022\n",
      "Epoch 15220/30000 Training Loss: 0.04676247388124466\n",
      "Epoch 15221/30000 Training Loss: 0.04658893123269081\n",
      "Epoch 15222/30000 Training Loss: 0.048798657953739166\n",
      "Epoch 15223/30000 Training Loss: 0.03724153712391853\n",
      "Epoch 15224/30000 Training Loss: 0.04465632513165474\n",
      "Epoch 15225/30000 Training Loss: 0.042632296681404114\n",
      "Epoch 15226/30000 Training Loss: 0.03786522522568703\n",
      "Epoch 15227/30000 Training Loss: 0.04469900578260422\n",
      "Epoch 15228/30000 Training Loss: 0.05166758969426155\n",
      "Epoch 15229/30000 Training Loss: 0.03920824080705643\n",
      "Epoch 15230/30000 Training Loss: 0.049968186765909195\n",
      "Epoch 15231/30000 Training Loss: 0.042925577610731125\n",
      "Epoch 15232/30000 Training Loss: 0.04997871443629265\n",
      "Epoch 15233/30000 Training Loss: 0.04914788901805878\n",
      "Epoch 15234/30000 Training Loss: 0.04904156178236008\n",
      "Epoch 15235/30000 Training Loss: 0.04071228578686714\n",
      "Epoch 15236/30000 Training Loss: 0.047980282455682755\n",
      "Epoch 15237/30000 Training Loss: 0.03203001618385315\n",
      "Epoch 15238/30000 Training Loss: 0.059039220213890076\n",
      "Epoch 15239/30000 Training Loss: 0.048624616116285324\n",
      "Epoch 15240/30000 Training Loss: 0.058367904275655746\n",
      "Epoch 15241/30000 Training Loss: 0.039319831877946854\n",
      "Epoch 15242/30000 Training Loss: 0.04733969271183014\n",
      "Epoch 15243/30000 Training Loss: 0.04939410090446472\n",
      "Epoch 15244/30000 Training Loss: 0.05265389382839203\n",
      "Epoch 15245/30000 Training Loss: 0.05820494890213013\n",
      "Epoch 15246/30000 Training Loss: 0.041540421545505524\n",
      "Epoch 15247/30000 Training Loss: 0.047398634254932404\n",
      "Epoch 15248/30000 Training Loss: 0.040365345776081085\n",
      "Epoch 15249/30000 Training Loss: 0.057465121150016785\n",
      "Epoch 15250/30000 Training Loss: 0.0673857256770134\n",
      "Epoch 15251/30000 Training Loss: 0.03806386888027191\n",
      "Epoch 15252/30000 Training Loss: 0.05121076852083206\n",
      "Epoch 15253/30000 Training Loss: 0.04532903432846069\n",
      "Epoch 15254/30000 Training Loss: 0.057042207568883896\n",
      "Epoch 15255/30000 Training Loss: 0.05453302711248398\n",
      "Epoch 15256/30000 Training Loss: 0.048270806670188904\n",
      "Epoch 15257/30000 Training Loss: 0.04982816427946091\n",
      "Epoch 15258/30000 Training Loss: 0.06122793257236481\n",
      "Epoch 15259/30000 Training Loss: 0.037475116550922394\n",
      "Epoch 15260/30000 Training Loss: 0.04323038086295128\n",
      "Epoch 15261/30000 Training Loss: 0.031344786286354065\n",
      "Epoch 15262/30000 Training Loss: 0.05473371595144272\n",
      "Epoch 15263/30000 Training Loss: 0.04063594713807106\n",
      "Epoch 15264/30000 Training Loss: 0.05081557109951973\n",
      "Epoch 15265/30000 Training Loss: 0.041378360241651535\n",
      "Epoch 15266/30000 Training Loss: 0.055644720792770386\n",
      "Epoch 15267/30000 Training Loss: 0.06889579445123672\n",
      "Epoch 15268/30000 Training Loss: 0.06478753685951233\n",
      "Epoch 15269/30000 Training Loss: 0.03863310068845749\n",
      "Epoch 15270/30000 Training Loss: 0.04719036445021629\n",
      "Epoch 15271/30000 Training Loss: 0.05273815244436264\n",
      "Epoch 15272/30000 Training Loss: 0.04381221532821655\n",
      "Epoch 15273/30000 Training Loss: 0.0591832771897316\n",
      "Epoch 15274/30000 Training Loss: 0.047144897282123566\n",
      "Epoch 15275/30000 Training Loss: 0.045875802636146545\n",
      "Epoch 15276/30000 Training Loss: 0.03675379604101181\n",
      "Epoch 15277/30000 Training Loss: 0.05077742412686348\n",
      "Epoch 15278/30000 Training Loss: 0.049347877502441406\n",
      "Epoch 15279/30000 Training Loss: 0.04626339301466942\n",
      "Epoch 15280/30000 Training Loss: 0.0496053509414196\n",
      "Epoch 15281/30000 Training Loss: 0.06045942008495331\n",
      "Epoch 15282/30000 Training Loss: 0.057793259620666504\n",
      "Epoch 15283/30000 Training Loss: 0.06844437122344971\n",
      "Epoch 15284/30000 Training Loss: 0.049084898084402084\n",
      "Epoch 15285/30000 Training Loss: 0.061827994883060455\n",
      "Epoch 15286/30000 Training Loss: 0.04464365914463997\n",
      "Epoch 15287/30000 Training Loss: 0.06839291006326675\n",
      "Epoch 15288/30000 Training Loss: 0.05156164988875389\n",
      "Epoch 15289/30000 Training Loss: 0.04851049184799194\n",
      "Epoch 15290/30000 Training Loss: 0.05394874885678291\n",
      "Epoch 15291/30000 Training Loss: 0.04836789891123772\n",
      "Epoch 15292/30000 Training Loss: 0.05667968839406967\n",
      "Epoch 15293/30000 Training Loss: 0.03998740017414093\n",
      "Epoch 15294/30000 Training Loss: 0.046720150858163834\n",
      "Epoch 15295/30000 Training Loss: 0.04528933763504028\n",
      "Epoch 15296/30000 Training Loss: 0.0557112842798233\n",
      "Epoch 15297/30000 Training Loss: 0.06514311581850052\n",
      "Epoch 15298/30000 Training Loss: 0.05030020698904991\n",
      "Epoch 15299/30000 Training Loss: 0.040944457054138184\n",
      "Epoch 15300/30000 Training Loss: 0.04138192906975746\n",
      "Epoch 15300/30000 Validation Loss: 0.05736222490668297\n",
      "Epoch 15301/30000 Training Loss: 0.049326907843351364\n",
      "Epoch 15302/30000 Training Loss: 0.048206962645053864\n",
      "Epoch 15303/30000 Training Loss: 0.056988202035427094\n",
      "Epoch 15304/30000 Training Loss: 0.04281117394566536\n",
      "Epoch 15305/30000 Training Loss: 0.04957917705178261\n",
      "Epoch 15306/30000 Training Loss: 0.04971148818731308\n",
      "Epoch 15307/30000 Training Loss: 0.06412448734045029\n",
      "Epoch 15308/30000 Training Loss: 0.05137580260634422\n",
      "Epoch 15309/30000 Training Loss: 0.05514434352517128\n",
      "Epoch 15310/30000 Training Loss: 0.05178900808095932\n",
      "Epoch 15311/30000 Training Loss: 0.04438283294439316\n",
      "Epoch 15312/30000 Training Loss: 0.05268295109272003\n",
      "Epoch 15313/30000 Training Loss: 0.057142890989780426\n",
      "Epoch 15314/30000 Training Loss: 0.05703799054026604\n",
      "Epoch 15315/30000 Training Loss: 0.05508490279316902\n",
      "Epoch 15316/30000 Training Loss: 0.07751485705375671\n",
      "Epoch 15317/30000 Training Loss: 0.04895426332950592\n",
      "Epoch 15318/30000 Training Loss: 0.04527352750301361\n",
      "Epoch 15319/30000 Training Loss: 0.0508875697851181\n",
      "Epoch 15320/30000 Training Loss: 0.04797307401895523\n",
      "Epoch 15321/30000 Training Loss: 0.04562001675367355\n",
      "Epoch 15322/30000 Training Loss: 0.04369477182626724\n",
      "Epoch 15323/30000 Training Loss: 0.05283914506435394\n",
      "Epoch 15324/30000 Training Loss: 0.04440736770629883\n",
      "Epoch 15325/30000 Training Loss: 0.054315004497766495\n",
      "Epoch 15326/30000 Training Loss: 0.03325258195400238\n",
      "Epoch 15327/30000 Training Loss: 0.038825131952762604\n",
      "Epoch 15328/30000 Training Loss: 0.03816189616918564\n",
      "Epoch 15329/30000 Training Loss: 0.048375505954027176\n",
      "Epoch 15330/30000 Training Loss: 0.03875807672739029\n",
      "Epoch 15331/30000 Training Loss: 0.04314877837896347\n",
      "Epoch 15332/30000 Training Loss: 0.04020346328616142\n",
      "Epoch 15333/30000 Training Loss: 0.044977154582738876\n",
      "Epoch 15334/30000 Training Loss: 0.06442131847143173\n",
      "Epoch 15335/30000 Training Loss: 0.04573933035135269\n",
      "Epoch 15336/30000 Training Loss: 0.058156948536634445\n",
      "Epoch 15337/30000 Training Loss: 0.06444968283176422\n",
      "Epoch 15338/30000 Training Loss: 0.044965192675590515\n",
      "Epoch 15339/30000 Training Loss: 0.0553525909781456\n",
      "Epoch 15340/30000 Training Loss: 0.03158586472272873\n",
      "Epoch 15341/30000 Training Loss: 0.04662150517106056\n",
      "Epoch 15342/30000 Training Loss: 0.049370717257261276\n",
      "Epoch 15343/30000 Training Loss: 0.05448836088180542\n",
      "Epoch 15344/30000 Training Loss: 0.03407619521021843\n",
      "Epoch 15345/30000 Training Loss: 0.053706251084804535\n",
      "Epoch 15346/30000 Training Loss: 0.038967207074165344\n",
      "Epoch 15347/30000 Training Loss: 0.04776201397180557\n",
      "Epoch 15348/30000 Training Loss: 0.0568256601691246\n",
      "Epoch 15349/30000 Training Loss: 0.04348856210708618\n",
      "Epoch 15350/30000 Training Loss: 0.05068453401327133\n",
      "Epoch 15351/30000 Training Loss: 0.062170885503292084\n",
      "Epoch 15352/30000 Training Loss: 0.04597245529294014\n",
      "Epoch 15353/30000 Training Loss: 0.05821067839860916\n",
      "Epoch 15354/30000 Training Loss: 0.05950123816728592\n",
      "Epoch 15355/30000 Training Loss: 0.040199413895606995\n",
      "Epoch 15356/30000 Training Loss: 0.04397805035114288\n",
      "Epoch 15357/30000 Training Loss: 0.05282751843333244\n",
      "Epoch 15358/30000 Training Loss: 0.03915117681026459\n",
      "Epoch 15359/30000 Training Loss: 0.04427419230341911\n",
      "Epoch 15360/30000 Training Loss: 0.05318744480609894\n",
      "Epoch 15361/30000 Training Loss: 0.04237048700451851\n",
      "Epoch 15362/30000 Training Loss: 0.03416946157813072\n",
      "Epoch 15363/30000 Training Loss: 0.05403820425271988\n",
      "Epoch 15364/30000 Training Loss: 0.06883694231510162\n",
      "Epoch 15365/30000 Training Loss: 0.04531140625476837\n",
      "Epoch 15366/30000 Training Loss: 0.049686964601278305\n",
      "Epoch 15367/30000 Training Loss: 0.04388648271560669\n",
      "Epoch 15368/30000 Training Loss: 0.05130309611558914\n",
      "Epoch 15369/30000 Training Loss: 0.03906981274485588\n",
      "Epoch 15370/30000 Training Loss: 0.041362643241882324\n",
      "Epoch 15371/30000 Training Loss: 0.06013476476073265\n",
      "Epoch 15372/30000 Training Loss: 0.05293509364128113\n",
      "Epoch 15373/30000 Training Loss: 0.051633503288030624\n",
      "Epoch 15374/30000 Training Loss: 0.0464833602309227\n",
      "Epoch 15375/30000 Training Loss: 0.06339630484580994\n",
      "Epoch 15376/30000 Training Loss: 0.05205896869301796\n",
      "Epoch 15377/30000 Training Loss: 0.04284987598657608\n",
      "Epoch 15378/30000 Training Loss: 0.05893333628773689\n",
      "Epoch 15379/30000 Training Loss: 0.042920731008052826\n",
      "Epoch 15380/30000 Training Loss: 0.04277534410357475\n",
      "Epoch 15381/30000 Training Loss: 0.0544247068464756\n",
      "Epoch 15382/30000 Training Loss: 0.04133271425962448\n",
      "Epoch 15383/30000 Training Loss: 0.05129808187484741\n",
      "Epoch 15384/30000 Training Loss: 0.061327483505010605\n",
      "Epoch 15385/30000 Training Loss: 0.04609613120555878\n",
      "Epoch 15386/30000 Training Loss: 0.0592769980430603\n",
      "Epoch 15387/30000 Training Loss: 0.03850298747420311\n",
      "Epoch 15388/30000 Training Loss: 0.03852929174900055\n",
      "Epoch 15389/30000 Training Loss: 0.048653993755578995\n",
      "Epoch 15390/30000 Training Loss: 0.047525085508823395\n",
      "Epoch 15391/30000 Training Loss: 0.05838152393698692\n",
      "Epoch 15392/30000 Training Loss: 0.059341102838516235\n",
      "Epoch 15393/30000 Training Loss: 0.035150811076164246\n",
      "Epoch 15394/30000 Training Loss: 0.08346991240978241\n",
      "Epoch 15395/30000 Training Loss: 0.05145123600959778\n",
      "Epoch 15396/30000 Training Loss: 0.057742442935705185\n",
      "Epoch 15397/30000 Training Loss: 0.039170149713754654\n",
      "Epoch 15398/30000 Training Loss: 0.0683220624923706\n",
      "Epoch 15399/30000 Training Loss: 0.0409715510904789\n",
      "Epoch 15400/30000 Training Loss: 0.04318559542298317\n",
      "Epoch 15400/30000 Validation Loss: 0.04874304309487343\n",
      "Epoch 15401/30000 Training Loss: 0.06340524554252625\n",
      "Epoch 15402/30000 Training Loss: 0.04046577215194702\n",
      "Epoch 15403/30000 Training Loss: 0.04824739322066307\n",
      "Epoch 15404/30000 Training Loss: 0.055683255195617676\n",
      "Epoch 15405/30000 Training Loss: 0.05200815945863724\n",
      "Epoch 15406/30000 Training Loss: 0.04425819218158722\n",
      "Epoch 15407/30000 Training Loss: 0.04657590389251709\n",
      "Epoch 15408/30000 Training Loss: 0.056970007717609406\n",
      "Epoch 15409/30000 Training Loss: 0.04850136116147041\n",
      "Epoch 15410/30000 Training Loss: 0.07024086266756058\n",
      "Epoch 15411/30000 Training Loss: 0.04278555139899254\n",
      "Epoch 15412/30000 Training Loss: 0.038752369582653046\n",
      "Epoch 15413/30000 Training Loss: 0.050597306340932846\n",
      "Epoch 15414/30000 Training Loss: 0.0497237965464592\n",
      "Epoch 15415/30000 Training Loss: 0.06073300540447235\n",
      "Epoch 15416/30000 Training Loss: 0.05151708796620369\n",
      "Epoch 15417/30000 Training Loss: 0.03923599049448967\n",
      "Epoch 15418/30000 Training Loss: 0.07488808035850525\n",
      "Epoch 15419/30000 Training Loss: 0.0537482425570488\n",
      "Epoch 15420/30000 Training Loss: 0.05673491209745407\n",
      "Epoch 15421/30000 Training Loss: 0.05192498117685318\n",
      "Epoch 15422/30000 Training Loss: 0.04918641597032547\n",
      "Epoch 15423/30000 Training Loss: 0.04996510595083237\n",
      "Epoch 15424/30000 Training Loss: 0.07269180566072464\n",
      "Epoch 15425/30000 Training Loss: 0.05173683539032936\n",
      "Epoch 15426/30000 Training Loss: 0.052761174738407135\n",
      "Epoch 15427/30000 Training Loss: 0.04552791640162468\n",
      "Epoch 15428/30000 Training Loss: 0.04488656669855118\n",
      "Epoch 15429/30000 Training Loss: 0.04608854651451111\n",
      "Epoch 15430/30000 Training Loss: 0.03829364851117134\n",
      "Epoch 15431/30000 Training Loss: 0.06863003224134445\n",
      "Epoch 15432/30000 Training Loss: 0.04405498132109642\n",
      "Epoch 15433/30000 Training Loss: 0.046782486140728\n",
      "Epoch 15434/30000 Training Loss: 0.04842797666788101\n",
      "Epoch 15435/30000 Training Loss: 0.04682541266083717\n",
      "Epoch 15436/30000 Training Loss: 0.06365519016981125\n",
      "Epoch 15437/30000 Training Loss: 0.04553505778312683\n",
      "Epoch 15438/30000 Training Loss: 0.05965277925133705\n",
      "Epoch 15439/30000 Training Loss: 0.04246589541435242\n",
      "Epoch 15440/30000 Training Loss: 0.03138568624854088\n",
      "Epoch 15441/30000 Training Loss: 0.050197944045066833\n",
      "Epoch 15442/30000 Training Loss: 0.07820586115121841\n",
      "Epoch 15443/30000 Training Loss: 0.0587606206536293\n",
      "Epoch 15444/30000 Training Loss: 0.05279819667339325\n",
      "Epoch 15445/30000 Training Loss: 0.0690707191824913\n",
      "Epoch 15446/30000 Training Loss: 0.041307177394628525\n",
      "Epoch 15447/30000 Training Loss: 0.03253697231411934\n",
      "Epoch 15448/30000 Training Loss: 0.0489923357963562\n",
      "Epoch 15449/30000 Training Loss: 0.04954122006893158\n",
      "Epoch 15450/30000 Training Loss: 0.04548973962664604\n",
      "Epoch 15451/30000 Training Loss: 0.04557693004608154\n",
      "Epoch 15452/30000 Training Loss: 0.049693986773490906\n",
      "Epoch 15453/30000 Training Loss: 0.055017437785863876\n",
      "Epoch 15454/30000 Training Loss: 0.0519908182322979\n",
      "Epoch 15455/30000 Training Loss: 0.08054257184267044\n",
      "Epoch 15456/30000 Training Loss: 0.05739223212003708\n",
      "Epoch 15457/30000 Training Loss: 0.04423966631293297\n",
      "Epoch 15458/30000 Training Loss: 0.05544263869524002\n",
      "Epoch 15459/30000 Training Loss: 0.046777307987213135\n",
      "Epoch 15460/30000 Training Loss: 0.038644395768642426\n",
      "Epoch 15461/30000 Training Loss: 0.04480832442641258\n",
      "Epoch 15462/30000 Training Loss: 0.05844300240278244\n",
      "Epoch 15463/30000 Training Loss: 0.056708987802267075\n",
      "Epoch 15464/30000 Training Loss: 0.035996779799461365\n",
      "Epoch 15465/30000 Training Loss: 0.06438836455345154\n",
      "Epoch 15466/30000 Training Loss: 0.051665160804986954\n",
      "Epoch 15467/30000 Training Loss: 0.041634757071733475\n",
      "Epoch 15468/30000 Training Loss: 0.03818663954734802\n",
      "Epoch 15469/30000 Training Loss: 0.054596174508333206\n",
      "Epoch 15470/30000 Training Loss: 0.04041466861963272\n",
      "Epoch 15471/30000 Training Loss: 0.043703220784664154\n",
      "Epoch 15472/30000 Training Loss: 0.039389513432979584\n",
      "Epoch 15473/30000 Training Loss: 0.04123011231422424\n",
      "Epoch 15474/30000 Training Loss: 0.05781009793281555\n",
      "Epoch 15475/30000 Training Loss: 0.04945194721221924\n",
      "Epoch 15476/30000 Training Loss: 0.042086631059646606\n",
      "Epoch 15477/30000 Training Loss: 0.03542393073439598\n",
      "Epoch 15478/30000 Training Loss: 0.04928702488541603\n",
      "Epoch 15479/30000 Training Loss: 0.0461098849773407\n",
      "Epoch 15480/30000 Training Loss: 0.04561138153076172\n",
      "Epoch 15481/30000 Training Loss: 0.05851520970463753\n",
      "Epoch 15482/30000 Training Loss: 0.037501852959394455\n",
      "Epoch 15483/30000 Training Loss: 0.046592820435762405\n",
      "Epoch 15484/30000 Training Loss: 0.040670059621334076\n",
      "Epoch 15485/30000 Training Loss: 0.06415589898824692\n",
      "Epoch 15486/30000 Training Loss: 0.04086293652653694\n",
      "Epoch 15487/30000 Training Loss: 0.05642127990722656\n",
      "Epoch 15488/30000 Training Loss: 0.05568356812000275\n",
      "Epoch 15489/30000 Training Loss: 0.04526598006486893\n",
      "Epoch 15490/30000 Training Loss: 0.05513182282447815\n",
      "Epoch 15491/30000 Training Loss: 0.04191075265407562\n",
      "Epoch 15492/30000 Training Loss: 0.041899681091308594\n",
      "Epoch 15493/30000 Training Loss: 0.048251088708639145\n",
      "Epoch 15494/30000 Training Loss: 0.04788614809513092\n",
      "Epoch 15495/30000 Training Loss: 0.07113815099000931\n",
      "Epoch 15496/30000 Training Loss: 0.04852563142776489\n",
      "Epoch 15497/30000 Training Loss: 0.033026617020368576\n",
      "Epoch 15498/30000 Training Loss: 0.029032642021775246\n",
      "Epoch 15499/30000 Training Loss: 0.049901049584150314\n",
      "Epoch 15500/30000 Training Loss: 0.04260978102684021\n",
      "Epoch 15500/30000 Validation Loss: 0.031492605805397034\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.031492605805397034<=============\n",
      "Epoch 15501/30000 Training Loss: 0.0544082373380661\n",
      "Epoch 15502/30000 Training Loss: 0.05119381844997406\n",
      "Epoch 15503/30000 Training Loss: 0.04200807586312294\n",
      "Epoch 15504/30000 Training Loss: 0.04404732584953308\n",
      "Epoch 15505/30000 Training Loss: 0.06653384119272232\n",
      "Epoch 15506/30000 Training Loss: 0.041128434240818024\n",
      "Epoch 15507/30000 Training Loss: 0.04507149010896683\n",
      "Epoch 15508/30000 Training Loss: 0.05172376334667206\n",
      "Epoch 15509/30000 Training Loss: 0.04916946217417717\n",
      "Epoch 15510/30000 Training Loss: 0.05700046569108963\n",
      "Epoch 15511/30000 Training Loss: 0.054487504065036774\n",
      "Epoch 15512/30000 Training Loss: 0.06584005802869797\n",
      "Epoch 15513/30000 Training Loss: 0.048989713191986084\n",
      "Epoch 15514/30000 Training Loss: 0.04756999760866165\n",
      "Epoch 15515/30000 Training Loss: 0.04117869213223457\n",
      "Epoch 15516/30000 Training Loss: 0.055725403130054474\n",
      "Epoch 15517/30000 Training Loss: 0.04785513877868652\n",
      "Epoch 15518/30000 Training Loss: 0.045383818447589874\n",
      "Epoch 15519/30000 Training Loss: 0.033446893095970154\n",
      "Epoch 15520/30000 Training Loss: 0.0396663174033165\n",
      "Epoch 15521/30000 Training Loss: 0.044198259711265564\n",
      "Epoch 15522/30000 Training Loss: 0.04971573129296303\n",
      "Epoch 15523/30000 Training Loss: 0.04168027639389038\n",
      "Epoch 15524/30000 Training Loss: 0.05007515102624893\n",
      "Epoch 15525/30000 Training Loss: 0.05110958591103554\n",
      "Epoch 15526/30000 Training Loss: 0.03787703812122345\n",
      "Epoch 15527/30000 Training Loss: 0.04644639045000076\n",
      "Epoch 15528/30000 Training Loss: 0.0514511875808239\n",
      "Epoch 15529/30000 Training Loss: 0.050652071833610535\n",
      "Epoch 15530/30000 Training Loss: 0.055643994361162186\n",
      "Epoch 15531/30000 Training Loss: 0.055203020572662354\n",
      "Epoch 15532/30000 Training Loss: 0.040009647607803345\n",
      "Epoch 15533/30000 Training Loss: 0.04927142709493637\n",
      "Epoch 15534/30000 Training Loss: 0.05154227837920189\n",
      "Epoch 15535/30000 Training Loss: 0.042229361832141876\n",
      "Epoch 15536/30000 Training Loss: 0.04251059889793396\n",
      "Epoch 15537/30000 Training Loss: 0.05452798679471016\n",
      "Epoch 15538/30000 Training Loss: 0.04786565527319908\n",
      "Epoch 15539/30000 Training Loss: 0.06745187193155289\n",
      "Epoch 15540/30000 Training Loss: 0.037710558623075485\n",
      "Epoch 15541/30000 Training Loss: 0.06711854040622711\n",
      "Epoch 15542/30000 Training Loss: 0.04758552461862564\n",
      "Epoch 15543/30000 Training Loss: 0.048411719501018524\n",
      "Epoch 15544/30000 Training Loss: 0.057357050478458405\n",
      "Epoch 15545/30000 Training Loss: 0.07052914798259735\n",
      "Epoch 15546/30000 Training Loss: 0.053834281861782074\n",
      "Epoch 15547/30000 Training Loss: 0.0611124262213707\n",
      "Epoch 15548/30000 Training Loss: 0.05907836928963661\n",
      "Epoch 15549/30000 Training Loss: 0.038062382489442825\n",
      "Epoch 15550/30000 Training Loss: 0.05077669024467468\n",
      "Epoch 15551/30000 Training Loss: 0.0498364195227623\n",
      "Epoch 15552/30000 Training Loss: 0.04678095504641533\n",
      "Epoch 15553/30000 Training Loss: 0.05388481169939041\n",
      "Epoch 15554/30000 Training Loss: 0.059668101370334625\n",
      "Epoch 15555/30000 Training Loss: 0.0591905377805233\n",
      "Epoch 15556/30000 Training Loss: 0.047808367758989334\n",
      "Epoch 15557/30000 Training Loss: 0.0422605536878109\n",
      "Epoch 15558/30000 Training Loss: 0.04508347809314728\n",
      "Epoch 15559/30000 Training Loss: 0.050232481211423874\n",
      "Epoch 15560/30000 Training Loss: 0.0599244087934494\n",
      "Epoch 15561/30000 Training Loss: 0.07059372961521149\n",
      "Epoch 15562/30000 Training Loss: 0.04804297536611557\n",
      "Epoch 15563/30000 Training Loss: 0.041703153401613235\n",
      "Epoch 15564/30000 Training Loss: 0.03931926190853119\n",
      "Epoch 15565/30000 Training Loss: 0.04583042114973068\n",
      "Epoch 15566/30000 Training Loss: 0.04423581063747406\n",
      "Epoch 15567/30000 Training Loss: 0.06672769039869308\n",
      "Epoch 15568/30000 Training Loss: 0.061078865081071854\n",
      "Epoch 15569/30000 Training Loss: 0.043235115706920624\n",
      "Epoch 15570/30000 Training Loss: 0.058442678302526474\n",
      "Epoch 15571/30000 Training Loss: 0.04739553853869438\n",
      "Epoch 15572/30000 Training Loss: 0.038741856813430786\n",
      "Epoch 15573/30000 Training Loss: 0.043572574853897095\n",
      "Epoch 15574/30000 Training Loss: 0.03506643325090408\n",
      "Epoch 15575/30000 Training Loss: 0.05537034571170807\n",
      "Epoch 15576/30000 Training Loss: 0.04248218238353729\n",
      "Epoch 15577/30000 Training Loss: 0.03876803070306778\n",
      "Epoch 15578/30000 Training Loss: 0.0438348762691021\n",
      "Epoch 15579/30000 Training Loss: 0.04492863640189171\n",
      "Epoch 15580/30000 Training Loss: 0.03856900334358215\n",
      "Epoch 15581/30000 Training Loss: 0.035330139100551605\n",
      "Epoch 15582/30000 Training Loss: 0.05842173844575882\n",
      "Epoch 15583/30000 Training Loss: 0.043281570076942444\n",
      "Epoch 15584/30000 Training Loss: 0.05459028109908104\n",
      "Epoch 15585/30000 Training Loss: 0.04451070725917816\n",
      "Epoch 15586/30000 Training Loss: 0.04647166281938553\n",
      "Epoch 15587/30000 Training Loss: 0.04860715940594673\n",
      "Epoch 15588/30000 Training Loss: 0.039567701518535614\n",
      "Epoch 15589/30000 Training Loss: 0.05739077925682068\n",
      "Epoch 15590/30000 Training Loss: 0.04495648294687271\n",
      "Epoch 15591/30000 Training Loss: 0.045932527631521225\n",
      "Epoch 15592/30000 Training Loss: 0.039187680929899216\n",
      "Epoch 15593/30000 Training Loss: 0.03834770247340202\n",
      "Epoch 15594/30000 Training Loss: 0.07381214201450348\n",
      "Epoch 15595/30000 Training Loss: 0.040298957377672195\n",
      "Epoch 15596/30000 Training Loss: 0.039045579731464386\n",
      "Epoch 15597/30000 Training Loss: 0.07549112290143967\n",
      "Epoch 15598/30000 Training Loss: 0.05388851463794708\n",
      "Epoch 15599/30000 Training Loss: 0.06545164436101913\n",
      "Epoch 15600/30000 Training Loss: 0.05208446457982063\n",
      "Epoch 15600/30000 Validation Loss: 0.052831608802080154\n",
      "Epoch 15601/30000 Training Loss: 0.04312365874648094\n",
      "Epoch 15602/30000 Training Loss: 0.05599260330200195\n",
      "Epoch 15603/30000 Training Loss: 0.04513981193304062\n",
      "Epoch 15604/30000 Training Loss: 0.050326813012361526\n",
      "Epoch 15605/30000 Training Loss: 0.05869422107934952\n",
      "Epoch 15606/30000 Training Loss: 0.030853334814310074\n",
      "Epoch 15607/30000 Training Loss: 0.045815497636795044\n",
      "Epoch 15608/30000 Training Loss: 0.046021997928619385\n",
      "Epoch 15609/30000 Training Loss: 0.052268870174884796\n",
      "Epoch 15610/30000 Training Loss: 0.0481656938791275\n",
      "Epoch 15611/30000 Training Loss: 0.05089651420712471\n",
      "Epoch 15612/30000 Training Loss: 0.04965149238705635\n",
      "Epoch 15613/30000 Training Loss: 0.0400773361325264\n",
      "Epoch 15614/30000 Training Loss: 0.027063606306910515\n",
      "Epoch 15615/30000 Training Loss: 0.0490664467215538\n",
      "Epoch 15616/30000 Training Loss: 0.08094271272420883\n",
      "Epoch 15617/30000 Training Loss: 0.04671739414334297\n",
      "Epoch 15618/30000 Training Loss: 0.04175511747598648\n",
      "Epoch 15619/30000 Training Loss: 0.05884109437465668\n",
      "Epoch 15620/30000 Training Loss: 0.058805324137210846\n",
      "Epoch 15621/30000 Training Loss: 0.043933071196079254\n",
      "Epoch 15622/30000 Training Loss: 0.049566980451345444\n",
      "Epoch 15623/30000 Training Loss: 0.03932393342256546\n",
      "Epoch 15624/30000 Training Loss: 0.056813616305589676\n",
      "Epoch 15625/30000 Training Loss: 0.04976604878902435\n",
      "Epoch 15626/30000 Training Loss: 0.04375935718417168\n",
      "Epoch 15627/30000 Training Loss: 0.04398846626281738\n",
      "Epoch 15628/30000 Training Loss: 0.03984834626317024\n",
      "Epoch 15629/30000 Training Loss: 0.05131954699754715\n",
      "Epoch 15630/30000 Training Loss: 0.04011230170726776\n",
      "Epoch 15631/30000 Training Loss: 0.04893556237220764\n",
      "Epoch 15632/30000 Training Loss: 0.047904111444950104\n",
      "Epoch 15633/30000 Training Loss: 0.0612185113132\n",
      "Epoch 15634/30000 Training Loss: 0.03383110091090202\n",
      "Epoch 15635/30000 Training Loss: 0.04791632294654846\n",
      "Epoch 15636/30000 Training Loss: 0.04500003904104233\n",
      "Epoch 15637/30000 Training Loss: 0.0581667423248291\n",
      "Epoch 15638/30000 Training Loss: 0.036304812878370285\n",
      "Epoch 15639/30000 Training Loss: 0.039176154881715775\n",
      "Epoch 15640/30000 Training Loss: 0.05087863281369209\n",
      "Epoch 15641/30000 Training Loss: 0.06236850470304489\n",
      "Epoch 15642/30000 Training Loss: 0.041140828281641006\n",
      "Epoch 15643/30000 Training Loss: 0.05806313082575798\n",
      "Epoch 15644/30000 Training Loss: 0.047788117080926895\n",
      "Epoch 15645/30000 Training Loss: 0.05039101094007492\n",
      "Epoch 15646/30000 Training Loss: 0.04482709988951683\n",
      "Epoch 15647/30000 Training Loss: 0.06523599475622177\n",
      "Epoch 15648/30000 Training Loss: 0.056373074650764465\n",
      "Epoch 15649/30000 Training Loss: 0.04127882048487663\n",
      "Epoch 15650/30000 Training Loss: 0.04725038260221481\n",
      "Epoch 15651/30000 Training Loss: 0.047357942909002304\n",
      "Epoch 15652/30000 Training Loss: 0.05072883144021034\n",
      "Epoch 15653/30000 Training Loss: 0.05774018540978432\n",
      "Epoch 15654/30000 Training Loss: 0.06305363774299622\n",
      "Epoch 15655/30000 Training Loss: 0.039941463619470596\n",
      "Epoch 15656/30000 Training Loss: 0.0517299547791481\n",
      "Epoch 15657/30000 Training Loss: 0.07404817640781403\n",
      "Epoch 15658/30000 Training Loss: 0.05549859255552292\n",
      "Epoch 15659/30000 Training Loss: 0.045544207096099854\n",
      "Epoch 15660/30000 Training Loss: 0.06444818526506424\n",
      "Epoch 15661/30000 Training Loss: 0.04236064851284027\n",
      "Epoch 15662/30000 Training Loss: 0.04243752732872963\n",
      "Epoch 15663/30000 Training Loss: 0.053898733109235764\n",
      "Epoch 15664/30000 Training Loss: 0.06312865018844604\n",
      "Epoch 15665/30000 Training Loss: 0.05563748627901077\n",
      "Epoch 15666/30000 Training Loss: 0.05826722830533981\n",
      "Epoch 15667/30000 Training Loss: 0.051248565316200256\n",
      "Epoch 15668/30000 Training Loss: 0.05423780903220177\n",
      "Epoch 15669/30000 Training Loss: 0.03938128054141998\n",
      "Epoch 15670/30000 Training Loss: 0.05356565862894058\n",
      "Epoch 15671/30000 Training Loss: 0.06990596652030945\n",
      "Epoch 15672/30000 Training Loss: 0.0442003533244133\n",
      "Epoch 15673/30000 Training Loss: 0.055514171719551086\n",
      "Epoch 15674/30000 Training Loss: 0.04857077822089195\n",
      "Epoch 15675/30000 Training Loss: 0.04271143674850464\n",
      "Epoch 15676/30000 Training Loss: 0.05524105578660965\n",
      "Epoch 15677/30000 Training Loss: 0.04463605582714081\n",
      "Epoch 15678/30000 Training Loss: 0.05641341209411621\n",
      "Epoch 15679/30000 Training Loss: 0.06259294599294662\n",
      "Epoch 15680/30000 Training Loss: 0.052071500569581985\n",
      "Epoch 15681/30000 Training Loss: 0.06734508275985718\n",
      "Epoch 15682/30000 Training Loss: 0.04407815262675285\n",
      "Epoch 15683/30000 Training Loss: 0.057777486741542816\n",
      "Epoch 15684/30000 Training Loss: 0.06044618785381317\n",
      "Epoch 15685/30000 Training Loss: 0.04482409730553627\n",
      "Epoch 15686/30000 Training Loss: 0.04699847847223282\n",
      "Epoch 15687/30000 Training Loss: 0.04546819627285004\n",
      "Epoch 15688/30000 Training Loss: 0.04170355573296547\n",
      "Epoch 15689/30000 Training Loss: 0.050323449075222015\n",
      "Epoch 15690/30000 Training Loss: 0.06465490162372589\n",
      "Epoch 15691/30000 Training Loss: 0.06142893433570862\n",
      "Epoch 15692/30000 Training Loss: 0.04430127516388893\n",
      "Epoch 15693/30000 Training Loss: 0.039939336478710175\n",
      "Epoch 15694/30000 Training Loss: 0.059513431042432785\n",
      "Epoch 15695/30000 Training Loss: 0.044189609587192535\n",
      "Epoch 15696/30000 Training Loss: 0.061237938702106476\n",
      "Epoch 15697/30000 Training Loss: 0.040252745151519775\n",
      "Epoch 15698/30000 Training Loss: 0.04924803972244263\n",
      "Epoch 15699/30000 Training Loss: 0.04763354733586311\n",
      "Epoch 15700/30000 Training Loss: 0.045453522354364395\n",
      "Epoch 15700/30000 Validation Loss: 0.04604854807257652\n",
      "Epoch 15701/30000 Training Loss: 0.04540732502937317\n",
      "Epoch 15702/30000 Training Loss: 0.055766500532627106\n",
      "Epoch 15703/30000 Training Loss: 0.06462890654802322\n",
      "Epoch 15704/30000 Training Loss: 0.03647255524992943\n",
      "Epoch 15705/30000 Training Loss: 0.06289508938789368\n",
      "Epoch 15706/30000 Training Loss: 0.04886277765035629\n",
      "Epoch 15707/30000 Training Loss: 0.0685764029622078\n",
      "Epoch 15708/30000 Training Loss: 0.05820313096046448\n",
      "Epoch 15709/30000 Training Loss: 0.046938423067331314\n",
      "Epoch 15710/30000 Training Loss: 0.046221453696489334\n",
      "Epoch 15711/30000 Training Loss: 0.06165176257491112\n",
      "Epoch 15712/30000 Training Loss: 0.048801347613334656\n",
      "Epoch 15713/30000 Training Loss: 0.041003286838531494\n",
      "Epoch 15714/30000 Training Loss: 0.050941962748765945\n",
      "Epoch 15715/30000 Training Loss: 0.05776575952768326\n",
      "Epoch 15716/30000 Training Loss: 0.056085966527462006\n",
      "Epoch 15717/30000 Training Loss: 0.0661889910697937\n",
      "Epoch 15718/30000 Training Loss: 0.05197029560804367\n",
      "Epoch 15719/30000 Training Loss: 0.05557860806584358\n",
      "Epoch 15720/30000 Training Loss: 0.0339089073240757\n",
      "Epoch 15721/30000 Training Loss: 0.05155576765537262\n",
      "Epoch 15722/30000 Training Loss: 0.06030084937810898\n",
      "Epoch 15723/30000 Training Loss: 0.06439893692731857\n",
      "Epoch 15724/30000 Training Loss: 0.031727615743875504\n",
      "Epoch 15725/30000 Training Loss: 0.03529127687215805\n",
      "Epoch 15726/30000 Training Loss: 0.058980733156204224\n",
      "Epoch 15727/30000 Training Loss: 0.032340362668037415\n",
      "Epoch 15728/30000 Training Loss: 0.03874275088310242\n",
      "Epoch 15729/30000 Training Loss: 0.05150352418422699\n",
      "Epoch 15730/30000 Training Loss: 0.04964042827486992\n",
      "Epoch 15731/30000 Training Loss: 0.03782908618450165\n",
      "Epoch 15732/30000 Training Loss: 0.05249764025211334\n",
      "Epoch 15733/30000 Training Loss: 0.03896681219339371\n",
      "Epoch 15734/30000 Training Loss: 0.037960801273584366\n",
      "Epoch 15735/30000 Training Loss: 0.05778355896472931\n",
      "Epoch 15736/30000 Training Loss: 0.04884974658489227\n",
      "Epoch 15737/30000 Training Loss: 0.052847087383270264\n",
      "Epoch 15738/30000 Training Loss: 0.054471489042043686\n",
      "Epoch 15739/30000 Training Loss: 0.04135444760322571\n",
      "Epoch 15740/30000 Training Loss: 0.047013815492391586\n",
      "Epoch 15741/30000 Training Loss: 0.05499649420380592\n",
      "Epoch 15742/30000 Training Loss: 0.03239261731505394\n",
      "Epoch 15743/30000 Training Loss: 0.04222142696380615\n",
      "Epoch 15744/30000 Training Loss: 0.05127784609794617\n",
      "Epoch 15745/30000 Training Loss: 0.05524713918566704\n",
      "Epoch 15746/30000 Training Loss: 0.038261763751506805\n",
      "Epoch 15747/30000 Training Loss: 0.05224599316716194\n",
      "Epoch 15748/30000 Training Loss: 0.03571910783648491\n",
      "Epoch 15749/30000 Training Loss: 0.039076659828424454\n",
      "Epoch 15750/30000 Training Loss: 0.04070335254073143\n",
      "Epoch 15751/30000 Training Loss: 0.06047574430704117\n",
      "Epoch 15752/30000 Training Loss: 0.05358857661485672\n",
      "Epoch 15753/30000 Training Loss: 0.05440419539809227\n",
      "Epoch 15754/30000 Training Loss: 0.04545055329799652\n",
      "Epoch 15755/30000 Training Loss: 0.05408497154712677\n",
      "Epoch 15756/30000 Training Loss: 0.05912119150161743\n",
      "Epoch 15757/30000 Training Loss: 0.050529249012470245\n",
      "Epoch 15758/30000 Training Loss: 0.04888898879289627\n",
      "Epoch 15759/30000 Training Loss: 0.04905959963798523\n",
      "Epoch 15760/30000 Training Loss: 0.0581117607653141\n",
      "Epoch 15761/30000 Training Loss: 0.037264272570610046\n",
      "Epoch 15762/30000 Training Loss: 0.038774605840444565\n",
      "Epoch 15763/30000 Training Loss: 0.049347370862960815\n",
      "Epoch 15764/30000 Training Loss: 0.05807960778474808\n",
      "Epoch 15765/30000 Training Loss: 0.04132547602057457\n",
      "Epoch 15766/30000 Training Loss: 0.03814545273780823\n",
      "Epoch 15767/30000 Training Loss: 0.0494530014693737\n",
      "Epoch 15768/30000 Training Loss: 0.0404103621840477\n",
      "Epoch 15769/30000 Training Loss: 0.044991135597229004\n",
      "Epoch 15770/30000 Training Loss: 0.04880070686340332\n",
      "Epoch 15771/30000 Training Loss: 0.04266198351979256\n",
      "Epoch 15772/30000 Training Loss: 0.04064876586198807\n",
      "Epoch 15773/30000 Training Loss: 0.06232208013534546\n",
      "Epoch 15774/30000 Training Loss: 0.04201976954936981\n",
      "Epoch 15775/30000 Training Loss: 0.04903330281376839\n",
      "Epoch 15776/30000 Training Loss: 0.062497492879629135\n",
      "Epoch 15777/30000 Training Loss: 0.0498824343085289\n",
      "Epoch 15778/30000 Training Loss: 0.06559889018535614\n",
      "Epoch 15779/30000 Training Loss: 0.06824598461389542\n",
      "Epoch 15780/30000 Training Loss: 0.0406646654009819\n",
      "Epoch 15781/30000 Training Loss: 0.04208110272884369\n",
      "Epoch 15782/30000 Training Loss: 0.052977386862039566\n",
      "Epoch 15783/30000 Training Loss: 0.04929177463054657\n",
      "Epoch 15784/30000 Training Loss: 0.0530998520553112\n",
      "Epoch 15785/30000 Training Loss: 0.049254175275564194\n",
      "Epoch 15786/30000 Training Loss: 0.04784562438726425\n",
      "Epoch 15787/30000 Training Loss: 0.06878134608268738\n",
      "Epoch 15788/30000 Training Loss: 0.05217915028333664\n",
      "Epoch 15789/30000 Training Loss: 0.04685332626104355\n",
      "Epoch 15790/30000 Training Loss: 0.056144971400499344\n",
      "Epoch 15791/30000 Training Loss: 0.04096926376223564\n",
      "Epoch 15792/30000 Training Loss: 0.06015273556113243\n",
      "Epoch 15793/30000 Training Loss: 0.06015189737081528\n",
      "Epoch 15794/30000 Training Loss: 0.04101855680346489\n",
      "Epoch 15795/30000 Training Loss: 0.05355294793844223\n",
      "Epoch 15796/30000 Training Loss: 0.048743877559900284\n",
      "Epoch 15797/30000 Training Loss: 0.043240174651145935\n",
      "Epoch 15798/30000 Training Loss: 0.04387044161558151\n",
      "Epoch 15799/30000 Training Loss: 0.055646441876888275\n",
      "Epoch 15800/30000 Training Loss: 0.03907960653305054\n",
      "Epoch 15800/30000 Validation Loss: 0.04130055010318756\n",
      "Epoch 15801/30000 Training Loss: 0.042717769742012024\n",
      "Epoch 15802/30000 Training Loss: 0.05386420339345932\n",
      "Epoch 15803/30000 Training Loss: 0.07398229837417603\n",
      "Epoch 15804/30000 Training Loss: 0.046567387878894806\n",
      "Epoch 15805/30000 Training Loss: 0.05824743211269379\n",
      "Epoch 15806/30000 Training Loss: 0.032354891300201416\n",
      "Epoch 15807/30000 Training Loss: 0.05311741679906845\n",
      "Epoch 15808/30000 Training Loss: 0.060049619525671005\n",
      "Epoch 15809/30000 Training Loss: 0.057375501841306686\n",
      "Epoch 15810/30000 Training Loss: 0.04579811543226242\n",
      "Epoch 15811/30000 Training Loss: 0.05604980140924454\n",
      "Epoch 15812/30000 Training Loss: 0.047590408474206924\n",
      "Epoch 15813/30000 Training Loss: 0.0407223142683506\n",
      "Epoch 15814/30000 Training Loss: 0.053072698414325714\n",
      "Epoch 15815/30000 Training Loss: 0.05083445459604263\n",
      "Epoch 15816/30000 Training Loss: 0.037890903651714325\n",
      "Epoch 15817/30000 Training Loss: 0.05337056145071983\n",
      "Epoch 15818/30000 Training Loss: 0.049921974539756775\n",
      "Epoch 15819/30000 Training Loss: 0.046939630061388016\n",
      "Epoch 15820/30000 Training Loss: 0.06695383042097092\n",
      "Epoch 15821/30000 Training Loss: 0.05279528349637985\n",
      "Epoch 15822/30000 Training Loss: 0.052315786480903625\n",
      "Epoch 15823/30000 Training Loss: 0.0584314689040184\n",
      "Epoch 15824/30000 Training Loss: 0.0547310896217823\n",
      "Epoch 15825/30000 Training Loss: 0.05151858925819397\n",
      "Epoch 15826/30000 Training Loss: 0.040943898260593414\n",
      "Epoch 15827/30000 Training Loss: 0.0572851337492466\n",
      "Epoch 15828/30000 Training Loss: 0.052864979952573776\n",
      "Epoch 15829/30000 Training Loss: 0.06131809204816818\n",
      "Epoch 15830/30000 Training Loss: 0.05037733167409897\n",
      "Epoch 15831/30000 Training Loss: 0.04672492295503616\n",
      "Epoch 15832/30000 Training Loss: 0.049578141421079636\n",
      "Epoch 15833/30000 Training Loss: 0.0415080189704895\n",
      "Epoch 15834/30000 Training Loss: 0.05953532084822655\n",
      "Epoch 15835/30000 Training Loss: 0.053399328142404556\n",
      "Epoch 15836/30000 Training Loss: 0.039593301713466644\n",
      "Epoch 15837/30000 Training Loss: 0.05033555254340172\n",
      "Epoch 15838/30000 Training Loss: 0.04957447201013565\n",
      "Epoch 15839/30000 Training Loss: 0.044658176600933075\n",
      "Epoch 15840/30000 Training Loss: 0.07321196049451828\n",
      "Epoch 15841/30000 Training Loss: 0.0464947372674942\n",
      "Epoch 15842/30000 Training Loss: 0.04069475829601288\n",
      "Epoch 15843/30000 Training Loss: 0.04551776498556137\n",
      "Epoch 15844/30000 Training Loss: 0.035649485886096954\n",
      "Epoch 15845/30000 Training Loss: 0.04225720837712288\n",
      "Epoch 15846/30000 Training Loss: 0.039637450128793716\n",
      "Epoch 15847/30000 Training Loss: 0.03699322044849396\n",
      "Epoch 15848/30000 Training Loss: 0.05641341581940651\n",
      "Epoch 15849/30000 Training Loss: 0.03707326948642731\n",
      "Epoch 15850/30000 Training Loss: 0.04769492894411087\n",
      "Epoch 15851/30000 Training Loss: 0.05442102253437042\n",
      "Epoch 15852/30000 Training Loss: 0.06846712529659271\n",
      "Epoch 15853/30000 Training Loss: 0.041058048605918884\n",
      "Epoch 15854/30000 Training Loss: 0.06317023932933807\n",
      "Epoch 15855/30000 Training Loss: 0.03913719952106476\n",
      "Epoch 15856/30000 Training Loss: 0.03853043168783188\n",
      "Epoch 15857/30000 Training Loss: 0.04639740660786629\n",
      "Epoch 15858/30000 Training Loss: 0.05203600972890854\n",
      "Epoch 15859/30000 Training Loss: 0.036704495549201965\n",
      "Epoch 15860/30000 Training Loss: 0.04529711976647377\n",
      "Epoch 15861/30000 Training Loss: 0.06677461415529251\n",
      "Epoch 15862/30000 Training Loss: 0.04105687141418457\n",
      "Epoch 15863/30000 Training Loss: 0.04451580345630646\n",
      "Epoch 15864/30000 Training Loss: 0.0625646561384201\n",
      "Epoch 15865/30000 Training Loss: 0.05603174492716789\n",
      "Epoch 15866/30000 Training Loss: 0.05872618034482002\n",
      "Epoch 15867/30000 Training Loss: 0.0388592854142189\n",
      "Epoch 15868/30000 Training Loss: 0.0629550889134407\n",
      "Epoch 15869/30000 Training Loss: 0.04728592559695244\n",
      "Epoch 15870/30000 Training Loss: 0.06094851717352867\n",
      "Epoch 15871/30000 Training Loss: 0.04584110528230667\n",
      "Epoch 15872/30000 Training Loss: 0.05340122804045677\n",
      "Epoch 15873/30000 Training Loss: 0.04609237611293793\n",
      "Epoch 15874/30000 Training Loss: 0.04116824269294739\n",
      "Epoch 15875/30000 Training Loss: 0.04845426231622696\n",
      "Epoch 15876/30000 Training Loss: 0.04993896186351776\n",
      "Epoch 15877/30000 Training Loss: 0.053419627249240875\n",
      "Epoch 15878/30000 Training Loss: 0.05115831643342972\n",
      "Epoch 15879/30000 Training Loss: 0.04406965523958206\n",
      "Epoch 15880/30000 Training Loss: 0.05762246996164322\n",
      "Epoch 15881/30000 Training Loss: 0.0383128859102726\n",
      "Epoch 15882/30000 Training Loss: 0.041165389120578766\n",
      "Epoch 15883/30000 Training Loss: 0.04028741270303726\n",
      "Epoch 15884/30000 Training Loss: 0.04113158583641052\n",
      "Epoch 15885/30000 Training Loss: 0.03647425025701523\n",
      "Epoch 15886/30000 Training Loss: 0.07127292454242706\n",
      "Epoch 15887/30000 Training Loss: 0.045088157057762146\n",
      "Epoch 15888/30000 Training Loss: 0.05539055913686752\n",
      "Epoch 15889/30000 Training Loss: 0.07019352912902832\n",
      "Epoch 15890/30000 Training Loss: 0.047691985964775085\n",
      "Epoch 15891/30000 Training Loss: 0.061380233615636826\n",
      "Epoch 15892/30000 Training Loss: 0.04428163915872574\n",
      "Epoch 15893/30000 Training Loss: 0.06257572770118713\n",
      "Epoch 15894/30000 Training Loss: 0.04735003411769867\n",
      "Epoch 15895/30000 Training Loss: 0.055764615535736084\n",
      "Epoch 15896/30000 Training Loss: 0.05185696482658386\n",
      "Epoch 15897/30000 Training Loss: 0.06296035647392273\n",
      "Epoch 15898/30000 Training Loss: 0.04176132380962372\n",
      "Epoch 15899/30000 Training Loss: 0.04349135607481003\n",
      "Epoch 15900/30000 Training Loss: 0.05792416259646416\n",
      "Epoch 15900/30000 Validation Loss: 0.043620526790618896\n",
      "Epoch 15901/30000 Training Loss: 0.06322412192821503\n",
      "Epoch 15902/30000 Training Loss: 0.059729959815740585\n",
      "Epoch 15903/30000 Training Loss: 0.0520026795566082\n",
      "Epoch 15904/30000 Training Loss: 0.04711465165019035\n",
      "Epoch 15905/30000 Training Loss: 0.0446554571390152\n",
      "Epoch 15906/30000 Training Loss: 0.04838241636753082\n",
      "Epoch 15907/30000 Training Loss: 0.05146392434835434\n",
      "Epoch 15908/30000 Training Loss: 0.05592087656259537\n",
      "Epoch 15909/30000 Training Loss: 0.06457044929265976\n",
      "Epoch 15910/30000 Training Loss: 0.05250426381826401\n",
      "Epoch 15911/30000 Training Loss: 0.0553673692047596\n",
      "Epoch 15912/30000 Training Loss: 0.033984217792749405\n",
      "Epoch 15913/30000 Training Loss: 0.04354146867990494\n",
      "Epoch 15914/30000 Training Loss: 0.03209148719906807\n",
      "Epoch 15915/30000 Training Loss: 0.044087450951337814\n",
      "Epoch 15916/30000 Training Loss: 0.03921569511294365\n",
      "Epoch 15917/30000 Training Loss: 0.04598856717348099\n",
      "Epoch 15918/30000 Training Loss: 0.04456547647714615\n",
      "Epoch 15919/30000 Training Loss: 0.058400027453899384\n",
      "Epoch 15920/30000 Training Loss: 0.05301690474152565\n",
      "Epoch 15921/30000 Training Loss: 0.05484382435679436\n",
      "Epoch 15922/30000 Training Loss: 0.07081989198923111\n",
      "Epoch 15923/30000 Training Loss: 0.04704709351062775\n",
      "Epoch 15924/30000 Training Loss: 0.051074232906103134\n",
      "Epoch 15925/30000 Training Loss: 0.057163093239068985\n",
      "Epoch 15926/30000 Training Loss: 0.04025751352310181\n",
      "Epoch 15927/30000 Training Loss: 0.051443517208099365\n",
      "Epoch 15928/30000 Training Loss: 0.04104367271065712\n",
      "Epoch 15929/30000 Training Loss: 0.060887910425662994\n",
      "Epoch 15930/30000 Training Loss: 0.05444483086466789\n",
      "Epoch 15931/30000 Training Loss: 0.06112862378358841\n",
      "Epoch 15932/30000 Training Loss: 0.03988172486424446\n",
      "Epoch 15933/30000 Training Loss: 0.04777049645781517\n",
      "Epoch 15934/30000 Training Loss: 0.05355028435587883\n",
      "Epoch 15935/30000 Training Loss: 0.04598627984523773\n",
      "Epoch 15936/30000 Training Loss: 0.043055228888988495\n",
      "Epoch 15937/30000 Training Loss: 0.059180598706007004\n",
      "Epoch 15938/30000 Training Loss: 0.04745744913816452\n",
      "Epoch 15939/30000 Training Loss: 0.05002567172050476\n",
      "Epoch 15940/30000 Training Loss: 0.06134292110800743\n",
      "Epoch 15941/30000 Training Loss: 0.04330062493681908\n",
      "Epoch 15942/30000 Training Loss: 0.04434166103601456\n",
      "Epoch 15943/30000 Training Loss: 0.041661009192466736\n",
      "Epoch 15944/30000 Training Loss: 0.054439179599285126\n",
      "Epoch 15945/30000 Training Loss: 0.043345384299755096\n",
      "Epoch 15946/30000 Training Loss: 0.04017728194594383\n",
      "Epoch 15947/30000 Training Loss: 0.04368776082992554\n",
      "Epoch 15948/30000 Training Loss: 0.048285797238349915\n",
      "Epoch 15949/30000 Training Loss: 0.04859001934528351\n",
      "Epoch 15950/30000 Training Loss: 0.03765290603041649\n",
      "Epoch 15951/30000 Training Loss: 0.04927406460046768\n",
      "Epoch 15952/30000 Training Loss: 0.05935239791870117\n",
      "Epoch 15953/30000 Training Loss: 0.05111822485923767\n",
      "Epoch 15954/30000 Training Loss: 0.04658699035644531\n",
      "Epoch 15955/30000 Training Loss: 0.059567276388406754\n",
      "Epoch 15956/30000 Training Loss: 0.05809255689382553\n",
      "Epoch 15957/30000 Training Loss: 0.05219264328479767\n",
      "Epoch 15958/30000 Training Loss: 0.05221293494105339\n",
      "Epoch 15959/30000 Training Loss: 0.037956804037094116\n",
      "Epoch 15960/30000 Training Loss: 0.0512816347181797\n",
      "Epoch 15961/30000 Training Loss: 0.06269481033086777\n",
      "Epoch 15962/30000 Training Loss: 0.04645124822854996\n",
      "Epoch 15963/30000 Training Loss: 0.06423547863960266\n",
      "Epoch 15964/30000 Training Loss: 0.05630185827612877\n",
      "Epoch 15965/30000 Training Loss: 0.045115455985069275\n",
      "Epoch 15966/30000 Training Loss: 0.03609771654009819\n",
      "Epoch 15967/30000 Training Loss: 0.03980180621147156\n",
      "Epoch 15968/30000 Training Loss: 0.04723276197910309\n",
      "Epoch 15969/30000 Training Loss: 0.046584352850914\n",
      "Epoch 15970/30000 Training Loss: 0.05068263038992882\n",
      "Epoch 15971/30000 Training Loss: 0.055594440549612045\n",
      "Epoch 15972/30000 Training Loss: 0.04514016956090927\n",
      "Epoch 15973/30000 Training Loss: 0.03427063673734665\n",
      "Epoch 15974/30000 Training Loss: 0.04754037410020828\n",
      "Epoch 15975/30000 Training Loss: 0.04285740852355957\n",
      "Epoch 15976/30000 Training Loss: 0.056138359010219574\n",
      "Epoch 15977/30000 Training Loss: 0.054906342178583145\n",
      "Epoch 15978/30000 Training Loss: 0.057317737489938736\n",
      "Epoch 15979/30000 Training Loss: 0.04783089458942413\n",
      "Epoch 15980/30000 Training Loss: 0.046494364738464355\n",
      "Epoch 15981/30000 Training Loss: 0.0440860241651535\n",
      "Epoch 15982/30000 Training Loss: 0.04662822559475899\n",
      "Epoch 15983/30000 Training Loss: 0.05303158238530159\n",
      "Epoch 15984/30000 Training Loss: 0.05921956151723862\n",
      "Epoch 15985/30000 Training Loss: 0.04910355806350708\n",
      "Epoch 15986/30000 Training Loss: 0.0524040088057518\n",
      "Epoch 15987/30000 Training Loss: 0.05233146995306015\n",
      "Epoch 15988/30000 Training Loss: 0.03481507673859596\n",
      "Epoch 15989/30000 Training Loss: 0.043344926089048386\n",
      "Epoch 15990/30000 Training Loss: 0.048906147480010986\n",
      "Epoch 15991/30000 Training Loss: 0.03233692795038223\n",
      "Epoch 15992/30000 Training Loss: 0.05272352322936058\n",
      "Epoch 15993/30000 Training Loss: 0.05988939106464386\n",
      "Epoch 15994/30000 Training Loss: 0.0555918924510479\n",
      "Epoch 15995/30000 Training Loss: 0.053702544420957565\n",
      "Epoch 15996/30000 Training Loss: 0.048637911677360535\n",
      "Epoch 15997/30000 Training Loss: 0.05983199179172516\n",
      "Epoch 15998/30000 Training Loss: 0.04767090827226639\n",
      "Epoch 15999/30000 Training Loss: 0.047407831996679306\n",
      "Epoch 16000/30000 Training Loss: 0.07427413016557693\n",
      "Epoch 16000/30000 Validation Loss: 0.057084303349256516\n",
      "Epoch 16001/30000 Training Loss: 0.041745517402887344\n",
      "Epoch 16002/30000 Training Loss: 0.041100114583969116\n",
      "Epoch 16003/30000 Training Loss: 0.04608999565243721\n",
      "Epoch 16004/30000 Training Loss: 0.04979655146598816\n",
      "Epoch 16005/30000 Training Loss: 0.044336602091789246\n",
      "Epoch 16006/30000 Training Loss: 0.04098443314433098\n",
      "Epoch 16007/30000 Training Loss: 0.032222263514995575\n",
      "Epoch 16008/30000 Training Loss: 0.04689977318048477\n",
      "Epoch 16009/30000 Training Loss: 0.043625641614198685\n",
      "Epoch 16010/30000 Training Loss: 0.05197155103087425\n",
      "Epoch 16011/30000 Training Loss: 0.05723010003566742\n",
      "Epoch 16012/30000 Training Loss: 0.04268505424261093\n",
      "Epoch 16013/30000 Training Loss: 0.04042983800172806\n",
      "Epoch 16014/30000 Training Loss: 0.05197147652506828\n",
      "Epoch 16015/30000 Training Loss: 0.05047246441245079\n",
      "Epoch 16016/30000 Training Loss: 0.04130237177014351\n",
      "Epoch 16017/30000 Training Loss: 0.050894320011138916\n",
      "Epoch 16018/30000 Training Loss: 0.048374567180871964\n",
      "Epoch 16019/30000 Training Loss: 0.05055234581232071\n",
      "Epoch 16020/30000 Training Loss: 0.053993433713912964\n",
      "Epoch 16021/30000 Training Loss: 0.05705532804131508\n",
      "Epoch 16022/30000 Training Loss: 0.0471719466149807\n",
      "Epoch 16023/30000 Training Loss: 0.05149677023291588\n",
      "Epoch 16024/30000 Training Loss: 0.04416167736053467\n",
      "Epoch 16025/30000 Training Loss: 0.0523708239197731\n",
      "Epoch 16026/30000 Training Loss: 0.055337607860565186\n",
      "Epoch 16027/30000 Training Loss: 0.04070601612329483\n",
      "Epoch 16028/30000 Training Loss: 0.04629664868116379\n",
      "Epoch 16029/30000 Training Loss: 0.060520876199007034\n",
      "Epoch 16030/30000 Training Loss: 0.04236246645450592\n",
      "Epoch 16031/30000 Training Loss: 0.048103217035532\n",
      "Epoch 16032/30000 Training Loss: 0.044099774211645126\n",
      "Epoch 16033/30000 Training Loss: 0.047540221363306046\n",
      "Epoch 16034/30000 Training Loss: 0.05700297653675079\n",
      "Epoch 16035/30000 Training Loss: 0.04835967347025871\n",
      "Epoch 16036/30000 Training Loss: 0.036489591002464294\n",
      "Epoch 16037/30000 Training Loss: 0.03720121085643768\n",
      "Epoch 16038/30000 Training Loss: 0.044066932052373886\n",
      "Epoch 16039/30000 Training Loss: 0.03879079967737198\n",
      "Epoch 16040/30000 Training Loss: 0.05906186252832413\n",
      "Epoch 16041/30000 Training Loss: 0.0515458919107914\n",
      "Epoch 16042/30000 Training Loss: 0.04711233451962471\n",
      "Epoch 16043/30000 Training Loss: 0.07055602222681046\n",
      "Epoch 16044/30000 Training Loss: 0.05379953607916832\n",
      "Epoch 16045/30000 Training Loss: 0.05572967976331711\n",
      "Epoch 16046/30000 Training Loss: 0.06939046829938889\n",
      "Epoch 16047/30000 Training Loss: 0.06126682832837105\n",
      "Epoch 16048/30000 Training Loss: 0.04987933486700058\n",
      "Epoch 16049/30000 Training Loss: 0.0375189408659935\n",
      "Epoch 16050/30000 Training Loss: 0.05855462700128555\n",
      "Epoch 16051/30000 Training Loss: 0.050116345286369324\n",
      "Epoch 16052/30000 Training Loss: 0.052610937505960464\n",
      "Epoch 16053/30000 Training Loss: 0.039481934159994125\n",
      "Epoch 16054/30000 Training Loss: 0.05670550465583801\n",
      "Epoch 16055/30000 Training Loss: 0.05979882925748825\n",
      "Epoch 16056/30000 Training Loss: 0.05138597637414932\n",
      "Epoch 16057/30000 Training Loss: 0.045319266617298126\n",
      "Epoch 16058/30000 Training Loss: 0.03693583607673645\n",
      "Epoch 16059/30000 Training Loss: 0.04744938015937805\n",
      "Epoch 16060/30000 Training Loss: 0.05630204826593399\n",
      "Epoch 16061/30000 Training Loss: 0.05914200842380524\n",
      "Epoch 16062/30000 Training Loss: 0.05763285607099533\n",
      "Epoch 16063/30000 Training Loss: 0.03636999800801277\n",
      "Epoch 16064/30000 Training Loss: 0.054913148283958435\n",
      "Epoch 16065/30000 Training Loss: 0.04648745805025101\n",
      "Epoch 16066/30000 Training Loss: 0.047375403344631195\n",
      "Epoch 16067/30000 Training Loss: 0.061790939420461655\n",
      "Epoch 16068/30000 Training Loss: 0.05096670612692833\n",
      "Epoch 16069/30000 Training Loss: 0.060935936868190765\n",
      "Epoch 16070/30000 Training Loss: 0.056372642517089844\n",
      "Epoch 16071/30000 Training Loss: 0.035527657717466354\n",
      "Epoch 16072/30000 Training Loss: 0.056202542036771774\n",
      "Epoch 16073/30000 Training Loss: 0.048668380826711655\n",
      "Epoch 16074/30000 Training Loss: 0.04347408935427666\n",
      "Epoch 16075/30000 Training Loss: 0.04207272455096245\n",
      "Epoch 16076/30000 Training Loss: 0.05035257339477539\n",
      "Epoch 16077/30000 Training Loss: 0.05017127841711044\n",
      "Epoch 16078/30000 Training Loss: 0.04640519991517067\n",
      "Epoch 16079/30000 Training Loss: 0.058166272938251495\n",
      "Epoch 16080/30000 Training Loss: 0.05209197476506233\n",
      "Epoch 16081/30000 Training Loss: 0.049255531281232834\n",
      "Epoch 16082/30000 Training Loss: 0.05565119907259941\n",
      "Epoch 16083/30000 Training Loss: 0.039214760065078735\n",
      "Epoch 16084/30000 Training Loss: 0.036602482199668884\n",
      "Epoch 16085/30000 Training Loss: 0.0534384660422802\n",
      "Epoch 16086/30000 Training Loss: 0.041717369109392166\n",
      "Epoch 16087/30000 Training Loss: 0.056456558406353\n",
      "Epoch 16088/30000 Training Loss: 0.0500972718000412\n",
      "Epoch 16089/30000 Training Loss: 0.04082660377025604\n",
      "Epoch 16090/30000 Training Loss: 0.056894928216934204\n",
      "Epoch 16091/30000 Training Loss: 0.05484285205602646\n",
      "Epoch 16092/30000 Training Loss: 0.04695022106170654\n",
      "Epoch 16093/30000 Training Loss: 0.05180133879184723\n",
      "Epoch 16094/30000 Training Loss: 0.036896709352731705\n",
      "Epoch 16095/30000 Training Loss: 0.058821387588977814\n",
      "Epoch 16096/30000 Training Loss: 0.06486380845308304\n",
      "Epoch 16097/30000 Training Loss: 0.05560895428061485\n",
      "Epoch 16098/30000 Training Loss: 0.037323132157325745\n",
      "Epoch 16099/30000 Training Loss: 0.03296910971403122\n",
      "Epoch 16100/30000 Training Loss: 0.050927918404340744\n",
      "Epoch 16100/30000 Validation Loss: 0.052694205194711685\n",
      "Epoch 16101/30000 Training Loss: 0.03783712536096573\n",
      "Epoch 16102/30000 Training Loss: 0.04903937503695488\n",
      "Epoch 16103/30000 Training Loss: 0.04610655456781387\n",
      "Epoch 16104/30000 Training Loss: 0.0417313352227211\n",
      "Epoch 16105/30000 Training Loss: 0.048757027834653854\n",
      "Epoch 16106/30000 Training Loss: 0.0522766150534153\n",
      "Epoch 16107/30000 Training Loss: 0.057740550488233566\n",
      "Epoch 16108/30000 Training Loss: 0.033289190381765366\n",
      "Epoch 16109/30000 Training Loss: 0.05055392533540726\n",
      "Epoch 16110/30000 Training Loss: 0.04629182815551758\n",
      "Epoch 16111/30000 Training Loss: 0.05350816622376442\n",
      "Epoch 16112/30000 Training Loss: 0.040417399257421494\n",
      "Epoch 16113/30000 Training Loss: 0.04890560731291771\n",
      "Epoch 16114/30000 Training Loss: 0.04717375710606575\n",
      "Epoch 16115/30000 Training Loss: 0.050814490765333176\n",
      "Epoch 16116/30000 Training Loss: 0.05337686091661453\n",
      "Epoch 16117/30000 Training Loss: 0.04923425614833832\n",
      "Epoch 16118/30000 Training Loss: 0.059927862137556076\n",
      "Epoch 16119/30000 Training Loss: 0.04233744740486145\n",
      "Epoch 16120/30000 Training Loss: 0.0636613667011261\n",
      "Epoch 16121/30000 Training Loss: 0.04641805216670036\n",
      "Epoch 16122/30000 Training Loss: 0.04923985153436661\n",
      "Epoch 16123/30000 Training Loss: 0.03518209233880043\n",
      "Epoch 16124/30000 Training Loss: 0.04566739499568939\n",
      "Epoch 16125/30000 Training Loss: 0.03929302468895912\n",
      "Epoch 16126/30000 Training Loss: 0.05867169797420502\n",
      "Epoch 16127/30000 Training Loss: 0.054855864495038986\n",
      "Epoch 16128/30000 Training Loss: 0.05729072540998459\n",
      "Epoch 16129/30000 Training Loss: 0.055951982736587524\n",
      "Epoch 16130/30000 Training Loss: 0.049663152545690536\n",
      "Epoch 16131/30000 Training Loss: 0.04688103869557381\n",
      "Epoch 16132/30000 Training Loss: 0.05296451598405838\n",
      "Epoch 16133/30000 Training Loss: 0.06293484568595886\n",
      "Epoch 16134/30000 Training Loss: 0.05908585339784622\n",
      "Epoch 16135/30000 Training Loss: 0.03316384553909302\n",
      "Epoch 16136/30000 Training Loss: 0.03519798070192337\n",
      "Epoch 16137/30000 Training Loss: 0.053822826594114304\n",
      "Epoch 16138/30000 Training Loss: 0.044736750423908234\n",
      "Epoch 16139/30000 Training Loss: 0.052122488617897034\n",
      "Epoch 16140/30000 Training Loss: 0.04965599626302719\n",
      "Epoch 16141/30000 Training Loss: 0.03747319057583809\n",
      "Epoch 16142/30000 Training Loss: 0.03307808190584183\n",
      "Epoch 16143/30000 Training Loss: 0.0677640438079834\n",
      "Epoch 16144/30000 Training Loss: 0.03664630651473999\n",
      "Epoch 16145/30000 Training Loss: 0.044822968542575836\n",
      "Epoch 16146/30000 Training Loss: 0.04468755051493645\n",
      "Epoch 16147/30000 Training Loss: 0.04194338247179985\n",
      "Epoch 16148/30000 Training Loss: 0.05816289782524109\n",
      "Epoch 16149/30000 Training Loss: 0.054978929460048676\n",
      "Epoch 16150/30000 Training Loss: 0.047108251601457596\n",
      "Epoch 16151/30000 Training Loss: 0.048311132937669754\n",
      "Epoch 16152/30000 Training Loss: 0.06885170936584473\n",
      "Epoch 16153/30000 Training Loss: 0.0499546155333519\n",
      "Epoch 16154/30000 Training Loss: 0.05067146569490433\n",
      "Epoch 16155/30000 Training Loss: 0.07484503090381622\n",
      "Epoch 16156/30000 Training Loss: 0.06156211346387863\n",
      "Epoch 16157/30000 Training Loss: 0.051161520183086395\n",
      "Epoch 16158/30000 Training Loss: 0.0435749776661396\n",
      "Epoch 16159/30000 Training Loss: 0.03799637407064438\n",
      "Epoch 16160/30000 Training Loss: 0.07198414206504822\n",
      "Epoch 16161/30000 Training Loss: 0.047622717916965485\n",
      "Epoch 16162/30000 Training Loss: 0.05130954459309578\n",
      "Epoch 16163/30000 Training Loss: 0.05016402155160904\n",
      "Epoch 16164/30000 Training Loss: 0.05190545320510864\n",
      "Epoch 16165/30000 Training Loss: 0.0693630799651146\n",
      "Epoch 16166/30000 Training Loss: 0.06008562445640564\n",
      "Epoch 16167/30000 Training Loss: 0.04677180200815201\n",
      "Epoch 16168/30000 Training Loss: 0.06996116042137146\n",
      "Epoch 16169/30000 Training Loss: 0.05519159510731697\n",
      "Epoch 16170/30000 Training Loss: 0.05185912549495697\n",
      "Epoch 16171/30000 Training Loss: 0.05227353796362877\n",
      "Epoch 16172/30000 Training Loss: 0.0569109171628952\n",
      "Epoch 16173/30000 Training Loss: 0.04525343328714371\n",
      "Epoch 16174/30000 Training Loss: 0.05789351090788841\n",
      "Epoch 16175/30000 Training Loss: 0.0504799410700798\n",
      "Epoch 16176/30000 Training Loss: 0.04240243136882782\n",
      "Epoch 16177/30000 Training Loss: 0.05401207506656647\n",
      "Epoch 16178/30000 Training Loss: 0.0461777001619339\n",
      "Epoch 16179/30000 Training Loss: 0.03987496346235275\n",
      "Epoch 16180/30000 Training Loss: 0.03960806876420975\n",
      "Epoch 16181/30000 Training Loss: 0.0396009124815464\n",
      "Epoch 16182/30000 Training Loss: 0.04771772027015686\n",
      "Epoch 16183/30000 Training Loss: 0.04392414912581444\n",
      "Epoch 16184/30000 Training Loss: 0.05913073197007179\n",
      "Epoch 16185/30000 Training Loss: 0.03625620901584625\n",
      "Epoch 16186/30000 Training Loss: 0.05157608538866043\n",
      "Epoch 16187/30000 Training Loss: 0.07046379894018173\n",
      "Epoch 16188/30000 Training Loss: 0.04519592970609665\n",
      "Epoch 16189/30000 Training Loss: 0.05763843283057213\n",
      "Epoch 16190/30000 Training Loss: 0.04012548178434372\n",
      "Epoch 16191/30000 Training Loss: 0.048820819705724716\n",
      "Epoch 16192/30000 Training Loss: 0.06789315491914749\n",
      "Epoch 16193/30000 Training Loss: 0.052191682159900665\n",
      "Epoch 16194/30000 Training Loss: 0.04936530813574791\n",
      "Epoch 16195/30000 Training Loss: 0.04131364822387695\n",
      "Epoch 16196/30000 Training Loss: 0.04937119781970978\n",
      "Epoch 16197/30000 Training Loss: 0.037217192351818085\n",
      "Epoch 16198/30000 Training Loss: 0.04477638006210327\n",
      "Epoch 16199/30000 Training Loss: 0.05273795127868652\n",
      "Epoch 16200/30000 Training Loss: 0.03916309401392937\n",
      "Epoch 16200/30000 Validation Loss: 0.05232344567775726\n",
      "Epoch 16201/30000 Training Loss: 0.04952220618724823\n",
      "Epoch 16202/30000 Training Loss: 0.045247167348861694\n",
      "Epoch 16203/30000 Training Loss: 0.07223585247993469\n",
      "Epoch 16204/30000 Training Loss: 0.04494457691907883\n",
      "Epoch 16205/30000 Training Loss: 0.058738529682159424\n",
      "Epoch 16206/30000 Training Loss: 0.04121822863817215\n",
      "Epoch 16207/30000 Training Loss: 0.047832190990448\n",
      "Epoch 16208/30000 Training Loss: 0.06184905022382736\n",
      "Epoch 16209/30000 Training Loss: 0.04609889164566994\n",
      "Epoch 16210/30000 Training Loss: 0.051803283393383026\n",
      "Epoch 16211/30000 Training Loss: 0.05269641801714897\n",
      "Epoch 16212/30000 Training Loss: 0.05000985041260719\n",
      "Epoch 16213/30000 Training Loss: 0.05592139810323715\n",
      "Epoch 16214/30000 Training Loss: 0.04007544741034508\n",
      "Epoch 16215/30000 Training Loss: 0.0488208569586277\n",
      "Epoch 16216/30000 Training Loss: 0.044150613248348236\n",
      "Epoch 16217/30000 Training Loss: 0.04257477447390556\n",
      "Epoch 16218/30000 Training Loss: 0.04170603305101395\n",
      "Epoch 16219/30000 Training Loss: 0.061501771211624146\n",
      "Epoch 16220/30000 Training Loss: 0.05079454183578491\n",
      "Epoch 16221/30000 Training Loss: 0.038380347192287445\n",
      "Epoch 16222/30000 Training Loss: 0.04139163717627525\n",
      "Epoch 16223/30000 Training Loss: 0.043132487684488297\n",
      "Epoch 16224/30000 Training Loss: 0.04846818372607231\n",
      "Epoch 16225/30000 Training Loss: 0.04785844683647156\n",
      "Epoch 16226/30000 Training Loss: 0.039010290056467056\n",
      "Epoch 16227/30000 Training Loss: 0.042613495141267776\n",
      "Epoch 16228/30000 Training Loss: 0.042065005749464035\n",
      "Epoch 16229/30000 Training Loss: 0.04445569962263107\n",
      "Epoch 16230/30000 Training Loss: 0.04134596139192581\n",
      "Epoch 16231/30000 Training Loss: 0.044374942779541016\n",
      "Epoch 16232/30000 Training Loss: 0.05709568411111832\n",
      "Epoch 16233/30000 Training Loss: 0.05279051512479782\n",
      "Epoch 16234/30000 Training Loss: 0.058152973651885986\n",
      "Epoch 16235/30000 Training Loss: 0.05890358239412308\n",
      "Epoch 16236/30000 Training Loss: 0.050374194979667664\n",
      "Epoch 16237/30000 Training Loss: 0.04056548699736595\n",
      "Epoch 16238/30000 Training Loss: 0.046602047979831696\n",
      "Epoch 16239/30000 Training Loss: 0.06686829775571823\n",
      "Epoch 16240/30000 Training Loss: 0.040892668068408966\n",
      "Epoch 16241/30000 Training Loss: 0.046410322189331055\n",
      "Epoch 16242/30000 Training Loss: 0.040564872324466705\n",
      "Epoch 16243/30000 Training Loss: 0.04617958515882492\n",
      "Epoch 16244/30000 Training Loss: 0.05813087150454521\n",
      "Epoch 16245/30000 Training Loss: 0.05069294199347496\n",
      "Epoch 16246/30000 Training Loss: 0.03669588640332222\n",
      "Epoch 16247/30000 Training Loss: 0.05139259248971939\n",
      "Epoch 16248/30000 Training Loss: 0.0505681149661541\n",
      "Epoch 16249/30000 Training Loss: 0.04380545765161514\n",
      "Epoch 16250/30000 Training Loss: 0.04872202128171921\n",
      "Epoch 16251/30000 Training Loss: 0.05362442880868912\n",
      "Epoch 16252/30000 Training Loss: 0.0635775625705719\n",
      "Epoch 16253/30000 Training Loss: 0.036937519907951355\n",
      "Epoch 16254/30000 Training Loss: 0.056621067225933075\n",
      "Epoch 16255/30000 Training Loss: 0.05181049555540085\n",
      "Epoch 16256/30000 Training Loss: 0.06722646951675415\n",
      "Epoch 16257/30000 Training Loss: 0.057580865919589996\n",
      "Epoch 16258/30000 Training Loss: 0.04100653529167175\n",
      "Epoch 16259/30000 Training Loss: 0.03643340244889259\n",
      "Epoch 16260/30000 Training Loss: 0.04117973521351814\n",
      "Epoch 16261/30000 Training Loss: 0.04015037417411804\n",
      "Epoch 16262/30000 Training Loss: 0.039047855883836746\n",
      "Epoch 16263/30000 Training Loss: 0.06367679685354233\n",
      "Epoch 16264/30000 Training Loss: 0.05112089961767197\n",
      "Epoch 16265/30000 Training Loss: 0.04386642202734947\n",
      "Epoch 16266/30000 Training Loss: 0.05369097366929054\n",
      "Epoch 16267/30000 Training Loss: 0.0590185672044754\n",
      "Epoch 16268/30000 Training Loss: 0.038862235844135284\n",
      "Epoch 16269/30000 Training Loss: 0.07392728328704834\n",
      "Epoch 16270/30000 Training Loss: 0.040915597230196\n",
      "Epoch 16271/30000 Training Loss: 0.06449391692876816\n",
      "Epoch 16272/30000 Training Loss: 0.04785697162151337\n",
      "Epoch 16273/30000 Training Loss: 0.032878465950489044\n",
      "Epoch 16274/30000 Training Loss: 0.04561211168766022\n",
      "Epoch 16275/30000 Training Loss: 0.05085746571421623\n",
      "Epoch 16276/30000 Training Loss: 0.032696548849344254\n",
      "Epoch 16277/30000 Training Loss: 0.030259374529123306\n",
      "Epoch 16278/30000 Training Loss: 0.050243787467479706\n",
      "Epoch 16279/30000 Training Loss: 0.054553624242544174\n",
      "Epoch 16280/30000 Training Loss: 0.058358702808618546\n",
      "Epoch 16281/30000 Training Loss: 0.03604600578546524\n",
      "Epoch 16282/30000 Training Loss: 0.05108567699790001\n",
      "Epoch 16283/30000 Training Loss: 0.035620588809251785\n",
      "Epoch 16284/30000 Training Loss: 0.05691053718328476\n",
      "Epoch 16285/30000 Training Loss: 0.061558447778224945\n",
      "Epoch 16286/30000 Training Loss: 0.053046196699142456\n",
      "Epoch 16287/30000 Training Loss: 0.054176852107048035\n",
      "Epoch 16288/30000 Training Loss: 0.04682915657758713\n",
      "Epoch 16289/30000 Training Loss: 0.05269934982061386\n",
      "Epoch 16290/30000 Training Loss: 0.04729755222797394\n",
      "Epoch 16291/30000 Training Loss: 0.05177924409508705\n",
      "Epoch 16292/30000 Training Loss: 0.0441235676407814\n",
      "Epoch 16293/30000 Training Loss: 0.03720327466726303\n",
      "Epoch 16294/30000 Training Loss: 0.058260053396224976\n",
      "Epoch 16295/30000 Training Loss: 0.05215570703148842\n",
      "Epoch 16296/30000 Training Loss: 0.055283550173044205\n",
      "Epoch 16297/30000 Training Loss: 0.04222335293889046\n",
      "Epoch 16298/30000 Training Loss: 0.04019731283187866\n",
      "Epoch 16299/30000 Training Loss: 0.04178397357463837\n",
      "Epoch 16300/30000 Training Loss: 0.0418073907494545\n",
      "Epoch 16300/30000 Validation Loss: 0.04466431215405464\n",
      "Epoch 16301/30000 Training Loss: 0.03796001896262169\n",
      "Epoch 16302/30000 Training Loss: 0.05008697137236595\n",
      "Epoch 16303/30000 Training Loss: 0.07694900035858154\n",
      "Epoch 16304/30000 Training Loss: 0.050089508295059204\n",
      "Epoch 16305/30000 Training Loss: 0.054607145488262177\n",
      "Epoch 16306/30000 Training Loss: 0.05516213923692703\n",
      "Epoch 16307/30000 Training Loss: 0.04540374130010605\n",
      "Epoch 16308/30000 Training Loss: 0.05494816601276398\n",
      "Epoch 16309/30000 Training Loss: 0.04414232820272446\n",
      "Epoch 16310/30000 Training Loss: 0.051959890872240067\n",
      "Epoch 16311/30000 Training Loss: 0.06621115654706955\n",
      "Epoch 16312/30000 Training Loss: 0.06121014058589935\n",
      "Epoch 16313/30000 Training Loss: 0.046611011028289795\n",
      "Epoch 16314/30000 Training Loss: 0.04404912889003754\n",
      "Epoch 16315/30000 Training Loss: 0.05148082971572876\n",
      "Epoch 16316/30000 Training Loss: 0.04982324689626694\n",
      "Epoch 16317/30000 Training Loss: 0.04482182860374451\n",
      "Epoch 16318/30000 Training Loss: 0.05780565366148949\n",
      "Epoch 16319/30000 Training Loss: 0.04148112237453461\n",
      "Epoch 16320/30000 Training Loss: 0.056971389800310135\n",
      "Epoch 16321/30000 Training Loss: 0.03606238588690758\n",
      "Epoch 16322/30000 Training Loss: 0.038338836282491684\n",
      "Epoch 16323/30000 Training Loss: 0.036910250782966614\n",
      "Epoch 16324/30000 Training Loss: 0.07238344103097916\n",
      "Epoch 16325/30000 Training Loss: 0.04507947713136673\n",
      "Epoch 16326/30000 Training Loss: 0.04688037186861038\n",
      "Epoch 16327/30000 Training Loss: 0.052304647862911224\n",
      "Epoch 16328/30000 Training Loss: 0.05490957200527191\n",
      "Epoch 16329/30000 Training Loss: 0.04615841060876846\n",
      "Epoch 16330/30000 Training Loss: 0.04122022166848183\n",
      "Epoch 16331/30000 Training Loss: 0.048299845308065414\n",
      "Epoch 16332/30000 Training Loss: 0.05320444330573082\n",
      "Epoch 16333/30000 Training Loss: 0.04032428190112114\n",
      "Epoch 16334/30000 Training Loss: 0.051975611597299576\n",
      "Epoch 16335/30000 Training Loss: 0.045615747570991516\n",
      "Epoch 16336/30000 Training Loss: 0.05382406339049339\n",
      "Epoch 16337/30000 Training Loss: 0.0521746501326561\n",
      "Epoch 16338/30000 Training Loss: 0.034546926617622375\n",
      "Epoch 16339/30000 Training Loss: 0.027970556169748306\n",
      "Epoch 16340/30000 Training Loss: 0.038619089871644974\n",
      "Epoch 16341/30000 Training Loss: 0.055539555847644806\n",
      "Epoch 16342/30000 Training Loss: 0.03509938716888428\n",
      "Epoch 16343/30000 Training Loss: 0.04571641981601715\n",
      "Epoch 16344/30000 Training Loss: 0.049591951072216034\n",
      "Epoch 16345/30000 Training Loss: 0.06321672350168228\n",
      "Epoch 16346/30000 Training Loss: 0.044535838067531586\n",
      "Epoch 16347/30000 Training Loss: 0.031150616705417633\n",
      "Epoch 16348/30000 Training Loss: 0.057676512748003006\n",
      "Epoch 16349/30000 Training Loss: 0.0563165545463562\n",
      "Epoch 16350/30000 Training Loss: 0.06688985228538513\n",
      "Epoch 16351/30000 Training Loss: 0.04328187555074692\n",
      "Epoch 16352/30000 Training Loss: 0.04604482278227806\n",
      "Epoch 16353/30000 Training Loss: 0.0482005849480629\n",
      "Epoch 16354/30000 Training Loss: 0.03880951926112175\n",
      "Epoch 16355/30000 Training Loss: 0.05087033286690712\n",
      "Epoch 16356/30000 Training Loss: 0.04337892681360245\n",
      "Epoch 16357/30000 Training Loss: 0.05683128535747528\n",
      "Epoch 16358/30000 Training Loss: 0.03716783598065376\n",
      "Epoch 16359/30000 Training Loss: 0.040073178708553314\n",
      "Epoch 16360/30000 Training Loss: 0.05028335005044937\n",
      "Epoch 16361/30000 Training Loss: 0.03899143263697624\n",
      "Epoch 16362/30000 Training Loss: 0.04771680384874344\n",
      "Epoch 16363/30000 Training Loss: 0.04799124598503113\n",
      "Epoch 16364/30000 Training Loss: 0.03762918710708618\n",
      "Epoch 16365/30000 Training Loss: 0.034965887665748596\n",
      "Epoch 16366/30000 Training Loss: 0.039957594126462936\n",
      "Epoch 16367/30000 Training Loss: 0.06096671149134636\n",
      "Epoch 16368/30000 Training Loss: 0.050986602902412415\n",
      "Epoch 16369/30000 Training Loss: 0.04184092581272125\n",
      "Epoch 16370/30000 Training Loss: 0.04647402465343475\n",
      "Epoch 16371/30000 Training Loss: 0.065520741045475\n",
      "Epoch 16372/30000 Training Loss: 0.044773168861866\n",
      "Epoch 16373/30000 Training Loss: 0.042787354439496994\n",
      "Epoch 16374/30000 Training Loss: 0.03335591033101082\n",
      "Epoch 16375/30000 Training Loss: 0.041789546608924866\n",
      "Epoch 16376/30000 Training Loss: 0.0742865726351738\n",
      "Epoch 16377/30000 Training Loss: 0.029222525656223297\n",
      "Epoch 16378/30000 Training Loss: 0.04141682758927345\n",
      "Epoch 16379/30000 Training Loss: 0.046990081667900085\n",
      "Epoch 16380/30000 Training Loss: 0.04725642502307892\n",
      "Epoch 16381/30000 Training Loss: 0.057185590267181396\n",
      "Epoch 16382/30000 Training Loss: 0.039094436913728714\n",
      "Epoch 16383/30000 Training Loss: 0.059052806347608566\n",
      "Epoch 16384/30000 Training Loss: 0.043420854955911636\n",
      "Epoch 16385/30000 Training Loss: 0.038521651178598404\n",
      "Epoch 16386/30000 Training Loss: 0.04192566126585007\n",
      "Epoch 16387/30000 Training Loss: 0.03526952490210533\n",
      "Epoch 16388/30000 Training Loss: 0.056885845959186554\n",
      "Epoch 16389/30000 Training Loss: 0.04382764548063278\n",
      "Epoch 16390/30000 Training Loss: 0.040848828852176666\n",
      "Epoch 16391/30000 Training Loss: 0.03787882998585701\n",
      "Epoch 16392/30000 Training Loss: 0.05536486208438873\n",
      "Epoch 16393/30000 Training Loss: 0.055627692490816116\n",
      "Epoch 16394/30000 Training Loss: 0.04512370377779007\n",
      "Epoch 16395/30000 Training Loss: 0.04743881896138191\n",
      "Epoch 16396/30000 Training Loss: 0.045404039323329926\n",
      "Epoch 16397/30000 Training Loss: 0.04109089449048042\n",
      "Epoch 16398/30000 Training Loss: 0.04171224310994148\n",
      "Epoch 16399/30000 Training Loss: 0.04245925322175026\n",
      "Epoch 16400/30000 Training Loss: 0.04115656763315201\n",
      "Epoch 16400/30000 Validation Loss: 0.052529461681842804\n",
      "Epoch 16401/30000 Training Loss: 0.062315963208675385\n",
      "Epoch 16402/30000 Training Loss: 0.045626454055309296\n",
      "Epoch 16403/30000 Training Loss: 0.054552432149648666\n",
      "Epoch 16404/30000 Training Loss: 0.04835239797830582\n",
      "Epoch 16405/30000 Training Loss: 0.04184960201382637\n",
      "Epoch 16406/30000 Training Loss: 0.06545939296483994\n",
      "Epoch 16407/30000 Training Loss: 0.07069474458694458\n",
      "Epoch 16408/30000 Training Loss: 0.05949509143829346\n",
      "Epoch 16409/30000 Training Loss: 0.05395941063761711\n",
      "Epoch 16410/30000 Training Loss: 0.054398201406002045\n",
      "Epoch 16411/30000 Training Loss: 0.04734009504318237\n",
      "Epoch 16412/30000 Training Loss: 0.06255930662155151\n",
      "Epoch 16413/30000 Training Loss: 0.0363619327545166\n",
      "Epoch 16414/30000 Training Loss: 0.03906232863664627\n",
      "Epoch 16415/30000 Training Loss: 0.047678299248218536\n",
      "Epoch 16416/30000 Training Loss: 0.05314551666378975\n",
      "Epoch 16417/30000 Training Loss: 0.05431271344423294\n",
      "Epoch 16418/30000 Training Loss: 0.051396049559116364\n",
      "Epoch 16419/30000 Training Loss: 0.050512202084064484\n",
      "Epoch 16420/30000 Training Loss: 0.03967813029885292\n",
      "Epoch 16421/30000 Training Loss: 0.038343098014593124\n",
      "Epoch 16422/30000 Training Loss: 0.04934016615152359\n",
      "Epoch 16423/30000 Training Loss: 0.06349103897809982\n",
      "Epoch 16424/30000 Training Loss: 0.04635799676179886\n",
      "Epoch 16425/30000 Training Loss: 0.04493354633450508\n",
      "Epoch 16426/30000 Training Loss: 0.04693097993731499\n",
      "Epoch 16427/30000 Training Loss: 0.04763595014810562\n",
      "Epoch 16428/30000 Training Loss: 0.052038561552762985\n",
      "Epoch 16429/30000 Training Loss: 0.05688805133104324\n",
      "Epoch 16430/30000 Training Loss: 0.03669515997171402\n",
      "Epoch 16431/30000 Training Loss: 0.03627540171146393\n",
      "Epoch 16432/30000 Training Loss: 0.05856386572122574\n",
      "Epoch 16433/30000 Training Loss: 0.06618905067443848\n",
      "Epoch 16434/30000 Training Loss: 0.03737258538603783\n",
      "Epoch 16435/30000 Training Loss: 0.04595162346959114\n",
      "Epoch 16436/30000 Training Loss: 0.04737133905291557\n",
      "Epoch 16437/30000 Training Loss: 0.057401254773139954\n",
      "Epoch 16438/30000 Training Loss: 0.04648873209953308\n",
      "Epoch 16439/30000 Training Loss: 0.04372603818774223\n",
      "Epoch 16440/30000 Training Loss: 0.04543006792664528\n",
      "Epoch 16441/30000 Training Loss: 0.04360438510775566\n",
      "Epoch 16442/30000 Training Loss: 0.04143347218632698\n",
      "Epoch 16443/30000 Training Loss: 0.04913344979286194\n",
      "Epoch 16444/30000 Training Loss: 0.04896916449069977\n",
      "Epoch 16445/30000 Training Loss: 0.056828320026397705\n",
      "Epoch 16446/30000 Training Loss: 0.05360042303800583\n",
      "Epoch 16447/30000 Training Loss: 0.04442552104592323\n",
      "Epoch 16448/30000 Training Loss: 0.04286603629589081\n",
      "Epoch 16449/30000 Training Loss: 0.05691596865653992\n",
      "Epoch 16450/30000 Training Loss: 0.06728602945804596\n",
      "Epoch 16451/30000 Training Loss: 0.06101042777299881\n",
      "Epoch 16452/30000 Training Loss: 0.0567278191447258\n",
      "Epoch 16453/30000 Training Loss: 0.06342876702547073\n",
      "Epoch 16454/30000 Training Loss: 0.04445865377783775\n",
      "Epoch 16455/30000 Training Loss: 0.06532508879899979\n",
      "Epoch 16456/30000 Training Loss: 0.04939354956150055\n",
      "Epoch 16457/30000 Training Loss: 0.04181718826293945\n",
      "Epoch 16458/30000 Training Loss: 0.05325588956475258\n",
      "Epoch 16459/30000 Training Loss: 0.04691804200410843\n",
      "Epoch 16460/30000 Training Loss: 0.03782161697745323\n",
      "Epoch 16461/30000 Training Loss: 0.06348507106304169\n",
      "Epoch 16462/30000 Training Loss: 0.06598655134439468\n",
      "Epoch 16463/30000 Training Loss: 0.033442940562963486\n",
      "Epoch 16464/30000 Training Loss: 0.05105040594935417\n",
      "Epoch 16465/30000 Training Loss: 0.061445608735084534\n",
      "Epoch 16466/30000 Training Loss: 0.055092763155698776\n",
      "Epoch 16467/30000 Training Loss: 0.037848375737667084\n",
      "Epoch 16468/30000 Training Loss: 0.0378599613904953\n",
      "Epoch 16469/30000 Training Loss: 0.04992315173149109\n",
      "Epoch 16470/30000 Training Loss: 0.04905269294977188\n",
      "Epoch 16471/30000 Training Loss: 0.07139420509338379\n",
      "Epoch 16472/30000 Training Loss: 0.04227814823389053\n",
      "Epoch 16473/30000 Training Loss: 0.06245777755975723\n",
      "Epoch 16474/30000 Training Loss: 0.05808331072330475\n",
      "Epoch 16475/30000 Training Loss: 0.04507964104413986\n",
      "Epoch 16476/30000 Training Loss: 0.04489940032362938\n",
      "Epoch 16477/30000 Training Loss: 0.05761956796050072\n",
      "Epoch 16478/30000 Training Loss: 0.043060556054115295\n",
      "Epoch 16479/30000 Training Loss: 0.05501318350434303\n",
      "Epoch 16480/30000 Training Loss: 0.04306184500455856\n",
      "Epoch 16481/30000 Training Loss: 0.051648885011672974\n",
      "Epoch 16482/30000 Training Loss: 0.06742233783006668\n",
      "Epoch 16483/30000 Training Loss: 0.04859105497598648\n",
      "Epoch 16484/30000 Training Loss: 0.052436843514442444\n",
      "Epoch 16485/30000 Training Loss: 0.03828951716423035\n",
      "Epoch 16486/30000 Training Loss: 0.029422804713249207\n",
      "Epoch 16487/30000 Training Loss: 0.04997821897268295\n",
      "Epoch 16488/30000 Training Loss: 0.03926574066281319\n",
      "Epoch 16489/30000 Training Loss: 0.057481344789266586\n",
      "Epoch 16490/30000 Training Loss: 0.056486643850803375\n",
      "Epoch 16491/30000 Training Loss: 0.07094001024961472\n",
      "Epoch 16492/30000 Training Loss: 0.06020471453666687\n",
      "Epoch 16493/30000 Training Loss: 0.06052922084927559\n",
      "Epoch 16494/30000 Training Loss: 0.042876388877630234\n",
      "Epoch 16495/30000 Training Loss: 0.05121874436736107\n",
      "Epoch 16496/30000 Training Loss: 0.06434138119220734\n",
      "Epoch 16497/30000 Training Loss: 0.059580039232969284\n",
      "Epoch 16498/30000 Training Loss: 0.044032033532857895\n",
      "Epoch 16499/30000 Training Loss: 0.05673326924443245\n",
      "Epoch 16500/30000 Training Loss: 0.05033209174871445\n",
      "Epoch 16500/30000 Validation Loss: 0.03930322825908661\n",
      "Epoch 16501/30000 Training Loss: 0.04050679877400398\n",
      "Epoch 16502/30000 Training Loss: 0.04174733906984329\n",
      "Epoch 16503/30000 Training Loss: 0.04732158035039902\n",
      "Epoch 16504/30000 Training Loss: 0.04765506088733673\n",
      "Epoch 16505/30000 Training Loss: 0.03471986949443817\n",
      "Epoch 16506/30000 Training Loss: 0.059008654206991196\n",
      "Epoch 16507/30000 Training Loss: 0.04546200856566429\n",
      "Epoch 16508/30000 Training Loss: 0.0520346537232399\n",
      "Epoch 16509/30000 Training Loss: 0.05515569821000099\n",
      "Epoch 16510/30000 Training Loss: 0.05453110486268997\n",
      "Epoch 16511/30000 Training Loss: 0.05002419650554657\n",
      "Epoch 16512/30000 Training Loss: 0.048330724239349365\n",
      "Epoch 16513/30000 Training Loss: 0.050230722874403\n",
      "Epoch 16514/30000 Training Loss: 0.05798380821943283\n",
      "Epoch 16515/30000 Training Loss: 0.06298775225877762\n",
      "Epoch 16516/30000 Training Loss: 0.05244768038392067\n",
      "Epoch 16517/30000 Training Loss: 0.04478295519948006\n",
      "Epoch 16518/30000 Training Loss: 0.04720205441117287\n",
      "Epoch 16519/30000 Training Loss: 0.03981499746441841\n",
      "Epoch 16520/30000 Training Loss: 0.043485380709171295\n",
      "Epoch 16521/30000 Training Loss: 0.05749562010169029\n",
      "Epoch 16522/30000 Training Loss: 0.04726940020918846\n",
      "Epoch 16523/30000 Training Loss: 0.04884634539484978\n",
      "Epoch 16524/30000 Training Loss: 0.06447838991880417\n",
      "Epoch 16525/30000 Training Loss: 0.05331423878669739\n",
      "Epoch 16526/30000 Training Loss: 0.05152669548988342\n",
      "Epoch 16527/30000 Training Loss: 0.056175172328948975\n",
      "Epoch 16528/30000 Training Loss: 0.041338786482810974\n",
      "Epoch 16529/30000 Training Loss: 0.030113430693745613\n",
      "Epoch 16530/30000 Training Loss: 0.056102897971868515\n",
      "Epoch 16531/30000 Training Loss: 0.05979417264461517\n",
      "Epoch 16532/30000 Training Loss: 0.06653229892253876\n",
      "Epoch 16533/30000 Training Loss: 0.05094397813081741\n",
      "Epoch 16534/30000 Training Loss: 0.05945245921611786\n",
      "Epoch 16535/30000 Training Loss: 0.0407831147313118\n",
      "Epoch 16536/30000 Training Loss: 0.037806034088134766\n",
      "Epoch 16537/30000 Training Loss: 0.03947281092405319\n",
      "Epoch 16538/30000 Training Loss: 0.03564160317182541\n",
      "Epoch 16539/30000 Training Loss: 0.05516525357961655\n",
      "Epoch 16540/30000 Training Loss: 0.0408942848443985\n",
      "Epoch 16541/30000 Training Loss: 0.04466346651315689\n",
      "Epoch 16542/30000 Training Loss: 0.05871451646089554\n",
      "Epoch 16543/30000 Training Loss: 0.048804786056280136\n",
      "Epoch 16544/30000 Training Loss: 0.06246938183903694\n",
      "Epoch 16545/30000 Training Loss: 0.04093112424015999\n",
      "Epoch 16546/30000 Training Loss: 0.036288052797317505\n",
      "Epoch 16547/30000 Training Loss: 0.06835782527923584\n",
      "Epoch 16548/30000 Training Loss: 0.05478718504309654\n",
      "Epoch 16549/30000 Training Loss: 0.05191711708903313\n",
      "Epoch 16550/30000 Training Loss: 0.04769931733608246\n",
      "Epoch 16551/30000 Training Loss: 0.05104614049196243\n",
      "Epoch 16552/30000 Training Loss: 0.03891889005899429\n",
      "Epoch 16553/30000 Training Loss: 0.044745951890945435\n",
      "Epoch 16554/30000 Training Loss: 0.055163756012916565\n",
      "Epoch 16555/30000 Training Loss: 0.04388254135847092\n",
      "Epoch 16556/30000 Training Loss: 0.050218649208545685\n",
      "Epoch 16557/30000 Training Loss: 0.051312342286109924\n",
      "Epoch 16558/30000 Training Loss: 0.04303941875696182\n",
      "Epoch 16559/30000 Training Loss: 0.04897193983197212\n",
      "Epoch 16560/30000 Training Loss: 0.047015126794576645\n",
      "Epoch 16561/30000 Training Loss: 0.04067002236843109\n",
      "Epoch 16562/30000 Training Loss: 0.05937867611646652\n",
      "Epoch 16563/30000 Training Loss: 0.0455806590616703\n",
      "Epoch 16564/30000 Training Loss: 0.04534366726875305\n",
      "Epoch 16565/30000 Training Loss: 0.03804878517985344\n",
      "Epoch 16566/30000 Training Loss: 0.04082338884472847\n",
      "Epoch 16567/30000 Training Loss: 0.04713908210396767\n",
      "Epoch 16568/30000 Training Loss: 0.03537869453430176\n",
      "Epoch 16569/30000 Training Loss: 0.046361178159713745\n",
      "Epoch 16570/30000 Training Loss: 0.04459965601563454\n",
      "Epoch 16571/30000 Training Loss: 0.04206286370754242\n",
      "Epoch 16572/30000 Training Loss: 0.06869174540042877\n",
      "Epoch 16573/30000 Training Loss: 0.06815356761217117\n",
      "Epoch 16574/30000 Training Loss: 0.04319988191127777\n",
      "Epoch 16575/30000 Training Loss: 0.048067156225442886\n",
      "Epoch 16576/30000 Training Loss: 0.03615731745958328\n",
      "Epoch 16577/30000 Training Loss: 0.039654795080423355\n",
      "Epoch 16578/30000 Training Loss: 0.07143672555685043\n",
      "Epoch 16579/30000 Training Loss: 0.06123845651745796\n",
      "Epoch 16580/30000 Training Loss: 0.05153578519821167\n",
      "Epoch 16581/30000 Training Loss: 0.0464443601667881\n",
      "Epoch 16582/30000 Training Loss: 0.04972825199365616\n",
      "Epoch 16583/30000 Training Loss: 0.04572053253650665\n",
      "Epoch 16584/30000 Training Loss: 0.06106024980545044\n",
      "Epoch 16585/30000 Training Loss: 0.042112916707992554\n",
      "Epoch 16586/30000 Training Loss: 0.050057440996170044\n",
      "Epoch 16587/30000 Training Loss: 0.04495633393526077\n",
      "Epoch 16588/30000 Training Loss: 0.05806359648704529\n",
      "Epoch 16589/30000 Training Loss: 0.050106752663850784\n",
      "Epoch 16590/30000 Training Loss: 0.037074387073516846\n",
      "Epoch 16591/30000 Training Loss: 0.036983150988817215\n",
      "Epoch 16592/30000 Training Loss: 0.05227081477642059\n",
      "Epoch 16593/30000 Training Loss: 0.04338894411921501\n",
      "Epoch 16594/30000 Training Loss: 0.04115641862154007\n",
      "Epoch 16595/30000 Training Loss: 0.03875798359513283\n",
      "Epoch 16596/30000 Training Loss: 0.05048292502760887\n",
      "Epoch 16597/30000 Training Loss: 0.037753473967313766\n",
      "Epoch 16598/30000 Training Loss: 0.04997970908880234\n",
      "Epoch 16599/30000 Training Loss: 0.04462265595793724\n",
      "Epoch 16600/30000 Training Loss: 0.04126952216029167\n",
      "Epoch 16600/30000 Validation Loss: 0.04389382526278496\n",
      "Epoch 16601/30000 Training Loss: 0.05818475782871246\n",
      "Epoch 16602/30000 Training Loss: 0.05711532384157181\n",
      "Epoch 16603/30000 Training Loss: 0.06473105400800705\n",
      "Epoch 16604/30000 Training Loss: 0.049826569855213165\n",
      "Epoch 16605/30000 Training Loss: 0.040696777403354645\n",
      "Epoch 16606/30000 Training Loss: 0.054005708545446396\n",
      "Epoch 16607/30000 Training Loss: 0.05484089255332947\n",
      "Epoch 16608/30000 Training Loss: 0.04257746785879135\n",
      "Epoch 16609/30000 Training Loss: 0.03541084751486778\n",
      "Epoch 16610/30000 Training Loss: 0.039239659905433655\n",
      "Epoch 16611/30000 Training Loss: 0.04659274220466614\n",
      "Epoch 16612/30000 Training Loss: 0.04983201250433922\n",
      "Epoch 16613/30000 Training Loss: 0.047429170459508896\n",
      "Epoch 16614/30000 Training Loss: 0.06295499205589294\n",
      "Epoch 16615/30000 Training Loss: 0.042960766702890396\n",
      "Epoch 16616/30000 Training Loss: 0.058376651257276535\n",
      "Epoch 16617/30000 Training Loss: 0.04636474698781967\n",
      "Epoch 16618/30000 Training Loss: 0.06926219910383224\n",
      "Epoch 16619/30000 Training Loss: 0.04133710265159607\n",
      "Epoch 16620/30000 Training Loss: 0.04021637886762619\n",
      "Epoch 16621/30000 Training Loss: 0.06049187108874321\n",
      "Epoch 16622/30000 Training Loss: 0.04163159430027008\n",
      "Epoch 16623/30000 Training Loss: 0.041473694145679474\n",
      "Epoch 16624/30000 Training Loss: 0.052693817764520645\n",
      "Epoch 16625/30000 Training Loss: 0.047968074679374695\n",
      "Epoch 16626/30000 Training Loss: 0.057944364845752716\n",
      "Epoch 16627/30000 Training Loss: 0.033158984035253525\n",
      "Epoch 16628/30000 Training Loss: 0.04358726739883423\n",
      "Epoch 16629/30000 Training Loss: 0.04642663896083832\n",
      "Epoch 16630/30000 Training Loss: 0.04799084737896919\n",
      "Epoch 16631/30000 Training Loss: 0.04535061866044998\n",
      "Epoch 16632/30000 Training Loss: 0.03955243527889252\n",
      "Epoch 16633/30000 Training Loss: 0.046075206249952316\n",
      "Epoch 16634/30000 Training Loss: 0.04873017221689224\n",
      "Epoch 16635/30000 Training Loss: 0.059958040714263916\n",
      "Epoch 16636/30000 Training Loss: 0.05549488216638565\n",
      "Epoch 16637/30000 Training Loss: 0.04621884971857071\n",
      "Epoch 16638/30000 Training Loss: 0.05501382052898407\n",
      "Epoch 16639/30000 Training Loss: 0.044016364961862564\n",
      "Epoch 16640/30000 Training Loss: 0.06536805629730225\n",
      "Epoch 16641/30000 Training Loss: 0.06319321691989899\n",
      "Epoch 16642/30000 Training Loss: 0.06915687024593353\n",
      "Epoch 16643/30000 Training Loss: 0.051771946251392365\n",
      "Epoch 16644/30000 Training Loss: 0.0490826778113842\n",
      "Epoch 16645/30000 Training Loss: 0.06586525589227676\n",
      "Epoch 16646/30000 Training Loss: 0.03301738202571869\n",
      "Epoch 16647/30000 Training Loss: 0.04416409134864807\n",
      "Epoch 16648/30000 Training Loss: 0.0379030667245388\n",
      "Epoch 16649/30000 Training Loss: 0.053327057510614395\n",
      "Epoch 16650/30000 Training Loss: 0.058325301855802536\n",
      "Epoch 16651/30000 Training Loss: 0.06050490587949753\n",
      "Epoch 16652/30000 Training Loss: 0.06494784355163574\n",
      "Epoch 16653/30000 Training Loss: 0.043519072234630585\n",
      "Epoch 16654/30000 Training Loss: 0.056016385555267334\n",
      "Epoch 16655/30000 Training Loss: 0.039358701556921005\n",
      "Epoch 16656/30000 Training Loss: 0.04943174496293068\n",
      "Epoch 16657/30000 Training Loss: 0.0508568175137043\n",
      "Epoch 16658/30000 Training Loss: 0.04433176666498184\n",
      "Epoch 16659/30000 Training Loss: 0.04898528754711151\n",
      "Epoch 16660/30000 Training Loss: 0.045674312859773636\n",
      "Epoch 16661/30000 Training Loss: 0.03258761391043663\n",
      "Epoch 16662/30000 Training Loss: 0.05414065718650818\n",
      "Epoch 16663/30000 Training Loss: 0.04241034388542175\n",
      "Epoch 16664/30000 Training Loss: 0.04764635115861893\n",
      "Epoch 16665/30000 Training Loss: 0.03680635243654251\n",
      "Epoch 16666/30000 Training Loss: 0.042538776993751526\n",
      "Epoch 16667/30000 Training Loss: 0.06758539378643036\n",
      "Epoch 16668/30000 Training Loss: 0.05098121613264084\n",
      "Epoch 16669/30000 Training Loss: 0.05235163867473602\n",
      "Epoch 16670/30000 Training Loss: 0.051213283091783524\n",
      "Epoch 16671/30000 Training Loss: 0.039379511028528214\n",
      "Epoch 16672/30000 Training Loss: 0.0534043088555336\n",
      "Epoch 16673/30000 Training Loss: 0.04767383635044098\n",
      "Epoch 16674/30000 Training Loss: 0.0480048842728138\n",
      "Epoch 16675/30000 Training Loss: 0.037652239203453064\n",
      "Epoch 16676/30000 Training Loss: 0.05429604649543762\n",
      "Epoch 16677/30000 Training Loss: 0.038628533482551575\n",
      "Epoch 16678/30000 Training Loss: 0.036311130970716476\n",
      "Epoch 16679/30000 Training Loss: 0.05735994875431061\n",
      "Epoch 16680/30000 Training Loss: 0.04331647604703903\n",
      "Epoch 16681/30000 Training Loss: 0.056972455233335495\n",
      "Epoch 16682/30000 Training Loss: 0.042123496532440186\n",
      "Epoch 16683/30000 Training Loss: 0.03881009295582771\n",
      "Epoch 16684/30000 Training Loss: 0.05396147817373276\n",
      "Epoch 16685/30000 Training Loss: 0.05224527046084404\n",
      "Epoch 16686/30000 Training Loss: 0.03834202140569687\n",
      "Epoch 16687/30000 Training Loss: 0.056828465312719345\n",
      "Epoch 16688/30000 Training Loss: 0.041940316557884216\n",
      "Epoch 16689/30000 Training Loss: 0.037245120853185654\n",
      "Epoch 16690/30000 Training Loss: 0.05039166286587715\n",
      "Epoch 16691/30000 Training Loss: 0.050086021423339844\n",
      "Epoch 16692/30000 Training Loss: 0.04527544230222702\n",
      "Epoch 16693/30000 Training Loss: 0.04571336507797241\n",
      "Epoch 16694/30000 Training Loss: 0.04975011199712753\n",
      "Epoch 16695/30000 Training Loss: 0.037638433277606964\n",
      "Epoch 16696/30000 Training Loss: 0.04679292067885399\n",
      "Epoch 16697/30000 Training Loss: 0.037983909249305725\n",
      "Epoch 16698/30000 Training Loss: 0.05184169113636017\n",
      "Epoch 16699/30000 Training Loss: 0.033083878457546234\n",
      "Epoch 16700/30000 Training Loss: 0.04988090693950653\n",
      "Epoch 16700/30000 Validation Loss: 0.04554647579789162\n",
      "Epoch 16701/30000 Training Loss: 0.05507885664701462\n",
      "Epoch 16702/30000 Training Loss: 0.04959942400455475\n",
      "Epoch 16703/30000 Training Loss: 0.044060833752155304\n",
      "Epoch 16704/30000 Training Loss: 0.053990647196769714\n",
      "Epoch 16705/30000 Training Loss: 0.040140170603990555\n",
      "Epoch 16706/30000 Training Loss: 0.04800979793071747\n",
      "Epoch 16707/30000 Training Loss: 0.06086535006761551\n",
      "Epoch 16708/30000 Training Loss: 0.052416738122701645\n",
      "Epoch 16709/30000 Training Loss: 0.05719519406557083\n",
      "Epoch 16710/30000 Training Loss: 0.05344424024224281\n",
      "Epoch 16711/30000 Training Loss: 0.065543994307518\n",
      "Epoch 16712/30000 Training Loss: 0.04728248715400696\n",
      "Epoch 16713/30000 Training Loss: 0.03577929735183716\n",
      "Epoch 16714/30000 Training Loss: 0.058767735958099365\n",
      "Epoch 16715/30000 Training Loss: 0.04843832924962044\n",
      "Epoch 16716/30000 Training Loss: 0.0520835779607296\n",
      "Epoch 16717/30000 Training Loss: 0.0637502521276474\n",
      "Epoch 16718/30000 Training Loss: 0.037638019770383835\n",
      "Epoch 16719/30000 Training Loss: 0.07570845633745193\n",
      "Epoch 16720/30000 Training Loss: 0.05380497872829437\n",
      "Epoch 16721/30000 Training Loss: 0.05269075185060501\n",
      "Epoch 16722/30000 Training Loss: 0.053909800946712494\n",
      "Epoch 16723/30000 Training Loss: 0.03992150351405144\n",
      "Epoch 16724/30000 Training Loss: 0.048416975885629654\n",
      "Epoch 16725/30000 Training Loss: 0.0557929128408432\n",
      "Epoch 16726/30000 Training Loss: 0.0476418174803257\n",
      "Epoch 16727/30000 Training Loss: 0.037808310240507126\n",
      "Epoch 16728/30000 Training Loss: 0.04988040775060654\n",
      "Epoch 16729/30000 Training Loss: 0.05131269991397858\n",
      "Epoch 16730/30000 Training Loss: 0.0632774829864502\n",
      "Epoch 16731/30000 Training Loss: 0.05197904258966446\n",
      "Epoch 16732/30000 Training Loss: 0.04803537577390671\n",
      "Epoch 16733/30000 Training Loss: 0.05481782555580139\n",
      "Epoch 16734/30000 Training Loss: 0.060265541076660156\n",
      "Epoch 16735/30000 Training Loss: 0.04881680756807327\n",
      "Epoch 16736/30000 Training Loss: 0.042850054800510406\n",
      "Epoch 16737/30000 Training Loss: 0.057605646550655365\n",
      "Epoch 16738/30000 Training Loss: 0.04212135449051857\n",
      "Epoch 16739/30000 Training Loss: 0.043237894773483276\n",
      "Epoch 16740/30000 Training Loss: 0.04373586177825928\n",
      "Epoch 16741/30000 Training Loss: 0.07319489121437073\n",
      "Epoch 16742/30000 Training Loss: 0.062081459909677505\n",
      "Epoch 16743/30000 Training Loss: 0.057934150099754333\n",
      "Epoch 16744/30000 Training Loss: 0.038627829402685165\n",
      "Epoch 16745/30000 Training Loss: 0.05057907849550247\n",
      "Epoch 16746/30000 Training Loss: 0.06154492124915123\n",
      "Epoch 16747/30000 Training Loss: 0.04143540933728218\n",
      "Epoch 16748/30000 Training Loss: 0.05693519115447998\n",
      "Epoch 16749/30000 Training Loss: 0.04212231934070587\n",
      "Epoch 16750/30000 Training Loss: 0.059477757662534714\n",
      "Epoch 16751/30000 Training Loss: 0.048350121825933456\n",
      "Epoch 16752/30000 Training Loss: 0.050834402441978455\n",
      "Epoch 16753/30000 Training Loss: 0.055516377091407776\n",
      "Epoch 16754/30000 Training Loss: 0.045411091297864914\n",
      "Epoch 16755/30000 Training Loss: 0.057086218148469925\n",
      "Epoch 16756/30000 Training Loss: 0.05462856590747833\n",
      "Epoch 16757/30000 Training Loss: 0.04083273932337761\n",
      "Epoch 16758/30000 Training Loss: 0.05547107383608818\n",
      "Epoch 16759/30000 Training Loss: 0.047421783208847046\n",
      "Epoch 16760/30000 Training Loss: 0.0581657774746418\n",
      "Epoch 16761/30000 Training Loss: 0.036648109555244446\n",
      "Epoch 16762/30000 Training Loss: 0.04430894926190376\n",
      "Epoch 16763/30000 Training Loss: 0.04292420297861099\n",
      "Epoch 16764/30000 Training Loss: 0.04900074377655983\n",
      "Epoch 16765/30000 Training Loss: 0.05056161433458328\n",
      "Epoch 16766/30000 Training Loss: 0.04546011611819267\n",
      "Epoch 16767/30000 Training Loss: 0.04241903871297836\n",
      "Epoch 16768/30000 Training Loss: 0.05584944412112236\n",
      "Epoch 16769/30000 Training Loss: 0.05469880998134613\n",
      "Epoch 16770/30000 Training Loss: 0.04184954985976219\n",
      "Epoch 16771/30000 Training Loss: 0.05347466096282005\n",
      "Epoch 16772/30000 Training Loss: 0.036677874624729156\n",
      "Epoch 16773/30000 Training Loss: 0.037489112466573715\n",
      "Epoch 16774/30000 Training Loss: 0.04924045130610466\n",
      "Epoch 16775/30000 Training Loss: 0.03882773965597153\n",
      "Epoch 16776/30000 Training Loss: 0.05213211849331856\n",
      "Epoch 16777/30000 Training Loss: 0.04547656327486038\n",
      "Epoch 16778/30000 Training Loss: 0.047064609825611115\n",
      "Epoch 16779/30000 Training Loss: 0.05124405026435852\n",
      "Epoch 16780/30000 Training Loss: 0.03210652619600296\n",
      "Epoch 16781/30000 Training Loss: 0.0502055287361145\n",
      "Epoch 16782/30000 Training Loss: 0.039359524846076965\n",
      "Epoch 16783/30000 Training Loss: 0.05192352086305618\n",
      "Epoch 16784/30000 Training Loss: 0.057574279606342316\n",
      "Epoch 16785/30000 Training Loss: 0.05793365463614464\n",
      "Epoch 16786/30000 Training Loss: 0.04938584193587303\n",
      "Epoch 16787/30000 Training Loss: 0.069613516330719\n",
      "Epoch 16788/30000 Training Loss: 0.0567588284611702\n",
      "Epoch 16789/30000 Training Loss: 0.04848126694560051\n",
      "Epoch 16790/30000 Training Loss: 0.045499928295612335\n",
      "Epoch 16791/30000 Training Loss: 0.05448419228196144\n",
      "Epoch 16792/30000 Training Loss: 0.06891120970249176\n",
      "Epoch 16793/30000 Training Loss: 0.05848163366317749\n",
      "Epoch 16794/30000 Training Loss: 0.04478331282734871\n",
      "Epoch 16795/30000 Training Loss: 0.04867442324757576\n",
      "Epoch 16796/30000 Training Loss: 0.05379631742835045\n",
      "Epoch 16797/30000 Training Loss: 0.04361961409449577\n",
      "Epoch 16798/30000 Training Loss: 0.061255376785993576\n",
      "Epoch 16799/30000 Training Loss: 0.054081521928310394\n",
      "Epoch 16800/30000 Training Loss: 0.04632467031478882\n",
      "Epoch 16800/30000 Validation Loss: 0.0448688343167305\n",
      "Epoch 16801/30000 Training Loss: 0.0551028847694397\n",
      "Epoch 16802/30000 Training Loss: 0.05343065410852432\n",
      "Epoch 16803/30000 Training Loss: 0.032693855464458466\n",
      "Epoch 16804/30000 Training Loss: 0.052271418273448944\n",
      "Epoch 16805/30000 Training Loss: 0.05154094845056534\n",
      "Epoch 16806/30000 Training Loss: 0.042841412127017975\n",
      "Epoch 16807/30000 Training Loss: 0.06749892979860306\n",
      "Epoch 16808/30000 Training Loss: 0.055229783058166504\n",
      "Epoch 16809/30000 Training Loss: 0.05496254190802574\n",
      "Epoch 16810/30000 Training Loss: 0.043800562620162964\n",
      "Epoch 16811/30000 Training Loss: 0.03244932368397713\n",
      "Epoch 16812/30000 Training Loss: 0.04757707193493843\n",
      "Epoch 16813/30000 Training Loss: 0.04819106683135033\n",
      "Epoch 16814/30000 Training Loss: 0.04895038902759552\n",
      "Epoch 16815/30000 Training Loss: 0.0482378751039505\n",
      "Epoch 16816/30000 Training Loss: 0.05785191059112549\n",
      "Epoch 16817/30000 Training Loss: 0.03724917024374008\n",
      "Epoch 16818/30000 Training Loss: 0.045430127531290054\n",
      "Epoch 16819/30000 Training Loss: 0.054985351860523224\n",
      "Epoch 16820/30000 Training Loss: 0.06936433166265488\n",
      "Epoch 16821/30000 Training Loss: 0.04653436690568924\n",
      "Epoch 16822/30000 Training Loss: 0.06617661565542221\n",
      "Epoch 16823/30000 Training Loss: 0.042457215487957\n",
      "Epoch 16824/30000 Training Loss: 0.03934725001454353\n",
      "Epoch 16825/30000 Training Loss: 0.03943605720996857\n",
      "Epoch 16826/30000 Training Loss: 0.05726727843284607\n",
      "Epoch 16827/30000 Training Loss: 0.04850858077406883\n",
      "Epoch 16828/30000 Training Loss: 0.05151408538222313\n",
      "Epoch 16829/30000 Training Loss: 0.0643080547451973\n",
      "Epoch 16830/30000 Training Loss: 0.057829391211271286\n",
      "Epoch 16831/30000 Training Loss: 0.06003565713763237\n",
      "Epoch 16832/30000 Training Loss: 0.03901737928390503\n",
      "Epoch 16833/30000 Training Loss: 0.0583256259560585\n",
      "Epoch 16834/30000 Training Loss: 0.04112887755036354\n",
      "Epoch 16835/30000 Training Loss: 0.03995926305651665\n",
      "Epoch 16836/30000 Training Loss: 0.04111528396606445\n",
      "Epoch 16837/30000 Training Loss: 0.052318088710308075\n",
      "Epoch 16838/30000 Training Loss: 0.05103680491447449\n",
      "Epoch 16839/30000 Training Loss: 0.03946607559919357\n",
      "Epoch 16840/30000 Training Loss: 0.0427016019821167\n",
      "Epoch 16841/30000 Training Loss: 0.05134173482656479\n",
      "Epoch 16842/30000 Training Loss: 0.04124321788549423\n",
      "Epoch 16843/30000 Training Loss: 0.051218368113040924\n",
      "Epoch 16844/30000 Training Loss: 0.04688924551010132\n",
      "Epoch 16845/30000 Training Loss: 0.04503720998764038\n",
      "Epoch 16846/30000 Training Loss: 0.05312253162264824\n",
      "Epoch 16847/30000 Training Loss: 0.06922455877065659\n",
      "Epoch 16848/30000 Training Loss: 0.04161833971738815\n",
      "Epoch 16849/30000 Training Loss: 0.06251855194568634\n",
      "Epoch 16850/30000 Training Loss: 0.05706554651260376\n",
      "Epoch 16851/30000 Training Loss: 0.03029465302824974\n",
      "Epoch 16852/30000 Training Loss: 0.03454665467143059\n",
      "Epoch 16853/30000 Training Loss: 0.062101587653160095\n",
      "Epoch 16854/30000 Training Loss: 0.07139428704977036\n",
      "Epoch 16855/30000 Training Loss: 0.06077737733721733\n",
      "Epoch 16856/30000 Training Loss: 0.04617675393819809\n",
      "Epoch 16857/30000 Training Loss: 0.058401383459568024\n",
      "Epoch 16858/30000 Training Loss: 0.04496150463819504\n",
      "Epoch 16859/30000 Training Loss: 0.043479807674884796\n",
      "Epoch 16860/30000 Training Loss: 0.04998262971639633\n",
      "Epoch 16861/30000 Training Loss: 0.03913405165076256\n",
      "Epoch 16862/30000 Training Loss: 0.04630230367183685\n",
      "Epoch 16863/30000 Training Loss: 0.03939306363463402\n",
      "Epoch 16864/30000 Training Loss: 0.053511373698711395\n",
      "Epoch 16865/30000 Training Loss: 0.04086984321475029\n",
      "Epoch 16866/30000 Training Loss: 0.038494594395160675\n",
      "Epoch 16867/30000 Training Loss: 0.06188299134373665\n",
      "Epoch 16868/30000 Training Loss: 0.0371006615459919\n",
      "Epoch 16869/30000 Training Loss: 0.04846270754933357\n",
      "Epoch 16870/30000 Training Loss: 0.03275667503476143\n",
      "Epoch 16871/30000 Training Loss: 0.03721063956618309\n",
      "Epoch 16872/30000 Training Loss: 0.04366818442940712\n",
      "Epoch 16873/30000 Training Loss: 0.05092081055045128\n",
      "Epoch 16874/30000 Training Loss: 0.037185147404670715\n",
      "Epoch 16875/30000 Training Loss: 0.057638801634311676\n",
      "Epoch 16876/30000 Training Loss: 0.05081069469451904\n",
      "Epoch 16877/30000 Training Loss: 0.059368208050727844\n",
      "Epoch 16878/30000 Training Loss: 0.042961861938238144\n",
      "Epoch 16879/30000 Training Loss: 0.04415202885866165\n",
      "Epoch 16880/30000 Training Loss: 0.032499898225069046\n",
      "Epoch 16881/30000 Training Loss: 0.0443558543920517\n",
      "Epoch 16882/30000 Training Loss: 0.04410887882113457\n",
      "Epoch 16883/30000 Training Loss: 0.0492180772125721\n",
      "Epoch 16884/30000 Training Loss: 0.047755166888237\n",
      "Epoch 16885/30000 Training Loss: 0.046711307018995285\n",
      "Epoch 16886/30000 Training Loss: 0.05830533057451248\n",
      "Epoch 16887/30000 Training Loss: 0.04250020161271095\n",
      "Epoch 16888/30000 Training Loss: 0.04409245774149895\n",
      "Epoch 16889/30000 Training Loss: 0.06541869044303894\n",
      "Epoch 16890/30000 Training Loss: 0.04815243184566498\n",
      "Epoch 16891/30000 Training Loss: 0.05369177460670471\n",
      "Epoch 16892/30000 Training Loss: 0.04616732895374298\n",
      "Epoch 16893/30000 Training Loss: 0.041759055107831955\n",
      "Epoch 16894/30000 Training Loss: 0.050919096916913986\n",
      "Epoch 16895/30000 Training Loss: 0.049611933529376984\n",
      "Epoch 16896/30000 Training Loss: 0.04736034944653511\n",
      "Epoch 16897/30000 Training Loss: 0.04351953789591789\n",
      "Epoch 16898/30000 Training Loss: 0.030859895050525665\n",
      "Epoch 16899/30000 Training Loss: 0.05082706734538078\n",
      "Epoch 16900/30000 Training Loss: 0.035483863204717636\n",
      "Epoch 16900/30000 Validation Loss: 0.04626157879829407\n",
      "Epoch 16901/30000 Training Loss: 0.04149743914604187\n",
      "Epoch 16902/30000 Training Loss: 0.05142854526638985\n",
      "Epoch 16903/30000 Training Loss: 0.05966734141111374\n",
      "Epoch 16904/30000 Training Loss: 0.04698158800601959\n",
      "Epoch 16905/30000 Training Loss: 0.04880785942077637\n",
      "Epoch 16906/30000 Training Loss: 0.04383951053023338\n",
      "Epoch 16907/30000 Training Loss: 0.05742117017507553\n",
      "Epoch 16908/30000 Training Loss: 0.05329776555299759\n",
      "Epoch 16909/30000 Training Loss: 0.03765067458152771\n",
      "Epoch 16910/30000 Training Loss: 0.04617571085691452\n",
      "Epoch 16911/30000 Training Loss: 0.053163833916187286\n",
      "Epoch 16912/30000 Training Loss: 0.088109590113163\n",
      "Epoch 16913/30000 Training Loss: 0.047248851507902145\n",
      "Epoch 16914/30000 Training Loss: 0.05791296809911728\n",
      "Epoch 16915/30000 Training Loss: 0.050052739679813385\n",
      "Epoch 16916/30000 Training Loss: 0.04835838824510574\n",
      "Epoch 16917/30000 Training Loss: 0.04679465666413307\n",
      "Epoch 16918/30000 Training Loss: 0.03811347857117653\n",
      "Epoch 16919/30000 Training Loss: 0.04079056158661842\n",
      "Epoch 16920/30000 Training Loss: 0.07015122473239899\n",
      "Epoch 16921/30000 Training Loss: 0.04030189663171768\n",
      "Epoch 16922/30000 Training Loss: 0.05053247511386871\n",
      "Epoch 16923/30000 Training Loss: 0.05270513892173767\n",
      "Epoch 16924/30000 Training Loss: 0.03984465450048447\n",
      "Epoch 16925/30000 Training Loss: 0.06530531495809555\n",
      "Epoch 16926/30000 Training Loss: 0.052222542464733124\n",
      "Epoch 16927/30000 Training Loss: 0.041210345923900604\n",
      "Epoch 16928/30000 Training Loss: 0.05579831823706627\n",
      "Epoch 16929/30000 Training Loss: 0.047559186816215515\n",
      "Epoch 16930/30000 Training Loss: 0.05591225251555443\n",
      "Epoch 16931/30000 Training Loss: 0.0393749438226223\n",
      "Epoch 16932/30000 Training Loss: 0.04053572192788124\n",
      "Epoch 16933/30000 Training Loss: 0.04701415076851845\n",
      "Epoch 16934/30000 Training Loss: 0.04853026568889618\n",
      "Epoch 16935/30000 Training Loss: 0.04988177865743637\n",
      "Epoch 16936/30000 Training Loss: 0.038401465862989426\n",
      "Epoch 16937/30000 Training Loss: 0.06715089827775955\n",
      "Epoch 16938/30000 Training Loss: 0.05836718529462814\n",
      "Epoch 16939/30000 Training Loss: 0.047638632357120514\n",
      "Epoch 16940/30000 Training Loss: 0.05631235986948013\n",
      "Epoch 16941/30000 Training Loss: 0.05140000581741333\n",
      "Epoch 16942/30000 Training Loss: 0.043941251933574677\n",
      "Epoch 16943/30000 Training Loss: 0.0670945942401886\n",
      "Epoch 16944/30000 Training Loss: 0.044183216989040375\n",
      "Epoch 16945/30000 Training Loss: 0.052890777587890625\n",
      "Epoch 16946/30000 Training Loss: 0.03951411694288254\n",
      "Epoch 16947/30000 Training Loss: 0.04680127650499344\n",
      "Epoch 16948/30000 Training Loss: 0.046895794570446014\n",
      "Epoch 16949/30000 Training Loss: 0.04876348748803139\n",
      "Epoch 16950/30000 Training Loss: 0.04886053130030632\n",
      "Epoch 16951/30000 Training Loss: 0.05260838568210602\n",
      "Epoch 16952/30000 Training Loss: 0.0520864762365818\n",
      "Epoch 16953/30000 Training Loss: 0.06091228500008583\n",
      "Epoch 16954/30000 Training Loss: 0.055103596299886703\n",
      "Epoch 16955/30000 Training Loss: 0.031617194414138794\n",
      "Epoch 16956/30000 Training Loss: 0.03942882642149925\n",
      "Epoch 16957/30000 Training Loss: 0.04976016655564308\n",
      "Epoch 16958/30000 Training Loss: 0.05141708627343178\n",
      "Epoch 16959/30000 Training Loss: 0.040750231593847275\n",
      "Epoch 16960/30000 Training Loss: 0.038966238498687744\n",
      "Epoch 16961/30000 Training Loss: 0.0517767071723938\n",
      "Epoch 16962/30000 Training Loss: 0.044016506522893906\n",
      "Epoch 16963/30000 Training Loss: 0.0470975823700428\n",
      "Epoch 16964/30000 Training Loss: 0.04230928793549538\n",
      "Epoch 16965/30000 Training Loss: 0.07029864937067032\n",
      "Epoch 16966/30000 Training Loss: 0.04186129570007324\n",
      "Epoch 16967/30000 Training Loss: 0.041591111570596695\n",
      "Epoch 16968/30000 Training Loss: 0.06666688621044159\n",
      "Epoch 16969/30000 Training Loss: 0.046749163419008255\n",
      "Epoch 16970/30000 Training Loss: 0.04913370683789253\n",
      "Epoch 16971/30000 Training Loss: 0.05136694014072418\n",
      "Epoch 16972/30000 Training Loss: 0.04075096920132637\n",
      "Epoch 16973/30000 Training Loss: 0.058654338121414185\n",
      "Epoch 16974/30000 Training Loss: 0.047898825258016586\n",
      "Epoch 16975/30000 Training Loss: 0.06156803295016289\n",
      "Epoch 16976/30000 Training Loss: 0.05010872334241867\n",
      "Epoch 16977/30000 Training Loss: 0.03894195705652237\n",
      "Epoch 16978/30000 Training Loss: 0.03924855589866638\n",
      "Epoch 16979/30000 Training Loss: 0.049695536494255066\n",
      "Epoch 16980/30000 Training Loss: 0.04214903339743614\n",
      "Epoch 16981/30000 Training Loss: 0.05937343090772629\n",
      "Epoch 16982/30000 Training Loss: 0.04758099466562271\n",
      "Epoch 16983/30000 Training Loss: 0.052452921867370605\n",
      "Epoch 16984/30000 Training Loss: 0.051670145243406296\n",
      "Epoch 16985/30000 Training Loss: 0.04897177964448929\n",
      "Epoch 16986/30000 Training Loss: 0.036501672118902206\n",
      "Epoch 16987/30000 Training Loss: 0.04592875763773918\n",
      "Epoch 16988/30000 Training Loss: 0.03961331397294998\n",
      "Epoch 16989/30000 Training Loss: 0.03979991748929024\n",
      "Epoch 16990/30000 Training Loss: 0.05976640433073044\n",
      "Epoch 16991/30000 Training Loss: 0.05274246260523796\n",
      "Epoch 16992/30000 Training Loss: 0.056636810302734375\n",
      "Epoch 16993/30000 Training Loss: 0.061244163662195206\n",
      "Epoch 16994/30000 Training Loss: 0.05435994267463684\n",
      "Epoch 16995/30000 Training Loss: 0.04855324327945709\n",
      "Epoch 16996/30000 Training Loss: 0.04664979502558708\n",
      "Epoch 16997/30000 Training Loss: 0.051889028400182724\n",
      "Epoch 16998/30000 Training Loss: 0.034994907677173615\n",
      "Epoch 16999/30000 Training Loss: 0.0398760549724102\n",
      "Epoch 17000/30000 Training Loss: 0.06158219650387764\n",
      "Epoch 17000/30000 Validation Loss: 0.04227834939956665\n",
      "Epoch 17001/30000 Training Loss: 0.042771436274051666\n",
      "Epoch 17002/30000 Training Loss: 0.031255852431058884\n",
      "Epoch 17003/30000 Training Loss: 0.053358763456344604\n",
      "Epoch 17004/30000 Training Loss: 0.05675484612584114\n",
      "Epoch 17005/30000 Training Loss: 0.05289538577198982\n",
      "Epoch 17006/30000 Training Loss: 0.04552039876580238\n",
      "Epoch 17007/30000 Training Loss: 0.05913889408111572\n",
      "Epoch 17008/30000 Training Loss: 0.059435248374938965\n",
      "Epoch 17009/30000 Training Loss: 0.06488490849733353\n",
      "Epoch 17010/30000 Training Loss: 0.04099148139357567\n",
      "Epoch 17011/30000 Training Loss: 0.04508068412542343\n",
      "Epoch 17012/30000 Training Loss: 0.042881179600954056\n",
      "Epoch 17013/30000 Training Loss: 0.0324551984667778\n",
      "Epoch 17014/30000 Training Loss: 0.0473959781229496\n",
      "Epoch 17015/30000 Training Loss: 0.050908513367176056\n",
      "Epoch 17016/30000 Training Loss: 0.04610477387905121\n",
      "Epoch 17017/30000 Training Loss: 0.05051548033952713\n",
      "Epoch 17018/30000 Training Loss: 0.05069596320390701\n",
      "Epoch 17019/30000 Training Loss: 0.04154869541525841\n",
      "Epoch 17020/30000 Training Loss: 0.03883904591202736\n",
      "Epoch 17021/30000 Training Loss: 0.07038486748933792\n",
      "Epoch 17022/30000 Training Loss: 0.05208723992109299\n",
      "Epoch 17023/30000 Training Loss: 0.04634507745504379\n",
      "Epoch 17024/30000 Training Loss: 0.0492711216211319\n",
      "Epoch 17025/30000 Training Loss: 0.05136687308549881\n",
      "Epoch 17026/30000 Training Loss: 0.03439667820930481\n",
      "Epoch 17027/30000 Training Loss: 0.055527862161397934\n",
      "Epoch 17028/30000 Training Loss: 0.04998082295060158\n",
      "Epoch 17029/30000 Training Loss: 0.050250064581632614\n",
      "Epoch 17030/30000 Training Loss: 0.06366176158189774\n",
      "Epoch 17031/30000 Training Loss: 0.05025694519281387\n",
      "Epoch 17032/30000 Training Loss: 0.0399712510406971\n",
      "Epoch 17033/30000 Training Loss: 0.05025129392743111\n",
      "Epoch 17034/30000 Training Loss: 0.037320684641599655\n",
      "Epoch 17035/30000 Training Loss: 0.038101039826869965\n",
      "Epoch 17036/30000 Training Loss: 0.05057593807578087\n",
      "Epoch 17037/30000 Training Loss: 0.04194590821862221\n",
      "Epoch 17038/30000 Training Loss: 0.04091678559780121\n",
      "Epoch 17039/30000 Training Loss: 0.0457000695168972\n",
      "Epoch 17040/30000 Training Loss: 0.042919524013996124\n",
      "Epoch 17041/30000 Training Loss: 0.049246612936258316\n",
      "Epoch 17042/30000 Training Loss: 0.07086963951587677\n",
      "Epoch 17043/30000 Training Loss: 0.07377307116985321\n",
      "Epoch 17044/30000 Training Loss: 0.04244047775864601\n",
      "Epoch 17045/30000 Training Loss: 0.05398637056350708\n",
      "Epoch 17046/30000 Training Loss: 0.057721223682165146\n",
      "Epoch 17047/30000 Training Loss: 0.039698921144008636\n",
      "Epoch 17048/30000 Training Loss: 0.030360152944922447\n",
      "Epoch 17049/30000 Training Loss: 0.060169145464897156\n",
      "Epoch 17050/30000 Training Loss: 0.038784414529800415\n",
      "Epoch 17051/30000 Training Loss: 0.03919007629156113\n",
      "Epoch 17052/30000 Training Loss: 0.06969645619392395\n",
      "Epoch 17053/30000 Training Loss: 0.04942788556218147\n",
      "Epoch 17054/30000 Training Loss: 0.04797576740384102\n",
      "Epoch 17055/30000 Training Loss: 0.03897731751203537\n",
      "Epoch 17056/30000 Training Loss: 0.04708574712276459\n",
      "Epoch 17057/30000 Training Loss: 0.04897768795490265\n",
      "Epoch 17058/30000 Training Loss: 0.04254524037241936\n",
      "Epoch 17059/30000 Training Loss: 0.050861649215221405\n",
      "Epoch 17060/30000 Training Loss: 0.0390537790954113\n",
      "Epoch 17061/30000 Training Loss: 0.04314098134636879\n",
      "Epoch 17062/30000 Training Loss: 0.05521181970834732\n",
      "Epoch 17063/30000 Training Loss: 0.06027035042643547\n",
      "Epoch 17064/30000 Training Loss: 0.04900283366441727\n",
      "Epoch 17065/30000 Training Loss: 0.05071856826543808\n",
      "Epoch 17066/30000 Training Loss: 0.050169140100479126\n",
      "Epoch 17067/30000 Training Loss: 0.04354001209139824\n",
      "Epoch 17068/30000 Training Loss: 0.050361040979623795\n",
      "Epoch 17069/30000 Training Loss: 0.04715544357895851\n",
      "Epoch 17070/30000 Training Loss: 0.044836804270744324\n",
      "Epoch 17071/30000 Training Loss: 0.05855117738246918\n",
      "Epoch 17072/30000 Training Loss: 0.04359085485339165\n",
      "Epoch 17073/30000 Training Loss: 0.05493225157260895\n",
      "Epoch 17074/30000 Training Loss: 0.046420373022556305\n",
      "Epoch 17075/30000 Training Loss: 0.049457352608442307\n",
      "Epoch 17076/30000 Training Loss: 0.045279212296009064\n",
      "Epoch 17077/30000 Training Loss: 0.045908380299806595\n",
      "Epoch 17078/30000 Training Loss: 0.03316176310181618\n",
      "Epoch 17079/30000 Training Loss: 0.05001365393400192\n",
      "Epoch 17080/30000 Training Loss: 0.04801518842577934\n",
      "Epoch 17081/30000 Training Loss: 0.04108300805091858\n",
      "Epoch 17082/30000 Training Loss: 0.04651590809226036\n",
      "Epoch 17083/30000 Training Loss: 0.03951285034418106\n",
      "Epoch 17084/30000 Training Loss: 0.051633063703775406\n",
      "Epoch 17085/30000 Training Loss: 0.046680402010679245\n",
      "Epoch 17086/30000 Training Loss: 0.04779263585805893\n",
      "Epoch 17087/30000 Training Loss: 0.05023186653852463\n",
      "Epoch 17088/30000 Training Loss: 0.03187335282564163\n",
      "Epoch 17089/30000 Training Loss: 0.04594315588474274\n",
      "Epoch 17090/30000 Training Loss: 0.044064901769161224\n",
      "Epoch 17091/30000 Training Loss: 0.03540123254060745\n",
      "Epoch 17092/30000 Training Loss: 0.04415849596261978\n",
      "Epoch 17093/30000 Training Loss: 0.05306202545762062\n",
      "Epoch 17094/30000 Training Loss: 0.048897579312324524\n",
      "Epoch 17095/30000 Training Loss: 0.052467212080955505\n",
      "Epoch 17096/30000 Training Loss: 0.06843380630016327\n",
      "Epoch 17097/30000 Training Loss: 0.05919952690601349\n",
      "Epoch 17098/30000 Training Loss: 0.04348892346024513\n",
      "Epoch 17099/30000 Training Loss: 0.04782336577773094\n",
      "Epoch 17100/30000 Training Loss: 0.036885105073451996\n",
      "Epoch 17100/30000 Validation Loss: 0.037472765892744064\n",
      "Epoch 17101/30000 Training Loss: 0.04003213718533516\n",
      "Epoch 17102/30000 Training Loss: 0.041380807757377625\n",
      "Epoch 17103/30000 Training Loss: 0.04963887855410576\n",
      "Epoch 17104/30000 Training Loss: 0.051436230540275574\n",
      "Epoch 17105/30000 Training Loss: 0.06552106142044067\n",
      "Epoch 17106/30000 Training Loss: 0.05996369570493698\n",
      "Epoch 17107/30000 Training Loss: 0.05465543642640114\n",
      "Epoch 17108/30000 Training Loss: 0.067531518638134\n",
      "Epoch 17109/30000 Training Loss: 0.054894063621759415\n",
      "Epoch 17110/30000 Training Loss: 0.05378013476729393\n",
      "Epoch 17111/30000 Training Loss: 0.04276842251420021\n",
      "Epoch 17112/30000 Training Loss: 0.052765145897865295\n",
      "Epoch 17113/30000 Training Loss: 0.05074036121368408\n",
      "Epoch 17114/30000 Training Loss: 0.04993496090173721\n",
      "Epoch 17115/30000 Training Loss: 0.04851805046200752\n",
      "Epoch 17116/30000 Training Loss: 0.06128370016813278\n",
      "Epoch 17117/30000 Training Loss: 0.056989412754774094\n",
      "Epoch 17118/30000 Training Loss: 0.04661351814866066\n",
      "Epoch 17119/30000 Training Loss: 0.04670880734920502\n",
      "Epoch 17120/30000 Training Loss: 0.03971124440431595\n",
      "Epoch 17121/30000 Training Loss: 0.04087928310036659\n",
      "Epoch 17122/30000 Training Loss: 0.042941343039274216\n",
      "Epoch 17123/30000 Training Loss: 0.04073254391551018\n",
      "Epoch 17124/30000 Training Loss: 0.04791036248207092\n",
      "Epoch 17125/30000 Training Loss: 0.031251661479473114\n",
      "Epoch 17126/30000 Training Loss: 0.07460281252861023\n",
      "Epoch 17127/30000 Training Loss: 0.040098898112773895\n",
      "Epoch 17128/30000 Training Loss: 0.061028748750686646\n",
      "Epoch 17129/30000 Training Loss: 0.06002713739871979\n",
      "Epoch 17130/30000 Training Loss: 0.032275617122650146\n",
      "Epoch 17131/30000 Training Loss: 0.0735584944486618\n",
      "Epoch 17132/30000 Training Loss: 0.06361781805753708\n",
      "Epoch 17133/30000 Training Loss: 0.03311306610703468\n",
      "Epoch 17134/30000 Training Loss: 0.04978226497769356\n",
      "Epoch 17135/30000 Training Loss: 0.047455355525016785\n",
      "Epoch 17136/30000 Training Loss: 0.04024529829621315\n",
      "Epoch 17137/30000 Training Loss: 0.053632572293281555\n",
      "Epoch 17138/30000 Training Loss: 0.0473993718624115\n",
      "Epoch 17139/30000 Training Loss: 0.055604949593544006\n",
      "Epoch 17140/30000 Training Loss: 0.04911677539348602\n",
      "Epoch 17141/30000 Training Loss: 0.05747077241539955\n",
      "Epoch 17142/30000 Training Loss: 0.052149802446365356\n",
      "Epoch 17143/30000 Training Loss: 0.051374681293964386\n",
      "Epoch 17144/30000 Training Loss: 0.07062788307666779\n",
      "Epoch 17145/30000 Training Loss: 0.04037239030003548\n",
      "Epoch 17146/30000 Training Loss: 0.044288504868745804\n",
      "Epoch 17147/30000 Training Loss: 0.03413959592580795\n",
      "Epoch 17148/30000 Training Loss: 0.054726868867874146\n",
      "Epoch 17149/30000 Training Loss: 0.04329497367143631\n",
      "Epoch 17150/30000 Training Loss: 0.051452286541461945\n",
      "Epoch 17151/30000 Training Loss: 0.046328168362379074\n",
      "Epoch 17152/30000 Training Loss: 0.046922434121370316\n",
      "Epoch 17153/30000 Training Loss: 0.03971659392118454\n",
      "Epoch 17154/30000 Training Loss: 0.040546223521232605\n",
      "Epoch 17155/30000 Training Loss: 0.06002618744969368\n",
      "Epoch 17156/30000 Training Loss: 0.04854215309023857\n",
      "Epoch 17157/30000 Training Loss: 0.045820362865924835\n",
      "Epoch 17158/30000 Training Loss: 0.04672103375196457\n",
      "Epoch 17159/30000 Training Loss: 0.042041074484586716\n",
      "Epoch 17160/30000 Training Loss: 0.04667378589510918\n",
      "Epoch 17161/30000 Training Loss: 0.04018468037247658\n",
      "Epoch 17162/30000 Training Loss: 0.03684540465474129\n",
      "Epoch 17163/30000 Training Loss: 0.048912450671195984\n",
      "Epoch 17164/30000 Training Loss: 0.0647345781326294\n",
      "Epoch 17165/30000 Training Loss: 0.05370123311877251\n",
      "Epoch 17166/30000 Training Loss: 0.04153093695640564\n",
      "Epoch 17167/30000 Training Loss: 0.038751281797885895\n",
      "Epoch 17168/30000 Training Loss: 0.0485268272459507\n",
      "Epoch 17169/30000 Training Loss: 0.034481726586818695\n",
      "Epoch 17170/30000 Training Loss: 0.03420107066631317\n",
      "Epoch 17171/30000 Training Loss: 0.05077020823955536\n",
      "Epoch 17172/30000 Training Loss: 0.047811202704906464\n",
      "Epoch 17173/30000 Training Loss: 0.04079468920826912\n",
      "Epoch 17174/30000 Training Loss: 0.057150594890117645\n",
      "Epoch 17175/30000 Training Loss: 0.05514633283019066\n",
      "Epoch 17176/30000 Training Loss: 0.04940801113843918\n",
      "Epoch 17177/30000 Training Loss: 0.04521229490637779\n",
      "Epoch 17178/30000 Training Loss: 0.04728974401950836\n",
      "Epoch 17179/30000 Training Loss: 0.08098405599594116\n",
      "Epoch 17180/30000 Training Loss: 0.04605758190155029\n",
      "Epoch 17181/30000 Training Loss: 0.05883822590112686\n",
      "Epoch 17182/30000 Training Loss: 0.06383964419364929\n",
      "Epoch 17183/30000 Training Loss: 0.05586925894021988\n",
      "Epoch 17184/30000 Training Loss: 0.062803715467453\n",
      "Epoch 17185/30000 Training Loss: 0.053496457636356354\n",
      "Epoch 17186/30000 Training Loss: 0.050561681389808655\n",
      "Epoch 17187/30000 Training Loss: 0.04231434315443039\n",
      "Epoch 17188/30000 Training Loss: 0.05550561100244522\n",
      "Epoch 17189/30000 Training Loss: 0.053678032010793686\n",
      "Epoch 17190/30000 Training Loss: 0.05275266617536545\n",
      "Epoch 17191/30000 Training Loss: 0.0360865592956543\n",
      "Epoch 17192/30000 Training Loss: 0.037257466465234756\n",
      "Epoch 17193/30000 Training Loss: 0.04997439682483673\n",
      "Epoch 17194/30000 Training Loss: 0.04012802243232727\n",
      "Epoch 17195/30000 Training Loss: 0.053248997777700424\n",
      "Epoch 17196/30000 Training Loss: 0.049954596906900406\n",
      "Epoch 17197/30000 Training Loss: 0.04758965224027634\n",
      "Epoch 17198/30000 Training Loss: 0.04335924983024597\n",
      "Epoch 17199/30000 Training Loss: 0.04477206990122795\n",
      "Epoch 17200/30000 Training Loss: 0.06566372513771057\n",
      "Epoch 17200/30000 Validation Loss: 0.04768678545951843\n",
      "Epoch 17201/30000 Training Loss: 0.06268522888422012\n",
      "Epoch 17202/30000 Training Loss: 0.05088280141353607\n",
      "Epoch 17203/30000 Training Loss: 0.04499856382608414\n",
      "Epoch 17204/30000 Training Loss: 0.0533963143825531\n",
      "Epoch 17205/30000 Training Loss: 0.03943854942917824\n",
      "Epoch 17206/30000 Training Loss: 0.05270030349493027\n",
      "Epoch 17207/30000 Training Loss: 0.043356314301490784\n",
      "Epoch 17208/30000 Training Loss: 0.05837886407971382\n",
      "Epoch 17209/30000 Training Loss: 0.042934417724609375\n",
      "Epoch 17210/30000 Training Loss: 0.04398028552532196\n",
      "Epoch 17211/30000 Training Loss: 0.05127682536840439\n",
      "Epoch 17212/30000 Training Loss: 0.03901620954275131\n",
      "Epoch 17213/30000 Training Loss: 0.056146420538425446\n",
      "Epoch 17214/30000 Training Loss: 0.04800701141357422\n",
      "Epoch 17215/30000 Training Loss: 0.05898284539580345\n",
      "Epoch 17216/30000 Training Loss: 0.06148421764373779\n",
      "Epoch 17217/30000 Training Loss: 0.04661785811185837\n",
      "Epoch 17218/30000 Training Loss: 0.05442989617586136\n",
      "Epoch 17219/30000 Training Loss: 0.06596459448337555\n",
      "Epoch 17220/30000 Training Loss: 0.047974832355976105\n",
      "Epoch 17221/30000 Training Loss: 0.03615465760231018\n",
      "Epoch 17222/30000 Training Loss: 0.03975938260555267\n",
      "Epoch 17223/30000 Training Loss: 0.07392920553684235\n",
      "Epoch 17224/30000 Training Loss: 0.0479324571788311\n",
      "Epoch 17225/30000 Training Loss: 0.03905650973320007\n",
      "Epoch 17226/30000 Training Loss: 0.052733536809682846\n",
      "Epoch 17227/30000 Training Loss: 0.04492419585585594\n",
      "Epoch 17228/30000 Training Loss: 0.04672101512551308\n",
      "Epoch 17229/30000 Training Loss: 0.05137372761964798\n",
      "Epoch 17230/30000 Training Loss: 0.04867677763104439\n",
      "Epoch 17231/30000 Training Loss: 0.035103246569633484\n",
      "Epoch 17232/30000 Training Loss: 0.05310219153761864\n",
      "Epoch 17233/30000 Training Loss: 0.0557539127767086\n",
      "Epoch 17234/30000 Training Loss: 0.043380770832300186\n",
      "Epoch 17235/30000 Training Loss: 0.043916966766119\n",
      "Epoch 17236/30000 Training Loss: 0.06167587265372276\n",
      "Epoch 17237/30000 Training Loss: 0.051098812371492386\n",
      "Epoch 17238/30000 Training Loss: 0.06148916855454445\n",
      "Epoch 17239/30000 Training Loss: 0.04837802052497864\n",
      "Epoch 17240/30000 Training Loss: 0.06197240948677063\n",
      "Epoch 17241/30000 Training Loss: 0.03927457332611084\n",
      "Epoch 17242/30000 Training Loss: 0.048046234995126724\n",
      "Epoch 17243/30000 Training Loss: 0.07276087999343872\n",
      "Epoch 17244/30000 Training Loss: 0.03220955282449722\n",
      "Epoch 17245/30000 Training Loss: 0.0379067026078701\n",
      "Epoch 17246/30000 Training Loss: 0.053677137941122055\n",
      "Epoch 17247/30000 Training Loss: 0.04257592931389809\n",
      "Epoch 17248/30000 Training Loss: 0.037260085344314575\n",
      "Epoch 17249/30000 Training Loss: 0.054438911378383636\n",
      "Epoch 17250/30000 Training Loss: 0.05754248797893524\n",
      "Epoch 17251/30000 Training Loss: 0.03280463069677353\n",
      "Epoch 17252/30000 Training Loss: 0.0435352548956871\n",
      "Epoch 17253/30000 Training Loss: 0.037299178540706635\n",
      "Epoch 17254/30000 Training Loss: 0.04824065789580345\n",
      "Epoch 17255/30000 Training Loss: 0.05942675843834877\n",
      "Epoch 17256/30000 Training Loss: 0.03916923701763153\n",
      "Epoch 17257/30000 Training Loss: 0.046631988137960434\n",
      "Epoch 17258/30000 Training Loss: 0.0437958762049675\n",
      "Epoch 17259/30000 Training Loss: 0.046302907168865204\n",
      "Epoch 17260/30000 Training Loss: 0.04288987070322037\n",
      "Epoch 17261/30000 Training Loss: 0.06078390032052994\n",
      "Epoch 17262/30000 Training Loss: 0.03299684077501297\n",
      "Epoch 17263/30000 Training Loss: 0.04786041006445885\n",
      "Epoch 17264/30000 Training Loss: 0.04773654043674469\n",
      "Epoch 17265/30000 Training Loss: 0.03983673080801964\n",
      "Epoch 17266/30000 Training Loss: 0.0354134663939476\n",
      "Epoch 17267/30000 Training Loss: 0.05131107196211815\n",
      "Epoch 17268/30000 Training Loss: 0.05756642669439316\n",
      "Epoch 17269/30000 Training Loss: 0.058341026306152344\n",
      "Epoch 17270/30000 Training Loss: 0.04236786440014839\n",
      "Epoch 17271/30000 Training Loss: 0.06563610583543777\n",
      "Epoch 17272/30000 Training Loss: 0.040403708815574646\n",
      "Epoch 17273/30000 Training Loss: 0.04907409846782684\n",
      "Epoch 17274/30000 Training Loss: 0.04661642760038376\n",
      "Epoch 17275/30000 Training Loss: 0.05449266731739044\n",
      "Epoch 17276/30000 Training Loss: 0.04512950778007507\n",
      "Epoch 17277/30000 Training Loss: 0.05108651518821716\n",
      "Epoch 17278/30000 Training Loss: 0.043051451444625854\n",
      "Epoch 17279/30000 Training Loss: 0.02961992658674717\n",
      "Epoch 17280/30000 Training Loss: 0.0463029108941555\n",
      "Epoch 17281/30000 Training Loss: 0.047324053943157196\n",
      "Epoch 17282/30000 Training Loss: 0.04604944959282875\n",
      "Epoch 17283/30000 Training Loss: 0.051978886127471924\n",
      "Epoch 17284/30000 Training Loss: 0.04129252955317497\n",
      "Epoch 17285/30000 Training Loss: 0.0499628521502018\n",
      "Epoch 17286/30000 Training Loss: 0.04482990875840187\n",
      "Epoch 17287/30000 Training Loss: 0.04907967895269394\n",
      "Epoch 17288/30000 Training Loss: 0.044833775609731674\n",
      "Epoch 17289/30000 Training Loss: 0.0320432074368\n",
      "Epoch 17290/30000 Training Loss: 0.0428120419383049\n",
      "Epoch 17291/30000 Training Loss: 0.02747163362801075\n",
      "Epoch 17292/30000 Training Loss: 0.03887924551963806\n",
      "Epoch 17293/30000 Training Loss: 0.042592503130435944\n",
      "Epoch 17294/30000 Training Loss: 0.03524789214134216\n",
      "Epoch 17295/30000 Training Loss: 0.06335674226284027\n",
      "Epoch 17296/30000 Training Loss: 0.04933582991361618\n",
      "Epoch 17297/30000 Training Loss: 0.047380365431308746\n",
      "Epoch 17298/30000 Training Loss: 0.056789204478263855\n",
      "Epoch 17299/30000 Training Loss: 0.047448866069316864\n",
      "Epoch 17300/30000 Training Loss: 0.05846518650650978\n",
      "Epoch 17300/30000 Validation Loss: 0.04322824254631996\n",
      "Epoch 17301/30000 Training Loss: 0.03767961263656616\n",
      "Epoch 17302/30000 Training Loss: 0.05618554353713989\n",
      "Epoch 17303/30000 Training Loss: 0.035376470535993576\n",
      "Epoch 17304/30000 Training Loss: 0.06813932955265045\n",
      "Epoch 17305/30000 Training Loss: 0.057320185005664825\n",
      "Epoch 17306/30000 Training Loss: 0.04308285564184189\n",
      "Epoch 17307/30000 Training Loss: 0.04876800999045372\n",
      "Epoch 17308/30000 Training Loss: 0.037753526121377945\n",
      "Epoch 17309/30000 Training Loss: 0.0606515035033226\n",
      "Epoch 17310/30000 Training Loss: 0.04086417704820633\n",
      "Epoch 17311/30000 Training Loss: 0.05881115794181824\n",
      "Epoch 17312/30000 Training Loss: 0.04519759118556976\n",
      "Epoch 17313/30000 Training Loss: 0.04475470259785652\n",
      "Epoch 17314/30000 Training Loss: 0.03811554238200188\n",
      "Epoch 17315/30000 Training Loss: 0.047736331820487976\n",
      "Epoch 17316/30000 Training Loss: 0.051679786294698715\n",
      "Epoch 17317/30000 Training Loss: 0.037793733179569244\n",
      "Epoch 17318/30000 Training Loss: 0.04427023604512215\n",
      "Epoch 17319/30000 Training Loss: 0.04188252240419388\n",
      "Epoch 17320/30000 Training Loss: 0.0612175315618515\n",
      "Epoch 17321/30000 Training Loss: 0.04675732180476189\n",
      "Epoch 17322/30000 Training Loss: 0.04573872312903404\n",
      "Epoch 17323/30000 Training Loss: 0.04675182327628136\n",
      "Epoch 17324/30000 Training Loss: 0.03976912796497345\n",
      "Epoch 17325/30000 Training Loss: 0.0483856201171875\n",
      "Epoch 17326/30000 Training Loss: 0.04251449555158615\n",
      "Epoch 17327/30000 Training Loss: 0.061031341552734375\n",
      "Epoch 17328/30000 Training Loss: 0.04387274757027626\n",
      "Epoch 17329/30000 Training Loss: 0.051932595670223236\n",
      "Epoch 17330/30000 Training Loss: 0.05189962685108185\n",
      "Epoch 17331/30000 Training Loss: 0.04903661459684372\n",
      "Epoch 17332/30000 Training Loss: 0.05229157209396362\n",
      "Epoch 17333/30000 Training Loss: 0.054646026343107224\n",
      "Epoch 17334/30000 Training Loss: 0.04980667680501938\n",
      "Epoch 17335/30000 Training Loss: 0.041346605867147446\n",
      "Epoch 17336/30000 Training Loss: 0.048527128994464874\n",
      "Epoch 17337/30000 Training Loss: 0.05190370976924896\n",
      "Epoch 17338/30000 Training Loss: 0.048541489988565445\n",
      "Epoch 17339/30000 Training Loss: 0.03749069198966026\n",
      "Epoch 17340/30000 Training Loss: 0.05069303512573242\n",
      "Epoch 17341/30000 Training Loss: 0.04419362545013428\n",
      "Epoch 17342/30000 Training Loss: 0.06794734299182892\n",
      "Epoch 17343/30000 Training Loss: 0.03860734403133392\n",
      "Epoch 17344/30000 Training Loss: 0.04121480509638786\n",
      "Epoch 17345/30000 Training Loss: 0.049381714314222336\n",
      "Epoch 17346/30000 Training Loss: 0.0413544587790966\n",
      "Epoch 17347/30000 Training Loss: 0.05883796513080597\n",
      "Epoch 17348/30000 Training Loss: 0.04005696624517441\n",
      "Epoch 17349/30000 Training Loss: 0.03930424898862839\n",
      "Epoch 17350/30000 Training Loss: 0.055436503142118454\n",
      "Epoch 17351/30000 Training Loss: 0.05849384516477585\n",
      "Epoch 17352/30000 Training Loss: 0.032359007745981216\n",
      "Epoch 17353/30000 Training Loss: 0.05263078957796097\n",
      "Epoch 17354/30000 Training Loss: 0.03891349956393242\n",
      "Epoch 17355/30000 Training Loss: 0.038770657032728195\n",
      "Epoch 17356/30000 Training Loss: 0.05379984527826309\n",
      "Epoch 17357/30000 Training Loss: 0.04186706244945526\n",
      "Epoch 17358/30000 Training Loss: 0.054432958364486694\n",
      "Epoch 17359/30000 Training Loss: 0.060464195907115936\n",
      "Epoch 17360/30000 Training Loss: 0.038053520023822784\n",
      "Epoch 17361/30000 Training Loss: 0.04624097794294357\n",
      "Epoch 17362/30000 Training Loss: 0.04477016627788544\n",
      "Epoch 17363/30000 Training Loss: 0.03608604520559311\n",
      "Epoch 17364/30000 Training Loss: 0.04883440583944321\n",
      "Epoch 17365/30000 Training Loss: 0.04683306813240051\n",
      "Epoch 17366/30000 Training Loss: 0.04527810961008072\n",
      "Epoch 17367/30000 Training Loss: 0.045938391238451004\n",
      "Epoch 17368/30000 Training Loss: 0.07150664180517197\n",
      "Epoch 17369/30000 Training Loss: 0.032035116106271744\n",
      "Epoch 17370/30000 Training Loss: 0.05454149469733238\n",
      "Epoch 17371/30000 Training Loss: 0.048407770693302155\n",
      "Epoch 17372/30000 Training Loss: 0.052926499396562576\n",
      "Epoch 17373/30000 Training Loss: 0.06207582354545593\n",
      "Epoch 17374/30000 Training Loss: 0.05797329545021057\n",
      "Epoch 17375/30000 Training Loss: 0.07335856556892395\n",
      "Epoch 17376/30000 Training Loss: 0.05202425643801689\n",
      "Epoch 17377/30000 Training Loss: 0.06552387028932571\n",
      "Epoch 17378/30000 Training Loss: 0.04167573153972626\n",
      "Epoch 17379/30000 Training Loss: 0.02716498076915741\n",
      "Epoch 17380/30000 Training Loss: 0.06365504860877991\n",
      "Epoch 17381/30000 Training Loss: 0.052719391882419586\n",
      "Epoch 17382/30000 Training Loss: 0.06412387639284134\n",
      "Epoch 17383/30000 Training Loss: 0.050661880522966385\n",
      "Epoch 17384/30000 Training Loss: 0.056529249995946884\n",
      "Epoch 17385/30000 Training Loss: 0.0483023002743721\n",
      "Epoch 17386/30000 Training Loss: 0.05940721929073334\n",
      "Epoch 17387/30000 Training Loss: 0.04102392494678497\n",
      "Epoch 17388/30000 Training Loss: 0.05708140879869461\n",
      "Epoch 17389/30000 Training Loss: 0.05212992802262306\n",
      "Epoch 17390/30000 Training Loss: 0.038513753563165665\n",
      "Epoch 17391/30000 Training Loss: 0.05070066824555397\n",
      "Epoch 17392/30000 Training Loss: 0.04909723624587059\n",
      "Epoch 17393/30000 Training Loss: 0.0554475337266922\n",
      "Epoch 17394/30000 Training Loss: 0.055797599256038666\n",
      "Epoch 17395/30000 Training Loss: 0.0356343574821949\n",
      "Epoch 17396/30000 Training Loss: 0.04713795334100723\n",
      "Epoch 17397/30000 Training Loss: 0.06584521383047104\n",
      "Epoch 17398/30000 Training Loss: 0.04459518566727638\n",
      "Epoch 17399/30000 Training Loss: 0.05400695651769638\n",
      "Epoch 17400/30000 Training Loss: 0.07471653074026108\n",
      "Epoch 17400/30000 Validation Loss: 0.04758603498339653\n",
      "Epoch 17401/30000 Training Loss: 0.05631644278764725\n",
      "Epoch 17402/30000 Training Loss: 0.03151135891675949\n",
      "Epoch 17403/30000 Training Loss: 0.05262673646211624\n",
      "Epoch 17404/30000 Training Loss: 0.0468408577144146\n",
      "Epoch 17405/30000 Training Loss: 0.04815391078591347\n",
      "Epoch 17406/30000 Training Loss: 0.0426400862634182\n",
      "Epoch 17407/30000 Training Loss: 0.03704283386468887\n",
      "Epoch 17408/30000 Training Loss: 0.06282434612512589\n",
      "Epoch 17409/30000 Training Loss: 0.055581867694854736\n",
      "Epoch 17410/30000 Training Loss: 0.03877146542072296\n",
      "Epoch 17411/30000 Training Loss: 0.0488559752702713\n",
      "Epoch 17412/30000 Training Loss: 0.04275907576084137\n",
      "Epoch 17413/30000 Training Loss: 0.04744582250714302\n",
      "Epoch 17414/30000 Training Loss: 0.04869993031024933\n",
      "Epoch 17415/30000 Training Loss: 0.05655640363693237\n",
      "Epoch 17416/30000 Training Loss: 0.049708642065525055\n",
      "Epoch 17417/30000 Training Loss: 0.04390096664428711\n",
      "Epoch 17418/30000 Training Loss: 0.05959552526473999\n",
      "Epoch 17419/30000 Training Loss: 0.04473168030381203\n",
      "Epoch 17420/30000 Training Loss: 0.06479932367801666\n",
      "Epoch 17421/30000 Training Loss: 0.05094707012176514\n",
      "Epoch 17422/30000 Training Loss: 0.046848829835653305\n",
      "Epoch 17423/30000 Training Loss: 0.05818983539938927\n",
      "Epoch 17424/30000 Training Loss: 0.05513351410627365\n",
      "Epoch 17425/30000 Training Loss: 0.035770997405052185\n",
      "Epoch 17426/30000 Training Loss: 0.05127345398068428\n",
      "Epoch 17427/30000 Training Loss: 0.06671018898487091\n",
      "Epoch 17428/30000 Training Loss: 0.04074903950095177\n",
      "Epoch 17429/30000 Training Loss: 0.054388076066970825\n",
      "Epoch 17430/30000 Training Loss: 0.04222430661320686\n",
      "Epoch 17431/30000 Training Loss: 0.051873624324798584\n",
      "Epoch 17432/30000 Training Loss: 0.0527154840528965\n",
      "Epoch 17433/30000 Training Loss: 0.0469634085893631\n",
      "Epoch 17434/30000 Training Loss: 0.03576316311955452\n",
      "Epoch 17435/30000 Training Loss: 0.04032820463180542\n",
      "Epoch 17436/30000 Training Loss: 0.04571599140763283\n",
      "Epoch 17437/30000 Training Loss: 0.03866080939769745\n",
      "Epoch 17438/30000 Training Loss: 0.03727857023477554\n",
      "Epoch 17439/30000 Training Loss: 0.044204823672771454\n",
      "Epoch 17440/30000 Training Loss: 0.034469105303287506\n",
      "Epoch 17441/30000 Training Loss: 0.053471073508262634\n",
      "Epoch 17442/30000 Training Loss: 0.03726107254624367\n",
      "Epoch 17443/30000 Training Loss: 0.041282422840595245\n",
      "Epoch 17444/30000 Training Loss: 0.049682989716529846\n",
      "Epoch 17445/30000 Training Loss: 0.048236142843961716\n",
      "Epoch 17446/30000 Training Loss: 0.03889267146587372\n",
      "Epoch 17447/30000 Training Loss: 0.047358620911836624\n",
      "Epoch 17448/30000 Training Loss: 0.05931568890810013\n",
      "Epoch 17449/30000 Training Loss: 0.06340992450714111\n",
      "Epoch 17450/30000 Training Loss: 0.04333259165287018\n",
      "Epoch 17451/30000 Training Loss: 0.048940401524305344\n",
      "Epoch 17452/30000 Training Loss: 0.053046345710754395\n",
      "Epoch 17453/30000 Training Loss: 0.046609967947006226\n",
      "Epoch 17454/30000 Training Loss: 0.04641960933804512\n",
      "Epoch 17455/30000 Training Loss: 0.05184698477387428\n",
      "Epoch 17456/30000 Training Loss: 0.045542601495981216\n",
      "Epoch 17457/30000 Training Loss: 0.03427924960851669\n",
      "Epoch 17458/30000 Training Loss: 0.05409983545541763\n",
      "Epoch 17459/30000 Training Loss: 0.053353577852249146\n",
      "Epoch 17460/30000 Training Loss: 0.06447151303291321\n",
      "Epoch 17461/30000 Training Loss: 0.0379180982708931\n",
      "Epoch 17462/30000 Training Loss: 0.04992494732141495\n",
      "Epoch 17463/30000 Training Loss: 0.048490121960639954\n",
      "Epoch 17464/30000 Training Loss: 0.06506478041410446\n",
      "Epoch 17465/30000 Training Loss: 0.057341620326042175\n",
      "Epoch 17466/30000 Training Loss: 0.050799690186977386\n",
      "Epoch 17467/30000 Training Loss: 0.044973570853471756\n",
      "Epoch 17468/30000 Training Loss: 0.0638868659734726\n",
      "Epoch 17469/30000 Training Loss: 0.04469722509384155\n",
      "Epoch 17470/30000 Training Loss: 0.06326492130756378\n",
      "Epoch 17471/30000 Training Loss: 0.06196323782205582\n",
      "Epoch 17472/30000 Training Loss: 0.060015514492988586\n",
      "Epoch 17473/30000 Training Loss: 0.04458751529455185\n",
      "Epoch 17474/30000 Training Loss: 0.057763561606407166\n",
      "Epoch 17475/30000 Training Loss: 0.04191208258271217\n",
      "Epoch 17476/30000 Training Loss: 0.059552595019340515\n",
      "Epoch 17477/30000 Training Loss: 0.052426330745220184\n",
      "Epoch 17478/30000 Training Loss: 0.06267721205949783\n",
      "Epoch 17479/30000 Training Loss: 0.05541360005736351\n",
      "Epoch 17480/30000 Training Loss: 0.053667329251766205\n",
      "Epoch 17481/30000 Training Loss: 0.039623912423849106\n",
      "Epoch 17482/30000 Training Loss: 0.07586771994829178\n",
      "Epoch 17483/30000 Training Loss: 0.06087105721235275\n",
      "Epoch 17484/30000 Training Loss: 0.05821608006954193\n",
      "Epoch 17485/30000 Training Loss: 0.05374092608690262\n",
      "Epoch 17486/30000 Training Loss: 0.06392870098352432\n",
      "Epoch 17487/30000 Training Loss: 0.037499599158763885\n",
      "Epoch 17488/30000 Training Loss: 0.04828639701008797\n",
      "Epoch 17489/30000 Training Loss: 0.04268890619277954\n",
      "Epoch 17490/30000 Training Loss: 0.051088642328977585\n",
      "Epoch 17491/30000 Training Loss: 0.04455440118908882\n",
      "Epoch 17492/30000 Training Loss: 0.04771006852388382\n",
      "Epoch 17493/30000 Training Loss: 0.06369872391223907\n",
      "Epoch 17494/30000 Training Loss: 0.045526474714279175\n",
      "Epoch 17495/30000 Training Loss: 0.059217795729637146\n",
      "Epoch 17496/30000 Training Loss: 0.0316733717918396\n",
      "Epoch 17497/30000 Training Loss: 0.055151455104351044\n",
      "Epoch 17498/30000 Training Loss: 0.04629047214984894\n",
      "Epoch 17499/30000 Training Loss: 0.04266709089279175\n",
      "Epoch 17500/30000 Training Loss: 0.03301999345421791\n",
      "Epoch 17500/30000 Validation Loss: 0.05494493246078491\n",
      "Epoch 17501/30000 Training Loss: 0.04264865815639496\n",
      "Epoch 17502/30000 Training Loss: 0.0693998783826828\n",
      "Epoch 17503/30000 Training Loss: 0.041866932064294815\n",
      "Epoch 17504/30000 Training Loss: 0.06837200373411179\n",
      "Epoch 17505/30000 Training Loss: 0.052268944680690765\n",
      "Epoch 17506/30000 Training Loss: 0.04634976014494896\n",
      "Epoch 17507/30000 Training Loss: 0.0471978522837162\n",
      "Epoch 17508/30000 Training Loss: 0.04775470867753029\n",
      "Epoch 17509/30000 Training Loss: 0.05620826780796051\n",
      "Epoch 17510/30000 Training Loss: 0.044076405465602875\n",
      "Epoch 17511/30000 Training Loss: 0.041211605072021484\n",
      "Epoch 17512/30000 Training Loss: 0.045777540653944016\n",
      "Epoch 17513/30000 Training Loss: 0.045584581792354584\n",
      "Epoch 17514/30000 Training Loss: 0.04755210503935814\n",
      "Epoch 17515/30000 Training Loss: 0.045265667140483856\n",
      "Epoch 17516/30000 Training Loss: 0.049654506146907806\n",
      "Epoch 17517/30000 Training Loss: 0.061582908034324646\n",
      "Epoch 17518/30000 Training Loss: 0.0561886727809906\n",
      "Epoch 17519/30000 Training Loss: 0.06358496844768524\n",
      "Epoch 17520/30000 Training Loss: 0.055288512259721756\n",
      "Epoch 17521/30000 Training Loss: 0.04923877492547035\n",
      "Epoch 17522/30000 Training Loss: 0.035597145557403564\n",
      "Epoch 17523/30000 Training Loss: 0.04604871943593025\n",
      "Epoch 17524/30000 Training Loss: 0.04472609609365463\n",
      "Epoch 17525/30000 Training Loss: 0.08290920406579971\n",
      "Epoch 17526/30000 Training Loss: 0.04180500656366348\n",
      "Epoch 17527/30000 Training Loss: 0.05581791698932648\n",
      "Epoch 17528/30000 Training Loss: 0.04493406414985657\n",
      "Epoch 17529/30000 Training Loss: 0.03766729682683945\n",
      "Epoch 17530/30000 Training Loss: 0.04511461406946182\n",
      "Epoch 17531/30000 Training Loss: 0.04776614159345627\n",
      "Epoch 17532/30000 Training Loss: 0.05024261027574539\n",
      "Epoch 17533/30000 Training Loss: 0.05191054195165634\n",
      "Epoch 17534/30000 Training Loss: 0.0623941645026207\n",
      "Epoch 17535/30000 Training Loss: 0.036888621747493744\n",
      "Epoch 17536/30000 Training Loss: 0.05236170440912247\n",
      "Epoch 17537/30000 Training Loss: 0.07033690810203552\n",
      "Epoch 17538/30000 Training Loss: 0.02881021983921528\n",
      "Epoch 17539/30000 Training Loss: 0.057069115340709686\n",
      "Epoch 17540/30000 Training Loss: 0.06085049360990524\n",
      "Epoch 17541/30000 Training Loss: 0.03673851862549782\n",
      "Epoch 17542/30000 Training Loss: 0.03848923370242119\n",
      "Epoch 17543/30000 Training Loss: 0.039263997226953506\n",
      "Epoch 17544/30000 Training Loss: 0.05001071095466614\n",
      "Epoch 17545/30000 Training Loss: 0.05419059097766876\n",
      "Epoch 17546/30000 Training Loss: 0.046762168407440186\n",
      "Epoch 17547/30000 Training Loss: 0.05081922188401222\n",
      "Epoch 17548/30000 Training Loss: 0.04506884887814522\n",
      "Epoch 17549/30000 Training Loss: 0.05457904934883118\n",
      "Epoch 17550/30000 Training Loss: 0.057138726115226746\n",
      "Epoch 17551/30000 Training Loss: 0.05902860313653946\n",
      "Epoch 17552/30000 Training Loss: 0.050222769379615784\n",
      "Epoch 17553/30000 Training Loss: 0.04598957300186157\n",
      "Epoch 17554/30000 Training Loss: 0.03729246184229851\n",
      "Epoch 17555/30000 Training Loss: 0.04946400970220566\n",
      "Epoch 17556/30000 Training Loss: 0.05293785780668259\n",
      "Epoch 17557/30000 Training Loss: 0.0439874567091465\n",
      "Epoch 17558/30000 Training Loss: 0.05287899076938629\n",
      "Epoch 17559/30000 Training Loss: 0.040039870887994766\n",
      "Epoch 17560/30000 Training Loss: 0.06167560815811157\n",
      "Epoch 17561/30000 Training Loss: 0.04089115560054779\n",
      "Epoch 17562/30000 Training Loss: 0.043630827218294144\n",
      "Epoch 17563/30000 Training Loss: 0.052848782390356064\n",
      "Epoch 17564/30000 Training Loss: 0.05115441977977753\n",
      "Epoch 17565/30000 Training Loss: 0.03810671716928482\n",
      "Epoch 17566/30000 Training Loss: 0.04256788268685341\n",
      "Epoch 17567/30000 Training Loss: 0.04947454854846001\n",
      "Epoch 17568/30000 Training Loss: 0.05107972025871277\n",
      "Epoch 17569/30000 Training Loss: 0.03235746920108795\n",
      "Epoch 17570/30000 Training Loss: 0.03942585363984108\n",
      "Epoch 17571/30000 Training Loss: 0.056514665484428406\n",
      "Epoch 17572/30000 Training Loss: 0.06685227900743484\n",
      "Epoch 17573/30000 Training Loss: 0.0753045380115509\n",
      "Epoch 17574/30000 Training Loss: 0.03967094048857689\n",
      "Epoch 17575/30000 Training Loss: 0.06033939868211746\n",
      "Epoch 17576/30000 Training Loss: 0.04103383794426918\n",
      "Epoch 17577/30000 Training Loss: 0.051820144057273865\n",
      "Epoch 17578/30000 Training Loss: 0.0426812618970871\n",
      "Epoch 17579/30000 Training Loss: 0.030979827046394348\n",
      "Epoch 17580/30000 Training Loss: 0.049428559839725494\n",
      "Epoch 17581/30000 Training Loss: 0.056344836950302124\n",
      "Epoch 17582/30000 Training Loss: 0.047285936772823334\n",
      "Epoch 17583/30000 Training Loss: 0.036733079701662064\n",
      "Epoch 17584/30000 Training Loss: 0.03904261440038681\n",
      "Epoch 17585/30000 Training Loss: 0.046933334320783615\n",
      "Epoch 17586/30000 Training Loss: 0.04193343594670296\n",
      "Epoch 17587/30000 Training Loss: 0.0448874868452549\n",
      "Epoch 17588/30000 Training Loss: 0.04317303001880646\n",
      "Epoch 17589/30000 Training Loss: 0.043420735746622086\n",
      "Epoch 17590/30000 Training Loss: 0.054953888058662415\n",
      "Epoch 17591/30000 Training Loss: 0.07569695264101028\n",
      "Epoch 17592/30000 Training Loss: 0.03855591267347336\n",
      "Epoch 17593/30000 Training Loss: 0.04670776054263115\n",
      "Epoch 17594/30000 Training Loss: 0.06569214165210724\n",
      "Epoch 17595/30000 Training Loss: 0.04387098550796509\n",
      "Epoch 17596/30000 Training Loss: 0.04991278052330017\n",
      "Epoch 17597/30000 Training Loss: 0.04958982765674591\n",
      "Epoch 17598/30000 Training Loss: 0.061000972986221313\n",
      "Epoch 17599/30000 Training Loss: 0.038237832486629486\n",
      "Epoch 17600/30000 Training Loss: 0.060080237686634064\n",
      "Epoch 17600/30000 Validation Loss: 0.04782431200146675\n",
      "Epoch 17601/30000 Training Loss: 0.04219011589884758\n",
      "Epoch 17602/30000 Training Loss: 0.041278161108493805\n",
      "Epoch 17603/30000 Training Loss: 0.04019676893949509\n",
      "Epoch 17604/30000 Training Loss: 0.06903205066919327\n",
      "Epoch 17605/30000 Training Loss: 0.05299719050526619\n",
      "Epoch 17606/30000 Training Loss: 0.05003134161233902\n",
      "Epoch 17607/30000 Training Loss: 0.055364202708005905\n",
      "Epoch 17608/30000 Training Loss: 0.0648154765367508\n",
      "Epoch 17609/30000 Training Loss: 0.052421264350414276\n",
      "Epoch 17610/30000 Training Loss: 0.03388991206884384\n",
      "Epoch 17611/30000 Training Loss: 0.0515885204076767\n",
      "Epoch 17612/30000 Training Loss: 0.04282306134700775\n",
      "Epoch 17613/30000 Training Loss: 0.04034608602523804\n",
      "Epoch 17614/30000 Training Loss: 0.043314602226018906\n",
      "Epoch 17615/30000 Training Loss: 0.05676207318902016\n",
      "Epoch 17616/30000 Training Loss: 0.07321303337812424\n",
      "Epoch 17617/30000 Training Loss: 0.059331487864255905\n",
      "Epoch 17618/30000 Training Loss: 0.046502143144607544\n",
      "Epoch 17619/30000 Training Loss: 0.04095657169818878\n",
      "Epoch 17620/30000 Training Loss: 0.06876246631145477\n",
      "Epoch 17621/30000 Training Loss: 0.04891819506883621\n",
      "Epoch 17622/30000 Training Loss: 0.06082408130168915\n",
      "Epoch 17623/30000 Training Loss: 0.04920735955238342\n",
      "Epoch 17624/30000 Training Loss: 0.039503876119852066\n",
      "Epoch 17625/30000 Training Loss: 0.06709642708301544\n",
      "Epoch 17626/30000 Training Loss: 0.046906597912311554\n",
      "Epoch 17627/30000 Training Loss: 0.04076702520251274\n",
      "Epoch 17628/30000 Training Loss: 0.0526726059615612\n",
      "Epoch 17629/30000 Training Loss: 0.04235551506280899\n",
      "Epoch 17630/30000 Training Loss: 0.05868207663297653\n",
      "Epoch 17631/30000 Training Loss: 0.056097809225320816\n",
      "Epoch 17632/30000 Training Loss: 0.0539468377828598\n",
      "Epoch 17633/30000 Training Loss: 0.05930281803011894\n",
      "Epoch 17634/30000 Training Loss: 0.04989999532699585\n",
      "Epoch 17635/30000 Training Loss: 0.0465569868683815\n",
      "Epoch 17636/30000 Training Loss: 0.06235404312610626\n",
      "Epoch 17637/30000 Training Loss: 0.04764411598443985\n",
      "Epoch 17638/30000 Training Loss: 0.038424838334321976\n",
      "Epoch 17639/30000 Training Loss: 0.03591465577483177\n",
      "Epoch 17640/30000 Training Loss: 0.04769570380449295\n",
      "Epoch 17641/30000 Training Loss: 0.04756598547101021\n",
      "Epoch 17642/30000 Training Loss: 0.03749347850680351\n",
      "Epoch 17643/30000 Training Loss: 0.046538345515728\n",
      "Epoch 17644/30000 Training Loss: 0.056173328310251236\n",
      "Epoch 17645/30000 Training Loss: 0.05097752809524536\n",
      "Epoch 17646/30000 Training Loss: 0.04375956580042839\n",
      "Epoch 17647/30000 Training Loss: 0.04083573818206787\n",
      "Epoch 17648/30000 Training Loss: 0.05055500566959381\n",
      "Epoch 17649/30000 Training Loss: 0.0352257564663887\n",
      "Epoch 17650/30000 Training Loss: 0.03937912732362747\n",
      "Epoch 17651/30000 Training Loss: 0.06674568355083466\n",
      "Epoch 17652/30000 Training Loss: 0.03922111913561821\n",
      "Epoch 17653/30000 Training Loss: 0.04661163315176964\n",
      "Epoch 17654/30000 Training Loss: 0.04772381857037544\n",
      "Epoch 17655/30000 Training Loss: 0.050569530576467514\n",
      "Epoch 17656/30000 Training Loss: 0.054162073880434036\n",
      "Epoch 17657/30000 Training Loss: 0.04964170232415199\n",
      "Epoch 17658/30000 Training Loss: 0.03689062222838402\n",
      "Epoch 17659/30000 Training Loss: 0.04076676815748215\n",
      "Epoch 17660/30000 Training Loss: 0.05308477580547333\n",
      "Epoch 17661/30000 Training Loss: 0.04736865684390068\n",
      "Epoch 17662/30000 Training Loss: 0.050806883722543716\n",
      "Epoch 17663/30000 Training Loss: 0.03581009805202484\n",
      "Epoch 17664/30000 Training Loss: 0.05427985638380051\n",
      "Epoch 17665/30000 Training Loss: 0.04530246928334236\n",
      "Epoch 17666/30000 Training Loss: 0.04421752691268921\n",
      "Epoch 17667/30000 Training Loss: 0.035910170525312424\n",
      "Epoch 17668/30000 Training Loss: 0.06018854305148125\n",
      "Epoch 17669/30000 Training Loss: 0.05104565620422363\n",
      "Epoch 17670/30000 Training Loss: 0.03925437480211258\n",
      "Epoch 17671/30000 Training Loss: 0.06268172711133957\n",
      "Epoch 17672/30000 Training Loss: 0.03912228345870972\n",
      "Epoch 17673/30000 Training Loss: 0.05384443327784538\n",
      "Epoch 17674/30000 Training Loss: 0.035282935947179794\n",
      "Epoch 17675/30000 Training Loss: 0.05838405713438988\n",
      "Epoch 17676/30000 Training Loss: 0.03792720288038254\n",
      "Epoch 17677/30000 Training Loss: 0.050788186490535736\n",
      "Epoch 17678/30000 Training Loss: 0.04653484746813774\n",
      "Epoch 17679/30000 Training Loss: 0.03807636350393295\n",
      "Epoch 17680/30000 Training Loss: 0.033843252807855606\n",
      "Epoch 17681/30000 Training Loss: 0.04857964441180229\n",
      "Epoch 17682/30000 Training Loss: 0.04240495339035988\n",
      "Epoch 17683/30000 Training Loss: 0.059166863560676575\n",
      "Epoch 17684/30000 Training Loss: 0.06545047461986542\n",
      "Epoch 17685/30000 Training Loss: 0.04658404365181923\n",
      "Epoch 17686/30000 Training Loss: 0.06210072338581085\n",
      "Epoch 17687/30000 Training Loss: 0.03971255198121071\n",
      "Epoch 17688/30000 Training Loss: 0.04472929239273071\n",
      "Epoch 17689/30000 Training Loss: 0.061272162944078445\n",
      "Epoch 17690/30000 Training Loss: 0.037113942205905914\n",
      "Epoch 17691/30000 Training Loss: 0.06154996156692505\n",
      "Epoch 17692/30000 Training Loss: 0.04662546142935753\n",
      "Epoch 17693/30000 Training Loss: 0.046637337654829025\n",
      "Epoch 17694/30000 Training Loss: 0.04866462200880051\n",
      "Epoch 17695/30000 Training Loss: 0.03843157738447189\n",
      "Epoch 17696/30000 Training Loss: 0.04744206368923187\n",
      "Epoch 17697/30000 Training Loss: 0.046180784702301025\n",
      "Epoch 17698/30000 Training Loss: 0.04815909266471863\n",
      "Epoch 17699/30000 Training Loss: 0.05262678489089012\n",
      "Epoch 17700/30000 Training Loss: 0.043952576816082\n",
      "Epoch 17700/30000 Validation Loss: 0.0632530078291893\n",
      "Epoch 17701/30000 Training Loss: 0.0315612331032753\n",
      "Epoch 17702/30000 Training Loss: 0.04433275759220123\n",
      "Epoch 17703/30000 Training Loss: 0.04314671829342842\n",
      "Epoch 17704/30000 Training Loss: 0.039272554218769073\n",
      "Epoch 17705/30000 Training Loss: 0.044920798391103745\n",
      "Epoch 17706/30000 Training Loss: 0.03769276291131973\n",
      "Epoch 17707/30000 Training Loss: 0.04335682839155197\n",
      "Epoch 17708/30000 Training Loss: 0.04225878790020943\n",
      "Epoch 17709/30000 Training Loss: 0.04765872657299042\n",
      "Epoch 17710/30000 Training Loss: 0.0587032288312912\n",
      "Epoch 17711/30000 Training Loss: 0.03882773220539093\n",
      "Epoch 17712/30000 Training Loss: 0.04182703047990799\n",
      "Epoch 17713/30000 Training Loss: 0.07259846478700638\n",
      "Epoch 17714/30000 Training Loss: 0.04805593937635422\n",
      "Epoch 17715/30000 Training Loss: 0.043288033455610275\n",
      "Epoch 17716/30000 Training Loss: 0.06345654278993607\n",
      "Epoch 17717/30000 Training Loss: 0.04104562848806381\n",
      "Epoch 17718/30000 Training Loss: 0.03773773834109306\n",
      "Epoch 17719/30000 Training Loss: 0.05260038003325462\n",
      "Epoch 17720/30000 Training Loss: 0.03745545446872711\n",
      "Epoch 17721/30000 Training Loss: 0.04402782768011093\n",
      "Epoch 17722/30000 Training Loss: 0.055675018578767776\n",
      "Epoch 17723/30000 Training Loss: 0.0476720854640007\n",
      "Epoch 17724/30000 Training Loss: 0.039334215223789215\n",
      "Epoch 17725/30000 Training Loss: 0.03562347590923309\n",
      "Epoch 17726/30000 Training Loss: 0.05865592509508133\n",
      "Epoch 17727/30000 Training Loss: 0.06845425069332123\n",
      "Epoch 17728/30000 Training Loss: 0.05099128931760788\n",
      "Epoch 17729/30000 Training Loss: 0.07686350494623184\n",
      "Epoch 17730/30000 Training Loss: 0.04443516582250595\n",
      "Epoch 17731/30000 Training Loss: 0.038411177694797516\n",
      "Epoch 17732/30000 Training Loss: 0.04463168978691101\n",
      "Epoch 17733/30000 Training Loss: 0.048323385417461395\n",
      "Epoch 17734/30000 Training Loss: 0.05087714642286301\n",
      "Epoch 17735/30000 Training Loss: 0.036672670394182205\n",
      "Epoch 17736/30000 Training Loss: 0.05264949053525925\n",
      "Epoch 17737/30000 Training Loss: 0.03725219517946243\n",
      "Epoch 17738/30000 Training Loss: 0.04788878932595253\n",
      "Epoch 17739/30000 Training Loss: 0.06346552819013596\n",
      "Epoch 17740/30000 Training Loss: 0.06033781170845032\n",
      "Epoch 17741/30000 Training Loss: 0.04156118631362915\n",
      "Epoch 17742/30000 Training Loss: 0.05101501941680908\n",
      "Epoch 17743/30000 Training Loss: 0.06321835517883301\n",
      "Epoch 17744/30000 Training Loss: 0.0544179268181324\n",
      "Epoch 17745/30000 Training Loss: 0.056934654712677\n",
      "Epoch 17746/30000 Training Loss: 0.05250372365117073\n",
      "Epoch 17747/30000 Training Loss: 0.04527828097343445\n",
      "Epoch 17748/30000 Training Loss: 0.053394999355077744\n",
      "Epoch 17749/30000 Training Loss: 0.03720809891819954\n",
      "Epoch 17750/30000 Training Loss: 0.0450117290019989\n",
      "Epoch 17751/30000 Training Loss: 0.05402711406350136\n",
      "Epoch 17752/30000 Training Loss: 0.047766417264938354\n",
      "Epoch 17753/30000 Training Loss: 0.049768559634685516\n",
      "Epoch 17754/30000 Training Loss: 0.054070472717285156\n",
      "Epoch 17755/30000 Training Loss: 0.05006847530603409\n",
      "Epoch 17756/30000 Training Loss: 0.05487111955881119\n",
      "Epoch 17757/30000 Training Loss: 0.036337874829769135\n",
      "Epoch 17758/30000 Training Loss: 0.047802090644836426\n",
      "Epoch 17759/30000 Training Loss: 0.043698523193597794\n",
      "Epoch 17760/30000 Training Loss: 0.040384724736213684\n",
      "Epoch 17761/30000 Training Loss: 0.05507994443178177\n",
      "Epoch 17762/30000 Training Loss: 0.03158702701330185\n",
      "Epoch 17763/30000 Training Loss: 0.03408200666308403\n",
      "Epoch 17764/30000 Training Loss: 0.0502331368625164\n",
      "Epoch 17765/30000 Training Loss: 0.04726289585232735\n",
      "Epoch 17766/30000 Training Loss: 0.04810039699077606\n",
      "Epoch 17767/30000 Training Loss: 0.057634904980659485\n",
      "Epoch 17768/30000 Training Loss: 0.06088681146502495\n",
      "Epoch 17769/30000 Training Loss: 0.04797174781560898\n",
      "Epoch 17770/30000 Training Loss: 0.039096757769584656\n",
      "Epoch 17771/30000 Training Loss: 0.043844208121299744\n",
      "Epoch 17772/30000 Training Loss: 0.04825413227081299\n",
      "Epoch 17773/30000 Training Loss: 0.054299965500831604\n",
      "Epoch 17774/30000 Training Loss: 0.06340044736862183\n",
      "Epoch 17775/30000 Training Loss: 0.03649517148733139\n",
      "Epoch 17776/30000 Training Loss: 0.045609042048454285\n",
      "Epoch 17777/30000 Training Loss: 0.050433285534381866\n",
      "Epoch 17778/30000 Training Loss: 0.061187006533145905\n",
      "Epoch 17779/30000 Training Loss: 0.05141250044107437\n",
      "Epoch 17780/30000 Training Loss: 0.04383479431271553\n",
      "Epoch 17781/30000 Training Loss: 0.05237138271331787\n",
      "Epoch 17782/30000 Training Loss: 0.041362084448337555\n",
      "Epoch 17783/30000 Training Loss: 0.046302132308483124\n",
      "Epoch 17784/30000 Training Loss: 0.0442645326256752\n",
      "Epoch 17785/30000 Training Loss: 0.04194759950041771\n",
      "Epoch 17786/30000 Training Loss: 0.052748434245586395\n",
      "Epoch 17787/30000 Training Loss: 0.04818563535809517\n",
      "Epoch 17788/30000 Training Loss: 0.060340382158756256\n",
      "Epoch 17789/30000 Training Loss: 0.03841780126094818\n",
      "Epoch 17790/30000 Training Loss: 0.065595842897892\n",
      "Epoch 17791/30000 Training Loss: 0.042728811502456665\n",
      "Epoch 17792/30000 Training Loss: 0.04014858603477478\n",
      "Epoch 17793/30000 Training Loss: 0.05370286479592323\n",
      "Epoch 17794/30000 Training Loss: 0.04708173871040344\n",
      "Epoch 17795/30000 Training Loss: 0.039630621671676636\n",
      "Epoch 17796/30000 Training Loss: 0.04792902246117592\n",
      "Epoch 17797/30000 Training Loss: 0.05265720188617706\n",
      "Epoch 17798/30000 Training Loss: 0.05669572949409485\n",
      "Epoch 17799/30000 Training Loss: 0.04199425131082535\n",
      "Epoch 17800/30000 Training Loss: 0.03620034456253052\n",
      "Epoch 17800/30000 Validation Loss: 0.05453478544950485\n",
      "Epoch 17801/30000 Training Loss: 0.044832196086645126\n",
      "Epoch 17802/30000 Training Loss: 0.04315347597002983\n",
      "Epoch 17803/30000 Training Loss: 0.046259768307209015\n",
      "Epoch 17804/30000 Training Loss: 0.04575303941965103\n",
      "Epoch 17805/30000 Training Loss: 0.05587591975927353\n",
      "Epoch 17806/30000 Training Loss: 0.057533882558345795\n",
      "Epoch 17807/30000 Training Loss: 0.05178202688694\n",
      "Epoch 17808/30000 Training Loss: 0.04855026304721832\n",
      "Epoch 17809/30000 Training Loss: 0.04157973453402519\n",
      "Epoch 17810/30000 Training Loss: 0.05235268175601959\n",
      "Epoch 17811/30000 Training Loss: 0.05578331649303436\n",
      "Epoch 17812/30000 Training Loss: 0.04648277163505554\n",
      "Epoch 17813/30000 Training Loss: 0.04258042201399803\n",
      "Epoch 17814/30000 Training Loss: 0.045132603496313095\n",
      "Epoch 17815/30000 Training Loss: 0.07608447968959808\n",
      "Epoch 17816/30000 Training Loss: 0.04083127900958061\n",
      "Epoch 17817/30000 Training Loss: 0.05642138794064522\n",
      "Epoch 17818/30000 Training Loss: 0.04957965388894081\n",
      "Epoch 17819/30000 Training Loss: 0.05385350435972214\n",
      "Epoch 17820/30000 Training Loss: 0.03842735290527344\n",
      "Epoch 17821/30000 Training Loss: 0.050135113298892975\n",
      "Epoch 17822/30000 Training Loss: 0.04712904989719391\n",
      "Epoch 17823/30000 Training Loss: 0.03729946166276932\n",
      "Epoch 17824/30000 Training Loss: 0.043736010789871216\n",
      "Epoch 17825/30000 Training Loss: 0.05275392532348633\n",
      "Epoch 17826/30000 Training Loss: 0.05260726809501648\n",
      "Epoch 17827/30000 Training Loss: 0.042292144149541855\n",
      "Epoch 17828/30000 Training Loss: 0.042545195668935776\n",
      "Epoch 17829/30000 Training Loss: 0.054480183869600296\n",
      "Epoch 17830/30000 Training Loss: 0.05022003501653671\n",
      "Epoch 17831/30000 Training Loss: 0.039955511689186096\n",
      "Epoch 17832/30000 Training Loss: 0.05145110934972763\n",
      "Epoch 17833/30000 Training Loss: 0.050837013870477676\n",
      "Epoch 17834/30000 Training Loss: 0.03485535830259323\n",
      "Epoch 17835/30000 Training Loss: 0.042492542415857315\n",
      "Epoch 17836/30000 Training Loss: 0.04320235922932625\n",
      "Epoch 17837/30000 Training Loss: 0.038175035268068314\n",
      "Epoch 17838/30000 Training Loss: 0.045139290392398834\n",
      "Epoch 17839/30000 Training Loss: 0.05476832389831543\n",
      "Epoch 17840/30000 Training Loss: 0.046056829392910004\n",
      "Epoch 17841/30000 Training Loss: 0.05509621277451515\n",
      "Epoch 17842/30000 Training Loss: 0.03692549094557762\n",
      "Epoch 17843/30000 Training Loss: 0.03527826443314552\n",
      "Epoch 17844/30000 Training Loss: 0.046325985342264175\n",
      "Epoch 17845/30000 Training Loss: 0.04932123422622681\n",
      "Epoch 17846/30000 Training Loss: 0.037828173488378525\n",
      "Epoch 17847/30000 Training Loss: 0.04865960031747818\n",
      "Epoch 17848/30000 Training Loss: 0.045518312603235245\n",
      "Epoch 17849/30000 Training Loss: 0.04633893072605133\n",
      "Epoch 17850/30000 Training Loss: 0.05335542932152748\n",
      "Epoch 17851/30000 Training Loss: 0.041986193507909775\n",
      "Epoch 17852/30000 Training Loss: 0.05005837604403496\n",
      "Epoch 17853/30000 Training Loss: 0.05813667178153992\n",
      "Epoch 17854/30000 Training Loss: 0.04157162457704544\n",
      "Epoch 17855/30000 Training Loss: 0.03652342036366463\n",
      "Epoch 17856/30000 Training Loss: 0.04168304055929184\n",
      "Epoch 17857/30000 Training Loss: 0.04903699830174446\n",
      "Epoch 17858/30000 Training Loss: 0.050713278353214264\n",
      "Epoch 17859/30000 Training Loss: 0.04251621663570404\n",
      "Epoch 17860/30000 Training Loss: 0.04257001727819443\n",
      "Epoch 17861/30000 Training Loss: 0.04788052663207054\n",
      "Epoch 17862/30000 Training Loss: 0.040563277900218964\n",
      "Epoch 17863/30000 Training Loss: 0.04133814573287964\n",
      "Epoch 17864/30000 Training Loss: 0.04652074724435806\n",
      "Epoch 17865/30000 Training Loss: 0.03883754089474678\n",
      "Epoch 17866/30000 Training Loss: 0.03902970999479294\n",
      "Epoch 17867/30000 Training Loss: 0.05113225057721138\n",
      "Epoch 17868/30000 Training Loss: 0.04115539416670799\n",
      "Epoch 17869/30000 Training Loss: 0.056994758546352386\n",
      "Epoch 17870/30000 Training Loss: 0.04721520096063614\n",
      "Epoch 17871/30000 Training Loss: 0.043403007090091705\n",
      "Epoch 17872/30000 Training Loss: 0.04375464469194412\n",
      "Epoch 17873/30000 Training Loss: 0.057407259941101074\n",
      "Epoch 17874/30000 Training Loss: 0.0439593642950058\n",
      "Epoch 17875/30000 Training Loss: 0.04793298989534378\n",
      "Epoch 17876/30000 Training Loss: 0.03968752548098564\n",
      "Epoch 17877/30000 Training Loss: 0.03400352597236633\n",
      "Epoch 17878/30000 Training Loss: 0.03988209739327431\n",
      "Epoch 17879/30000 Training Loss: 0.059579137712717056\n",
      "Epoch 17880/30000 Training Loss: 0.052524980157613754\n",
      "Epoch 17881/30000 Training Loss: 0.03870246559381485\n",
      "Epoch 17882/30000 Training Loss: 0.06093355268239975\n",
      "Epoch 17883/30000 Training Loss: 0.039600666612386703\n",
      "Epoch 17884/30000 Training Loss: 0.04872447997331619\n",
      "Epoch 17885/30000 Training Loss: 0.045731354504823685\n",
      "Epoch 17886/30000 Training Loss: 0.0398707278072834\n",
      "Epoch 17887/30000 Training Loss: 0.07126899063587189\n",
      "Epoch 17888/30000 Training Loss: 0.05132177472114563\n",
      "Epoch 17889/30000 Training Loss: 0.045768026262521744\n",
      "Epoch 17890/30000 Training Loss: 0.05180972442030907\n",
      "Epoch 17891/30000 Training Loss: 0.04698122665286064\n",
      "Epoch 17892/30000 Training Loss: 0.03814302012324333\n",
      "Epoch 17893/30000 Training Loss: 0.05295201763510704\n",
      "Epoch 17894/30000 Training Loss: 0.0626043826341629\n",
      "Epoch 17895/30000 Training Loss: 0.038739148527383804\n",
      "Epoch 17896/30000 Training Loss: 0.05176675319671631\n",
      "Epoch 17897/30000 Training Loss: 0.0425567664206028\n",
      "Epoch 17898/30000 Training Loss: 0.04564675688743591\n",
      "Epoch 17899/30000 Training Loss: 0.052698273211717606\n",
      "Epoch 17900/30000 Training Loss: 0.06192085146903992\n",
      "Epoch 17900/30000 Validation Loss: 0.040349215269088745\n",
      "Epoch 17901/30000 Training Loss: 0.03720185160636902\n",
      "Epoch 17902/30000 Training Loss: 0.044623635709285736\n",
      "Epoch 17903/30000 Training Loss: 0.04582737758755684\n",
      "Epoch 17904/30000 Training Loss: 0.03591807186603546\n",
      "Epoch 17905/30000 Training Loss: 0.054849714040756226\n",
      "Epoch 17906/30000 Training Loss: 0.04738077521324158\n",
      "Epoch 17907/30000 Training Loss: 0.037414804100990295\n",
      "Epoch 17908/30000 Training Loss: 0.04973891004920006\n",
      "Epoch 17909/30000 Training Loss: 0.04963865876197815\n",
      "Epoch 17910/30000 Training Loss: 0.04606417939066887\n",
      "Epoch 17911/30000 Training Loss: 0.04638032987713814\n",
      "Epoch 17912/30000 Training Loss: 0.04635441303253174\n",
      "Epoch 17913/30000 Training Loss: 0.05475746840238571\n",
      "Epoch 17914/30000 Training Loss: 0.0478789322078228\n",
      "Epoch 17915/30000 Training Loss: 0.05464760586619377\n",
      "Epoch 17916/30000 Training Loss: 0.04438396170735359\n",
      "Epoch 17917/30000 Training Loss: 0.050452932715415955\n",
      "Epoch 17918/30000 Training Loss: 0.03541279211640358\n",
      "Epoch 17919/30000 Training Loss: 0.04114645719528198\n",
      "Epoch 17920/30000 Training Loss: 0.04411832243204117\n",
      "Epoch 17921/30000 Training Loss: 0.03185086324810982\n",
      "Epoch 17922/30000 Training Loss: 0.04876476526260376\n",
      "Epoch 17923/30000 Training Loss: 0.05291767045855522\n",
      "Epoch 17924/30000 Training Loss: 0.03750981390476227\n",
      "Epoch 17925/30000 Training Loss: 0.05066850781440735\n",
      "Epoch 17926/30000 Training Loss: 0.04699509218335152\n",
      "Epoch 17927/30000 Training Loss: 0.0602404847741127\n",
      "Epoch 17928/30000 Training Loss: 0.041709452867507935\n",
      "Epoch 17929/30000 Training Loss: 0.052281029522418976\n",
      "Epoch 17930/30000 Training Loss: 0.04314028471708298\n",
      "Epoch 17931/30000 Training Loss: 0.045158375054597855\n",
      "Epoch 17932/30000 Training Loss: 0.06879619508981705\n",
      "Epoch 17933/30000 Training Loss: 0.05531226843595505\n",
      "Epoch 17934/30000 Training Loss: 0.029565569013357162\n",
      "Epoch 17935/30000 Training Loss: 0.05027219280600548\n",
      "Epoch 17936/30000 Training Loss: 0.04180436581373215\n",
      "Epoch 17937/30000 Training Loss: 0.04843784123659134\n",
      "Epoch 17938/30000 Training Loss: 0.032303884625434875\n",
      "Epoch 17939/30000 Training Loss: 0.05801902711391449\n",
      "Epoch 17940/30000 Training Loss: 0.05499255284667015\n",
      "Epoch 17941/30000 Training Loss: 0.06058859825134277\n",
      "Epoch 17942/30000 Training Loss: 0.05964742973446846\n",
      "Epoch 17943/30000 Training Loss: 0.03411991521716118\n",
      "Epoch 17944/30000 Training Loss: 0.045845549553632736\n",
      "Epoch 17945/30000 Training Loss: 0.05078444257378578\n",
      "Epoch 17946/30000 Training Loss: 0.06038718670606613\n",
      "Epoch 17947/30000 Training Loss: 0.04239615052938461\n",
      "Epoch 17948/30000 Training Loss: 0.035645872354507446\n",
      "Epoch 17949/30000 Training Loss: 0.05744852125644684\n",
      "Epoch 17950/30000 Training Loss: 0.03881649672985077\n",
      "Epoch 17951/30000 Training Loss: 0.06194749474525452\n",
      "Epoch 17952/30000 Training Loss: 0.05794786289334297\n",
      "Epoch 17953/30000 Training Loss: 0.0416586734354496\n",
      "Epoch 17954/30000 Training Loss: 0.04193716496229172\n",
      "Epoch 17955/30000 Training Loss: 0.04199838638305664\n",
      "Epoch 17956/30000 Training Loss: 0.04231070727109909\n",
      "Epoch 17957/30000 Training Loss: 0.046761926263570786\n",
      "Epoch 17958/30000 Training Loss: 0.05680970847606659\n",
      "Epoch 17959/30000 Training Loss: 0.06378653645515442\n",
      "Epoch 17960/30000 Training Loss: 0.060449857264757156\n",
      "Epoch 17961/30000 Training Loss: 0.04631949961185455\n",
      "Epoch 17962/30000 Training Loss: 0.04288531467318535\n",
      "Epoch 17963/30000 Training Loss: 0.039144281297922134\n",
      "Epoch 17964/30000 Training Loss: 0.027785489335656166\n",
      "Epoch 17965/30000 Training Loss: 0.041428159922361374\n",
      "Epoch 17966/30000 Training Loss: 0.060651395469903946\n",
      "Epoch 17967/30000 Training Loss: 0.05196703225374222\n",
      "Epoch 17968/30000 Training Loss: 0.04053781181573868\n",
      "Epoch 17969/30000 Training Loss: 0.05170849710702896\n",
      "Epoch 17970/30000 Training Loss: 0.03826180845499039\n",
      "Epoch 17971/30000 Training Loss: 0.04380599036812782\n",
      "Epoch 17972/30000 Training Loss: 0.0391046479344368\n",
      "Epoch 17973/30000 Training Loss: 0.06008439138531685\n",
      "Epoch 17974/30000 Training Loss: 0.04663477838039398\n",
      "Epoch 17975/30000 Training Loss: 0.052593477070331573\n",
      "Epoch 17976/30000 Training Loss: 0.03345007449388504\n",
      "Epoch 17977/30000 Training Loss: 0.04288031533360481\n",
      "Epoch 17978/30000 Training Loss: 0.0566914901137352\n",
      "Epoch 17979/30000 Training Loss: 0.04952191561460495\n",
      "Epoch 17980/30000 Training Loss: 0.053192269057035446\n",
      "Epoch 17981/30000 Training Loss: 0.04630454257130623\n",
      "Epoch 17982/30000 Training Loss: 0.047682374715805054\n",
      "Epoch 17983/30000 Training Loss: 0.035966791212558746\n",
      "Epoch 17984/30000 Training Loss: 0.06727970391511917\n",
      "Epoch 17985/30000 Training Loss: 0.06155586987733841\n",
      "Epoch 17986/30000 Training Loss: 0.0486685112118721\n",
      "Epoch 17987/30000 Training Loss: 0.03401011973619461\n",
      "Epoch 17988/30000 Training Loss: 0.04350868985056877\n",
      "Epoch 17989/30000 Training Loss: 0.041342899203300476\n",
      "Epoch 17990/30000 Training Loss: 0.06096959859132767\n",
      "Epoch 17991/30000 Training Loss: 0.045126475393772125\n",
      "Epoch 17992/30000 Training Loss: 0.04003755375742912\n",
      "Epoch 17993/30000 Training Loss: 0.05333731323480606\n",
      "Epoch 17994/30000 Training Loss: 0.060605596750974655\n",
      "Epoch 17995/30000 Training Loss: 0.04250440374016762\n",
      "Epoch 17996/30000 Training Loss: 0.04806062951683998\n",
      "Epoch 17997/30000 Training Loss: 0.05884194374084473\n",
      "Epoch 17998/30000 Training Loss: 0.04927191138267517\n",
      "Epoch 17999/30000 Training Loss: 0.03933680057525635\n",
      "Epoch 18000/30000 Training Loss: 0.04960355907678604\n",
      "Epoch 18000/30000 Validation Loss: 0.0534781813621521\n",
      "Epoch 18001/30000 Training Loss: 0.05954939126968384\n",
      "Epoch 18002/30000 Training Loss: 0.06603048741817474\n",
      "Epoch 18003/30000 Training Loss: 0.04283430054783821\n",
      "Epoch 18004/30000 Training Loss: 0.039592619985342026\n",
      "Epoch 18005/30000 Training Loss: 0.03530951961874962\n",
      "Epoch 18006/30000 Training Loss: 0.046647705137729645\n",
      "Epoch 18007/30000 Training Loss: 0.04552267864346504\n",
      "Epoch 18008/30000 Training Loss: 0.053645119071006775\n",
      "Epoch 18009/30000 Training Loss: 0.05225061997771263\n",
      "Epoch 18010/30000 Training Loss: 0.04566487669944763\n",
      "Epoch 18011/30000 Training Loss: 0.05862811952829361\n",
      "Epoch 18012/30000 Training Loss: 0.047829367220401764\n",
      "Epoch 18013/30000 Training Loss: 0.04568658024072647\n",
      "Epoch 18014/30000 Training Loss: 0.038257844746112823\n",
      "Epoch 18015/30000 Training Loss: 0.06064198166131973\n",
      "Epoch 18016/30000 Training Loss: 0.05512923002243042\n",
      "Epoch 18017/30000 Training Loss: 0.05090915411710739\n",
      "Epoch 18018/30000 Training Loss: 0.051585305482149124\n",
      "Epoch 18019/30000 Training Loss: 0.044627971947193146\n",
      "Epoch 18020/30000 Training Loss: 0.06152652949094772\n",
      "Epoch 18021/30000 Training Loss: 0.0597023144364357\n",
      "Epoch 18022/30000 Training Loss: 0.056160036474466324\n",
      "Epoch 18023/30000 Training Loss: 0.04175998270511627\n",
      "Epoch 18024/30000 Training Loss: 0.03711579367518425\n",
      "Epoch 18025/30000 Training Loss: 0.06716381013393402\n",
      "Epoch 18026/30000 Training Loss: 0.044139597564935684\n",
      "Epoch 18027/30000 Training Loss: 0.04924515634775162\n",
      "Epoch 18028/30000 Training Loss: 0.048660505563020706\n",
      "Epoch 18029/30000 Training Loss: 0.06408166140317917\n",
      "Epoch 18030/30000 Training Loss: 0.051731258630752563\n",
      "Epoch 18031/30000 Training Loss: 0.0484226793050766\n",
      "Epoch 18032/30000 Training Loss: 0.05495411902666092\n",
      "Epoch 18033/30000 Training Loss: 0.04629583656787872\n",
      "Epoch 18034/30000 Training Loss: 0.04322279989719391\n",
      "Epoch 18035/30000 Training Loss: 0.05436258018016815\n",
      "Epoch 18036/30000 Training Loss: 0.04949326068162918\n",
      "Epoch 18037/30000 Training Loss: 0.05119626596570015\n",
      "Epoch 18038/30000 Training Loss: 0.05900149047374725\n",
      "Epoch 18039/30000 Training Loss: 0.04600488394498825\n",
      "Epoch 18040/30000 Training Loss: 0.06877107918262482\n",
      "Epoch 18041/30000 Training Loss: 0.057612344622612\n",
      "Epoch 18042/30000 Training Loss: 0.04494670033454895\n",
      "Epoch 18043/30000 Training Loss: 0.056498490273952484\n",
      "Epoch 18044/30000 Training Loss: 0.0470232255756855\n",
      "Epoch 18045/30000 Training Loss: 0.0661962479352951\n",
      "Epoch 18046/30000 Training Loss: 0.0447826124727726\n",
      "Epoch 18047/30000 Training Loss: 0.05180487036705017\n",
      "Epoch 18048/30000 Training Loss: 0.06307364255189896\n",
      "Epoch 18049/30000 Training Loss: 0.041676197201013565\n",
      "Epoch 18050/30000 Training Loss: 0.04217837005853653\n",
      "Epoch 18051/30000 Training Loss: 0.04445301741361618\n",
      "Epoch 18052/30000 Training Loss: 0.0580974780023098\n",
      "Epoch 18053/30000 Training Loss: 0.05328850448131561\n",
      "Epoch 18054/30000 Training Loss: 0.05982973426580429\n",
      "Epoch 18055/30000 Training Loss: 0.034100644290447235\n",
      "Epoch 18056/30000 Training Loss: 0.06170186027884483\n",
      "Epoch 18057/30000 Training Loss: 0.0472036674618721\n",
      "Epoch 18058/30000 Training Loss: 0.053618647158145905\n",
      "Epoch 18059/30000 Training Loss: 0.048149216920137405\n",
      "Epoch 18060/30000 Training Loss: 0.04856720194220543\n",
      "Epoch 18061/30000 Training Loss: 0.05134876072406769\n",
      "Epoch 18062/30000 Training Loss: 0.04554355889558792\n",
      "Epoch 18063/30000 Training Loss: 0.04061552882194519\n",
      "Epoch 18064/30000 Training Loss: 0.061041489243507385\n",
      "Epoch 18065/30000 Training Loss: 0.05932561308145523\n",
      "Epoch 18066/30000 Training Loss: 0.035177454352378845\n",
      "Epoch 18067/30000 Training Loss: 0.04871729761362076\n",
      "Epoch 18068/30000 Training Loss: 0.05475124716758728\n",
      "Epoch 18069/30000 Training Loss: 0.05321589857339859\n",
      "Epoch 18070/30000 Training Loss: 0.038319364190101624\n",
      "Epoch 18071/30000 Training Loss: 0.055663011968135834\n",
      "Epoch 18072/30000 Training Loss: 0.04973943158984184\n",
      "Epoch 18073/30000 Training Loss: 0.05191277712583542\n",
      "Epoch 18074/30000 Training Loss: 0.050041329115629196\n",
      "Epoch 18075/30000 Training Loss: 0.05468381568789482\n",
      "Epoch 18076/30000 Training Loss: 0.047781120985746384\n",
      "Epoch 18077/30000 Training Loss: 0.05267815291881561\n",
      "Epoch 18078/30000 Training Loss: 0.0451713427901268\n",
      "Epoch 18079/30000 Training Loss: 0.0484747514128685\n",
      "Epoch 18080/30000 Training Loss: 0.04637632891535759\n",
      "Epoch 18081/30000 Training Loss: 0.04886429011821747\n",
      "Epoch 18082/30000 Training Loss: 0.050149284303188324\n",
      "Epoch 18083/30000 Training Loss: 0.044480226933956146\n",
      "Epoch 18084/30000 Training Loss: 0.05332508683204651\n",
      "Epoch 18085/30000 Training Loss: 0.059372007846832275\n",
      "Epoch 18086/30000 Training Loss: 0.052804335951805115\n",
      "Epoch 18087/30000 Training Loss: 0.05025598034262657\n",
      "Epoch 18088/30000 Training Loss: 0.05499948188662529\n",
      "Epoch 18089/30000 Training Loss: 0.034543391317129135\n",
      "Epoch 18090/30000 Training Loss: 0.06017431244254112\n",
      "Epoch 18091/30000 Training Loss: 0.04843989759683609\n",
      "Epoch 18092/30000 Training Loss: 0.06836600601673126\n",
      "Epoch 18093/30000 Training Loss: 0.041558220982551575\n",
      "Epoch 18094/30000 Training Loss: 0.04297994449734688\n",
      "Epoch 18095/30000 Training Loss: 0.046652331948280334\n",
      "Epoch 18096/30000 Training Loss: 0.042696263641119\n",
      "Epoch 18097/30000 Training Loss: 0.04489794746041298\n",
      "Epoch 18098/30000 Training Loss: 0.047011665999889374\n",
      "Epoch 18099/30000 Training Loss: 0.06292171031236649\n",
      "Epoch 18100/30000 Training Loss: 0.05437901243567467\n",
      "Epoch 18100/30000 Validation Loss: 0.0469912588596344\n",
      "Epoch 18101/30000 Training Loss: 0.0502089224755764\n",
      "Epoch 18102/30000 Training Loss: 0.05723738670349121\n",
      "Epoch 18103/30000 Training Loss: 0.07116789370775223\n",
      "Epoch 18104/30000 Training Loss: 0.054493047297000885\n",
      "Epoch 18105/30000 Training Loss: 0.05573965236544609\n",
      "Epoch 18106/30000 Training Loss: 0.036937810480594635\n",
      "Epoch 18107/30000 Training Loss: 0.05663260072469711\n",
      "Epoch 18108/30000 Training Loss: 0.04081713408231735\n",
      "Epoch 18109/30000 Training Loss: 0.05193335562944412\n",
      "Epoch 18110/30000 Training Loss: 0.0628601536154747\n",
      "Epoch 18111/30000 Training Loss: 0.051779717206954956\n",
      "Epoch 18112/30000 Training Loss: 0.04508836194872856\n",
      "Epoch 18113/30000 Training Loss: 0.06850215792655945\n",
      "Epoch 18114/30000 Training Loss: 0.0385083369910717\n",
      "Epoch 18115/30000 Training Loss: 0.04354015737771988\n",
      "Epoch 18116/30000 Training Loss: 0.03979325294494629\n",
      "Epoch 18117/30000 Training Loss: 0.07219239324331284\n",
      "Epoch 18118/30000 Training Loss: 0.0333264134824276\n",
      "Epoch 18119/30000 Training Loss: 0.05545327812433243\n",
      "Epoch 18120/30000 Training Loss: 0.051639702171087265\n",
      "Epoch 18121/30000 Training Loss: 0.04485165327787399\n",
      "Epoch 18122/30000 Training Loss: 0.05777469277381897\n",
      "Epoch 18123/30000 Training Loss: 0.05888676643371582\n",
      "Epoch 18124/30000 Training Loss: 0.035193365067243576\n",
      "Epoch 18125/30000 Training Loss: 0.05030989646911621\n",
      "Epoch 18126/30000 Training Loss: 0.058594051748514175\n",
      "Epoch 18127/30000 Training Loss: 0.039539504796266556\n",
      "Epoch 18128/30000 Training Loss: 0.07017402350902557\n",
      "Epoch 18129/30000 Training Loss: 0.04519815742969513\n",
      "Epoch 18130/30000 Training Loss: 0.05506431311368942\n",
      "Epoch 18131/30000 Training Loss: 0.05112113803625107\n",
      "Epoch 18132/30000 Training Loss: 0.033639099448919296\n",
      "Epoch 18133/30000 Training Loss: 0.0548483282327652\n",
      "Epoch 18134/30000 Training Loss: 0.041575461626052856\n",
      "Epoch 18135/30000 Training Loss: 0.05670377239584923\n",
      "Epoch 18136/30000 Training Loss: 0.04365754500031471\n",
      "Epoch 18137/30000 Training Loss: 0.05481480807065964\n",
      "Epoch 18138/30000 Training Loss: 0.04891100153326988\n",
      "Epoch 18139/30000 Training Loss: 0.07096298784017563\n",
      "Epoch 18140/30000 Training Loss: 0.051928140223026276\n",
      "Epoch 18141/30000 Training Loss: 0.05107418820261955\n",
      "Epoch 18142/30000 Training Loss: 0.05529516935348511\n",
      "Epoch 18143/30000 Training Loss: 0.03859885036945343\n",
      "Epoch 18144/30000 Training Loss: 0.05503647029399872\n",
      "Epoch 18145/30000 Training Loss: 0.042484626173973083\n",
      "Epoch 18146/30000 Training Loss: 0.05107242986559868\n",
      "Epoch 18147/30000 Training Loss: 0.054781585931777954\n",
      "Epoch 18148/30000 Training Loss: 0.050880901515483856\n",
      "Epoch 18149/30000 Training Loss: 0.034901004284620285\n",
      "Epoch 18150/30000 Training Loss: 0.04592956230044365\n",
      "Epoch 18151/30000 Training Loss: 0.04908987507224083\n",
      "Epoch 18152/30000 Training Loss: 0.042641010135412216\n",
      "Epoch 18153/30000 Training Loss: 0.04143939167261124\n",
      "Epoch 18154/30000 Training Loss: 0.050479527562856674\n",
      "Epoch 18155/30000 Training Loss: 0.05059463158249855\n",
      "Epoch 18156/30000 Training Loss: 0.07140186429023743\n",
      "Epoch 18157/30000 Training Loss: 0.058391451835632324\n",
      "Epoch 18158/30000 Training Loss: 0.05251416191458702\n",
      "Epoch 18159/30000 Training Loss: 0.03516583889722824\n",
      "Epoch 18160/30000 Training Loss: 0.04846453294157982\n",
      "Epoch 18161/30000 Training Loss: 0.04070381820201874\n",
      "Epoch 18162/30000 Training Loss: 0.06119808927178383\n",
      "Epoch 18163/30000 Training Loss: 0.04967328906059265\n",
      "Epoch 18164/30000 Training Loss: 0.049847736954689026\n",
      "Epoch 18165/30000 Training Loss: 0.039240144193172455\n",
      "Epoch 18166/30000 Training Loss: 0.054688431322574615\n",
      "Epoch 18167/30000 Training Loss: 0.03642209246754646\n",
      "Epoch 18168/30000 Training Loss: 0.04982946813106537\n",
      "Epoch 18169/30000 Training Loss: 0.05931372195482254\n",
      "Epoch 18170/30000 Training Loss: 0.04443731904029846\n",
      "Epoch 18171/30000 Training Loss: 0.04785116761922836\n",
      "Epoch 18172/30000 Training Loss: 0.04507863149046898\n",
      "Epoch 18173/30000 Training Loss: 0.050215791910886765\n",
      "Epoch 18174/30000 Training Loss: 0.05186862498521805\n",
      "Epoch 18175/30000 Training Loss: 0.05059691518545151\n",
      "Epoch 18176/30000 Training Loss: 0.03567519038915634\n",
      "Epoch 18177/30000 Training Loss: 0.06561017781496048\n",
      "Epoch 18178/30000 Training Loss: 0.05842715501785278\n",
      "Epoch 18179/30000 Training Loss: 0.04413536936044693\n",
      "Epoch 18180/30000 Training Loss: 0.059931814670562744\n",
      "Epoch 18181/30000 Training Loss: 0.06503850221633911\n",
      "Epoch 18182/30000 Training Loss: 0.04447969049215317\n",
      "Epoch 18183/30000 Training Loss: 0.04510987922549248\n",
      "Epoch 18184/30000 Training Loss: 0.046266671270132065\n",
      "Epoch 18185/30000 Training Loss: 0.034009262919425964\n",
      "Epoch 18186/30000 Training Loss: 0.03953314945101738\n",
      "Epoch 18187/30000 Training Loss: 0.04535933583974838\n",
      "Epoch 18188/30000 Training Loss: 0.05399095639586449\n",
      "Epoch 18189/30000 Training Loss: 0.05487222224473953\n",
      "Epoch 18190/30000 Training Loss: 0.05457498878240585\n",
      "Epoch 18191/30000 Training Loss: 0.07657669484615326\n",
      "Epoch 18192/30000 Training Loss: 0.050797492265701294\n",
      "Epoch 18193/30000 Training Loss: 0.0592181459069252\n",
      "Epoch 18194/30000 Training Loss: 0.04971001669764519\n",
      "Epoch 18195/30000 Training Loss: 0.047453608363866806\n",
      "Epoch 18196/30000 Training Loss: 0.046553511172533035\n",
      "Epoch 18197/30000 Training Loss: 0.0359039381146431\n",
      "Epoch 18198/30000 Training Loss: 0.0556446798145771\n",
      "Epoch 18199/30000 Training Loss: 0.04262213036417961\n",
      "Epoch 18200/30000 Training Loss: 0.03612475469708443\n",
      "Epoch 18200/30000 Validation Loss: 0.03954128175973892\n",
      "Epoch 18201/30000 Training Loss: 0.04666609689593315\n",
      "Epoch 18202/30000 Training Loss: 0.046035777777433395\n",
      "Epoch 18203/30000 Training Loss: 0.055901020765304565\n",
      "Epoch 18204/30000 Training Loss: 0.042307592928409576\n",
      "Epoch 18205/30000 Training Loss: 0.03963287174701691\n",
      "Epoch 18206/30000 Training Loss: 0.0474834218621254\n",
      "Epoch 18207/30000 Training Loss: 0.03848564252257347\n",
      "Epoch 18208/30000 Training Loss: 0.04219021275639534\n",
      "Epoch 18209/30000 Training Loss: 0.036183860152959824\n",
      "Epoch 18210/30000 Training Loss: 0.063471719622612\n",
      "Epoch 18211/30000 Training Loss: 0.0518704317510128\n",
      "Epoch 18212/30000 Training Loss: 0.04653533920645714\n",
      "Epoch 18213/30000 Training Loss: 0.040822092443704605\n",
      "Epoch 18214/30000 Training Loss: 0.04384573549032211\n",
      "Epoch 18215/30000 Training Loss: 0.0528605654835701\n",
      "Epoch 18216/30000 Training Loss: 0.05068174749612808\n",
      "Epoch 18217/30000 Training Loss: 0.04401608556509018\n",
      "Epoch 18218/30000 Training Loss: 0.044572945684194565\n",
      "Epoch 18219/30000 Training Loss: 0.04767337068915367\n",
      "Epoch 18220/30000 Training Loss: 0.051785022020339966\n",
      "Epoch 18221/30000 Training Loss: 0.056830912828445435\n",
      "Epoch 18222/30000 Training Loss: 0.04958039149641991\n",
      "Epoch 18223/30000 Training Loss: 0.04864715039730072\n",
      "Epoch 18224/30000 Training Loss: 0.04195922613143921\n",
      "Epoch 18225/30000 Training Loss: 0.051556166261434555\n",
      "Epoch 18226/30000 Training Loss: 0.055476948618888855\n",
      "Epoch 18227/30000 Training Loss: 0.037898577749729156\n",
      "Epoch 18228/30000 Training Loss: 0.053129877895116806\n",
      "Epoch 18229/30000 Training Loss: 0.054153840988874435\n",
      "Epoch 18230/30000 Training Loss: 0.04807743430137634\n",
      "Epoch 18231/30000 Training Loss: 0.05013161152601242\n",
      "Epoch 18232/30000 Training Loss: 0.054040875285863876\n",
      "Epoch 18233/30000 Training Loss: 0.051446106284856796\n",
      "Epoch 18234/30000 Training Loss: 0.045349352061748505\n",
      "Epoch 18235/30000 Training Loss: 0.04555976763367653\n",
      "Epoch 18236/30000 Training Loss: 0.05187727510929108\n",
      "Epoch 18237/30000 Training Loss: 0.04078301414847374\n",
      "Epoch 18238/30000 Training Loss: 0.0326446108520031\n",
      "Epoch 18239/30000 Training Loss: 0.04206547513604164\n",
      "Epoch 18240/30000 Training Loss: 0.06163184344768524\n",
      "Epoch 18241/30000 Training Loss: 0.06092708930373192\n",
      "Epoch 18242/30000 Training Loss: 0.042305219918489456\n",
      "Epoch 18243/30000 Training Loss: 0.06870224326848984\n",
      "Epoch 18244/30000 Training Loss: 0.06152603030204773\n",
      "Epoch 18245/30000 Training Loss: 0.05092817544937134\n",
      "Epoch 18246/30000 Training Loss: 0.07553378492593765\n",
      "Epoch 18247/30000 Training Loss: 0.03599725291132927\n",
      "Epoch 18248/30000 Training Loss: 0.061875954270362854\n",
      "Epoch 18249/30000 Training Loss: 0.056699227541685104\n",
      "Epoch 18250/30000 Training Loss: 0.05345054715871811\n",
      "Epoch 18251/30000 Training Loss: 0.06356941908597946\n",
      "Epoch 18252/30000 Training Loss: 0.04322703555226326\n",
      "Epoch 18253/30000 Training Loss: 0.0668288990855217\n",
      "Epoch 18254/30000 Training Loss: 0.035389695316553116\n",
      "Epoch 18255/30000 Training Loss: 0.05784371867775917\n",
      "Epoch 18256/30000 Training Loss: 0.03863314911723137\n",
      "Epoch 18257/30000 Training Loss: 0.040233466774225235\n",
      "Epoch 18258/30000 Training Loss: 0.04980350285768509\n",
      "Epoch 18259/30000 Training Loss: 0.044551800936460495\n",
      "Epoch 18260/30000 Training Loss: 0.05414224788546562\n",
      "Epoch 18261/30000 Training Loss: 0.04042268544435501\n",
      "Epoch 18262/30000 Training Loss: 0.04581444710493088\n",
      "Epoch 18263/30000 Training Loss: 0.036742839962244034\n",
      "Epoch 18264/30000 Training Loss: 0.05059925094246864\n",
      "Epoch 18265/30000 Training Loss: 0.03494488447904587\n",
      "Epoch 18266/30000 Training Loss: 0.04310528188943863\n",
      "Epoch 18267/30000 Training Loss: 0.04099969565868378\n",
      "Epoch 18268/30000 Training Loss: 0.06317238509654999\n",
      "Epoch 18269/30000 Training Loss: 0.04671046510338783\n",
      "Epoch 18270/30000 Training Loss: 0.05407062917947769\n",
      "Epoch 18271/30000 Training Loss: 0.040234144777059555\n",
      "Epoch 18272/30000 Training Loss: 0.04440615698695183\n",
      "Epoch 18273/30000 Training Loss: 0.040291059762239456\n",
      "Epoch 18274/30000 Training Loss: 0.041036222130060196\n",
      "Epoch 18275/30000 Training Loss: 0.05514470115303993\n",
      "Epoch 18276/30000 Training Loss: 0.0467018261551857\n",
      "Epoch 18277/30000 Training Loss: 0.04157770425081253\n",
      "Epoch 18278/30000 Training Loss: 0.04956303536891937\n",
      "Epoch 18279/30000 Training Loss: 0.04084460064768791\n",
      "Epoch 18280/30000 Training Loss: 0.06186845898628235\n",
      "Epoch 18281/30000 Training Loss: 0.0380549281835556\n",
      "Epoch 18282/30000 Training Loss: 0.04060596600174904\n",
      "Epoch 18283/30000 Training Loss: 0.04508651793003082\n",
      "Epoch 18284/30000 Training Loss: 0.051624976098537445\n",
      "Epoch 18285/30000 Training Loss: 0.03656024858355522\n",
      "Epoch 18286/30000 Training Loss: 0.03999755159020424\n",
      "Epoch 18287/30000 Training Loss: 0.058607764542102814\n",
      "Epoch 18288/30000 Training Loss: 0.03996945545077324\n",
      "Epoch 18289/30000 Training Loss: 0.04517032206058502\n",
      "Epoch 18290/30000 Training Loss: 0.0408361554145813\n",
      "Epoch 18291/30000 Training Loss: 0.04505234211683273\n",
      "Epoch 18292/30000 Training Loss: 0.06486354768276215\n",
      "Epoch 18293/30000 Training Loss: 0.03620142489671707\n",
      "Epoch 18294/30000 Training Loss: 0.044110316783189774\n",
      "Epoch 18295/30000 Training Loss: 0.03166700154542923\n",
      "Epoch 18296/30000 Training Loss: 0.04648326337337494\n",
      "Epoch 18297/30000 Training Loss: 0.0474981963634491\n",
      "Epoch 18298/30000 Training Loss: 0.05708920955657959\n",
      "Epoch 18299/30000 Training Loss: 0.0421445332467556\n",
      "Epoch 18300/30000 Training Loss: 0.04470659792423248\n",
      "Epoch 18300/30000 Validation Loss: 0.05160894617438316\n",
      "Epoch 18301/30000 Training Loss: 0.04219496250152588\n",
      "Epoch 18302/30000 Training Loss: 0.06694863736629486\n",
      "Epoch 18303/30000 Training Loss: 0.02821378782391548\n",
      "Epoch 18304/30000 Training Loss: 0.04763453081250191\n",
      "Epoch 18305/30000 Training Loss: 0.05125588923692703\n",
      "Epoch 18306/30000 Training Loss: 0.05085177347064018\n",
      "Epoch 18307/30000 Training Loss: 0.036710530519485474\n",
      "Epoch 18308/30000 Training Loss: 0.0472867451608181\n",
      "Epoch 18309/30000 Training Loss: 0.056582558900117874\n",
      "Epoch 18310/30000 Training Loss: 0.04544840753078461\n",
      "Epoch 18311/30000 Training Loss: 0.054498568177223206\n",
      "Epoch 18312/30000 Training Loss: 0.04932086169719696\n",
      "Epoch 18313/30000 Training Loss: 0.046708062291145325\n",
      "Epoch 18314/30000 Training Loss: 0.06928639113903046\n",
      "Epoch 18315/30000 Training Loss: 0.03578746318817139\n",
      "Epoch 18316/30000 Training Loss: 0.04945497214794159\n",
      "Epoch 18317/30000 Training Loss: 0.061439625918865204\n",
      "Epoch 18318/30000 Training Loss: 0.04442569985985756\n",
      "Epoch 18319/30000 Training Loss: 0.03869984671473503\n",
      "Epoch 18320/30000 Training Loss: 0.044376350939273834\n",
      "Epoch 18321/30000 Training Loss: 0.058849722146987915\n",
      "Epoch 18322/30000 Training Loss: 0.04456155002117157\n",
      "Epoch 18323/30000 Training Loss: 0.044921137392520905\n",
      "Epoch 18324/30000 Training Loss: 0.05283570662140846\n",
      "Epoch 18325/30000 Training Loss: 0.04630506783723831\n",
      "Epoch 18326/30000 Training Loss: 0.03957045450806618\n",
      "Epoch 18327/30000 Training Loss: 0.06526243686676025\n",
      "Epoch 18328/30000 Training Loss: 0.057308170944452286\n",
      "Epoch 18329/30000 Training Loss: 0.056743424385786057\n",
      "Epoch 18330/30000 Training Loss: 0.049013279378414154\n",
      "Epoch 18331/30000 Training Loss: 0.05108684301376343\n",
      "Epoch 18332/30000 Training Loss: 0.06759658455848694\n",
      "Epoch 18333/30000 Training Loss: 0.060309723019599915\n",
      "Epoch 18334/30000 Training Loss: 0.03641457483172417\n",
      "Epoch 18335/30000 Training Loss: 0.06040319800376892\n",
      "Epoch 18336/30000 Training Loss: 0.04810671880841255\n",
      "Epoch 18337/30000 Training Loss: 0.038339730352163315\n",
      "Epoch 18338/30000 Training Loss: 0.05511938035488129\n",
      "Epoch 18339/30000 Training Loss: 0.042029671370983124\n",
      "Epoch 18340/30000 Training Loss: 0.04548574611544609\n",
      "Epoch 18341/30000 Training Loss: 0.05904011055827141\n",
      "Epoch 18342/30000 Training Loss: 0.08414416760206223\n",
      "Epoch 18343/30000 Training Loss: 0.043878644704818726\n",
      "Epoch 18344/30000 Training Loss: 0.028506867587566376\n",
      "Epoch 18345/30000 Training Loss: 0.035686466842889786\n",
      "Epoch 18346/30000 Training Loss: 0.04137663543224335\n",
      "Epoch 18347/30000 Training Loss: 0.06679556518793106\n",
      "Epoch 18348/30000 Training Loss: 0.04281264543533325\n",
      "Epoch 18349/30000 Training Loss: 0.056204989552497864\n",
      "Epoch 18350/30000 Training Loss: 0.04138615354895592\n",
      "Epoch 18351/30000 Training Loss: 0.04786863178014755\n",
      "Epoch 18352/30000 Training Loss: 0.05372323468327522\n",
      "Epoch 18353/30000 Training Loss: 0.05217740312218666\n",
      "Epoch 18354/30000 Training Loss: 0.054802149534225464\n",
      "Epoch 18355/30000 Training Loss: 0.059957679361104965\n",
      "Epoch 18356/30000 Training Loss: 0.04154941812157631\n",
      "Epoch 18357/30000 Training Loss: 0.05904415249824524\n",
      "Epoch 18358/30000 Training Loss: 0.05814877897500992\n",
      "Epoch 18359/30000 Training Loss: 0.028852354735136032\n",
      "Epoch 18360/30000 Training Loss: 0.06105630844831467\n",
      "Epoch 18361/30000 Training Loss: 0.0564643070101738\n",
      "Epoch 18362/30000 Training Loss: 0.048245348036289215\n",
      "Epoch 18363/30000 Training Loss: 0.03988003730773926\n",
      "Epoch 18364/30000 Training Loss: 0.04269007593393326\n",
      "Epoch 18365/30000 Training Loss: 0.04001755267381668\n",
      "Epoch 18366/30000 Training Loss: 0.04997273534536362\n",
      "Epoch 18367/30000 Training Loss: 0.042009130120277405\n",
      "Epoch 18368/30000 Training Loss: 0.04586510360240936\n",
      "Epoch 18369/30000 Training Loss: 0.05167088657617569\n",
      "Epoch 18370/30000 Training Loss: 0.05453368276357651\n",
      "Epoch 18371/30000 Training Loss: 0.03942131996154785\n",
      "Epoch 18372/30000 Training Loss: 0.03880029171705246\n",
      "Epoch 18373/30000 Training Loss: 0.06254245340824127\n",
      "Epoch 18374/30000 Training Loss: 0.05302080512046814\n",
      "Epoch 18375/30000 Training Loss: 0.04796716570854187\n",
      "Epoch 18376/30000 Training Loss: 0.04197430610656738\n",
      "Epoch 18377/30000 Training Loss: 0.037494245916604996\n",
      "Epoch 18378/30000 Training Loss: 0.048269063234329224\n",
      "Epoch 18379/30000 Training Loss: 0.05163588002324104\n",
      "Epoch 18380/30000 Training Loss: 0.047829460352659225\n",
      "Epoch 18381/30000 Training Loss: 0.05273708701133728\n",
      "Epoch 18382/30000 Training Loss: 0.04413441941142082\n",
      "Epoch 18383/30000 Training Loss: 0.036202605813741684\n",
      "Epoch 18384/30000 Training Loss: 0.04583321884274483\n",
      "Epoch 18385/30000 Training Loss: 0.05781062692403793\n",
      "Epoch 18386/30000 Training Loss: 0.045256439596414566\n",
      "Epoch 18387/30000 Training Loss: 0.048269420862197876\n",
      "Epoch 18388/30000 Training Loss: 0.04901064187288284\n",
      "Epoch 18389/30000 Training Loss: 0.05789615213871002\n",
      "Epoch 18390/30000 Training Loss: 0.04158036783337593\n",
      "Epoch 18391/30000 Training Loss: 0.05210521072149277\n",
      "Epoch 18392/30000 Training Loss: 0.04739956185221672\n",
      "Epoch 18393/30000 Training Loss: 0.06104140356183052\n",
      "Epoch 18394/30000 Training Loss: 0.04974982142448425\n",
      "Epoch 18395/30000 Training Loss: 0.03767047077417374\n",
      "Epoch 18396/30000 Training Loss: 0.04899083450436592\n",
      "Epoch 18397/30000 Training Loss: 0.0550888329744339\n",
      "Epoch 18398/30000 Training Loss: 0.03509321063756943\n",
      "Epoch 18399/30000 Training Loss: 0.05353320389986038\n",
      "Epoch 18400/30000 Training Loss: 0.03840326890349388\n",
      "Epoch 18400/30000 Validation Loss: 0.03945976495742798\n",
      "Epoch 18401/30000 Training Loss: 0.0473344586789608\n",
      "Epoch 18402/30000 Training Loss: 0.03802327439188957\n",
      "Epoch 18403/30000 Training Loss: 0.04060077667236328\n",
      "Epoch 18404/30000 Training Loss: 0.03890722617506981\n",
      "Epoch 18405/30000 Training Loss: 0.03323368728160858\n",
      "Epoch 18406/30000 Training Loss: 0.044763561338186264\n",
      "Epoch 18407/30000 Training Loss: 0.05504019185900688\n",
      "Epoch 18408/30000 Training Loss: 0.03794137388467789\n",
      "Epoch 18409/30000 Training Loss: 0.04580690711736679\n",
      "Epoch 18410/30000 Training Loss: 0.05448541417717934\n",
      "Epoch 18411/30000 Training Loss: 0.051514673978090286\n",
      "Epoch 18412/30000 Training Loss: 0.047921840101480484\n",
      "Epoch 18413/30000 Training Loss: 0.057728733867406845\n",
      "Epoch 18414/30000 Training Loss: 0.043487798422575\n",
      "Epoch 18415/30000 Training Loss: 0.04594073072075844\n",
      "Epoch 18416/30000 Training Loss: 0.04588473215699196\n",
      "Epoch 18417/30000 Training Loss: 0.03693419694900513\n",
      "Epoch 18418/30000 Training Loss: 0.05117539316415787\n",
      "Epoch 18419/30000 Training Loss: 0.043779969215393066\n",
      "Epoch 18420/30000 Training Loss: 0.038435667753219604\n",
      "Epoch 18421/30000 Training Loss: 0.061475351452827454\n",
      "Epoch 18422/30000 Training Loss: 0.051903799176216125\n",
      "Epoch 18423/30000 Training Loss: 0.03792339190840721\n",
      "Epoch 18424/30000 Training Loss: 0.045957889407873154\n",
      "Epoch 18425/30000 Training Loss: 0.05083603784441948\n",
      "Epoch 18426/30000 Training Loss: 0.056287772953510284\n",
      "Epoch 18427/30000 Training Loss: 0.03989137336611748\n",
      "Epoch 18428/30000 Training Loss: 0.05029914155602455\n",
      "Epoch 18429/30000 Training Loss: 0.064259834587574\n",
      "Epoch 18430/30000 Training Loss: 0.043256890028715134\n",
      "Epoch 18431/30000 Training Loss: 0.05210509151220322\n",
      "Epoch 18432/30000 Training Loss: 0.034559860825538635\n",
      "Epoch 18433/30000 Training Loss: 0.04244912788271904\n",
      "Epoch 18434/30000 Training Loss: 0.05979638919234276\n",
      "Epoch 18435/30000 Training Loss: 0.044687047600746155\n",
      "Epoch 18436/30000 Training Loss: 0.042767662554979324\n",
      "Epoch 18437/30000 Training Loss: 0.06527070701122284\n",
      "Epoch 18438/30000 Training Loss: 0.052323658019304276\n",
      "Epoch 18439/30000 Training Loss: 0.07709722965955734\n",
      "Epoch 18440/30000 Training Loss: 0.04236998409032822\n",
      "Epoch 18441/30000 Training Loss: 0.050668735057115555\n",
      "Epoch 18442/30000 Training Loss: 0.05328989773988724\n",
      "Epoch 18443/30000 Training Loss: 0.05287311226129532\n",
      "Epoch 18444/30000 Training Loss: 0.05470690503716469\n",
      "Epoch 18445/30000 Training Loss: 0.04243485629558563\n",
      "Epoch 18446/30000 Training Loss: 0.05349401384592056\n",
      "Epoch 18447/30000 Training Loss: 0.062385447323322296\n",
      "Epoch 18448/30000 Training Loss: 0.03423614427447319\n",
      "Epoch 18449/30000 Training Loss: 0.05369095504283905\n",
      "Epoch 18450/30000 Training Loss: 0.03487076982855797\n",
      "Epoch 18451/30000 Training Loss: 0.040573298931121826\n",
      "Epoch 18452/30000 Training Loss: 0.044716380536556244\n",
      "Epoch 18453/30000 Training Loss: 0.042464178055524826\n",
      "Epoch 18454/30000 Training Loss: 0.05549575760960579\n",
      "Epoch 18455/30000 Training Loss: 0.03477328643202782\n",
      "Epoch 18456/30000 Training Loss: 0.049746930599212646\n",
      "Epoch 18457/30000 Training Loss: 0.05209222808480263\n",
      "Epoch 18458/30000 Training Loss: 0.05620291829109192\n",
      "Epoch 18459/30000 Training Loss: 0.0614679753780365\n",
      "Epoch 18460/30000 Training Loss: 0.03873895853757858\n",
      "Epoch 18461/30000 Training Loss: 0.04835580661892891\n",
      "Epoch 18462/30000 Training Loss: 0.0356624573469162\n",
      "Epoch 18463/30000 Training Loss: 0.04081597179174423\n",
      "Epoch 18464/30000 Training Loss: 0.03509414568543434\n",
      "Epoch 18465/30000 Training Loss: 0.04331156238913536\n",
      "Epoch 18466/30000 Training Loss: 0.032647330313920975\n",
      "Epoch 18467/30000 Training Loss: 0.04581547528505325\n",
      "Epoch 18468/30000 Training Loss: 0.05893765389919281\n",
      "Epoch 18469/30000 Training Loss: 0.047495387494564056\n",
      "Epoch 18470/30000 Training Loss: 0.050599247217178345\n",
      "Epoch 18471/30000 Training Loss: 0.04704839736223221\n",
      "Epoch 18472/30000 Training Loss: 0.03739755600690842\n",
      "Epoch 18473/30000 Training Loss: 0.06106063351035118\n",
      "Epoch 18474/30000 Training Loss: 0.05502909794449806\n",
      "Epoch 18475/30000 Training Loss: 0.059943847358226776\n",
      "Epoch 18476/30000 Training Loss: 0.06098523736000061\n",
      "Epoch 18477/30000 Training Loss: 0.05237288028001785\n",
      "Epoch 18478/30000 Training Loss: 0.04967275261878967\n",
      "Epoch 18479/30000 Training Loss: 0.04504677653312683\n",
      "Epoch 18480/30000 Training Loss: 0.037034772336483\n",
      "Epoch 18481/30000 Training Loss: 0.04233115166425705\n",
      "Epoch 18482/30000 Training Loss: 0.05575248599052429\n",
      "Epoch 18483/30000 Training Loss: 0.055435873568058014\n",
      "Epoch 18484/30000 Training Loss: 0.03895014524459839\n",
      "Epoch 18485/30000 Training Loss: 0.03182833641767502\n",
      "Epoch 18486/30000 Training Loss: 0.05011076480150223\n",
      "Epoch 18487/30000 Training Loss: 0.0535135455429554\n",
      "Epoch 18488/30000 Training Loss: 0.07376398891210556\n",
      "Epoch 18489/30000 Training Loss: 0.055280767381191254\n",
      "Epoch 18490/30000 Training Loss: 0.044529929757118225\n",
      "Epoch 18491/30000 Training Loss: 0.03589196503162384\n",
      "Epoch 18492/30000 Training Loss: 0.060042720288038254\n",
      "Epoch 18493/30000 Training Loss: 0.05219912528991699\n",
      "Epoch 18494/30000 Training Loss: 0.050295621156692505\n",
      "Epoch 18495/30000 Training Loss: 0.04859215393662453\n",
      "Epoch 18496/30000 Training Loss: 0.039068203419446945\n",
      "Epoch 18497/30000 Training Loss: 0.04941139742732048\n",
      "Epoch 18498/30000 Training Loss: 0.05571645498275757\n",
      "Epoch 18499/30000 Training Loss: 0.051279954612255096\n",
      "Epoch 18500/30000 Training Loss: 0.056001219898462296\n",
      "Epoch 18500/30000 Validation Loss: 0.03349323943257332\n",
      "Epoch 18501/30000 Training Loss: 0.05216421186923981\n",
      "Epoch 18502/30000 Training Loss: 0.046829696744680405\n",
      "Epoch 18503/30000 Training Loss: 0.04970517382025719\n",
      "Epoch 18504/30000 Training Loss: 0.0444510243833065\n",
      "Epoch 18505/30000 Training Loss: 0.05218531936407089\n",
      "Epoch 18506/30000 Training Loss: 0.0398908406496048\n",
      "Epoch 18507/30000 Training Loss: 0.04553239420056343\n",
      "Epoch 18508/30000 Training Loss: 0.048294566571712494\n",
      "Epoch 18509/30000 Training Loss: 0.05048936605453491\n",
      "Epoch 18510/30000 Training Loss: 0.054173074662685394\n",
      "Epoch 18511/30000 Training Loss: 0.054603613913059235\n",
      "Epoch 18512/30000 Training Loss: 0.05471174791455269\n",
      "Epoch 18513/30000 Training Loss: 0.046461790800094604\n",
      "Epoch 18514/30000 Training Loss: 0.05215460807085037\n",
      "Epoch 18515/30000 Training Loss: 0.04314517602324486\n",
      "Epoch 18516/30000 Training Loss: 0.05723288655281067\n",
      "Epoch 18517/30000 Training Loss: 0.03900163993239403\n",
      "Epoch 18518/30000 Training Loss: 0.04417285695672035\n",
      "Epoch 18519/30000 Training Loss: 0.04799610376358032\n",
      "Epoch 18520/30000 Training Loss: 0.05486700311303139\n",
      "Epoch 18521/30000 Training Loss: 0.03749905899167061\n",
      "Epoch 18522/30000 Training Loss: 0.037418805062770844\n",
      "Epoch 18523/30000 Training Loss: 0.039029330015182495\n",
      "Epoch 18524/30000 Training Loss: 0.051518019288778305\n",
      "Epoch 18525/30000 Training Loss: 0.05197589099407196\n",
      "Epoch 18526/30000 Training Loss: 0.06214224174618721\n",
      "Epoch 18527/30000 Training Loss: 0.04635830968618393\n",
      "Epoch 18528/30000 Training Loss: 0.057944588363170624\n",
      "Epoch 18529/30000 Training Loss: 0.058614857494831085\n",
      "Epoch 18530/30000 Training Loss: 0.058318380266427994\n",
      "Epoch 18531/30000 Training Loss: 0.033348605036735535\n",
      "Epoch 18532/30000 Training Loss: 0.049133580178022385\n",
      "Epoch 18533/30000 Training Loss: 0.04513167962431908\n",
      "Epoch 18534/30000 Training Loss: 0.03881792351603508\n",
      "Epoch 18535/30000 Training Loss: 0.05697185546159744\n",
      "Epoch 18536/30000 Training Loss: 0.042589303106069565\n",
      "Epoch 18537/30000 Training Loss: 0.055689603090286255\n",
      "Epoch 18538/30000 Training Loss: 0.05386689305305481\n",
      "Epoch 18539/30000 Training Loss: 0.039373576641082764\n",
      "Epoch 18540/30000 Training Loss: 0.05119771137833595\n",
      "Epoch 18541/30000 Training Loss: 0.0372290313243866\n",
      "Epoch 18542/30000 Training Loss: 0.04893019050359726\n",
      "Epoch 18543/30000 Training Loss: 0.05301529914140701\n",
      "Epoch 18544/30000 Training Loss: 0.048824239522218704\n",
      "Epoch 18545/30000 Training Loss: 0.04660838842391968\n",
      "Epoch 18546/30000 Training Loss: 0.060210078954696655\n",
      "Epoch 18547/30000 Training Loss: 0.05675448849797249\n",
      "Epoch 18548/30000 Training Loss: 0.03970438987016678\n",
      "Epoch 18549/30000 Training Loss: 0.057698220014572144\n",
      "Epoch 18550/30000 Training Loss: 0.056913185864686966\n",
      "Epoch 18551/30000 Training Loss: 0.072466179728508\n",
      "Epoch 18552/30000 Training Loss: 0.05449901521205902\n",
      "Epoch 18553/30000 Training Loss: 0.04020209237933159\n",
      "Epoch 18554/30000 Training Loss: 0.0518968366086483\n",
      "Epoch 18555/30000 Training Loss: 0.03254122659564018\n",
      "Epoch 18556/30000 Training Loss: 0.0532664954662323\n",
      "Epoch 18557/30000 Training Loss: 0.051578376442193985\n",
      "Epoch 18558/30000 Training Loss: 0.04263566806912422\n",
      "Epoch 18559/30000 Training Loss: 0.05104377493262291\n",
      "Epoch 18560/30000 Training Loss: 0.03832288458943367\n",
      "Epoch 18561/30000 Training Loss: 0.0319596491754055\n",
      "Epoch 18562/30000 Training Loss: 0.048845261335372925\n",
      "Epoch 18563/30000 Training Loss: 0.06536854803562164\n",
      "Epoch 18564/30000 Training Loss: 0.06031383201479912\n",
      "Epoch 18565/30000 Training Loss: 0.04068542271852493\n",
      "Epoch 18566/30000 Training Loss: 0.06535933911800385\n",
      "Epoch 18567/30000 Training Loss: 0.030261782929301262\n",
      "Epoch 18568/30000 Training Loss: 0.057347580790519714\n",
      "Epoch 18569/30000 Training Loss: 0.059651900082826614\n",
      "Epoch 18570/30000 Training Loss: 0.06274355202913284\n",
      "Epoch 18571/30000 Training Loss: 0.05629994347691536\n",
      "Epoch 18572/30000 Training Loss: 0.04361365735530853\n",
      "Epoch 18573/30000 Training Loss: 0.038852088153362274\n",
      "Epoch 18574/30000 Training Loss: 0.043795567005872726\n",
      "Epoch 18575/30000 Training Loss: 0.05453325808048248\n",
      "Epoch 18576/30000 Training Loss: 0.04864845424890518\n",
      "Epoch 18577/30000 Training Loss: 0.05045441910624504\n",
      "Epoch 18578/30000 Training Loss: 0.04183143749833107\n",
      "Epoch 18579/30000 Training Loss: 0.05497172847390175\n",
      "Epoch 18580/30000 Training Loss: 0.0438811369240284\n",
      "Epoch 18581/30000 Training Loss: 0.06846459209918976\n",
      "Epoch 18582/30000 Training Loss: 0.04326245188713074\n",
      "Epoch 18583/30000 Training Loss: 0.0440070703625679\n",
      "Epoch 18584/30000 Training Loss: 0.0585273839533329\n",
      "Epoch 18585/30000 Training Loss: 0.050414375960826874\n",
      "Epoch 18586/30000 Training Loss: 0.03816397860646248\n",
      "Epoch 18587/30000 Training Loss: 0.04526153579354286\n",
      "Epoch 18588/30000 Training Loss: 0.03512522578239441\n",
      "Epoch 18589/30000 Training Loss: 0.046666137874126434\n",
      "Epoch 18590/30000 Training Loss: 0.05007532984018326\n",
      "Epoch 18591/30000 Training Loss: 0.03982870653271675\n",
      "Epoch 18592/30000 Training Loss: 0.04068700969219208\n",
      "Epoch 18593/30000 Training Loss: 0.045354850590229034\n",
      "Epoch 18594/30000 Training Loss: 0.05415405333042145\n",
      "Epoch 18595/30000 Training Loss: 0.051896125078201294\n",
      "Epoch 18596/30000 Training Loss: 0.04268674924969673\n",
      "Epoch 18597/30000 Training Loss: 0.05454590916633606\n",
      "Epoch 18598/30000 Training Loss: 0.05343417078256607\n",
      "Epoch 18599/30000 Training Loss: 0.05247662961483002\n",
      "Epoch 18600/30000 Training Loss: 0.05312686786055565\n",
      "Epoch 18600/30000 Validation Loss: 0.057732954621315\n",
      "Epoch 18601/30000 Training Loss: 0.05567379668354988\n",
      "Epoch 18602/30000 Training Loss: 0.03939363360404968\n",
      "Epoch 18603/30000 Training Loss: 0.04491029307246208\n",
      "Epoch 18604/30000 Training Loss: 0.05178440362215042\n",
      "Epoch 18605/30000 Training Loss: 0.04884430393576622\n",
      "Epoch 18606/30000 Training Loss: 0.04621762037277222\n",
      "Epoch 18607/30000 Training Loss: 0.052154671400785446\n",
      "Epoch 18608/30000 Training Loss: 0.051710136234760284\n",
      "Epoch 18609/30000 Training Loss: 0.05903968960046768\n",
      "Epoch 18610/30000 Training Loss: 0.04977123811841011\n",
      "Epoch 18611/30000 Training Loss: 0.03580329939723015\n",
      "Epoch 18612/30000 Training Loss: 0.039070386439561844\n",
      "Epoch 18613/30000 Training Loss: 0.051621221005916595\n",
      "Epoch 18614/30000 Training Loss: 0.04530749469995499\n",
      "Epoch 18615/30000 Training Loss: 0.03956441953778267\n",
      "Epoch 18616/30000 Training Loss: 0.04387160390615463\n",
      "Epoch 18617/30000 Training Loss: 0.038381725549697876\n",
      "Epoch 18618/30000 Training Loss: 0.0551777184009552\n",
      "Epoch 18619/30000 Training Loss: 0.04796062782406807\n",
      "Epoch 18620/30000 Training Loss: 0.04027295112609863\n",
      "Epoch 18621/30000 Training Loss: 0.043303340673446655\n",
      "Epoch 18622/30000 Training Loss: 0.04593299701809883\n",
      "Epoch 18623/30000 Training Loss: 0.06004617363214493\n",
      "Epoch 18624/30000 Training Loss: 0.03611123561859131\n",
      "Epoch 18625/30000 Training Loss: 0.05821875482797623\n",
      "Epoch 18626/30000 Training Loss: 0.05181705951690674\n",
      "Epoch 18627/30000 Training Loss: 0.04042051360011101\n",
      "Epoch 18628/30000 Training Loss: 0.0408075749874115\n",
      "Epoch 18629/30000 Training Loss: 0.039057932794094086\n",
      "Epoch 18630/30000 Training Loss: 0.052753426134586334\n",
      "Epoch 18631/30000 Training Loss: 0.04775862395763397\n",
      "Epoch 18632/30000 Training Loss: 0.04091758280992508\n",
      "Epoch 18633/30000 Training Loss: 0.04659249633550644\n",
      "Epoch 18634/30000 Training Loss: 0.0639486014842987\n",
      "Epoch 18635/30000 Training Loss: 0.053354207426309586\n",
      "Epoch 18636/30000 Training Loss: 0.0499599426984787\n",
      "Epoch 18637/30000 Training Loss: 0.043878406286239624\n",
      "Epoch 18638/30000 Training Loss: 0.04682060331106186\n",
      "Epoch 18639/30000 Training Loss: 0.051711566746234894\n",
      "Epoch 18640/30000 Training Loss: 0.04289733245968819\n",
      "Epoch 18641/30000 Training Loss: 0.04311269521713257\n",
      "Epoch 18642/30000 Training Loss: 0.05069616064429283\n",
      "Epoch 18643/30000 Training Loss: 0.07220669090747833\n",
      "Epoch 18644/30000 Training Loss: 0.049576498568058014\n",
      "Epoch 18645/30000 Training Loss: 0.05473925918340683\n",
      "Epoch 18646/30000 Training Loss: 0.03800828754901886\n",
      "Epoch 18647/30000 Training Loss: 0.052262481302022934\n",
      "Epoch 18648/30000 Training Loss: 0.043617237359285355\n",
      "Epoch 18649/30000 Training Loss: 0.04527106508612633\n",
      "Epoch 18650/30000 Training Loss: 0.04233187809586525\n",
      "Epoch 18651/30000 Training Loss: 0.06536678969860077\n",
      "Epoch 18652/30000 Training Loss: 0.06026008352637291\n",
      "Epoch 18653/30000 Training Loss: 0.03874136880040169\n",
      "Epoch 18654/30000 Training Loss: 0.04684131592512131\n",
      "Epoch 18655/30000 Training Loss: 0.06533017754554749\n",
      "Epoch 18656/30000 Training Loss: 0.040394268929958344\n",
      "Epoch 18657/30000 Training Loss: 0.031704600900411606\n",
      "Epoch 18658/30000 Training Loss: 0.06041676551103592\n",
      "Epoch 18659/30000 Training Loss: 0.03528866544365883\n",
      "Epoch 18660/30000 Training Loss: 0.04808884859085083\n",
      "Epoch 18661/30000 Training Loss: 0.04554092139005661\n",
      "Epoch 18662/30000 Training Loss: 0.04544895887374878\n",
      "Epoch 18663/30000 Training Loss: 0.050228580832481384\n",
      "Epoch 18664/30000 Training Loss: 0.05116964131593704\n",
      "Epoch 18665/30000 Training Loss: 0.04758576676249504\n",
      "Epoch 18666/30000 Training Loss: 0.054506827145814896\n",
      "Epoch 18667/30000 Training Loss: 0.04295647144317627\n",
      "Epoch 18668/30000 Training Loss: 0.04170936718583107\n",
      "Epoch 18669/30000 Training Loss: 0.06018014997243881\n",
      "Epoch 18670/30000 Training Loss: 0.04205044358968735\n",
      "Epoch 18671/30000 Training Loss: 0.04839450120925903\n",
      "Epoch 18672/30000 Training Loss: 0.05841047316789627\n",
      "Epoch 18673/30000 Training Loss: 0.0523480586707592\n",
      "Epoch 18674/30000 Training Loss: 0.04858791083097458\n",
      "Epoch 18675/30000 Training Loss: 0.05225789546966553\n",
      "Epoch 18676/30000 Training Loss: 0.05329619720578194\n",
      "Epoch 18677/30000 Training Loss: 0.04233128949999809\n",
      "Epoch 18678/30000 Training Loss: 0.05159936100244522\n",
      "Epoch 18679/30000 Training Loss: 0.054900944232940674\n",
      "Epoch 18680/30000 Training Loss: 0.05815810710191727\n",
      "Epoch 18681/30000 Training Loss: 0.04433141648769379\n",
      "Epoch 18682/30000 Training Loss: 0.059197165071964264\n",
      "Epoch 18683/30000 Training Loss: 0.037988487631082535\n",
      "Epoch 18684/30000 Training Loss: 0.04385571926832199\n",
      "Epoch 18685/30000 Training Loss: 0.048410944640636444\n",
      "Epoch 18686/30000 Training Loss: 0.045946817845106125\n",
      "Epoch 18687/30000 Training Loss: 0.03586095944046974\n",
      "Epoch 18688/30000 Training Loss: 0.04787092283368111\n",
      "Epoch 18689/30000 Training Loss: 0.059763580560684204\n",
      "Epoch 18690/30000 Training Loss: 0.03626848757266998\n",
      "Epoch 18691/30000 Training Loss: 0.047912195324897766\n",
      "Epoch 18692/30000 Training Loss: 0.03702389821410179\n",
      "Epoch 18693/30000 Training Loss: 0.03339773416519165\n",
      "Epoch 18694/30000 Training Loss: 0.04114041477441788\n",
      "Epoch 18695/30000 Training Loss: 0.053010810166597366\n",
      "Epoch 18696/30000 Training Loss: 0.054893411695957184\n",
      "Epoch 18697/30000 Training Loss: 0.03719329833984375\n",
      "Epoch 18698/30000 Training Loss: 0.05303382873535156\n",
      "Epoch 18699/30000 Training Loss: 0.04031654819846153\n",
      "Epoch 18700/30000 Training Loss: 0.057644184678792953\n",
      "Epoch 18700/30000 Validation Loss: 0.035945892333984375\n",
      "Epoch 18701/30000 Training Loss: 0.05687011033296585\n",
      "Epoch 18702/30000 Training Loss: 0.04525000602006912\n",
      "Epoch 18703/30000 Training Loss: 0.06486102193593979\n",
      "Epoch 18704/30000 Training Loss: 0.03750580921769142\n",
      "Epoch 18705/30000 Training Loss: 0.07161813229322433\n",
      "Epoch 18706/30000 Training Loss: 0.057624515146017075\n",
      "Epoch 18707/30000 Training Loss: 0.05108693242073059\n",
      "Epoch 18708/30000 Training Loss: 0.041592635214328766\n",
      "Epoch 18709/30000 Training Loss: 0.05468052625656128\n",
      "Epoch 18710/30000 Training Loss: 0.0363864004611969\n",
      "Epoch 18711/30000 Training Loss: 0.034036196768283844\n",
      "Epoch 18712/30000 Training Loss: 0.046158865094184875\n",
      "Epoch 18713/30000 Training Loss: 0.042922355234622955\n",
      "Epoch 18714/30000 Training Loss: 0.04605242609977722\n",
      "Epoch 18715/30000 Training Loss: 0.0502333827316761\n",
      "Epoch 18716/30000 Training Loss: 0.04128967970609665\n",
      "Epoch 18717/30000 Training Loss: 0.051704056560993195\n",
      "Epoch 18718/30000 Training Loss: 0.03648446127772331\n",
      "Epoch 18719/30000 Training Loss: 0.04619910940527916\n",
      "Epoch 18720/30000 Training Loss: 0.05586341768503189\n",
      "Epoch 18721/30000 Training Loss: 0.053281258791685104\n",
      "Epoch 18722/30000 Training Loss: 0.04770486801862717\n",
      "Epoch 18723/30000 Training Loss: 0.055039286613464355\n",
      "Epoch 18724/30000 Training Loss: 0.04948540776968002\n",
      "Epoch 18725/30000 Training Loss: 0.05155676230788231\n",
      "Epoch 18726/30000 Training Loss: 0.05218832939863205\n",
      "Epoch 18727/30000 Training Loss: 0.04243672639131546\n",
      "Epoch 18728/30000 Training Loss: 0.05843748152256012\n",
      "Epoch 18729/30000 Training Loss: 0.04710306227207184\n",
      "Epoch 18730/30000 Training Loss: 0.04710144177079201\n",
      "Epoch 18731/30000 Training Loss: 0.045330386608839035\n",
      "Epoch 18732/30000 Training Loss: 0.03525079786777496\n",
      "Epoch 18733/30000 Training Loss: 0.05682521313428879\n",
      "Epoch 18734/30000 Training Loss: 0.035850826650857925\n",
      "Epoch 18735/30000 Training Loss: 0.04136054962873459\n",
      "Epoch 18736/30000 Training Loss: 0.03972339257597923\n",
      "Epoch 18737/30000 Training Loss: 0.045004889369010925\n",
      "Epoch 18738/30000 Training Loss: 0.037861891090869904\n",
      "Epoch 18739/30000 Training Loss: 0.06555528193712234\n",
      "Epoch 18740/30000 Training Loss: 0.041661668568849564\n",
      "Epoch 18741/30000 Training Loss: 0.041629791259765625\n",
      "Epoch 18742/30000 Training Loss: 0.0372895710170269\n",
      "Epoch 18743/30000 Training Loss: 0.0456257089972496\n",
      "Epoch 18744/30000 Training Loss: 0.04347240924835205\n",
      "Epoch 18745/30000 Training Loss: 0.04725228250026703\n",
      "Epoch 18746/30000 Training Loss: 0.046708546578884125\n",
      "Epoch 18747/30000 Training Loss: 0.057541824877262115\n",
      "Epoch 18748/30000 Training Loss: 0.039777860045433044\n",
      "Epoch 18749/30000 Training Loss: 0.03851049393415451\n",
      "Epoch 18750/30000 Training Loss: 0.056131429970264435\n",
      "Epoch 18751/30000 Training Loss: 0.04337499290704727\n",
      "Epoch 18752/30000 Training Loss: 0.04646940901875496\n",
      "Epoch 18753/30000 Training Loss: 0.04468867927789688\n",
      "Epoch 18754/30000 Training Loss: 0.04592448100447655\n",
      "Epoch 18755/30000 Training Loss: 0.0426168292760849\n",
      "Epoch 18756/30000 Training Loss: 0.03424084186553955\n",
      "Epoch 18757/30000 Training Loss: 0.04520347714424133\n",
      "Epoch 18758/30000 Training Loss: 0.061441291123628616\n",
      "Epoch 18759/30000 Training Loss: 0.04900035634636879\n",
      "Epoch 18760/30000 Training Loss: 0.040948525071144104\n",
      "Epoch 18761/30000 Training Loss: 0.0472228080034256\n",
      "Epoch 18762/30000 Training Loss: 0.04399038478732109\n",
      "Epoch 18763/30000 Training Loss: 0.05160350352525711\n",
      "Epoch 18764/30000 Training Loss: 0.06661966443061829\n",
      "Epoch 18765/30000 Training Loss: 0.053092096000909805\n",
      "Epoch 18766/30000 Training Loss: 0.045212727040052414\n",
      "Epoch 18767/30000 Training Loss: 0.05390392243862152\n",
      "Epoch 18768/30000 Training Loss: 0.042635172605514526\n",
      "Epoch 18769/30000 Training Loss: 0.04733654856681824\n",
      "Epoch 18770/30000 Training Loss: 0.04467937722802162\n",
      "Epoch 18771/30000 Training Loss: 0.04255110025405884\n",
      "Epoch 18772/30000 Training Loss: 0.07365529984235764\n",
      "Epoch 18773/30000 Training Loss: 0.07176299393177032\n",
      "Epoch 18774/30000 Training Loss: 0.04712718725204468\n",
      "Epoch 18775/30000 Training Loss: 0.04915131255984306\n",
      "Epoch 18776/30000 Training Loss: 0.03483228757977486\n",
      "Epoch 18777/30000 Training Loss: 0.057744838297367096\n",
      "Epoch 18778/30000 Training Loss: 0.034754678606987\n",
      "Epoch 18779/30000 Training Loss: 0.04543120414018631\n",
      "Epoch 18780/30000 Training Loss: 0.03951358050107956\n",
      "Epoch 18781/30000 Training Loss: 0.03802185505628586\n",
      "Epoch 18782/30000 Training Loss: 0.04201424494385719\n",
      "Epoch 18783/30000 Training Loss: 0.0444180890917778\n",
      "Epoch 18784/30000 Training Loss: 0.03897631913423538\n",
      "Epoch 18785/30000 Training Loss: 0.04639257863163948\n",
      "Epoch 18786/30000 Training Loss: 0.048387233167886734\n",
      "Epoch 18787/30000 Training Loss: 0.05113692581653595\n",
      "Epoch 18788/30000 Training Loss: 0.05053316801786423\n",
      "Epoch 18789/30000 Training Loss: 0.054624587297439575\n",
      "Epoch 18790/30000 Training Loss: 0.04465317726135254\n",
      "Epoch 18791/30000 Training Loss: 0.052634719759225845\n",
      "Epoch 18792/30000 Training Loss: 0.031971488147974014\n",
      "Epoch 18793/30000 Training Loss: 0.03200176730751991\n",
      "Epoch 18794/30000 Training Loss: 0.05479709431529045\n",
      "Epoch 18795/30000 Training Loss: 0.037714071571826935\n",
      "Epoch 18796/30000 Training Loss: 0.04659672826528549\n",
      "Epoch 18797/30000 Training Loss: 0.04243413358926773\n",
      "Epoch 18798/30000 Training Loss: 0.03766880929470062\n",
      "Epoch 18799/30000 Training Loss: 0.05090807378292084\n",
      "Epoch 18800/30000 Training Loss: 0.07282476127147675\n",
      "Epoch 18800/30000 Validation Loss: 0.06185918301343918\n",
      "Epoch 18801/30000 Training Loss: 0.0668177455663681\n",
      "Epoch 18802/30000 Training Loss: 0.0519661083817482\n",
      "Epoch 18803/30000 Training Loss: 0.054347071796655655\n",
      "Epoch 18804/30000 Training Loss: 0.04130810499191284\n",
      "Epoch 18805/30000 Training Loss: 0.05083233118057251\n",
      "Epoch 18806/30000 Training Loss: 0.033348195254802704\n",
      "Epoch 18807/30000 Training Loss: 0.031549371778964996\n",
      "Epoch 18808/30000 Training Loss: 0.044759124517440796\n",
      "Epoch 18809/30000 Training Loss: 0.05043245851993561\n",
      "Epoch 18810/30000 Training Loss: 0.04645388945937157\n",
      "Epoch 18811/30000 Training Loss: 0.06376184523105621\n",
      "Epoch 18812/30000 Training Loss: 0.03383900597691536\n",
      "Epoch 18813/30000 Training Loss: 0.057269323617219925\n",
      "Epoch 18814/30000 Training Loss: 0.047767940908670425\n",
      "Epoch 18815/30000 Training Loss: 0.052371032536029816\n",
      "Epoch 18816/30000 Training Loss: 0.05410381406545639\n",
      "Epoch 18817/30000 Training Loss: 0.04886263608932495\n",
      "Epoch 18818/30000 Training Loss: 0.04892221838235855\n",
      "Epoch 18819/30000 Training Loss: 0.04605632647871971\n",
      "Epoch 18820/30000 Training Loss: 0.05484108626842499\n",
      "Epoch 18821/30000 Training Loss: 0.06182851642370224\n",
      "Epoch 18822/30000 Training Loss: 0.04299288988113403\n",
      "Epoch 18823/30000 Training Loss: 0.04309471696615219\n",
      "Epoch 18824/30000 Training Loss: 0.04836493730545044\n",
      "Epoch 18825/30000 Training Loss: 0.04717428237199783\n",
      "Epoch 18826/30000 Training Loss: 0.05401591956615448\n",
      "Epoch 18827/30000 Training Loss: 0.04878770187497139\n",
      "Epoch 18828/30000 Training Loss: 0.04239411652088165\n",
      "Epoch 18829/30000 Training Loss: 0.05369674414396286\n",
      "Epoch 18830/30000 Training Loss: 0.038629889488220215\n",
      "Epoch 18831/30000 Training Loss: 0.06925563514232635\n",
      "Epoch 18832/30000 Training Loss: 0.04431396722793579\n",
      "Epoch 18833/30000 Training Loss: 0.046362705528736115\n",
      "Epoch 18834/30000 Training Loss: 0.05353809893131256\n",
      "Epoch 18835/30000 Training Loss: 0.07005840539932251\n",
      "Epoch 18836/30000 Training Loss: 0.05066246539354324\n",
      "Epoch 18837/30000 Training Loss: 0.04037284106016159\n",
      "Epoch 18838/30000 Training Loss: 0.04306305572390556\n",
      "Epoch 18839/30000 Training Loss: 0.054823070764541626\n",
      "Epoch 18840/30000 Training Loss: 0.045827172696590424\n",
      "Epoch 18841/30000 Training Loss: 0.04213182255625725\n",
      "Epoch 18842/30000 Training Loss: 0.03933628275990486\n",
      "Epoch 18843/30000 Training Loss: 0.0392039529979229\n",
      "Epoch 18844/30000 Training Loss: 0.04179910197854042\n",
      "Epoch 18845/30000 Training Loss: 0.04349282383918762\n",
      "Epoch 18846/30000 Training Loss: 0.03549224138259888\n",
      "Epoch 18847/30000 Training Loss: 0.06393267214298248\n",
      "Epoch 18848/30000 Training Loss: 0.04486842826008797\n",
      "Epoch 18849/30000 Training Loss: 0.03950338810682297\n",
      "Epoch 18850/30000 Training Loss: 0.040616437792778015\n",
      "Epoch 18851/30000 Training Loss: 0.04243513569235802\n",
      "Epoch 18852/30000 Training Loss: 0.04970050975680351\n",
      "Epoch 18853/30000 Training Loss: 0.04461364448070526\n",
      "Epoch 18854/30000 Training Loss: 0.04611651226878166\n",
      "Epoch 18855/30000 Training Loss: 0.04994998872280121\n",
      "Epoch 18856/30000 Training Loss: 0.05211910977959633\n",
      "Epoch 18857/30000 Training Loss: 0.048966385424137115\n",
      "Epoch 18858/30000 Training Loss: 0.04457078129053116\n",
      "Epoch 18859/30000 Training Loss: 0.05109654366970062\n",
      "Epoch 18860/30000 Training Loss: 0.0513981394469738\n",
      "Epoch 18861/30000 Training Loss: 0.045454688370227814\n",
      "Epoch 18862/30000 Training Loss: 0.040343161672353745\n",
      "Epoch 18863/30000 Training Loss: 0.04212529584765434\n",
      "Epoch 18864/30000 Training Loss: 0.052290380001068115\n",
      "Epoch 18865/30000 Training Loss: 0.04219945892691612\n",
      "Epoch 18866/30000 Training Loss: 0.056896112859249115\n",
      "Epoch 18867/30000 Training Loss: 0.03834014758467674\n",
      "Epoch 18868/30000 Training Loss: 0.05230278894305229\n",
      "Epoch 18869/30000 Training Loss: 0.05001575127243996\n",
      "Epoch 18870/30000 Training Loss: 0.04719746112823486\n",
      "Epoch 18871/30000 Training Loss: 0.0415009967982769\n",
      "Epoch 18872/30000 Training Loss: 0.04169351980090141\n",
      "Epoch 18873/30000 Training Loss: 0.041773051023483276\n",
      "Epoch 18874/30000 Training Loss: 0.042091578245162964\n",
      "Epoch 18875/30000 Training Loss: 0.04530767351388931\n",
      "Epoch 18876/30000 Training Loss: 0.04654596373438835\n",
      "Epoch 18877/30000 Training Loss: 0.042336590588092804\n",
      "Epoch 18878/30000 Training Loss: 0.04932400956749916\n",
      "Epoch 18879/30000 Training Loss: 0.06495102494955063\n",
      "Epoch 18880/30000 Training Loss: 0.047853462398052216\n",
      "Epoch 18881/30000 Training Loss: 0.04903733730316162\n",
      "Epoch 18882/30000 Training Loss: 0.04985309764742851\n",
      "Epoch 18883/30000 Training Loss: 0.033341698348522186\n",
      "Epoch 18884/30000 Training Loss: 0.04033319279551506\n",
      "Epoch 18885/30000 Training Loss: 0.05548585578799248\n",
      "Epoch 18886/30000 Training Loss: 0.04348026588559151\n",
      "Epoch 18887/30000 Training Loss: 0.04272913560271263\n",
      "Epoch 18888/30000 Training Loss: 0.040387384593486786\n",
      "Epoch 18889/30000 Training Loss: 0.03431084379553795\n",
      "Epoch 18890/30000 Training Loss: 0.04482058808207512\n",
      "Epoch 18891/30000 Training Loss: 0.04074707627296448\n",
      "Epoch 18892/30000 Training Loss: 0.03978128731250763\n",
      "Epoch 18893/30000 Training Loss: 0.04756789654493332\n",
      "Epoch 18894/30000 Training Loss: 0.041909582912921906\n",
      "Epoch 18895/30000 Training Loss: 0.04657938331365585\n",
      "Epoch 18896/30000 Training Loss: 0.06088513135910034\n",
      "Epoch 18897/30000 Training Loss: 0.05718541517853737\n",
      "Epoch 18898/30000 Training Loss: 0.04453842341899872\n",
      "Epoch 18899/30000 Training Loss: 0.038893476128578186\n",
      "Epoch 18900/30000 Training Loss: 0.05242380499839783\n",
      "Epoch 18900/30000 Validation Loss: 0.03935849666595459\n",
      "Epoch 18901/30000 Training Loss: 0.04033348709344864\n",
      "Epoch 18902/30000 Training Loss: 0.05443292111158371\n",
      "Epoch 18903/30000 Training Loss: 0.057324521243572235\n",
      "Epoch 18904/30000 Training Loss: 0.03409278765320778\n",
      "Epoch 18905/30000 Training Loss: 0.0507555827498436\n",
      "Epoch 18906/30000 Training Loss: 0.043791841715574265\n",
      "Epoch 18907/30000 Training Loss: 0.0610007718205452\n",
      "Epoch 18908/30000 Training Loss: 0.048175133764743805\n",
      "Epoch 18909/30000 Training Loss: 0.06687183678150177\n",
      "Epoch 18910/30000 Training Loss: 0.059430260211229324\n",
      "Epoch 18911/30000 Training Loss: 0.05833926051855087\n",
      "Epoch 18912/30000 Training Loss: 0.03666390851140022\n",
      "Epoch 18913/30000 Training Loss: 0.042522132396698\n",
      "Epoch 18914/30000 Training Loss: 0.05840294808149338\n",
      "Epoch 18915/30000 Training Loss: 0.04457581788301468\n",
      "Epoch 18916/30000 Training Loss: 0.048558130860328674\n",
      "Epoch 18917/30000 Training Loss: 0.04862995073199272\n",
      "Epoch 18918/30000 Training Loss: 0.06022294610738754\n",
      "Epoch 18919/30000 Training Loss: 0.05822834372520447\n",
      "Epoch 18920/30000 Training Loss: 0.044866375625133514\n",
      "Epoch 18921/30000 Training Loss: 0.04066689312458038\n",
      "Epoch 18922/30000 Training Loss: 0.04268743097782135\n",
      "Epoch 18923/30000 Training Loss: 0.05959318205714226\n",
      "Epoch 18924/30000 Training Loss: 0.03914337977766991\n",
      "Epoch 18925/30000 Training Loss: 0.04529344663023949\n",
      "Epoch 18926/30000 Training Loss: 0.05526214838027954\n",
      "Epoch 18927/30000 Training Loss: 0.04770267382264137\n",
      "Epoch 18928/30000 Training Loss: 0.044145941734313965\n",
      "Epoch 18929/30000 Training Loss: 0.04351576417684555\n",
      "Epoch 18930/30000 Training Loss: 0.04741620272397995\n",
      "Epoch 18931/30000 Training Loss: 0.04328017681837082\n",
      "Epoch 18932/30000 Training Loss: 0.03802558407187462\n",
      "Epoch 18933/30000 Training Loss: 0.06444752961397171\n",
      "Epoch 18934/30000 Training Loss: 0.041299816220998764\n",
      "Epoch 18935/30000 Training Loss: 0.05127936601638794\n",
      "Epoch 18936/30000 Training Loss: 0.037737295031547546\n",
      "Epoch 18937/30000 Training Loss: 0.05426749587059021\n",
      "Epoch 18938/30000 Training Loss: 0.07820719480514526\n",
      "Epoch 18939/30000 Training Loss: 0.05345774441957474\n",
      "Epoch 18940/30000 Training Loss: 0.055144622921943665\n",
      "Epoch 18941/30000 Training Loss: 0.06885840743780136\n",
      "Epoch 18942/30000 Training Loss: 0.032763611525297165\n",
      "Epoch 18943/30000 Training Loss: 0.051969729363918304\n",
      "Epoch 18944/30000 Training Loss: 0.03845543414354324\n",
      "Epoch 18945/30000 Training Loss: 0.055920422077178955\n",
      "Epoch 18946/30000 Training Loss: 0.033522333949804306\n",
      "Epoch 18947/30000 Training Loss: 0.05848892778158188\n",
      "Epoch 18948/30000 Training Loss: 0.04546605050563812\n",
      "Epoch 18949/30000 Training Loss: 0.04253131523728371\n",
      "Epoch 18950/30000 Training Loss: 0.04809035360813141\n",
      "Epoch 18951/30000 Training Loss: 0.03498690202832222\n",
      "Epoch 18952/30000 Training Loss: 0.07851912081241608\n",
      "Epoch 18953/30000 Training Loss: 0.0566350594162941\n",
      "Epoch 18954/30000 Training Loss: 0.05124622583389282\n",
      "Epoch 18955/30000 Training Loss: 0.05480215325951576\n",
      "Epoch 18956/30000 Training Loss: 0.03542577102780342\n",
      "Epoch 18957/30000 Training Loss: 0.06242861971259117\n",
      "Epoch 18958/30000 Training Loss: 0.04800647869706154\n",
      "Epoch 18959/30000 Training Loss: 0.04692094027996063\n",
      "Epoch 18960/30000 Training Loss: 0.060662783682346344\n",
      "Epoch 18961/30000 Training Loss: 0.053168173879384995\n",
      "Epoch 18962/30000 Training Loss: 0.06304740905761719\n",
      "Epoch 18963/30000 Training Loss: 0.06822414696216583\n",
      "Epoch 18964/30000 Training Loss: 0.04869425296783447\n",
      "Epoch 18965/30000 Training Loss: 0.04111410677433014\n",
      "Epoch 18966/30000 Training Loss: 0.039942581206560135\n",
      "Epoch 18967/30000 Training Loss: 0.054813776165246964\n",
      "Epoch 18968/30000 Training Loss: 0.038680460304021835\n",
      "Epoch 18969/30000 Training Loss: 0.050143782049417496\n",
      "Epoch 18970/30000 Training Loss: 0.059953220188617706\n",
      "Epoch 18971/30000 Training Loss: 0.05140119418501854\n",
      "Epoch 18972/30000 Training Loss: 0.042870160192251205\n",
      "Epoch 18973/30000 Training Loss: 0.03929203003644943\n",
      "Epoch 18974/30000 Training Loss: 0.06288330256938934\n",
      "Epoch 18975/30000 Training Loss: 0.047064319252967834\n",
      "Epoch 18976/30000 Training Loss: 0.048166416585445404\n",
      "Epoch 18977/30000 Training Loss: 0.037167876958847046\n",
      "Epoch 18978/30000 Training Loss: 0.0479915551841259\n",
      "Epoch 18979/30000 Training Loss: 0.048417262732982635\n",
      "Epoch 18980/30000 Training Loss: 0.050723202526569366\n",
      "Epoch 18981/30000 Training Loss: 0.05992558225989342\n",
      "Epoch 18982/30000 Training Loss: 0.0660751610994339\n",
      "Epoch 18983/30000 Training Loss: 0.048358045518398285\n",
      "Epoch 18984/30000 Training Loss: 0.051546234637498856\n",
      "Epoch 18985/30000 Training Loss: 0.061680227518081665\n",
      "Epoch 18986/30000 Training Loss: 0.06203971058130264\n",
      "Epoch 18987/30000 Training Loss: 0.05286572873592377\n",
      "Epoch 18988/30000 Training Loss: 0.04633501172065735\n",
      "Epoch 18989/30000 Training Loss: 0.04402489215135574\n",
      "Epoch 18990/30000 Training Loss: 0.0442633293569088\n",
      "Epoch 18991/30000 Training Loss: 0.05510946363210678\n",
      "Epoch 18992/30000 Training Loss: 0.05139860138297081\n",
      "Epoch 18993/30000 Training Loss: 0.0411117747426033\n",
      "Epoch 18994/30000 Training Loss: 0.04052174463868141\n",
      "Epoch 18995/30000 Training Loss: 0.04750635847449303\n",
      "Epoch 18996/30000 Training Loss: 0.06341757625341415\n",
      "Epoch 18997/30000 Training Loss: 0.04511403664946556\n",
      "Epoch 18998/30000 Training Loss: 0.06263981759548187\n",
      "Epoch 18999/30000 Training Loss: 0.050197627395391464\n",
      "Epoch 19000/30000 Training Loss: 0.03712806478142738\n",
      "Epoch 19000/30000 Validation Loss: 0.04937633126974106\n",
      "Epoch 19001/30000 Training Loss: 0.050550676882267\n",
      "Epoch 19002/30000 Training Loss: 0.04406564682722092\n",
      "Epoch 19003/30000 Training Loss: 0.04637614265084267\n",
      "Epoch 19004/30000 Training Loss: 0.05122913047671318\n",
      "Epoch 19005/30000 Training Loss: 0.047110214829444885\n",
      "Epoch 19006/30000 Training Loss: 0.041491180658340454\n",
      "Epoch 19007/30000 Training Loss: 0.04444289952516556\n",
      "Epoch 19008/30000 Training Loss: 0.05215328931808472\n",
      "Epoch 19009/30000 Training Loss: 0.06509631872177124\n",
      "Epoch 19010/30000 Training Loss: 0.0633571594953537\n",
      "Epoch 19011/30000 Training Loss: 0.05842271447181702\n",
      "Epoch 19012/30000 Training Loss: 0.04070969671010971\n",
      "Epoch 19013/30000 Training Loss: 0.04539765045046806\n",
      "Epoch 19014/30000 Training Loss: 0.039857011288404465\n",
      "Epoch 19015/30000 Training Loss: 0.039841827005147934\n",
      "Epoch 19016/30000 Training Loss: 0.03607733175158501\n",
      "Epoch 19017/30000 Training Loss: 0.03959666192531586\n",
      "Epoch 19018/30000 Training Loss: 0.04117754474282265\n",
      "Epoch 19019/30000 Training Loss: 0.04080140218138695\n",
      "Epoch 19020/30000 Training Loss: 0.0450020506978035\n",
      "Epoch 19021/30000 Training Loss: 0.047386325895786285\n",
      "Epoch 19022/30000 Training Loss: 0.04578536003828049\n",
      "Epoch 19023/30000 Training Loss: 0.03482269495725632\n",
      "Epoch 19024/30000 Training Loss: 0.04964013397693634\n",
      "Epoch 19025/30000 Training Loss: 0.06532595306634903\n",
      "Epoch 19026/30000 Training Loss: 0.026075735688209534\n",
      "Epoch 19027/30000 Training Loss: 0.03497140109539032\n",
      "Epoch 19028/30000 Training Loss: 0.06473606079816818\n",
      "Epoch 19029/30000 Training Loss: 0.042399004101753235\n",
      "Epoch 19030/30000 Training Loss: 0.03611795976758003\n",
      "Epoch 19031/30000 Training Loss: 0.04382065311074257\n",
      "Epoch 19032/30000 Training Loss: 0.04361908137798309\n",
      "Epoch 19033/30000 Training Loss: 0.04466509073972702\n",
      "Epoch 19034/30000 Training Loss: 0.03381069004535675\n",
      "Epoch 19035/30000 Training Loss: 0.04195977374911308\n",
      "Epoch 19036/30000 Training Loss: 0.037402600049972534\n",
      "Epoch 19037/30000 Training Loss: 0.06755412369966507\n",
      "Epoch 19038/30000 Training Loss: 0.05797296017408371\n",
      "Epoch 19039/30000 Training Loss: 0.06624291837215424\n",
      "Epoch 19040/30000 Training Loss: 0.02807604894042015\n",
      "Epoch 19041/30000 Training Loss: 0.050012052059173584\n",
      "Epoch 19042/30000 Training Loss: 0.04641316086053848\n",
      "Epoch 19043/30000 Training Loss: 0.051233287900686264\n",
      "Epoch 19044/30000 Training Loss: 0.04511073976755142\n",
      "Epoch 19045/30000 Training Loss: 0.05174250900745392\n",
      "Epoch 19046/30000 Training Loss: 0.044033657759428024\n",
      "Epoch 19047/30000 Training Loss: 0.04675096645951271\n",
      "Epoch 19048/30000 Training Loss: 0.05388370528817177\n",
      "Epoch 19049/30000 Training Loss: 0.06234976649284363\n",
      "Epoch 19050/30000 Training Loss: 0.047011155635118484\n",
      "Epoch 19051/30000 Training Loss: 0.062257423996925354\n",
      "Epoch 19052/30000 Training Loss: 0.03659989684820175\n",
      "Epoch 19053/30000 Training Loss: 0.04186579957604408\n",
      "Epoch 19054/30000 Training Loss: 0.028325142338871956\n",
      "Epoch 19055/30000 Training Loss: 0.05888592451810837\n",
      "Epoch 19056/30000 Training Loss: 0.046703942120075226\n",
      "Epoch 19057/30000 Training Loss: 0.04953388124704361\n",
      "Epoch 19058/30000 Training Loss: 0.04375052452087402\n",
      "Epoch 19059/30000 Training Loss: 0.060286685824394226\n",
      "Epoch 19060/30000 Training Loss: 0.03925234079360962\n",
      "Epoch 19061/30000 Training Loss: 0.060022253543138504\n",
      "Epoch 19062/30000 Training Loss: 0.05172542855143547\n",
      "Epoch 19063/30000 Training Loss: 0.04853891581296921\n",
      "Epoch 19064/30000 Training Loss: 0.045233044773340225\n",
      "Epoch 19065/30000 Training Loss: 0.03922503441572189\n",
      "Epoch 19066/30000 Training Loss: 0.044428762048482895\n",
      "Epoch 19067/30000 Training Loss: 0.05079057440161705\n",
      "Epoch 19068/30000 Training Loss: 0.039053790271282196\n",
      "Epoch 19069/30000 Training Loss: 0.04743337258696556\n",
      "Epoch 19070/30000 Training Loss: 0.056346695870161057\n",
      "Epoch 19071/30000 Training Loss: 0.05279364436864853\n",
      "Epoch 19072/30000 Training Loss: 0.06735815107822418\n",
      "Epoch 19073/30000 Training Loss: 0.050225354731082916\n",
      "Epoch 19074/30000 Training Loss: 0.0645577684044838\n",
      "Epoch 19075/30000 Training Loss: 0.07622029632329941\n",
      "Epoch 19076/30000 Training Loss: 0.05417688935995102\n",
      "Epoch 19077/30000 Training Loss: 0.03918983042240143\n",
      "Epoch 19078/30000 Training Loss: 0.027848947793245316\n",
      "Epoch 19079/30000 Training Loss: 0.06209798902273178\n",
      "Epoch 19080/30000 Training Loss: 0.038443151861429214\n",
      "Epoch 19081/30000 Training Loss: 0.04264606907963753\n",
      "Epoch 19082/30000 Training Loss: 0.041231829673051834\n",
      "Epoch 19083/30000 Training Loss: 0.0653621181845665\n",
      "Epoch 19084/30000 Training Loss: 0.04403175413608551\n",
      "Epoch 19085/30000 Training Loss: 0.0520014613866806\n",
      "Epoch 19086/30000 Training Loss: 0.04513316601514816\n",
      "Epoch 19087/30000 Training Loss: 0.04012742638587952\n",
      "Epoch 19088/30000 Training Loss: 0.05539962649345398\n",
      "Epoch 19089/30000 Training Loss: 0.04625042527914047\n",
      "Epoch 19090/30000 Training Loss: 0.036668602377176285\n",
      "Epoch 19091/30000 Training Loss: 0.05047670751810074\n",
      "Epoch 19092/30000 Training Loss: 0.052408408373594284\n",
      "Epoch 19093/30000 Training Loss: 0.0517665296792984\n",
      "Epoch 19094/30000 Training Loss: 0.05660732835531235\n",
      "Epoch 19095/30000 Training Loss: 0.05064241588115692\n",
      "Epoch 19096/30000 Training Loss: 0.04542268067598343\n",
      "Epoch 19097/30000 Training Loss: 0.04396955668926239\n",
      "Epoch 19098/30000 Training Loss: 0.04963007569313049\n",
      "Epoch 19099/30000 Training Loss: 0.039892904460430145\n",
      "Epoch 19100/30000 Training Loss: 0.048278920352458954\n",
      "Epoch 19100/30000 Validation Loss: 0.05396416038274765\n",
      "Epoch 19101/30000 Training Loss: 0.039292775094509125\n",
      "Epoch 19102/30000 Training Loss: 0.05083112418651581\n",
      "Epoch 19103/30000 Training Loss: 0.05051160603761673\n",
      "Epoch 19104/30000 Training Loss: 0.04295617714524269\n",
      "Epoch 19105/30000 Training Loss: 0.048198405653238297\n",
      "Epoch 19106/30000 Training Loss: 0.055844664573669434\n",
      "Epoch 19107/30000 Training Loss: 0.04606788977980614\n",
      "Epoch 19108/30000 Training Loss: 0.05351470783352852\n",
      "Epoch 19109/30000 Training Loss: 0.04338769614696503\n",
      "Epoch 19110/30000 Training Loss: 0.042518600821495056\n",
      "Epoch 19111/30000 Training Loss: 0.07183191180229187\n",
      "Epoch 19112/30000 Training Loss: 0.04425986856222153\n",
      "Epoch 19113/30000 Training Loss: 0.044202107936143875\n",
      "Epoch 19114/30000 Training Loss: 0.045275889337062836\n",
      "Epoch 19115/30000 Training Loss: 0.03374626487493515\n",
      "Epoch 19116/30000 Training Loss: 0.037247058004140854\n",
      "Epoch 19117/30000 Training Loss: 0.04488418996334076\n",
      "Epoch 19118/30000 Training Loss: 0.038201697170734406\n",
      "Epoch 19119/30000 Training Loss: 0.051184602081775665\n",
      "Epoch 19120/30000 Training Loss: 0.04544912278652191\n",
      "Epoch 19121/30000 Training Loss: 0.04653838276863098\n",
      "Epoch 19122/30000 Training Loss: 0.05243995040655136\n",
      "Epoch 19123/30000 Training Loss: 0.04360639303922653\n",
      "Epoch 19124/30000 Training Loss: 0.03687986731529236\n",
      "Epoch 19125/30000 Training Loss: 0.04681363329291344\n",
      "Epoch 19126/30000 Training Loss: 0.06937495619058609\n",
      "Epoch 19127/30000 Training Loss: 0.042570654302835464\n",
      "Epoch 19128/30000 Training Loss: 0.04643389955163002\n",
      "Epoch 19129/30000 Training Loss: 0.035914186388254166\n",
      "Epoch 19130/30000 Training Loss: 0.04680784419178963\n",
      "Epoch 19131/30000 Training Loss: 0.046563372015953064\n",
      "Epoch 19132/30000 Training Loss: 0.05505211278796196\n",
      "Epoch 19133/30000 Training Loss: 0.043059248477220535\n",
      "Epoch 19134/30000 Training Loss: 0.04201514273881912\n",
      "Epoch 19135/30000 Training Loss: 0.04100758954882622\n",
      "Epoch 19136/30000 Training Loss: 0.045203689485788345\n",
      "Epoch 19137/30000 Training Loss: 0.04907219484448433\n",
      "Epoch 19138/30000 Training Loss: 0.057940807193517685\n",
      "Epoch 19139/30000 Training Loss: 0.07331669330596924\n",
      "Epoch 19140/30000 Training Loss: 0.0587734691798687\n",
      "Epoch 19141/30000 Training Loss: 0.037352945655584335\n",
      "Epoch 19142/30000 Training Loss: 0.040118258446455\n",
      "Epoch 19143/30000 Training Loss: 0.05036696791648865\n",
      "Epoch 19144/30000 Training Loss: 0.07285035401582718\n",
      "Epoch 19145/30000 Training Loss: 0.05149948224425316\n",
      "Epoch 19146/30000 Training Loss: 0.047912243753671646\n",
      "Epoch 19147/30000 Training Loss: 0.04991526156663895\n",
      "Epoch 19148/30000 Training Loss: 0.0371544286608696\n",
      "Epoch 19149/30000 Training Loss: 0.03789117559790611\n",
      "Epoch 19150/30000 Training Loss: 0.05217877775430679\n",
      "Epoch 19151/30000 Training Loss: 0.045111723244190216\n",
      "Epoch 19152/30000 Training Loss: 0.05317356437444687\n",
      "Epoch 19153/30000 Training Loss: 0.055546052753925323\n",
      "Epoch 19154/30000 Training Loss: 0.048223115503787994\n",
      "Epoch 19155/30000 Training Loss: 0.04022211953997612\n",
      "Epoch 19156/30000 Training Loss: 0.05740191042423248\n",
      "Epoch 19157/30000 Training Loss: 0.05233045667409897\n",
      "Epoch 19158/30000 Training Loss: 0.049094729125499725\n",
      "Epoch 19159/30000 Training Loss: 0.05399433523416519\n",
      "Epoch 19160/30000 Training Loss: 0.043093740940093994\n",
      "Epoch 19161/30000 Training Loss: 0.039255429059267044\n",
      "Epoch 19162/30000 Training Loss: 0.04627527669072151\n",
      "Epoch 19163/30000 Training Loss: 0.04773808270692825\n",
      "Epoch 19164/30000 Training Loss: 0.0354415662586689\n",
      "Epoch 19165/30000 Training Loss: 0.03570310398936272\n",
      "Epoch 19166/30000 Training Loss: 0.05114147439599037\n",
      "Epoch 19167/30000 Training Loss: 0.03607058897614479\n",
      "Epoch 19168/30000 Training Loss: 0.05124067887663841\n",
      "Epoch 19169/30000 Training Loss: 0.047221746295690536\n",
      "Epoch 19170/30000 Training Loss: 0.042462944984436035\n",
      "Epoch 19171/30000 Training Loss: 0.040308114141225815\n",
      "Epoch 19172/30000 Training Loss: 0.041634757071733475\n",
      "Epoch 19173/30000 Training Loss: 0.04062908887863159\n",
      "Epoch 19174/30000 Training Loss: 0.0561915785074234\n",
      "Epoch 19175/30000 Training Loss: 0.052986398339271545\n",
      "Epoch 19176/30000 Training Loss: 0.0367310456931591\n",
      "Epoch 19177/30000 Training Loss: 0.040787845849990845\n",
      "Epoch 19178/30000 Training Loss: 0.044745493680238724\n",
      "Epoch 19179/30000 Training Loss: 0.07265269011259079\n",
      "Epoch 19180/30000 Training Loss: 0.04488128423690796\n",
      "Epoch 19181/30000 Training Loss: 0.03962499648332596\n",
      "Epoch 19182/30000 Training Loss: 0.05365261808037758\n",
      "Epoch 19183/30000 Training Loss: 0.03899351879954338\n",
      "Epoch 19184/30000 Training Loss: 0.045809559524059296\n",
      "Epoch 19185/30000 Training Loss: 0.06320599466562271\n",
      "Epoch 19186/30000 Training Loss: 0.0542464554309845\n",
      "Epoch 19187/30000 Training Loss: 0.04238799959421158\n",
      "Epoch 19188/30000 Training Loss: 0.041200362145900726\n",
      "Epoch 19189/30000 Training Loss: 0.04162701964378357\n",
      "Epoch 19190/30000 Training Loss: 0.03697037324309349\n",
      "Epoch 19191/30000 Training Loss: 0.04872415214776993\n",
      "Epoch 19192/30000 Training Loss: 0.04551433399319649\n",
      "Epoch 19193/30000 Training Loss: 0.045126087963581085\n",
      "Epoch 19194/30000 Training Loss: 0.032075051218271255\n",
      "Epoch 19195/30000 Training Loss: 0.042569901794195175\n",
      "Epoch 19196/30000 Training Loss: 0.04478280618786812\n",
      "Epoch 19197/30000 Training Loss: 0.037008509039878845\n",
      "Epoch 19198/30000 Training Loss: 0.05627410486340523\n",
      "Epoch 19199/30000 Training Loss: 0.05087200552225113\n",
      "Epoch 19200/30000 Training Loss: 0.039551813155412674\n",
      "Epoch 19200/30000 Validation Loss: 0.06557728350162506\n",
      "Epoch 19201/30000 Training Loss: 0.03608646243810654\n",
      "Epoch 19202/30000 Training Loss: 0.04380570352077484\n",
      "Epoch 19203/30000 Training Loss: 0.039331406354904175\n",
      "Epoch 19204/30000 Training Loss: 0.055531397461891174\n",
      "Epoch 19205/30000 Training Loss: 0.04736011102795601\n",
      "Epoch 19206/30000 Training Loss: 0.05704483389854431\n",
      "Epoch 19207/30000 Training Loss: 0.05033569037914276\n",
      "Epoch 19208/30000 Training Loss: 0.053322985768318176\n",
      "Epoch 19209/30000 Training Loss: 0.05018386244773865\n",
      "Epoch 19210/30000 Training Loss: 0.046843282878398895\n",
      "Epoch 19211/30000 Training Loss: 0.05843426287174225\n",
      "Epoch 19212/30000 Training Loss: 0.04599420353770256\n",
      "Epoch 19213/30000 Training Loss: 0.03864036127924919\n",
      "Epoch 19214/30000 Training Loss: 0.036036789417266846\n",
      "Epoch 19215/30000 Training Loss: 0.04558451473712921\n",
      "Epoch 19216/30000 Training Loss: 0.049379121512174606\n",
      "Epoch 19217/30000 Training Loss: 0.05790884047746658\n",
      "Epoch 19218/30000 Training Loss: 0.03344525769352913\n",
      "Epoch 19219/30000 Training Loss: 0.031193673610687256\n",
      "Epoch 19220/30000 Training Loss: 0.04392022639513016\n",
      "Epoch 19221/30000 Training Loss: 0.0439566895365715\n",
      "Epoch 19222/30000 Training Loss: 0.03209685534238815\n",
      "Epoch 19223/30000 Training Loss: 0.051460955291986465\n",
      "Epoch 19224/30000 Training Loss: 0.05949660763144493\n",
      "Epoch 19225/30000 Training Loss: 0.04498536139726639\n",
      "Epoch 19226/30000 Training Loss: 0.04734243080019951\n",
      "Epoch 19227/30000 Training Loss: 0.046647731214761734\n",
      "Epoch 19228/30000 Training Loss: 0.04040565714240074\n",
      "Epoch 19229/30000 Training Loss: 0.043387725949287415\n",
      "Epoch 19230/30000 Training Loss: 0.047186218202114105\n",
      "Epoch 19231/30000 Training Loss: 0.044891200959682465\n",
      "Epoch 19232/30000 Training Loss: 0.04399874433875084\n",
      "Epoch 19233/30000 Training Loss: 0.05381817743182182\n",
      "Epoch 19234/30000 Training Loss: 0.032291900366544724\n",
      "Epoch 19235/30000 Training Loss: 0.04328259825706482\n",
      "Epoch 19236/30000 Training Loss: 0.04954042658209801\n",
      "Epoch 19237/30000 Training Loss: 0.05931314080953598\n",
      "Epoch 19238/30000 Training Loss: 0.04778284579515457\n",
      "Epoch 19239/30000 Training Loss: 0.03776748478412628\n",
      "Epoch 19240/30000 Training Loss: 0.05325843766331673\n",
      "Epoch 19241/30000 Training Loss: 0.05186935514211655\n",
      "Epoch 19242/30000 Training Loss: 0.044962603598833084\n",
      "Epoch 19243/30000 Training Loss: 0.03493049368262291\n",
      "Epoch 19244/30000 Training Loss: 0.04470941051840782\n",
      "Epoch 19245/30000 Training Loss: 0.04793323576450348\n",
      "Epoch 19246/30000 Training Loss: 0.047158192843198776\n",
      "Epoch 19247/30000 Training Loss: 0.04872000962495804\n",
      "Epoch 19248/30000 Training Loss: 0.04587758332490921\n",
      "Epoch 19249/30000 Training Loss: 0.0390096977353096\n",
      "Epoch 19250/30000 Training Loss: 0.0351773239672184\n",
      "Epoch 19251/30000 Training Loss: 0.04809390753507614\n",
      "Epoch 19252/30000 Training Loss: 0.046427320688962936\n",
      "Epoch 19253/30000 Training Loss: 0.040743324905633926\n",
      "Epoch 19254/30000 Training Loss: 0.0507623627781868\n",
      "Epoch 19255/30000 Training Loss: 0.05376270413398743\n",
      "Epoch 19256/30000 Training Loss: 0.04013529792428017\n",
      "Epoch 19257/30000 Training Loss: 0.04102252051234245\n",
      "Epoch 19258/30000 Training Loss: 0.04498584568500519\n",
      "Epoch 19259/30000 Training Loss: 0.05783179774880409\n",
      "Epoch 19260/30000 Training Loss: 0.04749546945095062\n",
      "Epoch 19261/30000 Training Loss: 0.04820426553487778\n",
      "Epoch 19262/30000 Training Loss: 0.050239384174346924\n",
      "Epoch 19263/30000 Training Loss: 0.04691833630204201\n",
      "Epoch 19264/30000 Training Loss: 0.04357149079442024\n",
      "Epoch 19265/30000 Training Loss: 0.0438544899225235\n",
      "Epoch 19266/30000 Training Loss: 0.04798833653330803\n",
      "Epoch 19267/30000 Training Loss: 0.04115411639213562\n",
      "Epoch 19268/30000 Training Loss: 0.04714370518922806\n",
      "Epoch 19269/30000 Training Loss: 0.06325890868902206\n",
      "Epoch 19270/30000 Training Loss: 0.04232762008905411\n",
      "Epoch 19271/30000 Training Loss: 0.039547279477119446\n",
      "Epoch 19272/30000 Training Loss: 0.05128614604473114\n",
      "Epoch 19273/30000 Training Loss: 0.067078597843647\n",
      "Epoch 19274/30000 Training Loss: 0.050689056515693665\n",
      "Epoch 19275/30000 Training Loss: 0.04926776513457298\n",
      "Epoch 19276/30000 Training Loss: 0.04039618745446205\n",
      "Epoch 19277/30000 Training Loss: 0.050362952053546906\n",
      "Epoch 19278/30000 Training Loss: 0.04910486936569214\n",
      "Epoch 19279/30000 Training Loss: 0.042035527527332306\n",
      "Epoch 19280/30000 Training Loss: 0.04026530310511589\n",
      "Epoch 19281/30000 Training Loss: 0.0428459569811821\n",
      "Epoch 19282/30000 Training Loss: 0.06621722877025604\n",
      "Epoch 19283/30000 Training Loss: 0.038057856261730194\n",
      "Epoch 19284/30000 Training Loss: 0.06022251769900322\n",
      "Epoch 19285/30000 Training Loss: 0.056828174740076065\n",
      "Epoch 19286/30000 Training Loss: 0.03304934874176979\n",
      "Epoch 19287/30000 Training Loss: 0.0568116195499897\n",
      "Epoch 19288/30000 Training Loss: 0.05705944821238518\n",
      "Epoch 19289/30000 Training Loss: 0.04235859960317612\n",
      "Epoch 19290/30000 Training Loss: 0.0767233669757843\n",
      "Epoch 19291/30000 Training Loss: 0.056864283978939056\n",
      "Epoch 19292/30000 Training Loss: 0.030850199982523918\n",
      "Epoch 19293/30000 Training Loss: 0.037578288465738297\n",
      "Epoch 19294/30000 Training Loss: 0.05096772313117981\n",
      "Epoch 19295/30000 Training Loss: 0.0639503225684166\n",
      "Epoch 19296/30000 Training Loss: 0.04539663344621658\n",
      "Epoch 19297/30000 Training Loss: 0.044773511588573456\n",
      "Epoch 19298/30000 Training Loss: 0.06123948097229004\n",
      "Epoch 19299/30000 Training Loss: 0.06942163407802582\n",
      "Epoch 19300/30000 Training Loss: 0.04441709816455841\n",
      "Epoch 19300/30000 Validation Loss: 0.050566405057907104\n",
      "Epoch 19301/30000 Training Loss: 0.04948952794075012\n",
      "Epoch 19302/30000 Training Loss: 0.048663362860679626\n",
      "Epoch 19303/30000 Training Loss: 0.0693080797791481\n",
      "Epoch 19304/30000 Training Loss: 0.042266279458999634\n",
      "Epoch 19305/30000 Training Loss: 0.059390608221292496\n",
      "Epoch 19306/30000 Training Loss: 0.036753058433532715\n",
      "Epoch 19307/30000 Training Loss: 0.04115528613328934\n",
      "Epoch 19308/30000 Training Loss: 0.047814518213272095\n",
      "Epoch 19309/30000 Training Loss: 0.04589960351586342\n",
      "Epoch 19310/30000 Training Loss: 0.05069879814982414\n",
      "Epoch 19311/30000 Training Loss: 0.035517819225788116\n",
      "Epoch 19312/30000 Training Loss: 0.039922408759593964\n",
      "Epoch 19313/30000 Training Loss: 0.06296767294406891\n",
      "Epoch 19314/30000 Training Loss: 0.03734277933835983\n",
      "Epoch 19315/30000 Training Loss: 0.03399916738271713\n",
      "Epoch 19316/30000 Training Loss: 0.059285275638103485\n",
      "Epoch 19317/30000 Training Loss: 0.04134368523955345\n",
      "Epoch 19318/30000 Training Loss: 0.04163522273302078\n",
      "Epoch 19319/30000 Training Loss: 0.04697524383664131\n",
      "Epoch 19320/30000 Training Loss: 0.061280421912670135\n",
      "Epoch 19321/30000 Training Loss: 0.044421322643756866\n",
      "Epoch 19322/30000 Training Loss: 0.038566842675209045\n",
      "Epoch 19323/30000 Training Loss: 0.061969079077243805\n",
      "Epoch 19324/30000 Training Loss: 0.05965033546090126\n",
      "Epoch 19325/30000 Training Loss: 0.039611052721738815\n",
      "Epoch 19326/30000 Training Loss: 0.04759199917316437\n",
      "Epoch 19327/30000 Training Loss: 0.05480925738811493\n",
      "Epoch 19328/30000 Training Loss: 0.033836252987384796\n",
      "Epoch 19329/30000 Training Loss: 0.04160865396261215\n",
      "Epoch 19330/30000 Training Loss: 0.04888713359832764\n",
      "Epoch 19331/30000 Training Loss: 0.07438955456018448\n",
      "Epoch 19332/30000 Training Loss: 0.037849098443984985\n",
      "Epoch 19333/30000 Training Loss: 0.04265647754073143\n",
      "Epoch 19334/30000 Training Loss: 0.06438533961772919\n",
      "Epoch 19335/30000 Training Loss: 0.05002178996801376\n",
      "Epoch 19336/30000 Training Loss: 0.054105471819639206\n",
      "Epoch 19337/30000 Training Loss: 0.04570682719349861\n",
      "Epoch 19338/30000 Training Loss: 0.05306244641542435\n",
      "Epoch 19339/30000 Training Loss: 0.04866582900285721\n",
      "Epoch 19340/30000 Training Loss: 0.0546329990029335\n",
      "Epoch 19341/30000 Training Loss: 0.054602496325969696\n",
      "Epoch 19342/30000 Training Loss: 0.039381906390190125\n",
      "Epoch 19343/30000 Training Loss: 0.035434331744909286\n",
      "Epoch 19344/30000 Training Loss: 0.04486525058746338\n",
      "Epoch 19345/30000 Training Loss: 0.03817937523126602\n",
      "Epoch 19346/30000 Training Loss: 0.0598815381526947\n",
      "Epoch 19347/30000 Training Loss: 0.05682659521698952\n",
      "Epoch 19348/30000 Training Loss: 0.05845873802900314\n",
      "Epoch 19349/30000 Training Loss: 0.0488961860537529\n",
      "Epoch 19350/30000 Training Loss: 0.05913441255688667\n",
      "Epoch 19351/30000 Training Loss: 0.05690290406346321\n",
      "Epoch 19352/30000 Training Loss: 0.04897616058588028\n",
      "Epoch 19353/30000 Training Loss: 0.04837799444794655\n",
      "Epoch 19354/30000 Training Loss: 0.05417891591787338\n",
      "Epoch 19355/30000 Training Loss: 0.05230775475502014\n",
      "Epoch 19356/30000 Training Loss: 0.04630781337618828\n",
      "Epoch 19357/30000 Training Loss: 0.0660976693034172\n",
      "Epoch 19358/30000 Training Loss: 0.044253360480070114\n",
      "Epoch 19359/30000 Training Loss: 0.05805833637714386\n",
      "Epoch 19360/30000 Training Loss: 0.06389778852462769\n",
      "Epoch 19361/30000 Training Loss: 0.03338170796632767\n",
      "Epoch 19362/30000 Training Loss: 0.06327246874570847\n",
      "Epoch 19363/30000 Training Loss: 0.04920664429664612\n",
      "Epoch 19364/30000 Training Loss: 0.04889466613531113\n",
      "Epoch 19365/30000 Training Loss: 0.045506276190280914\n",
      "Epoch 19366/30000 Training Loss: 0.04131653159856796\n",
      "Epoch 19367/30000 Training Loss: 0.033336758613586426\n",
      "Epoch 19368/30000 Training Loss: 0.03732273727655411\n",
      "Epoch 19369/30000 Training Loss: 0.0627574548125267\n",
      "Epoch 19370/30000 Training Loss: 0.06815139949321747\n",
      "Epoch 19371/30000 Training Loss: 0.049340199679136276\n",
      "Epoch 19372/30000 Training Loss: 0.06328132748603821\n",
      "Epoch 19373/30000 Training Loss: 0.05162597447633743\n",
      "Epoch 19374/30000 Training Loss: 0.04532409459352493\n",
      "Epoch 19375/30000 Training Loss: 0.036715008318424225\n",
      "Epoch 19376/30000 Training Loss: 0.05341298133134842\n",
      "Epoch 19377/30000 Training Loss: 0.055423736572265625\n",
      "Epoch 19378/30000 Training Loss: 0.044921472668647766\n",
      "Epoch 19379/30000 Training Loss: 0.054181721061468124\n",
      "Epoch 19380/30000 Training Loss: 0.03951791673898697\n",
      "Epoch 19381/30000 Training Loss: 0.04204707220196724\n",
      "Epoch 19382/30000 Training Loss: 0.050655387341976166\n",
      "Epoch 19383/30000 Training Loss: 0.042705826461315155\n",
      "Epoch 19384/30000 Training Loss: 0.060085002332925797\n",
      "Epoch 19385/30000 Training Loss: 0.056510064750909805\n",
      "Epoch 19386/30000 Training Loss: 0.04223047196865082\n",
      "Epoch 19387/30000 Training Loss: 0.0404900461435318\n",
      "Epoch 19388/30000 Training Loss: 0.03584149107336998\n",
      "Epoch 19389/30000 Training Loss: 0.05240213871002197\n",
      "Epoch 19390/30000 Training Loss: 0.03635489568114281\n",
      "Epoch 19391/30000 Training Loss: 0.0539495050907135\n",
      "Epoch 19392/30000 Training Loss: 0.03795415163040161\n",
      "Epoch 19393/30000 Training Loss: 0.04789133369922638\n",
      "Epoch 19394/30000 Training Loss: 0.03661957010626793\n",
      "Epoch 19395/30000 Training Loss: 0.03852751851081848\n",
      "Epoch 19396/30000 Training Loss: 0.05252421647310257\n",
      "Epoch 19397/30000 Training Loss: 0.05451790615916252\n",
      "Epoch 19398/30000 Training Loss: 0.04791950434446335\n",
      "Epoch 19399/30000 Training Loss: 0.04290693998336792\n",
      "Epoch 19400/30000 Training Loss: 0.05447481572628021\n",
      "Epoch 19400/30000 Validation Loss: 0.054219283163547516\n",
      "Epoch 19401/30000 Training Loss: 0.029851535335183144\n",
      "Epoch 19402/30000 Training Loss: 0.03617095574736595\n",
      "Epoch 19403/30000 Training Loss: 0.04731146618723869\n",
      "Epoch 19404/30000 Training Loss: 0.04881809279322624\n",
      "Epoch 19405/30000 Training Loss: 0.047596290707588196\n",
      "Epoch 19406/30000 Training Loss: 0.041233547031879425\n",
      "Epoch 19407/30000 Training Loss: 0.04046328365802765\n",
      "Epoch 19408/30000 Training Loss: 0.05899472162127495\n",
      "Epoch 19409/30000 Training Loss: 0.048152562230825424\n",
      "Epoch 19410/30000 Training Loss: 0.04501841217279434\n",
      "Epoch 19411/30000 Training Loss: 0.05201705917716026\n",
      "Epoch 19412/30000 Training Loss: 0.04286406934261322\n",
      "Epoch 19413/30000 Training Loss: 0.059756726026535034\n",
      "Epoch 19414/30000 Training Loss: 0.06204667687416077\n",
      "Epoch 19415/30000 Training Loss: 0.04112856462597847\n",
      "Epoch 19416/30000 Training Loss: 0.04643692821264267\n",
      "Epoch 19417/30000 Training Loss: 0.0562715157866478\n",
      "Epoch 19418/30000 Training Loss: 0.050816453993320465\n",
      "Epoch 19419/30000 Training Loss: 0.05031605437397957\n",
      "Epoch 19420/30000 Training Loss: 0.039022937417030334\n",
      "Epoch 19421/30000 Training Loss: 0.04779733717441559\n",
      "Epoch 19422/30000 Training Loss: 0.043323010206222534\n",
      "Epoch 19423/30000 Training Loss: 0.04242363199591637\n",
      "Epoch 19424/30000 Training Loss: 0.04333797097206116\n",
      "Epoch 19425/30000 Training Loss: 0.04749030992388725\n",
      "Epoch 19426/30000 Training Loss: 0.04743243753910065\n",
      "Epoch 19427/30000 Training Loss: 0.039503373205661774\n",
      "Epoch 19428/30000 Training Loss: 0.05377747863531113\n",
      "Epoch 19429/30000 Training Loss: 0.06269397586584091\n",
      "Epoch 19430/30000 Training Loss: 0.04747520014643669\n",
      "Epoch 19431/30000 Training Loss: 0.036671120673418045\n",
      "Epoch 19432/30000 Training Loss: 0.04917449876666069\n",
      "Epoch 19433/30000 Training Loss: 0.04850838705897331\n",
      "Epoch 19434/30000 Training Loss: 0.0412166491150856\n",
      "Epoch 19435/30000 Training Loss: 0.03762320801615715\n",
      "Epoch 19436/30000 Training Loss: 0.03461139649152756\n",
      "Epoch 19437/30000 Training Loss: 0.043994709849357605\n",
      "Epoch 19438/30000 Training Loss: 0.04298112541437149\n",
      "Epoch 19439/30000 Training Loss: 0.04886140674352646\n",
      "Epoch 19440/30000 Training Loss: 0.0580177903175354\n",
      "Epoch 19441/30000 Training Loss: 0.043888479471206665\n",
      "Epoch 19442/30000 Training Loss: 0.050247520208358765\n",
      "Epoch 19443/30000 Training Loss: 0.060546763241291046\n",
      "Epoch 19444/30000 Training Loss: 0.053671207278966904\n",
      "Epoch 19445/30000 Training Loss: 0.04399406909942627\n",
      "Epoch 19446/30000 Training Loss: 0.051598042249679565\n",
      "Epoch 19447/30000 Training Loss: 0.047507524490356445\n",
      "Epoch 19448/30000 Training Loss: 0.05106416717171669\n",
      "Epoch 19449/30000 Training Loss: 0.04698536545038223\n",
      "Epoch 19450/30000 Training Loss: 0.03920223191380501\n",
      "Epoch 19451/30000 Training Loss: 0.03822345659136772\n",
      "Epoch 19452/30000 Training Loss: 0.05117000639438629\n",
      "Epoch 19453/30000 Training Loss: 0.039387449622154236\n",
      "Epoch 19454/30000 Training Loss: 0.04704985022544861\n",
      "Epoch 19455/30000 Training Loss: 0.07057657837867737\n",
      "Epoch 19456/30000 Training Loss: 0.04864808917045593\n",
      "Epoch 19457/30000 Training Loss: 0.041700854897499084\n",
      "Epoch 19458/30000 Training Loss: 0.03861825168132782\n",
      "Epoch 19459/30000 Training Loss: 0.050706665962934494\n",
      "Epoch 19460/30000 Training Loss: 0.0528687909245491\n",
      "Epoch 19461/30000 Training Loss: 0.04505245387554169\n",
      "Epoch 19462/30000 Training Loss: 0.08127035200595856\n",
      "Epoch 19463/30000 Training Loss: 0.04948761686682701\n",
      "Epoch 19464/30000 Training Loss: 0.029159827157855034\n",
      "Epoch 19465/30000 Training Loss: 0.04369865357875824\n",
      "Epoch 19466/30000 Training Loss: 0.051058411598205566\n",
      "Epoch 19467/30000 Training Loss: 0.04201206937432289\n",
      "Epoch 19468/30000 Training Loss: 0.0657566636800766\n",
      "Epoch 19469/30000 Training Loss: 0.03905179351568222\n",
      "Epoch 19470/30000 Training Loss: 0.051847413182258606\n",
      "Epoch 19471/30000 Training Loss: 0.04357210546731949\n",
      "Epoch 19472/30000 Training Loss: 0.04271978139877319\n",
      "Epoch 19473/30000 Training Loss: 0.04866618663072586\n",
      "Epoch 19474/30000 Training Loss: 0.06137806922197342\n",
      "Epoch 19475/30000 Training Loss: 0.056143563240766525\n",
      "Epoch 19476/30000 Training Loss: 0.03784364089369774\n",
      "Epoch 19477/30000 Training Loss: 0.05235235393047333\n",
      "Epoch 19478/30000 Training Loss: 0.041550010442733765\n",
      "Epoch 19479/30000 Training Loss: 0.03918124735355377\n",
      "Epoch 19480/30000 Training Loss: 0.0477759912610054\n",
      "Epoch 19481/30000 Training Loss: 0.052934955805540085\n",
      "Epoch 19482/30000 Training Loss: 0.058676525950431824\n",
      "Epoch 19483/30000 Training Loss: 0.039955999702215195\n",
      "Epoch 19484/30000 Training Loss: 0.03730671480298042\n",
      "Epoch 19485/30000 Training Loss: 0.043858617544174194\n",
      "Epoch 19486/30000 Training Loss: 0.05508781969547272\n",
      "Epoch 19487/30000 Training Loss: 0.06775879114866257\n",
      "Epoch 19488/30000 Training Loss: 0.03944139927625656\n",
      "Epoch 19489/30000 Training Loss: 0.04216814786195755\n",
      "Epoch 19490/30000 Training Loss: 0.04112580418586731\n",
      "Epoch 19491/30000 Training Loss: 0.03971538320183754\n",
      "Epoch 19492/30000 Training Loss: 0.04847520589828491\n",
      "Epoch 19493/30000 Training Loss: 0.05215650051832199\n",
      "Epoch 19494/30000 Training Loss: 0.06473194062709808\n",
      "Epoch 19495/30000 Training Loss: 0.03691558912396431\n",
      "Epoch 19496/30000 Training Loss: 0.046867966651916504\n",
      "Epoch 19497/30000 Training Loss: 0.06487451493740082\n",
      "Epoch 19498/30000 Training Loss: 0.035937707871198654\n",
      "Epoch 19499/30000 Training Loss: 0.0510653555393219\n",
      "Epoch 19500/30000 Training Loss: 0.03126438334584236\n",
      "Epoch 19500/30000 Validation Loss: 0.04922648146748543\n",
      "Epoch 19501/30000 Training Loss: 0.04468921944499016\n",
      "Epoch 19502/30000 Training Loss: 0.04936087876558304\n",
      "Epoch 19503/30000 Training Loss: 0.0455646887421608\n",
      "Epoch 19504/30000 Training Loss: 0.04499821364879608\n",
      "Epoch 19505/30000 Training Loss: 0.04035016894340515\n",
      "Epoch 19506/30000 Training Loss: 0.037477754056453705\n",
      "Epoch 19507/30000 Training Loss: 0.04412811994552612\n",
      "Epoch 19508/30000 Training Loss: 0.04384051635861397\n",
      "Epoch 19509/30000 Training Loss: 0.05741771310567856\n",
      "Epoch 19510/30000 Training Loss: 0.047642432153224945\n",
      "Epoch 19511/30000 Training Loss: 0.030575193464756012\n",
      "Epoch 19512/30000 Training Loss: 0.053283706307411194\n",
      "Epoch 19513/30000 Training Loss: 0.036220695823431015\n",
      "Epoch 19514/30000 Training Loss: 0.056752704083919525\n",
      "Epoch 19515/30000 Training Loss: 0.0339810736477375\n",
      "Epoch 19516/30000 Training Loss: 0.06690478324890137\n",
      "Epoch 19517/30000 Training Loss: 0.05737867206335068\n",
      "Epoch 19518/30000 Training Loss: 0.04270106181502342\n",
      "Epoch 19519/30000 Training Loss: 0.0319429412484169\n",
      "Epoch 19520/30000 Training Loss: 0.04848801717162132\n",
      "Epoch 19521/30000 Training Loss: 0.05646604672074318\n",
      "Epoch 19522/30000 Training Loss: 0.04690660536289215\n",
      "Epoch 19523/30000 Training Loss: 0.04577196016907692\n",
      "Epoch 19524/30000 Training Loss: 0.05144631117582321\n",
      "Epoch 19525/30000 Training Loss: 0.04952523857355118\n",
      "Epoch 19526/30000 Training Loss: 0.04345894232392311\n",
      "Epoch 19527/30000 Training Loss: 0.04645220935344696\n",
      "Epoch 19528/30000 Training Loss: 0.055902641266584396\n",
      "Epoch 19529/30000 Training Loss: 0.033843353390693665\n",
      "Epoch 19530/30000 Training Loss: 0.053295910358428955\n",
      "Epoch 19531/30000 Training Loss: 0.05561881512403488\n",
      "Epoch 19532/30000 Training Loss: 0.04884170740842819\n",
      "Epoch 19533/30000 Training Loss: 0.05186165124177933\n",
      "Epoch 19534/30000 Training Loss: 0.040646299719810486\n",
      "Epoch 19535/30000 Training Loss: 0.040224287658929825\n",
      "Epoch 19536/30000 Training Loss: 0.05217811465263367\n",
      "Epoch 19537/30000 Training Loss: 0.04378995671868324\n",
      "Epoch 19538/30000 Training Loss: 0.034715522080659866\n",
      "Epoch 19539/30000 Training Loss: 0.03687901794910431\n",
      "Epoch 19540/30000 Training Loss: 0.044790107756853104\n",
      "Epoch 19541/30000 Training Loss: 0.04840582609176636\n",
      "Epoch 19542/30000 Training Loss: 0.06665480136871338\n",
      "Epoch 19543/30000 Training Loss: 0.048650581389665604\n",
      "Epoch 19544/30000 Training Loss: 0.03694172203540802\n",
      "Epoch 19545/30000 Training Loss: 0.043054454028606415\n",
      "Epoch 19546/30000 Training Loss: 0.028570545837283134\n",
      "Epoch 19547/30000 Training Loss: 0.044552117586135864\n",
      "Epoch 19548/30000 Training Loss: 0.051057975739240646\n",
      "Epoch 19549/30000 Training Loss: 0.0411868579685688\n",
      "Epoch 19550/30000 Training Loss: 0.040528010576963425\n",
      "Epoch 19551/30000 Training Loss: 0.07621097564697266\n",
      "Epoch 19552/30000 Training Loss: 0.053291045129299164\n",
      "Epoch 19553/30000 Training Loss: 0.05319749191403389\n",
      "Epoch 19554/30000 Training Loss: 0.03681999444961548\n",
      "Epoch 19555/30000 Training Loss: 0.05019817501306534\n",
      "Epoch 19556/30000 Training Loss: 0.06001374498009682\n",
      "Epoch 19557/30000 Training Loss: 0.04009614139795303\n",
      "Epoch 19558/30000 Training Loss: 0.0578567199409008\n",
      "Epoch 19559/30000 Training Loss: 0.031306054443120956\n",
      "Epoch 19560/30000 Training Loss: 0.049010928720235825\n",
      "Epoch 19561/30000 Training Loss: 0.05042222887277603\n",
      "Epoch 19562/30000 Training Loss: 0.043291524052619934\n",
      "Epoch 19563/30000 Training Loss: 0.04665618762373924\n",
      "Epoch 19564/30000 Training Loss: 0.039054494351148605\n",
      "Epoch 19565/30000 Training Loss: 0.038971081376075745\n",
      "Epoch 19566/30000 Training Loss: 0.042621150612831116\n",
      "Epoch 19567/30000 Training Loss: 0.03929542377591133\n",
      "Epoch 19568/30000 Training Loss: 0.06230200082063675\n",
      "Epoch 19569/30000 Training Loss: 0.05735829472541809\n",
      "Epoch 19570/30000 Training Loss: 0.05484737455844879\n",
      "Epoch 19571/30000 Training Loss: 0.054678041487932205\n",
      "Epoch 19572/30000 Training Loss: 0.04749142378568649\n",
      "Epoch 19573/30000 Training Loss: 0.04415034130215645\n",
      "Epoch 19574/30000 Training Loss: 0.03795571252703667\n",
      "Epoch 19575/30000 Training Loss: 0.04788670688867569\n",
      "Epoch 19576/30000 Training Loss: 0.047841817140579224\n",
      "Epoch 19577/30000 Training Loss: 0.05736295133829117\n",
      "Epoch 19578/30000 Training Loss: 0.04402269423007965\n",
      "Epoch 19579/30000 Training Loss: 0.044546909630298615\n",
      "Epoch 19580/30000 Training Loss: 0.04716797173023224\n",
      "Epoch 19581/30000 Training Loss: 0.045397814363241196\n",
      "Epoch 19582/30000 Training Loss: 0.04951139912009239\n",
      "Epoch 19583/30000 Training Loss: 0.04233560711145401\n",
      "Epoch 19584/30000 Training Loss: 0.041107114404439926\n",
      "Epoch 19585/30000 Training Loss: 0.04837752878665924\n",
      "Epoch 19586/30000 Training Loss: 0.050411656498909\n",
      "Epoch 19587/30000 Training Loss: 0.03328464925289154\n",
      "Epoch 19588/30000 Training Loss: 0.06059204041957855\n",
      "Epoch 19589/30000 Training Loss: 0.042652737349271774\n",
      "Epoch 19590/30000 Training Loss: 0.055137164890766144\n",
      "Epoch 19591/30000 Training Loss: 0.039773210883140564\n",
      "Epoch 19592/30000 Training Loss: 0.044186390936374664\n",
      "Epoch 19593/30000 Training Loss: 0.029659319669008255\n",
      "Epoch 19594/30000 Training Loss: 0.05470909923315048\n",
      "Epoch 19595/30000 Training Loss: 0.037136875092983246\n",
      "Epoch 19596/30000 Training Loss: 0.0535733625292778\n",
      "Epoch 19597/30000 Training Loss: 0.051172226667404175\n",
      "Epoch 19598/30000 Training Loss: 0.047150325030088425\n",
      "Epoch 19599/30000 Training Loss: 0.04755218327045441\n",
      "Epoch 19600/30000 Training Loss: 0.04568492993712425\n",
      "Epoch 19600/30000 Validation Loss: 0.0395994633436203\n",
      "Epoch 19601/30000 Training Loss: 0.0401177778840065\n",
      "Epoch 19602/30000 Training Loss: 0.06219789385795593\n",
      "Epoch 19603/30000 Training Loss: 0.045110851526260376\n",
      "Epoch 19604/30000 Training Loss: 0.05549752712249756\n",
      "Epoch 19605/30000 Training Loss: 0.04048328474164009\n",
      "Epoch 19606/30000 Training Loss: 0.03916431963443756\n",
      "Epoch 19607/30000 Training Loss: 0.0586123913526535\n",
      "Epoch 19608/30000 Training Loss: 0.05416523292660713\n",
      "Epoch 19609/30000 Training Loss: 0.04752402752637863\n",
      "Epoch 19610/30000 Training Loss: 0.041347477585077286\n",
      "Epoch 19611/30000 Training Loss: 0.0663355141878128\n",
      "Epoch 19612/30000 Training Loss: 0.040810901671648026\n",
      "Epoch 19613/30000 Training Loss: 0.06988921761512756\n",
      "Epoch 19614/30000 Training Loss: 0.05689506232738495\n",
      "Epoch 19615/30000 Training Loss: 0.06485437601804733\n",
      "Epoch 19616/30000 Training Loss: 0.05208677798509598\n",
      "Epoch 19617/30000 Training Loss: 0.05989766865968704\n",
      "Epoch 19618/30000 Training Loss: 0.056602030992507935\n",
      "Epoch 19619/30000 Training Loss: 0.031093014404177666\n",
      "Epoch 19620/30000 Training Loss: 0.06377319246530533\n",
      "Epoch 19621/30000 Training Loss: 0.038560397922992706\n",
      "Epoch 19622/30000 Training Loss: 0.05978546291589737\n",
      "Epoch 19623/30000 Training Loss: 0.03678819164633751\n",
      "Epoch 19624/30000 Training Loss: 0.06736632436513901\n",
      "Epoch 19625/30000 Training Loss: 0.04251594841480255\n",
      "Epoch 19626/30000 Training Loss: 0.04199041426181793\n",
      "Epoch 19627/30000 Training Loss: 0.062275469303131104\n",
      "Epoch 19628/30000 Training Loss: 0.06345226615667343\n",
      "Epoch 19629/30000 Training Loss: 0.05112852901220322\n",
      "Epoch 19630/30000 Training Loss: 0.04601629450917244\n",
      "Epoch 19631/30000 Training Loss: 0.058244358748197556\n",
      "Epoch 19632/30000 Training Loss: 0.04770432785153389\n",
      "Epoch 19633/30000 Training Loss: 0.04347439110279083\n",
      "Epoch 19634/30000 Training Loss: 0.061244815587997437\n",
      "Epoch 19635/30000 Training Loss: 0.047809820622205734\n",
      "Epoch 19636/30000 Training Loss: 0.05970265716314316\n",
      "Epoch 19637/30000 Training Loss: 0.05538918823003769\n",
      "Epoch 19638/30000 Training Loss: 0.05101364850997925\n",
      "Epoch 19639/30000 Training Loss: 0.04607229307293892\n",
      "Epoch 19640/30000 Training Loss: 0.04898383095860481\n",
      "Epoch 19641/30000 Training Loss: 0.04722322151064873\n",
      "Epoch 19642/30000 Training Loss: 0.05636034905910492\n",
      "Epoch 19643/30000 Training Loss: 0.04291275516152382\n",
      "Epoch 19644/30000 Training Loss: 0.04403304308652878\n",
      "Epoch 19645/30000 Training Loss: 0.04462787136435509\n",
      "Epoch 19646/30000 Training Loss: 0.04332314431667328\n",
      "Epoch 19647/30000 Training Loss: 0.04401252791285515\n",
      "Epoch 19648/30000 Training Loss: 0.03739376366138458\n",
      "Epoch 19649/30000 Training Loss: 0.04790523648262024\n",
      "Epoch 19650/30000 Training Loss: 0.043710507452487946\n",
      "Epoch 19651/30000 Training Loss: 0.04947730898857117\n",
      "Epoch 19652/30000 Training Loss: 0.0647546574473381\n",
      "Epoch 19653/30000 Training Loss: 0.0440482571721077\n",
      "Epoch 19654/30000 Training Loss: 0.05786236375570297\n",
      "Epoch 19655/30000 Training Loss: 0.0554204098880291\n",
      "Epoch 19656/30000 Training Loss: 0.040314242243766785\n",
      "Epoch 19657/30000 Training Loss: 0.04661950469017029\n",
      "Epoch 19658/30000 Training Loss: 0.035575803369283676\n",
      "Epoch 19659/30000 Training Loss: 0.04096047580242157\n",
      "Epoch 19660/30000 Training Loss: 0.04575229063630104\n",
      "Epoch 19661/30000 Training Loss: 0.06513448059558868\n",
      "Epoch 19662/30000 Training Loss: 0.04529953375458717\n",
      "Epoch 19663/30000 Training Loss: 0.04157107323408127\n",
      "Epoch 19664/30000 Training Loss: 0.04894523695111275\n",
      "Epoch 19665/30000 Training Loss: 0.05614355206489563\n",
      "Epoch 19666/30000 Training Loss: 0.04397360980510712\n",
      "Epoch 19667/30000 Training Loss: 0.048260800540447235\n",
      "Epoch 19668/30000 Training Loss: 0.055901288986206055\n",
      "Epoch 19669/30000 Training Loss: 0.0632808655500412\n",
      "Epoch 19670/30000 Training Loss: 0.04442731291055679\n",
      "Epoch 19671/30000 Training Loss: 0.03341318666934967\n",
      "Epoch 19672/30000 Training Loss: 0.046248458325862885\n",
      "Epoch 19673/30000 Training Loss: 0.04768598824739456\n",
      "Epoch 19674/30000 Training Loss: 0.046252865344285965\n",
      "Epoch 19675/30000 Training Loss: 0.05328032746911049\n",
      "Epoch 19676/30000 Training Loss: 0.05439949780702591\n",
      "Epoch 19677/30000 Training Loss: 0.04478450492024422\n",
      "Epoch 19678/30000 Training Loss: 0.0471627414226532\n",
      "Epoch 19679/30000 Training Loss: 0.04096316546201706\n",
      "Epoch 19680/30000 Training Loss: 0.04783468693494797\n",
      "Epoch 19681/30000 Training Loss: 0.06359461694955826\n",
      "Epoch 19682/30000 Training Loss: 0.04905368387699127\n",
      "Epoch 19683/30000 Training Loss: 0.039601512253284454\n",
      "Epoch 19684/30000 Training Loss: 0.04203275963664055\n",
      "Epoch 19685/30000 Training Loss: 0.05624130368232727\n",
      "Epoch 19686/30000 Training Loss: 0.0636577382683754\n",
      "Epoch 19687/30000 Training Loss: 0.047216352075338364\n",
      "Epoch 19688/30000 Training Loss: 0.029985912144184113\n",
      "Epoch 19689/30000 Training Loss: 0.04496243596076965\n",
      "Epoch 19690/30000 Training Loss: 0.05071810632944107\n",
      "Epoch 19691/30000 Training Loss: 0.039359986782073975\n",
      "Epoch 19692/30000 Training Loss: 0.045427948236465454\n",
      "Epoch 19693/30000 Training Loss: 0.06241370737552643\n",
      "Epoch 19694/30000 Training Loss: 0.05210505425930023\n",
      "Epoch 19695/30000 Training Loss: 0.04866381362080574\n",
      "Epoch 19696/30000 Training Loss: 0.05449824780225754\n",
      "Epoch 19697/30000 Training Loss: 0.03407112509012222\n",
      "Epoch 19698/30000 Training Loss: 0.04374516010284424\n",
      "Epoch 19699/30000 Training Loss: 0.04754568636417389\n",
      "Epoch 19700/30000 Training Loss: 0.05333078280091286\n",
      "Epoch 19700/30000 Validation Loss: 0.04626486077904701\n",
      "Epoch 19701/30000 Training Loss: 0.046363282948732376\n",
      "Epoch 19702/30000 Training Loss: 0.04184497520327568\n",
      "Epoch 19703/30000 Training Loss: 0.04199853539466858\n",
      "Epoch 19704/30000 Training Loss: 0.049640752375125885\n",
      "Epoch 19705/30000 Training Loss: 0.07281637191772461\n",
      "Epoch 19706/30000 Training Loss: 0.051797136664390564\n",
      "Epoch 19707/30000 Training Loss: 0.04794173315167427\n",
      "Epoch 19708/30000 Training Loss: 0.06236433982849121\n",
      "Epoch 19709/30000 Training Loss: 0.04509542137384415\n",
      "Epoch 19710/30000 Training Loss: 0.046152446419000626\n",
      "Epoch 19711/30000 Training Loss: 0.05436407029628754\n",
      "Epoch 19712/30000 Training Loss: 0.04216006025671959\n",
      "Epoch 19713/30000 Training Loss: 0.041755300015211105\n",
      "Epoch 19714/30000 Training Loss: 0.06284802407026291\n",
      "Epoch 19715/30000 Training Loss: 0.04451971501111984\n",
      "Epoch 19716/30000 Training Loss: 0.03588447347283363\n",
      "Epoch 19717/30000 Training Loss: 0.035394929349422455\n",
      "Epoch 19718/30000 Training Loss: 0.05155458673834801\n",
      "Epoch 19719/30000 Training Loss: 0.05447752773761749\n",
      "Epoch 19720/30000 Training Loss: 0.05274670198559761\n",
      "Epoch 19721/30000 Training Loss: 0.056011080741882324\n",
      "Epoch 19722/30000 Training Loss: 0.05155809968709946\n",
      "Epoch 19723/30000 Training Loss: 0.03628819063305855\n",
      "Epoch 19724/30000 Training Loss: 0.05892016366124153\n",
      "Epoch 19725/30000 Training Loss: 0.056954946368932724\n",
      "Epoch 19726/30000 Training Loss: 0.05020374804735184\n",
      "Epoch 19727/30000 Training Loss: 0.044999007135629654\n",
      "Epoch 19728/30000 Training Loss: 0.041618335992097855\n",
      "Epoch 19729/30000 Training Loss: 0.042731545865535736\n",
      "Epoch 19730/30000 Training Loss: 0.04288927838206291\n",
      "Epoch 19731/30000 Training Loss: 0.04773283377289772\n",
      "Epoch 19732/30000 Training Loss: 0.06228731945157051\n",
      "Epoch 19733/30000 Training Loss: 0.03144671022891998\n",
      "Epoch 19734/30000 Training Loss: 0.058333009481430054\n",
      "Epoch 19735/30000 Training Loss: 0.04230859875679016\n",
      "Epoch 19736/30000 Training Loss: 0.050329431891441345\n",
      "Epoch 19737/30000 Training Loss: 0.04108133539557457\n",
      "Epoch 19738/30000 Training Loss: 0.06884121894836426\n",
      "Epoch 19739/30000 Training Loss: 0.05393211543560028\n",
      "Epoch 19740/30000 Training Loss: 0.046283476054668427\n",
      "Epoch 19741/30000 Training Loss: 0.04986047372221947\n",
      "Epoch 19742/30000 Training Loss: 0.03366442769765854\n",
      "Epoch 19743/30000 Training Loss: 0.05519409105181694\n",
      "Epoch 19744/30000 Training Loss: 0.03903123363852501\n",
      "Epoch 19745/30000 Training Loss: 0.056443896144628525\n",
      "Epoch 19746/30000 Training Loss: 0.03714495524764061\n",
      "Epoch 19747/30000 Training Loss: 0.03232719004154205\n",
      "Epoch 19748/30000 Training Loss: 0.06405047327280045\n",
      "Epoch 19749/30000 Training Loss: 0.04145155847072601\n",
      "Epoch 19750/30000 Training Loss: 0.04599449411034584\n",
      "Epoch 19751/30000 Training Loss: 0.06153339147567749\n",
      "Epoch 19752/30000 Training Loss: 0.059023573994636536\n",
      "Epoch 19753/30000 Training Loss: 0.03777782618999481\n",
      "Epoch 19754/30000 Training Loss: 0.04682740196585655\n",
      "Epoch 19755/30000 Training Loss: 0.05192555859684944\n",
      "Epoch 19756/30000 Training Loss: 0.040406160056591034\n",
      "Epoch 19757/30000 Training Loss: 0.043189637362957\n",
      "Epoch 19758/30000 Training Loss: 0.04034435749053955\n",
      "Epoch 19759/30000 Training Loss: 0.048581529408693314\n",
      "Epoch 19760/30000 Training Loss: 0.05285821110010147\n",
      "Epoch 19761/30000 Training Loss: 0.03946638107299805\n",
      "Epoch 19762/30000 Training Loss: 0.04333290085196495\n",
      "Epoch 19763/30000 Training Loss: 0.056096695363521576\n",
      "Epoch 19764/30000 Training Loss: 0.04376370087265968\n",
      "Epoch 19765/30000 Training Loss: 0.05251423269510269\n",
      "Epoch 19766/30000 Training Loss: 0.046111542731523514\n",
      "Epoch 19767/30000 Training Loss: 0.03558054938912392\n",
      "Epoch 19768/30000 Training Loss: 0.04894105717539787\n",
      "Epoch 19769/30000 Training Loss: 0.04481978714466095\n",
      "Epoch 19770/30000 Training Loss: 0.06372459977865219\n",
      "Epoch 19771/30000 Training Loss: 0.036710478365421295\n",
      "Epoch 19772/30000 Training Loss: 0.05113568156957626\n",
      "Epoch 19773/30000 Training Loss: 0.056708820164203644\n",
      "Epoch 19774/30000 Training Loss: 0.054832108318805695\n",
      "Epoch 19775/30000 Training Loss: 0.04755716398358345\n",
      "Epoch 19776/30000 Training Loss: 0.05108584091067314\n",
      "Epoch 19777/30000 Training Loss: 0.03395972028374672\n",
      "Epoch 19778/30000 Training Loss: 0.055596619844436646\n",
      "Epoch 19779/30000 Training Loss: 0.05364459753036499\n",
      "Epoch 19780/30000 Training Loss: 0.04213881492614746\n",
      "Epoch 19781/30000 Training Loss: 0.04919236898422241\n",
      "Epoch 19782/30000 Training Loss: 0.04795044660568237\n",
      "Epoch 19783/30000 Training Loss: 0.06375375390052795\n",
      "Epoch 19784/30000 Training Loss: 0.04923008382320404\n",
      "Epoch 19785/30000 Training Loss: 0.043008122593164444\n",
      "Epoch 19786/30000 Training Loss: 0.04378204792737961\n",
      "Epoch 19787/30000 Training Loss: 0.053610339760780334\n",
      "Epoch 19788/30000 Training Loss: 0.05124600976705551\n",
      "Epoch 19789/30000 Training Loss: 0.03940633311867714\n",
      "Epoch 19790/30000 Training Loss: 0.03458187356591225\n",
      "Epoch 19791/30000 Training Loss: 0.05696704983711243\n",
      "Epoch 19792/30000 Training Loss: 0.06544490158557892\n",
      "Epoch 19793/30000 Training Loss: 0.04172341525554657\n",
      "Epoch 19794/30000 Training Loss: 0.04413546621799469\n",
      "Epoch 19795/30000 Training Loss: 0.03905948996543884\n",
      "Epoch 19796/30000 Training Loss: 0.03468463569879532\n",
      "Epoch 19797/30000 Training Loss: 0.04691072180867195\n",
      "Epoch 19798/30000 Training Loss: 0.06079939007759094\n",
      "Epoch 19799/30000 Training Loss: 0.04084422066807747\n",
      "Epoch 19800/30000 Training Loss: 0.07054208219051361\n",
      "Epoch 19800/30000 Validation Loss: 0.04802442342042923\n",
      "Epoch 19801/30000 Training Loss: 0.04706166684627533\n",
      "Epoch 19802/30000 Training Loss: 0.04437023401260376\n",
      "Epoch 19803/30000 Training Loss: 0.05771666765213013\n",
      "Epoch 19804/30000 Training Loss: 0.06171801686286926\n",
      "Epoch 19805/30000 Training Loss: 0.05843424052000046\n",
      "Epoch 19806/30000 Training Loss: 0.03880787268280983\n",
      "Epoch 19807/30000 Training Loss: 0.04672315716743469\n",
      "Epoch 19808/30000 Training Loss: 0.04949936270713806\n",
      "Epoch 19809/30000 Training Loss: 0.04563986882567406\n",
      "Epoch 19810/30000 Training Loss: 0.05074012652039528\n",
      "Epoch 19811/30000 Training Loss: 0.042630597949028015\n",
      "Epoch 19812/30000 Training Loss: 0.0609307736158371\n",
      "Epoch 19813/30000 Training Loss: 0.04471183568239212\n",
      "Epoch 19814/30000 Training Loss: 0.052285086363554\n",
      "Epoch 19815/30000 Training Loss: 0.049468643963336945\n",
      "Epoch 19816/30000 Training Loss: 0.06314735859632492\n",
      "Epoch 19817/30000 Training Loss: 0.050550952553749084\n",
      "Epoch 19818/30000 Training Loss: 0.05816679447889328\n",
      "Epoch 19819/30000 Training Loss: 0.05339254438877106\n",
      "Epoch 19820/30000 Training Loss: 0.030953744426369667\n",
      "Epoch 19821/30000 Training Loss: 0.049367506057024\n",
      "Epoch 19822/30000 Training Loss: 0.044667407870292664\n",
      "Epoch 19823/30000 Training Loss: 0.04654228314757347\n",
      "Epoch 19824/30000 Training Loss: 0.05210118740797043\n",
      "Epoch 19825/30000 Training Loss: 0.05747697502374649\n",
      "Epoch 19826/30000 Training Loss: 0.04436793550848961\n",
      "Epoch 19827/30000 Training Loss: 0.042352620512247086\n",
      "Epoch 19828/30000 Training Loss: 0.029799792915582657\n",
      "Epoch 19829/30000 Training Loss: 0.04213988035917282\n",
      "Epoch 19830/30000 Training Loss: 0.05043746903538704\n",
      "Epoch 19831/30000 Training Loss: 0.053570497781038284\n",
      "Epoch 19832/30000 Training Loss: 0.05476495623588562\n",
      "Epoch 19833/30000 Training Loss: 0.04733959957957268\n",
      "Epoch 19834/30000 Training Loss: 0.05362231284379959\n",
      "Epoch 19835/30000 Training Loss: 0.05580928176641464\n",
      "Epoch 19836/30000 Training Loss: 0.049344584345817566\n",
      "Epoch 19837/30000 Training Loss: 0.04916762188076973\n",
      "Epoch 19838/30000 Training Loss: 0.04071655124425888\n",
      "Epoch 19839/30000 Training Loss: 0.04186854884028435\n",
      "Epoch 19840/30000 Training Loss: 0.05990302562713623\n",
      "Epoch 19841/30000 Training Loss: 0.037739429622888565\n",
      "Epoch 19842/30000 Training Loss: 0.03521796315908432\n",
      "Epoch 19843/30000 Training Loss: 0.04669824242591858\n",
      "Epoch 19844/30000 Training Loss: 0.041962288320064545\n",
      "Epoch 19845/30000 Training Loss: 0.04341395944356918\n",
      "Epoch 19846/30000 Training Loss: 0.038130246102809906\n",
      "Epoch 19847/30000 Training Loss: 0.0518476739525795\n",
      "Epoch 19848/30000 Training Loss: 0.04878859221935272\n",
      "Epoch 19849/30000 Training Loss: 0.036465827375650406\n",
      "Epoch 19850/30000 Training Loss: 0.03409470245242119\n",
      "Epoch 19851/30000 Training Loss: 0.04312873259186745\n",
      "Epoch 19852/30000 Training Loss: 0.04563853144645691\n",
      "Epoch 19853/30000 Training Loss: 0.0303664393723011\n",
      "Epoch 19854/30000 Training Loss: 0.04257219284772873\n",
      "Epoch 19855/30000 Training Loss: 0.039074502885341644\n",
      "Epoch 19856/30000 Training Loss: 0.04453149437904358\n",
      "Epoch 19857/30000 Training Loss: 0.058159239590168\n",
      "Epoch 19858/30000 Training Loss: 0.06130628660321236\n",
      "Epoch 19859/30000 Training Loss: 0.04732467979192734\n",
      "Epoch 19860/30000 Training Loss: 0.05544426292181015\n",
      "Epoch 19861/30000 Training Loss: 0.05173317342996597\n",
      "Epoch 19862/30000 Training Loss: 0.0667731910943985\n",
      "Epoch 19863/30000 Training Loss: 0.04399149864912033\n",
      "Epoch 19864/30000 Training Loss: 0.05824638903141022\n",
      "Epoch 19865/30000 Training Loss: 0.03605246543884277\n",
      "Epoch 19866/30000 Training Loss: 0.04856325685977936\n",
      "Epoch 19867/30000 Training Loss: 0.038923319429159164\n",
      "Epoch 19868/30000 Training Loss: 0.03860340267419815\n",
      "Epoch 19869/30000 Training Loss: 0.04052266851067543\n",
      "Epoch 19870/30000 Training Loss: 0.04252202808856964\n",
      "Epoch 19871/30000 Training Loss: 0.04448220133781433\n",
      "Epoch 19872/30000 Training Loss: 0.042029570788145065\n",
      "Epoch 19873/30000 Training Loss: 0.03336725011467934\n",
      "Epoch 19874/30000 Training Loss: 0.04274384677410126\n",
      "Epoch 19875/30000 Training Loss: 0.041647471487522125\n",
      "Epoch 19876/30000 Training Loss: 0.04427509754896164\n",
      "Epoch 19877/30000 Training Loss: 0.03690909594297409\n",
      "Epoch 19878/30000 Training Loss: 0.05078146979212761\n",
      "Epoch 19879/30000 Training Loss: 0.03730971738696098\n",
      "Epoch 19880/30000 Training Loss: 0.07275421172380447\n",
      "Epoch 19881/30000 Training Loss: 0.06467483192682266\n",
      "Epoch 19882/30000 Training Loss: 0.0491478368639946\n",
      "Epoch 19883/30000 Training Loss: 0.04610976204276085\n",
      "Epoch 19884/30000 Training Loss: 0.04289320111274719\n",
      "Epoch 19885/30000 Training Loss: 0.046638187021017075\n",
      "Epoch 19886/30000 Training Loss: 0.044823646545410156\n",
      "Epoch 19887/30000 Training Loss: 0.04679058492183685\n",
      "Epoch 19888/30000 Training Loss: 0.041701603680849075\n",
      "Epoch 19889/30000 Training Loss: 0.04342452436685562\n",
      "Epoch 19890/30000 Training Loss: 0.05256263166666031\n",
      "Epoch 19891/30000 Training Loss: 0.038373395800590515\n",
      "Epoch 19892/30000 Training Loss: 0.03920911252498627\n",
      "Epoch 19893/30000 Training Loss: 0.05068445950746536\n",
      "Epoch 19894/30000 Training Loss: 0.042073480784893036\n",
      "Epoch 19895/30000 Training Loss: 0.029707640409469604\n",
      "Epoch 19896/30000 Training Loss: 0.05184079334139824\n",
      "Epoch 19897/30000 Training Loss: 0.042734697461128235\n",
      "Epoch 19898/30000 Training Loss: 0.054438985884189606\n",
      "Epoch 19899/30000 Training Loss: 0.044000133872032166\n",
      "Epoch 19900/30000 Training Loss: 0.0507340207695961\n",
      "Epoch 19900/30000 Validation Loss: 0.053852155804634094\n",
      "Epoch 19901/30000 Training Loss: 0.03574931621551514\n",
      "Epoch 19902/30000 Training Loss: 0.05640594661235809\n",
      "Epoch 19903/30000 Training Loss: 0.048475995659828186\n",
      "Epoch 19904/30000 Training Loss: 0.033845897763967514\n",
      "Epoch 19905/30000 Training Loss: 0.055952876806259155\n",
      "Epoch 19906/30000 Training Loss: 0.055710598826408386\n",
      "Epoch 19907/30000 Training Loss: 0.06892073899507523\n",
      "Epoch 19908/30000 Training Loss: 0.05455200374126434\n",
      "Epoch 19909/30000 Training Loss: 0.04489731788635254\n",
      "Epoch 19910/30000 Training Loss: 0.04131927713751793\n",
      "Epoch 19911/30000 Training Loss: 0.04969009757041931\n",
      "Epoch 19912/30000 Training Loss: 0.036748290061950684\n",
      "Epoch 19913/30000 Training Loss: 0.05616488307714462\n",
      "Epoch 19914/30000 Training Loss: 0.06246598809957504\n",
      "Epoch 19915/30000 Training Loss: 0.048803482204675674\n",
      "Epoch 19916/30000 Training Loss: 0.03762718290090561\n",
      "Epoch 19917/30000 Training Loss: 0.051553141325712204\n",
      "Epoch 19918/30000 Training Loss: 0.048629142343997955\n",
      "Epoch 19919/30000 Training Loss: 0.03246968984603882\n",
      "Epoch 19920/30000 Training Loss: 0.06149779260158539\n",
      "Epoch 19921/30000 Training Loss: 0.06040622293949127\n",
      "Epoch 19922/30000 Training Loss: 0.051114704459905624\n",
      "Epoch 19923/30000 Training Loss: 0.04673292487859726\n",
      "Epoch 19924/30000 Training Loss: 0.045509159564971924\n",
      "Epoch 19925/30000 Training Loss: 0.04617444425821304\n",
      "Epoch 19926/30000 Training Loss: 0.06063143163919449\n",
      "Epoch 19927/30000 Training Loss: 0.047553546726703644\n",
      "Epoch 19928/30000 Training Loss: 0.06856024265289307\n",
      "Epoch 19929/30000 Training Loss: 0.049920663237571716\n",
      "Epoch 19930/30000 Training Loss: 0.04822771996259689\n",
      "Epoch 19931/30000 Training Loss: 0.04791245236992836\n",
      "Epoch 19932/30000 Training Loss: 0.054350197315216064\n",
      "Epoch 19933/30000 Training Loss: 0.045489925891160965\n",
      "Epoch 19934/30000 Training Loss: 0.056186873465776443\n",
      "Epoch 19935/30000 Training Loss: 0.03734526410698891\n",
      "Epoch 19936/30000 Training Loss: 0.03656376153230667\n",
      "Epoch 19937/30000 Training Loss: 0.03794100880622864\n",
      "Epoch 19938/30000 Training Loss: 0.045912791043519974\n",
      "Epoch 19939/30000 Training Loss: 0.042181145399808884\n",
      "Epoch 19940/30000 Training Loss: 0.04367956891655922\n",
      "Epoch 19941/30000 Training Loss: 0.045053139328956604\n",
      "Epoch 19942/30000 Training Loss: 0.07874710112810135\n",
      "Epoch 19943/30000 Training Loss: 0.05070082098245621\n",
      "Epoch 19944/30000 Training Loss: 0.054798293858766556\n",
      "Epoch 19945/30000 Training Loss: 0.05398781970143318\n",
      "Epoch 19946/30000 Training Loss: 0.05763760209083557\n",
      "Epoch 19947/30000 Training Loss: 0.04122895374894142\n",
      "Epoch 19948/30000 Training Loss: 0.042802244424819946\n",
      "Epoch 19949/30000 Training Loss: 0.05414111912250519\n",
      "Epoch 19950/30000 Training Loss: 0.03997167944908142\n",
      "Epoch 19951/30000 Training Loss: 0.05877500772476196\n",
      "Epoch 19952/30000 Training Loss: 0.05216260626912117\n",
      "Epoch 19953/30000 Training Loss: 0.06855810433626175\n",
      "Epoch 19954/30000 Training Loss: 0.04124502092599869\n",
      "Epoch 19955/30000 Training Loss: 0.04763353615999222\n",
      "Epoch 19956/30000 Training Loss: 0.0420001856982708\n",
      "Epoch 19957/30000 Training Loss: 0.05033384636044502\n",
      "Epoch 19958/30000 Training Loss: 0.05153095722198486\n",
      "Epoch 19959/30000 Training Loss: 0.05232549458742142\n",
      "Epoch 19960/30000 Training Loss: 0.05015599727630615\n",
      "Epoch 19961/30000 Training Loss: 0.034175194799900055\n",
      "Epoch 19962/30000 Training Loss: 0.0553559884428978\n",
      "Epoch 19963/30000 Training Loss: 0.04497893899679184\n",
      "Epoch 19964/30000 Training Loss: 0.0425293929874897\n",
      "Epoch 19965/30000 Training Loss: 0.05613121762871742\n",
      "Epoch 19966/30000 Training Loss: 0.05167132616043091\n",
      "Epoch 19967/30000 Training Loss: 0.042029839009046555\n",
      "Epoch 19968/30000 Training Loss: 0.03730843588709831\n",
      "Epoch 19969/30000 Training Loss: 0.05193805694580078\n",
      "Epoch 19970/30000 Training Loss: 0.06575135141611099\n",
      "Epoch 19971/30000 Training Loss: 0.049642354249954224\n",
      "Epoch 19972/30000 Training Loss: 0.04996611922979355\n",
      "Epoch 19973/30000 Training Loss: 0.06599733978509903\n",
      "Epoch 19974/30000 Training Loss: 0.0378415547311306\n",
      "Epoch 19975/30000 Training Loss: 0.04531972110271454\n",
      "Epoch 19976/30000 Training Loss: 0.05473300814628601\n",
      "Epoch 19977/30000 Training Loss: 0.06171632558107376\n",
      "Epoch 19978/30000 Training Loss: 0.034255944192409515\n",
      "Epoch 19979/30000 Training Loss: 0.05255969613790512\n",
      "Epoch 19980/30000 Training Loss: 0.04480384662747383\n",
      "Epoch 19981/30000 Training Loss: 0.03786283731460571\n",
      "Epoch 19982/30000 Training Loss: 0.040979500859975815\n",
      "Epoch 19983/30000 Training Loss: 0.04074706882238388\n",
      "Epoch 19984/30000 Training Loss: 0.03396383300423622\n",
      "Epoch 19985/30000 Training Loss: 0.04547365382313728\n",
      "Epoch 19986/30000 Training Loss: 0.04453242942690849\n",
      "Epoch 19987/30000 Training Loss: 0.04084620252251625\n",
      "Epoch 19988/30000 Training Loss: 0.03759053349494934\n",
      "Epoch 19989/30000 Training Loss: 0.04844565689563751\n",
      "Epoch 19990/30000 Training Loss: 0.04984726384282112\n",
      "Epoch 19991/30000 Training Loss: 0.034596312791109085\n",
      "Epoch 19992/30000 Training Loss: 0.05599057674407959\n",
      "Epoch 19993/30000 Training Loss: 0.0533076636493206\n",
      "Epoch 19994/30000 Training Loss: 0.03924927860498428\n",
      "Epoch 19995/30000 Training Loss: 0.05165962502360344\n",
      "Epoch 19996/30000 Training Loss: 0.05567348375916481\n",
      "Epoch 19997/30000 Training Loss: 0.04132344201207161\n",
      "Epoch 19998/30000 Training Loss: 0.043542392551898956\n",
      "Epoch 19999/30000 Training Loss: 0.04051478952169418\n",
      "Epoch 20000/30000 Training Loss: 0.04853690043091774\n",
      "Epoch 20000/30000 Validation Loss: 0.05028087645769119\n",
      "Epoch 20001/30000 Training Loss: 0.04382489621639252\n",
      "Epoch 20002/30000 Training Loss: 0.05657486990094185\n",
      "Epoch 20003/30000 Training Loss: 0.04165322706103325\n",
      "Epoch 20004/30000 Training Loss: 0.059209905564785004\n",
      "Epoch 20005/30000 Training Loss: 0.032952938228845596\n",
      "Epoch 20006/30000 Training Loss: 0.054901570081710815\n",
      "Epoch 20007/30000 Training Loss: 0.05090929567813873\n",
      "Epoch 20008/30000 Training Loss: 0.04247969388961792\n",
      "Epoch 20009/30000 Training Loss: 0.06555173546075821\n",
      "Epoch 20010/30000 Training Loss: 0.05218616873025894\n",
      "Epoch 20011/30000 Training Loss: 0.04110831022262573\n",
      "Epoch 20012/30000 Training Loss: 0.03261179476976395\n",
      "Epoch 20013/30000 Training Loss: 0.0490095280110836\n",
      "Epoch 20014/30000 Training Loss: 0.0637502670288086\n",
      "Epoch 20015/30000 Training Loss: 0.045473065227270126\n",
      "Epoch 20016/30000 Training Loss: 0.04140416160225868\n",
      "Epoch 20017/30000 Training Loss: 0.04304341599345207\n",
      "Epoch 20018/30000 Training Loss: 0.04492015391588211\n",
      "Epoch 20019/30000 Training Loss: 0.03601743280887604\n",
      "Epoch 20020/30000 Training Loss: 0.043238479644060135\n",
      "Epoch 20021/30000 Training Loss: 0.03330108895897865\n",
      "Epoch 20022/30000 Training Loss: 0.05521345138549805\n",
      "Epoch 20023/30000 Training Loss: 0.062253423035144806\n",
      "Epoch 20024/30000 Training Loss: 0.04296066612005234\n",
      "Epoch 20025/30000 Training Loss: 0.044307027012109756\n",
      "Epoch 20026/30000 Training Loss: 0.039782363921403885\n",
      "Epoch 20027/30000 Training Loss: 0.04558197781443596\n",
      "Epoch 20028/30000 Training Loss: 0.05981084704399109\n",
      "Epoch 20029/30000 Training Loss: 0.058832667768001556\n",
      "Epoch 20030/30000 Training Loss: 0.03801725059747696\n",
      "Epoch 20031/30000 Training Loss: 0.03946290537714958\n",
      "Epoch 20032/30000 Training Loss: 0.05022313445806503\n",
      "Epoch 20033/30000 Training Loss: 0.03275968134403229\n",
      "Epoch 20034/30000 Training Loss: 0.05315084382891655\n",
      "Epoch 20035/30000 Training Loss: 0.0407472625374794\n",
      "Epoch 20036/30000 Training Loss: 0.04558646306395531\n",
      "Epoch 20037/30000 Training Loss: 0.0440557487308979\n",
      "Epoch 20038/30000 Training Loss: 0.05343317240476608\n",
      "Epoch 20039/30000 Training Loss: 0.04590190201997757\n",
      "Epoch 20040/30000 Training Loss: 0.05057309567928314\n",
      "Epoch 20041/30000 Training Loss: 0.0418630950152874\n",
      "Epoch 20042/30000 Training Loss: 0.03432052955031395\n",
      "Epoch 20043/30000 Training Loss: 0.03512987121939659\n",
      "Epoch 20044/30000 Training Loss: 0.043298136442899704\n",
      "Epoch 20045/30000 Training Loss: 0.03753194585442543\n",
      "Epoch 20046/30000 Training Loss: 0.051053691655397415\n",
      "Epoch 20047/30000 Training Loss: 0.05960655212402344\n",
      "Epoch 20048/30000 Training Loss: 0.052256762981414795\n",
      "Epoch 20049/30000 Training Loss: 0.05847578123211861\n",
      "Epoch 20050/30000 Training Loss: 0.04914221167564392\n",
      "Epoch 20051/30000 Training Loss: 0.04037407040596008\n",
      "Epoch 20052/30000 Training Loss: 0.049368586391210556\n",
      "Epoch 20053/30000 Training Loss: 0.046104948967695236\n",
      "Epoch 20054/30000 Training Loss: 0.062276631593704224\n",
      "Epoch 20055/30000 Training Loss: 0.055933523923158646\n",
      "Epoch 20056/30000 Training Loss: 0.05136275663971901\n",
      "Epoch 20057/30000 Training Loss: 0.044773027300834656\n",
      "Epoch 20058/30000 Training Loss: 0.04862780123949051\n",
      "Epoch 20059/30000 Training Loss: 0.04110695421695709\n",
      "Epoch 20060/30000 Training Loss: 0.06310765445232391\n",
      "Epoch 20061/30000 Training Loss: 0.0484408400952816\n",
      "Epoch 20062/30000 Training Loss: 0.05014091730117798\n",
      "Epoch 20063/30000 Training Loss: 0.04610981419682503\n",
      "Epoch 20064/30000 Training Loss: 0.05398000404238701\n",
      "Epoch 20065/30000 Training Loss: 0.05248543620109558\n",
      "Epoch 20066/30000 Training Loss: 0.052340175956487656\n",
      "Epoch 20067/30000 Training Loss: 0.045137763023376465\n",
      "Epoch 20068/30000 Training Loss: 0.03786791115999222\n",
      "Epoch 20069/30000 Training Loss: 0.04232778400182724\n",
      "Epoch 20070/30000 Training Loss: 0.05290087312459946\n",
      "Epoch 20071/30000 Training Loss: 0.03871253505349159\n",
      "Epoch 20072/30000 Training Loss: 0.062222134321928024\n",
      "Epoch 20073/30000 Training Loss: 0.050909534096717834\n",
      "Epoch 20074/30000 Training Loss: 0.05207245051860809\n",
      "Epoch 20075/30000 Training Loss: 0.040995724499225616\n",
      "Epoch 20076/30000 Training Loss: 0.05604380741715431\n",
      "Epoch 20077/30000 Training Loss: 0.044919177889823914\n",
      "Epoch 20078/30000 Training Loss: 0.03659795597195625\n",
      "Epoch 20079/30000 Training Loss: 0.05300133675336838\n",
      "Epoch 20080/30000 Training Loss: 0.042099371552467346\n",
      "Epoch 20081/30000 Training Loss: 0.036856066435575485\n",
      "Epoch 20082/30000 Training Loss: 0.03782620280981064\n",
      "Epoch 20083/30000 Training Loss: 0.04046091064810753\n",
      "Epoch 20084/30000 Training Loss: 0.0505274273455143\n",
      "Epoch 20085/30000 Training Loss: 0.05278252810239792\n",
      "Epoch 20086/30000 Training Loss: 0.058500587940216064\n",
      "Epoch 20087/30000 Training Loss: 0.048288822174072266\n",
      "Epoch 20088/30000 Training Loss: 0.06037919968366623\n",
      "Epoch 20089/30000 Training Loss: 0.03095298260450363\n",
      "Epoch 20090/30000 Training Loss: 0.05027015879750252\n",
      "Epoch 20091/30000 Training Loss: 0.034680113196372986\n",
      "Epoch 20092/30000 Training Loss: 0.03229403495788574\n",
      "Epoch 20093/30000 Training Loss: 0.05133655667304993\n",
      "Epoch 20094/30000 Training Loss: 0.03886887803673744\n",
      "Epoch 20095/30000 Training Loss: 0.040836360305547714\n",
      "Epoch 20096/30000 Training Loss: 0.045147836208343506\n",
      "Epoch 20097/30000 Training Loss: 0.05452881008386612\n",
      "Epoch 20098/30000 Training Loss: 0.04809290170669556\n",
      "Epoch 20099/30000 Training Loss: 0.05071535333991051\n",
      "Epoch 20100/30000 Training Loss: 0.04598056897521019\n",
      "Epoch 20100/30000 Validation Loss: 0.0454881489276886\n",
      "Epoch 20101/30000 Training Loss: 0.052065327763557434\n",
      "Epoch 20102/30000 Training Loss: 0.04824697971343994\n",
      "Epoch 20103/30000 Training Loss: 0.046410977840423584\n",
      "Epoch 20104/30000 Training Loss: 0.05224374681711197\n",
      "Epoch 20105/30000 Training Loss: 0.04963270574808121\n",
      "Epoch 20106/30000 Training Loss: 0.038926951587200165\n",
      "Epoch 20107/30000 Training Loss: 0.04757427051663399\n",
      "Epoch 20108/30000 Training Loss: 0.05025460571050644\n",
      "Epoch 20109/30000 Training Loss: 0.044142961502075195\n",
      "Epoch 20110/30000 Training Loss: 0.05295346677303314\n",
      "Epoch 20111/30000 Training Loss: 0.04407302290201187\n",
      "Epoch 20112/30000 Training Loss: 0.045085299760103226\n",
      "Epoch 20113/30000 Training Loss: 0.03502552583813667\n",
      "Epoch 20114/30000 Training Loss: 0.04919423162937164\n",
      "Epoch 20115/30000 Training Loss: 0.05751867592334747\n",
      "Epoch 20116/30000 Training Loss: 0.05051949620246887\n",
      "Epoch 20117/30000 Training Loss: 0.041754283010959625\n",
      "Epoch 20118/30000 Training Loss: 0.04444681107997894\n",
      "Epoch 20119/30000 Training Loss: 0.06867404282093048\n",
      "Epoch 20120/30000 Training Loss: 0.05312056094408035\n",
      "Epoch 20121/30000 Training Loss: 0.0393490307033062\n",
      "Epoch 20122/30000 Training Loss: 0.048201870173215866\n",
      "Epoch 20123/30000 Training Loss: 0.07248169183731079\n",
      "Epoch 20124/30000 Training Loss: 0.03792041540145874\n",
      "Epoch 20125/30000 Training Loss: 0.04926327243447304\n",
      "Epoch 20126/30000 Training Loss: 0.0644550770521164\n",
      "Epoch 20127/30000 Training Loss: 0.06769543886184692\n",
      "Epoch 20128/30000 Training Loss: 0.03982238471508026\n",
      "Epoch 20129/30000 Training Loss: 0.04540134593844414\n",
      "Epoch 20130/30000 Training Loss: 0.06476020812988281\n",
      "Epoch 20131/30000 Training Loss: 0.040454454720020294\n",
      "Epoch 20132/30000 Training Loss: 0.0596579909324646\n",
      "Epoch 20133/30000 Training Loss: 0.036777205765247345\n",
      "Epoch 20134/30000 Training Loss: 0.05059574544429779\n",
      "Epoch 20135/30000 Training Loss: 0.05250691995024681\n",
      "Epoch 20136/30000 Training Loss: 0.045412104576826096\n",
      "Epoch 20137/30000 Training Loss: 0.047356776893138885\n",
      "Epoch 20138/30000 Training Loss: 0.0589696429669857\n",
      "Epoch 20139/30000 Training Loss: 0.05554499104619026\n",
      "Epoch 20140/30000 Training Loss: 0.040511466562747955\n",
      "Epoch 20141/30000 Training Loss: 0.04861849546432495\n",
      "Epoch 20142/30000 Training Loss: 0.04260432347655296\n",
      "Epoch 20143/30000 Training Loss: 0.0640411227941513\n",
      "Epoch 20144/30000 Training Loss: 0.05605503171682358\n",
      "Epoch 20145/30000 Training Loss: 0.06690093874931335\n",
      "Epoch 20146/30000 Training Loss: 0.0438719242811203\n",
      "Epoch 20147/30000 Training Loss: 0.05204358696937561\n",
      "Epoch 20148/30000 Training Loss: 0.057356156408786774\n",
      "Epoch 20149/30000 Training Loss: 0.053409475833177567\n",
      "Epoch 20150/30000 Training Loss: 0.038851622492074966\n",
      "Epoch 20151/30000 Training Loss: 0.05512378364801407\n",
      "Epoch 20152/30000 Training Loss: 0.040393728762865067\n",
      "Epoch 20153/30000 Training Loss: 0.04350030794739723\n",
      "Epoch 20154/30000 Training Loss: 0.043487876653671265\n",
      "Epoch 20155/30000 Training Loss: 0.04734005033969879\n",
      "Epoch 20156/30000 Training Loss: 0.0394630953669548\n",
      "Epoch 20157/30000 Training Loss: 0.04343028366565704\n",
      "Epoch 20158/30000 Training Loss: 0.04778490588068962\n",
      "Epoch 20159/30000 Training Loss: 0.05009644478559494\n",
      "Epoch 20160/30000 Training Loss: 0.030391689389944077\n",
      "Epoch 20161/30000 Training Loss: 0.058469973504543304\n",
      "Epoch 20162/30000 Training Loss: 0.060713667422533035\n",
      "Epoch 20163/30000 Training Loss: 0.02863297052681446\n",
      "Epoch 20164/30000 Training Loss: 0.04500124603509903\n",
      "Epoch 20165/30000 Training Loss: 0.04756961017847061\n",
      "Epoch 20166/30000 Training Loss: 0.04023861140012741\n",
      "Epoch 20167/30000 Training Loss: 0.05529668927192688\n",
      "Epoch 20168/30000 Training Loss: 0.052085183560848236\n",
      "Epoch 20169/30000 Training Loss: 0.04727470502257347\n",
      "Epoch 20170/30000 Training Loss: 0.05352158471941948\n",
      "Epoch 20171/30000 Training Loss: 0.05437466502189636\n",
      "Epoch 20172/30000 Training Loss: 0.058817315846681595\n",
      "Epoch 20173/30000 Training Loss: 0.04042403772473335\n",
      "Epoch 20174/30000 Training Loss: 0.042383525520563126\n",
      "Epoch 20175/30000 Training Loss: 0.05511774867773056\n",
      "Epoch 20176/30000 Training Loss: 0.05026929825544357\n",
      "Epoch 20177/30000 Training Loss: 0.041778162121772766\n",
      "Epoch 20178/30000 Training Loss: 0.04719523340463638\n",
      "Epoch 20179/30000 Training Loss: 0.06861218065023422\n",
      "Epoch 20180/30000 Training Loss: 0.038713905960321426\n",
      "Epoch 20181/30000 Training Loss: 0.053267888724803925\n",
      "Epoch 20182/30000 Training Loss: 0.0398041233420372\n",
      "Epoch 20183/30000 Training Loss: 0.047061823308467865\n",
      "Epoch 20184/30000 Training Loss: 0.04373970255255699\n",
      "Epoch 20185/30000 Training Loss: 0.042465053498744965\n",
      "Epoch 20186/30000 Training Loss: 0.05089152604341507\n",
      "Epoch 20187/30000 Training Loss: 0.0390276238322258\n",
      "Epoch 20188/30000 Training Loss: 0.05223432183265686\n",
      "Epoch 20189/30000 Training Loss: 0.041818127036094666\n",
      "Epoch 20190/30000 Training Loss: 0.035736292600631714\n",
      "Epoch 20191/30000 Training Loss: 0.04671680927276611\n",
      "Epoch 20192/30000 Training Loss: 0.048127733170986176\n",
      "Epoch 20193/30000 Training Loss: 0.04992980137467384\n",
      "Epoch 20194/30000 Training Loss: 0.03656330332159996\n",
      "Epoch 20195/30000 Training Loss: 0.04988853633403778\n",
      "Epoch 20196/30000 Training Loss: 0.049746401607990265\n",
      "Epoch 20197/30000 Training Loss: 0.055125847458839417\n",
      "Epoch 20198/30000 Training Loss: 0.06802573800086975\n",
      "Epoch 20199/30000 Training Loss: 0.04681888595223427\n",
      "Epoch 20200/30000 Training Loss: 0.06099274009466171\n",
      "Epoch 20200/30000 Validation Loss: 0.056855808943510056\n",
      "Epoch 20201/30000 Training Loss: 0.0311347134411335\n",
      "Epoch 20202/30000 Training Loss: 0.041926346719264984\n",
      "Epoch 20203/30000 Training Loss: 0.04356759041547775\n",
      "Epoch 20204/30000 Training Loss: 0.03828568756580353\n",
      "Epoch 20205/30000 Training Loss: 0.03698235750198364\n",
      "Epoch 20206/30000 Training Loss: 0.05105837434530258\n",
      "Epoch 20207/30000 Training Loss: 0.04772789031267166\n",
      "Epoch 20208/30000 Training Loss: 0.03827070817351341\n",
      "Epoch 20209/30000 Training Loss: 0.05471196398139\n",
      "Epoch 20210/30000 Training Loss: 0.04672980681061745\n",
      "Epoch 20211/30000 Training Loss: 0.04810209944844246\n",
      "Epoch 20212/30000 Training Loss: 0.0555896982550621\n",
      "Epoch 20213/30000 Training Loss: 0.0510639064013958\n",
      "Epoch 20214/30000 Training Loss: 0.04445929452776909\n",
      "Epoch 20215/30000 Training Loss: 0.03584570437669754\n",
      "Epoch 20216/30000 Training Loss: 0.03201325982809067\n",
      "Epoch 20217/30000 Training Loss: 0.05189737305045128\n",
      "Epoch 20218/30000 Training Loss: 0.06558019667863846\n",
      "Epoch 20219/30000 Training Loss: 0.04641279578208923\n",
      "Epoch 20220/30000 Training Loss: 0.03887028619647026\n",
      "Epoch 20221/30000 Training Loss: 0.03618567809462547\n",
      "Epoch 20222/30000 Training Loss: 0.05096220225095749\n",
      "Epoch 20223/30000 Training Loss: 0.06271228194236755\n",
      "Epoch 20224/30000 Training Loss: 0.03761373460292816\n",
      "Epoch 20225/30000 Training Loss: 0.0643070638179779\n",
      "Epoch 20226/30000 Training Loss: 0.05189733952283859\n",
      "Epoch 20227/30000 Training Loss: 0.05200760066509247\n",
      "Epoch 20228/30000 Training Loss: 0.04317163676023483\n",
      "Epoch 20229/30000 Training Loss: 0.06193487346172333\n",
      "Epoch 20230/30000 Training Loss: 0.05129528045654297\n",
      "Epoch 20231/30000 Training Loss: 0.03818309307098389\n",
      "Epoch 20232/30000 Training Loss: 0.0590880811214447\n",
      "Epoch 20233/30000 Training Loss: 0.04953060299158096\n",
      "Epoch 20234/30000 Training Loss: 0.035079848021268845\n",
      "Epoch 20235/30000 Training Loss: 0.04194817319512367\n",
      "Epoch 20236/30000 Training Loss: 0.03894650563597679\n",
      "Epoch 20237/30000 Training Loss: 0.047028400003910065\n",
      "Epoch 20238/30000 Training Loss: 0.04975946992635727\n",
      "Epoch 20239/30000 Training Loss: 0.047589223831892014\n",
      "Epoch 20240/30000 Training Loss: 0.04633060097694397\n",
      "Epoch 20241/30000 Training Loss: 0.040430132299661636\n",
      "Epoch 20242/30000 Training Loss: 0.038456499576568604\n",
      "Epoch 20243/30000 Training Loss: 0.04943627119064331\n",
      "Epoch 20244/30000 Training Loss: 0.04452458769083023\n",
      "Epoch 20245/30000 Training Loss: 0.044797465205192566\n",
      "Epoch 20246/30000 Training Loss: 0.04958142712712288\n",
      "Epoch 20247/30000 Training Loss: 0.04371330887079239\n",
      "Epoch 20248/30000 Training Loss: 0.054080262780189514\n",
      "Epoch 20249/30000 Training Loss: 0.059321071952581406\n",
      "Epoch 20250/30000 Training Loss: 0.03559477627277374\n",
      "Epoch 20251/30000 Training Loss: 0.07178163528442383\n",
      "Epoch 20252/30000 Training Loss: 0.056725796312093735\n",
      "Epoch 20253/30000 Training Loss: 0.04369425028562546\n",
      "Epoch 20254/30000 Training Loss: 0.053932204842567444\n",
      "Epoch 20255/30000 Training Loss: 0.045033976435661316\n",
      "Epoch 20256/30000 Training Loss: 0.04146953299641609\n",
      "Epoch 20257/30000 Training Loss: 0.04290386289358139\n",
      "Epoch 20258/30000 Training Loss: 0.036180246621370316\n",
      "Epoch 20259/30000 Training Loss: 0.06444430351257324\n",
      "Epoch 20260/30000 Training Loss: 0.043878309428691864\n",
      "Epoch 20261/30000 Training Loss: 0.04286302998661995\n",
      "Epoch 20262/30000 Training Loss: 0.053261708468198776\n",
      "Epoch 20263/30000 Training Loss: 0.035142932087183\n",
      "Epoch 20264/30000 Training Loss: 0.04998842999339104\n",
      "Epoch 20265/30000 Training Loss: 0.0329868420958519\n",
      "Epoch 20266/30000 Training Loss: 0.03459426760673523\n",
      "Epoch 20267/30000 Training Loss: 0.033333417028188705\n",
      "Epoch 20268/30000 Training Loss: 0.05089421570301056\n",
      "Epoch 20269/30000 Training Loss: 0.037386830896139145\n",
      "Epoch 20270/30000 Training Loss: 0.04639352485537529\n",
      "Epoch 20271/30000 Training Loss: 0.0672752782702446\n",
      "Epoch 20272/30000 Training Loss: 0.03608447685837746\n",
      "Epoch 20273/30000 Training Loss: 0.04341966286301613\n",
      "Epoch 20274/30000 Training Loss: 0.04940076917409897\n",
      "Epoch 20275/30000 Training Loss: 0.03312714397907257\n",
      "Epoch 20276/30000 Training Loss: 0.052359286695718765\n",
      "Epoch 20277/30000 Training Loss: 0.05977606400847435\n",
      "Epoch 20278/30000 Training Loss: 0.0635831207036972\n",
      "Epoch 20279/30000 Training Loss: 0.03824291750788689\n",
      "Epoch 20280/30000 Training Loss: 0.044437192380428314\n",
      "Epoch 20281/30000 Training Loss: 0.03481285274028778\n",
      "Epoch 20282/30000 Training Loss: 0.06836239993572235\n",
      "Epoch 20283/30000 Training Loss: 0.05024106800556183\n",
      "Epoch 20284/30000 Training Loss: 0.04001007601618767\n",
      "Epoch 20285/30000 Training Loss: 0.06230144947767258\n",
      "Epoch 20286/30000 Training Loss: 0.03894173353910446\n",
      "Epoch 20287/30000 Training Loss: 0.04276525601744652\n",
      "Epoch 20288/30000 Training Loss: 0.061913006007671356\n",
      "Epoch 20289/30000 Training Loss: 0.045411352068185806\n",
      "Epoch 20290/30000 Training Loss: 0.0382821261882782\n",
      "Epoch 20291/30000 Training Loss: 0.054390136152505875\n",
      "Epoch 20292/30000 Training Loss: 0.04441423714160919\n",
      "Epoch 20293/30000 Training Loss: 0.04721563309431076\n",
      "Epoch 20294/30000 Training Loss: 0.03789673373103142\n",
      "Epoch 20295/30000 Training Loss: 0.04322119802236557\n",
      "Epoch 20296/30000 Training Loss: 0.040230970829725266\n",
      "Epoch 20297/30000 Training Loss: 0.054591596126556396\n",
      "Epoch 20298/30000 Training Loss: 0.04463067650794983\n",
      "Epoch 20299/30000 Training Loss: 0.07066603004932404\n",
      "Epoch 20300/30000 Training Loss: 0.04086461663246155\n",
      "Epoch 20300/30000 Validation Loss: 0.03371797129511833\n",
      "Epoch 20301/30000 Training Loss: 0.061344314366579056\n",
      "Epoch 20302/30000 Training Loss: 0.03427274897694588\n",
      "Epoch 20303/30000 Training Loss: 0.05638837814331055\n",
      "Epoch 20304/30000 Training Loss: 0.03866036236286163\n",
      "Epoch 20305/30000 Training Loss: 0.04584081470966339\n",
      "Epoch 20306/30000 Training Loss: 0.047441188246011734\n",
      "Epoch 20307/30000 Training Loss: 0.047632720321416855\n",
      "Epoch 20308/30000 Training Loss: 0.04039641097187996\n",
      "Epoch 20309/30000 Training Loss: 0.038955286145210266\n",
      "Epoch 20310/30000 Training Loss: 0.04085717350244522\n",
      "Epoch 20311/30000 Training Loss: 0.04618288576602936\n",
      "Epoch 20312/30000 Training Loss: 0.05086619779467583\n",
      "Epoch 20313/30000 Training Loss: 0.04313773661851883\n",
      "Epoch 20314/30000 Training Loss: 0.03858107700943947\n",
      "Epoch 20315/30000 Training Loss: 0.042832888662815094\n",
      "Epoch 20316/30000 Training Loss: 0.05123237892985344\n",
      "Epoch 20317/30000 Training Loss: 0.04117592051625252\n",
      "Epoch 20318/30000 Training Loss: 0.03264041244983673\n",
      "Epoch 20319/30000 Training Loss: 0.04616716504096985\n",
      "Epoch 20320/30000 Training Loss: 0.05683908611536026\n",
      "Epoch 20321/30000 Training Loss: 0.04526640102267265\n",
      "Epoch 20322/30000 Training Loss: 0.03566860780119896\n",
      "Epoch 20323/30000 Training Loss: 0.04013171046972275\n",
      "Epoch 20324/30000 Training Loss: 0.03573976829648018\n",
      "Epoch 20325/30000 Training Loss: 0.042545389384031296\n",
      "Epoch 20326/30000 Training Loss: 0.054020486772060394\n",
      "Epoch 20327/30000 Training Loss: 0.051957108080387115\n",
      "Epoch 20328/30000 Training Loss: 0.046461496502161026\n",
      "Epoch 20329/30000 Training Loss: 0.04528103768825531\n",
      "Epoch 20330/30000 Training Loss: 0.0565674789249897\n",
      "Epoch 20331/30000 Training Loss: 0.04869716241955757\n",
      "Epoch 20332/30000 Training Loss: 0.061498723924160004\n",
      "Epoch 20333/30000 Training Loss: 0.0529797226190567\n",
      "Epoch 20334/30000 Training Loss: 0.056811973452568054\n",
      "Epoch 20335/30000 Training Loss: 0.04660714045166969\n",
      "Epoch 20336/30000 Training Loss: 0.029753193259239197\n",
      "Epoch 20337/30000 Training Loss: 0.043648235499858856\n",
      "Epoch 20338/30000 Training Loss: 0.05030331388115883\n",
      "Epoch 20339/30000 Training Loss: 0.04286392033100128\n",
      "Epoch 20340/30000 Training Loss: 0.04564100503921509\n",
      "Epoch 20341/30000 Training Loss: 0.03540157899260521\n",
      "Epoch 20342/30000 Training Loss: 0.037438929080963135\n",
      "Epoch 20343/30000 Training Loss: 0.05219761282205582\n",
      "Epoch 20344/30000 Training Loss: 0.03606000170111656\n",
      "Epoch 20345/30000 Training Loss: 0.06487996131181717\n",
      "Epoch 20346/30000 Training Loss: 0.04099062830209732\n",
      "Epoch 20347/30000 Training Loss: 0.04063861072063446\n",
      "Epoch 20348/30000 Training Loss: 0.05148680880665779\n",
      "Epoch 20349/30000 Training Loss: 0.05212891101837158\n",
      "Epoch 20350/30000 Training Loss: 0.028225891292095184\n",
      "Epoch 20351/30000 Training Loss: 0.029711171984672546\n",
      "Epoch 20352/30000 Training Loss: 0.04552547261118889\n",
      "Epoch 20353/30000 Training Loss: 0.058490656316280365\n",
      "Epoch 20354/30000 Training Loss: 0.05685136467218399\n",
      "Epoch 20355/30000 Training Loss: 0.04841645434498787\n",
      "Epoch 20356/30000 Training Loss: 0.042554304003715515\n",
      "Epoch 20357/30000 Training Loss: 0.04371460899710655\n",
      "Epoch 20358/30000 Training Loss: 0.05506330728530884\n",
      "Epoch 20359/30000 Training Loss: 0.04148358106613159\n",
      "Epoch 20360/30000 Training Loss: 0.05231185257434845\n",
      "Epoch 20361/30000 Training Loss: 0.05999081954360008\n",
      "Epoch 20362/30000 Training Loss: 0.035742927342653275\n",
      "Epoch 20363/30000 Training Loss: 0.05141949653625488\n",
      "Epoch 20364/30000 Training Loss: 0.038813650608062744\n",
      "Epoch 20365/30000 Training Loss: 0.035031795501708984\n",
      "Epoch 20366/30000 Training Loss: 0.039007075130939484\n",
      "Epoch 20367/30000 Training Loss: 0.041124507784843445\n",
      "Epoch 20368/30000 Training Loss: 0.04034847021102905\n",
      "Epoch 20369/30000 Training Loss: 0.050407737493515015\n",
      "Epoch 20370/30000 Training Loss: 0.04653061181306839\n",
      "Epoch 20371/30000 Training Loss: 0.04689894616603851\n",
      "Epoch 20372/30000 Training Loss: 0.050755519419908524\n",
      "Epoch 20373/30000 Training Loss: 0.06292056292295456\n",
      "Epoch 20374/30000 Training Loss: 0.07748183608055115\n",
      "Epoch 20375/30000 Training Loss: 0.03986480459570885\n",
      "Epoch 20376/30000 Training Loss: 0.06554311513900757\n",
      "Epoch 20377/30000 Training Loss: 0.049135301262140274\n",
      "Epoch 20378/30000 Training Loss: 0.04917006194591522\n",
      "Epoch 20379/30000 Training Loss: 0.06335395574569702\n",
      "Epoch 20380/30000 Training Loss: 0.04225805401802063\n",
      "Epoch 20381/30000 Training Loss: 0.039622142910957336\n",
      "Epoch 20382/30000 Training Loss: 0.04397319257259369\n",
      "Epoch 20383/30000 Training Loss: 0.05047997087240219\n",
      "Epoch 20384/30000 Training Loss: 0.06212785840034485\n",
      "Epoch 20385/30000 Training Loss: 0.05273044854402542\n",
      "Epoch 20386/30000 Training Loss: 0.04331327974796295\n",
      "Epoch 20387/30000 Training Loss: 0.03463060036301613\n",
      "Epoch 20388/30000 Training Loss: 0.06300351023674011\n",
      "Epoch 20389/30000 Training Loss: 0.06262052804231644\n",
      "Epoch 20390/30000 Training Loss: 0.055483635514974594\n",
      "Epoch 20391/30000 Training Loss: 0.0572521910071373\n",
      "Epoch 20392/30000 Training Loss: 0.04319309443235397\n",
      "Epoch 20393/30000 Training Loss: 0.04782530665397644\n",
      "Epoch 20394/30000 Training Loss: 0.053764428943395615\n",
      "Epoch 20395/30000 Training Loss: 0.047631874680519104\n",
      "Epoch 20396/30000 Training Loss: 0.05080856382846832\n",
      "Epoch 20397/30000 Training Loss: 0.04548082500696182\n",
      "Epoch 20398/30000 Training Loss: 0.04754826799035072\n",
      "Epoch 20399/30000 Training Loss: 0.056357089430093765\n",
      "Epoch 20400/30000 Training Loss: 0.040293771773576736\n",
      "Epoch 20400/30000 Validation Loss: 0.06421823799610138\n",
      "Epoch 20401/30000 Training Loss: 0.045809872448444366\n",
      "Epoch 20402/30000 Training Loss: 0.04411831498146057\n",
      "Epoch 20403/30000 Training Loss: 0.03525126352906227\n",
      "Epoch 20404/30000 Training Loss: 0.04219880327582359\n",
      "Epoch 20405/30000 Training Loss: 0.06589815765619278\n",
      "Epoch 20406/30000 Training Loss: 0.06283856928348541\n",
      "Epoch 20407/30000 Training Loss: 0.042896389961242676\n",
      "Epoch 20408/30000 Training Loss: 0.038134243339300156\n",
      "Epoch 20409/30000 Training Loss: 0.050029005855321884\n",
      "Epoch 20410/30000 Training Loss: 0.04548933729529381\n",
      "Epoch 20411/30000 Training Loss: 0.05341556295752525\n",
      "Epoch 20412/30000 Training Loss: 0.04759391024708748\n",
      "Epoch 20413/30000 Training Loss: 0.04662393778562546\n",
      "Epoch 20414/30000 Training Loss: 0.045013390481472015\n",
      "Epoch 20415/30000 Training Loss: 0.0425543375313282\n",
      "Epoch 20416/30000 Training Loss: 0.05229106917977333\n",
      "Epoch 20417/30000 Training Loss: 0.05558136850595474\n",
      "Epoch 20418/30000 Training Loss: 0.03599634766578674\n",
      "Epoch 20419/30000 Training Loss: 0.04477773606777191\n",
      "Epoch 20420/30000 Training Loss: 0.05648653954267502\n",
      "Epoch 20421/30000 Training Loss: 0.04701590910553932\n",
      "Epoch 20422/30000 Training Loss: 0.03412620350718498\n",
      "Epoch 20423/30000 Training Loss: 0.04649162292480469\n",
      "Epoch 20424/30000 Training Loss: 0.0494794137775898\n",
      "Epoch 20425/30000 Training Loss: 0.04014420509338379\n",
      "Epoch 20426/30000 Training Loss: 0.042288511991500854\n",
      "Epoch 20427/30000 Training Loss: 0.03953029215335846\n",
      "Epoch 20428/30000 Training Loss: 0.05891901254653931\n",
      "Epoch 20429/30000 Training Loss: 0.04542319476604462\n",
      "Epoch 20430/30000 Training Loss: 0.04590803012251854\n",
      "Epoch 20431/30000 Training Loss: 0.04067881405353546\n",
      "Epoch 20432/30000 Training Loss: 0.030703015625476837\n",
      "Epoch 20433/30000 Training Loss: 0.05538928136229515\n",
      "Epoch 20434/30000 Training Loss: 0.052859656512737274\n",
      "Epoch 20435/30000 Training Loss: 0.04777950420975685\n",
      "Epoch 20436/30000 Training Loss: 0.03716374188661575\n",
      "Epoch 20437/30000 Training Loss: 0.04362853616476059\n",
      "Epoch 20438/30000 Training Loss: 0.03944966197013855\n",
      "Epoch 20439/30000 Training Loss: 0.044550277292728424\n",
      "Epoch 20440/30000 Training Loss: 0.0588240846991539\n",
      "Epoch 20441/30000 Training Loss: 0.04258754849433899\n",
      "Epoch 20442/30000 Training Loss: 0.0721123218536377\n",
      "Epoch 20443/30000 Training Loss: 0.04399485141038895\n",
      "Epoch 20444/30000 Training Loss: 0.054056983441114426\n",
      "Epoch 20445/30000 Training Loss: 0.045398157089948654\n",
      "Epoch 20446/30000 Training Loss: 0.06430432200431824\n",
      "Epoch 20447/30000 Training Loss: 0.049631159752607346\n",
      "Epoch 20448/30000 Training Loss: 0.03912520781159401\n",
      "Epoch 20449/30000 Training Loss: 0.042235009372234344\n",
      "Epoch 20450/30000 Training Loss: 0.05251668393611908\n",
      "Epoch 20451/30000 Training Loss: 0.04332105070352554\n",
      "Epoch 20452/30000 Training Loss: 0.0508044958114624\n",
      "Epoch 20453/30000 Training Loss: 0.06119798868894577\n",
      "Epoch 20454/30000 Training Loss: 0.04197821766138077\n",
      "Epoch 20455/30000 Training Loss: 0.039107076823711395\n",
      "Epoch 20456/30000 Training Loss: 0.04670718312263489\n",
      "Epoch 20457/30000 Training Loss: 0.04554424434900284\n",
      "Epoch 20458/30000 Training Loss: 0.04328346997499466\n",
      "Epoch 20459/30000 Training Loss: 0.033031146973371506\n",
      "Epoch 20460/30000 Training Loss: 0.05723796412348747\n",
      "Epoch 20461/30000 Training Loss: 0.044943917542696\n",
      "Epoch 20462/30000 Training Loss: 0.06049366295337677\n",
      "Epoch 20463/30000 Training Loss: 0.042227406054735184\n",
      "Epoch 20464/30000 Training Loss: 0.04247651621699333\n",
      "Epoch 20465/30000 Training Loss: 0.04675503075122833\n",
      "Epoch 20466/30000 Training Loss: 0.04517483711242676\n",
      "Epoch 20467/30000 Training Loss: 0.03504685312509537\n",
      "Epoch 20468/30000 Training Loss: 0.0471743568778038\n",
      "Epoch 20469/30000 Training Loss: 0.046404723078012466\n",
      "Epoch 20470/30000 Training Loss: 0.0474042072892189\n",
      "Epoch 20471/30000 Training Loss: 0.04139399528503418\n",
      "Epoch 20472/30000 Training Loss: 0.04717725142836571\n",
      "Epoch 20473/30000 Training Loss: 0.05775699391961098\n",
      "Epoch 20474/30000 Training Loss: 0.04438435658812523\n",
      "Epoch 20475/30000 Training Loss: 0.060744889080524445\n",
      "Epoch 20476/30000 Training Loss: 0.05113223195075989\n",
      "Epoch 20477/30000 Training Loss: 0.02922491915524006\n",
      "Epoch 20478/30000 Training Loss: 0.026916753500699997\n",
      "Epoch 20479/30000 Training Loss: 0.03536584600806236\n",
      "Epoch 20480/30000 Training Loss: 0.054018788039684296\n",
      "Epoch 20481/30000 Training Loss: 0.04793253168463707\n",
      "Epoch 20482/30000 Training Loss: 0.04165489971637726\n",
      "Epoch 20483/30000 Training Loss: 0.04997425526380539\n",
      "Epoch 20484/30000 Training Loss: 0.05015219375491142\n",
      "Epoch 20485/30000 Training Loss: 0.044051021337509155\n",
      "Epoch 20486/30000 Training Loss: 0.04237396642565727\n",
      "Epoch 20487/30000 Training Loss: 0.03779560700058937\n",
      "Epoch 20488/30000 Training Loss: 0.04046078771352768\n",
      "Epoch 20489/30000 Training Loss: 0.05439111590385437\n",
      "Epoch 20490/30000 Training Loss: 0.061856433749198914\n",
      "Epoch 20491/30000 Training Loss: 0.04844381660223007\n",
      "Epoch 20492/30000 Training Loss: 0.04356059059500694\n",
      "Epoch 20493/30000 Training Loss: 0.044303931295871735\n",
      "Epoch 20494/30000 Training Loss: 0.03952404484152794\n",
      "Epoch 20495/30000 Training Loss: 0.047910481691360474\n",
      "Epoch 20496/30000 Training Loss: 0.05449298024177551\n",
      "Epoch 20497/30000 Training Loss: 0.06879960000514984\n",
      "Epoch 20498/30000 Training Loss: 0.044461339712142944\n",
      "Epoch 20499/30000 Training Loss: 0.037921253591775894\n",
      "Epoch 20500/30000 Training Loss: 0.038750238716602325\n",
      "Epoch 20500/30000 Validation Loss: 0.06190665811300278\n",
      "Epoch 20501/30000 Training Loss: 0.0415433906018734\n",
      "Epoch 20502/30000 Training Loss: 0.03406137973070145\n",
      "Epoch 20503/30000 Training Loss: 0.048691101372241974\n",
      "Epoch 20504/30000 Training Loss: 0.04265781491994858\n",
      "Epoch 20505/30000 Training Loss: 0.03553198650479317\n",
      "Epoch 20506/30000 Training Loss: 0.07056410610675812\n",
      "Epoch 20507/30000 Training Loss: 0.05967048555612564\n",
      "Epoch 20508/30000 Training Loss: 0.06144169718027115\n",
      "Epoch 20509/30000 Training Loss: 0.0463496558368206\n",
      "Epoch 20510/30000 Training Loss: 0.05268318951129913\n",
      "Epoch 20511/30000 Training Loss: 0.04129128158092499\n",
      "Epoch 20512/30000 Training Loss: 0.05257626250386238\n",
      "Epoch 20513/30000 Training Loss: 0.0421128012239933\n",
      "Epoch 20514/30000 Training Loss: 0.049568623304367065\n",
      "Epoch 20515/30000 Training Loss: 0.043991126120090485\n",
      "Epoch 20516/30000 Training Loss: 0.054962366819381714\n",
      "Epoch 20517/30000 Training Loss: 0.05011590197682381\n",
      "Epoch 20518/30000 Training Loss: 0.045477550476789474\n",
      "Epoch 20519/30000 Training Loss: 0.042495932430028915\n",
      "Epoch 20520/30000 Training Loss: 0.04070272296667099\n",
      "Epoch 20521/30000 Training Loss: 0.06021894887089729\n",
      "Epoch 20522/30000 Training Loss: 0.061277925968170166\n",
      "Epoch 20523/30000 Training Loss: 0.035427406430244446\n",
      "Epoch 20524/30000 Training Loss: 0.052007950842380524\n",
      "Epoch 20525/30000 Training Loss: 0.04949274659156799\n",
      "Epoch 20526/30000 Training Loss: 0.045547910034656525\n",
      "Epoch 20527/30000 Training Loss: 0.05887587368488312\n",
      "Epoch 20528/30000 Training Loss: 0.053000856190919876\n",
      "Epoch 20529/30000 Training Loss: 0.03700140118598938\n",
      "Epoch 20530/30000 Training Loss: 0.05394795164465904\n",
      "Epoch 20531/30000 Training Loss: 0.03467202186584473\n",
      "Epoch 20532/30000 Training Loss: 0.05796234309673309\n",
      "Epoch 20533/30000 Training Loss: 0.037249885499477386\n",
      "Epoch 20534/30000 Training Loss: 0.05139335244894028\n",
      "Epoch 20535/30000 Training Loss: 0.05498562008142471\n",
      "Epoch 20536/30000 Training Loss: 0.04235836863517761\n",
      "Epoch 20537/30000 Training Loss: 0.06874676048755646\n",
      "Epoch 20538/30000 Training Loss: 0.03582421690225601\n",
      "Epoch 20539/30000 Training Loss: 0.0521252416074276\n",
      "Epoch 20540/30000 Training Loss: 0.0649636909365654\n",
      "Epoch 20541/30000 Training Loss: 0.04250147193670273\n",
      "Epoch 20542/30000 Training Loss: 0.03382914885878563\n",
      "Epoch 20543/30000 Training Loss: 0.03536728769540787\n",
      "Epoch 20544/30000 Training Loss: 0.0470685139298439\n",
      "Epoch 20545/30000 Training Loss: 0.05529392883181572\n",
      "Epoch 20546/30000 Training Loss: 0.0330769345164299\n",
      "Epoch 20547/30000 Training Loss: 0.049050167202949524\n",
      "Epoch 20548/30000 Training Loss: 0.03969641774892807\n",
      "Epoch 20549/30000 Training Loss: 0.048828523606061935\n",
      "Epoch 20550/30000 Training Loss: 0.034101441502571106\n",
      "Epoch 20551/30000 Training Loss: 0.049068108201026917\n",
      "Epoch 20552/30000 Training Loss: 0.04693882539868355\n",
      "Epoch 20553/30000 Training Loss: 0.04422485828399658\n",
      "Epoch 20554/30000 Training Loss: 0.040172647684812546\n",
      "Epoch 20555/30000 Training Loss: 0.04626375064253807\n",
      "Epoch 20556/30000 Training Loss: 0.049751315265893936\n",
      "Epoch 20557/30000 Training Loss: 0.033137597143650055\n",
      "Epoch 20558/30000 Training Loss: 0.04529248923063278\n",
      "Epoch 20559/30000 Training Loss: 0.04435361921787262\n",
      "Epoch 20560/30000 Training Loss: 0.05643749609589577\n",
      "Epoch 20561/30000 Training Loss: 0.050223104655742645\n",
      "Epoch 20562/30000 Training Loss: 0.04805741459131241\n",
      "Epoch 20563/30000 Training Loss: 0.04363533854484558\n",
      "Epoch 20564/30000 Training Loss: 0.04024473577737808\n",
      "Epoch 20565/30000 Training Loss: 0.0450163409113884\n",
      "Epoch 20566/30000 Training Loss: 0.03541368991136551\n",
      "Epoch 20567/30000 Training Loss: 0.05208072438836098\n",
      "Epoch 20568/30000 Training Loss: 0.05587456002831459\n",
      "Epoch 20569/30000 Training Loss: 0.0730324387550354\n",
      "Epoch 20570/30000 Training Loss: 0.04289858043193817\n",
      "Epoch 20571/30000 Training Loss: 0.060539186000823975\n",
      "Epoch 20572/30000 Training Loss: 0.04918242245912552\n",
      "Epoch 20573/30000 Training Loss: 0.04798407480120659\n",
      "Epoch 20574/30000 Training Loss: 0.05447472631931305\n",
      "Epoch 20575/30000 Training Loss: 0.042931683361530304\n",
      "Epoch 20576/30000 Training Loss: 0.03409769386053085\n",
      "Epoch 20577/30000 Training Loss: 0.041190896183252335\n",
      "Epoch 20578/30000 Training Loss: 0.03777121379971504\n",
      "Epoch 20579/30000 Training Loss: 0.0500076562166214\n",
      "Epoch 20580/30000 Training Loss: 0.030723731964826584\n",
      "Epoch 20581/30000 Training Loss: 0.03376273438334465\n",
      "Epoch 20582/30000 Training Loss: 0.05930771678686142\n",
      "Epoch 20583/30000 Training Loss: 0.047093115746974945\n",
      "Epoch 20584/30000 Training Loss: 0.039121419191360474\n",
      "Epoch 20585/30000 Training Loss: 0.04758934676647186\n",
      "Epoch 20586/30000 Training Loss: 0.051543619483709335\n",
      "Epoch 20587/30000 Training Loss: 0.06946828216314316\n",
      "Epoch 20588/30000 Training Loss: 0.03668125718832016\n",
      "Epoch 20589/30000 Training Loss: 0.05470293015241623\n",
      "Epoch 20590/30000 Training Loss: 0.06186138838529587\n",
      "Epoch 20591/30000 Training Loss: 0.04420233890414238\n",
      "Epoch 20592/30000 Training Loss: 0.04097045212984085\n",
      "Epoch 20593/30000 Training Loss: 0.05589178949594498\n",
      "Epoch 20594/30000 Training Loss: 0.03758161887526512\n",
      "Epoch 20595/30000 Training Loss: 0.044534146785736084\n",
      "Epoch 20596/30000 Training Loss: 0.04871658608317375\n",
      "Epoch 20597/30000 Training Loss: 0.05290091037750244\n",
      "Epoch 20598/30000 Training Loss: 0.04793727770447731\n",
      "Epoch 20599/30000 Training Loss: 0.056158050894737244\n",
      "Epoch 20600/30000 Training Loss: 0.05252234265208244\n",
      "Epoch 20600/30000 Validation Loss: 0.044668763875961304\n",
      "Epoch 20601/30000 Training Loss: 0.05889221280813217\n",
      "Epoch 20602/30000 Training Loss: 0.06155344471335411\n",
      "Epoch 20603/30000 Training Loss: 0.05307523533701897\n",
      "Epoch 20604/30000 Training Loss: 0.04659494012594223\n",
      "Epoch 20605/30000 Training Loss: 0.051048897206783295\n",
      "Epoch 20606/30000 Training Loss: 0.04444725438952446\n",
      "Epoch 20607/30000 Training Loss: 0.05578463524580002\n",
      "Epoch 20608/30000 Training Loss: 0.052353668957948685\n",
      "Epoch 20609/30000 Training Loss: 0.06419877707958221\n",
      "Epoch 20610/30000 Training Loss: 0.05200355499982834\n",
      "Epoch 20611/30000 Training Loss: 0.04506541043519974\n",
      "Epoch 20612/30000 Training Loss: 0.03412238135933876\n",
      "Epoch 20613/30000 Training Loss: 0.052684176713228226\n",
      "Epoch 20614/30000 Training Loss: 0.05742662027478218\n",
      "Epoch 20615/30000 Training Loss: 0.03706102445721626\n",
      "Epoch 20616/30000 Training Loss: 0.05931093171238899\n",
      "Epoch 20617/30000 Training Loss: 0.047646209597587585\n",
      "Epoch 20618/30000 Training Loss: 0.0454815998673439\n",
      "Epoch 20619/30000 Training Loss: 0.04762188717722893\n",
      "Epoch 20620/30000 Training Loss: 0.04013395309448242\n",
      "Epoch 20621/30000 Training Loss: 0.050584662705659866\n",
      "Epoch 20622/30000 Training Loss: 0.04242761805653572\n",
      "Epoch 20623/30000 Training Loss: 0.036695435643196106\n",
      "Epoch 20624/30000 Training Loss: 0.049330540001392365\n",
      "Epoch 20625/30000 Training Loss: 0.04496443644165993\n",
      "Epoch 20626/30000 Training Loss: 0.06360651552677155\n",
      "Epoch 20627/30000 Training Loss: 0.04191254824399948\n",
      "Epoch 20628/30000 Training Loss: 0.047826945781707764\n",
      "Epoch 20629/30000 Training Loss: 0.04456145316362381\n",
      "Epoch 20630/30000 Training Loss: 0.03772281855344772\n",
      "Epoch 20631/30000 Training Loss: 0.042174868285655975\n",
      "Epoch 20632/30000 Training Loss: 0.040582284331321716\n",
      "Epoch 20633/30000 Training Loss: 0.037984807044267654\n",
      "Epoch 20634/30000 Training Loss: 0.03897639364004135\n",
      "Epoch 20635/30000 Training Loss: 0.03466391935944557\n",
      "Epoch 20636/30000 Training Loss: 0.03324621543288231\n",
      "Epoch 20637/30000 Training Loss: 0.04511873051524162\n",
      "Epoch 20638/30000 Training Loss: 0.052527137100696564\n",
      "Epoch 20639/30000 Training Loss: 0.07023409754037857\n",
      "Epoch 20640/30000 Training Loss: 0.050019409507513046\n",
      "Epoch 20641/30000 Training Loss: 0.04734814912080765\n",
      "Epoch 20642/30000 Training Loss: 0.06158424913883209\n",
      "Epoch 20643/30000 Training Loss: 0.047512445598840714\n",
      "Epoch 20644/30000 Training Loss: 0.050056979060173035\n",
      "Epoch 20645/30000 Training Loss: 0.044533852487802505\n",
      "Epoch 20646/30000 Training Loss: 0.035737331956624985\n",
      "Epoch 20647/30000 Training Loss: 0.03173939883708954\n",
      "Epoch 20648/30000 Training Loss: 0.03554093465209007\n",
      "Epoch 20649/30000 Training Loss: 0.048665519803762436\n",
      "Epoch 20650/30000 Training Loss: 0.03058893419802189\n",
      "Epoch 20651/30000 Training Loss: 0.0405501164495945\n",
      "Epoch 20652/30000 Training Loss: 0.039270441979169846\n",
      "Epoch 20653/30000 Training Loss: 0.03424793481826782\n",
      "Epoch 20654/30000 Training Loss: 0.05311838537454605\n",
      "Epoch 20655/30000 Training Loss: 0.03590960428118706\n",
      "Epoch 20656/30000 Training Loss: 0.040423233062028885\n",
      "Epoch 20657/30000 Training Loss: 0.029383022338151932\n",
      "Epoch 20658/30000 Training Loss: 0.03299890458583832\n",
      "Epoch 20659/30000 Training Loss: 0.040328338742256165\n",
      "Epoch 20660/30000 Training Loss: 0.04264112561941147\n",
      "Epoch 20661/30000 Training Loss: 0.04981600493192673\n",
      "Epoch 20662/30000 Training Loss: 0.03145529329776764\n",
      "Epoch 20663/30000 Training Loss: 0.049663759768009186\n",
      "Epoch 20664/30000 Training Loss: 0.03221924602985382\n",
      "Epoch 20665/30000 Training Loss: 0.052550479769706726\n",
      "Epoch 20666/30000 Training Loss: 0.04480496421456337\n",
      "Epoch 20667/30000 Training Loss: 0.06101129949092865\n",
      "Epoch 20668/30000 Training Loss: 0.04980643466114998\n",
      "Epoch 20669/30000 Training Loss: 0.04577411338686943\n",
      "Epoch 20670/30000 Training Loss: 0.03669458627700806\n",
      "Epoch 20671/30000 Training Loss: 0.0597880557179451\n",
      "Epoch 20672/30000 Training Loss: 0.04008672013878822\n",
      "Epoch 20673/30000 Training Loss: 0.04653877019882202\n",
      "Epoch 20674/30000 Training Loss: 0.042645394802093506\n",
      "Epoch 20675/30000 Training Loss: 0.048226408660411835\n",
      "Epoch 20676/30000 Training Loss: 0.06533315032720566\n",
      "Epoch 20677/30000 Training Loss: 0.051485322415828705\n",
      "Epoch 20678/30000 Training Loss: 0.04171210527420044\n",
      "Epoch 20679/30000 Training Loss: 0.04370841011404991\n",
      "Epoch 20680/30000 Training Loss: 0.059884537011384964\n",
      "Epoch 20681/30000 Training Loss: 0.053756386041641235\n",
      "Epoch 20682/30000 Training Loss: 0.04876108095049858\n",
      "Epoch 20683/30000 Training Loss: 0.044417694211006165\n",
      "Epoch 20684/30000 Training Loss: 0.037799734622240067\n",
      "Epoch 20685/30000 Training Loss: 0.05200269818305969\n",
      "Epoch 20686/30000 Training Loss: 0.04915647953748703\n",
      "Epoch 20687/30000 Training Loss: 0.045415621250867844\n",
      "Epoch 20688/30000 Training Loss: 0.043714020401239395\n",
      "Epoch 20689/30000 Training Loss: 0.03707636147737503\n",
      "Epoch 20690/30000 Training Loss: 0.03268706053495407\n",
      "Epoch 20691/30000 Training Loss: 0.042655229568481445\n",
      "Epoch 20692/30000 Training Loss: 0.03627564013004303\n",
      "Epoch 20693/30000 Training Loss: 0.045715078711509705\n",
      "Epoch 20694/30000 Training Loss: 0.04350923001766205\n",
      "Epoch 20695/30000 Training Loss: 0.05500778928399086\n",
      "Epoch 20696/30000 Training Loss: 0.05737187713384628\n",
      "Epoch 20697/30000 Training Loss: 0.045385997742414474\n",
      "Epoch 20698/30000 Training Loss: 0.03459605574607849\n",
      "Epoch 20699/30000 Training Loss: 0.045013800263404846\n",
      "Epoch 20700/30000 Training Loss: 0.034826505929231644\n",
      "Epoch 20700/30000 Validation Loss: 0.046925321221351624\n",
      "Epoch 20701/30000 Training Loss: 0.048646051436662674\n",
      "Epoch 20702/30000 Training Loss: 0.04105469956994057\n",
      "Epoch 20703/30000 Training Loss: 0.04175519943237305\n",
      "Epoch 20704/30000 Training Loss: 0.05410728603601456\n",
      "Epoch 20705/30000 Training Loss: 0.0488874688744545\n",
      "Epoch 20706/30000 Training Loss: 0.04582308977842331\n",
      "Epoch 20707/30000 Training Loss: 0.04505247622728348\n",
      "Epoch 20708/30000 Training Loss: 0.04730583727359772\n",
      "Epoch 20709/30000 Training Loss: 0.04029151052236557\n",
      "Epoch 20710/30000 Training Loss: 0.05146925896406174\n",
      "Epoch 20711/30000 Training Loss: 0.057689517736434937\n",
      "Epoch 20712/30000 Training Loss: 0.05758335068821907\n",
      "Epoch 20713/30000 Training Loss: 0.04918500408530235\n",
      "Epoch 20714/30000 Training Loss: 0.04995831102132797\n",
      "Epoch 20715/30000 Training Loss: 0.044392772018909454\n",
      "Epoch 20716/30000 Training Loss: 0.049847714602947235\n",
      "Epoch 20717/30000 Training Loss: 0.04259070008993149\n",
      "Epoch 20718/30000 Training Loss: 0.04776158928871155\n",
      "Epoch 20719/30000 Training Loss: 0.03380557522177696\n",
      "Epoch 20720/30000 Training Loss: 0.04224104806780815\n",
      "Epoch 20721/30000 Training Loss: 0.061131782829761505\n",
      "Epoch 20722/30000 Training Loss: 0.040964771062135696\n",
      "Epoch 20723/30000 Training Loss: 0.04392276331782341\n",
      "Epoch 20724/30000 Training Loss: 0.06411571800708771\n",
      "Epoch 20725/30000 Training Loss: 0.04497827962040901\n",
      "Epoch 20726/30000 Training Loss: 0.044610653072595596\n",
      "Epoch 20727/30000 Training Loss: 0.04353976249694824\n",
      "Epoch 20728/30000 Training Loss: 0.0566067099571228\n",
      "Epoch 20729/30000 Training Loss: 0.046432480216026306\n",
      "Epoch 20730/30000 Training Loss: 0.056745339184999466\n",
      "Epoch 20731/30000 Training Loss: 0.04555923119187355\n",
      "Epoch 20732/30000 Training Loss: 0.036548957228660583\n",
      "Epoch 20733/30000 Training Loss: 0.0520375519990921\n",
      "Epoch 20734/30000 Training Loss: 0.05766324698925018\n",
      "Epoch 20735/30000 Training Loss: 0.05677814036607742\n",
      "Epoch 20736/30000 Training Loss: 0.06606487929821014\n",
      "Epoch 20737/30000 Training Loss: 0.037752363830804825\n",
      "Epoch 20738/30000 Training Loss: 0.05669434368610382\n",
      "Epoch 20739/30000 Training Loss: 0.040570445358753204\n",
      "Epoch 20740/30000 Training Loss: 0.04813903570175171\n",
      "Epoch 20741/30000 Training Loss: 0.04669180512428284\n",
      "Epoch 20742/30000 Training Loss: 0.05498507618904114\n",
      "Epoch 20743/30000 Training Loss: 0.054192617535591125\n",
      "Epoch 20744/30000 Training Loss: 0.047401927411556244\n",
      "Epoch 20745/30000 Training Loss: 0.06200149655342102\n",
      "Epoch 20746/30000 Training Loss: 0.05961361899971962\n",
      "Epoch 20747/30000 Training Loss: 0.0366407185792923\n",
      "Epoch 20748/30000 Training Loss: 0.06760845333337784\n",
      "Epoch 20749/30000 Training Loss: 0.04130901023745537\n",
      "Epoch 20750/30000 Training Loss: 0.0693301409482956\n",
      "Epoch 20751/30000 Training Loss: 0.04706675559282303\n",
      "Epoch 20752/30000 Training Loss: 0.048577167093753815\n",
      "Epoch 20753/30000 Training Loss: 0.06331242620944977\n",
      "Epoch 20754/30000 Training Loss: 0.040498390793800354\n",
      "Epoch 20755/30000 Training Loss: 0.04234650731086731\n",
      "Epoch 20756/30000 Training Loss: 0.03896870091557503\n",
      "Epoch 20757/30000 Training Loss: 0.049284033477306366\n",
      "Epoch 20758/30000 Training Loss: 0.04647758603096008\n",
      "Epoch 20759/30000 Training Loss: 0.0525921955704689\n",
      "Epoch 20760/30000 Training Loss: 0.04250357300043106\n",
      "Epoch 20761/30000 Training Loss: 0.050524137914180756\n",
      "Epoch 20762/30000 Training Loss: 0.038822241127491\n",
      "Epoch 20763/30000 Training Loss: 0.042439643293619156\n",
      "Epoch 20764/30000 Training Loss: 0.0427187941968441\n",
      "Epoch 20765/30000 Training Loss: 0.0450047068297863\n",
      "Epoch 20766/30000 Training Loss: 0.04503314197063446\n",
      "Epoch 20767/30000 Training Loss: 0.04790695384144783\n",
      "Epoch 20768/30000 Training Loss: 0.05552917346358299\n",
      "Epoch 20769/30000 Training Loss: 0.05049914866685867\n",
      "Epoch 20770/30000 Training Loss: 0.040425289422273636\n",
      "Epoch 20771/30000 Training Loss: 0.048527102917432785\n",
      "Epoch 20772/30000 Training Loss: 0.04875947907567024\n",
      "Epoch 20773/30000 Training Loss: 0.07078012824058533\n",
      "Epoch 20774/30000 Training Loss: 0.0480625219643116\n",
      "Epoch 20775/30000 Training Loss: 0.04738539457321167\n",
      "Epoch 20776/30000 Training Loss: 0.03282292187213898\n",
      "Epoch 20777/30000 Training Loss: 0.03204987943172455\n",
      "Epoch 20778/30000 Training Loss: 0.0430331751704216\n",
      "Epoch 20779/30000 Training Loss: 0.048531465232372284\n",
      "Epoch 20780/30000 Training Loss: 0.05100150406360626\n",
      "Epoch 20781/30000 Training Loss: 0.05771884322166443\n",
      "Epoch 20782/30000 Training Loss: 0.046976733952760696\n",
      "Epoch 20783/30000 Training Loss: 0.03854992240667343\n",
      "Epoch 20784/30000 Training Loss: 0.046159058809280396\n",
      "Epoch 20785/30000 Training Loss: 0.035133056342601776\n",
      "Epoch 20786/30000 Training Loss: 0.03924856707453728\n",
      "Epoch 20787/30000 Training Loss: 0.034127622842788696\n",
      "Epoch 20788/30000 Training Loss: 0.0414046049118042\n",
      "Epoch 20789/30000 Training Loss: 0.04681190103292465\n",
      "Epoch 20790/30000 Training Loss: 0.03511061519384384\n",
      "Epoch 20791/30000 Training Loss: 0.04977653548121452\n",
      "Epoch 20792/30000 Training Loss: 0.060526590794324875\n",
      "Epoch 20793/30000 Training Loss: 0.03238186985254288\n",
      "Epoch 20794/30000 Training Loss: 0.061899952590465546\n",
      "Epoch 20795/30000 Training Loss: 0.05519222840666771\n",
      "Epoch 20796/30000 Training Loss: 0.03823067992925644\n",
      "Epoch 20797/30000 Training Loss: 0.05304703116416931\n",
      "Epoch 20798/30000 Training Loss: 0.03826151043176651\n",
      "Epoch 20799/30000 Training Loss: 0.046032000333070755\n",
      "Epoch 20800/30000 Training Loss: 0.03292207419872284\n",
      "Epoch 20800/30000 Validation Loss: 0.04590652883052826\n",
      "Epoch 20801/30000 Training Loss: 0.05988592281937599\n",
      "Epoch 20802/30000 Training Loss: 0.056277427822351456\n",
      "Epoch 20803/30000 Training Loss: 0.04326442629098892\n",
      "Epoch 20804/30000 Training Loss: 0.06093311309814453\n",
      "Epoch 20805/30000 Training Loss: 0.038353923708200455\n",
      "Epoch 20806/30000 Training Loss: 0.03880138695240021\n",
      "Epoch 20807/30000 Training Loss: 0.04031854867935181\n",
      "Epoch 20808/30000 Training Loss: 0.04225321114063263\n",
      "Epoch 20809/30000 Training Loss: 0.0463704951107502\n",
      "Epoch 20810/30000 Training Loss: 0.04386765509843826\n",
      "Epoch 20811/30000 Training Loss: 0.0479227676987648\n",
      "Epoch 20812/30000 Training Loss: 0.04431317374110222\n",
      "Epoch 20813/30000 Training Loss: 0.03945401310920715\n",
      "Epoch 20814/30000 Training Loss: 0.05547361820936203\n",
      "Epoch 20815/30000 Training Loss: 0.047214146703481674\n",
      "Epoch 20816/30000 Training Loss: 0.04668090492486954\n",
      "Epoch 20817/30000 Training Loss: 0.043345194309949875\n",
      "Epoch 20818/30000 Training Loss: 0.04089775308966637\n",
      "Epoch 20819/30000 Training Loss: 0.03688127174973488\n",
      "Epoch 20820/30000 Training Loss: 0.046610716730356216\n",
      "Epoch 20821/30000 Training Loss: 0.03541164845228195\n",
      "Epoch 20822/30000 Training Loss: 0.05630991607904434\n",
      "Epoch 20823/30000 Training Loss: 0.04757367819547653\n",
      "Epoch 20824/30000 Training Loss: 0.04620404541492462\n",
      "Epoch 20825/30000 Training Loss: 0.0545165091753006\n",
      "Epoch 20826/30000 Training Loss: 0.041427597403526306\n",
      "Epoch 20827/30000 Training Loss: 0.044639986008405685\n",
      "Epoch 20828/30000 Training Loss: 0.038730233907699585\n",
      "Epoch 20829/30000 Training Loss: 0.049355052411556244\n",
      "Epoch 20830/30000 Training Loss: 0.03677825629711151\n",
      "Epoch 20831/30000 Training Loss: 0.0477321520447731\n",
      "Epoch 20832/30000 Training Loss: 0.05344609543681145\n",
      "Epoch 20833/30000 Training Loss: 0.06743733584880829\n",
      "Epoch 20834/30000 Training Loss: 0.04748465493321419\n",
      "Epoch 20835/30000 Training Loss: 0.06018456071615219\n",
      "Epoch 20836/30000 Training Loss: 0.06385929137468338\n",
      "Epoch 20837/30000 Training Loss: 0.046026039868593216\n",
      "Epoch 20838/30000 Training Loss: 0.043827906250953674\n",
      "Epoch 20839/30000 Training Loss: 0.03391975909471512\n",
      "Epoch 20840/30000 Training Loss: 0.041877858340740204\n",
      "Epoch 20841/30000 Training Loss: 0.03997669368982315\n",
      "Epoch 20842/30000 Training Loss: 0.052550677210092545\n",
      "Epoch 20843/30000 Training Loss: 0.04537446051836014\n",
      "Epoch 20844/30000 Training Loss: 0.07637055218219757\n",
      "Epoch 20845/30000 Training Loss: 0.05149450898170471\n",
      "Epoch 20846/30000 Training Loss: 0.044086527079343796\n",
      "Epoch 20847/30000 Training Loss: 0.04226742312312126\n",
      "Epoch 20848/30000 Training Loss: 0.052960820496082306\n",
      "Epoch 20849/30000 Training Loss: 0.05531884729862213\n",
      "Epoch 20850/30000 Training Loss: 0.04037880152463913\n",
      "Epoch 20851/30000 Training Loss: 0.04279811307787895\n",
      "Epoch 20852/30000 Training Loss: 0.0441262312233448\n",
      "Epoch 20853/30000 Training Loss: 0.05627692863345146\n",
      "Epoch 20854/30000 Training Loss: 0.045303523540496826\n",
      "Epoch 20855/30000 Training Loss: 0.04429524764418602\n",
      "Epoch 20856/30000 Training Loss: 0.04311918839812279\n",
      "Epoch 20857/30000 Training Loss: 0.04847951605916023\n",
      "Epoch 20858/30000 Training Loss: 0.044618770480155945\n",
      "Epoch 20859/30000 Training Loss: 0.04634317755699158\n",
      "Epoch 20860/30000 Training Loss: 0.05591589957475662\n",
      "Epoch 20861/30000 Training Loss: 0.047868065536022186\n",
      "Epoch 20862/30000 Training Loss: 0.0632883757352829\n",
      "Epoch 20863/30000 Training Loss: 0.03552931919693947\n",
      "Epoch 20864/30000 Training Loss: 0.03925863280892372\n",
      "Epoch 20865/30000 Training Loss: 0.04830067604780197\n",
      "Epoch 20866/30000 Training Loss: 0.05953887104988098\n",
      "Epoch 20867/30000 Training Loss: 0.038824282586574554\n",
      "Epoch 20868/30000 Training Loss: 0.0627145990729332\n",
      "Epoch 20869/30000 Training Loss: 0.06545864045619965\n",
      "Epoch 20870/30000 Training Loss: 0.047385722398757935\n",
      "Epoch 20871/30000 Training Loss: 0.04334167018532753\n",
      "Epoch 20872/30000 Training Loss: 0.054536767303943634\n",
      "Epoch 20873/30000 Training Loss: 0.041598983108997345\n",
      "Epoch 20874/30000 Training Loss: 0.04746255278587341\n",
      "Epoch 20875/30000 Training Loss: 0.03365371376276016\n",
      "Epoch 20876/30000 Training Loss: 0.05202723294496536\n",
      "Epoch 20877/30000 Training Loss: 0.06751875579357147\n",
      "Epoch 20878/30000 Training Loss: 0.04525629058480263\n",
      "Epoch 20879/30000 Training Loss: 0.05087865889072418\n",
      "Epoch 20880/30000 Training Loss: 0.04125872254371643\n",
      "Epoch 20881/30000 Training Loss: 0.06169510632753372\n",
      "Epoch 20882/30000 Training Loss: 0.03676605969667435\n",
      "Epoch 20883/30000 Training Loss: 0.053931985050439835\n",
      "Epoch 20884/30000 Training Loss: 0.04983605444431305\n",
      "Epoch 20885/30000 Training Loss: 0.05373464524745941\n",
      "Epoch 20886/30000 Training Loss: 0.03411448746919632\n",
      "Epoch 20887/30000 Training Loss: 0.03540487587451935\n",
      "Epoch 20888/30000 Training Loss: 0.05975080281496048\n",
      "Epoch 20889/30000 Training Loss: 0.04323289543390274\n",
      "Epoch 20890/30000 Training Loss: 0.03826327994465828\n",
      "Epoch 20891/30000 Training Loss: 0.03557194769382477\n",
      "Epoch 20892/30000 Training Loss: 0.03825049102306366\n",
      "Epoch 20893/30000 Training Loss: 0.05507322773337364\n",
      "Epoch 20894/30000 Training Loss: 0.03521377593278885\n",
      "Epoch 20895/30000 Training Loss: 0.04428911581635475\n",
      "Epoch 20896/30000 Training Loss: 0.035088881850242615\n",
      "Epoch 20897/30000 Training Loss: 0.05271560698747635\n",
      "Epoch 20898/30000 Training Loss: 0.06603329628705978\n",
      "Epoch 20899/30000 Training Loss: 0.034988000988960266\n",
      "Epoch 20900/30000 Training Loss: 0.05820374935865402\n",
      "Epoch 20900/30000 Validation Loss: 0.049617089331150055\n",
      "Epoch 20901/30000 Training Loss: 0.047762542963027954\n",
      "Epoch 20902/30000 Training Loss: 0.05084356665611267\n",
      "Epoch 20903/30000 Training Loss: 0.048163674771785736\n",
      "Epoch 20904/30000 Training Loss: 0.04840012639760971\n",
      "Epoch 20905/30000 Training Loss: 0.048664845526218414\n",
      "Epoch 20906/30000 Training Loss: 0.057738497853279114\n",
      "Epoch 20907/30000 Training Loss: 0.04673127457499504\n",
      "Epoch 20908/30000 Training Loss: 0.04057198390364647\n",
      "Epoch 20909/30000 Training Loss: 0.05078788846731186\n",
      "Epoch 20910/30000 Training Loss: 0.05489449203014374\n",
      "Epoch 20911/30000 Training Loss: 0.03530557453632355\n",
      "Epoch 20912/30000 Training Loss: 0.042478617280721664\n",
      "Epoch 20913/30000 Training Loss: 0.060061052441596985\n",
      "Epoch 20914/30000 Training Loss: 0.04012244939804077\n",
      "Epoch 20915/30000 Training Loss: 0.03706490248441696\n",
      "Epoch 20916/30000 Training Loss: 0.0413338728249073\n",
      "Epoch 20917/30000 Training Loss: 0.061144404113292694\n",
      "Epoch 20918/30000 Training Loss: 0.061485230922698975\n",
      "Epoch 20919/30000 Training Loss: 0.0646531730890274\n",
      "Epoch 20920/30000 Training Loss: 0.04968857020139694\n",
      "Epoch 20921/30000 Training Loss: 0.05326467379927635\n",
      "Epoch 20922/30000 Training Loss: 0.042218808084726334\n",
      "Epoch 20923/30000 Training Loss: 0.04216714948415756\n",
      "Epoch 20924/30000 Training Loss: 0.05061974376440048\n",
      "Epoch 20925/30000 Training Loss: 0.03436601907014847\n",
      "Epoch 20926/30000 Training Loss: 0.05470391362905502\n",
      "Epoch 20927/30000 Training Loss: 0.042497240006923676\n",
      "Epoch 20928/30000 Training Loss: 0.04967605322599411\n",
      "Epoch 20929/30000 Training Loss: 0.051414426416158676\n",
      "Epoch 20930/30000 Training Loss: 0.04828617721796036\n",
      "Epoch 20931/30000 Training Loss: 0.03860017657279968\n",
      "Epoch 20932/30000 Training Loss: 0.03550179675221443\n",
      "Epoch 20933/30000 Training Loss: 0.04519149661064148\n",
      "Epoch 20934/30000 Training Loss: 0.03490177169442177\n",
      "Epoch 20935/30000 Training Loss: 0.045491382479667664\n",
      "Epoch 20936/30000 Training Loss: 0.03131046146154404\n",
      "Epoch 20937/30000 Training Loss: 0.05727091059088707\n",
      "Epoch 20938/30000 Training Loss: 0.0541343130171299\n",
      "Epoch 20939/30000 Training Loss: 0.035042356699705124\n",
      "Epoch 20940/30000 Training Loss: 0.05771970748901367\n",
      "Epoch 20941/30000 Training Loss: 0.043201200664043427\n",
      "Epoch 20942/30000 Training Loss: 0.05991451442241669\n",
      "Epoch 20943/30000 Training Loss: 0.05587936192750931\n",
      "Epoch 20944/30000 Training Loss: 0.04123524948954582\n",
      "Epoch 20945/30000 Training Loss: 0.04131407290697098\n",
      "Epoch 20946/30000 Training Loss: 0.05683501809835434\n",
      "Epoch 20947/30000 Training Loss: 0.03956335783004761\n",
      "Epoch 20948/30000 Training Loss: 0.047491416335105896\n",
      "Epoch 20949/30000 Training Loss: 0.038572899997234344\n",
      "Epoch 20950/30000 Training Loss: 0.04143691062927246\n",
      "Epoch 20951/30000 Training Loss: 0.05992511659860611\n",
      "Epoch 20952/30000 Training Loss: 0.040958479046821594\n",
      "Epoch 20953/30000 Training Loss: 0.054549816995859146\n",
      "Epoch 20954/30000 Training Loss: 0.038026660680770874\n",
      "Epoch 20955/30000 Training Loss: 0.03255285322666168\n",
      "Epoch 20956/30000 Training Loss: 0.05246196314692497\n",
      "Epoch 20957/30000 Training Loss: 0.06923041492700577\n",
      "Epoch 20958/30000 Training Loss: 0.04820834472775459\n",
      "Epoch 20959/30000 Training Loss: 0.039430052042007446\n",
      "Epoch 20960/30000 Training Loss: 0.057594627141952515\n",
      "Epoch 20961/30000 Training Loss: 0.060289423912763596\n",
      "Epoch 20962/30000 Training Loss: 0.03946968913078308\n",
      "Epoch 20963/30000 Training Loss: 0.05466747283935547\n",
      "Epoch 20964/30000 Training Loss: 0.055439479649066925\n",
      "Epoch 20965/30000 Training Loss: 0.0538269579410553\n",
      "Epoch 20966/30000 Training Loss: 0.041633084416389465\n",
      "Epoch 20967/30000 Training Loss: 0.03294454142451286\n",
      "Epoch 20968/30000 Training Loss: 0.04364044591784477\n",
      "Epoch 20969/30000 Training Loss: 0.054668866097927094\n",
      "Epoch 20970/30000 Training Loss: 0.045625027269124985\n",
      "Epoch 20971/30000 Training Loss: 0.04725760966539383\n",
      "Epoch 20972/30000 Training Loss: 0.05317866802215576\n",
      "Epoch 20973/30000 Training Loss: 0.03945198655128479\n",
      "Epoch 20974/30000 Training Loss: 0.059826962649822235\n",
      "Epoch 20975/30000 Training Loss: 0.04843837022781372\n",
      "Epoch 20976/30000 Training Loss: 0.04943311586976051\n",
      "Epoch 20977/30000 Training Loss: 0.04020554572343826\n",
      "Epoch 20978/30000 Training Loss: 0.061281751841306686\n",
      "Epoch 20979/30000 Training Loss: 0.05492786690592766\n",
      "Epoch 20980/30000 Training Loss: 0.04967699199914932\n",
      "Epoch 20981/30000 Training Loss: 0.06846827268600464\n",
      "Epoch 20982/30000 Training Loss: 0.03897556662559509\n",
      "Epoch 20983/30000 Training Loss: 0.03400011360645294\n",
      "Epoch 20984/30000 Training Loss: 0.05889109522104263\n",
      "Epoch 20985/30000 Training Loss: 0.045326702296733856\n",
      "Epoch 20986/30000 Training Loss: 0.04527657479047775\n",
      "Epoch 20987/30000 Training Loss: 0.04519215226173401\n",
      "Epoch 20988/30000 Training Loss: 0.05700580030679703\n",
      "Epoch 20989/30000 Training Loss: 0.04579520970582962\n",
      "Epoch 20990/30000 Training Loss: 0.04108061641454697\n",
      "Epoch 20991/30000 Training Loss: 0.0391882099211216\n",
      "Epoch 20992/30000 Training Loss: 0.0437292754650116\n",
      "Epoch 20993/30000 Training Loss: 0.05563972890377045\n",
      "Epoch 20994/30000 Training Loss: 0.030812276527285576\n",
      "Epoch 20995/30000 Training Loss: 0.04553086683154106\n",
      "Epoch 20996/30000 Training Loss: 0.056908875703811646\n",
      "Epoch 20997/30000 Training Loss: 0.06862786412239075\n",
      "Epoch 20998/30000 Training Loss: 0.05564561486244202\n",
      "Epoch 20999/30000 Training Loss: 0.05326797440648079\n",
      "Epoch 21000/30000 Training Loss: 0.05022460222244263\n",
      "Epoch 21000/30000 Validation Loss: 0.03704867139458656\n",
      "Epoch 21001/30000 Training Loss: 0.04217470437288284\n",
      "Epoch 21002/30000 Training Loss: 0.03959629684686661\n",
      "Epoch 21003/30000 Training Loss: 0.053083278238773346\n",
      "Epoch 21004/30000 Training Loss: 0.05632035434246063\n",
      "Epoch 21005/30000 Training Loss: 0.049985140562057495\n",
      "Epoch 21006/30000 Training Loss: 0.03434743732213974\n",
      "Epoch 21007/30000 Training Loss: 0.062033507972955704\n",
      "Epoch 21008/30000 Training Loss: 0.04586879909038544\n",
      "Epoch 21009/30000 Training Loss: 0.043875034898519516\n",
      "Epoch 21010/30000 Training Loss: 0.05304795503616333\n",
      "Epoch 21011/30000 Training Loss: 0.05723695084452629\n",
      "Epoch 21012/30000 Training Loss: 0.04806897044181824\n",
      "Epoch 21013/30000 Training Loss: 0.05687372758984566\n",
      "Epoch 21014/30000 Training Loss: 0.04729776829481125\n",
      "Epoch 21015/30000 Training Loss: 0.056036826223134995\n",
      "Epoch 21016/30000 Training Loss: 0.046712394803762436\n",
      "Epoch 21017/30000 Training Loss: 0.04594135284423828\n",
      "Epoch 21018/30000 Training Loss: 0.05851244926452637\n",
      "Epoch 21019/30000 Training Loss: 0.04287036508321762\n",
      "Epoch 21020/30000 Training Loss: 0.03793927654623985\n",
      "Epoch 21021/30000 Training Loss: 0.04634254053235054\n",
      "Epoch 21022/30000 Training Loss: 0.03514060378074646\n",
      "Epoch 21023/30000 Training Loss: 0.0438881516456604\n",
      "Epoch 21024/30000 Training Loss: 0.0358424186706543\n",
      "Epoch 21025/30000 Training Loss: 0.04735281318426132\n",
      "Epoch 21026/30000 Training Loss: 0.04517844319343567\n",
      "Epoch 21027/30000 Training Loss: 0.04969211667776108\n",
      "Epoch 21028/30000 Training Loss: 0.03693152219057083\n",
      "Epoch 21029/30000 Training Loss: 0.06005179136991501\n",
      "Epoch 21030/30000 Training Loss: 0.05242191255092621\n",
      "Epoch 21031/30000 Training Loss: 0.04535454511642456\n",
      "Epoch 21032/30000 Training Loss: 0.04289674386382103\n",
      "Epoch 21033/30000 Training Loss: 0.04978516697883606\n",
      "Epoch 21034/30000 Training Loss: 0.055289603769779205\n",
      "Epoch 21035/30000 Training Loss: 0.03802937641739845\n",
      "Epoch 21036/30000 Training Loss: 0.05153927206993103\n",
      "Epoch 21037/30000 Training Loss: 0.04167341813445091\n",
      "Epoch 21038/30000 Training Loss: 0.05734172835946083\n",
      "Epoch 21039/30000 Training Loss: 0.04614933580160141\n",
      "Epoch 21040/30000 Training Loss: 0.03621245175600052\n",
      "Epoch 21041/30000 Training Loss: 0.048977866768836975\n",
      "Epoch 21042/30000 Training Loss: 0.03830254077911377\n",
      "Epoch 21043/30000 Training Loss: 0.031190920621156693\n",
      "Epoch 21044/30000 Training Loss: 0.06049186363816261\n",
      "Epoch 21045/30000 Training Loss: 0.05574427545070648\n",
      "Epoch 21046/30000 Training Loss: 0.028862208127975464\n",
      "Epoch 21047/30000 Training Loss: 0.033514123409986496\n",
      "Epoch 21048/30000 Training Loss: 0.04473671317100525\n",
      "Epoch 21049/30000 Training Loss: 0.04569459706544876\n",
      "Epoch 21050/30000 Training Loss: 0.03579292446374893\n",
      "Epoch 21051/30000 Training Loss: 0.07146859169006348\n",
      "Epoch 21052/30000 Training Loss: 0.03613895922899246\n",
      "Epoch 21053/30000 Training Loss: 0.03402026742696762\n",
      "Epoch 21054/30000 Training Loss: 0.03435301035642624\n",
      "Epoch 21055/30000 Training Loss: 0.058636728674173355\n",
      "Epoch 21056/30000 Training Loss: 0.04521539807319641\n",
      "Epoch 21057/30000 Training Loss: 0.04324141517281532\n",
      "Epoch 21058/30000 Training Loss: 0.05406946688890457\n",
      "Epoch 21059/30000 Training Loss: 0.056150756776332855\n",
      "Epoch 21060/30000 Training Loss: 0.03374754637479782\n",
      "Epoch 21061/30000 Training Loss: 0.056762583553791046\n",
      "Epoch 21062/30000 Training Loss: 0.04393824562430382\n",
      "Epoch 21063/30000 Training Loss: 0.03866627439856529\n",
      "Epoch 21064/30000 Training Loss: 0.04400625824928284\n",
      "Epoch 21065/30000 Training Loss: 0.07292376458644867\n",
      "Epoch 21066/30000 Training Loss: 0.04965679347515106\n",
      "Epoch 21067/30000 Training Loss: 0.04013843089342117\n",
      "Epoch 21068/30000 Training Loss: 0.06327299773693085\n",
      "Epoch 21069/30000 Training Loss: 0.03489534184336662\n",
      "Epoch 21070/30000 Training Loss: 0.047489993274211884\n",
      "Epoch 21071/30000 Training Loss: 0.05692450702190399\n",
      "Epoch 21072/30000 Training Loss: 0.048692792654037476\n",
      "Epoch 21073/30000 Training Loss: 0.0372181162238121\n",
      "Epoch 21074/30000 Training Loss: 0.05075199156999588\n",
      "Epoch 21075/30000 Training Loss: 0.04103374853730202\n",
      "Epoch 21076/30000 Training Loss: 0.03600981831550598\n",
      "Epoch 21077/30000 Training Loss: 0.0341016910970211\n",
      "Epoch 21078/30000 Training Loss: 0.04250115901231766\n",
      "Epoch 21079/30000 Training Loss: 0.04483161121606827\n",
      "Epoch 21080/30000 Training Loss: 0.04213203862309456\n",
      "Epoch 21081/30000 Training Loss: 0.04854718595743179\n",
      "Epoch 21082/30000 Training Loss: 0.053168654441833496\n",
      "Epoch 21083/30000 Training Loss: 0.05040387436747551\n",
      "Epoch 21084/30000 Training Loss: 0.03695832937955856\n",
      "Epoch 21085/30000 Training Loss: 0.04151102155447006\n",
      "Epoch 21086/30000 Training Loss: 0.05508103966712952\n",
      "Epoch 21087/30000 Training Loss: 0.05264323949813843\n",
      "Epoch 21088/30000 Training Loss: 0.0446433462202549\n",
      "Epoch 21089/30000 Training Loss: 0.04491329938173294\n",
      "Epoch 21090/30000 Training Loss: 0.025998469442129135\n",
      "Epoch 21091/30000 Training Loss: 0.052336737513542175\n",
      "Epoch 21092/30000 Training Loss: 0.04946468025445938\n",
      "Epoch 21093/30000 Training Loss: 0.04336056485772133\n",
      "Epoch 21094/30000 Training Loss: 0.05277658998966217\n",
      "Epoch 21095/30000 Training Loss: 0.04855284467339516\n",
      "Epoch 21096/30000 Training Loss: 0.03553631901741028\n",
      "Epoch 21097/30000 Training Loss: 0.03988051787018776\n",
      "Epoch 21098/30000 Training Loss: 0.038765184581279755\n",
      "Epoch 21099/30000 Training Loss: 0.04224013909697533\n",
      "Epoch 21100/30000 Training Loss: 0.05479435995221138\n",
      "Epoch 21100/30000 Validation Loss: 0.04723013564944267\n",
      "Epoch 21101/30000 Training Loss: 0.04907696694135666\n",
      "Epoch 21102/30000 Training Loss: 0.04344470426440239\n",
      "Epoch 21103/30000 Training Loss: 0.04314090311527252\n",
      "Epoch 21104/30000 Training Loss: 0.03496652841567993\n",
      "Epoch 21105/30000 Training Loss: 0.04652662202715874\n",
      "Epoch 21106/30000 Training Loss: 0.04522301256656647\n",
      "Epoch 21107/30000 Training Loss: 0.04830687493085861\n",
      "Epoch 21108/30000 Training Loss: 0.05912754312157631\n",
      "Epoch 21109/30000 Training Loss: 0.06204011291265488\n",
      "Epoch 21110/30000 Training Loss: 0.03842855617403984\n",
      "Epoch 21111/30000 Training Loss: 0.05169443041086197\n",
      "Epoch 21112/30000 Training Loss: 0.04857015982270241\n",
      "Epoch 21113/30000 Training Loss: 0.03850916028022766\n",
      "Epoch 21114/30000 Training Loss: 0.03917798772454262\n",
      "Epoch 21115/30000 Training Loss: 0.042228322476148605\n",
      "Epoch 21116/30000 Training Loss: 0.04746881499886513\n",
      "Epoch 21117/30000 Training Loss: 0.0454915389418602\n",
      "Epoch 21118/30000 Training Loss: 0.056741006672382355\n",
      "Epoch 21119/30000 Training Loss: 0.03846656158566475\n",
      "Epoch 21120/30000 Training Loss: 0.04255439341068268\n",
      "Epoch 21121/30000 Training Loss: 0.062308378517627716\n",
      "Epoch 21122/30000 Training Loss: 0.04108194261789322\n",
      "Epoch 21123/30000 Training Loss: 0.05056057125329971\n",
      "Epoch 21124/30000 Training Loss: 0.04051515460014343\n",
      "Epoch 21125/30000 Training Loss: 0.039779163897037506\n",
      "Epoch 21126/30000 Training Loss: 0.04186670854687691\n",
      "Epoch 21127/30000 Training Loss: 0.054719336330890656\n",
      "Epoch 21128/30000 Training Loss: 0.03783078491687775\n",
      "Epoch 21129/30000 Training Loss: 0.03943853825330734\n",
      "Epoch 21130/30000 Training Loss: 0.06291703134775162\n",
      "Epoch 21131/30000 Training Loss: 0.04055153951048851\n",
      "Epoch 21132/30000 Training Loss: 0.041378725320100784\n",
      "Epoch 21133/30000 Training Loss: 0.046818748116493225\n",
      "Epoch 21134/30000 Training Loss: 0.07022701948881149\n",
      "Epoch 21135/30000 Training Loss: 0.05901530385017395\n",
      "Epoch 21136/30000 Training Loss: 0.04406467080116272\n",
      "Epoch 21137/30000 Training Loss: 0.04528549313545227\n",
      "Epoch 21138/30000 Training Loss: 0.03637076914310455\n",
      "Epoch 21139/30000 Training Loss: 0.047103531658649445\n",
      "Epoch 21140/30000 Training Loss: 0.038368452340364456\n",
      "Epoch 21141/30000 Training Loss: 0.04135359451174736\n",
      "Epoch 21142/30000 Training Loss: 0.04138323664665222\n",
      "Epoch 21143/30000 Training Loss: 0.05013278126716614\n",
      "Epoch 21144/30000 Training Loss: 0.06585588306188583\n",
      "Epoch 21145/30000 Training Loss: 0.04056549072265625\n",
      "Epoch 21146/30000 Training Loss: 0.04714914411306381\n",
      "Epoch 21147/30000 Training Loss: 0.05070611461997032\n",
      "Epoch 21148/30000 Training Loss: 0.04268929362297058\n",
      "Epoch 21149/30000 Training Loss: 0.04862958565354347\n",
      "Epoch 21150/30000 Training Loss: 0.043881915509700775\n",
      "Epoch 21151/30000 Training Loss: 0.048985790461301804\n",
      "Epoch 21152/30000 Training Loss: 0.04695211350917816\n",
      "Epoch 21153/30000 Training Loss: 0.0480131171643734\n",
      "Epoch 21154/30000 Training Loss: 0.041271746158599854\n",
      "Epoch 21155/30000 Training Loss: 0.05582151934504509\n",
      "Epoch 21156/30000 Training Loss: 0.05255981907248497\n",
      "Epoch 21157/30000 Training Loss: 0.045087821781635284\n",
      "Epoch 21158/30000 Training Loss: 0.045150309801101685\n",
      "Epoch 21159/30000 Training Loss: 0.06258003413677216\n",
      "Epoch 21160/30000 Training Loss: 0.04101990908384323\n",
      "Epoch 21161/30000 Training Loss: 0.053735408931970596\n",
      "Epoch 21162/30000 Training Loss: 0.04270588979125023\n",
      "Epoch 21163/30000 Training Loss: 0.0491018071770668\n",
      "Epoch 21164/30000 Training Loss: 0.035349585115909576\n",
      "Epoch 21165/30000 Training Loss: 0.053993694484233856\n",
      "Epoch 21166/30000 Training Loss: 0.04748877137899399\n",
      "Epoch 21167/30000 Training Loss: 0.05885155498981476\n",
      "Epoch 21168/30000 Training Loss: 0.052903398871421814\n",
      "Epoch 21169/30000 Training Loss: 0.05689192935824394\n",
      "Epoch 21170/30000 Training Loss: 0.04249735549092293\n",
      "Epoch 21171/30000 Training Loss: 0.04327905923128128\n",
      "Epoch 21172/30000 Training Loss: 0.048040252178907394\n",
      "Epoch 21173/30000 Training Loss: 0.04179542139172554\n",
      "Epoch 21174/30000 Training Loss: 0.04520450532436371\n",
      "Epoch 21175/30000 Training Loss: 0.05284087732434273\n",
      "Epoch 21176/30000 Training Loss: 0.042192358523607254\n",
      "Epoch 21177/30000 Training Loss: 0.03292790427803993\n",
      "Epoch 21178/30000 Training Loss: 0.04877474904060364\n",
      "Epoch 21179/30000 Training Loss: 0.04245714843273163\n",
      "Epoch 21180/30000 Training Loss: 0.050823576748371124\n",
      "Epoch 21181/30000 Training Loss: 0.03887229040265083\n",
      "Epoch 21182/30000 Training Loss: 0.05554854869842529\n",
      "Epoch 21183/30000 Training Loss: 0.04127529263496399\n",
      "Epoch 21184/30000 Training Loss: 0.048297710716724396\n",
      "Epoch 21185/30000 Training Loss: 0.04470554366707802\n",
      "Epoch 21186/30000 Training Loss: 0.03379489853978157\n",
      "Epoch 21187/30000 Training Loss: 0.036792606115341187\n",
      "Epoch 21188/30000 Training Loss: 0.05030921846628189\n",
      "Epoch 21189/30000 Training Loss: 0.051598742604255676\n",
      "Epoch 21190/30000 Training Loss: 0.03718220070004463\n",
      "Epoch 21191/30000 Training Loss: 0.046069830656051636\n",
      "Epoch 21192/30000 Training Loss: 0.03768216446042061\n",
      "Epoch 21193/30000 Training Loss: 0.062219031155109406\n",
      "Epoch 21194/30000 Training Loss: 0.04816565662622452\n",
      "Epoch 21195/30000 Training Loss: 0.043017588555812836\n",
      "Epoch 21196/30000 Training Loss: 0.0395810641348362\n",
      "Epoch 21197/30000 Training Loss: 0.040680404752492905\n",
      "Epoch 21198/30000 Training Loss: 0.05357801169157028\n",
      "Epoch 21199/30000 Training Loss: 0.04062441363930702\n",
      "Epoch 21200/30000 Training Loss: 0.04917595535516739\n",
      "Epoch 21200/30000 Validation Loss: 0.04205547645688057\n",
      "Epoch 21201/30000 Training Loss: 0.04913971573114395\n",
      "Epoch 21202/30000 Training Loss: 0.0446980856359005\n",
      "Epoch 21203/30000 Training Loss: 0.039579667150974274\n",
      "Epoch 21204/30000 Training Loss: 0.05646347627043724\n",
      "Epoch 21205/30000 Training Loss: 0.04356357455253601\n",
      "Epoch 21206/30000 Training Loss: 0.031735289841890335\n",
      "Epoch 21207/30000 Training Loss: 0.038506023585796356\n",
      "Epoch 21208/30000 Training Loss: 0.03830153867602348\n",
      "Epoch 21209/30000 Training Loss: 0.05870814621448517\n",
      "Epoch 21210/30000 Training Loss: 0.04537638649344444\n",
      "Epoch 21211/30000 Training Loss: 0.030208587646484375\n",
      "Epoch 21212/30000 Training Loss: 0.0407676063477993\n",
      "Epoch 21213/30000 Training Loss: 0.048906903713941574\n",
      "Epoch 21214/30000 Training Loss: 0.035275813192129135\n",
      "Epoch 21215/30000 Training Loss: 0.07195708900690079\n",
      "Epoch 21216/30000 Training Loss: 0.0547182522714138\n",
      "Epoch 21217/30000 Training Loss: 0.03903065249323845\n",
      "Epoch 21218/30000 Training Loss: 0.04043400287628174\n",
      "Epoch 21219/30000 Training Loss: 0.040685687214136124\n",
      "Epoch 21220/30000 Training Loss: 0.05061716213822365\n",
      "Epoch 21221/30000 Training Loss: 0.046746961772441864\n",
      "Epoch 21222/30000 Training Loss: 0.06263607740402222\n",
      "Epoch 21223/30000 Training Loss: 0.044763822108507156\n",
      "Epoch 21224/30000 Training Loss: 0.04691570624709129\n",
      "Epoch 21225/30000 Training Loss: 0.06030650436878204\n",
      "Epoch 21226/30000 Training Loss: 0.036277275532484055\n",
      "Epoch 21227/30000 Training Loss: 0.04474826902151108\n",
      "Epoch 21228/30000 Training Loss: 0.04366182163357735\n",
      "Epoch 21229/30000 Training Loss: 0.05206403136253357\n",
      "Epoch 21230/30000 Training Loss: 0.030108720064163208\n",
      "Epoch 21231/30000 Training Loss: 0.050143733620643616\n",
      "Epoch 21232/30000 Training Loss: 0.055535923689603806\n",
      "Epoch 21233/30000 Training Loss: 0.04250721633434296\n",
      "Epoch 21234/30000 Training Loss: 0.04988676309585571\n",
      "Epoch 21235/30000 Training Loss: 0.05235246941447258\n",
      "Epoch 21236/30000 Training Loss: 0.049902401864528656\n",
      "Epoch 21237/30000 Training Loss: 0.03931621462106705\n",
      "Epoch 21238/30000 Training Loss: 0.049908723682165146\n",
      "Epoch 21239/30000 Training Loss: 0.04085826128721237\n",
      "Epoch 21240/30000 Training Loss: 0.04137756675481796\n",
      "Epoch 21241/30000 Training Loss: 0.054753609001636505\n",
      "Epoch 21242/30000 Training Loss: 0.04478566348552704\n",
      "Epoch 21243/30000 Training Loss: 0.04346035048365593\n",
      "Epoch 21244/30000 Training Loss: 0.03712335228919983\n",
      "Epoch 21245/30000 Training Loss: 0.05374399572610855\n",
      "Epoch 21246/30000 Training Loss: 0.042396429926157\n",
      "Epoch 21247/30000 Training Loss: 0.04743650183081627\n",
      "Epoch 21248/30000 Training Loss: 0.04214552044868469\n",
      "Epoch 21249/30000 Training Loss: 0.050724029541015625\n",
      "Epoch 21250/30000 Training Loss: 0.03224707022309303\n",
      "Epoch 21251/30000 Training Loss: 0.03289679065346718\n",
      "Epoch 21252/30000 Training Loss: 0.0336083360016346\n",
      "Epoch 21253/30000 Training Loss: 0.05139689892530441\n",
      "Epoch 21254/30000 Training Loss: 0.04239380359649658\n",
      "Epoch 21255/30000 Training Loss: 0.05182269960641861\n",
      "Epoch 21256/30000 Training Loss: 0.05141261965036392\n",
      "Epoch 21257/30000 Training Loss: 0.04703522473573685\n",
      "Epoch 21258/30000 Training Loss: 0.03558221459388733\n",
      "Epoch 21259/30000 Training Loss: 0.04033875837922096\n",
      "Epoch 21260/30000 Training Loss: 0.04763585329055786\n",
      "Epoch 21261/30000 Training Loss: 0.045974764972925186\n",
      "Epoch 21262/30000 Training Loss: 0.038805827498435974\n",
      "Epoch 21263/30000 Training Loss: 0.07310247421264648\n",
      "Epoch 21264/30000 Training Loss: 0.04185352101922035\n",
      "Epoch 21265/30000 Training Loss: 0.05667993426322937\n",
      "Epoch 21266/30000 Training Loss: 0.04988567531108856\n",
      "Epoch 21267/30000 Training Loss: 0.04366747662425041\n",
      "Epoch 21268/30000 Training Loss: 0.05591030418872833\n",
      "Epoch 21269/30000 Training Loss: 0.04733020067214966\n",
      "Epoch 21270/30000 Training Loss: 0.052937768399715424\n",
      "Epoch 21271/30000 Training Loss: 0.04501214995980263\n",
      "Epoch 21272/30000 Training Loss: 0.06480490416288376\n",
      "Epoch 21273/30000 Training Loss: 0.048754796385765076\n",
      "Epoch 21274/30000 Training Loss: 0.05141754448413849\n",
      "Epoch 21275/30000 Training Loss: 0.04934793338179588\n",
      "Epoch 21276/30000 Training Loss: 0.04343847557902336\n",
      "Epoch 21277/30000 Training Loss: 0.028878260403871536\n",
      "Epoch 21278/30000 Training Loss: 0.04130247235298157\n",
      "Epoch 21279/30000 Training Loss: 0.04411832615733147\n",
      "Epoch 21280/30000 Training Loss: 0.04033065214753151\n",
      "Epoch 21281/30000 Training Loss: 0.06028714030981064\n",
      "Epoch 21282/30000 Training Loss: 0.03762885555624962\n",
      "Epoch 21283/30000 Training Loss: 0.04592398181557655\n",
      "Epoch 21284/30000 Training Loss: 0.04091457277536392\n",
      "Epoch 21285/30000 Training Loss: 0.041662052273750305\n",
      "Epoch 21286/30000 Training Loss: 0.04647626355290413\n",
      "Epoch 21287/30000 Training Loss: 0.03792893514037132\n",
      "Epoch 21288/30000 Training Loss: 0.04918773099780083\n",
      "Epoch 21289/30000 Training Loss: 0.04110373556613922\n",
      "Epoch 21290/30000 Training Loss: 0.04397939145565033\n",
      "Epoch 21291/30000 Training Loss: 0.04270778223872185\n",
      "Epoch 21292/30000 Training Loss: 0.062171611934900284\n",
      "Epoch 21293/30000 Training Loss: 0.04279845952987671\n",
      "Epoch 21294/30000 Training Loss: 0.05961894616484642\n",
      "Epoch 21295/30000 Training Loss: 0.055822357535362244\n",
      "Epoch 21296/30000 Training Loss: 0.04991992190480232\n",
      "Epoch 21297/30000 Training Loss: 0.05356759577989578\n",
      "Epoch 21298/30000 Training Loss: 0.03278779983520508\n",
      "Epoch 21299/30000 Training Loss: 0.05007093399763107\n",
      "Epoch 21300/30000 Training Loss: 0.039701156318187714\n",
      "Epoch 21300/30000 Validation Loss: 0.03994577378034592\n",
      "Epoch 21301/30000 Training Loss: 0.04744333028793335\n",
      "Epoch 21302/30000 Training Loss: 0.043138422071933746\n",
      "Epoch 21303/30000 Training Loss: 0.05721386894583702\n",
      "Epoch 21304/30000 Training Loss: 0.04020220786333084\n",
      "Epoch 21305/30000 Training Loss: 0.05200149491429329\n",
      "Epoch 21306/30000 Training Loss: 0.046695008873939514\n",
      "Epoch 21307/30000 Training Loss: 0.04602011293172836\n",
      "Epoch 21308/30000 Training Loss: 0.05526105687022209\n",
      "Epoch 21309/30000 Training Loss: 0.03193027153611183\n",
      "Epoch 21310/30000 Training Loss: 0.0553671196103096\n",
      "Epoch 21311/30000 Training Loss: 0.04347400739789009\n",
      "Epoch 21312/30000 Training Loss: 0.04338771849870682\n",
      "Epoch 21313/30000 Training Loss: 0.044646188616752625\n",
      "Epoch 21314/30000 Training Loss: 0.04178823530673981\n",
      "Epoch 21315/30000 Training Loss: 0.03957241773605347\n",
      "Epoch 21316/30000 Training Loss: 0.04492321237921715\n",
      "Epoch 21317/30000 Training Loss: 0.05012773722410202\n",
      "Epoch 21318/30000 Training Loss: 0.04307403042912483\n",
      "Epoch 21319/30000 Training Loss: 0.07084767520427704\n",
      "Epoch 21320/30000 Training Loss: 0.04014026001095772\n",
      "Epoch 21321/30000 Training Loss: 0.03969200700521469\n",
      "Epoch 21322/30000 Training Loss: 0.04453536123037338\n",
      "Epoch 21323/30000 Training Loss: 0.02989771217107773\n",
      "Epoch 21324/30000 Training Loss: 0.05042077973484993\n",
      "Epoch 21325/30000 Training Loss: 0.03819523751735687\n",
      "Epoch 21326/30000 Training Loss: 0.05813651904463768\n",
      "Epoch 21327/30000 Training Loss: 0.060400959104299545\n",
      "Epoch 21328/30000 Training Loss: 0.040755148977041245\n",
      "Epoch 21329/30000 Training Loss: 0.045011889189481735\n",
      "Epoch 21330/30000 Training Loss: 0.046655211597681046\n",
      "Epoch 21331/30000 Training Loss: 0.04619613289833069\n",
      "Epoch 21332/30000 Training Loss: 0.040352609008550644\n",
      "Epoch 21333/30000 Training Loss: 0.0471663698554039\n",
      "Epoch 21334/30000 Training Loss: 0.04931608960032463\n",
      "Epoch 21335/30000 Training Loss: 0.029946081340312958\n",
      "Epoch 21336/30000 Training Loss: 0.03436921909451485\n",
      "Epoch 21337/30000 Training Loss: 0.06004182621836662\n",
      "Epoch 21338/30000 Training Loss: 0.07204978913068771\n",
      "Epoch 21339/30000 Training Loss: 0.05124522000551224\n",
      "Epoch 21340/30000 Training Loss: 0.042140696197748184\n",
      "Epoch 21341/30000 Training Loss: 0.053295571357011795\n",
      "Epoch 21342/30000 Training Loss: 0.054824359714984894\n",
      "Epoch 21343/30000 Training Loss: 0.05591052770614624\n",
      "Epoch 21344/30000 Training Loss: 0.0422932393848896\n",
      "Epoch 21345/30000 Training Loss: 0.048744767904281616\n",
      "Epoch 21346/30000 Training Loss: 0.04176425188779831\n",
      "Epoch 21347/30000 Training Loss: 0.05051814764738083\n",
      "Epoch 21348/30000 Training Loss: 0.04589446634054184\n",
      "Epoch 21349/30000 Training Loss: 0.04135189205408096\n",
      "Epoch 21350/30000 Training Loss: 0.04979129135608673\n",
      "Epoch 21351/30000 Training Loss: 0.04202389344573021\n",
      "Epoch 21352/30000 Training Loss: 0.03571907430887222\n",
      "Epoch 21353/30000 Training Loss: 0.04543100297451019\n",
      "Epoch 21354/30000 Training Loss: 0.04537152498960495\n",
      "Epoch 21355/30000 Training Loss: 0.05609942600131035\n",
      "Epoch 21356/30000 Training Loss: 0.05008114129304886\n",
      "Epoch 21357/30000 Training Loss: 0.04336633160710335\n",
      "Epoch 21358/30000 Training Loss: 0.04874756559729576\n",
      "Epoch 21359/30000 Training Loss: 0.04288104176521301\n",
      "Epoch 21360/30000 Training Loss: 0.05022009462118149\n",
      "Epoch 21361/30000 Training Loss: 0.03614657372236252\n",
      "Epoch 21362/30000 Training Loss: 0.05452456697821617\n",
      "Epoch 21363/30000 Training Loss: 0.039458997547626495\n",
      "Epoch 21364/30000 Training Loss: 0.038934625685214996\n",
      "Epoch 21365/30000 Training Loss: 0.048947446048259735\n",
      "Epoch 21366/30000 Training Loss: 0.0356602817773819\n",
      "Epoch 21367/30000 Training Loss: 0.05111508071422577\n",
      "Epoch 21368/30000 Training Loss: 0.04474678263068199\n",
      "Epoch 21369/30000 Training Loss: 0.03879839554429054\n",
      "Epoch 21370/30000 Training Loss: 0.050248414278030396\n",
      "Epoch 21371/30000 Training Loss: 0.0417482927441597\n",
      "Epoch 21372/30000 Training Loss: 0.04777592420578003\n",
      "Epoch 21373/30000 Training Loss: 0.030612323433160782\n",
      "Epoch 21374/30000 Training Loss: 0.04152914509177208\n",
      "Epoch 21375/30000 Training Loss: 0.041597578674554825\n",
      "Epoch 21376/30000 Training Loss: 0.04098472744226456\n",
      "Epoch 21377/30000 Training Loss: 0.040238380432128906\n",
      "Epoch 21378/30000 Training Loss: 0.04277641698718071\n",
      "Epoch 21379/30000 Training Loss: 0.05911428481340408\n",
      "Epoch 21380/30000 Training Loss: 0.04834887385368347\n",
      "Epoch 21381/30000 Training Loss: 0.06781092286109924\n",
      "Epoch 21382/30000 Training Loss: 0.046720992773771286\n",
      "Epoch 21383/30000 Training Loss: 0.04508170485496521\n",
      "Epoch 21384/30000 Training Loss: 0.06044520437717438\n",
      "Epoch 21385/30000 Training Loss: 0.04354608803987503\n",
      "Epoch 21386/30000 Training Loss: 0.03268186002969742\n",
      "Epoch 21387/30000 Training Loss: 0.0394863560795784\n",
      "Epoch 21388/30000 Training Loss: 0.04223974049091339\n",
      "Epoch 21389/30000 Training Loss: 0.04024740681052208\n",
      "Epoch 21390/30000 Training Loss: 0.04394439607858658\n",
      "Epoch 21391/30000 Training Loss: 0.03990206494927406\n",
      "Epoch 21392/30000 Training Loss: 0.04785139858722687\n",
      "Epoch 21393/30000 Training Loss: 0.03914643079042435\n",
      "Epoch 21394/30000 Training Loss: 0.03993264585733414\n",
      "Epoch 21395/30000 Training Loss: 0.04590967297554016\n",
      "Epoch 21396/30000 Training Loss: 0.06839720904827118\n",
      "Epoch 21397/30000 Training Loss: 0.05450735241174698\n",
      "Epoch 21398/30000 Training Loss: 0.05174671858549118\n",
      "Epoch 21399/30000 Training Loss: 0.04086945950984955\n",
      "Epoch 21400/30000 Training Loss: 0.05060127377510071\n",
      "Epoch 21400/30000 Validation Loss: 0.040714915841817856\n",
      "Epoch 21401/30000 Training Loss: 0.059920534491539\n",
      "Epoch 21402/30000 Training Loss: 0.05398859828710556\n",
      "Epoch 21403/30000 Training Loss: 0.050486549735069275\n",
      "Epoch 21404/30000 Training Loss: 0.03478066623210907\n",
      "Epoch 21405/30000 Training Loss: 0.05522565171122551\n",
      "Epoch 21406/30000 Training Loss: 0.06234578788280487\n",
      "Epoch 21407/30000 Training Loss: 0.04293129965662956\n",
      "Epoch 21408/30000 Training Loss: 0.04152733460068703\n",
      "Epoch 21409/30000 Training Loss: 0.044001687318086624\n",
      "Epoch 21410/30000 Training Loss: 0.041034162044525146\n",
      "Epoch 21411/30000 Training Loss: 0.04525630176067352\n",
      "Epoch 21412/30000 Training Loss: 0.05642347037792206\n",
      "Epoch 21413/30000 Training Loss: 0.06332438439130783\n",
      "Epoch 21414/30000 Training Loss: 0.048740923404693604\n",
      "Epoch 21415/30000 Training Loss: 0.04766954109072685\n",
      "Epoch 21416/30000 Training Loss: 0.05355583876371384\n",
      "Epoch 21417/30000 Training Loss: 0.041615672409534454\n",
      "Epoch 21418/30000 Training Loss: 0.06846478581428528\n",
      "Epoch 21419/30000 Training Loss: 0.055282991379499435\n",
      "Epoch 21420/30000 Training Loss: 0.03544462099671364\n",
      "Epoch 21421/30000 Training Loss: 0.03473372757434845\n",
      "Epoch 21422/30000 Training Loss: 0.04642074927687645\n",
      "Epoch 21423/30000 Training Loss: 0.056602172553539276\n",
      "Epoch 21424/30000 Training Loss: 0.0420839749276638\n",
      "Epoch 21425/30000 Training Loss: 0.0408116914331913\n",
      "Epoch 21426/30000 Training Loss: 0.040857791900634766\n",
      "Epoch 21427/30000 Training Loss: 0.03795558959245682\n",
      "Epoch 21428/30000 Training Loss: 0.044034842401742935\n",
      "Epoch 21429/30000 Training Loss: 0.054822687059640884\n",
      "Epoch 21430/30000 Training Loss: 0.04738139733672142\n",
      "Epoch 21431/30000 Training Loss: 0.03540755808353424\n",
      "Epoch 21432/30000 Training Loss: 0.03427848219871521\n",
      "Epoch 21433/30000 Training Loss: 0.03368695080280304\n",
      "Epoch 21434/30000 Training Loss: 0.06000620871782303\n",
      "Epoch 21435/30000 Training Loss: 0.06412854045629501\n",
      "Epoch 21436/30000 Training Loss: 0.03593067824840546\n",
      "Epoch 21437/30000 Training Loss: 0.04651322215795517\n",
      "Epoch 21438/30000 Training Loss: 0.033418502658605576\n",
      "Epoch 21439/30000 Training Loss: 0.04587327316403389\n",
      "Epoch 21440/30000 Training Loss: 0.042819928377866745\n",
      "Epoch 21441/30000 Training Loss: 0.05487421900033951\n",
      "Epoch 21442/30000 Training Loss: 0.04050198197364807\n",
      "Epoch 21443/30000 Training Loss: 0.04800586774945259\n",
      "Epoch 21444/30000 Training Loss: 0.034085966646671295\n",
      "Epoch 21445/30000 Training Loss: 0.036943159997463226\n",
      "Epoch 21446/30000 Training Loss: 0.046725619584321976\n",
      "Epoch 21447/30000 Training Loss: 0.06279678642749786\n",
      "Epoch 21448/30000 Training Loss: 0.04276210814714432\n",
      "Epoch 21449/30000 Training Loss: 0.0510624498128891\n",
      "Epoch 21450/30000 Training Loss: 0.04716397821903229\n",
      "Epoch 21451/30000 Training Loss: 0.0389900766313076\n",
      "Epoch 21452/30000 Training Loss: 0.06295748800039291\n",
      "Epoch 21453/30000 Training Loss: 0.04287157207727432\n",
      "Epoch 21454/30000 Training Loss: 0.05495555326342583\n",
      "Epoch 21455/30000 Training Loss: 0.038880690932273865\n",
      "Epoch 21456/30000 Training Loss: 0.0551074743270874\n",
      "Epoch 21457/30000 Training Loss: 0.05800614506006241\n",
      "Epoch 21458/30000 Training Loss: 0.03684205189347267\n",
      "Epoch 21459/30000 Training Loss: 0.04538523033261299\n",
      "Epoch 21460/30000 Training Loss: 0.05483569949865341\n",
      "Epoch 21461/30000 Training Loss: 0.04548903927206993\n",
      "Epoch 21462/30000 Training Loss: 0.03291720524430275\n",
      "Epoch 21463/30000 Training Loss: 0.053624339401721954\n",
      "Epoch 21464/30000 Training Loss: 0.03984619304537773\n",
      "Epoch 21465/30000 Training Loss: 0.06235445290803909\n",
      "Epoch 21466/30000 Training Loss: 0.057215988636016846\n",
      "Epoch 21467/30000 Training Loss: 0.04520599916577339\n",
      "Epoch 21468/30000 Training Loss: 0.04468206316232681\n",
      "Epoch 21469/30000 Training Loss: 0.051216449588537216\n",
      "Epoch 21470/30000 Training Loss: 0.05616002902388573\n",
      "Epoch 21471/30000 Training Loss: 0.05368354916572571\n",
      "Epoch 21472/30000 Training Loss: 0.054304905235767365\n",
      "Epoch 21473/30000 Training Loss: 0.039149120450019836\n",
      "Epoch 21474/30000 Training Loss: 0.05955756828188896\n",
      "Epoch 21475/30000 Training Loss: 0.038339003920555115\n",
      "Epoch 21476/30000 Training Loss: 0.04933289438486099\n",
      "Epoch 21477/30000 Training Loss: 0.054298222064971924\n",
      "Epoch 21478/30000 Training Loss: 0.04635646194219589\n",
      "Epoch 21479/30000 Training Loss: 0.06322052329778671\n",
      "Epoch 21480/30000 Training Loss: 0.04181937873363495\n",
      "Epoch 21481/30000 Training Loss: 0.038414157927036285\n",
      "Epoch 21482/30000 Training Loss: 0.04563852772116661\n",
      "Epoch 21483/30000 Training Loss: 0.02905309945344925\n",
      "Epoch 21484/30000 Training Loss: 0.04146831855177879\n",
      "Epoch 21485/30000 Training Loss: 0.05196383222937584\n",
      "Epoch 21486/30000 Training Loss: 0.05200139805674553\n",
      "Epoch 21487/30000 Training Loss: 0.03356177732348442\n",
      "Epoch 21488/30000 Training Loss: 0.05274853855371475\n",
      "Epoch 21489/30000 Training Loss: 0.043196916580200195\n",
      "Epoch 21490/30000 Training Loss: 0.042815014719963074\n",
      "Epoch 21491/30000 Training Loss: 0.032744403928518295\n",
      "Epoch 21492/30000 Training Loss: 0.039308883249759674\n",
      "Epoch 21493/30000 Training Loss: 0.0349385105073452\n",
      "Epoch 21494/30000 Training Loss: 0.05022729933261871\n",
      "Epoch 21495/30000 Training Loss: 0.04657922685146332\n",
      "Epoch 21496/30000 Training Loss: 0.044727955013513565\n",
      "Epoch 21497/30000 Training Loss: 0.04357288405299187\n",
      "Epoch 21498/30000 Training Loss: 0.04988713935017586\n",
      "Epoch 21499/30000 Training Loss: 0.029366113245487213\n",
      "Epoch 21500/30000 Training Loss: 0.052651334553956985\n",
      "Epoch 21500/30000 Validation Loss: 0.0530727282166481\n",
      "Epoch 21501/30000 Training Loss: 0.033107608556747437\n",
      "Epoch 21502/30000 Training Loss: 0.05491990968585014\n",
      "Epoch 21503/30000 Training Loss: 0.06662999093532562\n",
      "Epoch 21504/30000 Training Loss: 0.03839704766869545\n",
      "Epoch 21505/30000 Training Loss: 0.041500307619571686\n",
      "Epoch 21506/30000 Training Loss: 0.04171330854296684\n",
      "Epoch 21507/30000 Training Loss: 0.0380067303776741\n",
      "Epoch 21508/30000 Training Loss: 0.04138777777552605\n",
      "Epoch 21509/30000 Training Loss: 0.048597898334264755\n",
      "Epoch 21510/30000 Training Loss: 0.03549700975418091\n",
      "Epoch 21511/30000 Training Loss: 0.04370252415537834\n",
      "Epoch 21512/30000 Training Loss: 0.05067157745361328\n",
      "Epoch 21513/30000 Training Loss: 0.05339723452925682\n",
      "Epoch 21514/30000 Training Loss: 0.045559633523225784\n",
      "Epoch 21515/30000 Training Loss: 0.046513013541698456\n",
      "Epoch 21516/30000 Training Loss: 0.046783387660980225\n",
      "Epoch 21517/30000 Training Loss: 0.04319194331765175\n",
      "Epoch 21518/30000 Training Loss: 0.0510888397693634\n",
      "Epoch 21519/30000 Training Loss: 0.04262629151344299\n",
      "Epoch 21520/30000 Training Loss: 0.04866904020309448\n",
      "Epoch 21521/30000 Training Loss: 0.03958788886666298\n",
      "Epoch 21522/30000 Training Loss: 0.05206204578280449\n",
      "Epoch 21523/30000 Training Loss: 0.06122221052646637\n",
      "Epoch 21524/30000 Training Loss: 0.0468258336186409\n",
      "Epoch 21525/30000 Training Loss: 0.05347872152924538\n",
      "Epoch 21526/30000 Training Loss: 0.05777246877551079\n",
      "Epoch 21527/30000 Training Loss: 0.03153951093554497\n",
      "Epoch 21528/30000 Training Loss: 0.04233100637793541\n",
      "Epoch 21529/30000 Training Loss: 0.042480453848838806\n",
      "Epoch 21530/30000 Training Loss: 0.04824594780802727\n",
      "Epoch 21531/30000 Training Loss: 0.04778790473937988\n",
      "Epoch 21532/30000 Training Loss: 0.05334751307964325\n",
      "Epoch 21533/30000 Training Loss: 0.041475195437669754\n",
      "Epoch 21534/30000 Training Loss: 0.045186661183834076\n",
      "Epoch 21535/30000 Training Loss: 0.06157420575618744\n",
      "Epoch 21536/30000 Training Loss: 0.055102672427892685\n",
      "Epoch 21537/30000 Training Loss: 0.04075215011835098\n",
      "Epoch 21538/30000 Training Loss: 0.05145980417728424\n",
      "Epoch 21539/30000 Training Loss: 0.040404822677373886\n",
      "Epoch 21540/30000 Training Loss: 0.04233808070421219\n",
      "Epoch 21541/30000 Training Loss: 0.050615094602108\n",
      "Epoch 21542/30000 Training Loss: 0.03835649788379669\n",
      "Epoch 21543/30000 Training Loss: 0.05484675243496895\n",
      "Epoch 21544/30000 Training Loss: 0.0500577948987484\n",
      "Epoch 21545/30000 Training Loss: 0.05475277826189995\n",
      "Epoch 21546/30000 Training Loss: 0.03359977900981903\n",
      "Epoch 21547/30000 Training Loss: 0.0408996045589447\n",
      "Epoch 21548/30000 Training Loss: 0.05142317712306976\n",
      "Epoch 21549/30000 Training Loss: 0.05666695535182953\n",
      "Epoch 21550/30000 Training Loss: 0.0493793785572052\n",
      "Epoch 21551/30000 Training Loss: 0.05350092053413391\n",
      "Epoch 21552/30000 Training Loss: 0.045043036341667175\n",
      "Epoch 21553/30000 Training Loss: 0.04169372469186783\n",
      "Epoch 21554/30000 Training Loss: 0.055962078273296356\n",
      "Epoch 21555/30000 Training Loss: 0.05475625395774841\n",
      "Epoch 21556/30000 Training Loss: 0.049000002443790436\n",
      "Epoch 21557/30000 Training Loss: 0.040060821920633316\n",
      "Epoch 21558/30000 Training Loss: 0.03563358634710312\n",
      "Epoch 21559/30000 Training Loss: 0.046038515865802765\n",
      "Epoch 21560/30000 Training Loss: 0.05180606245994568\n",
      "Epoch 21561/30000 Training Loss: 0.05171852186322212\n",
      "Epoch 21562/30000 Training Loss: 0.03781997039914131\n",
      "Epoch 21563/30000 Training Loss: 0.05209615081548691\n",
      "Epoch 21564/30000 Training Loss: 0.04581825062632561\n",
      "Epoch 21565/30000 Training Loss: 0.04665759950876236\n",
      "Epoch 21566/30000 Training Loss: 0.044732868671417236\n",
      "Epoch 21567/30000 Training Loss: 0.03818976879119873\n",
      "Epoch 21568/30000 Training Loss: 0.06263165175914764\n",
      "Epoch 21569/30000 Training Loss: 0.03494688868522644\n",
      "Epoch 21570/30000 Training Loss: 0.05717185512185097\n",
      "Epoch 21571/30000 Training Loss: 0.04791095852851868\n",
      "Epoch 21572/30000 Training Loss: 0.059579670429229736\n",
      "Epoch 21573/30000 Training Loss: 0.044211383908987045\n",
      "Epoch 21574/30000 Training Loss: 0.04408412054181099\n",
      "Epoch 21575/30000 Training Loss: 0.042310260236263275\n",
      "Epoch 21576/30000 Training Loss: 0.046929724514484406\n",
      "Epoch 21577/30000 Training Loss: 0.044780462980270386\n",
      "Epoch 21578/30000 Training Loss: 0.04931502789258957\n",
      "Epoch 21579/30000 Training Loss: 0.03924525901675224\n",
      "Epoch 21580/30000 Training Loss: 0.05446106195449829\n",
      "Epoch 21581/30000 Training Loss: 0.04683321714401245\n",
      "Epoch 21582/30000 Training Loss: 0.04242397099733353\n",
      "Epoch 21583/30000 Training Loss: 0.047710224986076355\n",
      "Epoch 21584/30000 Training Loss: 0.03707043081521988\n",
      "Epoch 21585/30000 Training Loss: 0.05385243147611618\n",
      "Epoch 21586/30000 Training Loss: 0.04072418063879013\n",
      "Epoch 21587/30000 Training Loss: 0.048192098736763\n",
      "Epoch 21588/30000 Training Loss: 0.050862159579992294\n",
      "Epoch 21589/30000 Training Loss: 0.04395216703414917\n",
      "Epoch 21590/30000 Training Loss: 0.05740748345851898\n",
      "Epoch 21591/30000 Training Loss: 0.04477967694401741\n",
      "Epoch 21592/30000 Training Loss: 0.05337966978549957\n",
      "Epoch 21593/30000 Training Loss: 0.05968111753463745\n",
      "Epoch 21594/30000 Training Loss: 0.05232222378253937\n",
      "Epoch 21595/30000 Training Loss: 0.050846539437770844\n",
      "Epoch 21596/30000 Training Loss: 0.04465357959270477\n",
      "Epoch 21597/30000 Training Loss: 0.05443524941802025\n",
      "Epoch 21598/30000 Training Loss: 0.052840083837509155\n",
      "Epoch 21599/30000 Training Loss: 0.035706713795661926\n",
      "Epoch 21600/30000 Training Loss: 0.03930774703621864\n",
      "Epoch 21600/30000 Validation Loss: 0.04218292981386185\n",
      "Epoch 21601/30000 Training Loss: 0.0467432402074337\n",
      "Epoch 21602/30000 Training Loss: 0.04218675568699837\n",
      "Epoch 21603/30000 Training Loss: 0.05665886029601097\n",
      "Epoch 21604/30000 Training Loss: 0.03475804999470711\n",
      "Epoch 21605/30000 Training Loss: 0.046173255890607834\n",
      "Epoch 21606/30000 Training Loss: 0.048317473381757736\n",
      "Epoch 21607/30000 Training Loss: 0.06589376926422119\n",
      "Epoch 21608/30000 Training Loss: 0.04491278529167175\n",
      "Epoch 21609/30000 Training Loss: 0.04039508476853371\n",
      "Epoch 21610/30000 Training Loss: 0.03205490857362747\n",
      "Epoch 21611/30000 Training Loss: 0.03911738842725754\n",
      "Epoch 21612/30000 Training Loss: 0.06146379932761192\n",
      "Epoch 21613/30000 Training Loss: 0.05603129789233208\n",
      "Epoch 21614/30000 Training Loss: 0.040923211723566055\n",
      "Epoch 21615/30000 Training Loss: 0.04510626569390297\n",
      "Epoch 21616/30000 Training Loss: 0.046128202229738235\n",
      "Epoch 21617/30000 Training Loss: 0.052112698554992676\n",
      "Epoch 21618/30000 Training Loss: 0.035930830985307693\n",
      "Epoch 21619/30000 Training Loss: 0.051548782736063004\n",
      "Epoch 21620/30000 Training Loss: 0.04947901889681816\n",
      "Epoch 21621/30000 Training Loss: 0.04718192666769028\n",
      "Epoch 21622/30000 Training Loss: 0.04885462671518326\n",
      "Epoch 21623/30000 Training Loss: 0.0549672469496727\n",
      "Epoch 21624/30000 Training Loss: 0.054094862192869186\n",
      "Epoch 21625/30000 Training Loss: 0.038631539791822433\n",
      "Epoch 21626/30000 Training Loss: 0.04912668466567993\n",
      "Epoch 21627/30000 Training Loss: 0.05262070521712303\n",
      "Epoch 21628/30000 Training Loss: 0.0425468385219574\n",
      "Epoch 21629/30000 Training Loss: 0.04582258686423302\n",
      "Epoch 21630/30000 Training Loss: 0.049171894788742065\n",
      "Epoch 21631/30000 Training Loss: 0.04430648684501648\n",
      "Epoch 21632/30000 Training Loss: 0.048149626702070236\n",
      "Epoch 21633/30000 Training Loss: 0.04030119627714157\n",
      "Epoch 21634/30000 Training Loss: 0.03128731623291969\n",
      "Epoch 21635/30000 Training Loss: 0.04880437254905701\n",
      "Epoch 21636/30000 Training Loss: 0.04386168345808983\n",
      "Epoch 21637/30000 Training Loss: 0.044382400810718536\n",
      "Epoch 21638/30000 Training Loss: 0.032590169459581375\n",
      "Epoch 21639/30000 Training Loss: 0.060589082539081573\n",
      "Epoch 21640/30000 Training Loss: 0.0557786300778389\n",
      "Epoch 21641/30000 Training Loss: 0.03983502835035324\n",
      "Epoch 21642/30000 Training Loss: 0.05347578600049019\n",
      "Epoch 21643/30000 Training Loss: 0.04394518584012985\n",
      "Epoch 21644/30000 Training Loss: 0.0519908145070076\n",
      "Epoch 21645/30000 Training Loss: 0.05573330074548721\n",
      "Epoch 21646/30000 Training Loss: 0.04611224681138992\n",
      "Epoch 21647/30000 Training Loss: 0.04684837907552719\n",
      "Epoch 21648/30000 Training Loss: 0.0501507967710495\n",
      "Epoch 21649/30000 Training Loss: 0.040058303624391556\n",
      "Epoch 21650/30000 Training Loss: 0.049575261771678925\n",
      "Epoch 21651/30000 Training Loss: 0.05373102053999901\n",
      "Epoch 21652/30000 Training Loss: 0.032582834362983704\n",
      "Epoch 21653/30000 Training Loss: 0.03861360624432564\n",
      "Epoch 21654/30000 Training Loss: 0.05201490968465805\n",
      "Epoch 21655/30000 Training Loss: 0.047118715941905975\n",
      "Epoch 21656/30000 Training Loss: 0.03580813109874725\n",
      "Epoch 21657/30000 Training Loss: 0.03906228393316269\n",
      "Epoch 21658/30000 Training Loss: 0.040418591350317\n",
      "Epoch 21659/30000 Training Loss: 0.04067906364798546\n",
      "Epoch 21660/30000 Training Loss: 0.04728449881076813\n",
      "Epoch 21661/30000 Training Loss: 0.046767473220825195\n",
      "Epoch 21662/30000 Training Loss: 0.05548066645860672\n",
      "Epoch 21663/30000 Training Loss: 0.04473663121461868\n",
      "Epoch 21664/30000 Training Loss: 0.03821627050638199\n",
      "Epoch 21665/30000 Training Loss: 0.056999191641807556\n",
      "Epoch 21666/30000 Training Loss: 0.03945043683052063\n",
      "Epoch 21667/30000 Training Loss: 0.052531518042087555\n",
      "Epoch 21668/30000 Training Loss: 0.03667547553777695\n",
      "Epoch 21669/30000 Training Loss: 0.04694543033838272\n",
      "Epoch 21670/30000 Training Loss: 0.03391026332974434\n",
      "Epoch 21671/30000 Training Loss: 0.04134184494614601\n",
      "Epoch 21672/30000 Training Loss: 0.04657502472400665\n",
      "Epoch 21673/30000 Training Loss: 0.05512792244553566\n",
      "Epoch 21674/30000 Training Loss: 0.03275654837489128\n",
      "Epoch 21675/30000 Training Loss: 0.036928582936525345\n",
      "Epoch 21676/30000 Training Loss: 0.05193781852722168\n",
      "Epoch 21677/30000 Training Loss: 0.050481073558330536\n",
      "Epoch 21678/30000 Training Loss: 0.041573040187358856\n",
      "Epoch 21679/30000 Training Loss: 0.04429412633180618\n",
      "Epoch 21680/30000 Training Loss: 0.06486630439758301\n",
      "Epoch 21681/30000 Training Loss: 0.04935644567012787\n",
      "Epoch 21682/30000 Training Loss: 0.052285466343164444\n",
      "Epoch 21683/30000 Training Loss: 0.06287054717540741\n",
      "Epoch 21684/30000 Training Loss: 0.056466300040483475\n",
      "Epoch 21685/30000 Training Loss: 0.058690331876277924\n",
      "Epoch 21686/30000 Training Loss: 0.04180026799440384\n",
      "Epoch 21687/30000 Training Loss: 0.04427088797092438\n",
      "Epoch 21688/30000 Training Loss: 0.04146725311875343\n",
      "Epoch 21689/30000 Training Loss: 0.03944641351699829\n",
      "Epoch 21690/30000 Training Loss: 0.03837962821125984\n",
      "Epoch 21691/30000 Training Loss: 0.03479546681046486\n",
      "Epoch 21692/30000 Training Loss: 0.04658898711204529\n",
      "Epoch 21693/30000 Training Loss: 0.04449738189578056\n",
      "Epoch 21694/30000 Training Loss: 0.051418762654066086\n",
      "Epoch 21695/30000 Training Loss: 0.048477400094270706\n",
      "Epoch 21696/30000 Training Loss: 0.043019481003284454\n",
      "Epoch 21697/30000 Training Loss: 0.06221410259604454\n",
      "Epoch 21698/30000 Training Loss: 0.03980066627264023\n",
      "Epoch 21699/30000 Training Loss: 0.028508516028523445\n",
      "Epoch 21700/30000 Training Loss: 0.05893046408891678\n",
      "Epoch 21700/30000 Validation Loss: 0.03580024838447571\n",
      "Epoch 21701/30000 Training Loss: 0.04329250007867813\n",
      "Epoch 21702/30000 Training Loss: 0.04835135117173195\n",
      "Epoch 21703/30000 Training Loss: 0.05426815524697304\n",
      "Epoch 21704/30000 Training Loss: 0.04728296399116516\n",
      "Epoch 21705/30000 Training Loss: 0.0522909015417099\n",
      "Epoch 21706/30000 Training Loss: 0.05151022598147392\n",
      "Epoch 21707/30000 Training Loss: 0.043047450482845306\n",
      "Epoch 21708/30000 Training Loss: 0.05368945747613907\n",
      "Epoch 21709/30000 Training Loss: 0.0559077188372612\n",
      "Epoch 21710/30000 Training Loss: 0.052414536476135254\n",
      "Epoch 21711/30000 Training Loss: 0.04791778326034546\n",
      "Epoch 21712/30000 Training Loss: 0.03787156194448471\n",
      "Epoch 21713/30000 Training Loss: 0.05163967236876488\n",
      "Epoch 21714/30000 Training Loss: 0.045077551156282425\n",
      "Epoch 21715/30000 Training Loss: 0.052132297307252884\n",
      "Epoch 21716/30000 Training Loss: 0.04719153046607971\n",
      "Epoch 21717/30000 Training Loss: 0.049618884921073914\n",
      "Epoch 21718/30000 Training Loss: 0.05266294255852699\n",
      "Epoch 21719/30000 Training Loss: 0.05427202954888344\n",
      "Epoch 21720/30000 Training Loss: 0.04156225919723511\n",
      "Epoch 21721/30000 Training Loss: 0.06498895585536957\n",
      "Epoch 21722/30000 Training Loss: 0.048699092119932175\n",
      "Epoch 21723/30000 Training Loss: 0.04526210576295853\n",
      "Epoch 21724/30000 Training Loss: 0.040445875376462936\n",
      "Epoch 21725/30000 Training Loss: 0.0431203693151474\n",
      "Epoch 21726/30000 Training Loss: 0.0559382364153862\n",
      "Epoch 21727/30000 Training Loss: 0.053544897586107254\n",
      "Epoch 21728/30000 Training Loss: 0.05183924734592438\n",
      "Epoch 21729/30000 Training Loss: 0.05982435867190361\n",
      "Epoch 21730/30000 Training Loss: 0.05530758574604988\n",
      "Epoch 21731/30000 Training Loss: 0.05983426421880722\n",
      "Epoch 21732/30000 Training Loss: 0.0395364873111248\n",
      "Epoch 21733/30000 Training Loss: 0.04769567400217056\n",
      "Epoch 21734/30000 Training Loss: 0.0426129512488842\n",
      "Epoch 21735/30000 Training Loss: 0.057663872838020325\n",
      "Epoch 21736/30000 Training Loss: 0.04715616628527641\n",
      "Epoch 21737/30000 Training Loss: 0.046285852789878845\n",
      "Epoch 21738/30000 Training Loss: 0.0503343865275383\n",
      "Epoch 21739/30000 Training Loss: 0.03907399997115135\n",
      "Epoch 21740/30000 Training Loss: 0.05305697023868561\n",
      "Epoch 21741/30000 Training Loss: 0.052480075508356094\n",
      "Epoch 21742/30000 Training Loss: 0.03074871376156807\n",
      "Epoch 21743/30000 Training Loss: 0.03920992091298103\n",
      "Epoch 21744/30000 Training Loss: 0.03488260135054588\n",
      "Epoch 21745/30000 Training Loss: 0.04337609186768532\n",
      "Epoch 21746/30000 Training Loss: 0.04852549731731415\n",
      "Epoch 21747/30000 Training Loss: 0.06036042794585228\n",
      "Epoch 21748/30000 Training Loss: 0.04059493914246559\n",
      "Epoch 21749/30000 Training Loss: 0.041191600263118744\n",
      "Epoch 21750/30000 Training Loss: 0.049552492797374725\n",
      "Epoch 21751/30000 Training Loss: 0.05137905105948448\n",
      "Epoch 21752/30000 Training Loss: 0.04091076925396919\n",
      "Epoch 21753/30000 Training Loss: 0.059405650943517685\n",
      "Epoch 21754/30000 Training Loss: 0.042473722249269485\n",
      "Epoch 21755/30000 Training Loss: 0.045767370611429214\n",
      "Epoch 21756/30000 Training Loss: 0.044908881187438965\n",
      "Epoch 21757/30000 Training Loss: 0.04224172979593277\n",
      "Epoch 21758/30000 Training Loss: 0.05736066773533821\n",
      "Epoch 21759/30000 Training Loss: 0.04352303594350815\n",
      "Epoch 21760/30000 Training Loss: 0.04654215648770332\n",
      "Epoch 21761/30000 Training Loss: 0.032932933419942856\n",
      "Epoch 21762/30000 Training Loss: 0.05140020698308945\n",
      "Epoch 21763/30000 Training Loss: 0.06056387722492218\n",
      "Epoch 21764/30000 Training Loss: 0.0325775109231472\n",
      "Epoch 21765/30000 Training Loss: 0.0427173376083374\n",
      "Epoch 21766/30000 Training Loss: 0.045055340975522995\n",
      "Epoch 21767/30000 Training Loss: 0.058948710560798645\n",
      "Epoch 21768/30000 Training Loss: 0.05633728578686714\n",
      "Epoch 21769/30000 Training Loss: 0.05168607085943222\n",
      "Epoch 21770/30000 Training Loss: 0.053364891558885574\n",
      "Epoch 21771/30000 Training Loss: 0.03376052901148796\n",
      "Epoch 21772/30000 Training Loss: 0.05020539462566376\n",
      "Epoch 21773/30000 Training Loss: 0.047334857285022736\n",
      "Epoch 21774/30000 Training Loss: 0.048324424773454666\n",
      "Epoch 21775/30000 Training Loss: 0.05129685252904892\n",
      "Epoch 21776/30000 Training Loss: 0.04673072695732117\n",
      "Epoch 21777/30000 Training Loss: 0.03534085303544998\n",
      "Epoch 21778/30000 Training Loss: 0.030328352004289627\n",
      "Epoch 21779/30000 Training Loss: 0.04685841500759125\n",
      "Epoch 21780/30000 Training Loss: 0.05948329716920853\n",
      "Epoch 21781/30000 Training Loss: 0.04316701740026474\n",
      "Epoch 21782/30000 Training Loss: 0.04347306862473488\n",
      "Epoch 21783/30000 Training Loss: 0.056991372257471085\n",
      "Epoch 21784/30000 Training Loss: 0.03972436487674713\n",
      "Epoch 21785/30000 Training Loss: 0.04826301336288452\n",
      "Epoch 21786/30000 Training Loss: 0.0482441708445549\n",
      "Epoch 21787/30000 Training Loss: 0.06770888715982437\n",
      "Epoch 21788/30000 Training Loss: 0.03830278292298317\n",
      "Epoch 21789/30000 Training Loss: 0.04315578565001488\n",
      "Epoch 21790/30000 Training Loss: 0.032668400555849075\n",
      "Epoch 21791/30000 Training Loss: 0.04027346521615982\n",
      "Epoch 21792/30000 Training Loss: 0.033430084586143494\n",
      "Epoch 21793/30000 Training Loss: 0.04635736718773842\n",
      "Epoch 21794/30000 Training Loss: 0.04012731835246086\n",
      "Epoch 21795/30000 Training Loss: 0.04668526351451874\n",
      "Epoch 21796/30000 Training Loss: 0.05259573459625244\n",
      "Epoch 21797/30000 Training Loss: 0.0392942801117897\n",
      "Epoch 21798/30000 Training Loss: 0.041804343461990356\n",
      "Epoch 21799/30000 Training Loss: 0.0443006232380867\n",
      "Epoch 21800/30000 Training Loss: 0.050622884184122086\n",
      "Epoch 21800/30000 Validation Loss: 0.03254593536257744\n",
      "Epoch 21801/30000 Training Loss: 0.03786518797278404\n",
      "Epoch 21802/30000 Training Loss: 0.0351245254278183\n",
      "Epoch 21803/30000 Training Loss: 0.05176505446434021\n",
      "Epoch 21804/30000 Training Loss: 0.04098287224769592\n",
      "Epoch 21805/30000 Training Loss: 0.06691862642765045\n",
      "Epoch 21806/30000 Training Loss: 0.037773407995700836\n",
      "Epoch 21807/30000 Training Loss: 0.05384618416428566\n",
      "Epoch 21808/30000 Training Loss: 0.043171629309654236\n",
      "Epoch 21809/30000 Training Loss: 0.04082434996962547\n",
      "Epoch 21810/30000 Training Loss: 0.03696000576019287\n",
      "Epoch 21811/30000 Training Loss: 0.0395338349044323\n",
      "Epoch 21812/30000 Training Loss: 0.04118708148598671\n",
      "Epoch 21813/30000 Training Loss: 0.03853454813361168\n",
      "Epoch 21814/30000 Training Loss: 0.04582267627120018\n",
      "Epoch 21815/30000 Training Loss: 0.04830140620470047\n",
      "Epoch 21816/30000 Training Loss: 0.04861952364444733\n",
      "Epoch 21817/30000 Training Loss: 0.05235057324171066\n",
      "Epoch 21818/30000 Training Loss: 0.04329509660601616\n",
      "Epoch 21819/30000 Training Loss: 0.03670026361942291\n",
      "Epoch 21820/30000 Training Loss: 0.07810638844966888\n",
      "Epoch 21821/30000 Training Loss: 0.03480172157287598\n",
      "Epoch 21822/30000 Training Loss: 0.035454247146844864\n",
      "Epoch 21823/30000 Training Loss: 0.04060591012239456\n",
      "Epoch 21824/30000 Training Loss: 0.05756314843893051\n",
      "Epoch 21825/30000 Training Loss: 0.04601475968956947\n",
      "Epoch 21826/30000 Training Loss: 0.037550609558820724\n",
      "Epoch 21827/30000 Training Loss: 0.045969605445861816\n",
      "Epoch 21828/30000 Training Loss: 0.051612600684165955\n",
      "Epoch 21829/30000 Training Loss: 0.05077571049332619\n",
      "Epoch 21830/30000 Training Loss: 0.04417553171515465\n",
      "Epoch 21831/30000 Training Loss: 0.0518607422709465\n",
      "Epoch 21832/30000 Training Loss: 0.053153496235609055\n",
      "Epoch 21833/30000 Training Loss: 0.04634062200784683\n",
      "Epoch 21834/30000 Training Loss: 0.03209805488586426\n",
      "Epoch 21835/30000 Training Loss: 0.049239691346883774\n",
      "Epoch 21836/30000 Training Loss: 0.06150539591908455\n",
      "Epoch 21837/30000 Training Loss: 0.045092709362506866\n",
      "Epoch 21838/30000 Training Loss: 0.03640257939696312\n",
      "Epoch 21839/30000 Training Loss: 0.0494760125875473\n",
      "Epoch 21840/30000 Training Loss: 0.03971070796251297\n",
      "Epoch 21841/30000 Training Loss: 0.041493456810712814\n",
      "Epoch 21842/30000 Training Loss: 0.044198714196681976\n",
      "Epoch 21843/30000 Training Loss: 0.06587371975183487\n",
      "Epoch 21844/30000 Training Loss: 0.04770173132419586\n",
      "Epoch 21845/30000 Training Loss: 0.03453568369150162\n",
      "Epoch 21846/30000 Training Loss: 0.050867289304733276\n",
      "Epoch 21847/30000 Training Loss: 0.03464016318321228\n",
      "Epoch 21848/30000 Training Loss: 0.04809097200632095\n",
      "Epoch 21849/30000 Training Loss: 0.033639341592788696\n",
      "Epoch 21850/30000 Training Loss: 0.05399027466773987\n",
      "Epoch 21851/30000 Training Loss: 0.03429511561989784\n",
      "Epoch 21852/30000 Training Loss: 0.028992995619773865\n",
      "Epoch 21853/30000 Training Loss: 0.05647048354148865\n",
      "Epoch 21854/30000 Training Loss: 0.05588829517364502\n",
      "Epoch 21855/30000 Training Loss: 0.03340454772114754\n",
      "Epoch 21856/30000 Training Loss: 0.056824974715709686\n",
      "Epoch 21857/30000 Training Loss: 0.041915059089660645\n",
      "Epoch 21858/30000 Training Loss: 0.03393317013978958\n",
      "Epoch 21859/30000 Training Loss: 0.05625823512673378\n",
      "Epoch 21860/30000 Training Loss: 0.04980316385626793\n",
      "Epoch 21861/30000 Training Loss: 0.053000226616859436\n",
      "Epoch 21862/30000 Training Loss: 0.039752550423145294\n",
      "Epoch 21863/30000 Training Loss: 0.04719530791044235\n",
      "Epoch 21864/30000 Training Loss: 0.03376246616244316\n",
      "Epoch 21865/30000 Training Loss: 0.05217671021819115\n",
      "Epoch 21866/30000 Training Loss: 0.036989472806453705\n",
      "Epoch 21867/30000 Training Loss: 0.051176030188798904\n",
      "Epoch 21868/30000 Training Loss: 0.0403621532022953\n",
      "Epoch 21869/30000 Training Loss: 0.04936820641160011\n",
      "Epoch 21870/30000 Training Loss: 0.03308050334453583\n",
      "Epoch 21871/30000 Training Loss: 0.048111241310834885\n",
      "Epoch 21872/30000 Training Loss: 0.04863857477903366\n",
      "Epoch 21873/30000 Training Loss: 0.05120953917503357\n",
      "Epoch 21874/30000 Training Loss: 0.04951940104365349\n",
      "Epoch 21875/30000 Training Loss: 0.04631589725613594\n",
      "Epoch 21876/30000 Training Loss: 0.06720049679279327\n",
      "Epoch 21877/30000 Training Loss: 0.028125137090682983\n",
      "Epoch 21878/30000 Training Loss: 0.04041562229394913\n",
      "Epoch 21879/30000 Training Loss: 0.03289904072880745\n",
      "Epoch 21880/30000 Training Loss: 0.03633161261677742\n",
      "Epoch 21881/30000 Training Loss: 0.04922236502170563\n",
      "Epoch 21882/30000 Training Loss: 0.04791334271430969\n",
      "Epoch 21883/30000 Training Loss: 0.05294138193130493\n",
      "Epoch 21884/30000 Training Loss: 0.048636287450790405\n",
      "Epoch 21885/30000 Training Loss: 0.05848855897784233\n",
      "Epoch 21886/30000 Training Loss: 0.04243624210357666\n",
      "Epoch 21887/30000 Training Loss: 0.04369988292455673\n",
      "Epoch 21888/30000 Training Loss: 0.0435948520898819\n",
      "Epoch 21889/30000 Training Loss: 0.049616772681474686\n",
      "Epoch 21890/30000 Training Loss: 0.0340474434196949\n",
      "Epoch 21891/30000 Training Loss: 0.03558213636279106\n",
      "Epoch 21892/30000 Training Loss: 0.06072477251291275\n",
      "Epoch 21893/30000 Training Loss: 0.039977218955755234\n",
      "Epoch 21894/30000 Training Loss: 0.04925069212913513\n",
      "Epoch 21895/30000 Training Loss: 0.04223795235157013\n",
      "Epoch 21896/30000 Training Loss: 0.04794986546039581\n",
      "Epoch 21897/30000 Training Loss: 0.05419223755598068\n",
      "Epoch 21898/30000 Training Loss: 0.03656412661075592\n",
      "Epoch 21899/30000 Training Loss: 0.03539961948990822\n",
      "Epoch 21900/30000 Training Loss: 0.043217770755290985\n",
      "Epoch 21900/30000 Validation Loss: 0.04241182282567024\n",
      "Epoch 21901/30000 Training Loss: 0.046822723001241684\n",
      "Epoch 21902/30000 Training Loss: 0.04575465992093086\n",
      "Epoch 21903/30000 Training Loss: 0.041836414486169815\n",
      "Epoch 21904/30000 Training Loss: 0.05237205699086189\n",
      "Epoch 21905/30000 Training Loss: 0.050347700715065\n",
      "Epoch 21906/30000 Training Loss: 0.060141563415527344\n",
      "Epoch 21907/30000 Training Loss: 0.05722120404243469\n",
      "Epoch 21908/30000 Training Loss: 0.06177189201116562\n",
      "Epoch 21909/30000 Training Loss: 0.055770259350538254\n",
      "Epoch 21910/30000 Training Loss: 0.033587608486413956\n",
      "Epoch 21911/30000 Training Loss: 0.04377471283078194\n",
      "Epoch 21912/30000 Training Loss: 0.0600750669836998\n",
      "Epoch 21913/30000 Training Loss: 0.04983426257967949\n",
      "Epoch 21914/30000 Training Loss: 0.056122347712516785\n",
      "Epoch 21915/30000 Training Loss: 0.03349033743143082\n",
      "Epoch 21916/30000 Training Loss: 0.06208713352680206\n",
      "Epoch 21917/30000 Training Loss: 0.06324727088212967\n",
      "Epoch 21918/30000 Training Loss: 0.055052462965250015\n",
      "Epoch 21919/30000 Training Loss: 0.06038249656558037\n",
      "Epoch 21920/30000 Training Loss: 0.0588519461452961\n",
      "Epoch 21921/30000 Training Loss: 0.05627065896987915\n",
      "Epoch 21922/30000 Training Loss: 0.03931470587849617\n",
      "Epoch 21923/30000 Training Loss: 0.05061794072389603\n",
      "Epoch 21924/30000 Training Loss: 0.05613496154546738\n",
      "Epoch 21925/30000 Training Loss: 0.050702597945928574\n",
      "Epoch 21926/30000 Training Loss: 0.04042743518948555\n",
      "Epoch 21927/30000 Training Loss: 0.057008519768714905\n",
      "Epoch 21928/30000 Training Loss: 0.0529913455247879\n",
      "Epoch 21929/30000 Training Loss: 0.0530276820063591\n",
      "Epoch 21930/30000 Training Loss: 0.041999250650405884\n",
      "Epoch 21931/30000 Training Loss: 0.04055449366569519\n",
      "Epoch 21932/30000 Training Loss: 0.034899987280368805\n",
      "Epoch 21933/30000 Training Loss: 0.03155110403895378\n",
      "Epoch 21934/30000 Training Loss: 0.06457564234733582\n",
      "Epoch 21935/30000 Training Loss: 0.037124328315258026\n",
      "Epoch 21936/30000 Training Loss: 0.03958670049905777\n",
      "Epoch 21937/30000 Training Loss: 0.02545846253633499\n",
      "Epoch 21938/30000 Training Loss: 0.050074201077222824\n",
      "Epoch 21939/30000 Training Loss: 0.036068376153707504\n",
      "Epoch 21940/30000 Training Loss: 0.04511582478880882\n",
      "Epoch 21941/30000 Training Loss: 0.04062805324792862\n",
      "Epoch 21942/30000 Training Loss: 0.05497260019183159\n",
      "Epoch 21943/30000 Training Loss: 0.04021673649549484\n",
      "Epoch 21944/30000 Training Loss: 0.04350138455629349\n",
      "Epoch 21945/30000 Training Loss: 0.03635350614786148\n",
      "Epoch 21946/30000 Training Loss: 0.04775005951523781\n",
      "Epoch 21947/30000 Training Loss: 0.04689004272222519\n",
      "Epoch 21948/30000 Training Loss: 0.039935559034347534\n",
      "Epoch 21949/30000 Training Loss: 0.04156491532921791\n",
      "Epoch 21950/30000 Training Loss: 0.04979420825839043\n",
      "Epoch 21951/30000 Training Loss: 0.03800232335925102\n",
      "Epoch 21952/30000 Training Loss: 0.06614889204502106\n",
      "Epoch 21953/30000 Training Loss: 0.03793851286172867\n",
      "Epoch 21954/30000 Training Loss: 0.047046080231666565\n",
      "Epoch 21955/30000 Training Loss: 0.049513813108205795\n",
      "Epoch 21956/30000 Training Loss: 0.044612180441617966\n",
      "Epoch 21957/30000 Training Loss: 0.036408230662345886\n",
      "Epoch 21958/30000 Training Loss: 0.050612032413482666\n",
      "Epoch 21959/30000 Training Loss: 0.04873987287282944\n",
      "Epoch 21960/30000 Training Loss: 0.03396663814783096\n",
      "Epoch 21961/30000 Training Loss: 0.0414452962577343\n",
      "Epoch 21962/30000 Training Loss: 0.045841656625270844\n",
      "Epoch 21963/30000 Training Loss: 0.055088333785533905\n",
      "Epoch 21964/30000 Training Loss: 0.03674148768186569\n",
      "Epoch 21965/30000 Training Loss: 0.03885430470108986\n",
      "Epoch 21966/30000 Training Loss: 0.042334847152233124\n",
      "Epoch 21967/30000 Training Loss: 0.04717208072543144\n",
      "Epoch 21968/30000 Training Loss: 0.05263819545507431\n",
      "Epoch 21969/30000 Training Loss: 0.046257637441158295\n",
      "Epoch 21970/30000 Training Loss: 0.06850630789995193\n",
      "Epoch 21971/30000 Training Loss: 0.031778763979673386\n",
      "Epoch 21972/30000 Training Loss: 0.04113536700606346\n",
      "Epoch 21973/30000 Training Loss: 0.05388101562857628\n",
      "Epoch 21974/30000 Training Loss: 0.041090115904808044\n",
      "Epoch 21975/30000 Training Loss: 0.03779848292469978\n",
      "Epoch 21976/30000 Training Loss: 0.050637125968933105\n",
      "Epoch 21977/30000 Training Loss: 0.058669865131378174\n",
      "Epoch 21978/30000 Training Loss: 0.03848440200090408\n",
      "Epoch 21979/30000 Training Loss: 0.05777087062597275\n",
      "Epoch 21980/30000 Training Loss: 0.04962243139743805\n",
      "Epoch 21981/30000 Training Loss: 0.046538740396499634\n",
      "Epoch 21982/30000 Training Loss: 0.0468432679772377\n",
      "Epoch 21983/30000 Training Loss: 0.04627668857574463\n",
      "Epoch 21984/30000 Training Loss: 0.04891865700483322\n",
      "Epoch 21985/30000 Training Loss: 0.04356779158115387\n",
      "Epoch 21986/30000 Training Loss: 0.04252372682094574\n",
      "Epoch 21987/30000 Training Loss: 0.0369705855846405\n",
      "Epoch 21988/30000 Training Loss: 0.04025484621524811\n",
      "Epoch 21989/30000 Training Loss: 0.0556907057762146\n",
      "Epoch 21990/30000 Training Loss: 0.03523468226194382\n",
      "Epoch 21991/30000 Training Loss: 0.04990033060312271\n",
      "Epoch 21992/30000 Training Loss: 0.03318380191922188\n",
      "Epoch 21993/30000 Training Loss: 0.05121859535574913\n",
      "Epoch 21994/30000 Training Loss: 0.03773977980017662\n",
      "Epoch 21995/30000 Training Loss: 0.04027552157640457\n",
      "Epoch 21996/30000 Training Loss: 0.049160558730363846\n",
      "Epoch 21997/30000 Training Loss: 0.041006289422512054\n",
      "Epoch 21998/30000 Training Loss: 0.048315443098545074\n",
      "Epoch 21999/30000 Training Loss: 0.04973709210753441\n",
      "Epoch 22000/30000 Training Loss: 0.035326093435287476\n",
      "Epoch 22000/30000 Validation Loss: 0.056618914008140564\n",
      "Epoch 22001/30000 Training Loss: 0.03362276032567024\n",
      "Epoch 22002/30000 Training Loss: 0.04879238083958626\n",
      "Epoch 22003/30000 Training Loss: 0.03831665590405464\n",
      "Epoch 22004/30000 Training Loss: 0.06377775967121124\n",
      "Epoch 22005/30000 Training Loss: 0.05114679038524628\n",
      "Epoch 22006/30000 Training Loss: 0.0559416189789772\n",
      "Epoch 22007/30000 Training Loss: 0.04314066469669342\n",
      "Epoch 22008/30000 Training Loss: 0.04110518842935562\n",
      "Epoch 22009/30000 Training Loss: 0.05376958101987839\n",
      "Epoch 22010/30000 Training Loss: 0.06176217645406723\n",
      "Epoch 22011/30000 Training Loss: 0.047028109431266785\n",
      "Epoch 22012/30000 Training Loss: 0.05501385033130646\n",
      "Epoch 22013/30000 Training Loss: 0.04371582716703415\n",
      "Epoch 22014/30000 Training Loss: 0.05110665038228035\n",
      "Epoch 22015/30000 Training Loss: 0.05189094319939613\n",
      "Epoch 22016/30000 Training Loss: 0.0446118526160717\n",
      "Epoch 22017/30000 Training Loss: 0.04762394726276398\n",
      "Epoch 22018/30000 Training Loss: 0.03963146731257439\n",
      "Epoch 22019/30000 Training Loss: 0.05562659353017807\n",
      "Epoch 22020/30000 Training Loss: 0.04575077071785927\n",
      "Epoch 22021/30000 Training Loss: 0.041116565465927124\n",
      "Epoch 22022/30000 Training Loss: 0.0516771525144577\n",
      "Epoch 22023/30000 Training Loss: 0.05083593726158142\n",
      "Epoch 22024/30000 Training Loss: 0.05621238797903061\n",
      "Epoch 22025/30000 Training Loss: 0.0516495555639267\n",
      "Epoch 22026/30000 Training Loss: 0.04324333742260933\n",
      "Epoch 22027/30000 Training Loss: 0.050402894616127014\n",
      "Epoch 22028/30000 Training Loss: 0.061083272099494934\n",
      "Epoch 22029/30000 Training Loss: 0.037539251148700714\n",
      "Epoch 22030/30000 Training Loss: 0.05435434728860855\n",
      "Epoch 22031/30000 Training Loss: 0.0489928275346756\n",
      "Epoch 22032/30000 Training Loss: 0.03500441461801529\n",
      "Epoch 22033/30000 Training Loss: 0.03589766472578049\n",
      "Epoch 22034/30000 Training Loss: 0.06408713757991791\n",
      "Epoch 22035/30000 Training Loss: 0.045057911425828934\n",
      "Epoch 22036/30000 Training Loss: 0.048256512731313705\n",
      "Epoch 22037/30000 Training Loss: 0.052044011652469635\n",
      "Epoch 22038/30000 Training Loss: 0.04277230054140091\n",
      "Epoch 22039/30000 Training Loss: 0.0549827516078949\n",
      "Epoch 22040/30000 Training Loss: 0.04362846538424492\n",
      "Epoch 22041/30000 Training Loss: 0.049059994518756866\n",
      "Epoch 22042/30000 Training Loss: 0.060660190880298615\n",
      "Epoch 22043/30000 Training Loss: 0.035730279982089996\n",
      "Epoch 22044/30000 Training Loss: 0.046371087431907654\n",
      "Epoch 22045/30000 Training Loss: 0.05686752498149872\n",
      "Epoch 22046/30000 Training Loss: 0.04310104250907898\n",
      "Epoch 22047/30000 Training Loss: 0.051344554871320724\n",
      "Epoch 22048/30000 Training Loss: 0.04659264162182808\n",
      "Epoch 22049/30000 Training Loss: 0.05855058506131172\n",
      "Epoch 22050/30000 Training Loss: 0.050466377288103104\n",
      "Epoch 22051/30000 Training Loss: 0.05545749142765999\n",
      "Epoch 22052/30000 Training Loss: 0.045552417635917664\n",
      "Epoch 22053/30000 Training Loss: 0.06199807673692703\n",
      "Epoch 22054/30000 Training Loss: 0.04038847237825394\n",
      "Epoch 22055/30000 Training Loss: 0.045402705669403076\n",
      "Epoch 22056/30000 Training Loss: 0.04489540681242943\n",
      "Epoch 22057/30000 Training Loss: 0.057140521705150604\n",
      "Epoch 22058/30000 Training Loss: 0.05071916803717613\n",
      "Epoch 22059/30000 Training Loss: 0.06430009007453918\n",
      "Epoch 22060/30000 Training Loss: 0.051180630922317505\n",
      "Epoch 22061/30000 Training Loss: 0.03165465220808983\n",
      "Epoch 22062/30000 Training Loss: 0.04259466007351875\n",
      "Epoch 22063/30000 Training Loss: 0.040139805525541306\n",
      "Epoch 22064/30000 Training Loss: 0.04635854810476303\n",
      "Epoch 22065/30000 Training Loss: 0.05470016598701477\n",
      "Epoch 22066/30000 Training Loss: 0.045277077704668045\n",
      "Epoch 22067/30000 Training Loss: 0.03624835982918739\n",
      "Epoch 22068/30000 Training Loss: 0.04759794473648071\n",
      "Epoch 22069/30000 Training Loss: 0.04659230634570122\n",
      "Epoch 22070/30000 Training Loss: 0.037419307976961136\n",
      "Epoch 22071/30000 Training Loss: 0.07266533374786377\n",
      "Epoch 22072/30000 Training Loss: 0.057309169322252274\n",
      "Epoch 22073/30000 Training Loss: 0.03899022191762924\n",
      "Epoch 22074/30000 Training Loss: 0.04148152098059654\n",
      "Epoch 22075/30000 Training Loss: 0.03948348015546799\n",
      "Epoch 22076/30000 Training Loss: 0.0315299890935421\n",
      "Epoch 22077/30000 Training Loss: 0.04459923505783081\n",
      "Epoch 22078/30000 Training Loss: 0.04141322150826454\n",
      "Epoch 22079/30000 Training Loss: 0.04070623219013214\n",
      "Epoch 22080/30000 Training Loss: 0.0714016780257225\n",
      "Epoch 22081/30000 Training Loss: 0.062296293675899506\n",
      "Epoch 22082/30000 Training Loss: 0.05817212536931038\n",
      "Epoch 22083/30000 Training Loss: 0.046718988567590714\n",
      "Epoch 22084/30000 Training Loss: 0.05279462784528732\n",
      "Epoch 22085/30000 Training Loss: 0.054959382861852646\n",
      "Epoch 22086/30000 Training Loss: 0.04382205381989479\n",
      "Epoch 22087/30000 Training Loss: 0.04723614454269409\n",
      "Epoch 22088/30000 Training Loss: 0.03654254227876663\n",
      "Epoch 22089/30000 Training Loss: 0.03898144140839577\n",
      "Epoch 22090/30000 Training Loss: 0.05211310461163521\n",
      "Epoch 22091/30000 Training Loss: 0.048317648470401764\n",
      "Epoch 22092/30000 Training Loss: 0.04935048148036003\n",
      "Epoch 22093/30000 Training Loss: 0.06168169155716896\n",
      "Epoch 22094/30000 Training Loss: 0.04736632853746414\n",
      "Epoch 22095/30000 Training Loss: 0.04845750331878662\n",
      "Epoch 22096/30000 Training Loss: 0.035697221755981445\n",
      "Epoch 22097/30000 Training Loss: 0.03093898668885231\n",
      "Epoch 22098/30000 Training Loss: 0.06282078474760056\n",
      "Epoch 22099/30000 Training Loss: 0.038327280431985855\n",
      "Epoch 22100/30000 Training Loss: 0.044544219970703125\n",
      "Epoch 22100/30000 Validation Loss: 0.03207267448306084\n",
      "Epoch 22101/30000 Training Loss: 0.043579794466495514\n",
      "Epoch 22102/30000 Training Loss: 0.0426909402012825\n",
      "Epoch 22103/30000 Training Loss: 0.0405476912856102\n",
      "Epoch 22104/30000 Training Loss: 0.045859962701797485\n",
      "Epoch 22105/30000 Training Loss: 0.05052761733531952\n",
      "Epoch 22106/30000 Training Loss: 0.047535043209791183\n",
      "Epoch 22107/30000 Training Loss: 0.04971391335129738\n",
      "Epoch 22108/30000 Training Loss: 0.03983931988477707\n",
      "Epoch 22109/30000 Training Loss: 0.04069748893380165\n",
      "Epoch 22110/30000 Training Loss: 0.049912936985492706\n",
      "Epoch 22111/30000 Training Loss: 0.031241290271282196\n",
      "Epoch 22112/30000 Training Loss: 0.049770962446928024\n",
      "Epoch 22113/30000 Training Loss: 0.05057473108172417\n",
      "Epoch 22114/30000 Training Loss: 0.039838094264268875\n",
      "Epoch 22115/30000 Training Loss: 0.04744329676032066\n",
      "Epoch 22116/30000 Training Loss: 0.04230666905641556\n",
      "Epoch 22117/30000 Training Loss: 0.04874400421977043\n",
      "Epoch 22118/30000 Training Loss: 0.029213251546025276\n",
      "Epoch 22119/30000 Training Loss: 0.060379114001989365\n",
      "Epoch 22120/30000 Training Loss: 0.040794242173433304\n",
      "Epoch 22121/30000 Training Loss: 0.057495538145303726\n",
      "Epoch 22122/30000 Training Loss: 0.03862636536359787\n",
      "Epoch 22123/30000 Training Loss: 0.041334882378578186\n",
      "Epoch 22124/30000 Training Loss: 0.04640039801597595\n",
      "Epoch 22125/30000 Training Loss: 0.04593333601951599\n",
      "Epoch 22126/30000 Training Loss: 0.04470415040850639\n",
      "Epoch 22127/30000 Training Loss: 0.05761353671550751\n",
      "Epoch 22128/30000 Training Loss: 0.04615704342722893\n",
      "Epoch 22129/30000 Training Loss: 0.05643034353852272\n",
      "Epoch 22130/30000 Training Loss: 0.03952823206782341\n",
      "Epoch 22131/30000 Training Loss: 0.044978611171245575\n",
      "Epoch 22132/30000 Training Loss: 0.0531056672334671\n",
      "Epoch 22133/30000 Training Loss: 0.0444953516125679\n",
      "Epoch 22134/30000 Training Loss: 0.05020967870950699\n",
      "Epoch 22135/30000 Training Loss: 0.05313003808259964\n",
      "Epoch 22136/30000 Training Loss: 0.04032858833670616\n",
      "Epoch 22137/30000 Training Loss: 0.036729596555233\n",
      "Epoch 22138/30000 Training Loss: 0.04165066033601761\n",
      "Epoch 22139/30000 Training Loss: 0.037398967891931534\n",
      "Epoch 22140/30000 Training Loss: 0.047750476747751236\n",
      "Epoch 22141/30000 Training Loss: 0.04733537137508392\n",
      "Epoch 22142/30000 Training Loss: 0.05094509199261665\n",
      "Epoch 22143/30000 Training Loss: 0.037982236593961716\n",
      "Epoch 22144/30000 Training Loss: 0.036889154464006424\n",
      "Epoch 22145/30000 Training Loss: 0.04836321994662285\n",
      "Epoch 22146/30000 Training Loss: 0.046961165964603424\n",
      "Epoch 22147/30000 Training Loss: 0.04396573826670647\n",
      "Epoch 22148/30000 Training Loss: 0.06722983717918396\n",
      "Epoch 22149/30000 Training Loss: 0.050504982471466064\n",
      "Epoch 22150/30000 Training Loss: 0.04884001612663269\n",
      "Epoch 22151/30000 Training Loss: 0.04704362526535988\n",
      "Epoch 22152/30000 Training Loss: 0.03585846722126007\n",
      "Epoch 22153/30000 Training Loss: 0.05181562155485153\n",
      "Epoch 22154/30000 Training Loss: 0.04970212280750275\n",
      "Epoch 22155/30000 Training Loss: 0.03639794886112213\n",
      "Epoch 22156/30000 Training Loss: 0.051731567829847336\n",
      "Epoch 22157/30000 Training Loss: 0.03627324476838112\n",
      "Epoch 22158/30000 Training Loss: 0.04756510630249977\n",
      "Epoch 22159/30000 Training Loss: 0.03896937891840935\n",
      "Epoch 22160/30000 Training Loss: 0.06679412722587585\n",
      "Epoch 22161/30000 Training Loss: 0.04890720918774605\n",
      "Epoch 22162/30000 Training Loss: 0.04842542111873627\n",
      "Epoch 22163/30000 Training Loss: 0.040134020149707794\n",
      "Epoch 22164/30000 Training Loss: 0.045809198170900345\n",
      "Epoch 22165/30000 Training Loss: 0.05088333040475845\n",
      "Epoch 22166/30000 Training Loss: 0.05863320827484131\n",
      "Epoch 22167/30000 Training Loss: 0.052993517369031906\n",
      "Epoch 22168/30000 Training Loss: 0.048860132694244385\n",
      "Epoch 22169/30000 Training Loss: 0.05250338464975357\n",
      "Epoch 22170/30000 Training Loss: 0.048839375376701355\n",
      "Epoch 22171/30000 Training Loss: 0.03749161213636398\n",
      "Epoch 22172/30000 Training Loss: 0.04707695543766022\n",
      "Epoch 22173/30000 Training Loss: 0.04720943793654442\n",
      "Epoch 22174/30000 Training Loss: 0.04512424394488335\n",
      "Epoch 22175/30000 Training Loss: 0.06411311775445938\n",
      "Epoch 22176/30000 Training Loss: 0.052816785871982574\n",
      "Epoch 22177/30000 Training Loss: 0.03725014999508858\n",
      "Epoch 22178/30000 Training Loss: 0.03212687000632286\n",
      "Epoch 22179/30000 Training Loss: 0.03437170758843422\n",
      "Epoch 22180/30000 Training Loss: 0.045666199177503586\n",
      "Epoch 22181/30000 Training Loss: 0.046192966401576996\n",
      "Epoch 22182/30000 Training Loss: 0.04484889656305313\n",
      "Epoch 22183/30000 Training Loss: 0.04507092013955116\n",
      "Epoch 22184/30000 Training Loss: 0.042483001947402954\n",
      "Epoch 22185/30000 Training Loss: 0.03807869926095009\n",
      "Epoch 22186/30000 Training Loss: 0.07283125072717667\n",
      "Epoch 22187/30000 Training Loss: 0.04311887547373772\n",
      "Epoch 22188/30000 Training Loss: 0.03818831965327263\n",
      "Epoch 22189/30000 Training Loss: 0.05140373855829239\n",
      "Epoch 22190/30000 Training Loss: 0.03848392516374588\n",
      "Epoch 22191/30000 Training Loss: 0.057600557804107666\n",
      "Epoch 22192/30000 Training Loss: 0.04051263630390167\n",
      "Epoch 22193/30000 Training Loss: 0.034423213452100754\n",
      "Epoch 22194/30000 Training Loss: 0.04434286430478096\n",
      "Epoch 22195/30000 Training Loss: 0.053667258471250534\n",
      "Epoch 22196/30000 Training Loss: 0.04745626077055931\n",
      "Epoch 22197/30000 Training Loss: 0.06617400795221329\n",
      "Epoch 22198/30000 Training Loss: 0.03991515189409256\n",
      "Epoch 22199/30000 Training Loss: 0.03472428396344185\n",
      "Epoch 22200/30000 Training Loss: 0.04235631972551346\n",
      "Epoch 22200/30000 Validation Loss: 0.04641260206699371\n",
      "Epoch 22201/30000 Training Loss: 0.047085974365472794\n",
      "Epoch 22202/30000 Training Loss: 0.03948093578219414\n",
      "Epoch 22203/30000 Training Loss: 0.053619809448719025\n",
      "Epoch 22204/30000 Training Loss: 0.06549014151096344\n",
      "Epoch 22205/30000 Training Loss: 0.05129081755876541\n",
      "Epoch 22206/30000 Training Loss: 0.045356255024671555\n",
      "Epoch 22207/30000 Training Loss: 0.05069144070148468\n",
      "Epoch 22208/30000 Training Loss: 0.030074341222643852\n",
      "Epoch 22209/30000 Training Loss: 0.0395018607378006\n",
      "Epoch 22210/30000 Training Loss: 0.03415042906999588\n",
      "Epoch 22211/30000 Training Loss: 0.04406053572893143\n",
      "Epoch 22212/30000 Training Loss: 0.027349624782800674\n",
      "Epoch 22213/30000 Training Loss: 0.04641726613044739\n",
      "Epoch 22214/30000 Training Loss: 0.04135836660861969\n",
      "Epoch 22215/30000 Training Loss: 0.050294049084186554\n",
      "Epoch 22216/30000 Training Loss: 0.07143480330705643\n",
      "Epoch 22217/30000 Training Loss: 0.039457544684410095\n",
      "Epoch 22218/30000 Training Loss: 0.05057207867503166\n",
      "Epoch 22219/30000 Training Loss: 0.03576941043138504\n",
      "Epoch 22220/30000 Training Loss: 0.05836540833115578\n",
      "Epoch 22221/30000 Training Loss: 0.03986546769738197\n",
      "Epoch 22222/30000 Training Loss: 0.05295626074075699\n",
      "Epoch 22223/30000 Training Loss: 0.0396394245326519\n",
      "Epoch 22224/30000 Training Loss: 0.03835297375917435\n",
      "Epoch 22225/30000 Training Loss: 0.040443304926157\n",
      "Epoch 22226/30000 Training Loss: 0.04047831892967224\n",
      "Epoch 22227/30000 Training Loss: 0.04500678926706314\n",
      "Epoch 22228/30000 Training Loss: 0.036072224378585815\n",
      "Epoch 22229/30000 Training Loss: 0.043582964688539505\n",
      "Epoch 22230/30000 Training Loss: 0.04381496086716652\n",
      "Epoch 22231/30000 Training Loss: 0.029053952544927597\n",
      "Epoch 22232/30000 Training Loss: 0.054043129086494446\n",
      "Epoch 22233/30000 Training Loss: 0.032974790781736374\n",
      "Epoch 22234/30000 Training Loss: 0.05427143722772598\n",
      "Epoch 22235/30000 Training Loss: 0.03734659403562546\n",
      "Epoch 22236/30000 Training Loss: 0.059335291385650635\n",
      "Epoch 22237/30000 Training Loss: 0.0553942546248436\n",
      "Epoch 22238/30000 Training Loss: 0.0449100062251091\n",
      "Epoch 22239/30000 Training Loss: 0.05200064927339554\n",
      "Epoch 22240/30000 Training Loss: 0.058025721460580826\n",
      "Epoch 22241/30000 Training Loss: 0.0471748486161232\n",
      "Epoch 22242/30000 Training Loss: 0.043227724730968475\n",
      "Epoch 22243/30000 Training Loss: 0.05912692844867706\n",
      "Epoch 22244/30000 Training Loss: 0.048069775104522705\n",
      "Epoch 22245/30000 Training Loss: 0.06338867545127869\n",
      "Epoch 22246/30000 Training Loss: 0.04537453502416611\n",
      "Epoch 22247/30000 Training Loss: 0.04596780985593796\n",
      "Epoch 22248/30000 Training Loss: 0.039196379482746124\n",
      "Epoch 22249/30000 Training Loss: 0.05113328993320465\n",
      "Epoch 22250/30000 Training Loss: 0.0525839701294899\n",
      "Epoch 22251/30000 Training Loss: 0.039890680462121964\n",
      "Epoch 22252/30000 Training Loss: 0.039691612124443054\n",
      "Epoch 22253/30000 Training Loss: 0.044812556356191635\n",
      "Epoch 22254/30000 Training Loss: 0.06817427277565002\n",
      "Epoch 22255/30000 Training Loss: 0.0444994755089283\n",
      "Epoch 22256/30000 Training Loss: 0.045209065079689026\n",
      "Epoch 22257/30000 Training Loss: 0.036824941635131836\n",
      "Epoch 22258/30000 Training Loss: 0.04331735149025917\n",
      "Epoch 22259/30000 Training Loss: 0.04148831591010094\n",
      "Epoch 22260/30000 Training Loss: 0.04763762652873993\n",
      "Epoch 22261/30000 Training Loss: 0.046946391463279724\n",
      "Epoch 22262/30000 Training Loss: 0.032689888030290604\n",
      "Epoch 22263/30000 Training Loss: 0.04069506749510765\n",
      "Epoch 22264/30000 Training Loss: 0.03832566738128662\n",
      "Epoch 22265/30000 Training Loss: 0.052565157413482666\n",
      "Epoch 22266/30000 Training Loss: 0.05495782941579819\n",
      "Epoch 22267/30000 Training Loss: 0.061963796615600586\n",
      "Epoch 22268/30000 Training Loss: 0.04780486226081848\n",
      "Epoch 22269/30000 Training Loss: 0.05578151345252991\n",
      "Epoch 22270/30000 Training Loss: 0.06312641501426697\n",
      "Epoch 22271/30000 Training Loss: 0.047928422689437866\n",
      "Epoch 22272/30000 Training Loss: 0.037930987775325775\n",
      "Epoch 22273/30000 Training Loss: 0.06673160195350647\n",
      "Epoch 22274/30000 Training Loss: 0.0397922582924366\n",
      "Epoch 22275/30000 Training Loss: 0.05508662760257721\n",
      "Epoch 22276/30000 Training Loss: 0.050582487136125565\n",
      "Epoch 22277/30000 Training Loss: 0.037730440497398376\n",
      "Epoch 22278/30000 Training Loss: 0.05059616267681122\n",
      "Epoch 22279/30000 Training Loss: 0.06430251896381378\n",
      "Epoch 22280/30000 Training Loss: 0.02978457510471344\n",
      "Epoch 22281/30000 Training Loss: 0.05314375087618828\n",
      "Epoch 22282/30000 Training Loss: 0.04477329179644585\n",
      "Epoch 22283/30000 Training Loss: 0.06794213503599167\n",
      "Epoch 22284/30000 Training Loss: 0.0425921268761158\n",
      "Epoch 22285/30000 Training Loss: 0.03817501291632652\n",
      "Epoch 22286/30000 Training Loss: 0.03648293763399124\n",
      "Epoch 22287/30000 Training Loss: 0.044363051652908325\n",
      "Epoch 22288/30000 Training Loss: 0.05415467172861099\n",
      "Epoch 22289/30000 Training Loss: 0.03726654499769211\n",
      "Epoch 22290/30000 Training Loss: 0.03477988392114639\n",
      "Epoch 22291/30000 Training Loss: 0.043649230152368546\n",
      "Epoch 22292/30000 Training Loss: 0.04455224424600601\n",
      "Epoch 22293/30000 Training Loss: 0.033063989132642746\n",
      "Epoch 22294/30000 Training Loss: 0.05366239696741104\n",
      "Epoch 22295/30000 Training Loss: 0.03551406413316727\n",
      "Epoch 22296/30000 Training Loss: 0.04161543771624565\n",
      "Epoch 22297/30000 Training Loss: 0.04225984215736389\n",
      "Epoch 22298/30000 Training Loss: 0.02791549265384674\n",
      "Epoch 22299/30000 Training Loss: 0.03735576197504997\n",
      "Epoch 22300/30000 Training Loss: 0.04922938719391823\n",
      "Epoch 22300/30000 Validation Loss: 0.052762217819690704\n",
      "Epoch 22301/30000 Training Loss: 0.06097209081053734\n",
      "Epoch 22302/30000 Training Loss: 0.04936935380101204\n",
      "Epoch 22303/30000 Training Loss: 0.04992949962615967\n",
      "Epoch 22304/30000 Training Loss: 0.0402553454041481\n",
      "Epoch 22305/30000 Training Loss: 0.03817255049943924\n",
      "Epoch 22306/30000 Training Loss: 0.04801241308450699\n",
      "Epoch 22307/30000 Training Loss: 0.04607686027884483\n",
      "Epoch 22308/30000 Training Loss: 0.05956429988145828\n",
      "Epoch 22309/30000 Training Loss: 0.04332003369927406\n",
      "Epoch 22310/30000 Training Loss: 0.046366263180971146\n",
      "Epoch 22311/30000 Training Loss: 0.044278115034103394\n",
      "Epoch 22312/30000 Training Loss: 0.06044277921319008\n",
      "Epoch 22313/30000 Training Loss: 0.04828377068042755\n",
      "Epoch 22314/30000 Training Loss: 0.046802882105112076\n",
      "Epoch 22315/30000 Training Loss: 0.06274615228176117\n",
      "Epoch 22316/30000 Training Loss: 0.05235991254448891\n",
      "Epoch 22317/30000 Training Loss: 0.047676023095846176\n",
      "Epoch 22318/30000 Training Loss: 0.04390674829483032\n",
      "Epoch 22319/30000 Training Loss: 0.05547312647104263\n",
      "Epoch 22320/30000 Training Loss: 0.045053426176309586\n",
      "Epoch 22321/30000 Training Loss: 0.04861538112163544\n",
      "Epoch 22322/30000 Training Loss: 0.04300829395651817\n",
      "Epoch 22323/30000 Training Loss: 0.04604461044073105\n",
      "Epoch 22324/30000 Training Loss: 0.03670434281229973\n",
      "Epoch 22325/30000 Training Loss: 0.02889716811478138\n",
      "Epoch 22326/30000 Training Loss: 0.03730184957385063\n",
      "Epoch 22327/30000 Training Loss: 0.030905472114682198\n",
      "Epoch 22328/30000 Training Loss: 0.03993532061576843\n",
      "Epoch 22329/30000 Training Loss: 0.04797380045056343\n",
      "Epoch 22330/30000 Training Loss: 0.04779437929391861\n",
      "Epoch 22331/30000 Training Loss: 0.033965080976486206\n",
      "Epoch 22332/30000 Training Loss: 0.05489329248666763\n",
      "Epoch 22333/30000 Training Loss: 0.04474485293030739\n",
      "Epoch 22334/30000 Training Loss: 0.04926307499408722\n",
      "Epoch 22335/30000 Training Loss: 0.06473711133003235\n",
      "Epoch 22336/30000 Training Loss: 0.0449720174074173\n",
      "Epoch 22337/30000 Training Loss: 0.044991713017225266\n",
      "Epoch 22338/30000 Training Loss: 0.046143919229507446\n",
      "Epoch 22339/30000 Training Loss: 0.04727406054735184\n",
      "Epoch 22340/30000 Training Loss: 0.06496487557888031\n",
      "Epoch 22341/30000 Training Loss: 0.037722013890743256\n",
      "Epoch 22342/30000 Training Loss: 0.054209135472774506\n",
      "Epoch 22343/30000 Training Loss: 0.05321633815765381\n",
      "Epoch 22344/30000 Training Loss: 0.04162346199154854\n",
      "Epoch 22345/30000 Training Loss: 0.0501510426402092\n",
      "Epoch 22346/30000 Training Loss: 0.044437017291784286\n",
      "Epoch 22347/30000 Training Loss: 0.04115743190050125\n",
      "Epoch 22348/30000 Training Loss: 0.03581184893846512\n",
      "Epoch 22349/30000 Training Loss: 0.040323834866285324\n",
      "Epoch 22350/30000 Training Loss: 0.05022891238331795\n",
      "Epoch 22351/30000 Training Loss: 0.04338529333472252\n",
      "Epoch 22352/30000 Training Loss: 0.03993913531303406\n",
      "Epoch 22353/30000 Training Loss: 0.051140256226062775\n",
      "Epoch 22354/30000 Training Loss: 0.044141799211502075\n",
      "Epoch 22355/30000 Training Loss: 0.04716498777270317\n",
      "Epoch 22356/30000 Training Loss: 0.04701199382543564\n",
      "Epoch 22357/30000 Training Loss: 0.06278830766677856\n",
      "Epoch 22358/30000 Training Loss: 0.05339379608631134\n",
      "Epoch 22359/30000 Training Loss: 0.04329278692603111\n",
      "Epoch 22360/30000 Training Loss: 0.03931112214922905\n",
      "Epoch 22361/30000 Training Loss: 0.042299892753362656\n",
      "Epoch 22362/30000 Training Loss: 0.036897964775562286\n",
      "Epoch 22363/30000 Training Loss: 0.03959992155432701\n",
      "Epoch 22364/30000 Training Loss: 0.044533394277095795\n",
      "Epoch 22365/30000 Training Loss: 0.045856185257434845\n",
      "Epoch 22366/30000 Training Loss: 0.06965027749538422\n",
      "Epoch 22367/30000 Training Loss: 0.05063600093126297\n",
      "Epoch 22368/30000 Training Loss: 0.04670000448822975\n",
      "Epoch 22369/30000 Training Loss: 0.04655270650982857\n",
      "Epoch 22370/30000 Training Loss: 0.05438154935836792\n",
      "Epoch 22371/30000 Training Loss: 0.0615551732480526\n",
      "Epoch 22372/30000 Training Loss: 0.049024030566215515\n",
      "Epoch 22373/30000 Training Loss: 0.05067818611860275\n",
      "Epoch 22374/30000 Training Loss: 0.054328784346580505\n",
      "Epoch 22375/30000 Training Loss: 0.03780921921133995\n",
      "Epoch 22376/30000 Training Loss: 0.050134915858507156\n",
      "Epoch 22377/30000 Training Loss: 0.03376959264278412\n",
      "Epoch 22378/30000 Training Loss: 0.043839115649461746\n",
      "Epoch 22379/30000 Training Loss: 0.060569629073143005\n",
      "Epoch 22380/30000 Training Loss: 0.05558779835700989\n",
      "Epoch 22381/30000 Training Loss: 0.0561474934220314\n",
      "Epoch 22382/30000 Training Loss: 0.03371456265449524\n",
      "Epoch 22383/30000 Training Loss: 0.05120227485895157\n",
      "Epoch 22384/30000 Training Loss: 0.0468965545296669\n",
      "Epoch 22385/30000 Training Loss: 0.06120559945702553\n",
      "Epoch 22386/30000 Training Loss: 0.033906240016222\n",
      "Epoch 22387/30000 Training Loss: 0.047812268137931824\n",
      "Epoch 22388/30000 Training Loss: 0.045011475682258606\n",
      "Epoch 22389/30000 Training Loss: 0.050111882388591766\n",
      "Epoch 22390/30000 Training Loss: 0.03934098780155182\n",
      "Epoch 22391/30000 Training Loss: 0.05102701112627983\n",
      "Epoch 22392/30000 Training Loss: 0.0509943813085556\n",
      "Epoch 22393/30000 Training Loss: 0.038664232939481735\n",
      "Epoch 22394/30000 Training Loss: 0.05504896864295006\n",
      "Epoch 22395/30000 Training Loss: 0.029978763312101364\n",
      "Epoch 22396/30000 Training Loss: 0.04427983611822128\n",
      "Epoch 22397/30000 Training Loss: 0.04314718395471573\n",
      "Epoch 22398/30000 Training Loss: 0.04352604225277901\n",
      "Epoch 22399/30000 Training Loss: 0.050306081771850586\n",
      "Epoch 22400/30000 Training Loss: 0.04065772145986557\n",
      "Epoch 22400/30000 Validation Loss: 0.027423538267612457\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.027423538267612457<=============\n",
      "Epoch 22401/30000 Training Loss: 0.05501371622085571\n",
      "Epoch 22402/30000 Training Loss: 0.050983406603336334\n",
      "Epoch 22403/30000 Training Loss: 0.04626796394586563\n",
      "Epoch 22404/30000 Training Loss: 0.0375186949968338\n",
      "Epoch 22405/30000 Training Loss: 0.05637615919113159\n",
      "Epoch 22406/30000 Training Loss: 0.04217039793729782\n",
      "Epoch 22407/30000 Training Loss: 0.03548701852560043\n",
      "Epoch 22408/30000 Training Loss: 0.04217825084924698\n",
      "Epoch 22409/30000 Training Loss: 0.051333434879779816\n",
      "Epoch 22410/30000 Training Loss: 0.049600690603256226\n",
      "Epoch 22411/30000 Training Loss: 0.03892401605844498\n",
      "Epoch 22412/30000 Training Loss: 0.0397954024374485\n",
      "Epoch 22413/30000 Training Loss: 0.041520122438669205\n",
      "Epoch 22414/30000 Training Loss: 0.055261921137571335\n",
      "Epoch 22415/30000 Training Loss: 0.0487573966383934\n",
      "Epoch 22416/30000 Training Loss: 0.06342553347349167\n",
      "Epoch 22417/30000 Training Loss: 0.04528599977493286\n",
      "Epoch 22418/30000 Training Loss: 0.051304806023836136\n",
      "Epoch 22419/30000 Training Loss: 0.04245573282241821\n",
      "Epoch 22420/30000 Training Loss: 0.05282217264175415\n",
      "Epoch 22421/30000 Training Loss: 0.05676731467247009\n",
      "Epoch 22422/30000 Training Loss: 0.05525602400302887\n",
      "Epoch 22423/30000 Training Loss: 0.03648575767874718\n",
      "Epoch 22424/30000 Training Loss: 0.04677040874958038\n",
      "Epoch 22425/30000 Training Loss: 0.05025261268019676\n",
      "Epoch 22426/30000 Training Loss: 0.05672163516283035\n",
      "Epoch 22427/30000 Training Loss: 0.0381179004907608\n",
      "Epoch 22428/30000 Training Loss: 0.041748665273189545\n",
      "Epoch 22429/30000 Training Loss: 0.050909075886011124\n",
      "Epoch 22430/30000 Training Loss: 0.041791610419750214\n",
      "Epoch 22431/30000 Training Loss: 0.052605628967285156\n",
      "Epoch 22432/30000 Training Loss: 0.05762458220124245\n",
      "Epoch 22433/30000 Training Loss: 0.040253717452287674\n",
      "Epoch 22434/30000 Training Loss: 0.03438819944858551\n",
      "Epoch 22435/30000 Training Loss: 0.044940415769815445\n",
      "Epoch 22436/30000 Training Loss: 0.048383209854364395\n",
      "Epoch 22437/30000 Training Loss: 0.03140964359045029\n",
      "Epoch 22438/30000 Training Loss: 0.046810779720544815\n",
      "Epoch 22439/30000 Training Loss: 0.04815170168876648\n",
      "Epoch 22440/30000 Training Loss: 0.05625484138727188\n",
      "Epoch 22441/30000 Training Loss: 0.06898108124732971\n",
      "Epoch 22442/30000 Training Loss: 0.04924226179718971\n",
      "Epoch 22443/30000 Training Loss: 0.05914967879652977\n",
      "Epoch 22444/30000 Training Loss: 0.0406789556145668\n",
      "Epoch 22445/30000 Training Loss: 0.029651373624801636\n",
      "Epoch 22446/30000 Training Loss: 0.05858569219708443\n",
      "Epoch 22447/30000 Training Loss: 0.04426632449030876\n",
      "Epoch 22448/30000 Training Loss: 0.05196210369467735\n",
      "Epoch 22449/30000 Training Loss: 0.029495393857359886\n",
      "Epoch 22450/30000 Training Loss: 0.06193793565034866\n",
      "Epoch 22451/30000 Training Loss: 0.033890582621097565\n",
      "Epoch 22452/30000 Training Loss: 0.04708929359912872\n",
      "Epoch 22453/30000 Training Loss: 0.0553223118185997\n",
      "Epoch 22454/30000 Training Loss: 0.04704052954912186\n",
      "Epoch 22455/30000 Training Loss: 0.057280898094177246\n",
      "Epoch 22456/30000 Training Loss: 0.04031023755669594\n",
      "Epoch 22457/30000 Training Loss: 0.04268506169319153\n",
      "Epoch 22458/30000 Training Loss: 0.02855602651834488\n",
      "Epoch 22459/30000 Training Loss: 0.0482938215136528\n",
      "Epoch 22460/30000 Training Loss: 0.05330243334174156\n",
      "Epoch 22461/30000 Training Loss: 0.056276097893714905\n",
      "Epoch 22462/30000 Training Loss: 0.04994257539510727\n",
      "Epoch 22463/30000 Training Loss: 0.036074794828891754\n",
      "Epoch 22464/30000 Training Loss: 0.06111322343349457\n",
      "Epoch 22465/30000 Training Loss: 0.042756691575050354\n",
      "Epoch 22466/30000 Training Loss: 0.047282032668590546\n",
      "Epoch 22467/30000 Training Loss: 0.038258690387010574\n",
      "Epoch 22468/30000 Training Loss: 0.04916858673095703\n",
      "Epoch 22469/30000 Training Loss: 0.04687541723251343\n",
      "Epoch 22470/30000 Training Loss: 0.05582829937338829\n",
      "Epoch 22471/30000 Training Loss: 0.04842997342348099\n",
      "Epoch 22472/30000 Training Loss: 0.042823050171136856\n",
      "Epoch 22473/30000 Training Loss: 0.049252983182668686\n",
      "Epoch 22474/30000 Training Loss: 0.04261944442987442\n",
      "Epoch 22475/30000 Training Loss: 0.03873266279697418\n",
      "Epoch 22476/30000 Training Loss: 0.03127659484744072\n",
      "Epoch 22477/30000 Training Loss: 0.038690101355314255\n",
      "Epoch 22478/30000 Training Loss: 0.04346033185720444\n",
      "Epoch 22479/30000 Training Loss: 0.044241275638341904\n",
      "Epoch 22480/30000 Training Loss: 0.04887329041957855\n",
      "Epoch 22481/30000 Training Loss: 0.05826601758599281\n",
      "Epoch 22482/30000 Training Loss: 0.04417714476585388\n",
      "Epoch 22483/30000 Training Loss: 0.04532039165496826\n",
      "Epoch 22484/30000 Training Loss: 0.05429820716381073\n",
      "Epoch 22485/30000 Training Loss: 0.04262222349643707\n",
      "Epoch 22486/30000 Training Loss: 0.0564347505569458\n",
      "Epoch 22487/30000 Training Loss: 0.05149184912443161\n",
      "Epoch 22488/30000 Training Loss: 0.0580863282084465\n",
      "Epoch 22489/30000 Training Loss: 0.03684152290225029\n",
      "Epoch 22490/30000 Training Loss: 0.038544367998838425\n",
      "Epoch 22491/30000 Training Loss: 0.06633203476667404\n",
      "Epoch 22492/30000 Training Loss: 0.05472814291715622\n",
      "Epoch 22493/30000 Training Loss: 0.04646370932459831\n",
      "Epoch 22494/30000 Training Loss: 0.05020850896835327\n",
      "Epoch 22495/30000 Training Loss: 0.04180391877889633\n",
      "Epoch 22496/30000 Training Loss: 0.0596446618437767\n",
      "Epoch 22497/30000 Training Loss: 0.04326864331960678\n",
      "Epoch 22498/30000 Training Loss: 0.04494781792163849\n",
      "Epoch 22499/30000 Training Loss: 0.041480809450149536\n",
      "Epoch 22500/30000 Training Loss: 0.05503476411104202\n",
      "Epoch 22500/30000 Validation Loss: 0.06553534418344498\n",
      "Epoch 22501/30000 Training Loss: 0.038263313472270966\n",
      "Epoch 22502/30000 Training Loss: 0.030102698132395744\n",
      "Epoch 22503/30000 Training Loss: 0.04965602234005928\n",
      "Epoch 22504/30000 Training Loss: 0.04654060676693916\n",
      "Epoch 22505/30000 Training Loss: 0.047891683876514435\n",
      "Epoch 22506/30000 Training Loss: 0.06460808217525482\n",
      "Epoch 22507/30000 Training Loss: 0.0472327396273613\n",
      "Epoch 22508/30000 Training Loss: 0.043978944420814514\n",
      "Epoch 22509/30000 Training Loss: 0.054073531180620193\n",
      "Epoch 22510/30000 Training Loss: 0.04867984727025032\n",
      "Epoch 22511/30000 Training Loss: 0.062209732830524445\n",
      "Epoch 22512/30000 Training Loss: 0.04524502158164978\n",
      "Epoch 22513/30000 Training Loss: 0.048267755657434464\n",
      "Epoch 22514/30000 Training Loss: 0.03395146131515503\n",
      "Epoch 22515/30000 Training Loss: 0.048827387392520905\n",
      "Epoch 22516/30000 Training Loss: 0.048023492097854614\n",
      "Epoch 22517/30000 Training Loss: 0.032894767820835114\n",
      "Epoch 22518/30000 Training Loss: 0.05683881789445877\n",
      "Epoch 22519/30000 Training Loss: 0.03875511884689331\n",
      "Epoch 22520/30000 Training Loss: 0.05717857554554939\n",
      "Epoch 22521/30000 Training Loss: 0.04553341120481491\n",
      "Epoch 22522/30000 Training Loss: 0.04823292791843414\n",
      "Epoch 22523/30000 Training Loss: 0.05156578868627548\n",
      "Epoch 22524/30000 Training Loss: 0.03512455150485039\n",
      "Epoch 22525/30000 Training Loss: 0.043806739151477814\n",
      "Epoch 22526/30000 Training Loss: 0.0466071255505085\n",
      "Epoch 22527/30000 Training Loss: 0.052297431975603104\n",
      "Epoch 22528/30000 Training Loss: 0.04940539598464966\n",
      "Epoch 22529/30000 Training Loss: 0.04649437591433525\n",
      "Epoch 22530/30000 Training Loss: 0.035532038658857346\n",
      "Epoch 22531/30000 Training Loss: 0.05732313543558121\n",
      "Epoch 22532/30000 Training Loss: 0.06288263946771622\n",
      "Epoch 22533/30000 Training Loss: 0.05058597773313522\n",
      "Epoch 22534/30000 Training Loss: 0.04091284051537514\n",
      "Epoch 22535/30000 Training Loss: 0.03793942555785179\n",
      "Epoch 22536/30000 Training Loss: 0.045370958745479584\n",
      "Epoch 22537/30000 Training Loss: 0.05844744294881821\n",
      "Epoch 22538/30000 Training Loss: 0.041939061135053635\n",
      "Epoch 22539/30000 Training Loss: 0.04914575070142746\n",
      "Epoch 22540/30000 Training Loss: 0.04852425679564476\n",
      "Epoch 22541/30000 Training Loss: 0.03989969193935394\n",
      "Epoch 22542/30000 Training Loss: 0.058066900819540024\n",
      "Epoch 22543/30000 Training Loss: 0.05144887417554855\n",
      "Epoch 22544/30000 Training Loss: 0.04332912713289261\n",
      "Epoch 22545/30000 Training Loss: 0.05166888236999512\n",
      "Epoch 22546/30000 Training Loss: 0.03878910467028618\n",
      "Epoch 22547/30000 Training Loss: 0.04316328093409538\n",
      "Epoch 22548/30000 Training Loss: 0.05334733426570892\n",
      "Epoch 22549/30000 Training Loss: 0.0592634417116642\n",
      "Epoch 22550/30000 Training Loss: 0.041197460144758224\n",
      "Epoch 22551/30000 Training Loss: 0.054862283170223236\n",
      "Epoch 22552/30000 Training Loss: 0.05279442295432091\n",
      "Epoch 22553/30000 Training Loss: 0.045049186795949936\n",
      "Epoch 22554/30000 Training Loss: 0.04584483057260513\n",
      "Epoch 22555/30000 Training Loss: 0.052001770585775375\n",
      "Epoch 22556/30000 Training Loss: 0.04420960694551468\n",
      "Epoch 22557/30000 Training Loss: 0.05433734133839607\n",
      "Epoch 22558/30000 Training Loss: 0.04683879390358925\n",
      "Epoch 22559/30000 Training Loss: 0.04726859927177429\n",
      "Epoch 22560/30000 Training Loss: 0.04354031756520271\n",
      "Epoch 22561/30000 Training Loss: 0.0509055070579052\n",
      "Epoch 22562/30000 Training Loss: 0.05318601056933403\n",
      "Epoch 22563/30000 Training Loss: 0.04433922842144966\n",
      "Epoch 22564/30000 Training Loss: 0.050150878727436066\n",
      "Epoch 22565/30000 Training Loss: 0.039917558431625366\n",
      "Epoch 22566/30000 Training Loss: 0.031070969998836517\n",
      "Epoch 22567/30000 Training Loss: 0.06679868698120117\n",
      "Epoch 22568/30000 Training Loss: 0.0436209961771965\n",
      "Epoch 22569/30000 Training Loss: 0.059391088783741\n",
      "Epoch 22570/30000 Training Loss: 0.061648890376091\n",
      "Epoch 22571/30000 Training Loss: 0.04742736369371414\n",
      "Epoch 22572/30000 Training Loss: 0.05342864990234375\n",
      "Epoch 22573/30000 Training Loss: 0.050168223679065704\n",
      "Epoch 22574/30000 Training Loss: 0.04357188567519188\n",
      "Epoch 22575/30000 Training Loss: 0.04773838073015213\n",
      "Epoch 22576/30000 Training Loss: 0.03895288705825806\n",
      "Epoch 22577/30000 Training Loss: 0.05059204250574112\n",
      "Epoch 22578/30000 Training Loss: 0.04882880300283432\n",
      "Epoch 22579/30000 Training Loss: 0.04379938542842865\n",
      "Epoch 22580/30000 Training Loss: 0.05646365135908127\n",
      "Epoch 22581/30000 Training Loss: 0.0406142994761467\n",
      "Epoch 22582/30000 Training Loss: 0.04124175012111664\n",
      "Epoch 22583/30000 Training Loss: 0.05364038795232773\n",
      "Epoch 22584/30000 Training Loss: 0.03782021254301071\n",
      "Epoch 22585/30000 Training Loss: 0.03662038594484329\n",
      "Epoch 22586/30000 Training Loss: 0.04120741784572601\n",
      "Epoch 22587/30000 Training Loss: 0.06963271647691727\n",
      "Epoch 22588/30000 Training Loss: 0.035788580775260925\n",
      "Epoch 22589/30000 Training Loss: 0.045025136321783066\n",
      "Epoch 22590/30000 Training Loss: 0.03460409492254257\n",
      "Epoch 22591/30000 Training Loss: 0.06058799847960472\n",
      "Epoch 22592/30000 Training Loss: 0.04951582849025726\n",
      "Epoch 22593/30000 Training Loss: 0.040227487683296204\n",
      "Epoch 22594/30000 Training Loss: 0.047060027718544006\n",
      "Epoch 22595/30000 Training Loss: 0.04424070939421654\n",
      "Epoch 22596/30000 Training Loss: 0.04988204315304756\n",
      "Epoch 22597/30000 Training Loss: 0.06253606081008911\n",
      "Epoch 22598/30000 Training Loss: 0.03714537248015404\n",
      "Epoch 22599/30000 Training Loss: 0.04301920533180237\n",
      "Epoch 22600/30000 Training Loss: 0.056225307285785675\n",
      "Epoch 22600/30000 Validation Loss: 0.045347440987825394\n",
      "Epoch 22601/30000 Training Loss: 0.03699316084384918\n",
      "Epoch 22602/30000 Training Loss: 0.043855100870132446\n",
      "Epoch 22603/30000 Training Loss: 0.05027853697538376\n",
      "Epoch 22604/30000 Training Loss: 0.04294782876968384\n",
      "Epoch 22605/30000 Training Loss: 0.046022944152355194\n",
      "Epoch 22606/30000 Training Loss: 0.04246984422206879\n",
      "Epoch 22607/30000 Training Loss: 0.04190345108509064\n",
      "Epoch 22608/30000 Training Loss: 0.0375174880027771\n",
      "Epoch 22609/30000 Training Loss: 0.05574040114879608\n",
      "Epoch 22610/30000 Training Loss: 0.03918450325727463\n",
      "Epoch 22611/30000 Training Loss: 0.057109393179416656\n",
      "Epoch 22612/30000 Training Loss: 0.043725233525037766\n",
      "Epoch 22613/30000 Training Loss: 0.04114620387554169\n",
      "Epoch 22614/30000 Training Loss: 0.04771502688527107\n",
      "Epoch 22615/30000 Training Loss: 0.03785974532365799\n",
      "Epoch 22616/30000 Training Loss: 0.04763846844434738\n",
      "Epoch 22617/30000 Training Loss: 0.04448690265417099\n",
      "Epoch 22618/30000 Training Loss: 0.04953575134277344\n",
      "Epoch 22619/30000 Training Loss: 0.030860070139169693\n",
      "Epoch 22620/30000 Training Loss: 0.0519975945353508\n",
      "Epoch 22621/30000 Training Loss: 0.03772950544953346\n",
      "Epoch 22622/30000 Training Loss: 0.05756543576717377\n",
      "Epoch 22623/30000 Training Loss: 0.04631137102842331\n",
      "Epoch 22624/30000 Training Loss: 0.03963979333639145\n",
      "Epoch 22625/30000 Training Loss: 0.04221583902835846\n",
      "Epoch 22626/30000 Training Loss: 0.04344618320465088\n",
      "Epoch 22627/30000 Training Loss: 0.04671838879585266\n",
      "Epoch 22628/30000 Training Loss: 0.04923500120639801\n",
      "Epoch 22629/30000 Training Loss: 0.03927522897720337\n",
      "Epoch 22630/30000 Training Loss: 0.05186270922422409\n",
      "Epoch 22631/30000 Training Loss: 0.05858257785439491\n",
      "Epoch 22632/30000 Training Loss: 0.05177880823612213\n",
      "Epoch 22633/30000 Training Loss: 0.04868541657924652\n",
      "Epoch 22634/30000 Training Loss: 0.031542155891656876\n",
      "Epoch 22635/30000 Training Loss: 0.05662112310528755\n",
      "Epoch 22636/30000 Training Loss: 0.04656755179166794\n",
      "Epoch 22637/30000 Training Loss: 0.048054348677396774\n",
      "Epoch 22638/30000 Training Loss: 0.051158785820007324\n",
      "Epoch 22639/30000 Training Loss: 0.04311053827404976\n",
      "Epoch 22640/30000 Training Loss: 0.05418623611330986\n",
      "Epoch 22641/30000 Training Loss: 0.04435727372765541\n",
      "Epoch 22642/30000 Training Loss: 0.047340378165245056\n",
      "Epoch 22643/30000 Training Loss: 0.06319762021303177\n",
      "Epoch 22644/30000 Training Loss: 0.04515337198972702\n",
      "Epoch 22645/30000 Training Loss: 0.04364706948399544\n",
      "Epoch 22646/30000 Training Loss: 0.038036782294511795\n",
      "Epoch 22647/30000 Training Loss: 0.042905233800411224\n",
      "Epoch 22648/30000 Training Loss: 0.03865903988480568\n",
      "Epoch 22649/30000 Training Loss: 0.0526370070874691\n",
      "Epoch 22650/30000 Training Loss: 0.04260746389627457\n",
      "Epoch 22651/30000 Training Loss: 0.04717351496219635\n",
      "Epoch 22652/30000 Training Loss: 0.04772017151117325\n",
      "Epoch 22653/30000 Training Loss: 0.04892650619149208\n",
      "Epoch 22654/30000 Training Loss: 0.04798728600144386\n",
      "Epoch 22655/30000 Training Loss: 0.04320741444826126\n",
      "Epoch 22656/30000 Training Loss: 0.06080573797225952\n",
      "Epoch 22657/30000 Training Loss: 0.04057104513049126\n",
      "Epoch 22658/30000 Training Loss: 0.04450203850865364\n",
      "Epoch 22659/30000 Training Loss: 0.046954769641160965\n",
      "Epoch 22660/30000 Training Loss: 0.06132464483380318\n",
      "Epoch 22661/30000 Training Loss: 0.04124635457992554\n",
      "Epoch 22662/30000 Training Loss: 0.03984861820936203\n",
      "Epoch 22663/30000 Training Loss: 0.04609319940209389\n",
      "Epoch 22664/30000 Training Loss: 0.040366463363170624\n",
      "Epoch 22665/30000 Training Loss: 0.03368309140205383\n",
      "Epoch 22666/30000 Training Loss: 0.0553913414478302\n",
      "Epoch 22667/30000 Training Loss: 0.0708252489566803\n",
      "Epoch 22668/30000 Training Loss: 0.054373156279325485\n",
      "Epoch 22669/30000 Training Loss: 0.04515817016363144\n",
      "Epoch 22670/30000 Training Loss: 0.04280645772814751\n",
      "Epoch 22671/30000 Training Loss: 0.03744359314441681\n",
      "Epoch 22672/30000 Training Loss: 0.04484542831778526\n",
      "Epoch 22673/30000 Training Loss: 0.04467454180121422\n",
      "Epoch 22674/30000 Training Loss: 0.04090128839015961\n",
      "Epoch 22675/30000 Training Loss: 0.04334452748298645\n",
      "Epoch 22676/30000 Training Loss: 0.03503141552209854\n",
      "Epoch 22677/30000 Training Loss: 0.054440103471279144\n",
      "Epoch 22678/30000 Training Loss: 0.03864508122205734\n",
      "Epoch 22679/30000 Training Loss: 0.057498835027217865\n",
      "Epoch 22680/30000 Training Loss: 0.05722668021917343\n",
      "Epoch 22681/30000 Training Loss: 0.05363039672374725\n",
      "Epoch 22682/30000 Training Loss: 0.047764673829078674\n",
      "Epoch 22683/30000 Training Loss: 0.0595136433839798\n",
      "Epoch 22684/30000 Training Loss: 0.04488016664981842\n",
      "Epoch 22685/30000 Training Loss: 0.04779192432761192\n",
      "Epoch 22686/30000 Training Loss: 0.06700360774993896\n",
      "Epoch 22687/30000 Training Loss: 0.04509802907705307\n",
      "Epoch 22688/30000 Training Loss: 0.058191072195768356\n",
      "Epoch 22689/30000 Training Loss: 0.040456146001815796\n",
      "Epoch 22690/30000 Training Loss: 0.061888594180345535\n",
      "Epoch 22691/30000 Training Loss: 0.035166963934898376\n",
      "Epoch 22692/30000 Training Loss: 0.04003943130373955\n",
      "Epoch 22693/30000 Training Loss: 0.04970957711338997\n",
      "Epoch 22694/30000 Training Loss: 0.049965132027864456\n",
      "Epoch 22695/30000 Training Loss: 0.04546766355633736\n",
      "Epoch 22696/30000 Training Loss: 0.04280892014503479\n",
      "Epoch 22697/30000 Training Loss: 0.04639050364494324\n",
      "Epoch 22698/30000 Training Loss: 0.04836282134056091\n",
      "Epoch 22699/30000 Training Loss: 0.05035330727696419\n",
      "Epoch 22700/30000 Training Loss: 0.05539068207144737\n",
      "Epoch 22700/30000 Validation Loss: 0.04100523144006729\n",
      "Epoch 22701/30000 Training Loss: 0.04915768280625343\n",
      "Epoch 22702/30000 Training Loss: 0.04375603049993515\n",
      "Epoch 22703/30000 Training Loss: 0.06534604728221893\n",
      "Epoch 22704/30000 Training Loss: 0.03771166875958443\n",
      "Epoch 22705/30000 Training Loss: 0.048462796956300735\n",
      "Epoch 22706/30000 Training Loss: 0.03679812699556351\n",
      "Epoch 22707/30000 Training Loss: 0.04355259984731674\n",
      "Epoch 22708/30000 Training Loss: 0.03193562850356102\n",
      "Epoch 22709/30000 Training Loss: 0.05788092687726021\n",
      "Epoch 22710/30000 Training Loss: 0.050912901759147644\n",
      "Epoch 22711/30000 Training Loss: 0.059243153780698776\n",
      "Epoch 22712/30000 Training Loss: 0.03940131515264511\n",
      "Epoch 22713/30000 Training Loss: 0.04290792718529701\n",
      "Epoch 22714/30000 Training Loss: 0.04149891436100006\n",
      "Epoch 22715/30000 Training Loss: 0.049873024225234985\n",
      "Epoch 22716/30000 Training Loss: 0.05200541019439697\n",
      "Epoch 22717/30000 Training Loss: 0.04162506386637688\n",
      "Epoch 22718/30000 Training Loss: 0.05138678848743439\n",
      "Epoch 22719/30000 Training Loss: 0.04831449314951897\n",
      "Epoch 22720/30000 Training Loss: 0.04758239537477493\n",
      "Epoch 22721/30000 Training Loss: 0.047346875071525574\n",
      "Epoch 22722/30000 Training Loss: 0.05127886310219765\n",
      "Epoch 22723/30000 Training Loss: 0.04320409148931503\n",
      "Epoch 22724/30000 Training Loss: 0.0408426895737648\n",
      "Epoch 22725/30000 Training Loss: 0.03586908429861069\n",
      "Epoch 22726/30000 Training Loss: 0.030449578538537025\n",
      "Epoch 22727/30000 Training Loss: 0.04530518501996994\n",
      "Epoch 22728/30000 Training Loss: 0.039719849824905396\n",
      "Epoch 22729/30000 Training Loss: 0.03484850376844406\n",
      "Epoch 22730/30000 Training Loss: 0.05173344537615776\n",
      "Epoch 22731/30000 Training Loss: 0.08566617965698242\n",
      "Epoch 22732/30000 Training Loss: 0.04874825477600098\n",
      "Epoch 22733/30000 Training Loss: 0.049607060849666595\n",
      "Epoch 22734/30000 Training Loss: 0.0339682474732399\n",
      "Epoch 22735/30000 Training Loss: 0.041972823441028595\n",
      "Epoch 22736/30000 Training Loss: 0.04427697882056236\n",
      "Epoch 22737/30000 Training Loss: 0.04963160306215286\n",
      "Epoch 22738/30000 Training Loss: 0.06244737654924393\n",
      "Epoch 22739/30000 Training Loss: 0.04662270471453667\n",
      "Epoch 22740/30000 Training Loss: 0.04036571830511093\n",
      "Epoch 22741/30000 Training Loss: 0.04153008759021759\n",
      "Epoch 22742/30000 Training Loss: 0.055134229362010956\n",
      "Epoch 22743/30000 Training Loss: 0.04104961082339287\n",
      "Epoch 22744/30000 Training Loss: 0.039065029472112656\n",
      "Epoch 22745/30000 Training Loss: 0.05809486284852028\n",
      "Epoch 22746/30000 Training Loss: 0.041321489959955215\n",
      "Epoch 22747/30000 Training Loss: 0.04011348634958267\n",
      "Epoch 22748/30000 Training Loss: 0.055407650768756866\n",
      "Epoch 22749/30000 Training Loss: 0.04167228192090988\n",
      "Epoch 22750/30000 Training Loss: 0.05185630917549133\n",
      "Epoch 22751/30000 Training Loss: 0.04416046291589737\n",
      "Epoch 22752/30000 Training Loss: 0.04225340485572815\n",
      "Epoch 22753/30000 Training Loss: 0.048812124878168106\n",
      "Epoch 22754/30000 Training Loss: 0.06337641924619675\n",
      "Epoch 22755/30000 Training Loss: 0.050734132528305054\n",
      "Epoch 22756/30000 Training Loss: 0.04821907728910446\n",
      "Epoch 22757/30000 Training Loss: 0.03847265988588333\n",
      "Epoch 22758/30000 Training Loss: 0.053583621978759766\n",
      "Epoch 22759/30000 Training Loss: 0.058785874396562576\n",
      "Epoch 22760/30000 Training Loss: 0.033016905188560486\n",
      "Epoch 22761/30000 Training Loss: 0.04708731919527054\n",
      "Epoch 22762/30000 Training Loss: 0.05002916604280472\n",
      "Epoch 22763/30000 Training Loss: 0.03873414173722267\n",
      "Epoch 22764/30000 Training Loss: 0.0473984070122242\n",
      "Epoch 22765/30000 Training Loss: 0.052853889763355255\n",
      "Epoch 22766/30000 Training Loss: 0.05876445770263672\n",
      "Epoch 22767/30000 Training Loss: 0.04808051884174347\n",
      "Epoch 22768/30000 Training Loss: 0.05597364529967308\n",
      "Epoch 22769/30000 Training Loss: 0.039343103766441345\n",
      "Epoch 22770/30000 Training Loss: 0.043018970638513565\n",
      "Epoch 22771/30000 Training Loss: 0.05339369177818298\n",
      "Epoch 22772/30000 Training Loss: 0.042434971779584885\n",
      "Epoch 22773/30000 Training Loss: 0.041149575263261795\n",
      "Epoch 22774/30000 Training Loss: 0.03782189264893532\n",
      "Epoch 22775/30000 Training Loss: 0.06118786707520485\n",
      "Epoch 22776/30000 Training Loss: 0.06514594703912735\n",
      "Epoch 22777/30000 Training Loss: 0.043139006942510605\n",
      "Epoch 22778/30000 Training Loss: 0.042082417756319046\n",
      "Epoch 22779/30000 Training Loss: 0.041583988815546036\n",
      "Epoch 22780/30000 Training Loss: 0.067608542740345\n",
      "Epoch 22781/30000 Training Loss: 0.04181313142180443\n",
      "Epoch 22782/30000 Training Loss: 0.04670480266213417\n",
      "Epoch 22783/30000 Training Loss: 0.04429439455270767\n",
      "Epoch 22784/30000 Training Loss: 0.056921496987342834\n",
      "Epoch 22785/30000 Training Loss: 0.044597409665584564\n",
      "Epoch 22786/30000 Training Loss: 0.05173204839229584\n",
      "Epoch 22787/30000 Training Loss: 0.050362009555101395\n",
      "Epoch 22788/30000 Training Loss: 0.038852572441101074\n",
      "Epoch 22789/30000 Training Loss: 0.044090013951063156\n",
      "Epoch 22790/30000 Training Loss: 0.03393588587641716\n",
      "Epoch 22791/30000 Training Loss: 0.0492851659655571\n",
      "Epoch 22792/30000 Training Loss: 0.040495846420526505\n",
      "Epoch 22793/30000 Training Loss: 0.055471569299697876\n",
      "Epoch 22794/30000 Training Loss: 0.043711815029382706\n",
      "Epoch 22795/30000 Training Loss: 0.05773249268531799\n",
      "Epoch 22796/30000 Training Loss: 0.04059595614671707\n",
      "Epoch 22797/30000 Training Loss: 0.04375011473894119\n",
      "Epoch 22798/30000 Training Loss: 0.04208313301205635\n",
      "Epoch 22799/30000 Training Loss: 0.05808445066213608\n",
      "Epoch 22800/30000 Training Loss: 0.05825912579894066\n",
      "Epoch 22800/30000 Validation Loss: 0.04016285389661789\n",
      "Epoch 22801/30000 Training Loss: 0.056619804352521896\n",
      "Epoch 22802/30000 Training Loss: 0.05281875282526016\n",
      "Epoch 22803/30000 Training Loss: 0.055308468639850616\n",
      "Epoch 22804/30000 Training Loss: 0.039273686707019806\n",
      "Epoch 22805/30000 Training Loss: 0.04339775815606117\n",
      "Epoch 22806/30000 Training Loss: 0.06390371173620224\n",
      "Epoch 22807/30000 Training Loss: 0.048534102737903595\n",
      "Epoch 22808/30000 Training Loss: 0.056250642985105515\n",
      "Epoch 22809/30000 Training Loss: 0.046451814472675323\n",
      "Epoch 22810/30000 Training Loss: 0.04876865819096565\n",
      "Epoch 22811/30000 Training Loss: 0.03449098765850067\n",
      "Epoch 22812/30000 Training Loss: 0.033833086490631104\n",
      "Epoch 22813/30000 Training Loss: 0.03813124820590019\n",
      "Epoch 22814/30000 Training Loss: 0.0461425855755806\n",
      "Epoch 22815/30000 Training Loss: 0.05455819144845009\n",
      "Epoch 22816/30000 Training Loss: 0.047518011182546616\n",
      "Epoch 22817/30000 Training Loss: 0.06444472074508667\n",
      "Epoch 22818/30000 Training Loss: 0.048996105790138245\n",
      "Epoch 22819/30000 Training Loss: 0.04570898786187172\n",
      "Epoch 22820/30000 Training Loss: 0.03880171477794647\n",
      "Epoch 22821/30000 Training Loss: 0.044344883412122726\n",
      "Epoch 22822/30000 Training Loss: 0.032653044909238815\n",
      "Epoch 22823/30000 Training Loss: 0.06296441704034805\n",
      "Epoch 22824/30000 Training Loss: 0.03457828238606453\n",
      "Epoch 22825/30000 Training Loss: 0.04529273882508278\n",
      "Epoch 22826/30000 Training Loss: 0.04714255407452583\n",
      "Epoch 22827/30000 Training Loss: 0.0341511033475399\n",
      "Epoch 22828/30000 Training Loss: 0.034027349203825\n",
      "Epoch 22829/30000 Training Loss: 0.042568743228912354\n",
      "Epoch 22830/30000 Training Loss: 0.06304236501455307\n",
      "Epoch 22831/30000 Training Loss: 0.044555552303791046\n",
      "Epoch 22832/30000 Training Loss: 0.030308378860354424\n",
      "Epoch 22833/30000 Training Loss: 0.04756462201476097\n",
      "Epoch 22834/30000 Training Loss: 0.04324812814593315\n",
      "Epoch 22835/30000 Training Loss: 0.05832064151763916\n",
      "Epoch 22836/30000 Training Loss: 0.03231724351644516\n",
      "Epoch 22837/30000 Training Loss: 0.03364023193717003\n",
      "Epoch 22838/30000 Training Loss: 0.041282638907432556\n",
      "Epoch 22839/30000 Training Loss: 0.05608727037906647\n",
      "Epoch 22840/30000 Training Loss: 0.054639969021081924\n",
      "Epoch 22841/30000 Training Loss: 0.05576600134372711\n",
      "Epoch 22842/30000 Training Loss: 0.05003754049539566\n",
      "Epoch 22843/30000 Training Loss: 0.05343061685562134\n",
      "Epoch 22844/30000 Training Loss: 0.034621164202690125\n",
      "Epoch 22845/30000 Training Loss: 0.034693896770477295\n",
      "Epoch 22846/30000 Training Loss: 0.03128879517316818\n",
      "Epoch 22847/30000 Training Loss: 0.05107024312019348\n",
      "Epoch 22848/30000 Training Loss: 0.044811513274908066\n",
      "Epoch 22849/30000 Training Loss: 0.05140412598848343\n",
      "Epoch 22850/30000 Training Loss: 0.03550519794225693\n",
      "Epoch 22851/30000 Training Loss: 0.027435867115855217\n",
      "Epoch 22852/30000 Training Loss: 0.04781793802976608\n",
      "Epoch 22853/30000 Training Loss: 0.037577591836452484\n",
      "Epoch 22854/30000 Training Loss: 0.03816469758749008\n",
      "Epoch 22855/30000 Training Loss: 0.0453767329454422\n",
      "Epoch 22856/30000 Training Loss: 0.04376968368887901\n",
      "Epoch 22857/30000 Training Loss: 0.04215874522924423\n",
      "Epoch 22858/30000 Training Loss: 0.03952757269144058\n",
      "Epoch 22859/30000 Training Loss: 0.05507960915565491\n",
      "Epoch 22860/30000 Training Loss: 0.037721797823905945\n",
      "Epoch 22861/30000 Training Loss: 0.036770034581422806\n",
      "Epoch 22862/30000 Training Loss: 0.03966602683067322\n",
      "Epoch 22863/30000 Training Loss: 0.04114721342921257\n",
      "Epoch 22864/30000 Training Loss: 0.03688075765967369\n",
      "Epoch 22865/30000 Training Loss: 0.045997217297554016\n",
      "Epoch 22866/30000 Training Loss: 0.05403418093919754\n",
      "Epoch 22867/30000 Training Loss: 0.04135267063975334\n",
      "Epoch 22868/30000 Training Loss: 0.04526568204164505\n",
      "Epoch 22869/30000 Training Loss: 0.04336978495121002\n",
      "Epoch 22870/30000 Training Loss: 0.033186208456754684\n",
      "Epoch 22871/30000 Training Loss: 0.04360656440258026\n",
      "Epoch 22872/30000 Training Loss: 0.03476735204458237\n",
      "Epoch 22873/30000 Training Loss: 0.04488533362746239\n",
      "Epoch 22874/30000 Training Loss: 0.0299755297601223\n",
      "Epoch 22875/30000 Training Loss: 0.030066797509789467\n",
      "Epoch 22876/30000 Training Loss: 0.03680151328444481\n",
      "Epoch 22877/30000 Training Loss: 0.033860765397548676\n",
      "Epoch 22878/30000 Training Loss: 0.05021623522043228\n",
      "Epoch 22879/30000 Training Loss: 0.050924528390169144\n",
      "Epoch 22880/30000 Training Loss: 0.04735792800784111\n",
      "Epoch 22881/30000 Training Loss: 0.04394426941871643\n",
      "Epoch 22882/30000 Training Loss: 0.05440160632133484\n",
      "Epoch 22883/30000 Training Loss: 0.06736983358860016\n",
      "Epoch 22884/30000 Training Loss: 0.04637180268764496\n",
      "Epoch 22885/30000 Training Loss: 0.0470992736518383\n",
      "Epoch 22886/30000 Training Loss: 0.04731174558401108\n",
      "Epoch 22887/30000 Training Loss: 0.04500872269272804\n",
      "Epoch 22888/30000 Training Loss: 0.036434635519981384\n",
      "Epoch 22889/30000 Training Loss: 0.03784298896789551\n",
      "Epoch 22890/30000 Training Loss: 0.040205810219049454\n",
      "Epoch 22891/30000 Training Loss: 0.04508387669920921\n",
      "Epoch 22892/30000 Training Loss: 0.054341815412044525\n",
      "Epoch 22893/30000 Training Loss: 0.042817965149879456\n",
      "Epoch 22894/30000 Training Loss: 0.052175506949424744\n",
      "Epoch 22895/30000 Training Loss: 0.046349961310625076\n",
      "Epoch 22896/30000 Training Loss: 0.07167987525463104\n",
      "Epoch 22897/30000 Training Loss: 0.0328202061355114\n",
      "Epoch 22898/30000 Training Loss: 0.047656260430812836\n",
      "Epoch 22899/30000 Training Loss: 0.04368206113576889\n",
      "Epoch 22900/30000 Training Loss: 0.04451043903827667\n",
      "Epoch 22900/30000 Validation Loss: 0.03934568911790848\n",
      "Epoch 22901/30000 Training Loss: 0.05240291729569435\n",
      "Epoch 22902/30000 Training Loss: 0.04508161544799805\n",
      "Epoch 22903/30000 Training Loss: 0.03896369785070419\n",
      "Epoch 22904/30000 Training Loss: 0.0414535254240036\n",
      "Epoch 22905/30000 Training Loss: 0.04132281616330147\n",
      "Epoch 22906/30000 Training Loss: 0.04548092931509018\n",
      "Epoch 22907/30000 Training Loss: 0.04602924734354019\n",
      "Epoch 22908/30000 Training Loss: 0.03846936672925949\n",
      "Epoch 22909/30000 Training Loss: 0.04696889594197273\n",
      "Epoch 22910/30000 Training Loss: 0.03923388198018074\n",
      "Epoch 22911/30000 Training Loss: 0.04407959431409836\n",
      "Epoch 22912/30000 Training Loss: 0.046366311609745026\n",
      "Epoch 22913/30000 Training Loss: 0.03527369350194931\n",
      "Epoch 22914/30000 Training Loss: 0.05043329298496246\n",
      "Epoch 22915/30000 Training Loss: 0.04758479818701744\n",
      "Epoch 22916/30000 Training Loss: 0.04305056482553482\n",
      "Epoch 22917/30000 Training Loss: 0.07585584372282028\n",
      "Epoch 22918/30000 Training Loss: 0.036313071846961975\n",
      "Epoch 22919/30000 Training Loss: 0.048228032886981964\n",
      "Epoch 22920/30000 Training Loss: 0.03882542625069618\n",
      "Epoch 22921/30000 Training Loss: 0.048619840294122696\n",
      "Epoch 22922/30000 Training Loss: 0.060294415801763535\n",
      "Epoch 22923/30000 Training Loss: 0.043638039380311966\n",
      "Epoch 22924/30000 Training Loss: 0.0389130562543869\n",
      "Epoch 22925/30000 Training Loss: 0.04647337272763252\n",
      "Epoch 22926/30000 Training Loss: 0.04480944573879242\n",
      "Epoch 22927/30000 Training Loss: 0.03799665346741676\n",
      "Epoch 22928/30000 Training Loss: 0.057159245014190674\n",
      "Epoch 22929/30000 Training Loss: 0.0572725385427475\n",
      "Epoch 22930/30000 Training Loss: 0.05200798436999321\n",
      "Epoch 22931/30000 Training Loss: 0.058996472507715225\n",
      "Epoch 22932/30000 Training Loss: 0.051787927746772766\n",
      "Epoch 22933/30000 Training Loss: 0.04488006979227066\n",
      "Epoch 22934/30000 Training Loss: 0.036092355847358704\n",
      "Epoch 22935/30000 Training Loss: 0.03966386616230011\n",
      "Epoch 22936/30000 Training Loss: 0.06225018948316574\n",
      "Epoch 22937/30000 Training Loss: 0.04507875815033913\n",
      "Epoch 22938/30000 Training Loss: 0.05467576906085014\n",
      "Epoch 22939/30000 Training Loss: 0.041589610278606415\n",
      "Epoch 22940/30000 Training Loss: 0.05474480241537094\n",
      "Epoch 22941/30000 Training Loss: 0.0652071014046669\n",
      "Epoch 22942/30000 Training Loss: 0.03804359212517738\n",
      "Epoch 22943/30000 Training Loss: 0.03857239708304405\n",
      "Epoch 22944/30000 Training Loss: 0.03767382726073265\n",
      "Epoch 22945/30000 Training Loss: 0.0502220094203949\n",
      "Epoch 22946/30000 Training Loss: 0.06339699774980545\n",
      "Epoch 22947/30000 Training Loss: 0.04334240034222603\n",
      "Epoch 22948/30000 Training Loss: 0.04082111269235611\n",
      "Epoch 22949/30000 Training Loss: 0.036366041749715805\n",
      "Epoch 22950/30000 Training Loss: 0.04926751181483269\n",
      "Epoch 22951/30000 Training Loss: 0.049735475331544876\n",
      "Epoch 22952/30000 Training Loss: 0.036528605967760086\n",
      "Epoch 22953/30000 Training Loss: 0.05248306691646576\n",
      "Epoch 22954/30000 Training Loss: 0.06489675492048264\n",
      "Epoch 22955/30000 Training Loss: 0.04449637979269028\n",
      "Epoch 22956/30000 Training Loss: 0.046062059700489044\n",
      "Epoch 22957/30000 Training Loss: 0.03841901943087578\n",
      "Epoch 22958/30000 Training Loss: 0.05728539824485779\n",
      "Epoch 22959/30000 Training Loss: 0.049606382846832275\n",
      "Epoch 22960/30000 Training Loss: 0.051843419671058655\n",
      "Epoch 22961/30000 Training Loss: 0.03104138746857643\n",
      "Epoch 22962/30000 Training Loss: 0.05330410227179527\n",
      "Epoch 22963/30000 Training Loss: 0.04430635645985603\n",
      "Epoch 22964/30000 Training Loss: 0.05177650600671768\n",
      "Epoch 22965/30000 Training Loss: 0.042470451444387436\n",
      "Epoch 22966/30000 Training Loss: 0.05015210807323456\n",
      "Epoch 22967/30000 Training Loss: 0.04271949827671051\n",
      "Epoch 22968/30000 Training Loss: 0.06658965349197388\n",
      "Epoch 22969/30000 Training Loss: 0.050974659621715546\n",
      "Epoch 22970/30000 Training Loss: 0.04141106829047203\n",
      "Epoch 22971/30000 Training Loss: 0.044938988983631134\n",
      "Epoch 22972/30000 Training Loss: 0.04549955949187279\n",
      "Epoch 22973/30000 Training Loss: 0.04988250136375427\n",
      "Epoch 22974/30000 Training Loss: 0.0662502720952034\n",
      "Epoch 22975/30000 Training Loss: 0.06314213573932648\n",
      "Epoch 22976/30000 Training Loss: 0.0713711678981781\n",
      "Epoch 22977/30000 Training Loss: 0.04999583214521408\n",
      "Epoch 22978/30000 Training Loss: 0.042763106524944305\n",
      "Epoch 22979/30000 Training Loss: 0.04563011974096298\n",
      "Epoch 22980/30000 Training Loss: 0.04311676695942879\n",
      "Epoch 22981/30000 Training Loss: 0.04582080990076065\n",
      "Epoch 22982/30000 Training Loss: 0.056588172912597656\n",
      "Epoch 22983/30000 Training Loss: 0.05828249454498291\n",
      "Epoch 22984/30000 Training Loss: 0.04670540988445282\n",
      "Epoch 22985/30000 Training Loss: 0.03157329186797142\n",
      "Epoch 22986/30000 Training Loss: 0.051264677196741104\n",
      "Epoch 22987/30000 Training Loss: 0.06301227957010269\n",
      "Epoch 22988/30000 Training Loss: 0.05652432143688202\n",
      "Epoch 22989/30000 Training Loss: 0.04926348850131035\n",
      "Epoch 22990/30000 Training Loss: 0.03517666831612587\n",
      "Epoch 22991/30000 Training Loss: 0.0316505953669548\n",
      "Epoch 22992/30000 Training Loss: 0.05124453455209732\n",
      "Epoch 22993/30000 Training Loss: 0.060545265674591064\n",
      "Epoch 22994/30000 Training Loss: 0.05154716596007347\n",
      "Epoch 22995/30000 Training Loss: 0.055822327733039856\n",
      "Epoch 22996/30000 Training Loss: 0.039511121809482574\n",
      "Epoch 22997/30000 Training Loss: 0.058867234736680984\n",
      "Epoch 22998/30000 Training Loss: 0.050462715327739716\n",
      "Epoch 22999/30000 Training Loss: 0.036091215908527374\n",
      "Epoch 23000/30000 Training Loss: 0.051521189510822296\n",
      "Epoch 23000/30000 Validation Loss: 0.038982417434453964\n",
      "Epoch 23001/30000 Training Loss: 0.04583127424120903\n",
      "Epoch 23002/30000 Training Loss: 0.05953945964574814\n",
      "Epoch 23003/30000 Training Loss: 0.050013355910778046\n",
      "Epoch 23004/30000 Training Loss: 0.05082477256655693\n",
      "Epoch 23005/30000 Training Loss: 0.03991035372018814\n",
      "Epoch 23006/30000 Training Loss: 0.051762767136096954\n",
      "Epoch 23007/30000 Training Loss: 0.04723960906267166\n",
      "Epoch 23008/30000 Training Loss: 0.04222017526626587\n",
      "Epoch 23009/30000 Training Loss: 0.05390104651451111\n",
      "Epoch 23010/30000 Training Loss: 0.04468497261404991\n",
      "Epoch 23011/30000 Training Loss: 0.051420968025922775\n",
      "Epoch 23012/30000 Training Loss: 0.03921299800276756\n",
      "Epoch 23013/30000 Training Loss: 0.05078333616256714\n",
      "Epoch 23014/30000 Training Loss: 0.03904442489147186\n",
      "Epoch 23015/30000 Training Loss: 0.044260647147893906\n",
      "Epoch 23016/30000 Training Loss: 0.05757254362106323\n",
      "Epoch 23017/30000 Training Loss: 0.03586841747164726\n",
      "Epoch 23018/30000 Training Loss: 0.03464459255337715\n",
      "Epoch 23019/30000 Training Loss: 0.04144786298274994\n",
      "Epoch 23020/30000 Training Loss: 0.05874950811266899\n",
      "Epoch 23021/30000 Training Loss: 0.042270392179489136\n",
      "Epoch 23022/30000 Training Loss: 0.054707787930965424\n",
      "Epoch 23023/30000 Training Loss: 0.03441843390464783\n",
      "Epoch 23024/30000 Training Loss: 0.04846370965242386\n",
      "Epoch 23025/30000 Training Loss: 0.03589259088039398\n",
      "Epoch 23026/30000 Training Loss: 0.06353222578763962\n",
      "Epoch 23027/30000 Training Loss: 0.04728379845619202\n",
      "Epoch 23028/30000 Training Loss: 0.05133039504289627\n",
      "Epoch 23029/30000 Training Loss: 0.035410791635513306\n",
      "Epoch 23030/30000 Training Loss: 0.03726085647940636\n",
      "Epoch 23031/30000 Training Loss: 0.045825619250535965\n",
      "Epoch 23032/30000 Training Loss: 0.035288382321596146\n",
      "Epoch 23033/30000 Training Loss: 0.03826630115509033\n",
      "Epoch 23034/30000 Training Loss: 0.05225051939487457\n",
      "Epoch 23035/30000 Training Loss: 0.04160382226109505\n",
      "Epoch 23036/30000 Training Loss: 0.043317876756191254\n",
      "Epoch 23037/30000 Training Loss: 0.02921963669359684\n",
      "Epoch 23038/30000 Training Loss: 0.04003700613975525\n",
      "Epoch 23039/30000 Training Loss: 0.042671360075473785\n",
      "Epoch 23040/30000 Training Loss: 0.051699940115213394\n",
      "Epoch 23041/30000 Training Loss: 0.042734671384096146\n",
      "Epoch 23042/30000 Training Loss: 0.04264497756958008\n",
      "Epoch 23043/30000 Training Loss: 0.06169244274497032\n",
      "Epoch 23044/30000 Training Loss: 0.046746499836444855\n",
      "Epoch 23045/30000 Training Loss: 0.049443431198596954\n",
      "Epoch 23046/30000 Training Loss: 0.045407943427562714\n",
      "Epoch 23047/30000 Training Loss: 0.049557991325855255\n",
      "Epoch 23048/30000 Training Loss: 0.04681362211704254\n",
      "Epoch 23049/30000 Training Loss: 0.04222683608531952\n",
      "Epoch 23050/30000 Training Loss: 0.0369328036904335\n",
      "Epoch 23051/30000 Training Loss: 0.04417581483721733\n",
      "Epoch 23052/30000 Training Loss: 0.04938851669430733\n",
      "Epoch 23053/30000 Training Loss: 0.05580240488052368\n",
      "Epoch 23054/30000 Training Loss: 0.04914247617125511\n",
      "Epoch 23055/30000 Training Loss: 0.05351467430591583\n",
      "Epoch 23056/30000 Training Loss: 0.05921931564807892\n",
      "Epoch 23057/30000 Training Loss: 0.05306712165474892\n",
      "Epoch 23058/30000 Training Loss: 0.06399984657764435\n",
      "Epoch 23059/30000 Training Loss: 0.04216737300157547\n",
      "Epoch 23060/30000 Training Loss: 0.046606749296188354\n",
      "Epoch 23061/30000 Training Loss: 0.05022575333714485\n",
      "Epoch 23062/30000 Training Loss: 0.05937333032488823\n",
      "Epoch 23063/30000 Training Loss: 0.043034251779317856\n",
      "Epoch 23064/30000 Training Loss: 0.04713231697678566\n",
      "Epoch 23065/30000 Training Loss: 0.040410060435533524\n",
      "Epoch 23066/30000 Training Loss: 0.04190433770418167\n",
      "Epoch 23067/30000 Training Loss: 0.051503367722034454\n",
      "Epoch 23068/30000 Training Loss: 0.04272368550300598\n",
      "Epoch 23069/30000 Training Loss: 0.04559476673603058\n",
      "Epoch 23070/30000 Training Loss: 0.05192699283361435\n",
      "Epoch 23071/30000 Training Loss: 0.03044900670647621\n",
      "Epoch 23072/30000 Training Loss: 0.05858341604471207\n",
      "Epoch 23073/30000 Training Loss: 0.03673708438873291\n",
      "Epoch 23074/30000 Training Loss: 0.057390131056308746\n",
      "Epoch 23075/30000 Training Loss: 0.04559534043073654\n",
      "Epoch 23076/30000 Training Loss: 0.06461390107870102\n",
      "Epoch 23077/30000 Training Loss: 0.04217088222503662\n",
      "Epoch 23078/30000 Training Loss: 0.030688762664794922\n",
      "Epoch 23079/30000 Training Loss: 0.05573757737874985\n",
      "Epoch 23080/30000 Training Loss: 0.03391976282000542\n",
      "Epoch 23081/30000 Training Loss: 0.040489502251148224\n",
      "Epoch 23082/30000 Training Loss: 0.046912580728530884\n",
      "Epoch 23083/30000 Training Loss: 0.0342421755194664\n",
      "Epoch 23084/30000 Training Loss: 0.04621761664748192\n",
      "Epoch 23085/30000 Training Loss: 0.0368109755218029\n",
      "Epoch 23086/30000 Training Loss: 0.06044679880142212\n",
      "Epoch 23087/30000 Training Loss: 0.04217108339071274\n",
      "Epoch 23088/30000 Training Loss: 0.03471387177705765\n",
      "Epoch 23089/30000 Training Loss: 0.040007349103689194\n",
      "Epoch 23090/30000 Training Loss: 0.052675552666187286\n",
      "Epoch 23091/30000 Training Loss: 0.04864303767681122\n",
      "Epoch 23092/30000 Training Loss: 0.05650137737393379\n",
      "Epoch 23093/30000 Training Loss: 0.04392767697572708\n",
      "Epoch 23094/30000 Training Loss: 0.047998860478401184\n",
      "Epoch 23095/30000 Training Loss: 0.055870912969112396\n",
      "Epoch 23096/30000 Training Loss: 0.06350190937519073\n",
      "Epoch 23097/30000 Training Loss: 0.041011374443769455\n",
      "Epoch 23098/30000 Training Loss: 0.052238184958696365\n",
      "Epoch 23099/30000 Training Loss: 0.06266402453184128\n",
      "Epoch 23100/30000 Training Loss: 0.04109295457601547\n",
      "Epoch 23100/30000 Validation Loss: 0.05433778092265129\n",
      "Epoch 23101/30000 Training Loss: 0.034758489578962326\n",
      "Epoch 23102/30000 Training Loss: 0.04592934250831604\n",
      "Epoch 23103/30000 Training Loss: 0.04578094556927681\n",
      "Epoch 23104/30000 Training Loss: 0.05192113667726517\n",
      "Epoch 23105/30000 Training Loss: 0.062208760529756546\n",
      "Epoch 23106/30000 Training Loss: 0.03144805505871773\n",
      "Epoch 23107/30000 Training Loss: 0.04370373487472534\n",
      "Epoch 23108/30000 Training Loss: 0.04494502395391464\n",
      "Epoch 23109/30000 Training Loss: 0.03695187717676163\n",
      "Epoch 23110/30000 Training Loss: 0.0385386198759079\n",
      "Epoch 23111/30000 Training Loss: 0.04065016657114029\n",
      "Epoch 23112/30000 Training Loss: 0.03998628631234169\n",
      "Epoch 23113/30000 Training Loss: 0.06250616163015366\n",
      "Epoch 23114/30000 Training Loss: 0.042462460696697235\n",
      "Epoch 23115/30000 Training Loss: 0.04133756458759308\n",
      "Epoch 23116/30000 Training Loss: 0.05738278850913048\n",
      "Epoch 23117/30000 Training Loss: 0.049341920763254166\n",
      "Epoch 23118/30000 Training Loss: 0.04490382596850395\n",
      "Epoch 23119/30000 Training Loss: 0.05915237218141556\n",
      "Epoch 23120/30000 Training Loss: 0.05665735900402069\n",
      "Epoch 23121/30000 Training Loss: 0.03259439766407013\n",
      "Epoch 23122/30000 Training Loss: 0.05295700207352638\n",
      "Epoch 23123/30000 Training Loss: 0.03277374058961868\n",
      "Epoch 23124/30000 Training Loss: 0.04868157207965851\n",
      "Epoch 23125/30000 Training Loss: 0.03729023039340973\n",
      "Epoch 23126/30000 Training Loss: 0.039367418736219406\n",
      "Epoch 23127/30000 Training Loss: 0.043470531702041626\n",
      "Epoch 23128/30000 Training Loss: 0.05814645439386368\n",
      "Epoch 23129/30000 Training Loss: 0.03563307225704193\n",
      "Epoch 23130/30000 Training Loss: 0.0508851483464241\n",
      "Epoch 23131/30000 Training Loss: 0.06633999943733215\n",
      "Epoch 23132/30000 Training Loss: 0.052682384848594666\n",
      "Epoch 23133/30000 Training Loss: 0.04037753492593765\n",
      "Epoch 23134/30000 Training Loss: 0.04610218107700348\n",
      "Epoch 23135/30000 Training Loss: 0.04993526637554169\n",
      "Epoch 23136/30000 Training Loss: 0.029879039153456688\n",
      "Epoch 23137/30000 Training Loss: 0.042767126113176346\n",
      "Epoch 23138/30000 Training Loss: 0.04614591971039772\n",
      "Epoch 23139/30000 Training Loss: 0.04346475005149841\n",
      "Epoch 23140/30000 Training Loss: 0.042667485773563385\n",
      "Epoch 23141/30000 Training Loss: 0.049646325409412384\n",
      "Epoch 23142/30000 Training Loss: 0.050292760133743286\n",
      "Epoch 23143/30000 Training Loss: 0.0551418662071228\n",
      "Epoch 23144/30000 Training Loss: 0.04965800419449806\n",
      "Epoch 23145/30000 Training Loss: 0.046927791088819504\n",
      "Epoch 23146/30000 Training Loss: 0.038749661296606064\n",
      "Epoch 23147/30000 Training Loss: 0.0511835440993309\n",
      "Epoch 23148/30000 Training Loss: 0.04132106900215149\n",
      "Epoch 23149/30000 Training Loss: 0.04275433346629143\n",
      "Epoch 23150/30000 Training Loss: 0.038347020745277405\n",
      "Epoch 23151/30000 Training Loss: 0.03897076100111008\n",
      "Epoch 23152/30000 Training Loss: 0.04906492680311203\n",
      "Epoch 23153/30000 Training Loss: 0.03880300745368004\n",
      "Epoch 23154/30000 Training Loss: 0.06169505417346954\n",
      "Epoch 23155/30000 Training Loss: 0.04379231855273247\n",
      "Epoch 23156/30000 Training Loss: 0.046736959367990494\n",
      "Epoch 23157/30000 Training Loss: 0.04330549016594887\n",
      "Epoch 23158/30000 Training Loss: 0.038045138120651245\n",
      "Epoch 23159/30000 Training Loss: 0.047464385628700256\n",
      "Epoch 23160/30000 Training Loss: 0.04037616029381752\n",
      "Epoch 23161/30000 Training Loss: 0.05129591003060341\n",
      "Epoch 23162/30000 Training Loss: 0.03346843272447586\n",
      "Epoch 23163/30000 Training Loss: 0.05982675403356552\n",
      "Epoch 23164/30000 Training Loss: 0.04796682298183441\n",
      "Epoch 23165/30000 Training Loss: 0.05412730574607849\n",
      "Epoch 23166/30000 Training Loss: 0.04815671965479851\n",
      "Epoch 23167/30000 Training Loss: 0.05041439086198807\n",
      "Epoch 23168/30000 Training Loss: 0.049345120787620544\n",
      "Epoch 23169/30000 Training Loss: 0.045028556138277054\n",
      "Epoch 23170/30000 Training Loss: 0.05365476757287979\n",
      "Epoch 23171/30000 Training Loss: 0.038886018097400665\n",
      "Epoch 23172/30000 Training Loss: 0.06389567255973816\n",
      "Epoch 23173/30000 Training Loss: 0.05076472461223602\n",
      "Epoch 23174/30000 Training Loss: 0.0467473566532135\n",
      "Epoch 23175/30000 Training Loss: 0.04335334151983261\n",
      "Epoch 23176/30000 Training Loss: 0.05700502172112465\n",
      "Epoch 23177/30000 Training Loss: 0.05655403807759285\n",
      "Epoch 23178/30000 Training Loss: 0.06548097729682922\n",
      "Epoch 23179/30000 Training Loss: 0.05651143565773964\n",
      "Epoch 23180/30000 Training Loss: 0.041247956454753876\n",
      "Epoch 23181/30000 Training Loss: 0.04561115428805351\n",
      "Epoch 23182/30000 Training Loss: 0.052190933376550674\n",
      "Epoch 23183/30000 Training Loss: 0.05833882838487625\n",
      "Epoch 23184/30000 Training Loss: 0.047548964619636536\n",
      "Epoch 23185/30000 Training Loss: 0.05062805861234665\n",
      "Epoch 23186/30000 Training Loss: 0.05351701378822327\n",
      "Epoch 23187/30000 Training Loss: 0.038670614361763\n",
      "Epoch 23188/30000 Training Loss: 0.045969683676958084\n",
      "Epoch 23189/30000 Training Loss: 0.04390217363834381\n",
      "Epoch 23190/30000 Training Loss: 0.05655071511864662\n",
      "Epoch 23191/30000 Training Loss: 0.06313086301088333\n",
      "Epoch 23192/30000 Training Loss: 0.03405199944972992\n",
      "Epoch 23193/30000 Training Loss: 0.04679504781961441\n",
      "Epoch 23194/30000 Training Loss: 0.05149075388908386\n",
      "Epoch 23195/30000 Training Loss: 0.04693989455699921\n",
      "Epoch 23196/30000 Training Loss: 0.04517856240272522\n",
      "Epoch 23197/30000 Training Loss: 0.04439990967512131\n",
      "Epoch 23198/30000 Training Loss: 0.04592049494385719\n",
      "Epoch 23199/30000 Training Loss: 0.04681595414876938\n",
      "Epoch 23200/30000 Training Loss: 0.054625459015369415\n",
      "Epoch 23200/30000 Validation Loss: 0.0486316978931427\n",
      "Epoch 23201/30000 Training Loss: 0.04150160029530525\n",
      "Epoch 23202/30000 Training Loss: 0.04304630309343338\n",
      "Epoch 23203/30000 Training Loss: 0.05190838500857353\n",
      "Epoch 23204/30000 Training Loss: 0.03353211283683777\n",
      "Epoch 23205/30000 Training Loss: 0.04723741114139557\n",
      "Epoch 23206/30000 Training Loss: 0.058138519525527954\n",
      "Epoch 23207/30000 Training Loss: 0.04775446653366089\n",
      "Epoch 23208/30000 Training Loss: 0.047821059823036194\n",
      "Epoch 23209/30000 Training Loss: 0.04679711163043976\n",
      "Epoch 23210/30000 Training Loss: 0.039719268679618835\n",
      "Epoch 23211/30000 Training Loss: 0.04573512822389603\n",
      "Epoch 23212/30000 Training Loss: 0.03834923356771469\n",
      "Epoch 23213/30000 Training Loss: 0.04680559039115906\n",
      "Epoch 23214/30000 Training Loss: 0.03884856775403023\n",
      "Epoch 23215/30000 Training Loss: 0.05720820277929306\n",
      "Epoch 23216/30000 Training Loss: 0.03770096227526665\n",
      "Epoch 23217/30000 Training Loss: 0.03141980990767479\n",
      "Epoch 23218/30000 Training Loss: 0.04431620612740517\n",
      "Epoch 23219/30000 Training Loss: 0.04529604688286781\n",
      "Epoch 23220/30000 Training Loss: 0.03586743026971817\n",
      "Epoch 23221/30000 Training Loss: 0.0395306721329689\n",
      "Epoch 23222/30000 Training Loss: 0.03519589453935623\n",
      "Epoch 23223/30000 Training Loss: 0.05240871012210846\n",
      "Epoch 23224/30000 Training Loss: 0.03828093037009239\n",
      "Epoch 23225/30000 Training Loss: 0.04814514145255089\n",
      "Epoch 23226/30000 Training Loss: 0.037076257169246674\n",
      "Epoch 23227/30000 Training Loss: 0.044393979012966156\n",
      "Epoch 23228/30000 Training Loss: 0.046544596552848816\n",
      "Epoch 23229/30000 Training Loss: 0.0549594983458519\n",
      "Epoch 23230/30000 Training Loss: 0.05316358059644699\n",
      "Epoch 23231/30000 Training Loss: 0.04836473986506462\n",
      "Epoch 23232/30000 Training Loss: 0.04929248243570328\n",
      "Epoch 23233/30000 Training Loss: 0.04898673668503761\n",
      "Epoch 23234/30000 Training Loss: 0.05408347398042679\n",
      "Epoch 23235/30000 Training Loss: 0.04485776275396347\n",
      "Epoch 23236/30000 Training Loss: 0.048166584223508835\n",
      "Epoch 23237/30000 Training Loss: 0.05467527359724045\n",
      "Epoch 23238/30000 Training Loss: 0.030050046741962433\n",
      "Epoch 23239/30000 Training Loss: 0.027919216081500053\n",
      "Epoch 23240/30000 Training Loss: 0.0420098602771759\n",
      "Epoch 23241/30000 Training Loss: 0.061593152582645416\n",
      "Epoch 23242/30000 Training Loss: 0.03845910355448723\n",
      "Epoch 23243/30000 Training Loss: 0.046473000198602676\n",
      "Epoch 23244/30000 Training Loss: 0.05415923520922661\n",
      "Epoch 23245/30000 Training Loss: 0.044317711144685745\n",
      "Epoch 23246/30000 Training Loss: 0.05951717123389244\n",
      "Epoch 23247/30000 Training Loss: 0.04209583252668381\n",
      "Epoch 23248/30000 Training Loss: 0.054672226309776306\n",
      "Epoch 23249/30000 Training Loss: 0.0490199439227581\n",
      "Epoch 23250/30000 Training Loss: 0.05247700214385986\n",
      "Epoch 23251/30000 Training Loss: 0.05262984335422516\n",
      "Epoch 23252/30000 Training Loss: 0.040438197553157806\n",
      "Epoch 23253/30000 Training Loss: 0.061067789793014526\n",
      "Epoch 23254/30000 Training Loss: 0.03675137460231781\n",
      "Epoch 23255/30000 Training Loss: 0.04603542760014534\n",
      "Epoch 23256/30000 Training Loss: 0.042807936668395996\n",
      "Epoch 23257/30000 Training Loss: 0.05743418633937836\n",
      "Epoch 23258/30000 Training Loss: 0.06141456216573715\n",
      "Epoch 23259/30000 Training Loss: 0.04700993746519089\n",
      "Epoch 23260/30000 Training Loss: 0.05333171412348747\n",
      "Epoch 23261/30000 Training Loss: 0.051240578293800354\n",
      "Epoch 23262/30000 Training Loss: 0.03095216490328312\n",
      "Epoch 23263/30000 Training Loss: 0.028796963393688202\n",
      "Epoch 23264/30000 Training Loss: 0.037223730236291885\n",
      "Epoch 23265/30000 Training Loss: 0.04032355546951294\n",
      "Epoch 23266/30000 Training Loss: 0.04114008694887161\n",
      "Epoch 23267/30000 Training Loss: 0.040063824504613876\n",
      "Epoch 23268/30000 Training Loss: 0.04535433650016785\n",
      "Epoch 23269/30000 Training Loss: 0.04589792340993881\n",
      "Epoch 23270/30000 Training Loss: 0.03494570776820183\n",
      "Epoch 23271/30000 Training Loss: 0.0356399305164814\n",
      "Epoch 23272/30000 Training Loss: 0.0430317223072052\n",
      "Epoch 23273/30000 Training Loss: 0.03587575629353523\n",
      "Epoch 23274/30000 Training Loss: 0.044592417776584625\n",
      "Epoch 23275/30000 Training Loss: 0.04677692800760269\n",
      "Epoch 23276/30000 Training Loss: 0.04012419283390045\n",
      "Epoch 23277/30000 Training Loss: 0.04473397880792618\n",
      "Epoch 23278/30000 Training Loss: 0.04145146161317825\n",
      "Epoch 23279/30000 Training Loss: 0.04409104213118553\n",
      "Epoch 23280/30000 Training Loss: 0.056078650057315826\n",
      "Epoch 23281/30000 Training Loss: 0.04198550432920456\n",
      "Epoch 23282/30000 Training Loss: 0.04157276824116707\n",
      "Epoch 23283/30000 Training Loss: 0.05725795030593872\n",
      "Epoch 23284/30000 Training Loss: 0.03454126417636871\n",
      "Epoch 23285/30000 Training Loss: 0.04064600169658661\n",
      "Epoch 23286/30000 Training Loss: 0.06057678908109665\n",
      "Epoch 23287/30000 Training Loss: 0.034209512174129486\n",
      "Epoch 23288/30000 Training Loss: 0.0536903440952301\n",
      "Epoch 23289/30000 Training Loss: 0.051841851323843\n",
      "Epoch 23290/30000 Training Loss: 0.03398028761148453\n",
      "Epoch 23291/30000 Training Loss: 0.0467381626367569\n",
      "Epoch 23292/30000 Training Loss: 0.03929882124066353\n",
      "Epoch 23293/30000 Training Loss: 0.04713232442736626\n",
      "Epoch 23294/30000 Training Loss: 0.042294491082429886\n",
      "Epoch 23295/30000 Training Loss: 0.03947960585355759\n",
      "Epoch 23296/30000 Training Loss: 0.04301823675632477\n",
      "Epoch 23297/30000 Training Loss: 0.04038622975349426\n",
      "Epoch 23298/30000 Training Loss: 0.038989804685115814\n",
      "Epoch 23299/30000 Training Loss: 0.04200436547398567\n",
      "Epoch 23300/30000 Training Loss: 0.05675192549824715\n",
      "Epoch 23300/30000 Validation Loss: 0.06941631436347961\n",
      "Epoch 23301/30000 Training Loss: 0.040178634226322174\n",
      "Epoch 23302/30000 Training Loss: 0.04611969739198685\n",
      "Epoch 23303/30000 Training Loss: 0.043420229107141495\n",
      "Epoch 23304/30000 Training Loss: 0.07571400701999664\n",
      "Epoch 23305/30000 Training Loss: 0.03858020901679993\n",
      "Epoch 23306/30000 Training Loss: 0.04975978657603264\n",
      "Epoch 23307/30000 Training Loss: 0.042931169271469116\n",
      "Epoch 23308/30000 Training Loss: 0.054596662521362305\n",
      "Epoch 23309/30000 Training Loss: 0.048829663544893265\n",
      "Epoch 23310/30000 Training Loss: 0.06113549321889877\n",
      "Epoch 23311/30000 Training Loss: 0.048481784760951996\n",
      "Epoch 23312/30000 Training Loss: 0.04213486984372139\n",
      "Epoch 23313/30000 Training Loss: 0.044337984174489975\n",
      "Epoch 23314/30000 Training Loss: 0.03852773457765579\n",
      "Epoch 23315/30000 Training Loss: 0.05381369590759277\n",
      "Epoch 23316/30000 Training Loss: 0.052663080394268036\n",
      "Epoch 23317/30000 Training Loss: 0.03933832049369812\n",
      "Epoch 23318/30000 Training Loss: 0.04508253186941147\n",
      "Epoch 23319/30000 Training Loss: 0.0447552353143692\n",
      "Epoch 23320/30000 Training Loss: 0.055417224764823914\n",
      "Epoch 23321/30000 Training Loss: 0.030920997262001038\n",
      "Epoch 23322/30000 Training Loss: 0.05296066775918007\n",
      "Epoch 23323/30000 Training Loss: 0.04722464084625244\n",
      "Epoch 23324/30000 Training Loss: 0.04950754716992378\n",
      "Epoch 23325/30000 Training Loss: 0.04768040031194687\n",
      "Epoch 23326/30000 Training Loss: 0.04102358594536781\n",
      "Epoch 23327/30000 Training Loss: 0.05219174548983574\n",
      "Epoch 23328/30000 Training Loss: 0.03678955137729645\n",
      "Epoch 23329/30000 Training Loss: 0.07439526170492172\n",
      "Epoch 23330/30000 Training Loss: 0.05040059983730316\n",
      "Epoch 23331/30000 Training Loss: 0.0439080148935318\n",
      "Epoch 23332/30000 Training Loss: 0.039169181138277054\n",
      "Epoch 23333/30000 Training Loss: 0.04818691313266754\n",
      "Epoch 23334/30000 Training Loss: 0.042427677661180496\n",
      "Epoch 23335/30000 Training Loss: 0.03953954204916954\n",
      "Epoch 23336/30000 Training Loss: 0.08499420434236526\n",
      "Epoch 23337/30000 Training Loss: 0.04283663630485535\n",
      "Epoch 23338/30000 Training Loss: 0.0387791208922863\n",
      "Epoch 23339/30000 Training Loss: 0.055983662605285645\n",
      "Epoch 23340/30000 Training Loss: 0.05495588481426239\n",
      "Epoch 23341/30000 Training Loss: 0.0644199326634407\n",
      "Epoch 23342/30000 Training Loss: 0.04312010109424591\n",
      "Epoch 23343/30000 Training Loss: 0.05139416828751564\n",
      "Epoch 23344/30000 Training Loss: 0.03802086040377617\n",
      "Epoch 23345/30000 Training Loss: 0.05144033581018448\n",
      "Epoch 23346/30000 Training Loss: 0.050147999078035355\n",
      "Epoch 23347/30000 Training Loss: 0.05612738057971001\n",
      "Epoch 23348/30000 Training Loss: 0.055086761713027954\n",
      "Epoch 23349/30000 Training Loss: 0.039874881505966187\n",
      "Epoch 23350/30000 Training Loss: 0.04184709116816521\n",
      "Epoch 23351/30000 Training Loss: 0.07643511146306992\n",
      "Epoch 23352/30000 Training Loss: 0.03316601365804672\n",
      "Epoch 23353/30000 Training Loss: 0.045173823833465576\n",
      "Epoch 23354/30000 Training Loss: 0.04743516072630882\n",
      "Epoch 23355/30000 Training Loss: 0.03927309066057205\n",
      "Epoch 23356/30000 Training Loss: 0.05772674083709717\n",
      "Epoch 23357/30000 Training Loss: 0.048402465879917145\n",
      "Epoch 23358/30000 Training Loss: 0.0576067715883255\n",
      "Epoch 23359/30000 Training Loss: 0.041362036019563675\n",
      "Epoch 23360/30000 Training Loss: 0.04060668125748634\n",
      "Epoch 23361/30000 Training Loss: 0.037586990743875504\n",
      "Epoch 23362/30000 Training Loss: 0.035915032029151917\n",
      "Epoch 23363/30000 Training Loss: 0.042423129081726074\n",
      "Epoch 23364/30000 Training Loss: 0.053710926324129105\n",
      "Epoch 23365/30000 Training Loss: 0.04404589906334877\n",
      "Epoch 23366/30000 Training Loss: 0.05073590576648712\n",
      "Epoch 23367/30000 Training Loss: 0.04460398480296135\n",
      "Epoch 23368/30000 Training Loss: 0.04183943197131157\n",
      "Epoch 23369/30000 Training Loss: 0.05881863087415695\n",
      "Epoch 23370/30000 Training Loss: 0.03941269591450691\n",
      "Epoch 23371/30000 Training Loss: 0.03597792237997055\n",
      "Epoch 23372/30000 Training Loss: 0.051346756517887115\n",
      "Epoch 23373/30000 Training Loss: 0.03643510490655899\n",
      "Epoch 23374/30000 Training Loss: 0.04399166628718376\n",
      "Epoch 23375/30000 Training Loss: 0.04710394889116287\n",
      "Epoch 23376/30000 Training Loss: 0.04837063327431679\n",
      "Epoch 23377/30000 Training Loss: 0.04122084379196167\n",
      "Epoch 23378/30000 Training Loss: 0.04000256955623627\n",
      "Epoch 23379/30000 Training Loss: 0.04327687993645668\n",
      "Epoch 23380/30000 Training Loss: 0.046952132135629654\n",
      "Epoch 23381/30000 Training Loss: 0.04716300219297409\n",
      "Epoch 23382/30000 Training Loss: 0.046166978776454926\n",
      "Epoch 23383/30000 Training Loss: 0.0490468330681324\n",
      "Epoch 23384/30000 Training Loss: 0.0432954803109169\n",
      "Epoch 23385/30000 Training Loss: 0.04196463152766228\n",
      "Epoch 23386/30000 Training Loss: 0.051100440323352814\n",
      "Epoch 23387/30000 Training Loss: 0.04041138291358948\n",
      "Epoch 23388/30000 Training Loss: 0.03578383848071098\n",
      "Epoch 23389/30000 Training Loss: 0.056532666087150574\n",
      "Epoch 23390/30000 Training Loss: 0.037679895758628845\n",
      "Epoch 23391/30000 Training Loss: 0.054785747081041336\n",
      "Epoch 23392/30000 Training Loss: 0.03823244944214821\n",
      "Epoch 23393/30000 Training Loss: 0.05306057631969452\n",
      "Epoch 23394/30000 Training Loss: 0.041953958570957184\n",
      "Epoch 23395/30000 Training Loss: 0.05487292259931564\n",
      "Epoch 23396/30000 Training Loss: 0.04559905454516411\n",
      "Epoch 23397/30000 Training Loss: 0.045957475900650024\n",
      "Epoch 23398/30000 Training Loss: 0.03394331410527229\n",
      "Epoch 23399/30000 Training Loss: 0.04134856536984444\n",
      "Epoch 23400/30000 Training Loss: 0.03735683113336563\n",
      "Epoch 23400/30000 Validation Loss: 0.04751712083816528\n",
      "Epoch 23401/30000 Training Loss: 0.045909225940704346\n",
      "Epoch 23402/30000 Training Loss: 0.04481414332985878\n",
      "Epoch 23403/30000 Training Loss: 0.057986069470644\n",
      "Epoch 23404/30000 Training Loss: 0.04901817813515663\n",
      "Epoch 23405/30000 Training Loss: 0.05240778625011444\n",
      "Epoch 23406/30000 Training Loss: 0.044628605246543884\n",
      "Epoch 23407/30000 Training Loss: 0.03999520093202591\n",
      "Epoch 23408/30000 Training Loss: 0.03962242975831032\n",
      "Epoch 23409/30000 Training Loss: 0.041081082075834274\n",
      "Epoch 23410/30000 Training Loss: 0.051404163241386414\n",
      "Epoch 23411/30000 Training Loss: 0.06465577334165573\n",
      "Epoch 23412/30000 Training Loss: 0.054653581231832504\n",
      "Epoch 23413/30000 Training Loss: 0.04921966418623924\n",
      "Epoch 23414/30000 Training Loss: 0.04809366911649704\n",
      "Epoch 23415/30000 Training Loss: 0.04176805913448334\n",
      "Epoch 23416/30000 Training Loss: 0.07115784287452698\n",
      "Epoch 23417/30000 Training Loss: 0.05154143273830414\n",
      "Epoch 23418/30000 Training Loss: 0.05402369052171707\n",
      "Epoch 23419/30000 Training Loss: 0.04296896979212761\n",
      "Epoch 23420/30000 Training Loss: 0.039602503180503845\n",
      "Epoch 23421/30000 Training Loss: 0.06316465884447098\n",
      "Epoch 23422/30000 Training Loss: 0.04790131002664566\n",
      "Epoch 23423/30000 Training Loss: 0.043565962463617325\n",
      "Epoch 23424/30000 Training Loss: 0.04493814334273338\n",
      "Epoch 23425/30000 Training Loss: 0.038540709763765335\n",
      "Epoch 23426/30000 Training Loss: 0.04188579320907593\n",
      "Epoch 23427/30000 Training Loss: 0.03800038993358612\n",
      "Epoch 23428/30000 Training Loss: 0.04954047128558159\n",
      "Epoch 23429/30000 Training Loss: 0.04996351897716522\n",
      "Epoch 23430/30000 Training Loss: 0.04828092083334923\n",
      "Epoch 23431/30000 Training Loss: 0.06123935431241989\n",
      "Epoch 23432/30000 Training Loss: 0.046795204281806946\n",
      "Epoch 23433/30000 Training Loss: 0.04539109021425247\n",
      "Epoch 23434/30000 Training Loss: 0.05446820706129074\n",
      "Epoch 23435/30000 Training Loss: 0.05478394031524658\n",
      "Epoch 23436/30000 Training Loss: 0.032818879932165146\n",
      "Epoch 23437/30000 Training Loss: 0.03775179758667946\n",
      "Epoch 23438/30000 Training Loss: 0.04084199666976929\n",
      "Epoch 23439/30000 Training Loss: 0.050820134580135345\n",
      "Epoch 23440/30000 Training Loss: 0.0610879510641098\n",
      "Epoch 23441/30000 Training Loss: 0.04769350588321686\n",
      "Epoch 23442/30000 Training Loss: 0.03630469739437103\n",
      "Epoch 23443/30000 Training Loss: 0.046503696590662\n",
      "Epoch 23444/30000 Training Loss: 0.04372204467654228\n",
      "Epoch 23445/30000 Training Loss: 0.07018560916185379\n",
      "Epoch 23446/30000 Training Loss: 0.031091934069991112\n",
      "Epoch 23447/30000 Training Loss: 0.04440319538116455\n",
      "Epoch 23448/30000 Training Loss: 0.05563757196068764\n",
      "Epoch 23449/30000 Training Loss: 0.05780453979969025\n",
      "Epoch 23450/30000 Training Loss: 0.029777660965919495\n",
      "Epoch 23451/30000 Training Loss: 0.07614366710186005\n",
      "Epoch 23452/30000 Training Loss: 0.053942278027534485\n",
      "Epoch 23453/30000 Training Loss: 0.04766204208135605\n",
      "Epoch 23454/30000 Training Loss: 0.030893202871084213\n",
      "Epoch 23455/30000 Training Loss: 0.05138450860977173\n",
      "Epoch 23456/30000 Training Loss: 0.05825774744153023\n",
      "Epoch 23457/30000 Training Loss: 0.045089900493621826\n",
      "Epoch 23458/30000 Training Loss: 0.031195927411317825\n",
      "Epoch 23459/30000 Training Loss: 0.052789345383644104\n",
      "Epoch 23460/30000 Training Loss: 0.04655280336737633\n",
      "Epoch 23461/30000 Training Loss: 0.04159800708293915\n",
      "Epoch 23462/30000 Training Loss: 0.05498756095767021\n",
      "Epoch 23463/30000 Training Loss: 0.04936891049146652\n",
      "Epoch 23464/30000 Training Loss: 0.03561784327030182\n",
      "Epoch 23465/30000 Training Loss: 0.0575210377573967\n",
      "Epoch 23466/30000 Training Loss: 0.047454312443733215\n",
      "Epoch 23467/30000 Training Loss: 0.03699319064617157\n",
      "Epoch 23468/30000 Training Loss: 0.04608222842216492\n",
      "Epoch 23469/30000 Training Loss: 0.04122351109981537\n",
      "Epoch 23470/30000 Training Loss: 0.04597077518701553\n",
      "Epoch 23471/30000 Training Loss: 0.05455493554472923\n",
      "Epoch 23472/30000 Training Loss: 0.040294960141181946\n",
      "Epoch 23473/30000 Training Loss: 0.03863605111837387\n",
      "Epoch 23474/30000 Training Loss: 0.03789446875452995\n",
      "Epoch 23475/30000 Training Loss: 0.03848828375339508\n",
      "Epoch 23476/30000 Training Loss: 0.04362674802541733\n",
      "Epoch 23477/30000 Training Loss: 0.036335911601781845\n",
      "Epoch 23478/30000 Training Loss: 0.04586361348628998\n",
      "Epoch 23479/30000 Training Loss: 0.0410495288670063\n",
      "Epoch 23480/30000 Training Loss: 0.047985415905714035\n",
      "Epoch 23481/30000 Training Loss: 0.04315173625946045\n",
      "Epoch 23482/30000 Training Loss: 0.04708702489733696\n",
      "Epoch 23483/30000 Training Loss: 0.046222709119319916\n",
      "Epoch 23484/30000 Training Loss: 0.054152585566043854\n",
      "Epoch 23485/30000 Training Loss: 0.0471617765724659\n",
      "Epoch 23486/30000 Training Loss: 0.036136701703071594\n",
      "Epoch 23487/30000 Training Loss: 0.04938539117574692\n",
      "Epoch 23488/30000 Training Loss: 0.038876138627529144\n",
      "Epoch 23489/30000 Training Loss: 0.04726499319076538\n",
      "Epoch 23490/30000 Training Loss: 0.04496399313211441\n",
      "Epoch 23491/30000 Training Loss: 0.05186886712908745\n",
      "Epoch 23492/30000 Training Loss: 0.04556933045387268\n",
      "Epoch 23493/30000 Training Loss: 0.04867533966898918\n",
      "Epoch 23494/30000 Training Loss: 0.037980977445840836\n",
      "Epoch 23495/30000 Training Loss: 0.03805523365736008\n",
      "Epoch 23496/30000 Training Loss: 0.03321119770407677\n",
      "Epoch 23497/30000 Training Loss: 0.054521262645721436\n",
      "Epoch 23498/30000 Training Loss: 0.04162200167775154\n",
      "Epoch 23499/30000 Training Loss: 0.04698209464550018\n",
      "Epoch 23500/30000 Training Loss: 0.04278867319226265\n",
      "Epoch 23500/30000 Validation Loss: 0.06432532519102097\n",
      "Epoch 23501/30000 Training Loss: 0.03380456939339638\n",
      "Epoch 23502/30000 Training Loss: 0.04751328378915787\n",
      "Epoch 23503/30000 Training Loss: 0.039303816854953766\n",
      "Epoch 23504/30000 Training Loss: 0.05411118268966675\n",
      "Epoch 23505/30000 Training Loss: 0.04631233215332031\n",
      "Epoch 23506/30000 Training Loss: 0.056434836238622665\n",
      "Epoch 23507/30000 Training Loss: 0.046199820935726166\n",
      "Epoch 23508/30000 Training Loss: 0.0429353304207325\n",
      "Epoch 23509/30000 Training Loss: 0.03689047694206238\n",
      "Epoch 23510/30000 Training Loss: 0.0518062487244606\n",
      "Epoch 23511/30000 Training Loss: 0.05049874633550644\n",
      "Epoch 23512/30000 Training Loss: 0.04020502045750618\n",
      "Epoch 23513/30000 Training Loss: 0.04379088059067726\n",
      "Epoch 23514/30000 Training Loss: 0.03587920218706131\n",
      "Epoch 23515/30000 Training Loss: 0.06147052347660065\n",
      "Epoch 23516/30000 Training Loss: 0.04178246110677719\n",
      "Epoch 23517/30000 Training Loss: 0.033768944442272186\n",
      "Epoch 23518/30000 Training Loss: 0.0424424484372139\n",
      "Epoch 23519/30000 Training Loss: 0.04931415617465973\n",
      "Epoch 23520/30000 Training Loss: 0.05038230121135712\n",
      "Epoch 23521/30000 Training Loss: 0.046790167689323425\n",
      "Epoch 23522/30000 Training Loss: 0.041190728545188904\n",
      "Epoch 23523/30000 Training Loss: 0.03342634066939354\n",
      "Epoch 23524/30000 Training Loss: 0.03772832453250885\n",
      "Epoch 23525/30000 Training Loss: 0.04201201722025871\n",
      "Epoch 23526/30000 Training Loss: 0.03690173849463463\n",
      "Epoch 23527/30000 Training Loss: 0.06512147188186646\n",
      "Epoch 23528/30000 Training Loss: 0.03586257994174957\n",
      "Epoch 23529/30000 Training Loss: 0.04575860872864723\n",
      "Epoch 23530/30000 Training Loss: 0.04380689188838005\n",
      "Epoch 23531/30000 Training Loss: 0.04225853458046913\n",
      "Epoch 23532/30000 Training Loss: 0.04916765168309212\n",
      "Epoch 23533/30000 Training Loss: 0.039756469428539276\n",
      "Epoch 23534/30000 Training Loss: 0.04012569785118103\n",
      "Epoch 23535/30000 Training Loss: 0.04017368704080582\n",
      "Epoch 23536/30000 Training Loss: 0.03967241197824478\n",
      "Epoch 23537/30000 Training Loss: 0.06189097464084625\n",
      "Epoch 23538/30000 Training Loss: 0.05048125982284546\n",
      "Epoch 23539/30000 Training Loss: 0.05206320062279701\n",
      "Epoch 23540/30000 Training Loss: 0.05887223035097122\n",
      "Epoch 23541/30000 Training Loss: 0.05863342434167862\n",
      "Epoch 23542/30000 Training Loss: 0.04859035089612007\n",
      "Epoch 23543/30000 Training Loss: 0.06054540351033211\n",
      "Epoch 23544/30000 Training Loss: 0.04820362851023674\n",
      "Epoch 23545/30000 Training Loss: 0.04118728265166283\n",
      "Epoch 23546/30000 Training Loss: 0.04179473593831062\n",
      "Epoch 23547/30000 Training Loss: 0.04469538480043411\n",
      "Epoch 23548/30000 Training Loss: 0.03653639927506447\n",
      "Epoch 23549/30000 Training Loss: 0.048928070813417435\n",
      "Epoch 23550/30000 Training Loss: 0.04250817745923996\n",
      "Epoch 23551/30000 Training Loss: 0.038939785212278366\n",
      "Epoch 23552/30000 Training Loss: 0.03913022577762604\n",
      "Epoch 23553/30000 Training Loss: 0.040434580296278\n",
      "Epoch 23554/30000 Training Loss: 0.04783220216631889\n",
      "Epoch 23555/30000 Training Loss: 0.05680687725543976\n",
      "Epoch 23556/30000 Training Loss: 0.048038873821496964\n",
      "Epoch 23557/30000 Training Loss: 0.035795580595731735\n",
      "Epoch 23558/30000 Training Loss: 0.0398315005004406\n",
      "Epoch 23559/30000 Training Loss: 0.0315796434879303\n",
      "Epoch 23560/30000 Training Loss: 0.037928465753793716\n",
      "Epoch 23561/30000 Training Loss: 0.035283979028463364\n",
      "Epoch 23562/30000 Training Loss: 0.04302787780761719\n",
      "Epoch 23563/30000 Training Loss: 0.05279181897640228\n",
      "Epoch 23564/30000 Training Loss: 0.02952364645898342\n",
      "Epoch 23565/30000 Training Loss: 0.043222956359386444\n",
      "Epoch 23566/30000 Training Loss: 0.0313546359539032\n",
      "Epoch 23567/30000 Training Loss: 0.046064067631959915\n",
      "Epoch 23568/30000 Training Loss: 0.03911248594522476\n",
      "Epoch 23569/30000 Training Loss: 0.04345481097698212\n",
      "Epoch 23570/30000 Training Loss: 0.04353602975606918\n",
      "Epoch 23571/30000 Training Loss: 0.05542013794183731\n",
      "Epoch 23572/30000 Training Loss: 0.05257274955511093\n",
      "Epoch 23573/30000 Training Loss: 0.04641854017972946\n",
      "Epoch 23574/30000 Training Loss: 0.05166242644190788\n",
      "Epoch 23575/30000 Training Loss: 0.04468564689159393\n",
      "Epoch 23576/30000 Training Loss: 0.05353555828332901\n",
      "Epoch 23577/30000 Training Loss: 0.03572309762239456\n",
      "Epoch 23578/30000 Training Loss: 0.05592060834169388\n",
      "Epoch 23579/30000 Training Loss: 0.03583444282412529\n",
      "Epoch 23580/30000 Training Loss: 0.047809895128011703\n",
      "Epoch 23581/30000 Training Loss: 0.04336592182517052\n",
      "Epoch 23582/30000 Training Loss: 0.03451560437679291\n",
      "Epoch 23583/30000 Training Loss: 0.059594590216875076\n",
      "Epoch 23584/30000 Training Loss: 0.043621379882097244\n",
      "Epoch 23585/30000 Training Loss: 0.05746138095855713\n",
      "Epoch 23586/30000 Training Loss: 0.049904800951480865\n",
      "Epoch 23587/30000 Training Loss: 0.042191505432128906\n",
      "Epoch 23588/30000 Training Loss: 0.04000229761004448\n",
      "Epoch 23589/30000 Training Loss: 0.04213149473071098\n",
      "Epoch 23590/30000 Training Loss: 0.02967032976448536\n",
      "Epoch 23591/30000 Training Loss: 0.05434298887848854\n",
      "Epoch 23592/30000 Training Loss: 0.042345061898231506\n",
      "Epoch 23593/30000 Training Loss: 0.046263232827186584\n",
      "Epoch 23594/30000 Training Loss: 0.04260501265525818\n",
      "Epoch 23595/30000 Training Loss: 0.08579493314027786\n",
      "Epoch 23596/30000 Training Loss: 0.056853923946619034\n",
      "Epoch 23597/30000 Training Loss: 0.04739116132259369\n",
      "Epoch 23598/30000 Training Loss: 0.040887631475925446\n",
      "Epoch 23599/30000 Training Loss: 0.05914287641644478\n",
      "Epoch 23600/30000 Training Loss: 0.05284123122692108\n",
      "Epoch 23600/30000 Validation Loss: 0.039946548640728\n",
      "Epoch 23601/30000 Training Loss: 0.05798358842730522\n",
      "Epoch 23602/30000 Training Loss: 0.047235552221536636\n",
      "Epoch 23603/30000 Training Loss: 0.06266104429960251\n",
      "Epoch 23604/30000 Training Loss: 0.04783547669649124\n",
      "Epoch 23605/30000 Training Loss: 0.03736168518662453\n",
      "Epoch 23606/30000 Training Loss: 0.038726843893527985\n",
      "Epoch 23607/30000 Training Loss: 0.06354759633541107\n",
      "Epoch 23608/30000 Training Loss: 0.06313791871070862\n",
      "Epoch 23609/30000 Training Loss: 0.045660290867090225\n",
      "Epoch 23610/30000 Training Loss: 0.03419407457113266\n",
      "Epoch 23611/30000 Training Loss: 0.06609220802783966\n",
      "Epoch 23612/30000 Training Loss: 0.061493344604969025\n",
      "Epoch 23613/30000 Training Loss: 0.054019540548324585\n",
      "Epoch 23614/30000 Training Loss: 0.05054479092359543\n",
      "Epoch 23615/30000 Training Loss: 0.04700740799307823\n",
      "Epoch 23616/30000 Training Loss: 0.036201898008584976\n",
      "Epoch 23617/30000 Training Loss: 0.03209536150097847\n",
      "Epoch 23618/30000 Training Loss: 0.04215843975543976\n",
      "Epoch 23619/30000 Training Loss: 0.04857587441802025\n",
      "Epoch 23620/30000 Training Loss: 0.04953964799642563\n",
      "Epoch 23621/30000 Training Loss: 0.03904024884104729\n",
      "Epoch 23622/30000 Training Loss: 0.051381491124629974\n",
      "Epoch 23623/30000 Training Loss: 0.036978576332330704\n",
      "Epoch 23624/30000 Training Loss: 0.04669768363237381\n",
      "Epoch 23625/30000 Training Loss: 0.0479327030479908\n",
      "Epoch 23626/30000 Training Loss: 0.04505707323551178\n",
      "Epoch 23627/30000 Training Loss: 0.04880449175834656\n",
      "Epoch 23628/30000 Training Loss: 0.07085728645324707\n",
      "Epoch 23629/30000 Training Loss: 0.05518591031432152\n",
      "Epoch 23630/30000 Training Loss: 0.05162687599658966\n",
      "Epoch 23631/30000 Training Loss: 0.03808293864130974\n",
      "Epoch 23632/30000 Training Loss: 0.038558073341846466\n",
      "Epoch 23633/30000 Training Loss: 0.047813285142183304\n",
      "Epoch 23634/30000 Training Loss: 0.04601115360856056\n",
      "Epoch 23635/30000 Training Loss: 0.04260602593421936\n",
      "Epoch 23636/30000 Training Loss: 0.05055772513151169\n",
      "Epoch 23637/30000 Training Loss: 0.045147281140089035\n",
      "Epoch 23638/30000 Training Loss: 0.04415089264512062\n",
      "Epoch 23639/30000 Training Loss: 0.03659407049417496\n",
      "Epoch 23640/30000 Training Loss: 0.04495220631361008\n",
      "Epoch 23641/30000 Training Loss: 0.049745216965675354\n",
      "Epoch 23642/30000 Training Loss: 0.030763983726501465\n",
      "Epoch 23643/30000 Training Loss: 0.03818042576313019\n",
      "Epoch 23644/30000 Training Loss: 0.041416823863983154\n",
      "Epoch 23645/30000 Training Loss: 0.038565754890441895\n",
      "Epoch 23646/30000 Training Loss: 0.06042070314288139\n",
      "Epoch 23647/30000 Training Loss: 0.04499528184533119\n",
      "Epoch 23648/30000 Training Loss: 0.04781576246023178\n",
      "Epoch 23649/30000 Training Loss: 0.0444822758436203\n",
      "Epoch 23650/30000 Training Loss: 0.042515017092227936\n",
      "Epoch 23651/30000 Training Loss: 0.047587014734745026\n",
      "Epoch 23652/30000 Training Loss: 0.0648626983165741\n",
      "Epoch 23653/30000 Training Loss: 0.054065167903900146\n",
      "Epoch 23654/30000 Training Loss: 0.05380736663937569\n",
      "Epoch 23655/30000 Training Loss: 0.03930865228176117\n",
      "Epoch 23656/30000 Training Loss: 0.03961728885769844\n",
      "Epoch 23657/30000 Training Loss: 0.032948657870292664\n",
      "Epoch 23658/30000 Training Loss: 0.06957981735467911\n",
      "Epoch 23659/30000 Training Loss: 0.05275348573923111\n",
      "Epoch 23660/30000 Training Loss: 0.046774011105298996\n",
      "Epoch 23661/30000 Training Loss: 0.04651632532477379\n",
      "Epoch 23662/30000 Training Loss: 0.040734175592660904\n",
      "Epoch 23663/30000 Training Loss: 0.060637619346380234\n",
      "Epoch 23664/30000 Training Loss: 0.04967502877116203\n",
      "Epoch 23665/30000 Training Loss: 0.04306774213910103\n",
      "Epoch 23666/30000 Training Loss: 0.04468172416090965\n",
      "Epoch 23667/30000 Training Loss: 0.036312367767095566\n",
      "Epoch 23668/30000 Training Loss: 0.05842126905918121\n",
      "Epoch 23669/30000 Training Loss: 0.042855411767959595\n",
      "Epoch 23670/30000 Training Loss: 0.04369550198316574\n",
      "Epoch 23671/30000 Training Loss: 0.04391703009605408\n",
      "Epoch 23672/30000 Training Loss: 0.056362975388765335\n",
      "Epoch 23673/30000 Training Loss: 0.03921160101890564\n",
      "Epoch 23674/30000 Training Loss: 0.04954720288515091\n",
      "Epoch 23675/30000 Training Loss: 0.047245267778635025\n",
      "Epoch 23676/30000 Training Loss: 0.04362289980053902\n",
      "Epoch 23677/30000 Training Loss: 0.046659618616104126\n",
      "Epoch 23678/30000 Training Loss: 0.05447394400835037\n",
      "Epoch 23679/30000 Training Loss: 0.034490037709474564\n",
      "Epoch 23680/30000 Training Loss: 0.048989441245794296\n",
      "Epoch 23681/30000 Training Loss: 0.040651626884937286\n",
      "Epoch 23682/30000 Training Loss: 0.038523003458976746\n",
      "Epoch 23683/30000 Training Loss: 0.05843871831893921\n",
      "Epoch 23684/30000 Training Loss: 0.044566117227077484\n",
      "Epoch 23685/30000 Training Loss: 0.04738960415124893\n",
      "Epoch 23686/30000 Training Loss: 0.053114037960767746\n",
      "Epoch 23687/30000 Training Loss: 0.04779667407274246\n",
      "Epoch 23688/30000 Training Loss: 0.04478127881884575\n",
      "Epoch 23689/30000 Training Loss: 0.04471442103385925\n",
      "Epoch 23690/30000 Training Loss: 0.032167963683605194\n",
      "Epoch 23691/30000 Training Loss: 0.04227215424180031\n",
      "Epoch 23692/30000 Training Loss: 0.04221665859222412\n",
      "Epoch 23693/30000 Training Loss: 0.051031578332185745\n",
      "Epoch 23694/30000 Training Loss: 0.04489357769489288\n",
      "Epoch 23695/30000 Training Loss: 0.06585901230573654\n",
      "Epoch 23696/30000 Training Loss: 0.05234994739294052\n",
      "Epoch 23697/30000 Training Loss: 0.0619078166782856\n",
      "Epoch 23698/30000 Training Loss: 0.03284979984164238\n",
      "Epoch 23699/30000 Training Loss: 0.03430793434381485\n",
      "Epoch 23700/30000 Training Loss: 0.04906405508518219\n",
      "Epoch 23700/30000 Validation Loss: 0.0503079891204834\n",
      "Epoch 23701/30000 Training Loss: 0.04530069977045059\n",
      "Epoch 23702/30000 Training Loss: 0.044826291501522064\n",
      "Epoch 23703/30000 Training Loss: 0.057162657380104065\n",
      "Epoch 23704/30000 Training Loss: 0.053232163190841675\n",
      "Epoch 23705/30000 Training Loss: 0.04822313413023949\n",
      "Epoch 23706/30000 Training Loss: 0.04887814819812775\n",
      "Epoch 23707/30000 Training Loss: 0.04108753055334091\n",
      "Epoch 23708/30000 Training Loss: 0.05062944069504738\n",
      "Epoch 23709/30000 Training Loss: 0.05481244623661041\n",
      "Epoch 23710/30000 Training Loss: 0.034544702619314194\n",
      "Epoch 23711/30000 Training Loss: 0.04746299237012863\n",
      "Epoch 23712/30000 Training Loss: 0.05390346795320511\n",
      "Epoch 23713/30000 Training Loss: 0.045030247420072556\n",
      "Epoch 23714/30000 Training Loss: 0.0433291420340538\n",
      "Epoch 23715/30000 Training Loss: 0.0499328151345253\n",
      "Epoch 23716/30000 Training Loss: 0.025007836520671844\n",
      "Epoch 23717/30000 Training Loss: 0.06850336492061615\n",
      "Epoch 23718/30000 Training Loss: 0.044555578380823135\n",
      "Epoch 23719/30000 Training Loss: 0.04365340247750282\n",
      "Epoch 23720/30000 Training Loss: 0.06405919790267944\n",
      "Epoch 23721/30000 Training Loss: 0.031122330576181412\n",
      "Epoch 23722/30000 Training Loss: 0.038820985704660416\n",
      "Epoch 23723/30000 Training Loss: 0.0553257018327713\n",
      "Epoch 23724/30000 Training Loss: 0.046150706708431244\n",
      "Epoch 23725/30000 Training Loss: 0.05047648027539253\n",
      "Epoch 23726/30000 Training Loss: 0.04836282134056091\n",
      "Epoch 23727/30000 Training Loss: 0.04400727525353432\n",
      "Epoch 23728/30000 Training Loss: 0.05891702324151993\n",
      "Epoch 23729/30000 Training Loss: 0.03638489544391632\n",
      "Epoch 23730/30000 Training Loss: 0.05184269696474075\n",
      "Epoch 23731/30000 Training Loss: 0.044619616121053696\n",
      "Epoch 23732/30000 Training Loss: 0.04154684394598007\n",
      "Epoch 23733/30000 Training Loss: 0.05348148196935654\n",
      "Epoch 23734/30000 Training Loss: 0.04463604465126991\n",
      "Epoch 23735/30000 Training Loss: 0.053125761449337006\n",
      "Epoch 23736/30000 Training Loss: 0.054583653807640076\n",
      "Epoch 23737/30000 Training Loss: 0.04697677493095398\n",
      "Epoch 23738/30000 Training Loss: 0.04277054965496063\n",
      "Epoch 23739/30000 Training Loss: 0.04602158069610596\n",
      "Epoch 23740/30000 Training Loss: 0.05137433111667633\n",
      "Epoch 23741/30000 Training Loss: 0.053165312856435776\n",
      "Epoch 23742/30000 Training Loss: 0.05226618051528931\n",
      "Epoch 23743/30000 Training Loss: 0.054568033665418625\n",
      "Epoch 23744/30000 Training Loss: 0.040244728326797485\n",
      "Epoch 23745/30000 Training Loss: 0.04597146436572075\n",
      "Epoch 23746/30000 Training Loss: 0.03556419909000397\n",
      "Epoch 23747/30000 Training Loss: 0.04974069818854332\n",
      "Epoch 23748/30000 Training Loss: 0.03865239769220352\n",
      "Epoch 23749/30000 Training Loss: 0.05779924988746643\n",
      "Epoch 23750/30000 Training Loss: 0.05177395045757294\n",
      "Epoch 23751/30000 Training Loss: 0.05365150794386864\n",
      "Epoch 23752/30000 Training Loss: 0.059547796845436096\n",
      "Epoch 23753/30000 Training Loss: 0.03844669833779335\n",
      "Epoch 23754/30000 Training Loss: 0.06363039463758469\n",
      "Epoch 23755/30000 Training Loss: 0.06107383221387863\n",
      "Epoch 23756/30000 Training Loss: 0.05567062646150589\n",
      "Epoch 23757/30000 Training Loss: 0.04790656268596649\n",
      "Epoch 23758/30000 Training Loss: 0.040156103670597076\n",
      "Epoch 23759/30000 Training Loss: 0.046584878116846085\n",
      "Epoch 23760/30000 Training Loss: 0.041514839977025986\n",
      "Epoch 23761/30000 Training Loss: 0.04550616070628166\n",
      "Epoch 23762/30000 Training Loss: 0.03373567387461662\n",
      "Epoch 23763/30000 Training Loss: 0.050808582454919815\n",
      "Epoch 23764/30000 Training Loss: 0.03501202538609505\n",
      "Epoch 23765/30000 Training Loss: 0.04599151015281677\n",
      "Epoch 23766/30000 Training Loss: 0.041213154792785645\n",
      "Epoch 23767/30000 Training Loss: 0.06449722498655319\n",
      "Epoch 23768/30000 Training Loss: 0.05148450285196304\n",
      "Epoch 23769/30000 Training Loss: 0.037061527371406555\n",
      "Epoch 23770/30000 Training Loss: 0.04836598038673401\n",
      "Epoch 23771/30000 Training Loss: 0.04784952849149704\n",
      "Epoch 23772/30000 Training Loss: 0.04783914238214493\n",
      "Epoch 23773/30000 Training Loss: 0.06739193946123123\n",
      "Epoch 23774/30000 Training Loss: 0.037877198308706284\n",
      "Epoch 23775/30000 Training Loss: 0.05167212337255478\n",
      "Epoch 23776/30000 Training Loss: 0.0443233996629715\n",
      "Epoch 23777/30000 Training Loss: 0.04970620572566986\n",
      "Epoch 23778/30000 Training Loss: 0.05566734820604324\n",
      "Epoch 23779/30000 Training Loss: 0.042950961738824844\n",
      "Epoch 23780/30000 Training Loss: 0.0408596396446228\n",
      "Epoch 23781/30000 Training Loss: 0.05053497850894928\n",
      "Epoch 23782/30000 Training Loss: 0.0299077145755291\n",
      "Epoch 23783/30000 Training Loss: 0.04586884379386902\n",
      "Epoch 23784/30000 Training Loss: 0.05992398411035538\n",
      "Epoch 23785/30000 Training Loss: 0.03419343754649162\n",
      "Epoch 23786/30000 Training Loss: 0.0541672483086586\n",
      "Epoch 23787/30000 Training Loss: 0.05578932911157608\n",
      "Epoch 23788/30000 Training Loss: 0.05325207859277725\n",
      "Epoch 23789/30000 Training Loss: 0.04562178626656532\n",
      "Epoch 23790/30000 Training Loss: 0.04344731196761131\n",
      "Epoch 23791/30000 Training Loss: 0.040855906903743744\n",
      "Epoch 23792/30000 Training Loss: 0.04692392796278\n",
      "Epoch 23793/30000 Training Loss: 0.029153812676668167\n",
      "Epoch 23794/30000 Training Loss: 0.05520248785614967\n",
      "Epoch 23795/30000 Training Loss: 0.05335656553506851\n",
      "Epoch 23796/30000 Training Loss: 0.04016052931547165\n",
      "Epoch 23797/30000 Training Loss: 0.04811597988009453\n",
      "Epoch 23798/30000 Training Loss: 0.04133225604891777\n",
      "Epoch 23799/30000 Training Loss: 0.043576620519161224\n",
      "Epoch 23800/30000 Training Loss: 0.03652198612689972\n",
      "Epoch 23800/30000 Validation Loss: 0.03671220317482948\n",
      "Epoch 23801/30000 Training Loss: 0.05487452447414398\n",
      "Epoch 23802/30000 Training Loss: 0.06053689867258072\n",
      "Epoch 23803/30000 Training Loss: 0.0559421107172966\n",
      "Epoch 23804/30000 Training Loss: 0.04563378915190697\n",
      "Epoch 23805/30000 Training Loss: 0.055176980793476105\n",
      "Epoch 23806/30000 Training Loss: 0.04129137098789215\n",
      "Epoch 23807/30000 Training Loss: 0.04180772975087166\n",
      "Epoch 23808/30000 Training Loss: 0.05709400773048401\n",
      "Epoch 23809/30000 Training Loss: 0.04345829412341118\n",
      "Epoch 23810/30000 Training Loss: 0.046556469053030014\n",
      "Epoch 23811/30000 Training Loss: 0.036445412784814835\n",
      "Epoch 23812/30000 Training Loss: 0.04189528897404671\n",
      "Epoch 23813/30000 Training Loss: 0.042073652148246765\n",
      "Epoch 23814/30000 Training Loss: 0.0670437216758728\n",
      "Epoch 23815/30000 Training Loss: 0.047048114240169525\n",
      "Epoch 23816/30000 Training Loss: 0.06332924216985703\n",
      "Epoch 23817/30000 Training Loss: 0.05463187396526337\n",
      "Epoch 23818/30000 Training Loss: 0.055315740406513214\n",
      "Epoch 23819/30000 Training Loss: 0.05529322102665901\n",
      "Epoch 23820/30000 Training Loss: 0.053773149847984314\n",
      "Epoch 23821/30000 Training Loss: 0.04660020023584366\n",
      "Epoch 23822/30000 Training Loss: 0.05807872116565704\n",
      "Epoch 23823/30000 Training Loss: 0.046764105558395386\n",
      "Epoch 23824/30000 Training Loss: 0.05515816807746887\n",
      "Epoch 23825/30000 Training Loss: 0.056954942643642426\n",
      "Epoch 23826/30000 Training Loss: 0.06412958353757858\n",
      "Epoch 23827/30000 Training Loss: 0.03337601199746132\n",
      "Epoch 23828/30000 Training Loss: 0.045478738844394684\n",
      "Epoch 23829/30000 Training Loss: 0.04878775030374527\n",
      "Epoch 23830/30000 Training Loss: 0.04031098634004593\n",
      "Epoch 23831/30000 Training Loss: 0.026933513581752777\n",
      "Epoch 23832/30000 Training Loss: 0.05725548416376114\n",
      "Epoch 23833/30000 Training Loss: 0.04565008729696274\n",
      "Epoch 23834/30000 Training Loss: 0.054420262575149536\n",
      "Epoch 23835/30000 Training Loss: 0.035577528178691864\n",
      "Epoch 23836/30000 Training Loss: 0.037548307329416275\n",
      "Epoch 23837/30000 Training Loss: 0.0428125262260437\n",
      "Epoch 23838/30000 Training Loss: 0.0396292619407177\n",
      "Epoch 23839/30000 Training Loss: 0.037458837032318115\n",
      "Epoch 23840/30000 Training Loss: 0.056061577051877975\n",
      "Epoch 23841/30000 Training Loss: 0.04200221970677376\n",
      "Epoch 23842/30000 Training Loss: 0.04380326718091965\n",
      "Epoch 23843/30000 Training Loss: 0.04605269804596901\n",
      "Epoch 23844/30000 Training Loss: 0.04691160097718239\n",
      "Epoch 23845/30000 Training Loss: 0.06285560876131058\n",
      "Epoch 23846/30000 Training Loss: 0.04748699441552162\n",
      "Epoch 23847/30000 Training Loss: 0.058271341025829315\n",
      "Epoch 23848/30000 Training Loss: 0.04321157559752464\n",
      "Epoch 23849/30000 Training Loss: 0.03919617086648941\n",
      "Epoch 23850/30000 Training Loss: 0.046901825815439224\n",
      "Epoch 23851/30000 Training Loss: 0.040142741054296494\n",
      "Epoch 23852/30000 Training Loss: 0.03982788696885109\n",
      "Epoch 23853/30000 Training Loss: 0.04887472838163376\n",
      "Epoch 23854/30000 Training Loss: 0.04156727343797684\n",
      "Epoch 23855/30000 Training Loss: 0.05414044111967087\n",
      "Epoch 23856/30000 Training Loss: 0.04405837133526802\n",
      "Epoch 23857/30000 Training Loss: 0.03924555331468582\n",
      "Epoch 23858/30000 Training Loss: 0.058080386370420456\n",
      "Epoch 23859/30000 Training Loss: 0.05288819223642349\n",
      "Epoch 23860/30000 Training Loss: 0.0529276505112648\n",
      "Epoch 23861/30000 Training Loss: 0.04601041227579117\n",
      "Epoch 23862/30000 Training Loss: 0.0389210507273674\n",
      "Epoch 23863/30000 Training Loss: 0.04022407531738281\n",
      "Epoch 23864/30000 Training Loss: 0.03493007272481918\n",
      "Epoch 23865/30000 Training Loss: 0.03400387987494469\n",
      "Epoch 23866/30000 Training Loss: 0.06128302961587906\n",
      "Epoch 23867/30000 Training Loss: 0.0473567359149456\n",
      "Epoch 23868/30000 Training Loss: 0.051354773342609406\n",
      "Epoch 23869/30000 Training Loss: 0.03746579959988594\n",
      "Epoch 23870/30000 Training Loss: 0.051006413996219635\n",
      "Epoch 23871/30000 Training Loss: 0.03855723887681961\n",
      "Epoch 23872/30000 Training Loss: 0.06108531355857849\n",
      "Epoch 23873/30000 Training Loss: 0.04101046547293663\n",
      "Epoch 23874/30000 Training Loss: 0.041875842958688736\n",
      "Epoch 23875/30000 Training Loss: 0.041113950312137604\n",
      "Epoch 23876/30000 Training Loss: 0.035329192876815796\n",
      "Epoch 23877/30000 Training Loss: 0.04122232273221016\n",
      "Epoch 23878/30000 Training Loss: 0.03660021349787712\n",
      "Epoch 23879/30000 Training Loss: 0.04934152588248253\n",
      "Epoch 23880/30000 Training Loss: 0.05287089943885803\n",
      "Epoch 23881/30000 Training Loss: 0.042826518416404724\n",
      "Epoch 23882/30000 Training Loss: 0.038760580122470856\n",
      "Epoch 23883/30000 Training Loss: 0.05603502690792084\n",
      "Epoch 23884/30000 Training Loss: 0.047793783247470856\n",
      "Epoch 23885/30000 Training Loss: 0.038624972105026245\n",
      "Epoch 23886/30000 Training Loss: 0.037721604108810425\n",
      "Epoch 23887/30000 Training Loss: 0.06817730516195297\n",
      "Epoch 23888/30000 Training Loss: 0.049238961189985275\n",
      "Epoch 23889/30000 Training Loss: 0.059634730219841\n",
      "Epoch 23890/30000 Training Loss: 0.05614887923002243\n",
      "Epoch 23891/30000 Training Loss: 0.03578054904937744\n",
      "Epoch 23892/30000 Training Loss: 0.043398432433605194\n",
      "Epoch 23893/30000 Training Loss: 0.03393453359603882\n",
      "Epoch 23894/30000 Training Loss: 0.0576649084687233\n",
      "Epoch 23895/30000 Training Loss: 0.03971020132303238\n",
      "Epoch 23896/30000 Training Loss: 0.04858963564038277\n",
      "Epoch 23897/30000 Training Loss: 0.05546919256448746\n",
      "Epoch 23898/30000 Training Loss: 0.04956599324941635\n",
      "Epoch 23899/30000 Training Loss: 0.04523026943206787\n",
      "Epoch 23900/30000 Training Loss: 0.04055745154619217\n",
      "Epoch 23900/30000 Validation Loss: 0.03880571946501732\n",
      "Epoch 23901/30000 Training Loss: 0.050900302827358246\n",
      "Epoch 23902/30000 Training Loss: 0.04194171726703644\n",
      "Epoch 23903/30000 Training Loss: 0.05063492804765701\n",
      "Epoch 23904/30000 Training Loss: 0.05505776405334473\n",
      "Epoch 23905/30000 Training Loss: 0.04433855414390564\n",
      "Epoch 23906/30000 Training Loss: 0.048869844526052475\n",
      "Epoch 23907/30000 Training Loss: 0.05014222115278244\n",
      "Epoch 23908/30000 Training Loss: 0.03887956961989403\n",
      "Epoch 23909/30000 Training Loss: 0.05653539299964905\n",
      "Epoch 23910/30000 Training Loss: 0.047781702131032944\n",
      "Epoch 23911/30000 Training Loss: 0.04747070372104645\n",
      "Epoch 23912/30000 Training Loss: 0.046260155737400055\n",
      "Epoch 23913/30000 Training Loss: 0.056339241564273834\n",
      "Epoch 23914/30000 Training Loss: 0.05516441538929939\n",
      "Epoch 23915/30000 Training Loss: 0.03476536273956299\n",
      "Epoch 23916/30000 Training Loss: 0.031329795718193054\n",
      "Epoch 23917/30000 Training Loss: 0.04966043308377266\n",
      "Epoch 23918/30000 Training Loss: 0.046619538217782974\n",
      "Epoch 23919/30000 Training Loss: 0.03198510408401489\n",
      "Epoch 23920/30000 Training Loss: 0.06526663899421692\n",
      "Epoch 23921/30000 Training Loss: 0.06626925617456436\n",
      "Epoch 23922/30000 Training Loss: 0.05436655879020691\n",
      "Epoch 23923/30000 Training Loss: 0.03759608790278435\n",
      "Epoch 23924/30000 Training Loss: 0.053927332162857056\n",
      "Epoch 23925/30000 Training Loss: 0.05192561447620392\n",
      "Epoch 23926/30000 Training Loss: 0.06750200688838959\n",
      "Epoch 23927/30000 Training Loss: 0.052374839782714844\n",
      "Epoch 23928/30000 Training Loss: 0.03583691269159317\n",
      "Epoch 23929/30000 Training Loss: 0.04482431709766388\n",
      "Epoch 23930/30000 Training Loss: 0.04638081043958664\n",
      "Epoch 23931/30000 Training Loss: 0.038812730461359024\n",
      "Epoch 23932/30000 Training Loss: 0.04263255000114441\n",
      "Epoch 23933/30000 Training Loss: 0.03434448689222336\n",
      "Epoch 23934/30000 Training Loss: 0.05237572640180588\n",
      "Epoch 23935/30000 Training Loss: 0.04810967296361923\n",
      "Epoch 23936/30000 Training Loss: 0.050867121666669846\n",
      "Epoch 23937/30000 Training Loss: 0.04059445858001709\n",
      "Epoch 23938/30000 Training Loss: 0.04712891951203346\n",
      "Epoch 23939/30000 Training Loss: 0.058612383902072906\n",
      "Epoch 23940/30000 Training Loss: 0.03970499336719513\n",
      "Epoch 23941/30000 Training Loss: 0.034079667180776596\n",
      "Epoch 23942/30000 Training Loss: 0.048367246985435486\n",
      "Epoch 23943/30000 Training Loss: 0.04716884717345238\n",
      "Epoch 23944/30000 Training Loss: 0.04921245202422142\n",
      "Epoch 23945/30000 Training Loss: 0.04155722260475159\n",
      "Epoch 23946/30000 Training Loss: 0.0463021956384182\n",
      "Epoch 23947/30000 Training Loss: 0.06016775220632553\n",
      "Epoch 23948/30000 Training Loss: 0.04462587088346481\n",
      "Epoch 23949/30000 Training Loss: 0.04655349627137184\n",
      "Epoch 23950/30000 Training Loss: 0.033728498965501785\n",
      "Epoch 23951/30000 Training Loss: 0.04512324184179306\n",
      "Epoch 23952/30000 Training Loss: 0.05340834707021713\n",
      "Epoch 23953/30000 Training Loss: 0.052072592079639435\n",
      "Epoch 23954/30000 Training Loss: 0.039960939437150955\n",
      "Epoch 23955/30000 Training Loss: 0.04872230440378189\n",
      "Epoch 23956/30000 Training Loss: 0.061393316835165024\n",
      "Epoch 23957/30000 Training Loss: 0.043096691370010376\n",
      "Epoch 23958/30000 Training Loss: 0.04324996843934059\n",
      "Epoch 23959/30000 Training Loss: 0.04116366431117058\n",
      "Epoch 23960/30000 Training Loss: 0.04817302152514458\n",
      "Epoch 23961/30000 Training Loss: 0.04675482213497162\n",
      "Epoch 23962/30000 Training Loss: 0.04868174344301224\n",
      "Epoch 23963/30000 Training Loss: 0.054501671344041824\n",
      "Epoch 23964/30000 Training Loss: 0.04559959098696709\n",
      "Epoch 23965/30000 Training Loss: 0.03705170750617981\n",
      "Epoch 23966/30000 Training Loss: 0.04381374269723892\n",
      "Epoch 23967/30000 Training Loss: 0.042428504675626755\n",
      "Epoch 23968/30000 Training Loss: 0.03808661177754402\n",
      "Epoch 23969/30000 Training Loss: 0.03827740252017975\n",
      "Epoch 23970/30000 Training Loss: 0.04079136252403259\n",
      "Epoch 23971/30000 Training Loss: 0.03553472086787224\n",
      "Epoch 23972/30000 Training Loss: 0.044955119490623474\n",
      "Epoch 23973/30000 Training Loss: 0.04525631666183472\n",
      "Epoch 23974/30000 Training Loss: 0.04834981635212898\n",
      "Epoch 23975/30000 Training Loss: 0.05144156888127327\n",
      "Epoch 23976/30000 Training Loss: 0.03787222132086754\n",
      "Epoch 23977/30000 Training Loss: 0.042694974690675735\n",
      "Epoch 23978/30000 Training Loss: 0.030845705419778824\n",
      "Epoch 23979/30000 Training Loss: 0.03587103262543678\n",
      "Epoch 23980/30000 Training Loss: 0.05520031228661537\n",
      "Epoch 23981/30000 Training Loss: 0.053181134164333344\n",
      "Epoch 23982/30000 Training Loss: 0.06511807441711426\n",
      "Epoch 23983/30000 Training Loss: 0.036448728293180466\n",
      "Epoch 23984/30000 Training Loss: 0.04426439106464386\n",
      "Epoch 23985/30000 Training Loss: 0.03788480535149574\n",
      "Epoch 23986/30000 Training Loss: 0.053664665669202805\n",
      "Epoch 23987/30000 Training Loss: 0.039522260427474976\n",
      "Epoch 23988/30000 Training Loss: 0.05069282650947571\n",
      "Epoch 23989/30000 Training Loss: 0.03742073103785515\n",
      "Epoch 23990/30000 Training Loss: 0.05512073636054993\n",
      "Epoch 23991/30000 Training Loss: 0.042577505111694336\n",
      "Epoch 23992/30000 Training Loss: 0.03543867543339729\n",
      "Epoch 23993/30000 Training Loss: 0.057754408568143845\n",
      "Epoch 23994/30000 Training Loss: 0.04803253710269928\n",
      "Epoch 23995/30000 Training Loss: 0.033075906336307526\n",
      "Epoch 23996/30000 Training Loss: 0.034307532012462616\n",
      "Epoch 23997/30000 Training Loss: 0.054832153022289276\n",
      "Epoch 23998/30000 Training Loss: 0.0583181157708168\n",
      "Epoch 23999/30000 Training Loss: 0.0497564896941185\n",
      "Epoch 24000/30000 Training Loss: 0.05021961033344269\n",
      "Epoch 24000/30000 Validation Loss: 0.03866969794034958\n",
      "Epoch 24001/30000 Training Loss: 0.034602075815200806\n",
      "Epoch 24002/30000 Training Loss: 0.05965548753738403\n",
      "Epoch 24003/30000 Training Loss: 0.03916683793067932\n",
      "Epoch 24004/30000 Training Loss: 0.04658377543091774\n",
      "Epoch 24005/30000 Training Loss: 0.031279463320970535\n",
      "Epoch 24006/30000 Training Loss: 0.05267622321844101\n",
      "Epoch 24007/30000 Training Loss: 0.039050497114658356\n",
      "Epoch 24008/30000 Training Loss: 0.03937791287899017\n",
      "Epoch 24009/30000 Training Loss: 0.038260381668806076\n",
      "Epoch 24010/30000 Training Loss: 0.03921039029955864\n",
      "Epoch 24011/30000 Training Loss: 0.03604438528418541\n",
      "Epoch 24012/30000 Training Loss: 0.046634070575237274\n",
      "Epoch 24013/30000 Training Loss: 0.052309565246105194\n",
      "Epoch 24014/30000 Training Loss: 0.04783865064382553\n",
      "Epoch 24015/30000 Training Loss: 0.04839074984192848\n",
      "Epoch 24016/30000 Training Loss: 0.0421391986310482\n",
      "Epoch 24017/30000 Training Loss: 0.044188592582941055\n",
      "Epoch 24018/30000 Training Loss: 0.05892545357346535\n",
      "Epoch 24019/30000 Training Loss: 0.04353326931595802\n",
      "Epoch 24020/30000 Training Loss: 0.04829320311546326\n",
      "Epoch 24021/30000 Training Loss: 0.0437060110270977\n",
      "Epoch 24022/30000 Training Loss: 0.043272849172353745\n",
      "Epoch 24023/30000 Training Loss: 0.0466265007853508\n",
      "Epoch 24024/30000 Training Loss: 0.050602130591869354\n",
      "Epoch 24025/30000 Training Loss: 0.03412076458334923\n",
      "Epoch 24026/30000 Training Loss: 0.06523426622152328\n",
      "Epoch 24027/30000 Training Loss: 0.039827972650527954\n",
      "Epoch 24028/30000 Training Loss: 0.030791040509939194\n",
      "Epoch 24029/30000 Training Loss: 0.04029661789536476\n",
      "Epoch 24030/30000 Training Loss: 0.04257926717400551\n",
      "Epoch 24031/30000 Training Loss: 0.043472643941640854\n",
      "Epoch 24032/30000 Training Loss: 0.02918296493589878\n",
      "Epoch 24033/30000 Training Loss: 0.04263533279299736\n",
      "Epoch 24034/30000 Training Loss: 0.04928838461637497\n",
      "Epoch 24035/30000 Training Loss: 0.04176782816648483\n",
      "Epoch 24036/30000 Training Loss: 0.03601941466331482\n",
      "Epoch 24037/30000 Training Loss: 0.05821980535984039\n",
      "Epoch 24038/30000 Training Loss: 0.0625295341014862\n",
      "Epoch 24039/30000 Training Loss: 0.04907748103141785\n",
      "Epoch 24040/30000 Training Loss: 0.04312095791101456\n",
      "Epoch 24041/30000 Training Loss: 0.04976994916796684\n",
      "Epoch 24042/30000 Training Loss: 0.03214097023010254\n",
      "Epoch 24043/30000 Training Loss: 0.04631469026207924\n",
      "Epoch 24044/30000 Training Loss: 0.05341988801956177\n",
      "Epoch 24045/30000 Training Loss: 0.05306199565529823\n",
      "Epoch 24046/30000 Training Loss: 0.03951224684715271\n",
      "Epoch 24047/30000 Training Loss: 0.055111341178417206\n",
      "Epoch 24048/30000 Training Loss: 0.040325913578271866\n",
      "Epoch 24049/30000 Training Loss: 0.04085254669189453\n",
      "Epoch 24050/30000 Training Loss: 0.052732616662979126\n",
      "Epoch 24051/30000 Training Loss: 0.03577757999300957\n",
      "Epoch 24052/30000 Training Loss: 0.03990771621465683\n",
      "Epoch 24053/30000 Training Loss: 0.03840804845094681\n",
      "Epoch 24054/30000 Training Loss: 0.051219016313552856\n",
      "Epoch 24055/30000 Training Loss: 0.037756845355033875\n",
      "Epoch 24056/30000 Training Loss: 0.06778621673583984\n",
      "Epoch 24057/30000 Training Loss: 0.05733315646648407\n",
      "Epoch 24058/30000 Training Loss: 0.04323092848062515\n",
      "Epoch 24059/30000 Training Loss: 0.055187202990055084\n",
      "Epoch 24060/30000 Training Loss: 0.05044412612915039\n",
      "Epoch 24061/30000 Training Loss: 0.05914478003978729\n",
      "Epoch 24062/30000 Training Loss: 0.042333558201789856\n",
      "Epoch 24063/30000 Training Loss: 0.048127561807632446\n",
      "Epoch 24064/30000 Training Loss: 0.04985674098134041\n",
      "Epoch 24065/30000 Training Loss: 0.05655728280544281\n",
      "Epoch 24066/30000 Training Loss: 0.026738397777080536\n",
      "Epoch 24067/30000 Training Loss: 0.043610475957393646\n",
      "Epoch 24068/30000 Training Loss: 0.04633495211601257\n",
      "Epoch 24069/30000 Training Loss: 0.0435832180082798\n",
      "Epoch 24070/30000 Training Loss: 0.04694514721632004\n",
      "Epoch 24071/30000 Training Loss: 0.05675908178091049\n",
      "Epoch 24072/30000 Training Loss: 0.056339144706726074\n",
      "Epoch 24073/30000 Training Loss: 0.0507441982626915\n",
      "Epoch 24074/30000 Training Loss: 0.06323747336864471\n",
      "Epoch 24075/30000 Training Loss: 0.0628117024898529\n",
      "Epoch 24076/30000 Training Loss: 0.04724620282649994\n",
      "Epoch 24077/30000 Training Loss: 0.03521985560655594\n",
      "Epoch 24078/30000 Training Loss: 0.04485637694597244\n",
      "Epoch 24079/30000 Training Loss: 0.04563676193356514\n",
      "Epoch 24080/30000 Training Loss: 0.05507512390613556\n",
      "Epoch 24081/30000 Training Loss: 0.042396772652864456\n",
      "Epoch 24082/30000 Training Loss: 0.0525205060839653\n",
      "Epoch 24083/30000 Training Loss: 0.04855336993932724\n",
      "Epoch 24084/30000 Training Loss: 0.047668859362602234\n",
      "Epoch 24085/30000 Training Loss: 0.043734338134527206\n",
      "Epoch 24086/30000 Training Loss: 0.04994114488363266\n",
      "Epoch 24087/30000 Training Loss: 0.05152330547571182\n",
      "Epoch 24088/30000 Training Loss: 0.049472060054540634\n",
      "Epoch 24089/30000 Training Loss: 0.05289577320218086\n",
      "Epoch 24090/30000 Training Loss: 0.04832812026143074\n",
      "Epoch 24091/30000 Training Loss: 0.05085752159357071\n",
      "Epoch 24092/30000 Training Loss: 0.052272211760282516\n",
      "Epoch 24093/30000 Training Loss: 0.05327271670103073\n",
      "Epoch 24094/30000 Training Loss: 0.037612613290548325\n",
      "Epoch 24095/30000 Training Loss: 0.061063267290592194\n",
      "Epoch 24096/30000 Training Loss: 0.04299277812242508\n",
      "Epoch 24097/30000 Training Loss: 0.04375869780778885\n",
      "Epoch 24098/30000 Training Loss: 0.04941745102405548\n",
      "Epoch 24099/30000 Training Loss: 0.046139080077409744\n",
      "Epoch 24100/30000 Training Loss: 0.04156974330544472\n",
      "Epoch 24100/30000 Validation Loss: 0.03571232408285141\n",
      "Epoch 24101/30000 Training Loss: 0.0663878470659256\n",
      "Epoch 24102/30000 Training Loss: 0.030412066727876663\n",
      "Epoch 24103/30000 Training Loss: 0.043798260390758514\n",
      "Epoch 24104/30000 Training Loss: 0.0526312030851841\n",
      "Epoch 24105/30000 Training Loss: 0.0475257933139801\n",
      "Epoch 24106/30000 Training Loss: 0.0535738542675972\n",
      "Epoch 24107/30000 Training Loss: 0.04984617978334427\n",
      "Epoch 24108/30000 Training Loss: 0.05197121202945709\n",
      "Epoch 24109/30000 Training Loss: 0.052184998989105225\n",
      "Epoch 24110/30000 Training Loss: 0.06357122957706451\n",
      "Epoch 24111/30000 Training Loss: 0.04352837800979614\n",
      "Epoch 24112/30000 Training Loss: 0.051163721829652786\n",
      "Epoch 24113/30000 Training Loss: 0.059285327792167664\n",
      "Epoch 24114/30000 Training Loss: 0.05264957621693611\n",
      "Epoch 24115/30000 Training Loss: 0.033665500581264496\n",
      "Epoch 24116/30000 Training Loss: 0.05151893198490143\n",
      "Epoch 24117/30000 Training Loss: 0.046651847660541534\n",
      "Epoch 24118/30000 Training Loss: 0.05978066846728325\n",
      "Epoch 24119/30000 Training Loss: 0.046793870627880096\n",
      "Epoch 24120/30000 Training Loss: 0.052301645278930664\n",
      "Epoch 24121/30000 Training Loss: 0.05264649540185928\n",
      "Epoch 24122/30000 Training Loss: 0.03713204711675644\n",
      "Epoch 24123/30000 Training Loss: 0.03324558585882187\n",
      "Epoch 24124/30000 Training Loss: 0.03340548276901245\n",
      "Epoch 24125/30000 Training Loss: 0.05639595910906792\n",
      "Epoch 24126/30000 Training Loss: 0.039063818752765656\n",
      "Epoch 24127/30000 Training Loss: 0.08559682220220566\n",
      "Epoch 24128/30000 Training Loss: 0.04681161791086197\n",
      "Epoch 24129/30000 Training Loss: 0.04167445749044418\n",
      "Epoch 24130/30000 Training Loss: 0.04551216959953308\n",
      "Epoch 24131/30000 Training Loss: 0.04632827639579773\n",
      "Epoch 24132/30000 Training Loss: 0.045495808124542236\n",
      "Epoch 24133/30000 Training Loss: 0.04404265806078911\n",
      "Epoch 24134/30000 Training Loss: 0.03899433836340904\n",
      "Epoch 24135/30000 Training Loss: 0.04210353642702103\n",
      "Epoch 24136/30000 Training Loss: 0.04494091868400574\n",
      "Epoch 24137/30000 Training Loss: 0.0506230890750885\n",
      "Epoch 24138/30000 Training Loss: 0.04623473435640335\n",
      "Epoch 24139/30000 Training Loss: 0.044734321534633636\n",
      "Epoch 24140/30000 Training Loss: 0.04531945288181305\n",
      "Epoch 24141/30000 Training Loss: 0.039790309965610504\n",
      "Epoch 24142/30000 Training Loss: 0.02898736670613289\n",
      "Epoch 24143/30000 Training Loss: 0.05058888718485832\n",
      "Epoch 24144/30000 Training Loss: 0.0543404296040535\n",
      "Epoch 24145/30000 Training Loss: 0.058169446885585785\n",
      "Epoch 24146/30000 Training Loss: 0.034637924283742905\n",
      "Epoch 24147/30000 Training Loss: 0.037952933460474014\n",
      "Epoch 24148/30000 Training Loss: 0.03777832165360451\n",
      "Epoch 24149/30000 Training Loss: 0.0548178032040596\n",
      "Epoch 24150/30000 Training Loss: 0.03396153450012207\n",
      "Epoch 24151/30000 Training Loss: 0.048037394881248474\n",
      "Epoch 24152/30000 Training Loss: 0.053971774876117706\n",
      "Epoch 24153/30000 Training Loss: 0.04910055547952652\n",
      "Epoch 24154/30000 Training Loss: 0.04714963585138321\n",
      "Epoch 24155/30000 Training Loss: 0.05004710704088211\n",
      "Epoch 24156/30000 Training Loss: 0.04453214630484581\n",
      "Epoch 24157/30000 Training Loss: 0.032159335911273956\n",
      "Epoch 24158/30000 Training Loss: 0.04433983191847801\n",
      "Epoch 24159/30000 Training Loss: 0.049534931778907776\n",
      "Epoch 24160/30000 Training Loss: 0.05359853059053421\n",
      "Epoch 24161/30000 Training Loss: 0.0468263253569603\n",
      "Epoch 24162/30000 Training Loss: 0.04611308127641678\n",
      "Epoch 24163/30000 Training Loss: 0.054240405559539795\n",
      "Epoch 24164/30000 Training Loss: 0.034508123993873596\n",
      "Epoch 24165/30000 Training Loss: 0.05714201182126999\n",
      "Epoch 24166/30000 Training Loss: 0.038129594177007675\n",
      "Epoch 24167/30000 Training Loss: 0.04663550853729248\n",
      "Epoch 24168/30000 Training Loss: 0.045611560344696045\n",
      "Epoch 24169/30000 Training Loss: 0.04452080279588699\n",
      "Epoch 24170/30000 Training Loss: 0.05054493248462677\n",
      "Epoch 24171/30000 Training Loss: 0.042316894978284836\n",
      "Epoch 24172/30000 Training Loss: 0.04440484941005707\n",
      "Epoch 24173/30000 Training Loss: 0.04151258245110512\n",
      "Epoch 24174/30000 Training Loss: 0.03445696085691452\n",
      "Epoch 24175/30000 Training Loss: 0.04107553884387016\n",
      "Epoch 24176/30000 Training Loss: 0.041637808084487915\n",
      "Epoch 24177/30000 Training Loss: 0.058451585471630096\n",
      "Epoch 24178/30000 Training Loss: 0.04520503431558609\n",
      "Epoch 24179/30000 Training Loss: 0.04648889973759651\n",
      "Epoch 24180/30000 Training Loss: 0.059736382216215134\n",
      "Epoch 24181/30000 Training Loss: 0.042865656316280365\n",
      "Epoch 24182/30000 Training Loss: 0.054928407073020935\n",
      "Epoch 24183/30000 Training Loss: 0.05547932907938957\n",
      "Epoch 24184/30000 Training Loss: 0.04043811932206154\n",
      "Epoch 24185/30000 Training Loss: 0.045004475861787796\n",
      "Epoch 24186/30000 Training Loss: 0.05873434245586395\n",
      "Epoch 24187/30000 Training Loss: 0.05112627148628235\n",
      "Epoch 24188/30000 Training Loss: 0.06231508404016495\n",
      "Epoch 24189/30000 Training Loss: 0.056570373475551605\n",
      "Epoch 24190/30000 Training Loss: 0.04570462927222252\n",
      "Epoch 24191/30000 Training Loss: 0.043182168155908585\n",
      "Epoch 24192/30000 Training Loss: 0.03870983421802521\n",
      "Epoch 24193/30000 Training Loss: 0.03189486637711525\n",
      "Epoch 24194/30000 Training Loss: 0.04417576640844345\n",
      "Epoch 24195/30000 Training Loss: 0.039995767176151276\n",
      "Epoch 24196/30000 Training Loss: 0.03345959261059761\n",
      "Epoch 24197/30000 Training Loss: 0.056136518716812134\n",
      "Epoch 24198/30000 Training Loss: 0.04089932516217232\n",
      "Epoch 24199/30000 Training Loss: 0.031949203461408615\n",
      "Epoch 24200/30000 Training Loss: 0.04811191186308861\n",
      "Epoch 24200/30000 Validation Loss: 0.07060973346233368\n",
      "Epoch 24201/30000 Training Loss: 0.047875452786684036\n",
      "Epoch 24202/30000 Training Loss: 0.03976497799158096\n",
      "Epoch 24203/30000 Training Loss: 0.04745497927069664\n",
      "Epoch 24204/30000 Training Loss: 0.035973530262708664\n",
      "Epoch 24205/30000 Training Loss: 0.04015875980257988\n",
      "Epoch 24206/30000 Training Loss: 0.06376656889915466\n",
      "Epoch 24207/30000 Training Loss: 0.05451837554574013\n",
      "Epoch 24208/30000 Training Loss: 0.057591814547777176\n",
      "Epoch 24209/30000 Training Loss: 0.03788071125745773\n",
      "Epoch 24210/30000 Training Loss: 0.040914107114076614\n",
      "Epoch 24211/30000 Training Loss: 0.0623210147023201\n",
      "Epoch 24212/30000 Training Loss: 0.06948243826627731\n",
      "Epoch 24213/30000 Training Loss: 0.04443781450390816\n",
      "Epoch 24214/30000 Training Loss: 0.0515144057571888\n",
      "Epoch 24215/30000 Training Loss: 0.033122364431619644\n",
      "Epoch 24216/30000 Training Loss: 0.04986690357327461\n",
      "Epoch 24217/30000 Training Loss: 0.03522171825170517\n",
      "Epoch 24218/30000 Training Loss: 0.0502605214715004\n",
      "Epoch 24219/30000 Training Loss: 0.03493563085794449\n",
      "Epoch 24220/30000 Training Loss: 0.047604646533727646\n",
      "Epoch 24221/30000 Training Loss: 0.04275885969400406\n",
      "Epoch 24222/30000 Training Loss: 0.04235117509961128\n",
      "Epoch 24223/30000 Training Loss: 0.03954728692770004\n",
      "Epoch 24224/30000 Training Loss: 0.03063846379518509\n",
      "Epoch 24225/30000 Training Loss: 0.035823941230773926\n",
      "Epoch 24226/30000 Training Loss: 0.03758929669857025\n",
      "Epoch 24227/30000 Training Loss: 0.033440474420785904\n",
      "Epoch 24228/30000 Training Loss: 0.04157407581806183\n",
      "Epoch 24229/30000 Training Loss: 0.04260372370481491\n",
      "Epoch 24230/30000 Training Loss: 0.05925143137574196\n",
      "Epoch 24231/30000 Training Loss: 0.04554964229464531\n",
      "Epoch 24232/30000 Training Loss: 0.05710591375827789\n",
      "Epoch 24233/30000 Training Loss: 0.04227104410529137\n",
      "Epoch 24234/30000 Training Loss: 0.05875565856695175\n",
      "Epoch 24235/30000 Training Loss: 0.05009513720870018\n",
      "Epoch 24236/30000 Training Loss: 0.04762066900730133\n",
      "Epoch 24237/30000 Training Loss: 0.03444072604179382\n",
      "Epoch 24238/30000 Training Loss: 0.05173793435096741\n",
      "Epoch 24239/30000 Training Loss: 0.049135513603687286\n",
      "Epoch 24240/30000 Training Loss: 0.039253316819667816\n",
      "Epoch 24241/30000 Training Loss: 0.05183205381035805\n",
      "Epoch 24242/30000 Training Loss: 0.048358768224716187\n",
      "Epoch 24243/30000 Training Loss: 0.03695771098136902\n",
      "Epoch 24244/30000 Training Loss: 0.05554675683379173\n",
      "Epoch 24245/30000 Training Loss: 0.048190947622060776\n",
      "Epoch 24246/30000 Training Loss: 0.035865891724824905\n",
      "Epoch 24247/30000 Training Loss: 0.047828786075115204\n",
      "Epoch 24248/30000 Training Loss: 0.06696067005395889\n",
      "Epoch 24249/30000 Training Loss: 0.046364255249500275\n",
      "Epoch 24250/30000 Training Loss: 0.05400226265192032\n",
      "Epoch 24251/30000 Training Loss: 0.026139147579669952\n",
      "Epoch 24252/30000 Training Loss: 0.036698076874017715\n",
      "Epoch 24253/30000 Training Loss: 0.04671219736337662\n",
      "Epoch 24254/30000 Training Loss: 0.04729685187339783\n",
      "Epoch 24255/30000 Training Loss: 0.045489951968193054\n",
      "Epoch 24256/30000 Training Loss: 0.055414289236068726\n",
      "Epoch 24257/30000 Training Loss: 0.034372903406620026\n",
      "Epoch 24258/30000 Training Loss: 0.03570331260561943\n",
      "Epoch 24259/30000 Training Loss: 0.04162599891424179\n",
      "Epoch 24260/30000 Training Loss: 0.052892446517944336\n",
      "Epoch 24261/30000 Training Loss: 0.047028541564941406\n",
      "Epoch 24262/30000 Training Loss: 0.0336320735514164\n",
      "Epoch 24263/30000 Training Loss: 0.05278192088007927\n",
      "Epoch 24264/30000 Training Loss: 0.06451703608036041\n",
      "Epoch 24265/30000 Training Loss: 0.033437181264162064\n",
      "Epoch 24266/30000 Training Loss: 0.046912822872400284\n",
      "Epoch 24267/30000 Training Loss: 0.05933470278978348\n",
      "Epoch 24268/30000 Training Loss: 0.057251155376434326\n",
      "Epoch 24269/30000 Training Loss: 0.05155184864997864\n",
      "Epoch 24270/30000 Training Loss: 0.056366223841905594\n",
      "Epoch 24271/30000 Training Loss: 0.04023890569806099\n",
      "Epoch 24272/30000 Training Loss: 0.04540032893419266\n",
      "Epoch 24273/30000 Training Loss: 0.045084379613399506\n",
      "Epoch 24274/30000 Training Loss: 0.04466782882809639\n",
      "Epoch 24275/30000 Training Loss: 0.040161583572626114\n",
      "Epoch 24276/30000 Training Loss: 0.032403796911239624\n",
      "Epoch 24277/30000 Training Loss: 0.0486890934407711\n",
      "Epoch 24278/30000 Training Loss: 0.044900279492139816\n",
      "Epoch 24279/30000 Training Loss: 0.05017106235027313\n",
      "Epoch 24280/30000 Training Loss: 0.04625975713133812\n",
      "Epoch 24281/30000 Training Loss: 0.04442750662565231\n",
      "Epoch 24282/30000 Training Loss: 0.0512504018843174\n",
      "Epoch 24283/30000 Training Loss: 0.06328806281089783\n",
      "Epoch 24284/30000 Training Loss: 0.04002135619521141\n",
      "Epoch 24285/30000 Training Loss: 0.04880024120211601\n",
      "Epoch 24286/30000 Training Loss: 0.04472940042614937\n",
      "Epoch 24287/30000 Training Loss: 0.042824070900678635\n",
      "Epoch 24288/30000 Training Loss: 0.045770302414894104\n",
      "Epoch 24289/30000 Training Loss: 0.04827037453651428\n",
      "Epoch 24290/30000 Training Loss: 0.04862821102142334\n",
      "Epoch 24291/30000 Training Loss: 0.03811652585864067\n",
      "Epoch 24292/30000 Training Loss: 0.044304583221673965\n",
      "Epoch 24293/30000 Training Loss: 0.07563730329275131\n",
      "Epoch 24294/30000 Training Loss: 0.044882141053676605\n",
      "Epoch 24295/30000 Training Loss: 0.036480776965618134\n",
      "Epoch 24296/30000 Training Loss: 0.041452810168266296\n",
      "Epoch 24297/30000 Training Loss: 0.03421298414468765\n",
      "Epoch 24298/30000 Training Loss: 0.03480318561196327\n",
      "Epoch 24299/30000 Training Loss: 0.04458131641149521\n",
      "Epoch 24300/30000 Training Loss: 0.04102599620819092\n",
      "Epoch 24300/30000 Validation Loss: 0.04263496398925781\n",
      "Epoch 24301/30000 Training Loss: 0.04963042587041855\n",
      "Epoch 24302/30000 Training Loss: 0.03597743436694145\n",
      "Epoch 24303/30000 Training Loss: 0.05653511360287666\n",
      "Epoch 24304/30000 Training Loss: 0.05521149933338165\n",
      "Epoch 24305/30000 Training Loss: 0.03162930905818939\n",
      "Epoch 24306/30000 Training Loss: 0.03335587680339813\n",
      "Epoch 24307/30000 Training Loss: 0.032760366797447205\n",
      "Epoch 24308/30000 Training Loss: 0.06262774765491486\n",
      "Epoch 24309/30000 Training Loss: 0.03790803253650665\n",
      "Epoch 24310/30000 Training Loss: 0.040395162999629974\n",
      "Epoch 24311/30000 Training Loss: 0.04816902428865433\n",
      "Epoch 24312/30000 Training Loss: 0.05187155306339264\n",
      "Epoch 24313/30000 Training Loss: 0.03933155536651611\n",
      "Epoch 24314/30000 Training Loss: 0.03888005018234253\n",
      "Epoch 24315/30000 Training Loss: 0.04820076376199722\n",
      "Epoch 24316/30000 Training Loss: 0.03647686913609505\n",
      "Epoch 24317/30000 Training Loss: 0.042890697717666626\n",
      "Epoch 24318/30000 Training Loss: 0.04193918779492378\n",
      "Epoch 24319/30000 Training Loss: 0.046120259910821915\n",
      "Epoch 24320/30000 Training Loss: 0.05244503915309906\n",
      "Epoch 24321/30000 Training Loss: 0.05040363222360611\n",
      "Epoch 24322/30000 Training Loss: 0.026744380593299866\n",
      "Epoch 24323/30000 Training Loss: 0.04229063540697098\n",
      "Epoch 24324/30000 Training Loss: 0.04902756214141846\n",
      "Epoch 24325/30000 Training Loss: 0.04934272915124893\n",
      "Epoch 24326/30000 Training Loss: 0.04568105936050415\n",
      "Epoch 24327/30000 Training Loss: 0.05037839338183403\n",
      "Epoch 24328/30000 Training Loss: 0.0316193625330925\n",
      "Epoch 24329/30000 Training Loss: 0.046922847628593445\n",
      "Epoch 24330/30000 Training Loss: 0.04206505045294762\n",
      "Epoch 24331/30000 Training Loss: 0.03982989862561226\n",
      "Epoch 24332/30000 Training Loss: 0.060009125620126724\n",
      "Epoch 24333/30000 Training Loss: 0.051886193454265594\n",
      "Epoch 24334/30000 Training Loss: 0.043528467416763306\n",
      "Epoch 24335/30000 Training Loss: 0.040838319808244705\n",
      "Epoch 24336/30000 Training Loss: 0.03968530148267746\n",
      "Epoch 24337/30000 Training Loss: 0.033441536128520966\n",
      "Epoch 24338/30000 Training Loss: 0.05562363564968109\n",
      "Epoch 24339/30000 Training Loss: 0.04843948036432266\n",
      "Epoch 24340/30000 Training Loss: 0.047487907111644745\n",
      "Epoch 24341/30000 Training Loss: 0.055896103382110596\n",
      "Epoch 24342/30000 Training Loss: 0.05120072513818741\n",
      "Epoch 24343/30000 Training Loss: 0.04647640138864517\n",
      "Epoch 24344/30000 Training Loss: 0.055748239159584045\n",
      "Epoch 24345/30000 Training Loss: 0.04337083548307419\n",
      "Epoch 24346/30000 Training Loss: 0.047718167304992676\n",
      "Epoch 24347/30000 Training Loss: 0.03713971748948097\n",
      "Epoch 24348/30000 Training Loss: 0.049795277416706085\n",
      "Epoch 24349/30000 Training Loss: 0.044429704546928406\n",
      "Epoch 24350/30000 Training Loss: 0.04611760377883911\n",
      "Epoch 24351/30000 Training Loss: 0.05648554861545563\n",
      "Epoch 24352/30000 Training Loss: 0.050619788467884064\n",
      "Epoch 24353/30000 Training Loss: 0.037867333739995956\n",
      "Epoch 24354/30000 Training Loss: 0.043546244502067566\n",
      "Epoch 24355/30000 Training Loss: 0.04033660516142845\n",
      "Epoch 24356/30000 Training Loss: 0.040130116045475006\n",
      "Epoch 24357/30000 Training Loss: 0.03766239061951637\n",
      "Epoch 24358/30000 Training Loss: 0.052853167057037354\n",
      "Epoch 24359/30000 Training Loss: 0.04244906082749367\n",
      "Epoch 24360/30000 Training Loss: 0.04899095371365547\n",
      "Epoch 24361/30000 Training Loss: 0.0394417829811573\n",
      "Epoch 24362/30000 Training Loss: 0.044398773461580276\n",
      "Epoch 24363/30000 Training Loss: 0.04489686340093613\n",
      "Epoch 24364/30000 Training Loss: 0.04075945168733597\n",
      "Epoch 24365/30000 Training Loss: 0.03844320774078369\n",
      "Epoch 24366/30000 Training Loss: 0.04227958247065544\n",
      "Epoch 24367/30000 Training Loss: 0.039837829768657684\n",
      "Epoch 24368/30000 Training Loss: 0.04179175943136215\n",
      "Epoch 24369/30000 Training Loss: 0.041205815970897675\n",
      "Epoch 24370/30000 Training Loss: 0.0316048264503479\n",
      "Epoch 24371/30000 Training Loss: 0.0430501289665699\n",
      "Epoch 24372/30000 Training Loss: 0.051974039524793625\n",
      "Epoch 24373/30000 Training Loss: 0.0379093773663044\n",
      "Epoch 24374/30000 Training Loss: 0.04161140322685242\n",
      "Epoch 24375/30000 Training Loss: 0.03533506393432617\n",
      "Epoch 24376/30000 Training Loss: 0.040506184101104736\n",
      "Epoch 24377/30000 Training Loss: 0.04789543151855469\n",
      "Epoch 24378/30000 Training Loss: 0.046189263463020325\n",
      "Epoch 24379/30000 Training Loss: 0.05970527231693268\n",
      "Epoch 24380/30000 Training Loss: 0.04375004395842552\n",
      "Epoch 24381/30000 Training Loss: 0.034185249358415604\n",
      "Epoch 24382/30000 Training Loss: 0.05096622556447983\n",
      "Epoch 24383/30000 Training Loss: 0.03830789774656296\n",
      "Epoch 24384/30000 Training Loss: 0.03063260018825531\n",
      "Epoch 24385/30000 Training Loss: 0.07787293195724487\n",
      "Epoch 24386/30000 Training Loss: 0.03531385213136673\n",
      "Epoch 24387/30000 Training Loss: 0.02942008152604103\n",
      "Epoch 24388/30000 Training Loss: 0.035923514515161514\n",
      "Epoch 24389/30000 Training Loss: 0.04114857316017151\n",
      "Epoch 24390/30000 Training Loss: 0.05234659090638161\n",
      "Epoch 24391/30000 Training Loss: 0.04128805175423622\n",
      "Epoch 24392/30000 Training Loss: 0.05619537830352783\n",
      "Epoch 24393/30000 Training Loss: 0.03873346745967865\n",
      "Epoch 24394/30000 Training Loss: 0.0348735973238945\n",
      "Epoch 24395/30000 Training Loss: 0.03969511389732361\n",
      "Epoch 24396/30000 Training Loss: 0.04562931880354881\n",
      "Epoch 24397/30000 Training Loss: 0.03946056216955185\n",
      "Epoch 24398/30000 Training Loss: 0.055370576679706573\n",
      "Epoch 24399/30000 Training Loss: 0.0425199456512928\n",
      "Epoch 24400/30000 Training Loss: 0.04883289337158203\n",
      "Epoch 24400/30000 Validation Loss: 0.041389286518096924\n",
      "Epoch 24401/30000 Training Loss: 0.054890211671590805\n",
      "Epoch 24402/30000 Training Loss: 0.04439963400363922\n",
      "Epoch 24403/30000 Training Loss: 0.04256235435605049\n",
      "Epoch 24404/30000 Training Loss: 0.05322948843240738\n",
      "Epoch 24405/30000 Training Loss: 0.05834144726395607\n",
      "Epoch 24406/30000 Training Loss: 0.05335552990436554\n",
      "Epoch 24407/30000 Training Loss: 0.03784329816699028\n",
      "Epoch 24408/30000 Training Loss: 0.04156508296728134\n",
      "Epoch 24409/30000 Training Loss: 0.04955627769231796\n",
      "Epoch 24410/30000 Training Loss: 0.038539472967386246\n",
      "Epoch 24411/30000 Training Loss: 0.04390083998441696\n",
      "Epoch 24412/30000 Training Loss: 0.056510306894779205\n",
      "Epoch 24413/30000 Training Loss: 0.043557681143283844\n",
      "Epoch 24414/30000 Training Loss: 0.03663009777665138\n",
      "Epoch 24415/30000 Training Loss: 0.05335265025496483\n",
      "Epoch 24416/30000 Training Loss: 0.045082442462444305\n",
      "Epoch 24417/30000 Training Loss: 0.03383595123887062\n",
      "Epoch 24418/30000 Training Loss: 0.03323331102728844\n",
      "Epoch 24419/30000 Training Loss: 0.06027298420667648\n",
      "Epoch 24420/30000 Training Loss: 0.04668699577450752\n",
      "Epoch 24421/30000 Training Loss: 0.04910881072282791\n",
      "Epoch 24422/30000 Training Loss: 0.05014590173959732\n",
      "Epoch 24423/30000 Training Loss: 0.031165651977062225\n",
      "Epoch 24424/30000 Training Loss: 0.042763471603393555\n",
      "Epoch 24425/30000 Training Loss: 0.05960465222597122\n",
      "Epoch 24426/30000 Training Loss: 0.047110918909311295\n",
      "Epoch 24427/30000 Training Loss: 0.046087704598903656\n",
      "Epoch 24428/30000 Training Loss: 0.05062851682305336\n",
      "Epoch 24429/30000 Training Loss: 0.046314310282468796\n",
      "Epoch 24430/30000 Training Loss: 0.038369983434677124\n",
      "Epoch 24431/30000 Training Loss: 0.051039453595876694\n",
      "Epoch 24432/30000 Training Loss: 0.0513271689414978\n",
      "Epoch 24433/30000 Training Loss: 0.05335768312215805\n",
      "Epoch 24434/30000 Training Loss: 0.05055522173643112\n",
      "Epoch 24435/30000 Training Loss: 0.06687219440937042\n",
      "Epoch 24436/30000 Training Loss: 0.05376240238547325\n",
      "Epoch 24437/30000 Training Loss: 0.06707160919904709\n",
      "Epoch 24438/30000 Training Loss: 0.03919876366853714\n",
      "Epoch 24439/30000 Training Loss: 0.046972524374723434\n",
      "Epoch 24440/30000 Training Loss: 0.03168174624443054\n",
      "Epoch 24441/30000 Training Loss: 0.05429217219352722\n",
      "Epoch 24442/30000 Training Loss: 0.06463789194822311\n",
      "Epoch 24443/30000 Training Loss: 0.04148406907916069\n",
      "Epoch 24444/30000 Training Loss: 0.0650157630443573\n",
      "Epoch 24445/30000 Training Loss: 0.040876567363739014\n",
      "Epoch 24446/30000 Training Loss: 0.058963924646377563\n",
      "Epoch 24447/30000 Training Loss: 0.04181382432579994\n",
      "Epoch 24448/30000 Training Loss: 0.044401951134204865\n",
      "Epoch 24449/30000 Training Loss: 0.03923669829964638\n",
      "Epoch 24450/30000 Training Loss: 0.04562416672706604\n",
      "Epoch 24451/30000 Training Loss: 0.043216556310653687\n",
      "Epoch 24452/30000 Training Loss: 0.0391702800989151\n",
      "Epoch 24453/30000 Training Loss: 0.05517593026161194\n",
      "Epoch 24454/30000 Training Loss: 0.03975791484117508\n",
      "Epoch 24455/30000 Training Loss: 0.04153955727815628\n",
      "Epoch 24456/30000 Training Loss: 0.054371096193790436\n",
      "Epoch 24457/30000 Training Loss: 0.054623622447252274\n",
      "Epoch 24458/30000 Training Loss: 0.0511106438934803\n",
      "Epoch 24459/30000 Training Loss: 0.040471985936164856\n",
      "Epoch 24460/30000 Training Loss: 0.06292849779129028\n",
      "Epoch 24461/30000 Training Loss: 0.03613368049263954\n",
      "Epoch 24462/30000 Training Loss: 0.039413366466760635\n",
      "Epoch 24463/30000 Training Loss: 0.04692764952778816\n",
      "Epoch 24464/30000 Training Loss: 0.03706606477499008\n",
      "Epoch 24465/30000 Training Loss: 0.04888366162776947\n",
      "Epoch 24466/30000 Training Loss: 0.04473298415541649\n",
      "Epoch 24467/30000 Training Loss: 0.051029056310653687\n",
      "Epoch 24468/30000 Training Loss: 0.058469921350479126\n",
      "Epoch 24469/30000 Training Loss: 0.048978760838508606\n",
      "Epoch 24470/30000 Training Loss: 0.05485612154006958\n",
      "Epoch 24471/30000 Training Loss: 0.051239896565675735\n",
      "Epoch 24472/30000 Training Loss: 0.03716985136270523\n",
      "Epoch 24473/30000 Training Loss: 0.038202736526727676\n",
      "Epoch 24474/30000 Training Loss: 0.04131047800183296\n",
      "Epoch 24475/30000 Training Loss: 0.03658955544233322\n",
      "Epoch 24476/30000 Training Loss: 0.050163693726062775\n",
      "Epoch 24477/30000 Training Loss: 0.03853356093168259\n",
      "Epoch 24478/30000 Training Loss: 0.04386919364333153\n",
      "Epoch 24479/30000 Training Loss: 0.02810507081449032\n",
      "Epoch 24480/30000 Training Loss: 0.05451313778758049\n",
      "Epoch 24481/30000 Training Loss: 0.046639878302812576\n",
      "Epoch 24482/30000 Training Loss: 0.03907245397567749\n",
      "Epoch 24483/30000 Training Loss: 0.07683604210615158\n",
      "Epoch 24484/30000 Training Loss: 0.06875704973936081\n",
      "Epoch 24485/30000 Training Loss: 0.03921322897076607\n",
      "Epoch 24486/30000 Training Loss: 0.029086679220199585\n",
      "Epoch 24487/30000 Training Loss: 0.04435168206691742\n",
      "Epoch 24488/30000 Training Loss: 0.042038336396217346\n",
      "Epoch 24489/30000 Training Loss: 0.061234720051288605\n",
      "Epoch 24490/30000 Training Loss: 0.04234841465950012\n",
      "Epoch 24491/30000 Training Loss: 0.043823689222335815\n",
      "Epoch 24492/30000 Training Loss: 0.04784932732582092\n",
      "Epoch 24493/30000 Training Loss: 0.04275140538811684\n",
      "Epoch 24494/30000 Training Loss: 0.05792435631155968\n",
      "Epoch 24495/30000 Training Loss: 0.05261629819869995\n",
      "Epoch 24496/30000 Training Loss: 0.053553640842437744\n",
      "Epoch 24497/30000 Training Loss: 0.054851170629262924\n",
      "Epoch 24498/30000 Training Loss: 0.03798677399754524\n",
      "Epoch 24499/30000 Training Loss: 0.04291480779647827\n",
      "Epoch 24500/30000 Training Loss: 0.05504078418016434\n",
      "Epoch 24500/30000 Validation Loss: 0.03784671053290367\n",
      "Epoch 24501/30000 Training Loss: 0.06203366443514824\n",
      "Epoch 24502/30000 Training Loss: 0.03475545346736908\n",
      "Epoch 24503/30000 Training Loss: 0.04323147237300873\n",
      "Epoch 24504/30000 Training Loss: 0.05076257884502411\n",
      "Epoch 24505/30000 Training Loss: 0.036529745906591415\n",
      "Epoch 24506/30000 Training Loss: 0.05446513369679451\n",
      "Epoch 24507/30000 Training Loss: 0.03942438215017319\n",
      "Epoch 24508/30000 Training Loss: 0.04164549708366394\n",
      "Epoch 24509/30000 Training Loss: 0.039411187171936035\n",
      "Epoch 24510/30000 Training Loss: 0.04445987194776535\n",
      "Epoch 24511/30000 Training Loss: 0.03144366294145584\n",
      "Epoch 24512/30000 Training Loss: 0.03630931302905083\n",
      "Epoch 24513/30000 Training Loss: 0.02753845788538456\n",
      "Epoch 24514/30000 Training Loss: 0.04744851589202881\n",
      "Epoch 24515/30000 Training Loss: 0.043631963431835175\n",
      "Epoch 24516/30000 Training Loss: 0.055634818971157074\n",
      "Epoch 24517/30000 Training Loss: 0.04594291374087334\n",
      "Epoch 24518/30000 Training Loss: 0.03687398508191109\n",
      "Epoch 24519/30000 Training Loss: 0.06185517832636833\n",
      "Epoch 24520/30000 Training Loss: 0.02792767994105816\n",
      "Epoch 24521/30000 Training Loss: 0.036872655153274536\n",
      "Epoch 24522/30000 Training Loss: 0.04478743299841881\n",
      "Epoch 24523/30000 Training Loss: 0.04033087566494942\n",
      "Epoch 24524/30000 Training Loss: 0.03670463338494301\n",
      "Epoch 24525/30000 Training Loss: 0.06364331394433975\n",
      "Epoch 24526/30000 Training Loss: 0.054792024195194244\n",
      "Epoch 24527/30000 Training Loss: 0.050722237676382065\n",
      "Epoch 24528/30000 Training Loss: 0.05284776911139488\n",
      "Epoch 24529/30000 Training Loss: 0.04931587725877762\n",
      "Epoch 24530/30000 Training Loss: 0.04572576284408569\n",
      "Epoch 24531/30000 Training Loss: 0.04166189581155777\n",
      "Epoch 24532/30000 Training Loss: 0.058275651186704636\n",
      "Epoch 24533/30000 Training Loss: 0.03651507571339607\n",
      "Epoch 24534/30000 Training Loss: 0.04532798007130623\n",
      "Epoch 24535/30000 Training Loss: 0.06663185358047485\n",
      "Epoch 24536/30000 Training Loss: 0.03854002058506012\n",
      "Epoch 24537/30000 Training Loss: 0.052700579166412354\n",
      "Epoch 24538/30000 Training Loss: 0.06460177898406982\n",
      "Epoch 24539/30000 Training Loss: 0.0476047582924366\n",
      "Epoch 24540/30000 Training Loss: 0.052578140050172806\n",
      "Epoch 24541/30000 Training Loss: 0.03511017560958862\n",
      "Epoch 24542/30000 Training Loss: 0.04768477380275726\n",
      "Epoch 24543/30000 Training Loss: 0.04504513368010521\n",
      "Epoch 24544/30000 Training Loss: 0.062288470566272736\n",
      "Epoch 24545/30000 Training Loss: 0.040300801396369934\n",
      "Epoch 24546/30000 Training Loss: 0.04600450396537781\n",
      "Epoch 24547/30000 Training Loss: 0.06243837997317314\n",
      "Epoch 24548/30000 Training Loss: 0.04166186973452568\n",
      "Epoch 24549/30000 Training Loss: 0.05516374111175537\n",
      "Epoch 24550/30000 Training Loss: 0.04322940856218338\n",
      "Epoch 24551/30000 Training Loss: 0.042724549770355225\n",
      "Epoch 24552/30000 Training Loss: 0.040259309113025665\n",
      "Epoch 24553/30000 Training Loss: 0.05017561838030815\n",
      "Epoch 24554/30000 Training Loss: 0.04049481824040413\n",
      "Epoch 24555/30000 Training Loss: 0.04933168739080429\n",
      "Epoch 24556/30000 Training Loss: 0.046065766364336014\n",
      "Epoch 24557/30000 Training Loss: 0.05180332809686661\n",
      "Epoch 24558/30000 Training Loss: 0.05743991956114769\n",
      "Epoch 24559/30000 Training Loss: 0.047098517417907715\n",
      "Epoch 24560/30000 Training Loss: 0.049673520028591156\n",
      "Epoch 24561/30000 Training Loss: 0.04293427616357803\n",
      "Epoch 24562/30000 Training Loss: 0.05631595104932785\n",
      "Epoch 24563/30000 Training Loss: 0.0400996133685112\n",
      "Epoch 24564/30000 Training Loss: 0.04450787603855133\n",
      "Epoch 24565/30000 Training Loss: 0.04674454778432846\n",
      "Epoch 24566/30000 Training Loss: 0.03431718051433563\n",
      "Epoch 24567/30000 Training Loss: 0.04597752168774605\n",
      "Epoch 24568/30000 Training Loss: 0.051127731800079346\n",
      "Epoch 24569/30000 Training Loss: 0.044394128024578094\n",
      "Epoch 24570/30000 Training Loss: 0.04982971400022507\n",
      "Epoch 24571/30000 Training Loss: 0.04131563752889633\n",
      "Epoch 24572/30000 Training Loss: 0.045320767909288406\n",
      "Epoch 24573/30000 Training Loss: 0.030738644301891327\n",
      "Epoch 24574/30000 Training Loss: 0.0417446605861187\n",
      "Epoch 24575/30000 Training Loss: 0.03535480052232742\n",
      "Epoch 24576/30000 Training Loss: 0.03599390387535095\n",
      "Epoch 24577/30000 Training Loss: 0.02977299876511097\n",
      "Epoch 24578/30000 Training Loss: 0.05148089677095413\n",
      "Epoch 24579/30000 Training Loss: 0.048819564282894135\n",
      "Epoch 24580/30000 Training Loss: 0.05842738598585129\n",
      "Epoch 24581/30000 Training Loss: 0.044023267924785614\n",
      "Epoch 24582/30000 Training Loss: 0.0348641499876976\n",
      "Epoch 24583/30000 Training Loss: 0.05154195427894592\n",
      "Epoch 24584/30000 Training Loss: 0.04650202393531799\n",
      "Epoch 24585/30000 Training Loss: 0.0319528803229332\n",
      "Epoch 24586/30000 Training Loss: 0.059844810515642166\n",
      "Epoch 24587/30000 Training Loss: 0.038872942328453064\n",
      "Epoch 24588/30000 Training Loss: 0.03290708735585213\n",
      "Epoch 24589/30000 Training Loss: 0.03404364734888077\n",
      "Epoch 24590/30000 Training Loss: 0.04972928389906883\n",
      "Epoch 24591/30000 Training Loss: 0.053248632699251175\n",
      "Epoch 24592/30000 Training Loss: 0.049477580934762955\n",
      "Epoch 24593/30000 Training Loss: 0.04374682903289795\n",
      "Epoch 24594/30000 Training Loss: 0.04110904783010483\n",
      "Epoch 24595/30000 Training Loss: 0.04933391883969307\n",
      "Epoch 24596/30000 Training Loss: 0.04336116090416908\n",
      "Epoch 24597/30000 Training Loss: 0.034669551998376846\n",
      "Epoch 24598/30000 Training Loss: 0.0385470874607563\n",
      "Epoch 24599/30000 Training Loss: 0.04562443494796753\n",
      "Epoch 24600/30000 Training Loss: 0.05708860605955124\n",
      "Epoch 24600/30000 Validation Loss: 0.044005103409290314\n",
      "Epoch 24601/30000 Training Loss: 0.043258458375930786\n",
      "Epoch 24602/30000 Training Loss: 0.039734791964292526\n",
      "Epoch 24603/30000 Training Loss: 0.04828716441988945\n",
      "Epoch 24604/30000 Training Loss: 0.0460766963660717\n",
      "Epoch 24605/30000 Training Loss: 0.052855439484119415\n",
      "Epoch 24606/30000 Training Loss: 0.04789254441857338\n",
      "Epoch 24607/30000 Training Loss: 0.08486092835664749\n",
      "Epoch 24608/30000 Training Loss: 0.04006781429052353\n",
      "Epoch 24609/30000 Training Loss: 0.040003519505262375\n",
      "Epoch 24610/30000 Training Loss: 0.05323243886232376\n",
      "Epoch 24611/30000 Training Loss: 0.029966041445732117\n",
      "Epoch 24612/30000 Training Loss: 0.039873309433460236\n",
      "Epoch 24613/30000 Training Loss: 0.052698954939842224\n",
      "Epoch 24614/30000 Training Loss: 0.03840380162000656\n",
      "Epoch 24615/30000 Training Loss: 0.044231317937374115\n",
      "Epoch 24616/30000 Training Loss: 0.043165430426597595\n",
      "Epoch 24617/30000 Training Loss: 0.057109348475933075\n",
      "Epoch 24618/30000 Training Loss: 0.035732902586460114\n",
      "Epoch 24619/30000 Training Loss: 0.051510609686374664\n",
      "Epoch 24620/30000 Training Loss: 0.029358699917793274\n",
      "Epoch 24621/30000 Training Loss: 0.043455593287944794\n",
      "Epoch 24622/30000 Training Loss: 0.04359758272767067\n",
      "Epoch 24623/30000 Training Loss: 0.045070622116327286\n",
      "Epoch 24624/30000 Training Loss: 0.049461476504802704\n",
      "Epoch 24625/30000 Training Loss: 0.04795254394412041\n",
      "Epoch 24626/30000 Training Loss: 0.043360330164432526\n",
      "Epoch 24627/30000 Training Loss: 0.0564424991607666\n",
      "Epoch 24628/30000 Training Loss: 0.05196826532483101\n",
      "Epoch 24629/30000 Training Loss: 0.045508258044719696\n",
      "Epoch 24630/30000 Training Loss: 0.046669136732816696\n",
      "Epoch 24631/30000 Training Loss: 0.051295481622219086\n",
      "Epoch 24632/30000 Training Loss: 0.064019575715065\n",
      "Epoch 24633/30000 Training Loss: 0.04061669111251831\n",
      "Epoch 24634/30000 Training Loss: 0.051362667232751846\n",
      "Epoch 24635/30000 Training Loss: 0.035480912774801254\n",
      "Epoch 24636/30000 Training Loss: 0.051528967916965485\n",
      "Epoch 24637/30000 Training Loss: 0.029915370047092438\n",
      "Epoch 24638/30000 Training Loss: 0.04913034662604332\n",
      "Epoch 24639/30000 Training Loss: 0.047784194350242615\n",
      "Epoch 24640/30000 Training Loss: 0.042291153222322464\n",
      "Epoch 24641/30000 Training Loss: 0.03674286976456642\n",
      "Epoch 24642/30000 Training Loss: 0.04508198797702789\n",
      "Epoch 24643/30000 Training Loss: 0.04784000664949417\n",
      "Epoch 24644/30000 Training Loss: 0.036671534180641174\n",
      "Epoch 24645/30000 Training Loss: 0.03514165058732033\n",
      "Epoch 24646/30000 Training Loss: 0.057223107665777206\n",
      "Epoch 24647/30000 Training Loss: 0.06147291511297226\n",
      "Epoch 24648/30000 Training Loss: 0.041962169110774994\n",
      "Epoch 24649/30000 Training Loss: 0.05644594505429268\n",
      "Epoch 24650/30000 Training Loss: 0.03589712828397751\n",
      "Epoch 24651/30000 Training Loss: 0.052105218172073364\n",
      "Epoch 24652/30000 Training Loss: 0.05129930377006531\n",
      "Epoch 24653/30000 Training Loss: 0.04954633116722107\n",
      "Epoch 24654/30000 Training Loss: 0.03843456506729126\n",
      "Epoch 24655/30000 Training Loss: 0.04937748610973358\n",
      "Epoch 24656/30000 Training Loss: 0.054834164679050446\n",
      "Epoch 24657/30000 Training Loss: 0.03802463412284851\n",
      "Epoch 24658/30000 Training Loss: 0.042472291737794876\n",
      "Epoch 24659/30000 Training Loss: 0.03507146239280701\n",
      "Epoch 24660/30000 Training Loss: 0.051330506801605225\n",
      "Epoch 24661/30000 Training Loss: 0.04960472881793976\n",
      "Epoch 24662/30000 Training Loss: 0.04514371603727341\n",
      "Epoch 24663/30000 Training Loss: 0.036448366940021515\n",
      "Epoch 24664/30000 Training Loss: 0.04117682948708534\n",
      "Epoch 24665/30000 Training Loss: 0.037342678755521774\n",
      "Epoch 24666/30000 Training Loss: 0.055249907076358795\n",
      "Epoch 24667/30000 Training Loss: 0.046383559703826904\n",
      "Epoch 24668/30000 Training Loss: 0.03686341643333435\n",
      "Epoch 24669/30000 Training Loss: 0.05049983784556389\n",
      "Epoch 24670/30000 Training Loss: 0.05099590867757797\n",
      "Epoch 24671/30000 Training Loss: 0.05458566173911095\n",
      "Epoch 24672/30000 Training Loss: 0.03683672845363617\n",
      "Epoch 24673/30000 Training Loss: 0.04280497506260872\n",
      "Epoch 24674/30000 Training Loss: 0.06331470608711243\n",
      "Epoch 24675/30000 Training Loss: 0.04599630832672119\n",
      "Epoch 24676/30000 Training Loss: 0.062315214425325394\n",
      "Epoch 24677/30000 Training Loss: 0.04618862643837929\n",
      "Epoch 24678/30000 Training Loss: 0.04534939303994179\n",
      "Epoch 24679/30000 Training Loss: 0.04811696708202362\n",
      "Epoch 24680/30000 Training Loss: 0.040275875478982925\n",
      "Epoch 24681/30000 Training Loss: 0.040985818952322006\n",
      "Epoch 24682/30000 Training Loss: 0.04254700988531113\n",
      "Epoch 24683/30000 Training Loss: 0.05245514214038849\n",
      "Epoch 24684/30000 Training Loss: 0.031012194231152534\n",
      "Epoch 24685/30000 Training Loss: 0.0326557531952858\n",
      "Epoch 24686/30000 Training Loss: 0.04923805221915245\n",
      "Epoch 24687/30000 Training Loss: 0.0325792096555233\n",
      "Epoch 24688/30000 Training Loss: 0.03996627777814865\n",
      "Epoch 24689/30000 Training Loss: 0.044900842010974884\n",
      "Epoch 24690/30000 Training Loss: 0.04879456013441086\n",
      "Epoch 24691/30000 Training Loss: 0.041195809841156006\n",
      "Epoch 24692/30000 Training Loss: 0.054861292243003845\n",
      "Epoch 24693/30000 Training Loss: 0.027887340635061264\n",
      "Epoch 24694/30000 Training Loss: 0.035680823028087616\n",
      "Epoch 24695/30000 Training Loss: 0.04993950575590134\n",
      "Epoch 24696/30000 Training Loss: 0.060936760157346725\n",
      "Epoch 24697/30000 Training Loss: 0.04021143913269043\n",
      "Epoch 24698/30000 Training Loss: 0.04489422217011452\n",
      "Epoch 24699/30000 Training Loss: 0.04656821861863136\n",
      "Epoch 24700/30000 Training Loss: 0.04555632174015045\n",
      "Epoch 24700/30000 Validation Loss: 0.057330258190631866\n",
      "Epoch 24701/30000 Training Loss: 0.04306892305612564\n",
      "Epoch 24702/30000 Training Loss: 0.047826964408159256\n",
      "Epoch 24703/30000 Training Loss: 0.038884956389665604\n",
      "Epoch 24704/30000 Training Loss: 0.041818831115961075\n",
      "Epoch 24705/30000 Training Loss: 0.055322349071502686\n",
      "Epoch 24706/30000 Training Loss: 0.025054173544049263\n",
      "Epoch 24707/30000 Training Loss: 0.03936335816979408\n",
      "Epoch 24708/30000 Training Loss: 0.05107240378856659\n",
      "Epoch 24709/30000 Training Loss: 0.05437085032463074\n",
      "Epoch 24710/30000 Training Loss: 0.0382552444934845\n",
      "Epoch 24711/30000 Training Loss: 0.04630805552005768\n",
      "Epoch 24712/30000 Training Loss: 0.040732886642217636\n",
      "Epoch 24713/30000 Training Loss: 0.03927300125360489\n",
      "Epoch 24714/30000 Training Loss: 0.028781138360500336\n",
      "Epoch 24715/30000 Training Loss: 0.044790852814912796\n",
      "Epoch 24716/30000 Training Loss: 0.04901514947414398\n",
      "Epoch 24717/30000 Training Loss: 0.03772075101733208\n",
      "Epoch 24718/30000 Training Loss: 0.03681861609220505\n",
      "Epoch 24719/30000 Training Loss: 0.03931298851966858\n",
      "Epoch 24720/30000 Training Loss: 0.04215437173843384\n",
      "Epoch 24721/30000 Training Loss: 0.044450413435697556\n",
      "Epoch 24722/30000 Training Loss: 0.04709158092737198\n",
      "Epoch 24723/30000 Training Loss: 0.04262091964483261\n",
      "Epoch 24724/30000 Training Loss: 0.034727297723293304\n",
      "Epoch 24725/30000 Training Loss: 0.050940610468387604\n",
      "Epoch 24726/30000 Training Loss: 0.0444394014775753\n",
      "Epoch 24727/30000 Training Loss: 0.052758052945137024\n",
      "Epoch 24728/30000 Training Loss: 0.04475446045398712\n",
      "Epoch 24729/30000 Training Loss: 0.06761742383241653\n",
      "Epoch 24730/30000 Training Loss: 0.050881240516901016\n",
      "Epoch 24731/30000 Training Loss: 0.039482809603214264\n",
      "Epoch 24732/30000 Training Loss: 0.05459653586149216\n",
      "Epoch 24733/30000 Training Loss: 0.04750583693385124\n",
      "Epoch 24734/30000 Training Loss: 0.038701776415109634\n",
      "Epoch 24735/30000 Training Loss: 0.037820544093847275\n",
      "Epoch 24736/30000 Training Loss: 0.03432045504450798\n",
      "Epoch 24737/30000 Training Loss: 0.042870115488767624\n",
      "Epoch 24738/30000 Training Loss: 0.04114662855863571\n",
      "Epoch 24739/30000 Training Loss: 0.030937526375055313\n",
      "Epoch 24740/30000 Training Loss: 0.036869220435619354\n",
      "Epoch 24741/30000 Training Loss: 0.06008121371269226\n",
      "Epoch 24742/30000 Training Loss: 0.04894419386982918\n",
      "Epoch 24743/30000 Training Loss: 0.04632529616355896\n",
      "Epoch 24744/30000 Training Loss: 0.04221730679273605\n",
      "Epoch 24745/30000 Training Loss: 0.0370837040245533\n",
      "Epoch 24746/30000 Training Loss: 0.04325784742832184\n",
      "Epoch 24747/30000 Training Loss: 0.0381145253777504\n",
      "Epoch 24748/30000 Training Loss: 0.04429195448756218\n",
      "Epoch 24749/30000 Training Loss: 0.05115019530057907\n",
      "Epoch 24750/30000 Training Loss: 0.06042253226041794\n",
      "Epoch 24751/30000 Training Loss: 0.0477345734834671\n",
      "Epoch 24752/30000 Training Loss: 0.05291164666414261\n",
      "Epoch 24753/30000 Training Loss: 0.041995707899332047\n",
      "Epoch 24754/30000 Training Loss: 0.03935302048921585\n",
      "Epoch 24755/30000 Training Loss: 0.057533979415893555\n",
      "Epoch 24756/30000 Training Loss: 0.029167667031288147\n",
      "Epoch 24757/30000 Training Loss: 0.057565078139305115\n",
      "Epoch 24758/30000 Training Loss: 0.0385887585580349\n",
      "Epoch 24759/30000 Training Loss: 0.03817128762602806\n",
      "Epoch 24760/30000 Training Loss: 0.04455990716814995\n",
      "Epoch 24761/30000 Training Loss: 0.04370170831680298\n",
      "Epoch 24762/30000 Training Loss: 0.04947000369429588\n",
      "Epoch 24763/30000 Training Loss: 0.04127311334013939\n",
      "Epoch 24764/30000 Training Loss: 0.04260006546974182\n",
      "Epoch 24765/30000 Training Loss: 0.04532727599143982\n",
      "Epoch 24766/30000 Training Loss: 0.05116405338048935\n",
      "Epoch 24767/30000 Training Loss: 0.04825253784656525\n",
      "Epoch 24768/30000 Training Loss: 0.03428516164422035\n",
      "Epoch 24769/30000 Training Loss: 0.08711516857147217\n",
      "Epoch 24770/30000 Training Loss: 0.045548614114522934\n",
      "Epoch 24771/30000 Training Loss: 0.05450838804244995\n",
      "Epoch 24772/30000 Training Loss: 0.05254007503390312\n",
      "Epoch 24773/30000 Training Loss: 0.06467229872941971\n",
      "Epoch 24774/30000 Training Loss: 0.05402984097599983\n",
      "Epoch 24775/30000 Training Loss: 0.04957583174109459\n",
      "Epoch 24776/30000 Training Loss: 0.043373070657253265\n",
      "Epoch 24777/30000 Training Loss: 0.04721811041235924\n",
      "Epoch 24778/30000 Training Loss: 0.04898088797926903\n",
      "Epoch 24779/30000 Training Loss: 0.047153960913419724\n",
      "Epoch 24780/30000 Training Loss: 0.05998563766479492\n",
      "Epoch 24781/30000 Training Loss: 0.035463642328977585\n",
      "Epoch 24782/30000 Training Loss: 0.051176972687244415\n",
      "Epoch 24783/30000 Training Loss: 0.04384496435523033\n",
      "Epoch 24784/30000 Training Loss: 0.040331728756427765\n",
      "Epoch 24785/30000 Training Loss: 0.05099506676197052\n",
      "Epoch 24786/30000 Training Loss: 0.05767950415611267\n",
      "Epoch 24787/30000 Training Loss: 0.058301933109760284\n",
      "Epoch 24788/30000 Training Loss: 0.05234624072909355\n",
      "Epoch 24789/30000 Training Loss: 0.049799151718616486\n",
      "Epoch 24790/30000 Training Loss: 0.05976640805602074\n",
      "Epoch 24791/30000 Training Loss: 0.043182723224163055\n",
      "Epoch 24792/30000 Training Loss: 0.049568112939596176\n",
      "Epoch 24793/30000 Training Loss: 0.06275555491447449\n",
      "Epoch 24794/30000 Training Loss: 0.046729255467653275\n",
      "Epoch 24795/30000 Training Loss: 0.04130548611283302\n",
      "Epoch 24796/30000 Training Loss: 0.05692877620458603\n",
      "Epoch 24797/30000 Training Loss: 0.03824177011847496\n",
      "Epoch 24798/30000 Training Loss: 0.04461236670613289\n",
      "Epoch 24799/30000 Training Loss: 0.05198577046394348\n",
      "Epoch 24800/30000 Training Loss: 0.04392784461379051\n",
      "Epoch 24800/30000 Validation Loss: 0.049477141350507736\n",
      "Epoch 24801/30000 Training Loss: 0.04629917070269585\n",
      "Epoch 24802/30000 Training Loss: 0.04003596305847168\n",
      "Epoch 24803/30000 Training Loss: 0.055556245148181915\n",
      "Epoch 24804/30000 Training Loss: 0.03817465901374817\n",
      "Epoch 24805/30000 Training Loss: 0.04802606627345085\n",
      "Epoch 24806/30000 Training Loss: 0.05741436034440994\n",
      "Epoch 24807/30000 Training Loss: 0.04060620069503784\n",
      "Epoch 24808/30000 Training Loss: 0.05018553510308266\n",
      "Epoch 24809/30000 Training Loss: 0.049263548105955124\n",
      "Epoch 24810/30000 Training Loss: 0.04614098742604256\n",
      "Epoch 24811/30000 Training Loss: 0.04761763662099838\n",
      "Epoch 24812/30000 Training Loss: 0.049740154296159744\n",
      "Epoch 24813/30000 Training Loss: 0.06578908860683441\n",
      "Epoch 24814/30000 Training Loss: 0.03584745153784752\n",
      "Epoch 24815/30000 Training Loss: 0.059364158660173416\n",
      "Epoch 24816/30000 Training Loss: 0.04709891974925995\n",
      "Epoch 24817/30000 Training Loss: 0.03997412696480751\n",
      "Epoch 24818/30000 Training Loss: 0.0373421385884285\n",
      "Epoch 24819/30000 Training Loss: 0.03703482821583748\n",
      "Epoch 24820/30000 Training Loss: 0.03244291990995407\n",
      "Epoch 24821/30000 Training Loss: 0.04236758127808571\n",
      "Epoch 24822/30000 Training Loss: 0.04493730515241623\n",
      "Epoch 24823/30000 Training Loss: 0.03935067728161812\n",
      "Epoch 24824/30000 Training Loss: 0.052902720868587494\n",
      "Epoch 24825/30000 Training Loss: 0.032080575823783875\n",
      "Epoch 24826/30000 Training Loss: 0.05026370659470558\n",
      "Epoch 24827/30000 Training Loss: 0.045035772025585175\n",
      "Epoch 24828/30000 Training Loss: 0.0434410460293293\n",
      "Epoch 24829/30000 Training Loss: 0.06461432576179504\n",
      "Epoch 24830/30000 Training Loss: 0.04188346117734909\n",
      "Epoch 24831/30000 Training Loss: 0.03537910059094429\n",
      "Epoch 24832/30000 Training Loss: 0.05349002033472061\n",
      "Epoch 24833/30000 Training Loss: 0.05186430737376213\n",
      "Epoch 24834/30000 Training Loss: 0.048153478652238846\n",
      "Epoch 24835/30000 Training Loss: 0.05642114207148552\n",
      "Epoch 24836/30000 Training Loss: 0.035208623856306076\n",
      "Epoch 24837/30000 Training Loss: 0.058914635330438614\n",
      "Epoch 24838/30000 Training Loss: 0.051877476274967194\n",
      "Epoch 24839/30000 Training Loss: 0.035536203533411026\n",
      "Epoch 24840/30000 Training Loss: 0.03918098658323288\n",
      "Epoch 24841/30000 Training Loss: 0.0450148768723011\n",
      "Epoch 24842/30000 Training Loss: 0.03506704792380333\n",
      "Epoch 24843/30000 Training Loss: 0.05246056243777275\n",
      "Epoch 24844/30000 Training Loss: 0.05046936497092247\n",
      "Epoch 24845/30000 Training Loss: 0.03718322515487671\n",
      "Epoch 24846/30000 Training Loss: 0.05658562481403351\n",
      "Epoch 24847/30000 Training Loss: 0.046420685946941376\n",
      "Epoch 24848/30000 Training Loss: 0.05217690393328667\n",
      "Epoch 24849/30000 Training Loss: 0.051007628440856934\n",
      "Epoch 24850/30000 Training Loss: 0.0492732971906662\n",
      "Epoch 24851/30000 Training Loss: 0.04301534593105316\n",
      "Epoch 24852/30000 Training Loss: 0.042779259383678436\n",
      "Epoch 24853/30000 Training Loss: 0.052502766251564026\n",
      "Epoch 24854/30000 Training Loss: 0.05589871108531952\n",
      "Epoch 24855/30000 Training Loss: 0.04868990555405617\n",
      "Epoch 24856/30000 Training Loss: 0.05828390270471573\n",
      "Epoch 24857/30000 Training Loss: 0.035925257951021194\n",
      "Epoch 24858/30000 Training Loss: 0.03381824493408203\n",
      "Epoch 24859/30000 Training Loss: 0.04909949749708176\n",
      "Epoch 24860/30000 Training Loss: 0.04419904947280884\n",
      "Epoch 24861/30000 Training Loss: 0.05454624071717262\n",
      "Epoch 24862/30000 Training Loss: 0.026776989921927452\n",
      "Epoch 24863/30000 Training Loss: 0.04706276208162308\n",
      "Epoch 24864/30000 Training Loss: 0.06493712961673737\n",
      "Epoch 24865/30000 Training Loss: 0.036247752606868744\n",
      "Epoch 24866/30000 Training Loss: 0.05117594450712204\n",
      "Epoch 24867/30000 Training Loss: 0.04488861933350563\n",
      "Epoch 24868/30000 Training Loss: 0.04399671033024788\n",
      "Epoch 24869/30000 Training Loss: 0.056142475455999374\n",
      "Epoch 24870/30000 Training Loss: 0.03194975107908249\n",
      "Epoch 24871/30000 Training Loss: 0.034527335315942764\n",
      "Epoch 24872/30000 Training Loss: 0.04903893172740936\n",
      "Epoch 24873/30000 Training Loss: 0.06695577502250671\n",
      "Epoch 24874/30000 Training Loss: 0.048019371926784515\n",
      "Epoch 24875/30000 Training Loss: 0.04702995344996452\n",
      "Epoch 24876/30000 Training Loss: 0.05905003845691681\n",
      "Epoch 24877/30000 Training Loss: 0.033104829490184784\n",
      "Epoch 24878/30000 Training Loss: 0.04801487922668457\n",
      "Epoch 24879/30000 Training Loss: 0.029734570533037186\n",
      "Epoch 24880/30000 Training Loss: 0.04544031247496605\n",
      "Epoch 24881/30000 Training Loss: 0.044697780162096024\n",
      "Epoch 24882/30000 Training Loss: 0.05397052690386772\n",
      "Epoch 24883/30000 Training Loss: 0.044973406940698624\n",
      "Epoch 24884/30000 Training Loss: 0.03307287395000458\n",
      "Epoch 24885/30000 Training Loss: 0.029506543651223183\n",
      "Epoch 24886/30000 Training Loss: 0.05835933983325958\n",
      "Epoch 24887/30000 Training Loss: 0.04948452115058899\n",
      "Epoch 24888/30000 Training Loss: 0.043772824108600616\n",
      "Epoch 24889/30000 Training Loss: 0.04770638793706894\n",
      "Epoch 24890/30000 Training Loss: 0.05705869197845459\n",
      "Epoch 24891/30000 Training Loss: 0.048918720334768295\n",
      "Epoch 24892/30000 Training Loss: 0.042178865522146225\n",
      "Epoch 24893/30000 Training Loss: 0.04158416762948036\n",
      "Epoch 24894/30000 Training Loss: 0.044302716851234436\n",
      "Epoch 24895/30000 Training Loss: 0.0548563152551651\n",
      "Epoch 24896/30000 Training Loss: 0.050527311861515045\n",
      "Epoch 24897/30000 Training Loss: 0.055445849895477295\n",
      "Epoch 24898/30000 Training Loss: 0.03483185917139053\n",
      "Epoch 24899/30000 Training Loss: 0.04804122447967529\n",
      "Epoch 24900/30000 Training Loss: 0.04516051337122917\n",
      "Epoch 24900/30000 Validation Loss: 0.033752549439668655\n",
      "Epoch 24901/30000 Training Loss: 0.04088662192225456\n",
      "Epoch 24902/30000 Training Loss: 0.04966041445732117\n",
      "Epoch 24903/30000 Training Loss: 0.03734468296170235\n",
      "Epoch 24904/30000 Training Loss: 0.03783562779426575\n",
      "Epoch 24905/30000 Training Loss: 0.05379973351955414\n",
      "Epoch 24906/30000 Training Loss: 0.0321207195520401\n",
      "Epoch 24907/30000 Training Loss: 0.03397658094763756\n",
      "Epoch 24908/30000 Training Loss: 0.03683982044458389\n",
      "Epoch 24909/30000 Training Loss: 0.04200905188918114\n",
      "Epoch 24910/30000 Training Loss: 0.06105274334549904\n",
      "Epoch 24911/30000 Training Loss: 0.038277510553598404\n",
      "Epoch 24912/30000 Training Loss: 0.04971490055322647\n",
      "Epoch 24913/30000 Training Loss: 0.04745696112513542\n",
      "Epoch 24914/30000 Training Loss: 0.04122386500239372\n",
      "Epoch 24915/30000 Training Loss: 0.04218224436044693\n",
      "Epoch 24916/30000 Training Loss: 0.05072185397148132\n",
      "Epoch 24917/30000 Training Loss: 0.04615117609500885\n",
      "Epoch 24918/30000 Training Loss: 0.039268940687179565\n",
      "Epoch 24919/30000 Training Loss: 0.05846903473138809\n",
      "Epoch 24920/30000 Training Loss: 0.03431426361203194\n",
      "Epoch 24921/30000 Training Loss: 0.05336788669228554\n",
      "Epoch 24922/30000 Training Loss: 0.04757973924279213\n",
      "Epoch 24923/30000 Training Loss: 0.05753031373023987\n",
      "Epoch 24924/30000 Training Loss: 0.058697693049907684\n",
      "Epoch 24925/30000 Training Loss: 0.05273771286010742\n",
      "Epoch 24926/30000 Training Loss: 0.07096529006958008\n",
      "Epoch 24927/30000 Training Loss: 0.06569602340459824\n",
      "Epoch 24928/30000 Training Loss: 0.04069076478481293\n",
      "Epoch 24929/30000 Training Loss: 0.047136399894952774\n",
      "Epoch 24930/30000 Training Loss: 0.0482029989361763\n",
      "Epoch 24931/30000 Training Loss: 0.05534256249666214\n",
      "Epoch 24932/30000 Training Loss: 0.05421240255236626\n",
      "Epoch 24933/30000 Training Loss: 0.0488550066947937\n",
      "Epoch 24934/30000 Training Loss: 0.04168267175555229\n",
      "Epoch 24935/30000 Training Loss: 0.028582563623785973\n",
      "Epoch 24936/30000 Training Loss: 0.027707474306225777\n",
      "Epoch 24937/30000 Training Loss: 0.047100238502025604\n",
      "Epoch 24938/30000 Training Loss: 0.051769863814115524\n",
      "Epoch 24939/30000 Training Loss: 0.052271388471126556\n",
      "Epoch 24940/30000 Training Loss: 0.03638051077723503\n",
      "Epoch 24941/30000 Training Loss: 0.04261188954114914\n",
      "Epoch 24942/30000 Training Loss: 0.053639549762010574\n",
      "Epoch 24943/30000 Training Loss: 0.05834779143333435\n",
      "Epoch 24944/30000 Training Loss: 0.047408197075128555\n",
      "Epoch 24945/30000 Training Loss: 0.0458756685256958\n",
      "Epoch 24946/30000 Training Loss: 0.03139312192797661\n",
      "Epoch 24947/30000 Training Loss: 0.05867347866296768\n",
      "Epoch 24948/30000 Training Loss: 0.04473915323615074\n",
      "Epoch 24949/30000 Training Loss: 0.038513485342264175\n",
      "Epoch 24950/30000 Training Loss: 0.04952008277177811\n",
      "Epoch 24951/30000 Training Loss: 0.04105532541871071\n",
      "Epoch 24952/30000 Training Loss: 0.04671791195869446\n",
      "Epoch 24953/30000 Training Loss: 0.05468423292040825\n",
      "Epoch 24954/30000 Training Loss: 0.0424174927175045\n",
      "Epoch 24955/30000 Training Loss: 0.06074269860982895\n",
      "Epoch 24956/30000 Training Loss: 0.044417187571525574\n",
      "Epoch 24957/30000 Training Loss: 0.05199017375707626\n",
      "Epoch 24958/30000 Training Loss: 0.03798822686076164\n",
      "Epoch 24959/30000 Training Loss: 0.04345561936497688\n",
      "Epoch 24960/30000 Training Loss: 0.03499894216656685\n",
      "Epoch 24961/30000 Training Loss: 0.054463885724544525\n",
      "Epoch 24962/30000 Training Loss: 0.07013877481222153\n",
      "Epoch 24963/30000 Training Loss: 0.04436817392706871\n",
      "Epoch 24964/30000 Training Loss: 0.0613354817032814\n",
      "Epoch 24965/30000 Training Loss: 0.033598631620407104\n",
      "Epoch 24966/30000 Training Loss: 0.043957240879535675\n",
      "Epoch 24967/30000 Training Loss: 0.03745457902550697\n",
      "Epoch 24968/30000 Training Loss: 0.061272598803043365\n",
      "Epoch 24969/30000 Training Loss: 0.03913160040974617\n",
      "Epoch 24970/30000 Training Loss: 0.03967595845460892\n",
      "Epoch 24971/30000 Training Loss: 0.046200890094041824\n",
      "Epoch 24972/30000 Training Loss: 0.04314962774515152\n",
      "Epoch 24973/30000 Training Loss: 0.04940113052725792\n",
      "Epoch 24974/30000 Training Loss: 0.05409369245171547\n",
      "Epoch 24975/30000 Training Loss: 0.07002900540828705\n",
      "Epoch 24976/30000 Training Loss: 0.033098626881837845\n",
      "Epoch 24977/30000 Training Loss: 0.03930332139134407\n",
      "Epoch 24978/30000 Training Loss: 0.03339487686753273\n",
      "Epoch 24979/30000 Training Loss: 0.03662702441215515\n",
      "Epoch 24980/30000 Training Loss: 0.05364643409848213\n",
      "Epoch 24981/30000 Training Loss: 0.04569059982895851\n",
      "Epoch 24982/30000 Training Loss: 0.05685950070619583\n",
      "Epoch 24983/30000 Training Loss: 0.039887603372335434\n",
      "Epoch 24984/30000 Training Loss: 0.03510071337223053\n",
      "Epoch 24985/30000 Training Loss: 0.04004218801856041\n",
      "Epoch 24986/30000 Training Loss: 0.04665141552686691\n",
      "Epoch 24987/30000 Training Loss: 0.0413045734167099\n",
      "Epoch 24988/30000 Training Loss: 0.04565851017832756\n",
      "Epoch 24989/30000 Training Loss: 0.03298843652009964\n",
      "Epoch 24990/30000 Training Loss: 0.06403927505016327\n",
      "Epoch 24991/30000 Training Loss: 0.053040485829114914\n",
      "Epoch 24992/30000 Training Loss: 0.05408557504415512\n",
      "Epoch 24993/30000 Training Loss: 0.044420816004276276\n",
      "Epoch 24994/30000 Training Loss: 0.04719696193933487\n",
      "Epoch 24995/30000 Training Loss: 0.047753844410181046\n",
      "Epoch 24996/30000 Training Loss: 0.038448501378297806\n",
      "Epoch 24997/30000 Training Loss: 0.05048588663339615\n",
      "Epoch 24998/30000 Training Loss: 0.05245375633239746\n",
      "Epoch 24999/30000 Training Loss: 0.05175201967358589\n",
      "Epoch 25000/30000 Training Loss: 0.05019587650895119\n",
      "Epoch 25000/30000 Validation Loss: 0.044319380074739456\n",
      "Epoch 25001/30000 Training Loss: 0.04534425958991051\n",
      "Epoch 25002/30000 Training Loss: 0.06999348849058151\n",
      "Epoch 25003/30000 Training Loss: 0.052534069865942\n",
      "Epoch 25004/30000 Training Loss: 0.030183982104063034\n",
      "Epoch 25005/30000 Training Loss: 0.04243643954396248\n",
      "Epoch 25006/30000 Training Loss: 0.044945910573005676\n",
      "Epoch 25007/30000 Training Loss: 0.06294171512126923\n",
      "Epoch 25008/30000 Training Loss: 0.04425087571144104\n",
      "Epoch 25009/30000 Training Loss: 0.05727069079875946\n",
      "Epoch 25010/30000 Training Loss: 0.0543086901307106\n",
      "Epoch 25011/30000 Training Loss: 0.04448557272553444\n",
      "Epoch 25012/30000 Training Loss: 0.05110633000731468\n",
      "Epoch 25013/30000 Training Loss: 0.03754689544439316\n",
      "Epoch 25014/30000 Training Loss: 0.033304613083601\n",
      "Epoch 25015/30000 Training Loss: 0.04772719368338585\n",
      "Epoch 25016/30000 Training Loss: 0.04188856855034828\n",
      "Epoch 25017/30000 Training Loss: 0.02988104149699211\n",
      "Epoch 25018/30000 Training Loss: 0.04757970944046974\n",
      "Epoch 25019/30000 Training Loss: 0.034201085567474365\n",
      "Epoch 25020/30000 Training Loss: 0.05170584097504616\n",
      "Epoch 25021/30000 Training Loss: 0.04021316021680832\n",
      "Epoch 25022/30000 Training Loss: 0.04680592566728592\n",
      "Epoch 25023/30000 Training Loss: 0.0573316365480423\n",
      "Epoch 25024/30000 Training Loss: 0.04852361977100372\n",
      "Epoch 25025/30000 Training Loss: 0.04608689993619919\n",
      "Epoch 25026/30000 Training Loss: 0.04164405167102814\n",
      "Epoch 25027/30000 Training Loss: 0.031224148347973824\n",
      "Epoch 25028/30000 Training Loss: 0.05004405602812767\n",
      "Epoch 25029/30000 Training Loss: 0.03413895145058632\n",
      "Epoch 25030/30000 Training Loss: 0.051970891654491425\n",
      "Epoch 25031/30000 Training Loss: 0.05562306195497513\n",
      "Epoch 25032/30000 Training Loss: 0.04775991290807724\n",
      "Epoch 25033/30000 Training Loss: 0.05302916839718819\n",
      "Epoch 25034/30000 Training Loss: 0.03278118744492531\n",
      "Epoch 25035/30000 Training Loss: 0.05508997663855553\n",
      "Epoch 25036/30000 Training Loss: 0.04031270369887352\n",
      "Epoch 25037/30000 Training Loss: 0.030813556164503098\n",
      "Epoch 25038/30000 Training Loss: 0.04939456284046173\n",
      "Epoch 25039/30000 Training Loss: 0.03624855726957321\n",
      "Epoch 25040/30000 Training Loss: 0.04298867657780647\n",
      "Epoch 25041/30000 Training Loss: 0.044564902782440186\n",
      "Epoch 25042/30000 Training Loss: 0.050511352717876434\n",
      "Epoch 25043/30000 Training Loss: 0.04796066880226135\n",
      "Epoch 25044/30000 Training Loss: 0.058090806007385254\n",
      "Epoch 25045/30000 Training Loss: 0.04278695210814476\n",
      "Epoch 25046/30000 Training Loss: 0.044830042868852615\n",
      "Epoch 25047/30000 Training Loss: 0.055289868265390396\n",
      "Epoch 25048/30000 Training Loss: 0.041499510407447815\n",
      "Epoch 25049/30000 Training Loss: 0.045473068952560425\n",
      "Epoch 25050/30000 Training Loss: 0.05892738699913025\n",
      "Epoch 25051/30000 Training Loss: 0.04796901345252991\n",
      "Epoch 25052/30000 Training Loss: 0.04028042405843735\n",
      "Epoch 25053/30000 Training Loss: 0.04278630390763283\n",
      "Epoch 25054/30000 Training Loss: 0.043068259954452515\n",
      "Epoch 25055/30000 Training Loss: 0.04629792273044586\n",
      "Epoch 25056/30000 Training Loss: 0.043232262134552\n",
      "Epoch 25057/30000 Training Loss: 0.04444798827171326\n",
      "Epoch 25058/30000 Training Loss: 0.033681273460388184\n",
      "Epoch 25059/30000 Training Loss: 0.038023196160793304\n",
      "Epoch 25060/30000 Training Loss: 0.06473897397518158\n",
      "Epoch 25061/30000 Training Loss: 0.04364204779267311\n",
      "Epoch 25062/30000 Training Loss: 0.06750451028347015\n",
      "Epoch 25063/30000 Training Loss: 0.04246436059474945\n",
      "Epoch 25064/30000 Training Loss: 0.04889586940407753\n",
      "Epoch 25065/30000 Training Loss: 0.04067607223987579\n",
      "Epoch 25066/30000 Training Loss: 0.03525683656334877\n",
      "Epoch 25067/30000 Training Loss: 0.04675423353910446\n",
      "Epoch 25068/30000 Training Loss: 0.035391420125961304\n",
      "Epoch 25069/30000 Training Loss: 0.05088971555233002\n",
      "Epoch 25070/30000 Training Loss: 0.036432646214962006\n",
      "Epoch 25071/30000 Training Loss: 0.04667336866259575\n",
      "Epoch 25072/30000 Training Loss: 0.0453064888715744\n",
      "Epoch 25073/30000 Training Loss: 0.04651186615228653\n",
      "Epoch 25074/30000 Training Loss: 0.04241131991147995\n",
      "Epoch 25075/30000 Training Loss: 0.04035532847046852\n",
      "Epoch 25076/30000 Training Loss: 0.04822003096342087\n",
      "Epoch 25077/30000 Training Loss: 0.05760956183075905\n",
      "Epoch 25078/30000 Training Loss: 0.03471441566944122\n",
      "Epoch 25079/30000 Training Loss: 0.05664561688899994\n",
      "Epoch 25080/30000 Training Loss: 0.03750837221741676\n",
      "Epoch 25081/30000 Training Loss: 0.05000973865389824\n",
      "Epoch 25082/30000 Training Loss: 0.05230631306767464\n",
      "Epoch 25083/30000 Training Loss: 0.035427533090114594\n",
      "Epoch 25084/30000 Training Loss: 0.0516507551074028\n",
      "Epoch 25085/30000 Training Loss: 0.037481822073459625\n",
      "Epoch 25086/30000 Training Loss: 0.04945916309952736\n",
      "Epoch 25087/30000 Training Loss: 0.045270390808582306\n",
      "Epoch 25088/30000 Training Loss: 0.044800765812397\n",
      "Epoch 25089/30000 Training Loss: 0.03993929550051689\n",
      "Epoch 25090/30000 Training Loss: 0.05110018327832222\n",
      "Epoch 25091/30000 Training Loss: 0.03499729931354523\n",
      "Epoch 25092/30000 Training Loss: 0.05202135071158409\n",
      "Epoch 25093/30000 Training Loss: 0.047117576003074646\n",
      "Epoch 25094/30000 Training Loss: 0.044521402567625046\n",
      "Epoch 25095/30000 Training Loss: 0.05080154538154602\n",
      "Epoch 25096/30000 Training Loss: 0.03869032487273216\n",
      "Epoch 25097/30000 Training Loss: 0.04696696251630783\n",
      "Epoch 25098/30000 Training Loss: 0.054936133325099945\n",
      "Epoch 25099/30000 Training Loss: 0.06219014525413513\n",
      "Epoch 25100/30000 Training Loss: 0.0463649146258831\n",
      "Epoch 25100/30000 Validation Loss: 0.03855926916003227\n",
      "Epoch 25101/30000 Training Loss: 0.05225076526403427\n",
      "Epoch 25102/30000 Training Loss: 0.03512703627347946\n",
      "Epoch 25103/30000 Training Loss: 0.04669211059808731\n",
      "Epoch 25104/30000 Training Loss: 0.04572261869907379\n",
      "Epoch 25105/30000 Training Loss: 0.042231105268001556\n",
      "Epoch 25106/30000 Training Loss: 0.05793067812919617\n",
      "Epoch 25107/30000 Training Loss: 0.05384218692779541\n",
      "Epoch 25108/30000 Training Loss: 0.04628155007958412\n",
      "Epoch 25109/30000 Training Loss: 0.051020264625549316\n",
      "Epoch 25110/30000 Training Loss: 0.03743785247206688\n",
      "Epoch 25111/30000 Training Loss: 0.04875607788562775\n",
      "Epoch 25112/30000 Training Loss: 0.04360044002532959\n",
      "Epoch 25113/30000 Training Loss: 0.047769978642463684\n",
      "Epoch 25114/30000 Training Loss: 0.04830273985862732\n",
      "Epoch 25115/30000 Training Loss: 0.05236229673027992\n",
      "Epoch 25116/30000 Training Loss: 0.05509374290704727\n",
      "Epoch 25117/30000 Training Loss: 0.05431681498885155\n",
      "Epoch 25118/30000 Training Loss: 0.042309217154979706\n",
      "Epoch 25119/30000 Training Loss: 0.04917650669813156\n",
      "Epoch 25120/30000 Training Loss: 0.05980201065540314\n",
      "Epoch 25121/30000 Training Loss: 0.03979695215821266\n",
      "Epoch 25122/30000 Training Loss: 0.04743011295795441\n",
      "Epoch 25123/30000 Training Loss: 0.046017710119485855\n",
      "Epoch 25124/30000 Training Loss: 0.04970642179250717\n",
      "Epoch 25125/30000 Training Loss: 0.030084926635026932\n",
      "Epoch 25126/30000 Training Loss: 0.05526524782180786\n",
      "Epoch 25127/30000 Training Loss: 0.05405307561159134\n",
      "Epoch 25128/30000 Training Loss: 0.04364107921719551\n",
      "Epoch 25129/30000 Training Loss: 0.04962018504738808\n",
      "Epoch 25130/30000 Training Loss: 0.03816632553935051\n",
      "Epoch 25131/30000 Training Loss: 0.05353512987494469\n",
      "Epoch 25132/30000 Training Loss: 0.04401994124054909\n",
      "Epoch 25133/30000 Training Loss: 0.0497467927634716\n",
      "Epoch 25134/30000 Training Loss: 0.05836187303066254\n",
      "Epoch 25135/30000 Training Loss: 0.043761447072029114\n",
      "Epoch 25136/30000 Training Loss: 0.04454393312335014\n",
      "Epoch 25137/30000 Training Loss: 0.040556807070970535\n",
      "Epoch 25138/30000 Training Loss: 0.06598769128322601\n",
      "Epoch 25139/30000 Training Loss: 0.053016431629657745\n",
      "Epoch 25140/30000 Training Loss: 0.037504080682992935\n",
      "Epoch 25141/30000 Training Loss: 0.05420961230993271\n",
      "Epoch 25142/30000 Training Loss: 0.05385827273130417\n",
      "Epoch 25143/30000 Training Loss: 0.05593874305486679\n",
      "Epoch 25144/30000 Training Loss: 0.03652745112776756\n",
      "Epoch 25145/30000 Training Loss: 0.04088979214429855\n",
      "Epoch 25146/30000 Training Loss: 0.05093793570995331\n",
      "Epoch 25147/30000 Training Loss: 0.04594157263636589\n",
      "Epoch 25148/30000 Training Loss: 0.03810331970453262\n",
      "Epoch 25149/30000 Training Loss: 0.05601639673113823\n",
      "Epoch 25150/30000 Training Loss: 0.047228891402482986\n",
      "Epoch 25151/30000 Training Loss: 0.049009013921022415\n",
      "Epoch 25152/30000 Training Loss: 0.04411344230175018\n",
      "Epoch 25153/30000 Training Loss: 0.04607005789875984\n",
      "Epoch 25154/30000 Training Loss: 0.03639378398656845\n",
      "Epoch 25155/30000 Training Loss: 0.050697602331638336\n",
      "Epoch 25156/30000 Training Loss: 0.039866793900728226\n",
      "Epoch 25157/30000 Training Loss: 0.04245437681674957\n",
      "Epoch 25158/30000 Training Loss: 0.043528806418180466\n",
      "Epoch 25159/30000 Training Loss: 0.03603861853480339\n",
      "Epoch 25160/30000 Training Loss: 0.029886774718761444\n",
      "Epoch 25161/30000 Training Loss: 0.04249238222837448\n",
      "Epoch 25162/30000 Training Loss: 0.050253674387931824\n",
      "Epoch 25163/30000 Training Loss: 0.05110563710331917\n",
      "Epoch 25164/30000 Training Loss: 0.041982848197221756\n",
      "Epoch 25165/30000 Training Loss: 0.053285881876945496\n",
      "Epoch 25166/30000 Training Loss: 0.0360010527074337\n",
      "Epoch 25167/30000 Training Loss: 0.04100187122821808\n",
      "Epoch 25168/30000 Training Loss: 0.045850615948438644\n",
      "Epoch 25169/30000 Training Loss: 0.04162095487117767\n",
      "Epoch 25170/30000 Training Loss: 0.05004839226603508\n",
      "Epoch 25171/30000 Training Loss: 0.05419410765171051\n",
      "Epoch 25172/30000 Training Loss: 0.04510140046477318\n",
      "Epoch 25173/30000 Training Loss: 0.048769447952508926\n",
      "Epoch 25174/30000 Training Loss: 0.03413648158311844\n",
      "Epoch 25175/30000 Training Loss: 0.04569078981876373\n",
      "Epoch 25176/30000 Training Loss: 0.04599932208657265\n",
      "Epoch 25177/30000 Training Loss: 0.05270330607891083\n",
      "Epoch 25178/30000 Training Loss: 0.03409084305167198\n",
      "Epoch 25179/30000 Training Loss: 0.04200253635644913\n",
      "Epoch 25180/30000 Training Loss: 0.0427151694893837\n",
      "Epoch 25181/30000 Training Loss: 0.04530815780162811\n",
      "Epoch 25182/30000 Training Loss: 0.03906724974513054\n",
      "Epoch 25183/30000 Training Loss: 0.06534400582313538\n",
      "Epoch 25184/30000 Training Loss: 0.04579091817140579\n",
      "Epoch 25185/30000 Training Loss: 0.03378761187195778\n",
      "Epoch 25186/30000 Training Loss: 0.05329056456685066\n",
      "Epoch 25187/30000 Training Loss: 0.03973831236362457\n",
      "Epoch 25188/30000 Training Loss: 0.05870980769395828\n",
      "Epoch 25189/30000 Training Loss: 0.042808033525943756\n",
      "Epoch 25190/30000 Training Loss: 0.05822727084159851\n",
      "Epoch 25191/30000 Training Loss: 0.04810580611228943\n",
      "Epoch 25192/30000 Training Loss: 0.04578087478876114\n",
      "Epoch 25193/30000 Training Loss: 0.040473803877830505\n",
      "Epoch 25194/30000 Training Loss: 0.06546193361282349\n",
      "Epoch 25195/30000 Training Loss: 0.04234044998884201\n",
      "Epoch 25196/30000 Training Loss: 0.044196344912052155\n",
      "Epoch 25197/30000 Training Loss: 0.05239933729171753\n",
      "Epoch 25198/30000 Training Loss: 0.03692582994699478\n",
      "Epoch 25199/30000 Training Loss: 0.0636521503329277\n",
      "Epoch 25200/30000 Training Loss: 0.050907500088214874\n",
      "Epoch 25200/30000 Validation Loss: 0.04647844284772873\n",
      "Epoch 25201/30000 Training Loss: 0.04364791885018349\n",
      "Epoch 25202/30000 Training Loss: 0.05963601917028427\n",
      "Epoch 25203/30000 Training Loss: 0.029706455767154694\n",
      "Epoch 25204/30000 Training Loss: 0.04825279861688614\n",
      "Epoch 25205/30000 Training Loss: 0.054513975977897644\n",
      "Epoch 25206/30000 Training Loss: 0.04482969269156456\n",
      "Epoch 25207/30000 Training Loss: 0.05046861618757248\n",
      "Epoch 25208/30000 Training Loss: 0.04352409020066261\n",
      "Epoch 25209/30000 Training Loss: 0.043651655316352844\n",
      "Epoch 25210/30000 Training Loss: 0.03436668589711189\n",
      "Epoch 25211/30000 Training Loss: 0.06869620084762573\n",
      "Epoch 25212/30000 Training Loss: 0.058388613164424896\n",
      "Epoch 25213/30000 Training Loss: 0.04126483201980591\n",
      "Epoch 25214/30000 Training Loss: 0.034701064229011536\n",
      "Epoch 25215/30000 Training Loss: 0.0435614176094532\n",
      "Epoch 25216/30000 Training Loss: 0.0379188135266304\n",
      "Epoch 25217/30000 Training Loss: 0.048929713666439056\n",
      "Epoch 25218/30000 Training Loss: 0.043651364743709564\n",
      "Epoch 25219/30000 Training Loss: 0.03363155201077461\n",
      "Epoch 25220/30000 Training Loss: 0.05112064629793167\n",
      "Epoch 25221/30000 Training Loss: 0.0355733297765255\n",
      "Epoch 25222/30000 Training Loss: 0.038270868360996246\n",
      "Epoch 25223/30000 Training Loss: 0.06453436613082886\n",
      "Epoch 25224/30000 Training Loss: 0.04927302896976471\n",
      "Epoch 25225/30000 Training Loss: 0.044406160712242126\n",
      "Epoch 25226/30000 Training Loss: 0.041970394551754\n",
      "Epoch 25227/30000 Training Loss: 0.04591657966375351\n",
      "Epoch 25228/30000 Training Loss: 0.04117125645279884\n",
      "Epoch 25229/30000 Training Loss: 0.04433312267065048\n",
      "Epoch 25230/30000 Training Loss: 0.03166796267032623\n",
      "Epoch 25231/30000 Training Loss: 0.05332179367542267\n",
      "Epoch 25232/30000 Training Loss: 0.04475032910704613\n",
      "Epoch 25233/30000 Training Loss: 0.06020461022853851\n",
      "Epoch 25234/30000 Training Loss: 0.0387737937271595\n",
      "Epoch 25235/30000 Training Loss: 0.045801371335983276\n",
      "Epoch 25236/30000 Training Loss: 0.0446806438267231\n",
      "Epoch 25237/30000 Training Loss: 0.05111857131123543\n",
      "Epoch 25238/30000 Training Loss: 0.049440521746873856\n",
      "Epoch 25239/30000 Training Loss: 0.05617253854870796\n",
      "Epoch 25240/30000 Training Loss: 0.06127806007862091\n",
      "Epoch 25241/30000 Training Loss: 0.04014162719249725\n",
      "Epoch 25242/30000 Training Loss: 0.047743070870637894\n",
      "Epoch 25243/30000 Training Loss: 0.03706550970673561\n",
      "Epoch 25244/30000 Training Loss: 0.04402962327003479\n",
      "Epoch 25245/30000 Training Loss: 0.05192205682396889\n",
      "Epoch 25246/30000 Training Loss: 0.052165526896715164\n",
      "Epoch 25247/30000 Training Loss: 0.04008563235402107\n",
      "Epoch 25248/30000 Training Loss: 0.04868762940168381\n",
      "Epoch 25249/30000 Training Loss: 0.04220912232995033\n",
      "Epoch 25250/30000 Training Loss: 0.038673754781484604\n",
      "Epoch 25251/30000 Training Loss: 0.038967035710811615\n",
      "Epoch 25252/30000 Training Loss: 0.033915575593709946\n",
      "Epoch 25253/30000 Training Loss: 0.04000790789723396\n",
      "Epoch 25254/30000 Training Loss: 0.04370229318737984\n",
      "Epoch 25255/30000 Training Loss: 0.0550629124045372\n",
      "Epoch 25256/30000 Training Loss: 0.036758556962013245\n",
      "Epoch 25257/30000 Training Loss: 0.042761046439409256\n",
      "Epoch 25258/30000 Training Loss: 0.04110466316342354\n",
      "Epoch 25259/30000 Training Loss: 0.052506931126117706\n",
      "Epoch 25260/30000 Training Loss: 0.04028512164950371\n",
      "Epoch 25261/30000 Training Loss: 0.05815554037690163\n",
      "Epoch 25262/30000 Training Loss: 0.043588217347860336\n",
      "Epoch 25263/30000 Training Loss: 0.053443558514118195\n",
      "Epoch 25264/30000 Training Loss: 0.041298117488622665\n",
      "Epoch 25265/30000 Training Loss: 0.05190836638212204\n",
      "Epoch 25266/30000 Training Loss: 0.05164194107055664\n",
      "Epoch 25267/30000 Training Loss: 0.040171440690755844\n",
      "Epoch 25268/30000 Training Loss: 0.05070483312010765\n",
      "Epoch 25269/30000 Training Loss: 0.04152587428689003\n",
      "Epoch 25270/30000 Training Loss: 0.05119778960943222\n",
      "Epoch 25271/30000 Training Loss: 0.04886304587125778\n",
      "Epoch 25272/30000 Training Loss: 0.03874487802386284\n",
      "Epoch 25273/30000 Training Loss: 0.04216000810265541\n",
      "Epoch 25274/30000 Training Loss: 0.05500498414039612\n",
      "Epoch 25275/30000 Training Loss: 0.03233049809932709\n",
      "Epoch 25276/30000 Training Loss: 0.03761456161737442\n",
      "Epoch 25277/30000 Training Loss: 0.05568587779998779\n",
      "Epoch 25278/30000 Training Loss: 0.047207269817590714\n",
      "Epoch 25279/30000 Training Loss: 0.04353996366262436\n",
      "Epoch 25280/30000 Training Loss: 0.051685091108083725\n",
      "Epoch 25281/30000 Training Loss: 0.04356156289577484\n",
      "Epoch 25282/30000 Training Loss: 0.04348546266555786\n",
      "Epoch 25283/30000 Training Loss: 0.04170461744070053\n",
      "Epoch 25284/30000 Training Loss: 0.046960338950157166\n",
      "Epoch 25285/30000 Training Loss: 0.033843159675598145\n",
      "Epoch 25286/30000 Training Loss: 0.05342573672533035\n",
      "Epoch 25287/30000 Training Loss: 0.047634322196245193\n",
      "Epoch 25288/30000 Training Loss: 0.05481097474694252\n",
      "Epoch 25289/30000 Training Loss: 0.05836651474237442\n",
      "Epoch 25290/30000 Training Loss: 0.04947040602564812\n",
      "Epoch 25291/30000 Training Loss: 0.046010877937078476\n",
      "Epoch 25292/30000 Training Loss: 0.048408642411231995\n",
      "Epoch 25293/30000 Training Loss: 0.03240642696619034\n",
      "Epoch 25294/30000 Training Loss: 0.03893164545297623\n",
      "Epoch 25295/30000 Training Loss: 0.042777903378009796\n",
      "Epoch 25296/30000 Training Loss: 0.03448791056871414\n",
      "Epoch 25297/30000 Training Loss: 0.039251141250133514\n",
      "Epoch 25298/30000 Training Loss: 0.06611596792936325\n",
      "Epoch 25299/30000 Training Loss: 0.06947223097085953\n",
      "Epoch 25300/30000 Training Loss: 0.05137575790286064\n",
      "Epoch 25300/30000 Validation Loss: 0.02771434187889099\n",
      "Epoch 25301/30000 Training Loss: 0.042245522141456604\n",
      "Epoch 25302/30000 Training Loss: 0.030484987422823906\n",
      "Epoch 25303/30000 Training Loss: 0.04841337725520134\n",
      "Epoch 25304/30000 Training Loss: 0.05445827543735504\n",
      "Epoch 25305/30000 Training Loss: 0.041462093591690063\n",
      "Epoch 25306/30000 Training Loss: 0.03736162930727005\n",
      "Epoch 25307/30000 Training Loss: 0.05584517866373062\n",
      "Epoch 25308/30000 Training Loss: 0.05279934033751488\n",
      "Epoch 25309/30000 Training Loss: 0.0384756401181221\n",
      "Epoch 25310/30000 Training Loss: 0.05774824321269989\n",
      "Epoch 25311/30000 Training Loss: 0.057451240718364716\n",
      "Epoch 25312/30000 Training Loss: 0.06456993520259857\n",
      "Epoch 25313/30000 Training Loss: 0.03643100708723068\n",
      "Epoch 25314/30000 Training Loss: 0.04821605607867241\n",
      "Epoch 25315/30000 Training Loss: 0.04070708528161049\n",
      "Epoch 25316/30000 Training Loss: 0.04799158126115799\n",
      "Epoch 25317/30000 Training Loss: 0.05067947134375572\n",
      "Epoch 25318/30000 Training Loss: 0.04763876274228096\n",
      "Epoch 25319/30000 Training Loss: 0.044630128890275955\n",
      "Epoch 25320/30000 Training Loss: 0.05260758101940155\n",
      "Epoch 25321/30000 Training Loss: 0.04348165541887283\n",
      "Epoch 25322/30000 Training Loss: 0.04390551894903183\n",
      "Epoch 25323/30000 Training Loss: 0.036941107362508774\n",
      "Epoch 25324/30000 Training Loss: 0.04349476099014282\n",
      "Epoch 25325/30000 Training Loss: 0.053661227226257324\n",
      "Epoch 25326/30000 Training Loss: 0.04002503678202629\n",
      "Epoch 25327/30000 Training Loss: 0.0584578663110733\n",
      "Epoch 25328/30000 Training Loss: 0.03779696673154831\n",
      "Epoch 25329/30000 Training Loss: 0.037902310490608215\n",
      "Epoch 25330/30000 Training Loss: 0.0438237227499485\n",
      "Epoch 25331/30000 Training Loss: 0.06211838498711586\n",
      "Epoch 25332/30000 Training Loss: 0.05473419278860092\n",
      "Epoch 25333/30000 Training Loss: 0.043872833251953125\n",
      "Epoch 25334/30000 Training Loss: 0.0388445109128952\n",
      "Epoch 25335/30000 Training Loss: 0.050832170993089676\n",
      "Epoch 25336/30000 Training Loss: 0.03335832059383392\n",
      "Epoch 25337/30000 Training Loss: 0.038412269204854965\n",
      "Epoch 25338/30000 Training Loss: 0.03652995824813843\n",
      "Epoch 25339/30000 Training Loss: 0.07660547643899918\n",
      "Epoch 25340/30000 Training Loss: 0.04420475289225578\n",
      "Epoch 25341/30000 Training Loss: 0.04760273918509483\n",
      "Epoch 25342/30000 Training Loss: 0.036401838064193726\n",
      "Epoch 25343/30000 Training Loss: 0.04367932304739952\n",
      "Epoch 25344/30000 Training Loss: 0.039976831525564194\n",
      "Epoch 25345/30000 Training Loss: 0.04846968501806259\n",
      "Epoch 25346/30000 Training Loss: 0.05527964234352112\n",
      "Epoch 25347/30000 Training Loss: 0.05086221918463707\n",
      "Epoch 25348/30000 Training Loss: 0.050257448107004166\n",
      "Epoch 25349/30000 Training Loss: 0.03849317505955696\n",
      "Epoch 25350/30000 Training Loss: 0.04205703362822533\n",
      "Epoch 25351/30000 Training Loss: 0.04304197430610657\n",
      "Epoch 25352/30000 Training Loss: 0.03512442857027054\n",
      "Epoch 25353/30000 Training Loss: 0.053183019161224365\n",
      "Epoch 25354/30000 Training Loss: 0.04013536870479584\n",
      "Epoch 25355/30000 Training Loss: 0.03857778012752533\n",
      "Epoch 25356/30000 Training Loss: 0.06320492178201675\n",
      "Epoch 25357/30000 Training Loss: 0.048243168741464615\n",
      "Epoch 25358/30000 Training Loss: 0.04652124270796776\n",
      "Epoch 25359/30000 Training Loss: 0.03176416456699371\n",
      "Epoch 25360/30000 Training Loss: 0.048739075660705566\n",
      "Epoch 25361/30000 Training Loss: 0.049300938844680786\n",
      "Epoch 25362/30000 Training Loss: 0.04205535352230072\n",
      "Epoch 25363/30000 Training Loss: 0.0389508381485939\n",
      "Epoch 25364/30000 Training Loss: 0.05706796050071716\n",
      "Epoch 25365/30000 Training Loss: 0.04756573215126991\n",
      "Epoch 25366/30000 Training Loss: 0.040143825113773346\n",
      "Epoch 25367/30000 Training Loss: 0.05821388587355614\n",
      "Epoch 25368/30000 Training Loss: 0.04118680953979492\n",
      "Epoch 25369/30000 Training Loss: 0.05079905316233635\n",
      "Epoch 25370/30000 Training Loss: 0.06520747393369675\n",
      "Epoch 25371/30000 Training Loss: 0.04699698090553284\n",
      "Epoch 25372/30000 Training Loss: 0.041142962872982025\n",
      "Epoch 25373/30000 Training Loss: 0.05946299433708191\n",
      "Epoch 25374/30000 Training Loss: 0.04838692024350166\n",
      "Epoch 25375/30000 Training Loss: 0.041396770626306534\n",
      "Epoch 25376/30000 Training Loss: 0.05031534656882286\n",
      "Epoch 25377/30000 Training Loss: 0.04442501440644264\n",
      "Epoch 25378/30000 Training Loss: 0.05159972980618477\n",
      "Epoch 25379/30000 Training Loss: 0.04528709873557091\n",
      "Epoch 25380/30000 Training Loss: 0.05333403870463371\n",
      "Epoch 25381/30000 Training Loss: 0.06719236820936203\n",
      "Epoch 25382/30000 Training Loss: 0.048461996018886566\n",
      "Epoch 25383/30000 Training Loss: 0.03846656158566475\n",
      "Epoch 25384/30000 Training Loss: 0.043324731290340424\n",
      "Epoch 25385/30000 Training Loss: 0.049120932817459106\n",
      "Epoch 25386/30000 Training Loss: 0.04180290549993515\n",
      "Epoch 25387/30000 Training Loss: 0.0544627383351326\n",
      "Epoch 25388/30000 Training Loss: 0.050393007695674896\n",
      "Epoch 25389/30000 Training Loss: 0.046817678958177567\n",
      "Epoch 25390/30000 Training Loss: 0.047860994935035706\n",
      "Epoch 25391/30000 Training Loss: 0.0411834791302681\n",
      "Epoch 25392/30000 Training Loss: 0.04950886219739914\n",
      "Epoch 25393/30000 Training Loss: 0.0440521165728569\n",
      "Epoch 25394/30000 Training Loss: 0.03775258734822273\n",
      "Epoch 25395/30000 Training Loss: 0.055405013263225555\n",
      "Epoch 25396/30000 Training Loss: 0.04662967845797539\n",
      "Epoch 25397/30000 Training Loss: 0.05639290064573288\n",
      "Epoch 25398/30000 Training Loss: 0.04863855987787247\n",
      "Epoch 25399/30000 Training Loss: 0.04366672784090042\n",
      "Epoch 25400/30000 Training Loss: 0.043996043503284454\n",
      "Epoch 25400/30000 Validation Loss: 0.037308692932128906\n",
      "Epoch 25401/30000 Training Loss: 0.044508546590805054\n",
      "Epoch 25402/30000 Training Loss: 0.04915755242109299\n",
      "Epoch 25403/30000 Training Loss: 0.05088424310088158\n",
      "Epoch 25404/30000 Training Loss: 0.0462118536233902\n",
      "Epoch 25405/30000 Training Loss: 0.041860561817884445\n",
      "Epoch 25406/30000 Training Loss: 0.05000767856836319\n",
      "Epoch 25407/30000 Training Loss: 0.04797329381108284\n",
      "Epoch 25408/30000 Training Loss: 0.04606860503554344\n",
      "Epoch 25409/30000 Training Loss: 0.050193414092063904\n",
      "Epoch 25410/30000 Training Loss: 0.034542642533779144\n",
      "Epoch 25411/30000 Training Loss: 0.06336619704961777\n",
      "Epoch 25412/30000 Training Loss: 0.04425179213285446\n",
      "Epoch 25413/30000 Training Loss: 0.05284768342971802\n",
      "Epoch 25414/30000 Training Loss: 0.053922221064567566\n",
      "Epoch 25415/30000 Training Loss: 0.04878265783190727\n",
      "Epoch 25416/30000 Training Loss: 0.032498616725206375\n",
      "Epoch 25417/30000 Training Loss: 0.029752913862466812\n",
      "Epoch 25418/30000 Training Loss: 0.057594358921051025\n",
      "Epoch 25419/30000 Training Loss: 0.051755163818597794\n",
      "Epoch 25420/30000 Training Loss: 0.04873993992805481\n",
      "Epoch 25421/30000 Training Loss: 0.04733782261610031\n",
      "Epoch 25422/30000 Training Loss: 0.053001902997493744\n",
      "Epoch 25423/30000 Training Loss: 0.043572116643190384\n",
      "Epoch 25424/30000 Training Loss: 0.057537950575351715\n",
      "Epoch 25425/30000 Training Loss: 0.04081666097044945\n",
      "Epoch 25426/30000 Training Loss: 0.03212783485651016\n",
      "Epoch 25427/30000 Training Loss: 0.05815652012825012\n",
      "Epoch 25428/30000 Training Loss: 0.05038043484091759\n",
      "Epoch 25429/30000 Training Loss: 0.045487381517887115\n",
      "Epoch 25430/30000 Training Loss: 0.045964136719703674\n",
      "Epoch 25431/30000 Training Loss: 0.03878343850374222\n",
      "Epoch 25432/30000 Training Loss: 0.05392807722091675\n",
      "Epoch 25433/30000 Training Loss: 0.04784563183784485\n",
      "Epoch 25434/30000 Training Loss: 0.04721571505069733\n",
      "Epoch 25435/30000 Training Loss: 0.03704367205500603\n",
      "Epoch 25436/30000 Training Loss: 0.039345379918813705\n",
      "Epoch 25437/30000 Training Loss: 0.05680440738797188\n",
      "Epoch 25438/30000 Training Loss: 0.05536135286092758\n",
      "Epoch 25439/30000 Training Loss: 0.041108276695013046\n",
      "Epoch 25440/30000 Training Loss: 0.043136611580848694\n",
      "Epoch 25441/30000 Training Loss: 0.030811592936515808\n",
      "Epoch 25442/30000 Training Loss: 0.04249852895736694\n",
      "Epoch 25443/30000 Training Loss: 0.04647638648748398\n",
      "Epoch 25444/30000 Training Loss: 0.04731563478708267\n",
      "Epoch 25445/30000 Training Loss: 0.0500529482960701\n",
      "Epoch 25446/30000 Training Loss: 0.03998591750860214\n",
      "Epoch 25447/30000 Training Loss: 0.039378974586725235\n",
      "Epoch 25448/30000 Training Loss: 0.03563380986452103\n",
      "Epoch 25449/30000 Training Loss: 0.046286821365356445\n",
      "Epoch 25450/30000 Training Loss: 0.03290537744760513\n",
      "Epoch 25451/30000 Training Loss: 0.04019928723573685\n",
      "Epoch 25452/30000 Training Loss: 0.04502449929714203\n",
      "Epoch 25453/30000 Training Loss: 0.04298723489046097\n",
      "Epoch 25454/30000 Training Loss: 0.05195627734065056\n",
      "Epoch 25455/30000 Training Loss: 0.04090120643377304\n",
      "Epoch 25456/30000 Training Loss: 0.04685910418629646\n",
      "Epoch 25457/30000 Training Loss: 0.044686127454042435\n",
      "Epoch 25458/30000 Training Loss: 0.06574493646621704\n",
      "Epoch 25459/30000 Training Loss: 0.03903590142726898\n",
      "Epoch 25460/30000 Training Loss: 0.04433625563979149\n",
      "Epoch 25461/30000 Training Loss: 0.04091527312994003\n",
      "Epoch 25462/30000 Training Loss: 0.05986239016056061\n",
      "Epoch 25463/30000 Training Loss: 0.04953398555517197\n",
      "Epoch 25464/30000 Training Loss: 0.044743090867996216\n",
      "Epoch 25465/30000 Training Loss: 0.044665634632110596\n",
      "Epoch 25466/30000 Training Loss: 0.05683746188879013\n",
      "Epoch 25467/30000 Training Loss: 0.04574085772037506\n",
      "Epoch 25468/30000 Training Loss: 0.047453563660383224\n",
      "Epoch 25469/30000 Training Loss: 0.04457366466522217\n",
      "Epoch 25470/30000 Training Loss: 0.040651194751262665\n",
      "Epoch 25471/30000 Training Loss: 0.03565693646669388\n",
      "Epoch 25472/30000 Training Loss: 0.04861713945865631\n",
      "Epoch 25473/30000 Training Loss: 0.04023667052388191\n",
      "Epoch 25474/30000 Training Loss: 0.05739772692322731\n",
      "Epoch 25475/30000 Training Loss: 0.043466679751873016\n",
      "Epoch 25476/30000 Training Loss: 0.035789649933576584\n",
      "Epoch 25477/30000 Training Loss: 0.03445880115032196\n",
      "Epoch 25478/30000 Training Loss: 0.04279894754290581\n",
      "Epoch 25479/30000 Training Loss: 0.05070043355226517\n",
      "Epoch 25480/30000 Training Loss: 0.05667824298143387\n",
      "Epoch 25481/30000 Training Loss: 0.0467255562543869\n",
      "Epoch 25482/30000 Training Loss: 0.03728464990854263\n",
      "Epoch 25483/30000 Training Loss: 0.03584352508187294\n",
      "Epoch 25484/30000 Training Loss: 0.045343637466430664\n",
      "Epoch 25485/30000 Training Loss: 0.05318320170044899\n",
      "Epoch 25486/30000 Training Loss: 0.04316209256649017\n",
      "Epoch 25487/30000 Training Loss: 0.06740030646324158\n",
      "Epoch 25488/30000 Training Loss: 0.044311780482530594\n",
      "Epoch 25489/30000 Training Loss: 0.052811894565820694\n",
      "Epoch 25490/30000 Training Loss: 0.06463111191987991\n",
      "Epoch 25491/30000 Training Loss: 0.042844440788030624\n",
      "Epoch 25492/30000 Training Loss: 0.037470363080501556\n",
      "Epoch 25493/30000 Training Loss: 0.0455327183008194\n",
      "Epoch 25494/30000 Training Loss: 0.03790797293186188\n",
      "Epoch 25495/30000 Training Loss: 0.039004746824502945\n",
      "Epoch 25496/30000 Training Loss: 0.04154616594314575\n",
      "Epoch 25497/30000 Training Loss: 0.0409187451004982\n",
      "Epoch 25498/30000 Training Loss: 0.053850796073675156\n",
      "Epoch 25499/30000 Training Loss: 0.0609896183013916\n",
      "Epoch 25500/30000 Training Loss: 0.04965372383594513\n",
      "Epoch 25500/30000 Validation Loss: 0.059266045689582825\n",
      "Epoch 25501/30000 Training Loss: 0.057041704654693604\n",
      "Epoch 25502/30000 Training Loss: 0.07089972496032715\n",
      "Epoch 25503/30000 Training Loss: 0.05030595883727074\n",
      "Epoch 25504/30000 Training Loss: 0.039300817996263504\n",
      "Epoch 25505/30000 Training Loss: 0.06044178456068039\n",
      "Epoch 25506/30000 Training Loss: 0.05176949128508568\n",
      "Epoch 25507/30000 Training Loss: 0.03660173714160919\n",
      "Epoch 25508/30000 Training Loss: 0.05527511611580849\n",
      "Epoch 25509/30000 Training Loss: 0.04615210369229317\n",
      "Epoch 25510/30000 Training Loss: 0.04340607300400734\n",
      "Epoch 25511/30000 Training Loss: 0.042679741978645325\n",
      "Epoch 25512/30000 Training Loss: 0.034330546855926514\n",
      "Epoch 25513/30000 Training Loss: 0.05612028390169144\n",
      "Epoch 25514/30000 Training Loss: 0.05106223002076149\n",
      "Epoch 25515/30000 Training Loss: 0.04259517416357994\n",
      "Epoch 25516/30000 Training Loss: 0.04199110344052315\n",
      "Epoch 25517/30000 Training Loss: 0.04515679553151131\n",
      "Epoch 25518/30000 Training Loss: 0.043036676943302155\n",
      "Epoch 25519/30000 Training Loss: 0.03846597671508789\n",
      "Epoch 25520/30000 Training Loss: 0.042548347264528275\n",
      "Epoch 25521/30000 Training Loss: 0.03644593060016632\n",
      "Epoch 25522/30000 Training Loss: 0.04125355929136276\n",
      "Epoch 25523/30000 Training Loss: 0.04611242935061455\n",
      "Epoch 25524/30000 Training Loss: 0.04682315140962601\n",
      "Epoch 25525/30000 Training Loss: 0.05492120981216431\n",
      "Epoch 25526/30000 Training Loss: 0.042206648737192154\n",
      "Epoch 25527/30000 Training Loss: 0.05315051227807999\n",
      "Epoch 25528/30000 Training Loss: 0.06011425703763962\n",
      "Epoch 25529/30000 Training Loss: 0.04151467978954315\n",
      "Epoch 25530/30000 Training Loss: 0.052214473485946655\n",
      "Epoch 25531/30000 Training Loss: 0.0494522750377655\n",
      "Epoch 25532/30000 Training Loss: 0.043077923357486725\n",
      "Epoch 25533/30000 Training Loss: 0.05626639351248741\n",
      "Epoch 25534/30000 Training Loss: 0.043622490018606186\n",
      "Epoch 25535/30000 Training Loss: 0.04714292287826538\n",
      "Epoch 25536/30000 Training Loss: 0.038139794021844864\n",
      "Epoch 25537/30000 Training Loss: 0.04655545949935913\n",
      "Epoch 25538/30000 Training Loss: 0.047900550067424774\n",
      "Epoch 25539/30000 Training Loss: 0.04194746911525726\n",
      "Epoch 25540/30000 Training Loss: 0.04846040904521942\n",
      "Epoch 25541/30000 Training Loss: 0.0492110550403595\n",
      "Epoch 25542/30000 Training Loss: 0.049985505640506744\n",
      "Epoch 25543/30000 Training Loss: 0.029763590544462204\n",
      "Epoch 25544/30000 Training Loss: 0.054387226700782776\n",
      "Epoch 25545/30000 Training Loss: 0.04795517027378082\n",
      "Epoch 25546/30000 Training Loss: 0.0437169149518013\n",
      "Epoch 25547/30000 Training Loss: 0.04307210072875023\n",
      "Epoch 25548/30000 Training Loss: 0.029730847105383873\n",
      "Epoch 25549/30000 Training Loss: 0.048316359519958496\n",
      "Epoch 25550/30000 Training Loss: 0.03825945034623146\n",
      "Epoch 25551/30000 Training Loss: 0.04426164925098419\n",
      "Epoch 25552/30000 Training Loss: 0.05176539719104767\n",
      "Epoch 25553/30000 Training Loss: 0.036285318434238434\n",
      "Epoch 25554/30000 Training Loss: 0.054371535778045654\n",
      "Epoch 25555/30000 Training Loss: 0.038740478456020355\n",
      "Epoch 25556/30000 Training Loss: 0.06119360402226448\n",
      "Epoch 25557/30000 Training Loss: 0.0477328896522522\n",
      "Epoch 25558/30000 Training Loss: 0.0352206826210022\n",
      "Epoch 25559/30000 Training Loss: 0.032773375511169434\n",
      "Epoch 25560/30000 Training Loss: 0.05505843833088875\n",
      "Epoch 25561/30000 Training Loss: 0.04739256203174591\n",
      "Epoch 25562/30000 Training Loss: 0.06164509803056717\n",
      "Epoch 25563/30000 Training Loss: 0.04730359837412834\n",
      "Epoch 25564/30000 Training Loss: 0.04237072914838791\n",
      "Epoch 25565/30000 Training Loss: 0.05297697335481644\n",
      "Epoch 25566/30000 Training Loss: 0.04453432559967041\n",
      "Epoch 25567/30000 Training Loss: 0.057612448930740356\n",
      "Epoch 25568/30000 Training Loss: 0.05377541109919548\n",
      "Epoch 25569/30000 Training Loss: 0.028647053986787796\n",
      "Epoch 25570/30000 Training Loss: 0.050919387489557266\n",
      "Epoch 25571/30000 Training Loss: 0.041834376752376556\n",
      "Epoch 25572/30000 Training Loss: 0.04982538893818855\n",
      "Epoch 25573/30000 Training Loss: 0.048844508826732635\n",
      "Epoch 25574/30000 Training Loss: 0.052159395068883896\n",
      "Epoch 25575/30000 Training Loss: 0.0569707490503788\n",
      "Epoch 25576/30000 Training Loss: 0.05837687849998474\n",
      "Epoch 25577/30000 Training Loss: 0.0554673969745636\n",
      "Epoch 25578/30000 Training Loss: 0.03448731452226639\n",
      "Epoch 25579/30000 Training Loss: 0.06156352162361145\n",
      "Epoch 25580/30000 Training Loss: 0.06381241977214813\n",
      "Epoch 25581/30000 Training Loss: 0.03134450316429138\n",
      "Epoch 25582/30000 Training Loss: 0.048063673079013824\n",
      "Epoch 25583/30000 Training Loss: 0.038265127688646317\n",
      "Epoch 25584/30000 Training Loss: 0.06763520836830139\n",
      "Epoch 25585/30000 Training Loss: 0.0651889368891716\n",
      "Epoch 25586/30000 Training Loss: 0.0455072745680809\n",
      "Epoch 25587/30000 Training Loss: 0.03561234474182129\n",
      "Epoch 25588/30000 Training Loss: 0.04831710830330849\n",
      "Epoch 25589/30000 Training Loss: 0.03837992250919342\n",
      "Epoch 25590/30000 Training Loss: 0.02866927906870842\n",
      "Epoch 25591/30000 Training Loss: 0.04922690615057945\n",
      "Epoch 25592/30000 Training Loss: 0.06150253862142563\n",
      "Epoch 25593/30000 Training Loss: 0.04798153042793274\n",
      "Epoch 25594/30000 Training Loss: 0.04801807180047035\n",
      "Epoch 25595/30000 Training Loss: 0.0564923956990242\n",
      "Epoch 25596/30000 Training Loss: 0.054620161652565\n",
      "Epoch 25597/30000 Training Loss: 0.042727384716272354\n",
      "Epoch 25598/30000 Training Loss: 0.04841635376214981\n",
      "Epoch 25599/30000 Training Loss: 0.05091894418001175\n",
      "Epoch 25600/30000 Training Loss: 0.04110705852508545\n",
      "Epoch 25600/30000 Validation Loss: 0.045845165848731995\n",
      "Epoch 25601/30000 Training Loss: 0.033162180334329605\n",
      "Epoch 25602/30000 Training Loss: 0.04433874413371086\n",
      "Epoch 25603/30000 Training Loss: 0.03486862778663635\n",
      "Epoch 25604/30000 Training Loss: 0.05343636870384216\n",
      "Epoch 25605/30000 Training Loss: 0.053702596575021744\n",
      "Epoch 25606/30000 Training Loss: 0.0446254163980484\n",
      "Epoch 25607/30000 Training Loss: 0.050506506115198135\n",
      "Epoch 25608/30000 Training Loss: 0.0485231988132\n",
      "Epoch 25609/30000 Training Loss: 0.029784642159938812\n",
      "Epoch 25610/30000 Training Loss: 0.04975132271647453\n",
      "Epoch 25611/30000 Training Loss: 0.04191530495882034\n",
      "Epoch 25612/30000 Training Loss: 0.055795252323150635\n",
      "Epoch 25613/30000 Training Loss: 0.05343526601791382\n",
      "Epoch 25614/30000 Training Loss: 0.06571709364652634\n",
      "Epoch 25615/30000 Training Loss: 0.06178674474358559\n",
      "Epoch 25616/30000 Training Loss: 0.055403102189302444\n",
      "Epoch 25617/30000 Training Loss: 0.05954504385590553\n",
      "Epoch 25618/30000 Training Loss: 0.05827392265200615\n",
      "Epoch 25619/30000 Training Loss: 0.04311748594045639\n",
      "Epoch 25620/30000 Training Loss: 0.03719990700483322\n",
      "Epoch 25621/30000 Training Loss: 0.05042149871587753\n",
      "Epoch 25622/30000 Training Loss: 0.04262450337409973\n",
      "Epoch 25623/30000 Training Loss: 0.04577814042568207\n",
      "Epoch 25624/30000 Training Loss: 0.040181711316108704\n",
      "Epoch 25625/30000 Training Loss: 0.05560244992375374\n",
      "Epoch 25626/30000 Training Loss: 0.03975775092840195\n",
      "Epoch 25627/30000 Training Loss: 0.04212845861911774\n",
      "Epoch 25628/30000 Training Loss: 0.03853026032447815\n",
      "Epoch 25629/30000 Training Loss: 0.034989431500434875\n",
      "Epoch 25630/30000 Training Loss: 0.03789877891540527\n",
      "Epoch 25631/30000 Training Loss: 0.04928074777126312\n",
      "Epoch 25632/30000 Training Loss: 0.04736306890845299\n",
      "Epoch 25633/30000 Training Loss: 0.04988974332809448\n",
      "Epoch 25634/30000 Training Loss: 0.04513292759656906\n",
      "Epoch 25635/30000 Training Loss: 0.03143487870693207\n",
      "Epoch 25636/30000 Training Loss: 0.0448279082775116\n",
      "Epoch 25637/30000 Training Loss: 0.041669365018606186\n",
      "Epoch 25638/30000 Training Loss: 0.04915875941514969\n",
      "Epoch 25639/30000 Training Loss: 0.03991861641407013\n",
      "Epoch 25640/30000 Training Loss: 0.03637970983982086\n",
      "Epoch 25641/30000 Training Loss: 0.048420608043670654\n",
      "Epoch 25642/30000 Training Loss: 0.03583464026451111\n",
      "Epoch 25643/30000 Training Loss: 0.06359747052192688\n",
      "Epoch 25644/30000 Training Loss: 0.04208002984523773\n",
      "Epoch 25645/30000 Training Loss: 0.06303413957357407\n",
      "Epoch 25646/30000 Training Loss: 0.03738604113459587\n",
      "Epoch 25647/30000 Training Loss: 0.04110071808099747\n",
      "Epoch 25648/30000 Training Loss: 0.057074204087257385\n",
      "Epoch 25649/30000 Training Loss: 0.03667199984192848\n",
      "Epoch 25650/30000 Training Loss: 0.04528488591313362\n",
      "Epoch 25651/30000 Training Loss: 0.06441380828619003\n",
      "Epoch 25652/30000 Training Loss: 0.04774414002895355\n",
      "Epoch 25653/30000 Training Loss: 0.04105468466877937\n",
      "Epoch 25654/30000 Training Loss: 0.054185330867767334\n",
      "Epoch 25655/30000 Training Loss: 0.04043994098901749\n",
      "Epoch 25656/30000 Training Loss: 0.04348260909318924\n",
      "Epoch 25657/30000 Training Loss: 0.051049381494522095\n",
      "Epoch 25658/30000 Training Loss: 0.049403488636016846\n",
      "Epoch 25659/30000 Training Loss: 0.04217029735445976\n",
      "Epoch 25660/30000 Training Loss: 0.04521523788571358\n",
      "Epoch 25661/30000 Training Loss: 0.04757719486951828\n",
      "Epoch 25662/30000 Training Loss: 0.0418834388256073\n",
      "Epoch 25663/30000 Training Loss: 0.02994774654507637\n",
      "Epoch 25664/30000 Training Loss: 0.04634585976600647\n",
      "Epoch 25665/30000 Training Loss: 0.039577730000019073\n",
      "Epoch 25666/30000 Training Loss: 0.0573701374232769\n",
      "Epoch 25667/30000 Training Loss: 0.05212012678384781\n",
      "Epoch 25668/30000 Training Loss: 0.04594770073890686\n",
      "Epoch 25669/30000 Training Loss: 0.05188199505209923\n",
      "Epoch 25670/30000 Training Loss: 0.035184551030397415\n",
      "Epoch 25671/30000 Training Loss: 0.04252122342586517\n",
      "Epoch 25672/30000 Training Loss: 0.03945408761501312\n",
      "Epoch 25673/30000 Training Loss: 0.04391976073384285\n",
      "Epoch 25674/30000 Training Loss: 0.05526335537433624\n",
      "Epoch 25675/30000 Training Loss: 0.04498278722167015\n",
      "Epoch 25676/30000 Training Loss: 0.04360537976026535\n",
      "Epoch 25677/30000 Training Loss: 0.061922501772642136\n",
      "Epoch 25678/30000 Training Loss: 0.04368910938501358\n",
      "Epoch 25679/30000 Training Loss: 0.05937501788139343\n",
      "Epoch 25680/30000 Training Loss: 0.03243451192975044\n",
      "Epoch 25681/30000 Training Loss: 0.04874316602945328\n",
      "Epoch 25682/30000 Training Loss: 0.058534130454063416\n",
      "Epoch 25683/30000 Training Loss: 0.05375887453556061\n",
      "Epoch 25684/30000 Training Loss: 0.054216280579566956\n",
      "Epoch 25685/30000 Training Loss: 0.04571904242038727\n",
      "Epoch 25686/30000 Training Loss: 0.055149875581264496\n",
      "Epoch 25687/30000 Training Loss: 0.03917307034134865\n",
      "Epoch 25688/30000 Training Loss: 0.0625675618648529\n",
      "Epoch 25689/30000 Training Loss: 0.0418713353574276\n",
      "Epoch 25690/30000 Training Loss: 0.05238141864538193\n",
      "Epoch 25691/30000 Training Loss: 0.046439580619335175\n",
      "Epoch 25692/30000 Training Loss: 0.04212440922856331\n",
      "Epoch 25693/30000 Training Loss: 0.037913814187049866\n",
      "Epoch 25694/30000 Training Loss: 0.034868936985731125\n",
      "Epoch 25695/30000 Training Loss: 0.05452677607536316\n",
      "Epoch 25696/30000 Training Loss: 0.04078374803066254\n",
      "Epoch 25697/30000 Training Loss: 0.044479724019765854\n",
      "Epoch 25698/30000 Training Loss: 0.055941492319107056\n",
      "Epoch 25699/30000 Training Loss: 0.05320785194635391\n",
      "Epoch 25700/30000 Training Loss: 0.05198061093688011\n",
      "Epoch 25700/30000 Validation Loss: 0.04275360703468323\n",
      "Epoch 25701/30000 Training Loss: 0.05859590321779251\n",
      "Epoch 25702/30000 Training Loss: 0.037534430623054504\n",
      "Epoch 25703/30000 Training Loss: 0.03880137950181961\n",
      "Epoch 25704/30000 Training Loss: 0.059873901307582855\n",
      "Epoch 25705/30000 Training Loss: 0.056267522275447845\n",
      "Epoch 25706/30000 Training Loss: 0.05394897609949112\n",
      "Epoch 25707/30000 Training Loss: 0.048465464264154434\n",
      "Epoch 25708/30000 Training Loss: 0.04415108263492584\n",
      "Epoch 25709/30000 Training Loss: 0.035769931972026825\n",
      "Epoch 25710/30000 Training Loss: 0.04570849612355232\n",
      "Epoch 25711/30000 Training Loss: 0.04244494438171387\n",
      "Epoch 25712/30000 Training Loss: 0.070484958589077\n",
      "Epoch 25713/30000 Training Loss: 0.048242732882499695\n",
      "Epoch 25714/30000 Training Loss: 0.04747324436903\n",
      "Epoch 25715/30000 Training Loss: 0.05261854827404022\n",
      "Epoch 25716/30000 Training Loss: 0.03473999351263046\n",
      "Epoch 25717/30000 Training Loss: 0.06710219383239746\n",
      "Epoch 25718/30000 Training Loss: 0.043696559965610504\n",
      "Epoch 25719/30000 Training Loss: 0.032787371426820755\n",
      "Epoch 25720/30000 Training Loss: 0.04671277478337288\n",
      "Epoch 25721/30000 Training Loss: 0.0447111576795578\n",
      "Epoch 25722/30000 Training Loss: 0.038645315915346146\n",
      "Epoch 25723/30000 Training Loss: 0.04868646338582039\n",
      "Epoch 25724/30000 Training Loss: 0.03799016773700714\n",
      "Epoch 25725/30000 Training Loss: 0.0515911802649498\n",
      "Epoch 25726/30000 Training Loss: 0.052511438727378845\n",
      "Epoch 25727/30000 Training Loss: 0.05662539601325989\n",
      "Epoch 25728/30000 Training Loss: 0.05894717201590538\n",
      "Epoch 25729/30000 Training Loss: 0.03587546944618225\n",
      "Epoch 25730/30000 Training Loss: 0.03562069311738014\n",
      "Epoch 25731/30000 Training Loss: 0.0443047396838665\n",
      "Epoch 25732/30000 Training Loss: 0.04837839677929878\n",
      "Epoch 25733/30000 Training Loss: 0.05624198168516159\n",
      "Epoch 25734/30000 Training Loss: 0.04685349017381668\n",
      "Epoch 25735/30000 Training Loss: 0.03926851227879524\n",
      "Epoch 25736/30000 Training Loss: 0.03854767233133316\n",
      "Epoch 25737/30000 Training Loss: 0.05093357712030411\n",
      "Epoch 25738/30000 Training Loss: 0.0378023162484169\n",
      "Epoch 25739/30000 Training Loss: 0.035603515803813934\n",
      "Epoch 25740/30000 Training Loss: 0.048645902425050735\n",
      "Epoch 25741/30000 Training Loss: 0.04803628474473953\n",
      "Epoch 25742/30000 Training Loss: 0.049648117274045944\n",
      "Epoch 25743/30000 Training Loss: 0.052352286875247955\n",
      "Epoch 25744/30000 Training Loss: 0.04288288950920105\n",
      "Epoch 25745/30000 Training Loss: 0.05107435956597328\n",
      "Epoch 25746/30000 Training Loss: 0.05465683713555336\n",
      "Epoch 25747/30000 Training Loss: 0.047070056200027466\n",
      "Epoch 25748/30000 Training Loss: 0.05452113226056099\n",
      "Epoch 25749/30000 Training Loss: 0.040384747087955475\n",
      "Epoch 25750/30000 Training Loss: 0.04628174751996994\n",
      "Epoch 25751/30000 Training Loss: 0.040469225496053696\n",
      "Epoch 25752/30000 Training Loss: 0.05817246437072754\n",
      "Epoch 25753/30000 Training Loss: 0.040094222873449326\n",
      "Epoch 25754/30000 Training Loss: 0.06344341486692429\n",
      "Epoch 25755/30000 Training Loss: 0.047347135841846466\n",
      "Epoch 25756/30000 Training Loss: 0.04787881299853325\n",
      "Epoch 25757/30000 Training Loss: 0.04926367104053497\n",
      "Epoch 25758/30000 Training Loss: 0.038306478410959244\n",
      "Epoch 25759/30000 Training Loss: 0.05691888555884361\n",
      "Epoch 25760/30000 Training Loss: 0.03816654533147812\n",
      "Epoch 25761/30000 Training Loss: 0.053938813507556915\n",
      "Epoch 25762/30000 Training Loss: 0.04633873701095581\n",
      "Epoch 25763/30000 Training Loss: 0.049452006816864014\n",
      "Epoch 25764/30000 Training Loss: 0.05415637046098709\n",
      "Epoch 25765/30000 Training Loss: 0.05503550171852112\n",
      "Epoch 25766/30000 Training Loss: 0.04943735897541046\n",
      "Epoch 25767/30000 Training Loss: 0.037626225501298904\n",
      "Epoch 25768/30000 Training Loss: 0.059019554406404495\n",
      "Epoch 25769/30000 Training Loss: 0.04403145983815193\n",
      "Epoch 25770/30000 Training Loss: 0.0502692386507988\n",
      "Epoch 25771/30000 Training Loss: 0.0486452542245388\n",
      "Epoch 25772/30000 Training Loss: 0.06393860280513763\n",
      "Epoch 25773/30000 Training Loss: 0.044004008173942566\n",
      "Epoch 25774/30000 Training Loss: 0.05174252390861511\n",
      "Epoch 25775/30000 Training Loss: 0.049137890338897705\n",
      "Epoch 25776/30000 Training Loss: 0.04099193587899208\n",
      "Epoch 25777/30000 Training Loss: 0.035338860005140305\n",
      "Epoch 25778/30000 Training Loss: 0.044203728437423706\n",
      "Epoch 25779/30000 Training Loss: 0.05143275484442711\n",
      "Epoch 25780/30000 Training Loss: 0.038973111659288406\n",
      "Epoch 25781/30000 Training Loss: 0.04170408099889755\n",
      "Epoch 25782/30000 Training Loss: 0.05298199504613876\n",
      "Epoch 25783/30000 Training Loss: 0.049949899315834045\n",
      "Epoch 25784/30000 Training Loss: 0.03608345240354538\n",
      "Epoch 25785/30000 Training Loss: 0.05650665611028671\n",
      "Epoch 25786/30000 Training Loss: 0.03270752727985382\n",
      "Epoch 25787/30000 Training Loss: 0.05079813301563263\n",
      "Epoch 25788/30000 Training Loss: 0.04568600282073021\n",
      "Epoch 25789/30000 Training Loss: 0.03704028204083443\n",
      "Epoch 25790/30000 Training Loss: 0.058972958475351334\n",
      "Epoch 25791/30000 Training Loss: 0.035232000052928925\n",
      "Epoch 25792/30000 Training Loss: 0.050887756049633026\n",
      "Epoch 25793/30000 Training Loss: 0.053017161786556244\n",
      "Epoch 25794/30000 Training Loss: 0.03154202550649643\n",
      "Epoch 25795/30000 Training Loss: 0.037921976298093796\n",
      "Epoch 25796/30000 Training Loss: 0.042044948786497116\n",
      "Epoch 25797/30000 Training Loss: 0.0427914597094059\n",
      "Epoch 25798/30000 Training Loss: 0.061318740248680115\n",
      "Epoch 25799/30000 Training Loss: 0.04093797877430916\n",
      "Epoch 25800/30000 Training Loss: 0.03612101450562477\n",
      "Epoch 25800/30000 Validation Loss: 0.05829102173447609\n",
      "Epoch 25801/30000 Training Loss: 0.04770117253065109\n",
      "Epoch 25802/30000 Training Loss: 0.04650047421455383\n",
      "Epoch 25803/30000 Training Loss: 0.03714299947023392\n",
      "Epoch 25804/30000 Training Loss: 0.05054921656847\n",
      "Epoch 25805/30000 Training Loss: 0.0540720671415329\n",
      "Epoch 25806/30000 Training Loss: 0.04266143590211868\n",
      "Epoch 25807/30000 Training Loss: 0.06096069514751434\n",
      "Epoch 25808/30000 Training Loss: 0.05462652072310448\n",
      "Epoch 25809/30000 Training Loss: 0.05693697929382324\n",
      "Epoch 25810/30000 Training Loss: 0.046764202415943146\n",
      "Epoch 25811/30000 Training Loss: 0.03672941029071808\n",
      "Epoch 25812/30000 Training Loss: 0.04669209569692612\n",
      "Epoch 25813/30000 Training Loss: 0.055876947939395905\n",
      "Epoch 25814/30000 Training Loss: 0.05283814296126366\n",
      "Epoch 25815/30000 Training Loss: 0.05656011402606964\n",
      "Epoch 25816/30000 Training Loss: 0.04705023020505905\n",
      "Epoch 25817/30000 Training Loss: 0.05002257972955704\n",
      "Epoch 25818/30000 Training Loss: 0.04613827168941498\n",
      "Epoch 25819/30000 Training Loss: 0.046099402010440826\n",
      "Epoch 25820/30000 Training Loss: 0.031612541526556015\n",
      "Epoch 25821/30000 Training Loss: 0.07459168136119843\n",
      "Epoch 25822/30000 Training Loss: 0.051874130964279175\n",
      "Epoch 25823/30000 Training Loss: 0.037543751299381256\n",
      "Epoch 25824/30000 Training Loss: 0.049372538924217224\n",
      "Epoch 25825/30000 Training Loss: 0.05449981987476349\n",
      "Epoch 25826/30000 Training Loss: 0.04070376977324486\n",
      "Epoch 25827/30000 Training Loss: 0.05157367140054703\n",
      "Epoch 25828/30000 Training Loss: 0.04125191643834114\n",
      "Epoch 25829/30000 Training Loss: 0.04048091545701027\n",
      "Epoch 25830/30000 Training Loss: 0.04929248243570328\n",
      "Epoch 25831/30000 Training Loss: 0.03698279708623886\n",
      "Epoch 25832/30000 Training Loss: 0.03443479910492897\n",
      "Epoch 25833/30000 Training Loss: 0.055789269506931305\n",
      "Epoch 25834/30000 Training Loss: 0.05199156701564789\n",
      "Epoch 25835/30000 Training Loss: 0.04377386346459389\n",
      "Epoch 25836/30000 Training Loss: 0.04704469442367554\n",
      "Epoch 25837/30000 Training Loss: 0.04620221629738808\n",
      "Epoch 25838/30000 Training Loss: 0.05832590162754059\n",
      "Epoch 25839/30000 Training Loss: 0.036888279020786285\n",
      "Epoch 25840/30000 Training Loss: 0.04505003243684769\n",
      "Epoch 25841/30000 Training Loss: 0.036769311875104904\n",
      "Epoch 25842/30000 Training Loss: 0.045223481953144073\n",
      "Epoch 25843/30000 Training Loss: 0.04467993229627609\n",
      "Epoch 25844/30000 Training Loss: 0.050609707832336426\n",
      "Epoch 25845/30000 Training Loss: 0.034704726189374924\n",
      "Epoch 25846/30000 Training Loss: 0.03569624945521355\n",
      "Epoch 25847/30000 Training Loss: 0.05042067542672157\n",
      "Epoch 25848/30000 Training Loss: 0.04433673620223999\n",
      "Epoch 25849/30000 Training Loss: 0.06061762571334839\n",
      "Epoch 25850/30000 Training Loss: 0.03257562220096588\n",
      "Epoch 25851/30000 Training Loss: 0.04569227993488312\n",
      "Epoch 25852/30000 Training Loss: 0.051273226737976074\n",
      "Epoch 25853/30000 Training Loss: 0.04510471969842911\n",
      "Epoch 25854/30000 Training Loss: 0.05048847571015358\n",
      "Epoch 25855/30000 Training Loss: 0.03409384936094284\n",
      "Epoch 25856/30000 Training Loss: 0.03878006339073181\n",
      "Epoch 25857/30000 Training Loss: 0.028382081538438797\n",
      "Epoch 25858/30000 Training Loss: 0.04237286373972893\n",
      "Epoch 25859/30000 Training Loss: 0.044001489877700806\n",
      "Epoch 25860/30000 Training Loss: 0.06300648301839828\n",
      "Epoch 25861/30000 Training Loss: 0.04902953281998634\n",
      "Epoch 25862/30000 Training Loss: 0.0461483933031559\n",
      "Epoch 25863/30000 Training Loss: 0.04403337091207504\n",
      "Epoch 25864/30000 Training Loss: 0.051683053374290466\n",
      "Epoch 25865/30000 Training Loss: 0.052964046597480774\n",
      "Epoch 25866/30000 Training Loss: 0.036064513027668\n",
      "Epoch 25867/30000 Training Loss: 0.05332712084054947\n",
      "Epoch 25868/30000 Training Loss: 0.04392123967409134\n",
      "Epoch 25869/30000 Training Loss: 0.036844633519649506\n",
      "Epoch 25870/30000 Training Loss: 0.05167178809642792\n",
      "Epoch 25871/30000 Training Loss: 0.041884347796440125\n",
      "Epoch 25872/30000 Training Loss: 0.048079147934913635\n",
      "Epoch 25873/30000 Training Loss: 0.04945070669054985\n",
      "Epoch 25874/30000 Training Loss: 0.04451894760131836\n",
      "Epoch 25875/30000 Training Loss: 0.031815264374017715\n",
      "Epoch 25876/30000 Training Loss: 0.04499311000108719\n",
      "Epoch 25877/30000 Training Loss: 0.07606671750545502\n",
      "Epoch 25878/30000 Training Loss: 0.051037296652793884\n",
      "Epoch 25879/30000 Training Loss: 0.02996843308210373\n",
      "Epoch 25880/30000 Training Loss: 0.04206034913659096\n",
      "Epoch 25881/30000 Training Loss: 0.047894302755594254\n",
      "Epoch 25882/30000 Training Loss: 0.043573908507823944\n",
      "Epoch 25883/30000 Training Loss: 0.060909420251846313\n",
      "Epoch 25884/30000 Training Loss: 0.05067947879433632\n",
      "Epoch 25885/30000 Training Loss: 0.043493229895830154\n",
      "Epoch 25886/30000 Training Loss: 0.05478106439113617\n",
      "Epoch 25887/30000 Training Loss: 0.04154282435774803\n",
      "Epoch 25888/30000 Training Loss: 0.06001313403248787\n",
      "Epoch 25889/30000 Training Loss: 0.0401490218937397\n",
      "Epoch 25890/30000 Training Loss: 0.03987046331167221\n",
      "Epoch 25891/30000 Training Loss: 0.05580095574259758\n",
      "Epoch 25892/30000 Training Loss: 0.046820979565382004\n",
      "Epoch 25893/30000 Training Loss: 0.06545966863632202\n",
      "Epoch 25894/30000 Training Loss: 0.04933357983827591\n",
      "Epoch 25895/30000 Training Loss: 0.057963669300079346\n",
      "Epoch 25896/30000 Training Loss: 0.052603237330913544\n",
      "Epoch 25897/30000 Training Loss: 0.04424281790852547\n",
      "Epoch 25898/30000 Training Loss: 0.04991888254880905\n",
      "Epoch 25899/30000 Training Loss: 0.03594483435153961\n",
      "Epoch 25900/30000 Training Loss: 0.05803234130144119\n",
      "Epoch 25900/30000 Validation Loss: 0.04117687791585922\n",
      "Epoch 25901/30000 Training Loss: 0.033331599086523056\n",
      "Epoch 25902/30000 Training Loss: 0.04115194454789162\n",
      "Epoch 25903/30000 Training Loss: 0.054604411125183105\n",
      "Epoch 25904/30000 Training Loss: 0.04690948501229286\n",
      "Epoch 25905/30000 Training Loss: 0.048660557717084885\n",
      "Epoch 25906/30000 Training Loss: 0.05145569145679474\n",
      "Epoch 25907/30000 Training Loss: 0.03558795526623726\n",
      "Epoch 25908/30000 Training Loss: 0.043151069432497025\n",
      "Epoch 25909/30000 Training Loss: 0.05540713295340538\n",
      "Epoch 25910/30000 Training Loss: 0.05093153193593025\n",
      "Epoch 25911/30000 Training Loss: 0.05960988625884056\n",
      "Epoch 25912/30000 Training Loss: 0.03584278002381325\n",
      "Epoch 25913/30000 Training Loss: 0.0457531213760376\n",
      "Epoch 25914/30000 Training Loss: 0.04931645840406418\n",
      "Epoch 25915/30000 Training Loss: 0.03899581730365753\n",
      "Epoch 25916/30000 Training Loss: 0.03072531893849373\n",
      "Epoch 25917/30000 Training Loss: 0.05680364742875099\n",
      "Epoch 25918/30000 Training Loss: 0.04569839686155319\n",
      "Epoch 25919/30000 Training Loss: 0.04479580000042915\n",
      "Epoch 25920/30000 Training Loss: 0.04189205914735794\n",
      "Epoch 25921/30000 Training Loss: 0.05482622981071472\n",
      "Epoch 25922/30000 Training Loss: 0.055190201848745346\n",
      "Epoch 25923/30000 Training Loss: 0.04837813973426819\n",
      "Epoch 25924/30000 Training Loss: 0.03465459495782852\n",
      "Epoch 25925/30000 Training Loss: 0.04388047382235527\n",
      "Epoch 25926/30000 Training Loss: 0.04625031352043152\n",
      "Epoch 25927/30000 Training Loss: 0.057247135788202286\n",
      "Epoch 25928/30000 Training Loss: 0.03577694296836853\n",
      "Epoch 25929/30000 Training Loss: 0.054013077169656754\n",
      "Epoch 25930/30000 Training Loss: 0.06259851902723312\n",
      "Epoch 25931/30000 Training Loss: 0.049543220549821854\n",
      "Epoch 25932/30000 Training Loss: 0.050878580659627914\n",
      "Epoch 25933/30000 Training Loss: 0.044371429830789566\n",
      "Epoch 25934/30000 Training Loss: 0.046486079692840576\n",
      "Epoch 25935/30000 Training Loss: 0.036412276327610016\n",
      "Epoch 25936/30000 Training Loss: 0.04929725080728531\n",
      "Epoch 25937/30000 Training Loss: 0.04584251716732979\n",
      "Epoch 25938/30000 Training Loss: 0.04277309775352478\n",
      "Epoch 25939/30000 Training Loss: 0.05408702418208122\n",
      "Epoch 25940/30000 Training Loss: 0.048432692885398865\n",
      "Epoch 25941/30000 Training Loss: 0.0438389927148819\n",
      "Epoch 25942/30000 Training Loss: 0.03608345240354538\n",
      "Epoch 25943/30000 Training Loss: 0.058155357837677\n",
      "Epoch 25944/30000 Training Loss: 0.03899114206433296\n",
      "Epoch 25945/30000 Training Loss: 0.04615674167871475\n",
      "Epoch 25946/30000 Training Loss: 0.03723429515957832\n",
      "Epoch 25947/30000 Training Loss: 0.048149384558200836\n",
      "Epoch 25948/30000 Training Loss: 0.04598354548215866\n",
      "Epoch 25949/30000 Training Loss: 0.04568081349134445\n",
      "Epoch 25950/30000 Training Loss: 0.04624711722135544\n",
      "Epoch 25951/30000 Training Loss: 0.053021062165498734\n",
      "Epoch 25952/30000 Training Loss: 0.03974812105298042\n",
      "Epoch 25953/30000 Training Loss: 0.03360969200730324\n",
      "Epoch 25954/30000 Training Loss: 0.030751602724194527\n",
      "Epoch 25955/30000 Training Loss: 0.03614611178636551\n",
      "Epoch 25956/30000 Training Loss: 0.048814911395311356\n",
      "Epoch 25957/30000 Training Loss: 0.04462303966283798\n",
      "Epoch 25958/30000 Training Loss: 0.04839284345507622\n",
      "Epoch 25959/30000 Training Loss: 0.049537673592567444\n",
      "Epoch 25960/30000 Training Loss: 0.0400831401348114\n",
      "Epoch 25961/30000 Training Loss: 0.04462157189846039\n",
      "Epoch 25962/30000 Training Loss: 0.049577176570892334\n",
      "Epoch 25963/30000 Training Loss: 0.047208480536937714\n",
      "Epoch 25964/30000 Training Loss: 0.040689025074243546\n",
      "Epoch 25965/30000 Training Loss: 0.04359869286417961\n",
      "Epoch 25966/30000 Training Loss: 0.05200065299868584\n",
      "Epoch 25967/30000 Training Loss: 0.04817581549286842\n",
      "Epoch 25968/30000 Training Loss: 0.06297046691179276\n",
      "Epoch 25969/30000 Training Loss: 0.039059750735759735\n",
      "Epoch 25970/30000 Training Loss: 0.04250673949718475\n",
      "Epoch 25971/30000 Training Loss: 0.048332445323467255\n",
      "Epoch 25972/30000 Training Loss: 0.05066601559519768\n",
      "Epoch 25973/30000 Training Loss: 0.044209323823451996\n",
      "Epoch 25974/30000 Training Loss: 0.04732852429151535\n",
      "Epoch 25975/30000 Training Loss: 0.04631161689758301\n",
      "Epoch 25976/30000 Training Loss: 0.03672630339860916\n",
      "Epoch 25977/30000 Training Loss: 0.044813722372055054\n",
      "Epoch 25978/30000 Training Loss: 0.058626607060432434\n",
      "Epoch 25979/30000 Training Loss: 0.057936303317546844\n",
      "Epoch 25980/30000 Training Loss: 0.0485355518758297\n",
      "Epoch 25981/30000 Training Loss: 0.05533169209957123\n",
      "Epoch 25982/30000 Training Loss: 0.05539239943027496\n",
      "Epoch 25983/30000 Training Loss: 0.05185000225901604\n",
      "Epoch 25984/30000 Training Loss: 0.03951099514961243\n",
      "Epoch 25985/30000 Training Loss: 0.04296993091702461\n",
      "Epoch 25986/30000 Training Loss: 0.04242875054478645\n",
      "Epoch 25987/30000 Training Loss: 0.03607121482491493\n",
      "Epoch 25988/30000 Training Loss: 0.05239691957831383\n",
      "Epoch 25989/30000 Training Loss: 0.04764113202691078\n",
      "Epoch 25990/30000 Training Loss: 0.04472596198320389\n",
      "Epoch 25991/30000 Training Loss: 0.03979552909731865\n",
      "Epoch 25992/30000 Training Loss: 0.05342312902212143\n",
      "Epoch 25993/30000 Training Loss: 0.05299682915210724\n",
      "Epoch 25994/30000 Training Loss: 0.04837820306420326\n",
      "Epoch 25995/30000 Training Loss: 0.05356457829475403\n",
      "Epoch 25996/30000 Training Loss: 0.04217405617237091\n",
      "Epoch 25997/30000 Training Loss: 0.035138364881277084\n",
      "Epoch 25998/30000 Training Loss: 0.04744752123951912\n",
      "Epoch 25999/30000 Training Loss: 0.06588906794786453\n",
      "Epoch 26000/30000 Training Loss: 0.055492039769887924\n",
      "Epoch 26000/30000 Validation Loss: 0.02925943396985531\n",
      "Epoch 26001/30000 Training Loss: 0.033077117055654526\n",
      "Epoch 26002/30000 Training Loss: 0.04878770187497139\n",
      "Epoch 26003/30000 Training Loss: 0.04809564724564552\n",
      "Epoch 26004/30000 Training Loss: 0.04474024847149849\n",
      "Epoch 26005/30000 Training Loss: 0.042890772223472595\n",
      "Epoch 26006/30000 Training Loss: 0.05716748535633087\n",
      "Epoch 26007/30000 Training Loss: 0.0453394390642643\n",
      "Epoch 26008/30000 Training Loss: 0.05478888005018234\n",
      "Epoch 26009/30000 Training Loss: 0.034481920301914215\n",
      "Epoch 26010/30000 Training Loss: 0.04573795944452286\n",
      "Epoch 26011/30000 Training Loss: 0.0432586632668972\n",
      "Epoch 26012/30000 Training Loss: 0.032671112567186356\n",
      "Epoch 26013/30000 Training Loss: 0.06265699118375778\n",
      "Epoch 26014/30000 Training Loss: 0.037461329251527786\n",
      "Epoch 26015/30000 Training Loss: 0.03182297199964523\n",
      "Epoch 26016/30000 Training Loss: 0.0570191964507103\n",
      "Epoch 26017/30000 Training Loss: 0.042201846837997437\n",
      "Epoch 26018/30000 Training Loss: 0.039932820945978165\n",
      "Epoch 26019/30000 Training Loss: 0.04609408229589462\n",
      "Epoch 26020/30000 Training Loss: 0.04634348303079605\n",
      "Epoch 26021/30000 Training Loss: 0.04922127351164818\n",
      "Epoch 26022/30000 Training Loss: 0.05418497323989868\n",
      "Epoch 26023/30000 Training Loss: 0.059182506054639816\n",
      "Epoch 26024/30000 Training Loss: 0.057888343930244446\n",
      "Epoch 26025/30000 Training Loss: 0.04843113571405411\n",
      "Epoch 26026/30000 Training Loss: 0.029799634590744972\n",
      "Epoch 26027/30000 Training Loss: 0.0479867160320282\n",
      "Epoch 26028/30000 Training Loss: 0.060805272310972214\n",
      "Epoch 26029/30000 Training Loss: 0.03944753482937813\n",
      "Epoch 26030/30000 Training Loss: 0.04553477093577385\n",
      "Epoch 26031/30000 Training Loss: 0.050960518419742584\n",
      "Epoch 26032/30000 Training Loss: 0.03403487056493759\n",
      "Epoch 26033/30000 Training Loss: 0.04027746990323067\n",
      "Epoch 26034/30000 Training Loss: 0.03953567519783974\n",
      "Epoch 26035/30000 Training Loss: 0.058356963098049164\n",
      "Epoch 26036/30000 Training Loss: 0.04452154040336609\n",
      "Epoch 26037/30000 Training Loss: 0.03758515045046806\n",
      "Epoch 26038/30000 Training Loss: 0.03970370069146156\n",
      "Epoch 26039/30000 Training Loss: 0.04974372684955597\n",
      "Epoch 26040/30000 Training Loss: 0.04532903432846069\n",
      "Epoch 26041/30000 Training Loss: 0.031704261898994446\n",
      "Epoch 26042/30000 Training Loss: 0.04574206843972206\n",
      "Epoch 26043/30000 Training Loss: 0.0400952510535717\n",
      "Epoch 26044/30000 Training Loss: 0.043847039341926575\n",
      "Epoch 26045/30000 Training Loss: 0.03272136673331261\n",
      "Epoch 26046/30000 Training Loss: 0.05256706103682518\n",
      "Epoch 26047/30000 Training Loss: 0.04951797425746918\n",
      "Epoch 26048/30000 Training Loss: 0.05436115339398384\n",
      "Epoch 26049/30000 Training Loss: 0.048883430659770966\n",
      "Epoch 26050/30000 Training Loss: 0.05050469934940338\n",
      "Epoch 26051/30000 Training Loss: 0.038655225187540054\n",
      "Epoch 26052/30000 Training Loss: 0.04321491718292236\n",
      "Epoch 26053/30000 Training Loss: 0.028324879705905914\n",
      "Epoch 26054/30000 Training Loss: 0.048388123512268066\n",
      "Epoch 26055/30000 Training Loss: 0.045467350631952286\n",
      "Epoch 26056/30000 Training Loss: 0.03765945881605148\n",
      "Epoch 26057/30000 Training Loss: 0.04779127985239029\n",
      "Epoch 26058/30000 Training Loss: 0.04172416031360626\n",
      "Epoch 26059/30000 Training Loss: 0.04896145686507225\n",
      "Epoch 26060/30000 Training Loss: 0.03222186863422394\n",
      "Epoch 26061/30000 Training Loss: 0.05250655859708786\n",
      "Epoch 26062/30000 Training Loss: 0.03531590849161148\n",
      "Epoch 26063/30000 Training Loss: 0.03965176269412041\n",
      "Epoch 26064/30000 Training Loss: 0.030061785131692886\n",
      "Epoch 26065/30000 Training Loss: 0.052762966603040695\n",
      "Epoch 26066/30000 Training Loss: 0.04039129987359047\n",
      "Epoch 26067/30000 Training Loss: 0.03959064558148384\n",
      "Epoch 26068/30000 Training Loss: 0.046094562858343124\n",
      "Epoch 26069/30000 Training Loss: 0.046182889491319656\n",
      "Epoch 26070/30000 Training Loss: 0.0362064354121685\n",
      "Epoch 26071/30000 Training Loss: 0.033515557646751404\n",
      "Epoch 26072/30000 Training Loss: 0.047960199415683746\n",
      "Epoch 26073/30000 Training Loss: 0.05130912736058235\n",
      "Epoch 26074/30000 Training Loss: 0.03935800492763519\n",
      "Epoch 26075/30000 Training Loss: 0.03567642718553543\n",
      "Epoch 26076/30000 Training Loss: 0.033769890666007996\n",
      "Epoch 26077/30000 Training Loss: 0.0418095625936985\n",
      "Epoch 26078/30000 Training Loss: 0.04469186067581177\n",
      "Epoch 26079/30000 Training Loss: 0.03954582288861275\n",
      "Epoch 26080/30000 Training Loss: 0.03456534445285797\n",
      "Epoch 26081/30000 Training Loss: 0.03635643422603607\n",
      "Epoch 26082/30000 Training Loss: 0.03983695060014725\n",
      "Epoch 26083/30000 Training Loss: 0.03723596781492233\n",
      "Epoch 26084/30000 Training Loss: 0.06435460597276688\n",
      "Epoch 26085/30000 Training Loss: 0.05538913235068321\n",
      "Epoch 26086/30000 Training Loss: 0.04235950857400894\n",
      "Epoch 26087/30000 Training Loss: 0.04455454647541046\n",
      "Epoch 26088/30000 Training Loss: 0.03391772136092186\n",
      "Epoch 26089/30000 Training Loss: 0.03458450734615326\n",
      "Epoch 26090/30000 Training Loss: 0.058665841817855835\n",
      "Epoch 26091/30000 Training Loss: 0.0362628772854805\n",
      "Epoch 26092/30000 Training Loss: 0.03704559803009033\n",
      "Epoch 26093/30000 Training Loss: 0.04167483001947403\n",
      "Epoch 26094/30000 Training Loss: 0.045343078672885895\n",
      "Epoch 26095/30000 Training Loss: 0.06710799038410187\n",
      "Epoch 26096/30000 Training Loss: 0.036154329776763916\n",
      "Epoch 26097/30000 Training Loss: 0.04319420084357262\n",
      "Epoch 26098/30000 Training Loss: 0.03975503146648407\n",
      "Epoch 26099/30000 Training Loss: 0.05637795478105545\n",
      "Epoch 26100/30000 Training Loss: 0.03897538781166077\n",
      "Epoch 26100/30000 Validation Loss: 0.04624078795313835\n",
      "Epoch 26101/30000 Training Loss: 0.05499843508005142\n",
      "Epoch 26102/30000 Training Loss: 0.04195252060890198\n",
      "Epoch 26103/30000 Training Loss: 0.038849927484989166\n",
      "Epoch 26104/30000 Training Loss: 0.04627878591418266\n",
      "Epoch 26105/30000 Training Loss: 0.04459235072135925\n",
      "Epoch 26106/30000 Training Loss: 0.04680577665567398\n",
      "Epoch 26107/30000 Training Loss: 0.06011848896741867\n",
      "Epoch 26108/30000 Training Loss: 0.048195116221904755\n",
      "Epoch 26109/30000 Training Loss: 0.038905344903469086\n",
      "Epoch 26110/30000 Training Loss: 0.06362558156251907\n",
      "Epoch 26111/30000 Training Loss: 0.03923077508807182\n",
      "Epoch 26112/30000 Training Loss: 0.04631587117910385\n",
      "Epoch 26113/30000 Training Loss: 0.040774233639240265\n",
      "Epoch 26114/30000 Training Loss: 0.039904892444610596\n",
      "Epoch 26115/30000 Training Loss: 0.046033311635255814\n",
      "Epoch 26116/30000 Training Loss: 0.04984557628631592\n",
      "Epoch 26117/30000 Training Loss: 0.0490349642932415\n",
      "Epoch 26118/30000 Training Loss: 0.038310978561639786\n",
      "Epoch 26119/30000 Training Loss: 0.04343916103243828\n",
      "Epoch 26120/30000 Training Loss: 0.03797077015042305\n",
      "Epoch 26121/30000 Training Loss: 0.049530819058418274\n",
      "Epoch 26122/30000 Training Loss: 0.029184632003307343\n",
      "Epoch 26123/30000 Training Loss: 0.03372938185930252\n",
      "Epoch 26124/30000 Training Loss: 0.05572941154241562\n",
      "Epoch 26125/30000 Training Loss: 0.039600834250450134\n",
      "Epoch 26126/30000 Training Loss: 0.07017560303211212\n",
      "Epoch 26127/30000 Training Loss: 0.04719226062297821\n",
      "Epoch 26128/30000 Training Loss: 0.04471645504236221\n",
      "Epoch 26129/30000 Training Loss: 0.043381355702877045\n",
      "Epoch 26130/30000 Training Loss: 0.03828945755958557\n",
      "Epoch 26131/30000 Training Loss: 0.04733695462346077\n",
      "Epoch 26132/30000 Training Loss: 0.0346386581659317\n",
      "Epoch 26133/30000 Training Loss: 0.039676159620285034\n",
      "Epoch 26134/30000 Training Loss: 0.04706559330224991\n",
      "Epoch 26135/30000 Training Loss: 0.05572924017906189\n",
      "Epoch 26136/30000 Training Loss: 0.04624859243631363\n",
      "Epoch 26137/30000 Training Loss: 0.03290429711341858\n",
      "Epoch 26138/30000 Training Loss: 0.04021663963794708\n",
      "Epoch 26139/30000 Training Loss: 0.041953638195991516\n",
      "Epoch 26140/30000 Training Loss: 0.057793330401182175\n",
      "Epoch 26141/30000 Training Loss: 0.06842811405658722\n",
      "Epoch 26142/30000 Training Loss: 0.03153911232948303\n",
      "Epoch 26143/30000 Training Loss: 0.0337136946618557\n",
      "Epoch 26144/30000 Training Loss: 0.06076040863990784\n",
      "Epoch 26145/30000 Training Loss: 0.039788708090782166\n",
      "Epoch 26146/30000 Training Loss: 0.0329168327152729\n",
      "Epoch 26147/30000 Training Loss: 0.045120902359485626\n",
      "Epoch 26148/30000 Training Loss: 0.042896464467048645\n",
      "Epoch 26149/30000 Training Loss: 0.04906187206506729\n",
      "Epoch 26150/30000 Training Loss: 0.06628317385911942\n",
      "Epoch 26151/30000 Training Loss: 0.0351438969373703\n",
      "Epoch 26152/30000 Training Loss: 0.0576658695936203\n",
      "Epoch 26153/30000 Training Loss: 0.030758466571569443\n",
      "Epoch 26154/30000 Training Loss: 0.056361399590969086\n",
      "Epoch 26155/30000 Training Loss: 0.0683954581618309\n",
      "Epoch 26156/30000 Training Loss: 0.038382913917303085\n",
      "Epoch 26157/30000 Training Loss: 0.04910273477435112\n",
      "Epoch 26158/30000 Training Loss: 0.0408291332423687\n",
      "Epoch 26159/30000 Training Loss: 0.031887952238321304\n",
      "Epoch 26160/30000 Training Loss: 0.045048270374536514\n",
      "Epoch 26161/30000 Training Loss: 0.05213397741317749\n",
      "Epoch 26162/30000 Training Loss: 0.030169323086738586\n",
      "Epoch 26163/30000 Training Loss: 0.03898172825574875\n",
      "Epoch 26164/30000 Training Loss: 0.05619092658162117\n",
      "Epoch 26165/30000 Training Loss: 0.0451442152261734\n",
      "Epoch 26166/30000 Training Loss: 0.047345489263534546\n",
      "Epoch 26167/30000 Training Loss: 0.04159682244062424\n",
      "Epoch 26168/30000 Training Loss: 0.045145101845264435\n",
      "Epoch 26169/30000 Training Loss: 0.047647625207901\n",
      "Epoch 26170/30000 Training Loss: 0.046096935868263245\n",
      "Epoch 26171/30000 Training Loss: 0.05654755234718323\n",
      "Epoch 26172/30000 Training Loss: 0.056159134954214096\n",
      "Epoch 26173/30000 Training Loss: 0.054832689464092255\n",
      "Epoch 26174/30000 Training Loss: 0.0356447659432888\n",
      "Epoch 26175/30000 Training Loss: 0.0510769747197628\n",
      "Epoch 26176/30000 Training Loss: 0.03881649672985077\n",
      "Epoch 26177/30000 Training Loss: 0.04237837344408035\n",
      "Epoch 26178/30000 Training Loss: 0.042011577636003494\n",
      "Epoch 26179/30000 Training Loss: 0.03126194700598717\n",
      "Epoch 26180/30000 Training Loss: 0.052359092980623245\n",
      "Epoch 26181/30000 Training Loss: 0.045107774436473846\n",
      "Epoch 26182/30000 Training Loss: 0.048345863819122314\n",
      "Epoch 26183/30000 Training Loss: 0.04685758426785469\n",
      "Epoch 26184/30000 Training Loss: 0.05864645913243294\n",
      "Epoch 26185/30000 Training Loss: 0.04432156682014465\n",
      "Epoch 26186/30000 Training Loss: 0.038346946239471436\n",
      "Epoch 26187/30000 Training Loss: 0.03121570125222206\n",
      "Epoch 26188/30000 Training Loss: 0.051760002970695496\n",
      "Epoch 26189/30000 Training Loss: 0.036842502653598785\n",
      "Epoch 26190/30000 Training Loss: 0.05871192365884781\n",
      "Epoch 26191/30000 Training Loss: 0.03893085569143295\n",
      "Epoch 26192/30000 Training Loss: 0.045999664813280106\n",
      "Epoch 26193/30000 Training Loss: 0.03610202670097351\n",
      "Epoch 26194/30000 Training Loss: 0.06674870103597641\n",
      "Epoch 26195/30000 Training Loss: 0.043034300208091736\n",
      "Epoch 26196/30000 Training Loss: 0.035985399037599564\n",
      "Epoch 26197/30000 Training Loss: 0.03434283286333084\n",
      "Epoch 26198/30000 Training Loss: 0.03921293467283249\n",
      "Epoch 26199/30000 Training Loss: 0.0519648976624012\n",
      "Epoch 26200/30000 Training Loss: 0.05715908110141754\n",
      "Epoch 26200/30000 Validation Loss: 0.03832379728555679\n",
      "Epoch 26201/30000 Training Loss: 0.03150700405240059\n",
      "Epoch 26202/30000 Training Loss: 0.027467910200357437\n",
      "Epoch 26203/30000 Training Loss: 0.0372038409113884\n",
      "Epoch 26204/30000 Training Loss: 0.041700758039951324\n",
      "Epoch 26205/30000 Training Loss: 0.047788169234991074\n",
      "Epoch 26206/30000 Training Loss: 0.04394962638616562\n",
      "Epoch 26207/30000 Training Loss: 0.04634273424744606\n",
      "Epoch 26208/30000 Training Loss: 0.037346817553043365\n",
      "Epoch 26209/30000 Training Loss: 0.04496114328503609\n",
      "Epoch 26210/30000 Training Loss: 0.05001700669527054\n",
      "Epoch 26211/30000 Training Loss: 0.03428725153207779\n",
      "Epoch 26212/30000 Training Loss: 0.04846574366092682\n",
      "Epoch 26213/30000 Training Loss: 0.039693400263786316\n",
      "Epoch 26214/30000 Training Loss: 0.03319738060235977\n",
      "Epoch 26215/30000 Training Loss: 0.04113936424255371\n",
      "Epoch 26216/30000 Training Loss: 0.04652193933725357\n",
      "Epoch 26217/30000 Training Loss: 0.04648379236459732\n",
      "Epoch 26218/30000 Training Loss: 0.03961082547903061\n",
      "Epoch 26219/30000 Training Loss: 0.04272737354040146\n",
      "Epoch 26220/30000 Training Loss: 0.038966644555330276\n",
      "Epoch 26221/30000 Training Loss: 0.042026326060295105\n",
      "Epoch 26222/30000 Training Loss: 0.05736662074923515\n",
      "Epoch 26223/30000 Training Loss: 0.046058539301157\n",
      "Epoch 26224/30000 Training Loss: 0.048610761761665344\n",
      "Epoch 26225/30000 Training Loss: 0.03463524207472801\n",
      "Epoch 26226/30000 Training Loss: 0.055292826145887375\n",
      "Epoch 26227/30000 Training Loss: 0.0307457335293293\n",
      "Epoch 26228/30000 Training Loss: 0.04942931607365608\n",
      "Epoch 26229/30000 Training Loss: 0.03519625589251518\n",
      "Epoch 26230/30000 Training Loss: 0.045086394995450974\n",
      "Epoch 26231/30000 Training Loss: 0.052643753588199615\n",
      "Epoch 26232/30000 Training Loss: 0.05494208261370659\n",
      "Epoch 26233/30000 Training Loss: 0.035961221903562546\n",
      "Epoch 26234/30000 Training Loss: 0.057764165103435516\n",
      "Epoch 26235/30000 Training Loss: 0.05926782637834549\n",
      "Epoch 26236/30000 Training Loss: 0.06537944823503494\n",
      "Epoch 26237/30000 Training Loss: 0.05651620402932167\n",
      "Epoch 26238/30000 Training Loss: 0.05515408515930176\n",
      "Epoch 26239/30000 Training Loss: 0.050471339374780655\n",
      "Epoch 26240/30000 Training Loss: 0.04694070667028427\n",
      "Epoch 26241/30000 Training Loss: 0.051724061369895935\n",
      "Epoch 26242/30000 Training Loss: 0.04094415903091431\n",
      "Epoch 26243/30000 Training Loss: 0.04633406177163124\n",
      "Epoch 26244/30000 Training Loss: 0.046963922679424286\n",
      "Epoch 26245/30000 Training Loss: 0.035107098519802094\n",
      "Epoch 26246/30000 Training Loss: 0.04355330020189285\n",
      "Epoch 26247/30000 Training Loss: 0.07054848223924637\n",
      "Epoch 26248/30000 Training Loss: 0.05354056507349014\n",
      "Epoch 26249/30000 Training Loss: 0.04776069521903992\n",
      "Epoch 26250/30000 Training Loss: 0.040628254413604736\n",
      "Epoch 26251/30000 Training Loss: 0.0492999404668808\n",
      "Epoch 26252/30000 Training Loss: 0.04120191931724548\n",
      "Epoch 26253/30000 Training Loss: 0.051964253187179565\n",
      "Epoch 26254/30000 Training Loss: 0.06025431305170059\n",
      "Epoch 26255/30000 Training Loss: 0.05785997956991196\n",
      "Epoch 26256/30000 Training Loss: 0.06057853624224663\n",
      "Epoch 26257/30000 Training Loss: 0.04904765635728836\n",
      "Epoch 26258/30000 Training Loss: 0.04437197372317314\n",
      "Epoch 26259/30000 Training Loss: 0.04338504746556282\n",
      "Epoch 26260/30000 Training Loss: 0.05055547505617142\n",
      "Epoch 26261/30000 Training Loss: 0.03614911437034607\n",
      "Epoch 26262/30000 Training Loss: 0.03836311772465706\n",
      "Epoch 26263/30000 Training Loss: 0.062136366963386536\n",
      "Epoch 26264/30000 Training Loss: 0.046159178018569946\n",
      "Epoch 26265/30000 Training Loss: 0.04089608043432236\n",
      "Epoch 26266/30000 Training Loss: 0.061285048723220825\n",
      "Epoch 26267/30000 Training Loss: 0.0499984472990036\n",
      "Epoch 26268/30000 Training Loss: 0.036656588315963745\n",
      "Epoch 26269/30000 Training Loss: 0.04551006481051445\n",
      "Epoch 26270/30000 Training Loss: 0.0594082847237587\n",
      "Epoch 26271/30000 Training Loss: 0.032078538089990616\n",
      "Epoch 26272/30000 Training Loss: 0.051086895167827606\n",
      "Epoch 26273/30000 Training Loss: 0.037048980593681335\n",
      "Epoch 26274/30000 Training Loss: 0.04911326989531517\n",
      "Epoch 26275/30000 Training Loss: 0.03911159932613373\n",
      "Epoch 26276/30000 Training Loss: 0.05204228311777115\n",
      "Epoch 26277/30000 Training Loss: 0.04227486997842789\n",
      "Epoch 26278/30000 Training Loss: 0.0502764917910099\n",
      "Epoch 26279/30000 Training Loss: 0.052900344133377075\n",
      "Epoch 26280/30000 Training Loss: 0.05053899809718132\n",
      "Epoch 26281/30000 Training Loss: 0.03756950795650482\n",
      "Epoch 26282/30000 Training Loss: 0.03650902956724167\n",
      "Epoch 26283/30000 Training Loss: 0.04360166937112808\n",
      "Epoch 26284/30000 Training Loss: 0.034303922206163406\n",
      "Epoch 26285/30000 Training Loss: 0.03786357119679451\n",
      "Epoch 26286/30000 Training Loss: 0.054451894015073776\n",
      "Epoch 26287/30000 Training Loss: 0.0525188185274601\n",
      "Epoch 26288/30000 Training Loss: 0.03914877027273178\n",
      "Epoch 26289/30000 Training Loss: 0.052913062274456024\n",
      "Epoch 26290/30000 Training Loss: 0.061544280499219894\n",
      "Epoch 26291/30000 Training Loss: 0.04994524270296097\n",
      "Epoch 26292/30000 Training Loss: 0.061868537217378616\n",
      "Epoch 26293/30000 Training Loss: 0.042940154671669006\n",
      "Epoch 26294/30000 Training Loss: 0.04649939388036728\n",
      "Epoch 26295/30000 Training Loss: 0.0488373264670372\n",
      "Epoch 26296/30000 Training Loss: 0.03535660356283188\n",
      "Epoch 26297/30000 Training Loss: 0.04108104109764099\n",
      "Epoch 26298/30000 Training Loss: 0.05754850059747696\n",
      "Epoch 26299/30000 Training Loss: 0.04294303059577942\n",
      "Epoch 26300/30000 Training Loss: 0.046975381672382355\n",
      "Epoch 26300/30000 Validation Loss: 0.04897337779402733\n",
      "Epoch 26301/30000 Training Loss: 0.05459728464484215\n",
      "Epoch 26302/30000 Training Loss: 0.03973603993654251\n",
      "Epoch 26303/30000 Training Loss: 0.04516807943582535\n",
      "Epoch 26304/30000 Training Loss: 0.041261106729507446\n",
      "Epoch 26305/30000 Training Loss: 0.05315757170319557\n",
      "Epoch 26306/30000 Training Loss: 0.05717328190803528\n",
      "Epoch 26307/30000 Training Loss: 0.059610575437545776\n",
      "Epoch 26308/30000 Training Loss: 0.05530012398958206\n",
      "Epoch 26309/30000 Training Loss: 0.05193484574556351\n",
      "Epoch 26310/30000 Training Loss: 0.03403976559638977\n",
      "Epoch 26311/30000 Training Loss: 0.0619255006313324\n",
      "Epoch 26312/30000 Training Loss: 0.02986260876059532\n",
      "Epoch 26313/30000 Training Loss: 0.04684172570705414\n",
      "Epoch 26314/30000 Training Loss: 0.04384666308760643\n",
      "Epoch 26315/30000 Training Loss: 0.047582585364580154\n",
      "Epoch 26316/30000 Training Loss: 0.05778898298740387\n",
      "Epoch 26317/30000 Training Loss: 0.031484656035900116\n",
      "Epoch 26318/30000 Training Loss: 0.03956181928515434\n",
      "Epoch 26319/30000 Training Loss: 0.050817862153053284\n",
      "Epoch 26320/30000 Training Loss: 0.04100197181105614\n",
      "Epoch 26321/30000 Training Loss: 0.040278926491737366\n",
      "Epoch 26322/30000 Training Loss: 0.03734221309423447\n",
      "Epoch 26323/30000 Training Loss: 0.03973803296685219\n",
      "Epoch 26324/30000 Training Loss: 0.037463292479515076\n",
      "Epoch 26325/30000 Training Loss: 0.041675034910440445\n",
      "Epoch 26326/30000 Training Loss: 0.032827530056238174\n",
      "Epoch 26327/30000 Training Loss: 0.056332726031541824\n",
      "Epoch 26328/30000 Training Loss: 0.045446425676345825\n",
      "Epoch 26329/30000 Training Loss: 0.048094552010297775\n",
      "Epoch 26330/30000 Training Loss: 0.040585488080978394\n",
      "Epoch 26331/30000 Training Loss: 0.04328916594386101\n",
      "Epoch 26332/30000 Training Loss: 0.038419123739004135\n",
      "Epoch 26333/30000 Training Loss: 0.032087285071611404\n",
      "Epoch 26334/30000 Training Loss: 0.0526159331202507\n",
      "Epoch 26335/30000 Training Loss: 0.04819675534963608\n",
      "Epoch 26336/30000 Training Loss: 0.03339758515357971\n",
      "Epoch 26337/30000 Training Loss: 0.05053764209151268\n",
      "Epoch 26338/30000 Training Loss: 0.030462179332971573\n",
      "Epoch 26339/30000 Training Loss: 0.05451081320643425\n",
      "Epoch 26340/30000 Training Loss: 0.049526967108249664\n",
      "Epoch 26341/30000 Training Loss: 0.061411015689373016\n",
      "Epoch 26342/30000 Training Loss: 0.05663108080625534\n",
      "Epoch 26343/30000 Training Loss: 0.04543258249759674\n",
      "Epoch 26344/30000 Training Loss: 0.07123463600873947\n",
      "Epoch 26345/30000 Training Loss: 0.034947190433740616\n",
      "Epoch 26346/30000 Training Loss: 0.04485572129487991\n",
      "Epoch 26347/30000 Training Loss: 0.0376250222325325\n",
      "Epoch 26348/30000 Training Loss: 0.04739906266331673\n",
      "Epoch 26349/30000 Training Loss: 0.046178992837667465\n",
      "Epoch 26350/30000 Training Loss: 0.04589381441473961\n",
      "Epoch 26351/30000 Training Loss: 0.045836370438337326\n",
      "Epoch 26352/30000 Training Loss: 0.03487499803304672\n",
      "Epoch 26353/30000 Training Loss: 0.04869162291288376\n",
      "Epoch 26354/30000 Training Loss: 0.042580313980579376\n",
      "Epoch 26355/30000 Training Loss: 0.03264252468943596\n",
      "Epoch 26356/30000 Training Loss: 0.05031154677271843\n",
      "Epoch 26357/30000 Training Loss: 0.059433672577142715\n",
      "Epoch 26358/30000 Training Loss: 0.03511924296617508\n",
      "Epoch 26359/30000 Training Loss: 0.048752497881650925\n",
      "Epoch 26360/30000 Training Loss: 0.06084470450878143\n",
      "Epoch 26361/30000 Training Loss: 0.04816542565822601\n",
      "Epoch 26362/30000 Training Loss: 0.056336455047130585\n",
      "Epoch 26363/30000 Training Loss: 0.03540213778614998\n",
      "Epoch 26364/30000 Training Loss: 0.04743240773677826\n",
      "Epoch 26365/30000 Training Loss: 0.035993706434965134\n",
      "Epoch 26366/30000 Training Loss: 0.031778622418642044\n",
      "Epoch 26367/30000 Training Loss: 0.043820563703775406\n",
      "Epoch 26368/30000 Training Loss: 0.042133577167987823\n",
      "Epoch 26369/30000 Training Loss: 0.06315887719392776\n",
      "Epoch 26370/30000 Training Loss: 0.040568526834249496\n",
      "Epoch 26371/30000 Training Loss: 0.06383603811264038\n",
      "Epoch 26372/30000 Training Loss: 0.05626817047595978\n",
      "Epoch 26373/30000 Training Loss: 0.044916436076164246\n",
      "Epoch 26374/30000 Training Loss: 0.04654879868030548\n",
      "Epoch 26375/30000 Training Loss: 0.03831631690263748\n",
      "Epoch 26376/30000 Training Loss: 0.05118296295404434\n",
      "Epoch 26377/30000 Training Loss: 0.04817311093211174\n",
      "Epoch 26378/30000 Training Loss: 0.06539258360862732\n",
      "Epoch 26379/30000 Training Loss: 0.0497632771730423\n",
      "Epoch 26380/30000 Training Loss: 0.042276281863451004\n",
      "Epoch 26381/30000 Training Loss: 0.05334761366248131\n",
      "Epoch 26382/30000 Training Loss: 0.041549939662218094\n",
      "Epoch 26383/30000 Training Loss: 0.044017914682626724\n",
      "Epoch 26384/30000 Training Loss: 0.04847749322652817\n",
      "Epoch 26385/30000 Training Loss: 0.03888667747378349\n",
      "Epoch 26386/30000 Training Loss: 0.027043085545301437\n",
      "Epoch 26387/30000 Training Loss: 0.050378285348415375\n",
      "Epoch 26388/30000 Training Loss: 0.03555937111377716\n",
      "Epoch 26389/30000 Training Loss: 0.03687756508588791\n",
      "Epoch 26390/30000 Training Loss: 0.04401788488030434\n",
      "Epoch 26391/30000 Training Loss: 0.03518141433596611\n",
      "Epoch 26392/30000 Training Loss: 0.045912373811006546\n",
      "Epoch 26393/30000 Training Loss: 0.03659436106681824\n",
      "Epoch 26394/30000 Training Loss: 0.043055541813373566\n",
      "Epoch 26395/30000 Training Loss: 0.05406741797924042\n",
      "Epoch 26396/30000 Training Loss: 0.048819076269865036\n",
      "Epoch 26397/30000 Training Loss: 0.03470819070935249\n",
      "Epoch 26398/30000 Training Loss: 0.041199199855327606\n",
      "Epoch 26399/30000 Training Loss: 0.04792110621929169\n",
      "Epoch 26400/30000 Training Loss: 0.0528404675424099\n",
      "Epoch 26400/30000 Validation Loss: 0.04011908918619156\n",
      "Epoch 26401/30000 Training Loss: 0.03914117440581322\n",
      "Epoch 26402/30000 Training Loss: 0.04474557936191559\n",
      "Epoch 26403/30000 Training Loss: 0.03390759229660034\n",
      "Epoch 26404/30000 Training Loss: 0.04208480939269066\n",
      "Epoch 26405/30000 Training Loss: 0.05540220066905022\n",
      "Epoch 26406/30000 Training Loss: 0.032461460679769516\n",
      "Epoch 26407/30000 Training Loss: 0.06266359984874725\n",
      "Epoch 26408/30000 Training Loss: 0.03923707455396652\n",
      "Epoch 26409/30000 Training Loss: 0.03022497147321701\n",
      "Epoch 26410/30000 Training Loss: 0.04125390574336052\n",
      "Epoch 26411/30000 Training Loss: 0.041687704622745514\n",
      "Epoch 26412/30000 Training Loss: 0.04753419756889343\n",
      "Epoch 26413/30000 Training Loss: 0.04086242616176605\n",
      "Epoch 26414/30000 Training Loss: 0.04231689125299454\n",
      "Epoch 26415/30000 Training Loss: 0.03598588705062866\n",
      "Epoch 26416/30000 Training Loss: 0.04962240159511566\n",
      "Epoch 26417/30000 Training Loss: 0.04247055947780609\n",
      "Epoch 26418/30000 Training Loss: 0.04035389423370361\n",
      "Epoch 26419/30000 Training Loss: 0.05312689393758774\n",
      "Epoch 26420/30000 Training Loss: 0.03848522901535034\n",
      "Epoch 26421/30000 Training Loss: 0.043036166578531265\n",
      "Epoch 26422/30000 Training Loss: 0.036825649440288544\n",
      "Epoch 26423/30000 Training Loss: 0.028734980151057243\n",
      "Epoch 26424/30000 Training Loss: 0.04877053573727608\n",
      "Epoch 26425/30000 Training Loss: 0.045411545783281326\n",
      "Epoch 26426/30000 Training Loss: 0.049094829708337784\n",
      "Epoch 26427/30000 Training Loss: 0.04135818034410477\n",
      "Epoch 26428/30000 Training Loss: 0.05521426349878311\n",
      "Epoch 26429/30000 Training Loss: 0.048961855471134186\n",
      "Epoch 26430/30000 Training Loss: 0.04323668032884598\n",
      "Epoch 26431/30000 Training Loss: 0.043770819902420044\n",
      "Epoch 26432/30000 Training Loss: 0.04065307229757309\n",
      "Epoch 26433/30000 Training Loss: 0.04403456673026085\n",
      "Epoch 26434/30000 Training Loss: 0.058824650943279266\n",
      "Epoch 26435/30000 Training Loss: 0.04545595496892929\n",
      "Epoch 26436/30000 Training Loss: 0.048627085983753204\n",
      "Epoch 26437/30000 Training Loss: 0.0325862355530262\n",
      "Epoch 26438/30000 Training Loss: 0.04829477518796921\n",
      "Epoch 26439/30000 Training Loss: 0.05860363692045212\n",
      "Epoch 26440/30000 Training Loss: 0.045191578567028046\n",
      "Epoch 26441/30000 Training Loss: 0.051302719861269\n",
      "Epoch 26442/30000 Training Loss: 0.05136784911155701\n",
      "Epoch 26443/30000 Training Loss: 0.0401567667722702\n",
      "Epoch 26444/30000 Training Loss: 0.057435594499111176\n",
      "Epoch 26445/30000 Training Loss: 0.06006810814142227\n",
      "Epoch 26446/30000 Training Loss: 0.04992390424013138\n",
      "Epoch 26447/30000 Training Loss: 0.04622800275683403\n",
      "Epoch 26448/30000 Training Loss: 0.054601170122623444\n",
      "Epoch 26449/30000 Training Loss: 0.04509910196065903\n",
      "Epoch 26450/30000 Training Loss: 0.04189155995845795\n",
      "Epoch 26451/30000 Training Loss: 0.038490358740091324\n",
      "Epoch 26452/30000 Training Loss: 0.048867642879486084\n",
      "Epoch 26453/30000 Training Loss: 0.03959250450134277\n",
      "Epoch 26454/30000 Training Loss: 0.04244773089885712\n",
      "Epoch 26455/30000 Training Loss: 0.032602325081825256\n",
      "Epoch 26456/30000 Training Loss: 0.05496925860643387\n",
      "Epoch 26457/30000 Training Loss: 0.05500318482518196\n",
      "Epoch 26458/30000 Training Loss: 0.06654089689254761\n",
      "Epoch 26459/30000 Training Loss: 0.053532589226961136\n",
      "Epoch 26460/30000 Training Loss: 0.05051400139927864\n",
      "Epoch 26461/30000 Training Loss: 0.03189752995967865\n",
      "Epoch 26462/30000 Training Loss: 0.05157530680298805\n",
      "Epoch 26463/30000 Training Loss: 0.06474626809358597\n",
      "Epoch 26464/30000 Training Loss: 0.04095963388681412\n",
      "Epoch 26465/30000 Training Loss: 0.05132022872567177\n",
      "Epoch 26466/30000 Training Loss: 0.04783046245574951\n",
      "Epoch 26467/30000 Training Loss: 0.06178384646773338\n",
      "Epoch 26468/30000 Training Loss: 0.0417167954146862\n",
      "Epoch 26469/30000 Training Loss: 0.04364309832453728\n",
      "Epoch 26470/30000 Training Loss: 0.055637091398239136\n",
      "Epoch 26471/30000 Training Loss: 0.050169363617897034\n",
      "Epoch 26472/30000 Training Loss: 0.048864252865314484\n",
      "Epoch 26473/30000 Training Loss: 0.03989399969577789\n",
      "Epoch 26474/30000 Training Loss: 0.03421013802289963\n",
      "Epoch 26475/30000 Training Loss: 0.028620533645153046\n",
      "Epoch 26476/30000 Training Loss: 0.04354017600417137\n",
      "Epoch 26477/30000 Training Loss: 0.03891769424080849\n",
      "Epoch 26478/30000 Training Loss: 0.0407199002802372\n",
      "Epoch 26479/30000 Training Loss: 0.056320562958717346\n",
      "Epoch 26480/30000 Training Loss: 0.044784046709537506\n",
      "Epoch 26481/30000 Training Loss: 0.04688182473182678\n",
      "Epoch 26482/30000 Training Loss: 0.04671119898557663\n",
      "Epoch 26483/30000 Training Loss: 0.037489816546440125\n",
      "Epoch 26484/30000 Training Loss: 0.049926191568374634\n",
      "Epoch 26485/30000 Training Loss: 0.04052086919546127\n",
      "Epoch 26486/30000 Training Loss: 0.04229807108640671\n",
      "Epoch 26487/30000 Training Loss: 0.040145501494407654\n",
      "Epoch 26488/30000 Training Loss: 0.05822039768099785\n",
      "Epoch 26489/30000 Training Loss: 0.05042421817779541\n",
      "Epoch 26490/30000 Training Loss: 0.0404922254383564\n",
      "Epoch 26491/30000 Training Loss: 0.04450137913227081\n",
      "Epoch 26492/30000 Training Loss: 0.04792844131588936\n",
      "Epoch 26493/30000 Training Loss: 0.04401327297091484\n",
      "Epoch 26494/30000 Training Loss: 0.04718906432390213\n",
      "Epoch 26495/30000 Training Loss: 0.049505241215229034\n",
      "Epoch 26496/30000 Training Loss: 0.04315285012125969\n",
      "Epoch 26497/30000 Training Loss: 0.05796289071440697\n",
      "Epoch 26498/30000 Training Loss: 0.036861829459667206\n",
      "Epoch 26499/30000 Training Loss: 0.038681510835886\n",
      "Epoch 26500/30000 Training Loss: 0.04648455232381821\n",
      "Epoch 26500/30000 Validation Loss: 0.057946741580963135\n",
      "Epoch 26501/30000 Training Loss: 0.04141063615679741\n",
      "Epoch 26502/30000 Training Loss: 0.03500567376613617\n",
      "Epoch 26503/30000 Training Loss: 0.03800457343459129\n",
      "Epoch 26504/30000 Training Loss: 0.035051532089710236\n",
      "Epoch 26505/30000 Training Loss: 0.051532767713069916\n",
      "Epoch 26506/30000 Training Loss: 0.0509137399494648\n",
      "Epoch 26507/30000 Training Loss: 0.05820745974779129\n",
      "Epoch 26508/30000 Training Loss: 0.05640806257724762\n",
      "Epoch 26509/30000 Training Loss: 0.06355858594179153\n",
      "Epoch 26510/30000 Training Loss: 0.054842397570610046\n",
      "Epoch 26511/30000 Training Loss: 0.03641507774591446\n",
      "Epoch 26512/30000 Training Loss: 0.03293294832110405\n",
      "Epoch 26513/30000 Training Loss: 0.054338667541742325\n",
      "Epoch 26514/30000 Training Loss: 0.060852956026792526\n",
      "Epoch 26515/30000 Training Loss: 0.04071986675262451\n",
      "Epoch 26516/30000 Training Loss: 0.03619014099240303\n",
      "Epoch 26517/30000 Training Loss: 0.0403699167072773\n",
      "Epoch 26518/30000 Training Loss: 0.03960549086332321\n",
      "Epoch 26519/30000 Training Loss: 0.045855529606342316\n",
      "Epoch 26520/30000 Training Loss: 0.052685052156448364\n",
      "Epoch 26521/30000 Training Loss: 0.03776354715228081\n",
      "Epoch 26522/30000 Training Loss: 0.049598198384046555\n",
      "Epoch 26523/30000 Training Loss: 0.05171855911612511\n",
      "Epoch 26524/30000 Training Loss: 0.04122592881321907\n",
      "Epoch 26525/30000 Training Loss: 0.06025096774101257\n",
      "Epoch 26526/30000 Training Loss: 0.053144343197345734\n",
      "Epoch 26527/30000 Training Loss: 0.04828333109617233\n",
      "Epoch 26528/30000 Training Loss: 0.045332904905080795\n",
      "Epoch 26529/30000 Training Loss: 0.0399816557765007\n",
      "Epoch 26530/30000 Training Loss: 0.05590403079986572\n",
      "Epoch 26531/30000 Training Loss: 0.046974338591098785\n",
      "Epoch 26532/30000 Training Loss: 0.061411745846271515\n",
      "Epoch 26533/30000 Training Loss: 0.05241917818784714\n",
      "Epoch 26534/30000 Training Loss: 0.04985857754945755\n",
      "Epoch 26535/30000 Training Loss: 0.045363835990428925\n",
      "Epoch 26536/30000 Training Loss: 0.04578913375735283\n",
      "Epoch 26537/30000 Training Loss: 0.054427385330200195\n",
      "Epoch 26538/30000 Training Loss: 0.04297706484794617\n",
      "Epoch 26539/30000 Training Loss: 0.04143943637609482\n",
      "Epoch 26540/30000 Training Loss: 0.05682600289583206\n",
      "Epoch 26541/30000 Training Loss: 0.044377390295267105\n",
      "Epoch 26542/30000 Training Loss: 0.05607735365629196\n",
      "Epoch 26543/30000 Training Loss: 0.04190225526690483\n",
      "Epoch 26544/30000 Training Loss: 0.059594765305519104\n",
      "Epoch 26545/30000 Training Loss: 0.04745527356863022\n",
      "Epoch 26546/30000 Training Loss: 0.026889465749263763\n",
      "Epoch 26547/30000 Training Loss: 0.061036400496959686\n",
      "Epoch 26548/30000 Training Loss: 0.039828043431043625\n",
      "Epoch 26549/30000 Training Loss: 0.048500511795282364\n",
      "Epoch 26550/30000 Training Loss: 0.04499613493680954\n",
      "Epoch 26551/30000 Training Loss: 0.03971680998802185\n",
      "Epoch 26552/30000 Training Loss: 0.06264161318540573\n",
      "Epoch 26553/30000 Training Loss: 0.05035792291164398\n",
      "Epoch 26554/30000 Training Loss: 0.03946465998888016\n",
      "Epoch 26555/30000 Training Loss: 0.03969139605760574\n",
      "Epoch 26556/30000 Training Loss: 0.03923116251826286\n",
      "Epoch 26557/30000 Training Loss: 0.0438356027007103\n",
      "Epoch 26558/30000 Training Loss: 0.06897242367267609\n",
      "Epoch 26559/30000 Training Loss: 0.048605646938085556\n",
      "Epoch 26560/30000 Training Loss: 0.040259651839733124\n",
      "Epoch 26561/30000 Training Loss: 0.038836460560560226\n",
      "Epoch 26562/30000 Training Loss: 0.04161049798130989\n",
      "Epoch 26563/30000 Training Loss: 0.062035609036684036\n",
      "Epoch 26564/30000 Training Loss: 0.04164102301001549\n",
      "Epoch 26565/30000 Training Loss: 0.04273838922381401\n",
      "Epoch 26566/30000 Training Loss: 0.05817090719938278\n",
      "Epoch 26567/30000 Training Loss: 0.06199146807193756\n",
      "Epoch 26568/30000 Training Loss: 0.03643949329853058\n",
      "Epoch 26569/30000 Training Loss: 0.05485469847917557\n",
      "Epoch 26570/30000 Training Loss: 0.0550251267850399\n",
      "Epoch 26571/30000 Training Loss: 0.0564989298582077\n",
      "Epoch 26572/30000 Training Loss: 0.05555078014731407\n",
      "Epoch 26573/30000 Training Loss: 0.050996508449316025\n",
      "Epoch 26574/30000 Training Loss: 0.04829222708940506\n",
      "Epoch 26575/30000 Training Loss: 0.06376942992210388\n",
      "Epoch 26576/30000 Training Loss: 0.03490173816680908\n",
      "Epoch 26577/30000 Training Loss: 0.04511509835720062\n",
      "Epoch 26578/30000 Training Loss: 0.040173448622226715\n",
      "Epoch 26579/30000 Training Loss: 0.049024201929569244\n",
      "Epoch 26580/30000 Training Loss: 0.04408096522092819\n",
      "Epoch 26581/30000 Training Loss: 0.04830428957939148\n",
      "Epoch 26582/30000 Training Loss: 0.051083870232105255\n",
      "Epoch 26583/30000 Training Loss: 0.04781997948884964\n",
      "Epoch 26584/30000 Training Loss: 0.048464253544807434\n",
      "Epoch 26585/30000 Training Loss: 0.047790177166461945\n",
      "Epoch 26586/30000 Training Loss: 0.041041694581508636\n",
      "Epoch 26587/30000 Training Loss: 0.05643750727176666\n",
      "Epoch 26588/30000 Training Loss: 0.042471181601285934\n",
      "Epoch 26589/30000 Training Loss: 0.039290182292461395\n",
      "Epoch 26590/30000 Training Loss: 0.04076163470745087\n",
      "Epoch 26591/30000 Training Loss: 0.042670004069805145\n",
      "Epoch 26592/30000 Training Loss: 0.043624162673950195\n",
      "Epoch 26593/30000 Training Loss: 0.04298613965511322\n",
      "Epoch 26594/30000 Training Loss: 0.05714746192097664\n",
      "Epoch 26595/30000 Training Loss: 0.06465950608253479\n",
      "Epoch 26596/30000 Training Loss: 0.039675917476415634\n",
      "Epoch 26597/30000 Training Loss: 0.05307856947183609\n",
      "Epoch 26598/30000 Training Loss: 0.03719322755932808\n",
      "Epoch 26599/30000 Training Loss: 0.051371969282627106\n",
      "Epoch 26600/30000 Training Loss: 0.05521994084119797\n",
      "Epoch 26600/30000 Validation Loss: 0.06005264073610306\n",
      "Epoch 26601/30000 Training Loss: 0.05303865671157837\n",
      "Epoch 26602/30000 Training Loss: 0.03785445913672447\n",
      "Epoch 26603/30000 Training Loss: 0.041395753622055054\n",
      "Epoch 26604/30000 Training Loss: 0.04407840967178345\n",
      "Epoch 26605/30000 Training Loss: 0.03339683264493942\n",
      "Epoch 26606/30000 Training Loss: 0.052021607756614685\n",
      "Epoch 26607/30000 Training Loss: 0.06756719201803207\n",
      "Epoch 26608/30000 Training Loss: 0.03739870339632034\n",
      "Epoch 26609/30000 Training Loss: 0.04635639488697052\n",
      "Epoch 26610/30000 Training Loss: 0.043027665466070175\n",
      "Epoch 26611/30000 Training Loss: 0.05373124033212662\n",
      "Epoch 26612/30000 Training Loss: 0.04120364412665367\n",
      "Epoch 26613/30000 Training Loss: 0.04374760761857033\n",
      "Epoch 26614/30000 Training Loss: 0.05237722396850586\n",
      "Epoch 26615/30000 Training Loss: 0.04687943309545517\n",
      "Epoch 26616/30000 Training Loss: 0.04455193132162094\n",
      "Epoch 26617/30000 Training Loss: 0.054974332451820374\n",
      "Epoch 26618/30000 Training Loss: 0.04357815161347389\n",
      "Epoch 26619/30000 Training Loss: 0.05150952935218811\n",
      "Epoch 26620/30000 Training Loss: 0.04614027589559555\n",
      "Epoch 26621/30000 Training Loss: 0.04444887861609459\n",
      "Epoch 26622/30000 Training Loss: 0.05649906396865845\n",
      "Epoch 26623/30000 Training Loss: 0.04986409842967987\n",
      "Epoch 26624/30000 Training Loss: 0.04541463404893875\n",
      "Epoch 26625/30000 Training Loss: 0.0465066097676754\n",
      "Epoch 26626/30000 Training Loss: 0.045030053704977036\n",
      "Epoch 26627/30000 Training Loss: 0.0510944202542305\n",
      "Epoch 26628/30000 Training Loss: 0.04459051787853241\n",
      "Epoch 26629/30000 Training Loss: 0.046516772359609604\n",
      "Epoch 26630/30000 Training Loss: 0.03943243250250816\n",
      "Epoch 26631/30000 Training Loss: 0.05427481234073639\n",
      "Epoch 26632/30000 Training Loss: 0.05576658248901367\n",
      "Epoch 26633/30000 Training Loss: 0.03658613562583923\n",
      "Epoch 26634/30000 Training Loss: 0.03637870401144028\n",
      "Epoch 26635/30000 Training Loss: 0.027587903663516045\n",
      "Epoch 26636/30000 Training Loss: 0.042903345078229904\n",
      "Epoch 26637/30000 Training Loss: 0.032055843621492386\n",
      "Epoch 26638/30000 Training Loss: 0.026693515479564667\n",
      "Epoch 26639/30000 Training Loss: 0.05025390908122063\n",
      "Epoch 26640/30000 Training Loss: 0.045376550406217575\n",
      "Epoch 26641/30000 Training Loss: 0.058737143874168396\n",
      "Epoch 26642/30000 Training Loss: 0.045272476971149445\n",
      "Epoch 26643/30000 Training Loss: 0.03664001449942589\n",
      "Epoch 26644/30000 Training Loss: 0.05395243316888809\n",
      "Epoch 26645/30000 Training Loss: 0.04602040722966194\n",
      "Epoch 26646/30000 Training Loss: 0.02649899758398533\n",
      "Epoch 26647/30000 Training Loss: 0.041977714747190475\n",
      "Epoch 26648/30000 Training Loss: 0.04955943673849106\n",
      "Epoch 26649/30000 Training Loss: 0.03346497565507889\n",
      "Epoch 26650/30000 Training Loss: 0.04016704112291336\n",
      "Epoch 26651/30000 Training Loss: 0.05323069542646408\n",
      "Epoch 26652/30000 Training Loss: 0.04938694089651108\n",
      "Epoch 26653/30000 Training Loss: 0.04362699016928673\n",
      "Epoch 26654/30000 Training Loss: 0.04912834241986275\n",
      "Epoch 26655/30000 Training Loss: 0.04292365908622742\n",
      "Epoch 26656/30000 Training Loss: 0.04213312268257141\n",
      "Epoch 26657/30000 Training Loss: 0.048521533608436584\n",
      "Epoch 26658/30000 Training Loss: 0.05163859575986862\n",
      "Epoch 26659/30000 Training Loss: 0.043638989329338074\n",
      "Epoch 26660/30000 Training Loss: 0.04273732006549835\n",
      "Epoch 26661/30000 Training Loss: 0.07052852213382721\n",
      "Epoch 26662/30000 Training Loss: 0.05011626332998276\n",
      "Epoch 26663/30000 Training Loss: 0.045519568026065826\n",
      "Epoch 26664/30000 Training Loss: 0.05592212826013565\n",
      "Epoch 26665/30000 Training Loss: 0.048286072909832\n",
      "Epoch 26666/30000 Training Loss: 0.04190319404006004\n",
      "Epoch 26667/30000 Training Loss: 0.044367965310811996\n",
      "Epoch 26668/30000 Training Loss: 0.06212174519896507\n",
      "Epoch 26669/30000 Training Loss: 0.04924727603793144\n",
      "Epoch 26670/30000 Training Loss: 0.06591183692216873\n",
      "Epoch 26671/30000 Training Loss: 0.03958408534526825\n",
      "Epoch 26672/30000 Training Loss: 0.04728931933641434\n",
      "Epoch 26673/30000 Training Loss: 0.05110438913106918\n",
      "Epoch 26674/30000 Training Loss: 0.02447112649679184\n",
      "Epoch 26675/30000 Training Loss: 0.03877846896648407\n",
      "Epoch 26676/30000 Training Loss: 0.033548492938280106\n",
      "Epoch 26677/30000 Training Loss: 0.034974269568920135\n",
      "Epoch 26678/30000 Training Loss: 0.044378239661455154\n",
      "Epoch 26679/30000 Training Loss: 0.04777533560991287\n",
      "Epoch 26680/30000 Training Loss: 0.06611020117998123\n",
      "Epoch 26681/30000 Training Loss: 0.06099390238523483\n",
      "Epoch 26682/30000 Training Loss: 0.05518507584929466\n",
      "Epoch 26683/30000 Training Loss: 0.03591769188642502\n",
      "Epoch 26684/30000 Training Loss: 0.04096870869398117\n",
      "Epoch 26685/30000 Training Loss: 0.0472550168633461\n",
      "Epoch 26686/30000 Training Loss: 0.06295660883188248\n",
      "Epoch 26687/30000 Training Loss: 0.0380571074783802\n",
      "Epoch 26688/30000 Training Loss: 0.03684324771165848\n",
      "Epoch 26689/30000 Training Loss: 0.031226882711052895\n",
      "Epoch 26690/30000 Training Loss: 0.04279123991727829\n",
      "Epoch 26691/30000 Training Loss: 0.045235831290483475\n",
      "Epoch 26692/30000 Training Loss: 0.040009692311286926\n",
      "Epoch 26693/30000 Training Loss: 0.04029751569032669\n",
      "Epoch 26694/30000 Training Loss: 0.041669946163892746\n",
      "Epoch 26695/30000 Training Loss: 0.05817239359021187\n",
      "Epoch 26696/30000 Training Loss: 0.04596810042858124\n",
      "Epoch 26697/30000 Training Loss: 0.03967006132006645\n",
      "Epoch 26698/30000 Training Loss: 0.05284716188907623\n",
      "Epoch 26699/30000 Training Loss: 0.044337380677461624\n",
      "Epoch 26700/30000 Training Loss: 0.04766884446144104\n",
      "Epoch 26700/30000 Validation Loss: 0.039401825517416\n",
      "Epoch 26701/30000 Training Loss: 0.07608626782894135\n",
      "Epoch 26702/30000 Training Loss: 0.03341463953256607\n",
      "Epoch 26703/30000 Training Loss: 0.04121186211705208\n",
      "Epoch 26704/30000 Training Loss: 0.03970728814601898\n",
      "Epoch 26705/30000 Training Loss: 0.04921136051416397\n",
      "Epoch 26706/30000 Training Loss: 0.034196481108665466\n",
      "Epoch 26707/30000 Training Loss: 0.051325008273124695\n",
      "Epoch 26708/30000 Training Loss: 0.04355422779917717\n",
      "Epoch 26709/30000 Training Loss: 0.031272295862436295\n",
      "Epoch 26710/30000 Training Loss: 0.05797983705997467\n",
      "Epoch 26711/30000 Training Loss: 0.03636990487575531\n",
      "Epoch 26712/30000 Training Loss: 0.04383024945855141\n",
      "Epoch 26713/30000 Training Loss: 0.04410005733370781\n",
      "Epoch 26714/30000 Training Loss: 0.03222433477640152\n",
      "Epoch 26715/30000 Training Loss: 0.04655051976442337\n",
      "Epoch 26716/30000 Training Loss: 0.0385497584939003\n",
      "Epoch 26717/30000 Training Loss: 0.04820139706134796\n",
      "Epoch 26718/30000 Training Loss: 0.056667134165763855\n",
      "Epoch 26719/30000 Training Loss: 0.04413001239299774\n",
      "Epoch 26720/30000 Training Loss: 0.05442851409316063\n",
      "Epoch 26721/30000 Training Loss: 0.04021814465522766\n",
      "Epoch 26722/30000 Training Loss: 0.04579033702611923\n",
      "Epoch 26723/30000 Training Loss: 0.033679720014333725\n",
      "Epoch 26724/30000 Training Loss: 0.0312297772616148\n",
      "Epoch 26725/30000 Training Loss: 0.05036582052707672\n",
      "Epoch 26726/30000 Training Loss: 0.03574417904019356\n",
      "Epoch 26727/30000 Training Loss: 0.050005972385406494\n",
      "Epoch 26728/30000 Training Loss: 0.05240931361913681\n",
      "Epoch 26729/30000 Training Loss: 0.03902946785092354\n",
      "Epoch 26730/30000 Training Loss: 0.03835270181298256\n",
      "Epoch 26731/30000 Training Loss: 0.03679344430565834\n",
      "Epoch 26732/30000 Training Loss: 0.033784691244363785\n",
      "Epoch 26733/30000 Training Loss: 0.05266878753900528\n",
      "Epoch 26734/30000 Training Loss: 0.03974743187427521\n",
      "Epoch 26735/30000 Training Loss: 0.04655364528298378\n",
      "Epoch 26736/30000 Training Loss: 0.045278385281562805\n",
      "Epoch 26737/30000 Training Loss: 0.05051777511835098\n",
      "Epoch 26738/30000 Training Loss: 0.04025014117360115\n",
      "Epoch 26739/30000 Training Loss: 0.03513450175523758\n",
      "Epoch 26740/30000 Training Loss: 0.05052352324128151\n",
      "Epoch 26741/30000 Training Loss: 0.04101750627160072\n",
      "Epoch 26742/30000 Training Loss: 0.04904789477586746\n",
      "Epoch 26743/30000 Training Loss: 0.04485386610031128\n",
      "Epoch 26744/30000 Training Loss: 0.055885639041662216\n",
      "Epoch 26745/30000 Training Loss: 0.039669524878263474\n",
      "Epoch 26746/30000 Training Loss: 0.05709749460220337\n",
      "Epoch 26747/30000 Training Loss: 0.049335792660713196\n",
      "Epoch 26748/30000 Training Loss: 0.05757329612970352\n",
      "Epoch 26749/30000 Training Loss: 0.045607563108205795\n",
      "Epoch 26750/30000 Training Loss: 0.03670525550842285\n",
      "Epoch 26751/30000 Training Loss: 0.03731905296444893\n",
      "Epoch 26752/30000 Training Loss: 0.03827015310525894\n",
      "Epoch 26753/30000 Training Loss: 0.057809118181467056\n",
      "Epoch 26754/30000 Training Loss: 0.038962472230196\n",
      "Epoch 26755/30000 Training Loss: 0.03837861120700836\n",
      "Epoch 26756/30000 Training Loss: 0.046187080442905426\n",
      "Epoch 26757/30000 Training Loss: 0.04368511214852333\n",
      "Epoch 26758/30000 Training Loss: 0.046302251517772675\n",
      "Epoch 26759/30000 Training Loss: 0.04966767132282257\n",
      "Epoch 26760/30000 Training Loss: 0.03309622406959534\n",
      "Epoch 26761/30000 Training Loss: 0.06352513283491135\n",
      "Epoch 26762/30000 Training Loss: 0.03711418807506561\n",
      "Epoch 26763/30000 Training Loss: 0.037356629967689514\n",
      "Epoch 26764/30000 Training Loss: 0.04115773364901543\n",
      "Epoch 26765/30000 Training Loss: 0.031839169561862946\n",
      "Epoch 26766/30000 Training Loss: 0.0440438911318779\n",
      "Epoch 26767/30000 Training Loss: 0.04950975626707077\n",
      "Epoch 26768/30000 Training Loss: 0.04204470291733742\n",
      "Epoch 26769/30000 Training Loss: 0.05432917922735214\n",
      "Epoch 26770/30000 Training Loss: 0.04573297128081322\n",
      "Epoch 26771/30000 Training Loss: 0.03888200595974922\n",
      "Epoch 26772/30000 Training Loss: 0.03943170979619026\n",
      "Epoch 26773/30000 Training Loss: 0.055568769574165344\n",
      "Epoch 26774/30000 Training Loss: 0.04261201620101929\n",
      "Epoch 26775/30000 Training Loss: 0.060975316911935806\n",
      "Epoch 26776/30000 Training Loss: 0.04313478618860245\n",
      "Epoch 26777/30000 Training Loss: 0.03950702026486397\n",
      "Epoch 26778/30000 Training Loss: 0.041107065975666046\n",
      "Epoch 26779/30000 Training Loss: 0.029951773583889008\n",
      "Epoch 26780/30000 Training Loss: 0.054355502128601074\n",
      "Epoch 26781/30000 Training Loss: 0.04182719439268112\n",
      "Epoch 26782/30000 Training Loss: 0.040427837520837784\n",
      "Epoch 26783/30000 Training Loss: 0.06314978748559952\n",
      "Epoch 26784/30000 Training Loss: 0.03589380532503128\n",
      "Epoch 26785/30000 Training Loss: 0.04161406308412552\n",
      "Epoch 26786/30000 Training Loss: 0.05176818370819092\n",
      "Epoch 26787/30000 Training Loss: 0.05082814395427704\n",
      "Epoch 26788/30000 Training Loss: 0.04108085110783577\n",
      "Epoch 26789/30000 Training Loss: 0.043364930897951126\n",
      "Epoch 26790/30000 Training Loss: 0.04667805880308151\n",
      "Epoch 26791/30000 Training Loss: 0.049495939165353775\n",
      "Epoch 26792/30000 Training Loss: 0.05111107602715492\n",
      "Epoch 26793/30000 Training Loss: 0.047655876725912094\n",
      "Epoch 26794/30000 Training Loss: 0.048003241419792175\n",
      "Epoch 26795/30000 Training Loss: 0.04655013978481293\n",
      "Epoch 26796/30000 Training Loss: 0.037703271955251694\n",
      "Epoch 26797/30000 Training Loss: 0.055241454392671585\n",
      "Epoch 26798/30000 Training Loss: 0.04145045951008797\n",
      "Epoch 26799/30000 Training Loss: 0.041511476039886475\n",
      "Epoch 26800/30000 Training Loss: 0.048429735004901886\n",
      "Epoch 26800/30000 Validation Loss: 0.056288182735443115\n",
      "Epoch 26801/30000 Training Loss: 0.03699001669883728\n",
      "Epoch 26802/30000 Training Loss: 0.04314595088362694\n",
      "Epoch 26803/30000 Training Loss: 0.04401371255517006\n",
      "Epoch 26804/30000 Training Loss: 0.0515652596950531\n",
      "Epoch 26805/30000 Training Loss: 0.04879387468099594\n",
      "Epoch 26806/30000 Training Loss: 0.033695898950099945\n",
      "Epoch 26807/30000 Training Loss: 0.050426796078681946\n",
      "Epoch 26808/30000 Training Loss: 0.050663791596889496\n",
      "Epoch 26809/30000 Training Loss: 0.057403020560741425\n",
      "Epoch 26810/30000 Training Loss: 0.05132153630256653\n",
      "Epoch 26811/30000 Training Loss: 0.043841347098350525\n",
      "Epoch 26812/30000 Training Loss: 0.04779844358563423\n",
      "Epoch 26813/30000 Training Loss: 0.04247181490063667\n",
      "Epoch 26814/30000 Training Loss: 0.04515650123357773\n",
      "Epoch 26815/30000 Training Loss: 0.03844863548874855\n",
      "Epoch 26816/30000 Training Loss: 0.05630066990852356\n",
      "Epoch 26817/30000 Training Loss: 0.03651352971792221\n",
      "Epoch 26818/30000 Training Loss: 0.04535456746816635\n",
      "Epoch 26819/30000 Training Loss: 0.0457145981490612\n",
      "Epoch 26820/30000 Training Loss: 0.05718299746513367\n",
      "Epoch 26821/30000 Training Loss: 0.05050382763147354\n",
      "Epoch 26822/30000 Training Loss: 0.06090330332517624\n",
      "Epoch 26823/30000 Training Loss: 0.03235932067036629\n",
      "Epoch 26824/30000 Training Loss: 0.05469201132655144\n",
      "Epoch 26825/30000 Training Loss: 0.038157641887664795\n",
      "Epoch 26826/30000 Training Loss: 0.05613977462053299\n",
      "Epoch 26827/30000 Training Loss: 0.03493206202983856\n",
      "Epoch 26828/30000 Training Loss: 0.05105302110314369\n",
      "Epoch 26829/30000 Training Loss: 0.049904339015483856\n",
      "Epoch 26830/30000 Training Loss: 0.043493714183568954\n",
      "Epoch 26831/30000 Training Loss: 0.04834771156311035\n",
      "Epoch 26832/30000 Training Loss: 0.05137399211525917\n",
      "Epoch 26833/30000 Training Loss: 0.04056711494922638\n",
      "Epoch 26834/30000 Training Loss: 0.05544287711381912\n",
      "Epoch 26835/30000 Training Loss: 0.04297518730163574\n",
      "Epoch 26836/30000 Training Loss: 0.04537081718444824\n",
      "Epoch 26837/30000 Training Loss: 0.05444127693772316\n",
      "Epoch 26838/30000 Training Loss: 0.05786392465233803\n",
      "Epoch 26839/30000 Training Loss: 0.04348630830645561\n",
      "Epoch 26840/30000 Training Loss: 0.04108206182718277\n",
      "Epoch 26841/30000 Training Loss: 0.054153796285390854\n",
      "Epoch 26842/30000 Training Loss: 0.035456582903862\n",
      "Epoch 26843/30000 Training Loss: 0.057802505791187286\n",
      "Epoch 26844/30000 Training Loss: 0.04581531137228012\n",
      "Epoch 26845/30000 Training Loss: 0.04388860985636711\n",
      "Epoch 26846/30000 Training Loss: 0.04627639055252075\n",
      "Epoch 26847/30000 Training Loss: 0.05423436686396599\n",
      "Epoch 26848/30000 Training Loss: 0.035701412707567215\n",
      "Epoch 26849/30000 Training Loss: 0.04265148565173149\n",
      "Epoch 26850/30000 Training Loss: 0.052466392517089844\n",
      "Epoch 26851/30000 Training Loss: 0.056623414158821106\n",
      "Epoch 26852/30000 Training Loss: 0.044674649834632874\n",
      "Epoch 26853/30000 Training Loss: 0.061399877071380615\n",
      "Epoch 26854/30000 Training Loss: 0.04075274616479874\n",
      "Epoch 26855/30000 Training Loss: 0.04941925033926964\n",
      "Epoch 26856/30000 Training Loss: 0.044187210500240326\n",
      "Epoch 26857/30000 Training Loss: 0.047381773591041565\n",
      "Epoch 26858/30000 Training Loss: 0.06166601553559303\n",
      "Epoch 26859/30000 Training Loss: 0.04599891975522041\n",
      "Epoch 26860/30000 Training Loss: 0.04774956405162811\n",
      "Epoch 26861/30000 Training Loss: 0.04137402027845383\n",
      "Epoch 26862/30000 Training Loss: 0.03868062421679497\n",
      "Epoch 26863/30000 Training Loss: 0.04162045940756798\n",
      "Epoch 26864/30000 Training Loss: 0.04108469933271408\n",
      "Epoch 26865/30000 Training Loss: 0.055830877274274826\n",
      "Epoch 26866/30000 Training Loss: 0.04252779483795166\n",
      "Epoch 26867/30000 Training Loss: 0.05058133602142334\n",
      "Epoch 26868/30000 Training Loss: 0.030501611530780792\n",
      "Epoch 26869/30000 Training Loss: 0.04061875864863396\n",
      "Epoch 26870/30000 Training Loss: 0.044688958674669266\n",
      "Epoch 26871/30000 Training Loss: 0.052189454436302185\n",
      "Epoch 26872/30000 Training Loss: 0.055234335362911224\n",
      "Epoch 26873/30000 Training Loss: 0.0487622395157814\n",
      "Epoch 26874/30000 Training Loss: 0.04013100266456604\n",
      "Epoch 26875/30000 Training Loss: 0.06545070558786392\n",
      "Epoch 26876/30000 Training Loss: 0.04496656730771065\n",
      "Epoch 26877/30000 Training Loss: 0.04526056349277496\n",
      "Epoch 26878/30000 Training Loss: 0.040711160749197006\n",
      "Epoch 26879/30000 Training Loss: 0.04450917989015579\n",
      "Epoch 26880/30000 Training Loss: 0.04309730604290962\n",
      "Epoch 26881/30000 Training Loss: 0.03753020241856575\n",
      "Epoch 26882/30000 Training Loss: 0.056575093418359756\n",
      "Epoch 26883/30000 Training Loss: 0.041119374334812164\n",
      "Epoch 26884/30000 Training Loss: 0.04325903207063675\n",
      "Epoch 26885/30000 Training Loss: 0.04500846564769745\n",
      "Epoch 26886/30000 Training Loss: 0.054033201187849045\n",
      "Epoch 26887/30000 Training Loss: 0.036317817866802216\n",
      "Epoch 26888/30000 Training Loss: 0.05847334489226341\n",
      "Epoch 26889/30000 Training Loss: 0.03531760722398758\n",
      "Epoch 26890/30000 Training Loss: 0.03583219647407532\n",
      "Epoch 26891/30000 Training Loss: 0.04636482149362564\n",
      "Epoch 26892/30000 Training Loss: 0.047694940119981766\n",
      "Epoch 26893/30000 Training Loss: 0.03553658723831177\n",
      "Epoch 26894/30000 Training Loss: 0.0434005931019783\n",
      "Epoch 26895/30000 Training Loss: 0.0492621585726738\n",
      "Epoch 26896/30000 Training Loss: 0.03394971042871475\n",
      "Epoch 26897/30000 Training Loss: 0.03640604019165039\n",
      "Epoch 26898/30000 Training Loss: 0.04522695764899254\n",
      "Epoch 26899/30000 Training Loss: 0.04767635464668274\n",
      "Epoch 26900/30000 Training Loss: 0.05506276339292526\n",
      "Epoch 26900/30000 Validation Loss: 0.05902308225631714\n",
      "Epoch 26901/30000 Training Loss: 0.048787377774715424\n",
      "Epoch 26902/30000 Training Loss: 0.05089936777949333\n",
      "Epoch 26903/30000 Training Loss: 0.048494525253772736\n",
      "Epoch 26904/30000 Training Loss: 0.045982781797647476\n",
      "Epoch 26905/30000 Training Loss: 0.0347052738070488\n",
      "Epoch 26906/30000 Training Loss: 0.048875100910663605\n",
      "Epoch 26907/30000 Training Loss: 0.04467647522687912\n",
      "Epoch 26908/30000 Training Loss: 0.050347644835710526\n",
      "Epoch 26909/30000 Training Loss: 0.05681880563497543\n",
      "Epoch 26910/30000 Training Loss: 0.044105179607868195\n",
      "Epoch 26911/30000 Training Loss: 0.043620042502880096\n",
      "Epoch 26912/30000 Training Loss: 0.055854856967926025\n",
      "Epoch 26913/30000 Training Loss: 0.048371076583862305\n",
      "Epoch 26914/30000 Training Loss: 0.03246523067355156\n",
      "Epoch 26915/30000 Training Loss: 0.03539876639842987\n",
      "Epoch 26916/30000 Training Loss: 0.0416850671172142\n",
      "Epoch 26917/30000 Training Loss: 0.03923530876636505\n",
      "Epoch 26918/30000 Training Loss: 0.04743233323097229\n",
      "Epoch 26919/30000 Training Loss: 0.054008759558200836\n",
      "Epoch 26920/30000 Training Loss: 0.04160269349813461\n",
      "Epoch 26921/30000 Training Loss: 0.045611899346113205\n",
      "Epoch 26922/30000 Training Loss: 0.04446696862578392\n",
      "Epoch 26923/30000 Training Loss: 0.04958595708012581\n",
      "Epoch 26924/30000 Training Loss: 0.04621417447924614\n",
      "Epoch 26925/30000 Training Loss: 0.04343405365943909\n",
      "Epoch 26926/30000 Training Loss: 0.05276263505220413\n",
      "Epoch 26927/30000 Training Loss: 0.04723602533340454\n",
      "Epoch 26928/30000 Training Loss: 0.05399441346526146\n",
      "Epoch 26929/30000 Training Loss: 0.03989749029278755\n",
      "Epoch 26930/30000 Training Loss: 0.037414632737636566\n",
      "Epoch 26931/30000 Training Loss: 0.04371166229248047\n",
      "Epoch 26932/30000 Training Loss: 0.055863745510578156\n",
      "Epoch 26933/30000 Training Loss: 0.07508639246225357\n",
      "Epoch 26934/30000 Training Loss: 0.0432475209236145\n",
      "Epoch 26935/30000 Training Loss: 0.04484138637781143\n",
      "Epoch 26936/30000 Training Loss: 0.05947640910744667\n",
      "Epoch 26937/30000 Training Loss: 0.048852160573005676\n",
      "Epoch 26938/30000 Training Loss: 0.03541884943842888\n",
      "Epoch 26939/30000 Training Loss: 0.04036781191825867\n",
      "Epoch 26940/30000 Training Loss: 0.04169352352619171\n",
      "Epoch 26941/30000 Training Loss: 0.045352086424827576\n",
      "Epoch 26942/30000 Training Loss: 0.03680548071861267\n",
      "Epoch 26943/30000 Training Loss: 0.04576866701245308\n",
      "Epoch 26944/30000 Training Loss: 0.052924998104572296\n",
      "Epoch 26945/30000 Training Loss: 0.034807994961738586\n",
      "Epoch 26946/30000 Training Loss: 0.04649552330374718\n",
      "Epoch 26947/30000 Training Loss: 0.05095827579498291\n",
      "Epoch 26948/30000 Training Loss: 0.07189758867025375\n",
      "Epoch 26949/30000 Training Loss: 0.06035264581441879\n",
      "Epoch 26950/30000 Training Loss: 0.0477854385972023\n",
      "Epoch 26951/30000 Training Loss: 0.03797237202525139\n",
      "Epoch 26952/30000 Training Loss: 0.04224521294236183\n",
      "Epoch 26953/30000 Training Loss: 0.043667711317539215\n",
      "Epoch 26954/30000 Training Loss: 0.04392602667212486\n",
      "Epoch 26955/30000 Training Loss: 0.05263266712427139\n",
      "Epoch 26956/30000 Training Loss: 0.05384930968284607\n",
      "Epoch 26957/30000 Training Loss: 0.042382754385471344\n",
      "Epoch 26958/30000 Training Loss: 0.05176190286874771\n",
      "Epoch 26959/30000 Training Loss: 0.043834488838911057\n",
      "Epoch 26960/30000 Training Loss: 0.03653371334075928\n",
      "Epoch 26961/30000 Training Loss: 0.06299857795238495\n",
      "Epoch 26962/30000 Training Loss: 0.05931985378265381\n",
      "Epoch 26963/30000 Training Loss: 0.05112644284963608\n",
      "Epoch 26964/30000 Training Loss: 0.03800424933433533\n",
      "Epoch 26965/30000 Training Loss: 0.059099964797496796\n",
      "Epoch 26966/30000 Training Loss: 0.049139514565467834\n",
      "Epoch 26967/30000 Training Loss: 0.042695265263319016\n",
      "Epoch 26968/30000 Training Loss: 0.045366741716861725\n",
      "Epoch 26969/30000 Training Loss: 0.04342779517173767\n",
      "Epoch 26970/30000 Training Loss: 0.05245085060596466\n",
      "Epoch 26971/30000 Training Loss: 0.049525484442710876\n",
      "Epoch 26972/30000 Training Loss: 0.04370445758104324\n",
      "Epoch 26973/30000 Training Loss: 0.049284785985946655\n",
      "Epoch 26974/30000 Training Loss: 0.05017782747745514\n",
      "Epoch 26975/30000 Training Loss: 0.04935561120510101\n",
      "Epoch 26976/30000 Training Loss: 0.046969011425971985\n",
      "Epoch 26977/30000 Training Loss: 0.046908214688301086\n",
      "Epoch 26978/30000 Training Loss: 0.03234634920954704\n",
      "Epoch 26979/30000 Training Loss: 0.04174189269542694\n",
      "Epoch 26980/30000 Training Loss: 0.053238123655319214\n",
      "Epoch 26981/30000 Training Loss: 0.0464625284075737\n",
      "Epoch 26982/30000 Training Loss: 0.037770695984363556\n",
      "Epoch 26983/30000 Training Loss: 0.04538559913635254\n",
      "Epoch 26984/30000 Training Loss: 0.036256324499845505\n",
      "Epoch 26985/30000 Training Loss: 0.0374612957239151\n",
      "Epoch 26986/30000 Training Loss: 0.03598218411207199\n",
      "Epoch 26987/30000 Training Loss: 0.057033099234104156\n",
      "Epoch 26988/30000 Training Loss: 0.059504613280296326\n",
      "Epoch 26989/30000 Training Loss: 0.055856697261333466\n",
      "Epoch 26990/30000 Training Loss: 0.04202454164624214\n",
      "Epoch 26991/30000 Training Loss: 0.052065860480070114\n",
      "Epoch 26992/30000 Training Loss: 0.03915918245911598\n",
      "Epoch 26993/30000 Training Loss: 0.05435258895158768\n",
      "Epoch 26994/30000 Training Loss: 0.041939571499824524\n",
      "Epoch 26995/30000 Training Loss: 0.045536283403635025\n",
      "Epoch 26996/30000 Training Loss: 0.03463180735707283\n",
      "Epoch 26997/30000 Training Loss: 0.03882722556591034\n",
      "Epoch 26998/30000 Training Loss: 0.037158843129873276\n",
      "Epoch 26999/30000 Training Loss: 0.042909037321805954\n",
      "Epoch 27000/30000 Training Loss: 0.05820504575967789\n",
      "Epoch 27000/30000 Validation Loss: 0.04375423863530159\n",
      "Epoch 27001/30000 Training Loss: 0.03452783823013306\n",
      "Epoch 27002/30000 Training Loss: 0.029921527951955795\n",
      "Epoch 27003/30000 Training Loss: 0.04042745754122734\n",
      "Epoch 27004/30000 Training Loss: 0.05563678592443466\n",
      "Epoch 27005/30000 Training Loss: 0.051699019968509674\n",
      "Epoch 27006/30000 Training Loss: 0.0562923327088356\n",
      "Epoch 27007/30000 Training Loss: 0.051660146564245224\n",
      "Epoch 27008/30000 Training Loss: 0.0446443185210228\n",
      "Epoch 27009/30000 Training Loss: 0.04204552620649338\n",
      "Epoch 27010/30000 Training Loss: 0.06047572195529938\n",
      "Epoch 27011/30000 Training Loss: 0.05054581165313721\n",
      "Epoch 27012/30000 Training Loss: 0.06309258937835693\n",
      "Epoch 27013/30000 Training Loss: 0.045924168080091476\n",
      "Epoch 27014/30000 Training Loss: 0.037974681705236435\n",
      "Epoch 27015/30000 Training Loss: 0.045956868678331375\n",
      "Epoch 27016/30000 Training Loss: 0.03456868231296539\n",
      "Epoch 27017/30000 Training Loss: 0.05369434878230095\n",
      "Epoch 27018/30000 Training Loss: 0.04126681387424469\n",
      "Epoch 27019/30000 Training Loss: 0.037898555397987366\n",
      "Epoch 27020/30000 Training Loss: 0.032846905291080475\n",
      "Epoch 27021/30000 Training Loss: 0.052679818123579025\n",
      "Epoch 27022/30000 Training Loss: 0.04491154104471207\n",
      "Epoch 27023/30000 Training Loss: 0.045915357768535614\n",
      "Epoch 27024/30000 Training Loss: 0.04907532408833504\n",
      "Epoch 27025/30000 Training Loss: 0.060909368097782135\n",
      "Epoch 27026/30000 Training Loss: 0.05690998584032059\n",
      "Epoch 27027/30000 Training Loss: 0.035810865461826324\n",
      "Epoch 27028/30000 Training Loss: 0.04605083912611008\n",
      "Epoch 27029/30000 Training Loss: 0.04042869806289673\n",
      "Epoch 27030/30000 Training Loss: 0.043883584439754486\n",
      "Epoch 27031/30000 Training Loss: 0.04000110924243927\n",
      "Epoch 27032/30000 Training Loss: 0.04251132532954216\n",
      "Epoch 27033/30000 Training Loss: 0.0446859709918499\n",
      "Epoch 27034/30000 Training Loss: 0.06210002303123474\n",
      "Epoch 27035/30000 Training Loss: 0.037163183093070984\n",
      "Epoch 27036/30000 Training Loss: 0.04492957144975662\n",
      "Epoch 27037/30000 Training Loss: 0.04035075753927231\n",
      "Epoch 27038/30000 Training Loss: 0.04090932011604309\n",
      "Epoch 27039/30000 Training Loss: 0.05438019335269928\n",
      "Epoch 27040/30000 Training Loss: 0.0496748685836792\n",
      "Epoch 27041/30000 Training Loss: 0.050240762531757355\n",
      "Epoch 27042/30000 Training Loss: 0.04640612378716469\n",
      "Epoch 27043/30000 Training Loss: 0.040519654750823975\n",
      "Epoch 27044/30000 Training Loss: 0.04910251498222351\n",
      "Epoch 27045/30000 Training Loss: 0.037330590188503265\n",
      "Epoch 27046/30000 Training Loss: 0.031162869185209274\n",
      "Epoch 27047/30000 Training Loss: 0.04280064254999161\n",
      "Epoch 27048/30000 Training Loss: 0.05451923608779907\n",
      "Epoch 27049/30000 Training Loss: 0.03594530001282692\n",
      "Epoch 27050/30000 Training Loss: 0.03718818351626396\n",
      "Epoch 27051/30000 Training Loss: 0.03868437930941582\n",
      "Epoch 27052/30000 Training Loss: 0.04143869876861572\n",
      "Epoch 27053/30000 Training Loss: 0.04403204470872879\n",
      "Epoch 27054/30000 Training Loss: 0.03719106316566467\n",
      "Epoch 27055/30000 Training Loss: 0.03764796257019043\n",
      "Epoch 27056/30000 Training Loss: 0.04815080761909485\n",
      "Epoch 27057/30000 Training Loss: 0.0397639200091362\n",
      "Epoch 27058/30000 Training Loss: 0.0550624318420887\n",
      "Epoch 27059/30000 Training Loss: 0.05399150401353836\n",
      "Epoch 27060/30000 Training Loss: 0.043398696929216385\n",
      "Epoch 27061/30000 Training Loss: 0.04367714375257492\n",
      "Epoch 27062/30000 Training Loss: 0.03705357015132904\n",
      "Epoch 27063/30000 Training Loss: 0.04548265412449837\n",
      "Epoch 27064/30000 Training Loss: 0.051059454679489136\n",
      "Epoch 27065/30000 Training Loss: 0.03970811516046524\n",
      "Epoch 27066/30000 Training Loss: 0.04211067035794258\n",
      "Epoch 27067/30000 Training Loss: 0.03837519884109497\n",
      "Epoch 27068/30000 Training Loss: 0.07229449599981308\n",
      "Epoch 27069/30000 Training Loss: 0.03523722663521767\n",
      "Epoch 27070/30000 Training Loss: 0.059723060578107834\n",
      "Epoch 27071/30000 Training Loss: 0.04205763339996338\n",
      "Epoch 27072/30000 Training Loss: 0.056946203112602234\n",
      "Epoch 27073/30000 Training Loss: 0.036212027072906494\n",
      "Epoch 27074/30000 Training Loss: 0.03510698676109314\n",
      "Epoch 27075/30000 Training Loss: 0.055871039628982544\n",
      "Epoch 27076/30000 Training Loss: 0.04063861444592476\n",
      "Epoch 27077/30000 Training Loss: 0.037914082407951355\n",
      "Epoch 27078/30000 Training Loss: 0.04082770645618439\n",
      "Epoch 27079/30000 Training Loss: 0.030786190181970596\n",
      "Epoch 27080/30000 Training Loss: 0.0491708368062973\n",
      "Epoch 27081/30000 Training Loss: 0.03608459606766701\n",
      "Epoch 27082/30000 Training Loss: 0.05770786479115486\n",
      "Epoch 27083/30000 Training Loss: 0.04014543816447258\n",
      "Epoch 27084/30000 Training Loss: 0.04177816957235336\n",
      "Epoch 27085/30000 Training Loss: 0.029955174773931503\n",
      "Epoch 27086/30000 Training Loss: 0.04367896169424057\n",
      "Epoch 27087/30000 Training Loss: 0.05149339884519577\n",
      "Epoch 27088/30000 Training Loss: 0.044182054698467255\n",
      "Epoch 27089/30000 Training Loss: 0.037199966609478\n",
      "Epoch 27090/30000 Training Loss: 0.03891828656196594\n",
      "Epoch 27091/30000 Training Loss: 0.058944351971149445\n",
      "Epoch 27092/30000 Training Loss: 0.04300612583756447\n",
      "Epoch 27093/30000 Training Loss: 0.04585986211895943\n",
      "Epoch 27094/30000 Training Loss: 0.04550342634320259\n",
      "Epoch 27095/30000 Training Loss: 0.03806423768401146\n",
      "Epoch 27096/30000 Training Loss: 0.0598248727619648\n",
      "Epoch 27097/30000 Training Loss: 0.05030520632863045\n",
      "Epoch 27098/30000 Training Loss: 0.04402622953057289\n",
      "Epoch 27099/30000 Training Loss: 0.045008428394794464\n",
      "Epoch 27100/30000 Training Loss: 0.03512542322278023\n",
      "Epoch 27100/30000 Validation Loss: 0.03858018293976784\n",
      "Epoch 27101/30000 Training Loss: 0.052907880395650864\n",
      "Epoch 27102/30000 Training Loss: 0.04058059677481651\n",
      "Epoch 27103/30000 Training Loss: 0.05172254890203476\n",
      "Epoch 27104/30000 Training Loss: 0.045575812458992004\n",
      "Epoch 27105/30000 Training Loss: 0.036571383476257324\n",
      "Epoch 27106/30000 Training Loss: 0.036821216344833374\n",
      "Epoch 27107/30000 Training Loss: 0.04973930865526199\n",
      "Epoch 27108/30000 Training Loss: 0.062282007187604904\n",
      "Epoch 27109/30000 Training Loss: 0.05081924796104431\n",
      "Epoch 27110/30000 Training Loss: 0.036800649017095566\n",
      "Epoch 27111/30000 Training Loss: 0.05659016966819763\n",
      "Epoch 27112/30000 Training Loss: 0.04858468845486641\n",
      "Epoch 27113/30000 Training Loss: 0.046568408608436584\n",
      "Epoch 27114/30000 Training Loss: 0.059772975742816925\n",
      "Epoch 27115/30000 Training Loss: 0.053821392357349396\n",
      "Epoch 27116/30000 Training Loss: 0.036690883338451385\n",
      "Epoch 27117/30000 Training Loss: 0.04801635071635246\n",
      "Epoch 27118/30000 Training Loss: 0.0525917112827301\n",
      "Epoch 27119/30000 Training Loss: 0.03430788964033127\n",
      "Epoch 27120/30000 Training Loss: 0.0622745156288147\n",
      "Epoch 27121/30000 Training Loss: 0.04282836988568306\n",
      "Epoch 27122/30000 Training Loss: 0.04164541885256767\n",
      "Epoch 27123/30000 Training Loss: 0.05259659141302109\n",
      "Epoch 27124/30000 Training Loss: 0.037355806678533554\n",
      "Epoch 27125/30000 Training Loss: 0.040444422513246536\n",
      "Epoch 27126/30000 Training Loss: 0.029079098254442215\n",
      "Epoch 27127/30000 Training Loss: 0.04676154628396034\n",
      "Epoch 27128/30000 Training Loss: 0.052455220371484756\n",
      "Epoch 27129/30000 Training Loss: 0.046367332339286804\n",
      "Epoch 27130/30000 Training Loss: 0.03939681500196457\n",
      "Epoch 27131/30000 Training Loss: 0.05984093248844147\n",
      "Epoch 27132/30000 Training Loss: 0.039449598640203476\n",
      "Epoch 27133/30000 Training Loss: 0.03525962680578232\n",
      "Epoch 27134/30000 Training Loss: 0.040226150304079056\n",
      "Epoch 27135/30000 Training Loss: 0.041308823972940445\n",
      "Epoch 27136/30000 Training Loss: 0.0504130944609642\n",
      "Epoch 27137/30000 Training Loss: 0.041784197092056274\n",
      "Epoch 27138/30000 Training Loss: 0.02918837033212185\n",
      "Epoch 27139/30000 Training Loss: 0.04996329918503761\n",
      "Epoch 27140/30000 Training Loss: 0.05836378410458565\n",
      "Epoch 27141/30000 Training Loss: 0.032527856528759\n",
      "Epoch 27142/30000 Training Loss: 0.03471115231513977\n",
      "Epoch 27143/30000 Training Loss: 0.045958295464515686\n",
      "Epoch 27144/30000 Training Loss: 0.029866984114050865\n",
      "Epoch 27145/30000 Training Loss: 0.036155544221401215\n",
      "Epoch 27146/30000 Training Loss: 0.0497589185833931\n",
      "Epoch 27147/30000 Training Loss: 0.04142589494585991\n",
      "Epoch 27148/30000 Training Loss: 0.0622171126306057\n",
      "Epoch 27149/30000 Training Loss: 0.04169652238488197\n",
      "Epoch 27150/30000 Training Loss: 0.04922965168952942\n",
      "Epoch 27151/30000 Training Loss: 0.056780293583869934\n",
      "Epoch 27152/30000 Training Loss: 0.048616357147693634\n",
      "Epoch 27153/30000 Training Loss: 0.03536982461810112\n",
      "Epoch 27154/30000 Training Loss: 0.05643134564161301\n",
      "Epoch 27155/30000 Training Loss: 0.0404711589217186\n",
      "Epoch 27156/30000 Training Loss: 0.036205191165208817\n",
      "Epoch 27157/30000 Training Loss: 0.037736013531684875\n",
      "Epoch 27158/30000 Training Loss: 0.052597418427467346\n",
      "Epoch 27159/30000 Training Loss: 0.04358343034982681\n",
      "Epoch 27160/30000 Training Loss: 0.05670502409338951\n",
      "Epoch 27161/30000 Training Loss: 0.05998583510518074\n",
      "Epoch 27162/30000 Training Loss: 0.05038023740053177\n",
      "Epoch 27163/30000 Training Loss: 0.03387070074677467\n",
      "Epoch 27164/30000 Training Loss: 0.04912194237112999\n",
      "Epoch 27165/30000 Training Loss: 0.04116366058588028\n",
      "Epoch 27166/30000 Training Loss: 0.035768989473581314\n",
      "Epoch 27167/30000 Training Loss: 0.04348132386803627\n",
      "Epoch 27168/30000 Training Loss: 0.03945242241024971\n",
      "Epoch 27169/30000 Training Loss: 0.043699055910110474\n",
      "Epoch 27170/30000 Training Loss: 0.03396625444293022\n",
      "Epoch 27171/30000 Training Loss: 0.03480299189686775\n",
      "Epoch 27172/30000 Training Loss: 0.0522620715200901\n",
      "Epoch 27173/30000 Training Loss: 0.04149691388010979\n",
      "Epoch 27174/30000 Training Loss: 0.03782257065176964\n",
      "Epoch 27175/30000 Training Loss: 0.03900659456849098\n",
      "Epoch 27176/30000 Training Loss: 0.045590318739414215\n",
      "Epoch 27177/30000 Training Loss: 0.05008784681558609\n",
      "Epoch 27178/30000 Training Loss: 0.05040762573480606\n",
      "Epoch 27179/30000 Training Loss: 0.03660917654633522\n",
      "Epoch 27180/30000 Training Loss: 0.04224367067217827\n",
      "Epoch 27181/30000 Training Loss: 0.04494313523173332\n",
      "Epoch 27182/30000 Training Loss: 0.03436097502708435\n",
      "Epoch 27183/30000 Training Loss: 0.038117147982120514\n",
      "Epoch 27184/30000 Training Loss: 0.052397020161151886\n",
      "Epoch 27185/30000 Training Loss: 0.03971783071756363\n",
      "Epoch 27186/30000 Training Loss: 0.05488218367099762\n",
      "Epoch 27187/30000 Training Loss: 0.03925677016377449\n",
      "Epoch 27188/30000 Training Loss: 0.038300421088933945\n",
      "Epoch 27189/30000 Training Loss: 0.03334944695234299\n",
      "Epoch 27190/30000 Training Loss: 0.026927409693598747\n",
      "Epoch 27191/30000 Training Loss: 0.03552093729376793\n",
      "Epoch 27192/30000 Training Loss: 0.038166701793670654\n",
      "Epoch 27193/30000 Training Loss: 0.04989613592624664\n",
      "Epoch 27194/30000 Training Loss: 0.04627440497279167\n",
      "Epoch 27195/30000 Training Loss: 0.04236038774251938\n",
      "Epoch 27196/30000 Training Loss: 0.04142222926020622\n",
      "Epoch 27197/30000 Training Loss: 0.05935923755168915\n",
      "Epoch 27198/30000 Training Loss: 0.04889589548110962\n",
      "Epoch 27199/30000 Training Loss: 0.050633303821086884\n",
      "Epoch 27200/30000 Training Loss: 0.04093393683433533\n",
      "Epoch 27200/30000 Validation Loss: 0.04500496760010719\n",
      "Epoch 27201/30000 Training Loss: 0.05070614814758301\n",
      "Epoch 27202/30000 Training Loss: 0.044159822165966034\n",
      "Epoch 27203/30000 Training Loss: 0.053312644362449646\n",
      "Epoch 27204/30000 Training Loss: 0.05565126985311508\n",
      "Epoch 27205/30000 Training Loss: 0.048071254044771194\n",
      "Epoch 27206/30000 Training Loss: 0.048247624188661575\n",
      "Epoch 27207/30000 Training Loss: 0.0327085517346859\n",
      "Epoch 27208/30000 Training Loss: 0.03849291056394577\n",
      "Epoch 27209/30000 Training Loss: 0.062328845262527466\n",
      "Epoch 27210/30000 Training Loss: 0.039199620485305786\n",
      "Epoch 27211/30000 Training Loss: 0.03367622196674347\n",
      "Epoch 27212/30000 Training Loss: 0.054188549518585205\n",
      "Epoch 27213/30000 Training Loss: 0.03734436631202698\n",
      "Epoch 27214/30000 Training Loss: 0.043072767555713654\n",
      "Epoch 27215/30000 Training Loss: 0.034661389887332916\n",
      "Epoch 27216/30000 Training Loss: 0.042577825486660004\n",
      "Epoch 27217/30000 Training Loss: 0.045235514640808105\n",
      "Epoch 27218/30000 Training Loss: 0.04491749405860901\n",
      "Epoch 27219/30000 Training Loss: 0.06028834357857704\n",
      "Epoch 27220/30000 Training Loss: 0.03307611122727394\n",
      "Epoch 27221/30000 Training Loss: 0.04790527746081352\n",
      "Epoch 27222/30000 Training Loss: 0.04473229497671127\n",
      "Epoch 27223/30000 Training Loss: 0.04233653098344803\n",
      "Epoch 27224/30000 Training Loss: 0.04731162637472153\n",
      "Epoch 27225/30000 Training Loss: 0.04486561194062233\n",
      "Epoch 27226/30000 Training Loss: 0.0315018966794014\n",
      "Epoch 27227/30000 Training Loss: 0.05372731387615204\n",
      "Epoch 27228/30000 Training Loss: 0.050557464361190796\n",
      "Epoch 27229/30000 Training Loss: 0.0398993045091629\n",
      "Epoch 27230/30000 Training Loss: 0.04057230055332184\n",
      "Epoch 27231/30000 Training Loss: 0.043927356600761414\n",
      "Epoch 27232/30000 Training Loss: 0.04246632009744644\n",
      "Epoch 27233/30000 Training Loss: 0.04156789556145668\n",
      "Epoch 27234/30000 Training Loss: 0.03953408822417259\n",
      "Epoch 27235/30000 Training Loss: 0.05219491943717003\n",
      "Epoch 27236/30000 Training Loss: 0.04957621544599533\n",
      "Epoch 27237/30000 Training Loss: 0.05944157391786575\n",
      "Epoch 27238/30000 Training Loss: 0.06517577916383743\n",
      "Epoch 27239/30000 Training Loss: 0.034818943589925766\n",
      "Epoch 27240/30000 Training Loss: 0.05663223937153816\n",
      "Epoch 27241/30000 Training Loss: 0.03479913994669914\n",
      "Epoch 27242/30000 Training Loss: 0.04418231546878815\n",
      "Epoch 27243/30000 Training Loss: 0.04624899849295616\n",
      "Epoch 27244/30000 Training Loss: 0.04910571873188019\n",
      "Epoch 27245/30000 Training Loss: 0.03214094042778015\n",
      "Epoch 27246/30000 Training Loss: 0.04777686670422554\n",
      "Epoch 27247/30000 Training Loss: 0.035310350358486176\n",
      "Epoch 27248/30000 Training Loss: 0.05809023976325989\n",
      "Epoch 27249/30000 Training Loss: 0.03242332488298416\n",
      "Epoch 27250/30000 Training Loss: 0.039942942559719086\n",
      "Epoch 27251/30000 Training Loss: 0.04663459211587906\n",
      "Epoch 27252/30000 Training Loss: 0.0420064777135849\n",
      "Epoch 27253/30000 Training Loss: 0.04789405316114426\n",
      "Epoch 27254/30000 Training Loss: 0.04253195971250534\n",
      "Epoch 27255/30000 Training Loss: 0.039377257227897644\n",
      "Epoch 27256/30000 Training Loss: 0.03884388133883476\n",
      "Epoch 27257/30000 Training Loss: 0.03991624340415001\n",
      "Epoch 27258/30000 Training Loss: 0.05960732698440552\n",
      "Epoch 27259/30000 Training Loss: 0.04340161755681038\n",
      "Epoch 27260/30000 Training Loss: 0.05123459920287132\n",
      "Epoch 27261/30000 Training Loss: 0.04324662685394287\n",
      "Epoch 27262/30000 Training Loss: 0.04093034937977791\n",
      "Epoch 27263/30000 Training Loss: 0.053262412548065186\n",
      "Epoch 27264/30000 Training Loss: 0.059910234063863754\n",
      "Epoch 27265/30000 Training Loss: 0.04472751170396805\n",
      "Epoch 27266/30000 Training Loss: 0.043287549167871475\n",
      "Epoch 27267/30000 Training Loss: 0.046702463179826736\n",
      "Epoch 27268/30000 Training Loss: 0.03764495626091957\n",
      "Epoch 27269/30000 Training Loss: 0.05040929466485977\n",
      "Epoch 27270/30000 Training Loss: 0.041381143033504486\n",
      "Epoch 27271/30000 Training Loss: 0.043833520263433456\n",
      "Epoch 27272/30000 Training Loss: 0.03756991773843765\n",
      "Epoch 27273/30000 Training Loss: 0.048261020332574844\n",
      "Epoch 27274/30000 Training Loss: 0.04271145537495613\n",
      "Epoch 27275/30000 Training Loss: 0.04107259213924408\n",
      "Epoch 27276/30000 Training Loss: 0.04641251266002655\n",
      "Epoch 27277/30000 Training Loss: 0.048496074974536896\n",
      "Epoch 27278/30000 Training Loss: 0.035312142223119736\n",
      "Epoch 27279/30000 Training Loss: 0.05625728517770767\n",
      "Epoch 27280/30000 Training Loss: 0.047820866107940674\n",
      "Epoch 27281/30000 Training Loss: 0.04532982409000397\n",
      "Epoch 27282/30000 Training Loss: 0.0436534583568573\n",
      "Epoch 27283/30000 Training Loss: 0.036865659058094025\n",
      "Epoch 27284/30000 Training Loss: 0.04216388240456581\n",
      "Epoch 27285/30000 Training Loss: 0.0361999049782753\n",
      "Epoch 27286/30000 Training Loss: 0.0536552369594574\n",
      "Epoch 27287/30000 Training Loss: 0.05185827612876892\n",
      "Epoch 27288/30000 Training Loss: 0.057546116411685944\n",
      "Epoch 27289/30000 Training Loss: 0.04190501570701599\n",
      "Epoch 27290/30000 Training Loss: 0.03674740344285965\n",
      "Epoch 27291/30000 Training Loss: 0.03567662462592125\n",
      "Epoch 27292/30000 Training Loss: 0.049405016005039215\n",
      "Epoch 27293/30000 Training Loss: 0.03972538560628891\n",
      "Epoch 27294/30000 Training Loss: 0.049491941928863525\n",
      "Epoch 27295/30000 Training Loss: 0.04688265174627304\n",
      "Epoch 27296/30000 Training Loss: 0.050946760922670364\n",
      "Epoch 27297/30000 Training Loss: 0.0379800945520401\n",
      "Epoch 27298/30000 Training Loss: 0.04299633949995041\n",
      "Epoch 27299/30000 Training Loss: 0.041488166898489\n",
      "Epoch 27300/30000 Training Loss: 0.04125383868813515\n",
      "Epoch 27300/30000 Validation Loss: 0.036478325724601746\n",
      "Epoch 27301/30000 Training Loss: 0.06657139211893082\n",
      "Epoch 27302/30000 Training Loss: 0.038796316832304\n",
      "Epoch 27303/30000 Training Loss: 0.04547768458724022\n",
      "Epoch 27304/30000 Training Loss: 0.04704364389181137\n",
      "Epoch 27305/30000 Training Loss: 0.04741513356566429\n",
      "Epoch 27306/30000 Training Loss: 0.0482604056596756\n",
      "Epoch 27307/30000 Training Loss: 0.05067785084247589\n",
      "Epoch 27308/30000 Training Loss: 0.0455716997385025\n",
      "Epoch 27309/30000 Training Loss: 0.04055692255496979\n",
      "Epoch 27310/30000 Training Loss: 0.052932288497686386\n",
      "Epoch 27311/30000 Training Loss: 0.04219712316989899\n",
      "Epoch 27312/30000 Training Loss: 0.05416618287563324\n",
      "Epoch 27313/30000 Training Loss: 0.0384802408516407\n",
      "Epoch 27314/30000 Training Loss: 0.04297409579157829\n",
      "Epoch 27315/30000 Training Loss: 0.04812629520893097\n",
      "Epoch 27316/30000 Training Loss: 0.03964429348707199\n",
      "Epoch 27317/30000 Training Loss: 0.045465778559446335\n",
      "Epoch 27318/30000 Training Loss: 0.05450638756155968\n",
      "Epoch 27319/30000 Training Loss: 0.049663711339235306\n",
      "Epoch 27320/30000 Training Loss: 0.03734339401125908\n",
      "Epoch 27321/30000 Training Loss: 0.04266461730003357\n",
      "Epoch 27322/30000 Training Loss: 0.060699135065078735\n",
      "Epoch 27323/30000 Training Loss: 0.038409095257520676\n",
      "Epoch 27324/30000 Training Loss: 0.05063677206635475\n",
      "Epoch 27325/30000 Training Loss: 0.046794015914201736\n",
      "Epoch 27326/30000 Training Loss: 0.05113253369927406\n",
      "Epoch 27327/30000 Training Loss: 0.04363342374563217\n",
      "Epoch 27328/30000 Training Loss: 0.06620486080646515\n",
      "Epoch 27329/30000 Training Loss: 0.05461344122886658\n",
      "Epoch 27330/30000 Training Loss: 0.04579364135861397\n",
      "Epoch 27331/30000 Training Loss: 0.031886354088783264\n",
      "Epoch 27332/30000 Training Loss: 0.03761110454797745\n",
      "Epoch 27333/30000 Training Loss: 0.03728032857179642\n",
      "Epoch 27334/30000 Training Loss: 0.03800056129693985\n",
      "Epoch 27335/30000 Training Loss: 0.05551990866661072\n",
      "Epoch 27336/30000 Training Loss: 0.04911237582564354\n",
      "Epoch 27337/30000 Training Loss: 0.06450209021568298\n",
      "Epoch 27338/30000 Training Loss: 0.051637351512908936\n",
      "Epoch 27339/30000 Training Loss: 0.04578166827559471\n",
      "Epoch 27340/30000 Training Loss: 0.04838625341653824\n",
      "Epoch 27341/30000 Training Loss: 0.0715671256184578\n",
      "Epoch 27342/30000 Training Loss: 0.051884446293115616\n",
      "Epoch 27343/30000 Training Loss: 0.04738020896911621\n",
      "Epoch 27344/30000 Training Loss: 0.04563824087381363\n",
      "Epoch 27345/30000 Training Loss: 0.03679986670613289\n",
      "Epoch 27346/30000 Training Loss: 0.03806107118725777\n",
      "Epoch 27347/30000 Training Loss: 0.036153823137283325\n",
      "Epoch 27348/30000 Training Loss: 0.034019771963357925\n",
      "Epoch 27349/30000 Training Loss: 0.04883408918976784\n",
      "Epoch 27350/30000 Training Loss: 0.03510737046599388\n",
      "Epoch 27351/30000 Training Loss: 0.05259016528725624\n",
      "Epoch 27352/30000 Training Loss: 0.05391699820756912\n",
      "Epoch 27353/30000 Training Loss: 0.05844874680042267\n",
      "Epoch 27354/30000 Training Loss: 0.059123944491147995\n",
      "Epoch 27355/30000 Training Loss: 0.038100872188806534\n",
      "Epoch 27356/30000 Training Loss: 0.05751137435436249\n",
      "Epoch 27357/30000 Training Loss: 0.03677273914217949\n",
      "Epoch 27358/30000 Training Loss: 0.050304021686315536\n",
      "Epoch 27359/30000 Training Loss: 0.03464749455451965\n",
      "Epoch 27360/30000 Training Loss: 0.0524427592754364\n",
      "Epoch 27361/30000 Training Loss: 0.04866734892129898\n",
      "Epoch 27362/30000 Training Loss: 0.06103306636214256\n",
      "Epoch 27363/30000 Training Loss: 0.03658146783709526\n",
      "Epoch 27364/30000 Training Loss: 0.040297795087099075\n",
      "Epoch 27365/30000 Training Loss: 0.04587117210030556\n",
      "Epoch 27366/30000 Training Loss: 0.05282248556613922\n",
      "Epoch 27367/30000 Training Loss: 0.02706512063741684\n",
      "Epoch 27368/30000 Training Loss: 0.05156971886754036\n",
      "Epoch 27369/30000 Training Loss: 0.053490594029426575\n",
      "Epoch 27370/30000 Training Loss: 0.041604749858379364\n",
      "Epoch 27371/30000 Training Loss: 0.037212491035461426\n",
      "Epoch 27372/30000 Training Loss: 0.05041923746466637\n",
      "Epoch 27373/30000 Training Loss: 0.043929360806941986\n",
      "Epoch 27374/30000 Training Loss: 0.044830434024333954\n",
      "Epoch 27375/30000 Training Loss: 0.039405278861522675\n",
      "Epoch 27376/30000 Training Loss: 0.041785433888435364\n",
      "Epoch 27377/30000 Training Loss: 0.049074962735176086\n",
      "Epoch 27378/30000 Training Loss: 0.0353856086730957\n",
      "Epoch 27379/30000 Training Loss: 0.04072045534849167\n",
      "Epoch 27380/30000 Training Loss: 0.05038546025753021\n",
      "Epoch 27381/30000 Training Loss: 0.03993818163871765\n",
      "Epoch 27382/30000 Training Loss: 0.05708622187376022\n",
      "Epoch 27383/30000 Training Loss: 0.05043640360236168\n",
      "Epoch 27384/30000 Training Loss: 0.05991658940911293\n",
      "Epoch 27385/30000 Training Loss: 0.05667326599359512\n",
      "Epoch 27386/30000 Training Loss: 0.05626298859715462\n",
      "Epoch 27387/30000 Training Loss: 0.05485816299915314\n",
      "Epoch 27388/30000 Training Loss: 0.05024697631597519\n",
      "Epoch 27389/30000 Training Loss: 0.054126493632793427\n",
      "Epoch 27390/30000 Training Loss: 0.04329391568899155\n",
      "Epoch 27391/30000 Training Loss: 0.03930869325995445\n",
      "Epoch 27392/30000 Training Loss: 0.044718898832798004\n",
      "Epoch 27393/30000 Training Loss: 0.03936434164643288\n",
      "Epoch 27394/30000 Training Loss: 0.046890392899513245\n",
      "Epoch 27395/30000 Training Loss: 0.03654123842716217\n",
      "Epoch 27396/30000 Training Loss: 0.04369492828845978\n",
      "Epoch 27397/30000 Training Loss: 0.0454159751534462\n",
      "Epoch 27398/30000 Training Loss: 0.042675089091062546\n",
      "Epoch 27399/30000 Training Loss: 0.0475342757999897\n",
      "Epoch 27400/30000 Training Loss: 0.05856984481215477\n",
      "Epoch 27400/30000 Validation Loss: 0.03903840854763985\n",
      "Epoch 27401/30000 Training Loss: 0.04298628866672516\n",
      "Epoch 27402/30000 Training Loss: 0.04105984419584274\n",
      "Epoch 27403/30000 Training Loss: 0.04888932779431343\n",
      "Epoch 27404/30000 Training Loss: 0.06245746836066246\n",
      "Epoch 27405/30000 Training Loss: 0.054257407784461975\n",
      "Epoch 27406/30000 Training Loss: 0.03391087427735329\n",
      "Epoch 27407/30000 Training Loss: 0.04294641315937042\n",
      "Epoch 27408/30000 Training Loss: 0.049266308546066284\n",
      "Epoch 27409/30000 Training Loss: 0.04735012352466583\n",
      "Epoch 27410/30000 Training Loss: 0.04204529523849487\n",
      "Epoch 27411/30000 Training Loss: 0.038854021579027176\n",
      "Epoch 27412/30000 Training Loss: 0.05507059395313263\n",
      "Epoch 27413/30000 Training Loss: 0.04193887114524841\n",
      "Epoch 27414/30000 Training Loss: 0.054176077246665955\n",
      "Epoch 27415/30000 Training Loss: 0.06179848685860634\n",
      "Epoch 27416/30000 Training Loss: 0.047353267669677734\n",
      "Epoch 27417/30000 Training Loss: 0.045482225716114044\n",
      "Epoch 27418/30000 Training Loss: 0.038208525627851486\n",
      "Epoch 27419/30000 Training Loss: 0.037744347006082535\n",
      "Epoch 27420/30000 Training Loss: 0.046528659760951996\n",
      "Epoch 27421/30000 Training Loss: 0.05304863303899765\n",
      "Epoch 27422/30000 Training Loss: 0.04537878930568695\n",
      "Epoch 27423/30000 Training Loss: 0.055430151522159576\n",
      "Epoch 27424/30000 Training Loss: 0.040222931653261185\n",
      "Epoch 27425/30000 Training Loss: 0.04676396772265434\n",
      "Epoch 27426/30000 Training Loss: 0.0652453601360321\n",
      "Epoch 27427/30000 Training Loss: 0.04991909861564636\n",
      "Epoch 27428/30000 Training Loss: 0.0500447079539299\n",
      "Epoch 27429/30000 Training Loss: 0.04503123462200165\n",
      "Epoch 27430/30000 Training Loss: 0.04464779794216156\n",
      "Epoch 27431/30000 Training Loss: 0.04616450518369675\n",
      "Epoch 27432/30000 Training Loss: 0.037297483533620834\n",
      "Epoch 27433/30000 Training Loss: 0.030925314873456955\n",
      "Epoch 27434/30000 Training Loss: 0.06554011255502701\n",
      "Epoch 27435/30000 Training Loss: 0.04507521539926529\n",
      "Epoch 27436/30000 Training Loss: 0.04677439481019974\n",
      "Epoch 27437/30000 Training Loss: 0.04377061873674393\n",
      "Epoch 27438/30000 Training Loss: 0.05908563733100891\n",
      "Epoch 27439/30000 Training Loss: 0.04260043427348137\n",
      "Epoch 27440/30000 Training Loss: 0.04706830903887749\n",
      "Epoch 27441/30000 Training Loss: 0.04921754449605942\n",
      "Epoch 27442/30000 Training Loss: 0.05058129504323006\n",
      "Epoch 27443/30000 Training Loss: 0.03321691229939461\n",
      "Epoch 27444/30000 Training Loss: 0.04537523537874222\n",
      "Epoch 27445/30000 Training Loss: 0.04541207104921341\n",
      "Epoch 27446/30000 Training Loss: 0.031250614672899246\n",
      "Epoch 27447/30000 Training Loss: 0.03845549002289772\n",
      "Epoch 27448/30000 Training Loss: 0.038011036813259125\n",
      "Epoch 27449/30000 Training Loss: 0.050990913063287735\n",
      "Epoch 27450/30000 Training Loss: 0.035492219030857086\n",
      "Epoch 27451/30000 Training Loss: 0.055070579051971436\n",
      "Epoch 27452/30000 Training Loss: 0.03525426238775253\n",
      "Epoch 27453/30000 Training Loss: 0.04008859768509865\n",
      "Epoch 27454/30000 Training Loss: 0.04801406338810921\n",
      "Epoch 27455/30000 Training Loss: 0.03896580636501312\n",
      "Epoch 27456/30000 Training Loss: 0.05617881566286087\n",
      "Epoch 27457/30000 Training Loss: 0.05802013725042343\n",
      "Epoch 27458/30000 Training Loss: 0.06091449782252312\n",
      "Epoch 27459/30000 Training Loss: 0.03198830038309097\n",
      "Epoch 27460/30000 Training Loss: 0.0350467786192894\n",
      "Epoch 27461/30000 Training Loss: 0.04686325415968895\n",
      "Epoch 27462/30000 Training Loss: 0.05250748246908188\n",
      "Epoch 27463/30000 Training Loss: 0.03575456514954567\n",
      "Epoch 27464/30000 Training Loss: 0.05148276686668396\n",
      "Epoch 27465/30000 Training Loss: 0.03397738188505173\n",
      "Epoch 27466/30000 Training Loss: 0.041331127285957336\n",
      "Epoch 27467/30000 Training Loss: 0.039463501423597336\n",
      "Epoch 27468/30000 Training Loss: 0.0356111042201519\n",
      "Epoch 27469/30000 Training Loss: 0.040729351341724396\n",
      "Epoch 27470/30000 Training Loss: 0.047030918300151825\n",
      "Epoch 27471/30000 Training Loss: 0.05635923519730568\n",
      "Epoch 27472/30000 Training Loss: 0.06011383980512619\n",
      "Epoch 27473/30000 Training Loss: 0.04358290135860443\n",
      "Epoch 27474/30000 Training Loss: 0.043437935411930084\n",
      "Epoch 27475/30000 Training Loss: 0.046351514756679535\n",
      "Epoch 27476/30000 Training Loss: 0.04854277893900871\n",
      "Epoch 27477/30000 Training Loss: 0.04682576283812523\n",
      "Epoch 27478/30000 Training Loss: 0.04733943194150925\n",
      "Epoch 27479/30000 Training Loss: 0.051495857536792755\n",
      "Epoch 27480/30000 Training Loss: 0.03918778523802757\n",
      "Epoch 27481/30000 Training Loss: 0.04834519326686859\n",
      "Epoch 27482/30000 Training Loss: 0.03469322249293327\n",
      "Epoch 27483/30000 Training Loss: 0.03831247240304947\n",
      "Epoch 27484/30000 Training Loss: 0.042469386011362076\n",
      "Epoch 27485/30000 Training Loss: 0.036838632076978683\n",
      "Epoch 27486/30000 Training Loss: 0.0351974181830883\n",
      "Epoch 27487/30000 Training Loss: 0.05119888111948967\n",
      "Epoch 27488/30000 Training Loss: 0.041877735406160355\n",
      "Epoch 27489/30000 Training Loss: 0.05032921954989433\n",
      "Epoch 27490/30000 Training Loss: 0.03293316811323166\n",
      "Epoch 27491/30000 Training Loss: 0.04698452353477478\n",
      "Epoch 27492/30000 Training Loss: 0.0419076532125473\n",
      "Epoch 27493/30000 Training Loss: 0.03621592000126839\n",
      "Epoch 27494/30000 Training Loss: 0.042788177728652954\n",
      "Epoch 27495/30000 Training Loss: 0.04979156702756882\n",
      "Epoch 27496/30000 Training Loss: 0.05945296213030815\n",
      "Epoch 27497/30000 Training Loss: 0.0448271781206131\n",
      "Epoch 27498/30000 Training Loss: 0.03147408366203308\n",
      "Epoch 27499/30000 Training Loss: 0.03463227301836014\n",
      "Epoch 27500/30000 Training Loss: 0.03441298380494118\n",
      "Epoch 27500/30000 Validation Loss: 0.039117150008678436\n",
      "Epoch 27501/30000 Training Loss: 0.037819553166627884\n",
      "Epoch 27502/30000 Training Loss: 0.0420561246573925\n",
      "Epoch 27503/30000 Training Loss: 0.039791401475667953\n",
      "Epoch 27504/30000 Training Loss: 0.04013621062040329\n",
      "Epoch 27505/30000 Training Loss: 0.05200333148241043\n",
      "Epoch 27506/30000 Training Loss: 0.04781413450837135\n",
      "Epoch 27507/30000 Training Loss: 0.04199742153286934\n",
      "Epoch 27508/30000 Training Loss: 0.04290153458714485\n",
      "Epoch 27509/30000 Training Loss: 0.038164056837558746\n",
      "Epoch 27510/30000 Training Loss: 0.03630547225475311\n",
      "Epoch 27511/30000 Training Loss: 0.04600346088409424\n",
      "Epoch 27512/30000 Training Loss: 0.04927876219153404\n",
      "Epoch 27513/30000 Training Loss: 0.05466639623045921\n",
      "Epoch 27514/30000 Training Loss: 0.0430920347571373\n",
      "Epoch 27515/30000 Training Loss: 0.05014535412192345\n",
      "Epoch 27516/30000 Training Loss: 0.04884406924247742\n",
      "Epoch 27517/30000 Training Loss: 0.046239059418439865\n",
      "Epoch 27518/30000 Training Loss: 0.04744303226470947\n",
      "Epoch 27519/30000 Training Loss: 0.049973201006650925\n",
      "Epoch 27520/30000 Training Loss: 0.039334818720817566\n",
      "Epoch 27521/30000 Training Loss: 0.044732049107551575\n",
      "Epoch 27522/30000 Training Loss: 0.04418522119522095\n",
      "Epoch 27523/30000 Training Loss: 0.05187854543328285\n",
      "Epoch 27524/30000 Training Loss: 0.05615521967411041\n",
      "Epoch 27525/30000 Training Loss: 0.05073169246315956\n",
      "Epoch 27526/30000 Training Loss: 0.05388917773962021\n",
      "Epoch 27527/30000 Training Loss: 0.039828136563301086\n",
      "Epoch 27528/30000 Training Loss: 0.05503416061401367\n",
      "Epoch 27529/30000 Training Loss: 0.049563173204660416\n",
      "Epoch 27530/30000 Training Loss: 0.04950913041830063\n",
      "Epoch 27531/30000 Training Loss: 0.05071878060698509\n",
      "Epoch 27532/30000 Training Loss: 0.05498623847961426\n",
      "Epoch 27533/30000 Training Loss: 0.052145816385746\n",
      "Epoch 27534/30000 Training Loss: 0.03926993906497955\n",
      "Epoch 27535/30000 Training Loss: 0.051026374101638794\n",
      "Epoch 27536/30000 Training Loss: 0.04212258756160736\n",
      "Epoch 27537/30000 Training Loss: 0.05911194533109665\n",
      "Epoch 27538/30000 Training Loss: 0.04665480554103851\n",
      "Epoch 27539/30000 Training Loss: 0.0598093718290329\n",
      "Epoch 27540/30000 Training Loss: 0.05746333301067352\n",
      "Epoch 27541/30000 Training Loss: 0.051375411450862885\n",
      "Epoch 27542/30000 Training Loss: 0.03916105628013611\n",
      "Epoch 27543/30000 Training Loss: 0.05483745038509369\n",
      "Epoch 27544/30000 Training Loss: 0.04582419618964195\n",
      "Epoch 27545/30000 Training Loss: 0.04851448908448219\n",
      "Epoch 27546/30000 Training Loss: 0.0592125728726387\n",
      "Epoch 27547/30000 Training Loss: 0.05593062937259674\n",
      "Epoch 27548/30000 Training Loss: 0.057298146188259125\n",
      "Epoch 27549/30000 Training Loss: 0.059498030692338943\n",
      "Epoch 27550/30000 Training Loss: 0.06800484657287598\n",
      "Epoch 27551/30000 Training Loss: 0.04412057623267174\n",
      "Epoch 27552/30000 Training Loss: 0.053634390234947205\n",
      "Epoch 27553/30000 Training Loss: 0.03782038018107414\n",
      "Epoch 27554/30000 Training Loss: 0.04693774878978729\n",
      "Epoch 27555/30000 Training Loss: 0.047327518463134766\n",
      "Epoch 27556/30000 Training Loss: 0.03150579333305359\n",
      "Epoch 27557/30000 Training Loss: 0.04811761528253555\n",
      "Epoch 27558/30000 Training Loss: 0.045048654079437256\n",
      "Epoch 27559/30000 Training Loss: 0.051079463213682175\n",
      "Epoch 27560/30000 Training Loss: 0.05177445709705353\n",
      "Epoch 27561/30000 Training Loss: 0.045682746917009354\n",
      "Epoch 27562/30000 Training Loss: 0.045393504202365875\n",
      "Epoch 27563/30000 Training Loss: 0.042019058018922806\n",
      "Epoch 27564/30000 Training Loss: 0.05174622684717178\n",
      "Epoch 27565/30000 Training Loss: 0.0443696454167366\n",
      "Epoch 27566/30000 Training Loss: 0.03557966277003288\n",
      "Epoch 27567/30000 Training Loss: 0.0507533885538578\n",
      "Epoch 27568/30000 Training Loss: 0.0389266237616539\n",
      "Epoch 27569/30000 Training Loss: 0.054987832903862\n",
      "Epoch 27570/30000 Training Loss: 0.06241722032427788\n",
      "Epoch 27571/30000 Training Loss: 0.051222383975982666\n",
      "Epoch 27572/30000 Training Loss: 0.04088526964187622\n",
      "Epoch 27573/30000 Training Loss: 0.04626097530126572\n",
      "Epoch 27574/30000 Training Loss: 0.041234225034713745\n",
      "Epoch 27575/30000 Training Loss: 0.04397482052445412\n",
      "Epoch 27576/30000 Training Loss: 0.04212808236479759\n",
      "Epoch 27577/30000 Training Loss: 0.032514944672584534\n",
      "Epoch 27578/30000 Training Loss: 0.0503702312707901\n",
      "Epoch 27579/30000 Training Loss: 0.05481771379709244\n",
      "Epoch 27580/30000 Training Loss: 0.05113055929541588\n",
      "Epoch 27581/30000 Training Loss: 0.06712838262319565\n",
      "Epoch 27582/30000 Training Loss: 0.04401829093694687\n",
      "Epoch 27583/30000 Training Loss: 0.03428168222308159\n",
      "Epoch 27584/30000 Training Loss: 0.032739169895648956\n",
      "Epoch 27585/30000 Training Loss: 0.058099035173654556\n",
      "Epoch 27586/30000 Training Loss: 0.05076262727379799\n",
      "Epoch 27587/30000 Training Loss: 0.051745302975177765\n",
      "Epoch 27588/30000 Training Loss: 0.05046108365058899\n",
      "Epoch 27589/30000 Training Loss: 0.04564933106303215\n",
      "Epoch 27590/30000 Training Loss: 0.03452262654900551\n",
      "Epoch 27591/30000 Training Loss: 0.04689007252454758\n",
      "Epoch 27592/30000 Training Loss: 0.05083924159407616\n",
      "Epoch 27593/30000 Training Loss: 0.05101953446865082\n",
      "Epoch 27594/30000 Training Loss: 0.04835817590355873\n",
      "Epoch 27595/30000 Training Loss: 0.042144447565078735\n",
      "Epoch 27596/30000 Training Loss: 0.05860086902976036\n",
      "Epoch 27597/30000 Training Loss: 0.0525050163269043\n",
      "Epoch 27598/30000 Training Loss: 0.05537065863609314\n",
      "Epoch 27599/30000 Training Loss: 0.05402548611164093\n",
      "Epoch 27600/30000 Training Loss: 0.03593050315976143\n",
      "Epoch 27600/30000 Validation Loss: 0.03160591423511505\n",
      "Epoch 27601/30000 Training Loss: 0.05841837078332901\n",
      "Epoch 27602/30000 Training Loss: 0.05478844419121742\n",
      "Epoch 27603/30000 Training Loss: 0.05449509248137474\n",
      "Epoch 27604/30000 Training Loss: 0.04193473234772682\n",
      "Epoch 27605/30000 Training Loss: 0.048281967639923096\n",
      "Epoch 27606/30000 Training Loss: 0.03084716759622097\n",
      "Epoch 27607/30000 Training Loss: 0.035250306129455566\n",
      "Epoch 27608/30000 Training Loss: 0.04326849430799484\n",
      "Epoch 27609/30000 Training Loss: 0.04825504124164581\n",
      "Epoch 27610/30000 Training Loss: 0.04786502942442894\n",
      "Epoch 27611/30000 Training Loss: 0.03889624774456024\n",
      "Epoch 27612/30000 Training Loss: 0.04279959201812744\n",
      "Epoch 27613/30000 Training Loss: 0.034061454236507416\n",
      "Epoch 27614/30000 Training Loss: 0.031493574380874634\n",
      "Epoch 27615/30000 Training Loss: 0.046173304319381714\n",
      "Epoch 27616/30000 Training Loss: 0.03861119598150253\n",
      "Epoch 27617/30000 Training Loss: 0.04197510704398155\n",
      "Epoch 27618/30000 Training Loss: 0.04396909475326538\n",
      "Epoch 27619/30000 Training Loss: 0.04704626277089119\n",
      "Epoch 27620/30000 Training Loss: 0.04688403010368347\n",
      "Epoch 27621/30000 Training Loss: 0.038723234087228775\n",
      "Epoch 27622/30000 Training Loss: 0.056729063391685486\n",
      "Epoch 27623/30000 Training Loss: 0.05392436683177948\n",
      "Epoch 27624/30000 Training Loss: 0.04330742359161377\n",
      "Epoch 27625/30000 Training Loss: 0.04744267091155052\n",
      "Epoch 27626/30000 Training Loss: 0.04216098412871361\n",
      "Epoch 27627/30000 Training Loss: 0.03434346243739128\n",
      "Epoch 27628/30000 Training Loss: 0.041042935103178024\n",
      "Epoch 27629/30000 Training Loss: 0.04627998173236847\n",
      "Epoch 27630/30000 Training Loss: 0.04371962696313858\n",
      "Epoch 27631/30000 Training Loss: 0.06751598417758942\n",
      "Epoch 27632/30000 Training Loss: 0.039906345307826996\n",
      "Epoch 27633/30000 Training Loss: 0.03427596017718315\n",
      "Epoch 27634/30000 Training Loss: 0.05159502476453781\n",
      "Epoch 27635/30000 Training Loss: 0.03270292282104492\n",
      "Epoch 27636/30000 Training Loss: 0.04162734001874924\n",
      "Epoch 27637/30000 Training Loss: 0.044069718569517136\n",
      "Epoch 27638/30000 Training Loss: 0.05491502583026886\n",
      "Epoch 27639/30000 Training Loss: 0.05439269542694092\n",
      "Epoch 27640/30000 Training Loss: 0.040775977075099945\n",
      "Epoch 27641/30000 Training Loss: 0.03699815645813942\n",
      "Epoch 27642/30000 Training Loss: 0.03858128935098648\n",
      "Epoch 27643/30000 Training Loss: 0.05343069136142731\n",
      "Epoch 27644/30000 Training Loss: 0.06173418462276459\n",
      "Epoch 27645/30000 Training Loss: 0.05155671015381813\n",
      "Epoch 27646/30000 Training Loss: 0.04137566685676575\n",
      "Epoch 27647/30000 Training Loss: 0.03854670748114586\n",
      "Epoch 27648/30000 Training Loss: 0.06087111681699753\n",
      "Epoch 27649/30000 Training Loss: 0.05414165183901787\n",
      "Epoch 27650/30000 Training Loss: 0.04088468477129936\n",
      "Epoch 27651/30000 Training Loss: 0.03652796149253845\n",
      "Epoch 27652/30000 Training Loss: 0.03893497213721275\n",
      "Epoch 27653/30000 Training Loss: 0.03542931750416756\n",
      "Epoch 27654/30000 Training Loss: 0.03264516964554787\n",
      "Epoch 27655/30000 Training Loss: 0.03160178288817406\n",
      "Epoch 27656/30000 Training Loss: 0.035160891711711884\n",
      "Epoch 27657/30000 Training Loss: 0.039333779364824295\n",
      "Epoch 27658/30000 Training Loss: 0.028731971979141235\n",
      "Epoch 27659/30000 Training Loss: 0.046408653259277344\n",
      "Epoch 27660/30000 Training Loss: 0.03592662140727043\n",
      "Epoch 27661/30000 Training Loss: 0.05813968926668167\n",
      "Epoch 27662/30000 Training Loss: 0.03495310992002487\n",
      "Epoch 27663/30000 Training Loss: 0.053909286856651306\n",
      "Epoch 27664/30000 Training Loss: 0.044486965984106064\n",
      "Epoch 27665/30000 Training Loss: 0.058249592781066895\n",
      "Epoch 27666/30000 Training Loss: 0.05361287295818329\n",
      "Epoch 27667/30000 Training Loss: 0.05128133296966553\n",
      "Epoch 27668/30000 Training Loss: 0.04404867812991142\n",
      "Epoch 27669/30000 Training Loss: 0.043810658156871796\n",
      "Epoch 27670/30000 Training Loss: 0.039059702306985855\n",
      "Epoch 27671/30000 Training Loss: 0.045554667711257935\n",
      "Epoch 27672/30000 Training Loss: 0.048899609595537186\n",
      "Epoch 27673/30000 Training Loss: 0.04816008731722832\n",
      "Epoch 27674/30000 Training Loss: 0.04349145293235779\n",
      "Epoch 27675/30000 Training Loss: 0.05125708505511284\n",
      "Epoch 27676/30000 Training Loss: 0.04718233644962311\n",
      "Epoch 27677/30000 Training Loss: 0.055950574576854706\n",
      "Epoch 27678/30000 Training Loss: 0.04736924171447754\n",
      "Epoch 27679/30000 Training Loss: 0.04348825663328171\n",
      "Epoch 27680/30000 Training Loss: 0.0460023395717144\n",
      "Epoch 27681/30000 Training Loss: 0.060277823358774185\n",
      "Epoch 27682/30000 Training Loss: 0.04456114023923874\n",
      "Epoch 27683/30000 Training Loss: 0.04657866805791855\n",
      "Epoch 27684/30000 Training Loss: 0.04822083190083504\n",
      "Epoch 27685/30000 Training Loss: 0.06492427736520767\n",
      "Epoch 27686/30000 Training Loss: 0.028949633240699768\n",
      "Epoch 27687/30000 Training Loss: 0.03871241211891174\n",
      "Epoch 27688/30000 Training Loss: 0.03735756874084473\n",
      "Epoch 27689/30000 Training Loss: 0.039959587156772614\n",
      "Epoch 27690/30000 Training Loss: 0.029915545135736465\n",
      "Epoch 27691/30000 Training Loss: 0.0383145697414875\n",
      "Epoch 27692/30000 Training Loss: 0.044728513807058334\n",
      "Epoch 27693/30000 Training Loss: 0.05116032436490059\n",
      "Epoch 27694/30000 Training Loss: 0.04153997823596001\n",
      "Epoch 27695/30000 Training Loss: 0.05474183335900307\n",
      "Epoch 27696/30000 Training Loss: 0.06919018179178238\n",
      "Epoch 27697/30000 Training Loss: 0.0698753148317337\n",
      "Epoch 27698/30000 Training Loss: 0.0518900565803051\n",
      "Epoch 27699/30000 Training Loss: 0.05927155166864395\n",
      "Epoch 27700/30000 Training Loss: 0.03769543021917343\n",
      "Epoch 27700/30000 Validation Loss: 0.05599483847618103\n",
      "Epoch 27701/30000 Training Loss: 0.04176037013530731\n",
      "Epoch 27702/30000 Training Loss: 0.04692256823182106\n",
      "Epoch 27703/30000 Training Loss: 0.06806322932243347\n",
      "Epoch 27704/30000 Training Loss: 0.05673220381140709\n",
      "Epoch 27705/30000 Training Loss: 0.04953257739543915\n",
      "Epoch 27706/30000 Training Loss: 0.0588015541434288\n",
      "Epoch 27707/30000 Training Loss: 0.03439287841320038\n",
      "Epoch 27708/30000 Training Loss: 0.041947897523641586\n",
      "Epoch 27709/30000 Training Loss: 0.049304328858852386\n",
      "Epoch 27710/30000 Training Loss: 0.03930801525712013\n",
      "Epoch 27711/30000 Training Loss: 0.03910381719470024\n",
      "Epoch 27712/30000 Training Loss: 0.04933171719312668\n",
      "Epoch 27713/30000 Training Loss: 0.04960499703884125\n",
      "Epoch 27714/30000 Training Loss: 0.06066158413887024\n",
      "Epoch 27715/30000 Training Loss: 0.05824815481901169\n",
      "Epoch 27716/30000 Training Loss: 0.04344690218567848\n",
      "Epoch 27717/30000 Training Loss: 0.03415175899863243\n",
      "Epoch 27718/30000 Training Loss: 0.04601238667964935\n",
      "Epoch 27719/30000 Training Loss: 0.06422066688537598\n",
      "Epoch 27720/30000 Training Loss: 0.03605765849351883\n",
      "Epoch 27721/30000 Training Loss: 0.050685178488492966\n",
      "Epoch 27722/30000 Training Loss: 0.05147409439086914\n",
      "Epoch 27723/30000 Training Loss: 0.05639634281396866\n",
      "Epoch 27724/30000 Training Loss: 0.05309925973415375\n",
      "Epoch 27725/30000 Training Loss: 0.06536601483821869\n",
      "Epoch 27726/30000 Training Loss: 0.04146154597401619\n",
      "Epoch 27727/30000 Training Loss: 0.03511989489197731\n",
      "Epoch 27728/30000 Training Loss: 0.03466483950614929\n",
      "Epoch 27729/30000 Training Loss: 0.029549576342105865\n",
      "Epoch 27730/30000 Training Loss: 0.042093176394701004\n",
      "Epoch 27731/30000 Training Loss: 0.04623965546488762\n",
      "Epoch 27732/30000 Training Loss: 0.047785405069589615\n",
      "Epoch 27733/30000 Training Loss: 0.05041007697582245\n",
      "Epoch 27734/30000 Training Loss: 0.04137125611305237\n",
      "Epoch 27735/30000 Training Loss: 0.03642207756638527\n",
      "Epoch 27736/30000 Training Loss: 0.052453771233558655\n",
      "Epoch 27737/30000 Training Loss: 0.0447310209274292\n",
      "Epoch 27738/30000 Training Loss: 0.0515434630215168\n",
      "Epoch 27739/30000 Training Loss: 0.042711276561021805\n",
      "Epoch 27740/30000 Training Loss: 0.03990907222032547\n",
      "Epoch 27741/30000 Training Loss: 0.03210737928748131\n",
      "Epoch 27742/30000 Training Loss: 0.04387980327010155\n",
      "Epoch 27743/30000 Training Loss: 0.05499263480305672\n",
      "Epoch 27744/30000 Training Loss: 0.06770273298025131\n",
      "Epoch 27745/30000 Training Loss: 0.03805457800626755\n",
      "Epoch 27746/30000 Training Loss: 0.0543724000453949\n",
      "Epoch 27747/30000 Training Loss: 0.040498096495866776\n",
      "Epoch 27748/30000 Training Loss: 0.06047631800174713\n",
      "Epoch 27749/30000 Training Loss: 0.03217795491218567\n",
      "Epoch 27750/30000 Training Loss: 0.04804446920752525\n",
      "Epoch 27751/30000 Training Loss: 0.052746739238500595\n",
      "Epoch 27752/30000 Training Loss: 0.03753034770488739\n",
      "Epoch 27753/30000 Training Loss: 0.035037290304899216\n",
      "Epoch 27754/30000 Training Loss: 0.05010614171624184\n",
      "Epoch 27755/30000 Training Loss: 0.04263201728463173\n",
      "Epoch 27756/30000 Training Loss: 0.048247065395116806\n",
      "Epoch 27757/30000 Training Loss: 0.04493798315525055\n",
      "Epoch 27758/30000 Training Loss: 0.043869830667972565\n",
      "Epoch 27759/30000 Training Loss: 0.04254913330078125\n",
      "Epoch 27760/30000 Training Loss: 0.05725005641579628\n",
      "Epoch 27761/30000 Training Loss: 0.06517382711172104\n",
      "Epoch 27762/30000 Training Loss: 0.0390082411468029\n",
      "Epoch 27763/30000 Training Loss: 0.03821408748626709\n",
      "Epoch 27764/30000 Training Loss: 0.05547993630170822\n",
      "Epoch 27765/30000 Training Loss: 0.046470001339912415\n",
      "Epoch 27766/30000 Training Loss: 0.025225967168807983\n",
      "Epoch 27767/30000 Training Loss: 0.04900360852479935\n",
      "Epoch 27768/30000 Training Loss: 0.03438578546047211\n",
      "Epoch 27769/30000 Training Loss: 0.03597318008542061\n",
      "Epoch 27770/30000 Training Loss: 0.05030493065714836\n",
      "Epoch 27771/30000 Training Loss: 0.05236773565411568\n",
      "Epoch 27772/30000 Training Loss: 0.039655257016420364\n",
      "Epoch 27773/30000 Training Loss: 0.05116136372089386\n",
      "Epoch 27774/30000 Training Loss: 0.0530175045132637\n",
      "Epoch 27775/30000 Training Loss: 0.05198357254266739\n",
      "Epoch 27776/30000 Training Loss: 0.04295271262526512\n",
      "Epoch 27777/30000 Training Loss: 0.04746987670660019\n",
      "Epoch 27778/30000 Training Loss: 0.04585317149758339\n",
      "Epoch 27779/30000 Training Loss: 0.04318934679031372\n",
      "Epoch 27780/30000 Training Loss: 0.04594649001955986\n",
      "Epoch 27781/30000 Training Loss: 0.054386429488658905\n",
      "Epoch 27782/30000 Training Loss: 0.042792245745658875\n",
      "Epoch 27783/30000 Training Loss: 0.03290954977273941\n",
      "Epoch 27784/30000 Training Loss: 0.04223838821053505\n",
      "Epoch 27785/30000 Training Loss: 0.03622237220406532\n",
      "Epoch 27786/30000 Training Loss: 0.05184927210211754\n",
      "Epoch 27787/30000 Training Loss: 0.048378027975559235\n",
      "Epoch 27788/30000 Training Loss: 0.04783114418387413\n",
      "Epoch 27789/30000 Training Loss: 0.04405570030212402\n",
      "Epoch 27790/30000 Training Loss: 0.05543309077620506\n",
      "Epoch 27791/30000 Training Loss: 0.04971112310886383\n",
      "Epoch 27792/30000 Training Loss: 0.038820669054985046\n",
      "Epoch 27793/30000 Training Loss: 0.04531190171837807\n",
      "Epoch 27794/30000 Training Loss: 0.051753006875514984\n",
      "Epoch 27795/30000 Training Loss: 0.05037352815270424\n",
      "Epoch 27796/30000 Training Loss: 0.0530729740858078\n",
      "Epoch 27797/30000 Training Loss: 0.05610133707523346\n",
      "Epoch 27798/30000 Training Loss: 0.037800855934619904\n",
      "Epoch 27799/30000 Training Loss: 0.03430331498384476\n",
      "Epoch 27800/30000 Training Loss: 0.05664089322090149\n",
      "Epoch 27800/30000 Validation Loss: 0.03654733672738075\n",
      "Epoch 27801/30000 Training Loss: 0.03615918010473251\n",
      "Epoch 27802/30000 Training Loss: 0.05251261219382286\n",
      "Epoch 27803/30000 Training Loss: 0.05085325241088867\n",
      "Epoch 27804/30000 Training Loss: 0.04142213612794876\n",
      "Epoch 27805/30000 Training Loss: 0.05155571922659874\n",
      "Epoch 27806/30000 Training Loss: 0.05029429495334625\n",
      "Epoch 27807/30000 Training Loss: 0.05409617722034454\n",
      "Epoch 27808/30000 Training Loss: 0.0383622832596302\n",
      "Epoch 27809/30000 Training Loss: 0.046465516090393066\n",
      "Epoch 27810/30000 Training Loss: 0.04634847491979599\n",
      "Epoch 27811/30000 Training Loss: 0.0459882915019989\n",
      "Epoch 27812/30000 Training Loss: 0.044317133724689484\n",
      "Epoch 27813/30000 Training Loss: 0.043649379163980484\n",
      "Epoch 27814/30000 Training Loss: 0.07499248534440994\n",
      "Epoch 27815/30000 Training Loss: 0.03409717231988907\n",
      "Epoch 27816/30000 Training Loss: 0.04330319166183472\n",
      "Epoch 27817/30000 Training Loss: 0.05893722176551819\n",
      "Epoch 27818/30000 Training Loss: 0.04542911425232887\n",
      "Epoch 27819/30000 Training Loss: 0.03338104858994484\n",
      "Epoch 27820/30000 Training Loss: 0.05007771775126457\n",
      "Epoch 27821/30000 Training Loss: 0.03441367670893669\n",
      "Epoch 27822/30000 Training Loss: 0.03516300395131111\n",
      "Epoch 27823/30000 Training Loss: 0.03603699803352356\n",
      "Epoch 27824/30000 Training Loss: 0.035511698573827744\n",
      "Epoch 27825/30000 Training Loss: 0.037258703261613846\n",
      "Epoch 27826/30000 Training Loss: 0.052986662834882736\n",
      "Epoch 27827/30000 Training Loss: 0.048203591257333755\n",
      "Epoch 27828/30000 Training Loss: 0.07786733657121658\n",
      "Epoch 27829/30000 Training Loss: 0.05577363073825836\n",
      "Epoch 27830/30000 Training Loss: 0.04241311550140381\n",
      "Epoch 27831/30000 Training Loss: 0.03954135626554489\n",
      "Epoch 27832/30000 Training Loss: 0.047552239149808884\n",
      "Epoch 27833/30000 Training Loss: 0.04595886915922165\n",
      "Epoch 27834/30000 Training Loss: 0.04955819621682167\n",
      "Epoch 27835/30000 Training Loss: 0.06145136058330536\n",
      "Epoch 27836/30000 Training Loss: 0.04473262280225754\n",
      "Epoch 27837/30000 Training Loss: 0.05692050978541374\n",
      "Epoch 27838/30000 Training Loss: 0.04360765591263771\n",
      "Epoch 27839/30000 Training Loss: 0.0641009658575058\n",
      "Epoch 27840/30000 Training Loss: 0.04184538498520851\n",
      "Epoch 27841/30000 Training Loss: 0.049924589693546295\n",
      "Epoch 27842/30000 Training Loss: 0.029768187552690506\n",
      "Epoch 27843/30000 Training Loss: 0.059745289385318756\n",
      "Epoch 27844/30000 Training Loss: 0.04594024270772934\n",
      "Epoch 27845/30000 Training Loss: 0.04819468408823013\n",
      "Epoch 27846/30000 Training Loss: 0.05645277351140976\n",
      "Epoch 27847/30000 Training Loss: 0.05072696506977081\n",
      "Epoch 27848/30000 Training Loss: 0.049104221165180206\n",
      "Epoch 27849/30000 Training Loss: 0.05201620236039162\n",
      "Epoch 27850/30000 Training Loss: 0.044263217598199844\n",
      "Epoch 27851/30000 Training Loss: 0.05123241990804672\n",
      "Epoch 27852/30000 Training Loss: 0.049098849296569824\n",
      "Epoch 27853/30000 Training Loss: 0.057473089545965195\n",
      "Epoch 27854/30000 Training Loss: 0.04520583525300026\n",
      "Epoch 27855/30000 Training Loss: 0.037329524755477905\n",
      "Epoch 27856/30000 Training Loss: 0.054695144295692444\n",
      "Epoch 27857/30000 Training Loss: 0.03889338672161102\n",
      "Epoch 27858/30000 Training Loss: 0.05695711821317673\n",
      "Epoch 27859/30000 Training Loss: 0.05955212935805321\n",
      "Epoch 27860/30000 Training Loss: 0.03675927594304085\n",
      "Epoch 27861/30000 Training Loss: 0.03873912990093231\n",
      "Epoch 27862/30000 Training Loss: 0.05947337672114372\n",
      "Epoch 27863/30000 Training Loss: 0.046200647950172424\n",
      "Epoch 27864/30000 Training Loss: 0.04822833463549614\n",
      "Epoch 27865/30000 Training Loss: 0.04865734651684761\n",
      "Epoch 27866/30000 Training Loss: 0.038361139595508575\n",
      "Epoch 27867/30000 Training Loss: 0.038480184972286224\n",
      "Epoch 27868/30000 Training Loss: 0.03849293664097786\n",
      "Epoch 27869/30000 Training Loss: 0.04992229491472244\n",
      "Epoch 27870/30000 Training Loss: 0.05370441451668739\n",
      "Epoch 27871/30000 Training Loss: 0.0416228286921978\n",
      "Epoch 27872/30000 Training Loss: 0.04104280099272728\n",
      "Epoch 27873/30000 Training Loss: 0.035840265452861786\n",
      "Epoch 27874/30000 Training Loss: 0.038518089801073074\n",
      "Epoch 27875/30000 Training Loss: 0.04971349239349365\n",
      "Epoch 27876/30000 Training Loss: 0.056307680904865265\n",
      "Epoch 27877/30000 Training Loss: 0.04863610491156578\n",
      "Epoch 27878/30000 Training Loss: 0.05150945112109184\n",
      "Epoch 27879/30000 Training Loss: 0.05729437991976738\n",
      "Epoch 27880/30000 Training Loss: 0.053138501942157745\n",
      "Epoch 27881/30000 Training Loss: 0.037892818450927734\n",
      "Epoch 27882/30000 Training Loss: 0.04228943586349487\n",
      "Epoch 27883/30000 Training Loss: 0.04052948206663132\n",
      "Epoch 27884/30000 Training Loss: 0.052111171185970306\n",
      "Epoch 27885/30000 Training Loss: 0.04530896991491318\n",
      "Epoch 27886/30000 Training Loss: 0.0573885552585125\n",
      "Epoch 27887/30000 Training Loss: 0.0471644252538681\n",
      "Epoch 27888/30000 Training Loss: 0.05436552315950394\n",
      "Epoch 27889/30000 Training Loss: 0.04817876219749451\n",
      "Epoch 27890/30000 Training Loss: 0.06272973865270615\n",
      "Epoch 27891/30000 Training Loss: 0.03438758850097656\n",
      "Epoch 27892/30000 Training Loss: 0.03845730796456337\n",
      "Epoch 27893/30000 Training Loss: 0.055274590849876404\n",
      "Epoch 27894/30000 Training Loss: 0.04850608855485916\n",
      "Epoch 27895/30000 Training Loss: 0.03863570839166641\n",
      "Epoch 27896/30000 Training Loss: 0.052605900913476944\n",
      "Epoch 27897/30000 Training Loss: 0.04216952621936798\n",
      "Epoch 27898/30000 Training Loss: 0.040270328521728516\n",
      "Epoch 27899/30000 Training Loss: 0.03537750989198685\n",
      "Epoch 27900/30000 Training Loss: 0.047451309859752655\n",
      "Epoch 27900/30000 Validation Loss: 0.04307394474744797\n",
      "Epoch 27901/30000 Training Loss: 0.040399547666311264\n",
      "Epoch 27902/30000 Training Loss: 0.05208601430058479\n",
      "Epoch 27903/30000 Training Loss: 0.05189500376582146\n",
      "Epoch 27904/30000 Training Loss: 0.04954478144645691\n",
      "Epoch 27905/30000 Training Loss: 0.031962547451257706\n",
      "Epoch 27906/30000 Training Loss: 0.04543564096093178\n",
      "Epoch 27907/30000 Training Loss: 0.028806526213884354\n",
      "Epoch 27908/30000 Training Loss: 0.04806980490684509\n",
      "Epoch 27909/30000 Training Loss: 0.03302185609936714\n",
      "Epoch 27910/30000 Training Loss: 0.04228834807872772\n",
      "Epoch 27911/30000 Training Loss: 0.03857126459479332\n",
      "Epoch 27912/30000 Training Loss: 0.05264493077993393\n",
      "Epoch 27913/30000 Training Loss: 0.05679960548877716\n",
      "Epoch 27914/30000 Training Loss: 0.04851100966334343\n",
      "Epoch 27915/30000 Training Loss: 0.05237724259495735\n",
      "Epoch 27916/30000 Training Loss: 0.05383283272385597\n",
      "Epoch 27917/30000 Training Loss: 0.05697701871395111\n",
      "Epoch 27918/30000 Training Loss: 0.03446924686431885\n",
      "Epoch 27919/30000 Training Loss: 0.06654904782772064\n",
      "Epoch 27920/30000 Training Loss: 0.045252617448568344\n",
      "Epoch 27921/30000 Training Loss: 0.04378248751163483\n",
      "Epoch 27922/30000 Training Loss: 0.037487395107746124\n",
      "Epoch 27923/30000 Training Loss: 0.04318336769938469\n",
      "Epoch 27924/30000 Training Loss: 0.0303119458258152\n",
      "Epoch 27925/30000 Training Loss: 0.04744190722703934\n",
      "Epoch 27926/30000 Training Loss: 0.03876855969429016\n",
      "Epoch 27927/30000 Training Loss: 0.05973564460873604\n",
      "Epoch 27928/30000 Training Loss: 0.039393287152051926\n",
      "Epoch 27929/30000 Training Loss: 0.036334507167339325\n",
      "Epoch 27930/30000 Training Loss: 0.03211844712495804\n",
      "Epoch 27931/30000 Training Loss: 0.048717934638261795\n",
      "Epoch 27932/30000 Training Loss: 0.041619472205638885\n",
      "Epoch 27933/30000 Training Loss: 0.0583304688334465\n",
      "Epoch 27934/30000 Training Loss: 0.033971887081861496\n",
      "Epoch 27935/30000 Training Loss: 0.039586037397384644\n",
      "Epoch 27936/30000 Training Loss: 0.04072495922446251\n",
      "Epoch 27937/30000 Training Loss: 0.03442277014255524\n",
      "Epoch 27938/30000 Training Loss: 0.053230226039886475\n",
      "Epoch 27939/30000 Training Loss: 0.038694389164447784\n",
      "Epoch 27940/30000 Training Loss: 0.06022075563669205\n",
      "Epoch 27941/30000 Training Loss: 0.04054144024848938\n",
      "Epoch 27942/30000 Training Loss: 0.05174735188484192\n",
      "Epoch 27943/30000 Training Loss: 0.043032191693782806\n",
      "Epoch 27944/30000 Training Loss: 0.03145584836602211\n",
      "Epoch 27945/30000 Training Loss: 0.052026085555553436\n",
      "Epoch 27946/30000 Training Loss: 0.03904038667678833\n",
      "Epoch 27947/30000 Training Loss: 0.035733286291360855\n",
      "Epoch 27948/30000 Training Loss: 0.0492621548473835\n",
      "Epoch 27949/30000 Training Loss: 0.05474357306957245\n",
      "Epoch 27950/30000 Training Loss: 0.04801119118928909\n",
      "Epoch 27951/30000 Training Loss: 0.03534336015582085\n",
      "Epoch 27952/30000 Training Loss: 0.05712175741791725\n",
      "Epoch 27953/30000 Training Loss: 0.03171877562999725\n",
      "Epoch 27954/30000 Training Loss: 0.04495391994714737\n",
      "Epoch 27955/30000 Training Loss: 0.03623516857624054\n",
      "Epoch 27956/30000 Training Loss: 0.05232271924614906\n",
      "Epoch 27957/30000 Training Loss: 0.05802587792277336\n",
      "Epoch 27958/30000 Training Loss: 0.042436372488737106\n",
      "Epoch 27959/30000 Training Loss: 0.047871921211481094\n",
      "Epoch 27960/30000 Training Loss: 0.04910017549991608\n",
      "Epoch 27961/30000 Training Loss: 0.0382814034819603\n",
      "Epoch 27962/30000 Training Loss: 0.0652264952659607\n",
      "Epoch 27963/30000 Training Loss: 0.04330994188785553\n",
      "Epoch 27964/30000 Training Loss: 0.032272107899188995\n",
      "Epoch 27965/30000 Training Loss: 0.044760093092918396\n",
      "Epoch 27966/30000 Training Loss: 0.034849219024181366\n",
      "Epoch 27967/30000 Training Loss: 0.04061518609523773\n",
      "Epoch 27968/30000 Training Loss: 0.046454057097435\n",
      "Epoch 27969/30000 Training Loss: 0.05173254758119583\n",
      "Epoch 27970/30000 Training Loss: 0.047056879848241806\n",
      "Epoch 27971/30000 Training Loss: 0.04183320701122284\n",
      "Epoch 27972/30000 Training Loss: 0.05559312552213669\n",
      "Epoch 27973/30000 Training Loss: 0.04254821315407753\n",
      "Epoch 27974/30000 Training Loss: 0.05852198973298073\n",
      "Epoch 27975/30000 Training Loss: 0.04183933511376381\n",
      "Epoch 27976/30000 Training Loss: 0.04087064787745476\n",
      "Epoch 27977/30000 Training Loss: 0.05221959576010704\n",
      "Epoch 27978/30000 Training Loss: 0.040899455547332764\n",
      "Epoch 27979/30000 Training Loss: 0.0405883863568306\n",
      "Epoch 27980/30000 Training Loss: 0.0414871871471405\n",
      "Epoch 27981/30000 Training Loss: 0.04803164303302765\n",
      "Epoch 27982/30000 Training Loss: 0.04809249937534332\n",
      "Epoch 27983/30000 Training Loss: 0.041230298578739166\n",
      "Epoch 27984/30000 Training Loss: 0.03416559100151062\n",
      "Epoch 27985/30000 Training Loss: 0.04763312265276909\n",
      "Epoch 27986/30000 Training Loss: 0.03650747239589691\n",
      "Epoch 27987/30000 Training Loss: 0.0357421338558197\n",
      "Epoch 27988/30000 Training Loss: 0.04341613128781319\n",
      "Epoch 27989/30000 Training Loss: 0.0452713780105114\n",
      "Epoch 27990/30000 Training Loss: 0.04217839241027832\n",
      "Epoch 27991/30000 Training Loss: 0.034929994493722916\n",
      "Epoch 27992/30000 Training Loss: 0.039322108030319214\n",
      "Epoch 27993/30000 Training Loss: 0.04531203210353851\n",
      "Epoch 27994/30000 Training Loss: 0.05669243261218071\n",
      "Epoch 27995/30000 Training Loss: 0.055808644741773605\n",
      "Epoch 27996/30000 Training Loss: 0.04580265283584595\n",
      "Epoch 27997/30000 Training Loss: 0.049926113337278366\n",
      "Epoch 27998/30000 Training Loss: 0.05953983590006828\n",
      "Epoch 27999/30000 Training Loss: 0.03499218821525574\n",
      "Epoch 28000/30000 Training Loss: 0.040518566966056824\n",
      "Epoch 28000/30000 Validation Loss: 0.04143185541033745\n",
      "Epoch 28001/30000 Training Loss: 0.041529711335897446\n",
      "Epoch 28002/30000 Training Loss: 0.03715425357222557\n",
      "Epoch 28003/30000 Training Loss: 0.05793974548578262\n",
      "Epoch 28004/30000 Training Loss: 0.05149576812982559\n",
      "Epoch 28005/30000 Training Loss: 0.04716626554727554\n",
      "Epoch 28006/30000 Training Loss: 0.03790535032749176\n",
      "Epoch 28007/30000 Training Loss: 0.041792936623096466\n",
      "Epoch 28008/30000 Training Loss: 0.05956678465008736\n",
      "Epoch 28009/30000 Training Loss: 0.05597382038831711\n",
      "Epoch 28010/30000 Training Loss: 0.037270642817020416\n",
      "Epoch 28011/30000 Training Loss: 0.040234021842479706\n",
      "Epoch 28012/30000 Training Loss: 0.06021615117788315\n",
      "Epoch 28013/30000 Training Loss: 0.04363943636417389\n",
      "Epoch 28014/30000 Training Loss: 0.048933956772089005\n",
      "Epoch 28015/30000 Training Loss: 0.04494199529290199\n",
      "Epoch 28016/30000 Training Loss: 0.03648336976766586\n",
      "Epoch 28017/30000 Training Loss: 0.05892842262983322\n",
      "Epoch 28018/30000 Training Loss: 0.03920533135533333\n",
      "Epoch 28019/30000 Training Loss: 0.03938902169466019\n",
      "Epoch 28020/30000 Training Loss: 0.06251557916402817\n",
      "Epoch 28021/30000 Training Loss: 0.04653066396713257\n",
      "Epoch 28022/30000 Training Loss: 0.0461076945066452\n",
      "Epoch 28023/30000 Training Loss: 0.04080317169427872\n",
      "Epoch 28024/30000 Training Loss: 0.051846910268068314\n",
      "Epoch 28025/30000 Training Loss: 0.04278205707669258\n",
      "Epoch 28026/30000 Training Loss: 0.04197084158658981\n",
      "Epoch 28027/30000 Training Loss: 0.04090457782149315\n",
      "Epoch 28028/30000 Training Loss: 0.052020251750946045\n",
      "Epoch 28029/30000 Training Loss: 0.04145679622888565\n",
      "Epoch 28030/30000 Training Loss: 0.04216369241476059\n",
      "Epoch 28031/30000 Training Loss: 0.039917051792144775\n",
      "Epoch 28032/30000 Training Loss: 0.037584371864795685\n",
      "Epoch 28033/30000 Training Loss: 0.04594830051064491\n",
      "Epoch 28034/30000 Training Loss: 0.03795231878757477\n",
      "Epoch 28035/30000 Training Loss: 0.03909949213266373\n",
      "Epoch 28036/30000 Training Loss: 0.050101153552532196\n",
      "Epoch 28037/30000 Training Loss: 0.044074494391679764\n",
      "Epoch 28038/30000 Training Loss: 0.04457545280456543\n",
      "Epoch 28039/30000 Training Loss: 0.03841236233711243\n",
      "Epoch 28040/30000 Training Loss: 0.06264548003673553\n",
      "Epoch 28041/30000 Training Loss: 0.04057138040661812\n",
      "Epoch 28042/30000 Training Loss: 0.038751356303691864\n",
      "Epoch 28043/30000 Training Loss: 0.0360412560403347\n",
      "Epoch 28044/30000 Training Loss: 0.038027480244636536\n",
      "Epoch 28045/30000 Training Loss: 0.054593220353126526\n",
      "Epoch 28046/30000 Training Loss: 0.07072462141513824\n",
      "Epoch 28047/30000 Training Loss: 0.04356808960437775\n",
      "Epoch 28048/30000 Training Loss: 0.04441891610622406\n",
      "Epoch 28049/30000 Training Loss: 0.046842947602272034\n",
      "Epoch 28050/30000 Training Loss: 0.03978125751018524\n",
      "Epoch 28051/30000 Training Loss: 0.0415838398039341\n",
      "Epoch 28052/30000 Training Loss: 0.0475773960351944\n",
      "Epoch 28053/30000 Training Loss: 0.05179767683148384\n",
      "Epoch 28054/30000 Training Loss: 0.03833482414484024\n",
      "Epoch 28055/30000 Training Loss: 0.04995333030819893\n",
      "Epoch 28056/30000 Training Loss: 0.05228113383054733\n",
      "Epoch 28057/30000 Training Loss: 0.05880764499306679\n",
      "Epoch 28058/30000 Training Loss: 0.048869770020246506\n",
      "Epoch 28059/30000 Training Loss: 0.06015335023403168\n",
      "Epoch 28060/30000 Training Loss: 0.048439569771289825\n",
      "Epoch 28061/30000 Training Loss: 0.035756610333919525\n",
      "Epoch 28062/30000 Training Loss: 0.042797431349754333\n",
      "Epoch 28063/30000 Training Loss: 0.051252610981464386\n",
      "Epoch 28064/30000 Training Loss: 0.04553399607539177\n",
      "Epoch 28065/30000 Training Loss: 0.03960917145013809\n",
      "Epoch 28066/30000 Training Loss: 0.044208988547325134\n",
      "Epoch 28067/30000 Training Loss: 0.04926590621471405\n",
      "Epoch 28068/30000 Training Loss: 0.034079041332006454\n",
      "Epoch 28069/30000 Training Loss: 0.032970257103443146\n",
      "Epoch 28070/30000 Training Loss: 0.049359455704689026\n",
      "Epoch 28071/30000 Training Loss: 0.034993868321180344\n",
      "Epoch 28072/30000 Training Loss: 0.032276228070259094\n",
      "Epoch 28073/30000 Training Loss: 0.04519969969987869\n",
      "Epoch 28074/30000 Training Loss: 0.04404101148247719\n",
      "Epoch 28075/30000 Training Loss: 0.04289347305893898\n",
      "Epoch 28076/30000 Training Loss: 0.0455823689699173\n",
      "Epoch 28077/30000 Training Loss: 0.050004858523607254\n",
      "Epoch 28078/30000 Training Loss: 0.038263168185949326\n",
      "Epoch 28079/30000 Training Loss: 0.049513500183820724\n",
      "Epoch 28080/30000 Training Loss: 0.04315025731921196\n",
      "Epoch 28081/30000 Training Loss: 0.045662060379981995\n",
      "Epoch 28082/30000 Training Loss: 0.04098956286907196\n",
      "Epoch 28083/30000 Training Loss: 0.050453849136829376\n",
      "Epoch 28084/30000 Training Loss: 0.05384472757577896\n",
      "Epoch 28085/30000 Training Loss: 0.04728754609823227\n",
      "Epoch 28086/30000 Training Loss: 0.050177112221717834\n",
      "Epoch 28087/30000 Training Loss: 0.04848223179578781\n",
      "Epoch 28088/30000 Training Loss: 0.03949170559644699\n",
      "Epoch 28089/30000 Training Loss: 0.044817596673965454\n",
      "Epoch 28090/30000 Training Loss: 0.04017014056444168\n",
      "Epoch 28091/30000 Training Loss: 0.045999471098184586\n",
      "Epoch 28092/30000 Training Loss: 0.06380430608987808\n",
      "Epoch 28093/30000 Training Loss: 0.04842788726091385\n",
      "Epoch 28094/30000 Training Loss: 0.057730477303266525\n",
      "Epoch 28095/30000 Training Loss: 0.05132272094488144\n",
      "Epoch 28096/30000 Training Loss: 0.06310450285673141\n",
      "Epoch 28097/30000 Training Loss: 0.043259426951408386\n",
      "Epoch 28098/30000 Training Loss: 0.052955348044633865\n",
      "Epoch 28099/30000 Training Loss: 0.06133992224931717\n",
      "Epoch 28100/30000 Training Loss: 0.046880971640348434\n",
      "Epoch 28100/30000 Validation Loss: 0.054543472826480865\n",
      "Epoch 28101/30000 Training Loss: 0.05761438235640526\n",
      "Epoch 28102/30000 Training Loss: 0.05014001205563545\n",
      "Epoch 28103/30000 Training Loss: 0.04484168067574501\n",
      "Epoch 28104/30000 Training Loss: 0.05747245252132416\n",
      "Epoch 28105/30000 Training Loss: 0.03818381205201149\n",
      "Epoch 28106/30000 Training Loss: 0.036036208271980286\n",
      "Epoch 28107/30000 Training Loss: 0.052390582859516144\n",
      "Epoch 28108/30000 Training Loss: 0.03883352130651474\n",
      "Epoch 28109/30000 Training Loss: 0.047213420271873474\n",
      "Epoch 28110/30000 Training Loss: 0.05310973525047302\n",
      "Epoch 28111/30000 Training Loss: 0.0467088483273983\n",
      "Epoch 28112/30000 Training Loss: 0.05069306492805481\n",
      "Epoch 28113/30000 Training Loss: 0.03899072855710983\n",
      "Epoch 28114/30000 Training Loss: 0.0453195795416832\n",
      "Epoch 28115/30000 Training Loss: 0.05671094357967377\n",
      "Epoch 28116/30000 Training Loss: 0.04057774692773819\n",
      "Epoch 28117/30000 Training Loss: 0.05214882642030716\n",
      "Epoch 28118/30000 Training Loss: 0.05336465686559677\n",
      "Epoch 28119/30000 Training Loss: 0.04607134312391281\n",
      "Epoch 28120/30000 Training Loss: 0.061979811638593674\n",
      "Epoch 28121/30000 Training Loss: 0.053755391389131546\n",
      "Epoch 28122/30000 Training Loss: 0.05287792533636093\n",
      "Epoch 28123/30000 Training Loss: 0.04270225763320923\n",
      "Epoch 28124/30000 Training Loss: 0.05129791796207428\n",
      "Epoch 28125/30000 Training Loss: 0.038414835929870605\n",
      "Epoch 28126/30000 Training Loss: 0.03650948032736778\n",
      "Epoch 28127/30000 Training Loss: 0.03433136269450188\n",
      "Epoch 28128/30000 Training Loss: 0.053349316120147705\n",
      "Epoch 28129/30000 Training Loss: 0.03299868479371071\n",
      "Epoch 28130/30000 Training Loss: 0.04262003302574158\n",
      "Epoch 28131/30000 Training Loss: 0.04222622513771057\n",
      "Epoch 28132/30000 Training Loss: 0.05387946218252182\n",
      "Epoch 28133/30000 Training Loss: 0.05485084652900696\n",
      "Epoch 28134/30000 Training Loss: 0.037962011992931366\n",
      "Epoch 28135/30000 Training Loss: 0.03372568264603615\n",
      "Epoch 28136/30000 Training Loss: 0.057164739817380905\n",
      "Epoch 28137/30000 Training Loss: 0.0555182620882988\n",
      "Epoch 28138/30000 Training Loss: 0.05133366957306862\n",
      "Epoch 28139/30000 Training Loss: 0.04385243356227875\n",
      "Epoch 28140/30000 Training Loss: 0.051013682037591934\n",
      "Epoch 28141/30000 Training Loss: 0.03680748492479324\n",
      "Epoch 28142/30000 Training Loss: 0.036129314452409744\n",
      "Epoch 28143/30000 Training Loss: 0.06286141276359558\n",
      "Epoch 28144/30000 Training Loss: 0.06735742092132568\n",
      "Epoch 28145/30000 Training Loss: 0.04240518435835838\n",
      "Epoch 28146/30000 Training Loss: 0.02907497249543667\n",
      "Epoch 28147/30000 Training Loss: 0.039507754147052765\n",
      "Epoch 28148/30000 Training Loss: 0.0324082113802433\n",
      "Epoch 28149/30000 Training Loss: 0.05731308460235596\n",
      "Epoch 28150/30000 Training Loss: 0.05097946897149086\n",
      "Epoch 28151/30000 Training Loss: 0.03177066147327423\n",
      "Epoch 28152/30000 Training Loss: 0.05819900706410408\n",
      "Epoch 28153/30000 Training Loss: 0.04661291837692261\n",
      "Epoch 28154/30000 Training Loss: 0.03810296580195427\n",
      "Epoch 28155/30000 Training Loss: 0.0470559187233448\n",
      "Epoch 28156/30000 Training Loss: 0.04490764066576958\n",
      "Epoch 28157/30000 Training Loss: 0.0531364269554615\n",
      "Epoch 28158/30000 Training Loss: 0.043251655995845795\n",
      "Epoch 28159/30000 Training Loss: 0.027307234704494476\n",
      "Epoch 28160/30000 Training Loss: 0.03729371353983879\n",
      "Epoch 28161/30000 Training Loss: 0.05129680782556534\n",
      "Epoch 28162/30000 Training Loss: 0.05020385608077049\n",
      "Epoch 28163/30000 Training Loss: 0.03838346526026726\n",
      "Epoch 28164/30000 Training Loss: 0.06729549169540405\n",
      "Epoch 28165/30000 Training Loss: 0.04805852472782135\n",
      "Epoch 28166/30000 Training Loss: 0.055166807025671005\n",
      "Epoch 28167/30000 Training Loss: 0.03249931335449219\n",
      "Epoch 28168/30000 Training Loss: 0.044533804059028625\n",
      "Epoch 28169/30000 Training Loss: 0.03731631487607956\n",
      "Epoch 28170/30000 Training Loss: 0.03809357434511185\n",
      "Epoch 28171/30000 Training Loss: 0.04725247621536255\n",
      "Epoch 28172/30000 Training Loss: 0.053725942969322205\n",
      "Epoch 28173/30000 Training Loss: 0.05016418546438217\n",
      "Epoch 28174/30000 Training Loss: 0.05506198853254318\n",
      "Epoch 28175/30000 Training Loss: 0.05235365033149719\n",
      "Epoch 28176/30000 Training Loss: 0.03345724195241928\n",
      "Epoch 28177/30000 Training Loss: 0.0431353822350502\n",
      "Epoch 28178/30000 Training Loss: 0.04514424130320549\n",
      "Epoch 28179/30000 Training Loss: 0.05019261687994003\n",
      "Epoch 28180/30000 Training Loss: 0.04719529673457146\n",
      "Epoch 28181/30000 Training Loss: 0.04030688852071762\n",
      "Epoch 28182/30000 Training Loss: 0.044411368668079376\n",
      "Epoch 28183/30000 Training Loss: 0.040338270366191864\n",
      "Epoch 28184/30000 Training Loss: 0.05703344941139221\n",
      "Epoch 28185/30000 Training Loss: 0.0406106598675251\n",
      "Epoch 28186/30000 Training Loss: 0.05436615273356438\n",
      "Epoch 28187/30000 Training Loss: 0.04448491334915161\n",
      "Epoch 28188/30000 Training Loss: 0.04837590083479881\n",
      "Epoch 28189/30000 Training Loss: 0.04400471970438957\n",
      "Epoch 28190/30000 Training Loss: 0.04576968029141426\n",
      "Epoch 28191/30000 Training Loss: 0.05193176493048668\n",
      "Epoch 28192/30000 Training Loss: 0.04864027351140976\n",
      "Epoch 28193/30000 Training Loss: 0.06845282763242722\n",
      "Epoch 28194/30000 Training Loss: 0.05865335464477539\n",
      "Epoch 28195/30000 Training Loss: 0.044325895607471466\n",
      "Epoch 28196/30000 Training Loss: 0.05602787435054779\n",
      "Epoch 28197/30000 Training Loss: 0.07432832568883896\n",
      "Epoch 28198/30000 Training Loss: 0.03792503848671913\n",
      "Epoch 28199/30000 Training Loss: 0.04334684833884239\n",
      "Epoch 28200/30000 Training Loss: 0.04786471650004387\n",
      "Epoch 28200/30000 Validation Loss: 0.04665679484605789\n",
      "Epoch 28201/30000 Training Loss: 0.0599815659224987\n",
      "Epoch 28202/30000 Training Loss: 0.051008448004722595\n",
      "Epoch 28203/30000 Training Loss: 0.060358524322509766\n",
      "Epoch 28204/30000 Training Loss: 0.03117748349905014\n",
      "Epoch 28205/30000 Training Loss: 0.04707972705364227\n",
      "Epoch 28206/30000 Training Loss: 0.04956025630235672\n",
      "Epoch 28207/30000 Training Loss: 0.04548794776201248\n",
      "Epoch 28208/30000 Training Loss: 0.0554385706782341\n",
      "Epoch 28209/30000 Training Loss: 0.055168915539979935\n",
      "Epoch 28210/30000 Training Loss: 0.071318618953228\n",
      "Epoch 28211/30000 Training Loss: 0.04637204110622406\n",
      "Epoch 28212/30000 Training Loss: 0.03750499337911606\n",
      "Epoch 28213/30000 Training Loss: 0.05869711935520172\n",
      "Epoch 28214/30000 Training Loss: 0.04396108165383339\n",
      "Epoch 28215/30000 Training Loss: 0.04717493802309036\n",
      "Epoch 28216/30000 Training Loss: 0.047312960028648376\n",
      "Epoch 28217/30000 Training Loss: 0.04972226917743683\n",
      "Epoch 28218/30000 Training Loss: 0.028852181509137154\n",
      "Epoch 28219/30000 Training Loss: 0.05443643033504486\n",
      "Epoch 28220/30000 Training Loss: 0.04562700539827347\n",
      "Epoch 28221/30000 Training Loss: 0.03566806763410568\n",
      "Epoch 28222/30000 Training Loss: 0.06412296742200851\n",
      "Epoch 28223/30000 Training Loss: 0.0513855442404747\n",
      "Epoch 28224/30000 Training Loss: 0.035450078547000885\n",
      "Epoch 28225/30000 Training Loss: 0.040140651166439056\n",
      "Epoch 28226/30000 Training Loss: 0.052715275436639786\n",
      "Epoch 28227/30000 Training Loss: 0.048306357115507126\n",
      "Epoch 28228/30000 Training Loss: 0.04124287888407707\n",
      "Epoch 28229/30000 Training Loss: 0.047176893800497055\n",
      "Epoch 28230/30000 Training Loss: 0.03571677953004837\n",
      "Epoch 28231/30000 Training Loss: 0.06205068528652191\n",
      "Epoch 28232/30000 Training Loss: 0.0391368493437767\n",
      "Epoch 28233/30000 Training Loss: 0.058472298085689545\n",
      "Epoch 28234/30000 Training Loss: 0.04202866926789284\n",
      "Epoch 28235/30000 Training Loss: 0.05279961973428726\n",
      "Epoch 28236/30000 Training Loss: 0.04407699778676033\n",
      "Epoch 28237/30000 Training Loss: 0.04702732339501381\n",
      "Epoch 28238/30000 Training Loss: 0.03905715048313141\n",
      "Epoch 28239/30000 Training Loss: 0.05712590739130974\n",
      "Epoch 28240/30000 Training Loss: 0.03587503731250763\n",
      "Epoch 28241/30000 Training Loss: 0.05006337910890579\n",
      "Epoch 28242/30000 Training Loss: 0.049275290220975876\n",
      "Epoch 28243/30000 Training Loss: 0.03564491495490074\n",
      "Epoch 28244/30000 Training Loss: 0.03342817723751068\n",
      "Epoch 28245/30000 Training Loss: 0.049129776656627655\n",
      "Epoch 28246/30000 Training Loss: 0.03790883719921112\n",
      "Epoch 28247/30000 Training Loss: 0.04750894755125046\n",
      "Epoch 28248/30000 Training Loss: 0.05288773030042648\n",
      "Epoch 28249/30000 Training Loss: 0.05032609403133392\n",
      "Epoch 28250/30000 Training Loss: 0.04777142405509949\n",
      "Epoch 28251/30000 Training Loss: 0.03826605901122093\n",
      "Epoch 28252/30000 Training Loss: 0.03868374228477478\n",
      "Epoch 28253/30000 Training Loss: 0.04953409358859062\n",
      "Epoch 28254/30000 Training Loss: 0.03986947983503342\n",
      "Epoch 28255/30000 Training Loss: 0.042470090091228485\n",
      "Epoch 28256/30000 Training Loss: 0.04236658290028572\n",
      "Epoch 28257/30000 Training Loss: 0.045694299042224884\n",
      "Epoch 28258/30000 Training Loss: 0.061416834592819214\n",
      "Epoch 28259/30000 Training Loss: 0.04970003291964531\n",
      "Epoch 28260/30000 Training Loss: 0.03921215608716011\n",
      "Epoch 28261/30000 Training Loss: 0.04976692795753479\n",
      "Epoch 28262/30000 Training Loss: 0.05204237625002861\n",
      "Epoch 28263/30000 Training Loss: 0.03763263672590256\n",
      "Epoch 28264/30000 Training Loss: 0.041513603180646896\n",
      "Epoch 28265/30000 Training Loss: 0.04541612043976784\n",
      "Epoch 28266/30000 Training Loss: 0.03700937330722809\n",
      "Epoch 28267/30000 Training Loss: 0.04354320093989372\n",
      "Epoch 28268/30000 Training Loss: 0.046603426337242126\n",
      "Epoch 28269/30000 Training Loss: 0.046398527920246124\n",
      "Epoch 28270/30000 Training Loss: 0.04573707655072212\n",
      "Epoch 28271/30000 Training Loss: 0.03168701007962227\n",
      "Epoch 28272/30000 Training Loss: 0.03896882012486458\n",
      "Epoch 28273/30000 Training Loss: 0.03994905576109886\n",
      "Epoch 28274/30000 Training Loss: 0.03703727945685387\n",
      "Epoch 28275/30000 Training Loss: 0.0427224226295948\n",
      "Epoch 28276/30000 Training Loss: 0.04991339519619942\n",
      "Epoch 28277/30000 Training Loss: 0.049658678472042084\n",
      "Epoch 28278/30000 Training Loss: 0.03687996417284012\n",
      "Epoch 28279/30000 Training Loss: 0.04013403505086899\n",
      "Epoch 28280/30000 Training Loss: 0.04966459423303604\n",
      "Epoch 28281/30000 Training Loss: 0.042515456676483154\n",
      "Epoch 28282/30000 Training Loss: 0.05587174743413925\n",
      "Epoch 28283/30000 Training Loss: 0.05178866535425186\n",
      "Epoch 28284/30000 Training Loss: 0.0477835051715374\n",
      "Epoch 28285/30000 Training Loss: 0.04793035611510277\n",
      "Epoch 28286/30000 Training Loss: 0.05066463351249695\n",
      "Epoch 28287/30000 Training Loss: 0.04166214168071747\n",
      "Epoch 28288/30000 Training Loss: 0.04381653293967247\n",
      "Epoch 28289/30000 Training Loss: 0.04342526197433472\n",
      "Epoch 28290/30000 Training Loss: 0.04710438847541809\n",
      "Epoch 28291/30000 Training Loss: 0.04107929393649101\n",
      "Epoch 28292/30000 Training Loss: 0.054776567965745926\n",
      "Epoch 28293/30000 Training Loss: 0.038775552064180374\n",
      "Epoch 28294/30000 Training Loss: 0.05469483509659767\n",
      "Epoch 28295/30000 Training Loss: 0.03661508113145828\n",
      "Epoch 28296/30000 Training Loss: 0.04970937967300415\n",
      "Epoch 28297/30000 Training Loss: 0.04179925099015236\n",
      "Epoch 28298/30000 Training Loss: 0.05407639592885971\n",
      "Epoch 28299/30000 Training Loss: 0.05382930859923363\n",
      "Epoch 28300/30000 Training Loss: 0.0661797821521759\n",
      "Epoch 28300/30000 Validation Loss: 0.05009957775473595\n",
      "Epoch 28301/30000 Training Loss: 0.028611529618501663\n",
      "Epoch 28302/30000 Training Loss: 0.04267409071326256\n",
      "Epoch 28303/30000 Training Loss: 0.050022803246974945\n",
      "Epoch 28304/30000 Training Loss: 0.043133243918418884\n",
      "Epoch 28305/30000 Training Loss: 0.04289564862847328\n",
      "Epoch 28306/30000 Training Loss: 0.05067376047372818\n",
      "Epoch 28307/30000 Training Loss: 0.056010741740465164\n",
      "Epoch 28308/30000 Training Loss: 0.044430844485759735\n",
      "Epoch 28309/30000 Training Loss: 0.0653839260339737\n",
      "Epoch 28310/30000 Training Loss: 0.04157990217208862\n",
      "Epoch 28311/30000 Training Loss: 0.037635136395692825\n",
      "Epoch 28312/30000 Training Loss: 0.04088524729013443\n",
      "Epoch 28313/30000 Training Loss: 0.03927231580018997\n",
      "Epoch 28314/30000 Training Loss: 0.0438358411192894\n",
      "Epoch 28315/30000 Training Loss: 0.04544898867607117\n",
      "Epoch 28316/30000 Training Loss: 0.031998444348573685\n",
      "Epoch 28317/30000 Training Loss: 0.04079852253198624\n",
      "Epoch 28318/30000 Training Loss: 0.04828144609928131\n",
      "Epoch 28319/30000 Training Loss: 0.031993210315704346\n",
      "Epoch 28320/30000 Training Loss: 0.05928949639201164\n",
      "Epoch 28321/30000 Training Loss: 0.0435522086918354\n",
      "Epoch 28322/30000 Training Loss: 0.043364401906728745\n",
      "Epoch 28323/30000 Training Loss: 0.052086908370256424\n",
      "Epoch 28324/30000 Training Loss: 0.03847339749336243\n",
      "Epoch 28325/30000 Training Loss: 0.03400883823633194\n",
      "Epoch 28326/30000 Training Loss: 0.04758841544389725\n",
      "Epoch 28327/30000 Training Loss: 0.04525766521692276\n",
      "Epoch 28328/30000 Training Loss: 0.053425636142492294\n",
      "Epoch 28329/30000 Training Loss: 0.04984617233276367\n",
      "Epoch 28330/30000 Training Loss: 0.0680595338344574\n",
      "Epoch 28331/30000 Training Loss: 0.051845524460077286\n",
      "Epoch 28332/30000 Training Loss: 0.05463072657585144\n",
      "Epoch 28333/30000 Training Loss: 0.03323439508676529\n",
      "Epoch 28334/30000 Training Loss: 0.035385824739933014\n",
      "Epoch 28335/30000 Training Loss: 0.06507333368062973\n",
      "Epoch 28336/30000 Training Loss: 0.051999714225530624\n",
      "Epoch 28337/30000 Training Loss: 0.05791653320193291\n",
      "Epoch 28338/30000 Training Loss: 0.03704269230365753\n",
      "Epoch 28339/30000 Training Loss: 0.03883300721645355\n",
      "Epoch 28340/30000 Training Loss: 0.06333062797784805\n",
      "Epoch 28341/30000 Training Loss: 0.05129019170999527\n",
      "Epoch 28342/30000 Training Loss: 0.03960500657558441\n",
      "Epoch 28343/30000 Training Loss: 0.03318559378385544\n",
      "Epoch 28344/30000 Training Loss: 0.041368238627910614\n",
      "Epoch 28345/30000 Training Loss: 0.05681933835148811\n",
      "Epoch 28346/30000 Training Loss: 0.04848219454288483\n",
      "Epoch 28347/30000 Training Loss: 0.04489567503333092\n",
      "Epoch 28348/30000 Training Loss: 0.050450608134269714\n",
      "Epoch 28349/30000 Training Loss: 0.03267350792884827\n",
      "Epoch 28350/30000 Training Loss: 0.05287571996450424\n",
      "Epoch 28351/30000 Training Loss: 0.06263896822929382\n",
      "Epoch 28352/30000 Training Loss: 0.03960731625556946\n",
      "Epoch 28353/30000 Training Loss: 0.05896366015076637\n",
      "Epoch 28354/30000 Training Loss: 0.043801598250865936\n",
      "Epoch 28355/30000 Training Loss: 0.056505851447582245\n",
      "Epoch 28356/30000 Training Loss: 0.04961659759283066\n",
      "Epoch 28357/30000 Training Loss: 0.04917001724243164\n",
      "Epoch 28358/30000 Training Loss: 0.04539883881807327\n",
      "Epoch 28359/30000 Training Loss: 0.05147299915552139\n",
      "Epoch 28360/30000 Training Loss: 0.0431496687233448\n",
      "Epoch 28361/30000 Training Loss: 0.047581110149621964\n",
      "Epoch 28362/30000 Training Loss: 0.04987724870443344\n",
      "Epoch 28363/30000 Training Loss: 0.04561985284090042\n",
      "Epoch 28364/30000 Training Loss: 0.03648636117577553\n",
      "Epoch 28365/30000 Training Loss: 0.037820108234882355\n",
      "Epoch 28366/30000 Training Loss: 0.04070417582988739\n",
      "Epoch 28367/30000 Training Loss: 0.04710385948419571\n",
      "Epoch 28368/30000 Training Loss: 0.0487697534263134\n",
      "Epoch 28369/30000 Training Loss: 0.04525168240070343\n",
      "Epoch 28370/30000 Training Loss: 0.05465492233633995\n",
      "Epoch 28371/30000 Training Loss: 0.033017273992300034\n",
      "Epoch 28372/30000 Training Loss: 0.039657335728406906\n",
      "Epoch 28373/30000 Training Loss: 0.03387250751256943\n",
      "Epoch 28374/30000 Training Loss: 0.052089206874370575\n",
      "Epoch 28375/30000 Training Loss: 0.052298225462436676\n",
      "Epoch 28376/30000 Training Loss: 0.045354072004556656\n",
      "Epoch 28377/30000 Training Loss: 0.051310401409864426\n",
      "Epoch 28378/30000 Training Loss: 0.042845189571380615\n",
      "Epoch 28379/30000 Training Loss: 0.034989405423402786\n",
      "Epoch 28380/30000 Training Loss: 0.042508214712142944\n",
      "Epoch 28381/30000 Training Loss: 0.03347519040107727\n",
      "Epoch 28382/30000 Training Loss: 0.052810944616794586\n",
      "Epoch 28383/30000 Training Loss: 0.038716576993465424\n",
      "Epoch 28384/30000 Training Loss: 0.050074175000190735\n",
      "Epoch 28385/30000 Training Loss: 0.04054948687553406\n",
      "Epoch 28386/30000 Training Loss: 0.03994601219892502\n",
      "Epoch 28387/30000 Training Loss: 0.04339320585131645\n",
      "Epoch 28388/30000 Training Loss: 0.045108407735824585\n",
      "Epoch 28389/30000 Training Loss: 0.0463937409222126\n",
      "Epoch 28390/30000 Training Loss: 0.05695529282093048\n",
      "Epoch 28391/30000 Training Loss: 0.04453306645154953\n",
      "Epoch 28392/30000 Training Loss: 0.03633686155080795\n",
      "Epoch 28393/30000 Training Loss: 0.04774266108870506\n",
      "Epoch 28394/30000 Training Loss: 0.04209525138139725\n",
      "Epoch 28395/30000 Training Loss: 0.043576501309871674\n",
      "Epoch 28396/30000 Training Loss: 0.04144986346364021\n",
      "Epoch 28397/30000 Training Loss: 0.04998571053147316\n",
      "Epoch 28398/30000 Training Loss: 0.05100182071328163\n",
      "Epoch 28399/30000 Training Loss: 0.0582074299454689\n",
      "Epoch 28400/30000 Training Loss: 0.0454086996614933\n",
      "Epoch 28400/30000 Validation Loss: 0.05637366324663162\n",
      "Epoch 28401/30000 Training Loss: 0.044269025325775146\n",
      "Epoch 28402/30000 Training Loss: 0.055392567068338394\n",
      "Epoch 28403/30000 Training Loss: 0.05411246418952942\n",
      "Epoch 28404/30000 Training Loss: 0.056970104575157166\n",
      "Epoch 28405/30000 Training Loss: 0.047531142830848694\n",
      "Epoch 28406/30000 Training Loss: 0.05972546339035034\n",
      "Epoch 28407/30000 Training Loss: 0.03957221657037735\n",
      "Epoch 28408/30000 Training Loss: 0.057687316089868546\n",
      "Epoch 28409/30000 Training Loss: 0.04501734673976898\n",
      "Epoch 28410/30000 Training Loss: 0.05062895640730858\n",
      "Epoch 28411/30000 Training Loss: 0.05016198009252548\n",
      "Epoch 28412/30000 Training Loss: 0.042376935482025146\n",
      "Epoch 28413/30000 Training Loss: 0.056517597287893295\n",
      "Epoch 28414/30000 Training Loss: 0.036242980509996414\n",
      "Epoch 28415/30000 Training Loss: 0.03436329588294029\n",
      "Epoch 28416/30000 Training Loss: 0.03214225172996521\n",
      "Epoch 28417/30000 Training Loss: 0.051387518644332886\n",
      "Epoch 28418/30000 Training Loss: 0.04043168947100639\n",
      "Epoch 28419/30000 Training Loss: 0.05087033659219742\n",
      "Epoch 28420/30000 Training Loss: 0.05443911999464035\n",
      "Epoch 28421/30000 Training Loss: 0.03021741285920143\n",
      "Epoch 28422/30000 Training Loss: 0.049861617386341095\n",
      "Epoch 28423/30000 Training Loss: 0.0567781999707222\n",
      "Epoch 28424/30000 Training Loss: 0.0574643649160862\n",
      "Epoch 28425/30000 Training Loss: 0.031106511130928993\n",
      "Epoch 28426/30000 Training Loss: 0.05768362805247307\n",
      "Epoch 28427/30000 Training Loss: 0.04832961782813072\n",
      "Epoch 28428/30000 Training Loss: 0.04550766944885254\n",
      "Epoch 28429/30000 Training Loss: 0.03856603801250458\n",
      "Epoch 28430/30000 Training Loss: 0.043830398470163345\n",
      "Epoch 28431/30000 Training Loss: 0.05939402058720589\n",
      "Epoch 28432/30000 Training Loss: 0.04695095866918564\n",
      "Epoch 28433/30000 Training Loss: 0.03992927446961403\n",
      "Epoch 28434/30000 Training Loss: 0.044089339673519135\n",
      "Epoch 28435/30000 Training Loss: 0.057390641421079636\n",
      "Epoch 28436/30000 Training Loss: 0.054377201944589615\n",
      "Epoch 28437/30000 Training Loss: 0.0402505099773407\n",
      "Epoch 28438/30000 Training Loss: 0.04248325526714325\n",
      "Epoch 28439/30000 Training Loss: 0.04650483652949333\n",
      "Epoch 28440/30000 Training Loss: 0.04894883930683136\n",
      "Epoch 28441/30000 Training Loss: 0.046653684228658676\n",
      "Epoch 28442/30000 Training Loss: 0.04876174405217171\n",
      "Epoch 28443/30000 Training Loss: 0.04548279568552971\n",
      "Epoch 28444/30000 Training Loss: 0.050178445875644684\n",
      "Epoch 28445/30000 Training Loss: 0.05370350554585457\n",
      "Epoch 28446/30000 Training Loss: 0.03274035453796387\n",
      "Epoch 28447/30000 Training Loss: 0.036820635199546814\n",
      "Epoch 28448/30000 Training Loss: 0.05321072041988373\n",
      "Epoch 28449/30000 Training Loss: 0.0406293049454689\n",
      "Epoch 28450/30000 Training Loss: 0.032368943095207214\n",
      "Epoch 28451/30000 Training Loss: 0.05739001929759979\n",
      "Epoch 28452/30000 Training Loss: 0.03191372752189636\n",
      "Epoch 28453/30000 Training Loss: 0.04122522473335266\n",
      "Epoch 28454/30000 Training Loss: 0.06040409579873085\n",
      "Epoch 28455/30000 Training Loss: 0.052959226071834564\n",
      "Epoch 28456/30000 Training Loss: 0.04297924041748047\n",
      "Epoch 28457/30000 Training Loss: 0.05312418192625046\n",
      "Epoch 28458/30000 Training Loss: 0.03959209844470024\n",
      "Epoch 28459/30000 Training Loss: 0.056540347635746\n",
      "Epoch 28460/30000 Training Loss: 0.050467196851968765\n",
      "Epoch 28461/30000 Training Loss: 0.04550555720925331\n",
      "Epoch 28462/30000 Training Loss: 0.034157607704401016\n",
      "Epoch 28463/30000 Training Loss: 0.05455760657787323\n",
      "Epoch 28464/30000 Training Loss: 0.027887677773833275\n",
      "Epoch 28465/30000 Training Loss: 0.04720435291528702\n",
      "Epoch 28466/30000 Training Loss: 0.0453353077173233\n",
      "Epoch 28467/30000 Training Loss: 0.050897471606731415\n",
      "Epoch 28468/30000 Training Loss: 0.04860231280326843\n",
      "Epoch 28469/30000 Training Loss: 0.04946735128760338\n",
      "Epoch 28470/30000 Training Loss: 0.03248923271894455\n",
      "Epoch 28471/30000 Training Loss: 0.058511435985565186\n",
      "Epoch 28472/30000 Training Loss: 0.03259764611721039\n",
      "Epoch 28473/30000 Training Loss: 0.037183977663517\n",
      "Epoch 28474/30000 Training Loss: 0.05209049955010414\n",
      "Epoch 28475/30000 Training Loss: 0.04251867160201073\n",
      "Epoch 28476/30000 Training Loss: 0.03221253305673599\n",
      "Epoch 28477/30000 Training Loss: 0.039378274232149124\n",
      "Epoch 28478/30000 Training Loss: 0.04809410870075226\n",
      "Epoch 28479/30000 Training Loss: 0.04358641430735588\n",
      "Epoch 28480/30000 Training Loss: 0.041195351630449295\n",
      "Epoch 28481/30000 Training Loss: 0.05787072330713272\n",
      "Epoch 28482/30000 Training Loss: 0.047469496726989746\n",
      "Epoch 28483/30000 Training Loss: 0.043127574026584625\n",
      "Epoch 28484/30000 Training Loss: 0.05272718518972397\n",
      "Epoch 28485/30000 Training Loss: 0.03794826567173004\n",
      "Epoch 28486/30000 Training Loss: 0.04687291383743286\n",
      "Epoch 28487/30000 Training Loss: 0.05744260549545288\n",
      "Epoch 28488/30000 Training Loss: 0.05460832640528679\n",
      "Epoch 28489/30000 Training Loss: 0.0841187983751297\n",
      "Epoch 28490/30000 Training Loss: 0.05800740048289299\n",
      "Epoch 28491/30000 Training Loss: 0.044184062629938126\n",
      "Epoch 28492/30000 Training Loss: 0.04126004874706268\n",
      "Epoch 28493/30000 Training Loss: 0.054257094860076904\n",
      "Epoch 28494/30000 Training Loss: 0.03356180712580681\n",
      "Epoch 28495/30000 Training Loss: 0.05339324474334717\n",
      "Epoch 28496/30000 Training Loss: 0.034034278243780136\n",
      "Epoch 28497/30000 Training Loss: 0.03649720549583435\n",
      "Epoch 28498/30000 Training Loss: 0.033178966492414474\n",
      "Epoch 28499/30000 Training Loss: 0.04271405562758446\n",
      "Epoch 28500/30000 Training Loss: 0.0650373101234436\n",
      "Epoch 28500/30000 Validation Loss: 0.03962165117263794\n",
      "Epoch 28501/30000 Training Loss: 0.052049972116947174\n",
      "Epoch 28502/30000 Training Loss: 0.039040639996528625\n",
      "Epoch 28503/30000 Training Loss: 0.0597311295568943\n",
      "Epoch 28504/30000 Training Loss: 0.042655590921640396\n",
      "Epoch 28505/30000 Training Loss: 0.06135532632470131\n",
      "Epoch 28506/30000 Training Loss: 0.07371636480093002\n",
      "Epoch 28507/30000 Training Loss: 0.04631534218788147\n",
      "Epoch 28508/30000 Training Loss: 0.043011248111724854\n",
      "Epoch 28509/30000 Training Loss: 0.05089908838272095\n",
      "Epoch 28510/30000 Training Loss: 0.04449567571282387\n",
      "Epoch 28511/30000 Training Loss: 0.03501684218645096\n",
      "Epoch 28512/30000 Training Loss: 0.03402841463685036\n",
      "Epoch 28513/30000 Training Loss: 0.04210244491696358\n",
      "Epoch 28514/30000 Training Loss: 0.03654859587550163\n",
      "Epoch 28515/30000 Training Loss: 0.03883525729179382\n",
      "Epoch 28516/30000 Training Loss: 0.05271536856889725\n",
      "Epoch 28517/30000 Training Loss: 0.04743966832756996\n",
      "Epoch 28518/30000 Training Loss: 0.05158365145325661\n",
      "Epoch 28519/30000 Training Loss: 0.04747548699378967\n",
      "Epoch 28520/30000 Training Loss: 0.034835271537303925\n",
      "Epoch 28521/30000 Training Loss: 0.05508486181497574\n",
      "Epoch 28522/30000 Training Loss: 0.036569785326719284\n",
      "Epoch 28523/30000 Training Loss: 0.04448133707046509\n",
      "Epoch 28524/30000 Training Loss: 0.042132019996643066\n",
      "Epoch 28525/30000 Training Loss: 0.043795689940452576\n",
      "Epoch 28526/30000 Training Loss: 0.05456385016441345\n",
      "Epoch 28527/30000 Training Loss: 0.053225692361593246\n",
      "Epoch 28528/30000 Training Loss: 0.050843074917793274\n",
      "Epoch 28529/30000 Training Loss: 0.03853645920753479\n",
      "Epoch 28530/30000 Training Loss: 0.03210010752081871\n",
      "Epoch 28531/30000 Training Loss: 0.04082430526614189\n",
      "Epoch 28532/30000 Training Loss: 0.06191123276948929\n",
      "Epoch 28533/30000 Training Loss: 0.05128590390086174\n",
      "Epoch 28534/30000 Training Loss: 0.04786421358585358\n",
      "Epoch 28535/30000 Training Loss: 0.048148658126592636\n",
      "Epoch 28536/30000 Training Loss: 0.04600992426276207\n",
      "Epoch 28537/30000 Training Loss: 0.04114594683051109\n",
      "Epoch 28538/30000 Training Loss: 0.03962218761444092\n",
      "Epoch 28539/30000 Training Loss: 0.05613860860466957\n",
      "Epoch 28540/30000 Training Loss: 0.046565067023038864\n",
      "Epoch 28541/30000 Training Loss: 0.04542512446641922\n",
      "Epoch 28542/30000 Training Loss: 0.04681168869137764\n",
      "Epoch 28543/30000 Training Loss: 0.048469435423612595\n",
      "Epoch 28544/30000 Training Loss: 0.03330300748348236\n",
      "Epoch 28545/30000 Training Loss: 0.054030682891607285\n",
      "Epoch 28546/30000 Training Loss: 0.044210344552993774\n",
      "Epoch 28547/30000 Training Loss: 0.07301533222198486\n",
      "Epoch 28548/30000 Training Loss: 0.046067655086517334\n",
      "Epoch 28549/30000 Training Loss: 0.055170658975839615\n",
      "Epoch 28550/30000 Training Loss: 0.04031018912792206\n",
      "Epoch 28551/30000 Training Loss: 0.05163189768791199\n",
      "Epoch 28552/30000 Training Loss: 0.03696657344698906\n",
      "Epoch 28553/30000 Training Loss: 0.03516688197851181\n",
      "Epoch 28554/30000 Training Loss: 0.038962457329034805\n",
      "Epoch 28555/30000 Training Loss: 0.04352368786931038\n",
      "Epoch 28556/30000 Training Loss: 0.047365158796310425\n",
      "Epoch 28557/30000 Training Loss: 0.061869531869888306\n",
      "Epoch 28558/30000 Training Loss: 0.05158788710832596\n",
      "Epoch 28559/30000 Training Loss: 0.04286728426814079\n",
      "Epoch 28560/30000 Training Loss: 0.040633928030729294\n",
      "Epoch 28561/30000 Training Loss: 0.05337115377187729\n",
      "Epoch 28562/30000 Training Loss: 0.04858262836933136\n",
      "Epoch 28563/30000 Training Loss: 0.04766901209950447\n",
      "Epoch 28564/30000 Training Loss: 0.05249276012182236\n",
      "Epoch 28565/30000 Training Loss: 0.029747866094112396\n",
      "Epoch 28566/30000 Training Loss: 0.05405406281352043\n",
      "Epoch 28567/30000 Training Loss: 0.05065733939409256\n",
      "Epoch 28568/30000 Training Loss: 0.03900790959596634\n",
      "Epoch 28569/30000 Training Loss: 0.03918350487947464\n",
      "Epoch 28570/30000 Training Loss: 0.04623790457844734\n",
      "Epoch 28571/30000 Training Loss: 0.055795323103666306\n",
      "Epoch 28572/30000 Training Loss: 0.05086363106966019\n",
      "Epoch 28573/30000 Training Loss: 0.06107066944241524\n",
      "Epoch 28574/30000 Training Loss: 0.05373309552669525\n",
      "Epoch 28575/30000 Training Loss: 0.05356014519929886\n",
      "Epoch 28576/30000 Training Loss: 0.062459565699100494\n",
      "Epoch 28577/30000 Training Loss: 0.058297060430049896\n",
      "Epoch 28578/30000 Training Loss: 0.04301983863115311\n",
      "Epoch 28579/30000 Training Loss: 0.056323327124118805\n",
      "Epoch 28580/30000 Training Loss: 0.03580193594098091\n",
      "Epoch 28581/30000 Training Loss: 0.04013815149664879\n",
      "Epoch 28582/30000 Training Loss: 0.05037038028240204\n",
      "Epoch 28583/30000 Training Loss: 0.06808742880821228\n",
      "Epoch 28584/30000 Training Loss: 0.03539635241031647\n",
      "Epoch 28585/30000 Training Loss: 0.05309908837080002\n",
      "Epoch 28586/30000 Training Loss: 0.03803667053580284\n",
      "Epoch 28587/30000 Training Loss: 0.03894751891493797\n",
      "Epoch 28588/30000 Training Loss: 0.04589102044701576\n",
      "Epoch 28589/30000 Training Loss: 0.0382809117436409\n",
      "Epoch 28590/30000 Training Loss: 0.057783275842666626\n",
      "Epoch 28591/30000 Training Loss: 0.04503069818019867\n",
      "Epoch 28592/30000 Training Loss: 0.049002159386873245\n",
      "Epoch 28593/30000 Training Loss: 0.040523115545511246\n",
      "Epoch 28594/30000 Training Loss: 0.039331890642642975\n",
      "Epoch 28595/30000 Training Loss: 0.05077802762389183\n",
      "Epoch 28596/30000 Training Loss: 0.04439669847488403\n",
      "Epoch 28597/30000 Training Loss: 0.053555432707071304\n",
      "Epoch 28598/30000 Training Loss: 0.04406210035085678\n",
      "Epoch 28599/30000 Training Loss: 0.03471175581216812\n",
      "Epoch 28600/30000 Training Loss: 0.06299508363008499\n",
      "Epoch 28600/30000 Validation Loss: 0.0732164978981018\n",
      "Epoch 28601/30000 Training Loss: 0.04834669083356857\n",
      "Epoch 28602/30000 Training Loss: 0.04321849346160889\n",
      "Epoch 28603/30000 Training Loss: 0.03914359584450722\n",
      "Epoch 28604/30000 Training Loss: 0.03410358354449272\n",
      "Epoch 28605/30000 Training Loss: 0.044418610632419586\n",
      "Epoch 28606/30000 Training Loss: 0.0677156150341034\n",
      "Epoch 28607/30000 Training Loss: 0.047913894057273865\n",
      "Epoch 28608/30000 Training Loss: 0.04217902570962906\n",
      "Epoch 28609/30000 Training Loss: 0.04084683582186699\n",
      "Epoch 28610/30000 Training Loss: 0.061031587421894073\n",
      "Epoch 28611/30000 Training Loss: 0.05326540023088455\n",
      "Epoch 28612/30000 Training Loss: 0.03114502876996994\n",
      "Epoch 28613/30000 Training Loss: 0.05424347147345543\n",
      "Epoch 28614/30000 Training Loss: 0.04366641119122505\n",
      "Epoch 28615/30000 Training Loss: 0.04738672450184822\n",
      "Epoch 28616/30000 Training Loss: 0.054585665464401245\n",
      "Epoch 28617/30000 Training Loss: 0.02862844616174698\n",
      "Epoch 28618/30000 Training Loss: 0.042928799986839294\n",
      "Epoch 28619/30000 Training Loss: 0.04884863644838333\n",
      "Epoch 28620/30000 Training Loss: 0.0418509803712368\n",
      "Epoch 28621/30000 Training Loss: 0.059013016521930695\n",
      "Epoch 28622/30000 Training Loss: 0.043888337910175323\n",
      "Epoch 28623/30000 Training Loss: 0.03155814856290817\n",
      "Epoch 28624/30000 Training Loss: 0.046177688986063004\n",
      "Epoch 28625/30000 Training Loss: 0.04518738016486168\n",
      "Epoch 28626/30000 Training Loss: 0.04514724761247635\n",
      "Epoch 28627/30000 Training Loss: 0.035164475440979004\n",
      "Epoch 28628/30000 Training Loss: 0.05011530593037605\n",
      "Epoch 28629/30000 Training Loss: 0.03763357922434807\n",
      "Epoch 28630/30000 Training Loss: 0.04873521998524666\n",
      "Epoch 28631/30000 Training Loss: 0.03887386620044708\n",
      "Epoch 28632/30000 Training Loss: 0.04870764911174774\n",
      "Epoch 28633/30000 Training Loss: 0.057349104434251785\n",
      "Epoch 28634/30000 Training Loss: 0.058770231902599335\n",
      "Epoch 28635/30000 Training Loss: 0.05451461672782898\n",
      "Epoch 28636/30000 Training Loss: 0.03515559434890747\n",
      "Epoch 28637/30000 Training Loss: 0.0385335348546505\n",
      "Epoch 28638/30000 Training Loss: 0.04145854711532593\n",
      "Epoch 28639/30000 Training Loss: 0.04582309350371361\n",
      "Epoch 28640/30000 Training Loss: 0.04373911768198013\n",
      "Epoch 28641/30000 Training Loss: 0.04354376345872879\n",
      "Epoch 28642/30000 Training Loss: 0.04764189198613167\n",
      "Epoch 28643/30000 Training Loss: 0.04369872063398361\n",
      "Epoch 28644/30000 Training Loss: 0.048435308039188385\n",
      "Epoch 28645/30000 Training Loss: 0.03502099961042404\n",
      "Epoch 28646/30000 Training Loss: 0.0578521229326725\n",
      "Epoch 28647/30000 Training Loss: 0.03904125466942787\n",
      "Epoch 28648/30000 Training Loss: 0.037142977118492126\n",
      "Epoch 28649/30000 Training Loss: 0.04045785218477249\n",
      "Epoch 28650/30000 Training Loss: 0.04262736812233925\n",
      "Epoch 28651/30000 Training Loss: 0.048034295439720154\n",
      "Epoch 28652/30000 Training Loss: 0.04733674228191376\n",
      "Epoch 28653/30000 Training Loss: 0.042938701808452606\n",
      "Epoch 28654/30000 Training Loss: 0.056189510971307755\n",
      "Epoch 28655/30000 Training Loss: 0.046585820615291595\n",
      "Epoch 28656/30000 Training Loss: 0.037537287920713425\n",
      "Epoch 28657/30000 Training Loss: 0.04681776836514473\n",
      "Epoch 28658/30000 Training Loss: 0.048979707062244415\n",
      "Epoch 28659/30000 Training Loss: 0.04809568077325821\n",
      "Epoch 28660/30000 Training Loss: 0.0378258116543293\n",
      "Epoch 28661/30000 Training Loss: 0.04719766229391098\n",
      "Epoch 28662/30000 Training Loss: 0.0401826873421669\n",
      "Epoch 28663/30000 Training Loss: 0.06530721485614777\n",
      "Epoch 28664/30000 Training Loss: 0.04573827609419823\n",
      "Epoch 28665/30000 Training Loss: 0.04274928569793701\n",
      "Epoch 28666/30000 Training Loss: 0.05130648612976074\n",
      "Epoch 28667/30000 Training Loss: 0.04727189987897873\n",
      "Epoch 28668/30000 Training Loss: 0.048387009650468826\n",
      "Epoch 28669/30000 Training Loss: 0.051437314599752426\n",
      "Epoch 28670/30000 Training Loss: 0.043824583292007446\n",
      "Epoch 28671/30000 Training Loss: 0.04510343074798584\n",
      "Epoch 28672/30000 Training Loss: 0.037926264107227325\n",
      "Epoch 28673/30000 Training Loss: 0.04762357100844383\n",
      "Epoch 28674/30000 Training Loss: 0.04533206298947334\n",
      "Epoch 28675/30000 Training Loss: 0.04450652375817299\n",
      "Epoch 28676/30000 Training Loss: 0.05181929096579552\n",
      "Epoch 28677/30000 Training Loss: 0.02661718800663948\n",
      "Epoch 28678/30000 Training Loss: 0.05290362983942032\n",
      "Epoch 28679/30000 Training Loss: 0.034813012927770615\n",
      "Epoch 28680/30000 Training Loss: 0.03819140046834946\n",
      "Epoch 28681/30000 Training Loss: 0.04053184390068054\n",
      "Epoch 28682/30000 Training Loss: 0.031320322304964066\n",
      "Epoch 28683/30000 Training Loss: 0.05181996524333954\n",
      "Epoch 28684/30000 Training Loss: 0.03891347721219063\n",
      "Epoch 28685/30000 Training Loss: 0.04642689600586891\n",
      "Epoch 28686/30000 Training Loss: 0.049539029598236084\n",
      "Epoch 28687/30000 Training Loss: 0.0419674888253212\n",
      "Epoch 28688/30000 Training Loss: 0.04458481818437576\n",
      "Epoch 28689/30000 Training Loss: 0.05825866758823395\n",
      "Epoch 28690/30000 Training Loss: 0.04835100471973419\n",
      "Epoch 28691/30000 Training Loss: 0.052519768476486206\n",
      "Epoch 28692/30000 Training Loss: 0.05766814947128296\n",
      "Epoch 28693/30000 Training Loss: 0.05292132869362831\n",
      "Epoch 28694/30000 Training Loss: 0.047633636742830276\n",
      "Epoch 28695/30000 Training Loss: 0.046390604227781296\n",
      "Epoch 28696/30000 Training Loss: 0.048580702394247055\n",
      "Epoch 28697/30000 Training Loss: 0.0382256880402565\n",
      "Epoch 28698/30000 Training Loss: 0.05991978198289871\n",
      "Epoch 28699/30000 Training Loss: 0.041226405650377274\n",
      "Epoch 28700/30000 Training Loss: 0.03441797196865082\n",
      "Epoch 28700/30000 Validation Loss: 0.0410286970436573\n",
      "Epoch 28701/30000 Training Loss: 0.05401374772191048\n",
      "Epoch 28702/30000 Training Loss: 0.06195397675037384\n",
      "Epoch 28703/30000 Training Loss: 0.05211886018514633\n",
      "Epoch 28704/30000 Training Loss: 0.05182737857103348\n",
      "Epoch 28705/30000 Training Loss: 0.05028431862592697\n",
      "Epoch 28706/30000 Training Loss: 0.05415032058954239\n",
      "Epoch 28707/30000 Training Loss: 0.05192243307828903\n",
      "Epoch 28708/30000 Training Loss: 0.02975638210773468\n",
      "Epoch 28709/30000 Training Loss: 0.054347820580005646\n",
      "Epoch 28710/30000 Training Loss: 0.05042523890733719\n",
      "Epoch 28711/30000 Training Loss: 0.06034388765692711\n",
      "Epoch 28712/30000 Training Loss: 0.039779651910066605\n",
      "Epoch 28713/30000 Training Loss: 0.04848435893654823\n",
      "Epoch 28714/30000 Training Loss: 0.03715316206216812\n",
      "Epoch 28715/30000 Training Loss: 0.05004354938864708\n",
      "Epoch 28716/30000 Training Loss: 0.05556578189134598\n",
      "Epoch 28717/30000 Training Loss: 0.04825465753674507\n",
      "Epoch 28718/30000 Training Loss: 0.029517104849219322\n",
      "Epoch 28719/30000 Training Loss: 0.0518457368016243\n",
      "Epoch 28720/30000 Training Loss: 0.042810384184122086\n",
      "Epoch 28721/30000 Training Loss: 0.03518737107515335\n",
      "Epoch 28722/30000 Training Loss: 0.047806575894355774\n",
      "Epoch 28723/30000 Training Loss: 0.0454632043838501\n",
      "Epoch 28724/30000 Training Loss: 0.05070093274116516\n",
      "Epoch 28725/30000 Training Loss: 0.04592295363545418\n",
      "Epoch 28726/30000 Training Loss: 0.03944368287920952\n",
      "Epoch 28727/30000 Training Loss: 0.04973633214831352\n",
      "Epoch 28728/30000 Training Loss: 0.03927818685770035\n",
      "Epoch 28729/30000 Training Loss: 0.038683995604515076\n",
      "Epoch 28730/30000 Training Loss: 0.06261993199586868\n",
      "Epoch 28731/30000 Training Loss: 0.04754738509654999\n",
      "Epoch 28732/30000 Training Loss: 0.04437725245952606\n",
      "Epoch 28733/30000 Training Loss: 0.04367562755942345\n",
      "Epoch 28734/30000 Training Loss: 0.07020530104637146\n",
      "Epoch 28735/30000 Training Loss: 0.04495717212557793\n",
      "Epoch 28736/30000 Training Loss: 0.044107913970947266\n",
      "Epoch 28737/30000 Training Loss: 0.04484320059418678\n",
      "Epoch 28738/30000 Training Loss: 0.04783850908279419\n",
      "Epoch 28739/30000 Training Loss: 0.043273866176605225\n",
      "Epoch 28740/30000 Training Loss: 0.04158991947770119\n",
      "Epoch 28741/30000 Training Loss: 0.05255543068051338\n",
      "Epoch 28742/30000 Training Loss: 0.042644161731004715\n",
      "Epoch 28743/30000 Training Loss: 0.04088012874126434\n",
      "Epoch 28744/30000 Training Loss: 0.03700609132647514\n",
      "Epoch 28745/30000 Training Loss: 0.05315514653921127\n",
      "Epoch 28746/30000 Training Loss: 0.03420416638255119\n",
      "Epoch 28747/30000 Training Loss: 0.05364416912198067\n",
      "Epoch 28748/30000 Training Loss: 0.061760786920785904\n",
      "Epoch 28749/30000 Training Loss: 0.05551750957965851\n",
      "Epoch 28750/30000 Training Loss: 0.045212097465991974\n",
      "Epoch 28751/30000 Training Loss: 0.057991087436676025\n",
      "Epoch 28752/30000 Training Loss: 0.0372372530400753\n",
      "Epoch 28753/30000 Training Loss: 0.0484032928943634\n",
      "Epoch 28754/30000 Training Loss: 0.034148797392845154\n",
      "Epoch 28755/30000 Training Loss: 0.05135232210159302\n",
      "Epoch 28756/30000 Training Loss: 0.053926050662994385\n",
      "Epoch 28757/30000 Training Loss: 0.042603325098752975\n",
      "Epoch 28758/30000 Training Loss: 0.0475134402513504\n",
      "Epoch 28759/30000 Training Loss: 0.042942725121974945\n",
      "Epoch 28760/30000 Training Loss: 0.03997302055358887\n",
      "Epoch 28761/30000 Training Loss: 0.06284651160240173\n",
      "Epoch 28762/30000 Training Loss: 0.04189373180270195\n",
      "Epoch 28763/30000 Training Loss: 0.045676812529563904\n",
      "Epoch 28764/30000 Training Loss: 0.061596523970365524\n",
      "Epoch 28765/30000 Training Loss: 0.04396483302116394\n",
      "Epoch 28766/30000 Training Loss: 0.055019959807395935\n",
      "Epoch 28767/30000 Training Loss: 0.03610675036907196\n",
      "Epoch 28768/30000 Training Loss: 0.04624127969145775\n",
      "Epoch 28769/30000 Training Loss: 0.03461378812789917\n",
      "Epoch 28770/30000 Training Loss: 0.058034852147102356\n",
      "Epoch 28771/30000 Training Loss: 0.03672721982002258\n",
      "Epoch 28772/30000 Training Loss: 0.0595492348074913\n",
      "Epoch 28773/30000 Training Loss: 0.06283076852560043\n",
      "Epoch 28774/30000 Training Loss: 0.03337685018777847\n",
      "Epoch 28775/30000 Training Loss: 0.05656447261571884\n",
      "Epoch 28776/30000 Training Loss: 0.044503696262836456\n",
      "Epoch 28777/30000 Training Loss: 0.04788089171051979\n",
      "Epoch 28778/30000 Training Loss: 0.04495704174041748\n",
      "Epoch 28779/30000 Training Loss: 0.05217893421649933\n",
      "Epoch 28780/30000 Training Loss: 0.04307384788990021\n",
      "Epoch 28781/30000 Training Loss: 0.04394819587469101\n",
      "Epoch 28782/30000 Training Loss: 0.038577862083911896\n",
      "Epoch 28783/30000 Training Loss: 0.027731629088521004\n",
      "Epoch 28784/30000 Training Loss: 0.04741228371858597\n",
      "Epoch 28785/30000 Training Loss: 0.03983234986662865\n",
      "Epoch 28786/30000 Training Loss: 0.042393263429403305\n",
      "Epoch 28787/30000 Training Loss: 0.041729725897312164\n",
      "Epoch 28788/30000 Training Loss: 0.036912161856889725\n",
      "Epoch 28789/30000 Training Loss: 0.03511833772063255\n",
      "Epoch 28790/30000 Training Loss: 0.06000072509050369\n",
      "Epoch 28791/30000 Training Loss: 0.0392848402261734\n",
      "Epoch 28792/30000 Training Loss: 0.03972996398806572\n",
      "Epoch 28793/30000 Training Loss: 0.02700423263013363\n",
      "Epoch 28794/30000 Training Loss: 0.04026414826512337\n",
      "Epoch 28795/30000 Training Loss: 0.03426583111286163\n",
      "Epoch 28796/30000 Training Loss: 0.03561463952064514\n",
      "Epoch 28797/30000 Training Loss: 0.043244924396276474\n",
      "Epoch 28798/30000 Training Loss: 0.038667380809783936\n",
      "Epoch 28799/30000 Training Loss: 0.04179422929883003\n",
      "Epoch 28800/30000 Training Loss: 0.04010995104908943\n",
      "Epoch 28800/30000 Validation Loss: 0.05493351072072983\n",
      "Epoch 28801/30000 Training Loss: 0.05524056404829025\n",
      "Epoch 28802/30000 Training Loss: 0.059160016477108\n",
      "Epoch 28803/30000 Training Loss: 0.051768720149993896\n",
      "Epoch 28804/30000 Training Loss: 0.05984601378440857\n",
      "Epoch 28805/30000 Training Loss: 0.03199207782745361\n",
      "Epoch 28806/30000 Training Loss: 0.05605512857437134\n",
      "Epoch 28807/30000 Training Loss: 0.03744105249643326\n",
      "Epoch 28808/30000 Training Loss: 0.051465317606925964\n",
      "Epoch 28809/30000 Training Loss: 0.04377859830856323\n",
      "Epoch 28810/30000 Training Loss: 0.05748623609542847\n",
      "Epoch 28811/30000 Training Loss: 0.05085837095975876\n",
      "Epoch 28812/30000 Training Loss: 0.060812562704086304\n",
      "Epoch 28813/30000 Training Loss: 0.036155760288238525\n",
      "Epoch 28814/30000 Training Loss: 0.03531960770487785\n",
      "Epoch 28815/30000 Training Loss: 0.03517383337020874\n",
      "Epoch 28816/30000 Training Loss: 0.04163691774010658\n",
      "Epoch 28817/30000 Training Loss: 0.03891400247812271\n",
      "Epoch 28818/30000 Training Loss: 0.03199067711830139\n",
      "Epoch 28819/30000 Training Loss: 0.04289029911160469\n",
      "Epoch 28820/30000 Training Loss: 0.047130465507507324\n",
      "Epoch 28821/30000 Training Loss: 0.03855844587087631\n",
      "Epoch 28822/30000 Training Loss: 0.046724412590265274\n",
      "Epoch 28823/30000 Training Loss: 0.03898334503173828\n",
      "Epoch 28824/30000 Training Loss: 0.04447803646326065\n",
      "Epoch 28825/30000 Training Loss: 0.06350897252559662\n",
      "Epoch 28826/30000 Training Loss: 0.040996767580509186\n",
      "Epoch 28827/30000 Training Loss: 0.04273681715130806\n",
      "Epoch 28828/30000 Training Loss: 0.06285037100315094\n",
      "Epoch 28829/30000 Training Loss: 0.04538701847195625\n",
      "Epoch 28830/30000 Training Loss: 0.060948412865400314\n",
      "Epoch 28831/30000 Training Loss: 0.04281572625041008\n",
      "Epoch 28832/30000 Training Loss: 0.04989347606897354\n",
      "Epoch 28833/30000 Training Loss: 0.035847872495651245\n",
      "Epoch 28834/30000 Training Loss: 0.045448146760463715\n",
      "Epoch 28835/30000 Training Loss: 0.041833825409412384\n",
      "Epoch 28836/30000 Training Loss: 0.04855286329984665\n",
      "Epoch 28837/30000 Training Loss: 0.06119265779852867\n",
      "Epoch 28838/30000 Training Loss: 0.04880911856889725\n",
      "Epoch 28839/30000 Training Loss: 0.052517205476760864\n",
      "Epoch 28840/30000 Training Loss: 0.05125431716442108\n",
      "Epoch 28841/30000 Training Loss: 0.029619459062814713\n",
      "Epoch 28842/30000 Training Loss: 0.04783722385764122\n",
      "Epoch 28843/30000 Training Loss: 0.0744219422340393\n",
      "Epoch 28844/30000 Training Loss: 0.033765919506549835\n",
      "Epoch 28845/30000 Training Loss: 0.05671396106481552\n",
      "Epoch 28846/30000 Training Loss: 0.05490768700838089\n",
      "Epoch 28847/30000 Training Loss: 0.0320972241461277\n",
      "Epoch 28848/30000 Training Loss: 0.040017277002334595\n",
      "Epoch 28849/30000 Training Loss: 0.0397455058991909\n",
      "Epoch 28850/30000 Training Loss: 0.03181419149041176\n",
      "Epoch 28851/30000 Training Loss: 0.06731364130973816\n",
      "Epoch 28852/30000 Training Loss: 0.05877488851547241\n",
      "Epoch 28853/30000 Training Loss: 0.03964712843298912\n",
      "Epoch 28854/30000 Training Loss: 0.0633002370595932\n",
      "Epoch 28855/30000 Training Loss: 0.04962688311934471\n",
      "Epoch 28856/30000 Training Loss: 0.042091500014066696\n",
      "Epoch 28857/30000 Training Loss: 0.03413432464003563\n",
      "Epoch 28858/30000 Training Loss: 0.04934556409716606\n",
      "Epoch 28859/30000 Training Loss: 0.04313536360859871\n",
      "Epoch 28860/30000 Training Loss: 0.047550689429044724\n",
      "Epoch 28861/30000 Training Loss: 0.05525589734315872\n",
      "Epoch 28862/30000 Training Loss: 0.04826854541897774\n",
      "Epoch 28863/30000 Training Loss: 0.049758218228816986\n",
      "Epoch 28864/30000 Training Loss: 0.0428658202290535\n",
      "Epoch 28865/30000 Training Loss: 0.036611903458833694\n",
      "Epoch 28866/30000 Training Loss: 0.04839198291301727\n",
      "Epoch 28867/30000 Training Loss: 0.05232881382107735\n",
      "Epoch 28868/30000 Training Loss: 0.04235902056097984\n",
      "Epoch 28869/30000 Training Loss: 0.0379016138613224\n",
      "Epoch 28870/30000 Training Loss: 0.0444745235145092\n",
      "Epoch 28871/30000 Training Loss: 0.05232555419206619\n",
      "Epoch 28872/30000 Training Loss: 0.05963464081287384\n",
      "Epoch 28873/30000 Training Loss: 0.0417984239757061\n",
      "Epoch 28874/30000 Training Loss: 0.05367781221866608\n",
      "Epoch 28875/30000 Training Loss: 0.03838356211781502\n",
      "Epoch 28876/30000 Training Loss: 0.04792163521051407\n",
      "Epoch 28877/30000 Training Loss: 0.035484421998262405\n",
      "Epoch 28878/30000 Training Loss: 0.04041732847690582\n",
      "Epoch 28879/30000 Training Loss: 0.0609387531876564\n",
      "Epoch 28880/30000 Training Loss: 0.032713018357753754\n",
      "Epoch 28881/30000 Training Loss: 0.05600336939096451\n",
      "Epoch 28882/30000 Training Loss: 0.047432128340005875\n",
      "Epoch 28883/30000 Training Loss: 0.05023963004350662\n",
      "Epoch 28884/30000 Training Loss: 0.04611789807677269\n",
      "Epoch 28885/30000 Training Loss: 0.0497523732483387\n",
      "Epoch 28886/30000 Training Loss: 0.05015411972999573\n",
      "Epoch 28887/30000 Training Loss: 0.04182720184326172\n",
      "Epoch 28888/30000 Training Loss: 0.05326516553759575\n",
      "Epoch 28889/30000 Training Loss: 0.04294396936893463\n",
      "Epoch 28890/30000 Training Loss: 0.047299012541770935\n",
      "Epoch 28891/30000 Training Loss: 0.0453086718916893\n",
      "Epoch 28892/30000 Training Loss: 0.03779256343841553\n",
      "Epoch 28893/30000 Training Loss: 0.03053337335586548\n",
      "Epoch 28894/30000 Training Loss: 0.04520339518785477\n",
      "Epoch 28895/30000 Training Loss: 0.032453905791044235\n",
      "Epoch 28896/30000 Training Loss: 0.051281195133924484\n",
      "Epoch 28897/30000 Training Loss: 0.03815631940960884\n",
      "Epoch 28898/30000 Training Loss: 0.05068034678697586\n",
      "Epoch 28899/30000 Training Loss: 0.04006858542561531\n",
      "Epoch 28900/30000 Training Loss: 0.03228333592414856\n",
      "Epoch 28900/30000 Validation Loss: 0.059399351477622986\n",
      "Epoch 28901/30000 Training Loss: 0.04440344497561455\n",
      "Epoch 28902/30000 Training Loss: 0.04343404248356819\n",
      "Epoch 28903/30000 Training Loss: 0.05179961770772934\n",
      "Epoch 28904/30000 Training Loss: 0.042591460049152374\n",
      "Epoch 28905/30000 Training Loss: 0.04543592780828476\n",
      "Epoch 28906/30000 Training Loss: 0.0330459326505661\n",
      "Epoch 28907/30000 Training Loss: 0.04370874911546707\n",
      "Epoch 28908/30000 Training Loss: 0.04481380432844162\n",
      "Epoch 28909/30000 Training Loss: 0.04607858136296272\n",
      "Epoch 28910/30000 Training Loss: 0.06103312224149704\n",
      "Epoch 28911/30000 Training Loss: 0.04078766703605652\n",
      "Epoch 28912/30000 Training Loss: 0.04562148451805115\n",
      "Epoch 28913/30000 Training Loss: 0.04855211079120636\n",
      "Epoch 28914/30000 Training Loss: 0.0361703597009182\n",
      "Epoch 28915/30000 Training Loss: 0.04280811920762062\n",
      "Epoch 28916/30000 Training Loss: 0.061290957033634186\n",
      "Epoch 28917/30000 Training Loss: 0.046361539512872696\n",
      "Epoch 28918/30000 Training Loss: 0.03748580068349838\n",
      "Epoch 28919/30000 Training Loss: 0.04761375114321709\n",
      "Epoch 28920/30000 Training Loss: 0.03135167434811592\n",
      "Epoch 28921/30000 Training Loss: 0.053246937692165375\n",
      "Epoch 28922/30000 Training Loss: 0.038942836225032806\n",
      "Epoch 28923/30000 Training Loss: 0.052350349724292755\n",
      "Epoch 28924/30000 Training Loss: 0.0471203476190567\n",
      "Epoch 28925/30000 Training Loss: 0.04776230454444885\n",
      "Epoch 28926/30000 Training Loss: 0.058472178876399994\n",
      "Epoch 28927/30000 Training Loss: 0.03660592436790466\n",
      "Epoch 28928/30000 Training Loss: 0.06342460960149765\n",
      "Epoch 28929/30000 Training Loss: 0.04618968069553375\n",
      "Epoch 28930/30000 Training Loss: 0.03628300502896309\n",
      "Epoch 28931/30000 Training Loss: 0.05116630345582962\n",
      "Epoch 28932/30000 Training Loss: 0.03375222161412239\n",
      "Epoch 28933/30000 Training Loss: 0.033153656870126724\n",
      "Epoch 28934/30000 Training Loss: 0.04997194558382034\n",
      "Epoch 28935/30000 Training Loss: 0.04593053460121155\n",
      "Epoch 28936/30000 Training Loss: 0.036799512803554535\n",
      "Epoch 28937/30000 Training Loss: 0.03794994205236435\n",
      "Epoch 28938/30000 Training Loss: 0.05212831869721413\n",
      "Epoch 28939/30000 Training Loss: 0.049342721700668335\n",
      "Epoch 28940/30000 Training Loss: 0.045632749795913696\n",
      "Epoch 28941/30000 Training Loss: 0.04391959309577942\n",
      "Epoch 28942/30000 Training Loss: 0.0488322451710701\n",
      "Epoch 28943/30000 Training Loss: 0.043376289308071136\n",
      "Epoch 28944/30000 Training Loss: 0.052353501319885254\n",
      "Epoch 28945/30000 Training Loss: 0.06498117744922638\n",
      "Epoch 28946/30000 Training Loss: 0.04891417175531387\n",
      "Epoch 28947/30000 Training Loss: 0.03625005483627319\n",
      "Epoch 28948/30000 Training Loss: 0.03778541833162308\n",
      "Epoch 28949/30000 Training Loss: 0.04265835881233215\n",
      "Epoch 28950/30000 Training Loss: 0.034860361367464066\n",
      "Epoch 28951/30000 Training Loss: 0.05222666636109352\n",
      "Epoch 28952/30000 Training Loss: 0.05036427080631256\n",
      "Epoch 28953/30000 Training Loss: 0.04332488402724266\n",
      "Epoch 28954/30000 Training Loss: 0.04638299718499184\n",
      "Epoch 28955/30000 Training Loss: 0.04144873097538948\n",
      "Epoch 28956/30000 Training Loss: 0.04437488690018654\n",
      "Epoch 28957/30000 Training Loss: 0.050392962992191315\n",
      "Epoch 28958/30000 Training Loss: 0.048063747584819794\n",
      "Epoch 28959/30000 Training Loss: 0.04918037727475166\n",
      "Epoch 28960/30000 Training Loss: 0.057388804852962494\n",
      "Epoch 28961/30000 Training Loss: 0.06127581745386124\n",
      "Epoch 28962/30000 Training Loss: 0.04234433174133301\n",
      "Epoch 28963/30000 Training Loss: 0.045664191246032715\n",
      "Epoch 28964/30000 Training Loss: 0.043131738901138306\n",
      "Epoch 28965/30000 Training Loss: 0.04593674838542938\n",
      "Epoch 28966/30000 Training Loss: 0.03396906703710556\n",
      "Epoch 28967/30000 Training Loss: 0.03823733702301979\n",
      "Epoch 28968/30000 Training Loss: 0.04020273685455322\n",
      "Epoch 28969/30000 Training Loss: 0.03802769258618355\n",
      "Epoch 28970/30000 Training Loss: 0.04076096788048744\n",
      "Epoch 28971/30000 Training Loss: 0.03339659422636032\n",
      "Epoch 28972/30000 Training Loss: 0.04019256681203842\n",
      "Epoch 28973/30000 Training Loss: 0.03551608324050903\n",
      "Epoch 28974/30000 Training Loss: 0.04717361554503441\n",
      "Epoch 28975/30000 Training Loss: 0.0400385707616806\n",
      "Epoch 28976/30000 Training Loss: 0.037750985473394394\n",
      "Epoch 28977/30000 Training Loss: 0.040665168315172195\n",
      "Epoch 28978/30000 Training Loss: 0.04885010048747063\n",
      "Epoch 28979/30000 Training Loss: 0.04919802397489548\n",
      "Epoch 28980/30000 Training Loss: 0.033364225178956985\n",
      "Epoch 28981/30000 Training Loss: 0.06417027115821838\n",
      "Epoch 28982/30000 Training Loss: 0.06404202431440353\n",
      "Epoch 28983/30000 Training Loss: 0.041290923953056335\n",
      "Epoch 28984/30000 Training Loss: 0.035209521651268005\n",
      "Epoch 28985/30000 Training Loss: 0.05609210580587387\n",
      "Epoch 28986/30000 Training Loss: 0.046286441385746\n",
      "Epoch 28987/30000 Training Loss: 0.05687430500984192\n",
      "Epoch 28988/30000 Training Loss: 0.04331749677658081\n",
      "Epoch 28989/30000 Training Loss: 0.04622916132211685\n",
      "Epoch 28990/30000 Training Loss: 0.040672969073057175\n",
      "Epoch 28991/30000 Training Loss: 0.03517235442996025\n",
      "Epoch 28992/30000 Training Loss: 0.03948451578617096\n",
      "Epoch 28993/30000 Training Loss: 0.04644592106342316\n",
      "Epoch 28994/30000 Training Loss: 0.03865332156419754\n",
      "Epoch 28995/30000 Training Loss: 0.047481734305620193\n",
      "Epoch 28996/30000 Training Loss: 0.03246254101395607\n",
      "Epoch 28997/30000 Training Loss: 0.039437126368284225\n",
      "Epoch 28998/30000 Training Loss: 0.05548841506242752\n",
      "Epoch 28999/30000 Training Loss: 0.04363736882805824\n",
      "Epoch 29000/30000 Training Loss: 0.04035056009888649\n",
      "Epoch 29000/30000 Validation Loss: 0.03310098126530647\n",
      "Epoch 29001/30000 Training Loss: 0.0674419030547142\n",
      "Epoch 29002/30000 Training Loss: 0.039744216948747635\n",
      "Epoch 29003/30000 Training Loss: 0.045505695044994354\n",
      "Epoch 29004/30000 Training Loss: 0.04344351217150688\n",
      "Epoch 29005/30000 Training Loss: 0.053009115159511566\n",
      "Epoch 29006/30000 Training Loss: 0.041243068873882294\n",
      "Epoch 29007/30000 Training Loss: 0.06791163980960846\n",
      "Epoch 29008/30000 Training Loss: 0.042635105550289154\n",
      "Epoch 29009/30000 Training Loss: 0.05044201388955116\n",
      "Epoch 29010/30000 Training Loss: 0.04862327501177788\n",
      "Epoch 29011/30000 Training Loss: 0.0409412756562233\n",
      "Epoch 29012/30000 Training Loss: 0.0311250202357769\n",
      "Epoch 29013/30000 Training Loss: 0.0641290694475174\n",
      "Epoch 29014/30000 Training Loss: 0.04349978268146515\n",
      "Epoch 29015/30000 Training Loss: 0.04401232302188873\n",
      "Epoch 29016/30000 Training Loss: 0.07385715842247009\n",
      "Epoch 29017/30000 Training Loss: 0.051416438072919846\n",
      "Epoch 29018/30000 Training Loss: 0.03491190820932388\n",
      "Epoch 29019/30000 Training Loss: 0.042729903012514114\n",
      "Epoch 29020/30000 Training Loss: 0.04128655046224594\n",
      "Epoch 29021/30000 Training Loss: 0.04100635647773743\n",
      "Epoch 29022/30000 Training Loss: 0.05756375193595886\n",
      "Epoch 29023/30000 Training Loss: 0.04268208518624306\n",
      "Epoch 29024/30000 Training Loss: 0.03601580113172531\n",
      "Epoch 29025/30000 Training Loss: 0.0572332963347435\n",
      "Epoch 29026/30000 Training Loss: 0.043232936412096024\n",
      "Epoch 29027/30000 Training Loss: 0.053264375776052475\n",
      "Epoch 29028/30000 Training Loss: 0.062069982290267944\n",
      "Epoch 29029/30000 Training Loss: 0.03642212972044945\n",
      "Epoch 29030/30000 Training Loss: 0.02469763346016407\n",
      "Epoch 29031/30000 Training Loss: 0.04336778074502945\n",
      "Epoch 29032/30000 Training Loss: 0.04781555011868477\n",
      "Epoch 29033/30000 Training Loss: 0.049042731523513794\n",
      "Epoch 29034/30000 Training Loss: 0.04036804288625717\n",
      "Epoch 29035/30000 Training Loss: 0.04214661940932274\n",
      "Epoch 29036/30000 Training Loss: 0.037465907633304596\n",
      "Epoch 29037/30000 Training Loss: 0.047709543257951736\n",
      "Epoch 29038/30000 Training Loss: 0.06258022040128708\n",
      "Epoch 29039/30000 Training Loss: 0.030068453401327133\n",
      "Epoch 29040/30000 Training Loss: 0.047833316028118134\n",
      "Epoch 29041/30000 Training Loss: 0.03565795719623566\n",
      "Epoch 29042/30000 Training Loss: 0.04743599146604538\n",
      "Epoch 29043/30000 Training Loss: 0.0415700227022171\n",
      "Epoch 29044/30000 Training Loss: 0.04105989634990692\n",
      "Epoch 29045/30000 Training Loss: 0.044102419167757034\n",
      "Epoch 29046/30000 Training Loss: 0.05762083828449249\n",
      "Epoch 29047/30000 Training Loss: 0.05859362334012985\n",
      "Epoch 29048/30000 Training Loss: 0.044236935675144196\n",
      "Epoch 29049/30000 Training Loss: 0.0607837438583374\n",
      "Epoch 29050/30000 Training Loss: 0.05006004869937897\n",
      "Epoch 29051/30000 Training Loss: 0.048283714801073074\n",
      "Epoch 29052/30000 Training Loss: 0.05588194727897644\n",
      "Epoch 29053/30000 Training Loss: 0.04719892144203186\n",
      "Epoch 29054/30000 Training Loss: 0.05668463185429573\n",
      "Epoch 29055/30000 Training Loss: 0.039871737360954285\n",
      "Epoch 29056/30000 Training Loss: 0.04232507199048996\n",
      "Epoch 29057/30000 Training Loss: 0.02752096578478813\n",
      "Epoch 29058/30000 Training Loss: 0.04323716089129448\n",
      "Epoch 29059/30000 Training Loss: 0.04998922348022461\n",
      "Epoch 29060/30000 Training Loss: 0.04487989470362663\n",
      "Epoch 29061/30000 Training Loss: 0.04060903936624527\n",
      "Epoch 29062/30000 Training Loss: 0.03583722561597824\n",
      "Epoch 29063/30000 Training Loss: 0.0569683238863945\n",
      "Epoch 29064/30000 Training Loss: 0.05375470221042633\n",
      "Epoch 29065/30000 Training Loss: 0.03623535484075546\n",
      "Epoch 29066/30000 Training Loss: 0.033599745482206345\n",
      "Epoch 29067/30000 Training Loss: 0.04050176590681076\n",
      "Epoch 29068/30000 Training Loss: 0.03843142092227936\n",
      "Epoch 29069/30000 Training Loss: 0.049629464745521545\n",
      "Epoch 29070/30000 Training Loss: 0.04090893641114235\n",
      "Epoch 29071/30000 Training Loss: 0.03222188353538513\n",
      "Epoch 29072/30000 Training Loss: 0.05305879935622215\n",
      "Epoch 29073/30000 Training Loss: 0.04830571636557579\n",
      "Epoch 29074/30000 Training Loss: 0.04269637167453766\n",
      "Epoch 29075/30000 Training Loss: 0.060011669993400574\n",
      "Epoch 29076/30000 Training Loss: 0.056919507682323456\n",
      "Epoch 29077/30000 Training Loss: 0.0389757864177227\n",
      "Epoch 29078/30000 Training Loss: 0.028028102591633797\n",
      "Epoch 29079/30000 Training Loss: 0.03803381323814392\n",
      "Epoch 29080/30000 Training Loss: 0.036566682159900665\n",
      "Epoch 29081/30000 Training Loss: 0.043937649577856064\n",
      "Epoch 29082/30000 Training Loss: 0.05198089778423309\n",
      "Epoch 29083/30000 Training Loss: 0.05043547973036766\n",
      "Epoch 29084/30000 Training Loss: 0.034754179418087006\n",
      "Epoch 29085/30000 Training Loss: 0.0416286438703537\n",
      "Epoch 29086/30000 Training Loss: 0.061387285590171814\n",
      "Epoch 29087/30000 Training Loss: 0.04801509529352188\n",
      "Epoch 29088/30000 Training Loss: 0.05346579849720001\n",
      "Epoch 29089/30000 Training Loss: 0.04381236806511879\n",
      "Epoch 29090/30000 Training Loss: 0.04742544889450073\n",
      "Epoch 29091/30000 Training Loss: 0.05519925057888031\n",
      "Epoch 29092/30000 Training Loss: 0.056268416345119476\n",
      "Epoch 29093/30000 Training Loss: 0.0324491448700428\n",
      "Epoch 29094/30000 Training Loss: 0.03676651790738106\n",
      "Epoch 29095/30000 Training Loss: 0.05608197674155235\n",
      "Epoch 29096/30000 Training Loss: 0.04301796108484268\n",
      "Epoch 29097/30000 Training Loss: 0.05149514228105545\n",
      "Epoch 29098/30000 Training Loss: 0.04640587419271469\n",
      "Epoch 29099/30000 Training Loss: 0.05277812480926514\n",
      "Epoch 29100/30000 Training Loss: 0.047349344938993454\n",
      "Epoch 29100/30000 Validation Loss: 0.047712236642837524\n",
      "Epoch 29101/30000 Training Loss: 0.04320202395319939\n",
      "Epoch 29102/30000 Training Loss: 0.05532189458608627\n",
      "Epoch 29103/30000 Training Loss: 0.04601150378584862\n",
      "Epoch 29104/30000 Training Loss: 0.052585698664188385\n",
      "Epoch 29105/30000 Training Loss: 0.043279748409986496\n",
      "Epoch 29106/30000 Training Loss: 0.032517075538635254\n",
      "Epoch 29107/30000 Training Loss: 0.044009122997522354\n",
      "Epoch 29108/30000 Training Loss: 0.05866673216223717\n",
      "Epoch 29109/30000 Training Loss: 0.03369908034801483\n",
      "Epoch 29110/30000 Training Loss: 0.044827450066804886\n",
      "Epoch 29111/30000 Training Loss: 0.045536305755376816\n",
      "Epoch 29112/30000 Training Loss: 0.046187110245227814\n",
      "Epoch 29113/30000 Training Loss: 0.04034649580717087\n",
      "Epoch 29114/30000 Training Loss: 0.060756854712963104\n",
      "Epoch 29115/30000 Training Loss: 0.03821578249335289\n",
      "Epoch 29116/30000 Training Loss: 0.03130517154932022\n",
      "Epoch 29117/30000 Training Loss: 0.04694049432873726\n",
      "Epoch 29118/30000 Training Loss: 0.052611030638217926\n",
      "Epoch 29119/30000 Training Loss: 0.039482664316892624\n",
      "Epoch 29120/30000 Training Loss: 0.05670984089374542\n",
      "Epoch 29121/30000 Training Loss: 0.047543514519929886\n",
      "Epoch 29122/30000 Training Loss: 0.046664759516716\n",
      "Epoch 29123/30000 Training Loss: 0.03395891189575195\n",
      "Epoch 29124/30000 Training Loss: 0.04729681462049484\n",
      "Epoch 29125/30000 Training Loss: 0.05768682435154915\n",
      "Epoch 29126/30000 Training Loss: 0.043309882283210754\n",
      "Epoch 29127/30000 Training Loss: 0.04634439945220947\n",
      "Epoch 29128/30000 Training Loss: 0.05028025805950165\n",
      "Epoch 29129/30000 Training Loss: 0.039588309824466705\n",
      "Epoch 29130/30000 Training Loss: 0.03859475255012512\n",
      "Epoch 29131/30000 Training Loss: 0.03973452374339104\n",
      "Epoch 29132/30000 Training Loss: 0.04469314217567444\n",
      "Epoch 29133/30000 Training Loss: 0.046287860721349716\n",
      "Epoch 29134/30000 Training Loss: 0.04917272925376892\n",
      "Epoch 29135/30000 Training Loss: 0.04241059347987175\n",
      "Epoch 29136/30000 Training Loss: 0.04715390130877495\n",
      "Epoch 29137/30000 Training Loss: 0.05926092714071274\n",
      "Epoch 29138/30000 Training Loss: 0.04534148424863815\n",
      "Epoch 29139/30000 Training Loss: 0.038278728723526\n",
      "Epoch 29140/30000 Training Loss: 0.03417770937085152\n",
      "Epoch 29141/30000 Training Loss: 0.03963693231344223\n",
      "Epoch 29142/30000 Training Loss: 0.07028636336326599\n",
      "Epoch 29143/30000 Training Loss: 0.04279198497533798\n",
      "Epoch 29144/30000 Training Loss: 0.04177943617105484\n",
      "Epoch 29145/30000 Training Loss: 0.06171068549156189\n",
      "Epoch 29146/30000 Training Loss: 0.05293326452374458\n",
      "Epoch 29147/30000 Training Loss: 0.05020143836736679\n",
      "Epoch 29148/30000 Training Loss: 0.042408302426338196\n",
      "Epoch 29149/30000 Training Loss: 0.04099956154823303\n",
      "Epoch 29150/30000 Training Loss: 0.057759929448366165\n",
      "Epoch 29151/30000 Training Loss: 0.048955559730529785\n",
      "Epoch 29152/30000 Training Loss: 0.04744962230324745\n",
      "Epoch 29153/30000 Training Loss: 0.036480385810136795\n",
      "Epoch 29154/30000 Training Loss: 0.05471706762909889\n",
      "Epoch 29155/30000 Training Loss: 0.036361634731292725\n",
      "Epoch 29156/30000 Training Loss: 0.03970819711685181\n",
      "Epoch 29157/30000 Training Loss: 0.03901265561580658\n",
      "Epoch 29158/30000 Training Loss: 0.03853549063205719\n",
      "Epoch 29159/30000 Training Loss: 0.0545801967382431\n",
      "Epoch 29160/30000 Training Loss: 0.05421195551753044\n",
      "Epoch 29161/30000 Training Loss: 0.0459054633975029\n",
      "Epoch 29162/30000 Training Loss: 0.04118535667657852\n",
      "Epoch 29163/30000 Training Loss: 0.04543823376297951\n",
      "Epoch 29164/30000 Training Loss: 0.04486021399497986\n",
      "Epoch 29165/30000 Training Loss: 0.035884931683540344\n",
      "Epoch 29166/30000 Training Loss: 0.042442966252565384\n",
      "Epoch 29167/30000 Training Loss: 0.05549690127372742\n",
      "Epoch 29168/30000 Training Loss: 0.03603064641356468\n",
      "Epoch 29169/30000 Training Loss: 0.02807305008172989\n",
      "Epoch 29170/30000 Training Loss: 0.050540100783109665\n",
      "Epoch 29171/30000 Training Loss: 0.04317338392138481\n",
      "Epoch 29172/30000 Training Loss: 0.05237221345305443\n",
      "Epoch 29173/30000 Training Loss: 0.03857038542628288\n",
      "Epoch 29174/30000 Training Loss: 0.036035407334566116\n",
      "Epoch 29175/30000 Training Loss: 0.03371405601501465\n",
      "Epoch 29176/30000 Training Loss: 0.044338956475257874\n",
      "Epoch 29177/30000 Training Loss: 0.03779374063014984\n",
      "Epoch 29178/30000 Training Loss: 0.0364777073264122\n",
      "Epoch 29179/30000 Training Loss: 0.045549217611551285\n",
      "Epoch 29180/30000 Training Loss: 0.0452849417924881\n",
      "Epoch 29181/30000 Training Loss: 0.034018777310848236\n",
      "Epoch 29182/30000 Training Loss: 0.04947895556688309\n",
      "Epoch 29183/30000 Training Loss: 0.04267112538218498\n",
      "Epoch 29184/30000 Training Loss: 0.05550849437713623\n",
      "Epoch 29185/30000 Training Loss: 0.04632420837879181\n",
      "Epoch 29186/30000 Training Loss: 0.046654608100652695\n",
      "Epoch 29187/30000 Training Loss: 0.04687044024467468\n",
      "Epoch 29188/30000 Training Loss: 0.04323042556643486\n",
      "Epoch 29189/30000 Training Loss: 0.04064271226525307\n",
      "Epoch 29190/30000 Training Loss: 0.05732088163495064\n",
      "Epoch 29191/30000 Training Loss: 0.032269734889268875\n",
      "Epoch 29192/30000 Training Loss: 0.04415994510054588\n",
      "Epoch 29193/30000 Training Loss: 0.04123762995004654\n",
      "Epoch 29194/30000 Training Loss: 0.0642317682504654\n",
      "Epoch 29195/30000 Training Loss: 0.04425068944692612\n",
      "Epoch 29196/30000 Training Loss: 0.04106885939836502\n",
      "Epoch 29197/30000 Training Loss: 0.03979668766260147\n",
      "Epoch 29198/30000 Training Loss: 0.032216716557741165\n",
      "Epoch 29199/30000 Training Loss: 0.03402470797300339\n",
      "Epoch 29200/30000 Training Loss: 0.05002451315522194\n",
      "Epoch 29200/30000 Validation Loss: 0.03531000390648842\n",
      "Epoch 29201/30000 Training Loss: 0.04074734449386597\n",
      "Epoch 29202/30000 Training Loss: 0.039796069264411926\n",
      "Epoch 29203/30000 Training Loss: 0.0541587732732296\n",
      "Epoch 29204/30000 Training Loss: 0.04498961567878723\n",
      "Epoch 29205/30000 Training Loss: 0.051854848861694336\n",
      "Epoch 29206/30000 Training Loss: 0.05829526484012604\n",
      "Epoch 29207/30000 Training Loss: 0.04392348974943161\n",
      "Epoch 29208/30000 Training Loss: 0.052602704614400864\n",
      "Epoch 29209/30000 Training Loss: 0.04834684357047081\n",
      "Epoch 29210/30000 Training Loss: 0.03853749483823776\n",
      "Epoch 29211/30000 Training Loss: 0.0559573620557785\n",
      "Epoch 29212/30000 Training Loss: 0.054114826023578644\n",
      "Epoch 29213/30000 Training Loss: 0.05058002099394798\n",
      "Epoch 29214/30000 Training Loss: 0.05585288628935814\n",
      "Epoch 29215/30000 Training Loss: 0.05354095995426178\n",
      "Epoch 29216/30000 Training Loss: 0.04423104226589203\n",
      "Epoch 29217/30000 Training Loss: 0.04320026934146881\n",
      "Epoch 29218/30000 Training Loss: 0.05883807688951492\n",
      "Epoch 29219/30000 Training Loss: 0.053369127213954926\n",
      "Epoch 29220/30000 Training Loss: 0.05030234903097153\n",
      "Epoch 29221/30000 Training Loss: 0.0332883857190609\n",
      "Epoch 29222/30000 Training Loss: 0.05265560373663902\n",
      "Epoch 29223/30000 Training Loss: 0.03635707125067711\n",
      "Epoch 29224/30000 Training Loss: 0.06839619576931\n",
      "Epoch 29225/30000 Training Loss: 0.040922604501247406\n",
      "Epoch 29226/30000 Training Loss: 0.04017490893602371\n",
      "Epoch 29227/30000 Training Loss: 0.05425271391868591\n",
      "Epoch 29228/30000 Training Loss: 0.05283317714929581\n",
      "Epoch 29229/30000 Training Loss: 0.03866616263985634\n",
      "Epoch 29230/30000 Training Loss: 0.0458407998085022\n",
      "Epoch 29231/30000 Training Loss: 0.048447031527757645\n",
      "Epoch 29232/30000 Training Loss: 0.04181473329663277\n",
      "Epoch 29233/30000 Training Loss: 0.0489380918443203\n",
      "Epoch 29234/30000 Training Loss: 0.04622916132211685\n",
      "Epoch 29235/30000 Training Loss: 0.039187945425510406\n",
      "Epoch 29236/30000 Training Loss: 0.04629286378622055\n",
      "Epoch 29237/30000 Training Loss: 0.0329112634062767\n",
      "Epoch 29238/30000 Training Loss: 0.04447675496339798\n",
      "Epoch 29239/30000 Training Loss: 0.032994456589221954\n",
      "Epoch 29240/30000 Training Loss: 0.05446002259850502\n",
      "Epoch 29241/30000 Training Loss: 0.04231126606464386\n",
      "Epoch 29242/30000 Training Loss: 0.037077344954013824\n",
      "Epoch 29243/30000 Training Loss: 0.05614133179187775\n",
      "Epoch 29244/30000 Training Loss: 0.038882069289684296\n",
      "Epoch 29245/30000 Training Loss: 0.04633118957281113\n",
      "Epoch 29246/30000 Training Loss: 0.04706626757979393\n",
      "Epoch 29247/30000 Training Loss: 0.04382453113794327\n",
      "Epoch 29248/30000 Training Loss: 0.04659605026245117\n",
      "Epoch 29249/30000 Training Loss: 0.04204696789383888\n",
      "Epoch 29250/30000 Training Loss: 0.04233494773507118\n",
      "Epoch 29251/30000 Training Loss: 0.03872067481279373\n",
      "Epoch 29252/30000 Training Loss: 0.04669754207134247\n",
      "Epoch 29253/30000 Training Loss: 0.05944342911243439\n",
      "Epoch 29254/30000 Training Loss: 0.045055948197841644\n",
      "Epoch 29255/30000 Training Loss: 0.04069973900914192\n",
      "Epoch 29256/30000 Training Loss: 0.053253937512636185\n",
      "Epoch 29257/30000 Training Loss: 0.027166545391082764\n",
      "Epoch 29258/30000 Training Loss: 0.0604531392455101\n",
      "Epoch 29259/30000 Training Loss: 0.037262771278619766\n",
      "Epoch 29260/30000 Training Loss: 0.04623636603355408\n",
      "Epoch 29261/30000 Training Loss: 0.037324704229831696\n",
      "Epoch 29262/30000 Training Loss: 0.031611718237400055\n",
      "Epoch 29263/30000 Training Loss: 0.048710621893405914\n",
      "Epoch 29264/30000 Training Loss: 0.052013952285051346\n",
      "Epoch 29265/30000 Training Loss: 0.07105240225791931\n",
      "Epoch 29266/30000 Training Loss: 0.0554560162127018\n",
      "Epoch 29267/30000 Training Loss: 0.03995577618479729\n",
      "Epoch 29268/30000 Training Loss: 0.04476328194141388\n",
      "Epoch 29269/30000 Training Loss: 0.037954892963171005\n",
      "Epoch 29270/30000 Training Loss: 0.04978937655687332\n",
      "Epoch 29271/30000 Training Loss: 0.04263465106487274\n",
      "Epoch 29272/30000 Training Loss: 0.044424768537282944\n",
      "Epoch 29273/30000 Training Loss: 0.06721464544534683\n",
      "Epoch 29274/30000 Training Loss: 0.0430368110537529\n",
      "Epoch 29275/30000 Training Loss: 0.06600842624902725\n",
      "Epoch 29276/30000 Training Loss: 0.05206913873553276\n",
      "Epoch 29277/30000 Training Loss: 0.047851528972387314\n",
      "Epoch 29278/30000 Training Loss: 0.05197688937187195\n",
      "Epoch 29279/30000 Training Loss: 0.03743448480963707\n",
      "Epoch 29280/30000 Training Loss: 0.04076949134469032\n",
      "Epoch 29281/30000 Training Loss: 0.049595288932323456\n",
      "Epoch 29282/30000 Training Loss: 0.037512823939323425\n",
      "Epoch 29283/30000 Training Loss: 0.04537099972367287\n",
      "Epoch 29284/30000 Training Loss: 0.03526979684829712\n",
      "Epoch 29285/30000 Training Loss: 0.03513740375638008\n",
      "Epoch 29286/30000 Training Loss: 0.04148469120264053\n",
      "Epoch 29287/30000 Training Loss: 0.0403052419424057\n",
      "Epoch 29288/30000 Training Loss: 0.0405338853597641\n",
      "Epoch 29289/30000 Training Loss: 0.04349268227815628\n",
      "Epoch 29290/30000 Training Loss: 0.04172486439347267\n",
      "Epoch 29291/30000 Training Loss: 0.049502547830343246\n",
      "Epoch 29292/30000 Training Loss: 0.04540505260229111\n",
      "Epoch 29293/30000 Training Loss: 0.04053794592618942\n",
      "Epoch 29294/30000 Training Loss: 0.056268930435180664\n",
      "Epoch 29295/30000 Training Loss: 0.05557708442211151\n",
      "Epoch 29296/30000 Training Loss: 0.041025690734386444\n",
      "Epoch 29297/30000 Training Loss: 0.05670526623725891\n",
      "Epoch 29298/30000 Training Loss: 0.053537335246801376\n",
      "Epoch 29299/30000 Training Loss: 0.047287411987781525\n",
      "Epoch 29300/30000 Training Loss: 0.042425885796546936\n",
      "Epoch 29300/30000 Validation Loss: 0.05369072034955025\n",
      "Epoch 29301/30000 Training Loss: 0.04442143440246582\n",
      "Epoch 29302/30000 Training Loss: 0.04674145206809044\n",
      "Epoch 29303/30000 Training Loss: 0.04336163029074669\n",
      "Epoch 29304/30000 Training Loss: 0.03949184715747833\n",
      "Epoch 29305/30000 Training Loss: 0.04239862039685249\n",
      "Epoch 29306/30000 Training Loss: 0.050689227879047394\n",
      "Epoch 29307/30000 Training Loss: 0.03819606080651283\n",
      "Epoch 29308/30000 Training Loss: 0.052080780267715454\n",
      "Epoch 29309/30000 Training Loss: 0.0542856864631176\n",
      "Epoch 29310/30000 Training Loss: 0.04580439627170563\n",
      "Epoch 29311/30000 Training Loss: 0.048671968281269073\n",
      "Epoch 29312/30000 Training Loss: 0.047780655324459076\n",
      "Epoch 29313/30000 Training Loss: 0.05108577758073807\n",
      "Epoch 29314/30000 Training Loss: 0.044520068913698196\n",
      "Epoch 29315/30000 Training Loss: 0.03586141765117645\n",
      "Epoch 29316/30000 Training Loss: 0.05283050239086151\n",
      "Epoch 29317/30000 Training Loss: 0.04792270064353943\n",
      "Epoch 29318/30000 Training Loss: 0.03981132060289383\n",
      "Epoch 29319/30000 Training Loss: 0.031692441552877426\n",
      "Epoch 29320/30000 Training Loss: 0.06338828057050705\n",
      "Epoch 29321/30000 Training Loss: 0.05990530923008919\n",
      "Epoch 29322/30000 Training Loss: 0.05302176997065544\n",
      "Epoch 29323/30000 Training Loss: 0.025654736906290054\n",
      "Epoch 29324/30000 Training Loss: 0.06076609343290329\n",
      "Epoch 29325/30000 Training Loss: 0.04080849885940552\n",
      "Epoch 29326/30000 Training Loss: 0.044708795845508575\n",
      "Epoch 29327/30000 Training Loss: 0.04794323444366455\n",
      "Epoch 29328/30000 Training Loss: 0.06850821524858475\n",
      "Epoch 29329/30000 Training Loss: 0.046936843544244766\n",
      "Epoch 29330/30000 Training Loss: 0.04196405038237572\n",
      "Epoch 29331/30000 Training Loss: 0.05297262594103813\n",
      "Epoch 29332/30000 Training Loss: 0.04597926139831543\n",
      "Epoch 29333/30000 Training Loss: 0.05560838431119919\n",
      "Epoch 29334/30000 Training Loss: 0.04902823641896248\n",
      "Epoch 29335/30000 Training Loss: 0.0619107261300087\n",
      "Epoch 29336/30000 Training Loss: 0.04129408672451973\n",
      "Epoch 29337/30000 Training Loss: 0.0436326265335083\n",
      "Epoch 29338/30000 Training Loss: 0.04863228648900986\n",
      "Epoch 29339/30000 Training Loss: 0.028640786185860634\n",
      "Epoch 29340/30000 Training Loss: 0.04833684116601944\n",
      "Epoch 29341/30000 Training Loss: 0.054791297763586044\n",
      "Epoch 29342/30000 Training Loss: 0.039697714149951935\n",
      "Epoch 29343/30000 Training Loss: 0.04204210266470909\n",
      "Epoch 29344/30000 Training Loss: 0.0554732121527195\n",
      "Epoch 29345/30000 Training Loss: 0.05496026948094368\n",
      "Epoch 29346/30000 Training Loss: 0.06026875227689743\n",
      "Epoch 29347/30000 Training Loss: 0.03521452099084854\n",
      "Epoch 29348/30000 Training Loss: 0.051215432584285736\n",
      "Epoch 29349/30000 Training Loss: 0.05305689573287964\n",
      "Epoch 29350/30000 Training Loss: 0.036473195999860764\n",
      "Epoch 29351/30000 Training Loss: 0.04376867040991783\n",
      "Epoch 29352/30000 Training Loss: 0.055523864924907684\n",
      "Epoch 29353/30000 Training Loss: 0.04553576558828354\n",
      "Epoch 29354/30000 Training Loss: 0.0404968187212944\n",
      "Epoch 29355/30000 Training Loss: 0.05441974103450775\n",
      "Epoch 29356/30000 Training Loss: 0.05446907877922058\n",
      "Epoch 29357/30000 Training Loss: 0.04457080736756325\n",
      "Epoch 29358/30000 Training Loss: 0.037594985216856\n",
      "Epoch 29359/30000 Training Loss: 0.03483955189585686\n",
      "Epoch 29360/30000 Training Loss: 0.05707250535488129\n",
      "Epoch 29361/30000 Training Loss: 0.03625544533133507\n",
      "Epoch 29362/30000 Training Loss: 0.04649607837200165\n",
      "Epoch 29363/30000 Training Loss: 0.04634711891412735\n",
      "Epoch 29364/30000 Training Loss: 0.05442720651626587\n",
      "Epoch 29365/30000 Training Loss: 0.05555238947272301\n",
      "Epoch 29366/30000 Training Loss: 0.05593842640519142\n",
      "Epoch 29367/30000 Training Loss: 0.04146416112780571\n",
      "Epoch 29368/30000 Training Loss: 0.043281134217977524\n",
      "Epoch 29369/30000 Training Loss: 0.0735207051038742\n",
      "Epoch 29370/30000 Training Loss: 0.03091692179441452\n",
      "Epoch 29371/30000 Training Loss: 0.03850485384464264\n",
      "Epoch 29372/30000 Training Loss: 0.04851873964071274\n",
      "Epoch 29373/30000 Training Loss: 0.04594412446022034\n",
      "Epoch 29374/30000 Training Loss: 0.051452163606882095\n",
      "Epoch 29375/30000 Training Loss: 0.0406259149312973\n",
      "Epoch 29376/30000 Training Loss: 0.05808131396770477\n",
      "Epoch 29377/30000 Training Loss: 0.04300156235694885\n",
      "Epoch 29378/30000 Training Loss: 0.03877551853656769\n",
      "Epoch 29379/30000 Training Loss: 0.04543306306004524\n",
      "Epoch 29380/30000 Training Loss: 0.059956397861242294\n",
      "Epoch 29381/30000 Training Loss: 0.03539830818772316\n",
      "Epoch 29382/30000 Training Loss: 0.032781802117824554\n",
      "Epoch 29383/30000 Training Loss: 0.04484293609857559\n",
      "Epoch 29384/30000 Training Loss: 0.04797779768705368\n",
      "Epoch 29385/30000 Training Loss: 0.04645612835884094\n",
      "Epoch 29386/30000 Training Loss: 0.049153201282024384\n",
      "Epoch 29387/30000 Training Loss: 0.039411820471286774\n",
      "Epoch 29388/30000 Training Loss: 0.04690081998705864\n",
      "Epoch 29389/30000 Training Loss: 0.06914105266332626\n",
      "Epoch 29390/30000 Training Loss: 0.042226970195770264\n",
      "Epoch 29391/30000 Training Loss: 0.04699668288230896\n",
      "Epoch 29392/30000 Training Loss: 0.05140772461891174\n",
      "Epoch 29393/30000 Training Loss: 0.04089982807636261\n",
      "Epoch 29394/30000 Training Loss: 0.05106944590806961\n",
      "Epoch 29395/30000 Training Loss: 0.03234856203198433\n",
      "Epoch 29396/30000 Training Loss: 0.0711585283279419\n",
      "Epoch 29397/30000 Training Loss: 0.03410079702734947\n",
      "Epoch 29398/30000 Training Loss: 0.04602310061454773\n",
      "Epoch 29399/30000 Training Loss: 0.035065870732069016\n",
      "Epoch 29400/30000 Training Loss: 0.053995437920093536\n",
      "Epoch 29400/30000 Validation Loss: 0.044462043792009354\n",
      "Epoch 29401/30000 Training Loss: 0.05520298331975937\n",
      "Epoch 29402/30000 Training Loss: 0.046108826994895935\n",
      "Epoch 29403/30000 Training Loss: 0.04230832681059837\n",
      "Epoch 29404/30000 Training Loss: 0.050621651113033295\n",
      "Epoch 29405/30000 Training Loss: 0.036066263914108276\n",
      "Epoch 29406/30000 Training Loss: 0.04649553447961807\n",
      "Epoch 29407/30000 Training Loss: 0.03833789750933647\n",
      "Epoch 29408/30000 Training Loss: 0.04923277348279953\n",
      "Epoch 29409/30000 Training Loss: 0.06937907636165619\n",
      "Epoch 29410/30000 Training Loss: 0.05019276589155197\n",
      "Epoch 29411/30000 Training Loss: 0.042235612869262695\n",
      "Epoch 29412/30000 Training Loss: 0.044468529522418976\n",
      "Epoch 29413/30000 Training Loss: 0.059499431401491165\n",
      "Epoch 29414/30000 Training Loss: 0.04499268904328346\n",
      "Epoch 29415/30000 Training Loss: 0.04931264370679855\n",
      "Epoch 29416/30000 Training Loss: 0.06040184944868088\n",
      "Epoch 29417/30000 Training Loss: 0.05419817939400673\n",
      "Epoch 29418/30000 Training Loss: 0.0366627462208271\n",
      "Epoch 29419/30000 Training Loss: 0.04291699454188347\n",
      "Epoch 29420/30000 Training Loss: 0.0449574775993824\n",
      "Epoch 29421/30000 Training Loss: 0.0504332035779953\n",
      "Epoch 29422/30000 Training Loss: 0.043102867901325226\n",
      "Epoch 29423/30000 Training Loss: 0.03571683168411255\n",
      "Epoch 29424/30000 Training Loss: 0.05140100046992302\n",
      "Epoch 29425/30000 Training Loss: 0.03581750765442848\n",
      "Epoch 29426/30000 Training Loss: 0.06214170157909393\n",
      "Epoch 29427/30000 Training Loss: 0.04869561642408371\n",
      "Epoch 29428/30000 Training Loss: 0.05535705387592316\n",
      "Epoch 29429/30000 Training Loss: 0.04835881292819977\n",
      "Epoch 29430/30000 Training Loss: 0.03554246947169304\n",
      "Epoch 29431/30000 Training Loss: 0.05591090768575668\n",
      "Epoch 29432/30000 Training Loss: 0.03909546509385109\n",
      "Epoch 29433/30000 Training Loss: 0.04519559070467949\n",
      "Epoch 29434/30000 Training Loss: 0.05749219283461571\n",
      "Epoch 29435/30000 Training Loss: 0.041498612612485886\n",
      "Epoch 29436/30000 Training Loss: 0.041125405579805374\n",
      "Epoch 29437/30000 Training Loss: 0.052660021930933\n",
      "Epoch 29438/30000 Training Loss: 0.05224185064435005\n",
      "Epoch 29439/30000 Training Loss: 0.05505151301622391\n",
      "Epoch 29440/30000 Training Loss: 0.03443153575062752\n",
      "Epoch 29441/30000 Training Loss: 0.050302326679229736\n",
      "Epoch 29442/30000 Training Loss: 0.050030820071697235\n",
      "Epoch 29443/30000 Training Loss: 0.04231271147727966\n",
      "Epoch 29444/30000 Training Loss: 0.052821725606918335\n",
      "Epoch 29445/30000 Training Loss: 0.051395170390605927\n",
      "Epoch 29446/30000 Training Loss: 0.034926410764455795\n",
      "Epoch 29447/30000 Training Loss: 0.04723944142460823\n",
      "Epoch 29448/30000 Training Loss: 0.04709509015083313\n",
      "Epoch 29449/30000 Training Loss: 0.06591778248548508\n",
      "Epoch 29450/30000 Training Loss: 0.039604172110557556\n",
      "Epoch 29451/30000 Training Loss: 0.03528096154332161\n",
      "Epoch 29452/30000 Training Loss: 0.0504126101732254\n",
      "Epoch 29453/30000 Training Loss: 0.05584520846605301\n",
      "Epoch 29454/30000 Training Loss: 0.037092410027980804\n",
      "Epoch 29455/30000 Training Loss: 0.061144620180130005\n",
      "Epoch 29456/30000 Training Loss: 0.0458923876285553\n",
      "Epoch 29457/30000 Training Loss: 0.039318572729825974\n",
      "Epoch 29458/30000 Training Loss: 0.04974065721035004\n",
      "Epoch 29459/30000 Training Loss: 0.05913892388343811\n",
      "Epoch 29460/30000 Training Loss: 0.047106750309467316\n",
      "Epoch 29461/30000 Training Loss: 0.02744285762310028\n",
      "Epoch 29462/30000 Training Loss: 0.03785152733325958\n",
      "Epoch 29463/30000 Training Loss: 0.06168079003691673\n",
      "Epoch 29464/30000 Training Loss: 0.044693779200315475\n",
      "Epoch 29465/30000 Training Loss: 0.04886513203382492\n",
      "Epoch 29466/30000 Training Loss: 0.035793520510196686\n",
      "Epoch 29467/30000 Training Loss: 0.04456837847828865\n",
      "Epoch 29468/30000 Training Loss: 0.03983563184738159\n",
      "Epoch 29469/30000 Training Loss: 0.03603363037109375\n",
      "Epoch 29470/30000 Training Loss: 0.05314435064792633\n",
      "Epoch 29471/30000 Training Loss: 0.05190274491906166\n",
      "Epoch 29472/30000 Training Loss: 0.04895773530006409\n",
      "Epoch 29473/30000 Training Loss: 0.041038885712623596\n",
      "Epoch 29474/30000 Training Loss: 0.04419632628560066\n",
      "Epoch 29475/30000 Training Loss: 0.038961201906204224\n",
      "Epoch 29476/30000 Training Loss: 0.04924231022596359\n",
      "Epoch 29477/30000 Training Loss: 0.05103205144405365\n",
      "Epoch 29478/30000 Training Loss: 0.03880459815263748\n",
      "Epoch 29479/30000 Training Loss: 0.05994877219200134\n",
      "Epoch 29480/30000 Training Loss: 0.041813865303993225\n",
      "Epoch 29481/30000 Training Loss: 0.05449444055557251\n",
      "Epoch 29482/30000 Training Loss: 0.045057930052280426\n",
      "Epoch 29483/30000 Training Loss: 0.03389062359929085\n",
      "Epoch 29484/30000 Training Loss: 0.03373494744300842\n",
      "Epoch 29485/30000 Training Loss: 0.034219712018966675\n",
      "Epoch 29486/30000 Training Loss: 0.03831002116203308\n",
      "Epoch 29487/30000 Training Loss: 0.054102495312690735\n",
      "Epoch 29488/30000 Training Loss: 0.05047029256820679\n",
      "Epoch 29489/30000 Training Loss: 0.03452996537089348\n",
      "Epoch 29490/30000 Training Loss: 0.04682139307260513\n",
      "Epoch 29491/30000 Training Loss: 0.04200541228055954\n",
      "Epoch 29492/30000 Training Loss: 0.05552143231034279\n",
      "Epoch 29493/30000 Training Loss: 0.04383406788110733\n",
      "Epoch 29494/30000 Training Loss: 0.05092278867959976\n",
      "Epoch 29495/30000 Training Loss: 0.06020306795835495\n",
      "Epoch 29496/30000 Training Loss: 0.04346572607755661\n",
      "Epoch 29497/30000 Training Loss: 0.037977613508701324\n",
      "Epoch 29498/30000 Training Loss: 0.04273303225636482\n",
      "Epoch 29499/30000 Training Loss: 0.05918958783149719\n",
      "Epoch 29500/30000 Training Loss: 0.053664304316043854\n",
      "Epoch 29500/30000 Validation Loss: 0.06502053886651993\n",
      "Epoch 29501/30000 Training Loss: 0.04406547546386719\n",
      "Epoch 29502/30000 Training Loss: 0.05658192187547684\n",
      "Epoch 29503/30000 Training Loss: 0.02887209877371788\n",
      "Epoch 29504/30000 Training Loss: 0.04494430124759674\n",
      "Epoch 29505/30000 Training Loss: 0.0396779403090477\n",
      "Epoch 29506/30000 Training Loss: 0.03371520712971687\n",
      "Epoch 29507/30000 Training Loss: 0.04311385750770569\n",
      "Epoch 29508/30000 Training Loss: 0.04081019386649132\n",
      "Epoch 29509/30000 Training Loss: 0.05381043627858162\n",
      "Epoch 29510/30000 Training Loss: 0.054684363305568695\n",
      "Epoch 29511/30000 Training Loss: 0.043956458568573\n",
      "Epoch 29512/30000 Training Loss: 0.041337672621011734\n",
      "Epoch 29513/30000 Training Loss: 0.044008977711200714\n",
      "Epoch 29514/30000 Training Loss: 0.03735541179776192\n",
      "Epoch 29515/30000 Training Loss: 0.049690160900354385\n",
      "Epoch 29516/30000 Training Loss: 0.0471523180603981\n",
      "Epoch 29517/30000 Training Loss: 0.03973321616649628\n",
      "Epoch 29518/30000 Training Loss: 0.04470674321055412\n",
      "Epoch 29519/30000 Training Loss: 0.0687691867351532\n",
      "Epoch 29520/30000 Training Loss: 0.034476883709430695\n",
      "Epoch 29521/30000 Training Loss: 0.04036980867385864\n",
      "Epoch 29522/30000 Training Loss: 0.043356362730264664\n",
      "Epoch 29523/30000 Training Loss: 0.039099738001823425\n",
      "Epoch 29524/30000 Training Loss: 0.04679669439792633\n",
      "Epoch 29525/30000 Training Loss: 0.03740033879876137\n",
      "Epoch 29526/30000 Training Loss: 0.0482148751616478\n",
      "Epoch 29527/30000 Training Loss: 0.04186228662729263\n",
      "Epoch 29528/30000 Training Loss: 0.03829336538910866\n",
      "Epoch 29529/30000 Training Loss: 0.0354851633310318\n",
      "Epoch 29530/30000 Training Loss: 0.05096237733960152\n",
      "Epoch 29531/30000 Training Loss: 0.0448078028857708\n",
      "Epoch 29532/30000 Training Loss: 0.04074256122112274\n",
      "Epoch 29533/30000 Training Loss: 0.042960308492183685\n",
      "Epoch 29534/30000 Training Loss: 0.049358926713466644\n",
      "Epoch 29535/30000 Training Loss: 0.043958667665719986\n",
      "Epoch 29536/30000 Training Loss: 0.035364389419555664\n",
      "Epoch 29537/30000 Training Loss: 0.03973434492945671\n",
      "Epoch 29538/30000 Training Loss: 0.044530659914016724\n",
      "Epoch 29539/30000 Training Loss: 0.034058474004268646\n",
      "Epoch 29540/30000 Training Loss: 0.046559594571590424\n",
      "Epoch 29541/30000 Training Loss: 0.04549854248762131\n",
      "Epoch 29542/30000 Training Loss: 0.04688449203968048\n",
      "Epoch 29543/30000 Training Loss: 0.05810975283384323\n",
      "Epoch 29544/30000 Training Loss: 0.05483821779489517\n",
      "Epoch 29545/30000 Training Loss: 0.03930055350065231\n",
      "Epoch 29546/30000 Training Loss: 0.03462174907326698\n",
      "Epoch 29547/30000 Training Loss: 0.055980682373046875\n",
      "Epoch 29548/30000 Training Loss: 0.03341654688119888\n",
      "Epoch 29549/30000 Training Loss: 0.059091486036777496\n",
      "Epoch 29550/30000 Training Loss: 0.041047602891922\n",
      "Epoch 29551/30000 Training Loss: 0.035350359976291656\n",
      "Epoch 29552/30000 Training Loss: 0.02819722518324852\n",
      "Epoch 29553/30000 Training Loss: 0.04101726412773132\n",
      "Epoch 29554/30000 Training Loss: 0.04446485638618469\n",
      "Epoch 29555/30000 Training Loss: 0.03853332996368408\n",
      "Epoch 29556/30000 Training Loss: 0.0403745099902153\n",
      "Epoch 29557/30000 Training Loss: 0.04169302433729172\n",
      "Epoch 29558/30000 Training Loss: 0.03792714700102806\n",
      "Epoch 29559/30000 Training Loss: 0.02768654190003872\n",
      "Epoch 29560/30000 Training Loss: 0.04384414851665497\n",
      "Epoch 29561/30000 Training Loss: 0.038484156131744385\n",
      "Epoch 29562/30000 Training Loss: 0.03654365986585617\n",
      "Epoch 29563/30000 Training Loss: 0.042026035487651825\n",
      "Epoch 29564/30000 Training Loss: 0.0431637279689312\n",
      "Epoch 29565/30000 Training Loss: 0.04896822199225426\n",
      "Epoch 29566/30000 Training Loss: 0.06013406068086624\n",
      "Epoch 29567/30000 Training Loss: 0.03833307698369026\n",
      "Epoch 29568/30000 Training Loss: 0.04852743819355965\n",
      "Epoch 29569/30000 Training Loss: 0.05592021718621254\n",
      "Epoch 29570/30000 Training Loss: 0.043105367571115494\n",
      "Epoch 29571/30000 Training Loss: 0.04326494783163071\n",
      "Epoch 29572/30000 Training Loss: 0.043799903243780136\n",
      "Epoch 29573/30000 Training Loss: 0.04430190101265907\n",
      "Epoch 29574/30000 Training Loss: 0.04937528073787689\n",
      "Epoch 29575/30000 Training Loss: 0.046052418649196625\n",
      "Epoch 29576/30000 Training Loss: 0.04232128709554672\n",
      "Epoch 29577/30000 Training Loss: 0.044653505086898804\n",
      "Epoch 29578/30000 Training Loss: 0.03940347209572792\n",
      "Epoch 29579/30000 Training Loss: 0.03553835675120354\n",
      "Epoch 29580/30000 Training Loss: 0.05406204238533974\n",
      "Epoch 29581/30000 Training Loss: 0.03998963162302971\n",
      "Epoch 29582/30000 Training Loss: 0.0303497314453125\n",
      "Epoch 29583/30000 Training Loss: 0.05655163899064064\n",
      "Epoch 29584/30000 Training Loss: 0.06222410127520561\n",
      "Epoch 29585/30000 Training Loss: 0.04841657727956772\n",
      "Epoch 29586/30000 Training Loss: 0.038168568164110184\n",
      "Epoch 29587/30000 Training Loss: 0.0307324081659317\n",
      "Epoch 29588/30000 Training Loss: 0.04195942357182503\n",
      "Epoch 29589/30000 Training Loss: 0.06402702629566193\n",
      "Epoch 29590/30000 Training Loss: 0.05397671461105347\n",
      "Epoch 29591/30000 Training Loss: 0.045415978878736496\n",
      "Epoch 29592/30000 Training Loss: 0.04781459644436836\n",
      "Epoch 29593/30000 Training Loss: 0.04022876173257828\n",
      "Epoch 29594/30000 Training Loss: 0.031944748014211655\n",
      "Epoch 29595/30000 Training Loss: 0.04221085458993912\n",
      "Epoch 29596/30000 Training Loss: 0.030043724924325943\n",
      "Epoch 29597/30000 Training Loss: 0.03747197985649109\n",
      "Epoch 29598/30000 Training Loss: 0.04536097124218941\n",
      "Epoch 29599/30000 Training Loss: 0.033405106514692307\n",
      "Epoch 29600/30000 Training Loss: 0.0454738587141037\n",
      "Epoch 29600/30000 Validation Loss: 0.05008916184306145\n",
      "Epoch 29601/30000 Training Loss: 0.05398818105459213\n",
      "Epoch 29602/30000 Training Loss: 0.04084169492125511\n",
      "Epoch 29603/30000 Training Loss: 0.04685430973768234\n",
      "Epoch 29604/30000 Training Loss: 0.061701368540525436\n",
      "Epoch 29605/30000 Training Loss: 0.05323781073093414\n",
      "Epoch 29606/30000 Training Loss: 0.03725574165582657\n",
      "Epoch 29607/30000 Training Loss: 0.0469483882188797\n",
      "Epoch 29608/30000 Training Loss: 0.05700981616973877\n",
      "Epoch 29609/30000 Training Loss: 0.0502224937081337\n",
      "Epoch 29610/30000 Training Loss: 0.050874024629592896\n",
      "Epoch 29611/30000 Training Loss: 0.03128030523657799\n",
      "Epoch 29612/30000 Training Loss: 0.039393551647663116\n",
      "Epoch 29613/30000 Training Loss: 0.05579148232936859\n",
      "Epoch 29614/30000 Training Loss: 0.03746260330080986\n",
      "Epoch 29615/30000 Training Loss: 0.05840718001127243\n",
      "Epoch 29616/30000 Training Loss: 0.05590952932834625\n",
      "Epoch 29617/30000 Training Loss: 0.05174826458096504\n",
      "Epoch 29618/30000 Training Loss: 0.046951912343502045\n",
      "Epoch 29619/30000 Training Loss: 0.044946037232875824\n",
      "Epoch 29620/30000 Training Loss: 0.02995619922876358\n",
      "Epoch 29621/30000 Training Loss: 0.03884345665574074\n",
      "Epoch 29622/30000 Training Loss: 0.03443807363510132\n",
      "Epoch 29623/30000 Training Loss: 0.042285725474357605\n",
      "Epoch 29624/30000 Training Loss: 0.05984257906675339\n",
      "Epoch 29625/30000 Training Loss: 0.033549755811691284\n",
      "Epoch 29626/30000 Training Loss: 0.0438937209546566\n",
      "Epoch 29627/30000 Training Loss: 0.04832351207733154\n",
      "Epoch 29628/30000 Training Loss: 0.047362737357616425\n",
      "Epoch 29629/30000 Training Loss: 0.04752197861671448\n",
      "Epoch 29630/30000 Training Loss: 0.04713235795497894\n",
      "Epoch 29631/30000 Training Loss: 0.03723113238811493\n",
      "Epoch 29632/30000 Training Loss: 0.04945109412074089\n",
      "Epoch 29633/30000 Training Loss: 0.04115920886397362\n",
      "Epoch 29634/30000 Training Loss: 0.039861831814050674\n",
      "Epoch 29635/30000 Training Loss: 0.04604960232973099\n",
      "Epoch 29636/30000 Training Loss: 0.04993735998868942\n",
      "Epoch 29637/30000 Training Loss: 0.04853329062461853\n",
      "Epoch 29638/30000 Training Loss: 0.05172388255596161\n",
      "Epoch 29639/30000 Training Loss: 0.0485454723238945\n",
      "Epoch 29640/30000 Training Loss: 0.03646501153707504\n",
      "Epoch 29641/30000 Training Loss: 0.0566030777990818\n",
      "Epoch 29642/30000 Training Loss: 0.03721889108419418\n",
      "Epoch 29643/30000 Training Loss: 0.05535268783569336\n",
      "Epoch 29644/30000 Training Loss: 0.04659999534487724\n",
      "Epoch 29645/30000 Training Loss: 0.03427650034427643\n",
      "Epoch 29646/30000 Training Loss: 0.04241740703582764\n",
      "Epoch 29647/30000 Training Loss: 0.038446635007858276\n",
      "Epoch 29648/30000 Training Loss: 0.053437307476997375\n",
      "Epoch 29649/30000 Training Loss: 0.031118284910917282\n",
      "Epoch 29650/30000 Training Loss: 0.04785372316837311\n",
      "Epoch 29651/30000 Training Loss: 0.059233009815216064\n",
      "Epoch 29652/30000 Training Loss: 0.05253385007381439\n",
      "Epoch 29653/30000 Training Loss: 0.056462131440639496\n",
      "Epoch 29654/30000 Training Loss: 0.05431513488292694\n",
      "Epoch 29655/30000 Training Loss: 0.03774026036262512\n",
      "Epoch 29656/30000 Training Loss: 0.05476723611354828\n",
      "Epoch 29657/30000 Training Loss: 0.04479123651981354\n",
      "Epoch 29658/30000 Training Loss: 0.04075793921947479\n",
      "Epoch 29659/30000 Training Loss: 0.04384857043623924\n",
      "Epoch 29660/30000 Training Loss: 0.03614632785320282\n",
      "Epoch 29661/30000 Training Loss: 0.03925111144781113\n",
      "Epoch 29662/30000 Training Loss: 0.04238984361290932\n",
      "Epoch 29663/30000 Training Loss: 0.04148111864924431\n",
      "Epoch 29664/30000 Training Loss: 0.042876921594142914\n",
      "Epoch 29665/30000 Training Loss: 0.04376333951950073\n",
      "Epoch 29666/30000 Training Loss: 0.042962536215782166\n",
      "Epoch 29667/30000 Training Loss: 0.038971804082393646\n",
      "Epoch 29668/30000 Training Loss: 0.05831807479262352\n",
      "Epoch 29669/30000 Training Loss: 0.04499898478388786\n",
      "Epoch 29670/30000 Training Loss: 0.051243431866168976\n",
      "Epoch 29671/30000 Training Loss: 0.04378123953938484\n",
      "Epoch 29672/30000 Training Loss: 0.04673559591174126\n",
      "Epoch 29673/30000 Training Loss: 0.052171867340803146\n",
      "Epoch 29674/30000 Training Loss: 0.06488107144832611\n",
      "Epoch 29675/30000 Training Loss: 0.037277232855558395\n",
      "Epoch 29676/30000 Training Loss: 0.038994185626506805\n",
      "Epoch 29677/30000 Training Loss: 0.03503870964050293\n",
      "Epoch 29678/30000 Training Loss: 0.04015527293086052\n",
      "Epoch 29679/30000 Training Loss: 0.045326076447963715\n",
      "Epoch 29680/30000 Training Loss: 0.06779114156961441\n",
      "Epoch 29681/30000 Training Loss: 0.054629307240247726\n",
      "Epoch 29682/30000 Training Loss: 0.03852785378694534\n",
      "Epoch 29683/30000 Training Loss: 0.050246309489011765\n",
      "Epoch 29684/30000 Training Loss: 0.039060112088918686\n",
      "Epoch 29685/30000 Training Loss: 0.04535625874996185\n",
      "Epoch 29686/30000 Training Loss: 0.04835609346628189\n",
      "Epoch 29687/30000 Training Loss: 0.042134158313274384\n",
      "Epoch 29688/30000 Training Loss: 0.042491793632507324\n",
      "Epoch 29689/30000 Training Loss: 0.03256013244390488\n",
      "Epoch 29690/30000 Training Loss: 0.05182506889104843\n",
      "Epoch 29691/30000 Training Loss: 0.037421658635139465\n",
      "Epoch 29692/30000 Training Loss: 0.03504803404211998\n",
      "Epoch 29693/30000 Training Loss: 0.04335293918848038\n",
      "Epoch 29694/30000 Training Loss: 0.055041804909706116\n",
      "Epoch 29695/30000 Training Loss: 0.047683313488960266\n",
      "Epoch 29696/30000 Training Loss: 0.04364262521266937\n",
      "Epoch 29697/30000 Training Loss: 0.04326561465859413\n",
      "Epoch 29698/30000 Training Loss: 0.05398068204522133\n",
      "Epoch 29699/30000 Training Loss: 0.042319294065237045\n",
      "Epoch 29700/30000 Training Loss: 0.049075961112976074\n",
      "Epoch 29700/30000 Validation Loss: 0.04358512535691261\n",
      "Epoch 29701/30000 Training Loss: 0.048269499093294144\n",
      "Epoch 29702/30000 Training Loss: 0.04950126260519028\n",
      "Epoch 29703/30000 Training Loss: 0.05031304061412811\n",
      "Epoch 29704/30000 Training Loss: 0.039273325353860855\n",
      "Epoch 29705/30000 Training Loss: 0.06597823649644852\n",
      "Epoch 29706/30000 Training Loss: 0.04294528812170029\n",
      "Epoch 29707/30000 Training Loss: 0.035356372594833374\n",
      "Epoch 29708/30000 Training Loss: 0.033727776259183884\n",
      "Epoch 29709/30000 Training Loss: 0.05023374408483505\n",
      "Epoch 29710/30000 Training Loss: 0.03548973798751831\n",
      "Epoch 29711/30000 Training Loss: 0.04016182944178581\n",
      "Epoch 29712/30000 Training Loss: 0.041232917457818985\n",
      "Epoch 29713/30000 Training Loss: 0.034381963312625885\n",
      "Epoch 29714/30000 Training Loss: 0.04068814218044281\n",
      "Epoch 29715/30000 Training Loss: 0.04620938375592232\n",
      "Epoch 29716/30000 Training Loss: 0.04663814231753349\n",
      "Epoch 29717/30000 Training Loss: 0.0504436120390892\n",
      "Epoch 29718/30000 Training Loss: 0.04815775901079178\n",
      "Epoch 29719/30000 Training Loss: 0.037590593099594116\n",
      "Epoch 29720/30000 Training Loss: 0.04623202607035637\n",
      "Epoch 29721/30000 Training Loss: 0.06761805713176727\n",
      "Epoch 29722/30000 Training Loss: 0.04174790903925896\n",
      "Epoch 29723/30000 Training Loss: 0.0371934249997139\n",
      "Epoch 29724/30000 Training Loss: 0.04016140475869179\n",
      "Epoch 29725/30000 Training Loss: 0.028009939938783646\n",
      "Epoch 29726/30000 Training Loss: 0.051028985530138016\n",
      "Epoch 29727/30000 Training Loss: 0.03658752143383026\n",
      "Epoch 29728/30000 Training Loss: 0.04719356447458267\n",
      "Epoch 29729/30000 Training Loss: 0.053363531827926636\n",
      "Epoch 29730/30000 Training Loss: 0.0358223058283329\n",
      "Epoch 29731/30000 Training Loss: 0.04511960595846176\n",
      "Epoch 29732/30000 Training Loss: 0.06197599321603775\n",
      "Epoch 29733/30000 Training Loss: 0.0488724522292614\n",
      "Epoch 29734/30000 Training Loss: 0.042798109352588654\n",
      "Epoch 29735/30000 Training Loss: 0.044611938297748566\n",
      "Epoch 29736/30000 Training Loss: 0.04663257300853729\n",
      "Epoch 29737/30000 Training Loss: 0.050610125064849854\n",
      "Epoch 29738/30000 Training Loss: 0.050397999584674835\n",
      "Epoch 29739/30000 Training Loss: 0.04388411343097687\n",
      "Epoch 29740/30000 Training Loss: 0.04867696762084961\n",
      "Epoch 29741/30000 Training Loss: 0.05820533260703087\n",
      "Epoch 29742/30000 Training Loss: 0.04482048377394676\n",
      "Epoch 29743/30000 Training Loss: 0.04634755849838257\n",
      "Epoch 29744/30000 Training Loss: 0.05260949209332466\n",
      "Epoch 29745/30000 Training Loss: 0.036210671067237854\n",
      "Epoch 29746/30000 Training Loss: 0.04125720262527466\n",
      "Epoch 29747/30000 Training Loss: 0.040722914040088654\n",
      "Epoch 29748/30000 Training Loss: 0.03311733156442642\n",
      "Epoch 29749/30000 Training Loss: 0.03689390793442726\n",
      "Epoch 29750/30000 Training Loss: 0.047657907009124756\n",
      "Epoch 29751/30000 Training Loss: 0.04039519652724266\n",
      "Epoch 29752/30000 Training Loss: 0.04207396134734154\n",
      "Epoch 29753/30000 Training Loss: 0.050786882638931274\n",
      "Epoch 29754/30000 Training Loss: 0.053989142179489136\n",
      "Epoch 29755/30000 Training Loss: 0.05260385945439339\n",
      "Epoch 29756/30000 Training Loss: 0.04738467559218407\n",
      "Epoch 29757/30000 Training Loss: 0.03705929219722748\n",
      "Epoch 29758/30000 Training Loss: 0.04790930822491646\n",
      "Epoch 29759/30000 Training Loss: 0.04440760985016823\n",
      "Epoch 29760/30000 Training Loss: 0.039937425404787064\n",
      "Epoch 29761/30000 Training Loss: 0.039330288767814636\n",
      "Epoch 29762/30000 Training Loss: 0.04667221009731293\n",
      "Epoch 29763/30000 Training Loss: 0.03999796882271767\n",
      "Epoch 29764/30000 Training Loss: 0.05074549838900566\n",
      "Epoch 29765/30000 Training Loss: 0.047121718525886536\n",
      "Epoch 29766/30000 Training Loss: 0.051993198692798615\n",
      "Epoch 29767/30000 Training Loss: 0.04386666417121887\n",
      "Epoch 29768/30000 Training Loss: 0.04323665052652359\n",
      "Epoch 29769/30000 Training Loss: 0.04136378690600395\n",
      "Epoch 29770/30000 Training Loss: 0.04776861518621445\n",
      "Epoch 29771/30000 Training Loss: 0.04753677174448967\n",
      "Epoch 29772/30000 Training Loss: 0.031022673472762108\n",
      "Epoch 29773/30000 Training Loss: 0.04735119268298149\n",
      "Epoch 29774/30000 Training Loss: 0.05240871384739876\n",
      "Epoch 29775/30000 Training Loss: 0.055180612951517105\n",
      "Epoch 29776/30000 Training Loss: 0.06983649730682373\n",
      "Epoch 29777/30000 Training Loss: 0.0389072448015213\n",
      "Epoch 29778/30000 Training Loss: 0.055718787014484406\n",
      "Epoch 29779/30000 Training Loss: 0.05649283155798912\n",
      "Epoch 29780/30000 Training Loss: 0.04380813613533974\n",
      "Epoch 29781/30000 Training Loss: 0.055286020040512085\n",
      "Epoch 29782/30000 Training Loss: 0.0437757782638073\n",
      "Epoch 29783/30000 Training Loss: 0.04876948148012161\n",
      "Epoch 29784/30000 Training Loss: 0.05105426162481308\n",
      "Epoch 29785/30000 Training Loss: 0.04938741773366928\n",
      "Epoch 29786/30000 Training Loss: 0.04651357978582382\n",
      "Epoch 29787/30000 Training Loss: 0.05930015444755554\n",
      "Epoch 29788/30000 Training Loss: 0.03926752880215645\n",
      "Epoch 29789/30000 Training Loss: 0.04844193160533905\n",
      "Epoch 29790/30000 Training Loss: 0.04469158500432968\n",
      "Epoch 29791/30000 Training Loss: 0.049754999577999115\n",
      "Epoch 29792/30000 Training Loss: 0.04232136532664299\n",
      "Epoch 29793/30000 Training Loss: 0.045500580221414566\n",
      "Epoch 29794/30000 Training Loss: 0.05999253690242767\n",
      "Epoch 29795/30000 Training Loss: 0.04257558286190033\n",
      "Epoch 29796/30000 Training Loss: 0.050698451697826385\n",
      "Epoch 29797/30000 Training Loss: 0.056814540177583694\n",
      "Epoch 29798/30000 Training Loss: 0.044326167553663254\n",
      "Epoch 29799/30000 Training Loss: 0.039049841463565826\n",
      "Epoch 29800/30000 Training Loss: 0.04413828253746033\n",
      "Epoch 29800/30000 Validation Loss: 0.05509014055132866\n",
      "Epoch 29801/30000 Training Loss: 0.05303700268268585\n",
      "Epoch 29802/30000 Training Loss: 0.04255549982190132\n",
      "Epoch 29803/30000 Training Loss: 0.06224643811583519\n",
      "Epoch 29804/30000 Training Loss: 0.030904151499271393\n",
      "Epoch 29805/30000 Training Loss: 0.040955036878585815\n",
      "Epoch 29806/30000 Training Loss: 0.035583265125751495\n",
      "Epoch 29807/30000 Training Loss: 0.048381365835666656\n",
      "Epoch 29808/30000 Training Loss: 0.06043223291635513\n",
      "Epoch 29809/30000 Training Loss: 0.051035940647125244\n",
      "Epoch 29810/30000 Training Loss: 0.03305778279900551\n",
      "Epoch 29811/30000 Training Loss: 0.0397806391119957\n",
      "Epoch 29812/30000 Training Loss: 0.036022111773490906\n",
      "Epoch 29813/30000 Training Loss: 0.03272758796811104\n",
      "Epoch 29814/30000 Training Loss: 0.058262310922145844\n",
      "Epoch 29815/30000 Training Loss: 0.036993447691202164\n",
      "Epoch 29816/30000 Training Loss: 0.05684615671634674\n",
      "Epoch 29817/30000 Training Loss: 0.03850734233856201\n",
      "Epoch 29818/30000 Training Loss: 0.04566111043095589\n",
      "Epoch 29819/30000 Training Loss: 0.048008427023887634\n",
      "Epoch 29820/30000 Training Loss: 0.03184870630502701\n",
      "Epoch 29821/30000 Training Loss: 0.050779685378074646\n",
      "Epoch 29822/30000 Training Loss: 0.04430973529815674\n",
      "Epoch 29823/30000 Training Loss: 0.04361201822757721\n",
      "Epoch 29824/30000 Training Loss: 0.056632257997989655\n",
      "Epoch 29825/30000 Training Loss: 0.03812986984848976\n",
      "Epoch 29826/30000 Training Loss: 0.03853968158364296\n",
      "Epoch 29827/30000 Training Loss: 0.05860212445259094\n",
      "Epoch 29828/30000 Training Loss: 0.044009074568748474\n",
      "Epoch 29829/30000 Training Loss: 0.04694380238652229\n",
      "Epoch 29830/30000 Training Loss: 0.05375584214925766\n",
      "Epoch 29831/30000 Training Loss: 0.0448797307908535\n",
      "Epoch 29832/30000 Training Loss: 0.051411695778369904\n",
      "Epoch 29833/30000 Training Loss: 0.03669489920139313\n",
      "Epoch 29834/30000 Training Loss: 0.04529474303126335\n",
      "Epoch 29835/30000 Training Loss: 0.04325447976589203\n",
      "Epoch 29836/30000 Training Loss: 0.04852106794714928\n",
      "Epoch 29837/30000 Training Loss: 0.05081174522638321\n",
      "Epoch 29838/30000 Training Loss: 0.04380763694643974\n",
      "Epoch 29839/30000 Training Loss: 0.04172494634985924\n",
      "Epoch 29840/30000 Training Loss: 0.03656761348247528\n",
      "Epoch 29841/30000 Training Loss: 0.04636245593428612\n",
      "Epoch 29842/30000 Training Loss: 0.04160919785499573\n",
      "Epoch 29843/30000 Training Loss: 0.05010831356048584\n",
      "Epoch 29844/30000 Training Loss: 0.03570142388343811\n",
      "Epoch 29845/30000 Training Loss: 0.054471638053655624\n",
      "Epoch 29846/30000 Training Loss: 0.03568152338266373\n",
      "Epoch 29847/30000 Training Loss: 0.05482975393533707\n",
      "Epoch 29848/30000 Training Loss: 0.04647567868232727\n",
      "Epoch 29849/30000 Training Loss: 0.04424575716257095\n",
      "Epoch 29850/30000 Training Loss: 0.0515141524374485\n",
      "Epoch 29851/30000 Training Loss: 0.03311740979552269\n",
      "Epoch 29852/30000 Training Loss: 0.042573608458042145\n",
      "Epoch 29853/30000 Training Loss: 0.052481479942798615\n",
      "Epoch 29854/30000 Training Loss: 0.060278087854385376\n",
      "Epoch 29855/30000 Training Loss: 0.04919110983610153\n",
      "Epoch 29856/30000 Training Loss: 0.05126117169857025\n",
      "Epoch 29857/30000 Training Loss: 0.04233170673251152\n",
      "Epoch 29858/30000 Training Loss: 0.05505470931529999\n",
      "Epoch 29859/30000 Training Loss: 0.0663563460111618\n",
      "Epoch 29860/30000 Training Loss: 0.04333123937249184\n",
      "Epoch 29861/30000 Training Loss: 0.04249082878232002\n",
      "Epoch 29862/30000 Training Loss: 0.03364858031272888\n",
      "Epoch 29863/30000 Training Loss: 0.05563948303461075\n",
      "Epoch 29864/30000 Training Loss: 0.044386789202690125\n",
      "Epoch 29865/30000 Training Loss: 0.032580260187387466\n",
      "Epoch 29866/30000 Training Loss: 0.05223987251520157\n",
      "Epoch 29867/30000 Training Loss: 0.0558764785528183\n",
      "Epoch 29868/30000 Training Loss: 0.04955875873565674\n",
      "Epoch 29869/30000 Training Loss: 0.052755143493413925\n",
      "Epoch 29870/30000 Training Loss: 0.0584082305431366\n",
      "Epoch 29871/30000 Training Loss: 0.03373896703124046\n",
      "Epoch 29872/30000 Training Loss: 0.047788847237825394\n",
      "Epoch 29873/30000 Training Loss: 0.0552615225315094\n",
      "Epoch 29874/30000 Training Loss: 0.04723232239484787\n",
      "Epoch 29875/30000 Training Loss: 0.04199432581663132\n",
      "Epoch 29876/30000 Training Loss: 0.05434033274650574\n",
      "Epoch 29877/30000 Training Loss: 0.04854205995798111\n",
      "Epoch 29878/30000 Training Loss: 0.04233921319246292\n",
      "Epoch 29879/30000 Training Loss: 0.049396812915802\n",
      "Epoch 29880/30000 Training Loss: 0.032586611807346344\n",
      "Epoch 29881/30000 Training Loss: 0.053142812103033066\n",
      "Epoch 29882/30000 Training Loss: 0.05022422596812248\n",
      "Epoch 29883/30000 Training Loss: 0.05489708110690117\n",
      "Epoch 29884/30000 Training Loss: 0.039095841348171234\n",
      "Epoch 29885/30000 Training Loss: 0.050204936414957047\n",
      "Epoch 29886/30000 Training Loss: 0.04991341009736061\n",
      "Epoch 29887/30000 Training Loss: 0.03828703239560127\n",
      "Epoch 29888/30000 Training Loss: 0.05309480056166649\n",
      "Epoch 29889/30000 Training Loss: 0.03948529064655304\n",
      "Epoch 29890/30000 Training Loss: 0.04919276386499405\n",
      "Epoch 29891/30000 Training Loss: 0.024658197537064552\n",
      "Epoch 29892/30000 Training Loss: 0.04498641565442085\n",
      "Epoch 29893/30000 Training Loss: 0.05099889636039734\n",
      "Epoch 29894/30000 Training Loss: 0.03564146161079407\n",
      "Epoch 29895/30000 Training Loss: 0.044285956770181656\n",
      "Epoch 29896/30000 Training Loss: 0.05258631706237793\n",
      "Epoch 29897/30000 Training Loss: 0.04066619649529457\n",
      "Epoch 29898/30000 Training Loss: 0.04125349223613739\n",
      "Epoch 29899/30000 Training Loss: 0.04610919579863548\n",
      "Epoch 29900/30000 Training Loss: 0.051240600645542145\n",
      "Epoch 29900/30000 Validation Loss: 0.03640550747513771\n",
      "Epoch 29901/30000 Training Loss: 0.04048086702823639\n",
      "Epoch 29902/30000 Training Loss: 0.05221521109342575\n",
      "Epoch 29903/30000 Training Loss: 0.061818938702344894\n",
      "Epoch 29904/30000 Training Loss: 0.034345973283052444\n",
      "Epoch 29905/30000 Training Loss: 0.057126060128211975\n",
      "Epoch 29906/30000 Training Loss: 0.04069102928042412\n",
      "Epoch 29907/30000 Training Loss: 0.0437949076294899\n",
      "Epoch 29908/30000 Training Loss: 0.03798199072480202\n",
      "Epoch 29909/30000 Training Loss: 0.032037124037742615\n",
      "Epoch 29910/30000 Training Loss: 0.04029916599392891\n",
      "Epoch 29911/30000 Training Loss: 0.04535617679357529\n",
      "Epoch 29912/30000 Training Loss: 0.04431398585438728\n",
      "Epoch 29913/30000 Training Loss: 0.05055297166109085\n",
      "Epoch 29914/30000 Training Loss: 0.043585631996393204\n",
      "Epoch 29915/30000 Training Loss: 0.03428548574447632\n",
      "Epoch 29916/30000 Training Loss: 0.0331035852432251\n",
      "Epoch 29917/30000 Training Loss: 0.046019889414310455\n",
      "Epoch 29918/30000 Training Loss: 0.04934830591082573\n",
      "Epoch 29919/30000 Training Loss: 0.0474989116191864\n",
      "Epoch 29920/30000 Training Loss: 0.038053832948207855\n",
      "Epoch 29921/30000 Training Loss: 0.03413970023393631\n",
      "Epoch 29922/30000 Training Loss: 0.04898958280682564\n",
      "Epoch 29923/30000 Training Loss: 0.05034241825342178\n",
      "Epoch 29924/30000 Training Loss: 0.05909064784646034\n",
      "Epoch 29925/30000 Training Loss: 0.038610100746154785\n",
      "Epoch 29926/30000 Training Loss: 0.05001154541969299\n",
      "Epoch 29927/30000 Training Loss: 0.040303487330675125\n",
      "Epoch 29928/30000 Training Loss: 0.04431261867284775\n",
      "Epoch 29929/30000 Training Loss: 0.04466851055622101\n",
      "Epoch 29930/30000 Training Loss: 0.03582192212343216\n",
      "Epoch 29931/30000 Training Loss: 0.04580092057585716\n",
      "Epoch 29932/30000 Training Loss: 0.039520468562841415\n",
      "Epoch 29933/30000 Training Loss: 0.035442933440208435\n",
      "Epoch 29934/30000 Training Loss: 0.04419337585568428\n",
      "Epoch 29935/30000 Training Loss: 0.03959853947162628\n",
      "Epoch 29936/30000 Training Loss: 0.03286450356245041\n",
      "Epoch 29937/30000 Training Loss: 0.0529535636305809\n",
      "Epoch 29938/30000 Training Loss: 0.042571425437927246\n",
      "Epoch 29939/30000 Training Loss: 0.04317121580243111\n",
      "Epoch 29940/30000 Training Loss: 0.04225267469882965\n",
      "Epoch 29941/30000 Training Loss: 0.04319644346833229\n",
      "Epoch 29942/30000 Training Loss: 0.04191458225250244\n",
      "Epoch 29943/30000 Training Loss: 0.04610312730073929\n",
      "Epoch 29944/30000 Training Loss: 0.03987206146121025\n",
      "Epoch 29945/30000 Training Loss: 0.04747847840189934\n",
      "Epoch 29946/30000 Training Loss: 0.056284837424755096\n",
      "Epoch 29947/30000 Training Loss: 0.04325971007347107\n",
      "Epoch 29948/30000 Training Loss: 0.04011308401823044\n",
      "Epoch 29949/30000 Training Loss: 0.038751937448978424\n",
      "Epoch 29950/30000 Training Loss: 0.05890410393476486\n",
      "Epoch 29951/30000 Training Loss: 0.05629590153694153\n",
      "Epoch 29952/30000 Training Loss: 0.04379276931285858\n",
      "Epoch 29953/30000 Training Loss: 0.04396229237318039\n",
      "Epoch 29954/30000 Training Loss: 0.03292194753885269\n",
      "Epoch 29955/30000 Training Loss: 0.06215866655111313\n",
      "Epoch 29956/30000 Training Loss: 0.053709059953689575\n",
      "Epoch 29957/30000 Training Loss: 0.0422290563583374\n",
      "Epoch 29958/30000 Training Loss: 0.03793826699256897\n",
      "Epoch 29959/30000 Training Loss: 0.03780592978000641\n",
      "Epoch 29960/30000 Training Loss: 0.047505274415016174\n",
      "Epoch 29961/30000 Training Loss: 0.050822410732507706\n",
      "Epoch 29962/30000 Training Loss: 0.03907642140984535\n",
      "Epoch 29963/30000 Training Loss: 0.04055282846093178\n",
      "Epoch 29964/30000 Training Loss: 0.04594719037413597\n",
      "Epoch 29965/30000 Training Loss: 0.03368058055639267\n",
      "Epoch 29966/30000 Training Loss: 0.0419662706553936\n",
      "Epoch 29967/30000 Training Loss: 0.04731878638267517\n",
      "Epoch 29968/30000 Training Loss: 0.04894161969423294\n",
      "Epoch 29969/30000 Training Loss: 0.04012700542807579\n",
      "Epoch 29970/30000 Training Loss: 0.04005475342273712\n",
      "Epoch 29971/30000 Training Loss: 0.043652188032865524\n",
      "Epoch 29972/30000 Training Loss: 0.038640812039375305\n",
      "Epoch 29973/30000 Training Loss: 0.041761159896850586\n",
      "Epoch 29974/30000 Training Loss: 0.039351899176836014\n",
      "Epoch 29975/30000 Training Loss: 0.05030461773276329\n",
      "Epoch 29976/30000 Training Loss: 0.048063937574625015\n",
      "Epoch 29977/30000 Training Loss: 0.03801751881837845\n",
      "Epoch 29978/30000 Training Loss: 0.052572574466466904\n",
      "Epoch 29979/30000 Training Loss: 0.036002494394779205\n",
      "Epoch 29980/30000 Training Loss: 0.034894511103630066\n",
      "Epoch 29981/30000 Training Loss: 0.05057805776596069\n",
      "Epoch 29982/30000 Training Loss: 0.057630762457847595\n",
      "Epoch 29983/30000 Training Loss: 0.038339752703905106\n",
      "Epoch 29984/30000 Training Loss: 0.07419781386852264\n",
      "Epoch 29985/30000 Training Loss: 0.047201793640851974\n",
      "Epoch 29986/30000 Training Loss: 0.052620720118284225\n",
      "Epoch 29987/30000 Training Loss: 0.052080992609262466\n",
      "Epoch 29988/30000 Training Loss: 0.05167657136917114\n",
      "Epoch 29989/30000 Training Loss: 0.042292702943086624\n",
      "Epoch 29990/30000 Training Loss: 0.0474931038916111\n",
      "Epoch 29991/30000 Training Loss: 0.043462611734867096\n",
      "Epoch 29992/30000 Training Loss: 0.04054136574268341\n",
      "Epoch 29993/30000 Training Loss: 0.04194360226392746\n",
      "Epoch 29994/30000 Training Loss: 0.045010849833488464\n",
      "Epoch 29995/30000 Training Loss: 0.04048319533467293\n",
      "Epoch 29996/30000 Training Loss: 0.05956657975912094\n",
      "Epoch 29997/30000 Training Loss: 0.0378798246383667\n",
      "Epoch 29998/30000 Training Loss: 0.05622885376214981\n",
      "Epoch 29999/30000 Training Loss: 0.0556979700922966\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "        eval_data=val,\n",
    "        eval_interval=eval_interval,\n",
    "        warm_up_steps=500,\n",
    "        llrd_rate=0.999\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d\")\n",
    "# best_model_fp = f'models/0221/model_best_epoch_20600_min_val_loss_0.028699999675154686.pkl'\n",
    "# with open(best_model_fp, 'rb') as handle:\n",
    "#     best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        2000,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref = sampling(model, \n",
    "                                                           diffusion, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184761",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80683b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
