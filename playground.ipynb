{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a01d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 16:05:11.654826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 16:05:11.654903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 16:05:11.665882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-03 16:05:11.683628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 16:05:13.463478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from model_arch.run_train import *\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from model_arch.train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from model_arch.tokenizer import load_tokenizer, load_model_emb\n",
    "from model_arch.sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=50\n",
    "microbatch=10\n",
    "epochs=30_000\n",
    "eval_interval=50\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=2000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.15\n",
    "seed=102\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data_dir='data'\n",
    "data_player_dir='data/with_player'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer('shakespeare', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30268, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['o hell! what have we here? a carrion death, within whose empty eye there is a written scroll!', 'and his disciples only envy at, ye blew the fire that burns ye now have at ye! enter king,'] [\"i'll read the writing. all that glitters is not gold, often have you heard that told\", 'frowning on them, takes his seat']\n",
      "RAM used: 710.78 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "RAM used: 729.76 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac2f28439114bf3bbf5c23276865b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "### tokenized_datasets...example [2, 37, 1300, 6, 164, 150, 133, 237, 22, 23, 7135, 432, 10, 906, 569, 3066, 756, 210, 121, 23, 4180, 7422, 6, 3]\n",
      "RAM used: 775.81 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369ee380e7d34ec488cab5aaf1dc3f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 809.20 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f480c6da3ec40019bbd9c623f0fd79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 48627\n",
      "}) padded dataset\n",
      "RAM used: 899.28 MB\n",
      "RAM used: 899.28 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"petruchio is my name, antonio's son, a man well known throughout all italy.\", 'the matter is to me, sir, as concerning jaquenetta. the manner of it is,'] ['i know him well you are welcome for his sake.', 'i was taken with the manner.']\n",
      "RAM used: 860.83 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "RAM used: 860.83 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29187e2f07b7416ba75e86b6774ee29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "### tokenized_datasets...example [2, 3886, 121, 105, 520, 10, 2546, 9, 41, 478, 10, 23, 211, 254, 1233, 9840, 187, 4043, 12, 3]\n",
      "RAM used: 872.35 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f3ad6b3a9c49b78c8fa4e83c88f146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 880.95 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6b1c88aed54f669a9f0a5d67ae70ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 12147\n",
      "}) padded dataset\n",
      "RAM used: 901.86 MB\n",
      "RAM used: 901.86 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight, # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(30268, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=30268, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.15, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.15, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.15, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )\n",
    "\n",
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192508"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 0.0001\n",
      "name: lm_head.bias, lr: 0.0001\n",
      "name: time_embed.0.weight, lr: 0.0001\n",
      "name: time_embed.0.bias, lr: 0.0001\n",
      "name: time_embed.2.weight, lr: 0.0001\n",
      "name: time_embed.2.bias, lr: 0.0001\n",
      "name: input_up_proj.0.weight, lr: 0.0001\n",
      "name: input_up_proj.0.bias, lr: 0.0001\n",
      "name: input_up_proj.2.weight, lr: 0.0001\n",
      "name: input_up_proj.2.bias, lr: 0.0001\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.000112817809950197\n",
      "name: position_embeddings.weight, lr: 0.00011395738378807778\n",
      "name: LayerNorm.weight, lr: 0.00011395738378807778\n",
      "name: LayerNorm.bias, lr: 0.00011395738378807778\n",
      "name: output_down_proj.0.weight, lr: 0.00011395738378807778\n",
      "name: output_down_proj.0.bias, lr: 0.00011395738378807778\n",
      "name: output_down_proj.2.weight, lr: 0.00011395738378807778\n",
      "name: output_down_proj.2.bias, lr: 0.00011395738378807778\n",
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30000 Training Loss: 1.1970067024230957\n",
      "Epoch 0/30000 Validation Loss: 1.1958271265029907\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=1.1958271265029907<=============\n",
      "Epoch 1/30000 Training Loss: 1.2004178762435913\n",
      "Epoch 2/30000 Training Loss: 1.1941783428192139\n",
      "Epoch 3/30000 Training Loss: 1.1934547424316406\n",
      "Epoch 4/30000 Training Loss: 1.1885653734207153\n",
      "Epoch 5/30000 Training Loss: 1.181810736656189\n",
      "Epoch 6/30000 Training Loss: 1.1719131469726562\n",
      "Epoch 7/30000 Training Loss: 1.1592326164245605\n",
      "Epoch 8/30000 Training Loss: 1.149044394493103\n",
      "Epoch 9/30000 Training Loss: 1.1384356021881104\n",
      "Epoch 10/30000 Training Loss: 1.1222847700119019\n",
      "Epoch 11/30000 Training Loss: 1.1068856716156006\n",
      "Epoch 12/30000 Training Loss: 1.0870779752731323\n",
      "Epoch 13/30000 Training Loss: 1.0685601234436035\n",
      "Epoch 14/30000 Training Loss: 1.051645040512085\n",
      "Epoch 15/30000 Training Loss: 1.0343204736709595\n",
      "Epoch 16/30000 Training Loss: 1.007979154586792\n",
      "Epoch 17/30000 Training Loss: 0.987214207649231\n",
      "Epoch 18/30000 Training Loss: 0.9627071619033813\n",
      "Epoch 19/30000 Training Loss: 0.9463809728622437\n",
      "Epoch 20/30000 Training Loss: 0.9207507967948914\n",
      "Epoch 21/30000 Training Loss: 0.8905667066574097\n",
      "Epoch 22/30000 Training Loss: 0.8733418583869934\n",
      "Epoch 23/30000 Training Loss: 0.849351704120636\n",
      "Epoch 24/30000 Training Loss: 0.8225786089897156\n",
      "Epoch 25/30000 Training Loss: 0.8032064437866211\n",
      "Epoch 26/30000 Training Loss: 0.7813860177993774\n",
      "Epoch 27/30000 Training Loss: 0.7610443234443665\n",
      "Epoch 28/30000 Training Loss: 0.7399049997329712\n",
      "Epoch 29/30000 Training Loss: 0.7219942808151245\n",
      "Epoch 30/30000 Training Loss: 0.7070871591567993\n",
      "Epoch 31/30000 Training Loss: 0.706251323223114\n",
      "Epoch 32/30000 Training Loss: 0.6870123147964478\n",
      "Epoch 33/30000 Training Loss: 0.6647860407829285\n",
      "Epoch 34/30000 Training Loss: 0.6571004390716553\n",
      "Epoch 35/30000 Training Loss: 0.6421462297439575\n",
      "Epoch 36/30000 Training Loss: 0.6424463987350464\n",
      "Epoch 37/30000 Training Loss: 0.6347985863685608\n",
      "Epoch 38/30000 Training Loss: 0.6110183000564575\n",
      "Epoch 39/30000 Training Loss: 0.6064215898513794\n",
      "Epoch 40/30000 Training Loss: 0.5972377061843872\n",
      "Epoch 41/30000 Training Loss: 0.6153232455253601\n",
      "Epoch 42/30000 Training Loss: 0.5956122279167175\n",
      "Epoch 43/30000 Training Loss: 0.5808294415473938\n",
      "Epoch 44/30000 Training Loss: 0.5796988010406494\n",
      "Epoch 45/30000 Training Loss: 0.6042453646659851\n",
      "Epoch 46/30000 Training Loss: 0.5893253087997437\n",
      "Epoch 47/30000 Training Loss: 0.5621951222419739\n",
      "Epoch 48/30000 Training Loss: 0.585788369178772\n",
      "Epoch 49/30000 Training Loss: 0.5500537753105164\n",
      "Epoch 50/30000 Training Loss: 0.5671964883804321\n",
      "Epoch 50/30000 Validation Loss: 0.5811264514923096\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.5811264514923096<=============\n",
      "Epoch 51/30000 Training Loss: 0.5366176962852478\n",
      "Epoch 52/30000 Training Loss: 0.5615078210830688\n",
      "Epoch 53/30000 Training Loss: 0.5510190725326538\n",
      "Epoch 54/30000 Training Loss: 0.5481305122375488\n",
      "Epoch 55/30000 Training Loss: 0.5326131582260132\n",
      "Epoch 56/30000 Training Loss: 0.5627139806747437\n",
      "Epoch 57/30000 Training Loss: 0.5281168222427368\n",
      "Epoch 58/30000 Training Loss: 0.5502723455429077\n",
      "Epoch 59/30000 Training Loss: 0.5515777468681335\n",
      "Epoch 60/30000 Training Loss: 0.5401934385299683\n",
      "Epoch 61/30000 Training Loss: 0.5267826318740845\n",
      "Epoch 62/30000 Training Loss: 0.5418804883956909\n",
      "Epoch 63/30000 Training Loss: 0.5243663787841797\n",
      "Epoch 64/30000 Training Loss: 0.535714328289032\n",
      "Epoch 65/30000 Training Loss: 0.5455024838447571\n",
      "Epoch 66/30000 Training Loss: 0.5528792142868042\n",
      "Epoch 67/30000 Training Loss: 0.5486477017402649\n",
      "Epoch 68/30000 Training Loss: 0.5332342386245728\n",
      "Epoch 69/30000 Training Loss: 0.5096827745437622\n",
      "Epoch 70/30000 Training Loss: 0.5630772709846497\n",
      "Epoch 71/30000 Training Loss: 0.55779629945755\n",
      "Epoch 72/30000 Training Loss: 0.5494336485862732\n",
      "Epoch 73/30000 Training Loss: 0.533037006855011\n",
      "Epoch 74/30000 Training Loss: 0.5440722703933716\n",
      "Epoch 75/30000 Training Loss: 0.5366629958152771\n",
      "Epoch 76/30000 Training Loss: 0.5287416577339172\n",
      "Epoch 77/30000 Training Loss: 0.5470104217529297\n",
      "Epoch 78/30000 Training Loss: 0.5217452645301819\n",
      "Epoch 79/30000 Training Loss: 0.5315961837768555\n",
      "Epoch 80/30000 Training Loss: 0.5059636831283569\n",
      "Epoch 81/30000 Training Loss: 0.5319191813468933\n",
      "Epoch 82/30000 Training Loss: 0.5440243482589722\n",
      "Epoch 83/30000 Training Loss: 0.5315830707550049\n",
      "Epoch 84/30000 Training Loss: 0.5237440466880798\n",
      "Epoch 85/30000 Training Loss: 0.5099000334739685\n",
      "Epoch 86/30000 Training Loss: 0.5318566560745239\n",
      "Epoch 87/30000 Training Loss: 0.5180288553237915\n",
      "Epoch 88/30000 Training Loss: 0.5403167605400085\n",
      "Epoch 89/30000 Training Loss: 0.54004967212677\n",
      "Epoch 90/30000 Training Loss: 0.5213409066200256\n",
      "Epoch 91/30000 Training Loss: 0.5167662501335144\n",
      "Epoch 92/30000 Training Loss: 0.5291605591773987\n",
      "Epoch 93/30000 Training Loss: 0.5293266177177429\n",
      "Epoch 94/30000 Training Loss: 0.5185636878013611\n",
      "Epoch 95/30000 Training Loss: 0.520990788936615\n",
      "Epoch 96/30000 Training Loss: 0.5542292594909668\n",
      "Epoch 97/30000 Training Loss: 0.5329056978225708\n",
      "Epoch 98/30000 Training Loss: 0.5101889967918396\n",
      "Epoch 99/30000 Training Loss: 0.5095702409744263\n",
      "Epoch 100/30000 Training Loss: 0.5065385699272156\n",
      "Epoch 100/30000 Validation Loss: 0.5113189220428467\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.5113189220428467<=============\n",
      "Epoch 101/30000 Training Loss: 0.5087641477584839\n",
      "Epoch 102/30000 Training Loss: 0.5022135972976685\n",
      "Epoch 103/30000 Training Loss: 0.5094485282897949\n",
      "Epoch 104/30000 Training Loss: 0.49289846420288086\n",
      "Epoch 105/30000 Training Loss: 0.4938674569129944\n",
      "Epoch 106/30000 Training Loss: 0.459690660238266\n",
      "Epoch 107/30000 Training Loss: 0.47169190645217896\n",
      "Epoch 108/30000 Training Loss: 0.46334099769592285\n",
      "Epoch 109/30000 Training Loss: 0.4534854292869568\n",
      "Epoch 110/30000 Training Loss: 0.4313727915287018\n",
      "Epoch 111/30000 Training Loss: 0.405263751745224\n",
      "Epoch 112/30000 Training Loss: 0.4095739722251892\n",
      "Epoch 113/30000 Training Loss: 0.39327472448349\n",
      "Epoch 114/30000 Training Loss: 0.38882699608802795\n",
      "Epoch 115/30000 Training Loss: 0.3829057514667511\n",
      "Epoch 116/30000 Training Loss: 0.3533690571784973\n",
      "Epoch 117/30000 Training Loss: 0.3434104919433594\n",
      "Epoch 118/30000 Training Loss: 0.3518664836883545\n",
      "Epoch 119/30000 Training Loss: 0.3459082245826721\n",
      "Epoch 120/30000 Training Loss: 0.3484257459640503\n",
      "Epoch 121/30000 Training Loss: 0.33934569358825684\n",
      "Epoch 122/30000 Training Loss: 0.3197264075279236\n",
      "Epoch 123/30000 Training Loss: 0.3218836784362793\n",
      "Epoch 124/30000 Training Loss: 0.3141082227230072\n",
      "Epoch 125/30000 Training Loss: 0.304017573595047\n",
      "Epoch 126/30000 Training Loss: 0.3298341631889343\n",
      "Epoch 127/30000 Training Loss: 0.32160788774490356\n",
      "Epoch 128/30000 Training Loss: 0.31146544218063354\n",
      "Epoch 129/30000 Training Loss: 0.30707693099975586\n",
      "Epoch 130/30000 Training Loss: 0.31999486684799194\n",
      "Epoch 131/30000 Training Loss: 0.32040536403656006\n",
      "Epoch 132/30000 Training Loss: 0.29975467920303345\n",
      "Epoch 133/30000 Training Loss: 0.31645888090133667\n",
      "Epoch 134/30000 Training Loss: 0.3122868537902832\n",
      "Epoch 135/30000 Training Loss: 0.3015814423561096\n",
      "Epoch 136/30000 Training Loss: 0.3111081123352051\n",
      "Epoch 137/30000 Training Loss: 0.3220481276512146\n",
      "Epoch 138/30000 Training Loss: 0.31672486662864685\n",
      "Epoch 139/30000 Training Loss: 0.3236430585384369\n",
      "Epoch 140/30000 Training Loss: 0.3314410448074341\n",
      "Epoch 141/30000 Training Loss: 0.304647296667099\n",
      "Epoch 142/30000 Training Loss: 0.3258165717124939\n",
      "Epoch 143/30000 Training Loss: 0.3135204315185547\n",
      "Epoch 144/30000 Training Loss: 0.2988068461418152\n",
      "Epoch 145/30000 Training Loss: 0.30122214555740356\n",
      "Epoch 146/30000 Training Loss: 0.28925400972366333\n",
      "Epoch 147/30000 Training Loss: 0.3031572699546814\n",
      "Epoch 148/30000 Training Loss: 0.29735296964645386\n",
      "Epoch 149/30000 Training Loss: 0.28917932510375977\n",
      "Epoch 150/30000 Training Loss: 0.3135322332382202\n",
      "Epoch 150/30000 Validation Loss: 0.31991708278656006\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.31991708278656006<=============\n",
      "Epoch 151/30000 Training Loss: 0.3054240345954895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/30000 Training Loss: 0.29737505316734314\n",
      "Epoch 153/30000 Training Loss: 0.31367024779319763\n",
      "Epoch 154/30000 Training Loss: 0.2956079840660095\n",
      "Epoch 155/30000 Training Loss: 0.2985587418079376\n",
      "Epoch 156/30000 Training Loss: 0.2929301857948303\n",
      "Epoch 157/30000 Training Loss: 0.29070812463760376\n",
      "Epoch 158/30000 Training Loss: 0.27864888310432434\n",
      "Epoch 159/30000 Training Loss: 0.3140440583229065\n",
      "Epoch 160/30000 Training Loss: 0.29640060663223267\n",
      "Epoch 161/30000 Training Loss: 0.29143720865249634\n",
      "Epoch 162/30000 Training Loss: 0.31726202368736267\n",
      "Epoch 163/30000 Training Loss: 0.30768561363220215\n",
      "Epoch 164/30000 Training Loss: 0.29375940561294556\n",
      "Epoch 165/30000 Training Loss: 0.2734200358390808\n",
      "Epoch 166/30000 Training Loss: 0.285648375749588\n",
      "Epoch 167/30000 Training Loss: 0.29704052209854126\n",
      "Epoch 168/30000 Training Loss: 0.2962569296360016\n",
      "Epoch 169/30000 Training Loss: 0.3092721402645111\n",
      "Epoch 170/30000 Training Loss: 0.3044865131378174\n",
      "Epoch 171/30000 Training Loss: 0.3110080659389496\n",
      "Epoch 172/30000 Training Loss: 0.2912440598011017\n",
      "Epoch 173/30000 Training Loss: 0.3087003827095032\n",
      "Epoch 174/30000 Training Loss: 0.29195213317871094\n",
      "Epoch 175/30000 Training Loss: 0.27769893407821655\n",
      "Epoch 176/30000 Training Loss: 0.27917852997779846\n",
      "Epoch 177/30000 Training Loss: 0.2818506062030792\n",
      "Epoch 178/30000 Training Loss: 0.2822137475013733\n",
      "Epoch 179/30000 Training Loss: 0.302735298871994\n",
      "Epoch 180/30000 Training Loss: 0.2968203127384186\n",
      "Epoch 181/30000 Training Loss: 0.2960204482078552\n",
      "Epoch 182/30000 Training Loss: 0.29384779930114746\n",
      "Epoch 183/30000 Training Loss: 0.2859666347503662\n",
      "Epoch 184/30000 Training Loss: 0.2917361259460449\n",
      "Epoch 185/30000 Training Loss: 0.29021912813186646\n",
      "Epoch 186/30000 Training Loss: 0.2916491627693176\n",
      "Epoch 187/30000 Training Loss: 0.2835816740989685\n",
      "Epoch 188/30000 Training Loss: 0.2965438961982727\n",
      "Epoch 189/30000 Training Loss: 0.2715587317943573\n",
      "Epoch 190/30000 Training Loss: 0.27802616357803345\n",
      "Epoch 191/30000 Training Loss: 0.2845434546470642\n",
      "Epoch 192/30000 Training Loss: 0.2756046652793884\n",
      "Epoch 193/30000 Training Loss: 0.284742146730423\n",
      "Epoch 194/30000 Training Loss: 0.27580201625823975\n",
      "Epoch 195/30000 Training Loss: 0.26061978936195374\n",
      "Epoch 196/30000 Training Loss: 0.27637338638305664\n",
      "Epoch 197/30000 Training Loss: 0.26978379487991333\n",
      "Epoch 198/30000 Training Loss: 0.28163212537765503\n",
      "Epoch 199/30000 Training Loss: 0.26469552516937256\n",
      "Epoch 200/30000 Training Loss: 0.25953859090805054\n",
      "Epoch 200/30000 Validation Loss: 0.2777394950389862\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.2777394950389862<=============\n",
      "Epoch 201/30000 Training Loss: 0.25249409675598145\n",
      "Epoch 202/30000 Training Loss: 0.26320815086364746\n",
      "Epoch 203/30000 Training Loss: 0.2683315873146057\n",
      "Epoch 204/30000 Training Loss: 0.256181538105011\n",
      "Epoch 205/30000 Training Loss: 0.2672466039657593\n",
      "Epoch 206/30000 Training Loss: 0.2616596519947052\n",
      "Epoch 207/30000 Training Loss: 0.2568652331829071\n",
      "Epoch 208/30000 Training Loss: 0.2648673951625824\n",
      "Epoch 209/30000 Training Loss: 0.2609727680683136\n",
      "Epoch 210/30000 Training Loss: 0.2679259181022644\n",
      "Epoch 211/30000 Training Loss: 0.2455800473690033\n",
      "Epoch 212/30000 Training Loss: 0.24069049954414368\n",
      "Epoch 213/30000 Training Loss: 0.24731969833374023\n",
      "Epoch 214/30000 Training Loss: 0.2584902346134186\n",
      "Epoch 215/30000 Training Loss: 0.2630442678928375\n",
      "Epoch 216/30000 Training Loss: 0.23559045791625977\n",
      "Epoch 217/30000 Training Loss: 0.2527123987674713\n",
      "Epoch 218/30000 Training Loss: 0.25072798132896423\n",
      "Epoch 219/30000 Training Loss: 0.2553597688674927\n",
      "Epoch 220/30000 Training Loss: 0.2555643916130066\n",
      "Epoch 221/30000 Training Loss: 0.26502442359924316\n",
      "Epoch 222/30000 Training Loss: 0.23710927367210388\n",
      "Epoch 223/30000 Training Loss: 0.24942946434020996\n",
      "Epoch 224/30000 Training Loss: 0.24819150567054749\n",
      "Epoch 225/30000 Training Loss: 0.242503359913826\n",
      "Epoch 226/30000 Training Loss: 0.2554400563240051\n",
      "Epoch 227/30000 Training Loss: 0.23890486359596252\n",
      "Epoch 228/30000 Training Loss: 0.24857597053050995\n",
      "Epoch 229/30000 Training Loss: 0.245161771774292\n",
      "Epoch 230/30000 Training Loss: 0.25012823939323425\n",
      "Epoch 231/30000 Training Loss: 0.21067793667316437\n",
      "Epoch 232/30000 Training Loss: 0.2476683109998703\n",
      "Epoch 233/30000 Training Loss: 0.23324227333068848\n",
      "Epoch 234/30000 Training Loss: 0.21010839939117432\n",
      "Epoch 235/30000 Training Loss: 0.23985955119132996\n",
      "Epoch 236/30000 Training Loss: 0.24391576647758484\n",
      "Epoch 237/30000 Training Loss: 0.23566003143787384\n",
      "Epoch 238/30000 Training Loss: 0.2352636754512787\n",
      "Epoch 239/30000 Training Loss: 0.2376931607723236\n",
      "Epoch 240/30000 Training Loss: 0.23585140705108643\n",
      "Epoch 241/30000 Training Loss: 0.23949949443340302\n",
      "Epoch 242/30000 Training Loss: 0.2284751683473587\n",
      "Epoch 243/30000 Training Loss: 0.2349877655506134\n",
      "Epoch 244/30000 Training Loss: 0.23325249552726746\n",
      "Epoch 245/30000 Training Loss: 0.21742157638072968\n",
      "Epoch 246/30000 Training Loss: 0.2372126579284668\n",
      "Epoch 247/30000 Training Loss: 0.22356641292572021\n",
      "Epoch 248/30000 Training Loss: 0.2240108996629715\n",
      "Epoch 249/30000 Training Loss: 0.21871760487556458\n",
      "Epoch 250/30000 Training Loss: 0.22851581871509552\n",
      "Epoch 250/30000 Validation Loss: 0.24116268754005432\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.24116268754005432<=============\n",
      "Epoch 251/30000 Training Loss: 0.22671940922737122\n",
      "Epoch 252/30000 Training Loss: 0.22829778492450714\n",
      "Epoch 253/30000 Training Loss: 0.22532013058662415\n",
      "Epoch 254/30000 Training Loss: 0.24635915458202362\n",
      "Epoch 255/30000 Training Loss: 0.21753421425819397\n",
      "Epoch 256/30000 Training Loss: 0.2364768236875534\n",
      "Epoch 257/30000 Training Loss: 0.2189256250858307\n",
      "Epoch 258/30000 Training Loss: 0.21641544997692108\n",
      "Epoch 259/30000 Training Loss: 0.2241383045911789\n",
      "Epoch 260/30000 Training Loss: 0.21086406707763672\n",
      "Epoch 261/30000 Training Loss: 0.22275523841381073\n",
      "Epoch 262/30000 Training Loss: 0.22798669338226318\n",
      "Epoch 263/30000 Training Loss: 0.21650953590869904\n",
      "Epoch 264/30000 Training Loss: 0.23016873002052307\n",
      "Epoch 265/30000 Training Loss: 0.21459129452705383\n",
      "Epoch 266/30000 Training Loss: 0.21770302951335907\n",
      "Epoch 267/30000 Training Loss: 0.21305540204048157\n",
      "Epoch 268/30000 Training Loss: 0.21637305617332458\n",
      "Epoch 269/30000 Training Loss: 0.22865143418312073\n",
      "Epoch 270/30000 Training Loss: 0.20199072360992432\n",
      "Epoch 271/30000 Training Loss: 0.20132453739643097\n",
      "Epoch 272/30000 Training Loss: 0.20763468742370605\n",
      "Epoch 273/30000 Training Loss: 0.20902809500694275\n",
      "Epoch 274/30000 Training Loss: 0.2008124589920044\n",
      "Epoch 275/30000 Training Loss: 0.21347036957740784\n",
      "Epoch 276/30000 Training Loss: 0.2056184560060501\n",
      "Epoch 277/30000 Training Loss: 0.19128987193107605\n",
      "Epoch 278/30000 Training Loss: 0.20625050365924835\n",
      "Epoch 279/30000 Training Loss: 0.20793545246124268\n",
      "Epoch 280/30000 Training Loss: 0.19486922025680542\n",
      "Epoch 281/30000 Training Loss: 0.21472227573394775\n",
      "Epoch 282/30000 Training Loss: 0.18587857484817505\n",
      "Epoch 283/30000 Training Loss: 0.20230889320373535\n",
      "Epoch 284/30000 Training Loss: 0.19410933554172516\n",
      "Epoch 285/30000 Training Loss: 0.20696993172168732\n",
      "Epoch 286/30000 Training Loss: 0.1927686631679535\n",
      "Epoch 287/30000 Training Loss: 0.19811508059501648\n",
      "Epoch 288/30000 Training Loss: 0.20049719512462616\n",
      "Epoch 289/30000 Training Loss: 0.20290358364582062\n",
      "Epoch 290/30000 Training Loss: 0.2125377357006073\n",
      "Epoch 291/30000 Training Loss: 0.1918690800666809\n",
      "Epoch 292/30000 Training Loss: 0.18640336394309998\n",
      "Epoch 293/30000 Training Loss: 0.19061562418937683\n",
      "Epoch 294/30000 Training Loss: 0.1950397789478302\n",
      "Epoch 295/30000 Training Loss: 0.19527728855609894\n",
      "Epoch 296/30000 Training Loss: 0.19942495226860046\n",
      "Epoch 297/30000 Training Loss: 0.1892877221107483\n",
      "Epoch 298/30000 Training Loss: 0.18349584937095642\n",
      "Epoch 299/30000 Training Loss: 0.19971488416194916\n",
      "Epoch 300/30000 Training Loss: 0.17953549325466156\n",
      "Epoch 300/30000 Validation Loss: 0.19285190105438232\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.19285190105438232<=============\n",
      "Epoch 301/30000 Training Loss: 0.19615556299686432\n",
      "Epoch 302/30000 Training Loss: 0.18301966786384583\n",
      "Epoch 303/30000 Training Loss: 0.188481867313385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 304/30000 Training Loss: 0.19740399718284607\n",
      "Epoch 305/30000 Training Loss: 0.19297859072685242\n",
      "Epoch 306/30000 Training Loss: 0.1779545247554779\n",
      "Epoch 307/30000 Training Loss: 0.18243622779846191\n",
      "Epoch 308/30000 Training Loss: 0.20063288509845734\n",
      "Epoch 309/30000 Training Loss: 0.1868320256471634\n",
      "Epoch 310/30000 Training Loss: 0.1787230223417282\n",
      "Epoch 311/30000 Training Loss: 0.1886361837387085\n",
      "Epoch 312/30000 Training Loss: 0.18657103180885315\n",
      "Epoch 313/30000 Training Loss: 0.17386110126972198\n",
      "Epoch 314/30000 Training Loss: 0.17631809413433075\n",
      "Epoch 315/30000 Training Loss: 0.18970216810703278\n",
      "Epoch 316/30000 Training Loss: 0.19067072868347168\n",
      "Epoch 317/30000 Training Loss: 0.2004091739654541\n",
      "Epoch 318/30000 Training Loss: 0.189642071723938\n",
      "Epoch 319/30000 Training Loss: 0.1978829801082611\n",
      "Epoch 320/30000 Training Loss: 0.18722756206989288\n",
      "Epoch 321/30000 Training Loss: 0.1832505762577057\n",
      "Epoch 322/30000 Training Loss: 0.18679013848304749\n",
      "Epoch 323/30000 Training Loss: 0.19232776761054993\n",
      "Epoch 324/30000 Training Loss: 0.18314087390899658\n",
      "Epoch 325/30000 Training Loss: 0.1887078881263733\n",
      "Epoch 326/30000 Training Loss: 0.18206918239593506\n",
      "Epoch 327/30000 Training Loss: 0.16679279506206512\n",
      "Epoch 328/30000 Training Loss: 0.17739591002464294\n",
      "Epoch 329/30000 Training Loss: 0.17631250619888306\n",
      "Epoch 330/30000 Training Loss: 0.18094702064990997\n",
      "Epoch 331/30000 Training Loss: 0.18709015846252441\n",
      "Epoch 332/30000 Training Loss: 0.1820034682750702\n",
      "Epoch 333/30000 Training Loss: 0.17291118204593658\n",
      "Epoch 334/30000 Training Loss: 0.16684173047542572\n",
      "Epoch 335/30000 Training Loss: 0.16293977200984955\n",
      "Epoch 336/30000 Training Loss: 0.1713576763868332\n",
      "Epoch 337/30000 Training Loss: 0.19512833654880524\n",
      "Epoch 338/30000 Training Loss: 0.16410401463508606\n",
      "Epoch 339/30000 Training Loss: 0.1790536791086197\n",
      "Epoch 340/30000 Training Loss: 0.1637355387210846\n",
      "Epoch 341/30000 Training Loss: 0.16395144164562225\n",
      "Epoch 342/30000 Training Loss: 0.17620845139026642\n",
      "Epoch 343/30000 Training Loss: 0.16751250624656677\n",
      "Epoch 344/30000 Training Loss: 0.1697627156972885\n",
      "Epoch 345/30000 Training Loss: 0.16884362697601318\n",
      "Epoch 346/30000 Training Loss: 0.1941380351781845\n",
      "Epoch 347/30000 Training Loss: 0.16137094795703888\n",
      "Epoch 348/30000 Training Loss: 0.1652492731809616\n",
      "Epoch 349/30000 Training Loss: 0.16308321058750153\n",
      "Epoch 350/30000 Training Loss: 0.1687559187412262\n",
      "Epoch 350/30000 Validation Loss: 0.16644051671028137\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.16644051671028137<=============\n",
      "Epoch 351/30000 Training Loss: 0.1707821786403656\n",
      "Epoch 352/30000 Training Loss: 0.16463303565979004\n",
      "Epoch 353/30000 Training Loss: 0.1534215211868286\n",
      "Epoch 354/30000 Training Loss: 0.17051438987255096\n",
      "Epoch 355/30000 Training Loss: 0.16021765768527985\n",
      "Epoch 356/30000 Training Loss: 0.1715298295021057\n",
      "Epoch 357/30000 Training Loss: 0.16785314679145813\n",
      "Epoch 358/30000 Training Loss: 0.17912177741527557\n",
      "Epoch 359/30000 Training Loss: 0.1623513549566269\n",
      "Epoch 360/30000 Training Loss: 0.16588616371154785\n",
      "Epoch 361/30000 Training Loss: 0.15982525050640106\n",
      "Epoch 362/30000 Training Loss: 0.1483525037765503\n",
      "Epoch 363/30000 Training Loss: 0.1452299803495407\n",
      "Epoch 364/30000 Training Loss: 0.16893598437309265\n",
      "Epoch 365/30000 Training Loss: 0.16371864080429077\n",
      "Epoch 366/30000 Training Loss: 0.17398889362812042\n",
      "Epoch 367/30000 Training Loss: 0.1639982908964157\n",
      "Epoch 368/30000 Training Loss: 0.1634378880262375\n",
      "Epoch 369/30000 Training Loss: 0.16237716376781464\n",
      "Epoch 370/30000 Training Loss: 0.16476956009864807\n",
      "Epoch 371/30000 Training Loss: 0.1608414351940155\n",
      "Epoch 372/30000 Training Loss: 0.1657121777534485\n",
      "Epoch 373/30000 Training Loss: 0.16506540775299072\n",
      "Epoch 374/30000 Training Loss: 0.16260480880737305\n",
      "Epoch 375/30000 Training Loss: 0.15434429049491882\n",
      "Epoch 376/30000 Training Loss: 0.16727674007415771\n",
      "Epoch 377/30000 Training Loss: 0.15611878037452698\n",
      "Epoch 378/30000 Training Loss: 0.1567631959915161\n",
      "Epoch 379/30000 Training Loss: 0.15957291424274445\n",
      "Epoch 380/30000 Training Loss: 0.162835955619812\n",
      "Epoch 381/30000 Training Loss: 0.16583822667598724\n",
      "Epoch 382/30000 Training Loss: 0.1472293585538864\n",
      "Epoch 383/30000 Training Loss: 0.1544381082057953\n",
      "Epoch 384/30000 Training Loss: 0.16841493546962738\n",
      "Epoch 385/30000 Training Loss: 0.14325478672981262\n",
      "Epoch 386/30000 Training Loss: 0.15882347524166107\n",
      "Epoch 387/30000 Training Loss: 0.1705089509487152\n",
      "Epoch 388/30000 Training Loss: 0.15741777420043945\n",
      "Epoch 389/30000 Training Loss: 0.14836837351322174\n",
      "Epoch 390/30000 Training Loss: 0.1444089412689209\n",
      "Epoch 391/30000 Training Loss: 0.1624438613653183\n",
      "Epoch 392/30000 Training Loss: 0.14726419746875763\n",
      "Epoch 393/30000 Training Loss: 0.1685297191143036\n",
      "Epoch 394/30000 Training Loss: 0.14627332985401154\n",
      "Epoch 395/30000 Training Loss: 0.1502300649881363\n",
      "Epoch 396/30000 Training Loss: 0.16091100871562958\n",
      "Epoch 397/30000 Training Loss: 0.14938542246818542\n",
      "Epoch 398/30000 Training Loss: 0.16266044974327087\n",
      "Epoch 399/30000 Training Loss: 0.16698940098285675\n",
      "Epoch 400/30000 Training Loss: 0.1478322148323059\n",
      "Epoch 400/30000 Validation Loss: 0.15558648109436035\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.15558648109436035<=============\n",
      "Epoch 401/30000 Training Loss: 0.15361347794532776\n",
      "Epoch 402/30000 Training Loss: 0.14707434177398682\n",
      "Epoch 403/30000 Training Loss: 0.15311849117279053\n",
      "Epoch 404/30000 Training Loss: 0.17646829783916473\n",
      "Epoch 405/30000 Training Loss: 0.14640279114246368\n",
      "Epoch 406/30000 Training Loss: 0.14525505900382996\n",
      "Epoch 407/30000 Training Loss: 0.14455553889274597\n",
      "Epoch 408/30000 Training Loss: 0.1414860188961029\n",
      "Epoch 409/30000 Training Loss: 0.15033096075057983\n",
      "Epoch 410/30000 Training Loss: 0.15225359797477722\n",
      "Epoch 411/30000 Training Loss: 0.14797797799110413\n",
      "Epoch 412/30000 Training Loss: 0.13466835021972656\n",
      "Epoch 413/30000 Training Loss: 0.1448090523481369\n",
      "Epoch 414/30000 Training Loss: 0.1502918154001236\n",
      "Epoch 415/30000 Training Loss: 0.143913134932518\n",
      "Epoch 416/30000 Training Loss: 0.1440044343471527\n",
      "Epoch 417/30000 Training Loss: 0.1456771194934845\n",
      "Epoch 418/30000 Training Loss: 0.16206076741218567\n",
      "Epoch 419/30000 Training Loss: 0.13897854089736938\n",
      "Epoch 420/30000 Training Loss: 0.15401190519332886\n",
      "Epoch 421/30000 Training Loss: 0.14354291558265686\n",
      "Epoch 422/30000 Training Loss: 0.13604764640331268\n",
      "Epoch 423/30000 Training Loss: 0.15159280598163605\n",
      "Epoch 424/30000 Training Loss: 0.1423594057559967\n",
      "Epoch 425/30000 Training Loss: 0.15087993443012238\n",
      "Epoch 426/30000 Training Loss: 0.1379309892654419\n",
      "Epoch 427/30000 Training Loss: 0.1361701935529709\n",
      "Epoch 428/30000 Training Loss: 0.1498928964138031\n",
      "Epoch 429/30000 Training Loss: 0.1377013474702835\n",
      "Epoch 430/30000 Training Loss: 0.13544590771198273\n",
      "Epoch 431/30000 Training Loss: 0.15804889798164368\n",
      "Epoch 432/30000 Training Loss: 0.15610387921333313\n",
      "Epoch 433/30000 Training Loss: 0.13086426258087158\n",
      "Epoch 434/30000 Training Loss: 0.14213626086711884\n",
      "Epoch 435/30000 Training Loss: 0.14587369561195374\n",
      "Epoch 436/30000 Training Loss: 0.145338237285614\n",
      "Epoch 437/30000 Training Loss: 0.14214715361595154\n",
      "Epoch 438/30000 Training Loss: 0.13511128723621368\n",
      "Epoch 439/30000 Training Loss: 0.1533176600933075\n",
      "Epoch 440/30000 Training Loss: 0.13948486745357513\n",
      "Epoch 441/30000 Training Loss: 0.15470781922340393\n",
      "Epoch 442/30000 Training Loss: 0.13799595832824707\n",
      "Epoch 443/30000 Training Loss: 0.1500665694475174\n",
      "Epoch 444/30000 Training Loss: 0.13243409991264343\n",
      "Epoch 445/30000 Training Loss: 0.13896742463111877\n",
      "Epoch 446/30000 Training Loss: 0.1468496322631836\n",
      "Epoch 447/30000 Training Loss: 0.140470951795578\n",
      "Epoch 448/30000 Training Loss: 0.14843447506427765\n",
      "Epoch 449/30000 Training Loss: 0.1325099915266037\n",
      "Epoch 450/30000 Training Loss: 0.13910217583179474\n",
      "Epoch 450/30000 Validation Loss: 0.1316489279270172\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.1316489279270172<=============\n",
      "Epoch 451/30000 Training Loss: 0.14037588238716125\n",
      "Epoch 452/30000 Training Loss: 0.1394881308078766\n",
      "Epoch 453/30000 Training Loss: 0.1479605883359909\n",
      "Epoch 454/30000 Training Loss: 0.13779056072235107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/30000 Training Loss: 0.13888239860534668\n",
      "Epoch 456/30000 Training Loss: 0.13394096493721008\n",
      "Epoch 457/30000 Training Loss: 0.14087611436843872\n",
      "Epoch 458/30000 Training Loss: 0.1354662925004959\n",
      "Epoch 459/30000 Training Loss: 0.1427529901266098\n",
      "Epoch 460/30000 Training Loss: 0.13249556720256805\n",
      "Epoch 461/30000 Training Loss: 0.13626427948474884\n",
      "Epoch 462/30000 Training Loss: 0.1332254409790039\n",
      "Epoch 463/30000 Training Loss: 0.13597211241722107\n",
      "Epoch 464/30000 Training Loss: 0.1355876922607422\n",
      "Epoch 465/30000 Training Loss: 0.14648839831352234\n",
      "Epoch 466/30000 Training Loss: 0.133464053273201\n",
      "Epoch 467/30000 Training Loss: 0.13944238424301147\n",
      "Epoch 468/30000 Training Loss: 0.12180743366479874\n",
      "Epoch 469/30000 Training Loss: 0.13645844161510468\n",
      "Epoch 470/30000 Training Loss: 0.12757885456085205\n",
      "Epoch 471/30000 Training Loss: 0.11784116923809052\n",
      "Epoch 472/30000 Training Loss: 0.12752962112426758\n",
      "Epoch 473/30000 Training Loss: 0.1366899013519287\n",
      "Epoch 474/30000 Training Loss: 0.1390874683856964\n",
      "Epoch 475/30000 Training Loss: 0.12757275998592377\n",
      "Epoch 476/30000 Training Loss: 0.12975622713565826\n",
      "Epoch 477/30000 Training Loss: 0.1301940530538559\n",
      "Epoch 478/30000 Training Loss: 0.1218612939119339\n",
      "Epoch 479/30000 Training Loss: 0.12468723207712173\n",
      "Epoch 480/30000 Training Loss: 0.12081855535507202\n",
      "Epoch 481/30000 Training Loss: 0.1301361620426178\n",
      "Epoch 482/30000 Training Loss: 0.11974084377288818\n",
      "Epoch 483/30000 Training Loss: 0.12707515060901642\n",
      "Epoch 484/30000 Training Loss: 0.12461157143115997\n",
      "Epoch 485/30000 Training Loss: 0.13400384783744812\n",
      "Epoch 486/30000 Training Loss: 0.14038267731666565\n",
      "Epoch 487/30000 Training Loss: 0.1349012404680252\n",
      "Epoch 488/30000 Training Loss: 0.12304747104644775\n",
      "Epoch 489/30000 Training Loss: 0.1329064965248108\n",
      "Epoch 490/30000 Training Loss: 0.14379654824733734\n",
      "Epoch 491/30000 Training Loss: 0.12700538337230682\n",
      "Epoch 492/30000 Training Loss: 0.12871678173542023\n",
      "Epoch 493/30000 Training Loss: 0.11182872951030731\n",
      "Epoch 494/30000 Training Loss: 0.12401292473077774\n",
      "Epoch 495/30000 Training Loss: 0.1210474744439125\n",
      "Epoch 496/30000 Training Loss: 0.12244977056980133\n",
      "Epoch 497/30000 Training Loss: 0.11910472810268402\n",
      "Epoch 498/30000 Training Loss: 0.14229057729244232\n",
      "Epoch 499/30000 Training Loss: 0.12004628032445908\n",
      "Epoch 500/30000 Training Loss: 0.13329938054084778\n",
      "Epoch 500/30000 Validation Loss: 0.1311468482017517\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.1311468482017517<=============\n",
      "Epoch 501/30000 Training Loss: 0.11757709830999374\n",
      "Epoch 502/30000 Training Loss: 0.13356219232082367\n",
      "Epoch 503/30000 Training Loss: 0.13072778284549713\n",
      "Epoch 504/30000 Training Loss: 0.11622532457113266\n",
      "Epoch 505/30000 Training Loss: 0.11804397404193878\n",
      "Epoch 506/30000 Training Loss: 0.11496768891811371\n",
      "Epoch 507/30000 Training Loss: 0.126458078622818\n",
      "Epoch 508/30000 Training Loss: 0.12485761940479279\n",
      "Epoch 509/30000 Training Loss: 0.12026473134756088\n",
      "Epoch 510/30000 Training Loss: 0.12588736414909363\n",
      "Epoch 511/30000 Training Loss: 0.12770603597164154\n",
      "Epoch 512/30000 Training Loss: 0.12873855233192444\n",
      "Epoch 513/30000 Training Loss: 0.11949115991592407\n",
      "Epoch 514/30000 Training Loss: 0.12739379703998566\n",
      "Epoch 515/30000 Training Loss: 0.12218047678470612\n",
      "Epoch 516/30000 Training Loss: 0.11479216814041138\n",
      "Epoch 517/30000 Training Loss: 0.12553362548351288\n",
      "Epoch 518/30000 Training Loss: 0.13215532898902893\n",
      "Epoch 519/30000 Training Loss: 0.12723830342292786\n",
      "Epoch 520/30000 Training Loss: 0.12245416641235352\n",
      "Epoch 521/30000 Training Loss: 0.12101994454860687\n",
      "Epoch 522/30000 Training Loss: 0.12665319442749023\n",
      "Epoch 523/30000 Training Loss: 0.12132718414068222\n",
      "Epoch 524/30000 Training Loss: 0.123479463160038\n",
      "Epoch 525/30000 Training Loss: 0.12820842862129211\n",
      "Epoch 526/30000 Training Loss: 0.1293761134147644\n",
      "Epoch 527/30000 Training Loss: 0.11626199632883072\n",
      "Epoch 528/30000 Training Loss: 0.11669053137302399\n",
      "Epoch 529/30000 Training Loss: 0.11683585494756699\n",
      "Epoch 530/30000 Training Loss: 0.11453865468502045\n",
      "Epoch 531/30000 Training Loss: 0.11897160857915878\n",
      "Epoch 532/30000 Training Loss: 0.12138116359710693\n",
      "Epoch 533/30000 Training Loss: 0.12187983095645905\n",
      "Epoch 534/30000 Training Loss: 0.1243029460310936\n",
      "Epoch 535/30000 Training Loss: 0.1257113516330719\n",
      "Epoch 536/30000 Training Loss: 0.10907040536403656\n",
      "Epoch 537/30000 Training Loss: 0.12885014712810516\n",
      "Epoch 538/30000 Training Loss: 0.12976709008216858\n",
      "Epoch 539/30000 Training Loss: 0.12361235916614532\n",
      "Epoch 540/30000 Training Loss: 0.12303030490875244\n",
      "Epoch 541/30000 Training Loss: 0.12255342304706573\n",
      "Epoch 542/30000 Training Loss: 0.12457867711782455\n",
      "Epoch 543/30000 Training Loss: 0.1203131228685379\n",
      "Epoch 544/30000 Training Loss: 0.12161581218242645\n",
      "Epoch 545/30000 Training Loss: 0.12399844080209732\n",
      "Epoch 546/30000 Training Loss: 0.11957728862762451\n",
      "Epoch 547/30000 Training Loss: 0.12109848111867905\n",
      "Epoch 548/30000 Training Loss: 0.11847331374883652\n",
      "Epoch 549/30000 Training Loss: 0.12333647161722183\n",
      "Epoch 550/30000 Training Loss: 0.11485187709331512\n",
      "Epoch 550/30000 Validation Loss: 0.12832438945770264\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.12832438945770264<=============\n",
      "Epoch 551/30000 Training Loss: 0.11808403581380844\n",
      "Epoch 552/30000 Training Loss: 0.11292992532253265\n",
      "Epoch 553/30000 Training Loss: 0.11347787082195282\n",
      "Epoch 554/30000 Training Loss: 0.12080468982458115\n",
      "Epoch 555/30000 Training Loss: 0.11224289238452911\n",
      "Epoch 556/30000 Training Loss: 0.135830819606781\n",
      "Epoch 557/30000 Training Loss: 0.13077689707279205\n",
      "Epoch 558/30000 Training Loss: 0.11452199518680573\n",
      "Epoch 559/30000 Training Loss: 0.11333154141902924\n",
      "Epoch 560/30000 Training Loss: 0.11598287522792816\n",
      "Epoch 561/30000 Training Loss: 0.12598372995853424\n",
      "Epoch 562/30000 Training Loss: 0.1152806431055069\n",
      "Epoch 563/30000 Training Loss: 0.1192634329199791\n",
      "Epoch 564/30000 Training Loss: 0.13443872332572937\n",
      "Epoch 565/30000 Training Loss: 0.1285487711429596\n",
      "Epoch 566/30000 Training Loss: 0.1292421966791153\n",
      "Epoch 567/30000 Training Loss: 0.12560805678367615\n",
      "Epoch 568/30000 Training Loss: 0.12722128629684448\n",
      "Epoch 569/30000 Training Loss: 0.11355997622013092\n",
      "Epoch 570/30000 Training Loss: 0.1179078072309494\n",
      "Epoch 571/30000 Training Loss: 0.11334963142871857\n",
      "Epoch 572/30000 Training Loss: 0.11885138601064682\n",
      "Epoch 573/30000 Training Loss: 0.11725036054849625\n",
      "Epoch 574/30000 Training Loss: 0.11634872108697891\n",
      "Epoch 575/30000 Training Loss: 0.10903199762105942\n",
      "Epoch 576/30000 Training Loss: 0.11795234680175781\n",
      "Epoch 577/30000 Training Loss: 0.12263993173837662\n",
      "Epoch 578/30000 Training Loss: 0.12024781852960587\n",
      "Epoch 579/30000 Training Loss: 0.11151395738124847\n",
      "Epoch 580/30000 Training Loss: 0.12086199223995209\n",
      "Epoch 581/30000 Training Loss: 0.11818395555019379\n",
      "Epoch 582/30000 Training Loss: 0.12109851837158203\n",
      "Epoch 583/30000 Training Loss: 0.1140345111489296\n",
      "Epoch 584/30000 Training Loss: 0.11346463114023209\n",
      "Epoch 585/30000 Training Loss: 0.11782854795455933\n",
      "Epoch 586/30000 Training Loss: 0.11624326556921005\n",
      "Epoch 587/30000 Training Loss: 0.11064781993627548\n",
      "Epoch 588/30000 Training Loss: 0.11352615058422089\n",
      "Epoch 589/30000 Training Loss: 0.1135178804397583\n",
      "Epoch 590/30000 Training Loss: 0.1176537498831749\n",
      "Epoch 591/30000 Training Loss: 0.1073090210556984\n",
      "Epoch 592/30000 Training Loss: 0.10932640731334686\n",
      "Epoch 593/30000 Training Loss: 0.11164027452468872\n",
      "Epoch 594/30000 Training Loss: 0.11799295991659164\n",
      "Epoch 595/30000 Training Loss: 0.11084923893213272\n",
      "Epoch 596/30000 Training Loss: 0.10376741737127304\n",
      "Epoch 597/30000 Training Loss: 0.11700233072042465\n",
      "Epoch 598/30000 Training Loss: 0.10920681804418564\n",
      "Epoch 599/30000 Training Loss: 0.12215067446231842\n",
      "Epoch 600/30000 Training Loss: 0.12162985652685165\n",
      "Epoch 600/30000 Validation Loss: 0.10833808034658432\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.10833808034658432<=============\n",
      "Epoch 601/30000 Training Loss: 0.12260804325342178\n",
      "Epoch 602/30000 Training Loss: 0.10744911432266235\n",
      "Epoch 603/30000 Training Loss: 0.11840669065713882\n",
      "Epoch 604/30000 Training Loss: 0.10769680887460709\n",
      "Epoch 605/30000 Training Loss: 0.11270946264266968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 606/30000 Training Loss: 0.11466541141271591\n",
      "Epoch 607/30000 Training Loss: 0.09965969622135162\n",
      "Epoch 608/30000 Training Loss: 0.11794687807559967\n",
      "Epoch 609/30000 Training Loss: 0.11353415250778198\n",
      "Epoch 610/30000 Training Loss: 0.11336641013622284\n",
      "Epoch 611/30000 Training Loss: 0.11727754026651382\n",
      "Epoch 612/30000 Training Loss: 0.10245747864246368\n",
      "Epoch 613/30000 Training Loss: 0.10764578729867935\n",
      "Epoch 614/30000 Training Loss: 0.10876395553350449\n",
      "Epoch 615/30000 Training Loss: 0.10557860136032104\n",
      "Epoch 616/30000 Training Loss: 0.12104666233062744\n",
      "Epoch 617/30000 Training Loss: 0.0994412899017334\n",
      "Epoch 618/30000 Training Loss: 0.11235024034976959\n",
      "Epoch 619/30000 Training Loss: 0.10385730117559433\n",
      "Epoch 620/30000 Training Loss: 0.11595241725444794\n",
      "Epoch 621/30000 Training Loss: 0.10783064365386963\n",
      "Epoch 622/30000 Training Loss: 0.11735470592975616\n",
      "Epoch 623/30000 Training Loss: 0.10265891253948212\n",
      "Epoch 624/30000 Training Loss: 0.10888774693012238\n",
      "Epoch 625/30000 Training Loss: 0.09927263855934143\n",
      "Epoch 626/30000 Training Loss: 0.10903141647577286\n",
      "Epoch 627/30000 Training Loss: 0.10508392006158829\n",
      "Epoch 628/30000 Training Loss: 0.11388196796178818\n",
      "Epoch 629/30000 Training Loss: 0.10792771726846695\n",
      "Epoch 630/30000 Training Loss: 0.11412094533443451\n",
      "Epoch 631/30000 Training Loss: 0.10343913733959198\n",
      "Epoch 632/30000 Training Loss: 0.09506800025701523\n",
      "Epoch 633/30000 Training Loss: 0.10996057093143463\n",
      "Epoch 634/30000 Training Loss: 0.1091015562415123\n",
      "Epoch 635/30000 Training Loss: 0.10721136629581451\n",
      "Epoch 636/30000 Training Loss: 0.10055629909038544\n",
      "Epoch 637/30000 Training Loss: 0.11311827600002289\n",
      "Epoch 638/30000 Training Loss: 0.10005806386470795\n",
      "Epoch 639/30000 Training Loss: 0.11444418132305145\n",
      "Epoch 640/30000 Training Loss: 0.11316220462322235\n",
      "Epoch 641/30000 Training Loss: 0.11487332731485367\n",
      "Epoch 642/30000 Training Loss: 0.1126556545495987\n",
      "Epoch 643/30000 Training Loss: 0.11629889905452728\n",
      "Epoch 644/30000 Training Loss: 0.10471566766500473\n",
      "Epoch 645/30000 Training Loss: 0.11408336460590363\n",
      "Epoch 646/30000 Training Loss: 0.10711608082056046\n",
      "Epoch 647/30000 Training Loss: 0.10451239347457886\n",
      "Epoch 648/30000 Training Loss: 0.09899269044399261\n",
      "Epoch 649/30000 Training Loss: 0.11283640563488007\n",
      "Epoch 650/30000 Training Loss: 0.1061212420463562\n",
      "Epoch 650/30000 Validation Loss: 0.10879562795162201\n",
      "Epoch 651/30000 Training Loss: 0.11126147210597992\n",
      "Epoch 652/30000 Training Loss: 0.09714750945568085\n",
      "Epoch 653/30000 Training Loss: 0.09362359344959259\n",
      "Epoch 654/30000 Training Loss: 0.10150196403265\n",
      "Epoch 655/30000 Training Loss: 0.10292179882526398\n",
      "Epoch 656/30000 Training Loss: 0.10964958369731903\n",
      "Epoch 657/30000 Training Loss: 0.1123833879828453\n",
      "Epoch 658/30000 Training Loss: 0.09809330105781555\n",
      "Epoch 659/30000 Training Loss: 0.10991321504116058\n",
      "Epoch 660/30000 Training Loss: 0.1062985435128212\n",
      "Epoch 661/30000 Training Loss: 0.11393783986568451\n",
      "Epoch 662/30000 Training Loss: 0.11548416316509247\n",
      "Epoch 663/30000 Training Loss: 0.09436798095703125\n",
      "Epoch 664/30000 Training Loss: 0.1005646213889122\n",
      "Epoch 665/30000 Training Loss: 0.1069926992058754\n",
      "Epoch 666/30000 Training Loss: 0.10735298693180084\n",
      "Epoch 667/30000 Training Loss: 0.09671133756637573\n",
      "Epoch 668/30000 Training Loss: 0.1028609648346901\n",
      "Epoch 669/30000 Training Loss: 0.11779798567295074\n",
      "Epoch 670/30000 Training Loss: 0.10527306795120239\n",
      "Epoch 671/30000 Training Loss: 0.10426037013530731\n",
      "Epoch 672/30000 Training Loss: 0.10170459747314453\n",
      "Epoch 673/30000 Training Loss: 0.10252755880355835\n",
      "Epoch 674/30000 Training Loss: 0.10766875743865967\n",
      "Epoch 675/30000 Training Loss: 0.10386210680007935\n",
      "Epoch 676/30000 Training Loss: 0.10561702400445938\n",
      "Epoch 677/30000 Training Loss: 0.10053732246160507\n",
      "Epoch 678/30000 Training Loss: 0.10053108632564545\n",
      "Epoch 679/30000 Training Loss: 0.10065630823373795\n",
      "Epoch 680/30000 Training Loss: 0.10668505728244781\n",
      "Epoch 681/30000 Training Loss: 0.10867049545049667\n",
      "Epoch 682/30000 Training Loss: 0.09911252558231354\n",
      "Epoch 683/30000 Training Loss: 0.10971013456583023\n",
      "Epoch 684/30000 Training Loss: 0.1062159314751625\n",
      "Epoch 685/30000 Training Loss: 0.11526761204004288\n",
      "Epoch 686/30000 Training Loss: 0.0928594246506691\n",
      "Epoch 687/30000 Training Loss: 0.10648228973150253\n",
      "Epoch 688/30000 Training Loss: 0.10779628902673721\n",
      "Epoch 689/30000 Training Loss: 0.10734019428491592\n",
      "Epoch 690/30000 Training Loss: 0.10267321765422821\n",
      "Epoch 691/30000 Training Loss: 0.10371385514736176\n",
      "Epoch 692/30000 Training Loss: 0.11848877370357513\n",
      "Epoch 693/30000 Training Loss: 0.1068195328116417\n",
      "Epoch 694/30000 Training Loss: 0.1055159792304039\n",
      "Epoch 695/30000 Training Loss: 0.09928115457296371\n",
      "Epoch 696/30000 Training Loss: 0.10396721214056015\n",
      "Epoch 697/30000 Training Loss: 0.09721140563488007\n",
      "Epoch 698/30000 Training Loss: 0.10678710043430328\n",
      "Epoch 699/30000 Training Loss: 0.09735238552093506\n",
      "Epoch 700/30000 Training Loss: 0.08992713689804077\n",
      "Epoch 700/30000 Validation Loss: 0.10373044013977051\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.10373044013977051<=============\n",
      "Epoch 701/30000 Training Loss: 0.09402483701705933\n",
      "Epoch 702/30000 Training Loss: 0.10991891473531723\n",
      "Epoch 703/30000 Training Loss: 0.10013000667095184\n",
      "Epoch 704/30000 Training Loss: 0.1019706130027771\n",
      "Epoch 705/30000 Training Loss: 0.10346166044473648\n",
      "Epoch 706/30000 Training Loss: 0.09842439740896225\n",
      "Epoch 707/30000 Training Loss: 0.1010458916425705\n",
      "Epoch 708/30000 Training Loss: 0.09799133241176605\n",
      "Epoch 709/30000 Training Loss: 0.09945736825466156\n",
      "Epoch 710/30000 Training Loss: 0.09929249435663223\n",
      "Epoch 711/30000 Training Loss: 0.10569421947002411\n",
      "Epoch 712/30000 Training Loss: 0.10647282749414444\n",
      "Epoch 713/30000 Training Loss: 0.08888636529445648\n",
      "Epoch 714/30000 Training Loss: 0.10480977594852448\n",
      "Epoch 715/30000 Training Loss: 0.10190501064062119\n",
      "Epoch 716/30000 Training Loss: 0.11177080869674683\n",
      "Epoch 717/30000 Training Loss: 0.10113836824893951\n",
      "Epoch 718/30000 Training Loss: 0.10531516373157501\n",
      "Epoch 719/30000 Training Loss: 0.10123642534017563\n",
      "Epoch 720/30000 Training Loss: 0.10133949667215347\n",
      "Epoch 721/30000 Training Loss: 0.10811305046081543\n",
      "Epoch 722/30000 Training Loss: 0.10640467703342438\n",
      "Epoch 723/30000 Training Loss: 0.10123404115438461\n",
      "Epoch 724/30000 Training Loss: 0.10030881315469742\n",
      "Epoch 725/30000 Training Loss: 0.10275924205780029\n",
      "Epoch 726/30000 Training Loss: 0.09707178175449371\n",
      "Epoch 727/30000 Training Loss: 0.10098378360271454\n",
      "Epoch 728/30000 Training Loss: 0.10323518514633179\n",
      "Epoch 729/30000 Training Loss: 0.10327617079019547\n",
      "Epoch 730/30000 Training Loss: 0.09039876610040665\n",
      "Epoch 731/30000 Training Loss: 0.10274698585271835\n",
      "Epoch 732/30000 Training Loss: 0.10011433064937592\n",
      "Epoch 733/30000 Training Loss: 0.10242809355258942\n",
      "Epoch 734/30000 Training Loss: 0.09923188388347626\n",
      "Epoch 735/30000 Training Loss: 0.10216250270605087\n",
      "Epoch 736/30000 Training Loss: 0.09451927989721298\n",
      "Epoch 737/30000 Training Loss: 0.10325924307107925\n",
      "Epoch 738/30000 Training Loss: 0.09498646855354309\n",
      "Epoch 739/30000 Training Loss: 0.1001192107796669\n",
      "Epoch 740/30000 Training Loss: 0.09486605226993561\n",
      "Epoch 741/30000 Training Loss: 0.10190035402774811\n",
      "Epoch 742/30000 Training Loss: 0.10838917642831802\n",
      "Epoch 743/30000 Training Loss: 0.0942341759800911\n",
      "Epoch 744/30000 Training Loss: 0.11580970138311386\n",
      "Epoch 745/30000 Training Loss: 0.09710335731506348\n",
      "Epoch 746/30000 Training Loss: 0.09937945753335953\n",
      "Epoch 747/30000 Training Loss: 0.09714238345623016\n",
      "Epoch 748/30000 Training Loss: 0.09918587654829025\n",
      "Epoch 749/30000 Training Loss: 0.09410922229290009\n",
      "Epoch 750/30000 Training Loss: 0.09557336568832397\n",
      "Epoch 750/30000 Validation Loss: 0.08431411534547806\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08431411534547806<=============\n",
      "Epoch 751/30000 Training Loss: 0.10819079726934433\n",
      "Epoch 752/30000 Training Loss: 0.09843875467777252\n",
      "Epoch 753/30000 Training Loss: 0.09434932470321655\n",
      "Epoch 754/30000 Training Loss: 0.1075931191444397\n",
      "Epoch 755/30000 Training Loss: 0.10114268213510513\n",
      "Epoch 756/30000 Training Loss: 0.10051605850458145\n",
      "Epoch 757/30000 Training Loss: 0.09436772018671036\n",
      "Epoch 758/30000 Training Loss: 0.09570477902889252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 759/30000 Training Loss: 0.11254282295703888\n",
      "Epoch 760/30000 Training Loss: 0.09853476285934448\n",
      "Epoch 761/30000 Training Loss: 0.08786597102880478\n",
      "Epoch 762/30000 Training Loss: 0.09458552300930023\n",
      "Epoch 763/30000 Training Loss: 0.09541066735982895\n",
      "Epoch 764/30000 Training Loss: 0.09340987354516983\n",
      "Epoch 765/30000 Training Loss: 0.10283813625574112\n",
      "Epoch 766/30000 Training Loss: 0.10351119190454483\n",
      "Epoch 767/30000 Training Loss: 0.1005210429430008\n",
      "Epoch 768/30000 Training Loss: 0.09409017860889435\n",
      "Epoch 769/30000 Training Loss: 0.10489729791879654\n",
      "Epoch 770/30000 Training Loss: 0.10179342329502106\n",
      "Epoch 771/30000 Training Loss: 0.10702606290578842\n",
      "Epoch 772/30000 Training Loss: 0.09718921035528183\n",
      "Epoch 773/30000 Training Loss: 0.09090950340032578\n",
      "Epoch 774/30000 Training Loss: 0.10280285030603409\n",
      "Epoch 775/30000 Training Loss: 0.10033415257930756\n",
      "Epoch 776/30000 Training Loss: 0.09952353686094284\n",
      "Epoch 777/30000 Training Loss: 0.09054981172084808\n",
      "Epoch 778/30000 Training Loss: 0.09929841756820679\n",
      "Epoch 779/30000 Training Loss: 0.10229973495006561\n",
      "Epoch 780/30000 Training Loss: 0.10027278959751129\n",
      "Epoch 781/30000 Training Loss: 0.09005539864301682\n",
      "Epoch 782/30000 Training Loss: 0.09145011752843857\n",
      "Epoch 783/30000 Training Loss: 0.11071906983852386\n",
      "Epoch 784/30000 Training Loss: 0.08864419162273407\n",
      "Epoch 785/30000 Training Loss: 0.09061184525489807\n",
      "Epoch 786/30000 Training Loss: 0.09599555283784866\n",
      "Epoch 787/30000 Training Loss: 0.09664039313793182\n",
      "Epoch 788/30000 Training Loss: 0.1030779629945755\n",
      "Epoch 789/30000 Training Loss: 0.09481398016214371\n",
      "Epoch 790/30000 Training Loss: 0.09675540775060654\n",
      "Epoch 791/30000 Training Loss: 0.0904744416475296\n",
      "Epoch 792/30000 Training Loss: 0.08418583124876022\n",
      "Epoch 793/30000 Training Loss: 0.08759386837482452\n",
      "Epoch 794/30000 Training Loss: 0.10469067096710205\n",
      "Epoch 795/30000 Training Loss: 0.09498672932386398\n",
      "Epoch 796/30000 Training Loss: 0.09713394939899445\n",
      "Epoch 797/30000 Training Loss: 0.09261208027601242\n",
      "Epoch 798/30000 Training Loss: 0.10191240161657333\n",
      "Epoch 799/30000 Training Loss: 0.10443095862865448\n",
      "Epoch 800/30000 Training Loss: 0.09529779106378555\n",
      "Epoch 800/30000 Validation Loss: 0.1010996475815773\n",
      "Epoch 801/30000 Training Loss: 0.08674883842468262\n",
      "Epoch 802/30000 Training Loss: 0.09518055617809296\n",
      "Epoch 803/30000 Training Loss: 0.0901770144701004\n",
      "Epoch 804/30000 Training Loss: 0.08985136449337006\n",
      "Epoch 805/30000 Training Loss: 0.09968624264001846\n",
      "Epoch 806/30000 Training Loss: 0.09700538218021393\n",
      "Epoch 807/30000 Training Loss: 0.09040144830942154\n",
      "Epoch 808/30000 Training Loss: 0.09934688359498978\n",
      "Epoch 809/30000 Training Loss: 0.08856834471225739\n",
      "Epoch 810/30000 Training Loss: 0.09840482473373413\n",
      "Epoch 811/30000 Training Loss: 0.08945436775684357\n",
      "Epoch 812/30000 Training Loss: 0.0969557911157608\n",
      "Epoch 813/30000 Training Loss: 0.09469413012266159\n",
      "Epoch 814/30000 Training Loss: 0.08875356614589691\n",
      "Epoch 815/30000 Training Loss: 0.0928015261888504\n",
      "Epoch 816/30000 Training Loss: 0.0872323289513588\n",
      "Epoch 817/30000 Training Loss: 0.09892676770687103\n",
      "Epoch 818/30000 Training Loss: 0.09647117555141449\n",
      "Epoch 819/30000 Training Loss: 0.10423461347818375\n",
      "Epoch 820/30000 Training Loss: 0.10450446605682373\n",
      "Epoch 821/30000 Training Loss: 0.09248790889978409\n",
      "Epoch 822/30000 Training Loss: 0.1012222170829773\n",
      "Epoch 823/30000 Training Loss: 0.09689612686634064\n",
      "Epoch 824/30000 Training Loss: 0.0878380760550499\n",
      "Epoch 825/30000 Training Loss: 0.08736687153577805\n",
      "Epoch 826/30000 Training Loss: 0.08926931768655777\n",
      "Epoch 827/30000 Training Loss: 0.0925682857632637\n",
      "Epoch 828/30000 Training Loss: 0.09174774587154388\n",
      "Epoch 829/30000 Training Loss: 0.10140208154916763\n",
      "Epoch 830/30000 Training Loss: 0.09402825683355331\n",
      "Epoch 831/30000 Training Loss: 0.10814917087554932\n",
      "Epoch 832/30000 Training Loss: 0.08335056900978088\n",
      "Epoch 833/30000 Training Loss: 0.10000602155923843\n",
      "Epoch 834/30000 Training Loss: 0.09372355043888092\n",
      "Epoch 835/30000 Training Loss: 0.09228047728538513\n",
      "Epoch 836/30000 Training Loss: 0.0922599583864212\n",
      "Epoch 837/30000 Training Loss: 0.0884425938129425\n",
      "Epoch 838/30000 Training Loss: 0.08557377755641937\n",
      "Epoch 839/30000 Training Loss: 0.09022307395935059\n",
      "Epoch 840/30000 Training Loss: 0.08698032051324844\n",
      "Epoch 841/30000 Training Loss: 0.08444060385227203\n",
      "Epoch 842/30000 Training Loss: 0.09118246287107468\n",
      "Epoch 843/30000 Training Loss: 0.09592509269714355\n",
      "Epoch 844/30000 Training Loss: 0.08539356291294098\n",
      "Epoch 845/30000 Training Loss: 0.0956415981054306\n",
      "Epoch 846/30000 Training Loss: 0.08565884828567505\n",
      "Epoch 847/30000 Training Loss: 0.0874050185084343\n",
      "Epoch 848/30000 Training Loss: 0.08015845715999603\n",
      "Epoch 849/30000 Training Loss: 0.08426688611507416\n",
      "Epoch 850/30000 Training Loss: 0.08951541781425476\n",
      "Epoch 850/30000 Validation Loss: 0.1028963103890419\n",
      "Epoch 851/30000 Training Loss: 0.092677541077137\n",
      "Epoch 852/30000 Training Loss: 0.0935734286904335\n",
      "Epoch 853/30000 Training Loss: 0.08738869428634644\n",
      "Epoch 854/30000 Training Loss: 0.09347613155841827\n",
      "Epoch 855/30000 Training Loss: 0.09329626709222794\n",
      "Epoch 856/30000 Training Loss: 0.09201838076114655\n",
      "Epoch 857/30000 Training Loss: 0.09726589918136597\n",
      "Epoch 858/30000 Training Loss: 0.09733711928129196\n",
      "Epoch 859/30000 Training Loss: 0.09648924320936203\n",
      "Epoch 860/30000 Training Loss: 0.08928199857473373\n",
      "Epoch 861/30000 Training Loss: 0.08797700703144073\n",
      "Epoch 862/30000 Training Loss: 0.09337158501148224\n",
      "Epoch 863/30000 Training Loss: 0.08849380910396576\n",
      "Epoch 864/30000 Training Loss: 0.08754290640354156\n",
      "Epoch 865/30000 Training Loss: 0.08740749210119247\n",
      "Epoch 866/30000 Training Loss: 0.09508516639471054\n",
      "Epoch 867/30000 Training Loss: 0.08500019460916519\n",
      "Epoch 868/30000 Training Loss: 0.09244532883167267\n",
      "Epoch 869/30000 Training Loss: 0.09599242359399796\n",
      "Epoch 870/30000 Training Loss: 0.0918320044875145\n",
      "Epoch 871/30000 Training Loss: 0.08086666464805603\n",
      "Epoch 872/30000 Training Loss: 0.08795337378978729\n",
      "Epoch 873/30000 Training Loss: 0.08275064080953598\n",
      "Epoch 874/30000 Training Loss: 0.08619439601898193\n",
      "Epoch 875/30000 Training Loss: 0.095974400639534\n",
      "Epoch 876/30000 Training Loss: 0.09504014253616333\n",
      "Epoch 877/30000 Training Loss: 0.10155216604471207\n",
      "Epoch 878/30000 Training Loss: 0.07709701359272003\n",
      "Epoch 879/30000 Training Loss: 0.0904197245836258\n",
      "Epoch 880/30000 Training Loss: 0.08922149240970612\n",
      "Epoch 881/30000 Training Loss: 0.0976174995303154\n",
      "Epoch 882/30000 Training Loss: 0.09554232656955719\n",
      "Epoch 883/30000 Training Loss: 0.09090659767389297\n",
      "Epoch 884/30000 Training Loss: 0.0857895016670227\n",
      "Epoch 885/30000 Training Loss: 0.08331100642681122\n",
      "Epoch 886/30000 Training Loss: 0.08160555362701416\n",
      "Epoch 887/30000 Training Loss: 0.08687461912631989\n",
      "Epoch 888/30000 Training Loss: 0.08986730873584747\n",
      "Epoch 889/30000 Training Loss: 0.09506833553314209\n",
      "Epoch 890/30000 Training Loss: 0.096010223031044\n",
      "Epoch 891/30000 Training Loss: 0.09630364924669266\n",
      "Epoch 892/30000 Training Loss: 0.08810745179653168\n",
      "Epoch 893/30000 Training Loss: 0.088374562561512\n",
      "Epoch 894/30000 Training Loss: 0.09320154041051865\n",
      "Epoch 895/30000 Training Loss: 0.10454535484313965\n",
      "Epoch 896/30000 Training Loss: 0.09125373512506485\n",
      "Epoch 897/30000 Training Loss: 0.08987382054328918\n",
      "Epoch 898/30000 Training Loss: 0.08125953376293182\n",
      "Epoch 899/30000 Training Loss: 0.08958911150693893\n",
      "Epoch 900/30000 Training Loss: 0.08621605485677719\n",
      "Epoch 900/30000 Validation Loss: 0.093803271651268\n",
      "Epoch 901/30000 Training Loss: 0.09007623046636581\n",
      "Epoch 902/30000 Training Loss: 0.08579706400632858\n",
      "Epoch 903/30000 Training Loss: 0.07599594444036484\n",
      "Epoch 904/30000 Training Loss: 0.09061667323112488\n",
      "Epoch 905/30000 Training Loss: 0.08798424899578094\n",
      "Epoch 906/30000 Training Loss: 0.09189359843730927\n",
      "Epoch 907/30000 Training Loss: 0.08928250521421432\n",
      "Epoch 908/30000 Training Loss: 0.08135583996772766\n",
      "Epoch 909/30000 Training Loss: 0.09880578517913818\n",
      "Epoch 910/30000 Training Loss: 0.08441749215126038\n",
      "Epoch 911/30000 Training Loss: 0.08354519307613373\n",
      "Epoch 912/30000 Training Loss: 0.09542618691921234\n",
      "Epoch 913/30000 Training Loss: 0.08295916020870209\n",
      "Epoch 914/30000 Training Loss: 0.08616699278354645\n",
      "Epoch 915/30000 Training Loss: 0.09116332232952118\n",
      "Epoch 916/30000 Training Loss: 0.09227072447538376\n",
      "Epoch 917/30000 Training Loss: 0.09289956837892532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 918/30000 Training Loss: 0.0984969511628151\n",
      "Epoch 919/30000 Training Loss: 0.08846308290958405\n",
      "Epoch 920/30000 Training Loss: 0.1000610962510109\n",
      "Epoch 921/30000 Training Loss: 0.08198435604572296\n",
      "Epoch 922/30000 Training Loss: 0.09384359419345856\n",
      "Epoch 923/30000 Training Loss: 0.08413806557655334\n",
      "Epoch 924/30000 Training Loss: 0.0882934182882309\n",
      "Epoch 925/30000 Training Loss: 0.08364789187908173\n",
      "Epoch 926/30000 Training Loss: 0.08999829739332199\n",
      "Epoch 927/30000 Training Loss: 0.09429404139518738\n",
      "Epoch 928/30000 Training Loss: 0.08267869055271149\n",
      "Epoch 929/30000 Training Loss: 0.09012826532125473\n",
      "Epoch 930/30000 Training Loss: 0.0839642882347107\n",
      "Epoch 931/30000 Training Loss: 0.08679822087287903\n",
      "Epoch 932/30000 Training Loss: 0.09132535755634308\n",
      "Epoch 933/30000 Training Loss: 0.08175256103277206\n",
      "Epoch 934/30000 Training Loss: 0.08638392388820648\n",
      "Epoch 935/30000 Training Loss: 0.09346888959407806\n",
      "Epoch 936/30000 Training Loss: 0.09310422837734222\n",
      "Epoch 937/30000 Training Loss: 0.09607166051864624\n",
      "Epoch 938/30000 Training Loss: 0.09245733916759491\n",
      "Epoch 939/30000 Training Loss: 0.0898505225777626\n",
      "Epoch 940/30000 Training Loss: 0.08136925846338272\n",
      "Epoch 941/30000 Training Loss: 0.09220577031373978\n",
      "Epoch 942/30000 Training Loss: 0.08183939754962921\n",
      "Epoch 943/30000 Training Loss: 0.0850459560751915\n",
      "Epoch 944/30000 Training Loss: 0.09155953675508499\n",
      "Epoch 945/30000 Training Loss: 0.08920513093471527\n",
      "Epoch 946/30000 Training Loss: 0.08345705270767212\n",
      "Epoch 947/30000 Training Loss: 0.08803115040063858\n",
      "Epoch 948/30000 Training Loss: 0.08914677053689957\n",
      "Epoch 949/30000 Training Loss: 0.08130063861608505\n",
      "Epoch 950/30000 Training Loss: 0.08280088007450104\n",
      "Epoch 950/30000 Validation Loss: 0.08904628455638885\n",
      "Epoch 951/30000 Training Loss: 0.08681897819042206\n",
      "Epoch 952/30000 Training Loss: 0.08603210747241974\n",
      "Epoch 953/30000 Training Loss: 0.08444584906101227\n",
      "Epoch 954/30000 Training Loss: 0.08710312098264694\n",
      "Epoch 955/30000 Training Loss: 0.0999145358800888\n",
      "Epoch 956/30000 Training Loss: 0.09071081131696701\n",
      "Epoch 957/30000 Training Loss: 0.09125878661870956\n",
      "Epoch 958/30000 Training Loss: 0.09207521378993988\n",
      "Epoch 959/30000 Training Loss: 0.08254984766244888\n",
      "Epoch 960/30000 Training Loss: 0.08986504375934601\n",
      "Epoch 961/30000 Training Loss: 0.08674606680870056\n",
      "Epoch 962/30000 Training Loss: 0.07964445650577545\n",
      "Epoch 963/30000 Training Loss: 0.08138434588909149\n",
      "Epoch 964/30000 Training Loss: 0.08819974213838577\n",
      "Epoch 965/30000 Training Loss: 0.08624088764190674\n",
      "Epoch 966/30000 Training Loss: 0.0896640345454216\n",
      "Epoch 967/30000 Training Loss: 0.09123750030994415\n",
      "Epoch 968/30000 Training Loss: 0.08160676807165146\n",
      "Epoch 969/30000 Training Loss: 0.09114044159650803\n",
      "Epoch 970/30000 Training Loss: 0.08460481464862823\n",
      "Epoch 971/30000 Training Loss: 0.08764459937810898\n",
      "Epoch 972/30000 Training Loss: 0.07780801504850388\n",
      "Epoch 973/30000 Training Loss: 0.0900406464934349\n",
      "Epoch 974/30000 Training Loss: 0.07899552583694458\n",
      "Epoch 975/30000 Training Loss: 0.08248014748096466\n",
      "Epoch 976/30000 Training Loss: 0.08097194135189056\n",
      "Epoch 977/30000 Training Loss: 0.08389320224523544\n",
      "Epoch 978/30000 Training Loss: 0.07512695342302322\n",
      "Epoch 979/30000 Training Loss: 0.07813198864459991\n",
      "Epoch 980/30000 Training Loss: 0.08286307752132416\n",
      "Epoch 981/30000 Training Loss: 0.0830482691526413\n",
      "Epoch 982/30000 Training Loss: 0.07881936430931091\n",
      "Epoch 983/30000 Training Loss: 0.08898509293794632\n",
      "Epoch 984/30000 Training Loss: 0.09216297417879105\n",
      "Epoch 985/30000 Training Loss: 0.09421245753765106\n",
      "Epoch 986/30000 Training Loss: 0.08754413574934006\n",
      "Epoch 987/30000 Training Loss: 0.0792810469865799\n",
      "Epoch 988/30000 Training Loss: 0.09030614793300629\n",
      "Epoch 989/30000 Training Loss: 0.08214618265628815\n",
      "Epoch 990/30000 Training Loss: 0.08479271829128265\n",
      "Epoch 991/30000 Training Loss: 0.08995983749628067\n",
      "Epoch 992/30000 Training Loss: 0.0851583406329155\n",
      "Epoch 993/30000 Training Loss: 0.07421804964542389\n",
      "Epoch 994/30000 Training Loss: 0.07916797697544098\n",
      "Epoch 995/30000 Training Loss: 0.08767791837453842\n",
      "Epoch 996/30000 Training Loss: 0.08214892446994781\n",
      "Epoch 997/30000 Training Loss: 0.0846695601940155\n",
      "Epoch 998/30000 Training Loss: 0.07641196995973587\n",
      "Epoch 999/30000 Training Loss: 0.08004777133464813\n",
      "Epoch 1000/30000 Training Loss: 0.0832531601190567\n",
      "Epoch 1000/30000 Validation Loss: 0.07358544319868088\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07358544319868088<=============\n",
      "Epoch 1001/30000 Training Loss: 0.07545141130685806\n",
      "Epoch 1002/30000 Training Loss: 0.08852079510688782\n",
      "Epoch 1003/30000 Training Loss: 0.08677005767822266\n",
      "Epoch 1004/30000 Training Loss: 0.08000032603740692\n",
      "Epoch 1005/30000 Training Loss: 0.08951805531978607\n",
      "Epoch 1006/30000 Training Loss: 0.08751742541790009\n",
      "Epoch 1007/30000 Training Loss: 0.08412375301122665\n",
      "Epoch 1008/30000 Training Loss: 0.07906118035316467\n",
      "Epoch 1009/30000 Training Loss: 0.0981055349111557\n",
      "Epoch 1010/30000 Training Loss: 0.08667952567338943\n",
      "Epoch 1011/30000 Training Loss: 0.08406039327383041\n",
      "Epoch 1012/30000 Training Loss: 0.07853896915912628\n",
      "Epoch 1013/30000 Training Loss: 0.07877682894468307\n",
      "Epoch 1014/30000 Training Loss: 0.09692201018333435\n",
      "Epoch 1015/30000 Training Loss: 0.08764617145061493\n",
      "Epoch 1016/30000 Training Loss: 0.07440080493688583\n",
      "Epoch 1017/30000 Training Loss: 0.08148034662008286\n",
      "Epoch 1018/30000 Training Loss: 0.09281421452760696\n",
      "Epoch 1019/30000 Training Loss: 0.09972043335437775\n",
      "Epoch 1020/30000 Training Loss: 0.07927676290273666\n",
      "Epoch 1021/30000 Training Loss: 0.09228025376796722\n",
      "Epoch 1022/30000 Training Loss: 0.0780424177646637\n",
      "Epoch 1023/30000 Training Loss: 0.08777986466884613\n",
      "Epoch 1024/30000 Training Loss: 0.07836158573627472\n",
      "Epoch 1025/30000 Training Loss: 0.08579274266958237\n",
      "Epoch 1026/30000 Training Loss: 0.08164814114570618\n",
      "Epoch 1027/30000 Training Loss: 0.07898452132940292\n",
      "Epoch 1028/30000 Training Loss: 0.07942749559879303\n",
      "Epoch 1029/30000 Training Loss: 0.08806052803993225\n",
      "Epoch 1030/30000 Training Loss: 0.0788828507065773\n",
      "Epoch 1031/30000 Training Loss: 0.09749206900596619\n",
      "Epoch 1032/30000 Training Loss: 0.08007045835256577\n",
      "Epoch 1033/30000 Training Loss: 0.07532420009374619\n",
      "Epoch 1034/30000 Training Loss: 0.0781097263097763\n",
      "Epoch 1035/30000 Training Loss: 0.09583927690982819\n",
      "Epoch 1036/30000 Training Loss: 0.07818127423524857\n",
      "Epoch 1037/30000 Training Loss: 0.08384132385253906\n",
      "Epoch 1038/30000 Training Loss: 0.08147375285625458\n",
      "Epoch 1039/30000 Training Loss: 0.09081406146287918\n",
      "Epoch 1040/30000 Training Loss: 0.09266574680805206\n",
      "Epoch 1041/30000 Training Loss: 0.0781683698296547\n",
      "Epoch 1042/30000 Training Loss: 0.08456999063491821\n",
      "Epoch 1043/30000 Training Loss: 0.08142845332622528\n",
      "Epoch 1044/30000 Training Loss: 0.10611404478549957\n",
      "Epoch 1045/30000 Training Loss: 0.08386329561471939\n",
      "Epoch 1046/30000 Training Loss: 0.07596845924854279\n",
      "Epoch 1047/30000 Training Loss: 0.08870486170053482\n",
      "Epoch 1048/30000 Training Loss: 0.08009041845798492\n",
      "Epoch 1049/30000 Training Loss: 0.07780759036540985\n",
      "Epoch 1050/30000 Training Loss: 0.08103905618190765\n",
      "Epoch 1050/30000 Validation Loss: 0.0905873253941536\n",
      "Epoch 1051/30000 Training Loss: 0.07748601585626602\n",
      "Epoch 1052/30000 Training Loss: 0.07928801327943802\n",
      "Epoch 1053/30000 Training Loss: 0.07920105755329132\n",
      "Epoch 1054/30000 Training Loss: 0.08553352952003479\n",
      "Epoch 1055/30000 Training Loss: 0.09191431105136871\n",
      "Epoch 1056/30000 Training Loss: 0.0840456560254097\n",
      "Epoch 1057/30000 Training Loss: 0.09159721434116364\n",
      "Epoch 1058/30000 Training Loss: 0.07865111529827118\n",
      "Epoch 1059/30000 Training Loss: 0.08692879974842072\n",
      "Epoch 1060/30000 Training Loss: 0.08272315561771393\n",
      "Epoch 1061/30000 Training Loss: 0.07399190962314606\n",
      "Epoch 1062/30000 Training Loss: 0.08130814880132675\n",
      "Epoch 1063/30000 Training Loss: 0.07090757042169571\n",
      "Epoch 1064/30000 Training Loss: 0.08060240745544434\n",
      "Epoch 1065/30000 Training Loss: 0.08359383046627045\n",
      "Epoch 1066/30000 Training Loss: 0.08099877834320068\n",
      "Epoch 1067/30000 Training Loss: 0.07845120131969452\n",
      "Epoch 1068/30000 Training Loss: 0.0808061733841896\n",
      "Epoch 1069/30000 Training Loss: 0.0889836773276329\n",
      "Epoch 1070/30000 Training Loss: 0.08350290358066559\n",
      "Epoch 1071/30000 Training Loss: 0.08796985447406769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1072/30000 Training Loss: 0.08039028942584991\n",
      "Epoch 1073/30000 Training Loss: 0.07766737788915634\n",
      "Epoch 1074/30000 Training Loss: 0.07543067634105682\n",
      "Epoch 1075/30000 Training Loss: 0.08506213873624802\n",
      "Epoch 1076/30000 Training Loss: 0.08047163486480713\n",
      "Epoch 1077/30000 Training Loss: 0.07600422948598862\n",
      "Epoch 1078/30000 Training Loss: 0.08320235460996628\n",
      "Epoch 1079/30000 Training Loss: 0.07797905057668686\n",
      "Epoch 1080/30000 Training Loss: 0.07840367406606674\n",
      "Epoch 1081/30000 Training Loss: 0.08443634957075119\n",
      "Epoch 1082/30000 Training Loss: 0.0753430500626564\n",
      "Epoch 1083/30000 Training Loss: 0.07916125655174255\n",
      "Epoch 1084/30000 Training Loss: 0.08316414058208466\n",
      "Epoch 1085/30000 Training Loss: 0.08107425272464752\n",
      "Epoch 1086/30000 Training Loss: 0.08165208995342255\n",
      "Epoch 1087/30000 Training Loss: 0.08143540471792221\n",
      "Epoch 1088/30000 Training Loss: 0.0781368687748909\n",
      "Epoch 1089/30000 Training Loss: 0.07714055478572845\n",
      "Epoch 1090/30000 Training Loss: 0.07866394519805908\n",
      "Epoch 1091/30000 Training Loss: 0.08089089393615723\n",
      "Epoch 1092/30000 Training Loss: 0.07634872943162918\n",
      "Epoch 1093/30000 Training Loss: 0.08210442215204239\n",
      "Epoch 1094/30000 Training Loss: 0.09085214883089066\n",
      "Epoch 1095/30000 Training Loss: 0.08449047058820724\n",
      "Epoch 1096/30000 Training Loss: 0.08979514241218567\n",
      "Epoch 1097/30000 Training Loss: 0.08219971507787704\n",
      "Epoch 1098/30000 Training Loss: 0.08671523630619049\n",
      "Epoch 1099/30000 Training Loss: 0.08063963055610657\n",
      "Epoch 1100/30000 Training Loss: 0.07951895892620087\n",
      "Epoch 1100/30000 Validation Loss: 0.08864261209964752\n",
      "Epoch 1101/30000 Training Loss: 0.07105062901973724\n",
      "Epoch 1102/30000 Training Loss: 0.08611173927783966\n",
      "Epoch 1103/30000 Training Loss: 0.10112243890762329\n",
      "Epoch 1104/30000 Training Loss: 0.08035334199666977\n",
      "Epoch 1105/30000 Training Loss: 0.08167030662298203\n",
      "Epoch 1106/30000 Training Loss: 0.08594144880771637\n",
      "Epoch 1107/30000 Training Loss: 0.07526824623346329\n",
      "Epoch 1108/30000 Training Loss: 0.07275120913982391\n",
      "Epoch 1109/30000 Training Loss: 0.08170975744724274\n",
      "Epoch 1110/30000 Training Loss: 0.08831878006458282\n",
      "Epoch 1111/30000 Training Loss: 0.08654125034809113\n",
      "Epoch 1112/30000 Training Loss: 0.08732108026742935\n",
      "Epoch 1113/30000 Training Loss: 0.08315413445234299\n",
      "Epoch 1114/30000 Training Loss: 0.07714743912220001\n",
      "Epoch 1115/30000 Training Loss: 0.09121935814619064\n",
      "Epoch 1116/30000 Training Loss: 0.0850546658039093\n",
      "Epoch 1117/30000 Training Loss: 0.08069820702075958\n",
      "Epoch 1118/30000 Training Loss: 0.07722868770360947\n",
      "Epoch 1119/30000 Training Loss: 0.07880796492099762\n",
      "Epoch 1120/30000 Training Loss: 0.08463410288095474\n",
      "Epoch 1121/30000 Training Loss: 0.07741344720125198\n",
      "Epoch 1122/30000 Training Loss: 0.07804131507873535\n",
      "Epoch 1123/30000 Training Loss: 0.08216188848018646\n",
      "Epoch 1124/30000 Training Loss: 0.08853383362293243\n",
      "Epoch 1125/30000 Training Loss: 0.08886023610830307\n",
      "Epoch 1126/30000 Training Loss: 0.08484524488449097\n",
      "Epoch 1127/30000 Training Loss: 0.0789162814617157\n",
      "Epoch 1128/30000 Training Loss: 0.07606920599937439\n",
      "Epoch 1129/30000 Training Loss: 0.07820060849189758\n",
      "Epoch 1130/30000 Training Loss: 0.08162512630224228\n",
      "Epoch 1131/30000 Training Loss: 0.07630728930234909\n",
      "Epoch 1132/30000 Training Loss: 0.08512047678232193\n",
      "Epoch 1133/30000 Training Loss: 0.0784483402967453\n",
      "Epoch 1134/30000 Training Loss: 0.08181401342153549\n",
      "Epoch 1135/30000 Training Loss: 0.09800957143306732\n",
      "Epoch 1136/30000 Training Loss: 0.08024066686630249\n",
      "Epoch 1137/30000 Training Loss: 0.07416265457868576\n",
      "Epoch 1138/30000 Training Loss: 0.08014440536499023\n",
      "Epoch 1139/30000 Training Loss: 0.07848166674375534\n",
      "Epoch 1140/30000 Training Loss: 0.0767141580581665\n",
      "Epoch 1141/30000 Training Loss: 0.08520770072937012\n",
      "Epoch 1142/30000 Training Loss: 0.07850861549377441\n",
      "Epoch 1143/30000 Training Loss: 0.08718180656433105\n",
      "Epoch 1144/30000 Training Loss: 0.07452422380447388\n",
      "Epoch 1145/30000 Training Loss: 0.07613293826580048\n",
      "Epoch 1146/30000 Training Loss: 0.08355464786291122\n",
      "Epoch 1147/30000 Training Loss: 0.08486822992563248\n",
      "Epoch 1148/30000 Training Loss: 0.07270615547895432\n",
      "Epoch 1149/30000 Training Loss: 0.07591090351343155\n",
      "Epoch 1150/30000 Training Loss: 0.07632610201835632\n",
      "Epoch 1150/30000 Validation Loss: 0.08252733200788498\n",
      "Epoch 1151/30000 Training Loss: 0.06376464664936066\n",
      "Epoch 1152/30000 Training Loss: 0.08493509888648987\n",
      "Epoch 1153/30000 Training Loss: 0.0865044966340065\n",
      "Epoch 1154/30000 Training Loss: 0.0787782073020935\n",
      "Epoch 1155/30000 Training Loss: 0.07678691297769547\n",
      "Epoch 1156/30000 Training Loss: 0.07477061450481415\n",
      "Epoch 1157/30000 Training Loss: 0.08504656702280045\n",
      "Epoch 1158/30000 Training Loss: 0.08583163470029831\n",
      "Epoch 1159/30000 Training Loss: 0.07564588636159897\n",
      "Epoch 1160/30000 Training Loss: 0.07791011035442352\n",
      "Epoch 1161/30000 Training Loss: 0.0833263024687767\n",
      "Epoch 1162/30000 Training Loss: 0.07511796057224274\n",
      "Epoch 1163/30000 Training Loss: 0.07579527050256729\n",
      "Epoch 1164/30000 Training Loss: 0.07914629578590393\n",
      "Epoch 1165/30000 Training Loss: 0.08490441739559174\n",
      "Epoch 1166/30000 Training Loss: 0.08733346313238144\n",
      "Epoch 1167/30000 Training Loss: 0.085968516767025\n",
      "Epoch 1168/30000 Training Loss: 0.07285483181476593\n",
      "Epoch 1169/30000 Training Loss: 0.0913875624537468\n",
      "Epoch 1170/30000 Training Loss: 0.07592476159334183\n",
      "Epoch 1171/30000 Training Loss: 0.07817392796278\n",
      "Epoch 1172/30000 Training Loss: 0.07531028985977173\n",
      "Epoch 1173/30000 Training Loss: 0.07847751677036285\n",
      "Epoch 1174/30000 Training Loss: 0.07314522564411163\n",
      "Epoch 1175/30000 Training Loss: 0.07639367133378983\n",
      "Epoch 1176/30000 Training Loss: 0.072666235268116\n",
      "Epoch 1177/30000 Training Loss: 0.07044672966003418\n",
      "Epoch 1178/30000 Training Loss: 0.07595794647932053\n",
      "Epoch 1179/30000 Training Loss: 0.08234009146690369\n",
      "Epoch 1180/30000 Training Loss: 0.07764727622270584\n",
      "Epoch 1181/30000 Training Loss: 0.07187368720769882\n",
      "Epoch 1182/30000 Training Loss: 0.07873807847499847\n",
      "Epoch 1183/30000 Training Loss: 0.07357588410377502\n",
      "Epoch 1184/30000 Training Loss: 0.08324304968118668\n",
      "Epoch 1185/30000 Training Loss: 0.07322471588850021\n",
      "Epoch 1186/30000 Training Loss: 0.06841044127941132\n",
      "Epoch 1187/30000 Training Loss: 0.07265836000442505\n",
      "Epoch 1188/30000 Training Loss: 0.08786249905824661\n",
      "Epoch 1189/30000 Training Loss: 0.07802616059780121\n",
      "Epoch 1190/30000 Training Loss: 0.07892336696386337\n",
      "Epoch 1191/30000 Training Loss: 0.07883407920598984\n",
      "Epoch 1192/30000 Training Loss: 0.08629731088876724\n",
      "Epoch 1193/30000 Training Loss: 0.09038291871547699\n",
      "Epoch 1194/30000 Training Loss: 0.07563157379627228\n",
      "Epoch 1195/30000 Training Loss: 0.07417932152748108\n",
      "Epoch 1196/30000 Training Loss: 0.06762245297431946\n",
      "Epoch 1197/30000 Training Loss: 0.07836679369211197\n",
      "Epoch 1198/30000 Training Loss: 0.08502749353647232\n",
      "Epoch 1199/30000 Training Loss: 0.07054615020751953\n",
      "Epoch 1200/30000 Training Loss: 0.08197596669197083\n",
      "Epoch 1200/30000 Validation Loss: 0.07010847330093384\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07010847330093384<=============\n",
      "Epoch 1201/30000 Training Loss: 0.08420643210411072\n",
      "Epoch 1202/30000 Training Loss: 0.08905571699142456\n",
      "Epoch 1203/30000 Training Loss: 0.07627452909946442\n",
      "Epoch 1204/30000 Training Loss: 0.07144984602928162\n",
      "Epoch 1205/30000 Training Loss: 0.08292226493358612\n",
      "Epoch 1206/30000 Training Loss: 0.0820959210395813\n",
      "Epoch 1207/30000 Training Loss: 0.07084718346595764\n",
      "Epoch 1208/30000 Training Loss: 0.08130618184804916\n",
      "Epoch 1209/30000 Training Loss: 0.07775717973709106\n",
      "Epoch 1210/30000 Training Loss: 0.08948753029108047\n",
      "Epoch 1211/30000 Training Loss: 0.07283137738704681\n",
      "Epoch 1212/30000 Training Loss: 0.07798116654157639\n",
      "Epoch 1213/30000 Training Loss: 0.07799998670816422\n",
      "Epoch 1214/30000 Training Loss: 0.0706489235162735\n",
      "Epoch 1215/30000 Training Loss: 0.08062533289194107\n",
      "Epoch 1216/30000 Training Loss: 0.08050257712602615\n",
      "Epoch 1217/30000 Training Loss: 0.07741562277078629\n",
      "Epoch 1218/30000 Training Loss: 0.08047806471586227\n",
      "Epoch 1219/30000 Training Loss: 0.07830062508583069\n",
      "Epoch 1220/30000 Training Loss: 0.07281346619129181\n",
      "Epoch 1221/30000 Training Loss: 0.07619833946228027\n",
      "Epoch 1222/30000 Training Loss: 0.07846839725971222\n",
      "Epoch 1223/30000 Training Loss: 0.06779734790325165\n",
      "Epoch 1224/30000 Training Loss: 0.07337141782045364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1225/30000 Training Loss: 0.07325716316699982\n",
      "Epoch 1226/30000 Training Loss: 0.07705171406269073\n",
      "Epoch 1227/30000 Training Loss: 0.0760667696595192\n",
      "Epoch 1228/30000 Training Loss: 0.07186125963926315\n",
      "Epoch 1229/30000 Training Loss: 0.07470887899398804\n",
      "Epoch 1230/30000 Training Loss: 0.07402728497982025\n",
      "Epoch 1231/30000 Training Loss: 0.0770372748374939\n",
      "Epoch 1232/30000 Training Loss: 0.08939513564109802\n",
      "Epoch 1233/30000 Training Loss: 0.08232816308736801\n",
      "Epoch 1234/30000 Training Loss: 0.07981713116168976\n",
      "Epoch 1235/30000 Training Loss: 0.0880938172340393\n",
      "Epoch 1236/30000 Training Loss: 0.0761152058839798\n",
      "Epoch 1237/30000 Training Loss: 0.08711971342563629\n",
      "Epoch 1238/30000 Training Loss: 0.07929643243551254\n",
      "Epoch 1239/30000 Training Loss: 0.07868972420692444\n",
      "Epoch 1240/30000 Training Loss: 0.07292958348989487\n",
      "Epoch 1241/30000 Training Loss: 0.07468058168888092\n",
      "Epoch 1242/30000 Training Loss: 0.09456966072320938\n",
      "Epoch 1243/30000 Training Loss: 0.06970170140266418\n",
      "Epoch 1244/30000 Training Loss: 0.07299510389566422\n",
      "Epoch 1245/30000 Training Loss: 0.08019189536571503\n",
      "Epoch 1246/30000 Training Loss: 0.07839080691337585\n",
      "Epoch 1247/30000 Training Loss: 0.07486961781978607\n",
      "Epoch 1248/30000 Training Loss: 0.06704013049602509\n",
      "Epoch 1249/30000 Training Loss: 0.08562193810939789\n",
      "Epoch 1250/30000 Training Loss: 0.07544892281293869\n",
      "Epoch 1250/30000 Validation Loss: 0.07985974848270416\n",
      "Epoch 1251/30000 Training Loss: 0.07860370725393295\n",
      "Epoch 1252/30000 Training Loss: 0.0790337398648262\n",
      "Epoch 1253/30000 Training Loss: 0.0723569244146347\n",
      "Epoch 1254/30000 Training Loss: 0.07790521532297134\n",
      "Epoch 1255/30000 Training Loss: 0.07651147991418839\n",
      "Epoch 1256/30000 Training Loss: 0.07796604931354523\n",
      "Epoch 1257/30000 Training Loss: 0.07796329259872437\n",
      "Epoch 1258/30000 Training Loss: 0.08567877858877182\n",
      "Epoch 1259/30000 Training Loss: 0.07379721105098724\n",
      "Epoch 1260/30000 Training Loss: 0.07279973477125168\n",
      "Epoch 1261/30000 Training Loss: 0.07554417103528976\n",
      "Epoch 1262/30000 Training Loss: 0.08077344298362732\n",
      "Epoch 1263/30000 Training Loss: 0.07617314159870148\n",
      "Epoch 1264/30000 Training Loss: 0.07363946735858917\n",
      "Epoch 1265/30000 Training Loss: 0.07067713886499405\n",
      "Epoch 1266/30000 Training Loss: 0.0755503922700882\n",
      "Epoch 1267/30000 Training Loss: 0.07178689539432526\n",
      "Epoch 1268/30000 Training Loss: 0.07368650287389755\n",
      "Epoch 1269/30000 Training Loss: 0.08519480377435684\n",
      "Epoch 1270/30000 Training Loss: 0.07451645284891129\n",
      "Epoch 1271/30000 Training Loss: 0.08045513927936554\n",
      "Epoch 1272/30000 Training Loss: 0.07540885359048843\n",
      "Epoch 1273/30000 Training Loss: 0.07165409624576569\n",
      "Epoch 1274/30000 Training Loss: 0.07736828923225403\n",
      "Epoch 1275/30000 Training Loss: 0.07612629979848862\n",
      "Epoch 1276/30000 Training Loss: 0.0775149017572403\n",
      "Epoch 1277/30000 Training Loss: 0.08088420331478119\n",
      "Epoch 1278/30000 Training Loss: 0.06755685806274414\n",
      "Epoch 1279/30000 Training Loss: 0.07132022082805634\n",
      "Epoch 1280/30000 Training Loss: 0.07664279639720917\n",
      "Epoch 1281/30000 Training Loss: 0.0821203887462616\n",
      "Epoch 1282/30000 Training Loss: 0.07619601488113403\n",
      "Epoch 1283/30000 Training Loss: 0.07171577215194702\n",
      "Epoch 1284/30000 Training Loss: 0.07980208098888397\n",
      "Epoch 1285/30000 Training Loss: 0.08181639015674591\n",
      "Epoch 1286/30000 Training Loss: 0.08150577545166016\n",
      "Epoch 1287/30000 Training Loss: 0.07649491727352142\n",
      "Epoch 1288/30000 Training Loss: 0.07959925383329391\n",
      "Epoch 1289/30000 Training Loss: 0.08008937537670135\n",
      "Epoch 1290/30000 Training Loss: 0.0748855397105217\n",
      "Epoch 1291/30000 Training Loss: 0.08842479437589645\n",
      "Epoch 1292/30000 Training Loss: 0.08146314322948456\n",
      "Epoch 1293/30000 Training Loss: 0.08251579850912094\n",
      "Epoch 1294/30000 Training Loss: 0.06887821853160858\n",
      "Epoch 1295/30000 Training Loss: 0.08223076164722443\n",
      "Epoch 1296/30000 Training Loss: 0.07731132954359055\n",
      "Epoch 1297/30000 Training Loss: 0.07372190803289413\n",
      "Epoch 1298/30000 Training Loss: 0.0736524686217308\n",
      "Epoch 1299/30000 Training Loss: 0.0797378346323967\n",
      "Epoch 1300/30000 Training Loss: 0.07377590984106064\n",
      "Epoch 1300/30000 Validation Loss: 0.07440904527902603\n",
      "Epoch 1301/30000 Training Loss: 0.08936537057161331\n",
      "Epoch 1302/30000 Training Loss: 0.06923998892307281\n",
      "Epoch 1303/30000 Training Loss: 0.0777694433927536\n",
      "Epoch 1304/30000 Training Loss: 0.07996055483818054\n",
      "Epoch 1305/30000 Training Loss: 0.08175842463970184\n",
      "Epoch 1306/30000 Training Loss: 0.08971169590950012\n",
      "Epoch 1307/30000 Training Loss: 0.06469706445932388\n",
      "Epoch 1308/30000 Training Loss: 0.07467208057641983\n",
      "Epoch 1309/30000 Training Loss: 0.08308538049459457\n",
      "Epoch 1310/30000 Training Loss: 0.08112291246652603\n",
      "Epoch 1311/30000 Training Loss: 0.07178612053394318\n",
      "Epoch 1312/30000 Training Loss: 0.07897163927555084\n",
      "Epoch 1313/30000 Training Loss: 0.07329356670379639\n",
      "Epoch 1314/30000 Training Loss: 0.06921814382076263\n",
      "Epoch 1315/30000 Training Loss: 0.08004327118396759\n",
      "Epoch 1316/30000 Training Loss: 0.07299920171499252\n",
      "Epoch 1317/30000 Training Loss: 0.07023920118808746\n",
      "Epoch 1318/30000 Training Loss: 0.0732792466878891\n",
      "Epoch 1319/30000 Training Loss: 0.07835842669010162\n",
      "Epoch 1320/30000 Training Loss: 0.07422588020563126\n",
      "Epoch 1321/30000 Training Loss: 0.07139070332050323\n",
      "Epoch 1322/30000 Training Loss: 0.07627318799495697\n",
      "Epoch 1323/30000 Training Loss: 0.0771564319729805\n",
      "Epoch 1324/30000 Training Loss: 0.07626346498727798\n",
      "Epoch 1325/30000 Training Loss: 0.07872617244720459\n",
      "Epoch 1326/30000 Training Loss: 0.0744064524769783\n",
      "Epoch 1327/30000 Training Loss: 0.07229121029376984\n",
      "Epoch 1328/30000 Training Loss: 0.07933290302753448\n",
      "Epoch 1329/30000 Training Loss: 0.08138006180524826\n",
      "Epoch 1330/30000 Training Loss: 0.06806520372629166\n",
      "Epoch 1331/30000 Training Loss: 0.08038026094436646\n",
      "Epoch 1332/30000 Training Loss: 0.0714363381266594\n",
      "Epoch 1333/30000 Training Loss: 0.08016042411327362\n",
      "Epoch 1334/30000 Training Loss: 0.07141397893428802\n",
      "Epoch 1335/30000 Training Loss: 0.0647515207529068\n",
      "Epoch 1336/30000 Training Loss: 0.06173894926905632\n",
      "Epoch 1337/30000 Training Loss: 0.07821674644947052\n",
      "Epoch 1338/30000 Training Loss: 0.08554673939943314\n",
      "Epoch 1339/30000 Training Loss: 0.07520146667957306\n",
      "Epoch 1340/30000 Training Loss: 0.08865144103765488\n",
      "Epoch 1341/30000 Training Loss: 0.07602386176586151\n",
      "Epoch 1342/30000 Training Loss: 0.07866214215755463\n",
      "Epoch 1343/30000 Training Loss: 0.0710597112774849\n",
      "Epoch 1344/30000 Training Loss: 0.07596661895513535\n",
      "Epoch 1345/30000 Training Loss: 0.07257095724344254\n",
      "Epoch 1346/30000 Training Loss: 0.08790812641382217\n",
      "Epoch 1347/30000 Training Loss: 0.0751151517033577\n",
      "Epoch 1348/30000 Training Loss: 0.0748782604932785\n",
      "Epoch 1349/30000 Training Loss: 0.07741135358810425\n",
      "Epoch 1350/30000 Training Loss: 0.07693884521722794\n",
      "Epoch 1350/30000 Validation Loss: 0.06713807582855225\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06713807582855225<=============\n",
      "Epoch 1351/30000 Training Loss: 0.07047060132026672\n",
      "Epoch 1352/30000 Training Loss: 0.06957294046878815\n",
      "Epoch 1353/30000 Training Loss: 0.08686719089746475\n",
      "Epoch 1354/30000 Training Loss: 0.07674528658390045\n",
      "Epoch 1355/30000 Training Loss: 0.0736749917268753\n",
      "Epoch 1356/30000 Training Loss: 0.07562318444252014\n",
      "Epoch 1357/30000 Training Loss: 0.08268450945615768\n",
      "Epoch 1358/30000 Training Loss: 0.0585286021232605\n",
      "Epoch 1359/30000 Training Loss: 0.08299872279167175\n",
      "Epoch 1360/30000 Training Loss: 0.08753902465105057\n",
      "Epoch 1361/30000 Training Loss: 0.0719497799873352\n",
      "Epoch 1362/30000 Training Loss: 0.06546814739704132\n",
      "Epoch 1363/30000 Training Loss: 0.07566308975219727\n",
      "Epoch 1364/30000 Training Loss: 0.07011916488409042\n",
      "Epoch 1365/30000 Training Loss: 0.059338755905628204\n",
      "Epoch 1366/30000 Training Loss: 0.07520319521427155\n",
      "Epoch 1367/30000 Training Loss: 0.07513605058193207\n",
      "Epoch 1368/30000 Training Loss: 0.06853997707366943\n",
      "Epoch 1369/30000 Training Loss: 0.07112100720405579\n",
      "Epoch 1370/30000 Training Loss: 0.06733253598213196\n",
      "Epoch 1371/30000 Training Loss: 0.06969932466745377\n",
      "Epoch 1372/30000 Training Loss: 0.08259021490812302\n",
      "Epoch 1373/30000 Training Loss: 0.07517892122268677\n",
      "Epoch 1374/30000 Training Loss: 0.07847878336906433\n",
      "Epoch 1375/30000 Training Loss: 0.0726122036576271\n",
      "Epoch 1376/30000 Training Loss: 0.0726999044418335\n",
      "Epoch 1377/30000 Training Loss: 0.07765807956457138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1378/30000 Training Loss: 0.07541845738887787\n",
      "Epoch 1379/30000 Training Loss: 0.07036192715167999\n",
      "Epoch 1380/30000 Training Loss: 0.07927827537059784\n",
      "Epoch 1381/30000 Training Loss: 0.07303182780742645\n",
      "Epoch 1382/30000 Training Loss: 0.07777342945337296\n",
      "Epoch 1383/30000 Training Loss: 0.08350814878940582\n",
      "Epoch 1384/30000 Training Loss: 0.07420080155134201\n",
      "Epoch 1385/30000 Training Loss: 0.06380151212215424\n",
      "Epoch 1386/30000 Training Loss: 0.07582883536815643\n",
      "Epoch 1387/30000 Training Loss: 0.07864450663328171\n",
      "Epoch 1388/30000 Training Loss: 0.06983470171689987\n",
      "Epoch 1389/30000 Training Loss: 0.07256211340427399\n",
      "Epoch 1390/30000 Training Loss: 0.08679356426000595\n",
      "Epoch 1391/30000 Training Loss: 0.07374381273984909\n",
      "Epoch 1392/30000 Training Loss: 0.07713143527507782\n",
      "Epoch 1393/30000 Training Loss: 0.07931742072105408\n",
      "Epoch 1394/30000 Training Loss: 0.07564011216163635\n",
      "Epoch 1395/30000 Training Loss: 0.07468841969966888\n",
      "Epoch 1396/30000 Training Loss: 0.0796130821108818\n",
      "Epoch 1397/30000 Training Loss: 0.06505181640386581\n",
      "Epoch 1398/30000 Training Loss: 0.0766536220908165\n",
      "Epoch 1399/30000 Training Loss: 0.0731097012758255\n",
      "Epoch 1400/30000 Training Loss: 0.07465966045856476\n",
      "Epoch 1400/30000 Validation Loss: 0.06455834209918976\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06455834209918976<=============\n",
      "Epoch 1401/30000 Training Loss: 0.07644330710172653\n",
      "Epoch 1402/30000 Training Loss: 0.07301492244005203\n",
      "Epoch 1403/30000 Training Loss: 0.07549764215946198\n",
      "Epoch 1404/30000 Training Loss: 0.08565954118967056\n",
      "Epoch 1405/30000 Training Loss: 0.0848204642534256\n",
      "Epoch 1406/30000 Training Loss: 0.0733216255903244\n",
      "Epoch 1407/30000 Training Loss: 0.07568828761577606\n",
      "Epoch 1408/30000 Training Loss: 0.0799393504858017\n",
      "Epoch 1409/30000 Training Loss: 0.08231009542942047\n",
      "Epoch 1410/30000 Training Loss: 0.07469000667333603\n",
      "Epoch 1411/30000 Training Loss: 0.062027741223573685\n",
      "Epoch 1412/30000 Training Loss: 0.08679240196943283\n",
      "Epoch 1413/30000 Training Loss: 0.07078362256288528\n",
      "Epoch 1414/30000 Training Loss: 0.07876986265182495\n",
      "Epoch 1415/30000 Training Loss: 0.08177490532398224\n",
      "Epoch 1416/30000 Training Loss: 0.07875426858663559\n",
      "Epoch 1417/30000 Training Loss: 0.06706668436527252\n",
      "Epoch 1418/30000 Training Loss: 0.0734080970287323\n",
      "Epoch 1419/30000 Training Loss: 0.07599111646413803\n",
      "Epoch 1420/30000 Training Loss: 0.06992276012897491\n",
      "Epoch 1421/30000 Training Loss: 0.06308726221323013\n",
      "Epoch 1422/30000 Training Loss: 0.0793585553765297\n",
      "Epoch 1423/30000 Training Loss: 0.0683789998292923\n",
      "Epoch 1424/30000 Training Loss: 0.07860927283763885\n",
      "Epoch 1425/30000 Training Loss: 0.07145382463932037\n",
      "Epoch 1426/30000 Training Loss: 0.09146397560834885\n",
      "Epoch 1427/30000 Training Loss: 0.07347254455089569\n",
      "Epoch 1428/30000 Training Loss: 0.06986715644598007\n",
      "Epoch 1429/30000 Training Loss: 0.08425716310739517\n",
      "Epoch 1430/30000 Training Loss: 0.07229874283075333\n",
      "Epoch 1431/30000 Training Loss: 0.06964407116174698\n",
      "Epoch 1432/30000 Training Loss: 0.07732446491718292\n",
      "Epoch 1433/30000 Training Loss: 0.07218935340642929\n",
      "Epoch 1434/30000 Training Loss: 0.07375439256429672\n",
      "Epoch 1435/30000 Training Loss: 0.06964036822319031\n",
      "Epoch 1436/30000 Training Loss: 0.0842004343867302\n",
      "Epoch 1437/30000 Training Loss: 0.06316395848989487\n",
      "Epoch 1438/30000 Training Loss: 0.08554824441671371\n",
      "Epoch 1439/30000 Training Loss: 0.07790882885456085\n",
      "Epoch 1440/30000 Training Loss: 0.07734598964452744\n",
      "Epoch 1441/30000 Training Loss: 0.06989003717899323\n",
      "Epoch 1442/30000 Training Loss: 0.07369501888751984\n",
      "Epoch 1443/30000 Training Loss: 0.07371307164430618\n",
      "Epoch 1444/30000 Training Loss: 0.06292302906513214\n",
      "Epoch 1445/30000 Training Loss: 0.07132373005151749\n",
      "Epoch 1446/30000 Training Loss: 0.06812314689159393\n",
      "Epoch 1447/30000 Training Loss: 0.082986980676651\n",
      "Epoch 1448/30000 Training Loss: 0.06957265734672546\n",
      "Epoch 1449/30000 Training Loss: 0.07508145272731781\n",
      "Epoch 1450/30000 Training Loss: 0.07354404777288437\n",
      "Epoch 1450/30000 Validation Loss: 0.07844044268131256\n",
      "Epoch 1451/30000 Training Loss: 0.0661332979798317\n",
      "Epoch 1452/30000 Training Loss: 0.07253693044185638\n",
      "Epoch 1453/30000 Training Loss: 0.06919439136981964\n",
      "Epoch 1454/30000 Training Loss: 0.07104773819446564\n",
      "Epoch 1455/30000 Training Loss: 0.07312929630279541\n",
      "Epoch 1456/30000 Training Loss: 0.0817723348736763\n",
      "Epoch 1457/30000 Training Loss: 0.06868141889572144\n",
      "Epoch 1458/30000 Training Loss: 0.07661984115839005\n",
      "Epoch 1459/30000 Training Loss: 0.07616342604160309\n",
      "Epoch 1460/30000 Training Loss: 0.07463890314102173\n",
      "Epoch 1461/30000 Training Loss: 0.06671614944934845\n",
      "Epoch 1462/30000 Training Loss: 0.08623825013637543\n",
      "Epoch 1463/30000 Training Loss: 0.08495393395423889\n",
      "Epoch 1464/30000 Training Loss: 0.06577517092227936\n",
      "Epoch 1465/30000 Training Loss: 0.07877229899168015\n",
      "Epoch 1466/30000 Training Loss: 0.06386880576610565\n",
      "Epoch 1467/30000 Training Loss: 0.07122635841369629\n",
      "Epoch 1468/30000 Training Loss: 0.06818941980600357\n",
      "Epoch 1469/30000 Training Loss: 0.075803741812706\n",
      "Epoch 1470/30000 Training Loss: 0.07504512369632721\n",
      "Epoch 1471/30000 Training Loss: 0.07392348349094391\n",
      "Epoch 1472/30000 Training Loss: 0.07039845734834671\n",
      "Epoch 1473/30000 Training Loss: 0.07428361475467682\n",
      "Epoch 1474/30000 Training Loss: 0.0644620731472969\n",
      "Epoch 1475/30000 Training Loss: 0.0841907486319542\n",
      "Epoch 1476/30000 Training Loss: 0.08127696067094803\n",
      "Epoch 1477/30000 Training Loss: 0.0717301219701767\n",
      "Epoch 1478/30000 Training Loss: 0.07756651937961578\n",
      "Epoch 1479/30000 Training Loss: 0.07406313717365265\n",
      "Epoch 1480/30000 Training Loss: 0.06413720548152924\n",
      "Epoch 1481/30000 Training Loss: 0.07264444977045059\n",
      "Epoch 1482/30000 Training Loss: 0.08539498597383499\n",
      "Epoch 1483/30000 Training Loss: 0.08270508795976639\n",
      "Epoch 1484/30000 Training Loss: 0.07527418434619904\n",
      "Epoch 1485/30000 Training Loss: 0.06768260896205902\n",
      "Epoch 1486/30000 Training Loss: 0.0806722566485405\n",
      "Epoch 1487/30000 Training Loss: 0.08138792216777802\n",
      "Epoch 1488/30000 Training Loss: 0.07034271955490112\n",
      "Epoch 1489/30000 Training Loss: 0.07677488029003143\n",
      "Epoch 1490/30000 Training Loss: 0.07676135003566742\n",
      "Epoch 1491/30000 Training Loss: 0.06929204612970352\n",
      "Epoch 1492/30000 Training Loss: 0.07594049721956253\n",
      "Epoch 1493/30000 Training Loss: 0.06723680347204208\n",
      "Epoch 1494/30000 Training Loss: 0.06646494567394257\n",
      "Epoch 1495/30000 Training Loss: 0.07857748121023178\n",
      "Epoch 1496/30000 Training Loss: 0.0635492354631424\n",
      "Epoch 1497/30000 Training Loss: 0.06911194324493408\n",
      "Epoch 1498/30000 Training Loss: 0.0763474628329277\n",
      "Epoch 1499/30000 Training Loss: 0.08676290512084961\n",
      "Epoch 1500/30000 Training Loss: 0.06491641700267792\n",
      "Epoch 1500/30000 Validation Loss: 0.08253682404756546\n",
      "Epoch 1501/30000 Training Loss: 0.06472016870975494\n",
      "Epoch 1502/30000 Training Loss: 0.08282678574323654\n",
      "Epoch 1503/30000 Training Loss: 0.064980149269104\n",
      "Epoch 1504/30000 Training Loss: 0.08166660368442535\n",
      "Epoch 1505/30000 Training Loss: 0.07117287069559097\n",
      "Epoch 1506/30000 Training Loss: 0.07555448263883591\n",
      "Epoch 1507/30000 Training Loss: 0.0710582509636879\n",
      "Epoch 1508/30000 Training Loss: 0.07645325362682343\n",
      "Epoch 1509/30000 Training Loss: 0.061507564038038254\n",
      "Epoch 1510/30000 Training Loss: 0.07395828515291214\n",
      "Epoch 1511/30000 Training Loss: 0.07229061424732208\n",
      "Epoch 1512/30000 Training Loss: 0.07025204598903656\n",
      "Epoch 1513/30000 Training Loss: 0.06981030851602554\n",
      "Epoch 1514/30000 Training Loss: 0.07900962233543396\n",
      "Epoch 1515/30000 Training Loss: 0.07350018620491028\n",
      "Epoch 1516/30000 Training Loss: 0.06963279843330383\n",
      "Epoch 1517/30000 Training Loss: 0.06979162245988846\n",
      "Epoch 1518/30000 Training Loss: 0.07946648448705673\n",
      "Epoch 1519/30000 Training Loss: 0.07370730489492416\n",
      "Epoch 1520/30000 Training Loss: 0.07742885500192642\n",
      "Epoch 1521/30000 Training Loss: 0.08113142102956772\n",
      "Epoch 1522/30000 Training Loss: 0.07245604693889618\n",
      "Epoch 1523/30000 Training Loss: 0.07752934843301773\n",
      "Epoch 1524/30000 Training Loss: 0.06876356154680252\n",
      "Epoch 1525/30000 Training Loss: 0.07115919888019562\n",
      "Epoch 1526/30000 Training Loss: 0.06468764692544937\n",
      "Epoch 1527/30000 Training Loss: 0.06934063136577606\n",
      "Epoch 1528/30000 Training Loss: 0.06955580413341522\n",
      "Epoch 1529/30000 Training Loss: 0.09378083050251007\n",
      "Epoch 1530/30000 Training Loss: 0.0732497051358223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1531/30000 Training Loss: 0.0695827305316925\n",
      "Epoch 1532/30000 Training Loss: 0.057427167892456055\n",
      "Epoch 1533/30000 Training Loss: 0.07650472223758698\n",
      "Epoch 1534/30000 Training Loss: 0.07087092101573944\n",
      "Epoch 1535/30000 Training Loss: 0.0640459954738617\n",
      "Epoch 1536/30000 Training Loss: 0.07370338588953018\n",
      "Epoch 1537/30000 Training Loss: 0.07163430750370026\n",
      "Epoch 1538/30000 Training Loss: 0.07897092401981354\n",
      "Epoch 1539/30000 Training Loss: 0.08431743085384369\n",
      "Epoch 1540/30000 Training Loss: 0.07519251853227615\n",
      "Epoch 1541/30000 Training Loss: 0.07529051601886749\n",
      "Epoch 1542/30000 Training Loss: 0.073297880589962\n",
      "Epoch 1543/30000 Training Loss: 0.07820124924182892\n",
      "Epoch 1544/30000 Training Loss: 0.07054884731769562\n",
      "Epoch 1545/30000 Training Loss: 0.0714927688241005\n",
      "Epoch 1546/30000 Training Loss: 0.07124719768762589\n",
      "Epoch 1547/30000 Training Loss: 0.07742875814437866\n",
      "Epoch 1548/30000 Training Loss: 0.061711084097623825\n",
      "Epoch 1549/30000 Training Loss: 0.07715392112731934\n",
      "Epoch 1550/30000 Training Loss: 0.07095135748386383\n",
      "Epoch 1550/30000 Validation Loss: 0.0737711638212204\n",
      "Epoch 1551/30000 Training Loss: 0.06783696264028549\n",
      "Epoch 1552/30000 Training Loss: 0.07362411916255951\n",
      "Epoch 1553/30000 Training Loss: 0.07489640265703201\n",
      "Epoch 1554/30000 Training Loss: 0.07829857617616653\n",
      "Epoch 1555/30000 Training Loss: 0.07299508899450302\n",
      "Epoch 1556/30000 Training Loss: 0.06764481216669083\n",
      "Epoch 1557/30000 Training Loss: 0.0748017430305481\n",
      "Epoch 1558/30000 Training Loss: 0.08994382619857788\n",
      "Epoch 1559/30000 Training Loss: 0.06454643607139587\n",
      "Epoch 1560/30000 Training Loss: 0.06432686746120453\n",
      "Epoch 1561/30000 Training Loss: 0.06577128916978836\n",
      "Epoch 1562/30000 Training Loss: 0.0682450532913208\n",
      "Epoch 1563/30000 Training Loss: 0.07204844057559967\n",
      "Epoch 1564/30000 Training Loss: 0.06521245092153549\n",
      "Epoch 1565/30000 Training Loss: 0.08221958577632904\n",
      "Epoch 1566/30000 Training Loss: 0.0738472044467926\n",
      "Epoch 1567/30000 Training Loss: 0.08293809741735458\n",
      "Epoch 1568/30000 Training Loss: 0.0788571834564209\n",
      "Epoch 1569/30000 Training Loss: 0.06267731636762619\n",
      "Epoch 1570/30000 Training Loss: 0.081656314432621\n",
      "Epoch 1571/30000 Training Loss: 0.07710782438516617\n",
      "Epoch 1572/30000 Training Loss: 0.07304402440786362\n",
      "Epoch 1573/30000 Training Loss: 0.0775543749332428\n",
      "Epoch 1574/30000 Training Loss: 0.07696400582790375\n",
      "Epoch 1575/30000 Training Loss: 0.06528519839048386\n",
      "Epoch 1576/30000 Training Loss: 0.07484160363674164\n",
      "Epoch 1577/30000 Training Loss: 0.06475111097097397\n",
      "Epoch 1578/30000 Training Loss: 0.08133160322904587\n",
      "Epoch 1579/30000 Training Loss: 0.0773828849196434\n",
      "Epoch 1580/30000 Training Loss: 0.06730947643518448\n",
      "Epoch 1581/30000 Training Loss: 0.06621818244457245\n",
      "Epoch 1582/30000 Training Loss: 0.07207120209932327\n",
      "Epoch 1583/30000 Training Loss: 0.0790325179696083\n",
      "Epoch 1584/30000 Training Loss: 0.064662866294384\n",
      "Epoch 1585/30000 Training Loss: 0.07388724386692047\n",
      "Epoch 1586/30000 Training Loss: 0.06631727516651154\n",
      "Epoch 1587/30000 Training Loss: 0.07241706550121307\n",
      "Epoch 1588/30000 Training Loss: 0.061703313142061234\n",
      "Epoch 1589/30000 Training Loss: 0.0846528634428978\n",
      "Epoch 1590/30000 Training Loss: 0.0657273530960083\n",
      "Epoch 1591/30000 Training Loss: 0.06991727650165558\n",
      "Epoch 1592/30000 Training Loss: 0.066072478890419\n",
      "Epoch 1593/30000 Training Loss: 0.07474926859140396\n",
      "Epoch 1594/30000 Training Loss: 0.0744846984744072\n",
      "Epoch 1595/30000 Training Loss: 0.06367606669664383\n",
      "Epoch 1596/30000 Training Loss: 0.073811836540699\n",
      "Epoch 1597/30000 Training Loss: 0.06767823547124863\n",
      "Epoch 1598/30000 Training Loss: 0.06863025575876236\n",
      "Epoch 1599/30000 Training Loss: 0.07139421999454498\n",
      "Epoch 1600/30000 Training Loss: 0.08024013042449951\n",
      "Epoch 1600/30000 Validation Loss: 0.07081908732652664\n",
      "Epoch 1601/30000 Training Loss: 0.06854841113090515\n",
      "Epoch 1602/30000 Training Loss: 0.07134608179330826\n",
      "Epoch 1603/30000 Training Loss: 0.07936234027147293\n",
      "Epoch 1604/30000 Training Loss: 0.0637858659029007\n",
      "Epoch 1605/30000 Training Loss: 0.0693337470293045\n",
      "Epoch 1606/30000 Training Loss: 0.07452889531850815\n",
      "Epoch 1607/30000 Training Loss: 0.07370182871818542\n",
      "Epoch 1608/30000 Training Loss: 0.06744466722011566\n",
      "Epoch 1609/30000 Training Loss: 0.065574511885643\n",
      "Epoch 1610/30000 Training Loss: 0.07488987594842911\n",
      "Epoch 1611/30000 Training Loss: 0.060978371649980545\n",
      "Epoch 1612/30000 Training Loss: 0.07384471595287323\n",
      "Epoch 1613/30000 Training Loss: 0.07224874198436737\n",
      "Epoch 1614/30000 Training Loss: 0.06882624328136444\n",
      "Epoch 1615/30000 Training Loss: 0.07255977392196655\n",
      "Epoch 1616/30000 Training Loss: 0.0684528574347496\n",
      "Epoch 1617/30000 Training Loss: 0.06513436883687973\n",
      "Epoch 1618/30000 Training Loss: 0.07650645077228546\n",
      "Epoch 1619/30000 Training Loss: 0.07153910398483276\n",
      "Epoch 1620/30000 Training Loss: 0.0677453801035881\n",
      "Epoch 1621/30000 Training Loss: 0.06724879890680313\n",
      "Epoch 1622/30000 Training Loss: 0.07563869655132294\n",
      "Epoch 1623/30000 Training Loss: 0.06560282409191132\n",
      "Epoch 1624/30000 Training Loss: 0.08203988522291183\n",
      "Epoch 1625/30000 Training Loss: 0.07580943405628204\n",
      "Epoch 1626/30000 Training Loss: 0.06945198029279709\n",
      "Epoch 1627/30000 Training Loss: 0.06693247705698013\n",
      "Epoch 1628/30000 Training Loss: 0.07621299475431442\n",
      "Epoch 1629/30000 Training Loss: 0.07062654197216034\n",
      "Epoch 1630/30000 Training Loss: 0.07611075788736343\n",
      "Epoch 1631/30000 Training Loss: 0.07013474404811859\n",
      "Epoch 1632/30000 Training Loss: 0.0784875825047493\n",
      "Epoch 1633/30000 Training Loss: 0.07121744751930237\n",
      "Epoch 1634/30000 Training Loss: 0.07875974476337433\n",
      "Epoch 1635/30000 Training Loss: 0.07553601264953613\n",
      "Epoch 1636/30000 Training Loss: 0.07355201244354248\n",
      "Epoch 1637/30000 Training Loss: 0.06700752675533295\n",
      "Epoch 1638/30000 Training Loss: 0.08387823402881622\n",
      "Epoch 1639/30000 Training Loss: 0.07134077697992325\n",
      "Epoch 1640/30000 Training Loss: 0.07081488519906998\n",
      "Epoch 1641/30000 Training Loss: 0.06625420600175858\n",
      "Epoch 1642/30000 Training Loss: 0.07537909597158432\n",
      "Epoch 1643/30000 Training Loss: 0.06913028657436371\n",
      "Epoch 1644/30000 Training Loss: 0.07683591544628143\n",
      "Epoch 1645/30000 Training Loss: 0.06929397583007812\n",
      "Epoch 1646/30000 Training Loss: 0.0665823295712471\n",
      "Epoch 1647/30000 Training Loss: 0.07712302356958389\n",
      "Epoch 1648/30000 Training Loss: 0.07671517133712769\n",
      "Epoch 1649/30000 Training Loss: 0.060778308659791946\n",
      "Epoch 1650/30000 Training Loss: 0.06598030775785446\n",
      "Epoch 1650/30000 Validation Loss: 0.07014192640781403\n",
      "Epoch 1651/30000 Training Loss: 0.0760543942451477\n",
      "Epoch 1652/30000 Training Loss: 0.0733431726694107\n",
      "Epoch 1653/30000 Training Loss: 0.07448335736989975\n",
      "Epoch 1654/30000 Training Loss: 0.08326154202222824\n",
      "Epoch 1655/30000 Training Loss: 0.060439445078372955\n",
      "Epoch 1656/30000 Training Loss: 0.07457000762224197\n",
      "Epoch 1657/30000 Training Loss: 0.07447167485952377\n",
      "Epoch 1658/30000 Training Loss: 0.07363694161176682\n",
      "Epoch 1659/30000 Training Loss: 0.06063628941774368\n",
      "Epoch 1660/30000 Training Loss: 0.06791607290506363\n",
      "Epoch 1661/30000 Training Loss: 0.0785311684012413\n",
      "Epoch 1662/30000 Training Loss: 0.06082949787378311\n",
      "Epoch 1663/30000 Training Loss: 0.07347795367240906\n",
      "Epoch 1664/30000 Training Loss: 0.06388312578201294\n",
      "Epoch 1665/30000 Training Loss: 0.07015100866556168\n",
      "Epoch 1666/30000 Training Loss: 0.06628058105707169\n",
      "Epoch 1667/30000 Training Loss: 0.0672500878572464\n",
      "Epoch 1668/30000 Training Loss: 0.07078468799591064\n",
      "Epoch 1669/30000 Training Loss: 0.06436244398355484\n",
      "Epoch 1670/30000 Training Loss: 0.0739903673529625\n",
      "Epoch 1671/30000 Training Loss: 0.06617896258831024\n",
      "Epoch 1672/30000 Training Loss: 0.06093226745724678\n",
      "Epoch 1673/30000 Training Loss: 0.0716315284371376\n",
      "Epoch 1674/30000 Training Loss: 0.06891877949237823\n",
      "Epoch 1675/30000 Training Loss: 0.07192076742649078\n",
      "Epoch 1676/30000 Training Loss: 0.06901635229587555\n",
      "Epoch 1677/30000 Training Loss: 0.07177482545375824\n",
      "Epoch 1678/30000 Training Loss: 0.06752542406320572\n",
      "Epoch 1679/30000 Training Loss: 0.07185430824756622\n",
      "Epoch 1680/30000 Training Loss: 0.07047265768051147\n",
      "Epoch 1681/30000 Training Loss: 0.0573735237121582\n",
      "Epoch 1682/30000 Training Loss: 0.0734875425696373\n",
      "Epoch 1683/30000 Training Loss: 0.06253087520599365\n",
      "Epoch 1684/30000 Training Loss: 0.07203392684459686\n",
      "Epoch 1685/30000 Training Loss: 0.07207689434289932\n",
      "Epoch 1686/30000 Training Loss: 0.06252060830593109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1687/30000 Training Loss: 0.07173089683055878\n",
      "Epoch 1688/30000 Training Loss: 0.07150373607873917\n",
      "Epoch 1689/30000 Training Loss: 0.07661561667919159\n",
      "Epoch 1690/30000 Training Loss: 0.07742971926927567\n",
      "Epoch 1691/30000 Training Loss: 0.06544660776853561\n",
      "Epoch 1692/30000 Training Loss: 0.06521325558423996\n",
      "Epoch 1693/30000 Training Loss: 0.06942953914403915\n",
      "Epoch 1694/30000 Training Loss: 0.06826214492321014\n",
      "Epoch 1695/30000 Training Loss: 0.0687137022614479\n",
      "Epoch 1696/30000 Training Loss: 0.06816013157367706\n",
      "Epoch 1697/30000 Training Loss: 0.06882353127002716\n",
      "Epoch 1698/30000 Training Loss: 0.07195644080638885\n",
      "Epoch 1699/30000 Training Loss: 0.06342961639165878\n",
      "Epoch 1700/30000 Training Loss: 0.0768890231847763\n",
      "Epoch 1700/30000 Validation Loss: 0.07496708631515503\n",
      "Epoch 1701/30000 Training Loss: 0.0697350800037384\n",
      "Epoch 1702/30000 Training Loss: 0.07186545431613922\n",
      "Epoch 1703/30000 Training Loss: 0.06239748001098633\n",
      "Epoch 1704/30000 Training Loss: 0.06700943410396576\n",
      "Epoch 1705/30000 Training Loss: 0.07203308492898941\n",
      "Epoch 1706/30000 Training Loss: 0.07086102664470673\n",
      "Epoch 1707/30000 Training Loss: 0.0673259049654007\n",
      "Epoch 1708/30000 Training Loss: 0.07861819118261337\n",
      "Epoch 1709/30000 Training Loss: 0.06976334750652313\n",
      "Epoch 1710/30000 Training Loss: 0.07747369259595871\n",
      "Epoch 1711/30000 Training Loss: 0.07183162868022919\n",
      "Epoch 1712/30000 Training Loss: 0.06618939340114594\n",
      "Epoch 1713/30000 Training Loss: 0.07674451172351837\n",
      "Epoch 1714/30000 Training Loss: 0.07721049338579178\n",
      "Epoch 1715/30000 Training Loss: 0.06043056398630142\n",
      "Epoch 1716/30000 Training Loss: 0.06749133765697479\n",
      "Epoch 1717/30000 Training Loss: 0.08149270713329315\n",
      "Epoch 1718/30000 Training Loss: 0.07642871141433716\n",
      "Epoch 1719/30000 Training Loss: 0.06825269013643265\n",
      "Epoch 1720/30000 Training Loss: 0.07670515775680542\n",
      "Epoch 1721/30000 Training Loss: 0.06316491216421127\n",
      "Epoch 1722/30000 Training Loss: 0.06627319753170013\n",
      "Epoch 1723/30000 Training Loss: 0.06971780955791473\n",
      "Epoch 1724/30000 Training Loss: 0.06213434785604477\n",
      "Epoch 1725/30000 Training Loss: 0.06859041750431061\n",
      "Epoch 1726/30000 Training Loss: 0.06361699849367142\n",
      "Epoch 1727/30000 Training Loss: 0.0672568827867508\n",
      "Epoch 1728/30000 Training Loss: 0.060344040393829346\n",
      "Epoch 1729/30000 Training Loss: 0.07135806232690811\n",
      "Epoch 1730/30000 Training Loss: 0.06710748374462128\n",
      "Epoch 1731/30000 Training Loss: 0.06741873919963837\n",
      "Epoch 1732/30000 Training Loss: 0.06051113083958626\n",
      "Epoch 1733/30000 Training Loss: 0.0713919922709465\n",
      "Epoch 1734/30000 Training Loss: 0.06688569486141205\n",
      "Epoch 1735/30000 Training Loss: 0.08101614564657211\n",
      "Epoch 1736/30000 Training Loss: 0.06836257129907608\n",
      "Epoch 1737/30000 Training Loss: 0.07336781919002533\n",
      "Epoch 1738/30000 Training Loss: 0.06534774601459503\n",
      "Epoch 1739/30000 Training Loss: 0.06258504092693329\n",
      "Epoch 1740/30000 Training Loss: 0.07367531955242157\n",
      "Epoch 1741/30000 Training Loss: 0.06739579141139984\n",
      "Epoch 1742/30000 Training Loss: 0.07296337932348251\n",
      "Epoch 1743/30000 Training Loss: 0.06834371387958527\n",
      "Epoch 1744/30000 Training Loss: 0.06368662416934967\n",
      "Epoch 1745/30000 Training Loss: 0.06432200223207474\n",
      "Epoch 1746/30000 Training Loss: 0.05692983418703079\n",
      "Epoch 1747/30000 Training Loss: 0.06800394505262375\n",
      "Epoch 1748/30000 Training Loss: 0.0675838366150856\n",
      "Epoch 1749/30000 Training Loss: 0.07195061445236206\n",
      "Epoch 1750/30000 Training Loss: 0.06935438513755798\n",
      "Epoch 1750/30000 Validation Loss: 0.08144756406545639\n",
      "Epoch 1751/30000 Training Loss: 0.07438737899065018\n",
      "Epoch 1752/30000 Training Loss: 0.06556200236082077\n",
      "Epoch 1753/30000 Training Loss: 0.07377056032419205\n",
      "Epoch 1754/30000 Training Loss: 0.07498641312122345\n",
      "Epoch 1755/30000 Training Loss: 0.07368907332420349\n",
      "Epoch 1756/30000 Training Loss: 0.07240761816501617\n",
      "Epoch 1757/30000 Training Loss: 0.06988810002803802\n",
      "Epoch 1758/30000 Training Loss: 0.07441060990095139\n",
      "Epoch 1759/30000 Training Loss: 0.07545630633831024\n",
      "Epoch 1760/30000 Training Loss: 0.06542733311653137\n",
      "Epoch 1761/30000 Training Loss: 0.07081641256809235\n",
      "Epoch 1762/30000 Training Loss: 0.07325780391693115\n",
      "Epoch 1763/30000 Training Loss: 0.06622175872325897\n",
      "Epoch 1764/30000 Training Loss: 0.06677478551864624\n",
      "Epoch 1765/30000 Training Loss: 0.07128026336431503\n",
      "Epoch 1766/30000 Training Loss: 0.061340682208538055\n",
      "Epoch 1767/30000 Training Loss: 0.06642259657382965\n",
      "Epoch 1768/30000 Training Loss: 0.06346346437931061\n",
      "Epoch 1769/30000 Training Loss: 0.07160055637359619\n",
      "Epoch 1770/30000 Training Loss: 0.07268722355365753\n",
      "Epoch 1771/30000 Training Loss: 0.06587444245815277\n",
      "Epoch 1772/30000 Training Loss: 0.08816824853420258\n",
      "Epoch 1773/30000 Training Loss: 0.06994639337062836\n",
      "Epoch 1774/30000 Training Loss: 0.06428265571594238\n",
      "Epoch 1775/30000 Training Loss: 0.06253091990947723\n",
      "Epoch 1776/30000 Training Loss: 0.07547672092914581\n",
      "Epoch 1777/30000 Training Loss: 0.06357815861701965\n",
      "Epoch 1778/30000 Training Loss: 0.08134636282920837\n",
      "Epoch 1779/30000 Training Loss: 0.06478329002857208\n",
      "Epoch 1780/30000 Training Loss: 0.059179097414016724\n",
      "Epoch 1781/30000 Training Loss: 0.07192126661539078\n",
      "Epoch 1782/30000 Training Loss: 0.0576028935611248\n",
      "Epoch 1783/30000 Training Loss: 0.065095916390419\n",
      "Epoch 1784/30000 Training Loss: 0.07945562899112701\n",
      "Epoch 1785/30000 Training Loss: 0.0621875524520874\n",
      "Epoch 1786/30000 Training Loss: 0.06442151963710785\n",
      "Epoch 1787/30000 Training Loss: 0.06330503523349762\n",
      "Epoch 1788/30000 Training Loss: 0.06409160792827606\n",
      "Epoch 1789/30000 Training Loss: 0.06449601799249649\n",
      "Epoch 1790/30000 Training Loss: 0.06513209640979767\n",
      "Epoch 1791/30000 Training Loss: 0.07467880100011826\n",
      "Epoch 1792/30000 Training Loss: 0.061128169298172\n",
      "Epoch 1793/30000 Training Loss: 0.0714225247502327\n",
      "Epoch 1794/30000 Training Loss: 0.06468366086483002\n",
      "Epoch 1795/30000 Training Loss: 0.0642545223236084\n",
      "Epoch 1796/30000 Training Loss: 0.07465382665395737\n",
      "Epoch 1797/30000 Training Loss: 0.0745706707239151\n",
      "Epoch 1798/30000 Training Loss: 0.06213339418172836\n",
      "Epoch 1799/30000 Training Loss: 0.06301252543926239\n",
      "Epoch 1800/30000 Training Loss: 0.07134205102920532\n",
      "Epoch 1800/30000 Validation Loss: 0.07523783296346664\n",
      "Epoch 1801/30000 Training Loss: 0.057008594274520874\n",
      "Epoch 1802/30000 Training Loss: 0.07718875259160995\n",
      "Epoch 1803/30000 Training Loss: 0.06310543417930603\n",
      "Epoch 1804/30000 Training Loss: 0.07715505361557007\n",
      "Epoch 1805/30000 Training Loss: 0.0615028515458107\n",
      "Epoch 1806/30000 Training Loss: 0.0660955160856247\n",
      "Epoch 1807/30000 Training Loss: 0.06636874377727509\n",
      "Epoch 1808/30000 Training Loss: 0.0654933974146843\n",
      "Epoch 1809/30000 Training Loss: 0.07153740525245667\n",
      "Epoch 1810/30000 Training Loss: 0.0633680447936058\n",
      "Epoch 1811/30000 Training Loss: 0.0606083981692791\n",
      "Epoch 1812/30000 Training Loss: 0.07249470800161362\n",
      "Epoch 1813/30000 Training Loss: 0.06746353954076767\n",
      "Epoch 1814/30000 Training Loss: 0.074618399143219\n",
      "Epoch 1815/30000 Training Loss: 0.07323408126831055\n",
      "Epoch 1816/30000 Training Loss: 0.07583088427782059\n",
      "Epoch 1817/30000 Training Loss: 0.0759492889046669\n",
      "Epoch 1818/30000 Training Loss: 0.0733935683965683\n",
      "Epoch 1819/30000 Training Loss: 0.07320169359445572\n",
      "Epoch 1820/30000 Training Loss: 0.07745717465877533\n",
      "Epoch 1821/30000 Training Loss: 0.06584237515926361\n",
      "Epoch 1822/30000 Training Loss: 0.06514804810285568\n",
      "Epoch 1823/30000 Training Loss: 0.06301300972700119\n",
      "Epoch 1824/30000 Training Loss: 0.07425133138895035\n",
      "Epoch 1825/30000 Training Loss: 0.06077806279063225\n",
      "Epoch 1826/30000 Training Loss: 0.07433290779590607\n",
      "Epoch 1827/30000 Training Loss: 0.08065381646156311\n",
      "Epoch 1828/30000 Training Loss: 0.06900293380022049\n",
      "Epoch 1829/30000 Training Loss: 0.06895303726196289\n",
      "Epoch 1830/30000 Training Loss: 0.06906696408987045\n",
      "Epoch 1831/30000 Training Loss: 0.0630965381860733\n",
      "Epoch 1832/30000 Training Loss: 0.06941676884889603\n",
      "Epoch 1833/30000 Training Loss: 0.0716983899474144\n",
      "Epoch 1834/30000 Training Loss: 0.0628131777048111\n",
      "Epoch 1835/30000 Training Loss: 0.07427112758159637\n",
      "Epoch 1836/30000 Training Loss: 0.06456334888935089\n",
      "Epoch 1837/30000 Training Loss: 0.06647180765867233\n",
      "Epoch 1838/30000 Training Loss: 0.07101263105869293\n",
      "Epoch 1839/30000 Training Loss: 0.07101137191057205\n",
      "Epoch 1840/30000 Training Loss: 0.06605850160121918\n",
      "Epoch 1841/30000 Training Loss: 0.06606317311525345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1842/30000 Training Loss: 0.06710018217563629\n",
      "Epoch 1843/30000 Training Loss: 0.07744304090738297\n",
      "Epoch 1844/30000 Training Loss: 0.05483918637037277\n",
      "Epoch 1845/30000 Training Loss: 0.07178814709186554\n",
      "Epoch 1846/30000 Training Loss: 0.0579913854598999\n",
      "Epoch 1847/30000 Training Loss: 0.05993684381246567\n",
      "Epoch 1848/30000 Training Loss: 0.06631717085838318\n",
      "Epoch 1849/30000 Training Loss: 0.06725947558879852\n",
      "Epoch 1850/30000 Training Loss: 0.06252581626176834\n",
      "Epoch 1850/30000 Validation Loss: 0.07433465868234634\n",
      "Epoch 1851/30000 Training Loss: 0.06711377203464508\n",
      "Epoch 1852/30000 Training Loss: 0.07563517987728119\n",
      "Epoch 1853/30000 Training Loss: 0.07109899073839188\n",
      "Epoch 1854/30000 Training Loss: 0.07032036781311035\n",
      "Epoch 1855/30000 Training Loss: 0.06808426976203918\n",
      "Epoch 1856/30000 Training Loss: 0.0735582560300827\n",
      "Epoch 1857/30000 Training Loss: 0.060873158276081085\n",
      "Epoch 1858/30000 Training Loss: 0.0655227079987526\n",
      "Epoch 1859/30000 Training Loss: 0.06453771144151688\n",
      "Epoch 1860/30000 Training Loss: 0.06175316125154495\n",
      "Epoch 1861/30000 Training Loss: 0.07028549164533615\n",
      "Epoch 1862/30000 Training Loss: 0.06076662987470627\n",
      "Epoch 1863/30000 Training Loss: 0.07055877894163132\n",
      "Epoch 1864/30000 Training Loss: 0.06473196297883987\n",
      "Epoch 1865/30000 Training Loss: 0.07177337259054184\n",
      "Epoch 1866/30000 Training Loss: 0.05875459313392639\n",
      "Epoch 1867/30000 Training Loss: 0.0687698945403099\n",
      "Epoch 1868/30000 Training Loss: 0.07148878276348114\n",
      "Epoch 1869/30000 Training Loss: 0.07856426388025284\n",
      "Epoch 1870/30000 Training Loss: 0.06304484605789185\n",
      "Epoch 1871/30000 Training Loss: 0.06414806097745895\n",
      "Epoch 1872/30000 Training Loss: 0.07363834977149963\n",
      "Epoch 1873/30000 Training Loss: 0.06385611742734909\n",
      "Epoch 1874/30000 Training Loss: 0.07514801621437073\n",
      "Epoch 1875/30000 Training Loss: 0.060783661901950836\n",
      "Epoch 1876/30000 Training Loss: 0.058341871947050095\n",
      "Epoch 1877/30000 Training Loss: 0.06924884021282196\n",
      "Epoch 1878/30000 Training Loss: 0.0708276629447937\n",
      "Epoch 1879/30000 Training Loss: 0.06659455597400665\n",
      "Epoch 1880/30000 Training Loss: 0.07582160830497742\n",
      "Epoch 1881/30000 Training Loss: 0.07717316597700119\n",
      "Epoch 1882/30000 Training Loss: 0.07228822261095047\n",
      "Epoch 1883/30000 Training Loss: 0.07179730385541916\n",
      "Epoch 1884/30000 Training Loss: 0.06470243632793427\n",
      "Epoch 1885/30000 Training Loss: 0.06773104518651962\n",
      "Epoch 1886/30000 Training Loss: 0.05998309701681137\n",
      "Epoch 1887/30000 Training Loss: 0.06211944669485092\n",
      "Epoch 1888/30000 Training Loss: 0.06343342363834381\n",
      "Epoch 1889/30000 Training Loss: 0.07248841971158981\n",
      "Epoch 1890/30000 Training Loss: 0.07364223897457123\n",
      "Epoch 1891/30000 Training Loss: 0.06560494750738144\n",
      "Epoch 1892/30000 Training Loss: 0.06898422539234161\n",
      "Epoch 1893/30000 Training Loss: 0.06639118492603302\n",
      "Epoch 1894/30000 Training Loss: 0.0655612200498581\n",
      "Epoch 1895/30000 Training Loss: 0.07404913008213043\n",
      "Epoch 1896/30000 Training Loss: 0.06802432239055634\n",
      "Epoch 1897/30000 Training Loss: 0.06125728413462639\n",
      "Epoch 1898/30000 Training Loss: 0.0570371150970459\n",
      "Epoch 1899/30000 Training Loss: 0.07415138185024261\n",
      "Epoch 1900/30000 Training Loss: 0.06358955800533295\n",
      "Epoch 1900/30000 Validation Loss: 0.0694430023431778\n",
      "Epoch 1901/30000 Training Loss: 0.06158466264605522\n",
      "Epoch 1902/30000 Training Loss: 0.0758904442191124\n",
      "Epoch 1903/30000 Training Loss: 0.06373196840286255\n",
      "Epoch 1904/30000 Training Loss: 0.07561054080724716\n",
      "Epoch 1905/30000 Training Loss: 0.06551662087440491\n",
      "Epoch 1906/30000 Training Loss: 0.059359900653362274\n",
      "Epoch 1907/30000 Training Loss: 0.0623154453933239\n",
      "Epoch 1908/30000 Training Loss: 0.07035008072853088\n",
      "Epoch 1909/30000 Training Loss: 0.065037302672863\n",
      "Epoch 1910/30000 Training Loss: 0.07395125925540924\n",
      "Epoch 1911/30000 Training Loss: 0.07142272591590881\n",
      "Epoch 1912/30000 Training Loss: 0.07844851911067963\n",
      "Epoch 1913/30000 Training Loss: 0.06123363971710205\n",
      "Epoch 1914/30000 Training Loss: 0.06272202730178833\n",
      "Epoch 1915/30000 Training Loss: 0.0578535795211792\n",
      "Epoch 1916/30000 Training Loss: 0.06607494503259659\n",
      "Epoch 1917/30000 Training Loss: 0.07257424294948578\n",
      "Epoch 1918/30000 Training Loss: 0.06074661761522293\n",
      "Epoch 1919/30000 Training Loss: 0.05980934575200081\n",
      "Epoch 1920/30000 Training Loss: 0.06486108154058456\n",
      "Epoch 1921/30000 Training Loss: 0.06832589954137802\n",
      "Epoch 1922/30000 Training Loss: 0.06221964955329895\n",
      "Epoch 1923/30000 Training Loss: 0.06886003911495209\n",
      "Epoch 1924/30000 Training Loss: 0.07221676409244537\n",
      "Epoch 1925/30000 Training Loss: 0.07999478280544281\n",
      "Epoch 1926/30000 Training Loss: 0.06346722692251205\n",
      "Epoch 1927/30000 Training Loss: 0.06224657967686653\n",
      "Epoch 1928/30000 Training Loss: 0.07160578668117523\n",
      "Epoch 1929/30000 Training Loss: 0.059800952672958374\n",
      "Epoch 1930/30000 Training Loss: 0.06521224230527878\n",
      "Epoch 1931/30000 Training Loss: 0.06673955917358398\n",
      "Epoch 1932/30000 Training Loss: 0.06849599629640579\n",
      "Epoch 1933/30000 Training Loss: 0.06479952484369278\n",
      "Epoch 1934/30000 Training Loss: 0.06907309591770172\n",
      "Epoch 1935/30000 Training Loss: 0.055758945643901825\n",
      "Epoch 1936/30000 Training Loss: 0.06644488126039505\n",
      "Epoch 1937/30000 Training Loss: 0.06352509558200836\n",
      "Epoch 1938/30000 Training Loss: 0.06929405778646469\n",
      "Epoch 1939/30000 Training Loss: 0.06888648122549057\n",
      "Epoch 1940/30000 Training Loss: 0.07397191226482391\n",
      "Epoch 1941/30000 Training Loss: 0.06899790465831757\n",
      "Epoch 1942/30000 Training Loss: 0.06451024115085602\n",
      "Epoch 1943/30000 Training Loss: 0.05957527086138725\n",
      "Epoch 1944/30000 Training Loss: 0.06722325831651688\n",
      "Epoch 1945/30000 Training Loss: 0.06953030824661255\n",
      "Epoch 1946/30000 Training Loss: 0.07448286563158035\n",
      "Epoch 1947/30000 Training Loss: 0.06329495459794998\n",
      "Epoch 1948/30000 Training Loss: 0.0580376572906971\n",
      "Epoch 1949/30000 Training Loss: 0.06303122639656067\n",
      "Epoch 1950/30000 Training Loss: 0.06327945739030838\n",
      "Epoch 1950/30000 Validation Loss: 0.06962797790765762\n",
      "Epoch 1951/30000 Training Loss: 0.06956310570240021\n",
      "Epoch 1952/30000 Training Loss: 0.07153761386871338\n",
      "Epoch 1953/30000 Training Loss: 0.07274354249238968\n",
      "Epoch 1954/30000 Training Loss: 0.06843222677707672\n",
      "Epoch 1955/30000 Training Loss: 0.07027646154165268\n",
      "Epoch 1956/30000 Training Loss: 0.05695406347513199\n",
      "Epoch 1957/30000 Training Loss: 0.06371547281742096\n",
      "Epoch 1958/30000 Training Loss: 0.06598114967346191\n",
      "Epoch 1959/30000 Training Loss: 0.06879139691591263\n",
      "Epoch 1960/30000 Training Loss: 0.0629335418343544\n",
      "Epoch 1961/30000 Training Loss: 0.0649552047252655\n",
      "Epoch 1962/30000 Training Loss: 0.06624983251094818\n",
      "Epoch 1963/30000 Training Loss: 0.06482113152742386\n",
      "Epoch 1964/30000 Training Loss: 0.0693492740392685\n",
      "Epoch 1965/30000 Training Loss: 0.06591418385505676\n",
      "Epoch 1966/30000 Training Loss: 0.06358470767736435\n",
      "Epoch 1967/30000 Training Loss: 0.06829659640789032\n",
      "Epoch 1968/30000 Training Loss: 0.05938837677240372\n",
      "Epoch 1969/30000 Training Loss: 0.06639672815799713\n",
      "Epoch 1970/30000 Training Loss: 0.08551491051912308\n",
      "Epoch 1971/30000 Training Loss: 0.06390304863452911\n",
      "Epoch 1972/30000 Training Loss: 0.057451166212558746\n",
      "Epoch 1973/30000 Training Loss: 0.060556989163160324\n",
      "Epoch 1974/30000 Training Loss: 0.0773015171289444\n",
      "Epoch 1975/30000 Training Loss: 0.07598306983709335\n",
      "Epoch 1976/30000 Training Loss: 0.06407274305820465\n",
      "Epoch 1977/30000 Training Loss: 0.06378497928380966\n",
      "Epoch 1978/30000 Training Loss: 0.06877392530441284\n",
      "Epoch 1979/30000 Training Loss: 0.0742439404129982\n",
      "Epoch 1980/30000 Training Loss: 0.06239258497953415\n",
      "Epoch 1981/30000 Training Loss: 0.06684072315692902\n",
      "Epoch 1982/30000 Training Loss: 0.07226629555225372\n",
      "Epoch 1983/30000 Training Loss: 0.07376234233379364\n",
      "Epoch 1984/30000 Training Loss: 0.06309372186660767\n",
      "Epoch 1985/30000 Training Loss: 0.05776137113571167\n",
      "Epoch 1986/30000 Training Loss: 0.06194969266653061\n",
      "Epoch 1987/30000 Training Loss: 0.06640829145908356\n",
      "Epoch 1988/30000 Training Loss: 0.06729315966367722\n",
      "Epoch 1989/30000 Training Loss: 0.06632838398218155\n",
      "Epoch 1990/30000 Training Loss: 0.05861581489443779\n",
      "Epoch 1991/30000 Training Loss: 0.07524845004081726\n",
      "Epoch 1992/30000 Training Loss: 0.07752847671508789\n",
      "Epoch 1993/30000 Training Loss: 0.06403283774852753\n",
      "Epoch 1994/30000 Training Loss: 0.061210207641124725\n",
      "Epoch 1995/30000 Training Loss: 0.06214011460542679\n",
      "Epoch 1996/30000 Training Loss: 0.06470395624637604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1997/30000 Training Loss: 0.05919623374938965\n",
      "Epoch 1998/30000 Training Loss: 0.06444209814071655\n",
      "Epoch 1999/30000 Training Loss: 0.07179124653339386\n",
      "Epoch 2000/30000 Training Loss: 0.0718868300318718\n",
      "Epoch 2000/30000 Validation Loss: 0.059147339314222336\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.059147339314222336<=============\n",
      "Epoch 2001/30000 Training Loss: 0.06970884650945663\n",
      "Epoch 2002/30000 Training Loss: 0.06604279577732086\n",
      "Epoch 2003/30000 Training Loss: 0.07048586755990982\n",
      "Epoch 2004/30000 Training Loss: 0.06382909417152405\n",
      "Epoch 2005/30000 Training Loss: 0.06017535179853439\n",
      "Epoch 2006/30000 Training Loss: 0.06273575127124786\n",
      "Epoch 2007/30000 Training Loss: 0.06317534297704697\n",
      "Epoch 2008/30000 Training Loss: 0.07887065410614014\n",
      "Epoch 2009/30000 Training Loss: 0.06924869865179062\n",
      "Epoch 2010/30000 Training Loss: 0.05476756766438484\n",
      "Epoch 2011/30000 Training Loss: 0.0664999932050705\n",
      "Epoch 2012/30000 Training Loss: 0.07440163195133209\n",
      "Epoch 2013/30000 Training Loss: 0.06624080240726471\n",
      "Epoch 2014/30000 Training Loss: 0.06033431738615036\n",
      "Epoch 2015/30000 Training Loss: 0.062456369400024414\n",
      "Epoch 2016/30000 Training Loss: 0.0818251520395279\n",
      "Epoch 2017/30000 Training Loss: 0.06984670460224152\n",
      "Epoch 2018/30000 Training Loss: 0.06824226677417755\n",
      "Epoch 2019/30000 Training Loss: 0.06984192878007889\n",
      "Epoch 2020/30000 Training Loss: 0.06740866601467133\n",
      "Epoch 2021/30000 Training Loss: 0.0677400752902031\n",
      "Epoch 2022/30000 Training Loss: 0.0641612783074379\n",
      "Epoch 2023/30000 Training Loss: 0.06161829084157944\n",
      "Epoch 2024/30000 Training Loss: 0.06424064189195633\n",
      "Epoch 2025/30000 Training Loss: 0.06703309714794159\n",
      "Epoch 2026/30000 Training Loss: 0.05679341405630112\n",
      "Epoch 2027/30000 Training Loss: 0.067258320748806\n",
      "Epoch 2028/30000 Training Loss: 0.07215415686368942\n",
      "Epoch 2029/30000 Training Loss: 0.06229589506983757\n",
      "Epoch 2030/30000 Training Loss: 0.06716263294219971\n",
      "Epoch 2031/30000 Training Loss: 0.0703909620642662\n",
      "Epoch 2032/30000 Training Loss: 0.06738303601741791\n",
      "Epoch 2033/30000 Training Loss: 0.07105310261249542\n",
      "Epoch 2034/30000 Training Loss: 0.061261214315891266\n",
      "Epoch 2035/30000 Training Loss: 0.06842149794101715\n",
      "Epoch 2036/30000 Training Loss: 0.06745077669620514\n",
      "Epoch 2037/30000 Training Loss: 0.062112219631671906\n",
      "Epoch 2038/30000 Training Loss: 0.06697702407836914\n",
      "Epoch 2039/30000 Training Loss: 0.0710376650094986\n",
      "Epoch 2040/30000 Training Loss: 0.05754516273736954\n",
      "Epoch 2041/30000 Training Loss: 0.07237285375595093\n",
      "Epoch 2042/30000 Training Loss: 0.06693412363529205\n",
      "Epoch 2043/30000 Training Loss: 0.07443255186080933\n",
      "Epoch 2044/30000 Training Loss: 0.06489984691143036\n",
      "Epoch 2045/30000 Training Loss: 0.06959376484155655\n",
      "Epoch 2046/30000 Training Loss: 0.07160916924476624\n",
      "Epoch 2047/30000 Training Loss: 0.06513995677232742\n",
      "Epoch 2048/30000 Training Loss: 0.06970280408859253\n",
      "Epoch 2049/30000 Training Loss: 0.0629744827747345\n",
      "Epoch 2050/30000 Training Loss: 0.06607431173324585\n",
      "Epoch 2050/30000 Validation Loss: 0.06776440888643265\n",
      "Epoch 2051/30000 Training Loss: 0.06135119870305061\n",
      "Epoch 2052/30000 Training Loss: 0.06865940988063812\n",
      "Epoch 2053/30000 Training Loss: 0.0727715864777565\n",
      "Epoch 2054/30000 Training Loss: 0.06619606912136078\n",
      "Epoch 2055/30000 Training Loss: 0.06413107365369797\n",
      "Epoch 2056/30000 Training Loss: 0.0682206004858017\n",
      "Epoch 2057/30000 Training Loss: 0.07139971852302551\n",
      "Epoch 2058/30000 Training Loss: 0.07221348583698273\n",
      "Epoch 2059/30000 Training Loss: 0.059159260243177414\n",
      "Epoch 2060/30000 Training Loss: 0.0573335662484169\n",
      "Epoch 2061/30000 Training Loss: 0.06496962159872055\n",
      "Epoch 2062/30000 Training Loss: 0.06018499284982681\n",
      "Epoch 2063/30000 Training Loss: 0.07117003947496414\n",
      "Epoch 2064/30000 Training Loss: 0.06976275891065598\n",
      "Epoch 2065/30000 Training Loss: 0.06078537553548813\n",
      "Epoch 2066/30000 Training Loss: 0.07037360221147537\n",
      "Epoch 2067/30000 Training Loss: 0.06953054666519165\n",
      "Epoch 2068/30000 Training Loss: 0.0620051808655262\n",
      "Epoch 2069/30000 Training Loss: 0.05395926907658577\n",
      "Epoch 2070/30000 Training Loss: 0.0667971819639206\n",
      "Epoch 2071/30000 Training Loss: 0.05441524460911751\n",
      "Epoch 2072/30000 Training Loss: 0.06703577190637589\n",
      "Epoch 2073/30000 Training Loss: 0.0644356980919838\n",
      "Epoch 2074/30000 Training Loss: 0.05565856024622917\n",
      "Epoch 2075/30000 Training Loss: 0.06565199047327042\n",
      "Epoch 2076/30000 Training Loss: 0.07514802366495132\n",
      "Epoch 2077/30000 Training Loss: 0.0710807666182518\n",
      "Epoch 2078/30000 Training Loss: 0.06035462021827698\n",
      "Epoch 2079/30000 Training Loss: 0.06536580622196198\n",
      "Epoch 2080/30000 Training Loss: 0.07018691301345825\n",
      "Epoch 2081/30000 Training Loss: 0.05971785634756088\n",
      "Epoch 2082/30000 Training Loss: 0.0669291764497757\n",
      "Epoch 2083/30000 Training Loss: 0.06584116816520691\n",
      "Epoch 2084/30000 Training Loss: 0.07525081932544708\n",
      "Epoch 2085/30000 Training Loss: 0.0657912939786911\n",
      "Epoch 2086/30000 Training Loss: 0.07351943105459213\n",
      "Epoch 2087/30000 Training Loss: 0.05725603550672531\n",
      "Epoch 2088/30000 Training Loss: 0.07524094730615616\n",
      "Epoch 2089/30000 Training Loss: 0.06764067709445953\n",
      "Epoch 2090/30000 Training Loss: 0.06748122721910477\n",
      "Epoch 2091/30000 Training Loss: 0.0712050125002861\n",
      "Epoch 2092/30000 Training Loss: 0.053926192224025726\n",
      "Epoch 2093/30000 Training Loss: 0.07625900208950043\n",
      "Epoch 2094/30000 Training Loss: 0.06176906079053879\n",
      "Epoch 2095/30000 Training Loss: 0.06473146378993988\n",
      "Epoch 2096/30000 Training Loss: 0.06822710484266281\n",
      "Epoch 2097/30000 Training Loss: 0.06040830537676811\n",
      "Epoch 2098/30000 Training Loss: 0.06127394363284111\n",
      "Epoch 2099/30000 Training Loss: 0.06237737461924553\n",
      "Epoch 2100/30000 Training Loss: 0.0563402958214283\n",
      "Epoch 2100/30000 Validation Loss: 0.06686744838953018\n",
      "Epoch 2101/30000 Training Loss: 0.06842561066150665\n",
      "Epoch 2102/30000 Training Loss: 0.06242163106799126\n",
      "Epoch 2103/30000 Training Loss: 0.07151968032121658\n",
      "Epoch 2104/30000 Training Loss: 0.06038428470492363\n",
      "Epoch 2105/30000 Training Loss: 0.07116727530956268\n",
      "Epoch 2106/30000 Training Loss: 0.06300535053014755\n",
      "Epoch 2107/30000 Training Loss: 0.06595110148191452\n",
      "Epoch 2108/30000 Training Loss: 0.06560830771923065\n",
      "Epoch 2109/30000 Training Loss: 0.07257197797298431\n",
      "Epoch 2110/30000 Training Loss: 0.06010031700134277\n",
      "Epoch 2111/30000 Training Loss: 0.05868750810623169\n",
      "Epoch 2112/30000 Training Loss: 0.05527679994702339\n",
      "Epoch 2113/30000 Training Loss: 0.06606100499629974\n",
      "Epoch 2114/30000 Training Loss: 0.06691065430641174\n",
      "Epoch 2115/30000 Training Loss: 0.07292903959751129\n",
      "Epoch 2116/30000 Training Loss: 0.06291116029024124\n",
      "Epoch 2117/30000 Training Loss: 0.07501307874917984\n",
      "Epoch 2118/30000 Training Loss: 0.06966555863618851\n",
      "Epoch 2119/30000 Training Loss: 0.06909486651420593\n",
      "Epoch 2120/30000 Training Loss: 0.06367620825767517\n",
      "Epoch 2121/30000 Training Loss: 0.06446431577205658\n",
      "Epoch 2122/30000 Training Loss: 0.07365898042917252\n",
      "Epoch 2123/30000 Training Loss: 0.06665486097335815\n",
      "Epoch 2124/30000 Training Loss: 0.06716091930866241\n",
      "Epoch 2125/30000 Training Loss: 0.06754010170698166\n",
      "Epoch 2126/30000 Training Loss: 0.06898132711648941\n",
      "Epoch 2127/30000 Training Loss: 0.060309428721666336\n",
      "Epoch 2128/30000 Training Loss: 0.06484769284725189\n",
      "Epoch 2129/30000 Training Loss: 0.05930941179394722\n",
      "Epoch 2130/30000 Training Loss: 0.05325886607170105\n",
      "Epoch 2131/30000 Training Loss: 0.06470867991447449\n",
      "Epoch 2132/30000 Training Loss: 0.0705023854970932\n",
      "Epoch 2133/30000 Training Loss: 0.06473836302757263\n",
      "Epoch 2134/30000 Training Loss: 0.06739502400159836\n",
      "Epoch 2135/30000 Training Loss: 0.061324991285800934\n",
      "Epoch 2136/30000 Training Loss: 0.07441171258687973\n",
      "Epoch 2137/30000 Training Loss: 0.0722237378358841\n",
      "Epoch 2138/30000 Training Loss: 0.05704467371106148\n",
      "Epoch 2139/30000 Training Loss: 0.07854726165533066\n",
      "Epoch 2140/30000 Training Loss: 0.07110830396413803\n",
      "Epoch 2141/30000 Training Loss: 0.059616826474666595\n",
      "Epoch 2142/30000 Training Loss: 0.06698702275753021\n",
      "Epoch 2143/30000 Training Loss: 0.06499910354614258\n",
      "Epoch 2144/30000 Training Loss: 0.06258787214756012\n",
      "Epoch 2145/30000 Training Loss: 0.0705743134021759\n",
      "Epoch 2146/30000 Training Loss: 0.06456165015697479\n",
      "Epoch 2147/30000 Training Loss: 0.06075291708111763\n",
      "Epoch 2148/30000 Training Loss: 0.06742505729198456\n",
      "Epoch 2149/30000 Training Loss: 0.07542040199041367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2150/30000 Training Loss: 0.06041497737169266\n",
      "Epoch 2150/30000 Validation Loss: 0.0630316212773323\n",
      "Epoch 2151/30000 Training Loss: 0.06253349781036377\n",
      "Epoch 2152/30000 Training Loss: 0.07002635300159454\n",
      "Epoch 2153/30000 Training Loss: 0.06833021342754364\n",
      "Epoch 2154/30000 Training Loss: 0.06474228203296661\n",
      "Epoch 2155/30000 Training Loss: 0.0673263818025589\n",
      "Epoch 2156/30000 Training Loss: 0.0714331790804863\n",
      "Epoch 2157/30000 Training Loss: 0.06031600758433342\n",
      "Epoch 2158/30000 Training Loss: 0.05765567347407341\n",
      "Epoch 2159/30000 Training Loss: 0.057404421269893646\n",
      "Epoch 2160/30000 Training Loss: 0.060957662761211395\n",
      "Epoch 2161/30000 Training Loss: 0.07068683207035065\n",
      "Epoch 2162/30000 Training Loss: 0.05337037518620491\n",
      "Epoch 2163/30000 Training Loss: 0.06997759640216827\n",
      "Epoch 2164/30000 Training Loss: 0.06297357380390167\n",
      "Epoch 2165/30000 Training Loss: 0.07246234267950058\n",
      "Epoch 2166/30000 Training Loss: 0.07253851741552353\n",
      "Epoch 2167/30000 Training Loss: 0.06408628076314926\n",
      "Epoch 2168/30000 Training Loss: 0.06463409960269928\n",
      "Epoch 2169/30000 Training Loss: 0.06840697675943375\n",
      "Epoch 2170/30000 Training Loss: 0.06569194793701172\n",
      "Epoch 2171/30000 Training Loss: 0.06103534623980522\n",
      "Epoch 2172/30000 Training Loss: 0.06834448873996735\n",
      "Epoch 2173/30000 Training Loss: 0.0634925588965416\n",
      "Epoch 2174/30000 Training Loss: 0.06499600410461426\n",
      "Epoch 2175/30000 Training Loss: 0.07126402109861374\n",
      "Epoch 2176/30000 Training Loss: 0.061116673052310944\n",
      "Epoch 2177/30000 Training Loss: 0.07142175734043121\n",
      "Epoch 2178/30000 Training Loss: 0.06967464834451675\n",
      "Epoch 2179/30000 Training Loss: 0.06334216892719269\n",
      "Epoch 2180/30000 Training Loss: 0.06614570319652557\n",
      "Epoch 2181/30000 Training Loss: 0.06242518872022629\n",
      "Epoch 2182/30000 Training Loss: 0.07257189601659775\n",
      "Epoch 2183/30000 Training Loss: 0.06420644372701645\n",
      "Epoch 2184/30000 Training Loss: 0.06360643357038498\n",
      "Epoch 2185/30000 Training Loss: 0.06754618138074875\n",
      "Epoch 2186/30000 Training Loss: 0.06657091528177261\n",
      "Epoch 2187/30000 Training Loss: 0.0696052834391594\n",
      "Epoch 2188/30000 Training Loss: 0.07407112419605255\n",
      "Epoch 2189/30000 Training Loss: 0.07043484598398209\n",
      "Epoch 2190/30000 Training Loss: 0.05646198242902756\n",
      "Epoch 2191/30000 Training Loss: 0.06159261614084244\n",
      "Epoch 2192/30000 Training Loss: 0.06359883397817612\n",
      "Epoch 2193/30000 Training Loss: 0.059753548353910446\n",
      "Epoch 2194/30000 Training Loss: 0.06971587240695953\n",
      "Epoch 2195/30000 Training Loss: 0.06477732956409454\n",
      "Epoch 2196/30000 Training Loss: 0.07180605083703995\n",
      "Epoch 2197/30000 Training Loss: 0.06881068646907806\n",
      "Epoch 2198/30000 Training Loss: 0.0778883546590805\n",
      "Epoch 2199/30000 Training Loss: 0.07329706847667694\n",
      "Epoch 2200/30000 Training Loss: 0.06544490903615952\n",
      "Epoch 2200/30000 Validation Loss: 0.060171764343976974\n",
      "Epoch 2201/30000 Training Loss: 0.06712496280670166\n",
      "Epoch 2202/30000 Training Loss: 0.06750039756298065\n",
      "Epoch 2203/30000 Training Loss: 0.055583011358976364\n",
      "Epoch 2204/30000 Training Loss: 0.06372632086277008\n",
      "Epoch 2205/30000 Training Loss: 0.06292764842510223\n",
      "Epoch 2206/30000 Training Loss: 0.06095113232731819\n",
      "Epoch 2207/30000 Training Loss: 0.06717792898416519\n",
      "Epoch 2208/30000 Training Loss: 0.06520489603281021\n",
      "Epoch 2209/30000 Training Loss: 0.07564591616392136\n",
      "Epoch 2210/30000 Training Loss: 0.07764047384262085\n",
      "Epoch 2211/30000 Training Loss: 0.0612746886909008\n",
      "Epoch 2212/30000 Training Loss: 0.06770288944244385\n",
      "Epoch 2213/30000 Training Loss: 0.06378260999917984\n",
      "Epoch 2214/30000 Training Loss: 0.05860477685928345\n",
      "Epoch 2215/30000 Training Loss: 0.06958377361297607\n",
      "Epoch 2216/30000 Training Loss: 0.06384313106536865\n",
      "Epoch 2217/30000 Training Loss: 0.0667320191860199\n",
      "Epoch 2218/30000 Training Loss: 0.0627032220363617\n",
      "Epoch 2219/30000 Training Loss: 0.0667465478181839\n",
      "Epoch 2220/30000 Training Loss: 0.06951657682657242\n",
      "Epoch 2221/30000 Training Loss: 0.07305020838975906\n",
      "Epoch 2222/30000 Training Loss: 0.059801436960697174\n",
      "Epoch 2223/30000 Training Loss: 0.06187158077955246\n",
      "Epoch 2224/30000 Training Loss: 0.06417713314294815\n",
      "Epoch 2225/30000 Training Loss: 0.06615089625120163\n",
      "Epoch 2226/30000 Training Loss: 0.071722611784935\n",
      "Epoch 2227/30000 Training Loss: 0.06521952897310257\n",
      "Epoch 2228/30000 Training Loss: 0.06015056371688843\n",
      "Epoch 2229/30000 Training Loss: 0.06876108795404434\n",
      "Epoch 2230/30000 Training Loss: 0.06744556128978729\n",
      "Epoch 2231/30000 Training Loss: 0.07509448379278183\n",
      "Epoch 2232/30000 Training Loss: 0.07256195694208145\n",
      "Epoch 2233/30000 Training Loss: 0.05938237905502319\n",
      "Epoch 2234/30000 Training Loss: 0.06194555014371872\n",
      "Epoch 2235/30000 Training Loss: 0.061311013996601105\n",
      "Epoch 2236/30000 Training Loss: 0.0656389370560646\n",
      "Epoch 2237/30000 Training Loss: 0.059813082218170166\n",
      "Epoch 2238/30000 Training Loss: 0.06263883411884308\n",
      "Epoch 2239/30000 Training Loss: 0.060326218605041504\n",
      "Epoch 2240/30000 Training Loss: 0.060808442533016205\n",
      "Epoch 2241/30000 Training Loss: 0.0661386027932167\n",
      "Epoch 2242/30000 Training Loss: 0.061030395328998566\n",
      "Epoch 2243/30000 Training Loss: 0.05732874199748039\n",
      "Epoch 2244/30000 Training Loss: 0.0587448887526989\n",
      "Epoch 2245/30000 Training Loss: 0.0691101998090744\n",
      "Epoch 2246/30000 Training Loss: 0.056090690195560455\n",
      "Epoch 2247/30000 Training Loss: 0.06731978803873062\n",
      "Epoch 2248/30000 Training Loss: 0.07046565413475037\n",
      "Epoch 2249/30000 Training Loss: 0.05909769982099533\n",
      "Epoch 2250/30000 Training Loss: 0.06620477139949799\n",
      "Epoch 2250/30000 Validation Loss: 0.07803826779127121\n",
      "Epoch 2251/30000 Training Loss: 0.06312860548496246\n",
      "Epoch 2252/30000 Training Loss: 0.06462819874286652\n",
      "Epoch 2253/30000 Training Loss: 0.06901690363883972\n",
      "Epoch 2254/30000 Training Loss: 0.07128535211086273\n",
      "Epoch 2255/30000 Training Loss: 0.07717110216617584\n",
      "Epoch 2256/30000 Training Loss: 0.06665243208408356\n",
      "Epoch 2257/30000 Training Loss: 0.06113393232226372\n",
      "Epoch 2258/30000 Training Loss: 0.06628601998090744\n",
      "Epoch 2259/30000 Training Loss: 0.06648335605859756\n",
      "Epoch 2260/30000 Training Loss: 0.05458451062440872\n",
      "Epoch 2261/30000 Training Loss: 0.0669189915060997\n",
      "Epoch 2262/30000 Training Loss: 0.0753130167722702\n",
      "Epoch 2263/30000 Training Loss: 0.07701832056045532\n",
      "Epoch 2264/30000 Training Loss: 0.05705173686146736\n",
      "Epoch 2265/30000 Training Loss: 0.0671173706650734\n",
      "Epoch 2266/30000 Training Loss: 0.061640284955501556\n",
      "Epoch 2267/30000 Training Loss: 0.06615184992551804\n",
      "Epoch 2268/30000 Training Loss: 0.0632697194814682\n",
      "Epoch 2269/30000 Training Loss: 0.061174094676971436\n",
      "Epoch 2270/30000 Training Loss: 0.07147374749183655\n",
      "Epoch 2271/30000 Training Loss: 0.07142505049705505\n",
      "Epoch 2272/30000 Training Loss: 0.059129368513822556\n",
      "Epoch 2273/30000 Training Loss: 0.0640677660703659\n",
      "Epoch 2274/30000 Training Loss: 0.06555646657943726\n",
      "Epoch 2275/30000 Training Loss: 0.05342595651745796\n",
      "Epoch 2276/30000 Training Loss: 0.06975157558917999\n",
      "Epoch 2277/30000 Training Loss: 0.07258352637290955\n",
      "Epoch 2278/30000 Training Loss: 0.0667610615491867\n",
      "Epoch 2279/30000 Training Loss: 0.06775791198015213\n",
      "Epoch 2280/30000 Training Loss: 0.06342671811580658\n",
      "Epoch 2281/30000 Training Loss: 0.05564115196466446\n",
      "Epoch 2282/30000 Training Loss: 0.07999592274427414\n",
      "Epoch 2283/30000 Training Loss: 0.06330093741416931\n",
      "Epoch 2284/30000 Training Loss: 0.07050244510173798\n",
      "Epoch 2285/30000 Training Loss: 0.06177676469087601\n",
      "Epoch 2286/30000 Training Loss: 0.05962827801704407\n",
      "Epoch 2287/30000 Training Loss: 0.05563490837812424\n",
      "Epoch 2288/30000 Training Loss: 0.06760793179273605\n",
      "Epoch 2289/30000 Training Loss: 0.0632912740111351\n",
      "Epoch 2290/30000 Training Loss: 0.055293090641498566\n",
      "Epoch 2291/30000 Training Loss: 0.06357338279485703\n",
      "Epoch 2292/30000 Training Loss: 0.06802308559417725\n",
      "Epoch 2293/30000 Training Loss: 0.06381268799304962\n",
      "Epoch 2294/30000 Training Loss: 0.06599132716655731\n",
      "Epoch 2295/30000 Training Loss: 0.06309674680233002\n",
      "Epoch 2296/30000 Training Loss: 0.06464318931102753\n",
      "Epoch 2297/30000 Training Loss: 0.07016375660896301\n",
      "Epoch 2298/30000 Training Loss: 0.06844846159219742\n",
      "Epoch 2299/30000 Training Loss: 0.053969770669937134\n",
      "Epoch 2300/30000 Training Loss: 0.06531347334384918\n",
      "Epoch 2300/30000 Validation Loss: 0.06739887595176697\n",
      "Epoch 2301/30000 Training Loss: 0.06487475335597992\n",
      "Epoch 2302/30000 Training Loss: 0.06174034625291824\n",
      "Epoch 2303/30000 Training Loss: 0.059671033173799515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2304/30000 Training Loss: 0.06502728164196014\n",
      "Epoch 2305/30000 Training Loss: 0.06074292212724686\n",
      "Epoch 2306/30000 Training Loss: 0.07099888473749161\n",
      "Epoch 2307/30000 Training Loss: 0.07291316241025925\n",
      "Epoch 2308/30000 Training Loss: 0.05658470839262009\n",
      "Epoch 2309/30000 Training Loss: 0.051075004041194916\n",
      "Epoch 2310/30000 Training Loss: 0.061960361897945404\n",
      "Epoch 2311/30000 Training Loss: 0.07215946912765503\n",
      "Epoch 2312/30000 Training Loss: 0.0667496994137764\n",
      "Epoch 2313/30000 Training Loss: 0.06952572613954544\n",
      "Epoch 2314/30000 Training Loss: 0.060328442603349686\n",
      "Epoch 2315/30000 Training Loss: 0.07529405504465103\n",
      "Epoch 2316/30000 Training Loss: 0.06480542570352554\n",
      "Epoch 2317/30000 Training Loss: 0.06782056391239166\n",
      "Epoch 2318/30000 Training Loss: 0.06771402060985565\n",
      "Epoch 2319/30000 Training Loss: 0.06387431919574738\n",
      "Epoch 2320/30000 Training Loss: 0.06844229251146317\n",
      "Epoch 2321/30000 Training Loss: 0.06110880896449089\n",
      "Epoch 2322/30000 Training Loss: 0.06197434663772583\n",
      "Epoch 2323/30000 Training Loss: 0.06459088623523712\n",
      "Epoch 2324/30000 Training Loss: 0.061492640525102615\n",
      "Epoch 2325/30000 Training Loss: 0.06807693839073181\n",
      "Epoch 2326/30000 Training Loss: 0.065187007188797\n",
      "Epoch 2327/30000 Training Loss: 0.06589940190315247\n",
      "Epoch 2328/30000 Training Loss: 0.0577421672642231\n",
      "Epoch 2329/30000 Training Loss: 0.0674683004617691\n",
      "Epoch 2330/30000 Training Loss: 0.0586031898856163\n",
      "Epoch 2331/30000 Training Loss: 0.07666504383087158\n",
      "Epoch 2332/30000 Training Loss: 0.07309035211801529\n",
      "Epoch 2333/30000 Training Loss: 0.07179370522499084\n",
      "Epoch 2334/30000 Training Loss: 0.05828430503606796\n",
      "Epoch 2335/30000 Training Loss: 0.06885180622339249\n",
      "Epoch 2336/30000 Training Loss: 0.0644097700715065\n",
      "Epoch 2337/30000 Training Loss: 0.06778723001480103\n",
      "Epoch 2338/30000 Training Loss: 0.058904051780700684\n",
      "Epoch 2339/30000 Training Loss: 0.07849280536174774\n",
      "Epoch 2340/30000 Training Loss: 0.06144004315137863\n",
      "Epoch 2341/30000 Training Loss: 0.060916148126125336\n",
      "Epoch 2342/30000 Training Loss: 0.06631220132112503\n",
      "Epoch 2343/30000 Training Loss: 0.060601841658353806\n",
      "Epoch 2344/30000 Training Loss: 0.0734652578830719\n",
      "Epoch 2345/30000 Training Loss: 0.06635037809610367\n",
      "Epoch 2346/30000 Training Loss: 0.06388379633426666\n",
      "Epoch 2347/30000 Training Loss: 0.05916985124349594\n",
      "Epoch 2348/30000 Training Loss: 0.06413145363330841\n",
      "Epoch 2349/30000 Training Loss: 0.05995355173945427\n",
      "Epoch 2350/30000 Training Loss: 0.07347949594259262\n",
      "Epoch 2350/30000 Validation Loss: 0.06521129608154297\n",
      "Epoch 2351/30000 Training Loss: 0.06176173686981201\n",
      "Epoch 2352/30000 Training Loss: 0.057368405163288116\n",
      "Epoch 2353/30000 Training Loss: 0.05859661102294922\n",
      "Epoch 2354/30000 Training Loss: 0.061144858598709106\n",
      "Epoch 2355/30000 Training Loss: 0.0655616745352745\n",
      "Epoch 2356/30000 Training Loss: 0.06758368760347366\n",
      "Epoch 2357/30000 Training Loss: 0.07105506956577301\n",
      "Epoch 2358/30000 Training Loss: 0.0663347840309143\n",
      "Epoch 2359/30000 Training Loss: 0.05842869356274605\n",
      "Epoch 2360/30000 Training Loss: 0.05506724864244461\n",
      "Epoch 2361/30000 Training Loss: 0.05649265646934509\n",
      "Epoch 2362/30000 Training Loss: 0.053488850593566895\n",
      "Epoch 2363/30000 Training Loss: 0.06587845087051392\n",
      "Epoch 2364/30000 Training Loss: 0.06314857304096222\n",
      "Epoch 2365/30000 Training Loss: 0.05461444705724716\n",
      "Epoch 2366/30000 Training Loss: 0.05688976123929024\n",
      "Epoch 2367/30000 Training Loss: 0.06179726868867874\n",
      "Epoch 2368/30000 Training Loss: 0.05834227055311203\n",
      "Epoch 2369/30000 Training Loss: 0.06693942844867706\n",
      "Epoch 2370/30000 Training Loss: 0.055617593228816986\n",
      "Epoch 2371/30000 Training Loss: 0.07119738310575485\n",
      "Epoch 2372/30000 Training Loss: 0.06667913496494293\n",
      "Epoch 2373/30000 Training Loss: 0.06254713237285614\n",
      "Epoch 2374/30000 Training Loss: 0.06070146709680557\n",
      "Epoch 2375/30000 Training Loss: 0.05824161320924759\n",
      "Epoch 2376/30000 Training Loss: 0.06608743965625763\n",
      "Epoch 2377/30000 Training Loss: 0.0679008960723877\n",
      "Epoch 2378/30000 Training Loss: 0.070350781083107\n",
      "Epoch 2379/30000 Training Loss: 0.06564904749393463\n",
      "Epoch 2380/30000 Training Loss: 0.06171476095914841\n",
      "Epoch 2381/30000 Training Loss: 0.06287532299757004\n",
      "Epoch 2382/30000 Training Loss: 0.07139299809932709\n",
      "Epoch 2383/30000 Training Loss: 0.06075258180499077\n",
      "Epoch 2384/30000 Training Loss: 0.0632905438542366\n",
      "Epoch 2385/30000 Training Loss: 0.06978021562099457\n",
      "Epoch 2386/30000 Training Loss: 0.06181027367711067\n",
      "Epoch 2387/30000 Training Loss: 0.056072771549224854\n",
      "Epoch 2388/30000 Training Loss: 0.07004617154598236\n",
      "Epoch 2389/30000 Training Loss: 0.05660848692059517\n",
      "Epoch 2390/30000 Training Loss: 0.06502075493335724\n",
      "Epoch 2391/30000 Training Loss: 0.05389471724629402\n",
      "Epoch 2392/30000 Training Loss: 0.06311985105276108\n",
      "Epoch 2393/30000 Training Loss: 0.07365192472934723\n",
      "Epoch 2394/30000 Training Loss: 0.07417716830968857\n",
      "Epoch 2395/30000 Training Loss: 0.0694986954331398\n",
      "Epoch 2396/30000 Training Loss: 0.06292548030614853\n",
      "Epoch 2397/30000 Training Loss: 0.06242046505212784\n",
      "Epoch 2398/30000 Training Loss: 0.060876380652189255\n",
      "Epoch 2399/30000 Training Loss: 0.07364629209041595\n",
      "Epoch 2400/30000 Training Loss: 0.06325970590114594\n",
      "Epoch 2400/30000 Validation Loss: 0.06505732238292694\n",
      "Epoch 2401/30000 Training Loss: 0.06045929715037346\n",
      "Epoch 2402/30000 Training Loss: 0.05992317944765091\n",
      "Epoch 2403/30000 Training Loss: 0.07773519307374954\n",
      "Epoch 2404/30000 Training Loss: 0.06384941190481186\n",
      "Epoch 2405/30000 Training Loss: 0.06713618338108063\n",
      "Epoch 2406/30000 Training Loss: 0.06562976539134979\n",
      "Epoch 2407/30000 Training Loss: 0.06718374043703079\n",
      "Epoch 2408/30000 Training Loss: 0.063531294465065\n",
      "Epoch 2409/30000 Training Loss: 0.06524112820625305\n",
      "Epoch 2410/30000 Training Loss: 0.0583660714328289\n",
      "Epoch 2411/30000 Training Loss: 0.06077738478779793\n",
      "Epoch 2412/30000 Training Loss: 0.0611453652381897\n",
      "Epoch 2413/30000 Training Loss: 0.06095149368047714\n",
      "Epoch 2414/30000 Training Loss: 0.07155902683734894\n",
      "Epoch 2415/30000 Training Loss: 0.059384383261203766\n",
      "Epoch 2416/30000 Training Loss: 0.05778083950281143\n",
      "Epoch 2417/30000 Training Loss: 0.0574176125228405\n",
      "Epoch 2418/30000 Training Loss: 0.05997344106435776\n",
      "Epoch 2419/30000 Training Loss: 0.06356588751077652\n",
      "Epoch 2420/30000 Training Loss: 0.06874553114175797\n",
      "Epoch 2421/30000 Training Loss: 0.0595085546374321\n",
      "Epoch 2422/30000 Training Loss: 0.056618668138980865\n",
      "Epoch 2423/30000 Training Loss: 0.06443258374929428\n",
      "Epoch 2424/30000 Training Loss: 0.06552822887897491\n",
      "Epoch 2425/30000 Training Loss: 0.06693728268146515\n",
      "Epoch 2426/30000 Training Loss: 0.058666080236434937\n",
      "Epoch 2427/30000 Training Loss: 0.06382805109024048\n",
      "Epoch 2428/30000 Training Loss: 0.06285766512155533\n",
      "Epoch 2429/30000 Training Loss: 0.06296946108341217\n",
      "Epoch 2430/30000 Training Loss: 0.06802020221948624\n",
      "Epoch 2431/30000 Training Loss: 0.06316443532705307\n",
      "Epoch 2432/30000 Training Loss: 0.07648250460624695\n",
      "Epoch 2433/30000 Training Loss: 0.059587061405181885\n",
      "Epoch 2434/30000 Training Loss: 0.058289848268032074\n",
      "Epoch 2435/30000 Training Loss: 0.06328126043081284\n",
      "Epoch 2436/30000 Training Loss: 0.05674413964152336\n",
      "Epoch 2437/30000 Training Loss: 0.0628146156668663\n",
      "Epoch 2438/30000 Training Loss: 0.0595446415245533\n",
      "Epoch 2439/30000 Training Loss: 0.06881745159626007\n",
      "Epoch 2440/30000 Training Loss: 0.0698414295911789\n",
      "Epoch 2441/30000 Training Loss: 0.06142841652035713\n",
      "Epoch 2442/30000 Training Loss: 0.05987025424838066\n",
      "Epoch 2443/30000 Training Loss: 0.05943049117922783\n",
      "Epoch 2444/30000 Training Loss: 0.0665290430188179\n",
      "Epoch 2445/30000 Training Loss: 0.07253547012805939\n",
      "Epoch 2446/30000 Training Loss: 0.06106700748205185\n",
      "Epoch 2447/30000 Training Loss: 0.06119122356176376\n",
      "Epoch 2448/30000 Training Loss: 0.074143186211586\n",
      "Epoch 2449/30000 Training Loss: 0.06439864635467529\n",
      "Epoch 2450/30000 Training Loss: 0.062227942049503326\n",
      "Epoch 2450/30000 Validation Loss: 0.07126278430223465\n",
      "Epoch 2451/30000 Training Loss: 0.05702313780784607\n",
      "Epoch 2452/30000 Training Loss: 0.0634048730134964\n",
      "Epoch 2453/30000 Training Loss: 0.07008467614650726\n",
      "Epoch 2454/30000 Training Loss: 0.0588744580745697\n",
      "Epoch 2455/30000 Training Loss: 0.06872344762086868\n",
      "Epoch 2456/30000 Training Loss: 0.06692399829626083\n",
      "Epoch 2457/30000 Training Loss: 0.06783421337604523\n",
      "Epoch 2458/30000 Training Loss: 0.06649323552846909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2459/30000 Training Loss: 0.0665082260966301\n",
      "Epoch 2460/30000 Training Loss: 0.06593562662601471\n",
      "Epoch 2461/30000 Training Loss: 0.06296686828136444\n",
      "Epoch 2462/30000 Training Loss: 0.05690712481737137\n",
      "Epoch 2463/30000 Training Loss: 0.0657331570982933\n",
      "Epoch 2464/30000 Training Loss: 0.059665195643901825\n",
      "Epoch 2465/30000 Training Loss: 0.05423795431852341\n",
      "Epoch 2466/30000 Training Loss: 0.05767560005187988\n",
      "Epoch 2467/30000 Training Loss: 0.06411852687597275\n",
      "Epoch 2468/30000 Training Loss: 0.07649556547403336\n",
      "Epoch 2469/30000 Training Loss: 0.05869918316602707\n",
      "Epoch 2470/30000 Training Loss: 0.06016132980585098\n",
      "Epoch 2471/30000 Training Loss: 0.06270972639322281\n",
      "Epoch 2472/30000 Training Loss: 0.06860728561878204\n",
      "Epoch 2473/30000 Training Loss: 0.06593045592308044\n",
      "Epoch 2474/30000 Training Loss: 0.05296901613473892\n",
      "Epoch 2475/30000 Training Loss: 0.06698166579008102\n",
      "Epoch 2476/30000 Training Loss: 0.07408718019723892\n",
      "Epoch 2477/30000 Training Loss: 0.059129126369953156\n",
      "Epoch 2478/30000 Training Loss: 0.05890461802482605\n",
      "Epoch 2479/30000 Training Loss: 0.06713967025279999\n",
      "Epoch 2480/30000 Training Loss: 0.06604430824518204\n",
      "Epoch 2481/30000 Training Loss: 0.06182629615068436\n",
      "Epoch 2482/30000 Training Loss: 0.06512494385242462\n",
      "Epoch 2483/30000 Training Loss: 0.06869322061538696\n",
      "Epoch 2484/30000 Training Loss: 0.0648953914642334\n",
      "Epoch 2485/30000 Training Loss: 0.06076003983616829\n",
      "Epoch 2486/30000 Training Loss: 0.06194417551159859\n",
      "Epoch 2487/30000 Training Loss: 0.06650616973638535\n",
      "Epoch 2488/30000 Training Loss: 0.06376083940267563\n",
      "Epoch 2489/30000 Training Loss: 0.055316220968961716\n",
      "Epoch 2490/30000 Training Loss: 0.05935973674058914\n",
      "Epoch 2491/30000 Training Loss: 0.06826071441173553\n",
      "Epoch 2492/30000 Training Loss: 0.07016501575708389\n",
      "Epoch 2493/30000 Training Loss: 0.06674066185951233\n",
      "Epoch 2494/30000 Training Loss: 0.060150980949401855\n",
      "Epoch 2495/30000 Training Loss: 0.06746114790439606\n",
      "Epoch 2496/30000 Training Loss: 0.07070212066173553\n",
      "Epoch 2497/30000 Training Loss: 0.06707920134067535\n",
      "Epoch 2498/30000 Training Loss: 0.04746817797422409\n",
      "Epoch 2499/30000 Training Loss: 0.059370528906583786\n",
      "Epoch 2500/30000 Training Loss: 0.06291031837463379\n",
      "Epoch 2500/30000 Validation Loss: 0.06308569014072418\n",
      "Epoch 2501/30000 Training Loss: 0.0642494410276413\n",
      "Epoch 2502/30000 Training Loss: 0.07040877640247345\n",
      "Epoch 2503/30000 Training Loss: 0.06381504237651825\n",
      "Epoch 2504/30000 Training Loss: 0.05213094875216484\n",
      "Epoch 2505/30000 Training Loss: 0.061262767761945724\n",
      "Epoch 2506/30000 Training Loss: 0.06490711867809296\n",
      "Epoch 2507/30000 Training Loss: 0.06420464813709259\n",
      "Epoch 2508/30000 Training Loss: 0.05667479708790779\n",
      "Epoch 2509/30000 Training Loss: 0.057917993515729904\n",
      "Epoch 2510/30000 Training Loss: 0.05622488260269165\n",
      "Epoch 2511/30000 Training Loss: 0.05934011936187744\n",
      "Epoch 2512/30000 Training Loss: 0.06853079050779343\n",
      "Epoch 2513/30000 Training Loss: 0.07264451682567596\n",
      "Epoch 2514/30000 Training Loss: 0.06896428018808365\n",
      "Epoch 2515/30000 Training Loss: 0.06055973097681999\n",
      "Epoch 2516/30000 Training Loss: 0.0708911269903183\n",
      "Epoch 2517/30000 Training Loss: 0.0686386451125145\n",
      "Epoch 2518/30000 Training Loss: 0.054072219878435135\n",
      "Epoch 2519/30000 Training Loss: 0.06166505813598633\n",
      "Epoch 2520/30000 Training Loss: 0.05718224123120308\n",
      "Epoch 2521/30000 Training Loss: 0.05754200369119644\n",
      "Epoch 2522/30000 Training Loss: 0.07521191984415054\n",
      "Epoch 2523/30000 Training Loss: 0.060963861644268036\n",
      "Epoch 2524/30000 Training Loss: 0.059954673051834106\n",
      "Epoch 2525/30000 Training Loss: 0.06460823118686676\n",
      "Epoch 2526/30000 Training Loss: 0.06969951838254929\n",
      "Epoch 2527/30000 Training Loss: 0.058477841317653656\n",
      "Epoch 2528/30000 Training Loss: 0.06580469012260437\n",
      "Epoch 2529/30000 Training Loss: 0.06298405677080154\n",
      "Epoch 2530/30000 Training Loss: 0.07655007392168045\n",
      "Epoch 2531/30000 Training Loss: 0.06411102414131165\n",
      "Epoch 2532/30000 Training Loss: 0.06550701707601547\n",
      "Epoch 2533/30000 Training Loss: 0.0642632320523262\n",
      "Epoch 2534/30000 Training Loss: 0.07382489740848541\n",
      "Epoch 2535/30000 Training Loss: 0.057102181017398834\n",
      "Epoch 2536/30000 Training Loss: 0.0722561627626419\n",
      "Epoch 2537/30000 Training Loss: 0.06657665222883224\n",
      "Epoch 2538/30000 Training Loss: 0.06485067307949066\n",
      "Epoch 2539/30000 Training Loss: 0.07152532041072845\n",
      "Epoch 2540/30000 Training Loss: 0.060897327959537506\n",
      "Epoch 2541/30000 Training Loss: 0.05941738560795784\n",
      "Epoch 2542/30000 Training Loss: 0.06288290023803711\n",
      "Epoch 2543/30000 Training Loss: 0.07800363004207611\n",
      "Epoch 2544/30000 Training Loss: 0.06221582740545273\n",
      "Epoch 2545/30000 Training Loss: 0.06558885425329208\n",
      "Epoch 2546/30000 Training Loss: 0.0635015144944191\n",
      "Epoch 2547/30000 Training Loss: 0.07159280776977539\n",
      "Epoch 2548/30000 Training Loss: 0.05520625784993172\n",
      "Epoch 2549/30000 Training Loss: 0.062165699899196625\n",
      "Epoch 2550/30000 Training Loss: 0.061867766082286835\n",
      "Epoch 2550/30000 Validation Loss: 0.06478781998157501\n",
      "Epoch 2551/30000 Training Loss: 0.06413103640079498\n",
      "Epoch 2552/30000 Training Loss: 0.05892229825258255\n",
      "Epoch 2553/30000 Training Loss: 0.06620056182146072\n",
      "Epoch 2554/30000 Training Loss: 0.05473146587610245\n",
      "Epoch 2555/30000 Training Loss: 0.06936068832874298\n",
      "Epoch 2556/30000 Training Loss: 0.06129957363009453\n",
      "Epoch 2557/30000 Training Loss: 0.06977192312479019\n",
      "Epoch 2558/30000 Training Loss: 0.05566328763961792\n",
      "Epoch 2559/30000 Training Loss: 0.05999161675572395\n",
      "Epoch 2560/30000 Training Loss: 0.052143532782793045\n",
      "Epoch 2561/30000 Training Loss: 0.05789386108517647\n",
      "Epoch 2562/30000 Training Loss: 0.07072840631008148\n",
      "Epoch 2563/30000 Training Loss: 0.055899184197187424\n",
      "Epoch 2564/30000 Training Loss: 0.06429789215326309\n",
      "Epoch 2565/30000 Training Loss: 0.06566169857978821\n",
      "Epoch 2566/30000 Training Loss: 0.06864245235919952\n",
      "Epoch 2567/30000 Training Loss: 0.05238752439618111\n",
      "Epoch 2568/30000 Training Loss: 0.05895218998193741\n",
      "Epoch 2569/30000 Training Loss: 0.06826846301555634\n",
      "Epoch 2570/30000 Training Loss: 0.06515859067440033\n",
      "Epoch 2571/30000 Training Loss: 0.05776553228497505\n",
      "Epoch 2572/30000 Training Loss: 0.06706305593252182\n",
      "Epoch 2573/30000 Training Loss: 0.06175777316093445\n",
      "Epoch 2574/30000 Training Loss: 0.05317334085702896\n",
      "Epoch 2575/30000 Training Loss: 0.0646800845861435\n",
      "Epoch 2576/30000 Training Loss: 0.06868165731430054\n",
      "Epoch 2577/30000 Training Loss: 0.05955534055829048\n",
      "Epoch 2578/30000 Training Loss: 0.05962409824132919\n",
      "Epoch 2579/30000 Training Loss: 0.06388579308986664\n",
      "Epoch 2580/30000 Training Loss: 0.06016859412193298\n",
      "Epoch 2581/30000 Training Loss: 0.0613173246383667\n",
      "Epoch 2582/30000 Training Loss: 0.05793017894029617\n",
      "Epoch 2583/30000 Training Loss: 0.06198229640722275\n",
      "Epoch 2584/30000 Training Loss: 0.059171468019485474\n",
      "Epoch 2585/30000 Training Loss: 0.0626426711678505\n",
      "Epoch 2586/30000 Training Loss: 0.0647570788860321\n",
      "Epoch 2587/30000 Training Loss: 0.052916668355464935\n",
      "Epoch 2588/30000 Training Loss: 0.06085096672177315\n",
      "Epoch 2589/30000 Training Loss: 0.0613395981490612\n",
      "Epoch 2590/30000 Training Loss: 0.06391458213329315\n",
      "Epoch 2591/30000 Training Loss: 0.06992228329181671\n",
      "Epoch 2592/30000 Training Loss: 0.057057082653045654\n",
      "Epoch 2593/30000 Training Loss: 0.060017604380846024\n",
      "Epoch 2594/30000 Training Loss: 0.061847008764743805\n",
      "Epoch 2595/30000 Training Loss: 0.06399387121200562\n",
      "Epoch 2596/30000 Training Loss: 0.06633145362138748\n",
      "Epoch 2597/30000 Training Loss: 0.0507267490029335\n",
      "Epoch 2598/30000 Training Loss: 0.059355538338422775\n",
      "Epoch 2599/30000 Training Loss: 0.05601293593645096\n",
      "Epoch 2600/30000 Training Loss: 0.059495411813259125\n",
      "Epoch 2600/30000 Validation Loss: 0.0583190843462944\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0583190843462944<=============\n",
      "Epoch 2601/30000 Training Loss: 0.05206512287259102\n",
      "Epoch 2602/30000 Training Loss: 0.06102997064590454\n",
      "Epoch 2603/30000 Training Loss: 0.05212213844060898\n",
      "Epoch 2604/30000 Training Loss: 0.06503023952245712\n",
      "Epoch 2605/30000 Training Loss: 0.057075779885053635\n",
      "Epoch 2606/30000 Training Loss: 0.06179460883140564\n",
      "Epoch 2607/30000 Training Loss: 0.06992168724536896\n",
      "Epoch 2608/30000 Training Loss: 0.06325440108776093\n",
      "Epoch 2609/30000 Training Loss: 0.0620667040348053\n",
      "Epoch 2610/30000 Training Loss: 0.0513940155506134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2611/30000 Training Loss: 0.06666950136423111\n",
      "Epoch 2612/30000 Training Loss: 0.0547381155192852\n",
      "Epoch 2613/30000 Training Loss: 0.05792395398020744\n",
      "Epoch 2614/30000 Training Loss: 0.06382731348276138\n",
      "Epoch 2615/30000 Training Loss: 0.06262176483869553\n",
      "Epoch 2616/30000 Training Loss: 0.055893171578645706\n",
      "Epoch 2617/30000 Training Loss: 0.06682680547237396\n",
      "Epoch 2618/30000 Training Loss: 0.06578704714775085\n",
      "Epoch 2619/30000 Training Loss: 0.0584266372025013\n",
      "Epoch 2620/30000 Training Loss: 0.060223013162612915\n",
      "Epoch 2621/30000 Training Loss: 0.05498545616865158\n",
      "Epoch 2622/30000 Training Loss: 0.0624314546585083\n",
      "Epoch 2623/30000 Training Loss: 0.053847409784793854\n",
      "Epoch 2624/30000 Training Loss: 0.06774812936782837\n",
      "Epoch 2625/30000 Training Loss: 0.057499147951602936\n",
      "Epoch 2626/30000 Training Loss: 0.05029860883951187\n",
      "Epoch 2627/30000 Training Loss: 0.07491739839315414\n",
      "Epoch 2628/30000 Training Loss: 0.06043652817606926\n",
      "Epoch 2629/30000 Training Loss: 0.0623694583773613\n",
      "Epoch 2630/30000 Training Loss: 0.07153250277042389\n",
      "Epoch 2631/30000 Training Loss: 0.07041052728891373\n",
      "Epoch 2632/30000 Training Loss: 0.0670623928308487\n",
      "Epoch 2633/30000 Training Loss: 0.05843541771173477\n",
      "Epoch 2634/30000 Training Loss: 0.05191882327198982\n",
      "Epoch 2635/30000 Training Loss: 0.0644150972366333\n",
      "Epoch 2636/30000 Training Loss: 0.06645002216100693\n",
      "Epoch 2637/30000 Training Loss: 0.06696412712335587\n",
      "Epoch 2638/30000 Training Loss: 0.060479454696178436\n",
      "Epoch 2639/30000 Training Loss: 0.06477342545986176\n",
      "Epoch 2640/30000 Training Loss: 0.06091419979929924\n",
      "Epoch 2641/30000 Training Loss: 0.055026836693286896\n",
      "Epoch 2642/30000 Training Loss: 0.05929361656308174\n",
      "Epoch 2643/30000 Training Loss: 0.06530077755451202\n",
      "Epoch 2644/30000 Training Loss: 0.061960794031620026\n",
      "Epoch 2645/30000 Training Loss: 0.06751123815774918\n",
      "Epoch 2646/30000 Training Loss: 0.06553558260202408\n",
      "Epoch 2647/30000 Training Loss: 0.0568351224064827\n",
      "Epoch 2648/30000 Training Loss: 0.06286325305700302\n",
      "Epoch 2649/30000 Training Loss: 0.06012774258852005\n",
      "Epoch 2650/30000 Training Loss: 0.055843252688646317\n",
      "Epoch 2650/30000 Validation Loss: 0.06819653511047363\n",
      "Epoch 2651/30000 Training Loss: 0.06644511222839355\n",
      "Epoch 2652/30000 Training Loss: 0.05791325494647026\n",
      "Epoch 2653/30000 Training Loss: 0.06313668191432953\n",
      "Epoch 2654/30000 Training Loss: 0.06041989475488663\n",
      "Epoch 2655/30000 Training Loss: 0.07300487905740738\n",
      "Epoch 2656/30000 Training Loss: 0.06528197228908539\n",
      "Epoch 2657/30000 Training Loss: 0.06612621992826462\n",
      "Epoch 2658/30000 Training Loss: 0.05917096138000488\n",
      "Epoch 2659/30000 Training Loss: 0.05288086086511612\n",
      "Epoch 2660/30000 Training Loss: 0.061904795467853546\n",
      "Epoch 2661/30000 Training Loss: 0.06078682094812393\n",
      "Epoch 2662/30000 Training Loss: 0.060603998601436615\n",
      "Epoch 2663/30000 Training Loss: 0.05211398005485535\n",
      "Epoch 2664/30000 Training Loss: 0.058196406811475754\n",
      "Epoch 2665/30000 Training Loss: 0.062307823449373245\n",
      "Epoch 2666/30000 Training Loss: 0.07527510821819305\n",
      "Epoch 2667/30000 Training Loss: 0.059243470430374146\n",
      "Epoch 2668/30000 Training Loss: 0.0673789530992508\n",
      "Epoch 2669/30000 Training Loss: 0.05896403267979622\n",
      "Epoch 2670/30000 Training Loss: 0.0622267946600914\n",
      "Epoch 2671/30000 Training Loss: 0.05988194793462753\n",
      "Epoch 2672/30000 Training Loss: 0.06122897192835808\n",
      "Epoch 2673/30000 Training Loss: 0.07392169535160065\n",
      "Epoch 2674/30000 Training Loss: 0.06390347331762314\n",
      "Epoch 2675/30000 Training Loss: 0.06492863595485687\n",
      "Epoch 2676/30000 Training Loss: 0.05444647744297981\n",
      "Epoch 2677/30000 Training Loss: 0.06316403299570084\n",
      "Epoch 2678/30000 Training Loss: 0.06270401179790497\n",
      "Epoch 2679/30000 Training Loss: 0.05974504351615906\n",
      "Epoch 2680/30000 Training Loss: 0.05782347172498703\n",
      "Epoch 2681/30000 Training Loss: 0.06508223712444305\n",
      "Epoch 2682/30000 Training Loss: 0.0718923807144165\n",
      "Epoch 2683/30000 Training Loss: 0.0705128088593483\n",
      "Epoch 2684/30000 Training Loss: 0.06688877940177917\n",
      "Epoch 2685/30000 Training Loss: 0.06207770109176636\n",
      "Epoch 2686/30000 Training Loss: 0.06208373233675957\n",
      "Epoch 2687/30000 Training Loss: 0.06557631492614746\n",
      "Epoch 2688/30000 Training Loss: 0.07137589156627655\n",
      "Epoch 2689/30000 Training Loss: 0.0594291090965271\n",
      "Epoch 2690/30000 Training Loss: 0.06748045235872269\n",
      "Epoch 2691/30000 Training Loss: 0.05751212686300278\n",
      "Epoch 2692/30000 Training Loss: 0.05779552459716797\n",
      "Epoch 2693/30000 Training Loss: 0.06198277324438095\n",
      "Epoch 2694/30000 Training Loss: 0.05770161747932434\n",
      "Epoch 2695/30000 Training Loss: 0.06104881688952446\n",
      "Epoch 2696/30000 Training Loss: 0.05734943225979805\n",
      "Epoch 2697/30000 Training Loss: 0.05957937240600586\n",
      "Epoch 2698/30000 Training Loss: 0.062417007982730865\n",
      "Epoch 2699/30000 Training Loss: 0.06599505990743637\n",
      "Epoch 2700/30000 Training Loss: 0.06352099776268005\n",
      "Epoch 2700/30000 Validation Loss: 0.05538196116685867\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05538196116685867<=============\n",
      "Epoch 2701/30000 Training Loss: 0.061428409069776535\n",
      "Epoch 2702/30000 Training Loss: 0.054778773337602615\n",
      "Epoch 2703/30000 Training Loss: 0.05808135122060776\n",
      "Epoch 2704/30000 Training Loss: 0.0653022974729538\n",
      "Epoch 2705/30000 Training Loss: 0.06274844706058502\n",
      "Epoch 2706/30000 Training Loss: 0.0561683252453804\n",
      "Epoch 2707/30000 Training Loss: 0.06109670549631119\n",
      "Epoch 2708/30000 Training Loss: 0.06570283323526382\n",
      "Epoch 2709/30000 Training Loss: 0.06406520307064056\n",
      "Epoch 2710/30000 Training Loss: 0.06493432074785233\n",
      "Epoch 2711/30000 Training Loss: 0.07304699718952179\n",
      "Epoch 2712/30000 Training Loss: 0.05899300426244736\n",
      "Epoch 2713/30000 Training Loss: 0.05574988201260567\n",
      "Epoch 2714/30000 Training Loss: 0.06640540808439255\n",
      "Epoch 2715/30000 Training Loss: 0.055123187601566315\n",
      "Epoch 2716/30000 Training Loss: 0.0584806390106678\n",
      "Epoch 2717/30000 Training Loss: 0.05922708660364151\n",
      "Epoch 2718/30000 Training Loss: 0.0558653399348259\n",
      "Epoch 2719/30000 Training Loss: 0.05864551663398743\n",
      "Epoch 2720/30000 Training Loss: 0.05619744211435318\n",
      "Epoch 2721/30000 Training Loss: 0.05772137641906738\n",
      "Epoch 2722/30000 Training Loss: 0.0538700595498085\n",
      "Epoch 2723/30000 Training Loss: 0.05886462330818176\n",
      "Epoch 2724/30000 Training Loss: 0.06671831011772156\n",
      "Epoch 2725/30000 Training Loss: 0.056605301797389984\n",
      "Epoch 2726/30000 Training Loss: 0.07185140997171402\n",
      "Epoch 2727/30000 Training Loss: 0.062037039548158646\n",
      "Epoch 2728/30000 Training Loss: 0.06677591800689697\n",
      "Epoch 2729/30000 Training Loss: 0.07537317276000977\n",
      "Epoch 2730/30000 Training Loss: 0.05837639421224594\n",
      "Epoch 2731/30000 Training Loss: 0.05406343191862106\n",
      "Epoch 2732/30000 Training Loss: 0.06511512398719788\n",
      "Epoch 2733/30000 Training Loss: 0.06001545116305351\n",
      "Epoch 2734/30000 Training Loss: 0.06696154922246933\n",
      "Epoch 2735/30000 Training Loss: 0.06300000101327896\n",
      "Epoch 2736/30000 Training Loss: 0.05832359939813614\n",
      "Epoch 2737/30000 Training Loss: 0.06461802124977112\n",
      "Epoch 2738/30000 Training Loss: 0.0652155876159668\n",
      "Epoch 2739/30000 Training Loss: 0.0643756240606308\n",
      "Epoch 2740/30000 Training Loss: 0.06554125249385834\n",
      "Epoch 2741/30000 Training Loss: 0.055414505302906036\n",
      "Epoch 2742/30000 Training Loss: 0.06079442426562309\n",
      "Epoch 2743/30000 Training Loss: 0.05951617285609245\n",
      "Epoch 2744/30000 Training Loss: 0.06292610615491867\n",
      "Epoch 2745/30000 Training Loss: 0.062647745013237\n",
      "Epoch 2746/30000 Training Loss: 0.06440582871437073\n",
      "Epoch 2747/30000 Training Loss: 0.0636720061302185\n",
      "Epoch 2748/30000 Training Loss: 0.055608201771974564\n",
      "Epoch 2749/30000 Training Loss: 0.05830873176455498\n",
      "Epoch 2750/30000 Training Loss: 0.057871777564287186\n",
      "Epoch 2750/30000 Validation Loss: 0.06256996095180511\n",
      "Epoch 2751/30000 Training Loss: 0.06838894635438919\n",
      "Epoch 2752/30000 Training Loss: 0.06458447873592377\n",
      "Epoch 2753/30000 Training Loss: 0.07198357582092285\n",
      "Epoch 2754/30000 Training Loss: 0.06494730710983276\n",
      "Epoch 2755/30000 Training Loss: 0.07245967537164688\n",
      "Epoch 2756/30000 Training Loss: 0.05882015824317932\n",
      "Epoch 2757/30000 Training Loss: 0.06268452852964401\n",
      "Epoch 2758/30000 Training Loss: 0.059300173074007034\n",
      "Epoch 2759/30000 Training Loss: 0.053379207849502563\n",
      "Epoch 2760/30000 Training Loss: 0.060383450239896774\n",
      "Epoch 2761/30000 Training Loss: 0.0606129989027977\n",
      "Epoch 2762/30000 Training Loss: 0.05771014094352722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2763/30000 Training Loss: 0.06055315583944321\n",
      "Epoch 2764/30000 Training Loss: 0.05644938349723816\n",
      "Epoch 2765/30000 Training Loss: 0.06790760159492493\n",
      "Epoch 2766/30000 Training Loss: 0.06264437735080719\n",
      "Epoch 2767/30000 Training Loss: 0.06179990619421005\n",
      "Epoch 2768/30000 Training Loss: 0.06682693958282471\n",
      "Epoch 2769/30000 Training Loss: 0.06229632347822189\n",
      "Epoch 2770/30000 Training Loss: 0.05437344312667847\n",
      "Epoch 2771/30000 Training Loss: 0.06405939161777496\n",
      "Epoch 2772/30000 Training Loss: 0.0627904087305069\n",
      "Epoch 2773/30000 Training Loss: 0.05204349756240845\n",
      "Epoch 2774/30000 Training Loss: 0.07003059983253479\n",
      "Epoch 2775/30000 Training Loss: 0.059827495366334915\n",
      "Epoch 2776/30000 Training Loss: 0.06326434761285782\n",
      "Epoch 2777/30000 Training Loss: 0.06031978130340576\n",
      "Epoch 2778/30000 Training Loss: 0.06367173045873642\n",
      "Epoch 2779/30000 Training Loss: 0.06182898208498955\n",
      "Epoch 2780/30000 Training Loss: 0.05797266960144043\n",
      "Epoch 2781/30000 Training Loss: 0.054676443338394165\n",
      "Epoch 2782/30000 Training Loss: 0.058995552361011505\n",
      "Epoch 2783/30000 Training Loss: 0.05865861847996712\n",
      "Epoch 2784/30000 Training Loss: 0.05816931650042534\n",
      "Epoch 2785/30000 Training Loss: 0.061215661466121674\n",
      "Epoch 2786/30000 Training Loss: 0.0555950403213501\n",
      "Epoch 2787/30000 Training Loss: 0.05280239135026932\n",
      "Epoch 2788/30000 Training Loss: 0.05184221267700195\n",
      "Epoch 2789/30000 Training Loss: 0.05714716389775276\n",
      "Epoch 2790/30000 Training Loss: 0.059777192771434784\n",
      "Epoch 2791/30000 Training Loss: 0.07214679569005966\n",
      "Epoch 2792/30000 Training Loss: 0.05734524875879288\n",
      "Epoch 2793/30000 Training Loss: 0.05826690047979355\n",
      "Epoch 2794/30000 Training Loss: 0.06149597093462944\n",
      "Epoch 2795/30000 Training Loss: 0.06324954330921173\n",
      "Epoch 2796/30000 Training Loss: 0.06627799570560455\n",
      "Epoch 2797/30000 Training Loss: 0.06557440757751465\n",
      "Epoch 2798/30000 Training Loss: 0.06367699801921844\n",
      "Epoch 2799/30000 Training Loss: 0.05495109409093857\n",
      "Epoch 2800/30000 Training Loss: 0.06296304613351822\n",
      "Epoch 2800/30000 Validation Loss: 0.06679047644138336\n",
      "Epoch 2801/30000 Training Loss: 0.05185290053486824\n",
      "Epoch 2802/30000 Training Loss: 0.06590820848941803\n",
      "Epoch 2803/30000 Training Loss: 0.06002644822001457\n",
      "Epoch 2804/30000 Training Loss: 0.06684635579586029\n",
      "Epoch 2805/30000 Training Loss: 0.056695543229579926\n",
      "Epoch 2806/30000 Training Loss: 0.06665591895580292\n",
      "Epoch 2807/30000 Training Loss: 0.06514638662338257\n",
      "Epoch 2808/30000 Training Loss: 0.06507228314876556\n",
      "Epoch 2809/30000 Training Loss: 0.06405244767665863\n",
      "Epoch 2810/30000 Training Loss: 0.059901218861341476\n",
      "Epoch 2811/30000 Training Loss: 0.06921175867319107\n",
      "Epoch 2812/30000 Training Loss: 0.06347166001796722\n",
      "Epoch 2813/30000 Training Loss: 0.05707833915948868\n",
      "Epoch 2814/30000 Training Loss: 0.06314592063426971\n",
      "Epoch 2815/30000 Training Loss: 0.060235489159822464\n",
      "Epoch 2816/30000 Training Loss: 0.06220109015703201\n",
      "Epoch 2817/30000 Training Loss: 0.05806789547204971\n",
      "Epoch 2818/30000 Training Loss: 0.059549786150455475\n",
      "Epoch 2819/30000 Training Loss: 0.061413031071424484\n",
      "Epoch 2820/30000 Training Loss: 0.061783868819475174\n",
      "Epoch 2821/30000 Training Loss: 0.06213078647851944\n",
      "Epoch 2822/30000 Training Loss: 0.06891442835330963\n",
      "Epoch 2823/30000 Training Loss: 0.06589329987764359\n",
      "Epoch 2824/30000 Training Loss: 0.05262775346636772\n",
      "Epoch 2825/30000 Training Loss: 0.08244459331035614\n",
      "Epoch 2826/30000 Training Loss: 0.06115969270467758\n",
      "Epoch 2827/30000 Training Loss: 0.0713316947221756\n",
      "Epoch 2828/30000 Training Loss: 0.06250229477882385\n",
      "Epoch 2829/30000 Training Loss: 0.054635196924209595\n",
      "Epoch 2830/30000 Training Loss: 0.05266297981142998\n",
      "Epoch 2831/30000 Training Loss: 0.059662509709596634\n",
      "Epoch 2832/30000 Training Loss: 0.054318688809871674\n",
      "Epoch 2833/30000 Training Loss: 0.06068789213895798\n",
      "Epoch 2834/30000 Training Loss: 0.05685374140739441\n",
      "Epoch 2835/30000 Training Loss: 0.061428166925907135\n",
      "Epoch 2836/30000 Training Loss: 0.060435064136981964\n",
      "Epoch 2837/30000 Training Loss: 0.06457623094320297\n",
      "Epoch 2838/30000 Training Loss: 0.05880286172032356\n",
      "Epoch 2839/30000 Training Loss: 0.049727536737918854\n",
      "Epoch 2840/30000 Training Loss: 0.0667613297700882\n",
      "Epoch 2841/30000 Training Loss: 0.07584913074970245\n",
      "Epoch 2842/30000 Training Loss: 0.06723960489034653\n",
      "Epoch 2843/30000 Training Loss: 0.052054375410079956\n",
      "Epoch 2844/30000 Training Loss: 0.049455247819423676\n",
      "Epoch 2845/30000 Training Loss: 0.06715323776006699\n",
      "Epoch 2846/30000 Training Loss: 0.06539340317249298\n",
      "Epoch 2847/30000 Training Loss: 0.06465063244104385\n",
      "Epoch 2848/30000 Training Loss: 0.06246824935078621\n",
      "Epoch 2849/30000 Training Loss: 0.05801668018102646\n",
      "Epoch 2850/30000 Training Loss: 0.05544912815093994\n",
      "Epoch 2850/30000 Validation Loss: 0.06005657836794853\n",
      "Epoch 2851/30000 Training Loss: 0.07063444703817368\n",
      "Epoch 2852/30000 Training Loss: 0.0726543515920639\n",
      "Epoch 2853/30000 Training Loss: 0.06155591458082199\n",
      "Epoch 2854/30000 Training Loss: 0.06193497031927109\n",
      "Epoch 2855/30000 Training Loss: 0.06466670334339142\n",
      "Epoch 2856/30000 Training Loss: 0.060576003044843674\n",
      "Epoch 2857/30000 Training Loss: 0.06661271303892136\n",
      "Epoch 2858/30000 Training Loss: 0.06587091088294983\n",
      "Epoch 2859/30000 Training Loss: 0.06098972633481026\n",
      "Epoch 2860/30000 Training Loss: 0.062018781900405884\n",
      "Epoch 2861/30000 Training Loss: 0.05482538416981697\n",
      "Epoch 2862/30000 Training Loss: 0.06135622411966324\n",
      "Epoch 2863/30000 Training Loss: 0.06572563946247101\n",
      "Epoch 2864/30000 Training Loss: 0.06420913338661194\n",
      "Epoch 2865/30000 Training Loss: 0.06019820645451546\n",
      "Epoch 2866/30000 Training Loss: 0.06677133589982986\n",
      "Epoch 2867/30000 Training Loss: 0.05875811725854874\n",
      "Epoch 2868/30000 Training Loss: 0.06267424672842026\n",
      "Epoch 2869/30000 Training Loss: 0.06132812425494194\n",
      "Epoch 2870/30000 Training Loss: 0.05769206956028938\n",
      "Epoch 2871/30000 Training Loss: 0.062368202954530716\n",
      "Epoch 2872/30000 Training Loss: 0.06366221606731415\n",
      "Epoch 2873/30000 Training Loss: 0.061778657138347626\n",
      "Epoch 2874/30000 Training Loss: 0.05322303622961044\n",
      "Epoch 2875/30000 Training Loss: 0.06638139486312866\n",
      "Epoch 2876/30000 Training Loss: 0.05867813900113106\n",
      "Epoch 2877/30000 Training Loss: 0.052404485642910004\n",
      "Epoch 2878/30000 Training Loss: 0.05630258843302727\n",
      "Epoch 2879/30000 Training Loss: 0.0632900819182396\n",
      "Epoch 2880/30000 Training Loss: 0.05749940872192383\n",
      "Epoch 2881/30000 Training Loss: 0.060903459787368774\n",
      "Epoch 2882/30000 Training Loss: 0.06641092151403427\n",
      "Epoch 2883/30000 Training Loss: 0.06234123557806015\n",
      "Epoch 2884/30000 Training Loss: 0.06716909259557724\n",
      "Epoch 2885/30000 Training Loss: 0.06441307067871094\n",
      "Epoch 2886/30000 Training Loss: 0.06542498618364334\n",
      "Epoch 2887/30000 Training Loss: 0.06018015742301941\n",
      "Epoch 2888/30000 Training Loss: 0.05817469209432602\n",
      "Epoch 2889/30000 Training Loss: 0.05723409727215767\n",
      "Epoch 2890/30000 Training Loss: 0.061689555644989014\n",
      "Epoch 2891/30000 Training Loss: 0.058172546327114105\n",
      "Epoch 2892/30000 Training Loss: 0.06710892170667648\n",
      "Epoch 2893/30000 Training Loss: 0.059791553765535355\n",
      "Epoch 2894/30000 Training Loss: 0.05912010744214058\n",
      "Epoch 2895/30000 Training Loss: 0.06270450353622437\n",
      "Epoch 2896/30000 Training Loss: 0.05444604903459549\n",
      "Epoch 2897/30000 Training Loss: 0.07027396559715271\n",
      "Epoch 2898/30000 Training Loss: 0.06885135918855667\n",
      "Epoch 2899/30000 Training Loss: 0.058249227702617645\n",
      "Epoch 2900/30000 Training Loss: 0.05443023890256882\n",
      "Epoch 2900/30000 Validation Loss: 0.05157340317964554\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05157340317964554<=============\n",
      "Epoch 2901/30000 Training Loss: 0.056106530129909515\n",
      "Epoch 2902/30000 Training Loss: 0.0607902891933918\n",
      "Epoch 2903/30000 Training Loss: 0.058006204664707184\n",
      "Epoch 2904/30000 Training Loss: 0.05834968015551567\n",
      "Epoch 2905/30000 Training Loss: 0.05854623392224312\n",
      "Epoch 2906/30000 Training Loss: 0.06375576555728912\n",
      "Epoch 2907/30000 Training Loss: 0.06030632182955742\n",
      "Epoch 2908/30000 Training Loss: 0.05879902094602585\n",
      "Epoch 2909/30000 Training Loss: 0.05452226847410202\n",
      "Epoch 2910/30000 Training Loss: 0.060577742755413055\n",
      "Epoch 2911/30000 Training Loss: 0.0597609281539917\n",
      "Epoch 2912/30000 Training Loss: 0.05825904756784439\n",
      "Epoch 2913/30000 Training Loss: 0.06483875215053558\n",
      "Epoch 2914/30000 Training Loss: 0.05372239276766777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2915/30000 Training Loss: 0.060658764094114304\n",
      "Epoch 2916/30000 Training Loss: 0.06257810443639755\n",
      "Epoch 2917/30000 Training Loss: 0.06147265434265137\n",
      "Epoch 2918/30000 Training Loss: 0.05408760532736778\n",
      "Epoch 2919/30000 Training Loss: 0.06464818865060806\n",
      "Epoch 2920/30000 Training Loss: 0.06484615802764893\n",
      "Epoch 2921/30000 Training Loss: 0.05995675176382065\n",
      "Epoch 2922/30000 Training Loss: 0.0557359978556633\n",
      "Epoch 2923/30000 Training Loss: 0.05690895766019821\n",
      "Epoch 2924/30000 Training Loss: 0.05765046924352646\n",
      "Epoch 2925/30000 Training Loss: 0.05333166569471359\n",
      "Epoch 2926/30000 Training Loss: 0.060938846319913864\n",
      "Epoch 2927/30000 Training Loss: 0.07082588970661163\n",
      "Epoch 2928/30000 Training Loss: 0.06660403311252594\n",
      "Epoch 2929/30000 Training Loss: 0.060208339244127274\n",
      "Epoch 2930/30000 Training Loss: 0.06191102787852287\n",
      "Epoch 2931/30000 Training Loss: 0.06728564947843552\n",
      "Epoch 2932/30000 Training Loss: 0.058829642832279205\n",
      "Epoch 2933/30000 Training Loss: 0.07184380292892456\n",
      "Epoch 2934/30000 Training Loss: 0.06700652837753296\n",
      "Epoch 2935/30000 Training Loss: 0.054355256259441376\n",
      "Epoch 2936/30000 Training Loss: 0.057444702833890915\n",
      "Epoch 2937/30000 Training Loss: 0.06063874810934067\n",
      "Epoch 2938/30000 Training Loss: 0.0655994564294815\n",
      "Epoch 2939/30000 Training Loss: 0.0700346827507019\n",
      "Epoch 2940/30000 Training Loss: 0.05931730940937996\n",
      "Epoch 2941/30000 Training Loss: 0.060206808149814606\n",
      "Epoch 2942/30000 Training Loss: 0.06237039715051651\n",
      "Epoch 2943/30000 Training Loss: 0.06146746873855591\n",
      "Epoch 2944/30000 Training Loss: 0.06032528355717659\n",
      "Epoch 2945/30000 Training Loss: 0.05020556598901749\n",
      "Epoch 2946/30000 Training Loss: 0.0658334344625473\n",
      "Epoch 2947/30000 Training Loss: 0.05903599411249161\n",
      "Epoch 2948/30000 Training Loss: 0.06517063826322556\n",
      "Epoch 2949/30000 Training Loss: 0.06722553819417953\n",
      "Epoch 2950/30000 Training Loss: 0.073279969394207\n",
      "Epoch 2950/30000 Validation Loss: 0.051569610834121704\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.051569610834121704<=============\n",
      "Epoch 2951/30000 Training Loss: 0.06262274086475372\n",
      "Epoch 2952/30000 Training Loss: 0.06365007162094116\n",
      "Epoch 2953/30000 Training Loss: 0.06721117347478867\n",
      "Epoch 2954/30000 Training Loss: 0.05247322842478752\n",
      "Epoch 2955/30000 Training Loss: 0.07234520465135574\n",
      "Epoch 2956/30000 Training Loss: 0.061633072793483734\n",
      "Epoch 2957/30000 Training Loss: 0.05452191084623337\n",
      "Epoch 2958/30000 Training Loss: 0.05412775278091431\n",
      "Epoch 2959/30000 Training Loss: 0.05843091011047363\n",
      "Epoch 2960/30000 Training Loss: 0.06652776151895523\n",
      "Epoch 2961/30000 Training Loss: 0.058882035315036774\n",
      "Epoch 2962/30000 Training Loss: 0.06101544573903084\n",
      "Epoch 2963/30000 Training Loss: 0.05571238324046135\n",
      "Epoch 2964/30000 Training Loss: 0.058931488543748856\n",
      "Epoch 2965/30000 Training Loss: 0.06761389970779419\n",
      "Epoch 2966/30000 Training Loss: 0.06149887293577194\n",
      "Epoch 2967/30000 Training Loss: 0.07124774158000946\n",
      "Epoch 2968/30000 Training Loss: 0.06292303651571274\n",
      "Epoch 2969/30000 Training Loss: 0.0656965970993042\n",
      "Epoch 2970/30000 Training Loss: 0.051584236323833466\n",
      "Epoch 2971/30000 Training Loss: 0.06092425435781479\n",
      "Epoch 2972/30000 Training Loss: 0.055315159261226654\n",
      "Epoch 2973/30000 Training Loss: 0.053941380232572556\n",
      "Epoch 2974/30000 Training Loss: 0.061138816177845\n",
      "Epoch 2975/30000 Training Loss: 0.058557234704494476\n",
      "Epoch 2976/30000 Training Loss: 0.06177208945155144\n",
      "Epoch 2977/30000 Training Loss: 0.06130430847406387\n",
      "Epoch 2978/30000 Training Loss: 0.06609155237674713\n",
      "Epoch 2979/30000 Training Loss: 0.06511802226305008\n",
      "Epoch 2980/30000 Training Loss: 0.05661778897047043\n",
      "Epoch 2981/30000 Training Loss: 0.06189769506454468\n",
      "Epoch 2982/30000 Training Loss: 0.060672443360090256\n",
      "Epoch 2983/30000 Training Loss: 0.05609825253486633\n",
      "Epoch 2984/30000 Training Loss: 0.07006941735744476\n",
      "Epoch 2985/30000 Training Loss: 0.06195374205708504\n",
      "Epoch 2986/30000 Training Loss: 0.059963516891002655\n",
      "Epoch 2987/30000 Training Loss: 0.062244050204753876\n",
      "Epoch 2988/30000 Training Loss: 0.053005218505859375\n",
      "Epoch 2989/30000 Training Loss: 0.0587138906121254\n",
      "Epoch 2990/30000 Training Loss: 0.06626926362514496\n",
      "Epoch 2991/30000 Training Loss: 0.06491409242153168\n",
      "Epoch 2992/30000 Training Loss: 0.05854455754160881\n",
      "Epoch 2993/30000 Training Loss: 0.058307088911533356\n",
      "Epoch 2994/30000 Training Loss: 0.06173529475927353\n",
      "Epoch 2995/30000 Training Loss: 0.05821901559829712\n",
      "Epoch 2996/30000 Training Loss: 0.06393951922655106\n",
      "Epoch 2997/30000 Training Loss: 0.07067252695560455\n",
      "Epoch 2998/30000 Training Loss: 0.052159618586301804\n",
      "Epoch 2999/30000 Training Loss: 0.05780944973230362\n",
      "Epoch 3000/30000 Training Loss: 0.06153012067079544\n",
      "Epoch 3000/30000 Validation Loss: 0.05816084146499634\n",
      "Epoch 3001/30000 Training Loss: 0.06121652573347092\n",
      "Epoch 3002/30000 Training Loss: 0.06427482515573502\n",
      "Epoch 3003/30000 Training Loss: 0.06499318778514862\n",
      "Epoch 3004/30000 Training Loss: 0.06400786340236664\n",
      "Epoch 3005/30000 Training Loss: 0.06417108327150345\n",
      "Epoch 3006/30000 Training Loss: 0.0628819689154625\n",
      "Epoch 3007/30000 Training Loss: 0.0579998716711998\n",
      "Epoch 3008/30000 Training Loss: 0.06922025978565216\n",
      "Epoch 3009/30000 Training Loss: 0.06575416028499603\n",
      "Epoch 3010/30000 Training Loss: 0.053517527878284454\n",
      "Epoch 3011/30000 Training Loss: 0.0666576474905014\n",
      "Epoch 3012/30000 Training Loss: 0.060965366661548615\n",
      "Epoch 3013/30000 Training Loss: 0.048692893236875534\n",
      "Epoch 3014/30000 Training Loss: 0.06719338893890381\n",
      "Epoch 3015/30000 Training Loss: 0.05550331994891167\n",
      "Epoch 3016/30000 Training Loss: 0.06317925453186035\n",
      "Epoch 3017/30000 Training Loss: 0.05984373018145561\n",
      "Epoch 3018/30000 Training Loss: 0.05993950366973877\n",
      "Epoch 3019/30000 Training Loss: 0.0535365454852581\n",
      "Epoch 3020/30000 Training Loss: 0.052593398839235306\n",
      "Epoch 3021/30000 Training Loss: 0.06593216955661774\n",
      "Epoch 3022/30000 Training Loss: 0.06946686655282974\n",
      "Epoch 3023/30000 Training Loss: 0.06195906549692154\n",
      "Epoch 3024/30000 Training Loss: 0.05861632898449898\n",
      "Epoch 3025/30000 Training Loss: 0.05820203945040703\n",
      "Epoch 3026/30000 Training Loss: 0.0549963042140007\n",
      "Epoch 3027/30000 Training Loss: 0.06726713478565216\n",
      "Epoch 3028/30000 Training Loss: 0.05841071531176567\n",
      "Epoch 3029/30000 Training Loss: 0.059245843440294266\n",
      "Epoch 3030/30000 Training Loss: 0.06793618947267532\n",
      "Epoch 3031/30000 Training Loss: 0.06233073025941849\n",
      "Epoch 3032/30000 Training Loss: 0.05471767112612724\n",
      "Epoch 3033/30000 Training Loss: 0.06913227587938309\n",
      "Epoch 3034/30000 Training Loss: 0.06762871891260147\n",
      "Epoch 3035/30000 Training Loss: 0.05291898921132088\n",
      "Epoch 3036/30000 Training Loss: 0.061645060777664185\n",
      "Epoch 3037/30000 Training Loss: 0.04970870167016983\n",
      "Epoch 3038/30000 Training Loss: 0.06433207541704178\n",
      "Epoch 3039/30000 Training Loss: 0.06516478210687637\n",
      "Epoch 3040/30000 Training Loss: 0.05615752190351486\n",
      "Epoch 3041/30000 Training Loss: 0.06280424445867538\n",
      "Epoch 3042/30000 Training Loss: 0.056625742465257645\n",
      "Epoch 3043/30000 Training Loss: 0.06097748875617981\n",
      "Epoch 3044/30000 Training Loss: 0.05884052440524101\n",
      "Epoch 3045/30000 Training Loss: 0.0658659115433693\n",
      "Epoch 3046/30000 Training Loss: 0.06159316748380661\n",
      "Epoch 3047/30000 Training Loss: 0.05309449881315231\n",
      "Epoch 3048/30000 Training Loss: 0.06649081408977509\n",
      "Epoch 3049/30000 Training Loss: 0.06407023221254349\n",
      "Epoch 3050/30000 Training Loss: 0.05744556710124016\n",
      "Epoch 3050/30000 Validation Loss: 0.0580144040286541\n",
      "Epoch 3051/30000 Training Loss: 0.06362705677747726\n",
      "Epoch 3052/30000 Training Loss: 0.060634929686784744\n",
      "Epoch 3053/30000 Training Loss: 0.06463001668453217\n",
      "Epoch 3054/30000 Training Loss: 0.05662227421998978\n",
      "Epoch 3055/30000 Training Loss: 0.060069017112255096\n",
      "Epoch 3056/30000 Training Loss: 0.06861647218465805\n",
      "Epoch 3057/30000 Training Loss: 0.060378558933734894\n",
      "Epoch 3058/30000 Training Loss: 0.0685199499130249\n",
      "Epoch 3059/30000 Training Loss: 0.06775714457035065\n",
      "Epoch 3060/30000 Training Loss: 0.053259097039699554\n",
      "Epoch 3061/30000 Training Loss: 0.06928713619709015\n",
      "Epoch 3062/30000 Training Loss: 0.07101067155599594\n",
      "Epoch 3063/30000 Training Loss: 0.07140311598777771\n",
      "Epoch 3064/30000 Training Loss: 0.05477254465222359\n",
      "Epoch 3065/30000 Training Loss: 0.05674876645207405\n",
      "Epoch 3066/30000 Training Loss: 0.05568519979715347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3067/30000 Training Loss: 0.05311354249715805\n",
      "Epoch 3068/30000 Training Loss: 0.056703705340623856\n",
      "Epoch 3069/30000 Training Loss: 0.0668821707367897\n",
      "Epoch 3070/30000 Training Loss: 0.05418514460325241\n",
      "Epoch 3071/30000 Training Loss: 0.06734520196914673\n",
      "Epoch 3072/30000 Training Loss: 0.05839425325393677\n",
      "Epoch 3073/30000 Training Loss: 0.047342099249362946\n",
      "Epoch 3074/30000 Training Loss: 0.06483857333660126\n",
      "Epoch 3075/30000 Training Loss: 0.05744599178433418\n",
      "Epoch 3076/30000 Training Loss: 0.05938060209155083\n",
      "Epoch 3077/30000 Training Loss: 0.05864961817860603\n",
      "Epoch 3078/30000 Training Loss: 0.05467723682522774\n",
      "Epoch 3079/30000 Training Loss: 0.06972017139196396\n",
      "Epoch 3080/30000 Training Loss: 0.060086559504270554\n",
      "Epoch 3081/30000 Training Loss: 0.06316090375185013\n",
      "Epoch 3082/30000 Training Loss: 0.0529317669570446\n",
      "Epoch 3083/30000 Training Loss: 0.059317074716091156\n",
      "Epoch 3084/30000 Training Loss: 0.04862472414970398\n",
      "Epoch 3085/30000 Training Loss: 0.05753384158015251\n",
      "Epoch 3086/30000 Training Loss: 0.058750443160533905\n",
      "Epoch 3087/30000 Training Loss: 0.06841249763965607\n",
      "Epoch 3088/30000 Training Loss: 0.05668995901942253\n",
      "Epoch 3089/30000 Training Loss: 0.05514568090438843\n",
      "Epoch 3090/30000 Training Loss: 0.06574518978595734\n",
      "Epoch 3091/30000 Training Loss: 0.0661102682352066\n",
      "Epoch 3092/30000 Training Loss: 0.060805898159742355\n",
      "Epoch 3093/30000 Training Loss: 0.052340101450681686\n",
      "Epoch 3094/30000 Training Loss: 0.05298929288983345\n",
      "Epoch 3095/30000 Training Loss: 0.05679725483059883\n",
      "Epoch 3096/30000 Training Loss: 0.05366096645593643\n",
      "Epoch 3097/30000 Training Loss: 0.05667240172624588\n",
      "Epoch 3098/30000 Training Loss: 0.058893729001283646\n",
      "Epoch 3099/30000 Training Loss: 0.06303327530622482\n",
      "Epoch 3100/30000 Training Loss: 0.052790701389312744\n",
      "Epoch 3100/30000 Validation Loss: 0.053828127682209015\n",
      "Epoch 3101/30000 Training Loss: 0.062087077647447586\n",
      "Epoch 3102/30000 Training Loss: 0.05340629070997238\n",
      "Epoch 3103/30000 Training Loss: 0.062208451330661774\n",
      "Epoch 3104/30000 Training Loss: 0.06981152296066284\n",
      "Epoch 3105/30000 Training Loss: 0.06106548383831978\n",
      "Epoch 3106/30000 Training Loss: 0.058401696383953094\n",
      "Epoch 3107/30000 Training Loss: 0.06441885232925415\n",
      "Epoch 3108/30000 Training Loss: 0.05905988812446594\n",
      "Epoch 3109/30000 Training Loss: 0.05845086649060249\n",
      "Epoch 3110/30000 Training Loss: 0.06191755086183548\n",
      "Epoch 3111/30000 Training Loss: 0.060164980590343475\n",
      "Epoch 3112/30000 Training Loss: 0.05712296441197395\n",
      "Epoch 3113/30000 Training Loss: 0.06588824838399887\n",
      "Epoch 3114/30000 Training Loss: 0.052993256598711014\n",
      "Epoch 3115/30000 Training Loss: 0.0551009364426136\n",
      "Epoch 3116/30000 Training Loss: 0.06080322340130806\n",
      "Epoch 3117/30000 Training Loss: 0.06822280585765839\n",
      "Epoch 3118/30000 Training Loss: 0.06500768661499023\n",
      "Epoch 3119/30000 Training Loss: 0.05048869177699089\n",
      "Epoch 3120/30000 Training Loss: 0.04991503804922104\n",
      "Epoch 3121/30000 Training Loss: 0.0637892410159111\n",
      "Epoch 3122/30000 Training Loss: 0.0578533299267292\n",
      "Epoch 3123/30000 Training Loss: 0.06328718364238739\n",
      "Epoch 3124/30000 Training Loss: 0.06553302705287933\n",
      "Epoch 3125/30000 Training Loss: 0.0635414570569992\n",
      "Epoch 3126/30000 Training Loss: 0.053013987839221954\n",
      "Epoch 3127/30000 Training Loss: 0.06670007854700089\n",
      "Epoch 3128/30000 Training Loss: 0.0622311532497406\n",
      "Epoch 3129/30000 Training Loss: 0.06324945390224457\n",
      "Epoch 3130/30000 Training Loss: 0.05651651695370674\n",
      "Epoch 3131/30000 Training Loss: 0.05424666404724121\n",
      "Epoch 3132/30000 Training Loss: 0.05758259817957878\n",
      "Epoch 3133/30000 Training Loss: 0.05474964901804924\n",
      "Epoch 3134/30000 Training Loss: 0.06282554566860199\n",
      "Epoch 3135/30000 Training Loss: 0.05648674815893173\n",
      "Epoch 3136/30000 Training Loss: 0.06027161329984665\n",
      "Epoch 3137/30000 Training Loss: 0.05288839340209961\n",
      "Epoch 3138/30000 Training Loss: 0.0588056817650795\n",
      "Epoch 3139/30000 Training Loss: 0.062466979026794434\n",
      "Epoch 3140/30000 Training Loss: 0.06901480257511139\n",
      "Epoch 3141/30000 Training Loss: 0.05477721244096756\n",
      "Epoch 3142/30000 Training Loss: 0.05982866883277893\n",
      "Epoch 3143/30000 Training Loss: 0.06007726117968559\n",
      "Epoch 3144/30000 Training Loss: 0.0645216852426529\n",
      "Epoch 3145/30000 Training Loss: 0.06678663194179535\n",
      "Epoch 3146/30000 Training Loss: 0.05871473625302315\n",
      "Epoch 3147/30000 Training Loss: 0.0651065930724144\n",
      "Epoch 3148/30000 Training Loss: 0.055463410913944244\n",
      "Epoch 3149/30000 Training Loss: 0.05314980819821358\n",
      "Epoch 3150/30000 Training Loss: 0.05320344120264053\n",
      "Epoch 3150/30000 Validation Loss: 0.05821025371551514\n",
      "Epoch 3151/30000 Training Loss: 0.06861715763807297\n",
      "Epoch 3152/30000 Training Loss: 0.05510696768760681\n",
      "Epoch 3153/30000 Training Loss: 0.04650118201971054\n",
      "Epoch 3154/30000 Training Loss: 0.06247042864561081\n",
      "Epoch 3155/30000 Training Loss: 0.06159893423318863\n",
      "Epoch 3156/30000 Training Loss: 0.058239005506038666\n",
      "Epoch 3157/30000 Training Loss: 0.058543138206005096\n",
      "Epoch 3158/30000 Training Loss: 0.05453932285308838\n",
      "Epoch 3159/30000 Training Loss: 0.0606597438454628\n",
      "Epoch 3160/30000 Training Loss: 0.05725731700658798\n",
      "Epoch 3161/30000 Training Loss: 0.05180593207478523\n",
      "Epoch 3162/30000 Training Loss: 0.0630614161491394\n",
      "Epoch 3163/30000 Training Loss: 0.06021193414926529\n",
      "Epoch 3164/30000 Training Loss: 0.058912329375743866\n",
      "Epoch 3165/30000 Training Loss: 0.06088077276945114\n",
      "Epoch 3166/30000 Training Loss: 0.07093895971775055\n",
      "Epoch 3167/30000 Training Loss: 0.06283030658960342\n",
      "Epoch 3168/30000 Training Loss: 0.055075753480196\n",
      "Epoch 3169/30000 Training Loss: 0.050717033445835114\n",
      "Epoch 3170/30000 Training Loss: 0.06279407441616058\n",
      "Epoch 3171/30000 Training Loss: 0.059497199952602386\n",
      "Epoch 3172/30000 Training Loss: 0.053534816950559616\n",
      "Epoch 3173/30000 Training Loss: 0.06876395642757416\n",
      "Epoch 3174/30000 Training Loss: 0.053401947021484375\n",
      "Epoch 3175/30000 Training Loss: 0.06318175792694092\n",
      "Epoch 3176/30000 Training Loss: 0.05091438442468643\n",
      "Epoch 3177/30000 Training Loss: 0.05557320639491081\n",
      "Epoch 3178/30000 Training Loss: 0.06648609787225723\n",
      "Epoch 3179/30000 Training Loss: 0.06323208659887314\n",
      "Epoch 3180/30000 Training Loss: 0.07365839183330536\n",
      "Epoch 3181/30000 Training Loss: 0.05786261707544327\n",
      "Epoch 3182/30000 Training Loss: 0.06477787345647812\n",
      "Epoch 3183/30000 Training Loss: 0.0666075274348259\n",
      "Epoch 3184/30000 Training Loss: 0.060564011335372925\n",
      "Epoch 3185/30000 Training Loss: 0.05564575642347336\n",
      "Epoch 3186/30000 Training Loss: 0.0561513714492321\n",
      "Epoch 3187/30000 Training Loss: 0.061807017773389816\n",
      "Epoch 3188/30000 Training Loss: 0.0588751956820488\n",
      "Epoch 3189/30000 Training Loss: 0.05538363382220268\n",
      "Epoch 3190/30000 Training Loss: 0.05943187326192856\n",
      "Epoch 3191/30000 Training Loss: 0.05312445014715195\n",
      "Epoch 3192/30000 Training Loss: 0.06390617787837982\n",
      "Epoch 3193/30000 Training Loss: 0.05857126787304878\n",
      "Epoch 3194/30000 Training Loss: 0.06771470606327057\n",
      "Epoch 3195/30000 Training Loss: 0.06304318457841873\n",
      "Epoch 3196/30000 Training Loss: 0.05715343356132507\n",
      "Epoch 3197/30000 Training Loss: 0.056645117700099945\n",
      "Epoch 3198/30000 Training Loss: 0.05396437644958496\n",
      "Epoch 3199/30000 Training Loss: 0.057296834886074066\n",
      "Epoch 3200/30000 Training Loss: 0.06760767847299576\n",
      "Epoch 3200/30000 Validation Loss: 0.059137649834156036\n",
      "Epoch 3201/30000 Training Loss: 0.049551140516996384\n",
      "Epoch 3202/30000 Training Loss: 0.06425787508487701\n",
      "Epoch 3203/30000 Training Loss: 0.06434604525566101\n",
      "Epoch 3204/30000 Training Loss: 0.053345195949077606\n",
      "Epoch 3205/30000 Training Loss: 0.07589691877365112\n",
      "Epoch 3206/30000 Training Loss: 0.05519157648086548\n",
      "Epoch 3207/30000 Training Loss: 0.05772405117750168\n",
      "Epoch 3208/30000 Training Loss: 0.06785931438207626\n",
      "Epoch 3209/30000 Training Loss: 0.06105908751487732\n",
      "Epoch 3210/30000 Training Loss: 0.0558050274848938\n",
      "Epoch 3211/30000 Training Loss: 0.05206341668963432\n",
      "Epoch 3212/30000 Training Loss: 0.05533469468355179\n",
      "Epoch 3213/30000 Training Loss: 0.055702775716781616\n",
      "Epoch 3214/30000 Training Loss: 0.06058487296104431\n",
      "Epoch 3215/30000 Training Loss: 0.060648806393146515\n",
      "Epoch 3216/30000 Training Loss: 0.054619282484054565\n",
      "Epoch 3217/30000 Training Loss: 0.05227436497807503\n",
      "Epoch 3218/30000 Training Loss: 0.05881519988179207\n",
      "Epoch 3219/30000 Training Loss: 0.0690540224313736\n",
      "Epoch 3220/30000 Training Loss: 0.05320992320775986\n",
      "Epoch 3221/30000 Training Loss: 0.05807463452219963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3222/30000 Training Loss: 0.04622723534703255\n",
      "Epoch 3223/30000 Training Loss: 0.07302708923816681\n",
      "Epoch 3224/30000 Training Loss: 0.06641916930675507\n",
      "Epoch 3225/30000 Training Loss: 0.05284382030367851\n",
      "Epoch 3226/30000 Training Loss: 0.0485164076089859\n",
      "Epoch 3227/30000 Training Loss: 0.06212002784013748\n",
      "Epoch 3228/30000 Training Loss: 0.06658195704221725\n",
      "Epoch 3229/30000 Training Loss: 0.06967166811227798\n",
      "Epoch 3230/30000 Training Loss: 0.057967089116573334\n",
      "Epoch 3231/30000 Training Loss: 0.06127631664276123\n",
      "Epoch 3232/30000 Training Loss: 0.05694027990102768\n",
      "Epoch 3233/30000 Training Loss: 0.05580354854464531\n",
      "Epoch 3234/30000 Training Loss: 0.05724027752876282\n",
      "Epoch 3235/30000 Training Loss: 0.06944525986909866\n",
      "Epoch 3236/30000 Training Loss: 0.052791714668273926\n",
      "Epoch 3237/30000 Training Loss: 0.05841439962387085\n",
      "Epoch 3238/30000 Training Loss: 0.0625360980629921\n",
      "Epoch 3239/30000 Training Loss: 0.058786652982234955\n",
      "Epoch 3240/30000 Training Loss: 0.06472644954919815\n",
      "Epoch 3241/30000 Training Loss: 0.062067072838544846\n",
      "Epoch 3242/30000 Training Loss: 0.07601681351661682\n",
      "Epoch 3243/30000 Training Loss: 0.06146886944770813\n",
      "Epoch 3244/30000 Training Loss: 0.0671030730009079\n",
      "Epoch 3245/30000 Training Loss: 0.05939456820487976\n",
      "Epoch 3246/30000 Training Loss: 0.048327572643756866\n",
      "Epoch 3247/30000 Training Loss: 0.06557044386863708\n",
      "Epoch 3248/30000 Training Loss: 0.06468965113162994\n",
      "Epoch 3249/30000 Training Loss: 0.06239018589258194\n",
      "Epoch 3250/30000 Training Loss: 0.05709279328584671\n",
      "Epoch 3250/30000 Validation Loss: 0.06759136915206909\n",
      "Epoch 3251/30000 Training Loss: 0.054308462888002396\n",
      "Epoch 3252/30000 Training Loss: 0.05839811637997627\n",
      "Epoch 3253/30000 Training Loss: 0.05987776443362236\n",
      "Epoch 3254/30000 Training Loss: 0.06384613364934921\n",
      "Epoch 3255/30000 Training Loss: 0.06147695332765579\n",
      "Epoch 3256/30000 Training Loss: 0.06249171495437622\n",
      "Epoch 3257/30000 Training Loss: 0.05854623392224312\n",
      "Epoch 3258/30000 Training Loss: 0.055108923465013504\n",
      "Epoch 3259/30000 Training Loss: 0.05835982412099838\n",
      "Epoch 3260/30000 Training Loss: 0.06286617368459702\n",
      "Epoch 3261/30000 Training Loss: 0.05554399639368057\n",
      "Epoch 3262/30000 Training Loss: 0.05618773773312569\n",
      "Epoch 3263/30000 Training Loss: 0.050494156777858734\n",
      "Epoch 3264/30000 Training Loss: 0.060809023678302765\n",
      "Epoch 3265/30000 Training Loss: 0.07408666610717773\n",
      "Epoch 3266/30000 Training Loss: 0.06102197617292404\n",
      "Epoch 3267/30000 Training Loss: 0.05882202833890915\n",
      "Epoch 3268/30000 Training Loss: 0.0536712221801281\n",
      "Epoch 3269/30000 Training Loss: 0.05975968390703201\n",
      "Epoch 3270/30000 Training Loss: 0.06476417928934097\n",
      "Epoch 3271/30000 Training Loss: 0.060916997492313385\n",
      "Epoch 3272/30000 Training Loss: 0.05430982634425163\n",
      "Epoch 3273/30000 Training Loss: 0.06162208318710327\n",
      "Epoch 3274/30000 Training Loss: 0.052759986370801926\n",
      "Epoch 3275/30000 Training Loss: 0.072004035115242\n",
      "Epoch 3276/30000 Training Loss: 0.07145629823207855\n",
      "Epoch 3277/30000 Training Loss: 0.0635179802775383\n",
      "Epoch 3278/30000 Training Loss: 0.05921401455998421\n",
      "Epoch 3279/30000 Training Loss: 0.06314867734909058\n",
      "Epoch 3280/30000 Training Loss: 0.054666917771101\n",
      "Epoch 3281/30000 Training Loss: 0.052990157157182693\n",
      "Epoch 3282/30000 Training Loss: 0.052856188267469406\n",
      "Epoch 3283/30000 Training Loss: 0.06169542670249939\n",
      "Epoch 3284/30000 Training Loss: 0.05685361102223396\n",
      "Epoch 3285/30000 Training Loss: 0.057847268879413605\n",
      "Epoch 3286/30000 Training Loss: 0.06500039994716644\n",
      "Epoch 3287/30000 Training Loss: 0.05539610981941223\n",
      "Epoch 3288/30000 Training Loss: 0.05758087709546089\n",
      "Epoch 3289/30000 Training Loss: 0.06142720580101013\n",
      "Epoch 3290/30000 Training Loss: 0.057210564613342285\n",
      "Epoch 3291/30000 Training Loss: 0.06251217424869537\n",
      "Epoch 3292/30000 Training Loss: 0.06115832179784775\n",
      "Epoch 3293/30000 Training Loss: 0.059352606534957886\n",
      "Epoch 3294/30000 Training Loss: 0.05216003209352493\n",
      "Epoch 3295/30000 Training Loss: 0.05684170126914978\n",
      "Epoch 3296/30000 Training Loss: 0.058207474648952484\n",
      "Epoch 3297/30000 Training Loss: 0.05794321745634079\n",
      "Epoch 3298/30000 Training Loss: 0.057711340487003326\n",
      "Epoch 3299/30000 Training Loss: 0.07713841646909714\n",
      "Epoch 3300/30000 Training Loss: 0.058076776564121246\n",
      "Epoch 3300/30000 Validation Loss: 0.06536604464054108\n",
      "Epoch 3301/30000 Training Loss: 0.056328870356082916\n",
      "Epoch 3302/30000 Training Loss: 0.06295618414878845\n",
      "Epoch 3303/30000 Training Loss: 0.06322912871837616\n",
      "Epoch 3304/30000 Training Loss: 0.06707055866718292\n",
      "Epoch 3305/30000 Training Loss: 0.061295606195926666\n",
      "Epoch 3306/30000 Training Loss: 0.06789180636405945\n",
      "Epoch 3307/30000 Training Loss: 0.06347145140171051\n",
      "Epoch 3308/30000 Training Loss: 0.05035749077796936\n",
      "Epoch 3309/30000 Training Loss: 0.05852421000599861\n",
      "Epoch 3310/30000 Training Loss: 0.06273756921291351\n",
      "Epoch 3311/30000 Training Loss: 0.04972301051020622\n",
      "Epoch 3312/30000 Training Loss: 0.05958898738026619\n",
      "Epoch 3313/30000 Training Loss: 0.05392605811357498\n",
      "Epoch 3314/30000 Training Loss: 0.049829043447971344\n",
      "Epoch 3315/30000 Training Loss: 0.05750591307878494\n",
      "Epoch 3316/30000 Training Loss: 0.056382160633802414\n",
      "Epoch 3317/30000 Training Loss: 0.055033206939697266\n",
      "Epoch 3318/30000 Training Loss: 0.057898420840501785\n",
      "Epoch 3319/30000 Training Loss: 0.05034639686346054\n",
      "Epoch 3320/30000 Training Loss: 0.059913069009780884\n",
      "Epoch 3321/30000 Training Loss: 0.05873067304491997\n",
      "Epoch 3322/30000 Training Loss: 0.06101185083389282\n",
      "Epoch 3323/30000 Training Loss: 0.06858363002538681\n",
      "Epoch 3324/30000 Training Loss: 0.055546145886182785\n",
      "Epoch 3325/30000 Training Loss: 0.05525773763656616\n",
      "Epoch 3326/30000 Training Loss: 0.06412731111049652\n",
      "Epoch 3327/30000 Training Loss: 0.05736213177442551\n",
      "Epoch 3328/30000 Training Loss: 0.05399712920188904\n",
      "Epoch 3329/30000 Training Loss: 0.06163856387138367\n",
      "Epoch 3330/30000 Training Loss: 0.0606047585606575\n",
      "Epoch 3331/30000 Training Loss: 0.053522009402513504\n",
      "Epoch 3332/30000 Training Loss: 0.061184801161289215\n",
      "Epoch 3333/30000 Training Loss: 0.06121610850095749\n",
      "Epoch 3334/30000 Training Loss: 0.05920635908842087\n",
      "Epoch 3335/30000 Training Loss: 0.05649377033114433\n",
      "Epoch 3336/30000 Training Loss: 0.05863124877214432\n",
      "Epoch 3337/30000 Training Loss: 0.05743755027651787\n",
      "Epoch 3338/30000 Training Loss: 0.05476280301809311\n",
      "Epoch 3339/30000 Training Loss: 0.07680036127567291\n",
      "Epoch 3340/30000 Training Loss: 0.058576762676239014\n",
      "Epoch 3341/30000 Training Loss: 0.0600726418197155\n",
      "Epoch 3342/30000 Training Loss: 0.06321271508932114\n",
      "Epoch 3343/30000 Training Loss: 0.06059864163398743\n",
      "Epoch 3344/30000 Training Loss: 0.0662103220820427\n",
      "Epoch 3345/30000 Training Loss: 0.05116992071270943\n",
      "Epoch 3346/30000 Training Loss: 0.06019148975610733\n",
      "Epoch 3347/30000 Training Loss: 0.07057473808526993\n",
      "Epoch 3348/30000 Training Loss: 0.054433636367321014\n",
      "Epoch 3349/30000 Training Loss: 0.062063753604888916\n",
      "Epoch 3350/30000 Training Loss: 0.06728807836771011\n",
      "Epoch 3350/30000 Validation Loss: 0.05875595659017563\n",
      "Epoch 3351/30000 Training Loss: 0.0638478472828865\n",
      "Epoch 3352/30000 Training Loss: 0.05864819139242172\n",
      "Epoch 3353/30000 Training Loss: 0.058593910187482834\n",
      "Epoch 3354/30000 Training Loss: 0.06381671130657196\n",
      "Epoch 3355/30000 Training Loss: 0.07514575868844986\n",
      "Epoch 3356/30000 Training Loss: 0.052716631442308426\n",
      "Epoch 3357/30000 Training Loss: 0.05797442048788071\n",
      "Epoch 3358/30000 Training Loss: 0.06613465398550034\n",
      "Epoch 3359/30000 Training Loss: 0.05736595392227173\n",
      "Epoch 3360/30000 Training Loss: 0.063343845307827\n",
      "Epoch 3361/30000 Training Loss: 0.06028399616479874\n",
      "Epoch 3362/30000 Training Loss: 0.0563477985560894\n",
      "Epoch 3363/30000 Training Loss: 0.058427415788173676\n",
      "Epoch 3364/30000 Training Loss: 0.06363961845636368\n",
      "Epoch 3365/30000 Training Loss: 0.05996183305978775\n",
      "Epoch 3366/30000 Training Loss: 0.0562894232571125\n",
      "Epoch 3367/30000 Training Loss: 0.061061494052410126\n",
      "Epoch 3368/30000 Training Loss: 0.06143512204289436\n",
      "Epoch 3369/30000 Training Loss: 0.06453351676464081\n",
      "Epoch 3370/30000 Training Loss: 0.06213110685348511\n",
      "Epoch 3371/30000 Training Loss: 0.06227082014083862\n",
      "Epoch 3372/30000 Training Loss: 0.05524120479822159\n",
      "Epoch 3373/30000 Training Loss: 0.06576687097549438\n",
      "Epoch 3374/30000 Training Loss: 0.05147738382220268\n",
      "Epoch 3375/30000 Training Loss: 0.06268303096294403\n",
      "Epoch 3376/30000 Training Loss: 0.06071484088897705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3377/30000 Training Loss: 0.060811907052993774\n",
      "Epoch 3378/30000 Training Loss: 0.05269138887524605\n",
      "Epoch 3379/30000 Training Loss: 0.058731209486722946\n",
      "Epoch 3380/30000 Training Loss: 0.05032869428396225\n",
      "Epoch 3381/30000 Training Loss: 0.05870836228132248\n",
      "Epoch 3382/30000 Training Loss: 0.06401615589857101\n",
      "Epoch 3383/30000 Training Loss: 0.07279787957668304\n",
      "Epoch 3384/30000 Training Loss: 0.06475595384836197\n",
      "Epoch 3385/30000 Training Loss: 0.05991161987185478\n",
      "Epoch 3386/30000 Training Loss: 0.05942578986287117\n",
      "Epoch 3387/30000 Training Loss: 0.061406929045915604\n",
      "Epoch 3388/30000 Training Loss: 0.05872661992907524\n",
      "Epoch 3389/30000 Training Loss: 0.047955457121133804\n",
      "Epoch 3390/30000 Training Loss: 0.05279616266489029\n",
      "Epoch 3391/30000 Training Loss: 0.0618138387799263\n",
      "Epoch 3392/30000 Training Loss: 0.05242224410176277\n",
      "Epoch 3393/30000 Training Loss: 0.058985184878110886\n",
      "Epoch 3394/30000 Training Loss: 0.05943158268928528\n",
      "Epoch 3395/30000 Training Loss: 0.06057683378458023\n",
      "Epoch 3396/30000 Training Loss: 0.05687727406620979\n",
      "Epoch 3397/30000 Training Loss: 0.057229332625865936\n",
      "Epoch 3398/30000 Training Loss: 0.05512382462620735\n",
      "Epoch 3399/30000 Training Loss: 0.05612108111381531\n",
      "Epoch 3400/30000 Training Loss: 0.05871713161468506\n",
      "Epoch 3400/30000 Validation Loss: 0.05858851224184036\n",
      "Epoch 3401/30000 Training Loss: 0.05364251136779785\n",
      "Epoch 3402/30000 Training Loss: 0.05611727386713028\n",
      "Epoch 3403/30000 Training Loss: 0.06496888399124146\n",
      "Epoch 3404/30000 Training Loss: 0.06169508025050163\n",
      "Epoch 3405/30000 Training Loss: 0.059976231306791306\n",
      "Epoch 3406/30000 Training Loss: 0.04837845638394356\n",
      "Epoch 3407/30000 Training Loss: 0.0545857772231102\n",
      "Epoch 3408/30000 Training Loss: 0.04917454347014427\n",
      "Epoch 3409/30000 Training Loss: 0.05791381746530533\n",
      "Epoch 3410/30000 Training Loss: 0.055493734776973724\n",
      "Epoch 3411/30000 Training Loss: 0.062389083206653595\n",
      "Epoch 3412/30000 Training Loss: 0.05913563817739487\n",
      "Epoch 3413/30000 Training Loss: 0.05580786615610123\n",
      "Epoch 3414/30000 Training Loss: 0.05623906850814819\n",
      "Epoch 3415/30000 Training Loss: 0.055065445601940155\n",
      "Epoch 3416/30000 Training Loss: 0.05177553743124008\n",
      "Epoch 3417/30000 Training Loss: 0.06316163390874863\n",
      "Epoch 3418/30000 Training Loss: 0.04990395903587341\n",
      "Epoch 3419/30000 Training Loss: 0.05987109988927841\n",
      "Epoch 3420/30000 Training Loss: 0.05814871937036514\n",
      "Epoch 3421/30000 Training Loss: 0.05295606702566147\n",
      "Epoch 3422/30000 Training Loss: 0.06055790185928345\n",
      "Epoch 3423/30000 Training Loss: 0.051212213933467865\n",
      "Epoch 3424/30000 Training Loss: 0.05393998697400093\n",
      "Epoch 3425/30000 Training Loss: 0.06222035363316536\n",
      "Epoch 3426/30000 Training Loss: 0.050935517996549606\n",
      "Epoch 3427/30000 Training Loss: 0.06513768434524536\n",
      "Epoch 3428/30000 Training Loss: 0.06160660833120346\n",
      "Epoch 3429/30000 Training Loss: 0.05097290128469467\n",
      "Epoch 3430/30000 Training Loss: 0.06286845356225967\n",
      "Epoch 3431/30000 Training Loss: 0.057906635105609894\n",
      "Epoch 3432/30000 Training Loss: 0.05985786393284798\n",
      "Epoch 3433/30000 Training Loss: 0.05577857419848442\n",
      "Epoch 3434/30000 Training Loss: 0.05951276421546936\n",
      "Epoch 3435/30000 Training Loss: 0.05558919161558151\n",
      "Epoch 3436/30000 Training Loss: 0.06659193336963654\n",
      "Epoch 3437/30000 Training Loss: 0.0651075690984726\n",
      "Epoch 3438/30000 Training Loss: 0.05521704629063606\n",
      "Epoch 3439/30000 Training Loss: 0.0572541244328022\n",
      "Epoch 3440/30000 Training Loss: 0.058828193694353104\n",
      "Epoch 3441/30000 Training Loss: 0.06340368837118149\n",
      "Epoch 3442/30000 Training Loss: 0.05650220438838005\n",
      "Epoch 3443/30000 Training Loss: 0.06730014085769653\n",
      "Epoch 3444/30000 Training Loss: 0.06302256882190704\n",
      "Epoch 3445/30000 Training Loss: 0.06268345564603806\n",
      "Epoch 3446/30000 Training Loss: 0.07071645557880402\n",
      "Epoch 3447/30000 Training Loss: 0.054828934371471405\n",
      "Epoch 3448/30000 Training Loss: 0.0623026080429554\n",
      "Epoch 3449/30000 Training Loss: 0.05748950317502022\n",
      "Epoch 3450/30000 Training Loss: 0.05920196324586868\n",
      "Epoch 3450/30000 Validation Loss: 0.07161508500576019\n",
      "Epoch 3451/30000 Training Loss: 0.06593821942806244\n",
      "Epoch 3452/30000 Training Loss: 0.053632862865924835\n",
      "Epoch 3453/30000 Training Loss: 0.056742288172245026\n",
      "Epoch 3454/30000 Training Loss: 0.06970639526844025\n",
      "Epoch 3455/30000 Training Loss: 0.048874590545892715\n",
      "Epoch 3456/30000 Training Loss: 0.06476021558046341\n",
      "Epoch 3457/30000 Training Loss: 0.07129939645528793\n",
      "Epoch 3458/30000 Training Loss: 0.06491155922412872\n",
      "Epoch 3459/30000 Training Loss: 0.05925191193819046\n",
      "Epoch 3460/30000 Training Loss: 0.060554228723049164\n",
      "Epoch 3461/30000 Training Loss: 0.062161363661289215\n",
      "Epoch 3462/30000 Training Loss: 0.05368100479245186\n",
      "Epoch 3463/30000 Training Loss: 0.06227516382932663\n",
      "Epoch 3464/30000 Training Loss: 0.0615488700568676\n",
      "Epoch 3465/30000 Training Loss: 0.059091966599226\n",
      "Epoch 3466/30000 Training Loss: 0.061650991439819336\n",
      "Epoch 3467/30000 Training Loss: 0.05797401815652847\n",
      "Epoch 3468/30000 Training Loss: 0.05251212790608406\n",
      "Epoch 3469/30000 Training Loss: 0.06108393520116806\n",
      "Epoch 3470/30000 Training Loss: 0.058783333748579025\n",
      "Epoch 3471/30000 Training Loss: 0.05785089731216431\n",
      "Epoch 3472/30000 Training Loss: 0.05824851244688034\n",
      "Epoch 3473/30000 Training Loss: 0.060401223599910736\n",
      "Epoch 3474/30000 Training Loss: 0.05148428678512573\n",
      "Epoch 3475/30000 Training Loss: 0.06902799755334854\n",
      "Epoch 3476/30000 Training Loss: 0.06167079880833626\n",
      "Epoch 3477/30000 Training Loss: 0.053361695259809494\n",
      "Epoch 3478/30000 Training Loss: 0.048147834837436676\n",
      "Epoch 3479/30000 Training Loss: 0.055035389959812164\n",
      "Epoch 3480/30000 Training Loss: 0.06430216133594513\n",
      "Epoch 3481/30000 Training Loss: 0.05524113029241562\n",
      "Epoch 3482/30000 Training Loss: 0.04475484788417816\n",
      "Epoch 3483/30000 Training Loss: 0.0640791803598404\n",
      "Epoch 3484/30000 Training Loss: 0.050489019602537155\n",
      "Epoch 3485/30000 Training Loss: 0.059274692088365555\n",
      "Epoch 3486/30000 Training Loss: 0.06497610360383987\n",
      "Epoch 3487/30000 Training Loss: 0.06065894290804863\n",
      "Epoch 3488/30000 Training Loss: 0.04957191273570061\n",
      "Epoch 3489/30000 Training Loss: 0.057091206312179565\n",
      "Epoch 3490/30000 Training Loss: 0.0515425018966198\n",
      "Epoch 3491/30000 Training Loss: 0.0586564838886261\n",
      "Epoch 3492/30000 Training Loss: 0.05124727636575699\n",
      "Epoch 3493/30000 Training Loss: 0.05451378971338272\n",
      "Epoch 3494/30000 Training Loss: 0.05537847429513931\n",
      "Epoch 3495/30000 Training Loss: 0.055075109004974365\n",
      "Epoch 3496/30000 Training Loss: 0.05983547121286392\n",
      "Epoch 3497/30000 Training Loss: 0.052050959318876266\n",
      "Epoch 3498/30000 Training Loss: 0.052420713007450104\n",
      "Epoch 3499/30000 Training Loss: 0.06071733683347702\n",
      "Epoch 3500/30000 Training Loss: 0.056804120540618896\n",
      "Epoch 3500/30000 Validation Loss: 0.06489576399326324\n",
      "Epoch 3501/30000 Training Loss: 0.05949398875236511\n",
      "Epoch 3502/30000 Training Loss: 0.053586144000291824\n",
      "Epoch 3503/30000 Training Loss: 0.06145055219531059\n",
      "Epoch 3504/30000 Training Loss: 0.05859644338488579\n",
      "Epoch 3505/30000 Training Loss: 0.057989515364170074\n",
      "Epoch 3506/30000 Training Loss: 0.06213563680648804\n",
      "Epoch 3507/30000 Training Loss: 0.05291661620140076\n",
      "Epoch 3508/30000 Training Loss: 0.046404577791690826\n",
      "Epoch 3509/30000 Training Loss: 0.05735953897237778\n",
      "Epoch 3510/30000 Training Loss: 0.05517764016985893\n",
      "Epoch 3511/30000 Training Loss: 0.0624840185046196\n",
      "Epoch 3512/30000 Training Loss: 0.060159869492053986\n",
      "Epoch 3513/30000 Training Loss: 0.06240144371986389\n",
      "Epoch 3514/30000 Training Loss: 0.06535696983337402\n",
      "Epoch 3515/30000 Training Loss: 0.060411639511585236\n",
      "Epoch 3516/30000 Training Loss: 0.05610541254281998\n",
      "Epoch 3517/30000 Training Loss: 0.051300473511219025\n",
      "Epoch 3518/30000 Training Loss: 0.052523303776979446\n",
      "Epoch 3519/30000 Training Loss: 0.06433568894863129\n",
      "Epoch 3520/30000 Training Loss: 0.0625072568655014\n",
      "Epoch 3521/30000 Training Loss: 0.051205892115831375\n",
      "Epoch 3522/30000 Training Loss: 0.055249184370040894\n",
      "Epoch 3523/30000 Training Loss: 0.06288619339466095\n",
      "Epoch 3524/30000 Training Loss: 0.058082737028598785\n",
      "Epoch 3525/30000 Training Loss: 0.06481364369392395\n",
      "Epoch 3526/30000 Training Loss: 0.05231849104166031\n",
      "Epoch 3527/30000 Training Loss: 0.06267467886209488\n",
      "Epoch 3528/30000 Training Loss: 0.05939125269651413\n",
      "Epoch 3529/30000 Training Loss: 0.050766270607709885\n",
      "Epoch 3530/30000 Training Loss: 0.05353548377752304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3531/30000 Training Loss: 0.0664597600698471\n",
      "Epoch 3532/30000 Training Loss: 0.06298302114009857\n",
      "Epoch 3533/30000 Training Loss: 0.053049374371767044\n",
      "Epoch 3534/30000 Training Loss: 0.057071469724178314\n",
      "Epoch 3535/30000 Training Loss: 0.0613660030066967\n",
      "Epoch 3536/30000 Training Loss: 0.05572991818189621\n",
      "Epoch 3537/30000 Training Loss: 0.06325148791074753\n",
      "Epoch 3538/30000 Training Loss: 0.06093441694974899\n",
      "Epoch 3539/30000 Training Loss: 0.05511265993118286\n",
      "Epoch 3540/30000 Training Loss: 0.06690242141485214\n",
      "Epoch 3541/30000 Training Loss: 0.057712405920028687\n",
      "Epoch 3542/30000 Training Loss: 0.05223087593913078\n",
      "Epoch 3543/30000 Training Loss: 0.0549488440155983\n",
      "Epoch 3544/30000 Training Loss: 0.060772109776735306\n",
      "Epoch 3545/30000 Training Loss: 0.05701420456171036\n",
      "Epoch 3546/30000 Training Loss: 0.05414296314120293\n",
      "Epoch 3547/30000 Training Loss: 0.057239264249801636\n",
      "Epoch 3548/30000 Training Loss: 0.05851318687200546\n",
      "Epoch 3549/30000 Training Loss: 0.06575174629688263\n",
      "Epoch 3550/30000 Training Loss: 0.05667586997151375\n",
      "Epoch 3550/30000 Validation Loss: 0.04741032421588898\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04741032421588898<=============\n",
      "Epoch 3551/30000 Training Loss: 0.0520407073199749\n",
      "Epoch 3552/30000 Training Loss: 0.0588812530040741\n",
      "Epoch 3553/30000 Training Loss: 0.05552130937576294\n",
      "Epoch 3554/30000 Training Loss: 0.05703093856573105\n",
      "Epoch 3555/30000 Training Loss: 0.05868759751319885\n",
      "Epoch 3556/30000 Training Loss: 0.05611769109964371\n",
      "Epoch 3557/30000 Training Loss: 0.06106467917561531\n",
      "Epoch 3558/30000 Training Loss: 0.05984178185462952\n",
      "Epoch 3559/30000 Training Loss: 0.0637483075261116\n",
      "Epoch 3560/30000 Training Loss: 0.05838431045413017\n",
      "Epoch 3561/30000 Training Loss: 0.05462392047047615\n",
      "Epoch 3562/30000 Training Loss: 0.06234695389866829\n",
      "Epoch 3563/30000 Training Loss: 0.05845067650079727\n",
      "Epoch 3564/30000 Training Loss: 0.06248537451028824\n",
      "Epoch 3565/30000 Training Loss: 0.05472783371806145\n",
      "Epoch 3566/30000 Training Loss: 0.041861921548843384\n",
      "Epoch 3567/30000 Training Loss: 0.0513225793838501\n",
      "Epoch 3568/30000 Training Loss: 0.05682376027107239\n",
      "Epoch 3569/30000 Training Loss: 0.055192381143569946\n",
      "Epoch 3570/30000 Training Loss: 0.06525056809186935\n",
      "Epoch 3571/30000 Training Loss: 0.06018521264195442\n",
      "Epoch 3572/30000 Training Loss: 0.05127054452896118\n",
      "Epoch 3573/30000 Training Loss: 0.04852350428700447\n",
      "Epoch 3574/30000 Training Loss: 0.058011431246995926\n",
      "Epoch 3575/30000 Training Loss: 0.059417057782411575\n",
      "Epoch 3576/30000 Training Loss: 0.06629325449466705\n",
      "Epoch 3577/30000 Training Loss: 0.05964018777012825\n",
      "Epoch 3578/30000 Training Loss: 0.05852975696325302\n",
      "Epoch 3579/30000 Training Loss: 0.06154589727520943\n",
      "Epoch 3580/30000 Training Loss: 0.06226077675819397\n",
      "Epoch 3581/30000 Training Loss: 0.07172580808401108\n",
      "Epoch 3582/30000 Training Loss: 0.05395643785595894\n",
      "Epoch 3583/30000 Training Loss: 0.05925145000219345\n",
      "Epoch 3584/30000 Training Loss: 0.06952913105487823\n",
      "Epoch 3585/30000 Training Loss: 0.056330543011426926\n",
      "Epoch 3586/30000 Training Loss: 0.05998772382736206\n",
      "Epoch 3587/30000 Training Loss: 0.05252872779965401\n",
      "Epoch 3588/30000 Training Loss: 0.058623023331165314\n",
      "Epoch 3589/30000 Training Loss: 0.054430656135082245\n",
      "Epoch 3590/30000 Training Loss: 0.06765265017747879\n",
      "Epoch 3591/30000 Training Loss: 0.05625846982002258\n",
      "Epoch 3592/30000 Training Loss: 0.060393787920475006\n",
      "Epoch 3593/30000 Training Loss: 0.05370514839887619\n",
      "Epoch 3594/30000 Training Loss: 0.054689180105924606\n",
      "Epoch 3595/30000 Training Loss: 0.05640428513288498\n",
      "Epoch 3596/30000 Training Loss: 0.04563511162996292\n",
      "Epoch 3597/30000 Training Loss: 0.06912334263324738\n",
      "Epoch 3598/30000 Training Loss: 0.060065604746341705\n",
      "Epoch 3599/30000 Training Loss: 0.058237142860889435\n",
      "Epoch 3600/30000 Training Loss: 0.06051687151193619\n",
      "Epoch 3600/30000 Validation Loss: 0.06128888204693794\n",
      "Epoch 3601/30000 Training Loss: 0.058537643402814865\n",
      "Epoch 3602/30000 Training Loss: 0.04496922343969345\n",
      "Epoch 3603/30000 Training Loss: 0.06746645271778107\n",
      "Epoch 3604/30000 Training Loss: 0.05740223079919815\n",
      "Epoch 3605/30000 Training Loss: 0.054035626351833344\n",
      "Epoch 3606/30000 Training Loss: 0.05475667119026184\n",
      "Epoch 3607/30000 Training Loss: 0.05945224314928055\n",
      "Epoch 3608/30000 Training Loss: 0.06003627926111221\n",
      "Epoch 3609/30000 Training Loss: 0.05450369790196419\n",
      "Epoch 3610/30000 Training Loss: 0.05030319094657898\n",
      "Epoch 3611/30000 Training Loss: 0.06741714477539062\n",
      "Epoch 3612/30000 Training Loss: 0.05670619010925293\n",
      "Epoch 3613/30000 Training Loss: 0.06409884989261627\n",
      "Epoch 3614/30000 Training Loss: 0.05624381825327873\n",
      "Epoch 3615/30000 Training Loss: 0.057871825993061066\n",
      "Epoch 3616/30000 Training Loss: 0.060205668210983276\n",
      "Epoch 3617/30000 Training Loss: 0.05690667778253555\n",
      "Epoch 3618/30000 Training Loss: 0.05905141308903694\n",
      "Epoch 3619/30000 Training Loss: 0.05578538030385971\n",
      "Epoch 3620/30000 Training Loss: 0.0631786584854126\n",
      "Epoch 3621/30000 Training Loss: 0.05754675343632698\n",
      "Epoch 3622/30000 Training Loss: 0.05210305005311966\n",
      "Epoch 3623/30000 Training Loss: 0.05294175073504448\n",
      "Epoch 3624/30000 Training Loss: 0.06159861013293266\n",
      "Epoch 3625/30000 Training Loss: 0.05872641131281853\n",
      "Epoch 3626/30000 Training Loss: 0.050865452736616135\n",
      "Epoch 3627/30000 Training Loss: 0.049859341233968735\n",
      "Epoch 3628/30000 Training Loss: 0.05020355433225632\n",
      "Epoch 3629/30000 Training Loss: 0.06043689325451851\n",
      "Epoch 3630/30000 Training Loss: 0.0627899095416069\n",
      "Epoch 3631/30000 Training Loss: 0.06461068242788315\n",
      "Epoch 3632/30000 Training Loss: 0.05157949775457382\n",
      "Epoch 3633/30000 Training Loss: 0.060984838753938675\n",
      "Epoch 3634/30000 Training Loss: 0.06638403981924057\n",
      "Epoch 3635/30000 Training Loss: 0.05412405729293823\n",
      "Epoch 3636/30000 Training Loss: 0.05096235126256943\n",
      "Epoch 3637/30000 Training Loss: 0.05601929500699043\n",
      "Epoch 3638/30000 Training Loss: 0.05675555393099785\n",
      "Epoch 3639/30000 Training Loss: 0.05955430120229721\n",
      "Epoch 3640/30000 Training Loss: 0.050924867391586304\n",
      "Epoch 3641/30000 Training Loss: 0.06888023763895035\n",
      "Epoch 3642/30000 Training Loss: 0.056020259857177734\n",
      "Epoch 3643/30000 Training Loss: 0.06004820019006729\n",
      "Epoch 3644/30000 Training Loss: 0.059519000351428986\n",
      "Epoch 3645/30000 Training Loss: 0.06938453018665314\n",
      "Epoch 3646/30000 Training Loss: 0.06073535233736038\n",
      "Epoch 3647/30000 Training Loss: 0.0558721199631691\n",
      "Epoch 3648/30000 Training Loss: 0.05212058871984482\n",
      "Epoch 3649/30000 Training Loss: 0.051802050322294235\n",
      "Epoch 3650/30000 Training Loss: 0.054851364344358444\n",
      "Epoch 3650/30000 Validation Loss: 0.05993169546127319\n",
      "Epoch 3651/30000 Training Loss: 0.05948023125529289\n",
      "Epoch 3652/30000 Training Loss: 0.052646927535533905\n",
      "Epoch 3653/30000 Training Loss: 0.05898839235305786\n",
      "Epoch 3654/30000 Training Loss: 0.053055208176374435\n",
      "Epoch 3655/30000 Training Loss: 0.049310166388750076\n",
      "Epoch 3656/30000 Training Loss: 0.06028218939900398\n",
      "Epoch 3657/30000 Training Loss: 0.05347809940576553\n",
      "Epoch 3658/30000 Training Loss: 0.055222444236278534\n",
      "Epoch 3659/30000 Training Loss: 0.059191979467868805\n",
      "Epoch 3660/30000 Training Loss: 0.058112405240535736\n",
      "Epoch 3661/30000 Training Loss: 0.05899534747004509\n",
      "Epoch 3662/30000 Training Loss: 0.06576517969369888\n",
      "Epoch 3663/30000 Training Loss: 0.04948443919420242\n",
      "Epoch 3664/30000 Training Loss: 0.05589508265256882\n",
      "Epoch 3665/30000 Training Loss: 0.06425084173679352\n",
      "Epoch 3666/30000 Training Loss: 0.055761802941560745\n",
      "Epoch 3667/30000 Training Loss: 0.05523134395480156\n",
      "Epoch 3668/30000 Training Loss: 0.05104886740446091\n",
      "Epoch 3669/30000 Training Loss: 0.056712210178375244\n",
      "Epoch 3670/30000 Training Loss: 0.0668230950832367\n",
      "Epoch 3671/30000 Training Loss: 0.06150117516517639\n",
      "Epoch 3672/30000 Training Loss: 0.06494981050491333\n",
      "Epoch 3673/30000 Training Loss: 0.06246877461671829\n",
      "Epoch 3674/30000 Training Loss: 0.06872741878032684\n",
      "Epoch 3675/30000 Training Loss: 0.05025824159383774\n",
      "Epoch 3676/30000 Training Loss: 0.05566992610692978\n",
      "Epoch 3677/30000 Training Loss: 0.05428600311279297\n",
      "Epoch 3678/30000 Training Loss: 0.058190274983644485\n",
      "Epoch 3679/30000 Training Loss: 0.07064260542392731\n",
      "Epoch 3680/30000 Training Loss: 0.055236659944057465\n",
      "Epoch 3681/30000 Training Loss: 0.054263580590486526\n",
      "Epoch 3682/30000 Training Loss: 0.051059603691101074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3683/30000 Training Loss: 0.062459271401166916\n",
      "Epoch 3684/30000 Training Loss: 0.06540654599666595\n",
      "Epoch 3685/30000 Training Loss: 0.05760834366083145\n",
      "Epoch 3686/30000 Training Loss: 0.052518080919981\n",
      "Epoch 3687/30000 Training Loss: 0.05365334078669548\n",
      "Epoch 3688/30000 Training Loss: 0.056220389902591705\n",
      "Epoch 3689/30000 Training Loss: 0.06272878497838974\n",
      "Epoch 3690/30000 Training Loss: 0.05651385709643364\n",
      "Epoch 3691/30000 Training Loss: 0.05829101800918579\n",
      "Epoch 3692/30000 Training Loss: 0.055579762905836105\n",
      "Epoch 3693/30000 Training Loss: 0.07020348310470581\n",
      "Epoch 3694/30000 Training Loss: 0.054339826107025146\n",
      "Epoch 3695/30000 Training Loss: 0.050912775099277496\n",
      "Epoch 3696/30000 Training Loss: 0.06387306749820709\n",
      "Epoch 3697/30000 Training Loss: 0.062002431601285934\n",
      "Epoch 3698/30000 Training Loss: 0.06105397269129753\n",
      "Epoch 3699/30000 Training Loss: 0.06388626992702484\n",
      "Epoch 3700/30000 Training Loss: 0.05766506865620613\n",
      "Epoch 3700/30000 Validation Loss: 0.060019977390766144\n",
      "Epoch 3701/30000 Training Loss: 0.0712318867444992\n",
      "Epoch 3702/30000 Training Loss: 0.07446809858083725\n",
      "Epoch 3703/30000 Training Loss: 0.06017183139920235\n",
      "Epoch 3704/30000 Training Loss: 0.059230953454971313\n",
      "Epoch 3705/30000 Training Loss: 0.06065225601196289\n",
      "Epoch 3706/30000 Training Loss: 0.054670415818691254\n",
      "Epoch 3707/30000 Training Loss: 0.056775838136672974\n",
      "Epoch 3708/30000 Training Loss: 0.05686379224061966\n",
      "Epoch 3709/30000 Training Loss: 0.05263970047235489\n",
      "Epoch 3710/30000 Training Loss: 0.054746873676776886\n",
      "Epoch 3711/30000 Training Loss: 0.05405207350850105\n",
      "Epoch 3712/30000 Training Loss: 0.06328724324703217\n",
      "Epoch 3713/30000 Training Loss: 0.06263726204633713\n",
      "Epoch 3714/30000 Training Loss: 0.060370732098817825\n",
      "Epoch 3715/30000 Training Loss: 0.06759156286716461\n",
      "Epoch 3716/30000 Training Loss: 0.053301889449357986\n",
      "Epoch 3717/30000 Training Loss: 0.05855477601289749\n",
      "Epoch 3718/30000 Training Loss: 0.053290463984012604\n",
      "Epoch 3719/30000 Training Loss: 0.05679777264595032\n",
      "Epoch 3720/30000 Training Loss: 0.06106053665280342\n",
      "Epoch 3721/30000 Training Loss: 0.06013492867350578\n",
      "Epoch 3722/30000 Training Loss: 0.04370630905032158\n",
      "Epoch 3723/30000 Training Loss: 0.051871348172426224\n",
      "Epoch 3724/30000 Training Loss: 0.05839196592569351\n",
      "Epoch 3725/30000 Training Loss: 0.0575375072658062\n",
      "Epoch 3726/30000 Training Loss: 0.053574271500110626\n",
      "Epoch 3727/30000 Training Loss: 0.05514068156480789\n",
      "Epoch 3728/30000 Training Loss: 0.05870634317398071\n",
      "Epoch 3729/30000 Training Loss: 0.04977407306432724\n",
      "Epoch 3730/30000 Training Loss: 0.04783208295702934\n",
      "Epoch 3731/30000 Training Loss: 0.05709748715162277\n",
      "Epoch 3732/30000 Training Loss: 0.05400718003511429\n",
      "Epoch 3733/30000 Training Loss: 0.05706590414047241\n",
      "Epoch 3734/30000 Training Loss: 0.05431874468922615\n",
      "Epoch 3735/30000 Training Loss: 0.057099152356386185\n",
      "Epoch 3736/30000 Training Loss: 0.05307185649871826\n",
      "Epoch 3737/30000 Training Loss: 0.061095017939805984\n",
      "Epoch 3738/30000 Training Loss: 0.06198187917470932\n",
      "Epoch 3739/30000 Training Loss: 0.05968700721859932\n",
      "Epoch 3740/30000 Training Loss: 0.0644611269235611\n",
      "Epoch 3741/30000 Training Loss: 0.05528606101870537\n",
      "Epoch 3742/30000 Training Loss: 0.06643781810998917\n",
      "Epoch 3743/30000 Training Loss: 0.04504547268152237\n",
      "Epoch 3744/30000 Training Loss: 0.05712311714887619\n",
      "Epoch 3745/30000 Training Loss: 0.06585662811994553\n",
      "Epoch 3746/30000 Training Loss: 0.060045622289180756\n",
      "Epoch 3747/30000 Training Loss: 0.05435099080204964\n",
      "Epoch 3748/30000 Training Loss: 0.060217004269361496\n",
      "Epoch 3749/30000 Training Loss: 0.06157022714614868\n",
      "Epoch 3750/30000 Training Loss: 0.05904209613800049\n",
      "Epoch 3750/30000 Validation Loss: 0.06246618181467056\n",
      "Epoch 3751/30000 Training Loss: 0.04456767812371254\n",
      "Epoch 3752/30000 Training Loss: 0.06442146003246307\n",
      "Epoch 3753/30000 Training Loss: 0.0605446882545948\n",
      "Epoch 3754/30000 Training Loss: 0.05165760964155197\n",
      "Epoch 3755/30000 Training Loss: 0.050756167620420456\n",
      "Epoch 3756/30000 Training Loss: 0.0580805167555809\n",
      "Epoch 3757/30000 Training Loss: 0.05005349963903427\n",
      "Epoch 3758/30000 Training Loss: 0.06443386524915695\n",
      "Epoch 3759/30000 Training Loss: 0.056978654116392136\n",
      "Epoch 3760/30000 Training Loss: 0.05266060680150986\n",
      "Epoch 3761/30000 Training Loss: 0.057049460709095\n",
      "Epoch 3762/30000 Training Loss: 0.06908894330263138\n",
      "Epoch 3763/30000 Training Loss: 0.058897048234939575\n",
      "Epoch 3764/30000 Training Loss: 0.05921562388539314\n",
      "Epoch 3765/30000 Training Loss: 0.05583773925900459\n",
      "Epoch 3766/30000 Training Loss: 0.05289575457572937\n",
      "Epoch 3767/30000 Training Loss: 0.05021513253450394\n",
      "Epoch 3768/30000 Training Loss: 0.059544771909713745\n",
      "Epoch 3769/30000 Training Loss: 0.05761987715959549\n",
      "Epoch 3770/30000 Training Loss: 0.058091551065444946\n",
      "Epoch 3771/30000 Training Loss: 0.04960961639881134\n",
      "Epoch 3772/30000 Training Loss: 0.06271784007549286\n",
      "Epoch 3773/30000 Training Loss: 0.0616682693362236\n",
      "Epoch 3774/30000 Training Loss: 0.05392199009656906\n",
      "Epoch 3775/30000 Training Loss: 0.06864313036203384\n",
      "Epoch 3776/30000 Training Loss: 0.05622106045484543\n",
      "Epoch 3777/30000 Training Loss: 0.05747706815600395\n",
      "Epoch 3778/30000 Training Loss: 0.06103553622961044\n",
      "Epoch 3779/30000 Training Loss: 0.05171196907758713\n",
      "Epoch 3780/30000 Training Loss: 0.04847526550292969\n",
      "Epoch 3781/30000 Training Loss: 0.057942431420087814\n",
      "Epoch 3782/30000 Training Loss: 0.06072169542312622\n",
      "Epoch 3783/30000 Training Loss: 0.05519896000623703\n",
      "Epoch 3784/30000 Training Loss: 0.06348399817943573\n",
      "Epoch 3785/30000 Training Loss: 0.05894831568002701\n",
      "Epoch 3786/30000 Training Loss: 0.0648224949836731\n",
      "Epoch 3787/30000 Training Loss: 0.07388373464345932\n",
      "Epoch 3788/30000 Training Loss: 0.05044303089380264\n",
      "Epoch 3789/30000 Training Loss: 0.05313529446721077\n",
      "Epoch 3790/30000 Training Loss: 0.04895758628845215\n",
      "Epoch 3791/30000 Training Loss: 0.04574468359351158\n",
      "Epoch 3792/30000 Training Loss: 0.05795057490468025\n",
      "Epoch 3793/30000 Training Loss: 0.05711456388235092\n",
      "Epoch 3794/30000 Training Loss: 0.05175518989562988\n",
      "Epoch 3795/30000 Training Loss: 0.060451388359069824\n",
      "Epoch 3796/30000 Training Loss: 0.06707024574279785\n",
      "Epoch 3797/30000 Training Loss: 0.04848896339535713\n",
      "Epoch 3798/30000 Training Loss: 0.06481505930423737\n",
      "Epoch 3799/30000 Training Loss: 0.055620454251766205\n",
      "Epoch 3800/30000 Training Loss: 0.061589114367961884\n",
      "Epoch 3800/30000 Validation Loss: 0.058506689965724945\n",
      "Epoch 3801/30000 Training Loss: 0.06144025921821594\n",
      "Epoch 3802/30000 Training Loss: 0.0639762133359909\n",
      "Epoch 3803/30000 Training Loss: 0.06201871484518051\n",
      "Epoch 3804/30000 Training Loss: 0.05585692077875137\n",
      "Epoch 3805/30000 Training Loss: 0.05303245782852173\n",
      "Epoch 3806/30000 Training Loss: 0.04873849079012871\n",
      "Epoch 3807/30000 Training Loss: 0.0547153577208519\n",
      "Epoch 3808/30000 Training Loss: 0.06611467152833939\n",
      "Epoch 3809/30000 Training Loss: 0.06457190215587616\n",
      "Epoch 3810/30000 Training Loss: 0.06015201658010483\n",
      "Epoch 3811/30000 Training Loss: 0.04906243458390236\n",
      "Epoch 3812/30000 Training Loss: 0.05339200422167778\n",
      "Epoch 3813/30000 Training Loss: 0.05117572471499443\n",
      "Epoch 3814/30000 Training Loss: 0.061925746500492096\n",
      "Epoch 3815/30000 Training Loss: 0.06280364841222763\n",
      "Epoch 3816/30000 Training Loss: 0.055454183369874954\n",
      "Epoch 3817/30000 Training Loss: 0.05339496210217476\n",
      "Epoch 3818/30000 Training Loss: 0.05833212658762932\n",
      "Epoch 3819/30000 Training Loss: 0.06490923464298248\n",
      "Epoch 3820/30000 Training Loss: 0.06022360175848007\n",
      "Epoch 3821/30000 Training Loss: 0.057789046317338943\n",
      "Epoch 3822/30000 Training Loss: 0.05466209724545479\n",
      "Epoch 3823/30000 Training Loss: 0.054213494062423706\n",
      "Epoch 3824/30000 Training Loss: 0.05750377103686333\n",
      "Epoch 3825/30000 Training Loss: 0.060279954224824905\n",
      "Epoch 3826/30000 Training Loss: 0.05835859104990959\n",
      "Epoch 3827/30000 Training Loss: 0.06394734233617783\n",
      "Epoch 3828/30000 Training Loss: 0.060817182064056396\n",
      "Epoch 3829/30000 Training Loss: 0.05620364099740982\n",
      "Epoch 3830/30000 Training Loss: 0.06222432851791382\n",
      "Epoch 3831/30000 Training Loss: 0.05055267736315727\n",
      "Epoch 3832/30000 Training Loss: 0.05735792592167854\n",
      "Epoch 3833/30000 Training Loss: 0.05920535326004028\n",
      "Epoch 3834/30000 Training Loss: 0.06301021575927734\n",
      "Epoch 3835/30000 Training Loss: 0.054825495928525925\n",
      "Epoch 3836/30000 Training Loss: 0.05771822854876518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3837/30000 Training Loss: 0.05320354551076889\n",
      "Epoch 3838/30000 Training Loss: 0.07484851777553558\n",
      "Epoch 3839/30000 Training Loss: 0.05584603548049927\n",
      "Epoch 3840/30000 Training Loss: 0.051091186702251434\n",
      "Epoch 3841/30000 Training Loss: 0.05324364826083183\n",
      "Epoch 3842/30000 Training Loss: 0.06674070656299591\n",
      "Epoch 3843/30000 Training Loss: 0.058334607630968094\n",
      "Epoch 3844/30000 Training Loss: 0.059034883975982666\n",
      "Epoch 3845/30000 Training Loss: 0.052469074726104736\n",
      "Epoch 3846/30000 Training Loss: 0.06673401594161987\n",
      "Epoch 3847/30000 Training Loss: 0.053377192467451096\n",
      "Epoch 3848/30000 Training Loss: 0.05972793698310852\n",
      "Epoch 3849/30000 Training Loss: 0.05761728435754776\n",
      "Epoch 3850/30000 Training Loss: 0.05783962085843086\n",
      "Epoch 3850/30000 Validation Loss: 0.04871245473623276\n",
      "Epoch 3851/30000 Training Loss: 0.05182313919067383\n",
      "Epoch 3852/30000 Training Loss: 0.06535238027572632\n",
      "Epoch 3853/30000 Training Loss: 0.05778355523943901\n",
      "Epoch 3854/30000 Training Loss: 0.05708488076925278\n",
      "Epoch 3855/30000 Training Loss: 0.05622826889157295\n",
      "Epoch 3856/30000 Training Loss: 0.06402088701725006\n",
      "Epoch 3857/30000 Training Loss: 0.06503109633922577\n",
      "Epoch 3858/30000 Training Loss: 0.06458674371242523\n",
      "Epoch 3859/30000 Training Loss: 0.0555269829928875\n",
      "Epoch 3860/30000 Training Loss: 0.05348439887166023\n",
      "Epoch 3861/30000 Training Loss: 0.05586123466491699\n",
      "Epoch 3862/30000 Training Loss: 0.058083128184080124\n",
      "Epoch 3863/30000 Training Loss: 0.05929134041070938\n",
      "Epoch 3864/30000 Training Loss: 0.048732955008745193\n",
      "Epoch 3865/30000 Training Loss: 0.05378478020429611\n",
      "Epoch 3866/30000 Training Loss: 0.05534285306930542\n",
      "Epoch 3867/30000 Training Loss: 0.06528142839670181\n",
      "Epoch 3868/30000 Training Loss: 0.056328147649765015\n",
      "Epoch 3869/30000 Training Loss: 0.05578566715121269\n",
      "Epoch 3870/30000 Training Loss: 0.051585931330919266\n",
      "Epoch 3871/30000 Training Loss: 0.07235836237668991\n",
      "Epoch 3872/30000 Training Loss: 0.05945581942796707\n",
      "Epoch 3873/30000 Training Loss: 0.0577392652630806\n",
      "Epoch 3874/30000 Training Loss: 0.055700112134218216\n",
      "Epoch 3875/30000 Training Loss: 0.06110338121652603\n",
      "Epoch 3876/30000 Training Loss: 0.05628087371587753\n",
      "Epoch 3877/30000 Training Loss: 0.059216905385255814\n",
      "Epoch 3878/30000 Training Loss: 0.06001117080450058\n",
      "Epoch 3879/30000 Training Loss: 0.06085019186139107\n",
      "Epoch 3880/30000 Training Loss: 0.05030854418873787\n",
      "Epoch 3881/30000 Training Loss: 0.05209921672940254\n",
      "Epoch 3882/30000 Training Loss: 0.052137963473796844\n",
      "Epoch 3883/30000 Training Loss: 0.05039365962147713\n",
      "Epoch 3884/30000 Training Loss: 0.05052943900227547\n",
      "Epoch 3885/30000 Training Loss: 0.05214561149477959\n",
      "Epoch 3886/30000 Training Loss: 0.051964085549116135\n",
      "Epoch 3887/30000 Training Loss: 0.056268878281116486\n",
      "Epoch 3888/30000 Training Loss: 0.05998147651553154\n",
      "Epoch 3889/30000 Training Loss: 0.057213008403778076\n",
      "Epoch 3890/30000 Training Loss: 0.05510149151086807\n",
      "Epoch 3891/30000 Training Loss: 0.05091707408428192\n",
      "Epoch 3892/30000 Training Loss: 0.06428656727075577\n",
      "Epoch 3893/30000 Training Loss: 0.052878864109516144\n",
      "Epoch 3894/30000 Training Loss: 0.059126369655132294\n",
      "Epoch 3895/30000 Training Loss: 0.05449981242418289\n",
      "Epoch 3896/30000 Training Loss: 0.054142583161592484\n",
      "Epoch 3897/30000 Training Loss: 0.04511649161577225\n",
      "Epoch 3898/30000 Training Loss: 0.04571617394685745\n",
      "Epoch 3899/30000 Training Loss: 0.060056351125240326\n",
      "Epoch 3900/30000 Training Loss: 0.06084100529551506\n",
      "Epoch 3900/30000 Validation Loss: 0.06461364775896072\n",
      "Epoch 3901/30000 Training Loss: 0.06784585118293762\n",
      "Epoch 3902/30000 Training Loss: 0.05724663287401199\n",
      "Epoch 3903/30000 Training Loss: 0.06156441569328308\n",
      "Epoch 3904/30000 Training Loss: 0.053628068417310715\n",
      "Epoch 3905/30000 Training Loss: 0.05975405126810074\n",
      "Epoch 3906/30000 Training Loss: 0.06969751417636871\n",
      "Epoch 3907/30000 Training Loss: 0.05167274922132492\n",
      "Epoch 3908/30000 Training Loss: 0.04995032027363777\n",
      "Epoch 3909/30000 Training Loss: 0.05801720172166824\n",
      "Epoch 3910/30000 Training Loss: 0.05757105350494385\n",
      "Epoch 3911/30000 Training Loss: 0.054283224046230316\n",
      "Epoch 3912/30000 Training Loss: 0.052537478506565094\n",
      "Epoch 3913/30000 Training Loss: 0.04772449657320976\n",
      "Epoch 3914/30000 Training Loss: 0.04615507647395134\n",
      "Epoch 3915/30000 Training Loss: 0.061015211045742035\n",
      "Epoch 3916/30000 Training Loss: 0.05067586153745651\n",
      "Epoch 3917/30000 Training Loss: 0.05614501237869263\n",
      "Epoch 3918/30000 Training Loss: 0.06613242626190186\n",
      "Epoch 3919/30000 Training Loss: 0.06958885490894318\n",
      "Epoch 3920/30000 Training Loss: 0.06150662899017334\n",
      "Epoch 3921/30000 Training Loss: 0.048891276121139526\n",
      "Epoch 3922/30000 Training Loss: 0.057383012026548386\n",
      "Epoch 3923/30000 Training Loss: 0.06005590409040451\n",
      "Epoch 3924/30000 Training Loss: 0.0634731724858284\n",
      "Epoch 3925/30000 Training Loss: 0.0493326373398304\n",
      "Epoch 3926/30000 Training Loss: 0.05415921285748482\n",
      "Epoch 3927/30000 Training Loss: 0.05233899503946304\n",
      "Epoch 3928/30000 Training Loss: 0.058404773473739624\n",
      "Epoch 3929/30000 Training Loss: 0.06133343651890755\n",
      "Epoch 3930/30000 Training Loss: 0.06820948421955109\n",
      "Epoch 3931/30000 Training Loss: 0.05056784301996231\n",
      "Epoch 3932/30000 Training Loss: 0.05364712327718735\n",
      "Epoch 3933/30000 Training Loss: 0.06456267088651657\n",
      "Epoch 3934/30000 Training Loss: 0.05162208154797554\n",
      "Epoch 3935/30000 Training Loss: 0.05761117488145828\n",
      "Epoch 3936/30000 Training Loss: 0.05456975847482681\n",
      "Epoch 3937/30000 Training Loss: 0.059454791247844696\n",
      "Epoch 3938/30000 Training Loss: 0.05779939889907837\n",
      "Epoch 3939/30000 Training Loss: 0.05515690892934799\n",
      "Epoch 3940/30000 Training Loss: 0.06017278879880905\n",
      "Epoch 3941/30000 Training Loss: 0.05012630298733711\n",
      "Epoch 3942/30000 Training Loss: 0.056915540248155594\n",
      "Epoch 3943/30000 Training Loss: 0.05428358167409897\n",
      "Epoch 3944/30000 Training Loss: 0.05587645620107651\n",
      "Epoch 3945/30000 Training Loss: 0.05349362641572952\n",
      "Epoch 3946/30000 Training Loss: 0.05256356671452522\n",
      "Epoch 3947/30000 Training Loss: 0.059015899896621704\n",
      "Epoch 3948/30000 Training Loss: 0.054856348782777786\n",
      "Epoch 3949/30000 Training Loss: 0.04721168056130409\n",
      "Epoch 3950/30000 Training Loss: 0.06492382287979126\n",
      "Epoch 3950/30000 Validation Loss: 0.05355445668101311\n",
      "Epoch 3951/30000 Training Loss: 0.05008625239133835\n",
      "Epoch 3952/30000 Training Loss: 0.06176747754216194\n",
      "Epoch 3953/30000 Training Loss: 0.05264141038060188\n",
      "Epoch 3954/30000 Training Loss: 0.05257240682840347\n",
      "Epoch 3955/30000 Training Loss: 0.06104524806141853\n",
      "Epoch 3956/30000 Training Loss: 0.05663352087140083\n",
      "Epoch 3957/30000 Training Loss: 0.058896638453006744\n",
      "Epoch 3958/30000 Training Loss: 0.05124645680189133\n",
      "Epoch 3959/30000 Training Loss: 0.0594007782638073\n",
      "Epoch 3960/30000 Training Loss: 0.05725187063217163\n",
      "Epoch 3961/30000 Training Loss: 0.04961065575480461\n",
      "Epoch 3962/30000 Training Loss: 0.0639018565416336\n",
      "Epoch 3963/30000 Training Loss: 0.06243566423654556\n",
      "Epoch 3964/30000 Training Loss: 0.06570281088352203\n",
      "Epoch 3965/30000 Training Loss: 0.054290540516376495\n",
      "Epoch 3966/30000 Training Loss: 0.05466845631599426\n",
      "Epoch 3967/30000 Training Loss: 0.061134498566389084\n",
      "Epoch 3968/30000 Training Loss: 0.04874994605779648\n",
      "Epoch 3969/30000 Training Loss: 0.057591844350099564\n",
      "Epoch 3970/30000 Training Loss: 0.06328114122152328\n",
      "Epoch 3971/30000 Training Loss: 0.05379169061779976\n",
      "Epoch 3972/30000 Training Loss: 0.05199258774518967\n",
      "Epoch 3973/30000 Training Loss: 0.06952022016048431\n",
      "Epoch 3974/30000 Training Loss: 0.05462971329689026\n",
      "Epoch 3975/30000 Training Loss: 0.07068565487861633\n",
      "Epoch 3976/30000 Training Loss: 0.05193347856402397\n",
      "Epoch 3977/30000 Training Loss: 0.05685146898031235\n",
      "Epoch 3978/30000 Training Loss: 0.055170197039842606\n",
      "Epoch 3979/30000 Training Loss: 0.05091971904039383\n",
      "Epoch 3980/30000 Training Loss: 0.05041096359491348\n",
      "Epoch 3981/30000 Training Loss: 0.06472083926200867\n",
      "Epoch 3982/30000 Training Loss: 0.04793864116072655\n",
      "Epoch 3983/30000 Training Loss: 0.05742005258798599\n",
      "Epoch 3984/30000 Training Loss: 0.04786881059408188\n",
      "Epoch 3985/30000 Training Loss: 0.060591667890548706\n",
      "Epoch 3986/30000 Training Loss: 0.057531096041202545\n",
      "Epoch 3987/30000 Training Loss: 0.0517316572368145\n",
      "Epoch 3988/30000 Training Loss: 0.06275289505720139\n",
      "Epoch 3989/30000 Training Loss: 0.05746394395828247\n",
      "Epoch 3990/30000 Training Loss: 0.0520368292927742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3991/30000 Training Loss: 0.05134456604719162\n",
      "Epoch 3992/30000 Training Loss: 0.047209568321704865\n",
      "Epoch 3993/30000 Training Loss: 0.0518752820789814\n",
      "Epoch 3994/30000 Training Loss: 0.05503477528691292\n",
      "Epoch 3995/30000 Training Loss: 0.05419488996267319\n",
      "Epoch 3996/30000 Training Loss: 0.04891466349363327\n",
      "Epoch 3997/30000 Training Loss: 0.0529736652970314\n",
      "Epoch 3998/30000 Training Loss: 0.051980387419462204\n",
      "Epoch 3999/30000 Training Loss: 0.061142660677433014\n",
      "Epoch 4000/30000 Training Loss: 0.06474733352661133\n",
      "Epoch 4000/30000 Validation Loss: 0.05599486082792282\n",
      "Epoch 4001/30000 Training Loss: 0.0627526342868805\n",
      "Epoch 4002/30000 Training Loss: 0.048965953290462494\n",
      "Epoch 4003/30000 Training Loss: 0.058958858251571655\n",
      "Epoch 4004/30000 Training Loss: 0.05209247022867203\n",
      "Epoch 4005/30000 Training Loss: 0.0519743375480175\n",
      "Epoch 4006/30000 Training Loss: 0.05971068888902664\n",
      "Epoch 4007/30000 Training Loss: 0.055682916194200516\n",
      "Epoch 4008/30000 Training Loss: 0.059001050889492035\n",
      "Epoch 4009/30000 Training Loss: 0.05910821631550789\n",
      "Epoch 4010/30000 Training Loss: 0.05289469286799431\n",
      "Epoch 4011/30000 Training Loss: 0.053639452904462814\n",
      "Epoch 4012/30000 Training Loss: 0.05932394787669182\n",
      "Epoch 4013/30000 Training Loss: 0.05059882998466492\n",
      "Epoch 4014/30000 Training Loss: 0.059263862669467926\n",
      "Epoch 4015/30000 Training Loss: 0.0506322905421257\n",
      "Epoch 4016/30000 Training Loss: 0.05743834376335144\n",
      "Epoch 4017/30000 Training Loss: 0.05088219791650772\n",
      "Epoch 4018/30000 Training Loss: 0.05937296152114868\n",
      "Epoch 4019/30000 Training Loss: 0.05277043581008911\n",
      "Epoch 4020/30000 Training Loss: 0.05594837665557861\n",
      "Epoch 4021/30000 Training Loss: 0.05225110799074173\n",
      "Epoch 4022/30000 Training Loss: 0.052166856825351715\n",
      "Epoch 4023/30000 Training Loss: 0.05810682848095894\n",
      "Epoch 4024/30000 Training Loss: 0.04677695035934448\n",
      "Epoch 4025/30000 Training Loss: 0.05285391956567764\n",
      "Epoch 4026/30000 Training Loss: 0.0530136413872242\n",
      "Epoch 4027/30000 Training Loss: 0.059005510061979294\n",
      "Epoch 4028/30000 Training Loss: 0.05534331873059273\n",
      "Epoch 4029/30000 Training Loss: 0.059711992740631104\n",
      "Epoch 4030/30000 Training Loss: 0.06391625106334686\n",
      "Epoch 4031/30000 Training Loss: 0.0595027320086956\n",
      "Epoch 4032/30000 Training Loss: 0.05933181568980217\n",
      "Epoch 4033/30000 Training Loss: 0.04693335294723511\n",
      "Epoch 4034/30000 Training Loss: 0.06557871401309967\n",
      "Epoch 4035/30000 Training Loss: 0.0595492348074913\n",
      "Epoch 4036/30000 Training Loss: 0.05823712795972824\n",
      "Epoch 4037/30000 Training Loss: 0.04878780245780945\n",
      "Epoch 4038/30000 Training Loss: 0.0449201725423336\n",
      "Epoch 4039/30000 Training Loss: 0.053561240434646606\n",
      "Epoch 4040/30000 Training Loss: 0.05510867387056351\n",
      "Epoch 4041/30000 Training Loss: 0.05619477108120918\n",
      "Epoch 4042/30000 Training Loss: 0.05004943534731865\n",
      "Epoch 4043/30000 Training Loss: 0.05236908793449402\n",
      "Epoch 4044/30000 Training Loss: 0.05030369758605957\n",
      "Epoch 4045/30000 Training Loss: 0.05651067942380905\n",
      "Epoch 4046/30000 Training Loss: 0.05190472677350044\n",
      "Epoch 4047/30000 Training Loss: 0.0724397525191307\n",
      "Epoch 4048/30000 Training Loss: 0.06106157228350639\n",
      "Epoch 4049/30000 Training Loss: 0.05113857984542847\n",
      "Epoch 4050/30000 Training Loss: 0.05087649077177048\n",
      "Epoch 4050/30000 Validation Loss: 0.052393633872270584\n",
      "Epoch 4051/30000 Training Loss: 0.055843062698841095\n",
      "Epoch 4052/30000 Training Loss: 0.059429943561553955\n",
      "Epoch 4053/30000 Training Loss: 0.049456384032964706\n",
      "Epoch 4054/30000 Training Loss: 0.06460648775100708\n",
      "Epoch 4055/30000 Training Loss: 0.054732583463191986\n",
      "Epoch 4056/30000 Training Loss: 0.05789514631032944\n",
      "Epoch 4057/30000 Training Loss: 0.05527796223759651\n",
      "Epoch 4058/30000 Training Loss: 0.05087977647781372\n",
      "Epoch 4059/30000 Training Loss: 0.06692516058683395\n",
      "Epoch 4060/30000 Training Loss: 0.06486133486032486\n",
      "Epoch 4061/30000 Training Loss: 0.05677551031112671\n",
      "Epoch 4062/30000 Training Loss: 0.053342513740062714\n",
      "Epoch 4063/30000 Training Loss: 0.06328067183494568\n",
      "Epoch 4064/30000 Training Loss: 0.05709744244813919\n",
      "Epoch 4065/30000 Training Loss: 0.060254089534282684\n",
      "Epoch 4066/30000 Training Loss: 0.05717001110315323\n",
      "Epoch 4067/30000 Training Loss: 0.05411608889698982\n",
      "Epoch 4068/30000 Training Loss: 0.06782006472349167\n",
      "Epoch 4069/30000 Training Loss: 0.05508606508374214\n",
      "Epoch 4070/30000 Training Loss: 0.05261444300413132\n",
      "Epoch 4071/30000 Training Loss: 0.05862460657954216\n",
      "Epoch 4072/30000 Training Loss: 0.06318040192127228\n",
      "Epoch 4073/30000 Training Loss: 0.0610082671046257\n",
      "Epoch 4074/30000 Training Loss: 0.0527074933052063\n",
      "Epoch 4075/30000 Training Loss: 0.06161622330546379\n",
      "Epoch 4076/30000 Training Loss: 0.06060434505343437\n",
      "Epoch 4077/30000 Training Loss: 0.06349537521600723\n",
      "Epoch 4078/30000 Training Loss: 0.05689765885472298\n",
      "Epoch 4079/30000 Training Loss: 0.05344652384519577\n",
      "Epoch 4080/30000 Training Loss: 0.053292643278837204\n",
      "Epoch 4081/30000 Training Loss: 0.055826395750045776\n",
      "Epoch 4082/30000 Training Loss: 0.05861382931470871\n",
      "Epoch 4083/30000 Training Loss: 0.05538512021303177\n",
      "Epoch 4084/30000 Training Loss: 0.05043436214327812\n",
      "Epoch 4085/30000 Training Loss: 0.06556341797113419\n",
      "Epoch 4086/30000 Training Loss: 0.06700541079044342\n",
      "Epoch 4087/30000 Training Loss: 0.06258472800254822\n",
      "Epoch 4088/30000 Training Loss: 0.0572834312915802\n",
      "Epoch 4089/30000 Training Loss: 0.045854274183511734\n",
      "Epoch 4090/30000 Training Loss: 0.05594782158732414\n",
      "Epoch 4091/30000 Training Loss: 0.0591486319899559\n",
      "Epoch 4092/30000 Training Loss: 0.0585780031979084\n",
      "Epoch 4093/30000 Training Loss: 0.05456938222050667\n",
      "Epoch 4094/30000 Training Loss: 0.06000036001205444\n",
      "Epoch 4095/30000 Training Loss: 0.059418875724077225\n",
      "Epoch 4096/30000 Training Loss: 0.0530100092291832\n",
      "Epoch 4097/30000 Training Loss: 0.057823799550533295\n",
      "Epoch 4098/30000 Training Loss: 0.06047666072845459\n",
      "Epoch 4099/30000 Training Loss: 0.05371459573507309\n",
      "Epoch 4100/30000 Training Loss: 0.056415826082229614\n",
      "Epoch 4100/30000 Validation Loss: 0.05116421729326248\n",
      "Epoch 4101/30000 Training Loss: 0.05326883867383003\n",
      "Epoch 4102/30000 Training Loss: 0.04826320335268974\n",
      "Epoch 4103/30000 Training Loss: 0.05322488397359848\n",
      "Epoch 4104/30000 Training Loss: 0.050880491733551025\n",
      "Epoch 4105/30000 Training Loss: 0.05783670023083687\n",
      "Epoch 4106/30000 Training Loss: 0.054700810462236404\n",
      "Epoch 4107/30000 Training Loss: 0.05781514197587967\n",
      "Epoch 4108/30000 Training Loss: 0.0533161386847496\n",
      "Epoch 4109/30000 Training Loss: 0.054973989725112915\n",
      "Epoch 4110/30000 Training Loss: 0.04798316955566406\n",
      "Epoch 4111/30000 Training Loss: 0.06209518760442734\n",
      "Epoch 4112/30000 Training Loss: 0.06509412080049515\n",
      "Epoch 4113/30000 Training Loss: 0.060226935893297195\n",
      "Epoch 4114/30000 Training Loss: 0.05485530570149422\n",
      "Epoch 4115/30000 Training Loss: 0.0432281568646431\n",
      "Epoch 4116/30000 Training Loss: 0.06083091348409653\n",
      "Epoch 4117/30000 Training Loss: 0.0527837872505188\n",
      "Epoch 4118/30000 Training Loss: 0.06003522872924805\n",
      "Epoch 4119/30000 Training Loss: 0.05155084282159805\n",
      "Epoch 4120/30000 Training Loss: 0.06712736934423447\n",
      "Epoch 4121/30000 Training Loss: 0.06255436688661575\n",
      "Epoch 4122/30000 Training Loss: 0.05841977521777153\n",
      "Epoch 4123/30000 Training Loss: 0.04591765254735947\n",
      "Epoch 4124/30000 Training Loss: 0.05989360809326172\n",
      "Epoch 4125/30000 Training Loss: 0.05097681283950806\n",
      "Epoch 4126/30000 Training Loss: 0.05330447480082512\n",
      "Epoch 4127/30000 Training Loss: 0.06726629287004471\n",
      "Epoch 4128/30000 Training Loss: 0.05934382230043411\n",
      "Epoch 4129/30000 Training Loss: 0.0631503239274025\n",
      "Epoch 4130/30000 Training Loss: 0.055589187890291214\n",
      "Epoch 4131/30000 Training Loss: 0.060129426419734955\n",
      "Epoch 4132/30000 Training Loss: 0.06411971896886826\n",
      "Epoch 4133/30000 Training Loss: 0.06096913665533066\n",
      "Epoch 4134/30000 Training Loss: 0.054342806339263916\n",
      "Epoch 4135/30000 Training Loss: 0.0520680733025074\n",
      "Epoch 4136/30000 Training Loss: 0.045995041728019714\n",
      "Epoch 4137/30000 Training Loss: 0.0524318628013134\n",
      "Epoch 4138/30000 Training Loss: 0.04270276799798012\n",
      "Epoch 4139/30000 Training Loss: 0.052689116448163986\n",
      "Epoch 4140/30000 Training Loss: 0.06009593605995178\n",
      "Epoch 4141/30000 Training Loss: 0.05180991813540459\n",
      "Epoch 4142/30000 Training Loss: 0.05310598015785217\n",
      "Epoch 4143/30000 Training Loss: 0.05067892745137215\n",
      "Epoch 4144/30000 Training Loss: 0.053128622472286224\n",
      "Epoch 4145/30000 Training Loss: 0.05564754083752632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4146/30000 Training Loss: 0.05042310431599617\n",
      "Epoch 4147/30000 Training Loss: 0.056638896465301514\n",
      "Epoch 4148/30000 Training Loss: 0.058431368321180344\n",
      "Epoch 4149/30000 Training Loss: 0.053951531648635864\n",
      "Epoch 4150/30000 Training Loss: 0.04878462105989456\n",
      "Epoch 4150/30000 Validation Loss: 0.05307922512292862\n",
      "Epoch 4151/30000 Training Loss: 0.06373334676027298\n",
      "Epoch 4152/30000 Training Loss: 0.053996454924345016\n",
      "Epoch 4153/30000 Training Loss: 0.05549154803156853\n",
      "Epoch 4154/30000 Training Loss: 0.06113408878445625\n",
      "Epoch 4155/30000 Training Loss: 0.04924209415912628\n",
      "Epoch 4156/30000 Training Loss: 0.0600842721760273\n",
      "Epoch 4157/30000 Training Loss: 0.051419783383607864\n",
      "Epoch 4158/30000 Training Loss: 0.05976384878158569\n",
      "Epoch 4159/30000 Training Loss: 0.055991850793361664\n",
      "Epoch 4160/30000 Training Loss: 0.05599427968263626\n",
      "Epoch 4161/30000 Training Loss: 0.06094236299395561\n",
      "Epoch 4162/30000 Training Loss: 0.05011783167719841\n",
      "Epoch 4163/30000 Training Loss: 0.0502481684088707\n",
      "Epoch 4164/30000 Training Loss: 0.06551489979028702\n",
      "Epoch 4165/30000 Training Loss: 0.066044881939888\n",
      "Epoch 4166/30000 Training Loss: 0.05315910652279854\n",
      "Epoch 4167/30000 Training Loss: 0.054574429988861084\n",
      "Epoch 4168/30000 Training Loss: 0.060042787343263626\n",
      "Epoch 4169/30000 Training Loss: 0.051960647106170654\n",
      "Epoch 4170/30000 Training Loss: 0.05307471752166748\n",
      "Epoch 4171/30000 Training Loss: 0.06478381156921387\n",
      "Epoch 4172/30000 Training Loss: 0.04900958389043808\n",
      "Epoch 4173/30000 Training Loss: 0.06686953455209732\n",
      "Epoch 4174/30000 Training Loss: 0.057540129870176315\n",
      "Epoch 4175/30000 Training Loss: 0.05653738975524902\n",
      "Epoch 4176/30000 Training Loss: 0.05640599876642227\n",
      "Epoch 4177/30000 Training Loss: 0.05483267456293106\n",
      "Epoch 4178/30000 Training Loss: 0.06244125962257385\n",
      "Epoch 4179/30000 Training Loss: 0.05904032662510872\n",
      "Epoch 4180/30000 Training Loss: 0.05049198865890503\n",
      "Epoch 4181/30000 Training Loss: 0.06292511522769928\n",
      "Epoch 4182/30000 Training Loss: 0.04498498886823654\n",
      "Epoch 4183/30000 Training Loss: 0.05420169234275818\n",
      "Epoch 4184/30000 Training Loss: 0.05369080230593681\n",
      "Epoch 4185/30000 Training Loss: 0.05331823229789734\n",
      "Epoch 4186/30000 Training Loss: 0.05223844572901726\n",
      "Epoch 4187/30000 Training Loss: 0.05577055364847183\n",
      "Epoch 4188/30000 Training Loss: 0.0547565333545208\n",
      "Epoch 4189/30000 Training Loss: 0.056831639260053635\n",
      "Epoch 4190/30000 Training Loss: 0.05649365112185478\n",
      "Epoch 4191/30000 Training Loss: 0.057075273245573044\n",
      "Epoch 4192/30000 Training Loss: 0.05152453854680061\n",
      "Epoch 4193/30000 Training Loss: 0.06166670843958855\n",
      "Epoch 4194/30000 Training Loss: 0.06235858052968979\n",
      "Epoch 4195/30000 Training Loss: 0.06015012413263321\n",
      "Epoch 4196/30000 Training Loss: 0.06033474951982498\n",
      "Epoch 4197/30000 Training Loss: 0.04868554323911667\n",
      "Epoch 4198/30000 Training Loss: 0.06169532984495163\n",
      "Epoch 4199/30000 Training Loss: 0.06013375520706177\n",
      "Epoch 4200/30000 Training Loss: 0.05743459612131119\n",
      "Epoch 4200/30000 Validation Loss: 0.05815708637237549\n",
      "Epoch 4201/30000 Training Loss: 0.06033774092793465\n",
      "Epoch 4202/30000 Training Loss: 0.0667445957660675\n",
      "Epoch 4203/30000 Training Loss: 0.05143245309591293\n",
      "Epoch 4204/30000 Training Loss: 0.06660310924053192\n",
      "Epoch 4205/30000 Training Loss: 0.057087402790784836\n",
      "Epoch 4206/30000 Training Loss: 0.05749979615211487\n",
      "Epoch 4207/30000 Training Loss: 0.05861939117312431\n",
      "Epoch 4208/30000 Training Loss: 0.05928052216768265\n",
      "Epoch 4209/30000 Training Loss: 0.05581341311335564\n",
      "Epoch 4210/30000 Training Loss: 0.05837564542889595\n",
      "Epoch 4211/30000 Training Loss: 0.06681665778160095\n",
      "Epoch 4212/30000 Training Loss: 0.05609564110636711\n",
      "Epoch 4213/30000 Training Loss: 0.062108516693115234\n",
      "Epoch 4214/30000 Training Loss: 0.05177760869264603\n",
      "Epoch 4215/30000 Training Loss: 0.05276588350534439\n",
      "Epoch 4216/30000 Training Loss: 0.05873013287782669\n",
      "Epoch 4217/30000 Training Loss: 0.0506790354847908\n",
      "Epoch 4218/30000 Training Loss: 0.05737713724374771\n",
      "Epoch 4219/30000 Training Loss: 0.05174756795167923\n",
      "Epoch 4220/30000 Training Loss: 0.060174666345119476\n",
      "Epoch 4221/30000 Training Loss: 0.05171404406428337\n",
      "Epoch 4222/30000 Training Loss: 0.05665756016969681\n",
      "Epoch 4223/30000 Training Loss: 0.05998947098851204\n",
      "Epoch 4224/30000 Training Loss: 0.06071053072810173\n",
      "Epoch 4225/30000 Training Loss: 0.052157171070575714\n",
      "Epoch 4226/30000 Training Loss: 0.054486680775880814\n",
      "Epoch 4227/30000 Training Loss: 0.06250710785388947\n",
      "Epoch 4228/30000 Training Loss: 0.05453662946820259\n",
      "Epoch 4229/30000 Training Loss: 0.06657848507165909\n",
      "Epoch 4230/30000 Training Loss: 0.056988269090652466\n",
      "Epoch 4231/30000 Training Loss: 0.061825286597013474\n",
      "Epoch 4232/30000 Training Loss: 0.04744639992713928\n",
      "Epoch 4233/30000 Training Loss: 0.06145072728395462\n",
      "Epoch 4234/30000 Training Loss: 0.06463205069303513\n",
      "Epoch 4235/30000 Training Loss: 0.05758507177233696\n",
      "Epoch 4236/30000 Training Loss: 0.04898510500788689\n",
      "Epoch 4237/30000 Training Loss: 0.053849607706069946\n",
      "Epoch 4238/30000 Training Loss: 0.058587364852428436\n",
      "Epoch 4239/30000 Training Loss: 0.05297769978642464\n",
      "Epoch 4240/30000 Training Loss: 0.05193891376256943\n",
      "Epoch 4241/30000 Training Loss: 0.04927363991737366\n",
      "Epoch 4242/30000 Training Loss: 0.05118734389543533\n",
      "Epoch 4243/30000 Training Loss: 0.050411857664585114\n",
      "Epoch 4244/30000 Training Loss: 0.0585811510682106\n",
      "Epoch 4245/30000 Training Loss: 0.05707358196377754\n",
      "Epoch 4246/30000 Training Loss: 0.05388559028506279\n",
      "Epoch 4247/30000 Training Loss: 0.05422401428222656\n",
      "Epoch 4248/30000 Training Loss: 0.0590478889644146\n",
      "Epoch 4249/30000 Training Loss: 0.05155475065112114\n",
      "Epoch 4250/30000 Training Loss: 0.0619279220700264\n",
      "Epoch 4250/30000 Validation Loss: 0.05670567601919174\n",
      "Epoch 4251/30000 Training Loss: 0.05070384591817856\n",
      "Epoch 4252/30000 Training Loss: 0.055503688752651215\n",
      "Epoch 4253/30000 Training Loss: 0.06119954586029053\n",
      "Epoch 4254/30000 Training Loss: 0.05148622393608093\n",
      "Epoch 4255/30000 Training Loss: 0.053330112248659134\n",
      "Epoch 4256/30000 Training Loss: 0.07110825926065445\n",
      "Epoch 4257/30000 Training Loss: 0.06713144481182098\n",
      "Epoch 4258/30000 Training Loss: 0.055846892297267914\n",
      "Epoch 4259/30000 Training Loss: 0.061236899346113205\n",
      "Epoch 4260/30000 Training Loss: 0.06026246026158333\n",
      "Epoch 4261/30000 Training Loss: 0.05230218917131424\n",
      "Epoch 4262/30000 Training Loss: 0.05349384620785713\n",
      "Epoch 4263/30000 Training Loss: 0.06386780738830566\n",
      "Epoch 4264/30000 Training Loss: 0.05507189780473709\n",
      "Epoch 4265/30000 Training Loss: 0.05939211696386337\n",
      "Epoch 4266/30000 Training Loss: 0.05837450549006462\n",
      "Epoch 4267/30000 Training Loss: 0.046261537820100784\n",
      "Epoch 4268/30000 Training Loss: 0.0539080873131752\n",
      "Epoch 4269/30000 Training Loss: 0.05395505949854851\n",
      "Epoch 4270/30000 Training Loss: 0.052479349076747894\n",
      "Epoch 4271/30000 Training Loss: 0.050720952451229095\n",
      "Epoch 4272/30000 Training Loss: 0.058197177946567535\n",
      "Epoch 4273/30000 Training Loss: 0.05316806957125664\n",
      "Epoch 4274/30000 Training Loss: 0.05292996019124985\n",
      "Epoch 4275/30000 Training Loss: 0.05671415850520134\n",
      "Epoch 4276/30000 Training Loss: 0.0526043102145195\n",
      "Epoch 4277/30000 Training Loss: 0.04832591488957405\n",
      "Epoch 4278/30000 Training Loss: 0.07078574597835541\n",
      "Epoch 4279/30000 Training Loss: 0.048975009471178055\n",
      "Epoch 4280/30000 Training Loss: 0.06137371063232422\n",
      "Epoch 4281/30000 Training Loss: 0.05311436206102371\n",
      "Epoch 4282/30000 Training Loss: 0.05661885812878609\n",
      "Epoch 4283/30000 Training Loss: 0.05492138862609863\n",
      "Epoch 4284/30000 Training Loss: 0.056163180619478226\n",
      "Epoch 4285/30000 Training Loss: 0.0582767017185688\n",
      "Epoch 4286/30000 Training Loss: 0.0555092990398407\n",
      "Epoch 4287/30000 Training Loss: 0.04925941303372383\n",
      "Epoch 4288/30000 Training Loss: 0.04354208707809448\n",
      "Epoch 4289/30000 Training Loss: 0.05447448417544365\n",
      "Epoch 4290/30000 Training Loss: 0.06017838791012764\n",
      "Epoch 4291/30000 Training Loss: 0.06283301115036011\n",
      "Epoch 4292/30000 Training Loss: 0.053415995091199875\n",
      "Epoch 4293/30000 Training Loss: 0.055348314344882965\n",
      "Epoch 4294/30000 Training Loss: 0.06262461096048355\n",
      "Epoch 4295/30000 Training Loss: 0.057226140052080154\n",
      "Epoch 4296/30000 Training Loss: 0.058234237134456635\n",
      "Epoch 4297/30000 Training Loss: 0.04629799723625183\n",
      "Epoch 4298/30000 Training Loss: 0.05431312322616577\n",
      "Epoch 4299/30000 Training Loss: 0.05934491753578186\n",
      "Epoch 4300/30000 Training Loss: 0.05835527926683426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4300/30000 Validation Loss: 0.052617572247982025\n",
      "Epoch 4301/30000 Training Loss: 0.05796179920434952\n",
      "Epoch 4302/30000 Training Loss: 0.053612638264894485\n",
      "Epoch 4303/30000 Training Loss: 0.05820580571889877\n",
      "Epoch 4304/30000 Training Loss: 0.05892249196767807\n",
      "Epoch 4305/30000 Training Loss: 0.05951780080795288\n",
      "Epoch 4306/30000 Training Loss: 0.05558408424258232\n",
      "Epoch 4307/30000 Training Loss: 0.04804464429616928\n",
      "Epoch 4308/30000 Training Loss: 0.06179816275835037\n",
      "Epoch 4309/30000 Training Loss: 0.06449026614427567\n",
      "Epoch 4310/30000 Training Loss: 0.06130584329366684\n",
      "Epoch 4311/30000 Training Loss: 0.053029775619506836\n",
      "Epoch 4312/30000 Training Loss: 0.050820767879486084\n",
      "Epoch 4313/30000 Training Loss: 0.05654751509428024\n",
      "Epoch 4314/30000 Training Loss: 0.0489550456404686\n",
      "Epoch 4315/30000 Training Loss: 0.052316177636384964\n",
      "Epoch 4316/30000 Training Loss: 0.05693810060620308\n",
      "Epoch 4317/30000 Training Loss: 0.06336674839258194\n",
      "Epoch 4318/30000 Training Loss: 0.05121196433901787\n",
      "Epoch 4319/30000 Training Loss: 0.054723042994737625\n",
      "Epoch 4320/30000 Training Loss: 0.05481327697634697\n",
      "Epoch 4321/30000 Training Loss: 0.056681178510189056\n",
      "Epoch 4322/30000 Training Loss: 0.05727308243513107\n",
      "Epoch 4323/30000 Training Loss: 0.06116282939910889\n",
      "Epoch 4324/30000 Training Loss: 0.06188962608575821\n",
      "Epoch 4325/30000 Training Loss: 0.05595991760492325\n",
      "Epoch 4326/30000 Training Loss: 0.05422161892056465\n",
      "Epoch 4327/30000 Training Loss: 0.04742078483104706\n",
      "Epoch 4328/30000 Training Loss: 0.05620177835226059\n",
      "Epoch 4329/30000 Training Loss: 0.06477746367454529\n",
      "Epoch 4330/30000 Training Loss: 0.057386286556720734\n",
      "Epoch 4331/30000 Training Loss: 0.05874418467283249\n",
      "Epoch 4332/30000 Training Loss: 0.05590780824422836\n",
      "Epoch 4333/30000 Training Loss: 0.055118314921855927\n",
      "Epoch 4334/30000 Training Loss: 0.052702002227306366\n",
      "Epoch 4335/30000 Training Loss: 0.06163937598466873\n",
      "Epoch 4336/30000 Training Loss: 0.06438427418470383\n",
      "Epoch 4337/30000 Training Loss: 0.04696648195385933\n",
      "Epoch 4338/30000 Training Loss: 0.05674629658460617\n",
      "Epoch 4339/30000 Training Loss: 0.05348104238510132\n",
      "Epoch 4340/30000 Training Loss: 0.05207834765315056\n",
      "Epoch 4341/30000 Training Loss: 0.0520344153046608\n",
      "Epoch 4342/30000 Training Loss: 0.055501919239759445\n",
      "Epoch 4343/30000 Training Loss: 0.05482663959264755\n",
      "Epoch 4344/30000 Training Loss: 0.0648747980594635\n",
      "Epoch 4345/30000 Training Loss: 0.06137729808688164\n",
      "Epoch 4346/30000 Training Loss: 0.053189463913440704\n",
      "Epoch 4347/30000 Training Loss: 0.05130774527788162\n",
      "Epoch 4348/30000 Training Loss: 0.04873140528798103\n",
      "Epoch 4349/30000 Training Loss: 0.05249887704849243\n",
      "Epoch 4350/30000 Training Loss: 0.05492620915174484\n",
      "Epoch 4350/30000 Validation Loss: 0.05263551324605942\n",
      "Epoch 4351/30000 Training Loss: 0.05791710689663887\n",
      "Epoch 4352/30000 Training Loss: 0.053093500435352325\n",
      "Epoch 4353/30000 Training Loss: 0.043545715510845184\n",
      "Epoch 4354/30000 Training Loss: 0.060109205543994904\n",
      "Epoch 4355/30000 Training Loss: 0.058850448578596115\n",
      "Epoch 4356/30000 Training Loss: 0.05333223193883896\n",
      "Epoch 4357/30000 Training Loss: 0.061844658106565475\n",
      "Epoch 4358/30000 Training Loss: 0.052571721374988556\n",
      "Epoch 4359/30000 Training Loss: 0.05075870826840401\n",
      "Epoch 4360/30000 Training Loss: 0.04705728217959404\n",
      "Epoch 4361/30000 Training Loss: 0.059954654425382614\n",
      "Epoch 4362/30000 Training Loss: 0.041459061205387115\n",
      "Epoch 4363/30000 Training Loss: 0.05358629673719406\n",
      "Epoch 4364/30000 Training Loss: 0.05224791169166565\n",
      "Epoch 4365/30000 Training Loss: 0.06107097864151001\n",
      "Epoch 4366/30000 Training Loss: 0.050735533237457275\n",
      "Epoch 4367/30000 Training Loss: 0.05569254606962204\n",
      "Epoch 4368/30000 Training Loss: 0.05591030791401863\n",
      "Epoch 4369/30000 Training Loss: 0.047566257417201996\n",
      "Epoch 4370/30000 Training Loss: 0.05156157538294792\n",
      "Epoch 4371/30000 Training Loss: 0.055649448186159134\n",
      "Epoch 4372/30000 Training Loss: 0.05264279246330261\n",
      "Epoch 4373/30000 Training Loss: 0.05953208729624748\n",
      "Epoch 4374/30000 Training Loss: 0.05484110116958618\n",
      "Epoch 4375/30000 Training Loss: 0.05183076858520508\n",
      "Epoch 4376/30000 Training Loss: 0.05950760096311569\n",
      "Epoch 4377/30000 Training Loss: 0.05905793979763985\n",
      "Epoch 4378/30000 Training Loss: 0.05497133731842041\n",
      "Epoch 4379/30000 Training Loss: 0.05458392947912216\n",
      "Epoch 4380/30000 Training Loss: 0.04844572767615318\n",
      "Epoch 4381/30000 Training Loss: 0.047373075038194656\n",
      "Epoch 4382/30000 Training Loss: 0.059762291610240936\n",
      "Epoch 4383/30000 Training Loss: 0.05293181538581848\n",
      "Epoch 4384/30000 Training Loss: 0.05579804629087448\n",
      "Epoch 4385/30000 Training Loss: 0.05030118301510811\n",
      "Epoch 4386/30000 Training Loss: 0.06111414358019829\n",
      "Epoch 4387/30000 Training Loss: 0.04520828276872635\n",
      "Epoch 4388/30000 Training Loss: 0.04768192023038864\n",
      "Epoch 4389/30000 Training Loss: 0.05352505296468735\n",
      "Epoch 4390/30000 Training Loss: 0.06039705127477646\n",
      "Epoch 4391/30000 Training Loss: 0.055481236428022385\n",
      "Epoch 4392/30000 Training Loss: 0.05828596279025078\n",
      "Epoch 4393/30000 Training Loss: 0.05155803635716438\n",
      "Epoch 4394/30000 Training Loss: 0.0517151840031147\n",
      "Epoch 4395/30000 Training Loss: 0.05513855069875717\n",
      "Epoch 4396/30000 Training Loss: 0.05951119586825371\n",
      "Epoch 4397/30000 Training Loss: 0.048780232667922974\n",
      "Epoch 4398/30000 Training Loss: 0.05661683529615402\n",
      "Epoch 4399/30000 Training Loss: 0.05825295299291611\n",
      "Epoch 4400/30000 Training Loss: 0.056382566690444946\n",
      "Epoch 4400/30000 Validation Loss: 0.05572165176272392\n",
      "Epoch 4401/30000 Training Loss: 0.04865347594022751\n",
      "Epoch 4402/30000 Training Loss: 0.052055519074201584\n",
      "Epoch 4403/30000 Training Loss: 0.05765280872583389\n",
      "Epoch 4404/30000 Training Loss: 0.048312678933143616\n",
      "Epoch 4405/30000 Training Loss: 0.05417479947209358\n",
      "Epoch 4406/30000 Training Loss: 0.06340721994638443\n",
      "Epoch 4407/30000 Training Loss: 0.06432316452264786\n",
      "Epoch 4408/30000 Training Loss: 0.05247420817613602\n",
      "Epoch 4409/30000 Training Loss: 0.05537477880716324\n",
      "Epoch 4410/30000 Training Loss: 0.05523527413606644\n",
      "Epoch 4411/30000 Training Loss: 0.06252972036600113\n",
      "Epoch 4412/30000 Training Loss: 0.0541539192199707\n",
      "Epoch 4413/30000 Training Loss: 0.05580819398164749\n",
      "Epoch 4414/30000 Training Loss: 0.05141009762883186\n",
      "Epoch 4415/30000 Training Loss: 0.05452721565961838\n",
      "Epoch 4416/30000 Training Loss: 0.06027977913618088\n",
      "Epoch 4417/30000 Training Loss: 0.056330662220716476\n",
      "Epoch 4418/30000 Training Loss: 0.051952820271253586\n",
      "Epoch 4419/30000 Training Loss: 0.06745254993438721\n",
      "Epoch 4420/30000 Training Loss: 0.054954707622528076\n",
      "Epoch 4421/30000 Training Loss: 0.05588861554861069\n",
      "Epoch 4422/30000 Training Loss: 0.052308421581983566\n",
      "Epoch 4423/30000 Training Loss: 0.05431971698999405\n",
      "Epoch 4424/30000 Training Loss: 0.061171770095825195\n",
      "Epoch 4425/30000 Training Loss: 0.05425787717103958\n",
      "Epoch 4426/30000 Training Loss: 0.058881938457489014\n",
      "Epoch 4427/30000 Training Loss: 0.053437840193510056\n",
      "Epoch 4428/30000 Training Loss: 0.055220820009708405\n",
      "Epoch 4429/30000 Training Loss: 0.04875767603516579\n",
      "Epoch 4430/30000 Training Loss: 0.056697528809309006\n",
      "Epoch 4431/30000 Training Loss: 0.05820082500576973\n",
      "Epoch 4432/30000 Training Loss: 0.0558524951338768\n",
      "Epoch 4433/30000 Training Loss: 0.053432513028383255\n",
      "Epoch 4434/30000 Training Loss: 0.05004008486866951\n",
      "Epoch 4435/30000 Training Loss: 0.053143102675676346\n",
      "Epoch 4436/30000 Training Loss: 0.05040665343403816\n",
      "Epoch 4437/30000 Training Loss: 0.06335531175136566\n",
      "Epoch 4438/30000 Training Loss: 0.05850904434919357\n",
      "Epoch 4439/30000 Training Loss: 0.06343792378902435\n",
      "Epoch 4440/30000 Training Loss: 0.06954608857631683\n",
      "Epoch 4441/30000 Training Loss: 0.04893958568572998\n",
      "Epoch 4442/30000 Training Loss: 0.05787299945950508\n",
      "Epoch 4443/30000 Training Loss: 0.05751369148492813\n",
      "Epoch 4444/30000 Training Loss: 0.048793189227581024\n",
      "Epoch 4445/30000 Training Loss: 0.0634029433131218\n",
      "Epoch 4446/30000 Training Loss: 0.06019454076886177\n",
      "Epoch 4447/30000 Training Loss: 0.050985585898160934\n",
      "Epoch 4448/30000 Training Loss: 0.06261584907770157\n",
      "Epoch 4449/30000 Training Loss: 0.053878068923950195\n",
      "Epoch 4450/30000 Training Loss: 0.05410291627049446\n",
      "Epoch 4450/30000 Validation Loss: 0.05801047757267952\n",
      "Epoch 4451/30000 Training Loss: 0.04582052677869797\n",
      "Epoch 4452/30000 Training Loss: 0.05385401099920273\n",
      "Epoch 4453/30000 Training Loss: 0.06002548336982727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4454/30000 Training Loss: 0.055034201592206955\n",
      "Epoch 4455/30000 Training Loss: 0.050507109612226486\n",
      "Epoch 4456/30000 Training Loss: 0.05208795145153999\n",
      "Epoch 4457/30000 Training Loss: 0.059638865292072296\n",
      "Epoch 4458/30000 Training Loss: 0.062202710658311844\n",
      "Epoch 4459/30000 Training Loss: 0.055701982229948044\n",
      "Epoch 4460/30000 Training Loss: 0.06072263792157173\n",
      "Epoch 4461/30000 Training Loss: 0.05791567638516426\n",
      "Epoch 4462/30000 Training Loss: 0.05459468811750412\n",
      "Epoch 4463/30000 Training Loss: 0.05332457274198532\n",
      "Epoch 4464/30000 Training Loss: 0.0444745197892189\n",
      "Epoch 4465/30000 Training Loss: 0.052242718636989594\n",
      "Epoch 4466/30000 Training Loss: 0.05253493785858154\n",
      "Epoch 4467/30000 Training Loss: 0.05184999108314514\n",
      "Epoch 4468/30000 Training Loss: 0.058655064553022385\n",
      "Epoch 4469/30000 Training Loss: 0.060943782329559326\n",
      "Epoch 4470/30000 Training Loss: 0.05157318711280823\n",
      "Epoch 4471/30000 Training Loss: 0.05005507916212082\n",
      "Epoch 4472/30000 Training Loss: 0.05349571257829666\n",
      "Epoch 4473/30000 Training Loss: 0.04852968454360962\n",
      "Epoch 4474/30000 Training Loss: 0.05906107276678085\n",
      "Epoch 4475/30000 Training Loss: 0.060938216745853424\n",
      "Epoch 4476/30000 Training Loss: 0.06463979184627533\n",
      "Epoch 4477/30000 Training Loss: 0.049499042332172394\n",
      "Epoch 4478/30000 Training Loss: 0.0505223274230957\n",
      "Epoch 4479/30000 Training Loss: 0.05505898594856262\n",
      "Epoch 4480/30000 Training Loss: 0.05758099630475044\n",
      "Epoch 4481/30000 Training Loss: 0.0518658347427845\n",
      "Epoch 4482/30000 Training Loss: 0.063548743724823\n",
      "Epoch 4483/30000 Training Loss: 0.047415781766176224\n",
      "Epoch 4484/30000 Training Loss: 0.05579989403486252\n",
      "Epoch 4485/30000 Training Loss: 0.05987606570124626\n",
      "Epoch 4486/30000 Training Loss: 0.06058897823095322\n",
      "Epoch 4487/30000 Training Loss: 0.06055097654461861\n",
      "Epoch 4488/30000 Training Loss: 0.049312055110931396\n",
      "Epoch 4489/30000 Training Loss: 0.05352619290351868\n",
      "Epoch 4490/30000 Training Loss: 0.05863754078745842\n",
      "Epoch 4491/30000 Training Loss: 0.05549175664782524\n",
      "Epoch 4492/30000 Training Loss: 0.05247897654771805\n",
      "Epoch 4493/30000 Training Loss: 0.058567583560943604\n",
      "Epoch 4494/30000 Training Loss: 0.04438911750912666\n",
      "Epoch 4495/30000 Training Loss: 0.05826625972986221\n",
      "Epoch 4496/30000 Training Loss: 0.048724789172410965\n",
      "Epoch 4497/30000 Training Loss: 0.057069431990385056\n",
      "Epoch 4498/30000 Training Loss: 0.06091931462287903\n",
      "Epoch 4499/30000 Training Loss: 0.053210221230983734\n",
      "Epoch 4500/30000 Training Loss: 0.05746040865778923\n",
      "Epoch 4500/30000 Validation Loss: 0.05409673973917961\n",
      "Epoch 4501/30000 Training Loss: 0.05834328010678291\n",
      "Epoch 4502/30000 Training Loss: 0.06339462101459503\n",
      "Epoch 4503/30000 Training Loss: 0.061856843531131744\n",
      "Epoch 4504/30000 Training Loss: 0.05504608154296875\n",
      "Epoch 4505/30000 Training Loss: 0.04875052720308304\n",
      "Epoch 4506/30000 Training Loss: 0.054335493594408035\n",
      "Epoch 4507/30000 Training Loss: 0.061892688274383545\n",
      "Epoch 4508/30000 Training Loss: 0.05417174845933914\n",
      "Epoch 4509/30000 Training Loss: 0.04668077826499939\n",
      "Epoch 4510/30000 Training Loss: 0.06007974594831467\n",
      "Epoch 4511/30000 Training Loss: 0.05305477976799011\n",
      "Epoch 4512/30000 Training Loss: 0.06369531899690628\n",
      "Epoch 4513/30000 Training Loss: 0.051169224083423615\n",
      "Epoch 4514/30000 Training Loss: 0.05373626947402954\n",
      "Epoch 4515/30000 Training Loss: 0.05311884731054306\n",
      "Epoch 4516/30000 Training Loss: 0.043163664638996124\n",
      "Epoch 4517/30000 Training Loss: 0.05557261034846306\n",
      "Epoch 4518/30000 Training Loss: 0.05229295417666435\n",
      "Epoch 4519/30000 Training Loss: 0.049438681453466415\n",
      "Epoch 4520/30000 Training Loss: 0.05316424369812012\n",
      "Epoch 4521/30000 Training Loss: 0.06274575740098953\n",
      "Epoch 4522/30000 Training Loss: 0.05305048078298569\n",
      "Epoch 4523/30000 Training Loss: 0.05275833606719971\n",
      "Epoch 4524/30000 Training Loss: 0.04767290875315666\n",
      "Epoch 4525/30000 Training Loss: 0.06365766376256943\n",
      "Epoch 4526/30000 Training Loss: 0.05051730200648308\n",
      "Epoch 4527/30000 Training Loss: 0.05495079606771469\n",
      "Epoch 4528/30000 Training Loss: 0.05085526034235954\n",
      "Epoch 4529/30000 Training Loss: 0.05874587967991829\n",
      "Epoch 4530/30000 Training Loss: 0.05593304708600044\n",
      "Epoch 4531/30000 Training Loss: 0.05506516620516777\n",
      "Epoch 4532/30000 Training Loss: 0.063392773270607\n",
      "Epoch 4533/30000 Training Loss: 0.05627303570508957\n",
      "Epoch 4534/30000 Training Loss: 0.05481671169400215\n",
      "Epoch 4535/30000 Training Loss: 0.059137605130672455\n",
      "Epoch 4536/30000 Training Loss: 0.05086641386151314\n",
      "Epoch 4537/30000 Training Loss: 0.05157622694969177\n",
      "Epoch 4538/30000 Training Loss: 0.057043276727199554\n",
      "Epoch 4539/30000 Training Loss: 0.05270172283053398\n",
      "Epoch 4540/30000 Training Loss: 0.05786607414484024\n",
      "Epoch 4541/30000 Training Loss: 0.054461054503917694\n",
      "Epoch 4542/30000 Training Loss: 0.05423952266573906\n",
      "Epoch 4543/30000 Training Loss: 0.0618889145553112\n",
      "Epoch 4544/30000 Training Loss: 0.053986500948667526\n",
      "Epoch 4545/30000 Training Loss: 0.05390921235084534\n",
      "Epoch 4546/30000 Training Loss: 0.055629193782806396\n",
      "Epoch 4547/30000 Training Loss: 0.06628800928592682\n",
      "Epoch 4548/30000 Training Loss: 0.059513963758945465\n",
      "Epoch 4549/30000 Training Loss: 0.05836649611592293\n",
      "Epoch 4550/30000 Training Loss: 0.05083111673593521\n",
      "Epoch 4550/30000 Validation Loss: 0.0637219026684761\n",
      "Epoch 4551/30000 Training Loss: 0.05147436261177063\n",
      "Epoch 4552/30000 Training Loss: 0.04737744852900505\n",
      "Epoch 4553/30000 Training Loss: 0.058505572378635406\n",
      "Epoch 4554/30000 Training Loss: 0.061885248869657516\n",
      "Epoch 4555/30000 Training Loss: 0.05409017205238342\n",
      "Epoch 4556/30000 Training Loss: 0.05322537571191788\n",
      "Epoch 4557/30000 Training Loss: 0.04546133801341057\n",
      "Epoch 4558/30000 Training Loss: 0.058094918727874756\n",
      "Epoch 4559/30000 Training Loss: 0.050613611936569214\n",
      "Epoch 4560/30000 Training Loss: 0.05840766429901123\n",
      "Epoch 4561/30000 Training Loss: 0.051252204924821854\n",
      "Epoch 4562/30000 Training Loss: 0.059937287122011185\n",
      "Epoch 4563/30000 Training Loss: 0.05399894714355469\n",
      "Epoch 4564/30000 Training Loss: 0.05649765580892563\n",
      "Epoch 4565/30000 Training Loss: 0.05406052991747856\n",
      "Epoch 4566/30000 Training Loss: 0.049665145576000214\n",
      "Epoch 4567/30000 Training Loss: 0.047183115035295486\n",
      "Epoch 4568/30000 Training Loss: 0.05819554254412651\n",
      "Epoch 4569/30000 Training Loss: 0.05383089929819107\n",
      "Epoch 4570/30000 Training Loss: 0.07601343095302582\n",
      "Epoch 4571/30000 Training Loss: 0.05229067802429199\n",
      "Epoch 4572/30000 Training Loss: 0.06459017843008041\n",
      "Epoch 4573/30000 Training Loss: 0.061853695660829544\n",
      "Epoch 4574/30000 Training Loss: 0.05687471106648445\n",
      "Epoch 4575/30000 Training Loss: 0.05424414947628975\n",
      "Epoch 4576/30000 Training Loss: 0.06765089929103851\n",
      "Epoch 4577/30000 Training Loss: 0.046535249799489975\n",
      "Epoch 4578/30000 Training Loss: 0.05875682830810547\n",
      "Epoch 4579/30000 Training Loss: 0.05423450469970703\n",
      "Epoch 4580/30000 Training Loss: 0.052184950560331345\n",
      "Epoch 4581/30000 Training Loss: 0.05134234577417374\n",
      "Epoch 4582/30000 Training Loss: 0.052550870925188065\n",
      "Epoch 4583/30000 Training Loss: 0.055279143154621124\n",
      "Epoch 4584/30000 Training Loss: 0.060955654829740524\n",
      "Epoch 4585/30000 Training Loss: 0.0495181642472744\n",
      "Epoch 4586/30000 Training Loss: 0.052938174456357956\n",
      "Epoch 4587/30000 Training Loss: 0.06866073608398438\n",
      "Epoch 4588/30000 Training Loss: 0.051351286470890045\n",
      "Epoch 4589/30000 Training Loss: 0.05909334868192673\n",
      "Epoch 4590/30000 Training Loss: 0.04975522682070732\n",
      "Epoch 4591/30000 Training Loss: 0.059420596808195114\n",
      "Epoch 4592/30000 Training Loss: 0.05598631501197815\n",
      "Epoch 4593/30000 Training Loss: 0.051469527184963226\n",
      "Epoch 4594/30000 Training Loss: 0.048523467034101486\n",
      "Epoch 4595/30000 Training Loss: 0.05308981612324715\n",
      "Epoch 4596/30000 Training Loss: 0.05278182029724121\n",
      "Epoch 4597/30000 Training Loss: 0.05515154451131821\n",
      "Epoch 4598/30000 Training Loss: 0.05259450525045395\n",
      "Epoch 4599/30000 Training Loss: 0.04925693944096565\n",
      "Epoch 4600/30000 Training Loss: 0.05310385301709175\n",
      "Epoch 4600/30000 Validation Loss: 0.055269449949264526\n",
      "Epoch 4601/30000 Training Loss: 0.053379423916339874\n",
      "Epoch 4602/30000 Training Loss: 0.05389607697725296\n",
      "Epoch 4603/30000 Training Loss: 0.04783047363162041\n",
      "Epoch 4604/30000 Training Loss: 0.05924403667449951\n",
      "Epoch 4605/30000 Training Loss: 0.053344082087278366\n",
      "Epoch 4606/30000 Training Loss: 0.05520879477262497\n",
      "Epoch 4607/30000 Training Loss: 0.05223304033279419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4608/30000 Training Loss: 0.058148015290498734\n",
      "Epoch 4609/30000 Training Loss: 0.05694860219955444\n",
      "Epoch 4610/30000 Training Loss: 0.048862237483263016\n",
      "Epoch 4611/30000 Training Loss: 0.054213814437389374\n",
      "Epoch 4612/30000 Training Loss: 0.049666814506053925\n",
      "Epoch 4613/30000 Training Loss: 0.05238896608352661\n",
      "Epoch 4614/30000 Training Loss: 0.05560079216957092\n",
      "Epoch 4615/30000 Training Loss: 0.051151253283023834\n",
      "Epoch 4616/30000 Training Loss: 0.05268295854330063\n",
      "Epoch 4617/30000 Training Loss: 0.0643908679485321\n",
      "Epoch 4618/30000 Training Loss: 0.057436466217041016\n",
      "Epoch 4619/30000 Training Loss: 0.06105848401784897\n",
      "Epoch 4620/30000 Training Loss: 0.047108493745326996\n",
      "Epoch 4621/30000 Training Loss: 0.058917172253131866\n",
      "Epoch 4622/30000 Training Loss: 0.05080466717481613\n",
      "Epoch 4623/30000 Training Loss: 0.060242652893066406\n",
      "Epoch 4624/30000 Training Loss: 0.06251133978366852\n",
      "Epoch 4625/30000 Training Loss: 0.06600204110145569\n",
      "Epoch 4626/30000 Training Loss: 0.04408170282840729\n",
      "Epoch 4627/30000 Training Loss: 0.048213373869657516\n",
      "Epoch 4628/30000 Training Loss: 0.05899815633893013\n",
      "Epoch 4629/30000 Training Loss: 0.050792206078767776\n",
      "Epoch 4630/30000 Training Loss: 0.04540897160768509\n",
      "Epoch 4631/30000 Training Loss: 0.05229763314127922\n",
      "Epoch 4632/30000 Training Loss: 0.05942182615399361\n",
      "Epoch 4633/30000 Training Loss: 0.05540340393781662\n",
      "Epoch 4634/30000 Training Loss: 0.06057658791542053\n",
      "Epoch 4635/30000 Training Loss: 0.05408380553126335\n",
      "Epoch 4636/30000 Training Loss: 0.06069917231798172\n",
      "Epoch 4637/30000 Training Loss: 0.04898061603307724\n",
      "Epoch 4638/30000 Training Loss: 0.050680894404649734\n",
      "Epoch 4639/30000 Training Loss: 0.056506626307964325\n",
      "Epoch 4640/30000 Training Loss: 0.049953244626522064\n",
      "Epoch 4641/30000 Training Loss: 0.048107728362083435\n",
      "Epoch 4642/30000 Training Loss: 0.047052331268787384\n",
      "Epoch 4643/30000 Training Loss: 0.05810556560754776\n",
      "Epoch 4644/30000 Training Loss: 0.053068239241838455\n",
      "Epoch 4645/30000 Training Loss: 0.0569811686873436\n",
      "Epoch 4646/30000 Training Loss: 0.05908769369125366\n",
      "Epoch 4647/30000 Training Loss: 0.05889606475830078\n",
      "Epoch 4648/30000 Training Loss: 0.052496593445539474\n",
      "Epoch 4649/30000 Training Loss: 0.05189888924360275\n",
      "Epoch 4650/30000 Training Loss: 0.049016404896974564\n",
      "Epoch 4650/30000 Validation Loss: 0.07428650557994843\n",
      "Epoch 4651/30000 Training Loss: 0.06382371485233307\n",
      "Epoch 4652/30000 Training Loss: 0.06572835147380829\n",
      "Epoch 4653/30000 Training Loss: 0.059552229940891266\n",
      "Epoch 4654/30000 Training Loss: 0.054003458470106125\n",
      "Epoch 4655/30000 Training Loss: 0.0514601469039917\n",
      "Epoch 4656/30000 Training Loss: 0.06307275593280792\n",
      "Epoch 4657/30000 Training Loss: 0.05566287785768509\n",
      "Epoch 4658/30000 Training Loss: 0.06213779002428055\n",
      "Epoch 4659/30000 Training Loss: 0.051566313952207565\n",
      "Epoch 4660/30000 Training Loss: 0.05964566022157669\n",
      "Epoch 4661/30000 Training Loss: 0.055944301187992096\n",
      "Epoch 4662/30000 Training Loss: 0.04884156584739685\n",
      "Epoch 4663/30000 Training Loss: 0.05187118053436279\n",
      "Epoch 4664/30000 Training Loss: 0.056209832429885864\n",
      "Epoch 4665/30000 Training Loss: 0.056627027690410614\n",
      "Epoch 4666/30000 Training Loss: 0.06340457499027252\n",
      "Epoch 4667/30000 Training Loss: 0.05697796493768692\n",
      "Epoch 4668/30000 Training Loss: 0.052272163331508636\n",
      "Epoch 4669/30000 Training Loss: 0.05078772455453873\n",
      "Epoch 4670/30000 Training Loss: 0.05589817836880684\n",
      "Epoch 4671/30000 Training Loss: 0.06111816316843033\n",
      "Epoch 4672/30000 Training Loss: 0.05611635372042656\n",
      "Epoch 4673/30000 Training Loss: 0.05894837528467178\n",
      "Epoch 4674/30000 Training Loss: 0.05255074426531792\n",
      "Epoch 4675/30000 Training Loss: 0.056544892489910126\n",
      "Epoch 4676/30000 Training Loss: 0.049616605043411255\n",
      "Epoch 4677/30000 Training Loss: 0.06172587722539902\n",
      "Epoch 4678/30000 Training Loss: 0.05436971038579941\n",
      "Epoch 4679/30000 Training Loss: 0.05165275186300278\n",
      "Epoch 4680/30000 Training Loss: 0.05851299688220024\n",
      "Epoch 4681/30000 Training Loss: 0.05025527626276016\n",
      "Epoch 4682/30000 Training Loss: 0.045864999294281006\n",
      "Epoch 4683/30000 Training Loss: 0.059382181614637375\n",
      "Epoch 4684/30000 Training Loss: 0.06299857050180435\n",
      "Epoch 4685/30000 Training Loss: 0.051939915865659714\n",
      "Epoch 4686/30000 Training Loss: 0.061583369970321655\n",
      "Epoch 4687/30000 Training Loss: 0.05306515097618103\n",
      "Epoch 4688/30000 Training Loss: 0.06367450207471848\n",
      "Epoch 4689/30000 Training Loss: 0.05528222769498825\n",
      "Epoch 4690/30000 Training Loss: 0.06070870906114578\n",
      "Epoch 4691/30000 Training Loss: 0.047445423901081085\n",
      "Epoch 4692/30000 Training Loss: 0.053357142955064774\n",
      "Epoch 4693/30000 Training Loss: 0.04919542372226715\n",
      "Epoch 4694/30000 Training Loss: 0.060079820454120636\n",
      "Epoch 4695/30000 Training Loss: 0.058519501239061356\n",
      "Epoch 4696/30000 Training Loss: 0.05127192288637161\n",
      "Epoch 4697/30000 Training Loss: 0.061401307582855225\n",
      "Epoch 4698/30000 Training Loss: 0.052442025393247604\n",
      "Epoch 4699/30000 Training Loss: 0.05341688543558121\n",
      "Epoch 4700/30000 Training Loss: 0.05268331244587898\n",
      "Epoch 4700/30000 Validation Loss: 0.05613807961344719\n",
      "Epoch 4701/30000 Training Loss: 0.054448455572128296\n",
      "Epoch 4702/30000 Training Loss: 0.05635417625308037\n",
      "Epoch 4703/30000 Training Loss: 0.05163843184709549\n",
      "Epoch 4704/30000 Training Loss: 0.048380203545093536\n",
      "Epoch 4705/30000 Training Loss: 0.04369327053427696\n",
      "Epoch 4706/30000 Training Loss: 0.0560847632586956\n",
      "Epoch 4707/30000 Training Loss: 0.05595821887254715\n",
      "Epoch 4708/30000 Training Loss: 0.05517170578241348\n",
      "Epoch 4709/30000 Training Loss: 0.054176926612854004\n",
      "Epoch 4710/30000 Training Loss: 0.06185705587267876\n",
      "Epoch 4711/30000 Training Loss: 0.0628734603524208\n",
      "Epoch 4712/30000 Training Loss: 0.06075241044163704\n",
      "Epoch 4713/30000 Training Loss: 0.05183185264468193\n",
      "Epoch 4714/30000 Training Loss: 0.05871627852320671\n",
      "Epoch 4715/30000 Training Loss: 0.05760357528924942\n",
      "Epoch 4716/30000 Training Loss: 0.05138934403657913\n",
      "Epoch 4717/30000 Training Loss: 0.05777587741613388\n",
      "Epoch 4718/30000 Training Loss: 0.05097012594342232\n",
      "Epoch 4719/30000 Training Loss: 0.051789768040180206\n",
      "Epoch 4720/30000 Training Loss: 0.04411953315138817\n",
      "Epoch 4721/30000 Training Loss: 0.05388947203755379\n",
      "Epoch 4722/30000 Training Loss: 0.05924864858388901\n",
      "Epoch 4723/30000 Training Loss: 0.06140859052538872\n",
      "Epoch 4724/30000 Training Loss: 0.05359405279159546\n",
      "Epoch 4725/30000 Training Loss: 0.055974312126636505\n",
      "Epoch 4726/30000 Training Loss: 0.05996789410710335\n",
      "Epoch 4727/30000 Training Loss: 0.053656406700611115\n",
      "Epoch 4728/30000 Training Loss: 0.050681065768003464\n",
      "Epoch 4729/30000 Training Loss: 0.055033326148986816\n",
      "Epoch 4730/30000 Training Loss: 0.0505666621029377\n",
      "Epoch 4731/30000 Training Loss: 0.05593651533126831\n",
      "Epoch 4732/30000 Training Loss: 0.05561617761850357\n",
      "Epoch 4733/30000 Training Loss: 0.04365570470690727\n",
      "Epoch 4734/30000 Training Loss: 0.052153218537569046\n",
      "Epoch 4735/30000 Training Loss: 0.05666649341583252\n",
      "Epoch 4736/30000 Training Loss: 0.05723649263381958\n",
      "Epoch 4737/30000 Training Loss: 0.05024624988436699\n",
      "Epoch 4738/30000 Training Loss: 0.055986545979976654\n",
      "Epoch 4739/30000 Training Loss: 0.057913728058338165\n",
      "Epoch 4740/30000 Training Loss: 0.04972591623663902\n",
      "Epoch 4741/30000 Training Loss: 0.05151820927858353\n",
      "Epoch 4742/30000 Training Loss: 0.05764206126332283\n",
      "Epoch 4743/30000 Training Loss: 0.05247194319963455\n",
      "Epoch 4744/30000 Training Loss: 0.055024076253175735\n",
      "Epoch 4745/30000 Training Loss: 0.053404517471790314\n",
      "Epoch 4746/30000 Training Loss: 0.0538458414375782\n",
      "Epoch 4747/30000 Training Loss: 0.049584586173295975\n",
      "Epoch 4748/30000 Training Loss: 0.05196332186460495\n",
      "Epoch 4749/30000 Training Loss: 0.05273881554603577\n",
      "Epoch 4750/30000 Training Loss: 0.05546141788363457\n",
      "Epoch 4750/30000 Validation Loss: 0.049216508865356445\n",
      "Epoch 4751/30000 Training Loss: 0.05716472864151001\n",
      "Epoch 4752/30000 Training Loss: 0.05600817874073982\n",
      "Epoch 4753/30000 Training Loss: 0.05153386667370796\n",
      "Epoch 4754/30000 Training Loss: 0.06415429711341858\n",
      "Epoch 4755/30000 Training Loss: 0.04856159910559654\n",
      "Epoch 4756/30000 Training Loss: 0.05802149325609207\n",
      "Epoch 4757/30000 Training Loss: 0.05260036513209343\n",
      "Epoch 4758/30000 Training Loss: 0.05885488912463188\n",
      "Epoch 4759/30000 Training Loss: 0.04488901421427727\n",
      "Epoch 4760/30000 Training Loss: 0.0513659305870533\n",
      "Epoch 4761/30000 Training Loss: 0.049687668681144714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4762/30000 Training Loss: 0.06146051362156868\n",
      "Epoch 4763/30000 Training Loss: 0.047631870955228806\n",
      "Epoch 4764/30000 Training Loss: 0.05594946816563606\n",
      "Epoch 4765/30000 Training Loss: 0.05628436803817749\n",
      "Epoch 4766/30000 Training Loss: 0.04841175675392151\n",
      "Epoch 4767/30000 Training Loss: 0.06095626950263977\n",
      "Epoch 4768/30000 Training Loss: 0.055855222046375275\n",
      "Epoch 4769/30000 Training Loss: 0.0640624538064003\n",
      "Epoch 4770/30000 Training Loss: 0.05505185201764107\n",
      "Epoch 4771/30000 Training Loss: 0.06790398061275482\n",
      "Epoch 4772/30000 Training Loss: 0.05772838741540909\n",
      "Epoch 4773/30000 Training Loss: 0.05628682300448418\n",
      "Epoch 4774/30000 Training Loss: 0.048297375440597534\n",
      "Epoch 4775/30000 Training Loss: 0.05112354829907417\n",
      "Epoch 4776/30000 Training Loss: 0.050081901252269745\n",
      "Epoch 4777/30000 Training Loss: 0.05322941392660141\n",
      "Epoch 4778/30000 Training Loss: 0.05222086235880852\n",
      "Epoch 4779/30000 Training Loss: 0.056025274097919464\n",
      "Epoch 4780/30000 Training Loss: 0.0567919984459877\n",
      "Epoch 4781/30000 Training Loss: 0.062091607600450516\n",
      "Epoch 4782/30000 Training Loss: 0.06452570855617523\n",
      "Epoch 4783/30000 Training Loss: 0.053682707250118256\n",
      "Epoch 4784/30000 Training Loss: 0.04861301928758621\n",
      "Epoch 4785/30000 Training Loss: 0.05714557692408562\n",
      "Epoch 4786/30000 Training Loss: 0.056035690009593964\n",
      "Epoch 4787/30000 Training Loss: 0.059864044189453125\n",
      "Epoch 4788/30000 Training Loss: 0.05933588743209839\n",
      "Epoch 4789/30000 Training Loss: 0.05356346815824509\n",
      "Epoch 4790/30000 Training Loss: 0.051233552396297455\n",
      "Epoch 4791/30000 Training Loss: 0.058904580771923065\n",
      "Epoch 4792/30000 Training Loss: 0.05334009975194931\n",
      "Epoch 4793/30000 Training Loss: 0.05868681147694588\n",
      "Epoch 4794/30000 Training Loss: 0.05593305081129074\n",
      "Epoch 4795/30000 Training Loss: 0.04967020824551582\n",
      "Epoch 4796/30000 Training Loss: 0.05047530680894852\n",
      "Epoch 4797/30000 Training Loss: 0.04850731045007706\n",
      "Epoch 4798/30000 Training Loss: 0.04772967845201492\n",
      "Epoch 4799/30000 Training Loss: 0.060474567115306854\n",
      "Epoch 4800/30000 Training Loss: 0.05663195252418518\n",
      "Epoch 4800/30000 Validation Loss: 0.05659598112106323\n",
      "Epoch 4801/30000 Training Loss: 0.056677330285310745\n",
      "Epoch 4802/30000 Training Loss: 0.05314926430583\n",
      "Epoch 4803/30000 Training Loss: 0.04380954056978226\n",
      "Epoch 4804/30000 Training Loss: 0.05387056991457939\n",
      "Epoch 4805/30000 Training Loss: 0.05933275818824768\n",
      "Epoch 4806/30000 Training Loss: 0.050932545214891434\n",
      "Epoch 4807/30000 Training Loss: 0.04945624992251396\n",
      "Epoch 4808/30000 Training Loss: 0.0631915032863617\n",
      "Epoch 4809/30000 Training Loss: 0.059024833142757416\n",
      "Epoch 4810/30000 Training Loss: 0.05566481873393059\n",
      "Epoch 4811/30000 Training Loss: 0.05785957723855972\n",
      "Epoch 4812/30000 Training Loss: 0.057360995560884476\n",
      "Epoch 4813/30000 Training Loss: 0.04655854031443596\n",
      "Epoch 4814/30000 Training Loss: 0.04469633102416992\n",
      "Epoch 4815/30000 Training Loss: 0.04891662672162056\n",
      "Epoch 4816/30000 Training Loss: 0.05447499826550484\n",
      "Epoch 4817/30000 Training Loss: 0.053738854825496674\n",
      "Epoch 4818/30000 Training Loss: 0.05243971198797226\n",
      "Epoch 4819/30000 Training Loss: 0.05965758115053177\n",
      "Epoch 4820/30000 Training Loss: 0.04408068209886551\n",
      "Epoch 4821/30000 Training Loss: 0.0657205581665039\n",
      "Epoch 4822/30000 Training Loss: 0.04512787610292435\n",
      "Epoch 4823/30000 Training Loss: 0.051494915038347244\n",
      "Epoch 4824/30000 Training Loss: 0.05788888409733772\n",
      "Epoch 4825/30000 Training Loss: 0.05077372118830681\n",
      "Epoch 4826/30000 Training Loss: 0.06027504801750183\n",
      "Epoch 4827/30000 Training Loss: 0.05031387880444527\n",
      "Epoch 4828/30000 Training Loss: 0.055705439299345016\n",
      "Epoch 4829/30000 Training Loss: 0.05978903919458389\n",
      "Epoch 4830/30000 Training Loss: 0.053498804569244385\n",
      "Epoch 4831/30000 Training Loss: 0.06097840145230293\n",
      "Epoch 4832/30000 Training Loss: 0.05329021066427231\n",
      "Epoch 4833/30000 Training Loss: 0.050524789839982986\n",
      "Epoch 4834/30000 Training Loss: 0.04975377768278122\n",
      "Epoch 4835/30000 Training Loss: 0.0635368600487709\n",
      "Epoch 4836/30000 Training Loss: 0.05028235912322998\n",
      "Epoch 4837/30000 Training Loss: 0.05489228293299675\n",
      "Epoch 4838/30000 Training Loss: 0.059519171714782715\n",
      "Epoch 4839/30000 Training Loss: 0.04718175530433655\n",
      "Epoch 4840/30000 Training Loss: 0.058584995567798615\n",
      "Epoch 4841/30000 Training Loss: 0.05768030136823654\n",
      "Epoch 4842/30000 Training Loss: 0.06067706272006035\n",
      "Epoch 4843/30000 Training Loss: 0.05318828672170639\n",
      "Epoch 4844/30000 Training Loss: 0.05055176466703415\n",
      "Epoch 4845/30000 Training Loss: 0.05708710104227066\n",
      "Epoch 4846/30000 Training Loss: 0.052352629601955414\n",
      "Epoch 4847/30000 Training Loss: 0.05431322380900383\n",
      "Epoch 4848/30000 Training Loss: 0.05914033576846123\n",
      "Epoch 4849/30000 Training Loss: 0.0486895926296711\n",
      "Epoch 4850/30000 Training Loss: 0.051158227026462555\n",
      "Epoch 4850/30000 Validation Loss: 0.055688004940748215\n",
      "Epoch 4851/30000 Training Loss: 0.05264399200677872\n",
      "Epoch 4852/30000 Training Loss: 0.0614168718457222\n",
      "Epoch 4853/30000 Training Loss: 0.056096065789461136\n",
      "Epoch 4854/30000 Training Loss: 0.04666993021965027\n",
      "Epoch 4855/30000 Training Loss: 0.047367729246616364\n",
      "Epoch 4856/30000 Training Loss: 0.05238111689686775\n",
      "Epoch 4857/30000 Training Loss: 0.05479611083865166\n",
      "Epoch 4858/30000 Training Loss: 0.05762684345245361\n",
      "Epoch 4859/30000 Training Loss: 0.05781115964055061\n",
      "Epoch 4860/30000 Training Loss: 0.05092928931117058\n",
      "Epoch 4861/30000 Training Loss: 0.049629129469394684\n",
      "Epoch 4862/30000 Training Loss: 0.048468634486198425\n",
      "Epoch 4863/30000 Training Loss: 0.0568438284099102\n",
      "Epoch 4864/30000 Training Loss: 0.07773465663194656\n",
      "Epoch 4865/30000 Training Loss: 0.0538456067442894\n",
      "Epoch 4866/30000 Training Loss: 0.053911905735731125\n",
      "Epoch 4867/30000 Training Loss: 0.05942564085125923\n",
      "Epoch 4868/30000 Training Loss: 0.0519472174346447\n",
      "Epoch 4869/30000 Training Loss: 0.056135691702365875\n",
      "Epoch 4870/30000 Training Loss: 0.051624052226543427\n",
      "Epoch 4871/30000 Training Loss: 0.057864367961883545\n",
      "Epoch 4872/30000 Training Loss: 0.06258503347635269\n",
      "Epoch 4873/30000 Training Loss: 0.05015500634908676\n",
      "Epoch 4874/30000 Training Loss: 0.04736506938934326\n",
      "Epoch 4875/30000 Training Loss: 0.049277715384960175\n",
      "Epoch 4876/30000 Training Loss: 0.05655433610081673\n",
      "Epoch 4877/30000 Training Loss: 0.06107180193066597\n",
      "Epoch 4878/30000 Training Loss: 0.05274469777941704\n",
      "Epoch 4879/30000 Training Loss: 0.05706813186407089\n",
      "Epoch 4880/30000 Training Loss: 0.0497400239109993\n",
      "Epoch 4881/30000 Training Loss: 0.05345935747027397\n",
      "Epoch 4882/30000 Training Loss: 0.05449110269546509\n",
      "Epoch 4883/30000 Training Loss: 0.058471936732530594\n",
      "Epoch 4884/30000 Training Loss: 0.04834597557783127\n",
      "Epoch 4885/30000 Training Loss: 0.0554681196808815\n",
      "Epoch 4886/30000 Training Loss: 0.0546085461974144\n",
      "Epoch 4887/30000 Training Loss: 0.04976971074938774\n",
      "Epoch 4888/30000 Training Loss: 0.05735287815332413\n",
      "Epoch 4889/30000 Training Loss: 0.050140220671892166\n",
      "Epoch 4890/30000 Training Loss: 0.05398944765329361\n",
      "Epoch 4891/30000 Training Loss: 0.050878822803497314\n",
      "Epoch 4892/30000 Training Loss: 0.05649453401565552\n",
      "Epoch 4893/30000 Training Loss: 0.0473974235355854\n",
      "Epoch 4894/30000 Training Loss: 0.05131428688764572\n",
      "Epoch 4895/30000 Training Loss: 0.058438174426555634\n",
      "Epoch 4896/30000 Training Loss: 0.060160499066114426\n",
      "Epoch 4897/30000 Training Loss: 0.0510275736451149\n",
      "Epoch 4898/30000 Training Loss: 0.05881068855524063\n",
      "Epoch 4899/30000 Training Loss: 0.04979429394006729\n",
      "Epoch 4900/30000 Training Loss: 0.04883875697851181\n",
      "Epoch 4900/30000 Validation Loss: 0.05779402330517769\n",
      "Epoch 4901/30000 Training Loss: 0.04942760244011879\n",
      "Epoch 4902/30000 Training Loss: 0.055066026747226715\n",
      "Epoch 4903/30000 Training Loss: 0.05405621603131294\n",
      "Epoch 4904/30000 Training Loss: 0.0586075484752655\n",
      "Epoch 4905/30000 Training Loss: 0.05278676748275757\n",
      "Epoch 4906/30000 Training Loss: 0.05645720288157463\n",
      "Epoch 4907/30000 Training Loss: 0.04815640300512314\n",
      "Epoch 4908/30000 Training Loss: 0.053061146289110184\n",
      "Epoch 4909/30000 Training Loss: 0.051015954464673996\n",
      "Epoch 4910/30000 Training Loss: 0.06537117063999176\n",
      "Epoch 4911/30000 Training Loss: 0.04645993560552597\n",
      "Epoch 4912/30000 Training Loss: 0.04827866703271866\n",
      "Epoch 4913/30000 Training Loss: 0.07044877111911774\n",
      "Epoch 4914/30000 Training Loss: 0.05456619709730148\n",
      "Epoch 4915/30000 Training Loss: 0.06759314984083176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4916/30000 Training Loss: 0.053135305643081665\n",
      "Epoch 4917/30000 Training Loss: 0.0583382323384285\n",
      "Epoch 4918/30000 Training Loss: 0.05243520811200142\n",
      "Epoch 4919/30000 Training Loss: 0.050157465040683746\n",
      "Epoch 4920/30000 Training Loss: 0.05100231245160103\n",
      "Epoch 4921/30000 Training Loss: 0.062432773411273956\n",
      "Epoch 4922/30000 Training Loss: 0.04668201506137848\n",
      "Epoch 4923/30000 Training Loss: 0.06555988639593124\n",
      "Epoch 4924/30000 Training Loss: 0.06286218762397766\n",
      "Epoch 4925/30000 Training Loss: 0.05713995546102524\n",
      "Epoch 4926/30000 Training Loss: 0.05372471362352371\n",
      "Epoch 4927/30000 Training Loss: 0.053908295929431915\n",
      "Epoch 4928/30000 Training Loss: 0.05188729241490364\n",
      "Epoch 4929/30000 Training Loss: 0.04985324293375015\n",
      "Epoch 4930/30000 Training Loss: 0.04570125415921211\n",
      "Epoch 4931/30000 Training Loss: 0.05335376411676407\n",
      "Epoch 4932/30000 Training Loss: 0.04217258095741272\n",
      "Epoch 4933/30000 Training Loss: 0.05214821174740791\n",
      "Epoch 4934/30000 Training Loss: 0.059805333614349365\n",
      "Epoch 4935/30000 Training Loss: 0.052586592733860016\n",
      "Epoch 4936/30000 Training Loss: 0.055673353374004364\n",
      "Epoch 4937/30000 Training Loss: 0.05118846148252487\n",
      "Epoch 4938/30000 Training Loss: 0.054805122315883636\n",
      "Epoch 4939/30000 Training Loss: 0.05791502445936203\n",
      "Epoch 4940/30000 Training Loss: 0.06235828995704651\n",
      "Epoch 4941/30000 Training Loss: 0.05090264603495598\n",
      "Epoch 4942/30000 Training Loss: 0.049178823828697205\n",
      "Epoch 4943/30000 Training Loss: 0.046322040259838104\n",
      "Epoch 4944/30000 Training Loss: 0.04734032228589058\n",
      "Epoch 4945/30000 Training Loss: 0.05119355767965317\n",
      "Epoch 4946/30000 Training Loss: 0.0692095160484314\n",
      "Epoch 4947/30000 Training Loss: 0.05266072601079941\n",
      "Epoch 4948/30000 Training Loss: 0.05865653231739998\n",
      "Epoch 4949/30000 Training Loss: 0.04813240468502045\n",
      "Epoch 4950/30000 Training Loss: 0.0602542981505394\n",
      "Epoch 4950/30000 Validation Loss: 0.059498585760593414\n",
      "Epoch 4951/30000 Training Loss: 0.05212605744600296\n",
      "Epoch 4952/30000 Training Loss: 0.05274590104818344\n",
      "Epoch 4953/30000 Training Loss: 0.058936893939971924\n",
      "Epoch 4954/30000 Training Loss: 0.051844727247953415\n",
      "Epoch 4955/30000 Training Loss: 0.044260747730731964\n",
      "Epoch 4956/30000 Training Loss: 0.05003250762820244\n",
      "Epoch 4957/30000 Training Loss: 0.04803435876965523\n",
      "Epoch 4958/30000 Training Loss: 0.050536952912807465\n",
      "Epoch 4959/30000 Training Loss: 0.04620911553502083\n",
      "Epoch 4960/30000 Training Loss: 0.05496104434132576\n",
      "Epoch 4961/30000 Training Loss: 0.06570594012737274\n",
      "Epoch 4962/30000 Training Loss: 0.05965231731534004\n",
      "Epoch 4963/30000 Training Loss: 0.05579662322998047\n",
      "Epoch 4964/30000 Training Loss: 0.05622159317135811\n",
      "Epoch 4965/30000 Training Loss: 0.04661845788359642\n",
      "Epoch 4966/30000 Training Loss: 0.050291288644075394\n",
      "Epoch 4967/30000 Training Loss: 0.051789961755275726\n",
      "Epoch 4968/30000 Training Loss: 0.053969353437423706\n",
      "Epoch 4969/30000 Training Loss: 0.05182179808616638\n",
      "Epoch 4970/30000 Training Loss: 0.052521515637636185\n",
      "Epoch 4971/30000 Training Loss: 0.04968119412660599\n",
      "Epoch 4972/30000 Training Loss: 0.053146492689847946\n",
      "Epoch 4973/30000 Training Loss: 0.06286048144102097\n",
      "Epoch 4974/30000 Training Loss: 0.06026216596364975\n",
      "Epoch 4975/30000 Training Loss: 0.05511956661939621\n",
      "Epoch 4976/30000 Training Loss: 0.05575188249349594\n",
      "Epoch 4977/30000 Training Loss: 0.06027480959892273\n",
      "Epoch 4978/30000 Training Loss: 0.06407135725021362\n",
      "Epoch 4979/30000 Training Loss: 0.05821406841278076\n",
      "Epoch 4980/30000 Training Loss: 0.050609271973371506\n",
      "Epoch 4981/30000 Training Loss: 0.04985309764742851\n",
      "Epoch 4982/30000 Training Loss: 0.04672708362340927\n",
      "Epoch 4983/30000 Training Loss: 0.05178646370768547\n",
      "Epoch 4984/30000 Training Loss: 0.046522803604602814\n",
      "Epoch 4985/30000 Training Loss: 0.06065070629119873\n",
      "Epoch 4986/30000 Training Loss: 0.05679739639163017\n",
      "Epoch 4987/30000 Training Loss: 0.05353362113237381\n",
      "Epoch 4988/30000 Training Loss: 0.05171249061822891\n",
      "Epoch 4989/30000 Training Loss: 0.05486566945910454\n",
      "Epoch 4990/30000 Training Loss: 0.048902519047260284\n",
      "Epoch 4991/30000 Training Loss: 0.05499614402651787\n",
      "Epoch 4992/30000 Training Loss: 0.05907873064279556\n",
      "Epoch 4993/30000 Training Loss: 0.05500496178865433\n",
      "Epoch 4994/30000 Training Loss: 0.042142968624830246\n",
      "Epoch 4995/30000 Training Loss: 0.050574369728565216\n",
      "Epoch 4996/30000 Training Loss: 0.0466085821390152\n",
      "Epoch 4997/30000 Training Loss: 0.05202922224998474\n",
      "Epoch 4998/30000 Training Loss: 0.06043359637260437\n",
      "Epoch 4999/30000 Training Loss: 0.05607802793383598\n",
      "Epoch 5000/30000 Training Loss: 0.0468803308904171\n",
      "Epoch 5000/30000 Validation Loss: 0.05128249526023865\n",
      "Epoch 5001/30000 Training Loss: 0.05801260471343994\n",
      "Epoch 5002/30000 Training Loss: 0.05807455629110336\n",
      "Epoch 5003/30000 Training Loss: 0.051458656787872314\n",
      "Epoch 5004/30000 Training Loss: 0.05501227825880051\n",
      "Epoch 5005/30000 Training Loss: 0.06731061637401581\n",
      "Epoch 5006/30000 Training Loss: 0.05736614391207695\n",
      "Epoch 5007/30000 Training Loss: 0.0550847165286541\n",
      "Epoch 5008/30000 Training Loss: 0.056592755019664764\n",
      "Epoch 5009/30000 Training Loss: 0.05968320369720459\n",
      "Epoch 5010/30000 Training Loss: 0.05416880175471306\n",
      "Epoch 5011/30000 Training Loss: 0.050401121377944946\n",
      "Epoch 5012/30000 Training Loss: 0.05096977949142456\n",
      "Epoch 5013/30000 Training Loss: 0.054215241223573685\n",
      "Epoch 5014/30000 Training Loss: 0.05345100164413452\n",
      "Epoch 5015/30000 Training Loss: 0.05541139096021652\n",
      "Epoch 5016/30000 Training Loss: 0.055638156831264496\n",
      "Epoch 5017/30000 Training Loss: 0.054831892251968384\n",
      "Epoch 5018/30000 Training Loss: 0.05728727579116821\n",
      "Epoch 5019/30000 Training Loss: 0.04583604633808136\n",
      "Epoch 5020/30000 Training Loss: 0.053262848407030106\n",
      "Epoch 5021/30000 Training Loss: 0.06222999095916748\n",
      "Epoch 5022/30000 Training Loss: 0.05364549160003662\n",
      "Epoch 5023/30000 Training Loss: 0.05370459705591202\n",
      "Epoch 5024/30000 Training Loss: 0.0650036484003067\n",
      "Epoch 5025/30000 Training Loss: 0.04763728007674217\n",
      "Epoch 5026/30000 Training Loss: 0.04404551908373833\n",
      "Epoch 5027/30000 Training Loss: 0.06847839057445526\n",
      "Epoch 5028/30000 Training Loss: 0.04879416525363922\n",
      "Epoch 5029/30000 Training Loss: 0.05610707402229309\n",
      "Epoch 5030/30000 Training Loss: 0.05731887370347977\n",
      "Epoch 5031/30000 Training Loss: 0.055586278438568115\n",
      "Epoch 5032/30000 Training Loss: 0.05402723699808121\n",
      "Epoch 5033/30000 Training Loss: 0.05045166611671448\n",
      "Epoch 5034/30000 Training Loss: 0.052354536950588226\n",
      "Epoch 5035/30000 Training Loss: 0.0492032989859581\n",
      "Epoch 5036/30000 Training Loss: 0.05896027758717537\n",
      "Epoch 5037/30000 Training Loss: 0.05524272844195366\n",
      "Epoch 5038/30000 Training Loss: 0.06296331435441971\n",
      "Epoch 5039/30000 Training Loss: 0.054290592670440674\n",
      "Epoch 5040/30000 Training Loss: 0.05124286934733391\n",
      "Epoch 5041/30000 Training Loss: 0.053148962557315826\n",
      "Epoch 5042/30000 Training Loss: 0.05141977220773697\n",
      "Epoch 5043/30000 Training Loss: 0.049451667815446854\n",
      "Epoch 5044/30000 Training Loss: 0.0534505620598793\n",
      "Epoch 5045/30000 Training Loss: 0.05748804286122322\n",
      "Epoch 5046/30000 Training Loss: 0.05820631980895996\n",
      "Epoch 5047/30000 Training Loss: 0.05462826415896416\n",
      "Epoch 5048/30000 Training Loss: 0.049251627177000046\n",
      "Epoch 5049/30000 Training Loss: 0.05735955387353897\n",
      "Epoch 5050/30000 Training Loss: 0.05113868787884712\n",
      "Epoch 5050/30000 Validation Loss: 0.04137188568711281\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04137188568711281<=============\n",
      "Epoch 5051/30000 Training Loss: 0.06702883541584015\n",
      "Epoch 5052/30000 Training Loss: 0.05203469842672348\n",
      "Epoch 5053/30000 Training Loss: 0.06017487123608589\n",
      "Epoch 5054/30000 Training Loss: 0.05145030468702316\n",
      "Epoch 5055/30000 Training Loss: 0.04807371646165848\n",
      "Epoch 5056/30000 Training Loss: 0.04625681787729263\n",
      "Epoch 5057/30000 Training Loss: 0.051687151193618774\n",
      "Epoch 5058/30000 Training Loss: 0.058778148144483566\n",
      "Epoch 5059/30000 Training Loss: 0.07077263295650482\n",
      "Epoch 5060/30000 Training Loss: 0.05012922361493111\n",
      "Epoch 5061/30000 Training Loss: 0.05683743953704834\n",
      "Epoch 5062/30000 Training Loss: 0.04572130739688873\n",
      "Epoch 5063/30000 Training Loss: 0.05409081652760506\n",
      "Epoch 5064/30000 Training Loss: 0.04855392128229141\n",
      "Epoch 5065/30000 Training Loss: 0.055889446288347244\n",
      "Epoch 5066/30000 Training Loss: 0.047459375113248825\n",
      "Epoch 5067/30000 Training Loss: 0.05644502490758896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5068/30000 Training Loss: 0.06594803184270859\n",
      "Epoch 5069/30000 Training Loss: 0.05228259414434433\n",
      "Epoch 5070/30000 Training Loss: 0.05495892092585564\n",
      "Epoch 5071/30000 Training Loss: 0.05378173664212227\n",
      "Epoch 5072/30000 Training Loss: 0.04661133885383606\n",
      "Epoch 5073/30000 Training Loss: 0.05291469767689705\n",
      "Epoch 5074/30000 Training Loss: 0.060924749821424484\n",
      "Epoch 5075/30000 Training Loss: 0.04790715128183365\n",
      "Epoch 5076/30000 Training Loss: 0.05038830637931824\n",
      "Epoch 5077/30000 Training Loss: 0.05345078557729721\n",
      "Epoch 5078/30000 Training Loss: 0.057455968111753464\n",
      "Epoch 5079/30000 Training Loss: 0.054697148501873016\n",
      "Epoch 5080/30000 Training Loss: 0.054594237357378006\n",
      "Epoch 5081/30000 Training Loss: 0.04343592748045921\n",
      "Epoch 5082/30000 Training Loss: 0.05733374506235123\n",
      "Epoch 5083/30000 Training Loss: 0.049619581550359726\n",
      "Epoch 5084/30000 Training Loss: 0.05365021899342537\n",
      "Epoch 5085/30000 Training Loss: 0.0544021800160408\n",
      "Epoch 5086/30000 Training Loss: 0.054064858704805374\n",
      "Epoch 5087/30000 Training Loss: 0.04831598699092865\n",
      "Epoch 5088/30000 Training Loss: 0.0531587190926075\n",
      "Epoch 5089/30000 Training Loss: 0.04577912762761116\n",
      "Epoch 5090/30000 Training Loss: 0.061955779790878296\n",
      "Epoch 5091/30000 Training Loss: 0.061626166105270386\n",
      "Epoch 5092/30000 Training Loss: 0.05535690858960152\n",
      "Epoch 5093/30000 Training Loss: 0.05526101589202881\n",
      "Epoch 5094/30000 Training Loss: 0.05185969918966293\n",
      "Epoch 5095/30000 Training Loss: 0.056450217962265015\n",
      "Epoch 5096/30000 Training Loss: 0.049234699457883835\n",
      "Epoch 5097/30000 Training Loss: 0.05488535016775131\n",
      "Epoch 5098/30000 Training Loss: 0.050491929054260254\n",
      "Epoch 5099/30000 Training Loss: 0.04853004962205887\n",
      "Epoch 5100/30000 Training Loss: 0.060071755200624466\n",
      "Epoch 5100/30000 Validation Loss: 0.06357866525650024\n",
      "Epoch 5101/30000 Training Loss: 0.05934673547744751\n",
      "Epoch 5102/30000 Training Loss: 0.050398677587509155\n",
      "Epoch 5103/30000 Training Loss: 0.05391347408294678\n",
      "Epoch 5104/30000 Training Loss: 0.046401359140872955\n",
      "Epoch 5105/30000 Training Loss: 0.0567021481692791\n",
      "Epoch 5106/30000 Training Loss: 0.05365049093961716\n",
      "Epoch 5107/30000 Training Loss: 0.05815616995096207\n",
      "Epoch 5108/30000 Training Loss: 0.05182049423456192\n",
      "Epoch 5109/30000 Training Loss: 0.05020145699381828\n",
      "Epoch 5110/30000 Training Loss: 0.05002548173069954\n",
      "Epoch 5111/30000 Training Loss: 0.049767233431339264\n",
      "Epoch 5112/30000 Training Loss: 0.04867479205131531\n",
      "Epoch 5113/30000 Training Loss: 0.054618895053863525\n",
      "Epoch 5114/30000 Training Loss: 0.053664982318878174\n",
      "Epoch 5115/30000 Training Loss: 0.04706040769815445\n",
      "Epoch 5116/30000 Training Loss: 0.053482990711927414\n",
      "Epoch 5117/30000 Training Loss: 0.05657036229968071\n",
      "Epoch 5118/30000 Training Loss: 0.05901694297790527\n",
      "Epoch 5119/30000 Training Loss: 0.05302582308650017\n",
      "Epoch 5120/30000 Training Loss: 0.05191769450902939\n",
      "Epoch 5121/30000 Training Loss: 0.05753403156995773\n",
      "Epoch 5122/30000 Training Loss: 0.047374360263347626\n",
      "Epoch 5123/30000 Training Loss: 0.046753253787755966\n",
      "Epoch 5124/30000 Training Loss: 0.05927376076579094\n",
      "Epoch 5125/30000 Training Loss: 0.057618480175733566\n",
      "Epoch 5126/30000 Training Loss: 0.06271573901176453\n",
      "Epoch 5127/30000 Training Loss: 0.05296089127659798\n",
      "Epoch 5128/30000 Training Loss: 0.057518959045410156\n",
      "Epoch 5129/30000 Training Loss: 0.05028035491704941\n",
      "Epoch 5130/30000 Training Loss: 0.05070192739367485\n",
      "Epoch 5131/30000 Training Loss: 0.06246725469827652\n",
      "Epoch 5132/30000 Training Loss: 0.04939354211091995\n",
      "Epoch 5133/30000 Training Loss: 0.05340494588017464\n",
      "Epoch 5134/30000 Training Loss: 0.05512532591819763\n",
      "Epoch 5135/30000 Training Loss: 0.05295276641845703\n",
      "Epoch 5136/30000 Training Loss: 0.04407532513141632\n",
      "Epoch 5137/30000 Training Loss: 0.05861810967326164\n",
      "Epoch 5138/30000 Training Loss: 0.06133153289556503\n",
      "Epoch 5139/30000 Training Loss: 0.062344539910554886\n",
      "Epoch 5140/30000 Training Loss: 0.05420898273587227\n",
      "Epoch 5141/30000 Training Loss: 0.05114711448550224\n",
      "Epoch 5142/30000 Training Loss: 0.05202812701463699\n",
      "Epoch 5143/30000 Training Loss: 0.05157154053449631\n",
      "Epoch 5144/30000 Training Loss: 0.05360665172338486\n",
      "Epoch 5145/30000 Training Loss: 0.05877946689724922\n",
      "Epoch 5146/30000 Training Loss: 0.05587385967373848\n",
      "Epoch 5147/30000 Training Loss: 0.049183543771505356\n",
      "Epoch 5148/30000 Training Loss: 0.05370836332440376\n",
      "Epoch 5149/30000 Training Loss: 0.055309612303972244\n",
      "Epoch 5150/30000 Training Loss: 0.05857192352414131\n",
      "Epoch 5150/30000 Validation Loss: 0.056040406227111816\n",
      "Epoch 5151/30000 Training Loss: 0.04995022341609001\n",
      "Epoch 5152/30000 Training Loss: 0.04920453578233719\n",
      "Epoch 5153/30000 Training Loss: 0.04570087045431137\n",
      "Epoch 5154/30000 Training Loss: 0.0495578832924366\n",
      "Epoch 5155/30000 Training Loss: 0.052351634949445724\n",
      "Epoch 5156/30000 Training Loss: 0.05712832883000374\n",
      "Epoch 5157/30000 Training Loss: 0.05549710988998413\n",
      "Epoch 5158/30000 Training Loss: 0.05065103620290756\n",
      "Epoch 5159/30000 Training Loss: 0.04873313754796982\n",
      "Epoch 5160/30000 Training Loss: 0.059290118515491486\n",
      "Epoch 5161/30000 Training Loss: 0.05652759224176407\n",
      "Epoch 5162/30000 Training Loss: 0.047664083540439606\n",
      "Epoch 5163/30000 Training Loss: 0.05993901938199997\n",
      "Epoch 5164/30000 Training Loss: 0.05693339183926582\n",
      "Epoch 5165/30000 Training Loss: 0.06790558993816376\n",
      "Epoch 5166/30000 Training Loss: 0.05818229168653488\n",
      "Epoch 5167/30000 Training Loss: 0.05633043497800827\n",
      "Epoch 5168/30000 Training Loss: 0.05300595611333847\n",
      "Epoch 5169/30000 Training Loss: 0.054642051458358765\n",
      "Epoch 5170/30000 Training Loss: 0.054547715932130814\n",
      "Epoch 5171/30000 Training Loss: 0.05489242821931839\n",
      "Epoch 5172/30000 Training Loss: 0.05288318917155266\n",
      "Epoch 5173/30000 Training Loss: 0.050882093608379364\n",
      "Epoch 5174/30000 Training Loss: 0.05854213982820511\n",
      "Epoch 5175/30000 Training Loss: 0.04801080375909805\n",
      "Epoch 5176/30000 Training Loss: 0.055404357612133026\n",
      "Epoch 5177/30000 Training Loss: 0.05816211178898811\n",
      "Epoch 5178/30000 Training Loss: 0.05088915675878525\n",
      "Epoch 5179/30000 Training Loss: 0.04291022941470146\n",
      "Epoch 5180/30000 Training Loss: 0.049549032002687454\n",
      "Epoch 5181/30000 Training Loss: 0.053281597793102264\n",
      "Epoch 5182/30000 Training Loss: 0.05348031967878342\n",
      "Epoch 5183/30000 Training Loss: 0.052227139472961426\n",
      "Epoch 5184/30000 Training Loss: 0.056583404541015625\n",
      "Epoch 5185/30000 Training Loss: 0.052796851843595505\n",
      "Epoch 5186/30000 Training Loss: 0.050442226231098175\n",
      "Epoch 5187/30000 Training Loss: 0.04935283958911896\n",
      "Epoch 5188/30000 Training Loss: 0.06418760120868683\n",
      "Epoch 5189/30000 Training Loss: 0.05833953619003296\n",
      "Epoch 5190/30000 Training Loss: 0.05235467106103897\n",
      "Epoch 5191/30000 Training Loss: 0.051546692848205566\n",
      "Epoch 5192/30000 Training Loss: 0.05557212978601456\n",
      "Epoch 5193/30000 Training Loss: 0.05547827482223511\n",
      "Epoch 5194/30000 Training Loss: 0.05824214220046997\n",
      "Epoch 5195/30000 Training Loss: 0.05473305657505989\n",
      "Epoch 5196/30000 Training Loss: 0.055111318826675415\n",
      "Epoch 5197/30000 Training Loss: 0.05977988243103027\n",
      "Epoch 5198/30000 Training Loss: 0.055564552545547485\n",
      "Epoch 5199/30000 Training Loss: 0.057319074869155884\n",
      "Epoch 5200/30000 Training Loss: 0.04802628606557846\n",
      "Epoch 5200/30000 Validation Loss: 0.0484498031437397\n",
      "Epoch 5201/30000 Training Loss: 0.05855526775121689\n",
      "Epoch 5202/30000 Training Loss: 0.06021670252084732\n",
      "Epoch 5203/30000 Training Loss: 0.05154125764966011\n",
      "Epoch 5204/30000 Training Loss: 0.059219978749752045\n",
      "Epoch 5205/30000 Training Loss: 0.05569985508918762\n",
      "Epoch 5206/30000 Training Loss: 0.05000501871109009\n",
      "Epoch 5207/30000 Training Loss: 0.05412595346570015\n",
      "Epoch 5208/30000 Training Loss: 0.05103272944688797\n",
      "Epoch 5209/30000 Training Loss: 0.058980755507946014\n",
      "Epoch 5210/30000 Training Loss: 0.050732046365737915\n",
      "Epoch 5211/30000 Training Loss: 0.048227883875370026\n",
      "Epoch 5212/30000 Training Loss: 0.04822370409965515\n",
      "Epoch 5213/30000 Training Loss: 0.05468321591615677\n",
      "Epoch 5214/30000 Training Loss: 0.049545567482709885\n",
      "Epoch 5215/30000 Training Loss: 0.05471234768629074\n",
      "Epoch 5216/30000 Training Loss: 0.05684288591146469\n",
      "Epoch 5217/30000 Training Loss: 0.06158328056335449\n",
      "Epoch 5218/30000 Training Loss: 0.05108748748898506\n",
      "Epoch 5219/30000 Training Loss: 0.04887308552861214\n",
      "Epoch 5220/30000 Training Loss: 0.05157526582479477\n",
      "Epoch 5221/30000 Training Loss: 0.049676913768053055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5222/30000 Training Loss: 0.055529870092868805\n",
      "Epoch 5223/30000 Training Loss: 0.06289151310920715\n",
      "Epoch 5224/30000 Training Loss: 0.05700149014592171\n",
      "Epoch 5225/30000 Training Loss: 0.051693640649318695\n",
      "Epoch 5226/30000 Training Loss: 0.05641288682818413\n",
      "Epoch 5227/30000 Training Loss: 0.05835654214024544\n",
      "Epoch 5228/30000 Training Loss: 0.05050034075975418\n",
      "Epoch 5229/30000 Training Loss: 0.05732419341802597\n",
      "Epoch 5230/30000 Training Loss: 0.05596740171313286\n",
      "Epoch 5231/30000 Training Loss: 0.0490541085600853\n",
      "Epoch 5232/30000 Training Loss: 0.05613725632429123\n",
      "Epoch 5233/30000 Training Loss: 0.05175982043147087\n",
      "Epoch 5234/30000 Training Loss: 0.05373639985918999\n",
      "Epoch 5235/30000 Training Loss: 0.05685872957110405\n",
      "Epoch 5236/30000 Training Loss: 0.05775613710284233\n",
      "Epoch 5237/30000 Training Loss: 0.05676112696528435\n",
      "Epoch 5238/30000 Training Loss: 0.048405446112155914\n",
      "Epoch 5239/30000 Training Loss: 0.06244920566678047\n",
      "Epoch 5240/30000 Training Loss: 0.0529816560447216\n",
      "Epoch 5241/30000 Training Loss: 0.060167618095874786\n",
      "Epoch 5242/30000 Training Loss: 0.056441884487867355\n",
      "Epoch 5243/30000 Training Loss: 0.05725107714533806\n",
      "Epoch 5244/30000 Training Loss: 0.05617322400212288\n",
      "Epoch 5245/30000 Training Loss: 0.04728787764906883\n",
      "Epoch 5246/30000 Training Loss: 0.062295544892549515\n",
      "Epoch 5247/30000 Training Loss: 0.05520432069897652\n",
      "Epoch 5248/30000 Training Loss: 0.05414004996418953\n",
      "Epoch 5249/30000 Training Loss: 0.07091303914785385\n",
      "Epoch 5250/30000 Training Loss: 0.04591476172208786\n",
      "Epoch 5250/30000 Validation Loss: 0.05591429024934769\n",
      "Epoch 5251/30000 Training Loss: 0.05551556870341301\n",
      "Epoch 5252/30000 Training Loss: 0.04820229485630989\n",
      "Epoch 5253/30000 Training Loss: 0.05977731943130493\n",
      "Epoch 5254/30000 Training Loss: 0.05703932046890259\n",
      "Epoch 5255/30000 Training Loss: 0.058272670954465866\n",
      "Epoch 5256/30000 Training Loss: 0.04720725491642952\n",
      "Epoch 5257/30000 Training Loss: 0.0503135621547699\n",
      "Epoch 5258/30000 Training Loss: 0.05715147778391838\n",
      "Epoch 5259/30000 Training Loss: 0.047132931649684906\n",
      "Epoch 5260/30000 Training Loss: 0.044539179652929306\n",
      "Epoch 5261/30000 Training Loss: 0.05282946676015854\n",
      "Epoch 5262/30000 Training Loss: 0.05704518035054207\n",
      "Epoch 5263/30000 Training Loss: 0.049377404153347015\n",
      "Epoch 5264/30000 Training Loss: 0.05060575529932976\n",
      "Epoch 5265/30000 Training Loss: 0.05107308179140091\n",
      "Epoch 5266/30000 Training Loss: 0.05064349249005318\n",
      "Epoch 5267/30000 Training Loss: 0.06320687383413315\n",
      "Epoch 5268/30000 Training Loss: 0.047993190586566925\n",
      "Epoch 5269/30000 Training Loss: 0.05934074521064758\n",
      "Epoch 5270/30000 Training Loss: 0.048951636999845505\n",
      "Epoch 5271/30000 Training Loss: 0.05294092744588852\n",
      "Epoch 5272/30000 Training Loss: 0.0543426088988781\n",
      "Epoch 5273/30000 Training Loss: 0.04682144895195961\n",
      "Epoch 5274/30000 Training Loss: 0.058902643620967865\n",
      "Epoch 5275/30000 Training Loss: 0.056591879576444626\n",
      "Epoch 5276/30000 Training Loss: 0.04870421439409256\n",
      "Epoch 5277/30000 Training Loss: 0.04738443344831467\n",
      "Epoch 5278/30000 Training Loss: 0.05376238748431206\n",
      "Epoch 5279/30000 Training Loss: 0.056675106287002563\n",
      "Epoch 5280/30000 Training Loss: 0.04369739443063736\n",
      "Epoch 5281/30000 Training Loss: 0.05241646245121956\n",
      "Epoch 5282/30000 Training Loss: 0.05093590170145035\n",
      "Epoch 5283/30000 Training Loss: 0.06085614114999771\n",
      "Epoch 5284/30000 Training Loss: 0.06446560472249985\n",
      "Epoch 5285/30000 Training Loss: 0.05101928859949112\n",
      "Epoch 5286/30000 Training Loss: 0.05136276036500931\n",
      "Epoch 5287/30000 Training Loss: 0.048009783029556274\n",
      "Epoch 5288/30000 Training Loss: 0.05323416739702225\n",
      "Epoch 5289/30000 Training Loss: 0.05429452657699585\n",
      "Epoch 5290/30000 Training Loss: 0.05500608682632446\n",
      "Epoch 5291/30000 Training Loss: 0.044676773250103\n",
      "Epoch 5292/30000 Training Loss: 0.052714813500642776\n",
      "Epoch 5293/30000 Training Loss: 0.052808184176683426\n",
      "Epoch 5294/30000 Training Loss: 0.04666821286082268\n",
      "Epoch 5295/30000 Training Loss: 0.047728072851896286\n",
      "Epoch 5296/30000 Training Loss: 0.06874565035104752\n",
      "Epoch 5297/30000 Training Loss: 0.06287629902362823\n",
      "Epoch 5298/30000 Training Loss: 0.05162946507334709\n",
      "Epoch 5299/30000 Training Loss: 0.053847890347242355\n",
      "Epoch 5300/30000 Training Loss: 0.05141589045524597\n",
      "Epoch 5300/30000 Validation Loss: 0.05609232932329178\n",
      "Epoch 5301/30000 Training Loss: 0.055428676307201385\n",
      "Epoch 5302/30000 Training Loss: 0.051313601434230804\n",
      "Epoch 5303/30000 Training Loss: 0.05743737146258354\n",
      "Epoch 5304/30000 Training Loss: 0.0568266324698925\n",
      "Epoch 5305/30000 Training Loss: 0.05196318030357361\n",
      "Epoch 5306/30000 Training Loss: 0.05096837133169174\n",
      "Epoch 5307/30000 Training Loss: 0.0612453930079937\n",
      "Epoch 5308/30000 Training Loss: 0.05501558631658554\n",
      "Epoch 5309/30000 Training Loss: 0.04939410462975502\n",
      "Epoch 5310/30000 Training Loss: 0.05071453005075455\n",
      "Epoch 5311/30000 Training Loss: 0.0551658496260643\n",
      "Epoch 5312/30000 Training Loss: 0.05528069660067558\n",
      "Epoch 5313/30000 Training Loss: 0.05406538397073746\n",
      "Epoch 5314/30000 Training Loss: 0.053786247968673706\n",
      "Epoch 5315/30000 Training Loss: 0.048478346318006516\n",
      "Epoch 5316/30000 Training Loss: 0.04929862171411514\n",
      "Epoch 5317/30000 Training Loss: 0.06140414625406265\n",
      "Epoch 5318/30000 Training Loss: 0.05554794520139694\n",
      "Epoch 5319/30000 Training Loss: 0.04849974438548088\n",
      "Epoch 5320/30000 Training Loss: 0.05249541997909546\n",
      "Epoch 5321/30000 Training Loss: 0.053453050553798676\n",
      "Epoch 5322/30000 Training Loss: 0.04932627081871033\n",
      "Epoch 5323/30000 Training Loss: 0.05452127382159233\n",
      "Epoch 5324/30000 Training Loss: 0.06521649658679962\n",
      "Epoch 5325/30000 Training Loss: 0.051616691052913666\n",
      "Epoch 5326/30000 Training Loss: 0.05461964011192322\n",
      "Epoch 5327/30000 Training Loss: 0.05740850418806076\n",
      "Epoch 5328/30000 Training Loss: 0.04900133237242699\n",
      "Epoch 5329/30000 Training Loss: 0.054142504930496216\n",
      "Epoch 5330/30000 Training Loss: 0.05020111799240112\n",
      "Epoch 5331/30000 Training Loss: 0.05555431917309761\n",
      "Epoch 5332/30000 Training Loss: 0.05461304262280464\n",
      "Epoch 5333/30000 Training Loss: 0.05227825790643692\n",
      "Epoch 5334/30000 Training Loss: 0.05348242446780205\n",
      "Epoch 5335/30000 Training Loss: 0.05263965204358101\n",
      "Epoch 5336/30000 Training Loss: 0.054101359099149704\n",
      "Epoch 5337/30000 Training Loss: 0.0523940846323967\n",
      "Epoch 5338/30000 Training Loss: 0.05554335564374924\n",
      "Epoch 5339/30000 Training Loss: 0.048672039061784744\n",
      "Epoch 5340/30000 Training Loss: 0.04948175698518753\n",
      "Epoch 5341/30000 Training Loss: 0.04450026899576187\n",
      "Epoch 5342/30000 Training Loss: 0.047674596309661865\n",
      "Epoch 5343/30000 Training Loss: 0.04866240173578262\n",
      "Epoch 5344/30000 Training Loss: 0.04715874791145325\n",
      "Epoch 5345/30000 Training Loss: 0.05057946592569351\n",
      "Epoch 5346/30000 Training Loss: 0.06402292102575302\n",
      "Epoch 5347/30000 Training Loss: 0.05204598978161812\n",
      "Epoch 5348/30000 Training Loss: 0.05695167928934097\n",
      "Epoch 5349/30000 Training Loss: 0.05114845186471939\n",
      "Epoch 5350/30000 Training Loss: 0.055877573788166046\n",
      "Epoch 5350/30000 Validation Loss: 0.054701197892427444\n",
      "Epoch 5351/30000 Training Loss: 0.06628219783306122\n",
      "Epoch 5352/30000 Training Loss: 0.05950875207781792\n",
      "Epoch 5353/30000 Training Loss: 0.05425571650266647\n",
      "Epoch 5354/30000 Training Loss: 0.05200326442718506\n",
      "Epoch 5355/30000 Training Loss: 0.04458557814359665\n",
      "Epoch 5356/30000 Training Loss: 0.049231261014938354\n",
      "Epoch 5357/30000 Training Loss: 0.048348963260650635\n",
      "Epoch 5358/30000 Training Loss: 0.05113372951745987\n",
      "Epoch 5359/30000 Training Loss: 0.056711018085479736\n",
      "Epoch 5360/30000 Training Loss: 0.05329493433237076\n",
      "Epoch 5361/30000 Training Loss: 0.05571049451828003\n",
      "Epoch 5362/30000 Training Loss: 0.046932827681303024\n",
      "Epoch 5363/30000 Training Loss: 0.05288069695234299\n",
      "Epoch 5364/30000 Training Loss: 0.04711252450942993\n",
      "Epoch 5365/30000 Training Loss: 0.05544840544462204\n",
      "Epoch 5366/30000 Training Loss: 0.045598745346069336\n",
      "Epoch 5367/30000 Training Loss: 0.06137385964393616\n",
      "Epoch 5368/30000 Training Loss: 0.061689455062150955\n",
      "Epoch 5369/30000 Training Loss: 0.051100872457027435\n",
      "Epoch 5370/30000 Training Loss: 0.05128571391105652\n",
      "Epoch 5371/30000 Training Loss: 0.055580031126737595\n",
      "Epoch 5372/30000 Training Loss: 0.050139617174863815\n",
      "Epoch 5373/30000 Training Loss: 0.04956492409110069\n",
      "Epoch 5374/30000 Training Loss: 0.04928131029009819\n",
      "Epoch 5375/30000 Training Loss: 0.06042978912591934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5376/30000 Training Loss: 0.053277116268873215\n",
      "Epoch 5377/30000 Training Loss: 0.049720678478479385\n",
      "Epoch 5378/30000 Training Loss: 0.06913483142852783\n",
      "Epoch 5379/30000 Training Loss: 0.056425146758556366\n",
      "Epoch 5380/30000 Training Loss: 0.06348474323749542\n",
      "Epoch 5381/30000 Training Loss: 0.04032047837972641\n",
      "Epoch 5382/30000 Training Loss: 0.05847511440515518\n",
      "Epoch 5383/30000 Training Loss: 0.046236418187618256\n",
      "Epoch 5384/30000 Training Loss: 0.052280932664871216\n",
      "Epoch 5385/30000 Training Loss: 0.05280517414212227\n",
      "Epoch 5386/30000 Training Loss: 0.055207718163728714\n",
      "Epoch 5387/30000 Training Loss: 0.05716606229543686\n",
      "Epoch 5388/30000 Training Loss: 0.04718707874417305\n",
      "Epoch 5389/30000 Training Loss: 0.0589420422911644\n",
      "Epoch 5390/30000 Training Loss: 0.057561587542295456\n",
      "Epoch 5391/30000 Training Loss: 0.05699698254466057\n",
      "Epoch 5392/30000 Training Loss: 0.04826534911990166\n",
      "Epoch 5393/30000 Training Loss: 0.058313094079494476\n",
      "Epoch 5394/30000 Training Loss: 0.05198051780462265\n",
      "Epoch 5395/30000 Training Loss: 0.04965590313076973\n",
      "Epoch 5396/30000 Training Loss: 0.04862959310412407\n",
      "Epoch 5397/30000 Training Loss: 0.05534929037094116\n",
      "Epoch 5398/30000 Training Loss: 0.054146576672792435\n",
      "Epoch 5399/30000 Training Loss: 0.051717329770326614\n",
      "Epoch 5400/30000 Training Loss: 0.04349097982048988\n",
      "Epoch 5400/30000 Validation Loss: 0.04435763135552406\n",
      "Epoch 5401/30000 Training Loss: 0.0500224232673645\n",
      "Epoch 5402/30000 Training Loss: 0.06202385947108269\n",
      "Epoch 5403/30000 Training Loss: 0.05171145871281624\n",
      "Epoch 5404/30000 Training Loss: 0.05441894382238388\n",
      "Epoch 5405/30000 Training Loss: 0.0533163920044899\n",
      "Epoch 5406/30000 Training Loss: 0.055964671075344086\n",
      "Epoch 5407/30000 Training Loss: 0.053858138620853424\n",
      "Epoch 5408/30000 Training Loss: 0.06034281849861145\n",
      "Epoch 5409/30000 Training Loss: 0.046898968517780304\n",
      "Epoch 5410/30000 Training Loss: 0.052832942456007004\n",
      "Epoch 5411/30000 Training Loss: 0.058447062969207764\n",
      "Epoch 5412/30000 Training Loss: 0.05172741413116455\n",
      "Epoch 5413/30000 Training Loss: 0.06287401169538498\n",
      "Epoch 5414/30000 Training Loss: 0.04627475142478943\n",
      "Epoch 5415/30000 Training Loss: 0.05997423082590103\n",
      "Epoch 5416/30000 Training Loss: 0.049260713160037994\n",
      "Epoch 5417/30000 Training Loss: 0.05572490766644478\n",
      "Epoch 5418/30000 Training Loss: 0.05713527649641037\n",
      "Epoch 5419/30000 Training Loss: 0.05228347331285477\n",
      "Epoch 5420/30000 Training Loss: 0.050266314297914505\n",
      "Epoch 5421/30000 Training Loss: 0.0554642453789711\n",
      "Epoch 5422/30000 Training Loss: 0.0592394582927227\n",
      "Epoch 5423/30000 Training Loss: 0.0562044195830822\n",
      "Epoch 5424/30000 Training Loss: 0.050266019999980927\n",
      "Epoch 5425/30000 Training Loss: 0.04987029358744621\n",
      "Epoch 5426/30000 Training Loss: 0.056401193141937256\n",
      "Epoch 5427/30000 Training Loss: 0.04818812757730484\n",
      "Epoch 5428/30000 Training Loss: 0.06409059464931488\n",
      "Epoch 5429/30000 Training Loss: 0.05169038847088814\n",
      "Epoch 5430/30000 Training Loss: 0.05732889100909233\n",
      "Epoch 5431/30000 Training Loss: 0.04995273798704147\n",
      "Epoch 5432/30000 Training Loss: 0.061911117285490036\n",
      "Epoch 5433/30000 Training Loss: 0.06494587659835815\n",
      "Epoch 5434/30000 Training Loss: 0.05732753872871399\n",
      "Epoch 5435/30000 Training Loss: 0.05767223984003067\n",
      "Epoch 5436/30000 Training Loss: 0.043626122176647186\n",
      "Epoch 5437/30000 Training Loss: 0.04663863033056259\n",
      "Epoch 5438/30000 Training Loss: 0.05306407809257507\n",
      "Epoch 5439/30000 Training Loss: 0.05201328918337822\n",
      "Epoch 5440/30000 Training Loss: 0.048767805099487305\n",
      "Epoch 5441/30000 Training Loss: 0.04978559538722038\n",
      "Epoch 5442/30000 Training Loss: 0.05727999657392502\n",
      "Epoch 5443/30000 Training Loss: 0.05188053846359253\n",
      "Epoch 5444/30000 Training Loss: 0.049633629620075226\n",
      "Epoch 5445/30000 Training Loss: 0.05660497024655342\n",
      "Epoch 5446/30000 Training Loss: 0.06642859429121017\n",
      "Epoch 5447/30000 Training Loss: 0.045328639447689056\n",
      "Epoch 5448/30000 Training Loss: 0.057624757289886475\n",
      "Epoch 5449/30000 Training Loss: 0.06292920559644699\n",
      "Epoch 5450/30000 Training Loss: 0.045184873044490814\n",
      "Epoch 5450/30000 Validation Loss: 0.052674926817417145\n",
      "Epoch 5451/30000 Training Loss: 0.055658869445323944\n",
      "Epoch 5452/30000 Training Loss: 0.053041450679302216\n",
      "Epoch 5453/30000 Training Loss: 0.048418693244457245\n",
      "Epoch 5454/30000 Training Loss: 0.050619304180145264\n",
      "Epoch 5455/30000 Training Loss: 0.05078696459531784\n",
      "Epoch 5456/30000 Training Loss: 0.044931091368198395\n",
      "Epoch 5457/30000 Training Loss: 0.053818732500076294\n",
      "Epoch 5458/30000 Training Loss: 0.05754140019416809\n",
      "Epoch 5459/30000 Training Loss: 0.05567320063710213\n",
      "Epoch 5460/30000 Training Loss: 0.06119445711374283\n",
      "Epoch 5461/30000 Training Loss: 0.04542107135057449\n",
      "Epoch 5462/30000 Training Loss: 0.06934977322816849\n",
      "Epoch 5463/30000 Training Loss: 0.05213739722967148\n",
      "Epoch 5464/30000 Training Loss: 0.05293021723628044\n",
      "Epoch 5465/30000 Training Loss: 0.05951325222849846\n",
      "Epoch 5466/30000 Training Loss: 0.05461319535970688\n",
      "Epoch 5467/30000 Training Loss: 0.05052103474736214\n",
      "Epoch 5468/30000 Training Loss: 0.05087798833847046\n",
      "Epoch 5469/30000 Training Loss: 0.04827490076422691\n",
      "Epoch 5470/30000 Training Loss: 0.05178401619195938\n",
      "Epoch 5471/30000 Training Loss: 0.06212921813130379\n",
      "Epoch 5472/30000 Training Loss: 0.06008803844451904\n",
      "Epoch 5473/30000 Training Loss: 0.04929988831281662\n",
      "Epoch 5474/30000 Training Loss: 0.06494559347629547\n",
      "Epoch 5475/30000 Training Loss: 0.05696168541908264\n",
      "Epoch 5476/30000 Training Loss: 0.0565534308552742\n",
      "Epoch 5477/30000 Training Loss: 0.04547582566738129\n",
      "Epoch 5478/30000 Training Loss: 0.0503668375313282\n",
      "Epoch 5479/30000 Training Loss: 0.04967130720615387\n",
      "Epoch 5480/30000 Training Loss: 0.04814128205180168\n",
      "Epoch 5481/30000 Training Loss: 0.06038308143615723\n",
      "Epoch 5482/30000 Training Loss: 0.0462275929749012\n",
      "Epoch 5483/30000 Training Loss: 0.044728465378284454\n",
      "Epoch 5484/30000 Training Loss: 0.046707991510629654\n",
      "Epoch 5485/30000 Training Loss: 0.054615892469882965\n",
      "Epoch 5486/30000 Training Loss: 0.052858710289001465\n",
      "Epoch 5487/30000 Training Loss: 0.053253285586833954\n",
      "Epoch 5488/30000 Training Loss: 0.054426826536655426\n",
      "Epoch 5489/30000 Training Loss: 0.04936334490776062\n",
      "Epoch 5490/30000 Training Loss: 0.04606027156114578\n",
      "Epoch 5491/30000 Training Loss: 0.05146225169301033\n",
      "Epoch 5492/30000 Training Loss: 0.05413733050227165\n",
      "Epoch 5493/30000 Training Loss: 0.053761500865221024\n",
      "Epoch 5494/30000 Training Loss: 0.05423671752214432\n",
      "Epoch 5495/30000 Training Loss: 0.06852968037128448\n",
      "Epoch 5496/30000 Training Loss: 0.045206326991319656\n",
      "Epoch 5497/30000 Training Loss: 0.04838263615965843\n",
      "Epoch 5498/30000 Training Loss: 0.05730118229985237\n",
      "Epoch 5499/30000 Training Loss: 0.058748818933963776\n",
      "Epoch 5500/30000 Training Loss: 0.054529059678316116\n",
      "Epoch 5500/30000 Validation Loss: 0.05549370497465134\n",
      "Epoch 5501/30000 Training Loss: 0.05751241371035576\n",
      "Epoch 5502/30000 Training Loss: 0.04839278385043144\n",
      "Epoch 5503/30000 Training Loss: 0.0453636571764946\n",
      "Epoch 5504/30000 Training Loss: 0.06212319806218147\n",
      "Epoch 5505/30000 Training Loss: 0.05491143465042114\n",
      "Epoch 5506/30000 Training Loss: 0.04041285440325737\n",
      "Epoch 5507/30000 Training Loss: 0.049912504851818085\n",
      "Epoch 5508/30000 Training Loss: 0.051268719136714935\n",
      "Epoch 5509/30000 Training Loss: 0.05115707963705063\n",
      "Epoch 5510/30000 Training Loss: 0.07301078736782074\n",
      "Epoch 5511/30000 Training Loss: 0.047817423939704895\n",
      "Epoch 5512/30000 Training Loss: 0.051040809601545334\n",
      "Epoch 5513/30000 Training Loss: 0.04921520873904228\n",
      "Epoch 5514/30000 Training Loss: 0.05301284044981003\n",
      "Epoch 5515/30000 Training Loss: 0.059846799820661545\n",
      "Epoch 5516/30000 Training Loss: 0.048337001353502274\n",
      "Epoch 5517/30000 Training Loss: 0.048471368849277496\n",
      "Epoch 5518/30000 Training Loss: 0.05229116603732109\n",
      "Epoch 5519/30000 Training Loss: 0.05334784835577011\n",
      "Epoch 5520/30000 Training Loss: 0.05865740031003952\n",
      "Epoch 5521/30000 Training Loss: 0.05984603241086006\n",
      "Epoch 5522/30000 Training Loss: 0.053496696054935455\n",
      "Epoch 5523/30000 Training Loss: 0.05588039755821228\n",
      "Epoch 5524/30000 Training Loss: 0.053466785699129105\n",
      "Epoch 5525/30000 Training Loss: 0.06166655570268631\n",
      "Epoch 5526/30000 Training Loss: 0.05910876393318176\n",
      "Epoch 5527/30000 Training Loss: 0.05358787253499031\n",
      "Epoch 5528/30000 Training Loss: 0.049444667994976044\n",
      "Epoch 5529/30000 Training Loss: 0.05664495378732681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5530/30000 Training Loss: 0.05267494171857834\n",
      "Epoch 5531/30000 Training Loss: 0.047146059572696686\n",
      "Epoch 5532/30000 Training Loss: 0.05186951160430908\n",
      "Epoch 5533/30000 Training Loss: 0.05421996861696243\n",
      "Epoch 5534/30000 Training Loss: 0.06364493817090988\n",
      "Epoch 5535/30000 Training Loss: 0.05361030623316765\n",
      "Epoch 5536/30000 Training Loss: 0.04843270033597946\n",
      "Epoch 5537/30000 Training Loss: 0.03969176486134529\n",
      "Epoch 5538/30000 Training Loss: 0.05370790511369705\n",
      "Epoch 5539/30000 Training Loss: 0.059239812195301056\n",
      "Epoch 5540/30000 Training Loss: 0.04967363551259041\n",
      "Epoch 5541/30000 Training Loss: 0.05342261120676994\n",
      "Epoch 5542/30000 Training Loss: 0.051105134189128876\n",
      "Epoch 5543/30000 Training Loss: 0.050307683646678925\n",
      "Epoch 5544/30000 Training Loss: 0.052589524537324905\n",
      "Epoch 5545/30000 Training Loss: 0.04911470040678978\n",
      "Epoch 5546/30000 Training Loss: 0.05318218469619751\n",
      "Epoch 5547/30000 Training Loss: 0.05047977715730667\n",
      "Epoch 5548/30000 Training Loss: 0.05122343823313713\n",
      "Epoch 5549/30000 Training Loss: 0.05737127736210823\n",
      "Epoch 5550/30000 Training Loss: 0.04222550988197327\n",
      "Epoch 5550/30000 Validation Loss: 0.04873846098780632\n",
      "Epoch 5551/30000 Training Loss: 0.045479655265808105\n",
      "Epoch 5552/30000 Training Loss: 0.05568139627575874\n",
      "Epoch 5553/30000 Training Loss: 0.05324429273605347\n",
      "Epoch 5554/30000 Training Loss: 0.053672175854444504\n",
      "Epoch 5555/30000 Training Loss: 0.054621171206235886\n",
      "Epoch 5556/30000 Training Loss: 0.05353087931871414\n",
      "Epoch 5557/30000 Training Loss: 0.05027652531862259\n",
      "Epoch 5558/30000 Training Loss: 0.04847092553973198\n",
      "Epoch 5559/30000 Training Loss: 0.05662296339869499\n",
      "Epoch 5560/30000 Training Loss: 0.049776725471019745\n",
      "Epoch 5561/30000 Training Loss: 0.056022416800260544\n",
      "Epoch 5562/30000 Training Loss: 0.04292144626379013\n",
      "Epoch 5563/30000 Training Loss: 0.045181501656770706\n",
      "Epoch 5564/30000 Training Loss: 0.04929140582680702\n",
      "Epoch 5565/30000 Training Loss: 0.04584047943353653\n",
      "Epoch 5566/30000 Training Loss: 0.042670391499996185\n",
      "Epoch 5567/30000 Training Loss: 0.0510096549987793\n",
      "Epoch 5568/30000 Training Loss: 0.049307115375995636\n",
      "Epoch 5569/30000 Training Loss: 0.04868745058774948\n",
      "Epoch 5570/30000 Training Loss: 0.05112846568226814\n",
      "Epoch 5571/30000 Training Loss: 0.04981160908937454\n",
      "Epoch 5572/30000 Training Loss: 0.057042092084884644\n",
      "Epoch 5573/30000 Training Loss: 0.04573593661189079\n",
      "Epoch 5574/30000 Training Loss: 0.056187599897384644\n",
      "Epoch 5575/30000 Training Loss: 0.04873737692832947\n",
      "Epoch 5576/30000 Training Loss: 0.05715233087539673\n",
      "Epoch 5577/30000 Training Loss: 0.04486997798085213\n",
      "Epoch 5578/30000 Training Loss: 0.0534764900803566\n",
      "Epoch 5579/30000 Training Loss: 0.04922852665185928\n",
      "Epoch 5580/30000 Training Loss: 0.05577588826417923\n",
      "Epoch 5581/30000 Training Loss: 0.061447661370038986\n",
      "Epoch 5582/30000 Training Loss: 0.048802681267261505\n",
      "Epoch 5583/30000 Training Loss: 0.04773850366473198\n",
      "Epoch 5584/30000 Training Loss: 0.05704346299171448\n",
      "Epoch 5585/30000 Training Loss: 0.048570938408374786\n",
      "Epoch 5586/30000 Training Loss: 0.05089731886982918\n",
      "Epoch 5587/30000 Training Loss: 0.05955151468515396\n",
      "Epoch 5588/30000 Training Loss: 0.06638235598802567\n",
      "Epoch 5589/30000 Training Loss: 0.05776321142911911\n",
      "Epoch 5590/30000 Training Loss: 0.053330134600400925\n",
      "Epoch 5591/30000 Training Loss: 0.055789291858673096\n",
      "Epoch 5592/30000 Training Loss: 0.06351645290851593\n",
      "Epoch 5593/30000 Training Loss: 0.04659508541226387\n",
      "Epoch 5594/30000 Training Loss: 0.05597228556871414\n",
      "Epoch 5595/30000 Training Loss: 0.05713678151369095\n",
      "Epoch 5596/30000 Training Loss: 0.052260398864746094\n",
      "Epoch 5597/30000 Training Loss: 0.05291345715522766\n",
      "Epoch 5598/30000 Training Loss: 0.05187859386205673\n",
      "Epoch 5599/30000 Training Loss: 0.05191531032323837\n",
      "Epoch 5600/30000 Training Loss: 0.05146442726254463\n",
      "Epoch 5600/30000 Validation Loss: 0.04578578099608421\n",
      "Epoch 5601/30000 Training Loss: 0.05473049730062485\n",
      "Epoch 5602/30000 Training Loss: 0.061326730996370316\n",
      "Epoch 5603/30000 Training Loss: 0.056255094707012177\n",
      "Epoch 5604/30000 Training Loss: 0.05231296271085739\n",
      "Epoch 5605/30000 Training Loss: 0.04755605384707451\n",
      "Epoch 5606/30000 Training Loss: 0.0567040853202343\n",
      "Epoch 5607/30000 Training Loss: 0.04954097419977188\n",
      "Epoch 5608/30000 Training Loss: 0.05117044597864151\n",
      "Epoch 5609/30000 Training Loss: 0.05595238134264946\n",
      "Epoch 5610/30000 Training Loss: 0.04770122468471527\n",
      "Epoch 5611/30000 Training Loss: 0.05612153932452202\n",
      "Epoch 5612/30000 Training Loss: 0.04611090198159218\n",
      "Epoch 5613/30000 Training Loss: 0.048960279673337936\n",
      "Epoch 5614/30000 Training Loss: 0.050930608063936234\n",
      "Epoch 5615/30000 Training Loss: 0.046614665538072586\n",
      "Epoch 5616/30000 Training Loss: 0.04591728374361992\n",
      "Epoch 5617/30000 Training Loss: 0.04729991406202316\n",
      "Epoch 5618/30000 Training Loss: 0.05380302667617798\n",
      "Epoch 5619/30000 Training Loss: 0.051274608820676804\n",
      "Epoch 5620/30000 Training Loss: 0.05650969594717026\n",
      "Epoch 5621/30000 Training Loss: 0.04760310426354408\n",
      "Epoch 5622/30000 Training Loss: 0.057512544095516205\n",
      "Epoch 5623/30000 Training Loss: 0.048812489956617355\n",
      "Epoch 5624/30000 Training Loss: 0.0489465668797493\n",
      "Epoch 5625/30000 Training Loss: 0.05010644346475601\n",
      "Epoch 5626/30000 Training Loss: 0.0476507805287838\n",
      "Epoch 5627/30000 Training Loss: 0.05034790188074112\n",
      "Epoch 5628/30000 Training Loss: 0.054616570472717285\n",
      "Epoch 5629/30000 Training Loss: 0.05286354944109917\n",
      "Epoch 5630/30000 Training Loss: 0.06297244131565094\n",
      "Epoch 5631/30000 Training Loss: 0.058937542140483856\n",
      "Epoch 5632/30000 Training Loss: 0.0571826808154583\n",
      "Epoch 5633/30000 Training Loss: 0.05041436478495598\n",
      "Epoch 5634/30000 Training Loss: 0.05202586203813553\n",
      "Epoch 5635/30000 Training Loss: 0.05994737893342972\n",
      "Epoch 5636/30000 Training Loss: 0.049411747604608536\n",
      "Epoch 5637/30000 Training Loss: 0.0600287988781929\n",
      "Epoch 5638/30000 Training Loss: 0.04656757414340973\n",
      "Epoch 5639/30000 Training Loss: 0.048126399517059326\n",
      "Epoch 5640/30000 Training Loss: 0.05532193183898926\n",
      "Epoch 5641/30000 Training Loss: 0.049456942826509476\n",
      "Epoch 5642/30000 Training Loss: 0.05791178345680237\n",
      "Epoch 5643/30000 Training Loss: 0.04379945248365402\n",
      "Epoch 5644/30000 Training Loss: 0.0609886534512043\n",
      "Epoch 5645/30000 Training Loss: 0.05236604064702988\n",
      "Epoch 5646/30000 Training Loss: 0.05132461339235306\n",
      "Epoch 5647/30000 Training Loss: 0.053733956068754196\n",
      "Epoch 5648/30000 Training Loss: 0.054709941148757935\n",
      "Epoch 5649/30000 Training Loss: 0.048808928579092026\n",
      "Epoch 5650/30000 Training Loss: 0.052839260548353195\n",
      "Epoch 5650/30000 Validation Loss: 0.04603017121553421\n",
      "Epoch 5651/30000 Training Loss: 0.061146754771471024\n",
      "Epoch 5652/30000 Training Loss: 0.051931679248809814\n",
      "Epoch 5653/30000 Training Loss: 0.059859178960323334\n",
      "Epoch 5654/30000 Training Loss: 0.04665355011820793\n",
      "Epoch 5655/30000 Training Loss: 0.046225614845752716\n",
      "Epoch 5656/30000 Training Loss: 0.04999152570962906\n",
      "Epoch 5657/30000 Training Loss: 0.055962275713682175\n",
      "Epoch 5658/30000 Training Loss: 0.0547729954123497\n",
      "Epoch 5659/30000 Training Loss: 0.05530105158686638\n",
      "Epoch 5660/30000 Training Loss: 0.05268925428390503\n",
      "Epoch 5661/30000 Training Loss: 0.053776539862155914\n",
      "Epoch 5662/30000 Training Loss: 0.05144461244344711\n",
      "Epoch 5663/30000 Training Loss: 0.05451953411102295\n",
      "Epoch 5664/30000 Training Loss: 0.05335848405957222\n",
      "Epoch 5665/30000 Training Loss: 0.04836610332131386\n",
      "Epoch 5666/30000 Training Loss: 0.04847141355276108\n",
      "Epoch 5667/30000 Training Loss: 0.045586567372083664\n",
      "Epoch 5668/30000 Training Loss: 0.0513242706656456\n",
      "Epoch 5669/30000 Training Loss: 0.05132700875401497\n",
      "Epoch 5670/30000 Training Loss: 0.05585441738367081\n",
      "Epoch 5671/30000 Training Loss: 0.058724116533994675\n",
      "Epoch 5672/30000 Training Loss: 0.05088653415441513\n",
      "Epoch 5673/30000 Training Loss: 0.05443825572729111\n",
      "Epoch 5674/30000 Training Loss: 0.06289800256490707\n",
      "Epoch 5675/30000 Training Loss: 0.04247138649225235\n",
      "Epoch 5676/30000 Training Loss: 0.049879010766744614\n",
      "Epoch 5677/30000 Training Loss: 0.04812855273485184\n",
      "Epoch 5678/30000 Training Loss: 0.05944526195526123\n",
      "Epoch 5679/30000 Training Loss: 0.05333906412124634\n",
      "Epoch 5680/30000 Training Loss: 0.050253331661224365\n",
      "Epoch 5681/30000 Training Loss: 0.051311612129211426\n",
      "Epoch 5682/30000 Training Loss: 0.04986153542995453\n",
      "Epoch 5683/30000 Training Loss: 0.05565323308110237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5684/30000 Training Loss: 0.05784226581454277\n",
      "Epoch 5685/30000 Training Loss: 0.05107862502336502\n",
      "Epoch 5686/30000 Training Loss: 0.04591929912567139\n",
      "Epoch 5687/30000 Training Loss: 0.05343785136938095\n",
      "Epoch 5688/30000 Training Loss: 0.05573385953903198\n",
      "Epoch 5689/30000 Training Loss: 0.051274627447128296\n",
      "Epoch 5690/30000 Training Loss: 0.04807524010539055\n",
      "Epoch 5691/30000 Training Loss: 0.05067174509167671\n",
      "Epoch 5692/30000 Training Loss: 0.059689007699489594\n",
      "Epoch 5693/30000 Training Loss: 0.05501938983798027\n",
      "Epoch 5694/30000 Training Loss: 0.04623507708311081\n",
      "Epoch 5695/30000 Training Loss: 0.04716401547193527\n",
      "Epoch 5696/30000 Training Loss: 0.060192834585905075\n",
      "Epoch 5697/30000 Training Loss: 0.05020476132631302\n",
      "Epoch 5698/30000 Training Loss: 0.06091129779815674\n",
      "Epoch 5699/30000 Training Loss: 0.058508068323135376\n",
      "Epoch 5700/30000 Training Loss: 0.05280333757400513\n",
      "Epoch 5700/30000 Validation Loss: 0.051750171929597855\n",
      "Epoch 5701/30000 Training Loss: 0.05053653568029404\n",
      "Epoch 5702/30000 Training Loss: 0.05225875973701477\n",
      "Epoch 5703/30000 Training Loss: 0.046208977699279785\n",
      "Epoch 5704/30000 Training Loss: 0.05189291760325432\n",
      "Epoch 5705/30000 Training Loss: 0.049541689455509186\n",
      "Epoch 5706/30000 Training Loss: 0.047021206468343735\n",
      "Epoch 5707/30000 Training Loss: 0.051178477704524994\n",
      "Epoch 5708/30000 Training Loss: 0.04927579313516617\n",
      "Epoch 5709/30000 Training Loss: 0.061073172837495804\n",
      "Epoch 5710/30000 Training Loss: 0.049267373979091644\n",
      "Epoch 5711/30000 Training Loss: 0.04425659030675888\n",
      "Epoch 5712/30000 Training Loss: 0.05773744732141495\n",
      "Epoch 5713/30000 Training Loss: 0.04741832613945007\n",
      "Epoch 5714/30000 Training Loss: 0.05401375889778137\n",
      "Epoch 5715/30000 Training Loss: 0.05548664927482605\n",
      "Epoch 5716/30000 Training Loss: 0.0530608706176281\n",
      "Epoch 5717/30000 Training Loss: 0.04550693556666374\n",
      "Epoch 5718/30000 Training Loss: 0.054891277104616165\n",
      "Epoch 5719/30000 Training Loss: 0.05401645973324776\n",
      "Epoch 5720/30000 Training Loss: 0.0467165932059288\n",
      "Epoch 5721/30000 Training Loss: 0.05472991615533829\n",
      "Epoch 5722/30000 Training Loss: 0.057317446917295456\n",
      "Epoch 5723/30000 Training Loss: 0.05663403868675232\n",
      "Epoch 5724/30000 Training Loss: 0.06067848205566406\n",
      "Epoch 5725/30000 Training Loss: 0.05217010900378227\n",
      "Epoch 5726/30000 Training Loss: 0.05407319217920303\n",
      "Epoch 5727/30000 Training Loss: 0.05717945843935013\n",
      "Epoch 5728/30000 Training Loss: 0.04694919288158417\n",
      "Epoch 5729/30000 Training Loss: 0.049432989209890366\n",
      "Epoch 5730/30000 Training Loss: 0.059985946863889694\n",
      "Epoch 5731/30000 Training Loss: 0.05417773127555847\n",
      "Epoch 5732/30000 Training Loss: 0.058793842792510986\n",
      "Epoch 5733/30000 Training Loss: 0.051218438893556595\n",
      "Epoch 5734/30000 Training Loss: 0.049935318529605865\n",
      "Epoch 5735/30000 Training Loss: 0.045313380658626556\n",
      "Epoch 5736/30000 Training Loss: 0.05611305311322212\n",
      "Epoch 5737/30000 Training Loss: 0.041614942252635956\n",
      "Epoch 5738/30000 Training Loss: 0.06029302626848221\n",
      "Epoch 5739/30000 Training Loss: 0.04890603572130203\n",
      "Epoch 5740/30000 Training Loss: 0.0474429652094841\n",
      "Epoch 5741/30000 Training Loss: 0.04790857434272766\n",
      "Epoch 5742/30000 Training Loss: 0.05782701447606087\n",
      "Epoch 5743/30000 Training Loss: 0.04793524369597435\n",
      "Epoch 5744/30000 Training Loss: 0.05789366364479065\n",
      "Epoch 5745/30000 Training Loss: 0.05803445726633072\n",
      "Epoch 5746/30000 Training Loss: 0.06469101458787918\n",
      "Epoch 5747/30000 Training Loss: 0.05687587335705757\n",
      "Epoch 5748/30000 Training Loss: 0.04522840306162834\n",
      "Epoch 5749/30000 Training Loss: 0.0501498281955719\n",
      "Epoch 5750/30000 Training Loss: 0.05249043554067612\n",
      "Epoch 5750/30000 Validation Loss: 0.056985341012477875\n",
      "Epoch 5751/30000 Training Loss: 0.04680074378848076\n",
      "Epoch 5752/30000 Training Loss: 0.05445546656847\n",
      "Epoch 5753/30000 Training Loss: 0.050004612654447556\n",
      "Epoch 5754/30000 Training Loss: 0.053062088787555695\n",
      "Epoch 5755/30000 Training Loss: 0.047395702451467514\n",
      "Epoch 5756/30000 Training Loss: 0.059378813952207565\n",
      "Epoch 5757/30000 Training Loss: 0.04205789044499397\n",
      "Epoch 5758/30000 Training Loss: 0.05190682411193848\n",
      "Epoch 5759/30000 Training Loss: 0.04735615476965904\n",
      "Epoch 5760/30000 Training Loss: 0.059479109942913055\n",
      "Epoch 5761/30000 Training Loss: 0.05812408775091171\n",
      "Epoch 5762/30000 Training Loss: 0.04993404448032379\n",
      "Epoch 5763/30000 Training Loss: 0.05281306058168411\n",
      "Epoch 5764/30000 Training Loss: 0.05663645267486572\n",
      "Epoch 5765/30000 Training Loss: 0.0558534674346447\n",
      "Epoch 5766/30000 Training Loss: 0.05346546694636345\n",
      "Epoch 5767/30000 Training Loss: 0.040272306650877\n",
      "Epoch 5768/30000 Training Loss: 0.048984795808792114\n",
      "Epoch 5769/30000 Training Loss: 0.04365008324384689\n",
      "Epoch 5770/30000 Training Loss: 0.05784127861261368\n",
      "Epoch 5771/30000 Training Loss: 0.04588548094034195\n",
      "Epoch 5772/30000 Training Loss: 0.05146148055791855\n",
      "Epoch 5773/30000 Training Loss: 0.06239413097500801\n",
      "Epoch 5774/30000 Training Loss: 0.05846765637397766\n",
      "Epoch 5775/30000 Training Loss: 0.055493492633104324\n",
      "Epoch 5776/30000 Training Loss: 0.04984439164400101\n",
      "Epoch 5777/30000 Training Loss: 0.04892474412918091\n",
      "Epoch 5778/30000 Training Loss: 0.05125418305397034\n",
      "Epoch 5779/30000 Training Loss: 0.052503615617752075\n",
      "Epoch 5780/30000 Training Loss: 0.05176951363682747\n",
      "Epoch 5781/30000 Training Loss: 0.05050318315625191\n",
      "Epoch 5782/30000 Training Loss: 0.056451816111803055\n",
      "Epoch 5783/30000 Training Loss: 0.05505720525979996\n",
      "Epoch 5784/30000 Training Loss: 0.0624481737613678\n",
      "Epoch 5785/30000 Training Loss: 0.0548408217728138\n",
      "Epoch 5786/30000 Training Loss: 0.05015053600072861\n",
      "Epoch 5787/30000 Training Loss: 0.05844525247812271\n",
      "Epoch 5788/30000 Training Loss: 0.043207213282585144\n",
      "Epoch 5789/30000 Training Loss: 0.0608668327331543\n",
      "Epoch 5790/30000 Training Loss: 0.04920375719666481\n",
      "Epoch 5791/30000 Training Loss: 0.052412718534469604\n",
      "Epoch 5792/30000 Training Loss: 0.0590960755944252\n",
      "Epoch 5793/30000 Training Loss: 0.04599020257592201\n",
      "Epoch 5794/30000 Training Loss: 0.05065257102251053\n",
      "Epoch 5795/30000 Training Loss: 0.053589046001434326\n",
      "Epoch 5796/30000 Training Loss: 0.04994184896349907\n",
      "Epoch 5797/30000 Training Loss: 0.05236906558275223\n",
      "Epoch 5798/30000 Training Loss: 0.0498398132622242\n",
      "Epoch 5799/30000 Training Loss: 0.054221492260694504\n",
      "Epoch 5800/30000 Training Loss: 0.05311203747987747\n",
      "Epoch 5800/30000 Validation Loss: 0.04525420814752579\n",
      "Epoch 5801/30000 Training Loss: 0.042231060564517975\n",
      "Epoch 5802/30000 Training Loss: 0.047404177486896515\n",
      "Epoch 5803/30000 Training Loss: 0.054551512002944946\n",
      "Epoch 5804/30000 Training Loss: 0.050984062254428864\n",
      "Epoch 5805/30000 Training Loss: 0.04890687018632889\n",
      "Epoch 5806/30000 Training Loss: 0.04896410554647446\n",
      "Epoch 5807/30000 Training Loss: 0.05327402427792549\n",
      "Epoch 5808/30000 Training Loss: 0.04773406311869621\n",
      "Epoch 5809/30000 Training Loss: 0.053415607661008835\n",
      "Epoch 5810/30000 Training Loss: 0.04382314532995224\n",
      "Epoch 5811/30000 Training Loss: 0.051899075508117676\n",
      "Epoch 5812/30000 Training Loss: 0.04763662815093994\n",
      "Epoch 5813/30000 Training Loss: 0.0494849868118763\n",
      "Epoch 5814/30000 Training Loss: 0.046056438237428665\n",
      "Epoch 5815/30000 Training Loss: 0.04706934466958046\n",
      "Epoch 5816/30000 Training Loss: 0.05745949223637581\n",
      "Epoch 5817/30000 Training Loss: 0.04708025977015495\n",
      "Epoch 5818/30000 Training Loss: 0.06187133118510246\n",
      "Epoch 5819/30000 Training Loss: 0.04445996135473251\n",
      "Epoch 5820/30000 Training Loss: 0.04998180270195007\n",
      "Epoch 5821/30000 Training Loss: 0.04884355142712593\n",
      "Epoch 5822/30000 Training Loss: 0.05714666098356247\n",
      "Epoch 5823/30000 Training Loss: 0.05565959960222244\n",
      "Epoch 5824/30000 Training Loss: 0.0511523112654686\n",
      "Epoch 5825/30000 Training Loss: 0.05439655855298042\n",
      "Epoch 5826/30000 Training Loss: 0.05684648081660271\n",
      "Epoch 5827/30000 Training Loss: 0.05394870787858963\n",
      "Epoch 5828/30000 Training Loss: 0.046729184687137604\n",
      "Epoch 5829/30000 Training Loss: 0.05236608907580376\n",
      "Epoch 5830/30000 Training Loss: 0.04884941875934601\n",
      "Epoch 5831/30000 Training Loss: 0.05082469433546066\n",
      "Epoch 5832/30000 Training Loss: 0.04139355942606926\n",
      "Epoch 5833/30000 Training Loss: 0.04327145218849182\n",
      "Epoch 5834/30000 Training Loss: 0.04511142522096634\n",
      "Epoch 5835/30000 Training Loss: 0.05752783268690109\n",
      "Epoch 5836/30000 Training Loss: 0.0558696985244751\n",
      "Epoch 5837/30000 Training Loss: 0.04242454841732979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5838/30000 Training Loss: 0.046672455966472626\n",
      "Epoch 5839/30000 Training Loss: 0.05095138028264046\n",
      "Epoch 5840/30000 Training Loss: 0.05106840282678604\n",
      "Epoch 5841/30000 Training Loss: 0.05509497970342636\n",
      "Epoch 5842/30000 Training Loss: 0.05043447017669678\n",
      "Epoch 5843/30000 Training Loss: 0.04437536001205444\n",
      "Epoch 5844/30000 Training Loss: 0.04716392606496811\n",
      "Epoch 5845/30000 Training Loss: 0.053954750299453735\n",
      "Epoch 5846/30000 Training Loss: 0.05338511988520622\n",
      "Epoch 5847/30000 Training Loss: 0.04257496818900108\n",
      "Epoch 5848/30000 Training Loss: 0.05303458124399185\n",
      "Epoch 5849/30000 Training Loss: 0.05732838436961174\n",
      "Epoch 5850/30000 Training Loss: 0.05312499403953552\n",
      "Epoch 5850/30000 Validation Loss: 0.04840446636080742\n",
      "Epoch 5851/30000 Training Loss: 0.05146735906600952\n",
      "Epoch 5852/30000 Training Loss: 0.055142082273960114\n",
      "Epoch 5853/30000 Training Loss: 0.05156787484884262\n",
      "Epoch 5854/30000 Training Loss: 0.05060072988271713\n",
      "Epoch 5855/30000 Training Loss: 0.05348530411720276\n",
      "Epoch 5856/30000 Training Loss: 0.057482969015836716\n",
      "Epoch 5857/30000 Training Loss: 0.055542729794979095\n",
      "Epoch 5858/30000 Training Loss: 0.04682154208421707\n",
      "Epoch 5859/30000 Training Loss: 0.046054258942604065\n",
      "Epoch 5860/30000 Training Loss: 0.04912230372428894\n",
      "Epoch 5861/30000 Training Loss: 0.05435371398925781\n",
      "Epoch 5862/30000 Training Loss: 0.06052600219845772\n",
      "Epoch 5863/30000 Training Loss: 0.0503661148250103\n",
      "Epoch 5864/30000 Training Loss: 0.051440782845020294\n",
      "Epoch 5865/30000 Training Loss: 0.05062698572874069\n",
      "Epoch 5866/30000 Training Loss: 0.05127675086259842\n",
      "Epoch 5867/30000 Training Loss: 0.05153897404670715\n",
      "Epoch 5868/30000 Training Loss: 0.0490834042429924\n",
      "Epoch 5869/30000 Training Loss: 0.05634278059005737\n",
      "Epoch 5870/30000 Training Loss: 0.05295810103416443\n",
      "Epoch 5871/30000 Training Loss: 0.06109087914228439\n",
      "Epoch 5872/30000 Training Loss: 0.052680909633636475\n",
      "Epoch 5873/30000 Training Loss: 0.05408646911382675\n",
      "Epoch 5874/30000 Training Loss: 0.050810158252716064\n",
      "Epoch 5875/30000 Training Loss: 0.06672251224517822\n",
      "Epoch 5876/30000 Training Loss: 0.048425547778606415\n",
      "Epoch 5877/30000 Training Loss: 0.05266956612467766\n",
      "Epoch 5878/30000 Training Loss: 0.04985284060239792\n",
      "Epoch 5879/30000 Training Loss: 0.050010718405246735\n",
      "Epoch 5880/30000 Training Loss: 0.04971364513039589\n",
      "Epoch 5881/30000 Training Loss: 0.048503488302230835\n",
      "Epoch 5882/30000 Training Loss: 0.0470457449555397\n",
      "Epoch 5883/30000 Training Loss: 0.05684949830174446\n",
      "Epoch 5884/30000 Training Loss: 0.05528254434466362\n",
      "Epoch 5885/30000 Training Loss: 0.055702537298202515\n",
      "Epoch 5886/30000 Training Loss: 0.05801420658826828\n",
      "Epoch 5887/30000 Training Loss: 0.060150451958179474\n",
      "Epoch 5888/30000 Training Loss: 0.052175331860780716\n",
      "Epoch 5889/30000 Training Loss: 0.047924719750881195\n",
      "Epoch 5890/30000 Training Loss: 0.04376738518476486\n",
      "Epoch 5891/30000 Training Loss: 0.05376730114221573\n",
      "Epoch 5892/30000 Training Loss: 0.05786100775003433\n",
      "Epoch 5893/30000 Training Loss: 0.0539729967713356\n",
      "Epoch 5894/30000 Training Loss: 0.05696827173233032\n",
      "Epoch 5895/30000 Training Loss: 0.043540265411138535\n",
      "Epoch 5896/30000 Training Loss: 0.056647323071956635\n",
      "Epoch 5897/30000 Training Loss: 0.05305856466293335\n",
      "Epoch 5898/30000 Training Loss: 0.04966253787279129\n",
      "Epoch 5899/30000 Training Loss: 0.05305482819676399\n",
      "Epoch 5900/30000 Training Loss: 0.04496358707547188\n",
      "Epoch 5900/30000 Validation Loss: 0.049801576882600784\n",
      "Epoch 5901/30000 Training Loss: 0.05416348576545715\n",
      "Epoch 5902/30000 Training Loss: 0.0409688726067543\n",
      "Epoch 5903/30000 Training Loss: 0.05555357411503792\n",
      "Epoch 5904/30000 Training Loss: 0.05436418578028679\n",
      "Epoch 5905/30000 Training Loss: 0.05164549499750137\n",
      "Epoch 5906/30000 Training Loss: 0.05337843298912048\n",
      "Epoch 5907/30000 Training Loss: 0.05039329454302788\n",
      "Epoch 5908/30000 Training Loss: 0.048129402101039886\n",
      "Epoch 5909/30000 Training Loss: 0.06368078291416168\n",
      "Epoch 5910/30000 Training Loss: 0.05120920017361641\n",
      "Epoch 5911/30000 Training Loss: 0.04833076894283295\n",
      "Epoch 5912/30000 Training Loss: 0.05147359520196915\n",
      "Epoch 5913/30000 Training Loss: 0.0532233826816082\n",
      "Epoch 5914/30000 Training Loss: 0.04589094966650009\n",
      "Epoch 5915/30000 Training Loss: 0.05269552394747734\n",
      "Epoch 5916/30000 Training Loss: 0.06546086817979813\n",
      "Epoch 5917/30000 Training Loss: 0.05219271779060364\n",
      "Epoch 5918/30000 Training Loss: 0.04815281927585602\n",
      "Epoch 5919/30000 Training Loss: 0.050611503422260284\n",
      "Epoch 5920/30000 Training Loss: 0.0448591485619545\n",
      "Epoch 5921/30000 Training Loss: 0.0507064089179039\n",
      "Epoch 5922/30000 Training Loss: 0.05233458802103996\n",
      "Epoch 5923/30000 Training Loss: 0.05592082813382149\n",
      "Epoch 5924/30000 Training Loss: 0.05650465562939644\n",
      "Epoch 5925/30000 Training Loss: 0.05040585249662399\n",
      "Epoch 5926/30000 Training Loss: 0.05818238854408264\n",
      "Epoch 5927/30000 Training Loss: 0.04857512563467026\n",
      "Epoch 5928/30000 Training Loss: 0.05271909758448601\n",
      "Epoch 5929/30000 Training Loss: 0.05112675949931145\n",
      "Epoch 5930/30000 Training Loss: 0.053228892385959625\n",
      "Epoch 5931/30000 Training Loss: 0.050933338701725006\n",
      "Epoch 5932/30000 Training Loss: 0.05266376584768295\n",
      "Epoch 5933/30000 Training Loss: 0.06067519262433052\n",
      "Epoch 5934/30000 Training Loss: 0.0537419430911541\n",
      "Epoch 5935/30000 Training Loss: 0.05630568787455559\n",
      "Epoch 5936/30000 Training Loss: 0.04806637391448021\n",
      "Epoch 5937/30000 Training Loss: 0.04521118849515915\n",
      "Epoch 5938/30000 Training Loss: 0.04943705350160599\n",
      "Epoch 5939/30000 Training Loss: 0.052140556275844574\n",
      "Epoch 5940/30000 Training Loss: 0.0474797785282135\n",
      "Epoch 5941/30000 Training Loss: 0.05855454131960869\n",
      "Epoch 5942/30000 Training Loss: 0.046339597553014755\n",
      "Epoch 5943/30000 Training Loss: 0.053130101412534714\n",
      "Epoch 5944/30000 Training Loss: 0.060542184859514236\n",
      "Epoch 5945/30000 Training Loss: 0.04927302151918411\n",
      "Epoch 5946/30000 Training Loss: 0.048434801399707794\n",
      "Epoch 5947/30000 Training Loss: 0.0458189994096756\n",
      "Epoch 5948/30000 Training Loss: 0.054822761565446854\n",
      "Epoch 5949/30000 Training Loss: 0.06091383844614029\n",
      "Epoch 5950/30000 Training Loss: 0.04879501461982727\n",
      "Epoch 5950/30000 Validation Loss: 0.049257371574640274\n",
      "Epoch 5951/30000 Training Loss: 0.05818530172109604\n",
      "Epoch 5952/30000 Training Loss: 0.0496610589325428\n",
      "Epoch 5953/30000 Training Loss: 0.05903484299778938\n",
      "Epoch 5954/30000 Training Loss: 0.044710490852594376\n",
      "Epoch 5955/30000 Training Loss: 0.04478251188993454\n",
      "Epoch 5956/30000 Training Loss: 0.05719207972288132\n",
      "Epoch 5957/30000 Training Loss: 0.05004795268177986\n",
      "Epoch 5958/30000 Training Loss: 0.05356448143720627\n",
      "Epoch 5959/30000 Training Loss: 0.043178584426641464\n",
      "Epoch 5960/30000 Training Loss: 0.05347440391778946\n",
      "Epoch 5961/30000 Training Loss: 0.04794953763484955\n",
      "Epoch 5962/30000 Training Loss: 0.05548306182026863\n",
      "Epoch 5963/30000 Training Loss: 0.05363739654421806\n",
      "Epoch 5964/30000 Training Loss: 0.05519253760576248\n",
      "Epoch 5965/30000 Training Loss: 0.049783919006586075\n",
      "Epoch 5966/30000 Training Loss: 0.0516340546309948\n",
      "Epoch 5967/30000 Training Loss: 0.04512875899672508\n",
      "Epoch 5968/30000 Training Loss: 0.06033146381378174\n",
      "Epoch 5969/30000 Training Loss: 0.045937735587358475\n",
      "Epoch 5970/30000 Training Loss: 0.0616193525493145\n",
      "Epoch 5971/30000 Training Loss: 0.05290321260690689\n",
      "Epoch 5972/30000 Training Loss: 0.052819471806287766\n",
      "Epoch 5973/30000 Training Loss: 0.0472601056098938\n",
      "Epoch 5974/30000 Training Loss: 0.04752882942557335\n",
      "Epoch 5975/30000 Training Loss: 0.051649849861860275\n",
      "Epoch 5976/30000 Training Loss: 0.046379346400499344\n",
      "Epoch 5977/30000 Training Loss: 0.05643649026751518\n",
      "Epoch 5978/30000 Training Loss: 0.05557303503155708\n",
      "Epoch 5979/30000 Training Loss: 0.05303999036550522\n",
      "Epoch 5980/30000 Training Loss: 0.0494195781648159\n",
      "Epoch 5981/30000 Training Loss: 0.056133873760700226\n",
      "Epoch 5982/30000 Training Loss: 0.06070142239332199\n",
      "Epoch 5983/30000 Training Loss: 0.0451897569000721\n",
      "Epoch 5984/30000 Training Loss: 0.048322100192308426\n",
      "Epoch 5985/30000 Training Loss: 0.051757074892520905\n",
      "Epoch 5986/30000 Training Loss: 0.05036517232656479\n",
      "Epoch 5987/30000 Training Loss: 0.05037691444158554\n",
      "Epoch 5988/30000 Training Loss: 0.0547436885535717\n",
      "Epoch 5989/30000 Training Loss: 0.04721755534410477\n",
      "Epoch 5990/30000 Training Loss: 0.052522629499435425\n",
      "Epoch 5991/30000 Training Loss: 0.06256290525197983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5992/30000 Training Loss: 0.038765303790569305\n",
      "Epoch 5993/30000 Training Loss: 0.05023839324712753\n",
      "Epoch 5994/30000 Training Loss: 0.049713384360075\n",
      "Epoch 5995/30000 Training Loss: 0.049222953617572784\n",
      "Epoch 5996/30000 Training Loss: 0.05365217477083206\n",
      "Epoch 5997/30000 Training Loss: 0.04953967407345772\n",
      "Epoch 5998/30000 Training Loss: 0.050365835428237915\n",
      "Epoch 5999/30000 Training Loss: 0.04500718042254448\n",
      "Epoch 6000/30000 Training Loss: 0.05217466875910759\n",
      "Epoch 6000/30000 Validation Loss: 0.06157781928777695\n",
      "Epoch 6001/30000 Training Loss: 0.044085968285799026\n",
      "Epoch 6002/30000 Training Loss: 0.050186146050691605\n",
      "Epoch 6003/30000 Training Loss: 0.041987061500549316\n",
      "Epoch 6004/30000 Training Loss: 0.055026810616254807\n",
      "Epoch 6005/30000 Training Loss: 0.047995228320360184\n",
      "Epoch 6006/30000 Training Loss: 0.05184844136238098\n",
      "Epoch 6007/30000 Training Loss: 0.05170518904924393\n",
      "Epoch 6008/30000 Training Loss: 0.060028064996004105\n",
      "Epoch 6009/30000 Training Loss: 0.05792482569813728\n",
      "Epoch 6010/30000 Training Loss: 0.05637233331799507\n",
      "Epoch 6011/30000 Training Loss: 0.0489683523774147\n",
      "Epoch 6012/30000 Training Loss: 0.04651335999369621\n",
      "Epoch 6013/30000 Training Loss: 0.05722283571958542\n",
      "Epoch 6014/30000 Training Loss: 0.05154266953468323\n",
      "Epoch 6015/30000 Training Loss: 0.04903104528784752\n",
      "Epoch 6016/30000 Training Loss: 0.05421177297830582\n",
      "Epoch 6017/30000 Training Loss: 0.05034384876489639\n",
      "Epoch 6018/30000 Training Loss: 0.06330268085002899\n",
      "Epoch 6019/30000 Training Loss: 0.051043957471847534\n",
      "Epoch 6020/30000 Training Loss: 0.04934129863977432\n",
      "Epoch 6021/30000 Training Loss: 0.040755532681941986\n",
      "Epoch 6022/30000 Training Loss: 0.052299849689006805\n",
      "Epoch 6023/30000 Training Loss: 0.05122189596295357\n",
      "Epoch 6024/30000 Training Loss: 0.05130939558148384\n",
      "Epoch 6025/30000 Training Loss: 0.05810632184147835\n",
      "Epoch 6026/30000 Training Loss: 0.06016171723604202\n",
      "Epoch 6027/30000 Training Loss: 0.0580199658870697\n",
      "Epoch 6028/30000 Training Loss: 0.05173436924815178\n",
      "Epoch 6029/30000 Training Loss: 0.053867142647504807\n",
      "Epoch 6030/30000 Training Loss: 0.051267147064208984\n",
      "Epoch 6031/30000 Training Loss: 0.054639093577861786\n",
      "Epoch 6032/30000 Training Loss: 0.05441292002797127\n",
      "Epoch 6033/30000 Training Loss: 0.04526760056614876\n",
      "Epoch 6034/30000 Training Loss: 0.047712840139865875\n",
      "Epoch 6035/30000 Training Loss: 0.05356119945645332\n",
      "Epoch 6036/30000 Training Loss: 0.05962085723876953\n",
      "Epoch 6037/30000 Training Loss: 0.051209837198257446\n",
      "Epoch 6038/30000 Training Loss: 0.05369985103607178\n",
      "Epoch 6039/30000 Training Loss: 0.04071931913495064\n",
      "Epoch 6040/30000 Training Loss: 0.06308041512966156\n",
      "Epoch 6041/30000 Training Loss: 0.0485980249941349\n",
      "Epoch 6042/30000 Training Loss: 0.05009614676237106\n",
      "Epoch 6043/30000 Training Loss: 0.052274297922849655\n",
      "Epoch 6044/30000 Training Loss: 0.048277538269758224\n",
      "Epoch 6045/30000 Training Loss: 0.04383929818868637\n",
      "Epoch 6046/30000 Training Loss: 0.04509759694337845\n",
      "Epoch 6047/30000 Training Loss: 0.055310092866420746\n",
      "Epoch 6048/30000 Training Loss: 0.04979286342859268\n",
      "Epoch 6049/30000 Training Loss: 0.052191801369190216\n",
      "Epoch 6050/30000 Training Loss: 0.0490264818072319\n",
      "Epoch 6050/30000 Validation Loss: 0.048026662319898605\n",
      "Epoch 6051/30000 Training Loss: 0.04968331381678581\n",
      "Epoch 6052/30000 Training Loss: 0.056532103568315506\n",
      "Epoch 6053/30000 Training Loss: 0.063645139336586\n",
      "Epoch 6054/30000 Training Loss: 0.05261420086026192\n",
      "Epoch 6055/30000 Training Loss: 0.053926147520542145\n",
      "Epoch 6056/30000 Training Loss: 0.05467488616704941\n",
      "Epoch 6057/30000 Training Loss: 0.04860753193497658\n",
      "Epoch 6058/30000 Training Loss: 0.05969243496656418\n",
      "Epoch 6059/30000 Training Loss: 0.05086224526166916\n",
      "Epoch 6060/30000 Training Loss: 0.05225782468914986\n",
      "Epoch 6061/30000 Training Loss: 0.047478120774030685\n",
      "Epoch 6062/30000 Training Loss: 0.05465732142329216\n",
      "Epoch 6063/30000 Training Loss: 0.04861138015985489\n",
      "Epoch 6064/30000 Training Loss: 0.054807841777801514\n",
      "Epoch 6065/30000 Training Loss: 0.05734032392501831\n",
      "Epoch 6066/30000 Training Loss: 0.061857670545578\n",
      "Epoch 6067/30000 Training Loss: 0.04855845123529434\n",
      "Epoch 6068/30000 Training Loss: 0.04704557731747627\n",
      "Epoch 6069/30000 Training Loss: 0.04517005756497383\n",
      "Epoch 6070/30000 Training Loss: 0.05210142210125923\n",
      "Epoch 6071/30000 Training Loss: 0.049258891493082047\n",
      "Epoch 6072/30000 Training Loss: 0.039458490908145905\n",
      "Epoch 6073/30000 Training Loss: 0.05294608324766159\n",
      "Epoch 6074/30000 Training Loss: 0.05960456654429436\n",
      "Epoch 6075/30000 Training Loss: 0.05367854982614517\n",
      "Epoch 6076/30000 Training Loss: 0.04323964565992355\n",
      "Epoch 6077/30000 Training Loss: 0.05170721933245659\n",
      "Epoch 6078/30000 Training Loss: 0.04626302421092987\n",
      "Epoch 6079/30000 Training Loss: 0.054387230426073074\n",
      "Epoch 6080/30000 Training Loss: 0.054946888238191605\n",
      "Epoch 6081/30000 Training Loss: 0.05335954949259758\n",
      "Epoch 6082/30000 Training Loss: 0.04854442551732063\n",
      "Epoch 6083/30000 Training Loss: 0.041718874126672745\n",
      "Epoch 6084/30000 Training Loss: 0.04995526373386383\n",
      "Epoch 6085/30000 Training Loss: 0.048498671501874924\n",
      "Epoch 6086/30000 Training Loss: 0.046962298452854156\n",
      "Epoch 6087/30000 Training Loss: 0.050136350095272064\n",
      "Epoch 6088/30000 Training Loss: 0.05389201641082764\n",
      "Epoch 6089/30000 Training Loss: 0.04868105798959732\n",
      "Epoch 6090/30000 Training Loss: 0.055766068398952484\n",
      "Epoch 6091/30000 Training Loss: 0.048178620636463165\n",
      "Epoch 6092/30000 Training Loss: 0.058645546436309814\n",
      "Epoch 6093/30000 Training Loss: 0.05337846279144287\n",
      "Epoch 6094/30000 Training Loss: 0.05505041033029556\n",
      "Epoch 6095/30000 Training Loss: 0.047484107315540314\n",
      "Epoch 6096/30000 Training Loss: 0.05031605809926987\n",
      "Epoch 6097/30000 Training Loss: 0.05840768665075302\n",
      "Epoch 6098/30000 Training Loss: 0.05380726978182793\n",
      "Epoch 6099/30000 Training Loss: 0.05863247439265251\n",
      "Epoch 6100/30000 Training Loss: 0.05595733970403671\n",
      "Epoch 6100/30000 Validation Loss: 0.047907862812280655\n",
      "Epoch 6101/30000 Training Loss: 0.06118462607264519\n",
      "Epoch 6102/30000 Training Loss: 0.052138637751340866\n",
      "Epoch 6103/30000 Training Loss: 0.05841532349586487\n",
      "Epoch 6104/30000 Training Loss: 0.053464461117982864\n",
      "Epoch 6105/30000 Training Loss: 0.06379450112581253\n",
      "Epoch 6106/30000 Training Loss: 0.05583217740058899\n",
      "Epoch 6107/30000 Training Loss: 0.05598025396466255\n",
      "Epoch 6108/30000 Training Loss: 0.050516970455646515\n",
      "Epoch 6109/30000 Training Loss: 0.05244027450680733\n",
      "Epoch 6110/30000 Training Loss: 0.048381924629211426\n",
      "Epoch 6111/30000 Training Loss: 0.05488193780183792\n",
      "Epoch 6112/30000 Training Loss: 0.050497133284807205\n",
      "Epoch 6113/30000 Training Loss: 0.05521798133850098\n",
      "Epoch 6114/30000 Training Loss: 0.05994201451539993\n",
      "Epoch 6115/30000 Training Loss: 0.0431218184530735\n",
      "Epoch 6116/30000 Training Loss: 0.05646602064371109\n",
      "Epoch 6117/30000 Training Loss: 0.0556771457195282\n",
      "Epoch 6118/30000 Training Loss: 0.044539280235767365\n",
      "Epoch 6119/30000 Training Loss: 0.04815050959587097\n",
      "Epoch 6120/30000 Training Loss: 0.046193573623895645\n",
      "Epoch 6121/30000 Training Loss: 0.06423065066337585\n",
      "Epoch 6122/30000 Training Loss: 0.04959162324666977\n",
      "Epoch 6123/30000 Training Loss: 0.05066165328025818\n",
      "Epoch 6124/30000 Training Loss: 0.05537107586860657\n",
      "Epoch 6125/30000 Training Loss: 0.05106688290834427\n",
      "Epoch 6126/30000 Training Loss: 0.04924062639474869\n",
      "Epoch 6127/30000 Training Loss: 0.04767519235610962\n",
      "Epoch 6128/30000 Training Loss: 0.04470350593328476\n",
      "Epoch 6129/30000 Training Loss: 0.051842398941516876\n",
      "Epoch 6130/30000 Training Loss: 0.047075580805540085\n",
      "Epoch 6131/30000 Training Loss: 0.04337523132562637\n",
      "Epoch 6132/30000 Training Loss: 0.04773227125406265\n",
      "Epoch 6133/30000 Training Loss: 0.050870418548583984\n",
      "Epoch 6134/30000 Training Loss: 0.06108970195055008\n",
      "Epoch 6135/30000 Training Loss: 0.048538655042648315\n",
      "Epoch 6136/30000 Training Loss: 0.049913663417100906\n",
      "Epoch 6137/30000 Training Loss: 0.056472618132829666\n",
      "Epoch 6138/30000 Training Loss: 0.04308931902050972\n",
      "Epoch 6139/30000 Training Loss: 0.050602834671735764\n",
      "Epoch 6140/30000 Training Loss: 0.057564012706279755\n",
      "Epoch 6141/30000 Training Loss: 0.04895897954702377\n",
      "Epoch 6142/30000 Training Loss: 0.045616842806339264\n",
      "Epoch 6143/30000 Training Loss: 0.0503440797328949\n",
      "Epoch 6144/30000 Training Loss: 0.05613546818494797\n",
      "Epoch 6145/30000 Training Loss: 0.04641633480787277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6146/30000 Training Loss: 0.04990730434656143\n",
      "Epoch 6147/30000 Training Loss: 0.04379342123866081\n",
      "Epoch 6148/30000 Training Loss: 0.04467291384935379\n",
      "Epoch 6149/30000 Training Loss: 0.04954569786787033\n",
      "Epoch 6150/30000 Training Loss: 0.04174303635954857\n",
      "Epoch 6150/30000 Validation Loss: 0.05104203149676323\n",
      "Epoch 6151/30000 Training Loss: 0.05313124507665634\n",
      "Epoch 6152/30000 Training Loss: 0.0430942177772522\n",
      "Epoch 6153/30000 Training Loss: 0.04869489744305611\n",
      "Epoch 6154/30000 Training Loss: 0.05340512841939926\n",
      "Epoch 6155/30000 Training Loss: 0.05496879667043686\n",
      "Epoch 6156/30000 Training Loss: 0.05260122939944267\n",
      "Epoch 6157/30000 Training Loss: 0.07064695656299591\n",
      "Epoch 6158/30000 Training Loss: 0.05142417550086975\n",
      "Epoch 6159/30000 Training Loss: 0.04504550248384476\n",
      "Epoch 6160/30000 Training Loss: 0.054480694234371185\n",
      "Epoch 6161/30000 Training Loss: 0.050144582986831665\n",
      "Epoch 6162/30000 Training Loss: 0.049741581082344055\n",
      "Epoch 6163/30000 Training Loss: 0.058946747332811356\n",
      "Epoch 6164/30000 Training Loss: 0.04131457954645157\n",
      "Epoch 6165/30000 Training Loss: 0.056031208485364914\n",
      "Epoch 6166/30000 Training Loss: 0.0613383874297142\n",
      "Epoch 6167/30000 Training Loss: 0.04419378191232681\n",
      "Epoch 6168/30000 Training Loss: 0.05491120368242264\n",
      "Epoch 6169/30000 Training Loss: 0.045991070568561554\n",
      "Epoch 6170/30000 Training Loss: 0.053814757615327835\n",
      "Epoch 6171/30000 Training Loss: 0.05628667399287224\n",
      "Epoch 6172/30000 Training Loss: 0.045366071164608\n",
      "Epoch 6173/30000 Training Loss: 0.043204985558986664\n",
      "Epoch 6174/30000 Training Loss: 0.05028051882982254\n",
      "Epoch 6175/30000 Training Loss: 0.04361087083816528\n",
      "Epoch 6176/30000 Training Loss: 0.05008552595973015\n",
      "Epoch 6177/30000 Training Loss: 0.052545689046382904\n",
      "Epoch 6178/30000 Training Loss: 0.05064520239830017\n",
      "Epoch 6179/30000 Training Loss: 0.051346518099308014\n",
      "Epoch 6180/30000 Training Loss: 0.05799632519483566\n",
      "Epoch 6181/30000 Training Loss: 0.047601304948329926\n",
      "Epoch 6182/30000 Training Loss: 0.046928830444812775\n",
      "Epoch 6183/30000 Training Loss: 0.046876322478055954\n",
      "Epoch 6184/30000 Training Loss: 0.048878736793994904\n",
      "Epoch 6185/30000 Training Loss: 0.0606514997780323\n",
      "Epoch 6186/30000 Training Loss: 0.04796772450208664\n",
      "Epoch 6187/30000 Training Loss: 0.05346924066543579\n",
      "Epoch 6188/30000 Training Loss: 0.04841107502579689\n",
      "Epoch 6189/30000 Training Loss: 0.05436647683382034\n",
      "Epoch 6190/30000 Training Loss: 0.04444139450788498\n",
      "Epoch 6191/30000 Training Loss: 0.04775193706154823\n",
      "Epoch 6192/30000 Training Loss: 0.05353020876646042\n",
      "Epoch 6193/30000 Training Loss: 0.04406163841485977\n",
      "Epoch 6194/30000 Training Loss: 0.04769488796591759\n",
      "Epoch 6195/30000 Training Loss: 0.05724145844578743\n",
      "Epoch 6196/30000 Training Loss: 0.05107389762997627\n",
      "Epoch 6197/30000 Training Loss: 0.04321805760264397\n",
      "Epoch 6198/30000 Training Loss: 0.058748722076416016\n",
      "Epoch 6199/30000 Training Loss: 0.0477130226790905\n",
      "Epoch 6200/30000 Training Loss: 0.05185343697667122\n",
      "Epoch 6200/30000 Validation Loss: 0.05064316466450691\n",
      "Epoch 6201/30000 Training Loss: 0.04900965467095375\n",
      "Epoch 6202/30000 Training Loss: 0.055749475955963135\n",
      "Epoch 6203/30000 Training Loss: 0.054353613406419754\n",
      "Epoch 6204/30000 Training Loss: 0.06408887356519699\n",
      "Epoch 6205/30000 Training Loss: 0.053063422441482544\n",
      "Epoch 6206/30000 Training Loss: 0.050075847655534744\n",
      "Epoch 6207/30000 Training Loss: 0.04990200698375702\n",
      "Epoch 6208/30000 Training Loss: 0.044623568654060364\n",
      "Epoch 6209/30000 Training Loss: 0.05901794880628586\n",
      "Epoch 6210/30000 Training Loss: 0.06051549315452576\n",
      "Epoch 6211/30000 Training Loss: 0.052988119423389435\n",
      "Epoch 6212/30000 Training Loss: 0.04976192116737366\n",
      "Epoch 6213/30000 Training Loss: 0.047073155641555786\n",
      "Epoch 6214/30000 Training Loss: 0.0545901358127594\n",
      "Epoch 6215/30000 Training Loss: 0.046826478093862534\n",
      "Epoch 6216/30000 Training Loss: 0.05781366676092148\n",
      "Epoch 6217/30000 Training Loss: 0.047893572598695755\n",
      "Epoch 6218/30000 Training Loss: 0.051966823637485504\n",
      "Epoch 6219/30000 Training Loss: 0.05997351557016373\n",
      "Epoch 6220/30000 Training Loss: 0.04913109913468361\n",
      "Epoch 6221/30000 Training Loss: 0.058188725262880325\n",
      "Epoch 6222/30000 Training Loss: 0.05297479033470154\n",
      "Epoch 6223/30000 Training Loss: 0.050285764038562775\n",
      "Epoch 6224/30000 Training Loss: 0.05868878215551376\n",
      "Epoch 6225/30000 Training Loss: 0.06342349201440811\n",
      "Epoch 6226/30000 Training Loss: 0.04823993891477585\n",
      "Epoch 6227/30000 Training Loss: 0.050387393683195114\n",
      "Epoch 6228/30000 Training Loss: 0.0469696968793869\n",
      "Epoch 6229/30000 Training Loss: 0.043194014579057693\n",
      "Epoch 6230/30000 Training Loss: 0.050058454275131226\n",
      "Epoch 6231/30000 Training Loss: 0.05150157958269119\n",
      "Epoch 6232/30000 Training Loss: 0.04910876601934433\n",
      "Epoch 6233/30000 Training Loss: 0.046923089772462845\n",
      "Epoch 6234/30000 Training Loss: 0.04748573154211044\n",
      "Epoch 6235/30000 Training Loss: 0.044362761080265045\n",
      "Epoch 6236/30000 Training Loss: 0.057304125279188156\n",
      "Epoch 6237/30000 Training Loss: 0.0495547279715538\n",
      "Epoch 6238/30000 Training Loss: 0.04040759056806564\n",
      "Epoch 6239/30000 Training Loss: 0.05632955953478813\n",
      "Epoch 6240/30000 Training Loss: 0.05710549280047417\n",
      "Epoch 6241/30000 Training Loss: 0.0497005358338356\n",
      "Epoch 6242/30000 Training Loss: 0.053214333951473236\n",
      "Epoch 6243/30000 Training Loss: 0.04573530703783035\n",
      "Epoch 6244/30000 Training Loss: 0.04853086918592453\n",
      "Epoch 6245/30000 Training Loss: 0.04734552651643753\n",
      "Epoch 6246/30000 Training Loss: 0.046984393149614334\n",
      "Epoch 6247/30000 Training Loss: 0.0495348796248436\n",
      "Epoch 6248/30000 Training Loss: 0.0482037216424942\n",
      "Epoch 6249/30000 Training Loss: 0.05799252539873123\n",
      "Epoch 6250/30000 Training Loss: 0.05118107795715332\n",
      "Epoch 6250/30000 Validation Loss: 0.05741458013653755\n",
      "Epoch 6251/30000 Training Loss: 0.05188431590795517\n",
      "Epoch 6252/30000 Training Loss: 0.06306114792823792\n",
      "Epoch 6253/30000 Training Loss: 0.050382621586322784\n",
      "Epoch 6254/30000 Training Loss: 0.05951341986656189\n",
      "Epoch 6255/30000 Training Loss: 0.05618184804916382\n",
      "Epoch 6256/30000 Training Loss: 0.052083034068346024\n",
      "Epoch 6257/30000 Training Loss: 0.04663281887769699\n",
      "Epoch 6258/30000 Training Loss: 0.05043119937181473\n",
      "Epoch 6259/30000 Training Loss: 0.05549458786845207\n",
      "Epoch 6260/30000 Training Loss: 0.05165449529886246\n",
      "Epoch 6261/30000 Training Loss: 0.05908646434545517\n",
      "Epoch 6262/30000 Training Loss: 0.0469696931540966\n",
      "Epoch 6263/30000 Training Loss: 0.055305372923612595\n",
      "Epoch 6264/30000 Training Loss: 0.04862776771187782\n",
      "Epoch 6265/30000 Training Loss: 0.04980333894491196\n",
      "Epoch 6266/30000 Training Loss: 0.06290345638990402\n",
      "Epoch 6267/30000 Training Loss: 0.05169513821601868\n",
      "Epoch 6268/30000 Training Loss: 0.049889035522937775\n",
      "Epoch 6269/30000 Training Loss: 0.06704549491405487\n",
      "Epoch 6270/30000 Training Loss: 0.056634921580553055\n",
      "Epoch 6271/30000 Training Loss: 0.04885631799697876\n",
      "Epoch 6272/30000 Training Loss: 0.05033208057284355\n",
      "Epoch 6273/30000 Training Loss: 0.04376661777496338\n",
      "Epoch 6274/30000 Training Loss: 0.06312692910432816\n",
      "Epoch 6275/30000 Training Loss: 0.044945646077394485\n",
      "Epoch 6276/30000 Training Loss: 0.04506818205118179\n",
      "Epoch 6277/30000 Training Loss: 0.050084084272384644\n",
      "Epoch 6278/30000 Training Loss: 0.04979800432920456\n",
      "Epoch 6279/30000 Training Loss: 0.057543687522411346\n",
      "Epoch 6280/30000 Training Loss: 0.04626021534204483\n",
      "Epoch 6281/30000 Training Loss: 0.051540713757276535\n",
      "Epoch 6282/30000 Training Loss: 0.05769442766904831\n",
      "Epoch 6283/30000 Training Loss: 0.05338829755783081\n",
      "Epoch 6284/30000 Training Loss: 0.05736692622303963\n",
      "Epoch 6285/30000 Training Loss: 0.05238053947687149\n",
      "Epoch 6286/30000 Training Loss: 0.05057113245129585\n",
      "Epoch 6287/30000 Training Loss: 0.04887700453400612\n",
      "Epoch 6288/30000 Training Loss: 0.04790720343589783\n",
      "Epoch 6289/30000 Training Loss: 0.046600185334682465\n",
      "Epoch 6290/30000 Training Loss: 0.050680506974458694\n",
      "Epoch 6291/30000 Training Loss: 0.045727379620075226\n",
      "Epoch 6292/30000 Training Loss: 0.05180908367037773\n",
      "Epoch 6293/30000 Training Loss: 0.052993398159742355\n",
      "Epoch 6294/30000 Training Loss: 0.05267626792192459\n",
      "Epoch 6295/30000 Training Loss: 0.05683908611536026\n",
      "Epoch 6296/30000 Training Loss: 0.05080554634332657\n",
      "Epoch 6297/30000 Training Loss: 0.047461509704589844\n",
      "Epoch 6298/30000 Training Loss: 0.051702629774808884\n",
      "Epoch 6299/30000 Training Loss: 0.05394969508051872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6300/30000 Training Loss: 0.04846177250146866\n",
      "Epoch 6300/30000 Validation Loss: 0.05709904432296753\n",
      "Epoch 6301/30000 Training Loss: 0.05400427058339119\n",
      "Epoch 6302/30000 Training Loss: 0.04629223048686981\n",
      "Epoch 6303/30000 Training Loss: 0.04310770705342293\n",
      "Epoch 6304/30000 Training Loss: 0.05098781734704971\n",
      "Epoch 6305/30000 Training Loss: 0.054388511925935745\n",
      "Epoch 6306/30000 Training Loss: 0.05480926111340523\n",
      "Epoch 6307/30000 Training Loss: 0.04857028275728226\n",
      "Epoch 6308/30000 Training Loss: 0.044920507818460464\n",
      "Epoch 6309/30000 Training Loss: 0.05166047066450119\n",
      "Epoch 6310/30000 Training Loss: 0.05335870385169983\n",
      "Epoch 6311/30000 Training Loss: 0.054089952260255814\n",
      "Epoch 6312/30000 Training Loss: 0.05310981348156929\n",
      "Epoch 6313/30000 Training Loss: 0.048830658197402954\n",
      "Epoch 6314/30000 Training Loss: 0.04327584058046341\n",
      "Epoch 6315/30000 Training Loss: 0.04930908605456352\n",
      "Epoch 6316/30000 Training Loss: 0.049563370645046234\n",
      "Epoch 6317/30000 Training Loss: 0.042483650147914886\n",
      "Epoch 6318/30000 Training Loss: 0.052311837673187256\n",
      "Epoch 6319/30000 Training Loss: 0.060049183666706085\n",
      "Epoch 6320/30000 Training Loss: 0.04655534029006958\n",
      "Epoch 6321/30000 Training Loss: 0.051152873784303665\n",
      "Epoch 6322/30000 Training Loss: 0.05334530025720596\n",
      "Epoch 6323/30000 Training Loss: 0.049371641129255295\n",
      "Epoch 6324/30000 Training Loss: 0.0472244918346405\n",
      "Epoch 6325/30000 Training Loss: 0.053166329860687256\n",
      "Epoch 6326/30000 Training Loss: 0.0461893193423748\n",
      "Epoch 6327/30000 Training Loss: 0.049419913440942764\n",
      "Epoch 6328/30000 Training Loss: 0.05433342605829239\n",
      "Epoch 6329/30000 Training Loss: 0.04821803793311119\n",
      "Epoch 6330/30000 Training Loss: 0.053836144506931305\n",
      "Epoch 6331/30000 Training Loss: 0.05264853313565254\n",
      "Epoch 6332/30000 Training Loss: 0.04993167892098427\n",
      "Epoch 6333/30000 Training Loss: 0.0433642603456974\n",
      "Epoch 6334/30000 Training Loss: 0.04981829598546028\n",
      "Epoch 6335/30000 Training Loss: 0.05052848905324936\n",
      "Epoch 6336/30000 Training Loss: 0.06083677336573601\n",
      "Epoch 6337/30000 Training Loss: 0.05760277062654495\n",
      "Epoch 6338/30000 Training Loss: 0.04815275967121124\n",
      "Epoch 6339/30000 Training Loss: 0.04411368444561958\n",
      "Epoch 6340/30000 Training Loss: 0.04378744214773178\n",
      "Epoch 6341/30000 Training Loss: 0.04972276836633682\n",
      "Epoch 6342/30000 Training Loss: 0.045174695551395416\n",
      "Epoch 6343/30000 Training Loss: 0.043715059757232666\n",
      "Epoch 6344/30000 Training Loss: 0.04746502637863159\n",
      "Epoch 6345/30000 Training Loss: 0.05104339122772217\n",
      "Epoch 6346/30000 Training Loss: 0.04922977462410927\n",
      "Epoch 6347/30000 Training Loss: 0.051468681544065475\n",
      "Epoch 6348/30000 Training Loss: 0.04797349497675896\n",
      "Epoch 6349/30000 Training Loss: 0.05690453201532364\n",
      "Epoch 6350/30000 Training Loss: 0.04944031685590744\n",
      "Epoch 6350/30000 Validation Loss: 0.05248863250017166\n",
      "Epoch 6351/30000 Training Loss: 0.044276099652051926\n",
      "Epoch 6352/30000 Training Loss: 0.05085187032818794\n",
      "Epoch 6353/30000 Training Loss: 0.049561742693185806\n",
      "Epoch 6354/30000 Training Loss: 0.053843457251787186\n",
      "Epoch 6355/30000 Training Loss: 0.060195185244083405\n",
      "Epoch 6356/30000 Training Loss: 0.05023699998855591\n",
      "Epoch 6357/30000 Training Loss: 0.05295677110552788\n",
      "Epoch 6358/30000 Training Loss: 0.048205241560935974\n",
      "Epoch 6359/30000 Training Loss: 0.05502593517303467\n",
      "Epoch 6360/30000 Training Loss: 0.052829910069704056\n",
      "Epoch 6361/30000 Training Loss: 0.053969405591487885\n",
      "Epoch 6362/30000 Training Loss: 0.054306406527757645\n",
      "Epoch 6363/30000 Training Loss: 0.0579848475754261\n",
      "Epoch 6364/30000 Training Loss: 0.05300923064351082\n",
      "Epoch 6365/30000 Training Loss: 0.04675052687525749\n",
      "Epoch 6366/30000 Training Loss: 0.04501168057322502\n",
      "Epoch 6367/30000 Training Loss: 0.05227113515138626\n",
      "Epoch 6368/30000 Training Loss: 0.04916367679834366\n",
      "Epoch 6369/30000 Training Loss: 0.05200537294149399\n",
      "Epoch 6370/30000 Training Loss: 0.05521487072110176\n",
      "Epoch 6371/30000 Training Loss: 0.05141545087099075\n",
      "Epoch 6372/30000 Training Loss: 0.04886660352349281\n",
      "Epoch 6373/30000 Training Loss: 0.0540359728038311\n",
      "Epoch 6374/30000 Training Loss: 0.04406111687421799\n",
      "Epoch 6375/30000 Training Loss: 0.05258883908390999\n",
      "Epoch 6376/30000 Training Loss: 0.0457286611199379\n",
      "Epoch 6377/30000 Training Loss: 0.059760332107543945\n",
      "Epoch 6378/30000 Training Loss: 0.050083380192518234\n",
      "Epoch 6379/30000 Training Loss: 0.051126398146152496\n",
      "Epoch 6380/30000 Training Loss: 0.048163607716560364\n",
      "Epoch 6381/30000 Training Loss: 0.0568053312599659\n",
      "Epoch 6382/30000 Training Loss: 0.04625553637742996\n",
      "Epoch 6383/30000 Training Loss: 0.04922007769346237\n",
      "Epoch 6384/30000 Training Loss: 0.04817516729235649\n",
      "Epoch 6385/30000 Training Loss: 0.05890680104494095\n",
      "Epoch 6386/30000 Training Loss: 0.05020062252879143\n",
      "Epoch 6387/30000 Training Loss: 0.04706384614109993\n",
      "Epoch 6388/30000 Training Loss: 0.05756492167711258\n",
      "Epoch 6389/30000 Training Loss: 0.05671750381588936\n",
      "Epoch 6390/30000 Training Loss: 0.04552018269896507\n",
      "Epoch 6391/30000 Training Loss: 0.04673027992248535\n",
      "Epoch 6392/30000 Training Loss: 0.053069572895765305\n",
      "Epoch 6393/30000 Training Loss: 0.04523513838648796\n",
      "Epoch 6394/30000 Training Loss: 0.05319838970899582\n",
      "Epoch 6395/30000 Training Loss: 0.04804983735084534\n",
      "Epoch 6396/30000 Training Loss: 0.04584895819425583\n",
      "Epoch 6397/30000 Training Loss: 0.04898454621434212\n",
      "Epoch 6398/30000 Training Loss: 0.052304334938526154\n",
      "Epoch 6399/30000 Training Loss: 0.06657464057207108\n",
      "Epoch 6400/30000 Training Loss: 0.045616719871759415\n",
      "Epoch 6400/30000 Validation Loss: 0.05725208669900894\n",
      "Epoch 6401/30000 Training Loss: 0.05343122407793999\n",
      "Epoch 6402/30000 Training Loss: 0.062326282262802124\n",
      "Epoch 6403/30000 Training Loss: 0.059720225632190704\n",
      "Epoch 6404/30000 Training Loss: 0.060279954224824905\n",
      "Epoch 6405/30000 Training Loss: 0.05902134254574776\n",
      "Epoch 6406/30000 Training Loss: 0.05517851188778877\n",
      "Epoch 6407/30000 Training Loss: 0.04881240800023079\n",
      "Epoch 6408/30000 Training Loss: 0.04496914893388748\n",
      "Epoch 6409/30000 Training Loss: 0.05135113000869751\n",
      "Epoch 6410/30000 Training Loss: 0.051059920340776443\n",
      "Epoch 6411/30000 Training Loss: 0.06825900077819824\n",
      "Epoch 6412/30000 Training Loss: 0.04977225139737129\n",
      "Epoch 6413/30000 Training Loss: 0.05442468076944351\n",
      "Epoch 6414/30000 Training Loss: 0.05290578678250313\n",
      "Epoch 6415/30000 Training Loss: 0.04576035216450691\n",
      "Epoch 6416/30000 Training Loss: 0.05213307589292526\n",
      "Epoch 6417/30000 Training Loss: 0.055594008415937424\n",
      "Epoch 6418/30000 Training Loss: 0.05703725665807724\n",
      "Epoch 6419/30000 Training Loss: 0.054218750447034836\n",
      "Epoch 6420/30000 Training Loss: 0.04844675958156586\n",
      "Epoch 6421/30000 Training Loss: 0.0583488829433918\n",
      "Epoch 6422/30000 Training Loss: 0.05098911374807358\n",
      "Epoch 6423/30000 Training Loss: 0.045810516923666\n",
      "Epoch 6424/30000 Training Loss: 0.056903399527072906\n",
      "Epoch 6425/30000 Training Loss: 0.04097087308764458\n",
      "Epoch 6426/30000 Training Loss: 0.052365221083164215\n",
      "Epoch 6427/30000 Training Loss: 0.04963259398937225\n",
      "Epoch 6428/30000 Training Loss: 0.04931376501917839\n",
      "Epoch 6429/30000 Training Loss: 0.052757423371076584\n",
      "Epoch 6430/30000 Training Loss: 0.05550116300582886\n",
      "Epoch 6431/30000 Training Loss: 0.047638095915317535\n",
      "Epoch 6432/30000 Training Loss: 0.05914461612701416\n",
      "Epoch 6433/30000 Training Loss: 0.04850054532289505\n",
      "Epoch 6434/30000 Training Loss: 0.05037640407681465\n",
      "Epoch 6435/30000 Training Loss: 0.047422461211681366\n",
      "Epoch 6436/30000 Training Loss: 0.04988429695367813\n",
      "Epoch 6437/30000 Training Loss: 0.05645573139190674\n",
      "Epoch 6438/30000 Training Loss: 0.05277370288968086\n",
      "Epoch 6439/30000 Training Loss: 0.06291009485721588\n",
      "Epoch 6440/30000 Training Loss: 0.04327515512704849\n",
      "Epoch 6441/30000 Training Loss: 0.047411613166332245\n",
      "Epoch 6442/30000 Training Loss: 0.05332716181874275\n",
      "Epoch 6443/30000 Training Loss: 0.054211270064115524\n",
      "Epoch 6444/30000 Training Loss: 0.05595896393060684\n",
      "Epoch 6445/30000 Training Loss: 0.046191368252038956\n",
      "Epoch 6446/30000 Training Loss: 0.05241534113883972\n",
      "Epoch 6447/30000 Training Loss: 0.04299558699131012\n",
      "Epoch 6448/30000 Training Loss: 0.05688771605491638\n",
      "Epoch 6449/30000 Training Loss: 0.048353902995586395\n",
      "Epoch 6450/30000 Training Loss: 0.05365560203790665\n",
      "Epoch 6450/30000 Validation Loss: 0.06106935814023018\n",
      "Epoch 6451/30000 Training Loss: 0.04943625628948212\n",
      "Epoch 6452/30000 Training Loss: 0.04838498681783676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6453/30000 Training Loss: 0.05178648978471756\n",
      "Epoch 6454/30000 Training Loss: 0.05216766521334648\n",
      "Epoch 6455/30000 Training Loss: 0.04696343094110489\n",
      "Epoch 6456/30000 Training Loss: 0.045624665915966034\n",
      "Epoch 6457/30000 Training Loss: 0.0569075271487236\n",
      "Epoch 6458/30000 Training Loss: 0.05885138362646103\n",
      "Epoch 6459/30000 Training Loss: 0.051386743783950806\n",
      "Epoch 6460/30000 Training Loss: 0.06381436437368393\n",
      "Epoch 6461/30000 Training Loss: 0.04815053567290306\n",
      "Epoch 6462/30000 Training Loss: 0.05159981921315193\n",
      "Epoch 6463/30000 Training Loss: 0.044920556247234344\n",
      "Epoch 6464/30000 Training Loss: 0.04887210950255394\n",
      "Epoch 6465/30000 Training Loss: 0.04897984489798546\n",
      "Epoch 6466/30000 Training Loss: 0.05245841667056084\n",
      "Epoch 6467/30000 Training Loss: 0.05522453784942627\n",
      "Epoch 6468/30000 Training Loss: 0.05123307555913925\n",
      "Epoch 6469/30000 Training Loss: 0.04900561645627022\n",
      "Epoch 6470/30000 Training Loss: 0.04722244292497635\n",
      "Epoch 6471/30000 Training Loss: 0.04791342094540596\n",
      "Epoch 6472/30000 Training Loss: 0.04019766300916672\n",
      "Epoch 6473/30000 Training Loss: 0.04564971476793289\n",
      "Epoch 6474/30000 Training Loss: 0.04759315773844719\n",
      "Epoch 6475/30000 Training Loss: 0.05835927650332451\n",
      "Epoch 6476/30000 Training Loss: 0.046518027782440186\n",
      "Epoch 6477/30000 Training Loss: 0.04665611311793327\n",
      "Epoch 6478/30000 Training Loss: 0.046776480972766876\n",
      "Epoch 6479/30000 Training Loss: 0.051116954535245895\n",
      "Epoch 6480/30000 Training Loss: 0.044033460319042206\n",
      "Epoch 6481/30000 Training Loss: 0.05154942721128464\n",
      "Epoch 6482/30000 Training Loss: 0.04475026577711105\n",
      "Epoch 6483/30000 Training Loss: 0.050499916076660156\n",
      "Epoch 6484/30000 Training Loss: 0.04748912528157234\n",
      "Epoch 6485/30000 Training Loss: 0.03966708481311798\n",
      "Epoch 6486/30000 Training Loss: 0.05184457451105118\n",
      "Epoch 6487/30000 Training Loss: 0.04552488774061203\n",
      "Epoch 6488/30000 Training Loss: 0.05289200693368912\n",
      "Epoch 6489/30000 Training Loss: 0.04482920840382576\n",
      "Epoch 6490/30000 Training Loss: 0.042068757116794586\n",
      "Epoch 6491/30000 Training Loss: 0.04684675857424736\n",
      "Epoch 6492/30000 Training Loss: 0.04276155307888985\n",
      "Epoch 6493/30000 Training Loss: 0.050215017050504684\n",
      "Epoch 6494/30000 Training Loss: 0.05302156135439873\n",
      "Epoch 6495/30000 Training Loss: 0.04263613000512123\n",
      "Epoch 6496/30000 Training Loss: 0.047403447329998016\n",
      "Epoch 6497/30000 Training Loss: 0.054612211883068085\n",
      "Epoch 6498/30000 Training Loss: 0.05699952319264412\n",
      "Epoch 6499/30000 Training Loss: 0.06286372244358063\n",
      "Epoch 6500/30000 Training Loss: 0.059683144092559814\n",
      "Epoch 6500/30000 Validation Loss: 0.05307502672076225\n",
      "Epoch 6501/30000 Training Loss: 0.05102084204554558\n",
      "Epoch 6502/30000 Training Loss: 0.05050729960203171\n",
      "Epoch 6503/30000 Training Loss: 0.04870999604463577\n",
      "Epoch 6504/30000 Training Loss: 0.05051250383257866\n",
      "Epoch 6505/30000 Training Loss: 0.04837389290332794\n",
      "Epoch 6506/30000 Training Loss: 0.05194159224629402\n",
      "Epoch 6507/30000 Training Loss: 0.050015054643154144\n",
      "Epoch 6508/30000 Training Loss: 0.051921140402555466\n",
      "Epoch 6509/30000 Training Loss: 0.048928696662187576\n",
      "Epoch 6510/30000 Training Loss: 0.050254445523023605\n",
      "Epoch 6511/30000 Training Loss: 0.047855086624622345\n",
      "Epoch 6512/30000 Training Loss: 0.0457090362906456\n",
      "Epoch 6513/30000 Training Loss: 0.05547725036740303\n",
      "Epoch 6514/30000 Training Loss: 0.0474361889064312\n",
      "Epoch 6515/30000 Training Loss: 0.06177385896444321\n",
      "Epoch 6516/30000 Training Loss: 0.05905469134449959\n",
      "Epoch 6517/30000 Training Loss: 0.051218628883361816\n",
      "Epoch 6518/30000 Training Loss: 0.048442430794239044\n",
      "Epoch 6519/30000 Training Loss: 0.05090079456567764\n",
      "Epoch 6520/30000 Training Loss: 0.052093256264925\n",
      "Epoch 6521/30000 Training Loss: 0.056685615330934525\n",
      "Epoch 6522/30000 Training Loss: 0.05197183042764664\n",
      "Epoch 6523/30000 Training Loss: 0.056030940264463425\n",
      "Epoch 6524/30000 Training Loss: 0.05385627597570419\n",
      "Epoch 6525/30000 Training Loss: 0.05837061256170273\n",
      "Epoch 6526/30000 Training Loss: 0.05087783932685852\n",
      "Epoch 6527/30000 Training Loss: 0.049021970480680466\n",
      "Epoch 6528/30000 Training Loss: 0.06012571603059769\n",
      "Epoch 6529/30000 Training Loss: 0.041772209107875824\n",
      "Epoch 6530/30000 Training Loss: 0.05008918046951294\n",
      "Epoch 6531/30000 Training Loss: 0.05712943524122238\n",
      "Epoch 6532/30000 Training Loss: 0.06456293910741806\n",
      "Epoch 6533/30000 Training Loss: 0.054567813873291016\n",
      "Epoch 6534/30000 Training Loss: 0.05967315286397934\n",
      "Epoch 6535/30000 Training Loss: 0.051321886479854584\n",
      "Epoch 6536/30000 Training Loss: 0.046974264085292816\n",
      "Epoch 6537/30000 Training Loss: 0.04552407190203667\n",
      "Epoch 6538/30000 Training Loss: 0.04605801776051521\n",
      "Epoch 6539/30000 Training Loss: 0.04768691956996918\n",
      "Epoch 6540/30000 Training Loss: 0.05706048011779785\n",
      "Epoch 6541/30000 Training Loss: 0.05310308188199997\n",
      "Epoch 6542/30000 Training Loss: 0.05785029008984566\n",
      "Epoch 6543/30000 Training Loss: 0.04455418139696121\n",
      "Epoch 6544/30000 Training Loss: 0.05332988500595093\n",
      "Epoch 6545/30000 Training Loss: 0.05027380585670471\n",
      "Epoch 6546/30000 Training Loss: 0.04758733883500099\n",
      "Epoch 6547/30000 Training Loss: 0.042251475155353546\n",
      "Epoch 6548/30000 Training Loss: 0.060997821390628815\n",
      "Epoch 6549/30000 Training Loss: 0.047387056052684784\n",
      "Epoch 6550/30000 Training Loss: 0.04874391108751297\n",
      "Epoch 6550/30000 Validation Loss: 0.04661371186375618\n",
      "Epoch 6551/30000 Training Loss: 0.04757383465766907\n",
      "Epoch 6552/30000 Training Loss: 0.05608405917882919\n",
      "Epoch 6553/30000 Training Loss: 0.04846293479204178\n",
      "Epoch 6554/30000 Training Loss: 0.05269979313015938\n",
      "Epoch 6555/30000 Training Loss: 0.046187616884708405\n",
      "Epoch 6556/30000 Training Loss: 0.049829091876745224\n",
      "Epoch 6557/30000 Training Loss: 0.048496998846530914\n",
      "Epoch 6558/30000 Training Loss: 0.04954218119382858\n",
      "Epoch 6559/30000 Training Loss: 0.046990759670734406\n",
      "Epoch 6560/30000 Training Loss: 0.05155099555850029\n",
      "Epoch 6561/30000 Training Loss: 0.04393962025642395\n",
      "Epoch 6562/30000 Training Loss: 0.05301767587661743\n",
      "Epoch 6563/30000 Training Loss: 0.05288194492459297\n",
      "Epoch 6564/30000 Training Loss: 0.05130896717309952\n",
      "Epoch 6565/30000 Training Loss: 0.05109413340687752\n",
      "Epoch 6566/30000 Training Loss: 0.05809267610311508\n",
      "Epoch 6567/30000 Training Loss: 0.056192703545093536\n",
      "Epoch 6568/30000 Training Loss: 0.04878916218876839\n",
      "Epoch 6569/30000 Training Loss: 0.05140950158238411\n",
      "Epoch 6570/30000 Training Loss: 0.04870060831308365\n",
      "Epoch 6571/30000 Training Loss: 0.04760302975773811\n",
      "Epoch 6572/30000 Training Loss: 0.044083401560783386\n",
      "Epoch 6573/30000 Training Loss: 0.049468208104372025\n",
      "Epoch 6574/30000 Training Loss: 0.052509237080812454\n",
      "Epoch 6575/30000 Training Loss: 0.05364124849438667\n",
      "Epoch 6576/30000 Training Loss: 0.05891493707895279\n",
      "Epoch 6577/30000 Training Loss: 0.051255714148283005\n",
      "Epoch 6578/30000 Training Loss: 0.05048413947224617\n",
      "Epoch 6579/30000 Training Loss: 0.048151466995477676\n",
      "Epoch 6580/30000 Training Loss: 0.0466347336769104\n",
      "Epoch 6581/30000 Training Loss: 0.0522305853664875\n",
      "Epoch 6582/30000 Training Loss: 0.05732319504022598\n",
      "Epoch 6583/30000 Training Loss: 0.05318592116236687\n",
      "Epoch 6584/30000 Training Loss: 0.05165645480155945\n",
      "Epoch 6585/30000 Training Loss: 0.053704701364040375\n",
      "Epoch 6586/30000 Training Loss: 0.04406000301241875\n",
      "Epoch 6587/30000 Training Loss: 0.050201088190078735\n",
      "Epoch 6588/30000 Training Loss: 0.05026078224182129\n",
      "Epoch 6589/30000 Training Loss: 0.05353768542408943\n",
      "Epoch 6590/30000 Training Loss: 0.05265824869275093\n",
      "Epoch 6591/30000 Training Loss: 0.05187518149614334\n",
      "Epoch 6592/30000 Training Loss: 0.05183868482708931\n",
      "Epoch 6593/30000 Training Loss: 0.05361498147249222\n",
      "Epoch 6594/30000 Training Loss: 0.043896861374378204\n",
      "Epoch 6595/30000 Training Loss: 0.057953618466854095\n",
      "Epoch 6596/30000 Training Loss: 0.057503439486026764\n",
      "Epoch 6597/30000 Training Loss: 0.049992598593235016\n",
      "Epoch 6598/30000 Training Loss: 0.05145728588104248\n",
      "Epoch 6599/30000 Training Loss: 0.04064350947737694\n",
      "Epoch 6600/30000 Training Loss: 0.04928293824195862\n",
      "Epoch 6600/30000 Validation Loss: 0.04541525989770889\n",
      "Epoch 6601/30000 Training Loss: 0.05751373618841171\n",
      "Epoch 6602/30000 Training Loss: 0.05051124840974808\n",
      "Epoch 6603/30000 Training Loss: 0.056110404431819916\n",
      "Epoch 6604/30000 Training Loss: 0.04403454810380936\n",
      "Epoch 6605/30000 Training Loss: 0.047998249530792236\n",
      "Epoch 6606/30000 Training Loss: 0.05509914085268974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6607/30000 Training Loss: 0.04682265967130661\n",
      "Epoch 6608/30000 Training Loss: 0.0511343777179718\n",
      "Epoch 6609/30000 Training Loss: 0.05060658976435661\n",
      "Epoch 6610/30000 Training Loss: 0.04941413179039955\n",
      "Epoch 6611/30000 Training Loss: 0.04728690907359123\n",
      "Epoch 6612/30000 Training Loss: 0.05391580983996391\n",
      "Epoch 6613/30000 Training Loss: 0.044397227466106415\n",
      "Epoch 6614/30000 Training Loss: 0.05073646456003189\n",
      "Epoch 6615/30000 Training Loss: 0.056229282170534134\n",
      "Epoch 6616/30000 Training Loss: 0.053214751183986664\n",
      "Epoch 6617/30000 Training Loss: 0.05336128547787666\n",
      "Epoch 6618/30000 Training Loss: 0.05310530588030815\n",
      "Epoch 6619/30000 Training Loss: 0.05098389834165573\n",
      "Epoch 6620/30000 Training Loss: 0.06196541339159012\n",
      "Epoch 6621/30000 Training Loss: 0.060400016605854034\n",
      "Epoch 6622/30000 Training Loss: 0.05756371468305588\n",
      "Epoch 6623/30000 Training Loss: 0.052076052874326706\n",
      "Epoch 6624/30000 Training Loss: 0.05403853580355644\n",
      "Epoch 6625/30000 Training Loss: 0.04763432592153549\n",
      "Epoch 6626/30000 Training Loss: 0.05602775141596794\n",
      "Epoch 6627/30000 Training Loss: 0.05297055095434189\n",
      "Epoch 6628/30000 Training Loss: 0.048264823853969574\n",
      "Epoch 6629/30000 Training Loss: 0.05036493018269539\n",
      "Epoch 6630/30000 Training Loss: 0.04655216634273529\n",
      "Epoch 6631/30000 Training Loss: 0.05649342015385628\n",
      "Epoch 6632/30000 Training Loss: 0.05355149507522583\n",
      "Epoch 6633/30000 Training Loss: 0.05092731863260269\n",
      "Epoch 6634/30000 Training Loss: 0.0566672682762146\n",
      "Epoch 6635/30000 Training Loss: 0.05397704988718033\n",
      "Epoch 6636/30000 Training Loss: 0.05391930788755417\n",
      "Epoch 6637/30000 Training Loss: 0.04990840703248978\n",
      "Epoch 6638/30000 Training Loss: 0.0542554147541523\n",
      "Epoch 6639/30000 Training Loss: 0.04044554382562637\n",
      "Epoch 6640/30000 Training Loss: 0.0470670722424984\n",
      "Epoch 6641/30000 Training Loss: 0.052119337022304535\n",
      "Epoch 6642/30000 Training Loss: 0.05425671860575676\n",
      "Epoch 6643/30000 Training Loss: 0.05217364430427551\n",
      "Epoch 6644/30000 Training Loss: 0.04816645383834839\n",
      "Epoch 6645/30000 Training Loss: 0.04991535842418671\n",
      "Epoch 6646/30000 Training Loss: 0.05032037943601608\n",
      "Epoch 6647/30000 Training Loss: 0.05183519050478935\n",
      "Epoch 6648/30000 Training Loss: 0.052336640655994415\n",
      "Epoch 6649/30000 Training Loss: 0.05788136646151543\n",
      "Epoch 6650/30000 Training Loss: 0.04364101216197014\n",
      "Epoch 6650/30000 Validation Loss: 0.05873659998178482\n",
      "Epoch 6651/30000 Training Loss: 0.046030011028051376\n",
      "Epoch 6652/30000 Training Loss: 0.050040118396282196\n",
      "Epoch 6653/30000 Training Loss: 0.04833660274744034\n",
      "Epoch 6654/30000 Training Loss: 0.05595880746841431\n",
      "Epoch 6655/30000 Training Loss: 0.04689938202500343\n",
      "Epoch 6656/30000 Training Loss: 0.04773951321840286\n",
      "Epoch 6657/30000 Training Loss: 0.06593440473079681\n",
      "Epoch 6658/30000 Training Loss: 0.05287935584783554\n",
      "Epoch 6659/30000 Training Loss: 0.042759887874126434\n",
      "Epoch 6660/30000 Training Loss: 0.05498474836349487\n",
      "Epoch 6661/30000 Training Loss: 0.048358283936977386\n",
      "Epoch 6662/30000 Training Loss: 0.051049839705228806\n",
      "Epoch 6663/30000 Training Loss: 0.046323832124471664\n",
      "Epoch 6664/30000 Training Loss: 0.046963196247816086\n",
      "Epoch 6665/30000 Training Loss: 0.05520118400454521\n",
      "Epoch 6666/30000 Training Loss: 0.0508338138461113\n",
      "Epoch 6667/30000 Training Loss: 0.05222552269697189\n",
      "Epoch 6668/30000 Training Loss: 0.048021480441093445\n",
      "Epoch 6669/30000 Training Loss: 0.05106808990240097\n",
      "Epoch 6670/30000 Training Loss: 0.0431772843003273\n",
      "Epoch 6671/30000 Training Loss: 0.04634265601634979\n",
      "Epoch 6672/30000 Training Loss: 0.05441650003194809\n",
      "Epoch 6673/30000 Training Loss: 0.05219518020749092\n",
      "Epoch 6674/30000 Training Loss: 0.04349196329712868\n",
      "Epoch 6675/30000 Training Loss: 0.05979879945516586\n",
      "Epoch 6676/30000 Training Loss: 0.05258098989725113\n",
      "Epoch 6677/30000 Training Loss: 0.050563931465148926\n",
      "Epoch 6678/30000 Training Loss: 0.044993333518505096\n",
      "Epoch 6679/30000 Training Loss: 0.045804381370544434\n",
      "Epoch 6680/30000 Training Loss: 0.03923744708299637\n",
      "Epoch 6681/30000 Training Loss: 0.044591568410396576\n",
      "Epoch 6682/30000 Training Loss: 0.046908263117074966\n",
      "Epoch 6683/30000 Training Loss: 0.04414232075214386\n",
      "Epoch 6684/30000 Training Loss: 0.05402214080095291\n",
      "Epoch 6685/30000 Training Loss: 0.052594803273677826\n",
      "Epoch 6686/30000 Training Loss: 0.04432930052280426\n",
      "Epoch 6687/30000 Training Loss: 0.050221122801303864\n",
      "Epoch 6688/30000 Training Loss: 0.05384541675448418\n",
      "Epoch 6689/30000 Training Loss: 0.04917283356189728\n",
      "Epoch 6690/30000 Training Loss: 0.04415508732199669\n",
      "Epoch 6691/30000 Training Loss: 0.05436476320028305\n",
      "Epoch 6692/30000 Training Loss: 0.04665009304881096\n",
      "Epoch 6693/30000 Training Loss: 0.04540183022618294\n",
      "Epoch 6694/30000 Training Loss: 0.046357959508895874\n",
      "Epoch 6695/30000 Training Loss: 0.05753452703356743\n",
      "Epoch 6696/30000 Training Loss: 0.045762449502944946\n",
      "Epoch 6697/30000 Training Loss: 0.05540518835186958\n",
      "Epoch 6698/30000 Training Loss: 0.05207454040646553\n",
      "Epoch 6699/30000 Training Loss: 0.060008563101291656\n",
      "Epoch 6700/30000 Training Loss: 0.051714539527893066\n",
      "Epoch 6700/30000 Validation Loss: 0.04622332751750946\n",
      "Epoch 6701/30000 Training Loss: 0.05361955240368843\n",
      "Epoch 6702/30000 Training Loss: 0.05558154731988907\n",
      "Epoch 6703/30000 Training Loss: 0.04958956316113472\n",
      "Epoch 6704/30000 Training Loss: 0.062209177762269974\n",
      "Epoch 6705/30000 Training Loss: 0.053088147193193436\n",
      "Epoch 6706/30000 Training Loss: 0.053352873772382736\n",
      "Epoch 6707/30000 Training Loss: 0.04849282279610634\n",
      "Epoch 6708/30000 Training Loss: 0.05754902958869934\n",
      "Epoch 6709/30000 Training Loss: 0.03904653713107109\n",
      "Epoch 6710/30000 Training Loss: 0.053340472280979156\n",
      "Epoch 6711/30000 Training Loss: 0.05427877977490425\n",
      "Epoch 6712/30000 Training Loss: 0.05099576711654663\n",
      "Epoch 6713/30000 Training Loss: 0.05710064247250557\n",
      "Epoch 6714/30000 Training Loss: 0.060493260622024536\n",
      "Epoch 6715/30000 Training Loss: 0.04833366721868515\n",
      "Epoch 6716/30000 Training Loss: 0.05118035152554512\n",
      "Epoch 6717/30000 Training Loss: 0.05645211413502693\n",
      "Epoch 6718/30000 Training Loss: 0.045740868896245956\n",
      "Epoch 6719/30000 Training Loss: 0.05214967578649521\n",
      "Epoch 6720/30000 Training Loss: 0.055253494530916214\n",
      "Epoch 6721/30000 Training Loss: 0.05436348170042038\n",
      "Epoch 6722/30000 Training Loss: 0.046681493520736694\n",
      "Epoch 6723/30000 Training Loss: 0.05077195167541504\n",
      "Epoch 6724/30000 Training Loss: 0.04467923194169998\n",
      "Epoch 6725/30000 Training Loss: 0.04857008904218674\n",
      "Epoch 6726/30000 Training Loss: 0.05923383682966232\n",
      "Epoch 6727/30000 Training Loss: 0.04941701516509056\n",
      "Epoch 6728/30000 Training Loss: 0.043884702026844025\n",
      "Epoch 6729/30000 Training Loss: 0.05297182872891426\n",
      "Epoch 6730/30000 Training Loss: 0.0536605603992939\n",
      "Epoch 6731/30000 Training Loss: 0.04475272446870804\n",
      "Epoch 6732/30000 Training Loss: 0.044587381184101105\n",
      "Epoch 6733/30000 Training Loss: 0.05500306934118271\n",
      "Epoch 6734/30000 Training Loss: 0.04765317589044571\n",
      "Epoch 6735/30000 Training Loss: 0.05008655786514282\n",
      "Epoch 6736/30000 Training Loss: 0.05116172879934311\n",
      "Epoch 6737/30000 Training Loss: 0.0598803386092186\n",
      "Epoch 6738/30000 Training Loss: 0.04441972076892853\n",
      "Epoch 6739/30000 Training Loss: 0.05039753392338753\n",
      "Epoch 6740/30000 Training Loss: 0.04804703965783119\n",
      "Epoch 6741/30000 Training Loss: 0.04017620533704758\n",
      "Epoch 6742/30000 Training Loss: 0.05506202578544617\n",
      "Epoch 6743/30000 Training Loss: 0.0524987168610096\n",
      "Epoch 6744/30000 Training Loss: 0.05389600992202759\n",
      "Epoch 6745/30000 Training Loss: 0.05213490128517151\n",
      "Epoch 6746/30000 Training Loss: 0.049992043524980545\n",
      "Epoch 6747/30000 Training Loss: 0.044786710292100906\n",
      "Epoch 6748/30000 Training Loss: 0.048911117017269135\n",
      "Epoch 6749/30000 Training Loss: 0.045894600450992584\n",
      "Epoch 6750/30000 Training Loss: 0.0631660595536232\n",
      "Epoch 6750/30000 Validation Loss: 0.050342101603746414\n",
      "Epoch 6751/30000 Training Loss: 0.04504463076591492\n",
      "Epoch 6752/30000 Training Loss: 0.048394352197647095\n",
      "Epoch 6753/30000 Training Loss: 0.051259081810712814\n",
      "Epoch 6754/30000 Training Loss: 0.043684132397174835\n",
      "Epoch 6755/30000 Training Loss: 0.04779147356748581\n",
      "Epoch 6756/30000 Training Loss: 0.04614904895424843\n",
      "Epoch 6757/30000 Training Loss: 0.05568239837884903\n",
      "Epoch 6758/30000 Training Loss: 0.052575044333934784\n",
      "Epoch 6759/30000 Training Loss: 0.049370329827070236\n",
      "Epoch 6760/30000 Training Loss: 0.046782661229372025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6761/30000 Training Loss: 0.05652569606900215\n",
      "Epoch 6762/30000 Training Loss: 0.04769231751561165\n",
      "Epoch 6763/30000 Training Loss: 0.045308440923690796\n",
      "Epoch 6764/30000 Training Loss: 0.045734044164419174\n",
      "Epoch 6765/30000 Training Loss: 0.05228152871131897\n",
      "Epoch 6766/30000 Training Loss: 0.047588102519512177\n",
      "Epoch 6767/30000 Training Loss: 0.05128145217895508\n",
      "Epoch 6768/30000 Training Loss: 0.045260921120643616\n",
      "Epoch 6769/30000 Training Loss: 0.0461839959025383\n",
      "Epoch 6770/30000 Training Loss: 0.054762087762355804\n",
      "Epoch 6771/30000 Training Loss: 0.05392298847436905\n",
      "Epoch 6772/30000 Training Loss: 0.04689904302358627\n",
      "Epoch 6773/30000 Training Loss: 0.044620562344789505\n",
      "Epoch 6774/30000 Training Loss: 0.047699540853500366\n",
      "Epoch 6775/30000 Training Loss: 0.05984196066856384\n",
      "Epoch 6776/30000 Training Loss: 0.05803954601287842\n",
      "Epoch 6777/30000 Training Loss: 0.055785052478313446\n",
      "Epoch 6778/30000 Training Loss: 0.057340752333402634\n",
      "Epoch 6779/30000 Training Loss: 0.04889693111181259\n",
      "Epoch 6780/30000 Training Loss: 0.04247809201478958\n",
      "Epoch 6781/30000 Training Loss: 0.051793090999126434\n",
      "Epoch 6782/30000 Training Loss: 0.05049774795770645\n",
      "Epoch 6783/30000 Training Loss: 0.044072844088077545\n",
      "Epoch 6784/30000 Training Loss: 0.05544402077794075\n",
      "Epoch 6785/30000 Training Loss: 0.059885699301958084\n",
      "Epoch 6786/30000 Training Loss: 0.049735166132450104\n",
      "Epoch 6787/30000 Training Loss: 0.04598460718989372\n",
      "Epoch 6788/30000 Training Loss: 0.04953129589557648\n",
      "Epoch 6789/30000 Training Loss: 0.05178635194897652\n",
      "Epoch 6790/30000 Training Loss: 0.06660767644643784\n",
      "Epoch 6791/30000 Training Loss: 0.042903218418359756\n",
      "Epoch 6792/30000 Training Loss: 0.04838171228766441\n",
      "Epoch 6793/30000 Training Loss: 0.05564618110656738\n",
      "Epoch 6794/30000 Training Loss: 0.048204801976680756\n",
      "Epoch 6795/30000 Training Loss: 0.04845404252409935\n",
      "Epoch 6796/30000 Training Loss: 0.044806092977523804\n",
      "Epoch 6797/30000 Training Loss: 0.04656993970274925\n",
      "Epoch 6798/30000 Training Loss: 0.05516435578465462\n",
      "Epoch 6799/30000 Training Loss: 0.04270327091217041\n",
      "Epoch 6800/30000 Training Loss: 0.04905153065919876\n",
      "Epoch 6800/30000 Validation Loss: 0.04233443737030029\n",
      "Epoch 6801/30000 Training Loss: 0.049097221344709396\n",
      "Epoch 6802/30000 Training Loss: 0.05133325979113579\n",
      "Epoch 6803/30000 Training Loss: 0.06021568924188614\n",
      "Epoch 6804/30000 Training Loss: 0.052355892956256866\n",
      "Epoch 6805/30000 Training Loss: 0.057618141174316406\n",
      "Epoch 6806/30000 Training Loss: 0.05363228917121887\n",
      "Epoch 6807/30000 Training Loss: 0.05237863212823868\n",
      "Epoch 6808/30000 Training Loss: 0.046483855694532394\n",
      "Epoch 6809/30000 Training Loss: 0.04941803961992264\n",
      "Epoch 6810/30000 Training Loss: 0.057179782539606094\n",
      "Epoch 6811/30000 Training Loss: 0.04436090588569641\n",
      "Epoch 6812/30000 Training Loss: 0.05570216104388237\n",
      "Epoch 6813/30000 Training Loss: 0.050399355590343475\n",
      "Epoch 6814/30000 Training Loss: 0.04219820722937584\n",
      "Epoch 6815/30000 Training Loss: 0.05950988456606865\n",
      "Epoch 6816/30000 Training Loss: 0.04409759119153023\n",
      "Epoch 6817/30000 Training Loss: 0.0465078167617321\n",
      "Epoch 6818/30000 Training Loss: 0.058731891214847565\n",
      "Epoch 6819/30000 Training Loss: 0.053363095968961716\n",
      "Epoch 6820/30000 Training Loss: 0.055851660668849945\n",
      "Epoch 6821/30000 Training Loss: 0.03936218470335007\n",
      "Epoch 6822/30000 Training Loss: 0.06010280176997185\n",
      "Epoch 6823/30000 Training Loss: 0.05327901989221573\n",
      "Epoch 6824/30000 Training Loss: 0.04686688259243965\n",
      "Epoch 6825/30000 Training Loss: 0.04854821041226387\n",
      "Epoch 6826/30000 Training Loss: 0.052033860236406326\n",
      "Epoch 6827/30000 Training Loss: 0.0509466826915741\n",
      "Epoch 6828/30000 Training Loss: 0.06582116335630417\n",
      "Epoch 6829/30000 Training Loss: 0.04637705162167549\n",
      "Epoch 6830/30000 Training Loss: 0.05310513824224472\n",
      "Epoch 6831/30000 Training Loss: 0.04542757198214531\n",
      "Epoch 6832/30000 Training Loss: 0.04701584205031395\n",
      "Epoch 6833/30000 Training Loss: 0.04874320700764656\n",
      "Epoch 6834/30000 Training Loss: 0.05048288777470589\n",
      "Epoch 6835/30000 Training Loss: 0.05251546576619148\n",
      "Epoch 6836/30000 Training Loss: 0.046861566603183746\n",
      "Epoch 6837/30000 Training Loss: 0.04415922984480858\n",
      "Epoch 6838/30000 Training Loss: 0.046916622668504715\n",
      "Epoch 6839/30000 Training Loss: 0.04660245031118393\n",
      "Epoch 6840/30000 Training Loss: 0.053384363651275635\n",
      "Epoch 6841/30000 Training Loss: 0.049884576350450516\n",
      "Epoch 6842/30000 Training Loss: 0.04866257682442665\n",
      "Epoch 6843/30000 Training Loss: 0.05536508560180664\n",
      "Epoch 6844/30000 Training Loss: 0.04335755854845047\n",
      "Epoch 6845/30000 Training Loss: 0.04534895345568657\n",
      "Epoch 6846/30000 Training Loss: 0.04900901019573212\n",
      "Epoch 6847/30000 Training Loss: 0.052615463733673096\n",
      "Epoch 6848/30000 Training Loss: 0.04772568494081497\n",
      "Epoch 6849/30000 Training Loss: 0.04205648601055145\n",
      "Epoch 6850/30000 Training Loss: 0.04938044399023056\n",
      "Epoch 6850/30000 Validation Loss: 0.050699204206466675\n",
      "Epoch 6851/30000 Training Loss: 0.05059652402997017\n",
      "Epoch 6852/30000 Training Loss: 0.05489453673362732\n",
      "Epoch 6853/30000 Training Loss: 0.05084238573908806\n",
      "Epoch 6854/30000 Training Loss: 0.04874919727444649\n",
      "Epoch 6855/30000 Training Loss: 0.05011902377009392\n",
      "Epoch 6856/30000 Training Loss: 0.05054701119661331\n",
      "Epoch 6857/30000 Training Loss: 0.051571350544691086\n",
      "Epoch 6858/30000 Training Loss: 0.05128093808889389\n",
      "Epoch 6859/30000 Training Loss: 0.05218508094549179\n",
      "Epoch 6860/30000 Training Loss: 0.052152227610349655\n",
      "Epoch 6861/30000 Training Loss: 0.04613322764635086\n",
      "Epoch 6862/30000 Training Loss: 0.043555475771427155\n",
      "Epoch 6863/30000 Training Loss: 0.05458555743098259\n",
      "Epoch 6864/30000 Training Loss: 0.05673389881849289\n",
      "Epoch 6865/30000 Training Loss: 0.05883512645959854\n",
      "Epoch 6866/30000 Training Loss: 0.04649648815393448\n",
      "Epoch 6867/30000 Training Loss: 0.05159524828195572\n",
      "Epoch 6868/30000 Training Loss: 0.046553146094083786\n",
      "Epoch 6869/30000 Training Loss: 0.05601286143064499\n",
      "Epoch 6870/30000 Training Loss: 0.05303456261754036\n",
      "Epoch 6871/30000 Training Loss: 0.052973102778196335\n",
      "Epoch 6872/30000 Training Loss: 0.04597771167755127\n",
      "Epoch 6873/30000 Training Loss: 0.04930909350514412\n",
      "Epoch 6874/30000 Training Loss: 0.05042542889714241\n",
      "Epoch 6875/30000 Training Loss: 0.052253663539886475\n",
      "Epoch 6876/30000 Training Loss: 0.04908877611160278\n",
      "Epoch 6877/30000 Training Loss: 0.0485176220536232\n",
      "Epoch 6878/30000 Training Loss: 0.04562356323003769\n",
      "Epoch 6879/30000 Training Loss: 0.048903677612543106\n",
      "Epoch 6880/30000 Training Loss: 0.04952957108616829\n",
      "Epoch 6881/30000 Training Loss: 0.055376023054122925\n",
      "Epoch 6882/30000 Training Loss: 0.06374134868383408\n",
      "Epoch 6883/30000 Training Loss: 0.051775455474853516\n",
      "Epoch 6884/30000 Training Loss: 0.046531978994607925\n",
      "Epoch 6885/30000 Training Loss: 0.0511387400329113\n",
      "Epoch 6886/30000 Training Loss: 0.05708624795079231\n",
      "Epoch 6887/30000 Training Loss: 0.05458784103393555\n",
      "Epoch 6888/30000 Training Loss: 0.05003497004508972\n",
      "Epoch 6889/30000 Training Loss: 0.059491515159606934\n",
      "Epoch 6890/30000 Training Loss: 0.04591267183423042\n",
      "Epoch 6891/30000 Training Loss: 0.053058914840221405\n",
      "Epoch 6892/30000 Training Loss: 0.05857498198747635\n",
      "Epoch 6893/30000 Training Loss: 0.044619448482990265\n",
      "Epoch 6894/30000 Training Loss: 0.05655025318264961\n",
      "Epoch 6895/30000 Training Loss: 0.0448961965739727\n",
      "Epoch 6896/30000 Training Loss: 0.057023048400878906\n",
      "Epoch 6897/30000 Training Loss: 0.05227204039692879\n",
      "Epoch 6898/30000 Training Loss: 0.04974951967597008\n",
      "Epoch 6899/30000 Training Loss: 0.0517626516520977\n",
      "Epoch 6900/30000 Training Loss: 0.05705535411834717\n",
      "Epoch 6900/30000 Validation Loss: 0.05876309797167778\n",
      "Epoch 6901/30000 Training Loss: 0.04640588536858559\n",
      "Epoch 6902/30000 Training Loss: 0.049293793737888336\n",
      "Epoch 6903/30000 Training Loss: 0.048924628645181656\n",
      "Epoch 6904/30000 Training Loss: 0.049293290823698044\n",
      "Epoch 6905/30000 Training Loss: 0.043694254010915756\n",
      "Epoch 6906/30000 Training Loss: 0.04123027250170708\n",
      "Epoch 6907/30000 Training Loss: 0.045055173337459564\n",
      "Epoch 6908/30000 Training Loss: 0.04957974702119827\n",
      "Epoch 6909/30000 Training Loss: 0.0450705885887146\n",
      "Epoch 6910/30000 Training Loss: 0.049228355288505554\n",
      "Epoch 6911/30000 Training Loss: 0.050221145153045654\n",
      "Epoch 6912/30000 Training Loss: 0.05447732284665108\n",
      "Epoch 6913/30000 Training Loss: 0.0481107160449028\n",
      "Epoch 6914/30000 Training Loss: 0.04715738445520401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6915/30000 Training Loss: 0.05511981248855591\n",
      "Epoch 6916/30000 Training Loss: 0.049139853566884995\n",
      "Epoch 6917/30000 Training Loss: 0.05483965948224068\n",
      "Epoch 6918/30000 Training Loss: 0.05733056738972664\n",
      "Epoch 6919/30000 Training Loss: 0.0664248988032341\n",
      "Epoch 6920/30000 Training Loss: 0.05940685793757439\n",
      "Epoch 6921/30000 Training Loss: 0.04953104257583618\n",
      "Epoch 6922/30000 Training Loss: 0.05359501764178276\n",
      "Epoch 6923/30000 Training Loss: 0.06757916510105133\n",
      "Epoch 6924/30000 Training Loss: 0.06434066593647003\n",
      "Epoch 6925/30000 Training Loss: 0.05543025583028793\n",
      "Epoch 6926/30000 Training Loss: 0.04781697317957878\n",
      "Epoch 6927/30000 Training Loss: 0.043521322309970856\n",
      "Epoch 6928/30000 Training Loss: 0.04575140401721001\n",
      "Epoch 6929/30000 Training Loss: 0.05715377256274223\n",
      "Epoch 6930/30000 Training Loss: 0.04714147746562958\n",
      "Epoch 6931/30000 Training Loss: 0.0510847344994545\n",
      "Epoch 6932/30000 Training Loss: 0.058236263692379\n",
      "Epoch 6933/30000 Training Loss: 0.05541924387216568\n",
      "Epoch 6934/30000 Training Loss: 0.045889366418123245\n",
      "Epoch 6935/30000 Training Loss: 0.048280976712703705\n",
      "Epoch 6936/30000 Training Loss: 0.04406748339533806\n",
      "Epoch 6937/30000 Training Loss: 0.0498768612742424\n",
      "Epoch 6938/30000 Training Loss: 0.05050723999738693\n",
      "Epoch 6939/30000 Training Loss: 0.05539489910006523\n",
      "Epoch 6940/30000 Training Loss: 0.05462496355175972\n",
      "Epoch 6941/30000 Training Loss: 0.0484393946826458\n",
      "Epoch 6942/30000 Training Loss: 0.0555163137614727\n",
      "Epoch 6943/30000 Training Loss: 0.04790022224187851\n",
      "Epoch 6944/30000 Training Loss: 0.05105139687657356\n",
      "Epoch 6945/30000 Training Loss: 0.0576222762465477\n",
      "Epoch 6946/30000 Training Loss: 0.04757606238126755\n",
      "Epoch 6947/30000 Training Loss: 0.05135933309793472\n",
      "Epoch 6948/30000 Training Loss: 0.0454590730369091\n",
      "Epoch 6949/30000 Training Loss: 0.04637607932090759\n",
      "Epoch 6950/30000 Training Loss: 0.055243782699108124\n",
      "Epoch 6950/30000 Validation Loss: 0.059830158948898315\n",
      "Epoch 6951/30000 Training Loss: 0.05992591381072998\n",
      "Epoch 6952/30000 Training Loss: 0.045819737017154694\n",
      "Epoch 6953/30000 Training Loss: 0.057002000510692596\n",
      "Epoch 6954/30000 Training Loss: 0.04815268889069557\n",
      "Epoch 6955/30000 Training Loss: 0.05169147253036499\n",
      "Epoch 6956/30000 Training Loss: 0.04342867434024811\n",
      "Epoch 6957/30000 Training Loss: 0.044356536120176315\n",
      "Epoch 6958/30000 Training Loss: 0.05186960846185684\n",
      "Epoch 6959/30000 Training Loss: 0.053774356842041016\n",
      "Epoch 6960/30000 Training Loss: 0.053846459835767746\n",
      "Epoch 6961/30000 Training Loss: 0.05260629579424858\n",
      "Epoch 6962/30000 Training Loss: 0.044753920286893845\n",
      "Epoch 6963/30000 Training Loss: 0.05810261890292168\n",
      "Epoch 6964/30000 Training Loss: 0.044086623936891556\n",
      "Epoch 6965/30000 Training Loss: 0.044791679829359055\n",
      "Epoch 6966/30000 Training Loss: 0.05046696588397026\n",
      "Epoch 6967/30000 Training Loss: 0.05327106639742851\n",
      "Epoch 6968/30000 Training Loss: 0.05318024009466171\n",
      "Epoch 6969/30000 Training Loss: 0.04799376428127289\n",
      "Epoch 6970/30000 Training Loss: 0.05224734544754028\n",
      "Epoch 6971/30000 Training Loss: 0.05470321699976921\n",
      "Epoch 6972/30000 Training Loss: 0.0435483492910862\n",
      "Epoch 6973/30000 Training Loss: 0.06046348810195923\n",
      "Epoch 6974/30000 Training Loss: 0.06543191522359848\n",
      "Epoch 6975/30000 Training Loss: 0.043423257768154144\n",
      "Epoch 6976/30000 Training Loss: 0.040114447474479675\n",
      "Epoch 6977/30000 Training Loss: 0.05220139026641846\n",
      "Epoch 6978/30000 Training Loss: 0.05218841880559921\n",
      "Epoch 6979/30000 Training Loss: 0.05328483134508133\n",
      "Epoch 6980/30000 Training Loss: 0.04915360361337662\n",
      "Epoch 6981/30000 Training Loss: 0.04472839832305908\n",
      "Epoch 6982/30000 Training Loss: 0.04911722242832184\n",
      "Epoch 6983/30000 Training Loss: 0.05527133494615555\n",
      "Epoch 6984/30000 Training Loss: 0.05525011941790581\n",
      "Epoch 6985/30000 Training Loss: 0.05370277166366577\n",
      "Epoch 6986/30000 Training Loss: 0.04751911014318466\n",
      "Epoch 6987/30000 Training Loss: 0.04676661267876625\n",
      "Epoch 6988/30000 Training Loss: 0.03610459342598915\n",
      "Epoch 6989/30000 Training Loss: 0.05284999683499336\n",
      "Epoch 6990/30000 Training Loss: 0.050739746540784836\n",
      "Epoch 6991/30000 Training Loss: 0.05046965926885605\n",
      "Epoch 6992/30000 Training Loss: 0.04824947938323021\n",
      "Epoch 6993/30000 Training Loss: 0.04815267398953438\n",
      "Epoch 6994/30000 Training Loss: 0.043171994388103485\n",
      "Epoch 6995/30000 Training Loss: 0.0476592592895031\n",
      "Epoch 6996/30000 Training Loss: 0.047692351043224335\n",
      "Epoch 6997/30000 Training Loss: 0.05092182010412216\n",
      "Epoch 6998/30000 Training Loss: 0.05021259933710098\n",
      "Epoch 6999/30000 Training Loss: 0.062454067170619965\n",
      "Epoch 7000/30000 Training Loss: 0.05043575167655945\n",
      "Epoch 7000/30000 Validation Loss: 0.05093567445874214\n",
      "Epoch 7001/30000 Training Loss: 0.04666147381067276\n",
      "Epoch 7002/30000 Training Loss: 0.04158303886651993\n",
      "Epoch 7003/30000 Training Loss: 0.058144330978393555\n",
      "Epoch 7004/30000 Training Loss: 0.0561382882297039\n",
      "Epoch 7005/30000 Training Loss: 0.05548746511340141\n",
      "Epoch 7006/30000 Training Loss: 0.04919327050447464\n",
      "Epoch 7007/30000 Training Loss: 0.05792611837387085\n",
      "Epoch 7008/30000 Training Loss: 0.04968785122036934\n",
      "Epoch 7009/30000 Training Loss: 0.05491026118397713\n",
      "Epoch 7010/30000 Training Loss: 0.05145775154232979\n",
      "Epoch 7011/30000 Training Loss: 0.043775953352451324\n",
      "Epoch 7012/30000 Training Loss: 0.03714791685342789\n",
      "Epoch 7013/30000 Training Loss: 0.044773928821086884\n",
      "Epoch 7014/30000 Training Loss: 0.051367782056331635\n",
      "Epoch 7015/30000 Training Loss: 0.04463572800159454\n",
      "Epoch 7016/30000 Training Loss: 0.05751122161746025\n",
      "Epoch 7017/30000 Training Loss: 0.057182420045137405\n",
      "Epoch 7018/30000 Training Loss: 0.04098526015877724\n",
      "Epoch 7019/30000 Training Loss: 0.04955704137682915\n",
      "Epoch 7020/30000 Training Loss: 0.045852288603782654\n",
      "Epoch 7021/30000 Training Loss: 0.058769963681697845\n",
      "Epoch 7022/30000 Training Loss: 0.05263025313615799\n",
      "Epoch 7023/30000 Training Loss: 0.04377196356654167\n",
      "Epoch 7024/30000 Training Loss: 0.04702604189515114\n",
      "Epoch 7025/30000 Training Loss: 0.04577331617474556\n",
      "Epoch 7026/30000 Training Loss: 0.05948609113693237\n",
      "Epoch 7027/30000 Training Loss: 0.043266553431749344\n",
      "Epoch 7028/30000 Training Loss: 0.06187649816274643\n",
      "Epoch 7029/30000 Training Loss: 0.046213336288928986\n",
      "Epoch 7030/30000 Training Loss: 0.049777574837207794\n",
      "Epoch 7031/30000 Training Loss: 0.06096426397562027\n",
      "Epoch 7032/30000 Training Loss: 0.05589483305811882\n",
      "Epoch 7033/30000 Training Loss: 0.04961652308702469\n",
      "Epoch 7034/30000 Training Loss: 0.042510442435741425\n",
      "Epoch 7035/30000 Training Loss: 0.04735049605369568\n",
      "Epoch 7036/30000 Training Loss: 0.04379057139158249\n",
      "Epoch 7037/30000 Training Loss: 0.04884549230337143\n",
      "Epoch 7038/30000 Training Loss: 0.05123335123062134\n",
      "Epoch 7039/30000 Training Loss: 0.05557427927851677\n",
      "Epoch 7040/30000 Training Loss: 0.052248913794755936\n",
      "Epoch 7041/30000 Training Loss: 0.048670701682567596\n",
      "Epoch 7042/30000 Training Loss: 0.04683496057987213\n",
      "Epoch 7043/30000 Training Loss: 0.05182193964719772\n",
      "Epoch 7044/30000 Training Loss: 0.048044852912425995\n",
      "Epoch 7045/30000 Training Loss: 0.04974643141031265\n",
      "Epoch 7046/30000 Training Loss: 0.0477447472512722\n",
      "Epoch 7047/30000 Training Loss: 0.05102487653493881\n",
      "Epoch 7048/30000 Training Loss: 0.045644957572221756\n",
      "Epoch 7049/30000 Training Loss: 0.047056201845407486\n",
      "Epoch 7050/30000 Training Loss: 0.049686774611473083\n",
      "Epoch 7050/30000 Validation Loss: 0.051453787833452225\n",
      "Epoch 7051/30000 Training Loss: 0.05217248946428299\n",
      "Epoch 7052/30000 Training Loss: 0.05654583126306534\n",
      "Epoch 7053/30000 Training Loss: 0.04993322119116783\n",
      "Epoch 7054/30000 Training Loss: 0.05662144348025322\n",
      "Epoch 7055/30000 Training Loss: 0.051232170313596725\n",
      "Epoch 7056/30000 Training Loss: 0.0509115569293499\n",
      "Epoch 7057/30000 Training Loss: 0.05247364565730095\n",
      "Epoch 7058/30000 Training Loss: 0.05126072093844414\n",
      "Epoch 7059/30000 Training Loss: 0.04883481189608574\n",
      "Epoch 7060/30000 Training Loss: 0.04990621656179428\n",
      "Epoch 7061/30000 Training Loss: 0.048450130969285965\n",
      "Epoch 7062/30000 Training Loss: 0.05117632821202278\n",
      "Epoch 7063/30000 Training Loss: 0.045003555715084076\n",
      "Epoch 7064/30000 Training Loss: 0.04646748676896095\n",
      "Epoch 7065/30000 Training Loss: 0.05364661663770676\n",
      "Epoch 7066/30000 Training Loss: 0.046146828681230545\n",
      "Epoch 7067/30000 Training Loss: 0.04393824189901352\n",
      "Epoch 7068/30000 Training Loss: 0.04423755034804344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7069/30000 Training Loss: 0.04801885783672333\n",
      "Epoch 7070/30000 Training Loss: 0.05221899598836899\n",
      "Epoch 7071/30000 Training Loss: 0.048214610666036606\n",
      "Epoch 7072/30000 Training Loss: 0.04672738164663315\n",
      "Epoch 7073/30000 Training Loss: 0.04678865894675255\n",
      "Epoch 7074/30000 Training Loss: 0.05404219031333923\n",
      "Epoch 7075/30000 Training Loss: 0.048818569630384445\n",
      "Epoch 7076/30000 Training Loss: 0.03976403921842575\n",
      "Epoch 7077/30000 Training Loss: 0.04709116369485855\n",
      "Epoch 7078/30000 Training Loss: 0.053074877709150314\n",
      "Epoch 7079/30000 Training Loss: 0.051372118294239044\n",
      "Epoch 7080/30000 Training Loss: 0.0485166534781456\n",
      "Epoch 7081/30000 Training Loss: 0.041670169681310654\n",
      "Epoch 7082/30000 Training Loss: 0.04669640585780144\n",
      "Epoch 7083/30000 Training Loss: 0.05136681720614433\n",
      "Epoch 7084/30000 Training Loss: 0.05069262906908989\n",
      "Epoch 7085/30000 Training Loss: 0.054069340229034424\n",
      "Epoch 7086/30000 Training Loss: 0.04781987518072128\n",
      "Epoch 7087/30000 Training Loss: 0.053986888378858566\n",
      "Epoch 7088/30000 Training Loss: 0.04623536393046379\n",
      "Epoch 7089/30000 Training Loss: 0.04363330826163292\n",
      "Epoch 7090/30000 Training Loss: 0.05154715105891228\n",
      "Epoch 7091/30000 Training Loss: 0.043978601694107056\n",
      "Epoch 7092/30000 Training Loss: 0.050925157964229584\n",
      "Epoch 7093/30000 Training Loss: 0.043861038982868195\n",
      "Epoch 7094/30000 Training Loss: 0.05428677052259445\n",
      "Epoch 7095/30000 Training Loss: 0.04628319293260574\n",
      "Epoch 7096/30000 Training Loss: 0.05108656361699104\n",
      "Epoch 7097/30000 Training Loss: 0.04660259187221527\n",
      "Epoch 7098/30000 Training Loss: 0.04599732905626297\n",
      "Epoch 7099/30000 Training Loss: 0.043080832809209824\n",
      "Epoch 7100/30000 Training Loss: 0.053155720233917236\n",
      "Epoch 7100/30000 Validation Loss: 0.04758191853761673\n",
      "Epoch 7101/30000 Training Loss: 0.05717112869024277\n",
      "Epoch 7102/30000 Training Loss: 0.06154998391866684\n",
      "Epoch 7103/30000 Training Loss: 0.03996570408344269\n",
      "Epoch 7104/30000 Training Loss: 0.041775353252887726\n",
      "Epoch 7105/30000 Training Loss: 0.05206199735403061\n",
      "Epoch 7106/30000 Training Loss: 0.055398326367139816\n",
      "Epoch 7107/30000 Training Loss: 0.04873104766011238\n",
      "Epoch 7108/30000 Training Loss: 0.05171092599630356\n",
      "Epoch 7109/30000 Training Loss: 0.045071616768836975\n",
      "Epoch 7110/30000 Training Loss: 0.053672753274440765\n",
      "Epoch 7111/30000 Training Loss: 0.04428628832101822\n",
      "Epoch 7112/30000 Training Loss: 0.05048836022615433\n",
      "Epoch 7113/30000 Training Loss: 0.06108952686190605\n",
      "Epoch 7114/30000 Training Loss: 0.04877251386642456\n",
      "Epoch 7115/30000 Training Loss: 0.045826368033885956\n",
      "Epoch 7116/30000 Training Loss: 0.052110861986875534\n",
      "Epoch 7117/30000 Training Loss: 0.04667012766003609\n",
      "Epoch 7118/30000 Training Loss: 0.04875384271144867\n",
      "Epoch 7119/30000 Training Loss: 0.05187442898750305\n",
      "Epoch 7120/30000 Training Loss: 0.04495643451809883\n",
      "Epoch 7121/30000 Training Loss: 0.04883449524641037\n",
      "Epoch 7122/30000 Training Loss: 0.04351816698908806\n",
      "Epoch 7123/30000 Training Loss: 0.056355226784944534\n",
      "Epoch 7124/30000 Training Loss: 0.042466409504413605\n",
      "Epoch 7125/30000 Training Loss: 0.050025396049022675\n",
      "Epoch 7126/30000 Training Loss: 0.04809422045946121\n",
      "Epoch 7127/30000 Training Loss: 0.05592834949493408\n",
      "Epoch 7128/30000 Training Loss: 0.059187233448028564\n",
      "Epoch 7129/30000 Training Loss: 0.0530017614364624\n",
      "Epoch 7130/30000 Training Loss: 0.06381725519895554\n",
      "Epoch 7131/30000 Training Loss: 0.05291518568992615\n",
      "Epoch 7132/30000 Training Loss: 0.04861903190612793\n",
      "Epoch 7133/30000 Training Loss: 0.05465453863143921\n",
      "Epoch 7134/30000 Training Loss: 0.05136411637067795\n",
      "Epoch 7135/30000 Training Loss: 0.051440976560115814\n",
      "Epoch 7136/30000 Training Loss: 0.060886286199092865\n",
      "Epoch 7137/30000 Training Loss: 0.05034281685948372\n",
      "Epoch 7138/30000 Training Loss: 0.05546970292925835\n",
      "Epoch 7139/30000 Training Loss: 0.04499969631433487\n",
      "Epoch 7140/30000 Training Loss: 0.053618259727954865\n",
      "Epoch 7141/30000 Training Loss: 0.05619560554623604\n",
      "Epoch 7142/30000 Training Loss: 0.050804197788238525\n",
      "Epoch 7143/30000 Training Loss: 0.04821276292204857\n",
      "Epoch 7144/30000 Training Loss: 0.0526638999581337\n",
      "Epoch 7145/30000 Training Loss: 0.05171537399291992\n",
      "Epoch 7146/30000 Training Loss: 0.05306132510304451\n",
      "Epoch 7147/30000 Training Loss: 0.04942655935883522\n",
      "Epoch 7148/30000 Training Loss: 0.06818840652704239\n",
      "Epoch 7149/30000 Training Loss: 0.04667007923126221\n",
      "Epoch 7150/30000 Training Loss: 0.04718747362494469\n",
      "Epoch 7150/30000 Validation Loss: 0.0561160072684288\n",
      "Epoch 7151/30000 Training Loss: 0.05000435560941696\n",
      "Epoch 7152/30000 Training Loss: 0.05089180916547775\n",
      "Epoch 7153/30000 Training Loss: 0.04773461073637009\n",
      "Epoch 7154/30000 Training Loss: 0.052191369235515594\n",
      "Epoch 7155/30000 Training Loss: 0.043178193271160126\n",
      "Epoch 7156/30000 Training Loss: 0.05055021122097969\n",
      "Epoch 7157/30000 Training Loss: 0.050678420811891556\n",
      "Epoch 7158/30000 Training Loss: 0.04584839195013046\n",
      "Epoch 7159/30000 Training Loss: 0.05021394416689873\n",
      "Epoch 7160/30000 Training Loss: 0.04235748201608658\n",
      "Epoch 7161/30000 Training Loss: 0.04868317395448685\n",
      "Epoch 7162/30000 Training Loss: 0.04925670102238655\n",
      "Epoch 7163/30000 Training Loss: 0.0571034736931324\n",
      "Epoch 7164/30000 Training Loss: 0.04748722165822983\n",
      "Epoch 7165/30000 Training Loss: 0.06366385519504547\n",
      "Epoch 7166/30000 Training Loss: 0.04484216123819351\n",
      "Epoch 7167/30000 Training Loss: 0.04760948568582535\n",
      "Epoch 7168/30000 Training Loss: 0.05126972869038582\n",
      "Epoch 7169/30000 Training Loss: 0.052060097455978394\n",
      "Epoch 7170/30000 Training Loss: 0.05810067802667618\n",
      "Epoch 7171/30000 Training Loss: 0.04613090306520462\n",
      "Epoch 7172/30000 Training Loss: 0.047152530401945114\n",
      "Epoch 7173/30000 Training Loss: 0.0448693186044693\n",
      "Epoch 7174/30000 Training Loss: 0.05114373564720154\n",
      "Epoch 7175/30000 Training Loss: 0.04832649976015091\n",
      "Epoch 7176/30000 Training Loss: 0.05382557958364487\n",
      "Epoch 7177/30000 Training Loss: 0.05845494940876961\n",
      "Epoch 7178/30000 Training Loss: 0.05411685258150101\n",
      "Epoch 7179/30000 Training Loss: 0.046712327748537064\n",
      "Epoch 7180/30000 Training Loss: 0.051109202206134796\n",
      "Epoch 7181/30000 Training Loss: 0.048922259360551834\n",
      "Epoch 7182/30000 Training Loss: 0.051208607852458954\n",
      "Epoch 7183/30000 Training Loss: 0.04944618046283722\n",
      "Epoch 7184/30000 Training Loss: 0.05019359663128853\n",
      "Epoch 7185/30000 Training Loss: 0.05996895581483841\n",
      "Epoch 7186/30000 Training Loss: 0.04851188510656357\n",
      "Epoch 7187/30000 Training Loss: 0.05141909793019295\n",
      "Epoch 7188/30000 Training Loss: 0.05736479163169861\n",
      "Epoch 7189/30000 Training Loss: 0.04785024747252464\n",
      "Epoch 7190/30000 Training Loss: 0.049346886575222015\n",
      "Epoch 7191/30000 Training Loss: 0.055012673139572144\n",
      "Epoch 7192/30000 Training Loss: 0.056086648255586624\n",
      "Epoch 7193/30000 Training Loss: 0.04577353969216347\n",
      "Epoch 7194/30000 Training Loss: 0.04812043532729149\n",
      "Epoch 7195/30000 Training Loss: 0.05254610627889633\n",
      "Epoch 7196/30000 Training Loss: 0.04535185173153877\n",
      "Epoch 7197/30000 Training Loss: 0.05057983472943306\n",
      "Epoch 7198/30000 Training Loss: 0.054438550025224686\n",
      "Epoch 7199/30000 Training Loss: 0.05252085253596306\n",
      "Epoch 7200/30000 Training Loss: 0.050579316914081573\n",
      "Epoch 7200/30000 Validation Loss: 0.05632376670837402\n",
      "Epoch 7201/30000 Training Loss: 0.052433859556913376\n",
      "Epoch 7202/30000 Training Loss: 0.039689987897872925\n",
      "Epoch 7203/30000 Training Loss: 0.042914506047964096\n",
      "Epoch 7204/30000 Training Loss: 0.05406692624092102\n",
      "Epoch 7205/30000 Training Loss: 0.06087455898523331\n",
      "Epoch 7206/30000 Training Loss: 0.04849163815379143\n",
      "Epoch 7207/30000 Training Loss: 0.04840346425771713\n",
      "Epoch 7208/30000 Training Loss: 0.041914261877536774\n",
      "Epoch 7209/30000 Training Loss: 0.05796930938959122\n",
      "Epoch 7210/30000 Training Loss: 0.05597341060638428\n",
      "Epoch 7211/30000 Training Loss: 0.041281215846538544\n",
      "Epoch 7212/30000 Training Loss: 0.0447261743247509\n",
      "Epoch 7213/30000 Training Loss: 0.0504937581717968\n",
      "Epoch 7214/30000 Training Loss: 0.04122444614768028\n",
      "Epoch 7215/30000 Training Loss: 0.052951615303754807\n",
      "Epoch 7216/30000 Training Loss: 0.04650544002652168\n",
      "Epoch 7217/30000 Training Loss: 0.05225087329745293\n",
      "Epoch 7218/30000 Training Loss: 0.04629725217819214\n",
      "Epoch 7219/30000 Training Loss: 0.0472148135304451\n",
      "Epoch 7220/30000 Training Loss: 0.05507821962237358\n",
      "Epoch 7221/30000 Training Loss: 0.05156487226486206\n",
      "Epoch 7222/30000 Training Loss: 0.04631882160902023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7223/30000 Training Loss: 0.04203462600708008\n",
      "Epoch 7224/30000 Training Loss: 0.04297155886888504\n",
      "Epoch 7225/30000 Training Loss: 0.049762267619371414\n",
      "Epoch 7226/30000 Training Loss: 0.05674652382731438\n",
      "Epoch 7227/30000 Training Loss: 0.05434788390994072\n",
      "Epoch 7228/30000 Training Loss: 0.055801521986722946\n",
      "Epoch 7229/30000 Training Loss: 0.05460057407617569\n",
      "Epoch 7230/30000 Training Loss: 0.05188693478703499\n",
      "Epoch 7231/30000 Training Loss: 0.04531196504831314\n",
      "Epoch 7232/30000 Training Loss: 0.05079547315835953\n",
      "Epoch 7233/30000 Training Loss: 0.04534483700990677\n",
      "Epoch 7234/30000 Training Loss: 0.05007766932249069\n",
      "Epoch 7235/30000 Training Loss: 0.04398851469159126\n",
      "Epoch 7236/30000 Training Loss: 0.06209496408700943\n",
      "Epoch 7237/30000 Training Loss: 0.04315667226910591\n",
      "Epoch 7238/30000 Training Loss: 0.04890621826052666\n",
      "Epoch 7239/30000 Training Loss: 0.04928470402956009\n",
      "Epoch 7240/30000 Training Loss: 0.054479993879795074\n",
      "Epoch 7241/30000 Training Loss: 0.048149943351745605\n",
      "Epoch 7242/30000 Training Loss: 0.05147717520594597\n",
      "Epoch 7243/30000 Training Loss: 0.05506201833486557\n",
      "Epoch 7244/30000 Training Loss: 0.045398272573947906\n",
      "Epoch 7245/30000 Training Loss: 0.05670912191271782\n",
      "Epoch 7246/30000 Training Loss: 0.042358409613370895\n",
      "Epoch 7247/30000 Training Loss: 0.05195195600390434\n",
      "Epoch 7248/30000 Training Loss: 0.04698953777551651\n",
      "Epoch 7249/30000 Training Loss: 0.046874500811100006\n",
      "Epoch 7250/30000 Training Loss: 0.05638927221298218\n",
      "Epoch 7250/30000 Validation Loss: 0.053232260048389435\n",
      "Epoch 7251/30000 Training Loss: 0.046407993882894516\n",
      "Epoch 7252/30000 Training Loss: 0.04893691465258598\n",
      "Epoch 7253/30000 Training Loss: 0.052235715091228485\n",
      "Epoch 7254/30000 Training Loss: 0.052658699452877045\n",
      "Epoch 7255/30000 Training Loss: 0.05266513675451279\n",
      "Epoch 7256/30000 Training Loss: 0.04575839638710022\n",
      "Epoch 7257/30000 Training Loss: 0.0540798120200634\n",
      "Epoch 7258/30000 Training Loss: 0.04811779782176018\n",
      "Epoch 7259/30000 Training Loss: 0.05710827186703682\n",
      "Epoch 7260/30000 Training Loss: 0.05156857892870903\n",
      "Epoch 7261/30000 Training Loss: 0.05609392002224922\n",
      "Epoch 7262/30000 Training Loss: 0.05255068466067314\n",
      "Epoch 7263/30000 Training Loss: 0.048715412616729736\n",
      "Epoch 7264/30000 Training Loss: 0.05853615328669548\n",
      "Epoch 7265/30000 Training Loss: 0.05478452518582344\n",
      "Epoch 7266/30000 Training Loss: 0.04203493148088455\n",
      "Epoch 7267/30000 Training Loss: 0.054329175502061844\n",
      "Epoch 7268/30000 Training Loss: 0.048108868300914764\n",
      "Epoch 7269/30000 Training Loss: 0.05415413901209831\n",
      "Epoch 7270/30000 Training Loss: 0.05304279923439026\n",
      "Epoch 7271/30000 Training Loss: 0.052287809550762177\n",
      "Epoch 7272/30000 Training Loss: 0.05582762509584427\n",
      "Epoch 7273/30000 Training Loss: 0.04488855227828026\n",
      "Epoch 7274/30000 Training Loss: 0.052635353058576584\n",
      "Epoch 7275/30000 Training Loss: 0.04330332577228546\n",
      "Epoch 7276/30000 Training Loss: 0.053266845643520355\n",
      "Epoch 7277/30000 Training Loss: 0.05806465819478035\n",
      "Epoch 7278/30000 Training Loss: 0.048707202076911926\n",
      "Epoch 7279/30000 Training Loss: 0.05032304674386978\n",
      "Epoch 7280/30000 Training Loss: 0.050079844892024994\n",
      "Epoch 7281/30000 Training Loss: 0.04298911243677139\n",
      "Epoch 7282/30000 Training Loss: 0.04788694530725479\n",
      "Epoch 7283/30000 Training Loss: 0.04779336601495743\n",
      "Epoch 7284/30000 Training Loss: 0.05156133323907852\n",
      "Epoch 7285/30000 Training Loss: 0.04945048689842224\n",
      "Epoch 7286/30000 Training Loss: 0.043327223509550095\n",
      "Epoch 7287/30000 Training Loss: 0.04598383232951164\n",
      "Epoch 7288/30000 Training Loss: 0.04643756523728371\n",
      "Epoch 7289/30000 Training Loss: 0.03908110409975052\n",
      "Epoch 7290/30000 Training Loss: 0.03875221684575081\n",
      "Epoch 7291/30000 Training Loss: 0.04867115244269371\n",
      "Epoch 7292/30000 Training Loss: 0.05165443569421768\n",
      "Epoch 7293/30000 Training Loss: 0.05552603676915169\n",
      "Epoch 7294/30000 Training Loss: 0.04848729446530342\n",
      "Epoch 7295/30000 Training Loss: 0.04112093895673752\n",
      "Epoch 7296/30000 Training Loss: 0.04981844872236252\n",
      "Epoch 7297/30000 Training Loss: 0.05590458959341049\n",
      "Epoch 7298/30000 Training Loss: 0.05195195600390434\n",
      "Epoch 7299/30000 Training Loss: 0.047988664358854294\n",
      "Epoch 7300/30000 Training Loss: 0.0490359291434288\n",
      "Epoch 7300/30000 Validation Loss: 0.04657776281237602\n",
      "Epoch 7301/30000 Training Loss: 0.05466756969690323\n",
      "Epoch 7302/30000 Training Loss: 0.04943590611219406\n",
      "Epoch 7303/30000 Training Loss: 0.05788673087954521\n",
      "Epoch 7304/30000 Training Loss: 0.04705963656306267\n",
      "Epoch 7305/30000 Training Loss: 0.04718657582998276\n",
      "Epoch 7306/30000 Training Loss: 0.04700212553143501\n",
      "Epoch 7307/30000 Training Loss: 0.04112114757299423\n",
      "Epoch 7308/30000 Training Loss: 0.044365741312503815\n",
      "Epoch 7309/30000 Training Loss: 0.04851417988538742\n",
      "Epoch 7310/30000 Training Loss: 0.045937277376651764\n",
      "Epoch 7311/30000 Training Loss: 0.0475248321890831\n",
      "Epoch 7312/30000 Training Loss: 0.03966571018099785\n",
      "Epoch 7313/30000 Training Loss: 0.03951382264494896\n",
      "Epoch 7314/30000 Training Loss: 0.04537653923034668\n",
      "Epoch 7315/30000 Training Loss: 0.04977152869105339\n",
      "Epoch 7316/30000 Training Loss: 0.05063595622777939\n",
      "Epoch 7317/30000 Training Loss: 0.05778086185455322\n",
      "Epoch 7318/30000 Training Loss: 0.056208234280347824\n",
      "Epoch 7319/30000 Training Loss: 0.04550204426050186\n",
      "Epoch 7320/30000 Training Loss: 0.052142154425382614\n",
      "Epoch 7321/30000 Training Loss: 0.04432018846273422\n",
      "Epoch 7322/30000 Training Loss: 0.061330199241638184\n",
      "Epoch 7323/30000 Training Loss: 0.0484900139272213\n",
      "Epoch 7324/30000 Training Loss: 0.05396559089422226\n",
      "Epoch 7325/30000 Training Loss: 0.050301771610975266\n",
      "Epoch 7326/30000 Training Loss: 0.04495098441839218\n",
      "Epoch 7327/30000 Training Loss: 0.05377364903688431\n",
      "Epoch 7328/30000 Training Loss: 0.048065152019262314\n",
      "Epoch 7329/30000 Training Loss: 0.048998553305864334\n",
      "Epoch 7330/30000 Training Loss: 0.05003530532121658\n",
      "Epoch 7331/30000 Training Loss: 0.05010383203625679\n",
      "Epoch 7332/30000 Training Loss: 0.04975103586912155\n",
      "Epoch 7333/30000 Training Loss: 0.05533812195062637\n",
      "Epoch 7334/30000 Training Loss: 0.05043758079409599\n",
      "Epoch 7335/30000 Training Loss: 0.05437804013490677\n",
      "Epoch 7336/30000 Training Loss: 0.051557935774326324\n",
      "Epoch 7337/30000 Training Loss: 0.045847997069358826\n",
      "Epoch 7338/30000 Training Loss: 0.05345733091235161\n",
      "Epoch 7339/30000 Training Loss: 0.05399937182664871\n",
      "Epoch 7340/30000 Training Loss: 0.05470041185617447\n",
      "Epoch 7341/30000 Training Loss: 0.04275235906243324\n",
      "Epoch 7342/30000 Training Loss: 0.058277957141399384\n",
      "Epoch 7343/30000 Training Loss: 0.04968363046646118\n",
      "Epoch 7344/30000 Training Loss: 0.04931839555501938\n",
      "Epoch 7345/30000 Training Loss: 0.05585931986570358\n",
      "Epoch 7346/30000 Training Loss: 0.05179867893457413\n",
      "Epoch 7347/30000 Training Loss: 0.044736240059137344\n",
      "Epoch 7348/30000 Training Loss: 0.0620429702103138\n",
      "Epoch 7349/30000 Training Loss: 0.04395913705229759\n",
      "Epoch 7350/30000 Training Loss: 0.04954258352518082\n",
      "Epoch 7350/30000 Validation Loss: 0.04548613354563713\n",
      "Epoch 7351/30000 Training Loss: 0.03935856744647026\n",
      "Epoch 7352/30000 Training Loss: 0.058035075664520264\n",
      "Epoch 7353/30000 Training Loss: 0.04512087255716324\n",
      "Epoch 7354/30000 Training Loss: 0.059399861842393875\n",
      "Epoch 7355/30000 Training Loss: 0.06175001338124275\n",
      "Epoch 7356/30000 Training Loss: 0.04778101295232773\n",
      "Epoch 7357/30000 Training Loss: 0.050964243710041046\n",
      "Epoch 7358/30000 Training Loss: 0.05405168607831001\n",
      "Epoch 7359/30000 Training Loss: 0.056015171110630035\n",
      "Epoch 7360/30000 Training Loss: 0.05038771778345108\n",
      "Epoch 7361/30000 Training Loss: 0.05135156959295273\n",
      "Epoch 7362/30000 Training Loss: 0.04840748757123947\n",
      "Epoch 7363/30000 Training Loss: 0.04752754047513008\n",
      "Epoch 7364/30000 Training Loss: 0.049651168286800385\n",
      "Epoch 7365/30000 Training Loss: 0.05013727396726608\n",
      "Epoch 7366/30000 Training Loss: 0.051579348742961884\n",
      "Epoch 7367/30000 Training Loss: 0.06562815606594086\n",
      "Epoch 7368/30000 Training Loss: 0.04955824092030525\n",
      "Epoch 7369/30000 Training Loss: 0.051199786365032196\n",
      "Epoch 7370/30000 Training Loss: 0.04742586612701416\n",
      "Epoch 7371/30000 Training Loss: 0.04995817691087723\n",
      "Epoch 7372/30000 Training Loss: 0.05156221240758896\n",
      "Epoch 7373/30000 Training Loss: 0.04970669746398926\n",
      "Epoch 7374/30000 Training Loss: 0.04390602558851242\n",
      "Epoch 7375/30000 Training Loss: 0.047193776816129684\n",
      "Epoch 7376/30000 Training Loss: 0.05380357429385185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7377/30000 Training Loss: 0.05462334305047989\n",
      "Epoch 7378/30000 Training Loss: 0.046461451798677444\n",
      "Epoch 7379/30000 Training Loss: 0.04889201000332832\n",
      "Epoch 7380/30000 Training Loss: 0.04738331958651543\n",
      "Epoch 7381/30000 Training Loss: 0.05038394406437874\n",
      "Epoch 7382/30000 Training Loss: 0.04740341380238533\n",
      "Epoch 7383/30000 Training Loss: 0.043871328234672546\n",
      "Epoch 7384/30000 Training Loss: 0.05777410417795181\n",
      "Epoch 7385/30000 Training Loss: 0.05429299548268318\n",
      "Epoch 7386/30000 Training Loss: 0.049562834203243256\n",
      "Epoch 7387/30000 Training Loss: 0.052338212728500366\n",
      "Epoch 7388/30000 Training Loss: 0.053520698100328445\n",
      "Epoch 7389/30000 Training Loss: 0.05096952244639397\n",
      "Epoch 7390/30000 Training Loss: 0.05139755457639694\n",
      "Epoch 7391/30000 Training Loss: 0.05016164854168892\n",
      "Epoch 7392/30000 Training Loss: 0.05463544651865959\n",
      "Epoch 7393/30000 Training Loss: 0.05358316749334335\n",
      "Epoch 7394/30000 Training Loss: 0.0462716706097126\n",
      "Epoch 7395/30000 Training Loss: 0.058518290519714355\n",
      "Epoch 7396/30000 Training Loss: 0.05349535867571831\n",
      "Epoch 7397/30000 Training Loss: 0.05006689950823784\n",
      "Epoch 7398/30000 Training Loss: 0.04745574668049812\n",
      "Epoch 7399/30000 Training Loss: 0.04982823505997658\n",
      "Epoch 7400/30000 Training Loss: 0.048689015209674835\n",
      "Epoch 7400/30000 Validation Loss: 0.05083617568016052\n",
      "Epoch 7401/30000 Training Loss: 0.047642193734645844\n",
      "Epoch 7402/30000 Training Loss: 0.04586058855056763\n",
      "Epoch 7403/30000 Training Loss: 0.046348195523023605\n",
      "Epoch 7404/30000 Training Loss: 0.05029217526316643\n",
      "Epoch 7405/30000 Training Loss: 0.05474844574928284\n",
      "Epoch 7406/30000 Training Loss: 0.04503350332379341\n",
      "Epoch 7407/30000 Training Loss: 0.043929923325777054\n",
      "Epoch 7408/30000 Training Loss: 0.05479903146624565\n",
      "Epoch 7409/30000 Training Loss: 0.06052478402853012\n",
      "Epoch 7410/30000 Training Loss: 0.04953952133655548\n",
      "Epoch 7411/30000 Training Loss: 0.04507805034518242\n",
      "Epoch 7412/30000 Training Loss: 0.05371706932783127\n",
      "Epoch 7413/30000 Training Loss: 0.046085406094789505\n",
      "Epoch 7414/30000 Training Loss: 0.04773341119289398\n",
      "Epoch 7415/30000 Training Loss: 0.046294718980789185\n",
      "Epoch 7416/30000 Training Loss: 0.05148705840110779\n",
      "Epoch 7417/30000 Training Loss: 0.049713827669620514\n",
      "Epoch 7418/30000 Training Loss: 0.047692324966192245\n",
      "Epoch 7419/30000 Training Loss: 0.04689355194568634\n",
      "Epoch 7420/30000 Training Loss: 0.04585583880543709\n",
      "Epoch 7421/30000 Training Loss: 0.04453515633940697\n",
      "Epoch 7422/30000 Training Loss: 0.05027692764997482\n",
      "Epoch 7423/30000 Training Loss: 0.043271563947200775\n",
      "Epoch 7424/30000 Training Loss: 0.050751347094774246\n",
      "Epoch 7425/30000 Training Loss: 0.050796013325452805\n",
      "Epoch 7426/30000 Training Loss: 0.04315865412354469\n",
      "Epoch 7427/30000 Training Loss: 0.052397243678569794\n",
      "Epoch 7428/30000 Training Loss: 0.050262968987226486\n",
      "Epoch 7429/30000 Training Loss: 0.04533037543296814\n",
      "Epoch 7430/30000 Training Loss: 0.05013072490692139\n",
      "Epoch 7431/30000 Training Loss: 0.04713227227330208\n",
      "Epoch 7432/30000 Training Loss: 0.060558516532182693\n",
      "Epoch 7433/30000 Training Loss: 0.0514514222741127\n",
      "Epoch 7434/30000 Training Loss: 0.044183529913425446\n",
      "Epoch 7435/30000 Training Loss: 0.05107234790921211\n",
      "Epoch 7436/30000 Training Loss: 0.05701625347137451\n",
      "Epoch 7437/30000 Training Loss: 0.048931561410427094\n",
      "Epoch 7438/30000 Training Loss: 0.045464009046554565\n",
      "Epoch 7439/30000 Training Loss: 0.04541207104921341\n",
      "Epoch 7440/30000 Training Loss: 0.050895579159259796\n",
      "Epoch 7441/30000 Training Loss: 0.05029277876019478\n",
      "Epoch 7442/30000 Training Loss: 0.05039317160844803\n",
      "Epoch 7443/30000 Training Loss: 0.042019497603178024\n",
      "Epoch 7444/30000 Training Loss: 0.048567693680524826\n",
      "Epoch 7445/30000 Training Loss: 0.04959181696176529\n",
      "Epoch 7446/30000 Training Loss: 0.04593644663691521\n",
      "Epoch 7447/30000 Training Loss: 0.06215854734182358\n",
      "Epoch 7448/30000 Training Loss: 0.05249267816543579\n",
      "Epoch 7449/30000 Training Loss: 0.037639036774635315\n",
      "Epoch 7450/30000 Training Loss: 0.049726326018571854\n",
      "Epoch 7450/30000 Validation Loss: 0.052703000605106354\n",
      "Epoch 7451/30000 Training Loss: 0.05057743936777115\n",
      "Epoch 7452/30000 Training Loss: 0.04728878661990166\n",
      "Epoch 7453/30000 Training Loss: 0.05210122466087341\n",
      "Epoch 7454/30000 Training Loss: 0.05954242870211601\n",
      "Epoch 7455/30000 Training Loss: 0.049118153750896454\n",
      "Epoch 7456/30000 Training Loss: 0.05415550619363785\n",
      "Epoch 7457/30000 Training Loss: 0.04971630871295929\n",
      "Epoch 7458/30000 Training Loss: 0.052609883248806\n",
      "Epoch 7459/30000 Training Loss: 0.04530626907944679\n",
      "Epoch 7460/30000 Training Loss: 0.05494285747408867\n",
      "Epoch 7461/30000 Training Loss: 0.05169782042503357\n",
      "Epoch 7462/30000 Training Loss: 0.047482527792453766\n",
      "Epoch 7463/30000 Training Loss: 0.03999153524637222\n",
      "Epoch 7464/30000 Training Loss: 0.03686069697141647\n",
      "Epoch 7465/30000 Training Loss: 0.04707702249288559\n",
      "Epoch 7466/30000 Training Loss: 0.0429302342236042\n",
      "Epoch 7467/30000 Training Loss: 0.05627239868044853\n",
      "Epoch 7468/30000 Training Loss: 0.05035128444433212\n",
      "Epoch 7469/30000 Training Loss: 0.04891660064458847\n",
      "Epoch 7470/30000 Training Loss: 0.047299984842538834\n",
      "Epoch 7471/30000 Training Loss: 0.04984261095523834\n",
      "Epoch 7472/30000 Training Loss: 0.051251985132694244\n",
      "Epoch 7473/30000 Training Loss: 0.05190499499440193\n",
      "Epoch 7474/30000 Training Loss: 0.04915625974535942\n",
      "Epoch 7475/30000 Training Loss: 0.045034270733594894\n",
      "Epoch 7476/30000 Training Loss: 0.0471273809671402\n",
      "Epoch 7477/30000 Training Loss: 0.04993337392807007\n",
      "Epoch 7478/30000 Training Loss: 0.048949841409921646\n",
      "Epoch 7479/30000 Training Loss: 0.0441754013299942\n",
      "Epoch 7480/30000 Training Loss: 0.04828093200922012\n",
      "Epoch 7481/30000 Training Loss: 0.0450340174138546\n",
      "Epoch 7482/30000 Training Loss: 0.04738714545965195\n",
      "Epoch 7483/30000 Training Loss: 0.04466814547777176\n",
      "Epoch 7484/30000 Training Loss: 0.050236545503139496\n",
      "Epoch 7485/30000 Training Loss: 0.04850976914167404\n",
      "Epoch 7486/30000 Training Loss: 0.050637949258089066\n",
      "Epoch 7487/30000 Training Loss: 0.05206584930419922\n",
      "Epoch 7488/30000 Training Loss: 0.0435592457652092\n",
      "Epoch 7489/30000 Training Loss: 0.060672350227832794\n",
      "Epoch 7490/30000 Training Loss: 0.06480912864208221\n",
      "Epoch 7491/30000 Training Loss: 0.05252889543771744\n",
      "Epoch 7492/30000 Training Loss: 0.06197420880198479\n",
      "Epoch 7493/30000 Training Loss: 0.05246837064623833\n",
      "Epoch 7494/30000 Training Loss: 0.05921614170074463\n",
      "Epoch 7495/30000 Training Loss: 0.04892795532941818\n",
      "Epoch 7496/30000 Training Loss: 0.05252818018198013\n",
      "Epoch 7497/30000 Training Loss: 0.048792507499456406\n",
      "Epoch 7498/30000 Training Loss: 0.04781315475702286\n",
      "Epoch 7499/30000 Training Loss: 0.048001259565353394\n",
      "Epoch 7500/30000 Training Loss: 0.04831875115633011\n",
      "Epoch 7500/30000 Validation Loss: 0.05177651718258858\n",
      "Epoch 7501/30000 Training Loss: 0.05477861687541008\n",
      "Epoch 7502/30000 Training Loss: 0.044804394245147705\n",
      "Epoch 7503/30000 Training Loss: 0.05436320975422859\n",
      "Epoch 7504/30000 Training Loss: 0.048814378678798676\n",
      "Epoch 7505/30000 Training Loss: 0.04617660865187645\n",
      "Epoch 7506/30000 Training Loss: 0.05808442085981369\n",
      "Epoch 7507/30000 Training Loss: 0.0574551522731781\n",
      "Epoch 7508/30000 Training Loss: 0.048513513058423996\n",
      "Epoch 7509/30000 Training Loss: 0.052596550434827805\n",
      "Epoch 7510/30000 Training Loss: 0.05651327967643738\n",
      "Epoch 7511/30000 Training Loss: 0.04828346520662308\n",
      "Epoch 7512/30000 Training Loss: 0.048281338065862656\n",
      "Epoch 7513/30000 Training Loss: 0.06313347071409225\n",
      "Epoch 7514/30000 Training Loss: 0.04745093733072281\n",
      "Epoch 7515/30000 Training Loss: 0.05141381174325943\n",
      "Epoch 7516/30000 Training Loss: 0.04707009345293045\n",
      "Epoch 7517/30000 Training Loss: 0.050816189497709274\n",
      "Epoch 7518/30000 Training Loss: 0.04930095002055168\n",
      "Epoch 7519/30000 Training Loss: 0.04466000199317932\n",
      "Epoch 7520/30000 Training Loss: 0.048956967890262604\n",
      "Epoch 7521/30000 Training Loss: 0.05518149584531784\n",
      "Epoch 7522/30000 Training Loss: 0.04558554291725159\n",
      "Epoch 7523/30000 Training Loss: 0.04684080928564072\n",
      "Epoch 7524/30000 Training Loss: 0.048906005918979645\n",
      "Epoch 7525/30000 Training Loss: 0.053189508616924286\n",
      "Epoch 7526/30000 Training Loss: 0.057030707597732544\n",
      "Epoch 7527/30000 Training Loss: 0.0488717146217823\n",
      "Epoch 7528/30000 Training Loss: 0.043251972645521164\n",
      "Epoch 7529/30000 Training Loss: 0.05380422994494438\n",
      "Epoch 7530/30000 Training Loss: 0.05458422750234604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7531/30000 Training Loss: 0.04253922775387764\n",
      "Epoch 7532/30000 Training Loss: 0.03929758444428444\n",
      "Epoch 7533/30000 Training Loss: 0.050214845687150955\n",
      "Epoch 7534/30000 Training Loss: 0.047015294432640076\n",
      "Epoch 7535/30000 Training Loss: 0.0471220500767231\n",
      "Epoch 7536/30000 Training Loss: 0.04204339534044266\n",
      "Epoch 7537/30000 Training Loss: 0.0568416491150856\n",
      "Epoch 7538/30000 Training Loss: 0.043862927705049515\n",
      "Epoch 7539/30000 Training Loss: 0.044833410531282425\n",
      "Epoch 7540/30000 Training Loss: 0.047060996294021606\n",
      "Epoch 7541/30000 Training Loss: 0.05960363894701004\n",
      "Epoch 7542/30000 Training Loss: 0.04271251708269119\n",
      "Epoch 7543/30000 Training Loss: 0.052207887172698975\n",
      "Epoch 7544/30000 Training Loss: 0.050462644547224045\n",
      "Epoch 7545/30000 Training Loss: 0.05095930024981499\n",
      "Epoch 7546/30000 Training Loss: 0.04986503720283508\n",
      "Epoch 7547/30000 Training Loss: 0.03919302299618721\n",
      "Epoch 7548/30000 Training Loss: 0.05606403946876526\n",
      "Epoch 7549/30000 Training Loss: 0.0559161901473999\n",
      "Epoch 7550/30000 Training Loss: 0.04927654191851616\n",
      "Epoch 7550/30000 Validation Loss: 0.0545128658413887\n",
      "Epoch 7551/30000 Training Loss: 0.04913562163710594\n",
      "Epoch 7552/30000 Training Loss: 0.05257687717676163\n",
      "Epoch 7553/30000 Training Loss: 0.056547023355960846\n",
      "Epoch 7554/30000 Training Loss: 0.050471264868974686\n",
      "Epoch 7555/30000 Training Loss: 0.05489487573504448\n",
      "Epoch 7556/30000 Training Loss: 0.046791333705186844\n",
      "Epoch 7557/30000 Training Loss: 0.04721100255846977\n",
      "Epoch 7558/30000 Training Loss: 0.05875995010137558\n",
      "Epoch 7559/30000 Training Loss: 0.04978010430932045\n",
      "Epoch 7560/30000 Training Loss: 0.04834343492984772\n",
      "Epoch 7561/30000 Training Loss: 0.04699383303523064\n",
      "Epoch 7562/30000 Training Loss: 0.05334555357694626\n",
      "Epoch 7563/30000 Training Loss: 0.04427188262343407\n",
      "Epoch 7564/30000 Training Loss: 0.043452680110931396\n",
      "Epoch 7565/30000 Training Loss: 0.049143534153699875\n",
      "Epoch 7566/30000 Training Loss: 0.046847153455019\n",
      "Epoch 7567/30000 Training Loss: 0.046912193298339844\n",
      "Epoch 7568/30000 Training Loss: 0.04774443060159683\n",
      "Epoch 7569/30000 Training Loss: 0.04501963034272194\n",
      "Epoch 7570/30000 Training Loss: 0.05144186690449715\n",
      "Epoch 7571/30000 Training Loss: 0.045508041977882385\n",
      "Epoch 7572/30000 Training Loss: 0.04332695156335831\n",
      "Epoch 7573/30000 Training Loss: 0.05024927854537964\n",
      "Epoch 7574/30000 Training Loss: 0.04430342838168144\n",
      "Epoch 7575/30000 Training Loss: 0.05609332397580147\n",
      "Epoch 7576/30000 Training Loss: 0.055902302265167236\n",
      "Epoch 7577/30000 Training Loss: 0.0496617928147316\n",
      "Epoch 7578/30000 Training Loss: 0.05092673748731613\n",
      "Epoch 7579/30000 Training Loss: 0.049564577639102936\n",
      "Epoch 7580/30000 Training Loss: 0.04636262729763985\n",
      "Epoch 7581/30000 Training Loss: 0.06333792954683304\n",
      "Epoch 7582/30000 Training Loss: 0.04561867564916611\n",
      "Epoch 7583/30000 Training Loss: 0.056364335119724274\n",
      "Epoch 7584/30000 Training Loss: 0.04435024783015251\n",
      "Epoch 7585/30000 Training Loss: 0.04979608580470085\n",
      "Epoch 7586/30000 Training Loss: 0.05072333663702011\n",
      "Epoch 7587/30000 Training Loss: 0.0475018247961998\n",
      "Epoch 7588/30000 Training Loss: 0.05851859599351883\n",
      "Epoch 7589/30000 Training Loss: 0.04678752273321152\n",
      "Epoch 7590/30000 Training Loss: 0.04772620275616646\n",
      "Epoch 7591/30000 Training Loss: 0.04291199892759323\n",
      "Epoch 7592/30000 Training Loss: 0.04772878438234329\n",
      "Epoch 7593/30000 Training Loss: 0.04874684661626816\n",
      "Epoch 7594/30000 Training Loss: 0.04648469015955925\n",
      "Epoch 7595/30000 Training Loss: 0.05011496692895889\n",
      "Epoch 7596/30000 Training Loss: 0.04727810621261597\n",
      "Epoch 7597/30000 Training Loss: 0.053191494196653366\n",
      "Epoch 7598/30000 Training Loss: 0.0448257252573967\n",
      "Epoch 7599/30000 Training Loss: 0.05985506623983383\n",
      "Epoch 7600/30000 Training Loss: 0.043960753828287125\n",
      "Epoch 7600/30000 Validation Loss: 0.04628352075815201\n",
      "Epoch 7601/30000 Training Loss: 0.04886072129011154\n",
      "Epoch 7602/30000 Training Loss: 0.042698394507169724\n",
      "Epoch 7603/30000 Training Loss: 0.04880581423640251\n",
      "Epoch 7604/30000 Training Loss: 0.042099639773368835\n",
      "Epoch 7605/30000 Training Loss: 0.053886305540800095\n",
      "Epoch 7606/30000 Training Loss: 0.04701834172010422\n",
      "Epoch 7607/30000 Training Loss: 0.05945940688252449\n",
      "Epoch 7608/30000 Training Loss: 0.04997836425900459\n",
      "Epoch 7609/30000 Training Loss: 0.05394895002245903\n",
      "Epoch 7610/30000 Training Loss: 0.0427553616464138\n",
      "Epoch 7611/30000 Training Loss: 0.050085701048374176\n",
      "Epoch 7612/30000 Training Loss: 0.03593664616346359\n",
      "Epoch 7613/30000 Training Loss: 0.04901346564292908\n",
      "Epoch 7614/30000 Training Loss: 0.046813346445560455\n",
      "Epoch 7615/30000 Training Loss: 0.046957552433013916\n",
      "Epoch 7616/30000 Training Loss: 0.04685429856181145\n",
      "Epoch 7617/30000 Training Loss: 0.04376118257641792\n",
      "Epoch 7618/30000 Training Loss: 0.051575321704149246\n",
      "Epoch 7619/30000 Training Loss: 0.05581625550985336\n",
      "Epoch 7620/30000 Training Loss: 0.053395140916109085\n",
      "Epoch 7621/30000 Training Loss: 0.04602935537695885\n",
      "Epoch 7622/30000 Training Loss: 0.0464373342692852\n",
      "Epoch 7623/30000 Training Loss: 0.05063744634389877\n",
      "Epoch 7624/30000 Training Loss: 0.043427687138319016\n",
      "Epoch 7625/30000 Training Loss: 0.044865481555461884\n",
      "Epoch 7626/30000 Training Loss: 0.04603281617164612\n",
      "Epoch 7627/30000 Training Loss: 0.04294397681951523\n",
      "Epoch 7628/30000 Training Loss: 0.048600342124700546\n",
      "Epoch 7629/30000 Training Loss: 0.055109668523073196\n",
      "Epoch 7630/30000 Training Loss: 0.05049508064985275\n",
      "Epoch 7631/30000 Training Loss: 0.04849968105554581\n",
      "Epoch 7632/30000 Training Loss: 0.0546673908829689\n",
      "Epoch 7633/30000 Training Loss: 0.05396842211484909\n",
      "Epoch 7634/30000 Training Loss: 0.059604234993457794\n",
      "Epoch 7635/30000 Training Loss: 0.039464905858039856\n",
      "Epoch 7636/30000 Training Loss: 0.040476687252521515\n",
      "Epoch 7637/30000 Training Loss: 0.05446280166506767\n",
      "Epoch 7638/30000 Training Loss: 0.05168968439102173\n",
      "Epoch 7639/30000 Training Loss: 0.046420082449913025\n",
      "Epoch 7640/30000 Training Loss: 0.049654193222522736\n",
      "Epoch 7641/30000 Training Loss: 0.05351237580180168\n",
      "Epoch 7642/30000 Training Loss: 0.05704234912991524\n",
      "Epoch 7643/30000 Training Loss: 0.04837336018681526\n",
      "Epoch 7644/30000 Training Loss: 0.05863143131136894\n",
      "Epoch 7645/30000 Training Loss: 0.05261475592851639\n",
      "Epoch 7646/30000 Training Loss: 0.05994226410984993\n",
      "Epoch 7647/30000 Training Loss: 0.04721780866384506\n",
      "Epoch 7648/30000 Training Loss: 0.04485534876585007\n",
      "Epoch 7649/30000 Training Loss: 0.050406187772750854\n",
      "Epoch 7650/30000 Training Loss: 0.05033227801322937\n",
      "Epoch 7650/30000 Validation Loss: 0.04567953199148178\n",
      "Epoch 7651/30000 Training Loss: 0.0425465852022171\n",
      "Epoch 7652/30000 Training Loss: 0.04941646009683609\n",
      "Epoch 7653/30000 Training Loss: 0.0484146811068058\n",
      "Epoch 7654/30000 Training Loss: 0.05216662958264351\n",
      "Epoch 7655/30000 Training Loss: 0.04830322414636612\n",
      "Epoch 7656/30000 Training Loss: 0.05916503071784973\n",
      "Epoch 7657/30000 Training Loss: 0.043958257883787155\n",
      "Epoch 7658/30000 Training Loss: 0.06235348433256149\n",
      "Epoch 7659/30000 Training Loss: 0.04659417271614075\n",
      "Epoch 7660/30000 Training Loss: 0.05437909811735153\n",
      "Epoch 7661/30000 Training Loss: 0.03907909244298935\n",
      "Epoch 7662/30000 Training Loss: 0.05524221062660217\n",
      "Epoch 7663/30000 Training Loss: 0.05238424986600876\n",
      "Epoch 7664/30000 Training Loss: 0.057309605181217194\n",
      "Epoch 7665/30000 Training Loss: 0.04911364987492561\n",
      "Epoch 7666/30000 Training Loss: 0.04993727058172226\n",
      "Epoch 7667/30000 Training Loss: 0.06172830983996391\n",
      "Epoch 7668/30000 Training Loss: 0.05000609904527664\n",
      "Epoch 7669/30000 Training Loss: 0.0512951985001564\n",
      "Epoch 7670/30000 Training Loss: 0.04790150374174118\n",
      "Epoch 7671/30000 Training Loss: 0.04572300985455513\n",
      "Epoch 7672/30000 Training Loss: 0.051946885883808136\n",
      "Epoch 7673/30000 Training Loss: 0.054443974047899246\n",
      "Epoch 7674/30000 Training Loss: 0.04372035339474678\n",
      "Epoch 7675/30000 Training Loss: 0.050270967185497284\n",
      "Epoch 7676/30000 Training Loss: 0.04660859704017639\n",
      "Epoch 7677/30000 Training Loss: 0.049557723104953766\n",
      "Epoch 7678/30000 Training Loss: 0.04830490052700043\n",
      "Epoch 7679/30000 Training Loss: 0.05121802166104317\n",
      "Epoch 7680/30000 Training Loss: 0.05651351064443588\n",
      "Epoch 7681/30000 Training Loss: 0.048193879425525665\n",
      "Epoch 7682/30000 Training Loss: 0.04556757211685181\n",
      "Epoch 7683/30000 Training Loss: 0.05032331496477127\n",
      "Epoch 7684/30000 Training Loss: 0.047861646860837936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7685/30000 Training Loss: 0.034780025482177734\n",
      "Epoch 7686/30000 Training Loss: 0.05272455886006355\n",
      "Epoch 7687/30000 Training Loss: 0.05064641684293747\n",
      "Epoch 7688/30000 Training Loss: 0.058392297476530075\n",
      "Epoch 7689/30000 Training Loss: 0.04989635944366455\n",
      "Epoch 7690/30000 Training Loss: 0.05607432872056961\n",
      "Epoch 7691/30000 Training Loss: 0.056265801191329956\n",
      "Epoch 7692/30000 Training Loss: 0.04701242968440056\n",
      "Epoch 7693/30000 Training Loss: 0.048540763556957245\n",
      "Epoch 7694/30000 Training Loss: 0.0425688773393631\n",
      "Epoch 7695/30000 Training Loss: 0.04765024781227112\n",
      "Epoch 7696/30000 Training Loss: 0.0526055209338665\n",
      "Epoch 7697/30000 Training Loss: 0.046439748257398605\n",
      "Epoch 7698/30000 Training Loss: 0.04350435733795166\n",
      "Epoch 7699/30000 Training Loss: 0.048084475100040436\n",
      "Epoch 7700/30000 Training Loss: 0.050933051854372025\n",
      "Epoch 7700/30000 Validation Loss: 0.05264389514923096\n",
      "Epoch 7701/30000 Training Loss: 0.05183719843626022\n",
      "Epoch 7702/30000 Training Loss: 0.055412571877241135\n",
      "Epoch 7703/30000 Training Loss: 0.050650645047426224\n",
      "Epoch 7704/30000 Training Loss: 0.04150538146495819\n",
      "Epoch 7705/30000 Training Loss: 0.05226057767868042\n",
      "Epoch 7706/30000 Training Loss: 0.05148433893918991\n",
      "Epoch 7707/30000 Training Loss: 0.05044791102409363\n",
      "Epoch 7708/30000 Training Loss: 0.055105872452259064\n",
      "Epoch 7709/30000 Training Loss: 0.04519262537360191\n",
      "Epoch 7710/30000 Training Loss: 0.05619475245475769\n",
      "Epoch 7711/30000 Training Loss: 0.05199248716235161\n",
      "Epoch 7712/30000 Training Loss: 0.05244801193475723\n",
      "Epoch 7713/30000 Training Loss: 0.052210498601198196\n",
      "Epoch 7714/30000 Training Loss: 0.0421258881688118\n",
      "Epoch 7715/30000 Training Loss: 0.04781262204051018\n",
      "Epoch 7716/30000 Training Loss: 0.05078194662928581\n",
      "Epoch 7717/30000 Training Loss: 0.04390648007392883\n",
      "Epoch 7718/30000 Training Loss: 0.05124642699956894\n",
      "Epoch 7719/30000 Training Loss: 0.0465843565762043\n",
      "Epoch 7720/30000 Training Loss: 0.04830438643693924\n",
      "Epoch 7721/30000 Training Loss: 0.05415559932589531\n",
      "Epoch 7722/30000 Training Loss: 0.044714100658893585\n",
      "Epoch 7723/30000 Training Loss: 0.05117093399167061\n",
      "Epoch 7724/30000 Training Loss: 0.04460539296269417\n",
      "Epoch 7725/30000 Training Loss: 0.049065183848142624\n",
      "Epoch 7726/30000 Training Loss: 0.04677777737379074\n",
      "Epoch 7727/30000 Training Loss: 0.053759269416332245\n",
      "Epoch 7728/30000 Training Loss: 0.04735492542386055\n",
      "Epoch 7729/30000 Training Loss: 0.044523920863866806\n",
      "Epoch 7730/30000 Training Loss: 0.04964226856827736\n",
      "Epoch 7731/30000 Training Loss: 0.04271373525261879\n",
      "Epoch 7732/30000 Training Loss: 0.04898109287023544\n",
      "Epoch 7733/30000 Training Loss: 0.04921277239918709\n",
      "Epoch 7734/30000 Training Loss: 0.045119524002075195\n",
      "Epoch 7735/30000 Training Loss: 0.045488614588975906\n",
      "Epoch 7736/30000 Training Loss: 0.04735534265637398\n",
      "Epoch 7737/30000 Training Loss: 0.04555930942296982\n",
      "Epoch 7738/30000 Training Loss: 0.0491778738796711\n",
      "Epoch 7739/30000 Training Loss: 0.03907858207821846\n",
      "Epoch 7740/30000 Training Loss: 0.05640695244073868\n",
      "Epoch 7741/30000 Training Loss: 0.0454898439347744\n",
      "Epoch 7742/30000 Training Loss: 0.04274960234761238\n",
      "Epoch 7743/30000 Training Loss: 0.04301959648728371\n",
      "Epoch 7744/30000 Training Loss: 0.05461946874856949\n",
      "Epoch 7745/30000 Training Loss: 0.04845578223466873\n",
      "Epoch 7746/30000 Training Loss: 0.04824793338775635\n",
      "Epoch 7747/30000 Training Loss: 0.04631393775343895\n",
      "Epoch 7748/30000 Training Loss: 0.05033727362751961\n",
      "Epoch 7749/30000 Training Loss: 0.04492924362421036\n",
      "Epoch 7750/30000 Training Loss: 0.0484861321747303\n",
      "Epoch 7750/30000 Validation Loss: 0.05867405980825424\n",
      "Epoch 7751/30000 Training Loss: 0.04843694344162941\n",
      "Epoch 7752/30000 Training Loss: 0.04199548810720444\n",
      "Epoch 7753/30000 Training Loss: 0.0492452010512352\n",
      "Epoch 7754/30000 Training Loss: 0.04662112519145012\n",
      "Epoch 7755/30000 Training Loss: 0.04962124302983284\n",
      "Epoch 7756/30000 Training Loss: 0.047182101756334305\n",
      "Epoch 7757/30000 Training Loss: 0.043913863599300385\n",
      "Epoch 7758/30000 Training Loss: 0.04934299364686012\n",
      "Epoch 7759/30000 Training Loss: 0.0477631539106369\n",
      "Epoch 7760/30000 Training Loss: 0.04744688421487808\n",
      "Epoch 7761/30000 Training Loss: 0.04729783535003662\n",
      "Epoch 7762/30000 Training Loss: 0.048101600259542465\n",
      "Epoch 7763/30000 Training Loss: 0.05053947493433952\n",
      "Epoch 7764/30000 Training Loss: 0.05233148857951164\n",
      "Epoch 7765/30000 Training Loss: 0.045380301773548126\n",
      "Epoch 7766/30000 Training Loss: 0.05487050861120224\n",
      "Epoch 7767/30000 Training Loss: 0.04848654940724373\n",
      "Epoch 7768/30000 Training Loss: 0.05858670920133591\n",
      "Epoch 7769/30000 Training Loss: 0.05828053876757622\n",
      "Epoch 7770/30000 Training Loss: 0.05369945615530014\n",
      "Epoch 7771/30000 Training Loss: 0.053165845572948456\n",
      "Epoch 7772/30000 Training Loss: 0.05173952132463455\n",
      "Epoch 7773/30000 Training Loss: 0.04362346976995468\n",
      "Epoch 7774/30000 Training Loss: 0.05651237815618515\n",
      "Epoch 7775/30000 Training Loss: 0.048619214445352554\n",
      "Epoch 7776/30000 Training Loss: 0.04454691335558891\n",
      "Epoch 7777/30000 Training Loss: 0.054879628121852875\n",
      "Epoch 7778/30000 Training Loss: 0.055985234677791595\n",
      "Epoch 7779/30000 Training Loss: 0.04856226593255997\n",
      "Epoch 7780/30000 Training Loss: 0.05114610120654106\n",
      "Epoch 7781/30000 Training Loss: 0.039756596088409424\n",
      "Epoch 7782/30000 Training Loss: 0.05133984610438347\n",
      "Epoch 7783/30000 Training Loss: 0.044791098684072495\n",
      "Epoch 7784/30000 Training Loss: 0.05049405246973038\n",
      "Epoch 7785/30000 Training Loss: 0.058424003422260284\n",
      "Epoch 7786/30000 Training Loss: 0.04806504026055336\n",
      "Epoch 7787/30000 Training Loss: 0.041731998324394226\n",
      "Epoch 7788/30000 Training Loss: 0.05177801102399826\n",
      "Epoch 7789/30000 Training Loss: 0.04509959742426872\n",
      "Epoch 7790/30000 Training Loss: 0.0573522225022316\n",
      "Epoch 7791/30000 Training Loss: 0.054742664098739624\n",
      "Epoch 7792/30000 Training Loss: 0.04793503135442734\n",
      "Epoch 7793/30000 Training Loss: 0.05004880949854851\n",
      "Epoch 7794/30000 Training Loss: 0.04588465020060539\n",
      "Epoch 7795/30000 Training Loss: 0.050911881029605865\n",
      "Epoch 7796/30000 Training Loss: 0.05776084586977959\n",
      "Epoch 7797/30000 Training Loss: 0.04722357913851738\n",
      "Epoch 7798/30000 Training Loss: 0.05359223484992981\n",
      "Epoch 7799/30000 Training Loss: 0.05234784632921219\n",
      "Epoch 7800/30000 Training Loss: 0.043462902307510376\n",
      "Epoch 7800/30000 Validation Loss: 0.05457738786935806\n",
      "Epoch 7801/30000 Training Loss: 0.044968876987695694\n",
      "Epoch 7802/30000 Training Loss: 0.046862270683050156\n",
      "Epoch 7803/30000 Training Loss: 0.04920211806893349\n",
      "Epoch 7804/30000 Training Loss: 0.050772953778505325\n",
      "Epoch 7805/30000 Training Loss: 0.05831803008913994\n",
      "Epoch 7806/30000 Training Loss: 0.047635652124881744\n",
      "Epoch 7807/30000 Training Loss: 0.05383455008268356\n",
      "Epoch 7808/30000 Training Loss: 0.04897274076938629\n",
      "Epoch 7809/30000 Training Loss: 0.041455961763858795\n",
      "Epoch 7810/30000 Training Loss: 0.05249552056193352\n",
      "Epoch 7811/30000 Training Loss: 0.05626480653882027\n",
      "Epoch 7812/30000 Training Loss: 0.0532715730369091\n",
      "Epoch 7813/30000 Training Loss: 0.05316691845655441\n",
      "Epoch 7814/30000 Training Loss: 0.04262584075331688\n",
      "Epoch 7815/30000 Training Loss: 0.045569323003292084\n",
      "Epoch 7816/30000 Training Loss: 0.052583109587430954\n",
      "Epoch 7817/30000 Training Loss: 0.05017871409654617\n",
      "Epoch 7818/30000 Training Loss: 0.04675375670194626\n",
      "Epoch 7819/30000 Training Loss: 0.04681067541241646\n",
      "Epoch 7820/30000 Training Loss: 0.04904180392622948\n",
      "Epoch 7821/30000 Training Loss: 0.05534087494015694\n",
      "Epoch 7822/30000 Training Loss: 0.04746698588132858\n",
      "Epoch 7823/30000 Training Loss: 0.05342425778508186\n",
      "Epoch 7824/30000 Training Loss: 0.05037721246480942\n",
      "Epoch 7825/30000 Training Loss: 0.048121146857738495\n",
      "Epoch 7826/30000 Training Loss: 0.040329236537218094\n",
      "Epoch 7827/30000 Training Loss: 0.04602678865194321\n",
      "Epoch 7828/30000 Training Loss: 0.04600456357002258\n",
      "Epoch 7829/30000 Training Loss: 0.05172276496887207\n",
      "Epoch 7830/30000 Training Loss: 0.05528394505381584\n",
      "Epoch 7831/30000 Training Loss: 0.04934336245059967\n",
      "Epoch 7832/30000 Training Loss: 0.06195103004574776\n",
      "Epoch 7833/30000 Training Loss: 0.049447283148765564\n",
      "Epoch 7834/30000 Training Loss: 0.048774830996990204\n",
      "Epoch 7835/30000 Training Loss: 0.05030372738838196\n",
      "Epoch 7836/30000 Training Loss: 0.053391337394714355\n",
      "Epoch 7837/30000 Training Loss: 0.052172161638736725\n",
      "Epoch 7838/30000 Training Loss: 0.04882822185754776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7839/30000 Training Loss: 0.04934249073266983\n",
      "Epoch 7840/30000 Training Loss: 0.04988495260477066\n",
      "Epoch 7841/30000 Training Loss: 0.03753461688756943\n",
      "Epoch 7842/30000 Training Loss: 0.05310029536485672\n",
      "Epoch 7843/30000 Training Loss: 0.05227775126695633\n",
      "Epoch 7844/30000 Training Loss: 0.05230288580060005\n",
      "Epoch 7845/30000 Training Loss: 0.051322124898433685\n",
      "Epoch 7846/30000 Training Loss: 0.04636065289378166\n",
      "Epoch 7847/30000 Training Loss: 0.051225461065769196\n",
      "Epoch 7848/30000 Training Loss: 0.045063480734825134\n",
      "Epoch 7849/30000 Training Loss: 0.052678339183330536\n",
      "Epoch 7850/30000 Training Loss: 0.056356023997068405\n",
      "Epoch 7850/30000 Validation Loss: 0.049022313207387924\n",
      "Epoch 7851/30000 Training Loss: 0.05028045177459717\n",
      "Epoch 7852/30000 Training Loss: 0.05051816627383232\n",
      "Epoch 7853/30000 Training Loss: 0.03896385803818703\n",
      "Epoch 7854/30000 Training Loss: 0.051379840821027756\n",
      "Epoch 7855/30000 Training Loss: 0.04964469373226166\n",
      "Epoch 7856/30000 Training Loss: 0.05055323243141174\n",
      "Epoch 7857/30000 Training Loss: 0.044057585299015045\n",
      "Epoch 7858/30000 Training Loss: 0.0463983528316021\n",
      "Epoch 7859/30000 Training Loss: 0.0487072579562664\n",
      "Epoch 7860/30000 Training Loss: 0.043594468384981155\n",
      "Epoch 7861/30000 Training Loss: 0.044674187898635864\n",
      "Epoch 7862/30000 Training Loss: 0.04460548236966133\n",
      "Epoch 7863/30000 Training Loss: 0.03929463028907776\n",
      "Epoch 7864/30000 Training Loss: 0.045446839183568954\n",
      "Epoch 7865/30000 Training Loss: 0.05807048827409744\n",
      "Epoch 7866/30000 Training Loss: 0.05334140732884407\n",
      "Epoch 7867/30000 Training Loss: 0.0631624162197113\n",
      "Epoch 7868/30000 Training Loss: 0.051240891218185425\n",
      "Epoch 7869/30000 Training Loss: 0.045413076877593994\n",
      "Epoch 7870/30000 Training Loss: 0.04623791575431824\n",
      "Epoch 7871/30000 Training Loss: 0.04336286708712578\n",
      "Epoch 7872/30000 Training Loss: 0.04529941454529762\n",
      "Epoch 7873/30000 Training Loss: 0.04674798250198364\n",
      "Epoch 7874/30000 Training Loss: 0.05246702954173088\n",
      "Epoch 7875/30000 Training Loss: 0.054450541734695435\n",
      "Epoch 7876/30000 Training Loss: 0.04422128200531006\n",
      "Epoch 7877/30000 Training Loss: 0.044594768434762955\n",
      "Epoch 7878/30000 Training Loss: 0.0463741235435009\n",
      "Epoch 7879/30000 Training Loss: 0.050193749368190765\n",
      "Epoch 7880/30000 Training Loss: 0.05215545371174812\n",
      "Epoch 7881/30000 Training Loss: 0.05817960575222969\n",
      "Epoch 7882/30000 Training Loss: 0.051626093685626984\n",
      "Epoch 7883/30000 Training Loss: 0.041389502584934235\n",
      "Epoch 7884/30000 Training Loss: 0.04909633845090866\n",
      "Epoch 7885/30000 Training Loss: 0.04933355003595352\n",
      "Epoch 7886/30000 Training Loss: 0.04658190533518791\n",
      "Epoch 7887/30000 Training Loss: 0.054618239402770996\n",
      "Epoch 7888/30000 Training Loss: 0.05429908633232117\n",
      "Epoch 7889/30000 Training Loss: 0.04693441838026047\n",
      "Epoch 7890/30000 Training Loss: 0.037030767649412155\n",
      "Epoch 7891/30000 Training Loss: 0.05446953698992729\n",
      "Epoch 7892/30000 Training Loss: 0.048828981816768646\n",
      "Epoch 7893/30000 Training Loss: 0.04857094585895538\n",
      "Epoch 7894/30000 Training Loss: 0.057178206741809845\n",
      "Epoch 7895/30000 Training Loss: 0.0561712272465229\n",
      "Epoch 7896/30000 Training Loss: 0.05727435275912285\n",
      "Epoch 7897/30000 Training Loss: 0.04049969092011452\n",
      "Epoch 7898/30000 Training Loss: 0.048544880002737045\n",
      "Epoch 7899/30000 Training Loss: 0.051741428673267365\n",
      "Epoch 7900/30000 Training Loss: 0.050387125462293625\n",
      "Epoch 7900/30000 Validation Loss: 0.05110647529363632\n",
      "Epoch 7901/30000 Training Loss: 0.04269670322537422\n",
      "Epoch 7902/30000 Training Loss: 0.049559883773326874\n",
      "Epoch 7903/30000 Training Loss: 0.054221075028181076\n",
      "Epoch 7904/30000 Training Loss: 0.05193621665239334\n",
      "Epoch 7905/30000 Training Loss: 0.049826573580503464\n",
      "Epoch 7906/30000 Training Loss: 0.045529384166002274\n",
      "Epoch 7907/30000 Training Loss: 0.04823366552591324\n",
      "Epoch 7908/30000 Training Loss: 0.048388730734586716\n",
      "Epoch 7909/30000 Training Loss: 0.047213394194841385\n",
      "Epoch 7910/30000 Training Loss: 0.05545461177825928\n",
      "Epoch 7911/30000 Training Loss: 0.050626836717128754\n",
      "Epoch 7912/30000 Training Loss: 0.04696669057011604\n",
      "Epoch 7913/30000 Training Loss: 0.04143424704670906\n",
      "Epoch 7914/30000 Training Loss: 0.039877068251371384\n",
      "Epoch 7915/30000 Training Loss: 0.037592269480228424\n",
      "Epoch 7916/30000 Training Loss: 0.04534953832626343\n",
      "Epoch 7917/30000 Training Loss: 0.04904861003160477\n",
      "Epoch 7918/30000 Training Loss: 0.04778469353914261\n",
      "Epoch 7919/30000 Training Loss: 0.049430619925260544\n",
      "Epoch 7920/30000 Training Loss: 0.04943648725748062\n",
      "Epoch 7921/30000 Training Loss: 0.04513628035783768\n",
      "Epoch 7922/30000 Training Loss: 0.05267946049571037\n",
      "Epoch 7923/30000 Training Loss: 0.05962599068880081\n",
      "Epoch 7924/30000 Training Loss: 0.05152076482772827\n",
      "Epoch 7925/30000 Training Loss: 0.045698318630456924\n",
      "Epoch 7926/30000 Training Loss: 0.050429217517375946\n",
      "Epoch 7927/30000 Training Loss: 0.04985843226313591\n",
      "Epoch 7928/30000 Training Loss: 0.047459088265895844\n",
      "Epoch 7929/30000 Training Loss: 0.04233905300498009\n",
      "Epoch 7930/30000 Training Loss: 0.04204219579696655\n",
      "Epoch 7931/30000 Training Loss: 0.050379205495119095\n",
      "Epoch 7932/30000 Training Loss: 0.041962914168834686\n",
      "Epoch 7933/30000 Training Loss: 0.0490754172205925\n",
      "Epoch 7934/30000 Training Loss: 0.04289628565311432\n",
      "Epoch 7935/30000 Training Loss: 0.04302648454904556\n",
      "Epoch 7936/30000 Training Loss: 0.046889156103134155\n",
      "Epoch 7937/30000 Training Loss: 0.049214888364076614\n",
      "Epoch 7938/30000 Training Loss: 0.0432719886302948\n",
      "Epoch 7939/30000 Training Loss: 0.04106525331735611\n",
      "Epoch 7940/30000 Training Loss: 0.04940088465809822\n",
      "Epoch 7941/30000 Training Loss: 0.0545971505343914\n",
      "Epoch 7942/30000 Training Loss: 0.04633484035730362\n",
      "Epoch 7943/30000 Training Loss: 0.055395834147930145\n",
      "Epoch 7944/30000 Training Loss: 0.05125383660197258\n",
      "Epoch 7945/30000 Training Loss: 0.044682685285806656\n",
      "Epoch 7946/30000 Training Loss: 0.05406830832362175\n",
      "Epoch 7947/30000 Training Loss: 0.05366960167884827\n",
      "Epoch 7948/30000 Training Loss: 0.050518810749053955\n",
      "Epoch 7949/30000 Training Loss: 0.03854456543922424\n",
      "Epoch 7950/30000 Training Loss: 0.0462389811873436\n",
      "Epoch 7950/30000 Validation Loss: 0.05163460224866867\n",
      "Epoch 7951/30000 Training Loss: 0.050830911844968796\n",
      "Epoch 7952/30000 Training Loss: 0.042513348162174225\n",
      "Epoch 7953/30000 Training Loss: 0.04540710896253586\n",
      "Epoch 7954/30000 Training Loss: 0.05038455128669739\n",
      "Epoch 7955/30000 Training Loss: 0.057656072080135345\n",
      "Epoch 7956/30000 Training Loss: 0.051422368735075\n",
      "Epoch 7957/30000 Training Loss: 0.052155815064907074\n",
      "Epoch 7958/30000 Training Loss: 0.038666266947984695\n",
      "Epoch 7959/30000 Training Loss: 0.04648815467953682\n",
      "Epoch 7960/30000 Training Loss: 0.0455409437417984\n",
      "Epoch 7961/30000 Training Loss: 0.049065880477428436\n",
      "Epoch 7962/30000 Training Loss: 0.05244150012731552\n",
      "Epoch 7963/30000 Training Loss: 0.050109148025512695\n",
      "Epoch 7964/30000 Training Loss: 0.050131164491176605\n",
      "Epoch 7965/30000 Training Loss: 0.04592054337263107\n",
      "Epoch 7966/30000 Training Loss: 0.040464986115694046\n",
      "Epoch 7967/30000 Training Loss: 0.044883739203214645\n",
      "Epoch 7968/30000 Training Loss: 0.04496205598115921\n",
      "Epoch 7969/30000 Training Loss: 0.0672694593667984\n",
      "Epoch 7970/30000 Training Loss: 0.05449480935931206\n",
      "Epoch 7971/30000 Training Loss: 0.053220234811306\n",
      "Epoch 7972/30000 Training Loss: 0.05676325410604477\n",
      "Epoch 7973/30000 Training Loss: 0.05052698776125908\n",
      "Epoch 7974/30000 Training Loss: 0.051748745143413544\n",
      "Epoch 7975/30000 Training Loss: 0.052053648978471756\n",
      "Epoch 7976/30000 Training Loss: 0.05845852941274643\n",
      "Epoch 7977/30000 Training Loss: 0.05577438324689865\n",
      "Epoch 7978/30000 Training Loss: 0.05059118941426277\n",
      "Epoch 7979/30000 Training Loss: 0.06030043959617615\n",
      "Epoch 7980/30000 Training Loss: 0.055015526711940765\n",
      "Epoch 7981/30000 Training Loss: 0.04958347603678703\n",
      "Epoch 7982/30000 Training Loss: 0.05033661052584648\n",
      "Epoch 7983/30000 Training Loss: 0.050073087215423584\n",
      "Epoch 7984/30000 Training Loss: 0.04806996136903763\n",
      "Epoch 7985/30000 Training Loss: 0.040001969784498215\n",
      "Epoch 7986/30000 Training Loss: 0.055411793291568756\n",
      "Epoch 7987/30000 Training Loss: 0.04439321160316467\n",
      "Epoch 7988/30000 Training Loss: 0.04839901626110077\n",
      "Epoch 7989/30000 Training Loss: 0.050058700144290924\n",
      "Epoch 7990/30000 Training Loss: 0.055287621915340424\n",
      "Epoch 7991/30000 Training Loss: 0.04422267898917198\n",
      "Epoch 7992/30000 Training Loss: 0.050198834389448166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7993/30000 Training Loss: 0.050171367824077606\n",
      "Epoch 7994/30000 Training Loss: 0.04980506747961044\n",
      "Epoch 7995/30000 Training Loss: 0.04280635342001915\n",
      "Epoch 7996/30000 Training Loss: 0.047161396592855453\n",
      "Epoch 7997/30000 Training Loss: 0.04737970232963562\n",
      "Epoch 7998/30000 Training Loss: 0.05103147029876709\n",
      "Epoch 7999/30000 Training Loss: 0.05067677050828934\n",
      "Epoch 8000/30000 Training Loss: 0.04800907149910927\n",
      "Epoch 8000/30000 Validation Loss: 0.055091239511966705\n",
      "Epoch 8001/30000 Training Loss: 0.047890059649944305\n",
      "Epoch 8002/30000 Training Loss: 0.04866595193743706\n",
      "Epoch 8003/30000 Training Loss: 0.04548269510269165\n",
      "Epoch 8004/30000 Training Loss: 0.046679943799972534\n",
      "Epoch 8005/30000 Training Loss: 0.05677587538957596\n",
      "Epoch 8006/30000 Training Loss: 0.04940018430352211\n",
      "Epoch 8007/30000 Training Loss: 0.041689369827508926\n",
      "Epoch 8008/30000 Training Loss: 0.05642607808113098\n",
      "Epoch 8009/30000 Training Loss: 0.043188583105802536\n",
      "Epoch 8010/30000 Training Loss: 0.051345814019441605\n",
      "Epoch 8011/30000 Training Loss: 0.058646250516176224\n",
      "Epoch 8012/30000 Training Loss: 0.055911608040332794\n",
      "Epoch 8013/30000 Training Loss: 0.051759909838438034\n",
      "Epoch 8014/30000 Training Loss: 0.05106840655207634\n",
      "Epoch 8015/30000 Training Loss: 0.04264472797513008\n",
      "Epoch 8016/30000 Training Loss: 0.05268700793385506\n",
      "Epoch 8017/30000 Training Loss: 0.04194147512316704\n",
      "Epoch 8018/30000 Training Loss: 0.04689493402838707\n",
      "Epoch 8019/30000 Training Loss: 0.04358922317624092\n",
      "Epoch 8020/30000 Training Loss: 0.05277315899729729\n",
      "Epoch 8021/30000 Training Loss: 0.04994639381766319\n",
      "Epoch 8022/30000 Training Loss: 0.051669955253601074\n",
      "Epoch 8023/30000 Training Loss: 0.05026955530047417\n",
      "Epoch 8024/30000 Training Loss: 0.05099957063794136\n",
      "Epoch 8025/30000 Training Loss: 0.04526665806770325\n",
      "Epoch 8026/30000 Training Loss: 0.04953252151608467\n",
      "Epoch 8027/30000 Training Loss: 0.03844299539923668\n",
      "Epoch 8028/30000 Training Loss: 0.04064524173736572\n",
      "Epoch 8029/30000 Training Loss: 0.05089609697461128\n",
      "Epoch 8030/30000 Training Loss: 0.044304441660642624\n",
      "Epoch 8031/30000 Training Loss: 0.048151321709156036\n",
      "Epoch 8032/30000 Training Loss: 0.04678759723901749\n",
      "Epoch 8033/30000 Training Loss: 0.04812435060739517\n",
      "Epoch 8034/30000 Training Loss: 0.0495319627225399\n",
      "Epoch 8035/30000 Training Loss: 0.04959198087453842\n",
      "Epoch 8036/30000 Training Loss: 0.04817335307598114\n",
      "Epoch 8037/30000 Training Loss: 0.04771191254258156\n",
      "Epoch 8038/30000 Training Loss: 0.04553244635462761\n",
      "Epoch 8039/30000 Training Loss: 0.04802636057138443\n",
      "Epoch 8040/30000 Training Loss: 0.05490412190556526\n",
      "Epoch 8041/30000 Training Loss: 0.039164651185274124\n",
      "Epoch 8042/30000 Training Loss: 0.049444641917943954\n",
      "Epoch 8043/30000 Training Loss: 0.044594619423151016\n",
      "Epoch 8044/30000 Training Loss: 0.04840228334069252\n",
      "Epoch 8045/30000 Training Loss: 0.05165240168571472\n",
      "Epoch 8046/30000 Training Loss: 0.055475007742643356\n",
      "Epoch 8047/30000 Training Loss: 0.04851498082280159\n",
      "Epoch 8048/30000 Training Loss: 0.04407917335629463\n",
      "Epoch 8049/30000 Training Loss: 0.04855100065469742\n",
      "Epoch 8050/30000 Training Loss: 0.040010254830121994\n",
      "Epoch 8050/30000 Validation Loss: 0.04916619509458542\n",
      "Epoch 8051/30000 Training Loss: 0.04812244325876236\n",
      "Epoch 8052/30000 Training Loss: 0.049978017807006836\n",
      "Epoch 8053/30000 Training Loss: 0.060127489268779755\n",
      "Epoch 8054/30000 Training Loss: 0.04138297215104103\n",
      "Epoch 8055/30000 Training Loss: 0.04271058738231659\n",
      "Epoch 8056/30000 Training Loss: 0.0529763288795948\n",
      "Epoch 8057/30000 Training Loss: 0.041866786777973175\n",
      "Epoch 8058/30000 Training Loss: 0.048683397471904755\n",
      "Epoch 8059/30000 Training Loss: 0.04541007801890373\n",
      "Epoch 8060/30000 Training Loss: 0.039667367935180664\n",
      "Epoch 8061/30000 Training Loss: 0.04567631334066391\n",
      "Epoch 8062/30000 Training Loss: 0.04699483513832092\n",
      "Epoch 8063/30000 Training Loss: 0.04769344627857208\n",
      "Epoch 8064/30000 Training Loss: 0.047822628170251846\n",
      "Epoch 8065/30000 Training Loss: 0.04977928102016449\n",
      "Epoch 8066/30000 Training Loss: 0.04122196137905121\n",
      "Epoch 8067/30000 Training Loss: 0.045093826949596405\n",
      "Epoch 8068/30000 Training Loss: 0.04744884371757507\n",
      "Epoch 8069/30000 Training Loss: 0.049986209720373154\n",
      "Epoch 8070/30000 Training Loss: 0.058171145617961884\n",
      "Epoch 8071/30000 Training Loss: 0.05117638781666756\n",
      "Epoch 8072/30000 Training Loss: 0.05020429566502571\n",
      "Epoch 8073/30000 Training Loss: 0.05122951418161392\n",
      "Epoch 8074/30000 Training Loss: 0.05327744409441948\n",
      "Epoch 8075/30000 Training Loss: 0.054632045328617096\n",
      "Epoch 8076/30000 Training Loss: 0.05633391812443733\n",
      "Epoch 8077/30000 Training Loss: 0.0427846722304821\n",
      "Epoch 8078/30000 Training Loss: 0.049299776554107666\n",
      "Epoch 8079/30000 Training Loss: 0.04925607517361641\n",
      "Epoch 8080/30000 Training Loss: 0.051359403878450394\n",
      "Epoch 8081/30000 Training Loss: 0.05223369598388672\n",
      "Epoch 8082/30000 Training Loss: 0.04708515480160713\n",
      "Epoch 8083/30000 Training Loss: 0.051700688898563385\n",
      "Epoch 8084/30000 Training Loss: 0.041867729276418686\n",
      "Epoch 8085/30000 Training Loss: 0.05236608907580376\n",
      "Epoch 8086/30000 Training Loss: 0.04767393693327904\n",
      "Epoch 8087/30000 Training Loss: 0.04923251271247864\n",
      "Epoch 8088/30000 Training Loss: 0.044437192380428314\n",
      "Epoch 8089/30000 Training Loss: 0.04891089349985123\n",
      "Epoch 8090/30000 Training Loss: 0.04418200999498367\n",
      "Epoch 8091/30000 Training Loss: 0.04380740225315094\n",
      "Epoch 8092/30000 Training Loss: 0.05033589527010918\n",
      "Epoch 8093/30000 Training Loss: 0.044075846672058105\n",
      "Epoch 8094/30000 Training Loss: 0.04995735362172127\n",
      "Epoch 8095/30000 Training Loss: 0.05238433927297592\n",
      "Epoch 8096/30000 Training Loss: 0.048269860446453094\n",
      "Epoch 8097/30000 Training Loss: 0.05325932428240776\n",
      "Epoch 8098/30000 Training Loss: 0.04860807955265045\n",
      "Epoch 8099/30000 Training Loss: 0.05245925113558769\n",
      "Epoch 8100/30000 Training Loss: 0.0504239983856678\n",
      "Epoch 8100/30000 Validation Loss: 0.05292794108390808\n",
      "Epoch 8101/30000 Training Loss: 0.05090826749801636\n",
      "Epoch 8102/30000 Training Loss: 0.04961841553449631\n",
      "Epoch 8103/30000 Training Loss: 0.04817134886980057\n",
      "Epoch 8104/30000 Training Loss: 0.051240790635347366\n",
      "Epoch 8105/30000 Training Loss: 0.04336024820804596\n",
      "Epoch 8106/30000 Training Loss: 0.04622577875852585\n",
      "Epoch 8107/30000 Training Loss: 0.05022939294576645\n",
      "Epoch 8108/30000 Training Loss: 0.04736074060201645\n",
      "Epoch 8109/30000 Training Loss: 0.05405223369598389\n",
      "Epoch 8110/30000 Training Loss: 0.0455746129155159\n",
      "Epoch 8111/30000 Training Loss: 0.03837541490793228\n",
      "Epoch 8112/30000 Training Loss: 0.04580114036798477\n",
      "Epoch 8113/30000 Training Loss: 0.04946282505989075\n",
      "Epoch 8114/30000 Training Loss: 0.04805867001414299\n",
      "Epoch 8115/30000 Training Loss: 0.05033542960882187\n",
      "Epoch 8116/30000 Training Loss: 0.05178647115826607\n",
      "Epoch 8117/30000 Training Loss: 0.044694576412439346\n",
      "Epoch 8118/30000 Training Loss: 0.05822194740176201\n",
      "Epoch 8119/30000 Training Loss: 0.04528838396072388\n",
      "Epoch 8120/30000 Training Loss: 0.05170553922653198\n",
      "Epoch 8121/30000 Training Loss: 0.04697548225522041\n",
      "Epoch 8122/30000 Training Loss: 0.046977538615465164\n",
      "Epoch 8123/30000 Training Loss: 0.05596635490655899\n",
      "Epoch 8124/30000 Training Loss: 0.04429195448756218\n",
      "Epoch 8125/30000 Training Loss: 0.04839398339390755\n",
      "Epoch 8126/30000 Training Loss: 0.05453705042600632\n",
      "Epoch 8127/30000 Training Loss: 0.04892442002892494\n",
      "Epoch 8128/30000 Training Loss: 0.04711846262216568\n",
      "Epoch 8129/30000 Training Loss: 0.04901394248008728\n",
      "Epoch 8130/30000 Training Loss: 0.04605501890182495\n",
      "Epoch 8131/30000 Training Loss: 0.05090253800153732\n",
      "Epoch 8132/30000 Training Loss: 0.046213068068027496\n",
      "Epoch 8133/30000 Training Loss: 0.049531154334545135\n",
      "Epoch 8134/30000 Training Loss: 0.04270942509174347\n",
      "Epoch 8135/30000 Training Loss: 0.05695972964167595\n",
      "Epoch 8136/30000 Training Loss: 0.05396096035838127\n",
      "Epoch 8137/30000 Training Loss: 0.036839794367551804\n",
      "Epoch 8138/30000 Training Loss: 0.04543431103229523\n",
      "Epoch 8139/30000 Training Loss: 0.04747812822461128\n",
      "Epoch 8140/30000 Training Loss: 0.04144283011555672\n",
      "Epoch 8141/30000 Training Loss: 0.04782982915639877\n",
      "Epoch 8142/30000 Training Loss: 0.04556573927402496\n",
      "Epoch 8143/30000 Training Loss: 0.04666105657815933\n",
      "Epoch 8144/30000 Training Loss: 0.047587063163518906\n",
      "Epoch 8145/30000 Training Loss: 0.04625846445560455\n",
      "Epoch 8146/30000 Training Loss: 0.035121120512485504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8147/30000 Training Loss: 0.04920843616127968\n",
      "Epoch 8148/30000 Training Loss: 0.06027204915881157\n",
      "Epoch 8149/30000 Training Loss: 0.0523928701877594\n",
      "Epoch 8150/30000 Training Loss: 0.04713468253612518\n",
      "Epoch 8150/30000 Validation Loss: 0.04306928813457489\n",
      "Epoch 8151/30000 Training Loss: 0.04098517820239067\n",
      "Epoch 8152/30000 Training Loss: 0.05269899219274521\n",
      "Epoch 8153/30000 Training Loss: 0.04950939491391182\n",
      "Epoch 8154/30000 Training Loss: 0.054726313799619675\n",
      "Epoch 8155/30000 Training Loss: 0.05377126485109329\n",
      "Epoch 8156/30000 Training Loss: 0.05015828087925911\n",
      "Epoch 8157/30000 Training Loss: 0.04702700302004814\n",
      "Epoch 8158/30000 Training Loss: 0.0471951961517334\n",
      "Epoch 8159/30000 Training Loss: 0.04739655554294586\n",
      "Epoch 8160/30000 Training Loss: 0.06384718418121338\n",
      "Epoch 8161/30000 Training Loss: 0.051448483020067215\n",
      "Epoch 8162/30000 Training Loss: 0.054688937962055206\n",
      "Epoch 8163/30000 Training Loss: 0.05678940936923027\n",
      "Epoch 8164/30000 Training Loss: 0.04683119058609009\n",
      "Epoch 8165/30000 Training Loss: 0.05098160356283188\n",
      "Epoch 8166/30000 Training Loss: 0.055350594222545624\n",
      "Epoch 8167/30000 Training Loss: 0.047366149723529816\n",
      "Epoch 8168/30000 Training Loss: 0.047344110906124115\n",
      "Epoch 8169/30000 Training Loss: 0.04232446104288101\n",
      "Epoch 8170/30000 Training Loss: 0.05320395156741142\n",
      "Epoch 8171/30000 Training Loss: 0.04582999274134636\n",
      "Epoch 8172/30000 Training Loss: 0.04672571271657944\n",
      "Epoch 8173/30000 Training Loss: 0.03983209282159805\n",
      "Epoch 8174/30000 Training Loss: 0.04666773974895477\n",
      "Epoch 8175/30000 Training Loss: 0.061996616423130035\n",
      "Epoch 8176/30000 Training Loss: 0.04536154121160507\n",
      "Epoch 8177/30000 Training Loss: 0.04628812149167061\n",
      "Epoch 8178/30000 Training Loss: 0.04148160293698311\n",
      "Epoch 8179/30000 Training Loss: 0.042289070785045624\n",
      "Epoch 8180/30000 Training Loss: 0.04233311489224434\n",
      "Epoch 8181/30000 Training Loss: 0.0395035594701767\n",
      "Epoch 8182/30000 Training Loss: 0.03941820561885834\n",
      "Epoch 8183/30000 Training Loss: 0.043047696352005005\n",
      "Epoch 8184/30000 Training Loss: 0.04079588130116463\n",
      "Epoch 8185/30000 Training Loss: 0.05255593731999397\n",
      "Epoch 8186/30000 Training Loss: 0.05529831722378731\n",
      "Epoch 8187/30000 Training Loss: 0.04335525631904602\n",
      "Epoch 8188/30000 Training Loss: 0.04615107923746109\n",
      "Epoch 8189/30000 Training Loss: 0.04263201728463173\n",
      "Epoch 8190/30000 Training Loss: 0.05506693571805954\n",
      "Epoch 8191/30000 Training Loss: 0.052007414400577545\n",
      "Epoch 8192/30000 Training Loss: 0.04435114562511444\n",
      "Epoch 8193/30000 Training Loss: 0.046353667974472046\n",
      "Epoch 8194/30000 Training Loss: 0.04917965456843376\n",
      "Epoch 8195/30000 Training Loss: 0.04195985198020935\n",
      "Epoch 8196/30000 Training Loss: 0.04251507297158241\n",
      "Epoch 8197/30000 Training Loss: 0.05129565671086311\n",
      "Epoch 8198/30000 Training Loss: 0.050327368080616\n",
      "Epoch 8199/30000 Training Loss: 0.03824940323829651\n",
      "Epoch 8200/30000 Training Loss: 0.05576357990503311\n",
      "Epoch 8200/30000 Validation Loss: 0.047649603337049484\n",
      "Epoch 8201/30000 Training Loss: 0.04872699826955795\n",
      "Epoch 8202/30000 Training Loss: 0.0430879108607769\n",
      "Epoch 8203/30000 Training Loss: 0.06150816008448601\n",
      "Epoch 8204/30000 Training Loss: 0.06250067800283432\n",
      "Epoch 8205/30000 Training Loss: 0.043196674436330795\n",
      "Epoch 8206/30000 Training Loss: 0.045856427401304245\n",
      "Epoch 8207/30000 Training Loss: 0.05503439903259277\n",
      "Epoch 8208/30000 Training Loss: 0.04830033332109451\n",
      "Epoch 8209/30000 Training Loss: 0.05031339079141617\n",
      "Epoch 8210/30000 Training Loss: 0.05176839232444763\n",
      "Epoch 8211/30000 Training Loss: 0.046677835285663605\n",
      "Epoch 8212/30000 Training Loss: 0.047189805656671524\n",
      "Epoch 8213/30000 Training Loss: 0.058064837008714676\n",
      "Epoch 8214/30000 Training Loss: 0.0524754635989666\n",
      "Epoch 8215/30000 Training Loss: 0.05018407106399536\n",
      "Epoch 8216/30000 Training Loss: 0.05255214124917984\n",
      "Epoch 8217/30000 Training Loss: 0.060172490775585175\n",
      "Epoch 8218/30000 Training Loss: 0.052534811198711395\n",
      "Epoch 8219/30000 Training Loss: 0.05284612253308296\n",
      "Epoch 8220/30000 Training Loss: 0.05720207095146179\n",
      "Epoch 8221/30000 Training Loss: 0.044387008994817734\n",
      "Epoch 8222/30000 Training Loss: 0.048478756099939346\n",
      "Epoch 8223/30000 Training Loss: 0.052094198763370514\n",
      "Epoch 8224/30000 Training Loss: 0.04372941330075264\n",
      "Epoch 8225/30000 Training Loss: 0.05267719179391861\n",
      "Epoch 8226/30000 Training Loss: 0.05104625225067139\n",
      "Epoch 8227/30000 Training Loss: 0.04003234952688217\n",
      "Epoch 8228/30000 Training Loss: 0.057895489037036896\n",
      "Epoch 8229/30000 Training Loss: 0.04262511059641838\n",
      "Epoch 8230/30000 Training Loss: 0.04278504103422165\n",
      "Epoch 8231/30000 Training Loss: 0.05399869754910469\n",
      "Epoch 8232/30000 Training Loss: 0.049071259796619415\n",
      "Epoch 8233/30000 Training Loss: 0.04267631843686104\n",
      "Epoch 8234/30000 Training Loss: 0.04050561413168907\n",
      "Epoch 8235/30000 Training Loss: 0.048955779522657394\n",
      "Epoch 8236/30000 Training Loss: 0.04746090993285179\n",
      "Epoch 8237/30000 Training Loss: 0.046776823699474335\n",
      "Epoch 8238/30000 Training Loss: 0.049944084137678146\n",
      "Epoch 8239/30000 Training Loss: 0.044487643986940384\n",
      "Epoch 8240/30000 Training Loss: 0.04811270162463188\n",
      "Epoch 8241/30000 Training Loss: 0.04803367331624031\n",
      "Epoch 8242/30000 Training Loss: 0.04429873079061508\n",
      "Epoch 8243/30000 Training Loss: 0.051375459879636765\n",
      "Epoch 8244/30000 Training Loss: 0.0477161779999733\n",
      "Epoch 8245/30000 Training Loss: 0.04688156396150589\n",
      "Epoch 8246/30000 Training Loss: 0.04825957864522934\n",
      "Epoch 8247/30000 Training Loss: 0.04646918922662735\n",
      "Epoch 8248/30000 Training Loss: 0.047099769115448\n",
      "Epoch 8249/30000 Training Loss: 0.049358218908309937\n",
      "Epoch 8250/30000 Training Loss: 0.04740285128355026\n",
      "Epoch 8250/30000 Validation Loss: 0.047455497086048126\n",
      "Epoch 8251/30000 Training Loss: 0.05029374361038208\n",
      "Epoch 8252/30000 Training Loss: 0.05297327786684036\n",
      "Epoch 8253/30000 Training Loss: 0.050465457141399384\n",
      "Epoch 8254/30000 Training Loss: 0.041556764394044876\n",
      "Epoch 8255/30000 Training Loss: 0.0422944650053978\n",
      "Epoch 8256/30000 Training Loss: 0.060377947986125946\n",
      "Epoch 8257/30000 Training Loss: 0.05045050382614136\n",
      "Epoch 8258/30000 Training Loss: 0.05101172998547554\n",
      "Epoch 8259/30000 Training Loss: 0.048468317836523056\n",
      "Epoch 8260/30000 Training Loss: 0.04662006348371506\n",
      "Epoch 8261/30000 Training Loss: 0.042713187634944916\n",
      "Epoch 8262/30000 Training Loss: 0.043498363345861435\n",
      "Epoch 8263/30000 Training Loss: 0.04990628734230995\n",
      "Epoch 8264/30000 Training Loss: 0.046876877546310425\n",
      "Epoch 8265/30000 Training Loss: 0.05525027960538864\n",
      "Epoch 8266/30000 Training Loss: 0.046488940715789795\n",
      "Epoch 8267/30000 Training Loss: 0.0462845079600811\n",
      "Epoch 8268/30000 Training Loss: 0.04634882137179375\n",
      "Epoch 8269/30000 Training Loss: 0.0520138144493103\n",
      "Epoch 8270/30000 Training Loss: 0.048376403748989105\n",
      "Epoch 8271/30000 Training Loss: 0.04673858731985092\n",
      "Epoch 8272/30000 Training Loss: 0.048404403030872345\n",
      "Epoch 8273/30000 Training Loss: 0.04439883679151535\n",
      "Epoch 8274/30000 Training Loss: 0.0418442077934742\n",
      "Epoch 8275/30000 Training Loss: 0.0465850792825222\n",
      "Epoch 8276/30000 Training Loss: 0.044457003474235535\n",
      "Epoch 8277/30000 Training Loss: 0.049848996102809906\n",
      "Epoch 8278/30000 Training Loss: 0.046700410544872284\n",
      "Epoch 8279/30000 Training Loss: 0.04586437717080116\n",
      "Epoch 8280/30000 Training Loss: 0.048025283962488174\n",
      "Epoch 8281/30000 Training Loss: 0.04746448993682861\n",
      "Epoch 8282/30000 Training Loss: 0.05377080291509628\n",
      "Epoch 8283/30000 Training Loss: 0.03771248459815979\n",
      "Epoch 8284/30000 Training Loss: 0.0507381334900856\n",
      "Epoch 8285/30000 Training Loss: 0.05294134095311165\n",
      "Epoch 8286/30000 Training Loss: 0.0575021393597126\n",
      "Epoch 8287/30000 Training Loss: 0.055186741054058075\n",
      "Epoch 8288/30000 Training Loss: 0.04536477103829384\n",
      "Epoch 8289/30000 Training Loss: 0.04139119014143944\n",
      "Epoch 8290/30000 Training Loss: 0.05051315575838089\n",
      "Epoch 8291/30000 Training Loss: 0.040088072419166565\n",
      "Epoch 8292/30000 Training Loss: 0.043841615319252014\n",
      "Epoch 8293/30000 Training Loss: 0.0436704084277153\n",
      "Epoch 8294/30000 Training Loss: 0.05468607693910599\n",
      "Epoch 8295/30000 Training Loss: 0.05440651252865791\n",
      "Epoch 8296/30000 Training Loss: 0.04266127198934555\n",
      "Epoch 8297/30000 Training Loss: 0.05333821848034859\n",
      "Epoch 8298/30000 Training Loss: 0.046937502920627594\n",
      "Epoch 8299/30000 Training Loss: 0.04857255145907402\n",
      "Epoch 8300/30000 Training Loss: 0.03920619562268257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8300/30000 Validation Loss: 0.051079075783491135\n",
      "Epoch 8301/30000 Training Loss: 0.04829688370227814\n",
      "Epoch 8302/30000 Training Loss: 0.051533423364162445\n",
      "Epoch 8303/30000 Training Loss: 0.05618045851588249\n",
      "Epoch 8304/30000 Training Loss: 0.04376550391316414\n",
      "Epoch 8305/30000 Training Loss: 0.04756353050470352\n",
      "Epoch 8306/30000 Training Loss: 0.056101731956005096\n",
      "Epoch 8307/30000 Training Loss: 0.05256100371479988\n",
      "Epoch 8308/30000 Training Loss: 0.046727973967790604\n",
      "Epoch 8309/30000 Training Loss: 0.05102013424038887\n",
      "Epoch 8310/30000 Training Loss: 0.050546448677778244\n",
      "Epoch 8311/30000 Training Loss: 0.04669268801808357\n",
      "Epoch 8312/30000 Training Loss: 0.05026152729988098\n",
      "Epoch 8313/30000 Training Loss: 0.053292982280254364\n",
      "Epoch 8314/30000 Training Loss: 0.04054885357618332\n",
      "Epoch 8315/30000 Training Loss: 0.047892894595861435\n",
      "Epoch 8316/30000 Training Loss: 0.05462395027279854\n",
      "Epoch 8317/30000 Training Loss: 0.052715908735990524\n",
      "Epoch 8318/30000 Training Loss: 0.05502360314130783\n",
      "Epoch 8319/30000 Training Loss: 0.04758533835411072\n",
      "Epoch 8320/30000 Training Loss: 0.04007406160235405\n",
      "Epoch 8321/30000 Training Loss: 0.04880187660455704\n",
      "Epoch 8322/30000 Training Loss: 0.06393108516931534\n",
      "Epoch 8323/30000 Training Loss: 0.05082010477781296\n",
      "Epoch 8324/30000 Training Loss: 0.04961133748292923\n",
      "Epoch 8325/30000 Training Loss: 0.05192200466990471\n",
      "Epoch 8326/30000 Training Loss: 0.0438404455780983\n",
      "Epoch 8327/30000 Training Loss: 0.0445440299808979\n",
      "Epoch 8328/30000 Training Loss: 0.04488952457904816\n",
      "Epoch 8329/30000 Training Loss: 0.04680275917053223\n",
      "Epoch 8330/30000 Training Loss: 0.049723416566848755\n",
      "Epoch 8331/30000 Training Loss: 0.04959872364997864\n",
      "Epoch 8332/30000 Training Loss: 0.053211767226457596\n",
      "Epoch 8333/30000 Training Loss: 0.04854772984981537\n",
      "Epoch 8334/30000 Training Loss: 0.04350525885820389\n",
      "Epoch 8335/30000 Training Loss: 0.045080672949552536\n",
      "Epoch 8336/30000 Training Loss: 0.04199519008398056\n",
      "Epoch 8337/30000 Training Loss: 0.048852819949388504\n",
      "Epoch 8338/30000 Training Loss: 0.04875892028212547\n",
      "Epoch 8339/30000 Training Loss: 0.0388355478644371\n",
      "Epoch 8340/30000 Training Loss: 0.05431216210126877\n",
      "Epoch 8341/30000 Training Loss: 0.05587202310562134\n",
      "Epoch 8342/30000 Training Loss: 0.04756031185388565\n",
      "Epoch 8343/30000 Training Loss: 0.038906313478946686\n",
      "Epoch 8344/30000 Training Loss: 0.04954565316438675\n",
      "Epoch 8345/30000 Training Loss: 0.05058428645133972\n",
      "Epoch 8346/30000 Training Loss: 0.042764708399772644\n",
      "Epoch 8347/30000 Training Loss: 0.0462702177464962\n",
      "Epoch 8348/30000 Training Loss: 0.04167945310473442\n",
      "Epoch 8349/30000 Training Loss: 0.04318581894040108\n",
      "Epoch 8350/30000 Training Loss: 0.05206745117902756\n",
      "Epoch 8350/30000 Validation Loss: 0.04927777126431465\n",
      "Epoch 8351/30000 Training Loss: 0.05111677199602127\n",
      "Epoch 8352/30000 Training Loss: 0.054170649498701096\n",
      "Epoch 8353/30000 Training Loss: 0.05174452066421509\n",
      "Epoch 8354/30000 Training Loss: 0.055461227893829346\n",
      "Epoch 8355/30000 Training Loss: 0.04465753585100174\n",
      "Epoch 8356/30000 Training Loss: 0.04024462029337883\n",
      "Epoch 8357/30000 Training Loss: 0.05192293971776962\n",
      "Epoch 8358/30000 Training Loss: 0.04833625629544258\n",
      "Epoch 8359/30000 Training Loss: 0.038271982222795486\n",
      "Epoch 8360/30000 Training Loss: 0.051049791276454926\n",
      "Epoch 8361/30000 Training Loss: 0.050915174186229706\n",
      "Epoch 8362/30000 Training Loss: 0.050031524151563644\n",
      "Epoch 8363/30000 Training Loss: 0.04317355901002884\n",
      "Epoch 8364/30000 Training Loss: 0.0530221052467823\n",
      "Epoch 8365/30000 Training Loss: 0.04864062741398811\n",
      "Epoch 8366/30000 Training Loss: 0.04784427210688591\n",
      "Epoch 8367/30000 Training Loss: 0.044977374374866486\n",
      "Epoch 8368/30000 Training Loss: 0.04538014531135559\n",
      "Epoch 8369/30000 Training Loss: 0.05336318165063858\n",
      "Epoch 8370/30000 Training Loss: 0.048984892666339874\n",
      "Epoch 8371/30000 Training Loss: 0.053087733685970306\n",
      "Epoch 8372/30000 Training Loss: 0.04282591864466667\n",
      "Epoch 8373/30000 Training Loss: 0.04521341249346733\n",
      "Epoch 8374/30000 Training Loss: 0.04031509906053543\n",
      "Epoch 8375/30000 Training Loss: 0.054163385182619095\n",
      "Epoch 8376/30000 Training Loss: 0.052457571029663086\n",
      "Epoch 8377/30000 Training Loss: 0.04685729742050171\n",
      "Epoch 8378/30000 Training Loss: 0.051048170775175095\n",
      "Epoch 8379/30000 Training Loss: 0.05267541483044624\n",
      "Epoch 8380/30000 Training Loss: 0.04548347741365433\n",
      "Epoch 8381/30000 Training Loss: 0.05340230464935303\n",
      "Epoch 8382/30000 Training Loss: 0.046792853623628616\n",
      "Epoch 8383/30000 Training Loss: 0.04844789206981659\n",
      "Epoch 8384/30000 Training Loss: 0.049160249531269073\n",
      "Epoch 8385/30000 Training Loss: 0.05097208172082901\n",
      "Epoch 8386/30000 Training Loss: 0.03755833953619003\n",
      "Epoch 8387/30000 Training Loss: 0.052659790962934494\n",
      "Epoch 8388/30000 Training Loss: 0.042094916105270386\n",
      "Epoch 8389/30000 Training Loss: 0.0459950789809227\n",
      "Epoch 8390/30000 Training Loss: 0.04557175561785698\n",
      "Epoch 8391/30000 Training Loss: 0.041500695049762726\n",
      "Epoch 8392/30000 Training Loss: 0.04228397458791733\n",
      "Epoch 8393/30000 Training Loss: 0.04735957458615303\n",
      "Epoch 8394/30000 Training Loss: 0.04732758179306984\n",
      "Epoch 8395/30000 Training Loss: 0.053860682994127274\n",
      "Epoch 8396/30000 Training Loss: 0.045957550406455994\n",
      "Epoch 8397/30000 Training Loss: 0.048641107976436615\n",
      "Epoch 8398/30000 Training Loss: 0.046951450407505035\n",
      "Epoch 8399/30000 Training Loss: 0.05139761418104172\n",
      "Epoch 8400/30000 Training Loss: 0.047984763979911804\n",
      "Epoch 8400/30000 Validation Loss: 0.04492788761854172\n",
      "Epoch 8401/30000 Training Loss: 0.04192063584923744\n",
      "Epoch 8402/30000 Training Loss: 0.041455671191215515\n",
      "Epoch 8403/30000 Training Loss: 0.0546262152493\n",
      "Epoch 8404/30000 Training Loss: 0.049205731600522995\n",
      "Epoch 8405/30000 Training Loss: 0.04739183932542801\n",
      "Epoch 8406/30000 Training Loss: 0.058404285460710526\n",
      "Epoch 8407/30000 Training Loss: 0.0461050346493721\n",
      "Epoch 8408/30000 Training Loss: 0.05629081279039383\n",
      "Epoch 8409/30000 Training Loss: 0.04110521823167801\n",
      "Epoch 8410/30000 Training Loss: 0.04597362503409386\n",
      "Epoch 8411/30000 Training Loss: 0.049433790147304535\n",
      "Epoch 8412/30000 Training Loss: 0.04843909665942192\n",
      "Epoch 8413/30000 Training Loss: 0.0508553571999073\n",
      "Epoch 8414/30000 Training Loss: 0.046273358166217804\n",
      "Epoch 8415/30000 Training Loss: 0.04492097347974777\n",
      "Epoch 8416/30000 Training Loss: 0.04630769416689873\n",
      "Epoch 8417/30000 Training Loss: 0.04182370752096176\n",
      "Epoch 8418/30000 Training Loss: 0.04388809949159622\n",
      "Epoch 8419/30000 Training Loss: 0.052549391984939575\n",
      "Epoch 8420/30000 Training Loss: 0.03958464786410332\n",
      "Epoch 8421/30000 Training Loss: 0.050613321363925934\n",
      "Epoch 8422/30000 Training Loss: 0.042028896510601044\n",
      "Epoch 8423/30000 Training Loss: 0.0501626618206501\n",
      "Epoch 8424/30000 Training Loss: 0.05239393189549446\n",
      "Epoch 8425/30000 Training Loss: 0.05177951976656914\n",
      "Epoch 8426/30000 Training Loss: 0.051838524639606476\n",
      "Epoch 8427/30000 Training Loss: 0.04417774826288223\n",
      "Epoch 8428/30000 Training Loss: 0.0461149625480175\n",
      "Epoch 8429/30000 Training Loss: 0.04792008548974991\n",
      "Epoch 8430/30000 Training Loss: 0.04262871667742729\n",
      "Epoch 8431/30000 Training Loss: 0.044664107263088226\n",
      "Epoch 8432/30000 Training Loss: 0.04428352043032646\n",
      "Epoch 8433/30000 Training Loss: 0.047857753932476044\n",
      "Epoch 8434/30000 Training Loss: 0.05857830494642258\n",
      "Epoch 8435/30000 Training Loss: 0.05515853315591812\n",
      "Epoch 8436/30000 Training Loss: 0.043221134692430496\n",
      "Epoch 8437/30000 Training Loss: 0.03874571993947029\n",
      "Epoch 8438/30000 Training Loss: 0.051275260746479034\n",
      "Epoch 8439/30000 Training Loss: 0.04514622315764427\n",
      "Epoch 8440/30000 Training Loss: 0.05100037530064583\n",
      "Epoch 8441/30000 Training Loss: 0.047113656997680664\n",
      "Epoch 8442/30000 Training Loss: 0.04462859034538269\n",
      "Epoch 8443/30000 Training Loss: 0.051377881318330765\n",
      "Epoch 8444/30000 Training Loss: 0.046498846262693405\n",
      "Epoch 8445/30000 Training Loss: 0.051621921360492706\n",
      "Epoch 8446/30000 Training Loss: 0.045296791940927505\n",
      "Epoch 8447/30000 Training Loss: 0.04844900965690613\n",
      "Epoch 8448/30000 Training Loss: 0.04735685512423515\n",
      "Epoch 8449/30000 Training Loss: 0.04773768037557602\n",
      "Epoch 8450/30000 Training Loss: 0.04929501190781593\n",
      "Epoch 8450/30000 Validation Loss: 0.04160086810588837\n",
      "Epoch 8451/30000 Training Loss: 0.05500977486371994\n",
      "Epoch 8452/30000 Training Loss: 0.05139652639627457\n",
      "Epoch 8453/30000 Training Loss: 0.052813075482845306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8454/30000 Training Loss: 0.0468735471367836\n",
      "Epoch 8455/30000 Training Loss: 0.051170069724321365\n",
      "Epoch 8456/30000 Training Loss: 0.04394873231649399\n",
      "Epoch 8457/30000 Training Loss: 0.05025193840265274\n",
      "Epoch 8458/30000 Training Loss: 0.05073975399136543\n",
      "Epoch 8459/30000 Training Loss: 0.04548872634768486\n",
      "Epoch 8460/30000 Training Loss: 0.047255367040634155\n",
      "Epoch 8461/30000 Training Loss: 0.04303155839443207\n",
      "Epoch 8462/30000 Training Loss: 0.046626292169094086\n",
      "Epoch 8463/30000 Training Loss: 0.04553034156560898\n",
      "Epoch 8464/30000 Training Loss: 0.04511989280581474\n",
      "Epoch 8465/30000 Training Loss: 0.04837385565042496\n",
      "Epoch 8466/30000 Training Loss: 0.04413927346467972\n",
      "Epoch 8467/30000 Training Loss: 0.042782533913850784\n",
      "Epoch 8468/30000 Training Loss: 0.057917285710573196\n",
      "Epoch 8469/30000 Training Loss: 0.05151216313242912\n",
      "Epoch 8470/30000 Training Loss: 0.04344379901885986\n",
      "Epoch 8471/30000 Training Loss: 0.038675419986248016\n",
      "Epoch 8472/30000 Training Loss: 0.04416612163186073\n",
      "Epoch 8473/30000 Training Loss: 0.04800807312130928\n",
      "Epoch 8474/30000 Training Loss: 0.049170076847076416\n",
      "Epoch 8475/30000 Training Loss: 0.04870663210749626\n",
      "Epoch 8476/30000 Training Loss: 0.052555643022060394\n",
      "Epoch 8477/30000 Training Loss: 0.05132285878062248\n",
      "Epoch 8478/30000 Training Loss: 0.04955645278096199\n",
      "Epoch 8479/30000 Training Loss: 0.04747641831636429\n",
      "Epoch 8480/30000 Training Loss: 0.04392946511507034\n",
      "Epoch 8481/30000 Training Loss: 0.046550095081329346\n",
      "Epoch 8482/30000 Training Loss: 0.047856152057647705\n",
      "Epoch 8483/30000 Training Loss: 0.04890042543411255\n",
      "Epoch 8484/30000 Training Loss: 0.05275162309408188\n",
      "Epoch 8485/30000 Training Loss: 0.038440462201833725\n",
      "Epoch 8486/30000 Training Loss: 0.04931539297103882\n",
      "Epoch 8487/30000 Training Loss: 0.042249009013175964\n",
      "Epoch 8488/30000 Training Loss: 0.0438631996512413\n",
      "Epoch 8489/30000 Training Loss: 0.048783086240291595\n",
      "Epoch 8490/30000 Training Loss: 0.048195481300354004\n",
      "Epoch 8491/30000 Training Loss: 0.04311170428991318\n",
      "Epoch 8492/30000 Training Loss: 0.038907960057258606\n",
      "Epoch 8493/30000 Training Loss: 0.045458562672138214\n",
      "Epoch 8494/30000 Training Loss: 0.04107821732759476\n",
      "Epoch 8495/30000 Training Loss: 0.05082521587610245\n",
      "Epoch 8496/30000 Training Loss: 0.048292405903339386\n",
      "Epoch 8497/30000 Training Loss: 0.04500245675444603\n",
      "Epoch 8498/30000 Training Loss: 0.04966947063803673\n",
      "Epoch 8499/30000 Training Loss: 0.059145696461200714\n",
      "Epoch 8500/30000 Training Loss: 0.05042104795575142\n",
      "Epoch 8500/30000 Validation Loss: 0.04753613471984863\n",
      "Epoch 8501/30000 Training Loss: 0.04641272500157356\n",
      "Epoch 8502/30000 Training Loss: 0.051812898367643356\n",
      "Epoch 8503/30000 Training Loss: 0.05409061908721924\n",
      "Epoch 8504/30000 Training Loss: 0.05492795258760452\n",
      "Epoch 8505/30000 Training Loss: 0.047298822551965714\n",
      "Epoch 8506/30000 Training Loss: 0.05118965357542038\n",
      "Epoch 8507/30000 Training Loss: 0.059317510575056076\n",
      "Epoch 8508/30000 Training Loss: 0.04535513371229172\n",
      "Epoch 8509/30000 Training Loss: 0.04757828265428543\n",
      "Epoch 8510/30000 Training Loss: 0.05523594468832016\n",
      "Epoch 8511/30000 Training Loss: 0.055073082447052\n",
      "Epoch 8512/30000 Training Loss: 0.04016752913594246\n",
      "Epoch 8513/30000 Training Loss: 0.04912231117486954\n",
      "Epoch 8514/30000 Training Loss: 0.053071387112140656\n",
      "Epoch 8515/30000 Training Loss: 0.038129907101392746\n",
      "Epoch 8516/30000 Training Loss: 0.049284350126981735\n",
      "Epoch 8517/30000 Training Loss: 0.055279750376939774\n",
      "Epoch 8518/30000 Training Loss: 0.040411584079265594\n",
      "Epoch 8519/30000 Training Loss: 0.043829359114170074\n",
      "Epoch 8520/30000 Training Loss: 0.04337293654680252\n",
      "Epoch 8521/30000 Training Loss: 0.05534027889370918\n",
      "Epoch 8522/30000 Training Loss: 0.04479609429836273\n",
      "Epoch 8523/30000 Training Loss: 0.04508141055703163\n",
      "Epoch 8524/30000 Training Loss: 0.05033249780535698\n",
      "Epoch 8525/30000 Training Loss: 0.05444896221160889\n",
      "Epoch 8526/30000 Training Loss: 0.05020599812269211\n",
      "Epoch 8527/30000 Training Loss: 0.042315009981393814\n",
      "Epoch 8528/30000 Training Loss: 0.04933646321296692\n",
      "Epoch 8529/30000 Training Loss: 0.04704154282808304\n",
      "Epoch 8530/30000 Training Loss: 0.055372536182403564\n",
      "Epoch 8531/30000 Training Loss: 0.04964359849691391\n",
      "Epoch 8532/30000 Training Loss: 0.04703980311751366\n",
      "Epoch 8533/30000 Training Loss: 0.04499133303761482\n",
      "Epoch 8534/30000 Training Loss: 0.04546338692307472\n",
      "Epoch 8535/30000 Training Loss: 0.05195837467908859\n",
      "Epoch 8536/30000 Training Loss: 0.05750005692243576\n",
      "Epoch 8537/30000 Training Loss: 0.04366655275225639\n",
      "Epoch 8538/30000 Training Loss: 0.052826859056949615\n",
      "Epoch 8539/30000 Training Loss: 0.04358016699552536\n",
      "Epoch 8540/30000 Training Loss: 0.04793766513466835\n",
      "Epoch 8541/30000 Training Loss: 0.04799243062734604\n",
      "Epoch 8542/30000 Training Loss: 0.04519865661859512\n",
      "Epoch 8543/30000 Training Loss: 0.04875416308641434\n",
      "Epoch 8544/30000 Training Loss: 0.05074549466371536\n",
      "Epoch 8545/30000 Training Loss: 0.05368056148290634\n",
      "Epoch 8546/30000 Training Loss: 0.042981892824172974\n",
      "Epoch 8547/30000 Training Loss: 0.04707198217511177\n",
      "Epoch 8548/30000 Training Loss: 0.05306591838598251\n",
      "Epoch 8549/30000 Training Loss: 0.0455947071313858\n",
      "Epoch 8550/30000 Training Loss: 0.04260317236185074\n",
      "Epoch 8550/30000 Validation Loss: 0.05082760378718376\n",
      "Epoch 8551/30000 Training Loss: 0.04647309333086014\n",
      "Epoch 8552/30000 Training Loss: 0.05246537923812866\n",
      "Epoch 8553/30000 Training Loss: 0.04802952706813812\n",
      "Epoch 8554/30000 Training Loss: 0.050394076853990555\n",
      "Epoch 8555/30000 Training Loss: 0.04521262273192406\n",
      "Epoch 8556/30000 Training Loss: 0.0515451654791832\n",
      "Epoch 8557/30000 Training Loss: 0.04509358108043671\n",
      "Epoch 8558/30000 Training Loss: 0.05157989263534546\n",
      "Epoch 8559/30000 Training Loss: 0.04047466069459915\n",
      "Epoch 8560/30000 Training Loss: 0.056402526795864105\n",
      "Epoch 8561/30000 Training Loss: 0.040738143026828766\n",
      "Epoch 8562/30000 Training Loss: 0.04869074374437332\n",
      "Epoch 8563/30000 Training Loss: 0.050984032452106476\n",
      "Epoch 8564/30000 Training Loss: 0.05140995234251022\n",
      "Epoch 8565/30000 Training Loss: 0.049591876566410065\n",
      "Epoch 8566/30000 Training Loss: 0.05330489203333855\n",
      "Epoch 8567/30000 Training Loss: 0.05024602264165878\n",
      "Epoch 8568/30000 Training Loss: 0.04885583743453026\n",
      "Epoch 8569/30000 Training Loss: 0.04662463814020157\n",
      "Epoch 8570/30000 Training Loss: 0.048246823251247406\n",
      "Epoch 8571/30000 Training Loss: 0.047007106244564056\n",
      "Epoch 8572/30000 Training Loss: 0.0520760640501976\n",
      "Epoch 8573/30000 Training Loss: 0.05002327635884285\n",
      "Epoch 8574/30000 Training Loss: 0.04217666760087013\n",
      "Epoch 8575/30000 Training Loss: 0.047453343868255615\n",
      "Epoch 8576/30000 Training Loss: 0.048791754990816116\n",
      "Epoch 8577/30000 Training Loss: 0.0450267419219017\n",
      "Epoch 8578/30000 Training Loss: 0.04603639990091324\n",
      "Epoch 8579/30000 Training Loss: 0.040319886058568954\n",
      "Epoch 8580/30000 Training Loss: 0.049854882061481476\n",
      "Epoch 8581/30000 Training Loss: 0.04513629525899887\n",
      "Epoch 8582/30000 Training Loss: 0.04524818807840347\n",
      "Epoch 8583/30000 Training Loss: 0.042502257972955704\n",
      "Epoch 8584/30000 Training Loss: 0.04362102970480919\n",
      "Epoch 8585/30000 Training Loss: 0.045772843062877655\n",
      "Epoch 8586/30000 Training Loss: 0.04897775501012802\n",
      "Epoch 8587/30000 Training Loss: 0.04850126802921295\n",
      "Epoch 8588/30000 Training Loss: 0.05627305433154106\n",
      "Epoch 8589/30000 Training Loss: 0.06214402988553047\n",
      "Epoch 8590/30000 Training Loss: 0.04605228826403618\n",
      "Epoch 8591/30000 Training Loss: 0.044888224452733994\n",
      "Epoch 8592/30000 Training Loss: 0.046229030936956406\n",
      "Epoch 8593/30000 Training Loss: 0.04890844598412514\n",
      "Epoch 8594/30000 Training Loss: 0.048456791788339615\n",
      "Epoch 8595/30000 Training Loss: 0.04277477040886879\n",
      "Epoch 8596/30000 Training Loss: 0.045248936861753464\n",
      "Epoch 8597/30000 Training Loss: 0.05082504823803902\n",
      "Epoch 8598/30000 Training Loss: 0.049430686980485916\n",
      "Epoch 8599/30000 Training Loss: 0.04654134809970856\n",
      "Epoch 8600/30000 Training Loss: 0.045384135097265244\n",
      "Epoch 8600/30000 Validation Loss: 0.046486932784318924\n",
      "Epoch 8601/30000 Training Loss: 0.04866266995668411\n",
      "Epoch 8602/30000 Training Loss: 0.05748239904642105\n",
      "Epoch 8603/30000 Training Loss: 0.05668637156486511\n",
      "Epoch 8604/30000 Training Loss: 0.04340509697794914\n",
      "Epoch 8605/30000 Training Loss: 0.05141504853963852\n",
      "Epoch 8606/30000 Training Loss: 0.059960633516311646\n",
      "Epoch 8607/30000 Training Loss: 0.05205864459276199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8608/30000 Training Loss: 0.04491221159696579\n",
      "Epoch 8609/30000 Training Loss: 0.057357121258974075\n",
      "Epoch 8610/30000 Training Loss: 0.04759387671947479\n",
      "Epoch 8611/30000 Training Loss: 0.05036700889468193\n",
      "Epoch 8612/30000 Training Loss: 0.04324593394994736\n",
      "Epoch 8613/30000 Training Loss: 0.045883338898420334\n",
      "Epoch 8614/30000 Training Loss: 0.045277275145053864\n",
      "Epoch 8615/30000 Training Loss: 0.055065054446458817\n",
      "Epoch 8616/30000 Training Loss: 0.048787131905555725\n",
      "Epoch 8617/30000 Training Loss: 0.04280854016542435\n",
      "Epoch 8618/30000 Training Loss: 0.050090719014406204\n",
      "Epoch 8619/30000 Training Loss: 0.05120665952563286\n",
      "Epoch 8620/30000 Training Loss: 0.04801061749458313\n",
      "Epoch 8621/30000 Training Loss: 0.044939976185560226\n",
      "Epoch 8622/30000 Training Loss: 0.05770956352353096\n",
      "Epoch 8623/30000 Training Loss: 0.0465979240834713\n",
      "Epoch 8624/30000 Training Loss: 0.04933594912290573\n",
      "Epoch 8625/30000 Training Loss: 0.04285581782460213\n",
      "Epoch 8626/30000 Training Loss: 0.04019554331898689\n",
      "Epoch 8627/30000 Training Loss: 0.05507413670420647\n",
      "Epoch 8628/30000 Training Loss: 0.05292484164237976\n",
      "Epoch 8629/30000 Training Loss: 0.05341944843530655\n",
      "Epoch 8630/30000 Training Loss: 0.044128675013780594\n",
      "Epoch 8631/30000 Training Loss: 0.04316316172480583\n",
      "Epoch 8632/30000 Training Loss: 0.046794526278972626\n",
      "Epoch 8633/30000 Training Loss: 0.04814109951257706\n",
      "Epoch 8634/30000 Training Loss: 0.0446518138051033\n",
      "Epoch 8635/30000 Training Loss: 0.049689531326293945\n",
      "Epoch 8636/30000 Training Loss: 0.043223705142736435\n",
      "Epoch 8637/30000 Training Loss: 0.050470560789108276\n",
      "Epoch 8638/30000 Training Loss: 0.053073983639478683\n",
      "Epoch 8639/30000 Training Loss: 0.04251120612025261\n",
      "Epoch 8640/30000 Training Loss: 0.04630813002586365\n",
      "Epoch 8641/30000 Training Loss: 0.04769204929471016\n",
      "Epoch 8642/30000 Training Loss: 0.0445857048034668\n",
      "Epoch 8643/30000 Training Loss: 0.05472399666905403\n",
      "Epoch 8644/30000 Training Loss: 0.04300294071435928\n",
      "Epoch 8645/30000 Training Loss: 0.0515996590256691\n",
      "Epoch 8646/30000 Training Loss: 0.04925142601132393\n",
      "Epoch 8647/30000 Training Loss: 0.04875202849507332\n",
      "Epoch 8648/30000 Training Loss: 0.05250873416662216\n",
      "Epoch 8649/30000 Training Loss: 0.04819134995341301\n",
      "Epoch 8650/30000 Training Loss: 0.04608713462948799\n",
      "Epoch 8650/30000 Validation Loss: 0.045346949249506\n",
      "Epoch 8651/30000 Training Loss: 0.04899201914668083\n",
      "Epoch 8652/30000 Training Loss: 0.046114105731248856\n",
      "Epoch 8653/30000 Training Loss: 0.0464315228164196\n",
      "Epoch 8654/30000 Training Loss: 0.04422679543495178\n",
      "Epoch 8655/30000 Training Loss: 0.03901553899049759\n",
      "Epoch 8656/30000 Training Loss: 0.04482382908463478\n",
      "Epoch 8657/30000 Training Loss: 0.047811783850193024\n",
      "Epoch 8658/30000 Training Loss: 0.051364727318286896\n",
      "Epoch 8659/30000 Training Loss: 0.05121776461601257\n",
      "Epoch 8660/30000 Training Loss: 0.045694973319768906\n",
      "Epoch 8661/30000 Training Loss: 0.052856214344501495\n",
      "Epoch 8662/30000 Training Loss: 0.04551375284790993\n",
      "Epoch 8663/30000 Training Loss: 0.051786743104457855\n",
      "Epoch 8664/30000 Training Loss: 0.050737954676151276\n",
      "Epoch 8665/30000 Training Loss: 0.044365447014570236\n",
      "Epoch 8666/30000 Training Loss: 0.04158053919672966\n",
      "Epoch 8667/30000 Training Loss: 0.05673335865139961\n",
      "Epoch 8668/30000 Training Loss: 0.05531475692987442\n",
      "Epoch 8669/30000 Training Loss: 0.04452819749712944\n",
      "Epoch 8670/30000 Training Loss: 0.04059942811727524\n",
      "Epoch 8671/30000 Training Loss: 0.048789963126182556\n",
      "Epoch 8672/30000 Training Loss: 0.04635562747716904\n",
      "Epoch 8673/30000 Training Loss: 0.04787326976656914\n",
      "Epoch 8674/30000 Training Loss: 0.04519858956336975\n",
      "Epoch 8675/30000 Training Loss: 0.049187690019607544\n",
      "Epoch 8676/30000 Training Loss: 0.050239719450473785\n",
      "Epoch 8677/30000 Training Loss: 0.052737992256879807\n",
      "Epoch 8678/30000 Training Loss: 0.051632463932037354\n",
      "Epoch 8679/30000 Training Loss: 0.05694175884127617\n",
      "Epoch 8680/30000 Training Loss: 0.04844775050878525\n",
      "Epoch 8681/30000 Training Loss: 0.04955332726240158\n",
      "Epoch 8682/30000 Training Loss: 0.0524633526802063\n",
      "Epoch 8683/30000 Training Loss: 0.04594496637582779\n",
      "Epoch 8684/30000 Training Loss: 0.05317456275224686\n",
      "Epoch 8685/30000 Training Loss: 0.04987993463873863\n",
      "Epoch 8686/30000 Training Loss: 0.04930407553911209\n",
      "Epoch 8687/30000 Training Loss: 0.03207770362496376\n",
      "Epoch 8688/30000 Training Loss: 0.04641358181834221\n",
      "Epoch 8689/30000 Training Loss: 0.046699441969394684\n",
      "Epoch 8690/30000 Training Loss: 0.0486728772521019\n",
      "Epoch 8691/30000 Training Loss: 0.05143306776881218\n",
      "Epoch 8692/30000 Training Loss: 0.04342871159315109\n",
      "Epoch 8693/30000 Training Loss: 0.04726652055978775\n",
      "Epoch 8694/30000 Training Loss: 0.04452124983072281\n",
      "Epoch 8695/30000 Training Loss: 0.045698922127485275\n",
      "Epoch 8696/30000 Training Loss: 0.054841794073581696\n",
      "Epoch 8697/30000 Training Loss: 0.044835496693849564\n",
      "Epoch 8698/30000 Training Loss: 0.038642507046461105\n",
      "Epoch 8699/30000 Training Loss: 0.05173860117793083\n",
      "Epoch 8700/30000 Training Loss: 0.042672108858823776\n",
      "Epoch 8700/30000 Validation Loss: 0.051534004509449005\n",
      "Epoch 8701/30000 Training Loss: 0.05810893326997757\n",
      "Epoch 8702/30000 Training Loss: 0.0459199920296669\n",
      "Epoch 8703/30000 Training Loss: 0.05066496133804321\n",
      "Epoch 8704/30000 Training Loss: 0.054058145731687546\n",
      "Epoch 8705/30000 Training Loss: 0.05671762675046921\n",
      "Epoch 8706/30000 Training Loss: 0.05231945961713791\n",
      "Epoch 8707/30000 Training Loss: 0.04766392707824707\n",
      "Epoch 8708/30000 Training Loss: 0.048120494931936264\n",
      "Epoch 8709/30000 Training Loss: 0.04375765100121498\n",
      "Epoch 8710/30000 Training Loss: 0.044913243502378464\n",
      "Epoch 8711/30000 Training Loss: 0.04241328686475754\n",
      "Epoch 8712/30000 Training Loss: 0.04738151654601097\n",
      "Epoch 8713/30000 Training Loss: 0.04762915521860123\n",
      "Epoch 8714/30000 Training Loss: 0.04907519742846489\n",
      "Epoch 8715/30000 Training Loss: 0.054612476378679276\n",
      "Epoch 8716/30000 Training Loss: 0.0496659092605114\n",
      "Epoch 8717/30000 Training Loss: 0.047035083174705505\n",
      "Epoch 8718/30000 Training Loss: 0.05062933638691902\n",
      "Epoch 8719/30000 Training Loss: 0.04407647252082825\n",
      "Epoch 8720/30000 Training Loss: 0.04530494660139084\n",
      "Epoch 8721/30000 Training Loss: 0.04926171898841858\n",
      "Epoch 8722/30000 Training Loss: 0.04980490356683731\n",
      "Epoch 8723/30000 Training Loss: 0.05653001740574837\n",
      "Epoch 8724/30000 Training Loss: 0.05140966176986694\n",
      "Epoch 8725/30000 Training Loss: 0.04354846477508545\n",
      "Epoch 8726/30000 Training Loss: 0.04078308492898941\n",
      "Epoch 8727/30000 Training Loss: 0.05666549876332283\n",
      "Epoch 8728/30000 Training Loss: 0.05002255365252495\n",
      "Epoch 8729/30000 Training Loss: 0.04433327168226242\n",
      "Epoch 8730/30000 Training Loss: 0.04918379336595535\n",
      "Epoch 8731/30000 Training Loss: 0.04111471399664879\n",
      "Epoch 8732/30000 Training Loss: 0.05554032325744629\n",
      "Epoch 8733/30000 Training Loss: 0.05465816333889961\n",
      "Epoch 8734/30000 Training Loss: 0.05450823903083801\n",
      "Epoch 8735/30000 Training Loss: 0.049768880009651184\n",
      "Epoch 8736/30000 Training Loss: 0.04953750595450401\n",
      "Epoch 8737/30000 Training Loss: 0.05009803920984268\n",
      "Epoch 8738/30000 Training Loss: 0.04278005659580231\n",
      "Epoch 8739/30000 Training Loss: 0.04828443378210068\n",
      "Epoch 8740/30000 Training Loss: 0.044607698917388916\n",
      "Epoch 8741/30000 Training Loss: 0.041490841656923294\n",
      "Epoch 8742/30000 Training Loss: 0.05707994103431702\n",
      "Epoch 8743/30000 Training Loss: 0.04835716634988785\n",
      "Epoch 8744/30000 Training Loss: 0.04216935485601425\n",
      "Epoch 8745/30000 Training Loss: 0.04373883455991745\n",
      "Epoch 8746/30000 Training Loss: 0.0536176934838295\n",
      "Epoch 8747/30000 Training Loss: 0.05032076686620712\n",
      "Epoch 8748/30000 Training Loss: 0.05008149892091751\n",
      "Epoch 8749/30000 Training Loss: 0.04678966850042343\n",
      "Epoch 8750/30000 Training Loss: 0.04746311902999878\n",
      "Epoch 8750/30000 Validation Loss: 0.047183774411678314\n",
      "Epoch 8751/30000 Training Loss: 0.039458271116018295\n",
      "Epoch 8752/30000 Training Loss: 0.04469715803861618\n",
      "Epoch 8753/30000 Training Loss: 0.04575299471616745\n",
      "Epoch 8754/30000 Training Loss: 0.0404539480805397\n",
      "Epoch 8755/30000 Training Loss: 0.05030013993382454\n",
      "Epoch 8756/30000 Training Loss: 0.05201885476708412\n",
      "Epoch 8757/30000 Training Loss: 0.043480195105075836\n",
      "Epoch 8758/30000 Training Loss: 0.040918175131082535\n",
      "Epoch 8759/30000 Training Loss: 0.04441925510764122\n",
      "Epoch 8760/30000 Training Loss: 0.042094480246305466\n",
      "Epoch 8761/30000 Training Loss: 0.047493595629930496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8762/30000 Training Loss: 0.03995276242494583\n",
      "Epoch 8763/30000 Training Loss: 0.04035462811589241\n",
      "Epoch 8764/30000 Training Loss: 0.051603902131319046\n",
      "Epoch 8765/30000 Training Loss: 0.05287444591522217\n",
      "Epoch 8766/30000 Training Loss: 0.042495246976614\n",
      "Epoch 8767/30000 Training Loss: 0.051440030336380005\n",
      "Epoch 8768/30000 Training Loss: 0.04607702046632767\n",
      "Epoch 8769/30000 Training Loss: 0.05392739176750183\n",
      "Epoch 8770/30000 Training Loss: 0.04551686346530914\n",
      "Epoch 8771/30000 Training Loss: 0.047222211956977844\n",
      "Epoch 8772/30000 Training Loss: 0.0446808785200119\n",
      "Epoch 8773/30000 Training Loss: 0.05460946634411812\n",
      "Epoch 8774/30000 Training Loss: 0.04302244633436203\n",
      "Epoch 8775/30000 Training Loss: 0.047732673585414886\n",
      "Epoch 8776/30000 Training Loss: 0.044998325407505035\n",
      "Epoch 8777/30000 Training Loss: 0.04415927454829216\n",
      "Epoch 8778/30000 Training Loss: 0.04639763757586479\n",
      "Epoch 8779/30000 Training Loss: 0.04168585687875748\n",
      "Epoch 8780/30000 Training Loss: 0.05615939944982529\n",
      "Epoch 8781/30000 Training Loss: 0.04985358938574791\n",
      "Epoch 8782/30000 Training Loss: 0.04054173082113266\n",
      "Epoch 8783/30000 Training Loss: 0.051659125834703445\n",
      "Epoch 8784/30000 Training Loss: 0.04323597252368927\n",
      "Epoch 8785/30000 Training Loss: 0.04578396677970886\n",
      "Epoch 8786/30000 Training Loss: 0.047116976231336594\n",
      "Epoch 8787/30000 Training Loss: 0.04469713568687439\n",
      "Epoch 8788/30000 Training Loss: 0.05402650311589241\n",
      "Epoch 8789/30000 Training Loss: 0.04597920924425125\n",
      "Epoch 8790/30000 Training Loss: 0.04807985946536064\n",
      "Epoch 8791/30000 Training Loss: 0.04193411394953728\n",
      "Epoch 8792/30000 Training Loss: 0.05287238210439682\n",
      "Epoch 8793/30000 Training Loss: 0.045410703867673874\n",
      "Epoch 8794/30000 Training Loss: 0.04852607846260071\n",
      "Epoch 8795/30000 Training Loss: 0.053431928157806396\n",
      "Epoch 8796/30000 Training Loss: 0.05341551452875137\n",
      "Epoch 8797/30000 Training Loss: 0.041832439601421356\n",
      "Epoch 8798/30000 Training Loss: 0.0424988828599453\n",
      "Epoch 8799/30000 Training Loss: 0.041981667280197144\n",
      "Epoch 8800/30000 Training Loss: 0.050833411514759064\n",
      "Epoch 8800/30000 Validation Loss: 0.046718720346689224\n",
      "Epoch 8801/30000 Training Loss: 0.04499635845422745\n",
      "Epoch 8802/30000 Training Loss: 0.04554786905646324\n",
      "Epoch 8803/30000 Training Loss: 0.04551435261964798\n",
      "Epoch 8804/30000 Training Loss: 0.04368358105421066\n",
      "Epoch 8805/30000 Training Loss: 0.0363687202334404\n",
      "Epoch 8806/30000 Training Loss: 0.04259943217039108\n",
      "Epoch 8807/30000 Training Loss: 0.04829728230834007\n",
      "Epoch 8808/30000 Training Loss: 0.04740278050303459\n",
      "Epoch 8809/30000 Training Loss: 0.05101056024432182\n",
      "Epoch 8810/30000 Training Loss: 0.05442725867033005\n",
      "Epoch 8811/30000 Training Loss: 0.05018293857574463\n",
      "Epoch 8812/30000 Training Loss: 0.0430927500128746\n",
      "Epoch 8813/30000 Training Loss: 0.04904278367757797\n",
      "Epoch 8814/30000 Training Loss: 0.041380830109119415\n",
      "Epoch 8815/30000 Training Loss: 0.05336230248212814\n",
      "Epoch 8816/30000 Training Loss: 0.043054450303316116\n",
      "Epoch 8817/30000 Training Loss: 0.0503525547683239\n",
      "Epoch 8818/30000 Training Loss: 0.03897714614868164\n",
      "Epoch 8819/30000 Training Loss: 0.0518825463950634\n",
      "Epoch 8820/30000 Training Loss: 0.051529329270124435\n",
      "Epoch 8821/30000 Training Loss: 0.05528416484594345\n",
      "Epoch 8822/30000 Training Loss: 0.051242221146821976\n",
      "Epoch 8823/30000 Training Loss: 0.0531754195690155\n",
      "Epoch 8824/30000 Training Loss: 0.044131942093372345\n",
      "Epoch 8825/30000 Training Loss: 0.04706132784485817\n",
      "Epoch 8826/30000 Training Loss: 0.051279209554195404\n",
      "Epoch 8827/30000 Training Loss: 0.04539802297949791\n",
      "Epoch 8828/30000 Training Loss: 0.050628285855054855\n",
      "Epoch 8829/30000 Training Loss: 0.043095655739307404\n",
      "Epoch 8830/30000 Training Loss: 0.03922327607870102\n",
      "Epoch 8831/30000 Training Loss: 0.04164348170161247\n",
      "Epoch 8832/30000 Training Loss: 0.042906809598207474\n",
      "Epoch 8833/30000 Training Loss: 0.05386878177523613\n",
      "Epoch 8834/30000 Training Loss: 0.05171515420079231\n",
      "Epoch 8835/30000 Training Loss: 0.04956838861107826\n",
      "Epoch 8836/30000 Training Loss: 0.047664593905210495\n",
      "Epoch 8837/30000 Training Loss: 0.04720357805490494\n",
      "Epoch 8838/30000 Training Loss: 0.053715337067842484\n",
      "Epoch 8839/30000 Training Loss: 0.05796448513865471\n",
      "Epoch 8840/30000 Training Loss: 0.05226501077413559\n",
      "Epoch 8841/30000 Training Loss: 0.05642303079366684\n",
      "Epoch 8842/30000 Training Loss: 0.04447958618402481\n",
      "Epoch 8843/30000 Training Loss: 0.04448602348566055\n",
      "Epoch 8844/30000 Training Loss: 0.04021947458386421\n",
      "Epoch 8845/30000 Training Loss: 0.05044823884963989\n",
      "Epoch 8846/30000 Training Loss: 0.05186992138624191\n",
      "Epoch 8847/30000 Training Loss: 0.05092216655611992\n",
      "Epoch 8848/30000 Training Loss: 0.0447222962975502\n",
      "Epoch 8849/30000 Training Loss: 0.05090935900807381\n",
      "Epoch 8850/30000 Training Loss: 0.046748124063014984\n",
      "Epoch 8850/30000 Validation Loss: 0.04929453879594803\n",
      "Epoch 8851/30000 Training Loss: 0.040125347673892975\n",
      "Epoch 8852/30000 Training Loss: 0.04470749944448471\n",
      "Epoch 8853/30000 Training Loss: 0.047139428555965424\n",
      "Epoch 8854/30000 Training Loss: 0.04529298096895218\n",
      "Epoch 8855/30000 Training Loss: 0.0531739667057991\n",
      "Epoch 8856/30000 Training Loss: 0.04045765846967697\n",
      "Epoch 8857/30000 Training Loss: 0.03967747837305069\n",
      "Epoch 8858/30000 Training Loss: 0.04870340973138809\n",
      "Epoch 8859/30000 Training Loss: 0.052408166229724884\n",
      "Epoch 8860/30000 Training Loss: 0.05193180590867996\n",
      "Epoch 8861/30000 Training Loss: 0.049041081219911575\n",
      "Epoch 8862/30000 Training Loss: 0.04868713393807411\n",
      "Epoch 8863/30000 Training Loss: 0.0395544059574604\n",
      "Epoch 8864/30000 Training Loss: 0.05715296417474747\n",
      "Epoch 8865/30000 Training Loss: 0.049024246633052826\n",
      "Epoch 8866/30000 Training Loss: 0.05288399010896683\n",
      "Epoch 8867/30000 Training Loss: 0.04724767059087753\n",
      "Epoch 8868/30000 Training Loss: 0.04485934600234032\n",
      "Epoch 8869/30000 Training Loss: 0.055292271077632904\n",
      "Epoch 8870/30000 Training Loss: 0.04456372559070587\n",
      "Epoch 8871/30000 Training Loss: 0.04561515152454376\n",
      "Epoch 8872/30000 Training Loss: 0.04937011003494263\n",
      "Epoch 8873/30000 Training Loss: 0.04414241388440132\n",
      "Epoch 8874/30000 Training Loss: 0.03969767317175865\n",
      "Epoch 8875/30000 Training Loss: 0.05055297166109085\n",
      "Epoch 8876/30000 Training Loss: 0.043143436312675476\n",
      "Epoch 8877/30000 Training Loss: 0.05064227059483528\n",
      "Epoch 8878/30000 Training Loss: 0.04553527757525444\n",
      "Epoch 8879/30000 Training Loss: 0.05640837550163269\n",
      "Epoch 8880/30000 Training Loss: 0.045507676899433136\n",
      "Epoch 8881/30000 Training Loss: 0.05603921413421631\n",
      "Epoch 8882/30000 Training Loss: 0.043647877871990204\n",
      "Epoch 8883/30000 Training Loss: 0.05449541285634041\n",
      "Epoch 8884/30000 Training Loss: 0.045491255819797516\n",
      "Epoch 8885/30000 Training Loss: 0.04600737616419792\n",
      "Epoch 8886/30000 Training Loss: 0.04274839162826538\n",
      "Epoch 8887/30000 Training Loss: 0.05028061196208\n",
      "Epoch 8888/30000 Training Loss: 0.0445280596613884\n",
      "Epoch 8889/30000 Training Loss: 0.059646010398864746\n",
      "Epoch 8890/30000 Training Loss: 0.04721509665250778\n",
      "Epoch 8891/30000 Training Loss: 0.0480392687022686\n",
      "Epoch 8892/30000 Training Loss: 0.043879833072423935\n",
      "Epoch 8893/30000 Training Loss: 0.050775568932294846\n",
      "Epoch 8894/30000 Training Loss: 0.05245913192629814\n",
      "Epoch 8895/30000 Training Loss: 0.04617512971162796\n",
      "Epoch 8896/30000 Training Loss: 0.04435477405786514\n",
      "Epoch 8897/30000 Training Loss: 0.05072334408760071\n",
      "Epoch 8898/30000 Training Loss: 0.047046490013599396\n",
      "Epoch 8899/30000 Training Loss: 0.05428784340620041\n",
      "Epoch 8900/30000 Training Loss: 0.048679690808057785\n",
      "Epoch 8900/30000 Validation Loss: 0.04997503384947777\n",
      "Epoch 8901/30000 Training Loss: 0.050739746540784836\n",
      "Epoch 8902/30000 Training Loss: 0.03803829476237297\n",
      "Epoch 8903/30000 Training Loss: 0.043197743594646454\n",
      "Epoch 8904/30000 Training Loss: 0.048915497958660126\n",
      "Epoch 8905/30000 Training Loss: 0.049198247492313385\n",
      "Epoch 8906/30000 Training Loss: 0.050709567964076996\n",
      "Epoch 8907/30000 Training Loss: 0.05070727318525314\n",
      "Epoch 8908/30000 Training Loss: 0.04715883731842041\n",
      "Epoch 8909/30000 Training Loss: 0.04618685692548752\n",
      "Epoch 8910/30000 Training Loss: 0.05076585337519646\n",
      "Epoch 8911/30000 Training Loss: 0.04385867714881897\n",
      "Epoch 8912/30000 Training Loss: 0.049868106842041016\n",
      "Epoch 8913/30000 Training Loss: 0.047453708946704865\n",
      "Epoch 8914/30000 Training Loss: 0.05383677035570145\n",
      "Epoch 8915/30000 Training Loss: 0.04584481567144394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8916/30000 Training Loss: 0.05604947730898857\n",
      "Epoch 8917/30000 Training Loss: 0.0493505597114563\n",
      "Epoch 8918/30000 Training Loss: 0.041753657162189484\n",
      "Epoch 8919/30000 Training Loss: 0.04600067064166069\n",
      "Epoch 8920/30000 Training Loss: 0.04818669706583023\n",
      "Epoch 8921/30000 Training Loss: 0.05335497111082077\n",
      "Epoch 8922/30000 Training Loss: 0.03614219278097153\n",
      "Epoch 8923/30000 Training Loss: 0.044301588088274\n",
      "Epoch 8924/30000 Training Loss: 0.0406816303730011\n",
      "Epoch 8925/30000 Training Loss: 0.05541830137372017\n",
      "Epoch 8926/30000 Training Loss: 0.04189348593354225\n",
      "Epoch 8927/30000 Training Loss: 0.04839833453297615\n",
      "Epoch 8928/30000 Training Loss: 0.05307555943727493\n",
      "Epoch 8929/30000 Training Loss: 0.05098375678062439\n",
      "Epoch 8930/30000 Training Loss: 0.043025389313697815\n",
      "Epoch 8931/30000 Training Loss: 0.04421302303671837\n",
      "Epoch 8932/30000 Training Loss: 0.0473284050822258\n",
      "Epoch 8933/30000 Training Loss: 0.05274537205696106\n",
      "Epoch 8934/30000 Training Loss: 0.045341502875089645\n",
      "Epoch 8935/30000 Training Loss: 0.04428080841898918\n",
      "Epoch 8936/30000 Training Loss: 0.050727926194667816\n",
      "Epoch 8937/30000 Training Loss: 0.04646174982190132\n",
      "Epoch 8938/30000 Training Loss: 0.04482898861169815\n",
      "Epoch 8939/30000 Training Loss: 0.05023200064897537\n",
      "Epoch 8940/30000 Training Loss: 0.05078432708978653\n",
      "Epoch 8941/30000 Training Loss: 0.051056694239377975\n",
      "Epoch 8942/30000 Training Loss: 0.05207052081823349\n",
      "Epoch 8943/30000 Training Loss: 0.04685040935873985\n",
      "Epoch 8944/30000 Training Loss: 0.0533868782222271\n",
      "Epoch 8945/30000 Training Loss: 0.04938175529241562\n",
      "Epoch 8946/30000 Training Loss: 0.042199455201625824\n",
      "Epoch 8947/30000 Training Loss: 0.04914553835988045\n",
      "Epoch 8948/30000 Training Loss: 0.047026947140693665\n",
      "Epoch 8949/30000 Training Loss: 0.04503023624420166\n",
      "Epoch 8950/30000 Training Loss: 0.051615096628665924\n",
      "Epoch 8950/30000 Validation Loss: 0.046938467770814896\n",
      "Epoch 8951/30000 Training Loss: 0.05939723178744316\n",
      "Epoch 8952/30000 Training Loss: 0.04467575624585152\n",
      "Epoch 8953/30000 Training Loss: 0.051648736000061035\n",
      "Epoch 8954/30000 Training Loss: 0.044894710183143616\n",
      "Epoch 8955/30000 Training Loss: 0.047709040343761444\n",
      "Epoch 8956/30000 Training Loss: 0.04242798313498497\n",
      "Epoch 8957/30000 Training Loss: 0.04664316028356552\n",
      "Epoch 8958/30000 Training Loss: 0.04763995483517647\n",
      "Epoch 8959/30000 Training Loss: 0.04401177912950516\n",
      "Epoch 8960/30000 Training Loss: 0.05424121767282486\n",
      "Epoch 8961/30000 Training Loss: 0.04785631597042084\n",
      "Epoch 8962/30000 Training Loss: 0.05231590196490288\n",
      "Epoch 8963/30000 Training Loss: 0.05015639588236809\n",
      "Epoch 8964/30000 Training Loss: 0.05054663494229317\n",
      "Epoch 8965/30000 Training Loss: 0.04488953948020935\n",
      "Epoch 8966/30000 Training Loss: 0.048751864582300186\n",
      "Epoch 8967/30000 Training Loss: 0.04768974334001541\n",
      "Epoch 8968/30000 Training Loss: 0.05050347000360489\n",
      "Epoch 8969/30000 Training Loss: 0.0502677783370018\n",
      "Epoch 8970/30000 Training Loss: 0.041256263852119446\n",
      "Epoch 8971/30000 Training Loss: 0.04737170785665512\n",
      "Epoch 8972/30000 Training Loss: 0.05405229330062866\n",
      "Epoch 8973/30000 Training Loss: 0.04499617964029312\n",
      "Epoch 8974/30000 Training Loss: 0.05011814832687378\n",
      "Epoch 8975/30000 Training Loss: 0.04670803248882294\n",
      "Epoch 8976/30000 Training Loss: 0.04528403654694557\n",
      "Epoch 8977/30000 Training Loss: 0.043612148612737656\n",
      "Epoch 8978/30000 Training Loss: 0.059074241667985916\n",
      "Epoch 8979/30000 Training Loss: 0.04305892437696457\n",
      "Epoch 8980/30000 Training Loss: 0.05019453912973404\n",
      "Epoch 8981/30000 Training Loss: 0.04400221258401871\n",
      "Epoch 8982/30000 Training Loss: 0.05027777701616287\n",
      "Epoch 8983/30000 Training Loss: 0.04723929986357689\n",
      "Epoch 8984/30000 Training Loss: 0.052242107689380646\n",
      "Epoch 8985/30000 Training Loss: 0.04911680519580841\n",
      "Epoch 8986/30000 Training Loss: 0.05324356630444527\n",
      "Epoch 8987/30000 Training Loss: 0.04633398354053497\n",
      "Epoch 8988/30000 Training Loss: 0.038985684514045715\n",
      "Epoch 8989/30000 Training Loss: 0.05140965059399605\n",
      "Epoch 8990/30000 Training Loss: 0.046268902719020844\n",
      "Epoch 8991/30000 Training Loss: 0.04712595418095589\n",
      "Epoch 8992/30000 Training Loss: 0.04831358417868614\n",
      "Epoch 8993/30000 Training Loss: 0.052886463701725006\n",
      "Epoch 8994/30000 Training Loss: 0.0429515466094017\n",
      "Epoch 8995/30000 Training Loss: 0.043668992817401886\n",
      "Epoch 8996/30000 Training Loss: 0.050004322081804276\n",
      "Epoch 8997/30000 Training Loss: 0.048334524035453796\n",
      "Epoch 8998/30000 Training Loss: 0.047729525715112686\n",
      "Epoch 8999/30000 Training Loss: 0.04677810147404671\n",
      "Epoch 9000/30000 Training Loss: 0.046947747468948364\n",
      "Epoch 9000/30000 Validation Loss: 0.04455641284584999\n",
      "Epoch 9001/30000 Training Loss: 0.035716261714696884\n",
      "Epoch 9002/30000 Training Loss: 0.040540434420108795\n",
      "Epoch 9003/30000 Training Loss: 0.06089450791478157\n",
      "Epoch 9004/30000 Training Loss: 0.045916393399238586\n",
      "Epoch 9005/30000 Training Loss: 0.04747682809829712\n",
      "Epoch 9006/30000 Training Loss: 0.05130432918667793\n",
      "Epoch 9007/30000 Training Loss: 0.04661323502659798\n",
      "Epoch 9008/30000 Training Loss: 0.050671786069869995\n",
      "Epoch 9009/30000 Training Loss: 0.04732193797826767\n",
      "Epoch 9010/30000 Training Loss: 0.046375926584005356\n",
      "Epoch 9011/30000 Training Loss: 0.055383794009685516\n",
      "Epoch 9012/30000 Training Loss: 0.048959556967020035\n",
      "Epoch 9013/30000 Training Loss: 0.04989054054021835\n",
      "Epoch 9014/30000 Training Loss: 0.04689083248376846\n",
      "Epoch 9015/30000 Training Loss: 0.04721662029623985\n",
      "Epoch 9016/30000 Training Loss: 0.05071908235549927\n",
      "Epoch 9017/30000 Training Loss: 0.057171422988176346\n",
      "Epoch 9018/30000 Training Loss: 0.054960232228040695\n",
      "Epoch 9019/30000 Training Loss: 0.04880262538790703\n",
      "Epoch 9020/30000 Training Loss: 0.047434139996767044\n",
      "Epoch 9021/30000 Training Loss: 0.057588450610637665\n",
      "Epoch 9022/30000 Training Loss: 0.045708466321229935\n",
      "Epoch 9023/30000 Training Loss: 0.05170772597193718\n",
      "Epoch 9024/30000 Training Loss: 0.05078963562846184\n",
      "Epoch 9025/30000 Training Loss: 0.04736943915486336\n",
      "Epoch 9026/30000 Training Loss: 0.0494154691696167\n",
      "Epoch 9027/30000 Training Loss: 0.042698781937360764\n",
      "Epoch 9028/30000 Training Loss: 0.04731178656220436\n",
      "Epoch 9029/30000 Training Loss: 0.053522802889347076\n",
      "Epoch 9030/30000 Training Loss: 0.05557858943939209\n",
      "Epoch 9031/30000 Training Loss: 0.05138888955116272\n",
      "Epoch 9032/30000 Training Loss: 0.0521746389567852\n",
      "Epoch 9033/30000 Training Loss: 0.042037978768348694\n",
      "Epoch 9034/30000 Training Loss: 0.05189087241888046\n",
      "Epoch 9035/30000 Training Loss: 0.041760288178920746\n",
      "Epoch 9036/30000 Training Loss: 0.03874339535832405\n",
      "Epoch 9037/30000 Training Loss: 0.04446829855442047\n",
      "Epoch 9038/30000 Training Loss: 0.05675608664751053\n",
      "Epoch 9039/30000 Training Loss: 0.04416166990995407\n",
      "Epoch 9040/30000 Training Loss: 0.04430448263883591\n",
      "Epoch 9041/30000 Training Loss: 0.04459361359477043\n",
      "Epoch 9042/30000 Training Loss: 0.04288366064429283\n",
      "Epoch 9043/30000 Training Loss: 0.05183594673871994\n",
      "Epoch 9044/30000 Training Loss: 0.04130982980132103\n",
      "Epoch 9045/30000 Training Loss: 0.049141719937324524\n",
      "Epoch 9046/30000 Training Loss: 0.040348418056964874\n",
      "Epoch 9047/30000 Training Loss: 0.047728992998600006\n",
      "Epoch 9048/30000 Training Loss: 0.05122460797429085\n",
      "Epoch 9049/30000 Training Loss: 0.0349971279501915\n",
      "Epoch 9050/30000 Training Loss: 0.037389129400253296\n",
      "Epoch 9050/30000 Validation Loss: 0.04606873542070389\n",
      "Epoch 9051/30000 Training Loss: 0.04336674511432648\n",
      "Epoch 9052/30000 Training Loss: 0.04791491478681564\n",
      "Epoch 9053/30000 Training Loss: 0.04490799456834793\n",
      "Epoch 9054/30000 Training Loss: 0.04890112578868866\n",
      "Epoch 9055/30000 Training Loss: 0.042132262140512466\n",
      "Epoch 9056/30000 Training Loss: 0.045537251979112625\n",
      "Epoch 9057/30000 Training Loss: 0.043978460133075714\n",
      "Epoch 9058/30000 Training Loss: 0.048094261437654495\n",
      "Epoch 9059/30000 Training Loss: 0.045430898666381836\n",
      "Epoch 9060/30000 Training Loss: 0.03214693441987038\n",
      "Epoch 9061/30000 Training Loss: 0.04683687165379524\n",
      "Epoch 9062/30000 Training Loss: 0.04722658544778824\n",
      "Epoch 9063/30000 Training Loss: 0.055883657187223434\n",
      "Epoch 9064/30000 Training Loss: 0.0559544637799263\n",
      "Epoch 9065/30000 Training Loss: 0.04371923953294754\n",
      "Epoch 9066/30000 Training Loss: 0.04044830799102783\n",
      "Epoch 9067/30000 Training Loss: 0.04638933017849922\n",
      "Epoch 9068/30000 Training Loss: 0.04606809839606285\n",
      "Epoch 9069/30000 Training Loss: 0.0493447370827198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9070/30000 Training Loss: 0.044460028409957886\n",
      "Epoch 9071/30000 Training Loss: 0.04603540524840355\n",
      "Epoch 9072/30000 Training Loss: 0.05120910331606865\n",
      "Epoch 9073/30000 Training Loss: 0.05128782242536545\n",
      "Epoch 9074/30000 Training Loss: 0.052314359694719315\n",
      "Epoch 9075/30000 Training Loss: 0.04804791510105133\n",
      "Epoch 9076/30000 Training Loss: 0.048560164868831635\n",
      "Epoch 9077/30000 Training Loss: 0.051484525203704834\n",
      "Epoch 9078/30000 Training Loss: 0.048430185765028\n",
      "Epoch 9079/30000 Training Loss: 0.04823657125234604\n",
      "Epoch 9080/30000 Training Loss: 0.04447517544031143\n",
      "Epoch 9081/30000 Training Loss: 0.0446489080786705\n",
      "Epoch 9082/30000 Training Loss: 0.05689204856753349\n",
      "Epoch 9083/30000 Training Loss: 0.05504278093576431\n",
      "Epoch 9084/30000 Training Loss: 0.0417029969394207\n",
      "Epoch 9085/30000 Training Loss: 0.05087381601333618\n",
      "Epoch 9086/30000 Training Loss: 0.04635046422481537\n",
      "Epoch 9087/30000 Training Loss: 0.06007108837366104\n",
      "Epoch 9088/30000 Training Loss: 0.04510406404733658\n",
      "Epoch 9089/30000 Training Loss: 0.06009114533662796\n",
      "Epoch 9090/30000 Training Loss: 0.04474853724241257\n",
      "Epoch 9091/30000 Training Loss: 0.044952284544706345\n",
      "Epoch 9092/30000 Training Loss: 0.047615982592105865\n",
      "Epoch 9093/30000 Training Loss: 0.04163016751408577\n",
      "Epoch 9094/30000 Training Loss: 0.05412258952856064\n",
      "Epoch 9095/30000 Training Loss: 0.04754921793937683\n",
      "Epoch 9096/30000 Training Loss: 0.050818223506212234\n",
      "Epoch 9097/30000 Training Loss: 0.04162973910570145\n",
      "Epoch 9098/30000 Training Loss: 0.05021163821220398\n",
      "Epoch 9099/30000 Training Loss: 0.05084995552897453\n",
      "Epoch 9100/30000 Training Loss: 0.05279228836297989\n",
      "Epoch 9100/30000 Validation Loss: 0.06575228273868561\n",
      "Epoch 9101/30000 Training Loss: 0.05152803659439087\n",
      "Epoch 9102/30000 Training Loss: 0.04625508189201355\n",
      "Epoch 9103/30000 Training Loss: 0.057379383593797684\n",
      "Epoch 9104/30000 Training Loss: 0.04240768030285835\n",
      "Epoch 9105/30000 Training Loss: 0.054310061037540436\n",
      "Epoch 9106/30000 Training Loss: 0.04262878745794296\n",
      "Epoch 9107/30000 Training Loss: 0.05475109815597534\n",
      "Epoch 9108/30000 Training Loss: 0.051830846816301346\n",
      "Epoch 9109/30000 Training Loss: 0.046496469527482986\n",
      "Epoch 9110/30000 Training Loss: 0.04215341806411743\n",
      "Epoch 9111/30000 Training Loss: 0.058161139488220215\n",
      "Epoch 9112/30000 Training Loss: 0.043159279972314835\n",
      "Epoch 9113/30000 Training Loss: 0.05427113175392151\n",
      "Epoch 9114/30000 Training Loss: 0.05311409756541252\n",
      "Epoch 9115/30000 Training Loss: 0.04801527410745621\n",
      "Epoch 9116/30000 Training Loss: 0.04654129594564438\n",
      "Epoch 9117/30000 Training Loss: 0.043796032667160034\n",
      "Epoch 9118/30000 Training Loss: 0.045327235013246536\n",
      "Epoch 9119/30000 Training Loss: 0.038323432207107544\n",
      "Epoch 9120/30000 Training Loss: 0.03781750798225403\n",
      "Epoch 9121/30000 Training Loss: 0.059042781591415405\n",
      "Epoch 9122/30000 Training Loss: 0.04388532415032387\n",
      "Epoch 9123/30000 Training Loss: 0.04520455002784729\n",
      "Epoch 9124/30000 Training Loss: 0.047601427882909775\n",
      "Epoch 9125/30000 Training Loss: 0.04258228465914726\n",
      "Epoch 9126/30000 Training Loss: 0.05316188186407089\n",
      "Epoch 9127/30000 Training Loss: 0.04914606735110283\n",
      "Epoch 9128/30000 Training Loss: 0.045002587139606476\n",
      "Epoch 9129/30000 Training Loss: 0.052171945571899414\n",
      "Epoch 9130/30000 Training Loss: 0.043248265981674194\n",
      "Epoch 9131/30000 Training Loss: 0.04900077357888222\n",
      "Epoch 9132/30000 Training Loss: 0.05055040866136551\n",
      "Epoch 9133/30000 Training Loss: 0.05537363141775131\n",
      "Epoch 9134/30000 Training Loss: 0.04622119665145874\n",
      "Epoch 9135/30000 Training Loss: 0.05031875520944595\n",
      "Epoch 9136/30000 Training Loss: 0.051207441836595535\n",
      "Epoch 9137/30000 Training Loss: 0.053084779530763626\n",
      "Epoch 9138/30000 Training Loss: 0.04587506502866745\n",
      "Epoch 9139/30000 Training Loss: 0.04172492027282715\n",
      "Epoch 9140/30000 Training Loss: 0.05112270265817642\n",
      "Epoch 9141/30000 Training Loss: 0.04403834789991379\n",
      "Epoch 9142/30000 Training Loss: 0.04790496081113815\n",
      "Epoch 9143/30000 Training Loss: 0.046615615487098694\n",
      "Epoch 9144/30000 Training Loss: 0.042358752340078354\n",
      "Epoch 9145/30000 Training Loss: 0.049322836101055145\n",
      "Epoch 9146/30000 Training Loss: 0.048787377774715424\n",
      "Epoch 9147/30000 Training Loss: 0.04715981334447861\n",
      "Epoch 9148/30000 Training Loss: 0.054656267166137695\n",
      "Epoch 9149/30000 Training Loss: 0.04719818755984306\n",
      "Epoch 9150/30000 Training Loss: 0.05066513270139694\n",
      "Epoch 9150/30000 Validation Loss: 0.04690925031900406\n",
      "Epoch 9151/30000 Training Loss: 0.046542759984731674\n",
      "Epoch 9152/30000 Training Loss: 0.044556744396686554\n",
      "Epoch 9153/30000 Training Loss: 0.043557651340961456\n",
      "Epoch 9154/30000 Training Loss: 0.044603340327739716\n",
      "Epoch 9155/30000 Training Loss: 0.04786171019077301\n",
      "Epoch 9156/30000 Training Loss: 0.053610771894454956\n",
      "Epoch 9157/30000 Training Loss: 0.05268489196896553\n",
      "Epoch 9158/30000 Training Loss: 0.045053113251924515\n",
      "Epoch 9159/30000 Training Loss: 0.04161616414785385\n",
      "Epoch 9160/30000 Training Loss: 0.044818729162216187\n",
      "Epoch 9161/30000 Training Loss: 0.0554821714758873\n",
      "Epoch 9162/30000 Training Loss: 0.05403953790664673\n",
      "Epoch 9163/30000 Training Loss: 0.040313348174095154\n",
      "Epoch 9164/30000 Training Loss: 0.04560386389493942\n",
      "Epoch 9165/30000 Training Loss: 0.044765353202819824\n",
      "Epoch 9166/30000 Training Loss: 0.044617727398872375\n",
      "Epoch 9167/30000 Training Loss: 0.05215902999043465\n",
      "Epoch 9168/30000 Training Loss: 0.04762660712003708\n",
      "Epoch 9169/30000 Training Loss: 0.046023450791835785\n",
      "Epoch 9170/30000 Training Loss: 0.043764661997556686\n",
      "Epoch 9171/30000 Training Loss: 0.058882422745227814\n",
      "Epoch 9172/30000 Training Loss: 0.040351442992687225\n",
      "Epoch 9173/30000 Training Loss: 0.0478835329413414\n",
      "Epoch 9174/30000 Training Loss: 0.04786967486143112\n",
      "Epoch 9175/30000 Training Loss: 0.05093703418970108\n",
      "Epoch 9176/30000 Training Loss: 0.05166538804769516\n",
      "Epoch 9177/30000 Training Loss: 0.05398956686258316\n",
      "Epoch 9178/30000 Training Loss: 0.04769029840826988\n",
      "Epoch 9179/30000 Training Loss: 0.045193471014499664\n",
      "Epoch 9180/30000 Training Loss: 0.05170772224664688\n",
      "Epoch 9181/30000 Training Loss: 0.05129121616482735\n",
      "Epoch 9182/30000 Training Loss: 0.05040432885289192\n",
      "Epoch 9183/30000 Training Loss: 0.05100482702255249\n",
      "Epoch 9184/30000 Training Loss: 0.043902166187763214\n",
      "Epoch 9185/30000 Training Loss: 0.04993918910622597\n",
      "Epoch 9186/30000 Training Loss: 0.0462394542992115\n",
      "Epoch 9187/30000 Training Loss: 0.050224192440509796\n",
      "Epoch 9188/30000 Training Loss: 0.05176196247339249\n",
      "Epoch 9189/30000 Training Loss: 0.05221610143780708\n",
      "Epoch 9190/30000 Training Loss: 0.04278124123811722\n",
      "Epoch 9191/30000 Training Loss: 0.04788525030016899\n",
      "Epoch 9192/30000 Training Loss: 0.04681176692247391\n",
      "Epoch 9193/30000 Training Loss: 0.05742047354578972\n",
      "Epoch 9194/30000 Training Loss: 0.04281020909547806\n",
      "Epoch 9195/30000 Training Loss: 0.04226521775126457\n",
      "Epoch 9196/30000 Training Loss: 0.055439792573451996\n",
      "Epoch 9197/30000 Training Loss: 0.03851994872093201\n",
      "Epoch 9198/30000 Training Loss: 0.04317782074213028\n",
      "Epoch 9199/30000 Training Loss: 0.040360692888498306\n",
      "Epoch 9200/30000 Training Loss: 0.04040534794330597\n",
      "Epoch 9200/30000 Validation Loss: 0.05263734608888626\n",
      "Epoch 9201/30000 Training Loss: 0.05425717309117317\n",
      "Epoch 9202/30000 Training Loss: 0.046603504568338394\n",
      "Epoch 9203/30000 Training Loss: 0.048415690660476685\n",
      "Epoch 9204/30000 Training Loss: 0.040354687720537186\n",
      "Epoch 9205/30000 Training Loss: 0.046990230679512024\n",
      "Epoch 9206/30000 Training Loss: 0.05220913887023926\n",
      "Epoch 9207/30000 Training Loss: 0.04678117483854294\n",
      "Epoch 9208/30000 Training Loss: 0.04737531766295433\n",
      "Epoch 9209/30000 Training Loss: 0.051375217735767365\n",
      "Epoch 9210/30000 Training Loss: 0.049159400165081024\n",
      "Epoch 9211/30000 Training Loss: 0.04490046203136444\n",
      "Epoch 9212/30000 Training Loss: 0.04902467504143715\n",
      "Epoch 9213/30000 Training Loss: 0.048984453082084656\n",
      "Epoch 9214/30000 Training Loss: 0.035950303077697754\n",
      "Epoch 9215/30000 Training Loss: 0.042033836245536804\n",
      "Epoch 9216/30000 Training Loss: 0.044477708637714386\n",
      "Epoch 9217/30000 Training Loss: 0.04188866540789604\n",
      "Epoch 9218/30000 Training Loss: 0.04165147989988327\n",
      "Epoch 9219/30000 Training Loss: 0.04519486799836159\n",
      "Epoch 9220/30000 Training Loss: 0.049331121146678925\n",
      "Epoch 9221/30000 Training Loss: 0.0469537191092968\n",
      "Epoch 9222/30000 Training Loss: 0.051127634942531586\n",
      "Epoch 9223/30000 Training Loss: 0.05611797422170639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9224/30000 Training Loss: 0.047890834510326385\n",
      "Epoch 9225/30000 Training Loss: 0.04049530625343323\n",
      "Epoch 9226/30000 Training Loss: 0.041509296745061874\n",
      "Epoch 9227/30000 Training Loss: 0.042486052960157394\n",
      "Epoch 9228/30000 Training Loss: 0.04012303426861763\n",
      "Epoch 9229/30000 Training Loss: 0.05449271947145462\n",
      "Epoch 9230/30000 Training Loss: 0.05171649530529976\n",
      "Epoch 9231/30000 Training Loss: 0.05098783224821091\n",
      "Epoch 9232/30000 Training Loss: 0.04348149523139\n",
      "Epoch 9233/30000 Training Loss: 0.045992251485586166\n",
      "Epoch 9234/30000 Training Loss: 0.04563628137111664\n",
      "Epoch 9235/30000 Training Loss: 0.045278698205947876\n",
      "Epoch 9236/30000 Training Loss: 0.03801899775862694\n",
      "Epoch 9237/30000 Training Loss: 0.050308454781770706\n",
      "Epoch 9238/30000 Training Loss: 0.04420749098062515\n",
      "Epoch 9239/30000 Training Loss: 0.05250712111592293\n",
      "Epoch 9240/30000 Training Loss: 0.048189714550971985\n",
      "Epoch 9241/30000 Training Loss: 0.053072623908519745\n",
      "Epoch 9242/30000 Training Loss: 0.05022471025586128\n",
      "Epoch 9243/30000 Training Loss: 0.04776955023407936\n",
      "Epoch 9244/30000 Training Loss: 0.05750070884823799\n",
      "Epoch 9245/30000 Training Loss: 0.04511309415102005\n",
      "Epoch 9246/30000 Training Loss: 0.05255545303225517\n",
      "Epoch 9247/30000 Training Loss: 0.050020813941955566\n",
      "Epoch 9248/30000 Training Loss: 0.05612809583544731\n",
      "Epoch 9249/30000 Training Loss: 0.04284316673874855\n",
      "Epoch 9250/30000 Training Loss: 0.045689165592193604\n",
      "Epoch 9250/30000 Validation Loss: 0.04356750100851059\n",
      "Epoch 9251/30000 Training Loss: 0.049921151250600815\n",
      "Epoch 9252/30000 Training Loss: 0.0435589961707592\n",
      "Epoch 9253/30000 Training Loss: 0.046164315193891525\n",
      "Epoch 9254/30000 Training Loss: 0.044914811849594116\n",
      "Epoch 9255/30000 Training Loss: 0.052048712968826294\n",
      "Epoch 9256/30000 Training Loss: 0.04281962662935257\n",
      "Epoch 9257/30000 Training Loss: 0.04489649087190628\n",
      "Epoch 9258/30000 Training Loss: 0.04347417131066322\n",
      "Epoch 9259/30000 Training Loss: 0.04524068534374237\n",
      "Epoch 9260/30000 Training Loss: 0.05298423767089844\n",
      "Epoch 9261/30000 Training Loss: 0.04675836116075516\n",
      "Epoch 9262/30000 Training Loss: 0.03891376405954361\n",
      "Epoch 9263/30000 Training Loss: 0.04315873235464096\n",
      "Epoch 9264/30000 Training Loss: 0.04209686443209648\n",
      "Epoch 9265/30000 Training Loss: 0.047699593007564545\n",
      "Epoch 9266/30000 Training Loss: 0.05501886084675789\n",
      "Epoch 9267/30000 Training Loss: 0.04862872511148453\n",
      "Epoch 9268/30000 Training Loss: 0.04621875286102295\n",
      "Epoch 9269/30000 Training Loss: 0.04114686697721481\n",
      "Epoch 9270/30000 Training Loss: 0.05641920492053032\n",
      "Epoch 9271/30000 Training Loss: 0.04848857969045639\n",
      "Epoch 9272/30000 Training Loss: 0.04548715427517891\n",
      "Epoch 9273/30000 Training Loss: 0.05267612263560295\n",
      "Epoch 9274/30000 Training Loss: 0.048434723168611526\n",
      "Epoch 9275/30000 Training Loss: 0.05121970176696777\n",
      "Epoch 9276/30000 Training Loss: 0.05174838751554489\n",
      "Epoch 9277/30000 Training Loss: 0.04837528616189957\n",
      "Epoch 9278/30000 Training Loss: 0.04132832959294319\n",
      "Epoch 9279/30000 Training Loss: 0.048215657472610474\n",
      "Epoch 9280/30000 Training Loss: 0.049540262669324875\n",
      "Epoch 9281/30000 Training Loss: 0.05457402393221855\n",
      "Epoch 9282/30000 Training Loss: 0.04696816951036453\n",
      "Epoch 9283/30000 Training Loss: 0.04219323396682739\n",
      "Epoch 9284/30000 Training Loss: 0.042679719626903534\n",
      "Epoch 9285/30000 Training Loss: 0.04797573760151863\n",
      "Epoch 9286/30000 Training Loss: 0.04474487528204918\n",
      "Epoch 9287/30000 Training Loss: 0.04607135429978371\n",
      "Epoch 9288/30000 Training Loss: 0.057662248611450195\n",
      "Epoch 9289/30000 Training Loss: 0.05701553821563721\n",
      "Epoch 9290/30000 Training Loss: 0.04016274958848953\n",
      "Epoch 9291/30000 Training Loss: 0.05727115273475647\n",
      "Epoch 9292/30000 Training Loss: 0.045207105576992035\n",
      "Epoch 9293/30000 Training Loss: 0.03955000638961792\n",
      "Epoch 9294/30000 Training Loss: 0.043186817318201065\n",
      "Epoch 9295/30000 Training Loss: 0.0500885546207428\n",
      "Epoch 9296/30000 Training Loss: 0.048720914870500565\n",
      "Epoch 9297/30000 Training Loss: 0.04572371020913124\n",
      "Epoch 9298/30000 Training Loss: 0.04062698036432266\n",
      "Epoch 9299/30000 Training Loss: 0.04103800654411316\n",
      "Epoch 9300/30000 Training Loss: 0.05223599821329117\n",
      "Epoch 9300/30000 Validation Loss: 0.045555487275123596\n",
      "Epoch 9301/30000 Training Loss: 0.049709491431713104\n",
      "Epoch 9302/30000 Training Loss: 0.04624134302139282\n",
      "Epoch 9303/30000 Training Loss: 0.045301977545022964\n",
      "Epoch 9304/30000 Training Loss: 0.05242163687944412\n",
      "Epoch 9305/30000 Training Loss: 0.0482274629175663\n",
      "Epoch 9306/30000 Training Loss: 0.038582779467105865\n",
      "Epoch 9307/30000 Training Loss: 0.04468250274658203\n",
      "Epoch 9308/30000 Training Loss: 0.052003633230924606\n",
      "Epoch 9309/30000 Training Loss: 0.04381094127893448\n",
      "Epoch 9310/30000 Training Loss: 0.05315535143017769\n",
      "Epoch 9311/30000 Training Loss: 0.0505458228290081\n",
      "Epoch 9312/30000 Training Loss: 0.04905327409505844\n",
      "Epoch 9313/30000 Training Loss: 0.04814256355166435\n",
      "Epoch 9314/30000 Training Loss: 0.0517205074429512\n",
      "Epoch 9315/30000 Training Loss: 0.0392511822283268\n",
      "Epoch 9316/30000 Training Loss: 0.04476659372448921\n",
      "Epoch 9317/30000 Training Loss: 0.04949657991528511\n",
      "Epoch 9318/30000 Training Loss: 0.04176822304725647\n",
      "Epoch 9319/30000 Training Loss: 0.046720296144485474\n",
      "Epoch 9320/30000 Training Loss: 0.04141620546579361\n",
      "Epoch 9321/30000 Training Loss: 0.04578062519431114\n",
      "Epoch 9322/30000 Training Loss: 0.04111749306321144\n",
      "Epoch 9323/30000 Training Loss: 0.053823284804821014\n",
      "Epoch 9324/30000 Training Loss: 0.051208674907684326\n",
      "Epoch 9325/30000 Training Loss: 0.0550190731883049\n",
      "Epoch 9326/30000 Training Loss: 0.04135536774992943\n",
      "Epoch 9327/30000 Training Loss: 0.0464303232729435\n",
      "Epoch 9328/30000 Training Loss: 0.04630475863814354\n",
      "Epoch 9329/30000 Training Loss: 0.03590945154428482\n",
      "Epoch 9330/30000 Training Loss: 0.05753657966852188\n",
      "Epoch 9331/30000 Training Loss: 0.03858053684234619\n",
      "Epoch 9332/30000 Training Loss: 0.04188460111618042\n",
      "Epoch 9333/30000 Training Loss: 0.04653380438685417\n",
      "Epoch 9334/30000 Training Loss: 0.03558977693319321\n",
      "Epoch 9335/30000 Training Loss: 0.04172677919268608\n",
      "Epoch 9336/30000 Training Loss: 0.04779204726219177\n",
      "Epoch 9337/30000 Training Loss: 0.05040251463651657\n",
      "Epoch 9338/30000 Training Loss: 0.0516187846660614\n",
      "Epoch 9339/30000 Training Loss: 0.049193598330020905\n",
      "Epoch 9340/30000 Training Loss: 0.0510401725769043\n",
      "Epoch 9341/30000 Training Loss: 0.04543979838490486\n",
      "Epoch 9342/30000 Training Loss: 0.05129115656018257\n",
      "Epoch 9343/30000 Training Loss: 0.04176082834601402\n",
      "Epoch 9344/30000 Training Loss: 0.03855915740132332\n",
      "Epoch 9345/30000 Training Loss: 0.04824315756559372\n",
      "Epoch 9346/30000 Training Loss: 0.05628461763262749\n",
      "Epoch 9347/30000 Training Loss: 0.04951699823141098\n",
      "Epoch 9348/30000 Training Loss: 0.0493941530585289\n",
      "Epoch 9349/30000 Training Loss: 0.052984364330768585\n",
      "Epoch 9350/30000 Training Loss: 0.05546357110142708\n",
      "Epoch 9350/30000 Validation Loss: 0.04704143479466438\n",
      "Epoch 9351/30000 Training Loss: 0.04925951361656189\n",
      "Epoch 9352/30000 Training Loss: 0.05698401853442192\n",
      "Epoch 9353/30000 Training Loss: 0.04690355062484741\n",
      "Epoch 9354/30000 Training Loss: 0.04567863047122955\n",
      "Epoch 9355/30000 Training Loss: 0.052160393446683884\n",
      "Epoch 9356/30000 Training Loss: 0.05542702600359917\n",
      "Epoch 9357/30000 Training Loss: 0.047255080193281174\n",
      "Epoch 9358/30000 Training Loss: 0.05104865878820419\n",
      "Epoch 9359/30000 Training Loss: 0.05634135752916336\n",
      "Epoch 9360/30000 Training Loss: 0.0461982898414135\n",
      "Epoch 9361/30000 Training Loss: 0.042836032807826996\n",
      "Epoch 9362/30000 Training Loss: 0.05055198818445206\n",
      "Epoch 9363/30000 Training Loss: 0.04628446698188782\n",
      "Epoch 9364/30000 Training Loss: 0.04323428124189377\n",
      "Epoch 9365/30000 Training Loss: 0.04255280643701553\n",
      "Epoch 9366/30000 Training Loss: 0.05353691428899765\n",
      "Epoch 9367/30000 Training Loss: 0.05046378821134567\n",
      "Epoch 9368/30000 Training Loss: 0.05407465621829033\n",
      "Epoch 9369/30000 Training Loss: 0.04844105988740921\n",
      "Epoch 9370/30000 Training Loss: 0.04016111046075821\n",
      "Epoch 9371/30000 Training Loss: 0.05618990585207939\n",
      "Epoch 9372/30000 Training Loss: 0.04825931787490845\n",
      "Epoch 9373/30000 Training Loss: 0.05438771843910217\n",
      "Epoch 9374/30000 Training Loss: 0.044313836842775345\n",
      "Epoch 9375/30000 Training Loss: 0.05055566504597664\n",
      "Epoch 9376/30000 Training Loss: 0.04767918586730957\n",
      "Epoch 9377/30000 Training Loss: 0.05063246563076973\n",
      "Epoch 9378/30000 Training Loss: 0.04561062902212143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9379/30000 Training Loss: 0.041399795562028885\n",
      "Epoch 9380/30000 Training Loss: 0.04836650937795639\n",
      "Epoch 9381/30000 Training Loss: 0.0487317331135273\n",
      "Epoch 9382/30000 Training Loss: 0.04850709065794945\n",
      "Epoch 9383/30000 Training Loss: 0.04488412290811539\n",
      "Epoch 9384/30000 Training Loss: 0.04098194092512131\n",
      "Epoch 9385/30000 Training Loss: 0.05113641545176506\n",
      "Epoch 9386/30000 Training Loss: 0.05005539208650589\n",
      "Epoch 9387/30000 Training Loss: 0.04134928807616234\n",
      "Epoch 9388/30000 Training Loss: 0.051934290677309036\n",
      "Epoch 9389/30000 Training Loss: 0.04387654736638069\n",
      "Epoch 9390/30000 Training Loss: 0.052525244653224945\n",
      "Epoch 9391/30000 Training Loss: 0.043523602187633514\n",
      "Epoch 9392/30000 Training Loss: 0.045449335128068924\n",
      "Epoch 9393/30000 Training Loss: 0.04677034169435501\n",
      "Epoch 9394/30000 Training Loss: 0.04338487610220909\n",
      "Epoch 9395/30000 Training Loss: 0.04962245374917984\n",
      "Epoch 9396/30000 Training Loss: 0.04622013494372368\n",
      "Epoch 9397/30000 Training Loss: 0.045897167176008224\n",
      "Epoch 9398/30000 Training Loss: 0.053429245948791504\n",
      "Epoch 9399/30000 Training Loss: 0.0516074076294899\n",
      "Epoch 9400/30000 Training Loss: 0.04981876537203789\n",
      "Epoch 9400/30000 Validation Loss: 0.04641955345869064\n",
      "Epoch 9401/30000 Training Loss: 0.05028194189071655\n",
      "Epoch 9402/30000 Training Loss: 0.05039014667272568\n",
      "Epoch 9403/30000 Training Loss: 0.04719232767820358\n",
      "Epoch 9404/30000 Training Loss: 0.05092189460992813\n",
      "Epoch 9405/30000 Training Loss: 0.041849538683891296\n",
      "Epoch 9406/30000 Training Loss: 0.04764349386096001\n",
      "Epoch 9407/30000 Training Loss: 0.051939237862825394\n",
      "Epoch 9408/30000 Training Loss: 0.04792547598481178\n",
      "Epoch 9409/30000 Training Loss: 0.05512286350131035\n",
      "Epoch 9410/30000 Training Loss: 0.03836603835225105\n",
      "Epoch 9411/30000 Training Loss: 0.046640679240226746\n",
      "Epoch 9412/30000 Training Loss: 0.03983689099550247\n",
      "Epoch 9413/30000 Training Loss: 0.045175038278102875\n",
      "Epoch 9414/30000 Training Loss: 0.0564129538834095\n",
      "Epoch 9415/30000 Training Loss: 0.04138394445180893\n",
      "Epoch 9416/30000 Training Loss: 0.0590529665350914\n",
      "Epoch 9417/30000 Training Loss: 0.04967065900564194\n",
      "Epoch 9418/30000 Training Loss: 0.056520976126194\n",
      "Epoch 9419/30000 Training Loss: 0.04779845476150513\n",
      "Epoch 9420/30000 Training Loss: 0.04102141410112381\n",
      "Epoch 9421/30000 Training Loss: 0.046434931457042694\n",
      "Epoch 9422/30000 Training Loss: 0.04670826718211174\n",
      "Epoch 9423/30000 Training Loss: 0.04199966415762901\n",
      "Epoch 9424/30000 Training Loss: 0.0416816845536232\n",
      "Epoch 9425/30000 Training Loss: 0.053702086210250854\n",
      "Epoch 9426/30000 Training Loss: 0.04707605391740799\n",
      "Epoch 9427/30000 Training Loss: 0.046080224215984344\n",
      "Epoch 9428/30000 Training Loss: 0.053959060460329056\n",
      "Epoch 9429/30000 Training Loss: 0.05202469974756241\n",
      "Epoch 9430/30000 Training Loss: 0.04548412561416626\n",
      "Epoch 9431/30000 Training Loss: 0.04584472253918648\n",
      "Epoch 9432/30000 Training Loss: 0.04074642062187195\n",
      "Epoch 9433/30000 Training Loss: 0.05513528734445572\n",
      "Epoch 9434/30000 Training Loss: 0.053804002702236176\n",
      "Epoch 9435/30000 Training Loss: 0.0525861456990242\n",
      "Epoch 9436/30000 Training Loss: 0.053479839116334915\n",
      "Epoch 9437/30000 Training Loss: 0.05461942031979561\n",
      "Epoch 9438/30000 Training Loss: 0.04945044964551926\n",
      "Epoch 9439/30000 Training Loss: 0.04761330038309097\n",
      "Epoch 9440/30000 Training Loss: 0.04157540947198868\n",
      "Epoch 9441/30000 Training Loss: 0.04983024671673775\n",
      "Epoch 9442/30000 Training Loss: 0.0418853797018528\n",
      "Epoch 9443/30000 Training Loss: 0.037758514285087585\n",
      "Epoch 9444/30000 Training Loss: 0.04094813019037247\n",
      "Epoch 9445/30000 Training Loss: 0.04912150278687477\n",
      "Epoch 9446/30000 Training Loss: 0.048669181764125824\n",
      "Epoch 9447/30000 Training Loss: 0.04695335775613785\n",
      "Epoch 9448/30000 Training Loss: 0.0395200178027153\n",
      "Epoch 9449/30000 Training Loss: 0.05105433985590935\n",
      "Epoch 9450/30000 Training Loss: 0.0475543737411499\n",
      "Epoch 9450/30000 Validation Loss: 0.051171183586120605\n",
      "Epoch 9451/30000 Training Loss: 0.04318936914205551\n",
      "Epoch 9452/30000 Training Loss: 0.03742154315114021\n",
      "Epoch 9453/30000 Training Loss: 0.057623863220214844\n",
      "Epoch 9454/30000 Training Loss: 0.04493824392557144\n",
      "Epoch 9455/30000 Training Loss: 0.04244208708405495\n",
      "Epoch 9456/30000 Training Loss: 0.038611527532339096\n",
      "Epoch 9457/30000 Training Loss: 0.04032722860574722\n",
      "Epoch 9458/30000 Training Loss: 0.04605496674776077\n",
      "Epoch 9459/30000 Training Loss: 0.04764232411980629\n",
      "Epoch 9460/30000 Training Loss: 0.04340842738747597\n",
      "Epoch 9461/30000 Training Loss: 0.05134589597582817\n",
      "Epoch 9462/30000 Training Loss: 0.06150384992361069\n",
      "Epoch 9463/30000 Training Loss: 0.03891899809241295\n",
      "Epoch 9464/30000 Training Loss: 0.04799026995897293\n",
      "Epoch 9465/30000 Training Loss: 0.04670881852507591\n",
      "Epoch 9466/30000 Training Loss: 0.05300595238804817\n",
      "Epoch 9467/30000 Training Loss: 0.05448994040489197\n",
      "Epoch 9468/30000 Training Loss: 0.052707064896821976\n",
      "Epoch 9469/30000 Training Loss: 0.048564594238996506\n",
      "Epoch 9470/30000 Training Loss: 0.04446547105908394\n",
      "Epoch 9471/30000 Training Loss: 0.043266355991363525\n",
      "Epoch 9472/30000 Training Loss: 0.05692301318049431\n",
      "Epoch 9473/30000 Training Loss: 0.040245555341243744\n",
      "Epoch 9474/30000 Training Loss: 0.044394638389348984\n",
      "Epoch 9475/30000 Training Loss: 0.0536356084048748\n",
      "Epoch 9476/30000 Training Loss: 0.05298937112092972\n",
      "Epoch 9477/30000 Training Loss: 0.05061265826225281\n",
      "Epoch 9478/30000 Training Loss: 0.0489339642226696\n",
      "Epoch 9479/30000 Training Loss: 0.04194377362728119\n",
      "Epoch 9480/30000 Training Loss: 0.043409563601017\n",
      "Epoch 9481/30000 Training Loss: 0.04605209082365036\n",
      "Epoch 9482/30000 Training Loss: 0.04970579221844673\n",
      "Epoch 9483/30000 Training Loss: 0.04042927175760269\n",
      "Epoch 9484/30000 Training Loss: 0.05467109754681587\n",
      "Epoch 9485/30000 Training Loss: 0.04460133984684944\n",
      "Epoch 9486/30000 Training Loss: 0.05100349709391594\n",
      "Epoch 9487/30000 Training Loss: 0.049645207822322845\n",
      "Epoch 9488/30000 Training Loss: 0.045416172593832016\n",
      "Epoch 9489/30000 Training Loss: 0.050242163240909576\n",
      "Epoch 9490/30000 Training Loss: 0.04625294730067253\n",
      "Epoch 9491/30000 Training Loss: 0.04856257513165474\n",
      "Epoch 9492/30000 Training Loss: 0.05101552605628967\n",
      "Epoch 9493/30000 Training Loss: 0.04264145344495773\n",
      "Epoch 9494/30000 Training Loss: 0.04050326347351074\n",
      "Epoch 9495/30000 Training Loss: 0.04186196252703667\n",
      "Epoch 9496/30000 Training Loss: 0.0497930571436882\n",
      "Epoch 9497/30000 Training Loss: 0.044066935777664185\n",
      "Epoch 9498/30000 Training Loss: 0.04162713140249252\n",
      "Epoch 9499/30000 Training Loss: 0.04954047128558159\n",
      "Epoch 9500/30000 Training Loss: 0.052166443318128586\n",
      "Epoch 9500/30000 Validation Loss: 0.04425272345542908\n",
      "Epoch 9501/30000 Training Loss: 0.05243823677301407\n",
      "Epoch 9502/30000 Training Loss: 0.05639380216598511\n",
      "Epoch 9503/30000 Training Loss: 0.046424027532339096\n",
      "Epoch 9504/30000 Training Loss: 0.042582251131534576\n",
      "Epoch 9505/30000 Training Loss: 0.05458466336131096\n",
      "Epoch 9506/30000 Training Loss: 0.04425002261996269\n",
      "Epoch 9507/30000 Training Loss: 0.04822571948170662\n",
      "Epoch 9508/30000 Training Loss: 0.045814838260412216\n",
      "Epoch 9509/30000 Training Loss: 0.04091387242078781\n",
      "Epoch 9510/30000 Training Loss: 0.04129336029291153\n",
      "Epoch 9511/30000 Training Loss: 0.043804410845041275\n",
      "Epoch 9512/30000 Training Loss: 0.04071566462516785\n",
      "Epoch 9513/30000 Training Loss: 0.047355957329273224\n",
      "Epoch 9514/30000 Training Loss: 0.0442429855465889\n",
      "Epoch 9515/30000 Training Loss: 0.04517997428774834\n",
      "Epoch 9516/30000 Training Loss: 0.04939604923129082\n",
      "Epoch 9517/30000 Training Loss: 0.044426724314689636\n",
      "Epoch 9518/30000 Training Loss: 0.04320811107754707\n",
      "Epoch 9519/30000 Training Loss: 0.0353083461523056\n",
      "Epoch 9520/30000 Training Loss: 0.04317016154527664\n",
      "Epoch 9521/30000 Training Loss: 0.058023594319820404\n",
      "Epoch 9522/30000 Training Loss: 0.05160420015454292\n",
      "Epoch 9523/30000 Training Loss: 0.042306892573833466\n",
      "Epoch 9524/30000 Training Loss: 0.058095015585422516\n",
      "Epoch 9525/30000 Training Loss: 0.04926266521215439\n",
      "Epoch 9526/30000 Training Loss: 0.046243783086538315\n",
      "Epoch 9527/30000 Training Loss: 0.04589623957872391\n",
      "Epoch 9528/30000 Training Loss: 0.048051320016384125\n",
      "Epoch 9529/30000 Training Loss: 0.05578988045454025\n",
      "Epoch 9530/30000 Training Loss: 0.04897822067141533\n",
      "Epoch 9531/30000 Training Loss: 0.045291583985090256\n",
      "Epoch 9532/30000 Training Loss: 0.05024188011884689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9533/30000 Training Loss: 0.04506797343492508\n",
      "Epoch 9534/30000 Training Loss: 0.05021021515130997\n",
      "Epoch 9535/30000 Training Loss: 0.05385453253984451\n",
      "Epoch 9536/30000 Training Loss: 0.05800073593854904\n",
      "Epoch 9537/30000 Training Loss: 0.04636828228831291\n",
      "Epoch 9538/30000 Training Loss: 0.047296859323978424\n",
      "Epoch 9539/30000 Training Loss: 0.04867366701364517\n",
      "Epoch 9540/30000 Training Loss: 0.05333145335316658\n",
      "Epoch 9541/30000 Training Loss: 0.04610789567232132\n",
      "Epoch 9542/30000 Training Loss: 0.04758652672171593\n",
      "Epoch 9543/30000 Training Loss: 0.04828411340713501\n",
      "Epoch 9544/30000 Training Loss: 0.04433761537075043\n",
      "Epoch 9545/30000 Training Loss: 0.0598808154463768\n",
      "Epoch 9546/30000 Training Loss: 0.046004071831703186\n",
      "Epoch 9547/30000 Training Loss: 0.044822655618190765\n",
      "Epoch 9548/30000 Training Loss: 0.05146590992808342\n",
      "Epoch 9549/30000 Training Loss: 0.05176674202084541\n",
      "Epoch 9550/30000 Training Loss: 0.05542784184217453\n",
      "Epoch 9550/30000 Validation Loss: 0.05445485562086105\n",
      "Epoch 9551/30000 Training Loss: 0.056693702936172485\n",
      "Epoch 9552/30000 Training Loss: 0.04387279599905014\n",
      "Epoch 9553/30000 Training Loss: 0.05609588697552681\n",
      "Epoch 9554/30000 Training Loss: 0.044810324907302856\n",
      "Epoch 9555/30000 Training Loss: 0.044239677488803864\n",
      "Epoch 9556/30000 Training Loss: 0.04790089279413223\n",
      "Epoch 9557/30000 Training Loss: 0.03813683241605759\n",
      "Epoch 9558/30000 Training Loss: 0.046427272260189056\n",
      "Epoch 9559/30000 Training Loss: 0.04672659561038017\n",
      "Epoch 9560/30000 Training Loss: 0.050363946706056595\n",
      "Epoch 9561/30000 Training Loss: 0.04369202256202698\n",
      "Epoch 9562/30000 Training Loss: 0.035426657646894455\n",
      "Epoch 9563/30000 Training Loss: 0.048693615943193436\n",
      "Epoch 9564/30000 Training Loss: 0.04850924015045166\n",
      "Epoch 9565/30000 Training Loss: 0.04605266824364662\n",
      "Epoch 9566/30000 Training Loss: 0.044154565781354904\n",
      "Epoch 9567/30000 Training Loss: 0.04636318236589432\n",
      "Epoch 9568/30000 Training Loss: 0.04451281577348709\n",
      "Epoch 9569/30000 Training Loss: 0.048210058361291885\n",
      "Epoch 9570/30000 Training Loss: 0.0420057475566864\n",
      "Epoch 9571/30000 Training Loss: 0.0452987439930439\n",
      "Epoch 9572/30000 Training Loss: 0.0435975156724453\n",
      "Epoch 9573/30000 Training Loss: 0.04326963052153587\n",
      "Epoch 9574/30000 Training Loss: 0.04596123844385147\n",
      "Epoch 9575/30000 Training Loss: 0.04575260356068611\n",
      "Epoch 9576/30000 Training Loss: 0.05319448187947273\n",
      "Epoch 9577/30000 Training Loss: 0.05379485338926315\n",
      "Epoch 9578/30000 Training Loss: 0.043019384145736694\n",
      "Epoch 9579/30000 Training Loss: 0.051758475601673126\n",
      "Epoch 9580/30000 Training Loss: 0.049172885715961456\n",
      "Epoch 9581/30000 Training Loss: 0.054739732295274734\n",
      "Epoch 9582/30000 Training Loss: 0.04557391256093979\n",
      "Epoch 9583/30000 Training Loss: 0.04166717827320099\n",
      "Epoch 9584/30000 Training Loss: 0.044320397078990936\n",
      "Epoch 9585/30000 Training Loss: 0.04571616277098656\n",
      "Epoch 9586/30000 Training Loss: 0.051589228212833405\n",
      "Epoch 9587/30000 Training Loss: 0.051247112452983856\n",
      "Epoch 9588/30000 Training Loss: 0.04608961567282677\n",
      "Epoch 9589/30000 Training Loss: 0.048022203147411346\n",
      "Epoch 9590/30000 Training Loss: 0.045249056071043015\n",
      "Epoch 9591/30000 Training Loss: 0.05325917527079582\n",
      "Epoch 9592/30000 Training Loss: 0.04589054733514786\n",
      "Epoch 9593/30000 Training Loss: 0.03648551553487778\n",
      "Epoch 9594/30000 Training Loss: 0.042889468371868134\n",
      "Epoch 9595/30000 Training Loss: 0.04476774483919144\n",
      "Epoch 9596/30000 Training Loss: 0.04096332564949989\n",
      "Epoch 9597/30000 Training Loss: 0.04429518058896065\n",
      "Epoch 9598/30000 Training Loss: 0.05071046203374863\n",
      "Epoch 9599/30000 Training Loss: 0.05569276213645935\n",
      "Epoch 9600/30000 Training Loss: 0.04362481087446213\n",
      "Epoch 9600/30000 Validation Loss: 0.040960706770420074\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.040960706770420074<=============\n",
      "Epoch 9601/30000 Training Loss: 0.04595301300287247\n",
      "Epoch 9602/30000 Training Loss: 0.04864973947405815\n",
      "Epoch 9603/30000 Training Loss: 0.05132179334759712\n",
      "Epoch 9604/30000 Training Loss: 0.06145300343632698\n",
      "Epoch 9605/30000 Training Loss: 0.045577455312013626\n",
      "Epoch 9606/30000 Training Loss: 0.04797104001045227\n",
      "Epoch 9607/30000 Training Loss: 0.04744616150856018\n",
      "Epoch 9608/30000 Training Loss: 0.04005119949579239\n",
      "Epoch 9609/30000 Training Loss: 0.04339892417192459\n",
      "Epoch 9610/30000 Training Loss: 0.051051169633865356\n",
      "Epoch 9611/30000 Training Loss: 0.04869683086872101\n",
      "Epoch 9612/30000 Training Loss: 0.0527656264603138\n",
      "Epoch 9613/30000 Training Loss: 0.05373019725084305\n",
      "Epoch 9614/30000 Training Loss: 0.052214182913303375\n",
      "Epoch 9615/30000 Training Loss: 0.04064091295003891\n",
      "Epoch 9616/30000 Training Loss: 0.05059275031089783\n",
      "Epoch 9617/30000 Training Loss: 0.04643186181783676\n",
      "Epoch 9618/30000 Training Loss: 0.050561606884002686\n",
      "Epoch 9619/30000 Training Loss: 0.04681655019521713\n",
      "Epoch 9620/30000 Training Loss: 0.0460909828543663\n",
      "Epoch 9621/30000 Training Loss: 0.04561559110879898\n",
      "Epoch 9622/30000 Training Loss: 0.06309501081705093\n",
      "Epoch 9623/30000 Training Loss: 0.04324391111731529\n",
      "Epoch 9624/30000 Training Loss: 0.03989299386739731\n",
      "Epoch 9625/30000 Training Loss: 0.04333394765853882\n",
      "Epoch 9626/30000 Training Loss: 0.04627544805407524\n",
      "Epoch 9627/30000 Training Loss: 0.05307904630899429\n",
      "Epoch 9628/30000 Training Loss: 0.03680239990353584\n",
      "Epoch 9629/30000 Training Loss: 0.04833487793803215\n",
      "Epoch 9630/30000 Training Loss: 0.05097426101565361\n",
      "Epoch 9631/30000 Training Loss: 0.03996426612138748\n",
      "Epoch 9632/30000 Training Loss: 0.039569735527038574\n",
      "Epoch 9633/30000 Training Loss: 0.04853256419301033\n",
      "Epoch 9634/30000 Training Loss: 0.041681017726659775\n",
      "Epoch 9635/30000 Training Loss: 0.04287956655025482\n",
      "Epoch 9636/30000 Training Loss: 0.05434253811836243\n",
      "Epoch 9637/30000 Training Loss: 0.04304053261876106\n",
      "Epoch 9638/30000 Training Loss: 0.05609273910522461\n",
      "Epoch 9639/30000 Training Loss: 0.042914051562547684\n",
      "Epoch 9640/30000 Training Loss: 0.04435373470187187\n",
      "Epoch 9641/30000 Training Loss: 0.04470311477780342\n",
      "Epoch 9642/30000 Training Loss: 0.05765927955508232\n",
      "Epoch 9643/30000 Training Loss: 0.04873160272836685\n",
      "Epoch 9644/30000 Training Loss: 0.043239034712314606\n",
      "Epoch 9645/30000 Training Loss: 0.04550083354115486\n",
      "Epoch 9646/30000 Training Loss: 0.046867359429597855\n",
      "Epoch 9647/30000 Training Loss: 0.03852604702115059\n",
      "Epoch 9648/30000 Training Loss: 0.050320815294981\n",
      "Epoch 9649/30000 Training Loss: 0.04402204602956772\n",
      "Epoch 9650/30000 Training Loss: 0.04403666406869888\n",
      "Epoch 9650/30000 Validation Loss: 0.04604995995759964\n",
      "Epoch 9651/30000 Training Loss: 0.04498603194952011\n",
      "Epoch 9652/30000 Training Loss: 0.04494292289018631\n",
      "Epoch 9653/30000 Training Loss: 0.047657452523708344\n",
      "Epoch 9654/30000 Training Loss: 0.04077805578708649\n",
      "Epoch 9655/30000 Training Loss: 0.05076003819704056\n",
      "Epoch 9656/30000 Training Loss: 0.04700900986790657\n",
      "Epoch 9657/30000 Training Loss: 0.045487433671951294\n",
      "Epoch 9658/30000 Training Loss: 0.055142998695373535\n",
      "Epoch 9659/30000 Training Loss: 0.03534854203462601\n",
      "Epoch 9660/30000 Training Loss: 0.039141785353422165\n",
      "Epoch 9661/30000 Training Loss: 0.046167727559804916\n",
      "Epoch 9662/30000 Training Loss: 0.04580887407064438\n",
      "Epoch 9663/30000 Training Loss: 0.04991640895605087\n",
      "Epoch 9664/30000 Training Loss: 0.049624860286712646\n",
      "Epoch 9665/30000 Training Loss: 0.04647167772054672\n",
      "Epoch 9666/30000 Training Loss: 0.05590583011507988\n",
      "Epoch 9667/30000 Training Loss: 0.048797160387039185\n",
      "Epoch 9668/30000 Training Loss: 0.04517031088471413\n",
      "Epoch 9669/30000 Training Loss: 0.04832878336310387\n",
      "Epoch 9670/30000 Training Loss: 0.04368140175938606\n",
      "Epoch 9671/30000 Training Loss: 0.03906300291419029\n",
      "Epoch 9672/30000 Training Loss: 0.05084993317723274\n",
      "Epoch 9673/30000 Training Loss: 0.052284859120845795\n",
      "Epoch 9674/30000 Training Loss: 0.04918280988931656\n",
      "Epoch 9675/30000 Training Loss: 0.05064547061920166\n",
      "Epoch 9676/30000 Training Loss: 0.04594937711954117\n",
      "Epoch 9677/30000 Training Loss: 0.04849345237016678\n",
      "Epoch 9678/30000 Training Loss: 0.04754790663719177\n",
      "Epoch 9679/30000 Training Loss: 0.0589313879609108\n",
      "Epoch 9680/30000 Training Loss: 0.04337012767791748\n",
      "Epoch 9681/30000 Training Loss: 0.049164481461048126\n",
      "Epoch 9682/30000 Training Loss: 0.049217589199543\n",
      "Epoch 9683/30000 Training Loss: 0.04724334552884102\n",
      "Epoch 9684/30000 Training Loss: 0.049076810479164124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9685/30000 Training Loss: 0.04224059358239174\n",
      "Epoch 9686/30000 Training Loss: 0.04717493802309036\n",
      "Epoch 9687/30000 Training Loss: 0.05035998672246933\n",
      "Epoch 9688/30000 Training Loss: 0.05359392613172531\n",
      "Epoch 9689/30000 Training Loss: 0.043840158730745316\n",
      "Epoch 9690/30000 Training Loss: 0.04716373607516289\n",
      "Epoch 9691/30000 Training Loss: 0.047697361558675766\n",
      "Epoch 9692/30000 Training Loss: 0.05301664024591446\n",
      "Epoch 9693/30000 Training Loss: 0.04229559376835823\n",
      "Epoch 9694/30000 Training Loss: 0.041292350739240646\n",
      "Epoch 9695/30000 Training Loss: 0.046680763363838196\n",
      "Epoch 9696/30000 Training Loss: 0.04202891141176224\n",
      "Epoch 9697/30000 Training Loss: 0.04420594871044159\n",
      "Epoch 9698/30000 Training Loss: 0.059437334537506104\n",
      "Epoch 9699/30000 Training Loss: 0.03419982269406319\n",
      "Epoch 9700/30000 Training Loss: 0.050193119794130325\n",
      "Epoch 9700/30000 Validation Loss: 0.04340217635035515\n",
      "Epoch 9701/30000 Training Loss: 0.05890916660428047\n",
      "Epoch 9702/30000 Training Loss: 0.044940508902072906\n",
      "Epoch 9703/30000 Training Loss: 0.04516241326928139\n",
      "Epoch 9704/30000 Training Loss: 0.04778217151761055\n",
      "Epoch 9705/30000 Training Loss: 0.046926308423280716\n",
      "Epoch 9706/30000 Training Loss: 0.04797406494617462\n",
      "Epoch 9707/30000 Training Loss: 0.0400201641023159\n",
      "Epoch 9708/30000 Training Loss: 0.04105136916041374\n",
      "Epoch 9709/30000 Training Loss: 0.0358164981007576\n",
      "Epoch 9710/30000 Training Loss: 0.045287493616342545\n",
      "Epoch 9711/30000 Training Loss: 0.04449107497930527\n",
      "Epoch 9712/30000 Training Loss: 0.035650137811899185\n",
      "Epoch 9713/30000 Training Loss: 0.04242303594946861\n",
      "Epoch 9714/30000 Training Loss: 0.05221699923276901\n",
      "Epoch 9715/30000 Training Loss: 0.04638126492500305\n",
      "Epoch 9716/30000 Training Loss: 0.04596316069364548\n",
      "Epoch 9717/30000 Training Loss: 0.04974965751171112\n",
      "Epoch 9718/30000 Training Loss: 0.050678640604019165\n",
      "Epoch 9719/30000 Training Loss: 0.05485790967941284\n",
      "Epoch 9720/30000 Training Loss: 0.044090598821640015\n",
      "Epoch 9721/30000 Training Loss: 0.05212419107556343\n",
      "Epoch 9722/30000 Training Loss: 0.04969954490661621\n",
      "Epoch 9723/30000 Training Loss: 0.054768674075603485\n",
      "Epoch 9724/30000 Training Loss: 0.04756144434213638\n",
      "Epoch 9725/30000 Training Loss: 0.04563051089644432\n",
      "Epoch 9726/30000 Training Loss: 0.04748039320111275\n",
      "Epoch 9727/30000 Training Loss: 0.0490688756108284\n",
      "Epoch 9728/30000 Training Loss: 0.04724840074777603\n",
      "Epoch 9729/30000 Training Loss: 0.05030949041247368\n",
      "Epoch 9730/30000 Training Loss: 0.04373786225914955\n",
      "Epoch 9731/30000 Training Loss: 0.047075241804122925\n",
      "Epoch 9732/30000 Training Loss: 0.05063832923769951\n",
      "Epoch 9733/30000 Training Loss: 0.04141329973936081\n",
      "Epoch 9734/30000 Training Loss: 0.045273326337337494\n",
      "Epoch 9735/30000 Training Loss: 0.03776886686682701\n",
      "Epoch 9736/30000 Training Loss: 0.05316804721951485\n",
      "Epoch 9737/30000 Training Loss: 0.05335269123315811\n",
      "Epoch 9738/30000 Training Loss: 0.04653976857662201\n",
      "Epoch 9739/30000 Training Loss: 0.04952928423881531\n",
      "Epoch 9740/30000 Training Loss: 0.04374122992157936\n",
      "Epoch 9741/30000 Training Loss: 0.04325578361749649\n",
      "Epoch 9742/30000 Training Loss: 0.049695245921611786\n",
      "Epoch 9743/30000 Training Loss: 0.043079674243927\n",
      "Epoch 9744/30000 Training Loss: 0.04896777868270874\n",
      "Epoch 9745/30000 Training Loss: 0.04447831213474274\n",
      "Epoch 9746/30000 Training Loss: 0.04425370693206787\n",
      "Epoch 9747/30000 Training Loss: 0.059659749269485474\n",
      "Epoch 9748/30000 Training Loss: 0.04892545938491821\n",
      "Epoch 9749/30000 Training Loss: 0.04531395062804222\n",
      "Epoch 9750/30000 Training Loss: 0.04851150885224342\n",
      "Epoch 9750/30000 Validation Loss: 0.05227351188659668\n",
      "Epoch 9751/30000 Training Loss: 0.04238158464431763\n",
      "Epoch 9752/30000 Training Loss: 0.03823079541325569\n",
      "Epoch 9753/30000 Training Loss: 0.04158734157681465\n",
      "Epoch 9754/30000 Training Loss: 0.054395537823438644\n",
      "Epoch 9755/30000 Training Loss: 0.05041266232728958\n",
      "Epoch 9756/30000 Training Loss: 0.050973523408174515\n",
      "Epoch 9757/30000 Training Loss: 0.05394596606492996\n",
      "Epoch 9758/30000 Training Loss: 0.04047273099422455\n",
      "Epoch 9759/30000 Training Loss: 0.044099751859903336\n",
      "Epoch 9760/30000 Training Loss: 0.0442468598484993\n",
      "Epoch 9761/30000 Training Loss: 0.03992703557014465\n",
      "Epoch 9762/30000 Training Loss: 0.046819139271974564\n",
      "Epoch 9763/30000 Training Loss: 0.05548539757728577\n",
      "Epoch 9764/30000 Training Loss: 0.05239588022232056\n",
      "Epoch 9765/30000 Training Loss: 0.04276216775178909\n",
      "Epoch 9766/30000 Training Loss: 0.055118899792432785\n",
      "Epoch 9767/30000 Training Loss: 0.04115165024995804\n",
      "Epoch 9768/30000 Training Loss: 0.047283075749874115\n",
      "Epoch 9769/30000 Training Loss: 0.041806887835264206\n",
      "Epoch 9770/30000 Training Loss: 0.048102967441082\n",
      "Epoch 9771/30000 Training Loss: 0.04095502570271492\n",
      "Epoch 9772/30000 Training Loss: 0.04663455858826637\n",
      "Epoch 9773/30000 Training Loss: 0.04420767351984978\n",
      "Epoch 9774/30000 Training Loss: 0.04982339218258858\n",
      "Epoch 9775/30000 Training Loss: 0.04854217916727066\n",
      "Epoch 9776/30000 Training Loss: 0.043195970356464386\n",
      "Epoch 9777/30000 Training Loss: 0.044602006673812866\n",
      "Epoch 9778/30000 Training Loss: 0.046899981796741486\n",
      "Epoch 9779/30000 Training Loss: 0.04556799679994583\n",
      "Epoch 9780/30000 Training Loss: 0.04927656799554825\n",
      "Epoch 9781/30000 Training Loss: 0.03900018334388733\n",
      "Epoch 9782/30000 Training Loss: 0.048518020659685135\n",
      "Epoch 9783/30000 Training Loss: 0.05328638106584549\n",
      "Epoch 9784/30000 Training Loss: 0.05037124082446098\n",
      "Epoch 9785/30000 Training Loss: 0.04561147093772888\n",
      "Epoch 9786/30000 Training Loss: 0.04758588224649429\n",
      "Epoch 9787/30000 Training Loss: 0.038237038999795914\n",
      "Epoch 9788/30000 Training Loss: 0.044297415763139725\n",
      "Epoch 9789/30000 Training Loss: 0.04653162136673927\n",
      "Epoch 9790/30000 Training Loss: 0.0427836999297142\n",
      "Epoch 9791/30000 Training Loss: 0.042439933866262436\n",
      "Epoch 9792/30000 Training Loss: 0.04195558279752731\n",
      "Epoch 9793/30000 Training Loss: 0.04584074765443802\n",
      "Epoch 9794/30000 Training Loss: 0.04495080187916756\n",
      "Epoch 9795/30000 Training Loss: 0.046956539154052734\n",
      "Epoch 9796/30000 Training Loss: 0.061343781650066376\n",
      "Epoch 9797/30000 Training Loss: 0.048393845558166504\n",
      "Epoch 9798/30000 Training Loss: 0.04374925419688225\n",
      "Epoch 9799/30000 Training Loss: 0.04421650618314743\n",
      "Epoch 9800/30000 Training Loss: 0.0451664999127388\n",
      "Epoch 9800/30000 Validation Loss: 0.05029371380805969\n",
      "Epoch 9801/30000 Training Loss: 0.04502382129430771\n",
      "Epoch 9802/30000 Training Loss: 0.0489187128841877\n",
      "Epoch 9803/30000 Training Loss: 0.0500430092215538\n",
      "Epoch 9804/30000 Training Loss: 0.043229490518569946\n",
      "Epoch 9805/30000 Training Loss: 0.05032262206077576\n",
      "Epoch 9806/30000 Training Loss: 0.04898246377706528\n",
      "Epoch 9807/30000 Training Loss: 0.047743357717990875\n",
      "Epoch 9808/30000 Training Loss: 0.04701041057705879\n",
      "Epoch 9809/30000 Training Loss: 0.04449953883886337\n",
      "Epoch 9810/30000 Training Loss: 0.043627627193927765\n",
      "Epoch 9811/30000 Training Loss: 0.05347370356321335\n",
      "Epoch 9812/30000 Training Loss: 0.052966684103012085\n",
      "Epoch 9813/30000 Training Loss: 0.05580363795161247\n",
      "Epoch 9814/30000 Training Loss: 0.055402837693691254\n",
      "Epoch 9815/30000 Training Loss: 0.05031247064471245\n",
      "Epoch 9816/30000 Training Loss: 0.049703873693943024\n",
      "Epoch 9817/30000 Training Loss: 0.04501698166131973\n",
      "Epoch 9818/30000 Training Loss: 0.04806991294026375\n",
      "Epoch 9819/30000 Training Loss: 0.05699850991368294\n",
      "Epoch 9820/30000 Training Loss: 0.0517912432551384\n",
      "Epoch 9821/30000 Training Loss: 0.04856296256184578\n",
      "Epoch 9822/30000 Training Loss: 0.04311668127775192\n",
      "Epoch 9823/30000 Training Loss: 0.04458855092525482\n",
      "Epoch 9824/30000 Training Loss: 0.045735929161310196\n",
      "Epoch 9825/30000 Training Loss: 0.043805867433547974\n",
      "Epoch 9826/30000 Training Loss: 0.048583902418613434\n",
      "Epoch 9827/30000 Training Loss: 0.05065527558326721\n",
      "Epoch 9828/30000 Training Loss: 0.04767794534564018\n",
      "Epoch 9829/30000 Training Loss: 0.04589472711086273\n",
      "Epoch 9830/30000 Training Loss: 0.04623538628220558\n",
      "Epoch 9831/30000 Training Loss: 0.04019217938184738\n",
      "Epoch 9832/30000 Training Loss: 0.03804551810026169\n",
      "Epoch 9833/30000 Training Loss: 0.05017918348312378\n",
      "Epoch 9834/30000 Training Loss: 0.04239547252655029\n",
      "Epoch 9835/30000 Training Loss: 0.04959771782159805\n",
      "Epoch 9836/30000 Training Loss: 0.04496881365776062\n",
      "Epoch 9837/30000 Training Loss: 0.04650716856122017\n",
      "Epoch 9838/30000 Training Loss: 0.050766121596097946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9839/30000 Training Loss: 0.04608025774359703\n",
      "Epoch 9840/30000 Training Loss: 0.041921842843294144\n",
      "Epoch 9841/30000 Training Loss: 0.0456722266972065\n",
      "Epoch 9842/30000 Training Loss: 0.05684906989336014\n",
      "Epoch 9843/30000 Training Loss: 0.047916579991579056\n",
      "Epoch 9844/30000 Training Loss: 0.046778399497270584\n",
      "Epoch 9845/30000 Training Loss: 0.047349292784929276\n",
      "Epoch 9846/30000 Training Loss: 0.040543850511312485\n",
      "Epoch 9847/30000 Training Loss: 0.04068648815155029\n",
      "Epoch 9848/30000 Training Loss: 0.044338032603263855\n",
      "Epoch 9849/30000 Training Loss: 0.04760321229696274\n",
      "Epoch 9850/30000 Training Loss: 0.04305148497223854\n",
      "Epoch 9850/30000 Validation Loss: 0.04263296723365784\n",
      "Epoch 9851/30000 Training Loss: 0.040466371923685074\n",
      "Epoch 9852/30000 Training Loss: 0.05118599534034729\n",
      "Epoch 9853/30000 Training Loss: 0.04157925397157669\n",
      "Epoch 9854/30000 Training Loss: 0.04224226623773575\n",
      "Epoch 9855/30000 Training Loss: 0.0519338957965374\n",
      "Epoch 9856/30000 Training Loss: 0.041794292628765106\n",
      "Epoch 9857/30000 Training Loss: 0.04677104949951172\n",
      "Epoch 9858/30000 Training Loss: 0.04656586796045303\n",
      "Epoch 9859/30000 Training Loss: 0.044741950929164886\n",
      "Epoch 9860/30000 Training Loss: 0.0489344522356987\n",
      "Epoch 9861/30000 Training Loss: 0.05209551006555557\n",
      "Epoch 9862/30000 Training Loss: 0.04561951756477356\n",
      "Epoch 9863/30000 Training Loss: 0.04230229929089546\n",
      "Epoch 9864/30000 Training Loss: 0.04424215853214264\n",
      "Epoch 9865/30000 Training Loss: 0.04127500206232071\n",
      "Epoch 9866/30000 Training Loss: 0.04117733985185623\n",
      "Epoch 9867/30000 Training Loss: 0.047622814774513245\n",
      "Epoch 9868/30000 Training Loss: 0.047346122562885284\n",
      "Epoch 9869/30000 Training Loss: 0.05028604343533516\n",
      "Epoch 9870/30000 Training Loss: 0.047176163643598557\n",
      "Epoch 9871/30000 Training Loss: 0.04903233423829079\n",
      "Epoch 9872/30000 Training Loss: 0.05137064307928085\n",
      "Epoch 9873/30000 Training Loss: 0.059545379132032394\n",
      "Epoch 9874/30000 Training Loss: 0.0568196065723896\n",
      "Epoch 9875/30000 Training Loss: 0.047982051968574524\n",
      "Epoch 9876/30000 Training Loss: 0.050093840807676315\n",
      "Epoch 9877/30000 Training Loss: 0.035462986677885056\n",
      "Epoch 9878/30000 Training Loss: 0.04690302908420563\n",
      "Epoch 9879/30000 Training Loss: 0.04259259253740311\n",
      "Epoch 9880/30000 Training Loss: 0.053132008761167526\n",
      "Epoch 9881/30000 Training Loss: 0.043084755539894104\n",
      "Epoch 9882/30000 Training Loss: 0.048763081431388855\n",
      "Epoch 9883/30000 Training Loss: 0.04423150420188904\n",
      "Epoch 9884/30000 Training Loss: 0.047213517129421234\n",
      "Epoch 9885/30000 Training Loss: 0.050519950687885284\n",
      "Epoch 9886/30000 Training Loss: 0.04675940424203873\n",
      "Epoch 9887/30000 Training Loss: 0.04906003922224045\n",
      "Epoch 9888/30000 Training Loss: 0.05399734526872635\n",
      "Epoch 9889/30000 Training Loss: 0.044759705662727356\n",
      "Epoch 9890/30000 Training Loss: 0.05065666511654854\n",
      "Epoch 9891/30000 Training Loss: 0.04875268414616585\n",
      "Epoch 9892/30000 Training Loss: 0.04977545887231827\n",
      "Epoch 9893/30000 Training Loss: 0.049849823117256165\n",
      "Epoch 9894/30000 Training Loss: 0.037067484110593796\n",
      "Epoch 9895/30000 Training Loss: 0.042361821979284286\n",
      "Epoch 9896/30000 Training Loss: 0.0417228564620018\n",
      "Epoch 9897/30000 Training Loss: 0.05026070028543472\n",
      "Epoch 9898/30000 Training Loss: 0.04008757323026657\n",
      "Epoch 9899/30000 Training Loss: 0.057064931839704514\n",
      "Epoch 9900/30000 Training Loss: 0.04358628764748573\n",
      "Epoch 9900/30000 Validation Loss: 0.047203317284584045\n",
      "Epoch 9901/30000 Training Loss: 0.057654522359371185\n",
      "Epoch 9902/30000 Training Loss: 0.04888232424855232\n",
      "Epoch 9903/30000 Training Loss: 0.05738047510385513\n",
      "Epoch 9904/30000 Training Loss: 0.03946550935506821\n",
      "Epoch 9905/30000 Training Loss: 0.04741130769252777\n",
      "Epoch 9906/30000 Training Loss: 0.04867665842175484\n",
      "Epoch 9907/30000 Training Loss: 0.04639435186982155\n",
      "Epoch 9908/30000 Training Loss: 0.042281098663806915\n",
      "Epoch 9909/30000 Training Loss: 0.048827096819877625\n",
      "Epoch 9910/30000 Training Loss: 0.04391933232545853\n",
      "Epoch 9911/30000 Training Loss: 0.04731354862451553\n",
      "Epoch 9912/30000 Training Loss: 0.045987874269485474\n",
      "Epoch 9913/30000 Training Loss: 0.042538441717624664\n",
      "Epoch 9914/30000 Training Loss: 0.04132222384214401\n",
      "Epoch 9915/30000 Training Loss: 0.05672837048768997\n",
      "Epoch 9916/30000 Training Loss: 0.04707738384604454\n",
      "Epoch 9917/30000 Training Loss: 0.039920624345541\n",
      "Epoch 9918/30000 Training Loss: 0.046444155275821686\n",
      "Epoch 9919/30000 Training Loss: 0.049169424921274185\n",
      "Epoch 9920/30000 Training Loss: 0.048538681119680405\n",
      "Epoch 9921/30000 Training Loss: 0.0515265166759491\n",
      "Epoch 9922/30000 Training Loss: 0.050084520131349564\n",
      "Epoch 9923/30000 Training Loss: 0.0541270487010479\n",
      "Epoch 9924/30000 Training Loss: 0.05034364387392998\n",
      "Epoch 9925/30000 Training Loss: 0.039810750633478165\n",
      "Epoch 9926/30000 Training Loss: 0.04344205558300018\n",
      "Epoch 9927/30000 Training Loss: 0.04675310477614403\n",
      "Epoch 9928/30000 Training Loss: 0.04798583686351776\n",
      "Epoch 9929/30000 Training Loss: 0.04339161887764931\n",
      "Epoch 9930/30000 Training Loss: 0.04062963277101517\n",
      "Epoch 9931/30000 Training Loss: 0.04541994258761406\n",
      "Epoch 9932/30000 Training Loss: 0.04261227324604988\n",
      "Epoch 9933/30000 Training Loss: 0.05131412297487259\n",
      "Epoch 9934/30000 Training Loss: 0.04320386424660683\n",
      "Epoch 9935/30000 Training Loss: 0.04352764040231705\n",
      "Epoch 9936/30000 Training Loss: 0.04309254512190819\n",
      "Epoch 9937/30000 Training Loss: 0.04674355685710907\n",
      "Epoch 9938/30000 Training Loss: 0.04110889881849289\n",
      "Epoch 9939/30000 Training Loss: 0.04773833602666855\n",
      "Epoch 9940/30000 Training Loss: 0.04282522574067116\n",
      "Epoch 9941/30000 Training Loss: 0.045661963522434235\n",
      "Epoch 9942/30000 Training Loss: 0.05304975062608719\n",
      "Epoch 9943/30000 Training Loss: 0.04064243286848068\n",
      "Epoch 9944/30000 Training Loss: 0.042162902653217316\n",
      "Epoch 9945/30000 Training Loss: 0.06228761747479439\n",
      "Epoch 9946/30000 Training Loss: 0.045380156487226486\n",
      "Epoch 9947/30000 Training Loss: 0.04382537677884102\n",
      "Epoch 9948/30000 Training Loss: 0.039130426943302155\n",
      "Epoch 9949/30000 Training Loss: 0.053797680884599686\n",
      "Epoch 9950/30000 Training Loss: 0.05011249706149101\n",
      "Epoch 9950/30000 Validation Loss: 0.050475090742111206\n",
      "Epoch 9951/30000 Training Loss: 0.05432406812906265\n",
      "Epoch 9952/30000 Training Loss: 0.042156241834163666\n",
      "Epoch 9953/30000 Training Loss: 0.04679865390062332\n",
      "Epoch 9954/30000 Training Loss: 0.05124545842409134\n",
      "Epoch 9955/30000 Training Loss: 0.04750007390975952\n",
      "Epoch 9956/30000 Training Loss: 0.04179907590150833\n",
      "Epoch 9957/30000 Training Loss: 0.051068078726530075\n",
      "Epoch 9958/30000 Training Loss: 0.05092983692884445\n",
      "Epoch 9959/30000 Training Loss: 0.049087874591350555\n",
      "Epoch 9960/30000 Training Loss: 0.043897464871406555\n",
      "Epoch 9961/30000 Training Loss: 0.043922148644924164\n",
      "Epoch 9962/30000 Training Loss: 0.04713932424783707\n",
      "Epoch 9963/30000 Training Loss: 0.038723696023225784\n",
      "Epoch 9964/30000 Training Loss: 0.040326349437236786\n",
      "Epoch 9965/30000 Training Loss: 0.0426303893327713\n",
      "Epoch 9966/30000 Training Loss: 0.04170135781168938\n",
      "Epoch 9967/30000 Training Loss: 0.04340038448572159\n",
      "Epoch 9968/30000 Training Loss: 0.04658936709165573\n",
      "Epoch 9969/30000 Training Loss: 0.048967644572257996\n",
      "Epoch 9970/30000 Training Loss: 0.050007324665784836\n",
      "Epoch 9971/30000 Training Loss: 0.046158261597156525\n",
      "Epoch 9972/30000 Training Loss: 0.045301519334316254\n",
      "Epoch 9973/30000 Training Loss: 0.04649128392338753\n",
      "Epoch 9974/30000 Training Loss: 0.046188127249479294\n",
      "Epoch 9975/30000 Training Loss: 0.04874392971396446\n",
      "Epoch 9976/30000 Training Loss: 0.04349568486213684\n",
      "Epoch 9977/30000 Training Loss: 0.048368263989686966\n",
      "Epoch 9978/30000 Training Loss: 0.044657982885837555\n",
      "Epoch 9979/30000 Training Loss: 0.05016014724969864\n",
      "Epoch 9980/30000 Training Loss: 0.04223252832889557\n",
      "Epoch 9981/30000 Training Loss: 0.04618397727608681\n",
      "Epoch 9982/30000 Training Loss: 0.04686553776264191\n",
      "Epoch 9983/30000 Training Loss: 0.043040622025728226\n",
      "Epoch 9984/30000 Training Loss: 0.05177430436015129\n",
      "Epoch 9985/30000 Training Loss: 0.04842190816998482\n",
      "Epoch 9986/30000 Training Loss: 0.04924682155251503\n",
      "Epoch 9987/30000 Training Loss: 0.04801424965262413\n",
      "Epoch 9988/30000 Training Loss: 0.04296315461397171\n",
      "Epoch 9989/30000 Training Loss: 0.04677646607160568\n",
      "Epoch 9990/30000 Training Loss: 0.04509576037526131\n",
      "Epoch 9991/30000 Training Loss: 0.0518658272922039\n",
      "Epoch 9992/30000 Training Loss: 0.046731945127248764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9993/30000 Training Loss: 0.048422347754240036\n",
      "Epoch 9994/30000 Training Loss: 0.03838900476694107\n",
      "Epoch 9995/30000 Training Loss: 0.04943431168794632\n",
      "Epoch 9996/30000 Training Loss: 0.04189416021108627\n",
      "Epoch 9997/30000 Training Loss: 0.04966453090310097\n",
      "Epoch 9998/30000 Training Loss: 0.052118223160505295\n",
      "Epoch 9999/30000 Training Loss: 0.04425326734781265\n",
      "Epoch 10000/30000 Training Loss: 0.04448730871081352\n",
      "Epoch 10000/30000 Validation Loss: 0.0430850014090538\n",
      "Epoch 10001/30000 Training Loss: 0.038418468087911606\n",
      "Epoch 10002/30000 Training Loss: 0.037041302770376205\n",
      "Epoch 10003/30000 Training Loss: 0.04643939062952995\n",
      "Epoch 10004/30000 Training Loss: 0.04257090762257576\n",
      "Epoch 10005/30000 Training Loss: 0.04547128826379776\n",
      "Epoch 10006/30000 Training Loss: 0.04864109307527542\n",
      "Epoch 10007/30000 Training Loss: 0.04539390653371811\n",
      "Epoch 10008/30000 Training Loss: 0.04494711384177208\n",
      "Epoch 10009/30000 Training Loss: 0.041196659207344055\n",
      "Epoch 10010/30000 Training Loss: 0.04354168102145195\n",
      "Epoch 10011/30000 Training Loss: 0.04534732177853584\n",
      "Epoch 10012/30000 Training Loss: 0.042175304144620895\n",
      "Epoch 10013/30000 Training Loss: 0.05177199840545654\n",
      "Epoch 10014/30000 Training Loss: 0.04258621111512184\n",
      "Epoch 10015/30000 Training Loss: 0.04762767627835274\n",
      "Epoch 10016/30000 Training Loss: 0.05342348664999008\n",
      "Epoch 10017/30000 Training Loss: 0.0530296191573143\n",
      "Epoch 10018/30000 Training Loss: 0.04036257416009903\n",
      "Epoch 10019/30000 Training Loss: 0.04992172122001648\n",
      "Epoch 10020/30000 Training Loss: 0.04645128548145294\n",
      "Epoch 10021/30000 Training Loss: 0.0514724925160408\n",
      "Epoch 10022/30000 Training Loss: 0.03763357922434807\n",
      "Epoch 10023/30000 Training Loss: 0.045743219554424286\n",
      "Epoch 10024/30000 Training Loss: 0.04873441532254219\n",
      "Epoch 10025/30000 Training Loss: 0.053952522575855255\n",
      "Epoch 10026/30000 Training Loss: 0.05789266154170036\n",
      "Epoch 10027/30000 Training Loss: 0.04318700358271599\n",
      "Epoch 10028/30000 Training Loss: 0.04416999965906143\n",
      "Epoch 10029/30000 Training Loss: 0.043257005512714386\n",
      "Epoch 10030/30000 Training Loss: 0.04578952118754387\n",
      "Epoch 10031/30000 Training Loss: 0.044806577265262604\n",
      "Epoch 10032/30000 Training Loss: 0.048883408308029175\n",
      "Epoch 10033/30000 Training Loss: 0.039912864565849304\n",
      "Epoch 10034/30000 Training Loss: 0.05406291410326958\n",
      "Epoch 10035/30000 Training Loss: 0.04645179212093353\n",
      "Epoch 10036/30000 Training Loss: 0.04143344238400459\n",
      "Epoch 10037/30000 Training Loss: 0.05028333142399788\n",
      "Epoch 10038/30000 Training Loss: 0.05244214087724686\n",
      "Epoch 10039/30000 Training Loss: 0.045940570533275604\n",
      "Epoch 10040/30000 Training Loss: 0.04731267690658569\n",
      "Epoch 10041/30000 Training Loss: 0.04351267218589783\n",
      "Epoch 10042/30000 Training Loss: 0.045454997569322586\n",
      "Epoch 10043/30000 Training Loss: 0.04475090280175209\n",
      "Epoch 10044/30000 Training Loss: 0.05118321254849434\n",
      "Epoch 10045/30000 Training Loss: 0.04314865916967392\n",
      "Epoch 10046/30000 Training Loss: 0.0478830523788929\n",
      "Epoch 10047/30000 Training Loss: 0.04667162150144577\n",
      "Epoch 10048/30000 Training Loss: 0.048760171979665756\n",
      "Epoch 10049/30000 Training Loss: 0.049444131553173065\n",
      "Epoch 10050/30000 Training Loss: 0.04578055441379547\n",
      "Epoch 10050/30000 Validation Loss: 0.055503420531749725\n",
      "Epoch 10051/30000 Training Loss: 0.04649266600608826\n",
      "Epoch 10052/30000 Training Loss: 0.04679795354604721\n",
      "Epoch 10053/30000 Training Loss: 0.05640069767832756\n",
      "Epoch 10054/30000 Training Loss: 0.046720825135707855\n",
      "Epoch 10055/30000 Training Loss: 0.04784826561808586\n",
      "Epoch 10056/30000 Training Loss: 0.054166216403245926\n",
      "Epoch 10057/30000 Training Loss: 0.04596520587801933\n",
      "Epoch 10058/30000 Training Loss: 0.04586474597454071\n",
      "Epoch 10059/30000 Training Loss: 0.049239739775657654\n",
      "Epoch 10060/30000 Training Loss: 0.04611101374030113\n",
      "Epoch 10061/30000 Training Loss: 0.04727935418486595\n",
      "Epoch 10062/30000 Training Loss: 0.049826569855213165\n",
      "Epoch 10063/30000 Training Loss: 0.048528049141168594\n",
      "Epoch 10064/30000 Training Loss: 0.04451634734869003\n",
      "Epoch 10065/30000 Training Loss: 0.042693592607975006\n",
      "Epoch 10066/30000 Training Loss: 0.04701559618115425\n",
      "Epoch 10067/30000 Training Loss: 0.047543350607156754\n",
      "Epoch 10068/30000 Training Loss: 0.047786079347133636\n",
      "Epoch 10069/30000 Training Loss: 0.048815153539180756\n",
      "Epoch 10070/30000 Training Loss: 0.04800910875201225\n",
      "Epoch 10071/30000 Training Loss: 0.043068889528512955\n",
      "Epoch 10072/30000 Training Loss: 0.05555110424757004\n",
      "Epoch 10073/30000 Training Loss: 0.043845269829034805\n",
      "Epoch 10074/30000 Training Loss: 0.043337754905223846\n",
      "Epoch 10075/30000 Training Loss: 0.041619446128606796\n",
      "Epoch 10076/30000 Training Loss: 0.05578276515007019\n",
      "Epoch 10077/30000 Training Loss: 0.0407472625374794\n",
      "Epoch 10078/30000 Training Loss: 0.04531782492995262\n",
      "Epoch 10079/30000 Training Loss: 0.04399765655398369\n",
      "Epoch 10080/30000 Training Loss: 0.04553251713514328\n",
      "Epoch 10081/30000 Training Loss: 0.04266846925020218\n",
      "Epoch 10082/30000 Training Loss: 0.04387995973229408\n",
      "Epoch 10083/30000 Training Loss: 0.041996877640485764\n",
      "Epoch 10084/30000 Training Loss: 0.05667953938245773\n",
      "Epoch 10085/30000 Training Loss: 0.048740703612565994\n",
      "Epoch 10086/30000 Training Loss: 0.0502953939139843\n",
      "Epoch 10087/30000 Training Loss: 0.048138026148080826\n",
      "Epoch 10088/30000 Training Loss: 0.05140224099159241\n",
      "Epoch 10089/30000 Training Loss: 0.03921564295887947\n",
      "Epoch 10090/30000 Training Loss: 0.04955478757619858\n",
      "Epoch 10091/30000 Training Loss: 0.0408121757209301\n",
      "Epoch 10092/30000 Training Loss: 0.0358763262629509\n",
      "Epoch 10093/30000 Training Loss: 0.04234135523438454\n",
      "Epoch 10094/30000 Training Loss: 0.04053613170981407\n",
      "Epoch 10095/30000 Training Loss: 0.05220787972211838\n",
      "Epoch 10096/30000 Training Loss: 0.05265326052904129\n",
      "Epoch 10097/30000 Training Loss: 0.05825241282582283\n",
      "Epoch 10098/30000 Training Loss: 0.04364123195409775\n",
      "Epoch 10099/30000 Training Loss: 0.045508451759815216\n",
      "Epoch 10100/30000 Training Loss: 0.057912759482860565\n",
      "Epoch 10100/30000 Validation Loss: 0.056462228298187256\n",
      "Epoch 10101/30000 Training Loss: 0.05613252520561218\n",
      "Epoch 10102/30000 Training Loss: 0.04686477780342102\n",
      "Epoch 10103/30000 Training Loss: 0.045299023389816284\n",
      "Epoch 10104/30000 Training Loss: 0.04515349119901657\n",
      "Epoch 10105/30000 Training Loss: 0.04306815564632416\n",
      "Epoch 10106/30000 Training Loss: 0.05737911909818649\n",
      "Epoch 10107/30000 Training Loss: 0.04473884031176567\n",
      "Epoch 10108/30000 Training Loss: 0.0453483983874321\n",
      "Epoch 10109/30000 Training Loss: 0.04394476115703583\n",
      "Epoch 10110/30000 Training Loss: 0.05284937471151352\n",
      "Epoch 10111/30000 Training Loss: 0.04715012386441231\n",
      "Epoch 10112/30000 Training Loss: 0.04665132239460945\n",
      "Epoch 10113/30000 Training Loss: 0.04623163491487503\n",
      "Epoch 10114/30000 Training Loss: 0.04783078655600548\n",
      "Epoch 10115/30000 Training Loss: 0.04068790748715401\n",
      "Epoch 10116/30000 Training Loss: 0.05128602311015129\n",
      "Epoch 10117/30000 Training Loss: 0.04321088269352913\n",
      "Epoch 10118/30000 Training Loss: 0.04981396347284317\n",
      "Epoch 10119/30000 Training Loss: 0.04005586728453636\n",
      "Epoch 10120/30000 Training Loss: 0.0401332825422287\n",
      "Epoch 10121/30000 Training Loss: 0.043639250099658966\n",
      "Epoch 10122/30000 Training Loss: 0.04228648543357849\n",
      "Epoch 10123/30000 Training Loss: 0.05341019481420517\n",
      "Epoch 10124/30000 Training Loss: 0.04551909863948822\n",
      "Epoch 10125/30000 Training Loss: 0.05065388232469559\n",
      "Epoch 10126/30000 Training Loss: 0.04647333174943924\n",
      "Epoch 10127/30000 Training Loss: 0.05404770374298096\n",
      "Epoch 10128/30000 Training Loss: 0.0476481132209301\n",
      "Epoch 10129/30000 Training Loss: 0.047334618866443634\n",
      "Epoch 10130/30000 Training Loss: 0.04496898874640465\n",
      "Epoch 10131/30000 Training Loss: 0.05189377814531326\n",
      "Epoch 10132/30000 Training Loss: 0.0557556226849556\n",
      "Epoch 10133/30000 Training Loss: 0.0438525564968586\n",
      "Epoch 10134/30000 Training Loss: 0.056644875556230545\n",
      "Epoch 10135/30000 Training Loss: 0.040438245981931686\n",
      "Epoch 10136/30000 Training Loss: 0.03934277966618538\n",
      "Epoch 10137/30000 Training Loss: 0.05676677078008652\n",
      "Epoch 10138/30000 Training Loss: 0.04719036817550659\n",
      "Epoch 10139/30000 Training Loss: 0.045957714319229126\n",
      "Epoch 10140/30000 Training Loss: 0.04736994579434395\n",
      "Epoch 10141/30000 Training Loss: 0.04126127064228058\n",
      "Epoch 10142/30000 Training Loss: 0.04385889321565628\n",
      "Epoch 10143/30000 Training Loss: 0.049053844064474106\n",
      "Epoch 10144/30000 Training Loss: 0.040041662752628326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10145/30000 Training Loss: 0.045820970088243484\n",
      "Epoch 10146/30000 Training Loss: 0.055790603160858154\n",
      "Epoch 10147/30000 Training Loss: 0.05777483060956001\n",
      "Epoch 10148/30000 Training Loss: 0.04399843141436577\n",
      "Epoch 10149/30000 Training Loss: 0.04363684728741646\n",
      "Epoch 10150/30000 Training Loss: 0.048475660383701324\n",
      "Epoch 10150/30000 Validation Loss: 0.04935384541749954\n",
      "Epoch 10151/30000 Training Loss: 0.03976500779390335\n",
      "Epoch 10152/30000 Training Loss: 0.04647839069366455\n",
      "Epoch 10153/30000 Training Loss: 0.046573273837566376\n",
      "Epoch 10154/30000 Training Loss: 0.040796104818582535\n",
      "Epoch 10155/30000 Training Loss: 0.042208749800920486\n",
      "Epoch 10156/30000 Training Loss: 0.046312764286994934\n",
      "Epoch 10157/30000 Training Loss: 0.045748163014650345\n",
      "Epoch 10158/30000 Training Loss: 0.05229397490620613\n",
      "Epoch 10159/30000 Training Loss: 0.04683998227119446\n",
      "Epoch 10160/30000 Training Loss: 0.05236492678523064\n",
      "Epoch 10161/30000 Training Loss: 0.051314592361450195\n",
      "Epoch 10162/30000 Training Loss: 0.0470084622502327\n",
      "Epoch 10163/30000 Training Loss: 0.04846678301692009\n",
      "Epoch 10164/30000 Training Loss: 0.05473172664642334\n",
      "Epoch 10165/30000 Training Loss: 0.03930109366774559\n",
      "Epoch 10166/30000 Training Loss: 0.05040793493390083\n",
      "Epoch 10167/30000 Training Loss: 0.04413171857595444\n",
      "Epoch 10168/30000 Training Loss: 0.041356224566698074\n",
      "Epoch 10169/30000 Training Loss: 0.05796782299876213\n",
      "Epoch 10170/30000 Training Loss: 0.042241085320711136\n",
      "Epoch 10171/30000 Training Loss: 0.049847185611724854\n",
      "Epoch 10172/30000 Training Loss: 0.047208383679389954\n",
      "Epoch 10173/30000 Training Loss: 0.04942633956670761\n",
      "Epoch 10174/30000 Training Loss: 0.051834046840667725\n",
      "Epoch 10175/30000 Training Loss: 0.045873381197452545\n",
      "Epoch 10176/30000 Training Loss: 0.04884453862905502\n",
      "Epoch 10177/30000 Training Loss: 0.04201211780309677\n",
      "Epoch 10178/30000 Training Loss: 0.046182628720998764\n",
      "Epoch 10179/30000 Training Loss: 0.052168793976306915\n",
      "Epoch 10180/30000 Training Loss: 0.04104987531900406\n",
      "Epoch 10181/30000 Training Loss: 0.04475311189889908\n",
      "Epoch 10182/30000 Training Loss: 0.05216144770383835\n",
      "Epoch 10183/30000 Training Loss: 0.050540339201688766\n",
      "Epoch 10184/30000 Training Loss: 0.04636736959218979\n",
      "Epoch 10185/30000 Training Loss: 0.04909577593207359\n",
      "Epoch 10186/30000 Training Loss: 0.045477092266082764\n",
      "Epoch 10187/30000 Training Loss: 0.05090482160449028\n",
      "Epoch 10188/30000 Training Loss: 0.04310450330376625\n",
      "Epoch 10189/30000 Training Loss: 0.05005071684718132\n",
      "Epoch 10190/30000 Training Loss: 0.04505360871553421\n",
      "Epoch 10191/30000 Training Loss: 0.05781962350010872\n",
      "Epoch 10192/30000 Training Loss: 0.04330644756555557\n",
      "Epoch 10193/30000 Training Loss: 0.04752332717180252\n",
      "Epoch 10194/30000 Training Loss: 0.04375492408871651\n",
      "Epoch 10195/30000 Training Loss: 0.04064764454960823\n",
      "Epoch 10196/30000 Training Loss: 0.05145777016878128\n",
      "Epoch 10197/30000 Training Loss: 0.04734859988093376\n",
      "Epoch 10198/30000 Training Loss: 0.04572894051671028\n",
      "Epoch 10199/30000 Training Loss: 0.05231484770774841\n",
      "Epoch 10200/30000 Training Loss: 0.041574858129024506\n",
      "Epoch 10200/30000 Validation Loss: 0.048439137637615204\n",
      "Epoch 10201/30000 Training Loss: 0.04619593173265457\n",
      "Epoch 10202/30000 Training Loss: 0.04140711575746536\n",
      "Epoch 10203/30000 Training Loss: 0.04657341167330742\n",
      "Epoch 10204/30000 Training Loss: 0.04806336760520935\n",
      "Epoch 10205/30000 Training Loss: 0.0495002456009388\n",
      "Epoch 10206/30000 Training Loss: 0.04495052620768547\n",
      "Epoch 10207/30000 Training Loss: 0.04385042563080788\n",
      "Epoch 10208/30000 Training Loss: 0.050477318465709686\n",
      "Epoch 10209/30000 Training Loss: 0.04083807021379471\n",
      "Epoch 10210/30000 Training Loss: 0.04344449192285538\n",
      "Epoch 10211/30000 Training Loss: 0.044862132519483566\n",
      "Epoch 10212/30000 Training Loss: 0.04777207225561142\n",
      "Epoch 10213/30000 Training Loss: 0.04900817573070526\n",
      "Epoch 10214/30000 Training Loss: 0.05150613933801651\n",
      "Epoch 10215/30000 Training Loss: 0.0486716702580452\n",
      "Epoch 10216/30000 Training Loss: 0.05229794234037399\n",
      "Epoch 10217/30000 Training Loss: 0.03698006272315979\n",
      "Epoch 10218/30000 Training Loss: 0.046792007982730865\n",
      "Epoch 10219/30000 Training Loss: 0.04830097034573555\n",
      "Epoch 10220/30000 Training Loss: 0.04595322161912918\n",
      "Epoch 10221/30000 Training Loss: 0.04586910828948021\n",
      "Epoch 10222/30000 Training Loss: 0.05320272594690323\n",
      "Epoch 10223/30000 Training Loss: 0.04996215179562569\n",
      "Epoch 10224/30000 Training Loss: 0.046449050307273865\n",
      "Epoch 10225/30000 Training Loss: 0.0503806471824646\n",
      "Epoch 10226/30000 Training Loss: 0.047363635152578354\n",
      "Epoch 10227/30000 Training Loss: 0.043490491807460785\n",
      "Epoch 10228/30000 Training Loss: 0.06359182298183441\n",
      "Epoch 10229/30000 Training Loss: 0.04142380878329277\n",
      "Epoch 10230/30000 Training Loss: 0.04783673956990242\n",
      "Epoch 10231/30000 Training Loss: 0.04074544459581375\n",
      "Epoch 10232/30000 Training Loss: 0.045945458114147186\n",
      "Epoch 10233/30000 Training Loss: 0.051630668342113495\n",
      "Epoch 10234/30000 Training Loss: 0.04291310906410217\n",
      "Epoch 10235/30000 Training Loss: 0.04377983137965202\n",
      "Epoch 10236/30000 Training Loss: 0.04772607237100601\n",
      "Epoch 10237/30000 Training Loss: 0.045240920037031174\n",
      "Epoch 10238/30000 Training Loss: 0.041804421693086624\n",
      "Epoch 10239/30000 Training Loss: 0.04616403207182884\n",
      "Epoch 10240/30000 Training Loss: 0.05068103224039078\n",
      "Epoch 10241/30000 Training Loss: 0.04388732090592384\n",
      "Epoch 10242/30000 Training Loss: 0.03829890862107277\n",
      "Epoch 10243/30000 Training Loss: 0.045778267085552216\n",
      "Epoch 10244/30000 Training Loss: 0.04940931126475334\n",
      "Epoch 10245/30000 Training Loss: 0.04747942090034485\n",
      "Epoch 10246/30000 Training Loss: 0.04250018671154976\n",
      "Epoch 10247/30000 Training Loss: 0.04484080523252487\n",
      "Epoch 10248/30000 Training Loss: 0.04916731268167496\n",
      "Epoch 10249/30000 Training Loss: 0.04628210514783859\n",
      "Epoch 10250/30000 Training Loss: 0.052500784397125244\n",
      "Epoch 10250/30000 Validation Loss: 0.04879695177078247\n",
      "Epoch 10251/30000 Training Loss: 0.04473124444484711\n",
      "Epoch 10252/30000 Training Loss: 0.05351397395133972\n",
      "Epoch 10253/30000 Training Loss: 0.04971475154161453\n",
      "Epoch 10254/30000 Training Loss: 0.052910059690475464\n",
      "Epoch 10255/30000 Training Loss: 0.0521264374256134\n",
      "Epoch 10256/30000 Training Loss: 0.05113544315099716\n",
      "Epoch 10257/30000 Training Loss: 0.052216459065675735\n",
      "Epoch 10258/30000 Training Loss: 0.04512127861380577\n",
      "Epoch 10259/30000 Training Loss: 0.043035659939050674\n",
      "Epoch 10260/30000 Training Loss: 0.04636890068650246\n",
      "Epoch 10261/30000 Training Loss: 0.04918082058429718\n",
      "Epoch 10262/30000 Training Loss: 0.04652298614382744\n",
      "Epoch 10263/30000 Training Loss: 0.05820048972964287\n",
      "Epoch 10264/30000 Training Loss: 0.05039077252149582\n",
      "Epoch 10265/30000 Training Loss: 0.03628778085112572\n",
      "Epoch 10266/30000 Training Loss: 0.043038029223680496\n",
      "Epoch 10267/30000 Training Loss: 0.049486421048641205\n",
      "Epoch 10268/30000 Training Loss: 0.05194989591836929\n",
      "Epoch 10269/30000 Training Loss: 0.043579526245594025\n",
      "Epoch 10270/30000 Training Loss: 0.04531928151845932\n",
      "Epoch 10271/30000 Training Loss: 0.05188845470547676\n",
      "Epoch 10272/30000 Training Loss: 0.05363655090332031\n",
      "Epoch 10273/30000 Training Loss: 0.04713455215096474\n",
      "Epoch 10274/30000 Training Loss: 0.04815468564629555\n",
      "Epoch 10275/30000 Training Loss: 0.05903899669647217\n",
      "Epoch 10276/30000 Training Loss: 0.04447101801633835\n",
      "Epoch 10277/30000 Training Loss: 0.04832541570067406\n",
      "Epoch 10278/30000 Training Loss: 0.05343085527420044\n",
      "Epoch 10279/30000 Training Loss: 0.04476385563611984\n",
      "Epoch 10280/30000 Training Loss: 0.04361627995967865\n",
      "Epoch 10281/30000 Training Loss: 0.0525185689330101\n",
      "Epoch 10282/30000 Training Loss: 0.04280655086040497\n",
      "Epoch 10283/30000 Training Loss: 0.045614417642354965\n",
      "Epoch 10284/30000 Training Loss: 0.046030428260564804\n",
      "Epoch 10285/30000 Training Loss: 0.04545427858829498\n",
      "Epoch 10286/30000 Training Loss: 0.058151137083768845\n",
      "Epoch 10287/30000 Training Loss: 0.04434961453080177\n",
      "Epoch 10288/30000 Training Loss: 0.042915139347314835\n",
      "Epoch 10289/30000 Training Loss: 0.04005549103021622\n",
      "Epoch 10290/30000 Training Loss: 0.050701480358839035\n",
      "Epoch 10291/30000 Training Loss: 0.05372290685772896\n",
      "Epoch 10292/30000 Training Loss: 0.04683363810181618\n",
      "Epoch 10293/30000 Training Loss: 0.03820899501442909\n",
      "Epoch 10294/30000 Training Loss: 0.04473220556974411\n",
      "Epoch 10295/30000 Training Loss: 0.047121331095695496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10296/30000 Training Loss: 0.05362033098936081\n",
      "Epoch 10297/30000 Training Loss: 0.04806479066610336\n",
      "Epoch 10298/30000 Training Loss: 0.04332100600004196\n",
      "Epoch 10299/30000 Training Loss: 0.05079278349876404\n",
      "Epoch 10300/30000 Training Loss: 0.04361656680703163\n",
      "Epoch 10300/30000 Validation Loss: 0.04045485332608223\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04045485332608223<=============\n",
      "Epoch 10301/30000 Training Loss: 0.046801041811704636\n",
      "Epoch 10302/30000 Training Loss: 0.04200158268213272\n",
      "Epoch 10303/30000 Training Loss: 0.04782796651124954\n",
      "Epoch 10304/30000 Training Loss: 0.04176631197333336\n",
      "Epoch 10305/30000 Training Loss: 0.04317081719636917\n",
      "Epoch 10306/30000 Training Loss: 0.048419512808322906\n",
      "Epoch 10307/30000 Training Loss: 0.05027932673692703\n",
      "Epoch 10308/30000 Training Loss: 0.04971129447221756\n",
      "Epoch 10309/30000 Training Loss: 0.0397203266620636\n",
      "Epoch 10310/30000 Training Loss: 0.051746916025877\n",
      "Epoch 10311/30000 Training Loss: 0.04136517271399498\n",
      "Epoch 10312/30000 Training Loss: 0.04644961282610893\n",
      "Epoch 10313/30000 Training Loss: 0.040948934853076935\n",
      "Epoch 10314/30000 Training Loss: 0.045843735337257385\n",
      "Epoch 10315/30000 Training Loss: 0.042970575392246246\n",
      "Epoch 10316/30000 Training Loss: 0.03925340995192528\n",
      "Epoch 10317/30000 Training Loss: 0.04937714338302612\n",
      "Epoch 10318/30000 Training Loss: 0.04890287667512894\n",
      "Epoch 10319/30000 Training Loss: 0.03506508469581604\n",
      "Epoch 10320/30000 Training Loss: 0.040865443646907806\n",
      "Epoch 10321/30000 Training Loss: 0.04694438725709915\n",
      "Epoch 10322/30000 Training Loss: 0.041387736797332764\n",
      "Epoch 10323/30000 Training Loss: 0.04650810360908508\n",
      "Epoch 10324/30000 Training Loss: 0.04612810164690018\n",
      "Epoch 10325/30000 Training Loss: 0.047341857105493546\n",
      "Epoch 10326/30000 Training Loss: 0.03922673687338829\n",
      "Epoch 10327/30000 Training Loss: 0.04422038048505783\n",
      "Epoch 10328/30000 Training Loss: 0.04274282604455948\n",
      "Epoch 10329/30000 Training Loss: 0.04252020642161369\n",
      "Epoch 10330/30000 Training Loss: 0.041958801448345184\n",
      "Epoch 10331/30000 Training Loss: 0.04143583029508591\n",
      "Epoch 10332/30000 Training Loss: 0.041481729596853256\n",
      "Epoch 10333/30000 Training Loss: 0.04486892744898796\n",
      "Epoch 10334/30000 Training Loss: 0.04236820340156555\n",
      "Epoch 10335/30000 Training Loss: 0.04954390600323677\n",
      "Epoch 10336/30000 Training Loss: 0.049607910215854645\n",
      "Epoch 10337/30000 Training Loss: 0.0410706102848053\n",
      "Epoch 10338/30000 Training Loss: 0.04732600599527359\n",
      "Epoch 10339/30000 Training Loss: 0.05012127012014389\n",
      "Epoch 10340/30000 Training Loss: 0.04316367581486702\n",
      "Epoch 10341/30000 Training Loss: 0.05170873552560806\n",
      "Epoch 10342/30000 Training Loss: 0.039317961782217026\n",
      "Epoch 10343/30000 Training Loss: 0.048556018620729446\n",
      "Epoch 10344/30000 Training Loss: 0.04598846659064293\n",
      "Epoch 10345/30000 Training Loss: 0.047899048775434494\n",
      "Epoch 10346/30000 Training Loss: 0.047604821622371674\n",
      "Epoch 10347/30000 Training Loss: 0.041303567588329315\n",
      "Epoch 10348/30000 Training Loss: 0.041583575308322906\n",
      "Epoch 10349/30000 Training Loss: 0.04838521033525467\n",
      "Epoch 10350/30000 Training Loss: 0.04558930546045303\n",
      "Epoch 10350/30000 Validation Loss: 0.04900578409433365\n",
      "Epoch 10351/30000 Training Loss: 0.04150709509849548\n",
      "Epoch 10352/30000 Training Loss: 0.04429136589169502\n",
      "Epoch 10353/30000 Training Loss: 0.040762513875961304\n",
      "Epoch 10354/30000 Training Loss: 0.04997880011796951\n",
      "Epoch 10355/30000 Training Loss: 0.04110950976610184\n",
      "Epoch 10356/30000 Training Loss: 0.03862377628684044\n",
      "Epoch 10357/30000 Training Loss: 0.05405467003583908\n",
      "Epoch 10358/30000 Training Loss: 0.05583927780389786\n",
      "Epoch 10359/30000 Training Loss: 0.046102531254291534\n",
      "Epoch 10360/30000 Training Loss: 0.053602904081344604\n",
      "Epoch 10361/30000 Training Loss: 0.048719845712184906\n",
      "Epoch 10362/30000 Training Loss: 0.042397476732730865\n",
      "Epoch 10363/30000 Training Loss: 0.04196014255285263\n",
      "Epoch 10364/30000 Training Loss: 0.046008072793483734\n",
      "Epoch 10365/30000 Training Loss: 0.04430587217211723\n",
      "Epoch 10366/30000 Training Loss: 0.047193221747875214\n",
      "Epoch 10367/30000 Training Loss: 0.05226706713438034\n",
      "Epoch 10368/30000 Training Loss: 0.04113633185625076\n",
      "Epoch 10369/30000 Training Loss: 0.04393259435892105\n",
      "Epoch 10370/30000 Training Loss: 0.04611014574766159\n",
      "Epoch 10371/30000 Training Loss: 0.04996825382113457\n",
      "Epoch 10372/30000 Training Loss: 0.04163529351353645\n",
      "Epoch 10373/30000 Training Loss: 0.0472571924328804\n",
      "Epoch 10374/30000 Training Loss: 0.04539114609360695\n",
      "Epoch 10375/30000 Training Loss: 0.04975877329707146\n",
      "Epoch 10376/30000 Training Loss: 0.04394339397549629\n",
      "Epoch 10377/30000 Training Loss: 0.04054015502333641\n",
      "Epoch 10378/30000 Training Loss: 0.044137828052043915\n",
      "Epoch 10379/30000 Training Loss: 0.035140011459589005\n",
      "Epoch 10380/30000 Training Loss: 0.04296709969639778\n",
      "Epoch 10381/30000 Training Loss: 0.043357256799936295\n",
      "Epoch 10382/30000 Training Loss: 0.045527685433626175\n",
      "Epoch 10383/30000 Training Loss: 0.0430731438100338\n",
      "Epoch 10384/30000 Training Loss: 0.03889194875955582\n",
      "Epoch 10385/30000 Training Loss: 0.0439600944519043\n",
      "Epoch 10386/30000 Training Loss: 0.04331495612859726\n",
      "Epoch 10387/30000 Training Loss: 0.04889383167028427\n",
      "Epoch 10388/30000 Training Loss: 0.043211281299591064\n",
      "Epoch 10389/30000 Training Loss: 0.04927588254213333\n",
      "Epoch 10390/30000 Training Loss: 0.04249811917543411\n",
      "Epoch 10391/30000 Training Loss: 0.05078940466046333\n",
      "Epoch 10392/30000 Training Loss: 0.05441049486398697\n",
      "Epoch 10393/30000 Training Loss: 0.052266549319028854\n",
      "Epoch 10394/30000 Training Loss: 0.05066351220011711\n",
      "Epoch 10395/30000 Training Loss: 0.048047445714473724\n",
      "Epoch 10396/30000 Training Loss: 0.04187386482954025\n",
      "Epoch 10397/30000 Training Loss: 0.05620413273572922\n",
      "Epoch 10398/30000 Training Loss: 0.046264927834272385\n",
      "Epoch 10399/30000 Training Loss: 0.048162367194890976\n",
      "Epoch 10400/30000 Training Loss: 0.058899957686662674\n",
      "Epoch 10400/30000 Validation Loss: 0.04604259878396988\n",
      "Epoch 10401/30000 Training Loss: 0.04521007463335991\n",
      "Epoch 10402/30000 Training Loss: 0.04978432506322861\n",
      "Epoch 10403/30000 Training Loss: 0.048812657594680786\n",
      "Epoch 10404/30000 Training Loss: 0.04802059754729271\n",
      "Epoch 10405/30000 Training Loss: 0.048678286373615265\n",
      "Epoch 10406/30000 Training Loss: 0.044668085873126984\n",
      "Epoch 10407/30000 Training Loss: 0.04037399962544441\n",
      "Epoch 10408/30000 Training Loss: 0.05140196159482002\n",
      "Epoch 10409/30000 Training Loss: 0.044766128063201904\n",
      "Epoch 10410/30000 Training Loss: 0.043702833354473114\n",
      "Epoch 10411/30000 Training Loss: 0.044945962727069855\n",
      "Epoch 10412/30000 Training Loss: 0.04744824394583702\n",
      "Epoch 10413/30000 Training Loss: 0.04853057488799095\n",
      "Epoch 10414/30000 Training Loss: 0.06047990918159485\n",
      "Epoch 10415/30000 Training Loss: 0.04557713121175766\n",
      "Epoch 10416/30000 Training Loss: 0.043453607708215714\n",
      "Epoch 10417/30000 Training Loss: 0.04398304224014282\n",
      "Epoch 10418/30000 Training Loss: 0.04181445389986038\n",
      "Epoch 10419/30000 Training Loss: 0.04398859664797783\n",
      "Epoch 10420/30000 Training Loss: 0.059241969138383865\n",
      "Epoch 10421/30000 Training Loss: 0.04433029517531395\n",
      "Epoch 10422/30000 Training Loss: 0.04863326624035835\n",
      "Epoch 10423/30000 Training Loss: 0.05119030922651291\n",
      "Epoch 10424/30000 Training Loss: 0.042711950838565826\n",
      "Epoch 10425/30000 Training Loss: 0.04328285902738571\n",
      "Epoch 10426/30000 Training Loss: 0.04419928044080734\n",
      "Epoch 10427/30000 Training Loss: 0.03714890405535698\n",
      "Epoch 10428/30000 Training Loss: 0.041287776082754135\n",
      "Epoch 10429/30000 Training Loss: 0.04207653924822807\n",
      "Epoch 10430/30000 Training Loss: 0.04439331963658333\n",
      "Epoch 10431/30000 Training Loss: 0.04429078847169876\n",
      "Epoch 10432/30000 Training Loss: 0.047093577682971954\n",
      "Epoch 10433/30000 Training Loss: 0.04891709238290787\n",
      "Epoch 10434/30000 Training Loss: 0.052654169499874115\n",
      "Epoch 10435/30000 Training Loss: 0.05114997178316116\n",
      "Epoch 10436/30000 Training Loss: 0.04234396293759346\n",
      "Epoch 10437/30000 Training Loss: 0.0467422716319561\n",
      "Epoch 10438/30000 Training Loss: 0.04342648759484291\n",
      "Epoch 10439/30000 Training Loss: 0.04319123178720474\n",
      "Epoch 10440/30000 Training Loss: 0.055525828152894974\n",
      "Epoch 10441/30000 Training Loss: 0.05172157287597656\n",
      "Epoch 10442/30000 Training Loss: 0.04869961738586426\n",
      "Epoch 10443/30000 Training Loss: 0.03605356067419052\n",
      "Epoch 10444/30000 Training Loss: 0.043347183614969254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10445/30000 Training Loss: 0.045821499079465866\n",
      "Epoch 10446/30000 Training Loss: 0.05599241331219673\n",
      "Epoch 10447/30000 Training Loss: 0.0446016788482666\n",
      "Epoch 10448/30000 Training Loss: 0.050514884293079376\n",
      "Epoch 10449/30000 Training Loss: 0.053648848086595535\n",
      "Epoch 10450/30000 Training Loss: 0.05252302438020706\n",
      "Epoch 10450/30000 Validation Loss: 0.048567455261945724\n",
      "Epoch 10451/30000 Training Loss: 0.039161477237939835\n",
      "Epoch 10452/30000 Training Loss: 0.04125125706195831\n",
      "Epoch 10453/30000 Training Loss: 0.04566338658332825\n",
      "Epoch 10454/30000 Training Loss: 0.04375334829092026\n",
      "Epoch 10455/30000 Training Loss: 0.04249858856201172\n",
      "Epoch 10456/30000 Training Loss: 0.046754561364650726\n",
      "Epoch 10457/30000 Training Loss: 0.04645576328039169\n",
      "Epoch 10458/30000 Training Loss: 0.041908148676157\n",
      "Epoch 10459/30000 Training Loss: 0.049567751586437225\n",
      "Epoch 10460/30000 Training Loss: 0.04679528623819351\n",
      "Epoch 10461/30000 Training Loss: 0.04112808406352997\n",
      "Epoch 10462/30000 Training Loss: 0.045848548412323\n",
      "Epoch 10463/30000 Training Loss: 0.044866982847452164\n",
      "Epoch 10464/30000 Training Loss: 0.04167795181274414\n",
      "Epoch 10465/30000 Training Loss: 0.04584730789065361\n",
      "Epoch 10466/30000 Training Loss: 0.04108978062868118\n",
      "Epoch 10467/30000 Training Loss: 0.0442979671061039\n",
      "Epoch 10468/30000 Training Loss: 0.04584814980626106\n",
      "Epoch 10469/30000 Training Loss: 0.04898917302489281\n",
      "Epoch 10470/30000 Training Loss: 0.045969076454639435\n",
      "Epoch 10471/30000 Training Loss: 0.04758834466338158\n",
      "Epoch 10472/30000 Training Loss: 0.039490800350904465\n",
      "Epoch 10473/30000 Training Loss: 0.041764311492443085\n",
      "Epoch 10474/30000 Training Loss: 0.05206453800201416\n",
      "Epoch 10475/30000 Training Loss: 0.054319124668836594\n",
      "Epoch 10476/30000 Training Loss: 0.0459267720580101\n",
      "Epoch 10477/30000 Training Loss: 0.039712220430374146\n",
      "Epoch 10478/30000 Training Loss: 0.046547241508960724\n",
      "Epoch 10479/30000 Training Loss: 0.0421140156686306\n",
      "Epoch 10480/30000 Training Loss: 0.05629413202404976\n",
      "Epoch 10481/30000 Training Loss: 0.048466674983501434\n",
      "Epoch 10482/30000 Training Loss: 0.049639515578746796\n",
      "Epoch 10483/30000 Training Loss: 0.052133023738861084\n",
      "Epoch 10484/30000 Training Loss: 0.04745914787054062\n",
      "Epoch 10485/30000 Training Loss: 0.0458870530128479\n",
      "Epoch 10486/30000 Training Loss: 0.04860079288482666\n",
      "Epoch 10487/30000 Training Loss: 0.03453397378325462\n",
      "Epoch 10488/30000 Training Loss: 0.04388909414410591\n",
      "Epoch 10489/30000 Training Loss: 0.051743119955062866\n",
      "Epoch 10490/30000 Training Loss: 0.04525575041770935\n",
      "Epoch 10491/30000 Training Loss: 0.04039812833070755\n",
      "Epoch 10492/30000 Training Loss: 0.048194337636232376\n",
      "Epoch 10493/30000 Training Loss: 0.04317846521735191\n",
      "Epoch 10494/30000 Training Loss: 0.04909981042146683\n",
      "Epoch 10495/30000 Training Loss: 0.04530256241559982\n",
      "Epoch 10496/30000 Training Loss: 0.05293872952461243\n",
      "Epoch 10497/30000 Training Loss: 0.04360401630401611\n",
      "Epoch 10498/30000 Training Loss: 0.03973902016878128\n",
      "Epoch 10499/30000 Training Loss: 0.04686573147773743\n",
      "Epoch 10500/30000 Training Loss: 0.05250438302755356\n",
      "Epoch 10500/30000 Validation Loss: 0.06011704355478287\n",
      "Epoch 10501/30000 Training Loss: 0.04051407426595688\n",
      "Epoch 10502/30000 Training Loss: 0.04920436441898346\n",
      "Epoch 10503/30000 Training Loss: 0.04257791489362717\n",
      "Epoch 10504/30000 Training Loss: 0.04151031747460365\n",
      "Epoch 10505/30000 Training Loss: 0.04974927753210068\n",
      "Epoch 10506/30000 Training Loss: 0.055760640650987625\n",
      "Epoch 10507/30000 Training Loss: 0.04547940194606781\n",
      "Epoch 10508/30000 Training Loss: 0.052606236189603806\n",
      "Epoch 10509/30000 Training Loss: 0.0567234642803669\n",
      "Epoch 10510/30000 Training Loss: 0.05142483860254288\n",
      "Epoch 10511/30000 Training Loss: 0.052100539207458496\n",
      "Epoch 10512/30000 Training Loss: 0.04808143526315689\n",
      "Epoch 10513/30000 Training Loss: 0.05467261001467705\n",
      "Epoch 10514/30000 Training Loss: 0.049843475222587585\n",
      "Epoch 10515/30000 Training Loss: 0.04625216871500015\n",
      "Epoch 10516/30000 Training Loss: 0.04753648489713669\n",
      "Epoch 10517/30000 Training Loss: 0.04725702479481697\n",
      "Epoch 10518/30000 Training Loss: 0.044467099010944366\n",
      "Epoch 10519/30000 Training Loss: 0.04872214049100876\n",
      "Epoch 10520/30000 Training Loss: 0.04174751043319702\n",
      "Epoch 10521/30000 Training Loss: 0.04768213629722595\n",
      "Epoch 10522/30000 Training Loss: 0.04569585993885994\n",
      "Epoch 10523/30000 Training Loss: 0.05010535195469856\n",
      "Epoch 10524/30000 Training Loss: 0.05413515120744705\n",
      "Epoch 10525/30000 Training Loss: 0.056165046989917755\n",
      "Epoch 10526/30000 Training Loss: 0.05308210849761963\n",
      "Epoch 10527/30000 Training Loss: 0.041251830756664276\n",
      "Epoch 10528/30000 Training Loss: 0.046568941324949265\n",
      "Epoch 10529/30000 Training Loss: 0.0512351393699646\n",
      "Epoch 10530/30000 Training Loss: 0.044472403824329376\n",
      "Epoch 10531/30000 Training Loss: 0.04608199745416641\n",
      "Epoch 10532/30000 Training Loss: 0.03965440392494202\n",
      "Epoch 10533/30000 Training Loss: 0.043941617012023926\n",
      "Epoch 10534/30000 Training Loss: 0.049408916383981705\n",
      "Epoch 10535/30000 Training Loss: 0.043924085795879364\n",
      "Epoch 10536/30000 Training Loss: 0.05015971139073372\n",
      "Epoch 10537/30000 Training Loss: 0.05368373915553093\n",
      "Epoch 10538/30000 Training Loss: 0.04885023087263107\n",
      "Epoch 10539/30000 Training Loss: 0.04524637386202812\n",
      "Epoch 10540/30000 Training Loss: 0.049970485270023346\n",
      "Epoch 10541/30000 Training Loss: 0.04898639768362045\n",
      "Epoch 10542/30000 Training Loss: 0.04245861992239952\n",
      "Epoch 10543/30000 Training Loss: 0.04325415939092636\n",
      "Epoch 10544/30000 Training Loss: 0.046603426337242126\n",
      "Epoch 10545/30000 Training Loss: 0.04528558999300003\n",
      "Epoch 10546/30000 Training Loss: 0.048838552087545395\n",
      "Epoch 10547/30000 Training Loss: 0.04445094242691994\n",
      "Epoch 10548/30000 Training Loss: 0.046466559171676636\n",
      "Epoch 10549/30000 Training Loss: 0.05375244468450546\n",
      "Epoch 10550/30000 Training Loss: 0.04039451479911804\n",
      "Epoch 10550/30000 Validation Loss: 0.03639005497097969\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03639005497097969<=============\n",
      "Epoch 10551/30000 Training Loss: 0.03460158780217171\n",
      "Epoch 10552/30000 Training Loss: 0.04762670025229454\n",
      "Epoch 10553/30000 Training Loss: 0.04834352061152458\n",
      "Epoch 10554/30000 Training Loss: 0.037349291145801544\n",
      "Epoch 10555/30000 Training Loss: 0.0405060350894928\n",
      "Epoch 10556/30000 Training Loss: 0.046829286962747574\n",
      "Epoch 10557/30000 Training Loss: 0.047680359333753586\n",
      "Epoch 10558/30000 Training Loss: 0.042645491659641266\n",
      "Epoch 10559/30000 Training Loss: 0.046357955783605576\n",
      "Epoch 10560/30000 Training Loss: 0.05453585833311081\n",
      "Epoch 10561/30000 Training Loss: 0.05127991363406181\n",
      "Epoch 10562/30000 Training Loss: 0.039075516164302826\n",
      "Epoch 10563/30000 Training Loss: 0.05072219297289848\n",
      "Epoch 10564/30000 Training Loss: 0.05071554705500603\n",
      "Epoch 10565/30000 Training Loss: 0.03982780873775482\n",
      "Epoch 10566/30000 Training Loss: 0.038727372884750366\n",
      "Epoch 10567/30000 Training Loss: 0.037563808262348175\n",
      "Epoch 10568/30000 Training Loss: 0.05494103580713272\n",
      "Epoch 10569/30000 Training Loss: 0.04044196754693985\n",
      "Epoch 10570/30000 Training Loss: 0.04959138482809067\n",
      "Epoch 10571/30000 Training Loss: 0.03824320435523987\n",
      "Epoch 10572/30000 Training Loss: 0.047008808702230453\n",
      "Epoch 10573/30000 Training Loss: 0.04457874223589897\n",
      "Epoch 10574/30000 Training Loss: 0.050938766449689865\n",
      "Epoch 10575/30000 Training Loss: 0.04522222280502319\n",
      "Epoch 10576/30000 Training Loss: 0.05195143073797226\n",
      "Epoch 10577/30000 Training Loss: 0.04615682363510132\n",
      "Epoch 10578/30000 Training Loss: 0.04488196596503258\n",
      "Epoch 10579/30000 Training Loss: 0.04630143195390701\n",
      "Epoch 10580/30000 Training Loss: 0.043562620878219604\n",
      "Epoch 10581/30000 Training Loss: 0.04553573951125145\n",
      "Epoch 10582/30000 Training Loss: 0.03948808088898659\n",
      "Epoch 10583/30000 Training Loss: 0.053669679909944534\n",
      "Epoch 10584/30000 Training Loss: 0.04178793355822563\n",
      "Epoch 10585/30000 Training Loss: 0.03718394786119461\n",
      "Epoch 10586/30000 Training Loss: 0.044357337057590485\n",
      "Epoch 10587/30000 Training Loss: 0.05045595020055771\n",
      "Epoch 10588/30000 Training Loss: 0.03901199251413345\n",
      "Epoch 10589/30000 Training Loss: 0.046590737998485565\n",
      "Epoch 10590/30000 Training Loss: 0.04347804933786392\n",
      "Epoch 10591/30000 Training Loss: 0.042851805686950684\n",
      "Epoch 10592/30000 Training Loss: 0.038700640201568604\n",
      "Epoch 10593/30000 Training Loss: 0.044949110597372055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10594/30000 Training Loss: 0.04943468049168587\n",
      "Epoch 10595/30000 Training Loss: 0.058023374527692795\n",
      "Epoch 10596/30000 Training Loss: 0.04400472342967987\n",
      "Epoch 10597/30000 Training Loss: 0.04586941376328468\n",
      "Epoch 10598/30000 Training Loss: 0.04696855694055557\n",
      "Epoch 10599/30000 Training Loss: 0.047457654029130936\n",
      "Epoch 10600/30000 Training Loss: 0.045373447239398956\n",
      "Epoch 10600/30000 Validation Loss: 0.046344880014657974\n",
      "Epoch 10601/30000 Training Loss: 0.04030358046293259\n",
      "Epoch 10602/30000 Training Loss: 0.04814300686120987\n",
      "Epoch 10603/30000 Training Loss: 0.04464748874306679\n",
      "Epoch 10604/30000 Training Loss: 0.04277194291353226\n",
      "Epoch 10605/30000 Training Loss: 0.04938412085175514\n",
      "Epoch 10606/30000 Training Loss: 0.039083465933799744\n",
      "Epoch 10607/30000 Training Loss: 0.04602847993373871\n",
      "Epoch 10608/30000 Training Loss: 0.04043305665254593\n",
      "Epoch 10609/30000 Training Loss: 0.05190882086753845\n",
      "Epoch 10610/30000 Training Loss: 0.04713964834809303\n",
      "Epoch 10611/30000 Training Loss: 0.04621576517820358\n",
      "Epoch 10612/30000 Training Loss: 0.05020918697118759\n",
      "Epoch 10613/30000 Training Loss: 0.042810216546058655\n",
      "Epoch 10614/30000 Training Loss: 0.04453958198428154\n",
      "Epoch 10615/30000 Training Loss: 0.046141527593135834\n",
      "Epoch 10616/30000 Training Loss: 0.04507318511605263\n",
      "Epoch 10617/30000 Training Loss: 0.04834267497062683\n",
      "Epoch 10618/30000 Training Loss: 0.043985795229673386\n",
      "Epoch 10619/30000 Training Loss: 0.058675091713666916\n",
      "Epoch 10620/30000 Training Loss: 0.0446041114628315\n",
      "Epoch 10621/30000 Training Loss: 0.04382353648543358\n",
      "Epoch 10622/30000 Training Loss: 0.04245223477482796\n",
      "Epoch 10623/30000 Training Loss: 0.04710189998149872\n",
      "Epoch 10624/30000 Training Loss: 0.052010394632816315\n",
      "Epoch 10625/30000 Training Loss: 0.05027785152196884\n",
      "Epoch 10626/30000 Training Loss: 0.046242035925388336\n",
      "Epoch 10627/30000 Training Loss: 0.04830732196569443\n",
      "Epoch 10628/30000 Training Loss: 0.04780394583940506\n",
      "Epoch 10629/30000 Training Loss: 0.05041838809847832\n",
      "Epoch 10630/30000 Training Loss: 0.0548623688519001\n",
      "Epoch 10631/30000 Training Loss: 0.0492066815495491\n",
      "Epoch 10632/30000 Training Loss: 0.048630207777023315\n",
      "Epoch 10633/30000 Training Loss: 0.042408622801303864\n",
      "Epoch 10634/30000 Training Loss: 0.0458710715174675\n",
      "Epoch 10635/30000 Training Loss: 0.04800789803266525\n",
      "Epoch 10636/30000 Training Loss: 0.05483890324831009\n",
      "Epoch 10637/30000 Training Loss: 0.04753521829843521\n",
      "Epoch 10638/30000 Training Loss: 0.05039525032043457\n",
      "Epoch 10639/30000 Training Loss: 0.0456070676445961\n",
      "Epoch 10640/30000 Training Loss: 0.04558078199625015\n",
      "Epoch 10641/30000 Training Loss: 0.043357666581869125\n",
      "Epoch 10642/30000 Training Loss: 0.04107724875211716\n",
      "Epoch 10643/30000 Training Loss: 0.04114959388971329\n",
      "Epoch 10644/30000 Training Loss: 0.052318572998046875\n",
      "Epoch 10645/30000 Training Loss: 0.05136963725090027\n",
      "Epoch 10646/30000 Training Loss: 0.0493595227599144\n",
      "Epoch 10647/30000 Training Loss: 0.0449834018945694\n",
      "Epoch 10648/30000 Training Loss: 0.05058573558926582\n",
      "Epoch 10649/30000 Training Loss: 0.048462994396686554\n",
      "Epoch 10650/30000 Training Loss: 0.048380009829998016\n",
      "Epoch 10650/30000 Validation Loss: 0.04038941115140915\n",
      "Epoch 10651/30000 Training Loss: 0.04581615328788757\n",
      "Epoch 10652/30000 Training Loss: 0.047830305993556976\n",
      "Epoch 10653/30000 Training Loss: 0.0443975105881691\n",
      "Epoch 10654/30000 Training Loss: 0.04294783994555473\n",
      "Epoch 10655/30000 Training Loss: 0.04807636886835098\n",
      "Epoch 10656/30000 Training Loss: 0.04364493489265442\n",
      "Epoch 10657/30000 Training Loss: 0.049691565334796906\n",
      "Epoch 10658/30000 Training Loss: 0.05251021310687065\n",
      "Epoch 10659/30000 Training Loss: 0.05252499505877495\n",
      "Epoch 10660/30000 Training Loss: 0.043370671570301056\n",
      "Epoch 10661/30000 Training Loss: 0.04315907135605812\n",
      "Epoch 10662/30000 Training Loss: 0.052546024322509766\n",
      "Epoch 10663/30000 Training Loss: 0.047380510717630386\n",
      "Epoch 10664/30000 Training Loss: 0.049366436898708344\n",
      "Epoch 10665/30000 Training Loss: 0.046446543186903\n",
      "Epoch 10666/30000 Training Loss: 0.03931460902094841\n",
      "Epoch 10667/30000 Training Loss: 0.04584542661905289\n",
      "Epoch 10668/30000 Training Loss: 0.06030040234327316\n",
      "Epoch 10669/30000 Training Loss: 0.043908748775720596\n",
      "Epoch 10670/30000 Training Loss: 0.04287656024098396\n",
      "Epoch 10671/30000 Training Loss: 0.040223948657512665\n",
      "Epoch 10672/30000 Training Loss: 0.04380286857485771\n",
      "Epoch 10673/30000 Training Loss: 0.04662983492016792\n",
      "Epoch 10674/30000 Training Loss: 0.059299975633621216\n",
      "Epoch 10675/30000 Training Loss: 0.04067685455083847\n",
      "Epoch 10676/30000 Training Loss: 0.041955213993787766\n",
      "Epoch 10677/30000 Training Loss: 0.04669102281332016\n",
      "Epoch 10678/30000 Training Loss: 0.04795382544398308\n",
      "Epoch 10679/30000 Training Loss: 0.0485752709209919\n",
      "Epoch 10680/30000 Training Loss: 0.04658258706331253\n",
      "Epoch 10681/30000 Training Loss: 0.04853123798966408\n",
      "Epoch 10682/30000 Training Loss: 0.04907769709825516\n",
      "Epoch 10683/30000 Training Loss: 0.04810940474271774\n",
      "Epoch 10684/30000 Training Loss: 0.043049272149801254\n",
      "Epoch 10685/30000 Training Loss: 0.04740298166871071\n",
      "Epoch 10686/30000 Training Loss: 0.044672973453998566\n",
      "Epoch 10687/30000 Training Loss: 0.040574170649051666\n",
      "Epoch 10688/30000 Training Loss: 0.04773782566189766\n",
      "Epoch 10689/30000 Training Loss: 0.0428936742246151\n",
      "Epoch 10690/30000 Training Loss: 0.05527409166097641\n",
      "Epoch 10691/30000 Training Loss: 0.046630773693323135\n",
      "Epoch 10692/30000 Training Loss: 0.0375589057803154\n",
      "Epoch 10693/30000 Training Loss: 0.03932928293943405\n",
      "Epoch 10694/30000 Training Loss: 0.040310800075531006\n",
      "Epoch 10695/30000 Training Loss: 0.0485319085419178\n",
      "Epoch 10696/30000 Training Loss: 0.03825240209698677\n",
      "Epoch 10697/30000 Training Loss: 0.050233613699674606\n",
      "Epoch 10698/30000 Training Loss: 0.04293803125619888\n",
      "Epoch 10699/30000 Training Loss: 0.048017121851444244\n",
      "Epoch 10700/30000 Training Loss: 0.04959549009799957\n",
      "Epoch 10700/30000 Validation Loss: 0.03997517377138138\n",
      "Epoch 10701/30000 Training Loss: 0.05109526962041855\n",
      "Epoch 10702/30000 Training Loss: 0.0382382832467556\n",
      "Epoch 10703/30000 Training Loss: 0.044049374759197235\n",
      "Epoch 10704/30000 Training Loss: 0.047189339995384216\n",
      "Epoch 10705/30000 Training Loss: 0.048499636352062225\n",
      "Epoch 10706/30000 Training Loss: 0.041466303169727325\n",
      "Epoch 10707/30000 Training Loss: 0.03975491225719452\n",
      "Epoch 10708/30000 Training Loss: 0.04316709190607071\n",
      "Epoch 10709/30000 Training Loss: 0.04010966047644615\n",
      "Epoch 10710/30000 Training Loss: 0.04343048483133316\n",
      "Epoch 10711/30000 Training Loss: 0.053132761269807816\n",
      "Epoch 10712/30000 Training Loss: 0.04717852175235748\n",
      "Epoch 10713/30000 Training Loss: 0.042062483727931976\n",
      "Epoch 10714/30000 Training Loss: 0.04567497968673706\n",
      "Epoch 10715/30000 Training Loss: 0.053176045417785645\n",
      "Epoch 10716/30000 Training Loss: 0.041252948343753815\n",
      "Epoch 10717/30000 Training Loss: 0.04194950684905052\n",
      "Epoch 10718/30000 Training Loss: 0.05222507566213608\n",
      "Epoch 10719/30000 Training Loss: 0.04207714647054672\n",
      "Epoch 10720/30000 Training Loss: 0.048295971006155014\n",
      "Epoch 10721/30000 Training Loss: 0.051374148577451706\n",
      "Epoch 10722/30000 Training Loss: 0.05308730527758598\n",
      "Epoch 10723/30000 Training Loss: 0.04750894382596016\n",
      "Epoch 10724/30000 Training Loss: 0.036361973732709885\n",
      "Epoch 10725/30000 Training Loss: 0.041646651923656464\n",
      "Epoch 10726/30000 Training Loss: 0.0383123978972435\n",
      "Epoch 10727/30000 Training Loss: 0.05372120812535286\n",
      "Epoch 10728/30000 Training Loss: 0.03748778626322746\n",
      "Epoch 10729/30000 Training Loss: 0.04167032986879349\n",
      "Epoch 10730/30000 Training Loss: 0.04145979508757591\n",
      "Epoch 10731/30000 Training Loss: 0.04844203591346741\n",
      "Epoch 10732/30000 Training Loss: 0.038576625287532806\n",
      "Epoch 10733/30000 Training Loss: 0.04881764203310013\n",
      "Epoch 10734/30000 Training Loss: 0.039133138954639435\n",
      "Epoch 10735/30000 Training Loss: 0.040477704256772995\n",
      "Epoch 10736/30000 Training Loss: 0.039604831486940384\n",
      "Epoch 10737/30000 Training Loss: 0.0377759113907814\n",
      "Epoch 10738/30000 Training Loss: 0.05608034133911133\n",
      "Epoch 10739/30000 Training Loss: 0.049583591520786285\n",
      "Epoch 10740/30000 Training Loss: 0.05400937795639038\n",
      "Epoch 10741/30000 Training Loss: 0.04425919055938721\n",
      "Epoch 10742/30000 Training Loss: 0.04612334817647934\n",
      "Epoch 10743/30000 Training Loss: 0.04862060397863388\n",
      "Epoch 10744/30000 Training Loss: 0.050222862511873245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10745/30000 Training Loss: 0.04164653271436691\n",
      "Epoch 10746/30000 Training Loss: 0.04223855957388878\n",
      "Epoch 10747/30000 Training Loss: 0.04917995259165764\n",
      "Epoch 10748/30000 Training Loss: 0.0415680930018425\n",
      "Epoch 10749/30000 Training Loss: 0.048908017575740814\n",
      "Epoch 10750/30000 Training Loss: 0.03995507210493088\n",
      "Epoch 10750/30000 Validation Loss: 0.04163484275341034\n",
      "Epoch 10751/30000 Training Loss: 0.041833531111478806\n",
      "Epoch 10752/30000 Training Loss: 0.043093882501125336\n",
      "Epoch 10753/30000 Training Loss: 0.046318888664245605\n",
      "Epoch 10754/30000 Training Loss: 0.05911486595869064\n",
      "Epoch 10755/30000 Training Loss: 0.04495269060134888\n",
      "Epoch 10756/30000 Training Loss: 0.04391021281480789\n",
      "Epoch 10757/30000 Training Loss: 0.03971124440431595\n",
      "Epoch 10758/30000 Training Loss: 0.05133535712957382\n",
      "Epoch 10759/30000 Training Loss: 0.04838920757174492\n",
      "Epoch 10760/30000 Training Loss: 0.040732625871896744\n",
      "Epoch 10761/30000 Training Loss: 0.05110897868871689\n",
      "Epoch 10762/30000 Training Loss: 0.03826593980193138\n",
      "Epoch 10763/30000 Training Loss: 0.04648157209157944\n",
      "Epoch 10764/30000 Training Loss: 0.045924533158540726\n",
      "Epoch 10765/30000 Training Loss: 0.04781079664826393\n",
      "Epoch 10766/30000 Training Loss: 0.041659027338027954\n",
      "Epoch 10767/30000 Training Loss: 0.039835650473833084\n",
      "Epoch 10768/30000 Training Loss: 0.042979005724191666\n",
      "Epoch 10769/30000 Training Loss: 0.043925873935222626\n",
      "Epoch 10770/30000 Training Loss: 0.03770505636930466\n",
      "Epoch 10771/30000 Training Loss: 0.04316902905702591\n",
      "Epoch 10772/30000 Training Loss: 0.04042162746191025\n",
      "Epoch 10773/30000 Training Loss: 0.055989086627960205\n",
      "Epoch 10774/30000 Training Loss: 0.05469025298953056\n",
      "Epoch 10775/30000 Training Loss: 0.04788816347718239\n",
      "Epoch 10776/30000 Training Loss: 0.04352336376905441\n",
      "Epoch 10777/30000 Training Loss: 0.03739762306213379\n",
      "Epoch 10778/30000 Training Loss: 0.04835040122270584\n",
      "Epoch 10779/30000 Training Loss: 0.04292769357562065\n",
      "Epoch 10780/30000 Training Loss: 0.042145512998104095\n",
      "Epoch 10781/30000 Training Loss: 0.0433775819838047\n",
      "Epoch 10782/30000 Training Loss: 0.04485069960355759\n",
      "Epoch 10783/30000 Training Loss: 0.04379652813076973\n",
      "Epoch 10784/30000 Training Loss: 0.04218491539359093\n",
      "Epoch 10785/30000 Training Loss: 0.042621247470378876\n",
      "Epoch 10786/30000 Training Loss: 0.044043075293302536\n",
      "Epoch 10787/30000 Training Loss: 0.04464568570256233\n",
      "Epoch 10788/30000 Training Loss: 0.05007190629839897\n",
      "Epoch 10789/30000 Training Loss: 0.04811513423919678\n",
      "Epoch 10790/30000 Training Loss: 0.04203065112233162\n",
      "Epoch 10791/30000 Training Loss: 0.04434984177350998\n",
      "Epoch 10792/30000 Training Loss: 0.04698438197374344\n",
      "Epoch 10793/30000 Training Loss: 0.04536702483892441\n",
      "Epoch 10794/30000 Training Loss: 0.04534037038683891\n",
      "Epoch 10795/30000 Training Loss: 0.04281935840845108\n",
      "Epoch 10796/30000 Training Loss: 0.04363418370485306\n",
      "Epoch 10797/30000 Training Loss: 0.047698963433504105\n",
      "Epoch 10798/30000 Training Loss: 0.048404961824417114\n",
      "Epoch 10799/30000 Training Loss: 0.04885706305503845\n",
      "Epoch 10800/30000 Training Loss: 0.05556827783584595\n",
      "Epoch 10800/30000 Validation Loss: 0.04488959163427353\n",
      "Epoch 10801/30000 Training Loss: 0.039796821773052216\n",
      "Epoch 10802/30000 Training Loss: 0.038447797298431396\n",
      "Epoch 10803/30000 Training Loss: 0.04502671957015991\n",
      "Epoch 10804/30000 Training Loss: 0.036629319190979004\n",
      "Epoch 10805/30000 Training Loss: 0.042524538934230804\n",
      "Epoch 10806/30000 Training Loss: 0.04881330579519272\n",
      "Epoch 10807/30000 Training Loss: 0.03835516422986984\n",
      "Epoch 10808/30000 Training Loss: 0.04652128368616104\n",
      "Epoch 10809/30000 Training Loss: 0.04549681022763252\n",
      "Epoch 10810/30000 Training Loss: 0.03819858282804489\n",
      "Epoch 10811/30000 Training Loss: 0.050712354481220245\n",
      "Epoch 10812/30000 Training Loss: 0.0419025681912899\n",
      "Epoch 10813/30000 Training Loss: 0.04356517642736435\n",
      "Epoch 10814/30000 Training Loss: 0.05000411719083786\n",
      "Epoch 10815/30000 Training Loss: 0.04841012880206108\n",
      "Epoch 10816/30000 Training Loss: 0.05049087852239609\n",
      "Epoch 10817/30000 Training Loss: 0.04783870652318001\n",
      "Epoch 10818/30000 Training Loss: 0.04862189292907715\n",
      "Epoch 10819/30000 Training Loss: 0.046965889632701874\n",
      "Epoch 10820/30000 Training Loss: 0.046652667224407196\n",
      "Epoch 10821/30000 Training Loss: 0.04456184059381485\n",
      "Epoch 10822/30000 Training Loss: 0.047259170562028885\n",
      "Epoch 10823/30000 Training Loss: 0.043795377016067505\n",
      "Epoch 10824/30000 Training Loss: 0.04984675347805023\n",
      "Epoch 10825/30000 Training Loss: 0.053756147623062134\n",
      "Epoch 10826/30000 Training Loss: 0.04627944156527519\n",
      "Epoch 10827/30000 Training Loss: 0.03424695506691933\n",
      "Epoch 10828/30000 Training Loss: 0.04828449338674545\n",
      "Epoch 10829/30000 Training Loss: 0.05653427913784981\n",
      "Epoch 10830/30000 Training Loss: 0.05309348553419113\n",
      "Epoch 10831/30000 Training Loss: 0.04550239071249962\n",
      "Epoch 10832/30000 Training Loss: 0.036168359220027924\n",
      "Epoch 10833/30000 Training Loss: 0.042727742344141006\n",
      "Epoch 10834/30000 Training Loss: 0.046375710517168045\n",
      "Epoch 10835/30000 Training Loss: 0.04945441707968712\n",
      "Epoch 10836/30000 Training Loss: 0.04274729639291763\n",
      "Epoch 10837/30000 Training Loss: 0.040710583329200745\n",
      "Epoch 10838/30000 Training Loss: 0.048275791108608246\n",
      "Epoch 10839/30000 Training Loss: 0.051340341567993164\n",
      "Epoch 10840/30000 Training Loss: 0.0473828986287117\n",
      "Epoch 10841/30000 Training Loss: 0.04800739884376526\n",
      "Epoch 10842/30000 Training Loss: 0.049597837030887604\n",
      "Epoch 10843/30000 Training Loss: 0.04700909182429314\n",
      "Epoch 10844/30000 Training Loss: 0.04232970252633095\n",
      "Epoch 10845/30000 Training Loss: 0.04665588587522507\n",
      "Epoch 10846/30000 Training Loss: 0.04977072775363922\n",
      "Epoch 10847/30000 Training Loss: 0.05686004087328911\n",
      "Epoch 10848/30000 Training Loss: 0.04911229759454727\n",
      "Epoch 10849/30000 Training Loss: 0.04111386463046074\n",
      "Epoch 10850/30000 Training Loss: 0.0406610369682312\n",
      "Epoch 10850/30000 Validation Loss: 0.045007217675447464\n",
      "Epoch 10851/30000 Training Loss: 0.041790563613176346\n",
      "Epoch 10852/30000 Training Loss: 0.05069645494222641\n",
      "Epoch 10853/30000 Training Loss: 0.05165231227874756\n",
      "Epoch 10854/30000 Training Loss: 0.04172218590974808\n",
      "Epoch 10855/30000 Training Loss: 0.04940905421972275\n",
      "Epoch 10856/30000 Training Loss: 0.043208442628383636\n",
      "Epoch 10857/30000 Training Loss: 0.045168038457632065\n",
      "Epoch 10858/30000 Training Loss: 0.041516322642564774\n",
      "Epoch 10859/30000 Training Loss: 0.05190693587064743\n",
      "Epoch 10860/30000 Training Loss: 0.044739533215761185\n",
      "Epoch 10861/30000 Training Loss: 0.04106011241674423\n",
      "Epoch 10862/30000 Training Loss: 0.05166397616267204\n",
      "Epoch 10863/30000 Training Loss: 0.04680261388421059\n",
      "Epoch 10864/30000 Training Loss: 0.047726113349199295\n",
      "Epoch 10865/30000 Training Loss: 0.051406096667051315\n",
      "Epoch 10866/30000 Training Loss: 0.03715137764811516\n",
      "Epoch 10867/30000 Training Loss: 0.048241518437862396\n",
      "Epoch 10868/30000 Training Loss: 0.04285367578268051\n",
      "Epoch 10869/30000 Training Loss: 0.045072294771671295\n",
      "Epoch 10870/30000 Training Loss: 0.04082223027944565\n",
      "Epoch 10871/30000 Training Loss: 0.04275944456458092\n",
      "Epoch 10872/30000 Training Loss: 0.040763821452856064\n",
      "Epoch 10873/30000 Training Loss: 0.04607279971241951\n",
      "Epoch 10874/30000 Training Loss: 0.04189231991767883\n",
      "Epoch 10875/30000 Training Loss: 0.04859190806746483\n",
      "Epoch 10876/30000 Training Loss: 0.058889102190732956\n",
      "Epoch 10877/30000 Training Loss: 0.03638841584324837\n",
      "Epoch 10878/30000 Training Loss: 0.04327366128563881\n",
      "Epoch 10879/30000 Training Loss: 0.044014181941747665\n",
      "Epoch 10880/30000 Training Loss: 0.036845896393060684\n",
      "Epoch 10881/30000 Training Loss: 0.038682665675878525\n",
      "Epoch 10882/30000 Training Loss: 0.05887501314282417\n",
      "Epoch 10883/30000 Training Loss: 0.046615175902843475\n",
      "Epoch 10884/30000 Training Loss: 0.045512180775403976\n",
      "Epoch 10885/30000 Training Loss: 0.04010012745857239\n",
      "Epoch 10886/30000 Training Loss: 0.03895275667309761\n",
      "Epoch 10887/30000 Training Loss: 0.045522164553403854\n",
      "Epoch 10888/30000 Training Loss: 0.052311234176158905\n",
      "Epoch 10889/30000 Training Loss: 0.04186965152621269\n",
      "Epoch 10890/30000 Training Loss: 0.04409605264663696\n",
      "Epoch 10891/30000 Training Loss: 0.05161147192120552\n",
      "Epoch 10892/30000 Training Loss: 0.039850786328315735\n",
      "Epoch 10893/30000 Training Loss: 0.04076613485813141\n",
      "Epoch 10894/30000 Training Loss: 0.04110001400113106\n",
      "Epoch 10895/30000 Training Loss: 0.04770410433411598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10896/30000 Training Loss: 0.04883063957095146\n",
      "Epoch 10897/30000 Training Loss: 0.046000607311725616\n",
      "Epoch 10898/30000 Training Loss: 0.051952313631772995\n",
      "Epoch 10899/30000 Training Loss: 0.04799335449934006\n",
      "Epoch 10900/30000 Training Loss: 0.04817386716604233\n",
      "Epoch 10900/30000 Validation Loss: 0.04584183543920517\n",
      "Epoch 10901/30000 Training Loss: 0.049583904445171356\n",
      "Epoch 10902/30000 Training Loss: 0.045355286449193954\n",
      "Epoch 10903/30000 Training Loss: 0.0468522384762764\n",
      "Epoch 10904/30000 Training Loss: 0.05368301272392273\n",
      "Epoch 10905/30000 Training Loss: 0.04552387073636055\n",
      "Epoch 10906/30000 Training Loss: 0.052393339574337006\n",
      "Epoch 10907/30000 Training Loss: 0.052584610879421234\n",
      "Epoch 10908/30000 Training Loss: 0.057379089295864105\n",
      "Epoch 10909/30000 Training Loss: 0.05140448361635208\n",
      "Epoch 10910/30000 Training Loss: 0.04515844210982323\n",
      "Epoch 10911/30000 Training Loss: 0.05286111310124397\n",
      "Epoch 10912/30000 Training Loss: 0.05713843181729317\n",
      "Epoch 10913/30000 Training Loss: 0.04616275429725647\n",
      "Epoch 10914/30000 Training Loss: 0.036653872579336166\n",
      "Epoch 10915/30000 Training Loss: 0.041198305785655975\n",
      "Epoch 10916/30000 Training Loss: 0.048600856214761734\n",
      "Epoch 10917/30000 Training Loss: 0.04648227617144585\n",
      "Epoch 10918/30000 Training Loss: 0.04803890362381935\n",
      "Epoch 10919/30000 Training Loss: 0.03923054784536362\n",
      "Epoch 10920/30000 Training Loss: 0.04897120222449303\n",
      "Epoch 10921/30000 Training Loss: 0.052359603345394135\n",
      "Epoch 10922/30000 Training Loss: 0.04483473300933838\n",
      "Epoch 10923/30000 Training Loss: 0.04501425474882126\n",
      "Epoch 10924/30000 Training Loss: 0.048554517328739166\n",
      "Epoch 10925/30000 Training Loss: 0.05003289133310318\n",
      "Epoch 10926/30000 Training Loss: 0.039939045906066895\n",
      "Epoch 10927/30000 Training Loss: 0.04917556047439575\n",
      "Epoch 10928/30000 Training Loss: 0.04961882531642914\n",
      "Epoch 10929/30000 Training Loss: 0.05003909021615982\n",
      "Epoch 10930/30000 Training Loss: 0.04664330929517746\n",
      "Epoch 10931/30000 Training Loss: 0.04909415915608406\n",
      "Epoch 10932/30000 Training Loss: 0.05436638742685318\n",
      "Epoch 10933/30000 Training Loss: 0.042783502489328384\n",
      "Epoch 10934/30000 Training Loss: 0.04717743769288063\n",
      "Epoch 10935/30000 Training Loss: 0.042673397809267044\n",
      "Epoch 10936/30000 Training Loss: 0.04245024174451828\n",
      "Epoch 10937/30000 Training Loss: 0.03958917781710625\n",
      "Epoch 10938/30000 Training Loss: 0.03867004066705704\n",
      "Epoch 10939/30000 Training Loss: 0.04621342942118645\n",
      "Epoch 10940/30000 Training Loss: 0.042783308774232864\n",
      "Epoch 10941/30000 Training Loss: 0.04900730773806572\n",
      "Epoch 10942/30000 Training Loss: 0.0452648289501667\n",
      "Epoch 10943/30000 Training Loss: 0.04789679870009422\n",
      "Epoch 10944/30000 Training Loss: 0.04374544695019722\n",
      "Epoch 10945/30000 Training Loss: 0.049342043697834015\n",
      "Epoch 10946/30000 Training Loss: 0.041692979633808136\n",
      "Epoch 10947/30000 Training Loss: 0.04196954146027565\n",
      "Epoch 10948/30000 Training Loss: 0.046673983335494995\n",
      "Epoch 10949/30000 Training Loss: 0.04565431550145149\n",
      "Epoch 10950/30000 Training Loss: 0.0505586676299572\n",
      "Epoch 10950/30000 Validation Loss: 0.051841944456100464\n",
      "Epoch 10951/30000 Training Loss: 0.050226472318172455\n",
      "Epoch 10952/30000 Training Loss: 0.04127443954348564\n",
      "Epoch 10953/30000 Training Loss: 0.04048902913928032\n",
      "Epoch 10954/30000 Training Loss: 0.03618800640106201\n",
      "Epoch 10955/30000 Training Loss: 0.04068230092525482\n",
      "Epoch 10956/30000 Training Loss: 0.05274851992726326\n",
      "Epoch 10957/30000 Training Loss: 0.052196748554706573\n",
      "Epoch 10958/30000 Training Loss: 0.04840248078107834\n",
      "Epoch 10959/30000 Training Loss: 0.047188419848680496\n",
      "Epoch 10960/30000 Training Loss: 0.040700800716876984\n",
      "Epoch 10961/30000 Training Loss: 0.043812476098537445\n",
      "Epoch 10962/30000 Training Loss: 0.04540688544511795\n",
      "Epoch 10963/30000 Training Loss: 0.04709729179739952\n",
      "Epoch 10964/30000 Training Loss: 0.0549386627972126\n",
      "Epoch 10965/30000 Training Loss: 0.050510961562395096\n",
      "Epoch 10966/30000 Training Loss: 0.044796690344810486\n",
      "Epoch 10967/30000 Training Loss: 0.04200587049126625\n",
      "Epoch 10968/30000 Training Loss: 0.04167019575834274\n",
      "Epoch 10969/30000 Training Loss: 0.04552075266838074\n",
      "Epoch 10970/30000 Training Loss: 0.05238882824778557\n",
      "Epoch 10971/30000 Training Loss: 0.06208912655711174\n",
      "Epoch 10972/30000 Training Loss: 0.05228399485349655\n",
      "Epoch 10973/30000 Training Loss: 0.04225712642073631\n",
      "Epoch 10974/30000 Training Loss: 0.04847618192434311\n",
      "Epoch 10975/30000 Training Loss: 0.04632944613695145\n",
      "Epoch 10976/30000 Training Loss: 0.04154274985194206\n",
      "Epoch 10977/30000 Training Loss: 0.04215739294886589\n",
      "Epoch 10978/30000 Training Loss: 0.05370163917541504\n",
      "Epoch 10979/30000 Training Loss: 0.042978983372449875\n",
      "Epoch 10980/30000 Training Loss: 0.042197175323963165\n",
      "Epoch 10981/30000 Training Loss: 0.05076242610812187\n",
      "Epoch 10982/30000 Training Loss: 0.05270680785179138\n",
      "Epoch 10983/30000 Training Loss: 0.03725394606590271\n",
      "Epoch 10984/30000 Training Loss: 0.03928186371922493\n",
      "Epoch 10985/30000 Training Loss: 0.04231654107570648\n",
      "Epoch 10986/30000 Training Loss: 0.048486240208148956\n",
      "Epoch 10987/30000 Training Loss: 0.0479646772146225\n",
      "Epoch 10988/30000 Training Loss: 0.04504740983247757\n",
      "Epoch 10989/30000 Training Loss: 0.04615793749690056\n",
      "Epoch 10990/30000 Training Loss: 0.047322556376457214\n",
      "Epoch 10991/30000 Training Loss: 0.04828524589538574\n",
      "Epoch 10992/30000 Training Loss: 0.045626942068338394\n",
      "Epoch 10993/30000 Training Loss: 0.044794779270887375\n",
      "Epoch 10994/30000 Training Loss: 0.046931445598602295\n",
      "Epoch 10995/30000 Training Loss: 0.041388723999261856\n",
      "Epoch 10996/30000 Training Loss: 0.04364059492945671\n",
      "Epoch 10997/30000 Training Loss: 0.04550391435623169\n",
      "Epoch 10998/30000 Training Loss: 0.04122451692819595\n",
      "Epoch 10999/30000 Training Loss: 0.046975694596767426\n",
      "Epoch 11000/30000 Training Loss: 0.045107077807188034\n",
      "Epoch 11000/30000 Validation Loss: 0.05359070748090744\n",
      "Epoch 11001/30000 Training Loss: 0.04904177412390709\n",
      "Epoch 11002/30000 Training Loss: 0.043551549315452576\n",
      "Epoch 11003/30000 Training Loss: 0.042121522128582\n",
      "Epoch 11004/30000 Training Loss: 0.05223875492811203\n",
      "Epoch 11005/30000 Training Loss: 0.04182855784893036\n",
      "Epoch 11006/30000 Training Loss: 0.048886604607105255\n",
      "Epoch 11007/30000 Training Loss: 0.04852268844842911\n",
      "Epoch 11008/30000 Training Loss: 0.047178205102682114\n",
      "Epoch 11009/30000 Training Loss: 0.0495886392891407\n",
      "Epoch 11010/30000 Training Loss: 0.055593498051166534\n",
      "Epoch 11011/30000 Training Loss: 0.04054791480302811\n",
      "Epoch 11012/30000 Training Loss: 0.0444059781730175\n",
      "Epoch 11013/30000 Training Loss: 0.0450291745364666\n",
      "Epoch 11014/30000 Training Loss: 0.05021503567695618\n",
      "Epoch 11015/30000 Training Loss: 0.04562080651521683\n",
      "Epoch 11016/30000 Training Loss: 0.044509612023830414\n",
      "Epoch 11017/30000 Training Loss: 0.051532644778490067\n",
      "Epoch 11018/30000 Training Loss: 0.040616534650325775\n",
      "Epoch 11019/30000 Training Loss: 0.04795380309224129\n",
      "Epoch 11020/30000 Training Loss: 0.04745303839445114\n",
      "Epoch 11021/30000 Training Loss: 0.03571883961558342\n",
      "Epoch 11022/30000 Training Loss: 0.046884022653102875\n",
      "Epoch 11023/30000 Training Loss: 0.057698946446180344\n",
      "Epoch 11024/30000 Training Loss: 0.041467659175395966\n",
      "Epoch 11025/30000 Training Loss: 0.04262639209628105\n",
      "Epoch 11026/30000 Training Loss: 0.05519995838403702\n",
      "Epoch 11027/30000 Training Loss: 0.04942788556218147\n",
      "Epoch 11028/30000 Training Loss: 0.048368170857429504\n",
      "Epoch 11029/30000 Training Loss: 0.04744616150856018\n",
      "Epoch 11030/30000 Training Loss: 0.042651984840631485\n",
      "Epoch 11031/30000 Training Loss: 0.056443531066179276\n",
      "Epoch 11032/30000 Training Loss: 0.04326998442411423\n",
      "Epoch 11033/30000 Training Loss: 0.04656985029578209\n",
      "Epoch 11034/30000 Training Loss: 0.045311227440834045\n",
      "Epoch 11035/30000 Training Loss: 0.05675375461578369\n",
      "Epoch 11036/30000 Training Loss: 0.04381249099969864\n",
      "Epoch 11037/30000 Training Loss: 0.04545053094625473\n",
      "Epoch 11038/30000 Training Loss: 0.040874309837818146\n",
      "Epoch 11039/30000 Training Loss: 0.04204295948147774\n",
      "Epoch 11040/30000 Training Loss: 0.05362212657928467\n",
      "Epoch 11041/30000 Training Loss: 0.03893103450536728\n",
      "Epoch 11042/30000 Training Loss: 0.05558744817972183\n",
      "Epoch 11043/30000 Training Loss: 0.051319293677806854\n",
      "Epoch 11044/30000 Training Loss: 0.04485594853758812\n",
      "Epoch 11045/30000 Training Loss: 0.049218591302633286\n",
      "Epoch 11046/30000 Training Loss: 0.04748833179473877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11047/30000 Training Loss: 0.05331996828317642\n",
      "Epoch 11048/30000 Training Loss: 0.048597097396850586\n",
      "Epoch 11049/30000 Training Loss: 0.05112725496292114\n",
      "Epoch 11050/30000 Training Loss: 0.0448082759976387\n",
      "Epoch 11050/30000 Validation Loss: 0.03885362669825554\n",
      "Epoch 11051/30000 Training Loss: 0.04638249799609184\n",
      "Epoch 11052/30000 Training Loss: 0.0469059944152832\n",
      "Epoch 11053/30000 Training Loss: 0.044147513806819916\n",
      "Epoch 11054/30000 Training Loss: 0.040779031813144684\n",
      "Epoch 11055/30000 Training Loss: 0.04456431046128273\n",
      "Epoch 11056/30000 Training Loss: 0.04516828805208206\n",
      "Epoch 11057/30000 Training Loss: 0.050785958766937256\n",
      "Epoch 11058/30000 Training Loss: 0.042750678956508636\n",
      "Epoch 11059/30000 Training Loss: 0.05288609117269516\n",
      "Epoch 11060/30000 Training Loss: 0.042844925075769424\n",
      "Epoch 11061/30000 Training Loss: 0.04233650118112564\n",
      "Epoch 11062/30000 Training Loss: 0.048499543219804764\n",
      "Epoch 11063/30000 Training Loss: 0.044273026287555695\n",
      "Epoch 11064/30000 Training Loss: 0.046814776957035065\n",
      "Epoch 11065/30000 Training Loss: 0.04153890162706375\n",
      "Epoch 11066/30000 Training Loss: 0.033800385892391205\n",
      "Epoch 11067/30000 Training Loss: 0.04693470895290375\n",
      "Epoch 11068/30000 Training Loss: 0.05116896703839302\n",
      "Epoch 11069/30000 Training Loss: 0.0436425507068634\n",
      "Epoch 11070/30000 Training Loss: 0.053518425673246384\n",
      "Epoch 11071/30000 Training Loss: 0.05140715092420578\n",
      "Epoch 11072/30000 Training Loss: 0.04371798411011696\n",
      "Epoch 11073/30000 Training Loss: 0.038838278502225876\n",
      "Epoch 11074/30000 Training Loss: 0.04979841783642769\n",
      "Epoch 11075/30000 Training Loss: 0.044063858687877655\n",
      "Epoch 11076/30000 Training Loss: 0.04844290018081665\n",
      "Epoch 11077/30000 Training Loss: 0.0355810709297657\n",
      "Epoch 11078/30000 Training Loss: 0.035957105457782745\n",
      "Epoch 11079/30000 Training Loss: 0.05005238205194473\n",
      "Epoch 11080/30000 Training Loss: 0.048884905874729156\n",
      "Epoch 11081/30000 Training Loss: 0.05042090266942978\n",
      "Epoch 11082/30000 Training Loss: 0.050691016018390656\n",
      "Epoch 11083/30000 Training Loss: 0.04772214964032173\n",
      "Epoch 11084/30000 Training Loss: 0.04693307355046272\n",
      "Epoch 11085/30000 Training Loss: 0.043693237006664276\n",
      "Epoch 11086/30000 Training Loss: 0.0413336381316185\n",
      "Epoch 11087/30000 Training Loss: 0.043006353080272675\n",
      "Epoch 11088/30000 Training Loss: 0.04006032273173332\n",
      "Epoch 11089/30000 Training Loss: 0.05248556286096573\n",
      "Epoch 11090/30000 Training Loss: 0.05216572806239128\n",
      "Epoch 11091/30000 Training Loss: 0.04209182411432266\n",
      "Epoch 11092/30000 Training Loss: 0.043597251176834106\n",
      "Epoch 11093/30000 Training Loss: 0.051169633865356445\n",
      "Epoch 11094/30000 Training Loss: 0.04208683595061302\n",
      "Epoch 11095/30000 Training Loss: 0.04354465752840042\n",
      "Epoch 11096/30000 Training Loss: 0.04422477260231972\n",
      "Epoch 11097/30000 Training Loss: 0.04961002618074417\n",
      "Epoch 11098/30000 Training Loss: 0.046364523470401764\n",
      "Epoch 11099/30000 Training Loss: 0.049536678940057755\n",
      "Epoch 11100/30000 Training Loss: 0.049683988094329834\n",
      "Epoch 11100/30000 Validation Loss: 0.04190370813012123\n",
      "Epoch 11101/30000 Training Loss: 0.044747550040483475\n",
      "Epoch 11102/30000 Training Loss: 0.04579704999923706\n",
      "Epoch 11103/30000 Training Loss: 0.039221107959747314\n",
      "Epoch 11104/30000 Training Loss: 0.047531209886074066\n",
      "Epoch 11105/30000 Training Loss: 0.04001159220933914\n",
      "Epoch 11106/30000 Training Loss: 0.047527238726615906\n",
      "Epoch 11107/30000 Training Loss: 0.04465475305914879\n",
      "Epoch 11108/30000 Training Loss: 0.0374051034450531\n",
      "Epoch 11109/30000 Training Loss: 0.05159315466880798\n",
      "Epoch 11110/30000 Training Loss: 0.046988531947135925\n",
      "Epoch 11111/30000 Training Loss: 0.040905870497226715\n",
      "Epoch 11112/30000 Training Loss: 0.050099074840545654\n",
      "Epoch 11113/30000 Training Loss: 0.05574151873588562\n",
      "Epoch 11114/30000 Training Loss: 0.047453347593545914\n",
      "Epoch 11115/30000 Training Loss: 0.0420016385614872\n",
      "Epoch 11116/30000 Training Loss: 0.0458221435546875\n",
      "Epoch 11117/30000 Training Loss: 0.04849496856331825\n",
      "Epoch 11118/30000 Training Loss: 0.031753890216350555\n",
      "Epoch 11119/30000 Training Loss: 0.04142855480313301\n",
      "Epoch 11120/30000 Training Loss: 0.04379672184586525\n",
      "Epoch 11121/30000 Training Loss: 0.04596879333257675\n",
      "Epoch 11122/30000 Training Loss: 0.042854491621255875\n",
      "Epoch 11123/30000 Training Loss: 0.04707532376050949\n",
      "Epoch 11124/30000 Training Loss: 0.04448443278670311\n",
      "Epoch 11125/30000 Training Loss: 0.049322452396154404\n",
      "Epoch 11126/30000 Training Loss: 0.04244614392518997\n",
      "Epoch 11127/30000 Training Loss: 0.05324169993400574\n",
      "Epoch 11128/30000 Training Loss: 0.050613660365343094\n",
      "Epoch 11129/30000 Training Loss: 0.04848063364624977\n",
      "Epoch 11130/30000 Training Loss: 0.05096202343702316\n",
      "Epoch 11131/30000 Training Loss: 0.04959278553724289\n",
      "Epoch 11132/30000 Training Loss: 0.044888369739055634\n",
      "Epoch 11133/30000 Training Loss: 0.04880049452185631\n",
      "Epoch 11134/30000 Training Loss: 0.04491058737039566\n",
      "Epoch 11135/30000 Training Loss: 0.05167751386761665\n",
      "Epoch 11136/30000 Training Loss: 0.0457075834274292\n",
      "Epoch 11137/30000 Training Loss: 0.04871148243546486\n",
      "Epoch 11138/30000 Training Loss: 0.05092348903417587\n",
      "Epoch 11139/30000 Training Loss: 0.05609797686338425\n",
      "Epoch 11140/30000 Training Loss: 0.046228084713220596\n",
      "Epoch 11141/30000 Training Loss: 0.038064904510974884\n",
      "Epoch 11142/30000 Training Loss: 0.04252921789884567\n",
      "Epoch 11143/30000 Training Loss: 0.04804322496056557\n",
      "Epoch 11144/30000 Training Loss: 0.04902341216802597\n",
      "Epoch 11145/30000 Training Loss: 0.04616478085517883\n",
      "Epoch 11146/30000 Training Loss: 0.04435925930738449\n",
      "Epoch 11147/30000 Training Loss: 0.051294129341840744\n",
      "Epoch 11148/30000 Training Loss: 0.04770652577280998\n",
      "Epoch 11149/30000 Training Loss: 0.04349295422434807\n",
      "Epoch 11150/30000 Training Loss: 0.04202937334775925\n",
      "Epoch 11150/30000 Validation Loss: 0.04281110316514969\n",
      "Epoch 11151/30000 Training Loss: 0.04185785353183746\n",
      "Epoch 11152/30000 Training Loss: 0.043336376547813416\n",
      "Epoch 11153/30000 Training Loss: 0.042759500443935394\n",
      "Epoch 11154/30000 Training Loss: 0.043385203927755356\n",
      "Epoch 11155/30000 Training Loss: 0.04758512228727341\n",
      "Epoch 11156/30000 Training Loss: 0.052172668278217316\n",
      "Epoch 11157/30000 Training Loss: 0.040918298065662384\n",
      "Epoch 11158/30000 Training Loss: 0.03817243129014969\n",
      "Epoch 11159/30000 Training Loss: 0.044808901846408844\n",
      "Epoch 11160/30000 Training Loss: 0.044784750789403915\n",
      "Epoch 11161/30000 Training Loss: 0.041634779423475266\n",
      "Epoch 11162/30000 Training Loss: 0.04425112530589104\n",
      "Epoch 11163/30000 Training Loss: 0.04837780445814133\n",
      "Epoch 11164/30000 Training Loss: 0.04730815067887306\n",
      "Epoch 11165/30000 Training Loss: 0.04628673940896988\n",
      "Epoch 11166/30000 Training Loss: 0.04594873636960983\n",
      "Epoch 11167/30000 Training Loss: 0.05195732042193413\n",
      "Epoch 11168/30000 Training Loss: 0.0482911616563797\n",
      "Epoch 11169/30000 Training Loss: 0.051015399396419525\n",
      "Epoch 11170/30000 Training Loss: 0.05239275097846985\n",
      "Epoch 11171/30000 Training Loss: 0.04811641573905945\n",
      "Epoch 11172/30000 Training Loss: 0.03991318494081497\n",
      "Epoch 11173/30000 Training Loss: 0.0461660660803318\n",
      "Epoch 11174/30000 Training Loss: 0.05542647838592529\n",
      "Epoch 11175/30000 Training Loss: 0.04489220306277275\n",
      "Epoch 11176/30000 Training Loss: 0.04793623089790344\n",
      "Epoch 11177/30000 Training Loss: 0.04835159704089165\n",
      "Epoch 11178/30000 Training Loss: 0.046375345438718796\n",
      "Epoch 11179/30000 Training Loss: 0.04905860871076584\n",
      "Epoch 11180/30000 Training Loss: 0.03901729732751846\n",
      "Epoch 11181/30000 Training Loss: 0.044391799718141556\n",
      "Epoch 11182/30000 Training Loss: 0.04600690305233002\n",
      "Epoch 11183/30000 Training Loss: 0.05144204571843147\n",
      "Epoch 11184/30000 Training Loss: 0.052026957273483276\n",
      "Epoch 11185/30000 Training Loss: 0.04274919256567955\n",
      "Epoch 11186/30000 Training Loss: 0.044331081211566925\n",
      "Epoch 11187/30000 Training Loss: 0.04573531448841095\n",
      "Epoch 11188/30000 Training Loss: 0.04601268842816353\n",
      "Epoch 11189/30000 Training Loss: 0.04714015871286392\n",
      "Epoch 11190/30000 Training Loss: 0.045148856937885284\n",
      "Epoch 11191/30000 Training Loss: 0.03961001709103584\n",
      "Epoch 11192/30000 Training Loss: 0.04371150583028793\n",
      "Epoch 11193/30000 Training Loss: 0.04704798012971878\n",
      "Epoch 11194/30000 Training Loss: 0.05238047242164612\n",
      "Epoch 11195/30000 Training Loss: 0.0518561415374279\n",
      "Epoch 11196/30000 Training Loss: 0.03997720032930374\n",
      "Epoch 11197/30000 Training Loss: 0.04084271192550659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11198/30000 Training Loss: 0.04595201462507248\n",
      "Epoch 11199/30000 Training Loss: 0.04166388884186745\n",
      "Epoch 11200/30000 Training Loss: 0.04093803092837334\n",
      "Epoch 11200/30000 Validation Loss: 0.0423174649477005\n",
      "Epoch 11201/30000 Training Loss: 0.04779582470655441\n",
      "Epoch 11202/30000 Training Loss: 0.04645233601331711\n",
      "Epoch 11203/30000 Training Loss: 0.04646523669362068\n",
      "Epoch 11204/30000 Training Loss: 0.0426182858645916\n",
      "Epoch 11205/30000 Training Loss: 0.05008124187588692\n",
      "Epoch 11206/30000 Training Loss: 0.04825545474886894\n",
      "Epoch 11207/30000 Training Loss: 0.03922408074140549\n",
      "Epoch 11208/30000 Training Loss: 0.04296690970659256\n",
      "Epoch 11209/30000 Training Loss: 0.05665941163897514\n",
      "Epoch 11210/30000 Training Loss: 0.045957837253808975\n",
      "Epoch 11211/30000 Training Loss: 0.04137802869081497\n",
      "Epoch 11212/30000 Training Loss: 0.05032629892230034\n",
      "Epoch 11213/30000 Training Loss: 0.04799724370241165\n",
      "Epoch 11214/30000 Training Loss: 0.042277269065380096\n",
      "Epoch 11215/30000 Training Loss: 0.039127714931964874\n",
      "Epoch 11216/30000 Training Loss: 0.043082449585199356\n",
      "Epoch 11217/30000 Training Loss: 0.04021797329187393\n",
      "Epoch 11218/30000 Training Loss: 0.04949088394641876\n",
      "Epoch 11219/30000 Training Loss: 0.03819545358419418\n",
      "Epoch 11220/30000 Training Loss: 0.03958239406347275\n",
      "Epoch 11221/30000 Training Loss: 0.0443127267062664\n",
      "Epoch 11222/30000 Training Loss: 0.0524924173951149\n",
      "Epoch 11223/30000 Training Loss: 0.03888479992747307\n",
      "Epoch 11224/30000 Training Loss: 0.05145897716283798\n",
      "Epoch 11225/30000 Training Loss: 0.04949887469410896\n",
      "Epoch 11226/30000 Training Loss: 0.052238982170820236\n",
      "Epoch 11227/30000 Training Loss: 0.04254790395498276\n",
      "Epoch 11228/30000 Training Loss: 0.048050038516521454\n",
      "Epoch 11229/30000 Training Loss: 0.041093938052654266\n",
      "Epoch 11230/30000 Training Loss: 0.05259048193693161\n",
      "Epoch 11231/30000 Training Loss: 0.04930313304066658\n",
      "Epoch 11232/30000 Training Loss: 0.0481090322136879\n",
      "Epoch 11233/30000 Training Loss: 0.05285213142633438\n",
      "Epoch 11234/30000 Training Loss: 0.04618363827466965\n",
      "Epoch 11235/30000 Training Loss: 0.04582691192626953\n",
      "Epoch 11236/30000 Training Loss: 0.04314505681395531\n",
      "Epoch 11237/30000 Training Loss: 0.047342292964458466\n",
      "Epoch 11238/30000 Training Loss: 0.046533986926078796\n",
      "Epoch 11239/30000 Training Loss: 0.04190382361412048\n",
      "Epoch 11240/30000 Training Loss: 0.04891614615917206\n",
      "Epoch 11241/30000 Training Loss: 0.041625022888183594\n",
      "Epoch 11242/30000 Training Loss: 0.046718548983335495\n",
      "Epoch 11243/30000 Training Loss: 0.044924233108758926\n",
      "Epoch 11244/30000 Training Loss: 0.05408700555562973\n",
      "Epoch 11245/30000 Training Loss: 0.044251449406147\n",
      "Epoch 11246/30000 Training Loss: 0.04401435703039169\n",
      "Epoch 11247/30000 Training Loss: 0.04299630969762802\n",
      "Epoch 11248/30000 Training Loss: 0.043892405927181244\n",
      "Epoch 11249/30000 Training Loss: 0.05009736865758896\n",
      "Epoch 11250/30000 Training Loss: 0.05082346126437187\n",
      "Epoch 11250/30000 Validation Loss: 0.04943184182047844\n",
      "Epoch 11251/30000 Training Loss: 0.05578046292066574\n",
      "Epoch 11252/30000 Training Loss: 0.039386898279190063\n",
      "Epoch 11253/30000 Training Loss: 0.04868420213460922\n",
      "Epoch 11254/30000 Training Loss: 0.05386669188737869\n",
      "Epoch 11255/30000 Training Loss: 0.03568865358829498\n",
      "Epoch 11256/30000 Training Loss: 0.04426437243819237\n",
      "Epoch 11257/30000 Training Loss: 0.050018154084682465\n",
      "Epoch 11258/30000 Training Loss: 0.05396883562207222\n",
      "Epoch 11259/30000 Training Loss: 0.0551304891705513\n",
      "Epoch 11260/30000 Training Loss: 0.046890757977962494\n",
      "Epoch 11261/30000 Training Loss: 0.0436650775372982\n",
      "Epoch 11262/30000 Training Loss: 0.03964535519480705\n",
      "Epoch 11263/30000 Training Loss: 0.05101243779063225\n",
      "Epoch 11264/30000 Training Loss: 0.04370740056037903\n",
      "Epoch 11265/30000 Training Loss: 0.04560687392950058\n",
      "Epoch 11266/30000 Training Loss: 0.04091515392065048\n",
      "Epoch 11267/30000 Training Loss: 0.05314156413078308\n",
      "Epoch 11268/30000 Training Loss: 0.038283608853816986\n",
      "Epoch 11269/30000 Training Loss: 0.047132376581430435\n",
      "Epoch 11270/30000 Training Loss: 0.04496435448527336\n",
      "Epoch 11271/30000 Training Loss: 0.047729603946208954\n",
      "Epoch 11272/30000 Training Loss: 0.050964076071977615\n",
      "Epoch 11273/30000 Training Loss: 0.04853590950369835\n",
      "Epoch 11274/30000 Training Loss: 0.04373401403427124\n",
      "Epoch 11275/30000 Training Loss: 0.03885903209447861\n",
      "Epoch 11276/30000 Training Loss: 0.047071006149053574\n",
      "Epoch 11277/30000 Training Loss: 0.045101139694452286\n",
      "Epoch 11278/30000 Training Loss: 0.03843620419502258\n",
      "Epoch 11279/30000 Training Loss: 0.044783417135477066\n",
      "Epoch 11280/30000 Training Loss: 0.04260063171386719\n",
      "Epoch 11281/30000 Training Loss: 0.05006009340286255\n",
      "Epoch 11282/30000 Training Loss: 0.04360582306981087\n",
      "Epoch 11283/30000 Training Loss: 0.0462278313934803\n",
      "Epoch 11284/30000 Training Loss: 0.056717731058597565\n",
      "Epoch 11285/30000 Training Loss: 0.05349559709429741\n",
      "Epoch 11286/30000 Training Loss: 0.04816196486353874\n",
      "Epoch 11287/30000 Training Loss: 0.04270527884364128\n",
      "Epoch 11288/30000 Training Loss: 0.03892537206411362\n",
      "Epoch 11289/30000 Training Loss: 0.049419183284044266\n",
      "Epoch 11290/30000 Training Loss: 0.040924765169620514\n",
      "Epoch 11291/30000 Training Loss: 0.052714645862579346\n",
      "Epoch 11292/30000 Training Loss: 0.054625701159238815\n",
      "Epoch 11293/30000 Training Loss: 0.04597435146570206\n",
      "Epoch 11294/30000 Training Loss: 0.03859208896756172\n",
      "Epoch 11295/30000 Training Loss: 0.042149920016527176\n",
      "Epoch 11296/30000 Training Loss: 0.04456697031855583\n",
      "Epoch 11297/30000 Training Loss: 0.05212577059864998\n",
      "Epoch 11298/30000 Training Loss: 0.04518405348062515\n",
      "Epoch 11299/30000 Training Loss: 0.04149101674556732\n",
      "Epoch 11300/30000 Training Loss: 0.05116187408566475\n",
      "Epoch 11300/30000 Validation Loss: 0.04292919486761093\n",
      "Epoch 11301/30000 Training Loss: 0.045074619352817535\n",
      "Epoch 11302/30000 Training Loss: 0.04909361153841019\n",
      "Epoch 11303/30000 Training Loss: 0.05243423581123352\n",
      "Epoch 11304/30000 Training Loss: 0.05281681567430496\n",
      "Epoch 11305/30000 Training Loss: 0.04157007858157158\n",
      "Epoch 11306/30000 Training Loss: 0.044212281703948975\n",
      "Epoch 11307/30000 Training Loss: 0.052848197519779205\n",
      "Epoch 11308/30000 Training Loss: 0.051667630672454834\n",
      "Epoch 11309/30000 Training Loss: 0.04682658612728119\n",
      "Epoch 11310/30000 Training Loss: 0.04019897058606148\n",
      "Epoch 11311/30000 Training Loss: 0.05454588681459427\n",
      "Epoch 11312/30000 Training Loss: 0.043603770434856415\n",
      "Epoch 11313/30000 Training Loss: 0.04736845940351486\n",
      "Epoch 11314/30000 Training Loss: 0.05031229183077812\n",
      "Epoch 11315/30000 Training Loss: 0.04566679149866104\n",
      "Epoch 11316/30000 Training Loss: 0.0382782481610775\n",
      "Epoch 11317/30000 Training Loss: 0.04968642443418503\n",
      "Epoch 11318/30000 Training Loss: 0.053674131631851196\n",
      "Epoch 11319/30000 Training Loss: 0.05807361751794815\n",
      "Epoch 11320/30000 Training Loss: 0.04829145595431328\n",
      "Epoch 11321/30000 Training Loss: 0.046139709651470184\n",
      "Epoch 11322/30000 Training Loss: 0.05066436529159546\n",
      "Epoch 11323/30000 Training Loss: 0.04229217767715454\n",
      "Epoch 11324/30000 Training Loss: 0.04314587265253067\n",
      "Epoch 11325/30000 Training Loss: 0.050976693630218506\n",
      "Epoch 11326/30000 Training Loss: 0.044945575296878815\n",
      "Epoch 11327/30000 Training Loss: 0.048156868666410446\n",
      "Epoch 11328/30000 Training Loss: 0.04902566224336624\n",
      "Epoch 11329/30000 Training Loss: 0.05154576897621155\n",
      "Epoch 11330/30000 Training Loss: 0.03795964643359184\n",
      "Epoch 11331/30000 Training Loss: 0.05003313347697258\n",
      "Epoch 11332/30000 Training Loss: 0.04421376436948776\n",
      "Epoch 11333/30000 Training Loss: 0.05490298941731453\n",
      "Epoch 11334/30000 Training Loss: 0.04260561615228653\n",
      "Epoch 11335/30000 Training Loss: 0.0413886159658432\n",
      "Epoch 11336/30000 Training Loss: 0.04286084696650505\n",
      "Epoch 11337/30000 Training Loss: 0.04298282787203789\n",
      "Epoch 11338/30000 Training Loss: 0.047876082360744476\n",
      "Epoch 11339/30000 Training Loss: 0.0468834787607193\n",
      "Epoch 11340/30000 Training Loss: 0.048368632793426514\n",
      "Epoch 11341/30000 Training Loss: 0.038489509373903275\n",
      "Epoch 11342/30000 Training Loss: 0.05067148804664612\n",
      "Epoch 11343/30000 Training Loss: 0.04950878769159317\n",
      "Epoch 11344/30000 Training Loss: 0.04265622794628143\n",
      "Epoch 11345/30000 Training Loss: 0.04591568186879158\n",
      "Epoch 11346/30000 Training Loss: 0.04639596492052078\n",
      "Epoch 11347/30000 Training Loss: 0.047466941177845\n",
      "Epoch 11348/30000 Training Loss: 0.042488664388656616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11349/30000 Training Loss: 0.04617923125624657\n",
      "Epoch 11350/30000 Training Loss: 0.048391833901405334\n",
      "Epoch 11350/30000 Validation Loss: 0.04294021427631378\n",
      "Epoch 11351/30000 Training Loss: 0.05387384444475174\n",
      "Epoch 11352/30000 Training Loss: 0.04019596427679062\n",
      "Epoch 11353/30000 Training Loss: 0.05195673182606697\n",
      "Epoch 11354/30000 Training Loss: 0.04321950674057007\n",
      "Epoch 11355/30000 Training Loss: 0.04580686241388321\n",
      "Epoch 11356/30000 Training Loss: 0.043895985931158066\n",
      "Epoch 11357/30000 Training Loss: 0.04234693571925163\n",
      "Epoch 11358/30000 Training Loss: 0.04711981862783432\n",
      "Epoch 11359/30000 Training Loss: 0.03991676867008209\n",
      "Epoch 11360/30000 Training Loss: 0.043130647391080856\n",
      "Epoch 11361/30000 Training Loss: 0.04377947747707367\n",
      "Epoch 11362/30000 Training Loss: 0.04635729640722275\n",
      "Epoch 11363/30000 Training Loss: 0.04606001451611519\n",
      "Epoch 11364/30000 Training Loss: 0.053327519446611404\n",
      "Epoch 11365/30000 Training Loss: 0.05152364820241928\n",
      "Epoch 11366/30000 Training Loss: 0.04062045365571976\n",
      "Epoch 11367/30000 Training Loss: 0.0457308292388916\n",
      "Epoch 11368/30000 Training Loss: 0.04400958865880966\n",
      "Epoch 11369/30000 Training Loss: 0.05372576043009758\n",
      "Epoch 11370/30000 Training Loss: 0.04817605018615723\n",
      "Epoch 11371/30000 Training Loss: 0.04340976104140282\n",
      "Epoch 11372/30000 Training Loss: 0.04583541676402092\n",
      "Epoch 11373/30000 Training Loss: 0.047352973371744156\n",
      "Epoch 11374/30000 Training Loss: 0.040604256093502045\n",
      "Epoch 11375/30000 Training Loss: 0.046664007008075714\n",
      "Epoch 11376/30000 Training Loss: 0.03932087868452072\n",
      "Epoch 11377/30000 Training Loss: 0.04209639132022858\n",
      "Epoch 11378/30000 Training Loss: 0.0432400107383728\n",
      "Epoch 11379/30000 Training Loss: 0.04415486007928848\n",
      "Epoch 11380/30000 Training Loss: 0.0535215325653553\n",
      "Epoch 11381/30000 Training Loss: 0.045807015150785446\n",
      "Epoch 11382/30000 Training Loss: 0.04951263219118118\n",
      "Epoch 11383/30000 Training Loss: 0.05490227788686752\n",
      "Epoch 11384/30000 Training Loss: 0.04285546764731407\n",
      "Epoch 11385/30000 Training Loss: 0.0415346696972847\n",
      "Epoch 11386/30000 Training Loss: 0.04618004336953163\n",
      "Epoch 11387/30000 Training Loss: 0.047634705901145935\n",
      "Epoch 11388/30000 Training Loss: 0.05283457785844803\n",
      "Epoch 11389/30000 Training Loss: 0.04038140922784805\n",
      "Epoch 11390/30000 Training Loss: 0.04480864480137825\n",
      "Epoch 11391/30000 Training Loss: 0.04455699399113655\n",
      "Epoch 11392/30000 Training Loss: 0.04917805641889572\n",
      "Epoch 11393/30000 Training Loss: 0.04974117875099182\n",
      "Epoch 11394/30000 Training Loss: 0.04720623046159744\n",
      "Epoch 11395/30000 Training Loss: 0.047289006412029266\n",
      "Epoch 11396/30000 Training Loss: 0.0463516041636467\n",
      "Epoch 11397/30000 Training Loss: 0.04983571916818619\n",
      "Epoch 11398/30000 Training Loss: 0.04793819040060043\n",
      "Epoch 11399/30000 Training Loss: 0.047519344836473465\n",
      "Epoch 11400/30000 Training Loss: 0.04327813535928726\n",
      "Epoch 11400/30000 Validation Loss: 0.03770266845822334\n",
      "Epoch 11401/30000 Training Loss: 0.0330832414329052\n",
      "Epoch 11402/30000 Training Loss: 0.04086640849709511\n",
      "Epoch 11403/30000 Training Loss: 0.04451923817396164\n",
      "Epoch 11404/30000 Training Loss: 0.041509099304676056\n",
      "Epoch 11405/30000 Training Loss: 0.03916487842798233\n",
      "Epoch 11406/30000 Training Loss: 0.04121803492307663\n",
      "Epoch 11407/30000 Training Loss: 0.039213310927152634\n",
      "Epoch 11408/30000 Training Loss: 0.04475215822458267\n",
      "Epoch 11409/30000 Training Loss: 0.04745879024267197\n",
      "Epoch 11410/30000 Training Loss: 0.0426018089056015\n",
      "Epoch 11411/30000 Training Loss: 0.042204104363918304\n",
      "Epoch 11412/30000 Training Loss: 0.05093478411436081\n",
      "Epoch 11413/30000 Training Loss: 0.04950619488954544\n",
      "Epoch 11414/30000 Training Loss: 0.039446134120225906\n",
      "Epoch 11415/30000 Training Loss: 0.04276934266090393\n",
      "Epoch 11416/30000 Training Loss: 0.042793385684490204\n",
      "Epoch 11417/30000 Training Loss: 0.04461304098367691\n",
      "Epoch 11418/30000 Training Loss: 0.04855438321828842\n",
      "Epoch 11419/30000 Training Loss: 0.0452977754175663\n",
      "Epoch 11420/30000 Training Loss: 0.05165978521108627\n",
      "Epoch 11421/30000 Training Loss: 0.04099248722195625\n",
      "Epoch 11422/30000 Training Loss: 0.049925047904253006\n",
      "Epoch 11423/30000 Training Loss: 0.046474285423755646\n",
      "Epoch 11424/30000 Training Loss: 0.04538547620177269\n",
      "Epoch 11425/30000 Training Loss: 0.05539490655064583\n",
      "Epoch 11426/30000 Training Loss: 0.05157148092985153\n",
      "Epoch 11427/30000 Training Loss: 0.04321783408522606\n",
      "Epoch 11428/30000 Training Loss: 0.047296736389398575\n",
      "Epoch 11429/30000 Training Loss: 0.05091315507888794\n",
      "Epoch 11430/30000 Training Loss: 0.052552707493305206\n",
      "Epoch 11431/30000 Training Loss: 0.04502085968852043\n",
      "Epoch 11432/30000 Training Loss: 0.051550544798374176\n",
      "Epoch 11433/30000 Training Loss: 0.0482005849480629\n",
      "Epoch 11434/30000 Training Loss: 0.039630450308322906\n",
      "Epoch 11435/30000 Training Loss: 0.046456653624773026\n",
      "Epoch 11436/30000 Training Loss: 0.04773453623056412\n",
      "Epoch 11437/30000 Training Loss: 0.0465836338698864\n",
      "Epoch 11438/30000 Training Loss: 0.03971940279006958\n",
      "Epoch 11439/30000 Training Loss: 0.044263970106840134\n",
      "Epoch 11440/30000 Training Loss: 0.04180433228611946\n",
      "Epoch 11441/30000 Training Loss: 0.04775908589363098\n",
      "Epoch 11442/30000 Training Loss: 0.04364349693059921\n",
      "Epoch 11443/30000 Training Loss: 0.0473937913775444\n",
      "Epoch 11444/30000 Training Loss: 0.044569388031959534\n",
      "Epoch 11445/30000 Training Loss: 0.03851769119501114\n",
      "Epoch 11446/30000 Training Loss: 0.04169157147407532\n",
      "Epoch 11447/30000 Training Loss: 0.04735371470451355\n",
      "Epoch 11448/30000 Training Loss: 0.04656994342803955\n",
      "Epoch 11449/30000 Training Loss: 0.04381749406456947\n",
      "Epoch 11450/30000 Training Loss: 0.04454658180475235\n",
      "Epoch 11450/30000 Validation Loss: 0.046454619616270065\n",
      "Epoch 11451/30000 Training Loss: 0.045134346932172775\n",
      "Epoch 11452/30000 Training Loss: 0.04388947784900665\n",
      "Epoch 11453/30000 Training Loss: 0.04338807240128517\n",
      "Epoch 11454/30000 Training Loss: 0.04390595108270645\n",
      "Epoch 11455/30000 Training Loss: 0.044040270149707794\n",
      "Epoch 11456/30000 Training Loss: 0.04417344555258751\n",
      "Epoch 11457/30000 Training Loss: 0.051935791969299316\n",
      "Epoch 11458/30000 Training Loss: 0.0461614765226841\n",
      "Epoch 11459/30000 Training Loss: 0.04669912904500961\n",
      "Epoch 11460/30000 Training Loss: 0.04013020172715187\n",
      "Epoch 11461/30000 Training Loss: 0.0456981286406517\n",
      "Epoch 11462/30000 Training Loss: 0.047899357974529266\n",
      "Epoch 11463/30000 Training Loss: 0.04812480881810188\n",
      "Epoch 11464/30000 Training Loss: 0.044129546731710434\n",
      "Epoch 11465/30000 Training Loss: 0.051067840307950974\n",
      "Epoch 11466/30000 Training Loss: 0.039013348519802094\n",
      "Epoch 11467/30000 Training Loss: 0.04443827643990517\n",
      "Epoch 11468/30000 Training Loss: 0.04759930819272995\n",
      "Epoch 11469/30000 Training Loss: 0.04636824131011963\n",
      "Epoch 11470/30000 Training Loss: 0.04306036978960037\n",
      "Epoch 11471/30000 Training Loss: 0.05195218324661255\n",
      "Epoch 11472/30000 Training Loss: 0.04519939050078392\n",
      "Epoch 11473/30000 Training Loss: 0.038155995309352875\n",
      "Epoch 11474/30000 Training Loss: 0.03898852691054344\n",
      "Epoch 11475/30000 Training Loss: 0.04382024332880974\n",
      "Epoch 11476/30000 Training Loss: 0.03983103483915329\n",
      "Epoch 11477/30000 Training Loss: 0.04307645559310913\n",
      "Epoch 11478/30000 Training Loss: 0.04698237404227257\n",
      "Epoch 11479/30000 Training Loss: 0.04488552734255791\n",
      "Epoch 11480/30000 Training Loss: 0.0549900159239769\n",
      "Epoch 11481/30000 Training Loss: 0.047693487256765366\n",
      "Epoch 11482/30000 Training Loss: 0.049307774752378464\n",
      "Epoch 11483/30000 Training Loss: 0.03575243800878525\n",
      "Epoch 11484/30000 Training Loss: 0.04495503753423691\n",
      "Epoch 11485/30000 Training Loss: 0.04373853653669357\n",
      "Epoch 11486/30000 Training Loss: 0.04645103961229324\n",
      "Epoch 11487/30000 Training Loss: 0.04223431274294853\n",
      "Epoch 11488/30000 Training Loss: 0.048357605934143066\n",
      "Epoch 11489/30000 Training Loss: 0.04564269632101059\n",
      "Epoch 11490/30000 Training Loss: 0.051839280873537064\n",
      "Epoch 11491/30000 Training Loss: 0.04875331372022629\n",
      "Epoch 11492/30000 Training Loss: 0.047590263187885284\n",
      "Epoch 11493/30000 Training Loss: 0.048093296587467194\n",
      "Epoch 11494/30000 Training Loss: 0.05156661942601204\n",
      "Epoch 11495/30000 Training Loss: 0.038699567317962646\n",
      "Epoch 11496/30000 Training Loss: 0.048861753195524216\n",
      "Epoch 11497/30000 Training Loss: 0.04839012771844864\n",
      "Epoch 11498/30000 Training Loss: 0.04605044052004814\n",
      "Epoch 11499/30000 Training Loss: 0.04508694261312485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11500/30000 Training Loss: 0.04306386783719063\n",
      "Epoch 11500/30000 Validation Loss: 0.04133232682943344\n",
      "Epoch 11501/30000 Training Loss: 0.04207959026098251\n",
      "Epoch 11502/30000 Training Loss: 0.047361381351947784\n",
      "Epoch 11503/30000 Training Loss: 0.04511462524533272\n",
      "Epoch 11504/30000 Training Loss: 0.03857233002781868\n",
      "Epoch 11505/30000 Training Loss: 0.05143977329134941\n",
      "Epoch 11506/30000 Training Loss: 0.0446913056075573\n",
      "Epoch 11507/30000 Training Loss: 0.04841470345854759\n",
      "Epoch 11508/30000 Training Loss: 0.047848284244537354\n",
      "Epoch 11509/30000 Training Loss: 0.040856342762708664\n",
      "Epoch 11510/30000 Training Loss: 0.049222182482481\n",
      "Epoch 11511/30000 Training Loss: 0.041193749755620956\n",
      "Epoch 11512/30000 Training Loss: 0.05068833380937576\n",
      "Epoch 11513/30000 Training Loss: 0.039411820471286774\n",
      "Epoch 11514/30000 Training Loss: 0.04612993448972702\n",
      "Epoch 11515/30000 Training Loss: 0.040879108011722565\n",
      "Epoch 11516/30000 Training Loss: 0.049038954079151154\n",
      "Epoch 11517/30000 Training Loss: 0.04504138603806496\n",
      "Epoch 11518/30000 Training Loss: 0.035686664283275604\n",
      "Epoch 11519/30000 Training Loss: 0.0444108322262764\n",
      "Epoch 11520/30000 Training Loss: 0.03578006476163864\n",
      "Epoch 11521/30000 Training Loss: 0.0501180998980999\n",
      "Epoch 11522/30000 Training Loss: 0.05490602180361748\n",
      "Epoch 11523/30000 Training Loss: 0.054508425295352936\n",
      "Epoch 11524/30000 Training Loss: 0.04076668620109558\n",
      "Epoch 11525/30000 Training Loss: 0.049594201147556305\n",
      "Epoch 11526/30000 Training Loss: 0.04960116744041443\n",
      "Epoch 11527/30000 Training Loss: 0.04151955246925354\n",
      "Epoch 11528/30000 Training Loss: 0.04344658553600311\n",
      "Epoch 11529/30000 Training Loss: 0.04270113259553909\n",
      "Epoch 11530/30000 Training Loss: 0.050437651574611664\n",
      "Epoch 11531/30000 Training Loss: 0.043149612843990326\n",
      "Epoch 11532/30000 Training Loss: 0.04629264399409294\n",
      "Epoch 11533/30000 Training Loss: 0.042373187839984894\n",
      "Epoch 11534/30000 Training Loss: 0.05219695717096329\n",
      "Epoch 11535/30000 Training Loss: 0.04727960005402565\n",
      "Epoch 11536/30000 Training Loss: 0.0420636348426342\n",
      "Epoch 11537/30000 Training Loss: 0.048705197870731354\n",
      "Epoch 11538/30000 Training Loss: 0.04364097863435745\n",
      "Epoch 11539/30000 Training Loss: 0.03633131459355354\n",
      "Epoch 11540/30000 Training Loss: 0.04784278944134712\n",
      "Epoch 11541/30000 Training Loss: 0.04394841939210892\n",
      "Epoch 11542/30000 Training Loss: 0.050443101674318314\n",
      "Epoch 11543/30000 Training Loss: 0.0377928763628006\n",
      "Epoch 11544/30000 Training Loss: 0.046474575996398926\n",
      "Epoch 11545/30000 Training Loss: 0.04751042276620865\n",
      "Epoch 11546/30000 Training Loss: 0.049573980271816254\n",
      "Epoch 11547/30000 Training Loss: 0.04821052402257919\n",
      "Epoch 11548/30000 Training Loss: 0.04473289102315903\n",
      "Epoch 11549/30000 Training Loss: 0.04585329443216324\n",
      "Epoch 11550/30000 Training Loss: 0.04955065995454788\n",
      "Epoch 11550/30000 Validation Loss: 0.04965958744287491\n",
      "Epoch 11551/30000 Training Loss: 0.0401410236954689\n",
      "Epoch 11552/30000 Training Loss: 0.04812789708375931\n",
      "Epoch 11553/30000 Training Loss: 0.04229321703314781\n",
      "Epoch 11554/30000 Training Loss: 0.046399567276239395\n",
      "Epoch 11555/30000 Training Loss: 0.04418349266052246\n",
      "Epoch 11556/30000 Training Loss: 0.04264567419886589\n",
      "Epoch 11557/30000 Training Loss: 0.04228966310620308\n",
      "Epoch 11558/30000 Training Loss: 0.036999281495809555\n",
      "Epoch 11559/30000 Training Loss: 0.041565075516700745\n",
      "Epoch 11560/30000 Training Loss: 0.05313192680478096\n",
      "Epoch 11561/30000 Training Loss: 0.04355917125940323\n",
      "Epoch 11562/30000 Training Loss: 0.04809360206127167\n",
      "Epoch 11563/30000 Training Loss: 0.04784923046827316\n",
      "Epoch 11564/30000 Training Loss: 0.04001348465681076\n",
      "Epoch 11565/30000 Training Loss: 0.037514541298151016\n",
      "Epoch 11566/30000 Training Loss: 0.038269322365522385\n",
      "Epoch 11567/30000 Training Loss: 0.04394678398966789\n",
      "Epoch 11568/30000 Training Loss: 0.044639281928539276\n",
      "Epoch 11569/30000 Training Loss: 0.04875607416033745\n",
      "Epoch 11570/30000 Training Loss: 0.04282557964324951\n",
      "Epoch 11571/30000 Training Loss: 0.056089770048856735\n",
      "Epoch 11572/30000 Training Loss: 0.043328166007995605\n",
      "Epoch 11573/30000 Training Loss: 0.05073821544647217\n",
      "Epoch 11574/30000 Training Loss: 0.041678495705127716\n",
      "Epoch 11575/30000 Training Loss: 0.04376379773020744\n",
      "Epoch 11576/30000 Training Loss: 0.046406131237745285\n",
      "Epoch 11577/30000 Training Loss: 0.05036625266075134\n",
      "Epoch 11578/30000 Training Loss: 0.044112008064985275\n",
      "Epoch 11579/30000 Training Loss: 0.049032241106033325\n",
      "Epoch 11580/30000 Training Loss: 0.047678567469120026\n",
      "Epoch 11581/30000 Training Loss: 0.03654685989022255\n",
      "Epoch 11582/30000 Training Loss: 0.053411275148391724\n",
      "Epoch 11583/30000 Training Loss: 0.04515933245420456\n",
      "Epoch 11584/30000 Training Loss: 0.040390223264694214\n",
      "Epoch 11585/30000 Training Loss: 0.04941723495721817\n",
      "Epoch 11586/30000 Training Loss: 0.04452910274267197\n",
      "Epoch 11587/30000 Training Loss: 0.053976528346538544\n",
      "Epoch 11588/30000 Training Loss: 0.048716574907302856\n",
      "Epoch 11589/30000 Training Loss: 0.041631557047367096\n",
      "Epoch 11590/30000 Training Loss: 0.04304010793566704\n",
      "Epoch 11591/30000 Training Loss: 0.044727399945259094\n",
      "Epoch 11592/30000 Training Loss: 0.05580178648233414\n",
      "Epoch 11593/30000 Training Loss: 0.04184982180595398\n",
      "Epoch 11594/30000 Training Loss: 0.05197039246559143\n",
      "Epoch 11595/30000 Training Loss: 0.051768817007541656\n",
      "Epoch 11596/30000 Training Loss: 0.04928293824195862\n",
      "Epoch 11597/30000 Training Loss: 0.042797695845365524\n",
      "Epoch 11598/30000 Training Loss: 0.04840213060379028\n",
      "Epoch 11599/30000 Training Loss: 0.04403699189424515\n",
      "Epoch 11600/30000 Training Loss: 0.03635627031326294\n",
      "Epoch 11600/30000 Validation Loss: 0.038463614881038666\n",
      "Epoch 11601/30000 Training Loss: 0.04186517745256424\n",
      "Epoch 11602/30000 Training Loss: 0.048337798565626144\n",
      "Epoch 11603/30000 Training Loss: 0.05263461545109749\n",
      "Epoch 11604/30000 Training Loss: 0.052852679044008255\n",
      "Epoch 11605/30000 Training Loss: 0.04548444598913193\n",
      "Epoch 11606/30000 Training Loss: 0.03936299681663513\n",
      "Epoch 11607/30000 Training Loss: 0.04385409504175186\n",
      "Epoch 11608/30000 Training Loss: 0.0468604639172554\n",
      "Epoch 11609/30000 Training Loss: 0.045686036348342896\n",
      "Epoch 11610/30000 Training Loss: 0.05025209113955498\n",
      "Epoch 11611/30000 Training Loss: 0.04665470868349075\n",
      "Epoch 11612/30000 Training Loss: 0.04875325784087181\n",
      "Epoch 11613/30000 Training Loss: 0.039916835725307465\n",
      "Epoch 11614/30000 Training Loss: 0.05028042942285538\n",
      "Epoch 11615/30000 Training Loss: 0.0454518087208271\n",
      "Epoch 11616/30000 Training Loss: 0.04589328169822693\n",
      "Epoch 11617/30000 Training Loss: 0.053110696375370026\n",
      "Epoch 11618/30000 Training Loss: 0.048578858375549316\n",
      "Epoch 11619/30000 Training Loss: 0.05026525259017944\n",
      "Epoch 11620/30000 Training Loss: 0.04832763597369194\n",
      "Epoch 11621/30000 Training Loss: 0.04547400400042534\n",
      "Epoch 11622/30000 Training Loss: 0.04907243326306343\n",
      "Epoch 11623/30000 Training Loss: 0.05103129893541336\n",
      "Epoch 11624/30000 Training Loss: 0.04712932929396629\n",
      "Epoch 11625/30000 Training Loss: 0.05013597011566162\n",
      "Epoch 11626/30000 Training Loss: 0.04194151237607002\n",
      "Epoch 11627/30000 Training Loss: 0.04313258081674576\n",
      "Epoch 11628/30000 Training Loss: 0.05011509731411934\n",
      "Epoch 11629/30000 Training Loss: 0.043519891798496246\n",
      "Epoch 11630/30000 Training Loss: 0.037350814789533615\n",
      "Epoch 11631/30000 Training Loss: 0.05068023130297661\n",
      "Epoch 11632/30000 Training Loss: 0.04699435085058212\n",
      "Epoch 11633/30000 Training Loss: 0.04828115925192833\n",
      "Epoch 11634/30000 Training Loss: 0.04188374802470207\n",
      "Epoch 11635/30000 Training Loss: 0.04912855103611946\n",
      "Epoch 11636/30000 Training Loss: 0.04530710726976395\n",
      "Epoch 11637/30000 Training Loss: 0.050670694559812546\n",
      "Epoch 11638/30000 Training Loss: 0.03472266346216202\n",
      "Epoch 11639/30000 Training Loss: 0.049827661365270615\n",
      "Epoch 11640/30000 Training Loss: 0.05155833810567856\n",
      "Epoch 11641/30000 Training Loss: 0.051277052611112595\n",
      "Epoch 11642/30000 Training Loss: 0.04576604813337326\n",
      "Epoch 11643/30000 Training Loss: 0.040657348930835724\n",
      "Epoch 11644/30000 Training Loss: 0.042527321726083755\n",
      "Epoch 11645/30000 Training Loss: 0.03859235346317291\n",
      "Epoch 11646/30000 Training Loss: 0.04669956490397453\n",
      "Epoch 11647/30000 Training Loss: 0.046308714896440506\n",
      "Epoch 11648/30000 Training Loss: 0.04182017594575882\n",
      "Epoch 11649/30000 Training Loss: 0.0456218346953392\n",
      "Epoch 11650/30000 Training Loss: 0.044204212725162506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11650/30000 Validation Loss: 0.050250761210918427\n",
      "Epoch 11651/30000 Training Loss: 0.05298503115773201\n",
      "Epoch 11652/30000 Training Loss: 0.046962153166532516\n",
      "Epoch 11653/30000 Training Loss: 0.04310207813978195\n",
      "Epoch 11654/30000 Training Loss: 0.044813159853219986\n",
      "Epoch 11655/30000 Training Loss: 0.04540420323610306\n",
      "Epoch 11656/30000 Training Loss: 0.045955248177051544\n",
      "Epoch 11657/30000 Training Loss: 0.03852400928735733\n",
      "Epoch 11658/30000 Training Loss: 0.04683613032102585\n",
      "Epoch 11659/30000 Training Loss: 0.04660429805517197\n",
      "Epoch 11660/30000 Training Loss: 0.05247015878558159\n",
      "Epoch 11661/30000 Training Loss: 0.04367459565401077\n",
      "Epoch 11662/30000 Training Loss: 0.04642529785633087\n",
      "Epoch 11663/30000 Training Loss: 0.04152225703001022\n",
      "Epoch 11664/30000 Training Loss: 0.05260248854756355\n",
      "Epoch 11665/30000 Training Loss: 0.04460195451974869\n",
      "Epoch 11666/30000 Training Loss: 0.04609289392828941\n",
      "Epoch 11667/30000 Training Loss: 0.04239211231470108\n",
      "Epoch 11668/30000 Training Loss: 0.04445536807179451\n",
      "Epoch 11669/30000 Training Loss: 0.042925700545310974\n",
      "Epoch 11670/30000 Training Loss: 0.05113878846168518\n",
      "Epoch 11671/30000 Training Loss: 0.03962666541337967\n",
      "Epoch 11672/30000 Training Loss: 0.04084824398159981\n",
      "Epoch 11673/30000 Training Loss: 0.04542726278305054\n",
      "Epoch 11674/30000 Training Loss: 0.0502733513712883\n",
      "Epoch 11675/30000 Training Loss: 0.04420022293925285\n",
      "Epoch 11676/30000 Training Loss: 0.052908170968294144\n",
      "Epoch 11677/30000 Training Loss: 0.051103055477142334\n",
      "Epoch 11678/30000 Training Loss: 0.047658734023571014\n",
      "Epoch 11679/30000 Training Loss: 0.033043958246707916\n",
      "Epoch 11680/30000 Training Loss: 0.04076959937810898\n",
      "Epoch 11681/30000 Training Loss: 0.040352433919906616\n",
      "Epoch 11682/30000 Training Loss: 0.049080587923526764\n",
      "Epoch 11683/30000 Training Loss: 0.054047126322984695\n",
      "Epoch 11684/30000 Training Loss: 0.04728541150689125\n",
      "Epoch 11685/30000 Training Loss: 0.05580390617251396\n",
      "Epoch 11686/30000 Training Loss: 0.057616300880908966\n",
      "Epoch 11687/30000 Training Loss: 0.04371117800474167\n",
      "Epoch 11688/30000 Training Loss: 0.0458841547369957\n",
      "Epoch 11689/30000 Training Loss: 0.04935871809720993\n",
      "Epoch 11690/30000 Training Loss: 0.05028892681002617\n",
      "Epoch 11691/30000 Training Loss: 0.047055117785930634\n",
      "Epoch 11692/30000 Training Loss: 0.04037858173251152\n",
      "Epoch 11693/30000 Training Loss: 0.05474954843521118\n",
      "Epoch 11694/30000 Training Loss: 0.038860615342855453\n",
      "Epoch 11695/30000 Training Loss: 0.04158467799425125\n",
      "Epoch 11696/30000 Training Loss: 0.03729641065001488\n",
      "Epoch 11697/30000 Training Loss: 0.03465829789638519\n",
      "Epoch 11698/30000 Training Loss: 0.042471326887607574\n",
      "Epoch 11699/30000 Training Loss: 0.04353570193052292\n",
      "Epoch 11700/30000 Training Loss: 0.04032218083739281\n",
      "Epoch 11700/30000 Validation Loss: 0.04286465793848038\n",
      "Epoch 11701/30000 Training Loss: 0.042383983731269836\n",
      "Epoch 11702/30000 Training Loss: 0.051070790737867355\n",
      "Epoch 11703/30000 Training Loss: 0.043707989156246185\n",
      "Epoch 11704/30000 Training Loss: 0.04297617822885513\n",
      "Epoch 11705/30000 Training Loss: 0.0399768128991127\n",
      "Epoch 11706/30000 Training Loss: 0.04197593405842781\n",
      "Epoch 11707/30000 Training Loss: 0.040727559477090836\n",
      "Epoch 11708/30000 Training Loss: 0.03711114078760147\n",
      "Epoch 11709/30000 Training Loss: 0.03842976689338684\n",
      "Epoch 11710/30000 Training Loss: 0.04164993390440941\n",
      "Epoch 11711/30000 Training Loss: 0.04917416721582413\n",
      "Epoch 11712/30000 Training Loss: 0.05383056402206421\n",
      "Epoch 11713/30000 Training Loss: 0.04659949988126755\n",
      "Epoch 11714/30000 Training Loss: 0.04495162144303322\n",
      "Epoch 11715/30000 Training Loss: 0.04023370146751404\n",
      "Epoch 11716/30000 Training Loss: 0.04187317192554474\n",
      "Epoch 11717/30000 Training Loss: 0.042906492948532104\n",
      "Epoch 11718/30000 Training Loss: 0.05051596090197563\n",
      "Epoch 11719/30000 Training Loss: 0.039292529225349426\n",
      "Epoch 11720/30000 Training Loss: 0.051941294223070145\n",
      "Epoch 11721/30000 Training Loss: 0.045249126851558685\n",
      "Epoch 11722/30000 Training Loss: 0.04290292039513588\n",
      "Epoch 11723/30000 Training Loss: 0.04072561487555504\n",
      "Epoch 11724/30000 Training Loss: 0.04887153580784798\n",
      "Epoch 11725/30000 Training Loss: 0.048493944108486176\n",
      "Epoch 11726/30000 Training Loss: 0.04693141207098961\n",
      "Epoch 11727/30000 Training Loss: 0.04972798749804497\n",
      "Epoch 11728/30000 Training Loss: 0.045378923416137695\n",
      "Epoch 11729/30000 Training Loss: 0.04955778270959854\n",
      "Epoch 11730/30000 Training Loss: 0.04159124568104744\n",
      "Epoch 11731/30000 Training Loss: 0.04394393414258957\n",
      "Epoch 11732/30000 Training Loss: 0.050285302102565765\n",
      "Epoch 11733/30000 Training Loss: 0.03838871791958809\n",
      "Epoch 11734/30000 Training Loss: 0.04360838979482651\n",
      "Epoch 11735/30000 Training Loss: 0.04899139702320099\n",
      "Epoch 11736/30000 Training Loss: 0.04861370846629143\n",
      "Epoch 11737/30000 Training Loss: 0.042677801102399826\n",
      "Epoch 11738/30000 Training Loss: 0.04462253674864769\n",
      "Epoch 11739/30000 Training Loss: 0.04567641764879227\n",
      "Epoch 11740/30000 Training Loss: 0.04513731971383095\n",
      "Epoch 11741/30000 Training Loss: 0.04832848161458969\n",
      "Epoch 11742/30000 Training Loss: 0.05176232382655144\n",
      "Epoch 11743/30000 Training Loss: 0.042273517698049545\n",
      "Epoch 11744/30000 Training Loss: 0.0428922101855278\n",
      "Epoch 11745/30000 Training Loss: 0.04152463749051094\n",
      "Epoch 11746/30000 Training Loss: 0.05057137459516525\n",
      "Epoch 11747/30000 Training Loss: 0.04889265447854996\n",
      "Epoch 11748/30000 Training Loss: 0.04623651131987572\n",
      "Epoch 11749/30000 Training Loss: 0.04110391065478325\n",
      "Epoch 11750/30000 Training Loss: 0.03840472549200058\n",
      "Epoch 11750/30000 Validation Loss: 0.044476184993982315\n",
      "Epoch 11751/30000 Training Loss: 0.049912579357624054\n",
      "Epoch 11752/30000 Training Loss: 0.04728882387280464\n",
      "Epoch 11753/30000 Training Loss: 0.04801824316382408\n",
      "Epoch 11754/30000 Training Loss: 0.05128396674990654\n",
      "Epoch 11755/30000 Training Loss: 0.03895258158445358\n",
      "Epoch 11756/30000 Training Loss: 0.04974260926246643\n",
      "Epoch 11757/30000 Training Loss: 0.048946209251880646\n",
      "Epoch 11758/30000 Training Loss: 0.04062099754810333\n",
      "Epoch 11759/30000 Training Loss: 0.04884108155965805\n",
      "Epoch 11760/30000 Training Loss: 0.04433465003967285\n",
      "Epoch 11761/30000 Training Loss: 0.04428485408425331\n",
      "Epoch 11762/30000 Training Loss: 0.050763655453920364\n",
      "Epoch 11763/30000 Training Loss: 0.04419487714767456\n",
      "Epoch 11764/30000 Training Loss: 0.04377998411655426\n",
      "Epoch 11765/30000 Training Loss: 0.0535597987473011\n",
      "Epoch 11766/30000 Training Loss: 0.04896555468440056\n",
      "Epoch 11767/30000 Training Loss: 0.04544994980096817\n",
      "Epoch 11768/30000 Training Loss: 0.04115381836891174\n",
      "Epoch 11769/30000 Training Loss: 0.041094403713941574\n",
      "Epoch 11770/30000 Training Loss: 0.04535387083888054\n",
      "Epoch 11771/30000 Training Loss: 0.04961255565285683\n",
      "Epoch 11772/30000 Training Loss: 0.04744832590222359\n",
      "Epoch 11773/30000 Training Loss: 0.05356670171022415\n",
      "Epoch 11774/30000 Training Loss: 0.03785766288638115\n",
      "Epoch 11775/30000 Training Loss: 0.03942381218075752\n",
      "Epoch 11776/30000 Training Loss: 0.05171825736761093\n",
      "Epoch 11777/30000 Training Loss: 0.04522741958498955\n",
      "Epoch 11778/30000 Training Loss: 0.042096953839063644\n",
      "Epoch 11779/30000 Training Loss: 0.051369599997997284\n",
      "Epoch 11780/30000 Training Loss: 0.04274159297347069\n",
      "Epoch 11781/30000 Training Loss: 0.05164041370153427\n",
      "Epoch 11782/30000 Training Loss: 0.04409460723400116\n",
      "Epoch 11783/30000 Training Loss: 0.04580407589673996\n",
      "Epoch 11784/30000 Training Loss: 0.045506902039051056\n",
      "Epoch 11785/30000 Training Loss: 0.043224938213825226\n",
      "Epoch 11786/30000 Training Loss: 0.04767261818051338\n",
      "Epoch 11787/30000 Training Loss: 0.042986851185560226\n",
      "Epoch 11788/30000 Training Loss: 0.05224495381116867\n",
      "Epoch 11789/30000 Training Loss: 0.04688166826963425\n",
      "Epoch 11790/30000 Training Loss: 0.04577917605638504\n",
      "Epoch 11791/30000 Training Loss: 0.045691825449466705\n",
      "Epoch 11792/30000 Training Loss: 0.040425337851047516\n",
      "Epoch 11793/30000 Training Loss: 0.0422995388507843\n",
      "Epoch 11794/30000 Training Loss: 0.04581061750650406\n",
      "Epoch 11795/30000 Training Loss: 0.04663991183042526\n",
      "Epoch 11796/30000 Training Loss: 0.05300827696919441\n",
      "Epoch 11797/30000 Training Loss: 0.04347161576151848\n",
      "Epoch 11798/30000 Training Loss: 0.04966006428003311\n",
      "Epoch 11799/30000 Training Loss: 0.047985367476940155\n",
      "Epoch 11800/30000 Training Loss: 0.040924228727817535\n",
      "Epoch 11800/30000 Validation Loss: 0.041397202759981155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11801/30000 Training Loss: 0.049905166029930115\n",
      "Epoch 11802/30000 Training Loss: 0.04782628268003464\n",
      "Epoch 11803/30000 Training Loss: 0.04141167551279068\n",
      "Epoch 11804/30000 Training Loss: 0.049653030931949615\n",
      "Epoch 11805/30000 Training Loss: 0.04721856117248535\n",
      "Epoch 11806/30000 Training Loss: 0.036936189979314804\n",
      "Epoch 11807/30000 Training Loss: 0.04731565713882446\n",
      "Epoch 11808/30000 Training Loss: 0.039884306490421295\n",
      "Epoch 11809/30000 Training Loss: 0.044081173837184906\n",
      "Epoch 11810/30000 Training Loss: 0.043483633548021317\n",
      "Epoch 11811/30000 Training Loss: 0.04420654848217964\n",
      "Epoch 11812/30000 Training Loss: 0.03955620899796486\n",
      "Epoch 11813/30000 Training Loss: 0.046268440783023834\n",
      "Epoch 11814/30000 Training Loss: 0.04485676437616348\n",
      "Epoch 11815/30000 Training Loss: 0.05746545270085335\n",
      "Epoch 11816/30000 Training Loss: 0.0514947846531868\n",
      "Epoch 11817/30000 Training Loss: 0.04121655225753784\n",
      "Epoch 11818/30000 Training Loss: 0.04937351867556572\n",
      "Epoch 11819/30000 Training Loss: 0.04745054244995117\n",
      "Epoch 11820/30000 Training Loss: 0.042117469012737274\n",
      "Epoch 11821/30000 Training Loss: 0.04076527804136276\n",
      "Epoch 11822/30000 Training Loss: 0.03971480578184128\n",
      "Epoch 11823/30000 Training Loss: 0.04893190786242485\n",
      "Epoch 11824/30000 Training Loss: 0.044836677610874176\n",
      "Epoch 11825/30000 Training Loss: 0.0463775135576725\n",
      "Epoch 11826/30000 Training Loss: 0.05277924984693527\n",
      "Epoch 11827/30000 Training Loss: 0.04407484456896782\n",
      "Epoch 11828/30000 Training Loss: 0.04632114619016647\n",
      "Epoch 11829/30000 Training Loss: 0.044724054634571075\n",
      "Epoch 11830/30000 Training Loss: 0.04404463618993759\n",
      "Epoch 11831/30000 Training Loss: 0.05137358233332634\n",
      "Epoch 11832/30000 Training Loss: 0.042620088905096054\n",
      "Epoch 11833/30000 Training Loss: 0.04909870773553848\n",
      "Epoch 11834/30000 Training Loss: 0.0433378592133522\n",
      "Epoch 11835/30000 Training Loss: 0.05366883799433708\n",
      "Epoch 11836/30000 Training Loss: 0.04495443031191826\n",
      "Epoch 11837/30000 Training Loss: 0.04626951739192009\n",
      "Epoch 11838/30000 Training Loss: 0.049243588000535965\n",
      "Epoch 11839/30000 Training Loss: 0.043558187782764435\n",
      "Epoch 11840/30000 Training Loss: 0.035432763397693634\n",
      "Epoch 11841/30000 Training Loss: 0.04174116626381874\n",
      "Epoch 11842/30000 Training Loss: 0.04162014275789261\n",
      "Epoch 11843/30000 Training Loss: 0.042741525918245316\n",
      "Epoch 11844/30000 Training Loss: 0.044369325041770935\n",
      "Epoch 11845/30000 Training Loss: 0.04733564704656601\n",
      "Epoch 11846/30000 Training Loss: 0.04954986646771431\n",
      "Epoch 11847/30000 Training Loss: 0.0436815544962883\n",
      "Epoch 11848/30000 Training Loss: 0.036056727170944214\n",
      "Epoch 11849/30000 Training Loss: 0.0497032068669796\n",
      "Epoch 11850/30000 Training Loss: 0.03882025182247162\n",
      "Epoch 11850/30000 Validation Loss: 0.041729383170604706\n",
      "Epoch 11851/30000 Training Loss: 0.04592413827776909\n",
      "Epoch 11852/30000 Training Loss: 0.04449934512376785\n",
      "Epoch 11853/30000 Training Loss: 0.048559579998254776\n",
      "Epoch 11854/30000 Training Loss: 0.039303217083215714\n",
      "Epoch 11855/30000 Training Loss: 0.04311724379658699\n",
      "Epoch 11856/30000 Training Loss: 0.04476376250386238\n",
      "Epoch 11857/30000 Training Loss: 0.03930085152387619\n",
      "Epoch 11858/30000 Training Loss: 0.04975853115320206\n",
      "Epoch 11859/30000 Training Loss: 0.04214731231331825\n",
      "Epoch 11860/30000 Training Loss: 0.04349427670240402\n",
      "Epoch 11861/30000 Training Loss: 0.04412774369120598\n",
      "Epoch 11862/30000 Training Loss: 0.05084599182009697\n",
      "Epoch 11863/30000 Training Loss: 0.0478055477142334\n",
      "Epoch 11864/30000 Training Loss: 0.053785812109708786\n",
      "Epoch 11865/30000 Training Loss: 0.040907472372055054\n",
      "Epoch 11866/30000 Training Loss: 0.04732126742601395\n",
      "Epoch 11867/30000 Training Loss: 0.043434396386146545\n",
      "Epoch 11868/30000 Training Loss: 0.05234827846288681\n",
      "Epoch 11869/30000 Training Loss: 0.04025089368224144\n",
      "Epoch 11870/30000 Training Loss: 0.04392855614423752\n",
      "Epoch 11871/30000 Training Loss: 0.04937058687210083\n",
      "Epoch 11872/30000 Training Loss: 0.03957457095384598\n",
      "Epoch 11873/30000 Training Loss: 0.04284025728702545\n",
      "Epoch 11874/30000 Training Loss: 0.0420726016163826\n",
      "Epoch 11875/30000 Training Loss: 0.04018036276102066\n",
      "Epoch 11876/30000 Training Loss: 0.046467993408441544\n",
      "Epoch 11877/30000 Training Loss: 0.05057375505566597\n",
      "Epoch 11878/30000 Training Loss: 0.047732412815093994\n",
      "Epoch 11879/30000 Training Loss: 0.04534321278333664\n",
      "Epoch 11880/30000 Training Loss: 0.03746467083692551\n",
      "Epoch 11881/30000 Training Loss: 0.043834686279296875\n",
      "Epoch 11882/30000 Training Loss: 0.041821323335170746\n",
      "Epoch 11883/30000 Training Loss: 0.04556872695684433\n",
      "Epoch 11884/30000 Training Loss: 0.04122815281152725\n",
      "Epoch 11885/30000 Training Loss: 0.04803408309817314\n",
      "Epoch 11886/30000 Training Loss: 0.041491273790597916\n",
      "Epoch 11887/30000 Training Loss: 0.04123348742723465\n",
      "Epoch 11888/30000 Training Loss: 0.051362551748752594\n",
      "Epoch 11889/30000 Training Loss: 0.051246900111436844\n",
      "Epoch 11890/30000 Training Loss: 0.042791638523340225\n",
      "Epoch 11891/30000 Training Loss: 0.0478353314101696\n",
      "Epoch 11892/30000 Training Loss: 0.039353739470243454\n",
      "Epoch 11893/30000 Training Loss: 0.047757405787706375\n",
      "Epoch 11894/30000 Training Loss: 0.04541901499032974\n",
      "Epoch 11895/30000 Training Loss: 0.046332038938999176\n",
      "Epoch 11896/30000 Training Loss: 0.045276667922735214\n",
      "Epoch 11897/30000 Training Loss: 0.04915153235197067\n",
      "Epoch 11898/30000 Training Loss: 0.04331975430250168\n",
      "Epoch 11899/30000 Training Loss: 0.045665860176086426\n",
      "Epoch 11900/30000 Training Loss: 0.051639724522829056\n",
      "Epoch 11900/30000 Validation Loss: 0.05002269148826599\n",
      "Epoch 11901/30000 Training Loss: 0.039753302931785583\n",
      "Epoch 11902/30000 Training Loss: 0.047544531524181366\n",
      "Epoch 11903/30000 Training Loss: 0.04703698679804802\n",
      "Epoch 11904/30000 Training Loss: 0.04588466137647629\n",
      "Epoch 11905/30000 Training Loss: 0.044023092836141586\n",
      "Epoch 11906/30000 Training Loss: 0.04386298358440399\n",
      "Epoch 11907/30000 Training Loss: 0.03831963241100311\n",
      "Epoch 11908/30000 Training Loss: 0.0487266480922699\n",
      "Epoch 11909/30000 Training Loss: 0.04624597355723381\n",
      "Epoch 11910/30000 Training Loss: 0.03820418566465378\n",
      "Epoch 11911/30000 Training Loss: 0.04472862929105759\n",
      "Epoch 11912/30000 Training Loss: 0.04597049206495285\n",
      "Epoch 11913/30000 Training Loss: 0.04866104573011398\n",
      "Epoch 11914/30000 Training Loss: 0.04843401908874512\n",
      "Epoch 11915/30000 Training Loss: 0.04221859574317932\n",
      "Epoch 11916/30000 Training Loss: 0.04709337279200554\n",
      "Epoch 11917/30000 Training Loss: 0.04879452660679817\n",
      "Epoch 11918/30000 Training Loss: 0.046140141785144806\n",
      "Epoch 11919/30000 Training Loss: 0.03926479443907738\n",
      "Epoch 11920/30000 Training Loss: 0.03917999938130379\n",
      "Epoch 11921/30000 Training Loss: 0.04182243347167969\n",
      "Epoch 11922/30000 Training Loss: 0.05369331315159798\n",
      "Epoch 11923/30000 Training Loss: 0.047577112913131714\n",
      "Epoch 11924/30000 Training Loss: 0.03708262741565704\n",
      "Epoch 11925/30000 Training Loss: 0.045949120074510574\n",
      "Epoch 11926/30000 Training Loss: 0.04056870937347412\n",
      "Epoch 11927/30000 Training Loss: 0.05105956643819809\n",
      "Epoch 11928/30000 Training Loss: 0.03910364210605621\n",
      "Epoch 11929/30000 Training Loss: 0.04199875518679619\n",
      "Epoch 11930/30000 Training Loss: 0.05470948666334152\n",
      "Epoch 11931/30000 Training Loss: 0.046520426869392395\n",
      "Epoch 11932/30000 Training Loss: 0.05099637433886528\n",
      "Epoch 11933/30000 Training Loss: 0.04585333913564682\n",
      "Epoch 11934/30000 Training Loss: 0.045725904405117035\n",
      "Epoch 11935/30000 Training Loss: 0.04770367965102196\n",
      "Epoch 11936/30000 Training Loss: 0.04396643117070198\n",
      "Epoch 11937/30000 Training Loss: 0.04418247565627098\n",
      "Epoch 11938/30000 Training Loss: 0.04464355856180191\n",
      "Epoch 11939/30000 Training Loss: 0.04127957671880722\n",
      "Epoch 11940/30000 Training Loss: 0.04456276446580887\n",
      "Epoch 11941/30000 Training Loss: 0.04302791506052017\n",
      "Epoch 11942/30000 Training Loss: 0.059388667345047\n",
      "Epoch 11943/30000 Training Loss: 0.04726026952266693\n",
      "Epoch 11944/30000 Training Loss: 0.04992934688925743\n",
      "Epoch 11945/30000 Training Loss: 0.04268442466855049\n",
      "Epoch 11946/30000 Training Loss: 0.04152098670601845\n",
      "Epoch 11947/30000 Training Loss: 0.042066581547260284\n",
      "Epoch 11948/30000 Training Loss: 0.056155014783144\n",
      "Epoch 11949/30000 Training Loss: 0.04526804760098457\n",
      "Epoch 11950/30000 Training Loss: 0.04748712107539177\n",
      "Epoch 11950/30000 Validation Loss: 0.05862031131982803\n",
      "Epoch 11951/30000 Training Loss: 0.04744298383593559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11952/30000 Training Loss: 0.049364324659109116\n",
      "Epoch 11953/30000 Training Loss: 0.043051280081272125\n",
      "Epoch 11954/30000 Training Loss: 0.04370289295911789\n",
      "Epoch 11955/30000 Training Loss: 0.041795019060373306\n",
      "Epoch 11956/30000 Training Loss: 0.037730872631073\n",
      "Epoch 11957/30000 Training Loss: 0.04333025589585304\n",
      "Epoch 11958/30000 Training Loss: 0.0533638522028923\n",
      "Epoch 11959/30000 Training Loss: 0.04320010542869568\n",
      "Epoch 11960/30000 Training Loss: 0.04789695516228676\n",
      "Epoch 11961/30000 Training Loss: 0.041011370718479156\n",
      "Epoch 11962/30000 Training Loss: 0.042582057416439056\n",
      "Epoch 11963/30000 Training Loss: 0.05293453484773636\n",
      "Epoch 11964/30000 Training Loss: 0.04142069071531296\n",
      "Epoch 11965/30000 Training Loss: 0.05815678834915161\n",
      "Epoch 11966/30000 Training Loss: 0.04753194749355316\n",
      "Epoch 11967/30000 Training Loss: 0.055393218994140625\n",
      "Epoch 11968/30000 Training Loss: 0.041965238749980927\n",
      "Epoch 11969/30000 Training Loss: 0.04433704912662506\n",
      "Epoch 11970/30000 Training Loss: 0.04410957545042038\n",
      "Epoch 11971/30000 Training Loss: 0.04900677129626274\n",
      "Epoch 11972/30000 Training Loss: 0.04601672664284706\n",
      "Epoch 11973/30000 Training Loss: 0.04291042685508728\n",
      "Epoch 11974/30000 Training Loss: 0.04030894488096237\n",
      "Epoch 11975/30000 Training Loss: 0.05221562460064888\n",
      "Epoch 11976/30000 Training Loss: 0.03859451785683632\n",
      "Epoch 11977/30000 Training Loss: 0.04383430257439613\n",
      "Epoch 11978/30000 Training Loss: 0.04393627494573593\n",
      "Epoch 11979/30000 Training Loss: 0.04753242805600166\n",
      "Epoch 11980/30000 Training Loss: 0.04231186956167221\n",
      "Epoch 11981/30000 Training Loss: 0.04470846429467201\n",
      "Epoch 11982/30000 Training Loss: 0.05150417611002922\n",
      "Epoch 11983/30000 Training Loss: 0.04220234602689743\n",
      "Epoch 11984/30000 Training Loss: 0.04843021556735039\n",
      "Epoch 11985/30000 Training Loss: 0.048227690160274506\n",
      "Epoch 11986/30000 Training Loss: 0.042865507304668427\n",
      "Epoch 11987/30000 Training Loss: 0.052649982273578644\n",
      "Epoch 11988/30000 Training Loss: 0.04072272405028343\n",
      "Epoch 11989/30000 Training Loss: 0.04632086306810379\n",
      "Epoch 11990/30000 Training Loss: 0.0477922149002552\n",
      "Epoch 11991/30000 Training Loss: 0.039841704070568085\n",
      "Epoch 11992/30000 Training Loss: 0.05479353666305542\n",
      "Epoch 11993/30000 Training Loss: 0.04697518050670624\n",
      "Epoch 11994/30000 Training Loss: 0.03345918282866478\n",
      "Epoch 11995/30000 Training Loss: 0.04393990710377693\n",
      "Epoch 11996/30000 Training Loss: 0.043597109615802765\n",
      "Epoch 11997/30000 Training Loss: 0.0468996986746788\n",
      "Epoch 11998/30000 Training Loss: 0.04480559006333351\n",
      "Epoch 11999/30000 Training Loss: 0.04747290164232254\n",
      "Epoch 12000/30000 Training Loss: 0.043768055737018585\n",
      "Epoch 12000/30000 Validation Loss: 0.04588925465941429\n",
      "Epoch 12001/30000 Training Loss: 0.04784080386161804\n",
      "Epoch 12002/30000 Training Loss: 0.0448494516313076\n",
      "Epoch 12003/30000 Training Loss: 0.04166949540376663\n",
      "Epoch 12004/30000 Training Loss: 0.045617543160915375\n",
      "Epoch 12005/30000 Training Loss: 0.04617147892713547\n",
      "Epoch 12006/30000 Training Loss: 0.04425928369164467\n",
      "Epoch 12007/30000 Training Loss: 0.06067202612757683\n",
      "Epoch 12008/30000 Training Loss: 0.043224770575761795\n",
      "Epoch 12009/30000 Training Loss: 0.04522480443120003\n",
      "Epoch 12010/30000 Training Loss: 0.045324645936489105\n",
      "Epoch 12011/30000 Training Loss: 0.04271039739251137\n",
      "Epoch 12012/30000 Training Loss: 0.045252881944179535\n",
      "Epoch 12013/30000 Training Loss: 0.0420546717941761\n",
      "Epoch 12014/30000 Training Loss: 0.0433816984295845\n",
      "Epoch 12015/30000 Training Loss: 0.03644207864999771\n",
      "Epoch 12016/30000 Training Loss: 0.05061813443899155\n",
      "Epoch 12017/30000 Training Loss: 0.04874251037836075\n",
      "Epoch 12018/30000 Training Loss: 0.045484982430934906\n",
      "Epoch 12019/30000 Training Loss: 0.05430249497294426\n",
      "Epoch 12020/30000 Training Loss: 0.04278406500816345\n",
      "Epoch 12021/30000 Training Loss: 0.04916180670261383\n",
      "Epoch 12022/30000 Training Loss: 0.04867365211248398\n",
      "Epoch 12023/30000 Training Loss: 0.03848812356591225\n",
      "Epoch 12024/30000 Training Loss: 0.048821933567523956\n",
      "Epoch 12025/30000 Training Loss: 0.04856754094362259\n",
      "Epoch 12026/30000 Training Loss: 0.05366716906428337\n",
      "Epoch 12027/30000 Training Loss: 0.05204587057232857\n",
      "Epoch 12028/30000 Training Loss: 0.04959431663155556\n",
      "Epoch 12029/30000 Training Loss: 0.04728620499372482\n",
      "Epoch 12030/30000 Training Loss: 0.04017411172389984\n",
      "Epoch 12031/30000 Training Loss: 0.04706226661801338\n",
      "Epoch 12032/30000 Training Loss: 0.049778781831264496\n",
      "Epoch 12033/30000 Training Loss: 0.045326557010412216\n",
      "Epoch 12034/30000 Training Loss: 0.04948732629418373\n",
      "Epoch 12035/30000 Training Loss: 0.04542069137096405\n",
      "Epoch 12036/30000 Training Loss: 0.045716606080532074\n",
      "Epoch 12037/30000 Training Loss: 0.04214825853705406\n",
      "Epoch 12038/30000 Training Loss: 0.04199269413948059\n",
      "Epoch 12039/30000 Training Loss: 0.03885222226381302\n",
      "Epoch 12040/30000 Training Loss: 0.0414876863360405\n",
      "Epoch 12041/30000 Training Loss: 0.04265814274549484\n",
      "Epoch 12042/30000 Training Loss: 0.0491543710231781\n",
      "Epoch 12043/30000 Training Loss: 0.04488794878125191\n",
      "Epoch 12044/30000 Training Loss: 0.04875703528523445\n",
      "Epoch 12045/30000 Training Loss: 0.04343472421169281\n",
      "Epoch 12046/30000 Training Loss: 0.0431806817650795\n",
      "Epoch 12047/30000 Training Loss: 0.052751004695892334\n",
      "Epoch 12048/30000 Training Loss: 0.04429949074983597\n",
      "Epoch 12049/30000 Training Loss: 0.05324919894337654\n",
      "Epoch 12050/30000 Training Loss: 0.0401763878762722\n",
      "Epoch 12050/30000 Validation Loss: 0.047872550785541534\n",
      "Epoch 12051/30000 Training Loss: 0.04527977481484413\n",
      "Epoch 12052/30000 Training Loss: 0.05060703679919243\n",
      "Epoch 12053/30000 Training Loss: 0.04377558454871178\n",
      "Epoch 12054/30000 Training Loss: 0.047086961567401886\n",
      "Epoch 12055/30000 Training Loss: 0.04445097967982292\n",
      "Epoch 12056/30000 Training Loss: 0.046267203986644745\n",
      "Epoch 12057/30000 Training Loss: 0.05091257020831108\n",
      "Epoch 12058/30000 Training Loss: 0.044788140803575516\n",
      "Epoch 12059/30000 Training Loss: 0.05563974380493164\n",
      "Epoch 12060/30000 Training Loss: 0.05046050623059273\n",
      "Epoch 12061/30000 Training Loss: 0.0441896989941597\n",
      "Epoch 12062/30000 Training Loss: 0.04746510833501816\n",
      "Epoch 12063/30000 Training Loss: 0.04836631938815117\n",
      "Epoch 12064/30000 Training Loss: 0.0480850413441658\n",
      "Epoch 12065/30000 Training Loss: 0.04636295884847641\n",
      "Epoch 12066/30000 Training Loss: 0.03932371735572815\n",
      "Epoch 12067/30000 Training Loss: 0.045509181916713715\n",
      "Epoch 12068/30000 Training Loss: 0.03892730548977852\n",
      "Epoch 12069/30000 Training Loss: 0.04471762478351593\n",
      "Epoch 12070/30000 Training Loss: 0.03641396015882492\n",
      "Epoch 12071/30000 Training Loss: 0.04840274527668953\n",
      "Epoch 12072/30000 Training Loss: 0.04390474408864975\n",
      "Epoch 12073/30000 Training Loss: 0.04184329882264137\n",
      "Epoch 12074/30000 Training Loss: 0.05029086023569107\n",
      "Epoch 12075/30000 Training Loss: 0.05328155308961868\n",
      "Epoch 12076/30000 Training Loss: 0.044481001794338226\n",
      "Epoch 12077/30000 Training Loss: 0.04191317409276962\n",
      "Epoch 12078/30000 Training Loss: 0.04691024497151375\n",
      "Epoch 12079/30000 Training Loss: 0.048501767218112946\n",
      "Epoch 12080/30000 Training Loss: 0.05061248689889908\n",
      "Epoch 12081/30000 Training Loss: 0.040730345994234085\n",
      "Epoch 12082/30000 Training Loss: 0.03799116611480713\n",
      "Epoch 12083/30000 Training Loss: 0.042456209659576416\n",
      "Epoch 12084/30000 Training Loss: 0.03712238371372223\n",
      "Epoch 12085/30000 Training Loss: 0.046560026705265045\n",
      "Epoch 12086/30000 Training Loss: 0.040109992027282715\n",
      "Epoch 12087/30000 Training Loss: 0.04144422337412834\n",
      "Epoch 12088/30000 Training Loss: 0.05001777410507202\n",
      "Epoch 12089/30000 Training Loss: 0.049603529274463654\n",
      "Epoch 12090/30000 Training Loss: 0.043189339339733124\n",
      "Epoch 12091/30000 Training Loss: 0.05268118530511856\n",
      "Epoch 12092/30000 Training Loss: 0.0478440597653389\n",
      "Epoch 12093/30000 Training Loss: 0.0529443733394146\n",
      "Epoch 12094/30000 Training Loss: 0.051968108862638474\n",
      "Epoch 12095/30000 Training Loss: 0.04346489906311035\n",
      "Epoch 12096/30000 Training Loss: 0.043540988117456436\n",
      "Epoch 12097/30000 Training Loss: 0.04413218796253204\n",
      "Epoch 12098/30000 Training Loss: 0.039248913526535034\n",
      "Epoch 12099/30000 Training Loss: 0.05158324912190437\n",
      "Epoch 12100/30000 Training Loss: 0.04870867729187012\n",
      "Epoch 12100/30000 Validation Loss: 0.05025290325284004\n",
      "Epoch 12101/30000 Training Loss: 0.042389560490846634\n",
      "Epoch 12102/30000 Training Loss: 0.0398080088198185\n",
      "Epoch 12103/30000 Training Loss: 0.05049964040517807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12104/30000 Training Loss: 0.05197244882583618\n",
      "Epoch 12105/30000 Training Loss: 0.04023345559835434\n",
      "Epoch 12106/30000 Training Loss: 0.038770273327827454\n",
      "Epoch 12107/30000 Training Loss: 0.04972466081380844\n",
      "Epoch 12108/30000 Training Loss: 0.04536018520593643\n",
      "Epoch 12109/30000 Training Loss: 0.043175309896469116\n",
      "Epoch 12110/30000 Training Loss: 0.04766053706407547\n",
      "Epoch 12111/30000 Training Loss: 0.04306391626596451\n",
      "Epoch 12112/30000 Training Loss: 0.04120757803320885\n",
      "Epoch 12113/30000 Training Loss: 0.04704843461513519\n",
      "Epoch 12114/30000 Training Loss: 0.04487103968858719\n",
      "Epoch 12115/30000 Training Loss: 0.045680511742830276\n",
      "Epoch 12116/30000 Training Loss: 0.05364032834768295\n",
      "Epoch 12117/30000 Training Loss: 0.03702525794506073\n",
      "Epoch 12118/30000 Training Loss: 0.04953940585255623\n",
      "Epoch 12119/30000 Training Loss: 0.049113571643829346\n",
      "Epoch 12120/30000 Training Loss: 0.052499283105134964\n",
      "Epoch 12121/30000 Training Loss: 0.03330673277378082\n",
      "Epoch 12122/30000 Training Loss: 0.05418366938829422\n",
      "Epoch 12123/30000 Training Loss: 0.043974507600069046\n",
      "Epoch 12124/30000 Training Loss: 0.04699908569455147\n",
      "Epoch 12125/30000 Training Loss: 0.044992655515670776\n",
      "Epoch 12126/30000 Training Loss: 0.04646486043930054\n",
      "Epoch 12127/30000 Training Loss: 0.04259232059121132\n",
      "Epoch 12128/30000 Training Loss: 0.04283839836716652\n",
      "Epoch 12129/30000 Training Loss: 0.05440864711999893\n",
      "Epoch 12130/30000 Training Loss: 0.042758509516716\n",
      "Epoch 12131/30000 Training Loss: 0.04442451521754265\n",
      "Epoch 12132/30000 Training Loss: 0.04592268913984299\n",
      "Epoch 12133/30000 Training Loss: 0.04182525724172592\n",
      "Epoch 12134/30000 Training Loss: 0.04477620869874954\n",
      "Epoch 12135/30000 Training Loss: 0.048640765249729156\n",
      "Epoch 12136/30000 Training Loss: 0.04950732737779617\n",
      "Epoch 12137/30000 Training Loss: 0.047162462025880814\n",
      "Epoch 12138/30000 Training Loss: 0.034964870661497116\n",
      "Epoch 12139/30000 Training Loss: 0.037652235478162766\n",
      "Epoch 12140/30000 Training Loss: 0.04623894765973091\n",
      "Epoch 12141/30000 Training Loss: 0.044323425740003586\n",
      "Epoch 12142/30000 Training Loss: 0.046214573085308075\n",
      "Epoch 12143/30000 Training Loss: 0.04591299965977669\n",
      "Epoch 12144/30000 Training Loss: 0.04283282160758972\n",
      "Epoch 12145/30000 Training Loss: 0.042059559375047684\n",
      "Epoch 12146/30000 Training Loss: 0.05269438773393631\n",
      "Epoch 12147/30000 Training Loss: 0.04382852092385292\n",
      "Epoch 12148/30000 Training Loss: 0.04965851455926895\n",
      "Epoch 12149/30000 Training Loss: 0.04071815684437752\n",
      "Epoch 12150/30000 Training Loss: 0.052967220544815063\n",
      "Epoch 12150/30000 Validation Loss: 0.05301393195986748\n",
      "Epoch 12151/30000 Training Loss: 0.04119493067264557\n",
      "Epoch 12152/30000 Training Loss: 0.042562417685985565\n",
      "Epoch 12153/30000 Training Loss: 0.0476364865899086\n",
      "Epoch 12154/30000 Training Loss: 0.04486939311027527\n",
      "Epoch 12155/30000 Training Loss: 0.04420630261301994\n",
      "Epoch 12156/30000 Training Loss: 0.043915726244449615\n",
      "Epoch 12157/30000 Training Loss: 0.04952085018157959\n",
      "Epoch 12158/30000 Training Loss: 0.041688285768032074\n",
      "Epoch 12159/30000 Training Loss: 0.05094107985496521\n",
      "Epoch 12160/30000 Training Loss: 0.04666329175233841\n",
      "Epoch 12161/30000 Training Loss: 0.04620542749762535\n",
      "Epoch 12162/30000 Training Loss: 0.051387570798397064\n",
      "Epoch 12163/30000 Training Loss: 0.04743879660964012\n",
      "Epoch 12164/30000 Training Loss: 0.041674014180898666\n",
      "Epoch 12165/30000 Training Loss: 0.037988025695085526\n",
      "Epoch 12166/30000 Training Loss: 0.048133403062820435\n",
      "Epoch 12167/30000 Training Loss: 0.051338303834199905\n",
      "Epoch 12168/30000 Training Loss: 0.05022772029042244\n",
      "Epoch 12169/30000 Training Loss: 0.03610304370522499\n",
      "Epoch 12170/30000 Training Loss: 0.03634360805153847\n",
      "Epoch 12171/30000 Training Loss: 0.03544723242521286\n",
      "Epoch 12172/30000 Training Loss: 0.056380219757556915\n",
      "Epoch 12173/30000 Training Loss: 0.04691066965460777\n",
      "Epoch 12174/30000 Training Loss: 0.046028632670640945\n",
      "Epoch 12175/30000 Training Loss: 0.04234236478805542\n",
      "Epoch 12176/30000 Training Loss: 0.04345666989684105\n",
      "Epoch 12177/30000 Training Loss: 0.03537123650312424\n",
      "Epoch 12178/30000 Training Loss: 0.047960229218006134\n",
      "Epoch 12179/30000 Training Loss: 0.05224546790122986\n",
      "Epoch 12180/30000 Training Loss: 0.05240442603826523\n",
      "Epoch 12181/30000 Training Loss: 0.04262919723987579\n",
      "Epoch 12182/30000 Training Loss: 0.0449773445725441\n",
      "Epoch 12183/30000 Training Loss: 0.04516128823161125\n",
      "Epoch 12184/30000 Training Loss: 0.04126700013875961\n",
      "Epoch 12185/30000 Training Loss: 0.044845420867204666\n",
      "Epoch 12186/30000 Training Loss: 0.04312434792518616\n",
      "Epoch 12187/30000 Training Loss: 0.03930606693029404\n",
      "Epoch 12188/30000 Training Loss: 0.038017548620700836\n",
      "Epoch 12189/30000 Training Loss: 0.05479062348604202\n",
      "Epoch 12190/30000 Training Loss: 0.045835163444280624\n",
      "Epoch 12191/30000 Training Loss: 0.04099871963262558\n",
      "Epoch 12192/30000 Training Loss: 0.03466358408331871\n",
      "Epoch 12193/30000 Training Loss: 0.04238350689411163\n",
      "Epoch 12194/30000 Training Loss: 0.0455533005297184\n",
      "Epoch 12195/30000 Training Loss: 0.037781406193971634\n",
      "Epoch 12196/30000 Training Loss: 0.04622186720371246\n",
      "Epoch 12197/30000 Training Loss: 0.0466577373445034\n",
      "Epoch 12198/30000 Training Loss: 0.048421137034893036\n",
      "Epoch 12199/30000 Training Loss: 0.035868071019649506\n",
      "Epoch 12200/30000 Training Loss: 0.045755427330732346\n",
      "Epoch 12200/30000 Validation Loss: 0.04663988575339317\n",
      "Epoch 12201/30000 Training Loss: 0.04191097617149353\n",
      "Epoch 12202/30000 Training Loss: 0.04056432843208313\n",
      "Epoch 12203/30000 Training Loss: 0.04087638109922409\n",
      "Epoch 12204/30000 Training Loss: 0.04746665060520172\n",
      "Epoch 12205/30000 Training Loss: 0.04619293287396431\n",
      "Epoch 12206/30000 Training Loss: 0.047956932336091995\n",
      "Epoch 12207/30000 Training Loss: 0.05162690207362175\n",
      "Epoch 12208/30000 Training Loss: 0.049580905586481094\n",
      "Epoch 12209/30000 Training Loss: 0.04328247159719467\n",
      "Epoch 12210/30000 Training Loss: 0.04714693874120712\n",
      "Epoch 12211/30000 Training Loss: 0.04158759117126465\n",
      "Epoch 12212/30000 Training Loss: 0.03568567335605621\n",
      "Epoch 12213/30000 Training Loss: 0.045728009194135666\n",
      "Epoch 12214/30000 Training Loss: 0.048229243606328964\n",
      "Epoch 12215/30000 Training Loss: 0.048657722771167755\n",
      "Epoch 12216/30000 Training Loss: 0.042475052177906036\n",
      "Epoch 12217/30000 Training Loss: 0.046013250946998596\n",
      "Epoch 12218/30000 Training Loss: 0.05354154109954834\n",
      "Epoch 12219/30000 Training Loss: 0.05565320700407028\n",
      "Epoch 12220/30000 Training Loss: 0.038811929523944855\n",
      "Epoch 12221/30000 Training Loss: 0.04536120966076851\n",
      "Epoch 12222/30000 Training Loss: 0.043616265058517456\n",
      "Epoch 12223/30000 Training Loss: 0.04541793465614319\n",
      "Epoch 12224/30000 Training Loss: 0.04595603793859482\n",
      "Epoch 12225/30000 Training Loss: 0.04101213067770004\n",
      "Epoch 12226/30000 Training Loss: 0.044108130037784576\n",
      "Epoch 12227/30000 Training Loss: 0.044179465621709824\n",
      "Epoch 12228/30000 Training Loss: 0.04259491339325905\n",
      "Epoch 12229/30000 Training Loss: 0.043171703815460205\n",
      "Epoch 12230/30000 Training Loss: 0.042259033769369125\n",
      "Epoch 12231/30000 Training Loss: 0.041001807898283005\n",
      "Epoch 12232/30000 Training Loss: 0.0497414730489254\n",
      "Epoch 12233/30000 Training Loss: 0.0496809296309948\n",
      "Epoch 12234/30000 Training Loss: 0.04163959249854088\n",
      "Epoch 12235/30000 Training Loss: 0.0415627583861351\n",
      "Epoch 12236/30000 Training Loss: 0.04479265585541725\n",
      "Epoch 12237/30000 Training Loss: 0.050285935401916504\n",
      "Epoch 12238/30000 Training Loss: 0.04017223045229912\n",
      "Epoch 12239/30000 Training Loss: 0.04641915112733841\n",
      "Epoch 12240/30000 Training Loss: 0.04299681633710861\n",
      "Epoch 12241/30000 Training Loss: 0.03873594105243683\n",
      "Epoch 12242/30000 Training Loss: 0.04732128605246544\n",
      "Epoch 12243/30000 Training Loss: 0.048065777868032455\n",
      "Epoch 12244/30000 Training Loss: 0.0512053444981575\n",
      "Epoch 12245/30000 Training Loss: 0.04408496990799904\n",
      "Epoch 12246/30000 Training Loss: 0.04329444840550423\n",
      "Epoch 12247/30000 Training Loss: 0.050514329224824905\n",
      "Epoch 12248/30000 Training Loss: 0.043034203350543976\n",
      "Epoch 12249/30000 Training Loss: 0.040327269583940506\n",
      "Epoch 12250/30000 Training Loss: 0.04284396395087242\n",
      "Epoch 12250/30000 Validation Loss: 0.04288319870829582\n",
      "Epoch 12251/30000 Training Loss: 0.04147564247250557\n",
      "Epoch 12252/30000 Training Loss: 0.04390817508101463\n",
      "Epoch 12253/30000 Training Loss: 0.046412430703639984\n",
      "Epoch 12254/30000 Training Loss: 0.046371422708034515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12255/30000 Training Loss: 0.039097100496292114\n",
      "Epoch 12256/30000 Training Loss: 0.04399961978197098\n",
      "Epoch 12257/30000 Training Loss: 0.051424961537122726\n",
      "Epoch 12258/30000 Training Loss: 0.03877680376172066\n",
      "Epoch 12259/30000 Training Loss: 0.03543367609381676\n",
      "Epoch 12260/30000 Training Loss: 0.04663240164518356\n",
      "Epoch 12261/30000 Training Loss: 0.04156871885061264\n",
      "Epoch 12262/30000 Training Loss: 0.05196060985326767\n",
      "Epoch 12263/30000 Training Loss: 0.043714072555303574\n",
      "Epoch 12264/30000 Training Loss: 0.050240516662597656\n",
      "Epoch 12265/30000 Training Loss: 0.04795918986201286\n",
      "Epoch 12266/30000 Training Loss: 0.04404370114207268\n",
      "Epoch 12267/30000 Training Loss: 0.04784325882792473\n",
      "Epoch 12268/30000 Training Loss: 0.04620136320590973\n",
      "Epoch 12269/30000 Training Loss: 0.045383501797914505\n",
      "Epoch 12270/30000 Training Loss: 0.05013111233711243\n",
      "Epoch 12271/30000 Training Loss: 0.043447528034448624\n",
      "Epoch 12272/30000 Training Loss: 0.04404943063855171\n",
      "Epoch 12273/30000 Training Loss: 0.05598152428865433\n",
      "Epoch 12274/30000 Training Loss: 0.046762872487306595\n",
      "Epoch 12275/30000 Training Loss: 0.040831729769706726\n",
      "Epoch 12276/30000 Training Loss: 0.050092171877622604\n",
      "Epoch 12277/30000 Training Loss: 0.05793233588337898\n",
      "Epoch 12278/30000 Training Loss: 0.04935591667890549\n",
      "Epoch 12279/30000 Training Loss: 0.04683868587017059\n",
      "Epoch 12280/30000 Training Loss: 0.04489342123270035\n",
      "Epoch 12281/30000 Training Loss: 0.04798167198896408\n",
      "Epoch 12282/30000 Training Loss: 0.050429053604602814\n",
      "Epoch 12283/30000 Training Loss: 0.04430488124489784\n",
      "Epoch 12284/30000 Training Loss: 0.04886703938245773\n",
      "Epoch 12285/30000 Training Loss: 0.047012075781822205\n",
      "Epoch 12286/30000 Training Loss: 0.05300644040107727\n",
      "Epoch 12287/30000 Training Loss: 0.050381530076265335\n",
      "Epoch 12288/30000 Training Loss: 0.04797118157148361\n",
      "Epoch 12289/30000 Training Loss: 0.053510136902332306\n",
      "Epoch 12290/30000 Training Loss: 0.04825716093182564\n",
      "Epoch 12291/30000 Training Loss: 0.045380815863609314\n",
      "Epoch 12292/30000 Training Loss: 0.0496399886906147\n",
      "Epoch 12293/30000 Training Loss: 0.04099971801042557\n",
      "Epoch 12294/30000 Training Loss: 0.048215415328741074\n",
      "Epoch 12295/30000 Training Loss: 0.047891680151224136\n",
      "Epoch 12296/30000 Training Loss: 0.05243496969342232\n",
      "Epoch 12297/30000 Training Loss: 0.045913998037576675\n",
      "Epoch 12298/30000 Training Loss: 0.046189047396183014\n",
      "Epoch 12299/30000 Training Loss: 0.04459512233734131\n",
      "Epoch 12300/30000 Training Loss: 0.05153794214129448\n",
      "Epoch 12300/30000 Validation Loss: 0.0453328937292099\n",
      "Epoch 12301/30000 Training Loss: 0.03963497653603554\n",
      "Epoch 12302/30000 Training Loss: 0.04567835107445717\n",
      "Epoch 12303/30000 Training Loss: 0.04280511662364006\n",
      "Epoch 12304/30000 Training Loss: 0.049593470990657806\n",
      "Epoch 12305/30000 Training Loss: 0.045303333550691605\n",
      "Epoch 12306/30000 Training Loss: 0.04913612827658653\n",
      "Epoch 12307/30000 Training Loss: 0.046113111078739166\n",
      "Epoch 12308/30000 Training Loss: 0.04063813015818596\n",
      "Epoch 12309/30000 Training Loss: 0.044374193996191025\n",
      "Epoch 12310/30000 Training Loss: 0.03978968411684036\n",
      "Epoch 12311/30000 Training Loss: 0.048474520444869995\n",
      "Epoch 12312/30000 Training Loss: 0.04865650087594986\n",
      "Epoch 12313/30000 Training Loss: 0.04936845600605011\n",
      "Epoch 12314/30000 Training Loss: 0.043081630021333694\n",
      "Epoch 12315/30000 Training Loss: 0.03726528212428093\n",
      "Epoch 12316/30000 Training Loss: 0.05302789807319641\n",
      "Epoch 12317/30000 Training Loss: 0.04469231888651848\n",
      "Epoch 12318/30000 Training Loss: 0.04328496754169464\n",
      "Epoch 12319/30000 Training Loss: 0.044320132583379745\n",
      "Epoch 12320/30000 Training Loss: 0.04584144428372383\n",
      "Epoch 12321/30000 Training Loss: 0.04496636241674423\n",
      "Epoch 12322/30000 Training Loss: 0.04754659906029701\n",
      "Epoch 12323/30000 Training Loss: 0.040291160345077515\n",
      "Epoch 12324/30000 Training Loss: 0.039773836731910706\n",
      "Epoch 12325/30000 Training Loss: 0.03982321918010712\n",
      "Epoch 12326/30000 Training Loss: 0.04687854275107384\n",
      "Epoch 12327/30000 Training Loss: 0.03906223922967911\n",
      "Epoch 12328/30000 Training Loss: 0.04657282307744026\n",
      "Epoch 12329/30000 Training Loss: 0.042370885610580444\n",
      "Epoch 12330/30000 Training Loss: 0.045753028243780136\n",
      "Epoch 12331/30000 Training Loss: 0.043763257563114166\n",
      "Epoch 12332/30000 Training Loss: 0.0389711931347847\n",
      "Epoch 12333/30000 Training Loss: 0.0441943034529686\n",
      "Epoch 12334/30000 Training Loss: 0.04638950526714325\n",
      "Epoch 12335/30000 Training Loss: 0.05026518180966377\n",
      "Epoch 12336/30000 Training Loss: 0.03857652097940445\n",
      "Epoch 12337/30000 Training Loss: 0.04385456442832947\n",
      "Epoch 12338/30000 Training Loss: 0.04284420609474182\n",
      "Epoch 12339/30000 Training Loss: 0.043955009430646896\n",
      "Epoch 12340/30000 Training Loss: 0.04470538720488548\n",
      "Epoch 12341/30000 Training Loss: 0.04276811704039574\n",
      "Epoch 12342/30000 Training Loss: 0.05528577044606209\n",
      "Epoch 12343/30000 Training Loss: 0.04545222967863083\n",
      "Epoch 12344/30000 Training Loss: 0.048842355608940125\n",
      "Epoch 12345/30000 Training Loss: 0.037187620997428894\n",
      "Epoch 12346/30000 Training Loss: 0.03881468623876572\n",
      "Epoch 12347/30000 Training Loss: 0.04383108764886856\n",
      "Epoch 12348/30000 Training Loss: 0.046114929020404816\n",
      "Epoch 12349/30000 Training Loss: 0.04454163834452629\n",
      "Epoch 12350/30000 Training Loss: 0.04009341821074486\n",
      "Epoch 12350/30000 Validation Loss: 0.0465502068400383\n",
      "Epoch 12351/30000 Training Loss: 0.04964574798941612\n",
      "Epoch 12352/30000 Training Loss: 0.05255310982465744\n",
      "Epoch 12353/30000 Training Loss: 0.04625996947288513\n",
      "Epoch 12354/30000 Training Loss: 0.0365765281021595\n",
      "Epoch 12355/30000 Training Loss: 0.04219713807106018\n",
      "Epoch 12356/30000 Training Loss: 0.046441372483968735\n",
      "Epoch 12357/30000 Training Loss: 0.04546346887946129\n",
      "Epoch 12358/30000 Training Loss: 0.047682520002126694\n",
      "Epoch 12359/30000 Training Loss: 0.050031550228595734\n",
      "Epoch 12360/30000 Training Loss: 0.049270160496234894\n",
      "Epoch 12361/30000 Training Loss: 0.04745855554938316\n",
      "Epoch 12362/30000 Training Loss: 0.03921714052557945\n",
      "Epoch 12363/30000 Training Loss: 0.04531405121088028\n",
      "Epoch 12364/30000 Training Loss: 0.04548151418566704\n",
      "Epoch 12365/30000 Training Loss: 0.042908791452646255\n",
      "Epoch 12366/30000 Training Loss: 0.043290846049785614\n",
      "Epoch 12367/30000 Training Loss: 0.048991911113262177\n",
      "Epoch 12368/30000 Training Loss: 0.04572327062487602\n",
      "Epoch 12369/30000 Training Loss: 0.04135947301983833\n",
      "Epoch 12370/30000 Training Loss: 0.050076086074113846\n",
      "Epoch 12371/30000 Training Loss: 0.04530080407857895\n",
      "Epoch 12372/30000 Training Loss: 0.05454857274889946\n",
      "Epoch 12373/30000 Training Loss: 0.04716082662343979\n",
      "Epoch 12374/30000 Training Loss: 0.04350375384092331\n",
      "Epoch 12375/30000 Training Loss: 0.044216081500053406\n",
      "Epoch 12376/30000 Training Loss: 0.0497976653277874\n",
      "Epoch 12377/30000 Training Loss: 0.04177253320813179\n",
      "Epoch 12378/30000 Training Loss: 0.053405504673719406\n",
      "Epoch 12379/30000 Training Loss: 0.046627771109342575\n",
      "Epoch 12380/30000 Training Loss: 0.04004046320915222\n",
      "Epoch 12381/30000 Training Loss: 0.050286877900362015\n",
      "Epoch 12382/30000 Training Loss: 0.05183899402618408\n",
      "Epoch 12383/30000 Training Loss: 0.041872989386320114\n",
      "Epoch 12384/30000 Training Loss: 0.04211995378136635\n",
      "Epoch 12385/30000 Training Loss: 0.04611246660351753\n",
      "Epoch 12386/30000 Training Loss: 0.050880152732133865\n",
      "Epoch 12387/30000 Training Loss: 0.049604177474975586\n",
      "Epoch 12388/30000 Training Loss: 0.045668717473745346\n",
      "Epoch 12389/30000 Training Loss: 0.04124258831143379\n",
      "Epoch 12390/30000 Training Loss: 0.04814302176237106\n",
      "Epoch 12391/30000 Training Loss: 0.047027915716171265\n",
      "Epoch 12392/30000 Training Loss: 0.04782188683748245\n",
      "Epoch 12393/30000 Training Loss: 0.03390722721815109\n",
      "Epoch 12394/30000 Training Loss: 0.046959664672613144\n",
      "Epoch 12395/30000 Training Loss: 0.04316980391740799\n",
      "Epoch 12396/30000 Training Loss: 0.044321365654468536\n",
      "Epoch 12397/30000 Training Loss: 0.04471354931592941\n",
      "Epoch 12398/30000 Training Loss: 0.049439795315265656\n",
      "Epoch 12399/30000 Training Loss: 0.047219280153512955\n",
      "Epoch 12400/30000 Training Loss: 0.05180252343416214\n",
      "Epoch 12400/30000 Validation Loss: 0.04406162351369858\n",
      "Epoch 12401/30000 Training Loss: 0.0451996885240078\n",
      "Epoch 12402/30000 Training Loss: 0.04443972557783127\n",
      "Epoch 12403/30000 Training Loss: 0.04393249377608299\n",
      "Epoch 12404/30000 Training Loss: 0.04344714432954788\n",
      "Epoch 12405/30000 Training Loss: 0.055230945348739624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12406/30000 Training Loss: 0.03785928338766098\n",
      "Epoch 12407/30000 Training Loss: 0.04399077966809273\n",
      "Epoch 12408/30000 Training Loss: 0.048888519406318665\n",
      "Epoch 12409/30000 Training Loss: 0.04576320946216583\n",
      "Epoch 12410/30000 Training Loss: 0.0396149680018425\n",
      "Epoch 12411/30000 Training Loss: 0.04107830673456192\n",
      "Epoch 12412/30000 Training Loss: 0.04570751637220383\n",
      "Epoch 12413/30000 Training Loss: 0.05013541504740715\n",
      "Epoch 12414/30000 Training Loss: 0.04313147813081741\n",
      "Epoch 12415/30000 Training Loss: 0.04477304220199585\n",
      "Epoch 12416/30000 Training Loss: 0.05092408508062363\n",
      "Epoch 12417/30000 Training Loss: 0.046328991651535034\n",
      "Epoch 12418/30000 Training Loss: 0.039585720747709274\n",
      "Epoch 12419/30000 Training Loss: 0.0421411469578743\n",
      "Epoch 12420/30000 Training Loss: 0.0469447560608387\n",
      "Epoch 12421/30000 Training Loss: 0.04257829114794731\n",
      "Epoch 12422/30000 Training Loss: 0.04466806352138519\n",
      "Epoch 12423/30000 Training Loss: 0.03576959669589996\n",
      "Epoch 12424/30000 Training Loss: 0.03775191679596901\n",
      "Epoch 12425/30000 Training Loss: 0.04074222594499588\n",
      "Epoch 12426/30000 Training Loss: 0.04078339785337448\n",
      "Epoch 12427/30000 Training Loss: 0.04767095297574997\n",
      "Epoch 12428/30000 Training Loss: 0.05240548774600029\n",
      "Epoch 12429/30000 Training Loss: 0.040771692991256714\n",
      "Epoch 12430/30000 Training Loss: 0.046277135610580444\n",
      "Epoch 12431/30000 Training Loss: 0.04656762629747391\n",
      "Epoch 12432/30000 Training Loss: 0.04425794258713722\n",
      "Epoch 12433/30000 Training Loss: 0.04380594193935394\n",
      "Epoch 12434/30000 Training Loss: 0.04239092394709587\n",
      "Epoch 12435/30000 Training Loss: 0.04543973505496979\n",
      "Epoch 12436/30000 Training Loss: 0.04547581821680069\n",
      "Epoch 12437/30000 Training Loss: 0.040186312049627304\n",
      "Epoch 12438/30000 Training Loss: 0.04872669279575348\n",
      "Epoch 12439/30000 Training Loss: 0.041063226759433746\n",
      "Epoch 12440/30000 Training Loss: 0.04360124468803406\n",
      "Epoch 12441/30000 Training Loss: 0.05437425896525383\n",
      "Epoch 12442/30000 Training Loss: 0.04694918170571327\n",
      "Epoch 12443/30000 Training Loss: 0.044966451823711395\n",
      "Epoch 12444/30000 Training Loss: 0.04301071912050247\n",
      "Epoch 12445/30000 Training Loss: 0.042852748185396194\n",
      "Epoch 12446/30000 Training Loss: 0.04322994500398636\n",
      "Epoch 12447/30000 Training Loss: 0.05103088542819023\n",
      "Epoch 12448/30000 Training Loss: 0.05154041200876236\n",
      "Epoch 12449/30000 Training Loss: 0.050753213465213776\n",
      "Epoch 12450/30000 Training Loss: 0.04810188710689545\n",
      "Epoch 12450/30000 Validation Loss: 0.05296187475323677\n",
      "Epoch 12451/30000 Training Loss: 0.041893213987350464\n",
      "Epoch 12452/30000 Training Loss: 0.05235857889056206\n",
      "Epoch 12453/30000 Training Loss: 0.05212260037660599\n",
      "Epoch 12454/30000 Training Loss: 0.04926682263612747\n",
      "Epoch 12455/30000 Training Loss: 0.05012007802724838\n",
      "Epoch 12456/30000 Training Loss: 0.04344841092824936\n",
      "Epoch 12457/30000 Training Loss: 0.0455605685710907\n",
      "Epoch 12458/30000 Training Loss: 0.048908744007349014\n",
      "Epoch 12459/30000 Training Loss: 0.043857917189598083\n",
      "Epoch 12460/30000 Training Loss: 0.043497372418642044\n",
      "Epoch 12461/30000 Training Loss: 0.05545545741915703\n",
      "Epoch 12462/30000 Training Loss: 0.04847240447998047\n",
      "Epoch 12463/30000 Training Loss: 0.04498191550374031\n",
      "Epoch 12464/30000 Training Loss: 0.04861075431108475\n",
      "Epoch 12465/30000 Training Loss: 0.04694264754652977\n",
      "Epoch 12466/30000 Training Loss: 0.039666347205638885\n",
      "Epoch 12467/30000 Training Loss: 0.04708583280444145\n",
      "Epoch 12468/30000 Training Loss: 0.04914344102144241\n",
      "Epoch 12469/30000 Training Loss: 0.04344883933663368\n",
      "Epoch 12470/30000 Training Loss: 0.052264612168073654\n",
      "Epoch 12471/30000 Training Loss: 0.04488937184214592\n",
      "Epoch 12472/30000 Training Loss: 0.0443003810942173\n",
      "Epoch 12473/30000 Training Loss: 0.041274093091487885\n",
      "Epoch 12474/30000 Training Loss: 0.04507917910814285\n",
      "Epoch 12475/30000 Training Loss: 0.04309782758355141\n",
      "Epoch 12476/30000 Training Loss: 0.046299584209918976\n",
      "Epoch 12477/30000 Training Loss: 0.04583928734064102\n",
      "Epoch 12478/30000 Training Loss: 0.04160011559724808\n",
      "Epoch 12479/30000 Training Loss: 0.041134439408779144\n",
      "Epoch 12480/30000 Training Loss: 0.05017127841711044\n",
      "Epoch 12481/30000 Training Loss: 0.04731303080916405\n",
      "Epoch 12482/30000 Training Loss: 0.05063563585281372\n",
      "Epoch 12483/30000 Training Loss: 0.04713698476552963\n",
      "Epoch 12484/30000 Training Loss: 0.043444495648145676\n",
      "Epoch 12485/30000 Training Loss: 0.04715209826827049\n",
      "Epoch 12486/30000 Training Loss: 0.053834252059459686\n",
      "Epoch 12487/30000 Training Loss: 0.04550216346979141\n",
      "Epoch 12488/30000 Training Loss: 0.043866705149412155\n",
      "Epoch 12489/30000 Training Loss: 0.03655463829636574\n",
      "Epoch 12490/30000 Training Loss: 0.04335365444421768\n",
      "Epoch 12491/30000 Training Loss: 0.039854444563388824\n",
      "Epoch 12492/30000 Training Loss: 0.046267639845609665\n",
      "Epoch 12493/30000 Training Loss: 0.03935401141643524\n",
      "Epoch 12494/30000 Training Loss: 0.04333068057894707\n",
      "Epoch 12495/30000 Training Loss: 0.055141352117061615\n",
      "Epoch 12496/30000 Training Loss: 0.045608557760715485\n",
      "Epoch 12497/30000 Training Loss: 0.042128220200538635\n",
      "Epoch 12498/30000 Training Loss: 0.048786990344524384\n",
      "Epoch 12499/30000 Training Loss: 0.044162776321172714\n",
      "Epoch 12500/30000 Training Loss: 0.035158924758434296\n",
      "Epoch 12500/30000 Validation Loss: 0.04192488268017769\n",
      "Epoch 12501/30000 Training Loss: 0.042718011885881424\n",
      "Epoch 12502/30000 Training Loss: 0.04930691048502922\n",
      "Epoch 12503/30000 Training Loss: 0.04846067726612091\n",
      "Epoch 12504/30000 Training Loss: 0.04526349529623985\n",
      "Epoch 12505/30000 Training Loss: 0.0393279567360878\n",
      "Epoch 12506/30000 Training Loss: 0.04573323577642441\n",
      "Epoch 12507/30000 Training Loss: 0.03859350457787514\n",
      "Epoch 12508/30000 Training Loss: 0.041457705199718475\n",
      "Epoch 12509/30000 Training Loss: 0.04234788939356804\n",
      "Epoch 12510/30000 Training Loss: 0.040663160383701324\n",
      "Epoch 12511/30000 Training Loss: 0.03631676733493805\n",
      "Epoch 12512/30000 Training Loss: 0.038876134902238846\n",
      "Epoch 12513/30000 Training Loss: 0.0497124120593071\n",
      "Epoch 12514/30000 Training Loss: 0.040658168494701385\n",
      "Epoch 12515/30000 Training Loss: 0.042928826063871384\n",
      "Epoch 12516/30000 Training Loss: 0.04151468724012375\n",
      "Epoch 12517/30000 Training Loss: 0.037617191672325134\n",
      "Epoch 12518/30000 Training Loss: 0.04774676635861397\n",
      "Epoch 12519/30000 Training Loss: 0.049303069710731506\n",
      "Epoch 12520/30000 Training Loss: 0.05833570286631584\n",
      "Epoch 12521/30000 Training Loss: 0.055411696434020996\n",
      "Epoch 12522/30000 Training Loss: 0.044712625443935394\n",
      "Epoch 12523/30000 Training Loss: 0.04804491624236107\n",
      "Epoch 12524/30000 Training Loss: 0.04945167526602745\n",
      "Epoch 12525/30000 Training Loss: 0.04204316437244415\n",
      "Epoch 12526/30000 Training Loss: 0.04351280257105827\n",
      "Epoch 12527/30000 Training Loss: 0.05271322280168533\n",
      "Epoch 12528/30000 Training Loss: 0.040707044303417206\n",
      "Epoch 12529/30000 Training Loss: 0.04566127434372902\n",
      "Epoch 12530/30000 Training Loss: 0.04736557975411415\n",
      "Epoch 12531/30000 Training Loss: 0.046428829431533813\n",
      "Epoch 12532/30000 Training Loss: 0.04277912154793739\n",
      "Epoch 12533/30000 Training Loss: 0.05138615518808365\n",
      "Epoch 12534/30000 Training Loss: 0.04086057096719742\n",
      "Epoch 12535/30000 Training Loss: 0.048222217708826065\n",
      "Epoch 12536/30000 Training Loss: 0.04791263863444328\n",
      "Epoch 12537/30000 Training Loss: 0.050418950617313385\n",
      "Epoch 12538/30000 Training Loss: 0.053757309913635254\n",
      "Epoch 12539/30000 Training Loss: 0.04764789342880249\n",
      "Epoch 12540/30000 Training Loss: 0.04240872338414192\n",
      "Epoch 12541/30000 Training Loss: 0.05078389495611191\n",
      "Epoch 12542/30000 Training Loss: 0.04651182144880295\n",
      "Epoch 12543/30000 Training Loss: 0.04367033392190933\n",
      "Epoch 12544/30000 Training Loss: 0.04987802356481552\n",
      "Epoch 12545/30000 Training Loss: 0.04314032942056656\n",
      "Epoch 12546/30000 Training Loss: 0.04493198171257973\n",
      "Epoch 12547/30000 Training Loss: 0.039505332708358765\n",
      "Epoch 12548/30000 Training Loss: 0.04800230264663696\n",
      "Epoch 12549/30000 Training Loss: 0.05299266427755356\n",
      "Epoch 12550/30000 Training Loss: 0.04269682615995407\n",
      "Epoch 12550/30000 Validation Loss: 0.04556044191122055\n",
      "Epoch 12551/30000 Training Loss: 0.04404274746775627\n",
      "Epoch 12552/30000 Training Loss: 0.045609984546899796\n",
      "Epoch 12553/30000 Training Loss: 0.043100327253341675\n",
      "Epoch 12554/30000 Training Loss: 0.043485112488269806\n",
      "Epoch 12555/30000 Training Loss: 0.04522698372602463\n",
      "Epoch 12556/30000 Training Loss: 0.04480414837598801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12557/30000 Training Loss: 0.05149991437792778\n",
      "Epoch 12558/30000 Training Loss: 0.04183244705200195\n",
      "Epoch 12559/30000 Training Loss: 0.04349494352936745\n",
      "Epoch 12560/30000 Training Loss: 0.047092922031879425\n",
      "Epoch 12561/30000 Training Loss: 0.04498899355530739\n",
      "Epoch 12562/30000 Training Loss: 0.05005604773759842\n",
      "Epoch 12563/30000 Training Loss: 0.04383428022265434\n",
      "Epoch 12564/30000 Training Loss: 0.03962321951985359\n",
      "Epoch 12565/30000 Training Loss: 0.04958309978246689\n",
      "Epoch 12566/30000 Training Loss: 0.0538281612098217\n",
      "Epoch 12567/30000 Training Loss: 0.04651372507214546\n",
      "Epoch 12568/30000 Training Loss: 0.04532909020781517\n",
      "Epoch 12569/30000 Training Loss: 0.04174356907606125\n",
      "Epoch 12570/30000 Training Loss: 0.04677475243806839\n",
      "Epoch 12571/30000 Training Loss: 0.061416178941726685\n",
      "Epoch 12572/30000 Training Loss: 0.049196019768714905\n",
      "Epoch 12573/30000 Training Loss: 0.0456165075302124\n",
      "Epoch 12574/30000 Training Loss: 0.04063796252012253\n",
      "Epoch 12575/30000 Training Loss: 0.04268687963485718\n",
      "Epoch 12576/30000 Training Loss: 0.0460793562233448\n",
      "Epoch 12577/30000 Training Loss: 0.05170558765530586\n",
      "Epoch 12578/30000 Training Loss: 0.04814502224326134\n",
      "Epoch 12579/30000 Training Loss: 0.03687237948179245\n",
      "Epoch 12580/30000 Training Loss: 0.04070144146680832\n",
      "Epoch 12581/30000 Training Loss: 0.041947536170482635\n",
      "Epoch 12582/30000 Training Loss: 0.04637349396944046\n",
      "Epoch 12583/30000 Training Loss: 0.046198271214962006\n",
      "Epoch 12584/30000 Training Loss: 0.03846510872244835\n",
      "Epoch 12585/30000 Training Loss: 0.05112294480204582\n",
      "Epoch 12586/30000 Training Loss: 0.050413619726896286\n",
      "Epoch 12587/30000 Training Loss: 0.037663042545318604\n",
      "Epoch 12588/30000 Training Loss: 0.03910728916525841\n",
      "Epoch 12589/30000 Training Loss: 0.03871838375926018\n",
      "Epoch 12590/30000 Training Loss: 0.038107823580503464\n",
      "Epoch 12591/30000 Training Loss: 0.05049293115735054\n",
      "Epoch 12592/30000 Training Loss: 0.04351536184549332\n",
      "Epoch 12593/30000 Training Loss: 0.038227833807468414\n",
      "Epoch 12594/30000 Training Loss: 0.04532240703701973\n",
      "Epoch 12595/30000 Training Loss: 0.04599689692258835\n",
      "Epoch 12596/30000 Training Loss: 0.05599796026945114\n",
      "Epoch 12597/30000 Training Loss: 0.04104709252715111\n",
      "Epoch 12598/30000 Training Loss: 0.04217015951871872\n",
      "Epoch 12599/30000 Training Loss: 0.03809661418199539\n",
      "Epoch 12600/30000 Training Loss: 0.03583477810025215\n",
      "Epoch 12600/30000 Validation Loss: 0.04179833084344864\n",
      "Epoch 12601/30000 Training Loss: 0.04224014654755592\n",
      "Epoch 12602/30000 Training Loss: 0.04242246598005295\n",
      "Epoch 12603/30000 Training Loss: 0.0466122142970562\n",
      "Epoch 12604/30000 Training Loss: 0.03676339611411095\n",
      "Epoch 12605/30000 Training Loss: 0.04573162645101547\n",
      "Epoch 12606/30000 Training Loss: 0.04858946055173874\n",
      "Epoch 12607/30000 Training Loss: 0.0451798252761364\n",
      "Epoch 12608/30000 Training Loss: 0.03815611079335213\n",
      "Epoch 12609/30000 Training Loss: 0.039676401764154434\n",
      "Epoch 12610/30000 Training Loss: 0.04518447071313858\n",
      "Epoch 12611/30000 Training Loss: 0.042312391102313995\n",
      "Epoch 12612/30000 Training Loss: 0.04077018052339554\n",
      "Epoch 12613/30000 Training Loss: 0.045275431126356125\n",
      "Epoch 12614/30000 Training Loss: 0.04653272405266762\n",
      "Epoch 12615/30000 Training Loss: 0.04508707672357559\n",
      "Epoch 12616/30000 Training Loss: 0.03902098163962364\n",
      "Epoch 12617/30000 Training Loss: 0.04155896231532097\n",
      "Epoch 12618/30000 Training Loss: 0.043808355927467346\n",
      "Epoch 12619/30000 Training Loss: 0.050709404051303864\n",
      "Epoch 12620/30000 Training Loss: 0.05378404259681702\n",
      "Epoch 12621/30000 Training Loss: 0.03991338610649109\n",
      "Epoch 12622/30000 Training Loss: 0.04922540858387947\n",
      "Epoch 12623/30000 Training Loss: 0.05081384256482124\n",
      "Epoch 12624/30000 Training Loss: 0.051797132939100266\n",
      "Epoch 12625/30000 Training Loss: 0.04534095153212547\n",
      "Epoch 12626/30000 Training Loss: 0.042935002595186234\n",
      "Epoch 12627/30000 Training Loss: 0.04389524832367897\n",
      "Epoch 12628/30000 Training Loss: 0.049982260912656784\n",
      "Epoch 12629/30000 Training Loss: 0.041385408490896225\n",
      "Epoch 12630/30000 Training Loss: 0.03659951686859131\n",
      "Epoch 12631/30000 Training Loss: 0.0411335714161396\n",
      "Epoch 12632/30000 Training Loss: 0.04169867932796478\n",
      "Epoch 12633/30000 Training Loss: 0.03915615379810333\n",
      "Epoch 12634/30000 Training Loss: 0.045285914093256\n",
      "Epoch 12635/30000 Training Loss: 0.04281659796833992\n",
      "Epoch 12636/30000 Training Loss: 0.05185984820127487\n",
      "Epoch 12637/30000 Training Loss: 0.0484391525387764\n",
      "Epoch 12638/30000 Training Loss: 0.039886776357889175\n",
      "Epoch 12639/30000 Training Loss: 0.04505927115678787\n",
      "Epoch 12640/30000 Training Loss: 0.04453446716070175\n",
      "Epoch 12641/30000 Training Loss: 0.04494812339544296\n",
      "Epoch 12642/30000 Training Loss: 0.04411767050623894\n",
      "Epoch 12643/30000 Training Loss: 0.050437428057193756\n",
      "Epoch 12644/30000 Training Loss: 0.042351823300123215\n",
      "Epoch 12645/30000 Training Loss: 0.04081621393561363\n",
      "Epoch 12646/30000 Training Loss: 0.04309041425585747\n",
      "Epoch 12647/30000 Training Loss: 0.04500116780400276\n",
      "Epoch 12648/30000 Training Loss: 0.048811640590429306\n",
      "Epoch 12649/30000 Training Loss: 0.04019094631075859\n",
      "Epoch 12650/30000 Training Loss: 0.05081595852971077\n",
      "Epoch 12650/30000 Validation Loss: 0.042584918439388275\n",
      "Epoch 12651/30000 Training Loss: 0.04370281845331192\n",
      "Epoch 12652/30000 Training Loss: 0.04528932273387909\n",
      "Epoch 12653/30000 Training Loss: 0.04490196704864502\n",
      "Epoch 12654/30000 Training Loss: 0.043427724391222\n",
      "Epoch 12655/30000 Training Loss: 0.045638639479875565\n",
      "Epoch 12656/30000 Training Loss: 0.04327697679400444\n",
      "Epoch 12657/30000 Training Loss: 0.04815272241830826\n",
      "Epoch 12658/30000 Training Loss: 0.048289258033037186\n",
      "Epoch 12659/30000 Training Loss: 0.041051555424928665\n",
      "Epoch 12660/30000 Training Loss: 0.058084964752197266\n",
      "Epoch 12661/30000 Training Loss: 0.04949735477566719\n",
      "Epoch 12662/30000 Training Loss: 0.05373193696141243\n",
      "Epoch 12663/30000 Training Loss: 0.044948138296604156\n",
      "Epoch 12664/30000 Training Loss: 0.0496884360909462\n",
      "Epoch 12665/30000 Training Loss: 0.04224688559770584\n",
      "Epoch 12666/30000 Training Loss: 0.043656203895807266\n",
      "Epoch 12667/30000 Training Loss: 0.05149755999445915\n",
      "Epoch 12668/30000 Training Loss: 0.035816047340631485\n",
      "Epoch 12669/30000 Training Loss: 0.036895595490932465\n",
      "Epoch 12670/30000 Training Loss: 0.04434368014335632\n",
      "Epoch 12671/30000 Training Loss: 0.042365752160549164\n",
      "Epoch 12672/30000 Training Loss: 0.04453011974692345\n",
      "Epoch 12673/30000 Training Loss: 0.04318064823746681\n",
      "Epoch 12674/30000 Training Loss: 0.05342743918299675\n",
      "Epoch 12675/30000 Training Loss: 0.049236852675676346\n",
      "Epoch 12676/30000 Training Loss: 0.05099096894264221\n",
      "Epoch 12677/30000 Training Loss: 0.04941777512431145\n",
      "Epoch 12678/30000 Training Loss: 0.04912416264414787\n",
      "Epoch 12679/30000 Training Loss: 0.043873097747564316\n",
      "Epoch 12680/30000 Training Loss: 0.04759624972939491\n",
      "Epoch 12681/30000 Training Loss: 0.04686426743865013\n",
      "Epoch 12682/30000 Training Loss: 0.047749198973178864\n",
      "Epoch 12683/30000 Training Loss: 0.04388599470257759\n",
      "Epoch 12684/30000 Training Loss: 0.0399196520447731\n",
      "Epoch 12685/30000 Training Loss: 0.045458417385816574\n",
      "Epoch 12686/30000 Training Loss: 0.04448024183511734\n",
      "Epoch 12687/30000 Training Loss: 0.051125966012477875\n",
      "Epoch 12688/30000 Training Loss: 0.050308384001255035\n",
      "Epoch 12689/30000 Training Loss: 0.044015586376190186\n",
      "Epoch 12690/30000 Training Loss: 0.04975932091474533\n",
      "Epoch 12691/30000 Training Loss: 0.04168274253606796\n",
      "Epoch 12692/30000 Training Loss: 0.04513624683022499\n",
      "Epoch 12693/30000 Training Loss: 0.04198960214853287\n",
      "Epoch 12694/30000 Training Loss: 0.041317686438560486\n",
      "Epoch 12695/30000 Training Loss: 0.04984058067202568\n",
      "Epoch 12696/30000 Training Loss: 0.041212208569049835\n",
      "Epoch 12697/30000 Training Loss: 0.05255729705095291\n",
      "Epoch 12698/30000 Training Loss: 0.04896940663456917\n",
      "Epoch 12699/30000 Training Loss: 0.049513619393110275\n",
      "Epoch 12700/30000 Training Loss: 0.047011081129312515\n",
      "Epoch 12700/30000 Validation Loss: 0.04614211246371269\n",
      "Epoch 12701/30000 Training Loss: 0.040703024715185165\n",
      "Epoch 12702/30000 Training Loss: 0.04852068051695824\n",
      "Epoch 12703/30000 Training Loss: 0.04521644115447998\n",
      "Epoch 12704/30000 Training Loss: 0.05227184295654297\n",
      "Epoch 12705/30000 Training Loss: 0.04456734657287598\n",
      "Epoch 12706/30000 Training Loss: 0.0416710339486599\n",
      "Epoch 12707/30000 Training Loss: 0.051792215555906296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12708/30000 Training Loss: 0.04628634452819824\n",
      "Epoch 12709/30000 Training Loss: 0.045031480491161346\n",
      "Epoch 12710/30000 Training Loss: 0.041127584874629974\n",
      "Epoch 12711/30000 Training Loss: 0.04290168732404709\n",
      "Epoch 12712/30000 Training Loss: 0.04534386843442917\n",
      "Epoch 12713/30000 Training Loss: 0.049088817089796066\n",
      "Epoch 12714/30000 Training Loss: 0.04374011978507042\n",
      "Epoch 12715/30000 Training Loss: 0.044014543294906616\n",
      "Epoch 12716/30000 Training Loss: 0.046049248427152634\n",
      "Epoch 12717/30000 Training Loss: 0.04076974466443062\n",
      "Epoch 12718/30000 Training Loss: 0.037101972848176956\n",
      "Epoch 12719/30000 Training Loss: 0.04050876200199127\n",
      "Epoch 12720/30000 Training Loss: 0.0528523214161396\n",
      "Epoch 12721/30000 Training Loss: 0.04956210032105446\n",
      "Epoch 12722/30000 Training Loss: 0.04529024288058281\n",
      "Epoch 12723/30000 Training Loss: 0.04609895497560501\n",
      "Epoch 12724/30000 Training Loss: 0.043289028108119965\n",
      "Epoch 12725/30000 Training Loss: 0.052462439984083176\n",
      "Epoch 12726/30000 Training Loss: 0.037421323359012604\n",
      "Epoch 12727/30000 Training Loss: 0.04575629532337189\n",
      "Epoch 12728/30000 Training Loss: 0.04263932257890701\n",
      "Epoch 12729/30000 Training Loss: 0.04482964426279068\n",
      "Epoch 12730/30000 Training Loss: 0.05152133107185364\n",
      "Epoch 12731/30000 Training Loss: 0.0541066937148571\n",
      "Epoch 12732/30000 Training Loss: 0.049369342625141144\n",
      "Epoch 12733/30000 Training Loss: 0.040117062628269196\n",
      "Epoch 12734/30000 Training Loss: 0.049139898270368576\n",
      "Epoch 12735/30000 Training Loss: 0.0402432456612587\n",
      "Epoch 12736/30000 Training Loss: 0.043899886310100555\n",
      "Epoch 12737/30000 Training Loss: 0.03874290734529495\n",
      "Epoch 12738/30000 Training Loss: 0.0440753735601902\n",
      "Epoch 12739/30000 Training Loss: 0.03896789625287056\n",
      "Epoch 12740/30000 Training Loss: 0.048190753906965256\n",
      "Epoch 12741/30000 Training Loss: 0.04459933564066887\n",
      "Epoch 12742/30000 Training Loss: 0.04232277721166611\n",
      "Epoch 12743/30000 Training Loss: 0.0489095039665699\n",
      "Epoch 12744/30000 Training Loss: 0.052260272204875946\n",
      "Epoch 12745/30000 Training Loss: 0.04282958433032036\n",
      "Epoch 12746/30000 Training Loss: 0.05083226040005684\n",
      "Epoch 12747/30000 Training Loss: 0.046343009918928146\n",
      "Epoch 12748/30000 Training Loss: 0.051883600652217865\n",
      "Epoch 12749/30000 Training Loss: 0.045880623161792755\n",
      "Epoch 12750/30000 Training Loss: 0.03535202890634537\n",
      "Epoch 12750/30000 Validation Loss: 0.041538406163454056\n",
      "Epoch 12751/30000 Training Loss: 0.047509849071502686\n",
      "Epoch 12752/30000 Training Loss: 0.05105935409665108\n",
      "Epoch 12753/30000 Training Loss: 0.0394260548055172\n",
      "Epoch 12754/30000 Training Loss: 0.04125892370939255\n",
      "Epoch 12755/30000 Training Loss: 0.03840213268995285\n",
      "Epoch 12756/30000 Training Loss: 0.03724202141165733\n",
      "Epoch 12757/30000 Training Loss: 0.046227410435676575\n",
      "Epoch 12758/30000 Training Loss: 0.04754861816763878\n",
      "Epoch 12759/30000 Training Loss: 0.04786256328225136\n",
      "Epoch 12760/30000 Training Loss: 0.04333222284913063\n",
      "Epoch 12761/30000 Training Loss: 0.04868825897574425\n",
      "Epoch 12762/30000 Training Loss: 0.03966143727302551\n",
      "Epoch 12763/30000 Training Loss: 0.04918750748038292\n",
      "Epoch 12764/30000 Training Loss: 0.04874766618013382\n",
      "Epoch 12765/30000 Training Loss: 0.03435371071100235\n",
      "Epoch 12766/30000 Training Loss: 0.0373464860022068\n",
      "Epoch 12767/30000 Training Loss: 0.04478855058550835\n",
      "Epoch 12768/30000 Training Loss: 0.04226870462298393\n",
      "Epoch 12769/30000 Training Loss: 0.04213220626115799\n",
      "Epoch 12770/30000 Training Loss: 0.0395180881023407\n",
      "Epoch 12771/30000 Training Loss: 0.045386213809251785\n",
      "Epoch 12772/30000 Training Loss: 0.0505807027220726\n",
      "Epoch 12773/30000 Training Loss: 0.05122492462396622\n",
      "Epoch 12774/30000 Training Loss: 0.04263605177402496\n",
      "Epoch 12775/30000 Training Loss: 0.048003245145082474\n",
      "Epoch 12776/30000 Training Loss: 0.04246235266327858\n",
      "Epoch 12777/30000 Training Loss: 0.04143620654940605\n",
      "Epoch 12778/30000 Training Loss: 0.0399160236120224\n",
      "Epoch 12779/30000 Training Loss: 0.04983287304639816\n",
      "Epoch 12780/30000 Training Loss: 0.05091734975576401\n",
      "Epoch 12781/30000 Training Loss: 0.048359133303165436\n",
      "Epoch 12782/30000 Training Loss: 0.047297313809394836\n",
      "Epoch 12783/30000 Training Loss: 0.04701488837599754\n",
      "Epoch 12784/30000 Training Loss: 0.05524814873933792\n",
      "Epoch 12785/30000 Training Loss: 0.053688935935497284\n",
      "Epoch 12786/30000 Training Loss: 0.05030623823404312\n",
      "Epoch 12787/30000 Training Loss: 0.049469709396362305\n",
      "Epoch 12788/30000 Training Loss: 0.046297602355480194\n",
      "Epoch 12789/30000 Training Loss: 0.04295283555984497\n",
      "Epoch 12790/30000 Training Loss: 0.04353038966655731\n",
      "Epoch 12791/30000 Training Loss: 0.047940514981746674\n",
      "Epoch 12792/30000 Training Loss: 0.05197950080037117\n",
      "Epoch 12793/30000 Training Loss: 0.04651354253292084\n",
      "Epoch 12794/30000 Training Loss: 0.04257085919380188\n",
      "Epoch 12795/30000 Training Loss: 0.04021100327372551\n",
      "Epoch 12796/30000 Training Loss: 0.04489741101861\n",
      "Epoch 12797/30000 Training Loss: 0.0433533638715744\n",
      "Epoch 12798/30000 Training Loss: 0.048319414258003235\n",
      "Epoch 12799/30000 Training Loss: 0.0489509180188179\n",
      "Epoch 12800/30000 Training Loss: 0.0399911068379879\n",
      "Epoch 12800/30000 Validation Loss: 0.041276536881923676\n",
      "Epoch 12801/30000 Training Loss: 0.04684961587190628\n",
      "Epoch 12802/30000 Training Loss: 0.04779622703790665\n",
      "Epoch 12803/30000 Training Loss: 0.044437915086746216\n",
      "Epoch 12804/30000 Training Loss: 0.04689791798591614\n",
      "Epoch 12805/30000 Training Loss: 0.04641309380531311\n",
      "Epoch 12806/30000 Training Loss: 0.04648708179593086\n",
      "Epoch 12807/30000 Training Loss: 0.037923663854599\n",
      "Epoch 12808/30000 Training Loss: 0.04449976980686188\n",
      "Epoch 12809/30000 Training Loss: 0.04343123733997345\n",
      "Epoch 12810/30000 Training Loss: 0.03716472163796425\n",
      "Epoch 12811/30000 Training Loss: 0.04274250194430351\n",
      "Epoch 12812/30000 Training Loss: 0.04392107203602791\n",
      "Epoch 12813/30000 Training Loss: 0.04613020271062851\n",
      "Epoch 12814/30000 Training Loss: 0.045257002115249634\n",
      "Epoch 12815/30000 Training Loss: 0.042798884212970734\n",
      "Epoch 12816/30000 Training Loss: 0.04264872148633003\n",
      "Epoch 12817/30000 Training Loss: 0.04124147444963455\n",
      "Epoch 12818/30000 Training Loss: 0.04268180578947067\n",
      "Epoch 12819/30000 Training Loss: 0.044111836701631546\n",
      "Epoch 12820/30000 Training Loss: 0.05172748491168022\n",
      "Epoch 12821/30000 Training Loss: 0.048875078558921814\n",
      "Epoch 12822/30000 Training Loss: 0.047231514006853104\n",
      "Epoch 12823/30000 Training Loss: 0.04565919563174248\n",
      "Epoch 12824/30000 Training Loss: 0.04925081506371498\n",
      "Epoch 12825/30000 Training Loss: 0.04322807863354683\n",
      "Epoch 12826/30000 Training Loss: 0.041683729737997055\n",
      "Epoch 12827/30000 Training Loss: 0.03918522968888283\n",
      "Epoch 12828/30000 Training Loss: 0.04806382581591606\n",
      "Epoch 12829/30000 Training Loss: 0.04940883815288544\n",
      "Epoch 12830/30000 Training Loss: 0.05046785995364189\n",
      "Epoch 12831/30000 Training Loss: 0.03777656704187393\n",
      "Epoch 12832/30000 Training Loss: 0.042471956461668015\n",
      "Epoch 12833/30000 Training Loss: 0.046652842313051224\n",
      "Epoch 12834/30000 Training Loss: 0.044188082218170166\n",
      "Epoch 12835/30000 Training Loss: 0.045659229159355164\n",
      "Epoch 12836/30000 Training Loss: 0.042462050914764404\n",
      "Epoch 12837/30000 Training Loss: 0.04321838170289993\n",
      "Epoch 12838/30000 Training Loss: 0.04742683097720146\n",
      "Epoch 12839/30000 Training Loss: 0.045382969081401825\n",
      "Epoch 12840/30000 Training Loss: 0.041580237448215485\n",
      "Epoch 12841/30000 Training Loss: 0.04804978892207146\n",
      "Epoch 12842/30000 Training Loss: 0.045165788382291794\n",
      "Epoch 12843/30000 Training Loss: 0.05204262584447861\n",
      "Epoch 12844/30000 Training Loss: 0.0431763119995594\n",
      "Epoch 12845/30000 Training Loss: 0.04819168522953987\n",
      "Epoch 12846/30000 Training Loss: 0.03708270564675331\n",
      "Epoch 12847/30000 Training Loss: 0.04828847944736481\n",
      "Epoch 12848/30000 Training Loss: 0.03754714876413345\n",
      "Epoch 12849/30000 Training Loss: 0.04879999905824661\n",
      "Epoch 12850/30000 Training Loss: 0.037574153393507004\n",
      "Epoch 12850/30000 Validation Loss: 0.048939891159534454\n",
      "Epoch 12851/30000 Training Loss: 0.038704268634319305\n",
      "Epoch 12852/30000 Training Loss: 0.052354902029037476\n",
      "Epoch 12853/30000 Training Loss: 0.03977857530117035\n",
      "Epoch 12854/30000 Training Loss: 0.04264499619603157\n",
      "Epoch 12855/30000 Training Loss: 0.04051264747977257\n",
      "Epoch 12856/30000 Training Loss: 0.049000658094882965\n",
      "Epoch 12857/30000 Training Loss: 0.04012446478009224\n",
      "Epoch 12858/30000 Training Loss: 0.03881729394197464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12859/30000 Training Loss: 0.04295117408037186\n",
      "Epoch 12860/30000 Training Loss: 0.045663587749004364\n",
      "Epoch 12861/30000 Training Loss: 0.041730768978595734\n",
      "Epoch 12862/30000 Training Loss: 0.0509350411593914\n",
      "Epoch 12863/30000 Training Loss: 0.03868989646434784\n",
      "Epoch 12864/30000 Training Loss: 0.04458920657634735\n",
      "Epoch 12865/30000 Training Loss: 0.04640484228730202\n",
      "Epoch 12866/30000 Training Loss: 0.03723170608282089\n",
      "Epoch 12867/30000 Training Loss: 0.04174064099788666\n",
      "Epoch 12868/30000 Training Loss: 0.05736350268125534\n",
      "Epoch 12869/30000 Training Loss: 0.050139714032411575\n",
      "Epoch 12870/30000 Training Loss: 0.04831055924296379\n",
      "Epoch 12871/30000 Training Loss: 0.04735330864787102\n",
      "Epoch 12872/30000 Training Loss: 0.04486853629350662\n",
      "Epoch 12873/30000 Training Loss: 0.04218965396285057\n",
      "Epoch 12874/30000 Training Loss: 0.045145951211452484\n",
      "Epoch 12875/30000 Training Loss: 0.057455431669950485\n",
      "Epoch 12876/30000 Training Loss: 0.04079452157020569\n",
      "Epoch 12877/30000 Training Loss: 0.04108964279294014\n",
      "Epoch 12878/30000 Training Loss: 0.04602774232625961\n",
      "Epoch 12879/30000 Training Loss: 0.03787487372756004\n",
      "Epoch 12880/30000 Training Loss: 0.03393714874982834\n",
      "Epoch 12881/30000 Training Loss: 0.046245038509368896\n",
      "Epoch 12882/30000 Training Loss: 0.049603305757045746\n",
      "Epoch 12883/30000 Training Loss: 0.03372041881084442\n",
      "Epoch 12884/30000 Training Loss: 0.056537527590990067\n",
      "Epoch 12885/30000 Training Loss: 0.05894456058740616\n",
      "Epoch 12886/30000 Training Loss: 0.046916428953409195\n",
      "Epoch 12887/30000 Training Loss: 0.05120525509119034\n",
      "Epoch 12888/30000 Training Loss: 0.042132169008255005\n",
      "Epoch 12889/30000 Training Loss: 0.05328521877527237\n",
      "Epoch 12890/30000 Training Loss: 0.049511589109897614\n",
      "Epoch 12891/30000 Training Loss: 0.04860180616378784\n",
      "Epoch 12892/30000 Training Loss: 0.04551585763692856\n",
      "Epoch 12893/30000 Training Loss: 0.045881498605012894\n",
      "Epoch 12894/30000 Training Loss: 0.039419014006853104\n",
      "Epoch 12895/30000 Training Loss: 0.042623795568943024\n",
      "Epoch 12896/30000 Training Loss: 0.04671386629343033\n",
      "Epoch 12897/30000 Training Loss: 0.047553349286317825\n",
      "Epoch 12898/30000 Training Loss: 0.049824364483356476\n",
      "Epoch 12899/30000 Training Loss: 0.046786218881607056\n",
      "Epoch 12900/30000 Training Loss: 0.04725407809019089\n",
      "Epoch 12900/30000 Validation Loss: 0.04275107383728027\n",
      "Epoch 12901/30000 Training Loss: 0.044810689985752106\n",
      "Epoch 12902/30000 Training Loss: 0.04185005649924278\n",
      "Epoch 12903/30000 Training Loss: 0.043612606823444366\n",
      "Epoch 12904/30000 Training Loss: 0.04609032720327377\n",
      "Epoch 12905/30000 Training Loss: 0.03847218677401543\n",
      "Epoch 12906/30000 Training Loss: 0.04370204359292984\n",
      "Epoch 12907/30000 Training Loss: 0.051824621856212616\n",
      "Epoch 12908/30000 Training Loss: 0.05017559975385666\n",
      "Epoch 12909/30000 Training Loss: 0.0436047725379467\n",
      "Epoch 12910/30000 Training Loss: 0.04870794713497162\n",
      "Epoch 12911/30000 Training Loss: 0.045126307755708694\n",
      "Epoch 12912/30000 Training Loss: 0.04099568724632263\n",
      "Epoch 12913/30000 Training Loss: 0.04302619397640228\n",
      "Epoch 12914/30000 Training Loss: 0.044146787375211716\n",
      "Epoch 12915/30000 Training Loss: 0.059273771941661835\n",
      "Epoch 12916/30000 Training Loss: 0.04026747867465019\n",
      "Epoch 12917/30000 Training Loss: 0.04766298085451126\n",
      "Epoch 12918/30000 Training Loss: 0.04811760038137436\n",
      "Epoch 12919/30000 Training Loss: 0.041582293808460236\n",
      "Epoch 12920/30000 Training Loss: 0.04313887655735016\n",
      "Epoch 12921/30000 Training Loss: 0.04530879855155945\n",
      "Epoch 12922/30000 Training Loss: 0.04657638818025589\n",
      "Epoch 12923/30000 Training Loss: 0.042760495096445084\n",
      "Epoch 12924/30000 Training Loss: 0.041305653750896454\n",
      "Epoch 12925/30000 Training Loss: 0.04723810777068138\n",
      "Epoch 12926/30000 Training Loss: 0.038040876388549805\n",
      "Epoch 12927/30000 Training Loss: 0.037861377000808716\n",
      "Epoch 12928/30000 Training Loss: 0.04445754736661911\n",
      "Epoch 12929/30000 Training Loss: 0.03857862576842308\n",
      "Epoch 12930/30000 Training Loss: 0.04835785552859306\n",
      "Epoch 12931/30000 Training Loss: 0.0398913249373436\n",
      "Epoch 12932/30000 Training Loss: 0.04822010546922684\n",
      "Epoch 12933/30000 Training Loss: 0.05492275953292847\n",
      "Epoch 12934/30000 Training Loss: 0.04975365102291107\n",
      "Epoch 12935/30000 Training Loss: 0.044110409915447235\n",
      "Epoch 12936/30000 Training Loss: 0.03969664126634598\n",
      "Epoch 12937/30000 Training Loss: 0.04424731805920601\n",
      "Epoch 12938/30000 Training Loss: 0.04936813935637474\n",
      "Epoch 12939/30000 Training Loss: 0.04509180411696434\n",
      "Epoch 12940/30000 Training Loss: 0.046275679022073746\n",
      "Epoch 12941/30000 Training Loss: 0.036884911358356476\n",
      "Epoch 12942/30000 Training Loss: 0.0412936732172966\n",
      "Epoch 12943/30000 Training Loss: 0.04471912980079651\n",
      "Epoch 12944/30000 Training Loss: 0.044266410171985626\n",
      "Epoch 12945/30000 Training Loss: 0.050088174641132355\n",
      "Epoch 12946/30000 Training Loss: 0.04528755694627762\n",
      "Epoch 12947/30000 Training Loss: 0.04410480707883835\n",
      "Epoch 12948/30000 Training Loss: 0.04412289336323738\n",
      "Epoch 12949/30000 Training Loss: 0.042403656989336014\n",
      "Epoch 12950/30000 Training Loss: 0.04761628434062004\n",
      "Epoch 12950/30000 Validation Loss: 0.05496280640363693\n",
      "Epoch 12951/30000 Training Loss: 0.04753657430410385\n",
      "Epoch 12952/30000 Training Loss: 0.03888688236474991\n",
      "Epoch 12953/30000 Training Loss: 0.047291100025177\n",
      "Epoch 12954/30000 Training Loss: 0.047557733952999115\n",
      "Epoch 12955/30000 Training Loss: 0.04956665635108948\n",
      "Epoch 12956/30000 Training Loss: 0.04277916997671127\n",
      "Epoch 12957/30000 Training Loss: 0.04647326469421387\n",
      "Epoch 12958/30000 Training Loss: 0.042328741401433945\n",
      "Epoch 12959/30000 Training Loss: 0.03702053055167198\n",
      "Epoch 12960/30000 Training Loss: 0.03973207622766495\n",
      "Epoch 12961/30000 Training Loss: 0.04267248511314392\n",
      "Epoch 12962/30000 Training Loss: 0.052606113255023956\n",
      "Epoch 12963/30000 Training Loss: 0.04793250560760498\n",
      "Epoch 12964/30000 Training Loss: 0.04410136863589287\n",
      "Epoch 12965/30000 Training Loss: 0.05223531648516655\n",
      "Epoch 12966/30000 Training Loss: 0.04407797008752823\n",
      "Epoch 12967/30000 Training Loss: 0.04182202368974686\n",
      "Epoch 12968/30000 Training Loss: 0.04592423886060715\n",
      "Epoch 12969/30000 Training Loss: 0.0505773201584816\n",
      "Epoch 12970/30000 Training Loss: 0.03964110463857651\n",
      "Epoch 12971/30000 Training Loss: 0.04504640772938728\n",
      "Epoch 12972/30000 Training Loss: 0.04100463166832924\n",
      "Epoch 12973/30000 Training Loss: 0.03945092484354973\n",
      "Epoch 12974/30000 Training Loss: 0.05197502300143242\n",
      "Epoch 12975/30000 Training Loss: 0.05441661551594734\n",
      "Epoch 12976/30000 Training Loss: 0.03676522523164749\n",
      "Epoch 12977/30000 Training Loss: 0.03944143280386925\n",
      "Epoch 12978/30000 Training Loss: 0.04285960644483566\n",
      "Epoch 12979/30000 Training Loss: 0.03856469690799713\n",
      "Epoch 12980/30000 Training Loss: 0.05266842246055603\n",
      "Epoch 12981/30000 Training Loss: 0.04481581225991249\n",
      "Epoch 12982/30000 Training Loss: 0.043469302356243134\n",
      "Epoch 12983/30000 Training Loss: 0.040996164083480835\n",
      "Epoch 12984/30000 Training Loss: 0.04609615355730057\n",
      "Epoch 12985/30000 Training Loss: 0.04049172252416611\n",
      "Epoch 12986/30000 Training Loss: 0.04769039526581764\n",
      "Epoch 12987/30000 Training Loss: 0.04441475123167038\n",
      "Epoch 12988/30000 Training Loss: 0.03501022607088089\n",
      "Epoch 12989/30000 Training Loss: 0.039034754037857056\n",
      "Epoch 12990/30000 Training Loss: 0.043678320944309235\n",
      "Epoch 12991/30000 Training Loss: 0.04561026394367218\n",
      "Epoch 12992/30000 Training Loss: 0.048134371638298035\n",
      "Epoch 12993/30000 Training Loss: 0.05286316201090813\n",
      "Epoch 12994/30000 Training Loss: 0.04409819096326828\n",
      "Epoch 12995/30000 Training Loss: 0.04414863511919975\n",
      "Epoch 12996/30000 Training Loss: 0.043187618255615234\n",
      "Epoch 12997/30000 Training Loss: 0.04783377796411514\n",
      "Epoch 12998/30000 Training Loss: 0.039130110293626785\n",
      "Epoch 12999/30000 Training Loss: 0.03737206757068634\n",
      "Epoch 13000/30000 Training Loss: 0.04462546110153198\n",
      "Epoch 13000/30000 Validation Loss: 0.04904467985033989\n",
      "Epoch 13001/30000 Training Loss: 0.04672934114933014\n",
      "Epoch 13002/30000 Training Loss: 0.04787520319223404\n",
      "Epoch 13003/30000 Training Loss: 0.043336667120456696\n",
      "Epoch 13004/30000 Training Loss: 0.051942676305770874\n",
      "Epoch 13005/30000 Training Loss: 0.04731229692697525\n",
      "Epoch 13006/30000 Training Loss: 0.044789351522922516\n",
      "Epoch 13007/30000 Training Loss: 0.05295538902282715\n",
      "Epoch 13008/30000 Training Loss: 0.037693724036216736\n",
      "Epoch 13009/30000 Training Loss: 0.055164091289043427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13010/30000 Training Loss: 0.045216433703899384\n",
      "Epoch 13011/30000 Training Loss: 0.04241124168038368\n",
      "Epoch 13012/30000 Training Loss: 0.03422091156244278\n",
      "Epoch 13013/30000 Training Loss: 0.04923959821462631\n",
      "Epoch 13014/30000 Training Loss: 0.04753303527832031\n",
      "Epoch 13015/30000 Training Loss: 0.046324990689754486\n",
      "Epoch 13016/30000 Training Loss: 0.05603409558534622\n",
      "Epoch 13017/30000 Training Loss: 0.04402756690979004\n",
      "Epoch 13018/30000 Training Loss: 0.049137182533741\n",
      "Epoch 13019/30000 Training Loss: 0.04237503558397293\n",
      "Epoch 13020/30000 Training Loss: 0.043868884444236755\n",
      "Epoch 13021/30000 Training Loss: 0.04503288492560387\n",
      "Epoch 13022/30000 Training Loss: 0.05586395785212517\n",
      "Epoch 13023/30000 Training Loss: 0.045572541654109955\n",
      "Epoch 13024/30000 Training Loss: 0.03739284351468086\n",
      "Epoch 13025/30000 Training Loss: 0.04621049016714096\n",
      "Epoch 13026/30000 Training Loss: 0.0501583106815815\n",
      "Epoch 13027/30000 Training Loss: 0.05213097482919693\n",
      "Epoch 13028/30000 Training Loss: 0.0512893870472908\n",
      "Epoch 13029/30000 Training Loss: 0.04915473610162735\n",
      "Epoch 13030/30000 Training Loss: 0.047387100756168365\n",
      "Epoch 13031/30000 Training Loss: 0.044969163835048676\n",
      "Epoch 13032/30000 Training Loss: 0.04313725605607033\n",
      "Epoch 13033/30000 Training Loss: 0.04919563978910446\n",
      "Epoch 13034/30000 Training Loss: 0.04119614511728287\n",
      "Epoch 13035/30000 Training Loss: 0.04404279217123985\n",
      "Epoch 13036/30000 Training Loss: 0.04577947407960892\n",
      "Epoch 13037/30000 Training Loss: 0.042808666825294495\n",
      "Epoch 13038/30000 Training Loss: 0.03835514560341835\n",
      "Epoch 13039/30000 Training Loss: 0.043061211705207825\n",
      "Epoch 13040/30000 Training Loss: 0.0455673448741436\n",
      "Epoch 13041/30000 Training Loss: 0.04672946408390999\n",
      "Epoch 13042/30000 Training Loss: 0.052251726388931274\n",
      "Epoch 13043/30000 Training Loss: 0.05075642466545105\n",
      "Epoch 13044/30000 Training Loss: 0.04180967062711716\n",
      "Epoch 13045/30000 Training Loss: 0.05725587159395218\n",
      "Epoch 13046/30000 Training Loss: 0.04327572137117386\n",
      "Epoch 13047/30000 Training Loss: 0.03981439024209976\n",
      "Epoch 13048/30000 Training Loss: 0.04152155667543411\n",
      "Epoch 13049/30000 Training Loss: 0.04187820851802826\n",
      "Epoch 13050/30000 Training Loss: 0.0414169542491436\n",
      "Epoch 13050/30000 Validation Loss: 0.043308209627866745\n",
      "Epoch 13051/30000 Training Loss: 0.04812264442443848\n",
      "Epoch 13052/30000 Training Loss: 0.037511326372623444\n",
      "Epoch 13053/30000 Training Loss: 0.04457883909344673\n",
      "Epoch 13054/30000 Training Loss: 0.03321024030447006\n",
      "Epoch 13055/30000 Training Loss: 0.04185701906681061\n",
      "Epoch 13056/30000 Training Loss: 0.04953275993466377\n",
      "Epoch 13057/30000 Training Loss: 0.04658152908086777\n",
      "Epoch 13058/30000 Training Loss: 0.048445601016283035\n",
      "Epoch 13059/30000 Training Loss: 0.03851093724370003\n",
      "Epoch 13060/30000 Training Loss: 0.05033789202570915\n",
      "Epoch 13061/30000 Training Loss: 0.04158393666148186\n",
      "Epoch 13062/30000 Training Loss: 0.054140277206897736\n",
      "Epoch 13063/30000 Training Loss: 0.04258354380726814\n",
      "Epoch 13064/30000 Training Loss: 0.04292779043316841\n",
      "Epoch 13065/30000 Training Loss: 0.047133948653936386\n",
      "Epoch 13066/30000 Training Loss: 0.05788394808769226\n",
      "Epoch 13067/30000 Training Loss: 0.04112610965967178\n",
      "Epoch 13068/30000 Training Loss: 0.03365824744105339\n",
      "Epoch 13069/30000 Training Loss: 0.0403633713722229\n",
      "Epoch 13070/30000 Training Loss: 0.04143106937408447\n",
      "Epoch 13071/30000 Training Loss: 0.0425889752805233\n",
      "Epoch 13072/30000 Training Loss: 0.05309037119150162\n",
      "Epoch 13073/30000 Training Loss: 0.045388467609882355\n",
      "Epoch 13074/30000 Training Loss: 0.05198157578706741\n",
      "Epoch 13075/30000 Training Loss: 0.04962358996272087\n",
      "Epoch 13076/30000 Training Loss: 0.04068060219287872\n",
      "Epoch 13077/30000 Training Loss: 0.03863624483346939\n",
      "Epoch 13078/30000 Training Loss: 0.049402229487895966\n",
      "Epoch 13079/30000 Training Loss: 0.04358155280351639\n",
      "Epoch 13080/30000 Training Loss: 0.04881501570343971\n",
      "Epoch 13081/30000 Training Loss: 0.04312955215573311\n",
      "Epoch 13082/30000 Training Loss: 0.04616396129131317\n",
      "Epoch 13083/30000 Training Loss: 0.04449670761823654\n",
      "Epoch 13084/30000 Training Loss: 0.04752379283308983\n",
      "Epoch 13085/30000 Training Loss: 0.04850068315863609\n",
      "Epoch 13086/30000 Training Loss: 0.04721671715378761\n",
      "Epoch 13087/30000 Training Loss: 0.038030579686164856\n",
      "Epoch 13088/30000 Training Loss: 0.05079349875450134\n",
      "Epoch 13089/30000 Training Loss: 0.03894329071044922\n",
      "Epoch 13090/30000 Training Loss: 0.038097865879535675\n",
      "Epoch 13091/30000 Training Loss: 0.04242522269487381\n",
      "Epoch 13092/30000 Training Loss: 0.053990550339221954\n",
      "Epoch 13093/30000 Training Loss: 0.046248629689216614\n",
      "Epoch 13094/30000 Training Loss: 0.034923456609249115\n",
      "Epoch 13095/30000 Training Loss: 0.04777348041534424\n",
      "Epoch 13096/30000 Training Loss: 0.04053029417991638\n",
      "Epoch 13097/30000 Training Loss: 0.049398668110370636\n",
      "Epoch 13098/30000 Training Loss: 0.045114774256944656\n",
      "Epoch 13099/30000 Training Loss: 0.040133215487003326\n",
      "Epoch 13100/30000 Training Loss: 0.03951026126742363\n",
      "Epoch 13100/30000 Validation Loss: 0.047441013157367706\n",
      "Epoch 13101/30000 Training Loss: 0.04418506473302841\n",
      "Epoch 13102/30000 Training Loss: 0.046344757080078125\n",
      "Epoch 13103/30000 Training Loss: 0.041562099009752274\n",
      "Epoch 13104/30000 Training Loss: 0.044287171214818954\n",
      "Epoch 13105/30000 Training Loss: 0.04275916889309883\n",
      "Epoch 13106/30000 Training Loss: 0.0423397496342659\n",
      "Epoch 13107/30000 Training Loss: 0.042726773768663406\n",
      "Epoch 13108/30000 Training Loss: 0.04987369850277901\n",
      "Epoch 13109/30000 Training Loss: 0.045221321284770966\n",
      "Epoch 13110/30000 Training Loss: 0.04292754828929901\n",
      "Epoch 13111/30000 Training Loss: 0.04164157807826996\n",
      "Epoch 13112/30000 Training Loss: 0.037191107869148254\n",
      "Epoch 13113/30000 Training Loss: 0.04205717891454697\n",
      "Epoch 13114/30000 Training Loss: 0.04505249112844467\n",
      "Epoch 13115/30000 Training Loss: 0.05585233122110367\n",
      "Epoch 13116/30000 Training Loss: 0.052605606615543365\n",
      "Epoch 13117/30000 Training Loss: 0.03799393028020859\n",
      "Epoch 13118/30000 Training Loss: 0.0448656901717186\n",
      "Epoch 13119/30000 Training Loss: 0.04402317479252815\n",
      "Epoch 13120/30000 Training Loss: 0.03928885608911514\n",
      "Epoch 13121/30000 Training Loss: 0.03835061192512512\n",
      "Epoch 13122/30000 Training Loss: 0.0419476144015789\n",
      "Epoch 13123/30000 Training Loss: 0.05244443938136101\n",
      "Epoch 13124/30000 Training Loss: 0.038678981363773346\n",
      "Epoch 13125/30000 Training Loss: 0.038441251963377\n",
      "Epoch 13126/30000 Training Loss: 0.04387510567903519\n",
      "Epoch 13127/30000 Training Loss: 0.04518790543079376\n",
      "Epoch 13128/30000 Training Loss: 0.048624541610479355\n",
      "Epoch 13129/30000 Training Loss: 0.04392377287149429\n",
      "Epoch 13130/30000 Training Loss: 0.05424756929278374\n",
      "Epoch 13131/30000 Training Loss: 0.03947850316762924\n",
      "Epoch 13132/30000 Training Loss: 0.05991900712251663\n",
      "Epoch 13133/30000 Training Loss: 0.041611261665821075\n",
      "Epoch 13134/30000 Training Loss: 0.04061169549822807\n",
      "Epoch 13135/30000 Training Loss: 0.055751342326402664\n",
      "Epoch 13136/30000 Training Loss: 0.0475076399743557\n",
      "Epoch 13137/30000 Training Loss: 0.04087163135409355\n",
      "Epoch 13138/30000 Training Loss: 0.04549849033355713\n",
      "Epoch 13139/30000 Training Loss: 0.03958161175251007\n",
      "Epoch 13140/30000 Training Loss: 0.0416221097111702\n",
      "Epoch 13141/30000 Training Loss: 0.04311211034655571\n",
      "Epoch 13142/30000 Training Loss: 0.04343733936548233\n",
      "Epoch 13143/30000 Training Loss: 0.047164835035800934\n",
      "Epoch 13144/30000 Training Loss: 0.04684565216302872\n",
      "Epoch 13145/30000 Training Loss: 0.0534333698451519\n",
      "Epoch 13146/30000 Training Loss: 0.042704883962869644\n",
      "Epoch 13147/30000 Training Loss: 0.0524187795817852\n",
      "Epoch 13148/30000 Training Loss: 0.04380634427070618\n",
      "Epoch 13149/30000 Training Loss: 0.052620094269514084\n",
      "Epoch 13150/30000 Training Loss: 0.04217858985066414\n",
      "Epoch 13150/30000 Validation Loss: 0.04749064892530441\n",
      "Epoch 13151/30000 Training Loss: 0.04545453190803528\n",
      "Epoch 13152/30000 Training Loss: 0.05545465275645256\n",
      "Epoch 13153/30000 Training Loss: 0.04157613217830658\n",
      "Epoch 13154/30000 Training Loss: 0.038699131458997726\n",
      "Epoch 13155/30000 Training Loss: 0.04276183992624283\n",
      "Epoch 13156/30000 Training Loss: 0.049142032861709595\n",
      "Epoch 13157/30000 Training Loss: 0.04064280539751053\n",
      "Epoch 13158/30000 Training Loss: 0.04987699165940285\n",
      "Epoch 13159/30000 Training Loss: 0.04703126102685928\n",
      "Epoch 13160/30000 Training Loss: 0.04942171648144722\n",
      "Epoch 13161/30000 Training Loss: 0.044127456843853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13162/30000 Training Loss: 0.05511587858200073\n",
      "Epoch 13163/30000 Training Loss: 0.03705083578824997\n",
      "Epoch 13164/30000 Training Loss: 0.04815711826086044\n",
      "Epoch 13165/30000 Training Loss: 0.03601047396659851\n",
      "Epoch 13166/30000 Training Loss: 0.04497060179710388\n",
      "Epoch 13167/30000 Training Loss: 0.04870565980672836\n",
      "Epoch 13168/30000 Training Loss: 0.05580651015043259\n",
      "Epoch 13169/30000 Training Loss: 0.04744087904691696\n",
      "Epoch 13170/30000 Training Loss: 0.042295269668102264\n",
      "Epoch 13171/30000 Training Loss: 0.0534587986767292\n",
      "Epoch 13172/30000 Training Loss: 0.04634819179773331\n",
      "Epoch 13173/30000 Training Loss: 0.04537584260106087\n",
      "Epoch 13174/30000 Training Loss: 0.051197730004787445\n",
      "Epoch 13175/30000 Training Loss: 0.04663975164294243\n",
      "Epoch 13176/30000 Training Loss: 0.04968477785587311\n",
      "Epoch 13177/30000 Training Loss: 0.0378083698451519\n",
      "Epoch 13178/30000 Training Loss: 0.043124839663505554\n",
      "Epoch 13179/30000 Training Loss: 0.04508055001497269\n",
      "Epoch 13180/30000 Training Loss: 0.05282170698046684\n",
      "Epoch 13181/30000 Training Loss: 0.0437077060341835\n",
      "Epoch 13182/30000 Training Loss: 0.04054684564471245\n",
      "Epoch 13183/30000 Training Loss: 0.05146848037838936\n",
      "Epoch 13184/30000 Training Loss: 0.05095662549138069\n",
      "Epoch 13185/30000 Training Loss: 0.036140698939561844\n",
      "Epoch 13186/30000 Training Loss: 0.056484829634428024\n",
      "Epoch 13187/30000 Training Loss: 0.046278562396764755\n",
      "Epoch 13188/30000 Training Loss: 0.040955670177936554\n",
      "Epoch 13189/30000 Training Loss: 0.045184798538684845\n",
      "Epoch 13190/30000 Training Loss: 0.04658455029129982\n",
      "Epoch 13191/30000 Training Loss: 0.04568682238459587\n",
      "Epoch 13192/30000 Training Loss: 0.046716563403606415\n",
      "Epoch 13193/30000 Training Loss: 0.04184243828058243\n",
      "Epoch 13194/30000 Training Loss: 0.04945894330739975\n",
      "Epoch 13195/30000 Training Loss: 0.039694517850875854\n",
      "Epoch 13196/30000 Training Loss: 0.0437619723379612\n",
      "Epoch 13197/30000 Training Loss: 0.05035897344350815\n",
      "Epoch 13198/30000 Training Loss: 0.044343482702970505\n",
      "Epoch 13199/30000 Training Loss: 0.04550301283597946\n",
      "Epoch 13200/30000 Training Loss: 0.03987887129187584\n",
      "Epoch 13200/30000 Validation Loss: 0.0517287477850914\n",
      "Epoch 13201/30000 Training Loss: 0.04568878933787346\n",
      "Epoch 13202/30000 Training Loss: 0.05102512985467911\n",
      "Epoch 13203/30000 Training Loss: 0.048347584903240204\n",
      "Epoch 13204/30000 Training Loss: 0.04088015481829643\n",
      "Epoch 13205/30000 Training Loss: 0.042466916143894196\n",
      "Epoch 13206/30000 Training Loss: 0.050689585506916046\n",
      "Epoch 13207/30000 Training Loss: 0.045657724142074585\n",
      "Epoch 13208/30000 Training Loss: 0.04650569334626198\n",
      "Epoch 13209/30000 Training Loss: 0.04201842471957207\n",
      "Epoch 13210/30000 Training Loss: 0.043584417551755905\n",
      "Epoch 13211/30000 Training Loss: 0.037570323795080185\n",
      "Epoch 13212/30000 Training Loss: 0.04536611586809158\n",
      "Epoch 13213/30000 Training Loss: 0.04957670718431473\n",
      "Epoch 13214/30000 Training Loss: 0.04527803137898445\n",
      "Epoch 13215/30000 Training Loss: 0.057186685502529144\n",
      "Epoch 13216/30000 Training Loss: 0.04331621527671814\n",
      "Epoch 13217/30000 Training Loss: 0.045800723135471344\n",
      "Epoch 13218/30000 Training Loss: 0.040427129715681076\n",
      "Epoch 13219/30000 Training Loss: 0.03753558173775673\n",
      "Epoch 13220/30000 Training Loss: 0.03628532961010933\n",
      "Epoch 13221/30000 Training Loss: 0.048257894814014435\n",
      "Epoch 13222/30000 Training Loss: 0.04665253311395645\n",
      "Epoch 13223/30000 Training Loss: 0.04471723362803459\n",
      "Epoch 13224/30000 Training Loss: 0.048332199454307556\n",
      "Epoch 13225/30000 Training Loss: 0.05013100430369377\n",
      "Epoch 13226/30000 Training Loss: 0.04753334820270538\n",
      "Epoch 13227/30000 Training Loss: 0.04559791088104248\n",
      "Epoch 13228/30000 Training Loss: 0.040809523314237595\n",
      "Epoch 13229/30000 Training Loss: 0.04334580898284912\n",
      "Epoch 13230/30000 Training Loss: 0.04446205496788025\n",
      "Epoch 13231/30000 Training Loss: 0.03691113740205765\n",
      "Epoch 13232/30000 Training Loss: 0.04300533980131149\n",
      "Epoch 13233/30000 Training Loss: 0.05135096237063408\n",
      "Epoch 13234/30000 Training Loss: 0.04264407604932785\n",
      "Epoch 13235/30000 Training Loss: 0.045882873237133026\n",
      "Epoch 13236/30000 Training Loss: 0.04398905113339424\n",
      "Epoch 13237/30000 Training Loss: 0.036251358687877655\n",
      "Epoch 13238/30000 Training Loss: 0.0449320524930954\n",
      "Epoch 13239/30000 Training Loss: 0.04060053080320358\n",
      "Epoch 13240/30000 Training Loss: 0.04115627706050873\n",
      "Epoch 13241/30000 Training Loss: 0.047784484922885895\n",
      "Epoch 13242/30000 Training Loss: 0.04515599459409714\n",
      "Epoch 13243/30000 Training Loss: 0.04661610722541809\n",
      "Epoch 13244/30000 Training Loss: 0.04210221767425537\n",
      "Epoch 13245/30000 Training Loss: 0.0306658037006855\n",
      "Epoch 13246/30000 Training Loss: 0.04997381195425987\n",
      "Epoch 13247/30000 Training Loss: 0.03999730199575424\n",
      "Epoch 13248/30000 Training Loss: 0.04576670005917549\n",
      "Epoch 13249/30000 Training Loss: 0.051959551870822906\n",
      "Epoch 13250/30000 Training Loss: 0.054176755249500275\n",
      "Epoch 13250/30000 Validation Loss: 0.05154051631689072\n",
      "Epoch 13251/30000 Training Loss: 0.042350783944129944\n",
      "Epoch 13252/30000 Training Loss: 0.043177612125873566\n",
      "Epoch 13253/30000 Training Loss: 0.03974024951457977\n",
      "Epoch 13254/30000 Training Loss: 0.044988684356212616\n",
      "Epoch 13255/30000 Training Loss: 0.042266231030225754\n",
      "Epoch 13256/30000 Training Loss: 0.03992516174912453\n",
      "Epoch 13257/30000 Training Loss: 0.04791925475001335\n",
      "Epoch 13258/30000 Training Loss: 0.04667416587471962\n",
      "Epoch 13259/30000 Training Loss: 0.047553423792123795\n",
      "Epoch 13260/30000 Training Loss: 0.04740315303206444\n",
      "Epoch 13261/30000 Training Loss: 0.037825893610715866\n",
      "Epoch 13262/30000 Training Loss: 0.05151395872235298\n",
      "Epoch 13263/30000 Training Loss: 0.0443410687148571\n",
      "Epoch 13264/30000 Training Loss: 0.04079385846853256\n",
      "Epoch 13265/30000 Training Loss: 0.054138265550136566\n",
      "Epoch 13266/30000 Training Loss: 0.036857180297374725\n",
      "Epoch 13267/30000 Training Loss: 0.0463460348546505\n",
      "Epoch 13268/30000 Training Loss: 0.039086904376745224\n",
      "Epoch 13269/30000 Training Loss: 0.04386115074157715\n",
      "Epoch 13270/30000 Training Loss: 0.047557465732097626\n",
      "Epoch 13271/30000 Training Loss: 0.0443790964782238\n",
      "Epoch 13272/30000 Training Loss: 0.04065379872918129\n",
      "Epoch 13273/30000 Training Loss: 0.04244004562497139\n",
      "Epoch 13274/30000 Training Loss: 0.043102093040943146\n",
      "Epoch 13275/30000 Training Loss: 0.04474661499261856\n",
      "Epoch 13276/30000 Training Loss: 0.03788284212350845\n",
      "Epoch 13277/30000 Training Loss: 0.04339132830500603\n",
      "Epoch 13278/30000 Training Loss: 0.05276903510093689\n",
      "Epoch 13279/30000 Training Loss: 0.04517572000622749\n",
      "Epoch 13280/30000 Training Loss: 0.0500708632171154\n",
      "Epoch 13281/30000 Training Loss: 0.039582282304763794\n",
      "Epoch 13282/30000 Training Loss: 0.046737007796764374\n",
      "Epoch 13283/30000 Training Loss: 0.03988493233919144\n",
      "Epoch 13284/30000 Training Loss: 0.04944128915667534\n",
      "Epoch 13285/30000 Training Loss: 0.0401342548429966\n",
      "Epoch 13286/30000 Training Loss: 0.04251041263341904\n",
      "Epoch 13287/30000 Training Loss: 0.03944680467247963\n",
      "Epoch 13288/30000 Training Loss: 0.043709881603717804\n",
      "Epoch 13289/30000 Training Loss: 0.04793084040284157\n",
      "Epoch 13290/30000 Training Loss: 0.046981267631053925\n",
      "Epoch 13291/30000 Training Loss: 0.04963028430938721\n",
      "Epoch 13292/30000 Training Loss: 0.03998932987451553\n",
      "Epoch 13293/30000 Training Loss: 0.0417441800236702\n",
      "Epoch 13294/30000 Training Loss: 0.0510404109954834\n",
      "Epoch 13295/30000 Training Loss: 0.0381716713309288\n",
      "Epoch 13296/30000 Training Loss: 0.05078406259417534\n",
      "Epoch 13297/30000 Training Loss: 0.04762980341911316\n",
      "Epoch 13298/30000 Training Loss: 0.04490535333752632\n",
      "Epoch 13299/30000 Training Loss: 0.05443020910024643\n",
      "Epoch 13300/30000 Training Loss: 0.04832364246249199\n",
      "Epoch 13300/30000 Validation Loss: 0.03362080454826355\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03362080454826355<=============\n",
      "Epoch 13301/30000 Training Loss: 0.0434306301176548\n",
      "Epoch 13302/30000 Training Loss: 0.04148942232131958\n",
      "Epoch 13303/30000 Training Loss: 0.03259993717074394\n",
      "Epoch 13304/30000 Training Loss: 0.04044575244188309\n",
      "Epoch 13305/30000 Training Loss: 0.039518803358078\n",
      "Epoch 13306/30000 Training Loss: 0.05259694531559944\n",
      "Epoch 13307/30000 Training Loss: 0.044641394168138504\n",
      "Epoch 13308/30000 Training Loss: 0.04626089707016945\n",
      "Epoch 13309/30000 Training Loss: 0.04979672655463219\n",
      "Epoch 13310/30000 Training Loss: 0.05052679777145386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13311/30000 Training Loss: 0.047757696360349655\n",
      "Epoch 13312/30000 Training Loss: 0.04488392546772957\n",
      "Epoch 13313/30000 Training Loss: 0.03628303110599518\n",
      "Epoch 13314/30000 Training Loss: 0.04734572768211365\n",
      "Epoch 13315/30000 Training Loss: 0.035429079085588455\n",
      "Epoch 13316/30000 Training Loss: 0.04188978672027588\n",
      "Epoch 13317/30000 Training Loss: 0.04349623620510101\n",
      "Epoch 13318/30000 Training Loss: 0.035843364894390106\n",
      "Epoch 13319/30000 Training Loss: 0.049405571073293686\n",
      "Epoch 13320/30000 Training Loss: 0.043961964547634125\n",
      "Epoch 13321/30000 Training Loss: 0.04680153727531433\n",
      "Epoch 13322/30000 Training Loss: 0.047006621956825256\n",
      "Epoch 13323/30000 Training Loss: 0.03993230313062668\n",
      "Epoch 13324/30000 Training Loss: 0.04312410205602646\n",
      "Epoch 13325/30000 Training Loss: 0.037843428552150726\n",
      "Epoch 13326/30000 Training Loss: 0.03904946893453598\n",
      "Epoch 13327/30000 Training Loss: 0.044650547206401825\n",
      "Epoch 13328/30000 Training Loss: 0.04303256794810295\n",
      "Epoch 13329/30000 Training Loss: 0.06218342110514641\n",
      "Epoch 13330/30000 Training Loss: 0.04856107011437416\n",
      "Epoch 13331/30000 Training Loss: 0.043444450944662094\n",
      "Epoch 13332/30000 Training Loss: 0.04075747728347778\n",
      "Epoch 13333/30000 Training Loss: 0.045499030500650406\n",
      "Epoch 13334/30000 Training Loss: 0.044111303985118866\n",
      "Epoch 13335/30000 Training Loss: 0.0428154431283474\n",
      "Epoch 13336/30000 Training Loss: 0.040560606867074966\n",
      "Epoch 13337/30000 Training Loss: 0.04533412680029869\n",
      "Epoch 13338/30000 Training Loss: 0.04603804275393486\n",
      "Epoch 13339/30000 Training Loss: 0.037406906485557556\n",
      "Epoch 13340/30000 Training Loss: 0.0393996462225914\n",
      "Epoch 13341/30000 Training Loss: 0.05602071434259415\n",
      "Epoch 13342/30000 Training Loss: 0.03818826004862785\n",
      "Epoch 13343/30000 Training Loss: 0.04043227434158325\n",
      "Epoch 13344/30000 Training Loss: 0.05009301379323006\n",
      "Epoch 13345/30000 Training Loss: 0.04867591708898544\n",
      "Epoch 13346/30000 Training Loss: 0.04275091737508774\n",
      "Epoch 13347/30000 Training Loss: 0.04310307651758194\n",
      "Epoch 13348/30000 Training Loss: 0.041621942073106766\n",
      "Epoch 13349/30000 Training Loss: 0.047391582280397415\n",
      "Epoch 13350/30000 Training Loss: 0.04219368100166321\n",
      "Epoch 13350/30000 Validation Loss: 0.041386183351278305\n",
      "Epoch 13351/30000 Training Loss: 0.05498601123690605\n",
      "Epoch 13352/30000 Training Loss: 0.04303232580423355\n",
      "Epoch 13353/30000 Training Loss: 0.04472372680902481\n",
      "Epoch 13354/30000 Training Loss: 0.03884682059288025\n",
      "Epoch 13355/30000 Training Loss: 0.04315159097313881\n",
      "Epoch 13356/30000 Training Loss: 0.03829895332455635\n",
      "Epoch 13357/30000 Training Loss: 0.036746807396411896\n",
      "Epoch 13358/30000 Training Loss: 0.04192955046892166\n",
      "Epoch 13359/30000 Training Loss: 0.051817476749420166\n",
      "Epoch 13360/30000 Training Loss: 0.043320782482624054\n",
      "Epoch 13361/30000 Training Loss: 0.04746684804558754\n",
      "Epoch 13362/30000 Training Loss: 0.03925516456365585\n",
      "Epoch 13363/30000 Training Loss: 0.04902774840593338\n",
      "Epoch 13364/30000 Training Loss: 0.042818762362003326\n",
      "Epoch 13365/30000 Training Loss: 0.03904268890619278\n",
      "Epoch 13366/30000 Training Loss: 0.044997360557317734\n",
      "Epoch 13367/30000 Training Loss: 0.04175055772066116\n",
      "Epoch 13368/30000 Training Loss: 0.045669518411159515\n",
      "Epoch 13369/30000 Training Loss: 0.041742824018001556\n",
      "Epoch 13370/30000 Training Loss: 0.04985653609037399\n",
      "Epoch 13371/30000 Training Loss: 0.04192449152469635\n",
      "Epoch 13372/30000 Training Loss: 0.04169581085443497\n",
      "Epoch 13373/30000 Training Loss: 0.046099353581666946\n",
      "Epoch 13374/30000 Training Loss: 0.04533256217837334\n",
      "Epoch 13375/30000 Training Loss: 0.042502135038375854\n",
      "Epoch 13376/30000 Training Loss: 0.05104777216911316\n",
      "Epoch 13377/30000 Training Loss: 0.04081344977021217\n",
      "Epoch 13378/30000 Training Loss: 0.03771335631608963\n",
      "Epoch 13379/30000 Training Loss: 0.0431552454829216\n",
      "Epoch 13380/30000 Training Loss: 0.047064997255802155\n",
      "Epoch 13381/30000 Training Loss: 0.04559659957885742\n",
      "Epoch 13382/30000 Training Loss: 0.04508637264370918\n",
      "Epoch 13383/30000 Training Loss: 0.04720793291926384\n",
      "Epoch 13384/30000 Training Loss: 0.050351809710264206\n",
      "Epoch 13385/30000 Training Loss: 0.04964137822389603\n",
      "Epoch 13386/30000 Training Loss: 0.04087350517511368\n",
      "Epoch 13387/30000 Training Loss: 0.038914766162633896\n",
      "Epoch 13388/30000 Training Loss: 0.03998088091611862\n",
      "Epoch 13389/30000 Training Loss: 0.04173342138528824\n",
      "Epoch 13390/30000 Training Loss: 0.0401904471218586\n",
      "Epoch 13391/30000 Training Loss: 0.04098083823919296\n",
      "Epoch 13392/30000 Training Loss: 0.04925372079014778\n",
      "Epoch 13393/30000 Training Loss: 0.04708876088261604\n",
      "Epoch 13394/30000 Training Loss: 0.04559953138232231\n",
      "Epoch 13395/30000 Training Loss: 0.04623754695057869\n",
      "Epoch 13396/30000 Training Loss: 0.04320036619901657\n",
      "Epoch 13397/30000 Training Loss: 0.03777342289686203\n",
      "Epoch 13398/30000 Training Loss: 0.048711441457271576\n",
      "Epoch 13399/30000 Training Loss: 0.04383489862084389\n",
      "Epoch 13400/30000 Training Loss: 0.04106101393699646\n",
      "Epoch 13400/30000 Validation Loss: 0.0442826971411705\n",
      "Epoch 13401/30000 Training Loss: 0.04643135145306587\n",
      "Epoch 13402/30000 Training Loss: 0.03992985934019089\n",
      "Epoch 13403/30000 Training Loss: 0.04164479300379753\n",
      "Epoch 13404/30000 Training Loss: 0.046932876110076904\n",
      "Epoch 13405/30000 Training Loss: 0.04504891112446785\n",
      "Epoch 13406/30000 Training Loss: 0.039283283054828644\n",
      "Epoch 13407/30000 Training Loss: 0.03772074729204178\n",
      "Epoch 13408/30000 Training Loss: 0.04829667508602142\n",
      "Epoch 13409/30000 Training Loss: 0.047543030232191086\n",
      "Epoch 13410/30000 Training Loss: 0.04320874810218811\n",
      "Epoch 13411/30000 Training Loss: 0.047765035182237625\n",
      "Epoch 13412/30000 Training Loss: 0.042908452451229095\n",
      "Epoch 13413/30000 Training Loss: 0.04756087809801102\n",
      "Epoch 13414/30000 Training Loss: 0.04882168769836426\n",
      "Epoch 13415/30000 Training Loss: 0.045869339257478714\n",
      "Epoch 13416/30000 Training Loss: 0.03769116476178169\n",
      "Epoch 13417/30000 Training Loss: 0.04662767052650452\n",
      "Epoch 13418/30000 Training Loss: 0.04272669553756714\n",
      "Epoch 13419/30000 Training Loss: 0.03833478316664696\n",
      "Epoch 13420/30000 Training Loss: 0.04347328469157219\n",
      "Epoch 13421/30000 Training Loss: 0.04650126397609711\n",
      "Epoch 13422/30000 Training Loss: 0.04218075051903725\n",
      "Epoch 13423/30000 Training Loss: 0.05515523627400398\n",
      "Epoch 13424/30000 Training Loss: 0.04471873119473457\n",
      "Epoch 13425/30000 Training Loss: 0.03916623070836067\n",
      "Epoch 13426/30000 Training Loss: 0.04118352383375168\n",
      "Epoch 13427/30000 Training Loss: 0.03986549377441406\n",
      "Epoch 13428/30000 Training Loss: 0.041002098470926285\n",
      "Epoch 13429/30000 Training Loss: 0.044260174036026\n",
      "Epoch 13430/30000 Training Loss: 0.044292714446783066\n",
      "Epoch 13431/30000 Training Loss: 0.04600031301379204\n",
      "Epoch 13432/30000 Training Loss: 0.044700738042593\n",
      "Epoch 13433/30000 Training Loss: 0.04851093143224716\n",
      "Epoch 13434/30000 Training Loss: 0.04243778437376022\n",
      "Epoch 13435/30000 Training Loss: 0.0523361936211586\n",
      "Epoch 13436/30000 Training Loss: 0.04064047336578369\n",
      "Epoch 13437/30000 Training Loss: 0.042776919901371\n",
      "Epoch 13438/30000 Training Loss: 0.040516410022974014\n",
      "Epoch 13439/30000 Training Loss: 0.045856915414333344\n",
      "Epoch 13440/30000 Training Loss: 0.04081742465496063\n",
      "Epoch 13441/30000 Training Loss: 0.042484086006879807\n",
      "Epoch 13442/30000 Training Loss: 0.04361672326922417\n",
      "Epoch 13443/30000 Training Loss: 0.0564366951584816\n",
      "Epoch 13444/30000 Training Loss: 0.04299833998084068\n",
      "Epoch 13445/30000 Training Loss: 0.041620176285505295\n",
      "Epoch 13446/30000 Training Loss: 0.04624008387327194\n",
      "Epoch 13447/30000 Training Loss: 0.04919064790010452\n",
      "Epoch 13448/30000 Training Loss: 0.04767819866538048\n",
      "Epoch 13449/30000 Training Loss: 0.039779603481292725\n",
      "Epoch 13450/30000 Training Loss: 0.04301312193274498\n",
      "Epoch 13450/30000 Validation Loss: 0.04174388572573662\n",
      "Epoch 13451/30000 Training Loss: 0.04476366192102432\n",
      "Epoch 13452/30000 Training Loss: 0.03926575928926468\n",
      "Epoch 13453/30000 Training Loss: 0.04106961190700531\n",
      "Epoch 13454/30000 Training Loss: 0.043977074325084686\n",
      "Epoch 13455/30000 Training Loss: 0.0498691126704216\n",
      "Epoch 13456/30000 Training Loss: 0.04039943590760231\n",
      "Epoch 13457/30000 Training Loss: 0.04539806768298149\n",
      "Epoch 13458/30000 Training Loss: 0.048372797667980194\n",
      "Epoch 13459/30000 Training Loss: 0.049222663044929504\n",
      "Epoch 13460/30000 Training Loss: 0.05231688544154167\n",
      "Epoch 13461/30000 Training Loss: 0.04407444968819618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13462/30000 Training Loss: 0.041163377463817596\n",
      "Epoch 13463/30000 Training Loss: 0.048059526830911636\n",
      "Epoch 13464/30000 Training Loss: 0.04332251101732254\n",
      "Epoch 13465/30000 Training Loss: 0.0468168742954731\n",
      "Epoch 13466/30000 Training Loss: 0.05157937481999397\n",
      "Epoch 13467/30000 Training Loss: 0.04893454909324646\n",
      "Epoch 13468/30000 Training Loss: 0.05087345838546753\n",
      "Epoch 13469/30000 Training Loss: 0.042367901653051376\n",
      "Epoch 13470/30000 Training Loss: 0.04390144720673561\n",
      "Epoch 13471/30000 Training Loss: 0.038168568164110184\n",
      "Epoch 13472/30000 Training Loss: 0.05498913675546646\n",
      "Epoch 13473/30000 Training Loss: 0.03636609762907028\n",
      "Epoch 13474/30000 Training Loss: 0.042864494025707245\n",
      "Epoch 13475/30000 Training Loss: 0.04466544836759567\n",
      "Epoch 13476/30000 Training Loss: 0.04108935594558716\n",
      "Epoch 13477/30000 Training Loss: 0.050744690001010895\n",
      "Epoch 13478/30000 Training Loss: 0.04213254526257515\n",
      "Epoch 13479/30000 Training Loss: 0.04439282417297363\n",
      "Epoch 13480/30000 Training Loss: 0.04258644953370094\n",
      "Epoch 13481/30000 Training Loss: 0.048676036298274994\n",
      "Epoch 13482/30000 Training Loss: 0.04120997339487076\n",
      "Epoch 13483/30000 Training Loss: 0.04769796133041382\n",
      "Epoch 13484/30000 Training Loss: 0.04387166351079941\n",
      "Epoch 13485/30000 Training Loss: 0.041169196367263794\n",
      "Epoch 13486/30000 Training Loss: 0.04595273360610008\n",
      "Epoch 13487/30000 Training Loss: 0.051268935203552246\n",
      "Epoch 13488/30000 Training Loss: 0.05164383724331856\n",
      "Epoch 13489/30000 Training Loss: 0.03686291724443436\n",
      "Epoch 13490/30000 Training Loss: 0.039951227605342865\n",
      "Epoch 13491/30000 Training Loss: 0.040821243077516556\n",
      "Epoch 13492/30000 Training Loss: 0.04476655274629593\n",
      "Epoch 13493/30000 Training Loss: 0.04778251796960831\n",
      "Epoch 13494/30000 Training Loss: 0.0362682081758976\n",
      "Epoch 13495/30000 Training Loss: 0.03944341465830803\n",
      "Epoch 13496/30000 Training Loss: 0.047416698187589645\n",
      "Epoch 13497/30000 Training Loss: 0.043950341641902924\n",
      "Epoch 13498/30000 Training Loss: 0.05148029327392578\n",
      "Epoch 13499/30000 Training Loss: 0.04450619965791702\n",
      "Epoch 13500/30000 Training Loss: 0.0481010340154171\n",
      "Epoch 13500/30000 Validation Loss: 0.045445527881383896\n",
      "Epoch 13501/30000 Training Loss: 0.05003891512751579\n",
      "Epoch 13502/30000 Training Loss: 0.049234695732593536\n",
      "Epoch 13503/30000 Training Loss: 0.06305403262376785\n",
      "Epoch 13504/30000 Training Loss: 0.044503651559352875\n",
      "Epoch 13505/30000 Training Loss: 0.04137939214706421\n",
      "Epoch 13506/30000 Training Loss: 0.041739098727703094\n",
      "Epoch 13507/30000 Training Loss: 0.042612481862306595\n",
      "Epoch 13508/30000 Training Loss: 0.0425250306725502\n",
      "Epoch 13509/30000 Training Loss: 0.04180045798420906\n",
      "Epoch 13510/30000 Training Loss: 0.04468743875622749\n",
      "Epoch 13511/30000 Training Loss: 0.043724171817302704\n",
      "Epoch 13512/30000 Training Loss: 0.045034922659397125\n",
      "Epoch 13513/30000 Training Loss: 0.0435822531580925\n",
      "Epoch 13514/30000 Training Loss: 0.04513666778802872\n",
      "Epoch 13515/30000 Training Loss: 0.046135127544403076\n",
      "Epoch 13516/30000 Training Loss: 0.035067662596702576\n",
      "Epoch 13517/30000 Training Loss: 0.0472739040851593\n",
      "Epoch 13518/30000 Training Loss: 0.04431643337011337\n",
      "Epoch 13519/30000 Training Loss: 0.04219234734773636\n",
      "Epoch 13520/30000 Training Loss: 0.04438818246126175\n",
      "Epoch 13521/30000 Training Loss: 0.03893087059259415\n",
      "Epoch 13522/30000 Training Loss: 0.04710565134882927\n",
      "Epoch 13523/30000 Training Loss: 0.04478982836008072\n",
      "Epoch 13524/30000 Training Loss: 0.03945481777191162\n",
      "Epoch 13525/30000 Training Loss: 0.046623293310403824\n",
      "Epoch 13526/30000 Training Loss: 0.04251629859209061\n",
      "Epoch 13527/30000 Training Loss: 0.03853162005543709\n",
      "Epoch 13528/30000 Training Loss: 0.050772301852703094\n",
      "Epoch 13529/30000 Training Loss: 0.04589863866567612\n",
      "Epoch 13530/30000 Training Loss: 0.043014686554670334\n",
      "Epoch 13531/30000 Training Loss: 0.05020436644554138\n",
      "Epoch 13532/30000 Training Loss: 0.04825233295559883\n",
      "Epoch 13533/30000 Training Loss: 0.0467800572514534\n",
      "Epoch 13534/30000 Training Loss: 0.04232930392026901\n",
      "Epoch 13535/30000 Training Loss: 0.049948472529649734\n",
      "Epoch 13536/30000 Training Loss: 0.04019301384687424\n",
      "Epoch 13537/30000 Training Loss: 0.04332002252340317\n",
      "Epoch 13538/30000 Training Loss: 0.052187204360961914\n",
      "Epoch 13539/30000 Training Loss: 0.043187301605939865\n",
      "Epoch 13540/30000 Training Loss: 0.04370308667421341\n",
      "Epoch 13541/30000 Training Loss: 0.038867078721523285\n",
      "Epoch 13542/30000 Training Loss: 0.04816805571317673\n",
      "Epoch 13543/30000 Training Loss: 0.03925180807709694\n",
      "Epoch 13544/30000 Training Loss: 0.05278526991605759\n",
      "Epoch 13545/30000 Training Loss: 0.039160702377557755\n",
      "Epoch 13546/30000 Training Loss: 0.040440719574689865\n",
      "Epoch 13547/30000 Training Loss: 0.04006660729646683\n",
      "Epoch 13548/30000 Training Loss: 0.04427499324083328\n",
      "Epoch 13549/30000 Training Loss: 0.04720257222652435\n",
      "Epoch 13550/30000 Training Loss: 0.050078146159648895\n",
      "Epoch 13550/30000 Validation Loss: 0.04773310199379921\n",
      "Epoch 13551/30000 Training Loss: 0.03933887556195259\n",
      "Epoch 13552/30000 Training Loss: 0.04072239249944687\n",
      "Epoch 13553/30000 Training Loss: 0.05297517776489258\n",
      "Epoch 13554/30000 Training Loss: 0.04369889944791794\n",
      "Epoch 13555/30000 Training Loss: 0.03511815518140793\n",
      "Epoch 13556/30000 Training Loss: 0.04284936189651489\n",
      "Epoch 13557/30000 Training Loss: 0.0505676344037056\n",
      "Epoch 13558/30000 Training Loss: 0.05188153311610222\n",
      "Epoch 13559/30000 Training Loss: 0.04572325199842453\n",
      "Epoch 13560/30000 Training Loss: 0.03770756721496582\n",
      "Epoch 13561/30000 Training Loss: 0.03973691910505295\n",
      "Epoch 13562/30000 Training Loss: 0.03952834755182266\n",
      "Epoch 13563/30000 Training Loss: 0.04107664152979851\n",
      "Epoch 13564/30000 Training Loss: 0.051015280187129974\n",
      "Epoch 13565/30000 Training Loss: 0.04937940835952759\n",
      "Epoch 13566/30000 Training Loss: 0.05103803426027298\n",
      "Epoch 13567/30000 Training Loss: 0.04459565132856369\n",
      "Epoch 13568/30000 Training Loss: 0.035950347781181335\n",
      "Epoch 13569/30000 Training Loss: 0.04110763221979141\n",
      "Epoch 13570/30000 Training Loss: 0.03921575844287872\n",
      "Epoch 13571/30000 Training Loss: 0.038483817130327225\n",
      "Epoch 13572/30000 Training Loss: 0.04636317864060402\n",
      "Epoch 13573/30000 Training Loss: 0.042082805186510086\n",
      "Epoch 13574/30000 Training Loss: 0.04734475165605545\n",
      "Epoch 13575/30000 Training Loss: 0.04389520362019539\n",
      "Epoch 13576/30000 Training Loss: 0.04944460466504097\n",
      "Epoch 13577/30000 Training Loss: 0.0443706214427948\n",
      "Epoch 13578/30000 Training Loss: 0.039082758128643036\n",
      "Epoch 13579/30000 Training Loss: 0.04413817822933197\n",
      "Epoch 13580/30000 Training Loss: 0.0396379753947258\n",
      "Epoch 13581/30000 Training Loss: 0.047339748591184616\n",
      "Epoch 13582/30000 Training Loss: 0.043546635657548904\n",
      "Epoch 13583/30000 Training Loss: 0.046115707606077194\n",
      "Epoch 13584/30000 Training Loss: 0.04220246151089668\n",
      "Epoch 13585/30000 Training Loss: 0.03643551468849182\n",
      "Epoch 13586/30000 Training Loss: 0.05054355412721634\n",
      "Epoch 13587/30000 Training Loss: 0.04898456111550331\n",
      "Epoch 13588/30000 Training Loss: 0.042338497936725616\n",
      "Epoch 13589/30000 Training Loss: 0.04563988372683525\n",
      "Epoch 13590/30000 Training Loss: 0.034256331622600555\n",
      "Epoch 13591/30000 Training Loss: 0.03997059538960457\n",
      "Epoch 13592/30000 Training Loss: 0.0407065749168396\n",
      "Epoch 13593/30000 Training Loss: 0.04358532279729843\n",
      "Epoch 13594/30000 Training Loss: 0.043999023735523224\n",
      "Epoch 13595/30000 Training Loss: 0.045051924884319305\n",
      "Epoch 13596/30000 Training Loss: 0.03445662930607796\n",
      "Epoch 13597/30000 Training Loss: 0.04187494516372681\n",
      "Epoch 13598/30000 Training Loss: 0.04853775352239609\n",
      "Epoch 13599/30000 Training Loss: 0.04755400866270065\n",
      "Epoch 13600/30000 Training Loss: 0.04131051152944565\n",
      "Epoch 13600/30000 Validation Loss: 0.043106913566589355\n",
      "Epoch 13601/30000 Training Loss: 0.04995469003915787\n",
      "Epoch 13602/30000 Training Loss: 0.03835070878267288\n",
      "Epoch 13603/30000 Training Loss: 0.0425003245472908\n",
      "Epoch 13604/30000 Training Loss: 0.05102391913533211\n",
      "Epoch 13605/30000 Training Loss: 0.049173496663570404\n",
      "Epoch 13606/30000 Training Loss: 0.04048861190676689\n",
      "Epoch 13607/30000 Training Loss: 0.050722502171993256\n",
      "Epoch 13608/30000 Training Loss: 0.04097966104745865\n",
      "Epoch 13609/30000 Training Loss: 0.04231713339686394\n",
      "Epoch 13610/30000 Training Loss: 0.041444532573223114\n",
      "Epoch 13611/30000 Training Loss: 0.0356915108859539\n",
      "Epoch 13612/30000 Training Loss: 0.043995361775159836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13613/30000 Training Loss: 0.045891109853982925\n",
      "Epoch 13614/30000 Training Loss: 0.05065333843231201\n",
      "Epoch 13615/30000 Training Loss: 0.04374369606375694\n",
      "Epoch 13616/30000 Training Loss: 0.04361587017774582\n",
      "Epoch 13617/30000 Training Loss: 0.04278476908802986\n",
      "Epoch 13618/30000 Training Loss: 0.053393274545669556\n",
      "Epoch 13619/30000 Training Loss: 0.046086400747299194\n",
      "Epoch 13620/30000 Training Loss: 0.0473102442920208\n",
      "Epoch 13621/30000 Training Loss: 0.042775366455316544\n",
      "Epoch 13622/30000 Training Loss: 0.04307685047388077\n",
      "Epoch 13623/30000 Training Loss: 0.04331904277205467\n",
      "Epoch 13624/30000 Training Loss: 0.048282112926244736\n",
      "Epoch 13625/30000 Training Loss: 0.04540625214576721\n",
      "Epoch 13626/30000 Training Loss: 0.047116879373788834\n",
      "Epoch 13627/30000 Training Loss: 0.03488217666745186\n",
      "Epoch 13628/30000 Training Loss: 0.040370356291532516\n",
      "Epoch 13629/30000 Training Loss: 0.047308821231126785\n",
      "Epoch 13630/30000 Training Loss: 0.047027647495269775\n",
      "Epoch 13631/30000 Training Loss: 0.039660919457674026\n",
      "Epoch 13632/30000 Training Loss: 0.03957914561033249\n",
      "Epoch 13633/30000 Training Loss: 0.046881433576345444\n",
      "Epoch 13634/30000 Training Loss: 0.0498170368373394\n",
      "Epoch 13635/30000 Training Loss: 0.050651777535676956\n",
      "Epoch 13636/30000 Training Loss: 0.047214575111866\n",
      "Epoch 13637/30000 Training Loss: 0.05096622556447983\n",
      "Epoch 13638/30000 Training Loss: 0.0421525202691555\n",
      "Epoch 13639/30000 Training Loss: 0.0509188249707222\n",
      "Epoch 13640/30000 Training Loss: 0.039162494242191315\n",
      "Epoch 13641/30000 Training Loss: 0.04085289686918259\n",
      "Epoch 13642/30000 Training Loss: 0.0453605130314827\n",
      "Epoch 13643/30000 Training Loss: 0.03999853879213333\n",
      "Epoch 13644/30000 Training Loss: 0.042950645089149475\n",
      "Epoch 13645/30000 Training Loss: 0.042897384613752365\n",
      "Epoch 13646/30000 Training Loss: 0.04340112954378128\n",
      "Epoch 13647/30000 Training Loss: 0.037485718727111816\n",
      "Epoch 13648/30000 Training Loss: 0.037803035229444504\n",
      "Epoch 13649/30000 Training Loss: 0.03927788510918617\n",
      "Epoch 13650/30000 Training Loss: 0.041453003883361816\n",
      "Epoch 13650/30000 Validation Loss: 0.04211035743355751\n",
      "Epoch 13651/30000 Training Loss: 0.0506657250225544\n",
      "Epoch 13652/30000 Training Loss: 0.047109656035900116\n",
      "Epoch 13653/30000 Training Loss: 0.04684446007013321\n",
      "Epoch 13654/30000 Training Loss: 0.04214795306324959\n",
      "Epoch 13655/30000 Training Loss: 0.05289861559867859\n",
      "Epoch 13656/30000 Training Loss: 0.047034308314323425\n",
      "Epoch 13657/30000 Training Loss: 0.04813715070486069\n",
      "Epoch 13658/30000 Training Loss: 0.04826187342405319\n",
      "Epoch 13659/30000 Training Loss: 0.041100263595581055\n",
      "Epoch 13660/30000 Training Loss: 0.03515862673521042\n",
      "Epoch 13661/30000 Training Loss: 0.04249442368745804\n",
      "Epoch 13662/30000 Training Loss: 0.04832989722490311\n",
      "Epoch 13663/30000 Training Loss: 0.04648885875940323\n",
      "Epoch 13664/30000 Training Loss: 0.0445588193833828\n",
      "Epoch 13665/30000 Training Loss: 0.03928511589765549\n",
      "Epoch 13666/30000 Training Loss: 0.043808840215206146\n",
      "Epoch 13667/30000 Training Loss: 0.047196295112371445\n",
      "Epoch 13668/30000 Training Loss: 0.045382458716630936\n",
      "Epoch 13669/30000 Training Loss: 0.04041598364710808\n",
      "Epoch 13670/30000 Training Loss: 0.04044245928525925\n",
      "Epoch 13671/30000 Training Loss: 0.03801488131284714\n",
      "Epoch 13672/30000 Training Loss: 0.039435382932424545\n",
      "Epoch 13673/30000 Training Loss: 0.04893382638692856\n",
      "Epoch 13674/30000 Training Loss: 0.04949982464313507\n",
      "Epoch 13675/30000 Training Loss: 0.03923474997282028\n",
      "Epoch 13676/30000 Training Loss: 0.04659407585859299\n",
      "Epoch 13677/30000 Training Loss: 0.04276135191321373\n",
      "Epoch 13678/30000 Training Loss: 0.04870038107037544\n",
      "Epoch 13679/30000 Training Loss: 0.033770203590393066\n",
      "Epoch 13680/30000 Training Loss: 0.05112827569246292\n",
      "Epoch 13681/30000 Training Loss: 0.045186251401901245\n",
      "Epoch 13682/30000 Training Loss: 0.03953137248754501\n",
      "Epoch 13683/30000 Training Loss: 0.03690957650542259\n",
      "Epoch 13684/30000 Training Loss: 0.04721393808722496\n",
      "Epoch 13685/30000 Training Loss: 0.03855300694704056\n",
      "Epoch 13686/30000 Training Loss: 0.048091012984514236\n",
      "Epoch 13687/30000 Training Loss: 0.058849215507507324\n",
      "Epoch 13688/30000 Training Loss: 0.04401745647192001\n",
      "Epoch 13689/30000 Training Loss: 0.04591391235589981\n",
      "Epoch 13690/30000 Training Loss: 0.048672761768102646\n",
      "Epoch 13691/30000 Training Loss: 0.04436546936631203\n",
      "Epoch 13692/30000 Training Loss: 0.03777077794075012\n",
      "Epoch 13693/30000 Training Loss: 0.04603980854153633\n",
      "Epoch 13694/30000 Training Loss: 0.04964737221598625\n",
      "Epoch 13695/30000 Training Loss: 0.038634441792964935\n",
      "Epoch 13696/30000 Training Loss: 0.03933250904083252\n",
      "Epoch 13697/30000 Training Loss: 0.05071417614817619\n",
      "Epoch 13698/30000 Training Loss: 0.04023677855730057\n",
      "Epoch 13699/30000 Training Loss: 0.043743934482336044\n",
      "Epoch 13700/30000 Training Loss: 0.05153653025627136\n",
      "Epoch 13700/30000 Validation Loss: 0.04458221048116684\n",
      "Epoch 13701/30000 Training Loss: 0.046048860996961594\n",
      "Epoch 13702/30000 Training Loss: 0.04227828234434128\n",
      "Epoch 13703/30000 Training Loss: 0.050281353294849396\n",
      "Epoch 13704/30000 Training Loss: 0.041256822645664215\n",
      "Epoch 13705/30000 Training Loss: 0.04342637583613396\n",
      "Epoch 13706/30000 Training Loss: 0.05762100964784622\n",
      "Epoch 13707/30000 Training Loss: 0.04374115914106369\n",
      "Epoch 13708/30000 Training Loss: 0.04610973596572876\n",
      "Epoch 13709/30000 Training Loss: 0.04039996117353439\n",
      "Epoch 13710/30000 Training Loss: 0.04573464393615723\n",
      "Epoch 13711/30000 Training Loss: 0.04770409315824509\n",
      "Epoch 13712/30000 Training Loss: 0.0410941056907177\n",
      "Epoch 13713/30000 Training Loss: 0.043559737503528595\n",
      "Epoch 13714/30000 Training Loss: 0.04277690500020981\n",
      "Epoch 13715/30000 Training Loss: 0.050452716648578644\n",
      "Epoch 13716/30000 Training Loss: 0.04123616963624954\n",
      "Epoch 13717/30000 Training Loss: 0.051280807703733444\n",
      "Epoch 13718/30000 Training Loss: 0.04881647601723671\n",
      "Epoch 13719/30000 Training Loss: 0.046580176800489426\n",
      "Epoch 13720/30000 Training Loss: 0.04479964077472687\n",
      "Epoch 13721/30000 Training Loss: 0.04521363973617554\n",
      "Epoch 13722/30000 Training Loss: 0.04164871573448181\n",
      "Epoch 13723/30000 Training Loss: 0.0403323620557785\n",
      "Epoch 13724/30000 Training Loss: 0.04362359270453453\n",
      "Epoch 13725/30000 Training Loss: 0.045214492827653885\n",
      "Epoch 13726/30000 Training Loss: 0.04057752713561058\n",
      "Epoch 13727/30000 Training Loss: 0.04574526101350784\n",
      "Epoch 13728/30000 Training Loss: 0.042634934186935425\n",
      "Epoch 13729/30000 Training Loss: 0.04723711311817169\n",
      "Epoch 13730/30000 Training Loss: 0.0430535189807415\n",
      "Epoch 13731/30000 Training Loss: 0.04104919359087944\n",
      "Epoch 13732/30000 Training Loss: 0.05123065039515495\n",
      "Epoch 13733/30000 Training Loss: 0.051306676119565964\n",
      "Epoch 13734/30000 Training Loss: 0.04945581033825874\n",
      "Epoch 13735/30000 Training Loss: 0.04808283597230911\n",
      "Epoch 13736/30000 Training Loss: 0.04085930064320564\n",
      "Epoch 13737/30000 Training Loss: 0.04001433774828911\n",
      "Epoch 13738/30000 Training Loss: 0.04296138882637024\n",
      "Epoch 13739/30000 Training Loss: 0.035066328942775726\n",
      "Epoch 13740/30000 Training Loss: 0.040074072778224945\n",
      "Epoch 13741/30000 Training Loss: 0.04675278812646866\n",
      "Epoch 13742/30000 Training Loss: 0.0480390265583992\n",
      "Epoch 13743/30000 Training Loss: 0.0447198860347271\n",
      "Epoch 13744/30000 Training Loss: 0.04216703772544861\n",
      "Epoch 13745/30000 Training Loss: 0.045959874987602234\n",
      "Epoch 13746/30000 Training Loss: 0.045217715203762054\n",
      "Epoch 13747/30000 Training Loss: 0.04394009709358215\n",
      "Epoch 13748/30000 Training Loss: 0.050528962165117264\n",
      "Epoch 13749/30000 Training Loss: 0.04514021426439285\n",
      "Epoch 13750/30000 Training Loss: 0.04372914507985115\n",
      "Epoch 13750/30000 Validation Loss: 0.03782850131392479\n",
      "Epoch 13751/30000 Training Loss: 0.04270234331488609\n",
      "Epoch 13752/30000 Training Loss: 0.04930352419614792\n",
      "Epoch 13753/30000 Training Loss: 0.04014492779970169\n",
      "Epoch 13754/30000 Training Loss: 0.04298543557524681\n",
      "Epoch 13755/30000 Training Loss: 0.04555044695734978\n",
      "Epoch 13756/30000 Training Loss: 0.04245765507221222\n",
      "Epoch 13757/30000 Training Loss: 0.04474768787622452\n",
      "Epoch 13758/30000 Training Loss: 0.052288927137851715\n",
      "Epoch 13759/30000 Training Loss: 0.03945200890302658\n",
      "Epoch 13760/30000 Training Loss: 0.05434034392237663\n",
      "Epoch 13761/30000 Training Loss: 0.05684814602136612\n",
      "Epoch 13762/30000 Training Loss: 0.045753225684165955\n",
      "Epoch 13763/30000 Training Loss: 0.04358617588877678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13764/30000 Training Loss: 0.053077198565006256\n",
      "Epoch 13765/30000 Training Loss: 0.038797102868556976\n",
      "Epoch 13766/30000 Training Loss: 0.03698856383562088\n",
      "Epoch 13767/30000 Training Loss: 0.04534279555082321\n",
      "Epoch 13768/30000 Training Loss: 0.04218708351254463\n",
      "Epoch 13769/30000 Training Loss: 0.0509701743721962\n",
      "Epoch 13770/30000 Training Loss: 0.04766647145152092\n",
      "Epoch 13771/30000 Training Loss: 0.04785909131169319\n",
      "Epoch 13772/30000 Training Loss: 0.047574497759342194\n",
      "Epoch 13773/30000 Training Loss: 0.04026251286268234\n",
      "Epoch 13774/30000 Training Loss: 0.04170159623026848\n",
      "Epoch 13775/30000 Training Loss: 0.04363038390874863\n",
      "Epoch 13776/30000 Training Loss: 0.04270593822002411\n",
      "Epoch 13777/30000 Training Loss: 0.04264838248491287\n",
      "Epoch 13778/30000 Training Loss: 0.04868003726005554\n",
      "Epoch 13779/30000 Training Loss: 0.040420688688755035\n",
      "Epoch 13780/30000 Training Loss: 0.03749041259288788\n",
      "Epoch 13781/30000 Training Loss: 0.054033853113651276\n",
      "Epoch 13782/30000 Training Loss: 0.04076363146305084\n",
      "Epoch 13783/30000 Training Loss: 0.032841600477695465\n",
      "Epoch 13784/30000 Training Loss: 0.05667831376194954\n",
      "Epoch 13785/30000 Training Loss: 0.053164880722761154\n",
      "Epoch 13786/30000 Training Loss: 0.03617379069328308\n",
      "Epoch 13787/30000 Training Loss: 0.0353357195854187\n",
      "Epoch 13788/30000 Training Loss: 0.04034697264432907\n",
      "Epoch 13789/30000 Training Loss: 0.05249534174799919\n",
      "Epoch 13790/30000 Training Loss: 0.04445233568549156\n",
      "Epoch 13791/30000 Training Loss: 0.042219989001750946\n",
      "Epoch 13792/30000 Training Loss: 0.03943382948637009\n",
      "Epoch 13793/30000 Training Loss: 0.04627168923616409\n",
      "Epoch 13794/30000 Training Loss: 0.04774852842092514\n",
      "Epoch 13795/30000 Training Loss: 0.042011238634586334\n",
      "Epoch 13796/30000 Training Loss: 0.03362327069044113\n",
      "Epoch 13797/30000 Training Loss: 0.046291545033454895\n",
      "Epoch 13798/30000 Training Loss: 0.04548101872205734\n",
      "Epoch 13799/30000 Training Loss: 0.04416946321725845\n",
      "Epoch 13800/30000 Training Loss: 0.03870454430580139\n",
      "Epoch 13800/30000 Validation Loss: 0.03878317400813103\n",
      "Epoch 13801/30000 Training Loss: 0.05596260353922844\n",
      "Epoch 13802/30000 Training Loss: 0.04446184262633324\n",
      "Epoch 13803/30000 Training Loss: 0.047434836626052856\n",
      "Epoch 13804/30000 Training Loss: 0.03541264683008194\n",
      "Epoch 13805/30000 Training Loss: 0.048309825360774994\n",
      "Epoch 13806/30000 Training Loss: 0.03769499808549881\n",
      "Epoch 13807/30000 Training Loss: 0.05291639640927315\n",
      "Epoch 13808/30000 Training Loss: 0.04191211611032486\n",
      "Epoch 13809/30000 Training Loss: 0.04255622997879982\n",
      "Epoch 13810/30000 Training Loss: 0.04668223485350609\n",
      "Epoch 13811/30000 Training Loss: 0.03844168037176132\n",
      "Epoch 13812/30000 Training Loss: 0.04858258739113808\n",
      "Epoch 13813/30000 Training Loss: 0.04229273647069931\n",
      "Epoch 13814/30000 Training Loss: 0.0434308722615242\n",
      "Epoch 13815/30000 Training Loss: 0.0470387265086174\n",
      "Epoch 13816/30000 Training Loss: 0.05268184095621109\n",
      "Epoch 13817/30000 Training Loss: 0.05071469023823738\n",
      "Epoch 13818/30000 Training Loss: 0.047389768064022064\n",
      "Epoch 13819/30000 Training Loss: 0.049302175641059875\n",
      "Epoch 13820/30000 Training Loss: 0.03565020114183426\n",
      "Epoch 13821/30000 Training Loss: 0.04621816426515579\n",
      "Epoch 13822/30000 Training Loss: 0.04556075483560562\n",
      "Epoch 13823/30000 Training Loss: 0.03907174989581108\n",
      "Epoch 13824/30000 Training Loss: 0.04350550100207329\n",
      "Epoch 13825/30000 Training Loss: 0.043910712003707886\n",
      "Epoch 13826/30000 Training Loss: 0.044785745441913605\n",
      "Epoch 13827/30000 Training Loss: 0.04579142853617668\n",
      "Epoch 13828/30000 Training Loss: 0.04136379808187485\n",
      "Epoch 13829/30000 Training Loss: 0.0454365611076355\n",
      "Epoch 13830/30000 Training Loss: 0.04506794363260269\n",
      "Epoch 13831/30000 Training Loss: 0.04395337030291557\n",
      "Epoch 13832/30000 Training Loss: 0.047048427164554596\n",
      "Epoch 13833/30000 Training Loss: 0.03988724201917648\n",
      "Epoch 13834/30000 Training Loss: 0.04341580718755722\n",
      "Epoch 13835/30000 Training Loss: 0.03909434750676155\n",
      "Epoch 13836/30000 Training Loss: 0.044111259281635284\n",
      "Epoch 13837/30000 Training Loss: 0.05023408681154251\n",
      "Epoch 13838/30000 Training Loss: 0.04636319726705551\n",
      "Epoch 13839/30000 Training Loss: 0.05150841921567917\n",
      "Epoch 13840/30000 Training Loss: 0.047031670808792114\n",
      "Epoch 13841/30000 Training Loss: 0.04736453667283058\n",
      "Epoch 13842/30000 Training Loss: 0.052193619310855865\n",
      "Epoch 13843/30000 Training Loss: 0.04872770234942436\n",
      "Epoch 13844/30000 Training Loss: 0.04794210195541382\n",
      "Epoch 13845/30000 Training Loss: 0.04817019775509834\n",
      "Epoch 13846/30000 Training Loss: 0.04827610403299332\n",
      "Epoch 13847/30000 Training Loss: 0.04192658141255379\n",
      "Epoch 13848/30000 Training Loss: 0.04246264696121216\n",
      "Epoch 13849/30000 Training Loss: 0.04942344129085541\n",
      "Epoch 13850/30000 Training Loss: 0.05170048400759697\n",
      "Epoch 13850/30000 Validation Loss: 0.041052285581827164\n",
      "Epoch 13851/30000 Training Loss: 0.044861987233161926\n",
      "Epoch 13852/30000 Training Loss: 0.04015963152050972\n",
      "Epoch 13853/30000 Training Loss: 0.0421038493514061\n",
      "Epoch 13854/30000 Training Loss: 0.044445715844631195\n",
      "Epoch 13855/30000 Training Loss: 0.04788589105010033\n",
      "Epoch 13856/30000 Training Loss: 0.040823690593242645\n",
      "Epoch 13857/30000 Training Loss: 0.04894855245947838\n",
      "Epoch 13858/30000 Training Loss: 0.04631569981575012\n",
      "Epoch 13859/30000 Training Loss: 0.051415957510471344\n",
      "Epoch 13860/30000 Training Loss: 0.04734335094690323\n",
      "Epoch 13861/30000 Training Loss: 0.04000615328550339\n",
      "Epoch 13862/30000 Training Loss: 0.05195651203393936\n",
      "Epoch 13863/30000 Training Loss: 0.038678091019392014\n",
      "Epoch 13864/30000 Training Loss: 0.04612981155514717\n",
      "Epoch 13865/30000 Training Loss: 0.04430759698152542\n",
      "Epoch 13866/30000 Training Loss: 0.03789035975933075\n",
      "Epoch 13867/30000 Training Loss: 0.034717775881290436\n",
      "Epoch 13868/30000 Training Loss: 0.04677627235651016\n",
      "Epoch 13869/30000 Training Loss: 0.03986441716551781\n",
      "Epoch 13870/30000 Training Loss: 0.03872054070234299\n",
      "Epoch 13871/30000 Training Loss: 0.0417056605219841\n",
      "Epoch 13872/30000 Training Loss: 0.04119228199124336\n",
      "Epoch 13873/30000 Training Loss: 0.03887353464961052\n",
      "Epoch 13874/30000 Training Loss: 0.04649985581636429\n",
      "Epoch 13875/30000 Training Loss: 0.04255298897624016\n",
      "Epoch 13876/30000 Training Loss: 0.04272080212831497\n",
      "Epoch 13877/30000 Training Loss: 0.04542691260576248\n",
      "Epoch 13878/30000 Training Loss: 0.04484667256474495\n",
      "Epoch 13879/30000 Training Loss: 0.04409528523683548\n",
      "Epoch 13880/30000 Training Loss: 0.049722373485565186\n",
      "Epoch 13881/30000 Training Loss: 0.04149005562067032\n",
      "Epoch 13882/30000 Training Loss: 0.04616105556488037\n",
      "Epoch 13883/30000 Training Loss: 0.045458681881427765\n",
      "Epoch 13884/30000 Training Loss: 0.04309123381972313\n",
      "Epoch 13885/30000 Training Loss: 0.050599776208400726\n",
      "Epoch 13886/30000 Training Loss: 0.04521317034959793\n",
      "Epoch 13887/30000 Training Loss: 0.040481261909008026\n",
      "Epoch 13888/30000 Training Loss: 0.04403812438249588\n",
      "Epoch 13889/30000 Training Loss: 0.037585485726594925\n",
      "Epoch 13890/30000 Training Loss: 0.05186883732676506\n",
      "Epoch 13891/30000 Training Loss: 0.03957642614841461\n",
      "Epoch 13892/30000 Training Loss: 0.04690020903944969\n",
      "Epoch 13893/30000 Training Loss: 0.04154477268457413\n",
      "Epoch 13894/30000 Training Loss: 0.04623977094888687\n",
      "Epoch 13895/30000 Training Loss: 0.043513815850019455\n",
      "Epoch 13896/30000 Training Loss: 0.03416081517934799\n",
      "Epoch 13897/30000 Training Loss: 0.05201896280050278\n",
      "Epoch 13898/30000 Training Loss: 0.03391990065574646\n",
      "Epoch 13899/30000 Training Loss: 0.038814522325992584\n",
      "Epoch 13900/30000 Training Loss: 0.04018507897853851\n",
      "Epoch 13900/30000 Validation Loss: 0.05250983312726021\n",
      "Epoch 13901/30000 Training Loss: 0.03818190470337868\n",
      "Epoch 13902/30000 Training Loss: 0.04045175015926361\n",
      "Epoch 13903/30000 Training Loss: 0.050422705709934235\n",
      "Epoch 13904/30000 Training Loss: 0.03862711414694786\n",
      "Epoch 13905/30000 Training Loss: 0.046350158751010895\n",
      "Epoch 13906/30000 Training Loss: 0.04424477741122246\n",
      "Epoch 13907/30000 Training Loss: 0.0381966233253479\n",
      "Epoch 13908/30000 Training Loss: 0.04606326296925545\n",
      "Epoch 13909/30000 Training Loss: 0.043976716697216034\n",
      "Epoch 13910/30000 Training Loss: 0.037151362746953964\n",
      "Epoch 13911/30000 Training Loss: 0.04027567431330681\n",
      "Epoch 13912/30000 Training Loss: 0.044174082577228546\n",
      "Epoch 13913/30000 Training Loss: 0.04131453111767769\n",
      "Epoch 13914/30000 Training Loss: 0.04082034155726433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13915/30000 Training Loss: 0.040505632758140564\n",
      "Epoch 13916/30000 Training Loss: 0.04076918959617615\n",
      "Epoch 13917/30000 Training Loss: 0.03884517028927803\n",
      "Epoch 13918/30000 Training Loss: 0.04847526177763939\n",
      "Epoch 13919/30000 Training Loss: 0.04415542632341385\n",
      "Epoch 13920/30000 Training Loss: 0.03874065726995468\n",
      "Epoch 13921/30000 Training Loss: 0.04737278074026108\n",
      "Epoch 13922/30000 Training Loss: 0.037750739604234695\n",
      "Epoch 13923/30000 Training Loss: 0.04608305171132088\n",
      "Epoch 13924/30000 Training Loss: 0.045689843595027924\n",
      "Epoch 13925/30000 Training Loss: 0.04010365903377533\n",
      "Epoch 13926/30000 Training Loss: 0.04614342749118805\n",
      "Epoch 13927/30000 Training Loss: 0.03927870839834213\n",
      "Epoch 13928/30000 Training Loss: 0.045870985835790634\n",
      "Epoch 13929/30000 Training Loss: 0.04568757861852646\n",
      "Epoch 13930/30000 Training Loss: 0.03796360269188881\n",
      "Epoch 13931/30000 Training Loss: 0.0411306656897068\n",
      "Epoch 13932/30000 Training Loss: 0.04599015414714813\n",
      "Epoch 13933/30000 Training Loss: 0.047154877334833145\n",
      "Epoch 13934/30000 Training Loss: 0.048552386462688446\n",
      "Epoch 13935/30000 Training Loss: 0.03901638835668564\n",
      "Epoch 13936/30000 Training Loss: 0.043010398745536804\n",
      "Epoch 13937/30000 Training Loss: 0.03630813583731651\n",
      "Epoch 13938/30000 Training Loss: 0.05035487934947014\n",
      "Epoch 13939/30000 Training Loss: 0.04741479083895683\n",
      "Epoch 13940/30000 Training Loss: 0.04220928996801376\n",
      "Epoch 13941/30000 Training Loss: 0.03996304050087929\n",
      "Epoch 13942/30000 Training Loss: 0.045731328427791595\n",
      "Epoch 13943/30000 Training Loss: 0.047026704996824265\n",
      "Epoch 13944/30000 Training Loss: 0.04428737610578537\n",
      "Epoch 13945/30000 Training Loss: 0.04673326760530472\n",
      "Epoch 13946/30000 Training Loss: 0.051220107823610306\n",
      "Epoch 13947/30000 Training Loss: 0.04282280430197716\n",
      "Epoch 13948/30000 Training Loss: 0.05143404006958008\n",
      "Epoch 13949/30000 Training Loss: 0.03692983463406563\n",
      "Epoch 13950/30000 Training Loss: 0.048844434320926666\n",
      "Epoch 13950/30000 Validation Loss: 0.05007041618227959\n",
      "Epoch 13951/30000 Training Loss: 0.04463476687669754\n",
      "Epoch 13952/30000 Training Loss: 0.04407209903001785\n",
      "Epoch 13953/30000 Training Loss: 0.04049622267484665\n",
      "Epoch 13954/30000 Training Loss: 0.04568538814783096\n",
      "Epoch 13955/30000 Training Loss: 0.042876556515693665\n",
      "Epoch 13956/30000 Training Loss: 0.048727672547101974\n",
      "Epoch 13957/30000 Training Loss: 0.03955059498548508\n",
      "Epoch 13958/30000 Training Loss: 0.0360708050429821\n",
      "Epoch 13959/30000 Training Loss: 0.048469413071870804\n",
      "Epoch 13960/30000 Training Loss: 0.03619297593832016\n",
      "Epoch 13961/30000 Training Loss: 0.0373055636882782\n",
      "Epoch 13962/30000 Training Loss: 0.038544911891222\n",
      "Epoch 13963/30000 Training Loss: 0.04649433493614197\n",
      "Epoch 13964/30000 Training Loss: 0.04229477047920227\n",
      "Epoch 13965/30000 Training Loss: 0.04049744829535484\n",
      "Epoch 13966/30000 Training Loss: 0.03882038593292236\n",
      "Epoch 13967/30000 Training Loss: 0.04415542632341385\n",
      "Epoch 13968/30000 Training Loss: 0.04511293023824692\n",
      "Epoch 13969/30000 Training Loss: 0.04095439240336418\n",
      "Epoch 13970/30000 Training Loss: 0.03859134390950203\n",
      "Epoch 13971/30000 Training Loss: 0.04825008660554886\n",
      "Epoch 13972/30000 Training Loss: 0.043379753828048706\n",
      "Epoch 13973/30000 Training Loss: 0.04139385372400284\n",
      "Epoch 13974/30000 Training Loss: 0.04452304169535637\n",
      "Epoch 13975/30000 Training Loss: 0.04187668114900589\n",
      "Epoch 13976/30000 Training Loss: 0.03944820538163185\n",
      "Epoch 13977/30000 Training Loss: 0.049706604331731796\n",
      "Epoch 13978/30000 Training Loss: 0.046364542096853256\n",
      "Epoch 13979/30000 Training Loss: 0.03593149036169052\n",
      "Epoch 13980/30000 Training Loss: 0.046602748334407806\n",
      "Epoch 13981/30000 Training Loss: 0.03901819512248039\n",
      "Epoch 13982/30000 Training Loss: 0.03860461339354515\n",
      "Epoch 13983/30000 Training Loss: 0.041196905076503754\n",
      "Epoch 13984/30000 Training Loss: 0.039099741727113724\n",
      "Epoch 13985/30000 Training Loss: 0.04019460827112198\n",
      "Epoch 13986/30000 Training Loss: 0.053663469851017\n",
      "Epoch 13987/30000 Training Loss: 0.04657992720603943\n",
      "Epoch 13988/30000 Training Loss: 0.03780718520283699\n",
      "Epoch 13989/30000 Training Loss: 0.048271648585796356\n",
      "Epoch 13990/30000 Training Loss: 0.05032218620181084\n",
      "Epoch 13991/30000 Training Loss: 0.04136163368821144\n",
      "Epoch 13992/30000 Training Loss: 0.04217175021767616\n",
      "Epoch 13993/30000 Training Loss: 0.044783689081668854\n",
      "Epoch 13994/30000 Training Loss: 0.04016980901360512\n",
      "Epoch 13995/30000 Training Loss: 0.054809607565402985\n",
      "Epoch 13996/30000 Training Loss: 0.0443970151245594\n",
      "Epoch 13997/30000 Training Loss: 0.04256504029035568\n",
      "Epoch 13998/30000 Training Loss: 0.05522116273641586\n",
      "Epoch 13999/30000 Training Loss: 0.043135762214660645\n",
      "Epoch 14000/30000 Training Loss: 0.042704541236162186\n",
      "Epoch 14000/30000 Validation Loss: 0.04319608956575394\n",
      "Epoch 14001/30000 Training Loss: 0.04058389738202095\n",
      "Epoch 14002/30000 Training Loss: 0.05181630328297615\n",
      "Epoch 14003/30000 Training Loss: 0.04361769184470177\n",
      "Epoch 14004/30000 Training Loss: 0.03998524323105812\n",
      "Epoch 14005/30000 Training Loss: 0.050727106630802155\n",
      "Epoch 14006/30000 Training Loss: 0.0405271090567112\n",
      "Epoch 14007/30000 Training Loss: 0.043949417769908905\n",
      "Epoch 14008/30000 Training Loss: 0.04358149319887161\n",
      "Epoch 14009/30000 Training Loss: 0.04556887224316597\n",
      "Epoch 14010/30000 Training Loss: 0.043238554149866104\n",
      "Epoch 14011/30000 Training Loss: 0.043587226420640945\n",
      "Epoch 14012/30000 Training Loss: 0.04354828968644142\n",
      "Epoch 14013/30000 Training Loss: 0.04022941738367081\n",
      "Epoch 14014/30000 Training Loss: 0.038690079003572464\n",
      "Epoch 14015/30000 Training Loss: 0.04303616285324097\n",
      "Epoch 14016/30000 Training Loss: 0.045995842665433884\n",
      "Epoch 14017/30000 Training Loss: 0.038233060389757156\n",
      "Epoch 14018/30000 Training Loss: 0.03994615748524666\n",
      "Epoch 14019/30000 Training Loss: 0.04505912587046623\n",
      "Epoch 14020/30000 Training Loss: 0.044889211654663086\n",
      "Epoch 14021/30000 Training Loss: 0.033183496445417404\n",
      "Epoch 14022/30000 Training Loss: 0.042015548795461655\n",
      "Epoch 14023/30000 Training Loss: 0.04020640254020691\n",
      "Epoch 14024/30000 Training Loss: 0.043738748878240585\n",
      "Epoch 14025/30000 Training Loss: 0.050329696387052536\n",
      "Epoch 14026/30000 Training Loss: 0.05785820633172989\n",
      "Epoch 14027/30000 Training Loss: 0.0355684719979763\n",
      "Epoch 14028/30000 Training Loss: 0.037699270993471146\n",
      "Epoch 14029/30000 Training Loss: 0.04172419756650925\n",
      "Epoch 14030/30000 Training Loss: 0.031575385481119156\n",
      "Epoch 14031/30000 Training Loss: 0.04826708510518074\n",
      "Epoch 14032/30000 Training Loss: 0.06329881399869919\n",
      "Epoch 14033/30000 Training Loss: 0.03479956090450287\n",
      "Epoch 14034/30000 Training Loss: 0.04005451872944832\n",
      "Epoch 14035/30000 Training Loss: 0.04093167930841446\n",
      "Epoch 14036/30000 Training Loss: 0.04699200391769409\n",
      "Epoch 14037/30000 Training Loss: 0.03863026201725006\n",
      "Epoch 14038/30000 Training Loss: 0.044478751718997955\n",
      "Epoch 14039/30000 Training Loss: 0.04374825209379196\n",
      "Epoch 14040/30000 Training Loss: 0.04092961177229881\n",
      "Epoch 14041/30000 Training Loss: 0.04305383190512657\n",
      "Epoch 14042/30000 Training Loss: 0.049758005887269974\n",
      "Epoch 14043/30000 Training Loss: 0.04162607342004776\n",
      "Epoch 14044/30000 Training Loss: 0.04517868161201477\n",
      "Epoch 14045/30000 Training Loss: 0.03616510331630707\n",
      "Epoch 14046/30000 Training Loss: 0.03933071345090866\n",
      "Epoch 14047/30000 Training Loss: 0.046956866979599\n",
      "Epoch 14048/30000 Training Loss: 0.04439396783709526\n",
      "Epoch 14049/30000 Training Loss: 0.04643753170967102\n",
      "Epoch 14050/30000 Training Loss: 0.042467210441827774\n",
      "Epoch 14050/30000 Validation Loss: 0.04454111307859421\n",
      "Epoch 14051/30000 Training Loss: 0.03466605767607689\n",
      "Epoch 14052/30000 Training Loss: 0.04454416781663895\n",
      "Epoch 14053/30000 Training Loss: 0.04597076028585434\n",
      "Epoch 14054/30000 Training Loss: 0.05055657774209976\n",
      "Epoch 14055/30000 Training Loss: 0.044418297708034515\n",
      "Epoch 14056/30000 Training Loss: 0.04624760523438454\n",
      "Epoch 14057/30000 Training Loss: 0.04502413421869278\n",
      "Epoch 14058/30000 Training Loss: 0.04399702697992325\n",
      "Epoch 14059/30000 Training Loss: 0.04134383797645569\n",
      "Epoch 14060/30000 Training Loss: 0.04940510541200638\n",
      "Epoch 14061/30000 Training Loss: 0.03992726653814316\n",
      "Epoch 14062/30000 Training Loss: 0.04414043202996254\n",
      "Epoch 14063/30000 Training Loss: 0.04113798961043358\n",
      "Epoch 14064/30000 Training Loss: 0.05057539790868759\n",
      "Epoch 14065/30000 Training Loss: 0.04076673835515976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14066/30000 Training Loss: 0.04015093296766281\n",
      "Epoch 14067/30000 Training Loss: 0.03351666033267975\n",
      "Epoch 14068/30000 Training Loss: 0.04088428616523743\n",
      "Epoch 14069/30000 Training Loss: 0.0455465130507946\n",
      "Epoch 14070/30000 Training Loss: 0.03445722907781601\n",
      "Epoch 14071/30000 Training Loss: 0.04378976300358772\n",
      "Epoch 14072/30000 Training Loss: 0.04735994338989258\n",
      "Epoch 14073/30000 Training Loss: 0.03388414531946182\n",
      "Epoch 14074/30000 Training Loss: 0.05514556169509888\n",
      "Epoch 14075/30000 Training Loss: 0.05328500270843506\n",
      "Epoch 14076/30000 Training Loss: 0.04663069173693657\n",
      "Epoch 14077/30000 Training Loss: 0.04334212839603424\n",
      "Epoch 14078/30000 Training Loss: 0.040742941200733185\n",
      "Epoch 14079/30000 Training Loss: 0.037397608160972595\n",
      "Epoch 14080/30000 Training Loss: 0.043552789837121964\n",
      "Epoch 14081/30000 Training Loss: 0.04416115954518318\n",
      "Epoch 14082/30000 Training Loss: 0.0423562154173851\n",
      "Epoch 14083/30000 Training Loss: 0.0454629585146904\n",
      "Epoch 14084/30000 Training Loss: 0.04461419954895973\n",
      "Epoch 14085/30000 Training Loss: 0.049198996275663376\n",
      "Epoch 14086/30000 Training Loss: 0.04520421102643013\n",
      "Epoch 14087/30000 Training Loss: 0.04699259251356125\n",
      "Epoch 14088/30000 Training Loss: 0.045419760048389435\n",
      "Epoch 14089/30000 Training Loss: 0.0377042256295681\n",
      "Epoch 14090/30000 Training Loss: 0.03726352006196976\n",
      "Epoch 14091/30000 Training Loss: 0.04640592262148857\n",
      "Epoch 14092/30000 Training Loss: 0.04254074767231941\n",
      "Epoch 14093/30000 Training Loss: 0.04221906140446663\n",
      "Epoch 14094/30000 Training Loss: 0.053901076316833496\n",
      "Epoch 14095/30000 Training Loss: 0.04998919740319252\n",
      "Epoch 14096/30000 Training Loss: 0.04869214445352554\n",
      "Epoch 14097/30000 Training Loss: 0.0402127243578434\n",
      "Epoch 14098/30000 Training Loss: 0.03501605615019798\n",
      "Epoch 14099/30000 Training Loss: 0.047806233167648315\n",
      "Epoch 14100/30000 Training Loss: 0.046797484159469604\n",
      "Epoch 14100/30000 Validation Loss: 0.04593122377991676\n",
      "Epoch 14101/30000 Training Loss: 0.03921270743012428\n",
      "Epoch 14102/30000 Training Loss: 0.0459812693297863\n",
      "Epoch 14103/30000 Training Loss: 0.04811408370733261\n",
      "Epoch 14104/30000 Training Loss: 0.04324270039796829\n",
      "Epoch 14105/30000 Training Loss: 0.04403836280107498\n",
      "Epoch 14106/30000 Training Loss: 0.04621930792927742\n",
      "Epoch 14107/30000 Training Loss: 0.04583100229501724\n",
      "Epoch 14108/30000 Training Loss: 0.050044335424900055\n",
      "Epoch 14109/30000 Training Loss: 0.0480937659740448\n",
      "Epoch 14110/30000 Training Loss: 0.0440281443297863\n",
      "Epoch 14111/30000 Training Loss: 0.053144097328186035\n",
      "Epoch 14112/30000 Training Loss: 0.04792257770895958\n",
      "Epoch 14113/30000 Training Loss: 0.0413331463932991\n",
      "Epoch 14114/30000 Training Loss: 0.046630166471004486\n",
      "Epoch 14115/30000 Training Loss: 0.04648361727595329\n",
      "Epoch 14116/30000 Training Loss: 0.048656564205884933\n",
      "Epoch 14117/30000 Training Loss: 0.03879594802856445\n",
      "Epoch 14118/30000 Training Loss: 0.0390508659183979\n",
      "Epoch 14119/30000 Training Loss: 0.035764507949352264\n",
      "Epoch 14120/30000 Training Loss: 0.04270147159695625\n",
      "Epoch 14121/30000 Training Loss: 0.04644623398780823\n",
      "Epoch 14122/30000 Training Loss: 0.037518855184316635\n",
      "Epoch 14123/30000 Training Loss: 0.03770942613482475\n",
      "Epoch 14124/30000 Training Loss: 0.045658715069293976\n",
      "Epoch 14125/30000 Training Loss: 0.045897506177425385\n",
      "Epoch 14126/30000 Training Loss: 0.042545996606349945\n",
      "Epoch 14127/30000 Training Loss: 0.03677007183432579\n",
      "Epoch 14128/30000 Training Loss: 0.0377858504652977\n",
      "Epoch 14129/30000 Training Loss: 0.043433237820863724\n",
      "Epoch 14130/30000 Training Loss: 0.04129559546709061\n",
      "Epoch 14131/30000 Training Loss: 0.0493001788854599\n",
      "Epoch 14132/30000 Training Loss: 0.053736548870801926\n",
      "Epoch 14133/30000 Training Loss: 0.04327697306871414\n",
      "Epoch 14134/30000 Training Loss: 0.03785526752471924\n",
      "Epoch 14135/30000 Training Loss: 0.03923129290342331\n",
      "Epoch 14136/30000 Training Loss: 0.04761992394924164\n",
      "Epoch 14137/30000 Training Loss: 0.04947764426469803\n",
      "Epoch 14138/30000 Training Loss: 0.04121405631303787\n",
      "Epoch 14139/30000 Training Loss: 0.04372548684477806\n",
      "Epoch 14140/30000 Training Loss: 0.04570946842432022\n",
      "Epoch 14141/30000 Training Loss: 0.051927901804447174\n",
      "Epoch 14142/30000 Training Loss: 0.038314271718263626\n",
      "Epoch 14143/30000 Training Loss: 0.0366862490773201\n",
      "Epoch 14144/30000 Training Loss: 0.05372162535786629\n",
      "Epoch 14145/30000 Training Loss: 0.0355161614716053\n",
      "Epoch 14146/30000 Training Loss: 0.04074893519282341\n",
      "Epoch 14147/30000 Training Loss: 0.04940846562385559\n",
      "Epoch 14148/30000 Training Loss: 0.040565259754657745\n",
      "Epoch 14149/30000 Training Loss: 0.04439932852983475\n",
      "Epoch 14150/30000 Training Loss: 0.040594685822725296\n",
      "Epoch 14150/30000 Validation Loss: 0.04502551630139351\n",
      "Epoch 14151/30000 Training Loss: 0.05085771530866623\n",
      "Epoch 14152/30000 Training Loss: 0.04401908069849014\n",
      "Epoch 14153/30000 Training Loss: 0.04733515903353691\n",
      "Epoch 14154/30000 Training Loss: 0.04060358181595802\n",
      "Epoch 14155/30000 Training Loss: 0.052786268293857574\n",
      "Epoch 14156/30000 Training Loss: 0.04594970494508743\n",
      "Epoch 14157/30000 Training Loss: 0.04067927598953247\n",
      "Epoch 14158/30000 Training Loss: 0.040927816182374954\n",
      "Epoch 14159/30000 Training Loss: 0.040429916232824326\n",
      "Epoch 14160/30000 Training Loss: 0.04387080669403076\n",
      "Epoch 14161/30000 Training Loss: 0.04971036687493324\n",
      "Epoch 14162/30000 Training Loss: 0.0400601364672184\n",
      "Epoch 14163/30000 Training Loss: 0.047611258924007416\n",
      "Epoch 14164/30000 Training Loss: 0.051291413605213165\n",
      "Epoch 14165/30000 Training Loss: 0.04273443669080734\n",
      "Epoch 14166/30000 Training Loss: 0.04502285271883011\n",
      "Epoch 14167/30000 Training Loss: 0.04327043518424034\n",
      "Epoch 14168/30000 Training Loss: 0.045605387538671494\n",
      "Epoch 14169/30000 Training Loss: 0.05292751267552376\n",
      "Epoch 14170/30000 Training Loss: 0.05962458997964859\n",
      "Epoch 14171/30000 Training Loss: 0.03814589977264404\n",
      "Epoch 14172/30000 Training Loss: 0.05126045271754265\n",
      "Epoch 14173/30000 Training Loss: 0.0504840686917305\n",
      "Epoch 14174/30000 Training Loss: 0.03987015411257744\n",
      "Epoch 14175/30000 Training Loss: 0.04388808459043503\n",
      "Epoch 14176/30000 Training Loss: 0.04770208150148392\n",
      "Epoch 14177/30000 Training Loss: 0.03848984092473984\n",
      "Epoch 14178/30000 Training Loss: 0.045134127140045166\n",
      "Epoch 14179/30000 Training Loss: 0.05784447863698006\n",
      "Epoch 14180/30000 Training Loss: 0.03974851965904236\n",
      "Epoch 14181/30000 Training Loss: 0.04517308995127678\n",
      "Epoch 14182/30000 Training Loss: 0.041822101920843124\n",
      "Epoch 14183/30000 Training Loss: 0.04372522234916687\n",
      "Epoch 14184/30000 Training Loss: 0.04882942512631416\n",
      "Epoch 14185/30000 Training Loss: 0.0440116822719574\n",
      "Epoch 14186/30000 Training Loss: 0.05439455434679985\n",
      "Epoch 14187/30000 Training Loss: 0.040672264993190765\n",
      "Epoch 14188/30000 Training Loss: 0.04834746569395065\n",
      "Epoch 14189/30000 Training Loss: 0.04242781177163124\n",
      "Epoch 14190/30000 Training Loss: 0.04756814241409302\n",
      "Epoch 14191/30000 Training Loss: 0.04516006261110306\n",
      "Epoch 14192/30000 Training Loss: 0.040459394454956055\n",
      "Epoch 14193/30000 Training Loss: 0.041875436902046204\n",
      "Epoch 14194/30000 Training Loss: 0.045243434607982635\n",
      "Epoch 14195/30000 Training Loss: 0.052086107432842255\n",
      "Epoch 14196/30000 Training Loss: 0.0471450500190258\n",
      "Epoch 14197/30000 Training Loss: 0.04447166249155998\n",
      "Epoch 14198/30000 Training Loss: 0.051998477429151535\n",
      "Epoch 14199/30000 Training Loss: 0.05022177845239639\n",
      "Epoch 14200/30000 Training Loss: 0.0402640737593174\n",
      "Epoch 14200/30000 Validation Loss: 0.039661504328250885\n",
      "Epoch 14201/30000 Training Loss: 0.04163377732038498\n",
      "Epoch 14202/30000 Training Loss: 0.05031265690922737\n",
      "Epoch 14203/30000 Training Loss: 0.051464952528476715\n",
      "Epoch 14204/30000 Training Loss: 0.042234912514686584\n",
      "Epoch 14205/30000 Training Loss: 0.04577890783548355\n",
      "Epoch 14206/30000 Training Loss: 0.047477178275585175\n",
      "Epoch 14207/30000 Training Loss: 0.04547996446490288\n",
      "Epoch 14208/30000 Training Loss: 0.032772280275821686\n",
      "Epoch 14209/30000 Training Loss: 0.04837816208600998\n",
      "Epoch 14210/30000 Training Loss: 0.043252132833004\n",
      "Epoch 14211/30000 Training Loss: 0.045128438621759415\n",
      "Epoch 14212/30000 Training Loss: 0.04908966273069382\n",
      "Epoch 14213/30000 Training Loss: 0.048879701644182205\n",
      "Epoch 14214/30000 Training Loss: 0.04555830731987953\n",
      "Epoch 14215/30000 Training Loss: 0.0485716313123703\n",
      "Epoch 14216/30000 Training Loss: 0.04912181571125984\n",
      "Epoch 14217/30000 Training Loss: 0.04242195561528206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14218/30000 Training Loss: 0.04211825877428055\n",
      "Epoch 14219/30000 Training Loss: 0.04908410459756851\n",
      "Epoch 14220/30000 Training Loss: 0.040481969714164734\n",
      "Epoch 14221/30000 Training Loss: 0.04317114129662514\n",
      "Epoch 14222/30000 Training Loss: 0.045396894216537476\n",
      "Epoch 14223/30000 Training Loss: 0.04841991513967514\n",
      "Epoch 14224/30000 Training Loss: 0.043562810868024826\n",
      "Epoch 14225/30000 Training Loss: 0.037051208317279816\n",
      "Epoch 14226/30000 Training Loss: 0.035273466259241104\n",
      "Epoch 14227/30000 Training Loss: 0.04550718516111374\n",
      "Epoch 14228/30000 Training Loss: 0.0550505593419075\n",
      "Epoch 14229/30000 Training Loss: 0.04218593239784241\n",
      "Epoch 14230/30000 Training Loss: 0.053930602967739105\n",
      "Epoch 14231/30000 Training Loss: 0.05179429054260254\n",
      "Epoch 14232/30000 Training Loss: 0.04628879949450493\n",
      "Epoch 14233/30000 Training Loss: 0.05243576318025589\n",
      "Epoch 14234/30000 Training Loss: 0.044273458421230316\n",
      "Epoch 14235/30000 Training Loss: 0.03699673339724541\n",
      "Epoch 14236/30000 Training Loss: 0.03712454438209534\n",
      "Epoch 14237/30000 Training Loss: 0.03925980627536774\n",
      "Epoch 14238/30000 Training Loss: 0.05349879339337349\n",
      "Epoch 14239/30000 Training Loss: 0.04414323344826698\n",
      "Epoch 14240/30000 Training Loss: 0.04293850436806679\n",
      "Epoch 14241/30000 Training Loss: 0.04808134585618973\n",
      "Epoch 14242/30000 Training Loss: 0.046027760952711105\n",
      "Epoch 14243/30000 Training Loss: 0.03787741810083389\n",
      "Epoch 14244/30000 Training Loss: 0.04317131265997887\n",
      "Epoch 14245/30000 Training Loss: 0.03697451204061508\n",
      "Epoch 14246/30000 Training Loss: 0.03546199947595596\n",
      "Epoch 14247/30000 Training Loss: 0.04705072194337845\n",
      "Epoch 14248/30000 Training Loss: 0.0393100269138813\n",
      "Epoch 14249/30000 Training Loss: 0.04303155094385147\n",
      "Epoch 14250/30000 Training Loss: 0.0436403714120388\n",
      "Epoch 14250/30000 Validation Loss: 0.048095740377902985\n",
      "Epoch 14251/30000 Training Loss: 0.045642636716365814\n",
      "Epoch 14252/30000 Training Loss: 0.04953135550022125\n",
      "Epoch 14253/30000 Training Loss: 0.047552403062582016\n",
      "Epoch 14254/30000 Training Loss: 0.036342550069093704\n",
      "Epoch 14255/30000 Training Loss: 0.03893093019723892\n",
      "Epoch 14256/30000 Training Loss: 0.04386287182569504\n",
      "Epoch 14257/30000 Training Loss: 0.04342978447675705\n",
      "Epoch 14258/30000 Training Loss: 0.03871180862188339\n",
      "Epoch 14259/30000 Training Loss: 0.04923345893621445\n",
      "Epoch 14260/30000 Training Loss: 0.03775060921907425\n",
      "Epoch 14261/30000 Training Loss: 0.04244383051991463\n",
      "Epoch 14262/30000 Training Loss: 0.03873623535037041\n",
      "Epoch 14263/30000 Training Loss: 0.040349893271923065\n",
      "Epoch 14264/30000 Training Loss: 0.042775142937898636\n",
      "Epoch 14265/30000 Training Loss: 0.048900503665208817\n",
      "Epoch 14266/30000 Training Loss: 0.04613377898931503\n",
      "Epoch 14267/30000 Training Loss: 0.04143610969185829\n",
      "Epoch 14268/30000 Training Loss: 0.03848736360669136\n",
      "Epoch 14269/30000 Training Loss: 0.046800561249256134\n",
      "Epoch 14270/30000 Training Loss: 0.03943324834108353\n",
      "Epoch 14271/30000 Training Loss: 0.04068993777036667\n",
      "Epoch 14272/30000 Training Loss: 0.04268219321966171\n",
      "Epoch 14273/30000 Training Loss: 0.042529523372650146\n",
      "Epoch 14274/30000 Training Loss: 0.03723634034395218\n",
      "Epoch 14275/30000 Training Loss: 0.041754208505153656\n",
      "Epoch 14276/30000 Training Loss: 0.04268738254904747\n",
      "Epoch 14277/30000 Training Loss: 0.04794127494096756\n",
      "Epoch 14278/30000 Training Loss: 0.04509107396006584\n",
      "Epoch 14279/30000 Training Loss: 0.04001085087656975\n",
      "Epoch 14280/30000 Training Loss: 0.04393647238612175\n",
      "Epoch 14281/30000 Training Loss: 0.04513489082455635\n",
      "Epoch 14282/30000 Training Loss: 0.04388654604554176\n",
      "Epoch 14283/30000 Training Loss: 0.039941150695085526\n",
      "Epoch 14284/30000 Training Loss: 0.04950735718011856\n",
      "Epoch 14285/30000 Training Loss: 0.03467308729887009\n",
      "Epoch 14286/30000 Training Loss: 0.046251215040683746\n",
      "Epoch 14287/30000 Training Loss: 0.04707968980073929\n",
      "Epoch 14288/30000 Training Loss: 0.05026102811098099\n",
      "Epoch 14289/30000 Training Loss: 0.03987131640315056\n",
      "Epoch 14290/30000 Training Loss: 0.041495680809020996\n",
      "Epoch 14291/30000 Training Loss: 0.046589821577072144\n",
      "Epoch 14292/30000 Training Loss: 0.043083012104034424\n",
      "Epoch 14293/30000 Training Loss: 0.04593168944120407\n",
      "Epoch 14294/30000 Training Loss: 0.035989753901958466\n",
      "Epoch 14295/30000 Training Loss: 0.045045945793390274\n",
      "Epoch 14296/30000 Training Loss: 0.05012519285082817\n",
      "Epoch 14297/30000 Training Loss: 0.04525444656610489\n",
      "Epoch 14298/30000 Training Loss: 0.04079373553395271\n",
      "Epoch 14299/30000 Training Loss: 0.04450971260666847\n",
      "Epoch 14300/30000 Training Loss: 0.04283217713236809\n",
      "Epoch 14300/30000 Validation Loss: 0.04794130474328995\n",
      "Epoch 14301/30000 Training Loss: 0.03924166038632393\n",
      "Epoch 14302/30000 Training Loss: 0.050330329686403275\n",
      "Epoch 14303/30000 Training Loss: 0.04926804080605507\n",
      "Epoch 14304/30000 Training Loss: 0.04607361555099487\n",
      "Epoch 14305/30000 Training Loss: 0.04591608792543411\n",
      "Epoch 14306/30000 Training Loss: 0.05212049558758736\n",
      "Epoch 14307/30000 Training Loss: 0.04275880753993988\n",
      "Epoch 14308/30000 Training Loss: 0.0433354489505291\n",
      "Epoch 14309/30000 Training Loss: 0.041413262486457825\n",
      "Epoch 14310/30000 Training Loss: 0.042010627686977386\n",
      "Epoch 14311/30000 Training Loss: 0.04547519236803055\n",
      "Epoch 14312/30000 Training Loss: 0.03998275846242905\n",
      "Epoch 14313/30000 Training Loss: 0.04103507846593857\n",
      "Epoch 14314/30000 Training Loss: 0.05474718660116196\n",
      "Epoch 14315/30000 Training Loss: 0.04967083781957626\n",
      "Epoch 14316/30000 Training Loss: 0.051730912178754807\n",
      "Epoch 14317/30000 Training Loss: 0.0459720715880394\n",
      "Epoch 14318/30000 Training Loss: 0.04586988687515259\n",
      "Epoch 14319/30000 Training Loss: 0.04937204718589783\n",
      "Epoch 14320/30000 Training Loss: 0.03546381741762161\n",
      "Epoch 14321/30000 Training Loss: 0.0377483107149601\n",
      "Epoch 14322/30000 Training Loss: 0.05731486156582832\n",
      "Epoch 14323/30000 Training Loss: 0.033202238380908966\n",
      "Epoch 14324/30000 Training Loss: 0.04709189012646675\n",
      "Epoch 14325/30000 Training Loss: 0.04676281660795212\n",
      "Epoch 14326/30000 Training Loss: 0.03892304748296738\n",
      "Epoch 14327/30000 Training Loss: 0.051197152584791183\n",
      "Epoch 14328/30000 Training Loss: 0.04978855699300766\n",
      "Epoch 14329/30000 Training Loss: 0.03161570802330971\n",
      "Epoch 14330/30000 Training Loss: 0.03455640375614166\n",
      "Epoch 14331/30000 Training Loss: 0.04547862336039543\n",
      "Epoch 14332/30000 Training Loss: 0.04429749771952629\n",
      "Epoch 14333/30000 Training Loss: 0.041159018874168396\n",
      "Epoch 14334/30000 Training Loss: 0.04616018384695053\n",
      "Epoch 14335/30000 Training Loss: 0.040216248482465744\n",
      "Epoch 14336/30000 Training Loss: 0.040793128311634064\n",
      "Epoch 14337/30000 Training Loss: 0.039727021008729935\n",
      "Epoch 14338/30000 Training Loss: 0.04756449535489082\n",
      "Epoch 14339/30000 Training Loss: 0.040255893021821976\n",
      "Epoch 14340/30000 Training Loss: 0.04951382428407669\n",
      "Epoch 14341/30000 Training Loss: 0.04721355810761452\n",
      "Epoch 14342/30000 Training Loss: 0.04377876594662666\n",
      "Epoch 14343/30000 Training Loss: 0.03953763470053673\n",
      "Epoch 14344/30000 Training Loss: 0.045292556285858154\n",
      "Epoch 14345/30000 Training Loss: 0.04411790147423744\n",
      "Epoch 14346/30000 Training Loss: 0.040058985352516174\n",
      "Epoch 14347/30000 Training Loss: 0.04118446260690689\n",
      "Epoch 14348/30000 Training Loss: 0.04050160199403763\n",
      "Epoch 14349/30000 Training Loss: 0.045449476689100266\n",
      "Epoch 14350/30000 Training Loss: 0.041200898587703705\n",
      "Epoch 14350/30000 Validation Loss: 0.04124800115823746\n",
      "Epoch 14351/30000 Training Loss: 0.03985969349741936\n",
      "Epoch 14352/30000 Training Loss: 0.04705490171909332\n",
      "Epoch 14353/30000 Training Loss: 0.04021584242582321\n",
      "Epoch 14354/30000 Training Loss: 0.04427448287606239\n",
      "Epoch 14355/30000 Training Loss: 0.04797923564910889\n",
      "Epoch 14356/30000 Training Loss: 0.0469001904129982\n",
      "Epoch 14357/30000 Training Loss: 0.04007817432284355\n",
      "Epoch 14358/30000 Training Loss: 0.0352129191160202\n",
      "Epoch 14359/30000 Training Loss: 0.04868204519152641\n",
      "Epoch 14360/30000 Training Loss: 0.04245401918888092\n",
      "Epoch 14361/30000 Training Loss: 0.03638728708028793\n",
      "Epoch 14362/30000 Training Loss: 0.045144349336624146\n",
      "Epoch 14363/30000 Training Loss: 0.04811110347509384\n",
      "Epoch 14364/30000 Training Loss: 0.04489179700613022\n",
      "Epoch 14365/30000 Training Loss: 0.040055640041828156\n",
      "Epoch 14366/30000 Training Loss: 0.037913888692855835\n",
      "Epoch 14367/30000 Training Loss: 0.044446736574172974\n",
      "Epoch 14368/30000 Training Loss: 0.043586838990449905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14369/30000 Training Loss: 0.04521302878856659\n",
      "Epoch 14370/30000 Training Loss: 0.04539791867136955\n",
      "Epoch 14371/30000 Training Loss: 0.0490260049700737\n",
      "Epoch 14372/30000 Training Loss: 0.04227188974618912\n",
      "Epoch 14373/30000 Training Loss: 0.045216672122478485\n",
      "Epoch 14374/30000 Training Loss: 0.040068306028842926\n",
      "Epoch 14375/30000 Training Loss: 0.03763171657919884\n",
      "Epoch 14376/30000 Training Loss: 0.04615858942270279\n",
      "Epoch 14377/30000 Training Loss: 0.05042537301778793\n",
      "Epoch 14378/30000 Training Loss: 0.0407571867108345\n",
      "Epoch 14379/30000 Training Loss: 0.04579281061887741\n",
      "Epoch 14380/30000 Training Loss: 0.04666326567530632\n",
      "Epoch 14381/30000 Training Loss: 0.047356970608234406\n",
      "Epoch 14382/30000 Training Loss: 0.04581790044903755\n",
      "Epoch 14383/30000 Training Loss: 0.04021243005990982\n",
      "Epoch 14384/30000 Training Loss: 0.050036776810884476\n",
      "Epoch 14385/30000 Training Loss: 0.039517540484666824\n",
      "Epoch 14386/30000 Training Loss: 0.040750861167907715\n",
      "Epoch 14387/30000 Training Loss: 0.04551716148853302\n",
      "Epoch 14388/30000 Training Loss: 0.038846250623464584\n",
      "Epoch 14389/30000 Training Loss: 0.03898650035262108\n",
      "Epoch 14390/30000 Training Loss: 0.03929968923330307\n",
      "Epoch 14391/30000 Training Loss: 0.04644206911325455\n",
      "Epoch 14392/30000 Training Loss: 0.054563671350479126\n",
      "Epoch 14393/30000 Training Loss: 0.03780195116996765\n",
      "Epoch 14394/30000 Training Loss: 0.053563691675662994\n",
      "Epoch 14395/30000 Training Loss: 0.03684545308351517\n",
      "Epoch 14396/30000 Training Loss: 0.048201631754636765\n",
      "Epoch 14397/30000 Training Loss: 0.04343128576874733\n",
      "Epoch 14398/30000 Training Loss: 0.03798212856054306\n",
      "Epoch 14399/30000 Training Loss: 0.04668007418513298\n",
      "Epoch 14400/30000 Training Loss: 0.0410599447786808\n",
      "Epoch 14400/30000 Validation Loss: 0.046940237283706665\n",
      "Epoch 14401/30000 Training Loss: 0.0420660525560379\n",
      "Epoch 14402/30000 Training Loss: 0.046111900359392166\n",
      "Epoch 14403/30000 Training Loss: 0.036648523062467575\n",
      "Epoch 14404/30000 Training Loss: 0.03609514981508255\n",
      "Epoch 14405/30000 Training Loss: 0.043681737035512924\n",
      "Epoch 14406/30000 Training Loss: 0.04676063358783722\n",
      "Epoch 14407/30000 Training Loss: 0.04270369932055473\n",
      "Epoch 14408/30000 Training Loss: 0.042638249695301056\n",
      "Epoch 14409/30000 Training Loss: 0.045929428189992905\n",
      "Epoch 14410/30000 Training Loss: 0.04525376856327057\n",
      "Epoch 14411/30000 Training Loss: 0.047325629740953445\n",
      "Epoch 14412/30000 Training Loss: 0.04429423063993454\n",
      "Epoch 14413/30000 Training Loss: 0.038326896727085114\n",
      "Epoch 14414/30000 Training Loss: 0.04342290014028549\n",
      "Epoch 14415/30000 Training Loss: 0.04327128455042839\n",
      "Epoch 14416/30000 Training Loss: 0.042560212314128876\n",
      "Epoch 14417/30000 Training Loss: 0.043114807456731796\n",
      "Epoch 14418/30000 Training Loss: 0.04726308956742287\n",
      "Epoch 14419/30000 Training Loss: 0.045373838394880295\n",
      "Epoch 14420/30000 Training Loss: 0.05277351289987564\n",
      "Epoch 14421/30000 Training Loss: 0.04047916829586029\n",
      "Epoch 14422/30000 Training Loss: 0.042610205709934235\n",
      "Epoch 14423/30000 Training Loss: 0.040488582104444504\n",
      "Epoch 14424/30000 Training Loss: 0.04518403857946396\n",
      "Epoch 14425/30000 Training Loss: 0.04836983233690262\n",
      "Epoch 14426/30000 Training Loss: 0.042021073400974274\n",
      "Epoch 14427/30000 Training Loss: 0.0479714497923851\n",
      "Epoch 14428/30000 Training Loss: 0.03706225007772446\n",
      "Epoch 14429/30000 Training Loss: 0.05449109151959419\n",
      "Epoch 14430/30000 Training Loss: 0.04485372453927994\n",
      "Epoch 14431/30000 Training Loss: 0.04248366504907608\n",
      "Epoch 14432/30000 Training Loss: 0.034827135503292084\n",
      "Epoch 14433/30000 Training Loss: 0.03950369358062744\n",
      "Epoch 14434/30000 Training Loss: 0.038401104509830475\n",
      "Epoch 14435/30000 Training Loss: 0.04757610708475113\n",
      "Epoch 14436/30000 Training Loss: 0.04099288582801819\n",
      "Epoch 14437/30000 Training Loss: 0.040906328707933426\n",
      "Epoch 14438/30000 Training Loss: 0.04992266744375229\n",
      "Epoch 14439/30000 Training Loss: 0.045655954629182816\n",
      "Epoch 14440/30000 Training Loss: 0.03637314960360527\n",
      "Epoch 14441/30000 Training Loss: 0.055979181081056595\n",
      "Epoch 14442/30000 Training Loss: 0.05003048852086067\n",
      "Epoch 14443/30000 Training Loss: 0.036955054849386215\n",
      "Epoch 14444/30000 Training Loss: 0.049170367419719696\n",
      "Epoch 14445/30000 Training Loss: 0.04867592826485634\n",
      "Epoch 14446/30000 Training Loss: 0.03022737428545952\n",
      "Epoch 14447/30000 Training Loss: 0.03983475640416145\n",
      "Epoch 14448/30000 Training Loss: 0.03487943857908249\n",
      "Epoch 14449/30000 Training Loss: 0.04433854669332504\n",
      "Epoch 14450/30000 Training Loss: 0.03936392068862915\n",
      "Epoch 14450/30000 Validation Loss: 0.043171290308237076\n",
      "Epoch 14451/30000 Training Loss: 0.04123318940401077\n",
      "Epoch 14452/30000 Training Loss: 0.04606441408395767\n",
      "Epoch 14453/30000 Training Loss: 0.03871575742959976\n",
      "Epoch 14454/30000 Training Loss: 0.039662521332502365\n",
      "Epoch 14455/30000 Training Loss: 0.04674212262034416\n",
      "Epoch 14456/30000 Training Loss: 0.04162535071372986\n",
      "Epoch 14457/30000 Training Loss: 0.04406781494617462\n",
      "Epoch 14458/30000 Training Loss: 0.036520034074783325\n",
      "Epoch 14459/30000 Training Loss: 0.041519228368997574\n",
      "Epoch 14460/30000 Training Loss: 0.04849027097225189\n",
      "Epoch 14461/30000 Training Loss: 0.05909235030412674\n",
      "Epoch 14462/30000 Training Loss: 0.038557663559913635\n",
      "Epoch 14463/30000 Training Loss: 0.03986138850450516\n",
      "Epoch 14464/30000 Training Loss: 0.0404282882809639\n",
      "Epoch 14465/30000 Training Loss: 0.038171082735061646\n",
      "Epoch 14466/30000 Training Loss: 0.04036559909582138\n",
      "Epoch 14467/30000 Training Loss: 0.04458790272474289\n",
      "Epoch 14468/30000 Training Loss: 0.034237317740917206\n",
      "Epoch 14469/30000 Training Loss: 0.04523007199168205\n",
      "Epoch 14470/30000 Training Loss: 0.04776248335838318\n",
      "Epoch 14471/30000 Training Loss: 0.050384242087602615\n",
      "Epoch 14472/30000 Training Loss: 0.04106808453798294\n",
      "Epoch 14473/30000 Training Loss: 0.04698476195335388\n",
      "Epoch 14474/30000 Training Loss: 0.03700071573257446\n",
      "Epoch 14475/30000 Training Loss: 0.04339410737156868\n",
      "Epoch 14476/30000 Training Loss: 0.048534929752349854\n",
      "Epoch 14477/30000 Training Loss: 0.04164882376790047\n",
      "Epoch 14478/30000 Training Loss: 0.0479501411318779\n",
      "Epoch 14479/30000 Training Loss: 0.04184412583708763\n",
      "Epoch 14480/30000 Training Loss: 0.04632791131734848\n",
      "Epoch 14481/30000 Training Loss: 0.043261561542749405\n",
      "Epoch 14482/30000 Training Loss: 0.05289705470204353\n",
      "Epoch 14483/30000 Training Loss: 0.03646049648523331\n",
      "Epoch 14484/30000 Training Loss: 0.052254416048526764\n",
      "Epoch 14485/30000 Training Loss: 0.03818405047059059\n",
      "Epoch 14486/30000 Training Loss: 0.03874053806066513\n",
      "Epoch 14487/30000 Training Loss: 0.04096072539687157\n",
      "Epoch 14488/30000 Training Loss: 0.04153015837073326\n",
      "Epoch 14489/30000 Training Loss: 0.04713248461484909\n",
      "Epoch 14490/30000 Training Loss: 0.04247274249792099\n",
      "Epoch 14491/30000 Training Loss: 0.03655880317091942\n",
      "Epoch 14492/30000 Training Loss: 0.04631292074918747\n",
      "Epoch 14493/30000 Training Loss: 0.04005971550941467\n",
      "Epoch 14494/30000 Training Loss: 0.04840550199151039\n",
      "Epoch 14495/30000 Training Loss: 0.051382165402173996\n",
      "Epoch 14496/30000 Training Loss: 0.03971520811319351\n",
      "Epoch 14497/30000 Training Loss: 0.04746724292635918\n",
      "Epoch 14498/30000 Training Loss: 0.04761480167508125\n",
      "Epoch 14499/30000 Training Loss: 0.04673929512500763\n",
      "Epoch 14500/30000 Training Loss: 0.035587284713983536\n",
      "Epoch 14500/30000 Validation Loss: 0.04540879651904106\n",
      "Epoch 14501/30000 Training Loss: 0.04766477644443512\n",
      "Epoch 14502/30000 Training Loss: 0.03883170336484909\n",
      "Epoch 14503/30000 Training Loss: 0.046525366604328156\n",
      "Epoch 14504/30000 Training Loss: 0.041219551116228104\n",
      "Epoch 14505/30000 Training Loss: 0.05070466548204422\n",
      "Epoch 14506/30000 Training Loss: 0.040054935961961746\n",
      "Epoch 14507/30000 Training Loss: 0.03278011828660965\n",
      "Epoch 14508/30000 Training Loss: 0.046418219804763794\n",
      "Epoch 14509/30000 Training Loss: 0.03555161878466606\n",
      "Epoch 14510/30000 Training Loss: 0.04572109133005142\n",
      "Epoch 14511/30000 Training Loss: 0.04955485835671425\n",
      "Epoch 14512/30000 Training Loss: 0.04159160703420639\n",
      "Epoch 14513/30000 Training Loss: 0.043198373168706894\n",
      "Epoch 14514/30000 Training Loss: 0.04825476557016373\n",
      "Epoch 14515/30000 Training Loss: 0.03733550384640694\n",
      "Epoch 14516/30000 Training Loss: 0.04236939549446106\n",
      "Epoch 14517/30000 Training Loss: 0.043038591742515564\n",
      "Epoch 14518/30000 Training Loss: 0.04063073918223381\n",
      "Epoch 14519/30000 Training Loss: 0.043225280940532684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14520/30000 Training Loss: 0.04055841267108917\n",
      "Epoch 14521/30000 Training Loss: 0.0543023943901062\n",
      "Epoch 14522/30000 Training Loss: 0.04283848777413368\n",
      "Epoch 14523/30000 Training Loss: 0.052442122250795364\n",
      "Epoch 14524/30000 Training Loss: 0.04191559925675392\n",
      "Epoch 14525/30000 Training Loss: 0.03554181009531021\n",
      "Epoch 14526/30000 Training Loss: 0.044813789427280426\n",
      "Epoch 14527/30000 Training Loss: 0.04954088106751442\n",
      "Epoch 14528/30000 Training Loss: 0.04284393787384033\n",
      "Epoch 14529/30000 Training Loss: 0.04255841672420502\n",
      "Epoch 14530/30000 Training Loss: 0.03937208652496338\n",
      "Epoch 14531/30000 Training Loss: 0.04598962515592575\n",
      "Epoch 14532/30000 Training Loss: 0.043647173792123795\n",
      "Epoch 14533/30000 Training Loss: 0.039181552827358246\n",
      "Epoch 14534/30000 Training Loss: 0.04371869936585426\n",
      "Epoch 14535/30000 Training Loss: 0.051875561475753784\n",
      "Epoch 14536/30000 Training Loss: 0.04270798712968826\n",
      "Epoch 14537/30000 Training Loss: 0.044143740087747574\n",
      "Epoch 14538/30000 Training Loss: 0.04310303181409836\n",
      "Epoch 14539/30000 Training Loss: 0.03728345409035683\n",
      "Epoch 14540/30000 Training Loss: 0.04466406628489494\n",
      "Epoch 14541/30000 Training Loss: 0.06071586161851883\n",
      "Epoch 14542/30000 Training Loss: 0.042270053178071976\n",
      "Epoch 14543/30000 Training Loss: 0.04317239671945572\n",
      "Epoch 14544/30000 Training Loss: 0.045004211366176605\n",
      "Epoch 14545/30000 Training Loss: 0.03704507648944855\n",
      "Epoch 14546/30000 Training Loss: 0.04762016236782074\n",
      "Epoch 14547/30000 Training Loss: 0.03483453392982483\n",
      "Epoch 14548/30000 Training Loss: 0.04496190696954727\n",
      "Epoch 14549/30000 Training Loss: 0.043429192155599594\n",
      "Epoch 14550/30000 Training Loss: 0.04520805552601814\n",
      "Epoch 14550/30000 Validation Loss: 0.047013167291879654\n",
      "Epoch 14551/30000 Training Loss: 0.04502797871828079\n",
      "Epoch 14552/30000 Training Loss: 0.03943509981036186\n",
      "Epoch 14553/30000 Training Loss: 0.047654129564762115\n",
      "Epoch 14554/30000 Training Loss: 0.04268614202737808\n",
      "Epoch 14555/30000 Training Loss: 0.047970302402973175\n",
      "Epoch 14556/30000 Training Loss: 0.04260557144880295\n",
      "Epoch 14557/30000 Training Loss: 0.0422309972345829\n",
      "Epoch 14558/30000 Training Loss: 0.040679119527339935\n",
      "Epoch 14559/30000 Training Loss: 0.04192877933382988\n",
      "Epoch 14560/30000 Training Loss: 0.04371870681643486\n",
      "Epoch 14561/30000 Training Loss: 0.03728076443076134\n",
      "Epoch 14562/30000 Training Loss: 0.043864257633686066\n",
      "Epoch 14563/30000 Training Loss: 0.043623216450214386\n",
      "Epoch 14564/30000 Training Loss: 0.04197172448039055\n",
      "Epoch 14565/30000 Training Loss: 0.04476114735007286\n",
      "Epoch 14566/30000 Training Loss: 0.04631206393241882\n",
      "Epoch 14567/30000 Training Loss: 0.03519531711935997\n",
      "Epoch 14568/30000 Training Loss: 0.050538480281829834\n",
      "Epoch 14569/30000 Training Loss: 0.04099961370229721\n",
      "Epoch 14570/30000 Training Loss: 0.04939187318086624\n",
      "Epoch 14571/30000 Training Loss: 0.043191030621528625\n",
      "Epoch 14572/30000 Training Loss: 0.04142718389630318\n",
      "Epoch 14573/30000 Training Loss: 0.032782722264528275\n",
      "Epoch 14574/30000 Training Loss: 0.04725724458694458\n",
      "Epoch 14575/30000 Training Loss: 0.044679731130599976\n",
      "Epoch 14576/30000 Training Loss: 0.041336558759212494\n",
      "Epoch 14577/30000 Training Loss: 0.04586099833250046\n",
      "Epoch 14578/30000 Training Loss: 0.042541421949863434\n",
      "Epoch 14579/30000 Training Loss: 0.04812579229474068\n",
      "Epoch 14580/30000 Training Loss: 0.04130764305591583\n",
      "Epoch 14581/30000 Training Loss: 0.0460357666015625\n",
      "Epoch 14582/30000 Training Loss: 0.041491083800792694\n",
      "Epoch 14583/30000 Training Loss: 0.041290175169706345\n",
      "Epoch 14584/30000 Training Loss: 0.03896229714155197\n",
      "Epoch 14585/30000 Training Loss: 0.037753112614154816\n",
      "Epoch 14586/30000 Training Loss: 0.040806304663419724\n",
      "Epoch 14587/30000 Training Loss: 0.04537270590662956\n",
      "Epoch 14588/30000 Training Loss: 0.044417016208171844\n",
      "Epoch 14589/30000 Training Loss: 0.04138937219977379\n",
      "Epoch 14590/30000 Training Loss: 0.04364469274878502\n",
      "Epoch 14591/30000 Training Loss: 0.04476398974657059\n",
      "Epoch 14592/30000 Training Loss: 0.03992375731468201\n",
      "Epoch 14593/30000 Training Loss: 0.043098609894514084\n",
      "Epoch 14594/30000 Training Loss: 0.03502301499247551\n",
      "Epoch 14595/30000 Training Loss: 0.04620213806629181\n",
      "Epoch 14596/30000 Training Loss: 0.038307927548885345\n",
      "Epoch 14597/30000 Training Loss: 0.039369840174913406\n",
      "Epoch 14598/30000 Training Loss: 0.043728575110435486\n",
      "Epoch 14599/30000 Training Loss: 0.04188046604394913\n",
      "Epoch 14600/30000 Training Loss: 0.03549855947494507\n",
      "Epoch 14600/30000 Validation Loss: 0.046031393110752106\n",
      "Epoch 14601/30000 Training Loss: 0.04711503908038139\n",
      "Epoch 14602/30000 Training Loss: 0.042795706540346146\n",
      "Epoch 14603/30000 Training Loss: 0.044376030564308167\n",
      "Epoch 14604/30000 Training Loss: 0.03880584239959717\n",
      "Epoch 14605/30000 Training Loss: 0.03682081028819084\n",
      "Epoch 14606/30000 Training Loss: 0.04791512340307236\n",
      "Epoch 14607/30000 Training Loss: 0.05236320570111275\n",
      "Epoch 14608/30000 Training Loss: 0.045075707137584686\n",
      "Epoch 14609/30000 Training Loss: 0.036288924515247345\n",
      "Epoch 14610/30000 Training Loss: 0.04640962928533554\n",
      "Epoch 14611/30000 Training Loss: 0.03915105015039444\n",
      "Epoch 14612/30000 Training Loss: 0.05002789944410324\n",
      "Epoch 14613/30000 Training Loss: 0.04400601238012314\n",
      "Epoch 14614/30000 Training Loss: 0.04551010578870773\n",
      "Epoch 14615/30000 Training Loss: 0.04234660789370537\n",
      "Epoch 14616/30000 Training Loss: 0.03847473859786987\n",
      "Epoch 14617/30000 Training Loss: 0.03801875188946724\n",
      "Epoch 14618/30000 Training Loss: 0.05075092986226082\n",
      "Epoch 14619/30000 Training Loss: 0.05137786269187927\n",
      "Epoch 14620/30000 Training Loss: 0.036863774061203\n",
      "Epoch 14621/30000 Training Loss: 0.04409288242459297\n",
      "Epoch 14622/30000 Training Loss: 0.046286340802907944\n",
      "Epoch 14623/30000 Training Loss: 0.039745453745126724\n",
      "Epoch 14624/30000 Training Loss: 0.03572016954421997\n",
      "Epoch 14625/30000 Training Loss: 0.03553878515958786\n",
      "Epoch 14626/30000 Training Loss: 0.0464080385863781\n",
      "Epoch 14627/30000 Training Loss: 0.05048723891377449\n",
      "Epoch 14628/30000 Training Loss: 0.04274294525384903\n",
      "Epoch 14629/30000 Training Loss: 0.04216163605451584\n",
      "Epoch 14630/30000 Training Loss: 0.04082381725311279\n",
      "Epoch 14631/30000 Training Loss: 0.04437989369034767\n",
      "Epoch 14632/30000 Training Loss: 0.04828738421201706\n",
      "Epoch 14633/30000 Training Loss: 0.03998877853155136\n",
      "Epoch 14634/30000 Training Loss: 0.03744909167289734\n",
      "Epoch 14635/30000 Training Loss: 0.04358655959367752\n",
      "Epoch 14636/30000 Training Loss: 0.048046328127384186\n",
      "Epoch 14637/30000 Training Loss: 0.0472843162715435\n",
      "Epoch 14638/30000 Training Loss: 0.044529374688863754\n",
      "Epoch 14639/30000 Training Loss: 0.04150308668613434\n",
      "Epoch 14640/30000 Training Loss: 0.051872801035642624\n",
      "Epoch 14641/30000 Training Loss: 0.050439734011888504\n",
      "Epoch 14642/30000 Training Loss: 0.045098915696144104\n",
      "Epoch 14643/30000 Training Loss: 0.05063127353787422\n",
      "Epoch 14644/30000 Training Loss: 0.0428810715675354\n",
      "Epoch 14645/30000 Training Loss: 0.04502632096409798\n",
      "Epoch 14646/30000 Training Loss: 0.03964177519083023\n",
      "Epoch 14647/30000 Training Loss: 0.044043879956007004\n",
      "Epoch 14648/30000 Training Loss: 0.041425660252571106\n",
      "Epoch 14649/30000 Training Loss: 0.045179784297943115\n",
      "Epoch 14650/30000 Training Loss: 0.04471839219331741\n",
      "Epoch 14650/30000 Validation Loss: 0.04273208603262901\n",
      "Epoch 14651/30000 Training Loss: 0.04126100242137909\n",
      "Epoch 14652/30000 Training Loss: 0.042016588151454926\n",
      "Epoch 14653/30000 Training Loss: 0.045266538858413696\n",
      "Epoch 14654/30000 Training Loss: 0.04651287570595741\n",
      "Epoch 14655/30000 Training Loss: 0.04253622144460678\n",
      "Epoch 14656/30000 Training Loss: 0.04062334820628166\n",
      "Epoch 14657/30000 Training Loss: 0.04356155917048454\n",
      "Epoch 14658/30000 Training Loss: 0.05080221965909004\n",
      "Epoch 14659/30000 Training Loss: 0.03823418915271759\n",
      "Epoch 14660/30000 Training Loss: 0.04658358544111252\n",
      "Epoch 14661/30000 Training Loss: 0.048001181334257126\n",
      "Epoch 14662/30000 Training Loss: 0.051511336117982864\n",
      "Epoch 14663/30000 Training Loss: 0.041049856692552567\n",
      "Epoch 14664/30000 Training Loss: 0.04029672592878342\n",
      "Epoch 14665/30000 Training Loss: 0.05046539753675461\n",
      "Epoch 14666/30000 Training Loss: 0.046451788395643234\n",
      "Epoch 14667/30000 Training Loss: 0.04892856627702713\n",
      "Epoch 14668/30000 Training Loss: 0.04246242344379425\n",
      "Epoch 14669/30000 Training Loss: 0.03982388600707054\n",
      "Epoch 14670/30000 Training Loss: 0.04284827038645744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14671/30000 Training Loss: 0.047160856425762177\n",
      "Epoch 14672/30000 Training Loss: 0.04275578260421753\n",
      "Epoch 14673/30000 Training Loss: 0.04443768039345741\n",
      "Epoch 14674/30000 Training Loss: 0.04122159630060196\n",
      "Epoch 14675/30000 Training Loss: 0.040544938296079636\n",
      "Epoch 14676/30000 Training Loss: 0.047739140689373016\n",
      "Epoch 14677/30000 Training Loss: 0.0497557558119297\n",
      "Epoch 14678/30000 Training Loss: 0.048288360238075256\n",
      "Epoch 14679/30000 Training Loss: 0.04105545952916145\n",
      "Epoch 14680/30000 Training Loss: 0.04431445524096489\n",
      "Epoch 14681/30000 Training Loss: 0.04773031920194626\n",
      "Epoch 14682/30000 Training Loss: 0.03963490575551987\n",
      "Epoch 14683/30000 Training Loss: 0.0465952530503273\n",
      "Epoch 14684/30000 Training Loss: 0.03993615880608559\n",
      "Epoch 14685/30000 Training Loss: 0.04710658639669418\n",
      "Epoch 14686/30000 Training Loss: 0.04417090490460396\n",
      "Epoch 14687/30000 Training Loss: 0.044741153717041016\n",
      "Epoch 14688/30000 Training Loss: 0.042405251413583755\n",
      "Epoch 14689/30000 Training Loss: 0.04717658832669258\n",
      "Epoch 14690/30000 Training Loss: 0.04541115462779999\n",
      "Epoch 14691/30000 Training Loss: 0.04354974627494812\n",
      "Epoch 14692/30000 Training Loss: 0.04508547857403755\n",
      "Epoch 14693/30000 Training Loss: 0.043292246758937836\n",
      "Epoch 14694/30000 Training Loss: 0.0356106236577034\n",
      "Epoch 14695/30000 Training Loss: 0.041689131408929825\n",
      "Epoch 14696/30000 Training Loss: 0.04406851902604103\n",
      "Epoch 14697/30000 Training Loss: 0.03908420354127884\n",
      "Epoch 14698/30000 Training Loss: 0.0428185760974884\n",
      "Epoch 14699/30000 Training Loss: 0.051849015057086945\n",
      "Epoch 14700/30000 Training Loss: 0.04376382753252983\n",
      "Epoch 14700/30000 Validation Loss: 0.04605097323656082\n",
      "Epoch 14701/30000 Training Loss: 0.03329505771398544\n",
      "Epoch 14702/30000 Training Loss: 0.038615234196186066\n",
      "Epoch 14703/30000 Training Loss: 0.04493536427617073\n",
      "Epoch 14704/30000 Training Loss: 0.03521720692515373\n",
      "Epoch 14705/30000 Training Loss: 0.03714233264327049\n",
      "Epoch 14706/30000 Training Loss: 0.04818737879395485\n",
      "Epoch 14707/30000 Training Loss: 0.04408641159534454\n",
      "Epoch 14708/30000 Training Loss: 0.04530036821961403\n",
      "Epoch 14709/30000 Training Loss: 0.04559537023305893\n",
      "Epoch 14710/30000 Training Loss: 0.03914635255932808\n",
      "Epoch 14711/30000 Training Loss: 0.038650598376989365\n",
      "Epoch 14712/30000 Training Loss: 0.03901585191488266\n",
      "Epoch 14713/30000 Training Loss: 0.040142972022295\n",
      "Epoch 14714/30000 Training Loss: 0.04814477264881134\n",
      "Epoch 14715/30000 Training Loss: 0.046642959117889404\n",
      "Epoch 14716/30000 Training Loss: 0.044214583933353424\n",
      "Epoch 14717/30000 Training Loss: 0.039715711027383804\n",
      "Epoch 14718/30000 Training Loss: 0.03767181187868118\n",
      "Epoch 14719/30000 Training Loss: 0.04047079756855965\n",
      "Epoch 14720/30000 Training Loss: 0.03965424373745918\n",
      "Epoch 14721/30000 Training Loss: 0.04210642725229263\n",
      "Epoch 14722/30000 Training Loss: 0.04626684635877609\n",
      "Epoch 14723/30000 Training Loss: 0.03298095613718033\n",
      "Epoch 14724/30000 Training Loss: 0.04720575362443924\n",
      "Epoch 14725/30000 Training Loss: 0.04444960504770279\n",
      "Epoch 14726/30000 Training Loss: 0.04654664546251297\n",
      "Epoch 14727/30000 Training Loss: 0.038301508873701096\n",
      "Epoch 14728/30000 Training Loss: 0.041623298078775406\n",
      "Epoch 14729/30000 Training Loss: 0.04694496467709541\n",
      "Epoch 14730/30000 Training Loss: 0.04063016176223755\n",
      "Epoch 14731/30000 Training Loss: 0.05199425294995308\n",
      "Epoch 14732/30000 Training Loss: 0.0404895581305027\n",
      "Epoch 14733/30000 Training Loss: 0.04638611525297165\n",
      "Epoch 14734/30000 Training Loss: 0.04934140294790268\n",
      "Epoch 14735/30000 Training Loss: 0.04023027792572975\n",
      "Epoch 14736/30000 Training Loss: 0.04132681339979172\n",
      "Epoch 14737/30000 Training Loss: 0.054979436099529266\n",
      "Epoch 14738/30000 Training Loss: 0.0359225794672966\n",
      "Epoch 14739/30000 Training Loss: 0.04995192959904671\n",
      "Epoch 14740/30000 Training Loss: 0.038357511162757874\n",
      "Epoch 14741/30000 Training Loss: 0.04308535158634186\n",
      "Epoch 14742/30000 Training Loss: 0.034752923995256424\n",
      "Epoch 14743/30000 Training Loss: 0.03801713138818741\n",
      "Epoch 14744/30000 Training Loss: 0.0417257584631443\n",
      "Epoch 14745/30000 Training Loss: 0.0407826267182827\n",
      "Epoch 14746/30000 Training Loss: 0.036801569163799286\n",
      "Epoch 14747/30000 Training Loss: 0.04246779903769493\n",
      "Epoch 14748/30000 Training Loss: 0.04183449223637581\n",
      "Epoch 14749/30000 Training Loss: 0.04128966107964516\n",
      "Epoch 14750/30000 Training Loss: 0.03824659436941147\n",
      "Epoch 14750/30000 Validation Loss: 0.05051586776971817\n",
      "Epoch 14751/30000 Training Loss: 0.04397162050008774\n",
      "Epoch 14752/30000 Training Loss: 0.03856803476810455\n",
      "Epoch 14753/30000 Training Loss: 0.04401182010769844\n",
      "Epoch 14754/30000 Training Loss: 0.039957255125045776\n",
      "Epoch 14755/30000 Training Loss: 0.04493381083011627\n",
      "Epoch 14756/30000 Training Loss: 0.038940466940402985\n",
      "Epoch 14757/30000 Training Loss: 0.04568670690059662\n",
      "Epoch 14758/30000 Training Loss: 0.04399146884679794\n",
      "Epoch 14759/30000 Training Loss: 0.03888855129480362\n",
      "Epoch 14760/30000 Training Loss: 0.036381326615810394\n",
      "Epoch 14761/30000 Training Loss: 0.04093744233250618\n",
      "Epoch 14762/30000 Training Loss: 0.041528258472681046\n",
      "Epoch 14763/30000 Training Loss: 0.04788193851709366\n",
      "Epoch 14764/30000 Training Loss: 0.04753581061959267\n",
      "Epoch 14765/30000 Training Loss: 0.04651043564081192\n",
      "Epoch 14766/30000 Training Loss: 0.06241806223988533\n",
      "Epoch 14767/30000 Training Loss: 0.052820831537246704\n",
      "Epoch 14768/30000 Training Loss: 0.04274816811084747\n",
      "Epoch 14769/30000 Training Loss: 0.0459582656621933\n",
      "Epoch 14770/30000 Training Loss: 0.0433652326464653\n",
      "Epoch 14771/30000 Training Loss: 0.04608780890703201\n",
      "Epoch 14772/30000 Training Loss: 0.041600991040468216\n",
      "Epoch 14773/30000 Training Loss: 0.03988299146294594\n",
      "Epoch 14774/30000 Training Loss: 0.03937574103474617\n",
      "Epoch 14775/30000 Training Loss: 0.04606633260846138\n",
      "Epoch 14776/30000 Training Loss: 0.03628232702612877\n",
      "Epoch 14777/30000 Training Loss: 0.04097124934196472\n",
      "Epoch 14778/30000 Training Loss: 0.0367157980799675\n",
      "Epoch 14779/30000 Training Loss: 0.04159434884786606\n",
      "Epoch 14780/30000 Training Loss: 0.04606270045042038\n",
      "Epoch 14781/30000 Training Loss: 0.050890326499938965\n",
      "Epoch 14782/30000 Training Loss: 0.04144866392016411\n",
      "Epoch 14783/30000 Training Loss: 0.039262913167476654\n",
      "Epoch 14784/30000 Training Loss: 0.038816310465335846\n",
      "Epoch 14785/30000 Training Loss: 0.03844544291496277\n",
      "Epoch 14786/30000 Training Loss: 0.037226948887109756\n",
      "Epoch 14787/30000 Training Loss: 0.04836387187242508\n",
      "Epoch 14788/30000 Training Loss: 0.04275156557559967\n",
      "Epoch 14789/30000 Training Loss: 0.046717479825019836\n",
      "Epoch 14790/30000 Training Loss: 0.05107097700238228\n",
      "Epoch 14791/30000 Training Loss: 0.047986336052417755\n",
      "Epoch 14792/30000 Training Loss: 0.04335372522473335\n",
      "Epoch 14793/30000 Training Loss: 0.04403633996844292\n",
      "Epoch 14794/30000 Training Loss: 0.04352918639779091\n",
      "Epoch 14795/30000 Training Loss: 0.04391757398843765\n",
      "Epoch 14796/30000 Training Loss: 0.041554249823093414\n",
      "Epoch 14797/30000 Training Loss: 0.03702486306428909\n",
      "Epoch 14798/30000 Training Loss: 0.03949444741010666\n",
      "Epoch 14799/30000 Training Loss: 0.051820360124111176\n",
      "Epoch 14800/30000 Training Loss: 0.04356681555509567\n",
      "Epoch 14800/30000 Validation Loss: 0.036587879061698914\n",
      "Epoch 14801/30000 Training Loss: 0.04664086550474167\n",
      "Epoch 14802/30000 Training Loss: 0.041482023894786835\n",
      "Epoch 14803/30000 Training Loss: 0.03992350026965141\n",
      "Epoch 14804/30000 Training Loss: 0.040909815579652786\n",
      "Epoch 14805/30000 Training Loss: 0.037087295204401016\n",
      "Epoch 14806/30000 Training Loss: 0.0430457666516304\n",
      "Epoch 14807/30000 Training Loss: 0.042562905699014664\n",
      "Epoch 14808/30000 Training Loss: 0.03597598522901535\n",
      "Epoch 14809/30000 Training Loss: 0.038834065198898315\n",
      "Epoch 14810/30000 Training Loss: 0.04640834778547287\n",
      "Epoch 14811/30000 Training Loss: 0.04028593748807907\n",
      "Epoch 14812/30000 Training Loss: 0.04976019263267517\n",
      "Epoch 14813/30000 Training Loss: 0.04082338884472847\n",
      "Epoch 14814/30000 Training Loss: 0.04607730358839035\n",
      "Epoch 14815/30000 Training Loss: 0.04183940961956978\n",
      "Epoch 14816/30000 Training Loss: 0.04540341719985008\n",
      "Epoch 14817/30000 Training Loss: 0.03881970793008804\n",
      "Epoch 14818/30000 Training Loss: 0.04668884351849556\n",
      "Epoch 14819/30000 Training Loss: 0.04642067849636078\n",
      "Epoch 14820/30000 Training Loss: 0.04252822324633598\n",
      "Epoch 14821/30000 Training Loss: 0.039606571197509766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14822/30000 Training Loss: 0.04327957704663277\n",
      "Epoch 14823/30000 Training Loss: 0.043057411909103394\n",
      "Epoch 14824/30000 Training Loss: 0.038587071001529694\n",
      "Epoch 14825/30000 Training Loss: 0.04141360521316528\n",
      "Epoch 14826/30000 Training Loss: 0.038597408682107925\n",
      "Epoch 14827/30000 Training Loss: 0.03759587183594704\n",
      "Epoch 14828/30000 Training Loss: 0.04691470414400101\n",
      "Epoch 14829/30000 Training Loss: 0.04713200777769089\n",
      "Epoch 14830/30000 Training Loss: 0.049252428114414215\n",
      "Epoch 14831/30000 Training Loss: 0.04960838705301285\n",
      "Epoch 14832/30000 Training Loss: 0.043671615421772\n",
      "Epoch 14833/30000 Training Loss: 0.04070819169282913\n",
      "Epoch 14834/30000 Training Loss: 0.04778566211462021\n",
      "Epoch 14835/30000 Training Loss: 0.04554665461182594\n",
      "Epoch 14836/30000 Training Loss: 0.049136869609355927\n",
      "Epoch 14837/30000 Training Loss: 0.049363091588020325\n",
      "Epoch 14838/30000 Training Loss: 0.04453500732779503\n",
      "Epoch 14839/30000 Training Loss: 0.0413488894701004\n",
      "Epoch 14840/30000 Training Loss: 0.03948468714952469\n",
      "Epoch 14841/30000 Training Loss: 0.0404290035367012\n",
      "Epoch 14842/30000 Training Loss: 0.041743919253349304\n",
      "Epoch 14843/30000 Training Loss: 0.045356620103120804\n",
      "Epoch 14844/30000 Training Loss: 0.0535888597369194\n",
      "Epoch 14845/30000 Training Loss: 0.04616006091237068\n",
      "Epoch 14846/30000 Training Loss: 0.042053136974573135\n",
      "Epoch 14847/30000 Training Loss: 0.04603591188788414\n",
      "Epoch 14848/30000 Training Loss: 0.0337238535284996\n",
      "Epoch 14849/30000 Training Loss: 0.04515800625085831\n",
      "Epoch 14850/30000 Training Loss: 0.047166235744953156\n",
      "Epoch 14850/30000 Validation Loss: 0.03930587321519852\n",
      "Epoch 14851/30000 Training Loss: 0.05018463730812073\n",
      "Epoch 14852/30000 Training Loss: 0.04188913851976395\n",
      "Epoch 14853/30000 Training Loss: 0.03557325527071953\n",
      "Epoch 14854/30000 Training Loss: 0.04491132125258446\n",
      "Epoch 14855/30000 Training Loss: 0.04263276234269142\n",
      "Epoch 14856/30000 Training Loss: 0.062056709080934525\n",
      "Epoch 14857/30000 Training Loss: 0.047559045255184174\n",
      "Epoch 14858/30000 Training Loss: 0.03405679017305374\n",
      "Epoch 14859/30000 Training Loss: 0.039279352873563766\n",
      "Epoch 14860/30000 Training Loss: 0.04277894273400307\n",
      "Epoch 14861/30000 Training Loss: 0.04356567934155464\n",
      "Epoch 14862/30000 Training Loss: 0.041624050587415695\n",
      "Epoch 14863/30000 Training Loss: 0.03843022510409355\n",
      "Epoch 14864/30000 Training Loss: 0.047040704637765884\n",
      "Epoch 14865/30000 Training Loss: 0.0404835008084774\n",
      "Epoch 14866/30000 Training Loss: 0.03799594193696976\n",
      "Epoch 14867/30000 Training Loss: 0.05270407348871231\n",
      "Epoch 14868/30000 Training Loss: 0.04815181344747543\n",
      "Epoch 14869/30000 Training Loss: 0.03944601118564606\n",
      "Epoch 14870/30000 Training Loss: 0.047564804553985596\n",
      "Epoch 14871/30000 Training Loss: 0.04279656708240509\n",
      "Epoch 14872/30000 Training Loss: 0.04081720858812332\n",
      "Epoch 14873/30000 Training Loss: 0.04112868756055832\n",
      "Epoch 14874/30000 Training Loss: 0.04792066290974617\n",
      "Epoch 14875/30000 Training Loss: 0.04307590052485466\n",
      "Epoch 14876/30000 Training Loss: 0.051422618329524994\n",
      "Epoch 14877/30000 Training Loss: 0.04496666043996811\n",
      "Epoch 14878/30000 Training Loss: 0.043180257081985474\n",
      "Epoch 14879/30000 Training Loss: 0.04119621217250824\n",
      "Epoch 14880/30000 Training Loss: 0.04706884175539017\n",
      "Epoch 14881/30000 Training Loss: 0.041324514895677567\n",
      "Epoch 14882/30000 Training Loss: 0.043858520686626434\n",
      "Epoch 14883/30000 Training Loss: 0.04243180900812149\n",
      "Epoch 14884/30000 Training Loss: 0.04718263819813728\n",
      "Epoch 14885/30000 Training Loss: 0.05469211935997009\n",
      "Epoch 14886/30000 Training Loss: 0.04082102328538895\n",
      "Epoch 14887/30000 Training Loss: 0.041185371577739716\n",
      "Epoch 14888/30000 Training Loss: 0.0365096852183342\n",
      "Epoch 14889/30000 Training Loss: 0.036096636205911636\n",
      "Epoch 14890/30000 Training Loss: 0.04997279495000839\n",
      "Epoch 14891/30000 Training Loss: 0.0458056665956974\n",
      "Epoch 14892/30000 Training Loss: 0.04601556435227394\n",
      "Epoch 14893/30000 Training Loss: 0.04126953333616257\n",
      "Epoch 14894/30000 Training Loss: 0.04081394150853157\n",
      "Epoch 14895/30000 Training Loss: 0.03957616537809372\n",
      "Epoch 14896/30000 Training Loss: 0.03900487348437309\n",
      "Epoch 14897/30000 Training Loss: 0.04673684388399124\n",
      "Epoch 14898/30000 Training Loss: 0.041739799082279205\n",
      "Epoch 14899/30000 Training Loss: 0.0444357767701149\n",
      "Epoch 14900/30000 Training Loss: 0.03514746204018593\n",
      "Epoch 14900/30000 Validation Loss: 0.04402706399559975\n",
      "Epoch 14901/30000 Training Loss: 0.04715578630566597\n",
      "Epoch 14902/30000 Training Loss: 0.049048714339733124\n",
      "Epoch 14903/30000 Training Loss: 0.045448921620845795\n",
      "Epoch 14904/30000 Training Loss: 0.05471191927790642\n",
      "Epoch 14905/30000 Training Loss: 0.0526871383190155\n",
      "Epoch 14906/30000 Training Loss: 0.047045014798641205\n",
      "Epoch 14907/30000 Training Loss: 0.05018504336476326\n",
      "Epoch 14908/30000 Training Loss: 0.039007894694805145\n",
      "Epoch 14909/30000 Training Loss: 0.04538014531135559\n",
      "Epoch 14910/30000 Training Loss: 0.04225558042526245\n",
      "Epoch 14911/30000 Training Loss: 0.04344134032726288\n",
      "Epoch 14912/30000 Training Loss: 0.05051976442337036\n",
      "Epoch 14913/30000 Training Loss: 0.042664408683776855\n",
      "Epoch 14914/30000 Training Loss: 0.0405864343047142\n",
      "Epoch 14915/30000 Training Loss: 0.040222905576229095\n",
      "Epoch 14916/30000 Training Loss: 0.04488480091094971\n",
      "Epoch 14917/30000 Training Loss: 0.04265877604484558\n",
      "Epoch 14918/30000 Training Loss: 0.048522062599658966\n",
      "Epoch 14919/30000 Training Loss: 0.03943287581205368\n",
      "Epoch 14920/30000 Training Loss: 0.04609346389770508\n",
      "Epoch 14921/30000 Training Loss: 0.04085655137896538\n",
      "Epoch 14922/30000 Training Loss: 0.04224511608481407\n",
      "Epoch 14923/30000 Training Loss: 0.04690587520599365\n",
      "Epoch 14924/30000 Training Loss: 0.04067882150411606\n",
      "Epoch 14925/30000 Training Loss: 0.046028994023799896\n",
      "Epoch 14926/30000 Training Loss: 0.04751092940568924\n",
      "Epoch 14927/30000 Training Loss: 0.0457533560693264\n",
      "Epoch 14928/30000 Training Loss: 0.049827881157398224\n",
      "Epoch 14929/30000 Training Loss: 0.038049280643463135\n",
      "Epoch 14930/30000 Training Loss: 0.045047275722026825\n",
      "Epoch 14931/30000 Training Loss: 0.05669473856687546\n",
      "Epoch 14932/30000 Training Loss: 0.04320596158504486\n",
      "Epoch 14933/30000 Training Loss: 0.04004641994833946\n",
      "Epoch 14934/30000 Training Loss: 0.042692188173532486\n",
      "Epoch 14935/30000 Training Loss: 0.0373247005045414\n",
      "Epoch 14936/30000 Training Loss: 0.05557301640510559\n",
      "Epoch 14937/30000 Training Loss: 0.047670863568782806\n",
      "Epoch 14938/30000 Training Loss: 0.041825491935014725\n",
      "Epoch 14939/30000 Training Loss: 0.03878076374530792\n",
      "Epoch 14940/30000 Training Loss: 0.03783353045582771\n",
      "Epoch 14941/30000 Training Loss: 0.04403644800186157\n",
      "Epoch 14942/30000 Training Loss: 0.040867406874895096\n",
      "Epoch 14943/30000 Training Loss: 0.03704182058572769\n",
      "Epoch 14944/30000 Training Loss: 0.04406573250889778\n",
      "Epoch 14945/30000 Training Loss: 0.04335220158100128\n",
      "Epoch 14946/30000 Training Loss: 0.04178967326879501\n",
      "Epoch 14947/30000 Training Loss: 0.05285711959004402\n",
      "Epoch 14948/30000 Training Loss: 0.046891361474990845\n",
      "Epoch 14949/30000 Training Loss: 0.047306906431913376\n",
      "Epoch 14950/30000 Training Loss: 0.04292987287044525\n",
      "Epoch 14950/30000 Validation Loss: 0.04779861867427826\n",
      "Epoch 14951/30000 Training Loss: 0.0553002767264843\n",
      "Epoch 14952/30000 Training Loss: 0.043812163174152374\n",
      "Epoch 14953/30000 Training Loss: 0.04329843819141388\n",
      "Epoch 14954/30000 Training Loss: 0.04276600107550621\n",
      "Epoch 14955/30000 Training Loss: 0.04611944779753685\n",
      "Epoch 14956/30000 Training Loss: 0.04600484296679497\n",
      "Epoch 14957/30000 Training Loss: 0.04324261099100113\n",
      "Epoch 14958/30000 Training Loss: 0.04543914273381233\n",
      "Epoch 14959/30000 Training Loss: 0.04299800843000412\n",
      "Epoch 14960/30000 Training Loss: 0.039343565702438354\n",
      "Epoch 14961/30000 Training Loss: 0.0474545992910862\n",
      "Epoch 14962/30000 Training Loss: 0.045820899307727814\n",
      "Epoch 14963/30000 Training Loss: 0.04153137654066086\n",
      "Epoch 14964/30000 Training Loss: 0.0420377142727375\n",
      "Epoch 14965/30000 Training Loss: 0.052283577620983124\n",
      "Epoch 14966/30000 Training Loss: 0.046074334532022476\n",
      "Epoch 14967/30000 Training Loss: 0.04914268106222153\n",
      "Epoch 14968/30000 Training Loss: 0.045226164162158966\n",
      "Epoch 14969/30000 Training Loss: 0.03954801335930824\n",
      "Epoch 14970/30000 Training Loss: 0.03957558423280716\n",
      "Epoch 14971/30000 Training Loss: 0.051147907972335815\n",
      "Epoch 14972/30000 Training Loss: 0.04755131155252457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14973/30000 Training Loss: 0.044478438794612885\n",
      "Epoch 14974/30000 Training Loss: 0.0447625033557415\n",
      "Epoch 14975/30000 Training Loss: 0.04537349194288254\n",
      "Epoch 14976/30000 Training Loss: 0.04486757516860962\n",
      "Epoch 14977/30000 Training Loss: 0.04073759913444519\n",
      "Epoch 14978/30000 Training Loss: 0.045989226549863815\n",
      "Epoch 14979/30000 Training Loss: 0.04599663242697716\n",
      "Epoch 14980/30000 Training Loss: 0.03810335323214531\n",
      "Epoch 14981/30000 Training Loss: 0.04337450489401817\n",
      "Epoch 14982/30000 Training Loss: 0.045069094747304916\n",
      "Epoch 14983/30000 Training Loss: 0.05724309757351875\n",
      "Epoch 14984/30000 Training Loss: 0.04290786013007164\n",
      "Epoch 14985/30000 Training Loss: 0.04231240600347519\n",
      "Epoch 14986/30000 Training Loss: 0.042877353727817535\n",
      "Epoch 14987/30000 Training Loss: 0.03992196172475815\n",
      "Epoch 14988/30000 Training Loss: 0.044997163116931915\n",
      "Epoch 14989/30000 Training Loss: 0.04177466779947281\n",
      "Epoch 14990/30000 Training Loss: 0.04087696224451065\n",
      "Epoch 14991/30000 Training Loss: 0.042309947311878204\n",
      "Epoch 14992/30000 Training Loss: 0.04716804623603821\n",
      "Epoch 14993/30000 Training Loss: 0.038235586136579514\n",
      "Epoch 14994/30000 Training Loss: 0.037188466638326645\n",
      "Epoch 14995/30000 Training Loss: 0.03440506011247635\n",
      "Epoch 14996/30000 Training Loss: 0.03917211294174194\n",
      "Epoch 14997/30000 Training Loss: 0.04833318293094635\n",
      "Epoch 14998/30000 Training Loss: 0.04317445307970047\n",
      "Epoch 14999/30000 Training Loss: 0.047829195857048035\n",
      "Epoch 15000/30000 Training Loss: 0.044118285179138184\n",
      "Epoch 15000/30000 Validation Loss: 0.03828861564397812\n",
      "Epoch 15001/30000 Training Loss: 0.042009126394987106\n",
      "Epoch 15002/30000 Training Loss: 0.04487011581659317\n",
      "Epoch 15003/30000 Training Loss: 0.04741387441754341\n",
      "Epoch 15004/30000 Training Loss: 0.03955856338143349\n",
      "Epoch 15005/30000 Training Loss: 0.04189648479223251\n",
      "Epoch 15006/30000 Training Loss: 0.03720613569021225\n",
      "Epoch 15007/30000 Training Loss: 0.04506371170282364\n",
      "Epoch 15008/30000 Training Loss: 0.04084622114896774\n",
      "Epoch 15009/30000 Training Loss: 0.041813887655735016\n",
      "Epoch 15010/30000 Training Loss: 0.04855206981301308\n",
      "Epoch 15011/30000 Training Loss: 0.04472440108656883\n",
      "Epoch 15012/30000 Training Loss: 0.04226792976260185\n",
      "Epoch 15013/30000 Training Loss: 0.04724281281232834\n",
      "Epoch 15014/30000 Training Loss: 0.03706247732043266\n",
      "Epoch 15015/30000 Training Loss: 0.04957764223217964\n",
      "Epoch 15016/30000 Training Loss: 0.03804786503314972\n",
      "Epoch 15017/30000 Training Loss: 0.05267881229519844\n",
      "Epoch 15018/30000 Training Loss: 0.040374744683504105\n",
      "Epoch 15019/30000 Training Loss: 0.0432242713868618\n",
      "Epoch 15020/30000 Training Loss: 0.04048027843236923\n",
      "Epoch 15021/30000 Training Loss: 0.05599294975399971\n",
      "Epoch 15022/30000 Training Loss: 0.04080205410718918\n",
      "Epoch 15023/30000 Training Loss: 0.0429268553853035\n",
      "Epoch 15024/30000 Training Loss: 0.044753216207027435\n",
      "Epoch 15025/30000 Training Loss: 0.04236919432878494\n",
      "Epoch 15026/30000 Training Loss: 0.05048827454447746\n",
      "Epoch 15027/30000 Training Loss: 0.04250303655862808\n",
      "Epoch 15028/30000 Training Loss: 0.046614110469818115\n",
      "Epoch 15029/30000 Training Loss: 0.04711800441145897\n",
      "Epoch 15030/30000 Training Loss: 0.036454007029533386\n",
      "Epoch 15031/30000 Training Loss: 0.05084159970283508\n",
      "Epoch 15032/30000 Training Loss: 0.046135254204273224\n",
      "Epoch 15033/30000 Training Loss: 0.044351138174533844\n",
      "Epoch 15034/30000 Training Loss: 0.05373978614807129\n",
      "Epoch 15035/30000 Training Loss: 0.05136928707361221\n",
      "Epoch 15036/30000 Training Loss: 0.048757340759038925\n",
      "Epoch 15037/30000 Training Loss: 0.044532909989356995\n",
      "Epoch 15038/30000 Training Loss: 0.0381929911673069\n",
      "Epoch 15039/30000 Training Loss: 0.03939678147435188\n",
      "Epoch 15040/30000 Training Loss: 0.04135355353355408\n",
      "Epoch 15041/30000 Training Loss: 0.04005618765950203\n",
      "Epoch 15042/30000 Training Loss: 0.036240898072719574\n",
      "Epoch 15043/30000 Training Loss: 0.04081779718399048\n",
      "Epoch 15044/30000 Training Loss: 0.0498664565384388\n",
      "Epoch 15045/30000 Training Loss: 0.03647611662745476\n",
      "Epoch 15046/30000 Training Loss: 0.04307883232831955\n",
      "Epoch 15047/30000 Training Loss: 0.043659914284944534\n",
      "Epoch 15048/30000 Training Loss: 0.0531611368060112\n",
      "Epoch 15049/30000 Training Loss: 0.044617779552936554\n",
      "Epoch 15050/30000 Training Loss: 0.04792670160531998\n",
      "Epoch 15050/30000 Validation Loss: 0.03904183954000473\n",
      "Epoch 15051/30000 Training Loss: 0.03994954749941826\n",
      "Epoch 15052/30000 Training Loss: 0.04303247481584549\n",
      "Epoch 15053/30000 Training Loss: 0.05028636008501053\n",
      "Epoch 15054/30000 Training Loss: 0.032838523387908936\n",
      "Epoch 15055/30000 Training Loss: 0.03868328779935837\n",
      "Epoch 15056/30000 Training Loss: 0.04343988373875618\n",
      "Epoch 15057/30000 Training Loss: 0.04284234344959259\n",
      "Epoch 15058/30000 Training Loss: 0.03785569593310356\n",
      "Epoch 15059/30000 Training Loss: 0.041045404970645905\n",
      "Epoch 15060/30000 Training Loss: 0.03925933688879013\n",
      "Epoch 15061/30000 Training Loss: 0.044880665838718414\n",
      "Epoch 15062/30000 Training Loss: 0.045365285128355026\n",
      "Epoch 15063/30000 Training Loss: 0.042522579431533813\n",
      "Epoch 15064/30000 Training Loss: 0.04572887346148491\n",
      "Epoch 15065/30000 Training Loss: 0.03812654688954353\n",
      "Epoch 15066/30000 Training Loss: 0.038692180067300797\n",
      "Epoch 15067/30000 Training Loss: 0.04031892865896225\n",
      "Epoch 15068/30000 Training Loss: 0.04550570994615555\n",
      "Epoch 15069/30000 Training Loss: 0.04887660592794418\n",
      "Epoch 15070/30000 Training Loss: 0.046072326600551605\n",
      "Epoch 15071/30000 Training Loss: 0.03777407854795456\n",
      "Epoch 15072/30000 Training Loss: 0.03788875415921211\n",
      "Epoch 15073/30000 Training Loss: 0.04419541731476784\n",
      "Epoch 15074/30000 Training Loss: 0.04106869176030159\n",
      "Epoch 15075/30000 Training Loss: 0.047659799456596375\n",
      "Epoch 15076/30000 Training Loss: 0.052065689116716385\n",
      "Epoch 15077/30000 Training Loss: 0.03709486499428749\n",
      "Epoch 15078/30000 Training Loss: 0.036315541714429855\n",
      "Epoch 15079/30000 Training Loss: 0.04638439416885376\n",
      "Epoch 15080/30000 Training Loss: 0.049978695809841156\n",
      "Epoch 15081/30000 Training Loss: 0.05538971349596977\n",
      "Epoch 15082/30000 Training Loss: 0.038431890308856964\n",
      "Epoch 15083/30000 Training Loss: 0.039384398609399796\n",
      "Epoch 15084/30000 Training Loss: 0.044038522988557816\n",
      "Epoch 15085/30000 Training Loss: 0.03785000368952751\n",
      "Epoch 15086/30000 Training Loss: 0.04180051013827324\n",
      "Epoch 15087/30000 Training Loss: 0.048083286732435226\n",
      "Epoch 15088/30000 Training Loss: 0.043938007205724716\n",
      "Epoch 15089/30000 Training Loss: 0.04474540799856186\n",
      "Epoch 15090/30000 Training Loss: 0.03591256961226463\n",
      "Epoch 15091/30000 Training Loss: 0.037566740065813065\n",
      "Epoch 15092/30000 Training Loss: 0.03798425942659378\n",
      "Epoch 15093/30000 Training Loss: 0.05070861428976059\n",
      "Epoch 15094/30000 Training Loss: 0.04188771918416023\n",
      "Epoch 15095/30000 Training Loss: 0.045903321355581284\n",
      "Epoch 15096/30000 Training Loss: 0.03672359511256218\n",
      "Epoch 15097/30000 Training Loss: 0.04401054233312607\n",
      "Epoch 15098/30000 Training Loss: 0.04971551150083542\n",
      "Epoch 15099/30000 Training Loss: 0.04139721393585205\n",
      "Epoch 15100/30000 Training Loss: 0.04323148727416992\n",
      "Epoch 15100/30000 Validation Loss: 0.04106762632727623\n",
      "Epoch 15101/30000 Training Loss: 0.046145904809236526\n",
      "Epoch 15102/30000 Training Loss: 0.040741048753261566\n",
      "Epoch 15103/30000 Training Loss: 0.04177606850862503\n",
      "Epoch 15104/30000 Training Loss: 0.0480131134390831\n",
      "Epoch 15105/30000 Training Loss: 0.052197158336639404\n",
      "Epoch 15106/30000 Training Loss: 0.04387245327234268\n",
      "Epoch 15107/30000 Training Loss: 0.03545523062348366\n",
      "Epoch 15108/30000 Training Loss: 0.03830759972333908\n",
      "Epoch 15109/30000 Training Loss: 0.043216295540332794\n",
      "Epoch 15110/30000 Training Loss: 0.051906753331422806\n",
      "Epoch 15111/30000 Training Loss: 0.03900230675935745\n",
      "Epoch 15112/30000 Training Loss: 0.04377928748726845\n",
      "Epoch 15113/30000 Training Loss: 0.04070354625582695\n",
      "Epoch 15114/30000 Training Loss: 0.04535236582159996\n",
      "Epoch 15115/30000 Training Loss: 0.03664398938417435\n",
      "Epoch 15116/30000 Training Loss: 0.04147058725357056\n",
      "Epoch 15117/30000 Training Loss: 0.04418426752090454\n",
      "Epoch 15118/30000 Training Loss: 0.05048860237002373\n",
      "Epoch 15119/30000 Training Loss: 0.04511917382478714\n",
      "Epoch 15120/30000 Training Loss: 0.04049112647771835\n",
      "Epoch 15121/30000 Training Loss: 0.048265404999256134\n",
      "Epoch 15122/30000 Training Loss: 0.0527968592941761\n",
      "Epoch 15123/30000 Training Loss: 0.047267038375139236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15124/30000 Training Loss: 0.05135679244995117\n",
      "Epoch 15125/30000 Training Loss: 0.037291042506694794\n",
      "Epoch 15126/30000 Training Loss: 0.03362999111413956\n",
      "Epoch 15127/30000 Training Loss: 0.0434061698615551\n",
      "Epoch 15128/30000 Training Loss: 0.04526811093091965\n",
      "Epoch 15129/30000 Training Loss: 0.042872585356235504\n",
      "Epoch 15130/30000 Training Loss: 0.05502074211835861\n",
      "Epoch 15131/30000 Training Loss: 0.047315292060375214\n",
      "Epoch 15132/30000 Training Loss: 0.04065969958901405\n",
      "Epoch 15133/30000 Training Loss: 0.042162247002124786\n",
      "Epoch 15134/30000 Training Loss: 0.04580210894346237\n",
      "Epoch 15135/30000 Training Loss: 0.04162765294313431\n",
      "Epoch 15136/30000 Training Loss: 0.048704344779253006\n",
      "Epoch 15137/30000 Training Loss: 0.046349894255399704\n",
      "Epoch 15138/30000 Training Loss: 0.045902736485004425\n",
      "Epoch 15139/30000 Training Loss: 0.04214373230934143\n",
      "Epoch 15140/30000 Training Loss: 0.048275284469127655\n",
      "Epoch 15141/30000 Training Loss: 0.03838676959276199\n",
      "Epoch 15142/30000 Training Loss: 0.04669579863548279\n",
      "Epoch 15143/30000 Training Loss: 0.048369400203228\n",
      "Epoch 15144/30000 Training Loss: 0.04239479452371597\n",
      "Epoch 15145/30000 Training Loss: 0.04381968080997467\n",
      "Epoch 15146/30000 Training Loss: 0.04232027381658554\n",
      "Epoch 15147/30000 Training Loss: 0.03421150520443916\n",
      "Epoch 15148/30000 Training Loss: 0.038012195378541946\n",
      "Epoch 15149/30000 Training Loss: 0.05179643630981445\n",
      "Epoch 15150/30000 Training Loss: 0.04055926203727722\n",
      "Epoch 15150/30000 Validation Loss: 0.041556768119335175\n",
      "Epoch 15151/30000 Training Loss: 0.051926035434007645\n",
      "Epoch 15152/30000 Training Loss: 0.05815387889742851\n",
      "Epoch 15153/30000 Training Loss: 0.03964638337492943\n",
      "Epoch 15154/30000 Training Loss: 0.031838249415159225\n",
      "Epoch 15155/30000 Training Loss: 0.04005798324942589\n",
      "Epoch 15156/30000 Training Loss: 0.049175478518009186\n",
      "Epoch 15157/30000 Training Loss: 0.03876803070306778\n",
      "Epoch 15158/30000 Training Loss: 0.04566652700304985\n",
      "Epoch 15159/30000 Training Loss: 0.040911637246608734\n",
      "Epoch 15160/30000 Training Loss: 0.0528869703412056\n",
      "Epoch 15161/30000 Training Loss: 0.04632002115249634\n",
      "Epoch 15162/30000 Training Loss: 0.041696976870298386\n",
      "Epoch 15163/30000 Training Loss: 0.05155178904533386\n",
      "Epoch 15164/30000 Training Loss: 0.05297071859240532\n",
      "Epoch 15165/30000 Training Loss: 0.04188302159309387\n",
      "Epoch 15166/30000 Training Loss: 0.047291893512010574\n",
      "Epoch 15167/30000 Training Loss: 0.04771963506937027\n",
      "Epoch 15168/30000 Training Loss: 0.04499321058392525\n",
      "Epoch 15169/30000 Training Loss: 0.048503998667001724\n",
      "Epoch 15170/30000 Training Loss: 0.04558597505092621\n",
      "Epoch 15171/30000 Training Loss: 0.0448014959692955\n",
      "Epoch 15172/30000 Training Loss: 0.03914663940668106\n",
      "Epoch 15173/30000 Training Loss: 0.044847309589385986\n",
      "Epoch 15174/30000 Training Loss: 0.04543645307421684\n",
      "Epoch 15175/30000 Training Loss: 0.04506010562181473\n",
      "Epoch 15176/30000 Training Loss: 0.03911188617348671\n",
      "Epoch 15177/30000 Training Loss: 0.04547639191150665\n",
      "Epoch 15178/30000 Training Loss: 0.039343852549791336\n",
      "Epoch 15179/30000 Training Loss: 0.03374455124139786\n",
      "Epoch 15180/30000 Training Loss: 0.03787214681506157\n",
      "Epoch 15181/30000 Training Loss: 0.04040561616420746\n",
      "Epoch 15182/30000 Training Loss: 0.04694423824548721\n",
      "Epoch 15183/30000 Training Loss: 0.03992793709039688\n",
      "Epoch 15184/30000 Training Loss: 0.044517889618873596\n",
      "Epoch 15185/30000 Training Loss: 0.04335184395313263\n",
      "Epoch 15186/30000 Training Loss: 0.04353618994355202\n",
      "Epoch 15187/30000 Training Loss: 0.04220498353242874\n",
      "Epoch 15188/30000 Training Loss: 0.04795847088098526\n",
      "Epoch 15189/30000 Training Loss: 0.05106304958462715\n",
      "Epoch 15190/30000 Training Loss: 0.04571818932890892\n",
      "Epoch 15191/30000 Training Loss: 0.04656101018190384\n",
      "Epoch 15192/30000 Training Loss: 0.05236756056547165\n",
      "Epoch 15193/30000 Training Loss: 0.04359446093440056\n",
      "Epoch 15194/30000 Training Loss: 0.05116903781890869\n",
      "Epoch 15195/30000 Training Loss: 0.044523853808641434\n",
      "Epoch 15196/30000 Training Loss: 0.05374564602971077\n",
      "Epoch 15197/30000 Training Loss: 0.039973337203264236\n",
      "Epoch 15198/30000 Training Loss: 0.04628068581223488\n",
      "Epoch 15199/30000 Training Loss: 0.042204875499010086\n",
      "Epoch 15200/30000 Training Loss: 0.03732055425643921\n",
      "Epoch 15200/30000 Validation Loss: 0.04566227272152901\n",
      "Epoch 15201/30000 Training Loss: 0.04764730855822563\n",
      "Epoch 15202/30000 Training Loss: 0.04738611727952957\n",
      "Epoch 15203/30000 Training Loss: 0.04491904005408287\n",
      "Epoch 15204/30000 Training Loss: 0.046629421412944794\n",
      "Epoch 15205/30000 Training Loss: 0.04624027758836746\n",
      "Epoch 15206/30000 Training Loss: 0.04133708402514458\n",
      "Epoch 15207/30000 Training Loss: 0.039773114025592804\n",
      "Epoch 15208/30000 Training Loss: 0.04624048247933388\n",
      "Epoch 15209/30000 Training Loss: 0.0441303588449955\n",
      "Epoch 15210/30000 Training Loss: 0.039924949407577515\n",
      "Epoch 15211/30000 Training Loss: 0.05014598369598389\n",
      "Epoch 15212/30000 Training Loss: 0.03854144364595413\n",
      "Epoch 15213/30000 Training Loss: 0.0444982536137104\n",
      "Epoch 15214/30000 Training Loss: 0.04184266924858093\n",
      "Epoch 15215/30000 Training Loss: 0.04978685826063156\n",
      "Epoch 15216/30000 Training Loss: 0.0336318165063858\n",
      "Epoch 15217/30000 Training Loss: 0.04154469445347786\n",
      "Epoch 15218/30000 Training Loss: 0.039814241230487823\n",
      "Epoch 15219/30000 Training Loss: 0.0419628843665123\n",
      "Epoch 15220/30000 Training Loss: 0.043556831777095795\n",
      "Epoch 15221/30000 Training Loss: 0.04058833792805672\n",
      "Epoch 15222/30000 Training Loss: 0.04130871593952179\n",
      "Epoch 15223/30000 Training Loss: 0.04678967595100403\n",
      "Epoch 15224/30000 Training Loss: 0.043869342654943466\n",
      "Epoch 15225/30000 Training Loss: 0.04446851462125778\n",
      "Epoch 15226/30000 Training Loss: 0.04038041830062866\n",
      "Epoch 15227/30000 Training Loss: 0.04365212470293045\n",
      "Epoch 15228/30000 Training Loss: 0.04573970288038254\n",
      "Epoch 15229/30000 Training Loss: 0.04351760074496269\n",
      "Epoch 15230/30000 Training Loss: 0.04207185283303261\n",
      "Epoch 15231/30000 Training Loss: 0.046828124672174454\n",
      "Epoch 15232/30000 Training Loss: 0.0445987805724144\n",
      "Epoch 15233/30000 Training Loss: 0.040141791105270386\n",
      "Epoch 15234/30000 Training Loss: 0.04019066318869591\n",
      "Epoch 15235/30000 Training Loss: 0.04258556663990021\n",
      "Epoch 15236/30000 Training Loss: 0.04788772389292717\n",
      "Epoch 15237/30000 Training Loss: 0.037594281136989594\n",
      "Epoch 15238/30000 Training Loss: 0.054053544998168945\n",
      "Epoch 15239/30000 Training Loss: 0.03838222473859787\n",
      "Epoch 15240/30000 Training Loss: 0.039720892906188965\n",
      "Epoch 15241/30000 Training Loss: 0.04194748029112816\n",
      "Epoch 15242/30000 Training Loss: 0.048699360340833664\n",
      "Epoch 15243/30000 Training Loss: 0.03569583594799042\n",
      "Epoch 15244/30000 Training Loss: 0.039093438535928726\n",
      "Epoch 15245/30000 Training Loss: 0.049706947058439255\n",
      "Epoch 15246/30000 Training Loss: 0.04630672186613083\n",
      "Epoch 15247/30000 Training Loss: 0.041806187480688095\n",
      "Epoch 15248/30000 Training Loss: 0.038174159824848175\n",
      "Epoch 15249/30000 Training Loss: 0.0381113663315773\n",
      "Epoch 15250/30000 Training Loss: 0.047389429062604904\n",
      "Epoch 15250/30000 Validation Loss: 0.03849218040704727\n",
      "Epoch 15251/30000 Training Loss: 0.037118829786777496\n",
      "Epoch 15252/30000 Training Loss: 0.04650672525167465\n",
      "Epoch 15253/30000 Training Loss: 0.04787389189004898\n",
      "Epoch 15254/30000 Training Loss: 0.04251759499311447\n",
      "Epoch 15255/30000 Training Loss: 0.042671702802181244\n",
      "Epoch 15256/30000 Training Loss: 0.05127028375864029\n",
      "Epoch 15257/30000 Training Loss: 0.045021336525678635\n",
      "Epoch 15258/30000 Training Loss: 0.040216438472270966\n",
      "Epoch 15259/30000 Training Loss: 0.04290821775794029\n",
      "Epoch 15260/30000 Training Loss: 0.04069142043590546\n",
      "Epoch 15261/30000 Training Loss: 0.047289371490478516\n",
      "Epoch 15262/30000 Training Loss: 0.04378341883420944\n",
      "Epoch 15263/30000 Training Loss: 0.04218858480453491\n",
      "Epoch 15264/30000 Training Loss: 0.038031429052352905\n",
      "Epoch 15265/30000 Training Loss: 0.03914870321750641\n",
      "Epoch 15266/30000 Training Loss: 0.04319832846522331\n",
      "Epoch 15267/30000 Training Loss: 0.03902284428477287\n",
      "Epoch 15268/30000 Training Loss: 0.04218775033950806\n",
      "Epoch 15269/30000 Training Loss: 0.041327234357595444\n",
      "Epoch 15270/30000 Training Loss: 0.04531148448586464\n",
      "Epoch 15271/30000 Training Loss: 0.03907719999551773\n",
      "Epoch 15272/30000 Training Loss: 0.04291588068008423\n",
      "Epoch 15273/30000 Training Loss: 0.04356726258993149\n",
      "Epoch 15274/30000 Training Loss: 0.0409931056201458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15275/30000 Training Loss: 0.05398700758814812\n",
      "Epoch 15276/30000 Training Loss: 0.048125527799129486\n",
      "Epoch 15277/30000 Training Loss: 0.03973337262868881\n",
      "Epoch 15278/30000 Training Loss: 0.038976702839136124\n",
      "Epoch 15279/30000 Training Loss: 0.0493040457367897\n",
      "Epoch 15280/30000 Training Loss: 0.04101432487368584\n",
      "Epoch 15281/30000 Training Loss: 0.04257355257868767\n",
      "Epoch 15282/30000 Training Loss: 0.045313917100429535\n",
      "Epoch 15283/30000 Training Loss: 0.04692305251955986\n",
      "Epoch 15284/30000 Training Loss: 0.03913808614015579\n",
      "Epoch 15285/30000 Training Loss: 0.039797451347112656\n",
      "Epoch 15286/30000 Training Loss: 0.04091958329081535\n",
      "Epoch 15287/30000 Training Loss: 0.03704621642827988\n",
      "Epoch 15288/30000 Training Loss: 0.04643317684531212\n",
      "Epoch 15289/30000 Training Loss: 0.04173847287893295\n",
      "Epoch 15290/30000 Training Loss: 0.03956156224012375\n",
      "Epoch 15291/30000 Training Loss: 0.040889158844947815\n",
      "Epoch 15292/30000 Training Loss: 0.04643505811691284\n",
      "Epoch 15293/30000 Training Loss: 0.03407059237360954\n",
      "Epoch 15294/30000 Training Loss: 0.047611016780138016\n",
      "Epoch 15295/30000 Training Loss: 0.04095304757356644\n",
      "Epoch 15296/30000 Training Loss: 0.03570730239152908\n",
      "Epoch 15297/30000 Training Loss: 0.04643259197473526\n",
      "Epoch 15298/30000 Training Loss: 0.0468558594584465\n",
      "Epoch 15299/30000 Training Loss: 0.041408710181713104\n",
      "Epoch 15300/30000 Training Loss: 0.05859503149986267\n",
      "Epoch 15300/30000 Validation Loss: 0.04958122968673706\n",
      "Epoch 15301/30000 Training Loss: 0.03737823665142059\n",
      "Epoch 15302/30000 Training Loss: 0.04508204013109207\n",
      "Epoch 15303/30000 Training Loss: 0.0437576100230217\n",
      "Epoch 15304/30000 Training Loss: 0.03880170360207558\n",
      "Epoch 15305/30000 Training Loss: 0.04280756786465645\n",
      "Epoch 15306/30000 Training Loss: 0.043334752321243286\n",
      "Epoch 15307/30000 Training Loss: 0.04782523959875107\n",
      "Epoch 15308/30000 Training Loss: 0.039206430315971375\n",
      "Epoch 15309/30000 Training Loss: 0.04327041655778885\n",
      "Epoch 15310/30000 Training Loss: 0.041900623589754105\n",
      "Epoch 15311/30000 Training Loss: 0.036693643778562546\n",
      "Epoch 15312/30000 Training Loss: 0.03566289693117142\n",
      "Epoch 15313/30000 Training Loss: 0.04484906792640686\n",
      "Epoch 15314/30000 Training Loss: 0.048272959887981415\n",
      "Epoch 15315/30000 Training Loss: 0.050344742834568024\n",
      "Epoch 15316/30000 Training Loss: 0.033243969082832336\n",
      "Epoch 15317/30000 Training Loss: 0.04359959438443184\n",
      "Epoch 15318/30000 Training Loss: 0.04599945619702339\n",
      "Epoch 15319/30000 Training Loss: 0.04261355102062225\n",
      "Epoch 15320/30000 Training Loss: 0.04872672259807587\n",
      "Epoch 15321/30000 Training Loss: 0.04584463685750961\n",
      "Epoch 15322/30000 Training Loss: 0.053114742040634155\n",
      "Epoch 15323/30000 Training Loss: 0.048469312489032745\n",
      "Epoch 15324/30000 Training Loss: 0.04650808498263359\n",
      "Epoch 15325/30000 Training Loss: 0.03802308812737465\n",
      "Epoch 15326/30000 Training Loss: 0.04441490024328232\n",
      "Epoch 15327/30000 Training Loss: 0.04549982398748398\n",
      "Epoch 15328/30000 Training Loss: 0.043527744710445404\n",
      "Epoch 15329/30000 Training Loss: 0.040351975709199905\n",
      "Epoch 15330/30000 Training Loss: 0.03694825619459152\n",
      "Epoch 15331/30000 Training Loss: 0.04045382887125015\n",
      "Epoch 15332/30000 Training Loss: 0.03967507183551788\n",
      "Epoch 15333/30000 Training Loss: 0.0401565358042717\n",
      "Epoch 15334/30000 Training Loss: 0.04688306525349617\n",
      "Epoch 15335/30000 Training Loss: 0.0408073253929615\n",
      "Epoch 15336/30000 Training Loss: 0.034661512821912766\n",
      "Epoch 15337/30000 Training Loss: 0.04113307222723961\n",
      "Epoch 15338/30000 Training Loss: 0.049227114766836166\n",
      "Epoch 15339/30000 Training Loss: 0.05153574422001839\n",
      "Epoch 15340/30000 Training Loss: 0.05084032565355301\n",
      "Epoch 15341/30000 Training Loss: 0.04585578292608261\n",
      "Epoch 15342/30000 Training Loss: 0.05345544219017029\n",
      "Epoch 15343/30000 Training Loss: 0.0424041748046875\n",
      "Epoch 15344/30000 Training Loss: 0.04036988690495491\n",
      "Epoch 15345/30000 Training Loss: 0.04650241509079933\n",
      "Epoch 15346/30000 Training Loss: 0.0474853441119194\n",
      "Epoch 15347/30000 Training Loss: 0.050595104694366455\n",
      "Epoch 15348/30000 Training Loss: 0.03844968229532242\n",
      "Epoch 15349/30000 Training Loss: 0.0369589701294899\n",
      "Epoch 15350/30000 Training Loss: 0.04353939741849899\n",
      "Epoch 15350/30000 Validation Loss: 0.04312156140804291\n",
      "Epoch 15351/30000 Training Loss: 0.0433710515499115\n",
      "Epoch 15352/30000 Training Loss: 0.04465475305914879\n",
      "Epoch 15353/30000 Training Loss: 0.0392114594578743\n",
      "Epoch 15354/30000 Training Loss: 0.04706255719065666\n",
      "Epoch 15355/30000 Training Loss: 0.040371373295784\n",
      "Epoch 15356/30000 Training Loss: 0.04904662445187569\n",
      "Epoch 15357/30000 Training Loss: 0.04561394825577736\n",
      "Epoch 15358/30000 Training Loss: 0.03669033199548721\n",
      "Epoch 15359/30000 Training Loss: 0.045795369893312454\n",
      "Epoch 15360/30000 Training Loss: 0.05074653774499893\n",
      "Epoch 15361/30000 Training Loss: 0.043102867901325226\n",
      "Epoch 15362/30000 Training Loss: 0.038795918226242065\n",
      "Epoch 15363/30000 Training Loss: 0.04568824544548988\n",
      "Epoch 15364/30000 Training Loss: 0.04685916751623154\n",
      "Epoch 15365/30000 Training Loss: 0.04643739014863968\n",
      "Epoch 15366/30000 Training Loss: 0.04205130785703659\n",
      "Epoch 15367/30000 Training Loss: 0.041838858276605606\n",
      "Epoch 15368/30000 Training Loss: 0.042400989681482315\n",
      "Epoch 15369/30000 Training Loss: 0.0408867783844471\n",
      "Epoch 15370/30000 Training Loss: 0.044918835163116455\n",
      "Epoch 15371/30000 Training Loss: 0.04084018990397453\n",
      "Epoch 15372/30000 Training Loss: 0.04292327165603638\n",
      "Epoch 15373/30000 Training Loss: 0.03857414424419403\n",
      "Epoch 15374/30000 Training Loss: 0.04781350865960121\n",
      "Epoch 15375/30000 Training Loss: 0.039273809641599655\n",
      "Epoch 15376/30000 Training Loss: 0.043563298881053925\n",
      "Epoch 15377/30000 Training Loss: 0.0374373197555542\n",
      "Epoch 15378/30000 Training Loss: 0.0497811958193779\n",
      "Epoch 15379/30000 Training Loss: 0.04396612197160721\n",
      "Epoch 15380/30000 Training Loss: 0.04075198248028755\n",
      "Epoch 15381/30000 Training Loss: 0.04461643844842911\n",
      "Epoch 15382/30000 Training Loss: 0.03803042694926262\n",
      "Epoch 15383/30000 Training Loss: 0.041455499827861786\n",
      "Epoch 15384/30000 Training Loss: 0.03713029995560646\n",
      "Epoch 15385/30000 Training Loss: 0.03714132308959961\n",
      "Epoch 15386/30000 Training Loss: 0.04504799842834473\n",
      "Epoch 15387/30000 Training Loss: 0.0452849306166172\n",
      "Epoch 15388/30000 Training Loss: 0.045416366308927536\n",
      "Epoch 15389/30000 Training Loss: 0.045265231281518936\n",
      "Epoch 15390/30000 Training Loss: 0.04616954177618027\n",
      "Epoch 15391/30000 Training Loss: 0.04474510997533798\n",
      "Epoch 15392/30000 Training Loss: 0.0412849560379982\n",
      "Epoch 15393/30000 Training Loss: 0.04162583500146866\n",
      "Epoch 15394/30000 Training Loss: 0.040723297744989395\n",
      "Epoch 15395/30000 Training Loss: 0.04237619787454605\n",
      "Epoch 15396/30000 Training Loss: 0.048914581537246704\n",
      "Epoch 15397/30000 Training Loss: 0.03438738361001015\n",
      "Epoch 15398/30000 Training Loss: 0.03795179724693298\n",
      "Epoch 15399/30000 Training Loss: 0.03782620280981064\n",
      "Epoch 15400/30000 Training Loss: 0.0474928542971611\n",
      "Epoch 15400/30000 Validation Loss: 0.040453363209962845\n",
      "Epoch 15401/30000 Training Loss: 0.048037391155958176\n",
      "Epoch 15402/30000 Training Loss: 0.03713275492191315\n",
      "Epoch 15403/30000 Training Loss: 0.046621229499578476\n",
      "Epoch 15404/30000 Training Loss: 0.0402318611741066\n",
      "Epoch 15405/30000 Training Loss: 0.04259023815393448\n",
      "Epoch 15406/30000 Training Loss: 0.047271132469177246\n",
      "Epoch 15407/30000 Training Loss: 0.043940383940935135\n",
      "Epoch 15408/30000 Training Loss: 0.04224687069654465\n",
      "Epoch 15409/30000 Training Loss: 0.04029203951358795\n",
      "Epoch 15410/30000 Training Loss: 0.04020674526691437\n",
      "Epoch 15411/30000 Training Loss: 0.04084368422627449\n",
      "Epoch 15412/30000 Training Loss: 0.038605909794569016\n",
      "Epoch 15413/30000 Training Loss: 0.041048649698495865\n",
      "Epoch 15414/30000 Training Loss: 0.047207221388816833\n",
      "Epoch 15415/30000 Training Loss: 0.04693218320608139\n",
      "Epoch 15416/30000 Training Loss: 0.03385920077562332\n",
      "Epoch 15417/30000 Training Loss: 0.04436939209699631\n",
      "Epoch 15418/30000 Training Loss: 0.04036926105618477\n",
      "Epoch 15419/30000 Training Loss: 0.041354406625032425\n",
      "Epoch 15420/30000 Training Loss: 0.04350266233086586\n",
      "Epoch 15421/30000 Training Loss: 0.041094180196523666\n",
      "Epoch 15422/30000 Training Loss: 0.04387516900897026\n",
      "Epoch 15423/30000 Training Loss: 0.039767973124980927\n",
      "Epoch 15424/30000 Training Loss: 0.04573672264814377\n",
      "Epoch 15425/30000 Training Loss: 0.047589149326086044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15426/30000 Training Loss: 0.04544126242399216\n",
      "Epoch 15427/30000 Training Loss: 0.038324467837810516\n",
      "Epoch 15428/30000 Training Loss: 0.04402177408337593\n",
      "Epoch 15429/30000 Training Loss: 0.04547210782766342\n",
      "Epoch 15430/30000 Training Loss: 0.04498715326189995\n",
      "Epoch 15431/30000 Training Loss: 0.045877497643232346\n",
      "Epoch 15432/30000 Training Loss: 0.041031986474990845\n",
      "Epoch 15433/30000 Training Loss: 0.04174165800213814\n",
      "Epoch 15434/30000 Training Loss: 0.04760397598147392\n",
      "Epoch 15435/30000 Training Loss: 0.03794213384389877\n",
      "Epoch 15436/30000 Training Loss: 0.04831405729055405\n",
      "Epoch 15437/30000 Training Loss: 0.046578288078308105\n",
      "Epoch 15438/30000 Training Loss: 0.04526480287313461\n",
      "Epoch 15439/30000 Training Loss: 0.052939675748348236\n",
      "Epoch 15440/30000 Training Loss: 0.04296504706144333\n",
      "Epoch 15441/30000 Training Loss: 0.04325640946626663\n",
      "Epoch 15442/30000 Training Loss: 0.036199189722537994\n",
      "Epoch 15443/30000 Training Loss: 0.035980500280857086\n",
      "Epoch 15444/30000 Training Loss: 0.047680411487817764\n",
      "Epoch 15445/30000 Training Loss: 0.03841192275285721\n",
      "Epoch 15446/30000 Training Loss: 0.04781361669301987\n",
      "Epoch 15447/30000 Training Loss: 0.04390309378504753\n",
      "Epoch 15448/30000 Training Loss: 0.034952737390995026\n",
      "Epoch 15449/30000 Training Loss: 0.045609068125486374\n",
      "Epoch 15450/30000 Training Loss: 0.04326877370476723\n",
      "Epoch 15450/30000 Validation Loss: 0.038291823118925095\n",
      "Epoch 15451/30000 Training Loss: 0.04258224740624428\n",
      "Epoch 15452/30000 Training Loss: 0.038205523043870926\n",
      "Epoch 15453/30000 Training Loss: 0.052963338792324066\n",
      "Epoch 15454/30000 Training Loss: 0.03570592403411865\n",
      "Epoch 15455/30000 Training Loss: 0.05628524348139763\n",
      "Epoch 15456/30000 Training Loss: 0.04202566668391228\n",
      "Epoch 15457/30000 Training Loss: 0.04197172075510025\n",
      "Epoch 15458/30000 Training Loss: 0.040570929646492004\n",
      "Epoch 15459/30000 Training Loss: 0.046080153435468674\n",
      "Epoch 15460/30000 Training Loss: 0.040553297847509384\n",
      "Epoch 15461/30000 Training Loss: 0.04524507001042366\n",
      "Epoch 15462/30000 Training Loss: 0.039995528757572174\n",
      "Epoch 15463/30000 Training Loss: 0.04559461027383804\n",
      "Epoch 15464/30000 Training Loss: 0.04183383658528328\n",
      "Epoch 15465/30000 Training Loss: 0.04179785028100014\n",
      "Epoch 15466/30000 Training Loss: 0.03965101018548012\n",
      "Epoch 15467/30000 Training Loss: 0.03978881239891052\n",
      "Epoch 15468/30000 Training Loss: 0.04810865968465805\n",
      "Epoch 15469/30000 Training Loss: 0.03593453764915466\n",
      "Epoch 15470/30000 Training Loss: 0.040771838277578354\n",
      "Epoch 15471/30000 Training Loss: 0.049925509840250015\n",
      "Epoch 15472/30000 Training Loss: 0.04217123985290527\n",
      "Epoch 15473/30000 Training Loss: 0.04474368318915367\n",
      "Epoch 15474/30000 Training Loss: 0.04903334379196167\n",
      "Epoch 15475/30000 Training Loss: 0.046823859214782715\n",
      "Epoch 15476/30000 Training Loss: 0.04376399889588356\n",
      "Epoch 15477/30000 Training Loss: 0.04552074894309044\n",
      "Epoch 15478/30000 Training Loss: 0.04073817655444145\n",
      "Epoch 15479/30000 Training Loss: 0.043130047619342804\n",
      "Epoch 15480/30000 Training Loss: 0.046729784458875656\n",
      "Epoch 15481/30000 Training Loss: 0.039384208619594574\n",
      "Epoch 15482/30000 Training Loss: 0.04203076660633087\n",
      "Epoch 15483/30000 Training Loss: 0.046627506613731384\n",
      "Epoch 15484/30000 Training Loss: 0.04532722383737564\n",
      "Epoch 15485/30000 Training Loss: 0.04419862851500511\n",
      "Epoch 15486/30000 Training Loss: 0.043452657759189606\n",
      "Epoch 15487/30000 Training Loss: 0.048466913402080536\n",
      "Epoch 15488/30000 Training Loss: 0.043928224593400955\n",
      "Epoch 15489/30000 Training Loss: 0.04811951518058777\n",
      "Epoch 15490/30000 Training Loss: 0.04097384959459305\n",
      "Epoch 15491/30000 Training Loss: 0.049169350415468216\n",
      "Epoch 15492/30000 Training Loss: 0.04138827696442604\n",
      "Epoch 15493/30000 Training Loss: 0.038167644292116165\n",
      "Epoch 15494/30000 Training Loss: 0.05043083429336548\n",
      "Epoch 15495/30000 Training Loss: 0.043901942670345306\n",
      "Epoch 15496/30000 Training Loss: 0.037438828498125076\n",
      "Epoch 15497/30000 Training Loss: 0.04784799739718437\n",
      "Epoch 15498/30000 Training Loss: 0.040031690150499344\n",
      "Epoch 15499/30000 Training Loss: 0.04088759049773216\n",
      "Epoch 15500/30000 Training Loss: 0.04668248072266579\n",
      "Epoch 15500/30000 Validation Loss: 0.05112045258283615\n",
      "Epoch 15501/30000 Training Loss: 0.03922157362103462\n",
      "Epoch 15502/30000 Training Loss: 0.04184139147400856\n",
      "Epoch 15503/30000 Training Loss: 0.04330147057771683\n",
      "Epoch 15504/30000 Training Loss: 0.03953571245074272\n",
      "Epoch 15505/30000 Training Loss: 0.035185735672712326\n",
      "Epoch 15506/30000 Training Loss: 0.043167874217033386\n",
      "Epoch 15507/30000 Training Loss: 0.038514118641614914\n",
      "Epoch 15508/30000 Training Loss: 0.043415796011686325\n",
      "Epoch 15509/30000 Training Loss: 0.04552927240729332\n",
      "Epoch 15510/30000 Training Loss: 0.03810880705714226\n",
      "Epoch 15511/30000 Training Loss: 0.047346942126750946\n",
      "Epoch 15512/30000 Training Loss: 0.04510258883237839\n",
      "Epoch 15513/30000 Training Loss: 0.04217264801263809\n",
      "Epoch 15514/30000 Training Loss: 0.04533037543296814\n",
      "Epoch 15515/30000 Training Loss: 0.04707738012075424\n",
      "Epoch 15516/30000 Training Loss: 0.04326670989394188\n",
      "Epoch 15517/30000 Training Loss: 0.04042508453130722\n",
      "Epoch 15518/30000 Training Loss: 0.049862444400787354\n",
      "Epoch 15519/30000 Training Loss: 0.04055805876851082\n",
      "Epoch 15520/30000 Training Loss: 0.03745109587907791\n",
      "Epoch 15521/30000 Training Loss: 0.04210498556494713\n",
      "Epoch 15522/30000 Training Loss: 0.04943300038576126\n",
      "Epoch 15523/30000 Training Loss: 0.03786342591047287\n",
      "Epoch 15524/30000 Training Loss: 0.04553747922182083\n",
      "Epoch 15525/30000 Training Loss: 0.04543469101190567\n",
      "Epoch 15526/30000 Training Loss: 0.05018395185470581\n",
      "Epoch 15527/30000 Training Loss: 0.04571054130792618\n",
      "Epoch 15528/30000 Training Loss: 0.04386817663908005\n",
      "Epoch 15529/30000 Training Loss: 0.04129679128527641\n",
      "Epoch 15530/30000 Training Loss: 0.046246062964200974\n",
      "Epoch 15531/30000 Training Loss: 0.0428520031273365\n",
      "Epoch 15532/30000 Training Loss: 0.0463097020983696\n",
      "Epoch 15533/30000 Training Loss: 0.054457567632198334\n",
      "Epoch 15534/30000 Training Loss: 0.041221536695957184\n",
      "Epoch 15535/30000 Training Loss: 0.04030121490359306\n",
      "Epoch 15536/30000 Training Loss: 0.043900199234485626\n",
      "Epoch 15537/30000 Training Loss: 0.04114813357591629\n",
      "Epoch 15538/30000 Training Loss: 0.04347632825374603\n",
      "Epoch 15539/30000 Training Loss: 0.04680831730365753\n",
      "Epoch 15540/30000 Training Loss: 0.04300392046570778\n",
      "Epoch 15541/30000 Training Loss: 0.036618638783693314\n",
      "Epoch 15542/30000 Training Loss: 0.038562316447496414\n",
      "Epoch 15543/30000 Training Loss: 0.04274533689022064\n",
      "Epoch 15544/30000 Training Loss: 0.04145427793264389\n",
      "Epoch 15545/30000 Training Loss: 0.04804804548621178\n",
      "Epoch 15546/30000 Training Loss: 0.04034455120563507\n",
      "Epoch 15547/30000 Training Loss: 0.0395686998963356\n",
      "Epoch 15548/30000 Training Loss: 0.03937586024403572\n",
      "Epoch 15549/30000 Training Loss: 0.04039841145277023\n",
      "Epoch 15550/30000 Training Loss: 0.04459163546562195\n",
      "Epoch 15550/30000 Validation Loss: 0.043652065098285675\n",
      "Epoch 15551/30000 Training Loss: 0.04214778169989586\n",
      "Epoch 15552/30000 Training Loss: 0.04056360572576523\n",
      "Epoch 15553/30000 Training Loss: 0.047167059034109116\n",
      "Epoch 15554/30000 Training Loss: 0.046468157321214676\n",
      "Epoch 15555/30000 Training Loss: 0.03796538710594177\n",
      "Epoch 15556/30000 Training Loss: 0.04390782490372658\n",
      "Epoch 15557/30000 Training Loss: 0.03947998210787773\n",
      "Epoch 15558/30000 Training Loss: 0.04066392034292221\n",
      "Epoch 15559/30000 Training Loss: 0.03894731402397156\n",
      "Epoch 15560/30000 Training Loss: 0.04090776666998863\n",
      "Epoch 15561/30000 Training Loss: 0.037615008652210236\n",
      "Epoch 15562/30000 Training Loss: 0.04008723050355911\n",
      "Epoch 15563/30000 Training Loss: 0.03867188096046448\n",
      "Epoch 15564/30000 Training Loss: 0.04123348742723465\n",
      "Epoch 15565/30000 Training Loss: 0.044536493718624115\n",
      "Epoch 15566/30000 Training Loss: 0.053187113255262375\n",
      "Epoch 15567/30000 Training Loss: 0.03981636092066765\n",
      "Epoch 15568/30000 Training Loss: 0.0422174297273159\n",
      "Epoch 15569/30000 Training Loss: 0.0443546399474144\n",
      "Epoch 15570/30000 Training Loss: 0.04026809707283974\n",
      "Epoch 15571/30000 Training Loss: 0.039177145808935165\n",
      "Epoch 15572/30000 Training Loss: 0.04922451451420784\n",
      "Epoch 15573/30000 Training Loss: 0.04446578770875931\n",
      "Epoch 15574/30000 Training Loss: 0.03855014592409134\n",
      "Epoch 15575/30000 Training Loss: 0.04404311627149582\n",
      "Epoch 15576/30000 Training Loss: 0.04848792403936386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15577/30000 Training Loss: 0.04241170361638069\n",
      "Epoch 15578/30000 Training Loss: 0.04429033771157265\n",
      "Epoch 15579/30000 Training Loss: 0.05270085483789444\n",
      "Epoch 15580/30000 Training Loss: 0.03916258364915848\n",
      "Epoch 15581/30000 Training Loss: 0.05497964099049568\n",
      "Epoch 15582/30000 Training Loss: 0.043176282197237015\n",
      "Epoch 15583/30000 Training Loss: 0.042500805109739304\n",
      "Epoch 15584/30000 Training Loss: 0.03889370709657669\n",
      "Epoch 15585/30000 Training Loss: 0.050364721566438675\n",
      "Epoch 15586/30000 Training Loss: 0.045713070780038834\n",
      "Epoch 15587/30000 Training Loss: 0.04091627150774002\n",
      "Epoch 15588/30000 Training Loss: 0.04380278289318085\n",
      "Epoch 15589/30000 Training Loss: 0.037191249430179596\n",
      "Epoch 15590/30000 Training Loss: 0.03976939618587494\n",
      "Epoch 15591/30000 Training Loss: 0.04023968055844307\n",
      "Epoch 15592/30000 Training Loss: 0.04351261630654335\n",
      "Epoch 15593/30000 Training Loss: 0.04391045868396759\n",
      "Epoch 15594/30000 Training Loss: 0.039320193231105804\n",
      "Epoch 15595/30000 Training Loss: 0.05561790615320206\n",
      "Epoch 15596/30000 Training Loss: 0.0424574576318264\n",
      "Epoch 15597/30000 Training Loss: 0.04689885303378105\n",
      "Epoch 15598/30000 Training Loss: 0.04260127991437912\n",
      "Epoch 15599/30000 Training Loss: 0.04931674897670746\n",
      "Epoch 15600/30000 Training Loss: 0.041654303669929504\n",
      "Epoch 15600/30000 Validation Loss: 0.04983266070485115\n",
      "Epoch 15601/30000 Training Loss: 0.051476288586854935\n",
      "Epoch 15602/30000 Training Loss: 0.04285643249750137\n",
      "Epoch 15603/30000 Training Loss: 0.05001361295580864\n",
      "Epoch 15604/30000 Training Loss: 0.04327530413866043\n",
      "Epoch 15605/30000 Training Loss: 0.05553077533841133\n",
      "Epoch 15606/30000 Training Loss: 0.03621310740709305\n",
      "Epoch 15607/30000 Training Loss: 0.041154105216264725\n",
      "Epoch 15608/30000 Training Loss: 0.04359437897801399\n",
      "Epoch 15609/30000 Training Loss: 0.04320261627435684\n",
      "Epoch 15610/30000 Training Loss: 0.045512743294239044\n",
      "Epoch 15611/30000 Training Loss: 0.03986641764640808\n",
      "Epoch 15612/30000 Training Loss: 0.04835052788257599\n",
      "Epoch 15613/30000 Training Loss: 0.043771807104349136\n",
      "Epoch 15614/30000 Training Loss: 0.048944395035505295\n",
      "Epoch 15615/30000 Training Loss: 0.046206034719944\n",
      "Epoch 15616/30000 Training Loss: 0.04559803754091263\n",
      "Epoch 15617/30000 Training Loss: 0.0446094274520874\n",
      "Epoch 15618/30000 Training Loss: 0.04096897691488266\n",
      "Epoch 15619/30000 Training Loss: 0.0365542434155941\n",
      "Epoch 15620/30000 Training Loss: 0.04857175424695015\n",
      "Epoch 15621/30000 Training Loss: 0.04819050431251526\n",
      "Epoch 15622/30000 Training Loss: 0.04254017025232315\n",
      "Epoch 15623/30000 Training Loss: 0.03674674779176712\n",
      "Epoch 15624/30000 Training Loss: 0.04484519362449646\n",
      "Epoch 15625/30000 Training Loss: 0.03389299288392067\n",
      "Epoch 15626/30000 Training Loss: 0.05016915872693062\n",
      "Epoch 15627/30000 Training Loss: 0.04148929566144943\n",
      "Epoch 15628/30000 Training Loss: 0.04041336849331856\n",
      "Epoch 15629/30000 Training Loss: 0.03684748336672783\n",
      "Epoch 15630/30000 Training Loss: 0.03808062896132469\n",
      "Epoch 15631/30000 Training Loss: 0.03474109619855881\n",
      "Epoch 15632/30000 Training Loss: 0.04242783412337303\n",
      "Epoch 15633/30000 Training Loss: 0.040149204432964325\n",
      "Epoch 15634/30000 Training Loss: 0.039191387593746185\n",
      "Epoch 15635/30000 Training Loss: 0.047430820763111115\n",
      "Epoch 15636/30000 Training Loss: 0.038223374634981155\n",
      "Epoch 15637/30000 Training Loss: 0.039963193237781525\n",
      "Epoch 15638/30000 Training Loss: 0.045680027455091476\n",
      "Epoch 15639/30000 Training Loss: 0.04594794660806656\n",
      "Epoch 15640/30000 Training Loss: 0.045115068554878235\n",
      "Epoch 15641/30000 Training Loss: 0.044633734971284866\n",
      "Epoch 15642/30000 Training Loss: 0.05053401738405228\n",
      "Epoch 15643/30000 Training Loss: 0.047084636986255646\n",
      "Epoch 15644/30000 Training Loss: 0.037039823830127716\n",
      "Epoch 15645/30000 Training Loss: 0.04048801213502884\n",
      "Epoch 15646/30000 Training Loss: 0.04390234872698784\n",
      "Epoch 15647/30000 Training Loss: 0.04061643034219742\n",
      "Epoch 15648/30000 Training Loss: 0.050362728536129\n",
      "Epoch 15649/30000 Training Loss: 0.04381681978702545\n",
      "Epoch 15650/30000 Training Loss: 0.040930092334747314\n",
      "Epoch 15650/30000 Validation Loss: 0.048181407153606415\n",
      "Epoch 15651/30000 Training Loss: 0.040296684950590134\n",
      "Epoch 15652/30000 Training Loss: 0.0387372188270092\n",
      "Epoch 15653/30000 Training Loss: 0.04428098350763321\n",
      "Epoch 15654/30000 Training Loss: 0.046150241047143936\n",
      "Epoch 15655/30000 Training Loss: 0.04132118076086044\n",
      "Epoch 15656/30000 Training Loss: 0.039419569075107574\n",
      "Epoch 15657/30000 Training Loss: 0.04405982047319412\n",
      "Epoch 15658/30000 Training Loss: 0.04908189922571182\n",
      "Epoch 15659/30000 Training Loss: 0.03723878785967827\n",
      "Epoch 15660/30000 Training Loss: 0.04226316884160042\n",
      "Epoch 15661/30000 Training Loss: 0.04299195483326912\n",
      "Epoch 15662/30000 Training Loss: 0.04247122257947922\n",
      "Epoch 15663/30000 Training Loss: 0.04980000853538513\n",
      "Epoch 15664/30000 Training Loss: 0.04732208698987961\n",
      "Epoch 15665/30000 Training Loss: 0.04223901033401489\n",
      "Epoch 15666/30000 Training Loss: 0.04694555327296257\n",
      "Epoch 15667/30000 Training Loss: 0.04104221612215042\n",
      "Epoch 15668/30000 Training Loss: 0.042296718806028366\n",
      "Epoch 15669/30000 Training Loss: 0.04900909960269928\n",
      "Epoch 15670/30000 Training Loss: 0.04272697493433952\n",
      "Epoch 15671/30000 Training Loss: 0.03613273799419403\n",
      "Epoch 15672/30000 Training Loss: 0.04073421657085419\n",
      "Epoch 15673/30000 Training Loss: 0.041141998022794724\n",
      "Epoch 15674/30000 Training Loss: 0.0422762967646122\n",
      "Epoch 15675/30000 Training Loss: 0.03919007629156113\n",
      "Epoch 15676/30000 Training Loss: 0.03975466266274452\n",
      "Epoch 15677/30000 Training Loss: 0.0456094890832901\n",
      "Epoch 15678/30000 Training Loss: 0.04117102921009064\n",
      "Epoch 15679/30000 Training Loss: 0.04187125712633133\n",
      "Epoch 15680/30000 Training Loss: 0.04385686293244362\n",
      "Epoch 15681/30000 Training Loss: 0.044371891766786575\n",
      "Epoch 15682/30000 Training Loss: 0.047680050134658813\n",
      "Epoch 15683/30000 Training Loss: 0.05148870870471001\n",
      "Epoch 15684/30000 Training Loss: 0.04079671949148178\n",
      "Epoch 15685/30000 Training Loss: 0.040124405175447464\n",
      "Epoch 15686/30000 Training Loss: 0.04028894752264023\n",
      "Epoch 15687/30000 Training Loss: 0.04255974665284157\n",
      "Epoch 15688/30000 Training Loss: 0.052518315613269806\n",
      "Epoch 15689/30000 Training Loss: 0.036974500864744186\n",
      "Epoch 15690/30000 Training Loss: 0.040498487651348114\n",
      "Epoch 15691/30000 Training Loss: 0.03322155773639679\n",
      "Epoch 15692/30000 Training Loss: 0.03833814337849617\n",
      "Epoch 15693/30000 Training Loss: 0.03876730054616928\n",
      "Epoch 15694/30000 Training Loss: 0.04869871959090233\n",
      "Epoch 15695/30000 Training Loss: 0.04006645455956459\n",
      "Epoch 15696/30000 Training Loss: 0.04106256365776062\n",
      "Epoch 15697/30000 Training Loss: 0.03938853740692139\n",
      "Epoch 15698/30000 Training Loss: 0.03861742839217186\n",
      "Epoch 15699/30000 Training Loss: 0.03370393067598343\n",
      "Epoch 15700/30000 Training Loss: 0.03940611332654953\n",
      "Epoch 15700/30000 Validation Loss: 0.03777678310871124\n",
      "Epoch 15701/30000 Training Loss: 0.04326215013861656\n",
      "Epoch 15702/30000 Training Loss: 0.039861708879470825\n",
      "Epoch 15703/30000 Training Loss: 0.044675376266241074\n",
      "Epoch 15704/30000 Training Loss: 0.04114087298512459\n",
      "Epoch 15705/30000 Training Loss: 0.051393188536167145\n",
      "Epoch 15706/30000 Training Loss: 0.047084905207157135\n",
      "Epoch 15707/30000 Training Loss: 0.04969039559364319\n",
      "Epoch 15708/30000 Training Loss: 0.04286692291498184\n",
      "Epoch 15709/30000 Training Loss: 0.05071009323000908\n",
      "Epoch 15710/30000 Training Loss: 0.0549742691218853\n",
      "Epoch 15711/30000 Training Loss: 0.04951554536819458\n",
      "Epoch 15712/30000 Training Loss: 0.045709311962127686\n",
      "Epoch 15713/30000 Training Loss: 0.04113701730966568\n",
      "Epoch 15714/30000 Training Loss: 0.03652503341436386\n",
      "Epoch 15715/30000 Training Loss: 0.04795787110924721\n",
      "Epoch 15716/30000 Training Loss: 0.05112776905298233\n",
      "Epoch 15717/30000 Training Loss: 0.04053877666592598\n",
      "Epoch 15718/30000 Training Loss: 0.04111296311020851\n",
      "Epoch 15719/30000 Training Loss: 0.04459454491734505\n",
      "Epoch 15720/30000 Training Loss: 0.04767710715532303\n",
      "Epoch 15721/30000 Training Loss: 0.04941178113222122\n",
      "Epoch 15722/30000 Training Loss: 0.05403394252061844\n",
      "Epoch 15723/30000 Training Loss: 0.047144100069999695\n",
      "Epoch 15724/30000 Training Loss: 0.0556715652346611\n",
      "Epoch 15725/30000 Training Loss: 0.03918304666876793\n",
      "Epoch 15726/30000 Training Loss: 0.03475354239344597\n",
      "Epoch 15727/30000 Training Loss: 0.05790708586573601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15728/30000 Training Loss: 0.0405406579375267\n",
      "Epoch 15729/30000 Training Loss: 0.041761916130781174\n",
      "Epoch 15730/30000 Training Loss: 0.043352194130420685\n",
      "Epoch 15731/30000 Training Loss: 0.040428854525089264\n",
      "Epoch 15732/30000 Training Loss: 0.042320750653743744\n",
      "Epoch 15733/30000 Training Loss: 0.03743854910135269\n",
      "Epoch 15734/30000 Training Loss: 0.03240159526467323\n",
      "Epoch 15735/30000 Training Loss: 0.048297613859176636\n",
      "Epoch 15736/30000 Training Loss: 0.04953746497631073\n",
      "Epoch 15737/30000 Training Loss: 0.049137406051158905\n",
      "Epoch 15738/30000 Training Loss: 0.040749602019786835\n",
      "Epoch 15739/30000 Training Loss: 0.04633878916501999\n",
      "Epoch 15740/30000 Training Loss: 0.04904672130942345\n",
      "Epoch 15741/30000 Training Loss: 0.04381895810365677\n",
      "Epoch 15742/30000 Training Loss: 0.03535258024930954\n",
      "Epoch 15743/30000 Training Loss: 0.04843838885426521\n",
      "Epoch 15744/30000 Training Loss: 0.039891526103019714\n",
      "Epoch 15745/30000 Training Loss: 0.038987498730421066\n",
      "Epoch 15746/30000 Training Loss: 0.03641770780086517\n",
      "Epoch 15747/30000 Training Loss: 0.044652748852968216\n",
      "Epoch 15748/30000 Training Loss: 0.0528305247426033\n",
      "Epoch 15749/30000 Training Loss: 0.03741717338562012\n",
      "Epoch 15750/30000 Training Loss: 0.04469709098339081\n",
      "Epoch 15750/30000 Validation Loss: 0.045054275542497635\n",
      "Epoch 15751/30000 Training Loss: 0.03581519424915314\n",
      "Epoch 15752/30000 Training Loss: 0.04633070155978203\n",
      "Epoch 15753/30000 Training Loss: 0.044199515134096146\n",
      "Epoch 15754/30000 Training Loss: 0.046036262065172195\n",
      "Epoch 15755/30000 Training Loss: 0.04607009142637253\n",
      "Epoch 15756/30000 Training Loss: 0.05382175371050835\n",
      "Epoch 15757/30000 Training Loss: 0.0462162159383297\n",
      "Epoch 15758/30000 Training Loss: 0.041731394827365875\n",
      "Epoch 15759/30000 Training Loss: 0.044530343264341354\n",
      "Epoch 15760/30000 Training Loss: 0.03867225721478462\n",
      "Epoch 15761/30000 Training Loss: 0.04304945468902588\n",
      "Epoch 15762/30000 Training Loss: 0.059838615357875824\n",
      "Epoch 15763/30000 Training Loss: 0.04306420683860779\n",
      "Epoch 15764/30000 Training Loss: 0.04644845426082611\n",
      "Epoch 15765/30000 Training Loss: 0.04522644728422165\n",
      "Epoch 15766/30000 Training Loss: 0.04193204268813133\n",
      "Epoch 15767/30000 Training Loss: 0.044664416462183\n",
      "Epoch 15768/30000 Training Loss: 0.04886370152235031\n",
      "Epoch 15769/30000 Training Loss: 0.044906169176101685\n",
      "Epoch 15770/30000 Training Loss: 0.04404016584157944\n",
      "Epoch 15771/30000 Training Loss: 0.04504323750734329\n",
      "Epoch 15772/30000 Training Loss: 0.03703015670180321\n",
      "Epoch 15773/30000 Training Loss: 0.04614550620317459\n",
      "Epoch 15774/30000 Training Loss: 0.046627286821603775\n",
      "Epoch 15775/30000 Training Loss: 0.03719281777739525\n",
      "Epoch 15776/30000 Training Loss: 0.04258757829666138\n",
      "Epoch 15777/30000 Training Loss: 0.03969767317175865\n",
      "Epoch 15778/30000 Training Loss: 0.040310971438884735\n",
      "Epoch 15779/30000 Training Loss: 0.040143344551324844\n",
      "Epoch 15780/30000 Training Loss: 0.042025621980428696\n",
      "Epoch 15781/30000 Training Loss: 0.04780479520559311\n",
      "Epoch 15782/30000 Training Loss: 0.04130452871322632\n",
      "Epoch 15783/30000 Training Loss: 0.045560773462057114\n",
      "Epoch 15784/30000 Training Loss: 0.04090894013643265\n",
      "Epoch 15785/30000 Training Loss: 0.03425759822130203\n",
      "Epoch 15786/30000 Training Loss: 0.03770945221185684\n",
      "Epoch 15787/30000 Training Loss: 0.05531724542379379\n",
      "Epoch 15788/30000 Training Loss: 0.039172906428575516\n",
      "Epoch 15789/30000 Training Loss: 0.04325711727142334\n",
      "Epoch 15790/30000 Training Loss: 0.04057660698890686\n",
      "Epoch 15791/30000 Training Loss: 0.04258277267217636\n",
      "Epoch 15792/30000 Training Loss: 0.046632129698991776\n",
      "Epoch 15793/30000 Training Loss: 0.04098379239439964\n",
      "Epoch 15794/30000 Training Loss: 0.043041348457336426\n",
      "Epoch 15795/30000 Training Loss: 0.044976118952035904\n",
      "Epoch 15796/30000 Training Loss: 0.05350873991847038\n",
      "Epoch 15797/30000 Training Loss: 0.04118040204048157\n",
      "Epoch 15798/30000 Training Loss: 0.04376988485455513\n",
      "Epoch 15799/30000 Training Loss: 0.03600037470459938\n",
      "Epoch 15800/30000 Training Loss: 0.04227697104215622\n",
      "Epoch 15800/30000 Validation Loss: 0.04455412179231644\n",
      "Epoch 15801/30000 Training Loss: 0.04302781820297241\n",
      "Epoch 15802/30000 Training Loss: 0.04310047626495361\n",
      "Epoch 15803/30000 Training Loss: 0.03900711610913277\n",
      "Epoch 15804/30000 Training Loss: 0.047659702599048615\n",
      "Epoch 15805/30000 Training Loss: 0.044820331037044525\n",
      "Epoch 15806/30000 Training Loss: 0.04059993103146553\n",
      "Epoch 15807/30000 Training Loss: 0.04370734840631485\n",
      "Epoch 15808/30000 Training Loss: 0.04785003885626793\n",
      "Epoch 15809/30000 Training Loss: 0.04104994237422943\n",
      "Epoch 15810/30000 Training Loss: 0.04592563956975937\n",
      "Epoch 15811/30000 Training Loss: 0.038992565125226974\n",
      "Epoch 15812/30000 Training Loss: 0.03610449656844139\n",
      "Epoch 15813/30000 Training Loss: 0.03889024257659912\n",
      "Epoch 15814/30000 Training Loss: 0.04350960999727249\n",
      "Epoch 15815/30000 Training Loss: 0.045259881764650345\n",
      "Epoch 15816/30000 Training Loss: 0.043056853115558624\n",
      "Epoch 15817/30000 Training Loss: 0.03857962414622307\n",
      "Epoch 15818/30000 Training Loss: 0.046855781227350235\n",
      "Epoch 15819/30000 Training Loss: 0.046144258230924606\n",
      "Epoch 15820/30000 Training Loss: 0.04361719638109207\n",
      "Epoch 15821/30000 Training Loss: 0.044841218739748\n",
      "Epoch 15822/30000 Training Loss: 0.043406788259744644\n",
      "Epoch 15823/30000 Training Loss: 0.044679466634988785\n",
      "Epoch 15824/30000 Training Loss: 0.04393082857131958\n",
      "Epoch 15825/30000 Training Loss: 0.036851633340120316\n",
      "Epoch 15826/30000 Training Loss: 0.03320804238319397\n",
      "Epoch 15827/30000 Training Loss: 0.035713080316782\n",
      "Epoch 15828/30000 Training Loss: 0.05689419060945511\n",
      "Epoch 15829/30000 Training Loss: 0.0464559867978096\n",
      "Epoch 15830/30000 Training Loss: 0.05007794499397278\n",
      "Epoch 15831/30000 Training Loss: 0.04432446509599686\n",
      "Epoch 15832/30000 Training Loss: 0.0491652674973011\n",
      "Epoch 15833/30000 Training Loss: 0.04210500046610832\n",
      "Epoch 15834/30000 Training Loss: 0.04960279539227486\n",
      "Epoch 15835/30000 Training Loss: 0.04752347990870476\n",
      "Epoch 15836/30000 Training Loss: 0.043996185064315796\n",
      "Epoch 15837/30000 Training Loss: 0.0374111607670784\n",
      "Epoch 15838/30000 Training Loss: 0.047064151614904404\n",
      "Epoch 15839/30000 Training Loss: 0.033235326409339905\n",
      "Epoch 15840/30000 Training Loss: 0.045080456882715225\n",
      "Epoch 15841/30000 Training Loss: 0.044258832931518555\n",
      "Epoch 15842/30000 Training Loss: 0.03641418740153313\n",
      "Epoch 15843/30000 Training Loss: 0.042017899453639984\n",
      "Epoch 15844/30000 Training Loss: 0.03905670717358589\n",
      "Epoch 15845/30000 Training Loss: 0.041776321828365326\n",
      "Epoch 15846/30000 Training Loss: 0.039001453667879105\n",
      "Epoch 15847/30000 Training Loss: 0.040650367736816406\n",
      "Epoch 15848/30000 Training Loss: 0.04536908119916916\n",
      "Epoch 15849/30000 Training Loss: 0.04727113991975784\n",
      "Epoch 15850/30000 Training Loss: 0.03934650123119354\n",
      "Epoch 15850/30000 Validation Loss: 0.03919076547026634\n",
      "Epoch 15851/30000 Training Loss: 0.04581388086080551\n",
      "Epoch 15852/30000 Training Loss: 0.05157500505447388\n",
      "Epoch 15853/30000 Training Loss: 0.04600914567708969\n",
      "Epoch 15854/30000 Training Loss: 0.039629656821489334\n",
      "Epoch 15855/30000 Training Loss: 0.04254316911101341\n",
      "Epoch 15856/30000 Training Loss: 0.03797926381230354\n",
      "Epoch 15857/30000 Training Loss: 0.044273559004068375\n",
      "Epoch 15858/30000 Training Loss: 0.04598645493388176\n",
      "Epoch 15859/30000 Training Loss: 0.04632522538304329\n",
      "Epoch 15860/30000 Training Loss: 0.03612971305847168\n",
      "Epoch 15861/30000 Training Loss: 0.041289810091257095\n",
      "Epoch 15862/30000 Training Loss: 0.04429549723863602\n",
      "Epoch 15863/30000 Training Loss: 0.04462943226099014\n",
      "Epoch 15864/30000 Training Loss: 0.04476754367351532\n",
      "Epoch 15865/30000 Training Loss: 0.04517928138375282\n",
      "Epoch 15866/30000 Training Loss: 0.04928595945239067\n",
      "Epoch 15867/30000 Training Loss: 0.04116903617978096\n",
      "Epoch 15868/30000 Training Loss: 0.04434967413544655\n",
      "Epoch 15869/30000 Training Loss: 0.045923590660095215\n",
      "Epoch 15870/30000 Training Loss: 0.039675094187259674\n",
      "Epoch 15871/30000 Training Loss: 0.03272571414709091\n",
      "Epoch 15872/30000 Training Loss: 0.04595346376299858\n",
      "Epoch 15873/30000 Training Loss: 0.0386941060423851\n",
      "Epoch 15874/30000 Training Loss: 0.0443892776966095\n",
      "Epoch 15875/30000 Training Loss: 0.040555842220783234\n",
      "Epoch 15876/30000 Training Loss: 0.0399877205491066\n",
      "Epoch 15877/30000 Training Loss: 0.047506917268037796\n",
      "Epoch 15878/30000 Training Loss: 0.04471699148416519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15879/30000 Training Loss: 0.0511307530105114\n",
      "Epoch 15880/30000 Training Loss: 0.04371443763375282\n",
      "Epoch 15881/30000 Training Loss: 0.04418816417455673\n",
      "Epoch 15882/30000 Training Loss: 0.041120607405900955\n",
      "Epoch 15883/30000 Training Loss: 0.03683562949299812\n",
      "Epoch 15884/30000 Training Loss: 0.05401860550045967\n",
      "Epoch 15885/30000 Training Loss: 0.04016324132680893\n",
      "Epoch 15886/30000 Training Loss: 0.04446567967534065\n",
      "Epoch 15887/30000 Training Loss: 0.04834181070327759\n",
      "Epoch 15888/30000 Training Loss: 0.05344393104314804\n",
      "Epoch 15889/30000 Training Loss: 0.04114382341504097\n",
      "Epoch 15890/30000 Training Loss: 0.040481798350811005\n",
      "Epoch 15891/30000 Training Loss: 0.036870166659355164\n",
      "Epoch 15892/30000 Training Loss: 0.0440569743514061\n",
      "Epoch 15893/30000 Training Loss: 0.043208979070186615\n",
      "Epoch 15894/30000 Training Loss: 0.04043816775083542\n",
      "Epoch 15895/30000 Training Loss: 0.04538147896528244\n",
      "Epoch 15896/30000 Training Loss: 0.04405757784843445\n",
      "Epoch 15897/30000 Training Loss: 0.0427899993956089\n",
      "Epoch 15898/30000 Training Loss: 0.045032940804958344\n",
      "Epoch 15899/30000 Training Loss: 0.038790035992860794\n",
      "Epoch 15900/30000 Training Loss: 0.04034541919827461\n",
      "Epoch 15900/30000 Validation Loss: 0.04000132530927658\n",
      "Epoch 15901/30000 Training Loss: 0.04586530104279518\n",
      "Epoch 15902/30000 Training Loss: 0.038246702402830124\n",
      "Epoch 15903/30000 Training Loss: 0.044456884264945984\n",
      "Epoch 15904/30000 Training Loss: 0.04060940071940422\n",
      "Epoch 15905/30000 Training Loss: 0.05767541378736496\n",
      "Epoch 15906/30000 Training Loss: 0.04197549447417259\n",
      "Epoch 15907/30000 Training Loss: 0.05038999393582344\n",
      "Epoch 15908/30000 Training Loss: 0.0444195382297039\n",
      "Epoch 15909/30000 Training Loss: 0.03996827453374863\n",
      "Epoch 15910/30000 Training Loss: 0.04305741563439369\n",
      "Epoch 15911/30000 Training Loss: 0.04247937723994255\n",
      "Epoch 15912/30000 Training Loss: 0.03909139707684517\n",
      "Epoch 15913/30000 Training Loss: 0.03964386507868767\n",
      "Epoch 15914/30000 Training Loss: 0.04330845922231674\n",
      "Epoch 15915/30000 Training Loss: 0.04159952327609062\n",
      "Epoch 15916/30000 Training Loss: 0.05230267718434334\n",
      "Epoch 15917/30000 Training Loss: 0.03968114033341408\n",
      "Epoch 15918/30000 Training Loss: 0.038629449903964996\n",
      "Epoch 15919/30000 Training Loss: 0.04159308969974518\n",
      "Epoch 15920/30000 Training Loss: 0.038250602781772614\n",
      "Epoch 15921/30000 Training Loss: 0.044298745691776276\n",
      "Epoch 15922/30000 Training Loss: 0.04182630032300949\n",
      "Epoch 15923/30000 Training Loss: 0.04880128055810928\n",
      "Epoch 15924/30000 Training Loss: 0.040000755339860916\n",
      "Epoch 15925/30000 Training Loss: 0.04405498877167702\n",
      "Epoch 15926/30000 Training Loss: 0.04448511451482773\n",
      "Epoch 15927/30000 Training Loss: 0.03897714614868164\n",
      "Epoch 15928/30000 Training Loss: 0.03860259801149368\n",
      "Epoch 15929/30000 Training Loss: 0.03844945877790451\n",
      "Epoch 15930/30000 Training Loss: 0.03884712606668472\n",
      "Epoch 15931/30000 Training Loss: 0.0358797088265419\n",
      "Epoch 15932/30000 Training Loss: 0.04191409423947334\n",
      "Epoch 15933/30000 Training Loss: 0.05044114589691162\n",
      "Epoch 15934/30000 Training Loss: 0.046057648956775665\n",
      "Epoch 15935/30000 Training Loss: 0.04501932114362717\n",
      "Epoch 15936/30000 Training Loss: 0.045575086027383804\n",
      "Epoch 15937/30000 Training Loss: 0.04600119963288307\n",
      "Epoch 15938/30000 Training Loss: 0.04735548794269562\n",
      "Epoch 15939/30000 Training Loss: 0.040659669786691666\n",
      "Epoch 15940/30000 Training Loss: 0.04532451927661896\n",
      "Epoch 15941/30000 Training Loss: 0.04241660609841347\n",
      "Epoch 15942/30000 Training Loss: 0.04846319183707237\n",
      "Epoch 15943/30000 Training Loss: 0.040197767317295074\n",
      "Epoch 15944/30000 Training Loss: 0.04108470305800438\n",
      "Epoch 15945/30000 Training Loss: 0.04485823214054108\n",
      "Epoch 15946/30000 Training Loss: 0.03648005798459053\n",
      "Epoch 15947/30000 Training Loss: 0.03721027821302414\n",
      "Epoch 15948/30000 Training Loss: 0.043554652482271194\n",
      "Epoch 15949/30000 Training Loss: 0.05384621024131775\n",
      "Epoch 15950/30000 Training Loss: 0.045119889080524445\n",
      "Epoch 15950/30000 Validation Loss: 0.0446227490901947\n",
      "Epoch 15951/30000 Training Loss: 0.048041000962257385\n",
      "Epoch 15952/30000 Training Loss: 0.04196635261178017\n",
      "Epoch 15953/30000 Training Loss: 0.03628220781683922\n",
      "Epoch 15954/30000 Training Loss: 0.047945767641067505\n",
      "Epoch 15955/30000 Training Loss: 0.048867519944906235\n",
      "Epoch 15956/30000 Training Loss: 0.04762183129787445\n",
      "Epoch 15957/30000 Training Loss: 0.037870749831199646\n",
      "Epoch 15958/30000 Training Loss: 0.03750751167535782\n",
      "Epoch 15959/30000 Training Loss: 0.04599148780107498\n",
      "Epoch 15960/30000 Training Loss: 0.039606522768735886\n",
      "Epoch 15961/30000 Training Loss: 0.042537909001111984\n",
      "Epoch 15962/30000 Training Loss: 0.0359637513756752\n",
      "Epoch 15963/30000 Training Loss: 0.04140719026327133\n",
      "Epoch 15964/30000 Training Loss: 0.041267454624176025\n",
      "Epoch 15965/30000 Training Loss: 0.04842294380068779\n",
      "Epoch 15966/30000 Training Loss: 0.037137001752853394\n",
      "Epoch 15967/30000 Training Loss: 0.043871521949768066\n",
      "Epoch 15968/30000 Training Loss: 0.037395838648080826\n",
      "Epoch 15969/30000 Training Loss: 0.049938857555389404\n",
      "Epoch 15970/30000 Training Loss: 0.047393593937158585\n",
      "Epoch 15971/30000 Training Loss: 0.03652359917759895\n",
      "Epoch 15972/30000 Training Loss: 0.04460686445236206\n",
      "Epoch 15973/30000 Training Loss: 0.03433790057897568\n",
      "Epoch 15974/30000 Training Loss: 0.03955774009227753\n",
      "Epoch 15975/30000 Training Loss: 0.041477952152490616\n",
      "Epoch 15976/30000 Training Loss: 0.038910314440727234\n",
      "Epoch 15977/30000 Training Loss: 0.045052677392959595\n",
      "Epoch 15978/30000 Training Loss: 0.049934662878513336\n",
      "Epoch 15979/30000 Training Loss: 0.034957729279994965\n",
      "Epoch 15980/30000 Training Loss: 0.04322606325149536\n",
      "Epoch 15981/30000 Training Loss: 0.038787201046943665\n",
      "Epoch 15982/30000 Training Loss: 0.04313160851597786\n",
      "Epoch 15983/30000 Training Loss: 0.04224691540002823\n",
      "Epoch 15984/30000 Training Loss: 0.04163447767496109\n",
      "Epoch 15985/30000 Training Loss: 0.044040508568286896\n",
      "Epoch 15986/30000 Training Loss: 0.050031878054142\n",
      "Epoch 15987/30000 Training Loss: 0.045902855694293976\n",
      "Epoch 15988/30000 Training Loss: 0.04527968913316727\n",
      "Epoch 15989/30000 Training Loss: 0.045599497854709625\n",
      "Epoch 15990/30000 Training Loss: 0.04046992212533951\n",
      "Epoch 15991/30000 Training Loss: 0.04573412984609604\n",
      "Epoch 15992/30000 Training Loss: 0.04278763383626938\n",
      "Epoch 15993/30000 Training Loss: 0.050132907927036285\n",
      "Epoch 15994/30000 Training Loss: 0.04915647208690643\n",
      "Epoch 15995/30000 Training Loss: 0.04087819904088974\n",
      "Epoch 15996/30000 Training Loss: 0.04688670486211777\n",
      "Epoch 15997/30000 Training Loss: 0.03919481486082077\n",
      "Epoch 15998/30000 Training Loss: 0.043129317462444305\n",
      "Epoch 15999/30000 Training Loss: 0.04954501613974571\n",
      "Epoch 16000/30000 Training Loss: 0.04427807033061981\n",
      "Epoch 16000/30000 Validation Loss: 0.038829706609249115\n",
      "Epoch 16001/30000 Training Loss: 0.04488956928253174\n",
      "Epoch 16002/30000 Training Loss: 0.03817882388830185\n",
      "Epoch 16003/30000 Training Loss: 0.03641197457909584\n",
      "Epoch 16004/30000 Training Loss: 0.04314402863383293\n",
      "Epoch 16005/30000 Training Loss: 0.04623226076364517\n",
      "Epoch 16006/30000 Training Loss: 0.044849567115306854\n",
      "Epoch 16007/30000 Training Loss: 0.05557985231280327\n",
      "Epoch 16008/30000 Training Loss: 0.0463956817984581\n",
      "Epoch 16009/30000 Training Loss: 0.043582938611507416\n",
      "Epoch 16010/30000 Training Loss: 0.039427127689123154\n",
      "Epoch 16011/30000 Training Loss: 0.04075844585895538\n",
      "Epoch 16012/30000 Training Loss: 0.044206809252500534\n",
      "Epoch 16013/30000 Training Loss: 0.04565603286027908\n",
      "Epoch 16014/30000 Training Loss: 0.03819425031542778\n",
      "Epoch 16015/30000 Training Loss: 0.04002273455262184\n",
      "Epoch 16016/30000 Training Loss: 0.04240171238780022\n",
      "Epoch 16017/30000 Training Loss: 0.043611034750938416\n",
      "Epoch 16018/30000 Training Loss: 0.04255232214927673\n",
      "Epoch 16019/30000 Training Loss: 0.03543708100914955\n",
      "Epoch 16020/30000 Training Loss: 0.05416833236813545\n",
      "Epoch 16021/30000 Training Loss: 0.03898422792553902\n",
      "Epoch 16022/30000 Training Loss: 0.04867560416460037\n",
      "Epoch 16023/30000 Training Loss: 0.040208056569099426\n",
      "Epoch 16024/30000 Training Loss: 0.034976936876773834\n",
      "Epoch 16025/30000 Training Loss: 0.041172806173563004\n",
      "Epoch 16026/30000 Training Loss: 0.03861020877957344\n",
      "Epoch 16027/30000 Training Loss: 0.040933310985565186\n",
      "Epoch 16028/30000 Training Loss: 0.04046356678009033\n",
      "Epoch 16029/30000 Training Loss: 0.05602420121431351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16030/30000 Training Loss: 0.052929043769836426\n",
      "Epoch 16031/30000 Training Loss: 0.03913296386599541\n",
      "Epoch 16032/30000 Training Loss: 0.05083400756120682\n",
      "Epoch 16033/30000 Training Loss: 0.03956997022032738\n",
      "Epoch 16034/30000 Training Loss: 0.03898847848176956\n",
      "Epoch 16035/30000 Training Loss: 0.03817184269428253\n",
      "Epoch 16036/30000 Training Loss: 0.04033856838941574\n",
      "Epoch 16037/30000 Training Loss: 0.035328492522239685\n",
      "Epoch 16038/30000 Training Loss: 0.049063730984926224\n",
      "Epoch 16039/30000 Training Loss: 0.03441211208701134\n",
      "Epoch 16040/30000 Training Loss: 0.038269247859716415\n",
      "Epoch 16041/30000 Training Loss: 0.04238555580377579\n",
      "Epoch 16042/30000 Training Loss: 0.04621611163020134\n",
      "Epoch 16043/30000 Training Loss: 0.03974265977740288\n",
      "Epoch 16044/30000 Training Loss: 0.0340128056704998\n",
      "Epoch 16045/30000 Training Loss: 0.04047579690814018\n",
      "Epoch 16046/30000 Training Loss: 0.039398763328790665\n",
      "Epoch 16047/30000 Training Loss: 0.04032357782125473\n",
      "Epoch 16048/30000 Training Loss: 0.042293038219213486\n",
      "Epoch 16049/30000 Training Loss: 0.04654500633478165\n",
      "Epoch 16050/30000 Training Loss: 0.03687669336795807\n",
      "Epoch 16050/30000 Validation Loss: 0.0503968670964241\n",
      "Epoch 16051/30000 Training Loss: 0.043873585760593414\n",
      "Epoch 16052/30000 Training Loss: 0.04292432591319084\n",
      "Epoch 16053/30000 Training Loss: 0.040016043931245804\n",
      "Epoch 16054/30000 Training Loss: 0.05006933957338333\n",
      "Epoch 16055/30000 Training Loss: 0.03918147087097168\n",
      "Epoch 16056/30000 Training Loss: 0.044795166701078415\n",
      "Epoch 16057/30000 Training Loss: 0.04242786765098572\n",
      "Epoch 16058/30000 Training Loss: 0.03941994905471802\n",
      "Epoch 16059/30000 Training Loss: 0.04734504967927933\n",
      "Epoch 16060/30000 Training Loss: 0.0426039882004261\n",
      "Epoch 16061/30000 Training Loss: 0.03831092268228531\n",
      "Epoch 16062/30000 Training Loss: 0.04746213182806969\n",
      "Epoch 16063/30000 Training Loss: 0.04206118732690811\n",
      "Epoch 16064/30000 Training Loss: 0.04473108798265457\n",
      "Epoch 16065/30000 Training Loss: 0.04390248283743858\n",
      "Epoch 16066/30000 Training Loss: 0.044190216809511185\n",
      "Epoch 16067/30000 Training Loss: 0.03931097313761711\n",
      "Epoch 16068/30000 Training Loss: 0.0439193956553936\n",
      "Epoch 16069/30000 Training Loss: 0.041259221732616425\n",
      "Epoch 16070/30000 Training Loss: 0.04065173864364624\n",
      "Epoch 16071/30000 Training Loss: 0.04211686924099922\n",
      "Epoch 16072/30000 Training Loss: 0.051625095307826996\n",
      "Epoch 16073/30000 Training Loss: 0.041757967323064804\n",
      "Epoch 16074/30000 Training Loss: 0.04315202310681343\n",
      "Epoch 16075/30000 Training Loss: 0.03710486367344856\n",
      "Epoch 16076/30000 Training Loss: 0.0392342172563076\n",
      "Epoch 16077/30000 Training Loss: 0.041784461587667465\n",
      "Epoch 16078/30000 Training Loss: 0.0406450517475605\n",
      "Epoch 16079/30000 Training Loss: 0.0464111790060997\n",
      "Epoch 16080/30000 Training Loss: 0.041795771569013596\n",
      "Epoch 16081/30000 Training Loss: 0.04789183288812637\n",
      "Epoch 16082/30000 Training Loss: 0.047876786440610886\n",
      "Epoch 16083/30000 Training Loss: 0.054139088839292526\n",
      "Epoch 16084/30000 Training Loss: 0.040747832506895065\n",
      "Epoch 16085/30000 Training Loss: 0.04429059848189354\n",
      "Epoch 16086/30000 Training Loss: 0.04899933561682701\n",
      "Epoch 16087/30000 Training Loss: 0.0450531467795372\n",
      "Epoch 16088/30000 Training Loss: 0.04330243915319443\n",
      "Epoch 16089/30000 Training Loss: 0.03988724946975708\n",
      "Epoch 16090/30000 Training Loss: 0.04450017586350441\n",
      "Epoch 16091/30000 Training Loss: 0.043545808643102646\n",
      "Epoch 16092/30000 Training Loss: 0.045856207609176636\n",
      "Epoch 16093/30000 Training Loss: 0.04724986106157303\n",
      "Epoch 16094/30000 Training Loss: 0.04060962051153183\n",
      "Epoch 16095/30000 Training Loss: 0.0484122633934021\n",
      "Epoch 16096/30000 Training Loss: 0.04264963045716286\n",
      "Epoch 16097/30000 Training Loss: 0.04661927744746208\n",
      "Epoch 16098/30000 Training Loss: 0.052166301757097244\n",
      "Epoch 16099/30000 Training Loss: 0.05114410072565079\n",
      "Epoch 16100/30000 Training Loss: 0.049939241260290146\n",
      "Epoch 16100/30000 Validation Loss: 0.04406888037919998\n",
      "Epoch 16101/30000 Training Loss: 0.04590647295117378\n",
      "Epoch 16102/30000 Training Loss: 0.04275931417942047\n",
      "Epoch 16103/30000 Training Loss: 0.0404411181807518\n",
      "Epoch 16104/30000 Training Loss: 0.03616579622030258\n",
      "Epoch 16105/30000 Training Loss: 0.044156137853860855\n",
      "Epoch 16106/30000 Training Loss: 0.03984958305954933\n",
      "Epoch 16107/30000 Training Loss: 0.03888850659132004\n",
      "Epoch 16108/30000 Training Loss: 0.0394725501537323\n",
      "Epoch 16109/30000 Training Loss: 0.049738045781850815\n",
      "Epoch 16110/30000 Training Loss: 0.03795655444264412\n",
      "Epoch 16111/30000 Training Loss: 0.04422292485833168\n",
      "Epoch 16112/30000 Training Loss: 0.04504562169313431\n",
      "Epoch 16113/30000 Training Loss: 0.05343025177717209\n",
      "Epoch 16114/30000 Training Loss: 0.043945007026195526\n",
      "Epoch 16115/30000 Training Loss: 0.040579669177532196\n",
      "Epoch 16116/30000 Training Loss: 0.043164901435375214\n",
      "Epoch 16117/30000 Training Loss: 0.035595156252384186\n",
      "Epoch 16118/30000 Training Loss: 0.042822424322366714\n",
      "Epoch 16119/30000 Training Loss: 0.047235120087862015\n",
      "Epoch 16120/30000 Training Loss: 0.043964799493551254\n",
      "Epoch 16121/30000 Training Loss: 0.042739469558000565\n",
      "Epoch 16122/30000 Training Loss: 0.04434679448604584\n",
      "Epoch 16123/30000 Training Loss: 0.03961890563368797\n",
      "Epoch 16124/30000 Training Loss: 0.04126342386007309\n",
      "Epoch 16125/30000 Training Loss: 0.04888115078210831\n",
      "Epoch 16126/30000 Training Loss: 0.04187122732400894\n",
      "Epoch 16127/30000 Training Loss: 0.04193127527832985\n",
      "Epoch 16128/30000 Training Loss: 0.04460704326629639\n",
      "Epoch 16129/30000 Training Loss: 0.03861246258020401\n",
      "Epoch 16130/30000 Training Loss: 0.035624418407678604\n",
      "Epoch 16131/30000 Training Loss: 0.04445352405309677\n",
      "Epoch 16132/30000 Training Loss: 0.03970734775066376\n",
      "Epoch 16133/30000 Training Loss: 0.04810978099703789\n",
      "Epoch 16134/30000 Training Loss: 0.0526137538254261\n",
      "Epoch 16135/30000 Training Loss: 0.04527522251009941\n",
      "Epoch 16136/30000 Training Loss: 0.05115469545125961\n",
      "Epoch 16137/30000 Training Loss: 0.04499641805887222\n",
      "Epoch 16138/30000 Training Loss: 0.04276680201292038\n",
      "Epoch 16139/30000 Training Loss: 0.034543003886938095\n",
      "Epoch 16140/30000 Training Loss: 0.042295489460229874\n",
      "Epoch 16141/30000 Training Loss: 0.04344424977898598\n",
      "Epoch 16142/30000 Training Loss: 0.038164474070072174\n",
      "Epoch 16143/30000 Training Loss: 0.03501143306493759\n",
      "Epoch 16144/30000 Training Loss: 0.054219312965869904\n",
      "Epoch 16145/30000 Training Loss: 0.04270680993795395\n",
      "Epoch 16146/30000 Training Loss: 0.0330771766602993\n",
      "Epoch 16147/30000 Training Loss: 0.039214733988046646\n",
      "Epoch 16148/30000 Training Loss: 0.043651141226291656\n",
      "Epoch 16149/30000 Training Loss: 0.05030263215303421\n",
      "Epoch 16150/30000 Training Loss: 0.041643671691417694\n",
      "Epoch 16150/30000 Validation Loss: 0.041107069700956345\n",
      "Epoch 16151/30000 Training Loss: 0.05045255273580551\n",
      "Epoch 16152/30000 Training Loss: 0.05237289518117905\n",
      "Epoch 16153/30000 Training Loss: 0.03613904491066933\n",
      "Epoch 16154/30000 Training Loss: 0.04197077453136444\n",
      "Epoch 16155/30000 Training Loss: 0.043954409658908844\n",
      "Epoch 16156/30000 Training Loss: 0.04557884484529495\n",
      "Epoch 16157/30000 Training Loss: 0.043656911700963974\n",
      "Epoch 16158/30000 Training Loss: 0.04637821763753891\n",
      "Epoch 16159/30000 Training Loss: 0.040875256061553955\n",
      "Epoch 16160/30000 Training Loss: 0.04351165518164635\n",
      "Epoch 16161/30000 Training Loss: 0.037577398121356964\n",
      "Epoch 16162/30000 Training Loss: 0.046391092240810394\n",
      "Epoch 16163/30000 Training Loss: 0.04369785636663437\n",
      "Epoch 16164/30000 Training Loss: 0.04258609563112259\n",
      "Epoch 16165/30000 Training Loss: 0.044752348214387894\n",
      "Epoch 16166/30000 Training Loss: 0.04246004670858383\n",
      "Epoch 16167/30000 Training Loss: 0.0534980371594429\n",
      "Epoch 16168/30000 Training Loss: 0.03930244594812393\n",
      "Epoch 16169/30000 Training Loss: 0.04675384610891342\n",
      "Epoch 16170/30000 Training Loss: 0.03493981435894966\n",
      "Epoch 16171/30000 Training Loss: 0.04693937301635742\n",
      "Epoch 16172/30000 Training Loss: 0.04010261967778206\n",
      "Epoch 16173/30000 Training Loss: 0.043016914278268814\n",
      "Epoch 16174/30000 Training Loss: 0.03849517181515694\n",
      "Epoch 16175/30000 Training Loss: 0.03945356607437134\n",
      "Epoch 16176/30000 Training Loss: 0.03667651489377022\n",
      "Epoch 16177/30000 Training Loss: 0.03585276007652283\n",
      "Epoch 16178/30000 Training Loss: 0.04230936989188194\n",
      "Epoch 16179/30000 Training Loss: 0.04886571317911148\n",
      "Epoch 16180/30000 Training Loss: 0.04321197792887688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16181/30000 Training Loss: 0.04804440587759018\n",
      "Epoch 16182/30000 Training Loss: 0.04644305631518364\n",
      "Epoch 16183/30000 Training Loss: 0.045898545533418655\n",
      "Epoch 16184/30000 Training Loss: 0.04877609387040138\n",
      "Epoch 16185/30000 Training Loss: 0.035171620547771454\n",
      "Epoch 16186/30000 Training Loss: 0.04105595126748085\n",
      "Epoch 16187/30000 Training Loss: 0.04141717776656151\n",
      "Epoch 16188/30000 Training Loss: 0.04901091009378433\n",
      "Epoch 16189/30000 Training Loss: 0.04303479939699173\n",
      "Epoch 16190/30000 Training Loss: 0.0458247996866703\n",
      "Epoch 16191/30000 Training Loss: 0.047630082815885544\n",
      "Epoch 16192/30000 Training Loss: 0.04191867262125015\n",
      "Epoch 16193/30000 Training Loss: 0.03596238046884537\n",
      "Epoch 16194/30000 Training Loss: 0.03651080280542374\n",
      "Epoch 16195/30000 Training Loss: 0.05304611474275589\n",
      "Epoch 16196/30000 Training Loss: 0.04433359205722809\n",
      "Epoch 16197/30000 Training Loss: 0.04615422338247299\n",
      "Epoch 16198/30000 Training Loss: 0.04654567316174507\n",
      "Epoch 16199/30000 Training Loss: 0.03684721514582634\n",
      "Epoch 16200/30000 Training Loss: 0.04093613475561142\n",
      "Epoch 16200/30000 Validation Loss: 0.04520699754357338\n",
      "Epoch 16201/30000 Training Loss: 0.04849257692694664\n",
      "Epoch 16202/30000 Training Loss: 0.044872671365737915\n",
      "Epoch 16203/30000 Training Loss: 0.037639982998371124\n",
      "Epoch 16204/30000 Training Loss: 0.04356556385755539\n",
      "Epoch 16205/30000 Training Loss: 0.050771791487932205\n",
      "Epoch 16206/30000 Training Loss: 0.036571599543094635\n",
      "Epoch 16207/30000 Training Loss: 0.04075676202774048\n",
      "Epoch 16208/30000 Training Loss: 0.039786577224731445\n",
      "Epoch 16209/30000 Training Loss: 0.04955976828932762\n",
      "Epoch 16210/30000 Training Loss: 0.04565606638789177\n",
      "Epoch 16211/30000 Training Loss: 0.041169628500938416\n",
      "Epoch 16212/30000 Training Loss: 0.045821744948625565\n",
      "Epoch 16213/30000 Training Loss: 0.044916898012161255\n",
      "Epoch 16214/30000 Training Loss: 0.04421449825167656\n",
      "Epoch 16215/30000 Training Loss: 0.040071725845336914\n",
      "Epoch 16216/30000 Training Loss: 0.03955965116620064\n",
      "Epoch 16217/30000 Training Loss: 0.03813523054122925\n",
      "Epoch 16218/30000 Training Loss: 0.04300916567444801\n",
      "Epoch 16219/30000 Training Loss: 0.04026644304394722\n",
      "Epoch 16220/30000 Training Loss: 0.046548031270504\n",
      "Epoch 16221/30000 Training Loss: 0.03701375797390938\n",
      "Epoch 16222/30000 Training Loss: 0.03870955482125282\n",
      "Epoch 16223/30000 Training Loss: 0.04449363797903061\n",
      "Epoch 16224/30000 Training Loss: 0.04449146240949631\n",
      "Epoch 16225/30000 Training Loss: 0.04699910804629326\n",
      "Epoch 16226/30000 Training Loss: 0.04165590927004814\n",
      "Epoch 16227/30000 Training Loss: 0.046052318066358566\n",
      "Epoch 16228/30000 Training Loss: 0.04381735622882843\n",
      "Epoch 16229/30000 Training Loss: 0.043256811797618866\n",
      "Epoch 16230/30000 Training Loss: 0.046623192727565765\n",
      "Epoch 16231/30000 Training Loss: 0.05084856599569321\n",
      "Epoch 16232/30000 Training Loss: 0.04042845219373703\n",
      "Epoch 16233/30000 Training Loss: 0.037339456379413605\n",
      "Epoch 16234/30000 Training Loss: 0.05023826286196709\n",
      "Epoch 16235/30000 Training Loss: 0.03929333761334419\n",
      "Epoch 16236/30000 Training Loss: 0.03961076959967613\n",
      "Epoch 16237/30000 Training Loss: 0.04446159303188324\n",
      "Epoch 16238/30000 Training Loss: 0.042350150644779205\n",
      "Epoch 16239/30000 Training Loss: 0.041006047278642654\n",
      "Epoch 16240/30000 Training Loss: 0.03877513110637665\n",
      "Epoch 16241/30000 Training Loss: 0.039005860686302185\n",
      "Epoch 16242/30000 Training Loss: 0.04401601478457451\n",
      "Epoch 16243/30000 Training Loss: 0.050774503499269485\n",
      "Epoch 16244/30000 Training Loss: 0.03718014433979988\n",
      "Epoch 16245/30000 Training Loss: 0.04372643679380417\n",
      "Epoch 16246/30000 Training Loss: 0.03986551612615585\n",
      "Epoch 16247/30000 Training Loss: 0.03881336748600006\n",
      "Epoch 16248/30000 Training Loss: 0.04273918643593788\n",
      "Epoch 16249/30000 Training Loss: 0.04991140589118004\n",
      "Epoch 16250/30000 Training Loss: 0.041638970375061035\n",
      "Epoch 16250/30000 Validation Loss: 0.0427352711558342\n",
      "Epoch 16251/30000 Training Loss: 0.04926938936114311\n",
      "Epoch 16252/30000 Training Loss: 0.05012248829007149\n",
      "Epoch 16253/30000 Training Loss: 0.035563550889492035\n",
      "Epoch 16254/30000 Training Loss: 0.04020504653453827\n",
      "Epoch 16255/30000 Training Loss: 0.051288265734910965\n",
      "Epoch 16256/30000 Training Loss: 0.043799400329589844\n",
      "Epoch 16257/30000 Training Loss: 0.04226914420723915\n",
      "Epoch 16258/30000 Training Loss: 0.043103836476802826\n",
      "Epoch 16259/30000 Training Loss: 0.051868684589862823\n",
      "Epoch 16260/30000 Training Loss: 0.04273449257016182\n",
      "Epoch 16261/30000 Training Loss: 0.04727485775947571\n",
      "Epoch 16262/30000 Training Loss: 0.04223630949854851\n",
      "Epoch 16263/30000 Training Loss: 0.04292060434818268\n",
      "Epoch 16264/30000 Training Loss: 0.04190731793642044\n",
      "Epoch 16265/30000 Training Loss: 0.04673705995082855\n",
      "Epoch 16266/30000 Training Loss: 0.03473276272416115\n",
      "Epoch 16267/30000 Training Loss: 0.04300817474722862\n",
      "Epoch 16268/30000 Training Loss: 0.03853814676403999\n",
      "Epoch 16269/30000 Training Loss: 0.03880362957715988\n",
      "Epoch 16270/30000 Training Loss: 0.04172036796808243\n",
      "Epoch 16271/30000 Training Loss: 0.04198359325528145\n",
      "Epoch 16272/30000 Training Loss: 0.039161406457424164\n",
      "Epoch 16273/30000 Training Loss: 0.04892304167151451\n",
      "Epoch 16274/30000 Training Loss: 0.044335465878248215\n",
      "Epoch 16275/30000 Training Loss: 0.042255353182554245\n",
      "Epoch 16276/30000 Training Loss: 0.039248861372470856\n",
      "Epoch 16277/30000 Training Loss: 0.04586175084114075\n",
      "Epoch 16278/30000 Training Loss: 0.03936893492937088\n",
      "Epoch 16279/30000 Training Loss: 0.04197954386472702\n",
      "Epoch 16280/30000 Training Loss: 0.04363902658224106\n",
      "Epoch 16281/30000 Training Loss: 0.03608626872301102\n",
      "Epoch 16282/30000 Training Loss: 0.04342065006494522\n",
      "Epoch 16283/30000 Training Loss: 0.04810761660337448\n",
      "Epoch 16284/30000 Training Loss: 0.04694174602627754\n",
      "Epoch 16285/30000 Training Loss: 0.0377943180501461\n",
      "Epoch 16286/30000 Training Loss: 0.04547891765832901\n",
      "Epoch 16287/30000 Training Loss: 0.04235516116023064\n",
      "Epoch 16288/30000 Training Loss: 0.05192158743739128\n",
      "Epoch 16289/30000 Training Loss: 0.04174555838108063\n",
      "Epoch 16290/30000 Training Loss: 0.04672335088253021\n",
      "Epoch 16291/30000 Training Loss: 0.04098047316074371\n",
      "Epoch 16292/30000 Training Loss: 0.049399539828300476\n",
      "Epoch 16293/30000 Training Loss: 0.04570160061120987\n",
      "Epoch 16294/30000 Training Loss: 0.0411602221429348\n",
      "Epoch 16295/30000 Training Loss: 0.04648739472031593\n",
      "Epoch 16296/30000 Training Loss: 0.04971702769398689\n",
      "Epoch 16297/30000 Training Loss: 0.04679540544748306\n",
      "Epoch 16298/30000 Training Loss: 0.04979810491204262\n",
      "Epoch 16299/30000 Training Loss: 0.05038483813405037\n",
      "Epoch 16300/30000 Training Loss: 0.04555835202336311\n",
      "Epoch 16300/30000 Validation Loss: 0.0476733073592186\n",
      "Epoch 16301/30000 Training Loss: 0.04579427093267441\n",
      "Epoch 16302/30000 Training Loss: 0.0350005142390728\n",
      "Epoch 16303/30000 Training Loss: 0.04609449952840805\n",
      "Epoch 16304/30000 Training Loss: 0.04620891809463501\n",
      "Epoch 16305/30000 Training Loss: 0.04018242657184601\n",
      "Epoch 16306/30000 Training Loss: 0.04510458558797836\n",
      "Epoch 16307/30000 Training Loss: 0.03561410307884216\n",
      "Epoch 16308/30000 Training Loss: 0.0461951307952404\n",
      "Epoch 16309/30000 Training Loss: 0.048497505486011505\n",
      "Epoch 16310/30000 Training Loss: 0.05009536072611809\n",
      "Epoch 16311/30000 Training Loss: 0.04254285246133804\n",
      "Epoch 16312/30000 Training Loss: 0.05085190385580063\n",
      "Epoch 16313/30000 Training Loss: 0.04735441878437996\n",
      "Epoch 16314/30000 Training Loss: 0.046019136905670166\n",
      "Epoch 16315/30000 Training Loss: 0.04260564595460892\n",
      "Epoch 16316/30000 Training Loss: 0.03944884240627289\n",
      "Epoch 16317/30000 Training Loss: 0.04164484515786171\n",
      "Epoch 16318/30000 Training Loss: 0.03531676158308983\n",
      "Epoch 16319/30000 Training Loss: 0.03988577798008919\n",
      "Epoch 16320/30000 Training Loss: 0.04886109381914139\n",
      "Epoch 16321/30000 Training Loss: 0.04098645597696304\n",
      "Epoch 16322/30000 Training Loss: 0.03930704668164253\n",
      "Epoch 16323/30000 Training Loss: 0.0500621423125267\n",
      "Epoch 16324/30000 Training Loss: 0.03821975737810135\n",
      "Epoch 16325/30000 Training Loss: 0.04508001357316971\n",
      "Epoch 16326/30000 Training Loss: 0.042916253209114075\n",
      "Epoch 16327/30000 Training Loss: 0.04531276971101761\n",
      "Epoch 16328/30000 Training Loss: 0.04506043717265129\n",
      "Epoch 16329/30000 Training Loss: 0.03367442637681961\n",
      "Epoch 16330/30000 Training Loss: 0.040574464946985245\n",
      "Epoch 16331/30000 Training Loss: 0.039385490119457245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16332/30000 Training Loss: 0.050202250480651855\n",
      "Epoch 16333/30000 Training Loss: 0.05181074142456055\n",
      "Epoch 16334/30000 Training Loss: 0.04359985515475273\n",
      "Epoch 16335/30000 Training Loss: 0.05111963301897049\n",
      "Epoch 16336/30000 Training Loss: 0.05123286321759224\n",
      "Epoch 16337/30000 Training Loss: 0.04453831911087036\n",
      "Epoch 16338/30000 Training Loss: 0.04556815326213837\n",
      "Epoch 16339/30000 Training Loss: 0.04334668070077896\n",
      "Epoch 16340/30000 Training Loss: 0.04259605333209038\n",
      "Epoch 16341/30000 Training Loss: 0.04422810673713684\n",
      "Epoch 16342/30000 Training Loss: 0.04483496770262718\n",
      "Epoch 16343/30000 Training Loss: 0.047569483518600464\n",
      "Epoch 16344/30000 Training Loss: 0.042338937520980835\n",
      "Epoch 16345/30000 Training Loss: 0.04422629624605179\n",
      "Epoch 16346/30000 Training Loss: 0.041050925850868225\n",
      "Epoch 16347/30000 Training Loss: 0.050301987677812576\n",
      "Epoch 16348/30000 Training Loss: 0.046274490654468536\n",
      "Epoch 16349/30000 Training Loss: 0.04295151308178902\n",
      "Epoch 16350/30000 Training Loss: 0.049599818885326385\n",
      "Epoch 16350/30000 Validation Loss: 0.042515359818935394\n",
      "Epoch 16351/30000 Training Loss: 0.045036960393190384\n",
      "Epoch 16352/30000 Training Loss: 0.04216451197862625\n",
      "Epoch 16353/30000 Training Loss: 0.04336114972829819\n",
      "Epoch 16354/30000 Training Loss: 0.05501832440495491\n",
      "Epoch 16355/30000 Training Loss: 0.042097557336091995\n",
      "Epoch 16356/30000 Training Loss: 0.04104308411478996\n",
      "Epoch 16357/30000 Training Loss: 0.04293132945895195\n",
      "Epoch 16358/30000 Training Loss: 0.03579890727996826\n",
      "Epoch 16359/30000 Training Loss: 0.03978953883051872\n",
      "Epoch 16360/30000 Training Loss: 0.050266362726688385\n",
      "Epoch 16361/30000 Training Loss: 0.04020301625132561\n",
      "Epoch 16362/30000 Training Loss: 0.05063922330737114\n",
      "Epoch 16363/30000 Training Loss: 0.03362401947379112\n",
      "Epoch 16364/30000 Training Loss: 0.04905431345105171\n",
      "Epoch 16365/30000 Training Loss: 0.03856239467859268\n",
      "Epoch 16366/30000 Training Loss: 0.04578905925154686\n",
      "Epoch 16367/30000 Training Loss: 0.051107008010149\n",
      "Epoch 16368/30000 Training Loss: 0.04640931636095047\n",
      "Epoch 16369/30000 Training Loss: 0.043005459010601044\n",
      "Epoch 16370/30000 Training Loss: 0.04155174642801285\n",
      "Epoch 16371/30000 Training Loss: 0.039418645203113556\n",
      "Epoch 16372/30000 Training Loss: 0.03847277909517288\n",
      "Epoch 16373/30000 Training Loss: 0.052695177495479584\n",
      "Epoch 16374/30000 Training Loss: 0.032283149659633636\n",
      "Epoch 16375/30000 Training Loss: 0.03801126033067703\n",
      "Epoch 16376/30000 Training Loss: 0.05060352012515068\n",
      "Epoch 16377/30000 Training Loss: 0.04003364220261574\n",
      "Epoch 16378/30000 Training Loss: 0.047631531953811646\n",
      "Epoch 16379/30000 Training Loss: 0.04904353991150856\n",
      "Epoch 16380/30000 Training Loss: 0.043976157903671265\n",
      "Epoch 16381/30000 Training Loss: 0.03613831847906113\n",
      "Epoch 16382/30000 Training Loss: 0.04163854941725731\n",
      "Epoch 16383/30000 Training Loss: 0.04725112393498421\n",
      "Epoch 16384/30000 Training Loss: 0.04245234280824661\n",
      "Epoch 16385/30000 Training Loss: 0.0437653474509716\n",
      "Epoch 16386/30000 Training Loss: 0.04068397730588913\n",
      "Epoch 16387/30000 Training Loss: 0.05351458862423897\n",
      "Epoch 16388/30000 Training Loss: 0.04263081029057503\n",
      "Epoch 16389/30000 Training Loss: 0.04016648605465889\n",
      "Epoch 16390/30000 Training Loss: 0.04246995598077774\n",
      "Epoch 16391/30000 Training Loss: 0.04243025928735733\n",
      "Epoch 16392/30000 Training Loss: 0.04485984519124031\n",
      "Epoch 16393/30000 Training Loss: 0.034484587609767914\n",
      "Epoch 16394/30000 Training Loss: 0.04309966415166855\n",
      "Epoch 16395/30000 Training Loss: 0.04578404501080513\n",
      "Epoch 16396/30000 Training Loss: 0.037599582225084305\n",
      "Epoch 16397/30000 Training Loss: 0.04450717568397522\n",
      "Epoch 16398/30000 Training Loss: 0.04330446571111679\n",
      "Epoch 16399/30000 Training Loss: 0.04174448177218437\n",
      "Epoch 16400/30000 Training Loss: 0.03894153609871864\n",
      "Epoch 16400/30000 Validation Loss: 0.049526747316122055\n",
      "Epoch 16401/30000 Training Loss: 0.042509086430072784\n",
      "Epoch 16402/30000 Training Loss: 0.05044850707054138\n",
      "Epoch 16403/30000 Training Loss: 0.05133630707859993\n",
      "Epoch 16404/30000 Training Loss: 0.046818677335977554\n",
      "Epoch 16405/30000 Training Loss: 0.03915627673268318\n",
      "Epoch 16406/30000 Training Loss: 0.03693690150976181\n",
      "Epoch 16407/30000 Training Loss: 0.04261165112257004\n",
      "Epoch 16408/30000 Training Loss: 0.04278161749243736\n",
      "Epoch 16409/30000 Training Loss: 0.04324931651353836\n",
      "Epoch 16410/30000 Training Loss: 0.045839276164770126\n",
      "Epoch 16411/30000 Training Loss: 0.04172087833285332\n",
      "Epoch 16412/30000 Training Loss: 0.038118503987789154\n",
      "Epoch 16413/30000 Training Loss: 0.056116215884685516\n",
      "Epoch 16414/30000 Training Loss: 0.047595731914043427\n",
      "Epoch 16415/30000 Training Loss: 0.039464764297008514\n",
      "Epoch 16416/30000 Training Loss: 0.05491732433438301\n",
      "Epoch 16417/30000 Training Loss: 0.043733879923820496\n",
      "Epoch 16418/30000 Training Loss: 0.03685816377401352\n",
      "Epoch 16419/30000 Training Loss: 0.053514428436756134\n",
      "Epoch 16420/30000 Training Loss: 0.03779659420251846\n",
      "Epoch 16421/30000 Training Loss: 0.04597914591431618\n",
      "Epoch 16422/30000 Training Loss: 0.04990929365158081\n",
      "Epoch 16423/30000 Training Loss: 0.03781143203377724\n",
      "Epoch 16424/30000 Training Loss: 0.035889703780412674\n",
      "Epoch 16425/30000 Training Loss: 0.04456670209765434\n",
      "Epoch 16426/30000 Training Loss: 0.04502937197685242\n",
      "Epoch 16427/30000 Training Loss: 0.03769631311297417\n",
      "Epoch 16428/30000 Training Loss: 0.04207885265350342\n",
      "Epoch 16429/30000 Training Loss: 0.04276382923126221\n",
      "Epoch 16430/30000 Training Loss: 0.04374859109520912\n",
      "Epoch 16431/30000 Training Loss: 0.04728054255247116\n",
      "Epoch 16432/30000 Training Loss: 0.04113353416323662\n",
      "Epoch 16433/30000 Training Loss: 0.04332602769136429\n",
      "Epoch 16434/30000 Training Loss: 0.042020879685878754\n",
      "Epoch 16435/30000 Training Loss: 0.04212517291307449\n",
      "Epoch 16436/30000 Training Loss: 0.0388956181704998\n",
      "Epoch 16437/30000 Training Loss: 0.040668826550245285\n",
      "Epoch 16438/30000 Training Loss: 0.04497116804122925\n",
      "Epoch 16439/30000 Training Loss: 0.04263797402381897\n",
      "Epoch 16440/30000 Training Loss: 0.03951122239232063\n",
      "Epoch 16441/30000 Training Loss: 0.050916094332933426\n",
      "Epoch 16442/30000 Training Loss: 0.041173357516527176\n",
      "Epoch 16443/30000 Training Loss: 0.042064011096954346\n",
      "Epoch 16444/30000 Training Loss: 0.03987482190132141\n",
      "Epoch 16445/30000 Training Loss: 0.04701470583677292\n",
      "Epoch 16446/30000 Training Loss: 0.039517246186733246\n",
      "Epoch 16447/30000 Training Loss: 0.04894612357020378\n",
      "Epoch 16448/30000 Training Loss: 0.035419005900621414\n",
      "Epoch 16449/30000 Training Loss: 0.04667087644338608\n",
      "Epoch 16450/30000 Training Loss: 0.043876342475414276\n",
      "Epoch 16450/30000 Validation Loss: 0.03836040943861008\n",
      "Epoch 16451/30000 Training Loss: 0.03979383036494255\n",
      "Epoch 16452/30000 Training Loss: 0.04346195608377457\n",
      "Epoch 16453/30000 Training Loss: 0.04501551389694214\n",
      "Epoch 16454/30000 Training Loss: 0.04482131451368332\n",
      "Epoch 16455/30000 Training Loss: 0.038242749869823456\n",
      "Epoch 16456/30000 Training Loss: 0.041350312530994415\n",
      "Epoch 16457/30000 Training Loss: 0.040257953107357025\n",
      "Epoch 16458/30000 Training Loss: 0.0635584369301796\n",
      "Epoch 16459/30000 Training Loss: 0.04819058999419212\n",
      "Epoch 16460/30000 Training Loss: 0.042749255895614624\n",
      "Epoch 16461/30000 Training Loss: 0.04232991486787796\n",
      "Epoch 16462/30000 Training Loss: 0.04856126382946968\n",
      "Epoch 16463/30000 Training Loss: 0.056857723742723465\n",
      "Epoch 16464/30000 Training Loss: 0.046233437955379486\n",
      "Epoch 16465/30000 Training Loss: 0.03959989547729492\n",
      "Epoch 16466/30000 Training Loss: 0.03872523829340935\n",
      "Epoch 16467/30000 Training Loss: 0.04458925873041153\n",
      "Epoch 16468/30000 Training Loss: 0.04360954090952873\n",
      "Epoch 16469/30000 Training Loss: 0.04637948423624039\n",
      "Epoch 16470/30000 Training Loss: 0.04344473034143448\n",
      "Epoch 16471/30000 Training Loss: 0.036793895065784454\n",
      "Epoch 16472/30000 Training Loss: 0.03939223662018776\n",
      "Epoch 16473/30000 Training Loss: 0.05553596094250679\n",
      "Epoch 16474/30000 Training Loss: 0.04069673269987106\n",
      "Epoch 16475/30000 Training Loss: 0.042057670652866364\n",
      "Epoch 16476/30000 Training Loss: 0.04744001477956772\n",
      "Epoch 16477/30000 Training Loss: 0.040154051035642624\n",
      "Epoch 16478/30000 Training Loss: 0.04458155110478401\n",
      "Epoch 16479/30000 Training Loss: 0.03893626481294632\n",
      "Epoch 16480/30000 Training Loss: 0.04438044875860214\n",
      "Epoch 16481/30000 Training Loss: 0.04453899338841438\n",
      "Epoch 16482/30000 Training Loss: 0.044591594487428665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16483/30000 Training Loss: 0.04334621503949165\n",
      "Epoch 16484/30000 Training Loss: 0.040863458067178726\n",
      "Epoch 16485/30000 Training Loss: 0.04854704812169075\n",
      "Epoch 16486/30000 Training Loss: 0.04577194154262543\n",
      "Epoch 16487/30000 Training Loss: 0.04132870212197304\n",
      "Epoch 16488/30000 Training Loss: 0.04176987707614899\n",
      "Epoch 16489/30000 Training Loss: 0.03669997304677963\n",
      "Epoch 16490/30000 Training Loss: 0.03967372328042984\n",
      "Epoch 16491/30000 Training Loss: 0.041813794523477554\n",
      "Epoch 16492/30000 Training Loss: 0.03758203983306885\n",
      "Epoch 16493/30000 Training Loss: 0.04182921722531319\n",
      "Epoch 16494/30000 Training Loss: 0.04532893747091293\n",
      "Epoch 16495/30000 Training Loss: 0.04025840759277344\n",
      "Epoch 16496/30000 Training Loss: 0.030530566349625587\n",
      "Epoch 16497/30000 Training Loss: 0.04443414881825447\n",
      "Epoch 16498/30000 Training Loss: 0.0396844781935215\n",
      "Epoch 16499/30000 Training Loss: 0.043051935732364655\n",
      "Epoch 16500/30000 Training Loss: 0.041686661541461945\n",
      "Epoch 16500/30000 Validation Loss: 0.04769245907664299\n",
      "Epoch 16501/30000 Training Loss: 0.04289623722434044\n",
      "Epoch 16502/30000 Training Loss: 0.04433382302522659\n",
      "Epoch 16503/30000 Training Loss: 0.046850062906742096\n",
      "Epoch 16504/30000 Training Loss: 0.045148588716983795\n",
      "Epoch 16505/30000 Training Loss: 0.044558655470609665\n",
      "Epoch 16506/30000 Training Loss: 0.051483362913131714\n",
      "Epoch 16507/30000 Training Loss: 0.04252566024661064\n",
      "Epoch 16508/30000 Training Loss: 0.041855715215206146\n",
      "Epoch 16509/30000 Training Loss: 0.035419683903455734\n",
      "Epoch 16510/30000 Training Loss: 0.034995369613170624\n",
      "Epoch 16511/30000 Training Loss: 0.03796429932117462\n",
      "Epoch 16512/30000 Training Loss: 0.0486515574157238\n",
      "Epoch 16513/30000 Training Loss: 0.040625326335430145\n",
      "Epoch 16514/30000 Training Loss: 0.0393950417637825\n",
      "Epoch 16515/30000 Training Loss: 0.044964127242565155\n",
      "Epoch 16516/30000 Training Loss: 0.04704933986067772\n",
      "Epoch 16517/30000 Training Loss: 0.04916560649871826\n",
      "Epoch 16518/30000 Training Loss: 0.040115661919116974\n",
      "Epoch 16519/30000 Training Loss: 0.0436580516397953\n",
      "Epoch 16520/30000 Training Loss: 0.046584926545619965\n",
      "Epoch 16521/30000 Training Loss: 0.04176905006170273\n",
      "Epoch 16522/30000 Training Loss: 0.042155779898166656\n",
      "Epoch 16523/30000 Training Loss: 0.04378754273056984\n",
      "Epoch 16524/30000 Training Loss: 0.045192427933216095\n",
      "Epoch 16525/30000 Training Loss: 0.04365115240216255\n",
      "Epoch 16526/30000 Training Loss: 0.04523274302482605\n",
      "Epoch 16527/30000 Training Loss: 0.04560261592268944\n",
      "Epoch 16528/30000 Training Loss: 0.041856348514556885\n",
      "Epoch 16529/30000 Training Loss: 0.034885503351688385\n",
      "Epoch 16530/30000 Training Loss: 0.03720858320593834\n",
      "Epoch 16531/30000 Training Loss: 0.047120463103055954\n",
      "Epoch 16532/30000 Training Loss: 0.0398031547665596\n",
      "Epoch 16533/30000 Training Loss: 0.046488068997859955\n",
      "Epoch 16534/30000 Training Loss: 0.044496726244688034\n",
      "Epoch 16535/30000 Training Loss: 0.03533697873353958\n",
      "Epoch 16536/30000 Training Loss: 0.05034463480114937\n",
      "Epoch 16537/30000 Training Loss: 0.04831259697675705\n",
      "Epoch 16538/30000 Training Loss: 0.04158487543463707\n",
      "Epoch 16539/30000 Training Loss: 0.04783877357840538\n",
      "Epoch 16540/30000 Training Loss: 0.05228930711746216\n",
      "Epoch 16541/30000 Training Loss: 0.038166798651218414\n",
      "Epoch 16542/30000 Training Loss: 0.04595259949564934\n",
      "Epoch 16543/30000 Training Loss: 0.04277347773313522\n",
      "Epoch 16544/30000 Training Loss: 0.05266733095049858\n",
      "Epoch 16545/30000 Training Loss: 0.035868920385837555\n",
      "Epoch 16546/30000 Training Loss: 0.03254184126853943\n",
      "Epoch 16547/30000 Training Loss: 0.042226873338222504\n",
      "Epoch 16548/30000 Training Loss: 0.04432397335767746\n",
      "Epoch 16549/30000 Training Loss: 0.04386443272233009\n",
      "Epoch 16550/30000 Training Loss: 0.04488704353570938\n",
      "Epoch 16550/30000 Validation Loss: 0.0363670289516449\n",
      "Epoch 16551/30000 Training Loss: 0.03735319525003433\n",
      "Epoch 16552/30000 Training Loss: 0.0459841713309288\n",
      "Epoch 16553/30000 Training Loss: 0.04429230839014053\n",
      "Epoch 16554/30000 Training Loss: 0.047808464616537094\n",
      "Epoch 16555/30000 Training Loss: 0.045521583408117294\n",
      "Epoch 16556/30000 Training Loss: 0.039903320372104645\n",
      "Epoch 16557/30000 Training Loss: 0.038642335683107376\n",
      "Epoch 16558/30000 Training Loss: 0.04147379845380783\n",
      "Epoch 16559/30000 Training Loss: 0.044922877103090286\n",
      "Epoch 16560/30000 Training Loss: 0.03844517096877098\n",
      "Epoch 16561/30000 Training Loss: 0.044572826474905014\n",
      "Epoch 16562/30000 Training Loss: 0.041118819266557693\n",
      "Epoch 16563/30000 Training Loss: 0.037375252693891525\n",
      "Epoch 16564/30000 Training Loss: 0.04590630158782005\n",
      "Epoch 16565/30000 Training Loss: 0.04079333692789078\n",
      "Epoch 16566/30000 Training Loss: 0.045761264860630035\n",
      "Epoch 16567/30000 Training Loss: 0.04157337173819542\n",
      "Epoch 16568/30000 Training Loss: 0.04807877540588379\n",
      "Epoch 16569/30000 Training Loss: 0.04355480894446373\n",
      "Epoch 16570/30000 Training Loss: 0.05162450671195984\n",
      "Epoch 16571/30000 Training Loss: 0.04429289326071739\n",
      "Epoch 16572/30000 Training Loss: 0.0451684296131134\n",
      "Epoch 16573/30000 Training Loss: 0.03797680884599686\n",
      "Epoch 16574/30000 Training Loss: 0.04563205689191818\n",
      "Epoch 16575/30000 Training Loss: 0.04410360008478165\n",
      "Epoch 16576/30000 Training Loss: 0.03992532938718796\n",
      "Epoch 16577/30000 Training Loss: 0.04033135250210762\n",
      "Epoch 16578/30000 Training Loss: 0.04749745875597\n",
      "Epoch 16579/30000 Training Loss: 0.04389147087931633\n",
      "Epoch 16580/30000 Training Loss: 0.04432641714811325\n",
      "Epoch 16581/30000 Training Loss: 0.04943632706999779\n",
      "Epoch 16582/30000 Training Loss: 0.04755435138940811\n",
      "Epoch 16583/30000 Training Loss: 0.04892987012863159\n",
      "Epoch 16584/30000 Training Loss: 0.03964812681078911\n",
      "Epoch 16585/30000 Training Loss: 0.03727387264370918\n",
      "Epoch 16586/30000 Training Loss: 0.056052714586257935\n",
      "Epoch 16587/30000 Training Loss: 0.04874565079808235\n",
      "Epoch 16588/30000 Training Loss: 0.039551448076963425\n",
      "Epoch 16589/30000 Training Loss: 0.04183554649353027\n",
      "Epoch 16590/30000 Training Loss: 0.03934141993522644\n",
      "Epoch 16591/30000 Training Loss: 0.04291742295026779\n",
      "Epoch 16592/30000 Training Loss: 0.03880695253610611\n",
      "Epoch 16593/30000 Training Loss: 0.04072561115026474\n",
      "Epoch 16594/30000 Training Loss: 0.042267560958862305\n",
      "Epoch 16595/30000 Training Loss: 0.03600085899233818\n",
      "Epoch 16596/30000 Training Loss: 0.04369743913412094\n",
      "Epoch 16597/30000 Training Loss: 0.03927763178944588\n",
      "Epoch 16598/30000 Training Loss: 0.03968491405248642\n",
      "Epoch 16599/30000 Training Loss: 0.04277770221233368\n",
      "Epoch 16600/30000 Training Loss: 0.041680946946144104\n",
      "Epoch 16600/30000 Validation Loss: 0.04011017456650734\n",
      "Epoch 16601/30000 Training Loss: 0.04417631775140762\n",
      "Epoch 16602/30000 Training Loss: 0.03971915692090988\n",
      "Epoch 16603/30000 Training Loss: 0.04229970648884773\n",
      "Epoch 16604/30000 Training Loss: 0.03908231854438782\n",
      "Epoch 16605/30000 Training Loss: 0.04086560010910034\n",
      "Epoch 16606/30000 Training Loss: 0.04826178401708603\n",
      "Epoch 16607/30000 Training Loss: 0.03822348266839981\n",
      "Epoch 16608/30000 Training Loss: 0.04503142088651657\n",
      "Epoch 16609/30000 Training Loss: 0.05085942894220352\n",
      "Epoch 16610/30000 Training Loss: 0.04642796516418457\n",
      "Epoch 16611/30000 Training Loss: 0.04312712699174881\n",
      "Epoch 16612/30000 Training Loss: 0.04851007089018822\n",
      "Epoch 16613/30000 Training Loss: 0.045510854572057724\n",
      "Epoch 16614/30000 Training Loss: 0.03504103049635887\n",
      "Epoch 16615/30000 Training Loss: 0.0380268394947052\n",
      "Epoch 16616/30000 Training Loss: 0.04423386603593826\n",
      "Epoch 16617/30000 Training Loss: 0.041161052882671356\n",
      "Epoch 16618/30000 Training Loss: 0.04144200682640076\n",
      "Epoch 16619/30000 Training Loss: 0.04772884026169777\n",
      "Epoch 16620/30000 Training Loss: 0.043089158833026886\n",
      "Epoch 16621/30000 Training Loss: 0.043108828365802765\n",
      "Epoch 16622/30000 Training Loss: 0.04031830281019211\n",
      "Epoch 16623/30000 Training Loss: 0.044048894196748734\n",
      "Epoch 16624/30000 Training Loss: 0.04287448525428772\n",
      "Epoch 16625/30000 Training Loss: 0.043691571801900864\n",
      "Epoch 16626/30000 Training Loss: 0.04319571703672409\n",
      "Epoch 16627/30000 Training Loss: 0.04360572621226311\n",
      "Epoch 16628/30000 Training Loss: 0.045151181519031525\n",
      "Epoch 16629/30000 Training Loss: 0.035538699477910995\n",
      "Epoch 16630/30000 Training Loss: 0.04475554823875427\n",
      "Epoch 16631/30000 Training Loss: 0.043289538472890854\n",
      "Epoch 16632/30000 Training Loss: 0.0487184152007103\n",
      "Epoch 16633/30000 Training Loss: 0.04083740711212158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16634/30000 Training Loss: 0.04221171885728836\n",
      "Epoch 16635/30000 Training Loss: 0.040880877524614334\n",
      "Epoch 16636/30000 Training Loss: 0.05097310617566109\n",
      "Epoch 16637/30000 Training Loss: 0.04495682194828987\n",
      "Epoch 16638/30000 Training Loss: 0.03888378664851189\n",
      "Epoch 16639/30000 Training Loss: 0.0455322191119194\n",
      "Epoch 16640/30000 Training Loss: 0.040745146572589874\n",
      "Epoch 16641/30000 Training Loss: 0.04740903154015541\n",
      "Epoch 16642/30000 Training Loss: 0.037549939006567\n",
      "Epoch 16643/30000 Training Loss: 0.03149982541799545\n",
      "Epoch 16644/30000 Training Loss: 0.048932671546936035\n",
      "Epoch 16645/30000 Training Loss: 0.037888992577791214\n",
      "Epoch 16646/30000 Training Loss: 0.04893689602613449\n",
      "Epoch 16647/30000 Training Loss: 0.03936537355184555\n",
      "Epoch 16648/30000 Training Loss: 0.04565634950995445\n",
      "Epoch 16649/30000 Training Loss: 0.04053308814764023\n",
      "Epoch 16650/30000 Training Loss: 0.04621013253927231\n",
      "Epoch 16650/30000 Validation Loss: 0.03949425742030144\n",
      "Epoch 16651/30000 Training Loss: 0.03802318871021271\n",
      "Epoch 16652/30000 Training Loss: 0.041261907666921616\n",
      "Epoch 16653/30000 Training Loss: 0.043485045433044434\n",
      "Epoch 16654/30000 Training Loss: 0.051047731190919876\n",
      "Epoch 16655/30000 Training Loss: 0.044215139001607895\n",
      "Epoch 16656/30000 Training Loss: 0.03870503231883049\n",
      "Epoch 16657/30000 Training Loss: 0.04049161821603775\n",
      "Epoch 16658/30000 Training Loss: 0.03782947361469269\n",
      "Epoch 16659/30000 Training Loss: 0.045845624059438705\n",
      "Epoch 16660/30000 Training Loss: 0.035070303827524185\n",
      "Epoch 16661/30000 Training Loss: 0.04578473046422005\n",
      "Epoch 16662/30000 Training Loss: 0.04595332592725754\n",
      "Epoch 16663/30000 Training Loss: 0.042859889566898346\n",
      "Epoch 16664/30000 Training Loss: 0.03763284534215927\n",
      "Epoch 16665/30000 Training Loss: 0.03849450498819351\n",
      "Epoch 16666/30000 Training Loss: 0.03659677878022194\n",
      "Epoch 16667/30000 Training Loss: 0.04914487525820732\n",
      "Epoch 16668/30000 Training Loss: 0.04510269686579704\n",
      "Epoch 16669/30000 Training Loss: 0.043945860117673874\n",
      "Epoch 16670/30000 Training Loss: 0.0444255955517292\n",
      "Epoch 16671/30000 Training Loss: 0.04589518904685974\n",
      "Epoch 16672/30000 Training Loss: 0.038673486560583115\n",
      "Epoch 16673/30000 Training Loss: 0.03604375571012497\n",
      "Epoch 16674/30000 Training Loss: 0.04858055338263512\n",
      "Epoch 16675/30000 Training Loss: 0.04056081175804138\n",
      "Epoch 16676/30000 Training Loss: 0.038913894444704056\n",
      "Epoch 16677/30000 Training Loss: 0.04226596653461456\n",
      "Epoch 16678/30000 Training Loss: 0.03895360976457596\n",
      "Epoch 16679/30000 Training Loss: 0.04271070286631584\n",
      "Epoch 16680/30000 Training Loss: 0.046487197279930115\n",
      "Epoch 16681/30000 Training Loss: 0.06048492714762688\n",
      "Epoch 16682/30000 Training Loss: 0.04469417780637741\n",
      "Epoch 16683/30000 Training Loss: 0.04505573958158493\n",
      "Epoch 16684/30000 Training Loss: 0.04041565954685211\n",
      "Epoch 16685/30000 Training Loss: 0.04457239806652069\n",
      "Epoch 16686/30000 Training Loss: 0.042038753628730774\n",
      "Epoch 16687/30000 Training Loss: 0.04538460820913315\n",
      "Epoch 16688/30000 Training Loss: 0.04067350924015045\n",
      "Epoch 16689/30000 Training Loss: 0.04008230194449425\n",
      "Epoch 16690/30000 Training Loss: 0.042448095977306366\n",
      "Epoch 16691/30000 Training Loss: 0.03479015827178955\n",
      "Epoch 16692/30000 Training Loss: 0.04454600438475609\n",
      "Epoch 16693/30000 Training Loss: 0.038340963423252106\n",
      "Epoch 16694/30000 Training Loss: 0.03634914755821228\n",
      "Epoch 16695/30000 Training Loss: 0.04073723405599594\n",
      "Epoch 16696/30000 Training Loss: 0.04142466560006142\n",
      "Epoch 16697/30000 Training Loss: 0.044702958315610886\n",
      "Epoch 16698/30000 Training Loss: 0.041935719549655914\n",
      "Epoch 16699/30000 Training Loss: 0.04450204223394394\n",
      "Epoch 16700/30000 Training Loss: 0.04612736776471138\n",
      "Epoch 16700/30000 Validation Loss: 0.037182070314884186\n",
      "Epoch 16701/30000 Training Loss: 0.0354946032166481\n",
      "Epoch 16702/30000 Training Loss: 0.03503895550966263\n",
      "Epoch 16703/30000 Training Loss: 0.05172262340784073\n",
      "Epoch 16704/30000 Training Loss: 0.042093425989151\n",
      "Epoch 16705/30000 Training Loss: 0.04815644025802612\n",
      "Epoch 16706/30000 Training Loss: 0.03473251312971115\n",
      "Epoch 16707/30000 Training Loss: 0.03646549582481384\n",
      "Epoch 16708/30000 Training Loss: 0.037156809121370316\n",
      "Epoch 16709/30000 Training Loss: 0.04183436185121536\n",
      "Epoch 16710/30000 Training Loss: 0.045992299914360046\n",
      "Epoch 16711/30000 Training Loss: 0.03459937497973442\n",
      "Epoch 16712/30000 Training Loss: 0.04585075378417969\n",
      "Epoch 16713/30000 Training Loss: 0.04703858867287636\n",
      "Epoch 16714/30000 Training Loss: 0.042141687124967575\n",
      "Epoch 16715/30000 Training Loss: 0.03967874124646187\n",
      "Epoch 16716/30000 Training Loss: 0.03718724846839905\n",
      "Epoch 16717/30000 Training Loss: 0.039836637675762177\n",
      "Epoch 16718/30000 Training Loss: 0.03658214956521988\n",
      "Epoch 16719/30000 Training Loss: 0.0377231240272522\n",
      "Epoch 16720/30000 Training Loss: 0.0454331710934639\n",
      "Epoch 16721/30000 Training Loss: 0.04131519794464111\n",
      "Epoch 16722/30000 Training Loss: 0.034796103835105896\n",
      "Epoch 16723/30000 Training Loss: 0.04806030914187431\n",
      "Epoch 16724/30000 Training Loss: 0.03868936374783516\n",
      "Epoch 16725/30000 Training Loss: 0.044937677681446075\n",
      "Epoch 16726/30000 Training Loss: 0.04107014089822769\n",
      "Epoch 16727/30000 Training Loss: 0.04781477525830269\n",
      "Epoch 16728/30000 Training Loss: 0.037349455058574677\n",
      "Epoch 16729/30000 Training Loss: 0.04247603565454483\n",
      "Epoch 16730/30000 Training Loss: 0.049702778458595276\n",
      "Epoch 16731/30000 Training Loss: 0.05468007177114487\n",
      "Epoch 16732/30000 Training Loss: 0.04598739370703697\n",
      "Epoch 16733/30000 Training Loss: 0.04073648154735565\n",
      "Epoch 16734/30000 Training Loss: 0.04215376824140549\n",
      "Epoch 16735/30000 Training Loss: 0.043978847563266754\n",
      "Epoch 16736/30000 Training Loss: 0.04563181474804878\n",
      "Epoch 16737/30000 Training Loss: 0.036489080637693405\n",
      "Epoch 16738/30000 Training Loss: 0.044203855097293854\n",
      "Epoch 16739/30000 Training Loss: 0.0431707538664341\n",
      "Epoch 16740/30000 Training Loss: 0.03944971412420273\n",
      "Epoch 16741/30000 Training Loss: 0.04236989468336105\n",
      "Epoch 16742/30000 Training Loss: 0.04492456465959549\n",
      "Epoch 16743/30000 Training Loss: 0.0421534962952137\n",
      "Epoch 16744/30000 Training Loss: 0.03828630596399307\n",
      "Epoch 16745/30000 Training Loss: 0.05168970674276352\n",
      "Epoch 16746/30000 Training Loss: 0.04228834807872772\n",
      "Epoch 16747/30000 Training Loss: 0.04550139978528023\n",
      "Epoch 16748/30000 Training Loss: 0.045119646936655045\n",
      "Epoch 16749/30000 Training Loss: 0.04608769714832306\n",
      "Epoch 16750/30000 Training Loss: 0.041680242866277695\n",
      "Epoch 16750/30000 Validation Loss: 0.04406485706567764\n",
      "Epoch 16751/30000 Training Loss: 0.03919459134340286\n",
      "Epoch 16752/30000 Training Loss: 0.043975893408060074\n",
      "Epoch 16753/30000 Training Loss: 0.0442141629755497\n",
      "Epoch 16754/30000 Training Loss: 0.04758346825838089\n",
      "Epoch 16755/30000 Training Loss: 0.03591842204332352\n",
      "Epoch 16756/30000 Training Loss: 0.03915705159306526\n",
      "Epoch 16757/30000 Training Loss: 0.0372726172208786\n",
      "Epoch 16758/30000 Training Loss: 0.046374838799238205\n",
      "Epoch 16759/30000 Training Loss: 0.05169185250997543\n",
      "Epoch 16760/30000 Training Loss: 0.04276512563228607\n",
      "Epoch 16761/30000 Training Loss: 0.04178892821073532\n",
      "Epoch 16762/30000 Training Loss: 0.05078296735882759\n",
      "Epoch 16763/30000 Training Loss: 0.03958364576101303\n",
      "Epoch 16764/30000 Training Loss: 0.03767147660255432\n",
      "Epoch 16765/30000 Training Loss: 0.04358039051294327\n",
      "Epoch 16766/30000 Training Loss: 0.030730942264199257\n",
      "Epoch 16767/30000 Training Loss: 0.04671039432287216\n",
      "Epoch 16768/30000 Training Loss: 0.04513021931052208\n",
      "Epoch 16769/30000 Training Loss: 0.04137810319662094\n",
      "Epoch 16770/30000 Training Loss: 0.04147503152489662\n",
      "Epoch 16771/30000 Training Loss: 0.03945911303162575\n",
      "Epoch 16772/30000 Training Loss: 0.04275088757276535\n",
      "Epoch 16773/30000 Training Loss: 0.050904590636491776\n",
      "Epoch 16774/30000 Training Loss: 0.044068753719329834\n",
      "Epoch 16775/30000 Training Loss: 0.04165782406926155\n",
      "Epoch 16776/30000 Training Loss: 0.04513226076960564\n",
      "Epoch 16777/30000 Training Loss: 0.049689240753650665\n",
      "Epoch 16778/30000 Training Loss: 0.04408637806773186\n",
      "Epoch 16779/30000 Training Loss: 0.04696213826537132\n",
      "Epoch 16780/30000 Training Loss: 0.05670301243662834\n",
      "Epoch 16781/30000 Training Loss: 0.04893931373953819\n",
      "Epoch 16782/30000 Training Loss: 0.04318183287978172\n",
      "Epoch 16783/30000 Training Loss: 0.039801888167858124\n",
      "Epoch 16784/30000 Training Loss: 0.04015156626701355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16785/30000 Training Loss: 0.041537392884492874\n",
      "Epoch 16786/30000 Training Loss: 0.039780836552381516\n",
      "Epoch 16787/30000 Training Loss: 0.03665542975068092\n",
      "Epoch 16788/30000 Training Loss: 0.04428208991885185\n",
      "Epoch 16789/30000 Training Loss: 0.04186255484819412\n",
      "Epoch 16790/30000 Training Loss: 0.04282978177070618\n",
      "Epoch 16791/30000 Training Loss: 0.042094118893146515\n",
      "Epoch 16792/30000 Training Loss: 0.03998595476150513\n",
      "Epoch 16793/30000 Training Loss: 0.04702068120241165\n",
      "Epoch 16794/30000 Training Loss: 0.049344222992658615\n",
      "Epoch 16795/30000 Training Loss: 0.05154221132397652\n",
      "Epoch 16796/30000 Training Loss: 0.046655524522066116\n",
      "Epoch 16797/30000 Training Loss: 0.044215127825737\n",
      "Epoch 16798/30000 Training Loss: 0.042297784239053726\n",
      "Epoch 16799/30000 Training Loss: 0.0435178168118\n",
      "Epoch 16800/30000 Training Loss: 0.03696319833397865\n",
      "Epoch 16800/30000 Validation Loss: 0.043833937495946884\n",
      "Epoch 16801/30000 Training Loss: 0.05021099001169205\n",
      "Epoch 16802/30000 Training Loss: 0.05248309299349785\n",
      "Epoch 16803/30000 Training Loss: 0.044398196041584015\n",
      "Epoch 16804/30000 Training Loss: 0.042801667004823685\n",
      "Epoch 16805/30000 Training Loss: 0.047543853521347046\n",
      "Epoch 16806/30000 Training Loss: 0.042732588946819305\n",
      "Epoch 16807/30000 Training Loss: 0.037370048463344574\n",
      "Epoch 16808/30000 Training Loss: 0.05055881664156914\n",
      "Epoch 16809/30000 Training Loss: 0.03814446181058884\n",
      "Epoch 16810/30000 Training Loss: 0.045013658702373505\n",
      "Epoch 16811/30000 Training Loss: 0.0398942194879055\n",
      "Epoch 16812/30000 Training Loss: 0.04048815742135048\n",
      "Epoch 16813/30000 Training Loss: 0.045301876962184906\n",
      "Epoch 16814/30000 Training Loss: 0.04298294708132744\n",
      "Epoch 16815/30000 Training Loss: 0.04527350887656212\n",
      "Epoch 16816/30000 Training Loss: 0.04145088791847229\n",
      "Epoch 16817/30000 Training Loss: 0.044929444789886475\n",
      "Epoch 16818/30000 Training Loss: 0.03935004770755768\n",
      "Epoch 16819/30000 Training Loss: 0.03902309760451317\n",
      "Epoch 16820/30000 Training Loss: 0.04220400005578995\n",
      "Epoch 16821/30000 Training Loss: 0.04073125123977661\n",
      "Epoch 16822/30000 Training Loss: 0.038939643651247025\n",
      "Epoch 16823/30000 Training Loss: 0.040045566856861115\n",
      "Epoch 16824/30000 Training Loss: 0.04820443317294121\n",
      "Epoch 16825/30000 Training Loss: 0.04427951201796532\n",
      "Epoch 16826/30000 Training Loss: 0.038340672850608826\n",
      "Epoch 16827/30000 Training Loss: 0.04302386939525604\n",
      "Epoch 16828/30000 Training Loss: 0.04033650457859039\n",
      "Epoch 16829/30000 Training Loss: 0.03847935050725937\n",
      "Epoch 16830/30000 Training Loss: 0.0401727557182312\n",
      "Epoch 16831/30000 Training Loss: 0.048482637852430344\n",
      "Epoch 16832/30000 Training Loss: 0.045344214886426926\n",
      "Epoch 16833/30000 Training Loss: 0.041090577840805054\n",
      "Epoch 16834/30000 Training Loss: 0.03766052797436714\n",
      "Epoch 16835/30000 Training Loss: 0.035748496651649475\n",
      "Epoch 16836/30000 Training Loss: 0.04412127286195755\n",
      "Epoch 16837/30000 Training Loss: 0.04168283939361572\n",
      "Epoch 16838/30000 Training Loss: 0.0428629145026207\n",
      "Epoch 16839/30000 Training Loss: 0.038540326058864594\n",
      "Epoch 16840/30000 Training Loss: 0.037416473031044006\n",
      "Epoch 16841/30000 Training Loss: 0.03552354499697685\n",
      "Epoch 16842/30000 Training Loss: 0.03956497460603714\n",
      "Epoch 16843/30000 Training Loss: 0.04418691247701645\n",
      "Epoch 16844/30000 Training Loss: 0.04636665806174278\n",
      "Epoch 16845/30000 Training Loss: 0.042225729674100876\n",
      "Epoch 16846/30000 Training Loss: 0.04150928184390068\n",
      "Epoch 16847/30000 Training Loss: 0.04151608422398567\n",
      "Epoch 16848/30000 Training Loss: 0.04712449759244919\n",
      "Epoch 16849/30000 Training Loss: 0.042651914060115814\n",
      "Epoch 16850/30000 Training Loss: 0.04833049327135086\n",
      "Epoch 16850/30000 Validation Loss: 0.042141251266002655\n",
      "Epoch 16851/30000 Training Loss: 0.04547572880983353\n",
      "Epoch 16852/30000 Training Loss: 0.03959403187036514\n",
      "Epoch 16853/30000 Training Loss: 0.04235563427209854\n",
      "Epoch 16854/30000 Training Loss: 0.04156326502561569\n",
      "Epoch 16855/30000 Training Loss: 0.04148202762007713\n",
      "Epoch 16856/30000 Training Loss: 0.04461943358182907\n",
      "Epoch 16857/30000 Training Loss: 0.038201577961444855\n",
      "Epoch 16858/30000 Training Loss: 0.046493083238601685\n",
      "Epoch 16859/30000 Training Loss: 0.03802698850631714\n",
      "Epoch 16860/30000 Training Loss: 0.041940297931432724\n",
      "Epoch 16861/30000 Training Loss: 0.04039810225367546\n",
      "Epoch 16862/30000 Training Loss: 0.04518238455057144\n",
      "Epoch 16863/30000 Training Loss: 0.042747557163238525\n",
      "Epoch 16864/30000 Training Loss: 0.04226600378751755\n",
      "Epoch 16865/30000 Training Loss: 0.04408237710595131\n",
      "Epoch 16866/30000 Training Loss: 0.03579915314912796\n",
      "Epoch 16867/30000 Training Loss: 0.05306580662727356\n",
      "Epoch 16868/30000 Training Loss: 0.042755089700222015\n",
      "Epoch 16869/30000 Training Loss: 0.0416254922747612\n",
      "Epoch 16870/30000 Training Loss: 0.03869103640317917\n",
      "Epoch 16871/30000 Training Loss: 0.046253789216279984\n",
      "Epoch 16872/30000 Training Loss: 0.04384361580014229\n",
      "Epoch 16873/30000 Training Loss: 0.042342036962509155\n",
      "Epoch 16874/30000 Training Loss: 0.045956339687108994\n",
      "Epoch 16875/30000 Training Loss: 0.035916805267333984\n",
      "Epoch 16876/30000 Training Loss: 0.038091160356998444\n",
      "Epoch 16877/30000 Training Loss: 0.03772903233766556\n",
      "Epoch 16878/30000 Training Loss: 0.04661845415830612\n",
      "Epoch 16879/30000 Training Loss: 0.04784318059682846\n",
      "Epoch 16880/30000 Training Loss: 0.041572749614715576\n",
      "Epoch 16881/30000 Training Loss: 0.04352252557873726\n",
      "Epoch 16882/30000 Training Loss: 0.03314812853932381\n",
      "Epoch 16883/30000 Training Loss: 0.044780004769563675\n",
      "Epoch 16884/30000 Training Loss: 0.03684883937239647\n",
      "Epoch 16885/30000 Training Loss: 0.04893115162849426\n",
      "Epoch 16886/30000 Training Loss: 0.04615703597664833\n",
      "Epoch 16887/30000 Training Loss: 0.04740350693464279\n",
      "Epoch 16888/30000 Training Loss: 0.03834512084722519\n",
      "Epoch 16889/30000 Training Loss: 0.04750822111964226\n",
      "Epoch 16890/30000 Training Loss: 0.04897848889231682\n",
      "Epoch 16891/30000 Training Loss: 0.03984397277235985\n",
      "Epoch 16892/30000 Training Loss: 0.03985276073217392\n",
      "Epoch 16893/30000 Training Loss: 0.0453731045126915\n",
      "Epoch 16894/30000 Training Loss: 0.0428132601082325\n",
      "Epoch 16895/30000 Training Loss: 0.041318316012620926\n",
      "Epoch 16896/30000 Training Loss: 0.0545639805495739\n",
      "Epoch 16897/30000 Training Loss: 0.043152421712875366\n",
      "Epoch 16898/30000 Training Loss: 0.041513729840517044\n",
      "Epoch 16899/30000 Training Loss: 0.04081614688038826\n",
      "Epoch 16900/30000 Training Loss: 0.04979567974805832\n",
      "Epoch 16900/30000 Validation Loss: 0.041883617639541626\n",
      "Epoch 16901/30000 Training Loss: 0.0476335808634758\n",
      "Epoch 16902/30000 Training Loss: 0.03773413598537445\n",
      "Epoch 16903/30000 Training Loss: 0.039221398532390594\n",
      "Epoch 16904/30000 Training Loss: 0.03520384430885315\n",
      "Epoch 16905/30000 Training Loss: 0.048002272844314575\n",
      "Epoch 16906/30000 Training Loss: 0.04816143959760666\n",
      "Epoch 16907/30000 Training Loss: 0.04606097191572189\n",
      "Epoch 16908/30000 Training Loss: 0.044710464775562286\n",
      "Epoch 16909/30000 Training Loss: 0.04016442224383354\n",
      "Epoch 16910/30000 Training Loss: 0.037645913660526276\n",
      "Epoch 16911/30000 Training Loss: 0.05037249997258186\n",
      "Epoch 16912/30000 Training Loss: 0.03983209282159805\n",
      "Epoch 16913/30000 Training Loss: 0.04376237839460373\n",
      "Epoch 16914/30000 Training Loss: 0.03766253963112831\n",
      "Epoch 16915/30000 Training Loss: 0.038656432181596756\n",
      "Epoch 16916/30000 Training Loss: 0.03854692354798317\n",
      "Epoch 16917/30000 Training Loss: 0.03963183984160423\n",
      "Epoch 16918/30000 Training Loss: 0.04903529956936836\n",
      "Epoch 16919/30000 Training Loss: 0.03888871520757675\n",
      "Epoch 16920/30000 Training Loss: 0.036787841469049454\n",
      "Epoch 16921/30000 Training Loss: 0.04042792692780495\n",
      "Epoch 16922/30000 Training Loss: 0.0362359955906868\n",
      "Epoch 16923/30000 Training Loss: 0.04096094146370888\n",
      "Epoch 16924/30000 Training Loss: 0.043904975056648254\n",
      "Epoch 16925/30000 Training Loss: 0.04248394817113876\n",
      "Epoch 16926/30000 Training Loss: 0.039775069802999496\n",
      "Epoch 16927/30000 Training Loss: 0.03806132823228836\n",
      "Epoch 16928/30000 Training Loss: 0.038401853293180466\n",
      "Epoch 16929/30000 Training Loss: 0.03888395428657532\n",
      "Epoch 16930/30000 Training Loss: 0.04352472722530365\n",
      "Epoch 16931/30000 Training Loss: 0.04567814618349075\n",
      "Epoch 16932/30000 Training Loss: 0.045409101992845535\n",
      "Epoch 16933/30000 Training Loss: 0.0411842055618763\n",
      "Epoch 16934/30000 Training Loss: 0.04658858850598335\n",
      "Epoch 16935/30000 Training Loss: 0.040609247982501984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16936/30000 Training Loss: 0.039639078080654144\n",
      "Epoch 16937/30000 Training Loss: 0.041909605264663696\n",
      "Epoch 16938/30000 Training Loss: 0.0371256098151207\n",
      "Epoch 16939/30000 Training Loss: 0.0433310940861702\n",
      "Epoch 16940/30000 Training Loss: 0.042295582592487335\n",
      "Epoch 16941/30000 Training Loss: 0.03768568113446236\n",
      "Epoch 16942/30000 Training Loss: 0.0477200448513031\n",
      "Epoch 16943/30000 Training Loss: 0.04092716425657272\n",
      "Epoch 16944/30000 Training Loss: 0.044743575155735016\n",
      "Epoch 16945/30000 Training Loss: 0.039907798171043396\n",
      "Epoch 16946/30000 Training Loss: 0.04028746858239174\n",
      "Epoch 16947/30000 Training Loss: 0.042080480605363846\n",
      "Epoch 16948/30000 Training Loss: 0.04114952310919762\n",
      "Epoch 16949/30000 Training Loss: 0.04679793119430542\n",
      "Epoch 16950/30000 Training Loss: 0.04378896951675415\n",
      "Epoch 16950/30000 Validation Loss: 0.03808325156569481\n",
      "Epoch 16951/30000 Training Loss: 0.04524725303053856\n",
      "Epoch 16952/30000 Training Loss: 0.04008036106824875\n",
      "Epoch 16953/30000 Training Loss: 0.040088437497615814\n",
      "Epoch 16954/30000 Training Loss: 0.04388595372438431\n",
      "Epoch 16955/30000 Training Loss: 0.04781978204846382\n",
      "Epoch 16956/30000 Training Loss: 0.038711193948984146\n",
      "Epoch 16957/30000 Training Loss: 0.04781560227274895\n",
      "Epoch 16958/30000 Training Loss: 0.04215846210718155\n",
      "Epoch 16959/30000 Training Loss: 0.03977949172258377\n",
      "Epoch 16960/30000 Training Loss: 0.04097597673535347\n",
      "Epoch 16961/30000 Training Loss: 0.04983793944120407\n",
      "Epoch 16962/30000 Training Loss: 0.036059774458408356\n",
      "Epoch 16963/30000 Training Loss: 0.044566810131073\n",
      "Epoch 16964/30000 Training Loss: 0.047304652631282806\n",
      "Epoch 16965/30000 Training Loss: 0.038969986140728\n",
      "Epoch 16966/30000 Training Loss: 0.042333800345659256\n",
      "Epoch 16967/30000 Training Loss: 0.04020512104034424\n",
      "Epoch 16968/30000 Training Loss: 0.03863793611526489\n",
      "Epoch 16969/30000 Training Loss: 0.04635036736726761\n",
      "Epoch 16970/30000 Training Loss: 0.04250491410493851\n",
      "Epoch 16971/30000 Training Loss: 0.04127294197678566\n",
      "Epoch 16972/30000 Training Loss: 0.05095406621694565\n",
      "Epoch 16973/30000 Training Loss: 0.046228304505348206\n",
      "Epoch 16974/30000 Training Loss: 0.04640704393386841\n",
      "Epoch 16975/30000 Training Loss: 0.037666354328393936\n",
      "Epoch 16976/30000 Training Loss: 0.04493079334497452\n",
      "Epoch 16977/30000 Training Loss: 0.03998924046754837\n",
      "Epoch 16978/30000 Training Loss: 0.035922396928071976\n",
      "Epoch 16979/30000 Training Loss: 0.03538072109222412\n",
      "Epoch 16980/30000 Training Loss: 0.04550088196992874\n",
      "Epoch 16981/30000 Training Loss: 0.04120538383722305\n",
      "Epoch 16982/30000 Training Loss: 0.04353296011686325\n",
      "Epoch 16983/30000 Training Loss: 0.036596134305000305\n",
      "Epoch 16984/30000 Training Loss: 0.04392109811306\n",
      "Epoch 16985/30000 Training Loss: 0.04336225241422653\n",
      "Epoch 16986/30000 Training Loss: 0.04000085964798927\n",
      "Epoch 16987/30000 Training Loss: 0.04416794329881668\n",
      "Epoch 16988/30000 Training Loss: 0.03981838375329971\n",
      "Epoch 16989/30000 Training Loss: 0.03426632657647133\n",
      "Epoch 16990/30000 Training Loss: 0.042337559163570404\n",
      "Epoch 16991/30000 Training Loss: 0.043866537511348724\n",
      "Epoch 16992/30000 Training Loss: 0.0423990897834301\n",
      "Epoch 16993/30000 Training Loss: 0.0421074740588665\n",
      "Epoch 16994/30000 Training Loss: 0.048256684094667435\n",
      "Epoch 16995/30000 Training Loss: 0.03871973976492882\n",
      "Epoch 16996/30000 Training Loss: 0.0326802134513855\n",
      "Epoch 16997/30000 Training Loss: 0.04045027866959572\n",
      "Epoch 16998/30000 Training Loss: 0.0453081876039505\n",
      "Epoch 16999/30000 Training Loss: 0.04309887811541557\n",
      "Epoch 17000/30000 Training Loss: 0.04922177270054817\n",
      "Epoch 17000/30000 Validation Loss: 0.041535597294569016\n",
      "Epoch 17001/30000 Training Loss: 0.04023083671927452\n",
      "Epoch 17002/30000 Training Loss: 0.04151167720556259\n",
      "Epoch 17003/30000 Training Loss: 0.04334331303834915\n",
      "Epoch 17004/30000 Training Loss: 0.04191305488348007\n",
      "Epoch 17005/30000 Training Loss: 0.04324512556195259\n",
      "Epoch 17006/30000 Training Loss: 0.06004054471850395\n",
      "Epoch 17007/30000 Training Loss: 0.045888349413871765\n",
      "Epoch 17008/30000 Training Loss: 0.038406990468502045\n",
      "Epoch 17009/30000 Training Loss: 0.034320998936891556\n",
      "Epoch 17010/30000 Training Loss: 0.04947425052523613\n",
      "Epoch 17011/30000 Training Loss: 0.045799680054187775\n",
      "Epoch 17012/30000 Training Loss: 0.04001913592219353\n",
      "Epoch 17013/30000 Training Loss: 0.04058099538087845\n",
      "Epoch 17014/30000 Training Loss: 0.04658453166484833\n",
      "Epoch 17015/30000 Training Loss: 0.045874156057834625\n",
      "Epoch 17016/30000 Training Loss: 0.042617861181497574\n",
      "Epoch 17017/30000 Training Loss: 0.03964821249246597\n",
      "Epoch 17018/30000 Training Loss: 0.04506770148873329\n",
      "Epoch 17019/30000 Training Loss: 0.03936011716723442\n",
      "Epoch 17020/30000 Training Loss: 0.03622882813215256\n",
      "Epoch 17021/30000 Training Loss: 0.046054013073444366\n",
      "Epoch 17022/30000 Training Loss: 0.046639036387205124\n",
      "Epoch 17023/30000 Training Loss: 0.03616071492433548\n",
      "Epoch 17024/30000 Training Loss: 0.044934917241334915\n",
      "Epoch 17025/30000 Training Loss: 0.043869175016880035\n",
      "Epoch 17026/30000 Training Loss: 0.04507046192884445\n",
      "Epoch 17027/30000 Training Loss: 0.04377548396587372\n",
      "Epoch 17028/30000 Training Loss: 0.047205276787281036\n",
      "Epoch 17029/30000 Training Loss: 0.04485005885362625\n",
      "Epoch 17030/30000 Training Loss: 0.04349999874830246\n",
      "Epoch 17031/30000 Training Loss: 0.04295957088470459\n",
      "Epoch 17032/30000 Training Loss: 0.04905492439866066\n",
      "Epoch 17033/30000 Training Loss: 0.049241576343774796\n",
      "Epoch 17034/30000 Training Loss: 0.0384589359164238\n",
      "Epoch 17035/30000 Training Loss: 0.04032296687364578\n",
      "Epoch 17036/30000 Training Loss: 0.036869876086711884\n",
      "Epoch 17037/30000 Training Loss: 0.04005538299679756\n",
      "Epoch 17038/30000 Training Loss: 0.03993615135550499\n",
      "Epoch 17039/30000 Training Loss: 0.04855576157569885\n",
      "Epoch 17040/30000 Training Loss: 0.038650333881378174\n",
      "Epoch 17041/30000 Training Loss: 0.04022908955812454\n",
      "Epoch 17042/30000 Training Loss: 0.03882255032658577\n",
      "Epoch 17043/30000 Training Loss: 0.04666610807180405\n",
      "Epoch 17044/30000 Training Loss: 0.046904243528842926\n",
      "Epoch 17045/30000 Training Loss: 0.03942259028553963\n",
      "Epoch 17046/30000 Training Loss: 0.045782770961523056\n",
      "Epoch 17047/30000 Training Loss: 0.04437903314828873\n",
      "Epoch 17048/30000 Training Loss: 0.037544768303632736\n",
      "Epoch 17049/30000 Training Loss: 0.04039085656404495\n",
      "Epoch 17050/30000 Training Loss: 0.039401985704898834\n",
      "Epoch 17050/30000 Validation Loss: 0.04240749031305313\n",
      "Epoch 17051/30000 Training Loss: 0.043430861085653305\n",
      "Epoch 17052/30000 Training Loss: 0.04824158176779747\n",
      "Epoch 17053/30000 Training Loss: 0.03698602318763733\n",
      "Epoch 17054/30000 Training Loss: 0.047588251531124115\n",
      "Epoch 17055/30000 Training Loss: 0.04347669333219528\n",
      "Epoch 17056/30000 Training Loss: 0.04000229388475418\n",
      "Epoch 17057/30000 Training Loss: 0.03993121534585953\n",
      "Epoch 17058/30000 Training Loss: 0.04176589101552963\n",
      "Epoch 17059/30000 Training Loss: 0.04736720398068428\n",
      "Epoch 17060/30000 Training Loss: 0.03916812688112259\n",
      "Epoch 17061/30000 Training Loss: 0.04479949176311493\n",
      "Epoch 17062/30000 Training Loss: 0.04831329733133316\n",
      "Epoch 17063/30000 Training Loss: 0.03546682372689247\n",
      "Epoch 17064/30000 Training Loss: 0.04165717214345932\n",
      "Epoch 17065/30000 Training Loss: 0.042606644332408905\n",
      "Epoch 17066/30000 Training Loss: 0.039141226559877396\n",
      "Epoch 17067/30000 Training Loss: 0.03895072266459465\n",
      "Epoch 17068/30000 Training Loss: 0.04365379363298416\n",
      "Epoch 17069/30000 Training Loss: 0.03977542370557785\n",
      "Epoch 17070/30000 Training Loss: 0.04504602029919624\n",
      "Epoch 17071/30000 Training Loss: 0.04266345128417015\n",
      "Epoch 17072/30000 Training Loss: 0.04922686144709587\n",
      "Epoch 17073/30000 Training Loss: 0.04107503965497017\n",
      "Epoch 17074/30000 Training Loss: 0.04531621187925339\n",
      "Epoch 17075/30000 Training Loss: 0.040173906832933426\n",
      "Epoch 17076/30000 Training Loss: 0.039587341248989105\n",
      "Epoch 17077/30000 Training Loss: 0.03776107355952263\n",
      "Epoch 17078/30000 Training Loss: 0.03597709536552429\n",
      "Epoch 17079/30000 Training Loss: 0.052812546491622925\n",
      "Epoch 17080/30000 Training Loss: 0.04105556756258011\n",
      "Epoch 17081/30000 Training Loss: 0.04532371833920479\n",
      "Epoch 17082/30000 Training Loss: 0.03792726621031761\n",
      "Epoch 17083/30000 Training Loss: 0.045753113925457\n",
      "Epoch 17084/30000 Training Loss: 0.040294915437698364\n",
      "Epoch 17085/30000 Training Loss: 0.03987499326467514\n",
      "Epoch 17086/30000 Training Loss: 0.04242822527885437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17087/30000 Training Loss: 0.04431139677762985\n",
      "Epoch 17088/30000 Training Loss: 0.040849719196558\n",
      "Epoch 17089/30000 Training Loss: 0.05037592723965645\n",
      "Epoch 17090/30000 Training Loss: 0.0368606373667717\n",
      "Epoch 17091/30000 Training Loss: 0.04077469930052757\n",
      "Epoch 17092/30000 Training Loss: 0.040293049067258835\n",
      "Epoch 17093/30000 Training Loss: 0.03743733465671539\n",
      "Epoch 17094/30000 Training Loss: 0.03901582956314087\n",
      "Epoch 17095/30000 Training Loss: 0.04588506743311882\n",
      "Epoch 17096/30000 Training Loss: 0.03893381357192993\n",
      "Epoch 17097/30000 Training Loss: 0.04602644965052605\n",
      "Epoch 17098/30000 Training Loss: 0.04355207085609436\n",
      "Epoch 17099/30000 Training Loss: 0.04618249461054802\n",
      "Epoch 17100/30000 Training Loss: 0.03608442470431328\n",
      "Epoch 17100/30000 Validation Loss: 0.038836706429719925\n",
      "Epoch 17101/30000 Training Loss: 0.04530809447169304\n",
      "Epoch 17102/30000 Training Loss: 0.04808112233877182\n",
      "Epoch 17103/30000 Training Loss: 0.04342027008533478\n",
      "Epoch 17104/30000 Training Loss: 0.04445522278547287\n",
      "Epoch 17105/30000 Training Loss: 0.03976937010884285\n",
      "Epoch 17106/30000 Training Loss: 0.041670482605695724\n",
      "Epoch 17107/30000 Training Loss: 0.04443798214197159\n",
      "Epoch 17108/30000 Training Loss: 0.039052579551935196\n",
      "Epoch 17109/30000 Training Loss: 0.05482298135757446\n",
      "Epoch 17110/30000 Training Loss: 0.038581669330596924\n",
      "Epoch 17111/30000 Training Loss: 0.043332505971193314\n",
      "Epoch 17112/30000 Training Loss: 0.04347286373376846\n",
      "Epoch 17113/30000 Training Loss: 0.04260096698999405\n",
      "Epoch 17114/30000 Training Loss: 0.04387534782290459\n",
      "Epoch 17115/30000 Training Loss: 0.045288071036338806\n",
      "Epoch 17116/30000 Training Loss: 0.050044506788253784\n",
      "Epoch 17117/30000 Training Loss: 0.03684169426560402\n",
      "Epoch 17118/30000 Training Loss: 0.03708315268158913\n",
      "Epoch 17119/30000 Training Loss: 0.04109812527894974\n",
      "Epoch 17120/30000 Training Loss: 0.03387880697846413\n",
      "Epoch 17121/30000 Training Loss: 0.048890046775341034\n",
      "Epoch 17122/30000 Training Loss: 0.04797442629933357\n",
      "Epoch 17123/30000 Training Loss: 0.04079373925924301\n",
      "Epoch 17124/30000 Training Loss: 0.05298041179776192\n",
      "Epoch 17125/30000 Training Loss: 0.051464665681123734\n",
      "Epoch 17126/30000 Training Loss: 0.034915827214717865\n",
      "Epoch 17127/30000 Training Loss: 0.03735649585723877\n",
      "Epoch 17128/30000 Training Loss: 0.04126865416765213\n",
      "Epoch 17129/30000 Training Loss: 0.04372936487197876\n",
      "Epoch 17130/30000 Training Loss: 0.03797062486410141\n",
      "Epoch 17131/30000 Training Loss: 0.047078438103199005\n",
      "Epoch 17132/30000 Training Loss: 0.03854081779718399\n",
      "Epoch 17133/30000 Training Loss: 0.05059066414833069\n",
      "Epoch 17134/30000 Training Loss: 0.04481431841850281\n",
      "Epoch 17135/30000 Training Loss: 0.04564020782709122\n",
      "Epoch 17136/30000 Training Loss: 0.046177320182323456\n",
      "Epoch 17137/30000 Training Loss: 0.040447987616062164\n",
      "Epoch 17138/30000 Training Loss: 0.042726535350084305\n",
      "Epoch 17139/30000 Training Loss: 0.049659572541713715\n",
      "Epoch 17140/30000 Training Loss: 0.04545722156763077\n",
      "Epoch 17141/30000 Training Loss: 0.041915714740753174\n",
      "Epoch 17142/30000 Training Loss: 0.04131554812192917\n",
      "Epoch 17143/30000 Training Loss: 0.036110710352659225\n",
      "Epoch 17144/30000 Training Loss: 0.0441533587872982\n",
      "Epoch 17145/30000 Training Loss: 0.038173891603946686\n",
      "Epoch 17146/30000 Training Loss: 0.04125272482633591\n",
      "Epoch 17147/30000 Training Loss: 0.041812289506196976\n",
      "Epoch 17148/30000 Training Loss: 0.036931637674570084\n",
      "Epoch 17149/30000 Training Loss: 0.04318048432469368\n",
      "Epoch 17150/30000 Training Loss: 0.04537937417626381\n",
      "Epoch 17150/30000 Validation Loss: 0.04065645486116409\n",
      "Epoch 17151/30000 Training Loss: 0.04193063825368881\n",
      "Epoch 17152/30000 Training Loss: 0.044492851942777634\n",
      "Epoch 17153/30000 Training Loss: 0.041780367493629456\n",
      "Epoch 17154/30000 Training Loss: 0.03634262830018997\n",
      "Epoch 17155/30000 Training Loss: 0.03901022672653198\n",
      "Epoch 17156/30000 Training Loss: 0.042761556804180145\n",
      "Epoch 17157/30000 Training Loss: 0.051579784601926804\n",
      "Epoch 17158/30000 Training Loss: 0.04301488399505615\n",
      "Epoch 17159/30000 Training Loss: 0.03884092718362808\n",
      "Epoch 17160/30000 Training Loss: 0.038874175399541855\n",
      "Epoch 17161/30000 Training Loss: 0.05054643005132675\n",
      "Epoch 17162/30000 Training Loss: 0.042291879653930664\n",
      "Epoch 17163/30000 Training Loss: 0.042933568358421326\n",
      "Epoch 17164/30000 Training Loss: 0.0464249886572361\n",
      "Epoch 17165/30000 Training Loss: 0.0461953766644001\n",
      "Epoch 17166/30000 Training Loss: 0.034449685364961624\n",
      "Epoch 17167/30000 Training Loss: 0.04532730579376221\n",
      "Epoch 17168/30000 Training Loss: 0.04530953988432884\n",
      "Epoch 17169/30000 Training Loss: 0.038495637476444244\n",
      "Epoch 17170/30000 Training Loss: 0.049944158643484116\n",
      "Epoch 17171/30000 Training Loss: 0.046136777848005295\n",
      "Epoch 17172/30000 Training Loss: 0.03962961584329605\n",
      "Epoch 17173/30000 Training Loss: 0.04572456702589989\n",
      "Epoch 17174/30000 Training Loss: 0.04348978027701378\n",
      "Epoch 17175/30000 Training Loss: 0.04322456941008568\n",
      "Epoch 17176/30000 Training Loss: 0.04496766999363899\n",
      "Epoch 17177/30000 Training Loss: 0.040481045842170715\n",
      "Epoch 17178/30000 Training Loss: 0.046698421239852905\n",
      "Epoch 17179/30000 Training Loss: 0.04184018447995186\n",
      "Epoch 17180/30000 Training Loss: 0.04880179092288017\n",
      "Epoch 17181/30000 Training Loss: 0.04853568226099014\n",
      "Epoch 17182/30000 Training Loss: 0.03632882982492447\n",
      "Epoch 17183/30000 Training Loss: 0.04673177748918533\n",
      "Epoch 17184/30000 Training Loss: 0.043309129774570465\n",
      "Epoch 17185/30000 Training Loss: 0.03784821927547455\n",
      "Epoch 17186/30000 Training Loss: 0.037575364112854004\n",
      "Epoch 17187/30000 Training Loss: 0.052889831364154816\n",
      "Epoch 17188/30000 Training Loss: 0.04470181465148926\n",
      "Epoch 17189/30000 Training Loss: 0.04666747525334358\n",
      "Epoch 17190/30000 Training Loss: 0.0442211851477623\n",
      "Epoch 17191/30000 Training Loss: 0.040722332894802094\n",
      "Epoch 17192/30000 Training Loss: 0.044982653111219406\n",
      "Epoch 17193/30000 Training Loss: 0.04284753277897835\n",
      "Epoch 17194/30000 Training Loss: 0.03409677743911743\n",
      "Epoch 17195/30000 Training Loss: 0.03956323489546776\n",
      "Epoch 17196/30000 Training Loss: 0.03998834639787674\n",
      "Epoch 17197/30000 Training Loss: 0.04405580461025238\n",
      "Epoch 17198/30000 Training Loss: 0.04328635334968567\n",
      "Epoch 17199/30000 Training Loss: 0.04744032770395279\n",
      "Epoch 17200/30000 Training Loss: 0.04868130758404732\n",
      "Epoch 17200/30000 Validation Loss: 0.03659187629818916\n",
      "Epoch 17201/30000 Training Loss: 0.042610324919223785\n",
      "Epoch 17202/30000 Training Loss: 0.04566863924264908\n",
      "Epoch 17203/30000 Training Loss: 0.045657239854335785\n",
      "Epoch 17204/30000 Training Loss: 0.03543552756309509\n",
      "Epoch 17205/30000 Training Loss: 0.039634834975004196\n",
      "Epoch 17206/30000 Training Loss: 0.04292086511850357\n",
      "Epoch 17207/30000 Training Loss: 0.04513603448867798\n",
      "Epoch 17208/30000 Training Loss: 0.036943864077329636\n",
      "Epoch 17209/30000 Training Loss: 0.04139033332467079\n",
      "Epoch 17210/30000 Training Loss: 0.04052921384572983\n",
      "Epoch 17211/30000 Training Loss: 0.037650249898433685\n",
      "Epoch 17212/30000 Training Loss: 0.03960513323545456\n",
      "Epoch 17213/30000 Training Loss: 0.04337792843580246\n",
      "Epoch 17214/30000 Training Loss: 0.03546935319900513\n",
      "Epoch 17215/30000 Training Loss: 0.04787261039018631\n",
      "Epoch 17216/30000 Training Loss: 0.05290977284312248\n",
      "Epoch 17217/30000 Training Loss: 0.03269784897565842\n",
      "Epoch 17218/30000 Training Loss: 0.0441497378051281\n",
      "Epoch 17219/30000 Training Loss: 0.0413222536444664\n",
      "Epoch 17220/30000 Training Loss: 0.04544362798333168\n",
      "Epoch 17221/30000 Training Loss: 0.04104510322213173\n",
      "Epoch 17222/30000 Training Loss: 0.0484226830303669\n",
      "Epoch 17223/30000 Training Loss: 0.054741568863391876\n",
      "Epoch 17224/30000 Training Loss: 0.04846590384840965\n",
      "Epoch 17225/30000 Training Loss: 0.05277516692876816\n",
      "Epoch 17226/30000 Training Loss: 0.04480484127998352\n",
      "Epoch 17227/30000 Training Loss: 0.03735564649105072\n",
      "Epoch 17228/30000 Training Loss: 0.04531214013695717\n",
      "Epoch 17229/30000 Training Loss: 0.049300845712423325\n",
      "Epoch 17230/30000 Training Loss: 0.039275530725717545\n",
      "Epoch 17231/30000 Training Loss: 0.0423692986369133\n",
      "Epoch 17232/30000 Training Loss: 0.03771002218127251\n",
      "Epoch 17233/30000 Training Loss: 0.04413467273116112\n",
      "Epoch 17234/30000 Training Loss: 0.04249881953001022\n",
      "Epoch 17235/30000 Training Loss: 0.04954563081264496\n",
      "Epoch 17236/30000 Training Loss: 0.052155815064907074\n",
      "Epoch 17237/30000 Training Loss: 0.04013539105653763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17238/30000 Training Loss: 0.0447341687977314\n",
      "Epoch 17239/30000 Training Loss: 0.039657462388277054\n",
      "Epoch 17240/30000 Training Loss: 0.0414966456592083\n",
      "Epoch 17241/30000 Training Loss: 0.037363048642873764\n",
      "Epoch 17242/30000 Training Loss: 0.0368565209209919\n",
      "Epoch 17243/30000 Training Loss: 0.03618202358484268\n",
      "Epoch 17244/30000 Training Loss: 0.03946944326162338\n",
      "Epoch 17245/30000 Training Loss: 0.04213006794452667\n",
      "Epoch 17246/30000 Training Loss: 0.05154275894165039\n",
      "Epoch 17247/30000 Training Loss: 0.037897739559412\n",
      "Epoch 17248/30000 Training Loss: 0.0464872345328331\n",
      "Epoch 17249/30000 Training Loss: 0.04009057953953743\n",
      "Epoch 17250/30000 Training Loss: 0.03615758568048477\n",
      "Epoch 17250/30000 Validation Loss: 0.04539017006754875\n",
      "Epoch 17251/30000 Training Loss: 0.048921700567007065\n",
      "Epoch 17252/30000 Training Loss: 0.0400889627635479\n",
      "Epoch 17253/30000 Training Loss: 0.04907383769750595\n",
      "Epoch 17254/30000 Training Loss: 0.03767434507608414\n",
      "Epoch 17255/30000 Training Loss: 0.04575708508491516\n",
      "Epoch 17256/30000 Training Loss: 0.04530559480190277\n",
      "Epoch 17257/30000 Training Loss: 0.039283908903598785\n",
      "Epoch 17258/30000 Training Loss: 0.03757502883672714\n",
      "Epoch 17259/30000 Training Loss: 0.042396727949380875\n",
      "Epoch 17260/30000 Training Loss: 0.05401705577969551\n",
      "Epoch 17261/30000 Training Loss: 0.03875233605504036\n",
      "Epoch 17262/30000 Training Loss: 0.050801217555999756\n",
      "Epoch 17263/30000 Training Loss: 0.041423700749874115\n",
      "Epoch 17264/30000 Training Loss: 0.04324429854750633\n",
      "Epoch 17265/30000 Training Loss: 0.04626297950744629\n",
      "Epoch 17266/30000 Training Loss: 0.04299795627593994\n",
      "Epoch 17267/30000 Training Loss: 0.04151279851794243\n",
      "Epoch 17268/30000 Training Loss: 0.04450025409460068\n",
      "Epoch 17269/30000 Training Loss: 0.04073939099907875\n",
      "Epoch 17270/30000 Training Loss: 0.040306419134140015\n",
      "Epoch 17271/30000 Training Loss: 0.03767765685915947\n",
      "Epoch 17272/30000 Training Loss: 0.043019916862249374\n",
      "Epoch 17273/30000 Training Loss: 0.05500689893960953\n",
      "Epoch 17274/30000 Training Loss: 0.0409136600792408\n",
      "Epoch 17275/30000 Training Loss: 0.038385070860385895\n",
      "Epoch 17276/30000 Training Loss: 0.03884178400039673\n",
      "Epoch 17277/30000 Training Loss: 0.04161820188164711\n",
      "Epoch 17278/30000 Training Loss: 0.05053349584341049\n",
      "Epoch 17279/30000 Training Loss: 0.044503916054964066\n",
      "Epoch 17280/30000 Training Loss: 0.040712542831897736\n",
      "Epoch 17281/30000 Training Loss: 0.04415871575474739\n",
      "Epoch 17282/30000 Training Loss: 0.046196792274713516\n",
      "Epoch 17283/30000 Training Loss: 0.044825054705142975\n",
      "Epoch 17284/30000 Training Loss: 0.0402289554476738\n",
      "Epoch 17285/30000 Training Loss: 0.047671664506196976\n",
      "Epoch 17286/30000 Training Loss: 0.0447741337120533\n",
      "Epoch 17287/30000 Training Loss: 0.045313648879528046\n",
      "Epoch 17288/30000 Training Loss: 0.052525680512189865\n",
      "Epoch 17289/30000 Training Loss: 0.03819756954908371\n",
      "Epoch 17290/30000 Training Loss: 0.045080773532390594\n",
      "Epoch 17291/30000 Training Loss: 0.04550072178244591\n",
      "Epoch 17292/30000 Training Loss: 0.04789324104785919\n",
      "Epoch 17293/30000 Training Loss: 0.0471998006105423\n",
      "Epoch 17294/30000 Training Loss: 0.047173865139484406\n",
      "Epoch 17295/30000 Training Loss: 0.044545359909534454\n",
      "Epoch 17296/30000 Training Loss: 0.03576618432998657\n",
      "Epoch 17297/30000 Training Loss: 0.04423234984278679\n",
      "Epoch 17298/30000 Training Loss: 0.032000165432691574\n",
      "Epoch 17299/30000 Training Loss: 0.037790581583976746\n",
      "Epoch 17300/30000 Training Loss: 0.04854946583509445\n",
      "Epoch 17300/30000 Validation Loss: 0.037733521312475204\n",
      "Epoch 17301/30000 Training Loss: 0.05096728354692459\n",
      "Epoch 17302/30000 Training Loss: 0.045231252908706665\n",
      "Epoch 17303/30000 Training Loss: 0.04341857507824898\n",
      "Epoch 17304/30000 Training Loss: 0.04421064257621765\n",
      "Epoch 17305/30000 Training Loss: 0.0397920124232769\n",
      "Epoch 17306/30000 Training Loss: 0.044366106390953064\n",
      "Epoch 17307/30000 Training Loss: 0.034754715859889984\n",
      "Epoch 17308/30000 Training Loss: 0.03345191851258278\n",
      "Epoch 17309/30000 Training Loss: 0.04223392531275749\n",
      "Epoch 17310/30000 Training Loss: 0.04292330890893936\n",
      "Epoch 17311/30000 Training Loss: 0.04017519950866699\n",
      "Epoch 17312/30000 Training Loss: 0.03948727250099182\n",
      "Epoch 17313/30000 Training Loss: 0.04275541380047798\n",
      "Epoch 17314/30000 Training Loss: 0.039509743452072144\n",
      "Epoch 17315/30000 Training Loss: 0.043162886053323746\n",
      "Epoch 17316/30000 Training Loss: 0.04350084066390991\n",
      "Epoch 17317/30000 Training Loss: 0.042369432747364044\n",
      "Epoch 17318/30000 Training Loss: 0.03595767915248871\n",
      "Epoch 17319/30000 Training Loss: 0.045292533934116364\n",
      "Epoch 17320/30000 Training Loss: 0.03674028068780899\n",
      "Epoch 17321/30000 Training Loss: 0.049495015293359756\n",
      "Epoch 17322/30000 Training Loss: 0.03762253373861313\n",
      "Epoch 17323/30000 Training Loss: 0.044666554778814316\n",
      "Epoch 17324/30000 Training Loss: 0.04059073328971863\n",
      "Epoch 17325/30000 Training Loss: 0.041696153581142426\n",
      "Epoch 17326/30000 Training Loss: 0.038354434072971344\n",
      "Epoch 17327/30000 Training Loss: 0.04203726723790169\n",
      "Epoch 17328/30000 Training Loss: 0.04113401472568512\n",
      "Epoch 17329/30000 Training Loss: 0.0385085828602314\n",
      "Epoch 17330/30000 Training Loss: 0.040446870028972626\n",
      "Epoch 17331/30000 Training Loss: 0.03568914532661438\n",
      "Epoch 17332/30000 Training Loss: 0.039235543459653854\n",
      "Epoch 17333/30000 Training Loss: 0.03960121050477028\n",
      "Epoch 17334/30000 Training Loss: 0.03414588421583176\n",
      "Epoch 17335/30000 Training Loss: 0.04697049781680107\n",
      "Epoch 17336/30000 Training Loss: 0.04809160158038139\n",
      "Epoch 17337/30000 Training Loss: 0.03747618943452835\n",
      "Epoch 17338/30000 Training Loss: 0.044804494827985764\n",
      "Epoch 17339/30000 Training Loss: 0.043458469212055206\n",
      "Epoch 17340/30000 Training Loss: 0.051883257925510406\n",
      "Epoch 17341/30000 Training Loss: 0.04607989266514778\n",
      "Epoch 17342/30000 Training Loss: 0.039806999266147614\n",
      "Epoch 17343/30000 Training Loss: 0.04329821839928627\n",
      "Epoch 17344/30000 Training Loss: 0.04106094688177109\n",
      "Epoch 17345/30000 Training Loss: 0.03885643556714058\n",
      "Epoch 17346/30000 Training Loss: 0.048969633877277374\n",
      "Epoch 17347/30000 Training Loss: 0.03965921327471733\n",
      "Epoch 17348/30000 Training Loss: 0.04601125419139862\n",
      "Epoch 17349/30000 Training Loss: 0.03841356933116913\n",
      "Epoch 17350/30000 Training Loss: 0.044257573783397675\n",
      "Epoch 17350/30000 Validation Loss: 0.04030890017747879\n",
      "Epoch 17351/30000 Training Loss: 0.037062037736177444\n",
      "Epoch 17352/30000 Training Loss: 0.044371724128723145\n",
      "Epoch 17353/30000 Training Loss: 0.047548502683639526\n",
      "Epoch 17354/30000 Training Loss: 0.05632364749908447\n",
      "Epoch 17355/30000 Training Loss: 0.048818573355674744\n",
      "Epoch 17356/30000 Training Loss: 0.03868761658668518\n",
      "Epoch 17357/30000 Training Loss: 0.04915543273091316\n",
      "Epoch 17358/30000 Training Loss: 0.03836323320865631\n",
      "Epoch 17359/30000 Training Loss: 0.03504026681184769\n",
      "Epoch 17360/30000 Training Loss: 0.042650818824768066\n",
      "Epoch 17361/30000 Training Loss: 0.041585005819797516\n",
      "Epoch 17362/30000 Training Loss: 0.03968152031302452\n",
      "Epoch 17363/30000 Training Loss: 0.04617489501833916\n",
      "Epoch 17364/30000 Training Loss: 0.04904601722955704\n",
      "Epoch 17365/30000 Training Loss: 0.03581571578979492\n",
      "Epoch 17366/30000 Training Loss: 0.03677593544125557\n",
      "Epoch 17367/30000 Training Loss: 0.03990386053919792\n",
      "Epoch 17368/30000 Training Loss: 0.04079977422952652\n",
      "Epoch 17369/30000 Training Loss: 0.037751857191324234\n",
      "Epoch 17370/30000 Training Loss: 0.048891421407461166\n",
      "Epoch 17371/30000 Training Loss: 0.045095302164554596\n",
      "Epoch 17372/30000 Training Loss: 0.047833047807216644\n",
      "Epoch 17373/30000 Training Loss: 0.042000919580459595\n",
      "Epoch 17374/30000 Training Loss: 0.04964154213666916\n",
      "Epoch 17375/30000 Training Loss: 0.04771503433585167\n",
      "Epoch 17376/30000 Training Loss: 0.04340603947639465\n",
      "Epoch 17377/30000 Training Loss: 0.034002967178821564\n",
      "Epoch 17378/30000 Training Loss: 0.0408148430287838\n",
      "Epoch 17379/30000 Training Loss: 0.04282435029745102\n",
      "Epoch 17380/30000 Training Loss: 0.04122598096728325\n",
      "Epoch 17381/30000 Training Loss: 0.04338802397251129\n",
      "Epoch 17382/30000 Training Loss: 0.037899941205978394\n",
      "Epoch 17383/30000 Training Loss: 0.0391666516661644\n",
      "Epoch 17384/30000 Training Loss: 0.04569210857152939\n",
      "Epoch 17385/30000 Training Loss: 0.038329996168613434\n",
      "Epoch 17386/30000 Training Loss: 0.04201311618089676\n",
      "Epoch 17387/30000 Training Loss: 0.038940370082855225\n",
      "Epoch 17388/30000 Training Loss: 0.04798523336648941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17389/30000 Training Loss: 0.04652409628033638\n",
      "Epoch 17390/30000 Training Loss: 0.041155971586704254\n",
      "Epoch 17391/30000 Training Loss: 0.03764873743057251\n",
      "Epoch 17392/30000 Training Loss: 0.03909387066960335\n",
      "Epoch 17393/30000 Training Loss: 0.037576574832201004\n",
      "Epoch 17394/30000 Training Loss: 0.044741805642843246\n",
      "Epoch 17395/30000 Training Loss: 0.039035238325595856\n",
      "Epoch 17396/30000 Training Loss: 0.04669235646724701\n",
      "Epoch 17397/30000 Training Loss: 0.034100279211997986\n",
      "Epoch 17398/30000 Training Loss: 0.03852841630578041\n",
      "Epoch 17399/30000 Training Loss: 0.03717251121997833\n",
      "Epoch 17400/30000 Training Loss: 0.042990751564502716\n",
      "Epoch 17400/30000 Validation Loss: 0.04158739373087883\n",
      "Epoch 17401/30000 Training Loss: 0.0473649799823761\n",
      "Epoch 17402/30000 Training Loss: 0.04578676074743271\n",
      "Epoch 17403/30000 Training Loss: 0.04300684109330177\n",
      "Epoch 17404/30000 Training Loss: 0.048017822206020355\n",
      "Epoch 17405/30000 Training Loss: 0.050269853323698044\n",
      "Epoch 17406/30000 Training Loss: 0.04961330443620682\n",
      "Epoch 17407/30000 Training Loss: 0.047967396676540375\n",
      "Epoch 17408/30000 Training Loss: 0.03781978413462639\n",
      "Epoch 17409/30000 Training Loss: 0.044805098325014114\n",
      "Epoch 17410/30000 Training Loss: 0.04682838171720505\n",
      "Epoch 17411/30000 Training Loss: 0.043857280164957047\n",
      "Epoch 17412/30000 Training Loss: 0.04287869483232498\n",
      "Epoch 17413/30000 Training Loss: 0.041248004883527756\n",
      "Epoch 17414/30000 Training Loss: 0.04306545481085777\n",
      "Epoch 17415/30000 Training Loss: 0.03646541386842728\n",
      "Epoch 17416/30000 Training Loss: 0.04408438876271248\n",
      "Epoch 17417/30000 Training Loss: 0.044193290174007416\n",
      "Epoch 17418/30000 Training Loss: 0.05608128383755684\n",
      "Epoch 17419/30000 Training Loss: 0.03908808156847954\n",
      "Epoch 17420/30000 Training Loss: 0.0481901541352272\n",
      "Epoch 17421/30000 Training Loss: 0.04541412740945816\n",
      "Epoch 17422/30000 Training Loss: 0.03782997652888298\n",
      "Epoch 17423/30000 Training Loss: 0.04311564564704895\n",
      "Epoch 17424/30000 Training Loss: 0.047325391322374344\n",
      "Epoch 17425/30000 Training Loss: 0.038202472031116486\n",
      "Epoch 17426/30000 Training Loss: 0.04127784073352814\n",
      "Epoch 17427/30000 Training Loss: 0.05184353515505791\n",
      "Epoch 17428/30000 Training Loss: 0.04260797053575516\n",
      "Epoch 17429/30000 Training Loss: 0.04320875182747841\n",
      "Epoch 17430/30000 Training Loss: 0.0422799251973629\n",
      "Epoch 17431/30000 Training Loss: 0.047762636095285416\n",
      "Epoch 17432/30000 Training Loss: 0.04340359941124916\n",
      "Epoch 17433/30000 Training Loss: 0.038459617644548416\n",
      "Epoch 17434/30000 Training Loss: 0.043182436376810074\n",
      "Epoch 17435/30000 Training Loss: 0.04324280470609665\n",
      "Epoch 17436/30000 Training Loss: 0.04265264421701431\n",
      "Epoch 17437/30000 Training Loss: 0.04237022250890732\n",
      "Epoch 17438/30000 Training Loss: 0.04200604185461998\n",
      "Epoch 17439/30000 Training Loss: 0.046949468553066254\n",
      "Epoch 17440/30000 Training Loss: 0.0448247455060482\n",
      "Epoch 17441/30000 Training Loss: 0.044647909700870514\n",
      "Epoch 17442/30000 Training Loss: 0.04566054418683052\n",
      "Epoch 17443/30000 Training Loss: 0.03638660907745361\n",
      "Epoch 17444/30000 Training Loss: 0.03899657726287842\n",
      "Epoch 17445/30000 Training Loss: 0.04773060232400894\n",
      "Epoch 17446/30000 Training Loss: 0.042544033378362656\n",
      "Epoch 17447/30000 Training Loss: 0.04320714250206947\n",
      "Epoch 17448/30000 Training Loss: 0.04510853812098503\n",
      "Epoch 17449/30000 Training Loss: 0.03834819048643112\n",
      "Epoch 17450/30000 Training Loss: 0.05069293826818466\n",
      "Epoch 17450/30000 Validation Loss: 0.03946072608232498\n",
      "Epoch 17451/30000 Training Loss: 0.04610317200422287\n",
      "Epoch 17452/30000 Training Loss: 0.03888793662190437\n",
      "Epoch 17453/30000 Training Loss: 0.03695261850953102\n",
      "Epoch 17454/30000 Training Loss: 0.04129663109779358\n",
      "Epoch 17455/30000 Training Loss: 0.04161427170038223\n",
      "Epoch 17456/30000 Training Loss: 0.04194258898496628\n",
      "Epoch 17457/30000 Training Loss: 0.042625561356544495\n",
      "Epoch 17458/30000 Training Loss: 0.037531182169914246\n",
      "Epoch 17459/30000 Training Loss: 0.04187832027673721\n",
      "Epoch 17460/30000 Training Loss: 0.047016389667987823\n",
      "Epoch 17461/30000 Training Loss: 0.04141958802938461\n",
      "Epoch 17462/30000 Training Loss: 0.04573959857225418\n",
      "Epoch 17463/30000 Training Loss: 0.04294484108686447\n",
      "Epoch 17464/30000 Training Loss: 0.04224257543683052\n",
      "Epoch 17465/30000 Training Loss: 0.04099123179912567\n",
      "Epoch 17466/30000 Training Loss: 0.04515257850289345\n",
      "Epoch 17467/30000 Training Loss: 0.05008441209793091\n",
      "Epoch 17468/30000 Training Loss: 0.03786446899175644\n",
      "Epoch 17469/30000 Training Loss: 0.0405481681227684\n",
      "Epoch 17470/30000 Training Loss: 0.04566115140914917\n",
      "Epoch 17471/30000 Training Loss: 0.0384989008307457\n",
      "Epoch 17472/30000 Training Loss: 0.04281847923994064\n",
      "Epoch 17473/30000 Training Loss: 0.044158920645713806\n",
      "Epoch 17474/30000 Training Loss: 0.044087253510951996\n",
      "Epoch 17475/30000 Training Loss: 0.039702270179986954\n",
      "Epoch 17476/30000 Training Loss: 0.03907255083322525\n",
      "Epoch 17477/30000 Training Loss: 0.04968921095132828\n",
      "Epoch 17478/30000 Training Loss: 0.05164344981312752\n",
      "Epoch 17479/30000 Training Loss: 0.04512897878885269\n",
      "Epoch 17480/30000 Training Loss: 0.04897930473089218\n",
      "Epoch 17481/30000 Training Loss: 0.035833027213811874\n",
      "Epoch 17482/30000 Training Loss: 0.040943048894405365\n",
      "Epoch 17483/30000 Training Loss: 0.03580367565155029\n",
      "Epoch 17484/30000 Training Loss: 0.047423508018255234\n",
      "Epoch 17485/30000 Training Loss: 0.04796823114156723\n",
      "Epoch 17486/30000 Training Loss: 0.04034079238772392\n",
      "Epoch 17487/30000 Training Loss: 0.04034288972616196\n",
      "Epoch 17488/30000 Training Loss: 0.034557074308395386\n",
      "Epoch 17489/30000 Training Loss: 0.041010115295648575\n",
      "Epoch 17490/30000 Training Loss: 0.036923885345458984\n",
      "Epoch 17491/30000 Training Loss: 0.04246765375137329\n",
      "Epoch 17492/30000 Training Loss: 0.03983720391988754\n",
      "Epoch 17493/30000 Training Loss: 0.03712175041437149\n",
      "Epoch 17494/30000 Training Loss: 0.03804728761315346\n",
      "Epoch 17495/30000 Training Loss: 0.04382737725973129\n",
      "Epoch 17496/30000 Training Loss: 0.03878277912735939\n",
      "Epoch 17497/30000 Training Loss: 0.04091937094926834\n",
      "Epoch 17498/30000 Training Loss: 0.03732800856232643\n",
      "Epoch 17499/30000 Training Loss: 0.049577899277210236\n",
      "Epoch 17500/30000 Training Loss: 0.047324247658252716\n",
      "Epoch 17500/30000 Validation Loss: 0.041111789643764496\n",
      "Epoch 17501/30000 Training Loss: 0.04440732300281525\n",
      "Epoch 17502/30000 Training Loss: 0.0433327853679657\n",
      "Epoch 17503/30000 Training Loss: 0.045259296894073486\n",
      "Epoch 17504/30000 Training Loss: 0.03811175748705864\n",
      "Epoch 17505/30000 Training Loss: 0.0463930144906044\n",
      "Epoch 17506/30000 Training Loss: 0.0418868251144886\n",
      "Epoch 17507/30000 Training Loss: 0.04334375634789467\n",
      "Epoch 17508/30000 Training Loss: 0.04360509663820267\n",
      "Epoch 17509/30000 Training Loss: 0.03961072117090225\n",
      "Epoch 17510/30000 Training Loss: 0.04800804331898689\n",
      "Epoch 17511/30000 Training Loss: 0.04112595319747925\n",
      "Epoch 17512/30000 Training Loss: 0.04459020495414734\n",
      "Epoch 17513/30000 Training Loss: 0.04283152148127556\n",
      "Epoch 17514/30000 Training Loss: 0.04174147546291351\n",
      "Epoch 17515/30000 Training Loss: 0.049954935908317566\n",
      "Epoch 17516/30000 Training Loss: 0.037363193929195404\n",
      "Epoch 17517/30000 Training Loss: 0.04549797624349594\n",
      "Epoch 17518/30000 Training Loss: 0.047329582273960114\n",
      "Epoch 17519/30000 Training Loss: 0.03485967218875885\n",
      "Epoch 17520/30000 Training Loss: 0.04221378639340401\n",
      "Epoch 17521/30000 Training Loss: 0.05060684680938721\n",
      "Epoch 17522/30000 Training Loss: 0.047578856348991394\n",
      "Epoch 17523/30000 Training Loss: 0.045509468764066696\n",
      "Epoch 17524/30000 Training Loss: 0.03913628309965134\n",
      "Epoch 17525/30000 Training Loss: 0.046588219702243805\n",
      "Epoch 17526/30000 Training Loss: 0.04782500118017197\n",
      "Epoch 17527/30000 Training Loss: 0.046880416572093964\n",
      "Epoch 17528/30000 Training Loss: 0.04114046320319176\n",
      "Epoch 17529/30000 Training Loss: 0.03628857433795929\n",
      "Epoch 17530/30000 Training Loss: 0.04032141715288162\n",
      "Epoch 17531/30000 Training Loss: 0.04029993340373039\n",
      "Epoch 17532/30000 Training Loss: 0.03620436415076256\n",
      "Epoch 17533/30000 Training Loss: 0.04144546017050743\n",
      "Epoch 17534/30000 Training Loss: 0.03412239998579025\n",
      "Epoch 17535/30000 Training Loss: 0.03526247665286064\n",
      "Epoch 17536/30000 Training Loss: 0.034871507436037064\n",
      "Epoch 17537/30000 Training Loss: 0.040154073387384415\n",
      "Epoch 17538/30000 Training Loss: 0.03921590745449066\n",
      "Epoch 17539/30000 Training Loss: 0.04133004695177078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17540/30000 Training Loss: 0.03988983854651451\n",
      "Epoch 17541/30000 Training Loss: 0.043158747255802155\n",
      "Epoch 17542/30000 Training Loss: 0.04662735387682915\n",
      "Epoch 17543/30000 Training Loss: 0.04229269176721573\n",
      "Epoch 17544/30000 Training Loss: 0.045148495584726334\n",
      "Epoch 17545/30000 Training Loss: 0.049619805067777634\n",
      "Epoch 17546/30000 Training Loss: 0.045305781066417694\n",
      "Epoch 17547/30000 Training Loss: 0.04603513702750206\n",
      "Epoch 17548/30000 Training Loss: 0.04339087754487991\n",
      "Epoch 17549/30000 Training Loss: 0.04275808483362198\n",
      "Epoch 17550/30000 Training Loss: 0.045913778245449066\n",
      "Epoch 17550/30000 Validation Loss: 0.04354684799909592\n",
      "Epoch 17551/30000 Training Loss: 0.0428299717605114\n",
      "Epoch 17552/30000 Training Loss: 0.04986891150474548\n",
      "Epoch 17553/30000 Training Loss: 0.04496791213750839\n",
      "Epoch 17554/30000 Training Loss: 0.04697640985250473\n",
      "Epoch 17555/30000 Training Loss: 0.04008740931749344\n",
      "Epoch 17556/30000 Training Loss: 0.04345877468585968\n",
      "Epoch 17557/30000 Training Loss: 0.04015885666012764\n",
      "Epoch 17558/30000 Training Loss: 0.03875327855348587\n",
      "Epoch 17559/30000 Training Loss: 0.038918692618608475\n",
      "Epoch 17560/30000 Training Loss: 0.045799050480127335\n",
      "Epoch 17561/30000 Training Loss: 0.041419412940740585\n",
      "Epoch 17562/30000 Training Loss: 0.0424354262650013\n",
      "Epoch 17563/30000 Training Loss: 0.039494238793849945\n",
      "Epoch 17564/30000 Training Loss: 0.0457075834274292\n",
      "Epoch 17565/30000 Training Loss: 0.04032176733016968\n",
      "Epoch 17566/30000 Training Loss: 0.03331746533513069\n",
      "Epoch 17567/30000 Training Loss: 0.040777239948511124\n",
      "Epoch 17568/30000 Training Loss: 0.04719136282801628\n",
      "Epoch 17569/30000 Training Loss: 0.03926115483045578\n",
      "Epoch 17570/30000 Training Loss: 0.04288256913423538\n",
      "Epoch 17571/30000 Training Loss: 0.035392049700021744\n",
      "Epoch 17572/30000 Training Loss: 0.04485170170664787\n",
      "Epoch 17573/30000 Training Loss: 0.04364959895610809\n",
      "Epoch 17574/30000 Training Loss: 0.0463414303958416\n",
      "Epoch 17575/30000 Training Loss: 0.04073492810130119\n",
      "Epoch 17576/30000 Training Loss: 0.043300919234752655\n",
      "Epoch 17577/30000 Training Loss: 0.03782152384519577\n",
      "Epoch 17578/30000 Training Loss: 0.043491095304489136\n",
      "Epoch 17579/30000 Training Loss: 0.04490210860967636\n",
      "Epoch 17580/30000 Training Loss: 0.03832303732633591\n",
      "Epoch 17581/30000 Training Loss: 0.03974148631095886\n",
      "Epoch 17582/30000 Training Loss: 0.04370439797639847\n",
      "Epoch 17583/30000 Training Loss: 0.04000271484255791\n",
      "Epoch 17584/30000 Training Loss: 0.04011928290128708\n",
      "Epoch 17585/30000 Training Loss: 0.04414335638284683\n",
      "Epoch 17586/30000 Training Loss: 0.043918170034885406\n",
      "Epoch 17587/30000 Training Loss: 0.04539669305086136\n",
      "Epoch 17588/30000 Training Loss: 0.03930939733982086\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "train_loss, val_loss = TrainLoop(\n",
    "                            model=model,\n",
    "                            diffusion=diffusion,\n",
    "                            data=data,\n",
    "                            batch_size=batch_size,\n",
    "                            microbatch=microbatch,\n",
    "                            lr=lr,\n",
    "                            ema_rate=ema_rate,\n",
    "                            schedule_sampler=schedule_sampler,\n",
    "                            weight_decay=weight_decay,\n",
    "                            epochs=epochs,\n",
    "                            eval_data=val,\n",
    "                            eval_interval=eval_interval,\n",
    "                            warm_up_steps=500,\n",
    "                            use_llrd=True,\n",
    "                            llrd_rate=0.99\n",
    "                        ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba46ab1-ecb2-4f6d-b65d-ab8d3bef93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(train_losses, val_losses):\n",
    "\n",
    "    title_size = 28\n",
    "    label_size = 21\n",
    "    legend_size = 18\n",
    "    tick_size = 17\n",
    "    \n",
    "    plot_folder_path = \"plots\"\n",
    "    if not os.path.exists(plot_folder_path):\n",
    "        os.makedirs(plot_folder_path)\n",
    "        \n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))  \n",
    "    train_epochs = np.arange(1, len(train_losses)+1)\n",
    "    val_epochs = np.arange(0, len(train_losses)+1, int(len(train_losses)/(len(val_losses)-1)))\n",
    "    val_epochs[0] = 1\n",
    "    \n",
    "    ax.plot(train_epochs, train_losses, 'tab:blue', label=\"Training Loss\")\n",
    "    ax.plot(val_epochs, val_losses, 'tab:orange', label=f\"Validation Loss (at every {int(len(train_losses)/(len(val_losses)-1))} interval)\")\n",
    "    ax.tick_params(labelsize=tick_size)\n",
    "    ax.set_title('Loss', fontsize=title_size)\n",
    "    ax.set_xlabel('Epochs', fontsize=label_size)\n",
    "    ax.set_ylabel('Loss', fontsize=label_size)\n",
    "    ax.legend(loc=\"upper right\", fontsize=legend_size)\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_folder_path, \"loss_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fa9c7-84c8-48bd-b368-318df3f0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d\")\n",
    "# best_model_fp = f'models/0221/model_best_epoch_20600_min_val_loss_0.028699999675154686.pkl'\n",
    "# with open(best_model_fp, 'rb') as handle:\n",
    "#     best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        2000,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3e232-af43-44f3-8acb-ec31f1b0bd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref = sampling(model, \n",
    "                                                           diffusion, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184761",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80683b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
