{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_arch.run_train import *\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from model_arch.train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from model_arch.tokenizer import load_tokenizer, load_model_emb\n",
    "from model_arch.sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=30\n",
    "val_batch_size=30\n",
    "microbatch=10\n",
    "epochs=30_000\n",
    "eval_interval=10\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=2500\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=10275679\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data_dir='data'\n",
    "data_player_dir='data/with_player'\n",
    "comedies_data_dir='data/comedies_only'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 07:29:18.208833: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-10 07:29:18.208936: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-10 07:29:18.210990: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-10 07:29:18.225627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 07:29:20.905592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer('shakespeare', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30268, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['this is the very false gallop of verses why do you infect yourself with them?', 'and fair men call for grace. aaron will have his soul black like his face.'] ['peace, you dull fool! i found them on a tree.', 'o, here i lift this one hand up to heaven, and bow this feeble ruin to the earth if any power pities wretched tears, to that i call! what, wilt thou kneel with me? do, then, dear heart,']\n",
      "RAM used: 1398.54 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 32531\n",
      "})\n",
      "RAM used: 1417.53 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b897f2e854f49969b11c968c45855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/32531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 32531\n",
      "})\n",
      "### tokenized_datasets...example [2, 138, 121, 78, 476, 880, 9871, 94, 5833, 329, 144, 89, 2960, 955, 131, 266, 22, 3]\n",
      "RAM used: 1459.36 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32cbc4fba9244f29c3bef6f88a2f6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/32531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 1493.67 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414850df6cb741068142e489bad7c614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/32531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 32531\n",
      "}) padded dataset\n",
      "RAM used: 1562.07 MB\n",
      "RAM used: 1562.09 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"kind o' thing than a fool and yet i would not be thee, nuncle, thou hast pared thy wit o' both sides, and left nothing i' the middle here comes one o' the parings.\", 'yes, by saint patrick, but there is, horatio, and much offence too. touching this vision here, it is an honest ghost, that let me tell you for your desire to know what is between us,'] [\"how now, daughter! what makes that frontlet on? methinks you are too much of late i' the frown.\", \"o'ermaster 't as you may. and now, good friends, as you are friends, scholars and soldiers, give me one poor request.\"]\n",
      "RAM used: 1528.05 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 8123\n",
      "})\n",
      "RAM used: 1524.35 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682da1ad56044ea7be28d2692a131620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/8123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 8123\n",
      "})\n",
      "### tokenized_datasets...example [2, 714, 37, 9, 749, 243, 23, 550, 85, 317, 31, 241, 125, 100, 208, 10, 6206, 10, 136, 469, 23187, 180, 646, 37, 9, 579, 3437, 10, 85, 1044, 549, 31, 9, 78, 5789, 237, 571, 295, 37, 9, 78, 14784, 12, 3]\n",
      "RAM used: 1534.86 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb9b449bd5d4af5a33118dc9e787307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/8123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 1551.21 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eef7bcebb849049361db0bc59c14ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/8123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 8123\n",
      "}) padded dataset\n",
      "RAM used: 1560.91 MB\n",
      "RAM used: 1560.91 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=val_batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight, # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192508"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1883bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import BertConfig\n",
    "\n",
    "# config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# for layer in model.input_transformers.layer[-1:]:\n",
    "#     for module in layer.modules():\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#             if module.padding_idx is not None:\n",
    "#                 module.weight.data[module.padding_idx].zero_()\n",
    "#         elif isinstance(module, nn.LayerNorm):\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 0.0001\n",
      "name: lm_head.bias, lr: 0.0001\n",
      "name: time_embed.0.weight, lr: 0.0001\n",
      "name: time_embed.0.bias, lr: 0.0001\n",
      "name: time_embed.2.weight, lr: 0.0001\n",
      "name: time_embed.2.bias, lr: 0.0001\n",
      "name: input_up_proj.0.weight, lr: 0.0001\n",
      "name: input_up_proj.0.bias, lr: 0.0001\n",
      "name: input_up_proj.2.weight, lr: 0.0001\n",
      "name: input_up_proj.2.bias, lr: 0.0001\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.00035407061614721485\n",
      "name: position_embeddings.weight, lr: 0.0003934117957191276\n",
      "name: LayerNorm.weight, lr: 0.0003934117957191276\n",
      "name: LayerNorm.bias, lr: 0.0003934117957191276\n",
      "name: output_down_proj.0.weight, lr: 0.0003934117957191276\n",
      "name: output_down_proj.0.bias, lr: 0.0003934117957191276\n",
      "name: output_down_proj.2.weight, lr: 0.0003934117957191276\n",
      "name: output_down_proj.2.bias, lr: 0.0003934117957191276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n",
      "Epoch 0/30000 Training Loss: 1.003659725189209\n",
      "Epoch 0/30000 Validation Loss: 1.0104681253433228\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=1.0104681253433228<=============\n",
      "Epoch 1/30000 Training Loss: 0.9923286437988281\n",
      "Epoch 2/30000 Training Loss: 1.0045844316482544\n",
      "Epoch 3/30000 Training Loss: 1.002435564994812\n",
      "Epoch 4/30000 Training Loss: 0.9960399270057678\n",
      "Epoch 5/30000 Training Loss: 0.9771733283996582\n",
      "Epoch 6/30000 Training Loss: 0.9623767733573914\n",
      "Epoch 7/30000 Training Loss: 0.9455592632293701\n",
      "Epoch 8/30000 Training Loss: 0.9263091087341309\n",
      "Epoch 9/30000 Training Loss: 0.9202634692192078\n",
      "Epoch 10/30000 Training Loss: 0.9158298373222351\n",
      "Epoch 10/30000 Validation Loss: 0.8674793839454651\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.8674793839454651<=============\n",
      "Epoch 11/30000 Training Loss: 0.8744320869445801\n",
      "Epoch 12/30000 Training Loss: 0.8744338154792786\n",
      "Epoch 13/30000 Training Loss: 0.8166527152061462\n",
      "Epoch 14/30000 Training Loss: 0.7972466349601746\n",
      "Epoch 15/30000 Training Loss: 0.7875547409057617\n",
      "Epoch 16/30000 Training Loss: 0.7519092559814453\n",
      "Epoch 17/30000 Training Loss: 0.7695183753967285\n",
      "Epoch 18/30000 Training Loss: 0.7773008942604065\n",
      "Epoch 19/30000 Training Loss: 0.7452135682106018\n",
      "Epoch 20/30000 Training Loss: 0.6956513524055481\n",
      "Epoch 20/30000 Validation Loss: 0.7167651057243347\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.7167651057243347<=============\n",
      "Epoch 21/30000 Training Loss: 0.7318603992462158\n",
      "Epoch 22/30000 Training Loss: 0.7122709155082703\n",
      "Epoch 23/30000 Training Loss: 0.668807327747345\n",
      "Epoch 24/30000 Training Loss: 0.6017162799835205\n",
      "Epoch 25/30000 Training Loss: 0.6440193057060242\n",
      "Epoch 26/30000 Training Loss: 0.6604313254356384\n",
      "Epoch 27/30000 Training Loss: 0.6321144104003906\n",
      "Epoch 28/30000 Training Loss: 0.6460342407226562\n",
      "Epoch 29/30000 Training Loss: 0.6164522171020508\n",
      "Epoch 30/30000 Training Loss: 0.6454841494560242\n",
      "Epoch 30/30000 Validation Loss: 0.6658620834350586\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.6658620834350586<=============\n",
      "Epoch 31/30000 Training Loss: 0.6107885837554932\n",
      "Epoch 32/30000 Training Loss: 0.6301366686820984\n",
      "Epoch 33/30000 Training Loss: 0.5940465927124023\n",
      "Epoch 34/30000 Training Loss: 0.6095762848854065\n",
      "Epoch 35/30000 Training Loss: 0.5440738201141357\n",
      "Epoch 36/30000 Training Loss: 0.6313301920890808\n",
      "Epoch 37/30000 Training Loss: 0.6226759552955627\n",
      "Epoch 38/30000 Training Loss: 0.5890327095985413\n",
      "Epoch 39/30000 Training Loss: 0.7006874680519104\n",
      "Epoch 40/30000 Training Loss: 0.6896891593933105\n",
      "Epoch 40/30000 Validation Loss: 0.6207982301712036\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.6207982301712036<=============\n",
      "Epoch 41/30000 Training Loss: 0.6095404028892517\n",
      "Epoch 42/30000 Training Loss: 0.6291512846946716\n",
      "Epoch 43/30000 Training Loss: 0.6175539493560791\n",
      "Epoch 44/30000 Training Loss: 0.6376139521598816\n",
      "Epoch 45/30000 Training Loss: 0.6456348299980164\n",
      "Epoch 46/30000 Training Loss: 0.6530457139015198\n",
      "Epoch 47/30000 Training Loss: 0.7367545962333679\n",
      "Epoch 48/30000 Training Loss: 0.6894639134407043\n",
      "Epoch 49/30000 Training Loss: 0.6057513356208801\n",
      "Epoch 50/30000 Training Loss: 0.6674239039421082\n",
      "Epoch 50/30000 Validation Loss: 0.629429817199707\n",
      "Epoch 51/30000 Training Loss: 0.6597661972045898\n",
      "Epoch 52/30000 Training Loss: 0.6260090470314026\n",
      "Epoch 53/30000 Training Loss: 0.5861883163452148\n",
      "Epoch 54/30000 Training Loss: 0.6278490424156189\n",
      "Epoch 55/30000 Training Loss: 0.5881878137588501\n",
      "Epoch 56/30000 Training Loss: 0.6224145293235779\n",
      "Epoch 57/30000 Training Loss: 0.5851566195487976\n",
      "Epoch 58/30000 Training Loss: 0.5293300151824951\n",
      "Epoch 59/30000 Training Loss: 0.5260130763053894\n",
      "Epoch 60/30000 Training Loss: 0.5489299297332764\n",
      "Epoch 60/30000 Validation Loss: 0.5225740075111389\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.5225740075111389<=============\n",
      "Epoch 61/30000 Training Loss: 0.5500667095184326\n",
      "Epoch 62/30000 Training Loss: 0.524355411529541\n",
      "Epoch 63/30000 Training Loss: 0.5039172172546387\n",
      "Epoch 64/30000 Training Loss: 0.5716431140899658\n",
      "Epoch 65/30000 Training Loss: 0.4979747533798218\n",
      "Epoch 66/30000 Training Loss: 0.5216593146324158\n",
      "Epoch 67/30000 Training Loss: 0.45912039279937744\n",
      "Epoch 68/30000 Training Loss: 0.48703718185424805\n",
      "Epoch 69/30000 Training Loss: 0.4602310359477997\n",
      "Epoch 70/30000 Training Loss: 0.44303908944129944\n",
      "Epoch 70/30000 Validation Loss: 0.5101564526557922\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.5101564526557922<=============\n",
      "Epoch 71/30000 Training Loss: 0.5120805501937866\n",
      "Epoch 72/30000 Training Loss: 0.44557449221611023\n",
      "Epoch 73/30000 Training Loss: 0.4232054650783539\n",
      "Epoch 74/30000 Training Loss: 0.5274438858032227\n",
      "Epoch 75/30000 Training Loss: 0.4699765741825104\n",
      "Epoch 76/30000 Training Loss: 0.43624481558799744\n",
      "Epoch 77/30000 Training Loss: 0.47216901183128357\n",
      "Epoch 78/30000 Training Loss: 0.45461180806159973\n",
      "Epoch 79/30000 Training Loss: 0.43093863129615784\n",
      "Epoch 80/30000 Training Loss: 0.44960618019104004\n",
      "Epoch 80/30000 Validation Loss: 0.3881840407848358\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.3881840407848358<=============\n",
      "Epoch 81/30000 Training Loss: 0.5017151236534119\n",
      "Epoch 82/30000 Training Loss: 0.44000115990638733\n",
      "Epoch 83/30000 Training Loss: 0.44191300868988037\n",
      "Epoch 84/30000 Training Loss: 0.3645440638065338\n",
      "Epoch 85/30000 Training Loss: 0.4389977753162384\n",
      "Epoch 86/30000 Training Loss: 0.400911808013916\n",
      "Epoch 87/30000 Training Loss: 0.4012831747531891\n",
      "Epoch 88/30000 Training Loss: 0.46652936935424805\n",
      "Epoch 89/30000 Training Loss: 0.40473970770835876\n",
      "Epoch 90/30000 Training Loss: 0.40519559383392334\n",
      "Epoch 90/30000 Validation Loss: 0.4587728679180145\n",
      "Epoch 91/30000 Training Loss: 0.43838486075401306\n",
      "Epoch 92/30000 Training Loss: 0.4154057502746582\n",
      "Epoch 93/30000 Training Loss: 0.423492431640625\n",
      "Epoch 94/30000 Training Loss: 0.4401439428329468\n",
      "Epoch 95/30000 Training Loss: 0.41994261741638184\n",
      "Epoch 96/30000 Training Loss: 0.47367358207702637\n",
      "Epoch 97/30000 Training Loss: 0.4881582260131836\n",
      "Epoch 98/30000 Training Loss: 0.4732280671596527\n",
      "Epoch 99/30000 Training Loss: 0.40814366936683655\n",
      "Epoch 100/30000 Training Loss: 0.40929996967315674\n",
      "Epoch 100/30000 Validation Loss: 0.4328542649745941\n",
      "Epoch 101/30000 Training Loss: 0.4228823184967041\n",
      "Epoch 102/30000 Training Loss: 0.4310922920703888\n",
      "Epoch 103/30000 Training Loss: 0.3863190710544586\n",
      "Epoch 104/30000 Training Loss: 0.4335886538028717\n",
      "Epoch 105/30000 Training Loss: 0.4614836275577545\n",
      "Epoch 106/30000 Training Loss: 0.4041287899017334\n",
      "Epoch 107/30000 Training Loss: 0.4163327217102051\n",
      "Epoch 108/30000 Training Loss: 0.42685842514038086\n",
      "Epoch 109/30000 Training Loss: 0.3643902838230133\n",
      "Epoch 110/30000 Training Loss: 0.38830724358558655\n",
      "Epoch 110/30000 Validation Loss: 0.42654475569725037\n",
      "Epoch 111/30000 Training Loss: 0.3902985751628876\n",
      "Epoch 112/30000 Training Loss: 0.4035455882549286\n",
      "Epoch 113/30000 Training Loss: 0.36373841762542725\n",
      "Epoch 114/30000 Training Loss: 0.33761098980903625\n",
      "Epoch 115/30000 Training Loss: 0.3902650773525238\n",
      "Epoch 116/30000 Training Loss: 0.377559632062912\n",
      "Epoch 117/30000 Training Loss: 0.3665495216846466\n",
      "Epoch 118/30000 Training Loss: 0.4099412262439728\n",
      "Epoch 119/30000 Training Loss: 0.3970388174057007\n",
      "Epoch 120/30000 Training Loss: 0.3576047122478485\n",
      "Epoch 120/30000 Validation Loss: 0.3447512090206146\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.3447512090206146<=============\n",
      "Epoch 121/30000 Training Loss: 0.3898656368255615\n",
      "Epoch 122/30000 Training Loss: 0.3816545903682709\n",
      "Epoch 123/30000 Training Loss: 0.4168972969055176\n",
      "Epoch 124/30000 Training Loss: 0.3527604043483734\n",
      "Epoch 125/30000 Training Loss: 0.3873178958892822\n",
      "Epoch 126/30000 Training Loss: 0.32392188906669617\n",
      "Epoch 127/30000 Training Loss: 0.38441887497901917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/30000 Training Loss: 0.345733642578125\n",
      "Epoch 129/30000 Training Loss: 0.38116654753685\n",
      "Epoch 130/30000 Training Loss: 0.3444702625274658\n",
      "Epoch 130/30000 Validation Loss: 0.4096357524394989\n",
      "Epoch 131/30000 Training Loss: 0.435497522354126\n",
      "Epoch 132/30000 Training Loss: 0.3981006145477295\n",
      "Epoch 133/30000 Training Loss: 0.406393438577652\n",
      "Epoch 134/30000 Training Loss: 0.3865523338317871\n",
      "Epoch 135/30000 Training Loss: 0.3896009027957916\n",
      "Epoch 136/30000 Training Loss: 0.37286362051963806\n",
      "Epoch 137/30000 Training Loss: 0.4284806549549103\n",
      "Epoch 138/30000 Training Loss: 0.36519357562065125\n",
      "Epoch 139/30000 Training Loss: 0.35374295711517334\n",
      "Epoch 140/30000 Training Loss: 0.4343454837799072\n",
      "Epoch 140/30000 Validation Loss: 0.37261509895324707\n",
      "Epoch 141/30000 Training Loss: 0.3805222511291504\n",
      "Epoch 142/30000 Training Loss: 0.37285280227661133\n",
      "Epoch 143/30000 Training Loss: 0.35261037945747375\n",
      "Epoch 144/30000 Training Loss: 0.3146049976348877\n",
      "Epoch 145/30000 Training Loss: 0.36741721630096436\n",
      "Epoch 146/30000 Training Loss: 0.31987056136131287\n",
      "Epoch 147/30000 Training Loss: 0.35756421089172363\n",
      "Epoch 148/30000 Training Loss: 0.3431839048862457\n",
      "Epoch 149/30000 Training Loss: 0.3112778961658478\n",
      "Epoch 150/30000 Training Loss: 0.3478999137878418\n",
      "Epoch 150/30000 Validation Loss: 0.34727755188941956\n",
      "Epoch 151/30000 Training Loss: 0.3518329858779907\n",
      "Epoch 152/30000 Training Loss: 0.3383745849132538\n",
      "Epoch 153/30000 Training Loss: 0.3675438463687897\n",
      "Epoch 154/30000 Training Loss: 0.34677115082740784\n",
      "Epoch 155/30000 Training Loss: 0.3088984191417694\n",
      "Epoch 156/30000 Training Loss: 0.33208155632019043\n",
      "Epoch 157/30000 Training Loss: 0.3294609785079956\n",
      "Epoch 158/30000 Training Loss: 0.38315674662590027\n",
      "Epoch 159/30000 Training Loss: 0.39366888999938965\n",
      "Epoch 160/30000 Training Loss: 0.3439415395259857\n",
      "Epoch 160/30000 Validation Loss: 0.30947256088256836\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.30947256088256836<=============\n",
      "Epoch 161/30000 Training Loss: 0.2993810176849365\n",
      "Epoch 162/30000 Training Loss: 0.35302114486694336\n",
      "Epoch 163/30000 Training Loss: 0.3161962330341339\n",
      "Epoch 164/30000 Training Loss: 0.3084297180175781\n",
      "Epoch 165/30000 Training Loss: 0.30658456683158875\n",
      "Epoch 166/30000 Training Loss: 0.29900896549224854\n",
      "Epoch 167/30000 Training Loss: 0.2928478419780731\n",
      "Epoch 168/30000 Training Loss: 0.2796604335308075\n",
      "Epoch 169/30000 Training Loss: 0.33345475792884827\n",
      "Epoch 170/30000 Training Loss: 0.31430432200431824\n",
      "Epoch 170/30000 Validation Loss: 0.30960679054260254\n",
      "Epoch 171/30000 Training Loss: 0.3030901253223419\n",
      "Epoch 172/30000 Training Loss: 0.32890042662620544\n",
      "Epoch 173/30000 Training Loss: 0.3003907799720764\n",
      "Epoch 174/30000 Training Loss: 0.29869595170021057\n",
      "Epoch 175/30000 Training Loss: 0.31589722633361816\n",
      "Epoch 176/30000 Training Loss: 0.2911301851272583\n",
      "Epoch 177/30000 Training Loss: 0.2868638038635254\n",
      "Epoch 178/30000 Training Loss: 0.25694456696510315\n",
      "Epoch 179/30000 Training Loss: 0.2751942574977875\n",
      "Epoch 180/30000 Training Loss: 0.31338801980018616\n",
      "Epoch 180/30000 Validation Loss: 0.31984224915504456\n",
      "Epoch 181/30000 Training Loss: 0.304593563079834\n",
      "Epoch 182/30000 Training Loss: 0.2723609507083893\n",
      "Epoch 183/30000 Training Loss: 0.3067977726459503\n",
      "Epoch 184/30000 Training Loss: 0.30639198422431946\n",
      "Epoch 185/30000 Training Loss: 0.30606138706207275\n",
      "Epoch 186/30000 Training Loss: 0.32144105434417725\n",
      "Epoch 187/30000 Training Loss: 0.310070276260376\n",
      "Epoch 188/30000 Training Loss: 0.268169105052948\n",
      "Epoch 189/30000 Training Loss: 0.2576553523540497\n",
      "Epoch 190/30000 Training Loss: 0.25170302391052246\n",
      "Epoch 190/30000 Validation Loss: 0.28584620356559753\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.28584620356559753<=============\n",
      "Epoch 191/30000 Training Loss: 0.29232606291770935\n",
      "Epoch 192/30000 Training Loss: 0.3202263116836548\n",
      "Epoch 193/30000 Training Loss: 0.27649667859077454\n",
      "Epoch 194/30000 Training Loss: 0.2890069782733917\n",
      "Epoch 195/30000 Training Loss: 0.2972046136856079\n",
      "Epoch 196/30000 Training Loss: 0.28113695979118347\n",
      "Epoch 197/30000 Training Loss: 0.2736622393131256\n",
      "Epoch 198/30000 Training Loss: 0.2425897866487503\n",
      "Epoch 199/30000 Training Loss: 0.297952800989151\n",
      "Epoch 200/30000 Training Loss: 0.2635333240032196\n",
      "Epoch 200/30000 Validation Loss: 0.2442312240600586\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.2442312240600586<=============\n",
      "Epoch 201/30000 Training Loss: 0.25809788703918457\n",
      "Epoch 202/30000 Training Loss: 0.2872805595397949\n",
      "Epoch 203/30000 Training Loss: 0.22851872444152832\n",
      "Epoch 204/30000 Training Loss: 0.28239694237709045\n",
      "Epoch 205/30000 Training Loss: 0.2663497030735016\n",
      "Epoch 206/30000 Training Loss: 0.27191710472106934\n",
      "Epoch 207/30000 Training Loss: 0.2475416660308838\n",
      "Epoch 208/30000 Training Loss: 0.29740265011787415\n",
      "Epoch 209/30000 Training Loss: 0.24406050145626068\n",
      "Epoch 210/30000 Training Loss: 0.2851231098175049\n",
      "Epoch 210/30000 Validation Loss: 0.2904775142669678\n",
      "Epoch 211/30000 Training Loss: 0.25736820697784424\n",
      "Epoch 212/30000 Training Loss: 0.2532029151916504\n",
      "Epoch 213/30000 Training Loss: 0.2752844989299774\n",
      "Epoch 214/30000 Training Loss: 0.23665101826190948\n",
      "Epoch 215/30000 Training Loss: 0.27948451042175293\n",
      "Epoch 216/30000 Training Loss: 0.24215500056743622\n",
      "Epoch 217/30000 Training Loss: 0.25782278180122375\n",
      "Epoch 218/30000 Training Loss: 0.28208687901496887\n",
      "Epoch 219/30000 Training Loss: 0.25728386640548706\n",
      "Epoch 220/30000 Training Loss: 0.25408852100372314\n",
      "Epoch 220/30000 Validation Loss: 0.3022176921367645\n",
      "Epoch 221/30000 Training Loss: 0.23974394798278809\n",
      "Epoch 222/30000 Training Loss: 0.26725780963897705\n",
      "Epoch 223/30000 Training Loss: 0.27450135350227356\n",
      "Epoch 224/30000 Training Loss: 0.25388988852500916\n",
      "Epoch 225/30000 Training Loss: 0.27803388237953186\n",
      "Epoch 226/30000 Training Loss: 0.24015377461910248\n",
      "Epoch 227/30000 Training Loss: 0.25226566195487976\n",
      "Epoch 228/30000 Training Loss: 0.27337920665740967\n",
      "Epoch 229/30000 Training Loss: 0.2343921661376953\n",
      "Epoch 230/30000 Training Loss: 0.24513880908489227\n",
      "Epoch 230/30000 Validation Loss: 0.2640407979488373\n",
      "Epoch 231/30000 Training Loss: 0.25902092456817627\n",
      "Epoch 232/30000 Training Loss: 0.2617711126804352\n",
      "Epoch 233/30000 Training Loss: 0.2433064728975296\n",
      "Epoch 234/30000 Training Loss: 0.2378143072128296\n",
      "Epoch 235/30000 Training Loss: 0.20223820209503174\n",
      "Epoch 236/30000 Training Loss: 0.2169068455696106\n",
      "Epoch 237/30000 Training Loss: 0.2532304525375366\n",
      "Epoch 238/30000 Training Loss: 0.27933427691459656\n",
      "Epoch 239/30000 Training Loss: 0.24147112667560577\n",
      "Epoch 240/30000 Training Loss: 0.2461230754852295\n",
      "Epoch 240/30000 Validation Loss: 0.23869609832763672\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.23869609832763672<=============\n",
      "Epoch 241/30000 Training Loss: 0.23544372618198395\n",
      "Epoch 242/30000 Training Loss: 0.27241992950439453\n",
      "Epoch 243/30000 Training Loss: 0.24004565179347992\n",
      "Epoch 244/30000 Training Loss: 0.21685533225536346\n",
      "Epoch 245/30000 Training Loss: 0.23943985998630524\n",
      "Epoch 246/30000 Training Loss: 0.2681930959224701\n",
      "Epoch 247/30000 Training Loss: 0.22970449924468994\n",
      "Epoch 248/30000 Training Loss: 0.21456579864025116\n",
      "Epoch 249/30000 Training Loss: 0.25028812885284424\n",
      "Epoch 250/30000 Training Loss: 0.23710019886493683\n",
      "Epoch 250/30000 Validation Loss: 0.2141994684934616\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.2141994684934616<=============\n",
      "Epoch 251/30000 Training Loss: 0.20437437295913696\n",
      "Epoch 252/30000 Training Loss: 0.2413395643234253\n",
      "Epoch 253/30000 Training Loss: 0.2407398372888565\n",
      "Epoch 254/30000 Training Loss: 0.24544699490070343\n",
      "Epoch 255/30000 Training Loss: 0.23174548149108887\n",
      "Epoch 256/30000 Training Loss: 0.2261737436056137\n",
      "Epoch 257/30000 Training Loss: 0.19833314418792725\n",
      "Epoch 258/30000 Training Loss: 0.2589128911495209\n",
      "Epoch 259/30000 Training Loss: 0.19966916739940643\n",
      "Epoch 260/30000 Training Loss: 0.22113782167434692\n",
      "Epoch 260/30000 Validation Loss: 0.22599788010120392\n",
      "Epoch 261/30000 Training Loss: 0.24071930348873138\n",
      "Epoch 262/30000 Training Loss: 0.25950437784194946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 263/30000 Training Loss: 0.23279626667499542\n",
      "Epoch 264/30000 Training Loss: 0.23650677502155304\n",
      "Epoch 265/30000 Training Loss: 0.22189448773860931\n",
      "Epoch 266/30000 Training Loss: 0.24423235654830933\n",
      "Epoch 267/30000 Training Loss: 0.23631392419338226\n",
      "Epoch 268/30000 Training Loss: 0.21535147726535797\n",
      "Epoch 269/30000 Training Loss: 0.22471962869167328\n",
      "Epoch 270/30000 Training Loss: 0.20850960910320282\n",
      "Epoch 270/30000 Validation Loss: 0.21432721614837646\n",
      "Epoch 271/30000 Training Loss: 0.19839288294315338\n",
      "Epoch 272/30000 Training Loss: 0.20996077358722687\n",
      "Epoch 273/30000 Training Loss: 0.21007150411605835\n",
      "Epoch 274/30000 Training Loss: 0.20856647193431854\n",
      "Epoch 275/30000 Training Loss: 0.19835250079631805\n",
      "Epoch 276/30000 Training Loss: 0.21460212767124176\n",
      "Epoch 277/30000 Training Loss: 0.2239631861448288\n",
      "Epoch 278/30000 Training Loss: 0.23002423346042633\n",
      "Epoch 279/30000 Training Loss: 0.23247285187244415\n",
      "Epoch 280/30000 Training Loss: 0.20708304643630981\n",
      "Epoch 280/30000 Validation Loss: 0.20758436620235443\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.20758436620235443<=============\n",
      "Epoch 281/30000 Training Loss: 0.2329363375902176\n",
      "Epoch 282/30000 Training Loss: 0.2263413518667221\n",
      "Epoch 283/30000 Training Loss: 0.240020751953125\n",
      "Epoch 284/30000 Training Loss: 0.191497340798378\n",
      "Epoch 285/30000 Training Loss: 0.20991696417331696\n",
      "Epoch 286/30000 Training Loss: 0.20756976306438446\n",
      "Epoch 287/30000 Training Loss: 0.17951816320419312\n",
      "Epoch 288/30000 Training Loss: 0.20760713517665863\n",
      "Epoch 289/30000 Training Loss: 0.2313329130411148\n",
      "Epoch 290/30000 Training Loss: 0.19521717727184296\n",
      "Epoch 290/30000 Validation Loss: 0.22324152290821075\n",
      "Epoch 291/30000 Training Loss: 0.2019532173871994\n",
      "Epoch 292/30000 Training Loss: 0.21110932528972626\n",
      "Epoch 293/30000 Training Loss: 0.22718572616577148\n",
      "Epoch 294/30000 Training Loss: 0.22094188630580902\n",
      "Epoch 295/30000 Training Loss: 0.2221958488225937\n",
      "Epoch 296/30000 Training Loss: 0.2127741575241089\n",
      "Epoch 297/30000 Training Loss: 0.2434348464012146\n",
      "Epoch 298/30000 Training Loss: 0.20563191175460815\n",
      "Epoch 299/30000 Training Loss: 0.21021366119384766\n",
      "Epoch 300/30000 Training Loss: 0.20471026003360748\n",
      "Epoch 300/30000 Validation Loss: 0.22129756212234497\n",
      "Epoch 301/30000 Training Loss: 0.1869741678237915\n",
      "Epoch 302/30000 Training Loss: 0.23539607226848602\n",
      "Epoch 303/30000 Training Loss: 0.1979445368051529\n",
      "Epoch 304/30000 Training Loss: 0.20257537066936493\n",
      "Epoch 305/30000 Training Loss: 0.20013974606990814\n",
      "Epoch 306/30000 Training Loss: 0.19132430851459503\n",
      "Epoch 307/30000 Training Loss: 0.1794699877500534\n",
      "Epoch 308/30000 Training Loss: 0.2094370573759079\n",
      "Epoch 309/30000 Training Loss: 0.20090390741825104\n",
      "Epoch 310/30000 Training Loss: 0.21306204795837402\n",
      "Epoch 310/30000 Validation Loss: 0.23112697899341583\n",
      "Epoch 311/30000 Training Loss: 0.20549851655960083\n",
      "Epoch 312/30000 Training Loss: 0.2552932798862457\n",
      "Epoch 313/30000 Training Loss: 0.21841184794902802\n",
      "Epoch 314/30000 Training Loss: 0.19802184402942657\n",
      "Epoch 315/30000 Training Loss: 0.17300313711166382\n",
      "Epoch 316/30000 Training Loss: 0.21648262441158295\n",
      "Epoch 317/30000 Training Loss: 0.22135138511657715\n",
      "Epoch 318/30000 Training Loss: 0.19034768640995026\n",
      "Epoch 319/30000 Training Loss: 0.21184055507183075\n",
      "Epoch 320/30000 Training Loss: 0.20806629955768585\n",
      "Epoch 320/30000 Validation Loss: 0.1926184892654419\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.1926184892654419<=============\n",
      "Epoch 321/30000 Training Loss: 0.2030820995569229\n",
      "Epoch 322/30000 Training Loss: 0.1939994841814041\n",
      "Epoch 323/30000 Training Loss: 0.2195528894662857\n",
      "Epoch 324/30000 Training Loss: 0.19891589879989624\n",
      "Epoch 325/30000 Training Loss: 0.2198260873556137\n",
      "Epoch 326/30000 Training Loss: 0.18509523570537567\n",
      "Epoch 327/30000 Training Loss: 0.19648747146129608\n",
      "Epoch 328/30000 Training Loss: 0.21517090499401093\n",
      "Epoch 329/30000 Training Loss: 0.17541223764419556\n",
      "Epoch 330/30000 Training Loss: 0.18690602481365204\n",
      "Epoch 330/30000 Validation Loss: 0.1662539690732956\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.1662539690732956<=============\n",
      "Epoch 331/30000 Training Loss: 0.23069514334201813\n",
      "Epoch 332/30000 Training Loss: 0.1963137835264206\n",
      "Epoch 333/30000 Training Loss: 0.21289999783039093\n",
      "Epoch 334/30000 Training Loss: 0.1775123029947281\n",
      "Epoch 335/30000 Training Loss: 0.18418651819229126\n",
      "Epoch 336/30000 Training Loss: 0.20436787605285645\n",
      "Epoch 337/30000 Training Loss: 0.17632727324962616\n",
      "Epoch 338/30000 Training Loss: 0.22304274141788483\n",
      "Epoch 339/30000 Training Loss: 0.16376172006130219\n",
      "Epoch 340/30000 Training Loss: 0.18284088373184204\n",
      "Epoch 340/30000 Validation Loss: 0.204181507229805\n",
      "Epoch 341/30000 Training Loss: 0.1607481986284256\n",
      "Epoch 342/30000 Training Loss: 0.16593067348003387\n",
      "Epoch 343/30000 Training Loss: 0.18572689592838287\n",
      "Epoch 344/30000 Training Loss: 0.16158463060855865\n",
      "Epoch 345/30000 Training Loss: 0.21184088289737701\n",
      "Epoch 346/30000 Training Loss: 0.17901988327503204\n",
      "Epoch 347/30000 Training Loss: 0.21088916063308716\n",
      "Epoch 348/30000 Training Loss: 0.16802187263965607\n",
      "Epoch 349/30000 Training Loss: 0.16927166283130646\n",
      "Epoch 350/30000 Training Loss: 0.18812964856624603\n",
      "Epoch 350/30000 Validation Loss: 0.17761553823947906\n",
      "Epoch 351/30000 Training Loss: 0.21229691803455353\n",
      "Epoch 352/30000 Training Loss: 0.16153188049793243\n",
      "Epoch 353/30000 Training Loss: 0.17094682157039642\n",
      "Epoch 354/30000 Training Loss: 0.19970373809337616\n",
      "Epoch 355/30000 Training Loss: 0.17680978775024414\n",
      "Epoch 356/30000 Training Loss: 0.1815086156129837\n",
      "Epoch 357/30000 Training Loss: 0.21439583599567413\n",
      "Epoch 358/30000 Training Loss: 0.19112657010555267\n",
      "Epoch 359/30000 Training Loss: 0.16769151389598846\n",
      "Epoch 360/30000 Training Loss: 0.17624777555465698\n",
      "Epoch 360/30000 Validation Loss: 0.18782848119735718\n",
      "Epoch 361/30000 Training Loss: 0.1631534844636917\n",
      "Epoch 362/30000 Training Loss: 0.20816534757614136\n",
      "Epoch 363/30000 Training Loss: 0.1398676186800003\n",
      "Epoch 364/30000 Training Loss: 0.19815291464328766\n",
      "Epoch 365/30000 Training Loss: 0.18552017211914062\n",
      "Epoch 366/30000 Training Loss: 0.19394762814044952\n",
      "Epoch 367/30000 Training Loss: 0.18965083360671997\n",
      "Epoch 368/30000 Training Loss: 0.1962532252073288\n",
      "Epoch 369/30000 Training Loss: 0.1812976449728012\n",
      "Epoch 370/30000 Training Loss: 0.17403198778629303\n",
      "Epoch 370/30000 Validation Loss: 0.2364695817232132\n",
      "Epoch 371/30000 Training Loss: 0.17825154960155487\n",
      "Epoch 372/30000 Training Loss: 0.19113309681415558\n",
      "Epoch 373/30000 Training Loss: 0.1824568510055542\n",
      "Epoch 374/30000 Training Loss: 0.17879484593868256\n",
      "Epoch 375/30000 Training Loss: 0.17678974568843842\n",
      "Epoch 376/30000 Training Loss: 0.17797499895095825\n",
      "Epoch 377/30000 Training Loss: 0.20536577701568604\n",
      "Epoch 378/30000 Training Loss: 0.1904846876859665\n",
      "Epoch 379/30000 Training Loss: 0.19658802449703217\n",
      "Epoch 380/30000 Training Loss: 0.1813606470823288\n",
      "Epoch 380/30000 Validation Loss: 0.17313599586486816\n",
      "Epoch 381/30000 Training Loss: 0.19349300861358643\n",
      "Epoch 382/30000 Training Loss: 0.1707286387681961\n",
      "Epoch 383/30000 Training Loss: 0.17555491626262665\n",
      "Epoch 384/30000 Training Loss: 0.19835956394672394\n",
      "Epoch 385/30000 Training Loss: 0.14839766919612885\n",
      "Epoch 386/30000 Training Loss: 0.17899245023727417\n",
      "Epoch 387/30000 Training Loss: 0.17793035507202148\n",
      "Epoch 388/30000 Training Loss: 0.16389012336730957\n",
      "Epoch 389/30000 Training Loss: 0.18054334819316864\n",
      "Epoch 390/30000 Training Loss: 0.18968047201633453\n",
      "Epoch 390/30000 Validation Loss: 0.19010670483112335\n",
      "Epoch 391/30000 Training Loss: 0.1641562432050705\n",
      "Epoch 392/30000 Training Loss: 0.18317008018493652\n",
      "Epoch 393/30000 Training Loss: 0.1903904527425766\n",
      "Epoch 394/30000 Training Loss: 0.19664065539836884\n",
      "Epoch 395/30000 Training Loss: 0.16167095303535461\n",
      "Epoch 396/30000 Training Loss: 0.2074655294418335\n",
      "Epoch 397/30000 Training Loss: 0.19174803793430328\n",
      "Epoch 398/30000 Training Loss: 0.1766345351934433\n",
      "Epoch 399/30000 Training Loss: 0.20223014056682587\n",
      "Epoch 400/30000 Training Loss: 0.16385973989963531\n",
      "Epoch 400/30000 Validation Loss: 0.16373267769813538\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.16373267769813538<=============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/30000 Training Loss: 0.21176010370254517\n",
      "Epoch 402/30000 Training Loss: 0.19729918241500854\n",
      "Epoch 403/30000 Training Loss: 0.21428458392620087\n",
      "Epoch 404/30000 Training Loss: 0.17617078125476837\n",
      "Epoch 405/30000 Training Loss: 0.16578342020511627\n",
      "Epoch 406/30000 Training Loss: 0.17628608644008636\n",
      "Epoch 407/30000 Training Loss: 0.1341099590063095\n",
      "Epoch 408/30000 Training Loss: 0.20161576569080353\n",
      "Epoch 409/30000 Training Loss: 0.15495429933071136\n",
      "Epoch 410/30000 Training Loss: 0.16808103024959564\n",
      "Epoch 410/30000 Validation Loss: 0.15759938955307007\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.15759938955307007<=============\n",
      "Epoch 411/30000 Training Loss: 0.1898844838142395\n",
      "Epoch 412/30000 Training Loss: 0.16599760949611664\n",
      "Epoch 413/30000 Training Loss: 0.1914958953857422\n",
      "Epoch 414/30000 Training Loss: 0.1489611268043518\n",
      "Epoch 415/30000 Training Loss: 0.15740834176540375\n",
      "Epoch 416/30000 Training Loss: 0.19764012098312378\n",
      "Epoch 417/30000 Training Loss: 0.17648597061634064\n",
      "Epoch 418/30000 Training Loss: 0.1810109168291092\n",
      "Epoch 419/30000 Training Loss: 0.1539304107427597\n",
      "Epoch 420/30000 Training Loss: 0.2044544816017151\n",
      "Epoch 420/30000 Validation Loss: 0.17951016128063202\n",
      "Epoch 421/30000 Training Loss: 0.17587924003601074\n",
      "Epoch 422/30000 Training Loss: 0.1978745460510254\n",
      "Epoch 423/30000 Training Loss: 0.1665220856666565\n",
      "Epoch 424/30000 Training Loss: 0.19370578229427338\n",
      "Epoch 425/30000 Training Loss: 0.19902198016643524\n",
      "Epoch 426/30000 Training Loss: 0.16122348606586456\n",
      "Epoch 427/30000 Training Loss: 0.16985034942626953\n",
      "Epoch 428/30000 Training Loss: 0.16914457082748413\n",
      "Epoch 429/30000 Training Loss: 0.18192417919635773\n",
      "Epoch 430/30000 Training Loss: 0.18543075025081635\n",
      "Epoch 430/30000 Validation Loss: 0.17641480267047882\n",
      "Epoch 431/30000 Training Loss: 0.13728155195713043\n",
      "Epoch 432/30000 Training Loss: 0.14776897430419922\n",
      "Epoch 433/30000 Training Loss: 0.1495305448770523\n",
      "Epoch 434/30000 Training Loss: 0.15516847372055054\n",
      "Epoch 435/30000 Training Loss: 0.18690089881420135\n",
      "Epoch 436/30000 Training Loss: 0.17883352935314178\n",
      "Epoch 437/30000 Training Loss: 0.16527387499809265\n",
      "Epoch 438/30000 Training Loss: 0.18255649507045746\n",
      "Epoch 439/30000 Training Loss: 0.17802117764949799\n",
      "Epoch 440/30000 Training Loss: 0.16573204100131989\n",
      "Epoch 440/30000 Validation Loss: 0.15895450115203857\n",
      "Epoch 441/30000 Training Loss: 0.17364561557769775\n",
      "Epoch 442/30000 Training Loss: 0.1731991320848465\n",
      "Epoch 443/30000 Training Loss: 0.18615387380123138\n",
      "Epoch 444/30000 Training Loss: 0.1538182497024536\n",
      "Epoch 445/30000 Training Loss: 0.17117364704608917\n",
      "Epoch 446/30000 Training Loss: 0.14882688224315643\n",
      "Epoch 447/30000 Training Loss: 0.15152232348918915\n",
      "Epoch 448/30000 Training Loss: 0.15494081377983093\n",
      "Epoch 449/30000 Training Loss: 0.17279557883739471\n",
      "Epoch 450/30000 Training Loss: 0.17821140587329865\n",
      "Epoch 450/30000 Validation Loss: 0.1890689879655838\n",
      "Epoch 451/30000 Training Loss: 0.20034575462341309\n",
      "Epoch 452/30000 Training Loss: 0.15982180833816528\n",
      "Epoch 453/30000 Training Loss: 0.17243318259716034\n",
      "Epoch 454/30000 Training Loss: 0.17959560453891754\n",
      "Epoch 455/30000 Training Loss: 0.1916925460100174\n",
      "Epoch 456/30000 Training Loss: 0.16622884571552277\n",
      "Epoch 457/30000 Training Loss: 0.18092061579227448\n",
      "Epoch 458/30000 Training Loss: 0.178155779838562\n",
      "Epoch 459/30000 Training Loss: 0.19068728387355804\n",
      "Epoch 460/30000 Training Loss: 0.15243586897850037\n",
      "Epoch 460/30000 Validation Loss: 0.16531206667423248\n",
      "Epoch 461/30000 Training Loss: 0.16072648763656616\n",
      "Epoch 462/30000 Training Loss: 0.1659528762102127\n",
      "Epoch 463/30000 Training Loss: 0.17147307097911835\n",
      "Epoch 464/30000 Training Loss: 0.13918311893939972\n",
      "Epoch 465/30000 Training Loss: 0.16892482340335846\n",
      "Epoch 466/30000 Training Loss: 0.1721523255109787\n",
      "Epoch 467/30000 Training Loss: 0.1587786227464676\n",
      "Epoch 468/30000 Training Loss: 0.16464269161224365\n",
      "Epoch 469/30000 Training Loss: 0.15080440044403076\n",
      "Epoch 470/30000 Training Loss: 0.1415480524301529\n",
      "Epoch 470/30000 Validation Loss: 0.17399896681308746\n",
      "Epoch 471/30000 Training Loss: 0.1692943572998047\n",
      "Epoch 472/30000 Training Loss: 0.15261679887771606\n",
      "Epoch 473/30000 Training Loss: 0.13513226807117462\n",
      "Epoch 474/30000 Training Loss: 0.16605015099048615\n",
      "Epoch 475/30000 Training Loss: 0.1657077521085739\n",
      "Epoch 476/30000 Training Loss: 0.17599229514598846\n",
      "Epoch 477/30000 Training Loss: 0.15393412113189697\n",
      "Epoch 478/30000 Training Loss: 0.15496046841144562\n",
      "Epoch 479/30000 Training Loss: 0.1442844718694687\n",
      "Epoch 480/30000 Training Loss: 0.12801562249660492\n",
      "Epoch 480/30000 Validation Loss: 0.15876762568950653\n",
      "Epoch 481/30000 Training Loss: 0.18424391746520996\n",
      "Epoch 482/30000 Training Loss: 0.1291978508234024\n",
      "Epoch 483/30000 Training Loss: 0.13339422643184662\n",
      "Epoch 484/30000 Training Loss: 0.11930448561906815\n",
      "Epoch 485/30000 Training Loss: 0.14548416435718536\n",
      "Epoch 486/30000 Training Loss: 0.15037329494953156\n",
      "Epoch 487/30000 Training Loss: 0.19436876475811005\n",
      "Epoch 488/30000 Training Loss: 0.14089754223823547\n",
      "Epoch 489/30000 Training Loss: 0.18016135692596436\n",
      "Epoch 490/30000 Training Loss: 0.17894108593463898\n",
      "Epoch 490/30000 Validation Loss: 0.138240247964859\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.138240247964859<=============\n",
      "Epoch 491/30000 Training Loss: 0.13399577140808105\n",
      "Epoch 492/30000 Training Loss: 0.14281350374221802\n",
      "Epoch 493/30000 Training Loss: 0.14199821650981903\n",
      "Epoch 494/30000 Training Loss: 0.16731782257556915\n",
      "Epoch 495/30000 Training Loss: 0.14811204373836517\n",
      "Epoch 496/30000 Training Loss: 0.13313442468643188\n",
      "Epoch 497/30000 Training Loss: 0.1409710794687271\n",
      "Epoch 498/30000 Training Loss: 0.12666712701320648\n",
      "Epoch 499/30000 Training Loss: 0.1805487722158432\n",
      "Epoch 500/30000 Training Loss: 0.16958121955394745\n",
      "Epoch 500/30000 Validation Loss: 0.15844076871871948\n",
      "Epoch 501/30000 Training Loss: 0.1484900563955307\n",
      "Epoch 502/30000 Training Loss: 0.18888245522975922\n",
      "Epoch 503/30000 Training Loss: 0.1424369364976883\n",
      "Epoch 504/30000 Training Loss: 0.1151040568947792\n",
      "Epoch 505/30000 Training Loss: 0.14825834333896637\n",
      "Epoch 506/30000 Training Loss: 0.13222789764404297\n",
      "Epoch 507/30000 Training Loss: 0.1495363861322403\n",
      "Epoch 508/30000 Training Loss: 0.1533288061618805\n",
      "Epoch 509/30000 Training Loss: 0.14888574182987213\n",
      "Epoch 510/30000 Training Loss: 0.16653139889240265\n",
      "Epoch 510/30000 Validation Loss: 0.15869028866291046\n",
      "Epoch 511/30000 Training Loss: 0.14134548604488373\n",
      "Epoch 512/30000 Training Loss: 0.15118543803691864\n",
      "Epoch 513/30000 Training Loss: 0.12919996678829193\n",
      "Epoch 514/30000 Training Loss: 0.15757517516613007\n",
      "Epoch 515/30000 Training Loss: 0.159747913479805\n",
      "Epoch 516/30000 Training Loss: 0.15621839463710785\n",
      "Epoch 517/30000 Training Loss: 0.1395961195230484\n",
      "Epoch 518/30000 Training Loss: 0.13124720752239227\n",
      "Epoch 519/30000 Training Loss: 0.1447041630744934\n",
      "Epoch 520/30000 Training Loss: 0.16270707547664642\n",
      "Epoch 520/30000 Validation Loss: 0.1580737680196762\n",
      "Epoch 521/30000 Training Loss: 0.21109497547149658\n",
      "Epoch 522/30000 Training Loss: 0.153046116232872\n",
      "Epoch 523/30000 Training Loss: 0.1446964591741562\n",
      "Epoch 524/30000 Training Loss: 0.16684067249298096\n",
      "Epoch 525/30000 Training Loss: 0.14587466418743134\n",
      "Epoch 526/30000 Training Loss: 0.15534962713718414\n",
      "Epoch 527/30000 Training Loss: 0.17165406048297882\n",
      "Epoch 528/30000 Training Loss: 0.15975578129291534\n",
      "Epoch 529/30000 Training Loss: 0.16546784341335297\n",
      "Epoch 530/30000 Training Loss: 0.15944914519786835\n",
      "Epoch 530/30000 Validation Loss: 0.1564456969499588\n",
      "Epoch 531/30000 Training Loss: 0.18434090912342072\n",
      "Epoch 532/30000 Training Loss: 0.14233016967773438\n",
      "Epoch 533/30000 Training Loss: 0.145436093211174\n",
      "Epoch 534/30000 Training Loss: 0.17078502476215363\n",
      "Epoch 535/30000 Training Loss: 0.16041205823421478\n",
      "Epoch 536/30000 Training Loss: 0.17325174808502197\n",
      "Epoch 537/30000 Training Loss: 0.15446795523166656\n",
      "Epoch 538/30000 Training Loss: 0.1683107167482376\n",
      "Epoch 539/30000 Training Loss: 0.14789025485515594\n",
      "Epoch 540/30000 Training Loss: 0.1371530443429947\n",
      "Epoch 540/30000 Validation Loss: 0.15860362350940704\n",
      "Epoch 541/30000 Training Loss: 0.1777896136045456\n",
      "Epoch 542/30000 Training Loss: 0.1559710055589676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 543/30000 Training Loss: 0.15197403728961945\n",
      "Epoch 544/30000 Training Loss: 0.1599084734916687\n",
      "Epoch 545/30000 Training Loss: 0.17619305849075317\n",
      "Epoch 546/30000 Training Loss: 0.11699619144201279\n",
      "Epoch 547/30000 Training Loss: 0.1580924540758133\n",
      "Epoch 548/30000 Training Loss: 0.16924892365932465\n",
      "Epoch 549/30000 Training Loss: 0.1345743089914322\n",
      "Epoch 550/30000 Training Loss: 0.15984980762004852\n",
      "Epoch 550/30000 Validation Loss: 0.15923519432544708\n",
      "Epoch 551/30000 Training Loss: 0.176229789853096\n",
      "Epoch 552/30000 Training Loss: 0.15370754897594452\n",
      "Epoch 553/30000 Training Loss: 0.15707461535930634\n",
      "Epoch 554/30000 Training Loss: 0.14465202391147614\n",
      "Epoch 555/30000 Training Loss: 0.15757502615451813\n",
      "Epoch 556/30000 Training Loss: 0.12391508370637894\n",
      "Epoch 557/30000 Training Loss: 0.1516331136226654\n",
      "Epoch 558/30000 Training Loss: 0.15595829486846924\n",
      "Epoch 559/30000 Training Loss: 0.17971569299697876\n",
      "Epoch 560/30000 Training Loss: 0.14943911135196686\n",
      "Epoch 560/30000 Validation Loss: 0.15817150473594666\n",
      "Epoch 561/30000 Training Loss: 0.14139796793460846\n",
      "Epoch 562/30000 Training Loss: 0.12422769516706467\n",
      "Epoch 563/30000 Training Loss: 0.11095903068780899\n",
      "Epoch 564/30000 Training Loss: 0.14981134235858917\n",
      "Epoch 565/30000 Training Loss: 0.11799096316099167\n",
      "Epoch 566/30000 Training Loss: 0.1381412148475647\n",
      "Epoch 567/30000 Training Loss: 0.15046752989292145\n",
      "Epoch 568/30000 Training Loss: 0.14453332126140594\n",
      "Epoch 569/30000 Training Loss: 0.13262112438678741\n",
      "Epoch 570/30000 Training Loss: 0.13606047630310059\n",
      "Epoch 570/30000 Validation Loss: 0.11082593351602554\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.11082593351602554<=============\n",
      "Epoch 571/30000 Training Loss: 0.16635318100452423\n",
      "Epoch 572/30000 Training Loss: 0.15414442121982574\n",
      "Epoch 573/30000 Training Loss: 0.1460081934928894\n",
      "Epoch 574/30000 Training Loss: 0.1381935030221939\n",
      "Epoch 575/30000 Training Loss: 0.13601206243038177\n",
      "Epoch 576/30000 Training Loss: 0.12802597880363464\n",
      "Epoch 577/30000 Training Loss: 0.1540277600288391\n",
      "Epoch 578/30000 Training Loss: 0.14017285406589508\n",
      "Epoch 579/30000 Training Loss: 0.15124495327472687\n",
      "Epoch 580/30000 Training Loss: 0.13998731970787048\n",
      "Epoch 580/30000 Validation Loss: 0.1545945256948471\n",
      "Epoch 581/30000 Training Loss: 0.15063758194446564\n",
      "Epoch 582/30000 Training Loss: 0.1454097181558609\n",
      "Epoch 583/30000 Training Loss: 0.1642424762248993\n",
      "Epoch 584/30000 Training Loss: 0.12880145013332367\n",
      "Epoch 585/30000 Training Loss: 0.14319300651550293\n",
      "Epoch 586/30000 Training Loss: 0.11766845732927322\n",
      "Epoch 587/30000 Training Loss: 0.15175403654575348\n",
      "Epoch 588/30000 Training Loss: 0.13034765422344208\n",
      "Epoch 589/30000 Training Loss: 0.14258617162704468\n",
      "Epoch 590/30000 Training Loss: 0.14019189774990082\n",
      "Epoch 590/30000 Validation Loss: 0.15806011855602264\n",
      "Epoch 591/30000 Training Loss: 0.14160767197608948\n",
      "Epoch 592/30000 Training Loss: 0.1284773349761963\n",
      "Epoch 593/30000 Training Loss: 0.14984039962291718\n",
      "Epoch 594/30000 Training Loss: 0.15637600421905518\n",
      "Epoch 595/30000 Training Loss: 0.14167502522468567\n",
      "Epoch 596/30000 Training Loss: 0.11340411752462387\n",
      "Epoch 597/30000 Training Loss: 0.13487492501735687\n",
      "Epoch 598/30000 Training Loss: 0.13627763092517853\n",
      "Epoch 599/30000 Training Loss: 0.12938357889652252\n",
      "Epoch 600/30000 Training Loss: 0.172790989279747\n",
      "Epoch 600/30000 Validation Loss: 0.14190764725208282\n",
      "Epoch 601/30000 Training Loss: 0.1289154291152954\n",
      "Epoch 602/30000 Training Loss: 0.14098632335662842\n",
      "Epoch 603/30000 Training Loss: 0.12484649568796158\n",
      "Epoch 604/30000 Training Loss: 0.14529596269130707\n",
      "Epoch 605/30000 Training Loss: 0.15035152435302734\n",
      "Epoch 606/30000 Training Loss: 0.13713322579860687\n",
      "Epoch 607/30000 Training Loss: 0.13635613024234772\n",
      "Epoch 608/30000 Training Loss: 0.12696559727191925\n",
      "Epoch 609/30000 Training Loss: 0.15445400774478912\n",
      "Epoch 610/30000 Training Loss: 0.138941690325737\n",
      "Epoch 610/30000 Validation Loss: 0.1406242698431015\n",
      "Epoch 611/30000 Training Loss: 0.1288105696439743\n",
      "Epoch 612/30000 Training Loss: 0.13138464093208313\n",
      "Epoch 613/30000 Training Loss: 0.11152537912130356\n",
      "Epoch 614/30000 Training Loss: 0.14918534457683563\n",
      "Epoch 615/30000 Training Loss: 0.12560376524925232\n",
      "Epoch 616/30000 Training Loss: 0.1730295568704605\n",
      "Epoch 617/30000 Training Loss: 0.16025546193122864\n",
      "Epoch 618/30000 Training Loss: 0.15960797667503357\n",
      "Epoch 619/30000 Training Loss: 0.11350556463003159\n",
      "Epoch 620/30000 Training Loss: 0.15824373066425323\n",
      "Epoch 620/30000 Validation Loss: 0.12708349525928497\n",
      "Epoch 621/30000 Training Loss: 0.14339174330234528\n",
      "Epoch 622/30000 Training Loss: 0.11502072215080261\n",
      "Epoch 623/30000 Training Loss: 0.1394086331129074\n",
      "Epoch 624/30000 Training Loss: 0.12272986024618149\n",
      "Epoch 625/30000 Training Loss: 0.15122823417186737\n",
      "Epoch 626/30000 Training Loss: 0.15611661970615387\n",
      "Epoch 627/30000 Training Loss: 0.16450630128383636\n",
      "Epoch 628/30000 Training Loss: 0.18479253351688385\n",
      "Epoch 629/30000 Training Loss: 0.14176608622074127\n",
      "Epoch 630/30000 Training Loss: 0.13647493720054626\n",
      "Epoch 630/30000 Validation Loss: 0.15612685680389404\n",
      "Epoch 631/30000 Training Loss: 0.13207896053791046\n",
      "Epoch 632/30000 Training Loss: 0.14429570734500885\n",
      "Epoch 633/30000 Training Loss: 0.12879620492458344\n",
      "Epoch 634/30000 Training Loss: 0.1348775178194046\n",
      "Epoch 635/30000 Training Loss: 0.12515175342559814\n",
      "Epoch 636/30000 Training Loss: 0.13263563811779022\n",
      "Epoch 637/30000 Training Loss: 0.11395782232284546\n",
      "Epoch 638/30000 Training Loss: 0.1661144644021988\n",
      "Epoch 639/30000 Training Loss: 0.1469731479883194\n",
      "Epoch 640/30000 Training Loss: 0.12667277455329895\n",
      "Epoch 640/30000 Validation Loss: 0.1640632450580597\n",
      "Epoch 641/30000 Training Loss: 0.1296667903661728\n",
      "Epoch 642/30000 Training Loss: 0.12724681198596954\n",
      "Epoch 643/30000 Training Loss: 0.1320839375257492\n",
      "Epoch 644/30000 Training Loss: 0.11547636985778809\n",
      "Epoch 645/30000 Training Loss: 0.16368712484836578\n",
      "Epoch 646/30000 Training Loss: 0.15310506522655487\n",
      "Epoch 647/30000 Training Loss: 0.13321170210838318\n",
      "Epoch 648/30000 Training Loss: 0.123892642557621\n",
      "Epoch 649/30000 Training Loss: 0.1425253003835678\n",
      "Epoch 650/30000 Training Loss: 0.12522757053375244\n",
      "Epoch 650/30000 Validation Loss: 0.10920515656471252\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.10920515656471252<=============\n",
      "Epoch 651/30000 Training Loss: 0.12123972177505493\n",
      "Epoch 652/30000 Training Loss: 0.16568060219287872\n",
      "Epoch 653/30000 Training Loss: 0.12446191906929016\n",
      "Epoch 654/30000 Training Loss: 0.11998126655817032\n",
      "Epoch 655/30000 Training Loss: 0.14908258616924286\n",
      "Epoch 656/30000 Training Loss: 0.13455650210380554\n",
      "Epoch 657/30000 Training Loss: 0.12825912237167358\n",
      "Epoch 658/30000 Training Loss: 0.13438788056373596\n",
      "Epoch 659/30000 Training Loss: 0.13344018161296844\n",
      "Epoch 660/30000 Training Loss: 0.15444965660572052\n",
      "Epoch 660/30000 Validation Loss: 0.14036689698696136\n",
      "Epoch 661/30000 Training Loss: 0.13367412984371185\n",
      "Epoch 662/30000 Training Loss: 0.14295925199985504\n",
      "Epoch 663/30000 Training Loss: 0.129398375749588\n",
      "Epoch 664/30000 Training Loss: 0.11836811155080795\n",
      "Epoch 665/30000 Training Loss: 0.13912327587604523\n",
      "Epoch 666/30000 Training Loss: 0.1318860501050949\n",
      "Epoch 667/30000 Training Loss: 0.1443563848733902\n",
      "Epoch 668/30000 Training Loss: 0.14104564487934113\n",
      "Epoch 669/30000 Training Loss: 0.122564397752285\n",
      "Epoch 670/30000 Training Loss: 0.12252583354711533\n",
      "Epoch 670/30000 Validation Loss: 0.12863579392433167\n",
      "Epoch 671/30000 Training Loss: 0.1470557600259781\n",
      "Epoch 672/30000 Training Loss: 0.12284857034683228\n",
      "Epoch 673/30000 Training Loss: 0.1337273120880127\n",
      "Epoch 674/30000 Training Loss: 0.15355251729488373\n",
      "Epoch 675/30000 Training Loss: 0.15989869832992554\n",
      "Epoch 676/30000 Training Loss: 0.15358440577983856\n",
      "Epoch 677/30000 Training Loss: 0.1419539451599121\n",
      "Epoch 678/30000 Training Loss: 0.13429442048072815\n",
      "Epoch 679/30000 Training Loss: 0.12583574652671814\n",
      "Epoch 680/30000 Training Loss: 0.13246464729309082\n",
      "Epoch 680/30000 Validation Loss: 0.14871364831924438\n",
      "Epoch 681/30000 Training Loss: 0.13374318182468414\n",
      "Epoch 682/30000 Training Loss: 0.13125082850456238\n",
      "Epoch 683/30000 Training Loss: 0.15039150416851044\n",
      "Epoch 684/30000 Training Loss: 0.15271435678005219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 685/30000 Training Loss: 0.13784609735012054\n",
      "Epoch 686/30000 Training Loss: 0.13181696832180023\n",
      "Epoch 687/30000 Training Loss: 0.1387433558702469\n",
      "Epoch 688/30000 Training Loss: 0.1463407725095749\n",
      "Epoch 689/30000 Training Loss: 0.1644272357225418\n",
      "Epoch 690/30000 Training Loss: 0.12293746322393417\n",
      "Epoch 690/30000 Validation Loss: 0.11855185031890869\n",
      "Epoch 691/30000 Training Loss: 0.13434384763240814\n",
      "Epoch 692/30000 Training Loss: 0.20267826318740845\n",
      "Epoch 693/30000 Training Loss: 0.1400725543498993\n",
      "Epoch 694/30000 Training Loss: 0.13095389306545258\n",
      "Epoch 695/30000 Training Loss: 0.14131274819374084\n",
      "Epoch 696/30000 Training Loss: 0.11286056786775589\n",
      "Epoch 697/30000 Training Loss: 0.13580988347530365\n",
      "Epoch 698/30000 Training Loss: 0.15337301790714264\n",
      "Epoch 699/30000 Training Loss: 0.13083451986312866\n",
      "Epoch 700/30000 Training Loss: 0.12912873923778534\n",
      "Epoch 700/30000 Validation Loss: 0.1381690949201584\n",
      "Epoch 701/30000 Training Loss: 0.1251785308122635\n",
      "Epoch 702/30000 Training Loss: 0.12202027440071106\n",
      "Epoch 703/30000 Training Loss: 0.1333949714899063\n",
      "Epoch 704/30000 Training Loss: 0.12032192945480347\n",
      "Epoch 705/30000 Training Loss: 0.15489806234836578\n",
      "Epoch 706/30000 Training Loss: 0.12491359561681747\n",
      "Epoch 707/30000 Training Loss: 0.1352023333311081\n",
      "Epoch 708/30000 Training Loss: 0.1113053560256958\n",
      "Epoch 709/30000 Training Loss: 0.12923121452331543\n",
      "Epoch 710/30000 Training Loss: 0.1296004205942154\n",
      "Epoch 710/30000 Validation Loss: 0.14481854438781738\n",
      "Epoch 711/30000 Training Loss: 0.1294371634721756\n",
      "Epoch 712/30000 Training Loss: 0.12301083654165268\n",
      "Epoch 713/30000 Training Loss: 0.1491456776857376\n",
      "Epoch 714/30000 Training Loss: 0.13568520545959473\n",
      "Epoch 715/30000 Training Loss: 0.12151309847831726\n",
      "Epoch 716/30000 Training Loss: 0.14681728184223175\n",
      "Epoch 717/30000 Training Loss: 0.13410161435604095\n",
      "Epoch 718/30000 Training Loss: 0.13721613585948944\n",
      "Epoch 719/30000 Training Loss: 0.1705610752105713\n",
      "Epoch 720/30000 Training Loss: 0.1258360743522644\n",
      "Epoch 720/30000 Validation Loss: 0.14300265908241272\n",
      "Epoch 721/30000 Training Loss: 0.13336044549942017\n",
      "Epoch 722/30000 Training Loss: 0.1437336802482605\n",
      "Epoch 723/30000 Training Loss: 0.16344453394412994\n",
      "Epoch 724/30000 Training Loss: 0.13805894553661346\n",
      "Epoch 725/30000 Training Loss: 0.13719205558300018\n",
      "Epoch 726/30000 Training Loss: 0.1320226788520813\n",
      "Epoch 727/30000 Training Loss: 0.13204143941402435\n",
      "Epoch 728/30000 Training Loss: 0.1290944665670395\n",
      "Epoch 729/30000 Training Loss: 0.14352647960186005\n",
      "Epoch 730/30000 Training Loss: 0.13168257474899292\n",
      "Epoch 730/30000 Validation Loss: 0.16058211028575897\n",
      "Epoch 731/30000 Training Loss: 0.1381276696920395\n",
      "Epoch 732/30000 Training Loss: 0.14921103417873383\n",
      "Epoch 733/30000 Training Loss: 0.10130610316991806\n",
      "Epoch 734/30000 Training Loss: 0.1511606127023697\n",
      "Epoch 735/30000 Training Loss: 0.14908243715763092\n",
      "Epoch 736/30000 Training Loss: 0.127205491065979\n",
      "Epoch 737/30000 Training Loss: 0.12896102666854858\n",
      "Epoch 738/30000 Training Loss: 0.1433977335691452\n",
      "Epoch 739/30000 Training Loss: 0.11957189440727234\n",
      "Epoch 740/30000 Training Loss: 0.10653791576623917\n",
      "Epoch 740/30000 Validation Loss: 0.14510536193847656\n",
      "Epoch 741/30000 Training Loss: 0.1239941343665123\n",
      "Epoch 742/30000 Training Loss: 0.11755117774009705\n",
      "Epoch 743/30000 Training Loss: 0.12016499042510986\n",
      "Epoch 744/30000 Training Loss: 0.12955988943576813\n",
      "Epoch 745/30000 Training Loss: 0.11095594614744186\n",
      "Epoch 746/30000 Training Loss: 0.1096126139163971\n",
      "Epoch 747/30000 Training Loss: 0.14455856382846832\n",
      "Epoch 748/30000 Training Loss: 0.1279517561197281\n",
      "Epoch 749/30000 Training Loss: 0.13709592819213867\n",
      "Epoch 750/30000 Training Loss: 0.1280563622713089\n",
      "Epoch 750/30000 Validation Loss: 0.11669349670410156\n",
      "Epoch 751/30000 Training Loss: 0.13927267491817474\n",
      "Epoch 752/30000 Training Loss: 0.13091391324996948\n",
      "Epoch 753/30000 Training Loss: 0.1262030452489853\n",
      "Epoch 754/30000 Training Loss: 0.13012073934078217\n",
      "Epoch 755/30000 Training Loss: 0.11498311161994934\n",
      "Epoch 756/30000 Training Loss: 0.13410024344921112\n",
      "Epoch 757/30000 Training Loss: 0.16305477917194366\n",
      "Epoch 758/30000 Training Loss: 0.1599772721529007\n",
      "Epoch 759/30000 Training Loss: 0.13268637657165527\n",
      "Epoch 760/30000 Training Loss: 0.14121395349502563\n",
      "Epoch 760/30000 Validation Loss: 0.10334941744804382\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.10334941744804382<=============\n",
      "Epoch 761/30000 Training Loss: 0.13432787358760834\n",
      "Epoch 762/30000 Training Loss: 0.14002831280231476\n",
      "Epoch 763/30000 Training Loss: 0.11240538954734802\n",
      "Epoch 764/30000 Training Loss: 0.13947023451328278\n",
      "Epoch 765/30000 Training Loss: 0.13907314836978912\n",
      "Epoch 766/30000 Training Loss: 0.12168803066015244\n",
      "Epoch 767/30000 Training Loss: 0.1285533308982849\n",
      "Epoch 768/30000 Training Loss: 0.1827523112297058\n",
      "Epoch 769/30000 Training Loss: 0.11725600808858871\n",
      "Epoch 770/30000 Training Loss: 0.13764233887195587\n",
      "Epoch 770/30000 Validation Loss: 0.1279643326997757\n",
      "Epoch 771/30000 Training Loss: 0.13845382630825043\n",
      "Epoch 772/30000 Training Loss: 0.12035156041383743\n",
      "Epoch 773/30000 Training Loss: 0.1361820548772812\n",
      "Epoch 774/30000 Training Loss: 0.12930409610271454\n",
      "Epoch 775/30000 Training Loss: 0.12868782877922058\n",
      "Epoch 776/30000 Training Loss: 0.10158956050872803\n",
      "Epoch 777/30000 Training Loss: 0.13005869090557098\n",
      "Epoch 778/30000 Training Loss: 0.13022871315479279\n",
      "Epoch 779/30000 Training Loss: 0.09975989907979965\n",
      "Epoch 780/30000 Training Loss: 0.1270439773797989\n",
      "Epoch 780/30000 Validation Loss: 0.12619267404079437\n",
      "Epoch 781/30000 Training Loss: 0.12083031982183456\n",
      "Epoch 782/30000 Training Loss: 0.1506178379058838\n",
      "Epoch 783/30000 Training Loss: 0.12730927765369415\n",
      "Epoch 784/30000 Training Loss: 0.10094862431287766\n",
      "Epoch 785/30000 Training Loss: 0.13344906270503998\n",
      "Epoch 786/30000 Training Loss: 0.11466369777917862\n",
      "Epoch 787/30000 Training Loss: 0.15725421905517578\n",
      "Epoch 788/30000 Training Loss: 0.1128355860710144\n",
      "Epoch 789/30000 Training Loss: 0.12807117402553558\n",
      "Epoch 790/30000 Training Loss: 0.13657264411449432\n",
      "Epoch 790/30000 Validation Loss: 0.16005849838256836\n",
      "Epoch 791/30000 Training Loss: 0.12053657323122025\n",
      "Epoch 792/30000 Training Loss: 0.10928502678871155\n",
      "Epoch 793/30000 Training Loss: 0.15142105519771576\n",
      "Epoch 794/30000 Training Loss: 0.13298068940639496\n",
      "Epoch 795/30000 Training Loss: 0.13089126348495483\n",
      "Epoch 796/30000 Training Loss: 0.1353578418493271\n",
      "Epoch 797/30000 Training Loss: 0.1396574229001999\n",
      "Epoch 798/30000 Training Loss: 0.16074185073375702\n",
      "Epoch 799/30000 Training Loss: 0.13467709720134735\n",
      "Epoch 800/30000 Training Loss: 0.11667948961257935\n",
      "Epoch 800/30000 Validation Loss: 0.12948232889175415\n",
      "Epoch 801/30000 Training Loss: 0.13927550613880157\n",
      "Epoch 802/30000 Training Loss: 0.14234347641468048\n",
      "Epoch 803/30000 Training Loss: 0.10906796902418137\n",
      "Epoch 804/30000 Training Loss: 0.14252673089504242\n",
      "Epoch 805/30000 Training Loss: 0.1444711983203888\n",
      "Epoch 806/30000 Training Loss: 0.12258756160736084\n",
      "Epoch 807/30000 Training Loss: 0.12022437900304794\n",
      "Epoch 808/30000 Training Loss: 0.12430908530950546\n",
      "Epoch 809/30000 Training Loss: 0.13488145172595978\n",
      "Epoch 810/30000 Training Loss: 0.14837463200092316\n",
      "Epoch 810/30000 Validation Loss: 0.14172963798046112\n",
      "Epoch 811/30000 Training Loss: 0.12312209606170654\n",
      "Epoch 812/30000 Training Loss: 0.1234082579612732\n",
      "Epoch 813/30000 Training Loss: 0.11029830574989319\n",
      "Epoch 814/30000 Training Loss: 0.10964405536651611\n",
      "Epoch 815/30000 Training Loss: 0.15673565864562988\n",
      "Epoch 816/30000 Training Loss: 0.11585042625665665\n",
      "Epoch 817/30000 Training Loss: 0.14325106143951416\n",
      "Epoch 818/30000 Training Loss: 0.13402371108531952\n",
      "Epoch 819/30000 Training Loss: 0.13695602118968964\n",
      "Epoch 820/30000 Training Loss: 0.11324360221624374\n",
      "Epoch 820/30000 Validation Loss: 0.13327185809612274\n",
      "Epoch 821/30000 Training Loss: 0.14530426263809204\n",
      "Epoch 822/30000 Training Loss: 0.12978364527225494\n",
      "Epoch 823/30000 Training Loss: 0.13136030733585358\n",
      "Epoch 824/30000 Training Loss: 0.10542953759431839\n",
      "Epoch 825/30000 Training Loss: 0.13769271969795227\n",
      "Epoch 826/30000 Training Loss: 0.10358516126871109\n",
      "Epoch 827/30000 Training Loss: 0.12698005139827728\n",
      "Epoch 828/30000 Training Loss: 0.12326673418283463\n",
      "Epoch 829/30000 Training Loss: 0.12140319496393204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 830/30000 Training Loss: 0.1245790496468544\n",
      "Epoch 830/30000 Validation Loss: 0.11766234785318375\n",
      "Epoch 831/30000 Training Loss: 0.15541256964206696\n",
      "Epoch 832/30000 Training Loss: 0.1348501443862915\n",
      "Epoch 833/30000 Training Loss: 0.13445477187633514\n",
      "Epoch 834/30000 Training Loss: 0.11247646808624268\n",
      "Epoch 835/30000 Training Loss: 0.11519825458526611\n",
      "Epoch 836/30000 Training Loss: 0.15635593235492706\n",
      "Epoch 837/30000 Training Loss: 0.10796131938695908\n",
      "Epoch 838/30000 Training Loss: 0.1325439065694809\n",
      "Epoch 839/30000 Training Loss: 0.10821845382452011\n",
      "Epoch 840/30000 Training Loss: 0.14875279366970062\n",
      "Epoch 840/30000 Validation Loss: 0.1119605004787445\n",
      "Epoch 841/30000 Training Loss: 0.15189513564109802\n",
      "Epoch 842/30000 Training Loss: 0.12622080743312836\n",
      "Epoch 843/30000 Training Loss: 0.12573109567165375\n",
      "Epoch 844/30000 Training Loss: 0.14560717344284058\n",
      "Epoch 845/30000 Training Loss: 0.11555298417806625\n",
      "Epoch 846/30000 Training Loss: 0.15391218662261963\n",
      "Epoch 847/30000 Training Loss: 0.12698522210121155\n",
      "Epoch 848/30000 Training Loss: 0.13778305053710938\n",
      "Epoch 849/30000 Training Loss: 0.11843091249465942\n",
      "Epoch 850/30000 Training Loss: 0.15554304420948029\n",
      "Epoch 850/30000 Validation Loss: 0.10872817039489746\n",
      "Epoch 851/30000 Training Loss: 0.1396915465593338\n",
      "Epoch 852/30000 Training Loss: 0.12726378440856934\n",
      "Epoch 853/30000 Training Loss: 0.10531697422266006\n",
      "Epoch 854/30000 Training Loss: 0.11280449479818344\n",
      "Epoch 855/30000 Training Loss: 0.13408169150352478\n",
      "Epoch 856/30000 Training Loss: 0.13426578044891357\n",
      "Epoch 857/30000 Training Loss: 0.12437880784273148\n",
      "Epoch 858/30000 Training Loss: 0.11358783394098282\n",
      "Epoch 859/30000 Training Loss: 0.15134742856025696\n",
      "Epoch 860/30000 Training Loss: 0.1192152127623558\n",
      "Epoch 860/30000 Validation Loss: 0.13556404411792755\n",
      "Epoch 861/30000 Training Loss: 0.10897437483072281\n",
      "Epoch 862/30000 Training Loss: 0.1364598572254181\n",
      "Epoch 863/30000 Training Loss: 0.13050448894500732\n",
      "Epoch 864/30000 Training Loss: 0.12092956155538559\n",
      "Epoch 865/30000 Training Loss: 0.12701283395290375\n",
      "Epoch 866/30000 Training Loss: 0.11643414944410324\n",
      "Epoch 867/30000 Training Loss: 0.1307063102722168\n",
      "Epoch 868/30000 Training Loss: 0.1275997906923294\n",
      "Epoch 869/30000 Training Loss: 0.13561727106571198\n",
      "Epoch 870/30000 Training Loss: 0.13420860469341278\n",
      "Epoch 870/30000 Validation Loss: 0.12978221476078033\n",
      "Epoch 871/30000 Training Loss: 0.09142781049013138\n",
      "Epoch 872/30000 Training Loss: 0.12297473102807999\n",
      "Epoch 873/30000 Training Loss: 0.14183342456817627\n",
      "Epoch 874/30000 Training Loss: 0.12107580155134201\n",
      "Epoch 875/30000 Training Loss: 0.13483518362045288\n",
      "Epoch 876/30000 Training Loss: 0.12592269480228424\n",
      "Epoch 877/30000 Training Loss: 0.1340719312429428\n",
      "Epoch 878/30000 Training Loss: 0.12052479386329651\n",
      "Epoch 879/30000 Training Loss: 0.11979911476373672\n",
      "Epoch 880/30000 Training Loss: 0.1271771639585495\n",
      "Epoch 880/30000 Validation Loss: 0.13487863540649414\n",
      "Epoch 881/30000 Training Loss: 0.12947507202625275\n",
      "Epoch 882/30000 Training Loss: 0.10794856399297714\n",
      "Epoch 883/30000 Training Loss: 0.14475446939468384\n",
      "Epoch 884/30000 Training Loss: 0.10529632121324539\n",
      "Epoch 885/30000 Training Loss: 0.14269877970218658\n",
      "Epoch 886/30000 Training Loss: 0.09784463047981262\n",
      "Epoch 887/30000 Training Loss: 0.11264830827713013\n",
      "Epoch 888/30000 Training Loss: 0.09509632736444473\n",
      "Epoch 889/30000 Training Loss: 0.13033197820186615\n",
      "Epoch 890/30000 Training Loss: 0.1545938104391098\n",
      "Epoch 890/30000 Validation Loss: 0.13138103485107422\n",
      "Epoch 891/30000 Training Loss: 0.12700533866882324\n",
      "Epoch 892/30000 Training Loss: 0.12482815980911255\n",
      "Epoch 893/30000 Training Loss: 0.11365818977355957\n",
      "Epoch 894/30000 Training Loss: 0.09502200037240982\n",
      "Epoch 895/30000 Training Loss: 0.12383139878511429\n",
      "Epoch 896/30000 Training Loss: 0.14058031141757965\n",
      "Epoch 897/30000 Training Loss: 0.11854783445596695\n",
      "Epoch 898/30000 Training Loss: 0.1453884243965149\n",
      "Epoch 899/30000 Training Loss: 0.11094897240400314\n",
      "Epoch 900/30000 Training Loss: 0.10844958573579788\n",
      "Epoch 900/30000 Validation Loss: 0.13530555367469788\n",
      "Epoch 901/30000 Training Loss: 0.11834758520126343\n",
      "Epoch 902/30000 Training Loss: 0.10269864648580551\n",
      "Epoch 903/30000 Training Loss: 0.1354682892560959\n",
      "Epoch 904/30000 Training Loss: 0.14413906633853912\n",
      "Epoch 905/30000 Training Loss: 0.11263740807771683\n",
      "Epoch 906/30000 Training Loss: 0.15321409702301025\n",
      "Epoch 907/30000 Training Loss: 0.1080000028014183\n",
      "Epoch 908/30000 Training Loss: 0.11203070729970932\n",
      "Epoch 909/30000 Training Loss: 0.1269775629043579\n",
      "Epoch 910/30000 Training Loss: 0.10777640342712402\n",
      "Epoch 910/30000 Validation Loss: 0.10371166467666626\n",
      "Epoch 911/30000 Training Loss: 0.1306934803724289\n",
      "Epoch 912/30000 Training Loss: 0.11010196059942245\n",
      "Epoch 913/30000 Training Loss: 0.13263081014156342\n",
      "Epoch 914/30000 Training Loss: 0.11309630423784256\n",
      "Epoch 915/30000 Training Loss: 0.13916444778442383\n",
      "Epoch 916/30000 Training Loss: 0.11340375989675522\n",
      "Epoch 917/30000 Training Loss: 0.12143657356500626\n",
      "Epoch 918/30000 Training Loss: 0.1336948722600937\n",
      "Epoch 919/30000 Training Loss: 0.11581698805093765\n",
      "Epoch 920/30000 Training Loss: 0.12371814250946045\n",
      "Epoch 920/30000 Validation Loss: 0.14406618475914001\n",
      "Epoch 921/30000 Training Loss: 0.12302841991186142\n",
      "Epoch 922/30000 Training Loss: 0.1247357726097107\n",
      "Epoch 923/30000 Training Loss: 0.11122307926416397\n",
      "Epoch 924/30000 Training Loss: 0.10993486642837524\n",
      "Epoch 925/30000 Training Loss: 0.09482377022504807\n",
      "Epoch 926/30000 Training Loss: 0.12002956867218018\n",
      "Epoch 927/30000 Training Loss: 0.12837828695774078\n",
      "Epoch 928/30000 Training Loss: 0.13849161565303802\n",
      "Epoch 929/30000 Training Loss: 0.11006645113229752\n",
      "Epoch 930/30000 Training Loss: 0.12358056753873825\n",
      "Epoch 930/30000 Validation Loss: 0.1537262201309204\n",
      "Epoch 931/30000 Training Loss: 0.1280503273010254\n",
      "Epoch 932/30000 Training Loss: 0.09738817065954208\n",
      "Epoch 933/30000 Training Loss: 0.11543598771095276\n",
      "Epoch 934/30000 Training Loss: 0.1107933521270752\n",
      "Epoch 935/30000 Training Loss: 0.12378662824630737\n",
      "Epoch 936/30000 Training Loss: 0.11821206659078598\n",
      "Epoch 937/30000 Training Loss: 0.12055736035108566\n",
      "Epoch 938/30000 Training Loss: 0.13018132746219635\n",
      "Epoch 939/30000 Training Loss: 0.10868038982152939\n",
      "Epoch 940/30000 Training Loss: 0.1386471539735794\n",
      "Epoch 940/30000 Validation Loss: 0.11786917597055435\n",
      "Epoch 941/30000 Training Loss: 0.10783085227012634\n",
      "Epoch 942/30000 Training Loss: 0.14163023233413696\n",
      "Epoch 943/30000 Training Loss: 0.10256598144769669\n",
      "Epoch 944/30000 Training Loss: 0.10789892822504044\n",
      "Epoch 945/30000 Training Loss: 0.12378678470849991\n",
      "Epoch 946/30000 Training Loss: 0.11600003391504288\n",
      "Epoch 947/30000 Training Loss: 0.14518044888973236\n",
      "Epoch 948/30000 Training Loss: 0.11450906842947006\n",
      "Epoch 949/30000 Training Loss: 0.15898513793945312\n",
      "Epoch 950/30000 Training Loss: 0.11491691321134567\n",
      "Epoch 950/30000 Validation Loss: 0.12115002423524857\n",
      "Epoch 951/30000 Training Loss: 0.1061369776725769\n",
      "Epoch 952/30000 Training Loss: 0.12739425897598267\n",
      "Epoch 953/30000 Training Loss: 0.10945121198892593\n",
      "Epoch 954/30000 Training Loss: 0.12529630959033966\n",
      "Epoch 955/30000 Training Loss: 0.14790700376033783\n",
      "Epoch 956/30000 Training Loss: 0.14275412261486053\n",
      "Epoch 957/30000 Training Loss: 0.09954539686441422\n",
      "Epoch 958/30000 Training Loss: 0.14809715747833252\n",
      "Epoch 959/30000 Training Loss: 0.12347770482301712\n",
      "Epoch 960/30000 Training Loss: 0.12512320280075073\n",
      "Epoch 960/30000 Validation Loss: 0.13174618780612946\n",
      "Epoch 961/30000 Training Loss: 0.12689156830310822\n",
      "Epoch 962/30000 Training Loss: 0.12795037031173706\n",
      "Epoch 963/30000 Training Loss: 0.10526862740516663\n",
      "Epoch 964/30000 Training Loss: 0.12279623001813889\n",
      "Epoch 965/30000 Training Loss: 0.1195915937423706\n",
      "Epoch 966/30000 Training Loss: 0.12734673917293549\n",
      "Epoch 967/30000 Training Loss: 0.11437902599573135\n",
      "Epoch 968/30000 Training Loss: 0.120069719851017\n",
      "Epoch 969/30000 Training Loss: 0.10621952265501022\n",
      "Epoch 970/30000 Training Loss: 0.11840302497148514\n",
      "Epoch 970/30000 Validation Loss: 0.12519332766532898\n",
      "Epoch 971/30000 Training Loss: 0.11591015011072159\n",
      "Epoch 972/30000 Training Loss: 0.11158635467290878\n",
      "Epoch 973/30000 Training Loss: 0.12038257718086243\n",
      "Epoch 974/30000 Training Loss: 0.11785726994276047\n",
      "Epoch 975/30000 Training Loss: 0.13197003304958344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 976/30000 Training Loss: 0.12314306944608688\n",
      "Epoch 977/30000 Training Loss: 0.13721461594104767\n",
      "Epoch 978/30000 Training Loss: 0.1181650161743164\n",
      "Epoch 979/30000 Training Loss: 0.11022481322288513\n",
      "Epoch 980/30000 Training Loss: 0.11240360885858536\n",
      "Epoch 980/30000 Validation Loss: 0.0962945893406868\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0962945893406868<=============\n",
      "Epoch 981/30000 Training Loss: 0.15040801465511322\n",
      "Epoch 982/30000 Training Loss: 0.1329708695411682\n",
      "Epoch 983/30000 Training Loss: 0.14589060842990875\n",
      "Epoch 984/30000 Training Loss: 0.11542856693267822\n",
      "Epoch 985/30000 Training Loss: 0.12000443786382675\n",
      "Epoch 986/30000 Training Loss: 0.1162908747792244\n",
      "Epoch 987/30000 Training Loss: 0.12249994277954102\n",
      "Epoch 988/30000 Training Loss: 0.14734171330928802\n",
      "Epoch 989/30000 Training Loss: 0.11738596111536026\n",
      "Epoch 990/30000 Training Loss: 0.11366358399391174\n",
      "Epoch 990/30000 Validation Loss: 0.12300354242324829\n",
      "Epoch 991/30000 Training Loss: 0.11876854300498962\n",
      "Epoch 992/30000 Training Loss: 0.13351784646511078\n",
      "Epoch 993/30000 Training Loss: 0.11623245477676392\n",
      "Epoch 994/30000 Training Loss: 0.11638345569372177\n",
      "Epoch 995/30000 Training Loss: 0.12980802357196808\n",
      "Epoch 996/30000 Training Loss: 0.1161346510052681\n",
      "Epoch 997/30000 Training Loss: 0.13080404698848724\n",
      "Epoch 998/30000 Training Loss: 0.1294306069612503\n",
      "Epoch 999/30000 Training Loss: 0.1348419040441513\n",
      "Epoch 1000/30000 Training Loss: 0.14582107961177826\n",
      "Epoch 1000/30000 Validation Loss: 0.09572947770357132\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09572947770357132<=============\n",
      "Epoch 1001/30000 Training Loss: 0.12223256379365921\n",
      "Epoch 1002/30000 Training Loss: 0.1154722273349762\n",
      "Epoch 1003/30000 Training Loss: 0.12252096086740494\n",
      "Epoch 1004/30000 Training Loss: 0.12257257103919983\n",
      "Epoch 1005/30000 Training Loss: 0.12058860063552856\n",
      "Epoch 1006/30000 Training Loss: 0.10935037583112717\n",
      "Epoch 1007/30000 Training Loss: 0.10712483525276184\n",
      "Epoch 1008/30000 Training Loss: 0.10422275215387344\n",
      "Epoch 1009/30000 Training Loss: 0.12425301223993301\n",
      "Epoch 1010/30000 Training Loss: 0.11031874269247055\n",
      "Epoch 1010/30000 Validation Loss: 0.11503317207098007\n",
      "Epoch 1011/30000 Training Loss: 0.088608019053936\n",
      "Epoch 1012/30000 Training Loss: 0.1191442608833313\n",
      "Epoch 1013/30000 Training Loss: 0.11494406312704086\n",
      "Epoch 1014/30000 Training Loss: 0.11042468994855881\n",
      "Epoch 1015/30000 Training Loss: 0.12218678742647171\n",
      "Epoch 1016/30000 Training Loss: 0.12396813184022903\n",
      "Epoch 1017/30000 Training Loss: 0.12027182430028915\n",
      "Epoch 1018/30000 Training Loss: 0.12056797742843628\n",
      "Epoch 1019/30000 Training Loss: 0.09322842955589294\n",
      "Epoch 1020/30000 Training Loss: 0.12037206441164017\n",
      "Epoch 1020/30000 Validation Loss: 0.14526993036270142\n",
      "Epoch 1021/30000 Training Loss: 0.13887682557106018\n",
      "Epoch 1022/30000 Training Loss: 0.11684774607419968\n",
      "Epoch 1023/30000 Training Loss: 0.13625943660736084\n",
      "Epoch 1024/30000 Training Loss: 0.11938033252954483\n",
      "Epoch 1025/30000 Training Loss: 0.13468556106090546\n",
      "Epoch 1026/30000 Training Loss: 0.11835768073797226\n",
      "Epoch 1027/30000 Training Loss: 0.11667118221521378\n",
      "Epoch 1028/30000 Training Loss: 0.13409842550754547\n",
      "Epoch 1029/30000 Training Loss: 0.11533898860216141\n",
      "Epoch 1030/30000 Training Loss: 0.12046021223068237\n",
      "Epoch 1030/30000 Validation Loss: 0.12452328205108643\n",
      "Epoch 1031/30000 Training Loss: 0.1158437430858612\n",
      "Epoch 1032/30000 Training Loss: 0.12354802340269089\n",
      "Epoch 1033/30000 Training Loss: 0.15434063971042633\n",
      "Epoch 1034/30000 Training Loss: 0.10332689434289932\n",
      "Epoch 1035/30000 Training Loss: 0.13421593606472015\n",
      "Epoch 1036/30000 Training Loss: 0.1331004500389099\n",
      "Epoch 1037/30000 Training Loss: 0.12604102492332458\n",
      "Epoch 1038/30000 Training Loss: 0.11139696836471558\n",
      "Epoch 1039/30000 Training Loss: 0.10890906304121017\n",
      "Epoch 1040/30000 Training Loss: 0.10672388225793839\n",
      "Epoch 1040/30000 Validation Loss: 0.11622235178947449\n",
      "Epoch 1041/30000 Training Loss: 0.14722725749015808\n",
      "Epoch 1042/30000 Training Loss: 0.11825839430093765\n",
      "Epoch 1043/30000 Training Loss: 0.13251858949661255\n",
      "Epoch 1044/30000 Training Loss: 0.11065101623535156\n",
      "Epoch 1045/30000 Training Loss: 0.12263777107000351\n",
      "Epoch 1046/30000 Training Loss: 0.11886492371559143\n",
      "Epoch 1047/30000 Training Loss: 0.09994011372327805\n",
      "Epoch 1048/30000 Training Loss: 0.09064327925443649\n",
      "Epoch 1049/30000 Training Loss: 0.10223843902349472\n",
      "Epoch 1050/30000 Training Loss: 0.10787999629974365\n",
      "Epoch 1050/30000 Validation Loss: 0.10374412685632706\n",
      "Epoch 1051/30000 Training Loss: 0.1221727654337883\n",
      "Epoch 1052/30000 Training Loss: 0.14133726060390472\n",
      "Epoch 1053/30000 Training Loss: 0.12361688166856766\n",
      "Epoch 1054/30000 Training Loss: 0.11354207992553711\n",
      "Epoch 1055/30000 Training Loss: 0.10361507534980774\n",
      "Epoch 1056/30000 Training Loss: 0.12122463434934616\n",
      "Epoch 1057/30000 Training Loss: 0.09933644533157349\n",
      "Epoch 1058/30000 Training Loss: 0.1327320784330368\n",
      "Epoch 1059/30000 Training Loss: 0.09618618339300156\n",
      "Epoch 1060/30000 Training Loss: 0.12481597810983658\n",
      "Epoch 1060/30000 Validation Loss: 0.10435297340154648\n",
      "Epoch 1061/30000 Training Loss: 0.1271228939294815\n",
      "Epoch 1062/30000 Training Loss: 0.10214205831289291\n",
      "Epoch 1063/30000 Training Loss: 0.12426326423883438\n",
      "Epoch 1064/30000 Training Loss: 0.09381777048110962\n",
      "Epoch 1065/30000 Training Loss: 0.12027155607938766\n",
      "Epoch 1066/30000 Training Loss: 0.10955265909433365\n",
      "Epoch 1067/30000 Training Loss: 0.09352179616689682\n",
      "Epoch 1068/30000 Training Loss: 0.10376017540693283\n",
      "Epoch 1069/30000 Training Loss: 0.1321713775396347\n",
      "Epoch 1070/30000 Training Loss: 0.13301245868206024\n",
      "Epoch 1070/30000 Validation Loss: 0.1284981518983841\n",
      "Epoch 1071/30000 Training Loss: 0.11504661291837692\n",
      "Epoch 1072/30000 Training Loss: 0.12037453800439835\n",
      "Epoch 1073/30000 Training Loss: 0.10130680352449417\n",
      "Epoch 1074/30000 Training Loss: 0.10605042427778244\n",
      "Epoch 1075/30000 Training Loss: 0.09478000551462173\n",
      "Epoch 1076/30000 Training Loss: 0.12450932711362839\n",
      "Epoch 1077/30000 Training Loss: 0.1088624969124794\n",
      "Epoch 1078/30000 Training Loss: 0.13236980140209198\n",
      "Epoch 1079/30000 Training Loss: 0.08407476544380188\n",
      "Epoch 1080/30000 Training Loss: 0.10231343656778336\n",
      "Epoch 1080/30000 Validation Loss: 0.12820348143577576\n",
      "Epoch 1081/30000 Training Loss: 0.09131062030792236\n",
      "Epoch 1082/30000 Training Loss: 0.1339171975851059\n",
      "Epoch 1083/30000 Training Loss: 0.11437908560037613\n",
      "Epoch 1084/30000 Training Loss: 0.08138047903776169\n",
      "Epoch 1085/30000 Training Loss: 0.12932629883289337\n",
      "Epoch 1086/30000 Training Loss: 0.08741923421621323\n",
      "Epoch 1087/30000 Training Loss: 0.11837482452392578\n",
      "Epoch 1088/30000 Training Loss: 0.11966097354888916\n",
      "Epoch 1089/30000 Training Loss: 0.15178674459457397\n",
      "Epoch 1090/30000 Training Loss: 0.10292985290288925\n",
      "Epoch 1090/30000 Validation Loss: 0.10185950994491577\n",
      "Epoch 1091/30000 Training Loss: 0.09755883365869522\n",
      "Epoch 1092/30000 Training Loss: 0.13896147906780243\n",
      "Epoch 1093/30000 Training Loss: 0.11009291559457779\n",
      "Epoch 1094/30000 Training Loss: 0.11136730760335922\n",
      "Epoch 1095/30000 Training Loss: 0.13138587772846222\n",
      "Epoch 1096/30000 Training Loss: 0.09182566404342651\n",
      "Epoch 1097/30000 Training Loss: 0.14883755147457123\n",
      "Epoch 1098/30000 Training Loss: 0.10408229380846024\n",
      "Epoch 1099/30000 Training Loss: 0.13327576220035553\n",
      "Epoch 1100/30000 Training Loss: 0.10364431887865067\n",
      "Epoch 1100/30000 Validation Loss: 0.11560910940170288\n",
      "Epoch 1101/30000 Training Loss: 0.11146662384271622\n",
      "Epoch 1102/30000 Training Loss: 0.10914898663759232\n",
      "Epoch 1103/30000 Training Loss: 0.14443786442279816\n",
      "Epoch 1104/30000 Training Loss: 0.12392202764749527\n",
      "Epoch 1105/30000 Training Loss: 0.09988135099411011\n",
      "Epoch 1106/30000 Training Loss: 0.14076049625873566\n",
      "Epoch 1107/30000 Training Loss: 0.11886865645647049\n",
      "Epoch 1108/30000 Training Loss: 0.10752842575311661\n",
      "Epoch 1109/30000 Training Loss: 0.13101592659950256\n",
      "Epoch 1110/30000 Training Loss: 0.14522616565227509\n",
      "Epoch 1110/30000 Validation Loss: 0.15431641042232513\n",
      "Epoch 1111/30000 Training Loss: 0.11847937852144241\n",
      "Epoch 1112/30000 Training Loss: 0.10585516691207886\n",
      "Epoch 1113/30000 Training Loss: 0.10016179084777832\n",
      "Epoch 1114/30000 Training Loss: 0.11427424103021622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1115/30000 Training Loss: 0.12303296476602554\n",
      "Epoch 1116/30000 Training Loss: 0.13185478746891022\n",
      "Epoch 1117/30000 Training Loss: 0.13459651172161102\n",
      "Epoch 1118/30000 Training Loss: 0.0856049656867981\n",
      "Epoch 1119/30000 Training Loss: 0.11563920974731445\n",
      "Epoch 1120/30000 Training Loss: 0.09491276741027832\n",
      "Epoch 1120/30000 Validation Loss: 0.09293397516012192\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09293397516012192<=============\n",
      "Epoch 1121/30000 Training Loss: 0.11120820790529251\n",
      "Epoch 1122/30000 Training Loss: 0.12369153648614883\n",
      "Epoch 1123/30000 Training Loss: 0.09623581916093826\n",
      "Epoch 1124/30000 Training Loss: 0.11024703830480576\n",
      "Epoch 1125/30000 Training Loss: 0.11485476046800613\n",
      "Epoch 1126/30000 Training Loss: 0.12724925577640533\n",
      "Epoch 1127/30000 Training Loss: 0.09612098336219788\n",
      "Epoch 1128/30000 Training Loss: 0.1323462575674057\n",
      "Epoch 1129/30000 Training Loss: 0.08913931250572205\n",
      "Epoch 1130/30000 Training Loss: 0.10988222807645798\n",
      "Epoch 1130/30000 Validation Loss: 0.12958799302577972\n",
      "Epoch 1131/30000 Training Loss: 0.12194202095270157\n",
      "Epoch 1132/30000 Training Loss: 0.13439221680164337\n",
      "Epoch 1133/30000 Training Loss: 0.14460894465446472\n",
      "Epoch 1134/30000 Training Loss: 0.1089007556438446\n",
      "Epoch 1135/30000 Training Loss: 0.12499865889549255\n",
      "Epoch 1136/30000 Training Loss: 0.11227773874998093\n",
      "Epoch 1137/30000 Training Loss: 0.11973186582326889\n",
      "Epoch 1138/30000 Training Loss: 0.10986467450857162\n",
      "Epoch 1139/30000 Training Loss: 0.12353088706731796\n",
      "Epoch 1140/30000 Training Loss: 0.11995750665664673\n",
      "Epoch 1140/30000 Validation Loss: 0.12387517839670181\n",
      "Epoch 1141/30000 Training Loss: 0.14457595348358154\n",
      "Epoch 1142/30000 Training Loss: 0.09638011455535889\n",
      "Epoch 1143/30000 Training Loss: 0.10639051347970963\n",
      "Epoch 1144/30000 Training Loss: 0.11522368341684341\n",
      "Epoch 1145/30000 Training Loss: 0.10642854124307632\n",
      "Epoch 1146/30000 Training Loss: 0.12098956853151321\n",
      "Epoch 1147/30000 Training Loss: 0.10862777382135391\n",
      "Epoch 1148/30000 Training Loss: 0.11300507932901382\n",
      "Epoch 1149/30000 Training Loss: 0.15309934318065643\n",
      "Epoch 1150/30000 Training Loss: 0.12127327919006348\n",
      "Epoch 1150/30000 Validation Loss: 0.11565092951059341\n",
      "Epoch 1151/30000 Training Loss: 0.12570536136627197\n",
      "Epoch 1152/30000 Training Loss: 0.10138870030641556\n",
      "Epoch 1153/30000 Training Loss: 0.14476729929447174\n",
      "Epoch 1154/30000 Training Loss: 0.11027394980192184\n",
      "Epoch 1155/30000 Training Loss: 0.12240187078714371\n",
      "Epoch 1156/30000 Training Loss: 0.127127006649971\n",
      "Epoch 1157/30000 Training Loss: 0.0985952690243721\n",
      "Epoch 1158/30000 Training Loss: 0.09347625821828842\n",
      "Epoch 1159/30000 Training Loss: 0.12088938802480698\n",
      "Epoch 1160/30000 Training Loss: 0.09267721325159073\n",
      "Epoch 1160/30000 Validation Loss: 0.12303227186203003\n",
      "Epoch 1161/30000 Training Loss: 0.10889697074890137\n",
      "Epoch 1162/30000 Training Loss: 0.1279146671295166\n",
      "Epoch 1163/30000 Training Loss: 0.12740595638751984\n",
      "Epoch 1164/30000 Training Loss: 0.11890312284231186\n",
      "Epoch 1165/30000 Training Loss: 0.11042714864015579\n",
      "Epoch 1166/30000 Training Loss: 0.11825770884752274\n",
      "Epoch 1167/30000 Training Loss: 0.11740195751190186\n",
      "Epoch 1168/30000 Training Loss: 0.10799122601747513\n",
      "Epoch 1169/30000 Training Loss: 0.10738246887922287\n",
      "Epoch 1170/30000 Training Loss: 0.11231943219900131\n",
      "Epoch 1170/30000 Validation Loss: 0.1019977405667305\n",
      "Epoch 1171/30000 Training Loss: 0.09614363312721252\n",
      "Epoch 1172/30000 Training Loss: 0.10278346389532089\n",
      "Epoch 1173/30000 Training Loss: 0.11367631703615189\n",
      "Epoch 1174/30000 Training Loss: 0.09739608317613602\n",
      "Epoch 1175/30000 Training Loss: 0.11142232269048691\n",
      "Epoch 1176/30000 Training Loss: 0.1084098219871521\n",
      "Epoch 1177/30000 Training Loss: 0.12384221702814102\n",
      "Epoch 1178/30000 Training Loss: 0.10942413657903671\n",
      "Epoch 1179/30000 Training Loss: 0.11857988685369492\n",
      "Epoch 1180/30000 Training Loss: 0.10488325357437134\n",
      "Epoch 1180/30000 Validation Loss: 0.10496321320533752\n",
      "Epoch 1181/30000 Training Loss: 0.1311669796705246\n",
      "Epoch 1182/30000 Training Loss: 0.11492826789617538\n",
      "Epoch 1183/30000 Training Loss: 0.13176482915878296\n",
      "Epoch 1184/30000 Training Loss: 0.12312429398298264\n",
      "Epoch 1185/30000 Training Loss: 0.1307121366262436\n",
      "Epoch 1186/30000 Training Loss: 0.13367237150669098\n",
      "Epoch 1187/30000 Training Loss: 0.12808041274547577\n",
      "Epoch 1188/30000 Training Loss: 0.11189201474189758\n",
      "Epoch 1189/30000 Training Loss: 0.10849914699792862\n",
      "Epoch 1190/30000 Training Loss: 0.10825326293706894\n",
      "Epoch 1190/30000 Validation Loss: 0.10624679177999496\n",
      "Epoch 1191/30000 Training Loss: 0.12717223167419434\n",
      "Epoch 1192/30000 Training Loss: 0.12501634657382965\n",
      "Epoch 1193/30000 Training Loss: 0.11891458183526993\n",
      "Epoch 1194/30000 Training Loss: 0.09512718766927719\n",
      "Epoch 1195/30000 Training Loss: 0.13284088671207428\n",
      "Epoch 1196/30000 Training Loss: 0.10253880172967911\n",
      "Epoch 1197/30000 Training Loss: 0.12053889036178589\n",
      "Epoch 1198/30000 Training Loss: 0.09849515557289124\n",
      "Epoch 1199/30000 Training Loss: 0.104349784553051\n",
      "Epoch 1200/30000 Training Loss: 0.10759053379297256\n",
      "Epoch 1200/30000 Validation Loss: 0.0960497185587883\n",
      "Epoch 1201/30000 Training Loss: 0.10501188039779663\n",
      "Epoch 1202/30000 Training Loss: 0.09548431634902954\n",
      "Epoch 1203/30000 Training Loss: 0.11938145011663437\n",
      "Epoch 1204/30000 Training Loss: 0.14620165526866913\n",
      "Epoch 1205/30000 Training Loss: 0.10910174995660782\n",
      "Epoch 1206/30000 Training Loss: 0.10568626970052719\n",
      "Epoch 1207/30000 Training Loss: 0.08982762694358826\n",
      "Epoch 1208/30000 Training Loss: 0.12331012636423111\n",
      "Epoch 1209/30000 Training Loss: 0.1071448102593422\n",
      "Epoch 1210/30000 Training Loss: 0.14721190929412842\n",
      "Epoch 1210/30000 Validation Loss: 0.11990254372358322\n",
      "Epoch 1211/30000 Training Loss: 0.1059383824467659\n",
      "Epoch 1212/30000 Training Loss: 0.11551648378372192\n",
      "Epoch 1213/30000 Training Loss: 0.10020831972360611\n",
      "Epoch 1214/30000 Training Loss: 0.12000388652086258\n",
      "Epoch 1215/30000 Training Loss: 0.10850004106760025\n",
      "Epoch 1216/30000 Training Loss: 0.158888578414917\n",
      "Epoch 1217/30000 Training Loss: 0.1189090684056282\n",
      "Epoch 1218/30000 Training Loss: 0.10664232820272446\n",
      "Epoch 1219/30000 Training Loss: 0.11850357055664062\n",
      "Epoch 1220/30000 Training Loss: 0.10557481646537781\n",
      "Epoch 1220/30000 Validation Loss: 0.10713177919387817\n",
      "Epoch 1221/30000 Training Loss: 0.11611800640821457\n",
      "Epoch 1222/30000 Training Loss: 0.1564202755689621\n",
      "Epoch 1223/30000 Training Loss: 0.11659301072359085\n",
      "Epoch 1224/30000 Training Loss: 0.11353384703397751\n",
      "Epoch 1225/30000 Training Loss: 0.11919665336608887\n",
      "Epoch 1226/30000 Training Loss: 0.1259351372718811\n",
      "Epoch 1227/30000 Training Loss: 0.1445312649011612\n",
      "Epoch 1228/30000 Training Loss: 0.11299696564674377\n",
      "Epoch 1229/30000 Training Loss: 0.10355976223945618\n",
      "Epoch 1230/30000 Training Loss: 0.1023942157626152\n",
      "Epoch 1230/30000 Validation Loss: 0.11063992232084274\n",
      "Epoch 1231/30000 Training Loss: 0.10864486545324326\n",
      "Epoch 1232/30000 Training Loss: 0.12434449046850204\n",
      "Epoch 1233/30000 Training Loss: 0.10615090280771255\n",
      "Epoch 1234/30000 Training Loss: 0.11988979578018188\n",
      "Epoch 1235/30000 Training Loss: 0.11268988251686096\n",
      "Epoch 1236/30000 Training Loss: 0.10995379835367203\n",
      "Epoch 1237/30000 Training Loss: 0.12107623368501663\n",
      "Epoch 1238/30000 Training Loss: 0.1093619242310524\n",
      "Epoch 1239/30000 Training Loss: 0.15092788636684418\n",
      "Epoch 1240/30000 Training Loss: 0.11343110352754593\n",
      "Epoch 1240/30000 Validation Loss: 0.11998368054628372\n",
      "Epoch 1241/30000 Training Loss: 0.09509077668190002\n",
      "Epoch 1242/30000 Training Loss: 0.11312823742628098\n",
      "Epoch 1243/30000 Training Loss: 0.13906823098659515\n",
      "Epoch 1244/30000 Training Loss: 0.13299427926540375\n",
      "Epoch 1245/30000 Training Loss: 0.15380363166332245\n",
      "Epoch 1246/30000 Training Loss: 0.1266757994890213\n",
      "Epoch 1247/30000 Training Loss: 0.1443902552127838\n",
      "Epoch 1248/30000 Training Loss: 0.10570470243692398\n",
      "Epoch 1249/30000 Training Loss: 0.11530845612287521\n",
      "Epoch 1250/30000 Training Loss: 0.10434611886739731\n",
      "Epoch 1250/30000 Validation Loss: 0.1110987663269043\n",
      "Epoch 1251/30000 Training Loss: 0.1213063970208168\n",
      "Epoch 1252/30000 Training Loss: 0.11125978827476501\n",
      "Epoch 1253/30000 Training Loss: 0.0951889380812645\n",
      "Epoch 1254/30000 Training Loss: 0.12527966499328613\n",
      "Epoch 1255/30000 Training Loss: 0.11858248710632324\n",
      "Epoch 1256/30000 Training Loss: 0.10584741830825806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1257/30000 Training Loss: 0.11763370037078857\n",
      "Epoch 1258/30000 Training Loss: 0.11772769689559937\n",
      "Epoch 1259/30000 Training Loss: 0.11043597012758255\n",
      "Epoch 1260/30000 Training Loss: 0.10647126287221909\n",
      "Epoch 1260/30000 Validation Loss: 0.1087581142783165\n",
      "Epoch 1261/30000 Training Loss: 0.09297481924295425\n",
      "Epoch 1262/30000 Training Loss: 0.11622599512338638\n",
      "Epoch 1263/30000 Training Loss: 0.09821563214063644\n",
      "Epoch 1264/30000 Training Loss: 0.10487369447946548\n",
      "Epoch 1265/30000 Training Loss: 0.10960519313812256\n",
      "Epoch 1266/30000 Training Loss: 0.11225835233926773\n",
      "Epoch 1267/30000 Training Loss: 0.09567049890756607\n",
      "Epoch 1268/30000 Training Loss: 0.12555575370788574\n",
      "Epoch 1269/30000 Training Loss: 0.10714775323867798\n",
      "Epoch 1270/30000 Training Loss: 0.14131198823451996\n",
      "Epoch 1270/30000 Validation Loss: 0.11008135229349136\n",
      "Epoch 1271/30000 Training Loss: 0.10737299919128418\n",
      "Epoch 1272/30000 Training Loss: 0.1316545456647873\n",
      "Epoch 1273/30000 Training Loss: 0.12409374117851257\n",
      "Epoch 1274/30000 Training Loss: 0.10583598166704178\n",
      "Epoch 1275/30000 Training Loss: 0.10605546087026596\n",
      "Epoch 1276/30000 Training Loss: 0.13393141329288483\n",
      "Epoch 1277/30000 Training Loss: 0.11330562084913254\n",
      "Epoch 1278/30000 Training Loss: 0.1128423735499382\n",
      "Epoch 1279/30000 Training Loss: 0.11469411849975586\n",
      "Epoch 1280/30000 Training Loss: 0.10333868116140366\n",
      "Epoch 1280/30000 Validation Loss: 0.10186552256345749\n",
      "Epoch 1281/30000 Training Loss: 0.12550948560237885\n",
      "Epoch 1282/30000 Training Loss: 0.109004907310009\n",
      "Epoch 1283/30000 Training Loss: 0.11672854423522949\n",
      "Epoch 1284/30000 Training Loss: 0.1229468509554863\n",
      "Epoch 1285/30000 Training Loss: 0.10602953284978867\n",
      "Epoch 1286/30000 Training Loss: 0.11385474354028702\n",
      "Epoch 1287/30000 Training Loss: 0.11889668554067612\n",
      "Epoch 1288/30000 Training Loss: 0.09546902030706406\n",
      "Epoch 1289/30000 Training Loss: 0.11304052919149399\n",
      "Epoch 1290/30000 Training Loss: 0.09642169624567032\n",
      "Epoch 1290/30000 Validation Loss: 0.12967181205749512\n",
      "Epoch 1291/30000 Training Loss: 0.09387163072824478\n",
      "Epoch 1292/30000 Training Loss: 0.12383799999952316\n",
      "Epoch 1293/30000 Training Loss: 0.12350840121507645\n",
      "Epoch 1294/30000 Training Loss: 0.09564509242773056\n",
      "Epoch 1295/30000 Training Loss: 0.1307690292596817\n",
      "Epoch 1296/30000 Training Loss: 0.1135067269206047\n",
      "Epoch 1297/30000 Training Loss: 0.10407014936208725\n",
      "Epoch 1298/30000 Training Loss: 0.0866301953792572\n",
      "Epoch 1299/30000 Training Loss: 0.11346713453531265\n",
      "Epoch 1300/30000 Training Loss: 0.1371757835149765\n",
      "Epoch 1300/30000 Validation Loss: 0.10454589873552322\n",
      "Epoch 1301/30000 Training Loss: 0.11443211883306503\n",
      "Epoch 1302/30000 Training Loss: 0.1241731345653534\n",
      "Epoch 1303/30000 Training Loss: 0.10298915952444077\n",
      "Epoch 1304/30000 Training Loss: 0.11048651486635208\n",
      "Epoch 1305/30000 Training Loss: 0.1282833069562912\n",
      "Epoch 1306/30000 Training Loss: 0.1081707552075386\n",
      "Epoch 1307/30000 Training Loss: 0.11167547851800919\n",
      "Epoch 1308/30000 Training Loss: 0.12767945230007172\n",
      "Epoch 1309/30000 Training Loss: 0.12692229449748993\n",
      "Epoch 1310/30000 Training Loss: 0.12149073928594589\n",
      "Epoch 1310/30000 Validation Loss: 0.10355553030967712\n",
      "Epoch 1311/30000 Training Loss: 0.10700324922800064\n",
      "Epoch 1312/30000 Training Loss: 0.10237080603837967\n",
      "Epoch 1313/30000 Training Loss: 0.13888166844844818\n",
      "Epoch 1314/30000 Training Loss: 0.11201848834753036\n",
      "Epoch 1315/30000 Training Loss: 0.11439261585474014\n",
      "Epoch 1316/30000 Training Loss: 0.10832241177558899\n",
      "Epoch 1317/30000 Training Loss: 0.10911760479211807\n",
      "Epoch 1318/30000 Training Loss: 0.11459452658891678\n",
      "Epoch 1319/30000 Training Loss: 0.10630396753549576\n",
      "Epoch 1320/30000 Training Loss: 0.10366838425397873\n",
      "Epoch 1320/30000 Validation Loss: 0.09304992109537125\n",
      "Epoch 1321/30000 Training Loss: 0.0921686589717865\n",
      "Epoch 1322/30000 Training Loss: 0.1271682232618332\n",
      "Epoch 1323/30000 Training Loss: 0.10017979890108109\n",
      "Epoch 1324/30000 Training Loss: 0.12961041927337646\n",
      "Epoch 1325/30000 Training Loss: 0.11718857288360596\n",
      "Epoch 1326/30000 Training Loss: 0.13342882692813873\n",
      "Epoch 1327/30000 Training Loss: 0.10682658106088638\n",
      "Epoch 1328/30000 Training Loss: 0.10981746762990952\n",
      "Epoch 1329/30000 Training Loss: 0.09247130900621414\n",
      "Epoch 1330/30000 Training Loss: 0.10595216602087021\n",
      "Epoch 1330/30000 Validation Loss: 0.12981481850147247\n",
      "Epoch 1331/30000 Training Loss: 0.11830439418554306\n",
      "Epoch 1332/30000 Training Loss: 0.11582352966070175\n",
      "Epoch 1333/30000 Training Loss: 0.10693933814764023\n",
      "Epoch 1334/30000 Training Loss: 0.11395203322172165\n",
      "Epoch 1335/30000 Training Loss: 0.1021759882569313\n",
      "Epoch 1336/30000 Training Loss: 0.0926518514752388\n",
      "Epoch 1337/30000 Training Loss: 0.1224183663725853\n",
      "Epoch 1338/30000 Training Loss: 0.10668924450874329\n",
      "Epoch 1339/30000 Training Loss: 0.11716463416814804\n",
      "Epoch 1340/30000 Training Loss: 0.12282154709100723\n",
      "Epoch 1340/30000 Validation Loss: 0.08510816097259521\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08510816097259521<=============\n",
      "Epoch 1341/30000 Training Loss: 0.10515550523996353\n",
      "Epoch 1342/30000 Training Loss: 0.0948377475142479\n",
      "Epoch 1343/30000 Training Loss: 0.11207407712936401\n",
      "Epoch 1344/30000 Training Loss: 0.10433145612478256\n",
      "Epoch 1345/30000 Training Loss: 0.12203115969896317\n",
      "Epoch 1346/30000 Training Loss: 0.12028474360704422\n",
      "Epoch 1347/30000 Training Loss: 0.13501136004924774\n",
      "Epoch 1348/30000 Training Loss: 0.10432509332895279\n",
      "Epoch 1349/30000 Training Loss: 0.1275857537984848\n",
      "Epoch 1350/30000 Training Loss: 0.10910043865442276\n",
      "Epoch 1350/30000 Validation Loss: 0.10515129566192627\n",
      "Epoch 1351/30000 Training Loss: 0.10162431001663208\n",
      "Epoch 1352/30000 Training Loss: 0.12924724817276\n",
      "Epoch 1353/30000 Training Loss: 0.13344305753707886\n",
      "Epoch 1354/30000 Training Loss: 0.12730702757835388\n",
      "Epoch 1355/30000 Training Loss: 0.09401404857635498\n",
      "Epoch 1356/30000 Training Loss: 0.09767857939004898\n",
      "Epoch 1357/30000 Training Loss: 0.10936615616083145\n",
      "Epoch 1358/30000 Training Loss: 0.1021449938416481\n",
      "Epoch 1359/30000 Training Loss: 0.09124323725700378\n",
      "Epoch 1360/30000 Training Loss: 0.10867834836244583\n",
      "Epoch 1360/30000 Validation Loss: 0.1122080609202385\n",
      "Epoch 1361/30000 Training Loss: 0.098513662815094\n",
      "Epoch 1362/30000 Training Loss: 0.10978656262159348\n",
      "Epoch 1363/30000 Training Loss: 0.1162772849202156\n",
      "Epoch 1364/30000 Training Loss: 0.10905062407255173\n",
      "Epoch 1365/30000 Training Loss: 0.13625073432922363\n",
      "Epoch 1366/30000 Training Loss: 0.11709359288215637\n",
      "Epoch 1367/30000 Training Loss: 0.123096764087677\n",
      "Epoch 1368/30000 Training Loss: 0.11764603853225708\n",
      "Epoch 1369/30000 Training Loss: 0.10418752580881119\n",
      "Epoch 1370/30000 Training Loss: 0.10028507560491562\n",
      "Epoch 1370/30000 Validation Loss: 0.0986611619591713\n",
      "Epoch 1371/30000 Training Loss: 0.10426250100135803\n",
      "Epoch 1372/30000 Training Loss: 0.09743607044219971\n",
      "Epoch 1373/30000 Training Loss: 0.12169492989778519\n",
      "Epoch 1374/30000 Training Loss: 0.12022023648023605\n",
      "Epoch 1375/30000 Training Loss: 0.10010423511266708\n",
      "Epoch 1376/30000 Training Loss: 0.10192853212356567\n",
      "Epoch 1377/30000 Training Loss: 0.11716333031654358\n",
      "Epoch 1378/30000 Training Loss: 0.11409283429384232\n",
      "Epoch 1379/30000 Training Loss: 0.10763474553823471\n",
      "Epoch 1380/30000 Training Loss: 0.11771952360868454\n",
      "Epoch 1380/30000 Validation Loss: 0.13209256529808044\n",
      "Epoch 1381/30000 Training Loss: 0.12202080339193344\n",
      "Epoch 1382/30000 Training Loss: 0.14163102209568024\n",
      "Epoch 1383/30000 Training Loss: 0.09864004701375961\n",
      "Epoch 1384/30000 Training Loss: 0.09353691339492798\n",
      "Epoch 1385/30000 Training Loss: 0.10339123755693436\n",
      "Epoch 1386/30000 Training Loss: 0.08509907126426697\n",
      "Epoch 1387/30000 Training Loss: 0.11839187145233154\n",
      "Epoch 1388/30000 Training Loss: 0.1080731451511383\n",
      "Epoch 1389/30000 Training Loss: 0.0990687906742096\n",
      "Epoch 1390/30000 Training Loss: 0.10066133737564087\n",
      "Epoch 1390/30000 Validation Loss: 0.12496451288461685\n",
      "Epoch 1391/30000 Training Loss: 0.09997561573982239\n",
      "Epoch 1392/30000 Training Loss: 0.10095313936471939\n",
      "Epoch 1393/30000 Training Loss: 0.1322467178106308\n",
      "Epoch 1394/30000 Training Loss: 0.10293972492218018\n",
      "Epoch 1395/30000 Training Loss: 0.09817007184028625\n",
      "Epoch 1396/30000 Training Loss: 0.09660574793815613\n",
      "Epoch 1397/30000 Training Loss: 0.12230634689331055\n",
      "Epoch 1398/30000 Training Loss: 0.12281379848718643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1399/30000 Training Loss: 0.09389904141426086\n",
      "Epoch 1400/30000 Training Loss: 0.08748751133680344\n",
      "Epoch 1400/30000 Validation Loss: 0.11643106490373611\n",
      "Epoch 1401/30000 Training Loss: 0.1071125939488411\n",
      "Epoch 1402/30000 Training Loss: 0.1256018728017807\n",
      "Epoch 1403/30000 Training Loss: 0.13300973176956177\n",
      "Epoch 1404/30000 Training Loss: 0.14136098325252533\n",
      "Epoch 1405/30000 Training Loss: 0.11104761809110641\n",
      "Epoch 1406/30000 Training Loss: 0.10182800143957138\n",
      "Epoch 1407/30000 Training Loss: 0.10954251140356064\n",
      "Epoch 1408/30000 Training Loss: 0.11737702041864395\n",
      "Epoch 1409/30000 Training Loss: 0.11734145134687424\n",
      "Epoch 1410/30000 Training Loss: 0.09478103369474411\n",
      "Epoch 1410/30000 Validation Loss: 0.10812167078256607\n",
      "Epoch 1411/30000 Training Loss: 0.12187721580266953\n",
      "Epoch 1412/30000 Training Loss: 0.12662148475646973\n",
      "Epoch 1413/30000 Training Loss: 0.12439495325088501\n",
      "Epoch 1414/30000 Training Loss: 0.11971908062696457\n",
      "Epoch 1415/30000 Training Loss: 0.08027318120002747\n",
      "Epoch 1416/30000 Training Loss: 0.1049342155456543\n",
      "Epoch 1417/30000 Training Loss: 0.11321566253900528\n",
      "Epoch 1418/30000 Training Loss: 0.11275742202997208\n",
      "Epoch 1419/30000 Training Loss: 0.10400287061929703\n",
      "Epoch 1420/30000 Training Loss: 0.12994521856307983\n",
      "Epoch 1420/30000 Validation Loss: 0.1340509057044983\n",
      "Epoch 1421/30000 Training Loss: 0.1238393783569336\n",
      "Epoch 1422/30000 Training Loss: 0.10854652523994446\n",
      "Epoch 1423/30000 Training Loss: 0.11425679177045822\n",
      "Epoch 1424/30000 Training Loss: 0.08810929208993912\n",
      "Epoch 1425/30000 Training Loss: 0.10933340340852737\n",
      "Epoch 1426/30000 Training Loss: 0.11450550705194473\n",
      "Epoch 1427/30000 Training Loss: 0.10031285136938095\n",
      "Epoch 1428/30000 Training Loss: 0.11873909085988998\n",
      "Epoch 1429/30000 Training Loss: 0.10877734422683716\n",
      "Epoch 1430/30000 Training Loss: 0.09404295682907104\n",
      "Epoch 1430/30000 Validation Loss: 0.11753986030817032\n",
      "Epoch 1431/30000 Training Loss: 0.1317179650068283\n",
      "Epoch 1432/30000 Training Loss: 0.1250516027212143\n",
      "Epoch 1433/30000 Training Loss: 0.11550042778253555\n",
      "Epoch 1434/30000 Training Loss: 0.0954195037484169\n",
      "Epoch 1435/30000 Training Loss: 0.09982528537511826\n",
      "Epoch 1436/30000 Training Loss: 0.13188864290714264\n",
      "Epoch 1437/30000 Training Loss: 0.10818860679864883\n",
      "Epoch 1438/30000 Training Loss: 0.090175099670887\n",
      "Epoch 1439/30000 Training Loss: 0.13120697438716888\n",
      "Epoch 1440/30000 Training Loss: 0.10451605916023254\n",
      "Epoch 1440/30000 Validation Loss: 0.11411190032958984\n",
      "Epoch 1441/30000 Training Loss: 0.09328968077898026\n",
      "Epoch 1442/30000 Training Loss: 0.13907970488071442\n",
      "Epoch 1443/30000 Training Loss: 0.13465960323810577\n",
      "Epoch 1444/30000 Training Loss: 0.11370638757944107\n",
      "Epoch 1445/30000 Training Loss: 0.10473217815160751\n",
      "Epoch 1446/30000 Training Loss: 0.12445481866598129\n",
      "Epoch 1447/30000 Training Loss: 0.1326475888490677\n",
      "Epoch 1448/30000 Training Loss: 0.07915115356445312\n",
      "Epoch 1449/30000 Training Loss: 0.11938053369522095\n",
      "Epoch 1450/30000 Training Loss: 0.11795596033334732\n",
      "Epoch 1450/30000 Validation Loss: 0.0872093066573143\n",
      "Epoch 1451/30000 Training Loss: 0.1373385787010193\n",
      "Epoch 1452/30000 Training Loss: 0.10266707092523575\n",
      "Epoch 1453/30000 Training Loss: 0.10524731874465942\n",
      "Epoch 1454/30000 Training Loss: 0.09832640737295151\n",
      "Epoch 1455/30000 Training Loss: 0.11130494624376297\n",
      "Epoch 1456/30000 Training Loss: 0.10920705646276474\n",
      "Epoch 1457/30000 Training Loss: 0.10905414074659348\n",
      "Epoch 1458/30000 Training Loss: 0.10421925783157349\n",
      "Epoch 1459/30000 Training Loss: 0.10308680683374405\n",
      "Epoch 1460/30000 Training Loss: 0.1055997684597969\n",
      "Epoch 1460/30000 Validation Loss: 0.10119657963514328\n",
      "Epoch 1461/30000 Training Loss: 0.10850790143013\n",
      "Epoch 1462/30000 Training Loss: 0.1125379204750061\n",
      "Epoch 1463/30000 Training Loss: 0.10101916640996933\n",
      "Epoch 1464/30000 Training Loss: 0.0960785448551178\n",
      "Epoch 1465/30000 Training Loss: 0.10814660787582397\n",
      "Epoch 1466/30000 Training Loss: 0.14950893819332123\n",
      "Epoch 1467/30000 Training Loss: 0.08717667311429977\n",
      "Epoch 1468/30000 Training Loss: 0.10920006781816483\n",
      "Epoch 1469/30000 Training Loss: 0.1294078677892685\n",
      "Epoch 1470/30000 Training Loss: 0.08124933391809464\n",
      "Epoch 1470/30000 Validation Loss: 0.11935987323522568\n",
      "Epoch 1471/30000 Training Loss: 0.15069065988063812\n",
      "Epoch 1472/30000 Training Loss: 0.08987385034561157\n",
      "Epoch 1473/30000 Training Loss: 0.11091652512550354\n",
      "Epoch 1474/30000 Training Loss: 0.11661183834075928\n",
      "Epoch 1475/30000 Training Loss: 0.0971444845199585\n",
      "Epoch 1476/30000 Training Loss: 0.11683688312768936\n",
      "Epoch 1477/30000 Training Loss: 0.10928294062614441\n",
      "Epoch 1478/30000 Training Loss: 0.10129915922880173\n",
      "Epoch 1479/30000 Training Loss: 0.10732319205999374\n",
      "Epoch 1480/30000 Training Loss: 0.10652252286672592\n",
      "Epoch 1480/30000 Validation Loss: 0.12653504312038422\n",
      "Epoch 1481/30000 Training Loss: 0.09515917301177979\n",
      "Epoch 1482/30000 Training Loss: 0.1326625794172287\n",
      "Epoch 1483/30000 Training Loss: 0.09738435596227646\n",
      "Epoch 1484/30000 Training Loss: 0.14189833402633667\n",
      "Epoch 1485/30000 Training Loss: 0.10049229115247726\n",
      "Epoch 1486/30000 Training Loss: 0.11908329278230667\n",
      "Epoch 1487/30000 Training Loss: 0.11532989889383316\n",
      "Epoch 1488/30000 Training Loss: 0.10671081393957138\n",
      "Epoch 1489/30000 Training Loss: 0.12263396382331848\n",
      "Epoch 1490/30000 Training Loss: 0.1003786027431488\n",
      "Epoch 1490/30000 Validation Loss: 0.1107192263007164\n",
      "Epoch 1491/30000 Training Loss: 0.1387072503566742\n",
      "Epoch 1492/30000 Training Loss: 0.0768752470612526\n",
      "Epoch 1493/30000 Training Loss: 0.1118781790137291\n",
      "Epoch 1494/30000 Training Loss: 0.10975223779678345\n",
      "Epoch 1495/30000 Training Loss: 0.11027482151985168\n",
      "Epoch 1496/30000 Training Loss: 0.14701849222183228\n",
      "Epoch 1497/30000 Training Loss: 0.10156995058059692\n",
      "Epoch 1498/30000 Training Loss: 0.1318751871585846\n",
      "Epoch 1499/30000 Training Loss: 0.10306953638792038\n",
      "Epoch 1500/30000 Training Loss: 0.10526618361473083\n",
      "Epoch 1500/30000 Validation Loss: 0.09577218443155289\n",
      "Epoch 1501/30000 Training Loss: 0.10135478526353836\n",
      "Epoch 1502/30000 Training Loss: 0.09765980392694473\n",
      "Epoch 1503/30000 Training Loss: 0.11576922982931137\n",
      "Epoch 1504/30000 Training Loss: 0.10350114107131958\n",
      "Epoch 1505/30000 Training Loss: 0.12554702162742615\n",
      "Epoch 1506/30000 Training Loss: 0.10403802245855331\n",
      "Epoch 1507/30000 Training Loss: 0.10712486505508423\n",
      "Epoch 1508/30000 Training Loss: 0.08676519244909286\n",
      "Epoch 1509/30000 Training Loss: 0.13008885085582733\n",
      "Epoch 1510/30000 Training Loss: 0.10988303273916245\n",
      "Epoch 1510/30000 Validation Loss: 0.12425985932350159\n",
      "Epoch 1511/30000 Training Loss: 0.12629097700119019\n",
      "Epoch 1512/30000 Training Loss: 0.10022900253534317\n",
      "Epoch 1513/30000 Training Loss: 0.09887763112783432\n",
      "Epoch 1514/30000 Training Loss: 0.11996039003133774\n",
      "Epoch 1515/30000 Training Loss: 0.11124660819768906\n",
      "Epoch 1516/30000 Training Loss: 0.10071288794279099\n",
      "Epoch 1517/30000 Training Loss: 0.10074689239263535\n",
      "Epoch 1518/30000 Training Loss: 0.1097816750407219\n",
      "Epoch 1519/30000 Training Loss: 0.0903002917766571\n",
      "Epoch 1520/30000 Training Loss: 0.09137421101331711\n",
      "Epoch 1520/30000 Validation Loss: 0.11170575022697449\n",
      "Epoch 1521/30000 Training Loss: 0.11634695529937744\n",
      "Epoch 1522/30000 Training Loss: 0.09215641021728516\n",
      "Epoch 1523/30000 Training Loss: 0.11286678165197372\n",
      "Epoch 1524/30000 Training Loss: 0.10008474439382553\n",
      "Epoch 1525/30000 Training Loss: 0.13402952253818512\n",
      "Epoch 1526/30000 Training Loss: 0.12447508424520493\n",
      "Epoch 1527/30000 Training Loss: 0.11791586875915527\n",
      "Epoch 1528/30000 Training Loss: 0.10151030868291855\n",
      "Epoch 1529/30000 Training Loss: 0.08869455009698868\n",
      "Epoch 1530/30000 Training Loss: 0.10154664516448975\n",
      "Epoch 1530/30000 Validation Loss: 0.0933905839920044\n",
      "Epoch 1531/30000 Training Loss: 0.09965916723012924\n",
      "Epoch 1532/30000 Training Loss: 0.11284453421831131\n",
      "Epoch 1533/30000 Training Loss: 0.09304532408714294\n",
      "Epoch 1534/30000 Training Loss: 0.11866751313209534\n",
      "Epoch 1535/30000 Training Loss: 0.10456985235214233\n",
      "Epoch 1536/30000 Training Loss: 0.10290262848138809\n",
      "Epoch 1537/30000 Training Loss: 0.11286529153585434\n",
      "Epoch 1538/30000 Training Loss: 0.10494905710220337\n",
      "Epoch 1539/30000 Training Loss: 0.1144067645072937\n",
      "Epoch 1540/30000 Training Loss: 0.11039146035909653\n",
      "Epoch 1540/30000 Validation Loss: 0.10355854779481888\n",
      "Epoch 1541/30000 Training Loss: 0.12344089150428772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1542/30000 Training Loss: 0.11439164727926254\n",
      "Epoch 1543/30000 Training Loss: 0.11768759042024612\n",
      "Epoch 1544/30000 Training Loss: 0.12269138544797897\n",
      "Epoch 1545/30000 Training Loss: 0.10955438017845154\n",
      "Epoch 1546/30000 Training Loss: 0.10208526998758316\n",
      "Epoch 1547/30000 Training Loss: 0.10285753011703491\n",
      "Epoch 1548/30000 Training Loss: 0.10126189142465591\n",
      "Epoch 1549/30000 Training Loss: 0.10552690178155899\n",
      "Epoch 1550/30000 Training Loss: 0.09820175915956497\n",
      "Epoch 1550/30000 Validation Loss: 0.10156164318323135\n",
      "Epoch 1551/30000 Training Loss: 0.1342676877975464\n",
      "Epoch 1552/30000 Training Loss: 0.09024093300104141\n",
      "Epoch 1553/30000 Training Loss: 0.12610363960266113\n",
      "Epoch 1554/30000 Training Loss: 0.11337337642908096\n",
      "Epoch 1555/30000 Training Loss: 0.08841603249311447\n",
      "Epoch 1556/30000 Training Loss: 0.11730305105447769\n",
      "Epoch 1557/30000 Training Loss: 0.11465225368738174\n",
      "Epoch 1558/30000 Training Loss: 0.09995070844888687\n",
      "Epoch 1559/30000 Training Loss: 0.11456561088562012\n",
      "Epoch 1560/30000 Training Loss: 0.12974844872951508\n",
      "Epoch 1560/30000 Validation Loss: 0.12644082307815552\n",
      "Epoch 1561/30000 Training Loss: 0.10847955197095871\n",
      "Epoch 1562/30000 Training Loss: 0.10589233040809631\n",
      "Epoch 1563/30000 Training Loss: 0.1033797636628151\n",
      "Epoch 1564/30000 Training Loss: 0.10174039751291275\n",
      "Epoch 1565/30000 Training Loss: 0.09635473042726517\n",
      "Epoch 1566/30000 Training Loss: 0.10665848106145859\n",
      "Epoch 1567/30000 Training Loss: 0.08599817752838135\n",
      "Epoch 1568/30000 Training Loss: 0.09434545785188675\n",
      "Epoch 1569/30000 Training Loss: 0.07988828420639038\n",
      "Epoch 1570/30000 Training Loss: 0.11348351091146469\n",
      "Epoch 1570/30000 Validation Loss: 0.10352611541748047\n",
      "Epoch 1571/30000 Training Loss: 0.10511282831430435\n",
      "Epoch 1572/30000 Training Loss: 0.12537015974521637\n",
      "Epoch 1573/30000 Training Loss: 0.09927481412887573\n",
      "Epoch 1574/30000 Training Loss: 0.11231604218482971\n",
      "Epoch 1575/30000 Training Loss: 0.1213730201125145\n",
      "Epoch 1576/30000 Training Loss: 0.11139380186796188\n",
      "Epoch 1577/30000 Training Loss: 0.12083783745765686\n",
      "Epoch 1578/30000 Training Loss: 0.0976083055138588\n",
      "Epoch 1579/30000 Training Loss: 0.09493697434663773\n",
      "Epoch 1580/30000 Training Loss: 0.10713327676057816\n",
      "Epoch 1580/30000 Validation Loss: 0.11471745371818542\n",
      "Epoch 1581/30000 Training Loss: 0.10264060646295547\n",
      "Epoch 1582/30000 Training Loss: 0.09405892342329025\n",
      "Epoch 1583/30000 Training Loss: 0.08811024576425552\n",
      "Epoch 1584/30000 Training Loss: 0.1057671383023262\n",
      "Epoch 1585/30000 Training Loss: 0.11154647916555405\n",
      "Epoch 1586/30000 Training Loss: 0.09692851454019547\n",
      "Epoch 1587/30000 Training Loss: 0.0967598557472229\n",
      "Epoch 1588/30000 Training Loss: 0.09834947437047958\n",
      "Epoch 1589/30000 Training Loss: 0.08130853623151779\n",
      "Epoch 1590/30000 Training Loss: 0.11428821831941605\n",
      "Epoch 1590/30000 Validation Loss: 0.10040122270584106\n",
      "Epoch 1591/30000 Training Loss: 0.09638899564743042\n",
      "Epoch 1592/30000 Training Loss: 0.09867474436759949\n",
      "Epoch 1593/30000 Training Loss: 0.12529195845127106\n",
      "Epoch 1594/30000 Training Loss: 0.09140332788228989\n",
      "Epoch 1595/30000 Training Loss: 0.11282908916473389\n",
      "Epoch 1596/30000 Training Loss: 0.11727800965309143\n",
      "Epoch 1597/30000 Training Loss: 0.11186305433511734\n",
      "Epoch 1598/30000 Training Loss: 0.099401094019413\n",
      "Epoch 1599/30000 Training Loss: 0.12211815267801285\n",
      "Epoch 1600/30000 Training Loss: 0.11413701623678207\n",
      "Epoch 1600/30000 Validation Loss: 0.11747419834136963\n",
      "Epoch 1601/30000 Training Loss: 0.12292156368494034\n",
      "Epoch 1602/30000 Training Loss: 0.11381512880325317\n",
      "Epoch 1603/30000 Training Loss: 0.10552696138620377\n",
      "Epoch 1604/30000 Training Loss: 0.1258000284433365\n",
      "Epoch 1605/30000 Training Loss: 0.08878856897354126\n",
      "Epoch 1606/30000 Training Loss: 0.11440352350473404\n",
      "Epoch 1607/30000 Training Loss: 0.10263393074274063\n",
      "Epoch 1608/30000 Training Loss: 0.10722652822732925\n",
      "Epoch 1609/30000 Training Loss: 0.13086606562137604\n",
      "Epoch 1610/30000 Training Loss: 0.08235865831375122\n",
      "Epoch 1610/30000 Validation Loss: 0.11443556100130081\n",
      "Epoch 1611/30000 Training Loss: 0.13016487658023834\n",
      "Epoch 1612/30000 Training Loss: 0.11363015323877335\n",
      "Epoch 1613/30000 Training Loss: 0.11544996500015259\n",
      "Epoch 1614/30000 Training Loss: 0.1116880550980568\n",
      "Epoch 1615/30000 Training Loss: 0.09844078868627548\n",
      "Epoch 1616/30000 Training Loss: 0.1260615736246109\n",
      "Epoch 1617/30000 Training Loss: 0.0858909860253334\n",
      "Epoch 1618/30000 Training Loss: 0.07743639498949051\n",
      "Epoch 1619/30000 Training Loss: 0.13251899182796478\n",
      "Epoch 1620/30000 Training Loss: 0.11469844728708267\n",
      "Epoch 1620/30000 Validation Loss: 0.1382148414850235\n",
      "Epoch 1621/30000 Training Loss: 0.10662739723920822\n",
      "Epoch 1622/30000 Training Loss: 0.09765439480543137\n",
      "Epoch 1623/30000 Training Loss: 0.12455395609140396\n",
      "Epoch 1624/30000 Training Loss: 0.11292792111635208\n",
      "Epoch 1625/30000 Training Loss: 0.12513485550880432\n",
      "Epoch 1626/30000 Training Loss: 0.12406034022569656\n",
      "Epoch 1627/30000 Training Loss: 0.12998726963996887\n",
      "Epoch 1628/30000 Training Loss: 0.12412343174219131\n",
      "Epoch 1629/30000 Training Loss: 0.0972466766834259\n",
      "Epoch 1630/30000 Training Loss: 0.12901104986667633\n",
      "Epoch 1630/30000 Validation Loss: 0.12475075572729111\n",
      "Epoch 1631/30000 Training Loss: 0.11093614250421524\n",
      "Epoch 1632/30000 Training Loss: 0.10853920131921768\n",
      "Epoch 1633/30000 Training Loss: 0.14708665013313293\n",
      "Epoch 1634/30000 Training Loss: 0.08610666543245316\n",
      "Epoch 1635/30000 Training Loss: 0.10835590958595276\n",
      "Epoch 1636/30000 Training Loss: 0.09902894496917725\n",
      "Epoch 1637/30000 Training Loss: 0.12169299274682999\n",
      "Epoch 1638/30000 Training Loss: 0.10847065597772598\n",
      "Epoch 1639/30000 Training Loss: 0.10531529039144516\n",
      "Epoch 1640/30000 Training Loss: 0.09735270589590073\n",
      "Epoch 1640/30000 Validation Loss: 0.12519152462482452\n",
      "Epoch 1641/30000 Training Loss: 0.08866887539625168\n",
      "Epoch 1642/30000 Training Loss: 0.117622971534729\n",
      "Epoch 1643/30000 Training Loss: 0.1028342917561531\n",
      "Epoch 1644/30000 Training Loss: 0.10194191336631775\n",
      "Epoch 1645/30000 Training Loss: 0.11415410041809082\n",
      "Epoch 1646/30000 Training Loss: 0.09912586212158203\n",
      "Epoch 1647/30000 Training Loss: 0.09896370768547058\n",
      "Epoch 1648/30000 Training Loss: 0.0778784453868866\n",
      "Epoch 1649/30000 Training Loss: 0.1125645861029625\n",
      "Epoch 1650/30000 Training Loss: 0.08867162466049194\n",
      "Epoch 1650/30000 Validation Loss: 0.0935380756855011\n",
      "Epoch 1651/30000 Training Loss: 0.10779289156198502\n",
      "Epoch 1652/30000 Training Loss: 0.09028410166501999\n",
      "Epoch 1653/30000 Training Loss: 0.10551410168409348\n",
      "Epoch 1654/30000 Training Loss: 0.10378632694482803\n",
      "Epoch 1655/30000 Training Loss: 0.10440712422132492\n",
      "Epoch 1656/30000 Training Loss: 0.12159150838851929\n",
      "Epoch 1657/30000 Training Loss: 0.11752063035964966\n",
      "Epoch 1658/30000 Training Loss: 0.10697297006845474\n",
      "Epoch 1659/30000 Training Loss: 0.12543316185474396\n",
      "Epoch 1660/30000 Training Loss: 0.10224318504333496\n",
      "Epoch 1660/30000 Validation Loss: 0.1076856181025505\n",
      "Epoch 1661/30000 Training Loss: 0.0792781338095665\n",
      "Epoch 1662/30000 Training Loss: 0.11619996279478073\n",
      "Epoch 1663/30000 Training Loss: 0.10410966724157333\n",
      "Epoch 1664/30000 Training Loss: 0.11594510078430176\n",
      "Epoch 1665/30000 Training Loss: 0.1234215795993805\n",
      "Epoch 1666/30000 Training Loss: 0.12878839671611786\n",
      "Epoch 1667/30000 Training Loss: 0.093558169901371\n",
      "Epoch 1668/30000 Training Loss: 0.09575732797384262\n",
      "Epoch 1669/30000 Training Loss: 0.11005914211273193\n",
      "Epoch 1670/30000 Training Loss: 0.08822403103113174\n",
      "Epoch 1670/30000 Validation Loss: 0.11745873838663101\n",
      "Epoch 1671/30000 Training Loss: 0.10320338606834412\n",
      "Epoch 1672/30000 Training Loss: 0.10885582119226456\n",
      "Epoch 1673/30000 Training Loss: 0.09413784742355347\n",
      "Epoch 1674/30000 Training Loss: 0.10901211947202682\n",
      "Epoch 1675/30000 Training Loss: 0.08529570698738098\n",
      "Epoch 1676/30000 Training Loss: 0.10248720645904541\n",
      "Epoch 1677/30000 Training Loss: 0.10302743315696716\n",
      "Epoch 1678/30000 Training Loss: 0.09892799705266953\n",
      "Epoch 1679/30000 Training Loss: 0.1112029179930687\n",
      "Epoch 1680/30000 Training Loss: 0.09512069076299667\n",
      "Epoch 1680/30000 Validation Loss: 0.09495067596435547\n",
      "Epoch 1681/30000 Training Loss: 0.11725089699029922\n",
      "Epoch 1682/30000 Training Loss: 0.09215658903121948\n",
      "Epoch 1683/30000 Training Loss: 0.0995732843875885\n",
      "Epoch 1684/30000 Training Loss: 0.11266196519136429\n",
      "Epoch 1685/30000 Training Loss: 0.10337918251752853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1686/30000 Training Loss: 0.0975610539317131\n",
      "Epoch 1687/30000 Training Loss: 0.10563991218805313\n",
      "Epoch 1688/30000 Training Loss: 0.08266651630401611\n",
      "Epoch 1689/30000 Training Loss: 0.09736218303442001\n",
      "Epoch 1690/30000 Training Loss: 0.10369493812322617\n",
      "Epoch 1690/30000 Validation Loss: 0.10780304670333862\n",
      "Epoch 1691/30000 Training Loss: 0.11090276390314102\n",
      "Epoch 1692/30000 Training Loss: 0.08957798033952713\n",
      "Epoch 1693/30000 Training Loss: 0.09841663390398026\n",
      "Epoch 1694/30000 Training Loss: 0.08534091711044312\n",
      "Epoch 1695/30000 Training Loss: 0.11569354683160782\n",
      "Epoch 1696/30000 Training Loss: 0.1090628132224083\n",
      "Epoch 1697/30000 Training Loss: 0.10502195358276367\n",
      "Epoch 1698/30000 Training Loss: 0.09090513736009598\n",
      "Epoch 1699/30000 Training Loss: 0.10818112641572952\n",
      "Epoch 1700/30000 Training Loss: 0.11695733666419983\n",
      "Epoch 1700/30000 Validation Loss: 0.10831567645072937\n",
      "Epoch 1701/30000 Training Loss: 0.11288321763277054\n",
      "Epoch 1702/30000 Training Loss: 0.0935007706284523\n",
      "Epoch 1703/30000 Training Loss: 0.10773465782403946\n",
      "Epoch 1704/30000 Training Loss: 0.10084676742553711\n",
      "Epoch 1705/30000 Training Loss: 0.11905032396316528\n",
      "Epoch 1706/30000 Training Loss: 0.1213415265083313\n",
      "Epoch 1707/30000 Training Loss: 0.08875244855880737\n",
      "Epoch 1708/30000 Training Loss: 0.1117262914776802\n",
      "Epoch 1709/30000 Training Loss: 0.11933121830224991\n",
      "Epoch 1710/30000 Training Loss: 0.09623727947473526\n",
      "Epoch 1710/30000 Validation Loss: 0.1223384365439415\n",
      "Epoch 1711/30000 Training Loss: 0.12887954711914062\n",
      "Epoch 1712/30000 Training Loss: 0.11528227478265762\n",
      "Epoch 1713/30000 Training Loss: 0.12928247451782227\n",
      "Epoch 1714/30000 Training Loss: 0.12403768301010132\n",
      "Epoch 1715/30000 Training Loss: 0.09144819527864456\n",
      "Epoch 1716/30000 Training Loss: 0.09883957356214523\n",
      "Epoch 1717/30000 Training Loss: 0.11112558841705322\n",
      "Epoch 1718/30000 Training Loss: 0.0953289195895195\n",
      "Epoch 1719/30000 Training Loss: 0.10207343101501465\n",
      "Epoch 1720/30000 Training Loss: 0.09722167998552322\n",
      "Epoch 1720/30000 Validation Loss: 0.10664200782775879\n",
      "Epoch 1721/30000 Training Loss: 0.10554759949445724\n",
      "Epoch 1722/30000 Training Loss: 0.10556257516145706\n",
      "Epoch 1723/30000 Training Loss: 0.10471376031637192\n",
      "Epoch 1724/30000 Training Loss: 0.10591146349906921\n",
      "Epoch 1725/30000 Training Loss: 0.09279011934995651\n",
      "Epoch 1726/30000 Training Loss: 0.08748292922973633\n",
      "Epoch 1727/30000 Training Loss: 0.10924073308706284\n",
      "Epoch 1728/30000 Training Loss: 0.10198116302490234\n",
      "Epoch 1729/30000 Training Loss: 0.09026869386434555\n",
      "Epoch 1730/30000 Training Loss: 0.10650519281625748\n",
      "Epoch 1730/30000 Validation Loss: 0.13471752405166626\n",
      "Epoch 1731/30000 Training Loss: 0.10248231887817383\n",
      "Epoch 1732/30000 Training Loss: 0.0910702720284462\n",
      "Epoch 1733/30000 Training Loss: 0.1104397177696228\n",
      "Epoch 1734/30000 Training Loss: 0.11121752113103867\n",
      "Epoch 1735/30000 Training Loss: 0.09114591032266617\n",
      "Epoch 1736/30000 Training Loss: 0.09868375211954117\n",
      "Epoch 1737/30000 Training Loss: 0.12308009713888168\n",
      "Epoch 1738/30000 Training Loss: 0.11205623298883438\n",
      "Epoch 1739/30000 Training Loss: 0.11693781614303589\n",
      "Epoch 1740/30000 Training Loss: 0.1064525619149208\n",
      "Epoch 1740/30000 Validation Loss: 0.09072277694940567\n",
      "Epoch 1741/30000 Training Loss: 0.08906296640634537\n",
      "Epoch 1742/30000 Training Loss: 0.09911935776472092\n",
      "Epoch 1743/30000 Training Loss: 0.1124008372426033\n",
      "Epoch 1744/30000 Training Loss: 0.09946169704198837\n",
      "Epoch 1745/30000 Training Loss: 0.09487324953079224\n",
      "Epoch 1746/30000 Training Loss: 0.08115601539611816\n",
      "Epoch 1747/30000 Training Loss: 0.09456369280815125\n",
      "Epoch 1748/30000 Training Loss: 0.11615100502967834\n",
      "Epoch 1749/30000 Training Loss: 0.10732273012399673\n",
      "Epoch 1750/30000 Training Loss: 0.11376708745956421\n",
      "Epoch 1750/30000 Validation Loss: 0.1088123694062233\n",
      "Epoch 1751/30000 Training Loss: 0.1274372786283493\n",
      "Epoch 1752/30000 Training Loss: 0.120992012321949\n",
      "Epoch 1753/30000 Training Loss: 0.12345623224973679\n",
      "Epoch 1754/30000 Training Loss: 0.0983802080154419\n",
      "Epoch 1755/30000 Training Loss: 0.09938608855009079\n",
      "Epoch 1756/30000 Training Loss: 0.11456478387117386\n",
      "Epoch 1757/30000 Training Loss: 0.09945601969957352\n",
      "Epoch 1758/30000 Training Loss: 0.1039280965924263\n",
      "Epoch 1759/30000 Training Loss: 0.10610265284776688\n",
      "Epoch 1760/30000 Training Loss: 0.1041654422879219\n",
      "Epoch 1760/30000 Validation Loss: 0.09320399165153503\n",
      "Epoch 1761/30000 Training Loss: 0.10189589112997055\n",
      "Epoch 1762/30000 Training Loss: 0.11488387733697891\n",
      "Epoch 1763/30000 Training Loss: 0.09677311033010483\n",
      "Epoch 1764/30000 Training Loss: 0.09945806860923767\n",
      "Epoch 1765/30000 Training Loss: 0.09055238217115402\n",
      "Epoch 1766/30000 Training Loss: 0.11876127868890762\n",
      "Epoch 1767/30000 Training Loss: 0.10537263005971909\n",
      "Epoch 1768/30000 Training Loss: 0.1054486334323883\n",
      "Epoch 1769/30000 Training Loss: 0.10089381784200668\n",
      "Epoch 1770/30000 Training Loss: 0.11119814962148666\n",
      "Epoch 1770/30000 Validation Loss: 0.10062304884195328\n",
      "Epoch 1771/30000 Training Loss: 0.11866927891969681\n",
      "Epoch 1772/30000 Training Loss: 0.10990651696920395\n",
      "Epoch 1773/30000 Training Loss: 0.11637052148580551\n",
      "Epoch 1774/30000 Training Loss: 0.12327618151903152\n",
      "Epoch 1775/30000 Training Loss: 0.10100271552801132\n",
      "Epoch 1776/30000 Training Loss: 0.10664793848991394\n",
      "Epoch 1777/30000 Training Loss: 0.08883669227361679\n",
      "Epoch 1778/30000 Training Loss: 0.13196180760860443\n",
      "Epoch 1779/30000 Training Loss: 0.11038485914468765\n",
      "Epoch 1780/30000 Training Loss: 0.11288708448410034\n",
      "Epoch 1780/30000 Validation Loss: 0.11843422055244446\n",
      "Epoch 1781/30000 Training Loss: 0.09231099486351013\n",
      "Epoch 1782/30000 Training Loss: 0.10723999887704849\n",
      "Epoch 1783/30000 Training Loss: 0.10951589792966843\n",
      "Epoch 1784/30000 Training Loss: 0.09123462438583374\n",
      "Epoch 1785/30000 Training Loss: 0.09719332307577133\n",
      "Epoch 1786/30000 Training Loss: 0.11115896701812744\n",
      "Epoch 1787/30000 Training Loss: 0.11108294874429703\n",
      "Epoch 1788/30000 Training Loss: 0.10281825065612793\n",
      "Epoch 1789/30000 Training Loss: 0.10638060420751572\n",
      "Epoch 1790/30000 Training Loss: 0.12539629638195038\n",
      "Epoch 1790/30000 Validation Loss: 0.11440330743789673\n",
      "Epoch 1791/30000 Training Loss: 0.09161994606256485\n",
      "Epoch 1792/30000 Training Loss: 0.09962952882051468\n",
      "Epoch 1793/30000 Training Loss: 0.09047341346740723\n",
      "Epoch 1794/30000 Training Loss: 0.10671138763427734\n",
      "Epoch 1795/30000 Training Loss: 0.1354445368051529\n",
      "Epoch 1796/30000 Training Loss: 0.09470829367637634\n",
      "Epoch 1797/30000 Training Loss: 0.08675559610128403\n",
      "Epoch 1798/30000 Training Loss: 0.1119990348815918\n",
      "Epoch 1799/30000 Training Loss: 0.12060103565454483\n",
      "Epoch 1800/30000 Training Loss: 0.10750836133956909\n",
      "Epoch 1800/30000 Validation Loss: 0.09346111863851547\n",
      "Epoch 1801/30000 Training Loss: 0.10984136909246445\n",
      "Epoch 1802/30000 Training Loss: 0.09414703398942947\n",
      "Epoch 1803/30000 Training Loss: 0.10846181958913803\n",
      "Epoch 1804/30000 Training Loss: 0.1217455044388771\n",
      "Epoch 1805/30000 Training Loss: 0.09752381592988968\n",
      "Epoch 1806/30000 Training Loss: 0.12621349096298218\n",
      "Epoch 1807/30000 Training Loss: 0.10738781839609146\n",
      "Epoch 1808/30000 Training Loss: 0.08473392575979233\n",
      "Epoch 1809/30000 Training Loss: 0.1293906718492508\n",
      "Epoch 1810/30000 Training Loss: 0.11690465360879898\n",
      "Epoch 1810/30000 Validation Loss: 0.08820769935846329\n",
      "Epoch 1811/30000 Training Loss: 0.11833411455154419\n",
      "Epoch 1812/30000 Training Loss: 0.10546386986970901\n",
      "Epoch 1813/30000 Training Loss: 0.09377241879701614\n",
      "Epoch 1814/30000 Training Loss: 0.11003994196653366\n",
      "Epoch 1815/30000 Training Loss: 0.08628454059362411\n",
      "Epoch 1816/30000 Training Loss: 0.09207826852798462\n",
      "Epoch 1817/30000 Training Loss: 0.1039477288722992\n",
      "Epoch 1818/30000 Training Loss: 0.09887802600860596\n",
      "Epoch 1819/30000 Training Loss: 0.09806185215711594\n",
      "Epoch 1820/30000 Training Loss: 0.12981049716472626\n",
      "Epoch 1820/30000 Validation Loss: 0.10584554076194763\n",
      "Epoch 1821/30000 Training Loss: 0.08773797750473022\n",
      "Epoch 1822/30000 Training Loss: 0.11730436235666275\n",
      "Epoch 1823/30000 Training Loss: 0.0995720699429512\n",
      "Epoch 1824/30000 Training Loss: 0.12627065181732178\n",
      "Epoch 1825/30000 Training Loss: 0.10760075598955154\n",
      "Epoch 1826/30000 Training Loss: 0.10247723013162613\n",
      "Epoch 1827/30000 Training Loss: 0.11567067354917526\n",
      "Epoch 1828/30000 Training Loss: 0.08762300759553909\n",
      "Epoch 1829/30000 Training Loss: 0.11410532146692276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1830/30000 Training Loss: 0.09378168731927872\n",
      "Epoch 1830/30000 Validation Loss: 0.08614423125982285\n",
      "Epoch 1831/30000 Training Loss: 0.09779868274927139\n",
      "Epoch 1832/30000 Training Loss: 0.130037322640419\n",
      "Epoch 1833/30000 Training Loss: 0.10397196561098099\n",
      "Epoch 1834/30000 Training Loss: 0.11040839552879333\n",
      "Epoch 1835/30000 Training Loss: 0.11839870363473892\n",
      "Epoch 1836/30000 Training Loss: 0.12691165506839752\n",
      "Epoch 1837/30000 Training Loss: 0.11311707645654678\n",
      "Epoch 1838/30000 Training Loss: 0.10642305016517639\n",
      "Epoch 1839/30000 Training Loss: 0.07988515496253967\n",
      "Epoch 1840/30000 Training Loss: 0.10030408948659897\n",
      "Epoch 1840/30000 Validation Loss: 0.1033456027507782\n",
      "Epoch 1841/30000 Training Loss: 0.08134150505065918\n",
      "Epoch 1842/30000 Training Loss: 0.11306091398000717\n",
      "Epoch 1843/30000 Training Loss: 0.09847241640090942\n",
      "Epoch 1844/30000 Training Loss: 0.09619244188070297\n",
      "Epoch 1845/30000 Training Loss: 0.12143240123987198\n",
      "Epoch 1846/30000 Training Loss: 0.10541748255491257\n",
      "Epoch 1847/30000 Training Loss: 0.11510541290044785\n",
      "Epoch 1848/30000 Training Loss: 0.09947490692138672\n",
      "Epoch 1849/30000 Training Loss: 0.11945156008005142\n",
      "Epoch 1850/30000 Training Loss: 0.1030864492058754\n",
      "Epoch 1850/30000 Validation Loss: 0.08690005540847778\n",
      "Epoch 1851/30000 Training Loss: 0.08279251307249069\n",
      "Epoch 1852/30000 Training Loss: 0.1058652326464653\n",
      "Epoch 1853/30000 Training Loss: 0.12806062400341034\n",
      "Epoch 1854/30000 Training Loss: 0.10727851837873459\n",
      "Epoch 1855/30000 Training Loss: 0.10151475667953491\n",
      "Epoch 1856/30000 Training Loss: 0.1070496141910553\n",
      "Epoch 1857/30000 Training Loss: 0.08447865396738052\n",
      "Epoch 1858/30000 Training Loss: 0.11259084939956665\n",
      "Epoch 1859/30000 Training Loss: 0.10441344976425171\n",
      "Epoch 1860/30000 Training Loss: 0.10670366138219833\n",
      "Epoch 1860/30000 Validation Loss: 0.12140478938817978\n",
      "Epoch 1861/30000 Training Loss: 0.10664930939674377\n",
      "Epoch 1862/30000 Training Loss: 0.08785694092512131\n",
      "Epoch 1863/30000 Training Loss: 0.09472406655550003\n",
      "Epoch 1864/30000 Training Loss: 0.09635767340660095\n",
      "Epoch 1865/30000 Training Loss: 0.11062145978212357\n",
      "Epoch 1866/30000 Training Loss: 0.1183069720864296\n",
      "Epoch 1867/30000 Training Loss: 0.11835860460996628\n",
      "Epoch 1868/30000 Training Loss: 0.08550339937210083\n",
      "Epoch 1869/30000 Training Loss: 0.08737657219171524\n",
      "Epoch 1870/30000 Training Loss: 0.0964522734284401\n",
      "Epoch 1870/30000 Validation Loss: 0.0858096182346344\n",
      "Epoch 1871/30000 Training Loss: 0.10588657855987549\n",
      "Epoch 1872/30000 Training Loss: 0.12239432334899902\n",
      "Epoch 1873/30000 Training Loss: 0.09888696670532227\n",
      "Epoch 1874/30000 Training Loss: 0.10735881328582764\n",
      "Epoch 1875/30000 Training Loss: 0.10944028943777084\n",
      "Epoch 1876/30000 Training Loss: 0.10723360627889633\n",
      "Epoch 1877/30000 Training Loss: 0.09975939244031906\n",
      "Epoch 1878/30000 Training Loss: 0.12150025367736816\n",
      "Epoch 1879/30000 Training Loss: 0.10161540657281876\n",
      "Epoch 1880/30000 Training Loss: 0.08909192681312561\n",
      "Epoch 1880/30000 Validation Loss: 0.08481382578611374\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08481382578611374<=============\n",
      "Epoch 1881/30000 Training Loss: 0.1032397523522377\n",
      "Epoch 1882/30000 Training Loss: 0.11299749463796616\n",
      "Epoch 1883/30000 Training Loss: 0.11146435886621475\n",
      "Epoch 1884/30000 Training Loss: 0.09295669943094254\n",
      "Epoch 1885/30000 Training Loss: 0.10051778703927994\n",
      "Epoch 1886/30000 Training Loss: 0.11804118752479553\n",
      "Epoch 1887/30000 Training Loss: 0.09881069511175156\n",
      "Epoch 1888/30000 Training Loss: 0.10020235925912857\n",
      "Epoch 1889/30000 Training Loss: 0.10720209032297134\n",
      "Epoch 1890/30000 Training Loss: 0.10372503846883774\n",
      "Epoch 1890/30000 Validation Loss: 0.0938149020075798\n",
      "Epoch 1891/30000 Training Loss: 0.0926223024725914\n",
      "Epoch 1892/30000 Training Loss: 0.09389448910951614\n",
      "Epoch 1893/30000 Training Loss: 0.08334684371948242\n",
      "Epoch 1894/30000 Training Loss: 0.11737751215696335\n",
      "Epoch 1895/30000 Training Loss: 0.09518778324127197\n",
      "Epoch 1896/30000 Training Loss: 0.10296037793159485\n",
      "Epoch 1897/30000 Training Loss: 0.09843667596578598\n",
      "Epoch 1898/30000 Training Loss: 0.13599009811878204\n",
      "Epoch 1899/30000 Training Loss: 0.09380561858415604\n",
      "Epoch 1900/30000 Training Loss: 0.10038036108016968\n",
      "Epoch 1900/30000 Validation Loss: 0.10600385814905167\n",
      "Epoch 1901/30000 Training Loss: 0.0801190510392189\n",
      "Epoch 1902/30000 Training Loss: 0.12068486213684082\n",
      "Epoch 1903/30000 Training Loss: 0.0964089035987854\n",
      "Epoch 1904/30000 Training Loss: 0.11687681823968887\n",
      "Epoch 1905/30000 Training Loss: 0.13084453344345093\n",
      "Epoch 1906/30000 Training Loss: 0.10089889913797379\n",
      "Epoch 1907/30000 Training Loss: 0.11627490073442459\n",
      "Epoch 1908/30000 Training Loss: 0.09787753969430923\n",
      "Epoch 1909/30000 Training Loss: 0.08709671348333359\n",
      "Epoch 1910/30000 Training Loss: 0.09046586602926254\n",
      "Epoch 1910/30000 Validation Loss: 0.11698856204748154\n",
      "Epoch 1911/30000 Training Loss: 0.0989861711859703\n",
      "Epoch 1912/30000 Training Loss: 0.10401823371648788\n",
      "Epoch 1913/30000 Training Loss: 0.08349583297967911\n",
      "Epoch 1914/30000 Training Loss: 0.09681371599435806\n",
      "Epoch 1915/30000 Training Loss: 0.11231771856546402\n",
      "Epoch 1916/30000 Training Loss: 0.11032363772392273\n",
      "Epoch 1917/30000 Training Loss: 0.10826367884874344\n",
      "Epoch 1918/30000 Training Loss: 0.07861700654029846\n",
      "Epoch 1919/30000 Training Loss: 0.11442320793867111\n",
      "Epoch 1920/30000 Training Loss: 0.09087156504392624\n",
      "Epoch 1920/30000 Validation Loss: 0.09167622774839401\n",
      "Epoch 1921/30000 Training Loss: 0.11529520153999329\n",
      "Epoch 1922/30000 Training Loss: 0.08946084976196289\n",
      "Epoch 1923/30000 Training Loss: 0.09333445876836777\n",
      "Epoch 1924/30000 Training Loss: 0.105352021753788\n",
      "Epoch 1925/30000 Training Loss: 0.10842674970626831\n",
      "Epoch 1926/30000 Training Loss: 0.09423007816076279\n",
      "Epoch 1927/30000 Training Loss: 0.10333631187677383\n",
      "Epoch 1928/30000 Training Loss: 0.09009155631065369\n",
      "Epoch 1929/30000 Training Loss: 0.09524957090616226\n",
      "Epoch 1930/30000 Training Loss: 0.10421367734670639\n",
      "Epoch 1930/30000 Validation Loss: 0.1016937866806984\n",
      "Epoch 1931/30000 Training Loss: 0.1101488396525383\n",
      "Epoch 1932/30000 Training Loss: 0.10447313636541367\n",
      "Epoch 1933/30000 Training Loss: 0.10083156824111938\n",
      "Epoch 1934/30000 Training Loss: 0.08247511088848114\n",
      "Epoch 1935/30000 Training Loss: 0.12416078895330429\n",
      "Epoch 1936/30000 Training Loss: 0.1144958958029747\n",
      "Epoch 1937/30000 Training Loss: 0.09750596433877945\n",
      "Epoch 1938/30000 Training Loss: 0.10446628928184509\n",
      "Epoch 1939/30000 Training Loss: 0.095704086124897\n",
      "Epoch 1940/30000 Training Loss: 0.09371155500411987\n",
      "Epoch 1940/30000 Validation Loss: 0.0919119119644165\n",
      "Epoch 1941/30000 Training Loss: 0.10456299781799316\n",
      "Epoch 1942/30000 Training Loss: 0.10464499145746231\n",
      "Epoch 1943/30000 Training Loss: 0.10782059282064438\n",
      "Epoch 1944/30000 Training Loss: 0.11117023229598999\n",
      "Epoch 1945/30000 Training Loss: 0.09383133053779602\n",
      "Epoch 1946/30000 Training Loss: 0.0831926241517067\n",
      "Epoch 1947/30000 Training Loss: 0.10049551725387573\n",
      "Epoch 1948/30000 Training Loss: 0.10738623887300491\n",
      "Epoch 1949/30000 Training Loss: 0.11018913984298706\n",
      "Epoch 1950/30000 Training Loss: 0.11287584155797958\n",
      "Epoch 1950/30000 Validation Loss: 0.0829484760761261\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0829484760761261<=============\n",
      "Epoch 1951/30000 Training Loss: 0.09588102251291275\n",
      "Epoch 1952/30000 Training Loss: 0.11669095605611801\n",
      "Epoch 1953/30000 Training Loss: 0.12080615758895874\n",
      "Epoch 1954/30000 Training Loss: 0.10403751581907272\n",
      "Epoch 1955/30000 Training Loss: 0.12050191313028336\n",
      "Epoch 1956/30000 Training Loss: 0.08494023233652115\n",
      "Epoch 1957/30000 Training Loss: 0.08982113003730774\n",
      "Epoch 1958/30000 Training Loss: 0.10507782548666\n",
      "Epoch 1959/30000 Training Loss: 0.10307712107896805\n",
      "Epoch 1960/30000 Training Loss: 0.10345149785280228\n",
      "Epoch 1960/30000 Validation Loss: 0.08101290464401245\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08101290464401245<=============\n",
      "Epoch 1961/30000 Training Loss: 0.09610667824745178\n",
      "Epoch 1962/30000 Training Loss: 0.10228857398033142\n",
      "Epoch 1963/30000 Training Loss: 0.11456179618835449\n",
      "Epoch 1964/30000 Training Loss: 0.14478738605976105\n",
      "Epoch 1965/30000 Training Loss: 0.08257528394460678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1966/30000 Training Loss: 0.08197087049484253\n",
      "Epoch 1967/30000 Training Loss: 0.11071152240037918\n",
      "Epoch 1968/30000 Training Loss: 0.07948683947324753\n",
      "Epoch 1969/30000 Training Loss: 0.11323350667953491\n",
      "Epoch 1970/30000 Training Loss: 0.10844322293996811\n",
      "Epoch 1970/30000 Validation Loss: 0.11355224996805191\n",
      "Epoch 1971/30000 Training Loss: 0.09419166296720505\n",
      "Epoch 1972/30000 Training Loss: 0.08065646886825562\n",
      "Epoch 1973/30000 Training Loss: 0.08199390769004822\n",
      "Epoch 1974/30000 Training Loss: 0.08507108688354492\n",
      "Epoch 1975/30000 Training Loss: 0.13702325522899628\n",
      "Epoch 1976/30000 Training Loss: 0.1258843094110489\n",
      "Epoch 1977/30000 Training Loss: 0.12595929205417633\n",
      "Epoch 1978/30000 Training Loss: 0.08997204154729843\n",
      "Epoch 1979/30000 Training Loss: 0.09037834405899048\n",
      "Epoch 1980/30000 Training Loss: 0.11839470267295837\n",
      "Epoch 1980/30000 Validation Loss: 0.09893542528152466\n",
      "Epoch 1981/30000 Training Loss: 0.11316797137260437\n",
      "Epoch 1982/30000 Training Loss: 0.0961681604385376\n",
      "Epoch 1983/30000 Training Loss: 0.10966701060533524\n",
      "Epoch 1984/30000 Training Loss: 0.10080450028181076\n",
      "Epoch 1985/30000 Training Loss: 0.09861553460359573\n",
      "Epoch 1986/30000 Training Loss: 0.12435264140367508\n",
      "Epoch 1987/30000 Training Loss: 0.09793324023485184\n",
      "Epoch 1988/30000 Training Loss: 0.12145068496465683\n",
      "Epoch 1989/30000 Training Loss: 0.13821645081043243\n",
      "Epoch 1990/30000 Training Loss: 0.09901198744773865\n",
      "Epoch 1990/30000 Validation Loss: 0.11198633909225464\n",
      "Epoch 1991/30000 Training Loss: 0.10074111819267273\n",
      "Epoch 1992/30000 Training Loss: 0.09326095134019852\n",
      "Epoch 1993/30000 Training Loss: 0.10445461422204971\n",
      "Epoch 1994/30000 Training Loss: 0.09585576504468918\n",
      "Epoch 1995/30000 Training Loss: 0.0814703032374382\n",
      "Epoch 1996/30000 Training Loss: 0.09755659103393555\n",
      "Epoch 1997/30000 Training Loss: 0.08315863460302353\n",
      "Epoch 1998/30000 Training Loss: 0.1003919243812561\n",
      "Epoch 1999/30000 Training Loss: 0.10445795208215714\n",
      "Epoch 2000/30000 Training Loss: 0.09030093997716904\n",
      "Epoch 2000/30000 Validation Loss: 0.10259298235177994\n",
      "Epoch 2001/30000 Training Loss: 0.11702796071767807\n",
      "Epoch 2002/30000 Training Loss: 0.10387689620256424\n",
      "Epoch 2003/30000 Training Loss: 0.09768442064523697\n",
      "Epoch 2004/30000 Training Loss: 0.09753488749265671\n",
      "Epoch 2005/30000 Training Loss: 0.1033894494175911\n",
      "Epoch 2006/30000 Training Loss: 0.0982404425740242\n",
      "Epoch 2007/30000 Training Loss: 0.14060144126415253\n",
      "Epoch 2008/30000 Training Loss: 0.10524650663137436\n",
      "Epoch 2009/30000 Training Loss: 0.10513297468423843\n",
      "Epoch 2010/30000 Training Loss: 0.0892300084233284\n",
      "Epoch 2010/30000 Validation Loss: 0.08775532245635986\n",
      "Epoch 2011/30000 Training Loss: 0.10918266326189041\n",
      "Epoch 2012/30000 Training Loss: 0.08958825469017029\n",
      "Epoch 2013/30000 Training Loss: 0.11545521020889282\n",
      "Epoch 2014/30000 Training Loss: 0.08701293915510178\n",
      "Epoch 2015/30000 Training Loss: 0.11925236135721207\n",
      "Epoch 2016/30000 Training Loss: 0.109494648873806\n",
      "Epoch 2017/30000 Training Loss: 0.0745244026184082\n",
      "Epoch 2018/30000 Training Loss: 0.10764861851930618\n",
      "Epoch 2019/30000 Training Loss: 0.10663118958473206\n",
      "Epoch 2020/30000 Training Loss: 0.12218636274337769\n",
      "Epoch 2020/30000 Validation Loss: 0.09588125348091125\n",
      "Epoch 2021/30000 Training Loss: 0.12444144487380981\n",
      "Epoch 2022/30000 Training Loss: 0.08098727464675903\n",
      "Epoch 2023/30000 Training Loss: 0.09397780150175095\n",
      "Epoch 2024/30000 Training Loss: 0.08675836771726608\n",
      "Epoch 2025/30000 Training Loss: 0.1117062196135521\n",
      "Epoch 2026/30000 Training Loss: 0.0982421338558197\n",
      "Epoch 2027/30000 Training Loss: 0.10062864422798157\n",
      "Epoch 2028/30000 Training Loss: 0.08458446711301804\n",
      "Epoch 2029/30000 Training Loss: 0.10216542333364487\n",
      "Epoch 2030/30000 Training Loss: 0.11264451593160629\n",
      "Epoch 2030/30000 Validation Loss: 0.08186321705579758\n",
      "Epoch 2031/30000 Training Loss: 0.10949995368719101\n",
      "Epoch 2032/30000 Training Loss: 0.11369166523218155\n",
      "Epoch 2033/30000 Training Loss: 0.11014523357152939\n",
      "Epoch 2034/30000 Training Loss: 0.13590411841869354\n",
      "Epoch 2035/30000 Training Loss: 0.08617237210273743\n",
      "Epoch 2036/30000 Training Loss: 0.11013936996459961\n",
      "Epoch 2037/30000 Training Loss: 0.11221766471862793\n",
      "Epoch 2038/30000 Training Loss: 0.10265740007162094\n",
      "Epoch 2039/30000 Training Loss: 0.11311880499124527\n",
      "Epoch 2040/30000 Training Loss: 0.11230482906103134\n",
      "Epoch 2040/30000 Validation Loss: 0.12411031872034073\n",
      "Epoch 2041/30000 Training Loss: 0.11843445152044296\n",
      "Epoch 2042/30000 Training Loss: 0.11472611874341965\n",
      "Epoch 2043/30000 Training Loss: 0.0942692831158638\n",
      "Epoch 2044/30000 Training Loss: 0.11902708560228348\n",
      "Epoch 2045/30000 Training Loss: 0.09616836160421371\n",
      "Epoch 2046/30000 Training Loss: 0.08619517087936401\n",
      "Epoch 2047/30000 Training Loss: 0.11123856902122498\n",
      "Epoch 2048/30000 Training Loss: 0.0788814052939415\n",
      "Epoch 2049/30000 Training Loss: 0.09539888054132462\n",
      "Epoch 2050/30000 Training Loss: 0.10833963006734848\n",
      "Epoch 2050/30000 Validation Loss: 0.08965811133384705\n",
      "Epoch 2051/30000 Training Loss: 0.12032246589660645\n",
      "Epoch 2052/30000 Training Loss: 0.11525005102157593\n",
      "Epoch 2053/30000 Training Loss: 0.09799978882074356\n",
      "Epoch 2054/30000 Training Loss: 0.10458579659461975\n",
      "Epoch 2055/30000 Training Loss: 0.11873935908079147\n",
      "Epoch 2056/30000 Training Loss: 0.13702082633972168\n",
      "Epoch 2057/30000 Training Loss: 0.1157049611210823\n",
      "Epoch 2058/30000 Training Loss: 0.10929272323846817\n",
      "Epoch 2059/30000 Training Loss: 0.10332503914833069\n",
      "Epoch 2060/30000 Training Loss: 0.10098155587911606\n",
      "Epoch 2060/30000 Validation Loss: 0.10239731520414352\n",
      "Epoch 2061/30000 Training Loss: 0.09763667732477188\n",
      "Epoch 2062/30000 Training Loss: 0.09123779088258743\n",
      "Epoch 2063/30000 Training Loss: 0.09559422731399536\n",
      "Epoch 2064/30000 Training Loss: 0.1008138582110405\n",
      "Epoch 2065/30000 Training Loss: 0.09762993454933167\n",
      "Epoch 2066/30000 Training Loss: 0.1112150177359581\n",
      "Epoch 2067/30000 Training Loss: 0.11253553628921509\n",
      "Epoch 2068/30000 Training Loss: 0.10552656650543213\n",
      "Epoch 2069/30000 Training Loss: 0.09928195923566818\n",
      "Epoch 2070/30000 Training Loss: 0.10164972394704819\n",
      "Epoch 2070/30000 Validation Loss: 0.09543135017156601\n",
      "Epoch 2071/30000 Training Loss: 0.09787482023239136\n",
      "Epoch 2072/30000 Training Loss: 0.09940072149038315\n",
      "Epoch 2073/30000 Training Loss: 0.1426667720079422\n",
      "Epoch 2074/30000 Training Loss: 0.09410322457551956\n",
      "Epoch 2075/30000 Training Loss: 0.09274416416883469\n",
      "Epoch 2076/30000 Training Loss: 0.11384093761444092\n",
      "Epoch 2077/30000 Training Loss: 0.09392567723989487\n",
      "Epoch 2078/30000 Training Loss: 0.10661699622869492\n",
      "Epoch 2079/30000 Training Loss: 0.10543686151504517\n",
      "Epoch 2080/30000 Training Loss: 0.10006167739629745\n",
      "Epoch 2080/30000 Validation Loss: 0.09660693258047104\n",
      "Epoch 2081/30000 Training Loss: 0.09149328619241714\n",
      "Epoch 2082/30000 Training Loss: 0.11752896755933762\n",
      "Epoch 2083/30000 Training Loss: 0.08243764936923981\n",
      "Epoch 2084/30000 Training Loss: 0.0982518419623375\n",
      "Epoch 2085/30000 Training Loss: 0.09462391585111618\n",
      "Epoch 2086/30000 Training Loss: 0.11517234891653061\n",
      "Epoch 2087/30000 Training Loss: 0.09360899776220322\n",
      "Epoch 2088/30000 Training Loss: 0.11175107955932617\n",
      "Epoch 2089/30000 Training Loss: 0.10329034179449081\n",
      "Epoch 2090/30000 Training Loss: 0.09850243479013443\n",
      "Epoch 2090/30000 Validation Loss: 0.09119486063718796\n",
      "Epoch 2091/30000 Training Loss: 0.12149211019277573\n",
      "Epoch 2092/30000 Training Loss: 0.08096262067556381\n",
      "Epoch 2093/30000 Training Loss: 0.09068012982606888\n",
      "Epoch 2094/30000 Training Loss: 0.08347928524017334\n",
      "Epoch 2095/30000 Training Loss: 0.090445376932621\n",
      "Epoch 2096/30000 Training Loss: 0.10083678364753723\n",
      "Epoch 2097/30000 Training Loss: 0.11205300688743591\n",
      "Epoch 2098/30000 Training Loss: 0.08775970339775085\n",
      "Epoch 2099/30000 Training Loss: 0.07843037694692612\n",
      "Epoch 2100/30000 Training Loss: 0.10857752710580826\n",
      "Epoch 2100/30000 Validation Loss: 0.10194598883390427\n",
      "Epoch 2101/30000 Training Loss: 0.11954433470964432\n",
      "Epoch 2102/30000 Training Loss: 0.09754916280508041\n",
      "Epoch 2103/30000 Training Loss: 0.09576388448476791\n",
      "Epoch 2104/30000 Training Loss: 0.11791756749153137\n",
      "Epoch 2105/30000 Training Loss: 0.11506593972444534\n",
      "Epoch 2106/30000 Training Loss: 0.11283247917890549\n",
      "Epoch 2107/30000 Training Loss: 0.09464878588914871\n",
      "Epoch 2108/30000 Training Loss: 0.11160245537757874\n",
      "Epoch 2109/30000 Training Loss: 0.10950052738189697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2110/30000 Training Loss: 0.12126433849334717\n",
      "Epoch 2110/30000 Validation Loss: 0.09935792535543442\n",
      "Epoch 2111/30000 Training Loss: 0.07946711778640747\n",
      "Epoch 2112/30000 Training Loss: 0.1254633516073227\n",
      "Epoch 2113/30000 Training Loss: 0.09713279455900192\n",
      "Epoch 2114/30000 Training Loss: 0.10344254970550537\n",
      "Epoch 2115/30000 Training Loss: 0.08912410587072372\n",
      "Epoch 2116/30000 Training Loss: 0.09532485157251358\n",
      "Epoch 2117/30000 Training Loss: 0.1045181155204773\n",
      "Epoch 2118/30000 Training Loss: 0.11245701462030411\n",
      "Epoch 2119/30000 Training Loss: 0.08569091558456421\n",
      "Epoch 2120/30000 Training Loss: 0.09822392463684082\n",
      "Epoch 2120/30000 Validation Loss: 0.10107650607824326\n",
      "Epoch 2121/30000 Training Loss: 0.10146357864141464\n",
      "Epoch 2122/30000 Training Loss: 0.10348706692457199\n",
      "Epoch 2123/30000 Training Loss: 0.09758388251066208\n",
      "Epoch 2124/30000 Training Loss: 0.09167402237653732\n",
      "Epoch 2125/30000 Training Loss: 0.08943165093660355\n",
      "Epoch 2126/30000 Training Loss: 0.11230681091547012\n",
      "Epoch 2127/30000 Training Loss: 0.11276926845312119\n",
      "Epoch 2128/30000 Training Loss: 0.12149008363485336\n",
      "Epoch 2129/30000 Training Loss: 0.07794096320867538\n",
      "Epoch 2130/30000 Training Loss: 0.10849720984697342\n",
      "Epoch 2130/30000 Validation Loss: 0.0974494218826294\n",
      "Epoch 2131/30000 Training Loss: 0.10923328250646591\n",
      "Epoch 2132/30000 Training Loss: 0.10012266784906387\n",
      "Epoch 2133/30000 Training Loss: 0.09678536653518677\n",
      "Epoch 2134/30000 Training Loss: 0.09780894964933395\n",
      "Epoch 2135/30000 Training Loss: 0.09903692454099655\n",
      "Epoch 2136/30000 Training Loss: 0.09654634445905685\n",
      "Epoch 2137/30000 Training Loss: 0.0901758149266243\n",
      "Epoch 2138/30000 Training Loss: 0.11680781841278076\n",
      "Epoch 2139/30000 Training Loss: 0.10029695183038712\n",
      "Epoch 2140/30000 Training Loss: 0.0962439775466919\n",
      "Epoch 2140/30000 Validation Loss: 0.11224913597106934\n",
      "Epoch 2141/30000 Training Loss: 0.11715316772460938\n",
      "Epoch 2142/30000 Training Loss: 0.08707056194543839\n",
      "Epoch 2143/30000 Training Loss: 0.08441934734582901\n",
      "Epoch 2144/30000 Training Loss: 0.10298898071050644\n",
      "Epoch 2145/30000 Training Loss: 0.09046705812215805\n",
      "Epoch 2146/30000 Training Loss: 0.099771648645401\n",
      "Epoch 2147/30000 Training Loss: 0.11349532753229141\n",
      "Epoch 2148/30000 Training Loss: 0.10116162896156311\n",
      "Epoch 2149/30000 Training Loss: 0.1004672572016716\n",
      "Epoch 2150/30000 Training Loss: 0.10242068022489548\n",
      "Epoch 2150/30000 Validation Loss: 0.10259675234556198\n",
      "Epoch 2151/30000 Training Loss: 0.08810315281152725\n",
      "Epoch 2152/30000 Training Loss: 0.09694328159093857\n",
      "Epoch 2153/30000 Training Loss: 0.11117062717676163\n",
      "Epoch 2154/30000 Training Loss: 0.11759980767965317\n",
      "Epoch 2155/30000 Training Loss: 0.10829069465398788\n",
      "Epoch 2156/30000 Training Loss: 0.10945142060518265\n",
      "Epoch 2157/30000 Training Loss: 0.11054938286542892\n",
      "Epoch 2158/30000 Training Loss: 0.1096322163939476\n",
      "Epoch 2159/30000 Training Loss: 0.08013606816530228\n",
      "Epoch 2160/30000 Training Loss: 0.09100564569234848\n",
      "Epoch 2160/30000 Validation Loss: 0.0988316759467125\n",
      "Epoch 2161/30000 Training Loss: 0.09597416967153549\n",
      "Epoch 2162/30000 Training Loss: 0.0994541123509407\n",
      "Epoch 2163/30000 Training Loss: 0.10340086370706558\n",
      "Epoch 2164/30000 Training Loss: 0.08110999315977097\n",
      "Epoch 2165/30000 Training Loss: 0.09032265096902847\n",
      "Epoch 2166/30000 Training Loss: 0.08885881304740906\n",
      "Epoch 2167/30000 Training Loss: 0.10403838753700256\n",
      "Epoch 2168/30000 Training Loss: 0.11861035972833633\n",
      "Epoch 2169/30000 Training Loss: 0.07671427726745605\n",
      "Epoch 2170/30000 Training Loss: 0.08271101117134094\n",
      "Epoch 2170/30000 Validation Loss: 0.11551976203918457\n",
      "Epoch 2171/30000 Training Loss: 0.08890549093484879\n",
      "Epoch 2172/30000 Training Loss: 0.10656058043241501\n",
      "Epoch 2173/30000 Training Loss: 0.09891252964735031\n",
      "Epoch 2174/30000 Training Loss: 0.13552157580852509\n",
      "Epoch 2175/30000 Training Loss: 0.10127377510070801\n",
      "Epoch 2176/30000 Training Loss: 0.10631963610649109\n",
      "Epoch 2177/30000 Training Loss: 0.09861066192388535\n",
      "Epoch 2178/30000 Training Loss: 0.09142357110977173\n",
      "Epoch 2179/30000 Training Loss: 0.11226678639650345\n",
      "Epoch 2180/30000 Training Loss: 0.11516156047582626\n",
      "Epoch 2180/30000 Validation Loss: 0.10437900573015213\n",
      "Epoch 2181/30000 Training Loss: 0.10462502390146255\n",
      "Epoch 2182/30000 Training Loss: 0.10329451411962509\n",
      "Epoch 2183/30000 Training Loss: 0.07802262902259827\n",
      "Epoch 2184/30000 Training Loss: 0.09406471252441406\n",
      "Epoch 2185/30000 Training Loss: 0.10543517023324966\n",
      "Epoch 2186/30000 Training Loss: 0.09903211146593094\n",
      "Epoch 2187/30000 Training Loss: 0.10688191652297974\n",
      "Epoch 2188/30000 Training Loss: 0.1158905029296875\n",
      "Epoch 2189/30000 Training Loss: 0.09218359738588333\n",
      "Epoch 2190/30000 Training Loss: 0.0879049226641655\n",
      "Epoch 2190/30000 Validation Loss: 0.11320126056671143\n",
      "Epoch 2191/30000 Training Loss: 0.09865358471870422\n",
      "Epoch 2192/30000 Training Loss: 0.08802926540374756\n",
      "Epoch 2193/30000 Training Loss: 0.09997007995843887\n",
      "Epoch 2194/30000 Training Loss: 0.10230761766433716\n",
      "Epoch 2195/30000 Training Loss: 0.10958992689847946\n",
      "Epoch 2196/30000 Training Loss: 0.0913306474685669\n",
      "Epoch 2197/30000 Training Loss: 0.0761057510972023\n",
      "Epoch 2198/30000 Training Loss: 0.10812139511108398\n",
      "Epoch 2199/30000 Training Loss: 0.09947284311056137\n",
      "Epoch 2200/30000 Training Loss: 0.11322688311338425\n",
      "Epoch 2200/30000 Validation Loss: 0.09231793880462646\n",
      "Epoch 2201/30000 Training Loss: 0.10814010351896286\n",
      "Epoch 2202/30000 Training Loss: 0.10682511329650879\n",
      "Epoch 2203/30000 Training Loss: 0.06908311694860458\n",
      "Epoch 2204/30000 Training Loss: 0.12155396491289139\n",
      "Epoch 2205/30000 Training Loss: 0.09061708301305771\n",
      "Epoch 2206/30000 Training Loss: 0.1252993494272232\n",
      "Epoch 2207/30000 Training Loss: 0.0974714383482933\n",
      "Epoch 2208/30000 Training Loss: 0.08348512649536133\n",
      "Epoch 2209/30000 Training Loss: 0.10059121996164322\n",
      "Epoch 2210/30000 Training Loss: 0.11107475310564041\n",
      "Epoch 2210/30000 Validation Loss: 0.13084746897220612\n",
      "Epoch 2211/30000 Training Loss: 0.09157881885766983\n",
      "Epoch 2212/30000 Training Loss: 0.08700206875801086\n",
      "Epoch 2213/30000 Training Loss: 0.08026235550642014\n",
      "Epoch 2214/30000 Training Loss: 0.07381830364465714\n",
      "Epoch 2215/30000 Training Loss: 0.1482226699590683\n",
      "Epoch 2216/30000 Training Loss: 0.1430359035730362\n",
      "Epoch 2217/30000 Training Loss: 0.11798892170190811\n",
      "Epoch 2218/30000 Training Loss: 0.12068084627389908\n",
      "Epoch 2219/30000 Training Loss: 0.09868410229682922\n",
      "Epoch 2220/30000 Training Loss: 0.12188584357500076\n",
      "Epoch 2220/30000 Validation Loss: 0.09041931480169296\n",
      "Epoch 2221/30000 Training Loss: 0.1340305358171463\n",
      "Epoch 2222/30000 Training Loss: 0.10754495859146118\n",
      "Epoch 2223/30000 Training Loss: 0.077668197453022\n",
      "Epoch 2224/30000 Training Loss: 0.08853165060281754\n",
      "Epoch 2225/30000 Training Loss: 0.09976287931203842\n",
      "Epoch 2226/30000 Training Loss: 0.09795114398002625\n",
      "Epoch 2227/30000 Training Loss: 0.10524245351552963\n",
      "Epoch 2228/30000 Training Loss: 0.0933169350028038\n",
      "Epoch 2229/30000 Training Loss: 0.08610431104898453\n",
      "Epoch 2230/30000 Training Loss: 0.10699786990880966\n",
      "Epoch 2230/30000 Validation Loss: 0.10739562660455704\n",
      "Epoch 2231/30000 Training Loss: 0.11315622925758362\n",
      "Epoch 2232/30000 Training Loss: 0.10174349695444107\n",
      "Epoch 2233/30000 Training Loss: 0.08931057900190353\n",
      "Epoch 2234/30000 Training Loss: 0.09361469745635986\n",
      "Epoch 2235/30000 Training Loss: 0.1075231209397316\n",
      "Epoch 2236/30000 Training Loss: 0.09542623162269592\n",
      "Epoch 2237/30000 Training Loss: 0.08211889863014221\n",
      "Epoch 2238/30000 Training Loss: 0.10781749337911606\n",
      "Epoch 2239/30000 Training Loss: 0.09961217641830444\n",
      "Epoch 2240/30000 Training Loss: 0.09104540199041367\n",
      "Epoch 2240/30000 Validation Loss: 0.09341132640838623\n",
      "Epoch 2241/30000 Training Loss: 0.10015464574098587\n",
      "Epoch 2242/30000 Training Loss: 0.09927006810903549\n",
      "Epoch 2243/30000 Training Loss: 0.08827587962150574\n",
      "Epoch 2244/30000 Training Loss: 0.12131301313638687\n",
      "Epoch 2245/30000 Training Loss: 0.08025937527418137\n",
      "Epoch 2246/30000 Training Loss: 0.08633998781442642\n",
      "Epoch 2247/30000 Training Loss: 0.08936170488595963\n",
      "Epoch 2248/30000 Training Loss: 0.11072517186403275\n",
      "Epoch 2249/30000 Training Loss: 0.11162165552377701\n",
      "Epoch 2250/30000 Training Loss: 0.0941411554813385\n",
      "Epoch 2250/30000 Validation Loss: 0.08621370792388916\n",
      "Epoch 2251/30000 Training Loss: 0.10667125135660172\n",
      "Epoch 2252/30000 Training Loss: 0.09324523061513901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2253/30000 Training Loss: 0.08637392520904541\n",
      "Epoch 2254/30000 Training Loss: 0.10938242077827454\n",
      "Epoch 2255/30000 Training Loss: 0.11836204677820206\n",
      "Epoch 2256/30000 Training Loss: 0.08248475939035416\n",
      "Epoch 2257/30000 Training Loss: 0.09539482742547989\n",
      "Epoch 2258/30000 Training Loss: 0.09390310198068619\n",
      "Epoch 2259/30000 Training Loss: 0.1184353455901146\n",
      "Epoch 2260/30000 Training Loss: 0.0917009636759758\n",
      "Epoch 2260/30000 Validation Loss: 0.09126400947570801\n",
      "Epoch 2261/30000 Training Loss: 0.09114164113998413\n",
      "Epoch 2262/30000 Training Loss: 0.1120937243103981\n",
      "Epoch 2263/30000 Training Loss: 0.10498308390378952\n",
      "Epoch 2264/30000 Training Loss: 0.10519877821207047\n",
      "Epoch 2265/30000 Training Loss: 0.10012626647949219\n",
      "Epoch 2266/30000 Training Loss: 0.12154621630907059\n",
      "Epoch 2267/30000 Training Loss: 0.09936458617448807\n",
      "Epoch 2268/30000 Training Loss: 0.10583925247192383\n",
      "Epoch 2269/30000 Training Loss: 0.10559853911399841\n",
      "Epoch 2270/30000 Training Loss: 0.10258661955595016\n",
      "Epoch 2270/30000 Validation Loss: 0.11135941743850708\n",
      "Epoch 2271/30000 Training Loss: 0.0947982668876648\n",
      "Epoch 2272/30000 Training Loss: 0.10264107584953308\n",
      "Epoch 2273/30000 Training Loss: 0.08777874708175659\n",
      "Epoch 2274/30000 Training Loss: 0.12273745983839035\n",
      "Epoch 2275/30000 Training Loss: 0.09775013476610184\n",
      "Epoch 2276/30000 Training Loss: 0.10535112768411636\n",
      "Epoch 2277/30000 Training Loss: 0.10531198978424072\n",
      "Epoch 2278/30000 Training Loss: 0.09302318841218948\n",
      "Epoch 2279/30000 Training Loss: 0.08443444967269897\n",
      "Epoch 2280/30000 Training Loss: 0.10453173518180847\n",
      "Epoch 2280/30000 Validation Loss: 0.09028436988592148\n",
      "Epoch 2281/30000 Training Loss: 0.10171830654144287\n",
      "Epoch 2282/30000 Training Loss: 0.09150337427854538\n",
      "Epoch 2283/30000 Training Loss: 0.10199851542711258\n",
      "Epoch 2284/30000 Training Loss: 0.0847916230559349\n",
      "Epoch 2285/30000 Training Loss: 0.10068695992231369\n",
      "Epoch 2286/30000 Training Loss: 0.09861040115356445\n",
      "Epoch 2287/30000 Training Loss: 0.09330809116363525\n",
      "Epoch 2288/30000 Training Loss: 0.12098485231399536\n",
      "Epoch 2289/30000 Training Loss: 0.11120820045471191\n",
      "Epoch 2290/30000 Training Loss: 0.09018629789352417\n",
      "Epoch 2290/30000 Validation Loss: 0.1059102788567543\n",
      "Epoch 2291/30000 Training Loss: 0.10446646809577942\n",
      "Epoch 2292/30000 Training Loss: 0.09982939809560776\n",
      "Epoch 2293/30000 Training Loss: 0.10512556880712509\n",
      "Epoch 2294/30000 Training Loss: 0.0987505242228508\n",
      "Epoch 2295/30000 Training Loss: 0.08393394201993942\n",
      "Epoch 2296/30000 Training Loss: 0.08946336060762405\n",
      "Epoch 2297/30000 Training Loss: 0.09752311557531357\n",
      "Epoch 2298/30000 Training Loss: 0.09073710441589355\n",
      "Epoch 2299/30000 Training Loss: 0.10228576511144638\n",
      "Epoch 2300/30000 Training Loss: 0.1007661446928978\n",
      "Epoch 2300/30000 Validation Loss: 0.1088363528251648\n",
      "Epoch 2301/30000 Training Loss: 0.10167621821165085\n",
      "Epoch 2302/30000 Training Loss: 0.11916667222976685\n",
      "Epoch 2303/30000 Training Loss: 0.10570808500051498\n",
      "Epoch 2304/30000 Training Loss: 0.08693502098321915\n",
      "Epoch 2305/30000 Training Loss: 0.10099566727876663\n",
      "Epoch 2306/30000 Training Loss: 0.1151217594742775\n",
      "Epoch 2307/30000 Training Loss: 0.12848283350467682\n",
      "Epoch 2308/30000 Training Loss: 0.11347249150276184\n",
      "Epoch 2309/30000 Training Loss: 0.09494515508413315\n",
      "Epoch 2310/30000 Training Loss: 0.11045625805854797\n",
      "Epoch 2310/30000 Validation Loss: 0.09270916134119034\n",
      "Epoch 2311/30000 Training Loss: 0.08516940474510193\n",
      "Epoch 2312/30000 Training Loss: 0.11548896878957748\n",
      "Epoch 2313/30000 Training Loss: 0.1005418598651886\n",
      "Epoch 2314/30000 Training Loss: 0.0953850969672203\n",
      "Epoch 2315/30000 Training Loss: 0.11322981864213943\n",
      "Epoch 2316/30000 Training Loss: 0.09991122037172318\n",
      "Epoch 2317/30000 Training Loss: 0.09989985823631287\n",
      "Epoch 2318/30000 Training Loss: 0.1093512549996376\n",
      "Epoch 2319/30000 Training Loss: 0.07650312781333923\n",
      "Epoch 2320/30000 Training Loss: 0.09110933542251587\n",
      "Epoch 2320/30000 Validation Loss: 0.09298508614301682\n",
      "Epoch 2321/30000 Training Loss: 0.08600213378667831\n",
      "Epoch 2322/30000 Training Loss: 0.12062778323888779\n",
      "Epoch 2323/30000 Training Loss: 0.1055198386311531\n",
      "Epoch 2324/30000 Training Loss: 0.11969717592000961\n",
      "Epoch 2325/30000 Training Loss: 0.09931674599647522\n",
      "Epoch 2326/30000 Training Loss: 0.12620793282985687\n",
      "Epoch 2327/30000 Training Loss: 0.10867279022932053\n",
      "Epoch 2328/30000 Training Loss: 0.11131832003593445\n",
      "Epoch 2329/30000 Training Loss: 0.12234995514154434\n",
      "Epoch 2330/30000 Training Loss: 0.10165857523679733\n",
      "Epoch 2330/30000 Validation Loss: 0.10916737467050552\n",
      "Epoch 2331/30000 Training Loss: 0.10424255579710007\n",
      "Epoch 2332/30000 Training Loss: 0.12961255013942719\n",
      "Epoch 2333/30000 Training Loss: 0.10386178642511368\n",
      "Epoch 2334/30000 Training Loss: 0.0891139805316925\n",
      "Epoch 2335/30000 Training Loss: 0.10044974088668823\n",
      "Epoch 2336/30000 Training Loss: 0.09211486577987671\n",
      "Epoch 2337/30000 Training Loss: 0.07961922138929367\n",
      "Epoch 2338/30000 Training Loss: 0.09800980240106583\n",
      "Epoch 2339/30000 Training Loss: 0.11669068783521652\n",
      "Epoch 2340/30000 Training Loss: 0.09806358814239502\n",
      "Epoch 2340/30000 Validation Loss: 0.11225726455450058\n",
      "Epoch 2341/30000 Training Loss: 0.08556357771158218\n",
      "Epoch 2342/30000 Training Loss: 0.12421204894781113\n",
      "Epoch 2343/30000 Training Loss: 0.11851177364587784\n",
      "Epoch 2344/30000 Training Loss: 0.09387967735528946\n",
      "Epoch 2345/30000 Training Loss: 0.12577398121356964\n",
      "Epoch 2346/30000 Training Loss: 0.10110505670309067\n",
      "Epoch 2347/30000 Training Loss: 0.09083088487386703\n",
      "Epoch 2348/30000 Training Loss: 0.10024198144674301\n",
      "Epoch 2349/30000 Training Loss: 0.0838371142745018\n",
      "Epoch 2350/30000 Training Loss: 0.11121711879968643\n",
      "Epoch 2350/30000 Validation Loss: 0.09838338941335678\n",
      "Epoch 2351/30000 Training Loss: 0.09254313260316849\n",
      "Epoch 2352/30000 Training Loss: 0.09768166393041611\n",
      "Epoch 2353/30000 Training Loss: 0.1018272116780281\n",
      "Epoch 2354/30000 Training Loss: 0.1183319017291069\n",
      "Epoch 2355/30000 Training Loss: 0.10437598824501038\n",
      "Epoch 2356/30000 Training Loss: 0.10901518911123276\n",
      "Epoch 2357/30000 Training Loss: 0.10027670115232468\n",
      "Epoch 2358/30000 Training Loss: 0.10995212197303772\n",
      "Epoch 2359/30000 Training Loss: 0.08774896711111069\n",
      "Epoch 2360/30000 Training Loss: 0.0821172222495079\n",
      "Epoch 2360/30000 Validation Loss: 0.07952319830656052\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07952319830656052<=============\n",
      "Epoch 2361/30000 Training Loss: 0.10099669545888901\n",
      "Epoch 2362/30000 Training Loss: 0.13446618616580963\n",
      "Epoch 2363/30000 Training Loss: 0.08265739679336548\n",
      "Epoch 2364/30000 Training Loss: 0.0814615786075592\n",
      "Epoch 2365/30000 Training Loss: 0.09305175393819809\n",
      "Epoch 2366/30000 Training Loss: 0.10694185644388199\n",
      "Epoch 2367/30000 Training Loss: 0.08919399231672287\n",
      "Epoch 2368/30000 Training Loss: 0.09770647436380386\n",
      "Epoch 2369/30000 Training Loss: 0.1005256175994873\n",
      "Epoch 2370/30000 Training Loss: 0.09821490198373795\n",
      "Epoch 2370/30000 Validation Loss: 0.10661232471466064\n",
      "Epoch 2371/30000 Training Loss: 0.11854415386915207\n",
      "Epoch 2372/30000 Training Loss: 0.1228925958275795\n",
      "Epoch 2373/30000 Training Loss: 0.06969615817070007\n",
      "Epoch 2374/30000 Training Loss: 0.09175035357475281\n",
      "Epoch 2375/30000 Training Loss: 0.10717865824699402\n",
      "Epoch 2376/30000 Training Loss: 0.09606338292360306\n",
      "Epoch 2377/30000 Training Loss: 0.08475727587938309\n",
      "Epoch 2378/30000 Training Loss: 0.11374816298484802\n",
      "Epoch 2379/30000 Training Loss: 0.10660016536712646\n",
      "Epoch 2380/30000 Training Loss: 0.10103806853294373\n",
      "Epoch 2380/30000 Validation Loss: 0.10962814837694168\n",
      "Epoch 2381/30000 Training Loss: 0.09265109151601791\n",
      "Epoch 2382/30000 Training Loss: 0.08632329851388931\n",
      "Epoch 2383/30000 Training Loss: 0.09992683678865433\n",
      "Epoch 2384/30000 Training Loss: 0.1254514753818512\n",
      "Epoch 2385/30000 Training Loss: 0.10895488411188126\n",
      "Epoch 2386/30000 Training Loss: 0.08984404802322388\n",
      "Epoch 2387/30000 Training Loss: 0.09159060567617416\n",
      "Epoch 2388/30000 Training Loss: 0.08812740445137024\n",
      "Epoch 2389/30000 Training Loss: 0.12809519469738007\n",
      "Epoch 2390/30000 Training Loss: 0.10912638902664185\n",
      "Epoch 2390/30000 Validation Loss: 0.12449095398187637\n",
      "Epoch 2391/30000 Training Loss: 0.09110555797815323\n",
      "Epoch 2392/30000 Training Loss: 0.10155894607305527\n",
      "Epoch 2393/30000 Training Loss: 0.1046399250626564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2394/30000 Training Loss: 0.11647500842809677\n",
      "Epoch 2395/30000 Training Loss: 0.10047510266304016\n",
      "Epoch 2396/30000 Training Loss: 0.10628894716501236\n",
      "Epoch 2397/30000 Training Loss: 0.08552289009094238\n",
      "Epoch 2398/30000 Training Loss: 0.10920631885528564\n",
      "Epoch 2399/30000 Training Loss: 0.09078066796064377\n",
      "Epoch 2400/30000 Training Loss: 0.09031263738870621\n",
      "Epoch 2400/30000 Validation Loss: 0.11422129720449448\n",
      "Epoch 2401/30000 Training Loss: 0.11907958984375\n",
      "Epoch 2402/30000 Training Loss: 0.09790607541799545\n",
      "Epoch 2403/30000 Training Loss: 0.10139432549476624\n",
      "Epoch 2404/30000 Training Loss: 0.09371326118707657\n",
      "Epoch 2405/30000 Training Loss: 0.10076304525136948\n",
      "Epoch 2406/30000 Training Loss: 0.08069942146539688\n",
      "Epoch 2407/30000 Training Loss: 0.11144962906837463\n",
      "Epoch 2408/30000 Training Loss: 0.106772281229496\n",
      "Epoch 2409/30000 Training Loss: 0.0939033254981041\n",
      "Epoch 2410/30000 Training Loss: 0.08234075456857681\n",
      "Epoch 2410/30000 Validation Loss: 0.12881045043468475\n",
      "Epoch 2411/30000 Training Loss: 0.11139871925115585\n",
      "Epoch 2412/30000 Training Loss: 0.10332845896482468\n",
      "Epoch 2413/30000 Training Loss: 0.098736472427845\n",
      "Epoch 2414/30000 Training Loss: 0.09247326850891113\n",
      "Epoch 2415/30000 Training Loss: 0.11700715869665146\n",
      "Epoch 2416/30000 Training Loss: 0.08489764481782913\n",
      "Epoch 2417/30000 Training Loss: 0.09231126308441162\n",
      "Epoch 2418/30000 Training Loss: 0.08852425962686539\n",
      "Epoch 2419/30000 Training Loss: 0.10771355032920837\n",
      "Epoch 2420/30000 Training Loss: 0.10357090085744858\n",
      "Epoch 2420/30000 Validation Loss: 0.11106261610984802\n",
      "Epoch 2421/30000 Training Loss: 0.08853965997695923\n",
      "Epoch 2422/30000 Training Loss: 0.09394017606973648\n",
      "Epoch 2423/30000 Training Loss: 0.08695545047521591\n",
      "Epoch 2424/30000 Training Loss: 0.09399759024381638\n",
      "Epoch 2425/30000 Training Loss: 0.10235705971717834\n",
      "Epoch 2426/30000 Training Loss: 0.1048697829246521\n",
      "Epoch 2427/30000 Training Loss: 0.09719011932611465\n",
      "Epoch 2428/30000 Training Loss: 0.11164671182632446\n",
      "Epoch 2429/30000 Training Loss: 0.08939924836158752\n",
      "Epoch 2430/30000 Training Loss: 0.09234229475259781\n",
      "Epoch 2430/30000 Validation Loss: 0.09866386651992798\n",
      "Epoch 2431/30000 Training Loss: 0.105412058532238\n",
      "Epoch 2432/30000 Training Loss: 0.1066473051905632\n",
      "Epoch 2433/30000 Training Loss: 0.1102287545800209\n",
      "Epoch 2434/30000 Training Loss: 0.11387983709573746\n",
      "Epoch 2435/30000 Training Loss: 0.0929022803902626\n",
      "Epoch 2436/30000 Training Loss: 0.07801534235477448\n",
      "Epoch 2437/30000 Training Loss: 0.11234002560377121\n",
      "Epoch 2438/30000 Training Loss: 0.09412205219268799\n",
      "Epoch 2439/30000 Training Loss: 0.10445061326026917\n",
      "Epoch 2440/30000 Training Loss: 0.11455556005239487\n",
      "Epoch 2440/30000 Validation Loss: 0.08942338824272156\n",
      "Epoch 2441/30000 Training Loss: 0.0979456678032875\n",
      "Epoch 2442/30000 Training Loss: 0.07619064301252365\n",
      "Epoch 2443/30000 Training Loss: 0.10444602370262146\n",
      "Epoch 2444/30000 Training Loss: 0.10430648922920227\n",
      "Epoch 2445/30000 Training Loss: 0.09470351785421371\n",
      "Epoch 2446/30000 Training Loss: 0.08842634409666061\n",
      "Epoch 2447/30000 Training Loss: 0.10656525939702988\n",
      "Epoch 2448/30000 Training Loss: 0.10854116082191467\n",
      "Epoch 2449/30000 Training Loss: 0.0972573459148407\n",
      "Epoch 2450/30000 Training Loss: 0.1219300702214241\n",
      "Epoch 2450/30000 Validation Loss: 0.06946835666894913\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06946835666894913<=============\n",
      "Epoch 2451/30000 Training Loss: 0.11341822147369385\n",
      "Epoch 2452/30000 Training Loss: 0.1052682027220726\n",
      "Epoch 2453/30000 Training Loss: 0.12791617214679718\n",
      "Epoch 2454/30000 Training Loss: 0.07542630285024643\n",
      "Epoch 2455/30000 Training Loss: 0.08673093467950821\n",
      "Epoch 2456/30000 Training Loss: 0.10342884063720703\n",
      "Epoch 2457/30000 Training Loss: 0.07745762914419174\n",
      "Epoch 2458/30000 Training Loss: 0.09975843876600266\n",
      "Epoch 2459/30000 Training Loss: 0.0976405069231987\n",
      "Epoch 2460/30000 Training Loss: 0.12367546558380127\n",
      "Epoch 2460/30000 Validation Loss: 0.10351874679327011\n",
      "Epoch 2461/30000 Training Loss: 0.09985911846160889\n",
      "Epoch 2462/30000 Training Loss: 0.10945243388414383\n",
      "Epoch 2463/30000 Training Loss: 0.12355441600084305\n",
      "Epoch 2464/30000 Training Loss: 0.09896687418222427\n",
      "Epoch 2465/30000 Training Loss: 0.08656524866819382\n",
      "Epoch 2466/30000 Training Loss: 0.08793262392282486\n",
      "Epoch 2467/30000 Training Loss: 0.09440121054649353\n",
      "Epoch 2468/30000 Training Loss: 0.10571380704641342\n",
      "Epoch 2469/30000 Training Loss: 0.08436105400323868\n",
      "Epoch 2470/30000 Training Loss: 0.10924429446458817\n",
      "Epoch 2470/30000 Validation Loss: 0.10328342765569687\n",
      "Epoch 2471/30000 Training Loss: 0.08631185442209244\n",
      "Epoch 2472/30000 Training Loss: 0.10410318523645401\n",
      "Epoch 2473/30000 Training Loss: 0.09603104740381241\n",
      "Epoch 2474/30000 Training Loss: 0.08322671055793762\n",
      "Epoch 2475/30000 Training Loss: 0.10836184769868851\n",
      "Epoch 2476/30000 Training Loss: 0.08930879831314087\n",
      "Epoch 2477/30000 Training Loss: 0.09687741845846176\n",
      "Epoch 2478/30000 Training Loss: 0.09489458799362183\n",
      "Epoch 2479/30000 Training Loss: 0.11030713468790054\n",
      "Epoch 2480/30000 Training Loss: 0.10717902332544327\n",
      "Epoch 2480/30000 Validation Loss: 0.11092013120651245\n",
      "Epoch 2481/30000 Training Loss: 0.11145952343940735\n",
      "Epoch 2482/30000 Training Loss: 0.15329797565937042\n",
      "Epoch 2483/30000 Training Loss: 0.11169058084487915\n",
      "Epoch 2484/30000 Training Loss: 0.11598575860261917\n",
      "Epoch 2485/30000 Training Loss: 0.08324480056762695\n",
      "Epoch 2486/30000 Training Loss: 0.09836136549711227\n",
      "Epoch 2487/30000 Training Loss: 0.1162017285823822\n",
      "Epoch 2488/30000 Training Loss: 0.0900154784321785\n",
      "Epoch 2489/30000 Training Loss: 0.11452878266572952\n",
      "Epoch 2490/30000 Training Loss: 0.10926534980535507\n",
      "Epoch 2490/30000 Validation Loss: 0.0753663033246994\n",
      "Epoch 2491/30000 Training Loss: 0.10674015432596207\n",
      "Epoch 2492/30000 Training Loss: 0.11974827200174332\n",
      "Epoch 2493/30000 Training Loss: 0.11555642634630203\n",
      "Epoch 2494/30000 Training Loss: 0.0884224995970726\n",
      "Epoch 2495/30000 Training Loss: 0.1101635992527008\n",
      "Epoch 2496/30000 Training Loss: 0.09764792770147324\n",
      "Epoch 2497/30000 Training Loss: 0.08743881434202194\n",
      "Epoch 2498/30000 Training Loss: 0.11975979804992676\n",
      "Epoch 2499/30000 Training Loss: 0.08970669656991959\n",
      "Epoch 2500/30000 Training Loss: 0.1011032834649086\n",
      "Epoch 2500/30000 Validation Loss: 0.13063204288482666\n",
      "Epoch 2501/30000 Training Loss: 0.09369317442178726\n",
      "Epoch 2502/30000 Training Loss: 0.10544802993535995\n",
      "Epoch 2503/30000 Training Loss: 0.12064488977193832\n",
      "Epoch 2504/30000 Training Loss: 0.08562362194061279\n",
      "Epoch 2505/30000 Training Loss: 0.0883742943406105\n",
      "Epoch 2506/30000 Training Loss: 0.10538142919540405\n",
      "Epoch 2507/30000 Training Loss: 0.08568248152732849\n",
      "Epoch 2508/30000 Training Loss: 0.1171470656991005\n",
      "Epoch 2509/30000 Training Loss: 0.07847572863101959\n",
      "Epoch 2510/30000 Training Loss: 0.1073857769370079\n",
      "Epoch 2510/30000 Validation Loss: 0.09353850036859512\n",
      "Epoch 2511/30000 Training Loss: 0.0689968466758728\n",
      "Epoch 2512/30000 Training Loss: 0.08992466330528259\n",
      "Epoch 2513/30000 Training Loss: 0.10906680673360825\n",
      "Epoch 2514/30000 Training Loss: 0.09273338317871094\n",
      "Epoch 2515/30000 Training Loss: 0.09978631883859634\n",
      "Epoch 2516/30000 Training Loss: 0.12260042876005173\n",
      "Epoch 2517/30000 Training Loss: 0.08893802016973495\n",
      "Epoch 2518/30000 Training Loss: 0.09582797437906265\n",
      "Epoch 2519/30000 Training Loss: 0.08955594152212143\n",
      "Epoch 2520/30000 Training Loss: 0.13016146421432495\n",
      "Epoch 2520/30000 Validation Loss: 0.08709344267845154\n",
      "Epoch 2521/30000 Training Loss: 0.10196975618600845\n",
      "Epoch 2522/30000 Training Loss: 0.09310256689786911\n",
      "Epoch 2523/30000 Training Loss: 0.10055907815694809\n",
      "Epoch 2524/30000 Training Loss: 0.09980398416519165\n",
      "Epoch 2525/30000 Training Loss: 0.09167561680078506\n",
      "Epoch 2526/30000 Training Loss: 0.09824944287538528\n",
      "Epoch 2527/30000 Training Loss: 0.11803076416254044\n",
      "Epoch 2528/30000 Training Loss: 0.1094127893447876\n",
      "Epoch 2529/30000 Training Loss: 0.07884323596954346\n",
      "Epoch 2530/30000 Training Loss: 0.1222819909453392\n",
      "Epoch 2530/30000 Validation Loss: 0.09186288714408875\n",
      "Epoch 2531/30000 Training Loss: 0.10195890814065933\n",
      "Epoch 2532/30000 Training Loss: 0.09479793161153793\n",
      "Epoch 2533/30000 Training Loss: 0.08291894942522049\n",
      "Epoch 2534/30000 Training Loss: 0.10176444798707962\n",
      "Epoch 2535/30000 Training Loss: 0.08517378568649292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2536/30000 Training Loss: 0.10674210637807846\n",
      "Epoch 2537/30000 Training Loss: 0.12054303288459778\n",
      "Epoch 2538/30000 Training Loss: 0.09853169322013855\n",
      "Epoch 2539/30000 Training Loss: 0.09577514976263046\n",
      "Epoch 2540/30000 Training Loss: 0.08799710869789124\n",
      "Epoch 2540/30000 Validation Loss: 0.10843268781900406\n",
      "Epoch 2541/30000 Training Loss: 0.10798025876283646\n",
      "Epoch 2542/30000 Training Loss: 0.11290832608938217\n",
      "Epoch 2543/30000 Training Loss: 0.10927993059158325\n",
      "Epoch 2544/30000 Training Loss: 0.09496723860502243\n",
      "Epoch 2545/30000 Training Loss: 0.1136656403541565\n",
      "Epoch 2546/30000 Training Loss: 0.08425184339284897\n",
      "Epoch 2547/30000 Training Loss: 0.11456495523452759\n",
      "Epoch 2548/30000 Training Loss: 0.10081344842910767\n",
      "Epoch 2549/30000 Training Loss: 0.09127864986658096\n",
      "Epoch 2550/30000 Training Loss: 0.09443237632513046\n",
      "Epoch 2550/30000 Validation Loss: 0.0915912389755249\n",
      "Epoch 2551/30000 Training Loss: 0.10416236519813538\n",
      "Epoch 2552/30000 Training Loss: 0.09319042414426804\n",
      "Epoch 2553/30000 Training Loss: 0.10190016031265259\n",
      "Epoch 2554/30000 Training Loss: 0.09862060099840164\n",
      "Epoch 2555/30000 Training Loss: 0.09417884796857834\n",
      "Epoch 2556/30000 Training Loss: 0.07009181380271912\n",
      "Epoch 2557/30000 Training Loss: 0.07882652431726456\n",
      "Epoch 2558/30000 Training Loss: 0.0890776738524437\n",
      "Epoch 2559/30000 Training Loss: 0.10129060596227646\n",
      "Epoch 2560/30000 Training Loss: 0.09472868591547012\n",
      "Epoch 2560/30000 Validation Loss: 0.09131179004907608\n",
      "Epoch 2561/30000 Training Loss: 0.09869181364774704\n",
      "Epoch 2562/30000 Training Loss: 0.11038490384817123\n",
      "Epoch 2563/30000 Training Loss: 0.10186485201120377\n",
      "Epoch 2564/30000 Training Loss: 0.09314025193452835\n",
      "Epoch 2565/30000 Training Loss: 0.08479416370391846\n",
      "Epoch 2566/30000 Training Loss: 0.08922500163316727\n",
      "Epoch 2567/30000 Training Loss: 0.10814261436462402\n",
      "Epoch 2568/30000 Training Loss: 0.09384671598672867\n",
      "Epoch 2569/30000 Training Loss: 0.11830329895019531\n",
      "Epoch 2570/30000 Training Loss: 0.10951578617095947\n",
      "Epoch 2570/30000 Validation Loss: 0.07631706446409225\n",
      "Epoch 2571/30000 Training Loss: 0.11131588369607925\n",
      "Epoch 2572/30000 Training Loss: 0.08590725064277649\n",
      "Epoch 2573/30000 Training Loss: 0.08897337317466736\n",
      "Epoch 2574/30000 Training Loss: 0.11958035081624985\n",
      "Epoch 2575/30000 Training Loss: 0.08883560448884964\n",
      "Epoch 2576/30000 Training Loss: 0.11285299807786942\n",
      "Epoch 2577/30000 Training Loss: 0.0789758712053299\n",
      "Epoch 2578/30000 Training Loss: 0.10790729522705078\n",
      "Epoch 2579/30000 Training Loss: 0.09587525576353073\n",
      "Epoch 2580/30000 Training Loss: 0.0985630452632904\n",
      "Epoch 2580/30000 Validation Loss: 0.09298849105834961\n",
      "Epoch 2581/30000 Training Loss: 0.09429514408111572\n",
      "Epoch 2582/30000 Training Loss: 0.0927051529288292\n",
      "Epoch 2583/30000 Training Loss: 0.09590094536542892\n",
      "Epoch 2584/30000 Training Loss: 0.07594484835863113\n",
      "Epoch 2585/30000 Training Loss: 0.09777643531560898\n",
      "Epoch 2586/30000 Training Loss: 0.10584063082933426\n",
      "Epoch 2587/30000 Training Loss: 0.11030697822570801\n",
      "Epoch 2588/30000 Training Loss: 0.10747792571783066\n",
      "Epoch 2589/30000 Training Loss: 0.09229425340890884\n",
      "Epoch 2590/30000 Training Loss: 0.12594904005527496\n",
      "Epoch 2590/30000 Validation Loss: 0.09620898216962814\n",
      "Epoch 2591/30000 Training Loss: 0.09827122837305069\n",
      "Epoch 2592/30000 Training Loss: 0.10832449048757553\n",
      "Epoch 2593/30000 Training Loss: 0.0863906666636467\n",
      "Epoch 2594/30000 Training Loss: 0.10829544812440872\n",
      "Epoch 2595/30000 Training Loss: 0.11862782388925552\n",
      "Epoch 2596/30000 Training Loss: 0.08602878451347351\n",
      "Epoch 2597/30000 Training Loss: 0.10530853271484375\n",
      "Epoch 2598/30000 Training Loss: 0.08609924465417862\n",
      "Epoch 2599/30000 Training Loss: 0.11717718094587326\n",
      "Epoch 2600/30000 Training Loss: 0.11788595467805862\n",
      "Epoch 2600/30000 Validation Loss: 0.10675021260976791\n",
      "Epoch 2601/30000 Training Loss: 0.08377376943826675\n",
      "Epoch 2602/30000 Training Loss: 0.10315483063459396\n",
      "Epoch 2603/30000 Training Loss: 0.09716802835464478\n",
      "Epoch 2604/30000 Training Loss: 0.0940009132027626\n",
      "Epoch 2605/30000 Training Loss: 0.08729966729879379\n",
      "Epoch 2606/30000 Training Loss: 0.11176115274429321\n",
      "Epoch 2607/30000 Training Loss: 0.08771368116140366\n",
      "Epoch 2608/30000 Training Loss: 0.10066720843315125\n",
      "Epoch 2609/30000 Training Loss: 0.08363717049360275\n",
      "Epoch 2610/30000 Training Loss: 0.10836078971624374\n",
      "Epoch 2610/30000 Validation Loss: 0.108875572681427\n",
      "Epoch 2611/30000 Training Loss: 0.09646173566579819\n",
      "Epoch 2612/30000 Training Loss: 0.09291642159223557\n",
      "Epoch 2613/30000 Training Loss: 0.11087482422590256\n",
      "Epoch 2614/30000 Training Loss: 0.0997687354683876\n",
      "Epoch 2615/30000 Training Loss: 0.08801836520433426\n",
      "Epoch 2616/30000 Training Loss: 0.10628064721822739\n",
      "Epoch 2617/30000 Training Loss: 0.1011798307299614\n",
      "Epoch 2618/30000 Training Loss: 0.09668061137199402\n",
      "Epoch 2619/30000 Training Loss: 0.09985649585723877\n",
      "Epoch 2620/30000 Training Loss: 0.09669709205627441\n",
      "Epoch 2620/30000 Validation Loss: 0.10440456867218018\n",
      "Epoch 2621/30000 Training Loss: 0.09081052988767624\n",
      "Epoch 2622/30000 Training Loss: 0.09347852319478989\n",
      "Epoch 2623/30000 Training Loss: 0.07937130331993103\n",
      "Epoch 2624/30000 Training Loss: 0.08810710161924362\n",
      "Epoch 2625/30000 Training Loss: 0.09842757135629654\n",
      "Epoch 2626/30000 Training Loss: 0.0925559401512146\n",
      "Epoch 2627/30000 Training Loss: 0.11802888661623001\n",
      "Epoch 2628/30000 Training Loss: 0.11625459045171738\n",
      "Epoch 2629/30000 Training Loss: 0.11708705872297287\n",
      "Epoch 2630/30000 Training Loss: 0.10196605324745178\n",
      "Epoch 2630/30000 Validation Loss: 0.08152670413255692\n",
      "Epoch 2631/30000 Training Loss: 0.0915108248591423\n",
      "Epoch 2632/30000 Training Loss: 0.1096009686589241\n",
      "Epoch 2633/30000 Training Loss: 0.11660397052764893\n",
      "Epoch 2634/30000 Training Loss: 0.09181976318359375\n",
      "Epoch 2635/30000 Training Loss: 0.10221772640943527\n",
      "Epoch 2636/30000 Training Loss: 0.09029770642518997\n",
      "Epoch 2637/30000 Training Loss: 0.09142570942640305\n",
      "Epoch 2638/30000 Training Loss: 0.1298041194677353\n",
      "Epoch 2639/30000 Training Loss: 0.07935447245836258\n",
      "Epoch 2640/30000 Training Loss: 0.07322391867637634\n",
      "Epoch 2640/30000 Validation Loss: 0.09076908975839615\n",
      "Epoch 2641/30000 Training Loss: 0.09906104952096939\n",
      "Epoch 2642/30000 Training Loss: 0.1376572698354721\n",
      "Epoch 2643/30000 Training Loss: 0.08430430293083191\n",
      "Epoch 2644/30000 Training Loss: 0.11494453996419907\n",
      "Epoch 2645/30000 Training Loss: 0.10503093153238297\n",
      "Epoch 2646/30000 Training Loss: 0.09729829430580139\n",
      "Epoch 2647/30000 Training Loss: 0.09295517951250076\n",
      "Epoch 2648/30000 Training Loss: 0.09979335218667984\n",
      "Epoch 2649/30000 Training Loss: 0.1185753121972084\n",
      "Epoch 2650/30000 Training Loss: 0.08772190660238266\n",
      "Epoch 2650/30000 Validation Loss: 0.11455247551202774\n",
      "Epoch 2651/30000 Training Loss: 0.08574715256690979\n",
      "Epoch 2652/30000 Training Loss: 0.07549434155225754\n",
      "Epoch 2653/30000 Training Loss: 0.08413159847259521\n",
      "Epoch 2654/30000 Training Loss: 0.06662983447313309\n",
      "Epoch 2655/30000 Training Loss: 0.09319726377725601\n",
      "Epoch 2656/30000 Training Loss: 0.09903257340192795\n",
      "Epoch 2657/30000 Training Loss: 0.10359413176774979\n",
      "Epoch 2658/30000 Training Loss: 0.08229460567235947\n",
      "Epoch 2659/30000 Training Loss: 0.10727431625127792\n",
      "Epoch 2660/30000 Training Loss: 0.11746025085449219\n",
      "Epoch 2660/30000 Validation Loss: 0.10158386081457138\n",
      "Epoch 2661/30000 Training Loss: 0.0857231393456459\n",
      "Epoch 2662/30000 Training Loss: 0.09261545538902283\n",
      "Epoch 2663/30000 Training Loss: 0.09323003143072128\n",
      "Epoch 2664/30000 Training Loss: 0.1182379350066185\n",
      "Epoch 2665/30000 Training Loss: 0.10445418953895569\n",
      "Epoch 2666/30000 Training Loss: 0.10446465015411377\n",
      "Epoch 2667/30000 Training Loss: 0.07979416847229004\n",
      "Epoch 2668/30000 Training Loss: 0.07673975080251694\n",
      "Epoch 2669/30000 Training Loss: 0.10971090942621231\n",
      "Epoch 2670/30000 Training Loss: 0.0850311890244484\n",
      "Epoch 2670/30000 Validation Loss: 0.09859324246644974\n",
      "Epoch 2671/30000 Training Loss: 0.0848563089966774\n",
      "Epoch 2672/30000 Training Loss: 0.09438520669937134\n",
      "Epoch 2673/30000 Training Loss: 0.08321992307901382\n",
      "Epoch 2674/30000 Training Loss: 0.0706435814499855\n",
      "Epoch 2675/30000 Training Loss: 0.09486225992441177\n",
      "Epoch 2676/30000 Training Loss: 0.10755637288093567\n",
      "Epoch 2677/30000 Training Loss: 0.09012483805418015\n",
      "Epoch 2678/30000 Training Loss: 0.09777303785085678\n",
      "Epoch 2679/30000 Training Loss: 0.09090352058410645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2680/30000 Training Loss: 0.11382358521223068\n",
      "Epoch 2680/30000 Validation Loss: 0.11449561268091202\n",
      "Epoch 2681/30000 Training Loss: 0.09051061421632767\n",
      "Epoch 2682/30000 Training Loss: 0.0982833281159401\n",
      "Epoch 2683/30000 Training Loss: 0.09439754486083984\n",
      "Epoch 2684/30000 Training Loss: 0.09742045402526855\n",
      "Epoch 2685/30000 Training Loss: 0.10849025100469589\n",
      "Epoch 2686/30000 Training Loss: 0.10938597470521927\n",
      "Epoch 2687/30000 Training Loss: 0.09860464185476303\n",
      "Epoch 2688/30000 Training Loss: 0.08936824649572372\n",
      "Epoch 2689/30000 Training Loss: 0.09576653689146042\n",
      "Epoch 2690/30000 Training Loss: 0.08746712654829025\n",
      "Epoch 2690/30000 Validation Loss: 0.0977470651268959\n",
      "Epoch 2691/30000 Training Loss: 0.09245198220014572\n",
      "Epoch 2692/30000 Training Loss: 0.10507122427225113\n",
      "Epoch 2693/30000 Training Loss: 0.07080917805433273\n",
      "Epoch 2694/30000 Training Loss: 0.09735264629125595\n",
      "Epoch 2695/30000 Training Loss: 0.09706389904022217\n",
      "Epoch 2696/30000 Training Loss: 0.11108461767435074\n",
      "Epoch 2697/30000 Training Loss: 0.10663161426782608\n",
      "Epoch 2698/30000 Training Loss: 0.08884629607200623\n",
      "Epoch 2699/30000 Training Loss: 0.084311842918396\n",
      "Epoch 2700/30000 Training Loss: 0.08682515472173691\n",
      "Epoch 2700/30000 Validation Loss: 0.1128900945186615\n",
      "Epoch 2701/30000 Training Loss: 0.10390317440032959\n",
      "Epoch 2702/30000 Training Loss: 0.10492769628763199\n",
      "Epoch 2703/30000 Training Loss: 0.08635763078927994\n",
      "Epoch 2704/30000 Training Loss: 0.09274786710739136\n",
      "Epoch 2705/30000 Training Loss: 0.1041058897972107\n",
      "Epoch 2706/30000 Training Loss: 0.0860627293586731\n",
      "Epoch 2707/30000 Training Loss: 0.10198677331209183\n",
      "Epoch 2708/30000 Training Loss: 0.10632967948913574\n",
      "Epoch 2709/30000 Training Loss: 0.08293905854225159\n",
      "Epoch 2710/30000 Training Loss: 0.08650372177362442\n",
      "Epoch 2710/30000 Validation Loss: 0.10120856761932373\n",
      "Epoch 2711/30000 Training Loss: 0.09170025587081909\n",
      "Epoch 2712/30000 Training Loss: 0.08702981472015381\n",
      "Epoch 2713/30000 Training Loss: 0.09343791007995605\n",
      "Epoch 2714/30000 Training Loss: 0.08511409163475037\n",
      "Epoch 2715/30000 Training Loss: 0.0970541313290596\n",
      "Epoch 2716/30000 Training Loss: 0.10510947555303574\n",
      "Epoch 2717/30000 Training Loss: 0.08220851421356201\n",
      "Epoch 2718/30000 Training Loss: 0.07738390564918518\n",
      "Epoch 2719/30000 Training Loss: 0.07940317690372467\n",
      "Epoch 2720/30000 Training Loss: 0.09040725231170654\n",
      "Epoch 2720/30000 Validation Loss: 0.10353556275367737\n",
      "Epoch 2721/30000 Training Loss: 0.11232496052980423\n",
      "Epoch 2722/30000 Training Loss: 0.12463217973709106\n",
      "Epoch 2723/30000 Training Loss: 0.10632777959108353\n",
      "Epoch 2724/30000 Training Loss: 0.09778914600610733\n",
      "Epoch 2725/30000 Training Loss: 0.07987634092569351\n",
      "Epoch 2726/30000 Training Loss: 0.09976742416620255\n",
      "Epoch 2727/30000 Training Loss: 0.1322856992483139\n",
      "Epoch 2728/30000 Training Loss: 0.10858935117721558\n",
      "Epoch 2729/30000 Training Loss: 0.11740239709615707\n",
      "Epoch 2730/30000 Training Loss: 0.09472579509019852\n",
      "Epoch 2730/30000 Validation Loss: 0.09730032831430435\n",
      "Epoch 2731/30000 Training Loss: 0.09669303894042969\n",
      "Epoch 2732/30000 Training Loss: 0.09634584933519363\n",
      "Epoch 2733/30000 Training Loss: 0.08442556858062744\n",
      "Epoch 2734/30000 Training Loss: 0.12721401453018188\n",
      "Epoch 2735/30000 Training Loss: 0.09576084464788437\n",
      "Epoch 2736/30000 Training Loss: 0.10013804584741592\n",
      "Epoch 2737/30000 Training Loss: 0.09604459255933762\n",
      "Epoch 2738/30000 Training Loss: 0.11830677837133408\n",
      "Epoch 2739/30000 Training Loss: 0.07960726320743561\n",
      "Epoch 2740/30000 Training Loss: 0.08289989829063416\n",
      "Epoch 2740/30000 Validation Loss: 0.07917231321334839\n",
      "Epoch 2741/30000 Training Loss: 0.09601866453886032\n",
      "Epoch 2742/30000 Training Loss: 0.09279397875070572\n",
      "Epoch 2743/30000 Training Loss: 0.10266673564910889\n",
      "Epoch 2744/30000 Training Loss: 0.10457918047904968\n",
      "Epoch 2745/30000 Training Loss: 0.1018914207816124\n",
      "Epoch 2746/30000 Training Loss: 0.09253830462694168\n",
      "Epoch 2747/30000 Training Loss: 0.09004411846399307\n",
      "Epoch 2748/30000 Training Loss: 0.09980794042348862\n",
      "Epoch 2749/30000 Training Loss: 0.11393152922391891\n",
      "Epoch 2750/30000 Training Loss: 0.08689629286527634\n",
      "Epoch 2750/30000 Validation Loss: 0.1092761978507042\n",
      "Epoch 2751/30000 Training Loss: 0.11001405119895935\n",
      "Epoch 2752/30000 Training Loss: 0.08746727555990219\n",
      "Epoch 2753/30000 Training Loss: 0.0928676649928093\n",
      "Epoch 2754/30000 Training Loss: 0.09764232486486435\n",
      "Epoch 2755/30000 Training Loss: 0.09318224340677261\n",
      "Epoch 2756/30000 Training Loss: 0.10538468509912491\n",
      "Epoch 2757/30000 Training Loss: 0.11670703440904617\n",
      "Epoch 2758/30000 Training Loss: 0.0715116634964943\n",
      "Epoch 2759/30000 Training Loss: 0.07680240273475647\n",
      "Epoch 2760/30000 Training Loss: 0.09325116872787476\n",
      "Epoch 2760/30000 Validation Loss: 0.0798499807715416\n",
      "Epoch 2761/30000 Training Loss: 0.08356967568397522\n",
      "Epoch 2762/30000 Training Loss: 0.09546393156051636\n",
      "Epoch 2763/30000 Training Loss: 0.1012052372097969\n",
      "Epoch 2764/30000 Training Loss: 0.0932440236210823\n",
      "Epoch 2765/30000 Training Loss: 0.10012611001729965\n",
      "Epoch 2766/30000 Training Loss: 0.08489354699850082\n",
      "Epoch 2767/30000 Training Loss: 0.09497570246458054\n",
      "Epoch 2768/30000 Training Loss: 0.09081888198852539\n",
      "Epoch 2769/30000 Training Loss: 0.09238805621862411\n",
      "Epoch 2770/30000 Training Loss: 0.11545664072036743\n",
      "Epoch 2770/30000 Validation Loss: 0.11025793105363846\n",
      "Epoch 2771/30000 Training Loss: 0.09404295682907104\n",
      "Epoch 2772/30000 Training Loss: 0.11667824536561966\n",
      "Epoch 2773/30000 Training Loss: 0.09261975437402725\n",
      "Epoch 2774/30000 Training Loss: 0.08666875213384628\n",
      "Epoch 2775/30000 Training Loss: 0.10190541297197342\n",
      "Epoch 2776/30000 Training Loss: 0.10248898714780807\n",
      "Epoch 2777/30000 Training Loss: 0.08121208101511002\n",
      "Epoch 2778/30000 Training Loss: 0.09394663572311401\n",
      "Epoch 2779/30000 Training Loss: 0.12021856755018234\n",
      "Epoch 2780/30000 Training Loss: 0.09636468440294266\n",
      "Epoch 2780/30000 Validation Loss: 0.09746691584587097\n",
      "Epoch 2781/30000 Training Loss: 0.09067869186401367\n",
      "Epoch 2782/30000 Training Loss: 0.09910168498754501\n",
      "Epoch 2783/30000 Training Loss: 0.08309924602508545\n",
      "Epoch 2784/30000 Training Loss: 0.09678230434656143\n",
      "Epoch 2785/30000 Training Loss: 0.11482938379049301\n",
      "Epoch 2786/30000 Training Loss: 0.11612170189619064\n",
      "Epoch 2787/30000 Training Loss: 0.11043596267700195\n",
      "Epoch 2788/30000 Training Loss: 0.09472890943288803\n",
      "Epoch 2789/30000 Training Loss: 0.08229720592498779\n",
      "Epoch 2790/30000 Training Loss: 0.11271020025014877\n",
      "Epoch 2790/30000 Validation Loss: 0.08099981397390366\n",
      "Epoch 2791/30000 Training Loss: 0.10848063975572586\n",
      "Epoch 2792/30000 Training Loss: 0.0845189169049263\n",
      "Epoch 2793/30000 Training Loss: 0.0935765728354454\n",
      "Epoch 2794/30000 Training Loss: 0.10699661821126938\n",
      "Epoch 2795/30000 Training Loss: 0.08713889867067337\n",
      "Epoch 2796/30000 Training Loss: 0.0829266905784607\n",
      "Epoch 2797/30000 Training Loss: 0.11568450182676315\n",
      "Epoch 2798/30000 Training Loss: 0.13434989750385284\n",
      "Epoch 2799/30000 Training Loss: 0.1046113446354866\n",
      "Epoch 2800/30000 Training Loss: 0.11013331264257431\n",
      "Epoch 2800/30000 Validation Loss: 0.11009171605110168\n",
      "Epoch 2801/30000 Training Loss: 0.07075115293264389\n",
      "Epoch 2802/30000 Training Loss: 0.10468757897615433\n",
      "Epoch 2803/30000 Training Loss: 0.07289385050535202\n",
      "Epoch 2804/30000 Training Loss: 0.10596457123756409\n",
      "Epoch 2805/30000 Training Loss: 0.08886266499757767\n",
      "Epoch 2806/30000 Training Loss: 0.09073350578546524\n",
      "Epoch 2807/30000 Training Loss: 0.09328987449407578\n",
      "Epoch 2808/30000 Training Loss: 0.0803154781460762\n",
      "Epoch 2809/30000 Training Loss: 0.11238785833120346\n",
      "Epoch 2810/30000 Training Loss: 0.07275087386369705\n",
      "Epoch 2810/30000 Validation Loss: 0.08995171636343002\n",
      "Epoch 2811/30000 Training Loss: 0.08174426853656769\n",
      "Epoch 2812/30000 Training Loss: 0.09326835721731186\n",
      "Epoch 2813/30000 Training Loss: 0.09385862201452255\n",
      "Epoch 2814/30000 Training Loss: 0.0759883001446724\n",
      "Epoch 2815/30000 Training Loss: 0.12600229680538177\n",
      "Epoch 2816/30000 Training Loss: 0.10031590610742569\n",
      "Epoch 2817/30000 Training Loss: 0.08283388614654541\n",
      "Epoch 2818/30000 Training Loss: 0.07483468949794769\n",
      "Epoch 2819/30000 Training Loss: 0.10464771836996078\n",
      "Epoch 2820/30000 Training Loss: 0.10976795107126236\n",
      "Epoch 2820/30000 Validation Loss: 0.0953570082783699\n",
      "Epoch 2821/30000 Training Loss: 0.10858557373285294\n",
      "Epoch 2822/30000 Training Loss: 0.10198105126619339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2823/30000 Training Loss: 0.1093800738453865\n",
      "Epoch 2824/30000 Training Loss: 0.10325515270233154\n",
      "Epoch 2825/30000 Training Loss: 0.10005687922239304\n",
      "Epoch 2826/30000 Training Loss: 0.09741305559873581\n",
      "Epoch 2827/30000 Training Loss: 0.1321905255317688\n",
      "Epoch 2828/30000 Training Loss: 0.09964796155691147\n",
      "Epoch 2829/30000 Training Loss: 0.07936456054449081\n",
      "Epoch 2830/30000 Training Loss: 0.10001526027917862\n",
      "Epoch 2830/30000 Validation Loss: 0.10105517506599426\n",
      "Epoch 2831/30000 Training Loss: 0.11568734049797058\n",
      "Epoch 2832/30000 Training Loss: 0.09414979815483093\n",
      "Epoch 2833/30000 Training Loss: 0.0978943333029747\n",
      "Epoch 2834/30000 Training Loss: 0.09219179302453995\n",
      "Epoch 2835/30000 Training Loss: 0.10963895171880722\n",
      "Epoch 2836/30000 Training Loss: 0.10435149818658829\n",
      "Epoch 2837/30000 Training Loss: 0.1057789996266365\n",
      "Epoch 2838/30000 Training Loss: 0.09152037650346756\n",
      "Epoch 2839/30000 Training Loss: 0.08621180057525635\n",
      "Epoch 2840/30000 Training Loss: 0.11830008029937744\n",
      "Epoch 2840/30000 Validation Loss: 0.10943472385406494\n",
      "Epoch 2841/30000 Training Loss: 0.09712056070566177\n",
      "Epoch 2842/30000 Training Loss: 0.08510924130678177\n",
      "Epoch 2843/30000 Training Loss: 0.08839482069015503\n",
      "Epoch 2844/30000 Training Loss: 0.09398611634969711\n",
      "Epoch 2845/30000 Training Loss: 0.1055651530623436\n",
      "Epoch 2846/30000 Training Loss: 0.09575417637825012\n",
      "Epoch 2847/30000 Training Loss: 0.08687364310026169\n",
      "Epoch 2848/30000 Training Loss: 0.10528171062469482\n",
      "Epoch 2849/30000 Training Loss: 0.09992492198944092\n",
      "Epoch 2850/30000 Training Loss: 0.10954102128744125\n",
      "Epoch 2850/30000 Validation Loss: 0.08598625659942627\n",
      "Epoch 2851/30000 Training Loss: 0.09796474128961563\n",
      "Epoch 2852/30000 Training Loss: 0.09666576236486435\n",
      "Epoch 2853/30000 Training Loss: 0.12396791577339172\n",
      "Epoch 2854/30000 Training Loss: 0.12709951400756836\n",
      "Epoch 2855/30000 Training Loss: 0.1147739514708519\n",
      "Epoch 2856/30000 Training Loss: 0.09961291402578354\n",
      "Epoch 2857/30000 Training Loss: 0.08134139329195023\n",
      "Epoch 2858/30000 Training Loss: 0.12627188861370087\n",
      "Epoch 2859/30000 Training Loss: 0.10050901770591736\n",
      "Epoch 2860/30000 Training Loss: 0.08633267879486084\n",
      "Epoch 2860/30000 Validation Loss: 0.11131193488836288\n",
      "Epoch 2861/30000 Training Loss: 0.10769864171743393\n",
      "Epoch 2862/30000 Training Loss: 0.1319793313741684\n",
      "Epoch 2863/30000 Training Loss: 0.126264289021492\n",
      "Epoch 2864/30000 Training Loss: 0.10406725853681564\n",
      "Epoch 2865/30000 Training Loss: 0.12093430012464523\n",
      "Epoch 2866/30000 Training Loss: 0.07847956568002701\n",
      "Epoch 2867/30000 Training Loss: 0.12438500672578812\n",
      "Epoch 2868/30000 Training Loss: 0.09992866963148117\n",
      "Epoch 2869/30000 Training Loss: 0.06939513236284256\n",
      "Epoch 2870/30000 Training Loss: 0.10820412635803223\n",
      "Epoch 2870/30000 Validation Loss: 0.10619872063398361\n",
      "Epoch 2871/30000 Training Loss: 0.08642318844795227\n",
      "Epoch 2872/30000 Training Loss: 0.08243473619222641\n",
      "Epoch 2873/30000 Training Loss: 0.08966419845819473\n",
      "Epoch 2874/30000 Training Loss: 0.10099848359823227\n",
      "Epoch 2875/30000 Training Loss: 0.11815539747476578\n",
      "Epoch 2876/30000 Training Loss: 0.07671363651752472\n",
      "Epoch 2877/30000 Training Loss: 0.09698214381933212\n",
      "Epoch 2878/30000 Training Loss: 0.08105462044477463\n",
      "Epoch 2879/30000 Training Loss: 0.09641969203948975\n",
      "Epoch 2880/30000 Training Loss: 0.11755416542291641\n",
      "Epoch 2880/30000 Validation Loss: 0.09856761246919632\n",
      "Epoch 2881/30000 Training Loss: 0.08176042884588242\n",
      "Epoch 2882/30000 Training Loss: 0.08801204711198807\n",
      "Epoch 2883/30000 Training Loss: 0.09983012825250626\n",
      "Epoch 2884/30000 Training Loss: 0.10142428427934647\n",
      "Epoch 2885/30000 Training Loss: 0.09688607603311539\n",
      "Epoch 2886/30000 Training Loss: 0.12538841366767883\n",
      "Epoch 2887/30000 Training Loss: 0.09068018943071365\n",
      "Epoch 2888/30000 Training Loss: 0.10544401407241821\n",
      "Epoch 2889/30000 Training Loss: 0.10452565550804138\n",
      "Epoch 2890/30000 Training Loss: 0.0887976661324501\n",
      "Epoch 2890/30000 Validation Loss: 0.08971411734819412\n",
      "Epoch 2891/30000 Training Loss: 0.09926748275756836\n",
      "Epoch 2892/30000 Training Loss: 0.08956313133239746\n",
      "Epoch 2893/30000 Training Loss: 0.08737734705209732\n",
      "Epoch 2894/30000 Training Loss: 0.09170418232679367\n",
      "Epoch 2895/30000 Training Loss: 0.08184021711349487\n",
      "Epoch 2896/30000 Training Loss: 0.09822540730237961\n",
      "Epoch 2897/30000 Training Loss: 0.114443339407444\n",
      "Epoch 2898/30000 Training Loss: 0.10774385929107666\n",
      "Epoch 2899/30000 Training Loss: 0.1219308003783226\n",
      "Epoch 2900/30000 Training Loss: 0.08483949303627014\n",
      "Epoch 2900/30000 Validation Loss: 0.0849718227982521\n",
      "Epoch 2901/30000 Training Loss: 0.10886243730783463\n",
      "Epoch 2902/30000 Training Loss: 0.10539223998785019\n",
      "Epoch 2903/30000 Training Loss: 0.0967145785689354\n",
      "Epoch 2904/30000 Training Loss: 0.11452395468950272\n",
      "Epoch 2905/30000 Training Loss: 0.10294941812753677\n",
      "Epoch 2906/30000 Training Loss: 0.09704124927520752\n",
      "Epoch 2907/30000 Training Loss: 0.08917168527841568\n",
      "Epoch 2908/30000 Training Loss: 0.09519540518522263\n",
      "Epoch 2909/30000 Training Loss: 0.10797715187072754\n",
      "Epoch 2910/30000 Training Loss: 0.10461831092834473\n",
      "Epoch 2910/30000 Validation Loss: 0.08766353875398636\n",
      "Epoch 2911/30000 Training Loss: 0.09978530555963516\n",
      "Epoch 2912/30000 Training Loss: 0.08348604291677475\n",
      "Epoch 2913/30000 Training Loss: 0.08512020111083984\n",
      "Epoch 2914/30000 Training Loss: 0.09798995405435562\n",
      "Epoch 2915/30000 Training Loss: 0.08771833032369614\n",
      "Epoch 2916/30000 Training Loss: 0.09338252991437912\n",
      "Epoch 2917/30000 Training Loss: 0.11277008801698685\n",
      "Epoch 2918/30000 Training Loss: 0.1129777804017067\n",
      "Epoch 2919/30000 Training Loss: 0.09377770870923996\n",
      "Epoch 2920/30000 Training Loss: 0.09709319472312927\n",
      "Epoch 2920/30000 Validation Loss: 0.09597861766815186\n",
      "Epoch 2921/30000 Training Loss: 0.11031196266412735\n",
      "Epoch 2922/30000 Training Loss: 0.09019988775253296\n",
      "Epoch 2923/30000 Training Loss: 0.10281085968017578\n",
      "Epoch 2924/30000 Training Loss: 0.0891684889793396\n",
      "Epoch 2925/30000 Training Loss: 0.11511414498090744\n",
      "Epoch 2926/30000 Training Loss: 0.08569683879613876\n",
      "Epoch 2927/30000 Training Loss: 0.0949428454041481\n",
      "Epoch 2928/30000 Training Loss: 0.08793029189109802\n",
      "Epoch 2929/30000 Training Loss: 0.133119598031044\n",
      "Epoch 2930/30000 Training Loss: 0.08783530443906784\n",
      "Epoch 2930/30000 Validation Loss: 0.11052068322896957\n",
      "Epoch 2931/30000 Training Loss: 0.0865459069609642\n",
      "Epoch 2932/30000 Training Loss: 0.09252729266881943\n",
      "Epoch 2933/30000 Training Loss: 0.09650921821594238\n",
      "Epoch 2934/30000 Training Loss: 0.10866212099790573\n",
      "Epoch 2935/30000 Training Loss: 0.08795150369405746\n",
      "Epoch 2936/30000 Training Loss: 0.08475125581026077\n",
      "Epoch 2937/30000 Training Loss: 0.09251903742551804\n",
      "Epoch 2938/30000 Training Loss: 0.127349391579628\n",
      "Epoch 2939/30000 Training Loss: 0.11597273498773575\n",
      "Epoch 2940/30000 Training Loss: 0.11107753962278366\n",
      "Epoch 2940/30000 Validation Loss: 0.09194860607385635\n",
      "Epoch 2941/30000 Training Loss: 0.09414903074502945\n",
      "Epoch 2942/30000 Training Loss: 0.08223237097263336\n",
      "Epoch 2943/30000 Training Loss: 0.08641547709703445\n",
      "Epoch 2944/30000 Training Loss: 0.10582030564546585\n",
      "Epoch 2945/30000 Training Loss: 0.09674034267663956\n",
      "Epoch 2946/30000 Training Loss: 0.09299486875534058\n",
      "Epoch 2947/30000 Training Loss: 0.07775169610977173\n",
      "Epoch 2948/30000 Training Loss: 0.09579012542963028\n",
      "Epoch 2949/30000 Training Loss: 0.09378201514482498\n",
      "Epoch 2950/30000 Training Loss: 0.09141930937767029\n",
      "Epoch 2950/30000 Validation Loss: 0.09885796159505844\n",
      "Epoch 2951/30000 Training Loss: 0.10418781638145447\n",
      "Epoch 2952/30000 Training Loss: 0.08663779497146606\n",
      "Epoch 2953/30000 Training Loss: 0.07915330678224564\n",
      "Epoch 2954/30000 Training Loss: 0.07981277257204056\n",
      "Epoch 2955/30000 Training Loss: 0.08624270558357239\n",
      "Epoch 2956/30000 Training Loss: 0.08938684314489365\n",
      "Epoch 2957/30000 Training Loss: 0.10663550347089767\n",
      "Epoch 2958/30000 Training Loss: 0.08456484228372574\n",
      "Epoch 2959/30000 Training Loss: 0.11763168126344681\n",
      "Epoch 2960/30000 Training Loss: 0.09575972706079483\n",
      "Epoch 2960/30000 Validation Loss: 0.11347541958093643\n",
      "Epoch 2961/30000 Training Loss: 0.08587103337049484\n",
      "Epoch 2962/30000 Training Loss: 0.08444968611001968\n",
      "Epoch 2963/30000 Training Loss: 0.10115597397089005\n",
      "Epoch 2964/30000 Training Loss: 0.0984039306640625\n",
      "Epoch 2965/30000 Training Loss: 0.09187355637550354\n",
      "Epoch 2966/30000 Training Loss: 0.09023591130971909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2967/30000 Training Loss: 0.09071868658065796\n",
      "Epoch 2968/30000 Training Loss: 0.11241629719734192\n",
      "Epoch 2969/30000 Training Loss: 0.07247096300125122\n",
      "Epoch 2970/30000 Training Loss: 0.08332439512014389\n",
      "Epoch 2970/30000 Validation Loss: 0.09424159675836563\n",
      "Epoch 2971/30000 Training Loss: 0.10627024620771408\n",
      "Epoch 2972/30000 Training Loss: 0.08822152763605118\n",
      "Epoch 2973/30000 Training Loss: 0.07589738816022873\n",
      "Epoch 2974/30000 Training Loss: 0.09175703674554825\n",
      "Epoch 2975/30000 Training Loss: 0.098183773458004\n",
      "Epoch 2976/30000 Training Loss: 0.08804050087928772\n",
      "Epoch 2977/30000 Training Loss: 0.09379800409078598\n",
      "Epoch 2978/30000 Training Loss: 0.07917916029691696\n",
      "Epoch 2979/30000 Training Loss: 0.08523619174957275\n",
      "Epoch 2980/30000 Training Loss: 0.12027507275342941\n",
      "Epoch 2980/30000 Validation Loss: 0.07953952997922897\n",
      "Epoch 2981/30000 Training Loss: 0.09683986753225327\n",
      "Epoch 2982/30000 Training Loss: 0.10023031383752823\n",
      "Epoch 2983/30000 Training Loss: 0.0789397656917572\n",
      "Epoch 2984/30000 Training Loss: 0.06704532355070114\n",
      "Epoch 2985/30000 Training Loss: 0.12167950719594955\n",
      "Epoch 2986/30000 Training Loss: 0.08224992454051971\n",
      "Epoch 2987/30000 Training Loss: 0.12438883632421494\n",
      "Epoch 2988/30000 Training Loss: 0.07278245687484741\n",
      "Epoch 2989/30000 Training Loss: 0.09730202704668045\n",
      "Epoch 2990/30000 Training Loss: 0.08402476459741592\n",
      "Epoch 2990/30000 Validation Loss: 0.07010937482118607\n",
      "Epoch 2991/30000 Training Loss: 0.1207253709435463\n",
      "Epoch 2992/30000 Training Loss: 0.08278961479663849\n",
      "Epoch 2993/30000 Training Loss: 0.08202273398637772\n",
      "Epoch 2994/30000 Training Loss: 0.08351383358240128\n",
      "Epoch 2995/30000 Training Loss: 0.1028803363442421\n",
      "Epoch 2996/30000 Training Loss: 0.09994414448738098\n",
      "Epoch 2997/30000 Training Loss: 0.10237138718366623\n",
      "Epoch 2998/30000 Training Loss: 0.09118201583623886\n",
      "Epoch 2999/30000 Training Loss: 0.09804823249578476\n",
      "Epoch 3000/30000 Training Loss: 0.08574510365724564\n",
      "Epoch 3000/30000 Validation Loss: 0.10438808053731918\n",
      "Epoch 3001/30000 Training Loss: 0.08811970800161362\n",
      "Epoch 3002/30000 Training Loss: 0.09854824095964432\n",
      "Epoch 3003/30000 Training Loss: 0.08415940403938293\n",
      "Epoch 3004/30000 Training Loss: 0.08642514795064926\n",
      "Epoch 3005/30000 Training Loss: 0.10924121737480164\n",
      "Epoch 3006/30000 Training Loss: 0.11883827298879623\n",
      "Epoch 3007/30000 Training Loss: 0.10622536391019821\n",
      "Epoch 3008/30000 Training Loss: 0.08109789341688156\n",
      "Epoch 3009/30000 Training Loss: 0.08671089261770248\n",
      "Epoch 3010/30000 Training Loss: 0.10453981161117554\n",
      "Epoch 3010/30000 Validation Loss: 0.11684087663888931\n",
      "Epoch 3011/30000 Training Loss: 0.09975839406251907\n",
      "Epoch 3012/30000 Training Loss: 0.11686119437217712\n",
      "Epoch 3013/30000 Training Loss: 0.08887767791748047\n",
      "Epoch 3014/30000 Training Loss: 0.09148615598678589\n",
      "Epoch 3015/30000 Training Loss: 0.09943673014640808\n",
      "Epoch 3016/30000 Training Loss: 0.12134405970573425\n",
      "Epoch 3017/30000 Training Loss: 0.09914781898260117\n",
      "Epoch 3018/30000 Training Loss: 0.09181917458772659\n",
      "Epoch 3019/30000 Training Loss: 0.08636832237243652\n",
      "Epoch 3020/30000 Training Loss: 0.1250080019235611\n",
      "Epoch 3020/30000 Validation Loss: 0.10315040498971939\n",
      "Epoch 3021/30000 Training Loss: 0.09751815348863602\n",
      "Epoch 3022/30000 Training Loss: 0.10071117430925369\n",
      "Epoch 3023/30000 Training Loss: 0.0915905237197876\n",
      "Epoch 3024/30000 Training Loss: 0.08418533951044083\n",
      "Epoch 3025/30000 Training Loss: 0.09636463969945908\n",
      "Epoch 3026/30000 Training Loss: 0.10358811169862747\n",
      "Epoch 3027/30000 Training Loss: 0.09617891907691956\n",
      "Epoch 3028/30000 Training Loss: 0.07837901264429092\n",
      "Epoch 3029/30000 Training Loss: 0.10288911312818527\n",
      "Epoch 3030/30000 Training Loss: 0.07966423034667969\n",
      "Epoch 3030/30000 Validation Loss: 0.08290805667638779\n",
      "Epoch 3031/30000 Training Loss: 0.07666002959012985\n",
      "Epoch 3032/30000 Training Loss: 0.1043601855635643\n",
      "Epoch 3033/30000 Training Loss: 0.09292901307344437\n",
      "Epoch 3034/30000 Training Loss: 0.11273331195116043\n",
      "Epoch 3035/30000 Training Loss: 0.10312303155660629\n",
      "Epoch 3036/30000 Training Loss: 0.10849905014038086\n",
      "Epoch 3037/30000 Training Loss: 0.10356571525335312\n",
      "Epoch 3038/30000 Training Loss: 0.08729630708694458\n",
      "Epoch 3039/30000 Training Loss: 0.09165177494287491\n",
      "Epoch 3040/30000 Training Loss: 0.10777986794710159\n",
      "Epoch 3040/30000 Validation Loss: 0.09376451373100281\n",
      "Epoch 3041/30000 Training Loss: 0.06584569066762924\n",
      "Epoch 3042/30000 Training Loss: 0.08772651106119156\n",
      "Epoch 3043/30000 Training Loss: 0.09941811114549637\n",
      "Epoch 3044/30000 Training Loss: 0.1200050339102745\n",
      "Epoch 3045/30000 Training Loss: 0.10563898086547852\n",
      "Epoch 3046/30000 Training Loss: 0.09413032978773117\n",
      "Epoch 3047/30000 Training Loss: 0.09538859128952026\n",
      "Epoch 3048/30000 Training Loss: 0.11161293834447861\n",
      "Epoch 3049/30000 Training Loss: 0.09285057336091995\n",
      "Epoch 3050/30000 Training Loss: 0.08648157119750977\n",
      "Epoch 3050/30000 Validation Loss: 0.10695169121026993\n",
      "Epoch 3051/30000 Training Loss: 0.0834813043475151\n",
      "Epoch 3052/30000 Training Loss: 0.08968374133110046\n",
      "Epoch 3053/30000 Training Loss: 0.09076008200645447\n",
      "Epoch 3054/30000 Training Loss: 0.08647160977125168\n",
      "Epoch 3055/30000 Training Loss: 0.10497147589921951\n",
      "Epoch 3056/30000 Training Loss: 0.07689841836690903\n",
      "Epoch 3057/30000 Training Loss: 0.09913992881774902\n",
      "Epoch 3058/30000 Training Loss: 0.0771249383687973\n",
      "Epoch 3059/30000 Training Loss: 0.08073320984840393\n",
      "Epoch 3060/30000 Training Loss: 0.1064896211028099\n",
      "Epoch 3060/30000 Validation Loss: 0.11330422759056091\n",
      "Epoch 3061/30000 Training Loss: 0.09106054157018661\n",
      "Epoch 3062/30000 Training Loss: 0.0980275496840477\n",
      "Epoch 3063/30000 Training Loss: 0.06526186317205429\n",
      "Epoch 3064/30000 Training Loss: 0.0797794982790947\n",
      "Epoch 3065/30000 Training Loss: 0.10421750694513321\n",
      "Epoch 3066/30000 Training Loss: 0.10029365867376328\n",
      "Epoch 3067/30000 Training Loss: 0.07427825778722763\n",
      "Epoch 3068/30000 Training Loss: 0.1148211732506752\n",
      "Epoch 3069/30000 Training Loss: 0.0803384780883789\n",
      "Epoch 3070/30000 Training Loss: 0.10135316103696823\n",
      "Epoch 3070/30000 Validation Loss: 0.13478323817253113\n",
      "Epoch 3071/30000 Training Loss: 0.08829453587532043\n",
      "Epoch 3072/30000 Training Loss: 0.08550546318292618\n",
      "Epoch 3073/30000 Training Loss: 0.10397083312273026\n",
      "Epoch 3074/30000 Training Loss: 0.10481541603803635\n",
      "Epoch 3075/30000 Training Loss: 0.10169138759374619\n",
      "Epoch 3076/30000 Training Loss: 0.09540432691574097\n",
      "Epoch 3077/30000 Training Loss: 0.08556324988603592\n",
      "Epoch 3078/30000 Training Loss: 0.08117694407701492\n",
      "Epoch 3079/30000 Training Loss: 0.08690578490495682\n",
      "Epoch 3080/30000 Training Loss: 0.0730244517326355\n",
      "Epoch 3080/30000 Validation Loss: 0.1197795495390892\n",
      "Epoch 3081/30000 Training Loss: 0.10302291065454483\n",
      "Epoch 3082/30000 Training Loss: 0.08173166960477829\n",
      "Epoch 3083/30000 Training Loss: 0.11434799432754517\n",
      "Epoch 3084/30000 Training Loss: 0.0907771959900856\n",
      "Epoch 3085/30000 Training Loss: 0.08812952041625977\n",
      "Epoch 3086/30000 Training Loss: 0.10848488658666611\n",
      "Epoch 3087/30000 Training Loss: 0.08791539818048477\n",
      "Epoch 3088/30000 Training Loss: 0.09782508760690689\n",
      "Epoch 3089/30000 Training Loss: 0.0894322395324707\n",
      "Epoch 3090/30000 Training Loss: 0.10075277090072632\n",
      "Epoch 3090/30000 Validation Loss: 0.09008661657571793\n",
      "Epoch 3091/30000 Training Loss: 0.13037872314453125\n",
      "Epoch 3092/30000 Training Loss: 0.11331024020910263\n",
      "Epoch 3093/30000 Training Loss: 0.08661290258169174\n",
      "Epoch 3094/30000 Training Loss: 0.10114332288503647\n",
      "Epoch 3095/30000 Training Loss: 0.08568225055932999\n",
      "Epoch 3096/30000 Training Loss: 0.09072334319353104\n",
      "Epoch 3097/30000 Training Loss: 0.09395747631788254\n",
      "Epoch 3098/30000 Training Loss: 0.1017656996846199\n",
      "Epoch 3099/30000 Training Loss: 0.08378537744283676\n",
      "Epoch 3100/30000 Training Loss: 0.07828149199485779\n",
      "Epoch 3100/30000 Validation Loss: 0.09848668426275253\n",
      "Epoch 3101/30000 Training Loss: 0.08625561743974686\n",
      "Epoch 3102/30000 Training Loss: 0.09594083577394485\n",
      "Epoch 3103/30000 Training Loss: 0.09001778811216354\n",
      "Epoch 3104/30000 Training Loss: 0.10177260637283325\n",
      "Epoch 3105/30000 Training Loss: 0.11213584989309311\n",
      "Epoch 3106/30000 Training Loss: 0.08960863202810287\n",
      "Epoch 3107/30000 Training Loss: 0.09703201055526733\n",
      "Epoch 3108/30000 Training Loss: 0.08898235112428665\n",
      "Epoch 3109/30000 Training Loss: 0.10160364955663681\n",
      "Epoch 3110/30000 Training Loss: 0.08480129390954971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3110/30000 Validation Loss: 0.09766333550214767\n",
      "Epoch 3111/30000 Training Loss: 0.07830238342285156\n",
      "Epoch 3112/30000 Training Loss: 0.10179492086172104\n",
      "Epoch 3113/30000 Training Loss: 0.08469311147928238\n",
      "Epoch 3114/30000 Training Loss: 0.10147342085838318\n",
      "Epoch 3115/30000 Training Loss: 0.10016906261444092\n",
      "Epoch 3116/30000 Training Loss: 0.09789875894784927\n",
      "Epoch 3117/30000 Training Loss: 0.09849683195352554\n",
      "Epoch 3118/30000 Training Loss: 0.09354259818792343\n",
      "Epoch 3119/30000 Training Loss: 0.12203965336084366\n",
      "Epoch 3120/30000 Training Loss: 0.09919083118438721\n",
      "Epoch 3120/30000 Validation Loss: 0.09170664101839066\n",
      "Epoch 3121/30000 Training Loss: 0.09175580739974976\n",
      "Epoch 3122/30000 Training Loss: 0.1351953148841858\n",
      "Epoch 3123/30000 Training Loss: 0.12651190161705017\n",
      "Epoch 3124/30000 Training Loss: 0.11183727532625198\n",
      "Epoch 3125/30000 Training Loss: 0.11311737447977066\n",
      "Epoch 3126/30000 Training Loss: 0.09963669627904892\n",
      "Epoch 3127/30000 Training Loss: 0.09277524799108505\n",
      "Epoch 3128/30000 Training Loss: 0.1177155151963234\n",
      "Epoch 3129/30000 Training Loss: 0.09553033113479614\n",
      "Epoch 3130/30000 Training Loss: 0.09820568561553955\n",
      "Epoch 3130/30000 Validation Loss: 0.11379852145910263\n",
      "Epoch 3131/30000 Training Loss: 0.10422676801681519\n",
      "Epoch 3132/30000 Training Loss: 0.09104713052511215\n",
      "Epoch 3133/30000 Training Loss: 0.09561837464570999\n",
      "Epoch 3134/30000 Training Loss: 0.07632608711719513\n",
      "Epoch 3135/30000 Training Loss: 0.0957028865814209\n",
      "Epoch 3136/30000 Training Loss: 0.08935882896184921\n",
      "Epoch 3137/30000 Training Loss: 0.10179054737091064\n",
      "Epoch 3138/30000 Training Loss: 0.08645933121442795\n",
      "Epoch 3139/30000 Training Loss: 0.09459749609231949\n",
      "Epoch 3140/30000 Training Loss: 0.08752032369375229\n",
      "Epoch 3140/30000 Validation Loss: 0.10300233215093613\n",
      "Epoch 3141/30000 Training Loss: 0.09033691138029099\n",
      "Epoch 3142/30000 Training Loss: 0.09490075707435608\n",
      "Epoch 3143/30000 Training Loss: 0.09468940645456314\n",
      "Epoch 3144/30000 Training Loss: 0.08470196276903152\n",
      "Epoch 3145/30000 Training Loss: 0.10780251771211624\n",
      "Epoch 3146/30000 Training Loss: 0.07997564226388931\n",
      "Epoch 3147/30000 Training Loss: 0.12580260634422302\n",
      "Epoch 3148/30000 Training Loss: 0.11174877732992172\n",
      "Epoch 3149/30000 Training Loss: 0.09420644491910934\n",
      "Epoch 3150/30000 Training Loss: 0.09641584753990173\n",
      "Epoch 3150/30000 Validation Loss: 0.08623049408197403\n",
      "Epoch 3151/30000 Training Loss: 0.11653000861406326\n",
      "Epoch 3152/30000 Training Loss: 0.10778240114450455\n",
      "Epoch 3153/30000 Training Loss: 0.08657664060592651\n",
      "Epoch 3154/30000 Training Loss: 0.08348862081766129\n",
      "Epoch 3155/30000 Training Loss: 0.07849112898111343\n",
      "Epoch 3156/30000 Training Loss: 0.08959945291280746\n",
      "Epoch 3157/30000 Training Loss: 0.09433349221944809\n",
      "Epoch 3158/30000 Training Loss: 0.1163477897644043\n",
      "Epoch 3159/30000 Training Loss: 0.08297490328550339\n",
      "Epoch 3160/30000 Training Loss: 0.08633333444595337\n",
      "Epoch 3160/30000 Validation Loss: 0.09938871115446091\n",
      "Epoch 3161/30000 Training Loss: 0.09185933321714401\n",
      "Epoch 3162/30000 Training Loss: 0.07517573982477188\n",
      "Epoch 3163/30000 Training Loss: 0.10220655798912048\n",
      "Epoch 3164/30000 Training Loss: 0.0961013063788414\n",
      "Epoch 3165/30000 Training Loss: 0.09609480947256088\n",
      "Epoch 3166/30000 Training Loss: 0.09485875815153122\n",
      "Epoch 3167/30000 Training Loss: 0.10285082459449768\n",
      "Epoch 3168/30000 Training Loss: 0.08275730162858963\n",
      "Epoch 3169/30000 Training Loss: 0.0986802875995636\n",
      "Epoch 3170/30000 Training Loss: 0.10136497765779495\n",
      "Epoch 3170/30000 Validation Loss: 0.08809290081262589\n",
      "Epoch 3171/30000 Training Loss: 0.10593070834875107\n",
      "Epoch 3172/30000 Training Loss: 0.09088257700204849\n",
      "Epoch 3173/30000 Training Loss: 0.09243490546941757\n",
      "Epoch 3174/30000 Training Loss: 0.08480280637741089\n",
      "Epoch 3175/30000 Training Loss: 0.09948229789733887\n",
      "Epoch 3176/30000 Training Loss: 0.12921728193759918\n",
      "Epoch 3177/30000 Training Loss: 0.08705120533704758\n",
      "Epoch 3178/30000 Training Loss: 0.09275803714990616\n",
      "Epoch 3179/30000 Training Loss: 0.08435054868459702\n",
      "Epoch 3180/30000 Training Loss: 0.08504115790128708\n",
      "Epoch 3180/30000 Validation Loss: 0.10436376184225082\n",
      "Epoch 3181/30000 Training Loss: 0.06773485243320465\n",
      "Epoch 3182/30000 Training Loss: 0.10240838676691055\n",
      "Epoch 3183/30000 Training Loss: 0.09502915292978287\n",
      "Epoch 3184/30000 Training Loss: 0.09838920831680298\n",
      "Epoch 3185/30000 Training Loss: 0.0771431252360344\n",
      "Epoch 3186/30000 Training Loss: 0.08607051521539688\n",
      "Epoch 3187/30000 Training Loss: 0.08264521509408951\n",
      "Epoch 3188/30000 Training Loss: 0.09550461918115616\n",
      "Epoch 3189/30000 Training Loss: 0.11353593319654465\n",
      "Epoch 3190/30000 Training Loss: 0.09562697261571884\n",
      "Epoch 3190/30000 Validation Loss: 0.09071429818868637\n",
      "Epoch 3191/30000 Training Loss: 0.0914299264550209\n",
      "Epoch 3192/30000 Training Loss: 0.10264154523611069\n",
      "Epoch 3193/30000 Training Loss: 0.10525453090667725\n",
      "Epoch 3194/30000 Training Loss: 0.11052953451871872\n",
      "Epoch 3195/30000 Training Loss: 0.09357503801584244\n",
      "Epoch 3196/30000 Training Loss: 0.08077409118413925\n",
      "Epoch 3197/30000 Training Loss: 0.10176274180412292\n",
      "Epoch 3198/30000 Training Loss: 0.10002332925796509\n",
      "Epoch 3199/30000 Training Loss: 0.08047499507665634\n",
      "Epoch 3200/30000 Training Loss: 0.10493600368499756\n",
      "Epoch 3200/30000 Validation Loss: 0.09580006450414658\n",
      "Epoch 3201/30000 Training Loss: 0.09744999557733536\n",
      "Epoch 3202/30000 Training Loss: 0.11762532591819763\n",
      "Epoch 3203/30000 Training Loss: 0.09005973488092422\n",
      "Epoch 3204/30000 Training Loss: 0.07554297894239426\n",
      "Epoch 3205/30000 Training Loss: 0.0854773223400116\n",
      "Epoch 3206/30000 Training Loss: 0.11384376138448715\n",
      "Epoch 3207/30000 Training Loss: 0.11688455194234848\n",
      "Epoch 3208/30000 Training Loss: 0.08087985962629318\n",
      "Epoch 3209/30000 Training Loss: 0.09106763452291489\n",
      "Epoch 3210/30000 Training Loss: 0.07789421826601028\n",
      "Epoch 3210/30000 Validation Loss: 0.13022784888744354\n",
      "Epoch 3211/30000 Training Loss: 0.07981452345848083\n",
      "Epoch 3212/30000 Training Loss: 0.12464114278554916\n",
      "Epoch 3213/30000 Training Loss: 0.11121898889541626\n",
      "Epoch 3214/30000 Training Loss: 0.086344413459301\n",
      "Epoch 3215/30000 Training Loss: 0.08891194313764572\n",
      "Epoch 3216/30000 Training Loss: 0.10985102504491806\n",
      "Epoch 3217/30000 Training Loss: 0.08662576228380203\n",
      "Epoch 3218/30000 Training Loss: 0.08090671896934509\n",
      "Epoch 3219/30000 Training Loss: 0.09118783473968506\n",
      "Epoch 3220/30000 Training Loss: 0.08366122096776962\n",
      "Epoch 3220/30000 Validation Loss: 0.09674108773469925\n",
      "Epoch 3221/30000 Training Loss: 0.10076961666345596\n",
      "Epoch 3222/30000 Training Loss: 0.11844589561223984\n",
      "Epoch 3223/30000 Training Loss: 0.0922582671046257\n",
      "Epoch 3224/30000 Training Loss: 0.07881969958543777\n",
      "Epoch 3225/30000 Training Loss: 0.07761361449956894\n",
      "Epoch 3226/30000 Training Loss: 0.09199327975511551\n",
      "Epoch 3227/30000 Training Loss: 0.0795341357588768\n",
      "Epoch 3228/30000 Training Loss: 0.10626232624053955\n",
      "Epoch 3229/30000 Training Loss: 0.09705977886915207\n",
      "Epoch 3230/30000 Training Loss: 0.11216926574707031\n",
      "Epoch 3230/30000 Validation Loss: 0.09527222067117691\n",
      "Epoch 3231/30000 Training Loss: 0.08581265062093735\n",
      "Epoch 3232/30000 Training Loss: 0.11997135728597641\n",
      "Epoch 3233/30000 Training Loss: 0.07404728978872299\n",
      "Epoch 3234/30000 Training Loss: 0.1010531410574913\n",
      "Epoch 3235/30000 Training Loss: 0.08886110037565231\n",
      "Epoch 3236/30000 Training Loss: 0.07683733105659485\n",
      "Epoch 3237/30000 Training Loss: 0.09029281884431839\n",
      "Epoch 3238/30000 Training Loss: 0.09797972440719604\n",
      "Epoch 3239/30000 Training Loss: 0.10970132797956467\n",
      "Epoch 3240/30000 Training Loss: 0.1236523762345314\n",
      "Epoch 3240/30000 Validation Loss: 0.08967699855566025\n",
      "Epoch 3241/30000 Training Loss: 0.09615373611450195\n",
      "Epoch 3242/30000 Training Loss: 0.09913721680641174\n",
      "Epoch 3243/30000 Training Loss: 0.08343590050935745\n",
      "Epoch 3244/30000 Training Loss: 0.0968923568725586\n",
      "Epoch 3245/30000 Training Loss: 0.10273021459579468\n",
      "Epoch 3246/30000 Training Loss: 0.1113155260682106\n",
      "Epoch 3247/30000 Training Loss: 0.09341859817504883\n",
      "Epoch 3248/30000 Training Loss: 0.08406569808721542\n",
      "Epoch 3249/30000 Training Loss: 0.08074010163545609\n",
      "Epoch 3250/30000 Training Loss: 0.0878649353981018\n",
      "Epoch 3250/30000 Validation Loss: 0.1080847904086113\n",
      "Epoch 3251/30000 Training Loss: 0.089175283908844\n",
      "Epoch 3252/30000 Training Loss: 0.09783416986465454\n",
      "Epoch 3253/30000 Training Loss: 0.09517946094274521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3254/30000 Training Loss: 0.056438155472278595\n",
      "Epoch 3255/30000 Training Loss: 0.08622743934392929\n",
      "Epoch 3256/30000 Training Loss: 0.09843659400939941\n",
      "Epoch 3257/30000 Training Loss: 0.11594215035438538\n",
      "Epoch 3258/30000 Training Loss: 0.09432383626699448\n",
      "Epoch 3259/30000 Training Loss: 0.11603347212076187\n",
      "Epoch 3260/30000 Training Loss: 0.06956323236227036\n",
      "Epoch 3260/30000 Validation Loss: 0.11541512608528137\n",
      "Epoch 3261/30000 Training Loss: 0.07669427245855331\n",
      "Epoch 3262/30000 Training Loss: 0.08314087241888046\n",
      "Epoch 3263/30000 Training Loss: 0.0912233367562294\n",
      "Epoch 3264/30000 Training Loss: 0.10798698663711548\n",
      "Epoch 3265/30000 Training Loss: 0.10042815655469894\n",
      "Epoch 3266/30000 Training Loss: 0.07394116371870041\n",
      "Epoch 3267/30000 Training Loss: 0.1095677837729454\n",
      "Epoch 3268/30000 Training Loss: 0.08744629472494125\n",
      "Epoch 3269/30000 Training Loss: 0.09454331547021866\n",
      "Epoch 3270/30000 Training Loss: 0.0854429304599762\n",
      "Epoch 3270/30000 Validation Loss: 0.11560643464326859\n",
      "Epoch 3271/30000 Training Loss: 0.09644194692373276\n",
      "Epoch 3272/30000 Training Loss: 0.09128165245056152\n",
      "Epoch 3273/30000 Training Loss: 0.0958852469921112\n",
      "Epoch 3274/30000 Training Loss: 0.09007342904806137\n",
      "Epoch 3275/30000 Training Loss: 0.10271430760622025\n",
      "Epoch 3276/30000 Training Loss: 0.09530576318502426\n",
      "Epoch 3277/30000 Training Loss: 0.09583883732557297\n",
      "Epoch 3278/30000 Training Loss: 0.10663021355867386\n",
      "Epoch 3279/30000 Training Loss: 0.08812400698661804\n",
      "Epoch 3280/30000 Training Loss: 0.09109249711036682\n",
      "Epoch 3280/30000 Validation Loss: 0.06955228745937347\n",
      "Epoch 3281/30000 Training Loss: 0.10118376463651657\n",
      "Epoch 3282/30000 Training Loss: 0.07087331265211105\n",
      "Epoch 3283/30000 Training Loss: 0.08396074920892715\n",
      "Epoch 3284/30000 Training Loss: 0.09049782902002335\n",
      "Epoch 3285/30000 Training Loss: 0.08471817523241043\n",
      "Epoch 3286/30000 Training Loss: 0.08095800131559372\n",
      "Epoch 3287/30000 Training Loss: 0.0862497016787529\n",
      "Epoch 3288/30000 Training Loss: 0.06259802728891373\n",
      "Epoch 3289/30000 Training Loss: 0.10756515711545944\n",
      "Epoch 3290/30000 Training Loss: 0.07603288441896439\n",
      "Epoch 3290/30000 Validation Loss: 0.08679452538490295\n",
      "Epoch 3291/30000 Training Loss: 0.07617742568254471\n",
      "Epoch 3292/30000 Training Loss: 0.07581733912229538\n",
      "Epoch 3293/30000 Training Loss: 0.08723060041666031\n",
      "Epoch 3294/30000 Training Loss: 0.11440339684486389\n",
      "Epoch 3295/30000 Training Loss: 0.11505919694900513\n",
      "Epoch 3296/30000 Training Loss: 0.07442064583301544\n",
      "Epoch 3297/30000 Training Loss: 0.09001124650239944\n",
      "Epoch 3298/30000 Training Loss: 0.09269366413354874\n",
      "Epoch 3299/30000 Training Loss: 0.10867392271757126\n",
      "Epoch 3300/30000 Training Loss: 0.10500087589025497\n",
      "Epoch 3300/30000 Validation Loss: 0.08486554771661758\n",
      "Epoch 3301/30000 Training Loss: 0.12084278464317322\n",
      "Epoch 3302/30000 Training Loss: 0.11945497989654541\n",
      "Epoch 3303/30000 Training Loss: 0.11142931133508682\n",
      "Epoch 3304/30000 Training Loss: 0.1001478061079979\n",
      "Epoch 3305/30000 Training Loss: 0.10087227821350098\n",
      "Epoch 3306/30000 Training Loss: 0.10349429398775101\n",
      "Epoch 3307/30000 Training Loss: 0.10657884925603867\n",
      "Epoch 3308/30000 Training Loss: 0.07621649652719498\n",
      "Epoch 3309/30000 Training Loss: 0.09633579105138779\n",
      "Epoch 3310/30000 Training Loss: 0.08643513917922974\n",
      "Epoch 3310/30000 Validation Loss: 0.09766524285078049\n",
      "Epoch 3311/30000 Training Loss: 0.08976525068283081\n",
      "Epoch 3312/30000 Training Loss: 0.08093441277742386\n",
      "Epoch 3313/30000 Training Loss: 0.10350105166435242\n",
      "Epoch 3314/30000 Training Loss: 0.09699022769927979\n",
      "Epoch 3315/30000 Training Loss: 0.09655051678419113\n",
      "Epoch 3316/30000 Training Loss: 0.10945302248001099\n",
      "Epoch 3317/30000 Training Loss: 0.08938119560480118\n",
      "Epoch 3318/30000 Training Loss: 0.07817734032869339\n",
      "Epoch 3319/30000 Training Loss: 0.11101026087999344\n",
      "Epoch 3320/30000 Training Loss: 0.08503293991088867\n",
      "Epoch 3320/30000 Validation Loss: 0.09310641139745712\n",
      "Epoch 3321/30000 Training Loss: 0.08581336587667465\n",
      "Epoch 3322/30000 Training Loss: 0.10449537634849548\n",
      "Epoch 3323/30000 Training Loss: 0.09921830892562866\n",
      "Epoch 3324/30000 Training Loss: 0.10119103640317917\n",
      "Epoch 3325/30000 Training Loss: 0.08945909887552261\n",
      "Epoch 3326/30000 Training Loss: 0.12890367209911346\n",
      "Epoch 3327/30000 Training Loss: 0.08744347095489502\n",
      "Epoch 3328/30000 Training Loss: 0.09170639514923096\n",
      "Epoch 3329/30000 Training Loss: 0.11466716974973679\n",
      "Epoch 3330/30000 Training Loss: 0.0882793441414833\n",
      "Epoch 3330/30000 Validation Loss: 0.08860639482736588\n",
      "Epoch 3331/30000 Training Loss: 0.1074272021651268\n",
      "Epoch 3332/30000 Training Loss: 0.09264067560434341\n",
      "Epoch 3333/30000 Training Loss: 0.10590078681707382\n",
      "Epoch 3334/30000 Training Loss: 0.09366314858198166\n",
      "Epoch 3335/30000 Training Loss: 0.0835598036646843\n",
      "Epoch 3336/30000 Training Loss: 0.10952994972467422\n",
      "Epoch 3337/30000 Training Loss: 0.10302164405584335\n",
      "Epoch 3338/30000 Training Loss: 0.10597062110900879\n",
      "Epoch 3339/30000 Training Loss: 0.09545166045427322\n",
      "Epoch 3340/30000 Training Loss: 0.09717565029859543\n",
      "Epoch 3340/30000 Validation Loss: 0.11097551137208939\n",
      "Epoch 3341/30000 Training Loss: 0.08770204335451126\n",
      "Epoch 3342/30000 Training Loss: 0.08327103406190872\n",
      "Epoch 3343/30000 Training Loss: 0.08302941918373108\n",
      "Epoch 3344/30000 Training Loss: 0.10080090910196304\n",
      "Epoch 3345/30000 Training Loss: 0.0892735943198204\n",
      "Epoch 3346/30000 Training Loss: 0.0899052545428276\n",
      "Epoch 3347/30000 Training Loss: 0.10100222378969193\n",
      "Epoch 3348/30000 Training Loss: 0.08705572038888931\n",
      "Epoch 3349/30000 Training Loss: 0.09423009306192398\n",
      "Epoch 3350/30000 Training Loss: 0.10244827717542648\n",
      "Epoch 3350/30000 Validation Loss: 0.09092069417238235\n",
      "Epoch 3351/30000 Training Loss: 0.10399538278579712\n",
      "Epoch 3352/30000 Training Loss: 0.10065106302499771\n",
      "Epoch 3353/30000 Training Loss: 0.0926394984126091\n",
      "Epoch 3354/30000 Training Loss: 0.09237458556890488\n",
      "Epoch 3355/30000 Training Loss: 0.09729790687561035\n",
      "Epoch 3356/30000 Training Loss: 0.10452857613563538\n",
      "Epoch 3357/30000 Training Loss: 0.09599527716636658\n",
      "Epoch 3358/30000 Training Loss: 0.09437229484319687\n",
      "Epoch 3359/30000 Training Loss: 0.09461778402328491\n",
      "Epoch 3360/30000 Training Loss: 0.07774665206670761\n",
      "Epoch 3360/30000 Validation Loss: 0.07333626598119736\n",
      "Epoch 3361/30000 Training Loss: 0.10584475845098495\n",
      "Epoch 3362/30000 Training Loss: 0.08153340965509415\n",
      "Epoch 3363/30000 Training Loss: 0.1212063655257225\n",
      "Epoch 3364/30000 Training Loss: 0.09458869695663452\n",
      "Epoch 3365/30000 Training Loss: 0.10445485264062881\n",
      "Epoch 3366/30000 Training Loss: 0.09411825984716415\n",
      "Epoch 3367/30000 Training Loss: 0.07815995812416077\n",
      "Epoch 3368/30000 Training Loss: 0.09421578794717789\n",
      "Epoch 3369/30000 Training Loss: 0.0782826840877533\n",
      "Epoch 3370/30000 Training Loss: 0.07206839323043823\n",
      "Epoch 3370/30000 Validation Loss: 0.08796223998069763\n",
      "Epoch 3371/30000 Training Loss: 0.09390590339899063\n",
      "Epoch 3372/30000 Training Loss: 0.09753984212875366\n",
      "Epoch 3373/30000 Training Loss: 0.08317914605140686\n",
      "Epoch 3374/30000 Training Loss: 0.10897710174322128\n",
      "Epoch 3375/30000 Training Loss: 0.12284795194864273\n",
      "Epoch 3376/30000 Training Loss: 0.08253329992294312\n",
      "Epoch 3377/30000 Training Loss: 0.07720273733139038\n",
      "Epoch 3378/30000 Training Loss: 0.11208972334861755\n",
      "Epoch 3379/30000 Training Loss: 0.07770156115293503\n",
      "Epoch 3380/30000 Training Loss: 0.09667801111936569\n",
      "Epoch 3380/30000 Validation Loss: 0.08222383260726929\n",
      "Epoch 3381/30000 Training Loss: 0.09273064881563187\n",
      "Epoch 3382/30000 Training Loss: 0.0907452329993248\n",
      "Epoch 3383/30000 Training Loss: 0.07411849498748779\n",
      "Epoch 3384/30000 Training Loss: 0.09939240664243698\n",
      "Epoch 3385/30000 Training Loss: 0.07523375749588013\n",
      "Epoch 3386/30000 Training Loss: 0.10629203170537949\n",
      "Epoch 3387/30000 Training Loss: 0.09750283509492874\n",
      "Epoch 3388/30000 Training Loss: 0.09178610891103745\n",
      "Epoch 3389/30000 Training Loss: 0.08397188782691956\n",
      "Epoch 3390/30000 Training Loss: 0.08503594994544983\n",
      "Epoch 3390/30000 Validation Loss: 0.12763728201389313\n",
      "Epoch 3391/30000 Training Loss: 0.10906919091939926\n",
      "Epoch 3392/30000 Training Loss: 0.10447459667921066\n",
      "Epoch 3393/30000 Training Loss: 0.08435194939374924\n",
      "Epoch 3394/30000 Training Loss: 0.08389806002378464\n",
      "Epoch 3395/30000 Training Loss: 0.11978242546319962\n",
      "Epoch 3396/30000 Training Loss: 0.0989060178399086\n",
      "Epoch 3397/30000 Training Loss: 0.09867637604475021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3398/30000 Training Loss: 0.10418739169836044\n",
      "Epoch 3399/30000 Training Loss: 0.08902927488088608\n",
      "Epoch 3400/30000 Training Loss: 0.09764374047517776\n",
      "Epoch 3400/30000 Validation Loss: 0.07564189285039902\n",
      "Epoch 3401/30000 Training Loss: 0.07394243031740189\n",
      "Epoch 3402/30000 Training Loss: 0.10033544898033142\n",
      "Epoch 3403/30000 Training Loss: 0.09519597887992859\n",
      "Epoch 3404/30000 Training Loss: 0.09004285931587219\n",
      "Epoch 3405/30000 Training Loss: 0.09863853454589844\n",
      "Epoch 3406/30000 Training Loss: 0.0821089968085289\n",
      "Epoch 3407/30000 Training Loss: 0.11443999409675598\n",
      "Epoch 3408/30000 Training Loss: 0.0903402641415596\n",
      "Epoch 3409/30000 Training Loss: 0.08384837955236435\n",
      "Epoch 3410/30000 Training Loss: 0.08272338658571243\n",
      "Epoch 3410/30000 Validation Loss: 0.07366553694009781\n",
      "Epoch 3411/30000 Training Loss: 0.07799644023180008\n",
      "Epoch 3412/30000 Training Loss: 0.08458027243614197\n",
      "Epoch 3413/30000 Training Loss: 0.11328554153442383\n",
      "Epoch 3414/30000 Training Loss: 0.10560795664787292\n",
      "Epoch 3415/30000 Training Loss: 0.1110846996307373\n",
      "Epoch 3416/30000 Training Loss: 0.08541993051767349\n",
      "Epoch 3417/30000 Training Loss: 0.09804437309503555\n",
      "Epoch 3418/30000 Training Loss: 0.10260503739118576\n",
      "Epoch 3419/30000 Training Loss: 0.08827722072601318\n",
      "Epoch 3420/30000 Training Loss: 0.0948319211602211\n",
      "Epoch 3420/30000 Validation Loss: 0.09259513765573502\n",
      "Epoch 3421/30000 Training Loss: 0.09052161127328873\n",
      "Epoch 3422/30000 Training Loss: 0.11754641681909561\n",
      "Epoch 3423/30000 Training Loss: 0.08026403933763504\n",
      "Epoch 3424/30000 Training Loss: 0.09417792409658432\n",
      "Epoch 3425/30000 Training Loss: 0.08639073371887207\n",
      "Epoch 3426/30000 Training Loss: 0.08020053058862686\n",
      "Epoch 3427/30000 Training Loss: 0.09356293082237244\n",
      "Epoch 3428/30000 Training Loss: 0.08458427339792252\n",
      "Epoch 3429/30000 Training Loss: 0.0813935175538063\n",
      "Epoch 3430/30000 Training Loss: 0.10990685224533081\n",
      "Epoch 3430/30000 Validation Loss: 0.0967705026268959\n",
      "Epoch 3431/30000 Training Loss: 0.0837211012840271\n",
      "Epoch 3432/30000 Training Loss: 0.10248583555221558\n",
      "Epoch 3433/30000 Training Loss: 0.0922345295548439\n",
      "Epoch 3434/30000 Training Loss: 0.0894421935081482\n",
      "Epoch 3435/30000 Training Loss: 0.10061266273260117\n",
      "Epoch 3436/30000 Training Loss: 0.1130380630493164\n",
      "Epoch 3437/30000 Training Loss: 0.07570506632328033\n",
      "Epoch 3438/30000 Training Loss: 0.10973941534757614\n",
      "Epoch 3439/30000 Training Loss: 0.10105069726705551\n",
      "Epoch 3440/30000 Training Loss: 0.11201494932174683\n",
      "Epoch 3440/30000 Validation Loss: 0.10768961161375046\n",
      "Epoch 3441/30000 Training Loss: 0.10766935348510742\n",
      "Epoch 3442/30000 Training Loss: 0.08172299712896347\n",
      "Epoch 3443/30000 Training Loss: 0.09188899397850037\n",
      "Epoch 3444/30000 Training Loss: 0.0999016985297203\n",
      "Epoch 3445/30000 Training Loss: 0.11081007122993469\n",
      "Epoch 3446/30000 Training Loss: 0.11375945806503296\n",
      "Epoch 3447/30000 Training Loss: 0.13732515275478363\n",
      "Epoch 3448/30000 Training Loss: 0.08555647730827332\n",
      "Epoch 3449/30000 Training Loss: 0.12115762382745743\n",
      "Epoch 3450/30000 Training Loss: 0.10617252439260483\n",
      "Epoch 3450/30000 Validation Loss: 0.10106831789016724\n",
      "Epoch 3451/30000 Training Loss: 0.09915132075548172\n",
      "Epoch 3452/30000 Training Loss: 0.10076335072517395\n",
      "Epoch 3453/30000 Training Loss: 0.07808489352464676\n",
      "Epoch 3454/30000 Training Loss: 0.11272037029266357\n",
      "Epoch 3455/30000 Training Loss: 0.09157019108533859\n",
      "Epoch 3456/30000 Training Loss: 0.08251908421516418\n",
      "Epoch 3457/30000 Training Loss: 0.09046211838722229\n",
      "Epoch 3458/30000 Training Loss: 0.0778728500008583\n",
      "Epoch 3459/30000 Training Loss: 0.08759143203496933\n",
      "Epoch 3460/30000 Training Loss: 0.09642408043146133\n",
      "Epoch 3460/30000 Validation Loss: 0.09344752877950668\n",
      "Epoch 3461/30000 Training Loss: 0.10027724504470825\n",
      "Epoch 3462/30000 Training Loss: 0.10397884249687195\n",
      "Epoch 3463/30000 Training Loss: 0.11362192034721375\n",
      "Epoch 3464/30000 Training Loss: 0.1038660779595375\n",
      "Epoch 3465/30000 Training Loss: 0.12476042658090591\n",
      "Epoch 3466/30000 Training Loss: 0.09430006891489029\n",
      "Epoch 3467/30000 Training Loss: 0.08151726424694061\n",
      "Epoch 3468/30000 Training Loss: 0.09577123075723648\n",
      "Epoch 3469/30000 Training Loss: 0.09349467605352402\n",
      "Epoch 3470/30000 Training Loss: 0.1044972762465477\n",
      "Epoch 3470/30000 Validation Loss: 0.08154591917991638\n",
      "Epoch 3471/30000 Training Loss: 0.0755869448184967\n",
      "Epoch 3472/30000 Training Loss: 0.10808094590902328\n",
      "Epoch 3473/30000 Training Loss: 0.11020305007696152\n",
      "Epoch 3474/30000 Training Loss: 0.10049694776535034\n",
      "Epoch 3475/30000 Training Loss: 0.10263705998659134\n",
      "Epoch 3476/30000 Training Loss: 0.07974659651517868\n",
      "Epoch 3477/30000 Training Loss: 0.13219332695007324\n",
      "Epoch 3478/30000 Training Loss: 0.09373412281274796\n",
      "Epoch 3479/30000 Training Loss: 0.1118360161781311\n",
      "Epoch 3480/30000 Training Loss: 0.09181765466928482\n",
      "Epoch 3480/30000 Validation Loss: 0.08413969725370407\n",
      "Epoch 3481/30000 Training Loss: 0.12009555846452713\n",
      "Epoch 3482/30000 Training Loss: 0.09658730030059814\n",
      "Epoch 3483/30000 Training Loss: 0.1120954155921936\n",
      "Epoch 3484/30000 Training Loss: 0.0937192365527153\n",
      "Epoch 3485/30000 Training Loss: 0.09910442680120468\n",
      "Epoch 3486/30000 Training Loss: 0.08470191806554794\n",
      "Epoch 3487/30000 Training Loss: 0.11157378554344177\n",
      "Epoch 3488/30000 Training Loss: 0.09524199366569519\n",
      "Epoch 3489/30000 Training Loss: 0.0964488759636879\n",
      "Epoch 3490/30000 Training Loss: 0.09610676765441895\n",
      "Epoch 3490/30000 Validation Loss: 0.09769436717033386\n",
      "Epoch 3491/30000 Training Loss: 0.07359804213047028\n",
      "Epoch 3492/30000 Training Loss: 0.12863723933696747\n",
      "Epoch 3493/30000 Training Loss: 0.07401011139154434\n",
      "Epoch 3494/30000 Training Loss: 0.10206180065870285\n",
      "Epoch 3495/30000 Training Loss: 0.08528556674718857\n",
      "Epoch 3496/30000 Training Loss: 0.10881322622299194\n",
      "Epoch 3497/30000 Training Loss: 0.09515853971242905\n",
      "Epoch 3498/30000 Training Loss: 0.08610337972640991\n",
      "Epoch 3499/30000 Training Loss: 0.08577599376440048\n",
      "Epoch 3500/30000 Training Loss: 0.08761683106422424\n",
      "Epoch 3500/30000 Validation Loss: 0.11029805988073349\n",
      "Epoch 3501/30000 Training Loss: 0.08715959638357162\n",
      "Epoch 3502/30000 Training Loss: 0.10328222066164017\n",
      "Epoch 3503/30000 Training Loss: 0.09006457775831223\n",
      "Epoch 3504/30000 Training Loss: 0.11787308007478714\n",
      "Epoch 3505/30000 Training Loss: 0.11968809366226196\n",
      "Epoch 3506/30000 Training Loss: 0.09157893806695938\n",
      "Epoch 3507/30000 Training Loss: 0.08801373094320297\n",
      "Epoch 3508/30000 Training Loss: 0.07719382643699646\n",
      "Epoch 3509/30000 Training Loss: 0.10030388832092285\n",
      "Epoch 3510/30000 Training Loss: 0.10976594686508179\n",
      "Epoch 3510/30000 Validation Loss: 0.11830484867095947\n",
      "Epoch 3511/30000 Training Loss: 0.0986315980553627\n",
      "Epoch 3512/30000 Training Loss: 0.0930677056312561\n",
      "Epoch 3513/30000 Training Loss: 0.10792049765586853\n",
      "Epoch 3514/30000 Training Loss: 0.0718998908996582\n",
      "Epoch 3515/30000 Training Loss: 0.10628273338079453\n",
      "Epoch 3516/30000 Training Loss: 0.10407856851816177\n",
      "Epoch 3517/30000 Training Loss: 0.11222239583730698\n",
      "Epoch 3518/30000 Training Loss: 0.10927551239728928\n",
      "Epoch 3519/30000 Training Loss: 0.11631041020154953\n",
      "Epoch 3520/30000 Training Loss: 0.0907893106341362\n",
      "Epoch 3520/30000 Validation Loss: 0.0866776779294014\n",
      "Epoch 3521/30000 Training Loss: 0.11032074689865112\n",
      "Epoch 3522/30000 Training Loss: 0.09070909023284912\n",
      "Epoch 3523/30000 Training Loss: 0.08545970916748047\n",
      "Epoch 3524/30000 Training Loss: 0.0897434651851654\n",
      "Epoch 3525/30000 Training Loss: 0.10177668184041977\n",
      "Epoch 3526/30000 Training Loss: 0.10281016677618027\n",
      "Epoch 3527/30000 Training Loss: 0.08979666978120804\n",
      "Epoch 3528/30000 Training Loss: 0.08275836706161499\n",
      "Epoch 3529/30000 Training Loss: 0.09743223339319229\n",
      "Epoch 3530/30000 Training Loss: 0.09582865238189697\n",
      "Epoch 3530/30000 Validation Loss: 0.09501964598894119\n",
      "Epoch 3531/30000 Training Loss: 0.08937505632638931\n",
      "Epoch 3532/30000 Training Loss: 0.10140484571456909\n",
      "Epoch 3533/30000 Training Loss: 0.09561080485582352\n",
      "Epoch 3534/30000 Training Loss: 0.08757076412439346\n",
      "Epoch 3535/30000 Training Loss: 0.10956273227930069\n",
      "Epoch 3536/30000 Training Loss: 0.09235993027687073\n",
      "Epoch 3537/30000 Training Loss: 0.10103937238454819\n",
      "Epoch 3538/30000 Training Loss: 0.09834805130958557\n",
      "Epoch 3539/30000 Training Loss: 0.09164657443761826\n",
      "Epoch 3540/30000 Training Loss: 0.10073024034500122\n",
      "Epoch 3540/30000 Validation Loss: 0.10681551694869995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3541/30000 Training Loss: 0.09322434663772583\n",
      "Epoch 3542/30000 Training Loss: 0.10243430733680725\n",
      "Epoch 3543/30000 Training Loss: 0.09321704506874084\n",
      "Epoch 3544/30000 Training Loss: 0.09869644790887833\n",
      "Epoch 3545/30000 Training Loss: 0.10479086637496948\n",
      "Epoch 3546/30000 Training Loss: 0.08637529611587524\n",
      "Epoch 3547/30000 Training Loss: 0.0960051640868187\n",
      "Epoch 3548/30000 Training Loss: 0.08052185922861099\n",
      "Epoch 3549/30000 Training Loss: 0.08235196024179459\n",
      "Epoch 3550/30000 Training Loss: 0.13424713909626007\n",
      "Epoch 3550/30000 Validation Loss: 0.08682247251272202\n",
      "Epoch 3551/30000 Training Loss: 0.10446282476186752\n",
      "Epoch 3552/30000 Training Loss: 0.07762517780065536\n",
      "Epoch 3553/30000 Training Loss: 0.1044253334403038\n",
      "Epoch 3554/30000 Training Loss: 0.10122328251600266\n",
      "Epoch 3555/30000 Training Loss: 0.0985628068447113\n",
      "Epoch 3556/30000 Training Loss: 0.08601952344179153\n",
      "Epoch 3557/30000 Training Loss: 0.09870479255914688\n",
      "Epoch 3558/30000 Training Loss: 0.09170353412628174\n",
      "Epoch 3559/30000 Training Loss: 0.08030251413583755\n",
      "Epoch 3560/30000 Training Loss: 0.0873788371682167\n",
      "Epoch 3560/30000 Validation Loss: 0.12768279016017914\n",
      "Epoch 3561/30000 Training Loss: 0.09098135679960251\n",
      "Epoch 3562/30000 Training Loss: 0.08625907450914383\n",
      "Epoch 3563/30000 Training Loss: 0.10097551345825195\n",
      "Epoch 3564/30000 Training Loss: 0.09931925684213638\n",
      "Epoch 3565/30000 Training Loss: 0.09406814724206924\n",
      "Epoch 3566/30000 Training Loss: 0.08491560071706772\n",
      "Epoch 3567/30000 Training Loss: 0.10927758365869522\n",
      "Epoch 3568/30000 Training Loss: 0.09770438820123672\n",
      "Epoch 3569/30000 Training Loss: 0.09484895318746567\n",
      "Epoch 3570/30000 Training Loss: 0.08747819066047668\n",
      "Epoch 3570/30000 Validation Loss: 0.11413715034723282\n",
      "Epoch 3571/30000 Training Loss: 0.08460894972085953\n",
      "Epoch 3572/30000 Training Loss: 0.07792327553033829\n",
      "Epoch 3573/30000 Training Loss: 0.10062974691390991\n",
      "Epoch 3574/30000 Training Loss: 0.11131039261817932\n",
      "Epoch 3575/30000 Training Loss: 0.08507797122001648\n",
      "Epoch 3576/30000 Training Loss: 0.09274182468652725\n",
      "Epoch 3577/30000 Training Loss: 0.10498982667922974\n",
      "Epoch 3578/30000 Training Loss: 0.10699387639760971\n",
      "Epoch 3579/30000 Training Loss: 0.09712165594100952\n",
      "Epoch 3580/30000 Training Loss: 0.08994096517562866\n",
      "Epoch 3580/30000 Validation Loss: 0.10821578651666641\n",
      "Epoch 3581/30000 Training Loss: 0.0885624811053276\n",
      "Epoch 3582/30000 Training Loss: 0.09202364087104797\n",
      "Epoch 3583/30000 Training Loss: 0.12520189583301544\n",
      "Epoch 3584/30000 Training Loss: 0.0785663053393364\n",
      "Epoch 3585/30000 Training Loss: 0.11956312507390976\n",
      "Epoch 3586/30000 Training Loss: 0.08654376119375229\n",
      "Epoch 3587/30000 Training Loss: 0.10041972249746323\n",
      "Epoch 3588/30000 Training Loss: 0.08923444896936417\n",
      "Epoch 3589/30000 Training Loss: 0.0683959499001503\n",
      "Epoch 3590/30000 Training Loss: 0.08382812142372131\n",
      "Epoch 3590/30000 Validation Loss: 0.11835217475891113\n",
      "Epoch 3591/30000 Training Loss: 0.08765298128128052\n",
      "Epoch 3592/30000 Training Loss: 0.10861905664205551\n",
      "Epoch 3593/30000 Training Loss: 0.0818222239613533\n",
      "Epoch 3594/30000 Training Loss: 0.08113492280244827\n",
      "Epoch 3595/30000 Training Loss: 0.0855802595615387\n",
      "Epoch 3596/30000 Training Loss: 0.08834467083215714\n",
      "Epoch 3597/30000 Training Loss: 0.07652311772108078\n",
      "Epoch 3598/30000 Training Loss: 0.09925737231969833\n",
      "Epoch 3599/30000 Training Loss: 0.09881814569234848\n",
      "Epoch 3600/30000 Training Loss: 0.0880376324057579\n",
      "Epoch 3600/30000 Validation Loss: 0.10305885225534439\n",
      "Epoch 3601/30000 Training Loss: 0.08356747031211853\n",
      "Epoch 3602/30000 Training Loss: 0.07897409796714783\n",
      "Epoch 3603/30000 Training Loss: 0.10701724141836166\n",
      "Epoch 3604/30000 Training Loss: 0.07804437726736069\n",
      "Epoch 3605/30000 Training Loss: 0.11196234822273254\n",
      "Epoch 3606/30000 Training Loss: 0.11945643275976181\n",
      "Epoch 3607/30000 Training Loss: 0.09382464736700058\n",
      "Epoch 3608/30000 Training Loss: 0.1021370217204094\n",
      "Epoch 3609/30000 Training Loss: 0.08992484956979752\n",
      "Epoch 3610/30000 Training Loss: 0.08520432561635971\n",
      "Epoch 3610/30000 Validation Loss: 0.10857909917831421\n",
      "Epoch 3611/30000 Training Loss: 0.07478000968694687\n",
      "Epoch 3612/30000 Training Loss: 0.11709216982126236\n",
      "Epoch 3613/30000 Training Loss: 0.0956154465675354\n",
      "Epoch 3614/30000 Training Loss: 0.08596450090408325\n",
      "Epoch 3615/30000 Training Loss: 0.08240284770727158\n",
      "Epoch 3616/30000 Training Loss: 0.09566804766654968\n",
      "Epoch 3617/30000 Training Loss: 0.08520015329122543\n",
      "Epoch 3618/30000 Training Loss: 0.07418059557676315\n",
      "Epoch 3619/30000 Training Loss: 0.1004541888833046\n",
      "Epoch 3620/30000 Training Loss: 0.0795939490199089\n",
      "Epoch 3620/30000 Validation Loss: 0.1040792465209961\n",
      "Epoch 3621/30000 Training Loss: 0.10789990425109863\n",
      "Epoch 3622/30000 Training Loss: 0.09489881247282028\n",
      "Epoch 3623/30000 Training Loss: 0.10753457993268967\n",
      "Epoch 3624/30000 Training Loss: 0.089884914457798\n",
      "Epoch 3625/30000 Training Loss: 0.08604788780212402\n",
      "Epoch 3626/30000 Training Loss: 0.09419471025466919\n",
      "Epoch 3627/30000 Training Loss: 0.10396280139684677\n",
      "Epoch 3628/30000 Training Loss: 0.09401810169219971\n",
      "Epoch 3629/30000 Training Loss: 0.09314071387052536\n",
      "Epoch 3630/30000 Training Loss: 0.08504676073789597\n",
      "Epoch 3630/30000 Validation Loss: 0.10074461251497269\n",
      "Epoch 3631/30000 Training Loss: 0.07997720688581467\n",
      "Epoch 3632/30000 Training Loss: 0.09380219131708145\n",
      "Epoch 3633/30000 Training Loss: 0.08293181657791138\n",
      "Epoch 3634/30000 Training Loss: 0.08175919950008392\n",
      "Epoch 3635/30000 Training Loss: 0.0969066396355629\n",
      "Epoch 3636/30000 Training Loss: 0.10899537801742554\n",
      "Epoch 3637/30000 Training Loss: 0.07684396207332611\n",
      "Epoch 3638/30000 Training Loss: 0.0881681963801384\n",
      "Epoch 3639/30000 Training Loss: 0.0982043668627739\n",
      "Epoch 3640/30000 Training Loss: 0.08444402366876602\n",
      "Epoch 3640/30000 Validation Loss: 0.10252051800489426\n",
      "Epoch 3641/30000 Training Loss: 0.09081602096557617\n",
      "Epoch 3642/30000 Training Loss: 0.09402864426374435\n",
      "Epoch 3643/30000 Training Loss: 0.07658953219652176\n",
      "Epoch 3644/30000 Training Loss: 0.09927558898925781\n",
      "Epoch 3645/30000 Training Loss: 0.10009416192770004\n",
      "Epoch 3646/30000 Training Loss: 0.09771740436553955\n",
      "Epoch 3647/30000 Training Loss: 0.09390728920698166\n",
      "Epoch 3648/30000 Training Loss: 0.1140848770737648\n",
      "Epoch 3649/30000 Training Loss: 0.10534220933914185\n",
      "Epoch 3650/30000 Training Loss: 0.09637817740440369\n",
      "Epoch 3650/30000 Validation Loss: 0.09780248999595642\n",
      "Epoch 3651/30000 Training Loss: 0.11118712276220322\n",
      "Epoch 3652/30000 Training Loss: 0.08956209570169449\n",
      "Epoch 3653/30000 Training Loss: 0.09830274432897568\n",
      "Epoch 3654/30000 Training Loss: 0.10305726528167725\n",
      "Epoch 3655/30000 Training Loss: 0.10294485092163086\n",
      "Epoch 3656/30000 Training Loss: 0.12882788479328156\n",
      "Epoch 3657/30000 Training Loss: 0.09281256049871445\n",
      "Epoch 3658/30000 Training Loss: 0.09233784675598145\n",
      "Epoch 3659/30000 Training Loss: 0.09243748337030411\n",
      "Epoch 3660/30000 Training Loss: 0.07707051187753677\n",
      "Epoch 3660/30000 Validation Loss: 0.08594536781311035\n",
      "Epoch 3661/30000 Training Loss: 0.09233134984970093\n",
      "Epoch 3662/30000 Training Loss: 0.07020428776741028\n",
      "Epoch 3663/30000 Training Loss: 0.10696139186620712\n",
      "Epoch 3664/30000 Training Loss: 0.0771980881690979\n",
      "Epoch 3665/30000 Training Loss: 0.076804518699646\n",
      "Epoch 3666/30000 Training Loss: 0.0831858292222023\n",
      "Epoch 3667/30000 Training Loss: 0.10193363577127457\n",
      "Epoch 3668/30000 Training Loss: 0.10292986780405045\n",
      "Epoch 3669/30000 Training Loss: 0.08949887752532959\n",
      "Epoch 3670/30000 Training Loss: 0.11933565884828568\n",
      "Epoch 3670/30000 Validation Loss: 0.09333562850952148\n",
      "Epoch 3671/30000 Training Loss: 0.08069200813770294\n",
      "Epoch 3672/30000 Training Loss: 0.09803328663110733\n",
      "Epoch 3673/30000 Training Loss: 0.08175094425678253\n",
      "Epoch 3674/30000 Training Loss: 0.09677120298147202\n",
      "Epoch 3675/30000 Training Loss: 0.09308240562677383\n",
      "Epoch 3676/30000 Training Loss: 0.10068116337060928\n",
      "Epoch 3677/30000 Training Loss: 0.10215505957603455\n",
      "Epoch 3678/30000 Training Loss: 0.07932235300540924\n",
      "Epoch 3679/30000 Training Loss: 0.10355063527822495\n",
      "Epoch 3680/30000 Training Loss: 0.10852048546075821\n",
      "Epoch 3680/30000 Validation Loss: 0.08592908829450607\n",
      "Epoch 3681/30000 Training Loss: 0.10452451556921005\n",
      "Epoch 3682/30000 Training Loss: 0.08783381432294846\n",
      "Epoch 3683/30000 Training Loss: 0.08978394418954849\n",
      "Epoch 3684/30000 Training Loss: 0.09224632382392883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3685/30000 Training Loss: 0.10168979316949844\n",
      "Epoch 3686/30000 Training Loss: 0.08788952231407166\n",
      "Epoch 3687/30000 Training Loss: 0.08234775811433792\n",
      "Epoch 3688/30000 Training Loss: 0.08494830131530762\n",
      "Epoch 3689/30000 Training Loss: 0.08588943630456924\n",
      "Epoch 3690/30000 Training Loss: 0.08641158789396286\n",
      "Epoch 3690/30000 Validation Loss: 0.07105684280395508\n",
      "Epoch 3691/30000 Training Loss: 0.09313758462667465\n",
      "Epoch 3692/30000 Training Loss: 0.08543658256530762\n",
      "Epoch 3693/30000 Training Loss: 0.10786650329828262\n",
      "Epoch 3694/30000 Training Loss: 0.08901167660951614\n",
      "Epoch 3695/30000 Training Loss: 0.09921105951070786\n",
      "Epoch 3696/30000 Training Loss: 0.09661667793989182\n",
      "Epoch 3697/30000 Training Loss: 0.09792765229940414\n",
      "Epoch 3698/30000 Training Loss: 0.10232270509004593\n",
      "Epoch 3699/30000 Training Loss: 0.09158473461866379\n",
      "Epoch 3700/30000 Training Loss: 0.11363450437784195\n",
      "Epoch 3700/30000 Validation Loss: 0.0724836066365242\n",
      "Epoch 3701/30000 Training Loss: 0.06936679035425186\n",
      "Epoch 3702/30000 Training Loss: 0.09902744740247726\n",
      "Epoch 3703/30000 Training Loss: 0.0817641019821167\n",
      "Epoch 3704/30000 Training Loss: 0.08594673126935959\n",
      "Epoch 3705/30000 Training Loss: 0.08823027461767197\n",
      "Epoch 3706/30000 Training Loss: 0.08323729783296585\n",
      "Epoch 3707/30000 Training Loss: 0.09182842820882797\n",
      "Epoch 3708/30000 Training Loss: 0.09550998359918594\n",
      "Epoch 3709/30000 Training Loss: 0.10155121237039566\n",
      "Epoch 3710/30000 Training Loss: 0.09033486992120743\n",
      "Epoch 3710/30000 Validation Loss: 0.08371006697416306\n",
      "Epoch 3711/30000 Training Loss: 0.09599202126264572\n",
      "Epoch 3712/30000 Training Loss: 0.07406985014677048\n",
      "Epoch 3713/30000 Training Loss: 0.10167708247900009\n",
      "Epoch 3714/30000 Training Loss: 0.1129741445183754\n",
      "Epoch 3715/30000 Training Loss: 0.11211001873016357\n",
      "Epoch 3716/30000 Training Loss: 0.09303545951843262\n",
      "Epoch 3717/30000 Training Loss: 0.06545184552669525\n",
      "Epoch 3718/30000 Training Loss: 0.10242249816656113\n",
      "Epoch 3719/30000 Training Loss: 0.07897567003965378\n",
      "Epoch 3720/30000 Training Loss: 0.09474309533834457\n",
      "Epoch 3720/30000 Validation Loss: 0.08868464082479477\n",
      "Epoch 3721/30000 Training Loss: 0.10167285799980164\n",
      "Epoch 3722/30000 Training Loss: 0.08530034869909286\n",
      "Epoch 3723/30000 Training Loss: 0.08689311891794205\n",
      "Epoch 3724/30000 Training Loss: 0.10833141952753067\n",
      "Epoch 3725/30000 Training Loss: 0.09884775429964066\n",
      "Epoch 3726/30000 Training Loss: 0.08405230194330215\n",
      "Epoch 3727/30000 Training Loss: 0.08601736277341843\n",
      "Epoch 3728/30000 Training Loss: 0.08651585131883621\n",
      "Epoch 3729/30000 Training Loss: 0.08815347403287888\n",
      "Epoch 3730/30000 Training Loss: 0.08943462371826172\n",
      "Epoch 3730/30000 Validation Loss: 0.10056471824645996\n",
      "Epoch 3731/30000 Training Loss: 0.08778148889541626\n",
      "Epoch 3732/30000 Training Loss: 0.08982967585325241\n",
      "Epoch 3733/30000 Training Loss: 0.10820307582616806\n",
      "Epoch 3734/30000 Training Loss: 0.1099473237991333\n",
      "Epoch 3735/30000 Training Loss: 0.09414003044366837\n",
      "Epoch 3736/30000 Training Loss: 0.11847933381795883\n",
      "Epoch 3737/30000 Training Loss: 0.07055816054344177\n",
      "Epoch 3738/30000 Training Loss: 0.08857181668281555\n",
      "Epoch 3739/30000 Training Loss: 0.08432071655988693\n",
      "Epoch 3740/30000 Training Loss: 0.10954660922288895\n",
      "Epoch 3740/30000 Validation Loss: 0.1015767827630043\n",
      "Epoch 3741/30000 Training Loss: 0.11434435844421387\n",
      "Epoch 3742/30000 Training Loss: 0.08583590388298035\n",
      "Epoch 3743/30000 Training Loss: 0.09852761030197144\n",
      "Epoch 3744/30000 Training Loss: 0.10331157594919205\n",
      "Epoch 3745/30000 Training Loss: 0.08689310401678085\n",
      "Epoch 3746/30000 Training Loss: 0.08927122503519058\n",
      "Epoch 3747/30000 Training Loss: 0.0852920413017273\n",
      "Epoch 3748/30000 Training Loss: 0.09510374814271927\n",
      "Epoch 3749/30000 Training Loss: 0.11426738649606705\n",
      "Epoch 3750/30000 Training Loss: 0.07900843769311905\n",
      "Epoch 3750/30000 Validation Loss: 0.08244404196739197\n",
      "Epoch 3751/30000 Training Loss: 0.09679511934518814\n",
      "Epoch 3752/30000 Training Loss: 0.08291211724281311\n",
      "Epoch 3753/30000 Training Loss: 0.08474752306938171\n",
      "Epoch 3754/30000 Training Loss: 0.10622336715459824\n",
      "Epoch 3755/30000 Training Loss: 0.09195706993341446\n",
      "Epoch 3756/30000 Training Loss: 0.09530691057443619\n",
      "Epoch 3757/30000 Training Loss: 0.07949311286211014\n",
      "Epoch 3758/30000 Training Loss: 0.07598350197076797\n",
      "Epoch 3759/30000 Training Loss: 0.08168214559555054\n",
      "Epoch 3760/30000 Training Loss: 0.09598219394683838\n",
      "Epoch 3760/30000 Validation Loss: 0.07933635264635086\n",
      "Epoch 3761/30000 Training Loss: 0.07851213961839676\n",
      "Epoch 3762/30000 Training Loss: 0.08504638820886612\n",
      "Epoch 3763/30000 Training Loss: 0.08633429557085037\n",
      "Epoch 3764/30000 Training Loss: 0.07778700441122055\n",
      "Epoch 3765/30000 Training Loss: 0.10188447684049606\n",
      "Epoch 3766/30000 Training Loss: 0.08927806466817856\n",
      "Epoch 3767/30000 Training Loss: 0.10514688491821289\n",
      "Epoch 3768/30000 Training Loss: 0.08353788405656815\n",
      "Epoch 3769/30000 Training Loss: 0.10151922702789307\n",
      "Epoch 3770/30000 Training Loss: 0.09994915127754211\n",
      "Epoch 3770/30000 Validation Loss: 0.09985870122909546\n",
      "Epoch 3771/30000 Training Loss: 0.11170301586389542\n",
      "Epoch 3772/30000 Training Loss: 0.07734192162752151\n",
      "Epoch 3773/30000 Training Loss: 0.11101289838552475\n",
      "Epoch 3774/30000 Training Loss: 0.1121087595820427\n",
      "Epoch 3775/30000 Training Loss: 0.0729123130440712\n",
      "Epoch 3776/30000 Training Loss: 0.11024626344442368\n",
      "Epoch 3777/30000 Training Loss: 0.08604047447443008\n",
      "Epoch 3778/30000 Training Loss: 0.07558219134807587\n",
      "Epoch 3779/30000 Training Loss: 0.10532071441411972\n",
      "Epoch 3780/30000 Training Loss: 0.08432916551828384\n",
      "Epoch 3780/30000 Validation Loss: 0.11299055814743042\n",
      "Epoch 3781/30000 Training Loss: 0.11013251543045044\n",
      "Epoch 3782/30000 Training Loss: 0.09306340664625168\n",
      "Epoch 3783/30000 Training Loss: 0.08728737384080887\n",
      "Epoch 3784/30000 Training Loss: 0.10289245843887329\n",
      "Epoch 3785/30000 Training Loss: 0.09318149089813232\n",
      "Epoch 3786/30000 Training Loss: 0.11939249187707901\n",
      "Epoch 3787/30000 Training Loss: 0.09873483330011368\n",
      "Epoch 3788/30000 Training Loss: 0.08569648116827011\n",
      "Epoch 3789/30000 Training Loss: 0.09609824419021606\n",
      "Epoch 3790/30000 Training Loss: 0.08010595291852951\n",
      "Epoch 3790/30000 Validation Loss: 0.08472904562950134\n",
      "Epoch 3791/30000 Training Loss: 0.08000563830137253\n",
      "Epoch 3792/30000 Training Loss: 0.09608378261327744\n",
      "Epoch 3793/30000 Training Loss: 0.08439893275499344\n",
      "Epoch 3794/30000 Training Loss: 0.09286332875490189\n",
      "Epoch 3795/30000 Training Loss: 0.09309612959623337\n",
      "Epoch 3796/30000 Training Loss: 0.08679720014333725\n",
      "Epoch 3797/30000 Training Loss: 0.1074780747294426\n",
      "Epoch 3798/30000 Training Loss: 0.09067263454198837\n",
      "Epoch 3799/30000 Training Loss: 0.09366867691278458\n",
      "Epoch 3800/30000 Training Loss: 0.09605682641267776\n",
      "Epoch 3800/30000 Validation Loss: 0.09407972544431686\n",
      "Epoch 3801/30000 Training Loss: 0.08363797515630722\n",
      "Epoch 3802/30000 Training Loss: 0.09846674650907516\n",
      "Epoch 3803/30000 Training Loss: 0.09874827414751053\n",
      "Epoch 3804/30000 Training Loss: 0.07919790595769882\n",
      "Epoch 3805/30000 Training Loss: 0.08780031651258469\n",
      "Epoch 3806/30000 Training Loss: 0.10330571979284286\n",
      "Epoch 3807/30000 Training Loss: 0.10548633337020874\n",
      "Epoch 3808/30000 Training Loss: 0.08685710281133652\n",
      "Epoch 3809/30000 Training Loss: 0.09829455614089966\n",
      "Epoch 3810/30000 Training Loss: 0.0727904886007309\n",
      "Epoch 3810/30000 Validation Loss: 0.10719287395477295\n",
      "Epoch 3811/30000 Training Loss: 0.0719791129231453\n",
      "Epoch 3812/30000 Training Loss: 0.09866377711296082\n",
      "Epoch 3813/30000 Training Loss: 0.10390415787696838\n",
      "Epoch 3814/30000 Training Loss: 0.10002913326025009\n",
      "Epoch 3815/30000 Training Loss: 0.09679698199033737\n",
      "Epoch 3816/30000 Training Loss: 0.0767110213637352\n",
      "Epoch 3817/30000 Training Loss: 0.08384793251752853\n",
      "Epoch 3818/30000 Training Loss: 0.0845407173037529\n",
      "Epoch 3819/30000 Training Loss: 0.08994903415441513\n",
      "Epoch 3820/30000 Training Loss: 0.09386128187179565\n",
      "Epoch 3820/30000 Validation Loss: 0.08236733824014664\n",
      "Epoch 3821/30000 Training Loss: 0.10523447394371033\n",
      "Epoch 3822/30000 Training Loss: 0.07084674388170242\n",
      "Epoch 3823/30000 Training Loss: 0.08620402961969376\n",
      "Epoch 3824/30000 Training Loss: 0.06832053512334824\n",
      "Epoch 3825/30000 Training Loss: 0.08033943921327591\n",
      "Epoch 3826/30000 Training Loss: 0.10195675492286682\n",
      "Epoch 3827/30000 Training Loss: 0.09015928953886032\n",
      "Epoch 3828/30000 Training Loss: 0.08138754218816757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3829/30000 Training Loss: 0.08947042375802994\n",
      "Epoch 3830/30000 Training Loss: 0.09629952907562256\n",
      "Epoch 3830/30000 Validation Loss: 0.09166869521141052\n",
      "Epoch 3831/30000 Training Loss: 0.07612870633602142\n",
      "Epoch 3832/30000 Training Loss: 0.10796063393354416\n",
      "Epoch 3833/30000 Training Loss: 0.08902496099472046\n",
      "Epoch 3834/30000 Training Loss: 0.09908944368362427\n",
      "Epoch 3835/30000 Training Loss: 0.0789484903216362\n",
      "Epoch 3836/30000 Training Loss: 0.09691795706748962\n",
      "Epoch 3837/30000 Training Loss: 0.07290806621313095\n",
      "Epoch 3838/30000 Training Loss: 0.11381486803293228\n",
      "Epoch 3839/30000 Training Loss: 0.08058566600084305\n",
      "Epoch 3840/30000 Training Loss: 0.08941926807165146\n",
      "Epoch 3840/30000 Validation Loss: 0.1250821053981781\n",
      "Epoch 3841/30000 Training Loss: 0.08675429970026016\n",
      "Epoch 3842/30000 Training Loss: 0.1118248701095581\n",
      "Epoch 3843/30000 Training Loss: 0.08767325431108475\n",
      "Epoch 3844/30000 Training Loss: 0.09435930103063583\n",
      "Epoch 3845/30000 Training Loss: 0.08594720810651779\n",
      "Epoch 3846/30000 Training Loss: 0.08291708678007126\n",
      "Epoch 3847/30000 Training Loss: 0.10100594162940979\n",
      "Epoch 3848/30000 Training Loss: 0.10346680879592896\n",
      "Epoch 3849/30000 Training Loss: 0.09836512058973312\n",
      "Epoch 3850/30000 Training Loss: 0.1014103889465332\n",
      "Epoch 3850/30000 Validation Loss: 0.08652373403310776\n",
      "Epoch 3851/30000 Training Loss: 0.08821994066238403\n",
      "Epoch 3852/30000 Training Loss: 0.07762957364320755\n",
      "Epoch 3853/30000 Training Loss: 0.08375686407089233\n",
      "Epoch 3854/30000 Training Loss: 0.081917405128479\n",
      "Epoch 3855/30000 Training Loss: 0.10401345044374466\n",
      "Epoch 3856/30000 Training Loss: 0.0897195041179657\n",
      "Epoch 3857/30000 Training Loss: 0.08472994714975357\n",
      "Epoch 3858/30000 Training Loss: 0.07933295518159866\n",
      "Epoch 3859/30000 Training Loss: 0.10528268665075302\n",
      "Epoch 3860/30000 Training Loss: 0.10440755635499954\n",
      "Epoch 3860/30000 Validation Loss: 0.09077334403991699\n",
      "Epoch 3861/30000 Training Loss: 0.08775150030851364\n",
      "Epoch 3862/30000 Training Loss: 0.0755966529250145\n",
      "Epoch 3863/30000 Training Loss: 0.08976321667432785\n",
      "Epoch 3864/30000 Training Loss: 0.07428691536188126\n",
      "Epoch 3865/30000 Training Loss: 0.09133367985486984\n",
      "Epoch 3866/30000 Training Loss: 0.08514809608459473\n",
      "Epoch 3867/30000 Training Loss: 0.09363759309053421\n",
      "Epoch 3868/30000 Training Loss: 0.07882917672395706\n",
      "Epoch 3869/30000 Training Loss: 0.09497345238924026\n",
      "Epoch 3870/30000 Training Loss: 0.09586802870035172\n",
      "Epoch 3870/30000 Validation Loss: 0.09485257416963577\n",
      "Epoch 3871/30000 Training Loss: 0.08922174572944641\n",
      "Epoch 3872/30000 Training Loss: 0.07859911769628525\n",
      "Epoch 3873/30000 Training Loss: 0.10566157102584839\n",
      "Epoch 3874/30000 Training Loss: 0.10714457184076309\n",
      "Epoch 3875/30000 Training Loss: 0.10778976231813431\n",
      "Epoch 3876/30000 Training Loss: 0.09258681535720825\n",
      "Epoch 3877/30000 Training Loss: 0.1060231551527977\n",
      "Epoch 3878/30000 Training Loss: 0.07399024814367294\n",
      "Epoch 3879/30000 Training Loss: 0.08039233833551407\n",
      "Epoch 3880/30000 Training Loss: 0.10591215640306473\n",
      "Epoch 3880/30000 Validation Loss: 0.08462957292795181\n",
      "Epoch 3881/30000 Training Loss: 0.09010788053274155\n",
      "Epoch 3882/30000 Training Loss: 0.11003240942955017\n",
      "Epoch 3883/30000 Training Loss: 0.07467246800661087\n",
      "Epoch 3884/30000 Training Loss: 0.07413078099489212\n",
      "Epoch 3885/30000 Training Loss: 0.10190218687057495\n",
      "Epoch 3886/30000 Training Loss: 0.09307464212179184\n",
      "Epoch 3887/30000 Training Loss: 0.07808045297861099\n",
      "Epoch 3888/30000 Training Loss: 0.08903936296701431\n",
      "Epoch 3889/30000 Training Loss: 0.07804117351770401\n",
      "Epoch 3890/30000 Training Loss: 0.09548590332269669\n",
      "Epoch 3890/30000 Validation Loss: 0.10332312434911728\n",
      "Epoch 3891/30000 Training Loss: 0.07693624496459961\n",
      "Epoch 3892/30000 Training Loss: 0.09417658299207687\n",
      "Epoch 3893/30000 Training Loss: 0.09429822117090225\n",
      "Epoch 3894/30000 Training Loss: 0.0933438166975975\n",
      "Epoch 3895/30000 Training Loss: 0.08106032013893127\n",
      "Epoch 3896/30000 Training Loss: 0.0903342068195343\n",
      "Epoch 3897/30000 Training Loss: 0.0831792876124382\n",
      "Epoch 3898/30000 Training Loss: 0.10550081729888916\n",
      "Epoch 3899/30000 Training Loss: 0.06753425300121307\n",
      "Epoch 3900/30000 Training Loss: 0.0803958848118782\n",
      "Epoch 3900/30000 Validation Loss: 0.09404126554727554\n",
      "Epoch 3901/30000 Training Loss: 0.09206631779670715\n",
      "Epoch 3902/30000 Training Loss: 0.08298331499099731\n",
      "Epoch 3903/30000 Training Loss: 0.08890140056610107\n",
      "Epoch 3904/30000 Training Loss: 0.08921436220407486\n",
      "Epoch 3905/30000 Training Loss: 0.07955021411180496\n",
      "Epoch 3906/30000 Training Loss: 0.09702977538108826\n",
      "Epoch 3907/30000 Training Loss: 0.08448203653097153\n",
      "Epoch 3908/30000 Training Loss: 0.0875835195183754\n",
      "Epoch 3909/30000 Training Loss: 0.08708570152521133\n",
      "Epoch 3910/30000 Training Loss: 0.09773451089859009\n",
      "Epoch 3910/30000 Validation Loss: 0.07758226245641708\n",
      "Epoch 3911/30000 Training Loss: 0.08632495999336243\n",
      "Epoch 3912/30000 Training Loss: 0.10423215478658676\n",
      "Epoch 3913/30000 Training Loss: 0.09241939336061478\n",
      "Epoch 3914/30000 Training Loss: 0.08400478214025497\n",
      "Epoch 3915/30000 Training Loss: 0.08860311657190323\n",
      "Epoch 3916/30000 Training Loss: 0.09952443838119507\n",
      "Epoch 3917/30000 Training Loss: 0.08180151134729385\n",
      "Epoch 3918/30000 Training Loss: 0.1120600476861\n",
      "Epoch 3919/30000 Training Loss: 0.0781853124499321\n",
      "Epoch 3920/30000 Training Loss: 0.08946046233177185\n",
      "Epoch 3920/30000 Validation Loss: 0.09704159945249557\n",
      "Epoch 3921/30000 Training Loss: 0.0878690779209137\n",
      "Epoch 3922/30000 Training Loss: 0.08033853024244308\n",
      "Epoch 3923/30000 Training Loss: 0.10926564782857895\n",
      "Epoch 3924/30000 Training Loss: 0.0847264751791954\n",
      "Epoch 3925/30000 Training Loss: 0.10834900289773941\n",
      "Epoch 3926/30000 Training Loss: 0.09150005131959915\n",
      "Epoch 3927/30000 Training Loss: 0.09818583726882935\n",
      "Epoch 3928/30000 Training Loss: 0.10153432935476303\n",
      "Epoch 3929/30000 Training Loss: 0.10546177625656128\n",
      "Epoch 3930/30000 Training Loss: 0.0742497444152832\n",
      "Epoch 3930/30000 Validation Loss: 0.08877462148666382\n",
      "Epoch 3931/30000 Training Loss: 0.09484664350748062\n",
      "Epoch 3932/30000 Training Loss: 0.12188252061605453\n",
      "Epoch 3933/30000 Training Loss: 0.09468892216682434\n",
      "Epoch 3934/30000 Training Loss: 0.10232002288103104\n",
      "Epoch 3935/30000 Training Loss: 0.08360876888036728\n",
      "Epoch 3936/30000 Training Loss: 0.09727992862462997\n",
      "Epoch 3937/30000 Training Loss: 0.0934433862566948\n",
      "Epoch 3938/30000 Training Loss: 0.08197551220655441\n",
      "Epoch 3939/30000 Training Loss: 0.09287219494581223\n",
      "Epoch 3940/30000 Training Loss: 0.08816498517990112\n",
      "Epoch 3940/30000 Validation Loss: 0.08157023787498474\n",
      "Epoch 3941/30000 Training Loss: 0.0785222128033638\n",
      "Epoch 3942/30000 Training Loss: 0.08931434154510498\n",
      "Epoch 3943/30000 Training Loss: 0.0968254879117012\n",
      "Epoch 3944/30000 Training Loss: 0.11847192049026489\n",
      "Epoch 3945/30000 Training Loss: 0.08136843889951706\n",
      "Epoch 3946/30000 Training Loss: 0.098609060049057\n",
      "Epoch 3947/30000 Training Loss: 0.09075383096933365\n",
      "Epoch 3948/30000 Training Loss: 0.1114354208111763\n",
      "Epoch 3949/30000 Training Loss: 0.07821620255708694\n",
      "Epoch 3950/30000 Training Loss: 0.115955650806427\n",
      "Epoch 3950/30000 Validation Loss: 0.08650001883506775\n",
      "Epoch 3951/30000 Training Loss: 0.09190189838409424\n",
      "Epoch 3952/30000 Training Loss: 0.07221492379903793\n",
      "Epoch 3953/30000 Training Loss: 0.09732065349817276\n",
      "Epoch 3954/30000 Training Loss: 0.07445744425058365\n",
      "Epoch 3955/30000 Training Loss: 0.10999133437871933\n",
      "Epoch 3956/30000 Training Loss: 0.12375807762145996\n",
      "Epoch 3957/30000 Training Loss: 0.09817630797624588\n",
      "Epoch 3958/30000 Training Loss: 0.08491164445877075\n",
      "Epoch 3959/30000 Training Loss: 0.07766749709844589\n",
      "Epoch 3960/30000 Training Loss: 0.09513115137815475\n",
      "Epoch 3960/30000 Validation Loss: 0.10188239812850952\n",
      "Epoch 3961/30000 Training Loss: 0.07672830671072006\n",
      "Epoch 3962/30000 Training Loss: 0.11879395693540573\n",
      "Epoch 3963/30000 Training Loss: 0.08936502784490585\n",
      "Epoch 3964/30000 Training Loss: 0.08026016503572464\n",
      "Epoch 3965/30000 Training Loss: 0.08418681472539902\n",
      "Epoch 3966/30000 Training Loss: 0.09245898574590683\n",
      "Epoch 3967/30000 Training Loss: 0.08273399621248245\n",
      "Epoch 3968/30000 Training Loss: 0.09721735864877701\n",
      "Epoch 3969/30000 Training Loss: 0.09998641163110733\n",
      "Epoch 3970/30000 Training Loss: 0.0998578742146492\n",
      "Epoch 3970/30000 Validation Loss: 0.09034693241119385\n",
      "Epoch 3971/30000 Training Loss: 0.09071015566587448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3972/30000 Training Loss: 0.09498772770166397\n",
      "Epoch 3973/30000 Training Loss: 0.09770014882087708\n",
      "Epoch 3974/30000 Training Loss: 0.09584912657737732\n",
      "Epoch 3975/30000 Training Loss: 0.0893845334649086\n",
      "Epoch 3976/30000 Training Loss: 0.10499298572540283\n",
      "Epoch 3977/30000 Training Loss: 0.10325902700424194\n",
      "Epoch 3978/30000 Training Loss: 0.08223407715559006\n",
      "Epoch 3979/30000 Training Loss: 0.08904876559972763\n",
      "Epoch 3980/30000 Training Loss: 0.0900789424777031\n",
      "Epoch 3980/30000 Validation Loss: 0.08709311485290527\n",
      "Epoch 3981/30000 Training Loss: 0.07793551683425903\n",
      "Epoch 3982/30000 Training Loss: 0.08731460571289062\n",
      "Epoch 3983/30000 Training Loss: 0.08781414479017258\n",
      "Epoch 3984/30000 Training Loss: 0.09789425879716873\n",
      "Epoch 3985/30000 Training Loss: 0.09345187991857529\n",
      "Epoch 3986/30000 Training Loss: 0.1044926717877388\n",
      "Epoch 3987/30000 Training Loss: 0.09123561531305313\n",
      "Epoch 3988/30000 Training Loss: 0.10777858644723892\n",
      "Epoch 3989/30000 Training Loss: 0.11038127541542053\n",
      "Epoch 3990/30000 Training Loss: 0.10175644606351852\n",
      "Epoch 3990/30000 Validation Loss: 0.09139814972877502\n",
      "Epoch 3991/30000 Training Loss: 0.07441040873527527\n",
      "Epoch 3992/30000 Training Loss: 0.0985846221446991\n",
      "Epoch 3993/30000 Training Loss: 0.09705829620361328\n",
      "Epoch 3994/30000 Training Loss: 0.09771651774644852\n",
      "Epoch 3995/30000 Training Loss: 0.08584039658308029\n",
      "Epoch 3996/30000 Training Loss: 0.08825274556875229\n",
      "Epoch 3997/30000 Training Loss: 0.11739607900381088\n",
      "Epoch 3998/30000 Training Loss: 0.07747530192136765\n",
      "Epoch 3999/30000 Training Loss: 0.08083721250295639\n",
      "Epoch 4000/30000 Training Loss: 0.09592577815055847\n",
      "Epoch 4000/30000 Validation Loss: 0.07698918879032135\n",
      "Epoch 4001/30000 Training Loss: 0.09025523066520691\n",
      "Epoch 4002/30000 Training Loss: 0.09560991078615189\n",
      "Epoch 4003/30000 Training Loss: 0.07567265629768372\n",
      "Epoch 4004/30000 Training Loss: 0.09540209174156189\n",
      "Epoch 4005/30000 Training Loss: 0.08562714606523514\n",
      "Epoch 4006/30000 Training Loss: 0.11287358403205872\n",
      "Epoch 4007/30000 Training Loss: 0.07463397830724716\n",
      "Epoch 4008/30000 Training Loss: 0.08481723070144653\n",
      "Epoch 4009/30000 Training Loss: 0.0804784819483757\n",
      "Epoch 4010/30000 Training Loss: 0.10767088085412979\n",
      "Epoch 4010/30000 Validation Loss: 0.08701344579458237\n",
      "Epoch 4011/30000 Training Loss: 0.08993437886238098\n",
      "Epoch 4012/30000 Training Loss: 0.10012456774711609\n",
      "Epoch 4013/30000 Training Loss: 0.09165555238723755\n",
      "Epoch 4014/30000 Training Loss: 0.07268517464399338\n",
      "Epoch 4015/30000 Training Loss: 0.11024297028779984\n",
      "Epoch 4016/30000 Training Loss: 0.09780680388212204\n",
      "Epoch 4017/30000 Training Loss: 0.10461948066949844\n",
      "Epoch 4018/30000 Training Loss: 0.10326076298952103\n",
      "Epoch 4019/30000 Training Loss: 0.08060973882675171\n",
      "Epoch 4020/30000 Training Loss: 0.08973640203475952\n",
      "Epoch 4020/30000 Validation Loss: 0.09416395425796509\n",
      "Epoch 4021/30000 Training Loss: 0.08103274554014206\n",
      "Epoch 4022/30000 Training Loss: 0.10035745054483414\n",
      "Epoch 4023/30000 Training Loss: 0.08096521347761154\n",
      "Epoch 4024/30000 Training Loss: 0.1109025701880455\n",
      "Epoch 4025/30000 Training Loss: 0.0982736349105835\n",
      "Epoch 4026/30000 Training Loss: 0.08971016854047775\n",
      "Epoch 4027/30000 Training Loss: 0.07966061681509018\n",
      "Epoch 4028/30000 Training Loss: 0.09110277891159058\n",
      "Epoch 4029/30000 Training Loss: 0.07470766454935074\n",
      "Epoch 4030/30000 Training Loss: 0.10111694782972336\n",
      "Epoch 4030/30000 Validation Loss: 0.10063519328832626\n",
      "Epoch 4031/30000 Training Loss: 0.0928908959031105\n",
      "Epoch 4032/30000 Training Loss: 0.09282926470041275\n",
      "Epoch 4033/30000 Training Loss: 0.10018115490674973\n",
      "Epoch 4034/30000 Training Loss: 0.1018974557518959\n",
      "Epoch 4035/30000 Training Loss: 0.07631897181272507\n",
      "Epoch 4036/30000 Training Loss: 0.09431900829076767\n",
      "Epoch 4037/30000 Training Loss: 0.11700469255447388\n",
      "Epoch 4038/30000 Training Loss: 0.08226276189088821\n",
      "Epoch 4039/30000 Training Loss: 0.07222052663564682\n",
      "Epoch 4040/30000 Training Loss: 0.08863959461450577\n",
      "Epoch 4040/30000 Validation Loss: 0.09924433380365372\n",
      "Epoch 4041/30000 Training Loss: 0.09019458293914795\n",
      "Epoch 4042/30000 Training Loss: 0.10069867968559265\n",
      "Epoch 4043/30000 Training Loss: 0.0938008651137352\n",
      "Epoch 4044/30000 Training Loss: 0.10849536210298538\n",
      "Epoch 4045/30000 Training Loss: 0.10013259202241898\n",
      "Epoch 4046/30000 Training Loss: 0.07890399545431137\n",
      "Epoch 4047/30000 Training Loss: 0.08008117228746414\n",
      "Epoch 4048/30000 Training Loss: 0.09372369199991226\n",
      "Epoch 4049/30000 Training Loss: 0.08229167014360428\n",
      "Epoch 4050/30000 Training Loss: 0.08092087507247925\n",
      "Epoch 4050/30000 Validation Loss: 0.1030639111995697\n",
      "Epoch 4051/30000 Training Loss: 0.09938115626573563\n",
      "Epoch 4052/30000 Training Loss: 0.0946824923157692\n",
      "Epoch 4053/30000 Training Loss: 0.08053439110517502\n",
      "Epoch 4054/30000 Training Loss: 0.06950370222330093\n",
      "Epoch 4055/30000 Training Loss: 0.07940582185983658\n",
      "Epoch 4056/30000 Training Loss: 0.09944099932909012\n",
      "Epoch 4057/30000 Training Loss: 0.0869862362742424\n",
      "Epoch 4058/30000 Training Loss: 0.08312832564115524\n",
      "Epoch 4059/30000 Training Loss: 0.1248684898018837\n",
      "Epoch 4060/30000 Training Loss: 0.09637445211410522\n",
      "Epoch 4060/30000 Validation Loss: 0.08322971314191818\n",
      "Epoch 4061/30000 Training Loss: 0.09468472003936768\n",
      "Epoch 4062/30000 Training Loss: 0.08892621845006943\n",
      "Epoch 4063/30000 Training Loss: 0.076580710709095\n",
      "Epoch 4064/30000 Training Loss: 0.09784311801195145\n",
      "Epoch 4065/30000 Training Loss: 0.12921111285686493\n",
      "Epoch 4066/30000 Training Loss: 0.09339010715484619\n",
      "Epoch 4067/30000 Training Loss: 0.08266732841730118\n",
      "Epoch 4068/30000 Training Loss: 0.10147332400083542\n",
      "Epoch 4069/30000 Training Loss: 0.09623328596353531\n",
      "Epoch 4070/30000 Training Loss: 0.10225570201873779\n",
      "Epoch 4070/30000 Validation Loss: 0.09130396693944931\n",
      "Epoch 4071/30000 Training Loss: 0.07651039958000183\n",
      "Epoch 4072/30000 Training Loss: 0.09682676196098328\n",
      "Epoch 4073/30000 Training Loss: 0.09162113070487976\n",
      "Epoch 4074/30000 Training Loss: 0.09459906816482544\n",
      "Epoch 4075/30000 Training Loss: 0.0856349989771843\n",
      "Epoch 4076/30000 Training Loss: 0.10045293718576431\n",
      "Epoch 4077/30000 Training Loss: 0.08463454246520996\n",
      "Epoch 4078/30000 Training Loss: 0.10016407817602158\n",
      "Epoch 4079/30000 Training Loss: 0.07318644225597382\n",
      "Epoch 4080/30000 Training Loss: 0.07508151978254318\n",
      "Epoch 4080/30000 Validation Loss: 0.09521880000829697\n",
      "Epoch 4081/30000 Training Loss: 0.07914593070745468\n",
      "Epoch 4082/30000 Training Loss: 0.08923963457345963\n",
      "Epoch 4083/30000 Training Loss: 0.09549671411514282\n",
      "Epoch 4084/30000 Training Loss: 0.09369361400604248\n",
      "Epoch 4085/30000 Training Loss: 0.09650567173957825\n",
      "Epoch 4086/30000 Training Loss: 0.11067002266645432\n",
      "Epoch 4087/30000 Training Loss: 0.08027411252260208\n",
      "Epoch 4088/30000 Training Loss: 0.07078268378973007\n",
      "Epoch 4089/30000 Training Loss: 0.0931360125541687\n",
      "Epoch 4090/30000 Training Loss: 0.07310544699430466\n",
      "Epoch 4090/30000 Validation Loss: 0.10693573951721191\n",
      "Epoch 4091/30000 Training Loss: 0.10114818811416626\n",
      "Epoch 4092/30000 Training Loss: 0.07333013415336609\n",
      "Epoch 4093/30000 Training Loss: 0.09150558710098267\n",
      "Epoch 4094/30000 Training Loss: 0.0738852247595787\n",
      "Epoch 4095/30000 Training Loss: 0.12837384641170502\n",
      "Epoch 4096/30000 Training Loss: 0.11494764685630798\n",
      "Epoch 4097/30000 Training Loss: 0.09862645715475082\n",
      "Epoch 4098/30000 Training Loss: 0.07563665509223938\n",
      "Epoch 4099/30000 Training Loss: 0.07369210571050644\n",
      "Epoch 4100/30000 Training Loss: 0.10609821230173111\n",
      "Epoch 4100/30000 Validation Loss: 0.10728753358125687\n",
      "Epoch 4101/30000 Training Loss: 0.10467036813497543\n",
      "Epoch 4102/30000 Training Loss: 0.0771605372428894\n",
      "Epoch 4103/30000 Training Loss: 0.08153913915157318\n",
      "Epoch 4104/30000 Training Loss: 0.10402199625968933\n",
      "Epoch 4105/30000 Training Loss: 0.08232314884662628\n",
      "Epoch 4106/30000 Training Loss: 0.10087943822145462\n",
      "Epoch 4107/30000 Training Loss: 0.11523038148880005\n",
      "Epoch 4108/30000 Training Loss: 0.07319695502519608\n",
      "Epoch 4109/30000 Training Loss: 0.08319440484046936\n",
      "Epoch 4110/30000 Training Loss: 0.09275742620229721\n",
      "Epoch 4110/30000 Validation Loss: 0.09039798378944397\n",
      "Epoch 4111/30000 Training Loss: 0.07612400501966476\n",
      "Epoch 4112/30000 Training Loss: 0.0893123522400856\n",
      "Epoch 4113/30000 Training Loss: 0.0761953592300415\n",
      "Epoch 4114/30000 Training Loss: 0.11192502826452255\n",
      "Epoch 4115/30000 Training Loss: 0.079832524061203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4116/30000 Training Loss: 0.08392032235860825\n",
      "Epoch 4117/30000 Training Loss: 0.09388820081949234\n",
      "Epoch 4118/30000 Training Loss: 0.0816897451877594\n",
      "Epoch 4119/30000 Training Loss: 0.0979333445429802\n",
      "Epoch 4120/30000 Training Loss: 0.11589754372835159\n",
      "Epoch 4120/30000 Validation Loss: 0.08730866760015488\n",
      "Epoch 4121/30000 Training Loss: 0.10864266008138657\n",
      "Epoch 4122/30000 Training Loss: 0.09584447741508484\n",
      "Epoch 4123/30000 Training Loss: 0.07586473226547241\n",
      "Epoch 4124/30000 Training Loss: 0.0920386090874672\n",
      "Epoch 4125/30000 Training Loss: 0.1041482612490654\n",
      "Epoch 4126/30000 Training Loss: 0.09879079461097717\n",
      "Epoch 4127/30000 Training Loss: 0.07717963308095932\n",
      "Epoch 4128/30000 Training Loss: 0.09210574626922607\n",
      "Epoch 4129/30000 Training Loss: 0.09775616973638535\n",
      "Epoch 4130/30000 Training Loss: 0.11862778663635254\n",
      "Epoch 4130/30000 Validation Loss: 0.08833374828100204\n",
      "Epoch 4131/30000 Training Loss: 0.0952104702591896\n",
      "Epoch 4132/30000 Training Loss: 0.09546598047018051\n",
      "Epoch 4133/30000 Training Loss: 0.0989263653755188\n",
      "Epoch 4134/30000 Training Loss: 0.12200659513473511\n",
      "Epoch 4135/30000 Training Loss: 0.11043021827936172\n",
      "Epoch 4136/30000 Training Loss: 0.091189444065094\n",
      "Epoch 4137/30000 Training Loss: 0.07728774100542068\n",
      "Epoch 4138/30000 Training Loss: 0.103887178003788\n",
      "Epoch 4139/30000 Training Loss: 0.0984363779425621\n",
      "Epoch 4140/30000 Training Loss: 0.10426163673400879\n",
      "Epoch 4140/30000 Validation Loss: 0.09388377517461777\n",
      "Epoch 4141/30000 Training Loss: 0.0821155235171318\n",
      "Epoch 4142/30000 Training Loss: 0.07366741448640823\n",
      "Epoch 4143/30000 Training Loss: 0.08995983749628067\n",
      "Epoch 4144/30000 Training Loss: 0.08703915029764175\n",
      "Epoch 4145/30000 Training Loss: 0.0928686261177063\n",
      "Epoch 4146/30000 Training Loss: 0.10786345601081848\n",
      "Epoch 4147/30000 Training Loss: 0.11699670553207397\n",
      "Epoch 4148/30000 Training Loss: 0.09834548085927963\n",
      "Epoch 4149/30000 Training Loss: 0.07832511514425278\n",
      "Epoch 4150/30000 Training Loss: 0.08763053268194199\n",
      "Epoch 4150/30000 Validation Loss: 0.0947403684258461\n",
      "Epoch 4151/30000 Training Loss: 0.11970878392457962\n",
      "Epoch 4152/30000 Training Loss: 0.07545284181833267\n",
      "Epoch 4153/30000 Training Loss: 0.11563354730606079\n",
      "Epoch 4154/30000 Training Loss: 0.07345890253782272\n",
      "Epoch 4155/30000 Training Loss: 0.08288007229566574\n",
      "Epoch 4156/30000 Training Loss: 0.08413130044937134\n",
      "Epoch 4157/30000 Training Loss: 0.07046542316675186\n",
      "Epoch 4158/30000 Training Loss: 0.09524815529584885\n",
      "Epoch 4159/30000 Training Loss: 0.10973700881004333\n",
      "Epoch 4160/30000 Training Loss: 0.11288204789161682\n",
      "Epoch 4160/30000 Validation Loss: 0.08266209810972214\n",
      "Epoch 4161/30000 Training Loss: 0.09150499850511551\n",
      "Epoch 4162/30000 Training Loss: 0.07903163880109787\n",
      "Epoch 4163/30000 Training Loss: 0.10205352306365967\n",
      "Epoch 4164/30000 Training Loss: 0.09532027691602707\n",
      "Epoch 4165/30000 Training Loss: 0.0971759557723999\n",
      "Epoch 4166/30000 Training Loss: 0.10465524345636368\n",
      "Epoch 4167/30000 Training Loss: 0.08002886921167374\n",
      "Epoch 4168/30000 Training Loss: 0.11004791408777237\n",
      "Epoch 4169/30000 Training Loss: 0.10053633898496628\n",
      "Epoch 4170/30000 Training Loss: 0.11325208097696304\n",
      "Epoch 4170/30000 Validation Loss: 0.08702763170003891\n",
      "Epoch 4171/30000 Training Loss: 0.09782294183969498\n",
      "Epoch 4172/30000 Training Loss: 0.09125306457281113\n",
      "Epoch 4173/30000 Training Loss: 0.09486230462789536\n",
      "Epoch 4174/30000 Training Loss: 0.09426183253526688\n",
      "Epoch 4175/30000 Training Loss: 0.10074055194854736\n",
      "Epoch 4176/30000 Training Loss: 0.09772425889968872\n",
      "Epoch 4177/30000 Training Loss: 0.07053256779909134\n",
      "Epoch 4178/30000 Training Loss: 0.08737906068563461\n",
      "Epoch 4179/30000 Training Loss: 0.06970382481813431\n",
      "Epoch 4180/30000 Training Loss: 0.08809834718704224\n",
      "Epoch 4180/30000 Validation Loss: 0.10164875537157059\n",
      "Epoch 4181/30000 Training Loss: 0.09767406433820724\n",
      "Epoch 4182/30000 Training Loss: 0.11150994151830673\n",
      "Epoch 4183/30000 Training Loss: 0.0914812684059143\n",
      "Epoch 4184/30000 Training Loss: 0.07867281883955002\n",
      "Epoch 4185/30000 Training Loss: 0.0802687257528305\n",
      "Epoch 4186/30000 Training Loss: 0.08270420879125595\n",
      "Epoch 4187/30000 Training Loss: 0.0746791660785675\n",
      "Epoch 4188/30000 Training Loss: 0.08977246284484863\n",
      "Epoch 4189/30000 Training Loss: 0.0864238440990448\n",
      "Epoch 4190/30000 Training Loss: 0.09862818568944931\n",
      "Epoch 4190/30000 Validation Loss: 0.08411531895399094\n",
      "Epoch 4191/30000 Training Loss: 0.09555860608816147\n",
      "Epoch 4192/30000 Training Loss: 0.07395294308662415\n",
      "Epoch 4193/30000 Training Loss: 0.10862068086862564\n",
      "Epoch 4194/30000 Training Loss: 0.11313062161207199\n",
      "Epoch 4195/30000 Training Loss: 0.08966227620840073\n",
      "Epoch 4196/30000 Training Loss: 0.07558751106262207\n",
      "Epoch 4197/30000 Training Loss: 0.09668584913015366\n",
      "Epoch 4198/30000 Training Loss: 0.09236839413642883\n",
      "Epoch 4199/30000 Training Loss: 0.08434724807739258\n",
      "Epoch 4200/30000 Training Loss: 0.09083396196365356\n",
      "Epoch 4200/30000 Validation Loss: 0.07784568518400192\n",
      "Epoch 4201/30000 Training Loss: 0.09292721748352051\n",
      "Epoch 4202/30000 Training Loss: 0.08164562284946442\n",
      "Epoch 4203/30000 Training Loss: 0.08124982565641403\n",
      "Epoch 4204/30000 Training Loss: 0.13211436569690704\n",
      "Epoch 4205/30000 Training Loss: 0.09562572091817856\n",
      "Epoch 4206/30000 Training Loss: 0.08600934594869614\n",
      "Epoch 4207/30000 Training Loss: 0.09496573358774185\n",
      "Epoch 4208/30000 Training Loss: 0.08608029037714005\n",
      "Epoch 4209/30000 Training Loss: 0.08914273232221603\n",
      "Epoch 4210/30000 Training Loss: 0.09132815152406693\n",
      "Epoch 4210/30000 Validation Loss: 0.10457605868577957\n",
      "Epoch 4211/30000 Training Loss: 0.11078400164842606\n",
      "Epoch 4212/30000 Training Loss: 0.0762627124786377\n",
      "Epoch 4213/30000 Training Loss: 0.09482532739639282\n",
      "Epoch 4214/30000 Training Loss: 0.07688960433006287\n",
      "Epoch 4215/30000 Training Loss: 0.09557787328958511\n",
      "Epoch 4216/30000 Training Loss: 0.08088027685880661\n",
      "Epoch 4217/30000 Training Loss: 0.0902116522192955\n",
      "Epoch 4218/30000 Training Loss: 0.08786997199058533\n",
      "Epoch 4219/30000 Training Loss: 0.07132191210985184\n",
      "Epoch 4220/30000 Training Loss: 0.07033193856477737\n",
      "Epoch 4220/30000 Validation Loss: 0.09225056320428848\n",
      "Epoch 4221/30000 Training Loss: 0.09070426225662231\n",
      "Epoch 4222/30000 Training Loss: 0.09228124469518661\n",
      "Epoch 4223/30000 Training Loss: 0.08526317030191422\n",
      "Epoch 4224/30000 Training Loss: 0.0970863625407219\n",
      "Epoch 4225/30000 Training Loss: 0.07852325588464737\n",
      "Epoch 4226/30000 Training Loss: 0.09224560856819153\n",
      "Epoch 4227/30000 Training Loss: 0.09743524342775345\n",
      "Epoch 4228/30000 Training Loss: 0.10526806116104126\n",
      "Epoch 4229/30000 Training Loss: 0.08269202709197998\n",
      "Epoch 4230/30000 Training Loss: 0.07463101297616959\n",
      "Epoch 4230/30000 Validation Loss: 0.07551450282335281\n",
      "Epoch 4231/30000 Training Loss: 0.09797992557287216\n",
      "Epoch 4232/30000 Training Loss: 0.10143575072288513\n",
      "Epoch 4233/30000 Training Loss: 0.07223401218652725\n",
      "Epoch 4234/30000 Training Loss: 0.07320871949195862\n",
      "Epoch 4235/30000 Training Loss: 0.07998686283826828\n",
      "Epoch 4236/30000 Training Loss: 0.06836474686861038\n",
      "Epoch 4237/30000 Training Loss: 0.1050369068980217\n",
      "Epoch 4238/30000 Training Loss: 0.07915744185447693\n",
      "Epoch 4239/30000 Training Loss: 0.0982787236571312\n",
      "Epoch 4240/30000 Training Loss: 0.10983952134847641\n",
      "Epoch 4240/30000 Validation Loss: 0.07568591088056564\n",
      "Epoch 4241/30000 Training Loss: 0.0843454971909523\n",
      "Epoch 4242/30000 Training Loss: 0.09189707040786743\n",
      "Epoch 4243/30000 Training Loss: 0.11630938202142715\n",
      "Epoch 4244/30000 Training Loss: 0.08343271166086197\n",
      "Epoch 4245/30000 Training Loss: 0.09717301279306412\n",
      "Epoch 4246/30000 Training Loss: 0.11381841450929642\n",
      "Epoch 4247/30000 Training Loss: 0.08541373163461685\n",
      "Epoch 4248/30000 Training Loss: 0.08040856570005417\n",
      "Epoch 4249/30000 Training Loss: 0.08071455359458923\n",
      "Epoch 4250/30000 Training Loss: 0.0869254469871521\n",
      "Epoch 4250/30000 Validation Loss: 0.07558292150497437\n",
      "Epoch 4251/30000 Training Loss: 0.10091885179281235\n",
      "Epoch 4252/30000 Training Loss: 0.07966980338096619\n",
      "Epoch 4253/30000 Training Loss: 0.07626315206289291\n",
      "Epoch 4254/30000 Training Loss: 0.0881277546286583\n",
      "Epoch 4255/30000 Training Loss: 0.07809003442525864\n",
      "Epoch 4256/30000 Training Loss: 0.12483382225036621\n",
      "Epoch 4257/30000 Training Loss: 0.0768161192536354\n",
      "Epoch 4258/30000 Training Loss: 0.07679059356451035\n",
      "Epoch 4259/30000 Training Loss: 0.08869647234678268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4260/30000 Training Loss: 0.10965672880411148\n",
      "Epoch 4260/30000 Validation Loss: 0.0844757929444313\n",
      "Epoch 4261/30000 Training Loss: 0.10684585571289062\n",
      "Epoch 4262/30000 Training Loss: 0.07728296518325806\n",
      "Epoch 4263/30000 Training Loss: 0.07642048597335815\n",
      "Epoch 4264/30000 Training Loss: 0.08098975569009781\n",
      "Epoch 4265/30000 Training Loss: 0.09928848594427109\n",
      "Epoch 4266/30000 Training Loss: 0.06670869141817093\n",
      "Epoch 4267/30000 Training Loss: 0.07193911075592041\n",
      "Epoch 4268/30000 Training Loss: 0.08651208132505417\n",
      "Epoch 4269/30000 Training Loss: 0.08957406878471375\n",
      "Epoch 4270/30000 Training Loss: 0.10459774732589722\n",
      "Epoch 4270/30000 Validation Loss: 0.09403566271066666\n",
      "Epoch 4271/30000 Training Loss: 0.09646978229284286\n",
      "Epoch 4272/30000 Training Loss: 0.08030541986227036\n",
      "Epoch 4273/30000 Training Loss: 0.0850740447640419\n",
      "Epoch 4274/30000 Training Loss: 0.07866309583187103\n",
      "Epoch 4275/30000 Training Loss: 0.09314573556184769\n",
      "Epoch 4276/30000 Training Loss: 0.07742449641227722\n",
      "Epoch 4277/30000 Training Loss: 0.09961012005805969\n",
      "Epoch 4278/30000 Training Loss: 0.10228540748357773\n",
      "Epoch 4279/30000 Training Loss: 0.1041518822312355\n",
      "Epoch 4280/30000 Training Loss: 0.09018968790769577\n",
      "Epoch 4280/30000 Validation Loss: 0.08780767768621445\n",
      "Epoch 4281/30000 Training Loss: 0.08157158643007278\n",
      "Epoch 4282/30000 Training Loss: 0.09537262469530106\n",
      "Epoch 4283/30000 Training Loss: 0.08538326621055603\n",
      "Epoch 4284/30000 Training Loss: 0.08173046261072159\n",
      "Epoch 4285/30000 Training Loss: 0.0951174795627594\n",
      "Epoch 4286/30000 Training Loss: 0.07355708628892899\n",
      "Epoch 4287/30000 Training Loss: 0.11672499775886536\n",
      "Epoch 4288/30000 Training Loss: 0.10905446857213974\n",
      "Epoch 4289/30000 Training Loss: 0.07947506755590439\n",
      "Epoch 4290/30000 Training Loss: 0.08597495406866074\n",
      "Epoch 4290/30000 Validation Loss: 0.1049831435084343\n",
      "Epoch 4291/30000 Training Loss: 0.09835235029459\n",
      "Epoch 4292/30000 Training Loss: 0.08329679071903229\n",
      "Epoch 4293/30000 Training Loss: 0.09667008370161057\n",
      "Epoch 4294/30000 Training Loss: 0.09641104191541672\n",
      "Epoch 4295/30000 Training Loss: 0.09499242156744003\n",
      "Epoch 4296/30000 Training Loss: 0.11398670077323914\n",
      "Epoch 4297/30000 Training Loss: 0.08636823296546936\n",
      "Epoch 4298/30000 Training Loss: 0.1008436381816864\n",
      "Epoch 4299/30000 Training Loss: 0.07951207458972931\n",
      "Epoch 4300/30000 Training Loss: 0.08444401621818542\n",
      "Epoch 4300/30000 Validation Loss: 0.07380873709917068\n",
      "Epoch 4301/30000 Training Loss: 0.09027288109064102\n",
      "Epoch 4302/30000 Training Loss: 0.07560597360134125\n",
      "Epoch 4303/30000 Training Loss: 0.0885956808924675\n",
      "Epoch 4304/30000 Training Loss: 0.10304755717515945\n",
      "Epoch 4305/30000 Training Loss: 0.07636099308729172\n",
      "Epoch 4306/30000 Training Loss: 0.10653834789991379\n",
      "Epoch 4307/30000 Training Loss: 0.11251295357942581\n",
      "Epoch 4308/30000 Training Loss: 0.11131036281585693\n",
      "Epoch 4309/30000 Training Loss: 0.08294882625341415\n",
      "Epoch 4310/30000 Training Loss: 0.08278907835483551\n",
      "Epoch 4310/30000 Validation Loss: 0.09929627180099487\n",
      "Epoch 4311/30000 Training Loss: 0.12941040098667145\n",
      "Epoch 4312/30000 Training Loss: 0.08033437281847\n",
      "Epoch 4313/30000 Training Loss: 0.10345923900604248\n",
      "Epoch 4314/30000 Training Loss: 0.08722797781229019\n",
      "Epoch 4315/30000 Training Loss: 0.0866675153374672\n",
      "Epoch 4316/30000 Training Loss: 0.10053467750549316\n",
      "Epoch 4317/30000 Training Loss: 0.09327834844589233\n",
      "Epoch 4318/30000 Training Loss: 0.0859360471367836\n",
      "Epoch 4319/30000 Training Loss: 0.10154587030410767\n",
      "Epoch 4320/30000 Training Loss: 0.10014458745718002\n",
      "Epoch 4320/30000 Validation Loss: 0.0868646577000618\n",
      "Epoch 4321/30000 Training Loss: 0.09902378171682358\n",
      "Epoch 4322/30000 Training Loss: 0.08237319439649582\n",
      "Epoch 4323/30000 Training Loss: 0.1031014546751976\n",
      "Epoch 4324/30000 Training Loss: 0.09429826587438583\n",
      "Epoch 4325/30000 Training Loss: 0.08059511333703995\n",
      "Epoch 4326/30000 Training Loss: 0.07613754272460938\n",
      "Epoch 4327/30000 Training Loss: 0.07222113758325577\n",
      "Epoch 4328/30000 Training Loss: 0.09705245494842529\n",
      "Epoch 4329/30000 Training Loss: 0.06793969124555588\n",
      "Epoch 4330/30000 Training Loss: 0.08805655688047409\n",
      "Epoch 4330/30000 Validation Loss: 0.10548633337020874\n",
      "Epoch 4331/30000 Training Loss: 0.09784173965454102\n",
      "Epoch 4332/30000 Training Loss: 0.0973920226097107\n",
      "Epoch 4333/30000 Training Loss: 0.09182997792959213\n",
      "Epoch 4334/30000 Training Loss: 0.07374081015586853\n",
      "Epoch 4335/30000 Training Loss: 0.07310372591018677\n",
      "Epoch 4336/30000 Training Loss: 0.07753235846757889\n",
      "Epoch 4337/30000 Training Loss: 0.07947570085525513\n",
      "Epoch 4338/30000 Training Loss: 0.08187678456306458\n",
      "Epoch 4339/30000 Training Loss: 0.05419977009296417\n",
      "Epoch 4340/30000 Training Loss: 0.09774679690599442\n",
      "Epoch 4340/30000 Validation Loss: 0.10143914818763733\n",
      "Epoch 4341/30000 Training Loss: 0.07747980952262878\n",
      "Epoch 4342/30000 Training Loss: 0.11231774091720581\n",
      "Epoch 4343/30000 Training Loss: 0.08737748116254807\n",
      "Epoch 4344/30000 Training Loss: 0.10071805119514465\n",
      "Epoch 4345/30000 Training Loss: 0.0843283161520958\n",
      "Epoch 4346/30000 Training Loss: 0.09159982204437256\n",
      "Epoch 4347/30000 Training Loss: 0.10438085347414017\n",
      "Epoch 4348/30000 Training Loss: 0.07971533387899399\n",
      "Epoch 4349/30000 Training Loss: 0.09537126868963242\n",
      "Epoch 4350/30000 Training Loss: 0.09558134526014328\n",
      "Epoch 4350/30000 Validation Loss: 0.09734916687011719\n",
      "Epoch 4351/30000 Training Loss: 0.08655139058828354\n",
      "Epoch 4352/30000 Training Loss: 0.10059040039777756\n",
      "Epoch 4353/30000 Training Loss: 0.0659133717417717\n",
      "Epoch 4354/30000 Training Loss: 0.08324103057384491\n",
      "Epoch 4355/30000 Training Loss: 0.08370837569236755\n",
      "Epoch 4356/30000 Training Loss: 0.08102977275848389\n",
      "Epoch 4357/30000 Training Loss: 0.08651896566152573\n",
      "Epoch 4358/30000 Training Loss: 0.07334687560796738\n",
      "Epoch 4359/30000 Training Loss: 0.09102543443441391\n",
      "Epoch 4360/30000 Training Loss: 0.08826622366905212\n",
      "Epoch 4360/30000 Validation Loss: 0.082730233669281\n",
      "Epoch 4361/30000 Training Loss: 0.0889817550778389\n",
      "Epoch 4362/30000 Training Loss: 0.08555655926465988\n",
      "Epoch 4363/30000 Training Loss: 0.09614589065313339\n",
      "Epoch 4364/30000 Training Loss: 0.07040915638208389\n",
      "Epoch 4365/30000 Training Loss: 0.08776908367872238\n",
      "Epoch 4366/30000 Training Loss: 0.09983759373426437\n",
      "Epoch 4367/30000 Training Loss: 0.07950695604085922\n",
      "Epoch 4368/30000 Training Loss: 0.08201820403337479\n",
      "Epoch 4369/30000 Training Loss: 0.07793194055557251\n",
      "Epoch 4370/30000 Training Loss: 0.09790951013565063\n",
      "Epoch 4370/30000 Validation Loss: 0.09221240133047104\n",
      "Epoch 4371/30000 Training Loss: 0.0845792293548584\n",
      "Epoch 4372/30000 Training Loss: 0.10954736918210983\n",
      "Epoch 4373/30000 Training Loss: 0.0753495916724205\n",
      "Epoch 4374/30000 Training Loss: 0.11133923381567001\n",
      "Epoch 4375/30000 Training Loss: 0.06120898947119713\n",
      "Epoch 4376/30000 Training Loss: 0.09353730827569962\n",
      "Epoch 4377/30000 Training Loss: 0.08717913180589676\n",
      "Epoch 4378/30000 Training Loss: 0.08031300455331802\n",
      "Epoch 4379/30000 Training Loss: 0.07054919749498367\n",
      "Epoch 4380/30000 Training Loss: 0.09734270721673965\n",
      "Epoch 4380/30000 Validation Loss: 0.07891195267438889\n",
      "Epoch 4381/30000 Training Loss: 0.10310513526201248\n",
      "Epoch 4382/30000 Training Loss: 0.08694689720869064\n",
      "Epoch 4383/30000 Training Loss: 0.08024480193853378\n",
      "Epoch 4384/30000 Training Loss: 0.07732506096363068\n",
      "Epoch 4385/30000 Training Loss: 0.11218822002410889\n",
      "Epoch 4386/30000 Training Loss: 0.09430775046348572\n",
      "Epoch 4387/30000 Training Loss: 0.08455004543066025\n",
      "Epoch 4388/30000 Training Loss: 0.07314319908618927\n",
      "Epoch 4389/30000 Training Loss: 0.08538227528333664\n",
      "Epoch 4390/30000 Training Loss: 0.10077740997076035\n",
      "Epoch 4390/30000 Validation Loss: 0.08588380366563797\n",
      "Epoch 4391/30000 Training Loss: 0.08987167477607727\n",
      "Epoch 4392/30000 Training Loss: 0.08339929580688477\n",
      "Epoch 4393/30000 Training Loss: 0.07507874816656113\n",
      "Epoch 4394/30000 Training Loss: 0.09540332108736038\n",
      "Epoch 4395/30000 Training Loss: 0.08539540320634842\n",
      "Epoch 4396/30000 Training Loss: 0.09445295482873917\n",
      "Epoch 4397/30000 Training Loss: 0.08248678594827652\n",
      "Epoch 4398/30000 Training Loss: 0.09124495834112167\n",
      "Epoch 4399/30000 Training Loss: 0.07608950883150101\n",
      "Epoch 4400/30000 Training Loss: 0.10095737129449844\n",
      "Epoch 4400/30000 Validation Loss: 0.10615987330675125\n",
      "Epoch 4401/30000 Training Loss: 0.10093126446008682\n",
      "Epoch 4402/30000 Training Loss: 0.09700417518615723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4403/30000 Training Loss: 0.08871778845787048\n",
      "Epoch 4404/30000 Training Loss: 0.10343632847070694\n",
      "Epoch 4405/30000 Training Loss: 0.09714757651090622\n",
      "Epoch 4406/30000 Training Loss: 0.11968708783388138\n",
      "Epoch 4407/30000 Training Loss: 0.083798348903656\n",
      "Epoch 4408/30000 Training Loss: 0.1111646518111229\n",
      "Epoch 4409/30000 Training Loss: 0.08186423033475876\n",
      "Epoch 4410/30000 Training Loss: 0.0987929031252861\n",
      "Epoch 4410/30000 Validation Loss: 0.09454572200775146\n",
      "Epoch 4411/30000 Training Loss: 0.09836103767156601\n",
      "Epoch 4412/30000 Training Loss: 0.11184170842170715\n",
      "Epoch 4413/30000 Training Loss: 0.09667583554983139\n",
      "Epoch 4414/30000 Training Loss: 0.07365082949399948\n",
      "Epoch 4415/30000 Training Loss: 0.08424007892608643\n",
      "Epoch 4416/30000 Training Loss: 0.06743259727954865\n",
      "Epoch 4417/30000 Training Loss: 0.06369339674711227\n",
      "Epoch 4418/30000 Training Loss: 0.08406152576208115\n",
      "Epoch 4419/30000 Training Loss: 0.07644513249397278\n",
      "Epoch 4420/30000 Training Loss: 0.09898406267166138\n",
      "Epoch 4420/30000 Validation Loss: 0.10730090737342834\n",
      "Epoch 4421/30000 Training Loss: 0.10783994942903519\n",
      "Epoch 4422/30000 Training Loss: 0.08878996968269348\n",
      "Epoch 4423/30000 Training Loss: 0.06682024151086807\n",
      "Epoch 4424/30000 Training Loss: 0.07964131236076355\n",
      "Epoch 4425/30000 Training Loss: 0.09459158033132553\n",
      "Epoch 4426/30000 Training Loss: 0.07228096574544907\n",
      "Epoch 4427/30000 Training Loss: 0.08086305111646652\n",
      "Epoch 4428/30000 Training Loss: 0.08231353759765625\n",
      "Epoch 4429/30000 Training Loss: 0.08514948934316635\n",
      "Epoch 4430/30000 Training Loss: 0.08168824017047882\n",
      "Epoch 4430/30000 Validation Loss: 0.08489146828651428\n",
      "Epoch 4431/30000 Training Loss: 0.09226158261299133\n",
      "Epoch 4432/30000 Training Loss: 0.09648066759109497\n",
      "Epoch 4433/30000 Training Loss: 0.0894913598895073\n",
      "Epoch 4434/30000 Training Loss: 0.09448950737714767\n",
      "Epoch 4435/30000 Training Loss: 0.0787208080291748\n",
      "Epoch 4436/30000 Training Loss: 0.09044117480516434\n",
      "Epoch 4437/30000 Training Loss: 0.08797178417444229\n",
      "Epoch 4438/30000 Training Loss: 0.13285799324512482\n",
      "Epoch 4439/30000 Training Loss: 0.08658242970705032\n",
      "Epoch 4440/30000 Training Loss: 0.08230777829885483\n",
      "Epoch 4440/30000 Validation Loss: 0.09129363298416138\n",
      "Epoch 4441/30000 Training Loss: 0.07836226373910904\n",
      "Epoch 4442/30000 Training Loss: 0.10776275396347046\n",
      "Epoch 4443/30000 Training Loss: 0.06956276297569275\n",
      "Epoch 4444/30000 Training Loss: 0.0935688391327858\n",
      "Epoch 4445/30000 Training Loss: 0.08267264068126678\n",
      "Epoch 4446/30000 Training Loss: 0.07998178154230118\n",
      "Epoch 4447/30000 Training Loss: 0.10571400076150894\n",
      "Epoch 4448/30000 Training Loss: 0.10203936696052551\n",
      "Epoch 4449/30000 Training Loss: 0.0809122696518898\n",
      "Epoch 4450/30000 Training Loss: 0.09668966382741928\n",
      "Epoch 4450/30000 Validation Loss: 0.08373501151800156\n",
      "Epoch 4451/30000 Training Loss: 0.09449741989374161\n",
      "Epoch 4452/30000 Training Loss: 0.12080834060907364\n",
      "Epoch 4453/30000 Training Loss: 0.0846242681145668\n",
      "Epoch 4454/30000 Training Loss: 0.07921043783426285\n",
      "Epoch 4455/30000 Training Loss: 0.08552239090204239\n",
      "Epoch 4456/30000 Training Loss: 0.09792196750640869\n",
      "Epoch 4457/30000 Training Loss: 0.07836351543664932\n",
      "Epoch 4458/30000 Training Loss: 0.08796345442533493\n",
      "Epoch 4459/30000 Training Loss: 0.09525803476572037\n",
      "Epoch 4460/30000 Training Loss: 0.1079375371336937\n",
      "Epoch 4460/30000 Validation Loss: 0.08179263025522232\n",
      "Epoch 4461/30000 Training Loss: 0.10068400949239731\n",
      "Epoch 4462/30000 Training Loss: 0.08797255903482437\n",
      "Epoch 4463/30000 Training Loss: 0.09068978577852249\n",
      "Epoch 4464/30000 Training Loss: 0.07954699546098709\n",
      "Epoch 4465/30000 Training Loss: 0.0843677744269371\n",
      "Epoch 4466/30000 Training Loss: 0.07291197031736374\n",
      "Epoch 4467/30000 Training Loss: 0.08908950537443161\n",
      "Epoch 4468/30000 Training Loss: 0.08608468621969223\n",
      "Epoch 4469/30000 Training Loss: 0.0948662981390953\n",
      "Epoch 4470/30000 Training Loss: 0.08615806698799133\n",
      "Epoch 4470/30000 Validation Loss: 0.07620684057474136\n",
      "Epoch 4471/30000 Training Loss: 0.08102100342512131\n",
      "Epoch 4472/30000 Training Loss: 0.09155156463384628\n",
      "Epoch 4473/30000 Training Loss: 0.11079269647598267\n",
      "Epoch 4474/30000 Training Loss: 0.09572237730026245\n",
      "Epoch 4475/30000 Training Loss: 0.06846383213996887\n",
      "Epoch 4476/30000 Training Loss: 0.08914174884557724\n",
      "Epoch 4477/30000 Training Loss: 0.09115777164697647\n",
      "Epoch 4478/30000 Training Loss: 0.09755170345306396\n",
      "Epoch 4479/30000 Training Loss: 0.08713751286268234\n",
      "Epoch 4480/30000 Training Loss: 0.12297145277261734\n",
      "Epoch 4480/30000 Validation Loss: 0.09245438128709793\n",
      "Epoch 4481/30000 Training Loss: 0.0894041582942009\n",
      "Epoch 4482/30000 Training Loss: 0.09698592871427536\n",
      "Epoch 4483/30000 Training Loss: 0.08890646696090698\n",
      "Epoch 4484/30000 Training Loss: 0.07195290923118591\n",
      "Epoch 4485/30000 Training Loss: 0.08832186460494995\n",
      "Epoch 4486/30000 Training Loss: 0.0912412479519844\n",
      "Epoch 4487/30000 Training Loss: 0.08048693090677261\n",
      "Epoch 4488/30000 Training Loss: 0.09008847922086716\n",
      "Epoch 4489/30000 Training Loss: 0.08309057354927063\n",
      "Epoch 4490/30000 Training Loss: 0.11898014694452286\n",
      "Epoch 4490/30000 Validation Loss: 0.0729866549372673\n",
      "Epoch 4491/30000 Training Loss: 0.08674604445695877\n",
      "Epoch 4492/30000 Training Loss: 0.09831029921770096\n",
      "Epoch 4493/30000 Training Loss: 0.10987222194671631\n",
      "Epoch 4494/30000 Training Loss: 0.11710353940725327\n",
      "Epoch 4495/30000 Training Loss: 0.09309283643960953\n",
      "Epoch 4496/30000 Training Loss: 0.08377572894096375\n",
      "Epoch 4497/30000 Training Loss: 0.09839993715286255\n",
      "Epoch 4498/30000 Training Loss: 0.1140357032418251\n",
      "Epoch 4499/30000 Training Loss: 0.09848412126302719\n",
      "Epoch 4500/30000 Training Loss: 0.11125498265028\n",
      "Epoch 4500/30000 Validation Loss: 0.07995838671922684\n",
      "Epoch 4501/30000 Training Loss: 0.07349437475204468\n",
      "Epoch 4502/30000 Training Loss: 0.10007036477327347\n",
      "Epoch 4503/30000 Training Loss: 0.08789686113595963\n",
      "Epoch 4504/30000 Training Loss: 0.07629306614398956\n",
      "Epoch 4505/30000 Training Loss: 0.09105091542005539\n",
      "Epoch 4506/30000 Training Loss: 0.0802578404545784\n",
      "Epoch 4507/30000 Training Loss: 0.08910688012838364\n",
      "Epoch 4508/30000 Training Loss: 0.08729029446840286\n",
      "Epoch 4509/30000 Training Loss: 0.09887682646512985\n",
      "Epoch 4510/30000 Training Loss: 0.10101441293954849\n",
      "Epoch 4510/30000 Validation Loss: 0.08704879879951477\n",
      "Epoch 4511/30000 Training Loss: 0.10518106073141098\n",
      "Epoch 4512/30000 Training Loss: 0.10905265063047409\n",
      "Epoch 4513/30000 Training Loss: 0.08317942172288895\n",
      "Epoch 4514/30000 Training Loss: 0.09464895725250244\n",
      "Epoch 4515/30000 Training Loss: 0.10600432008504868\n",
      "Epoch 4516/30000 Training Loss: 0.0667048767209053\n",
      "Epoch 4517/30000 Training Loss: 0.08270584046840668\n",
      "Epoch 4518/30000 Training Loss: 0.06659790128469467\n",
      "Epoch 4519/30000 Training Loss: 0.09365955740213394\n",
      "Epoch 4520/30000 Training Loss: 0.07575846463441849\n",
      "Epoch 4520/30000 Validation Loss: 0.06776072084903717\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06776072084903717<=============\n",
      "Epoch 4521/30000 Training Loss: 0.07406488806009293\n",
      "Epoch 4522/30000 Training Loss: 0.09672365337610245\n",
      "Epoch 4523/30000 Training Loss: 0.09419438987970352\n",
      "Epoch 4524/30000 Training Loss: 0.09694650024175644\n",
      "Epoch 4525/30000 Training Loss: 0.10004331916570663\n",
      "Epoch 4526/30000 Training Loss: 0.11541319638490677\n",
      "Epoch 4527/30000 Training Loss: 0.09386062622070312\n",
      "Epoch 4528/30000 Training Loss: 0.08626309782266617\n",
      "Epoch 4529/30000 Training Loss: 0.0994824543595314\n",
      "Epoch 4530/30000 Training Loss: 0.08699095249176025\n",
      "Epoch 4530/30000 Validation Loss: 0.08689669519662857\n",
      "Epoch 4531/30000 Training Loss: 0.10701050609350204\n",
      "Epoch 4532/30000 Training Loss: 0.09626702219247818\n",
      "Epoch 4533/30000 Training Loss: 0.07379117608070374\n",
      "Epoch 4534/30000 Training Loss: 0.08731073141098022\n",
      "Epoch 4535/30000 Training Loss: 0.08084068447351456\n",
      "Epoch 4536/30000 Training Loss: 0.10494842380285263\n",
      "Epoch 4537/30000 Training Loss: 0.08482819050550461\n",
      "Epoch 4538/30000 Training Loss: 0.0869632363319397\n",
      "Epoch 4539/30000 Training Loss: 0.07720110565423965\n",
      "Epoch 4540/30000 Training Loss: 0.08722955733537674\n",
      "Epoch 4540/30000 Validation Loss: 0.10941928625106812\n",
      "Epoch 4541/30000 Training Loss: 0.08367326110601425\n",
      "Epoch 4542/30000 Training Loss: 0.1017443835735321\n",
      "Epoch 4543/30000 Training Loss: 0.08701813220977783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4544/30000 Training Loss: 0.0898098573088646\n",
      "Epoch 4545/30000 Training Loss: 0.0964871421456337\n",
      "Epoch 4546/30000 Training Loss: 0.09390491247177124\n",
      "Epoch 4547/30000 Training Loss: 0.09930058568716049\n",
      "Epoch 4548/30000 Training Loss: 0.10673121362924576\n",
      "Epoch 4549/30000 Training Loss: 0.08477720618247986\n",
      "Epoch 4550/30000 Training Loss: 0.10774631053209305\n",
      "Epoch 4550/30000 Validation Loss: 0.08275273442268372\n",
      "Epoch 4551/30000 Training Loss: 0.07963759452104568\n",
      "Epoch 4552/30000 Training Loss: 0.0731763169169426\n",
      "Epoch 4553/30000 Training Loss: 0.08511445671319962\n",
      "Epoch 4554/30000 Training Loss: 0.09194790571928024\n",
      "Epoch 4555/30000 Training Loss: 0.10124385356903076\n",
      "Epoch 4556/30000 Training Loss: 0.08126851916313171\n",
      "Epoch 4557/30000 Training Loss: 0.1300867795944214\n",
      "Epoch 4558/30000 Training Loss: 0.12055420875549316\n",
      "Epoch 4559/30000 Training Loss: 0.09456852078437805\n",
      "Epoch 4560/30000 Training Loss: 0.08958180993795395\n",
      "Epoch 4560/30000 Validation Loss: 0.10661324113607407\n",
      "Epoch 4561/30000 Training Loss: 0.10147219896316528\n",
      "Epoch 4562/30000 Training Loss: 0.09925033897161484\n",
      "Epoch 4563/30000 Training Loss: 0.09284476190805435\n",
      "Epoch 4564/30000 Training Loss: 0.08718210458755493\n",
      "Epoch 4565/30000 Training Loss: 0.09644754976034164\n",
      "Epoch 4566/30000 Training Loss: 0.09156819432973862\n",
      "Epoch 4567/30000 Training Loss: 0.09356632083654404\n",
      "Epoch 4568/30000 Training Loss: 0.09949208050966263\n",
      "Epoch 4569/30000 Training Loss: 0.09820201247930527\n",
      "Epoch 4570/30000 Training Loss: 0.08043854683637619\n",
      "Epoch 4570/30000 Validation Loss: 0.10671008378267288\n",
      "Epoch 4571/30000 Training Loss: 0.10622826963663101\n",
      "Epoch 4572/30000 Training Loss: 0.0991658940911293\n",
      "Epoch 4573/30000 Training Loss: 0.0945102646946907\n",
      "Epoch 4574/30000 Training Loss: 0.10471630096435547\n",
      "Epoch 4575/30000 Training Loss: 0.08021251112222672\n",
      "Epoch 4576/30000 Training Loss: 0.07231233268976212\n",
      "Epoch 4577/30000 Training Loss: 0.09980971366167068\n",
      "Epoch 4578/30000 Training Loss: 0.10414896160364151\n",
      "Epoch 4579/30000 Training Loss: 0.0891963541507721\n",
      "Epoch 4580/30000 Training Loss: 0.07961247116327286\n",
      "Epoch 4580/30000 Validation Loss: 0.09303978830575943\n",
      "Epoch 4581/30000 Training Loss: 0.10129142552614212\n",
      "Epoch 4582/30000 Training Loss: 0.10701315850019455\n",
      "Epoch 4583/30000 Training Loss: 0.06544478982686996\n",
      "Epoch 4584/30000 Training Loss: 0.08550067991018295\n",
      "Epoch 4585/30000 Training Loss: 0.0714133083820343\n",
      "Epoch 4586/30000 Training Loss: 0.09673098474740982\n",
      "Epoch 4587/30000 Training Loss: 0.09398660808801651\n",
      "Epoch 4588/30000 Training Loss: 0.07506831735372543\n",
      "Epoch 4589/30000 Training Loss: 0.08701563626527786\n",
      "Epoch 4590/30000 Training Loss: 0.09662415832281113\n",
      "Epoch 4590/30000 Validation Loss: 0.0915542021393776\n",
      "Epoch 4591/30000 Training Loss: 0.08606526255607605\n",
      "Epoch 4592/30000 Training Loss: 0.10511533170938492\n",
      "Epoch 4593/30000 Training Loss: 0.10928436368703842\n",
      "Epoch 4594/30000 Training Loss: 0.12439411878585815\n",
      "Epoch 4595/30000 Training Loss: 0.07728635519742966\n",
      "Epoch 4596/30000 Training Loss: 0.08441740274429321\n",
      "Epoch 4597/30000 Training Loss: 0.09854675084352493\n",
      "Epoch 4598/30000 Training Loss: 0.10226273536682129\n",
      "Epoch 4599/30000 Training Loss: 0.08444011211395264\n",
      "Epoch 4600/30000 Training Loss: 0.08465516567230225\n",
      "Epoch 4600/30000 Validation Loss: 0.08253086358308792\n",
      "Epoch 4601/30000 Training Loss: 0.08210016041994095\n",
      "Epoch 4602/30000 Training Loss: 0.09673670679330826\n",
      "Epoch 4603/30000 Training Loss: 0.10007981210947037\n",
      "Epoch 4604/30000 Training Loss: 0.09356933832168579\n",
      "Epoch 4605/30000 Training Loss: 0.07506648451089859\n",
      "Epoch 4606/30000 Training Loss: 0.0857776626944542\n",
      "Epoch 4607/30000 Training Loss: 0.09418326616287231\n",
      "Epoch 4608/30000 Training Loss: 0.08265649527311325\n",
      "Epoch 4609/30000 Training Loss: 0.08400989323854446\n",
      "Epoch 4610/30000 Training Loss: 0.0892050638794899\n",
      "Epoch 4610/30000 Validation Loss: 0.09054882079362869\n",
      "Epoch 4611/30000 Training Loss: 0.0789780542254448\n",
      "Epoch 4612/30000 Training Loss: 0.08067483454942703\n",
      "Epoch 4613/30000 Training Loss: 0.1137271299958229\n",
      "Epoch 4614/30000 Training Loss: 0.08653245121240616\n",
      "Epoch 4615/30000 Training Loss: 0.09222818166017532\n",
      "Epoch 4616/30000 Training Loss: 0.10156098753213882\n",
      "Epoch 4617/30000 Training Loss: 0.08770298957824707\n",
      "Epoch 4618/30000 Training Loss: 0.09698528051376343\n",
      "Epoch 4619/30000 Training Loss: 0.09420642256736755\n",
      "Epoch 4620/30000 Training Loss: 0.08295886963605881\n",
      "Epoch 4620/30000 Validation Loss: 0.10164180397987366\n",
      "Epoch 4621/30000 Training Loss: 0.09049180895090103\n",
      "Epoch 4622/30000 Training Loss: 0.0910787284374237\n",
      "Epoch 4623/30000 Training Loss: 0.08710271120071411\n",
      "Epoch 4624/30000 Training Loss: 0.08774512261152267\n",
      "Epoch 4625/30000 Training Loss: 0.0703052207827568\n",
      "Epoch 4626/30000 Training Loss: 0.10088533908128738\n",
      "Epoch 4627/30000 Training Loss: 0.08749710768461227\n",
      "Epoch 4628/30000 Training Loss: 0.09558356553316116\n",
      "Epoch 4629/30000 Training Loss: 0.09258076548576355\n",
      "Epoch 4630/30000 Training Loss: 0.07198694348335266\n",
      "Epoch 4630/30000 Validation Loss: 0.09303951263427734\n",
      "Epoch 4631/30000 Training Loss: 0.09441275149583817\n",
      "Epoch 4632/30000 Training Loss: 0.08718466758728027\n",
      "Epoch 4633/30000 Training Loss: 0.08623474091291428\n",
      "Epoch 4634/30000 Training Loss: 0.09172281622886658\n",
      "Epoch 4635/30000 Training Loss: 0.10671964287757874\n",
      "Epoch 4636/30000 Training Loss: 0.09411170333623886\n",
      "Epoch 4637/30000 Training Loss: 0.07701212912797928\n",
      "Epoch 4638/30000 Training Loss: 0.08687152713537216\n",
      "Epoch 4639/30000 Training Loss: 0.08983204513788223\n",
      "Epoch 4640/30000 Training Loss: 0.10389789193868637\n",
      "Epoch 4640/30000 Validation Loss: 0.10436127334833145\n",
      "Epoch 4641/30000 Training Loss: 0.07599537819623947\n",
      "Epoch 4642/30000 Training Loss: 0.10591115802526474\n",
      "Epoch 4643/30000 Training Loss: 0.07750751823186874\n",
      "Epoch 4644/30000 Training Loss: 0.07814423739910126\n",
      "Epoch 4645/30000 Training Loss: 0.0895707979798317\n",
      "Epoch 4646/30000 Training Loss: 0.08488267660140991\n",
      "Epoch 4647/30000 Training Loss: 0.0880732610821724\n",
      "Epoch 4648/30000 Training Loss: 0.12296947091817856\n",
      "Epoch 4649/30000 Training Loss: 0.07804235816001892\n",
      "Epoch 4650/30000 Training Loss: 0.09257542341947556\n",
      "Epoch 4650/30000 Validation Loss: 0.07690297812223434\n",
      "Epoch 4651/30000 Training Loss: 0.09070906788110733\n",
      "Epoch 4652/30000 Training Loss: 0.09919758886098862\n",
      "Epoch 4653/30000 Training Loss: 0.096478670835495\n",
      "Epoch 4654/30000 Training Loss: 0.08615543693304062\n",
      "Epoch 4655/30000 Training Loss: 0.12147615104913712\n",
      "Epoch 4656/30000 Training Loss: 0.0987597107887268\n",
      "Epoch 4657/30000 Training Loss: 0.09467142075300217\n",
      "Epoch 4658/30000 Training Loss: 0.08517488092184067\n",
      "Epoch 4659/30000 Training Loss: 0.09877646714448929\n",
      "Epoch 4660/30000 Training Loss: 0.09197577089071274\n",
      "Epoch 4660/30000 Validation Loss: 0.09105747193098068\n",
      "Epoch 4661/30000 Training Loss: 0.08312150090932846\n",
      "Epoch 4662/30000 Training Loss: 0.0962863638997078\n",
      "Epoch 4663/30000 Training Loss: 0.11160776764154434\n",
      "Epoch 4664/30000 Training Loss: 0.08949527889490128\n",
      "Epoch 4665/30000 Training Loss: 0.1283397525548935\n",
      "Epoch 4666/30000 Training Loss: 0.07451892644166946\n",
      "Epoch 4667/30000 Training Loss: 0.0954771563410759\n",
      "Epoch 4668/30000 Training Loss: 0.09598783403635025\n",
      "Epoch 4669/30000 Training Loss: 0.09699466079473495\n",
      "Epoch 4670/30000 Training Loss: 0.07182352244853973\n",
      "Epoch 4670/30000 Validation Loss: 0.09894124418497086\n",
      "Epoch 4671/30000 Training Loss: 0.10076343268156052\n",
      "Epoch 4672/30000 Training Loss: 0.091898113489151\n",
      "Epoch 4673/30000 Training Loss: 0.07841823250055313\n",
      "Epoch 4674/30000 Training Loss: 0.07456228882074356\n",
      "Epoch 4675/30000 Training Loss: 0.10108118504285812\n",
      "Epoch 4676/30000 Training Loss: 0.08035475015640259\n",
      "Epoch 4677/30000 Training Loss: 0.09339220076799393\n",
      "Epoch 4678/30000 Training Loss: 0.11304149031639099\n",
      "Epoch 4679/30000 Training Loss: 0.0781278982758522\n",
      "Epoch 4680/30000 Training Loss: 0.07979054003953934\n",
      "Epoch 4680/30000 Validation Loss: 0.09686557203531265\n",
      "Epoch 4681/30000 Training Loss: 0.07166910916566849\n",
      "Epoch 4682/30000 Training Loss: 0.0923444852232933\n",
      "Epoch 4683/30000 Training Loss: 0.08269166946411133\n",
      "Epoch 4684/30000 Training Loss: 0.07859767228364944\n",
      "Epoch 4685/30000 Training Loss: 0.09535815566778183\n",
      "Epoch 4686/30000 Training Loss: 0.10306528955698013\n",
      "Epoch 4687/30000 Training Loss: 0.09545127302408218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4688/30000 Training Loss: 0.10547962784767151\n",
      "Epoch 4689/30000 Training Loss: 0.07738261669874191\n",
      "Epoch 4690/30000 Training Loss: 0.07232203334569931\n",
      "Epoch 4690/30000 Validation Loss: 0.08318036049604416\n",
      "Epoch 4691/30000 Training Loss: 0.1057216227054596\n",
      "Epoch 4692/30000 Training Loss: 0.0750589594244957\n",
      "Epoch 4693/30000 Training Loss: 0.0935300961136818\n",
      "Epoch 4694/30000 Training Loss: 0.0886591449379921\n",
      "Epoch 4695/30000 Training Loss: 0.08530024439096451\n",
      "Epoch 4696/30000 Training Loss: 0.09173782914876938\n",
      "Epoch 4697/30000 Training Loss: 0.11032231897115707\n",
      "Epoch 4698/30000 Training Loss: 0.11321841925382614\n",
      "Epoch 4699/30000 Training Loss: 0.08520331233739853\n",
      "Epoch 4700/30000 Training Loss: 0.1031375303864479\n",
      "Epoch 4700/30000 Validation Loss: 0.10653984546661377\n",
      "Epoch 4701/30000 Training Loss: 0.07274919748306274\n",
      "Epoch 4702/30000 Training Loss: 0.09131184965372086\n",
      "Epoch 4703/30000 Training Loss: 0.06445116549730301\n",
      "Epoch 4704/30000 Training Loss: 0.10663053393363953\n",
      "Epoch 4705/30000 Training Loss: 0.08250713348388672\n",
      "Epoch 4706/30000 Training Loss: 0.08968233317136765\n",
      "Epoch 4707/30000 Training Loss: 0.10561075061559677\n",
      "Epoch 4708/30000 Training Loss: 0.08723855763673782\n",
      "Epoch 4709/30000 Training Loss: 0.09320411086082458\n",
      "Epoch 4710/30000 Training Loss: 0.07229726761579514\n",
      "Epoch 4710/30000 Validation Loss: 0.0638839527964592\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0638839527964592<=============\n",
      "Epoch 4711/30000 Training Loss: 0.08772923797369003\n",
      "Epoch 4712/30000 Training Loss: 0.07429134100675583\n",
      "Epoch 4713/30000 Training Loss: 0.09301120042800903\n",
      "Epoch 4714/30000 Training Loss: 0.08116483688354492\n",
      "Epoch 4715/30000 Training Loss: 0.07966786623001099\n",
      "Epoch 4716/30000 Training Loss: 0.08229997009038925\n",
      "Epoch 4717/30000 Training Loss: 0.07912559062242508\n",
      "Epoch 4718/30000 Training Loss: 0.08222773671150208\n",
      "Epoch 4719/30000 Training Loss: 0.11552298069000244\n",
      "Epoch 4720/30000 Training Loss: 0.07567203789949417\n",
      "Epoch 4720/30000 Validation Loss: 0.06496364623308182\n",
      "Epoch 4721/30000 Training Loss: 0.10423827171325684\n",
      "Epoch 4722/30000 Training Loss: 0.10076438635587692\n",
      "Epoch 4723/30000 Training Loss: 0.06947270780801773\n",
      "Epoch 4724/30000 Training Loss: 0.09796424955129623\n",
      "Epoch 4725/30000 Training Loss: 0.07286062091588974\n",
      "Epoch 4726/30000 Training Loss: 0.08385100960731506\n",
      "Epoch 4727/30000 Training Loss: 0.07781922072172165\n",
      "Epoch 4728/30000 Training Loss: 0.07356833666563034\n",
      "Epoch 4729/30000 Training Loss: 0.09353999048471451\n",
      "Epoch 4730/30000 Training Loss: 0.12205672264099121\n",
      "Epoch 4730/30000 Validation Loss: 0.08680801838636398\n",
      "Epoch 4731/30000 Training Loss: 0.08231434971094131\n",
      "Epoch 4732/30000 Training Loss: 0.07536882907152176\n",
      "Epoch 4733/30000 Training Loss: 0.09413635730743408\n",
      "Epoch 4734/30000 Training Loss: 0.11184445768594742\n",
      "Epoch 4735/30000 Training Loss: 0.08487530797719955\n",
      "Epoch 4736/30000 Training Loss: 0.10995239019393921\n",
      "Epoch 4737/30000 Training Loss: 0.09817422181367874\n",
      "Epoch 4738/30000 Training Loss: 0.07520044595003128\n",
      "Epoch 4739/30000 Training Loss: 0.09981431812047958\n",
      "Epoch 4740/30000 Training Loss: 0.07860364764928818\n",
      "Epoch 4740/30000 Validation Loss: 0.09219919890165329\n",
      "Epoch 4741/30000 Training Loss: 0.08851955085992813\n",
      "Epoch 4742/30000 Training Loss: 0.08502315729856491\n",
      "Epoch 4743/30000 Training Loss: 0.08792677521705627\n",
      "Epoch 4744/30000 Training Loss: 0.11148521304130554\n",
      "Epoch 4745/30000 Training Loss: 0.06560114771127701\n",
      "Epoch 4746/30000 Training Loss: 0.07591035217046738\n",
      "Epoch 4747/30000 Training Loss: 0.06664667278528214\n",
      "Epoch 4748/30000 Training Loss: 0.08889749646186829\n",
      "Epoch 4749/30000 Training Loss: 0.08290129154920578\n",
      "Epoch 4750/30000 Training Loss: 0.07501497119665146\n",
      "Epoch 4750/30000 Validation Loss: 0.10681337863206863\n",
      "Epoch 4751/30000 Training Loss: 0.10790201276540756\n",
      "Epoch 4752/30000 Training Loss: 0.06937416642904282\n",
      "Epoch 4753/30000 Training Loss: 0.09801805764436722\n",
      "Epoch 4754/30000 Training Loss: 0.09414380043745041\n",
      "Epoch 4755/30000 Training Loss: 0.09378030896186829\n",
      "Epoch 4756/30000 Training Loss: 0.10263525694608688\n",
      "Epoch 4757/30000 Training Loss: 0.09782720357179642\n",
      "Epoch 4758/30000 Training Loss: 0.08756565302610397\n",
      "Epoch 4759/30000 Training Loss: 0.07919182628393173\n",
      "Epoch 4760/30000 Training Loss: 0.0637611523270607\n",
      "Epoch 4760/30000 Validation Loss: 0.08379145711660385\n",
      "Epoch 4761/30000 Training Loss: 0.07618332654237747\n",
      "Epoch 4762/30000 Training Loss: 0.09440044313669205\n",
      "Epoch 4763/30000 Training Loss: 0.07453706115484238\n",
      "Epoch 4764/30000 Training Loss: 0.08471191674470901\n",
      "Epoch 4765/30000 Training Loss: 0.09298274666070938\n",
      "Epoch 4766/30000 Training Loss: 0.10954710841178894\n",
      "Epoch 4767/30000 Training Loss: 0.0983206257224083\n",
      "Epoch 4768/30000 Training Loss: 0.07772479206323624\n",
      "Epoch 4769/30000 Training Loss: 0.08414804190397263\n",
      "Epoch 4770/30000 Training Loss: 0.08791366964578629\n",
      "Epoch 4770/30000 Validation Loss: 0.09859886765480042\n",
      "Epoch 4771/30000 Training Loss: 0.07089398056268692\n",
      "Epoch 4772/30000 Training Loss: 0.0909825935959816\n",
      "Epoch 4773/30000 Training Loss: 0.10503125190734863\n",
      "Epoch 4774/30000 Training Loss: 0.0931958332657814\n",
      "Epoch 4775/30000 Training Loss: 0.09925112873315811\n",
      "Epoch 4776/30000 Training Loss: 0.08360514044761658\n",
      "Epoch 4777/30000 Training Loss: 0.10110863298177719\n",
      "Epoch 4778/30000 Training Loss: 0.08892796188592911\n",
      "Epoch 4779/30000 Training Loss: 0.0816013291478157\n",
      "Epoch 4780/30000 Training Loss: 0.09670909494161606\n",
      "Epoch 4780/30000 Validation Loss: 0.08054820448160172\n",
      "Epoch 4781/30000 Training Loss: 0.0904650017619133\n",
      "Epoch 4782/30000 Training Loss: 0.07990741729736328\n",
      "Epoch 4783/30000 Training Loss: 0.0826798528432846\n",
      "Epoch 4784/30000 Training Loss: 0.08665195107460022\n",
      "Epoch 4785/30000 Training Loss: 0.07644882798194885\n",
      "Epoch 4786/30000 Training Loss: 0.06968788802623749\n",
      "Epoch 4787/30000 Training Loss: 0.07178949564695358\n",
      "Epoch 4788/30000 Training Loss: 0.08714751154184341\n",
      "Epoch 4789/30000 Training Loss: 0.07782083004713058\n",
      "Epoch 4790/30000 Training Loss: 0.10157793015241623\n",
      "Epoch 4790/30000 Validation Loss: 0.09163490682840347\n",
      "Epoch 4791/30000 Training Loss: 0.10309363156557083\n",
      "Epoch 4792/30000 Training Loss: 0.0721975788474083\n",
      "Epoch 4793/30000 Training Loss: 0.10347198694944382\n",
      "Epoch 4794/30000 Training Loss: 0.0787920281291008\n",
      "Epoch 4795/30000 Training Loss: 0.0861603245139122\n",
      "Epoch 4796/30000 Training Loss: 0.08063254505395889\n",
      "Epoch 4797/30000 Training Loss: 0.07857726514339447\n",
      "Epoch 4798/30000 Training Loss: 0.09491989761590958\n",
      "Epoch 4799/30000 Training Loss: 0.09742770344018936\n",
      "Epoch 4800/30000 Training Loss: 0.09599379450082779\n",
      "Epoch 4800/30000 Validation Loss: 0.10753518342971802\n",
      "Epoch 4801/30000 Training Loss: 0.10086939483880997\n",
      "Epoch 4802/30000 Training Loss: 0.10254693776369095\n",
      "Epoch 4803/30000 Training Loss: 0.09795580059289932\n",
      "Epoch 4804/30000 Training Loss: 0.07476412504911423\n",
      "Epoch 4805/30000 Training Loss: 0.0913706049323082\n",
      "Epoch 4806/30000 Training Loss: 0.11896452307701111\n",
      "Epoch 4807/30000 Training Loss: 0.08028621226549149\n",
      "Epoch 4808/30000 Training Loss: 0.09881218522787094\n",
      "Epoch 4809/30000 Training Loss: 0.10409816354513168\n",
      "Epoch 4810/30000 Training Loss: 0.07829427719116211\n",
      "Epoch 4810/30000 Validation Loss: 0.07706905156373978\n",
      "Epoch 4811/30000 Training Loss: 0.10254546999931335\n",
      "Epoch 4812/30000 Training Loss: 0.10455167293548584\n",
      "Epoch 4813/30000 Training Loss: 0.09949285537004471\n",
      "Epoch 4814/30000 Training Loss: 0.1039748564362526\n",
      "Epoch 4815/30000 Training Loss: 0.08425190299749374\n",
      "Epoch 4816/30000 Training Loss: 0.09227477759122849\n",
      "Epoch 4817/30000 Training Loss: 0.07746680825948715\n",
      "Epoch 4818/30000 Training Loss: 0.09908533841371536\n",
      "Epoch 4819/30000 Training Loss: 0.076153963804245\n",
      "Epoch 4820/30000 Training Loss: 0.08480717986822128\n",
      "Epoch 4820/30000 Validation Loss: 0.08413108438253403\n",
      "Epoch 4821/30000 Training Loss: 0.0939645767211914\n",
      "Epoch 4822/30000 Training Loss: 0.0614161491394043\n",
      "Epoch 4823/30000 Training Loss: 0.07598970085382462\n",
      "Epoch 4824/30000 Training Loss: 0.0746845081448555\n",
      "Epoch 4825/30000 Training Loss: 0.10083988308906555\n",
      "Epoch 4826/30000 Training Loss: 0.10619854182004929\n",
      "Epoch 4827/30000 Training Loss: 0.1058628186583519\n",
      "Epoch 4828/30000 Training Loss: 0.08357656002044678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4829/30000 Training Loss: 0.08009574562311172\n",
      "Epoch 4830/30000 Training Loss: 0.0687042847275734\n",
      "Epoch 4830/30000 Validation Loss: 0.08831808716058731\n",
      "Epoch 4831/30000 Training Loss: 0.07982101291418076\n",
      "Epoch 4832/30000 Training Loss: 0.0820857360959053\n",
      "Epoch 4833/30000 Training Loss: 0.08942537754774094\n",
      "Epoch 4834/30000 Training Loss: 0.11377198249101639\n",
      "Epoch 4835/30000 Training Loss: 0.07776838541030884\n",
      "Epoch 4836/30000 Training Loss: 0.09118316322565079\n",
      "Epoch 4837/30000 Training Loss: 0.09823763370513916\n",
      "Epoch 4838/30000 Training Loss: 0.08591944724321365\n",
      "Epoch 4839/30000 Training Loss: 0.09856618195772171\n",
      "Epoch 4840/30000 Training Loss: 0.09248871356248856\n",
      "Epoch 4840/30000 Validation Loss: 0.08091283589601517\n",
      "Epoch 4841/30000 Training Loss: 0.10091209411621094\n",
      "Epoch 4842/30000 Training Loss: 0.08225755393505096\n",
      "Epoch 4843/30000 Training Loss: 0.06821497529745102\n",
      "Epoch 4844/30000 Training Loss: 0.0661521777510643\n",
      "Epoch 4845/30000 Training Loss: 0.08115174621343613\n",
      "Epoch 4846/30000 Training Loss: 0.08597996830940247\n",
      "Epoch 4847/30000 Training Loss: 0.09716064482927322\n",
      "Epoch 4848/30000 Training Loss: 0.08259886503219604\n",
      "Epoch 4849/30000 Training Loss: 0.07752756029367447\n",
      "Epoch 4850/30000 Training Loss: 0.10453591495752335\n",
      "Epoch 4850/30000 Validation Loss: 0.1121332049369812\n",
      "Epoch 4851/30000 Training Loss: 0.09212008118629456\n",
      "Epoch 4852/30000 Training Loss: 0.0800272598862648\n",
      "Epoch 4853/30000 Training Loss: 0.07743099331855774\n",
      "Epoch 4854/30000 Training Loss: 0.10402508825063705\n",
      "Epoch 4855/30000 Training Loss: 0.09456034749746323\n",
      "Epoch 4856/30000 Training Loss: 0.09249043464660645\n",
      "Epoch 4857/30000 Training Loss: 0.07381203770637512\n",
      "Epoch 4858/30000 Training Loss: 0.07925935834646225\n",
      "Epoch 4859/30000 Training Loss: 0.10034194588661194\n",
      "Epoch 4860/30000 Training Loss: 0.07340645790100098\n",
      "Epoch 4860/30000 Validation Loss: 0.09455183893442154\n",
      "Epoch 4861/30000 Training Loss: 0.07496800273656845\n",
      "Epoch 4862/30000 Training Loss: 0.08813151717185974\n",
      "Epoch 4863/30000 Training Loss: 0.0628482773900032\n",
      "Epoch 4864/30000 Training Loss: 0.11452999711036682\n",
      "Epoch 4865/30000 Training Loss: 0.09850241988897324\n",
      "Epoch 4866/30000 Training Loss: 0.07260295748710632\n",
      "Epoch 4867/30000 Training Loss: 0.08160873502492905\n",
      "Epoch 4868/30000 Training Loss: 0.11864230036735535\n",
      "Epoch 4869/30000 Training Loss: 0.07849159091711044\n",
      "Epoch 4870/30000 Training Loss: 0.10021397471427917\n",
      "Epoch 4870/30000 Validation Loss: 0.11506381630897522\n",
      "Epoch 4871/30000 Training Loss: 0.10227895528078079\n",
      "Epoch 4872/30000 Training Loss: 0.1049485132098198\n",
      "Epoch 4873/30000 Training Loss: 0.08790700882673264\n",
      "Epoch 4874/30000 Training Loss: 0.09806445986032486\n",
      "Epoch 4875/30000 Training Loss: 0.08339536190032959\n",
      "Epoch 4876/30000 Training Loss: 0.08932247012853622\n",
      "Epoch 4877/30000 Training Loss: 0.08569559454917908\n",
      "Epoch 4878/30000 Training Loss: 0.09576290845870972\n",
      "Epoch 4879/30000 Training Loss: 0.0971386656165123\n",
      "Epoch 4880/30000 Training Loss: 0.07574973255395889\n",
      "Epoch 4880/30000 Validation Loss: 0.08736982941627502\n",
      "Epoch 4881/30000 Training Loss: 0.10994065552949905\n",
      "Epoch 4882/30000 Training Loss: 0.08301478624343872\n",
      "Epoch 4883/30000 Training Loss: 0.09844081848859787\n",
      "Epoch 4884/30000 Training Loss: 0.09069767594337463\n",
      "Epoch 4885/30000 Training Loss: 0.10664539784193039\n",
      "Epoch 4886/30000 Training Loss: 0.08244458585977554\n",
      "Epoch 4887/30000 Training Loss: 0.08201271295547485\n",
      "Epoch 4888/30000 Training Loss: 0.09547358751296997\n",
      "Epoch 4889/30000 Training Loss: 0.0712571069598198\n",
      "Epoch 4890/30000 Training Loss: 0.09512647986412048\n",
      "Epoch 4890/30000 Validation Loss: 0.11178980022668839\n",
      "Epoch 4891/30000 Training Loss: 0.07725564390420914\n",
      "Epoch 4892/30000 Training Loss: 0.09382105618715286\n",
      "Epoch 4893/30000 Training Loss: 0.10113217681646347\n",
      "Epoch 4894/30000 Training Loss: 0.06736049801111221\n",
      "Epoch 4895/30000 Training Loss: 0.09729942679405212\n",
      "Epoch 4896/30000 Training Loss: 0.0745825543999672\n",
      "Epoch 4897/30000 Training Loss: 0.09642696380615234\n",
      "Epoch 4898/30000 Training Loss: 0.08505172282457352\n",
      "Epoch 4899/30000 Training Loss: 0.10846468806266785\n",
      "Epoch 4900/30000 Training Loss: 0.10031146556138992\n",
      "Epoch 4900/30000 Validation Loss: 0.06842824071645737\n",
      "Epoch 4901/30000 Training Loss: 0.0676146149635315\n",
      "Epoch 4902/30000 Training Loss: 0.08043570071458817\n",
      "Epoch 4903/30000 Training Loss: 0.06235625967383385\n",
      "Epoch 4904/30000 Training Loss: 0.07457057386636734\n",
      "Epoch 4905/30000 Training Loss: 0.07356223464012146\n",
      "Epoch 4906/30000 Training Loss: 0.10295794159173965\n",
      "Epoch 4907/30000 Training Loss: 0.07137639075517654\n",
      "Epoch 4908/30000 Training Loss: 0.1017339751124382\n",
      "Epoch 4909/30000 Training Loss: 0.07197605818510056\n",
      "Epoch 4910/30000 Training Loss: 0.07495781779289246\n",
      "Epoch 4910/30000 Validation Loss: 0.08443994075059891\n",
      "Epoch 4911/30000 Training Loss: 0.09849966317415237\n",
      "Epoch 4912/30000 Training Loss: 0.08932182192802429\n",
      "Epoch 4913/30000 Training Loss: 0.07826169580221176\n",
      "Epoch 4914/30000 Training Loss: 0.10212374478578568\n",
      "Epoch 4915/30000 Training Loss: 0.12144698947668076\n",
      "Epoch 4916/30000 Training Loss: 0.06438815593719482\n",
      "Epoch 4917/30000 Training Loss: 0.0825120136141777\n",
      "Epoch 4918/30000 Training Loss: 0.08650106191635132\n",
      "Epoch 4919/30000 Training Loss: 0.10278352349996567\n",
      "Epoch 4920/30000 Training Loss: 0.07016948610544205\n",
      "Epoch 4920/30000 Validation Loss: 0.09275772422552109\n",
      "Epoch 4921/30000 Training Loss: 0.10537475347518921\n",
      "Epoch 4922/30000 Training Loss: 0.08255881816148758\n",
      "Epoch 4923/30000 Training Loss: 0.10479781031608582\n",
      "Epoch 4924/30000 Training Loss: 0.06595268845558167\n",
      "Epoch 4925/30000 Training Loss: 0.08689019829034805\n",
      "Epoch 4926/30000 Training Loss: 0.1052875891327858\n",
      "Epoch 4927/30000 Training Loss: 0.09447156637907028\n",
      "Epoch 4928/30000 Training Loss: 0.09526901692152023\n",
      "Epoch 4929/30000 Training Loss: 0.08368922024965286\n",
      "Epoch 4930/30000 Training Loss: 0.0759824886918068\n",
      "Epoch 4930/30000 Validation Loss: 0.08851443976163864\n",
      "Epoch 4931/30000 Training Loss: 0.07128816843032837\n",
      "Epoch 4932/30000 Training Loss: 0.0900585874915123\n",
      "Epoch 4933/30000 Training Loss: 0.10627442598342896\n",
      "Epoch 4934/30000 Training Loss: 0.08796734362840652\n",
      "Epoch 4935/30000 Training Loss: 0.0813661441206932\n",
      "Epoch 4936/30000 Training Loss: 0.08702931553125381\n",
      "Epoch 4937/30000 Training Loss: 0.08611229062080383\n",
      "Epoch 4938/30000 Training Loss: 0.08587338775396347\n",
      "Epoch 4939/30000 Training Loss: 0.07663455605506897\n",
      "Epoch 4940/30000 Training Loss: 0.1180109977722168\n",
      "Epoch 4940/30000 Validation Loss: 0.07775264233350754\n",
      "Epoch 4941/30000 Training Loss: 0.10637237876653671\n",
      "Epoch 4942/30000 Training Loss: 0.08998282998800278\n",
      "Epoch 4943/30000 Training Loss: 0.08869317919015884\n",
      "Epoch 4944/30000 Training Loss: 0.0890752449631691\n",
      "Epoch 4945/30000 Training Loss: 0.08472918719053268\n",
      "Epoch 4946/30000 Training Loss: 0.08647773414850235\n",
      "Epoch 4947/30000 Training Loss: 0.08206375688314438\n",
      "Epoch 4948/30000 Training Loss: 0.08208673447370529\n",
      "Epoch 4949/30000 Training Loss: 0.07283095270395279\n",
      "Epoch 4950/30000 Training Loss: 0.12240352481603622\n",
      "Epoch 4950/30000 Validation Loss: 0.07280992716550827\n",
      "Epoch 4951/30000 Training Loss: 0.11891153454780579\n",
      "Epoch 4952/30000 Training Loss: 0.07370088249444962\n",
      "Epoch 4953/30000 Training Loss: 0.08830201625823975\n",
      "Epoch 4954/30000 Training Loss: 0.1167672649025917\n",
      "Epoch 4955/30000 Training Loss: 0.08173009008169174\n",
      "Epoch 4956/30000 Training Loss: 0.09961120039224625\n",
      "Epoch 4957/30000 Training Loss: 0.08298718184232712\n",
      "Epoch 4958/30000 Training Loss: 0.12374880164861679\n",
      "Epoch 4959/30000 Training Loss: 0.0757022276520729\n",
      "Epoch 4960/30000 Training Loss: 0.1041853204369545\n",
      "Epoch 4960/30000 Validation Loss: 0.07331901788711548\n",
      "Epoch 4961/30000 Training Loss: 0.09831923246383667\n",
      "Epoch 4962/30000 Training Loss: 0.07401955872774124\n",
      "Epoch 4963/30000 Training Loss: 0.09687036275863647\n",
      "Epoch 4964/30000 Training Loss: 0.0956219807267189\n",
      "Epoch 4965/30000 Training Loss: 0.0839986726641655\n",
      "Epoch 4966/30000 Training Loss: 0.0860220417380333\n",
      "Epoch 4967/30000 Training Loss: 0.08923088759183884\n",
      "Epoch 4968/30000 Training Loss: 0.10522434860467911\n",
      "Epoch 4969/30000 Training Loss: 0.10216894745826721\n",
      "Epoch 4970/30000 Training Loss: 0.10300412029027939\n",
      "Epoch 4970/30000 Validation Loss: 0.0823475643992424\n",
      "Epoch 4971/30000 Training Loss: 0.09958777576684952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4972/30000 Training Loss: 0.08352958410978317\n",
      "Epoch 4973/30000 Training Loss: 0.10523656010627747\n",
      "Epoch 4974/30000 Training Loss: 0.07812657952308655\n",
      "Epoch 4975/30000 Training Loss: 0.1073017418384552\n",
      "Epoch 4976/30000 Training Loss: 0.0669010654091835\n",
      "Epoch 4977/30000 Training Loss: 0.10946323722600937\n",
      "Epoch 4978/30000 Training Loss: 0.0938459038734436\n",
      "Epoch 4979/30000 Training Loss: 0.09349682182073593\n",
      "Epoch 4980/30000 Training Loss: 0.09928005188703537\n",
      "Epoch 4980/30000 Validation Loss: 0.09186583012342453\n",
      "Epoch 4981/30000 Training Loss: 0.09949839860200882\n",
      "Epoch 4982/30000 Training Loss: 0.09241217374801636\n",
      "Epoch 4983/30000 Training Loss: 0.08278185874223709\n",
      "Epoch 4984/30000 Training Loss: 0.06811690330505371\n",
      "Epoch 4985/30000 Training Loss: 0.10223627090454102\n",
      "Epoch 4986/30000 Training Loss: 0.093085378408432\n",
      "Epoch 4987/30000 Training Loss: 0.07279930263757706\n",
      "Epoch 4988/30000 Training Loss: 0.09830973297357559\n",
      "Epoch 4989/30000 Training Loss: 0.08161160349845886\n",
      "Epoch 4990/30000 Training Loss: 0.07646538317203522\n",
      "Epoch 4990/30000 Validation Loss: 0.0991806611418724\n",
      "Epoch 4991/30000 Training Loss: 0.09524577856063843\n",
      "Epoch 4992/30000 Training Loss: 0.07800260931253433\n",
      "Epoch 4993/30000 Training Loss: 0.0844668447971344\n",
      "Epoch 4994/30000 Training Loss: 0.08403045684099197\n",
      "Epoch 4995/30000 Training Loss: 0.08650630712509155\n",
      "Epoch 4996/30000 Training Loss: 0.08326436579227448\n",
      "Epoch 4997/30000 Training Loss: 0.11691651493310928\n",
      "Epoch 4998/30000 Training Loss: 0.10304945707321167\n",
      "Epoch 4999/30000 Training Loss: 0.09056803584098816\n",
      "Epoch 5000/30000 Training Loss: 0.08545740693807602\n",
      "Epoch 5000/30000 Validation Loss: 0.10314268618822098\n",
      "Epoch 5001/30000 Training Loss: 0.08904069662094116\n",
      "Epoch 5002/30000 Training Loss: 0.09384561330080032\n",
      "Epoch 5003/30000 Training Loss: 0.11188841611146927\n",
      "Epoch 5004/30000 Training Loss: 0.09026388078927994\n",
      "Epoch 5005/30000 Training Loss: 0.11791735887527466\n",
      "Epoch 5006/30000 Training Loss: 0.09794259816408157\n",
      "Epoch 5007/30000 Training Loss: 0.09597166627645493\n",
      "Epoch 5008/30000 Training Loss: 0.0705716535449028\n",
      "Epoch 5009/30000 Training Loss: 0.10495448112487793\n",
      "Epoch 5010/30000 Training Loss: 0.09769794344902039\n",
      "Epoch 5010/30000 Validation Loss: 0.11832014471292496\n",
      "Epoch 5011/30000 Training Loss: 0.0956377387046814\n",
      "Epoch 5012/30000 Training Loss: 0.08739569783210754\n",
      "Epoch 5013/30000 Training Loss: 0.0858672633767128\n",
      "Epoch 5014/30000 Training Loss: 0.08898518234491348\n",
      "Epoch 5015/30000 Training Loss: 0.07446227967739105\n",
      "Epoch 5016/30000 Training Loss: 0.08001092076301575\n",
      "Epoch 5017/30000 Training Loss: 0.10440567880868912\n",
      "Epoch 5018/30000 Training Loss: 0.08360419422388077\n",
      "Epoch 5019/30000 Training Loss: 0.08857313543558121\n",
      "Epoch 5020/30000 Training Loss: 0.08725481480360031\n",
      "Epoch 5020/30000 Validation Loss: 0.07558775693178177\n",
      "Epoch 5021/30000 Training Loss: 0.08929929882287979\n",
      "Epoch 5022/30000 Training Loss: 0.07722841948270798\n",
      "Epoch 5023/30000 Training Loss: 0.07912790030241013\n",
      "Epoch 5024/30000 Training Loss: 0.08435460180044174\n",
      "Epoch 5025/30000 Training Loss: 0.10691756755113602\n",
      "Epoch 5026/30000 Training Loss: 0.08506201952695847\n",
      "Epoch 5027/30000 Training Loss: 0.0803731232881546\n",
      "Epoch 5028/30000 Training Loss: 0.08866766840219498\n",
      "Epoch 5029/30000 Training Loss: 0.08375239372253418\n",
      "Epoch 5030/30000 Training Loss: 0.06704646348953247\n",
      "Epoch 5030/30000 Validation Loss: 0.0915255919098854\n",
      "Epoch 5031/30000 Training Loss: 0.08766403794288635\n",
      "Epoch 5032/30000 Training Loss: 0.08732866495847702\n",
      "Epoch 5033/30000 Training Loss: 0.09140092879533768\n",
      "Epoch 5034/30000 Training Loss: 0.10070011764764786\n",
      "Epoch 5035/30000 Training Loss: 0.12249234318733215\n",
      "Epoch 5036/30000 Training Loss: 0.08413547277450562\n",
      "Epoch 5037/30000 Training Loss: 0.09077000617980957\n",
      "Epoch 5038/30000 Training Loss: 0.08852845430374146\n",
      "Epoch 5039/30000 Training Loss: 0.08081459999084473\n",
      "Epoch 5040/30000 Training Loss: 0.07385440915822983\n",
      "Epoch 5040/30000 Validation Loss: 0.08489280194044113\n",
      "Epoch 5041/30000 Training Loss: 0.0838199332356453\n",
      "Epoch 5042/30000 Training Loss: 0.082667775452137\n",
      "Epoch 5043/30000 Training Loss: 0.07979240268468857\n",
      "Epoch 5044/30000 Training Loss: 0.07550854235887527\n",
      "Epoch 5045/30000 Training Loss: 0.07824525982141495\n",
      "Epoch 5046/30000 Training Loss: 0.08199933171272278\n",
      "Epoch 5047/30000 Training Loss: 0.08514372259378433\n",
      "Epoch 5048/30000 Training Loss: 0.09551850706338882\n",
      "Epoch 5049/30000 Training Loss: 0.0816936120390892\n",
      "Epoch 5050/30000 Training Loss: 0.08005810528993607\n",
      "Epoch 5050/30000 Validation Loss: 0.1119256317615509\n",
      "Epoch 5051/30000 Training Loss: 0.06779634952545166\n",
      "Epoch 5052/30000 Training Loss: 0.07480435818433762\n",
      "Epoch 5053/30000 Training Loss: 0.0876440480351448\n",
      "Epoch 5054/30000 Training Loss: 0.12458667159080505\n",
      "Epoch 5055/30000 Training Loss: 0.09069972485303879\n",
      "Epoch 5056/30000 Training Loss: 0.10588342696428299\n",
      "Epoch 5057/30000 Training Loss: 0.10079074651002884\n",
      "Epoch 5058/30000 Training Loss: 0.08596637845039368\n",
      "Epoch 5059/30000 Training Loss: 0.0913999080657959\n",
      "Epoch 5060/30000 Training Loss: 0.09302268177270889\n",
      "Epoch 5060/30000 Validation Loss: 0.08386219292879105\n",
      "Epoch 5061/30000 Training Loss: 0.09986311942338943\n",
      "Epoch 5062/30000 Training Loss: 0.101833276450634\n",
      "Epoch 5063/30000 Training Loss: 0.07567469030618668\n",
      "Epoch 5064/30000 Training Loss: 0.10170766711235046\n",
      "Epoch 5065/30000 Training Loss: 0.08427005261182785\n",
      "Epoch 5066/30000 Training Loss: 0.0812998041510582\n",
      "Epoch 5067/30000 Training Loss: 0.0717383325099945\n",
      "Epoch 5068/30000 Training Loss: 0.10535553842782974\n",
      "Epoch 5069/30000 Training Loss: 0.0810951292514801\n",
      "Epoch 5070/30000 Training Loss: 0.09611255675554276\n",
      "Epoch 5070/30000 Validation Loss: 0.1037851944565773\n",
      "Epoch 5071/30000 Training Loss: 0.08567783236503601\n",
      "Epoch 5072/30000 Training Loss: 0.09825396537780762\n",
      "Epoch 5073/30000 Training Loss: 0.07519631832838058\n",
      "Epoch 5074/30000 Training Loss: 0.09439457207918167\n",
      "Epoch 5075/30000 Training Loss: 0.08613651990890503\n",
      "Epoch 5076/30000 Training Loss: 0.09557072073221207\n",
      "Epoch 5077/30000 Training Loss: 0.07739120721817017\n",
      "Epoch 5078/30000 Training Loss: 0.08081410080194473\n",
      "Epoch 5079/30000 Training Loss: 0.08079399913549423\n",
      "Epoch 5080/30000 Training Loss: 0.06736629456281662\n",
      "Epoch 5080/30000 Validation Loss: 0.09139107912778854\n",
      "Epoch 5081/30000 Training Loss: 0.08030020445585251\n",
      "Epoch 5082/30000 Training Loss: 0.10326752811670303\n",
      "Epoch 5083/30000 Training Loss: 0.08281686156988144\n",
      "Epoch 5084/30000 Training Loss: 0.09260612726211548\n",
      "Epoch 5085/30000 Training Loss: 0.08873904496431351\n",
      "Epoch 5086/30000 Training Loss: 0.09052953869104385\n",
      "Epoch 5087/30000 Training Loss: 0.10148359090089798\n",
      "Epoch 5088/30000 Training Loss: 0.08523503690958023\n",
      "Epoch 5089/30000 Training Loss: 0.08693606406450272\n",
      "Epoch 5090/30000 Training Loss: 0.0877440795302391\n",
      "Epoch 5090/30000 Validation Loss: 0.10030164569616318\n",
      "Epoch 5091/30000 Training Loss: 0.1141076385974884\n",
      "Epoch 5092/30000 Training Loss: 0.08089554309844971\n",
      "Epoch 5093/30000 Training Loss: 0.1042550802230835\n",
      "Epoch 5094/30000 Training Loss: 0.06900770962238312\n",
      "Epoch 5095/30000 Training Loss: 0.07663458585739136\n",
      "Epoch 5096/30000 Training Loss: 0.09041161090135574\n",
      "Epoch 5097/30000 Training Loss: 0.10335725545883179\n",
      "Epoch 5098/30000 Training Loss: 0.08348094671964645\n",
      "Epoch 5099/30000 Training Loss: 0.09045875072479248\n",
      "Epoch 5100/30000 Training Loss: 0.0976107195019722\n",
      "Epoch 5100/30000 Validation Loss: 0.1162392869591713\n",
      "Epoch 5101/30000 Training Loss: 0.10587907582521439\n",
      "Epoch 5102/30000 Training Loss: 0.08279640972614288\n",
      "Epoch 5103/30000 Training Loss: 0.0705636590719223\n",
      "Epoch 5104/30000 Training Loss: 0.09565945714712143\n",
      "Epoch 5105/30000 Training Loss: 0.08619039505720139\n",
      "Epoch 5106/30000 Training Loss: 0.07459206879138947\n",
      "Epoch 5107/30000 Training Loss: 0.09886794537305832\n",
      "Epoch 5108/30000 Training Loss: 0.11297594755887985\n",
      "Epoch 5109/30000 Training Loss: 0.0911681056022644\n",
      "Epoch 5110/30000 Training Loss: 0.1002211645245552\n",
      "Epoch 5110/30000 Validation Loss: 0.10192910581827164\n",
      "Epoch 5111/30000 Training Loss: 0.10741012543439865\n",
      "Epoch 5112/30000 Training Loss: 0.07565190643072128\n",
      "Epoch 5113/30000 Training Loss: 0.11518716812133789\n",
      "Epoch 5114/30000 Training Loss: 0.10776170343160629\n",
      "Epoch 5115/30000 Training Loss: 0.11411704868078232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5116/30000 Training Loss: 0.08874690532684326\n",
      "Epoch 5117/30000 Training Loss: 0.07821958512067795\n",
      "Epoch 5118/30000 Training Loss: 0.08410636335611343\n",
      "Epoch 5119/30000 Training Loss: 0.08310826867818832\n",
      "Epoch 5120/30000 Training Loss: 0.07964310795068741\n",
      "Epoch 5120/30000 Validation Loss: 0.09587737172842026\n",
      "Epoch 5121/30000 Training Loss: 0.08887936919927597\n",
      "Epoch 5122/30000 Training Loss: 0.09548517316579819\n",
      "Epoch 5123/30000 Training Loss: 0.10138998180627823\n",
      "Epoch 5124/30000 Training Loss: 0.08522230386734009\n",
      "Epoch 5125/30000 Training Loss: 0.09321106225252151\n",
      "Epoch 5126/30000 Training Loss: 0.08626613020896912\n",
      "Epoch 5127/30000 Training Loss: 0.11049306392669678\n",
      "Epoch 5128/30000 Training Loss: 0.10058820992708206\n",
      "Epoch 5129/30000 Training Loss: 0.09247288852930069\n",
      "Epoch 5130/30000 Training Loss: 0.11178333312273026\n",
      "Epoch 5130/30000 Validation Loss: 0.09578799456357956\n",
      "Epoch 5131/30000 Training Loss: 0.08565839380025864\n",
      "Epoch 5132/30000 Training Loss: 0.09389512985944748\n",
      "Epoch 5133/30000 Training Loss: 0.13150262832641602\n",
      "Epoch 5134/30000 Training Loss: 0.10315496474504471\n",
      "Epoch 5135/30000 Training Loss: 0.08202718943357468\n",
      "Epoch 5136/30000 Training Loss: 0.08393806219100952\n",
      "Epoch 5137/30000 Training Loss: 0.10377490520477295\n",
      "Epoch 5138/30000 Training Loss: 0.09437943249940872\n",
      "Epoch 5139/30000 Training Loss: 0.07908397912979126\n",
      "Epoch 5140/30000 Training Loss: 0.0748777762055397\n",
      "Epoch 5140/30000 Validation Loss: 0.10086390376091003\n",
      "Epoch 5141/30000 Training Loss: 0.08808832615613937\n",
      "Epoch 5142/30000 Training Loss: 0.06559783965349197\n",
      "Epoch 5143/30000 Training Loss: 0.09027726203203201\n",
      "Epoch 5144/30000 Training Loss: 0.12536096572875977\n",
      "Epoch 5145/30000 Training Loss: 0.08640745282173157\n",
      "Epoch 5146/30000 Training Loss: 0.08646067976951599\n",
      "Epoch 5147/30000 Training Loss: 0.09425231069326401\n",
      "Epoch 5148/30000 Training Loss: 0.08240142464637756\n",
      "Epoch 5149/30000 Training Loss: 0.0817241221666336\n",
      "Epoch 5150/30000 Training Loss: 0.10483264178037643\n",
      "Epoch 5150/30000 Validation Loss: 0.08442763239145279\n",
      "Epoch 5151/30000 Training Loss: 0.08760291337966919\n",
      "Epoch 5152/30000 Training Loss: 0.10385787487030029\n",
      "Epoch 5153/30000 Training Loss: 0.10489198565483093\n",
      "Epoch 5154/30000 Training Loss: 0.08428999036550522\n",
      "Epoch 5155/30000 Training Loss: 0.10761351138353348\n",
      "Epoch 5156/30000 Training Loss: 0.06723097711801529\n",
      "Epoch 5157/30000 Training Loss: 0.07857942581176758\n",
      "Epoch 5158/30000 Training Loss: 0.07274553924798965\n",
      "Epoch 5159/30000 Training Loss: 0.07980581372976303\n",
      "Epoch 5160/30000 Training Loss: 0.07850299030542374\n",
      "Epoch 5160/30000 Validation Loss: 0.07451481372117996\n",
      "Epoch 5161/30000 Training Loss: 0.10640469193458557\n",
      "Epoch 5162/30000 Training Loss: 0.08037006109952927\n",
      "Epoch 5163/30000 Training Loss: 0.07804623246192932\n",
      "Epoch 5164/30000 Training Loss: 0.09202444553375244\n",
      "Epoch 5165/30000 Training Loss: 0.09243615716695786\n",
      "Epoch 5166/30000 Training Loss: 0.08264071494340897\n",
      "Epoch 5167/30000 Training Loss: 0.11812806874513626\n",
      "Epoch 5168/30000 Training Loss: 0.09691909700632095\n",
      "Epoch 5169/30000 Training Loss: 0.08883447200059891\n",
      "Epoch 5170/30000 Training Loss: 0.08918217569589615\n",
      "Epoch 5170/30000 Validation Loss: 0.07429702579975128\n",
      "Epoch 5171/30000 Training Loss: 0.08955714851617813\n",
      "Epoch 5172/30000 Training Loss: 0.09169834852218628\n",
      "Epoch 5173/30000 Training Loss: 0.0943327322602272\n",
      "Epoch 5174/30000 Training Loss: 0.0859542265534401\n",
      "Epoch 5175/30000 Training Loss: 0.08184411376714706\n",
      "Epoch 5176/30000 Training Loss: 0.09953296184539795\n",
      "Epoch 5177/30000 Training Loss: 0.08820638060569763\n",
      "Epoch 5178/30000 Training Loss: 0.07883735001087189\n",
      "Epoch 5179/30000 Training Loss: 0.07819453626871109\n",
      "Epoch 5180/30000 Training Loss: 0.1032186970114708\n",
      "Epoch 5180/30000 Validation Loss: 0.10543984919786453\n",
      "Epoch 5181/30000 Training Loss: 0.07962075620889664\n",
      "Epoch 5182/30000 Training Loss: 0.08047153800725937\n",
      "Epoch 5183/30000 Training Loss: 0.07773735374212265\n",
      "Epoch 5184/30000 Training Loss: 0.09083273261785507\n",
      "Epoch 5185/30000 Training Loss: 0.09507882595062256\n",
      "Epoch 5186/30000 Training Loss: 0.11342862248420715\n",
      "Epoch 5187/30000 Training Loss: 0.08435497432947159\n",
      "Epoch 5188/30000 Training Loss: 0.07949387282133102\n",
      "Epoch 5189/30000 Training Loss: 0.08059632033109665\n",
      "Epoch 5190/30000 Training Loss: 0.10347328335046768\n",
      "Epoch 5190/30000 Validation Loss: 0.09380773454904556\n",
      "Epoch 5191/30000 Training Loss: 0.10067552328109741\n",
      "Epoch 5192/30000 Training Loss: 0.10987898707389832\n",
      "Epoch 5193/30000 Training Loss: 0.0981069803237915\n",
      "Epoch 5194/30000 Training Loss: 0.07154802232980728\n",
      "Epoch 5195/30000 Training Loss: 0.08071696013212204\n",
      "Epoch 5196/30000 Training Loss: 0.09905734658241272\n",
      "Epoch 5197/30000 Training Loss: 0.10650181025266647\n",
      "Epoch 5198/30000 Training Loss: 0.08523187786340714\n",
      "Epoch 5199/30000 Training Loss: 0.09252575784921646\n",
      "Epoch 5200/30000 Training Loss: 0.09399767965078354\n",
      "Epoch 5200/30000 Validation Loss: 0.08012989163398743\n",
      "Epoch 5201/30000 Training Loss: 0.07003582268953323\n",
      "Epoch 5202/30000 Training Loss: 0.10075941681861877\n",
      "Epoch 5203/30000 Training Loss: 0.09054505825042725\n",
      "Epoch 5204/30000 Training Loss: 0.0959395095705986\n",
      "Epoch 5205/30000 Training Loss: 0.08153337985277176\n",
      "Epoch 5206/30000 Training Loss: 0.0879041776061058\n",
      "Epoch 5207/30000 Training Loss: 0.06987554579973221\n",
      "Epoch 5208/30000 Training Loss: 0.08238217234611511\n",
      "Epoch 5209/30000 Training Loss: 0.0759056955575943\n",
      "Epoch 5210/30000 Training Loss: 0.08371611684560776\n",
      "Epoch 5210/30000 Validation Loss: 0.1060885414481163\n",
      "Epoch 5211/30000 Training Loss: 0.08423510193824768\n",
      "Epoch 5212/30000 Training Loss: 0.10480860620737076\n",
      "Epoch 5213/30000 Training Loss: 0.08840682357549667\n",
      "Epoch 5214/30000 Training Loss: 0.10487473756074905\n",
      "Epoch 5215/30000 Training Loss: 0.11301001161336899\n",
      "Epoch 5216/30000 Training Loss: 0.1000271663069725\n",
      "Epoch 5217/30000 Training Loss: 0.09016373008489609\n",
      "Epoch 5218/30000 Training Loss: 0.08148884028196335\n",
      "Epoch 5219/30000 Training Loss: 0.11659882217645645\n",
      "Epoch 5220/30000 Training Loss: 0.08454820513725281\n",
      "Epoch 5220/30000 Validation Loss: 0.08120518177747726\n",
      "Epoch 5221/30000 Training Loss: 0.08957883715629578\n",
      "Epoch 5222/30000 Training Loss: 0.0665479525923729\n",
      "Epoch 5223/30000 Training Loss: 0.07846850901842117\n",
      "Epoch 5224/30000 Training Loss: 0.07598205655813217\n",
      "Epoch 5225/30000 Training Loss: 0.11896564811468124\n",
      "Epoch 5226/30000 Training Loss: 0.08074387162923813\n",
      "Epoch 5227/30000 Training Loss: 0.09036651998758316\n",
      "Epoch 5228/30000 Training Loss: 0.08813038468360901\n",
      "Epoch 5229/30000 Training Loss: 0.07313401252031326\n",
      "Epoch 5230/30000 Training Loss: 0.09871762990951538\n",
      "Epoch 5230/30000 Validation Loss: 0.07141227275133133\n",
      "Epoch 5231/30000 Training Loss: 0.12448700517416\n",
      "Epoch 5232/30000 Training Loss: 0.08645462244749069\n",
      "Epoch 5233/30000 Training Loss: 0.07518350332975388\n",
      "Epoch 5234/30000 Training Loss: 0.0657181441783905\n",
      "Epoch 5235/30000 Training Loss: 0.0817658081650734\n",
      "Epoch 5236/30000 Training Loss: 0.07547128945589066\n",
      "Epoch 5237/30000 Training Loss: 0.07043162733316422\n",
      "Epoch 5238/30000 Training Loss: 0.10193851590156555\n",
      "Epoch 5239/30000 Training Loss: 0.08503731340169907\n",
      "Epoch 5240/30000 Training Loss: 0.06732915341854095\n",
      "Epoch 5240/30000 Validation Loss: 0.09936806559562683\n",
      "Epoch 5241/30000 Training Loss: 0.10807669162750244\n",
      "Epoch 5242/30000 Training Loss: 0.10011223703622818\n",
      "Epoch 5243/30000 Training Loss: 0.10432044416666031\n",
      "Epoch 5244/30000 Training Loss: 0.11116594076156616\n",
      "Epoch 5245/30000 Training Loss: 0.11182773113250732\n",
      "Epoch 5246/30000 Training Loss: 0.09137927740812302\n",
      "Epoch 5247/30000 Training Loss: 0.08770876377820969\n",
      "Epoch 5248/30000 Training Loss: 0.08490749448537827\n",
      "Epoch 5249/30000 Training Loss: 0.08614801615476608\n",
      "Epoch 5250/30000 Training Loss: 0.08535260707139969\n",
      "Epoch 5250/30000 Validation Loss: 0.09104666858911514\n",
      "Epoch 5251/30000 Training Loss: 0.09514296799898148\n",
      "Epoch 5252/30000 Training Loss: 0.08864607661962509\n",
      "Epoch 5253/30000 Training Loss: 0.09034707397222519\n",
      "Epoch 5254/30000 Training Loss: 0.09749253839254379\n",
      "Epoch 5255/30000 Training Loss: 0.09063252061605453\n",
      "Epoch 5256/30000 Training Loss: 0.08416327089071274\n",
      "Epoch 5257/30000 Training Loss: 0.08104114979505539\n",
      "Epoch 5258/30000 Training Loss: 0.07416584342718124\n",
      "Epoch 5259/30000 Training Loss: 0.07311972230672836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5260/30000 Training Loss: 0.08070356398820877\n",
      "Epoch 5260/30000 Validation Loss: 0.08321245014667511\n",
      "Epoch 5261/30000 Training Loss: 0.08553691953420639\n",
      "Epoch 5262/30000 Training Loss: 0.08653493970632553\n",
      "Epoch 5263/30000 Training Loss: 0.09451226145029068\n",
      "Epoch 5264/30000 Training Loss: 0.10253608226776123\n",
      "Epoch 5265/30000 Training Loss: 0.07039594650268555\n",
      "Epoch 5266/30000 Training Loss: 0.10680568218231201\n",
      "Epoch 5267/30000 Training Loss: 0.10023012012243271\n",
      "Epoch 5268/30000 Training Loss: 0.07989300787448883\n",
      "Epoch 5269/30000 Training Loss: 0.08158638328313828\n",
      "Epoch 5270/30000 Training Loss: 0.08617490530014038\n",
      "Epoch 5270/30000 Validation Loss: 0.07629639655351639\n",
      "Epoch 5271/30000 Training Loss: 0.0782284140586853\n",
      "Epoch 5272/30000 Training Loss: 0.08446463942527771\n",
      "Epoch 5273/30000 Training Loss: 0.10544111579656601\n",
      "Epoch 5274/30000 Training Loss: 0.0827656164765358\n",
      "Epoch 5275/30000 Training Loss: 0.10776732116937637\n",
      "Epoch 5276/30000 Training Loss: 0.11269793659448624\n",
      "Epoch 5277/30000 Training Loss: 0.08463048189878464\n",
      "Epoch 5278/30000 Training Loss: 0.12921170890331268\n",
      "Epoch 5279/30000 Training Loss: 0.1028100922703743\n",
      "Epoch 5280/30000 Training Loss: 0.0845101997256279\n",
      "Epoch 5280/30000 Validation Loss: 0.07989103347063065\n",
      "Epoch 5281/30000 Training Loss: 0.08198511600494385\n",
      "Epoch 5282/30000 Training Loss: 0.09369993209838867\n",
      "Epoch 5283/30000 Training Loss: 0.07665999978780746\n",
      "Epoch 5284/30000 Training Loss: 0.08452310413122177\n",
      "Epoch 5285/30000 Training Loss: 0.08452574163675308\n",
      "Epoch 5286/30000 Training Loss: 0.06541678309440613\n",
      "Epoch 5287/30000 Training Loss: 0.09511742740869522\n",
      "Epoch 5288/30000 Training Loss: 0.10301744937896729\n",
      "Epoch 5289/30000 Training Loss: 0.10666226595640182\n",
      "Epoch 5290/30000 Training Loss: 0.09584957361221313\n",
      "Epoch 5290/30000 Validation Loss: 0.08284137398004532\n",
      "Epoch 5291/30000 Training Loss: 0.07759598642587662\n",
      "Epoch 5292/30000 Training Loss: 0.09946047514677048\n",
      "Epoch 5293/30000 Training Loss: 0.07234220951795578\n",
      "Epoch 5294/30000 Training Loss: 0.108242928981781\n",
      "Epoch 5295/30000 Training Loss: 0.09301700443029404\n",
      "Epoch 5296/30000 Training Loss: 0.08568499237298965\n",
      "Epoch 5297/30000 Training Loss: 0.06442311406135559\n",
      "Epoch 5298/30000 Training Loss: 0.07392827421426773\n",
      "Epoch 5299/30000 Training Loss: 0.08030898123979568\n",
      "Epoch 5300/30000 Training Loss: 0.11799388378858566\n",
      "Epoch 5300/30000 Validation Loss: 0.09873166680335999\n",
      "Epoch 5301/30000 Training Loss: 0.09226346015930176\n",
      "Epoch 5302/30000 Training Loss: 0.09636283665895462\n",
      "Epoch 5303/30000 Training Loss: 0.09641867876052856\n",
      "Epoch 5304/30000 Training Loss: 0.09483620524406433\n",
      "Epoch 5305/30000 Training Loss: 0.1165783628821373\n",
      "Epoch 5306/30000 Training Loss: 0.13358932733535767\n",
      "Epoch 5307/30000 Training Loss: 0.08904988318681717\n",
      "Epoch 5308/30000 Training Loss: 0.08481967449188232\n",
      "Epoch 5309/30000 Training Loss: 0.09242590516805649\n",
      "Epoch 5310/30000 Training Loss: 0.08622878044843674\n",
      "Epoch 5310/30000 Validation Loss: 0.09189488738775253\n",
      "Epoch 5311/30000 Training Loss: 0.10199583321809769\n",
      "Epoch 5312/30000 Training Loss: 0.10574626922607422\n",
      "Epoch 5313/30000 Training Loss: 0.08794855326414108\n",
      "Epoch 5314/30000 Training Loss: 0.08657028526067734\n",
      "Epoch 5315/30000 Training Loss: 0.0897490456700325\n",
      "Epoch 5316/30000 Training Loss: 0.08476921916007996\n",
      "Epoch 5317/30000 Training Loss: 0.0794873833656311\n",
      "Epoch 5318/30000 Training Loss: 0.10914041846990585\n",
      "Epoch 5319/30000 Training Loss: 0.07287827879190445\n",
      "Epoch 5320/30000 Training Loss: 0.09259038418531418\n",
      "Epoch 5320/30000 Validation Loss: 0.07134704291820526\n",
      "Epoch 5321/30000 Training Loss: 0.11390531063079834\n",
      "Epoch 5322/30000 Training Loss: 0.09520096331834793\n",
      "Epoch 5323/30000 Training Loss: 0.07340724021196365\n",
      "Epoch 5324/30000 Training Loss: 0.08075570315122604\n",
      "Epoch 5325/30000 Training Loss: 0.09299412369728088\n",
      "Epoch 5326/30000 Training Loss: 0.10332254320383072\n",
      "Epoch 5327/30000 Training Loss: 0.09830086678266525\n",
      "Epoch 5328/30000 Training Loss: 0.09317189455032349\n",
      "Epoch 5329/30000 Training Loss: 0.07887760549783707\n",
      "Epoch 5330/30000 Training Loss: 0.09175783395767212\n",
      "Epoch 5330/30000 Validation Loss: 0.06269780546426773\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06269780546426773<=============\n",
      "Epoch 5331/30000 Training Loss: 0.08450645208358765\n",
      "Epoch 5332/30000 Training Loss: 0.10312005877494812\n",
      "Epoch 5333/30000 Training Loss: 0.08250021934509277\n",
      "Epoch 5334/30000 Training Loss: 0.0795237347483635\n",
      "Epoch 5335/30000 Training Loss: 0.08121335506439209\n",
      "Epoch 5336/30000 Training Loss: 0.07888361066579819\n",
      "Epoch 5337/30000 Training Loss: 0.08327426016330719\n",
      "Epoch 5338/30000 Training Loss: 0.06948091834783554\n",
      "Epoch 5339/30000 Training Loss: 0.1185985580086708\n",
      "Epoch 5340/30000 Training Loss: 0.08817359060049057\n",
      "Epoch 5340/30000 Validation Loss: 0.09071624279022217\n",
      "Epoch 5341/30000 Training Loss: 0.07162683457136154\n",
      "Epoch 5342/30000 Training Loss: 0.09548863768577576\n",
      "Epoch 5343/30000 Training Loss: 0.09271226078271866\n",
      "Epoch 5344/30000 Training Loss: 0.07617627829313278\n",
      "Epoch 5345/30000 Training Loss: 0.0779821053147316\n",
      "Epoch 5346/30000 Training Loss: 0.1086369976401329\n",
      "Epoch 5347/30000 Training Loss: 0.0729346051812172\n",
      "Epoch 5348/30000 Training Loss: 0.07920657843351364\n",
      "Epoch 5349/30000 Training Loss: 0.10107920318841934\n",
      "Epoch 5350/30000 Training Loss: 0.07499080151319504\n",
      "Epoch 5350/30000 Validation Loss: 0.08087668567895889\n",
      "Epoch 5351/30000 Training Loss: 0.06988165527582169\n",
      "Epoch 5352/30000 Training Loss: 0.09656999260187149\n",
      "Epoch 5353/30000 Training Loss: 0.09495207667350769\n",
      "Epoch 5354/30000 Training Loss: 0.08561607450246811\n",
      "Epoch 5355/30000 Training Loss: 0.08099333196878433\n",
      "Epoch 5356/30000 Training Loss: 0.07331034541130066\n",
      "Epoch 5357/30000 Training Loss: 0.09881945699453354\n",
      "Epoch 5358/30000 Training Loss: 0.08696520328521729\n",
      "Epoch 5359/30000 Training Loss: 0.0980534479022026\n",
      "Epoch 5360/30000 Training Loss: 0.0847509503364563\n",
      "Epoch 5360/30000 Validation Loss: 0.11572501808404922\n",
      "Epoch 5361/30000 Training Loss: 0.09465976804494858\n",
      "Epoch 5362/30000 Training Loss: 0.09128958731889725\n",
      "Epoch 5363/30000 Training Loss: 0.08581764250993729\n",
      "Epoch 5364/30000 Training Loss: 0.10239309817552567\n",
      "Epoch 5365/30000 Training Loss: 0.10742846876382828\n",
      "Epoch 5366/30000 Training Loss: 0.06625115126371384\n",
      "Epoch 5367/30000 Training Loss: 0.08633429557085037\n",
      "Epoch 5368/30000 Training Loss: 0.0919855460524559\n",
      "Epoch 5369/30000 Training Loss: 0.07313720881938934\n",
      "Epoch 5370/30000 Training Loss: 0.08148267865180969\n",
      "Epoch 5370/30000 Validation Loss: 0.10979371517896652\n",
      "Epoch 5371/30000 Training Loss: 0.09097594022750854\n",
      "Epoch 5372/30000 Training Loss: 0.08218850940465927\n",
      "Epoch 5373/30000 Training Loss: 0.09680842608213425\n",
      "Epoch 5374/30000 Training Loss: 0.08026186376810074\n",
      "Epoch 5375/30000 Training Loss: 0.1032475009560585\n",
      "Epoch 5376/30000 Training Loss: 0.09957800060510635\n",
      "Epoch 5377/30000 Training Loss: 0.08109772205352783\n",
      "Epoch 5378/30000 Training Loss: 0.08468523621559143\n",
      "Epoch 5379/30000 Training Loss: 0.060882315039634705\n",
      "Epoch 5380/30000 Training Loss: 0.08359348028898239\n",
      "Epoch 5380/30000 Validation Loss: 0.08051755279302597\n",
      "Epoch 5381/30000 Training Loss: 0.07561060041189194\n",
      "Epoch 5382/30000 Training Loss: 0.10670413821935654\n",
      "Epoch 5383/30000 Training Loss: 0.10492223501205444\n",
      "Epoch 5384/30000 Training Loss: 0.07522178441286087\n",
      "Epoch 5385/30000 Training Loss: 0.07955032587051392\n",
      "Epoch 5386/30000 Training Loss: 0.0956052765250206\n",
      "Epoch 5387/30000 Training Loss: 0.07179509848356247\n",
      "Epoch 5388/30000 Training Loss: 0.07097459584474564\n",
      "Epoch 5389/30000 Training Loss: 0.09120861440896988\n",
      "Epoch 5390/30000 Training Loss: 0.07860814779996872\n",
      "Epoch 5390/30000 Validation Loss: 0.11552825570106506\n",
      "Epoch 5391/30000 Training Loss: 0.12923692166805267\n",
      "Epoch 5392/30000 Training Loss: 0.09360257536172867\n",
      "Epoch 5393/30000 Training Loss: 0.08263515681028366\n",
      "Epoch 5394/30000 Training Loss: 0.09653075784444809\n",
      "Epoch 5395/30000 Training Loss: 0.08902671188116074\n",
      "Epoch 5396/30000 Training Loss: 0.10572276264429092\n",
      "Epoch 5397/30000 Training Loss: 0.07947375625371933\n",
      "Epoch 5398/30000 Training Loss: 0.08145274966955185\n",
      "Epoch 5399/30000 Training Loss: 0.09245239943265915\n",
      "Epoch 5400/30000 Training Loss: 0.08787284046411514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5400/30000 Validation Loss: 0.09864765405654907\n",
      "Epoch 5401/30000 Training Loss: 0.07883208990097046\n",
      "Epoch 5402/30000 Training Loss: 0.09060005098581314\n",
      "Epoch 5403/30000 Training Loss: 0.0715705081820488\n",
      "Epoch 5404/30000 Training Loss: 0.09466182440519333\n",
      "Epoch 5405/30000 Training Loss: 0.07732689380645752\n",
      "Epoch 5406/30000 Training Loss: 0.08309947699308395\n",
      "Epoch 5407/30000 Training Loss: 0.07339366525411606\n",
      "Epoch 5408/30000 Training Loss: 0.08509490638971329\n",
      "Epoch 5409/30000 Training Loss: 0.0994519367814064\n",
      "Epoch 5410/30000 Training Loss: 0.12986840307712555\n",
      "Epoch 5410/30000 Validation Loss: 0.07378335297107697\n",
      "Epoch 5411/30000 Training Loss: 0.10279390215873718\n",
      "Epoch 5412/30000 Training Loss: 0.08088915795087814\n",
      "Epoch 5413/30000 Training Loss: 0.0962805449962616\n",
      "Epoch 5414/30000 Training Loss: 0.09265544265508652\n",
      "Epoch 5415/30000 Training Loss: 0.08199617266654968\n",
      "Epoch 5416/30000 Training Loss: 0.09418243169784546\n",
      "Epoch 5417/30000 Training Loss: 0.07838872820138931\n",
      "Epoch 5418/30000 Training Loss: 0.08045510202646255\n",
      "Epoch 5419/30000 Training Loss: 0.09787583351135254\n",
      "Epoch 5420/30000 Training Loss: 0.0923757553100586\n",
      "Epoch 5420/30000 Validation Loss: 0.09863824397325516\n",
      "Epoch 5421/30000 Training Loss: 0.07944109290838242\n",
      "Epoch 5422/30000 Training Loss: 0.09498423337936401\n",
      "Epoch 5423/30000 Training Loss: 0.09145615249872208\n",
      "Epoch 5424/30000 Training Loss: 0.06541605293750763\n",
      "Epoch 5425/30000 Training Loss: 0.07439320534467697\n",
      "Epoch 5426/30000 Training Loss: 0.10365530103445053\n",
      "Epoch 5427/30000 Training Loss: 0.10389295965433121\n",
      "Epoch 5428/30000 Training Loss: 0.10098105669021606\n",
      "Epoch 5429/30000 Training Loss: 0.08371857553720474\n",
      "Epoch 5430/30000 Training Loss: 0.09941645711660385\n",
      "Epoch 5430/30000 Validation Loss: 0.10030237585306168\n",
      "Epoch 5431/30000 Training Loss: 0.09178098291158676\n",
      "Epoch 5432/30000 Training Loss: 0.08093496412038803\n",
      "Epoch 5433/30000 Training Loss: 0.09675487875938416\n",
      "Epoch 5434/30000 Training Loss: 0.09598058462142944\n",
      "Epoch 5435/30000 Training Loss: 0.07657250016927719\n",
      "Epoch 5436/30000 Training Loss: 0.10502424836158752\n",
      "Epoch 5437/30000 Training Loss: 0.11040996760129929\n",
      "Epoch 5438/30000 Training Loss: 0.0793018713593483\n",
      "Epoch 5439/30000 Training Loss: 0.07562600821256638\n",
      "Epoch 5440/30000 Training Loss: 0.0991017147898674\n",
      "Epoch 5440/30000 Validation Loss: 0.12221594899892807\n",
      "Epoch 5441/30000 Training Loss: 0.06620828807353973\n",
      "Epoch 5442/30000 Training Loss: 0.09228237718343735\n",
      "Epoch 5443/30000 Training Loss: 0.10735827684402466\n",
      "Epoch 5444/30000 Training Loss: 0.10975801199674606\n",
      "Epoch 5445/30000 Training Loss: 0.09527625888586044\n",
      "Epoch 5446/30000 Training Loss: 0.08931980282068253\n",
      "Epoch 5447/30000 Training Loss: 0.10359477996826172\n",
      "Epoch 5448/30000 Training Loss: 0.08731158822774887\n",
      "Epoch 5449/30000 Training Loss: 0.08749990910291672\n",
      "Epoch 5450/30000 Training Loss: 0.08712059259414673\n",
      "Epoch 5450/30000 Validation Loss: 0.09756644815206528\n",
      "Epoch 5451/30000 Training Loss: 0.090509332716465\n",
      "Epoch 5452/30000 Training Loss: 0.08589249849319458\n",
      "Epoch 5453/30000 Training Loss: 0.09400755912065506\n",
      "Epoch 5454/30000 Training Loss: 0.09065741300582886\n",
      "Epoch 5455/30000 Training Loss: 0.07687097042798996\n",
      "Epoch 5456/30000 Training Loss: 0.08641469478607178\n",
      "Epoch 5457/30000 Training Loss: 0.1056283488869667\n",
      "Epoch 5458/30000 Training Loss: 0.07142850756645203\n",
      "Epoch 5459/30000 Training Loss: 0.06930419057607651\n",
      "Epoch 5460/30000 Training Loss: 0.07041241228580475\n",
      "Epoch 5460/30000 Validation Loss: 0.09563835710287094\n",
      "Epoch 5461/30000 Training Loss: 0.09151667356491089\n",
      "Epoch 5462/30000 Training Loss: 0.09457201510667801\n",
      "Epoch 5463/30000 Training Loss: 0.08295149356126785\n",
      "Epoch 5464/30000 Training Loss: 0.08937981724739075\n",
      "Epoch 5465/30000 Training Loss: 0.1027296856045723\n",
      "Epoch 5466/30000 Training Loss: 0.07755779474973679\n",
      "Epoch 5467/30000 Training Loss: 0.09711702913045883\n",
      "Epoch 5468/30000 Training Loss: 0.07782886177301407\n",
      "Epoch 5469/30000 Training Loss: 0.09078017622232437\n",
      "Epoch 5470/30000 Training Loss: 0.09053019434213638\n",
      "Epoch 5470/30000 Validation Loss: 0.09662368148565292\n",
      "Epoch 5471/30000 Training Loss: 0.11070757359266281\n",
      "Epoch 5472/30000 Training Loss: 0.08508875966072083\n",
      "Epoch 5473/30000 Training Loss: 0.11365044862031937\n",
      "Epoch 5474/30000 Training Loss: 0.08658210188150406\n",
      "Epoch 5475/30000 Training Loss: 0.11283645778894424\n",
      "Epoch 5476/30000 Training Loss: 0.0905965268611908\n",
      "Epoch 5477/30000 Training Loss: 0.09171873331069946\n",
      "Epoch 5478/30000 Training Loss: 0.08117379993200302\n",
      "Epoch 5479/30000 Training Loss: 0.10846700519323349\n",
      "Epoch 5480/30000 Training Loss: 0.08406034111976624\n",
      "Epoch 5480/30000 Validation Loss: 0.09042000770568848\n",
      "Epoch 5481/30000 Training Loss: 0.0967336967587471\n",
      "Epoch 5482/30000 Training Loss: 0.0920531377196312\n",
      "Epoch 5483/30000 Training Loss: 0.09281959384679794\n",
      "Epoch 5484/30000 Training Loss: 0.10286464542150497\n",
      "Epoch 5485/30000 Training Loss: 0.09597165137529373\n",
      "Epoch 5486/30000 Training Loss: 0.0916215181350708\n",
      "Epoch 5487/30000 Training Loss: 0.09844725579023361\n",
      "Epoch 5488/30000 Training Loss: 0.07758282124996185\n",
      "Epoch 5489/30000 Training Loss: 0.10190564393997192\n",
      "Epoch 5490/30000 Training Loss: 0.10321152955293655\n",
      "Epoch 5490/30000 Validation Loss: 0.10601500421762466\n",
      "Epoch 5491/30000 Training Loss: 0.08743679523468018\n",
      "Epoch 5492/30000 Training Loss: 0.09271886199712753\n",
      "Epoch 5493/30000 Training Loss: 0.0938577651977539\n",
      "Epoch 5494/30000 Training Loss: 0.07748591154813766\n",
      "Epoch 5495/30000 Training Loss: 0.1052575409412384\n",
      "Epoch 5496/30000 Training Loss: 0.09916257858276367\n",
      "Epoch 5497/30000 Training Loss: 0.08185910433530807\n",
      "Epoch 5498/30000 Training Loss: 0.08919212967157364\n",
      "Epoch 5499/30000 Training Loss: 0.10472941398620605\n",
      "Epoch 5500/30000 Training Loss: 0.08888270705938339\n",
      "Epoch 5500/30000 Validation Loss: 0.08435937762260437\n",
      "Epoch 5501/30000 Training Loss: 0.07824495434761047\n",
      "Epoch 5502/30000 Training Loss: 0.11501583456993103\n",
      "Epoch 5503/30000 Training Loss: 0.1232370138168335\n",
      "Epoch 5504/30000 Training Loss: 0.07268135994672775\n",
      "Epoch 5505/30000 Training Loss: 0.07275929301977158\n",
      "Epoch 5506/30000 Training Loss: 0.09342300146818161\n",
      "Epoch 5507/30000 Training Loss: 0.06475584208965302\n",
      "Epoch 5508/30000 Training Loss: 0.08549957722425461\n",
      "Epoch 5509/30000 Training Loss: 0.0865994393825531\n",
      "Epoch 5510/30000 Training Loss: 0.09363996982574463\n",
      "Epoch 5510/30000 Validation Loss: 0.0803910568356514\n",
      "Epoch 5511/30000 Training Loss: 0.10031768679618835\n",
      "Epoch 5512/30000 Training Loss: 0.0734713152050972\n",
      "Epoch 5513/30000 Training Loss: 0.08749132603406906\n",
      "Epoch 5514/30000 Training Loss: 0.07315999269485474\n",
      "Epoch 5515/30000 Training Loss: 0.09853998571634293\n",
      "Epoch 5516/30000 Training Loss: 0.08703873306512833\n",
      "Epoch 5517/30000 Training Loss: 0.08185338228940964\n",
      "Epoch 5518/30000 Training Loss: 0.08818458765745163\n",
      "Epoch 5519/30000 Training Loss: 0.09484990686178207\n",
      "Epoch 5520/30000 Training Loss: 0.06362234801054001\n",
      "Epoch 5520/30000 Validation Loss: 0.08003649860620499\n",
      "Epoch 5521/30000 Training Loss: 0.08012675493955612\n",
      "Epoch 5522/30000 Training Loss: 0.08362574130296707\n",
      "Epoch 5523/30000 Training Loss: 0.0984530821442604\n",
      "Epoch 5524/30000 Training Loss: 0.08063077926635742\n",
      "Epoch 5525/30000 Training Loss: 0.09231842309236526\n",
      "Epoch 5526/30000 Training Loss: 0.10170529037714005\n",
      "Epoch 5527/30000 Training Loss: 0.09356829524040222\n",
      "Epoch 5528/30000 Training Loss: 0.08097339421510696\n",
      "Epoch 5529/30000 Training Loss: 0.10639947652816772\n",
      "Epoch 5530/30000 Training Loss: 0.09364338964223862\n",
      "Epoch 5530/30000 Validation Loss: 0.11614497750997543\n",
      "Epoch 5531/30000 Training Loss: 0.08745958656072617\n",
      "Epoch 5532/30000 Training Loss: 0.10513051599264145\n",
      "Epoch 5533/30000 Training Loss: 0.11985322088003159\n",
      "Epoch 5534/30000 Training Loss: 0.0850762128829956\n",
      "Epoch 5535/30000 Training Loss: 0.09851670265197754\n",
      "Epoch 5536/30000 Training Loss: 0.07389089465141296\n",
      "Epoch 5537/30000 Training Loss: 0.0771137997508049\n",
      "Epoch 5538/30000 Training Loss: 0.06183384731411934\n",
      "Epoch 5539/30000 Training Loss: 0.07354643940925598\n",
      "Epoch 5540/30000 Training Loss: 0.08365268260240555\n",
      "Epoch 5540/30000 Validation Loss: 0.0817304477095604\n",
      "Epoch 5541/30000 Training Loss: 0.0770970806479454\n",
      "Epoch 5542/30000 Training Loss: 0.07927119731903076\n",
      "Epoch 5543/30000 Training Loss: 0.0880562961101532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5544/30000 Training Loss: 0.08137459307909012\n",
      "Epoch 5545/30000 Training Loss: 0.07566415518522263\n",
      "Epoch 5546/30000 Training Loss: 0.08572570234537125\n",
      "Epoch 5547/30000 Training Loss: 0.10017368942499161\n",
      "Epoch 5548/30000 Training Loss: 0.0888543426990509\n",
      "Epoch 5549/30000 Training Loss: 0.09002389758825302\n",
      "Epoch 5550/30000 Training Loss: 0.10019101947546005\n",
      "Epoch 5550/30000 Validation Loss: 0.0858113095164299\n",
      "Epoch 5551/30000 Training Loss: 0.07884529232978821\n",
      "Epoch 5552/30000 Training Loss: 0.08895348757505417\n",
      "Epoch 5553/30000 Training Loss: 0.08286681771278381\n",
      "Epoch 5554/30000 Training Loss: 0.10475743561983109\n",
      "Epoch 5555/30000 Training Loss: 0.08152564615011215\n",
      "Epoch 5556/30000 Training Loss: 0.12583200633525848\n",
      "Epoch 5557/30000 Training Loss: 0.10031875967979431\n",
      "Epoch 5558/30000 Training Loss: 0.09554123133420944\n",
      "Epoch 5559/30000 Training Loss: 0.09834117442369461\n",
      "Epoch 5560/30000 Training Loss: 0.10021913051605225\n",
      "Epoch 5560/30000 Validation Loss: 0.08310070633888245\n",
      "Epoch 5561/30000 Training Loss: 0.07965592294931412\n",
      "Epoch 5562/30000 Training Loss: 0.08797264099121094\n",
      "Epoch 5563/30000 Training Loss: 0.06932774931192398\n",
      "Epoch 5564/30000 Training Loss: 0.06896863132715225\n",
      "Epoch 5565/30000 Training Loss: 0.11346260458230972\n",
      "Epoch 5566/30000 Training Loss: 0.11508218199014664\n",
      "Epoch 5567/30000 Training Loss: 0.09038260579109192\n",
      "Epoch 5568/30000 Training Loss: 0.09107869118452072\n",
      "Epoch 5569/30000 Training Loss: 0.07127829641103745\n",
      "Epoch 5570/30000 Training Loss: 0.08224209398031235\n",
      "Epoch 5570/30000 Validation Loss: 0.09041855484247208\n",
      "Epoch 5571/30000 Training Loss: 0.07653515040874481\n",
      "Epoch 5572/30000 Training Loss: 0.09180489182472229\n",
      "Epoch 5573/30000 Training Loss: 0.08186337351799011\n",
      "Epoch 5574/30000 Training Loss: 0.08315401524305344\n",
      "Epoch 5575/30000 Training Loss: 0.08947748690843582\n",
      "Epoch 5576/30000 Training Loss: 0.08961302787065506\n",
      "Epoch 5577/30000 Training Loss: 0.08150789141654968\n",
      "Epoch 5578/30000 Training Loss: 0.08242378383874893\n",
      "Epoch 5579/30000 Training Loss: 0.12544582784175873\n",
      "Epoch 5580/30000 Training Loss: 0.08548194169998169\n",
      "Epoch 5580/30000 Validation Loss: 0.08678162842988968\n",
      "Epoch 5581/30000 Training Loss: 0.07541211694478989\n",
      "Epoch 5582/30000 Training Loss: 0.10944192856550217\n",
      "Epoch 5583/30000 Training Loss: 0.0802098736166954\n",
      "Epoch 5584/30000 Training Loss: 0.10432527214288712\n",
      "Epoch 5585/30000 Training Loss: 0.10279535502195358\n",
      "Epoch 5586/30000 Training Loss: 0.09185585379600525\n",
      "Epoch 5587/30000 Training Loss: 0.08494586497545242\n",
      "Epoch 5588/30000 Training Loss: 0.08622223138809204\n",
      "Epoch 5589/30000 Training Loss: 0.1235169842839241\n",
      "Epoch 5590/30000 Training Loss: 0.09249251335859299\n",
      "Epoch 5590/30000 Validation Loss: 0.09534382075071335\n",
      "Epoch 5591/30000 Training Loss: 0.09085241705179214\n",
      "Epoch 5592/30000 Training Loss: 0.08351978659629822\n",
      "Epoch 5593/30000 Training Loss: 0.08255495876073837\n",
      "Epoch 5594/30000 Training Loss: 0.07932140678167343\n",
      "Epoch 5595/30000 Training Loss: 0.09216155856847763\n",
      "Epoch 5596/30000 Training Loss: 0.10349667072296143\n",
      "Epoch 5597/30000 Training Loss: 0.08688053488731384\n",
      "Epoch 5598/30000 Training Loss: 0.08044993132352829\n",
      "Epoch 5599/30000 Training Loss: 0.09310728311538696\n",
      "Epoch 5600/30000 Training Loss: 0.09238036721944809\n",
      "Epoch 5600/30000 Validation Loss: 0.1100400760769844\n",
      "Epoch 5601/30000 Training Loss: 0.09035297483205795\n",
      "Epoch 5602/30000 Training Loss: 0.07678312808275223\n",
      "Epoch 5603/30000 Training Loss: 0.08404088765382767\n",
      "Epoch 5604/30000 Training Loss: 0.08222061395645142\n",
      "Epoch 5605/30000 Training Loss: 0.06102655455470085\n",
      "Epoch 5606/30000 Training Loss: 0.09685403108596802\n",
      "Epoch 5607/30000 Training Loss: 0.07620050758123398\n",
      "Epoch 5608/30000 Training Loss: 0.07415824383497238\n",
      "Epoch 5609/30000 Training Loss: 0.09436672925949097\n",
      "Epoch 5610/30000 Training Loss: 0.10061216354370117\n",
      "Epoch 5610/30000 Validation Loss: 0.081123948097229\n",
      "Epoch 5611/30000 Training Loss: 0.08299184590578079\n",
      "Epoch 5612/30000 Training Loss: 0.09131529182195663\n",
      "Epoch 5613/30000 Training Loss: 0.10542412847280502\n",
      "Epoch 5614/30000 Training Loss: 0.07539782673120499\n",
      "Epoch 5615/30000 Training Loss: 0.07741232216358185\n",
      "Epoch 5616/30000 Training Loss: 0.07131370902061462\n",
      "Epoch 5617/30000 Training Loss: 0.10015684366226196\n",
      "Epoch 5618/30000 Training Loss: 0.08439474552869797\n",
      "Epoch 5619/30000 Training Loss: 0.09560158103704453\n",
      "Epoch 5620/30000 Training Loss: 0.08349522203207016\n",
      "Epoch 5620/30000 Validation Loss: 0.07465475797653198\n",
      "Epoch 5621/30000 Training Loss: 0.09327385574579239\n",
      "Epoch 5622/30000 Training Loss: 0.0938243567943573\n",
      "Epoch 5623/30000 Training Loss: 0.07511996477842331\n",
      "Epoch 5624/30000 Training Loss: 0.07779312878847122\n",
      "Epoch 5625/30000 Training Loss: 0.07772138714790344\n",
      "Epoch 5626/30000 Training Loss: 0.08558335155248642\n",
      "Epoch 5627/30000 Training Loss: 0.10674210637807846\n",
      "Epoch 5628/30000 Training Loss: 0.0659240260720253\n",
      "Epoch 5629/30000 Training Loss: 0.10879570990800858\n",
      "Epoch 5630/30000 Training Loss: 0.11209534853696823\n",
      "Epoch 5630/30000 Validation Loss: 0.09256280213594437\n",
      "Epoch 5631/30000 Training Loss: 0.08027157932519913\n",
      "Epoch 5632/30000 Training Loss: 0.08340062946081161\n",
      "Epoch 5633/30000 Training Loss: 0.11098042875528336\n",
      "Epoch 5634/30000 Training Loss: 0.07135383039712906\n",
      "Epoch 5635/30000 Training Loss: 0.09290728718042374\n",
      "Epoch 5636/30000 Training Loss: 0.0944536030292511\n",
      "Epoch 5637/30000 Training Loss: 0.0824776366353035\n",
      "Epoch 5638/30000 Training Loss: 0.0837453082203865\n",
      "Epoch 5639/30000 Training Loss: 0.07926866412162781\n",
      "Epoch 5640/30000 Training Loss: 0.08524203300476074\n",
      "Epoch 5640/30000 Validation Loss: 0.11815280467271805\n",
      "Epoch 5641/30000 Training Loss: 0.06603608280420303\n",
      "Epoch 5642/30000 Training Loss: 0.10458757728338242\n",
      "Epoch 5643/30000 Training Loss: 0.11081447452306747\n",
      "Epoch 5644/30000 Training Loss: 0.07126853615045547\n",
      "Epoch 5645/30000 Training Loss: 0.09106471389532089\n",
      "Epoch 5646/30000 Training Loss: 0.08031085133552551\n",
      "Epoch 5647/30000 Training Loss: 0.105444997549057\n",
      "Epoch 5648/30000 Training Loss: 0.10138475894927979\n",
      "Epoch 5649/30000 Training Loss: 0.10664742439985275\n",
      "Epoch 5650/30000 Training Loss: 0.10247981548309326\n",
      "Epoch 5650/30000 Validation Loss: 0.07858211547136307\n",
      "Epoch 5651/30000 Training Loss: 0.09861642122268677\n",
      "Epoch 5652/30000 Training Loss: 0.09579873830080032\n",
      "Epoch 5653/30000 Training Loss: 0.08613032847642899\n",
      "Epoch 5654/30000 Training Loss: 0.1076597347855568\n",
      "Epoch 5655/30000 Training Loss: 0.06230879947543144\n",
      "Epoch 5656/30000 Training Loss: 0.10472732037305832\n",
      "Epoch 5657/30000 Training Loss: 0.1069861650466919\n",
      "Epoch 5658/30000 Training Loss: 0.08390448242425919\n",
      "Epoch 5659/30000 Training Loss: 0.09093225747346878\n",
      "Epoch 5660/30000 Training Loss: 0.0906364917755127\n",
      "Epoch 5660/30000 Validation Loss: 0.1027008593082428\n",
      "Epoch 5661/30000 Training Loss: 0.06309177726507187\n",
      "Epoch 5662/30000 Training Loss: 0.08523408323526382\n",
      "Epoch 5663/30000 Training Loss: 0.097442626953125\n",
      "Epoch 5664/30000 Training Loss: 0.07464460283517838\n",
      "Epoch 5665/30000 Training Loss: 0.08398260921239853\n",
      "Epoch 5666/30000 Training Loss: 0.08456778526306152\n",
      "Epoch 5667/30000 Training Loss: 0.08974029868841171\n",
      "Epoch 5668/30000 Training Loss: 0.08993450552225113\n",
      "Epoch 5669/30000 Training Loss: 0.09737443923950195\n",
      "Epoch 5670/30000 Training Loss: 0.08926859498023987\n",
      "Epoch 5670/30000 Validation Loss: 0.08366147428750992\n",
      "Epoch 5671/30000 Training Loss: 0.09295305609703064\n",
      "Epoch 5672/30000 Training Loss: 0.09198097139596939\n",
      "Epoch 5673/30000 Training Loss: 0.0866943821310997\n",
      "Epoch 5674/30000 Training Loss: 0.09006267786026001\n",
      "Epoch 5675/30000 Training Loss: 0.06564690917730331\n",
      "Epoch 5676/30000 Training Loss: 0.09183310717344284\n",
      "Epoch 5677/30000 Training Loss: 0.08545158058404922\n",
      "Epoch 5678/30000 Training Loss: 0.08453663438558578\n",
      "Epoch 5679/30000 Training Loss: 0.10250318050384521\n",
      "Epoch 5680/30000 Training Loss: 0.09700137376785278\n",
      "Epoch 5680/30000 Validation Loss: 0.07570026814937592\n",
      "Epoch 5681/30000 Training Loss: 0.06978607177734375\n",
      "Epoch 5682/30000 Training Loss: 0.07563892751932144\n",
      "Epoch 5683/30000 Training Loss: 0.09740316867828369\n",
      "Epoch 5684/30000 Training Loss: 0.07328522950410843\n",
      "Epoch 5685/30000 Training Loss: 0.08603563904762268\n",
      "Epoch 5686/30000 Training Loss: 0.12085453420877457\n",
      "Epoch 5687/30000 Training Loss: 0.10032574087381363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5688/30000 Training Loss: 0.08262598514556885\n",
      "Epoch 5689/30000 Training Loss: 0.10672038793563843\n",
      "Epoch 5690/30000 Training Loss: 0.08519044518470764\n",
      "Epoch 5690/30000 Validation Loss: 0.07557860016822815\n",
      "Epoch 5691/30000 Training Loss: 0.08437508344650269\n",
      "Epoch 5692/30000 Training Loss: 0.07396029680967331\n",
      "Epoch 5693/30000 Training Loss: 0.08141735941171646\n",
      "Epoch 5694/30000 Training Loss: 0.08111970871686935\n",
      "Epoch 5695/30000 Training Loss: 0.08701977878808975\n",
      "Epoch 5696/30000 Training Loss: 0.0658346489071846\n",
      "Epoch 5697/30000 Training Loss: 0.08784854412078857\n",
      "Epoch 5698/30000 Training Loss: 0.10686170309782028\n",
      "Epoch 5699/30000 Training Loss: 0.07821395248174667\n",
      "Epoch 5700/30000 Training Loss: 0.09411659836769104\n",
      "Epoch 5700/30000 Validation Loss: 0.06782745569944382\n",
      "Epoch 5701/30000 Training Loss: 0.09469596296548843\n",
      "Epoch 5702/30000 Training Loss: 0.09617834538221359\n",
      "Epoch 5703/30000 Training Loss: 0.10072772949934006\n",
      "Epoch 5704/30000 Training Loss: 0.09004569053649902\n",
      "Epoch 5705/30000 Training Loss: 0.1118185743689537\n",
      "Epoch 5706/30000 Training Loss: 0.08504977822303772\n",
      "Epoch 5707/30000 Training Loss: 0.09327330440282822\n",
      "Epoch 5708/30000 Training Loss: 0.1267782300710678\n",
      "Epoch 5709/30000 Training Loss: 0.06503021717071533\n",
      "Epoch 5710/30000 Training Loss: 0.07129383832216263\n",
      "Epoch 5710/30000 Validation Loss: 0.07017091661691666\n",
      "Epoch 5711/30000 Training Loss: 0.06856850534677505\n",
      "Epoch 5712/30000 Training Loss: 0.08630681037902832\n",
      "Epoch 5713/30000 Training Loss: 0.0923418328166008\n",
      "Epoch 5714/30000 Training Loss: 0.09307528287172318\n",
      "Epoch 5715/30000 Training Loss: 0.08392781019210815\n",
      "Epoch 5716/30000 Training Loss: 0.07335004955530167\n",
      "Epoch 5717/30000 Training Loss: 0.11395273357629776\n",
      "Epoch 5718/30000 Training Loss: 0.09861960262060165\n",
      "Epoch 5719/30000 Training Loss: 0.07520731538534164\n",
      "Epoch 5720/30000 Training Loss: 0.09900394827127457\n",
      "Epoch 5720/30000 Validation Loss: 0.07537295669317245\n",
      "Epoch 5721/30000 Training Loss: 0.10076873749494553\n",
      "Epoch 5722/30000 Training Loss: 0.12348523736000061\n",
      "Epoch 5723/30000 Training Loss: 0.09579411894083023\n",
      "Epoch 5724/30000 Training Loss: 0.09005659818649292\n",
      "Epoch 5725/30000 Training Loss: 0.08821440488100052\n",
      "Epoch 5726/30000 Training Loss: 0.07926761358976364\n",
      "Epoch 5727/30000 Training Loss: 0.08012530952692032\n",
      "Epoch 5728/30000 Training Loss: 0.07234236598014832\n",
      "Epoch 5729/30000 Training Loss: 0.08070480078458786\n",
      "Epoch 5730/30000 Training Loss: 0.08214186877012253\n",
      "Epoch 5730/30000 Validation Loss: 0.09483134746551514\n",
      "Epoch 5731/30000 Training Loss: 0.06887318193912506\n",
      "Epoch 5732/30000 Training Loss: 0.07693370431661606\n",
      "Epoch 5733/30000 Training Loss: 0.07766997814178467\n",
      "Epoch 5734/30000 Training Loss: 0.07559338957071304\n",
      "Epoch 5735/30000 Training Loss: 0.09304898977279663\n",
      "Epoch 5736/30000 Training Loss: 0.08247414976358414\n",
      "Epoch 5737/30000 Training Loss: 0.11958899348974228\n",
      "Epoch 5738/30000 Training Loss: 0.0932023748755455\n",
      "Epoch 5739/30000 Training Loss: 0.09440455585718155\n",
      "Epoch 5740/30000 Training Loss: 0.08349007368087769\n",
      "Epoch 5740/30000 Validation Loss: 0.10359405726194382\n",
      "Epoch 5741/30000 Training Loss: 0.09203385561704636\n",
      "Epoch 5742/30000 Training Loss: 0.1093049868941307\n",
      "Epoch 5743/30000 Training Loss: 0.07838259637355804\n",
      "Epoch 5744/30000 Training Loss: 0.1063087210059166\n",
      "Epoch 5745/30000 Training Loss: 0.10414978116750717\n",
      "Epoch 5746/30000 Training Loss: 0.10193777829408646\n",
      "Epoch 5747/30000 Training Loss: 0.07563258707523346\n",
      "Epoch 5748/30000 Training Loss: 0.0945797935128212\n",
      "Epoch 5749/30000 Training Loss: 0.07739695906639099\n",
      "Epoch 5750/30000 Training Loss: 0.10740090161561966\n",
      "Epoch 5750/30000 Validation Loss: 0.07412395626306534\n",
      "Epoch 5751/30000 Training Loss: 0.06696199625730515\n",
      "Epoch 5752/30000 Training Loss: 0.0876469835639\n",
      "Epoch 5753/30000 Training Loss: 0.08144872635602951\n",
      "Epoch 5754/30000 Training Loss: 0.08033991605043411\n",
      "Epoch 5755/30000 Training Loss: 0.0845770463347435\n",
      "Epoch 5756/30000 Training Loss: 0.08327236771583557\n",
      "Epoch 5757/30000 Training Loss: 0.09959105402231216\n",
      "Epoch 5758/30000 Training Loss: 0.11054423451423645\n",
      "Epoch 5759/30000 Training Loss: 0.07449921220541\n",
      "Epoch 5760/30000 Training Loss: 0.09368491172790527\n",
      "Epoch 5760/30000 Validation Loss: 0.08579648286104202\n",
      "Epoch 5761/30000 Training Loss: 0.08288097381591797\n",
      "Epoch 5762/30000 Training Loss: 0.06425923109054565\n",
      "Epoch 5763/30000 Training Loss: 0.09117141366004944\n",
      "Epoch 5764/30000 Training Loss: 0.07158514112234116\n",
      "Epoch 5765/30000 Training Loss: 0.09094443917274475\n",
      "Epoch 5766/30000 Training Loss: 0.08128418773412704\n",
      "Epoch 5767/30000 Training Loss: 0.07956963032484055\n",
      "Epoch 5768/30000 Training Loss: 0.09950199723243713\n",
      "Epoch 5769/30000 Training Loss: 0.07830122858285904\n",
      "Epoch 5770/30000 Training Loss: 0.09375127404928207\n",
      "Epoch 5770/30000 Validation Loss: 0.08683892339468002\n",
      "Epoch 5771/30000 Training Loss: 0.09083402156829834\n",
      "Epoch 5772/30000 Training Loss: 0.10630122572183609\n",
      "Epoch 5773/30000 Training Loss: 0.08410786837339401\n",
      "Epoch 5774/30000 Training Loss: 0.07330463081598282\n",
      "Epoch 5775/30000 Training Loss: 0.10978064686059952\n",
      "Epoch 5776/30000 Training Loss: 0.07994826883077621\n",
      "Epoch 5777/30000 Training Loss: 0.08905839174985886\n",
      "Epoch 5778/30000 Training Loss: 0.08382320404052734\n",
      "Epoch 5779/30000 Training Loss: 0.09873997420072556\n",
      "Epoch 5780/30000 Training Loss: 0.09588579088449478\n",
      "Epoch 5780/30000 Validation Loss: 0.10520140081644058\n",
      "Epoch 5781/30000 Training Loss: 0.07293086498975754\n",
      "Epoch 5782/30000 Training Loss: 0.1206180676817894\n",
      "Epoch 5783/30000 Training Loss: 0.08077476173639297\n",
      "Epoch 5784/30000 Training Loss: 0.07973191887140274\n",
      "Epoch 5785/30000 Training Loss: 0.0754019096493721\n",
      "Epoch 5786/30000 Training Loss: 0.09390044957399368\n",
      "Epoch 5787/30000 Training Loss: 0.08376964181661606\n",
      "Epoch 5788/30000 Training Loss: 0.08732762932777405\n",
      "Epoch 5789/30000 Training Loss: 0.11151740700006485\n",
      "Epoch 5790/30000 Training Loss: 0.07796358317136765\n",
      "Epoch 5790/30000 Validation Loss: 0.11752476543188095\n",
      "Epoch 5791/30000 Training Loss: 0.09295868873596191\n",
      "Epoch 5792/30000 Training Loss: 0.09158835560083389\n",
      "Epoch 5793/30000 Training Loss: 0.09192012995481491\n",
      "Epoch 5794/30000 Training Loss: 0.07200322300195694\n",
      "Epoch 5795/30000 Training Loss: 0.08046425133943558\n",
      "Epoch 5796/30000 Training Loss: 0.07757625728845596\n",
      "Epoch 5797/30000 Training Loss: 0.08191564679145813\n",
      "Epoch 5798/30000 Training Loss: 0.09523878246545792\n",
      "Epoch 5799/30000 Training Loss: 0.07925034314393997\n",
      "Epoch 5800/30000 Training Loss: 0.07847663760185242\n",
      "Epoch 5800/30000 Validation Loss: 0.09600341320037842\n",
      "Epoch 5801/30000 Training Loss: 0.08015018701553345\n",
      "Epoch 5802/30000 Training Loss: 0.08306458592414856\n",
      "Epoch 5803/30000 Training Loss: 0.07678394764661789\n",
      "Epoch 5804/30000 Training Loss: 0.07773467153310776\n",
      "Epoch 5805/30000 Training Loss: 0.09562403708696365\n",
      "Epoch 5806/30000 Training Loss: 0.09896841645240784\n",
      "Epoch 5807/30000 Training Loss: 0.08754017949104309\n",
      "Epoch 5808/30000 Training Loss: 0.08247911930084229\n",
      "Epoch 5809/30000 Training Loss: 0.09737882018089294\n",
      "Epoch 5810/30000 Training Loss: 0.09649642556905746\n",
      "Epoch 5810/30000 Validation Loss: 0.0850265845656395\n",
      "Epoch 5811/30000 Training Loss: 0.07729514688253403\n",
      "Epoch 5812/30000 Training Loss: 0.07349535077810287\n",
      "Epoch 5813/30000 Training Loss: 0.08759281039237976\n",
      "Epoch 5814/30000 Training Loss: 0.10260540246963501\n",
      "Epoch 5815/30000 Training Loss: 0.09643235057592392\n",
      "Epoch 5816/30000 Training Loss: 0.07948242127895355\n",
      "Epoch 5817/30000 Training Loss: 0.09847116470336914\n",
      "Epoch 5818/30000 Training Loss: 0.10950122028589249\n",
      "Epoch 5819/30000 Training Loss: 0.09001653641462326\n",
      "Epoch 5820/30000 Training Loss: 0.09295185655355453\n",
      "Epoch 5820/30000 Validation Loss: 0.10261285305023193\n",
      "Epoch 5821/30000 Training Loss: 0.0917348563671112\n",
      "Epoch 5822/30000 Training Loss: 0.09236378222703934\n",
      "Epoch 5823/30000 Training Loss: 0.09431958198547363\n",
      "Epoch 5824/30000 Training Loss: 0.11418968439102173\n",
      "Epoch 5825/30000 Training Loss: 0.07476309686899185\n",
      "Epoch 5826/30000 Training Loss: 0.08776003122329712\n",
      "Epoch 5827/30000 Training Loss: 0.0999717190861702\n",
      "Epoch 5828/30000 Training Loss: 0.10144376754760742\n",
      "Epoch 5829/30000 Training Loss: 0.10366693884134293\n",
      "Epoch 5830/30000 Training Loss: 0.08353685587644577\n",
      "Epoch 5830/30000 Validation Loss: 0.08160840719938278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5831/30000 Training Loss: 0.07359126210212708\n",
      "Epoch 5832/30000 Training Loss: 0.06698492169380188\n",
      "Epoch 5833/30000 Training Loss: 0.10816138237714767\n",
      "Epoch 5834/30000 Training Loss: 0.07080577313899994\n",
      "Epoch 5835/30000 Training Loss: 0.06949660927057266\n",
      "Epoch 5836/30000 Training Loss: 0.10071367025375366\n",
      "Epoch 5837/30000 Training Loss: 0.06817195564508438\n",
      "Epoch 5838/30000 Training Loss: 0.0964445099234581\n",
      "Epoch 5839/30000 Training Loss: 0.11124637722969055\n",
      "Epoch 5840/30000 Training Loss: 0.09144941717386246\n",
      "Epoch 5840/30000 Validation Loss: 0.08379730582237244\n",
      "Epoch 5841/30000 Training Loss: 0.09699928760528564\n",
      "Epoch 5842/30000 Training Loss: 0.08225161582231522\n",
      "Epoch 5843/30000 Training Loss: 0.09065469354391098\n",
      "Epoch 5844/30000 Training Loss: 0.09585566073656082\n",
      "Epoch 5845/30000 Training Loss: 0.10675586014986038\n",
      "Epoch 5846/30000 Training Loss: 0.09497997909784317\n",
      "Epoch 5847/30000 Training Loss: 0.0931243896484375\n",
      "Epoch 5848/30000 Training Loss: 0.10842999815940857\n",
      "Epoch 5849/30000 Training Loss: 0.1066378578543663\n",
      "Epoch 5850/30000 Training Loss: 0.07054116576910019\n",
      "Epoch 5850/30000 Validation Loss: 0.08562571555376053\n",
      "Epoch 5851/30000 Training Loss: 0.10889852792024612\n",
      "Epoch 5852/30000 Training Loss: 0.09162813425064087\n",
      "Epoch 5853/30000 Training Loss: 0.081232450902462\n",
      "Epoch 5854/30000 Training Loss: 0.08220018446445465\n",
      "Epoch 5855/30000 Training Loss: 0.1060827374458313\n",
      "Epoch 5856/30000 Training Loss: 0.06574343144893646\n",
      "Epoch 5857/30000 Training Loss: 0.08255957067012787\n",
      "Epoch 5858/30000 Training Loss: 0.08465375751256943\n",
      "Epoch 5859/30000 Training Loss: 0.0879698172211647\n",
      "Epoch 5860/30000 Training Loss: 0.09928622096776962\n",
      "Epoch 5860/30000 Validation Loss: 0.09577545523643494\n",
      "Epoch 5861/30000 Training Loss: 0.09290377050638199\n",
      "Epoch 5862/30000 Training Loss: 0.09868651628494263\n",
      "Epoch 5863/30000 Training Loss: 0.09162098914384842\n",
      "Epoch 5864/30000 Training Loss: 0.08803533762693405\n",
      "Epoch 5865/30000 Training Loss: 0.07880304008722305\n",
      "Epoch 5866/30000 Training Loss: 0.10575131326913834\n",
      "Epoch 5867/30000 Training Loss: 0.0886833444237709\n",
      "Epoch 5868/30000 Training Loss: 0.12709669768810272\n",
      "Epoch 5869/30000 Training Loss: 0.1063457801938057\n",
      "Epoch 5870/30000 Training Loss: 0.09830829501152039\n",
      "Epoch 5870/30000 Validation Loss: 0.08415805548429489\n",
      "Epoch 5871/30000 Training Loss: 0.07364338636398315\n",
      "Epoch 5872/30000 Training Loss: 0.08099670708179474\n",
      "Epoch 5873/30000 Training Loss: 0.09619105607271194\n",
      "Epoch 5874/30000 Training Loss: 0.09396958351135254\n",
      "Epoch 5875/30000 Training Loss: 0.07942317426204681\n",
      "Epoch 5876/30000 Training Loss: 0.08800707012414932\n",
      "Epoch 5877/30000 Training Loss: 0.09353426098823547\n",
      "Epoch 5878/30000 Training Loss: 0.09018833190202713\n",
      "Epoch 5879/30000 Training Loss: 0.0886949971318245\n",
      "Epoch 5880/30000 Training Loss: 0.07652395963668823\n",
      "Epoch 5880/30000 Validation Loss: 0.07529114931821823\n",
      "Epoch 5881/30000 Training Loss: 0.07774833589792252\n",
      "Epoch 5882/30000 Training Loss: 0.07635771483182907\n",
      "Epoch 5883/30000 Training Loss: 0.12019342184066772\n",
      "Epoch 5884/30000 Training Loss: 0.11714745312929153\n",
      "Epoch 5885/30000 Training Loss: 0.08072153478860855\n",
      "Epoch 5886/30000 Training Loss: 0.09379160404205322\n",
      "Epoch 5887/30000 Training Loss: 0.09968756884336472\n",
      "Epoch 5888/30000 Training Loss: 0.08370626717805862\n",
      "Epoch 5889/30000 Training Loss: 0.07828312367200851\n",
      "Epoch 5890/30000 Training Loss: 0.08477115631103516\n",
      "Epoch 5890/30000 Validation Loss: 0.08877718448638916\n",
      "Epoch 5891/30000 Training Loss: 0.12024357914924622\n",
      "Epoch 5892/30000 Training Loss: 0.06555984914302826\n",
      "Epoch 5893/30000 Training Loss: 0.08540909737348557\n",
      "Epoch 5894/30000 Training Loss: 0.1158972755074501\n",
      "Epoch 5895/30000 Training Loss: 0.0710776224732399\n",
      "Epoch 5896/30000 Training Loss: 0.0888921394944191\n",
      "Epoch 5897/30000 Training Loss: 0.08876796811819077\n",
      "Epoch 5898/30000 Training Loss: 0.08223184198141098\n",
      "Epoch 5899/30000 Training Loss: 0.08759941905736923\n",
      "Epoch 5900/30000 Training Loss: 0.09202937036752701\n",
      "Epoch 5900/30000 Validation Loss: 0.07212207466363907\n",
      "Epoch 5901/30000 Training Loss: 0.10210052877664566\n",
      "Epoch 5902/30000 Training Loss: 0.09739948064088821\n",
      "Epoch 5903/30000 Training Loss: 0.1178862676024437\n",
      "Epoch 5904/30000 Training Loss: 0.09398087114095688\n",
      "Epoch 5905/30000 Training Loss: 0.08666548877954483\n",
      "Epoch 5906/30000 Training Loss: 0.07121935486793518\n",
      "Epoch 5907/30000 Training Loss: 0.07033566385507584\n",
      "Epoch 5908/30000 Training Loss: 0.08223716169595718\n",
      "Epoch 5909/30000 Training Loss: 0.08414456993341446\n",
      "Epoch 5910/30000 Training Loss: 0.10062602907419205\n",
      "Epoch 5910/30000 Validation Loss: 0.0781809464097023\n",
      "Epoch 5911/30000 Training Loss: 0.07986065000295639\n",
      "Epoch 5912/30000 Training Loss: 0.09503863006830215\n",
      "Epoch 5913/30000 Training Loss: 0.07253531366586685\n",
      "Epoch 5914/30000 Training Loss: 0.07526341825723648\n",
      "Epoch 5915/30000 Training Loss: 0.08025115728378296\n",
      "Epoch 5916/30000 Training Loss: 0.08795145153999329\n",
      "Epoch 5917/30000 Training Loss: 0.08789879828691483\n",
      "Epoch 5918/30000 Training Loss: 0.07417162507772446\n",
      "Epoch 5919/30000 Training Loss: 0.08279746025800705\n",
      "Epoch 5920/30000 Training Loss: 0.08873343467712402\n",
      "Epoch 5920/30000 Validation Loss: 0.09959712624549866\n",
      "Epoch 5921/30000 Training Loss: 0.07991857826709747\n",
      "Epoch 5922/30000 Training Loss: 0.06404571980237961\n",
      "Epoch 5923/30000 Training Loss: 0.08902084082365036\n",
      "Epoch 5924/30000 Training Loss: 0.11049403995275497\n",
      "Epoch 5925/30000 Training Loss: 0.08573788404464722\n",
      "Epoch 5926/30000 Training Loss: 0.08417170494794846\n",
      "Epoch 5927/30000 Training Loss: 0.09642735868692398\n",
      "Epoch 5928/30000 Training Loss: 0.08005455881357193\n",
      "Epoch 5929/30000 Training Loss: 0.06963273882865906\n",
      "Epoch 5930/30000 Training Loss: 0.07926537841558456\n",
      "Epoch 5930/30000 Validation Loss: 0.08537176996469498\n",
      "Epoch 5931/30000 Training Loss: 0.08519753813743591\n",
      "Epoch 5932/30000 Training Loss: 0.07107830792665482\n",
      "Epoch 5933/30000 Training Loss: 0.05993619188666344\n",
      "Epoch 5934/30000 Training Loss: 0.06974658370018005\n",
      "Epoch 5935/30000 Training Loss: 0.10479482263326645\n",
      "Epoch 5936/30000 Training Loss: 0.10270550847053528\n",
      "Epoch 5937/30000 Training Loss: 0.07597670704126358\n",
      "Epoch 5938/30000 Training Loss: 0.07826701551675797\n",
      "Epoch 5939/30000 Training Loss: 0.0753408893942833\n",
      "Epoch 5940/30000 Training Loss: 0.09585019946098328\n",
      "Epoch 5940/30000 Validation Loss: 0.10599374771118164\n",
      "Epoch 5941/30000 Training Loss: 0.07453200221061707\n",
      "Epoch 5942/30000 Training Loss: 0.07099130749702454\n",
      "Epoch 5943/30000 Training Loss: 0.09126707166433334\n",
      "Epoch 5944/30000 Training Loss: 0.10007452219724655\n",
      "Epoch 5945/30000 Training Loss: 0.07696238905191422\n",
      "Epoch 5946/30000 Training Loss: 0.09950211644172668\n",
      "Epoch 5947/30000 Training Loss: 0.07315904647111893\n",
      "Epoch 5948/30000 Training Loss: 0.08280915766954422\n",
      "Epoch 5949/30000 Training Loss: 0.10187850147485733\n",
      "Epoch 5950/30000 Training Loss: 0.09633848816156387\n",
      "Epoch 5950/30000 Validation Loss: 0.08007583767175674\n",
      "Epoch 5951/30000 Training Loss: 0.0975240170955658\n",
      "Epoch 5952/30000 Training Loss: 0.09481901675462723\n",
      "Epoch 5953/30000 Training Loss: 0.08472920209169388\n",
      "Epoch 5954/30000 Training Loss: 0.07104481011629105\n",
      "Epoch 5955/30000 Training Loss: 0.09759151935577393\n",
      "Epoch 5956/30000 Training Loss: 0.09012746810913086\n",
      "Epoch 5957/30000 Training Loss: 0.09074584394693375\n",
      "Epoch 5958/30000 Training Loss: 0.07295533269643784\n",
      "Epoch 5959/30000 Training Loss: 0.10536137968301773\n",
      "Epoch 5960/30000 Training Loss: 0.07646000385284424\n",
      "Epoch 5960/30000 Validation Loss: 0.07817891985177994\n",
      "Epoch 5961/30000 Training Loss: 0.10356730222702026\n",
      "Epoch 5962/30000 Training Loss: 0.09122731536626816\n",
      "Epoch 5963/30000 Training Loss: 0.09372740983963013\n",
      "Epoch 5964/30000 Training Loss: 0.09334056824445724\n",
      "Epoch 5965/30000 Training Loss: 0.06739642471075058\n",
      "Epoch 5966/30000 Training Loss: 0.08553730696439743\n",
      "Epoch 5967/30000 Training Loss: 0.08204609155654907\n",
      "Epoch 5968/30000 Training Loss: 0.10156486183404922\n",
      "Epoch 5969/30000 Training Loss: 0.09365317970514297\n",
      "Epoch 5970/30000 Training Loss: 0.09154503792524338\n",
      "Epoch 5970/30000 Validation Loss: 0.08032188564538956\n",
      "Epoch 5971/30000 Training Loss: 0.07544323801994324\n",
      "Epoch 5972/30000 Training Loss: 0.10059130936861038\n",
      "Epoch 5973/30000 Training Loss: 0.09808256477117538\n",
      "Epoch 5974/30000 Training Loss: 0.07651498168706894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5975/30000 Training Loss: 0.08088237792253494\n",
      "Epoch 5976/30000 Training Loss: 0.08073227852582932\n",
      "Epoch 5977/30000 Training Loss: 0.09362411499023438\n",
      "Epoch 5978/30000 Training Loss: 0.09508685022592545\n",
      "Epoch 5979/30000 Training Loss: 0.08754398673772812\n",
      "Epoch 5980/30000 Training Loss: 0.06563416868448257\n",
      "Epoch 5980/30000 Validation Loss: 0.11380747705698013\n",
      "Epoch 5981/30000 Training Loss: 0.08720827102661133\n",
      "Epoch 5982/30000 Training Loss: 0.12166988104581833\n",
      "Epoch 5983/30000 Training Loss: 0.08574093133211136\n",
      "Epoch 5984/30000 Training Loss: 0.08420640230178833\n",
      "Epoch 5985/30000 Training Loss: 0.09564954042434692\n",
      "Epoch 5986/30000 Training Loss: 0.0729515552520752\n",
      "Epoch 5987/30000 Training Loss: 0.07352093607187271\n",
      "Epoch 5988/30000 Training Loss: 0.09052526950836182\n",
      "Epoch 5989/30000 Training Loss: 0.07687463611364365\n",
      "Epoch 5990/30000 Training Loss: 0.08176440000534058\n",
      "Epoch 5990/30000 Validation Loss: 0.06521855294704437\n",
      "Epoch 5991/30000 Training Loss: 0.10911043733358383\n",
      "Epoch 5992/30000 Training Loss: 0.07045555859804153\n",
      "Epoch 5993/30000 Training Loss: 0.09400293231010437\n",
      "Epoch 5994/30000 Training Loss: 0.09441093355417252\n",
      "Epoch 5995/30000 Training Loss: 0.09809381514787674\n",
      "Epoch 5996/30000 Training Loss: 0.09635287523269653\n",
      "Epoch 5997/30000 Training Loss: 0.10481389611959457\n",
      "Epoch 5998/30000 Training Loss: 0.088145412504673\n",
      "Epoch 5999/30000 Training Loss: 0.07557611912488937\n",
      "Epoch 6000/30000 Training Loss: 0.06329982727766037\n",
      "Epoch 6000/30000 Validation Loss: 0.09453745931386948\n",
      "Epoch 6001/30000 Training Loss: 0.0921424850821495\n",
      "Epoch 6002/30000 Training Loss: 0.0854727253317833\n",
      "Epoch 6003/30000 Training Loss: 0.08362720161676407\n",
      "Epoch 6004/30000 Training Loss: 0.07826164364814758\n",
      "Epoch 6005/30000 Training Loss: 0.06662379950284958\n",
      "Epoch 6006/30000 Training Loss: 0.09253843873739243\n",
      "Epoch 6007/30000 Training Loss: 0.07182743400335312\n",
      "Epoch 6008/30000 Training Loss: 0.08950205892324448\n",
      "Epoch 6009/30000 Training Loss: 0.08096718043088913\n",
      "Epoch 6010/30000 Training Loss: 0.0901699885725975\n",
      "Epoch 6010/30000 Validation Loss: 0.0884125828742981\n",
      "Epoch 6011/30000 Training Loss: 0.0991094708442688\n",
      "Epoch 6012/30000 Training Loss: 0.08450436592102051\n",
      "Epoch 6013/30000 Training Loss: 0.08458421379327774\n",
      "Epoch 6014/30000 Training Loss: 0.07046424597501755\n",
      "Epoch 6015/30000 Training Loss: 0.07507402449846268\n",
      "Epoch 6016/30000 Training Loss: 0.08152279257774353\n",
      "Epoch 6017/30000 Training Loss: 0.07918491959571838\n",
      "Epoch 6018/30000 Training Loss: 0.09768667072057724\n",
      "Epoch 6019/30000 Training Loss: 0.07728419452905655\n",
      "Epoch 6020/30000 Training Loss: 0.09471248835325241\n",
      "Epoch 6020/30000 Validation Loss: 0.06478073447942734\n",
      "Epoch 6021/30000 Training Loss: 0.07917272299528122\n",
      "Epoch 6022/30000 Training Loss: 0.09552398324012756\n",
      "Epoch 6023/30000 Training Loss: 0.08222141861915588\n",
      "Epoch 6024/30000 Training Loss: 0.0890217050909996\n",
      "Epoch 6025/30000 Training Loss: 0.08863721042871475\n",
      "Epoch 6026/30000 Training Loss: 0.08962756395339966\n",
      "Epoch 6027/30000 Training Loss: 0.08495175093412399\n",
      "Epoch 6028/30000 Training Loss: 0.07979894429445267\n",
      "Epoch 6029/30000 Training Loss: 0.08128797262907028\n",
      "Epoch 6030/30000 Training Loss: 0.08949393033981323\n",
      "Epoch 6030/30000 Validation Loss: 0.0757342278957367\n",
      "Epoch 6031/30000 Training Loss: 0.07389584928750992\n",
      "Epoch 6032/30000 Training Loss: 0.07025472074747086\n",
      "Epoch 6033/30000 Training Loss: 0.07533209770917892\n",
      "Epoch 6034/30000 Training Loss: 0.07435048371553421\n",
      "Epoch 6035/30000 Training Loss: 0.08312816172838211\n",
      "Epoch 6036/30000 Training Loss: 0.08996964246034622\n",
      "Epoch 6037/30000 Training Loss: 0.09286510944366455\n",
      "Epoch 6038/30000 Training Loss: 0.07400316745042801\n",
      "Epoch 6039/30000 Training Loss: 0.0685204491019249\n",
      "Epoch 6040/30000 Training Loss: 0.07366391271352768\n",
      "Epoch 6040/30000 Validation Loss: 0.08317634463310242\n",
      "Epoch 6041/30000 Training Loss: 0.11651948094367981\n",
      "Epoch 6042/30000 Training Loss: 0.08263234049081802\n",
      "Epoch 6043/30000 Training Loss: 0.09741729497909546\n",
      "Epoch 6044/30000 Training Loss: 0.07042783498764038\n",
      "Epoch 6045/30000 Training Loss: 0.10626756399869919\n",
      "Epoch 6046/30000 Training Loss: 0.12974433600902557\n",
      "Epoch 6047/30000 Training Loss: 0.07370486110448837\n",
      "Epoch 6048/30000 Training Loss: 0.0911310687661171\n",
      "Epoch 6049/30000 Training Loss: 0.09829334169626236\n",
      "Epoch 6050/30000 Training Loss: 0.1089334487915039\n",
      "Epoch 6050/30000 Validation Loss: 0.10827621072530746\n",
      "Epoch 6051/30000 Training Loss: 0.1196785643696785\n",
      "Epoch 6052/30000 Training Loss: 0.10398270934820175\n",
      "Epoch 6053/30000 Training Loss: 0.09744920581579208\n",
      "Epoch 6054/30000 Training Loss: 0.08281738311052322\n",
      "Epoch 6055/30000 Training Loss: 0.089284248650074\n",
      "Epoch 6056/30000 Training Loss: 0.06460069864988327\n",
      "Epoch 6057/30000 Training Loss: 0.08559039235115051\n",
      "Epoch 6058/30000 Training Loss: 0.08567356318235397\n",
      "Epoch 6059/30000 Training Loss: 0.0707879289984703\n",
      "Epoch 6060/30000 Training Loss: 0.08763381838798523\n",
      "Epoch 6060/30000 Validation Loss: 0.09973383694887161\n",
      "Epoch 6061/30000 Training Loss: 0.06916680932044983\n",
      "Epoch 6062/30000 Training Loss: 0.07871221005916595\n",
      "Epoch 6063/30000 Training Loss: 0.08522612601518631\n",
      "Epoch 6064/30000 Training Loss: 0.09299299865961075\n",
      "Epoch 6065/30000 Training Loss: 0.08633381128311157\n",
      "Epoch 6066/30000 Training Loss: 0.06986058503389359\n",
      "Epoch 6067/30000 Training Loss: 0.10608064383268356\n",
      "Epoch 6068/30000 Training Loss: 0.09152265638113022\n",
      "Epoch 6069/30000 Training Loss: 0.07375288754701614\n",
      "Epoch 6070/30000 Training Loss: 0.09367304295301437\n",
      "Epoch 6070/30000 Validation Loss: 0.07210363447666168\n",
      "Epoch 6071/30000 Training Loss: 0.08850518614053726\n",
      "Epoch 6072/30000 Training Loss: 0.0677260309457779\n",
      "Epoch 6073/30000 Training Loss: 0.07882615923881531\n",
      "Epoch 6074/30000 Training Loss: 0.08139407634735107\n",
      "Epoch 6075/30000 Training Loss: 0.08243802934885025\n",
      "Epoch 6076/30000 Training Loss: 0.07941514998674393\n",
      "Epoch 6077/30000 Training Loss: 0.09030143171548843\n",
      "Epoch 6078/30000 Training Loss: 0.08353609591722488\n",
      "Epoch 6079/30000 Training Loss: 0.07425928860902786\n",
      "Epoch 6080/30000 Training Loss: 0.10843073576688766\n",
      "Epoch 6080/30000 Validation Loss: 0.0760020837187767\n",
      "Epoch 6081/30000 Training Loss: 0.07458236068487167\n",
      "Epoch 6082/30000 Training Loss: 0.09527149796485901\n",
      "Epoch 6083/30000 Training Loss: 0.10583880543708801\n",
      "Epoch 6084/30000 Training Loss: 0.07418670505285263\n",
      "Epoch 6085/30000 Training Loss: 0.07814791053533554\n",
      "Epoch 6086/30000 Training Loss: 0.08339057117700577\n",
      "Epoch 6087/30000 Training Loss: 0.08074497431516647\n",
      "Epoch 6088/30000 Training Loss: 0.08177722245454788\n",
      "Epoch 6089/30000 Training Loss: 0.07105617225170135\n",
      "Epoch 6090/30000 Training Loss: 0.10154791921377182\n",
      "Epoch 6090/30000 Validation Loss: 0.08684899657964706\n",
      "Epoch 6091/30000 Training Loss: 0.07467588037252426\n",
      "Epoch 6092/30000 Training Loss: 0.07883108407258987\n",
      "Epoch 6093/30000 Training Loss: 0.09195024520158768\n",
      "Epoch 6094/30000 Training Loss: 0.08502133935689926\n",
      "Epoch 6095/30000 Training Loss: 0.10279306024312973\n",
      "Epoch 6096/30000 Training Loss: 0.08342891186475754\n",
      "Epoch 6097/30000 Training Loss: 0.0763121247291565\n",
      "Epoch 6098/30000 Training Loss: 0.08330696076154709\n",
      "Epoch 6099/30000 Training Loss: 0.08203475922346115\n",
      "Epoch 6100/30000 Training Loss: 0.09815487265586853\n",
      "Epoch 6100/30000 Validation Loss: 0.08804785460233688\n",
      "Epoch 6101/30000 Training Loss: 0.08200091123580933\n",
      "Epoch 6102/30000 Training Loss: 0.10383396595716476\n",
      "Epoch 6103/30000 Training Loss: 0.091659776866436\n",
      "Epoch 6104/30000 Training Loss: 0.08059608936309814\n",
      "Epoch 6105/30000 Training Loss: 0.08797100931406021\n",
      "Epoch 6106/30000 Training Loss: 0.10736344009637833\n",
      "Epoch 6107/30000 Training Loss: 0.06769406050443649\n",
      "Epoch 6108/30000 Training Loss: 0.0804809182882309\n",
      "Epoch 6109/30000 Training Loss: 0.0866558849811554\n",
      "Epoch 6110/30000 Training Loss: 0.07898176461458206\n",
      "Epoch 6110/30000 Validation Loss: 0.07482527941465378\n",
      "Epoch 6111/30000 Training Loss: 0.06273303925991058\n",
      "Epoch 6112/30000 Training Loss: 0.10468742251396179\n",
      "Epoch 6113/30000 Training Loss: 0.07836300879716873\n",
      "Epoch 6114/30000 Training Loss: 0.08303790539503098\n",
      "Epoch 6115/30000 Training Loss: 0.07828482240438461\n",
      "Epoch 6116/30000 Training Loss: 0.08415261656045914\n",
      "Epoch 6117/30000 Training Loss: 0.11230167746543884\n",
      "Epoch 6118/30000 Training Loss: 0.09206938743591309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6119/30000 Training Loss: 0.10158552974462509\n",
      "Epoch 6120/30000 Training Loss: 0.07838253676891327\n",
      "Epoch 6120/30000 Validation Loss: 0.10203877091407776\n",
      "Epoch 6121/30000 Training Loss: 0.07814794033765793\n",
      "Epoch 6122/30000 Training Loss: 0.08508283644914627\n",
      "Epoch 6123/30000 Training Loss: 0.10349661856889725\n",
      "Epoch 6124/30000 Training Loss: 0.08908172696828842\n",
      "Epoch 6125/30000 Training Loss: 0.10775522142648697\n",
      "Epoch 6126/30000 Training Loss: 0.09129516035318375\n",
      "Epoch 6127/30000 Training Loss: 0.0971449688076973\n",
      "Epoch 6128/30000 Training Loss: 0.09166723489761353\n",
      "Epoch 6129/30000 Training Loss: 0.10346713662147522\n",
      "Epoch 6130/30000 Training Loss: 0.1170106902718544\n",
      "Epoch 6130/30000 Validation Loss: 0.09290921688079834\n",
      "Epoch 6131/30000 Training Loss: 0.06920269131660461\n",
      "Epoch 6132/30000 Training Loss: 0.07695513963699341\n",
      "Epoch 6133/30000 Training Loss: 0.07747332006692886\n",
      "Epoch 6134/30000 Training Loss: 0.09144371747970581\n",
      "Epoch 6135/30000 Training Loss: 0.10018661618232727\n",
      "Epoch 6136/30000 Training Loss: 0.08868974447250366\n",
      "Epoch 6137/30000 Training Loss: 0.07699556648731232\n",
      "Epoch 6138/30000 Training Loss: 0.09150192886590958\n",
      "Epoch 6139/30000 Training Loss: 0.08313015848398209\n",
      "Epoch 6140/30000 Training Loss: 0.08857700973749161\n",
      "Epoch 6140/30000 Validation Loss: 0.08648283034563065\n",
      "Epoch 6141/30000 Training Loss: 0.08141616731882095\n",
      "Epoch 6142/30000 Training Loss: 0.07979566603899002\n",
      "Epoch 6143/30000 Training Loss: 0.060751933604478836\n",
      "Epoch 6144/30000 Training Loss: 0.1159728392958641\n",
      "Epoch 6145/30000 Training Loss: 0.08367346972227097\n",
      "Epoch 6146/30000 Training Loss: 0.10322722792625427\n",
      "Epoch 6147/30000 Training Loss: 0.08316043019294739\n",
      "Epoch 6148/30000 Training Loss: 0.08275871723890305\n",
      "Epoch 6149/30000 Training Loss: 0.0896293893456459\n",
      "Epoch 6150/30000 Training Loss: 0.08154474198818207\n",
      "Epoch 6150/30000 Validation Loss: 0.09146984666585922\n",
      "Epoch 6151/30000 Training Loss: 0.10389098525047302\n",
      "Epoch 6152/30000 Training Loss: 0.07823257893323898\n",
      "Epoch 6153/30000 Training Loss: 0.08673727512359619\n",
      "Epoch 6154/30000 Training Loss: 0.0960390642285347\n",
      "Epoch 6155/30000 Training Loss: 0.08751610666513443\n",
      "Epoch 6156/30000 Training Loss: 0.09618360549211502\n",
      "Epoch 6157/30000 Training Loss: 0.07959944754838943\n",
      "Epoch 6158/30000 Training Loss: 0.0718432143330574\n",
      "Epoch 6159/30000 Training Loss: 0.10289009660482407\n",
      "Epoch 6160/30000 Training Loss: 0.10353913903236389\n",
      "Epoch 6160/30000 Validation Loss: 0.09623173624277115\n",
      "Epoch 6161/30000 Training Loss: 0.08654746413230896\n",
      "Epoch 6162/30000 Training Loss: 0.09888016432523727\n",
      "Epoch 6163/30000 Training Loss: 0.0804034098982811\n",
      "Epoch 6164/30000 Training Loss: 0.08409515023231506\n",
      "Epoch 6165/30000 Training Loss: 0.08590352535247803\n",
      "Epoch 6166/30000 Training Loss: 0.07990144938230515\n",
      "Epoch 6167/30000 Training Loss: 0.09391844272613525\n",
      "Epoch 6168/30000 Training Loss: 0.07710938900709152\n",
      "Epoch 6169/30000 Training Loss: 0.08944206684827805\n",
      "Epoch 6170/30000 Training Loss: 0.07836794853210449\n",
      "Epoch 6170/30000 Validation Loss: 0.08673056960105896\n",
      "Epoch 6171/30000 Training Loss: 0.0839395523071289\n",
      "Epoch 6172/30000 Training Loss: 0.08221393078565598\n",
      "Epoch 6173/30000 Training Loss: 0.08586028963327408\n",
      "Epoch 6174/30000 Training Loss: 0.07680041342973709\n",
      "Epoch 6175/30000 Training Loss: 0.10970167070627213\n",
      "Epoch 6176/30000 Training Loss: 0.09236874431371689\n",
      "Epoch 6177/30000 Training Loss: 0.07542113214731216\n",
      "Epoch 6178/30000 Training Loss: 0.10194575041532516\n",
      "Epoch 6179/30000 Training Loss: 0.06571754813194275\n",
      "Epoch 6180/30000 Training Loss: 0.09835604578256607\n",
      "Epoch 6180/30000 Validation Loss: 0.06658618897199631\n",
      "Epoch 6181/30000 Training Loss: 0.09842995554208755\n",
      "Epoch 6182/30000 Training Loss: 0.09817185997962952\n",
      "Epoch 6183/30000 Training Loss: 0.10529343038797379\n",
      "Epoch 6184/30000 Training Loss: 0.07796704024076462\n",
      "Epoch 6185/30000 Training Loss: 0.09997108578681946\n",
      "Epoch 6186/30000 Training Loss: 0.08003200590610504\n",
      "Epoch 6187/30000 Training Loss: 0.07950099557638168\n",
      "Epoch 6188/30000 Training Loss: 0.08190497756004333\n",
      "Epoch 6189/30000 Training Loss: 0.07933387905359268\n",
      "Epoch 6190/30000 Training Loss: 0.0852428674697876\n",
      "Epoch 6190/30000 Validation Loss: 0.0971754789352417\n",
      "Epoch 6191/30000 Training Loss: 0.07980982214212418\n",
      "Epoch 6192/30000 Training Loss: 0.09786389023065567\n",
      "Epoch 6193/30000 Training Loss: 0.09130341559648514\n",
      "Epoch 6194/30000 Training Loss: 0.12060823291540146\n",
      "Epoch 6195/30000 Training Loss: 0.0772319957613945\n",
      "Epoch 6196/30000 Training Loss: 0.10746798664331436\n",
      "Epoch 6197/30000 Training Loss: 0.0881890282034874\n",
      "Epoch 6198/30000 Training Loss: 0.07714693993330002\n",
      "Epoch 6199/30000 Training Loss: 0.0823950245976448\n",
      "Epoch 6200/30000 Training Loss: 0.09352043271064758\n",
      "Epoch 6200/30000 Validation Loss: 0.08699484914541245\n",
      "Epoch 6201/30000 Training Loss: 0.07252640277147293\n",
      "Epoch 6202/30000 Training Loss: 0.07794059067964554\n",
      "Epoch 6203/30000 Training Loss: 0.09559732675552368\n",
      "Epoch 6204/30000 Training Loss: 0.08043772727251053\n",
      "Epoch 6205/30000 Training Loss: 0.09861990809440613\n",
      "Epoch 6206/30000 Training Loss: 0.0969650074839592\n",
      "Epoch 6207/30000 Training Loss: 0.07755600661039352\n",
      "Epoch 6208/30000 Training Loss: 0.09216352552175522\n",
      "Epoch 6209/30000 Training Loss: 0.06172995641827583\n",
      "Epoch 6210/30000 Training Loss: 0.0852893814444542\n",
      "Epoch 6210/30000 Validation Loss: 0.11489400267601013\n",
      "Epoch 6211/30000 Training Loss: 0.09558254480361938\n",
      "Epoch 6212/30000 Training Loss: 0.09466937929391861\n",
      "Epoch 6213/30000 Training Loss: 0.07566061615943909\n",
      "Epoch 6214/30000 Training Loss: 0.08196847140789032\n",
      "Epoch 6215/30000 Training Loss: 0.09226103872060776\n",
      "Epoch 6216/30000 Training Loss: 0.08465832471847534\n",
      "Epoch 6217/30000 Training Loss: 0.06572576612234116\n",
      "Epoch 6218/30000 Training Loss: 0.11038559675216675\n",
      "Epoch 6219/30000 Training Loss: 0.08323056250810623\n",
      "Epoch 6220/30000 Training Loss: 0.09838486462831497\n",
      "Epoch 6220/30000 Validation Loss: 0.09501860290765762\n",
      "Epoch 6221/30000 Training Loss: 0.08322859555482864\n",
      "Epoch 6222/30000 Training Loss: 0.09276885539293289\n",
      "Epoch 6223/30000 Training Loss: 0.0784880593419075\n",
      "Epoch 6224/30000 Training Loss: 0.07115788012742996\n",
      "Epoch 6225/30000 Training Loss: 0.07790353894233704\n",
      "Epoch 6226/30000 Training Loss: 0.09497472643852234\n",
      "Epoch 6227/30000 Training Loss: 0.08848818391561508\n",
      "Epoch 6228/30000 Training Loss: 0.07610270380973816\n",
      "Epoch 6229/30000 Training Loss: 0.06035879626870155\n",
      "Epoch 6230/30000 Training Loss: 0.08509493619203568\n",
      "Epoch 6230/30000 Validation Loss: 0.08963596075773239\n",
      "Epoch 6231/30000 Training Loss: 0.07352808862924576\n",
      "Epoch 6232/30000 Training Loss: 0.09332636743783951\n",
      "Epoch 6233/30000 Training Loss: 0.07439310103654861\n",
      "Epoch 6234/30000 Training Loss: 0.0808253362774849\n",
      "Epoch 6235/30000 Training Loss: 0.1037059798836708\n",
      "Epoch 6236/30000 Training Loss: 0.07898199558258057\n",
      "Epoch 6237/30000 Training Loss: 0.07973930984735489\n",
      "Epoch 6238/30000 Training Loss: 0.061379075050354004\n",
      "Epoch 6239/30000 Training Loss: 0.0934414267539978\n",
      "Epoch 6240/30000 Training Loss: 0.08206843584775925\n",
      "Epoch 6240/30000 Validation Loss: 0.09695199877023697\n",
      "Epoch 6241/30000 Training Loss: 0.08410630375146866\n",
      "Epoch 6242/30000 Training Loss: 0.08748169988393784\n",
      "Epoch 6243/30000 Training Loss: 0.0642133578658104\n",
      "Epoch 6244/30000 Training Loss: 0.08396754413843155\n",
      "Epoch 6245/30000 Training Loss: 0.08707467466592789\n",
      "Epoch 6246/30000 Training Loss: 0.08831111341714859\n",
      "Epoch 6247/30000 Training Loss: 0.07478991150856018\n",
      "Epoch 6248/30000 Training Loss: 0.07320546358823776\n",
      "Epoch 6249/30000 Training Loss: 0.07078158110380173\n",
      "Epoch 6250/30000 Training Loss: 0.07052826881408691\n",
      "Epoch 6250/30000 Validation Loss: 0.09473994374275208\n",
      "Epoch 6251/30000 Training Loss: 0.08287390321493149\n",
      "Epoch 6252/30000 Training Loss: 0.11479147523641586\n",
      "Epoch 6253/30000 Training Loss: 0.08214984089136124\n",
      "Epoch 6254/30000 Training Loss: 0.07967720925807953\n",
      "Epoch 6255/30000 Training Loss: 0.09977022558450699\n",
      "Epoch 6256/30000 Training Loss: 0.0880826935172081\n",
      "Epoch 6257/30000 Training Loss: 0.09633556753396988\n",
      "Epoch 6258/30000 Training Loss: 0.07875415682792664\n",
      "Epoch 6259/30000 Training Loss: 0.06779354065656662\n",
      "Epoch 6260/30000 Training Loss: 0.09769503027200699\n",
      "Epoch 6260/30000 Validation Loss: 0.08147689700126648\n",
      "Epoch 6261/30000 Training Loss: 0.08810053020715714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6262/30000 Training Loss: 0.06836351752281189\n",
      "Epoch 6263/30000 Training Loss: 0.0688542053103447\n",
      "Epoch 6264/30000 Training Loss: 0.07083433866500854\n",
      "Epoch 6265/30000 Training Loss: 0.08762988448143005\n",
      "Epoch 6266/30000 Training Loss: 0.07830487936735153\n",
      "Epoch 6267/30000 Training Loss: 0.08041087538003922\n",
      "Epoch 6268/30000 Training Loss: 0.07699703425168991\n",
      "Epoch 6269/30000 Training Loss: 0.10357304662466049\n",
      "Epoch 6270/30000 Training Loss: 0.08401936292648315\n",
      "Epoch 6270/30000 Validation Loss: 0.0842113196849823\n",
      "Epoch 6271/30000 Training Loss: 0.11068109422922134\n",
      "Epoch 6272/30000 Training Loss: 0.09214842319488525\n",
      "Epoch 6273/30000 Training Loss: 0.07948888093233109\n",
      "Epoch 6274/30000 Training Loss: 0.09408154338598251\n",
      "Epoch 6275/30000 Training Loss: 0.10811885446310043\n",
      "Epoch 6276/30000 Training Loss: 0.07564261555671692\n",
      "Epoch 6277/30000 Training Loss: 0.09208343178033829\n",
      "Epoch 6278/30000 Training Loss: 0.09656526893377304\n",
      "Epoch 6279/30000 Training Loss: 0.06883170455694199\n",
      "Epoch 6280/30000 Training Loss: 0.08479110151529312\n",
      "Epoch 6280/30000 Validation Loss: 0.09609013795852661\n",
      "Epoch 6281/30000 Training Loss: 0.08958396315574646\n",
      "Epoch 6282/30000 Training Loss: 0.07845756411552429\n",
      "Epoch 6283/30000 Training Loss: 0.08749232441186905\n",
      "Epoch 6284/30000 Training Loss: 0.09156175702810287\n",
      "Epoch 6285/30000 Training Loss: 0.08010903745889664\n",
      "Epoch 6286/30000 Training Loss: 0.0826733186841011\n",
      "Epoch 6287/30000 Training Loss: 0.07544150948524475\n",
      "Epoch 6288/30000 Training Loss: 0.08687881380319595\n",
      "Epoch 6289/30000 Training Loss: 0.07474341243505478\n",
      "Epoch 6290/30000 Training Loss: 0.10030066967010498\n",
      "Epoch 6290/30000 Validation Loss: 0.08502596616744995\n",
      "Epoch 6291/30000 Training Loss: 0.08060741424560547\n",
      "Epoch 6292/30000 Training Loss: 0.10794529318809509\n",
      "Epoch 6293/30000 Training Loss: 0.07557521015405655\n",
      "Epoch 6294/30000 Training Loss: 0.08905971795320511\n",
      "Epoch 6295/30000 Training Loss: 0.09662803262472153\n",
      "Epoch 6296/30000 Training Loss: 0.07168442755937576\n",
      "Epoch 6297/30000 Training Loss: 0.0884077250957489\n",
      "Epoch 6298/30000 Training Loss: 0.10395041853189468\n",
      "Epoch 6299/30000 Training Loss: 0.10245583206415176\n",
      "Epoch 6300/30000 Training Loss: 0.09054971486330032\n",
      "Epoch 6300/30000 Validation Loss: 0.09508734196424484\n",
      "Epoch 6301/30000 Training Loss: 0.11067994683980942\n",
      "Epoch 6302/30000 Training Loss: 0.08674800395965576\n",
      "Epoch 6303/30000 Training Loss: 0.09477675706148148\n",
      "Epoch 6304/30000 Training Loss: 0.10292691737413406\n",
      "Epoch 6305/30000 Training Loss: 0.07023921608924866\n",
      "Epoch 6306/30000 Training Loss: 0.07175423949956894\n",
      "Epoch 6307/30000 Training Loss: 0.08489677309989929\n",
      "Epoch 6308/30000 Training Loss: 0.09314856678247452\n",
      "Epoch 6309/30000 Training Loss: 0.08826034516096115\n",
      "Epoch 6310/30000 Training Loss: 0.10756037384271622\n",
      "Epoch 6310/30000 Validation Loss: 0.09001060575246811\n",
      "Epoch 6311/30000 Training Loss: 0.09255292266607285\n",
      "Epoch 6312/30000 Training Loss: 0.0914677157998085\n",
      "Epoch 6313/30000 Training Loss: 0.08537382632493973\n",
      "Epoch 6314/30000 Training Loss: 0.075896255671978\n",
      "Epoch 6315/30000 Training Loss: 0.09288521856069565\n",
      "Epoch 6316/30000 Training Loss: 0.0917867124080658\n",
      "Epoch 6317/30000 Training Loss: 0.09916602820158005\n",
      "Epoch 6318/30000 Training Loss: 0.05703658238053322\n",
      "Epoch 6319/30000 Training Loss: 0.08223665505647659\n",
      "Epoch 6320/30000 Training Loss: 0.09053326398134232\n",
      "Epoch 6320/30000 Validation Loss: 0.09707184880971909\n",
      "Epoch 6321/30000 Training Loss: 0.08638224750757217\n",
      "Epoch 6322/30000 Training Loss: 0.0884103998541832\n",
      "Epoch 6323/30000 Training Loss: 0.10141565650701523\n",
      "Epoch 6324/30000 Training Loss: 0.07127576321363449\n",
      "Epoch 6325/30000 Training Loss: 0.08437181264162064\n",
      "Epoch 6326/30000 Training Loss: 0.07321986556053162\n",
      "Epoch 6327/30000 Training Loss: 0.06594853848218918\n",
      "Epoch 6328/30000 Training Loss: 0.10843167454004288\n",
      "Epoch 6329/30000 Training Loss: 0.10274070501327515\n",
      "Epoch 6330/30000 Training Loss: 0.09897705912590027\n",
      "Epoch 6330/30000 Validation Loss: 0.08545953035354614\n",
      "Epoch 6331/30000 Training Loss: 0.09454865008592606\n",
      "Epoch 6332/30000 Training Loss: 0.0855572447180748\n",
      "Epoch 6333/30000 Training Loss: 0.08591168373823166\n",
      "Epoch 6334/30000 Training Loss: 0.10143057256937027\n",
      "Epoch 6335/30000 Training Loss: 0.09608015418052673\n",
      "Epoch 6336/30000 Training Loss: 0.08647433668375015\n",
      "Epoch 6337/30000 Training Loss: 0.07137178629636765\n",
      "Epoch 6338/30000 Training Loss: 0.09323308616876602\n",
      "Epoch 6339/30000 Training Loss: 0.09473925828933716\n",
      "Epoch 6340/30000 Training Loss: 0.06962001323699951\n",
      "Epoch 6340/30000 Validation Loss: 0.0732368603348732\n",
      "Epoch 6341/30000 Training Loss: 0.10205812007188797\n",
      "Epoch 6342/30000 Training Loss: 0.08922794461250305\n",
      "Epoch 6343/30000 Training Loss: 0.09517929702997208\n",
      "Epoch 6344/30000 Training Loss: 0.09069963544607162\n",
      "Epoch 6345/30000 Training Loss: 0.07726728916168213\n",
      "Epoch 6346/30000 Training Loss: 0.08419638127088547\n",
      "Epoch 6347/30000 Training Loss: 0.0850181058049202\n",
      "Epoch 6348/30000 Training Loss: 0.07392394542694092\n",
      "Epoch 6349/30000 Training Loss: 0.07582572847604752\n",
      "Epoch 6350/30000 Training Loss: 0.07600206136703491\n",
      "Epoch 6350/30000 Validation Loss: 0.09471803158521652\n",
      "Epoch 6351/30000 Training Loss: 0.08907734602689743\n",
      "Epoch 6352/30000 Training Loss: 0.0958298072218895\n",
      "Epoch 6353/30000 Training Loss: 0.07315993309020996\n",
      "Epoch 6354/30000 Training Loss: 0.07905063778162003\n",
      "Epoch 6355/30000 Training Loss: 0.08161794394254684\n",
      "Epoch 6356/30000 Training Loss: 0.0808848887681961\n",
      "Epoch 6357/30000 Training Loss: 0.08632934093475342\n",
      "Epoch 6358/30000 Training Loss: 0.06721494346857071\n",
      "Epoch 6359/30000 Training Loss: 0.0812549963593483\n",
      "Epoch 6360/30000 Training Loss: 0.08797814697027206\n",
      "Epoch 6360/30000 Validation Loss: 0.0726599395275116\n",
      "Epoch 6361/30000 Training Loss: 0.09702339768409729\n",
      "Epoch 6362/30000 Training Loss: 0.08410047739744186\n",
      "Epoch 6363/30000 Training Loss: 0.08700466901063919\n",
      "Epoch 6364/30000 Training Loss: 0.11711031198501587\n",
      "Epoch 6365/30000 Training Loss: 0.0871812105178833\n",
      "Epoch 6366/30000 Training Loss: 0.07456067204475403\n",
      "Epoch 6367/30000 Training Loss: 0.0794486254453659\n",
      "Epoch 6368/30000 Training Loss: 0.06230051442980766\n",
      "Epoch 6369/30000 Training Loss: 0.0688672587275505\n",
      "Epoch 6370/30000 Training Loss: 0.11464495211839676\n",
      "Epoch 6370/30000 Validation Loss: 0.09139009565114975\n",
      "Epoch 6371/30000 Training Loss: 0.0959959328174591\n",
      "Epoch 6372/30000 Training Loss: 0.07589306682348251\n",
      "Epoch 6373/30000 Training Loss: 0.07921040058135986\n",
      "Epoch 6374/30000 Training Loss: 0.0863368883728981\n",
      "Epoch 6375/30000 Training Loss: 0.11456071585416794\n",
      "Epoch 6376/30000 Training Loss: 0.07241240888834\n",
      "Epoch 6377/30000 Training Loss: 0.07850510627031326\n",
      "Epoch 6378/30000 Training Loss: 0.06889761239290237\n",
      "Epoch 6379/30000 Training Loss: 0.10905689746141434\n",
      "Epoch 6380/30000 Training Loss: 0.09195689111948013\n",
      "Epoch 6380/30000 Validation Loss: 0.09944617003202438\n",
      "Epoch 6381/30000 Training Loss: 0.08532004803419113\n",
      "Epoch 6382/30000 Training Loss: 0.09265787154436111\n",
      "Epoch 6383/30000 Training Loss: 0.07461554557085037\n",
      "Epoch 6384/30000 Training Loss: 0.07865529507398605\n",
      "Epoch 6385/30000 Training Loss: 0.09760227054357529\n",
      "Epoch 6386/30000 Training Loss: 0.10311895608901978\n",
      "Epoch 6387/30000 Training Loss: 0.09678617864847183\n",
      "Epoch 6388/30000 Training Loss: 0.06629923731088638\n",
      "Epoch 6389/30000 Training Loss: 0.06753332167863846\n",
      "Epoch 6390/30000 Training Loss: 0.08819592744112015\n",
      "Epoch 6390/30000 Validation Loss: 0.07919704169034958\n",
      "Epoch 6391/30000 Training Loss: 0.11837389320135117\n",
      "Epoch 6392/30000 Training Loss: 0.07070297747850418\n",
      "Epoch 6393/30000 Training Loss: 0.07777276635169983\n",
      "Epoch 6394/30000 Training Loss: 0.07643388956785202\n",
      "Epoch 6395/30000 Training Loss: 0.09448155015707016\n",
      "Epoch 6396/30000 Training Loss: 0.07071859389543533\n",
      "Epoch 6397/30000 Training Loss: 0.08067193627357483\n",
      "Epoch 6398/30000 Training Loss: 0.0716034546494484\n",
      "Epoch 6399/30000 Training Loss: 0.0872216746211052\n",
      "Epoch 6400/30000 Training Loss: 0.09197064489126205\n",
      "Epoch 6400/30000 Validation Loss: 0.09299597144126892\n",
      "Epoch 6401/30000 Training Loss: 0.08965124934911728\n",
      "Epoch 6402/30000 Training Loss: 0.08724772930145264\n",
      "Epoch 6403/30000 Training Loss: 0.1060110554099083\n",
      "Epoch 6404/30000 Training Loss: 0.08155932277441025\n",
      "Epoch 6405/30000 Training Loss: 0.07593091577291489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6406/30000 Training Loss: 0.08376754075288773\n",
      "Epoch 6407/30000 Training Loss: 0.0978819727897644\n",
      "Epoch 6408/30000 Training Loss: 0.06872528046369553\n",
      "Epoch 6409/30000 Training Loss: 0.08861923962831497\n",
      "Epoch 6410/30000 Training Loss: 0.08486628532409668\n",
      "Epoch 6410/30000 Validation Loss: 0.0708426907658577\n",
      "Epoch 6411/30000 Training Loss: 0.09648271650075912\n",
      "Epoch 6412/30000 Training Loss: 0.0809299647808075\n",
      "Epoch 6413/30000 Training Loss: 0.1065998300909996\n",
      "Epoch 6414/30000 Training Loss: 0.0854271724820137\n",
      "Epoch 6415/30000 Training Loss: 0.08445676416158676\n",
      "Epoch 6416/30000 Training Loss: 0.0903930738568306\n",
      "Epoch 6417/30000 Training Loss: 0.10887753218412399\n",
      "Epoch 6418/30000 Training Loss: 0.07580648362636566\n",
      "Epoch 6419/30000 Training Loss: 0.0855659544467926\n",
      "Epoch 6420/30000 Training Loss: 0.07110860198736191\n",
      "Epoch 6420/30000 Validation Loss: 0.07896992564201355\n",
      "Epoch 6421/30000 Training Loss: 0.08850038051605225\n",
      "Epoch 6422/30000 Training Loss: 0.07820513844490051\n",
      "Epoch 6423/30000 Training Loss: 0.10437702387571335\n",
      "Epoch 6424/30000 Training Loss: 0.09197964519262314\n",
      "Epoch 6425/30000 Training Loss: 0.0781983956694603\n",
      "Epoch 6426/30000 Training Loss: 0.07500825077295303\n",
      "Epoch 6427/30000 Training Loss: 0.07986293733119965\n",
      "Epoch 6428/30000 Training Loss: 0.06839125603437424\n",
      "Epoch 6429/30000 Training Loss: 0.06896116584539413\n",
      "Epoch 6430/30000 Training Loss: 0.09264549612998962\n",
      "Epoch 6430/30000 Validation Loss: 0.06996612995862961\n",
      "Epoch 6431/30000 Training Loss: 0.09752136468887329\n",
      "Epoch 6432/30000 Training Loss: 0.065345399081707\n",
      "Epoch 6433/30000 Training Loss: 0.09070416539907455\n",
      "Epoch 6434/30000 Training Loss: 0.10620295256376266\n",
      "Epoch 6435/30000 Training Loss: 0.10217391699552536\n",
      "Epoch 6436/30000 Training Loss: 0.06384202092885971\n",
      "Epoch 6437/30000 Training Loss: 0.0771915540099144\n",
      "Epoch 6438/30000 Training Loss: 0.06078149005770683\n",
      "Epoch 6439/30000 Training Loss: 0.07481642812490463\n",
      "Epoch 6440/30000 Training Loss: 0.08324333280324936\n",
      "Epoch 6440/30000 Validation Loss: 0.08264818787574768\n",
      "Epoch 6441/30000 Training Loss: 0.06899262219667435\n",
      "Epoch 6442/30000 Training Loss: 0.09746279567480087\n",
      "Epoch 6443/30000 Training Loss: 0.06296514719724655\n",
      "Epoch 6444/30000 Training Loss: 0.07617288082838058\n",
      "Epoch 6445/30000 Training Loss: 0.07705941796302795\n",
      "Epoch 6446/30000 Training Loss: 0.07180372625589371\n",
      "Epoch 6447/30000 Training Loss: 0.08424752205610275\n",
      "Epoch 6448/30000 Training Loss: 0.08497590571641922\n",
      "Epoch 6449/30000 Training Loss: 0.09826041013002396\n",
      "Epoch 6450/30000 Training Loss: 0.09988749772310257\n",
      "Epoch 6450/30000 Validation Loss: 0.08921527117490768\n",
      "Epoch 6451/30000 Training Loss: 0.07542689144611359\n",
      "Epoch 6452/30000 Training Loss: 0.08830489963293076\n",
      "Epoch 6453/30000 Training Loss: 0.09451425075531006\n",
      "Epoch 6454/30000 Training Loss: 0.08285989612340927\n",
      "Epoch 6455/30000 Training Loss: 0.08247407525777817\n",
      "Epoch 6456/30000 Training Loss: 0.09123256057500839\n",
      "Epoch 6457/30000 Training Loss: 0.10940318554639816\n",
      "Epoch 6458/30000 Training Loss: 0.08494007587432861\n",
      "Epoch 6459/30000 Training Loss: 0.09159360080957413\n",
      "Epoch 6460/30000 Training Loss: 0.08354249596595764\n",
      "Epoch 6460/30000 Validation Loss: 0.076458640396595\n",
      "Epoch 6461/30000 Training Loss: 0.09877864271402359\n",
      "Epoch 6462/30000 Training Loss: 0.0837896540760994\n",
      "Epoch 6463/30000 Training Loss: 0.08300191164016724\n",
      "Epoch 6464/30000 Training Loss: 0.07768941670656204\n",
      "Epoch 6465/30000 Training Loss: 0.07719045132398605\n",
      "Epoch 6466/30000 Training Loss: 0.10504645109176636\n",
      "Epoch 6467/30000 Training Loss: 0.09010607749223709\n",
      "Epoch 6468/30000 Training Loss: 0.08319040387868881\n",
      "Epoch 6469/30000 Training Loss: 0.07821928709745407\n",
      "Epoch 6470/30000 Training Loss: 0.09230981022119522\n",
      "Epoch 6470/30000 Validation Loss: 0.07330728322267532\n",
      "Epoch 6471/30000 Training Loss: 0.08369427174329758\n",
      "Epoch 6472/30000 Training Loss: 0.06537693738937378\n",
      "Epoch 6473/30000 Training Loss: 0.07555387169122696\n",
      "Epoch 6474/30000 Training Loss: 0.09226304292678833\n",
      "Epoch 6475/30000 Training Loss: 0.08504325151443481\n",
      "Epoch 6476/30000 Training Loss: 0.12723985314369202\n",
      "Epoch 6477/30000 Training Loss: 0.10693377256393433\n",
      "Epoch 6478/30000 Training Loss: 0.09962735325098038\n",
      "Epoch 6479/30000 Training Loss: 0.09177152067422867\n",
      "Epoch 6480/30000 Training Loss: 0.09277317672967911\n",
      "Epoch 6480/30000 Validation Loss: 0.06282690912485123\n",
      "Epoch 6481/30000 Training Loss: 0.08060934394598007\n",
      "Epoch 6482/30000 Training Loss: 0.08061640709638596\n",
      "Epoch 6483/30000 Training Loss: 0.10160479694604874\n",
      "Epoch 6484/30000 Training Loss: 0.08415565639734268\n",
      "Epoch 6485/30000 Training Loss: 0.09298805147409439\n",
      "Epoch 6486/30000 Training Loss: 0.09597175568342209\n",
      "Epoch 6487/30000 Training Loss: 0.07993515580892563\n",
      "Epoch 6488/30000 Training Loss: 0.062372297048568726\n",
      "Epoch 6489/30000 Training Loss: 0.07186320424079895\n",
      "Epoch 6490/30000 Training Loss: 0.09942708164453506\n",
      "Epoch 6490/30000 Validation Loss: 0.07325267046689987\n",
      "Epoch 6491/30000 Training Loss: 0.07735992968082428\n",
      "Epoch 6492/30000 Training Loss: 0.07930482178926468\n",
      "Epoch 6493/30000 Training Loss: 0.07745032757520676\n",
      "Epoch 6494/30000 Training Loss: 0.093960702419281\n",
      "Epoch 6495/30000 Training Loss: 0.10278898477554321\n",
      "Epoch 6496/30000 Training Loss: 0.07375551760196686\n",
      "Epoch 6497/30000 Training Loss: 0.09291595220565796\n",
      "Epoch 6498/30000 Training Loss: 0.09252718091011047\n",
      "Epoch 6499/30000 Training Loss: 0.07677111774682999\n",
      "Epoch 6500/30000 Training Loss: 0.08896202594041824\n",
      "Epoch 6500/30000 Validation Loss: 0.09263143688440323\n",
      "Epoch 6501/30000 Training Loss: 0.10982302576303482\n",
      "Epoch 6502/30000 Training Loss: 0.08757530897855759\n",
      "Epoch 6503/30000 Training Loss: 0.09100347757339478\n",
      "Epoch 6504/30000 Training Loss: 0.07608136534690857\n",
      "Epoch 6505/30000 Training Loss: 0.07757046818733215\n",
      "Epoch 6506/30000 Training Loss: 0.08534719794988632\n",
      "Epoch 6507/30000 Training Loss: 0.09551156312227249\n",
      "Epoch 6508/30000 Training Loss: 0.09546134620904922\n",
      "Epoch 6509/30000 Training Loss: 0.05617917329072952\n",
      "Epoch 6510/30000 Training Loss: 0.07987037301063538\n",
      "Epoch 6510/30000 Validation Loss: 0.0824979916214943\n",
      "Epoch 6511/30000 Training Loss: 0.10000398755073547\n",
      "Epoch 6512/30000 Training Loss: 0.08952879160642624\n",
      "Epoch 6513/30000 Training Loss: 0.09361625462770462\n",
      "Epoch 6514/30000 Training Loss: 0.10386663675308228\n",
      "Epoch 6515/30000 Training Loss: 0.07305887341499329\n",
      "Epoch 6516/30000 Training Loss: 0.07327265292406082\n",
      "Epoch 6517/30000 Training Loss: 0.0994686707854271\n",
      "Epoch 6518/30000 Training Loss: 0.08056927472352982\n",
      "Epoch 6519/30000 Training Loss: 0.08568743616342545\n",
      "Epoch 6520/30000 Training Loss: 0.0715315118432045\n",
      "Epoch 6520/30000 Validation Loss: 0.10466394573450089\n",
      "Epoch 6521/30000 Training Loss: 0.07547790557146072\n",
      "Epoch 6522/30000 Training Loss: 0.09839984774589539\n",
      "Epoch 6523/30000 Training Loss: 0.06780537217855453\n",
      "Epoch 6524/30000 Training Loss: 0.08381250500679016\n",
      "Epoch 6525/30000 Training Loss: 0.07190492004156113\n",
      "Epoch 6526/30000 Training Loss: 0.06544951349496841\n",
      "Epoch 6527/30000 Training Loss: 0.08108056336641312\n",
      "Epoch 6528/30000 Training Loss: 0.06878006458282471\n",
      "Epoch 6529/30000 Training Loss: 0.07870204001665115\n",
      "Epoch 6530/30000 Training Loss: 0.07767271995544434\n",
      "Epoch 6530/30000 Validation Loss: 0.09693165868520737\n",
      "Epoch 6531/30000 Training Loss: 0.08009310811758041\n",
      "Epoch 6532/30000 Training Loss: 0.09384755045175552\n",
      "Epoch 6533/30000 Training Loss: 0.09215661138296127\n",
      "Epoch 6534/30000 Training Loss: 0.07618194818496704\n",
      "Epoch 6535/30000 Training Loss: 0.07471012324094772\n",
      "Epoch 6536/30000 Training Loss: 0.07931237667798996\n",
      "Epoch 6537/30000 Training Loss: 0.06499382108449936\n",
      "Epoch 6538/30000 Training Loss: 0.11699765920639038\n",
      "Epoch 6539/30000 Training Loss: 0.07401973754167557\n",
      "Epoch 6540/30000 Training Loss: 0.0934353768825531\n",
      "Epoch 6540/30000 Validation Loss: 0.07702004909515381\n",
      "Epoch 6541/30000 Training Loss: 0.09036335349082947\n",
      "Epoch 6542/30000 Training Loss: 0.12329552322626114\n",
      "Epoch 6543/30000 Training Loss: 0.08271380513906479\n",
      "Epoch 6544/30000 Training Loss: 0.08988014608621597\n",
      "Epoch 6545/30000 Training Loss: 0.08065315335988998\n",
      "Epoch 6546/30000 Training Loss: 0.07243452966213226\n",
      "Epoch 6547/30000 Training Loss: 0.10265954583883286\n",
      "Epoch 6548/30000 Training Loss: 0.07377300411462784\n",
      "Epoch 6549/30000 Training Loss: 0.10027257353067398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6550/30000 Training Loss: 0.08093833923339844\n",
      "Epoch 6550/30000 Validation Loss: 0.09829583019018173\n",
      "Epoch 6551/30000 Training Loss: 0.0953371450304985\n",
      "Epoch 6552/30000 Training Loss: 0.08640410751104355\n",
      "Epoch 6553/30000 Training Loss: 0.07913107424974442\n",
      "Epoch 6554/30000 Training Loss: 0.06890671700239182\n",
      "Epoch 6555/30000 Training Loss: 0.08989424258470535\n",
      "Epoch 6556/30000 Training Loss: 0.10445445030927658\n",
      "Epoch 6557/30000 Training Loss: 0.08818739652633667\n",
      "Epoch 6558/30000 Training Loss: 0.12252228707075119\n",
      "Epoch 6559/30000 Training Loss: 0.07781800627708435\n",
      "Epoch 6560/30000 Training Loss: 0.10282286256551743\n",
      "Epoch 6560/30000 Validation Loss: 0.07060848921537399\n",
      "Epoch 6561/30000 Training Loss: 0.08808884769678116\n",
      "Epoch 6562/30000 Training Loss: 0.08990201354026794\n",
      "Epoch 6563/30000 Training Loss: 0.09241916984319687\n",
      "Epoch 6564/30000 Training Loss: 0.09595324844121933\n",
      "Epoch 6565/30000 Training Loss: 0.09452889114618301\n",
      "Epoch 6566/30000 Training Loss: 0.0936492308974266\n",
      "Epoch 6567/30000 Training Loss: 0.06395566463470459\n",
      "Epoch 6568/30000 Training Loss: 0.08610561490058899\n",
      "Epoch 6569/30000 Training Loss: 0.07022178918123245\n",
      "Epoch 6570/30000 Training Loss: 0.09985566884279251\n",
      "Epoch 6570/30000 Validation Loss: 0.09709062427282333\n",
      "Epoch 6571/30000 Training Loss: 0.1043943539261818\n",
      "Epoch 6572/30000 Training Loss: 0.08770161867141724\n",
      "Epoch 6573/30000 Training Loss: 0.07411781698465347\n",
      "Epoch 6574/30000 Training Loss: 0.0992816612124443\n",
      "Epoch 6575/30000 Training Loss: 0.08192206174135208\n",
      "Epoch 6576/30000 Training Loss: 0.07986881583929062\n",
      "Epoch 6577/30000 Training Loss: 0.07609642297029495\n",
      "Epoch 6578/30000 Training Loss: 0.10878195613622665\n",
      "Epoch 6579/30000 Training Loss: 0.08405051380395889\n",
      "Epoch 6580/30000 Training Loss: 0.08177756518125534\n",
      "Epoch 6580/30000 Validation Loss: 0.09045040607452393\n",
      "Epoch 6581/30000 Training Loss: 0.10780268162488937\n",
      "Epoch 6582/30000 Training Loss: 0.08308491110801697\n",
      "Epoch 6583/30000 Training Loss: 0.07202138751745224\n",
      "Epoch 6584/30000 Training Loss: 0.10730001330375671\n",
      "Epoch 6585/30000 Training Loss: 0.09312786906957626\n",
      "Epoch 6586/30000 Training Loss: 0.0854087695479393\n",
      "Epoch 6587/30000 Training Loss: 0.09693799167871475\n",
      "Epoch 6588/30000 Training Loss: 0.09927788376808167\n",
      "Epoch 6589/30000 Training Loss: 0.08748040348291397\n",
      "Epoch 6590/30000 Training Loss: 0.08909797668457031\n",
      "Epoch 6590/30000 Validation Loss: 0.08028046786785126\n",
      "Epoch 6591/30000 Training Loss: 0.10540150851011276\n",
      "Epoch 6592/30000 Training Loss: 0.09839177876710892\n",
      "Epoch 6593/30000 Training Loss: 0.08342939615249634\n",
      "Epoch 6594/30000 Training Loss: 0.07236575335264206\n",
      "Epoch 6595/30000 Training Loss: 0.07852170616388321\n",
      "Epoch 6596/30000 Training Loss: 0.0837334394454956\n",
      "Epoch 6597/30000 Training Loss: 0.06308870762586594\n",
      "Epoch 6598/30000 Training Loss: 0.1005760058760643\n",
      "Epoch 6599/30000 Training Loss: 0.08274582773447037\n",
      "Epoch 6600/30000 Training Loss: 0.09077558666467667\n",
      "Epoch 6600/30000 Validation Loss: 0.07559534162282944\n",
      "Epoch 6601/30000 Training Loss: 0.07830306142568588\n",
      "Epoch 6602/30000 Training Loss: 0.09641183167695999\n",
      "Epoch 6603/30000 Training Loss: 0.08583491295576096\n",
      "Epoch 6604/30000 Training Loss: 0.11474131792783737\n",
      "Epoch 6605/30000 Training Loss: 0.09502164274454117\n",
      "Epoch 6606/30000 Training Loss: 0.08315465599298477\n",
      "Epoch 6607/30000 Training Loss: 0.11231732368469238\n",
      "Epoch 6608/30000 Training Loss: 0.10733111947774887\n",
      "Epoch 6609/30000 Training Loss: 0.07859762012958527\n",
      "Epoch 6610/30000 Training Loss: 0.06859155744314194\n",
      "Epoch 6610/30000 Validation Loss: 0.10354482382535934\n",
      "Epoch 6611/30000 Training Loss: 0.10186316817998886\n",
      "Epoch 6612/30000 Training Loss: 0.08196519315242767\n",
      "Epoch 6613/30000 Training Loss: 0.0794939249753952\n",
      "Epoch 6614/30000 Training Loss: 0.07787295430898666\n",
      "Epoch 6615/30000 Training Loss: 0.08036388456821442\n",
      "Epoch 6616/30000 Training Loss: 0.09228993207216263\n",
      "Epoch 6617/30000 Training Loss: 0.10402894765138626\n",
      "Epoch 6618/30000 Training Loss: 0.11503062397241592\n",
      "Epoch 6619/30000 Training Loss: 0.08959843963384628\n",
      "Epoch 6620/30000 Training Loss: 0.09432399272918701\n",
      "Epoch 6620/30000 Validation Loss: 0.07039497047662735\n",
      "Epoch 6621/30000 Training Loss: 0.07091382145881653\n",
      "Epoch 6622/30000 Training Loss: 0.1165914312005043\n",
      "Epoch 6623/30000 Training Loss: 0.09503338485956192\n",
      "Epoch 6624/30000 Training Loss: 0.07403618842363358\n",
      "Epoch 6625/30000 Training Loss: 0.09965261071920395\n",
      "Epoch 6626/30000 Training Loss: 0.10443934053182602\n",
      "Epoch 6627/30000 Training Loss: 0.0686144232749939\n",
      "Epoch 6628/30000 Training Loss: 0.09683289378881454\n",
      "Epoch 6629/30000 Training Loss: 0.08626732975244522\n",
      "Epoch 6630/30000 Training Loss: 0.07996966689825058\n",
      "Epoch 6630/30000 Validation Loss: 0.07302653044462204\n",
      "Epoch 6631/30000 Training Loss: 0.07892896980047226\n",
      "Epoch 6632/30000 Training Loss: 0.07867351919412613\n",
      "Epoch 6633/30000 Training Loss: 0.08270212262868881\n",
      "Epoch 6634/30000 Training Loss: 0.09677159786224365\n",
      "Epoch 6635/30000 Training Loss: 0.10572264343500137\n",
      "Epoch 6636/30000 Training Loss: 0.09269189834594727\n",
      "Epoch 6637/30000 Training Loss: 0.08565034717321396\n",
      "Epoch 6638/30000 Training Loss: 0.07513902336359024\n",
      "Epoch 6639/30000 Training Loss: 0.0856454074382782\n",
      "Epoch 6640/30000 Training Loss: 0.10921045392751694\n",
      "Epoch 6640/30000 Validation Loss: 0.08707944303750992\n",
      "Epoch 6641/30000 Training Loss: 0.09893491119146347\n",
      "Epoch 6642/30000 Training Loss: 0.09459143877029419\n",
      "Epoch 6643/30000 Training Loss: 0.08370711654424667\n",
      "Epoch 6644/30000 Training Loss: 0.11469045281410217\n",
      "Epoch 6645/30000 Training Loss: 0.083880715072155\n",
      "Epoch 6646/30000 Training Loss: 0.09044274687767029\n",
      "Epoch 6647/30000 Training Loss: 0.11752989143133163\n",
      "Epoch 6648/30000 Training Loss: 0.09362957626581192\n",
      "Epoch 6649/30000 Training Loss: 0.07216446101665497\n",
      "Epoch 6650/30000 Training Loss: 0.10160452872514725\n",
      "Epoch 6650/30000 Validation Loss: 0.1015637144446373\n",
      "Epoch 6651/30000 Training Loss: 0.10376671701669693\n",
      "Epoch 6652/30000 Training Loss: 0.07555722445249557\n",
      "Epoch 6653/30000 Training Loss: 0.08850663900375366\n",
      "Epoch 6654/30000 Training Loss: 0.06932085752487183\n",
      "Epoch 6655/30000 Training Loss: 0.0905909314751625\n",
      "Epoch 6656/30000 Training Loss: 0.07329194992780685\n",
      "Epoch 6657/30000 Training Loss: 0.07899557054042816\n",
      "Epoch 6658/30000 Training Loss: 0.08226606994867325\n",
      "Epoch 6659/30000 Training Loss: 0.08627726882696152\n",
      "Epoch 6660/30000 Training Loss: 0.07942875474691391\n",
      "Epoch 6660/30000 Validation Loss: 0.07498776912689209\n",
      "Epoch 6661/30000 Training Loss: 0.06389004737138748\n",
      "Epoch 6662/30000 Training Loss: 0.11551354080438614\n",
      "Epoch 6663/30000 Training Loss: 0.10026079416275024\n",
      "Epoch 6664/30000 Training Loss: 0.10064273327589035\n",
      "Epoch 6665/30000 Training Loss: 0.07707422971725464\n",
      "Epoch 6666/30000 Training Loss: 0.08164463192224503\n",
      "Epoch 6667/30000 Training Loss: 0.08719712495803833\n",
      "Epoch 6668/30000 Training Loss: 0.09534206241369247\n",
      "Epoch 6669/30000 Training Loss: 0.10031899064779282\n",
      "Epoch 6670/30000 Training Loss: 0.09369724988937378\n",
      "Epoch 6670/30000 Validation Loss: 0.06885881721973419\n",
      "Epoch 6671/30000 Training Loss: 0.08898844569921494\n",
      "Epoch 6672/30000 Training Loss: 0.1084553673863411\n",
      "Epoch 6673/30000 Training Loss: 0.07381817698478699\n",
      "Epoch 6674/30000 Training Loss: 0.09415892511606216\n",
      "Epoch 6675/30000 Training Loss: 0.08383064717054367\n",
      "Epoch 6676/30000 Training Loss: 0.08903708308935165\n",
      "Epoch 6677/30000 Training Loss: 0.09347792714834213\n",
      "Epoch 6678/30000 Training Loss: 0.09019780158996582\n",
      "Epoch 6679/30000 Training Loss: 0.0838308110833168\n",
      "Epoch 6680/30000 Training Loss: 0.10678843408823013\n",
      "Epoch 6680/30000 Validation Loss: 0.08098851889371872\n",
      "Epoch 6681/30000 Training Loss: 0.08398512005805969\n",
      "Epoch 6682/30000 Training Loss: 0.08274104446172714\n",
      "Epoch 6683/30000 Training Loss: 0.08253992348909378\n",
      "Epoch 6684/30000 Training Loss: 0.07613322883844376\n",
      "Epoch 6685/30000 Training Loss: 0.11697554588317871\n",
      "Epoch 6686/30000 Training Loss: 0.08126544207334518\n",
      "Epoch 6687/30000 Training Loss: 0.07131896167993546\n",
      "Epoch 6688/30000 Training Loss: 0.08449991792440414\n",
      "Epoch 6689/30000 Training Loss: 0.08631465584039688\n",
      "Epoch 6690/30000 Training Loss: 0.08720000833272934\n",
      "Epoch 6690/30000 Validation Loss: 0.0918642058968544\n",
      "Epoch 6691/30000 Training Loss: 0.09633395075798035\n",
      "Epoch 6692/30000 Training Loss: 0.07622205466032028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6693/30000 Training Loss: 0.07826805114746094\n",
      "Epoch 6694/30000 Training Loss: 0.09968599677085876\n",
      "Epoch 6695/30000 Training Loss: 0.07493054121732712\n",
      "Epoch 6696/30000 Training Loss: 0.10044115036725998\n",
      "Epoch 6697/30000 Training Loss: 0.08952780812978745\n",
      "Epoch 6698/30000 Training Loss: 0.08104179054498672\n",
      "Epoch 6699/30000 Training Loss: 0.07898225635290146\n",
      "Epoch 6700/30000 Training Loss: 0.07366956025362015\n",
      "Epoch 6700/30000 Validation Loss: 0.0826946496963501\n",
      "Epoch 6701/30000 Training Loss: 0.10294154286384583\n",
      "Epoch 6702/30000 Training Loss: 0.10135851055383682\n",
      "Epoch 6703/30000 Training Loss: 0.09599331766366959\n",
      "Epoch 6704/30000 Training Loss: 0.07800041884183884\n",
      "Epoch 6705/30000 Training Loss: 0.10709252208471298\n",
      "Epoch 6706/30000 Training Loss: 0.097966767847538\n",
      "Epoch 6707/30000 Training Loss: 0.07896158844232559\n",
      "Epoch 6708/30000 Training Loss: 0.07354620844125748\n",
      "Epoch 6709/30000 Training Loss: 0.08759254217147827\n",
      "Epoch 6710/30000 Training Loss: 0.10647866874933243\n",
      "Epoch 6710/30000 Validation Loss: 0.07840252667665482\n",
      "Epoch 6711/30000 Training Loss: 0.07090591639280319\n",
      "Epoch 6712/30000 Training Loss: 0.0973089262843132\n",
      "Epoch 6713/30000 Training Loss: 0.07743018120527267\n",
      "Epoch 6714/30000 Training Loss: 0.09060436487197876\n",
      "Epoch 6715/30000 Training Loss: 0.10333507508039474\n",
      "Epoch 6716/30000 Training Loss: 0.10259241610765457\n",
      "Epoch 6717/30000 Training Loss: 0.08723536878824234\n",
      "Epoch 6718/30000 Training Loss: 0.07802120596170425\n",
      "Epoch 6719/30000 Training Loss: 0.06948934495449066\n",
      "Epoch 6720/30000 Training Loss: 0.06877496838569641\n",
      "Epoch 6720/30000 Validation Loss: 0.07833161950111389\n",
      "Epoch 6721/30000 Training Loss: 0.09365615248680115\n",
      "Epoch 6722/30000 Training Loss: 0.07883777469396591\n",
      "Epoch 6723/30000 Training Loss: 0.07057339698076248\n",
      "Epoch 6724/30000 Training Loss: 0.07364870607852936\n",
      "Epoch 6725/30000 Training Loss: 0.0985984206199646\n",
      "Epoch 6726/30000 Training Loss: 0.06769970059394836\n",
      "Epoch 6727/30000 Training Loss: 0.11747825145721436\n",
      "Epoch 6728/30000 Training Loss: 0.09879982471466064\n",
      "Epoch 6729/30000 Training Loss: 0.08682539314031601\n",
      "Epoch 6730/30000 Training Loss: 0.11026320606470108\n",
      "Epoch 6730/30000 Validation Loss: 0.09656861424446106\n",
      "Epoch 6731/30000 Training Loss: 0.09302730113267899\n",
      "Epoch 6732/30000 Training Loss: 0.08957458287477493\n",
      "Epoch 6733/30000 Training Loss: 0.07839717715978622\n",
      "Epoch 6734/30000 Training Loss: 0.11085453629493713\n",
      "Epoch 6735/30000 Training Loss: 0.10367473214864731\n",
      "Epoch 6736/30000 Training Loss: 0.10707048326730728\n",
      "Epoch 6737/30000 Training Loss: 0.08458080887794495\n",
      "Epoch 6738/30000 Training Loss: 0.0846036896109581\n",
      "Epoch 6739/30000 Training Loss: 0.07025492191314697\n",
      "Epoch 6740/30000 Training Loss: 0.10294324159622192\n",
      "Epoch 6740/30000 Validation Loss: 0.1128268763422966\n",
      "Epoch 6741/30000 Training Loss: 0.08912555128335953\n",
      "Epoch 6742/30000 Training Loss: 0.08300285786390305\n",
      "Epoch 6743/30000 Training Loss: 0.08557950705289841\n",
      "Epoch 6744/30000 Training Loss: 0.09150440245866776\n",
      "Epoch 6745/30000 Training Loss: 0.07605081796646118\n",
      "Epoch 6746/30000 Training Loss: 0.07672594487667084\n",
      "Epoch 6747/30000 Training Loss: 0.0810367688536644\n",
      "Epoch 6748/30000 Training Loss: 0.07443507760763168\n",
      "Epoch 6749/30000 Training Loss: 0.08684462308883667\n",
      "Epoch 6750/30000 Training Loss: 0.09829062223434448\n",
      "Epoch 6750/30000 Validation Loss: 0.09074652194976807\n",
      "Epoch 6751/30000 Training Loss: 0.07708462327718735\n",
      "Epoch 6752/30000 Training Loss: 0.09574011713266373\n",
      "Epoch 6753/30000 Training Loss: 0.06744230538606644\n",
      "Epoch 6754/30000 Training Loss: 0.07747802883386612\n",
      "Epoch 6755/30000 Training Loss: 0.09715207666158676\n",
      "Epoch 6756/30000 Training Loss: 0.07992499321699142\n",
      "Epoch 6757/30000 Training Loss: 0.08344276994466782\n",
      "Epoch 6758/30000 Training Loss: 0.07465869188308716\n",
      "Epoch 6759/30000 Training Loss: 0.09124932438135147\n",
      "Epoch 6760/30000 Training Loss: 0.08973478525876999\n",
      "Epoch 6760/30000 Validation Loss: 0.07361380010843277\n",
      "Epoch 6761/30000 Training Loss: 0.10059546679258347\n",
      "Epoch 6762/30000 Training Loss: 0.08746489137411118\n",
      "Epoch 6763/30000 Training Loss: 0.0844515934586525\n",
      "Epoch 6764/30000 Training Loss: 0.09178922325372696\n",
      "Epoch 6765/30000 Training Loss: 0.11448884010314941\n",
      "Epoch 6766/30000 Training Loss: 0.0935349091887474\n",
      "Epoch 6767/30000 Training Loss: 0.09754994511604309\n",
      "Epoch 6768/30000 Training Loss: 0.07620364427566528\n",
      "Epoch 6769/30000 Training Loss: 0.07082483917474747\n",
      "Epoch 6770/30000 Training Loss: 0.0751030370593071\n",
      "Epoch 6770/30000 Validation Loss: 0.0805666372179985\n",
      "Epoch 6771/30000 Training Loss: 0.08585063368082047\n",
      "Epoch 6772/30000 Training Loss: 0.10053493827581406\n",
      "Epoch 6773/30000 Training Loss: 0.09900090098381042\n",
      "Epoch 6774/30000 Training Loss: 0.10729154944419861\n",
      "Epoch 6775/30000 Training Loss: 0.07507865875959396\n",
      "Epoch 6776/30000 Training Loss: 0.07615344971418381\n",
      "Epoch 6777/30000 Training Loss: 0.09480588883161545\n",
      "Epoch 6778/30000 Training Loss: 0.08637786656618118\n",
      "Epoch 6779/30000 Training Loss: 0.09832855314016342\n",
      "Epoch 6780/30000 Training Loss: 0.09116223454475403\n",
      "Epoch 6780/30000 Validation Loss: 0.06641647964715958\n",
      "Epoch 6781/30000 Training Loss: 0.0798771008849144\n",
      "Epoch 6782/30000 Training Loss: 0.09405366331338882\n",
      "Epoch 6783/30000 Training Loss: 0.09274202585220337\n",
      "Epoch 6784/30000 Training Loss: 0.06864113360643387\n",
      "Epoch 6785/30000 Training Loss: 0.08722756057977676\n",
      "Epoch 6786/30000 Training Loss: 0.09133759886026382\n",
      "Epoch 6787/30000 Training Loss: 0.07880282402038574\n",
      "Epoch 6788/30000 Training Loss: 0.07209842652082443\n",
      "Epoch 6789/30000 Training Loss: 0.08244174718856812\n",
      "Epoch 6790/30000 Training Loss: 0.08740243315696716\n",
      "Epoch 6790/30000 Validation Loss: 0.09588360786437988\n",
      "Epoch 6791/30000 Training Loss: 0.07413989305496216\n",
      "Epoch 6792/30000 Training Loss: 0.08469560742378235\n",
      "Epoch 6793/30000 Training Loss: 0.0708472728729248\n",
      "Epoch 6794/30000 Training Loss: 0.06249147653579712\n",
      "Epoch 6795/30000 Training Loss: 0.08991242200136185\n",
      "Epoch 6796/30000 Training Loss: 0.07357508689165115\n",
      "Epoch 6797/30000 Training Loss: 0.07538463175296783\n",
      "Epoch 6798/30000 Training Loss: 0.0815269872546196\n",
      "Epoch 6799/30000 Training Loss: 0.1078149750828743\n",
      "Epoch 6800/30000 Training Loss: 0.07481982558965683\n",
      "Epoch 6800/30000 Validation Loss: 0.09783295542001724\n",
      "Epoch 6801/30000 Training Loss: 0.07551991194486618\n",
      "Epoch 6802/30000 Training Loss: 0.08987782150506973\n",
      "Epoch 6803/30000 Training Loss: 0.08559978008270264\n",
      "Epoch 6804/30000 Training Loss: 0.07899171859025955\n",
      "Epoch 6805/30000 Training Loss: 0.09128281474113464\n",
      "Epoch 6806/30000 Training Loss: 0.07595080882310867\n",
      "Epoch 6807/30000 Training Loss: 0.08937438577413559\n",
      "Epoch 6808/30000 Training Loss: 0.09812811762094498\n",
      "Epoch 6809/30000 Training Loss: 0.07987699657678604\n",
      "Epoch 6810/30000 Training Loss: 0.07302270084619522\n",
      "Epoch 6810/30000 Validation Loss: 0.10001923888921738\n",
      "Epoch 6811/30000 Training Loss: 0.07923820614814758\n",
      "Epoch 6812/30000 Training Loss: 0.07753799855709076\n",
      "Epoch 6813/30000 Training Loss: 0.07642116397619247\n",
      "Epoch 6814/30000 Training Loss: 0.08339417725801468\n",
      "Epoch 6815/30000 Training Loss: 0.096555195748806\n",
      "Epoch 6816/30000 Training Loss: 0.0872851312160492\n",
      "Epoch 6817/30000 Training Loss: 0.07528210431337357\n",
      "Epoch 6818/30000 Training Loss: 0.07790059596300125\n",
      "Epoch 6819/30000 Training Loss: 0.10665380954742432\n",
      "Epoch 6820/30000 Training Loss: 0.08705625683069229\n",
      "Epoch 6820/30000 Validation Loss: 0.08348065614700317\n",
      "Epoch 6821/30000 Training Loss: 0.0799240991473198\n",
      "Epoch 6822/30000 Training Loss: 0.11783548444509506\n",
      "Epoch 6823/30000 Training Loss: 0.08480336517095566\n",
      "Epoch 6824/30000 Training Loss: 0.09047109633684158\n",
      "Epoch 6825/30000 Training Loss: 0.0747460126876831\n",
      "Epoch 6826/30000 Training Loss: 0.08517259359359741\n",
      "Epoch 6827/30000 Training Loss: 0.08790910989046097\n",
      "Epoch 6828/30000 Training Loss: 0.07137303799390793\n",
      "Epoch 6829/30000 Training Loss: 0.0762161836028099\n",
      "Epoch 6830/30000 Training Loss: 0.1070815697312355\n",
      "Epoch 6830/30000 Validation Loss: 0.09119955450296402\n",
      "Epoch 6831/30000 Training Loss: 0.10053795576095581\n",
      "Epoch 6832/30000 Training Loss: 0.08995596319437027\n",
      "Epoch 6833/30000 Training Loss: 0.09592998027801514\n",
      "Epoch 6834/30000 Training Loss: 0.08935829252004623\n",
      "Epoch 6835/30000 Training Loss: 0.10042569041252136\n",
      "Epoch 6836/30000 Training Loss: 0.0910310447216034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6837/30000 Training Loss: 0.07510445266962051\n",
      "Epoch 6838/30000 Training Loss: 0.08232652395963669\n",
      "Epoch 6839/30000 Training Loss: 0.08602166920900345\n",
      "Epoch 6840/30000 Training Loss: 0.07358210533857346\n",
      "Epoch 6840/30000 Validation Loss: 0.11187926679849625\n",
      "Epoch 6841/30000 Training Loss: 0.08241666108369827\n",
      "Epoch 6842/30000 Training Loss: 0.07121473550796509\n",
      "Epoch 6843/30000 Training Loss: 0.09309015423059464\n",
      "Epoch 6844/30000 Training Loss: 0.06885264068841934\n",
      "Epoch 6845/30000 Training Loss: 0.08749031275510788\n",
      "Epoch 6846/30000 Training Loss: 0.09525232762098312\n",
      "Epoch 6847/30000 Training Loss: 0.08131473511457443\n",
      "Epoch 6848/30000 Training Loss: 0.08716174215078354\n",
      "Epoch 6849/30000 Training Loss: 0.06308544427156448\n",
      "Epoch 6850/30000 Training Loss: 0.08405335992574692\n",
      "Epoch 6850/30000 Validation Loss: 0.07003346085548401\n",
      "Epoch 6851/30000 Training Loss: 0.07566782832145691\n",
      "Epoch 6852/30000 Training Loss: 0.07607080787420273\n",
      "Epoch 6853/30000 Training Loss: 0.08124830573797226\n",
      "Epoch 6854/30000 Training Loss: 0.07800231128931046\n",
      "Epoch 6855/30000 Training Loss: 0.07354513555765152\n",
      "Epoch 6856/30000 Training Loss: 0.09636050462722778\n",
      "Epoch 6857/30000 Training Loss: 0.1033276692032814\n",
      "Epoch 6858/30000 Training Loss: 0.09238556772470474\n",
      "Epoch 6859/30000 Training Loss: 0.07155174762010574\n",
      "Epoch 6860/30000 Training Loss: 0.09573230892419815\n",
      "Epoch 6860/30000 Validation Loss: 0.08307436853647232\n",
      "Epoch 6861/30000 Training Loss: 0.07791392505168915\n",
      "Epoch 6862/30000 Training Loss: 0.09173157066106796\n",
      "Epoch 6863/30000 Training Loss: 0.08593404293060303\n",
      "Epoch 6864/30000 Training Loss: 0.10049277544021606\n",
      "Epoch 6865/30000 Training Loss: 0.06943833082914352\n",
      "Epoch 6866/30000 Training Loss: 0.07277816534042358\n",
      "Epoch 6867/30000 Training Loss: 0.12722595036029816\n",
      "Epoch 6868/30000 Training Loss: 0.09843134135007858\n",
      "Epoch 6869/30000 Training Loss: 0.08367150276899338\n",
      "Epoch 6870/30000 Training Loss: 0.09246623516082764\n",
      "Epoch 6870/30000 Validation Loss: 0.06473850458860397\n",
      "Epoch 6871/30000 Training Loss: 0.08982118964195251\n",
      "Epoch 6872/30000 Training Loss: 0.09360692650079727\n",
      "Epoch 6873/30000 Training Loss: 0.07693080604076385\n",
      "Epoch 6874/30000 Training Loss: 0.08428170531988144\n",
      "Epoch 6875/30000 Training Loss: 0.09849413484334946\n",
      "Epoch 6876/30000 Training Loss: 0.07526446133852005\n",
      "Epoch 6877/30000 Training Loss: 0.08063454180955887\n",
      "Epoch 6878/30000 Training Loss: 0.07563654333353043\n",
      "Epoch 6879/30000 Training Loss: 0.06694022566080093\n",
      "Epoch 6880/30000 Training Loss: 0.08748533576726913\n",
      "Epoch 6880/30000 Validation Loss: 0.08304426074028015\n",
      "Epoch 6881/30000 Training Loss: 0.07650519162416458\n",
      "Epoch 6882/30000 Training Loss: 0.09296067804098129\n",
      "Epoch 6883/30000 Training Loss: 0.07450485974550247\n",
      "Epoch 6884/30000 Training Loss: 0.09061484783887863\n",
      "Epoch 6885/30000 Training Loss: 0.09573647379875183\n",
      "Epoch 6886/30000 Training Loss: 0.07235889881849289\n",
      "Epoch 6887/30000 Training Loss: 0.09571801871061325\n",
      "Epoch 6888/30000 Training Loss: 0.09383072704076767\n",
      "Epoch 6889/30000 Training Loss: 0.06692317128181458\n",
      "Epoch 6890/30000 Training Loss: 0.08181356638669968\n",
      "Epoch 6890/30000 Validation Loss: 0.08146089315414429\n",
      "Epoch 6891/30000 Training Loss: 0.0869341716170311\n",
      "Epoch 6892/30000 Training Loss: 0.06150301173329353\n",
      "Epoch 6893/30000 Training Loss: 0.06899404525756836\n",
      "Epoch 6894/30000 Training Loss: 0.08466195315122604\n",
      "Epoch 6895/30000 Training Loss: 0.0907670259475708\n",
      "Epoch 6896/30000 Training Loss: 0.09258844703435898\n",
      "Epoch 6897/30000 Training Loss: 0.08403906226158142\n",
      "Epoch 6898/30000 Training Loss: 0.07222374528646469\n",
      "Epoch 6899/30000 Training Loss: 0.09043950587511063\n",
      "Epoch 6900/30000 Training Loss: 0.07764575630426407\n",
      "Epoch 6900/30000 Validation Loss: 0.10817345976829529\n",
      "Epoch 6901/30000 Training Loss: 0.09038309007883072\n",
      "Epoch 6902/30000 Training Loss: 0.09471579641103745\n",
      "Epoch 6903/30000 Training Loss: 0.08553207665681839\n",
      "Epoch 6904/30000 Training Loss: 0.09862381219863892\n",
      "Epoch 6905/30000 Training Loss: 0.08359929174184799\n",
      "Epoch 6906/30000 Training Loss: 0.07571761310100555\n",
      "Epoch 6907/30000 Training Loss: 0.09166577458381653\n",
      "Epoch 6908/30000 Training Loss: 0.07882612943649292\n",
      "Epoch 6909/30000 Training Loss: 0.10523546487092972\n",
      "Epoch 6910/30000 Training Loss: 0.08506987243890762\n",
      "Epoch 6910/30000 Validation Loss: 0.08276072889566422\n",
      "Epoch 6911/30000 Training Loss: 0.08493310958147049\n",
      "Epoch 6912/30000 Training Loss: 0.0844663605093956\n",
      "Epoch 6913/30000 Training Loss: 0.0887008011341095\n",
      "Epoch 6914/30000 Training Loss: 0.08211924880743027\n",
      "Epoch 6915/30000 Training Loss: 0.08278164267539978\n",
      "Epoch 6916/30000 Training Loss: 0.10262506455183029\n",
      "Epoch 6917/30000 Training Loss: 0.0741666778922081\n",
      "Epoch 6918/30000 Training Loss: 0.07908610254526138\n",
      "Epoch 6919/30000 Training Loss: 0.08492948859930038\n",
      "Epoch 6920/30000 Training Loss: 0.08235947042703629\n",
      "Epoch 6920/30000 Validation Loss: 0.08725091069936752\n",
      "Epoch 6921/30000 Training Loss: 0.10457805544137955\n",
      "Epoch 6922/30000 Training Loss: 0.08706321567296982\n",
      "Epoch 6923/30000 Training Loss: 0.08488208055496216\n",
      "Epoch 6924/30000 Training Loss: 0.07464777678251266\n",
      "Epoch 6925/30000 Training Loss: 0.09102419763803482\n",
      "Epoch 6926/30000 Training Loss: 0.09411124140024185\n",
      "Epoch 6927/30000 Training Loss: 0.09143543243408203\n",
      "Epoch 6928/30000 Training Loss: 0.07456473261117935\n",
      "Epoch 6929/30000 Training Loss: 0.06978342682123184\n",
      "Epoch 6930/30000 Training Loss: 0.08345093578100204\n",
      "Epoch 6930/30000 Validation Loss: 0.09543464332818985\n",
      "Epoch 6931/30000 Training Loss: 0.08578162640333176\n",
      "Epoch 6932/30000 Training Loss: 0.09043405205011368\n",
      "Epoch 6933/30000 Training Loss: 0.07128620892763138\n",
      "Epoch 6934/30000 Training Loss: 0.10795814543962479\n",
      "Epoch 6935/30000 Training Loss: 0.07746150344610214\n",
      "Epoch 6936/30000 Training Loss: 0.08242448419332504\n",
      "Epoch 6937/30000 Training Loss: 0.09909162670373917\n",
      "Epoch 6938/30000 Training Loss: 0.0795033648610115\n",
      "Epoch 6939/30000 Training Loss: 0.0874132513999939\n",
      "Epoch 6940/30000 Training Loss: 0.11344107985496521\n",
      "Epoch 6940/30000 Validation Loss: 0.0853019580245018\n",
      "Epoch 6941/30000 Training Loss: 0.0915159210562706\n",
      "Epoch 6942/30000 Training Loss: 0.08335713297128677\n",
      "Epoch 6943/30000 Training Loss: 0.07984132319688797\n",
      "Epoch 6944/30000 Training Loss: 0.07266528159379959\n",
      "Epoch 6945/30000 Training Loss: 0.08656879514455795\n",
      "Epoch 6946/30000 Training Loss: 0.09322191029787064\n",
      "Epoch 6947/30000 Training Loss: 0.07830045372247696\n",
      "Epoch 6948/30000 Training Loss: 0.0964600145816803\n",
      "Epoch 6949/30000 Training Loss: 0.07795906066894531\n",
      "Epoch 6950/30000 Training Loss: 0.07024281471967697\n",
      "Epoch 6950/30000 Validation Loss: 0.0748831033706665\n",
      "Epoch 6951/30000 Training Loss: 0.10176870971918106\n",
      "Epoch 6952/30000 Training Loss: 0.08740250021219254\n",
      "Epoch 6953/30000 Training Loss: 0.0750420093536377\n",
      "Epoch 6954/30000 Training Loss: 0.07717182487249374\n",
      "Epoch 6955/30000 Training Loss: 0.0726514682173729\n",
      "Epoch 6956/30000 Training Loss: 0.07942438125610352\n",
      "Epoch 6957/30000 Training Loss: 0.07465893030166626\n",
      "Epoch 6958/30000 Training Loss: 0.08332078903913498\n",
      "Epoch 6959/30000 Training Loss: 0.0852431058883667\n",
      "Epoch 6960/30000 Training Loss: 0.08370307832956314\n",
      "Epoch 6960/30000 Validation Loss: 0.0906340554356575\n",
      "Epoch 6961/30000 Training Loss: 0.0807407796382904\n",
      "Epoch 6962/30000 Training Loss: 0.09266826510429382\n",
      "Epoch 6963/30000 Training Loss: 0.07809016108512878\n",
      "Epoch 6964/30000 Training Loss: 0.08450140804052353\n",
      "Epoch 6965/30000 Training Loss: 0.08340215682983398\n",
      "Epoch 6966/30000 Training Loss: 0.07257355004549026\n",
      "Epoch 6967/30000 Training Loss: 0.08654043078422546\n",
      "Epoch 6968/30000 Training Loss: 0.09918812662363052\n",
      "Epoch 6969/30000 Training Loss: 0.10217404365539551\n",
      "Epoch 6970/30000 Training Loss: 0.08888283371925354\n",
      "Epoch 6970/30000 Validation Loss: 0.0802827998995781\n",
      "Epoch 6971/30000 Training Loss: 0.09282884746789932\n",
      "Epoch 6972/30000 Training Loss: 0.09019113332033157\n",
      "Epoch 6973/30000 Training Loss: 0.07579748332500458\n",
      "Epoch 6974/30000 Training Loss: 0.07723545283079147\n",
      "Epoch 6975/30000 Training Loss: 0.09533321857452393\n",
      "Epoch 6976/30000 Training Loss: 0.07791739702224731\n",
      "Epoch 6977/30000 Training Loss: 0.07434683293104172\n",
      "Epoch 6978/30000 Training Loss: 0.09666135907173157\n",
      "Epoch 6979/30000 Training Loss: 0.08846748620271683\n",
      "Epoch 6980/30000 Training Loss: 0.0845649242401123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6980/30000 Validation Loss: 0.09110137820243835\n",
      "Epoch 6981/30000 Training Loss: 0.08356788754463196\n",
      "Epoch 6982/30000 Training Loss: 0.09621760994195938\n",
      "Epoch 6983/30000 Training Loss: 0.06396433711051941\n",
      "Epoch 6984/30000 Training Loss: 0.08257900923490524\n",
      "Epoch 6985/30000 Training Loss: 0.08067186176776886\n",
      "Epoch 6986/30000 Training Loss: 0.08980996161699295\n",
      "Epoch 6987/30000 Training Loss: 0.10369151830673218\n",
      "Epoch 6988/30000 Training Loss: 0.08875468373298645\n",
      "Epoch 6989/30000 Training Loss: 0.0801914855837822\n",
      "Epoch 6990/30000 Training Loss: 0.06272722035646439\n",
      "Epoch 6990/30000 Validation Loss: 0.08598483353853226\n",
      "Epoch 6991/30000 Training Loss: 0.08482875674962997\n",
      "Epoch 6992/30000 Training Loss: 0.06969480961561203\n",
      "Epoch 6993/30000 Training Loss: 0.09172335267066956\n",
      "Epoch 6994/30000 Training Loss: 0.08095622062683105\n",
      "Epoch 6995/30000 Training Loss: 0.09332609176635742\n",
      "Epoch 6996/30000 Training Loss: 0.0753343477845192\n",
      "Epoch 6997/30000 Training Loss: 0.10806050151586533\n",
      "Epoch 6998/30000 Training Loss: 0.11774434894323349\n",
      "Epoch 6999/30000 Training Loss: 0.10209465771913528\n",
      "Epoch 7000/30000 Training Loss: 0.08113939315080643\n",
      "Epoch 7000/30000 Validation Loss: 0.09716832637786865\n",
      "Epoch 7001/30000 Training Loss: 0.08346974849700928\n",
      "Epoch 7002/30000 Training Loss: 0.08997716754674911\n",
      "Epoch 7003/30000 Training Loss: 0.083348847925663\n",
      "Epoch 7004/30000 Training Loss: 0.0924832820892334\n",
      "Epoch 7005/30000 Training Loss: 0.08296170830726624\n",
      "Epoch 7006/30000 Training Loss: 0.08382325619459152\n",
      "Epoch 7007/30000 Training Loss: 0.07558822631835938\n",
      "Epoch 7008/30000 Training Loss: 0.06933792680501938\n",
      "Epoch 7009/30000 Training Loss: 0.08541399240493774\n",
      "Epoch 7010/30000 Training Loss: 0.07180462032556534\n",
      "Epoch 7010/30000 Validation Loss: 0.06807264685630798\n",
      "Epoch 7011/30000 Training Loss: 0.07492690533399582\n",
      "Epoch 7012/30000 Training Loss: 0.11506157368421555\n",
      "Epoch 7013/30000 Training Loss: 0.06686041504144669\n",
      "Epoch 7014/30000 Training Loss: 0.08533307164907455\n",
      "Epoch 7015/30000 Training Loss: 0.10779305547475815\n",
      "Epoch 7016/30000 Training Loss: 0.07144451141357422\n",
      "Epoch 7017/30000 Training Loss: 0.07037806510925293\n",
      "Epoch 7018/30000 Training Loss: 0.0859195664525032\n",
      "Epoch 7019/30000 Training Loss: 0.0814000740647316\n",
      "Epoch 7020/30000 Training Loss: 0.1020236387848854\n",
      "Epoch 7020/30000 Validation Loss: 0.10101894289255142\n",
      "Epoch 7021/30000 Training Loss: 0.07363974303007126\n",
      "Epoch 7022/30000 Training Loss: 0.08430445194244385\n",
      "Epoch 7023/30000 Training Loss: 0.062251437455415726\n",
      "Epoch 7024/30000 Training Loss: 0.09954100847244263\n",
      "Epoch 7025/30000 Training Loss: 0.0830041766166687\n",
      "Epoch 7026/30000 Training Loss: 0.09808573126792908\n",
      "Epoch 7027/30000 Training Loss: 0.08339309692382812\n",
      "Epoch 7028/30000 Training Loss: 0.08344831317663193\n",
      "Epoch 7029/30000 Training Loss: 0.09362439066171646\n",
      "Epoch 7030/30000 Training Loss: 0.09344470500946045\n",
      "Epoch 7030/30000 Validation Loss: 0.08621402829885483\n",
      "Epoch 7031/30000 Training Loss: 0.11531299352645874\n",
      "Epoch 7032/30000 Training Loss: 0.08354821056127548\n",
      "Epoch 7033/30000 Training Loss: 0.0662355050444603\n",
      "Epoch 7034/30000 Training Loss: 0.0914064571261406\n",
      "Epoch 7035/30000 Training Loss: 0.08702927827835083\n",
      "Epoch 7036/30000 Training Loss: 0.08541405946016312\n",
      "Epoch 7037/30000 Training Loss: 0.09723591059446335\n",
      "Epoch 7038/30000 Training Loss: 0.09123165160417557\n",
      "Epoch 7039/30000 Training Loss: 0.07068288326263428\n",
      "Epoch 7040/30000 Training Loss: 0.09455931931734085\n",
      "Epoch 7040/30000 Validation Loss: 0.08240360766649246\n",
      "Epoch 7041/30000 Training Loss: 0.09402942657470703\n",
      "Epoch 7042/30000 Training Loss: 0.07415781170129776\n",
      "Epoch 7043/30000 Training Loss: 0.069857157766819\n",
      "Epoch 7044/30000 Training Loss: 0.07856029272079468\n",
      "Epoch 7045/30000 Training Loss: 0.0860079750418663\n",
      "Epoch 7046/30000 Training Loss: 0.07727616280317307\n",
      "Epoch 7047/30000 Training Loss: 0.08320140838623047\n",
      "Epoch 7048/30000 Training Loss: 0.07589661329984665\n",
      "Epoch 7049/30000 Training Loss: 0.10042516142129898\n",
      "Epoch 7050/30000 Training Loss: 0.08858084678649902\n",
      "Epoch 7050/30000 Validation Loss: 0.09092116355895996\n",
      "Epoch 7051/30000 Training Loss: 0.0792929157614708\n",
      "Epoch 7052/30000 Training Loss: 0.10443654656410217\n",
      "Epoch 7053/30000 Training Loss: 0.09466975927352905\n",
      "Epoch 7054/30000 Training Loss: 0.10442471504211426\n",
      "Epoch 7055/30000 Training Loss: 0.09057226032018661\n",
      "Epoch 7056/30000 Training Loss: 0.07615349441766739\n",
      "Epoch 7057/30000 Training Loss: 0.08117970824241638\n",
      "Epoch 7058/30000 Training Loss: 0.08446761965751648\n",
      "Epoch 7059/30000 Training Loss: 0.07515468448400497\n",
      "Epoch 7060/30000 Training Loss: 0.0733007863163948\n",
      "Epoch 7060/30000 Validation Loss: 0.09780517965555191\n",
      "Epoch 7061/30000 Training Loss: 0.0908978208899498\n",
      "Epoch 7062/30000 Training Loss: 0.093289315700531\n",
      "Epoch 7063/30000 Training Loss: 0.08789076656103134\n",
      "Epoch 7064/30000 Training Loss: 0.079485684633255\n",
      "Epoch 7065/30000 Training Loss: 0.08325091004371643\n",
      "Epoch 7066/30000 Training Loss: 0.07795623689889908\n",
      "Epoch 7067/30000 Training Loss: 0.07973458617925644\n",
      "Epoch 7068/30000 Training Loss: 0.09150556474924088\n",
      "Epoch 7069/30000 Training Loss: 0.10090136528015137\n",
      "Epoch 7070/30000 Training Loss: 0.06888779252767563\n",
      "Epoch 7070/30000 Validation Loss: 0.08788546919822693\n",
      "Epoch 7071/30000 Training Loss: 0.08631211519241333\n",
      "Epoch 7072/30000 Training Loss: 0.08085218816995621\n",
      "Epoch 7073/30000 Training Loss: 0.09315399080514908\n",
      "Epoch 7074/30000 Training Loss: 0.0717848539352417\n",
      "Epoch 7075/30000 Training Loss: 0.08834734559059143\n",
      "Epoch 7076/30000 Training Loss: 0.07570821791887283\n",
      "Epoch 7077/30000 Training Loss: 0.07232168316841125\n",
      "Epoch 7078/30000 Training Loss: 0.0781913474202156\n",
      "Epoch 7079/30000 Training Loss: 0.06543969362974167\n",
      "Epoch 7080/30000 Training Loss: 0.08044030517339706\n",
      "Epoch 7080/30000 Validation Loss: 0.0768641009926796\n",
      "Epoch 7081/30000 Training Loss: 0.08961889892816544\n",
      "Epoch 7082/30000 Training Loss: 0.06662818044424057\n",
      "Epoch 7083/30000 Training Loss: 0.08632776141166687\n",
      "Epoch 7084/30000 Training Loss: 0.07987931370735168\n",
      "Epoch 7085/30000 Training Loss: 0.10115659236907959\n",
      "Epoch 7086/30000 Training Loss: 0.0772019624710083\n",
      "Epoch 7087/30000 Training Loss: 0.078278549015522\n",
      "Epoch 7088/30000 Training Loss: 0.06988384574651718\n",
      "Epoch 7089/30000 Training Loss: 0.078398197889328\n",
      "Epoch 7090/30000 Training Loss: 0.0809386745095253\n",
      "Epoch 7090/30000 Validation Loss: 0.06621084362268448\n",
      "Epoch 7091/30000 Training Loss: 0.0833553597331047\n",
      "Epoch 7092/30000 Training Loss: 0.0841454342007637\n",
      "Epoch 7093/30000 Training Loss: 0.0870935395359993\n",
      "Epoch 7094/30000 Training Loss: 0.06377457827329636\n",
      "Epoch 7095/30000 Training Loss: 0.09707611799240112\n",
      "Epoch 7096/30000 Training Loss: 0.08052784949541092\n",
      "Epoch 7097/30000 Training Loss: 0.11415225267410278\n",
      "Epoch 7098/30000 Training Loss: 0.08024381101131439\n",
      "Epoch 7099/30000 Training Loss: 0.09177746623754501\n",
      "Epoch 7100/30000 Training Loss: 0.08309795707464218\n",
      "Epoch 7100/30000 Validation Loss: 0.10945713520050049\n",
      "Epoch 7101/30000 Training Loss: 0.10666721314191818\n",
      "Epoch 7102/30000 Training Loss: 0.08977410942316055\n",
      "Epoch 7103/30000 Training Loss: 0.0841870903968811\n",
      "Epoch 7104/30000 Training Loss: 0.08630633354187012\n",
      "Epoch 7105/30000 Training Loss: 0.07951512187719345\n",
      "Epoch 7106/30000 Training Loss: 0.08519991487264633\n",
      "Epoch 7107/30000 Training Loss: 0.08961013704538345\n",
      "Epoch 7108/30000 Training Loss: 0.08083073794841766\n",
      "Epoch 7109/30000 Training Loss: 0.07493821531534195\n",
      "Epoch 7110/30000 Training Loss: 0.08873007446527481\n",
      "Epoch 7110/30000 Validation Loss: 0.09306961297988892\n",
      "Epoch 7111/30000 Training Loss: 0.11401364952325821\n",
      "Epoch 7112/30000 Training Loss: 0.07496311515569687\n",
      "Epoch 7113/30000 Training Loss: 0.07470479607582092\n",
      "Epoch 7114/30000 Training Loss: 0.09417259693145752\n",
      "Epoch 7115/30000 Training Loss: 0.07529240846633911\n",
      "Epoch 7116/30000 Training Loss: 0.0740879476070404\n",
      "Epoch 7117/30000 Training Loss: 0.10602591186761856\n",
      "Epoch 7118/30000 Training Loss: 0.08431553095579147\n",
      "Epoch 7119/30000 Training Loss: 0.08116092532873154\n",
      "Epoch 7120/30000 Training Loss: 0.08948595076799393\n",
      "Epoch 7120/30000 Validation Loss: 0.07090248912572861\n",
      "Epoch 7121/30000 Training Loss: 0.07093843072652817\n",
      "Epoch 7122/30000 Training Loss: 0.07765375822782516\n",
      "Epoch 7123/30000 Training Loss: 0.07965511828660965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7124/30000 Training Loss: 0.09074902534484863\n",
      "Epoch 7125/30000 Training Loss: 0.07633937150239944\n",
      "Epoch 7126/30000 Training Loss: 0.10518556833267212\n",
      "Epoch 7127/30000 Training Loss: 0.09318852424621582\n",
      "Epoch 7128/30000 Training Loss: 0.08150038123130798\n",
      "Epoch 7129/30000 Training Loss: 0.07673324644565582\n",
      "Epoch 7130/30000 Training Loss: 0.10521336644887924\n",
      "Epoch 7130/30000 Validation Loss: 0.10670540481805801\n",
      "Epoch 7131/30000 Training Loss: 0.09469085931777954\n",
      "Epoch 7132/30000 Training Loss: 0.06622571498155594\n",
      "Epoch 7133/30000 Training Loss: 0.0838823989033699\n",
      "Epoch 7134/30000 Training Loss: 0.10030461102724075\n",
      "Epoch 7135/30000 Training Loss: 0.08186835795640945\n",
      "Epoch 7136/30000 Training Loss: 0.08432134240865707\n",
      "Epoch 7137/30000 Training Loss: 0.08920508623123169\n",
      "Epoch 7138/30000 Training Loss: 0.0818638876080513\n",
      "Epoch 7139/30000 Training Loss: 0.104119211435318\n",
      "Epoch 7140/30000 Training Loss: 0.0674220472574234\n",
      "Epoch 7140/30000 Validation Loss: 0.07714911550283432\n",
      "Epoch 7141/30000 Training Loss: 0.08419593423604965\n",
      "Epoch 7142/30000 Training Loss: 0.08431354910135269\n",
      "Epoch 7143/30000 Training Loss: 0.09891574829816818\n",
      "Epoch 7144/30000 Training Loss: 0.07929491996765137\n",
      "Epoch 7145/30000 Training Loss: 0.10647401958703995\n",
      "Epoch 7146/30000 Training Loss: 0.08797457814216614\n",
      "Epoch 7147/30000 Training Loss: 0.08437754958868027\n",
      "Epoch 7148/30000 Training Loss: 0.08471322804689407\n",
      "Epoch 7149/30000 Training Loss: 0.0989086851477623\n",
      "Epoch 7150/30000 Training Loss: 0.07873950153589249\n",
      "Epoch 7150/30000 Validation Loss: 0.08338538557291031\n",
      "Epoch 7151/30000 Training Loss: 0.06962413340806961\n",
      "Epoch 7152/30000 Training Loss: 0.09362903982400894\n",
      "Epoch 7153/30000 Training Loss: 0.08470020443201065\n",
      "Epoch 7154/30000 Training Loss: 0.07178928703069687\n",
      "Epoch 7155/30000 Training Loss: 0.09132768958806992\n",
      "Epoch 7156/30000 Training Loss: 0.07136216759681702\n",
      "Epoch 7157/30000 Training Loss: 0.08232646435499191\n",
      "Epoch 7158/30000 Training Loss: 0.09744391590356827\n",
      "Epoch 7159/30000 Training Loss: 0.08050691336393356\n",
      "Epoch 7160/30000 Training Loss: 0.06820142269134521\n",
      "Epoch 7160/30000 Validation Loss: 0.08596564084291458\n",
      "Epoch 7161/30000 Training Loss: 0.07248944044113159\n",
      "Epoch 7162/30000 Training Loss: 0.07993980497121811\n",
      "Epoch 7163/30000 Training Loss: 0.09785755723714828\n",
      "Epoch 7164/30000 Training Loss: 0.08715206384658813\n",
      "Epoch 7165/30000 Training Loss: 0.10527560859918594\n",
      "Epoch 7166/30000 Training Loss: 0.0794752761721611\n",
      "Epoch 7167/30000 Training Loss: 0.11731764674186707\n",
      "Epoch 7168/30000 Training Loss: 0.09516841173171997\n",
      "Epoch 7169/30000 Training Loss: 0.0697740986943245\n",
      "Epoch 7170/30000 Training Loss: 0.08765140175819397\n",
      "Epoch 7170/30000 Validation Loss: 0.08296552300453186\n",
      "Epoch 7171/30000 Training Loss: 0.09765920788049698\n",
      "Epoch 7172/30000 Training Loss: 0.07525374740362167\n",
      "Epoch 7173/30000 Training Loss: 0.10428112745285034\n",
      "Epoch 7174/30000 Training Loss: 0.0718977078795433\n",
      "Epoch 7175/30000 Training Loss: 0.08350703865289688\n",
      "Epoch 7176/30000 Training Loss: 0.07092003524303436\n",
      "Epoch 7177/30000 Training Loss: 0.07498887926340103\n",
      "Epoch 7178/30000 Training Loss: 0.10042590647935867\n",
      "Epoch 7179/30000 Training Loss: 0.06840481609106064\n",
      "Epoch 7180/30000 Training Loss: 0.08069629222154617\n",
      "Epoch 7180/30000 Validation Loss: 0.06855008751153946\n",
      "Epoch 7181/30000 Training Loss: 0.0917859897017479\n",
      "Epoch 7182/30000 Training Loss: 0.10440097004175186\n",
      "Epoch 7183/30000 Training Loss: 0.0728280171751976\n",
      "Epoch 7184/30000 Training Loss: 0.09584174305200577\n",
      "Epoch 7185/30000 Training Loss: 0.09144658595323563\n",
      "Epoch 7186/30000 Training Loss: 0.08920709043741226\n",
      "Epoch 7187/30000 Training Loss: 0.09850647300481796\n",
      "Epoch 7188/30000 Training Loss: 0.0795089453458786\n",
      "Epoch 7189/30000 Training Loss: 0.09626040607690811\n",
      "Epoch 7190/30000 Training Loss: 0.07860293239355087\n",
      "Epoch 7190/30000 Validation Loss: 0.08819756656885147\n",
      "Epoch 7191/30000 Training Loss: 0.09716144949197769\n",
      "Epoch 7192/30000 Training Loss: 0.0900009274482727\n",
      "Epoch 7193/30000 Training Loss: 0.0932249128818512\n",
      "Epoch 7194/30000 Training Loss: 0.08968596905469894\n",
      "Epoch 7195/30000 Training Loss: 0.080987848341465\n",
      "Epoch 7196/30000 Training Loss: 0.08343309164047241\n",
      "Epoch 7197/30000 Training Loss: 0.09260112047195435\n",
      "Epoch 7198/30000 Training Loss: 0.10504200309515\n",
      "Epoch 7199/30000 Training Loss: 0.09404826909303665\n",
      "Epoch 7200/30000 Training Loss: 0.09120751172304153\n",
      "Epoch 7200/30000 Validation Loss: 0.08004492521286011\n",
      "Epoch 7201/30000 Training Loss: 0.09479216486215591\n",
      "Epoch 7202/30000 Training Loss: 0.0915987566113472\n",
      "Epoch 7203/30000 Training Loss: 0.09082382917404175\n",
      "Epoch 7204/30000 Training Loss: 0.10283277183771133\n",
      "Epoch 7205/30000 Training Loss: 0.09006267040967941\n",
      "Epoch 7206/30000 Training Loss: 0.09841186553239822\n",
      "Epoch 7207/30000 Training Loss: 0.08618249744176865\n",
      "Epoch 7208/30000 Training Loss: 0.08221041411161423\n",
      "Epoch 7209/30000 Training Loss: 0.07180612534284592\n",
      "Epoch 7210/30000 Training Loss: 0.10157012194395065\n",
      "Epoch 7210/30000 Validation Loss: 0.09574982523918152\n",
      "Epoch 7211/30000 Training Loss: 0.08913608640432358\n",
      "Epoch 7212/30000 Training Loss: 0.08160687237977982\n",
      "Epoch 7213/30000 Training Loss: 0.08674173802137375\n",
      "Epoch 7214/30000 Training Loss: 0.06951340287923813\n",
      "Epoch 7215/30000 Training Loss: 0.07401493936777115\n",
      "Epoch 7216/30000 Training Loss: 0.0780271366238594\n",
      "Epoch 7217/30000 Training Loss: 0.08742684870958328\n",
      "Epoch 7218/30000 Training Loss: 0.08370864391326904\n",
      "Epoch 7219/30000 Training Loss: 0.07510972768068314\n",
      "Epoch 7220/30000 Training Loss: 0.0900190994143486\n",
      "Epoch 7220/30000 Validation Loss: 0.09940138459205627\n",
      "Epoch 7221/30000 Training Loss: 0.10424401611089706\n",
      "Epoch 7222/30000 Training Loss: 0.096895731985569\n",
      "Epoch 7223/30000 Training Loss: 0.08179570734500885\n",
      "Epoch 7224/30000 Training Loss: 0.1151944100856781\n",
      "Epoch 7225/30000 Training Loss: 0.06419951468706131\n",
      "Epoch 7226/30000 Training Loss: 0.10327502340078354\n",
      "Epoch 7227/30000 Training Loss: 0.08695919066667557\n",
      "Epoch 7228/30000 Training Loss: 0.09205956012010574\n",
      "Epoch 7229/30000 Training Loss: 0.08547631651163101\n",
      "Epoch 7230/30000 Training Loss: 0.09305598586797714\n",
      "Epoch 7230/30000 Validation Loss: 0.07091938704252243\n",
      "Epoch 7231/30000 Training Loss: 0.114194355905056\n",
      "Epoch 7232/30000 Training Loss: 0.10162293165922165\n",
      "Epoch 7233/30000 Training Loss: 0.07707887142896652\n",
      "Epoch 7234/30000 Training Loss: 0.09174930304288864\n",
      "Epoch 7235/30000 Training Loss: 0.07808076590299606\n",
      "Epoch 7236/30000 Training Loss: 0.08401455730199814\n",
      "Epoch 7237/30000 Training Loss: 0.06947340816259384\n",
      "Epoch 7238/30000 Training Loss: 0.08546032756567001\n",
      "Epoch 7239/30000 Training Loss: 0.07791799306869507\n",
      "Epoch 7240/30000 Training Loss: 0.09974247217178345\n",
      "Epoch 7240/30000 Validation Loss: 0.07856494933366776\n",
      "Epoch 7241/30000 Training Loss: 0.07988359779119492\n",
      "Epoch 7242/30000 Training Loss: 0.08410116285085678\n",
      "Epoch 7243/30000 Training Loss: 0.08460881561040878\n",
      "Epoch 7244/30000 Training Loss: 0.08307313174009323\n",
      "Epoch 7245/30000 Training Loss: 0.09071823209524155\n",
      "Epoch 7246/30000 Training Loss: 0.07177388668060303\n",
      "Epoch 7247/30000 Training Loss: 0.0698830857872963\n",
      "Epoch 7248/30000 Training Loss: 0.09716614335775375\n",
      "Epoch 7249/30000 Training Loss: 0.08215558528900146\n",
      "Epoch 7250/30000 Training Loss: 0.0949782207608223\n",
      "Epoch 7250/30000 Validation Loss: 0.08090467005968094\n",
      "Epoch 7251/30000 Training Loss: 0.0959877148270607\n",
      "Epoch 7252/30000 Training Loss: 0.0808117613196373\n",
      "Epoch 7253/30000 Training Loss: 0.07517984509468079\n",
      "Epoch 7254/30000 Training Loss: 0.08870009332895279\n",
      "Epoch 7255/30000 Training Loss: 0.08383121341466904\n",
      "Epoch 7256/30000 Training Loss: 0.07172131538391113\n",
      "Epoch 7257/30000 Training Loss: 0.11267030984163284\n",
      "Epoch 7258/30000 Training Loss: 0.11311870813369751\n",
      "Epoch 7259/30000 Training Loss: 0.08179870992898941\n",
      "Epoch 7260/30000 Training Loss: 0.09999487549066544\n",
      "Epoch 7260/30000 Validation Loss: 0.0788310244679451\n",
      "Epoch 7261/30000 Training Loss: 0.11893143504858017\n",
      "Epoch 7262/30000 Training Loss: 0.08615761995315552\n",
      "Epoch 7263/30000 Training Loss: 0.0734267458319664\n",
      "Epoch 7264/30000 Training Loss: 0.07922457903623581\n",
      "Epoch 7265/30000 Training Loss: 0.09815987944602966\n",
      "Epoch 7266/30000 Training Loss: 0.06759310513734818\n",
      "Epoch 7267/30000 Training Loss: 0.09069637209177017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7268/30000 Training Loss: 0.0835084393620491\n",
      "Epoch 7269/30000 Training Loss: 0.07125633209943771\n",
      "Epoch 7270/30000 Training Loss: 0.08677289634943008\n",
      "Epoch 7270/30000 Validation Loss: 0.08648765832185745\n",
      "Epoch 7271/30000 Training Loss: 0.10674991458654404\n",
      "Epoch 7272/30000 Training Loss: 0.08751437067985535\n",
      "Epoch 7273/30000 Training Loss: 0.08015865832567215\n",
      "Epoch 7274/30000 Training Loss: 0.10266998410224915\n",
      "Epoch 7275/30000 Training Loss: 0.08891890197992325\n",
      "Epoch 7276/30000 Training Loss: 0.07831364125013351\n",
      "Epoch 7277/30000 Training Loss: 0.09327241778373718\n",
      "Epoch 7278/30000 Training Loss: 0.09997402876615524\n",
      "Epoch 7279/30000 Training Loss: 0.08128609508275986\n",
      "Epoch 7280/30000 Training Loss: 0.07480555772781372\n",
      "Epoch 7280/30000 Validation Loss: 0.08263436704874039\n",
      "Epoch 7281/30000 Training Loss: 0.10521168261766434\n",
      "Epoch 7282/30000 Training Loss: 0.06891492009162903\n",
      "Epoch 7283/30000 Training Loss: 0.089565210044384\n",
      "Epoch 7284/30000 Training Loss: 0.08507978171110153\n",
      "Epoch 7285/30000 Training Loss: 0.10680943727493286\n",
      "Epoch 7286/30000 Training Loss: 0.0803895965218544\n",
      "Epoch 7287/30000 Training Loss: 0.08864349126815796\n",
      "Epoch 7288/30000 Training Loss: 0.08354967087507248\n",
      "Epoch 7289/30000 Training Loss: 0.07857394963502884\n",
      "Epoch 7290/30000 Training Loss: 0.0959598496556282\n",
      "Epoch 7290/30000 Validation Loss: 0.0988624319434166\n",
      "Epoch 7291/30000 Training Loss: 0.09789467602968216\n",
      "Epoch 7292/30000 Training Loss: 0.0901569351553917\n",
      "Epoch 7293/30000 Training Loss: 0.07552400976419449\n",
      "Epoch 7294/30000 Training Loss: 0.07233395427465439\n",
      "Epoch 7295/30000 Training Loss: 0.0682675763964653\n",
      "Epoch 7296/30000 Training Loss: 0.07880290597677231\n",
      "Epoch 7297/30000 Training Loss: 0.08120565861463547\n",
      "Epoch 7298/30000 Training Loss: 0.07421223074197769\n",
      "Epoch 7299/30000 Training Loss: 0.10586541891098022\n",
      "Epoch 7300/30000 Training Loss: 0.08856767416000366\n",
      "Epoch 7300/30000 Validation Loss: 0.06743931025266647\n",
      "Epoch 7301/30000 Training Loss: 0.08239185065031052\n",
      "Epoch 7302/30000 Training Loss: 0.07899671047925949\n",
      "Epoch 7303/30000 Training Loss: 0.08543301373720169\n",
      "Epoch 7304/30000 Training Loss: 0.09385328739881516\n",
      "Epoch 7305/30000 Training Loss: 0.07890868186950684\n",
      "Epoch 7306/30000 Training Loss: 0.06806264817714691\n",
      "Epoch 7307/30000 Training Loss: 0.0848388597369194\n",
      "Epoch 7308/30000 Training Loss: 0.09502080082893372\n",
      "Epoch 7309/30000 Training Loss: 0.09477725625038147\n",
      "Epoch 7310/30000 Training Loss: 0.08462011069059372\n",
      "Epoch 7310/30000 Validation Loss: 0.08521122485399246\n",
      "Epoch 7311/30000 Training Loss: 0.07976268976926804\n",
      "Epoch 7312/30000 Training Loss: 0.08577217906713486\n",
      "Epoch 7313/30000 Training Loss: 0.08199536800384521\n",
      "Epoch 7314/30000 Training Loss: 0.08948542922735214\n",
      "Epoch 7315/30000 Training Loss: 0.097126305103302\n",
      "Epoch 7316/30000 Training Loss: 0.0818960890173912\n",
      "Epoch 7317/30000 Training Loss: 0.08952724188566208\n",
      "Epoch 7318/30000 Training Loss: 0.08651936054229736\n",
      "Epoch 7319/30000 Training Loss: 0.07003773003816605\n",
      "Epoch 7320/30000 Training Loss: 0.10137464851140976\n",
      "Epoch 7320/30000 Validation Loss: 0.08121684938669205\n",
      "Epoch 7321/30000 Training Loss: 0.08771800249814987\n",
      "Epoch 7322/30000 Training Loss: 0.10495259612798691\n",
      "Epoch 7323/30000 Training Loss: 0.08185573667287827\n",
      "Epoch 7324/30000 Training Loss: 0.0920901894569397\n",
      "Epoch 7325/30000 Training Loss: 0.08706680685281754\n",
      "Epoch 7326/30000 Training Loss: 0.0960908830165863\n",
      "Epoch 7327/30000 Training Loss: 0.1048199012875557\n",
      "Epoch 7328/30000 Training Loss: 0.07570082694292068\n",
      "Epoch 7329/30000 Training Loss: 0.09304594993591309\n",
      "Epoch 7330/30000 Training Loss: 0.09330236911773682\n",
      "Epoch 7330/30000 Validation Loss: 0.09720367938280106\n",
      "Epoch 7331/30000 Training Loss: 0.0937267541885376\n",
      "Epoch 7332/30000 Training Loss: 0.10947468131780624\n",
      "Epoch 7333/30000 Training Loss: 0.09204743057489395\n",
      "Epoch 7334/30000 Training Loss: 0.07728087902069092\n",
      "Epoch 7335/30000 Training Loss: 0.06007532402873039\n",
      "Epoch 7336/30000 Training Loss: 0.07970542460680008\n",
      "Epoch 7337/30000 Training Loss: 0.07458069920539856\n",
      "Epoch 7338/30000 Training Loss: 0.07398458570241928\n",
      "Epoch 7339/30000 Training Loss: 0.07833849638700485\n",
      "Epoch 7340/30000 Training Loss: 0.09720856696367264\n",
      "Epoch 7340/30000 Validation Loss: 0.07044300436973572\n",
      "Epoch 7341/30000 Training Loss: 0.09604989737272263\n",
      "Epoch 7342/30000 Training Loss: 0.07930800318717957\n",
      "Epoch 7343/30000 Training Loss: 0.07199884206056595\n",
      "Epoch 7344/30000 Training Loss: 0.08918803185224533\n",
      "Epoch 7345/30000 Training Loss: 0.08294980227947235\n",
      "Epoch 7346/30000 Training Loss: 0.10753212124109268\n",
      "Epoch 7347/30000 Training Loss: 0.07819051295518875\n",
      "Epoch 7348/30000 Training Loss: 0.07584676891565323\n",
      "Epoch 7349/30000 Training Loss: 0.09182387590408325\n",
      "Epoch 7350/30000 Training Loss: 0.08311652392148972\n",
      "Epoch 7350/30000 Validation Loss: 0.09987544268369675\n",
      "Epoch 7351/30000 Training Loss: 0.09892382472753525\n",
      "Epoch 7352/30000 Training Loss: 0.09297677874565125\n",
      "Epoch 7353/30000 Training Loss: 0.08660773187875748\n",
      "Epoch 7354/30000 Training Loss: 0.0786629393696785\n",
      "Epoch 7355/30000 Training Loss: 0.09681982547044754\n",
      "Epoch 7356/30000 Training Loss: 0.10042233020067215\n",
      "Epoch 7357/30000 Training Loss: 0.07903024554252625\n",
      "Epoch 7358/30000 Training Loss: 0.08180872350931168\n",
      "Epoch 7359/30000 Training Loss: 0.09364354610443115\n",
      "Epoch 7360/30000 Training Loss: 0.0876360833644867\n",
      "Epoch 7360/30000 Validation Loss: 0.062320876866579056\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.062320876866579056<=============\n",
      "Epoch 7361/30000 Training Loss: 0.08255791664123535\n",
      "Epoch 7362/30000 Training Loss: 0.09371551871299744\n",
      "Epoch 7363/30000 Training Loss: 0.09131738543510437\n",
      "Epoch 7364/30000 Training Loss: 0.09884730726480484\n",
      "Epoch 7365/30000 Training Loss: 0.08244490623474121\n",
      "Epoch 7366/30000 Training Loss: 0.08004117012023926\n",
      "Epoch 7367/30000 Training Loss: 0.08286768943071365\n",
      "Epoch 7368/30000 Training Loss: 0.09581315517425537\n",
      "Epoch 7369/30000 Training Loss: 0.07132022827863693\n",
      "Epoch 7370/30000 Training Loss: 0.10536859184503555\n",
      "Epoch 7370/30000 Validation Loss: 0.0865929126739502\n",
      "Epoch 7371/30000 Training Loss: 0.07917719334363937\n",
      "Epoch 7372/30000 Training Loss: 0.0837884172797203\n",
      "Epoch 7373/30000 Training Loss: 0.0842096135020256\n",
      "Epoch 7374/30000 Training Loss: 0.09377938508987427\n",
      "Epoch 7375/30000 Training Loss: 0.0905672088265419\n",
      "Epoch 7376/30000 Training Loss: 0.08489594608545303\n",
      "Epoch 7377/30000 Training Loss: 0.06582946330308914\n",
      "Epoch 7378/30000 Training Loss: 0.07544361799955368\n",
      "Epoch 7379/30000 Training Loss: 0.08747925609350204\n",
      "Epoch 7380/30000 Training Loss: 0.10828721523284912\n",
      "Epoch 7380/30000 Validation Loss: 0.09072839468717575\n",
      "Epoch 7381/30000 Training Loss: 0.07774008810520172\n",
      "Epoch 7382/30000 Training Loss: 0.07540250569581985\n",
      "Epoch 7383/30000 Training Loss: 0.09424490481615067\n",
      "Epoch 7384/30000 Training Loss: 0.08239669352769852\n",
      "Epoch 7385/30000 Training Loss: 0.08346587419509888\n",
      "Epoch 7386/30000 Training Loss: 0.07790503650903702\n",
      "Epoch 7387/30000 Training Loss: 0.09034806489944458\n",
      "Epoch 7388/30000 Training Loss: 0.07398278266191483\n",
      "Epoch 7389/30000 Training Loss: 0.09214261174201965\n",
      "Epoch 7390/30000 Training Loss: 0.09569958597421646\n",
      "Epoch 7390/30000 Validation Loss: 0.08192948251962662\n",
      "Epoch 7391/30000 Training Loss: 0.10133377462625504\n",
      "Epoch 7392/30000 Training Loss: 0.06793370097875595\n",
      "Epoch 7393/30000 Training Loss: 0.1083100214600563\n",
      "Epoch 7394/30000 Training Loss: 0.07392754405736923\n",
      "Epoch 7395/30000 Training Loss: 0.09150724858045578\n",
      "Epoch 7396/30000 Training Loss: 0.0695534348487854\n",
      "Epoch 7397/30000 Training Loss: 0.08899900317192078\n",
      "Epoch 7398/30000 Training Loss: 0.06921335309743881\n",
      "Epoch 7399/30000 Training Loss: 0.07156477123498917\n",
      "Epoch 7400/30000 Training Loss: 0.10160723328590393\n",
      "Epoch 7400/30000 Validation Loss: 0.08092907071113586\n",
      "Epoch 7401/30000 Training Loss: 0.07422522455453873\n",
      "Epoch 7402/30000 Training Loss: 0.07362833619117737\n",
      "Epoch 7403/30000 Training Loss: 0.0727531835436821\n",
      "Epoch 7404/30000 Training Loss: 0.06852272897958755\n",
      "Epoch 7405/30000 Training Loss: 0.09901517629623413\n",
      "Epoch 7406/30000 Training Loss: 0.09139257669448853\n",
      "Epoch 7407/30000 Training Loss: 0.07099746912717819\n",
      "Epoch 7408/30000 Training Loss: 0.1058422103524208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7409/30000 Training Loss: 0.07106123119592667\n",
      "Epoch 7410/30000 Training Loss: 0.07588576525449753\n",
      "Epoch 7410/30000 Validation Loss: 0.0734819695353508\n",
      "Epoch 7411/30000 Training Loss: 0.10360831022262573\n",
      "Epoch 7412/30000 Training Loss: 0.06063569709658623\n",
      "Epoch 7413/30000 Training Loss: 0.09579571336507797\n",
      "Epoch 7414/30000 Training Loss: 0.11671667546033859\n",
      "Epoch 7415/30000 Training Loss: 0.11290717869997025\n",
      "Epoch 7416/30000 Training Loss: 0.08002590388059616\n",
      "Epoch 7417/30000 Training Loss: 0.08366890996694565\n",
      "Epoch 7418/30000 Training Loss: 0.10077736526727676\n",
      "Epoch 7419/30000 Training Loss: 0.0792977437376976\n",
      "Epoch 7420/30000 Training Loss: 0.07715075463056564\n",
      "Epoch 7420/30000 Validation Loss: 0.06782931834459305\n",
      "Epoch 7421/30000 Training Loss: 0.08981596678495407\n",
      "Epoch 7422/30000 Training Loss: 0.07499140501022339\n",
      "Epoch 7423/30000 Training Loss: 0.09172073751688004\n",
      "Epoch 7424/30000 Training Loss: 0.11412516981363297\n",
      "Epoch 7425/30000 Training Loss: 0.1138027086853981\n",
      "Epoch 7426/30000 Training Loss: 0.07967629283666611\n",
      "Epoch 7427/30000 Training Loss: 0.06841472536325455\n",
      "Epoch 7428/30000 Training Loss: 0.08660926669836044\n",
      "Epoch 7429/30000 Training Loss: 0.11000724881887436\n",
      "Epoch 7430/30000 Training Loss: 0.09632370620965958\n",
      "Epoch 7430/30000 Validation Loss: 0.08407672494649887\n",
      "Epoch 7431/30000 Training Loss: 0.0928950309753418\n",
      "Epoch 7432/30000 Training Loss: 0.08036821335554123\n",
      "Epoch 7433/30000 Training Loss: 0.0745779275894165\n",
      "Epoch 7434/30000 Training Loss: 0.07886587828397751\n",
      "Epoch 7435/30000 Training Loss: 0.0681745707988739\n",
      "Epoch 7436/30000 Training Loss: 0.08800273388624191\n",
      "Epoch 7437/30000 Training Loss: 0.0820898711681366\n",
      "Epoch 7438/30000 Training Loss: 0.10685137659311295\n",
      "Epoch 7439/30000 Training Loss: 0.07815726101398468\n",
      "Epoch 7440/30000 Training Loss: 0.091156505048275\n",
      "Epoch 7440/30000 Validation Loss: 0.08905985951423645\n",
      "Epoch 7441/30000 Training Loss: 0.06337837874889374\n",
      "Epoch 7442/30000 Training Loss: 0.07492867112159729\n",
      "Epoch 7443/30000 Training Loss: 0.08373662084341049\n",
      "Epoch 7444/30000 Training Loss: 0.08587946742773056\n",
      "Epoch 7445/30000 Training Loss: 0.10661310702562332\n",
      "Epoch 7446/30000 Training Loss: 0.0994393453001976\n",
      "Epoch 7447/30000 Training Loss: 0.08697997778654099\n",
      "Epoch 7448/30000 Training Loss: 0.09870774298906326\n",
      "Epoch 7449/30000 Training Loss: 0.06515210121870041\n",
      "Epoch 7450/30000 Training Loss: 0.07412473112344742\n",
      "Epoch 7450/30000 Validation Loss: 0.07776955515146255\n",
      "Epoch 7451/30000 Training Loss: 0.086986243724823\n",
      "Epoch 7452/30000 Training Loss: 0.0906180739402771\n",
      "Epoch 7453/30000 Training Loss: 0.08779210597276688\n",
      "Epoch 7454/30000 Training Loss: 0.09690773487091064\n",
      "Epoch 7455/30000 Training Loss: 0.07879073172807693\n",
      "Epoch 7456/30000 Training Loss: 0.11392732709646225\n",
      "Epoch 7457/30000 Training Loss: 0.0965721383690834\n",
      "Epoch 7458/30000 Training Loss: 0.08631414920091629\n",
      "Epoch 7459/30000 Training Loss: 0.09832295030355453\n",
      "Epoch 7460/30000 Training Loss: 0.09147467464208603\n",
      "Epoch 7460/30000 Validation Loss: 0.11300024390220642\n",
      "Epoch 7461/30000 Training Loss: 0.08653341978788376\n",
      "Epoch 7462/30000 Training Loss: 0.08640269190073013\n",
      "Epoch 7463/30000 Training Loss: 0.08578017354011536\n",
      "Epoch 7464/30000 Training Loss: 0.09570788592100143\n",
      "Epoch 7465/30000 Training Loss: 0.07381734251976013\n",
      "Epoch 7466/30000 Training Loss: 0.08805739134550095\n",
      "Epoch 7467/30000 Training Loss: 0.07821992039680481\n",
      "Epoch 7468/30000 Training Loss: 0.08878227323293686\n",
      "Epoch 7469/30000 Training Loss: 0.06973952054977417\n",
      "Epoch 7470/30000 Training Loss: 0.09180089086294174\n",
      "Epoch 7470/30000 Validation Loss: 0.0807824432849884\n",
      "Epoch 7471/30000 Training Loss: 0.07367558032274246\n",
      "Epoch 7472/30000 Training Loss: 0.09019875526428223\n",
      "Epoch 7473/30000 Training Loss: 0.08267863839864731\n",
      "Epoch 7474/30000 Training Loss: 0.07288593053817749\n",
      "Epoch 7475/30000 Training Loss: 0.09733704477548599\n",
      "Epoch 7476/30000 Training Loss: 0.1113487258553505\n",
      "Epoch 7477/30000 Training Loss: 0.0983806848526001\n",
      "Epoch 7478/30000 Training Loss: 0.07140528410673141\n",
      "Epoch 7479/30000 Training Loss: 0.06173373758792877\n",
      "Epoch 7480/30000 Training Loss: 0.07471218705177307\n",
      "Epoch 7480/30000 Validation Loss: 0.08273085951805115\n",
      "Epoch 7481/30000 Training Loss: 0.09095556288957596\n",
      "Epoch 7482/30000 Training Loss: 0.0848105326294899\n",
      "Epoch 7483/30000 Training Loss: 0.0827709510922432\n",
      "Epoch 7484/30000 Training Loss: 0.0935969352722168\n",
      "Epoch 7485/30000 Training Loss: 0.07873868197202682\n",
      "Epoch 7486/30000 Training Loss: 0.08923047035932541\n",
      "Epoch 7487/30000 Training Loss: 0.1054646372795105\n",
      "Epoch 7488/30000 Training Loss: 0.08871302753686905\n",
      "Epoch 7489/30000 Training Loss: 0.07012235373258591\n",
      "Epoch 7490/30000 Training Loss: 0.07770083099603653\n",
      "Epoch 7490/30000 Validation Loss: 0.0726199522614479\n",
      "Epoch 7491/30000 Training Loss: 0.08932050317525864\n",
      "Epoch 7492/30000 Training Loss: 0.07983291149139404\n",
      "Epoch 7493/30000 Training Loss: 0.08203660696744919\n",
      "Epoch 7494/30000 Training Loss: 0.10113035887479782\n",
      "Epoch 7495/30000 Training Loss: 0.07834606617689133\n",
      "Epoch 7496/30000 Training Loss: 0.08399969339370728\n",
      "Epoch 7497/30000 Training Loss: 0.09224576503038406\n",
      "Epoch 7498/30000 Training Loss: 0.09964895248413086\n",
      "Epoch 7499/30000 Training Loss: 0.1029616966843605\n",
      "Epoch 7500/30000 Training Loss: 0.09981316328048706\n",
      "Epoch 7500/30000 Validation Loss: 0.08276673406362534\n",
      "Epoch 7501/30000 Training Loss: 0.08653604984283447\n",
      "Epoch 7502/30000 Training Loss: 0.0952545627951622\n",
      "Epoch 7503/30000 Training Loss: 0.08513524383306503\n",
      "Epoch 7504/30000 Training Loss: 0.0891117975115776\n",
      "Epoch 7505/30000 Training Loss: 0.09187918901443481\n",
      "Epoch 7506/30000 Training Loss: 0.07641246914863586\n",
      "Epoch 7507/30000 Training Loss: 0.09235131740570068\n",
      "Epoch 7508/30000 Training Loss: 0.08731848001480103\n",
      "Epoch 7509/30000 Training Loss: 0.09403295069932938\n",
      "Epoch 7510/30000 Training Loss: 0.07243668287992477\n",
      "Epoch 7510/30000 Validation Loss: 0.09120041131973267\n",
      "Epoch 7511/30000 Training Loss: 0.09060042351484299\n",
      "Epoch 7512/30000 Training Loss: 0.10716887563467026\n",
      "Epoch 7513/30000 Training Loss: 0.0691215991973877\n",
      "Epoch 7514/30000 Training Loss: 0.07311693578958511\n",
      "Epoch 7515/30000 Training Loss: 0.08571010828018188\n",
      "Epoch 7516/30000 Training Loss: 0.07403960078954697\n",
      "Epoch 7517/30000 Training Loss: 0.0641561970114708\n",
      "Epoch 7518/30000 Training Loss: 0.08317378163337708\n",
      "Epoch 7519/30000 Training Loss: 0.07768669724464417\n",
      "Epoch 7520/30000 Training Loss: 0.08605945855379105\n",
      "Epoch 7520/30000 Validation Loss: 0.06965715438127518\n",
      "Epoch 7521/30000 Training Loss: 0.0629972293972969\n",
      "Epoch 7522/30000 Training Loss: 0.10226812958717346\n",
      "Epoch 7523/30000 Training Loss: 0.08896396309137344\n",
      "Epoch 7524/30000 Training Loss: 0.07051080465316772\n",
      "Epoch 7525/30000 Training Loss: 0.08456588536500931\n",
      "Epoch 7526/30000 Training Loss: 0.0762811005115509\n",
      "Epoch 7527/30000 Training Loss: 0.062345460057258606\n",
      "Epoch 7528/30000 Training Loss: 0.08831512928009033\n",
      "Epoch 7529/30000 Training Loss: 0.06957264989614487\n",
      "Epoch 7530/30000 Training Loss: 0.07407328486442566\n",
      "Epoch 7530/30000 Validation Loss: 0.08357157558202744\n",
      "Epoch 7531/30000 Training Loss: 0.08813676238059998\n",
      "Epoch 7532/30000 Training Loss: 0.09877339750528336\n",
      "Epoch 7533/30000 Training Loss: 0.07418999075889587\n",
      "Epoch 7534/30000 Training Loss: 0.10521344095468521\n",
      "Epoch 7535/30000 Training Loss: 0.08262360095977783\n",
      "Epoch 7536/30000 Training Loss: 0.08638078719377518\n",
      "Epoch 7537/30000 Training Loss: 0.07531280070543289\n",
      "Epoch 7538/30000 Training Loss: 0.11608169227838516\n",
      "Epoch 7539/30000 Training Loss: 0.08620258420705795\n",
      "Epoch 7540/30000 Training Loss: 0.08775774389505386\n",
      "Epoch 7540/30000 Validation Loss: 0.08577460050582886\n",
      "Epoch 7541/30000 Training Loss: 0.0788244977593422\n",
      "Epoch 7542/30000 Training Loss: 0.08102069050073624\n",
      "Epoch 7543/30000 Training Loss: 0.11032474040985107\n",
      "Epoch 7544/30000 Training Loss: 0.0816691443324089\n",
      "Epoch 7545/30000 Training Loss: 0.08434689790010452\n",
      "Epoch 7546/30000 Training Loss: 0.11169261485338211\n",
      "Epoch 7547/30000 Training Loss: 0.09792515635490417\n",
      "Epoch 7548/30000 Training Loss: 0.10243380069732666\n",
      "Epoch 7549/30000 Training Loss: 0.07086288183927536\n",
      "Epoch 7550/30000 Training Loss: 0.08247397840023041\n",
      "Epoch 7550/30000 Validation Loss: 0.07273750007152557\n",
      "Epoch 7551/30000 Training Loss: 0.07804512977600098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7552/30000 Training Loss: 0.07653840631246567\n",
      "Epoch 7553/30000 Training Loss: 0.09203097969293594\n",
      "Epoch 7554/30000 Training Loss: 0.07375212758779526\n",
      "Epoch 7555/30000 Training Loss: 0.08915763348340988\n",
      "Epoch 7556/30000 Training Loss: 0.07607265561819077\n",
      "Epoch 7557/30000 Training Loss: 0.08925139904022217\n",
      "Epoch 7558/30000 Training Loss: 0.09251370280981064\n",
      "Epoch 7559/30000 Training Loss: 0.08097787946462631\n",
      "Epoch 7560/30000 Training Loss: 0.07106346637010574\n",
      "Epoch 7560/30000 Validation Loss: 0.09084898978471756\n",
      "Epoch 7561/30000 Training Loss: 0.09973549097776413\n",
      "Epoch 7562/30000 Training Loss: 0.09694767743349075\n",
      "Epoch 7563/30000 Training Loss: 0.1013185977935791\n",
      "Epoch 7564/30000 Training Loss: 0.08394976705312729\n",
      "Epoch 7565/30000 Training Loss: 0.08962666243314743\n",
      "Epoch 7566/30000 Training Loss: 0.076082743704319\n",
      "Epoch 7567/30000 Training Loss: 0.07601872086524963\n",
      "Epoch 7568/30000 Training Loss: 0.0862252339720726\n",
      "Epoch 7569/30000 Training Loss: 0.09224110096693039\n",
      "Epoch 7570/30000 Training Loss: 0.07818155735731125\n",
      "Epoch 7570/30000 Validation Loss: 0.0686366930603981\n",
      "Epoch 7571/30000 Training Loss: 0.08193760365247726\n",
      "Epoch 7572/30000 Training Loss: 0.076284259557724\n",
      "Epoch 7573/30000 Training Loss: 0.07598299533128738\n",
      "Epoch 7574/30000 Training Loss: 0.09022928029298782\n",
      "Epoch 7575/30000 Training Loss: 0.08401473611593246\n",
      "Epoch 7576/30000 Training Loss: 0.0874134823679924\n",
      "Epoch 7577/30000 Training Loss: 0.08173243701457977\n",
      "Epoch 7578/30000 Training Loss: 0.08564334362745285\n",
      "Epoch 7579/30000 Training Loss: 0.09083572030067444\n",
      "Epoch 7580/30000 Training Loss: 0.07114406675100327\n",
      "Epoch 7580/30000 Validation Loss: 0.10624927282333374\n",
      "Epoch 7581/30000 Training Loss: 0.08754400163888931\n",
      "Epoch 7582/30000 Training Loss: 0.08850008994340897\n",
      "Epoch 7583/30000 Training Loss: 0.10130997747182846\n",
      "Epoch 7584/30000 Training Loss: 0.07920757681131363\n",
      "Epoch 7585/30000 Training Loss: 0.08177313208580017\n",
      "Epoch 7586/30000 Training Loss: 0.1025889441370964\n",
      "Epoch 7587/30000 Training Loss: 0.07964148372411728\n",
      "Epoch 7588/30000 Training Loss: 0.09014859050512314\n",
      "Epoch 7589/30000 Training Loss: 0.07132842391729355\n",
      "Epoch 7590/30000 Training Loss: 0.07250243425369263\n",
      "Epoch 7590/30000 Validation Loss: 0.0873090848326683\n",
      "Epoch 7591/30000 Training Loss: 0.07919981330633163\n",
      "Epoch 7592/30000 Training Loss: 0.10652697831392288\n",
      "Epoch 7593/30000 Training Loss: 0.0990925133228302\n",
      "Epoch 7594/30000 Training Loss: 0.07133173197507858\n",
      "Epoch 7595/30000 Training Loss: 0.10412684828042984\n",
      "Epoch 7596/30000 Training Loss: 0.08326473832130432\n",
      "Epoch 7597/30000 Training Loss: 0.09146291762590408\n",
      "Epoch 7598/30000 Training Loss: 0.09145703166723251\n",
      "Epoch 7599/30000 Training Loss: 0.12031704932451248\n",
      "Epoch 7600/30000 Training Loss: 0.08835762739181519\n",
      "Epoch 7600/30000 Validation Loss: 0.06501638144254684\n",
      "Epoch 7601/30000 Training Loss: 0.07809183746576309\n",
      "Epoch 7602/30000 Training Loss: 0.07582166790962219\n",
      "Epoch 7603/30000 Training Loss: 0.07810529321432114\n",
      "Epoch 7604/30000 Training Loss: 0.10039689391851425\n",
      "Epoch 7605/30000 Training Loss: 0.08978458493947983\n",
      "Epoch 7606/30000 Training Loss: 0.07310646027326584\n",
      "Epoch 7607/30000 Training Loss: 0.08685962110757828\n",
      "Epoch 7608/30000 Training Loss: 0.08175325393676758\n",
      "Epoch 7609/30000 Training Loss: 0.10243121534585953\n",
      "Epoch 7610/30000 Training Loss: 0.08005727082490921\n",
      "Epoch 7610/30000 Validation Loss: 0.10036337375640869\n",
      "Epoch 7611/30000 Training Loss: 0.09766752272844315\n",
      "Epoch 7612/30000 Training Loss: 0.0813111960887909\n",
      "Epoch 7613/30000 Training Loss: 0.10153593868017197\n",
      "Epoch 7614/30000 Training Loss: 0.07222189754247665\n",
      "Epoch 7615/30000 Training Loss: 0.08756042271852493\n",
      "Epoch 7616/30000 Training Loss: 0.09096517413854599\n",
      "Epoch 7617/30000 Training Loss: 0.09828004986047745\n",
      "Epoch 7618/30000 Training Loss: 0.06227152422070503\n",
      "Epoch 7619/30000 Training Loss: 0.07347645610570908\n",
      "Epoch 7620/30000 Training Loss: 0.07781389355659485\n",
      "Epoch 7620/30000 Validation Loss: 0.07849881052970886\n",
      "Epoch 7621/30000 Training Loss: 0.10055947303771973\n",
      "Epoch 7622/30000 Training Loss: 0.06974529474973679\n",
      "Epoch 7623/30000 Training Loss: 0.0818629339337349\n",
      "Epoch 7624/30000 Training Loss: 0.09066156297922134\n",
      "Epoch 7625/30000 Training Loss: 0.09521210193634033\n",
      "Epoch 7626/30000 Training Loss: 0.08914995193481445\n",
      "Epoch 7627/30000 Training Loss: 0.0884573832154274\n",
      "Epoch 7628/30000 Training Loss: 0.07278075069189072\n",
      "Epoch 7629/30000 Training Loss: 0.06863471865653992\n",
      "Epoch 7630/30000 Training Loss: 0.06839840859174728\n",
      "Epoch 7630/30000 Validation Loss: 0.08081141859292984\n",
      "Epoch 7631/30000 Training Loss: 0.09588313847780228\n",
      "Epoch 7632/30000 Training Loss: 0.11254232376813889\n",
      "Epoch 7633/30000 Training Loss: 0.07700909674167633\n",
      "Epoch 7634/30000 Training Loss: 0.06811229139566422\n",
      "Epoch 7635/30000 Training Loss: 0.09241896867752075\n",
      "Epoch 7636/30000 Training Loss: 0.09038517624139786\n",
      "Epoch 7637/30000 Training Loss: 0.0797785297036171\n",
      "Epoch 7638/30000 Training Loss: 0.0938764289021492\n",
      "Epoch 7639/30000 Training Loss: 0.06741166859865189\n",
      "Epoch 7640/30000 Training Loss: 0.10850584506988525\n",
      "Epoch 7640/30000 Validation Loss: 0.07217034697532654\n",
      "Epoch 7641/30000 Training Loss: 0.08393651992082596\n",
      "Epoch 7642/30000 Training Loss: 0.07427419722080231\n",
      "Epoch 7643/30000 Training Loss: 0.09304844588041306\n",
      "Epoch 7644/30000 Training Loss: 0.11079812049865723\n",
      "Epoch 7645/30000 Training Loss: 0.08442333340644836\n",
      "Epoch 7646/30000 Training Loss: 0.0913553237915039\n",
      "Epoch 7647/30000 Training Loss: 0.07981065660715103\n",
      "Epoch 7648/30000 Training Loss: 0.07440797984600067\n",
      "Epoch 7649/30000 Training Loss: 0.09881332516670227\n",
      "Epoch 7650/30000 Training Loss: 0.08177266269922256\n",
      "Epoch 7650/30000 Validation Loss: 0.09126755595207214\n",
      "Epoch 7651/30000 Training Loss: 0.08613725751638412\n",
      "Epoch 7652/30000 Training Loss: 0.07566359639167786\n",
      "Epoch 7653/30000 Training Loss: 0.08411864191293716\n",
      "Epoch 7654/30000 Training Loss: 0.07784515619277954\n",
      "Epoch 7655/30000 Training Loss: 0.06874328851699829\n",
      "Epoch 7656/30000 Training Loss: 0.06844452768564224\n",
      "Epoch 7657/30000 Training Loss: 0.0819983184337616\n",
      "Epoch 7658/30000 Training Loss: 0.0789669081568718\n",
      "Epoch 7659/30000 Training Loss: 0.0879242792725563\n",
      "Epoch 7660/30000 Training Loss: 0.06907106190919876\n",
      "Epoch 7660/30000 Validation Loss: 0.0670439824461937\n",
      "Epoch 7661/30000 Training Loss: 0.07128484547138214\n",
      "Epoch 7662/30000 Training Loss: 0.08382145315408707\n",
      "Epoch 7663/30000 Training Loss: 0.08266269415616989\n",
      "Epoch 7664/30000 Training Loss: 0.08142489939928055\n",
      "Epoch 7665/30000 Training Loss: 0.07170092314481735\n",
      "Epoch 7666/30000 Training Loss: 0.07813664525747299\n",
      "Epoch 7667/30000 Training Loss: 0.0792892798781395\n",
      "Epoch 7668/30000 Training Loss: 0.07254647463560104\n",
      "Epoch 7669/30000 Training Loss: 0.09658405184745789\n",
      "Epoch 7670/30000 Training Loss: 0.07458110898733139\n",
      "Epoch 7670/30000 Validation Loss: 0.06926366686820984\n",
      "Epoch 7671/30000 Training Loss: 0.06702835112810135\n",
      "Epoch 7672/30000 Training Loss: 0.07239418476819992\n",
      "Epoch 7673/30000 Training Loss: 0.09893091768026352\n",
      "Epoch 7674/30000 Training Loss: 0.1010032668709755\n",
      "Epoch 7675/30000 Training Loss: 0.08920243382453918\n",
      "Epoch 7676/30000 Training Loss: 0.06693992018699646\n",
      "Epoch 7677/30000 Training Loss: 0.07211174815893173\n",
      "Epoch 7678/30000 Training Loss: 0.08088552951812744\n",
      "Epoch 7679/30000 Training Loss: 0.07913727313280106\n",
      "Epoch 7680/30000 Training Loss: 0.08365263789892197\n",
      "Epoch 7680/30000 Validation Loss: 0.10445085167884827\n",
      "Epoch 7681/30000 Training Loss: 0.08899777382612228\n",
      "Epoch 7682/30000 Training Loss: 0.0896633043885231\n",
      "Epoch 7683/30000 Training Loss: 0.07977590709924698\n",
      "Epoch 7684/30000 Training Loss: 0.07790381461381912\n",
      "Epoch 7685/30000 Training Loss: 0.09346532821655273\n",
      "Epoch 7686/30000 Training Loss: 0.08207083493471146\n",
      "Epoch 7687/30000 Training Loss: 0.10891034454107285\n",
      "Epoch 7688/30000 Training Loss: 0.09568390995264053\n",
      "Epoch 7689/30000 Training Loss: 0.09217887371778488\n",
      "Epoch 7690/30000 Training Loss: 0.06977178156375885\n",
      "Epoch 7690/30000 Validation Loss: 0.08730270713567734\n",
      "Epoch 7691/30000 Training Loss: 0.09035593271255493\n",
      "Epoch 7692/30000 Training Loss: 0.11808183044195175\n",
      "Epoch 7693/30000 Training Loss: 0.083467997610569\n",
      "Epoch 7694/30000 Training Loss: 0.08865318447351456\n",
      "Epoch 7695/30000 Training Loss: 0.06926422566175461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7696/30000 Training Loss: 0.08348330110311508\n",
      "Epoch 7697/30000 Training Loss: 0.07454683631658554\n",
      "Epoch 7698/30000 Training Loss: 0.08266296237707138\n",
      "Epoch 7699/30000 Training Loss: 0.08647807687520981\n",
      "Epoch 7700/30000 Training Loss: 0.10600367933511734\n",
      "Epoch 7700/30000 Validation Loss: 0.10179617255926132\n",
      "Epoch 7701/30000 Training Loss: 0.08964136987924576\n",
      "Epoch 7702/30000 Training Loss: 0.092341847717762\n",
      "Epoch 7703/30000 Training Loss: 0.09994987398386002\n",
      "Epoch 7704/30000 Training Loss: 0.06727787852287292\n",
      "Epoch 7705/30000 Training Loss: 0.08767494559288025\n",
      "Epoch 7706/30000 Training Loss: 0.0770096480846405\n",
      "Epoch 7707/30000 Training Loss: 0.0958295539021492\n",
      "Epoch 7708/30000 Training Loss: 0.0837603211402893\n",
      "Epoch 7709/30000 Training Loss: 0.07626551389694214\n",
      "Epoch 7710/30000 Training Loss: 0.07718091458082199\n",
      "Epoch 7710/30000 Validation Loss: 0.08063214272260666\n",
      "Epoch 7711/30000 Training Loss: 0.08244961500167847\n",
      "Epoch 7712/30000 Training Loss: 0.07903189212083817\n",
      "Epoch 7713/30000 Training Loss: 0.11655215173959732\n",
      "Epoch 7714/30000 Training Loss: 0.09118171781301498\n",
      "Epoch 7715/30000 Training Loss: 0.0664498433470726\n",
      "Epoch 7716/30000 Training Loss: 0.0671195462346077\n",
      "Epoch 7717/30000 Training Loss: 0.07733718305826187\n",
      "Epoch 7718/30000 Training Loss: 0.09692681580781937\n",
      "Epoch 7719/30000 Training Loss: 0.07307867705821991\n",
      "Epoch 7720/30000 Training Loss: 0.09310116618871689\n",
      "Epoch 7720/30000 Validation Loss: 0.09851265698671341\n",
      "Epoch 7721/30000 Training Loss: 0.08899486809968948\n",
      "Epoch 7722/30000 Training Loss: 0.0672965869307518\n",
      "Epoch 7723/30000 Training Loss: 0.08918922394514084\n",
      "Epoch 7724/30000 Training Loss: 0.07816090434789658\n",
      "Epoch 7725/30000 Training Loss: 0.06296495348215103\n",
      "Epoch 7726/30000 Training Loss: 0.09552070498466492\n",
      "Epoch 7727/30000 Training Loss: 0.08031361550092697\n",
      "Epoch 7728/30000 Training Loss: 0.0860210731625557\n",
      "Epoch 7729/30000 Training Loss: 0.10205652564764023\n",
      "Epoch 7730/30000 Training Loss: 0.09755637496709824\n",
      "Epoch 7730/30000 Validation Loss: 0.11109792441129684\n",
      "Epoch 7731/30000 Training Loss: 0.09967126697301865\n",
      "Epoch 7732/30000 Training Loss: 0.07127629220485687\n",
      "Epoch 7733/30000 Training Loss: 0.06968748569488525\n",
      "Epoch 7734/30000 Training Loss: 0.06821651011705399\n",
      "Epoch 7735/30000 Training Loss: 0.10432395339012146\n",
      "Epoch 7736/30000 Training Loss: 0.07513751834630966\n",
      "Epoch 7737/30000 Training Loss: 0.10824901610612869\n",
      "Epoch 7738/30000 Training Loss: 0.08685604482889175\n",
      "Epoch 7739/30000 Training Loss: 0.08329131454229355\n",
      "Epoch 7740/30000 Training Loss: 0.09311503916978836\n",
      "Epoch 7740/30000 Validation Loss: 0.10007169097661972\n",
      "Epoch 7741/30000 Training Loss: 0.08523591607809067\n",
      "Epoch 7742/30000 Training Loss: 0.08278927206993103\n",
      "Epoch 7743/30000 Training Loss: 0.09961346536874771\n",
      "Epoch 7744/30000 Training Loss: 0.06038298085331917\n",
      "Epoch 7745/30000 Training Loss: 0.09517711400985718\n",
      "Epoch 7746/30000 Training Loss: 0.07884246855974197\n",
      "Epoch 7747/30000 Training Loss: 0.07177803665399551\n",
      "Epoch 7748/30000 Training Loss: 0.09681251645088196\n",
      "Epoch 7749/30000 Training Loss: 0.07863897830247879\n",
      "Epoch 7750/30000 Training Loss: 0.07219422608613968\n",
      "Epoch 7750/30000 Validation Loss: 0.09167975187301636\n",
      "Epoch 7751/30000 Training Loss: 0.10844695568084717\n",
      "Epoch 7752/30000 Training Loss: 0.11024024337530136\n",
      "Epoch 7753/30000 Training Loss: 0.1293380707502365\n",
      "Epoch 7754/30000 Training Loss: 0.08100294321775436\n",
      "Epoch 7755/30000 Training Loss: 0.11793005466461182\n",
      "Epoch 7756/30000 Training Loss: 0.09737163037061691\n",
      "Epoch 7757/30000 Training Loss: 0.09875866025686264\n",
      "Epoch 7758/30000 Training Loss: 0.07564204931259155\n",
      "Epoch 7759/30000 Training Loss: 0.09089019894599915\n",
      "Epoch 7760/30000 Training Loss: 0.07705269008874893\n",
      "Epoch 7760/30000 Validation Loss: 0.09657829254865646\n",
      "Epoch 7761/30000 Training Loss: 0.06897402554750443\n",
      "Epoch 7762/30000 Training Loss: 0.07967200130224228\n",
      "Epoch 7763/30000 Training Loss: 0.08420369029045105\n",
      "Epoch 7764/30000 Training Loss: 0.08427309989929199\n",
      "Epoch 7765/30000 Training Loss: 0.0663822591304779\n",
      "Epoch 7766/30000 Training Loss: 0.09687557071447372\n",
      "Epoch 7767/30000 Training Loss: 0.079806387424469\n",
      "Epoch 7768/30000 Training Loss: 0.08140727877616882\n",
      "Epoch 7769/30000 Training Loss: 0.11086515337228775\n",
      "Epoch 7770/30000 Training Loss: 0.08572734147310257\n",
      "Epoch 7770/30000 Validation Loss: 0.07620683312416077\n",
      "Epoch 7771/30000 Training Loss: 0.09947564452886581\n",
      "Epoch 7772/30000 Training Loss: 0.08935797214508057\n",
      "Epoch 7773/30000 Training Loss: 0.06601298600435257\n",
      "Epoch 7774/30000 Training Loss: 0.06615649908781052\n",
      "Epoch 7775/30000 Training Loss: 0.08743660897016525\n",
      "Epoch 7776/30000 Training Loss: 0.0671265497803688\n",
      "Epoch 7777/30000 Training Loss: 0.06546900421380997\n",
      "Epoch 7778/30000 Training Loss: 0.11653965711593628\n",
      "Epoch 7779/30000 Training Loss: 0.09044795483350754\n",
      "Epoch 7780/30000 Training Loss: 0.0960812196135521\n",
      "Epoch 7780/30000 Validation Loss: 0.07637783139944077\n",
      "Epoch 7781/30000 Training Loss: 0.09525186568498611\n",
      "Epoch 7782/30000 Training Loss: 0.07713576406240463\n",
      "Epoch 7783/30000 Training Loss: 0.07560233771800995\n",
      "Epoch 7784/30000 Training Loss: 0.09290634840726852\n",
      "Epoch 7785/30000 Training Loss: 0.07776787877082825\n",
      "Epoch 7786/30000 Training Loss: 0.07252383232116699\n",
      "Epoch 7787/30000 Training Loss: 0.1022777259349823\n",
      "Epoch 7788/30000 Training Loss: 0.08498507738113403\n",
      "Epoch 7789/30000 Training Loss: 0.08461835980415344\n",
      "Epoch 7790/30000 Training Loss: 0.08218532800674438\n",
      "Epoch 7790/30000 Validation Loss: 0.0771048292517662\n",
      "Epoch 7791/30000 Training Loss: 0.0885596051812172\n",
      "Epoch 7792/30000 Training Loss: 0.08650832623243332\n",
      "Epoch 7793/30000 Training Loss: 0.08222857862710953\n",
      "Epoch 7794/30000 Training Loss: 0.08804885298013687\n",
      "Epoch 7795/30000 Training Loss: 0.08355283737182617\n",
      "Epoch 7796/30000 Training Loss: 0.07207822054624557\n",
      "Epoch 7797/30000 Training Loss: 0.0854228213429451\n",
      "Epoch 7798/30000 Training Loss: 0.08081534504890442\n",
      "Epoch 7799/30000 Training Loss: 0.08240056782960892\n",
      "Epoch 7800/30000 Training Loss: 0.08319419622421265\n",
      "Epoch 7800/30000 Validation Loss: 0.10097146779298782\n",
      "Epoch 7801/30000 Training Loss: 0.10778039693832397\n",
      "Epoch 7802/30000 Training Loss: 0.08621913939714432\n",
      "Epoch 7803/30000 Training Loss: 0.08381708711385727\n",
      "Epoch 7804/30000 Training Loss: 0.08608805388212204\n",
      "Epoch 7805/30000 Training Loss: 0.08307453244924545\n",
      "Epoch 7806/30000 Training Loss: 0.0890761986374855\n",
      "Epoch 7807/30000 Training Loss: 0.08812614530324936\n",
      "Epoch 7808/30000 Training Loss: 0.07872284203767776\n",
      "Epoch 7809/30000 Training Loss: 0.0791507288813591\n",
      "Epoch 7810/30000 Training Loss: 0.09351866692304611\n",
      "Epoch 7810/30000 Validation Loss: 0.1018366888165474\n",
      "Epoch 7811/30000 Training Loss: 0.08577441424131393\n",
      "Epoch 7812/30000 Training Loss: 0.09711449593305588\n",
      "Epoch 7813/30000 Training Loss: 0.09561259299516678\n",
      "Epoch 7814/30000 Training Loss: 0.0835845097899437\n",
      "Epoch 7815/30000 Training Loss: 0.0799543485045433\n",
      "Epoch 7816/30000 Training Loss: 0.09118727594614029\n",
      "Epoch 7817/30000 Training Loss: 0.0990723967552185\n",
      "Epoch 7818/30000 Training Loss: 0.08696946501731873\n",
      "Epoch 7819/30000 Training Loss: 0.09010294824838638\n",
      "Epoch 7820/30000 Training Loss: 0.09712079167366028\n",
      "Epoch 7820/30000 Validation Loss: 0.11242860555648804\n",
      "Epoch 7821/30000 Training Loss: 0.0885956808924675\n",
      "Epoch 7822/30000 Training Loss: 0.07527690380811691\n",
      "Epoch 7823/30000 Training Loss: 0.09240864962339401\n",
      "Epoch 7824/30000 Training Loss: 0.0784713551402092\n",
      "Epoch 7825/30000 Training Loss: 0.09120028465986252\n",
      "Epoch 7826/30000 Training Loss: 0.09264298528432846\n",
      "Epoch 7827/30000 Training Loss: 0.0882052481174469\n",
      "Epoch 7828/30000 Training Loss: 0.0890767052769661\n",
      "Epoch 7829/30000 Training Loss: 0.1060713604092598\n",
      "Epoch 7830/30000 Training Loss: 0.07355400174856186\n",
      "Epoch 7830/30000 Validation Loss: 0.11120716482400894\n",
      "Epoch 7831/30000 Training Loss: 0.07806860655546188\n",
      "Epoch 7832/30000 Training Loss: 0.10295212268829346\n",
      "Epoch 7833/30000 Training Loss: 0.09162851423025131\n",
      "Epoch 7834/30000 Training Loss: 0.0803510844707489\n",
      "Epoch 7835/30000 Training Loss: 0.08889066427946091\n",
      "Epoch 7836/30000 Training Loss: 0.10423385351896286\n",
      "Epoch 7837/30000 Training Loss: 0.09977256506681442\n",
      "Epoch 7838/30000 Training Loss: 0.08531805872917175\n",
      "Epoch 7839/30000 Training Loss: 0.08774500340223312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7840/30000 Training Loss: 0.10277695208787918\n",
      "Epoch 7840/30000 Validation Loss: 0.06545592844486237\n",
      "Epoch 7841/30000 Training Loss: 0.0857575461268425\n",
      "Epoch 7842/30000 Training Loss: 0.08397567272186279\n",
      "Epoch 7843/30000 Training Loss: 0.06829234957695007\n",
      "Epoch 7844/30000 Training Loss: 0.08143173903226852\n",
      "Epoch 7845/30000 Training Loss: 0.07968524098396301\n",
      "Epoch 7846/30000 Training Loss: 0.07907838374376297\n",
      "Epoch 7847/30000 Training Loss: 0.09893465042114258\n",
      "Epoch 7848/30000 Training Loss: 0.07391605526208878\n",
      "Epoch 7849/30000 Training Loss: 0.08715581893920898\n",
      "Epoch 7850/30000 Training Loss: 0.08711650967597961\n",
      "Epoch 7850/30000 Validation Loss: 0.06657569110393524\n",
      "Epoch 7851/30000 Training Loss: 0.08161696046590805\n",
      "Epoch 7852/30000 Training Loss: 0.07209061831235886\n",
      "Epoch 7853/30000 Training Loss: 0.08288981765508652\n",
      "Epoch 7854/30000 Training Loss: 0.0750994011759758\n",
      "Epoch 7855/30000 Training Loss: 0.07400858402252197\n",
      "Epoch 7856/30000 Training Loss: 0.08588104695081711\n",
      "Epoch 7857/30000 Training Loss: 0.06994929164648056\n",
      "Epoch 7858/30000 Training Loss: 0.07301507890224457\n",
      "Epoch 7859/30000 Training Loss: 0.10558507591485977\n",
      "Epoch 7860/30000 Training Loss: 0.09613112360239029\n",
      "Epoch 7860/30000 Validation Loss: 0.062207918614149094\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.062207918614149094<=============\n",
      "Epoch 7861/30000 Training Loss: 0.08739400655031204\n",
      "Epoch 7862/30000 Training Loss: 0.1248624324798584\n",
      "Epoch 7863/30000 Training Loss: 0.10003506392240524\n",
      "Epoch 7864/30000 Training Loss: 0.06558806449174881\n",
      "Epoch 7865/30000 Training Loss: 0.07620371133089066\n",
      "Epoch 7866/30000 Training Loss: 0.06749097257852554\n",
      "Epoch 7867/30000 Training Loss: 0.08959732204675674\n",
      "Epoch 7868/30000 Training Loss: 0.09531281143426895\n",
      "Epoch 7869/30000 Training Loss: 0.09091013669967651\n",
      "Epoch 7870/30000 Training Loss: 0.07411161810159683\n",
      "Epoch 7870/30000 Validation Loss: 0.06571047753095627\n",
      "Epoch 7871/30000 Training Loss: 0.1091083511710167\n",
      "Epoch 7872/30000 Training Loss: 0.07710189372301102\n",
      "Epoch 7873/30000 Training Loss: 0.08870475739240646\n",
      "Epoch 7874/30000 Training Loss: 0.09543710201978683\n",
      "Epoch 7875/30000 Training Loss: 0.11623471230268478\n",
      "Epoch 7876/30000 Training Loss: 0.09442829340696335\n",
      "Epoch 7877/30000 Training Loss: 0.08722803741693497\n",
      "Epoch 7878/30000 Training Loss: 0.08472474664449692\n",
      "Epoch 7879/30000 Training Loss: 0.0664244219660759\n",
      "Epoch 7880/30000 Training Loss: 0.10300853103399277\n",
      "Epoch 7880/30000 Validation Loss: 0.08482634276151657\n",
      "Epoch 7881/30000 Training Loss: 0.08973336964845657\n",
      "Epoch 7882/30000 Training Loss: 0.09099400788545609\n",
      "Epoch 7883/30000 Training Loss: 0.08729030936956406\n",
      "Epoch 7884/30000 Training Loss: 0.10007885843515396\n",
      "Epoch 7885/30000 Training Loss: 0.08173947036266327\n",
      "Epoch 7886/30000 Training Loss: 0.078497014939785\n",
      "Epoch 7887/30000 Training Loss: 0.07934945821762085\n",
      "Epoch 7888/30000 Training Loss: 0.08427520841360092\n",
      "Epoch 7889/30000 Training Loss: 0.08750388771295547\n",
      "Epoch 7890/30000 Training Loss: 0.09617310762405396\n",
      "Epoch 7890/30000 Validation Loss: 0.09090489149093628\n",
      "Epoch 7891/30000 Training Loss: 0.10423239320516586\n",
      "Epoch 7892/30000 Training Loss: 0.12054242938756943\n",
      "Epoch 7893/30000 Training Loss: 0.09862738847732544\n",
      "Epoch 7894/30000 Training Loss: 0.08169981837272644\n",
      "Epoch 7895/30000 Training Loss: 0.08512198179960251\n",
      "Epoch 7896/30000 Training Loss: 0.07845789939165115\n",
      "Epoch 7897/30000 Training Loss: 0.09551321715116501\n",
      "Epoch 7898/30000 Training Loss: 0.07301312685012817\n",
      "Epoch 7899/30000 Training Loss: 0.08327869325876236\n",
      "Epoch 7900/30000 Training Loss: 0.07957665622234344\n",
      "Epoch 7900/30000 Validation Loss: 0.08754105120897293\n",
      "Epoch 7901/30000 Training Loss: 0.07808879017829895\n",
      "Epoch 7902/30000 Training Loss: 0.08297444134950638\n",
      "Epoch 7903/30000 Training Loss: 0.0923418402671814\n",
      "Epoch 7904/30000 Training Loss: 0.09838704019784927\n",
      "Epoch 7905/30000 Training Loss: 0.08817621320486069\n",
      "Epoch 7906/30000 Training Loss: 0.07259702682495117\n",
      "Epoch 7907/30000 Training Loss: 0.10157179087400436\n",
      "Epoch 7908/30000 Training Loss: 0.0754973292350769\n",
      "Epoch 7909/30000 Training Loss: 0.07760933041572571\n",
      "Epoch 7910/30000 Training Loss: 0.07828453183174133\n",
      "Epoch 7910/30000 Validation Loss: 0.06593939661979675\n",
      "Epoch 7911/30000 Training Loss: 0.07846910506486893\n",
      "Epoch 7912/30000 Training Loss: 0.08780621737241745\n",
      "Epoch 7913/30000 Training Loss: 0.09855303913354874\n",
      "Epoch 7914/30000 Training Loss: 0.08183599263429642\n",
      "Epoch 7915/30000 Training Loss: 0.089266337454319\n",
      "Epoch 7916/30000 Training Loss: 0.09913766384124756\n",
      "Epoch 7917/30000 Training Loss: 0.07625114172697067\n",
      "Epoch 7918/30000 Training Loss: 0.07512572407722473\n",
      "Epoch 7919/30000 Training Loss: 0.08021368831396103\n",
      "Epoch 7920/30000 Training Loss: 0.08846664428710938\n",
      "Epoch 7920/30000 Validation Loss: 0.095305897295475\n",
      "Epoch 7921/30000 Training Loss: 0.07393823564052582\n",
      "Epoch 7922/30000 Training Loss: 0.11109653860330582\n",
      "Epoch 7923/30000 Training Loss: 0.0945809856057167\n",
      "Epoch 7924/30000 Training Loss: 0.05769723653793335\n",
      "Epoch 7925/30000 Training Loss: 0.07134304195642471\n",
      "Epoch 7926/30000 Training Loss: 0.07641104608774185\n",
      "Epoch 7927/30000 Training Loss: 0.08702246099710464\n",
      "Epoch 7928/30000 Training Loss: 0.09321511536836624\n",
      "Epoch 7929/30000 Training Loss: 0.08300439268350601\n",
      "Epoch 7930/30000 Training Loss: 0.09337816387414932\n",
      "Epoch 7930/30000 Validation Loss: 0.07823619991540909\n",
      "Epoch 7931/30000 Training Loss: 0.09154658764600754\n",
      "Epoch 7932/30000 Training Loss: 0.08014577627182007\n",
      "Epoch 7933/30000 Training Loss: 0.10035684704780579\n",
      "Epoch 7934/30000 Training Loss: 0.07627768069505692\n",
      "Epoch 7935/30000 Training Loss: 0.07623168081045151\n",
      "Epoch 7936/30000 Training Loss: 0.07055357098579407\n",
      "Epoch 7937/30000 Training Loss: 0.08197977393865585\n",
      "Epoch 7938/30000 Training Loss: 0.07597985118627548\n",
      "Epoch 7939/30000 Training Loss: 0.09524529427289963\n",
      "Epoch 7940/30000 Training Loss: 0.08622018247842789\n",
      "Epoch 7940/30000 Validation Loss: 0.07698464393615723\n",
      "Epoch 7941/30000 Training Loss: 0.08198893815279007\n",
      "Epoch 7942/30000 Training Loss: 0.0943608283996582\n",
      "Epoch 7943/30000 Training Loss: 0.0813487097620964\n",
      "Epoch 7944/30000 Training Loss: 0.061495017260313034\n",
      "Epoch 7945/30000 Training Loss: 0.09779495000839233\n",
      "Epoch 7946/30000 Training Loss: 0.08128704875707626\n",
      "Epoch 7947/30000 Training Loss: 0.08884930610656738\n",
      "Epoch 7948/30000 Training Loss: 0.09053916484117508\n",
      "Epoch 7949/30000 Training Loss: 0.07593303918838501\n",
      "Epoch 7950/30000 Training Loss: 0.07044927030801773\n",
      "Epoch 7950/30000 Validation Loss: 0.07831844687461853\n",
      "Epoch 7951/30000 Training Loss: 0.06204230710864067\n",
      "Epoch 7952/30000 Training Loss: 0.09343817085027695\n",
      "Epoch 7953/30000 Training Loss: 0.08171132951974869\n",
      "Epoch 7954/30000 Training Loss: 0.08078259229660034\n",
      "Epoch 7955/30000 Training Loss: 0.08814668655395508\n",
      "Epoch 7956/30000 Training Loss: 0.07495579123497009\n",
      "Epoch 7957/30000 Training Loss: 0.10095426440238953\n",
      "Epoch 7958/30000 Training Loss: 0.0705571323633194\n",
      "Epoch 7959/30000 Training Loss: 0.09274837374687195\n",
      "Epoch 7960/30000 Training Loss: 0.09065907448530197\n",
      "Epoch 7960/30000 Validation Loss: 0.07968444377183914\n",
      "Epoch 7961/30000 Training Loss: 0.07990502566099167\n",
      "Epoch 7962/30000 Training Loss: 0.11267843097448349\n",
      "Epoch 7963/30000 Training Loss: 0.10886455327272415\n",
      "Epoch 7964/30000 Training Loss: 0.07173075526952744\n",
      "Epoch 7965/30000 Training Loss: 0.08110291510820389\n",
      "Epoch 7966/30000 Training Loss: 0.07845167070627213\n",
      "Epoch 7967/30000 Training Loss: 0.09852779656648636\n",
      "Epoch 7968/30000 Training Loss: 0.0757642462849617\n",
      "Epoch 7969/30000 Training Loss: 0.08483395725488663\n",
      "Epoch 7970/30000 Training Loss: 0.08459904044866562\n",
      "Epoch 7970/30000 Validation Loss: 0.07886085659265518\n",
      "Epoch 7971/30000 Training Loss: 0.07610248774290085\n",
      "Epoch 7972/30000 Training Loss: 0.0844385027885437\n",
      "Epoch 7973/30000 Training Loss: 0.08918437361717224\n",
      "Epoch 7974/30000 Training Loss: 0.07758480310440063\n",
      "Epoch 7975/30000 Training Loss: 0.09136257320642471\n",
      "Epoch 7976/30000 Training Loss: 0.07941753417253494\n",
      "Epoch 7977/30000 Training Loss: 0.08751538395881653\n",
      "Epoch 7978/30000 Training Loss: 0.07744356989860535\n",
      "Epoch 7979/30000 Training Loss: 0.07385779172182083\n",
      "Epoch 7980/30000 Training Loss: 0.0808725357055664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7980/30000 Validation Loss: 0.0781850516796112\n",
      "Epoch 7981/30000 Training Loss: 0.07216884940862656\n",
      "Epoch 7982/30000 Training Loss: 0.08461824804544449\n",
      "Epoch 7983/30000 Training Loss: 0.0799221619963646\n",
      "Epoch 7984/30000 Training Loss: 0.07433035224676132\n",
      "Epoch 7985/30000 Training Loss: 0.09535356611013412\n",
      "Epoch 7986/30000 Training Loss: 0.06696584820747375\n",
      "Epoch 7987/30000 Training Loss: 0.07225454598665237\n",
      "Epoch 7988/30000 Training Loss: 0.10925447940826416\n",
      "Epoch 7989/30000 Training Loss: 0.09475838392972946\n",
      "Epoch 7990/30000 Training Loss: 0.06563559919595718\n",
      "Epoch 7990/30000 Validation Loss: 0.07386348396539688\n",
      "Epoch 7991/30000 Training Loss: 0.08808860182762146\n",
      "Epoch 7992/30000 Training Loss: 0.08493039757013321\n",
      "Epoch 7993/30000 Training Loss: 0.08420184254646301\n",
      "Epoch 7994/30000 Training Loss: 0.11199871450662613\n",
      "Epoch 7995/30000 Training Loss: 0.08684146404266357\n",
      "Epoch 7996/30000 Training Loss: 0.10753768682479858\n",
      "Epoch 7997/30000 Training Loss: 0.1002747118473053\n",
      "Epoch 7998/30000 Training Loss: 0.0843515619635582\n",
      "Epoch 7999/30000 Training Loss: 0.08528562635183334\n",
      "Epoch 8000/30000 Training Loss: 0.06833075731992722\n",
      "Epoch 8000/30000 Validation Loss: 0.07880714535713196\n",
      "Epoch 8001/30000 Training Loss: 0.07093749195337296\n",
      "Epoch 8002/30000 Training Loss: 0.0758918970823288\n",
      "Epoch 8003/30000 Training Loss: 0.0778348296880722\n",
      "Epoch 8004/30000 Training Loss: 0.07229531556367874\n",
      "Epoch 8005/30000 Training Loss: 0.07766664028167725\n",
      "Epoch 8006/30000 Training Loss: 0.09409304708242416\n",
      "Epoch 8007/30000 Training Loss: 0.08643727749586105\n",
      "Epoch 8008/30000 Training Loss: 0.09210141748189926\n",
      "Epoch 8009/30000 Training Loss: 0.06952128559350967\n",
      "Epoch 8010/30000 Training Loss: 0.07978027313947678\n",
      "Epoch 8010/30000 Validation Loss: 0.0777108296751976\n",
      "Epoch 8011/30000 Training Loss: 0.08964899182319641\n",
      "Epoch 8012/30000 Training Loss: 0.08724775910377502\n",
      "Epoch 8013/30000 Training Loss: 0.08204451948404312\n",
      "Epoch 8014/30000 Training Loss: 0.09530996531248093\n",
      "Epoch 8015/30000 Training Loss: 0.0876607596874237\n",
      "Epoch 8016/30000 Training Loss: 0.06443661451339722\n",
      "Epoch 8017/30000 Training Loss: 0.08440223336219788\n",
      "Epoch 8018/30000 Training Loss: 0.08095350861549377\n",
      "Epoch 8019/30000 Training Loss: 0.09458252042531967\n",
      "Epoch 8020/30000 Training Loss: 0.07955557852983475\n",
      "Epoch 8020/30000 Validation Loss: 0.08876360207796097\n",
      "Epoch 8021/30000 Training Loss: 0.09513122588396072\n",
      "Epoch 8022/30000 Training Loss: 0.09442272037267685\n",
      "Epoch 8023/30000 Training Loss: 0.10725677013397217\n",
      "Epoch 8024/30000 Training Loss: 0.09336211532354355\n",
      "Epoch 8025/30000 Training Loss: 0.09934204816818237\n",
      "Epoch 8026/30000 Training Loss: 0.09560209512710571\n",
      "Epoch 8027/30000 Training Loss: 0.08300232142210007\n",
      "Epoch 8028/30000 Training Loss: 0.09551786631345749\n",
      "Epoch 8029/30000 Training Loss: 0.07551807165145874\n",
      "Epoch 8030/30000 Training Loss: 0.08298984169960022\n",
      "Epoch 8030/30000 Validation Loss: 0.07790745794773102\n",
      "Epoch 8031/30000 Training Loss: 0.10589001327753067\n",
      "Epoch 8032/30000 Training Loss: 0.07688595354557037\n",
      "Epoch 8033/30000 Training Loss: 0.06951164454221725\n",
      "Epoch 8034/30000 Training Loss: 0.06602021306753159\n",
      "Epoch 8035/30000 Training Loss: 0.06023262068629265\n",
      "Epoch 8036/30000 Training Loss: 0.07643061876296997\n",
      "Epoch 8037/30000 Training Loss: 0.08563832193613052\n",
      "Epoch 8038/30000 Training Loss: 0.07101931422948837\n",
      "Epoch 8039/30000 Training Loss: 0.07281544059515\n",
      "Epoch 8040/30000 Training Loss: 0.07575124502182007\n",
      "Epoch 8040/30000 Validation Loss: 0.08657637238502502\n",
      "Epoch 8041/30000 Training Loss: 0.06870052218437195\n",
      "Epoch 8042/30000 Training Loss: 0.08213912695646286\n",
      "Epoch 8043/30000 Training Loss: 0.0703665092587471\n",
      "Epoch 8044/30000 Training Loss: 0.09140843898057938\n",
      "Epoch 8045/30000 Training Loss: 0.0669388547539711\n",
      "Epoch 8046/30000 Training Loss: 0.09934309870004654\n",
      "Epoch 8047/30000 Training Loss: 0.09431126713752747\n",
      "Epoch 8048/30000 Training Loss: 0.08136186748743057\n",
      "Epoch 8049/30000 Training Loss: 0.08580905199050903\n",
      "Epoch 8050/30000 Training Loss: 0.06767424941062927\n",
      "Epoch 8050/30000 Validation Loss: 0.08680602163076401\n",
      "Epoch 8051/30000 Training Loss: 0.08041410893201828\n",
      "Epoch 8052/30000 Training Loss: 0.07785984128713608\n",
      "Epoch 8053/30000 Training Loss: 0.09492310136556625\n",
      "Epoch 8054/30000 Training Loss: 0.08736709505319595\n",
      "Epoch 8055/30000 Training Loss: 0.08013686537742615\n",
      "Epoch 8056/30000 Training Loss: 0.0791737511754036\n",
      "Epoch 8057/30000 Training Loss: 0.08198818564414978\n",
      "Epoch 8058/30000 Training Loss: 0.08545731753110886\n",
      "Epoch 8059/30000 Training Loss: 0.08744525909423828\n",
      "Epoch 8060/30000 Training Loss: 0.07722553610801697\n",
      "Epoch 8060/30000 Validation Loss: 0.08334651589393616\n",
      "Epoch 8061/30000 Training Loss: 0.08505459874868393\n",
      "Epoch 8062/30000 Training Loss: 0.08149046450853348\n",
      "Epoch 8063/30000 Training Loss: 0.07370248436927795\n",
      "Epoch 8064/30000 Training Loss: 0.06173325702548027\n",
      "Epoch 8065/30000 Training Loss: 0.08219496160745621\n",
      "Epoch 8066/30000 Training Loss: 0.08375268429517746\n",
      "Epoch 8067/30000 Training Loss: 0.0861726701259613\n",
      "Epoch 8068/30000 Training Loss: 0.07790357619524002\n",
      "Epoch 8069/30000 Training Loss: 0.08929089456796646\n",
      "Epoch 8070/30000 Training Loss: 0.08071493357419968\n",
      "Epoch 8070/30000 Validation Loss: 0.09778865426778793\n",
      "Epoch 8071/30000 Training Loss: 0.09840679168701172\n",
      "Epoch 8072/30000 Training Loss: 0.08509141951799393\n",
      "Epoch 8073/30000 Training Loss: 0.07012330740690231\n",
      "Epoch 8074/30000 Training Loss: 0.08623865246772766\n",
      "Epoch 8075/30000 Training Loss: 0.0810825452208519\n",
      "Epoch 8076/30000 Training Loss: 0.09780193120241165\n",
      "Epoch 8077/30000 Training Loss: 0.06106990575790405\n",
      "Epoch 8078/30000 Training Loss: 0.06214549019932747\n",
      "Epoch 8079/30000 Training Loss: 0.0647236779332161\n",
      "Epoch 8080/30000 Training Loss: 0.10901115089654922\n",
      "Epoch 8080/30000 Validation Loss: 0.08781609684228897\n",
      "Epoch 8081/30000 Training Loss: 0.07937636226415634\n",
      "Epoch 8082/30000 Training Loss: 0.09451284259557724\n",
      "Epoch 8083/30000 Training Loss: 0.08984637260437012\n",
      "Epoch 8084/30000 Training Loss: 0.10855718702077866\n",
      "Epoch 8085/30000 Training Loss: 0.078978031873703\n",
      "Epoch 8086/30000 Training Loss: 0.08733215183019638\n",
      "Epoch 8087/30000 Training Loss: 0.06989871710538864\n",
      "Epoch 8088/30000 Training Loss: 0.0913732573390007\n",
      "Epoch 8089/30000 Training Loss: 0.07927611470222473\n",
      "Epoch 8090/30000 Training Loss: 0.08280225843191147\n",
      "Epoch 8090/30000 Validation Loss: 0.11066049337387085\n",
      "Epoch 8091/30000 Training Loss: 0.07427947968244553\n",
      "Epoch 8092/30000 Training Loss: 0.06848479062318802\n",
      "Epoch 8093/30000 Training Loss: 0.08440455794334412\n",
      "Epoch 8094/30000 Training Loss: 0.09767472743988037\n",
      "Epoch 8095/30000 Training Loss: 0.08956038951873779\n",
      "Epoch 8096/30000 Training Loss: 0.08132755756378174\n",
      "Epoch 8097/30000 Training Loss: 0.09350981563329697\n",
      "Epoch 8098/30000 Training Loss: 0.06773475557565689\n",
      "Epoch 8099/30000 Training Loss: 0.0661291778087616\n",
      "Epoch 8100/30000 Training Loss: 0.10251583904027939\n",
      "Epoch 8100/30000 Validation Loss: 0.09070875495672226\n",
      "Epoch 8101/30000 Training Loss: 0.07977233082056046\n",
      "Epoch 8102/30000 Training Loss: 0.08649003505706787\n",
      "Epoch 8103/30000 Training Loss: 0.07393264025449753\n",
      "Epoch 8104/30000 Training Loss: 0.0769830271601677\n",
      "Epoch 8105/30000 Training Loss: 0.08535343408584595\n",
      "Epoch 8106/30000 Training Loss: 0.06946101784706116\n",
      "Epoch 8107/30000 Training Loss: 0.08301308006048203\n",
      "Epoch 8108/30000 Training Loss: 0.0584011934697628\n",
      "Epoch 8109/30000 Training Loss: 0.07976678758859634\n",
      "Epoch 8110/30000 Training Loss: 0.10127433389425278\n",
      "Epoch 8110/30000 Validation Loss: 0.11003986746072769\n",
      "Epoch 8111/30000 Training Loss: 0.08816158026456833\n",
      "Epoch 8112/30000 Training Loss: 0.0668320506811142\n",
      "Epoch 8113/30000 Training Loss: 0.09379977732896805\n",
      "Epoch 8114/30000 Training Loss: 0.08471223711967468\n",
      "Epoch 8115/30000 Training Loss: 0.07585888355970383\n",
      "Epoch 8116/30000 Training Loss: 0.09540646523237228\n",
      "Epoch 8117/30000 Training Loss: 0.09674464911222458\n",
      "Epoch 8118/30000 Training Loss: 0.0753648579120636\n",
      "Epoch 8119/30000 Training Loss: 0.1049203872680664\n",
      "Epoch 8120/30000 Training Loss: 0.07743130624294281\n",
      "Epoch 8120/30000 Validation Loss: 0.07888331264257431\n",
      "Epoch 8121/30000 Training Loss: 0.08048243075609207\n",
      "Epoch 8122/30000 Training Loss: 0.10052093118429184\n",
      "Epoch 8123/30000 Training Loss: 0.06961661577224731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8124/30000 Training Loss: 0.08486175537109375\n",
      "Epoch 8125/30000 Training Loss: 0.06645840406417847\n",
      "Epoch 8126/30000 Training Loss: 0.09395813941955566\n",
      "Epoch 8127/30000 Training Loss: 0.07383977621793747\n",
      "Epoch 8128/30000 Training Loss: 0.07303085178136826\n",
      "Epoch 8129/30000 Training Loss: 0.07347693294286728\n",
      "Epoch 8130/30000 Training Loss: 0.09807463735342026\n",
      "Epoch 8130/30000 Validation Loss: 0.08437689393758774\n",
      "Epoch 8131/30000 Training Loss: 0.07615012675523758\n",
      "Epoch 8132/30000 Training Loss: 0.0792967826128006\n",
      "Epoch 8133/30000 Training Loss: 0.07871231436729431\n",
      "Epoch 8134/30000 Training Loss: 0.07417606562376022\n",
      "Epoch 8135/30000 Training Loss: 0.08392324298620224\n",
      "Epoch 8136/30000 Training Loss: 0.08896193653345108\n",
      "Epoch 8137/30000 Training Loss: 0.09807879477739334\n",
      "Epoch 8138/30000 Training Loss: 0.07556454092264175\n",
      "Epoch 8139/30000 Training Loss: 0.07396355271339417\n",
      "Epoch 8140/30000 Training Loss: 0.07674532383680344\n",
      "Epoch 8140/30000 Validation Loss: 0.0799488052725792\n",
      "Epoch 8141/30000 Training Loss: 0.06838623434305191\n",
      "Epoch 8142/30000 Training Loss: 0.0867883488535881\n",
      "Epoch 8143/30000 Training Loss: 0.09062326699495316\n",
      "Epoch 8144/30000 Training Loss: 0.07735923677682877\n",
      "Epoch 8145/30000 Training Loss: 0.10535556077957153\n",
      "Epoch 8146/30000 Training Loss: 0.10649636387825012\n",
      "Epoch 8147/30000 Training Loss: 0.08318302035331726\n",
      "Epoch 8148/30000 Training Loss: 0.10653355717658997\n",
      "Epoch 8149/30000 Training Loss: 0.06930030882358551\n",
      "Epoch 8150/30000 Training Loss: 0.0702824518084526\n",
      "Epoch 8150/30000 Validation Loss: 0.0838412269949913\n",
      "Epoch 8151/30000 Training Loss: 0.07500908523797989\n",
      "Epoch 8152/30000 Training Loss: 0.10178092122077942\n",
      "Epoch 8153/30000 Training Loss: 0.094534732401371\n",
      "Epoch 8154/30000 Training Loss: 0.08434656262397766\n",
      "Epoch 8155/30000 Training Loss: 0.07208273559808731\n",
      "Epoch 8156/30000 Training Loss: 0.07302757352590561\n",
      "Epoch 8157/30000 Training Loss: 0.0803850069642067\n",
      "Epoch 8158/30000 Training Loss: 0.05924176052212715\n",
      "Epoch 8159/30000 Training Loss: 0.06095646694302559\n",
      "Epoch 8160/30000 Training Loss: 0.07747367769479752\n",
      "Epoch 8160/30000 Validation Loss: 0.10980819910764694\n",
      "Epoch 8161/30000 Training Loss: 0.09108007699251175\n",
      "Epoch 8162/30000 Training Loss: 0.09141556173563004\n",
      "Epoch 8163/30000 Training Loss: 0.08957314491271973\n",
      "Epoch 8164/30000 Training Loss: 0.07813048362731934\n",
      "Epoch 8165/30000 Training Loss: 0.07501374930143356\n",
      "Epoch 8166/30000 Training Loss: 0.09613033384084702\n",
      "Epoch 8167/30000 Training Loss: 0.08461341261863708\n",
      "Epoch 8168/30000 Training Loss: 0.09034878015518188\n",
      "Epoch 8169/30000 Training Loss: 0.09171649068593979\n",
      "Epoch 8170/30000 Training Loss: 0.0750788226723671\n",
      "Epoch 8170/30000 Validation Loss: 0.06995803862810135\n",
      "Epoch 8171/30000 Training Loss: 0.08024346083402634\n",
      "Epoch 8172/30000 Training Loss: 0.09110171347856522\n",
      "Epoch 8173/30000 Training Loss: 0.07657447457313538\n",
      "Epoch 8174/30000 Training Loss: 0.0806976780295372\n",
      "Epoch 8175/30000 Training Loss: 0.0792710930109024\n",
      "Epoch 8176/30000 Training Loss: 0.08632711321115494\n",
      "Epoch 8177/30000 Training Loss: 0.08106163144111633\n",
      "Epoch 8178/30000 Training Loss: 0.08741673082113266\n",
      "Epoch 8179/30000 Training Loss: 0.0867486372590065\n",
      "Epoch 8180/30000 Training Loss: 0.11594092845916748\n",
      "Epoch 8180/30000 Validation Loss: 0.09110251814126968\n",
      "Epoch 8181/30000 Training Loss: 0.10992171615362167\n",
      "Epoch 8182/30000 Training Loss: 0.08415146917104721\n",
      "Epoch 8183/30000 Training Loss: 0.07590478658676147\n",
      "Epoch 8184/30000 Training Loss: 0.07831986993551254\n",
      "Epoch 8185/30000 Training Loss: 0.08261791616678238\n",
      "Epoch 8186/30000 Training Loss: 0.08443877100944519\n",
      "Epoch 8187/30000 Training Loss: 0.07175656408071518\n",
      "Epoch 8188/30000 Training Loss: 0.10299801081418991\n",
      "Epoch 8189/30000 Training Loss: 0.07485578209161758\n",
      "Epoch 8190/30000 Training Loss: 0.08214329928159714\n",
      "Epoch 8190/30000 Validation Loss: 0.09179963916540146\n",
      "Epoch 8191/30000 Training Loss: 0.07332449406385422\n",
      "Epoch 8192/30000 Training Loss: 0.07804547995328903\n",
      "Epoch 8193/30000 Training Loss: 0.06674576550722122\n",
      "Epoch 8194/30000 Training Loss: 0.09439176321029663\n",
      "Epoch 8195/30000 Training Loss: 0.09029180556535721\n",
      "Epoch 8196/30000 Training Loss: 0.09613745659589767\n",
      "Epoch 8197/30000 Training Loss: 0.07694759219884872\n",
      "Epoch 8198/30000 Training Loss: 0.0829317569732666\n",
      "Epoch 8199/30000 Training Loss: 0.07793908566236496\n",
      "Epoch 8200/30000 Training Loss: 0.07741958647966385\n",
      "Epoch 8200/30000 Validation Loss: 0.07352569699287415\n",
      "Epoch 8201/30000 Training Loss: 0.09662619233131409\n",
      "Epoch 8202/30000 Training Loss: 0.08602000027894974\n",
      "Epoch 8203/30000 Training Loss: 0.0959262028336525\n",
      "Epoch 8204/30000 Training Loss: 0.10655134171247482\n",
      "Epoch 8205/30000 Training Loss: 0.09687191247940063\n",
      "Epoch 8206/30000 Training Loss: 0.06631538271903992\n",
      "Epoch 8207/30000 Training Loss: 0.06487278640270233\n",
      "Epoch 8208/30000 Training Loss: 0.06750721484422684\n",
      "Epoch 8209/30000 Training Loss: 0.09280166774988174\n",
      "Epoch 8210/30000 Training Loss: 0.07481252402067184\n",
      "Epoch 8210/30000 Validation Loss: 0.06550568342208862\n",
      "Epoch 8211/30000 Training Loss: 0.08834423869848251\n",
      "Epoch 8212/30000 Training Loss: 0.09302103519439697\n",
      "Epoch 8213/30000 Training Loss: 0.10513889789581299\n",
      "Epoch 8214/30000 Training Loss: 0.07239562273025513\n",
      "Epoch 8215/30000 Training Loss: 0.1196138858795166\n",
      "Epoch 8216/30000 Training Loss: 0.1120874285697937\n",
      "Epoch 8217/30000 Training Loss: 0.06778524070978165\n",
      "Epoch 8218/30000 Training Loss: 0.0886421799659729\n",
      "Epoch 8219/30000 Training Loss: 0.10459399968385696\n",
      "Epoch 8220/30000 Training Loss: 0.06854020804166794\n",
      "Epoch 8220/30000 Validation Loss: 0.08376330137252808\n",
      "Epoch 8221/30000 Training Loss: 0.08366185426712036\n",
      "Epoch 8222/30000 Training Loss: 0.09668298810720444\n",
      "Epoch 8223/30000 Training Loss: 0.08108291774988174\n",
      "Epoch 8224/30000 Training Loss: 0.09207141399383545\n",
      "Epoch 8225/30000 Training Loss: 0.07136819511651993\n",
      "Epoch 8226/30000 Training Loss: 0.09230770915746689\n",
      "Epoch 8227/30000 Training Loss: 0.09517145156860352\n",
      "Epoch 8228/30000 Training Loss: 0.0777696892619133\n",
      "Epoch 8229/30000 Training Loss: 0.09181710332632065\n",
      "Epoch 8230/30000 Training Loss: 0.07261105626821518\n",
      "Epoch 8230/30000 Validation Loss: 0.07057535648345947\n",
      "Epoch 8231/30000 Training Loss: 0.09683523327112198\n",
      "Epoch 8232/30000 Training Loss: 0.0794273242354393\n",
      "Epoch 8233/30000 Training Loss: 0.06859774142503738\n",
      "Epoch 8234/30000 Training Loss: 0.08132584393024445\n",
      "Epoch 8235/30000 Training Loss: 0.08194144815206528\n",
      "Epoch 8236/30000 Training Loss: 0.0704241394996643\n",
      "Epoch 8237/30000 Training Loss: 0.08556070923805237\n",
      "Epoch 8238/30000 Training Loss: 0.09059692174196243\n",
      "Epoch 8239/30000 Training Loss: 0.067229725420475\n",
      "Epoch 8240/30000 Training Loss: 0.09117091447114944\n",
      "Epoch 8240/30000 Validation Loss: 0.1032676175236702\n",
      "Epoch 8241/30000 Training Loss: 0.09488987922668457\n",
      "Epoch 8242/30000 Training Loss: 0.07319668680429459\n",
      "Epoch 8243/30000 Training Loss: 0.09215153008699417\n",
      "Epoch 8244/30000 Training Loss: 0.07842227816581726\n",
      "Epoch 8245/30000 Training Loss: 0.06283385306596756\n",
      "Epoch 8246/30000 Training Loss: 0.09878939390182495\n",
      "Epoch 8247/30000 Training Loss: 0.09400662779808044\n",
      "Epoch 8248/30000 Training Loss: 0.08940758556127548\n",
      "Epoch 8249/30000 Training Loss: 0.07486382126808167\n",
      "Epoch 8250/30000 Training Loss: 0.07317579537630081\n",
      "Epoch 8250/30000 Validation Loss: 0.08442016690969467\n",
      "Epoch 8251/30000 Training Loss: 0.05983387306332588\n",
      "Epoch 8252/30000 Training Loss: 0.09223845601081848\n",
      "Epoch 8253/30000 Training Loss: 0.0744871124625206\n",
      "Epoch 8254/30000 Training Loss: 0.08132665604352951\n",
      "Epoch 8255/30000 Training Loss: 0.06756321340799332\n",
      "Epoch 8256/30000 Training Loss: 0.09733224660158157\n",
      "Epoch 8257/30000 Training Loss: 0.08705338090658188\n",
      "Epoch 8258/30000 Training Loss: 0.10849809646606445\n",
      "Epoch 8259/30000 Training Loss: 0.08085518330335617\n",
      "Epoch 8260/30000 Training Loss: 0.10282831639051437\n",
      "Epoch 8260/30000 Validation Loss: 0.08182823657989502\n",
      "Epoch 8261/30000 Training Loss: 0.10182905942201614\n",
      "Epoch 8262/30000 Training Loss: 0.09071461111307144\n",
      "Epoch 8263/30000 Training Loss: 0.08377790451049805\n",
      "Epoch 8264/30000 Training Loss: 0.07394585758447647\n",
      "Epoch 8265/30000 Training Loss: 0.07473617792129517\n",
      "Epoch 8266/30000 Training Loss: 0.07986101508140564\n",
      "Epoch 8267/30000 Training Loss: 0.07598728686571121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8268/30000 Training Loss: 0.07367014139890671\n",
      "Epoch 8269/30000 Training Loss: 0.1037321388721466\n",
      "Epoch 8270/30000 Training Loss: 0.08872220665216446\n",
      "Epoch 8270/30000 Validation Loss: 0.08144894242286682\n",
      "Epoch 8271/30000 Training Loss: 0.09234466403722763\n",
      "Epoch 8272/30000 Training Loss: 0.08308078348636627\n",
      "Epoch 8273/30000 Training Loss: 0.0792224109172821\n",
      "Epoch 8274/30000 Training Loss: 0.09743324667215347\n",
      "Epoch 8275/30000 Training Loss: 0.0920930877327919\n",
      "Epoch 8276/30000 Training Loss: 0.10429652780294418\n",
      "Epoch 8277/30000 Training Loss: 0.07177301496267319\n",
      "Epoch 8278/30000 Training Loss: 0.07736694067716599\n",
      "Epoch 8279/30000 Training Loss: 0.09661075472831726\n",
      "Epoch 8280/30000 Training Loss: 0.09309637546539307\n",
      "Epoch 8280/30000 Validation Loss: 0.09652014821767807\n",
      "Epoch 8281/30000 Training Loss: 0.08795437216758728\n",
      "Epoch 8282/30000 Training Loss: 0.0662388727068901\n",
      "Epoch 8283/30000 Training Loss: 0.08133747428655624\n",
      "Epoch 8284/30000 Training Loss: 0.07690117508172989\n",
      "Epoch 8285/30000 Training Loss: 0.08032837510108948\n",
      "Epoch 8286/30000 Training Loss: 0.0746925100684166\n",
      "Epoch 8287/30000 Training Loss: 0.119296133518219\n",
      "Epoch 8288/30000 Training Loss: 0.07731720060110092\n",
      "Epoch 8289/30000 Training Loss: 0.09239653497934341\n",
      "Epoch 8290/30000 Training Loss: 0.10268544405698776\n",
      "Epoch 8290/30000 Validation Loss: 0.08915615826845169\n",
      "Epoch 8291/30000 Training Loss: 0.07311756163835526\n",
      "Epoch 8292/30000 Training Loss: 0.09412864595651627\n",
      "Epoch 8293/30000 Training Loss: 0.07925555855035782\n",
      "Epoch 8294/30000 Training Loss: 0.07737939804792404\n",
      "Epoch 8295/30000 Training Loss: 0.07671549171209335\n",
      "Epoch 8296/30000 Training Loss: 0.0873456671833992\n",
      "Epoch 8297/30000 Training Loss: 0.07823268324136734\n",
      "Epoch 8298/30000 Training Loss: 0.08954308182001114\n",
      "Epoch 8299/30000 Training Loss: 0.08274263888597488\n",
      "Epoch 8300/30000 Training Loss: 0.09389060735702515\n",
      "Epoch 8300/30000 Validation Loss: 0.0823357030749321\n",
      "Epoch 8301/30000 Training Loss: 0.07930973172187805\n",
      "Epoch 8302/30000 Training Loss: 0.09795811027288437\n",
      "Epoch 8303/30000 Training Loss: 0.07597743719816208\n",
      "Epoch 8304/30000 Training Loss: 0.07526836544275284\n",
      "Epoch 8305/30000 Training Loss: 0.09496007114648819\n",
      "Epoch 8306/30000 Training Loss: 0.08161742985248566\n",
      "Epoch 8307/30000 Training Loss: 0.0739075243473053\n",
      "Epoch 8308/30000 Training Loss: 0.08859583735466003\n",
      "Epoch 8309/30000 Training Loss: 0.09373196214437485\n",
      "Epoch 8310/30000 Training Loss: 0.07687880843877792\n",
      "Epoch 8310/30000 Validation Loss: 0.08983016014099121\n",
      "Epoch 8311/30000 Training Loss: 0.09267576783895493\n",
      "Epoch 8312/30000 Training Loss: 0.0999259278178215\n",
      "Epoch 8313/30000 Training Loss: 0.07396649569272995\n",
      "Epoch 8314/30000 Training Loss: 0.09034416079521179\n",
      "Epoch 8315/30000 Training Loss: 0.08440495282411575\n",
      "Epoch 8316/30000 Training Loss: 0.09241775423288345\n",
      "Epoch 8317/30000 Training Loss: 0.08479472994804382\n",
      "Epoch 8318/30000 Training Loss: 0.1008768379688263\n",
      "Epoch 8319/30000 Training Loss: 0.0985373929142952\n",
      "Epoch 8320/30000 Training Loss: 0.09455660730600357\n",
      "Epoch 8320/30000 Validation Loss: 0.07514963299036026\n",
      "Epoch 8321/30000 Training Loss: 0.07465753704309464\n",
      "Epoch 8322/30000 Training Loss: 0.07724297791719437\n",
      "Epoch 8323/30000 Training Loss: 0.08411555737257004\n",
      "Epoch 8324/30000 Training Loss: 0.09593605250120163\n",
      "Epoch 8325/30000 Training Loss: 0.09583286195993423\n",
      "Epoch 8326/30000 Training Loss: 0.09168872982263565\n",
      "Epoch 8327/30000 Training Loss: 0.08069100230932236\n",
      "Epoch 8328/30000 Training Loss: 0.07505745440721512\n",
      "Epoch 8329/30000 Training Loss: 0.1064940094947815\n",
      "Epoch 8330/30000 Training Loss: 0.1015552505850792\n",
      "Epoch 8330/30000 Validation Loss: 0.07250448316335678\n",
      "Epoch 8331/30000 Training Loss: 0.08070769160985947\n",
      "Epoch 8332/30000 Training Loss: 0.06844552606344223\n",
      "Epoch 8333/30000 Training Loss: 0.11111265420913696\n",
      "Epoch 8334/30000 Training Loss: 0.0940525233745575\n",
      "Epoch 8335/30000 Training Loss: 0.08962830156087875\n",
      "Epoch 8336/30000 Training Loss: 0.07207966595888138\n",
      "Epoch 8337/30000 Training Loss: 0.07426237314939499\n",
      "Epoch 8338/30000 Training Loss: 0.07690182328224182\n",
      "Epoch 8339/30000 Training Loss: 0.0922270193696022\n",
      "Epoch 8340/30000 Training Loss: 0.10482659935951233\n",
      "Epoch 8340/30000 Validation Loss: 0.0886705294251442\n",
      "Epoch 8341/30000 Training Loss: 0.06985079497098923\n",
      "Epoch 8342/30000 Training Loss: 0.09257044643163681\n",
      "Epoch 8343/30000 Training Loss: 0.07664201408624649\n",
      "Epoch 8344/30000 Training Loss: 0.07884755730628967\n",
      "Epoch 8345/30000 Training Loss: 0.09477245807647705\n",
      "Epoch 8346/30000 Training Loss: 0.08564374595880508\n",
      "Epoch 8347/30000 Training Loss: 0.08873406797647476\n",
      "Epoch 8348/30000 Training Loss: 0.08195430040359497\n",
      "Epoch 8349/30000 Training Loss: 0.08611482381820679\n",
      "Epoch 8350/30000 Training Loss: 0.09571588039398193\n",
      "Epoch 8350/30000 Validation Loss: 0.10755214840173721\n",
      "Epoch 8351/30000 Training Loss: 0.07871086150407791\n",
      "Epoch 8352/30000 Training Loss: 0.08721449971199036\n",
      "Epoch 8353/30000 Training Loss: 0.10618060827255249\n",
      "Epoch 8354/30000 Training Loss: 0.07500562816858292\n",
      "Epoch 8355/30000 Training Loss: 0.07782313972711563\n",
      "Epoch 8356/30000 Training Loss: 0.07500636577606201\n",
      "Epoch 8357/30000 Training Loss: 0.09247539192438126\n",
      "Epoch 8358/30000 Training Loss: 0.06915079802274704\n",
      "Epoch 8359/30000 Training Loss: 0.06356777995824814\n",
      "Epoch 8360/30000 Training Loss: 0.08924194425344467\n",
      "Epoch 8360/30000 Validation Loss: 0.07060251384973526\n",
      "Epoch 8361/30000 Training Loss: 0.08431973308324814\n",
      "Epoch 8362/30000 Training Loss: 0.0851144790649414\n",
      "Epoch 8363/30000 Training Loss: 0.09930873662233353\n",
      "Epoch 8364/30000 Training Loss: 0.09856841713190079\n",
      "Epoch 8365/30000 Training Loss: 0.07232766598463058\n",
      "Epoch 8366/30000 Training Loss: 0.08388743549585342\n",
      "Epoch 8367/30000 Training Loss: 0.07319185882806778\n",
      "Epoch 8368/30000 Training Loss: 0.09473365545272827\n",
      "Epoch 8369/30000 Training Loss: 0.07488531619310379\n",
      "Epoch 8370/30000 Training Loss: 0.07621174305677414\n",
      "Epoch 8370/30000 Validation Loss: 0.08666449785232544\n",
      "Epoch 8371/30000 Training Loss: 0.08002382516860962\n",
      "Epoch 8372/30000 Training Loss: 0.07318820059299469\n",
      "Epoch 8373/30000 Training Loss: 0.0620320700109005\n",
      "Epoch 8374/30000 Training Loss: 0.08394593000411987\n",
      "Epoch 8375/30000 Training Loss: 0.09475412219762802\n",
      "Epoch 8376/30000 Training Loss: 0.08690228313207626\n",
      "Epoch 8377/30000 Training Loss: 0.09481919556856155\n",
      "Epoch 8378/30000 Training Loss: 0.06719911843538284\n",
      "Epoch 8379/30000 Training Loss: 0.07082787901163101\n",
      "Epoch 8380/30000 Training Loss: 0.07620314508676529\n",
      "Epoch 8380/30000 Validation Loss: 0.08365180343389511\n",
      "Epoch 8381/30000 Training Loss: 0.07652812451124191\n",
      "Epoch 8382/30000 Training Loss: 0.0837635025382042\n",
      "Epoch 8383/30000 Training Loss: 0.08300266414880753\n",
      "Epoch 8384/30000 Training Loss: 0.1006825640797615\n",
      "Epoch 8385/30000 Training Loss: 0.10572317242622375\n",
      "Epoch 8386/30000 Training Loss: 0.07991210371255875\n",
      "Epoch 8387/30000 Training Loss: 0.07697081565856934\n",
      "Epoch 8388/30000 Training Loss: 0.10754835605621338\n",
      "Epoch 8389/30000 Training Loss: 0.09134504944086075\n",
      "Epoch 8390/30000 Training Loss: 0.06145920231938362\n",
      "Epoch 8390/30000 Validation Loss: 0.07889531552791595\n",
      "Epoch 8391/30000 Training Loss: 0.09362614899873734\n",
      "Epoch 8392/30000 Training Loss: 0.08856699615716934\n",
      "Epoch 8393/30000 Training Loss: 0.10870888829231262\n",
      "Epoch 8394/30000 Training Loss: 0.062015142291784286\n",
      "Epoch 8395/30000 Training Loss: 0.06872475147247314\n",
      "Epoch 8396/30000 Training Loss: 0.0874238982796669\n",
      "Epoch 8397/30000 Training Loss: 0.05443212017416954\n",
      "Epoch 8398/30000 Training Loss: 0.07429268956184387\n",
      "Epoch 8399/30000 Training Loss: 0.10233881324529648\n",
      "Epoch 8400/30000 Training Loss: 0.06604308634996414\n",
      "Epoch 8400/30000 Validation Loss: 0.07793762534856796\n",
      "Epoch 8401/30000 Training Loss: 0.08398102968931198\n",
      "Epoch 8402/30000 Training Loss: 0.08735501021146774\n",
      "Epoch 8403/30000 Training Loss: 0.05864802375435829\n",
      "Epoch 8404/30000 Training Loss: 0.08325660973787308\n",
      "Epoch 8405/30000 Training Loss: 0.08995763212442398\n",
      "Epoch 8406/30000 Training Loss: 0.07766930013895035\n",
      "Epoch 8407/30000 Training Loss: 0.08930686861276627\n",
      "Epoch 8408/30000 Training Loss: 0.07247456163167953\n",
      "Epoch 8409/30000 Training Loss: 0.07707720994949341\n",
      "Epoch 8410/30000 Training Loss: 0.08316098898649216\n",
      "Epoch 8410/30000 Validation Loss: 0.08410146087408066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8411/30000 Training Loss: 0.09858199954032898\n",
      "Epoch 8412/30000 Training Loss: 0.09153783321380615\n",
      "Epoch 8413/30000 Training Loss: 0.08842872828245163\n",
      "Epoch 8414/30000 Training Loss: 0.08436132222414017\n",
      "Epoch 8415/30000 Training Loss: 0.07508761435747147\n",
      "Epoch 8416/30000 Training Loss: 0.07880739122629166\n",
      "Epoch 8417/30000 Training Loss: 0.07670660316944122\n",
      "Epoch 8418/30000 Training Loss: 0.07876032590866089\n",
      "Epoch 8419/30000 Training Loss: 0.07890749722719193\n",
      "Epoch 8420/30000 Training Loss: 0.07260626554489136\n",
      "Epoch 8420/30000 Validation Loss: 0.08117008954286575\n",
      "Epoch 8421/30000 Training Loss: 0.07888653874397278\n",
      "Epoch 8422/30000 Training Loss: 0.062497470527887344\n",
      "Epoch 8423/30000 Training Loss: 0.06108212471008301\n",
      "Epoch 8424/30000 Training Loss: 0.0756421908736229\n",
      "Epoch 8425/30000 Training Loss: 0.0818033516407013\n",
      "Epoch 8426/30000 Training Loss: 0.10757043212652206\n",
      "Epoch 8427/30000 Training Loss: 0.07004538923501968\n",
      "Epoch 8428/30000 Training Loss: 0.06796903163194656\n",
      "Epoch 8429/30000 Training Loss: 0.10276784747838974\n",
      "Epoch 8430/30000 Training Loss: 0.08293681591749191\n",
      "Epoch 8430/30000 Validation Loss: 0.09111443907022476\n",
      "Epoch 8431/30000 Training Loss: 0.08031637221574783\n",
      "Epoch 8432/30000 Training Loss: 0.09116479009389877\n",
      "Epoch 8433/30000 Training Loss: 0.06616049259901047\n",
      "Epoch 8434/30000 Training Loss: 0.08044802397489548\n",
      "Epoch 8435/30000 Training Loss: 0.08911751955747604\n",
      "Epoch 8436/30000 Training Loss: 0.0787622332572937\n",
      "Epoch 8437/30000 Training Loss: 0.08282863348722458\n",
      "Epoch 8438/30000 Training Loss: 0.09525743871927261\n",
      "Epoch 8439/30000 Training Loss: 0.07437481731176376\n",
      "Epoch 8440/30000 Training Loss: 0.0998389720916748\n",
      "Epoch 8440/30000 Validation Loss: 0.10204631835222244\n",
      "Epoch 8441/30000 Training Loss: 0.1017986610531807\n",
      "Epoch 8442/30000 Training Loss: 0.07693103700876236\n",
      "Epoch 8443/30000 Training Loss: 0.0765303298830986\n",
      "Epoch 8444/30000 Training Loss: 0.08859220147132874\n",
      "Epoch 8445/30000 Training Loss: 0.10801386088132858\n",
      "Epoch 8446/30000 Training Loss: 0.07476142048835754\n",
      "Epoch 8447/30000 Training Loss: 0.0930103063583374\n",
      "Epoch 8448/30000 Training Loss: 0.09867066890001297\n",
      "Epoch 8449/30000 Training Loss: 0.08440399169921875\n",
      "Epoch 8450/30000 Training Loss: 0.06725277751684189\n",
      "Epoch 8450/30000 Validation Loss: 0.08985457569360733\n",
      "Epoch 8451/30000 Training Loss: 0.06736412644386292\n",
      "Epoch 8452/30000 Training Loss: 0.08292616158723831\n",
      "Epoch 8453/30000 Training Loss: 0.09463059902191162\n",
      "Epoch 8454/30000 Training Loss: 0.0881229117512703\n",
      "Epoch 8455/30000 Training Loss: 0.09170186519622803\n",
      "Epoch 8456/30000 Training Loss: 0.0867663025856018\n",
      "Epoch 8457/30000 Training Loss: 0.09019216150045395\n",
      "Epoch 8458/30000 Training Loss: 0.08085223287343979\n",
      "Epoch 8459/30000 Training Loss: 0.0637347623705864\n",
      "Epoch 8460/30000 Training Loss: 0.06949109584093094\n",
      "Epoch 8460/30000 Validation Loss: 0.08087965846061707\n",
      "Epoch 8461/30000 Training Loss: 0.08135738223791122\n",
      "Epoch 8462/30000 Training Loss: 0.07663039118051529\n",
      "Epoch 8463/30000 Training Loss: 0.06681755185127258\n",
      "Epoch 8464/30000 Training Loss: 0.07466874271631241\n",
      "Epoch 8465/30000 Training Loss: 0.07932353764772415\n",
      "Epoch 8466/30000 Training Loss: 0.07369032502174377\n",
      "Epoch 8467/30000 Training Loss: 0.07023601233959198\n",
      "Epoch 8468/30000 Training Loss: 0.10485464334487915\n",
      "Epoch 8469/30000 Training Loss: 0.08246030658483505\n",
      "Epoch 8470/30000 Training Loss: 0.09887098520994186\n",
      "Epoch 8470/30000 Validation Loss: 0.09022561460733414\n",
      "Epoch 8471/30000 Training Loss: 0.09214705228805542\n",
      "Epoch 8472/30000 Training Loss: 0.08336609601974487\n",
      "Epoch 8473/30000 Training Loss: 0.07373582571744919\n",
      "Epoch 8474/30000 Training Loss: 0.09406956285238266\n",
      "Epoch 8475/30000 Training Loss: 0.06810948997735977\n",
      "Epoch 8476/30000 Training Loss: 0.07778465002775192\n",
      "Epoch 8477/30000 Training Loss: 0.08000115305185318\n",
      "Epoch 8478/30000 Training Loss: 0.07158488035202026\n",
      "Epoch 8479/30000 Training Loss: 0.08885066956281662\n",
      "Epoch 8480/30000 Training Loss: 0.112110435962677\n",
      "Epoch 8480/30000 Validation Loss: 0.06936105340719223\n",
      "Epoch 8481/30000 Training Loss: 0.08310705423355103\n",
      "Epoch 8482/30000 Training Loss: 0.07691269367933273\n",
      "Epoch 8483/30000 Training Loss: 0.06351076066493988\n",
      "Epoch 8484/30000 Training Loss: 0.06609507650136948\n",
      "Epoch 8485/30000 Training Loss: 0.09815654158592224\n",
      "Epoch 8486/30000 Training Loss: 0.08144873380661011\n",
      "Epoch 8487/30000 Training Loss: 0.09549393504858017\n",
      "Epoch 8488/30000 Training Loss: 0.06752186268568039\n",
      "Epoch 8489/30000 Training Loss: 0.07389216870069504\n",
      "Epoch 8490/30000 Training Loss: 0.08849915117025375\n",
      "Epoch 8490/30000 Validation Loss: 0.08497872948646545\n",
      "Epoch 8491/30000 Training Loss: 0.09382740408182144\n",
      "Epoch 8492/30000 Training Loss: 0.06780391186475754\n",
      "Epoch 8493/30000 Training Loss: 0.08109381794929504\n",
      "Epoch 8494/30000 Training Loss: 0.06894806027412415\n",
      "Epoch 8495/30000 Training Loss: 0.08138676732778549\n",
      "Epoch 8496/30000 Training Loss: 0.08276975899934769\n",
      "Epoch 8497/30000 Training Loss: 0.06738834828138351\n",
      "Epoch 8498/30000 Training Loss: 0.08700961619615555\n",
      "Epoch 8499/30000 Training Loss: 0.10206270217895508\n",
      "Epoch 8500/30000 Training Loss: 0.09280677884817123\n",
      "Epoch 8500/30000 Validation Loss: 0.10622402280569077\n",
      "Epoch 8501/30000 Training Loss: 0.0716758593916893\n",
      "Epoch 8502/30000 Training Loss: 0.06808197498321533\n",
      "Epoch 8503/30000 Training Loss: 0.09524478763341904\n",
      "Epoch 8504/30000 Training Loss: 0.08344883471727371\n",
      "Epoch 8505/30000 Training Loss: 0.08908296376466751\n",
      "Epoch 8506/30000 Training Loss: 0.07780221849679947\n",
      "Epoch 8507/30000 Training Loss: 0.0709049329161644\n",
      "Epoch 8508/30000 Training Loss: 0.10389108210802078\n",
      "Epoch 8509/30000 Training Loss: 0.08619743585586548\n",
      "Epoch 8510/30000 Training Loss: 0.07762571424245834\n",
      "Epoch 8510/30000 Validation Loss: 0.07434067875146866\n",
      "Epoch 8511/30000 Training Loss: 0.09918105602264404\n",
      "Epoch 8512/30000 Training Loss: 0.06399872153997421\n",
      "Epoch 8513/30000 Training Loss: 0.06259392946958542\n",
      "Epoch 8514/30000 Training Loss: 0.07677113264799118\n",
      "Epoch 8515/30000 Training Loss: 0.08478712290525436\n",
      "Epoch 8516/30000 Training Loss: 0.0740002691745758\n",
      "Epoch 8517/30000 Training Loss: 0.07859335094690323\n",
      "Epoch 8518/30000 Training Loss: 0.08397947996854782\n",
      "Epoch 8519/30000 Training Loss: 0.08756721019744873\n",
      "Epoch 8520/30000 Training Loss: 0.07570747286081314\n",
      "Epoch 8520/30000 Validation Loss: 0.07410026341676712\n",
      "Epoch 8521/30000 Training Loss: 0.08758338540792465\n",
      "Epoch 8522/30000 Training Loss: 0.13174909353256226\n",
      "Epoch 8523/30000 Training Loss: 0.0973193570971489\n",
      "Epoch 8524/30000 Training Loss: 0.08534806966781616\n",
      "Epoch 8525/30000 Training Loss: 0.10343306511640549\n",
      "Epoch 8526/30000 Training Loss: 0.07125356793403625\n",
      "Epoch 8527/30000 Training Loss: 0.06945066899061203\n",
      "Epoch 8528/30000 Training Loss: 0.10095416754484177\n",
      "Epoch 8529/30000 Training Loss: 0.09628978371620178\n",
      "Epoch 8530/30000 Training Loss: 0.08782238513231277\n",
      "Epoch 8530/30000 Validation Loss: 0.08739439398050308\n",
      "Epoch 8531/30000 Training Loss: 0.0930999219417572\n",
      "Epoch 8532/30000 Training Loss: 0.08039971441030502\n",
      "Epoch 8533/30000 Training Loss: 0.08387044072151184\n",
      "Epoch 8534/30000 Training Loss: 0.08366041630506516\n",
      "Epoch 8535/30000 Training Loss: 0.05915950611233711\n",
      "Epoch 8536/30000 Training Loss: 0.09241286665201187\n",
      "Epoch 8537/30000 Training Loss: 0.08666197210550308\n",
      "Epoch 8538/30000 Training Loss: 0.08003879338502884\n",
      "Epoch 8539/30000 Training Loss: 0.06054211035370827\n",
      "Epoch 8540/30000 Training Loss: 0.0840919017791748\n",
      "Epoch 8540/30000 Validation Loss: 0.08541028946638107\n",
      "Epoch 8541/30000 Training Loss: 0.07110954076051712\n",
      "Epoch 8542/30000 Training Loss: 0.07504577189683914\n",
      "Epoch 8543/30000 Training Loss: 0.07800101488828659\n",
      "Epoch 8544/30000 Training Loss: 0.08328583091497421\n",
      "Epoch 8545/30000 Training Loss: 0.07449693232774734\n",
      "Epoch 8546/30000 Training Loss: 0.08104584366083145\n",
      "Epoch 8547/30000 Training Loss: 0.09184185415506363\n",
      "Epoch 8548/30000 Training Loss: 0.08243989199399948\n",
      "Epoch 8549/30000 Training Loss: 0.07108742743730545\n",
      "Epoch 8550/30000 Training Loss: 0.07840190082788467\n",
      "Epoch 8550/30000 Validation Loss: 0.07405131310224533\n",
      "Epoch 8551/30000 Training Loss: 0.0657993033528328\n",
      "Epoch 8552/30000 Training Loss: 0.08128196746110916\n",
      "Epoch 8553/30000 Training Loss: 0.08962158113718033\n",
      "Epoch 8554/30000 Training Loss: 0.07741159945726395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8555/30000 Training Loss: 0.09022236615419388\n",
      "Epoch 8556/30000 Training Loss: 0.0812915563583374\n",
      "Epoch 8557/30000 Training Loss: 0.07123813778162003\n",
      "Epoch 8558/30000 Training Loss: 0.08796381205320358\n",
      "Epoch 8559/30000 Training Loss: 0.07781344652175903\n",
      "Epoch 8560/30000 Training Loss: 0.09351805597543716\n",
      "Epoch 8560/30000 Validation Loss: 0.09240072965621948\n",
      "Epoch 8561/30000 Training Loss: 0.09256080538034439\n",
      "Epoch 8562/30000 Training Loss: 0.0772218331694603\n",
      "Epoch 8563/30000 Training Loss: 0.06784293800592422\n",
      "Epoch 8564/30000 Training Loss: 0.08531922101974487\n",
      "Epoch 8565/30000 Training Loss: 0.08532137423753738\n",
      "Epoch 8566/30000 Training Loss: 0.07934844493865967\n",
      "Epoch 8567/30000 Training Loss: 0.08572811633348465\n",
      "Epoch 8568/30000 Training Loss: 0.07960418611764908\n",
      "Epoch 8569/30000 Training Loss: 0.0857362225651741\n",
      "Epoch 8570/30000 Training Loss: 0.0703846886754036\n",
      "Epoch 8570/30000 Validation Loss: 0.08125940710306168\n",
      "Epoch 8571/30000 Training Loss: 0.07377010583877563\n",
      "Epoch 8572/30000 Training Loss: 0.07776006311178207\n",
      "Epoch 8573/30000 Training Loss: 0.08050451427698135\n",
      "Epoch 8574/30000 Training Loss: 0.06137596443295479\n",
      "Epoch 8575/30000 Training Loss: 0.07422468066215515\n",
      "Epoch 8576/30000 Training Loss: 0.09879573434591293\n",
      "Epoch 8577/30000 Training Loss: 0.10882511734962463\n",
      "Epoch 8578/30000 Training Loss: 0.06820577383041382\n",
      "Epoch 8579/30000 Training Loss: 0.07206282764673233\n",
      "Epoch 8580/30000 Training Loss: 0.10702905803918839\n",
      "Epoch 8580/30000 Validation Loss: 0.08924181014299393\n",
      "Epoch 8581/30000 Training Loss: 0.09223568439483643\n",
      "Epoch 8582/30000 Training Loss: 0.07749786972999573\n",
      "Epoch 8583/30000 Training Loss: 0.0912354588508606\n",
      "Epoch 8584/30000 Training Loss: 0.07702308893203735\n",
      "Epoch 8585/30000 Training Loss: 0.08946140855550766\n",
      "Epoch 8586/30000 Training Loss: 0.08956614136695862\n",
      "Epoch 8587/30000 Training Loss: 0.09159714728593826\n",
      "Epoch 8588/30000 Training Loss: 0.09410097450017929\n",
      "Epoch 8589/30000 Training Loss: 0.09183166176080704\n",
      "Epoch 8590/30000 Training Loss: 0.10112738609313965\n",
      "Epoch 8590/30000 Validation Loss: 0.08196056634187698\n",
      "Epoch 8591/30000 Training Loss: 0.07103092223405838\n",
      "Epoch 8592/30000 Training Loss: 0.0942070260643959\n",
      "Epoch 8593/30000 Training Loss: 0.08611787110567093\n",
      "Epoch 8594/30000 Training Loss: 0.08513232320547104\n",
      "Epoch 8595/30000 Training Loss: 0.08041077107191086\n",
      "Epoch 8596/30000 Training Loss: 0.06783299148082733\n",
      "Epoch 8597/30000 Training Loss: 0.08450474590063095\n",
      "Epoch 8598/30000 Training Loss: 0.07699476927518845\n",
      "Epoch 8599/30000 Training Loss: 0.07977244257926941\n",
      "Epoch 8600/30000 Training Loss: 0.08136019110679626\n",
      "Epoch 8600/30000 Validation Loss: 0.06986533105373383\n",
      "Epoch 8601/30000 Training Loss: 0.09342177957296371\n",
      "Epoch 8602/30000 Training Loss: 0.090853750705719\n",
      "Epoch 8603/30000 Training Loss: 0.07259824872016907\n",
      "Epoch 8604/30000 Training Loss: 0.08916440606117249\n",
      "Epoch 8605/30000 Training Loss: 0.06229336932301521\n",
      "Epoch 8606/30000 Training Loss: 0.06837331503629684\n",
      "Epoch 8607/30000 Training Loss: 0.10057886689901352\n",
      "Epoch 8608/30000 Training Loss: 0.09838604182004929\n",
      "Epoch 8609/30000 Training Loss: 0.07129532098770142\n",
      "Epoch 8610/30000 Training Loss: 0.07881724089384079\n",
      "Epoch 8610/30000 Validation Loss: 0.08190479874610901\n",
      "Epoch 8611/30000 Training Loss: 0.08001581579446793\n",
      "Epoch 8612/30000 Training Loss: 0.07333063334226608\n",
      "Epoch 8613/30000 Training Loss: 0.07178734987974167\n",
      "Epoch 8614/30000 Training Loss: 0.08192089945077896\n",
      "Epoch 8615/30000 Training Loss: 0.08909542113542557\n",
      "Epoch 8616/30000 Training Loss: 0.08298663049936295\n",
      "Epoch 8617/30000 Training Loss: 0.09612876921892166\n",
      "Epoch 8618/30000 Training Loss: 0.07904139161109924\n",
      "Epoch 8619/30000 Training Loss: 0.09101606160402298\n",
      "Epoch 8620/30000 Training Loss: 0.13298286497592926\n",
      "Epoch 8620/30000 Validation Loss: 0.0660734474658966\n",
      "Epoch 8621/30000 Training Loss: 0.07878010720014572\n",
      "Epoch 8622/30000 Training Loss: 0.07406318932771683\n",
      "Epoch 8623/30000 Training Loss: 0.09211689233779907\n",
      "Epoch 8624/30000 Training Loss: 0.0917341336607933\n",
      "Epoch 8625/30000 Training Loss: 0.08468783646821976\n",
      "Epoch 8626/30000 Training Loss: 0.0746111273765564\n",
      "Epoch 8627/30000 Training Loss: 0.0985470712184906\n",
      "Epoch 8628/30000 Training Loss: 0.10350141674280167\n",
      "Epoch 8629/30000 Training Loss: 0.06355511397123337\n",
      "Epoch 8630/30000 Training Loss: 0.08051209896802902\n",
      "Epoch 8630/30000 Validation Loss: 0.08178149908781052\n",
      "Epoch 8631/30000 Training Loss: 0.1062399223446846\n",
      "Epoch 8632/30000 Training Loss: 0.08081801980733871\n",
      "Epoch 8633/30000 Training Loss: 0.1284995675086975\n",
      "Epoch 8634/30000 Training Loss: 0.07222554832696915\n",
      "Epoch 8635/30000 Training Loss: 0.08550349622964859\n",
      "Epoch 8636/30000 Training Loss: 0.07601908594369888\n",
      "Epoch 8637/30000 Training Loss: 0.07807326316833496\n",
      "Epoch 8638/30000 Training Loss: 0.10457441955804825\n",
      "Epoch 8639/30000 Training Loss: 0.07796449214220047\n",
      "Epoch 8640/30000 Training Loss: 0.08822112530469894\n",
      "Epoch 8640/30000 Validation Loss: 0.101942278444767\n",
      "Epoch 8641/30000 Training Loss: 0.086365707218647\n",
      "Epoch 8642/30000 Training Loss: 0.08233768492937088\n",
      "Epoch 8643/30000 Training Loss: 0.07023131102323532\n",
      "Epoch 8644/30000 Training Loss: 0.08159051835536957\n",
      "Epoch 8645/30000 Training Loss: 0.08804076910018921\n",
      "Epoch 8646/30000 Training Loss: 0.09144478291273117\n",
      "Epoch 8647/30000 Training Loss: 0.10464256256818771\n",
      "Epoch 8648/30000 Training Loss: 0.09408579021692276\n",
      "Epoch 8649/30000 Training Loss: 0.07695925980806351\n",
      "Epoch 8650/30000 Training Loss: 0.09352653473615646\n",
      "Epoch 8650/30000 Validation Loss: 0.0943308100104332\n",
      "Epoch 8651/30000 Training Loss: 0.10789112001657486\n",
      "Epoch 8652/30000 Training Loss: 0.08250820636749268\n",
      "Epoch 8653/30000 Training Loss: 0.08351198583841324\n",
      "Epoch 8654/30000 Training Loss: 0.08668776601552963\n",
      "Epoch 8655/30000 Training Loss: 0.09920994192361832\n",
      "Epoch 8656/30000 Training Loss: 0.07775579392910004\n",
      "Epoch 8657/30000 Training Loss: 0.09283957630395889\n",
      "Epoch 8658/30000 Training Loss: 0.08579882234334946\n",
      "Epoch 8659/30000 Training Loss: 0.08169708400964737\n",
      "Epoch 8660/30000 Training Loss: 0.10474612563848495\n",
      "Epoch 8660/30000 Validation Loss: 0.07034409791231155\n",
      "Epoch 8661/30000 Training Loss: 0.08199582248926163\n",
      "Epoch 8662/30000 Training Loss: 0.0758439227938652\n",
      "Epoch 8663/30000 Training Loss: 0.10066785663366318\n",
      "Epoch 8664/30000 Training Loss: 0.10004173964262009\n",
      "Epoch 8665/30000 Training Loss: 0.07506450265645981\n",
      "Epoch 8666/30000 Training Loss: 0.07725424319505692\n",
      "Epoch 8667/30000 Training Loss: 0.08740535378456116\n",
      "Epoch 8668/30000 Training Loss: 0.07743886858224869\n",
      "Epoch 8669/30000 Training Loss: 0.07697790861129761\n",
      "Epoch 8670/30000 Training Loss: 0.09310692548751831\n",
      "Epoch 8670/30000 Validation Loss: 0.06899108737707138\n",
      "Epoch 8671/30000 Training Loss: 0.0971858873963356\n",
      "Epoch 8672/30000 Training Loss: 0.07825201004743576\n",
      "Epoch 8673/30000 Training Loss: 0.08555581420660019\n",
      "Epoch 8674/30000 Training Loss: 0.06639353185892105\n",
      "Epoch 8675/30000 Training Loss: 0.08292511850595474\n",
      "Epoch 8676/30000 Training Loss: 0.06650664657354355\n",
      "Epoch 8677/30000 Training Loss: 0.08511754870414734\n",
      "Epoch 8678/30000 Training Loss: 0.09514983743429184\n",
      "Epoch 8679/30000 Training Loss: 0.04433693736791611\n",
      "Epoch 8680/30000 Training Loss: 0.06234702095389366\n",
      "Epoch 8680/30000 Validation Loss: 0.08135733753442764\n",
      "Epoch 8681/30000 Training Loss: 0.07386074215173721\n",
      "Epoch 8682/30000 Training Loss: 0.08936378359794617\n",
      "Epoch 8683/30000 Training Loss: 0.09825482964515686\n",
      "Epoch 8684/30000 Training Loss: 0.09313309192657471\n",
      "Epoch 8685/30000 Training Loss: 0.08721986413002014\n",
      "Epoch 8686/30000 Training Loss: 0.08881437033414841\n",
      "Epoch 8687/30000 Training Loss: 0.08833097666501999\n",
      "Epoch 8688/30000 Training Loss: 0.08305659145116806\n",
      "Epoch 8689/30000 Training Loss: 0.08858802169561386\n",
      "Epoch 8690/30000 Training Loss: 0.0775790587067604\n",
      "Epoch 8690/30000 Validation Loss: 0.08746615797281265\n",
      "Epoch 8691/30000 Training Loss: 0.08109845966100693\n",
      "Epoch 8692/30000 Training Loss: 0.08044035732746124\n",
      "Epoch 8693/30000 Training Loss: 0.06748976558446884\n",
      "Epoch 8694/30000 Training Loss: 0.07764261215925217\n",
      "Epoch 8695/30000 Training Loss: 0.0875592827796936\n",
      "Epoch 8696/30000 Training Loss: 0.08283828943967819\n",
      "Epoch 8697/30000 Training Loss: 0.07893731445074081\n",
      "Epoch 8698/30000 Training Loss: 0.07740937918424606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8699/30000 Training Loss: 0.0813070759177208\n",
      "Epoch 8700/30000 Training Loss: 0.07894007861614227\n",
      "Epoch 8700/30000 Validation Loss: 0.06011622026562691\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06011622026562691<=============\n",
      "Epoch 8701/30000 Training Loss: 0.08733672648668289\n",
      "Epoch 8702/30000 Training Loss: 0.08000746369361877\n",
      "Epoch 8703/30000 Training Loss: 0.08872618526220322\n",
      "Epoch 8704/30000 Training Loss: 0.078095942735672\n",
      "Epoch 8705/30000 Training Loss: 0.08147405833005905\n",
      "Epoch 8706/30000 Training Loss: 0.08238997310400009\n",
      "Epoch 8707/30000 Training Loss: 0.07401632517576218\n",
      "Epoch 8708/30000 Training Loss: 0.06522388011217117\n",
      "Epoch 8709/30000 Training Loss: 0.07571021467447281\n",
      "Epoch 8710/30000 Training Loss: 0.06721428781747818\n",
      "Epoch 8710/30000 Validation Loss: 0.07763472944498062\n",
      "Epoch 8711/30000 Training Loss: 0.09324581176042557\n",
      "Epoch 8712/30000 Training Loss: 0.08253490179777145\n",
      "Epoch 8713/30000 Training Loss: 0.06250131875276566\n",
      "Epoch 8714/30000 Training Loss: 0.08653926849365234\n",
      "Epoch 8715/30000 Training Loss: 0.07701549679040909\n",
      "Epoch 8716/30000 Training Loss: 0.07792467623949051\n",
      "Epoch 8717/30000 Training Loss: 0.057393889874219894\n",
      "Epoch 8718/30000 Training Loss: 0.06992412358522415\n",
      "Epoch 8719/30000 Training Loss: 0.0918228030204773\n",
      "Epoch 8720/30000 Training Loss: 0.10742184519767761\n",
      "Epoch 8720/30000 Validation Loss: 0.0872301384806633\n",
      "Epoch 8721/30000 Training Loss: 0.0918576791882515\n",
      "Epoch 8722/30000 Training Loss: 0.0751163586974144\n",
      "Epoch 8723/30000 Training Loss: 0.0793594941496849\n",
      "Epoch 8724/30000 Training Loss: 0.09002399444580078\n",
      "Epoch 8725/30000 Training Loss: 0.08037904649972916\n",
      "Epoch 8726/30000 Training Loss: 0.08768609911203384\n",
      "Epoch 8727/30000 Training Loss: 0.08020228147506714\n",
      "Epoch 8728/30000 Training Loss: 0.07758279889822006\n",
      "Epoch 8729/30000 Training Loss: 0.08308637142181396\n",
      "Epoch 8730/30000 Training Loss: 0.08901319652795792\n",
      "Epoch 8730/30000 Validation Loss: 0.08732973784208298\n",
      "Epoch 8731/30000 Training Loss: 0.09861885756254196\n",
      "Epoch 8732/30000 Training Loss: 0.07598833739757538\n",
      "Epoch 8733/30000 Training Loss: 0.06705763190984726\n",
      "Epoch 8734/30000 Training Loss: 0.0984591543674469\n",
      "Epoch 8735/30000 Training Loss: 0.08401589840650558\n",
      "Epoch 8736/30000 Training Loss: 0.09662047773599625\n",
      "Epoch 8737/30000 Training Loss: 0.07895549386739731\n",
      "Epoch 8738/30000 Training Loss: 0.08376365154981613\n",
      "Epoch 8739/30000 Training Loss: 0.06244192644953728\n",
      "Epoch 8740/30000 Training Loss: 0.08849609643220901\n",
      "Epoch 8740/30000 Validation Loss: 0.10102254897356033\n",
      "Epoch 8741/30000 Training Loss: 0.09878844022750854\n",
      "Epoch 8742/30000 Training Loss: 0.09296973794698715\n",
      "Epoch 8743/30000 Training Loss: 0.06034001708030701\n",
      "Epoch 8744/30000 Training Loss: 0.08761313557624817\n",
      "Epoch 8745/30000 Training Loss: 0.09478756040334702\n",
      "Epoch 8746/30000 Training Loss: 0.07175371050834656\n",
      "Epoch 8747/30000 Training Loss: 0.08909031748771667\n",
      "Epoch 8748/30000 Training Loss: 0.10122338682413101\n",
      "Epoch 8749/30000 Training Loss: 0.07633163779973984\n",
      "Epoch 8750/30000 Training Loss: 0.09632748365402222\n",
      "Epoch 8750/30000 Validation Loss: 0.08355563879013062\n",
      "Epoch 8751/30000 Training Loss: 0.09679385274648666\n",
      "Epoch 8752/30000 Training Loss: 0.08546961098909378\n",
      "Epoch 8753/30000 Training Loss: 0.06891954690217972\n",
      "Epoch 8754/30000 Training Loss: 0.11138410121202469\n",
      "Epoch 8755/30000 Training Loss: 0.08135779201984406\n",
      "Epoch 8756/30000 Training Loss: 0.08949723094701767\n",
      "Epoch 8757/30000 Training Loss: 0.08210957795381546\n",
      "Epoch 8758/30000 Training Loss: 0.09041621536016464\n",
      "Epoch 8759/30000 Training Loss: 0.0811089500784874\n",
      "Epoch 8760/30000 Training Loss: 0.09575255960226059\n",
      "Epoch 8760/30000 Validation Loss: 0.0663919523358345\n",
      "Epoch 8761/30000 Training Loss: 0.08999801427125931\n",
      "Epoch 8762/30000 Training Loss: 0.09007123857736588\n",
      "Epoch 8763/30000 Training Loss: 0.069693423807621\n",
      "Epoch 8764/30000 Training Loss: 0.05932291969656944\n",
      "Epoch 8765/30000 Training Loss: 0.06954724341630936\n",
      "Epoch 8766/30000 Training Loss: 0.06331458687782288\n",
      "Epoch 8767/30000 Training Loss: 0.07022659480571747\n",
      "Epoch 8768/30000 Training Loss: 0.08283623307943344\n",
      "Epoch 8769/30000 Training Loss: 0.07886415719985962\n",
      "Epoch 8770/30000 Training Loss: 0.0822766125202179\n",
      "Epoch 8770/30000 Validation Loss: 0.0763382837176323\n",
      "Epoch 8771/30000 Training Loss: 0.07830172777175903\n",
      "Epoch 8772/30000 Training Loss: 0.07998671382665634\n",
      "Epoch 8773/30000 Training Loss: 0.0861111506819725\n",
      "Epoch 8774/30000 Training Loss: 0.0885869637131691\n",
      "Epoch 8775/30000 Training Loss: 0.07763352990150452\n",
      "Epoch 8776/30000 Training Loss: 0.07591325789690018\n",
      "Epoch 8777/30000 Training Loss: 0.11389527469873428\n",
      "Epoch 8778/30000 Training Loss: 0.12759120762348175\n",
      "Epoch 8779/30000 Training Loss: 0.06407881528139114\n",
      "Epoch 8780/30000 Training Loss: 0.08423861116170883\n",
      "Epoch 8780/30000 Validation Loss: 0.06530040502548218\n",
      "Epoch 8781/30000 Training Loss: 0.08349169045686722\n",
      "Epoch 8782/30000 Training Loss: 0.08630523085594177\n",
      "Epoch 8783/30000 Training Loss: 0.07863738387823105\n",
      "Epoch 8784/30000 Training Loss: 0.08209764957427979\n",
      "Epoch 8785/30000 Training Loss: 0.10201796889305115\n",
      "Epoch 8786/30000 Training Loss: 0.08586087077856064\n",
      "Epoch 8787/30000 Training Loss: 0.0941406860947609\n",
      "Epoch 8788/30000 Training Loss: 0.09802227467298508\n",
      "Epoch 8789/30000 Training Loss: 0.0574411042034626\n",
      "Epoch 8790/30000 Training Loss: 0.06901972740888596\n",
      "Epoch 8790/30000 Validation Loss: 0.0823846235871315\n",
      "Epoch 8791/30000 Training Loss: 0.08490529656410217\n",
      "Epoch 8792/30000 Training Loss: 0.0949678048491478\n",
      "Epoch 8793/30000 Training Loss: 0.06562765687704086\n",
      "Epoch 8794/30000 Training Loss: 0.07452962547540665\n",
      "Epoch 8795/30000 Training Loss: 0.08246728032827377\n",
      "Epoch 8796/30000 Training Loss: 0.08253151178359985\n",
      "Epoch 8797/30000 Training Loss: 0.08078895509243011\n",
      "Epoch 8798/30000 Training Loss: 0.10992341488599777\n",
      "Epoch 8799/30000 Training Loss: 0.07925686985254288\n",
      "Epoch 8800/30000 Training Loss: 0.08358880132436752\n",
      "Epoch 8800/30000 Validation Loss: 0.09544640779495239\n",
      "Epoch 8801/30000 Training Loss: 0.07761380076408386\n",
      "Epoch 8802/30000 Training Loss: 0.08566480875015259\n",
      "Epoch 8803/30000 Training Loss: 0.09583544731140137\n",
      "Epoch 8804/30000 Training Loss: 0.06432738155126572\n",
      "Epoch 8805/30000 Training Loss: 0.08500584214925766\n",
      "Epoch 8806/30000 Training Loss: 0.06522393226623535\n",
      "Epoch 8807/30000 Training Loss: 0.08568626642227173\n",
      "Epoch 8808/30000 Training Loss: 0.08808692544698715\n",
      "Epoch 8809/30000 Training Loss: 0.07626376301050186\n",
      "Epoch 8810/30000 Training Loss: 0.0813337191939354\n",
      "Epoch 8810/30000 Validation Loss: 0.08036977797746658\n",
      "Epoch 8811/30000 Training Loss: 0.09613772481679916\n",
      "Epoch 8812/30000 Training Loss: 0.07076806575059891\n",
      "Epoch 8813/30000 Training Loss: 0.09104781597852707\n",
      "Epoch 8814/30000 Training Loss: 0.09260091185569763\n",
      "Epoch 8815/30000 Training Loss: 0.06632915884256363\n",
      "Epoch 8816/30000 Training Loss: 0.0848415195941925\n",
      "Epoch 8817/30000 Training Loss: 0.09903571754693985\n",
      "Epoch 8818/30000 Training Loss: 0.10117880254983902\n",
      "Epoch 8819/30000 Training Loss: 0.09006885439157486\n",
      "Epoch 8820/30000 Training Loss: 0.10168787837028503\n",
      "Epoch 8820/30000 Validation Loss: 0.0754028856754303\n",
      "Epoch 8821/30000 Training Loss: 0.10133480280637741\n",
      "Epoch 8822/30000 Training Loss: 0.10951795428991318\n",
      "Epoch 8823/30000 Training Loss: 0.09807389974594116\n",
      "Epoch 8824/30000 Training Loss: 0.07689911872148514\n",
      "Epoch 8825/30000 Training Loss: 0.08786004781723022\n",
      "Epoch 8826/30000 Training Loss: 0.07419678568840027\n",
      "Epoch 8827/30000 Training Loss: 0.09828170388936996\n",
      "Epoch 8828/30000 Training Loss: 0.061750903725624084\n",
      "Epoch 8829/30000 Training Loss: 0.08407028764486313\n",
      "Epoch 8830/30000 Training Loss: 0.08529376238584518\n",
      "Epoch 8830/30000 Validation Loss: 0.08334522694349289\n",
      "Epoch 8831/30000 Training Loss: 0.06788121163845062\n",
      "Epoch 8832/30000 Training Loss: 0.08991855382919312\n",
      "Epoch 8833/30000 Training Loss: 0.07136475294828415\n",
      "Epoch 8834/30000 Training Loss: 0.0857987180352211\n",
      "Epoch 8835/30000 Training Loss: 0.08618529886007309\n",
      "Epoch 8836/30000 Training Loss: 0.07806510478258133\n",
      "Epoch 8837/30000 Training Loss: 0.09819129854440689\n",
      "Epoch 8838/30000 Training Loss: 0.09546799212694168\n",
      "Epoch 8839/30000 Training Loss: 0.08222067356109619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8840/30000 Training Loss: 0.10314371436834335\n",
      "Epoch 8840/30000 Validation Loss: 0.09850553423166275\n",
      "Epoch 8841/30000 Training Loss: 0.06717235594987869\n",
      "Epoch 8842/30000 Training Loss: 0.08060476183891296\n",
      "Epoch 8843/30000 Training Loss: 0.06800127774477005\n",
      "Epoch 8844/30000 Training Loss: 0.08144847303628922\n",
      "Epoch 8845/30000 Training Loss: 0.08297120034694672\n",
      "Epoch 8846/30000 Training Loss: 0.06575445830821991\n",
      "Epoch 8847/30000 Training Loss: 0.07419001311063766\n",
      "Epoch 8848/30000 Training Loss: 0.0695711076259613\n",
      "Epoch 8849/30000 Training Loss: 0.09579616039991379\n",
      "Epoch 8850/30000 Training Loss: 0.07949892431497574\n",
      "Epoch 8850/30000 Validation Loss: 0.08231014758348465\n",
      "Epoch 8851/30000 Training Loss: 0.08267101645469666\n",
      "Epoch 8852/30000 Training Loss: 0.0899864062666893\n",
      "Epoch 8853/30000 Training Loss: 0.0929989144206047\n",
      "Epoch 8854/30000 Training Loss: 0.1051236167550087\n",
      "Epoch 8855/30000 Training Loss: 0.08631103485822678\n",
      "Epoch 8856/30000 Training Loss: 0.08490421622991562\n",
      "Epoch 8857/30000 Training Loss: 0.08559799194335938\n",
      "Epoch 8858/30000 Training Loss: 0.07866284996271133\n",
      "Epoch 8859/30000 Training Loss: 0.0895596370100975\n",
      "Epoch 8860/30000 Training Loss: 0.0914713516831398\n",
      "Epoch 8860/30000 Validation Loss: 0.08895901590585709\n",
      "Epoch 8861/30000 Training Loss: 0.07189399003982544\n",
      "Epoch 8862/30000 Training Loss: 0.08655995875597\n",
      "Epoch 8863/30000 Training Loss: 0.08501078933477402\n",
      "Epoch 8864/30000 Training Loss: 0.08968186378479004\n",
      "Epoch 8865/30000 Training Loss: 0.09498637914657593\n",
      "Epoch 8866/30000 Training Loss: 0.09900256246328354\n",
      "Epoch 8867/30000 Training Loss: 0.07535911351442337\n",
      "Epoch 8868/30000 Training Loss: 0.09471520036458969\n",
      "Epoch 8869/30000 Training Loss: 0.07152154296636581\n",
      "Epoch 8870/30000 Training Loss: 0.06744331866502762\n",
      "Epoch 8870/30000 Validation Loss: 0.1034102514386177\n",
      "Epoch 8871/30000 Training Loss: 0.09761008620262146\n",
      "Epoch 8872/30000 Training Loss: 0.07778657227754593\n",
      "Epoch 8873/30000 Training Loss: 0.07007571309804916\n",
      "Epoch 8874/30000 Training Loss: 0.08070710301399231\n",
      "Epoch 8875/30000 Training Loss: 0.08403488248586655\n",
      "Epoch 8876/30000 Training Loss: 0.06340710073709488\n",
      "Epoch 8877/30000 Training Loss: 0.08613329380750656\n",
      "Epoch 8878/30000 Training Loss: 0.07625504583120346\n",
      "Epoch 8879/30000 Training Loss: 0.08526748418807983\n",
      "Epoch 8880/30000 Training Loss: 0.08709583431482315\n",
      "Epoch 8880/30000 Validation Loss: 0.09337256103754044\n",
      "Epoch 8881/30000 Training Loss: 0.07523804903030396\n",
      "Epoch 8882/30000 Training Loss: 0.11224380135536194\n",
      "Epoch 8883/30000 Training Loss: 0.06757089495658875\n",
      "Epoch 8884/30000 Training Loss: 0.10226121544837952\n",
      "Epoch 8885/30000 Training Loss: 0.06355214864015579\n",
      "Epoch 8886/30000 Training Loss: 0.08247843384742737\n",
      "Epoch 8887/30000 Training Loss: 0.09891872853040695\n",
      "Epoch 8888/30000 Training Loss: 0.09527072310447693\n",
      "Epoch 8889/30000 Training Loss: 0.07981183379888535\n",
      "Epoch 8890/30000 Training Loss: 0.07105764001607895\n",
      "Epoch 8890/30000 Validation Loss: 0.06909069418907166\n",
      "Epoch 8891/30000 Training Loss: 0.0813303217291832\n",
      "Epoch 8892/30000 Training Loss: 0.08553540706634521\n",
      "Epoch 8893/30000 Training Loss: 0.08267230540513992\n",
      "Epoch 8894/30000 Training Loss: 0.07693585753440857\n",
      "Epoch 8895/30000 Training Loss: 0.09466996788978577\n",
      "Epoch 8896/30000 Training Loss: 0.07354974001646042\n",
      "Epoch 8897/30000 Training Loss: 0.0744357481598854\n",
      "Epoch 8898/30000 Training Loss: 0.09933748841285706\n",
      "Epoch 8899/30000 Training Loss: 0.07191411405801773\n",
      "Epoch 8900/30000 Training Loss: 0.08949647098779678\n",
      "Epoch 8900/30000 Validation Loss: 0.06217718496918678\n",
      "Epoch 8901/30000 Training Loss: 0.06750812381505966\n",
      "Epoch 8902/30000 Training Loss: 0.07692737132310867\n",
      "Epoch 8903/30000 Training Loss: 0.07781044393777847\n",
      "Epoch 8904/30000 Training Loss: 0.10970459133386612\n",
      "Epoch 8905/30000 Training Loss: 0.08296526223421097\n",
      "Epoch 8906/30000 Training Loss: 0.09505071491003036\n",
      "Epoch 8907/30000 Training Loss: 0.08129192143678665\n",
      "Epoch 8908/30000 Training Loss: 0.09578275680541992\n",
      "Epoch 8909/30000 Training Loss: 0.05916789174079895\n",
      "Epoch 8910/30000 Training Loss: 0.08533617854118347\n",
      "Epoch 8910/30000 Validation Loss: 0.07526234537363052\n",
      "Epoch 8911/30000 Training Loss: 0.07323621958494186\n",
      "Epoch 8912/30000 Training Loss: 0.07315164059400558\n",
      "Epoch 8913/30000 Training Loss: 0.06919962912797928\n",
      "Epoch 8914/30000 Training Loss: 0.06437075883150101\n",
      "Epoch 8915/30000 Training Loss: 0.07683589309453964\n",
      "Epoch 8916/30000 Training Loss: 0.07881457358598709\n",
      "Epoch 8917/30000 Training Loss: 0.09109925478696823\n",
      "Epoch 8918/30000 Training Loss: 0.08576125651597977\n",
      "Epoch 8919/30000 Training Loss: 0.07915221899747849\n",
      "Epoch 8920/30000 Training Loss: 0.06734379380941391\n",
      "Epoch 8920/30000 Validation Loss: 0.07365790754556656\n",
      "Epoch 8921/30000 Training Loss: 0.06684332340955734\n",
      "Epoch 8922/30000 Training Loss: 0.08386129140853882\n",
      "Epoch 8923/30000 Training Loss: 0.08482147008180618\n",
      "Epoch 8924/30000 Training Loss: 0.07949545234441757\n",
      "Epoch 8925/30000 Training Loss: 0.0720624253153801\n",
      "Epoch 8926/30000 Training Loss: 0.08821121603250504\n",
      "Epoch 8927/30000 Training Loss: 0.08080235868692398\n",
      "Epoch 8928/30000 Training Loss: 0.062076110392808914\n",
      "Epoch 8929/30000 Training Loss: 0.09132594615221024\n",
      "Epoch 8930/30000 Training Loss: 0.10329374670982361\n",
      "Epoch 8930/30000 Validation Loss: 0.10798511654138565\n",
      "Epoch 8931/30000 Training Loss: 0.06817970424890518\n",
      "Epoch 8932/30000 Training Loss: 0.09393131732940674\n",
      "Epoch 8933/30000 Training Loss: 0.08502695709466934\n",
      "Epoch 8934/30000 Training Loss: 0.09453865885734558\n",
      "Epoch 8935/30000 Training Loss: 0.08290087431669235\n",
      "Epoch 8936/30000 Training Loss: 0.08163423836231232\n",
      "Epoch 8937/30000 Training Loss: 0.07988329976797104\n",
      "Epoch 8938/30000 Training Loss: 0.09491372108459473\n",
      "Epoch 8939/30000 Training Loss: 0.061345845460891724\n",
      "Epoch 8940/30000 Training Loss: 0.07325664907693863\n",
      "Epoch 8940/30000 Validation Loss: 0.08243859559297562\n",
      "Epoch 8941/30000 Training Loss: 0.07469296455383301\n",
      "Epoch 8942/30000 Training Loss: 0.09975560754537582\n",
      "Epoch 8943/30000 Training Loss: 0.09178146719932556\n",
      "Epoch 8944/30000 Training Loss: 0.09501224756240845\n",
      "Epoch 8945/30000 Training Loss: 0.06170869991183281\n",
      "Epoch 8946/30000 Training Loss: 0.07481234520673752\n",
      "Epoch 8947/30000 Training Loss: 0.07916506379842758\n",
      "Epoch 8948/30000 Training Loss: 0.07043380290269852\n",
      "Epoch 8949/30000 Training Loss: 0.10599040240049362\n",
      "Epoch 8950/30000 Training Loss: 0.08662226051092148\n",
      "Epoch 8950/30000 Validation Loss: 0.07733730226755142\n",
      "Epoch 8951/30000 Training Loss: 0.08655384927988052\n",
      "Epoch 8952/30000 Training Loss: 0.07857660204172134\n",
      "Epoch 8953/30000 Training Loss: 0.09773636609315872\n",
      "Epoch 8954/30000 Training Loss: 0.07221264392137527\n",
      "Epoch 8955/30000 Training Loss: 0.06997059285640717\n",
      "Epoch 8956/30000 Training Loss: 0.07837861031293869\n",
      "Epoch 8957/30000 Training Loss: 0.0721810832619667\n",
      "Epoch 8958/30000 Training Loss: 0.0976618155837059\n",
      "Epoch 8959/30000 Training Loss: 0.07479216903448105\n",
      "Epoch 8960/30000 Training Loss: 0.10818123817443848\n",
      "Epoch 8960/30000 Validation Loss: 0.06556006520986557\n",
      "Epoch 8961/30000 Training Loss: 0.1025737002491951\n",
      "Epoch 8962/30000 Training Loss: 0.10035517811775208\n",
      "Epoch 8963/30000 Training Loss: 0.09123986214399338\n",
      "Epoch 8964/30000 Training Loss: 0.06634511798620224\n",
      "Epoch 8965/30000 Training Loss: 0.07359250634908676\n",
      "Epoch 8966/30000 Training Loss: 0.08361881971359253\n",
      "Epoch 8967/30000 Training Loss: 0.07421674579381943\n",
      "Epoch 8968/30000 Training Loss: 0.06113390997052193\n",
      "Epoch 8969/30000 Training Loss: 0.07451658695936203\n",
      "Epoch 8970/30000 Training Loss: 0.07464185357093811\n",
      "Epoch 8970/30000 Validation Loss: 0.07551037520170212\n",
      "Epoch 8971/30000 Training Loss: 0.08521351963281631\n",
      "Epoch 8972/30000 Training Loss: 0.0886651873588562\n",
      "Epoch 8973/30000 Training Loss: 0.06990297883749008\n",
      "Epoch 8974/30000 Training Loss: 0.09328200668096542\n",
      "Epoch 8975/30000 Training Loss: 0.0783376544713974\n",
      "Epoch 8976/30000 Training Loss: 0.09171774238348007\n",
      "Epoch 8977/30000 Training Loss: 0.09237038344144821\n",
      "Epoch 8978/30000 Training Loss: 0.08113984018564224\n",
      "Epoch 8979/30000 Training Loss: 0.0874413251876831\n",
      "Epoch 8980/30000 Training Loss: 0.07729332894086838\n",
      "Epoch 8980/30000 Validation Loss: 0.06485287845134735\n",
      "Epoch 8981/30000 Training Loss: 0.07565420120954514\n",
      "Epoch 8982/30000 Training Loss: 0.09223269671201706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8983/30000 Training Loss: 0.08123672008514404\n",
      "Epoch 8984/30000 Training Loss: 0.08475273102521896\n",
      "Epoch 8985/30000 Training Loss: 0.08525765687227249\n",
      "Epoch 8986/30000 Training Loss: 0.07525136321783066\n",
      "Epoch 8987/30000 Training Loss: 0.07733935862779617\n",
      "Epoch 8988/30000 Training Loss: 0.10606509447097778\n",
      "Epoch 8989/30000 Training Loss: 0.09573283046483994\n",
      "Epoch 8990/30000 Training Loss: 0.10261332243680954\n",
      "Epoch 8990/30000 Validation Loss: 0.10348358005285263\n",
      "Epoch 8991/30000 Training Loss: 0.07861322909593582\n",
      "Epoch 8992/30000 Training Loss: 0.09872705489397049\n",
      "Epoch 8993/30000 Training Loss: 0.09483952075242996\n",
      "Epoch 8994/30000 Training Loss: 0.060490380972623825\n",
      "Epoch 8995/30000 Training Loss: 0.06152559444308281\n",
      "Epoch 8996/30000 Training Loss: 0.07749881595373154\n",
      "Epoch 8997/30000 Training Loss: 0.07024092227220535\n",
      "Epoch 8998/30000 Training Loss: 0.07933975011110306\n",
      "Epoch 8999/30000 Training Loss: 0.09584054350852966\n",
      "Epoch 9000/30000 Training Loss: 0.07469994574785233\n",
      "Epoch 9000/30000 Validation Loss: 0.09140811115503311\n",
      "Epoch 9001/30000 Training Loss: 0.08469158411026001\n",
      "Epoch 9002/30000 Training Loss: 0.08115535974502563\n",
      "Epoch 9003/30000 Training Loss: 0.08444445580244064\n",
      "Epoch 9004/30000 Training Loss: 0.07888379693031311\n",
      "Epoch 9005/30000 Training Loss: 0.0699567198753357\n",
      "Epoch 9006/30000 Training Loss: 0.07218923419713974\n",
      "Epoch 9007/30000 Training Loss: 0.0760381817817688\n",
      "Epoch 9008/30000 Training Loss: 0.09136483818292618\n",
      "Epoch 9009/30000 Training Loss: 0.08148638159036636\n",
      "Epoch 9010/30000 Training Loss: 0.06784559041261673\n",
      "Epoch 9010/30000 Validation Loss: 0.08674582093954086\n",
      "Epoch 9011/30000 Training Loss: 0.08206994086503983\n",
      "Epoch 9012/30000 Training Loss: 0.07668869197368622\n",
      "Epoch 9013/30000 Training Loss: 0.09609419852495193\n",
      "Epoch 9014/30000 Training Loss: 0.08373191207647324\n",
      "Epoch 9015/30000 Training Loss: 0.10691829770803452\n",
      "Epoch 9016/30000 Training Loss: 0.08197357505559921\n",
      "Epoch 9017/30000 Training Loss: 0.0769835039973259\n",
      "Epoch 9018/30000 Training Loss: 0.0886760950088501\n",
      "Epoch 9019/30000 Training Loss: 0.07693928480148315\n",
      "Epoch 9020/30000 Training Loss: 0.07407256215810776\n",
      "Epoch 9020/30000 Validation Loss: 0.07607961446046829\n",
      "Epoch 9021/30000 Training Loss: 0.07761073857545853\n",
      "Epoch 9022/30000 Training Loss: 0.0799989178776741\n",
      "Epoch 9023/30000 Training Loss: 0.08684241026639938\n",
      "Epoch 9024/30000 Training Loss: 0.07909425348043442\n",
      "Epoch 9025/30000 Training Loss: 0.10738900303840637\n",
      "Epoch 9026/30000 Training Loss: 0.09139718860387802\n",
      "Epoch 9027/30000 Training Loss: 0.07368787378072739\n",
      "Epoch 9028/30000 Training Loss: 0.08389708399772644\n",
      "Epoch 9029/30000 Training Loss: 0.0729822888970375\n",
      "Epoch 9030/30000 Training Loss: 0.09110940247774124\n",
      "Epoch 9030/30000 Validation Loss: 0.1041010245680809\n",
      "Epoch 9031/30000 Training Loss: 0.10033487528562546\n",
      "Epoch 9032/30000 Training Loss: 0.07526887953281403\n",
      "Epoch 9033/30000 Training Loss: 0.06461650878190994\n",
      "Epoch 9034/30000 Training Loss: 0.09011097997426987\n",
      "Epoch 9035/30000 Training Loss: 0.06571363657712936\n",
      "Epoch 9036/30000 Training Loss: 0.08051791042089462\n",
      "Epoch 9037/30000 Training Loss: 0.09545157104730606\n",
      "Epoch 9038/30000 Training Loss: 0.09291290491819382\n",
      "Epoch 9039/30000 Training Loss: 0.07042247802019119\n",
      "Epoch 9040/30000 Training Loss: 0.07559726387262344\n",
      "Epoch 9040/30000 Validation Loss: 0.09512615948915482\n",
      "Epoch 9041/30000 Training Loss: 0.07987645268440247\n",
      "Epoch 9042/30000 Training Loss: 0.07630182802677155\n",
      "Epoch 9043/30000 Training Loss: 0.0575905442237854\n",
      "Epoch 9044/30000 Training Loss: 0.08189257234334946\n",
      "Epoch 9045/30000 Training Loss: 0.08998356014490128\n",
      "Epoch 9046/30000 Training Loss: 0.08955321460962296\n",
      "Epoch 9047/30000 Training Loss: 0.07792112231254578\n",
      "Epoch 9048/30000 Training Loss: 0.08060640096664429\n",
      "Epoch 9049/30000 Training Loss: 0.09581756591796875\n",
      "Epoch 9050/30000 Training Loss: 0.06596300005912781\n",
      "Epoch 9050/30000 Validation Loss: 0.09358517080545425\n",
      "Epoch 9051/30000 Training Loss: 0.07146783918142319\n",
      "Epoch 9052/30000 Training Loss: 0.08634519577026367\n",
      "Epoch 9053/30000 Training Loss: 0.08042579144239426\n",
      "Epoch 9054/30000 Training Loss: 0.07418519258499146\n",
      "Epoch 9055/30000 Training Loss: 0.08094843477010727\n",
      "Epoch 9056/30000 Training Loss: 0.09130790084600449\n",
      "Epoch 9057/30000 Training Loss: 0.10116804391145706\n",
      "Epoch 9058/30000 Training Loss: 0.07682638615369797\n",
      "Epoch 9059/30000 Training Loss: 0.06908170133829117\n",
      "Epoch 9060/30000 Training Loss: 0.08621987700462341\n",
      "Epoch 9060/30000 Validation Loss: 0.0949174165725708\n",
      "Epoch 9061/30000 Training Loss: 0.11066048592329025\n",
      "Epoch 9062/30000 Training Loss: 0.07398752868175507\n",
      "Epoch 9063/30000 Training Loss: 0.0711708813905716\n",
      "Epoch 9064/30000 Training Loss: 0.09458956122398376\n",
      "Epoch 9065/30000 Training Loss: 0.07973075658082962\n",
      "Epoch 9066/30000 Training Loss: 0.07541538029909134\n",
      "Epoch 9067/30000 Training Loss: 0.06694032996892929\n",
      "Epoch 9068/30000 Training Loss: 0.07076161354780197\n",
      "Epoch 9069/30000 Training Loss: 0.0922660306096077\n",
      "Epoch 9070/30000 Training Loss: 0.09739336371421814\n",
      "Epoch 9070/30000 Validation Loss: 0.08525487035512924\n",
      "Epoch 9071/30000 Training Loss: 0.08401260524988174\n",
      "Epoch 9072/30000 Training Loss: 0.09511429071426392\n",
      "Epoch 9073/30000 Training Loss: 0.08561009168624878\n",
      "Epoch 9074/30000 Training Loss: 0.08243528008460999\n",
      "Epoch 9075/30000 Training Loss: 0.08414667099714279\n",
      "Epoch 9076/30000 Training Loss: 0.09231451898813248\n",
      "Epoch 9077/30000 Training Loss: 0.10837117582559586\n",
      "Epoch 9078/30000 Training Loss: 0.08493123203516006\n",
      "Epoch 9079/30000 Training Loss: 0.10293833166360855\n",
      "Epoch 9080/30000 Training Loss: 0.07205203175544739\n",
      "Epoch 9080/30000 Validation Loss: 0.09337303042411804\n",
      "Epoch 9081/30000 Training Loss: 0.08962220698595047\n",
      "Epoch 9082/30000 Training Loss: 0.08108490705490112\n",
      "Epoch 9083/30000 Training Loss: 0.06958968937397003\n",
      "Epoch 9084/30000 Training Loss: 0.07174170017242432\n",
      "Epoch 9085/30000 Training Loss: 0.06492593139410019\n",
      "Epoch 9086/30000 Training Loss: 0.10054492205381393\n",
      "Epoch 9087/30000 Training Loss: 0.0694003775715828\n",
      "Epoch 9088/30000 Training Loss: 0.11134081333875656\n",
      "Epoch 9089/30000 Training Loss: 0.07581178843975067\n",
      "Epoch 9090/30000 Training Loss: 0.072977215051651\n",
      "Epoch 9090/30000 Validation Loss: 0.07911782711744308\n",
      "Epoch 9091/30000 Training Loss: 0.08145753294229507\n",
      "Epoch 9092/30000 Training Loss: 0.08333569020032883\n",
      "Epoch 9093/30000 Training Loss: 0.08846542984247208\n",
      "Epoch 9094/30000 Training Loss: 0.09062894433736801\n",
      "Epoch 9095/30000 Training Loss: 0.09408269077539444\n",
      "Epoch 9096/30000 Training Loss: 0.07846225798130035\n",
      "Epoch 9097/30000 Training Loss: 0.08357211947441101\n",
      "Epoch 9098/30000 Training Loss: 0.09562567621469498\n",
      "Epoch 9099/30000 Training Loss: 0.08615205436944962\n",
      "Epoch 9100/30000 Training Loss: 0.07379258424043655\n",
      "Epoch 9100/30000 Validation Loss: 0.10373083502054214\n",
      "Epoch 9101/30000 Training Loss: 0.08164072036743164\n",
      "Epoch 9102/30000 Training Loss: 0.11701014637947083\n",
      "Epoch 9103/30000 Training Loss: 0.05303477868437767\n",
      "Epoch 9104/30000 Training Loss: 0.08791366219520569\n",
      "Epoch 9105/30000 Training Loss: 0.10045862942934036\n",
      "Epoch 9106/30000 Training Loss: 0.08508437871932983\n",
      "Epoch 9107/30000 Training Loss: 0.08341342210769653\n",
      "Epoch 9108/30000 Training Loss: 0.09027298539876938\n",
      "Epoch 9109/30000 Training Loss: 0.0875997468829155\n",
      "Epoch 9110/30000 Training Loss: 0.07311185449361801\n",
      "Epoch 9110/30000 Validation Loss: 0.06735387444496155\n",
      "Epoch 9111/30000 Training Loss: 0.08218195289373398\n",
      "Epoch 9112/30000 Training Loss: 0.07417205721139908\n",
      "Epoch 9113/30000 Training Loss: 0.08785075694322586\n",
      "Epoch 9114/30000 Training Loss: 0.07887076586484909\n",
      "Epoch 9115/30000 Training Loss: 0.07982761412858963\n",
      "Epoch 9116/30000 Training Loss: 0.10234859585762024\n",
      "Epoch 9117/30000 Training Loss: 0.07896671444177628\n",
      "Epoch 9118/30000 Training Loss: 0.07229495048522949\n",
      "Epoch 9119/30000 Training Loss: 0.06494829058647156\n",
      "Epoch 9120/30000 Training Loss: 0.06834131479263306\n",
      "Epoch 9120/30000 Validation Loss: 0.07239183783531189\n",
      "Epoch 9121/30000 Training Loss: 0.07574691623449326\n",
      "Epoch 9122/30000 Training Loss: 0.09612248092889786\n",
      "Epoch 9123/30000 Training Loss: 0.09474679827690125\n",
      "Epoch 9124/30000 Training Loss: 0.08650755137205124\n",
      "Epoch 9125/30000 Training Loss: 0.08734337240457535\n",
      "Epoch 9126/30000 Training Loss: 0.07096021622419357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9127/30000 Training Loss: 0.08367045968770981\n",
      "Epoch 9128/30000 Training Loss: 0.08271388709545135\n",
      "Epoch 9129/30000 Training Loss: 0.07752266526222229\n",
      "Epoch 9130/30000 Training Loss: 0.10318338871002197\n",
      "Epoch 9130/30000 Validation Loss: 0.06771314144134521\n",
      "Epoch 9131/30000 Training Loss: 0.07697734981775284\n",
      "Epoch 9132/30000 Training Loss: 0.08805331587791443\n",
      "Epoch 9133/30000 Training Loss: 0.08149266242980957\n",
      "Epoch 9134/30000 Training Loss: 0.07558173686265945\n",
      "Epoch 9135/30000 Training Loss: 0.07991424947977066\n",
      "Epoch 9136/30000 Training Loss: 0.06124353036284447\n",
      "Epoch 9137/30000 Training Loss: 0.08830494433641434\n",
      "Epoch 9138/30000 Training Loss: 0.09529250115156174\n",
      "Epoch 9139/30000 Training Loss: 0.0981997475028038\n",
      "Epoch 9140/30000 Training Loss: 0.1088804304599762\n",
      "Epoch 9140/30000 Validation Loss: 0.1026148796081543\n",
      "Epoch 9141/30000 Training Loss: 0.10740157216787338\n",
      "Epoch 9142/30000 Training Loss: 0.07800403237342834\n",
      "Epoch 9143/30000 Training Loss: 0.06682934612035751\n",
      "Epoch 9144/30000 Training Loss: 0.07219918072223663\n",
      "Epoch 9145/30000 Training Loss: 0.07210557907819748\n",
      "Epoch 9146/30000 Training Loss: 0.08474842458963394\n",
      "Epoch 9147/30000 Training Loss: 0.0796407163143158\n",
      "Epoch 9148/30000 Training Loss: 0.08090900629758835\n",
      "Epoch 9149/30000 Training Loss: 0.08675306290388107\n",
      "Epoch 9150/30000 Training Loss: 0.06931185722351074\n",
      "Epoch 9150/30000 Validation Loss: 0.07652141153812408\n",
      "Epoch 9151/30000 Training Loss: 0.07477283477783203\n",
      "Epoch 9152/30000 Training Loss: 0.08428303152322769\n",
      "Epoch 9153/30000 Training Loss: 0.07008590549230576\n",
      "Epoch 9154/30000 Training Loss: 0.08966383337974548\n",
      "Epoch 9155/30000 Training Loss: 0.07246201485395432\n",
      "Epoch 9156/30000 Training Loss: 0.0853615403175354\n",
      "Epoch 9157/30000 Training Loss: 0.0790170431137085\n",
      "Epoch 9158/30000 Training Loss: 0.0829223021864891\n",
      "Epoch 9159/30000 Training Loss: 0.07433869689702988\n",
      "Epoch 9160/30000 Training Loss: 0.07180976867675781\n",
      "Epoch 9160/30000 Validation Loss: 0.0916704535484314\n",
      "Epoch 9161/30000 Training Loss: 0.0912233516573906\n",
      "Epoch 9162/30000 Training Loss: 0.07738398015499115\n",
      "Epoch 9163/30000 Training Loss: 0.07586439698934555\n",
      "Epoch 9164/30000 Training Loss: 0.06499072164297104\n",
      "Epoch 9165/30000 Training Loss: 0.09005860239267349\n",
      "Epoch 9166/30000 Training Loss: 0.08945485949516296\n",
      "Epoch 9167/30000 Training Loss: 0.0771520733833313\n",
      "Epoch 9168/30000 Training Loss: 0.07168077677488327\n",
      "Epoch 9169/30000 Training Loss: 0.07608430832624435\n",
      "Epoch 9170/30000 Training Loss: 0.08056236058473587\n",
      "Epoch 9170/30000 Validation Loss: 0.10325909405946732\n",
      "Epoch 9171/30000 Training Loss: 0.07389364391565323\n",
      "Epoch 9172/30000 Training Loss: 0.07352571934461594\n",
      "Epoch 9173/30000 Training Loss: 0.07563742250204086\n",
      "Epoch 9174/30000 Training Loss: 0.08947515487670898\n",
      "Epoch 9175/30000 Training Loss: 0.08794918656349182\n",
      "Epoch 9176/30000 Training Loss: 0.06440498679876328\n",
      "Epoch 9177/30000 Training Loss: 0.06802237033843994\n",
      "Epoch 9178/30000 Training Loss: 0.08224255591630936\n",
      "Epoch 9179/30000 Training Loss: 0.11372524499893188\n",
      "Epoch 9180/30000 Training Loss: 0.08788206428289413\n",
      "Epoch 9180/30000 Validation Loss: 0.07821374386548996\n",
      "Epoch 9181/30000 Training Loss: 0.09936833381652832\n",
      "Epoch 9182/30000 Training Loss: 0.06964918226003647\n",
      "Epoch 9183/30000 Training Loss: 0.07701312005519867\n",
      "Epoch 9184/30000 Training Loss: 0.06716933101415634\n",
      "Epoch 9185/30000 Training Loss: 0.0890471339225769\n",
      "Epoch 9186/30000 Training Loss: 0.07490494847297668\n",
      "Epoch 9187/30000 Training Loss: 0.09217997640371323\n",
      "Epoch 9188/30000 Training Loss: 0.07239431142807007\n",
      "Epoch 9189/30000 Training Loss: 0.07534686475992203\n",
      "Epoch 9190/30000 Training Loss: 0.08262871950864792\n",
      "Epoch 9190/30000 Validation Loss: 0.07213529199361801\n",
      "Epoch 9191/30000 Training Loss: 0.09780126810073853\n",
      "Epoch 9192/30000 Training Loss: 0.06946571171283722\n",
      "Epoch 9193/30000 Training Loss: 0.07061298936605453\n",
      "Epoch 9194/30000 Training Loss: 0.07536814361810684\n",
      "Epoch 9195/30000 Training Loss: 0.08995630592107773\n",
      "Epoch 9196/30000 Training Loss: 0.0949811115860939\n",
      "Epoch 9197/30000 Training Loss: 0.07414586842060089\n",
      "Epoch 9198/30000 Training Loss: 0.07822870463132858\n",
      "Epoch 9199/30000 Training Loss: 0.07108210772275925\n",
      "Epoch 9200/30000 Training Loss: 0.09018930047750473\n",
      "Epoch 9200/30000 Validation Loss: 0.06426170468330383\n",
      "Epoch 9201/30000 Training Loss: 0.07336630672216415\n",
      "Epoch 9202/30000 Training Loss: 0.09719052910804749\n",
      "Epoch 9203/30000 Training Loss: 0.05910414084792137\n",
      "Epoch 9204/30000 Training Loss: 0.08036711812019348\n",
      "Epoch 9205/30000 Training Loss: 0.07461503893136978\n",
      "Epoch 9206/30000 Training Loss: 0.07896622270345688\n",
      "Epoch 9207/30000 Training Loss: 0.09276791661977768\n",
      "Epoch 9208/30000 Training Loss: 0.10079032927751541\n",
      "Epoch 9209/30000 Training Loss: 0.0814894586801529\n",
      "Epoch 9210/30000 Training Loss: 0.07678445428609848\n",
      "Epoch 9210/30000 Validation Loss: 0.07206741720438004\n",
      "Epoch 9211/30000 Training Loss: 0.09946813434362411\n",
      "Epoch 9212/30000 Training Loss: 0.07250627130270004\n",
      "Epoch 9213/30000 Training Loss: 0.07368219643831253\n",
      "Epoch 9214/30000 Training Loss: 0.11648142337799072\n",
      "Epoch 9215/30000 Training Loss: 0.07744941115379333\n",
      "Epoch 9216/30000 Training Loss: 0.07568185776472092\n",
      "Epoch 9217/30000 Training Loss: 0.08242323994636536\n",
      "Epoch 9218/30000 Training Loss: 0.08588894456624985\n",
      "Epoch 9219/30000 Training Loss: 0.0682254359126091\n",
      "Epoch 9220/30000 Training Loss: 0.07620267570018768\n",
      "Epoch 9220/30000 Validation Loss: 0.07321606576442719\n",
      "Epoch 9221/30000 Training Loss: 0.09233392030000687\n",
      "Epoch 9222/30000 Training Loss: 0.07990139722824097\n",
      "Epoch 9223/30000 Training Loss: 0.0965832993388176\n",
      "Epoch 9224/30000 Training Loss: 0.07884528487920761\n",
      "Epoch 9225/30000 Training Loss: 0.08674952387809753\n",
      "Epoch 9226/30000 Training Loss: 0.06849800795316696\n",
      "Epoch 9227/30000 Training Loss: 0.08811181038618088\n",
      "Epoch 9228/30000 Training Loss: 0.09236780554056168\n",
      "Epoch 9229/30000 Training Loss: 0.06411923468112946\n",
      "Epoch 9230/30000 Training Loss: 0.07610496878623962\n",
      "Epoch 9230/30000 Validation Loss: 0.10356483608484268\n",
      "Epoch 9231/30000 Training Loss: 0.0789889320731163\n",
      "Epoch 9232/30000 Training Loss: 0.09690544009208679\n",
      "Epoch 9233/30000 Training Loss: 0.09702876955270767\n",
      "Epoch 9234/30000 Training Loss: 0.09885039180517197\n",
      "Epoch 9235/30000 Training Loss: 0.08613943308591843\n",
      "Epoch 9236/30000 Training Loss: 0.0729719027876854\n",
      "Epoch 9237/30000 Training Loss: 0.09330485016107559\n",
      "Epoch 9238/30000 Training Loss: 0.08696609735488892\n",
      "Epoch 9239/30000 Training Loss: 0.08175598829984665\n",
      "Epoch 9240/30000 Training Loss: 0.06624893099069595\n",
      "Epoch 9240/30000 Validation Loss: 0.09285831451416016\n",
      "Epoch 9241/30000 Training Loss: 0.073055200278759\n",
      "Epoch 9242/30000 Training Loss: 0.06870397180318832\n",
      "Epoch 9243/30000 Training Loss: 0.09200481325387955\n",
      "Epoch 9244/30000 Training Loss: 0.07377103716135025\n",
      "Epoch 9245/30000 Training Loss: 0.06454932689666748\n",
      "Epoch 9246/30000 Training Loss: 0.09919297695159912\n",
      "Epoch 9247/30000 Training Loss: 0.08881817013025284\n",
      "Epoch 9248/30000 Training Loss: 0.1106187179684639\n",
      "Epoch 9249/30000 Training Loss: 0.07969365268945694\n",
      "Epoch 9250/30000 Training Loss: 0.06738049536943436\n",
      "Epoch 9250/30000 Validation Loss: 0.059401705861091614\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.059401705861091614<=============\n",
      "Epoch 9251/30000 Training Loss: 0.07743650674819946\n",
      "Epoch 9252/30000 Training Loss: 0.0725431814789772\n",
      "Epoch 9253/30000 Training Loss: 0.06978096812963486\n",
      "Epoch 9254/30000 Training Loss: 0.0742650255560875\n",
      "Epoch 9255/30000 Training Loss: 0.09697109460830688\n",
      "Epoch 9256/30000 Training Loss: 0.06299488246440887\n",
      "Epoch 9257/30000 Training Loss: 0.09068085998296738\n",
      "Epoch 9258/30000 Training Loss: 0.08934443444013596\n",
      "Epoch 9259/30000 Training Loss: 0.10645060986280441\n",
      "Epoch 9260/30000 Training Loss: 0.07898964732885361\n",
      "Epoch 9260/30000 Validation Loss: 0.09182407706975937\n",
      "Epoch 9261/30000 Training Loss: 0.08040597289800644\n",
      "Epoch 9262/30000 Training Loss: 0.07686883211135864\n",
      "Epoch 9263/30000 Training Loss: 0.08858126401901245\n",
      "Epoch 9264/30000 Training Loss: 0.06752298027276993\n",
      "Epoch 9265/30000 Training Loss: 0.09934035688638687\n",
      "Epoch 9266/30000 Training Loss: 0.08432171493768692\n",
      "Epoch 9267/30000 Training Loss: 0.08723888546228409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9268/30000 Training Loss: 0.07061740756034851\n",
      "Epoch 9269/30000 Training Loss: 0.06025823578238487\n",
      "Epoch 9270/30000 Training Loss: 0.07998847961425781\n",
      "Epoch 9270/30000 Validation Loss: 0.06891555339097977\n",
      "Epoch 9271/30000 Training Loss: 0.08308946341276169\n",
      "Epoch 9272/30000 Training Loss: 0.07320845127105713\n",
      "Epoch 9273/30000 Training Loss: 0.09837350994348526\n",
      "Epoch 9274/30000 Training Loss: 0.0815470889210701\n",
      "Epoch 9275/30000 Training Loss: 0.06552644073963165\n",
      "Epoch 9276/30000 Training Loss: 0.09084359556436539\n",
      "Epoch 9277/30000 Training Loss: 0.07698725908994675\n",
      "Epoch 9278/30000 Training Loss: 0.09011588245630264\n",
      "Epoch 9279/30000 Training Loss: 0.06232002377510071\n",
      "Epoch 9280/30000 Training Loss: 0.11328721791505814\n",
      "Epoch 9280/30000 Validation Loss: 0.0837823748588562\n",
      "Epoch 9281/30000 Training Loss: 0.09489016979932785\n",
      "Epoch 9282/30000 Training Loss: 0.07872659713029861\n",
      "Epoch 9283/30000 Training Loss: 0.06051502749323845\n",
      "Epoch 9284/30000 Training Loss: 0.08313635736703873\n",
      "Epoch 9285/30000 Training Loss: 0.088102787733078\n",
      "Epoch 9286/30000 Training Loss: 0.08411423116922379\n",
      "Epoch 9287/30000 Training Loss: 0.0893213152885437\n",
      "Epoch 9288/30000 Training Loss: 0.07942316681146622\n",
      "Epoch 9289/30000 Training Loss: 0.08918101340532303\n",
      "Epoch 9290/30000 Training Loss: 0.10165726393461227\n",
      "Epoch 9290/30000 Validation Loss: 0.08315058052539825\n",
      "Epoch 9291/30000 Training Loss: 0.08194509148597717\n",
      "Epoch 9292/30000 Training Loss: 0.06515685468912125\n",
      "Epoch 9293/30000 Training Loss: 0.07251433283090591\n",
      "Epoch 9294/30000 Training Loss: 0.08037707954645157\n",
      "Epoch 9295/30000 Training Loss: 0.07686012983322144\n",
      "Epoch 9296/30000 Training Loss: 0.10378501564264297\n",
      "Epoch 9297/30000 Training Loss: 0.08464477211236954\n",
      "Epoch 9298/30000 Training Loss: 0.08372942358255386\n",
      "Epoch 9299/30000 Training Loss: 0.06904467195272446\n",
      "Epoch 9300/30000 Training Loss: 0.10683828592300415\n",
      "Epoch 9300/30000 Validation Loss: 0.07160211354494095\n",
      "Epoch 9301/30000 Training Loss: 0.07956562936306\n",
      "Epoch 9302/30000 Training Loss: 0.0690203532576561\n",
      "Epoch 9303/30000 Training Loss: 0.10650873929262161\n",
      "Epoch 9304/30000 Training Loss: 0.08732569217681885\n",
      "Epoch 9305/30000 Training Loss: 0.0786375105381012\n",
      "Epoch 9306/30000 Training Loss: 0.07378639280796051\n",
      "Epoch 9307/30000 Training Loss: 0.08668617159128189\n",
      "Epoch 9308/30000 Training Loss: 0.09303242713212967\n",
      "Epoch 9309/30000 Training Loss: 0.0800212100148201\n",
      "Epoch 9310/30000 Training Loss: 0.08631182461977005\n",
      "Epoch 9310/30000 Validation Loss: 0.07776037603616714\n",
      "Epoch 9311/30000 Training Loss: 0.08328744769096375\n",
      "Epoch 9312/30000 Training Loss: 0.0888250395655632\n",
      "Epoch 9313/30000 Training Loss: 0.07819904386997223\n",
      "Epoch 9314/30000 Training Loss: 0.07206439226865768\n",
      "Epoch 9315/30000 Training Loss: 0.08327432721853256\n",
      "Epoch 9316/30000 Training Loss: 0.07035959511995316\n",
      "Epoch 9317/30000 Training Loss: 0.08520375937223434\n",
      "Epoch 9318/30000 Training Loss: 0.0840567946434021\n",
      "Epoch 9319/30000 Training Loss: 0.0857766643166542\n",
      "Epoch 9320/30000 Training Loss: 0.07441255450248718\n",
      "Epoch 9320/30000 Validation Loss: 0.06943643093109131\n",
      "Epoch 9321/30000 Training Loss: 0.06168787553906441\n",
      "Epoch 9322/30000 Training Loss: 0.06743723899126053\n",
      "Epoch 9323/30000 Training Loss: 0.07811234146356583\n",
      "Epoch 9324/30000 Training Loss: 0.07229272276163101\n",
      "Epoch 9325/30000 Training Loss: 0.09442014247179031\n",
      "Epoch 9326/30000 Training Loss: 0.08242049068212509\n",
      "Epoch 9327/30000 Training Loss: 0.0754169151186943\n",
      "Epoch 9328/30000 Training Loss: 0.07712738960981369\n",
      "Epoch 9329/30000 Training Loss: 0.07959415763616562\n",
      "Epoch 9330/30000 Training Loss: 0.059774477034807205\n",
      "Epoch 9330/30000 Validation Loss: 0.08443411439657211\n",
      "Epoch 9331/30000 Training Loss: 0.07318290323019028\n",
      "Epoch 9332/30000 Training Loss: 0.08754279464483261\n",
      "Epoch 9333/30000 Training Loss: 0.07747765630483627\n",
      "Epoch 9334/30000 Training Loss: 0.07702621072530746\n",
      "Epoch 9335/30000 Training Loss: 0.0776112899184227\n",
      "Epoch 9336/30000 Training Loss: 0.08184801787137985\n",
      "Epoch 9337/30000 Training Loss: 0.08847571164369583\n",
      "Epoch 9338/30000 Training Loss: 0.08737383037805557\n",
      "Epoch 9339/30000 Training Loss: 0.08217978477478027\n",
      "Epoch 9340/30000 Training Loss: 0.0611853189766407\n",
      "Epoch 9340/30000 Validation Loss: 0.09338096529245377\n",
      "Epoch 9341/30000 Training Loss: 0.07939156889915466\n",
      "Epoch 9342/30000 Training Loss: 0.08277866989374161\n",
      "Epoch 9343/30000 Training Loss: 0.07771629095077515\n",
      "Epoch 9344/30000 Training Loss: 0.09380370378494263\n",
      "Epoch 9345/30000 Training Loss: 0.07766614854335785\n",
      "Epoch 9346/30000 Training Loss: 0.07738909125328064\n",
      "Epoch 9347/30000 Training Loss: 0.08733714371919632\n",
      "Epoch 9348/30000 Training Loss: 0.08593722432851791\n",
      "Epoch 9349/30000 Training Loss: 0.0847567692399025\n",
      "Epoch 9350/30000 Training Loss: 0.07224802672863007\n",
      "Epoch 9350/30000 Validation Loss: 0.08255476504564285\n",
      "Epoch 9351/30000 Training Loss: 0.0903201624751091\n",
      "Epoch 9352/30000 Training Loss: 0.08204396814107895\n",
      "Epoch 9353/30000 Training Loss: 0.06663863360881805\n",
      "Epoch 9354/30000 Training Loss: 0.10369863361120224\n",
      "Epoch 9355/30000 Training Loss: 0.08971557766199112\n",
      "Epoch 9356/30000 Training Loss: 0.0683445855975151\n",
      "Epoch 9357/30000 Training Loss: 0.08368083834648132\n",
      "Epoch 9358/30000 Training Loss: 0.0652136281132698\n",
      "Epoch 9359/30000 Training Loss: 0.06763648241758347\n",
      "Epoch 9360/30000 Training Loss: 0.06890533119440079\n",
      "Epoch 9360/30000 Validation Loss: 0.08031145483255386\n",
      "Epoch 9361/30000 Training Loss: 0.08138465881347656\n",
      "Epoch 9362/30000 Training Loss: 0.0911870226264\n",
      "Epoch 9363/30000 Training Loss: 0.09031691402196884\n",
      "Epoch 9364/30000 Training Loss: 0.09492122381925583\n",
      "Epoch 9365/30000 Training Loss: 0.08667683601379395\n",
      "Epoch 9366/30000 Training Loss: 0.09293856471776962\n",
      "Epoch 9367/30000 Training Loss: 0.07995292544364929\n",
      "Epoch 9368/30000 Training Loss: 0.072887122631073\n",
      "Epoch 9369/30000 Training Loss: 0.09246043115854263\n",
      "Epoch 9370/30000 Training Loss: 0.07111293822526932\n",
      "Epoch 9370/30000 Validation Loss: 0.0690963938832283\n",
      "Epoch 9371/30000 Training Loss: 0.09504365921020508\n",
      "Epoch 9372/30000 Training Loss: 0.10124038904905319\n",
      "Epoch 9373/30000 Training Loss: 0.0865263119339943\n",
      "Epoch 9374/30000 Training Loss: 0.10087615251541138\n",
      "Epoch 9375/30000 Training Loss: 0.08325818926095963\n",
      "Epoch 9376/30000 Training Loss: 0.0887841060757637\n",
      "Epoch 9377/30000 Training Loss: 0.0958450436592102\n",
      "Epoch 9378/30000 Training Loss: 0.08920984715223312\n",
      "Epoch 9379/30000 Training Loss: 0.0675036683678627\n",
      "Epoch 9380/30000 Training Loss: 0.08220259100198746\n",
      "Epoch 9380/30000 Validation Loss: 0.07900584489107132\n",
      "Epoch 9381/30000 Training Loss: 0.0966363325715065\n",
      "Epoch 9382/30000 Training Loss: 0.07220447063446045\n",
      "Epoch 9383/30000 Training Loss: 0.07193931937217712\n",
      "Epoch 9384/30000 Training Loss: 0.06487101316452026\n",
      "Epoch 9385/30000 Training Loss: 0.08572584390640259\n",
      "Epoch 9386/30000 Training Loss: 0.07160831242799759\n",
      "Epoch 9387/30000 Training Loss: 0.08253972977399826\n",
      "Epoch 9388/30000 Training Loss: 0.07453982532024384\n",
      "Epoch 9389/30000 Training Loss: 0.08225357532501221\n",
      "Epoch 9390/30000 Training Loss: 0.08939263969659805\n",
      "Epoch 9390/30000 Validation Loss: 0.08384272456169128\n",
      "Epoch 9391/30000 Training Loss: 0.06849796324968338\n",
      "Epoch 9392/30000 Training Loss: 0.08501257747411728\n",
      "Epoch 9393/30000 Training Loss: 0.08703792840242386\n",
      "Epoch 9394/30000 Training Loss: 0.05960780754685402\n",
      "Epoch 9395/30000 Training Loss: 0.07309924066066742\n",
      "Epoch 9396/30000 Training Loss: 0.07440673559904099\n",
      "Epoch 9397/30000 Training Loss: 0.0696069747209549\n",
      "Epoch 9398/30000 Training Loss: 0.11155828088521957\n",
      "Epoch 9399/30000 Training Loss: 0.06472013145685196\n",
      "Epoch 9400/30000 Training Loss: 0.07779089361429214\n",
      "Epoch 9400/30000 Validation Loss: 0.07632419466972351\n",
      "Epoch 9401/30000 Training Loss: 0.0694524422287941\n",
      "Epoch 9402/30000 Training Loss: 0.0667274072766304\n",
      "Epoch 9403/30000 Training Loss: 0.08000551909208298\n",
      "Epoch 9404/30000 Training Loss: 0.08194798976182938\n",
      "Epoch 9405/30000 Training Loss: 0.095932237803936\n",
      "Epoch 9406/30000 Training Loss: 0.08407306671142578\n",
      "Epoch 9407/30000 Training Loss: 0.07670018076896667\n",
      "Epoch 9408/30000 Training Loss: 0.06803245097398758\n",
      "Epoch 9409/30000 Training Loss: 0.06774499267339706\n",
      "Epoch 9410/30000 Training Loss: 0.0723147988319397\n",
      "Epoch 9410/30000 Validation Loss: 0.0652926042675972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9411/30000 Training Loss: 0.07234682887792587\n",
      "Epoch 9412/30000 Training Loss: 0.07550223171710968\n",
      "Epoch 9413/30000 Training Loss: 0.06871543824672699\n",
      "Epoch 9414/30000 Training Loss: 0.09639333933591843\n",
      "Epoch 9415/30000 Training Loss: 0.08915805071592331\n",
      "Epoch 9416/30000 Training Loss: 0.07362575083971024\n",
      "Epoch 9417/30000 Training Loss: 0.08817818015813828\n",
      "Epoch 9418/30000 Training Loss: 0.08942360430955887\n",
      "Epoch 9419/30000 Training Loss: 0.08385694026947021\n",
      "Epoch 9420/30000 Training Loss: 0.06301367282867432\n",
      "Epoch 9420/30000 Validation Loss: 0.09863295406103134\n",
      "Epoch 9421/30000 Training Loss: 0.06539119780063629\n",
      "Epoch 9422/30000 Training Loss: 0.06287708133459091\n",
      "Epoch 9423/30000 Training Loss: 0.06191079318523407\n",
      "Epoch 9424/30000 Training Loss: 0.07853516936302185\n",
      "Epoch 9425/30000 Training Loss: 0.08999175578355789\n",
      "Epoch 9426/30000 Training Loss: 0.05820870399475098\n",
      "Epoch 9427/30000 Training Loss: 0.09573826938867569\n",
      "Epoch 9428/30000 Training Loss: 0.09042791277170181\n",
      "Epoch 9429/30000 Training Loss: 0.08075631409883499\n",
      "Epoch 9430/30000 Training Loss: 0.09503263235092163\n",
      "Epoch 9430/30000 Validation Loss: 0.05827070772647858\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05827070772647858<=============\n",
      "Epoch 9431/30000 Training Loss: 0.11303240060806274\n",
      "Epoch 9432/30000 Training Loss: 0.07752127200365067\n",
      "Epoch 9433/30000 Training Loss: 0.06276396661996841\n",
      "Epoch 9434/30000 Training Loss: 0.07822638005018234\n",
      "Epoch 9435/30000 Training Loss: 0.09498912841081619\n",
      "Epoch 9436/30000 Training Loss: 0.06712055206298828\n",
      "Epoch 9437/30000 Training Loss: 0.08688151091337204\n",
      "Epoch 9438/30000 Training Loss: 0.10969313234090805\n",
      "Epoch 9439/30000 Training Loss: 0.08461374044418335\n",
      "Epoch 9440/30000 Training Loss: 0.08945117145776749\n",
      "Epoch 9440/30000 Validation Loss: 0.06988071650266647\n",
      "Epoch 9441/30000 Training Loss: 0.09380968660116196\n",
      "Epoch 9442/30000 Training Loss: 0.08979059010744095\n",
      "Epoch 9443/30000 Training Loss: 0.09475744515657425\n",
      "Epoch 9444/30000 Training Loss: 0.08296116441488266\n",
      "Epoch 9445/30000 Training Loss: 0.08788762241601944\n",
      "Epoch 9446/30000 Training Loss: 0.06900227069854736\n",
      "Epoch 9447/30000 Training Loss: 0.07522746920585632\n",
      "Epoch 9448/30000 Training Loss: 0.09916802495718002\n",
      "Epoch 9449/30000 Training Loss: 0.10221680253744125\n",
      "Epoch 9450/30000 Training Loss: 0.07760804146528244\n",
      "Epoch 9450/30000 Validation Loss: 0.07930102944374084\n",
      "Epoch 9451/30000 Training Loss: 0.08847922086715698\n",
      "Epoch 9452/30000 Training Loss: 0.06495314091444016\n",
      "Epoch 9453/30000 Training Loss: 0.08358783274888992\n",
      "Epoch 9454/30000 Training Loss: 0.07781103998422623\n",
      "Epoch 9455/30000 Training Loss: 0.07904516905546188\n",
      "Epoch 9456/30000 Training Loss: 0.08422546833753586\n",
      "Epoch 9457/30000 Training Loss: 0.0820406973361969\n",
      "Epoch 9458/30000 Training Loss: 0.08000922948122025\n",
      "Epoch 9459/30000 Training Loss: 0.07639480382204056\n",
      "Epoch 9460/30000 Training Loss: 0.07569613307714462\n",
      "Epoch 9460/30000 Validation Loss: 0.09515868872404099\n",
      "Epoch 9461/30000 Training Loss: 0.09208240360021591\n",
      "Epoch 9462/30000 Training Loss: 0.0903083086013794\n",
      "Epoch 9463/30000 Training Loss: 0.06904809921979904\n",
      "Epoch 9464/30000 Training Loss: 0.07993026822805405\n",
      "Epoch 9465/30000 Training Loss: 0.07271856814622879\n",
      "Epoch 9466/30000 Training Loss: 0.08616835623979568\n",
      "Epoch 9467/30000 Training Loss: 0.0793449804186821\n",
      "Epoch 9468/30000 Training Loss: 0.08432644605636597\n",
      "Epoch 9469/30000 Training Loss: 0.06827397644519806\n",
      "Epoch 9470/30000 Training Loss: 0.06530077010393143\n",
      "Epoch 9470/30000 Validation Loss: 0.0806528702378273\n",
      "Epoch 9471/30000 Training Loss: 0.06770677119493484\n",
      "Epoch 9472/30000 Training Loss: 0.08356792479753494\n",
      "Epoch 9473/30000 Training Loss: 0.12033901363611221\n",
      "Epoch 9474/30000 Training Loss: 0.07532309740781784\n",
      "Epoch 9475/30000 Training Loss: 0.08912094682455063\n",
      "Epoch 9476/30000 Training Loss: 0.0705627053976059\n",
      "Epoch 9477/30000 Training Loss: 0.0790439248085022\n",
      "Epoch 9478/30000 Training Loss: 0.1095026507973671\n",
      "Epoch 9479/30000 Training Loss: 0.07931622862815857\n",
      "Epoch 9480/30000 Training Loss: 0.07949703931808472\n",
      "Epoch 9480/30000 Validation Loss: 0.07866521924734116\n",
      "Epoch 9481/30000 Training Loss: 0.08778198808431625\n",
      "Epoch 9482/30000 Training Loss: 0.0904134139418602\n",
      "Epoch 9483/30000 Training Loss: 0.08464983850717545\n",
      "Epoch 9484/30000 Training Loss: 0.08408889919519424\n",
      "Epoch 9485/30000 Training Loss: 0.07221803814172745\n",
      "Epoch 9486/30000 Training Loss: 0.09141767024993896\n",
      "Epoch 9487/30000 Training Loss: 0.082539401948452\n",
      "Epoch 9488/30000 Training Loss: 0.0799611508846283\n",
      "Epoch 9489/30000 Training Loss: 0.08316722512245178\n",
      "Epoch 9490/30000 Training Loss: 0.11289647966623306\n",
      "Epoch 9490/30000 Validation Loss: 0.061428409069776535\n",
      "Epoch 9491/30000 Training Loss: 0.08103916794061661\n",
      "Epoch 9492/30000 Training Loss: 0.07899659126996994\n",
      "Epoch 9493/30000 Training Loss: 0.07413837313652039\n",
      "Epoch 9494/30000 Training Loss: 0.0812540352344513\n",
      "Epoch 9495/30000 Training Loss: 0.0903521180152893\n",
      "Epoch 9496/30000 Training Loss: 0.09516193717718124\n",
      "Epoch 9497/30000 Training Loss: 0.09252754598855972\n",
      "Epoch 9498/30000 Training Loss: 0.09311983734369278\n",
      "Epoch 9499/30000 Training Loss: 0.08589885383844376\n",
      "Epoch 9500/30000 Training Loss: 0.07416220754384995\n",
      "Epoch 9500/30000 Validation Loss: 0.08876695483922958\n",
      "Epoch 9501/30000 Training Loss: 0.07896820455789566\n",
      "Epoch 9502/30000 Training Loss: 0.070670485496521\n",
      "Epoch 9503/30000 Training Loss: 0.08315899223089218\n",
      "Epoch 9504/30000 Training Loss: 0.0782051756978035\n",
      "Epoch 9505/30000 Training Loss: 0.08299486339092255\n",
      "Epoch 9506/30000 Training Loss: 0.06999849528074265\n",
      "Epoch 9507/30000 Training Loss: 0.1011338159441948\n",
      "Epoch 9508/30000 Training Loss: 0.09043628722429276\n",
      "Epoch 9509/30000 Training Loss: 0.0871480330824852\n",
      "Epoch 9510/30000 Training Loss: 0.08058422058820724\n",
      "Epoch 9510/30000 Validation Loss: 0.07583393901586533\n",
      "Epoch 9511/30000 Training Loss: 0.09318593144416809\n",
      "Epoch 9512/30000 Training Loss: 0.08835873007774353\n",
      "Epoch 9513/30000 Training Loss: 0.06266939640045166\n",
      "Epoch 9514/30000 Training Loss: 0.09657121449708939\n",
      "Epoch 9515/30000 Training Loss: 0.06073123216629028\n",
      "Epoch 9516/30000 Training Loss: 0.07468774914741516\n",
      "Epoch 9517/30000 Training Loss: 0.07109952718019485\n",
      "Epoch 9518/30000 Training Loss: 0.08249402046203613\n",
      "Epoch 9519/30000 Training Loss: 0.08399835973978043\n",
      "Epoch 9520/30000 Training Loss: 0.09478259831666946\n",
      "Epoch 9520/30000 Validation Loss: 0.09357345104217529\n",
      "Epoch 9521/30000 Training Loss: 0.07830143719911575\n",
      "Epoch 9522/30000 Training Loss: 0.08152823895215988\n",
      "Epoch 9523/30000 Training Loss: 0.07134855538606644\n",
      "Epoch 9524/30000 Training Loss: 0.09357532858848572\n",
      "Epoch 9525/30000 Training Loss: 0.09944221377372742\n",
      "Epoch 9526/30000 Training Loss: 0.09248439222574234\n",
      "Epoch 9527/30000 Training Loss: 0.09000300616025925\n",
      "Epoch 9528/30000 Training Loss: 0.06274561583995819\n",
      "Epoch 9529/30000 Training Loss: 0.09162376075983047\n",
      "Epoch 9530/30000 Training Loss: 0.08555933833122253\n",
      "Epoch 9530/30000 Validation Loss: 0.08061613142490387\n",
      "Epoch 9531/30000 Training Loss: 0.0753735899925232\n",
      "Epoch 9532/30000 Training Loss: 0.0882800742983818\n",
      "Epoch 9533/30000 Training Loss: 0.082740418612957\n",
      "Epoch 9534/30000 Training Loss: 0.08473595231771469\n",
      "Epoch 9535/30000 Training Loss: 0.07573298364877701\n",
      "Epoch 9536/30000 Training Loss: 0.09944096207618713\n",
      "Epoch 9537/30000 Training Loss: 0.08718576282262802\n",
      "Epoch 9538/30000 Training Loss: 0.078544981777668\n",
      "Epoch 9539/30000 Training Loss: 0.07576752454042435\n",
      "Epoch 9540/30000 Training Loss: 0.07926890254020691\n",
      "Epoch 9540/30000 Validation Loss: 0.08466882258653641\n",
      "Epoch 9541/30000 Training Loss: 0.06359761208295822\n",
      "Epoch 9542/30000 Training Loss: 0.07336262613534927\n",
      "Epoch 9543/30000 Training Loss: 0.05992923304438591\n",
      "Epoch 9544/30000 Training Loss: 0.09020287543535233\n",
      "Epoch 9545/30000 Training Loss: 0.094518281519413\n",
      "Epoch 9546/30000 Training Loss: 0.0881049856543541\n",
      "Epoch 9547/30000 Training Loss: 0.09395050257444382\n",
      "Epoch 9548/30000 Training Loss: 0.07434382289648056\n",
      "Epoch 9549/30000 Training Loss: 0.07275816798210144\n",
      "Epoch 9550/30000 Training Loss: 0.0801323652267456\n",
      "Epoch 9550/30000 Validation Loss: 0.08617009967565536\n",
      "Epoch 9551/30000 Training Loss: 0.07198891788721085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9552/30000 Training Loss: 0.0788113996386528\n",
      "Epoch 9553/30000 Training Loss: 0.06521943211555481\n",
      "Epoch 9554/30000 Training Loss: 0.11575760692358017\n",
      "Epoch 9555/30000 Training Loss: 0.08105144649744034\n",
      "Epoch 9556/30000 Training Loss: 0.09217176586389542\n",
      "Epoch 9557/30000 Training Loss: 0.07851973921060562\n",
      "Epoch 9558/30000 Training Loss: 0.06858328729867935\n",
      "Epoch 9559/30000 Training Loss: 0.09706848859786987\n",
      "Epoch 9560/30000 Training Loss: 0.0779910609126091\n",
      "Epoch 9560/30000 Validation Loss: 0.07591577619314194\n",
      "Epoch 9561/30000 Training Loss: 0.09004563093185425\n",
      "Epoch 9562/30000 Training Loss: 0.07218468934297562\n",
      "Epoch 9563/30000 Training Loss: 0.07066072523593903\n",
      "Epoch 9564/30000 Training Loss: 0.08873876184225082\n",
      "Epoch 9565/30000 Training Loss: 0.10832470655441284\n",
      "Epoch 9566/30000 Training Loss: 0.0892069935798645\n",
      "Epoch 9567/30000 Training Loss: 0.06723616272211075\n",
      "Epoch 9568/30000 Training Loss: 0.07554532587528229\n",
      "Epoch 9569/30000 Training Loss: 0.08372391015291214\n",
      "Epoch 9570/30000 Training Loss: 0.08849639445543289\n",
      "Epoch 9570/30000 Validation Loss: 0.09342744201421738\n",
      "Epoch 9571/30000 Training Loss: 0.0822717696428299\n",
      "Epoch 9572/30000 Training Loss: 0.07850775122642517\n",
      "Epoch 9573/30000 Training Loss: 0.06921067833900452\n",
      "Epoch 9574/30000 Training Loss: 0.06294072419404984\n",
      "Epoch 9575/30000 Training Loss: 0.0735744759440422\n",
      "Epoch 9576/30000 Training Loss: 0.09317997843027115\n",
      "Epoch 9577/30000 Training Loss: 0.08015844970941544\n",
      "Epoch 9578/30000 Training Loss: 0.07511352747678757\n",
      "Epoch 9579/30000 Training Loss: 0.06957628577947617\n",
      "Epoch 9580/30000 Training Loss: 0.08402737975120544\n",
      "Epoch 9580/30000 Validation Loss: 0.0607946403324604\n",
      "Epoch 9581/30000 Training Loss: 0.06962177902460098\n",
      "Epoch 9582/30000 Training Loss: 0.07993321120738983\n",
      "Epoch 9583/30000 Training Loss: 0.07468542456626892\n",
      "Epoch 9584/30000 Training Loss: 0.0755072832107544\n",
      "Epoch 9585/30000 Training Loss: 0.09557513147592545\n",
      "Epoch 9586/30000 Training Loss: 0.0818825289607048\n",
      "Epoch 9587/30000 Training Loss: 0.07303939014673233\n",
      "Epoch 9588/30000 Training Loss: 0.0857653021812439\n",
      "Epoch 9589/30000 Training Loss: 0.10515495389699936\n",
      "Epoch 9590/30000 Training Loss: 0.0699797049164772\n",
      "Epoch 9590/30000 Validation Loss: 0.08364105224609375\n",
      "Epoch 9591/30000 Training Loss: 0.07677610963582993\n",
      "Epoch 9592/30000 Training Loss: 0.06709485501050949\n",
      "Epoch 9593/30000 Training Loss: 0.08365288376808167\n",
      "Epoch 9594/30000 Training Loss: 0.08690089732408524\n",
      "Epoch 9595/30000 Training Loss: 0.07119346410036087\n",
      "Epoch 9596/30000 Training Loss: 0.07212377339601517\n",
      "Epoch 9597/30000 Training Loss: 0.0764818862080574\n",
      "Epoch 9598/30000 Training Loss: 0.09171409159898758\n",
      "Epoch 9599/30000 Training Loss: 0.11398222297430038\n",
      "Epoch 9600/30000 Training Loss: 0.09902840852737427\n",
      "Epoch 9600/30000 Validation Loss: 0.06457006186246872\n",
      "Epoch 9601/30000 Training Loss: 0.07359126210212708\n",
      "Epoch 9602/30000 Training Loss: 0.08262040466070175\n",
      "Epoch 9603/30000 Training Loss: 0.07528313994407654\n",
      "Epoch 9604/30000 Training Loss: 0.08258556574583054\n",
      "Epoch 9605/30000 Training Loss: 0.06736300140619278\n",
      "Epoch 9606/30000 Training Loss: 0.07926899194717407\n",
      "Epoch 9607/30000 Training Loss: 0.0775410607457161\n",
      "Epoch 9608/30000 Training Loss: 0.09952935576438904\n",
      "Epoch 9609/30000 Training Loss: 0.06742548197507858\n",
      "Epoch 9610/30000 Training Loss: 0.07718715071678162\n",
      "Epoch 9610/30000 Validation Loss: 0.08205842971801758\n",
      "Epoch 9611/30000 Training Loss: 0.06328443437814713\n",
      "Epoch 9612/30000 Training Loss: 0.07831292599439621\n",
      "Epoch 9613/30000 Training Loss: 0.06607560068368912\n",
      "Epoch 9614/30000 Training Loss: 0.07578905671834946\n",
      "Epoch 9615/30000 Training Loss: 0.09314180165529251\n",
      "Epoch 9616/30000 Training Loss: 0.07676516473293304\n",
      "Epoch 9617/30000 Training Loss: 0.06945157051086426\n",
      "Epoch 9618/30000 Training Loss: 0.11314833164215088\n",
      "Epoch 9619/30000 Training Loss: 0.08150108903646469\n",
      "Epoch 9620/30000 Training Loss: 0.0708160325884819\n",
      "Epoch 9620/30000 Validation Loss: 0.06473041325807571\n",
      "Epoch 9621/30000 Training Loss: 0.076008141040802\n",
      "Epoch 9622/30000 Training Loss: 0.0828583836555481\n",
      "Epoch 9623/30000 Training Loss: 0.07588526606559753\n",
      "Epoch 9624/30000 Training Loss: 0.08660329133272171\n",
      "Epoch 9625/30000 Training Loss: 0.06807417422533035\n",
      "Epoch 9626/30000 Training Loss: 0.06687133014202118\n",
      "Epoch 9627/30000 Training Loss: 0.09132256358861923\n",
      "Epoch 9628/30000 Training Loss: 0.07239113748073578\n",
      "Epoch 9629/30000 Training Loss: 0.0810895636677742\n",
      "Epoch 9630/30000 Training Loss: 0.062414467334747314\n",
      "Epoch 9630/30000 Validation Loss: 0.07864060997962952\n",
      "Epoch 9631/30000 Training Loss: 0.06510069221258163\n",
      "Epoch 9632/30000 Training Loss: 0.0992211177945137\n",
      "Epoch 9633/30000 Training Loss: 0.07374230772256851\n",
      "Epoch 9634/30000 Training Loss: 0.08858851343393326\n",
      "Epoch 9635/30000 Training Loss: 0.08261879533529282\n",
      "Epoch 9636/30000 Training Loss: 0.08986062556505203\n",
      "Epoch 9637/30000 Training Loss: 0.08219373226165771\n",
      "Epoch 9638/30000 Training Loss: 0.07338190823793411\n",
      "Epoch 9639/30000 Training Loss: 0.079523004591465\n",
      "Epoch 9640/30000 Training Loss: 0.10600358992815018\n",
      "Epoch 9640/30000 Validation Loss: 0.10027538985013962\n",
      "Epoch 9641/30000 Training Loss: 0.09068789333105087\n",
      "Epoch 9642/30000 Training Loss: 0.06579390913248062\n",
      "Epoch 9643/30000 Training Loss: 0.06775564700365067\n",
      "Epoch 9644/30000 Training Loss: 0.060474295169115067\n",
      "Epoch 9645/30000 Training Loss: 0.1126665249466896\n",
      "Epoch 9646/30000 Training Loss: 0.09250828623771667\n",
      "Epoch 9647/30000 Training Loss: 0.07194238901138306\n",
      "Epoch 9648/30000 Training Loss: 0.08263189345598221\n",
      "Epoch 9649/30000 Training Loss: 0.07910844683647156\n",
      "Epoch 9650/30000 Training Loss: 0.08028734475374222\n",
      "Epoch 9650/30000 Validation Loss: 0.0646987035870552\n",
      "Epoch 9651/30000 Training Loss: 0.08202623575925827\n",
      "Epoch 9652/30000 Training Loss: 0.06949636340141296\n",
      "Epoch 9653/30000 Training Loss: 0.06662456691265106\n",
      "Epoch 9654/30000 Training Loss: 0.07684239745140076\n",
      "Epoch 9655/30000 Training Loss: 0.07792484015226364\n",
      "Epoch 9656/30000 Training Loss: 0.10106285661458969\n",
      "Epoch 9657/30000 Training Loss: 0.08828073740005493\n",
      "Epoch 9658/30000 Training Loss: 0.09205776453018188\n",
      "Epoch 9659/30000 Training Loss: 0.08388802409172058\n",
      "Epoch 9660/30000 Training Loss: 0.06491903215646744\n",
      "Epoch 9660/30000 Validation Loss: 0.07461214810609818\n",
      "Epoch 9661/30000 Training Loss: 0.060813501477241516\n",
      "Epoch 9662/30000 Training Loss: 0.08019376546144485\n",
      "Epoch 9663/30000 Training Loss: 0.08043809980154037\n",
      "Epoch 9664/30000 Training Loss: 0.11943947523832321\n",
      "Epoch 9665/30000 Training Loss: 0.06665199995040894\n",
      "Epoch 9666/30000 Training Loss: 0.10375261306762695\n",
      "Epoch 9667/30000 Training Loss: 0.07660538703203201\n",
      "Epoch 9668/30000 Training Loss: 0.08901068568229675\n",
      "Epoch 9669/30000 Training Loss: 0.07893664389848709\n",
      "Epoch 9670/30000 Training Loss: 0.10006936639547348\n",
      "Epoch 9670/30000 Validation Loss: 0.07680542021989822\n",
      "Epoch 9671/30000 Training Loss: 0.06563621014356613\n",
      "Epoch 9672/30000 Training Loss: 0.06917721033096313\n",
      "Epoch 9673/30000 Training Loss: 0.06781328469514847\n",
      "Epoch 9674/30000 Training Loss: 0.09043499827384949\n",
      "Epoch 9675/30000 Training Loss: 0.08017229288816452\n",
      "Epoch 9676/30000 Training Loss: 0.07826947420835495\n",
      "Epoch 9677/30000 Training Loss: 0.0911422148346901\n",
      "Epoch 9678/30000 Training Loss: 0.0728321447968483\n",
      "Epoch 9679/30000 Training Loss: 0.08937999606132507\n",
      "Epoch 9680/30000 Training Loss: 0.0837143063545227\n",
      "Epoch 9680/30000 Validation Loss: 0.09193160384893417\n",
      "Epoch 9681/30000 Training Loss: 0.0910840705037117\n",
      "Epoch 9682/30000 Training Loss: 0.07287650555372238\n",
      "Epoch 9683/30000 Training Loss: 0.05956896021962166\n",
      "Epoch 9684/30000 Training Loss: 0.07505010813474655\n",
      "Epoch 9685/30000 Training Loss: 0.07625016570091248\n",
      "Epoch 9686/30000 Training Loss: 0.08531267195940018\n",
      "Epoch 9687/30000 Training Loss: 0.08699014782905579\n",
      "Epoch 9688/30000 Training Loss: 0.07097736746072769\n",
      "Epoch 9689/30000 Training Loss: 0.06981897354125977\n",
      "Epoch 9690/30000 Training Loss: 0.07841590791940689\n",
      "Epoch 9690/30000 Validation Loss: 0.0839567556977272\n",
      "Epoch 9691/30000 Training Loss: 0.06421072781085968\n",
      "Epoch 9692/30000 Training Loss: 0.09435955435037613\n",
      "Epoch 9693/30000 Training Loss: 0.0743284747004509\n",
      "Epoch 9694/30000 Training Loss: 0.08301415294408798\n",
      "Epoch 9695/30000 Training Loss: 0.08488734811544418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9696/30000 Training Loss: 0.07101212441921234\n",
      "Epoch 9697/30000 Training Loss: 0.07952114939689636\n",
      "Epoch 9698/30000 Training Loss: 0.07534667104482651\n",
      "Epoch 9699/30000 Training Loss: 0.08908423036336899\n",
      "Epoch 9700/30000 Training Loss: 0.08898556232452393\n",
      "Epoch 9700/30000 Validation Loss: 0.10485293716192245\n",
      "Epoch 9701/30000 Training Loss: 0.10247065871953964\n",
      "Epoch 9702/30000 Training Loss: 0.08137097954750061\n",
      "Epoch 9703/30000 Training Loss: 0.08198971301317215\n",
      "Epoch 9704/30000 Training Loss: 0.08466742187738419\n",
      "Epoch 9705/30000 Training Loss: 0.09391900897026062\n",
      "Epoch 9706/30000 Training Loss: 0.08165933936834335\n",
      "Epoch 9707/30000 Training Loss: 0.0786224827170372\n",
      "Epoch 9708/30000 Training Loss: 0.06976282596588135\n",
      "Epoch 9709/30000 Training Loss: 0.07858613133430481\n",
      "Epoch 9710/30000 Training Loss: 0.06724545359611511\n",
      "Epoch 9710/30000 Validation Loss: 0.07063358277082443\n",
      "Epoch 9711/30000 Training Loss: 0.09341299533843994\n",
      "Epoch 9712/30000 Training Loss: 0.07198391109704971\n",
      "Epoch 9713/30000 Training Loss: 0.08333776146173477\n",
      "Epoch 9714/30000 Training Loss: 0.08199930191040039\n",
      "Epoch 9715/30000 Training Loss: 0.09647003561258316\n",
      "Epoch 9716/30000 Training Loss: 0.0895213782787323\n",
      "Epoch 9717/30000 Training Loss: 0.06654951721429825\n",
      "Epoch 9718/30000 Training Loss: 0.08187392354011536\n",
      "Epoch 9719/30000 Training Loss: 0.0717063844203949\n",
      "Epoch 9720/30000 Training Loss: 0.09102349728345871\n",
      "Epoch 9720/30000 Validation Loss: 0.10037213563919067\n",
      "Epoch 9721/30000 Training Loss: 0.06268961727619171\n",
      "Epoch 9722/30000 Training Loss: 0.08681545406579971\n",
      "Epoch 9723/30000 Training Loss: 0.09661856293678284\n",
      "Epoch 9724/30000 Training Loss: 0.0693737268447876\n",
      "Epoch 9725/30000 Training Loss: 0.07308345288038254\n",
      "Epoch 9726/30000 Training Loss: 0.10139768570661545\n",
      "Epoch 9727/30000 Training Loss: 0.0757024958729744\n",
      "Epoch 9728/30000 Training Loss: 0.061105549335479736\n",
      "Epoch 9729/30000 Training Loss: 0.0803077444434166\n",
      "Epoch 9730/30000 Training Loss: 0.07546795159578323\n",
      "Epoch 9730/30000 Validation Loss: 0.10057524591684341\n",
      "Epoch 9731/30000 Training Loss: 0.10629162937402725\n",
      "Epoch 9732/30000 Training Loss: 0.08512266725301743\n",
      "Epoch 9733/30000 Training Loss: 0.10659108310937881\n",
      "Epoch 9734/30000 Training Loss: 0.09580698609352112\n",
      "Epoch 9735/30000 Training Loss: 0.08190639317035675\n",
      "Epoch 9736/30000 Training Loss: 0.09027545899152756\n",
      "Epoch 9737/30000 Training Loss: 0.07657886296510696\n",
      "Epoch 9738/30000 Training Loss: 0.07317826896905899\n",
      "Epoch 9739/30000 Training Loss: 0.09281420707702637\n",
      "Epoch 9740/30000 Training Loss: 0.07852238416671753\n",
      "Epoch 9740/30000 Validation Loss: 0.08851978927850723\n",
      "Epoch 9741/30000 Training Loss: 0.09543605893850327\n",
      "Epoch 9742/30000 Training Loss: 0.08917653560638428\n",
      "Epoch 9743/30000 Training Loss: 0.07492265105247498\n",
      "Epoch 9744/30000 Training Loss: 0.08355750888586044\n",
      "Epoch 9745/30000 Training Loss: 0.08352821320295334\n",
      "Epoch 9746/30000 Training Loss: 0.06747132539749146\n",
      "Epoch 9747/30000 Training Loss: 0.06800325959920883\n",
      "Epoch 9748/30000 Training Loss: 0.07939010113477707\n",
      "Epoch 9749/30000 Training Loss: 0.10635894536972046\n",
      "Epoch 9750/30000 Training Loss: 0.09752070158720016\n",
      "Epoch 9750/30000 Validation Loss: 0.08863559365272522\n",
      "Epoch 9751/30000 Training Loss: 0.10079097002744675\n",
      "Epoch 9752/30000 Training Loss: 0.08587995171546936\n",
      "Epoch 9753/30000 Training Loss: 0.07100339978933334\n",
      "Epoch 9754/30000 Training Loss: 0.08281811326742172\n",
      "Epoch 9755/30000 Training Loss: 0.10193783044815063\n",
      "Epoch 9756/30000 Training Loss: 0.07545221596956253\n",
      "Epoch 9757/30000 Training Loss: 0.06314386427402496\n",
      "Epoch 9758/30000 Training Loss: 0.08872338384389877\n",
      "Epoch 9759/30000 Training Loss: 0.07746883481740952\n",
      "Epoch 9760/30000 Training Loss: 0.08133799582719803\n",
      "Epoch 9760/30000 Validation Loss: 0.0715857520699501\n",
      "Epoch 9761/30000 Training Loss: 0.07809848338365555\n",
      "Epoch 9762/30000 Training Loss: 0.0622524619102478\n",
      "Epoch 9763/30000 Training Loss: 0.10745557397603989\n",
      "Epoch 9764/30000 Training Loss: 0.0787128433585167\n",
      "Epoch 9765/30000 Training Loss: 0.09857814759016037\n",
      "Epoch 9766/30000 Training Loss: 0.07711175084114075\n",
      "Epoch 9767/30000 Training Loss: 0.06984924525022507\n",
      "Epoch 9768/30000 Training Loss: 0.10192373394966125\n",
      "Epoch 9769/30000 Training Loss: 0.08564694970846176\n",
      "Epoch 9770/30000 Training Loss: 0.08382287621498108\n",
      "Epoch 9770/30000 Validation Loss: 0.08475646376609802\n",
      "Epoch 9771/30000 Training Loss: 0.06982862949371338\n",
      "Epoch 9772/30000 Training Loss: 0.07519000768661499\n",
      "Epoch 9773/30000 Training Loss: 0.07994583994150162\n",
      "Epoch 9774/30000 Training Loss: 0.0830930545926094\n",
      "Epoch 9775/30000 Training Loss: 0.08461270481348038\n",
      "Epoch 9776/30000 Training Loss: 0.07324089854955673\n",
      "Epoch 9777/30000 Training Loss: 0.08664213865995407\n",
      "Epoch 9778/30000 Training Loss: 0.06315261870622635\n",
      "Epoch 9779/30000 Training Loss: 0.10298451036214828\n",
      "Epoch 9780/30000 Training Loss: 0.09156704694032669\n",
      "Epoch 9780/30000 Validation Loss: 0.08789420127868652\n",
      "Epoch 9781/30000 Training Loss: 0.0678255558013916\n",
      "Epoch 9782/30000 Training Loss: 0.07740310579538345\n",
      "Epoch 9783/30000 Training Loss: 0.08075772225856781\n",
      "Epoch 9784/30000 Training Loss: 0.09163886308670044\n",
      "Epoch 9785/30000 Training Loss: 0.07766575366258621\n",
      "Epoch 9786/30000 Training Loss: 0.08037599921226501\n",
      "Epoch 9787/30000 Training Loss: 0.08582395315170288\n",
      "Epoch 9788/30000 Training Loss: 0.10257985442876816\n",
      "Epoch 9789/30000 Training Loss: 0.08011689782142639\n",
      "Epoch 9790/30000 Training Loss: 0.06567536294460297\n",
      "Epoch 9790/30000 Validation Loss: 0.09075695276260376\n",
      "Epoch 9791/30000 Training Loss: 0.0744762197136879\n",
      "Epoch 9792/30000 Training Loss: 0.0778888538479805\n",
      "Epoch 9793/30000 Training Loss: 0.08209285140037537\n",
      "Epoch 9794/30000 Training Loss: 0.0745701715350151\n",
      "Epoch 9795/30000 Training Loss: 0.08136159926652908\n",
      "Epoch 9796/30000 Training Loss: 0.08198866248130798\n",
      "Epoch 9797/30000 Training Loss: 0.07941842824220657\n",
      "Epoch 9798/30000 Training Loss: 0.08424591273069382\n",
      "Epoch 9799/30000 Training Loss: 0.10040105134248734\n",
      "Epoch 9800/30000 Training Loss: 0.07533257454633713\n",
      "Epoch 9800/30000 Validation Loss: 0.09454929828643799\n",
      "Epoch 9801/30000 Training Loss: 0.08606916666030884\n",
      "Epoch 9802/30000 Training Loss: 0.07890015840530396\n",
      "Epoch 9803/30000 Training Loss: 0.06219809129834175\n",
      "Epoch 9804/30000 Training Loss: 0.08715546876192093\n",
      "Epoch 9805/30000 Training Loss: 0.11810353398323059\n",
      "Epoch 9806/30000 Training Loss: 0.07130017131567001\n",
      "Epoch 9807/30000 Training Loss: 0.06765703111886978\n",
      "Epoch 9808/30000 Training Loss: 0.08262815326452255\n",
      "Epoch 9809/30000 Training Loss: 0.06913183629512787\n",
      "Epoch 9810/30000 Training Loss: 0.08027429133653641\n",
      "Epoch 9810/30000 Validation Loss: 0.09637104719877243\n",
      "Epoch 9811/30000 Training Loss: 0.10046315938234329\n",
      "Epoch 9812/30000 Training Loss: 0.08839604258537292\n",
      "Epoch 9813/30000 Training Loss: 0.10194117575883865\n",
      "Epoch 9814/30000 Training Loss: 0.07538344711065292\n",
      "Epoch 9815/30000 Training Loss: 0.09088518470525742\n",
      "Epoch 9816/30000 Training Loss: 0.07921843230724335\n",
      "Epoch 9817/30000 Training Loss: 0.07158125191926956\n",
      "Epoch 9818/30000 Training Loss: 0.060664668679237366\n",
      "Epoch 9819/30000 Training Loss: 0.09225118905305862\n",
      "Epoch 9820/30000 Training Loss: 0.08942123502492905\n",
      "Epoch 9820/30000 Validation Loss: 0.10907244682312012\n",
      "Epoch 9821/30000 Training Loss: 0.0977252796292305\n",
      "Epoch 9822/30000 Training Loss: 0.07009532302618027\n",
      "Epoch 9823/30000 Training Loss: 0.07682035118341446\n",
      "Epoch 9824/30000 Training Loss: 0.07165191322565079\n",
      "Epoch 9825/30000 Training Loss: 0.06954813003540039\n",
      "Epoch 9826/30000 Training Loss: 0.08346423506736755\n",
      "Epoch 9827/30000 Training Loss: 0.11276056617498398\n",
      "Epoch 9828/30000 Training Loss: 0.08906963467597961\n",
      "Epoch 9829/30000 Training Loss: 0.06914013624191284\n",
      "Epoch 9830/30000 Training Loss: 0.07665873318910599\n",
      "Epoch 9830/30000 Validation Loss: 0.07599825412034988\n",
      "Epoch 9831/30000 Training Loss: 0.07981204241514206\n",
      "Epoch 9832/30000 Training Loss: 0.07872148603200912\n",
      "Epoch 9833/30000 Training Loss: 0.08924870938062668\n",
      "Epoch 9834/30000 Training Loss: 0.07885152101516724\n",
      "Epoch 9835/30000 Training Loss: 0.06951908022165298\n",
      "Epoch 9836/30000 Training Loss: 0.11240709573030472\n",
      "Epoch 9837/30000 Training Loss: 0.09436658769845963\n",
      "Epoch 9838/30000 Training Loss: 0.07119422405958176\n",
      "Epoch 9839/30000 Training Loss: 0.12623946368694305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9840/30000 Training Loss: 0.06555952876806259\n",
      "Epoch 9840/30000 Validation Loss: 0.08954022079706192\n",
      "Epoch 9841/30000 Training Loss: 0.06340628117322922\n",
      "Epoch 9842/30000 Training Loss: 0.08107026666402817\n",
      "Epoch 9843/30000 Training Loss: 0.09495029598474503\n",
      "Epoch 9844/30000 Training Loss: 0.08302422612905502\n",
      "Epoch 9845/30000 Training Loss: 0.06340241432189941\n",
      "Epoch 9846/30000 Training Loss: 0.09846776723861694\n",
      "Epoch 9847/30000 Training Loss: 0.0871959701180458\n",
      "Epoch 9848/30000 Training Loss: 0.07807743549346924\n",
      "Epoch 9849/30000 Training Loss: 0.07527267187833786\n",
      "Epoch 9850/30000 Training Loss: 0.06995564699172974\n",
      "Epoch 9850/30000 Validation Loss: 0.07368158549070358\n",
      "Epoch 9851/30000 Training Loss: 0.07485634088516235\n",
      "Epoch 9852/30000 Training Loss: 0.08173730224370956\n",
      "Epoch 9853/30000 Training Loss: 0.07529130578041077\n",
      "Epoch 9854/30000 Training Loss: 0.08233609795570374\n",
      "Epoch 9855/30000 Training Loss: 0.09244334697723389\n",
      "Epoch 9856/30000 Training Loss: 0.09153888374567032\n",
      "Epoch 9857/30000 Training Loss: 0.09182000160217285\n",
      "Epoch 9858/30000 Training Loss: 0.09408381581306458\n",
      "Epoch 9859/30000 Training Loss: 0.0927278995513916\n",
      "Epoch 9860/30000 Training Loss: 0.07948998361825943\n",
      "Epoch 9860/30000 Validation Loss: 0.08720799535512924\n",
      "Epoch 9861/30000 Training Loss: 0.07902307063341141\n",
      "Epoch 9862/30000 Training Loss: 0.10875660926103592\n",
      "Epoch 9863/30000 Training Loss: 0.09293434768915176\n",
      "Epoch 9864/30000 Training Loss: 0.07706066966056824\n",
      "Epoch 9865/30000 Training Loss: 0.06932113319635391\n",
      "Epoch 9866/30000 Training Loss: 0.0642385184764862\n",
      "Epoch 9867/30000 Training Loss: 0.1015104427933693\n",
      "Epoch 9868/30000 Training Loss: 0.07070595771074295\n",
      "Epoch 9869/30000 Training Loss: 0.07329025119543076\n",
      "Epoch 9870/30000 Training Loss: 0.07704045623540878\n",
      "Epoch 9870/30000 Validation Loss: 0.08645462989807129\n",
      "Epoch 9871/30000 Training Loss: 0.07404641062021255\n",
      "Epoch 9872/30000 Training Loss: 0.082838274538517\n",
      "Epoch 9873/30000 Training Loss: 0.09906717389822006\n",
      "Epoch 9874/30000 Training Loss: 0.07823548465967178\n",
      "Epoch 9875/30000 Training Loss: 0.10104823112487793\n",
      "Epoch 9876/30000 Training Loss: 0.07187343388795853\n",
      "Epoch 9877/30000 Training Loss: 0.07868916541337967\n",
      "Epoch 9878/30000 Training Loss: 0.06071673706173897\n",
      "Epoch 9879/30000 Training Loss: 0.05579349026083946\n",
      "Epoch 9880/30000 Training Loss: 0.07616820186376572\n",
      "Epoch 9880/30000 Validation Loss: 0.09392016381025314\n",
      "Epoch 9881/30000 Training Loss: 0.10997895151376724\n",
      "Epoch 9882/30000 Training Loss: 0.06553700566291809\n",
      "Epoch 9883/30000 Training Loss: 0.07879122346639633\n",
      "Epoch 9884/30000 Training Loss: 0.08339041471481323\n",
      "Epoch 9885/30000 Training Loss: 0.09111558645963669\n",
      "Epoch 9886/30000 Training Loss: 0.10027410835027695\n",
      "Epoch 9887/30000 Training Loss: 0.09261992573738098\n",
      "Epoch 9888/30000 Training Loss: 0.08945801854133606\n",
      "Epoch 9889/30000 Training Loss: 0.06871509552001953\n",
      "Epoch 9890/30000 Training Loss: 0.08650685101747513\n",
      "Epoch 9890/30000 Validation Loss: 0.06741916388273239\n",
      "Epoch 9891/30000 Training Loss: 0.07593416422605515\n",
      "Epoch 9892/30000 Training Loss: 0.06744057685136795\n",
      "Epoch 9893/30000 Training Loss: 0.0789182111620903\n",
      "Epoch 9894/30000 Training Loss: 0.10471135377883911\n",
      "Epoch 9895/30000 Training Loss: 0.07415974885225296\n",
      "Epoch 9896/30000 Training Loss: 0.08043093234300613\n",
      "Epoch 9897/30000 Training Loss: 0.07919330149888992\n",
      "Epoch 9898/30000 Training Loss: 0.06774713844060898\n",
      "Epoch 9899/30000 Training Loss: 0.07435139268636703\n",
      "Epoch 9900/30000 Training Loss: 0.07589603215456009\n",
      "Epoch 9900/30000 Validation Loss: 0.07377908378839493\n",
      "Epoch 9901/30000 Training Loss: 0.09238901734352112\n",
      "Epoch 9902/30000 Training Loss: 0.09178508073091507\n",
      "Epoch 9903/30000 Training Loss: 0.07481292635202408\n",
      "Epoch 9904/30000 Training Loss: 0.08500238507986069\n",
      "Epoch 9905/30000 Training Loss: 0.07884568721055984\n",
      "Epoch 9906/30000 Training Loss: 0.09963781386613846\n",
      "Epoch 9907/30000 Training Loss: 0.09361525624990463\n",
      "Epoch 9908/30000 Training Loss: 0.08023568242788315\n",
      "Epoch 9909/30000 Training Loss: 0.06323655694723129\n",
      "Epoch 9910/30000 Training Loss: 0.08480168133974075\n",
      "Epoch 9910/30000 Validation Loss: 0.08405400067567825\n",
      "Epoch 9911/30000 Training Loss: 0.07242602854967117\n",
      "Epoch 9912/30000 Training Loss: 0.10341920703649521\n",
      "Epoch 9913/30000 Training Loss: 0.07802731543779373\n",
      "Epoch 9914/30000 Training Loss: 0.0705823078751564\n",
      "Epoch 9915/30000 Training Loss: 0.09152790158987045\n",
      "Epoch 9916/30000 Training Loss: 0.0811317190527916\n",
      "Epoch 9917/30000 Training Loss: 0.06788437813520432\n",
      "Epoch 9918/30000 Training Loss: 0.07045839726924896\n",
      "Epoch 9919/30000 Training Loss: 0.09190545231103897\n",
      "Epoch 9920/30000 Training Loss: 0.0726543739438057\n",
      "Epoch 9920/30000 Validation Loss: 0.09901949763298035\n",
      "Epoch 9921/30000 Training Loss: 0.08501731604337692\n",
      "Epoch 9922/30000 Training Loss: 0.09925228357315063\n",
      "Epoch 9923/30000 Training Loss: 0.09159598499536514\n",
      "Epoch 9924/30000 Training Loss: 0.07964440435171127\n",
      "Epoch 9925/30000 Training Loss: 0.10237342119216919\n",
      "Epoch 9926/30000 Training Loss: 0.08302753418684006\n",
      "Epoch 9927/30000 Training Loss: 0.09490355849266052\n",
      "Epoch 9928/30000 Training Loss: 0.08058685064315796\n",
      "Epoch 9929/30000 Training Loss: 0.09515371173620224\n",
      "Epoch 9930/30000 Training Loss: 0.09448911994695663\n",
      "Epoch 9930/30000 Validation Loss: 0.0828191339969635\n",
      "Epoch 9931/30000 Training Loss: 0.07281913608312607\n",
      "Epoch 9932/30000 Training Loss: 0.07025264948606491\n",
      "Epoch 9933/30000 Training Loss: 0.07594440877437592\n",
      "Epoch 9934/30000 Training Loss: 0.08435233682394028\n",
      "Epoch 9935/30000 Training Loss: 0.10722558945417404\n",
      "Epoch 9936/30000 Training Loss: 0.08894086629152298\n",
      "Epoch 9937/30000 Training Loss: 0.07538973540067673\n",
      "Epoch 9938/30000 Training Loss: 0.08729436993598938\n",
      "Epoch 9939/30000 Training Loss: 0.07828599214553833\n",
      "Epoch 9940/30000 Training Loss: 0.0908723697066307\n",
      "Epoch 9940/30000 Validation Loss: 0.07853177934885025\n",
      "Epoch 9941/30000 Training Loss: 0.07951343059539795\n",
      "Epoch 9942/30000 Training Loss: 0.08932825922966003\n",
      "Epoch 9943/30000 Training Loss: 0.06642180681228638\n",
      "Epoch 9944/30000 Training Loss: 0.08556467294692993\n",
      "Epoch 9945/30000 Training Loss: 0.08592889457941055\n",
      "Epoch 9946/30000 Training Loss: 0.0899655893445015\n",
      "Epoch 9947/30000 Training Loss: 0.06604611873626709\n",
      "Epoch 9948/30000 Training Loss: 0.09971779584884644\n",
      "Epoch 9949/30000 Training Loss: 0.09169428795576096\n",
      "Epoch 9950/30000 Training Loss: 0.08596725016832352\n",
      "Epoch 9950/30000 Validation Loss: 0.08113471418619156\n",
      "Epoch 9951/30000 Training Loss: 0.08588265627622604\n",
      "Epoch 9952/30000 Training Loss: 0.08454331755638123\n",
      "Epoch 9953/30000 Training Loss: 0.08859586715698242\n",
      "Epoch 9954/30000 Training Loss: 0.06649138778448105\n",
      "Epoch 9955/30000 Training Loss: 0.0848555862903595\n",
      "Epoch 9956/30000 Training Loss: 0.07626331597566605\n",
      "Epoch 9957/30000 Training Loss: 0.08128246665000916\n",
      "Epoch 9958/30000 Training Loss: 0.06843169778585434\n",
      "Epoch 9959/30000 Training Loss: 0.06284987181425095\n",
      "Epoch 9960/30000 Training Loss: 0.07631704211235046\n",
      "Epoch 9960/30000 Validation Loss: 0.08439135551452637\n",
      "Epoch 9961/30000 Training Loss: 0.08536180108785629\n",
      "Epoch 9962/30000 Training Loss: 0.0947251096367836\n",
      "Epoch 9963/30000 Training Loss: 0.0878899097442627\n",
      "Epoch 9964/30000 Training Loss: 0.09771255403757095\n",
      "Epoch 9965/30000 Training Loss: 0.06919343024492264\n",
      "Epoch 9966/30000 Training Loss: 0.06054891645908356\n",
      "Epoch 9967/30000 Training Loss: 0.08106916397809982\n",
      "Epoch 9968/30000 Training Loss: 0.056583959609270096\n",
      "Epoch 9969/30000 Training Loss: 0.07557502388954163\n",
      "Epoch 9970/30000 Training Loss: 0.07938605546951294\n",
      "Epoch 9970/30000 Validation Loss: 0.07569209486246109\n",
      "Epoch 9971/30000 Training Loss: 0.07076769322156906\n",
      "Epoch 9972/30000 Training Loss: 0.10187145322561264\n",
      "Epoch 9973/30000 Training Loss: 0.08553911000490189\n",
      "Epoch 9974/30000 Training Loss: 0.07357869297266006\n",
      "Epoch 9975/30000 Training Loss: 0.07692807167768478\n",
      "Epoch 9976/30000 Training Loss: 0.07902543991804123\n",
      "Epoch 9977/30000 Training Loss: 0.07194116711616516\n",
      "Epoch 9978/30000 Training Loss: 0.06884946674108505\n",
      "Epoch 9979/30000 Training Loss: 0.0780516192317009\n",
      "Epoch 9980/30000 Training Loss: 0.09071147441864014\n",
      "Epoch 9980/30000 Validation Loss: 0.07579437643289566\n",
      "Epoch 9981/30000 Training Loss: 0.06857376545667648\n",
      "Epoch 9982/30000 Training Loss: 0.09101512283086777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9983/30000 Training Loss: 0.09123478084802628\n",
      "Epoch 9984/30000 Training Loss: 0.09108570963144302\n",
      "Epoch 9985/30000 Training Loss: 0.06587303429841995\n",
      "Epoch 9986/30000 Training Loss: 0.08042622357606888\n",
      "Epoch 9987/30000 Training Loss: 0.10011201351881027\n",
      "Epoch 9988/30000 Training Loss: 0.07998337596654892\n",
      "Epoch 9989/30000 Training Loss: 0.08100739866495132\n",
      "Epoch 9990/30000 Training Loss: 0.08855199068784714\n",
      "Epoch 9990/30000 Validation Loss: 0.07685593515634537\n",
      "Epoch 9991/30000 Training Loss: 0.0768648013472557\n",
      "Epoch 9992/30000 Training Loss: 0.07098424434661865\n",
      "Epoch 9993/30000 Training Loss: 0.07190022617578506\n",
      "Epoch 9994/30000 Training Loss: 0.0947018638253212\n",
      "Epoch 9995/30000 Training Loss: 0.08207996934652328\n",
      "Epoch 9996/30000 Training Loss: 0.09171772003173828\n",
      "Epoch 9997/30000 Training Loss: 0.10296116024255753\n",
      "Epoch 9998/30000 Training Loss: 0.06621882319450378\n",
      "Epoch 9999/30000 Training Loss: 0.09005284309387207\n",
      "Epoch 10000/30000 Training Loss: 0.0636347308754921\n",
      "Epoch 10000/30000 Validation Loss: 0.08119095861911774\n",
      "Epoch 10001/30000 Training Loss: 0.08465737104415894\n",
      "Epoch 10002/30000 Training Loss: 0.08519604057073593\n",
      "Epoch 10003/30000 Training Loss: 0.08064208924770355\n",
      "Epoch 10004/30000 Training Loss: 0.08949124813079834\n",
      "Epoch 10005/30000 Training Loss: 0.08216836303472519\n",
      "Epoch 10006/30000 Training Loss: 0.07558993250131607\n",
      "Epoch 10007/30000 Training Loss: 0.08066966384649277\n",
      "Epoch 10008/30000 Training Loss: 0.08777951449155807\n",
      "Epoch 10009/30000 Training Loss: 0.06954970210790634\n",
      "Epoch 10010/30000 Training Loss: 0.07704022526741028\n",
      "Epoch 10010/30000 Validation Loss: 0.10333409160375595\n",
      "Epoch 10011/30000 Training Loss: 0.07897809147834778\n",
      "Epoch 10012/30000 Training Loss: 0.0920248031616211\n",
      "Epoch 10013/30000 Training Loss: 0.06773213297128677\n",
      "Epoch 10014/30000 Training Loss: 0.08220630139112473\n",
      "Epoch 10015/30000 Training Loss: 0.08343484252691269\n",
      "Epoch 10016/30000 Training Loss: 0.06349802762269974\n",
      "Epoch 10017/30000 Training Loss: 0.07746043056249619\n",
      "Epoch 10018/30000 Training Loss: 0.10854441672563553\n",
      "Epoch 10019/30000 Training Loss: 0.08592679351568222\n",
      "Epoch 10020/30000 Training Loss: 0.06651204079389572\n",
      "Epoch 10020/30000 Validation Loss: 0.08163020759820938\n",
      "Epoch 10021/30000 Training Loss: 0.06829383224248886\n",
      "Epoch 10022/30000 Training Loss: 0.07056034356355667\n",
      "Epoch 10023/30000 Training Loss: 0.08802143484354019\n",
      "Epoch 10024/30000 Training Loss: 0.08416646718978882\n",
      "Epoch 10025/30000 Training Loss: 0.08212798833847046\n",
      "Epoch 10026/30000 Training Loss: 0.08823081105947495\n",
      "Epoch 10027/30000 Training Loss: 0.0888884887099266\n",
      "Epoch 10028/30000 Training Loss: 0.07394319027662277\n",
      "Epoch 10029/30000 Training Loss: 0.08192535489797592\n",
      "Epoch 10030/30000 Training Loss: 0.07446619868278503\n",
      "Epoch 10030/30000 Validation Loss: 0.07942060381174088\n",
      "Epoch 10031/30000 Training Loss: 0.07878756523132324\n",
      "Epoch 10032/30000 Training Loss: 0.08186370879411697\n",
      "Epoch 10033/30000 Training Loss: 0.08999096602201462\n",
      "Epoch 10034/30000 Training Loss: 0.07383120805025101\n",
      "Epoch 10035/30000 Training Loss: 0.06886950135231018\n",
      "Epoch 10036/30000 Training Loss: 0.06350678205490112\n",
      "Epoch 10037/30000 Training Loss: 0.07923349738121033\n",
      "Epoch 10038/30000 Training Loss: 0.08409237861633301\n",
      "Epoch 10039/30000 Training Loss: 0.08664602786302567\n",
      "Epoch 10040/30000 Training Loss: 0.07311972975730896\n",
      "Epoch 10040/30000 Validation Loss: 0.08604269474744797\n",
      "Epoch 10041/30000 Training Loss: 0.07392943650484085\n",
      "Epoch 10042/30000 Training Loss: 0.09789308160543442\n",
      "Epoch 10043/30000 Training Loss: 0.10039738565683365\n",
      "Epoch 10044/30000 Training Loss: 0.07310449331998825\n",
      "Epoch 10045/30000 Training Loss: 0.09984692931175232\n",
      "Epoch 10046/30000 Training Loss: 0.07539752870798111\n",
      "Epoch 10047/30000 Training Loss: 0.09621719270944595\n",
      "Epoch 10048/30000 Training Loss: 0.10214299708604813\n",
      "Epoch 10049/30000 Training Loss: 0.06838668137788773\n",
      "Epoch 10050/30000 Training Loss: 0.07148490101099014\n",
      "Epoch 10050/30000 Validation Loss: 0.06584390997886658\n",
      "Epoch 10051/30000 Training Loss: 0.07542674988508224\n",
      "Epoch 10052/30000 Training Loss: 0.09271936863660812\n",
      "Epoch 10053/30000 Training Loss: 0.06415435671806335\n",
      "Epoch 10054/30000 Training Loss: 0.08637560158967972\n",
      "Epoch 10055/30000 Training Loss: 0.07993599027395248\n",
      "Epoch 10056/30000 Training Loss: 0.07523614913225174\n",
      "Epoch 10057/30000 Training Loss: 0.11400023102760315\n",
      "Epoch 10058/30000 Training Loss: 0.08290360867977142\n",
      "Epoch 10059/30000 Training Loss: 0.06696673482656479\n",
      "Epoch 10060/30000 Training Loss: 0.08523109555244446\n",
      "Epoch 10060/30000 Validation Loss: 0.08641795068979263\n",
      "Epoch 10061/30000 Training Loss: 0.08467316627502441\n",
      "Epoch 10062/30000 Training Loss: 0.10537710785865784\n",
      "Epoch 10063/30000 Training Loss: 0.09164469689130783\n",
      "Epoch 10064/30000 Training Loss: 0.09050309658050537\n",
      "Epoch 10065/30000 Training Loss: 0.08609818667173386\n",
      "Epoch 10066/30000 Training Loss: 0.08010213822126389\n",
      "Epoch 10067/30000 Training Loss: 0.08912289142608643\n",
      "Epoch 10068/30000 Training Loss: 0.09106453508138657\n",
      "Epoch 10069/30000 Training Loss: 0.07097932696342468\n",
      "Epoch 10070/30000 Training Loss: 0.08407007902860641\n",
      "Epoch 10070/30000 Validation Loss: 0.0638330802321434\n",
      "Epoch 10071/30000 Training Loss: 0.08412515372037888\n",
      "Epoch 10072/30000 Training Loss: 0.0669718086719513\n",
      "Epoch 10073/30000 Training Loss: 0.08407875150442123\n",
      "Epoch 10074/30000 Training Loss: 0.10145726054906845\n",
      "Epoch 10075/30000 Training Loss: 0.0769244059920311\n",
      "Epoch 10076/30000 Training Loss: 0.09031063318252563\n",
      "Epoch 10077/30000 Training Loss: 0.08560050278902054\n",
      "Epoch 10078/30000 Training Loss: 0.09753639250993729\n",
      "Epoch 10079/30000 Training Loss: 0.07775149494409561\n",
      "Epoch 10080/30000 Training Loss: 0.0609896294772625\n",
      "Epoch 10080/30000 Validation Loss: 0.09706046432256699\n",
      "Epoch 10081/30000 Training Loss: 0.08236996084451675\n",
      "Epoch 10082/30000 Training Loss: 0.09156603366136551\n",
      "Epoch 10083/30000 Training Loss: 0.07194680720567703\n",
      "Epoch 10084/30000 Training Loss: 0.08649299293756485\n",
      "Epoch 10085/30000 Training Loss: 0.07691452652215958\n",
      "Epoch 10086/30000 Training Loss: 0.08206046372652054\n",
      "Epoch 10087/30000 Training Loss: 0.10166025906801224\n",
      "Epoch 10088/30000 Training Loss: 0.07483423501253128\n",
      "Epoch 10089/30000 Training Loss: 0.06504296511411667\n",
      "Epoch 10090/30000 Training Loss: 0.08265955001115799\n",
      "Epoch 10090/30000 Validation Loss: 0.0843886137008667\n",
      "Epoch 10091/30000 Training Loss: 0.0733073428273201\n",
      "Epoch 10092/30000 Training Loss: 0.06810154765844345\n",
      "Epoch 10093/30000 Training Loss: 0.09017827361822128\n",
      "Epoch 10094/30000 Training Loss: 0.06960277259349823\n",
      "Epoch 10095/30000 Training Loss: 0.08787649869918823\n",
      "Epoch 10096/30000 Training Loss: 0.07392679899930954\n",
      "Epoch 10097/30000 Training Loss: 0.07860889285802841\n",
      "Epoch 10098/30000 Training Loss: 0.07985600084066391\n",
      "Epoch 10099/30000 Training Loss: 0.07124733179807663\n",
      "Epoch 10100/30000 Training Loss: 0.07190900295972824\n",
      "Epoch 10100/30000 Validation Loss: 0.078000009059906\n",
      "Epoch 10101/30000 Training Loss: 0.10413846373558044\n",
      "Epoch 10102/30000 Training Loss: 0.07407152652740479\n",
      "Epoch 10103/30000 Training Loss: 0.10777729749679565\n",
      "Epoch 10104/30000 Training Loss: 0.06473488360643387\n",
      "Epoch 10105/30000 Training Loss: 0.08767489343881607\n",
      "Epoch 10106/30000 Training Loss: 0.07676814496517181\n",
      "Epoch 10107/30000 Training Loss: 0.06539707630872726\n",
      "Epoch 10108/30000 Training Loss: 0.07366397231817245\n",
      "Epoch 10109/30000 Training Loss: 0.06680341809988022\n",
      "Epoch 10110/30000 Training Loss: 0.07150513678789139\n",
      "Epoch 10110/30000 Validation Loss: 0.0782599076628685\n",
      "Epoch 10111/30000 Training Loss: 0.08922326564788818\n",
      "Epoch 10112/30000 Training Loss: 0.08207955956459045\n",
      "Epoch 10113/30000 Training Loss: 0.08188483864068985\n",
      "Epoch 10114/30000 Training Loss: 0.06538531929254532\n",
      "Epoch 10115/30000 Training Loss: 0.10010519623756409\n",
      "Epoch 10116/30000 Training Loss: 0.07766333967447281\n",
      "Epoch 10117/30000 Training Loss: 0.07032104581594467\n",
      "Epoch 10118/30000 Training Loss: 0.08072221279144287\n",
      "Epoch 10119/30000 Training Loss: 0.07547812908887863\n",
      "Epoch 10120/30000 Training Loss: 0.09400755167007446\n",
      "Epoch 10120/30000 Validation Loss: 0.08096648752689362\n",
      "Epoch 10121/30000 Training Loss: 0.07713129371404648\n",
      "Epoch 10122/30000 Training Loss: 0.10369843989610672\n",
      "Epoch 10123/30000 Training Loss: 0.07458090037107468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10124/30000 Training Loss: 0.06914781779050827\n",
      "Epoch 10125/30000 Training Loss: 0.092609703540802\n",
      "Epoch 10126/30000 Training Loss: 0.08519283682107925\n",
      "Epoch 10127/30000 Training Loss: 0.0740557387471199\n",
      "Epoch 10128/30000 Training Loss: 0.058616768568754196\n",
      "Epoch 10129/30000 Training Loss: 0.08110376447439194\n",
      "Epoch 10130/30000 Training Loss: 0.08273085206747055\n",
      "Epoch 10130/30000 Validation Loss: 0.06429433077573776\n",
      "Epoch 10131/30000 Training Loss: 0.09273060411214828\n",
      "Epoch 10132/30000 Training Loss: 0.07419418543577194\n",
      "Epoch 10133/30000 Training Loss: 0.07479707151651382\n",
      "Epoch 10134/30000 Training Loss: 0.10164451599121094\n",
      "Epoch 10135/30000 Training Loss: 0.07761816680431366\n",
      "Epoch 10136/30000 Training Loss: 0.06551437824964523\n",
      "Epoch 10137/30000 Training Loss: 0.06824053078889847\n",
      "Epoch 10138/30000 Training Loss: 0.07291238009929657\n",
      "Epoch 10139/30000 Training Loss: 0.07388269901275635\n",
      "Epoch 10140/30000 Training Loss: 0.06929371505975723\n",
      "Epoch 10140/30000 Validation Loss: 0.07776960730552673\n",
      "Epoch 10141/30000 Training Loss: 0.06932352483272552\n",
      "Epoch 10142/30000 Training Loss: 0.07617565989494324\n",
      "Epoch 10143/30000 Training Loss: 0.07991316169500351\n",
      "Epoch 10144/30000 Training Loss: 0.07043042033910751\n",
      "Epoch 10145/30000 Training Loss: 0.08179015666246414\n",
      "Epoch 10146/30000 Training Loss: 0.09299091249704361\n",
      "Epoch 10147/30000 Training Loss: 0.08832520246505737\n",
      "Epoch 10148/30000 Training Loss: 0.07923004776239395\n",
      "Epoch 10149/30000 Training Loss: 0.10108057409524918\n",
      "Epoch 10150/30000 Training Loss: 0.05999879166483879\n",
      "Epoch 10150/30000 Validation Loss: 0.07917368412017822\n",
      "Epoch 10151/30000 Training Loss: 0.08420487493276596\n",
      "Epoch 10152/30000 Training Loss: 0.08374899625778198\n",
      "Epoch 10153/30000 Training Loss: 0.08206091821193695\n",
      "Epoch 10154/30000 Training Loss: 0.06824769824743271\n",
      "Epoch 10155/30000 Training Loss: 0.0878472700715065\n",
      "Epoch 10156/30000 Training Loss: 0.07392808049917221\n",
      "Epoch 10157/30000 Training Loss: 0.0865090861916542\n",
      "Epoch 10158/30000 Training Loss: 0.07251744717359543\n",
      "Epoch 10159/30000 Training Loss: 0.06420192122459412\n",
      "Epoch 10160/30000 Training Loss: 0.07099635899066925\n",
      "Epoch 10160/30000 Validation Loss: 0.07408557087182999\n",
      "Epoch 10161/30000 Training Loss: 0.10386393219232559\n",
      "Epoch 10162/30000 Training Loss: 0.09736869484186172\n",
      "Epoch 10163/30000 Training Loss: 0.08094855397939682\n",
      "Epoch 10164/30000 Training Loss: 0.08898237347602844\n",
      "Epoch 10165/30000 Training Loss: 0.07025326043367386\n",
      "Epoch 10166/30000 Training Loss: 0.09339794516563416\n",
      "Epoch 10167/30000 Training Loss: 0.07351849228143692\n",
      "Epoch 10168/30000 Training Loss: 0.07956954091787338\n",
      "Epoch 10169/30000 Training Loss: 0.07933476567268372\n",
      "Epoch 10170/30000 Training Loss: 0.09578463435173035\n",
      "Epoch 10170/30000 Validation Loss: 0.07347377389669418\n",
      "Epoch 10171/30000 Training Loss: 0.09234470874071121\n",
      "Epoch 10172/30000 Training Loss: 0.07053163647651672\n",
      "Epoch 10173/30000 Training Loss: 0.07288656383752823\n",
      "Epoch 10174/30000 Training Loss: 0.07434707134962082\n",
      "Epoch 10175/30000 Training Loss: 0.09183185547590256\n",
      "Epoch 10176/30000 Training Loss: 0.0948430597782135\n",
      "Epoch 10177/30000 Training Loss: 0.06168368458747864\n",
      "Epoch 10178/30000 Training Loss: 0.09664193540811539\n",
      "Epoch 10179/30000 Training Loss: 0.0834711566567421\n",
      "Epoch 10180/30000 Training Loss: 0.06553769111633301\n",
      "Epoch 10180/30000 Validation Loss: 0.06679060310125351\n",
      "Epoch 10181/30000 Training Loss: 0.08052891492843628\n",
      "Epoch 10182/30000 Training Loss: 0.07125339657068253\n",
      "Epoch 10183/30000 Training Loss: 0.09491168707609177\n",
      "Epoch 10184/30000 Training Loss: 0.09912914037704468\n",
      "Epoch 10185/30000 Training Loss: 0.07744303345680237\n",
      "Epoch 10186/30000 Training Loss: 0.11091640591621399\n",
      "Epoch 10187/30000 Training Loss: 0.12115871161222458\n",
      "Epoch 10188/30000 Training Loss: 0.06938615441322327\n",
      "Epoch 10189/30000 Training Loss: 0.08748972415924072\n",
      "Epoch 10190/30000 Training Loss: 0.0885256752371788\n",
      "Epoch 10190/30000 Validation Loss: 0.08701156824827194\n",
      "Epoch 10191/30000 Training Loss: 0.07653240114450455\n",
      "Epoch 10192/30000 Training Loss: 0.06815250962972641\n",
      "Epoch 10193/30000 Training Loss: 0.08162634819746017\n",
      "Epoch 10194/30000 Training Loss: 0.09507446736097336\n",
      "Epoch 10195/30000 Training Loss: 0.08101639896631241\n",
      "Epoch 10196/30000 Training Loss: 0.08098834753036499\n",
      "Epoch 10197/30000 Training Loss: 0.08419837802648544\n",
      "Epoch 10198/30000 Training Loss: 0.07442160695791245\n",
      "Epoch 10199/30000 Training Loss: 0.06892850995063782\n",
      "Epoch 10200/30000 Training Loss: 0.07842925935983658\n",
      "Epoch 10200/30000 Validation Loss: 0.08121192455291748\n",
      "Epoch 10201/30000 Training Loss: 0.08262503147125244\n",
      "Epoch 10202/30000 Training Loss: 0.0860774889588356\n",
      "Epoch 10203/30000 Training Loss: 0.0914359763264656\n",
      "Epoch 10204/30000 Training Loss: 0.07437888532876968\n",
      "Epoch 10205/30000 Training Loss: 0.1008511558175087\n",
      "Epoch 10206/30000 Training Loss: 0.0678337886929512\n",
      "Epoch 10207/30000 Training Loss: 0.09377064555883408\n",
      "Epoch 10208/30000 Training Loss: 0.08018181473016739\n",
      "Epoch 10209/30000 Training Loss: 0.09332140535116196\n",
      "Epoch 10210/30000 Training Loss: 0.08114749193191528\n",
      "Epoch 10210/30000 Validation Loss: 0.0835135206580162\n",
      "Epoch 10211/30000 Training Loss: 0.07753664255142212\n",
      "Epoch 10212/30000 Training Loss: 0.07064389437437057\n",
      "Epoch 10213/30000 Training Loss: 0.07261360436677933\n",
      "Epoch 10214/30000 Training Loss: 0.06996820867061615\n",
      "Epoch 10215/30000 Training Loss: 0.09371107816696167\n",
      "Epoch 10216/30000 Training Loss: 0.0851655900478363\n",
      "Epoch 10217/30000 Training Loss: 0.07140080630779266\n",
      "Epoch 10218/30000 Training Loss: 0.06315479427576065\n",
      "Epoch 10219/30000 Training Loss: 0.07368752360343933\n",
      "Epoch 10220/30000 Training Loss: 0.09008526802062988\n",
      "Epoch 10220/30000 Validation Loss: 0.09746938198804855\n",
      "Epoch 10221/30000 Training Loss: 0.07381870597600937\n",
      "Epoch 10222/30000 Training Loss: 0.07655856758356094\n",
      "Epoch 10223/30000 Training Loss: 0.10380623489618301\n",
      "Epoch 10224/30000 Training Loss: 0.0913027748465538\n",
      "Epoch 10225/30000 Training Loss: 0.08285386860370636\n",
      "Epoch 10226/30000 Training Loss: 0.08740609884262085\n",
      "Epoch 10227/30000 Training Loss: 0.10416668653488159\n",
      "Epoch 10228/30000 Training Loss: 0.068109892308712\n",
      "Epoch 10229/30000 Training Loss: 0.07541966438293457\n",
      "Epoch 10230/30000 Training Loss: 0.08353531360626221\n",
      "Epoch 10230/30000 Validation Loss: 0.06651314347982407\n",
      "Epoch 10231/30000 Training Loss: 0.08473596721887589\n",
      "Epoch 10232/30000 Training Loss: 0.09215158969163895\n",
      "Epoch 10233/30000 Training Loss: 0.08578719943761826\n",
      "Epoch 10234/30000 Training Loss: 0.07762212306261063\n",
      "Epoch 10235/30000 Training Loss: 0.0671934261918068\n",
      "Epoch 10236/30000 Training Loss: 0.09130676835775375\n",
      "Epoch 10237/30000 Training Loss: 0.09065068513154984\n",
      "Epoch 10238/30000 Training Loss: 0.06973085552453995\n",
      "Epoch 10239/30000 Training Loss: 0.07767488807439804\n",
      "Epoch 10240/30000 Training Loss: 0.06097133457660675\n",
      "Epoch 10240/30000 Validation Loss: 0.08126591891050339\n",
      "Epoch 10241/30000 Training Loss: 0.07611727714538574\n",
      "Epoch 10242/30000 Training Loss: 0.08221118897199631\n",
      "Epoch 10243/30000 Training Loss: 0.07786010205745697\n",
      "Epoch 10244/30000 Training Loss: 0.08209076523780823\n",
      "Epoch 10245/30000 Training Loss: 0.07640744000673294\n",
      "Epoch 10246/30000 Training Loss: 0.09028879553079605\n",
      "Epoch 10247/30000 Training Loss: 0.0552326999604702\n",
      "Epoch 10248/30000 Training Loss: 0.07811666280031204\n",
      "Epoch 10249/30000 Training Loss: 0.06677564978599548\n",
      "Epoch 10250/30000 Training Loss: 0.08446037769317627\n",
      "Epoch 10250/30000 Validation Loss: 0.07444372028112411\n",
      "Epoch 10251/30000 Training Loss: 0.08400832861661911\n",
      "Epoch 10252/30000 Training Loss: 0.08559498935937881\n",
      "Epoch 10253/30000 Training Loss: 0.08319912105798721\n",
      "Epoch 10254/30000 Training Loss: 0.08808553963899612\n",
      "Epoch 10255/30000 Training Loss: 0.07626133412122726\n",
      "Epoch 10256/30000 Training Loss: 0.080303855240345\n",
      "Epoch 10257/30000 Training Loss: 0.07811049371957779\n",
      "Epoch 10258/30000 Training Loss: 0.07018644362688065\n",
      "Epoch 10259/30000 Training Loss: 0.08363180607557297\n",
      "Epoch 10260/30000 Training Loss: 0.07870255410671234\n",
      "Epoch 10260/30000 Validation Loss: 0.0775989517569542\n",
      "Epoch 10261/30000 Training Loss: 0.06959358602762222\n",
      "Epoch 10262/30000 Training Loss: 0.0627211332321167\n",
      "Epoch 10263/30000 Training Loss: 0.07069634646177292\n",
      "Epoch 10264/30000 Training Loss: 0.07906103134155273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10265/30000 Training Loss: 0.08601618558168411\n",
      "Epoch 10266/30000 Training Loss: 0.0804256871342659\n",
      "Epoch 10267/30000 Training Loss: 0.08443976193666458\n",
      "Epoch 10268/30000 Training Loss: 0.0869501605629921\n",
      "Epoch 10269/30000 Training Loss: 0.06103399395942688\n",
      "Epoch 10270/30000 Training Loss: 0.09597432613372803\n",
      "Epoch 10270/30000 Validation Loss: 0.09019625186920166\n",
      "Epoch 10271/30000 Training Loss: 0.07753386348485947\n",
      "Epoch 10272/30000 Training Loss: 0.08084261417388916\n",
      "Epoch 10273/30000 Training Loss: 0.08383854478597641\n",
      "Epoch 10274/30000 Training Loss: 0.07440793514251709\n",
      "Epoch 10275/30000 Training Loss: 0.08490556478500366\n",
      "Epoch 10276/30000 Training Loss: 0.09868214279413223\n",
      "Epoch 10277/30000 Training Loss: 0.08046422153711319\n",
      "Epoch 10278/30000 Training Loss: 0.07861461490392685\n",
      "Epoch 10279/30000 Training Loss: 0.09017584472894669\n",
      "Epoch 10280/30000 Training Loss: 0.07478370517492294\n",
      "Epoch 10280/30000 Validation Loss: 0.07758036255836487\n",
      "Epoch 10281/30000 Training Loss: 0.07604458183050156\n",
      "Epoch 10282/30000 Training Loss: 0.07838953286409378\n",
      "Epoch 10283/30000 Training Loss: 0.08049016445875168\n",
      "Epoch 10284/30000 Training Loss: 0.08554589003324509\n",
      "Epoch 10285/30000 Training Loss: 0.06576135009527206\n",
      "Epoch 10286/30000 Training Loss: 0.10168981552124023\n",
      "Epoch 10287/30000 Training Loss: 0.06985224038362503\n",
      "Epoch 10288/30000 Training Loss: 0.07134288549423218\n",
      "Epoch 10289/30000 Training Loss: 0.08421379327774048\n",
      "Epoch 10290/30000 Training Loss: 0.0770554468035698\n",
      "Epoch 10290/30000 Validation Loss: 0.11142251640558243\n",
      "Epoch 10291/30000 Training Loss: 0.0792977437376976\n",
      "Epoch 10292/30000 Training Loss: 0.08901422470808029\n",
      "Epoch 10293/30000 Training Loss: 0.07551117986440659\n",
      "Epoch 10294/30000 Training Loss: 0.07697100192308426\n",
      "Epoch 10295/30000 Training Loss: 0.09925446659326553\n",
      "Epoch 10296/30000 Training Loss: 0.10055998712778091\n",
      "Epoch 10297/30000 Training Loss: 0.05743228271603584\n",
      "Epoch 10298/30000 Training Loss: 0.07716816663742065\n",
      "Epoch 10299/30000 Training Loss: 0.0757504478096962\n",
      "Epoch 10300/30000 Training Loss: 0.07388664036989212\n",
      "Epoch 10300/30000 Validation Loss: 0.07427192479372025\n",
      "Epoch 10301/30000 Training Loss: 0.08623097091913223\n",
      "Epoch 10302/30000 Training Loss: 0.08105725795030594\n",
      "Epoch 10303/30000 Training Loss: 0.07076098769903183\n",
      "Epoch 10304/30000 Training Loss: 0.09258278459310532\n",
      "Epoch 10305/30000 Training Loss: 0.0668194517493248\n",
      "Epoch 10306/30000 Training Loss: 0.06862780451774597\n",
      "Epoch 10307/30000 Training Loss: 0.08531201630830765\n",
      "Epoch 10308/30000 Training Loss: 0.07893866300582886\n",
      "Epoch 10309/30000 Training Loss: 0.09467947483062744\n",
      "Epoch 10310/30000 Training Loss: 0.09244808554649353\n",
      "Epoch 10310/30000 Validation Loss: 0.07354727387428284\n",
      "Epoch 10311/30000 Training Loss: 0.07901270687580109\n",
      "Epoch 10312/30000 Training Loss: 0.09055283665657043\n",
      "Epoch 10313/30000 Training Loss: 0.08176791667938232\n",
      "Epoch 10314/30000 Training Loss: 0.06418672949075699\n",
      "Epoch 10315/30000 Training Loss: 0.08620109409093857\n",
      "Epoch 10316/30000 Training Loss: 0.08754146099090576\n",
      "Epoch 10317/30000 Training Loss: 0.10607996582984924\n",
      "Epoch 10318/30000 Training Loss: 0.06980904936790466\n",
      "Epoch 10319/30000 Training Loss: 0.07985937595367432\n",
      "Epoch 10320/30000 Training Loss: 0.0860322043299675\n",
      "Epoch 10320/30000 Validation Loss: 0.06237092241644859\n",
      "Epoch 10321/30000 Training Loss: 0.06788277626037598\n",
      "Epoch 10322/30000 Training Loss: 0.08310822397470474\n",
      "Epoch 10323/30000 Training Loss: 0.08089902251958847\n",
      "Epoch 10324/30000 Training Loss: 0.08769285678863525\n",
      "Epoch 10325/30000 Training Loss: 0.0668923556804657\n",
      "Epoch 10326/30000 Training Loss: 0.09266501665115356\n",
      "Epoch 10327/30000 Training Loss: 0.08139518648386002\n",
      "Epoch 10328/30000 Training Loss: 0.09155658632516861\n",
      "Epoch 10329/30000 Training Loss: 0.09101653099060059\n",
      "Epoch 10330/30000 Training Loss: 0.08563164621591568\n",
      "Epoch 10330/30000 Validation Loss: 0.09408684819936752\n",
      "Epoch 10331/30000 Training Loss: 0.08093071728944778\n",
      "Epoch 10332/30000 Training Loss: 0.10516118258237839\n",
      "Epoch 10333/30000 Training Loss: 0.09850967675447464\n",
      "Epoch 10334/30000 Training Loss: 0.08297096937894821\n",
      "Epoch 10335/30000 Training Loss: 0.06932497024536133\n",
      "Epoch 10336/30000 Training Loss: 0.08747822046279907\n",
      "Epoch 10337/30000 Training Loss: 0.07458433508872986\n",
      "Epoch 10338/30000 Training Loss: 0.07717259973287582\n",
      "Epoch 10339/30000 Training Loss: 0.06462448090314865\n",
      "Epoch 10340/30000 Training Loss: 0.08066267520189285\n",
      "Epoch 10340/30000 Validation Loss: 0.07329469919204712\n",
      "Epoch 10341/30000 Training Loss: 0.06376869976520538\n",
      "Epoch 10342/30000 Training Loss: 0.0660596564412117\n",
      "Epoch 10343/30000 Training Loss: 0.0643714889883995\n",
      "Epoch 10344/30000 Training Loss: 0.07290560752153397\n",
      "Epoch 10345/30000 Training Loss: 0.08069484680891037\n",
      "Epoch 10346/30000 Training Loss: 0.08385590463876724\n",
      "Epoch 10347/30000 Training Loss: 0.07912930101156235\n",
      "Epoch 10348/30000 Training Loss: 0.08283103257417679\n",
      "Epoch 10349/30000 Training Loss: 0.08523198217153549\n",
      "Epoch 10350/30000 Training Loss: 0.07938624918460846\n",
      "Epoch 10350/30000 Validation Loss: 0.08232568949460983\n",
      "Epoch 10351/30000 Training Loss: 0.07477041333913803\n",
      "Epoch 10352/30000 Training Loss: 0.0786498412489891\n",
      "Epoch 10353/30000 Training Loss: 0.062362734228372574\n",
      "Epoch 10354/30000 Training Loss: 0.07544473558664322\n",
      "Epoch 10355/30000 Training Loss: 0.09294446557760239\n",
      "Epoch 10356/30000 Training Loss: 0.08961503952741623\n",
      "Epoch 10357/30000 Training Loss: 0.08002787083387375\n",
      "Epoch 10358/30000 Training Loss: 0.0792727842926979\n",
      "Epoch 10359/30000 Training Loss: 0.10031020641326904\n",
      "Epoch 10360/30000 Training Loss: 0.07618450373411179\n",
      "Epoch 10360/30000 Validation Loss: 0.07169368118047714\n",
      "Epoch 10361/30000 Training Loss: 0.08361033350229263\n",
      "Epoch 10362/30000 Training Loss: 0.08259042352437973\n",
      "Epoch 10363/30000 Training Loss: 0.07456453144550323\n",
      "Epoch 10364/30000 Training Loss: 0.07782218605279922\n",
      "Epoch 10365/30000 Training Loss: 0.09756169468164444\n",
      "Epoch 10366/30000 Training Loss: 0.06771749258041382\n",
      "Epoch 10367/30000 Training Loss: 0.0755050778388977\n",
      "Epoch 10368/30000 Training Loss: 0.06598618626594543\n",
      "Epoch 10369/30000 Training Loss: 0.0826718881726265\n",
      "Epoch 10370/30000 Training Loss: 0.07103658467531204\n",
      "Epoch 10370/30000 Validation Loss: 0.08109312504529953\n",
      "Epoch 10371/30000 Training Loss: 0.08581551164388657\n",
      "Epoch 10372/30000 Training Loss: 0.09286224842071533\n",
      "Epoch 10373/30000 Training Loss: 0.07002872973680496\n",
      "Epoch 10374/30000 Training Loss: 0.0769486054778099\n",
      "Epoch 10375/30000 Training Loss: 0.10726135224103928\n",
      "Epoch 10376/30000 Training Loss: 0.08267287909984589\n",
      "Epoch 10377/30000 Training Loss: 0.07793477922677994\n",
      "Epoch 10378/30000 Training Loss: 0.06709278374910355\n",
      "Epoch 10379/30000 Training Loss: 0.08331578224897385\n",
      "Epoch 10380/30000 Training Loss: 0.09519734978675842\n",
      "Epoch 10380/30000 Validation Loss: 0.06736325472593307\n",
      "Epoch 10381/30000 Training Loss: 0.07838869839906693\n",
      "Epoch 10382/30000 Training Loss: 0.0928313210606575\n",
      "Epoch 10383/30000 Training Loss: 0.09817271679639816\n",
      "Epoch 10384/30000 Training Loss: 0.07236769795417786\n",
      "Epoch 10385/30000 Training Loss: 0.12163195013999939\n",
      "Epoch 10386/30000 Training Loss: 0.08491333574056625\n",
      "Epoch 10387/30000 Training Loss: 0.06626969575881958\n",
      "Epoch 10388/30000 Training Loss: 0.07224395126104355\n",
      "Epoch 10389/30000 Training Loss: 0.11174296587705612\n",
      "Epoch 10390/30000 Training Loss: 0.08198665827512741\n",
      "Epoch 10390/30000 Validation Loss: 0.07484888285398483\n",
      "Epoch 10391/30000 Training Loss: 0.0646352469921112\n",
      "Epoch 10392/30000 Training Loss: 0.09558548778295517\n",
      "Epoch 10393/30000 Training Loss: 0.09129578620195389\n",
      "Epoch 10394/30000 Training Loss: 0.07352464646100998\n",
      "Epoch 10395/30000 Training Loss: 0.08280476182699203\n",
      "Epoch 10396/30000 Training Loss: 0.07253328710794449\n",
      "Epoch 10397/30000 Training Loss: 0.08913823962211609\n",
      "Epoch 10398/30000 Training Loss: 0.0874660536646843\n",
      "Epoch 10399/30000 Training Loss: 0.06470892578363419\n",
      "Epoch 10400/30000 Training Loss: 0.06571891158819199\n",
      "Epoch 10400/30000 Validation Loss: 0.08501558750867844\n",
      "Epoch 10401/30000 Training Loss: 0.08619839698076248\n",
      "Epoch 10402/30000 Training Loss: 0.07444969564676285\n",
      "Epoch 10403/30000 Training Loss: 0.09213604778051376\n",
      "Epoch 10404/30000 Training Loss: 0.0629805400967598\n",
      "Epoch 10405/30000 Training Loss: 0.07046625018119812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10406/30000 Training Loss: 0.06618133932352066\n",
      "Epoch 10407/30000 Training Loss: 0.08902370184659958\n",
      "Epoch 10408/30000 Training Loss: 0.08365469425916672\n",
      "Epoch 10409/30000 Training Loss: 0.06817033886909485\n",
      "Epoch 10410/30000 Training Loss: 0.08348052948713303\n",
      "Epoch 10410/30000 Validation Loss: 0.08493120223283768\n",
      "Epoch 10411/30000 Training Loss: 0.08203687518835068\n",
      "Epoch 10412/30000 Training Loss: 0.07226894050836563\n",
      "Epoch 10413/30000 Training Loss: 0.08253496885299683\n",
      "Epoch 10414/30000 Training Loss: 0.07498116046190262\n",
      "Epoch 10415/30000 Training Loss: 0.09588730335235596\n",
      "Epoch 10416/30000 Training Loss: 0.07664267718791962\n",
      "Epoch 10417/30000 Training Loss: 0.08362025022506714\n",
      "Epoch 10418/30000 Training Loss: 0.08472112566232681\n",
      "Epoch 10419/30000 Training Loss: 0.08998953551054001\n",
      "Epoch 10420/30000 Training Loss: 0.07534313946962357\n",
      "Epoch 10420/30000 Validation Loss: 0.09900862723588943\n",
      "Epoch 10421/30000 Training Loss: 0.06474270671606064\n",
      "Epoch 10422/30000 Training Loss: 0.07847186923027039\n",
      "Epoch 10423/30000 Training Loss: 0.0872514620423317\n",
      "Epoch 10424/30000 Training Loss: 0.060926537960767746\n",
      "Epoch 10425/30000 Training Loss: 0.07229746133089066\n",
      "Epoch 10426/30000 Training Loss: 0.07294278591871262\n",
      "Epoch 10427/30000 Training Loss: 0.08901909738779068\n",
      "Epoch 10428/30000 Training Loss: 0.0909363329410553\n",
      "Epoch 10429/30000 Training Loss: 0.08360785990953445\n",
      "Epoch 10430/30000 Training Loss: 0.08392330259084702\n",
      "Epoch 10430/30000 Validation Loss: 0.09073381870985031\n",
      "Epoch 10431/30000 Training Loss: 0.08536901324987411\n",
      "Epoch 10432/30000 Training Loss: 0.08505451679229736\n",
      "Epoch 10433/30000 Training Loss: 0.10165098309516907\n",
      "Epoch 10434/30000 Training Loss: 0.06895475089550018\n",
      "Epoch 10435/30000 Training Loss: 0.08195707947015762\n",
      "Epoch 10436/30000 Training Loss: 0.10333138704299927\n",
      "Epoch 10437/30000 Training Loss: 0.07307470589876175\n",
      "Epoch 10438/30000 Training Loss: 0.08736417442560196\n",
      "Epoch 10439/30000 Training Loss: 0.07729034870862961\n",
      "Epoch 10440/30000 Training Loss: 0.09065283089876175\n",
      "Epoch 10440/30000 Validation Loss: 0.076345294713974\n",
      "Epoch 10441/30000 Training Loss: 0.07179970294237137\n",
      "Epoch 10442/30000 Training Loss: 0.09293863922357559\n",
      "Epoch 10443/30000 Training Loss: 0.0757829025387764\n",
      "Epoch 10444/30000 Training Loss: 0.07464785128831863\n",
      "Epoch 10445/30000 Training Loss: 0.0793028250336647\n",
      "Epoch 10446/30000 Training Loss: 0.09230732917785645\n",
      "Epoch 10447/30000 Training Loss: 0.07066824287176132\n",
      "Epoch 10448/30000 Training Loss: 0.08883267641067505\n",
      "Epoch 10449/30000 Training Loss: 0.07583627849817276\n",
      "Epoch 10450/30000 Training Loss: 0.08464629203081131\n",
      "Epoch 10450/30000 Validation Loss: 0.08777281641960144\n",
      "Epoch 10451/30000 Training Loss: 0.06787844747304916\n",
      "Epoch 10452/30000 Training Loss: 0.08113009482622147\n",
      "Epoch 10453/30000 Training Loss: 0.06548022478818893\n",
      "Epoch 10454/30000 Training Loss: 0.09065455198287964\n",
      "Epoch 10455/30000 Training Loss: 0.06833358108997345\n",
      "Epoch 10456/30000 Training Loss: 0.09163915365934372\n",
      "Epoch 10457/30000 Training Loss: 0.09323269128799438\n",
      "Epoch 10458/30000 Training Loss: 0.10552915185689926\n",
      "Epoch 10459/30000 Training Loss: 0.0791676864027977\n",
      "Epoch 10460/30000 Training Loss: 0.07821863889694214\n",
      "Epoch 10460/30000 Validation Loss: 0.07490105926990509\n",
      "Epoch 10461/30000 Training Loss: 0.0626949593424797\n",
      "Epoch 10462/30000 Training Loss: 0.08870017528533936\n",
      "Epoch 10463/30000 Training Loss: 0.08460857719182968\n",
      "Epoch 10464/30000 Training Loss: 0.0723920688033104\n",
      "Epoch 10465/30000 Training Loss: 0.07833410054445267\n",
      "Epoch 10466/30000 Training Loss: 0.10607831925153732\n",
      "Epoch 10467/30000 Training Loss: 0.07474691420793533\n",
      "Epoch 10468/30000 Training Loss: 0.08261977881193161\n",
      "Epoch 10469/30000 Training Loss: 0.07976049929857254\n",
      "Epoch 10470/30000 Training Loss: 0.09000664949417114\n",
      "Epoch 10470/30000 Validation Loss: 0.0738600566983223\n",
      "Epoch 10471/30000 Training Loss: 0.08255355805158615\n",
      "Epoch 10472/30000 Training Loss: 0.07987376302480698\n",
      "Epoch 10473/30000 Training Loss: 0.07501687109470367\n",
      "Epoch 10474/30000 Training Loss: 0.08847632259130478\n",
      "Epoch 10475/30000 Training Loss: 0.07346373051404953\n",
      "Epoch 10476/30000 Training Loss: 0.0781756043434143\n",
      "Epoch 10477/30000 Training Loss: 0.08641370385885239\n",
      "Epoch 10478/30000 Training Loss: 0.07721272855997086\n",
      "Epoch 10479/30000 Training Loss: 0.08291666954755783\n",
      "Epoch 10480/30000 Training Loss: 0.09116045385599136\n",
      "Epoch 10480/30000 Validation Loss: 0.07553588598966599\n",
      "Epoch 10481/30000 Training Loss: 0.08424942940473557\n",
      "Epoch 10482/30000 Training Loss: 0.0861319899559021\n",
      "Epoch 10483/30000 Training Loss: 0.09122259169816971\n",
      "Epoch 10484/30000 Training Loss: 0.058572594076395035\n",
      "Epoch 10485/30000 Training Loss: 0.07362235337495804\n",
      "Epoch 10486/30000 Training Loss: 0.07572142034769058\n",
      "Epoch 10487/30000 Training Loss: 0.08710455894470215\n",
      "Epoch 10488/30000 Training Loss: 0.08714891225099564\n",
      "Epoch 10489/30000 Training Loss: 0.06244969740509987\n",
      "Epoch 10490/30000 Training Loss: 0.08300352841615677\n",
      "Epoch 10490/30000 Validation Loss: 0.07094947248697281\n",
      "Epoch 10491/30000 Training Loss: 0.09995776414871216\n",
      "Epoch 10492/30000 Training Loss: 0.08132470399141312\n",
      "Epoch 10493/30000 Training Loss: 0.06213788315653801\n",
      "Epoch 10494/30000 Training Loss: 0.09768126159906387\n",
      "Epoch 10495/30000 Training Loss: 0.0808090791106224\n",
      "Epoch 10496/30000 Training Loss: 0.07970523834228516\n",
      "Epoch 10497/30000 Training Loss: 0.10219574719667435\n",
      "Epoch 10498/30000 Training Loss: 0.07662565261125565\n",
      "Epoch 10499/30000 Training Loss: 0.08927899599075317\n",
      "Epoch 10500/30000 Training Loss: 0.09732270240783691\n",
      "Epoch 10500/30000 Validation Loss: 0.07340794056653976\n",
      "Epoch 10501/30000 Training Loss: 0.08596628159284592\n",
      "Epoch 10502/30000 Training Loss: 0.07302606105804443\n",
      "Epoch 10503/30000 Training Loss: 0.07880569249391556\n",
      "Epoch 10504/30000 Training Loss: 0.09172898530960083\n",
      "Epoch 10505/30000 Training Loss: 0.08813855797052383\n",
      "Epoch 10506/30000 Training Loss: 0.06707317382097244\n",
      "Epoch 10507/30000 Training Loss: 0.09146428853273392\n",
      "Epoch 10508/30000 Training Loss: 0.06991404294967651\n",
      "Epoch 10509/30000 Training Loss: 0.07302060723304749\n",
      "Epoch 10510/30000 Training Loss: 0.08006056398153305\n",
      "Epoch 10510/30000 Validation Loss: 0.09651532024145126\n",
      "Epoch 10511/30000 Training Loss: 0.08529568463563919\n",
      "Epoch 10512/30000 Training Loss: 0.0768611952662468\n",
      "Epoch 10513/30000 Training Loss: 0.08421778678894043\n",
      "Epoch 10514/30000 Training Loss: 0.07512699067592621\n",
      "Epoch 10515/30000 Training Loss: 0.09340124577283859\n",
      "Epoch 10516/30000 Training Loss: 0.082583487033844\n",
      "Epoch 10517/30000 Training Loss: 0.06598678231239319\n",
      "Epoch 10518/30000 Training Loss: 0.07961153239011765\n",
      "Epoch 10519/30000 Training Loss: 0.07957359403371811\n",
      "Epoch 10520/30000 Training Loss: 0.07658780366182327\n",
      "Epoch 10520/30000 Validation Loss: 0.08253402262926102\n",
      "Epoch 10521/30000 Training Loss: 0.071026511490345\n",
      "Epoch 10522/30000 Training Loss: 0.08014517277479172\n",
      "Epoch 10523/30000 Training Loss: 0.07835250347852707\n",
      "Epoch 10524/30000 Training Loss: 0.07827770709991455\n",
      "Epoch 10525/30000 Training Loss: 0.08262503892183304\n",
      "Epoch 10526/30000 Training Loss: 0.07260961085557938\n",
      "Epoch 10527/30000 Training Loss: 0.08283623307943344\n",
      "Epoch 10528/30000 Training Loss: 0.07304801791906357\n",
      "Epoch 10529/30000 Training Loss: 0.08672573417425156\n",
      "Epoch 10530/30000 Training Loss: 0.08429715037345886\n",
      "Epoch 10530/30000 Validation Loss: 0.07966779917478561\n",
      "Epoch 10531/30000 Training Loss: 0.0819537341594696\n",
      "Epoch 10532/30000 Training Loss: 0.07837174087762833\n",
      "Epoch 10533/30000 Training Loss: 0.08697903156280518\n",
      "Epoch 10534/30000 Training Loss: 0.08481166511774063\n",
      "Epoch 10535/30000 Training Loss: 0.08262825757265091\n",
      "Epoch 10536/30000 Training Loss: 0.09120794385671616\n",
      "Epoch 10537/30000 Training Loss: 0.07231524586677551\n",
      "Epoch 10538/30000 Training Loss: 0.09483746439218521\n",
      "Epoch 10539/30000 Training Loss: 0.07873138785362244\n",
      "Epoch 10540/30000 Training Loss: 0.08846116065979004\n",
      "Epoch 10540/30000 Validation Loss: 0.10183755308389664\n",
      "Epoch 10541/30000 Training Loss: 0.07658947259187698\n",
      "Epoch 10542/30000 Training Loss: 0.07271280139684677\n",
      "Epoch 10543/30000 Training Loss: 0.06708112359046936\n",
      "Epoch 10544/30000 Training Loss: 0.05985328555107117\n",
      "Epoch 10545/30000 Training Loss: 0.08187389373779297\n",
      "Epoch 10546/30000 Training Loss: 0.07826006412506104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10547/30000 Training Loss: 0.10002962499856949\n",
      "Epoch 10548/30000 Training Loss: 0.09320694953203201\n",
      "Epoch 10549/30000 Training Loss: 0.06481795758008957\n",
      "Epoch 10550/30000 Training Loss: 0.08015010505914688\n",
      "Epoch 10550/30000 Validation Loss: 0.09388262778520584\n",
      "Epoch 10551/30000 Training Loss: 0.09875348955392838\n",
      "Epoch 10552/30000 Training Loss: 0.0900476947426796\n",
      "Epoch 10553/30000 Training Loss: 0.06575711816549301\n",
      "Epoch 10554/30000 Training Loss: 0.06583372503519058\n",
      "Epoch 10555/30000 Training Loss: 0.08573085814714432\n",
      "Epoch 10556/30000 Training Loss: 0.0959557369351387\n",
      "Epoch 10557/30000 Training Loss: 0.07212025672197342\n",
      "Epoch 10558/30000 Training Loss: 0.09081423282623291\n",
      "Epoch 10559/30000 Training Loss: 0.0751715898513794\n",
      "Epoch 10560/30000 Training Loss: 0.06973880529403687\n",
      "Epoch 10560/30000 Validation Loss: 0.10082713514566422\n",
      "Epoch 10561/30000 Training Loss: 0.07672927528619766\n",
      "Epoch 10562/30000 Training Loss: 0.0751953050494194\n",
      "Epoch 10563/30000 Training Loss: 0.07357773929834366\n",
      "Epoch 10564/30000 Training Loss: 0.08355452865362167\n",
      "Epoch 10565/30000 Training Loss: 0.06109536066651344\n",
      "Epoch 10566/30000 Training Loss: 0.08271517604589462\n",
      "Epoch 10567/30000 Training Loss: 0.08171869069337845\n",
      "Epoch 10568/30000 Training Loss: 0.06511227041482925\n",
      "Epoch 10569/30000 Training Loss: 0.09183686226606369\n",
      "Epoch 10570/30000 Training Loss: 0.08596739917993546\n",
      "Epoch 10570/30000 Validation Loss: 0.08874103426933289\n",
      "Epoch 10571/30000 Training Loss: 0.06925249844789505\n",
      "Epoch 10572/30000 Training Loss: 0.07498195022344589\n",
      "Epoch 10573/30000 Training Loss: 0.07892543077468872\n",
      "Epoch 10574/30000 Training Loss: 0.07294484227895737\n",
      "Epoch 10575/30000 Training Loss: 0.1037404015660286\n",
      "Epoch 10576/30000 Training Loss: 0.09060599654912949\n",
      "Epoch 10577/30000 Training Loss: 0.07380812615156174\n",
      "Epoch 10578/30000 Training Loss: 0.05782380327582359\n",
      "Epoch 10579/30000 Training Loss: 0.07881102710962296\n",
      "Epoch 10580/30000 Training Loss: 0.08906710147857666\n",
      "Epoch 10580/30000 Validation Loss: 0.06186069920659065\n",
      "Epoch 10581/30000 Training Loss: 0.07995735853910446\n",
      "Epoch 10582/30000 Training Loss: 0.08682987093925476\n",
      "Epoch 10583/30000 Training Loss: 0.07205981761217117\n",
      "Epoch 10584/30000 Training Loss: 0.08941128104925156\n",
      "Epoch 10585/30000 Training Loss: 0.06774774938821793\n",
      "Epoch 10586/30000 Training Loss: 0.09222986549139023\n",
      "Epoch 10587/30000 Training Loss: 0.09015282988548279\n",
      "Epoch 10588/30000 Training Loss: 0.07908279448747635\n",
      "Epoch 10589/30000 Training Loss: 0.06106065586209297\n",
      "Epoch 10590/30000 Training Loss: 0.0671432763338089\n",
      "Epoch 10590/30000 Validation Loss: 0.07151255756616592\n",
      "Epoch 10591/30000 Training Loss: 0.08194411545991898\n",
      "Epoch 10592/30000 Training Loss: 0.09892866760492325\n",
      "Epoch 10593/30000 Training Loss: 0.0841621533036232\n",
      "Epoch 10594/30000 Training Loss: 0.07747820764780045\n",
      "Epoch 10595/30000 Training Loss: 0.07900639623403549\n",
      "Epoch 10596/30000 Training Loss: 0.09731093794107437\n",
      "Epoch 10597/30000 Training Loss: 0.08855011314153671\n",
      "Epoch 10598/30000 Training Loss: 0.0746954157948494\n",
      "Epoch 10599/30000 Training Loss: 0.08515676856040955\n",
      "Epoch 10600/30000 Training Loss: 0.0833246111869812\n",
      "Epoch 10600/30000 Validation Loss: 0.09370863437652588\n",
      "Epoch 10601/30000 Training Loss: 0.07512249797582626\n",
      "Epoch 10602/30000 Training Loss: 0.081050343811512\n",
      "Epoch 10603/30000 Training Loss: 0.0721563994884491\n",
      "Epoch 10604/30000 Training Loss: 0.07288873195648193\n",
      "Epoch 10605/30000 Training Loss: 0.07792321592569351\n",
      "Epoch 10606/30000 Training Loss: 0.08005998283624649\n",
      "Epoch 10607/30000 Training Loss: 0.07731452584266663\n",
      "Epoch 10608/30000 Training Loss: 0.08419066667556763\n",
      "Epoch 10609/30000 Training Loss: 0.09516063332557678\n",
      "Epoch 10610/30000 Training Loss: 0.08797509223222733\n",
      "Epoch 10610/30000 Validation Loss: 0.0846169963479042\n",
      "Epoch 10611/30000 Training Loss: 0.07778536528348923\n",
      "Epoch 10612/30000 Training Loss: 0.06987059861421585\n",
      "Epoch 10613/30000 Training Loss: 0.08638379722833633\n",
      "Epoch 10614/30000 Training Loss: 0.0807204470038414\n",
      "Epoch 10615/30000 Training Loss: 0.1018899455666542\n",
      "Epoch 10616/30000 Training Loss: 0.09565673023462296\n",
      "Epoch 10617/30000 Training Loss: 0.10073550790548325\n",
      "Epoch 10618/30000 Training Loss: 0.07405386865139008\n",
      "Epoch 10619/30000 Training Loss: 0.07267152518033981\n",
      "Epoch 10620/30000 Training Loss: 0.06528504192829132\n",
      "Epoch 10620/30000 Validation Loss: 0.07979690283536911\n",
      "Epoch 10621/30000 Training Loss: 0.07554937154054642\n",
      "Epoch 10622/30000 Training Loss: 0.11539679765701294\n",
      "Epoch 10623/30000 Training Loss: 0.07551536709070206\n",
      "Epoch 10624/30000 Training Loss: 0.09202482551336288\n",
      "Epoch 10625/30000 Training Loss: 0.0922095999121666\n",
      "Epoch 10626/30000 Training Loss: 0.07537297159433365\n",
      "Epoch 10627/30000 Training Loss: 0.0660325214266777\n",
      "Epoch 10628/30000 Training Loss: 0.09310326725244522\n",
      "Epoch 10629/30000 Training Loss: 0.08511390537023544\n",
      "Epoch 10630/30000 Training Loss: 0.075137197971344\n",
      "Epoch 10630/30000 Validation Loss: 0.08239123970270157\n",
      "Epoch 10631/30000 Training Loss: 0.07792981714010239\n",
      "Epoch 10632/30000 Training Loss: 0.07526906579732895\n",
      "Epoch 10633/30000 Training Loss: 0.07893236726522446\n",
      "Epoch 10634/30000 Training Loss: 0.07226788252592087\n",
      "Epoch 10635/30000 Training Loss: 0.1061033234000206\n",
      "Epoch 10636/30000 Training Loss: 0.06289372593164444\n",
      "Epoch 10637/30000 Training Loss: 0.07114414125680923\n",
      "Epoch 10638/30000 Training Loss: 0.1075357124209404\n",
      "Epoch 10639/30000 Training Loss: 0.09571491926908493\n",
      "Epoch 10640/30000 Training Loss: 0.08556371927261353\n",
      "Epoch 10640/30000 Validation Loss: 0.07302456349134445\n",
      "Epoch 10641/30000 Training Loss: 0.07774398475885391\n",
      "Epoch 10642/30000 Training Loss: 0.1051112711429596\n",
      "Epoch 10643/30000 Training Loss: 0.08077723532915115\n",
      "Epoch 10644/30000 Training Loss: 0.08935407549142838\n",
      "Epoch 10645/30000 Training Loss: 0.07153510302305222\n",
      "Epoch 10646/30000 Training Loss: 0.07198642939329147\n",
      "Epoch 10647/30000 Training Loss: 0.07313541322946548\n",
      "Epoch 10648/30000 Training Loss: 0.07766645401716232\n",
      "Epoch 10649/30000 Training Loss: 0.08019580692052841\n",
      "Epoch 10650/30000 Training Loss: 0.09266509860754013\n",
      "Epoch 10650/30000 Validation Loss: 0.06880515068769455\n",
      "Epoch 10651/30000 Training Loss: 0.06563235074281693\n",
      "Epoch 10652/30000 Training Loss: 0.07002582401037216\n",
      "Epoch 10653/30000 Training Loss: 0.06608828902244568\n",
      "Epoch 10654/30000 Training Loss: 0.06828340142965317\n",
      "Epoch 10655/30000 Training Loss: 0.07084962725639343\n",
      "Epoch 10656/30000 Training Loss: 0.0892142578959465\n",
      "Epoch 10657/30000 Training Loss: 0.08366075158119202\n",
      "Epoch 10658/30000 Training Loss: 0.08441121131181717\n",
      "Epoch 10659/30000 Training Loss: 0.07271513342857361\n",
      "Epoch 10660/30000 Training Loss: 0.07751277834177017\n",
      "Epoch 10660/30000 Validation Loss: 0.08001817017793655\n",
      "Epoch 10661/30000 Training Loss: 0.07391911000013351\n",
      "Epoch 10662/30000 Training Loss: 0.07333407551050186\n",
      "Epoch 10663/30000 Training Loss: 0.08422673493623734\n",
      "Epoch 10664/30000 Training Loss: 0.06361886113882065\n",
      "Epoch 10665/30000 Training Loss: 0.07666700333356857\n",
      "Epoch 10666/30000 Training Loss: 0.08946341276168823\n",
      "Epoch 10667/30000 Training Loss: 0.05518312379717827\n",
      "Epoch 10668/30000 Training Loss: 0.09042961150407791\n",
      "Epoch 10669/30000 Training Loss: 0.08951690793037415\n",
      "Epoch 10670/30000 Training Loss: 0.08732295781373978\n",
      "Epoch 10670/30000 Validation Loss: 0.0746469497680664\n",
      "Epoch 10671/30000 Training Loss: 0.09205316752195358\n",
      "Epoch 10672/30000 Training Loss: 0.06796006113290787\n",
      "Epoch 10673/30000 Training Loss: 0.0817488431930542\n",
      "Epoch 10674/30000 Training Loss: 0.08170469850301743\n",
      "Epoch 10675/30000 Training Loss: 0.07216817885637283\n",
      "Epoch 10676/30000 Training Loss: 0.08838450163602829\n",
      "Epoch 10677/30000 Training Loss: 0.06619837880134583\n",
      "Epoch 10678/30000 Training Loss: 0.08024246245622635\n",
      "Epoch 10679/30000 Training Loss: 0.08233382552862167\n",
      "Epoch 10680/30000 Training Loss: 0.0786551907658577\n",
      "Epoch 10680/30000 Validation Loss: 0.08018159121274948\n",
      "Epoch 10681/30000 Training Loss: 0.10811278223991394\n",
      "Epoch 10682/30000 Training Loss: 0.06434222310781479\n",
      "Epoch 10683/30000 Training Loss: 0.08374746888875961\n",
      "Epoch 10684/30000 Training Loss: 0.07180045545101166\n",
      "Epoch 10685/30000 Training Loss: 0.09183937311172485\n",
      "Epoch 10686/30000 Training Loss: 0.06900551170110703\n",
      "Epoch 10687/30000 Training Loss: 0.08790802210569382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10688/30000 Training Loss: 0.07865238189697266\n",
      "Epoch 10689/30000 Training Loss: 0.061375658959150314\n",
      "Epoch 10690/30000 Training Loss: 0.06019289419054985\n",
      "Epoch 10690/30000 Validation Loss: 0.06057146191596985\n",
      "Epoch 10691/30000 Training Loss: 0.08688650280237198\n",
      "Epoch 10692/30000 Training Loss: 0.09334751218557358\n",
      "Epoch 10693/30000 Training Loss: 0.07203903794288635\n",
      "Epoch 10694/30000 Training Loss: 0.07555272430181503\n",
      "Epoch 10695/30000 Training Loss: 0.08412516862154007\n",
      "Epoch 10696/30000 Training Loss: 0.07148427516222\n",
      "Epoch 10697/30000 Training Loss: 0.09790494292974472\n",
      "Epoch 10698/30000 Training Loss: 0.07824999839067459\n",
      "Epoch 10699/30000 Training Loss: 0.09277696162462234\n",
      "Epoch 10700/30000 Training Loss: 0.10046228021383286\n",
      "Epoch 10700/30000 Validation Loss: 0.06462568044662476\n",
      "Epoch 10701/30000 Training Loss: 0.07452655583620071\n",
      "Epoch 10702/30000 Training Loss: 0.08324573189020157\n",
      "Epoch 10703/30000 Training Loss: 0.07392405718564987\n",
      "Epoch 10704/30000 Training Loss: 0.06346700340509415\n",
      "Epoch 10705/30000 Training Loss: 0.07251065224409103\n",
      "Epoch 10706/30000 Training Loss: 0.0752231776714325\n",
      "Epoch 10707/30000 Training Loss: 0.08045151084661484\n",
      "Epoch 10708/30000 Training Loss: 0.0811527743935585\n",
      "Epoch 10709/30000 Training Loss: 0.08411833643913269\n",
      "Epoch 10710/30000 Training Loss: 0.09123959392309189\n",
      "Epoch 10710/30000 Validation Loss: 0.08473026007413864\n",
      "Epoch 10711/30000 Training Loss: 0.08351004123687744\n",
      "Epoch 10712/30000 Training Loss: 0.09838420897722244\n",
      "Epoch 10713/30000 Training Loss: 0.07641860097646713\n",
      "Epoch 10714/30000 Training Loss: 0.07425465434789658\n",
      "Epoch 10715/30000 Training Loss: 0.08590622991323471\n",
      "Epoch 10716/30000 Training Loss: 0.07021205872297287\n",
      "Epoch 10717/30000 Training Loss: 0.08299176394939423\n",
      "Epoch 10718/30000 Training Loss: 0.08493577688932419\n",
      "Epoch 10719/30000 Training Loss: 0.07560994476079941\n",
      "Epoch 10720/30000 Training Loss: 0.07932314276695251\n",
      "Epoch 10720/30000 Validation Loss: 0.08863242715597153\n",
      "Epoch 10721/30000 Training Loss: 0.09995716065168381\n",
      "Epoch 10722/30000 Training Loss: 0.08160930126905441\n",
      "Epoch 10723/30000 Training Loss: 0.10182448476552963\n",
      "Epoch 10724/30000 Training Loss: 0.07702221721410751\n",
      "Epoch 10725/30000 Training Loss: 0.09367480129003525\n",
      "Epoch 10726/30000 Training Loss: 0.07794911414384842\n",
      "Epoch 10727/30000 Training Loss: 0.0860230028629303\n",
      "Epoch 10728/30000 Training Loss: 0.06526976078748703\n",
      "Epoch 10729/30000 Training Loss: 0.07854477316141129\n",
      "Epoch 10730/30000 Training Loss: 0.06819739937782288\n",
      "Epoch 10730/30000 Validation Loss: 0.06660289317369461\n",
      "Epoch 10731/30000 Training Loss: 0.07994159311056137\n",
      "Epoch 10732/30000 Training Loss: 0.08804026246070862\n",
      "Epoch 10733/30000 Training Loss: 0.06893846392631531\n",
      "Epoch 10734/30000 Training Loss: 0.07966437190771103\n",
      "Epoch 10735/30000 Training Loss: 0.08720610290765762\n",
      "Epoch 10736/30000 Training Loss: 0.08157586306333542\n",
      "Epoch 10737/30000 Training Loss: 0.08571963757276535\n",
      "Epoch 10738/30000 Training Loss: 0.10578889399766922\n",
      "Epoch 10739/30000 Training Loss: 0.07625647634267807\n",
      "Epoch 10740/30000 Training Loss: 0.08875203132629395\n",
      "Epoch 10740/30000 Validation Loss: 0.06542688608169556\n",
      "Epoch 10741/30000 Training Loss: 0.08755818754434586\n",
      "Epoch 10742/30000 Training Loss: 0.08238839358091354\n",
      "Epoch 10743/30000 Training Loss: 0.0911841168999672\n",
      "Epoch 10744/30000 Training Loss: 0.06310387700796127\n",
      "Epoch 10745/30000 Training Loss: 0.07515453547239304\n",
      "Epoch 10746/30000 Training Loss: 0.08958964794874191\n",
      "Epoch 10747/30000 Training Loss: 0.11493983864784241\n",
      "Epoch 10748/30000 Training Loss: 0.0742235854268074\n",
      "Epoch 10749/30000 Training Loss: 0.08570591360330582\n",
      "Epoch 10750/30000 Training Loss: 0.0693511962890625\n",
      "Epoch 10750/30000 Validation Loss: 0.06419909745454788\n",
      "Epoch 10751/30000 Training Loss: 0.08555188030004501\n",
      "Epoch 10752/30000 Training Loss: 0.08436354249715805\n",
      "Epoch 10753/30000 Training Loss: 0.12306097149848938\n",
      "Epoch 10754/30000 Training Loss: 0.09198716282844543\n",
      "Epoch 10755/30000 Training Loss: 0.07907110452651978\n",
      "Epoch 10756/30000 Training Loss: 0.08242640644311905\n",
      "Epoch 10757/30000 Training Loss: 0.07116235047578812\n",
      "Epoch 10758/30000 Training Loss: 0.0767950713634491\n",
      "Epoch 10759/30000 Training Loss: 0.07752040773630142\n",
      "Epoch 10760/30000 Training Loss: 0.0845782682299614\n",
      "Epoch 10760/30000 Validation Loss: 0.08191259950399399\n",
      "Epoch 10761/30000 Training Loss: 0.0737794041633606\n",
      "Epoch 10762/30000 Training Loss: 0.06959258764982224\n",
      "Epoch 10763/30000 Training Loss: 0.07998564094305038\n",
      "Epoch 10764/30000 Training Loss: 0.09073106199502945\n",
      "Epoch 10765/30000 Training Loss: 0.08086525648832321\n",
      "Epoch 10766/30000 Training Loss: 0.0927911028265953\n",
      "Epoch 10767/30000 Training Loss: 0.07576807588338852\n",
      "Epoch 10768/30000 Training Loss: 0.08589711040258408\n",
      "Epoch 10769/30000 Training Loss: 0.07161311060190201\n",
      "Epoch 10770/30000 Training Loss: 0.074927918612957\n",
      "Epoch 10770/30000 Validation Loss: 0.061792343854904175\n",
      "Epoch 10771/30000 Training Loss: 0.08122527599334717\n",
      "Epoch 10772/30000 Training Loss: 0.05584322288632393\n",
      "Epoch 10773/30000 Training Loss: 0.054032791405916214\n",
      "Epoch 10774/30000 Training Loss: 0.06156518682837486\n",
      "Epoch 10775/30000 Training Loss: 0.08310573548078537\n",
      "Epoch 10776/30000 Training Loss: 0.07356729358434677\n",
      "Epoch 10777/30000 Training Loss: 0.09088414162397385\n",
      "Epoch 10778/30000 Training Loss: 0.07308684289455414\n",
      "Epoch 10779/30000 Training Loss: 0.0606219619512558\n",
      "Epoch 10780/30000 Training Loss: 0.08290684223175049\n",
      "Epoch 10780/30000 Validation Loss: 0.07793107628822327\n",
      "Epoch 10781/30000 Training Loss: 0.07336368411779404\n",
      "Epoch 10782/30000 Training Loss: 0.08865594863891602\n",
      "Epoch 10783/30000 Training Loss: 0.08202645182609558\n",
      "Epoch 10784/30000 Training Loss: 0.08754187822341919\n",
      "Epoch 10785/30000 Training Loss: 0.07132312655448914\n",
      "Epoch 10786/30000 Training Loss: 0.0841219425201416\n",
      "Epoch 10787/30000 Training Loss: 0.07827233523130417\n",
      "Epoch 10788/30000 Training Loss: 0.07476434856653214\n",
      "Epoch 10789/30000 Training Loss: 0.08206295967102051\n",
      "Epoch 10790/30000 Training Loss: 0.1070331558585167\n",
      "Epoch 10790/30000 Validation Loss: 0.08365421742200851\n",
      "Epoch 10791/30000 Training Loss: 0.07526450604200363\n",
      "Epoch 10792/30000 Training Loss: 0.08311685919761658\n",
      "Epoch 10793/30000 Training Loss: 0.08305004239082336\n",
      "Epoch 10794/30000 Training Loss: 0.07446222752332687\n",
      "Epoch 10795/30000 Training Loss: 0.06883006542921066\n",
      "Epoch 10796/30000 Training Loss: 0.07444625347852707\n",
      "Epoch 10797/30000 Training Loss: 0.08080461621284485\n",
      "Epoch 10798/30000 Training Loss: 0.07471033185720444\n",
      "Epoch 10799/30000 Training Loss: 0.09011093527078629\n",
      "Epoch 10800/30000 Training Loss: 0.0874229371547699\n",
      "Epoch 10800/30000 Validation Loss: 0.08989176899194717\n",
      "Epoch 10801/30000 Training Loss: 0.09826558083295822\n",
      "Epoch 10802/30000 Training Loss: 0.088709257543087\n",
      "Epoch 10803/30000 Training Loss: 0.059898972511291504\n",
      "Epoch 10804/30000 Training Loss: 0.06787410378456116\n",
      "Epoch 10805/30000 Training Loss: 0.07964179664850235\n",
      "Epoch 10806/30000 Training Loss: 0.0685560405254364\n",
      "Epoch 10807/30000 Training Loss: 0.06239451840519905\n",
      "Epoch 10808/30000 Training Loss: 0.08350735902786255\n",
      "Epoch 10809/30000 Training Loss: 0.07069282978773117\n",
      "Epoch 10810/30000 Training Loss: 0.08702269941568375\n",
      "Epoch 10810/30000 Validation Loss: 0.09490102529525757\n",
      "Epoch 10811/30000 Training Loss: 0.07082797586917877\n",
      "Epoch 10812/30000 Training Loss: 0.08599268645048141\n",
      "Epoch 10813/30000 Training Loss: 0.05765169486403465\n",
      "Epoch 10814/30000 Training Loss: 0.09264979511499405\n",
      "Epoch 10815/30000 Training Loss: 0.06852453202009201\n",
      "Epoch 10816/30000 Training Loss: 0.09663515537977219\n",
      "Epoch 10817/30000 Training Loss: 0.07814829796552658\n",
      "Epoch 10818/30000 Training Loss: 0.06740754097700119\n",
      "Epoch 10819/30000 Training Loss: 0.08679483085870743\n",
      "Epoch 10820/30000 Training Loss: 0.0638328567147255\n",
      "Epoch 10820/30000 Validation Loss: 0.07056542485952377\n",
      "Epoch 10821/30000 Training Loss: 0.08764714002609253\n",
      "Epoch 10822/30000 Training Loss: 0.0895712673664093\n",
      "Epoch 10823/30000 Training Loss: 0.0599902980029583\n",
      "Epoch 10824/30000 Training Loss: 0.07821522653102875\n",
      "Epoch 10825/30000 Training Loss: 0.09009786695241928\n",
      "Epoch 10826/30000 Training Loss: 0.08586978912353516\n",
      "Epoch 10827/30000 Training Loss: 0.08771916478872299\n",
      "Epoch 10828/30000 Training Loss: 0.08017431944608688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10829/30000 Training Loss: 0.0730861946940422\n",
      "Epoch 10830/30000 Training Loss: 0.08404775708913803\n",
      "Epoch 10830/30000 Validation Loss: 0.0628742054104805\n",
      "Epoch 10831/30000 Training Loss: 0.0791037455201149\n",
      "Epoch 10832/30000 Training Loss: 0.08988964557647705\n",
      "Epoch 10833/30000 Training Loss: 0.07348576188087463\n",
      "Epoch 10834/30000 Training Loss: 0.07869550585746765\n",
      "Epoch 10835/30000 Training Loss: 0.0869322419166565\n",
      "Epoch 10836/30000 Training Loss: 0.07389196008443832\n",
      "Epoch 10837/30000 Training Loss: 0.0927538201212883\n",
      "Epoch 10838/30000 Training Loss: 0.07769826799631119\n",
      "Epoch 10839/30000 Training Loss: 0.07879770547151566\n",
      "Epoch 10840/30000 Training Loss: 0.0854906365275383\n",
      "Epoch 10840/30000 Validation Loss: 0.08825718611478806\n",
      "Epoch 10841/30000 Training Loss: 0.08836176991462708\n",
      "Epoch 10842/30000 Training Loss: 0.10473033040761948\n",
      "Epoch 10843/30000 Training Loss: 0.07014871388673782\n",
      "Epoch 10844/30000 Training Loss: 0.06789395958185196\n",
      "Epoch 10845/30000 Training Loss: 0.08067355304956436\n",
      "Epoch 10846/30000 Training Loss: 0.0708950087428093\n",
      "Epoch 10847/30000 Training Loss: 0.08800939470529556\n",
      "Epoch 10848/30000 Training Loss: 0.07697448879480362\n",
      "Epoch 10849/30000 Training Loss: 0.06383393704891205\n",
      "Epoch 10850/30000 Training Loss: 0.08030868321657181\n",
      "Epoch 10850/30000 Validation Loss: 0.08659762889146805\n",
      "Epoch 10851/30000 Training Loss: 0.08781534433364868\n",
      "Epoch 10852/30000 Training Loss: 0.09172075986862183\n",
      "Epoch 10853/30000 Training Loss: 0.07714063674211502\n",
      "Epoch 10854/30000 Training Loss: 0.08635743707418442\n",
      "Epoch 10855/30000 Training Loss: 0.07168572396039963\n",
      "Epoch 10856/30000 Training Loss: 0.09058848768472672\n",
      "Epoch 10857/30000 Training Loss: 0.07960615307092667\n",
      "Epoch 10858/30000 Training Loss: 0.06806086003780365\n",
      "Epoch 10859/30000 Training Loss: 0.07453697919845581\n",
      "Epoch 10860/30000 Training Loss: 0.09734171628952026\n",
      "Epoch 10860/30000 Validation Loss: 0.06409186869859695\n",
      "Epoch 10861/30000 Training Loss: 0.07163352519273758\n",
      "Epoch 10862/30000 Training Loss: 0.0851617231965065\n",
      "Epoch 10863/30000 Training Loss: 0.08850681781768799\n",
      "Epoch 10864/30000 Training Loss: 0.083197221159935\n",
      "Epoch 10865/30000 Training Loss: 0.06776992231607437\n",
      "Epoch 10866/30000 Training Loss: 0.07939960807561874\n",
      "Epoch 10867/30000 Training Loss: 0.07684408873319626\n",
      "Epoch 10868/30000 Training Loss: 0.07561299204826355\n",
      "Epoch 10869/30000 Training Loss: 0.08039508014917374\n",
      "Epoch 10870/30000 Training Loss: 0.08813860267400742\n",
      "Epoch 10870/30000 Validation Loss: 0.06861552596092224\n",
      "Epoch 10871/30000 Training Loss: 0.0846136137843132\n",
      "Epoch 10872/30000 Training Loss: 0.08223623782396317\n",
      "Epoch 10873/30000 Training Loss: 0.07680276036262512\n",
      "Epoch 10874/30000 Training Loss: 0.07409199327230453\n",
      "Epoch 10875/30000 Training Loss: 0.0807599350810051\n",
      "Epoch 10876/30000 Training Loss: 0.07457686215639114\n",
      "Epoch 10877/30000 Training Loss: 0.07308737188577652\n",
      "Epoch 10878/30000 Training Loss: 0.09928134828805923\n",
      "Epoch 10879/30000 Training Loss: 0.07829493284225464\n",
      "Epoch 10880/30000 Training Loss: 0.08959907293319702\n",
      "Epoch 10880/30000 Validation Loss: 0.06963425129652023\n",
      "Epoch 10881/30000 Training Loss: 0.0866294875741005\n",
      "Epoch 10882/30000 Training Loss: 0.07046078890562057\n",
      "Epoch 10883/30000 Training Loss: 0.07015126198530197\n",
      "Epoch 10884/30000 Training Loss: 0.06774065643548965\n",
      "Epoch 10885/30000 Training Loss: 0.0632714256644249\n",
      "Epoch 10886/30000 Training Loss: 0.06735506653785706\n",
      "Epoch 10887/30000 Training Loss: 0.07712230831384659\n",
      "Epoch 10888/30000 Training Loss: 0.07784724235534668\n",
      "Epoch 10889/30000 Training Loss: 0.06404642760753632\n",
      "Epoch 10890/30000 Training Loss: 0.08278194814920425\n",
      "Epoch 10890/30000 Validation Loss: 0.09252727776765823\n",
      "Epoch 10891/30000 Training Loss: 0.09818735718727112\n",
      "Epoch 10892/30000 Training Loss: 0.08133267611265182\n",
      "Epoch 10893/30000 Training Loss: 0.08915368467569351\n",
      "Epoch 10894/30000 Training Loss: 0.06454366445541382\n",
      "Epoch 10895/30000 Training Loss: 0.08225708454847336\n",
      "Epoch 10896/30000 Training Loss: 0.0848890021443367\n",
      "Epoch 10897/30000 Training Loss: 0.08164229989051819\n",
      "Epoch 10898/30000 Training Loss: 0.07221465557813644\n",
      "Epoch 10899/30000 Training Loss: 0.07943873852491379\n",
      "Epoch 10900/30000 Training Loss: 0.07172861695289612\n",
      "Epoch 10900/30000 Validation Loss: 0.07684657722711563\n",
      "Epoch 10901/30000 Training Loss: 0.06785804778337479\n",
      "Epoch 10902/30000 Training Loss: 0.101949542760849\n",
      "Epoch 10903/30000 Training Loss: 0.0628143772482872\n",
      "Epoch 10904/30000 Training Loss: 0.0747050866484642\n",
      "Epoch 10905/30000 Training Loss: 0.09517177194356918\n",
      "Epoch 10906/30000 Training Loss: 0.08562544733285904\n",
      "Epoch 10907/30000 Training Loss: 0.08379173278808594\n",
      "Epoch 10908/30000 Training Loss: 0.08083682507276535\n",
      "Epoch 10909/30000 Training Loss: 0.0820661187171936\n",
      "Epoch 10910/30000 Training Loss: 0.07299033552408218\n",
      "Epoch 10910/30000 Validation Loss: 0.09921202063560486\n",
      "Epoch 10911/30000 Training Loss: 0.07418236136436462\n",
      "Epoch 10912/30000 Training Loss: 0.07316463440656662\n",
      "Epoch 10913/30000 Training Loss: 0.07966718822717667\n",
      "Epoch 10914/30000 Training Loss: 0.11308228224515915\n",
      "Epoch 10915/30000 Training Loss: 0.08371182531118393\n",
      "Epoch 10916/30000 Training Loss: 0.07143223285675049\n",
      "Epoch 10917/30000 Training Loss: 0.08176698535680771\n",
      "Epoch 10918/30000 Training Loss: 0.0649663582444191\n",
      "Epoch 10919/30000 Training Loss: 0.06519883126020432\n",
      "Epoch 10920/30000 Training Loss: 0.10358398407697678\n",
      "Epoch 10920/30000 Validation Loss: 0.07226545363664627\n",
      "Epoch 10921/30000 Training Loss: 0.0757126584649086\n",
      "Epoch 10922/30000 Training Loss: 0.08675786107778549\n",
      "Epoch 10923/30000 Training Loss: 0.06438246369361877\n",
      "Epoch 10924/30000 Training Loss: 0.08869423717260361\n",
      "Epoch 10925/30000 Training Loss: 0.07722605019807816\n",
      "Epoch 10926/30000 Training Loss: 0.07018130272626877\n",
      "Epoch 10927/30000 Training Loss: 0.09305896610021591\n",
      "Epoch 10928/30000 Training Loss: 0.0884368047118187\n",
      "Epoch 10929/30000 Training Loss: 0.07345306873321533\n",
      "Epoch 10930/30000 Training Loss: 0.09079358726739883\n",
      "Epoch 10930/30000 Validation Loss: 0.08432786911725998\n",
      "Epoch 10931/30000 Training Loss: 0.09136846661567688\n",
      "Epoch 10932/30000 Training Loss: 0.06593682616949081\n",
      "Epoch 10933/30000 Training Loss: 0.06355530023574829\n",
      "Epoch 10934/30000 Training Loss: 0.06888117641210556\n",
      "Epoch 10935/30000 Training Loss: 0.0735984742641449\n",
      "Epoch 10936/30000 Training Loss: 0.06829800456762314\n",
      "Epoch 10937/30000 Training Loss: 0.08029945939779282\n",
      "Epoch 10938/30000 Training Loss: 0.08390925079584122\n",
      "Epoch 10939/30000 Training Loss: 0.06410100311040878\n",
      "Epoch 10940/30000 Training Loss: 0.0903397798538208\n",
      "Epoch 10940/30000 Validation Loss: 0.06307321041822433\n",
      "Epoch 10941/30000 Training Loss: 0.07762662321329117\n",
      "Epoch 10942/30000 Training Loss: 0.08377333730459213\n",
      "Epoch 10943/30000 Training Loss: 0.076083242893219\n",
      "Epoch 10944/30000 Training Loss: 0.07412140816450119\n",
      "Epoch 10945/30000 Training Loss: 0.07261615991592407\n",
      "Epoch 10946/30000 Training Loss: 0.0802219808101654\n",
      "Epoch 10947/30000 Training Loss: 0.08208417892456055\n",
      "Epoch 10948/30000 Training Loss: 0.1092657670378685\n",
      "Epoch 10949/30000 Training Loss: 0.0760682225227356\n",
      "Epoch 10950/30000 Training Loss: 0.07324188947677612\n",
      "Epoch 10950/30000 Validation Loss: 0.07947027683258057\n",
      "Epoch 10951/30000 Training Loss: 0.07066157460212708\n",
      "Epoch 10952/30000 Training Loss: 0.08905810862779617\n",
      "Epoch 10953/30000 Training Loss: 0.07869015634059906\n",
      "Epoch 10954/30000 Training Loss: 0.1115286648273468\n",
      "Epoch 10955/30000 Training Loss: 0.09860843420028687\n",
      "Epoch 10956/30000 Training Loss: 0.098839171230793\n",
      "Epoch 10957/30000 Training Loss: 0.07667288929224014\n",
      "Epoch 10958/30000 Training Loss: 0.08113034069538116\n",
      "Epoch 10959/30000 Training Loss: 0.06296422332525253\n",
      "Epoch 10960/30000 Training Loss: 0.0641866996884346\n",
      "Epoch 10960/30000 Validation Loss: 0.08199340850114822\n",
      "Epoch 10961/30000 Training Loss: 0.07763182371854782\n",
      "Epoch 10962/30000 Training Loss: 0.06783965975046158\n",
      "Epoch 10963/30000 Training Loss: 0.08340281993150711\n",
      "Epoch 10964/30000 Training Loss: 0.09429550170898438\n",
      "Epoch 10965/30000 Training Loss: 0.08382382243871689\n",
      "Epoch 10966/30000 Training Loss: 0.0742361843585968\n",
      "Epoch 10967/30000 Training Loss: 0.0932760164141655\n",
      "Epoch 10968/30000 Training Loss: 0.10862600803375244\n",
      "Epoch 10969/30000 Training Loss: 0.07662174850702286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10970/30000 Training Loss: 0.07992912083864212\n",
      "Epoch 10970/30000 Validation Loss: 0.08013686537742615\n",
      "Epoch 10971/30000 Training Loss: 0.06838944554328918\n",
      "Epoch 10972/30000 Training Loss: 0.07367783039808273\n",
      "Epoch 10973/30000 Training Loss: 0.09672113507986069\n",
      "Epoch 10974/30000 Training Loss: 0.07514068484306335\n",
      "Epoch 10975/30000 Training Loss: 0.08739674091339111\n",
      "Epoch 10976/30000 Training Loss: 0.07950367778539658\n",
      "Epoch 10977/30000 Training Loss: 0.07741571217775345\n",
      "Epoch 10978/30000 Training Loss: 0.06913217157125473\n",
      "Epoch 10979/30000 Training Loss: 0.10443434864282608\n",
      "Epoch 10980/30000 Training Loss: 0.07656443119049072\n",
      "Epoch 10980/30000 Validation Loss: 0.09032591432332993\n",
      "Epoch 10981/30000 Training Loss: 0.08093849569559097\n",
      "Epoch 10982/30000 Training Loss: 0.09309780597686768\n",
      "Epoch 10983/30000 Training Loss: 0.09424754232168198\n",
      "Epoch 10984/30000 Training Loss: 0.07837454229593277\n",
      "Epoch 10985/30000 Training Loss: 0.07699936628341675\n",
      "Epoch 10986/30000 Training Loss: 0.09859976172447205\n",
      "Epoch 10987/30000 Training Loss: 0.09695335477590561\n",
      "Epoch 10988/30000 Training Loss: 0.0827365443110466\n",
      "Epoch 10989/30000 Training Loss: 0.08499389886856079\n",
      "Epoch 10990/30000 Training Loss: 0.10900423675775528\n",
      "Epoch 10990/30000 Validation Loss: 0.09737551957368851\n",
      "Epoch 10991/30000 Training Loss: 0.0755697563290596\n",
      "Epoch 10992/30000 Training Loss: 0.09331252425909042\n",
      "Epoch 10993/30000 Training Loss: 0.08314628899097443\n",
      "Epoch 10994/30000 Training Loss: 0.0662771463394165\n",
      "Epoch 10995/30000 Training Loss: 0.08306235820055008\n",
      "Epoch 10996/30000 Training Loss: 0.07408457249403\n",
      "Epoch 10997/30000 Training Loss: 0.08418881148099899\n",
      "Epoch 10998/30000 Training Loss: 0.0717509463429451\n",
      "Epoch 10999/30000 Training Loss: 0.07593255490064621\n",
      "Epoch 11000/30000 Training Loss: 0.08582774549722672\n",
      "Epoch 11000/30000 Validation Loss: 0.06589531898498535\n",
      "Epoch 11001/30000 Training Loss: 0.054002463817596436\n",
      "Epoch 11002/30000 Training Loss: 0.076679527759552\n",
      "Epoch 11003/30000 Training Loss: 0.07729358226060867\n",
      "Epoch 11004/30000 Training Loss: 0.09150177985429764\n",
      "Epoch 11005/30000 Training Loss: 0.07489659637212753\n",
      "Epoch 11006/30000 Training Loss: 0.07542113959789276\n",
      "Epoch 11007/30000 Training Loss: 0.0758802518248558\n",
      "Epoch 11008/30000 Training Loss: 0.08877012133598328\n",
      "Epoch 11009/30000 Training Loss: 0.09388511627912521\n",
      "Epoch 11010/30000 Training Loss: 0.09816331416368484\n",
      "Epoch 11010/30000 Validation Loss: 0.0778285339474678\n",
      "Epoch 11011/30000 Training Loss: 0.07412076741456985\n",
      "Epoch 11012/30000 Training Loss: 0.11522195488214493\n",
      "Epoch 11013/30000 Training Loss: 0.06706533581018448\n",
      "Epoch 11014/30000 Training Loss: 0.08159177750349045\n",
      "Epoch 11015/30000 Training Loss: 0.09746727347373962\n",
      "Epoch 11016/30000 Training Loss: 0.06501327455043793\n",
      "Epoch 11017/30000 Training Loss: 0.06221495941281319\n",
      "Epoch 11018/30000 Training Loss: 0.07480242848396301\n",
      "Epoch 11019/30000 Training Loss: 0.08932307362556458\n",
      "Epoch 11020/30000 Training Loss: 0.09363511949777603\n",
      "Epoch 11020/30000 Validation Loss: 0.07698111236095428\n",
      "Epoch 11021/30000 Training Loss: 0.09587690234184265\n",
      "Epoch 11022/30000 Training Loss: 0.07519611716270447\n",
      "Epoch 11023/30000 Training Loss: 0.06778779625892639\n",
      "Epoch 11024/30000 Training Loss: 0.06899245828390121\n",
      "Epoch 11025/30000 Training Loss: 0.0816316083073616\n",
      "Epoch 11026/30000 Training Loss: 0.07882451266050339\n",
      "Epoch 11027/30000 Training Loss: 0.08561137318611145\n",
      "Epoch 11028/30000 Training Loss: 0.0746605172753334\n",
      "Epoch 11029/30000 Training Loss: 0.05961203575134277\n",
      "Epoch 11030/30000 Training Loss: 0.06943059712648392\n",
      "Epoch 11030/30000 Validation Loss: 0.07097373157739639\n",
      "Epoch 11031/30000 Training Loss: 0.06869634240865707\n",
      "Epoch 11032/30000 Training Loss: 0.06875529140233994\n",
      "Epoch 11033/30000 Training Loss: 0.0753367468714714\n",
      "Epoch 11034/30000 Training Loss: 0.1063007116317749\n",
      "Epoch 11035/30000 Training Loss: 0.10541566461324692\n",
      "Epoch 11036/30000 Training Loss: 0.08640462160110474\n",
      "Epoch 11037/30000 Training Loss: 0.07886940240859985\n",
      "Epoch 11038/30000 Training Loss: 0.07232299447059631\n",
      "Epoch 11039/30000 Training Loss: 0.07928746938705444\n",
      "Epoch 11040/30000 Training Loss: 0.08112071454524994\n",
      "Epoch 11040/30000 Validation Loss: 0.0781823918223381\n",
      "Epoch 11041/30000 Training Loss: 0.06849328428506851\n",
      "Epoch 11042/30000 Training Loss: 0.09372761845588684\n",
      "Epoch 11043/30000 Training Loss: 0.06538884341716766\n",
      "Epoch 11044/30000 Training Loss: 0.07091010361909866\n",
      "Epoch 11045/30000 Training Loss: 0.07929714769124985\n",
      "Epoch 11046/30000 Training Loss: 0.10341893881559372\n",
      "Epoch 11047/30000 Training Loss: 0.07453808933496475\n",
      "Epoch 11048/30000 Training Loss: 0.0782644972205162\n",
      "Epoch 11049/30000 Training Loss: 0.07797012478113174\n",
      "Epoch 11050/30000 Training Loss: 0.05903656780719757\n",
      "Epoch 11050/30000 Validation Loss: 0.07924149185419083\n",
      "Epoch 11051/30000 Training Loss: 0.09908651560544968\n",
      "Epoch 11052/30000 Training Loss: 0.09562018513679504\n",
      "Epoch 11053/30000 Training Loss: 0.06003336235880852\n",
      "Epoch 11054/30000 Training Loss: 0.08885884284973145\n",
      "Epoch 11055/30000 Training Loss: 0.08961024135351181\n",
      "Epoch 11056/30000 Training Loss: 0.0689915269613266\n",
      "Epoch 11057/30000 Training Loss: 0.06464282423257828\n",
      "Epoch 11058/30000 Training Loss: 0.09846899658441544\n",
      "Epoch 11059/30000 Training Loss: 0.08214590698480606\n",
      "Epoch 11060/30000 Training Loss: 0.08459475636482239\n",
      "Epoch 11060/30000 Validation Loss: 0.1019558534026146\n",
      "Epoch 11061/30000 Training Loss: 0.05667653679847717\n",
      "Epoch 11062/30000 Training Loss: 0.06566806882619858\n",
      "Epoch 11063/30000 Training Loss: 0.08533614873886108\n",
      "Epoch 11064/30000 Training Loss: 0.08591815084218979\n",
      "Epoch 11065/30000 Training Loss: 0.0957099124789238\n",
      "Epoch 11066/30000 Training Loss: 0.0876525416970253\n",
      "Epoch 11067/30000 Training Loss: 0.06785519421100616\n",
      "Epoch 11068/30000 Training Loss: 0.10511353611946106\n",
      "Epoch 11069/30000 Training Loss: 0.07655355334281921\n",
      "Epoch 11070/30000 Training Loss: 0.0844188928604126\n",
      "Epoch 11070/30000 Validation Loss: 0.07598903775215149\n",
      "Epoch 11071/30000 Training Loss: 0.0890590026974678\n",
      "Epoch 11072/30000 Training Loss: 0.07446218281984329\n",
      "Epoch 11073/30000 Training Loss: 0.09635436534881592\n",
      "Epoch 11074/30000 Training Loss: 0.10085395723581314\n",
      "Epoch 11075/30000 Training Loss: 0.07851624488830566\n",
      "Epoch 11076/30000 Training Loss: 0.08362209051847458\n",
      "Epoch 11077/30000 Training Loss: 0.08209971338510513\n",
      "Epoch 11078/30000 Training Loss: 0.09339046478271484\n",
      "Epoch 11079/30000 Training Loss: 0.06758033484220505\n",
      "Epoch 11080/30000 Training Loss: 0.05813354253768921\n",
      "Epoch 11080/30000 Validation Loss: 0.0758092924952507\n",
      "Epoch 11081/30000 Training Loss: 0.0797746554017067\n",
      "Epoch 11082/30000 Training Loss: 0.08523079007863998\n",
      "Epoch 11083/30000 Training Loss: 0.07248035073280334\n",
      "Epoch 11084/30000 Training Loss: 0.0889001116156578\n",
      "Epoch 11085/30000 Training Loss: 0.07213859260082245\n",
      "Epoch 11086/30000 Training Loss: 0.07183372974395752\n",
      "Epoch 11087/30000 Training Loss: 0.07927052676677704\n",
      "Epoch 11088/30000 Training Loss: 0.06509023159742355\n",
      "Epoch 11089/30000 Training Loss: 0.07131343334913254\n",
      "Epoch 11090/30000 Training Loss: 0.07833211869001389\n",
      "Epoch 11090/30000 Validation Loss: 0.08588773012161255\n",
      "Epoch 11091/30000 Training Loss: 0.07485786080360413\n",
      "Epoch 11092/30000 Training Loss: 0.07266592234373093\n",
      "Epoch 11093/30000 Training Loss: 0.07588901370763779\n",
      "Epoch 11094/30000 Training Loss: 0.10820278525352478\n",
      "Epoch 11095/30000 Training Loss: 0.08846565335988998\n",
      "Epoch 11096/30000 Training Loss: 0.07416466623544693\n",
      "Epoch 11097/30000 Training Loss: 0.08099251240491867\n",
      "Epoch 11098/30000 Training Loss: 0.06772018224000931\n",
      "Epoch 11099/30000 Training Loss: 0.08399540930986404\n",
      "Epoch 11100/30000 Training Loss: 0.06806456297636032\n",
      "Epoch 11100/30000 Validation Loss: 0.08930013328790665\n",
      "Epoch 11101/30000 Training Loss: 0.08943933248519897\n",
      "Epoch 11102/30000 Training Loss: 0.12298784404993057\n",
      "Epoch 11103/30000 Training Loss: 0.07677366584539413\n",
      "Epoch 11104/30000 Training Loss: 0.10275187343358994\n",
      "Epoch 11105/30000 Training Loss: 0.07575950771570206\n",
      "Epoch 11106/30000 Training Loss: 0.0875902846455574\n",
      "Epoch 11107/30000 Training Loss: 0.06708908081054688\n",
      "Epoch 11108/30000 Training Loss: 0.07136347144842148\n",
      "Epoch 11109/30000 Training Loss: 0.06711917370557785\n",
      "Epoch 11110/30000 Training Loss: 0.07203986495733261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11110/30000 Validation Loss: 0.07994047552347183\n",
      "Epoch 11111/30000 Training Loss: 0.0878620371222496\n",
      "Epoch 11112/30000 Training Loss: 0.09372114390134811\n",
      "Epoch 11113/30000 Training Loss: 0.07207479327917099\n",
      "Epoch 11114/30000 Training Loss: 0.115936778485775\n",
      "Epoch 11115/30000 Training Loss: 0.06951484829187393\n",
      "Epoch 11116/30000 Training Loss: 0.08551889657974243\n",
      "Epoch 11117/30000 Training Loss: 0.0948517844080925\n",
      "Epoch 11118/30000 Training Loss: 0.08468478173017502\n",
      "Epoch 11119/30000 Training Loss: 0.09695854038000107\n",
      "Epoch 11120/30000 Training Loss: 0.09350347518920898\n",
      "Epoch 11120/30000 Validation Loss: 0.07040556520223618\n",
      "Epoch 11121/30000 Training Loss: 0.07310012727975845\n",
      "Epoch 11122/30000 Training Loss: 0.07869206368923187\n",
      "Epoch 11123/30000 Training Loss: 0.08500538021326065\n",
      "Epoch 11124/30000 Training Loss: 0.08672618865966797\n",
      "Epoch 11125/30000 Training Loss: 0.07417628169059753\n",
      "Epoch 11126/30000 Training Loss: 0.08641261607408524\n",
      "Epoch 11127/30000 Training Loss: 0.06963410973548889\n",
      "Epoch 11128/30000 Training Loss: 0.09090778231620789\n",
      "Epoch 11129/30000 Training Loss: 0.09527549892663956\n",
      "Epoch 11130/30000 Training Loss: 0.07832091301679611\n",
      "Epoch 11130/30000 Validation Loss: 0.08538558334112167\n",
      "Epoch 11131/30000 Training Loss: 0.07103344053030014\n",
      "Epoch 11132/30000 Training Loss: 0.0933142676949501\n",
      "Epoch 11133/30000 Training Loss: 0.06027479097247124\n",
      "Epoch 11134/30000 Training Loss: 0.06436648219823837\n",
      "Epoch 11135/30000 Training Loss: 0.061811719089746475\n",
      "Epoch 11136/30000 Training Loss: 0.05987156927585602\n",
      "Epoch 11137/30000 Training Loss: 0.07388979941606522\n",
      "Epoch 11138/30000 Training Loss: 0.06384878605604172\n",
      "Epoch 11139/30000 Training Loss: 0.06704451888799667\n",
      "Epoch 11140/30000 Training Loss: 0.07419738173484802\n",
      "Epoch 11140/30000 Validation Loss: 0.08656439930200577\n",
      "Epoch 11141/30000 Training Loss: 0.08805954456329346\n",
      "Epoch 11142/30000 Training Loss: 0.08279889076948166\n",
      "Epoch 11143/30000 Training Loss: 0.08681287616491318\n",
      "Epoch 11144/30000 Training Loss: 0.07900235056877136\n",
      "Epoch 11145/30000 Training Loss: 0.06538756936788559\n",
      "Epoch 11146/30000 Training Loss: 0.0901411771774292\n",
      "Epoch 11147/30000 Training Loss: 0.0823926031589508\n",
      "Epoch 11148/30000 Training Loss: 0.06485667079687119\n",
      "Epoch 11149/30000 Training Loss: 0.08532927185297012\n",
      "Epoch 11150/30000 Training Loss: 0.08271719515323639\n",
      "Epoch 11150/30000 Validation Loss: 0.08526432514190674\n",
      "Epoch 11151/30000 Training Loss: 0.06841838359832764\n",
      "Epoch 11152/30000 Training Loss: 0.07861372083425522\n",
      "Epoch 11153/30000 Training Loss: 0.07769408077001572\n",
      "Epoch 11154/30000 Training Loss: 0.09693419188261032\n",
      "Epoch 11155/30000 Training Loss: 0.09120399504899979\n",
      "Epoch 11156/30000 Training Loss: 0.05896560847759247\n",
      "Epoch 11157/30000 Training Loss: 0.07712149620056152\n",
      "Epoch 11158/30000 Training Loss: 0.08042630553245544\n",
      "Epoch 11159/30000 Training Loss: 0.07388389855623245\n",
      "Epoch 11160/30000 Training Loss: 0.092945396900177\n",
      "Epoch 11160/30000 Validation Loss: 0.10489537566900253\n",
      "Epoch 11161/30000 Training Loss: 0.08330447971820831\n",
      "Epoch 11162/30000 Training Loss: 0.08722743391990662\n",
      "Epoch 11163/30000 Training Loss: 0.09492296725511551\n",
      "Epoch 11164/30000 Training Loss: 0.07163702696561813\n",
      "Epoch 11165/30000 Training Loss: 0.05799265205860138\n",
      "Epoch 11166/30000 Training Loss: 0.09166350215673447\n",
      "Epoch 11167/30000 Training Loss: 0.08727264404296875\n",
      "Epoch 11168/30000 Training Loss: 0.0705031231045723\n",
      "Epoch 11169/30000 Training Loss: 0.0812578871846199\n",
      "Epoch 11170/30000 Training Loss: 0.08107172697782516\n",
      "Epoch 11170/30000 Validation Loss: 0.08276399970054626\n",
      "Epoch 11171/30000 Training Loss: 0.0748998299241066\n",
      "Epoch 11172/30000 Training Loss: 0.07799337059259415\n",
      "Epoch 11173/30000 Training Loss: 0.0851350799202919\n",
      "Epoch 11174/30000 Training Loss: 0.07487260550260544\n",
      "Epoch 11175/30000 Training Loss: 0.05821828171610832\n",
      "Epoch 11176/30000 Training Loss: 0.07416628301143646\n",
      "Epoch 11177/30000 Training Loss: 0.08566158264875412\n",
      "Epoch 11178/30000 Training Loss: 0.07593945413827896\n",
      "Epoch 11179/30000 Training Loss: 0.08807820081710815\n",
      "Epoch 11180/30000 Training Loss: 0.07904096692800522\n",
      "Epoch 11180/30000 Validation Loss: 0.07968338578939438\n",
      "Epoch 11181/30000 Training Loss: 0.0677998811006546\n",
      "Epoch 11182/30000 Training Loss: 0.08590910583734512\n",
      "Epoch 11183/30000 Training Loss: 0.08459919691085815\n",
      "Epoch 11184/30000 Training Loss: 0.06501371413469315\n",
      "Epoch 11185/30000 Training Loss: 0.07969462871551514\n",
      "Epoch 11186/30000 Training Loss: 0.08831114321947098\n",
      "Epoch 11187/30000 Training Loss: 0.08252368867397308\n",
      "Epoch 11188/30000 Training Loss: 0.11215963214635849\n",
      "Epoch 11189/30000 Training Loss: 0.0702613964676857\n",
      "Epoch 11190/30000 Training Loss: 0.08930542320013046\n",
      "Epoch 11190/30000 Validation Loss: 0.07208862900733948\n",
      "Epoch 11191/30000 Training Loss: 0.058706577867269516\n",
      "Epoch 11192/30000 Training Loss: 0.08530604094266891\n",
      "Epoch 11193/30000 Training Loss: 0.06183556839823723\n",
      "Epoch 11194/30000 Training Loss: 0.0635330080986023\n",
      "Epoch 11195/30000 Training Loss: 0.09743335843086243\n",
      "Epoch 11196/30000 Training Loss: 0.0938575267791748\n",
      "Epoch 11197/30000 Training Loss: 0.0940932035446167\n",
      "Epoch 11198/30000 Training Loss: 0.07351317256689072\n",
      "Epoch 11199/30000 Training Loss: 0.060881104320287704\n",
      "Epoch 11200/30000 Training Loss: 0.07800719141960144\n",
      "Epoch 11200/30000 Validation Loss: 0.09877609461545944\n",
      "Epoch 11201/30000 Training Loss: 0.08649256825447083\n",
      "Epoch 11202/30000 Training Loss: 0.07646780461072922\n",
      "Epoch 11203/30000 Training Loss: 0.07448849081993103\n",
      "Epoch 11204/30000 Training Loss: 0.08660415560007095\n",
      "Epoch 11205/30000 Training Loss: 0.08386135846376419\n",
      "Epoch 11206/30000 Training Loss: 0.08290070295333862\n",
      "Epoch 11207/30000 Training Loss: 0.09938118606805801\n",
      "Epoch 11208/30000 Training Loss: 0.07959932088851929\n",
      "Epoch 11209/30000 Training Loss: 0.06518618017435074\n",
      "Epoch 11210/30000 Training Loss: 0.0705351009964943\n",
      "Epoch 11210/30000 Validation Loss: 0.10577218979597092\n",
      "Epoch 11211/30000 Training Loss: 0.08739341050386429\n",
      "Epoch 11212/30000 Training Loss: 0.08688610792160034\n",
      "Epoch 11213/30000 Training Loss: 0.0687469020485878\n",
      "Epoch 11214/30000 Training Loss: 0.08038270473480225\n",
      "Epoch 11215/30000 Training Loss: 0.0886630043387413\n",
      "Epoch 11216/30000 Training Loss: 0.07824481278657913\n",
      "Epoch 11217/30000 Training Loss: 0.10108885169029236\n",
      "Epoch 11218/30000 Training Loss: 0.07858303934335709\n",
      "Epoch 11219/30000 Training Loss: 0.0713767483830452\n",
      "Epoch 11220/30000 Training Loss: 0.07343077659606934\n",
      "Epoch 11220/30000 Validation Loss: 0.08424483984708786\n",
      "Epoch 11221/30000 Training Loss: 0.09199603646993637\n",
      "Epoch 11222/30000 Training Loss: 0.07152969390153885\n",
      "Epoch 11223/30000 Training Loss: 0.07070230692625046\n",
      "Epoch 11224/30000 Training Loss: 0.07354751229286194\n",
      "Epoch 11225/30000 Training Loss: 0.08905058354139328\n",
      "Epoch 11226/30000 Training Loss: 0.07730623334646225\n",
      "Epoch 11227/30000 Training Loss: 0.07172096520662308\n",
      "Epoch 11228/30000 Training Loss: 0.07423976808786392\n",
      "Epoch 11229/30000 Training Loss: 0.06846042722463608\n",
      "Epoch 11230/30000 Training Loss: 0.06710151582956314\n",
      "Epoch 11230/30000 Validation Loss: 0.07239603251218796\n",
      "Epoch 11231/30000 Training Loss: 0.0926586464047432\n",
      "Epoch 11232/30000 Training Loss: 0.07626566290855408\n",
      "Epoch 11233/30000 Training Loss: 0.0888979509472847\n",
      "Epoch 11234/30000 Training Loss: 0.06365925818681717\n",
      "Epoch 11235/30000 Training Loss: 0.07150285691022873\n",
      "Epoch 11236/30000 Training Loss: 0.07205241173505783\n",
      "Epoch 11237/30000 Training Loss: 0.059510961174964905\n",
      "Epoch 11238/30000 Training Loss: 0.07013764977455139\n",
      "Epoch 11239/30000 Training Loss: 0.07387907058000565\n",
      "Epoch 11240/30000 Training Loss: 0.08074257522821426\n",
      "Epoch 11240/30000 Validation Loss: 0.07292274385690689\n",
      "Epoch 11241/30000 Training Loss: 0.06802057474851608\n",
      "Epoch 11242/30000 Training Loss: 0.08695606142282486\n",
      "Epoch 11243/30000 Training Loss: 0.08634575456380844\n",
      "Epoch 11244/30000 Training Loss: 0.10070430487394333\n",
      "Epoch 11245/30000 Training Loss: 0.05644267797470093\n",
      "Epoch 11246/30000 Training Loss: 0.07257234305143356\n",
      "Epoch 11247/30000 Training Loss: 0.07836753875017166\n",
      "Epoch 11248/30000 Training Loss: 0.07397543638944626\n",
      "Epoch 11249/30000 Training Loss: 0.07844970375299454\n",
      "Epoch 11250/30000 Training Loss: 0.08605774492025375\n",
      "Epoch 11250/30000 Validation Loss: 0.09191328287124634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11251/30000 Training Loss: 0.07942220568656921\n",
      "Epoch 11252/30000 Training Loss: 0.09660650044679642\n",
      "Epoch 11253/30000 Training Loss: 0.08948934078216553\n",
      "Epoch 11254/30000 Training Loss: 0.09898865222930908\n",
      "Epoch 11255/30000 Training Loss: 0.06634965538978577\n",
      "Epoch 11256/30000 Training Loss: 0.09172894805669785\n",
      "Epoch 11257/30000 Training Loss: 0.05888959765434265\n",
      "Epoch 11258/30000 Training Loss: 0.11234944313764572\n",
      "Epoch 11259/30000 Training Loss: 0.08183401077985764\n",
      "Epoch 11260/30000 Training Loss: 0.07767456769943237\n",
      "Epoch 11260/30000 Validation Loss: 0.06635117530822754\n",
      "Epoch 11261/30000 Training Loss: 0.08328311890363693\n",
      "Epoch 11262/30000 Training Loss: 0.06761512160301208\n",
      "Epoch 11263/30000 Training Loss: 0.0826684907078743\n",
      "Epoch 11264/30000 Training Loss: 0.08004698157310486\n",
      "Epoch 11265/30000 Training Loss: 0.07317683100700378\n",
      "Epoch 11266/30000 Training Loss: 0.07786843180656433\n",
      "Epoch 11267/30000 Training Loss: 0.07376840710639954\n",
      "Epoch 11268/30000 Training Loss: 0.07210677117109299\n",
      "Epoch 11269/30000 Training Loss: 0.0679711326956749\n",
      "Epoch 11270/30000 Training Loss: 0.07686402648687363\n",
      "Epoch 11270/30000 Validation Loss: 0.07311739772558212\n",
      "Epoch 11271/30000 Training Loss: 0.07138916105031967\n",
      "Epoch 11272/30000 Training Loss: 0.07727689296007156\n",
      "Epoch 11273/30000 Training Loss: 0.0748659148812294\n",
      "Epoch 11274/30000 Training Loss: 0.08921747654676437\n",
      "Epoch 11275/30000 Training Loss: 0.07435944676399231\n",
      "Epoch 11276/30000 Training Loss: 0.08212951570749283\n",
      "Epoch 11277/30000 Training Loss: 0.07828190922737122\n",
      "Epoch 11278/30000 Training Loss: 0.07844100147485733\n",
      "Epoch 11279/30000 Training Loss: 0.08326030522584915\n",
      "Epoch 11280/30000 Training Loss: 0.1098470464348793\n",
      "Epoch 11280/30000 Validation Loss: 0.08439850807189941\n",
      "Epoch 11281/30000 Training Loss: 0.06296743452548981\n",
      "Epoch 11282/30000 Training Loss: 0.07176854461431503\n",
      "Epoch 11283/30000 Training Loss: 0.07328382134437561\n",
      "Epoch 11284/30000 Training Loss: 0.07101491838693619\n",
      "Epoch 11285/30000 Training Loss: 0.07929366081953049\n",
      "Epoch 11286/30000 Training Loss: 0.08050975203514099\n",
      "Epoch 11287/30000 Training Loss: 0.0710272267460823\n",
      "Epoch 11288/30000 Training Loss: 0.08458686619997025\n",
      "Epoch 11289/30000 Training Loss: 0.07779175043106079\n",
      "Epoch 11290/30000 Training Loss: 0.08271043747663498\n",
      "Epoch 11290/30000 Validation Loss: 0.09230604022741318\n",
      "Epoch 11291/30000 Training Loss: 0.06502122431993484\n",
      "Epoch 11292/30000 Training Loss: 0.08837002515792847\n",
      "Epoch 11293/30000 Training Loss: 0.08523339033126831\n",
      "Epoch 11294/30000 Training Loss: 0.0832790732383728\n",
      "Epoch 11295/30000 Training Loss: 0.08843151479959488\n",
      "Epoch 11296/30000 Training Loss: 0.08591920137405396\n",
      "Epoch 11297/30000 Training Loss: 0.06768642365932465\n",
      "Epoch 11298/30000 Training Loss: 0.07470285147428513\n",
      "Epoch 11299/30000 Training Loss: 0.0789712443947792\n",
      "Epoch 11300/30000 Training Loss: 0.0839398130774498\n",
      "Epoch 11300/30000 Validation Loss: 0.06642353534698486\n",
      "Epoch 11301/30000 Training Loss: 0.08914458751678467\n",
      "Epoch 11302/30000 Training Loss: 0.07062605768442154\n",
      "Epoch 11303/30000 Training Loss: 0.0763717070221901\n",
      "Epoch 11304/30000 Training Loss: 0.08445706218481064\n",
      "Epoch 11305/30000 Training Loss: 0.08939532190561295\n",
      "Epoch 11306/30000 Training Loss: 0.0855395495891571\n",
      "Epoch 11307/30000 Training Loss: 0.06402280181646347\n",
      "Epoch 11308/30000 Training Loss: 0.07811883091926575\n",
      "Epoch 11309/30000 Training Loss: 0.094945527613163\n",
      "Epoch 11310/30000 Training Loss: 0.07870350033044815\n",
      "Epoch 11310/30000 Validation Loss: 0.0655609592795372\n",
      "Epoch 11311/30000 Training Loss: 0.06105651333928108\n",
      "Epoch 11312/30000 Training Loss: 0.06697631627321243\n",
      "Epoch 11313/30000 Training Loss: 0.09673114866018295\n",
      "Epoch 11314/30000 Training Loss: 0.058984797447919846\n",
      "Epoch 11315/30000 Training Loss: 0.06700149178504944\n",
      "Epoch 11316/30000 Training Loss: 0.11080848425626755\n",
      "Epoch 11317/30000 Training Loss: 0.0846240296959877\n",
      "Epoch 11318/30000 Training Loss: 0.07614176720380783\n",
      "Epoch 11319/30000 Training Loss: 0.093454509973526\n",
      "Epoch 11320/30000 Training Loss: 0.07047729939222336\n",
      "Epoch 11320/30000 Validation Loss: 0.07233899086713791\n",
      "Epoch 11321/30000 Training Loss: 0.06418466567993164\n",
      "Epoch 11322/30000 Training Loss: 0.08752808719873428\n",
      "Epoch 11323/30000 Training Loss: 0.07436206191778183\n",
      "Epoch 11324/30000 Training Loss: 0.08722414821386337\n",
      "Epoch 11325/30000 Training Loss: 0.07841920852661133\n",
      "Epoch 11326/30000 Training Loss: 0.0836385115981102\n",
      "Epoch 11327/30000 Training Loss: 0.06900214403867722\n",
      "Epoch 11328/30000 Training Loss: 0.09528625011444092\n",
      "Epoch 11329/30000 Training Loss: 0.06936705112457275\n",
      "Epoch 11330/30000 Training Loss: 0.06347153335809708\n",
      "Epoch 11330/30000 Validation Loss: 0.0934608206152916\n",
      "Epoch 11331/30000 Training Loss: 0.10455916076898575\n",
      "Epoch 11332/30000 Training Loss: 0.07243660092353821\n",
      "Epoch 11333/30000 Training Loss: 0.08090225607156754\n",
      "Epoch 11334/30000 Training Loss: 0.06561172008514404\n",
      "Epoch 11335/30000 Training Loss: 0.08158397674560547\n",
      "Epoch 11336/30000 Training Loss: 0.08866297453641891\n",
      "Epoch 11337/30000 Training Loss: 0.07396090775728226\n",
      "Epoch 11338/30000 Training Loss: 0.07575974613428116\n",
      "Epoch 11339/30000 Training Loss: 0.08581341058015823\n",
      "Epoch 11340/30000 Training Loss: 0.0868518128991127\n",
      "Epoch 11340/30000 Validation Loss: 0.07683923095464706\n",
      "Epoch 11341/30000 Training Loss: 0.07262088358402252\n",
      "Epoch 11342/30000 Training Loss: 0.08599540591239929\n",
      "Epoch 11343/30000 Training Loss: 0.0864107608795166\n",
      "Epoch 11344/30000 Training Loss: 0.08037188649177551\n",
      "Epoch 11345/30000 Training Loss: 0.08376643061637878\n",
      "Epoch 11346/30000 Training Loss: 0.08255130797624588\n",
      "Epoch 11347/30000 Training Loss: 0.06825058907270432\n",
      "Epoch 11348/30000 Training Loss: 0.08246936649084091\n",
      "Epoch 11349/30000 Training Loss: 0.10163616389036179\n",
      "Epoch 11350/30000 Training Loss: 0.07470187544822693\n",
      "Epoch 11350/30000 Validation Loss: 0.0759241133928299\n",
      "Epoch 11351/30000 Training Loss: 0.06396406143903732\n",
      "Epoch 11352/30000 Training Loss: 0.07882962375879288\n",
      "Epoch 11353/30000 Training Loss: 0.06594083458185196\n",
      "Epoch 11354/30000 Training Loss: 0.07448860257863998\n",
      "Epoch 11355/30000 Training Loss: 0.08620084077119827\n",
      "Epoch 11356/30000 Training Loss: 0.09581632167100906\n",
      "Epoch 11357/30000 Training Loss: 0.07523634284734726\n",
      "Epoch 11358/30000 Training Loss: 0.10301876813173294\n",
      "Epoch 11359/30000 Training Loss: 0.08880230784416199\n",
      "Epoch 11360/30000 Training Loss: 0.0856223925948143\n",
      "Epoch 11360/30000 Validation Loss: 0.0679612010717392\n",
      "Epoch 11361/30000 Training Loss: 0.08055196702480316\n",
      "Epoch 11362/30000 Training Loss: 0.0936194583773613\n",
      "Epoch 11363/30000 Training Loss: 0.0776335671544075\n",
      "Epoch 11364/30000 Training Loss: 0.0634504035115242\n",
      "Epoch 11365/30000 Training Loss: 0.09328008443117142\n",
      "Epoch 11366/30000 Training Loss: 0.08633401989936829\n",
      "Epoch 11367/30000 Training Loss: 0.06549079716205597\n",
      "Epoch 11368/30000 Training Loss: 0.08422154188156128\n",
      "Epoch 11369/30000 Training Loss: 0.08596670627593994\n",
      "Epoch 11370/30000 Training Loss: 0.0744391456246376\n",
      "Epoch 11370/30000 Validation Loss: 0.07836412638425827\n",
      "Epoch 11371/30000 Training Loss: 0.08574086427688599\n",
      "Epoch 11372/30000 Training Loss: 0.08765774965286255\n",
      "Epoch 11373/30000 Training Loss: 0.07348065823316574\n",
      "Epoch 11374/30000 Training Loss: 0.10614398866891861\n",
      "Epoch 11375/30000 Training Loss: 0.07104336470365524\n",
      "Epoch 11376/30000 Training Loss: 0.06290008872747421\n",
      "Epoch 11377/30000 Training Loss: 0.087445467710495\n",
      "Epoch 11378/30000 Training Loss: 0.08350246399641037\n",
      "Epoch 11379/30000 Training Loss: 0.0738675519824028\n",
      "Epoch 11380/30000 Training Loss: 0.06294237822294235\n",
      "Epoch 11380/30000 Validation Loss: 0.06932581216096878\n",
      "Epoch 11381/30000 Training Loss: 0.07269402593374252\n",
      "Epoch 11382/30000 Training Loss: 0.07382574677467346\n",
      "Epoch 11383/30000 Training Loss: 0.07170223444700241\n",
      "Epoch 11384/30000 Training Loss: 0.08460482209920883\n",
      "Epoch 11385/30000 Training Loss: 0.07286233454942703\n",
      "Epoch 11386/30000 Training Loss: 0.10805294662714005\n",
      "Epoch 11387/30000 Training Loss: 0.06752932071685791\n",
      "Epoch 11388/30000 Training Loss: 0.07734599709510803\n",
      "Epoch 11389/30000 Training Loss: 0.07318945974111557\n",
      "Epoch 11390/30000 Training Loss: 0.07211858779191971\n",
      "Epoch 11390/30000 Validation Loss: 0.09039900451898575\n",
      "Epoch 11391/30000 Training Loss: 0.10102430731058121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11392/30000 Training Loss: 0.09979641437530518\n",
      "Epoch 11393/30000 Training Loss: 0.09601038694381714\n",
      "Epoch 11394/30000 Training Loss: 0.09301570802927017\n",
      "Epoch 11395/30000 Training Loss: 0.0784822627902031\n",
      "Epoch 11396/30000 Training Loss: 0.07176796346902847\n",
      "Epoch 11397/30000 Training Loss: 0.07657602429389954\n",
      "Epoch 11398/30000 Training Loss: 0.09377270936965942\n",
      "Epoch 11399/30000 Training Loss: 0.0729605033993721\n",
      "Epoch 11400/30000 Training Loss: 0.09440290927886963\n",
      "Epoch 11400/30000 Validation Loss: 0.06422433257102966\n",
      "Epoch 11401/30000 Training Loss: 0.06386251747608185\n",
      "Epoch 11402/30000 Training Loss: 0.08940327167510986\n",
      "Epoch 11403/30000 Training Loss: 0.09709218889474869\n",
      "Epoch 11404/30000 Training Loss: 0.07857582718133926\n",
      "Epoch 11405/30000 Training Loss: 0.06851542741060257\n",
      "Epoch 11406/30000 Training Loss: 0.06185275316238403\n",
      "Epoch 11407/30000 Training Loss: 0.08687881380319595\n",
      "Epoch 11408/30000 Training Loss: 0.07221640646457672\n",
      "Epoch 11409/30000 Training Loss: 0.07741393893957138\n",
      "Epoch 11410/30000 Training Loss: 0.08315995335578918\n",
      "Epoch 11410/30000 Validation Loss: 0.07197015732526779\n",
      "Epoch 11411/30000 Training Loss: 0.07223793864250183\n",
      "Epoch 11412/30000 Training Loss: 0.07006857544183731\n",
      "Epoch 11413/30000 Training Loss: 0.08371394127607346\n",
      "Epoch 11414/30000 Training Loss: 0.08851832896471024\n",
      "Epoch 11415/30000 Training Loss: 0.0684775859117508\n",
      "Epoch 11416/30000 Training Loss: 0.08508798480033875\n",
      "Epoch 11417/30000 Training Loss: 0.07991069555282593\n",
      "Epoch 11418/30000 Training Loss: 0.07827063649892807\n",
      "Epoch 11419/30000 Training Loss: 0.06517734378576279\n",
      "Epoch 11420/30000 Training Loss: 0.07527215033769608\n",
      "Epoch 11420/30000 Validation Loss: 0.08680034428834915\n",
      "Epoch 11421/30000 Training Loss: 0.08016929030418396\n",
      "Epoch 11422/30000 Training Loss: 0.06878919154405594\n",
      "Epoch 11423/30000 Training Loss: 0.07529261708259583\n",
      "Epoch 11424/30000 Training Loss: 0.0981123149394989\n",
      "Epoch 11425/30000 Training Loss: 0.09137202054262161\n",
      "Epoch 11426/30000 Training Loss: 0.06910888105630875\n",
      "Epoch 11427/30000 Training Loss: 0.08351868391036987\n",
      "Epoch 11428/30000 Training Loss: 0.0944204106926918\n",
      "Epoch 11429/30000 Training Loss: 0.08043556660413742\n",
      "Epoch 11430/30000 Training Loss: 0.07361786812543869\n",
      "Epoch 11430/30000 Validation Loss: 0.0915273055434227\n",
      "Epoch 11431/30000 Training Loss: 0.06688039004802704\n",
      "Epoch 11432/30000 Training Loss: 0.061731237918138504\n",
      "Epoch 11433/30000 Training Loss: 0.08595173805952072\n",
      "Epoch 11434/30000 Training Loss: 0.0820220336318016\n",
      "Epoch 11435/30000 Training Loss: 0.07472427934408188\n",
      "Epoch 11436/30000 Training Loss: 0.07773154973983765\n",
      "Epoch 11437/30000 Training Loss: 0.08999359607696533\n",
      "Epoch 11438/30000 Training Loss: 0.059185583144426346\n",
      "Epoch 11439/30000 Training Loss: 0.06211163103580475\n",
      "Epoch 11440/30000 Training Loss: 0.07967539876699448\n",
      "Epoch 11440/30000 Validation Loss: 0.085047148168087\n",
      "Epoch 11441/30000 Training Loss: 0.08539532870054245\n",
      "Epoch 11442/30000 Training Loss: 0.0715254470705986\n",
      "Epoch 11443/30000 Training Loss: 0.08845164626836777\n",
      "Epoch 11444/30000 Training Loss: 0.07866755872964859\n",
      "Epoch 11445/30000 Training Loss: 0.07762802392244339\n",
      "Epoch 11446/30000 Training Loss: 0.07472597807645798\n",
      "Epoch 11447/30000 Training Loss: 0.07558684796094894\n",
      "Epoch 11448/30000 Training Loss: 0.06964728236198425\n",
      "Epoch 11449/30000 Training Loss: 0.08864563703536987\n",
      "Epoch 11450/30000 Training Loss: 0.08204235881567001\n",
      "Epoch 11450/30000 Validation Loss: 0.06152729690074921\n",
      "Epoch 11451/30000 Training Loss: 0.08991458266973495\n",
      "Epoch 11452/30000 Training Loss: 0.07497220486402512\n",
      "Epoch 11453/30000 Training Loss: 0.08155196905136108\n",
      "Epoch 11454/30000 Training Loss: 0.0842321515083313\n",
      "Epoch 11455/30000 Training Loss: 0.0902852937579155\n",
      "Epoch 11456/30000 Training Loss: 0.07124286890029907\n",
      "Epoch 11457/30000 Training Loss: 0.07165790349245071\n",
      "Epoch 11458/30000 Training Loss: 0.0893435850739479\n",
      "Epoch 11459/30000 Training Loss: 0.07952763140201569\n",
      "Epoch 11460/30000 Training Loss: 0.09909948706626892\n",
      "Epoch 11460/30000 Validation Loss: 0.06466270238161087\n",
      "Epoch 11461/30000 Training Loss: 0.06841013580560684\n",
      "Epoch 11462/30000 Training Loss: 0.08253234624862671\n",
      "Epoch 11463/30000 Training Loss: 0.07980363816022873\n",
      "Epoch 11464/30000 Training Loss: 0.10850086063146591\n",
      "Epoch 11465/30000 Training Loss: 0.07683469355106354\n",
      "Epoch 11466/30000 Training Loss: 0.06786134093999863\n",
      "Epoch 11467/30000 Training Loss: 0.08138889819383621\n",
      "Epoch 11468/30000 Training Loss: 0.08438533544540405\n",
      "Epoch 11469/30000 Training Loss: 0.08110254257917404\n",
      "Epoch 11470/30000 Training Loss: 0.08899015188217163\n",
      "Epoch 11470/30000 Validation Loss: 0.10968021303415298\n",
      "Epoch 11471/30000 Training Loss: 0.08003322035074234\n",
      "Epoch 11472/30000 Training Loss: 0.08025606721639633\n",
      "Epoch 11473/30000 Training Loss: 0.07031137496232986\n",
      "Epoch 11474/30000 Training Loss: 0.07543089240789413\n",
      "Epoch 11475/30000 Training Loss: 0.08934595435857773\n",
      "Epoch 11476/30000 Training Loss: 0.08404351025819778\n",
      "Epoch 11477/30000 Training Loss: 0.08090471476316452\n",
      "Epoch 11478/30000 Training Loss: 0.08725475519895554\n",
      "Epoch 11479/30000 Training Loss: 0.0656232163310051\n",
      "Epoch 11480/30000 Training Loss: 0.07917142659425735\n",
      "Epoch 11480/30000 Validation Loss: 0.0798477753996849\n",
      "Epoch 11481/30000 Training Loss: 0.08706856518983841\n",
      "Epoch 11482/30000 Training Loss: 0.08237268030643463\n",
      "Epoch 11483/30000 Training Loss: 0.07182847708463669\n",
      "Epoch 11484/30000 Training Loss: 0.06322623044252396\n",
      "Epoch 11485/30000 Training Loss: 0.06886457651853561\n",
      "Epoch 11486/30000 Training Loss: 0.08353332430124283\n",
      "Epoch 11487/30000 Training Loss: 0.06866198778152466\n",
      "Epoch 11488/30000 Training Loss: 0.07337303459644318\n",
      "Epoch 11489/30000 Training Loss: 0.07884667068719864\n",
      "Epoch 11490/30000 Training Loss: 0.06844953447580338\n",
      "Epoch 11490/30000 Validation Loss: 0.07450559735298157\n",
      "Epoch 11491/30000 Training Loss: 0.06758919358253479\n",
      "Epoch 11492/30000 Training Loss: 0.0829525962471962\n",
      "Epoch 11493/30000 Training Loss: 0.07987674325704575\n",
      "Epoch 11494/30000 Training Loss: 0.09236570447683334\n",
      "Epoch 11495/30000 Training Loss: 0.0930417850613594\n",
      "Epoch 11496/30000 Training Loss: 0.10096559673547745\n",
      "Epoch 11497/30000 Training Loss: 0.06783527880907059\n",
      "Epoch 11498/30000 Training Loss: 0.07804016768932343\n",
      "Epoch 11499/30000 Training Loss: 0.07143756002187729\n",
      "Epoch 11500/30000 Training Loss: 0.07146788388490677\n",
      "Epoch 11500/30000 Validation Loss: 0.07198605686426163\n",
      "Epoch 11501/30000 Training Loss: 0.08024480193853378\n",
      "Epoch 11502/30000 Training Loss: 0.08921249955892563\n",
      "Epoch 11503/30000 Training Loss: 0.09829046577215195\n",
      "Epoch 11504/30000 Training Loss: 0.08859533071517944\n",
      "Epoch 11505/30000 Training Loss: 0.07025936990976334\n",
      "Epoch 11506/30000 Training Loss: 0.05778851732611656\n",
      "Epoch 11507/30000 Training Loss: 0.08016422390937805\n",
      "Epoch 11508/30000 Training Loss: 0.05968287214636803\n",
      "Epoch 11509/30000 Training Loss: 0.06622901558876038\n",
      "Epoch 11510/30000 Training Loss: 0.08652124553918839\n",
      "Epoch 11510/30000 Validation Loss: 0.08750823140144348\n",
      "Epoch 11511/30000 Training Loss: 0.07314826548099518\n",
      "Epoch 11512/30000 Training Loss: 0.07390464842319489\n",
      "Epoch 11513/30000 Training Loss: 0.0732525885105133\n",
      "Epoch 11514/30000 Training Loss: 0.07711920887231827\n",
      "Epoch 11515/30000 Training Loss: 0.06598291546106339\n",
      "Epoch 11516/30000 Training Loss: 0.09105408191680908\n",
      "Epoch 11517/30000 Training Loss: 0.06967825442552567\n",
      "Epoch 11518/30000 Training Loss: 0.067738838493824\n",
      "Epoch 11519/30000 Training Loss: 0.07381734251976013\n",
      "Epoch 11520/30000 Training Loss: 0.09708687663078308\n",
      "Epoch 11520/30000 Validation Loss: 0.09101453423500061\n",
      "Epoch 11521/30000 Training Loss: 0.09643343836069107\n",
      "Epoch 11522/30000 Training Loss: 0.06983176618814468\n",
      "Epoch 11523/30000 Training Loss: 0.09117438644170761\n",
      "Epoch 11524/30000 Training Loss: 0.0925377830862999\n",
      "Epoch 11525/30000 Training Loss: 0.09411284327507019\n",
      "Epoch 11526/30000 Training Loss: 0.07098527997732162\n",
      "Epoch 11527/30000 Training Loss: 0.07172119617462158\n",
      "Epoch 11528/30000 Training Loss: 0.07176528126001358\n",
      "Epoch 11529/30000 Training Loss: 0.07644513249397278\n",
      "Epoch 11530/30000 Training Loss: 0.08670506626367569\n",
      "Epoch 11530/30000 Validation Loss: 0.06830836087465286\n",
      "Epoch 11531/30000 Training Loss: 0.06817930191755295\n",
      "Epoch 11532/30000 Training Loss: 0.07015625387430191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11533/30000 Training Loss: 0.09654910117387772\n",
      "Epoch 11534/30000 Training Loss: 0.07293546944856644\n",
      "Epoch 11535/30000 Training Loss: 0.0642789751291275\n",
      "Epoch 11536/30000 Training Loss: 0.08723678439855576\n",
      "Epoch 11537/30000 Training Loss: 0.0790989026427269\n",
      "Epoch 11538/30000 Training Loss: 0.08449582010507584\n",
      "Epoch 11539/30000 Training Loss: 0.0775047168135643\n",
      "Epoch 11540/30000 Training Loss: 0.07297716289758682\n",
      "Epoch 11540/30000 Validation Loss: 0.10357227176427841\n",
      "Epoch 11541/30000 Training Loss: 0.07751592993736267\n",
      "Epoch 11542/30000 Training Loss: 0.07600780576467514\n",
      "Epoch 11543/30000 Training Loss: 0.09137596935033798\n",
      "Epoch 11544/30000 Training Loss: 0.07442193478345871\n",
      "Epoch 11545/30000 Training Loss: 0.08630264550447464\n",
      "Epoch 11546/30000 Training Loss: 0.07613588869571686\n",
      "Epoch 11547/30000 Training Loss: 0.08096795529127121\n",
      "Epoch 11548/30000 Training Loss: 0.06993657350540161\n",
      "Epoch 11549/30000 Training Loss: 0.0633644163608551\n",
      "Epoch 11550/30000 Training Loss: 0.09422394633293152\n",
      "Epoch 11550/30000 Validation Loss: 0.0744243785738945\n",
      "Epoch 11551/30000 Training Loss: 0.10666976124048233\n",
      "Epoch 11552/30000 Training Loss: 0.06653153896331787\n",
      "Epoch 11553/30000 Training Loss: 0.09294752031564713\n",
      "Epoch 11554/30000 Training Loss: 0.07244495302438736\n",
      "Epoch 11555/30000 Training Loss: 0.08898677676916122\n",
      "Epoch 11556/30000 Training Loss: 0.07563114911317825\n",
      "Epoch 11557/30000 Training Loss: 0.08177206665277481\n",
      "Epoch 11558/30000 Training Loss: 0.07407268136739731\n",
      "Epoch 11559/30000 Training Loss: 0.07728654891252518\n",
      "Epoch 11560/30000 Training Loss: 0.07996531575918198\n",
      "Epoch 11560/30000 Validation Loss: 0.07733216881752014\n",
      "Epoch 11561/30000 Training Loss: 0.060854773968458176\n",
      "Epoch 11562/30000 Training Loss: 0.08429937809705734\n",
      "Epoch 11563/30000 Training Loss: 0.07523488998413086\n",
      "Epoch 11564/30000 Training Loss: 0.08567322045564651\n",
      "Epoch 11565/30000 Training Loss: 0.08752813190221786\n",
      "Epoch 11566/30000 Training Loss: 0.08817515522241592\n",
      "Epoch 11567/30000 Training Loss: 0.09907340258359909\n",
      "Epoch 11568/30000 Training Loss: 0.08543619513511658\n",
      "Epoch 11569/30000 Training Loss: 0.09929889440536499\n",
      "Epoch 11570/30000 Training Loss: 0.07866007834672928\n",
      "Epoch 11570/30000 Validation Loss: 0.09151927381753922\n",
      "Epoch 11571/30000 Training Loss: 0.0952892079949379\n",
      "Epoch 11572/30000 Training Loss: 0.08368254452943802\n",
      "Epoch 11573/30000 Training Loss: 0.07622143626213074\n",
      "Epoch 11574/30000 Training Loss: 0.07599631696939468\n",
      "Epoch 11575/30000 Training Loss: 0.09149140119552612\n",
      "Epoch 11576/30000 Training Loss: 0.08414236456155777\n",
      "Epoch 11577/30000 Training Loss: 0.06483521312475204\n",
      "Epoch 11578/30000 Training Loss: 0.08795124292373657\n",
      "Epoch 11579/30000 Training Loss: 0.08762452751398087\n",
      "Epoch 11580/30000 Training Loss: 0.08513552695512772\n",
      "Epoch 11580/30000 Validation Loss: 0.08910566568374634\n",
      "Epoch 11581/30000 Training Loss: 0.08557840436697006\n",
      "Epoch 11582/30000 Training Loss: 0.07171013206243515\n",
      "Epoch 11583/30000 Training Loss: 0.08852650970220566\n",
      "Epoch 11584/30000 Training Loss: 0.09657403081655502\n",
      "Epoch 11585/30000 Training Loss: 0.06552135944366455\n",
      "Epoch 11586/30000 Training Loss: 0.06560830026865005\n",
      "Epoch 11587/30000 Training Loss: 0.07799773663282394\n",
      "Epoch 11588/30000 Training Loss: 0.06882034987211227\n",
      "Epoch 11589/30000 Training Loss: 0.08445560187101364\n",
      "Epoch 11590/30000 Training Loss: 0.0669652447104454\n",
      "Epoch 11590/30000 Validation Loss: 0.06459149718284607\n",
      "Epoch 11591/30000 Training Loss: 0.07166027277708054\n",
      "Epoch 11592/30000 Training Loss: 0.07680683583021164\n",
      "Epoch 11593/30000 Training Loss: 0.08085048198699951\n",
      "Epoch 11594/30000 Training Loss: 0.07895021885633469\n",
      "Epoch 11595/30000 Training Loss: 0.0787254348397255\n",
      "Epoch 11596/30000 Training Loss: 0.07829905301332474\n",
      "Epoch 11597/30000 Training Loss: 0.07324475795030594\n",
      "Epoch 11598/30000 Training Loss: 0.08270405977964401\n",
      "Epoch 11599/30000 Training Loss: 0.07811252027750015\n",
      "Epoch 11600/30000 Training Loss: 0.08302607387304306\n",
      "Epoch 11600/30000 Validation Loss: 0.07404886931180954\n",
      "Epoch 11601/30000 Training Loss: 0.0925506055355072\n",
      "Epoch 11602/30000 Training Loss: 0.06557814031839371\n",
      "Epoch 11603/30000 Training Loss: 0.07641740888357162\n",
      "Epoch 11604/30000 Training Loss: 0.06833749264478683\n",
      "Epoch 11605/30000 Training Loss: 0.07914986461400986\n",
      "Epoch 11606/30000 Training Loss: 0.07665534317493439\n",
      "Epoch 11607/30000 Training Loss: 0.09282267093658447\n",
      "Epoch 11608/30000 Training Loss: 0.09309975057840347\n",
      "Epoch 11609/30000 Training Loss: 0.07975286990404129\n",
      "Epoch 11610/30000 Training Loss: 0.08115608245134354\n",
      "Epoch 11610/30000 Validation Loss: 0.06785421818494797\n",
      "Epoch 11611/30000 Training Loss: 0.08051179349422455\n",
      "Epoch 11612/30000 Training Loss: 0.0867372676730156\n",
      "Epoch 11613/30000 Training Loss: 0.0726064145565033\n",
      "Epoch 11614/30000 Training Loss: 0.07153577357530594\n",
      "Epoch 11615/30000 Training Loss: 0.09210425615310669\n",
      "Epoch 11616/30000 Training Loss: 0.08172095566987991\n",
      "Epoch 11617/30000 Training Loss: 0.08932798355817795\n",
      "Epoch 11618/30000 Training Loss: 0.09343980997800827\n",
      "Epoch 11619/30000 Training Loss: 0.07501693814992905\n",
      "Epoch 11620/30000 Training Loss: 0.06645804643630981\n",
      "Epoch 11620/30000 Validation Loss: 0.07226704806089401\n",
      "Epoch 11621/30000 Training Loss: 0.08720452338457108\n",
      "Epoch 11622/30000 Training Loss: 0.06540609151124954\n",
      "Epoch 11623/30000 Training Loss: 0.0998116135597229\n",
      "Epoch 11624/30000 Training Loss: 0.09723586589097977\n",
      "Epoch 11625/30000 Training Loss: 0.07708776742219925\n",
      "Epoch 11626/30000 Training Loss: 0.06774138659238815\n",
      "Epoch 11627/30000 Training Loss: 0.07012322545051575\n",
      "Epoch 11628/30000 Training Loss: 0.06977228075265884\n",
      "Epoch 11629/30000 Training Loss: 0.0643404871225357\n",
      "Epoch 11630/30000 Training Loss: 0.06578914821147919\n",
      "Epoch 11630/30000 Validation Loss: 0.07768350839614868\n",
      "Epoch 11631/30000 Training Loss: 0.09766260534524918\n",
      "Epoch 11632/30000 Training Loss: 0.07703766971826553\n",
      "Epoch 11633/30000 Training Loss: 0.08406580239534378\n",
      "Epoch 11634/30000 Training Loss: 0.08048047870397568\n",
      "Epoch 11635/30000 Training Loss: 0.08080079406499863\n",
      "Epoch 11636/30000 Training Loss: 0.08572987467050552\n",
      "Epoch 11637/30000 Training Loss: 0.09831004589796066\n",
      "Epoch 11638/30000 Training Loss: 0.08103304356336594\n",
      "Epoch 11639/30000 Training Loss: 0.11172294616699219\n",
      "Epoch 11640/30000 Training Loss: 0.08460459113121033\n",
      "Epoch 11640/30000 Validation Loss: 0.07054588198661804\n",
      "Epoch 11641/30000 Training Loss: 0.07468000799417496\n",
      "Epoch 11642/30000 Training Loss: 0.08043870329856873\n",
      "Epoch 11643/30000 Training Loss: 0.06994994729757309\n",
      "Epoch 11644/30000 Training Loss: 0.06787380576133728\n",
      "Epoch 11645/30000 Training Loss: 0.08137229830026627\n",
      "Epoch 11646/30000 Training Loss: 0.07783225178718567\n",
      "Epoch 11647/30000 Training Loss: 0.09961781650781631\n",
      "Epoch 11648/30000 Training Loss: 0.08161275088787079\n",
      "Epoch 11649/30000 Training Loss: 0.09011819213628769\n",
      "Epoch 11650/30000 Training Loss: 0.08214551955461502\n",
      "Epoch 11650/30000 Validation Loss: 0.07699916511774063\n",
      "Epoch 11651/30000 Training Loss: 0.07518421858549118\n",
      "Epoch 11652/30000 Training Loss: 0.06813994795084\n",
      "Epoch 11653/30000 Training Loss: 0.0634404718875885\n",
      "Epoch 11654/30000 Training Loss: 0.08907758444547653\n",
      "Epoch 11655/30000 Training Loss: 0.0802970603108406\n",
      "Epoch 11656/30000 Training Loss: 0.07518402487039566\n",
      "Epoch 11657/30000 Training Loss: 0.08862337470054626\n",
      "Epoch 11658/30000 Training Loss: 0.08101654797792435\n",
      "Epoch 11659/30000 Training Loss: 0.07974683493375778\n",
      "Epoch 11660/30000 Training Loss: 0.08919765800237656\n",
      "Epoch 11660/30000 Validation Loss: 0.08155704289674759\n",
      "Epoch 11661/30000 Training Loss: 0.08975211530923843\n",
      "Epoch 11662/30000 Training Loss: 0.07834506779909134\n",
      "Epoch 11663/30000 Training Loss: 0.07470666617155075\n",
      "Epoch 11664/30000 Training Loss: 0.06473148614168167\n",
      "Epoch 11665/30000 Training Loss: 0.06927942484617233\n",
      "Epoch 11666/30000 Training Loss: 0.06411454826593399\n",
      "Epoch 11667/30000 Training Loss: 0.08889049291610718\n",
      "Epoch 11668/30000 Training Loss: 0.0717778354883194\n",
      "Epoch 11669/30000 Training Loss: 0.08104122430086136\n",
      "Epoch 11670/30000 Training Loss: 0.0711066946387291\n",
      "Epoch 11670/30000 Validation Loss: 0.07840608805418015\n",
      "Epoch 11671/30000 Training Loss: 0.07896287739276886\n",
      "Epoch 11672/30000 Training Loss: 0.08276012539863586\n",
      "Epoch 11673/30000 Training Loss: 0.08994048833847046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11674/30000 Training Loss: 0.0712965652346611\n",
      "Epoch 11675/30000 Training Loss: 0.08340618759393692\n",
      "Epoch 11676/30000 Training Loss: 0.08321917057037354\n",
      "Epoch 11677/30000 Training Loss: 0.09416928887367249\n",
      "Epoch 11678/30000 Training Loss: 0.06168816611170769\n",
      "Epoch 11679/30000 Training Loss: 0.100240558385849\n",
      "Epoch 11680/30000 Training Loss: 0.07775061577558517\n",
      "Epoch 11680/30000 Validation Loss: 0.08273009210824966\n",
      "Epoch 11681/30000 Training Loss: 0.07664976269006729\n",
      "Epoch 11682/30000 Training Loss: 0.08158203959465027\n",
      "Epoch 11683/30000 Training Loss: 0.07599327713251114\n",
      "Epoch 11684/30000 Training Loss: 0.08508097380399704\n",
      "Epoch 11685/30000 Training Loss: 0.07419347018003464\n",
      "Epoch 11686/30000 Training Loss: 0.0830608382821083\n",
      "Epoch 11687/30000 Training Loss: 0.07024582475423813\n",
      "Epoch 11688/30000 Training Loss: 0.08716615289449692\n",
      "Epoch 11689/30000 Training Loss: 0.06503740698099136\n",
      "Epoch 11690/30000 Training Loss: 0.09404239803552628\n",
      "Epoch 11690/30000 Validation Loss: 0.07829773426055908\n",
      "Epoch 11691/30000 Training Loss: 0.06769012659788132\n",
      "Epoch 11692/30000 Training Loss: 0.08775924891233444\n",
      "Epoch 11693/30000 Training Loss: 0.06473278254270554\n",
      "Epoch 11694/30000 Training Loss: 0.08241987973451614\n",
      "Epoch 11695/30000 Training Loss: 0.06638801842927933\n",
      "Epoch 11696/30000 Training Loss: 0.10078559070825577\n",
      "Epoch 11697/30000 Training Loss: 0.06109239533543587\n",
      "Epoch 11698/30000 Training Loss: 0.07902447134256363\n",
      "Epoch 11699/30000 Training Loss: 0.09799899905920029\n",
      "Epoch 11700/30000 Training Loss: 0.07449594885110855\n",
      "Epoch 11700/30000 Validation Loss: 0.09460736066102982\n",
      "Epoch 11701/30000 Training Loss: 0.09112323075532913\n",
      "Epoch 11702/30000 Training Loss: 0.07754261046648026\n",
      "Epoch 11703/30000 Training Loss: 0.0811021625995636\n",
      "Epoch 11704/30000 Training Loss: 0.06412603706121445\n",
      "Epoch 11705/30000 Training Loss: 0.09010978788137436\n",
      "Epoch 11706/30000 Training Loss: 0.06489615887403488\n",
      "Epoch 11707/30000 Training Loss: 0.07637959718704224\n",
      "Epoch 11708/30000 Training Loss: 0.07526666671037674\n",
      "Epoch 11709/30000 Training Loss: 0.07507701963186264\n",
      "Epoch 11710/30000 Training Loss: 0.061490606516599655\n",
      "Epoch 11710/30000 Validation Loss: 0.08635496348142624\n",
      "Epoch 11711/30000 Training Loss: 0.06897034496068954\n",
      "Epoch 11712/30000 Training Loss: 0.08024585247039795\n",
      "Epoch 11713/30000 Training Loss: 0.07926946878433228\n",
      "Epoch 11714/30000 Training Loss: 0.09613052755594254\n",
      "Epoch 11715/30000 Training Loss: 0.08732318878173828\n",
      "Epoch 11716/30000 Training Loss: 0.07665345817804337\n",
      "Epoch 11717/30000 Training Loss: 0.08425991982221603\n",
      "Epoch 11718/30000 Training Loss: 0.07675782591104507\n",
      "Epoch 11719/30000 Training Loss: 0.06186354160308838\n",
      "Epoch 11720/30000 Training Loss: 0.08818662166595459\n",
      "Epoch 11720/30000 Validation Loss: 0.07526938617229462\n",
      "Epoch 11721/30000 Training Loss: 0.07510527223348618\n",
      "Epoch 11722/30000 Training Loss: 0.0711967945098877\n",
      "Epoch 11723/30000 Training Loss: 0.07614368945360184\n",
      "Epoch 11724/30000 Training Loss: 0.0643693283200264\n",
      "Epoch 11725/30000 Training Loss: 0.1012585237622261\n",
      "Epoch 11726/30000 Training Loss: 0.08505507558584213\n",
      "Epoch 11727/30000 Training Loss: 0.08345688134431839\n",
      "Epoch 11728/30000 Training Loss: 0.07267940789461136\n",
      "Epoch 11729/30000 Training Loss: 0.09085462242364883\n",
      "Epoch 11730/30000 Training Loss: 0.08917645364999771\n",
      "Epoch 11730/30000 Validation Loss: 0.09161005169153214\n",
      "Epoch 11731/30000 Training Loss: 0.0805700421333313\n",
      "Epoch 11732/30000 Training Loss: 0.07459568977355957\n",
      "Epoch 11733/30000 Training Loss: 0.059244304895401\n",
      "Epoch 11734/30000 Training Loss: 0.09051113575696945\n",
      "Epoch 11735/30000 Training Loss: 0.09466636925935745\n",
      "Epoch 11736/30000 Training Loss: 0.07079660147428513\n",
      "Epoch 11737/30000 Training Loss: 0.055154819041490555\n",
      "Epoch 11738/30000 Training Loss: 0.07465089112520218\n",
      "Epoch 11739/30000 Training Loss: 0.06557732820510864\n",
      "Epoch 11740/30000 Training Loss: 0.07766062766313553\n",
      "Epoch 11740/30000 Validation Loss: 0.09164568036794662\n",
      "Epoch 11741/30000 Training Loss: 0.07477647811174393\n",
      "Epoch 11742/30000 Training Loss: 0.07829534262418747\n",
      "Epoch 11743/30000 Training Loss: 0.0649438127875328\n",
      "Epoch 11744/30000 Training Loss: 0.05895833298563957\n",
      "Epoch 11745/30000 Training Loss: 0.0847751796245575\n",
      "Epoch 11746/30000 Training Loss: 0.07164687663316727\n",
      "Epoch 11747/30000 Training Loss: 0.07040301710367203\n",
      "Epoch 11748/30000 Training Loss: 0.12692351639270782\n",
      "Epoch 11749/30000 Training Loss: 0.08280833810567856\n",
      "Epoch 11750/30000 Training Loss: 0.0826396644115448\n",
      "Epoch 11750/30000 Validation Loss: 0.0820956826210022\n",
      "Epoch 11751/30000 Training Loss: 0.10533499717712402\n",
      "Epoch 11752/30000 Training Loss: 0.0619623102247715\n",
      "Epoch 11753/30000 Training Loss: 0.07879260182380676\n",
      "Epoch 11754/30000 Training Loss: 0.08536631613969803\n",
      "Epoch 11755/30000 Training Loss: 0.08524607867002487\n",
      "Epoch 11756/30000 Training Loss: 0.07440449297428131\n",
      "Epoch 11757/30000 Training Loss: 0.064650759100914\n",
      "Epoch 11758/30000 Training Loss: 0.08603767305612564\n",
      "Epoch 11759/30000 Training Loss: 0.07977469265460968\n",
      "Epoch 11760/30000 Training Loss: 0.06721433252096176\n",
      "Epoch 11760/30000 Validation Loss: 0.08222126215696335\n",
      "Epoch 11761/30000 Training Loss: 0.08633805066347122\n",
      "Epoch 11762/30000 Training Loss: 0.06566095352172852\n",
      "Epoch 11763/30000 Training Loss: 0.07280582189559937\n",
      "Epoch 11764/30000 Training Loss: 0.067564457654953\n",
      "Epoch 11765/30000 Training Loss: 0.07578019052743912\n",
      "Epoch 11766/30000 Training Loss: 0.09425107389688492\n",
      "Epoch 11767/30000 Training Loss: 0.09017428755760193\n",
      "Epoch 11768/30000 Training Loss: 0.07700549811124802\n",
      "Epoch 11769/30000 Training Loss: 0.06500373780727386\n",
      "Epoch 11770/30000 Training Loss: 0.08068981021642685\n",
      "Epoch 11770/30000 Validation Loss: 0.09258213639259338\n",
      "Epoch 11771/30000 Training Loss: 0.07745818048715591\n",
      "Epoch 11772/30000 Training Loss: 0.07209508866071701\n",
      "Epoch 11773/30000 Training Loss: 0.08249646425247192\n",
      "Epoch 11774/30000 Training Loss: 0.09892213344573975\n",
      "Epoch 11775/30000 Training Loss: 0.07567862421274185\n",
      "Epoch 11776/30000 Training Loss: 0.07821965217590332\n",
      "Epoch 11777/30000 Training Loss: 0.08333440870046616\n",
      "Epoch 11778/30000 Training Loss: 0.07077329605817795\n",
      "Epoch 11779/30000 Training Loss: 0.07236137241125107\n",
      "Epoch 11780/30000 Training Loss: 0.07493086904287338\n",
      "Epoch 11780/30000 Validation Loss: 0.08748567849397659\n",
      "Epoch 11781/30000 Training Loss: 0.07671698182821274\n",
      "Epoch 11782/30000 Training Loss: 0.07676618546247482\n",
      "Epoch 11783/30000 Training Loss: 0.07693339139223099\n",
      "Epoch 11784/30000 Training Loss: 0.055304188281297684\n",
      "Epoch 11785/30000 Training Loss: 0.08102098107337952\n",
      "Epoch 11786/30000 Training Loss: 0.10231829434633255\n",
      "Epoch 11787/30000 Training Loss: 0.08635661005973816\n",
      "Epoch 11788/30000 Training Loss: 0.08380790799856186\n",
      "Epoch 11789/30000 Training Loss: 0.09848645329475403\n",
      "Epoch 11790/30000 Training Loss: 0.09852772206068039\n",
      "Epoch 11790/30000 Validation Loss: 0.08954671770334244\n",
      "Epoch 11791/30000 Training Loss: 0.06384573131799698\n",
      "Epoch 11792/30000 Training Loss: 0.07657354325056076\n",
      "Epoch 11793/30000 Training Loss: 0.07681017369031906\n",
      "Epoch 11794/30000 Training Loss: 0.12104534357786179\n",
      "Epoch 11795/30000 Training Loss: 0.06969504058361053\n",
      "Epoch 11796/30000 Training Loss: 0.07358171790838242\n",
      "Epoch 11797/30000 Training Loss: 0.07141133397817612\n",
      "Epoch 11798/30000 Training Loss: 0.08242322504520416\n",
      "Epoch 11799/30000 Training Loss: 0.08695339411497116\n",
      "Epoch 11800/30000 Training Loss: 0.07303168624639511\n",
      "Epoch 11800/30000 Validation Loss: 0.06698551028966904\n",
      "Epoch 11801/30000 Training Loss: 0.08735936880111694\n",
      "Epoch 11802/30000 Training Loss: 0.09169463068246841\n",
      "Epoch 11803/30000 Training Loss: 0.06862201541662216\n",
      "Epoch 11804/30000 Training Loss: 0.08111222088336945\n",
      "Epoch 11805/30000 Training Loss: 0.08664775639772415\n",
      "Epoch 11806/30000 Training Loss: 0.10399537533521652\n",
      "Epoch 11807/30000 Training Loss: 0.05670279264450073\n",
      "Epoch 11808/30000 Training Loss: 0.07235889881849289\n",
      "Epoch 11809/30000 Training Loss: 0.08471280336380005\n",
      "Epoch 11810/30000 Training Loss: 0.09234542399644852\n",
      "Epoch 11810/30000 Validation Loss: 0.08023938536643982\n",
      "Epoch 11811/30000 Training Loss: 0.08542077988386154\n",
      "Epoch 11812/30000 Training Loss: 0.07372687011957169\n",
      "Epoch 11813/30000 Training Loss: 0.06839988380670547\n",
      "Epoch 11814/30000 Training Loss: 0.07161494344472885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11815/30000 Training Loss: 0.05856931582093239\n",
      "Epoch 11816/30000 Training Loss: 0.10505137592554092\n",
      "Epoch 11817/30000 Training Loss: 0.09587869048118591\n",
      "Epoch 11818/30000 Training Loss: 0.05920926854014397\n",
      "Epoch 11819/30000 Training Loss: 0.07905261963605881\n",
      "Epoch 11820/30000 Training Loss: 0.07424689084291458\n",
      "Epoch 11820/30000 Validation Loss: 0.08274274319410324\n",
      "Epoch 11821/30000 Training Loss: 0.08215124905109406\n",
      "Epoch 11822/30000 Training Loss: 0.08274304866790771\n",
      "Epoch 11823/30000 Training Loss: 0.08682504296302795\n",
      "Epoch 11824/30000 Training Loss: 0.0766180157661438\n",
      "Epoch 11825/30000 Training Loss: 0.0724460557103157\n",
      "Epoch 11826/30000 Training Loss: 0.07274070382118225\n",
      "Epoch 11827/30000 Training Loss: 0.09749593585729599\n",
      "Epoch 11828/30000 Training Loss: 0.08195814490318298\n",
      "Epoch 11829/30000 Training Loss: 0.07256696373224258\n",
      "Epoch 11830/30000 Training Loss: 0.07975918799638748\n",
      "Epoch 11830/30000 Validation Loss: 0.08178138732910156\n",
      "Epoch 11831/30000 Training Loss: 0.09618597477674484\n",
      "Epoch 11832/30000 Training Loss: 0.08420022577047348\n",
      "Epoch 11833/30000 Training Loss: 0.06705969572067261\n",
      "Epoch 11834/30000 Training Loss: 0.08223048597574234\n",
      "Epoch 11835/30000 Training Loss: 0.071274034678936\n",
      "Epoch 11836/30000 Training Loss: 0.08868816494941711\n",
      "Epoch 11837/30000 Training Loss: 0.07972019165754318\n",
      "Epoch 11838/30000 Training Loss: 0.07478171586990356\n",
      "Epoch 11839/30000 Training Loss: 0.09854190796613693\n",
      "Epoch 11840/30000 Training Loss: 0.0814751461148262\n",
      "Epoch 11840/30000 Validation Loss: 0.06631001085042953\n",
      "Epoch 11841/30000 Training Loss: 0.077847421169281\n",
      "Epoch 11842/30000 Training Loss: 0.07523413747549057\n",
      "Epoch 11843/30000 Training Loss: 0.07161984592676163\n",
      "Epoch 11844/30000 Training Loss: 0.08798447996377945\n",
      "Epoch 11845/30000 Training Loss: 0.0677419900894165\n",
      "Epoch 11846/30000 Training Loss: 0.0760129988193512\n",
      "Epoch 11847/30000 Training Loss: 0.06505372375249863\n",
      "Epoch 11848/30000 Training Loss: 0.08229400962591171\n",
      "Epoch 11849/30000 Training Loss: 0.09070237725973129\n",
      "Epoch 11850/30000 Training Loss: 0.06927895545959473\n",
      "Epoch 11850/30000 Validation Loss: 0.07872896641492844\n",
      "Epoch 11851/30000 Training Loss: 0.0832466408610344\n",
      "Epoch 11852/30000 Training Loss: 0.08385656028985977\n",
      "Epoch 11853/30000 Training Loss: 0.07829102128744125\n",
      "Epoch 11854/30000 Training Loss: 0.06736185401678085\n",
      "Epoch 11855/30000 Training Loss: 0.07070104032754898\n",
      "Epoch 11856/30000 Training Loss: 0.07745722681283951\n",
      "Epoch 11857/30000 Training Loss: 0.0739903450012207\n",
      "Epoch 11858/30000 Training Loss: 0.06661046296358109\n",
      "Epoch 11859/30000 Training Loss: 0.08661126345396042\n",
      "Epoch 11860/30000 Training Loss: 0.0777200311422348\n",
      "Epoch 11860/30000 Validation Loss: 0.0759940966963768\n",
      "Epoch 11861/30000 Training Loss: 0.05438476800918579\n",
      "Epoch 11862/30000 Training Loss: 0.08375490456819534\n",
      "Epoch 11863/30000 Training Loss: 0.07659366726875305\n",
      "Epoch 11864/30000 Training Loss: 0.06586519628763199\n",
      "Epoch 11865/30000 Training Loss: 0.07066718488931656\n",
      "Epoch 11866/30000 Training Loss: 0.08357051759958267\n",
      "Epoch 11867/30000 Training Loss: 0.07191396504640579\n",
      "Epoch 11868/30000 Training Loss: 0.07378876954317093\n",
      "Epoch 11869/30000 Training Loss: 0.08504223823547363\n",
      "Epoch 11870/30000 Training Loss: 0.06909102946519852\n",
      "Epoch 11870/30000 Validation Loss: 0.10464709252119064\n",
      "Epoch 11871/30000 Training Loss: 0.07921123504638672\n",
      "Epoch 11872/30000 Training Loss: 0.08698613196611404\n",
      "Epoch 11873/30000 Training Loss: 0.07202034443616867\n",
      "Epoch 11874/30000 Training Loss: 0.08477143198251724\n",
      "Epoch 11875/30000 Training Loss: 0.09258238226175308\n",
      "Epoch 11876/30000 Training Loss: 0.07903721183538437\n",
      "Epoch 11877/30000 Training Loss: 0.07595016807317734\n",
      "Epoch 11878/30000 Training Loss: 0.08832671493291855\n",
      "Epoch 11879/30000 Training Loss: 0.08148298412561417\n",
      "Epoch 11880/30000 Training Loss: 0.08803320676088333\n",
      "Epoch 11880/30000 Validation Loss: 0.0715569481253624\n",
      "Epoch 11881/30000 Training Loss: 0.0929163470864296\n",
      "Epoch 11882/30000 Training Loss: 0.09385401010513306\n",
      "Epoch 11883/30000 Training Loss: 0.08236419409513474\n",
      "Epoch 11884/30000 Training Loss: 0.07761353999376297\n",
      "Epoch 11885/30000 Training Loss: 0.08855520933866501\n",
      "Epoch 11886/30000 Training Loss: 0.07555725425481796\n",
      "Epoch 11887/30000 Training Loss: 0.05509252846240997\n",
      "Epoch 11888/30000 Training Loss: 0.08194414526224136\n",
      "Epoch 11889/30000 Training Loss: 0.0805993378162384\n",
      "Epoch 11890/30000 Training Loss: 0.07071276754140854\n",
      "Epoch 11890/30000 Validation Loss: 0.0713275894522667\n",
      "Epoch 11891/30000 Training Loss: 0.08630958944559097\n",
      "Epoch 11892/30000 Training Loss: 0.1063297763466835\n",
      "Epoch 11893/30000 Training Loss: 0.09242292493581772\n",
      "Epoch 11894/30000 Training Loss: 0.056730326265096664\n",
      "Epoch 11895/30000 Training Loss: 0.07557778805494308\n",
      "Epoch 11896/30000 Training Loss: 0.08608803898096085\n",
      "Epoch 11897/30000 Training Loss: 0.06698235124349594\n",
      "Epoch 11898/30000 Training Loss: 0.05593039095401764\n",
      "Epoch 11899/30000 Training Loss: 0.0842897891998291\n",
      "Epoch 11900/30000 Training Loss: 0.07283013314008713\n",
      "Epoch 11900/30000 Validation Loss: 0.0645437017083168\n",
      "Epoch 11901/30000 Training Loss: 0.0950419008731842\n",
      "Epoch 11902/30000 Training Loss: 0.08704841881990433\n",
      "Epoch 11903/30000 Training Loss: 0.09132646769285202\n",
      "Epoch 11904/30000 Training Loss: 0.08191364258527756\n",
      "Epoch 11905/30000 Training Loss: 0.07255525887012482\n",
      "Epoch 11906/30000 Training Loss: 0.08929052203893661\n",
      "Epoch 11907/30000 Training Loss: 0.06526585668325424\n",
      "Epoch 11908/30000 Training Loss: 0.06614304333925247\n",
      "Epoch 11909/30000 Training Loss: 0.07981104403734207\n",
      "Epoch 11910/30000 Training Loss: 0.0851849913597107\n",
      "Epoch 11910/30000 Validation Loss: 0.07776474207639694\n",
      "Epoch 11911/30000 Training Loss: 0.07702312618494034\n",
      "Epoch 11912/30000 Training Loss: 0.07068368792533875\n",
      "Epoch 11913/30000 Training Loss: 0.0719052106142044\n",
      "Epoch 11914/30000 Training Loss: 0.06810148805379868\n",
      "Epoch 11915/30000 Training Loss: 0.10301458090543747\n",
      "Epoch 11916/30000 Training Loss: 0.06491091847419739\n",
      "Epoch 11917/30000 Training Loss: 0.06312701106071472\n",
      "Epoch 11918/30000 Training Loss: 0.0627683773636818\n",
      "Epoch 11919/30000 Training Loss: 0.08892891556024551\n",
      "Epoch 11920/30000 Training Loss: 0.08565820008516312\n",
      "Epoch 11920/30000 Validation Loss: 0.0699974000453949\n",
      "Epoch 11921/30000 Training Loss: 0.08159919083118439\n",
      "Epoch 11922/30000 Training Loss: 0.07969566434621811\n",
      "Epoch 11923/30000 Training Loss: 0.08588556200265884\n",
      "Epoch 11924/30000 Training Loss: 0.07089073956012726\n",
      "Epoch 11925/30000 Training Loss: 0.07964787632226944\n",
      "Epoch 11926/30000 Training Loss: 0.0822518914937973\n",
      "Epoch 11927/30000 Training Loss: 0.08896335959434509\n",
      "Epoch 11928/30000 Training Loss: 0.0701533854007721\n",
      "Epoch 11929/30000 Training Loss: 0.07229393720626831\n",
      "Epoch 11930/30000 Training Loss: 0.07254508137702942\n",
      "Epoch 11930/30000 Validation Loss: 0.07203921675682068\n",
      "Epoch 11931/30000 Training Loss: 0.07492316514253616\n",
      "Epoch 11932/30000 Training Loss: 0.07979976385831833\n",
      "Epoch 11933/30000 Training Loss: 0.08022201806306839\n",
      "Epoch 11934/30000 Training Loss: 0.04865797236561775\n",
      "Epoch 11935/30000 Training Loss: 0.07594040036201477\n",
      "Epoch 11936/30000 Training Loss: 0.06555867940187454\n",
      "Epoch 11937/30000 Training Loss: 0.11085716634988785\n",
      "Epoch 11938/30000 Training Loss: 0.08013585954904556\n",
      "Epoch 11939/30000 Training Loss: 0.10084861516952515\n",
      "Epoch 11940/30000 Training Loss: 0.0845627561211586\n",
      "Epoch 11940/30000 Validation Loss: 0.0829136073589325\n",
      "Epoch 11941/30000 Training Loss: 0.07880711555480957\n",
      "Epoch 11942/30000 Training Loss: 0.08295440673828125\n",
      "Epoch 11943/30000 Training Loss: 0.06404978036880493\n",
      "Epoch 11944/30000 Training Loss: 0.08774968236684799\n",
      "Epoch 11945/30000 Training Loss: 0.06986653059720993\n",
      "Epoch 11946/30000 Training Loss: 0.07432035356760025\n",
      "Epoch 11947/30000 Training Loss: 0.09064304828643799\n",
      "Epoch 11948/30000 Training Loss: 0.061243411153554916\n",
      "Epoch 11949/30000 Training Loss: 0.07406160980463028\n",
      "Epoch 11950/30000 Training Loss: 0.0701911598443985\n",
      "Epoch 11950/30000 Validation Loss: 0.07315954566001892\n",
      "Epoch 11951/30000 Training Loss: 0.06551278382539749\n",
      "Epoch 11952/30000 Training Loss: 0.09572157263755798\n",
      "Epoch 11953/30000 Training Loss: 0.07974425703287125\n",
      "Epoch 11954/30000 Training Loss: 0.08658996969461441\n",
      "Epoch 11955/30000 Training Loss: 0.06997540593147278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11956/30000 Training Loss: 0.10773656517267227\n",
      "Epoch 11957/30000 Training Loss: 0.07580704987049103\n",
      "Epoch 11958/30000 Training Loss: 0.09120287746191025\n",
      "Epoch 11959/30000 Training Loss: 0.07849640399217606\n",
      "Epoch 11960/30000 Training Loss: 0.07826951146125793\n",
      "Epoch 11960/30000 Validation Loss: 0.07482078671455383\n",
      "Epoch 11961/30000 Training Loss: 0.08899179100990295\n",
      "Epoch 11962/30000 Training Loss: 0.05773279070854187\n",
      "Epoch 11963/30000 Training Loss: 0.06940338760614395\n",
      "Epoch 11964/30000 Training Loss: 0.07418614625930786\n",
      "Epoch 11965/30000 Training Loss: 0.07046299427747726\n",
      "Epoch 11966/30000 Training Loss: 0.07990574836730957\n",
      "Epoch 11967/30000 Training Loss: 0.11900565773248672\n",
      "Epoch 11968/30000 Training Loss: 0.07496505975723267\n",
      "Epoch 11969/30000 Training Loss: 0.06944368034601212\n",
      "Epoch 11970/30000 Training Loss: 0.05908608064055443\n",
      "Epoch 11970/30000 Validation Loss: 0.09250304847955704\n",
      "Epoch 11971/30000 Training Loss: 0.05804125592112541\n",
      "Epoch 11972/30000 Training Loss: 0.06418664753437042\n",
      "Epoch 11973/30000 Training Loss: 0.06117687001824379\n",
      "Epoch 11974/30000 Training Loss: 0.05811910703778267\n",
      "Epoch 11975/30000 Training Loss: 0.08234026283025742\n",
      "Epoch 11976/30000 Training Loss: 0.08370853215456009\n",
      "Epoch 11977/30000 Training Loss: 0.06273887306451797\n",
      "Epoch 11978/30000 Training Loss: 0.06554297357797623\n",
      "Epoch 11979/30000 Training Loss: 0.07110396027565002\n",
      "Epoch 11980/30000 Training Loss: 0.10289671272039413\n",
      "Epoch 11980/30000 Validation Loss: 0.10369062423706055\n",
      "Epoch 11981/30000 Training Loss: 0.08302173763513565\n",
      "Epoch 11982/30000 Training Loss: 0.11571107059717178\n",
      "Epoch 11983/30000 Training Loss: 0.09702800959348679\n",
      "Epoch 11984/30000 Training Loss: 0.0758456215262413\n",
      "Epoch 11985/30000 Training Loss: 0.09648963063955307\n",
      "Epoch 11986/30000 Training Loss: 0.08411142230033875\n",
      "Epoch 11987/30000 Training Loss: 0.07938437908887863\n",
      "Epoch 11988/30000 Training Loss: 0.06828711926937103\n",
      "Epoch 11989/30000 Training Loss: 0.09701023250818253\n",
      "Epoch 11990/30000 Training Loss: 0.06569642573595047\n",
      "Epoch 11990/30000 Validation Loss: 0.060383353382349014\n",
      "Epoch 11991/30000 Training Loss: 0.07281134277582169\n",
      "Epoch 11992/30000 Training Loss: 0.08552175015211105\n",
      "Epoch 11993/30000 Training Loss: 0.1022016704082489\n",
      "Epoch 11994/30000 Training Loss: 0.08991042524576187\n",
      "Epoch 11995/30000 Training Loss: 0.06408078223466873\n",
      "Epoch 11996/30000 Training Loss: 0.088033527135849\n",
      "Epoch 11997/30000 Training Loss: 0.08052889257669449\n",
      "Epoch 11998/30000 Training Loss: 0.08053455501794815\n",
      "Epoch 11999/30000 Training Loss: 0.09304738789796829\n",
      "Epoch 12000/30000 Training Loss: 0.0740719810128212\n",
      "Epoch 12000/30000 Validation Loss: 0.0781993567943573\n",
      "Epoch 12001/30000 Training Loss: 0.08597338199615479\n",
      "Epoch 12002/30000 Training Loss: 0.06197920814156532\n",
      "Epoch 12003/30000 Training Loss: 0.0874529480934143\n",
      "Epoch 12004/30000 Training Loss: 0.06772246956825256\n",
      "Epoch 12005/30000 Training Loss: 0.06853464990854263\n",
      "Epoch 12006/30000 Training Loss: 0.07912468165159225\n",
      "Epoch 12007/30000 Training Loss: 0.06400854140520096\n",
      "Epoch 12008/30000 Training Loss: 0.0630570724606514\n",
      "Epoch 12009/30000 Training Loss: 0.07670038193464279\n",
      "Epoch 12010/30000 Training Loss: 0.08092040568590164\n",
      "Epoch 12010/30000 Validation Loss: 0.08324738591909409\n",
      "Epoch 12011/30000 Training Loss: 0.08600589632987976\n",
      "Epoch 12012/30000 Training Loss: 0.08044485002756119\n",
      "Epoch 12013/30000 Training Loss: 0.07750538736581802\n",
      "Epoch 12014/30000 Training Loss: 0.08854483813047409\n",
      "Epoch 12015/30000 Training Loss: 0.06103404238820076\n",
      "Epoch 12016/30000 Training Loss: 0.071448914706707\n",
      "Epoch 12017/30000 Training Loss: 0.07358825951814651\n",
      "Epoch 12018/30000 Training Loss: 0.08225277066230774\n",
      "Epoch 12019/30000 Training Loss: 0.07473740726709366\n",
      "Epoch 12020/30000 Training Loss: 0.05651434138417244\n",
      "Epoch 12020/30000 Validation Loss: 0.11627205461263657\n",
      "Epoch 12021/30000 Training Loss: 0.08204569667577744\n",
      "Epoch 12022/30000 Training Loss: 0.07189318537712097\n",
      "Epoch 12023/30000 Training Loss: 0.08085741847753525\n",
      "Epoch 12024/30000 Training Loss: 0.0681091845035553\n",
      "Epoch 12025/30000 Training Loss: 0.08100607991218567\n",
      "Epoch 12026/30000 Training Loss: 0.07010821998119354\n",
      "Epoch 12027/30000 Training Loss: 0.10712116211652756\n",
      "Epoch 12028/30000 Training Loss: 0.07567646354436874\n",
      "Epoch 12029/30000 Training Loss: 0.08600463718175888\n",
      "Epoch 12030/30000 Training Loss: 0.060612816363573074\n",
      "Epoch 12030/30000 Validation Loss: 0.07098846882581711\n",
      "Epoch 12031/30000 Training Loss: 0.07980981469154358\n",
      "Epoch 12032/30000 Training Loss: 0.08811459690332413\n",
      "Epoch 12033/30000 Training Loss: 0.11114990711212158\n",
      "Epoch 12034/30000 Training Loss: 0.08067385107278824\n",
      "Epoch 12035/30000 Training Loss: 0.06879633665084839\n",
      "Epoch 12036/30000 Training Loss: 0.06967178732156754\n",
      "Epoch 12037/30000 Training Loss: 0.07602275162935257\n",
      "Epoch 12038/30000 Training Loss: 0.09135943651199341\n",
      "Epoch 12039/30000 Training Loss: 0.0811152532696724\n",
      "Epoch 12040/30000 Training Loss: 0.06801381707191467\n",
      "Epoch 12040/30000 Validation Loss: 0.06818035989999771\n",
      "Epoch 12041/30000 Training Loss: 0.07264018058776855\n",
      "Epoch 12042/30000 Training Loss: 0.10363747924566269\n",
      "Epoch 12043/30000 Training Loss: 0.07547226548194885\n",
      "Epoch 12044/30000 Training Loss: 0.08190514892339706\n",
      "Epoch 12045/30000 Training Loss: 0.07557472586631775\n",
      "Epoch 12046/30000 Training Loss: 0.07241959124803543\n",
      "Epoch 12047/30000 Training Loss: 0.08478445559740067\n",
      "Epoch 12048/30000 Training Loss: 0.061293940991163254\n",
      "Epoch 12049/30000 Training Loss: 0.08153960853815079\n",
      "Epoch 12050/30000 Training Loss: 0.07182668894529343\n",
      "Epoch 12050/30000 Validation Loss: 0.09014732390642166\n",
      "Epoch 12051/30000 Training Loss: 0.08984873443841934\n",
      "Epoch 12052/30000 Training Loss: 0.07165742665529251\n",
      "Epoch 12053/30000 Training Loss: 0.10531453043222427\n",
      "Epoch 12054/30000 Training Loss: 0.08643224090337753\n",
      "Epoch 12055/30000 Training Loss: 0.0717257559299469\n",
      "Epoch 12056/30000 Training Loss: 0.07474863529205322\n",
      "Epoch 12057/30000 Training Loss: 0.08859029412269592\n",
      "Epoch 12058/30000 Training Loss: 0.08433222770690918\n",
      "Epoch 12059/30000 Training Loss: 0.05574246868491173\n",
      "Epoch 12060/30000 Training Loss: 0.08042767643928528\n",
      "Epoch 12060/30000 Validation Loss: 0.08014097064733505\n",
      "Epoch 12061/30000 Training Loss: 0.05886527895927429\n",
      "Epoch 12062/30000 Training Loss: 0.08601540327072144\n",
      "Epoch 12063/30000 Training Loss: 0.07153905183076859\n",
      "Epoch 12064/30000 Training Loss: 0.079116590321064\n",
      "Epoch 12065/30000 Training Loss: 0.10112723708152771\n",
      "Epoch 12066/30000 Training Loss: 0.08959584683179855\n",
      "Epoch 12067/30000 Training Loss: 0.062108367681503296\n",
      "Epoch 12068/30000 Training Loss: 0.08577118068933487\n",
      "Epoch 12069/30000 Training Loss: 0.09586688876152039\n",
      "Epoch 12070/30000 Training Loss: 0.06898579746484756\n",
      "Epoch 12070/30000 Validation Loss: 0.0865873172879219\n",
      "Epoch 12071/30000 Training Loss: 0.07806974649429321\n",
      "Epoch 12072/30000 Training Loss: 0.10041987150907516\n",
      "Epoch 12073/30000 Training Loss: 0.0902533233165741\n",
      "Epoch 12074/30000 Training Loss: 0.08218228071928024\n",
      "Epoch 12075/30000 Training Loss: 0.11482390761375427\n",
      "Epoch 12076/30000 Training Loss: 0.08582565933465958\n",
      "Epoch 12077/30000 Training Loss: 0.09772947430610657\n",
      "Epoch 12078/30000 Training Loss: 0.07538938522338867\n",
      "Epoch 12079/30000 Training Loss: 0.07310158014297485\n",
      "Epoch 12080/30000 Training Loss: 0.08431699872016907\n",
      "Epoch 12080/30000 Validation Loss: 0.08801382780075073\n",
      "Epoch 12081/30000 Training Loss: 0.06209912523627281\n",
      "Epoch 12082/30000 Training Loss: 0.08929064869880676\n",
      "Epoch 12083/30000 Training Loss: 0.06245286390185356\n",
      "Epoch 12084/30000 Training Loss: 0.05491006001830101\n",
      "Epoch 12085/30000 Training Loss: 0.10774550586938858\n",
      "Epoch 12086/30000 Training Loss: 0.06036053225398064\n",
      "Epoch 12087/30000 Training Loss: 0.07073741406202316\n",
      "Epoch 12088/30000 Training Loss: 0.09629649668931961\n",
      "Epoch 12089/30000 Training Loss: 0.08542942255735397\n",
      "Epoch 12090/30000 Training Loss: 0.07401968538761139\n",
      "Epoch 12090/30000 Validation Loss: 0.06768511980772018\n",
      "Epoch 12091/30000 Training Loss: 0.08554035425186157\n",
      "Epoch 12092/30000 Training Loss: 0.08888473361730576\n",
      "Epoch 12093/30000 Training Loss: 0.0769275650382042\n",
      "Epoch 12094/30000 Training Loss: 0.08804624527692795\n",
      "Epoch 12095/30000 Training Loss: 0.07424170523881912\n",
      "Epoch 12096/30000 Training Loss: 0.07375912368297577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12097/30000 Training Loss: 0.06877098232507706\n",
      "Epoch 12098/30000 Training Loss: 0.071183942258358\n",
      "Epoch 12099/30000 Training Loss: 0.08480431884527206\n",
      "Epoch 12100/30000 Training Loss: 0.08164749294519424\n",
      "Epoch 12100/30000 Validation Loss: 0.08149027824401855\n",
      "Epoch 12101/30000 Training Loss: 0.06452075392007828\n",
      "Epoch 12102/30000 Training Loss: 0.05573577061295509\n",
      "Epoch 12103/30000 Training Loss: 0.06417792290449142\n",
      "Epoch 12104/30000 Training Loss: 0.11843619495630264\n",
      "Epoch 12105/30000 Training Loss: 0.09247352927923203\n",
      "Epoch 12106/30000 Training Loss: 0.08282419294118881\n",
      "Epoch 12107/30000 Training Loss: 0.07364112883806229\n",
      "Epoch 12108/30000 Training Loss: 0.08701273053884506\n",
      "Epoch 12109/30000 Training Loss: 0.06954603642225266\n",
      "Epoch 12110/30000 Training Loss: 0.09088483452796936\n",
      "Epoch 12110/30000 Validation Loss: 0.07311904430389404\n",
      "Epoch 12111/30000 Training Loss: 0.07004200667142868\n",
      "Epoch 12112/30000 Training Loss: 0.0828244611620903\n",
      "Epoch 12113/30000 Training Loss: 0.0771133229136467\n",
      "Epoch 12114/30000 Training Loss: 0.06381078064441681\n",
      "Epoch 12115/30000 Training Loss: 0.07964256405830383\n",
      "Epoch 12116/30000 Training Loss: 0.08980415016412735\n",
      "Epoch 12117/30000 Training Loss: 0.0746007040143013\n",
      "Epoch 12118/30000 Training Loss: 0.07564371824264526\n",
      "Epoch 12119/30000 Training Loss: 0.08003280311822891\n",
      "Epoch 12120/30000 Training Loss: 0.09609848260879517\n",
      "Epoch 12120/30000 Validation Loss: 0.08817615360021591\n",
      "Epoch 12121/30000 Training Loss: 0.0854206457734108\n",
      "Epoch 12122/30000 Training Loss: 0.09911835193634033\n",
      "Epoch 12123/30000 Training Loss: 0.0655352994799614\n",
      "Epoch 12124/30000 Training Loss: 0.08044148236513138\n",
      "Epoch 12125/30000 Training Loss: 0.10664224624633789\n",
      "Epoch 12126/30000 Training Loss: 0.07651686668395996\n",
      "Epoch 12127/30000 Training Loss: 0.09497236460447311\n",
      "Epoch 12128/30000 Training Loss: 0.06895086169242859\n",
      "Epoch 12129/30000 Training Loss: 0.07054619491100311\n",
      "Epoch 12130/30000 Training Loss: 0.0764278918504715\n",
      "Epoch 12130/30000 Validation Loss: 0.07889262586832047\n",
      "Epoch 12131/30000 Training Loss: 0.07635251432657242\n",
      "Epoch 12132/30000 Training Loss: 0.09203878790140152\n",
      "Epoch 12133/30000 Training Loss: 0.0662003681063652\n",
      "Epoch 12134/30000 Training Loss: 0.104522205889225\n",
      "Epoch 12135/30000 Training Loss: 0.0750354528427124\n",
      "Epoch 12136/30000 Training Loss: 0.0778682604432106\n",
      "Epoch 12137/30000 Training Loss: 0.09113055467605591\n",
      "Epoch 12138/30000 Training Loss: 0.06757296621799469\n",
      "Epoch 12139/30000 Training Loss: 0.08275661617517471\n",
      "Epoch 12140/30000 Training Loss: 0.08261140435934067\n",
      "Epoch 12140/30000 Validation Loss: 0.06157869100570679\n",
      "Epoch 12141/30000 Training Loss: 0.08052968233823776\n",
      "Epoch 12142/30000 Training Loss: 0.07946040481328964\n",
      "Epoch 12143/30000 Training Loss: 0.09356886148452759\n",
      "Epoch 12144/30000 Training Loss: 0.0627172663807869\n",
      "Epoch 12145/30000 Training Loss: 0.07632841914892197\n",
      "Epoch 12146/30000 Training Loss: 0.06783150881528854\n",
      "Epoch 12147/30000 Training Loss: 0.06548503786325455\n",
      "Epoch 12148/30000 Training Loss: 0.08070116490125656\n",
      "Epoch 12149/30000 Training Loss: 0.0863543152809143\n",
      "Epoch 12150/30000 Training Loss: 0.06626703590154648\n",
      "Epoch 12150/30000 Validation Loss: 0.08077237010002136\n",
      "Epoch 12151/30000 Training Loss: 0.07888751477003098\n",
      "Epoch 12152/30000 Training Loss: 0.07764295488595963\n",
      "Epoch 12153/30000 Training Loss: 0.08884503692388535\n",
      "Epoch 12154/30000 Training Loss: 0.08387873321771622\n",
      "Epoch 12155/30000 Training Loss: 0.06219710782170296\n",
      "Epoch 12156/30000 Training Loss: 0.06316982954740524\n",
      "Epoch 12157/30000 Training Loss: 0.0982607826590538\n",
      "Epoch 12158/30000 Training Loss: 0.07662392407655716\n",
      "Epoch 12159/30000 Training Loss: 0.1028384268283844\n",
      "Epoch 12160/30000 Training Loss: 0.08680886775255203\n",
      "Epoch 12160/30000 Validation Loss: 0.07774171978235245\n",
      "Epoch 12161/30000 Training Loss: 0.07304201275110245\n",
      "Epoch 12162/30000 Training Loss: 0.08544021844863892\n",
      "Epoch 12163/30000 Training Loss: 0.08214295655488968\n",
      "Epoch 12164/30000 Training Loss: 0.08893797546625137\n",
      "Epoch 12165/30000 Training Loss: 0.06792178750038147\n",
      "Epoch 12166/30000 Training Loss: 0.08008106797933578\n",
      "Epoch 12167/30000 Training Loss: 0.10287035256624222\n",
      "Epoch 12168/30000 Training Loss: 0.0846039354801178\n",
      "Epoch 12169/30000 Training Loss: 0.08049937337636948\n",
      "Epoch 12170/30000 Training Loss: 0.0741269439458847\n",
      "Epoch 12170/30000 Validation Loss: 0.11889756470918655\n",
      "Epoch 12171/30000 Training Loss: 0.07841018587350845\n",
      "Epoch 12172/30000 Training Loss: 0.06851156800985336\n",
      "Epoch 12173/30000 Training Loss: 0.08560595661401749\n",
      "Epoch 12174/30000 Training Loss: 0.10471566766500473\n",
      "Epoch 12175/30000 Training Loss: 0.08374368399381638\n",
      "Epoch 12176/30000 Training Loss: 0.08914545178413391\n",
      "Epoch 12177/30000 Training Loss: 0.09726811200380325\n",
      "Epoch 12178/30000 Training Loss: 0.0745466873049736\n",
      "Epoch 12179/30000 Training Loss: 0.0684455931186676\n",
      "Epoch 12180/30000 Training Loss: 0.0625743716955185\n",
      "Epoch 12180/30000 Validation Loss: 0.057822246104478836\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.057822246104478836<=============\n",
      "Epoch 12181/30000 Training Loss: 0.09675762802362442\n",
      "Epoch 12182/30000 Training Loss: 0.08542145043611526\n",
      "Epoch 12183/30000 Training Loss: 0.06754723936319351\n",
      "Epoch 12184/30000 Training Loss: 0.05757260322570801\n",
      "Epoch 12185/30000 Training Loss: 0.07408849149942398\n",
      "Epoch 12186/30000 Training Loss: 0.07210154831409454\n",
      "Epoch 12187/30000 Training Loss: 0.0820888802409172\n",
      "Epoch 12188/30000 Training Loss: 0.07446417212486267\n",
      "Epoch 12189/30000 Training Loss: 0.08510053157806396\n",
      "Epoch 12190/30000 Training Loss: 0.07222027331590652\n",
      "Epoch 12190/30000 Validation Loss: 0.06954917311668396\n",
      "Epoch 12191/30000 Training Loss: 0.07494987547397614\n",
      "Epoch 12192/30000 Training Loss: 0.06902385503053665\n",
      "Epoch 12193/30000 Training Loss: 0.10646558552980423\n",
      "Epoch 12194/30000 Training Loss: 0.07749444246292114\n",
      "Epoch 12195/30000 Training Loss: 0.07193759828805923\n",
      "Epoch 12196/30000 Training Loss: 0.07764394581317902\n",
      "Epoch 12197/30000 Training Loss: 0.07874805480241776\n",
      "Epoch 12198/30000 Training Loss: 0.08091003447771072\n",
      "Epoch 12199/30000 Training Loss: 0.08431216329336166\n",
      "Epoch 12200/30000 Training Loss: 0.08366546779870987\n",
      "Epoch 12200/30000 Validation Loss: 0.09091096371412277\n",
      "Epoch 12201/30000 Training Loss: 0.08271103352308273\n",
      "Epoch 12202/30000 Training Loss: 0.10456637293100357\n",
      "Epoch 12203/30000 Training Loss: 0.0901312604546547\n",
      "Epoch 12204/30000 Training Loss: 0.07649848610162735\n",
      "Epoch 12205/30000 Training Loss: 0.0788232609629631\n",
      "Epoch 12206/30000 Training Loss: 0.05823319032788277\n",
      "Epoch 12207/30000 Training Loss: 0.09572134166955948\n",
      "Epoch 12208/30000 Training Loss: 0.08018045872449875\n",
      "Epoch 12209/30000 Training Loss: 0.07773201912641525\n",
      "Epoch 12210/30000 Training Loss: 0.07966958731412888\n",
      "Epoch 12210/30000 Validation Loss: 0.10041496157646179\n",
      "Epoch 12211/30000 Training Loss: 0.07723326236009598\n",
      "Epoch 12212/30000 Training Loss: 0.08431679010391235\n",
      "Epoch 12213/30000 Training Loss: 0.08702194690704346\n",
      "Epoch 12214/30000 Training Loss: 0.09208179265260696\n",
      "Epoch 12215/30000 Training Loss: 0.12775035202503204\n",
      "Epoch 12216/30000 Training Loss: 0.07341168075799942\n",
      "Epoch 12217/30000 Training Loss: 0.08301755785942078\n",
      "Epoch 12218/30000 Training Loss: 0.0673932209610939\n",
      "Epoch 12219/30000 Training Loss: 0.06402036547660828\n",
      "Epoch 12220/30000 Training Loss: 0.07573568820953369\n",
      "Epoch 12220/30000 Validation Loss: 0.08833107352256775\n",
      "Epoch 12221/30000 Training Loss: 0.0660073310136795\n",
      "Epoch 12222/30000 Training Loss: 0.07315552979707718\n",
      "Epoch 12223/30000 Training Loss: 0.08592020720243454\n",
      "Epoch 12224/30000 Training Loss: 0.1010938510298729\n",
      "Epoch 12225/30000 Training Loss: 0.07971233129501343\n",
      "Epoch 12226/30000 Training Loss: 0.0809541866183281\n",
      "Epoch 12227/30000 Training Loss: 0.08676201105117798\n",
      "Epoch 12228/30000 Training Loss: 0.0632038414478302\n",
      "Epoch 12229/30000 Training Loss: 0.06973781436681747\n",
      "Epoch 12230/30000 Training Loss: 0.08441182225942612\n",
      "Epoch 12230/30000 Validation Loss: 0.08725130558013916\n",
      "Epoch 12231/30000 Training Loss: 0.0886744037270546\n",
      "Epoch 12232/30000 Training Loss: 0.10097676515579224\n",
      "Epoch 12233/30000 Training Loss: 0.0884861871600151\n",
      "Epoch 12234/30000 Training Loss: 0.08423909544944763\n",
      "Epoch 12235/30000 Training Loss: 0.09846342355012894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12236/30000 Training Loss: 0.07069000601768494\n",
      "Epoch 12237/30000 Training Loss: 0.08314786106348038\n",
      "Epoch 12238/30000 Training Loss: 0.07173799723386765\n",
      "Epoch 12239/30000 Training Loss: 0.06272359937429428\n",
      "Epoch 12240/30000 Training Loss: 0.08592363446950912\n",
      "Epoch 12240/30000 Validation Loss: 0.08132495731115341\n",
      "Epoch 12241/30000 Training Loss: 0.06454046815633774\n",
      "Epoch 12242/30000 Training Loss: 0.0645211935043335\n",
      "Epoch 12243/30000 Training Loss: 0.0881258025765419\n",
      "Epoch 12244/30000 Training Loss: 0.07180086523294449\n",
      "Epoch 12245/30000 Training Loss: 0.0809924528002739\n",
      "Epoch 12246/30000 Training Loss: 0.0816182866692543\n",
      "Epoch 12247/30000 Training Loss: 0.0863615944981575\n",
      "Epoch 12248/30000 Training Loss: 0.08171889185905457\n",
      "Epoch 12249/30000 Training Loss: 0.07760610431432724\n",
      "Epoch 12250/30000 Training Loss: 0.06749855726957321\n",
      "Epoch 12250/30000 Validation Loss: 0.10307404398918152\n",
      "Epoch 12251/30000 Training Loss: 0.08002922683954239\n",
      "Epoch 12252/30000 Training Loss: 0.07835692167282104\n",
      "Epoch 12253/30000 Training Loss: 0.08787582069635391\n",
      "Epoch 12254/30000 Training Loss: 0.06339582800865173\n",
      "Epoch 12255/30000 Training Loss: 0.07279438525438309\n",
      "Epoch 12256/30000 Training Loss: 0.07634765654802322\n",
      "Epoch 12257/30000 Training Loss: 0.08065042644739151\n",
      "Epoch 12258/30000 Training Loss: 0.0989852249622345\n",
      "Epoch 12259/30000 Training Loss: 0.07348150759935379\n",
      "Epoch 12260/30000 Training Loss: 0.08213726431131363\n",
      "Epoch 12260/30000 Validation Loss: 0.07653919607400894\n",
      "Epoch 12261/30000 Training Loss: 0.06920293718576431\n",
      "Epoch 12262/30000 Training Loss: 0.07138913124799728\n",
      "Epoch 12263/30000 Training Loss: 0.08722596615552902\n",
      "Epoch 12264/30000 Training Loss: 0.07908204942941666\n",
      "Epoch 12265/30000 Training Loss: 0.0735774114727974\n",
      "Epoch 12266/30000 Training Loss: 0.07501322776079178\n",
      "Epoch 12267/30000 Training Loss: 0.07783240079879761\n",
      "Epoch 12268/30000 Training Loss: 0.0802341178059578\n",
      "Epoch 12269/30000 Training Loss: 0.07598325610160828\n",
      "Epoch 12270/30000 Training Loss: 0.09080848842859268\n",
      "Epoch 12270/30000 Validation Loss: 0.09801435470581055\n",
      "Epoch 12271/30000 Training Loss: 0.08558838814496994\n",
      "Epoch 12272/30000 Training Loss: 0.09528392553329468\n",
      "Epoch 12273/30000 Training Loss: 0.09025118499994278\n",
      "Epoch 12274/30000 Training Loss: 0.06832001358270645\n",
      "Epoch 12275/30000 Training Loss: 0.07347748428583145\n",
      "Epoch 12276/30000 Training Loss: 0.07495474815368652\n",
      "Epoch 12277/30000 Training Loss: 0.08024050295352936\n",
      "Epoch 12278/30000 Training Loss: 0.0680529996752739\n",
      "Epoch 12279/30000 Training Loss: 0.07396192103624344\n",
      "Epoch 12280/30000 Training Loss: 0.08151983469724655\n",
      "Epoch 12280/30000 Validation Loss: 0.07016104459762573\n",
      "Epoch 12281/30000 Training Loss: 0.08847277611494064\n",
      "Epoch 12282/30000 Training Loss: 0.08760786801576614\n",
      "Epoch 12283/30000 Training Loss: 0.07633594423532486\n",
      "Epoch 12284/30000 Training Loss: 0.07033706456422806\n",
      "Epoch 12285/30000 Training Loss: 0.09184061735868454\n",
      "Epoch 12286/30000 Training Loss: 0.09358548372983932\n",
      "Epoch 12287/30000 Training Loss: 0.0778394341468811\n",
      "Epoch 12288/30000 Training Loss: 0.05609344318509102\n",
      "Epoch 12289/30000 Training Loss: 0.08322643488645554\n",
      "Epoch 12290/30000 Training Loss: 0.10102779418230057\n",
      "Epoch 12290/30000 Validation Loss: 0.06858315318822861\n",
      "Epoch 12291/30000 Training Loss: 0.07386117428541183\n",
      "Epoch 12292/30000 Training Loss: 0.07536279410123825\n",
      "Epoch 12293/30000 Training Loss: 0.10016179829835892\n",
      "Epoch 12294/30000 Training Loss: 0.06587637215852737\n",
      "Epoch 12295/30000 Training Loss: 0.07645663619041443\n",
      "Epoch 12296/30000 Training Loss: 0.06531370431184769\n",
      "Epoch 12297/30000 Training Loss: 0.10819331556558609\n",
      "Epoch 12298/30000 Training Loss: 0.06658071279525757\n",
      "Epoch 12299/30000 Training Loss: 0.08371702581644058\n",
      "Epoch 12300/30000 Training Loss: 0.07271648198366165\n",
      "Epoch 12300/30000 Validation Loss: 0.0717134103178978\n",
      "Epoch 12301/30000 Training Loss: 0.11611317843198776\n",
      "Epoch 12302/30000 Training Loss: 0.09582018852233887\n",
      "Epoch 12303/30000 Training Loss: 0.06739311665296555\n",
      "Epoch 12304/30000 Training Loss: 0.08127007633447647\n",
      "Epoch 12305/30000 Training Loss: 0.06396432965993881\n",
      "Epoch 12306/30000 Training Loss: 0.07840007543563843\n",
      "Epoch 12307/30000 Training Loss: 0.11050111055374146\n",
      "Epoch 12308/30000 Training Loss: 0.08556807786226273\n",
      "Epoch 12309/30000 Training Loss: 0.0835493877530098\n",
      "Epoch 12310/30000 Training Loss: 0.10046812146902084\n",
      "Epoch 12310/30000 Validation Loss: 0.08255591988563538\n",
      "Epoch 12311/30000 Training Loss: 0.0637044832110405\n",
      "Epoch 12312/30000 Training Loss: 0.07690685242414474\n",
      "Epoch 12313/30000 Training Loss: 0.0739210695028305\n",
      "Epoch 12314/30000 Training Loss: 0.08340177685022354\n",
      "Epoch 12315/30000 Training Loss: 0.07077775150537491\n",
      "Epoch 12316/30000 Training Loss: 0.11102857440710068\n",
      "Epoch 12317/30000 Training Loss: 0.08262715488672256\n",
      "Epoch 12318/30000 Training Loss: 0.05820915102958679\n",
      "Epoch 12319/30000 Training Loss: 0.07316084951162338\n",
      "Epoch 12320/30000 Training Loss: 0.06548518687486649\n",
      "Epoch 12320/30000 Validation Loss: 0.08283283561468124\n",
      "Epoch 12321/30000 Training Loss: 0.06587917357683182\n",
      "Epoch 12322/30000 Training Loss: 0.07822155952453613\n",
      "Epoch 12323/30000 Training Loss: 0.07333492487668991\n",
      "Epoch 12324/30000 Training Loss: 0.0616287998855114\n",
      "Epoch 12325/30000 Training Loss: 0.08527028560638428\n",
      "Epoch 12326/30000 Training Loss: 0.08554401248693466\n",
      "Epoch 12327/30000 Training Loss: 0.0860147550702095\n",
      "Epoch 12328/30000 Training Loss: 0.05722996965050697\n",
      "Epoch 12329/30000 Training Loss: 0.10100831836462021\n",
      "Epoch 12330/30000 Training Loss: 0.06468000262975693\n",
      "Epoch 12330/30000 Validation Loss: 0.07205211371183395\n",
      "Epoch 12331/30000 Training Loss: 0.07690595835447311\n",
      "Epoch 12332/30000 Training Loss: 0.10846873372793198\n",
      "Epoch 12333/30000 Training Loss: 0.0842670202255249\n",
      "Epoch 12334/30000 Training Loss: 0.07935862988233566\n",
      "Epoch 12335/30000 Training Loss: 0.07421571761369705\n",
      "Epoch 12336/30000 Training Loss: 0.07728245109319687\n",
      "Epoch 12337/30000 Training Loss: 0.0780976265668869\n",
      "Epoch 12338/30000 Training Loss: 0.0837225690484047\n",
      "Epoch 12339/30000 Training Loss: 0.08805469423532486\n",
      "Epoch 12340/30000 Training Loss: 0.07551191002130508\n",
      "Epoch 12340/30000 Validation Loss: 0.08905792236328125\n",
      "Epoch 12341/30000 Training Loss: 0.08084466308355331\n",
      "Epoch 12342/30000 Training Loss: 0.0747663751244545\n",
      "Epoch 12343/30000 Training Loss: 0.06867510825395584\n",
      "Epoch 12344/30000 Training Loss: 0.08100595325231552\n",
      "Epoch 12345/30000 Training Loss: 0.08234954625368118\n",
      "Epoch 12346/30000 Training Loss: 0.10524138808250427\n",
      "Epoch 12347/30000 Training Loss: 0.08212660998106003\n",
      "Epoch 12348/30000 Training Loss: 0.09259802848100662\n",
      "Epoch 12349/30000 Training Loss: 0.08151077479124069\n",
      "Epoch 12350/30000 Training Loss: 0.08998284488916397\n",
      "Epoch 12350/30000 Validation Loss: 0.09058159589767456\n",
      "Epoch 12351/30000 Training Loss: 0.08538398891687393\n",
      "Epoch 12352/30000 Training Loss: 0.08924553543329239\n",
      "Epoch 12353/30000 Training Loss: 0.07180733978748322\n",
      "Epoch 12354/30000 Training Loss: 0.07791822403669357\n",
      "Epoch 12355/30000 Training Loss: 0.08305492997169495\n",
      "Epoch 12356/30000 Training Loss: 0.07929284125566483\n",
      "Epoch 12357/30000 Training Loss: 0.07309902459383011\n",
      "Epoch 12358/30000 Training Loss: 0.06765194982290268\n",
      "Epoch 12359/30000 Training Loss: 0.08775138109922409\n",
      "Epoch 12360/30000 Training Loss: 0.09429269284009933\n",
      "Epoch 12360/30000 Validation Loss: 0.06923951953649521\n",
      "Epoch 12361/30000 Training Loss: 0.07426486164331436\n",
      "Epoch 12362/30000 Training Loss: 0.07923982292413712\n",
      "Epoch 12363/30000 Training Loss: 0.06914681941270828\n",
      "Epoch 12364/30000 Training Loss: 0.078475721180439\n",
      "Epoch 12365/30000 Training Loss: 0.08512227982282639\n",
      "Epoch 12366/30000 Training Loss: 0.083726666867733\n",
      "Epoch 12367/30000 Training Loss: 0.06883459538221359\n",
      "Epoch 12368/30000 Training Loss: 0.0704047754406929\n",
      "Epoch 12369/30000 Training Loss: 0.09119582176208496\n",
      "Epoch 12370/30000 Training Loss: 0.09139474481344223\n",
      "Epoch 12370/30000 Validation Loss: 0.09231346100568771\n",
      "Epoch 12371/30000 Training Loss: 0.07883089780807495\n",
      "Epoch 12372/30000 Training Loss: 0.07976218312978745\n",
      "Epoch 12373/30000 Training Loss: 0.09377602487802505\n",
      "Epoch 12374/30000 Training Loss: 0.07840604335069656\n",
      "Epoch 12375/30000 Training Loss: 0.10003349930047989\n",
      "Epoch 12376/30000 Training Loss: 0.07556479424238205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12377/30000 Training Loss: 0.09905794262886047\n",
      "Epoch 12378/30000 Training Loss: 0.08706510066986084\n",
      "Epoch 12379/30000 Training Loss: 0.10277869552373886\n",
      "Epoch 12380/30000 Training Loss: 0.06756669282913208\n",
      "Epoch 12380/30000 Validation Loss: 0.06033918261528015\n",
      "Epoch 12381/30000 Training Loss: 0.07808670401573181\n",
      "Epoch 12382/30000 Training Loss: 0.07530494779348373\n",
      "Epoch 12383/30000 Training Loss: 0.06143029406666756\n",
      "Epoch 12384/30000 Training Loss: 0.08339431881904602\n",
      "Epoch 12385/30000 Training Loss: 0.10136192291975021\n",
      "Epoch 12386/30000 Training Loss: 0.08114811033010483\n",
      "Epoch 12387/30000 Training Loss: 0.06962224096059799\n",
      "Epoch 12388/30000 Training Loss: 0.09572093933820724\n",
      "Epoch 12389/30000 Training Loss: 0.09099296480417252\n",
      "Epoch 12390/30000 Training Loss: 0.08763048052787781\n",
      "Epoch 12390/30000 Validation Loss: 0.06903444230556488\n",
      "Epoch 12391/30000 Training Loss: 0.06187829375267029\n",
      "Epoch 12392/30000 Training Loss: 0.09325122833251953\n",
      "Epoch 12393/30000 Training Loss: 0.10570133477449417\n",
      "Epoch 12394/30000 Training Loss: 0.10644018650054932\n",
      "Epoch 12395/30000 Training Loss: 0.08031568676233292\n",
      "Epoch 12396/30000 Training Loss: 0.0958847925066948\n",
      "Epoch 12397/30000 Training Loss: 0.07810840010643005\n",
      "Epoch 12398/30000 Training Loss: 0.08598402142524719\n",
      "Epoch 12399/30000 Training Loss: 0.0794147402048111\n",
      "Epoch 12400/30000 Training Loss: 0.0712086632847786\n",
      "Epoch 12400/30000 Validation Loss: 0.08018723875284195\n",
      "Epoch 12401/30000 Training Loss: 0.0961235985159874\n",
      "Epoch 12402/30000 Training Loss: 0.06093490123748779\n",
      "Epoch 12403/30000 Training Loss: 0.06317231804132462\n",
      "Epoch 12404/30000 Training Loss: 0.09275835752487183\n",
      "Epoch 12405/30000 Training Loss: 0.07342538982629776\n",
      "Epoch 12406/30000 Training Loss: 0.08500126749277115\n",
      "Epoch 12407/30000 Training Loss: 0.08796324580907822\n",
      "Epoch 12408/30000 Training Loss: 0.08698108047246933\n",
      "Epoch 12409/30000 Training Loss: 0.09202954173088074\n",
      "Epoch 12410/30000 Training Loss: 0.08470290899276733\n",
      "Epoch 12410/30000 Validation Loss: 0.09054357558488846\n",
      "Epoch 12411/30000 Training Loss: 0.09208887815475464\n",
      "Epoch 12412/30000 Training Loss: 0.07630466669797897\n",
      "Epoch 12413/30000 Training Loss: 0.08055450767278671\n",
      "Epoch 12414/30000 Training Loss: 0.08383180946111679\n",
      "Epoch 12415/30000 Training Loss: 0.06758677214384079\n",
      "Epoch 12416/30000 Training Loss: 0.10267172008752823\n",
      "Epoch 12417/30000 Training Loss: 0.06366106122732162\n",
      "Epoch 12418/30000 Training Loss: 0.09274714440107346\n",
      "Epoch 12419/30000 Training Loss: 0.07577691227197647\n",
      "Epoch 12420/30000 Training Loss: 0.08813297748565674\n",
      "Epoch 12420/30000 Validation Loss: 0.07769414782524109\n",
      "Epoch 12421/30000 Training Loss: 0.07756542414426804\n",
      "Epoch 12422/30000 Training Loss: 0.06866664439439774\n",
      "Epoch 12423/30000 Training Loss: 0.07342153787612915\n",
      "Epoch 12424/30000 Training Loss: 0.07701427489519119\n",
      "Epoch 12425/30000 Training Loss: 0.09261509031057358\n",
      "Epoch 12426/30000 Training Loss: 0.07303328067064285\n",
      "Epoch 12427/30000 Training Loss: 0.06558170169591904\n",
      "Epoch 12428/30000 Training Loss: 0.08748947829008102\n",
      "Epoch 12429/30000 Training Loss: 0.0662873387336731\n",
      "Epoch 12430/30000 Training Loss: 0.061953216791152954\n",
      "Epoch 12430/30000 Validation Loss: 0.06253305077552795\n",
      "Epoch 12431/30000 Training Loss: 0.07714110612869263\n",
      "Epoch 12432/30000 Training Loss: 0.08530008047819138\n",
      "Epoch 12433/30000 Training Loss: 0.05395912751555443\n",
      "Epoch 12434/30000 Training Loss: 0.09027921408414841\n",
      "Epoch 12435/30000 Training Loss: 0.07503210753202438\n",
      "Epoch 12436/30000 Training Loss: 0.07258617132902145\n",
      "Epoch 12437/30000 Training Loss: 0.06950952857732773\n",
      "Epoch 12438/30000 Training Loss: 0.0508926659822464\n",
      "Epoch 12439/30000 Training Loss: 0.06866488605737686\n",
      "Epoch 12440/30000 Training Loss: 0.07526025176048279\n",
      "Epoch 12440/30000 Validation Loss: 0.09179951995611191\n",
      "Epoch 12441/30000 Training Loss: 0.06932196766138077\n",
      "Epoch 12442/30000 Training Loss: 0.0736425593495369\n",
      "Epoch 12443/30000 Training Loss: 0.0941321849822998\n",
      "Epoch 12444/30000 Training Loss: 0.06929078698158264\n",
      "Epoch 12445/30000 Training Loss: 0.07452870160341263\n",
      "Epoch 12446/30000 Training Loss: 0.08553151041269302\n",
      "Epoch 12447/30000 Training Loss: 0.07013285905122757\n",
      "Epoch 12448/30000 Training Loss: 0.06241892650723457\n",
      "Epoch 12449/30000 Training Loss: 0.09126142412424088\n",
      "Epoch 12450/30000 Training Loss: 0.09368214756250381\n",
      "Epoch 12450/30000 Validation Loss: 0.07680237293243408\n",
      "Epoch 12451/30000 Training Loss: 0.10038363188505173\n",
      "Epoch 12452/30000 Training Loss: 0.08469710499048233\n",
      "Epoch 12453/30000 Training Loss: 0.0783722773194313\n",
      "Epoch 12454/30000 Training Loss: 0.08720377087593079\n",
      "Epoch 12455/30000 Training Loss: 0.06738683581352234\n",
      "Epoch 12456/30000 Training Loss: 0.10394122451543808\n",
      "Epoch 12457/30000 Training Loss: 0.07031958550214767\n",
      "Epoch 12458/30000 Training Loss: 0.06577012687921524\n",
      "Epoch 12459/30000 Training Loss: 0.09410596638917923\n",
      "Epoch 12460/30000 Training Loss: 0.07258755713701248\n",
      "Epoch 12460/30000 Validation Loss: 0.08354959636926651\n",
      "Epoch 12461/30000 Training Loss: 0.08176678419113159\n",
      "Epoch 12462/30000 Training Loss: 0.10052018612623215\n",
      "Epoch 12463/30000 Training Loss: 0.06933938711881638\n",
      "Epoch 12464/30000 Training Loss: 0.08681943267583847\n",
      "Epoch 12465/30000 Training Loss: 0.0754215344786644\n",
      "Epoch 12466/30000 Training Loss: 0.08637934178113937\n",
      "Epoch 12467/30000 Training Loss: 0.0577392540872097\n",
      "Epoch 12468/30000 Training Loss: 0.07682662457227707\n",
      "Epoch 12469/30000 Training Loss: 0.09869014471769333\n",
      "Epoch 12470/30000 Training Loss: 0.08402354270219803\n",
      "Epoch 12470/30000 Validation Loss: 0.10099077969789505\n",
      "Epoch 12471/30000 Training Loss: 0.08180606365203857\n",
      "Epoch 12472/30000 Training Loss: 0.07437840849161148\n",
      "Epoch 12473/30000 Training Loss: 0.06994225829839706\n",
      "Epoch 12474/30000 Training Loss: 0.08735924959182739\n",
      "Epoch 12475/30000 Training Loss: 0.08030924201011658\n",
      "Epoch 12476/30000 Training Loss: 0.08158547431230545\n",
      "Epoch 12477/30000 Training Loss: 0.06465194374322891\n",
      "Epoch 12478/30000 Training Loss: 0.08995529264211655\n",
      "Epoch 12479/30000 Training Loss: 0.0760197564959526\n",
      "Epoch 12480/30000 Training Loss: 0.07378285378217697\n",
      "Epoch 12480/30000 Validation Loss: 0.06142689660191536\n",
      "Epoch 12481/30000 Training Loss: 0.07108104974031448\n",
      "Epoch 12482/30000 Training Loss: 0.07423824071884155\n",
      "Epoch 12483/30000 Training Loss: 0.07670693844556808\n",
      "Epoch 12484/30000 Training Loss: 0.07008475810289383\n",
      "Epoch 12485/30000 Training Loss: 0.08789253979921341\n",
      "Epoch 12486/30000 Training Loss: 0.06898703426122665\n",
      "Epoch 12487/30000 Training Loss: 0.09159203618764877\n",
      "Epoch 12488/30000 Training Loss: 0.07477658241987228\n",
      "Epoch 12489/30000 Training Loss: 0.06807702779769897\n",
      "Epoch 12490/30000 Training Loss: 0.07275527715682983\n",
      "Epoch 12490/30000 Validation Loss: 0.07779206335544586\n",
      "Epoch 12491/30000 Training Loss: 0.0653516948223114\n",
      "Epoch 12492/30000 Training Loss: 0.0798477903008461\n",
      "Epoch 12493/30000 Training Loss: 0.10590618848800659\n",
      "Epoch 12494/30000 Training Loss: 0.06819179654121399\n",
      "Epoch 12495/30000 Training Loss: 0.08214074373245239\n",
      "Epoch 12496/30000 Training Loss: 0.07573365420103073\n",
      "Epoch 12497/30000 Training Loss: 0.07917620986700058\n",
      "Epoch 12498/30000 Training Loss: 0.07195181399583817\n",
      "Epoch 12499/30000 Training Loss: 0.09324514120817184\n",
      "Epoch 12500/30000 Training Loss: 0.07263015955686569\n",
      "Epoch 12500/30000 Validation Loss: 0.0769534483551979\n",
      "Epoch 12501/30000 Training Loss: 0.0689680203795433\n",
      "Epoch 12502/30000 Training Loss: 0.06574761122465134\n",
      "Epoch 12503/30000 Training Loss: 0.06328085064888\n",
      "Epoch 12504/30000 Training Loss: 0.09570098668336868\n",
      "Epoch 12505/30000 Training Loss: 0.07601123303174973\n",
      "Epoch 12506/30000 Training Loss: 0.08177661150693893\n",
      "Epoch 12507/30000 Training Loss: 0.08086471259593964\n",
      "Epoch 12508/30000 Training Loss: 0.07497900724411011\n",
      "Epoch 12509/30000 Training Loss: 0.0785290077328682\n",
      "Epoch 12510/30000 Training Loss: 0.07921063154935837\n",
      "Epoch 12510/30000 Validation Loss: 0.06917696446180344\n",
      "Epoch 12511/30000 Training Loss: 0.06903944164514542\n",
      "Epoch 12512/30000 Training Loss: 0.06543903797864914\n",
      "Epoch 12513/30000 Training Loss: 0.0749603658914566\n",
      "Epoch 12514/30000 Training Loss: 0.08645506948232651\n",
      "Epoch 12515/30000 Training Loss: 0.10022109746932983\n",
      "Epoch 12516/30000 Training Loss: 0.06647732853889465\n",
      "Epoch 12517/30000 Training Loss: 0.06536383181810379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12518/30000 Training Loss: 0.08933704346418381\n",
      "Epoch 12519/30000 Training Loss: 0.06893747299909592\n",
      "Epoch 12520/30000 Training Loss: 0.07347005605697632\n",
      "Epoch 12520/30000 Validation Loss: 0.08238493651151657\n",
      "Epoch 12521/30000 Training Loss: 0.08743979781866074\n",
      "Epoch 12522/30000 Training Loss: 0.09152577072381973\n",
      "Epoch 12523/30000 Training Loss: 0.08009245991706848\n",
      "Epoch 12524/30000 Training Loss: 0.07428029179573059\n",
      "Epoch 12525/30000 Training Loss: 0.06991875171661377\n",
      "Epoch 12526/30000 Training Loss: 0.06384140253067017\n",
      "Epoch 12527/30000 Training Loss: 0.0741715133190155\n",
      "Epoch 12528/30000 Training Loss: 0.07405329495668411\n",
      "Epoch 12529/30000 Training Loss: 0.10254327207803726\n",
      "Epoch 12530/30000 Training Loss: 0.08963850140571594\n",
      "Epoch 12530/30000 Validation Loss: 0.07234311103820801\n",
      "Epoch 12531/30000 Training Loss: 0.06158221885561943\n",
      "Epoch 12532/30000 Training Loss: 0.07270380109548569\n",
      "Epoch 12533/30000 Training Loss: 0.0770222544670105\n",
      "Epoch 12534/30000 Training Loss: 0.07109443098306656\n",
      "Epoch 12535/30000 Training Loss: 0.09942590445280075\n",
      "Epoch 12536/30000 Training Loss: 0.10226688534021378\n",
      "Epoch 12537/30000 Training Loss: 0.0750650092959404\n",
      "Epoch 12538/30000 Training Loss: 0.08349546045064926\n",
      "Epoch 12539/30000 Training Loss: 0.08359973877668381\n",
      "Epoch 12540/30000 Training Loss: 0.07379298657178879\n",
      "Epoch 12540/30000 Validation Loss: 0.08433113247156143\n",
      "Epoch 12541/30000 Training Loss: 0.0793089047074318\n",
      "Epoch 12542/30000 Training Loss: 0.09218993037939072\n",
      "Epoch 12543/30000 Training Loss: 0.081124447286129\n",
      "Epoch 12544/30000 Training Loss: 0.07487626373767853\n",
      "Epoch 12545/30000 Training Loss: 0.07812140136957169\n",
      "Epoch 12546/30000 Training Loss: 0.07372308522462845\n",
      "Epoch 12547/30000 Training Loss: 0.07665342837572098\n",
      "Epoch 12548/30000 Training Loss: 0.06609678268432617\n",
      "Epoch 12549/30000 Training Loss: 0.093214251101017\n",
      "Epoch 12550/30000 Training Loss: 0.061498939990997314\n",
      "Epoch 12550/30000 Validation Loss: 0.08445953577756882\n",
      "Epoch 12551/30000 Training Loss: 0.07622562348842621\n",
      "Epoch 12552/30000 Training Loss: 0.08388596028089523\n",
      "Epoch 12553/30000 Training Loss: 0.0793362483382225\n",
      "Epoch 12554/30000 Training Loss: 0.06895360350608826\n",
      "Epoch 12555/30000 Training Loss: 0.08427491039037704\n",
      "Epoch 12556/30000 Training Loss: 0.07210872322320938\n",
      "Epoch 12557/30000 Training Loss: 0.062060851603746414\n",
      "Epoch 12558/30000 Training Loss: 0.07275620102882385\n",
      "Epoch 12559/30000 Training Loss: 0.06409186869859695\n",
      "Epoch 12560/30000 Training Loss: 0.08318006247282028\n",
      "Epoch 12560/30000 Validation Loss: 0.07733872532844543\n",
      "Epoch 12561/30000 Training Loss: 0.07691920548677444\n",
      "Epoch 12562/30000 Training Loss: 0.09026742726564407\n",
      "Epoch 12563/30000 Training Loss: 0.07263653725385666\n",
      "Epoch 12564/30000 Training Loss: 0.10048118978738785\n",
      "Epoch 12565/30000 Training Loss: 0.07740087062120438\n",
      "Epoch 12566/30000 Training Loss: 0.08355829119682312\n",
      "Epoch 12567/30000 Training Loss: 0.06950309872627258\n",
      "Epoch 12568/30000 Training Loss: 0.0745941698551178\n",
      "Epoch 12569/30000 Training Loss: 0.06628505140542984\n",
      "Epoch 12570/30000 Training Loss: 0.07868606597185135\n",
      "Epoch 12570/30000 Validation Loss: 0.09056220203638077\n",
      "Epoch 12571/30000 Training Loss: 0.06699062138795853\n",
      "Epoch 12572/30000 Training Loss: 0.05758040025830269\n",
      "Epoch 12573/30000 Training Loss: 0.07051541656255722\n",
      "Epoch 12574/30000 Training Loss: 0.07775300741195679\n",
      "Epoch 12575/30000 Training Loss: 0.07137634605169296\n",
      "Epoch 12576/30000 Training Loss: 0.04902108386158943\n",
      "Epoch 12577/30000 Training Loss: 0.0641292855143547\n",
      "Epoch 12578/30000 Training Loss: 0.0915670320391655\n",
      "Epoch 12579/30000 Training Loss: 0.0667942464351654\n",
      "Epoch 12580/30000 Training Loss: 0.08680785447359085\n",
      "Epoch 12580/30000 Validation Loss: 0.07439584285020828\n",
      "Epoch 12581/30000 Training Loss: 0.07353004068136215\n",
      "Epoch 12582/30000 Training Loss: 0.06815323233604431\n",
      "Epoch 12583/30000 Training Loss: 0.08547419309616089\n",
      "Epoch 12584/30000 Training Loss: 0.08408153057098389\n",
      "Epoch 12585/30000 Training Loss: 0.07220154255628586\n",
      "Epoch 12586/30000 Training Loss: 0.07947520166635513\n",
      "Epoch 12587/30000 Training Loss: 0.08095831423997879\n",
      "Epoch 12588/30000 Training Loss: 0.06604596972465515\n",
      "Epoch 12589/30000 Training Loss: 0.08074513077735901\n",
      "Epoch 12590/30000 Training Loss: 0.08536934852600098\n",
      "Epoch 12590/30000 Validation Loss: 0.0785069540143013\n",
      "Epoch 12591/30000 Training Loss: 0.08231452852487564\n",
      "Epoch 12592/30000 Training Loss: 0.08502937108278275\n",
      "Epoch 12593/30000 Training Loss: 0.0834796354174614\n",
      "Epoch 12594/30000 Training Loss: 0.05681534484028816\n",
      "Epoch 12595/30000 Training Loss: 0.07172953337430954\n",
      "Epoch 12596/30000 Training Loss: 0.07386723905801773\n",
      "Epoch 12597/30000 Training Loss: 0.06675694137811661\n",
      "Epoch 12598/30000 Training Loss: 0.07945322245359421\n",
      "Epoch 12599/30000 Training Loss: 0.09075015783309937\n",
      "Epoch 12600/30000 Training Loss: 0.07399196177721024\n",
      "Epoch 12600/30000 Validation Loss: 0.0842166543006897\n",
      "Epoch 12601/30000 Training Loss: 0.0924539566040039\n",
      "Epoch 12602/30000 Training Loss: 0.08460190147161484\n",
      "Epoch 12603/30000 Training Loss: 0.06516701728105545\n",
      "Epoch 12604/30000 Training Loss: 0.09143684059381485\n",
      "Epoch 12605/30000 Training Loss: 0.06714408844709396\n",
      "Epoch 12606/30000 Training Loss: 0.09258631616830826\n",
      "Epoch 12607/30000 Training Loss: 0.0734577551484108\n",
      "Epoch 12608/30000 Training Loss: 0.08501613885164261\n",
      "Epoch 12609/30000 Training Loss: 0.0857570469379425\n",
      "Epoch 12610/30000 Training Loss: 0.07032129913568497\n",
      "Epoch 12610/30000 Validation Loss: 0.0785747691988945\n",
      "Epoch 12611/30000 Training Loss: 0.07353141158819199\n",
      "Epoch 12612/30000 Training Loss: 0.08151698857545853\n",
      "Epoch 12613/30000 Training Loss: 0.06509900838136673\n",
      "Epoch 12614/30000 Training Loss: 0.08610192686319351\n",
      "Epoch 12615/30000 Training Loss: 0.08833438158035278\n",
      "Epoch 12616/30000 Training Loss: 0.09428272396326065\n",
      "Epoch 12617/30000 Training Loss: 0.06000199913978577\n",
      "Epoch 12618/30000 Training Loss: 0.06701598316431046\n",
      "Epoch 12619/30000 Training Loss: 0.09203775972127914\n",
      "Epoch 12620/30000 Training Loss: 0.06784806400537491\n",
      "Epoch 12620/30000 Validation Loss: 0.0824737623333931\n",
      "Epoch 12621/30000 Training Loss: 0.0649954229593277\n",
      "Epoch 12622/30000 Training Loss: 0.05062250792980194\n",
      "Epoch 12623/30000 Training Loss: 0.07109472155570984\n",
      "Epoch 12624/30000 Training Loss: 0.06662430614233017\n",
      "Epoch 12625/30000 Training Loss: 0.07483938336372375\n",
      "Epoch 12626/30000 Training Loss: 0.07939509302377701\n",
      "Epoch 12627/30000 Training Loss: 0.08688338845968246\n",
      "Epoch 12628/30000 Training Loss: 0.08100289851427078\n",
      "Epoch 12629/30000 Training Loss: 0.06800061464309692\n",
      "Epoch 12630/30000 Training Loss: 0.08701983839273453\n",
      "Epoch 12630/30000 Validation Loss: 0.09303805232048035\n",
      "Epoch 12631/30000 Training Loss: 0.0682520642876625\n",
      "Epoch 12632/30000 Training Loss: 0.08984404802322388\n",
      "Epoch 12633/30000 Training Loss: 0.07033835351467133\n",
      "Epoch 12634/30000 Training Loss: 0.0723240077495575\n",
      "Epoch 12635/30000 Training Loss: 0.08278033882379532\n",
      "Epoch 12636/30000 Training Loss: 0.10583391040563583\n",
      "Epoch 12637/30000 Training Loss: 0.09112763404846191\n",
      "Epoch 12638/30000 Training Loss: 0.07341014593839645\n",
      "Epoch 12639/30000 Training Loss: 0.06266578286886215\n",
      "Epoch 12640/30000 Training Loss: 0.10255061835050583\n",
      "Epoch 12640/30000 Validation Loss: 0.07142192125320435\n",
      "Epoch 12641/30000 Training Loss: 0.06679505109786987\n",
      "Epoch 12642/30000 Training Loss: 0.07990318536758423\n",
      "Epoch 12643/30000 Training Loss: 0.07402671128511429\n",
      "Epoch 12644/30000 Training Loss: 0.08381549268960953\n",
      "Epoch 12645/30000 Training Loss: 0.06675449758768082\n",
      "Epoch 12646/30000 Training Loss: 0.07673513144254684\n",
      "Epoch 12647/30000 Training Loss: 0.06793857365846634\n",
      "Epoch 12648/30000 Training Loss: 0.06658512353897095\n",
      "Epoch 12649/30000 Training Loss: 0.0845346674323082\n",
      "Epoch 12650/30000 Training Loss: 0.0615660734474659\n",
      "Epoch 12650/30000 Validation Loss: 0.06615991145372391\n",
      "Epoch 12651/30000 Training Loss: 0.07652590423822403\n",
      "Epoch 12652/30000 Training Loss: 0.07741642743349075\n",
      "Epoch 12653/30000 Training Loss: 0.07664230465888977\n",
      "Epoch 12654/30000 Training Loss: 0.06803182512521744\n",
      "Epoch 12655/30000 Training Loss: 0.10483387857675552\n",
      "Epoch 12656/30000 Training Loss: 0.08162833005189896\n",
      "Epoch 12657/30000 Training Loss: 0.07443510740995407\n",
      "Epoch 12658/30000 Training Loss: 0.08013913035392761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12659/30000 Training Loss: 0.08523312956094742\n",
      "Epoch 12660/30000 Training Loss: 0.09085467457771301\n",
      "Epoch 12660/30000 Validation Loss: 0.07376045733690262\n",
      "Epoch 12661/30000 Training Loss: 0.07829037308692932\n",
      "Epoch 12662/30000 Training Loss: 0.07428881525993347\n",
      "Epoch 12663/30000 Training Loss: 0.0725296139717102\n",
      "Epoch 12664/30000 Training Loss: 0.08269166201353073\n",
      "Epoch 12665/30000 Training Loss: 0.09450932592153549\n",
      "Epoch 12666/30000 Training Loss: 0.07913145422935486\n",
      "Epoch 12667/30000 Training Loss: 0.06750599294900894\n",
      "Epoch 12668/30000 Training Loss: 0.0753486379981041\n",
      "Epoch 12669/30000 Training Loss: 0.08385858684778214\n",
      "Epoch 12670/30000 Training Loss: 0.09162121266126633\n",
      "Epoch 12670/30000 Validation Loss: 0.08496930450201035\n",
      "Epoch 12671/30000 Training Loss: 0.06709478050470352\n",
      "Epoch 12672/30000 Training Loss: 0.06665993481874466\n",
      "Epoch 12673/30000 Training Loss: 0.07948621362447739\n",
      "Epoch 12674/30000 Training Loss: 0.06532219052314758\n",
      "Epoch 12675/30000 Training Loss: 0.07575877755880356\n",
      "Epoch 12676/30000 Training Loss: 0.07335811108350754\n",
      "Epoch 12677/30000 Training Loss: 0.07508596777915955\n",
      "Epoch 12678/30000 Training Loss: 0.07562848180532455\n",
      "Epoch 12679/30000 Training Loss: 0.08984574675559998\n",
      "Epoch 12680/30000 Training Loss: 0.07977420091629028\n",
      "Epoch 12680/30000 Validation Loss: 0.06838308274745941\n",
      "Epoch 12681/30000 Training Loss: 0.053749337792396545\n",
      "Epoch 12682/30000 Training Loss: 0.0895112082362175\n",
      "Epoch 12683/30000 Training Loss: 0.08796418458223343\n",
      "Epoch 12684/30000 Training Loss: 0.07945171743631363\n",
      "Epoch 12685/30000 Training Loss: 0.07025311142206192\n",
      "Epoch 12686/30000 Training Loss: 0.08847025781869888\n",
      "Epoch 12687/30000 Training Loss: 0.06959585100412369\n",
      "Epoch 12688/30000 Training Loss: 0.09406255930662155\n",
      "Epoch 12689/30000 Training Loss: 0.0652642622590065\n",
      "Epoch 12690/30000 Training Loss: 0.11114150285720825\n",
      "Epoch 12690/30000 Validation Loss: 0.08311046659946442\n",
      "Epoch 12691/30000 Training Loss: 0.0737181007862091\n",
      "Epoch 12692/30000 Training Loss: 0.08851928263902664\n",
      "Epoch 12693/30000 Training Loss: 0.08500222116708755\n",
      "Epoch 12694/30000 Training Loss: 0.08383560180664062\n",
      "Epoch 12695/30000 Training Loss: 0.08512496948242188\n",
      "Epoch 12696/30000 Training Loss: 0.06415795534849167\n",
      "Epoch 12697/30000 Training Loss: 0.10024938732385635\n",
      "Epoch 12698/30000 Training Loss: 0.06553458422422409\n",
      "Epoch 12699/30000 Training Loss: 0.09004198759794235\n",
      "Epoch 12700/30000 Training Loss: 0.08681579679250717\n",
      "Epoch 12700/30000 Validation Loss: 0.05430888012051582\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05430888012051582<=============\n",
      "Epoch 12701/30000 Training Loss: 0.07471806555986404\n",
      "Epoch 12702/30000 Training Loss: 0.07719425112009048\n",
      "Epoch 12703/30000 Training Loss: 0.07723890990018845\n",
      "Epoch 12704/30000 Training Loss: 0.0729152038693428\n",
      "Epoch 12705/30000 Training Loss: 0.062332332134246826\n",
      "Epoch 12706/30000 Training Loss: 0.08797689527273178\n",
      "Epoch 12707/30000 Training Loss: 0.05201471969485283\n",
      "Epoch 12708/30000 Training Loss: 0.074800044298172\n",
      "Epoch 12709/30000 Training Loss: 0.07743687182664871\n",
      "Epoch 12710/30000 Training Loss: 0.07710502296686172\n",
      "Epoch 12710/30000 Validation Loss: 0.07715874910354614\n",
      "Epoch 12711/30000 Training Loss: 0.07558538764715195\n",
      "Epoch 12712/30000 Training Loss: 0.06403271108865738\n",
      "Epoch 12713/30000 Training Loss: 0.056579213589429855\n",
      "Epoch 12714/30000 Training Loss: 0.06536473333835602\n",
      "Epoch 12715/30000 Training Loss: 0.07106238603591919\n",
      "Epoch 12716/30000 Training Loss: 0.10190358012914658\n",
      "Epoch 12717/30000 Training Loss: 0.07863461971282959\n",
      "Epoch 12718/30000 Training Loss: 0.07454903423786163\n",
      "Epoch 12719/30000 Training Loss: 0.07174867391586304\n",
      "Epoch 12720/30000 Training Loss: 0.07436522841453552\n",
      "Epoch 12720/30000 Validation Loss: 0.06908157467842102\n",
      "Epoch 12721/30000 Training Loss: 0.08217224478721619\n",
      "Epoch 12722/30000 Training Loss: 0.07277128845453262\n",
      "Epoch 12723/30000 Training Loss: 0.06892982125282288\n",
      "Epoch 12724/30000 Training Loss: 0.08532654494047165\n",
      "Epoch 12725/30000 Training Loss: 0.07135274261236191\n",
      "Epoch 12726/30000 Training Loss: 0.05761316791176796\n",
      "Epoch 12727/30000 Training Loss: 0.05856223776936531\n",
      "Epoch 12728/30000 Training Loss: 0.07797705382108688\n",
      "Epoch 12729/30000 Training Loss: 0.07018407434225082\n",
      "Epoch 12730/30000 Training Loss: 0.08307872712612152\n",
      "Epoch 12730/30000 Validation Loss: 0.06071789562702179\n",
      "Epoch 12731/30000 Training Loss: 0.0757865384221077\n",
      "Epoch 12732/30000 Training Loss: 0.0660218670964241\n",
      "Epoch 12733/30000 Training Loss: 0.0710776075720787\n",
      "Epoch 12734/30000 Training Loss: 0.06764280796051025\n",
      "Epoch 12735/30000 Training Loss: 0.06542841345071793\n",
      "Epoch 12736/30000 Training Loss: 0.07660501450300217\n",
      "Epoch 12737/30000 Training Loss: 0.07302326709032059\n",
      "Epoch 12738/30000 Training Loss: 0.07928464561700821\n",
      "Epoch 12739/30000 Training Loss: 0.075469970703125\n",
      "Epoch 12740/30000 Training Loss: 0.08703357726335526\n",
      "Epoch 12740/30000 Validation Loss: 0.09773453325033188\n",
      "Epoch 12741/30000 Training Loss: 0.07193225622177124\n",
      "Epoch 12742/30000 Training Loss: 0.0750318095088005\n",
      "Epoch 12743/30000 Training Loss: 0.074713334441185\n",
      "Epoch 12744/30000 Training Loss: 0.07837773114442825\n",
      "Epoch 12745/30000 Training Loss: 0.07529345154762268\n",
      "Epoch 12746/30000 Training Loss: 0.08038147538900375\n",
      "Epoch 12747/30000 Training Loss: 0.07929430156946182\n",
      "Epoch 12748/30000 Training Loss: 0.08003146201372147\n",
      "Epoch 12749/30000 Training Loss: 0.07633636146783829\n",
      "Epoch 12750/30000 Training Loss: 0.07498074322938919\n",
      "Epoch 12750/30000 Validation Loss: 0.060079868882894516\n",
      "Epoch 12751/30000 Training Loss: 0.07422185689210892\n",
      "Epoch 12752/30000 Training Loss: 0.08368179202079773\n",
      "Epoch 12753/30000 Training Loss: 0.06749691814184189\n",
      "Epoch 12754/30000 Training Loss: 0.08131717890501022\n",
      "Epoch 12755/30000 Training Loss: 0.06437929719686508\n",
      "Epoch 12756/30000 Training Loss: 0.07318031042814255\n",
      "Epoch 12757/30000 Training Loss: 0.08972956985235214\n",
      "Epoch 12758/30000 Training Loss: 0.07248249650001526\n",
      "Epoch 12759/30000 Training Loss: 0.06517528742551804\n",
      "Epoch 12760/30000 Training Loss: 0.07005129754543304\n",
      "Epoch 12760/30000 Validation Loss: 0.06671852618455887\n",
      "Epoch 12761/30000 Training Loss: 0.07539081573486328\n",
      "Epoch 12762/30000 Training Loss: 0.05383797362446785\n",
      "Epoch 12763/30000 Training Loss: 0.058498602360486984\n",
      "Epoch 12764/30000 Training Loss: 0.09318539500236511\n",
      "Epoch 12765/30000 Training Loss: 0.0781475231051445\n",
      "Epoch 12766/30000 Training Loss: 0.09292283654212952\n",
      "Epoch 12767/30000 Training Loss: 0.09223341196775436\n",
      "Epoch 12768/30000 Training Loss: 0.06909951567649841\n",
      "Epoch 12769/30000 Training Loss: 0.0819283053278923\n",
      "Epoch 12770/30000 Training Loss: 0.06416475772857666\n",
      "Epoch 12770/30000 Validation Loss: 0.08308667689561844\n",
      "Epoch 12771/30000 Training Loss: 0.08549314737319946\n",
      "Epoch 12772/30000 Training Loss: 0.0603700689971447\n",
      "Epoch 12773/30000 Training Loss: 0.07380158454179764\n",
      "Epoch 12774/30000 Training Loss: 0.08829527348279953\n",
      "Epoch 12775/30000 Training Loss: 0.07780653238296509\n",
      "Epoch 12776/30000 Training Loss: 0.07049766927957535\n",
      "Epoch 12777/30000 Training Loss: 0.07286689430475235\n",
      "Epoch 12778/30000 Training Loss: 0.08684685081243515\n",
      "Epoch 12779/30000 Training Loss: 0.07682734727859497\n",
      "Epoch 12780/30000 Training Loss: 0.08336690813302994\n",
      "Epoch 12780/30000 Validation Loss: 0.07848197966814041\n",
      "Epoch 12781/30000 Training Loss: 0.08816502243280411\n",
      "Epoch 12782/30000 Training Loss: 0.06136847659945488\n",
      "Epoch 12783/30000 Training Loss: 0.07032947987318039\n",
      "Epoch 12784/30000 Training Loss: 0.06969830393791199\n",
      "Epoch 12785/30000 Training Loss: 0.08903533965349197\n",
      "Epoch 12786/30000 Training Loss: 0.08935777097940445\n",
      "Epoch 12787/30000 Training Loss: 0.06540138274431229\n",
      "Epoch 12788/30000 Training Loss: 0.07077131420373917\n",
      "Epoch 12789/30000 Training Loss: 0.06970515102148056\n",
      "Epoch 12790/30000 Training Loss: 0.07910829782485962\n",
      "Epoch 12790/30000 Validation Loss: 0.06704822927713394\n",
      "Epoch 12791/30000 Training Loss: 0.07066067308187485\n",
      "Epoch 12792/30000 Training Loss: 0.07790298014879227\n",
      "Epoch 12793/30000 Training Loss: 0.08964387327432632\n",
      "Epoch 12794/30000 Training Loss: 0.07247618585824966\n",
      "Epoch 12795/30000 Training Loss: 0.07679077237844467\n",
      "Epoch 12796/30000 Training Loss: 0.06770806759595871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12797/30000 Training Loss: 0.09533794969320297\n",
      "Epoch 12798/30000 Training Loss: 0.09691251069307327\n",
      "Epoch 12799/30000 Training Loss: 0.08060687780380249\n",
      "Epoch 12800/30000 Training Loss: 0.07788126915693283\n",
      "Epoch 12800/30000 Validation Loss: 0.07567209750413895\n",
      "Epoch 12801/30000 Training Loss: 0.09125534445047379\n",
      "Epoch 12802/30000 Training Loss: 0.08058644086122513\n",
      "Epoch 12803/30000 Training Loss: 0.09286455065011978\n",
      "Epoch 12804/30000 Training Loss: 0.0745849534869194\n",
      "Epoch 12805/30000 Training Loss: 0.07216610759496689\n",
      "Epoch 12806/30000 Training Loss: 0.08283805102109909\n",
      "Epoch 12807/30000 Training Loss: 0.07707303017377853\n",
      "Epoch 12808/30000 Training Loss: 0.08766528964042664\n",
      "Epoch 12809/30000 Training Loss: 0.08843794465065002\n",
      "Epoch 12810/30000 Training Loss: 0.07451248168945312\n",
      "Epoch 12810/30000 Validation Loss: 0.09548904746770859\n",
      "Epoch 12811/30000 Training Loss: 0.09400782734155655\n",
      "Epoch 12812/30000 Training Loss: 0.07386665791273117\n",
      "Epoch 12813/30000 Training Loss: 0.0843721553683281\n",
      "Epoch 12814/30000 Training Loss: 0.10318390280008316\n",
      "Epoch 12815/30000 Training Loss: 0.06972313672304153\n",
      "Epoch 12816/30000 Training Loss: 0.0631270781159401\n",
      "Epoch 12817/30000 Training Loss: 0.07338986545801163\n",
      "Epoch 12818/30000 Training Loss: 0.07225292176008224\n",
      "Epoch 12819/30000 Training Loss: 0.08475054055452347\n",
      "Epoch 12820/30000 Training Loss: 0.1127178892493248\n",
      "Epoch 12820/30000 Validation Loss: 0.06969819217920303\n",
      "Epoch 12821/30000 Training Loss: 0.07677283883094788\n",
      "Epoch 12822/30000 Training Loss: 0.06482791155576706\n",
      "Epoch 12823/30000 Training Loss: 0.0924636647105217\n",
      "Epoch 12824/30000 Training Loss: 0.053681761026382446\n",
      "Epoch 12825/30000 Training Loss: 0.09220495074987411\n",
      "Epoch 12826/30000 Training Loss: 0.07640787959098816\n",
      "Epoch 12827/30000 Training Loss: 0.08595480769872665\n",
      "Epoch 12828/30000 Training Loss: 0.06661168485879898\n",
      "Epoch 12829/30000 Training Loss: 0.07488203793764114\n",
      "Epoch 12830/30000 Training Loss: 0.06965035945177078\n",
      "Epoch 12830/30000 Validation Loss: 0.06434104591608047\n",
      "Epoch 12831/30000 Training Loss: 0.09018304198980331\n",
      "Epoch 12832/30000 Training Loss: 0.07175209373235703\n",
      "Epoch 12833/30000 Training Loss: 0.0951368436217308\n",
      "Epoch 12834/30000 Training Loss: 0.08088068664073944\n",
      "Epoch 12835/30000 Training Loss: 0.07056163996458054\n",
      "Epoch 12836/30000 Training Loss: 0.08895207196474075\n",
      "Epoch 12837/30000 Training Loss: 0.07426699250936508\n",
      "Epoch 12838/30000 Training Loss: 0.08284955471754074\n",
      "Epoch 12839/30000 Training Loss: 0.089420847594738\n",
      "Epoch 12840/30000 Training Loss: 0.07760543376207352\n",
      "Epoch 12840/30000 Validation Loss: 0.07003363221883774\n",
      "Epoch 12841/30000 Training Loss: 0.06973019987344742\n",
      "Epoch 12842/30000 Training Loss: 0.0641811415553093\n",
      "Epoch 12843/30000 Training Loss: 0.07371161133050919\n",
      "Epoch 12844/30000 Training Loss: 0.08454068750143051\n",
      "Epoch 12845/30000 Training Loss: 0.06944159418344498\n",
      "Epoch 12846/30000 Training Loss: 0.07570119947195053\n",
      "Epoch 12847/30000 Training Loss: 0.06723302602767944\n",
      "Epoch 12848/30000 Training Loss: 0.07617679238319397\n",
      "Epoch 12849/30000 Training Loss: 0.06337784975767136\n",
      "Epoch 12850/30000 Training Loss: 0.08252772688865662\n",
      "Epoch 12850/30000 Validation Loss: 0.060798659920692444\n",
      "Epoch 12851/30000 Training Loss: 0.10736957937479019\n",
      "Epoch 12852/30000 Training Loss: 0.06815958768129349\n",
      "Epoch 12853/30000 Training Loss: 0.07259122282266617\n",
      "Epoch 12854/30000 Training Loss: 0.0827808678150177\n",
      "Epoch 12855/30000 Training Loss: 0.08167216926813126\n",
      "Epoch 12856/30000 Training Loss: 0.06532341241836548\n",
      "Epoch 12857/30000 Training Loss: 0.06585005670785904\n",
      "Epoch 12858/30000 Training Loss: 0.07631289958953857\n",
      "Epoch 12859/30000 Training Loss: 0.0696367546916008\n",
      "Epoch 12860/30000 Training Loss: 0.0711495503783226\n",
      "Epoch 12860/30000 Validation Loss: 0.07151975482702255\n",
      "Epoch 12861/30000 Training Loss: 0.06906571984291077\n",
      "Epoch 12862/30000 Training Loss: 0.06227170303463936\n",
      "Epoch 12863/30000 Training Loss: 0.09308154135942459\n",
      "Epoch 12864/30000 Training Loss: 0.07571280747652054\n",
      "Epoch 12865/30000 Training Loss: 0.0679364800453186\n",
      "Epoch 12866/30000 Training Loss: 0.06718380004167557\n",
      "Epoch 12867/30000 Training Loss: 0.0846470296382904\n",
      "Epoch 12868/30000 Training Loss: 0.07043679058551788\n",
      "Epoch 12869/30000 Training Loss: 0.0649276152253151\n",
      "Epoch 12870/30000 Training Loss: 0.08354439586400986\n",
      "Epoch 12870/30000 Validation Loss: 0.06893759220838547\n",
      "Epoch 12871/30000 Training Loss: 0.08037450909614563\n",
      "Epoch 12872/30000 Training Loss: 0.06680122762918472\n",
      "Epoch 12873/30000 Training Loss: 0.07186149805784225\n",
      "Epoch 12874/30000 Training Loss: 0.07921794056892395\n",
      "Epoch 12875/30000 Training Loss: 0.07250668853521347\n",
      "Epoch 12876/30000 Training Loss: 0.08662751317024231\n",
      "Epoch 12877/30000 Training Loss: 0.09252817183732986\n",
      "Epoch 12878/30000 Training Loss: 0.08079230040311813\n",
      "Epoch 12879/30000 Training Loss: 0.08150654286146164\n",
      "Epoch 12880/30000 Training Loss: 0.07853752374649048\n",
      "Epoch 12880/30000 Validation Loss: 0.07499641925096512\n",
      "Epoch 12881/30000 Training Loss: 0.07020922750234604\n",
      "Epoch 12882/30000 Training Loss: 0.07476738840341568\n",
      "Epoch 12883/30000 Training Loss: 0.06982076913118362\n",
      "Epoch 12884/30000 Training Loss: 0.0754518136382103\n",
      "Epoch 12885/30000 Training Loss: 0.08787775039672852\n",
      "Epoch 12886/30000 Training Loss: 0.0677269697189331\n",
      "Epoch 12887/30000 Training Loss: 0.09380894154310226\n",
      "Epoch 12888/30000 Training Loss: 0.08840864896774292\n",
      "Epoch 12889/30000 Training Loss: 0.08901096135377884\n",
      "Epoch 12890/30000 Training Loss: 0.07835352420806885\n",
      "Epoch 12890/30000 Validation Loss: 0.10382393002510071\n",
      "Epoch 12891/30000 Training Loss: 0.07712601870298386\n",
      "Epoch 12892/30000 Training Loss: 0.07471150904893875\n",
      "Epoch 12893/30000 Training Loss: 0.07584688812494278\n",
      "Epoch 12894/30000 Training Loss: 0.0705312192440033\n",
      "Epoch 12895/30000 Training Loss: 0.06953764706850052\n",
      "Epoch 12896/30000 Training Loss: 0.07616573572158813\n",
      "Epoch 12897/30000 Training Loss: 0.07815638929605484\n",
      "Epoch 12898/30000 Training Loss: 0.0672004446387291\n",
      "Epoch 12899/30000 Training Loss: 0.06542224436998367\n",
      "Epoch 12900/30000 Training Loss: 0.0766487792134285\n",
      "Epoch 12900/30000 Validation Loss: 0.07374846935272217\n",
      "Epoch 12901/30000 Training Loss: 0.07850103825330734\n",
      "Epoch 12902/30000 Training Loss: 0.07883235067129135\n",
      "Epoch 12903/30000 Training Loss: 0.0733482614159584\n",
      "Epoch 12904/30000 Training Loss: 0.07969635725021362\n",
      "Epoch 12905/30000 Training Loss: 0.06657988578081131\n",
      "Epoch 12906/30000 Training Loss: 0.07263863831758499\n",
      "Epoch 12907/30000 Training Loss: 0.088901586830616\n",
      "Epoch 12908/30000 Training Loss: 0.08159542828798294\n",
      "Epoch 12909/30000 Training Loss: 0.08041612058877945\n",
      "Epoch 12910/30000 Training Loss: 0.06551078706979752\n",
      "Epoch 12910/30000 Validation Loss: 0.09965679794549942\n",
      "Epoch 12911/30000 Training Loss: 0.06767114996910095\n",
      "Epoch 12912/30000 Training Loss: 0.07103065401315689\n",
      "Epoch 12913/30000 Training Loss: 0.06692513078451157\n",
      "Epoch 12914/30000 Training Loss: 0.06519880890846252\n",
      "Epoch 12915/30000 Training Loss: 0.08669682592153549\n",
      "Epoch 12916/30000 Training Loss: 0.09427502751350403\n",
      "Epoch 12917/30000 Training Loss: 0.07584796100854874\n",
      "Epoch 12918/30000 Training Loss: 0.08253408223390579\n",
      "Epoch 12919/30000 Training Loss: 0.07032138109207153\n",
      "Epoch 12920/30000 Training Loss: 0.09318212419748306\n",
      "Epoch 12920/30000 Validation Loss: 0.08090060204267502\n",
      "Epoch 12921/30000 Training Loss: 0.08542466163635254\n",
      "Epoch 12922/30000 Training Loss: 0.08586598187685013\n",
      "Epoch 12923/30000 Training Loss: 0.08411580324172974\n",
      "Epoch 12924/30000 Training Loss: 0.06114955618977547\n",
      "Epoch 12925/30000 Training Loss: 0.07672572135925293\n",
      "Epoch 12926/30000 Training Loss: 0.07221706211566925\n",
      "Epoch 12927/30000 Training Loss: 0.07646433264017105\n",
      "Epoch 12928/30000 Training Loss: 0.07407191395759583\n",
      "Epoch 12929/30000 Training Loss: 0.08030277490615845\n",
      "Epoch 12930/30000 Training Loss: 0.08426836878061295\n",
      "Epoch 12930/30000 Validation Loss: 0.07762247323989868\n",
      "Epoch 12931/30000 Training Loss: 0.06342682987451553\n",
      "Epoch 12932/30000 Training Loss: 0.06685037165880203\n",
      "Epoch 12933/30000 Training Loss: 0.06866750121116638\n",
      "Epoch 12934/30000 Training Loss: 0.07393281906843185\n",
      "Epoch 12935/30000 Training Loss: 0.05778009817004204\n",
      "Epoch 12936/30000 Training Loss: 0.07109475880861282\n",
      "Epoch 12937/30000 Training Loss: 0.06795775890350342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12938/30000 Training Loss: 0.07659939676523209\n",
      "Epoch 12939/30000 Training Loss: 0.06786877661943436\n",
      "Epoch 12940/30000 Training Loss: 0.08229433000087738\n",
      "Epoch 12940/30000 Validation Loss: 0.08325674384832382\n",
      "Epoch 12941/30000 Training Loss: 0.07965675741434097\n",
      "Epoch 12942/30000 Training Loss: 0.08004402369260788\n",
      "Epoch 12943/30000 Training Loss: 0.08032698184251785\n",
      "Epoch 12944/30000 Training Loss: 0.07258657366037369\n",
      "Epoch 12945/30000 Training Loss: 0.09065674990415573\n",
      "Epoch 12946/30000 Training Loss: 0.05897270143032074\n",
      "Epoch 12947/30000 Training Loss: 0.08711377531290054\n",
      "Epoch 12948/30000 Training Loss: 0.0824386477470398\n",
      "Epoch 12949/30000 Training Loss: 0.07267264276742935\n",
      "Epoch 12950/30000 Training Loss: 0.07030727714300156\n",
      "Epoch 12950/30000 Validation Loss: 0.08008839190006256\n",
      "Epoch 12951/30000 Training Loss: 0.07276859134435654\n",
      "Epoch 12952/30000 Training Loss: 0.07186917215585709\n",
      "Epoch 12953/30000 Training Loss: 0.09744594246149063\n",
      "Epoch 12954/30000 Training Loss: 0.06729797273874283\n",
      "Epoch 12955/30000 Training Loss: 0.06931395083665848\n",
      "Epoch 12956/30000 Training Loss: 0.06259306520223618\n",
      "Epoch 12957/30000 Training Loss: 0.08889657258987427\n",
      "Epoch 12958/30000 Training Loss: 0.06433970481157303\n",
      "Epoch 12959/30000 Training Loss: 0.06982805579900742\n",
      "Epoch 12960/30000 Training Loss: 0.06671752780675888\n",
      "Epoch 12960/30000 Validation Loss: 0.08418655395507812\n",
      "Epoch 12961/30000 Training Loss: 0.07015904784202576\n",
      "Epoch 12962/30000 Training Loss: 0.09406080842018127\n",
      "Epoch 12963/30000 Training Loss: 0.06189526617527008\n",
      "Epoch 12964/30000 Training Loss: 0.06878763437271118\n",
      "Epoch 12965/30000 Training Loss: 0.07229814678430557\n",
      "Epoch 12966/30000 Training Loss: 0.07550262659788132\n",
      "Epoch 12967/30000 Training Loss: 0.09588077664375305\n",
      "Epoch 12968/30000 Training Loss: 0.08659205585718155\n",
      "Epoch 12969/30000 Training Loss: 0.07533026486635208\n",
      "Epoch 12970/30000 Training Loss: 0.07433847337961197\n",
      "Epoch 12970/30000 Validation Loss: 0.09067108482122421\n",
      "Epoch 12971/30000 Training Loss: 0.07309013605117798\n",
      "Epoch 12972/30000 Training Loss: 0.07270757108926773\n",
      "Epoch 12973/30000 Training Loss: 0.08404320478439331\n",
      "Epoch 12974/30000 Training Loss: 0.05612548813223839\n",
      "Epoch 12975/30000 Training Loss: 0.08054721355438232\n",
      "Epoch 12976/30000 Training Loss: 0.10026253014802933\n",
      "Epoch 12977/30000 Training Loss: 0.07063964009284973\n",
      "Epoch 12978/30000 Training Loss: 0.08012387901544571\n",
      "Epoch 12979/30000 Training Loss: 0.06700561195611954\n",
      "Epoch 12980/30000 Training Loss: 0.0670243427157402\n",
      "Epoch 12980/30000 Validation Loss: 0.07304742187261581\n",
      "Epoch 12981/30000 Training Loss: 0.078098826110363\n",
      "Epoch 12982/30000 Training Loss: 0.061666976660490036\n",
      "Epoch 12983/30000 Training Loss: 0.05509118735790253\n",
      "Epoch 12984/30000 Training Loss: 0.07962861657142639\n",
      "Epoch 12985/30000 Training Loss: 0.08144181221723557\n",
      "Epoch 12986/30000 Training Loss: 0.08732672780752182\n",
      "Epoch 12987/30000 Training Loss: 0.08279944211244583\n",
      "Epoch 12988/30000 Training Loss: 0.0829273983836174\n",
      "Epoch 12989/30000 Training Loss: 0.07098982483148575\n",
      "Epoch 12990/30000 Training Loss: 0.0601903460919857\n",
      "Epoch 12990/30000 Validation Loss: 0.058023955672979355\n",
      "Epoch 12991/30000 Training Loss: 0.08533606678247452\n",
      "Epoch 12992/30000 Training Loss: 0.07171034812927246\n",
      "Epoch 12993/30000 Training Loss: 0.07733022421598434\n",
      "Epoch 12994/30000 Training Loss: 0.08314558863639832\n",
      "Epoch 12995/30000 Training Loss: 0.10848603397607803\n",
      "Epoch 12996/30000 Training Loss: 0.10419609397649765\n",
      "Epoch 12997/30000 Training Loss: 0.07408414036035538\n",
      "Epoch 12998/30000 Training Loss: 0.05882922187447548\n",
      "Epoch 12999/30000 Training Loss: 0.09047698229551315\n",
      "Epoch 13000/30000 Training Loss: 0.07645142823457718\n",
      "Epoch 13000/30000 Validation Loss: 0.08808353543281555\n",
      "Epoch 13001/30000 Training Loss: 0.08615667372941971\n",
      "Epoch 13002/30000 Training Loss: 0.07939819246530533\n",
      "Epoch 13003/30000 Training Loss: 0.07764199376106262\n",
      "Epoch 13004/30000 Training Loss: 0.07705142349004745\n",
      "Epoch 13005/30000 Training Loss: 0.07155204564332962\n",
      "Epoch 13006/30000 Training Loss: 0.07493781298398972\n",
      "Epoch 13007/30000 Training Loss: 0.08381873369216919\n",
      "Epoch 13008/30000 Training Loss: 0.07690469175577164\n",
      "Epoch 13009/30000 Training Loss: 0.05870042368769646\n",
      "Epoch 13010/30000 Training Loss: 0.08012184500694275\n",
      "Epoch 13010/30000 Validation Loss: 0.06271719932556152\n",
      "Epoch 13011/30000 Training Loss: 0.07345112413167953\n",
      "Epoch 13012/30000 Training Loss: 0.07053817063570023\n",
      "Epoch 13013/30000 Training Loss: 0.09182751923799515\n",
      "Epoch 13014/30000 Training Loss: 0.06734044849872589\n",
      "Epoch 13015/30000 Training Loss: 0.07588905841112137\n",
      "Epoch 13016/30000 Training Loss: 0.07003163546323776\n",
      "Epoch 13017/30000 Training Loss: 0.09352105855941772\n",
      "Epoch 13018/30000 Training Loss: 0.07645528763532639\n",
      "Epoch 13019/30000 Training Loss: 0.06338417530059814\n",
      "Epoch 13020/30000 Training Loss: 0.085368812084198\n",
      "Epoch 13020/30000 Validation Loss: 0.08333927392959595\n",
      "Epoch 13021/30000 Training Loss: 0.06823073327541351\n",
      "Epoch 13022/30000 Training Loss: 0.08212824910879135\n",
      "Epoch 13023/30000 Training Loss: 0.07763192802667618\n",
      "Epoch 13024/30000 Training Loss: 0.08206947892904282\n",
      "Epoch 13025/30000 Training Loss: 0.060258593410253525\n",
      "Epoch 13026/30000 Training Loss: 0.08087283372879028\n",
      "Epoch 13027/30000 Training Loss: 0.07459350675344467\n",
      "Epoch 13028/30000 Training Loss: 0.07217373698949814\n",
      "Epoch 13029/30000 Training Loss: 0.0748862624168396\n",
      "Epoch 13030/30000 Training Loss: 0.0884089544415474\n",
      "Epoch 13030/30000 Validation Loss: 0.06325938552618027\n",
      "Epoch 13031/30000 Training Loss: 0.07983367890119553\n",
      "Epoch 13032/30000 Training Loss: 0.07409486919641495\n",
      "Epoch 13033/30000 Training Loss: 0.06731338053941727\n",
      "Epoch 13034/30000 Training Loss: 0.08600933104753494\n",
      "Epoch 13035/30000 Training Loss: 0.06247563287615776\n",
      "Epoch 13036/30000 Training Loss: 0.07044025510549545\n",
      "Epoch 13037/30000 Training Loss: 0.10429162532091141\n",
      "Epoch 13038/30000 Training Loss: 0.08361469954252243\n",
      "Epoch 13039/30000 Training Loss: 0.08681270480155945\n",
      "Epoch 13040/30000 Training Loss: 0.08302021026611328\n",
      "Epoch 13040/30000 Validation Loss: 0.07031498104333878\n",
      "Epoch 13041/30000 Training Loss: 0.08552283048629761\n",
      "Epoch 13042/30000 Training Loss: 0.09058933705091476\n",
      "Epoch 13043/30000 Training Loss: 0.07897193729877472\n",
      "Epoch 13044/30000 Training Loss: 0.09705562144517899\n",
      "Epoch 13045/30000 Training Loss: 0.06824728101491928\n",
      "Epoch 13046/30000 Training Loss: 0.08226902782917023\n",
      "Epoch 13047/30000 Training Loss: 0.06049264594912529\n",
      "Epoch 13048/30000 Training Loss: 0.0756581649184227\n",
      "Epoch 13049/30000 Training Loss: 0.06414591521024704\n",
      "Epoch 13050/30000 Training Loss: 0.0817359983921051\n",
      "Epoch 13050/30000 Validation Loss: 0.06603047251701355\n",
      "Epoch 13051/30000 Training Loss: 0.0724940299987793\n",
      "Epoch 13052/30000 Training Loss: 0.10264727473258972\n",
      "Epoch 13053/30000 Training Loss: 0.06428530067205429\n",
      "Epoch 13054/30000 Training Loss: 0.07247132807970047\n",
      "Epoch 13055/30000 Training Loss: 0.056248247623443604\n",
      "Epoch 13056/30000 Training Loss: 0.06793024390935898\n",
      "Epoch 13057/30000 Training Loss: 0.07547899335622787\n",
      "Epoch 13058/30000 Training Loss: 0.07826625555753708\n",
      "Epoch 13059/30000 Training Loss: 0.0940793976187706\n",
      "Epoch 13060/30000 Training Loss: 0.06352213025093079\n",
      "Epoch 13060/30000 Validation Loss: 0.07241992652416229\n",
      "Epoch 13061/30000 Training Loss: 0.062314748764038086\n",
      "Epoch 13062/30000 Training Loss: 0.08913803100585938\n",
      "Epoch 13063/30000 Training Loss: 0.08412935584783554\n",
      "Epoch 13064/30000 Training Loss: 0.07224157452583313\n",
      "Epoch 13065/30000 Training Loss: 0.08344686776399612\n",
      "Epoch 13066/30000 Training Loss: 0.08834481239318848\n",
      "Epoch 13067/30000 Training Loss: 0.07988589257001877\n",
      "Epoch 13068/30000 Training Loss: 0.07487238198518753\n",
      "Epoch 13069/30000 Training Loss: 0.08104265481233597\n",
      "Epoch 13070/30000 Training Loss: 0.0781349167227745\n",
      "Epoch 13070/30000 Validation Loss: 0.1006535068154335\n",
      "Epoch 13071/30000 Training Loss: 0.05738738179206848\n",
      "Epoch 13072/30000 Training Loss: 0.08519574254751205\n",
      "Epoch 13073/30000 Training Loss: 0.05699663236737251\n",
      "Epoch 13074/30000 Training Loss: 0.10835099220275879\n",
      "Epoch 13075/30000 Training Loss: 0.0630936548113823\n",
      "Epoch 13076/30000 Training Loss: 0.07741738855838776\n",
      "Epoch 13077/30000 Training Loss: 0.06609061360359192\n",
      "Epoch 13078/30000 Training Loss: 0.06261070817708969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13079/30000 Training Loss: 0.07253533601760864\n",
      "Epoch 13080/30000 Training Loss: 0.0793566107749939\n",
      "Epoch 13080/30000 Validation Loss: 0.0802161693572998\n",
      "Epoch 13081/30000 Training Loss: 0.06833881884813309\n",
      "Epoch 13082/30000 Training Loss: 0.07907253503799438\n",
      "Epoch 13083/30000 Training Loss: 0.0717310979962349\n",
      "Epoch 13084/30000 Training Loss: 0.09676879644393921\n",
      "Epoch 13085/30000 Training Loss: 0.08992979675531387\n",
      "Epoch 13086/30000 Training Loss: 0.08354034274816513\n",
      "Epoch 13087/30000 Training Loss: 0.08553779870271683\n",
      "Epoch 13088/30000 Training Loss: 0.06315036863088608\n",
      "Epoch 13089/30000 Training Loss: 0.08054973930120468\n",
      "Epoch 13090/30000 Training Loss: 0.07377640157938004\n",
      "Epoch 13090/30000 Validation Loss: 0.06712416559457779\n",
      "Epoch 13091/30000 Training Loss: 0.08560303598642349\n",
      "Epoch 13092/30000 Training Loss: 0.07583820074796677\n",
      "Epoch 13093/30000 Training Loss: 0.06186265870928764\n",
      "Epoch 13094/30000 Training Loss: 0.07392320036888123\n",
      "Epoch 13095/30000 Training Loss: 0.0827609971165657\n",
      "Epoch 13096/30000 Training Loss: 0.08317961543798447\n",
      "Epoch 13097/30000 Training Loss: 0.08305028080940247\n",
      "Epoch 13098/30000 Training Loss: 0.08539513498544693\n",
      "Epoch 13099/30000 Training Loss: 0.06210649013519287\n",
      "Epoch 13100/30000 Training Loss: 0.07332056760787964\n",
      "Epoch 13100/30000 Validation Loss: 0.0664149671792984\n",
      "Epoch 13101/30000 Training Loss: 0.08239936083555222\n",
      "Epoch 13102/30000 Training Loss: 0.0982927680015564\n",
      "Epoch 13103/30000 Training Loss: 0.07696440070867538\n",
      "Epoch 13104/30000 Training Loss: 0.07260210067033768\n",
      "Epoch 13105/30000 Training Loss: 0.06382112950086594\n",
      "Epoch 13106/30000 Training Loss: 0.07386880367994308\n",
      "Epoch 13107/30000 Training Loss: 0.06011129543185234\n",
      "Epoch 13108/30000 Training Loss: 0.08771225064992905\n",
      "Epoch 13109/30000 Training Loss: 0.07620665431022644\n",
      "Epoch 13110/30000 Training Loss: 0.07344353944063187\n",
      "Epoch 13110/30000 Validation Loss: 0.07610953599214554\n",
      "Epoch 13111/30000 Training Loss: 0.06293574720621109\n",
      "Epoch 13112/30000 Training Loss: 0.08789397031068802\n",
      "Epoch 13113/30000 Training Loss: 0.0627887025475502\n",
      "Epoch 13114/30000 Training Loss: 0.08432202786207199\n",
      "Epoch 13115/30000 Training Loss: 0.07657857984304428\n",
      "Epoch 13116/30000 Training Loss: 0.0960712730884552\n",
      "Epoch 13117/30000 Training Loss: 0.08959192782640457\n",
      "Epoch 13118/30000 Training Loss: 0.07932573556900024\n",
      "Epoch 13119/30000 Training Loss: 0.06842956691980362\n",
      "Epoch 13120/30000 Training Loss: 0.05897660180926323\n",
      "Epoch 13120/30000 Validation Loss: 0.08367397636175156\n",
      "Epoch 13121/30000 Training Loss: 0.08880316466093063\n",
      "Epoch 13122/30000 Training Loss: 0.06271690875291824\n",
      "Epoch 13123/30000 Training Loss: 0.09673330187797546\n",
      "Epoch 13124/30000 Training Loss: 0.08955663442611694\n",
      "Epoch 13125/30000 Training Loss: 0.06743968278169632\n",
      "Epoch 13126/30000 Training Loss: 0.08411171287298203\n",
      "Epoch 13127/30000 Training Loss: 0.07692433148622513\n",
      "Epoch 13128/30000 Training Loss: 0.11840268224477768\n",
      "Epoch 13129/30000 Training Loss: 0.06768239289522171\n",
      "Epoch 13130/30000 Training Loss: 0.07462456077337265\n",
      "Epoch 13130/30000 Validation Loss: 0.08995554596185684\n",
      "Epoch 13131/30000 Training Loss: 0.0622430145740509\n",
      "Epoch 13132/30000 Training Loss: 0.06674924492835999\n",
      "Epoch 13133/30000 Training Loss: 0.06570465117692947\n",
      "Epoch 13134/30000 Training Loss: 0.08142399042844772\n",
      "Epoch 13135/30000 Training Loss: 0.07459750026464462\n",
      "Epoch 13136/30000 Training Loss: 0.09317079931497574\n",
      "Epoch 13137/30000 Training Loss: 0.07606372982263565\n",
      "Epoch 13138/30000 Training Loss: 0.08411004394292831\n",
      "Epoch 13139/30000 Training Loss: 0.07125118374824524\n",
      "Epoch 13140/30000 Training Loss: 0.06277153640985489\n",
      "Epoch 13140/30000 Validation Loss: 0.08011850714683533\n",
      "Epoch 13141/30000 Training Loss: 0.09172447770833969\n",
      "Epoch 13142/30000 Training Loss: 0.1088237389922142\n",
      "Epoch 13143/30000 Training Loss: 0.0723194107413292\n",
      "Epoch 13144/30000 Training Loss: 0.08638521283864975\n",
      "Epoch 13145/30000 Training Loss: 0.07698523998260498\n",
      "Epoch 13146/30000 Training Loss: 0.06719636172056198\n",
      "Epoch 13147/30000 Training Loss: 0.07204350084066391\n",
      "Epoch 13148/30000 Training Loss: 0.06713686138391495\n",
      "Epoch 13149/30000 Training Loss: 0.0722469612956047\n",
      "Epoch 13150/30000 Training Loss: 0.06876599043607712\n",
      "Epoch 13150/30000 Validation Loss: 0.06782850623130798\n",
      "Epoch 13151/30000 Training Loss: 0.10146570950746536\n",
      "Epoch 13152/30000 Training Loss: 0.07902691513299942\n",
      "Epoch 13153/30000 Training Loss: 0.058142438530921936\n",
      "Epoch 13154/30000 Training Loss: 0.07189803570508957\n",
      "Epoch 13155/30000 Training Loss: 0.09100651741027832\n",
      "Epoch 13156/30000 Training Loss: 0.06783724576234818\n",
      "Epoch 13157/30000 Training Loss: 0.0817156434059143\n",
      "Epoch 13158/30000 Training Loss: 0.07631991803646088\n",
      "Epoch 13159/30000 Training Loss: 0.07063540816307068\n",
      "Epoch 13160/30000 Training Loss: 0.0805763527750969\n",
      "Epoch 13160/30000 Validation Loss: 0.07001350075006485\n",
      "Epoch 13161/30000 Training Loss: 0.09013710170984268\n",
      "Epoch 13162/30000 Training Loss: 0.08287813514471054\n",
      "Epoch 13163/30000 Training Loss: 0.07949609309434891\n",
      "Epoch 13164/30000 Training Loss: 0.07901164889335632\n",
      "Epoch 13165/30000 Training Loss: 0.06824276596307755\n",
      "Epoch 13166/30000 Training Loss: 0.08103473484516144\n",
      "Epoch 13167/30000 Training Loss: 0.08922946453094482\n",
      "Epoch 13168/30000 Training Loss: 0.06611412018537521\n",
      "Epoch 13169/30000 Training Loss: 0.074444480240345\n",
      "Epoch 13170/30000 Training Loss: 0.09062066674232483\n",
      "Epoch 13170/30000 Validation Loss: 0.08526357263326645\n",
      "Epoch 13171/30000 Training Loss: 0.06856802850961685\n",
      "Epoch 13172/30000 Training Loss: 0.09233320504426956\n",
      "Epoch 13173/30000 Training Loss: 0.10618185251951218\n",
      "Epoch 13174/30000 Training Loss: 0.07957416772842407\n",
      "Epoch 13175/30000 Training Loss: 0.07682731002569199\n",
      "Epoch 13176/30000 Training Loss: 0.06677521020174026\n",
      "Epoch 13177/30000 Training Loss: 0.08624642342329025\n",
      "Epoch 13178/30000 Training Loss: 0.06805434823036194\n",
      "Epoch 13179/30000 Training Loss: 0.08016929030418396\n",
      "Epoch 13180/30000 Training Loss: 0.08415663242340088\n",
      "Epoch 13180/30000 Validation Loss: 0.07897696644067764\n",
      "Epoch 13181/30000 Training Loss: 0.08861591666936874\n",
      "Epoch 13182/30000 Training Loss: 0.08656378835439682\n",
      "Epoch 13183/30000 Training Loss: 0.06586623936891556\n",
      "Epoch 13184/30000 Training Loss: 0.07501354813575745\n",
      "Epoch 13185/30000 Training Loss: 0.08083362132310867\n",
      "Epoch 13186/30000 Training Loss: 0.07025469094514847\n",
      "Epoch 13187/30000 Training Loss: 0.08528675884008408\n",
      "Epoch 13188/30000 Training Loss: 0.08354610949754715\n",
      "Epoch 13189/30000 Training Loss: 0.07353822141885757\n",
      "Epoch 13190/30000 Training Loss: 0.09054026752710342\n",
      "Epoch 13190/30000 Validation Loss: 0.07609674334526062\n",
      "Epoch 13191/30000 Training Loss: 0.08075965940952301\n",
      "Epoch 13192/30000 Training Loss: 0.0626564621925354\n",
      "Epoch 13193/30000 Training Loss: 0.090904600918293\n",
      "Epoch 13194/30000 Training Loss: 0.0720716193318367\n",
      "Epoch 13195/30000 Training Loss: 0.09478434175252914\n",
      "Epoch 13196/30000 Training Loss: 0.07392094284296036\n",
      "Epoch 13197/30000 Training Loss: 0.07099262624979019\n",
      "Epoch 13198/30000 Training Loss: 0.06939034163951874\n",
      "Epoch 13199/30000 Training Loss: 0.06307246536016464\n",
      "Epoch 13200/30000 Training Loss: 0.0750032365322113\n",
      "Epoch 13200/30000 Validation Loss: 0.08836361020803452\n",
      "Epoch 13201/30000 Training Loss: 0.06725607067346573\n",
      "Epoch 13202/30000 Training Loss: 0.06749393790960312\n",
      "Epoch 13203/30000 Training Loss: 0.0857633575797081\n",
      "Epoch 13204/30000 Training Loss: 0.07044803351163864\n",
      "Epoch 13205/30000 Training Loss: 0.08768362551927567\n",
      "Epoch 13206/30000 Training Loss: 0.0909452810883522\n",
      "Epoch 13207/30000 Training Loss: 0.0725812315940857\n",
      "Epoch 13208/30000 Training Loss: 0.07066205143928528\n",
      "Epoch 13209/30000 Training Loss: 0.06667355448007584\n",
      "Epoch 13210/30000 Training Loss: 0.07609975337982178\n",
      "Epoch 13210/30000 Validation Loss: 0.08442331105470657\n",
      "Epoch 13211/30000 Training Loss: 0.08724749088287354\n",
      "Epoch 13212/30000 Training Loss: 0.09832636266946793\n",
      "Epoch 13213/30000 Training Loss: 0.0791541337966919\n",
      "Epoch 13214/30000 Training Loss: 0.05498237535357475\n",
      "Epoch 13215/30000 Training Loss: 0.08862947672605515\n",
      "Epoch 13216/30000 Training Loss: 0.08538085967302322\n",
      "Epoch 13217/30000 Training Loss: 0.07950830459594727\n",
      "Epoch 13218/30000 Training Loss: 0.06805843114852905\n",
      "Epoch 13219/30000 Training Loss: 0.08986411243677139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13220/30000 Training Loss: 0.0705590769648552\n",
      "Epoch 13220/30000 Validation Loss: 0.08333153277635574\n",
      "Epoch 13221/30000 Training Loss: 0.08471689373254776\n",
      "Epoch 13222/30000 Training Loss: 0.08307152986526489\n",
      "Epoch 13223/30000 Training Loss: 0.06419426202774048\n",
      "Epoch 13224/30000 Training Loss: 0.074522465467453\n",
      "Epoch 13225/30000 Training Loss: 0.06554429978132248\n",
      "Epoch 13226/30000 Training Loss: 0.06265249848365784\n",
      "Epoch 13227/30000 Training Loss: 0.07313606142997742\n",
      "Epoch 13228/30000 Training Loss: 0.06503928452730179\n",
      "Epoch 13229/30000 Training Loss: 0.08177098631858826\n",
      "Epoch 13230/30000 Training Loss: 0.08465003222227097\n",
      "Epoch 13230/30000 Validation Loss: 0.09300708025693893\n",
      "Epoch 13231/30000 Training Loss: 0.07072397321462631\n",
      "Epoch 13232/30000 Training Loss: 0.07745533436536789\n",
      "Epoch 13233/30000 Training Loss: 0.08229891210794449\n",
      "Epoch 13234/30000 Training Loss: 0.06158728525042534\n",
      "Epoch 13235/30000 Training Loss: 0.07768583297729492\n",
      "Epoch 13236/30000 Training Loss: 0.07832219451665878\n",
      "Epoch 13237/30000 Training Loss: 0.08145982027053833\n",
      "Epoch 13238/30000 Training Loss: 0.06978977471590042\n",
      "Epoch 13239/30000 Training Loss: 0.09463837742805481\n",
      "Epoch 13240/30000 Training Loss: 0.06603708118200302\n",
      "Epoch 13240/30000 Validation Loss: 0.07055263966321945\n",
      "Epoch 13241/30000 Training Loss: 0.07407369464635849\n",
      "Epoch 13242/30000 Training Loss: 0.08838063478469849\n",
      "Epoch 13243/30000 Training Loss: 0.06573033332824707\n",
      "Epoch 13244/30000 Training Loss: 0.07084134221076965\n",
      "Epoch 13245/30000 Training Loss: 0.09506019204854965\n",
      "Epoch 13246/30000 Training Loss: 0.09037717431783676\n",
      "Epoch 13247/30000 Training Loss: 0.07372912019491196\n",
      "Epoch 13248/30000 Training Loss: 0.08199933171272278\n",
      "Epoch 13249/30000 Training Loss: 0.07220106571912766\n",
      "Epoch 13250/30000 Training Loss: 0.06243148446083069\n",
      "Epoch 13250/30000 Validation Loss: 0.08946140855550766\n",
      "Epoch 13251/30000 Training Loss: 0.07323259115219116\n",
      "Epoch 13252/30000 Training Loss: 0.08676911145448685\n",
      "Epoch 13253/30000 Training Loss: 0.08762577176094055\n",
      "Epoch 13254/30000 Training Loss: 0.07114339619874954\n",
      "Epoch 13255/30000 Training Loss: 0.06959536671638489\n",
      "Epoch 13256/30000 Training Loss: 0.07101815193891525\n",
      "Epoch 13257/30000 Training Loss: 0.08700892329216003\n",
      "Epoch 13258/30000 Training Loss: 0.08039174228906631\n",
      "Epoch 13259/30000 Training Loss: 0.08995729684829712\n",
      "Epoch 13260/30000 Training Loss: 0.08293832093477249\n",
      "Epoch 13260/30000 Validation Loss: 0.06515418738126755\n",
      "Epoch 13261/30000 Training Loss: 0.08370089530944824\n",
      "Epoch 13262/30000 Training Loss: 0.08878370374441147\n",
      "Epoch 13263/30000 Training Loss: 0.07303229719400406\n",
      "Epoch 13264/30000 Training Loss: 0.06602656096220016\n",
      "Epoch 13265/30000 Training Loss: 0.10342039912939072\n",
      "Epoch 13266/30000 Training Loss: 0.07322776317596436\n",
      "Epoch 13267/30000 Training Loss: 0.07108931988477707\n",
      "Epoch 13268/30000 Training Loss: 0.08090880513191223\n",
      "Epoch 13269/30000 Training Loss: 0.08374547958374023\n",
      "Epoch 13270/30000 Training Loss: 0.06705714017152786\n",
      "Epoch 13270/30000 Validation Loss: 0.07046150416135788\n",
      "Epoch 13271/30000 Training Loss: 0.08176406472921371\n",
      "Epoch 13272/30000 Training Loss: 0.0850282683968544\n",
      "Epoch 13273/30000 Training Loss: 0.07322646677494049\n",
      "Epoch 13274/30000 Training Loss: 0.09437429159879684\n",
      "Epoch 13275/30000 Training Loss: 0.07127612829208374\n",
      "Epoch 13276/30000 Training Loss: 0.09131716936826706\n",
      "Epoch 13277/30000 Training Loss: 0.08299209922552109\n",
      "Epoch 13278/30000 Training Loss: 0.073686383664608\n",
      "Epoch 13279/30000 Training Loss: 0.05776682123541832\n",
      "Epoch 13280/30000 Training Loss: 0.07576379925012589\n",
      "Epoch 13280/30000 Validation Loss: 0.08911343663930893\n",
      "Epoch 13281/30000 Training Loss: 0.08149360120296478\n",
      "Epoch 13282/30000 Training Loss: 0.06557173281908035\n",
      "Epoch 13283/30000 Training Loss: 0.07662913203239441\n",
      "Epoch 13284/30000 Training Loss: 0.07988984882831573\n",
      "Epoch 13285/30000 Training Loss: 0.06899724155664444\n",
      "Epoch 13286/30000 Training Loss: 0.0802711471915245\n",
      "Epoch 13287/30000 Training Loss: 0.07650640606880188\n",
      "Epoch 13288/30000 Training Loss: 0.07958879321813583\n",
      "Epoch 13289/30000 Training Loss: 0.08455995470285416\n",
      "Epoch 13290/30000 Training Loss: 0.08612003177404404\n",
      "Epoch 13290/30000 Validation Loss: 0.05930531024932861\n",
      "Epoch 13291/30000 Training Loss: 0.07255468517541885\n",
      "Epoch 13292/30000 Training Loss: 0.07648054510354996\n",
      "Epoch 13293/30000 Training Loss: 0.09695591777563095\n",
      "Epoch 13294/30000 Training Loss: 0.07731246948242188\n",
      "Epoch 13295/30000 Training Loss: 0.06480762362480164\n",
      "Epoch 13296/30000 Training Loss: 0.07008100301027298\n",
      "Epoch 13297/30000 Training Loss: 0.07625999301671982\n",
      "Epoch 13298/30000 Training Loss: 0.06912361830472946\n",
      "Epoch 13299/30000 Training Loss: 0.06990750879049301\n",
      "Epoch 13300/30000 Training Loss: 0.08830627053976059\n",
      "Epoch 13300/30000 Validation Loss: 0.08052036166191101\n",
      "Epoch 13301/30000 Training Loss: 0.08262749761343002\n",
      "Epoch 13302/30000 Training Loss: 0.08014870434999466\n",
      "Epoch 13303/30000 Training Loss: 0.07064352184534073\n",
      "Epoch 13304/30000 Training Loss: 0.06915590167045593\n",
      "Epoch 13305/30000 Training Loss: 0.06272847205400467\n",
      "Epoch 13306/30000 Training Loss: 0.08605915307998657\n",
      "Epoch 13307/30000 Training Loss: 0.07641279697418213\n",
      "Epoch 13308/30000 Training Loss: 0.07838916033506393\n",
      "Epoch 13309/30000 Training Loss: 0.0809645876288414\n",
      "Epoch 13310/30000 Training Loss: 0.07386722415685654\n",
      "Epoch 13310/30000 Validation Loss: 0.0719379112124443\n",
      "Epoch 13311/30000 Training Loss: 0.07410239428281784\n",
      "Epoch 13312/30000 Training Loss: 0.08815493434667587\n",
      "Epoch 13313/30000 Training Loss: 0.08040490746498108\n",
      "Epoch 13314/30000 Training Loss: 0.07292353361845016\n",
      "Epoch 13315/30000 Training Loss: 0.1005786582827568\n",
      "Epoch 13316/30000 Training Loss: 0.08064548671245575\n",
      "Epoch 13317/30000 Training Loss: 0.08345180749893188\n",
      "Epoch 13318/30000 Training Loss: 0.08530724048614502\n",
      "Epoch 13319/30000 Training Loss: 0.0816834345459938\n",
      "Epoch 13320/30000 Training Loss: 0.06152507662773132\n",
      "Epoch 13320/30000 Validation Loss: 0.06427010893821716\n",
      "Epoch 13321/30000 Training Loss: 0.07683102786540985\n",
      "Epoch 13322/30000 Training Loss: 0.07126005738973618\n",
      "Epoch 13323/30000 Training Loss: 0.07724913209676743\n",
      "Epoch 13324/30000 Training Loss: 0.07521065324544907\n",
      "Epoch 13325/30000 Training Loss: 0.07862190157175064\n",
      "Epoch 13326/30000 Training Loss: 0.07580386847257614\n",
      "Epoch 13327/30000 Training Loss: 0.06680411845445633\n",
      "Epoch 13328/30000 Training Loss: 0.08329698443412781\n",
      "Epoch 13329/30000 Training Loss: 0.06798955798149109\n",
      "Epoch 13330/30000 Training Loss: 0.0709179937839508\n",
      "Epoch 13330/30000 Validation Loss: 0.08032204955816269\n",
      "Epoch 13331/30000 Training Loss: 0.08346837013959885\n",
      "Epoch 13332/30000 Training Loss: 0.08466681838035583\n",
      "Epoch 13333/30000 Training Loss: 0.07304573059082031\n",
      "Epoch 13334/30000 Training Loss: 0.10070466250181198\n",
      "Epoch 13335/30000 Training Loss: 0.06174277141690254\n",
      "Epoch 13336/30000 Training Loss: 0.08708720654249191\n",
      "Epoch 13337/30000 Training Loss: 0.0933486744761467\n",
      "Epoch 13338/30000 Training Loss: 0.0828162133693695\n",
      "Epoch 13339/30000 Training Loss: 0.08458433300256729\n",
      "Epoch 13340/30000 Training Loss: 0.06531072407960892\n",
      "Epoch 13340/30000 Validation Loss: 0.08589214086532593\n",
      "Epoch 13341/30000 Training Loss: 0.09232831001281738\n",
      "Epoch 13342/30000 Training Loss: 0.08076313883066177\n",
      "Epoch 13343/30000 Training Loss: 0.08925700187683105\n",
      "Epoch 13344/30000 Training Loss: 0.07454230636358261\n",
      "Epoch 13345/30000 Training Loss: 0.07301152497529984\n",
      "Epoch 13346/30000 Training Loss: 0.07560548931360245\n",
      "Epoch 13347/30000 Training Loss: 0.07464801520109177\n",
      "Epoch 13348/30000 Training Loss: 0.09446888417005539\n",
      "Epoch 13349/30000 Training Loss: 0.0780288502573967\n",
      "Epoch 13350/30000 Training Loss: 0.07325581461191177\n",
      "Epoch 13350/30000 Validation Loss: 0.08006874471902847\n",
      "Epoch 13351/30000 Training Loss: 0.09558316320180893\n",
      "Epoch 13352/30000 Training Loss: 0.0832989439368248\n",
      "Epoch 13353/30000 Training Loss: 0.09398163110017776\n",
      "Epoch 13354/30000 Training Loss: 0.06464269012212753\n",
      "Epoch 13355/30000 Training Loss: 0.06622213870286942\n",
      "Epoch 13356/30000 Training Loss: 0.07223602384328842\n",
      "Epoch 13357/30000 Training Loss: 0.07406962662935257\n",
      "Epoch 13358/30000 Training Loss: 0.06366810202598572\n",
      "Epoch 13359/30000 Training Loss: 0.07860241085290909\n",
      "Epoch 13360/30000 Training Loss: 0.07522246241569519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13360/30000 Validation Loss: 0.05080244317650795\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05080244317650795<=============\n",
      "Epoch 13361/30000 Training Loss: 0.06513982266187668\n",
      "Epoch 13362/30000 Training Loss: 0.07135235518217087\n",
      "Epoch 13363/30000 Training Loss: 0.07745947688817978\n",
      "Epoch 13364/30000 Training Loss: 0.08162856101989746\n",
      "Epoch 13365/30000 Training Loss: 0.08578315377235413\n",
      "Epoch 13366/30000 Training Loss: 0.07644418627023697\n",
      "Epoch 13367/30000 Training Loss: 0.08319706469774246\n",
      "Epoch 13368/30000 Training Loss: 0.07230425626039505\n",
      "Epoch 13369/30000 Training Loss: 0.07319153100252151\n",
      "Epoch 13370/30000 Training Loss: 0.08488867431879044\n",
      "Epoch 13370/30000 Validation Loss: 0.08927223831415176\n",
      "Epoch 13371/30000 Training Loss: 0.06382917612791061\n",
      "Epoch 13372/30000 Training Loss: 0.0749397799372673\n",
      "Epoch 13373/30000 Training Loss: 0.08041732013225555\n",
      "Epoch 13374/30000 Training Loss: 0.0790005698800087\n",
      "Epoch 13375/30000 Training Loss: 0.08596382290124893\n",
      "Epoch 13376/30000 Training Loss: 0.06564810127019882\n",
      "Epoch 13377/30000 Training Loss: 0.1273779720067978\n",
      "Epoch 13378/30000 Training Loss: 0.07888742536306381\n",
      "Epoch 13379/30000 Training Loss: 0.06997541338205338\n",
      "Epoch 13380/30000 Training Loss: 0.08213556557893753\n",
      "Epoch 13380/30000 Validation Loss: 0.06654663383960724\n",
      "Epoch 13381/30000 Training Loss: 0.05813873931765556\n",
      "Epoch 13382/30000 Training Loss: 0.09337109327316284\n",
      "Epoch 13383/30000 Training Loss: 0.05684580281376839\n",
      "Epoch 13384/30000 Training Loss: 0.06988533586263657\n",
      "Epoch 13385/30000 Training Loss: 0.09437642246484756\n",
      "Epoch 13386/30000 Training Loss: 0.08934750407934189\n",
      "Epoch 13387/30000 Training Loss: 0.07603375613689423\n",
      "Epoch 13388/30000 Training Loss: 0.07398269325494766\n",
      "Epoch 13389/30000 Training Loss: 0.08629950135946274\n",
      "Epoch 13390/30000 Training Loss: 0.07907962799072266\n",
      "Epoch 13390/30000 Validation Loss: 0.08187393099069595\n",
      "Epoch 13391/30000 Training Loss: 0.06769419461488724\n",
      "Epoch 13392/30000 Training Loss: 0.07273521274328232\n",
      "Epoch 13393/30000 Training Loss: 0.07130850851535797\n",
      "Epoch 13394/30000 Training Loss: 0.07912827283143997\n",
      "Epoch 13395/30000 Training Loss: 0.07295466959476471\n",
      "Epoch 13396/30000 Training Loss: 0.06581420451402664\n",
      "Epoch 13397/30000 Training Loss: 0.07911818474531174\n",
      "Epoch 13398/30000 Training Loss: 0.07874632626771927\n",
      "Epoch 13399/30000 Training Loss: 0.08639997988939285\n",
      "Epoch 13400/30000 Training Loss: 0.08614262193441391\n",
      "Epoch 13400/30000 Validation Loss: 0.07546770572662354\n",
      "Epoch 13401/30000 Training Loss: 0.07439931482076645\n",
      "Epoch 13402/30000 Training Loss: 0.060497671365737915\n",
      "Epoch 13403/30000 Training Loss: 0.059862058609724045\n",
      "Epoch 13404/30000 Training Loss: 0.09489770978689194\n",
      "Epoch 13405/30000 Training Loss: 0.06611751019954681\n",
      "Epoch 13406/30000 Training Loss: 0.06717833131551743\n",
      "Epoch 13407/30000 Training Loss: 0.10960441827774048\n",
      "Epoch 13408/30000 Training Loss: 0.059292446821928024\n",
      "Epoch 13409/30000 Training Loss: 0.08701636642217636\n",
      "Epoch 13410/30000 Training Loss: 0.07699339836835861\n",
      "Epoch 13410/30000 Validation Loss: 0.06430944055318832\n",
      "Epoch 13411/30000 Training Loss: 0.06517333537340164\n",
      "Epoch 13412/30000 Training Loss: 0.08197635412216187\n",
      "Epoch 13413/30000 Training Loss: 0.06972554326057434\n",
      "Epoch 13414/30000 Training Loss: 0.09837877750396729\n",
      "Epoch 13415/30000 Training Loss: 0.07003820687532425\n",
      "Epoch 13416/30000 Training Loss: 0.08878909796476364\n",
      "Epoch 13417/30000 Training Loss: 0.07428329437971115\n",
      "Epoch 13418/30000 Training Loss: 0.07843341678380966\n",
      "Epoch 13419/30000 Training Loss: 0.09234140068292618\n",
      "Epoch 13420/30000 Training Loss: 0.07955769449472427\n",
      "Epoch 13420/30000 Validation Loss: 0.06646149605512619\n",
      "Epoch 13421/30000 Training Loss: 0.07987058907747269\n",
      "Epoch 13422/30000 Training Loss: 0.07048442959785461\n",
      "Epoch 13423/30000 Training Loss: 0.10040352493524551\n",
      "Epoch 13424/30000 Training Loss: 0.0780140832066536\n",
      "Epoch 13425/30000 Training Loss: 0.06431608647108078\n",
      "Epoch 13426/30000 Training Loss: 0.07499674707651138\n",
      "Epoch 13427/30000 Training Loss: 0.056932609528303146\n",
      "Epoch 13428/30000 Training Loss: 0.09914081543684006\n",
      "Epoch 13429/30000 Training Loss: 0.06855493783950806\n",
      "Epoch 13430/30000 Training Loss: 0.07941277325153351\n",
      "Epoch 13430/30000 Validation Loss: 0.07622173428535461\n",
      "Epoch 13431/30000 Training Loss: 0.1270214468240738\n",
      "Epoch 13432/30000 Training Loss: 0.06658414751291275\n",
      "Epoch 13433/30000 Training Loss: 0.07271876186132431\n",
      "Epoch 13434/30000 Training Loss: 0.06505044549703598\n",
      "Epoch 13435/30000 Training Loss: 0.08417165279388428\n",
      "Epoch 13436/30000 Training Loss: 0.05725794658064842\n",
      "Epoch 13437/30000 Training Loss: 0.08020184189081192\n",
      "Epoch 13438/30000 Training Loss: 0.07376914471387863\n",
      "Epoch 13439/30000 Training Loss: 0.09084451198577881\n",
      "Epoch 13440/30000 Training Loss: 0.07548144459724426\n",
      "Epoch 13440/30000 Validation Loss: 0.065236896276474\n",
      "Epoch 13441/30000 Training Loss: 0.07315663993358612\n",
      "Epoch 13442/30000 Training Loss: 0.07933317124843597\n",
      "Epoch 13443/30000 Training Loss: 0.05866529047489166\n",
      "Epoch 13444/30000 Training Loss: 0.06827288120985031\n",
      "Epoch 13445/30000 Training Loss: 0.08636348694562912\n",
      "Epoch 13446/30000 Training Loss: 0.06936225295066833\n",
      "Epoch 13447/30000 Training Loss: 0.07117321342229843\n",
      "Epoch 13448/30000 Training Loss: 0.0723959431052208\n",
      "Epoch 13449/30000 Training Loss: 0.06642037630081177\n",
      "Epoch 13450/30000 Training Loss: 0.11457318067550659\n",
      "Epoch 13450/30000 Validation Loss: 0.057905059307813644\n",
      "Epoch 13451/30000 Training Loss: 0.05942864343523979\n",
      "Epoch 13452/30000 Training Loss: 0.06876469403505325\n",
      "Epoch 13453/30000 Training Loss: 0.08270088583230972\n",
      "Epoch 13454/30000 Training Loss: 0.0691518783569336\n",
      "Epoch 13455/30000 Training Loss: 0.07520879060029984\n",
      "Epoch 13456/30000 Training Loss: 0.06814084202051163\n",
      "Epoch 13457/30000 Training Loss: 0.06710971146821976\n",
      "Epoch 13458/30000 Training Loss: 0.08346056938171387\n",
      "Epoch 13459/30000 Training Loss: 0.06408466398715973\n",
      "Epoch 13460/30000 Training Loss: 0.07162044197320938\n",
      "Epoch 13460/30000 Validation Loss: 0.06951070576906204\n",
      "Epoch 13461/30000 Training Loss: 0.07475052028894424\n",
      "Epoch 13462/30000 Training Loss: 0.06506922096014023\n",
      "Epoch 13463/30000 Training Loss: 0.09563989192247391\n",
      "Epoch 13464/30000 Training Loss: 0.06690553575754166\n",
      "Epoch 13465/30000 Training Loss: 0.06461712718009949\n",
      "Epoch 13466/30000 Training Loss: 0.07210420817136765\n",
      "Epoch 13467/30000 Training Loss: 0.0833277627825737\n",
      "Epoch 13468/30000 Training Loss: 0.08357558399438858\n",
      "Epoch 13469/30000 Training Loss: 0.0826612189412117\n",
      "Epoch 13470/30000 Training Loss: 0.07770631462335587\n",
      "Epoch 13470/30000 Validation Loss: 0.08048694580793381\n",
      "Epoch 13471/30000 Training Loss: 0.06433523446321487\n",
      "Epoch 13472/30000 Training Loss: 0.0749438926577568\n",
      "Epoch 13473/30000 Training Loss: 0.09001515060663223\n",
      "Epoch 13474/30000 Training Loss: 0.07827088981866837\n",
      "Epoch 13475/30000 Training Loss: 0.07329204678535461\n",
      "Epoch 13476/30000 Training Loss: 0.07987192273139954\n",
      "Epoch 13477/30000 Training Loss: 0.06425591558218002\n",
      "Epoch 13478/30000 Training Loss: 0.08164268732070923\n",
      "Epoch 13479/30000 Training Loss: 0.08215460926294327\n",
      "Epoch 13480/30000 Training Loss: 0.07564996927976608\n",
      "Epoch 13480/30000 Validation Loss: 0.0704386904835701\n",
      "Epoch 13481/30000 Training Loss: 0.088506780564785\n",
      "Epoch 13482/30000 Training Loss: 0.06912550330162048\n",
      "Epoch 13483/30000 Training Loss: 0.0697314664721489\n",
      "Epoch 13484/30000 Training Loss: 0.07961996644735336\n",
      "Epoch 13485/30000 Training Loss: 0.06655571609735489\n",
      "Epoch 13486/30000 Training Loss: 0.09784845262765884\n",
      "Epoch 13487/30000 Training Loss: 0.07436759024858475\n",
      "Epoch 13488/30000 Training Loss: 0.07582971453666687\n",
      "Epoch 13489/30000 Training Loss: 0.08741445094347\n",
      "Epoch 13490/30000 Training Loss: 0.06610723584890366\n",
      "Epoch 13490/30000 Validation Loss: 0.07986468076705933\n",
      "Epoch 13491/30000 Training Loss: 0.07712483406066895\n",
      "Epoch 13492/30000 Training Loss: 0.09270340949296951\n",
      "Epoch 13493/30000 Training Loss: 0.07034280896186829\n",
      "Epoch 13494/30000 Training Loss: 0.07063969224691391\n",
      "Epoch 13495/30000 Training Loss: 0.0767095685005188\n",
      "Epoch 13496/30000 Training Loss: 0.09312043339014053\n",
      "Epoch 13497/30000 Training Loss: 0.06414392590522766\n",
      "Epoch 13498/30000 Training Loss: 0.08786546438932419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13499/30000 Training Loss: 0.06728293746709824\n",
      "Epoch 13500/30000 Training Loss: 0.07075855135917664\n",
      "Epoch 13500/30000 Validation Loss: 0.07035097479820251\n",
      "Epoch 13501/30000 Training Loss: 0.08472933620214462\n",
      "Epoch 13502/30000 Training Loss: 0.04888947308063507\n",
      "Epoch 13503/30000 Training Loss: 0.06749538332223892\n",
      "Epoch 13504/30000 Training Loss: 0.06614869832992554\n",
      "Epoch 13505/30000 Training Loss: 0.07515407353639603\n",
      "Epoch 13506/30000 Training Loss: 0.07352948933839798\n",
      "Epoch 13507/30000 Training Loss: 0.08474788814783096\n",
      "Epoch 13508/30000 Training Loss: 0.07193674892187119\n",
      "Epoch 13509/30000 Training Loss: 0.08405455201864243\n",
      "Epoch 13510/30000 Training Loss: 0.1220947727560997\n",
      "Epoch 13510/30000 Validation Loss: 0.08120133727788925\n",
      "Epoch 13511/30000 Training Loss: 0.06567167490720749\n",
      "Epoch 13512/30000 Training Loss: 0.06721914559602737\n",
      "Epoch 13513/30000 Training Loss: 0.08226856589317322\n",
      "Epoch 13514/30000 Training Loss: 0.07337022572755814\n",
      "Epoch 13515/30000 Training Loss: 0.08141694217920303\n",
      "Epoch 13516/30000 Training Loss: 0.09604495763778687\n",
      "Epoch 13517/30000 Training Loss: 0.07375022768974304\n",
      "Epoch 13518/30000 Training Loss: 0.07753557711839676\n",
      "Epoch 13519/30000 Training Loss: 0.08199895173311234\n",
      "Epoch 13520/30000 Training Loss: 0.06531157344579697\n",
      "Epoch 13520/30000 Validation Loss: 0.08297153562307358\n",
      "Epoch 13521/30000 Training Loss: 0.09881890565156937\n",
      "Epoch 13522/30000 Training Loss: 0.06971308588981628\n",
      "Epoch 13523/30000 Training Loss: 0.07817066460847855\n",
      "Epoch 13524/30000 Training Loss: 0.06831586360931396\n",
      "Epoch 13525/30000 Training Loss: 0.0923546552658081\n",
      "Epoch 13526/30000 Training Loss: 0.06382251530885696\n",
      "Epoch 13527/30000 Training Loss: 0.062672920525074\n",
      "Epoch 13528/30000 Training Loss: 0.05885535851120949\n",
      "Epoch 13529/30000 Training Loss: 0.06914696842432022\n",
      "Epoch 13530/30000 Training Loss: 0.09146329015493393\n",
      "Epoch 13530/30000 Validation Loss: 0.07354544848203659\n",
      "Epoch 13531/30000 Training Loss: 0.07947967201471329\n",
      "Epoch 13532/30000 Training Loss: 0.05720467492938042\n",
      "Epoch 13533/30000 Training Loss: 0.06571393460035324\n",
      "Epoch 13534/30000 Training Loss: 0.09759771078824997\n",
      "Epoch 13535/30000 Training Loss: 0.07783441990613937\n",
      "Epoch 13536/30000 Training Loss: 0.08472494035959244\n",
      "Epoch 13537/30000 Training Loss: 0.06151789426803589\n",
      "Epoch 13538/30000 Training Loss: 0.07799306511878967\n",
      "Epoch 13539/30000 Training Loss: 0.10748512297868729\n",
      "Epoch 13540/30000 Training Loss: 0.05831896886229515\n",
      "Epoch 13540/30000 Validation Loss: 0.06902795284986496\n",
      "Epoch 13541/30000 Training Loss: 0.06951413303613663\n",
      "Epoch 13542/30000 Training Loss: 0.08539865165948868\n",
      "Epoch 13543/30000 Training Loss: 0.05614982172846794\n",
      "Epoch 13544/30000 Training Loss: 0.07674536854028702\n",
      "Epoch 13545/30000 Training Loss: 0.06311733275651932\n",
      "Epoch 13546/30000 Training Loss: 0.10033291578292847\n",
      "Epoch 13547/30000 Training Loss: 0.08518669754266739\n",
      "Epoch 13548/30000 Training Loss: 0.07641404122114182\n",
      "Epoch 13549/30000 Training Loss: 0.06341181695461273\n",
      "Epoch 13550/30000 Training Loss: 0.08798260241746902\n",
      "Epoch 13550/30000 Validation Loss: 0.07224860042333603\n",
      "Epoch 13551/30000 Training Loss: 0.07652808725833893\n",
      "Epoch 13552/30000 Training Loss: 0.07126794010400772\n",
      "Epoch 13553/30000 Training Loss: 0.07388701289892197\n",
      "Epoch 13554/30000 Training Loss: 0.1057489737868309\n",
      "Epoch 13555/30000 Training Loss: 0.06644928455352783\n",
      "Epoch 13556/30000 Training Loss: 0.0702379122376442\n",
      "Epoch 13557/30000 Training Loss: 0.05801747366786003\n",
      "Epoch 13558/30000 Training Loss: 0.07602290064096451\n",
      "Epoch 13559/30000 Training Loss: 0.0768851712346077\n",
      "Epoch 13560/30000 Training Loss: 0.07160492241382599\n",
      "Epoch 13560/30000 Validation Loss: 0.09132342785596848\n",
      "Epoch 13561/30000 Training Loss: 0.07433078438043594\n",
      "Epoch 13562/30000 Training Loss: 0.0934884324669838\n",
      "Epoch 13563/30000 Training Loss: 0.0806896984577179\n",
      "Epoch 13564/30000 Training Loss: 0.08629769086837769\n",
      "Epoch 13565/30000 Training Loss: 0.08239493519067764\n",
      "Epoch 13566/30000 Training Loss: 0.0774141177535057\n",
      "Epoch 13567/30000 Training Loss: 0.07076936960220337\n",
      "Epoch 13568/30000 Training Loss: 0.07920680940151215\n",
      "Epoch 13569/30000 Training Loss: 0.06824901700019836\n",
      "Epoch 13570/30000 Training Loss: 0.07623792439699173\n",
      "Epoch 13570/30000 Validation Loss: 0.06816122680902481\n",
      "Epoch 13571/30000 Training Loss: 0.0840873122215271\n",
      "Epoch 13572/30000 Training Loss: 0.08496842533349991\n",
      "Epoch 13573/30000 Training Loss: 0.0802154466509819\n",
      "Epoch 13574/30000 Training Loss: 0.093794085085392\n",
      "Epoch 13575/30000 Training Loss: 0.0780274048447609\n",
      "Epoch 13576/30000 Training Loss: 0.07299456745386124\n",
      "Epoch 13577/30000 Training Loss: 0.0627792552113533\n",
      "Epoch 13578/30000 Training Loss: 0.07086344063282013\n",
      "Epoch 13579/30000 Training Loss: 0.08223127573728561\n",
      "Epoch 13580/30000 Training Loss: 0.09150265902280807\n",
      "Epoch 13580/30000 Validation Loss: 0.1000322476029396\n",
      "Epoch 13581/30000 Training Loss: 0.06722163408994675\n",
      "Epoch 13582/30000 Training Loss: 0.067219078540802\n",
      "Epoch 13583/30000 Training Loss: 0.06756160408258438\n",
      "Epoch 13584/30000 Training Loss: 0.09444556385278702\n",
      "Epoch 13585/30000 Training Loss: 0.05877656862139702\n",
      "Epoch 13586/30000 Training Loss: 0.0823841467499733\n",
      "Epoch 13587/30000 Training Loss: 0.06284943968057632\n",
      "Epoch 13588/30000 Training Loss: 0.08454180508852005\n",
      "Epoch 13589/30000 Training Loss: 0.08599381893873215\n",
      "Epoch 13590/30000 Training Loss: 0.06561454385519028\n",
      "Epoch 13590/30000 Validation Loss: 0.07703167200088501\n",
      "Epoch 13591/30000 Training Loss: 0.07506000250577927\n",
      "Epoch 13592/30000 Training Loss: 0.06303465366363525\n",
      "Epoch 13593/30000 Training Loss: 0.07081999629735947\n",
      "Epoch 13594/30000 Training Loss: 0.096825510263443\n",
      "Epoch 13595/30000 Training Loss: 0.08165810257196426\n",
      "Epoch 13596/30000 Training Loss: 0.06282258778810501\n",
      "Epoch 13597/30000 Training Loss: 0.07240185886621475\n",
      "Epoch 13598/30000 Training Loss: 0.07607615739107132\n",
      "Epoch 13599/30000 Training Loss: 0.07042292505502701\n",
      "Epoch 13600/30000 Training Loss: 0.08921394497156143\n",
      "Epoch 13600/30000 Validation Loss: 0.06173807010054588\n",
      "Epoch 13601/30000 Training Loss: 0.06523344665765762\n",
      "Epoch 13602/30000 Training Loss: 0.0737079456448555\n",
      "Epoch 13603/30000 Training Loss: 0.08331438153982162\n",
      "Epoch 13604/30000 Training Loss: 0.06814929097890854\n",
      "Epoch 13605/30000 Training Loss: 0.06626784056425095\n",
      "Epoch 13606/30000 Training Loss: 0.07295489311218262\n",
      "Epoch 13607/30000 Training Loss: 0.08252380043268204\n",
      "Epoch 13608/30000 Training Loss: 0.07082866877317429\n",
      "Epoch 13609/30000 Training Loss: 0.06868264824151993\n",
      "Epoch 13610/30000 Training Loss: 0.06867607682943344\n",
      "Epoch 13610/30000 Validation Loss: 0.06849841773509979\n",
      "Epoch 13611/30000 Training Loss: 0.08481118083000183\n",
      "Epoch 13612/30000 Training Loss: 0.08717260509729385\n",
      "Epoch 13613/30000 Training Loss: 0.0682765543460846\n",
      "Epoch 13614/30000 Training Loss: 0.07947686314582825\n",
      "Epoch 13615/30000 Training Loss: 0.07094215601682663\n",
      "Epoch 13616/30000 Training Loss: 0.06741205602884293\n",
      "Epoch 13617/30000 Training Loss: 0.0749545618891716\n",
      "Epoch 13618/30000 Training Loss: 0.07204223424196243\n",
      "Epoch 13619/30000 Training Loss: 0.05979037284851074\n",
      "Epoch 13620/30000 Training Loss: 0.06483778357505798\n",
      "Epoch 13620/30000 Validation Loss: 0.07439608126878738\n",
      "Epoch 13621/30000 Training Loss: 0.07590141147375107\n",
      "Epoch 13622/30000 Training Loss: 0.06323599070310593\n",
      "Epoch 13623/30000 Training Loss: 0.06750442832708359\n",
      "Epoch 13624/30000 Training Loss: 0.08178193867206573\n",
      "Epoch 13625/30000 Training Loss: 0.09867378324270248\n",
      "Epoch 13626/30000 Training Loss: 0.0772901177406311\n",
      "Epoch 13627/30000 Training Loss: 0.057907890528440475\n",
      "Epoch 13628/30000 Training Loss: 0.0869840756058693\n",
      "Epoch 13629/30000 Training Loss: 0.06823659688234329\n",
      "Epoch 13630/30000 Training Loss: 0.09572725743055344\n",
      "Epoch 13630/30000 Validation Loss: 0.0705086961388588\n",
      "Epoch 13631/30000 Training Loss: 0.0587439239025116\n",
      "Epoch 13632/30000 Training Loss: 0.07834693044424057\n",
      "Epoch 13633/30000 Training Loss: 0.07405149191617966\n",
      "Epoch 13634/30000 Training Loss: 0.0862153172492981\n",
      "Epoch 13635/30000 Training Loss: 0.09505648165941238\n",
      "Epoch 13636/30000 Training Loss: 0.1046801507472992\n",
      "Epoch 13637/30000 Training Loss: 0.08122030645608902\n",
      "Epoch 13638/30000 Training Loss: 0.08058128505945206\n",
      "Epoch 13639/30000 Training Loss: 0.07392066717147827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13640/30000 Training Loss: 0.08752971142530441\n",
      "Epoch 13640/30000 Validation Loss: 0.0927828773856163\n",
      "Epoch 13641/30000 Training Loss: 0.08359486609697342\n",
      "Epoch 13642/30000 Training Loss: 0.0626247450709343\n",
      "Epoch 13643/30000 Training Loss: 0.07886248081922531\n",
      "Epoch 13644/30000 Training Loss: 0.09529641270637512\n",
      "Epoch 13645/30000 Training Loss: 0.05954720452427864\n",
      "Epoch 13646/30000 Training Loss: 0.0733974352478981\n",
      "Epoch 13647/30000 Training Loss: 0.07994554936885834\n",
      "Epoch 13648/30000 Training Loss: 0.09396663308143616\n",
      "Epoch 13649/30000 Training Loss: 0.07784023880958557\n",
      "Epoch 13650/30000 Training Loss: 0.07174521684646606\n",
      "Epoch 13650/30000 Validation Loss: 0.08184468746185303\n",
      "Epoch 13651/30000 Training Loss: 0.06294899433851242\n",
      "Epoch 13652/30000 Training Loss: 0.09881958365440369\n",
      "Epoch 13653/30000 Training Loss: 0.06995422393083572\n",
      "Epoch 13654/30000 Training Loss: 0.06143489480018616\n",
      "Epoch 13655/30000 Training Loss: 0.07416445016860962\n",
      "Epoch 13656/30000 Training Loss: 0.08467942476272583\n",
      "Epoch 13657/30000 Training Loss: 0.06632167845964432\n",
      "Epoch 13658/30000 Training Loss: 0.07898665964603424\n",
      "Epoch 13659/30000 Training Loss: 0.08368989080190659\n",
      "Epoch 13660/30000 Training Loss: 0.05976639315485954\n",
      "Epoch 13660/30000 Validation Loss: 0.08068843930959702\n",
      "Epoch 13661/30000 Training Loss: 0.056261803954839706\n",
      "Epoch 13662/30000 Training Loss: 0.06518610566854477\n",
      "Epoch 13663/30000 Training Loss: 0.09164493530988693\n",
      "Epoch 13664/30000 Training Loss: 0.07411659508943558\n",
      "Epoch 13665/30000 Training Loss: 0.08575037121772766\n",
      "Epoch 13666/30000 Training Loss: 0.07192204147577286\n",
      "Epoch 13667/30000 Training Loss: 0.06478042155504227\n",
      "Epoch 13668/30000 Training Loss: 0.06920202821493149\n",
      "Epoch 13669/30000 Training Loss: 0.07333987951278687\n",
      "Epoch 13670/30000 Training Loss: 0.06158095598220825\n",
      "Epoch 13670/30000 Validation Loss: 0.0704062208533287\n",
      "Epoch 13671/30000 Training Loss: 0.06735935062170029\n",
      "Epoch 13672/30000 Training Loss: 0.08943938463926315\n",
      "Epoch 13673/30000 Training Loss: 0.07743614166975021\n",
      "Epoch 13674/30000 Training Loss: 0.06828679889440536\n",
      "Epoch 13675/30000 Training Loss: 0.06577897071838379\n",
      "Epoch 13676/30000 Training Loss: 0.06525558978319168\n",
      "Epoch 13677/30000 Training Loss: 0.07920856028795242\n",
      "Epoch 13678/30000 Training Loss: 0.06555034220218658\n",
      "Epoch 13679/30000 Training Loss: 0.07334195822477341\n",
      "Epoch 13680/30000 Training Loss: 0.08935892581939697\n",
      "Epoch 13680/30000 Validation Loss: 0.08379363268613815\n",
      "Epoch 13681/30000 Training Loss: 0.06026644632220268\n",
      "Epoch 13682/30000 Training Loss: 0.06897474825382233\n",
      "Epoch 13683/30000 Training Loss: 0.0817248597741127\n",
      "Epoch 13684/30000 Training Loss: 0.06648611277341843\n",
      "Epoch 13685/30000 Training Loss: 0.07823751121759415\n",
      "Epoch 13686/30000 Training Loss: 0.08342093229293823\n",
      "Epoch 13687/30000 Training Loss: 0.11996663361787796\n",
      "Epoch 13688/30000 Training Loss: 0.07855954766273499\n",
      "Epoch 13689/30000 Training Loss: 0.06690390408039093\n",
      "Epoch 13690/30000 Training Loss: 0.06724811345338821\n",
      "Epoch 13690/30000 Validation Loss: 0.08340054005384445\n",
      "Epoch 13691/30000 Training Loss: 0.08834704011678696\n",
      "Epoch 13692/30000 Training Loss: 0.09629108756780624\n",
      "Epoch 13693/30000 Training Loss: 0.06530909240245819\n",
      "Epoch 13694/30000 Training Loss: 0.07607554644346237\n",
      "Epoch 13695/30000 Training Loss: 0.0643664225935936\n",
      "Epoch 13696/30000 Training Loss: 0.06056107580661774\n",
      "Epoch 13697/30000 Training Loss: 0.08608299493789673\n",
      "Epoch 13698/30000 Training Loss: 0.06885826587677002\n",
      "Epoch 13699/30000 Training Loss: 0.06911562383174896\n",
      "Epoch 13700/30000 Training Loss: 0.060378625988960266\n",
      "Epoch 13700/30000 Validation Loss: 0.08806919306516647\n",
      "Epoch 13701/30000 Training Loss: 0.06755256652832031\n",
      "Epoch 13702/30000 Training Loss: 0.07200860232114792\n",
      "Epoch 13703/30000 Training Loss: 0.07337731868028641\n",
      "Epoch 13704/30000 Training Loss: 0.08797305077314377\n",
      "Epoch 13705/30000 Training Loss: 0.0971522405743599\n",
      "Epoch 13706/30000 Training Loss: 0.0703754648566246\n",
      "Epoch 13707/30000 Training Loss: 0.07872681319713593\n",
      "Epoch 13708/30000 Training Loss: 0.07269806414842606\n",
      "Epoch 13709/30000 Training Loss: 0.06899437308311462\n",
      "Epoch 13710/30000 Training Loss: 0.06964246928691864\n",
      "Epoch 13710/30000 Validation Loss: 0.08550160378217697\n",
      "Epoch 13711/30000 Training Loss: 0.097541444003582\n",
      "Epoch 13712/30000 Training Loss: 0.07778504490852356\n",
      "Epoch 13713/30000 Training Loss: 0.0964336097240448\n",
      "Epoch 13714/30000 Training Loss: 0.08455338329076767\n",
      "Epoch 13715/30000 Training Loss: 0.06420018523931503\n",
      "Epoch 13716/30000 Training Loss: 0.05508003756403923\n",
      "Epoch 13717/30000 Training Loss: 0.07210151106119156\n",
      "Epoch 13718/30000 Training Loss: 0.09034756571054459\n",
      "Epoch 13719/30000 Training Loss: 0.07478708028793335\n",
      "Epoch 13720/30000 Training Loss: 0.08093679696321487\n",
      "Epoch 13720/30000 Validation Loss: 0.07709059864282608\n",
      "Epoch 13721/30000 Training Loss: 0.0721331387758255\n",
      "Epoch 13722/30000 Training Loss: 0.0953507050871849\n",
      "Epoch 13723/30000 Training Loss: 0.0752156674861908\n",
      "Epoch 13724/30000 Training Loss: 0.056920990347862244\n",
      "Epoch 13725/30000 Training Loss: 0.08480509370565414\n",
      "Epoch 13726/30000 Training Loss: 0.06669812649488449\n",
      "Epoch 13727/30000 Training Loss: 0.0703168734908104\n",
      "Epoch 13728/30000 Training Loss: 0.0702500119805336\n",
      "Epoch 13729/30000 Training Loss: 0.06310417503118515\n",
      "Epoch 13730/30000 Training Loss: 0.07310425490140915\n",
      "Epoch 13730/30000 Validation Loss: 0.07803293317556381\n",
      "Epoch 13731/30000 Training Loss: 0.0705566480755806\n",
      "Epoch 13732/30000 Training Loss: 0.08906891196966171\n",
      "Epoch 13733/30000 Training Loss: 0.07601389288902283\n",
      "Epoch 13734/30000 Training Loss: 0.0892038345336914\n",
      "Epoch 13735/30000 Training Loss: 0.06402327865362167\n",
      "Epoch 13736/30000 Training Loss: 0.07531864196062088\n",
      "Epoch 13737/30000 Training Loss: 0.06966327875852585\n",
      "Epoch 13738/30000 Training Loss: 0.06719744205474854\n",
      "Epoch 13739/30000 Training Loss: 0.08528582006692886\n",
      "Epoch 13740/30000 Training Loss: 0.08429455012083054\n",
      "Epoch 13740/30000 Validation Loss: 0.06895885616540909\n",
      "Epoch 13741/30000 Training Loss: 0.08207245916128159\n",
      "Epoch 13742/30000 Training Loss: 0.11021510511636734\n",
      "Epoch 13743/30000 Training Loss: 0.056288499385118484\n",
      "Epoch 13744/30000 Training Loss: 0.07298864424228668\n",
      "Epoch 13745/30000 Training Loss: 0.07822404056787491\n",
      "Epoch 13746/30000 Training Loss: 0.058660924434661865\n",
      "Epoch 13747/30000 Training Loss: 0.08520597219467163\n",
      "Epoch 13748/30000 Training Loss: 0.067288838326931\n",
      "Epoch 13749/30000 Training Loss: 0.08330690115690231\n",
      "Epoch 13750/30000 Training Loss: 0.07640310376882553\n",
      "Epoch 13750/30000 Validation Loss: 0.07193689793348312\n",
      "Epoch 13751/30000 Training Loss: 0.08617783337831497\n",
      "Epoch 13752/30000 Training Loss: 0.07804279774427414\n",
      "Epoch 13753/30000 Training Loss: 0.07153195887804031\n",
      "Epoch 13754/30000 Training Loss: 0.06231200695037842\n",
      "Epoch 13755/30000 Training Loss: 0.07802297919988632\n",
      "Epoch 13756/30000 Training Loss: 0.07174984365701675\n",
      "Epoch 13757/30000 Training Loss: 0.07103712856769562\n",
      "Epoch 13758/30000 Training Loss: 0.08243250846862793\n",
      "Epoch 13759/30000 Training Loss: 0.09494509547948837\n",
      "Epoch 13760/30000 Training Loss: 0.08430648595094681\n",
      "Epoch 13760/30000 Validation Loss: 0.09519926458597183\n",
      "Epoch 13761/30000 Training Loss: 0.07158126682043076\n",
      "Epoch 13762/30000 Training Loss: 0.09099027514457703\n",
      "Epoch 13763/30000 Training Loss: 0.07321429997682571\n",
      "Epoch 13764/30000 Training Loss: 0.08166265487670898\n",
      "Epoch 13765/30000 Training Loss: 0.07585640996694565\n",
      "Epoch 13766/30000 Training Loss: 0.06006233021616936\n",
      "Epoch 13767/30000 Training Loss: 0.08680099248886108\n",
      "Epoch 13768/30000 Training Loss: 0.08207551389932632\n",
      "Epoch 13769/30000 Training Loss: 0.0647771805524826\n",
      "Epoch 13770/30000 Training Loss: 0.07198769599199295\n",
      "Epoch 13770/30000 Validation Loss: 0.11143925786018372\n",
      "Epoch 13771/30000 Training Loss: 0.09843072295188904\n",
      "Epoch 13772/30000 Training Loss: 0.07009492069482803\n",
      "Epoch 13773/30000 Training Loss: 0.0765853077173233\n",
      "Epoch 13774/30000 Training Loss: 0.07325204461812973\n",
      "Epoch 13775/30000 Training Loss: 0.08403405547142029\n",
      "Epoch 13776/30000 Training Loss: 0.05627605319023132\n",
      "Epoch 13777/30000 Training Loss: 0.08977707475423813\n",
      "Epoch 13778/30000 Training Loss: 0.08064896613359451\n",
      "Epoch 13779/30000 Training Loss: 0.07763408869504929\n",
      "Epoch 13780/30000 Training Loss: 0.08685308694839478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13780/30000 Validation Loss: 0.07421495765447617\n",
      "Epoch 13781/30000 Training Loss: 0.08054298907518387\n",
      "Epoch 13782/30000 Training Loss: 0.09412189573049545\n",
      "Epoch 13783/30000 Training Loss: 0.06357086449861526\n",
      "Epoch 13784/30000 Training Loss: 0.06261210888624191\n",
      "Epoch 13785/30000 Training Loss: 0.0675940290093422\n",
      "Epoch 13786/30000 Training Loss: 0.06118747591972351\n",
      "Epoch 13787/30000 Training Loss: 0.09470111131668091\n",
      "Epoch 13788/30000 Training Loss: 0.10956243425607681\n",
      "Epoch 13789/30000 Training Loss: 0.06789222359657288\n",
      "Epoch 13790/30000 Training Loss: 0.08207264542579651\n",
      "Epoch 13790/30000 Validation Loss: 0.09568241238594055\n",
      "Epoch 13791/30000 Training Loss: 0.06605219095945358\n",
      "Epoch 13792/30000 Training Loss: 0.07836969941854477\n",
      "Epoch 13793/30000 Training Loss: 0.07129225134849548\n",
      "Epoch 13794/30000 Training Loss: 0.06148366257548332\n",
      "Epoch 13795/30000 Training Loss: 0.07433703541755676\n",
      "Epoch 13796/30000 Training Loss: 0.053475454449653625\n",
      "Epoch 13797/30000 Training Loss: 0.06627944111824036\n",
      "Epoch 13798/30000 Training Loss: 0.07413237541913986\n",
      "Epoch 13799/30000 Training Loss: 0.07454025000333786\n",
      "Epoch 13800/30000 Training Loss: 0.08207807689905167\n",
      "Epoch 13800/30000 Validation Loss: 0.0779154896736145\n",
      "Epoch 13801/30000 Training Loss: 0.07251247018575668\n",
      "Epoch 13802/30000 Training Loss: 0.07675838470458984\n",
      "Epoch 13803/30000 Training Loss: 0.07411937415599823\n",
      "Epoch 13804/30000 Training Loss: 0.06330004334449768\n",
      "Epoch 13805/30000 Training Loss: 0.07071503251791\n",
      "Epoch 13806/30000 Training Loss: 0.08476892858743668\n",
      "Epoch 13807/30000 Training Loss: 0.0738859623670578\n",
      "Epoch 13808/30000 Training Loss: 0.07219629734754562\n",
      "Epoch 13809/30000 Training Loss: 0.07337001711130142\n",
      "Epoch 13810/30000 Training Loss: 0.07896921783685684\n",
      "Epoch 13810/30000 Validation Loss: 0.07126317173242569\n",
      "Epoch 13811/30000 Training Loss: 0.08733755350112915\n",
      "Epoch 13812/30000 Training Loss: 0.07344455271959305\n",
      "Epoch 13813/30000 Training Loss: 0.081386998295784\n",
      "Epoch 13814/30000 Training Loss: 0.06934578716754913\n",
      "Epoch 13815/30000 Training Loss: 0.08870396763086319\n",
      "Epoch 13816/30000 Training Loss: 0.07126612216234207\n",
      "Epoch 13817/30000 Training Loss: 0.06349395960569382\n",
      "Epoch 13818/30000 Training Loss: 0.09022241830825806\n",
      "Epoch 13819/30000 Training Loss: 0.07255765050649643\n",
      "Epoch 13820/30000 Training Loss: 0.059038612991571426\n",
      "Epoch 13820/30000 Validation Loss: 0.07656482607126236\n",
      "Epoch 13821/30000 Training Loss: 0.07435842603445053\n",
      "Epoch 13822/30000 Training Loss: 0.06929784268140793\n",
      "Epoch 13823/30000 Training Loss: 0.08091329038143158\n",
      "Epoch 13824/30000 Training Loss: 0.07143666595220566\n",
      "Epoch 13825/30000 Training Loss: 0.07495760172605515\n",
      "Epoch 13826/30000 Training Loss: 0.09122560173273087\n",
      "Epoch 13827/30000 Training Loss: 0.08296629786491394\n",
      "Epoch 13828/30000 Training Loss: 0.06873498111963272\n",
      "Epoch 13829/30000 Training Loss: 0.05300527811050415\n",
      "Epoch 13830/30000 Training Loss: 0.07760762423276901\n",
      "Epoch 13830/30000 Validation Loss: 0.08987823873758316\n",
      "Epoch 13831/30000 Training Loss: 0.06880569458007812\n",
      "Epoch 13832/30000 Training Loss: 0.07570331543684006\n",
      "Epoch 13833/30000 Training Loss: 0.0718860924243927\n",
      "Epoch 13834/30000 Training Loss: 0.07297082245349884\n",
      "Epoch 13835/30000 Training Loss: 0.0801834687590599\n",
      "Epoch 13836/30000 Training Loss: 0.07601668685674667\n",
      "Epoch 13837/30000 Training Loss: 0.08190342783927917\n",
      "Epoch 13838/30000 Training Loss: 0.07386528700590134\n",
      "Epoch 13839/30000 Training Loss: 0.08281086385250092\n",
      "Epoch 13840/30000 Training Loss: 0.07412191480398178\n",
      "Epoch 13840/30000 Validation Loss: 0.08473780751228333\n",
      "Epoch 13841/30000 Training Loss: 0.07984349876642227\n",
      "Epoch 13842/30000 Training Loss: 0.07934105396270752\n",
      "Epoch 13843/30000 Training Loss: 0.07338402420282364\n",
      "Epoch 13844/30000 Training Loss: 0.0610550194978714\n",
      "Epoch 13845/30000 Training Loss: 0.06742294132709503\n",
      "Epoch 13846/30000 Training Loss: 0.07002144306898117\n",
      "Epoch 13847/30000 Training Loss: 0.07763384282588959\n",
      "Epoch 13848/30000 Training Loss: 0.073204405605793\n",
      "Epoch 13849/30000 Training Loss: 0.0852009579539299\n",
      "Epoch 13850/30000 Training Loss: 0.08243418484926224\n",
      "Epoch 13850/30000 Validation Loss: 0.10081398487091064\n",
      "Epoch 13851/30000 Training Loss: 0.07132051140069962\n",
      "Epoch 13852/30000 Training Loss: 0.08073132485151291\n",
      "Epoch 13853/30000 Training Loss: 0.07087583094835281\n",
      "Epoch 13854/30000 Training Loss: 0.07047386467456818\n",
      "Epoch 13855/30000 Training Loss: 0.061907511204481125\n",
      "Epoch 13856/30000 Training Loss: 0.08429638296365738\n",
      "Epoch 13857/30000 Training Loss: 0.06806328892707825\n",
      "Epoch 13858/30000 Training Loss: 0.06934667378664017\n",
      "Epoch 13859/30000 Training Loss: 0.08344274759292603\n",
      "Epoch 13860/30000 Training Loss: 0.0773526206612587\n",
      "Epoch 13860/30000 Validation Loss: 0.07704032212495804\n",
      "Epoch 13861/30000 Training Loss: 0.08180966973304749\n",
      "Epoch 13862/30000 Training Loss: 0.0927252545952797\n",
      "Epoch 13863/30000 Training Loss: 0.0818360224366188\n",
      "Epoch 13864/30000 Training Loss: 0.07942266017198563\n",
      "Epoch 13865/30000 Training Loss: 0.06484633684158325\n",
      "Epoch 13866/30000 Training Loss: 0.09918585419654846\n",
      "Epoch 13867/30000 Training Loss: 0.07234098762273788\n",
      "Epoch 13868/30000 Training Loss: 0.08659686893224716\n",
      "Epoch 13869/30000 Training Loss: 0.07136596739292145\n",
      "Epoch 13870/30000 Training Loss: 0.10693768411874771\n",
      "Epoch 13870/30000 Validation Loss: 0.06899482011795044\n",
      "Epoch 13871/30000 Training Loss: 0.08444693684577942\n",
      "Epoch 13872/30000 Training Loss: 0.08472442626953125\n",
      "Epoch 13873/30000 Training Loss: 0.08017263561487198\n",
      "Epoch 13874/30000 Training Loss: 0.08172775059938431\n",
      "Epoch 13875/30000 Training Loss: 0.10272232443094254\n",
      "Epoch 13876/30000 Training Loss: 0.06701038032770157\n",
      "Epoch 13877/30000 Training Loss: 0.07315821200609207\n",
      "Epoch 13878/30000 Training Loss: 0.0794653594493866\n",
      "Epoch 13879/30000 Training Loss: 0.10016509890556335\n",
      "Epoch 13880/30000 Training Loss: 0.07184477150440216\n",
      "Epoch 13880/30000 Validation Loss: 0.07670258730649948\n",
      "Epoch 13881/30000 Training Loss: 0.0726110115647316\n",
      "Epoch 13882/30000 Training Loss: 0.07987947016954422\n",
      "Epoch 13883/30000 Training Loss: 0.07308855652809143\n",
      "Epoch 13884/30000 Training Loss: 0.06897834688425064\n",
      "Epoch 13885/30000 Training Loss: 0.07212245464324951\n",
      "Epoch 13886/30000 Training Loss: 0.06438372284173965\n",
      "Epoch 13887/30000 Training Loss: 0.07301215082406998\n",
      "Epoch 13888/30000 Training Loss: 0.05800938606262207\n",
      "Epoch 13889/30000 Training Loss: 0.08774302154779434\n",
      "Epoch 13890/30000 Training Loss: 0.09574059396982193\n",
      "Epoch 13890/30000 Validation Loss: 0.09085371345281601\n",
      "Epoch 13891/30000 Training Loss: 0.06762046366930008\n",
      "Epoch 13892/30000 Training Loss: 0.08044736832380295\n",
      "Epoch 13893/30000 Training Loss: 0.09054137021303177\n",
      "Epoch 13894/30000 Training Loss: 0.08417844772338867\n",
      "Epoch 13895/30000 Training Loss: 0.0806032121181488\n",
      "Epoch 13896/30000 Training Loss: 0.07508673518896103\n",
      "Epoch 13897/30000 Training Loss: 0.08309879899024963\n",
      "Epoch 13898/30000 Training Loss: 0.07242203503847122\n",
      "Epoch 13899/30000 Training Loss: 0.0993478074669838\n",
      "Epoch 13900/30000 Training Loss: 0.07333254814147949\n",
      "Epoch 13900/30000 Validation Loss: 0.06644251197576523\n",
      "Epoch 13901/30000 Training Loss: 0.08749470114707947\n",
      "Epoch 13902/30000 Training Loss: 0.07034385204315186\n",
      "Epoch 13903/30000 Training Loss: 0.07167908549308777\n",
      "Epoch 13904/30000 Training Loss: 0.07934606820344925\n",
      "Epoch 13905/30000 Training Loss: 0.06949897855520248\n",
      "Epoch 13906/30000 Training Loss: 0.07267871499061584\n",
      "Epoch 13907/30000 Training Loss: 0.06852796673774719\n",
      "Epoch 13908/30000 Training Loss: 0.06666442006826401\n",
      "Epoch 13909/30000 Training Loss: 0.08594801276922226\n",
      "Epoch 13910/30000 Training Loss: 0.0774330198764801\n",
      "Epoch 13910/30000 Validation Loss: 0.0691891685128212\n",
      "Epoch 13911/30000 Training Loss: 0.08296311646699905\n",
      "Epoch 13912/30000 Training Loss: 0.10058147460222244\n",
      "Epoch 13913/30000 Training Loss: 0.08339426666498184\n",
      "Epoch 13914/30000 Training Loss: 0.06826891750097275\n",
      "Epoch 13915/30000 Training Loss: 0.07937788218259811\n",
      "Epoch 13916/30000 Training Loss: 0.10882440954446793\n",
      "Epoch 13917/30000 Training Loss: 0.06380156427621841\n",
      "Epoch 13918/30000 Training Loss: 0.08022821694612503\n",
      "Epoch 13919/30000 Training Loss: 0.08474069833755493\n",
      "Epoch 13920/30000 Training Loss: 0.06929417699575424\n",
      "Epoch 13920/30000 Validation Loss: 0.11693903803825378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13921/30000 Training Loss: 0.07899060845375061\n",
      "Epoch 13922/30000 Training Loss: 0.07808404415845871\n",
      "Epoch 13923/30000 Training Loss: 0.10594832897186279\n",
      "Epoch 13924/30000 Training Loss: 0.0817922055721283\n",
      "Epoch 13925/30000 Training Loss: 0.08062829822301865\n",
      "Epoch 13926/30000 Training Loss: 0.08472109586000443\n",
      "Epoch 13927/30000 Training Loss: 0.06787826120853424\n",
      "Epoch 13928/30000 Training Loss: 0.05726705491542816\n",
      "Epoch 13929/30000 Training Loss: 0.07963886857032776\n",
      "Epoch 13930/30000 Training Loss: 0.06703424453735352\n",
      "Epoch 13930/30000 Validation Loss: 0.08755415678024292\n",
      "Epoch 13931/30000 Training Loss: 0.0790162906050682\n",
      "Epoch 13932/30000 Training Loss: 0.07233424484729767\n",
      "Epoch 13933/30000 Training Loss: 0.09515883773565292\n",
      "Epoch 13934/30000 Training Loss: 0.07377418130636215\n",
      "Epoch 13935/30000 Training Loss: 0.07034984230995178\n",
      "Epoch 13936/30000 Training Loss: 0.09372114390134811\n",
      "Epoch 13937/30000 Training Loss: 0.06455270946025848\n",
      "Epoch 13938/30000 Training Loss: 0.07378115504980087\n",
      "Epoch 13939/30000 Training Loss: 0.06790066510438919\n",
      "Epoch 13940/30000 Training Loss: 0.08312150835990906\n",
      "Epoch 13940/30000 Validation Loss: 0.0772569328546524\n",
      "Epoch 13941/30000 Training Loss: 0.06556516140699387\n",
      "Epoch 13942/30000 Training Loss: 0.07716378569602966\n",
      "Epoch 13943/30000 Training Loss: 0.08758241683244705\n",
      "Epoch 13944/30000 Training Loss: 0.08312661200761795\n",
      "Epoch 13945/30000 Training Loss: 0.07047749310731888\n",
      "Epoch 13946/30000 Training Loss: 0.0697619840502739\n",
      "Epoch 13947/30000 Training Loss: 0.11685192584991455\n",
      "Epoch 13948/30000 Training Loss: 0.08992958068847656\n",
      "Epoch 13949/30000 Training Loss: 0.08513277769088745\n",
      "Epoch 13950/30000 Training Loss: 0.0625714436173439\n",
      "Epoch 13950/30000 Validation Loss: 0.07441834360361099\n",
      "Epoch 13951/30000 Training Loss: 0.06695735454559326\n",
      "Epoch 13952/30000 Training Loss: 0.05958757922053337\n",
      "Epoch 13953/30000 Training Loss: 0.07728500664234161\n",
      "Epoch 13954/30000 Training Loss: 0.07237458974123001\n",
      "Epoch 13955/30000 Training Loss: 0.06515422463417053\n",
      "Epoch 13956/30000 Training Loss: 0.07509519159793854\n",
      "Epoch 13957/30000 Training Loss: 0.08742775768041611\n",
      "Epoch 13958/30000 Training Loss: 0.07525373250246048\n",
      "Epoch 13959/30000 Training Loss: 0.05686585232615471\n",
      "Epoch 13960/30000 Training Loss: 0.08057470619678497\n",
      "Epoch 13960/30000 Validation Loss: 0.07289833575487137\n",
      "Epoch 13961/30000 Training Loss: 0.06436455249786377\n",
      "Epoch 13962/30000 Training Loss: 0.08214887231588364\n",
      "Epoch 13963/30000 Training Loss: 0.06223815679550171\n",
      "Epoch 13964/30000 Training Loss: 0.07228334993124008\n",
      "Epoch 13965/30000 Training Loss: 0.056039486080408096\n",
      "Epoch 13966/30000 Training Loss: 0.08041145652532578\n",
      "Epoch 13967/30000 Training Loss: 0.07004529982805252\n",
      "Epoch 13968/30000 Training Loss: 0.06919372826814651\n",
      "Epoch 13969/30000 Training Loss: 0.08635979145765305\n",
      "Epoch 13970/30000 Training Loss: 0.07147341221570969\n",
      "Epoch 13970/30000 Validation Loss: 0.06333914399147034\n",
      "Epoch 13971/30000 Training Loss: 0.06148914992809296\n",
      "Epoch 13972/30000 Training Loss: 0.08801377564668655\n",
      "Epoch 13973/30000 Training Loss: 0.060513079166412354\n",
      "Epoch 13974/30000 Training Loss: 0.07395607978105545\n",
      "Epoch 13975/30000 Training Loss: 0.07258342951536179\n",
      "Epoch 13976/30000 Training Loss: 0.0945698544383049\n",
      "Epoch 13977/30000 Training Loss: 0.06374618411064148\n",
      "Epoch 13978/30000 Training Loss: 0.07804342359304428\n",
      "Epoch 13979/30000 Training Loss: 0.08218087255954742\n",
      "Epoch 13980/30000 Training Loss: 0.07123962044715881\n",
      "Epoch 13980/30000 Validation Loss: 0.08230918645858765\n",
      "Epoch 13981/30000 Training Loss: 0.08518768101930618\n",
      "Epoch 13982/30000 Training Loss: 0.0943843424320221\n",
      "Epoch 13983/30000 Training Loss: 0.06187709793448448\n",
      "Epoch 13984/30000 Training Loss: 0.05713921785354614\n",
      "Epoch 13985/30000 Training Loss: 0.06674560159444809\n",
      "Epoch 13986/30000 Training Loss: 0.0843764916062355\n",
      "Epoch 13987/30000 Training Loss: 0.06578001379966736\n",
      "Epoch 13988/30000 Training Loss: 0.08387721329927444\n",
      "Epoch 13989/30000 Training Loss: 0.07458681613206863\n",
      "Epoch 13990/30000 Training Loss: 0.10278555750846863\n",
      "Epoch 13990/30000 Validation Loss: 0.07559815794229507\n",
      "Epoch 13991/30000 Training Loss: 0.09048106521368027\n",
      "Epoch 13992/30000 Training Loss: 0.06755318492650986\n",
      "Epoch 13993/30000 Training Loss: 0.08167015761137009\n",
      "Epoch 13994/30000 Training Loss: 0.08661141246557236\n",
      "Epoch 13995/30000 Training Loss: 0.07179892808198929\n",
      "Epoch 13996/30000 Training Loss: 0.07118228077888489\n",
      "Epoch 13997/30000 Training Loss: 0.05313633009791374\n",
      "Epoch 13998/30000 Training Loss: 0.07524095475673676\n",
      "Epoch 13999/30000 Training Loss: 0.06917013972997665\n",
      "Epoch 14000/30000 Training Loss: 0.06438137590885162\n",
      "Epoch 14000/30000 Validation Loss: 0.07316940277814865\n",
      "Epoch 14001/30000 Training Loss: 0.06922189891338348\n",
      "Epoch 14002/30000 Training Loss: 0.10596033930778503\n",
      "Epoch 14003/30000 Training Loss: 0.0800003707408905\n",
      "Epoch 14004/30000 Training Loss: 0.07374317198991776\n",
      "Epoch 14005/30000 Training Loss: 0.06765811890363693\n",
      "Epoch 14006/30000 Training Loss: 0.08814003318548203\n",
      "Epoch 14007/30000 Training Loss: 0.07134710997343063\n",
      "Epoch 14008/30000 Training Loss: 0.11024997383356094\n",
      "Epoch 14009/30000 Training Loss: 0.07960205525159836\n",
      "Epoch 14010/30000 Training Loss: 0.07895529270172119\n",
      "Epoch 14010/30000 Validation Loss: 0.06347194314002991\n",
      "Epoch 14011/30000 Training Loss: 0.08215264230966568\n",
      "Epoch 14012/30000 Training Loss: 0.06580968201160431\n",
      "Epoch 14013/30000 Training Loss: 0.07279648631811142\n",
      "Epoch 14014/30000 Training Loss: 0.07534125447273254\n",
      "Epoch 14015/30000 Training Loss: 0.06689894944429398\n",
      "Epoch 14016/30000 Training Loss: 0.07090745121240616\n",
      "Epoch 14017/30000 Training Loss: 0.07453086972236633\n",
      "Epoch 14018/30000 Training Loss: 0.07781711965799332\n",
      "Epoch 14019/30000 Training Loss: 0.08371353149414062\n",
      "Epoch 14020/30000 Training Loss: 0.06277377903461456\n",
      "Epoch 14020/30000 Validation Loss: 0.07842999696731567\n",
      "Epoch 14021/30000 Training Loss: 0.07888946682214737\n",
      "Epoch 14022/30000 Training Loss: 0.07606472820043564\n",
      "Epoch 14023/30000 Training Loss: 0.06534480303525925\n",
      "Epoch 14024/30000 Training Loss: 0.0650351345539093\n",
      "Epoch 14025/30000 Training Loss: 0.10137730836868286\n",
      "Epoch 14026/30000 Training Loss: 0.08653052896261215\n",
      "Epoch 14027/30000 Training Loss: 0.07838209718465805\n",
      "Epoch 14028/30000 Training Loss: 0.06373360008001328\n",
      "Epoch 14029/30000 Training Loss: 0.07405952364206314\n",
      "Epoch 14030/30000 Training Loss: 0.07494468241930008\n",
      "Epoch 14030/30000 Validation Loss: 0.06305605173110962\n",
      "Epoch 14031/30000 Training Loss: 0.07268476486206055\n",
      "Epoch 14032/30000 Training Loss: 0.07700405269861221\n",
      "Epoch 14033/30000 Training Loss: 0.07894866913557053\n",
      "Epoch 14034/30000 Training Loss: 0.06246310472488403\n",
      "Epoch 14035/30000 Training Loss: 0.0740973949432373\n",
      "Epoch 14036/30000 Training Loss: 0.06959148496389389\n",
      "Epoch 14037/30000 Training Loss: 0.08206895738840103\n",
      "Epoch 14038/30000 Training Loss: 0.06308510899543762\n",
      "Epoch 14039/30000 Training Loss: 0.067385233938694\n",
      "Epoch 14040/30000 Training Loss: 0.07781824469566345\n",
      "Epoch 14040/30000 Validation Loss: 0.07049883902072906\n",
      "Epoch 14041/30000 Training Loss: 0.09100868552923203\n",
      "Epoch 14042/30000 Training Loss: 0.07291256636381149\n",
      "Epoch 14043/30000 Training Loss: 0.07421497255563736\n",
      "Epoch 14044/30000 Training Loss: 0.08320807665586472\n",
      "Epoch 14045/30000 Training Loss: 0.09105797857046127\n",
      "Epoch 14046/30000 Training Loss: 0.07739191502332687\n",
      "Epoch 14047/30000 Training Loss: 0.08118164539337158\n",
      "Epoch 14048/30000 Training Loss: 0.09133311361074448\n",
      "Epoch 14049/30000 Training Loss: 0.07604949921369553\n",
      "Epoch 14050/30000 Training Loss: 0.08223019540309906\n",
      "Epoch 14050/30000 Validation Loss: 0.08183372765779495\n",
      "Epoch 14051/30000 Training Loss: 0.08217503875494003\n",
      "Epoch 14052/30000 Training Loss: 0.08593988418579102\n",
      "Epoch 14053/30000 Training Loss: 0.06641105562448502\n",
      "Epoch 14054/30000 Training Loss: 0.07239693403244019\n",
      "Epoch 14055/30000 Training Loss: 0.08139608800411224\n",
      "Epoch 14056/30000 Training Loss: 0.08089912682771683\n",
      "Epoch 14057/30000 Training Loss: 0.0693642646074295\n",
      "Epoch 14058/30000 Training Loss: 0.07285314053297043\n",
      "Epoch 14059/30000 Training Loss: 0.06926470994949341\n",
      "Epoch 14060/30000 Training Loss: 0.06828444451093674\n",
      "Epoch 14060/30000 Validation Loss: 0.06840094923973083\n",
      "Epoch 14061/30000 Training Loss: 0.06377359479665756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14062/30000 Training Loss: 0.07236378639936447\n",
      "Epoch 14063/30000 Training Loss: 0.07789120078086853\n",
      "Epoch 14064/30000 Training Loss: 0.07644779980182648\n",
      "Epoch 14065/30000 Training Loss: 0.08382268995046616\n",
      "Epoch 14066/30000 Training Loss: 0.07015834003686905\n",
      "Epoch 14067/30000 Training Loss: 0.05411742255091667\n",
      "Epoch 14068/30000 Training Loss: 0.06548026949167252\n",
      "Epoch 14069/30000 Training Loss: 0.0880926251411438\n",
      "Epoch 14070/30000 Training Loss: 0.06535456329584122\n",
      "Epoch 14070/30000 Validation Loss: 0.08228161931037903\n",
      "Epoch 14071/30000 Training Loss: 0.07018160074949265\n",
      "Epoch 14072/30000 Training Loss: 0.08274906128644943\n",
      "Epoch 14073/30000 Training Loss: 0.0731559619307518\n",
      "Epoch 14074/30000 Training Loss: 0.07094130665063858\n",
      "Epoch 14075/30000 Training Loss: 0.06903380155563354\n",
      "Epoch 14076/30000 Training Loss: 0.08057831972837448\n",
      "Epoch 14077/30000 Training Loss: 0.0632481575012207\n",
      "Epoch 14078/30000 Training Loss: 0.07202976197004318\n",
      "Epoch 14079/30000 Training Loss: 0.08700013160705566\n",
      "Epoch 14080/30000 Training Loss: 0.07523129135370255\n",
      "Epoch 14080/30000 Validation Loss: 0.0699000209569931\n",
      "Epoch 14081/30000 Training Loss: 0.08612162619829178\n",
      "Epoch 14082/30000 Training Loss: 0.07666980475187302\n",
      "Epoch 14083/30000 Training Loss: 0.06879254430532455\n",
      "Epoch 14084/30000 Training Loss: 0.07651977241039276\n",
      "Epoch 14085/30000 Training Loss: 0.07805337756872177\n",
      "Epoch 14086/30000 Training Loss: 0.0803845003247261\n",
      "Epoch 14087/30000 Training Loss: 0.06892900913953781\n",
      "Epoch 14088/30000 Training Loss: 0.06969226151704788\n",
      "Epoch 14089/30000 Training Loss: 0.09858143329620361\n",
      "Epoch 14090/30000 Training Loss: 0.0717049315571785\n",
      "Epoch 14090/30000 Validation Loss: 0.0752042606472969\n",
      "Epoch 14091/30000 Training Loss: 0.07034527510404587\n",
      "Epoch 14092/30000 Training Loss: 0.0629504844546318\n",
      "Epoch 14093/30000 Training Loss: 0.06908126175403595\n",
      "Epoch 14094/30000 Training Loss: 0.08833837509155273\n",
      "Epoch 14095/30000 Training Loss: 0.085549496114254\n",
      "Epoch 14096/30000 Training Loss: 0.0761898085474968\n",
      "Epoch 14097/30000 Training Loss: 0.07501513510942459\n",
      "Epoch 14098/30000 Training Loss: 0.06808654218912125\n",
      "Epoch 14099/30000 Training Loss: 0.06945475190877914\n",
      "Epoch 14100/30000 Training Loss: 0.06596186012029648\n",
      "Epoch 14100/30000 Validation Loss: 0.10141560435295105\n",
      "Epoch 14101/30000 Training Loss: 0.06391111016273499\n",
      "Epoch 14102/30000 Training Loss: 0.07421674579381943\n",
      "Epoch 14103/30000 Training Loss: 0.07919759303331375\n",
      "Epoch 14104/30000 Training Loss: 0.060676753520965576\n",
      "Epoch 14105/30000 Training Loss: 0.07907440513372421\n",
      "Epoch 14106/30000 Training Loss: 0.06696366518735886\n",
      "Epoch 14107/30000 Training Loss: 0.08386529237031937\n",
      "Epoch 14108/30000 Training Loss: 0.08798572421073914\n",
      "Epoch 14109/30000 Training Loss: 0.07576825469732285\n",
      "Epoch 14110/30000 Training Loss: 0.09455984830856323\n",
      "Epoch 14110/30000 Validation Loss: 0.07449781149625778\n",
      "Epoch 14111/30000 Training Loss: 0.10914119333028793\n",
      "Epoch 14112/30000 Training Loss: 0.08365226536989212\n",
      "Epoch 14113/30000 Training Loss: 0.07058750092983246\n",
      "Epoch 14114/30000 Training Loss: 0.07753273099660873\n",
      "Epoch 14115/30000 Training Loss: 0.07562737166881561\n",
      "Epoch 14116/30000 Training Loss: 0.0786728635430336\n",
      "Epoch 14117/30000 Training Loss: 0.09870640188455582\n",
      "Epoch 14118/30000 Training Loss: 0.08384052664041519\n",
      "Epoch 14119/30000 Training Loss: 0.08452451229095459\n",
      "Epoch 14120/30000 Training Loss: 0.0743461325764656\n",
      "Epoch 14120/30000 Validation Loss: 0.04954015091061592\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04954015091061592<=============\n",
      "Epoch 14121/30000 Training Loss: 0.06723243743181229\n",
      "Epoch 14122/30000 Training Loss: 0.06849772483110428\n",
      "Epoch 14123/30000 Training Loss: 0.07412701100111008\n",
      "Epoch 14124/30000 Training Loss: 0.07599060982465744\n",
      "Epoch 14125/30000 Training Loss: 0.08237393945455551\n",
      "Epoch 14126/30000 Training Loss: 0.06366556882858276\n",
      "Epoch 14127/30000 Training Loss: 0.07323332875967026\n",
      "Epoch 14128/30000 Training Loss: 0.08430880308151245\n",
      "Epoch 14129/30000 Training Loss: 0.06704429537057877\n",
      "Epoch 14130/30000 Training Loss: 0.07189985364675522\n",
      "Epoch 14130/30000 Validation Loss: 0.07155495882034302\n",
      "Epoch 14131/30000 Training Loss: 0.06840896606445312\n",
      "Epoch 14132/30000 Training Loss: 0.06925985217094421\n",
      "Epoch 14133/30000 Training Loss: 0.07443148642778397\n",
      "Epoch 14134/30000 Training Loss: 0.07035726308822632\n",
      "Epoch 14135/30000 Training Loss: 0.0862058773636818\n",
      "Epoch 14136/30000 Training Loss: 0.08864971250295639\n",
      "Epoch 14137/30000 Training Loss: 0.09092552214860916\n",
      "Epoch 14138/30000 Training Loss: 0.07852410525083542\n",
      "Epoch 14139/30000 Training Loss: 0.07120344787836075\n",
      "Epoch 14140/30000 Training Loss: 0.07146546244621277\n",
      "Epoch 14140/30000 Validation Loss: 0.07667804509401321\n",
      "Epoch 14141/30000 Training Loss: 0.059397656470537186\n",
      "Epoch 14142/30000 Training Loss: 0.05754108354449272\n",
      "Epoch 14143/30000 Training Loss: 0.06707154959440231\n",
      "Epoch 14144/30000 Training Loss: 0.07972090691328049\n",
      "Epoch 14145/30000 Training Loss: 0.08348875492811203\n",
      "Epoch 14146/30000 Training Loss: 0.07535292953252792\n",
      "Epoch 14147/30000 Training Loss: 0.06670535355806351\n",
      "Epoch 14148/30000 Training Loss: 0.08066574484109879\n",
      "Epoch 14149/30000 Training Loss: 0.06537190824747086\n",
      "Epoch 14150/30000 Training Loss: 0.08483337610960007\n",
      "Epoch 14150/30000 Validation Loss: 0.08045738190412521\n",
      "Epoch 14151/30000 Training Loss: 0.07932084798812866\n",
      "Epoch 14152/30000 Training Loss: 0.06515207141637802\n",
      "Epoch 14153/30000 Training Loss: 0.0773644968867302\n",
      "Epoch 14154/30000 Training Loss: 0.08333716541528702\n",
      "Epoch 14155/30000 Training Loss: 0.08461645245552063\n",
      "Epoch 14156/30000 Training Loss: 0.09453612565994263\n",
      "Epoch 14157/30000 Training Loss: 0.07292366772890091\n",
      "Epoch 14158/30000 Training Loss: 0.07254916429519653\n",
      "Epoch 14159/30000 Training Loss: 0.08298852294683456\n",
      "Epoch 14160/30000 Training Loss: 0.09805335849523544\n",
      "Epoch 14160/30000 Validation Loss: 0.07606213539838791\n",
      "Epoch 14161/30000 Training Loss: 0.1039290651679039\n",
      "Epoch 14162/30000 Training Loss: 0.0646500214934349\n",
      "Epoch 14163/30000 Training Loss: 0.08786780387163162\n",
      "Epoch 14164/30000 Training Loss: 0.0750541016459465\n",
      "Epoch 14165/30000 Training Loss: 0.0669529139995575\n",
      "Epoch 14166/30000 Training Loss: 0.08321329206228256\n",
      "Epoch 14167/30000 Training Loss: 0.07690735161304474\n",
      "Epoch 14168/30000 Training Loss: 0.06388295441865921\n",
      "Epoch 14169/30000 Training Loss: 0.08385336399078369\n",
      "Epoch 14170/30000 Training Loss: 0.09558161348104477\n",
      "Epoch 14170/30000 Validation Loss: 0.0640183612704277\n",
      "Epoch 14171/30000 Training Loss: 0.06917134672403336\n",
      "Epoch 14172/30000 Training Loss: 0.070963554084301\n",
      "Epoch 14173/30000 Training Loss: 0.06784147024154663\n",
      "Epoch 14174/30000 Training Loss: 0.06525243073701859\n",
      "Epoch 14175/30000 Training Loss: 0.07625630497932434\n",
      "Epoch 14176/30000 Training Loss: 0.0905008390545845\n",
      "Epoch 14177/30000 Training Loss: 0.06860726326704025\n",
      "Epoch 14178/30000 Training Loss: 0.06553905457258224\n",
      "Epoch 14179/30000 Training Loss: 0.07095279544591904\n",
      "Epoch 14180/30000 Training Loss: 0.08898182958364487\n",
      "Epoch 14180/30000 Validation Loss: 0.06460964679718018\n",
      "Epoch 14181/30000 Training Loss: 0.08149776607751846\n",
      "Epoch 14182/30000 Training Loss: 0.08100652694702148\n",
      "Epoch 14183/30000 Training Loss: 0.09399536997079849\n",
      "Epoch 14184/30000 Training Loss: 0.07536698132753372\n",
      "Epoch 14185/30000 Training Loss: 0.07922118902206421\n",
      "Epoch 14186/30000 Training Loss: 0.07195829600095749\n",
      "Epoch 14187/30000 Training Loss: 0.07472651451826096\n",
      "Epoch 14188/30000 Training Loss: 0.058874424546957016\n",
      "Epoch 14189/30000 Training Loss: 0.06116621196269989\n",
      "Epoch 14190/30000 Training Loss: 0.07631700485944748\n",
      "Epoch 14190/30000 Validation Loss: 0.08051914721727371\n",
      "Epoch 14191/30000 Training Loss: 0.07148563116788864\n",
      "Epoch 14192/30000 Training Loss: 0.07366953790187836\n",
      "Epoch 14193/30000 Training Loss: 0.0778115838766098\n",
      "Epoch 14194/30000 Training Loss: 0.07161497324705124\n",
      "Epoch 14195/30000 Training Loss: 0.07388634979724884\n",
      "Epoch 14196/30000 Training Loss: 0.07315982133150101\n",
      "Epoch 14197/30000 Training Loss: 0.08451002836227417\n",
      "Epoch 14198/30000 Training Loss: 0.06900650262832642\n",
      "Epoch 14199/30000 Training Loss: 0.06622127443552017\n",
      "Epoch 14200/30000 Training Loss: 0.061735253781080246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14200/30000 Validation Loss: 0.06704897433519363\n",
      "Epoch 14201/30000 Training Loss: 0.08106254786252975\n",
      "Epoch 14202/30000 Training Loss: 0.0742352232336998\n",
      "Epoch 14203/30000 Training Loss: 0.08483461290597916\n",
      "Epoch 14204/30000 Training Loss: 0.08799479156732559\n",
      "Epoch 14205/30000 Training Loss: 0.08009153604507446\n",
      "Epoch 14206/30000 Training Loss: 0.07678086310625076\n",
      "Epoch 14207/30000 Training Loss: 0.07561026513576508\n",
      "Epoch 14208/30000 Training Loss: 0.058064479380846024\n",
      "Epoch 14209/30000 Training Loss: 0.07965847104787827\n",
      "Epoch 14210/30000 Training Loss: 0.07560504227876663\n",
      "Epoch 14210/30000 Validation Loss: 0.07629945129156113\n",
      "Epoch 14211/30000 Training Loss: 0.06250777095556259\n",
      "Epoch 14212/30000 Training Loss: 0.07969653606414795\n",
      "Epoch 14213/30000 Training Loss: 0.0835544541478157\n",
      "Epoch 14214/30000 Training Loss: 0.06709776073694229\n",
      "Epoch 14215/30000 Training Loss: 0.07171987742185593\n",
      "Epoch 14216/30000 Training Loss: 0.06580103188753128\n",
      "Epoch 14217/30000 Training Loss: 0.06683371216058731\n",
      "Epoch 14218/30000 Training Loss: 0.07891711592674255\n",
      "Epoch 14219/30000 Training Loss: 0.05661335587501526\n",
      "Epoch 14220/30000 Training Loss: 0.08973520994186401\n",
      "Epoch 14220/30000 Validation Loss: 0.07207247614860535\n",
      "Epoch 14221/30000 Training Loss: 0.07044794410467148\n",
      "Epoch 14222/30000 Training Loss: 0.0668695792555809\n",
      "Epoch 14223/30000 Training Loss: 0.08871492743492126\n",
      "Epoch 14224/30000 Training Loss: 0.09155622869729996\n",
      "Epoch 14225/30000 Training Loss: 0.0749850794672966\n",
      "Epoch 14226/30000 Training Loss: 0.08707321435213089\n",
      "Epoch 14227/30000 Training Loss: 0.06945610046386719\n",
      "Epoch 14228/30000 Training Loss: 0.09051594138145447\n",
      "Epoch 14229/30000 Training Loss: 0.06827224045991898\n",
      "Epoch 14230/30000 Training Loss: 0.09049862623214722\n",
      "Epoch 14230/30000 Validation Loss: 0.08963976055383682\n",
      "Epoch 14231/30000 Training Loss: 0.06603506207466125\n",
      "Epoch 14232/30000 Training Loss: 0.07316049933433533\n",
      "Epoch 14233/30000 Training Loss: 0.07339021563529968\n",
      "Epoch 14234/30000 Training Loss: 0.06453535705804825\n",
      "Epoch 14235/30000 Training Loss: 0.07861433923244476\n",
      "Epoch 14236/30000 Training Loss: 0.10126819461584091\n",
      "Epoch 14237/30000 Training Loss: 0.07168637216091156\n",
      "Epoch 14238/30000 Training Loss: 0.07587301731109619\n",
      "Epoch 14239/30000 Training Loss: 0.07942701131105423\n",
      "Epoch 14240/30000 Training Loss: 0.08727612346410751\n",
      "Epoch 14240/30000 Validation Loss: 0.058887723833322525\n",
      "Epoch 14241/30000 Training Loss: 0.10032811015844345\n",
      "Epoch 14242/30000 Training Loss: 0.10286041349172592\n",
      "Epoch 14243/30000 Training Loss: 0.0701698511838913\n",
      "Epoch 14244/30000 Training Loss: 0.07880263775587082\n",
      "Epoch 14245/30000 Training Loss: 0.10943569988012314\n",
      "Epoch 14246/30000 Training Loss: 0.0733441635966301\n",
      "Epoch 14247/30000 Training Loss: 0.07337027043104172\n",
      "Epoch 14248/30000 Training Loss: 0.08957614749670029\n",
      "Epoch 14249/30000 Training Loss: 0.07346629351377487\n",
      "Epoch 14250/30000 Training Loss: 0.05784124135971069\n",
      "Epoch 14250/30000 Validation Loss: 0.0735354796051979\n",
      "Epoch 14251/30000 Training Loss: 0.0739276334643364\n",
      "Epoch 14252/30000 Training Loss: 0.0813751295208931\n",
      "Epoch 14253/30000 Training Loss: 0.0738842636346817\n",
      "Epoch 14254/30000 Training Loss: 0.057830583304166794\n",
      "Epoch 14255/30000 Training Loss: 0.06878980249166489\n",
      "Epoch 14256/30000 Training Loss: 0.07869146019220352\n",
      "Epoch 14257/30000 Training Loss: 0.08470634371042252\n",
      "Epoch 14258/30000 Training Loss: 0.08544910699129105\n",
      "Epoch 14259/30000 Training Loss: 0.06671654433012009\n",
      "Epoch 14260/30000 Training Loss: 0.05852467939257622\n",
      "Epoch 14260/30000 Validation Loss: 0.07953175157308578\n",
      "Epoch 14261/30000 Training Loss: 0.07808584719896317\n",
      "Epoch 14262/30000 Training Loss: 0.07441302388906479\n",
      "Epoch 14263/30000 Training Loss: 0.08701959252357483\n",
      "Epoch 14264/30000 Training Loss: 0.07952231913805008\n",
      "Epoch 14265/30000 Training Loss: 0.07915263622999191\n",
      "Epoch 14266/30000 Training Loss: 0.060435306280851364\n",
      "Epoch 14267/30000 Training Loss: 0.0712905153632164\n",
      "Epoch 14268/30000 Training Loss: 0.07980064302682877\n",
      "Epoch 14269/30000 Training Loss: 0.07759254425764084\n",
      "Epoch 14270/30000 Training Loss: 0.07367110252380371\n",
      "Epoch 14270/30000 Validation Loss: 0.09012439101934433\n",
      "Epoch 14271/30000 Training Loss: 0.06556108593940735\n",
      "Epoch 14272/30000 Training Loss: 0.060720499604940414\n",
      "Epoch 14273/30000 Training Loss: 0.0631023570895195\n",
      "Epoch 14274/30000 Training Loss: 0.08364725112915039\n",
      "Epoch 14275/30000 Training Loss: 0.07854661345481873\n",
      "Epoch 14276/30000 Training Loss: 0.07671099901199341\n",
      "Epoch 14277/30000 Training Loss: 0.068479023873806\n",
      "Epoch 14278/30000 Training Loss: 0.07905571907758713\n",
      "Epoch 14279/30000 Training Loss: 0.0688134953379631\n",
      "Epoch 14280/30000 Training Loss: 0.1006203293800354\n",
      "Epoch 14280/30000 Validation Loss: 0.08735039830207825\n",
      "Epoch 14281/30000 Training Loss: 0.08840662986040115\n",
      "Epoch 14282/30000 Training Loss: 0.07386007905006409\n",
      "Epoch 14283/30000 Training Loss: 0.062131911516189575\n",
      "Epoch 14284/30000 Training Loss: 0.05963301286101341\n",
      "Epoch 14285/30000 Training Loss: 0.0726010873913765\n",
      "Epoch 14286/30000 Training Loss: 0.06159934401512146\n",
      "Epoch 14287/30000 Training Loss: 0.07005788385868073\n",
      "Epoch 14288/30000 Training Loss: 0.08624432235956192\n",
      "Epoch 14289/30000 Training Loss: 0.0934801697731018\n",
      "Epoch 14290/30000 Training Loss: 0.06210631504654884\n",
      "Epoch 14290/30000 Validation Loss: 0.08266481757164001\n",
      "Epoch 14291/30000 Training Loss: 0.0741473063826561\n",
      "Epoch 14292/30000 Training Loss: 0.07909616827964783\n",
      "Epoch 14293/30000 Training Loss: 0.06425119936466217\n",
      "Epoch 14294/30000 Training Loss: 0.07053375989198685\n",
      "Epoch 14295/30000 Training Loss: 0.07104482501745224\n",
      "Epoch 14296/30000 Training Loss: 0.08581262826919556\n",
      "Epoch 14297/30000 Training Loss: 0.08789751678705215\n",
      "Epoch 14298/30000 Training Loss: 0.06193561479449272\n",
      "Epoch 14299/30000 Training Loss: 0.08253402262926102\n",
      "Epoch 14300/30000 Training Loss: 0.0846940279006958\n",
      "Epoch 14300/30000 Validation Loss: 0.08184852451086044\n",
      "Epoch 14301/30000 Training Loss: 0.06161845847964287\n",
      "Epoch 14302/30000 Training Loss: 0.08509985357522964\n",
      "Epoch 14303/30000 Training Loss: 0.0753801092505455\n",
      "Epoch 14304/30000 Training Loss: 0.06656978279352188\n",
      "Epoch 14305/30000 Training Loss: 0.05956463888287544\n",
      "Epoch 14306/30000 Training Loss: 0.08149250596761703\n",
      "Epoch 14307/30000 Training Loss: 0.07132743299007416\n",
      "Epoch 14308/30000 Training Loss: 0.0686967596411705\n",
      "Epoch 14309/30000 Training Loss: 0.09532883763313293\n",
      "Epoch 14310/30000 Training Loss: 0.06247759982943535\n",
      "Epoch 14310/30000 Validation Loss: 0.08002206683158875\n",
      "Epoch 14311/30000 Training Loss: 0.05783437192440033\n",
      "Epoch 14312/30000 Training Loss: 0.05923846364021301\n",
      "Epoch 14313/30000 Training Loss: 0.07076197117567062\n",
      "Epoch 14314/30000 Training Loss: 0.09120471030473709\n",
      "Epoch 14315/30000 Training Loss: 0.07718721777200699\n",
      "Epoch 14316/30000 Training Loss: 0.08492269366979599\n",
      "Epoch 14317/30000 Training Loss: 0.06360108405351639\n",
      "Epoch 14318/30000 Training Loss: 0.07558120042085648\n",
      "Epoch 14319/30000 Training Loss: 0.07440987974405289\n",
      "Epoch 14320/30000 Training Loss: 0.08420751243829727\n",
      "Epoch 14320/30000 Validation Loss: 0.08152725547552109\n",
      "Epoch 14321/30000 Training Loss: 0.07510410249233246\n",
      "Epoch 14322/30000 Training Loss: 0.09096769243478775\n",
      "Epoch 14323/30000 Training Loss: 0.0816301703453064\n",
      "Epoch 14324/30000 Training Loss: 0.09141338616609573\n",
      "Epoch 14325/30000 Training Loss: 0.0707525908946991\n",
      "Epoch 14326/30000 Training Loss: 0.0683637261390686\n",
      "Epoch 14327/30000 Training Loss: 0.06652776896953583\n",
      "Epoch 14328/30000 Training Loss: 0.09139702469110489\n",
      "Epoch 14329/30000 Training Loss: 0.07611880451440811\n",
      "Epoch 14330/30000 Training Loss: 0.06822691112756729\n",
      "Epoch 14330/30000 Validation Loss: 0.06190180778503418\n",
      "Epoch 14331/30000 Training Loss: 0.08986025303602219\n",
      "Epoch 14332/30000 Training Loss: 0.07895135879516602\n",
      "Epoch 14333/30000 Training Loss: 0.07049647718667984\n",
      "Epoch 14334/30000 Training Loss: 0.07839518785476685\n",
      "Epoch 14335/30000 Training Loss: 0.0625830665230751\n",
      "Epoch 14336/30000 Training Loss: 0.06980756670236588\n",
      "Epoch 14337/30000 Training Loss: 0.0813841000199318\n",
      "Epoch 14338/30000 Training Loss: 0.06527089327573776\n",
      "Epoch 14339/30000 Training Loss: 0.057270508259534836\n",
      "Epoch 14340/30000 Training Loss: 0.07347080856561661\n",
      "Epoch 14340/30000 Validation Loss: 0.10780802369117737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14341/30000 Training Loss: 0.06431114673614502\n",
      "Epoch 14342/30000 Training Loss: 0.0861346498131752\n",
      "Epoch 14343/30000 Training Loss: 0.09128836542367935\n",
      "Epoch 14344/30000 Training Loss: 0.08186016231775284\n",
      "Epoch 14345/30000 Training Loss: 0.06060066819190979\n",
      "Epoch 14346/30000 Training Loss: 0.074700266122818\n",
      "Epoch 14347/30000 Training Loss: 0.10688892751932144\n",
      "Epoch 14348/30000 Training Loss: 0.06905878335237503\n",
      "Epoch 14349/30000 Training Loss: 0.09594681113958359\n",
      "Epoch 14350/30000 Training Loss: 0.10427945107221603\n",
      "Epoch 14350/30000 Validation Loss: 0.09020933508872986\n",
      "Epoch 14351/30000 Training Loss: 0.0762488916516304\n",
      "Epoch 14352/30000 Training Loss: 0.07076578587293625\n",
      "Epoch 14353/30000 Training Loss: 0.07007263600826263\n",
      "Epoch 14354/30000 Training Loss: 0.07304850965738297\n",
      "Epoch 14355/30000 Training Loss: 0.08582206815481186\n",
      "Epoch 14356/30000 Training Loss: 0.06959599256515503\n",
      "Epoch 14357/30000 Training Loss: 0.09211874008178711\n",
      "Epoch 14358/30000 Training Loss: 0.06604558974504471\n",
      "Epoch 14359/30000 Training Loss: 0.08639685064554214\n",
      "Epoch 14360/30000 Training Loss: 0.08022639900445938\n",
      "Epoch 14360/30000 Validation Loss: 0.06748455762863159\n",
      "Epoch 14361/30000 Training Loss: 0.06960640102624893\n",
      "Epoch 14362/30000 Training Loss: 0.06739506125450134\n",
      "Epoch 14363/30000 Training Loss: 0.08302295207977295\n",
      "Epoch 14364/30000 Training Loss: 0.06915188580751419\n",
      "Epoch 14365/30000 Training Loss: 0.0718550980091095\n",
      "Epoch 14366/30000 Training Loss: 0.07676968723535538\n",
      "Epoch 14367/30000 Training Loss: 0.09404364228248596\n",
      "Epoch 14368/30000 Training Loss: 0.08135044574737549\n",
      "Epoch 14369/30000 Training Loss: 0.081259585916996\n",
      "Epoch 14370/30000 Training Loss: 0.060229819267988205\n",
      "Epoch 14370/30000 Validation Loss: 0.076234832406044\n",
      "Epoch 14371/30000 Training Loss: 0.06962969154119492\n",
      "Epoch 14372/30000 Training Loss: 0.09441769123077393\n",
      "Epoch 14373/30000 Training Loss: 0.10297662764787674\n",
      "Epoch 14374/30000 Training Loss: 0.07415885478258133\n",
      "Epoch 14375/30000 Training Loss: 0.060869354754686356\n",
      "Epoch 14376/30000 Training Loss: 0.07865684479475021\n",
      "Epoch 14377/30000 Training Loss: 0.08261210471391678\n",
      "Epoch 14378/30000 Training Loss: 0.06525299698114395\n",
      "Epoch 14379/30000 Training Loss: 0.08009662479162216\n",
      "Epoch 14380/30000 Training Loss: 0.06723717600107193\n",
      "Epoch 14380/30000 Validation Loss: 0.06135737895965576\n",
      "Epoch 14381/30000 Training Loss: 0.06521299481391907\n",
      "Epoch 14382/30000 Training Loss: 0.08829084038734436\n",
      "Epoch 14383/30000 Training Loss: 0.07038746774196625\n",
      "Epoch 14384/30000 Training Loss: 0.07412374764680862\n",
      "Epoch 14385/30000 Training Loss: 0.09061887860298157\n",
      "Epoch 14386/30000 Training Loss: 0.06952609121799469\n",
      "Epoch 14387/30000 Training Loss: 0.06258931756019592\n",
      "Epoch 14388/30000 Training Loss: 0.09942585974931717\n",
      "Epoch 14389/30000 Training Loss: 0.055941011756658554\n",
      "Epoch 14390/30000 Training Loss: 0.0826568603515625\n",
      "Epoch 14390/30000 Validation Loss: 0.060995977371931076\n",
      "Epoch 14391/30000 Training Loss: 0.08886333554983139\n",
      "Epoch 14392/30000 Training Loss: 0.054286956787109375\n",
      "Epoch 14393/30000 Training Loss: 0.07622676342725754\n",
      "Epoch 14394/30000 Training Loss: 0.0835060253739357\n",
      "Epoch 14395/30000 Training Loss: 0.07687877863645554\n",
      "Epoch 14396/30000 Training Loss: 0.07553205639123917\n",
      "Epoch 14397/30000 Training Loss: 0.06826730817556381\n",
      "Epoch 14398/30000 Training Loss: 0.07848536223173141\n",
      "Epoch 14399/30000 Training Loss: 0.08484960347414017\n",
      "Epoch 14400/30000 Training Loss: 0.08225566893815994\n",
      "Epoch 14400/30000 Validation Loss: 0.07025537639856339\n",
      "Epoch 14401/30000 Training Loss: 0.0731373056769371\n",
      "Epoch 14402/30000 Training Loss: 0.10005555301904678\n",
      "Epoch 14403/30000 Training Loss: 0.07831075042486191\n",
      "Epoch 14404/30000 Training Loss: 0.08375941962003708\n",
      "Epoch 14405/30000 Training Loss: 0.08737508207559586\n",
      "Epoch 14406/30000 Training Loss: 0.08510076254606247\n",
      "Epoch 14407/30000 Training Loss: 0.09365644305944443\n",
      "Epoch 14408/30000 Training Loss: 0.053188055753707886\n",
      "Epoch 14409/30000 Training Loss: 0.08260083198547363\n",
      "Epoch 14410/30000 Training Loss: 0.06687114387750626\n",
      "Epoch 14410/30000 Validation Loss: 0.07090719789266586\n",
      "Epoch 14411/30000 Training Loss: 0.07459872961044312\n",
      "Epoch 14412/30000 Training Loss: 0.06589976698160172\n",
      "Epoch 14413/30000 Training Loss: 0.07497907429933548\n",
      "Epoch 14414/30000 Training Loss: 0.07549913972616196\n",
      "Epoch 14415/30000 Training Loss: 0.08553441613912582\n",
      "Epoch 14416/30000 Training Loss: 0.06017326936125755\n",
      "Epoch 14417/30000 Training Loss: 0.0876903235912323\n",
      "Epoch 14418/30000 Training Loss: 0.07362907379865646\n",
      "Epoch 14419/30000 Training Loss: 0.06510073691606522\n",
      "Epoch 14420/30000 Training Loss: 0.051328737288713455\n",
      "Epoch 14420/30000 Validation Loss: 0.07762470096349716\n",
      "Epoch 14421/30000 Training Loss: 0.07111456990242004\n",
      "Epoch 14422/30000 Training Loss: 0.06832186877727509\n",
      "Epoch 14423/30000 Training Loss: 0.06910223513841629\n",
      "Epoch 14424/30000 Training Loss: 0.08179333060979843\n",
      "Epoch 14425/30000 Training Loss: 0.08118748664855957\n",
      "Epoch 14426/30000 Training Loss: 0.08185584098100662\n",
      "Epoch 14427/30000 Training Loss: 0.07104422897100449\n",
      "Epoch 14428/30000 Training Loss: 0.07339472323656082\n",
      "Epoch 14429/30000 Training Loss: 0.07878828793764114\n",
      "Epoch 14430/30000 Training Loss: 0.09062886238098145\n",
      "Epoch 14430/30000 Validation Loss: 0.08863681554794312\n",
      "Epoch 14431/30000 Training Loss: 0.06658360362052917\n",
      "Epoch 14432/30000 Training Loss: 0.06649450957775116\n",
      "Epoch 14433/30000 Training Loss: 0.07753275334835052\n",
      "Epoch 14434/30000 Training Loss: 0.05635325238108635\n",
      "Epoch 14435/30000 Training Loss: 0.07315608859062195\n",
      "Epoch 14436/30000 Training Loss: 0.0805627629160881\n",
      "Epoch 14437/30000 Training Loss: 0.08820346742868423\n",
      "Epoch 14438/30000 Training Loss: 0.07719963043928146\n",
      "Epoch 14439/30000 Training Loss: 0.0643380880355835\n",
      "Epoch 14440/30000 Training Loss: 0.09043388813734055\n",
      "Epoch 14440/30000 Validation Loss: 0.07321727275848389\n",
      "Epoch 14441/30000 Training Loss: 0.10192505270242691\n",
      "Epoch 14442/30000 Training Loss: 0.06296620517969131\n",
      "Epoch 14443/30000 Training Loss: 0.06718281656503677\n",
      "Epoch 14444/30000 Training Loss: 0.07293030619621277\n",
      "Epoch 14445/30000 Training Loss: 0.07443080842494965\n",
      "Epoch 14446/30000 Training Loss: 0.0622665099799633\n",
      "Epoch 14447/30000 Training Loss: 0.073396697640419\n",
      "Epoch 14448/30000 Training Loss: 0.06665325909852982\n",
      "Epoch 14449/30000 Training Loss: 0.07900481671094894\n",
      "Epoch 14450/30000 Training Loss: 0.0931946411728859\n",
      "Epoch 14450/30000 Validation Loss: 0.0899750217795372\n",
      "Epoch 14451/30000 Training Loss: 0.08182035386562347\n",
      "Epoch 14452/30000 Training Loss: 0.09167259186506271\n",
      "Epoch 14453/30000 Training Loss: 0.08192527294158936\n",
      "Epoch 14454/30000 Training Loss: 0.08771903067827225\n",
      "Epoch 14455/30000 Training Loss: 0.07697857916355133\n",
      "Epoch 14456/30000 Training Loss: 0.09176287800073624\n",
      "Epoch 14457/30000 Training Loss: 0.08406096696853638\n",
      "Epoch 14458/30000 Training Loss: 0.08026478439569473\n",
      "Epoch 14459/30000 Training Loss: 0.07446089386940002\n",
      "Epoch 14460/30000 Training Loss: 0.08606278151273727\n",
      "Epoch 14460/30000 Validation Loss: 0.0696677714586258\n",
      "Epoch 14461/30000 Training Loss: 0.0886896625161171\n",
      "Epoch 14462/30000 Training Loss: 0.09068766981363297\n",
      "Epoch 14463/30000 Training Loss: 0.06986812502145767\n",
      "Epoch 14464/30000 Training Loss: 0.08015354722738266\n",
      "Epoch 14465/30000 Training Loss: 0.07534370571374893\n",
      "Epoch 14466/30000 Training Loss: 0.08057736605405807\n",
      "Epoch 14467/30000 Training Loss: 0.08360075950622559\n",
      "Epoch 14468/30000 Training Loss: 0.06922747939825058\n",
      "Epoch 14469/30000 Training Loss: 0.0613933801651001\n",
      "Epoch 14470/30000 Training Loss: 0.08809769153594971\n",
      "Epoch 14470/30000 Validation Loss: 0.07319434732198715\n",
      "Epoch 14471/30000 Training Loss: 0.0972612276673317\n",
      "Epoch 14472/30000 Training Loss: 0.05506421998143196\n",
      "Epoch 14473/30000 Training Loss: 0.08274561911821365\n",
      "Epoch 14474/30000 Training Loss: 0.06351839005947113\n",
      "Epoch 14475/30000 Training Loss: 0.0785360261797905\n",
      "Epoch 14476/30000 Training Loss: 0.07961400598287582\n",
      "Epoch 14477/30000 Training Loss: 0.07403809577226639\n",
      "Epoch 14478/30000 Training Loss: 0.06548133492469788\n",
      "Epoch 14479/30000 Training Loss: 0.06645958125591278\n",
      "Epoch 14480/30000 Training Loss: 0.07578974217176437\n",
      "Epoch 14480/30000 Validation Loss: 0.08882585167884827\n",
      "Epoch 14481/30000 Training Loss: 0.05902360379695892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14482/30000 Training Loss: 0.08317292481660843\n",
      "Epoch 14483/30000 Training Loss: 0.06814049184322357\n",
      "Epoch 14484/30000 Training Loss: 0.08143078535795212\n",
      "Epoch 14485/30000 Training Loss: 0.07535674422979355\n",
      "Epoch 14486/30000 Training Loss: 0.08393862843513489\n",
      "Epoch 14487/30000 Training Loss: 0.06171141564846039\n",
      "Epoch 14488/30000 Training Loss: 0.06732947379350662\n",
      "Epoch 14489/30000 Training Loss: 0.08711952716112137\n",
      "Epoch 14490/30000 Training Loss: 0.06667792797088623\n",
      "Epoch 14490/30000 Validation Loss: 0.07280013710260391\n",
      "Epoch 14491/30000 Training Loss: 0.09978953003883362\n",
      "Epoch 14492/30000 Training Loss: 0.061642084270715714\n",
      "Epoch 14493/30000 Training Loss: 0.056985318660736084\n",
      "Epoch 14494/30000 Training Loss: 0.09176450222730637\n",
      "Epoch 14495/30000 Training Loss: 0.06657635420560837\n",
      "Epoch 14496/30000 Training Loss: 0.06949657201766968\n",
      "Epoch 14497/30000 Training Loss: 0.07216305285692215\n",
      "Epoch 14498/30000 Training Loss: 0.06785786896944046\n",
      "Epoch 14499/30000 Training Loss: 0.07394739240407944\n",
      "Epoch 14500/30000 Training Loss: 0.08850139379501343\n",
      "Epoch 14500/30000 Validation Loss: 0.07046479731798172\n",
      "Epoch 14501/30000 Training Loss: 0.09605433791875839\n",
      "Epoch 14502/30000 Training Loss: 0.09992396831512451\n",
      "Epoch 14503/30000 Training Loss: 0.06980844587087631\n",
      "Epoch 14504/30000 Training Loss: 0.07950636744499207\n",
      "Epoch 14505/30000 Training Loss: 0.07160159945487976\n",
      "Epoch 14506/30000 Training Loss: 0.07578928023576736\n",
      "Epoch 14507/30000 Training Loss: 0.07611071318387985\n",
      "Epoch 14508/30000 Training Loss: 0.07015147805213928\n",
      "Epoch 14509/30000 Training Loss: 0.07912582904100418\n",
      "Epoch 14510/30000 Training Loss: 0.07914858311414719\n",
      "Epoch 14510/30000 Validation Loss: 0.06742782145738602\n",
      "Epoch 14511/30000 Training Loss: 0.09277573972940445\n",
      "Epoch 14512/30000 Training Loss: 0.05551104620099068\n",
      "Epoch 14513/30000 Training Loss: 0.0889928862452507\n",
      "Epoch 14514/30000 Training Loss: 0.08644625544548035\n",
      "Epoch 14515/30000 Training Loss: 0.08024392277002335\n",
      "Epoch 14516/30000 Training Loss: 0.07480398565530777\n",
      "Epoch 14517/30000 Training Loss: 0.07185065746307373\n",
      "Epoch 14518/30000 Training Loss: 0.07184384018182755\n",
      "Epoch 14519/30000 Training Loss: 0.07425188273191452\n",
      "Epoch 14520/30000 Training Loss: 0.0784090980887413\n",
      "Epoch 14520/30000 Validation Loss: 0.08053071051836014\n",
      "Epoch 14521/30000 Training Loss: 0.0862327516078949\n",
      "Epoch 14522/30000 Training Loss: 0.058457642793655396\n",
      "Epoch 14523/30000 Training Loss: 0.08592741936445236\n",
      "Epoch 14524/30000 Training Loss: 0.07842010259628296\n",
      "Epoch 14525/30000 Training Loss: 0.09305957704782486\n",
      "Epoch 14526/30000 Training Loss: 0.06859125196933746\n",
      "Epoch 14527/30000 Training Loss: 0.08813297003507614\n",
      "Epoch 14528/30000 Training Loss: 0.0613425076007843\n",
      "Epoch 14529/30000 Training Loss: 0.07672413438558578\n",
      "Epoch 14530/30000 Training Loss: 0.07292404025793076\n",
      "Epoch 14530/30000 Validation Loss: 0.07150357216596603\n",
      "Epoch 14531/30000 Training Loss: 0.06899983435869217\n",
      "Epoch 14532/30000 Training Loss: 0.07120136171579361\n",
      "Epoch 14533/30000 Training Loss: 0.08188525587320328\n",
      "Epoch 14534/30000 Training Loss: 0.06949759274721146\n",
      "Epoch 14535/30000 Training Loss: 0.08942726254463196\n",
      "Epoch 14536/30000 Training Loss: 0.07376927882432938\n",
      "Epoch 14537/30000 Training Loss: 0.07340583950281143\n",
      "Epoch 14538/30000 Training Loss: 0.06638696789741516\n",
      "Epoch 14539/30000 Training Loss: 0.0607769675552845\n",
      "Epoch 14540/30000 Training Loss: 0.08026590198278427\n",
      "Epoch 14540/30000 Validation Loss: 0.05995745584368706\n",
      "Epoch 14541/30000 Training Loss: 0.0587163083255291\n",
      "Epoch 14542/30000 Training Loss: 0.06925301253795624\n",
      "Epoch 14543/30000 Training Loss: 0.09048900753259659\n",
      "Epoch 14544/30000 Training Loss: 0.06612888723611832\n",
      "Epoch 14545/30000 Training Loss: 0.07609667629003525\n",
      "Epoch 14546/30000 Training Loss: 0.07815691828727722\n",
      "Epoch 14547/30000 Training Loss: 0.11219734698534012\n",
      "Epoch 14548/30000 Training Loss: 0.06742411106824875\n",
      "Epoch 14549/30000 Training Loss: 0.07286841422319412\n",
      "Epoch 14550/30000 Training Loss: 0.07211484760046005\n",
      "Epoch 14550/30000 Validation Loss: 0.06370671838521957\n",
      "Epoch 14551/30000 Training Loss: 0.06800539046525955\n",
      "Epoch 14552/30000 Training Loss: 0.07429467886686325\n",
      "Epoch 14553/30000 Training Loss: 0.0641288161277771\n",
      "Epoch 14554/30000 Training Loss: 0.07806398719549179\n",
      "Epoch 14555/30000 Training Loss: 0.08321525901556015\n",
      "Epoch 14556/30000 Training Loss: 0.09261965751647949\n",
      "Epoch 14557/30000 Training Loss: 0.07726829499006271\n",
      "Epoch 14558/30000 Training Loss: 0.08124992996454239\n",
      "Epoch 14559/30000 Training Loss: 0.060181956738233566\n",
      "Epoch 14560/30000 Training Loss: 0.07654538750648499\n",
      "Epoch 14560/30000 Validation Loss: 0.07696377485990524\n",
      "Epoch 14561/30000 Training Loss: 0.07914206385612488\n",
      "Epoch 14562/30000 Training Loss: 0.0870957151055336\n",
      "Epoch 14563/30000 Training Loss: 0.081955187022686\n",
      "Epoch 14564/30000 Training Loss: 0.09755945205688477\n",
      "Epoch 14565/30000 Training Loss: 0.06381116062402725\n",
      "Epoch 14566/30000 Training Loss: 0.06698783487081528\n",
      "Epoch 14567/30000 Training Loss: 0.08829202502965927\n",
      "Epoch 14568/30000 Training Loss: 0.0765213593840599\n",
      "Epoch 14569/30000 Training Loss: 0.06965722888708115\n",
      "Epoch 14570/30000 Training Loss: 0.0884331464767456\n",
      "Epoch 14570/30000 Validation Loss: 0.06945011764764786\n",
      "Epoch 14571/30000 Training Loss: 0.10249290615320206\n",
      "Epoch 14572/30000 Training Loss: 0.08116749674081802\n",
      "Epoch 14573/30000 Training Loss: 0.06036793068051338\n",
      "Epoch 14574/30000 Training Loss: 0.06654715538024902\n",
      "Epoch 14575/30000 Training Loss: 0.05306883528828621\n",
      "Epoch 14576/30000 Training Loss: 0.06369435787200928\n",
      "Epoch 14577/30000 Training Loss: 0.0732945129275322\n",
      "Epoch 14578/30000 Training Loss: 0.06924919039011002\n",
      "Epoch 14579/30000 Training Loss: 0.07073461264371872\n",
      "Epoch 14580/30000 Training Loss: 0.07258319854736328\n",
      "Epoch 14580/30000 Validation Loss: 0.09271452575922012\n",
      "Epoch 14581/30000 Training Loss: 0.07230795174837112\n",
      "Epoch 14582/30000 Training Loss: 0.06380566209554672\n",
      "Epoch 14583/30000 Training Loss: 0.07316511869430542\n",
      "Epoch 14584/30000 Training Loss: 0.07202324271202087\n",
      "Epoch 14585/30000 Training Loss: 0.06326089054346085\n",
      "Epoch 14586/30000 Training Loss: 0.07578984647989273\n",
      "Epoch 14587/30000 Training Loss: 0.06917554885149002\n",
      "Epoch 14588/30000 Training Loss: 0.06746333837509155\n",
      "Epoch 14589/30000 Training Loss: 0.06572961062192917\n",
      "Epoch 14590/30000 Training Loss: 0.08802681416273117\n",
      "Epoch 14590/30000 Validation Loss: 0.07060138136148453\n",
      "Epoch 14591/30000 Training Loss: 0.08315949887037277\n",
      "Epoch 14592/30000 Training Loss: 0.09265878051519394\n",
      "Epoch 14593/30000 Training Loss: 0.06750308722257614\n",
      "Epoch 14594/30000 Training Loss: 0.08613625168800354\n",
      "Epoch 14595/30000 Training Loss: 0.07226578146219254\n",
      "Epoch 14596/30000 Training Loss: 0.07462317496538162\n",
      "Epoch 14597/30000 Training Loss: 0.05610169097781181\n",
      "Epoch 14598/30000 Training Loss: 0.07386194914579391\n",
      "Epoch 14599/30000 Training Loss: 0.07142100483179092\n",
      "Epoch 14600/30000 Training Loss: 0.09391536563634872\n",
      "Epoch 14600/30000 Validation Loss: 0.06533177196979523\n",
      "Epoch 14601/30000 Training Loss: 0.061869774013757706\n",
      "Epoch 14602/30000 Training Loss: 0.044568102806806564\n",
      "Epoch 14603/30000 Training Loss: 0.08360926061868668\n",
      "Epoch 14604/30000 Training Loss: 0.08405787497758865\n",
      "Epoch 14605/30000 Training Loss: 0.06693931668996811\n",
      "Epoch 14606/30000 Training Loss: 0.08093968033790588\n",
      "Epoch 14607/30000 Training Loss: 0.06504423171281815\n",
      "Epoch 14608/30000 Training Loss: 0.07714522629976273\n",
      "Epoch 14609/30000 Training Loss: 0.061283599585294724\n",
      "Epoch 14610/30000 Training Loss: 0.07095356285572052\n",
      "Epoch 14610/30000 Validation Loss: 0.0587434358894825\n",
      "Epoch 14611/30000 Training Loss: 0.06918162107467651\n",
      "Epoch 14612/30000 Training Loss: 0.05739571526646614\n",
      "Epoch 14613/30000 Training Loss: 0.08662975579500198\n",
      "Epoch 14614/30000 Training Loss: 0.05926141515374184\n",
      "Epoch 14615/30000 Training Loss: 0.08217671513557434\n",
      "Epoch 14616/30000 Training Loss: 0.07301565259695053\n",
      "Epoch 14617/30000 Training Loss: 0.07995661348104477\n",
      "Epoch 14618/30000 Training Loss: 0.06918022036552429\n",
      "Epoch 14619/30000 Training Loss: 0.07314104586839676\n",
      "Epoch 14620/30000 Training Loss: 0.07771408557891846\n",
      "Epoch 14620/30000 Validation Loss: 0.07121723145246506\n",
      "Epoch 14621/30000 Training Loss: 0.09404051303863525\n",
      "Epoch 14622/30000 Training Loss: 0.07044597715139389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14623/30000 Training Loss: 0.08001597970724106\n",
      "Epoch 14624/30000 Training Loss: 0.08985555917024612\n",
      "Epoch 14625/30000 Training Loss: 0.07262763381004333\n",
      "Epoch 14626/30000 Training Loss: 0.07841657847166061\n",
      "Epoch 14627/30000 Training Loss: 0.07856011390686035\n",
      "Epoch 14628/30000 Training Loss: 0.0656108558177948\n",
      "Epoch 14629/30000 Training Loss: 0.07039174437522888\n",
      "Epoch 14630/30000 Training Loss: 0.06381941586732864\n",
      "Epoch 14630/30000 Validation Loss: 0.066874660551548\n",
      "Epoch 14631/30000 Training Loss: 0.07062391191720963\n",
      "Epoch 14632/30000 Training Loss: 0.09056755900382996\n",
      "Epoch 14633/30000 Training Loss: 0.06351057440042496\n",
      "Epoch 14634/30000 Training Loss: 0.09290196746587753\n",
      "Epoch 14635/30000 Training Loss: 0.09136728197336197\n",
      "Epoch 14636/30000 Training Loss: 0.07577671110630035\n",
      "Epoch 14637/30000 Training Loss: 0.07167048007249832\n",
      "Epoch 14638/30000 Training Loss: 0.06765598058700562\n",
      "Epoch 14639/30000 Training Loss: 0.06887935847043991\n",
      "Epoch 14640/30000 Training Loss: 0.07838204503059387\n",
      "Epoch 14640/30000 Validation Loss: 0.06303903460502625\n",
      "Epoch 14641/30000 Training Loss: 0.06774859875440598\n",
      "Epoch 14642/30000 Training Loss: 0.06448154151439667\n",
      "Epoch 14643/30000 Training Loss: 0.07387625426054001\n",
      "Epoch 14644/30000 Training Loss: 0.06700552254915237\n",
      "Epoch 14645/30000 Training Loss: 0.06637907028198242\n",
      "Epoch 14646/30000 Training Loss: 0.08603180199861526\n",
      "Epoch 14647/30000 Training Loss: 0.0962231233716011\n",
      "Epoch 14648/30000 Training Loss: 0.07322754710912704\n",
      "Epoch 14649/30000 Training Loss: 0.10111591219902039\n",
      "Epoch 14650/30000 Training Loss: 0.07379931956529617\n",
      "Epoch 14650/30000 Validation Loss: 0.08169365674257278\n",
      "Epoch 14651/30000 Training Loss: 0.0597197525203228\n",
      "Epoch 14652/30000 Training Loss: 0.06804128736257553\n",
      "Epoch 14653/30000 Training Loss: 0.06370338052511215\n",
      "Epoch 14654/30000 Training Loss: 0.05847965553402901\n",
      "Epoch 14655/30000 Training Loss: 0.08149921894073486\n",
      "Epoch 14656/30000 Training Loss: 0.07928665727376938\n",
      "Epoch 14657/30000 Training Loss: 0.07928983867168427\n",
      "Epoch 14658/30000 Training Loss: 0.0726163238286972\n",
      "Epoch 14659/30000 Training Loss: 0.07859401404857635\n",
      "Epoch 14660/30000 Training Loss: 0.08532971888780594\n",
      "Epoch 14660/30000 Validation Loss: 0.0794190764427185\n",
      "Epoch 14661/30000 Training Loss: 0.059920985251665115\n",
      "Epoch 14662/30000 Training Loss: 0.07333728671073914\n",
      "Epoch 14663/30000 Training Loss: 0.08205028623342514\n",
      "Epoch 14664/30000 Training Loss: 0.09125008434057236\n",
      "Epoch 14665/30000 Training Loss: 0.08074548840522766\n",
      "Epoch 14666/30000 Training Loss: 0.056659553200006485\n",
      "Epoch 14667/30000 Training Loss: 0.06156407296657562\n",
      "Epoch 14668/30000 Training Loss: 0.05808744952082634\n",
      "Epoch 14669/30000 Training Loss: 0.06662685424089432\n",
      "Epoch 14670/30000 Training Loss: 0.05865098163485527\n",
      "Epoch 14670/30000 Validation Loss: 0.09854146093130112\n",
      "Epoch 14671/30000 Training Loss: 0.06392204761505127\n",
      "Epoch 14672/30000 Training Loss: 0.06353882700204849\n",
      "Epoch 14673/30000 Training Loss: 0.094824880361557\n",
      "Epoch 14674/30000 Training Loss: 0.08433756232261658\n",
      "Epoch 14675/30000 Training Loss: 0.06380046159029007\n",
      "Epoch 14676/30000 Training Loss: 0.07428832352161407\n",
      "Epoch 14677/30000 Training Loss: 0.06533645838499069\n",
      "Epoch 14678/30000 Training Loss: 0.0852321982383728\n",
      "Epoch 14679/30000 Training Loss: 0.07592292875051498\n",
      "Epoch 14680/30000 Training Loss: 0.08501575142145157\n",
      "Epoch 14680/30000 Validation Loss: 0.07807110995054245\n",
      "Epoch 14681/30000 Training Loss: 0.07687365263700485\n",
      "Epoch 14682/30000 Training Loss: 0.07210711389780045\n",
      "Epoch 14683/30000 Training Loss: 0.07396340370178223\n",
      "Epoch 14684/30000 Training Loss: 0.0793016254901886\n",
      "Epoch 14685/30000 Training Loss: 0.08103729039430618\n",
      "Epoch 14686/30000 Training Loss: 0.07403098791837692\n",
      "Epoch 14687/30000 Training Loss: 0.06531883031129837\n",
      "Epoch 14688/30000 Training Loss: 0.06885555386543274\n",
      "Epoch 14689/30000 Training Loss: 0.08170398324728012\n",
      "Epoch 14690/30000 Training Loss: 0.0833950862288475\n",
      "Epoch 14690/30000 Validation Loss: 0.06801781803369522\n",
      "Epoch 14691/30000 Training Loss: 0.06889767199754715\n",
      "Epoch 14692/30000 Training Loss: 0.08171668648719788\n",
      "Epoch 14693/30000 Training Loss: 0.07813209295272827\n",
      "Epoch 14694/30000 Training Loss: 0.0704844519495964\n",
      "Epoch 14695/30000 Training Loss: 0.08993902802467346\n",
      "Epoch 14696/30000 Training Loss: 0.07148875296115875\n",
      "Epoch 14697/30000 Training Loss: 0.07287030667066574\n",
      "Epoch 14698/30000 Training Loss: 0.08205301314592361\n",
      "Epoch 14699/30000 Training Loss: 0.06902307271957397\n",
      "Epoch 14700/30000 Training Loss: 0.05981345847249031\n",
      "Epoch 14700/30000 Validation Loss: 0.06720811873674393\n",
      "Epoch 14701/30000 Training Loss: 0.07397959381341934\n",
      "Epoch 14702/30000 Training Loss: 0.08621606975793839\n",
      "Epoch 14703/30000 Training Loss: 0.07998020201921463\n",
      "Epoch 14704/30000 Training Loss: 0.06437742710113525\n",
      "Epoch 14705/30000 Training Loss: 0.07628080248832703\n",
      "Epoch 14706/30000 Training Loss: 0.06787347048521042\n",
      "Epoch 14707/30000 Training Loss: 0.08014848083257675\n",
      "Epoch 14708/30000 Training Loss: 0.06773263216018677\n",
      "Epoch 14709/30000 Training Loss: 0.07593601942062378\n",
      "Epoch 14710/30000 Training Loss: 0.06559895724058151\n",
      "Epoch 14710/30000 Validation Loss: 0.08331260085105896\n",
      "Epoch 14711/30000 Training Loss: 0.07625037431716919\n",
      "Epoch 14712/30000 Training Loss: 0.0777672603726387\n",
      "Epoch 14713/30000 Training Loss: 0.08136811852455139\n",
      "Epoch 14714/30000 Training Loss: 0.08355165272951126\n",
      "Epoch 14715/30000 Training Loss: 0.060091644525527954\n",
      "Epoch 14716/30000 Training Loss: 0.06297147274017334\n",
      "Epoch 14717/30000 Training Loss: 0.0660141184926033\n",
      "Epoch 14718/30000 Training Loss: 0.07196290791034698\n",
      "Epoch 14719/30000 Training Loss: 0.06733161956071854\n",
      "Epoch 14720/30000 Training Loss: 0.08336333185434341\n",
      "Epoch 14720/30000 Validation Loss: 0.09337747097015381\n",
      "Epoch 14721/30000 Training Loss: 0.07187771052122116\n",
      "Epoch 14722/30000 Training Loss: 0.09316292405128479\n",
      "Epoch 14723/30000 Training Loss: 0.07683700323104858\n",
      "Epoch 14724/30000 Training Loss: 0.06556450575590134\n",
      "Epoch 14725/30000 Training Loss: 0.08426323533058167\n",
      "Epoch 14726/30000 Training Loss: 0.091436468064785\n",
      "Epoch 14727/30000 Training Loss: 0.07762826234102249\n",
      "Epoch 14728/30000 Training Loss: 0.0682080015540123\n",
      "Epoch 14729/30000 Training Loss: 0.07674390077590942\n",
      "Epoch 14730/30000 Training Loss: 0.07948865741491318\n",
      "Epoch 14730/30000 Validation Loss: 0.0878564640879631\n",
      "Epoch 14731/30000 Training Loss: 0.06160641834139824\n",
      "Epoch 14732/30000 Training Loss: 0.0734325647354126\n",
      "Epoch 14733/30000 Training Loss: 0.070099838078022\n",
      "Epoch 14734/30000 Training Loss: 0.07944143563508987\n",
      "Epoch 14735/30000 Training Loss: 0.07685764133930206\n",
      "Epoch 14736/30000 Training Loss: 0.058620650321245193\n",
      "Epoch 14737/30000 Training Loss: 0.055687278509140015\n",
      "Epoch 14738/30000 Training Loss: 0.07339224219322205\n",
      "Epoch 14739/30000 Training Loss: 0.06880488246679306\n",
      "Epoch 14740/30000 Training Loss: 0.07758820056915283\n",
      "Epoch 14740/30000 Validation Loss: 0.06370475888252258\n",
      "Epoch 14741/30000 Training Loss: 0.06756488233804703\n",
      "Epoch 14742/30000 Training Loss: 0.06514636427164078\n",
      "Epoch 14743/30000 Training Loss: 0.08961015939712524\n",
      "Epoch 14744/30000 Training Loss: 0.09305455535650253\n",
      "Epoch 14745/30000 Training Loss: 0.0663594976067543\n",
      "Epoch 14746/30000 Training Loss: 0.04691258445382118\n",
      "Epoch 14747/30000 Training Loss: 0.08722952753305435\n",
      "Epoch 14748/30000 Training Loss: 0.06910300254821777\n",
      "Epoch 14749/30000 Training Loss: 0.05630863830447197\n",
      "Epoch 14750/30000 Training Loss: 0.06731808930635452\n",
      "Epoch 14750/30000 Validation Loss: 0.061969250440597534\n",
      "Epoch 14751/30000 Training Loss: 0.08446293324232101\n",
      "Epoch 14752/30000 Training Loss: 0.059715792536735535\n",
      "Epoch 14753/30000 Training Loss: 0.07750129699707031\n",
      "Epoch 14754/30000 Training Loss: 0.06628543138504028\n",
      "Epoch 14755/30000 Training Loss: 0.06222282722592354\n",
      "Epoch 14756/30000 Training Loss: 0.09087395668029785\n",
      "Epoch 14757/30000 Training Loss: 0.07321106642484665\n",
      "Epoch 14758/30000 Training Loss: 0.0633404329419136\n",
      "Epoch 14759/30000 Training Loss: 0.061633262783288956\n",
      "Epoch 14760/30000 Training Loss: 0.0882539227604866\n",
      "Epoch 14760/30000 Validation Loss: 0.05853468179702759\n",
      "Epoch 14761/30000 Training Loss: 0.08108319342136383\n",
      "Epoch 14762/30000 Training Loss: 0.08456593006849289\n",
      "Epoch 14763/30000 Training Loss: 0.0630652904510498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14764/30000 Training Loss: 0.06117817386984825\n",
      "Epoch 14765/30000 Training Loss: 0.07569202780723572\n",
      "Epoch 14766/30000 Training Loss: 0.06920816749334335\n",
      "Epoch 14767/30000 Training Loss: 0.0744188129901886\n",
      "Epoch 14768/30000 Training Loss: 0.06765725463628769\n",
      "Epoch 14769/30000 Training Loss: 0.061633218079805374\n",
      "Epoch 14770/30000 Training Loss: 0.09658364206552505\n",
      "Epoch 14770/30000 Validation Loss: 0.09350574016571045\n",
      "Epoch 14771/30000 Training Loss: 0.07577631622552872\n",
      "Epoch 14772/30000 Training Loss: 0.0845402255654335\n",
      "Epoch 14773/30000 Training Loss: 0.07612452656030655\n",
      "Epoch 14774/30000 Training Loss: 0.06261089444160461\n",
      "Epoch 14775/30000 Training Loss: 0.08358407765626907\n",
      "Epoch 14776/30000 Training Loss: 0.07406020909547806\n",
      "Epoch 14777/30000 Training Loss: 0.08661732077598572\n",
      "Epoch 14778/30000 Training Loss: 0.08635791391134262\n",
      "Epoch 14779/30000 Training Loss: 0.07845982164144516\n",
      "Epoch 14780/30000 Training Loss: 0.08331678062677383\n",
      "Epoch 14780/30000 Validation Loss: 0.06328033655881882\n",
      "Epoch 14781/30000 Training Loss: 0.07435797899961472\n",
      "Epoch 14782/30000 Training Loss: 0.06960774213075638\n",
      "Epoch 14783/30000 Training Loss: 0.058808859437704086\n",
      "Epoch 14784/30000 Training Loss: 0.07229378074407578\n",
      "Epoch 14785/30000 Training Loss: 0.07097483426332474\n",
      "Epoch 14786/30000 Training Loss: 0.089381642639637\n",
      "Epoch 14787/30000 Training Loss: 0.07768166810274124\n",
      "Epoch 14788/30000 Training Loss: 0.07010152190923691\n",
      "Epoch 14789/30000 Training Loss: 0.0822700634598732\n",
      "Epoch 14790/30000 Training Loss: 0.06869062781333923\n",
      "Epoch 14790/30000 Validation Loss: 0.07311362028121948\n",
      "Epoch 14791/30000 Training Loss: 0.0919279232621193\n",
      "Epoch 14792/30000 Training Loss: 0.07477759569883347\n",
      "Epoch 14793/30000 Training Loss: 0.0631394311785698\n",
      "Epoch 14794/30000 Training Loss: 0.0658693015575409\n",
      "Epoch 14795/30000 Training Loss: 0.07449579983949661\n",
      "Epoch 14796/30000 Training Loss: 0.0716809406876564\n",
      "Epoch 14797/30000 Training Loss: 0.10564351081848145\n",
      "Epoch 14798/30000 Training Loss: 0.0804513692855835\n",
      "Epoch 14799/30000 Training Loss: 0.07951844483613968\n",
      "Epoch 14800/30000 Training Loss: 0.07475681602954865\n",
      "Epoch 14800/30000 Validation Loss: 0.07150011509656906\n",
      "Epoch 14801/30000 Training Loss: 0.07253173738718033\n",
      "Epoch 14802/30000 Training Loss: 0.06821294873952866\n",
      "Epoch 14803/30000 Training Loss: 0.10761169344186783\n",
      "Epoch 14804/30000 Training Loss: 0.06830259412527084\n",
      "Epoch 14805/30000 Training Loss: 0.07953787595033646\n",
      "Epoch 14806/30000 Training Loss: 0.05930846929550171\n",
      "Epoch 14807/30000 Training Loss: 0.06170333921909332\n",
      "Epoch 14808/30000 Training Loss: 0.0749586671590805\n",
      "Epoch 14809/30000 Training Loss: 0.09467748552560806\n",
      "Epoch 14810/30000 Training Loss: 0.07720199972391129\n",
      "Epoch 14810/30000 Validation Loss: 0.09168491512537003\n",
      "Epoch 14811/30000 Training Loss: 0.06323801726102829\n",
      "Epoch 14812/30000 Training Loss: 0.0805436223745346\n",
      "Epoch 14813/30000 Training Loss: 0.07776123285293579\n",
      "Epoch 14814/30000 Training Loss: 0.06399073451757431\n",
      "Epoch 14815/30000 Training Loss: 0.07134052366018295\n",
      "Epoch 14816/30000 Training Loss: 0.05665149167180061\n",
      "Epoch 14817/30000 Training Loss: 0.07949735969305038\n",
      "Epoch 14818/30000 Training Loss: 0.08281750977039337\n",
      "Epoch 14819/30000 Training Loss: 0.06439337879419327\n",
      "Epoch 14820/30000 Training Loss: 0.07315785437822342\n",
      "Epoch 14820/30000 Validation Loss: 0.06623651832342148\n",
      "Epoch 14821/30000 Training Loss: 0.090315081179142\n",
      "Epoch 14822/30000 Training Loss: 0.07797182351350784\n",
      "Epoch 14823/30000 Training Loss: 0.05743764713406563\n",
      "Epoch 14824/30000 Training Loss: 0.0859159454703331\n",
      "Epoch 14825/30000 Training Loss: 0.09049966931343079\n",
      "Epoch 14826/30000 Training Loss: 0.11784633249044418\n",
      "Epoch 14827/30000 Training Loss: 0.08432906121015549\n",
      "Epoch 14828/30000 Training Loss: 0.06604430079460144\n",
      "Epoch 14829/30000 Training Loss: 0.06320980191230774\n",
      "Epoch 14830/30000 Training Loss: 0.08065285533666611\n",
      "Epoch 14830/30000 Validation Loss: 0.06650909781455994\n",
      "Epoch 14831/30000 Training Loss: 0.07432786375284195\n",
      "Epoch 14832/30000 Training Loss: 0.07561903446912766\n",
      "Epoch 14833/30000 Training Loss: 0.05856826901435852\n",
      "Epoch 14834/30000 Training Loss: 0.07346615940332413\n",
      "Epoch 14835/30000 Training Loss: 0.07080774754285812\n",
      "Epoch 14836/30000 Training Loss: 0.08632776886224747\n",
      "Epoch 14837/30000 Training Loss: 0.05773760750889778\n",
      "Epoch 14838/30000 Training Loss: 0.06364127993583679\n",
      "Epoch 14839/30000 Training Loss: 0.06958078593015671\n",
      "Epoch 14840/30000 Training Loss: 0.08292857557535172\n",
      "Epoch 14840/30000 Validation Loss: 0.06514209508895874\n",
      "Epoch 14841/30000 Training Loss: 0.0785827562212944\n",
      "Epoch 14842/30000 Training Loss: 0.06497014313936234\n",
      "Epoch 14843/30000 Training Loss: 0.07922666519880295\n",
      "Epoch 14844/30000 Training Loss: 0.08800283074378967\n",
      "Epoch 14845/30000 Training Loss: 0.07240358740091324\n",
      "Epoch 14846/30000 Training Loss: 0.07908204197883606\n",
      "Epoch 14847/30000 Training Loss: 0.07525786012411118\n",
      "Epoch 14848/30000 Training Loss: 0.07360003143548965\n",
      "Epoch 14849/30000 Training Loss: 0.07583817839622498\n",
      "Epoch 14850/30000 Training Loss: 0.07413437962532043\n",
      "Epoch 14850/30000 Validation Loss: 0.06566137075424194\n",
      "Epoch 14851/30000 Training Loss: 0.061916012316942215\n",
      "Epoch 14852/30000 Training Loss: 0.08558636158704758\n",
      "Epoch 14853/30000 Training Loss: 0.085274338722229\n",
      "Epoch 14854/30000 Training Loss: 0.0616244412958622\n",
      "Epoch 14855/30000 Training Loss: 0.06741201132535934\n",
      "Epoch 14856/30000 Training Loss: 0.07356444746255875\n",
      "Epoch 14857/30000 Training Loss: 0.06857500970363617\n",
      "Epoch 14858/30000 Training Loss: 0.06303489208221436\n",
      "Epoch 14859/30000 Training Loss: 0.06178843975067139\n",
      "Epoch 14860/30000 Training Loss: 0.07626130431890488\n",
      "Epoch 14860/30000 Validation Loss: 0.077677883207798\n",
      "Epoch 14861/30000 Training Loss: 0.08889132738113403\n",
      "Epoch 14862/30000 Training Loss: 0.08601530641317368\n",
      "Epoch 14863/30000 Training Loss: 0.07318157702684402\n",
      "Epoch 14864/30000 Training Loss: 0.06878077983856201\n",
      "Epoch 14865/30000 Training Loss: 0.06311850994825363\n",
      "Epoch 14866/30000 Training Loss: 0.05995045229792595\n",
      "Epoch 14867/30000 Training Loss: 0.07828564196825027\n",
      "Epoch 14868/30000 Training Loss: 0.06613845378160477\n",
      "Epoch 14869/30000 Training Loss: 0.06397972255945206\n",
      "Epoch 14870/30000 Training Loss: 0.08359185606241226\n",
      "Epoch 14870/30000 Validation Loss: 0.06617002189159393\n",
      "Epoch 14871/30000 Training Loss: 0.057362888008356094\n",
      "Epoch 14872/30000 Training Loss: 0.08651549369096756\n",
      "Epoch 14873/30000 Training Loss: 0.0839158371090889\n",
      "Epoch 14874/30000 Training Loss: 0.06084144487977028\n",
      "Epoch 14875/30000 Training Loss: 0.06352581828832626\n",
      "Epoch 14876/30000 Training Loss: 0.08786675333976746\n",
      "Epoch 14877/30000 Training Loss: 0.08621871471405029\n",
      "Epoch 14878/30000 Training Loss: 0.08028638362884521\n",
      "Epoch 14879/30000 Training Loss: 0.08190468698740005\n",
      "Epoch 14880/30000 Training Loss: 0.06667070835828781\n",
      "Epoch 14880/30000 Validation Loss: 0.0871024951338768\n",
      "Epoch 14881/30000 Training Loss: 0.07167909294366837\n",
      "Epoch 14882/30000 Training Loss: 0.068646639585495\n",
      "Epoch 14883/30000 Training Loss: 0.059188127517700195\n",
      "Epoch 14884/30000 Training Loss: 0.07459449023008347\n",
      "Epoch 14885/30000 Training Loss: 0.0849374309182167\n",
      "Epoch 14886/30000 Training Loss: 0.08506795763969421\n",
      "Epoch 14887/30000 Training Loss: 0.09564688056707382\n",
      "Epoch 14888/30000 Training Loss: 0.06279106438159943\n",
      "Epoch 14889/30000 Training Loss: 0.0673382356762886\n",
      "Epoch 14890/30000 Training Loss: 0.07584156841039658\n",
      "Epoch 14890/30000 Validation Loss: 0.06721772998571396\n",
      "Epoch 14891/30000 Training Loss: 0.08297000080347061\n",
      "Epoch 14892/30000 Training Loss: 0.09629600495100021\n",
      "Epoch 14893/30000 Training Loss: 0.0752025842666626\n",
      "Epoch 14894/30000 Training Loss: 0.08900492638349533\n",
      "Epoch 14895/30000 Training Loss: 0.08351564407348633\n",
      "Epoch 14896/30000 Training Loss: 0.05747367814183235\n",
      "Epoch 14897/30000 Training Loss: 0.08874428272247314\n",
      "Epoch 14898/30000 Training Loss: 0.0830715075135231\n",
      "Epoch 14899/30000 Training Loss: 0.07395649701356888\n",
      "Epoch 14900/30000 Training Loss: 0.0714772567152977\n",
      "Epoch 14900/30000 Validation Loss: 0.06799864023923874\n",
      "Epoch 14901/30000 Training Loss: 0.07760255038738251\n",
      "Epoch 14902/30000 Training Loss: 0.07670459151268005\n",
      "Epoch 14903/30000 Training Loss: 0.09592374414205551\n",
      "Epoch 14904/30000 Training Loss: 0.0910395160317421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14905/30000 Training Loss: 0.07296732813119888\n",
      "Epoch 14906/30000 Training Loss: 0.06867464631795883\n",
      "Epoch 14907/30000 Training Loss: 0.07079681009054184\n",
      "Epoch 14908/30000 Training Loss: 0.06430566310882568\n",
      "Epoch 14909/30000 Training Loss: 0.0742945745587349\n",
      "Epoch 14910/30000 Training Loss: 0.07817646861076355\n",
      "Epoch 14910/30000 Validation Loss: 0.07203368097543716\n",
      "Epoch 14911/30000 Training Loss: 0.06392794102430344\n",
      "Epoch 14912/30000 Training Loss: 0.07105126231908798\n",
      "Epoch 14913/30000 Training Loss: 0.06347108632326126\n",
      "Epoch 14914/30000 Training Loss: 0.07153857499361038\n",
      "Epoch 14915/30000 Training Loss: 0.07513194531202316\n",
      "Epoch 14916/30000 Training Loss: 0.07415632158517838\n",
      "Epoch 14917/30000 Training Loss: 0.06482553482055664\n",
      "Epoch 14918/30000 Training Loss: 0.07768122106790543\n",
      "Epoch 14919/30000 Training Loss: 0.06700136512517929\n",
      "Epoch 14920/30000 Training Loss: 0.07809010148048401\n",
      "Epoch 14920/30000 Validation Loss: 0.0775548443198204\n",
      "Epoch 14921/30000 Training Loss: 0.0866033211350441\n",
      "Epoch 14922/30000 Training Loss: 0.09790842980146408\n",
      "Epoch 14923/30000 Training Loss: 0.07533266395330429\n",
      "Epoch 14924/30000 Training Loss: 0.09223779290914536\n",
      "Epoch 14925/30000 Training Loss: 0.0665089339017868\n",
      "Epoch 14926/30000 Training Loss: 0.07847366482019424\n",
      "Epoch 14927/30000 Training Loss: 0.08028242737054825\n",
      "Epoch 14928/30000 Training Loss: 0.06883946061134338\n",
      "Epoch 14929/30000 Training Loss: 0.0846865102648735\n",
      "Epoch 14930/30000 Training Loss: 0.07569929957389832\n",
      "Epoch 14930/30000 Validation Loss: 0.09786025434732437\n",
      "Epoch 14931/30000 Training Loss: 0.07131248712539673\n",
      "Epoch 14932/30000 Training Loss: 0.07578133791685104\n",
      "Epoch 14933/30000 Training Loss: 0.0683104544878006\n",
      "Epoch 14934/30000 Training Loss: 0.0695919394493103\n",
      "Epoch 14935/30000 Training Loss: 0.07204463332891464\n",
      "Epoch 14936/30000 Training Loss: 0.06957357376813889\n",
      "Epoch 14937/30000 Training Loss: 0.08125665783882141\n",
      "Epoch 14938/30000 Training Loss: 0.06832931935787201\n",
      "Epoch 14939/30000 Training Loss: 0.06683212518692017\n",
      "Epoch 14940/30000 Training Loss: 0.06026138737797737\n",
      "Epoch 14940/30000 Validation Loss: 0.078631691634655\n",
      "Epoch 14941/30000 Training Loss: 0.08600016683340073\n",
      "Epoch 14942/30000 Training Loss: 0.07469379901885986\n",
      "Epoch 14943/30000 Training Loss: 0.07249622046947479\n",
      "Epoch 14944/30000 Training Loss: 0.06796743720769882\n",
      "Epoch 14945/30000 Training Loss: 0.07812348008155823\n",
      "Epoch 14946/30000 Training Loss: 0.07760780304670334\n",
      "Epoch 14947/30000 Training Loss: 0.09280625730752945\n",
      "Epoch 14948/30000 Training Loss: 0.07108984142541885\n",
      "Epoch 14949/30000 Training Loss: 0.07663393765687943\n",
      "Epoch 14950/30000 Training Loss: 0.07062963396310806\n",
      "Epoch 14950/30000 Validation Loss: 0.08038097620010376\n",
      "Epoch 14951/30000 Training Loss: 0.06374651938676834\n",
      "Epoch 14952/30000 Training Loss: 0.06417873501777649\n",
      "Epoch 14953/30000 Training Loss: 0.07625927031040192\n",
      "Epoch 14954/30000 Training Loss: 0.0886528491973877\n",
      "Epoch 14955/30000 Training Loss: 0.07825398445129395\n",
      "Epoch 14956/30000 Training Loss: 0.0790860652923584\n",
      "Epoch 14957/30000 Training Loss: 0.07860193401575089\n",
      "Epoch 14958/30000 Training Loss: 0.09033969789743423\n",
      "Epoch 14959/30000 Training Loss: 0.0781654417514801\n",
      "Epoch 14960/30000 Training Loss: 0.06134830787777901\n",
      "Epoch 14960/30000 Validation Loss: 0.07567156106233597\n",
      "Epoch 14961/30000 Training Loss: 0.06561701744794846\n",
      "Epoch 14962/30000 Training Loss: 0.07461010664701462\n",
      "Epoch 14963/30000 Training Loss: 0.055857669562101364\n",
      "Epoch 14964/30000 Training Loss: 0.08805950731039047\n",
      "Epoch 14965/30000 Training Loss: 0.06239980086684227\n",
      "Epoch 14966/30000 Training Loss: 0.06614620238542557\n",
      "Epoch 14967/30000 Training Loss: 0.06584527343511581\n",
      "Epoch 14968/30000 Training Loss: 0.08259674906730652\n",
      "Epoch 14969/30000 Training Loss: 0.07514233142137527\n",
      "Epoch 14970/30000 Training Loss: 0.08762692660093307\n",
      "Epoch 14970/30000 Validation Loss: 0.07273714989423752\n",
      "Epoch 14971/30000 Training Loss: 0.08092960715293884\n",
      "Epoch 14972/30000 Training Loss: 0.09155074506998062\n",
      "Epoch 14973/30000 Training Loss: 0.063718281686306\n",
      "Epoch 14974/30000 Training Loss: 0.0699784979224205\n",
      "Epoch 14975/30000 Training Loss: 0.07517334818840027\n",
      "Epoch 14976/30000 Training Loss: 0.07317811995744705\n",
      "Epoch 14977/30000 Training Loss: 0.060802411288022995\n",
      "Epoch 14978/30000 Training Loss: 0.07402769476175308\n",
      "Epoch 14979/30000 Training Loss: 0.06822387129068375\n",
      "Epoch 14980/30000 Training Loss: 0.08570299297571182\n",
      "Epoch 14980/30000 Validation Loss: 0.08288908004760742\n",
      "Epoch 14981/30000 Training Loss: 0.08170235902070999\n",
      "Epoch 14982/30000 Training Loss: 0.09656164795160294\n",
      "Epoch 14983/30000 Training Loss: 0.08776014298200607\n",
      "Epoch 14984/30000 Training Loss: 0.07155628502368927\n",
      "Epoch 14985/30000 Training Loss: 0.08074682950973511\n",
      "Epoch 14986/30000 Training Loss: 0.07778675854206085\n",
      "Epoch 14987/30000 Training Loss: 0.0812273845076561\n",
      "Epoch 14988/30000 Training Loss: 0.09419789165258408\n",
      "Epoch 14989/30000 Training Loss: 0.09449351578950882\n",
      "Epoch 14990/30000 Training Loss: 0.09897148609161377\n",
      "Epoch 14990/30000 Validation Loss: 0.08310610800981522\n",
      "Epoch 14991/30000 Training Loss: 0.07306170463562012\n",
      "Epoch 14992/30000 Training Loss: 0.07024846225976944\n",
      "Epoch 14993/30000 Training Loss: 0.06691472977399826\n",
      "Epoch 14994/30000 Training Loss: 0.06633937358856201\n",
      "Epoch 14995/30000 Training Loss: 0.0898902639746666\n",
      "Epoch 14996/30000 Training Loss: 0.09698996692895889\n",
      "Epoch 14997/30000 Training Loss: 0.0745735615491867\n",
      "Epoch 14998/30000 Training Loss: 0.06990540027618408\n",
      "Epoch 14999/30000 Training Loss: 0.07362297177314758\n",
      "Epoch 15000/30000 Training Loss: 0.08065197616815567\n",
      "Epoch 15000/30000 Validation Loss: 0.06787052005529404\n",
      "Epoch 15001/30000 Training Loss: 0.09166970103979111\n",
      "Epoch 15002/30000 Training Loss: 0.06708725541830063\n",
      "Epoch 15003/30000 Training Loss: 0.08375652879476547\n",
      "Epoch 15004/30000 Training Loss: 0.07396309822797775\n",
      "Epoch 15005/30000 Training Loss: 0.059978414326906204\n",
      "Epoch 15006/30000 Training Loss: 0.0964706763625145\n",
      "Epoch 15007/30000 Training Loss: 0.05866335704922676\n",
      "Epoch 15008/30000 Training Loss: 0.06579089909791946\n",
      "Epoch 15009/30000 Training Loss: 0.06306477636098862\n",
      "Epoch 15010/30000 Training Loss: 0.08160990476608276\n",
      "Epoch 15010/30000 Validation Loss: 0.08445795625448227\n",
      "Epoch 15011/30000 Training Loss: 0.0761125311255455\n",
      "Epoch 15012/30000 Training Loss: 0.0917518362402916\n",
      "Epoch 15013/30000 Training Loss: 0.07233402132987976\n",
      "Epoch 15014/30000 Training Loss: 0.07640978693962097\n",
      "Epoch 15015/30000 Training Loss: 0.08399910479784012\n",
      "Epoch 15016/30000 Training Loss: 0.08906320482492447\n",
      "Epoch 15017/30000 Training Loss: 0.07028540968894958\n",
      "Epoch 15018/30000 Training Loss: 0.08391457796096802\n",
      "Epoch 15019/30000 Training Loss: 0.06853038817644119\n",
      "Epoch 15020/30000 Training Loss: 0.11759454011917114\n",
      "Epoch 15020/30000 Validation Loss: 0.0713312029838562\n",
      "Epoch 15021/30000 Training Loss: 0.07643827795982361\n",
      "Epoch 15022/30000 Training Loss: 0.06648475676774979\n",
      "Epoch 15023/30000 Training Loss: 0.07799021154642105\n",
      "Epoch 15024/30000 Training Loss: 0.08357097953557968\n",
      "Epoch 15025/30000 Training Loss: 0.08692381531000137\n",
      "Epoch 15026/30000 Training Loss: 0.08148910105228424\n",
      "Epoch 15027/30000 Training Loss: 0.07666914910078049\n",
      "Epoch 15028/30000 Training Loss: 0.08364786952733994\n",
      "Epoch 15029/30000 Training Loss: 0.067897267639637\n",
      "Epoch 15030/30000 Training Loss: 0.057343412190675735\n",
      "Epoch 15030/30000 Validation Loss: 0.10082081705331802\n",
      "Epoch 15031/30000 Training Loss: 0.08777035027742386\n",
      "Epoch 15032/30000 Training Loss: 0.06519188731908798\n",
      "Epoch 15033/30000 Training Loss: 0.09695279598236084\n",
      "Epoch 15034/30000 Training Loss: 0.058338265866041183\n",
      "Epoch 15035/30000 Training Loss: 0.07490792870521545\n",
      "Epoch 15036/30000 Training Loss: 0.05905674025416374\n",
      "Epoch 15037/30000 Training Loss: 0.06909316033124924\n",
      "Epoch 15038/30000 Training Loss: 0.0613630972802639\n",
      "Epoch 15039/30000 Training Loss: 0.07418771833181381\n",
      "Epoch 15040/30000 Training Loss: 0.08978990465402603\n",
      "Epoch 15040/30000 Validation Loss: 0.0689438059926033\n",
      "Epoch 15041/30000 Training Loss: 0.08031386882066727\n",
      "Epoch 15042/30000 Training Loss: 0.0650608167052269\n",
      "Epoch 15043/30000 Training Loss: 0.09831035137176514\n",
      "Epoch 15044/30000 Training Loss: 0.0716136172413826\n",
      "Epoch 15045/30000 Training Loss: 0.06667345017194748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15046/30000 Training Loss: 0.06617382168769836\n",
      "Epoch 15047/30000 Training Loss: 0.0613710843026638\n",
      "Epoch 15048/30000 Training Loss: 0.08727069944143295\n",
      "Epoch 15049/30000 Training Loss: 0.08087243884801865\n",
      "Epoch 15050/30000 Training Loss: 0.09138687700033188\n",
      "Epoch 15050/30000 Validation Loss: 0.07424397766590118\n",
      "Epoch 15051/30000 Training Loss: 0.0771566852927208\n",
      "Epoch 15052/30000 Training Loss: 0.0922478437423706\n",
      "Epoch 15053/30000 Training Loss: 0.07116501778364182\n",
      "Epoch 15054/30000 Training Loss: 0.08583968877792358\n",
      "Epoch 15055/30000 Training Loss: 0.08931661397218704\n",
      "Epoch 15056/30000 Training Loss: 0.08100498467683792\n",
      "Epoch 15057/30000 Training Loss: 0.07694339752197266\n",
      "Epoch 15058/30000 Training Loss: 0.06599368900060654\n",
      "Epoch 15059/30000 Training Loss: 0.07186956703662872\n",
      "Epoch 15060/30000 Training Loss: 0.07217809557914734\n",
      "Epoch 15060/30000 Validation Loss: 0.08197277039289474\n",
      "Epoch 15061/30000 Training Loss: 0.07535190135240555\n",
      "Epoch 15062/30000 Training Loss: 0.06391426175832748\n",
      "Epoch 15063/30000 Training Loss: 0.09043212980031967\n",
      "Epoch 15064/30000 Training Loss: 0.07940620183944702\n",
      "Epoch 15065/30000 Training Loss: 0.06672254204750061\n",
      "Epoch 15066/30000 Training Loss: 0.08436650037765503\n",
      "Epoch 15067/30000 Training Loss: 0.08153662830591202\n",
      "Epoch 15068/30000 Training Loss: 0.06545678526163101\n",
      "Epoch 15069/30000 Training Loss: 0.06648525595664978\n",
      "Epoch 15070/30000 Training Loss: 0.06269948929548264\n",
      "Epoch 15070/30000 Validation Loss: 0.06963358074426651\n",
      "Epoch 15071/30000 Training Loss: 0.08846702426671982\n",
      "Epoch 15072/30000 Training Loss: 0.0838548019528389\n",
      "Epoch 15073/30000 Training Loss: 0.06752186268568039\n",
      "Epoch 15074/30000 Training Loss: 0.08498332649469376\n",
      "Epoch 15075/30000 Training Loss: 0.0696374848484993\n",
      "Epoch 15076/30000 Training Loss: 0.08110275119543076\n",
      "Epoch 15077/30000 Training Loss: 0.09055422991514206\n",
      "Epoch 15078/30000 Training Loss: 0.06495165824890137\n",
      "Epoch 15079/30000 Training Loss: 0.06022593006491661\n",
      "Epoch 15080/30000 Training Loss: 0.07069988548755646\n",
      "Epoch 15080/30000 Validation Loss: 0.06997908651828766\n",
      "Epoch 15081/30000 Training Loss: 0.07062505930662155\n",
      "Epoch 15082/30000 Training Loss: 0.08436513692140579\n",
      "Epoch 15083/30000 Training Loss: 0.07100453972816467\n",
      "Epoch 15084/30000 Training Loss: 0.06375322490930557\n",
      "Epoch 15085/30000 Training Loss: 0.07652953267097473\n",
      "Epoch 15086/30000 Training Loss: 0.09143199771642685\n",
      "Epoch 15087/30000 Training Loss: 0.0996067002415657\n",
      "Epoch 15088/30000 Training Loss: 0.07095035165548325\n",
      "Epoch 15089/30000 Training Loss: 0.06472732871770859\n",
      "Epoch 15090/30000 Training Loss: 0.0711531862616539\n",
      "Epoch 15090/30000 Validation Loss: 0.0733235701918602\n",
      "Epoch 15091/30000 Training Loss: 0.06619373708963394\n",
      "Epoch 15092/30000 Training Loss: 0.08266424387693405\n",
      "Epoch 15093/30000 Training Loss: 0.09100911766290665\n",
      "Epoch 15094/30000 Training Loss: 0.07328251004219055\n",
      "Epoch 15095/30000 Training Loss: 0.07457364350557327\n",
      "Epoch 15096/30000 Training Loss: 0.07876405119895935\n",
      "Epoch 15097/30000 Training Loss: 0.07369095832109451\n",
      "Epoch 15098/30000 Training Loss: 0.08411980420351028\n",
      "Epoch 15099/30000 Training Loss: 0.07916036993265152\n",
      "Epoch 15100/30000 Training Loss: 0.11811045557260513\n",
      "Epoch 15100/30000 Validation Loss: 0.07055053114891052\n",
      "Epoch 15101/30000 Training Loss: 0.06939326971769333\n",
      "Epoch 15102/30000 Training Loss: 0.062307920306921005\n",
      "Epoch 15103/30000 Training Loss: 0.06612089276313782\n",
      "Epoch 15104/30000 Training Loss: 0.06236119940876961\n",
      "Epoch 15105/30000 Training Loss: 0.07111924141645432\n",
      "Epoch 15106/30000 Training Loss: 0.06737754493951797\n",
      "Epoch 15107/30000 Training Loss: 0.09159135818481445\n",
      "Epoch 15108/30000 Training Loss: 0.06527635455131531\n",
      "Epoch 15109/30000 Training Loss: 0.07802952080965042\n",
      "Epoch 15110/30000 Training Loss: 0.09153085201978683\n",
      "Epoch 15110/30000 Validation Loss: 0.08841194957494736\n",
      "Epoch 15111/30000 Training Loss: 0.08793439716100693\n",
      "Epoch 15112/30000 Training Loss: 0.05734147131443024\n",
      "Epoch 15113/30000 Training Loss: 0.07913994044065475\n",
      "Epoch 15114/30000 Training Loss: 0.06828182935714722\n",
      "Epoch 15115/30000 Training Loss: 0.05804194509983063\n",
      "Epoch 15116/30000 Training Loss: 0.06344165652990341\n",
      "Epoch 15117/30000 Training Loss: 0.09090603142976761\n",
      "Epoch 15118/30000 Training Loss: 0.07373776286840439\n",
      "Epoch 15119/30000 Training Loss: 0.07753589749336243\n",
      "Epoch 15120/30000 Training Loss: 0.07971056550741196\n",
      "Epoch 15120/30000 Validation Loss: 0.07765909284353256\n",
      "Epoch 15121/30000 Training Loss: 0.07827755808830261\n",
      "Epoch 15122/30000 Training Loss: 0.05983846262097359\n",
      "Epoch 15123/30000 Training Loss: 0.061356548219919205\n",
      "Epoch 15124/30000 Training Loss: 0.06080291047692299\n",
      "Epoch 15125/30000 Training Loss: 0.0618678443133831\n",
      "Epoch 15126/30000 Training Loss: 0.0757402777671814\n",
      "Epoch 15127/30000 Training Loss: 0.06782575696706772\n",
      "Epoch 15128/30000 Training Loss: 0.08104871958494186\n",
      "Epoch 15129/30000 Training Loss: 0.07175188511610031\n",
      "Epoch 15130/30000 Training Loss: 0.07582824677228928\n",
      "Epoch 15130/30000 Validation Loss: 0.06227286532521248\n",
      "Epoch 15131/30000 Training Loss: 0.06991683691740036\n",
      "Epoch 15132/30000 Training Loss: 0.06938429921865463\n",
      "Epoch 15133/30000 Training Loss: 0.0792633593082428\n",
      "Epoch 15134/30000 Training Loss: 0.07046718150377274\n",
      "Epoch 15135/30000 Training Loss: 0.06857464462518692\n",
      "Epoch 15136/30000 Training Loss: 0.06728585809469223\n",
      "Epoch 15137/30000 Training Loss: 0.08046026527881622\n",
      "Epoch 15138/30000 Training Loss: 0.07818178832530975\n",
      "Epoch 15139/30000 Training Loss: 0.08345221728086472\n",
      "Epoch 15140/30000 Training Loss: 0.0587555468082428\n",
      "Epoch 15140/30000 Validation Loss: 0.07986818999052048\n",
      "Epoch 15141/30000 Training Loss: 0.07798868417739868\n",
      "Epoch 15142/30000 Training Loss: 0.07142442464828491\n",
      "Epoch 15143/30000 Training Loss: 0.07341686636209488\n",
      "Epoch 15144/30000 Training Loss: 0.07850337773561478\n",
      "Epoch 15145/30000 Training Loss: 0.057898323982954025\n",
      "Epoch 15146/30000 Training Loss: 0.06994020193815231\n",
      "Epoch 15147/30000 Training Loss: 0.06424527615308762\n",
      "Epoch 15148/30000 Training Loss: 0.08380895107984543\n",
      "Epoch 15149/30000 Training Loss: 0.07399211078882217\n",
      "Epoch 15150/30000 Training Loss: 0.07719910144805908\n",
      "Epoch 15150/30000 Validation Loss: 0.08236749470233917\n",
      "Epoch 15151/30000 Training Loss: 0.07648959010839462\n",
      "Epoch 15152/30000 Training Loss: 0.06471895426511765\n",
      "Epoch 15153/30000 Training Loss: 0.06782940030097961\n",
      "Epoch 15154/30000 Training Loss: 0.07450958341360092\n",
      "Epoch 15155/30000 Training Loss: 0.06900671869516373\n",
      "Epoch 15156/30000 Training Loss: 0.07328378409147263\n",
      "Epoch 15157/30000 Training Loss: 0.06774598360061646\n",
      "Epoch 15158/30000 Training Loss: 0.06508058309555054\n",
      "Epoch 15159/30000 Training Loss: 0.05807417258620262\n",
      "Epoch 15160/30000 Training Loss: 0.09393402189016342\n",
      "Epoch 15160/30000 Validation Loss: 0.0883668065071106\n",
      "Epoch 15161/30000 Training Loss: 0.07889752835035324\n",
      "Epoch 15162/30000 Training Loss: 0.06779546290636063\n",
      "Epoch 15163/30000 Training Loss: 0.0655924379825592\n",
      "Epoch 15164/30000 Training Loss: 0.07999130338430405\n",
      "Epoch 15165/30000 Training Loss: 0.06883307546377182\n",
      "Epoch 15166/30000 Training Loss: 0.07439195364713669\n",
      "Epoch 15167/30000 Training Loss: 0.07891669869422913\n",
      "Epoch 15168/30000 Training Loss: 0.07041484862565994\n",
      "Epoch 15169/30000 Training Loss: 0.06329817324876785\n",
      "Epoch 15170/30000 Training Loss: 0.0639471486210823\n",
      "Epoch 15170/30000 Validation Loss: 0.0886513814330101\n",
      "Epoch 15171/30000 Training Loss: 0.0806550458073616\n",
      "Epoch 15172/30000 Training Loss: 0.06309106200933456\n",
      "Epoch 15173/30000 Training Loss: 0.0658937320113182\n",
      "Epoch 15174/30000 Training Loss: 0.09511666744947433\n",
      "Epoch 15175/30000 Training Loss: 0.10177335888147354\n",
      "Epoch 15176/30000 Training Loss: 0.09081432223320007\n",
      "Epoch 15177/30000 Training Loss: 0.08154801279306412\n",
      "Epoch 15178/30000 Training Loss: 0.11886347085237503\n",
      "Epoch 15179/30000 Training Loss: 0.07219592481851578\n",
      "Epoch 15180/30000 Training Loss: 0.06606773287057877\n",
      "Epoch 15180/30000 Validation Loss: 0.0765535905957222\n",
      "Epoch 15181/30000 Training Loss: 0.06451890617609024\n",
      "Epoch 15182/30000 Training Loss: 0.06831244379281998\n",
      "Epoch 15183/30000 Training Loss: 0.10747852176427841\n",
      "Epoch 15184/30000 Training Loss: 0.06856013089418411\n",
      "Epoch 15185/30000 Training Loss: 0.07325074076652527\n",
      "Epoch 15186/30000 Training Loss: 0.06933542340993881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15187/30000 Training Loss: 0.07762395590543747\n",
      "Epoch 15188/30000 Training Loss: 0.08906025439500809\n",
      "Epoch 15189/30000 Training Loss: 0.04687881097197533\n",
      "Epoch 15190/30000 Training Loss: 0.06648934632539749\n",
      "Epoch 15190/30000 Validation Loss: 0.07465096563100815\n",
      "Epoch 15191/30000 Training Loss: 0.08752714842557907\n",
      "Epoch 15192/30000 Training Loss: 0.07220743596553802\n",
      "Epoch 15193/30000 Training Loss: 0.08061059564352036\n",
      "Epoch 15194/30000 Training Loss: 0.06765570491552353\n",
      "Epoch 15195/30000 Training Loss: 0.07215151190757751\n",
      "Epoch 15196/30000 Training Loss: 0.07192221283912659\n",
      "Epoch 15197/30000 Training Loss: 0.08854928612709045\n",
      "Epoch 15198/30000 Training Loss: 0.06605979055166245\n",
      "Epoch 15199/30000 Training Loss: 0.07260768115520477\n",
      "Epoch 15200/30000 Training Loss: 0.07227432727813721\n",
      "Epoch 15200/30000 Validation Loss: 0.07282159477472305\n",
      "Epoch 15201/30000 Training Loss: 0.0641769990324974\n",
      "Epoch 15202/30000 Training Loss: 0.0696123018860817\n",
      "Epoch 15203/30000 Training Loss: 0.05916206166148186\n",
      "Epoch 15204/30000 Training Loss: 0.0782892107963562\n",
      "Epoch 15205/30000 Training Loss: 0.07478425651788712\n",
      "Epoch 15206/30000 Training Loss: 0.07863491773605347\n",
      "Epoch 15207/30000 Training Loss: 0.08526962995529175\n",
      "Epoch 15208/30000 Training Loss: 0.07824572175741196\n",
      "Epoch 15209/30000 Training Loss: 0.09441215544939041\n",
      "Epoch 15210/30000 Training Loss: 0.07219434529542923\n",
      "Epoch 15210/30000 Validation Loss: 0.10303705930709839\n",
      "Epoch 15211/30000 Training Loss: 0.09176228195428848\n",
      "Epoch 15212/30000 Training Loss: 0.06261597573757172\n",
      "Epoch 15213/30000 Training Loss: 0.0756029561161995\n",
      "Epoch 15214/30000 Training Loss: 0.07624674588441849\n",
      "Epoch 15215/30000 Training Loss: 0.08114740252494812\n",
      "Epoch 15216/30000 Training Loss: 0.06981263309717178\n",
      "Epoch 15217/30000 Training Loss: 0.06520114839076996\n",
      "Epoch 15218/30000 Training Loss: 0.0718938410282135\n",
      "Epoch 15219/30000 Training Loss: 0.05814763903617859\n",
      "Epoch 15220/30000 Training Loss: 0.06475260108709335\n",
      "Epoch 15220/30000 Validation Loss: 0.07421477884054184\n",
      "Epoch 15221/30000 Training Loss: 0.07043550163507462\n",
      "Epoch 15222/30000 Training Loss: 0.09629195183515549\n",
      "Epoch 15223/30000 Training Loss: 0.05455358698964119\n",
      "Epoch 15224/30000 Training Loss: 0.08311933279037476\n",
      "Epoch 15225/30000 Training Loss: 0.058063115924596786\n",
      "Epoch 15226/30000 Training Loss: 0.08115272969007492\n",
      "Epoch 15227/30000 Training Loss: 0.061206966638565063\n",
      "Epoch 15228/30000 Training Loss: 0.06679216772317886\n",
      "Epoch 15229/30000 Training Loss: 0.09133726358413696\n",
      "Epoch 15230/30000 Training Loss: 0.09569019079208374\n",
      "Epoch 15230/30000 Validation Loss: 0.10070724040269852\n",
      "Epoch 15231/30000 Training Loss: 0.10477954149246216\n",
      "Epoch 15232/30000 Training Loss: 0.08106250315904617\n",
      "Epoch 15233/30000 Training Loss: 0.08209329098463058\n",
      "Epoch 15234/30000 Training Loss: 0.0676112249493599\n",
      "Epoch 15235/30000 Training Loss: 0.07503806799650192\n",
      "Epoch 15236/30000 Training Loss: 0.07411786168813705\n",
      "Epoch 15237/30000 Training Loss: 0.08413784950971603\n",
      "Epoch 15238/30000 Training Loss: 0.07751387357711792\n",
      "Epoch 15239/30000 Training Loss: 0.07091441005468369\n",
      "Epoch 15240/30000 Training Loss: 0.08277944475412369\n",
      "Epoch 15240/30000 Validation Loss: 0.09080293029546738\n",
      "Epoch 15241/30000 Training Loss: 0.07911554723978043\n",
      "Epoch 15242/30000 Training Loss: 0.05775017663836479\n",
      "Epoch 15243/30000 Training Loss: 0.06791884452104568\n",
      "Epoch 15244/30000 Training Loss: 0.07534093409776688\n",
      "Epoch 15245/30000 Training Loss: 0.07771352678537369\n",
      "Epoch 15246/30000 Training Loss: 0.07837177067995071\n",
      "Epoch 15247/30000 Training Loss: 0.0664980411529541\n",
      "Epoch 15248/30000 Training Loss: 0.07571257650852203\n",
      "Epoch 15249/30000 Training Loss: 0.06331200152635574\n",
      "Epoch 15250/30000 Training Loss: 0.08633220195770264\n",
      "Epoch 15250/30000 Validation Loss: 0.06710904091596603\n",
      "Epoch 15251/30000 Training Loss: 0.07487838715314865\n",
      "Epoch 15252/30000 Training Loss: 0.05757841095328331\n",
      "Epoch 15253/30000 Training Loss: 0.06518843024969101\n",
      "Epoch 15254/30000 Training Loss: 0.0901227667927742\n",
      "Epoch 15255/30000 Training Loss: 0.07773718982934952\n",
      "Epoch 15256/30000 Training Loss: 0.069552481174469\n",
      "Epoch 15257/30000 Training Loss: 0.07564236968755722\n",
      "Epoch 15258/30000 Training Loss: 0.07888218760490417\n",
      "Epoch 15259/30000 Training Loss: 0.07648590952157974\n",
      "Epoch 15260/30000 Training Loss: 0.08569999784231186\n",
      "Epoch 15260/30000 Validation Loss: 0.08388181775808334\n",
      "Epoch 15261/30000 Training Loss: 0.08608686178922653\n",
      "Epoch 15262/30000 Training Loss: 0.06548052281141281\n",
      "Epoch 15263/30000 Training Loss: 0.0695115402340889\n",
      "Epoch 15264/30000 Training Loss: 0.08818745613098145\n",
      "Epoch 15265/30000 Training Loss: 0.0753118023276329\n",
      "Epoch 15266/30000 Training Loss: 0.08161047101020813\n",
      "Epoch 15267/30000 Training Loss: 0.06094213202595711\n",
      "Epoch 15268/30000 Training Loss: 0.09070565551519394\n",
      "Epoch 15269/30000 Training Loss: 0.07301697880029678\n",
      "Epoch 15270/30000 Training Loss: 0.07012968510389328\n",
      "Epoch 15270/30000 Validation Loss: 0.06369355320930481\n",
      "Epoch 15271/30000 Training Loss: 0.05459684506058693\n",
      "Epoch 15272/30000 Training Loss: 0.0721614733338356\n",
      "Epoch 15273/30000 Training Loss: 0.0737842246890068\n",
      "Epoch 15274/30000 Training Loss: 0.059475142508745193\n",
      "Epoch 15275/30000 Training Loss: 0.06280557066202164\n",
      "Epoch 15276/30000 Training Loss: 0.07139021158218384\n",
      "Epoch 15277/30000 Training Loss: 0.06596790999174118\n",
      "Epoch 15278/30000 Training Loss: 0.06345788389444351\n",
      "Epoch 15279/30000 Training Loss: 0.07137499749660492\n",
      "Epoch 15280/30000 Training Loss: 0.07758760452270508\n",
      "Epoch 15280/30000 Validation Loss: 0.05866704881191254\n",
      "Epoch 15281/30000 Training Loss: 0.07481565326452255\n",
      "Epoch 15282/30000 Training Loss: 0.08603189140558243\n",
      "Epoch 15283/30000 Training Loss: 0.08518821001052856\n",
      "Epoch 15284/30000 Training Loss: 0.08505737781524658\n",
      "Epoch 15285/30000 Training Loss: 0.08022182434797287\n",
      "Epoch 15286/30000 Training Loss: 0.07195788621902466\n",
      "Epoch 15287/30000 Training Loss: 0.07241108268499374\n",
      "Epoch 15288/30000 Training Loss: 0.10593274980783463\n",
      "Epoch 15289/30000 Training Loss: 0.08671072870492935\n",
      "Epoch 15290/30000 Training Loss: 0.07829421013593674\n",
      "Epoch 15290/30000 Validation Loss: 0.08488905429840088\n",
      "Epoch 15291/30000 Training Loss: 0.0748692974448204\n",
      "Epoch 15292/30000 Training Loss: 0.08778008073568344\n",
      "Epoch 15293/30000 Training Loss: 0.0675557553768158\n",
      "Epoch 15294/30000 Training Loss: 0.08564656227827072\n",
      "Epoch 15295/30000 Training Loss: 0.09510272741317749\n",
      "Epoch 15296/30000 Training Loss: 0.0669008418917656\n",
      "Epoch 15297/30000 Training Loss: 0.09831724315881729\n",
      "Epoch 15298/30000 Training Loss: 0.08189614862203598\n",
      "Epoch 15299/30000 Training Loss: 0.061666157096624374\n",
      "Epoch 15300/30000 Training Loss: 0.06831306964159012\n",
      "Epoch 15300/30000 Validation Loss: 0.08286254853010178\n",
      "Epoch 15301/30000 Training Loss: 0.08338936418294907\n",
      "Epoch 15302/30000 Training Loss: 0.0799122229218483\n",
      "Epoch 15303/30000 Training Loss: 0.0801474079489708\n",
      "Epoch 15304/30000 Training Loss: 0.06064673140645027\n",
      "Epoch 15305/30000 Training Loss: 0.06708088517189026\n",
      "Epoch 15306/30000 Training Loss: 0.09087610989809036\n",
      "Epoch 15307/30000 Training Loss: 0.08868438750505447\n",
      "Epoch 15308/30000 Training Loss: 0.08003593236207962\n",
      "Epoch 15309/30000 Training Loss: 0.07427994161844254\n",
      "Epoch 15310/30000 Training Loss: 0.09544763714075089\n",
      "Epoch 15310/30000 Validation Loss: 0.0758586898446083\n",
      "Epoch 15311/30000 Training Loss: 0.0830027237534523\n",
      "Epoch 15312/30000 Training Loss: 0.0760723128914833\n",
      "Epoch 15313/30000 Training Loss: 0.09731616824865341\n",
      "Epoch 15314/30000 Training Loss: 0.06700827926397324\n",
      "Epoch 15315/30000 Training Loss: 0.07479768991470337\n",
      "Epoch 15316/30000 Training Loss: 0.0971747413277626\n",
      "Epoch 15317/30000 Training Loss: 0.07242197543382645\n",
      "Epoch 15318/30000 Training Loss: 0.06077511981129646\n",
      "Epoch 15319/30000 Training Loss: 0.07498602569103241\n",
      "Epoch 15320/30000 Training Loss: 0.07369785755872726\n",
      "Epoch 15320/30000 Validation Loss: 0.0592506118118763\n",
      "Epoch 15321/30000 Training Loss: 0.07968191802501678\n",
      "Epoch 15322/30000 Training Loss: 0.07019708305597305\n",
      "Epoch 15323/30000 Training Loss: 0.06398898363113403\n",
      "Epoch 15324/30000 Training Loss: 0.07433851808309555\n",
      "Epoch 15325/30000 Training Loss: 0.08809494227170944\n",
      "Epoch 15326/30000 Training Loss: 0.08276727795600891\n",
      "Epoch 15327/30000 Training Loss: 0.09675753116607666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15328/30000 Training Loss: 0.06210031732916832\n",
      "Epoch 15329/30000 Training Loss: 0.08176882565021515\n",
      "Epoch 15330/30000 Training Loss: 0.1048765555024147\n",
      "Epoch 15330/30000 Validation Loss: 0.07296936959028244\n",
      "Epoch 15331/30000 Training Loss: 0.08913113921880722\n",
      "Epoch 15332/30000 Training Loss: 0.0710834264755249\n",
      "Epoch 15333/30000 Training Loss: 0.055374547839164734\n",
      "Epoch 15334/30000 Training Loss: 0.06890477985143661\n",
      "Epoch 15335/30000 Training Loss: 0.06886061280965805\n",
      "Epoch 15336/30000 Training Loss: 0.06304281204938889\n",
      "Epoch 15337/30000 Training Loss: 0.0941147729754448\n",
      "Epoch 15338/30000 Training Loss: 0.06557036191225052\n",
      "Epoch 15339/30000 Training Loss: 0.06763198226690292\n",
      "Epoch 15340/30000 Training Loss: 0.113919198513031\n",
      "Epoch 15340/30000 Validation Loss: 0.06525423377752304\n",
      "Epoch 15341/30000 Training Loss: 0.07461760193109512\n",
      "Epoch 15342/30000 Training Loss: 0.08283776789903641\n",
      "Epoch 15343/30000 Training Loss: 0.10123837739229202\n",
      "Epoch 15344/30000 Training Loss: 0.07348009943962097\n",
      "Epoch 15345/30000 Training Loss: 0.08374639600515366\n",
      "Epoch 15346/30000 Training Loss: 0.07072505354881287\n",
      "Epoch 15347/30000 Training Loss: 0.07090585678815842\n",
      "Epoch 15348/30000 Training Loss: 0.08736512064933777\n",
      "Epoch 15349/30000 Training Loss: 0.07581975311040878\n",
      "Epoch 15350/30000 Training Loss: 0.07110694795846939\n",
      "Epoch 15350/30000 Validation Loss: 0.07173808664083481\n",
      "Epoch 15351/30000 Training Loss: 0.0819678008556366\n",
      "Epoch 15352/30000 Training Loss: 0.08630534261465073\n",
      "Epoch 15353/30000 Training Loss: 0.0735335648059845\n",
      "Epoch 15354/30000 Training Loss: 0.07236049324274063\n",
      "Epoch 15355/30000 Training Loss: 0.06434565037488937\n",
      "Epoch 15356/30000 Training Loss: 0.054049864411354065\n",
      "Epoch 15357/30000 Training Loss: 0.06298016756772995\n",
      "Epoch 15358/30000 Training Loss: 0.08246030658483505\n",
      "Epoch 15359/30000 Training Loss: 0.08080419152975082\n",
      "Epoch 15360/30000 Training Loss: 0.07504274696111679\n",
      "Epoch 15360/30000 Validation Loss: 0.07319823652505875\n",
      "Epoch 15361/30000 Training Loss: 0.07698763161897659\n",
      "Epoch 15362/30000 Training Loss: 0.09561443328857422\n",
      "Epoch 15363/30000 Training Loss: 0.08097945898771286\n",
      "Epoch 15364/30000 Training Loss: 0.07432231307029724\n",
      "Epoch 15365/30000 Training Loss: 0.0783480629324913\n",
      "Epoch 15366/30000 Training Loss: 0.07464996725320816\n",
      "Epoch 15367/30000 Training Loss: 0.07296013087034225\n",
      "Epoch 15368/30000 Training Loss: 0.07155662775039673\n",
      "Epoch 15369/30000 Training Loss: 0.060537319630384445\n",
      "Epoch 15370/30000 Training Loss: 0.062296587973833084\n",
      "Epoch 15370/30000 Validation Loss: 0.08283708244562149\n",
      "Epoch 15371/30000 Training Loss: 0.08800531178712845\n",
      "Epoch 15372/30000 Training Loss: 0.06778448820114136\n",
      "Epoch 15373/30000 Training Loss: 0.07126060128211975\n",
      "Epoch 15374/30000 Training Loss: 0.08392208814620972\n",
      "Epoch 15375/30000 Training Loss: 0.07842492312192917\n",
      "Epoch 15376/30000 Training Loss: 0.07335112243890762\n",
      "Epoch 15377/30000 Training Loss: 0.06697898358106613\n",
      "Epoch 15378/30000 Training Loss: 0.06576405465602875\n",
      "Epoch 15379/30000 Training Loss: 0.06061449646949768\n",
      "Epoch 15380/30000 Training Loss: 0.08196112513542175\n",
      "Epoch 15380/30000 Validation Loss: 0.06779376417398453\n",
      "Epoch 15381/30000 Training Loss: 0.0842282772064209\n",
      "Epoch 15382/30000 Training Loss: 0.07823456078767776\n",
      "Epoch 15383/30000 Training Loss: 0.07133182138204575\n",
      "Epoch 15384/30000 Training Loss: 0.07414939254522324\n",
      "Epoch 15385/30000 Training Loss: 0.07243088632822037\n",
      "Epoch 15386/30000 Training Loss: 0.08485031127929688\n",
      "Epoch 15387/30000 Training Loss: 0.07181745767593384\n",
      "Epoch 15388/30000 Training Loss: 0.055098507553339005\n",
      "Epoch 15389/30000 Training Loss: 0.09538275003433228\n",
      "Epoch 15390/30000 Training Loss: 0.054111335426568985\n",
      "Epoch 15390/30000 Validation Loss: 0.06351829320192337\n",
      "Epoch 15391/30000 Training Loss: 0.06827637553215027\n",
      "Epoch 15392/30000 Training Loss: 0.08817693591117859\n",
      "Epoch 15393/30000 Training Loss: 0.06277088075876236\n",
      "Epoch 15394/30000 Training Loss: 0.10158737748861313\n",
      "Epoch 15395/30000 Training Loss: 0.06594037264585495\n",
      "Epoch 15396/30000 Training Loss: 0.05986781045794487\n",
      "Epoch 15397/30000 Training Loss: 0.07846894860267639\n",
      "Epoch 15398/30000 Training Loss: 0.09376463294029236\n",
      "Epoch 15399/30000 Training Loss: 0.07350941747426987\n",
      "Epoch 15400/30000 Training Loss: 0.09962750226259232\n",
      "Epoch 15400/30000 Validation Loss: 0.08851484209299088\n",
      "Epoch 15401/30000 Training Loss: 0.07482456415891647\n",
      "Epoch 15402/30000 Training Loss: 0.06439759582281113\n",
      "Epoch 15403/30000 Training Loss: 0.06589749455451965\n",
      "Epoch 15404/30000 Training Loss: 0.06995387375354767\n",
      "Epoch 15405/30000 Training Loss: 0.08987286686897278\n",
      "Epoch 15406/30000 Training Loss: 0.07105010747909546\n",
      "Epoch 15407/30000 Training Loss: 0.07377678900957108\n",
      "Epoch 15408/30000 Training Loss: 0.09086637943983078\n",
      "Epoch 15409/30000 Training Loss: 0.07679926604032516\n",
      "Epoch 15410/30000 Training Loss: 0.08660312741994858\n",
      "Epoch 15410/30000 Validation Loss: 0.06199002265930176\n",
      "Epoch 15411/30000 Training Loss: 0.06632141023874283\n",
      "Epoch 15412/30000 Training Loss: 0.09004714339971542\n",
      "Epoch 15413/30000 Training Loss: 0.07893827557563782\n",
      "Epoch 15414/30000 Training Loss: 0.07421872764825821\n",
      "Epoch 15415/30000 Training Loss: 0.09013012796640396\n",
      "Epoch 15416/30000 Training Loss: 0.0838504359126091\n",
      "Epoch 15417/30000 Training Loss: 0.07791584730148315\n",
      "Epoch 15418/30000 Training Loss: 0.05370998755097389\n",
      "Epoch 15419/30000 Training Loss: 0.08219548314809799\n",
      "Epoch 15420/30000 Training Loss: 0.06218685582280159\n",
      "Epoch 15420/30000 Validation Loss: 0.07235663384199142\n",
      "Epoch 15421/30000 Training Loss: 0.07765243202447891\n",
      "Epoch 15422/30000 Training Loss: 0.1002371683716774\n",
      "Epoch 15423/30000 Training Loss: 0.07480257004499435\n",
      "Epoch 15424/30000 Training Loss: 0.07379760593175888\n",
      "Epoch 15425/30000 Training Loss: 0.055405888706445694\n",
      "Epoch 15426/30000 Training Loss: 0.06168350204825401\n",
      "Epoch 15427/30000 Training Loss: 0.08370701223611832\n",
      "Epoch 15428/30000 Training Loss: 0.07895303517580032\n",
      "Epoch 15429/30000 Training Loss: 0.08400049805641174\n",
      "Epoch 15430/30000 Training Loss: 0.07152125239372253\n",
      "Epoch 15430/30000 Validation Loss: 0.08205985277891159\n",
      "Epoch 15431/30000 Training Loss: 0.06688455492258072\n",
      "Epoch 15432/30000 Training Loss: 0.0666876956820488\n",
      "Epoch 15433/30000 Training Loss: 0.07486032694578171\n",
      "Epoch 15434/30000 Training Loss: 0.0660238191485405\n",
      "Epoch 15435/30000 Training Loss: 0.07564914971590042\n",
      "Epoch 15436/30000 Training Loss: 0.08020627498626709\n",
      "Epoch 15437/30000 Training Loss: 0.0760427713394165\n",
      "Epoch 15438/30000 Training Loss: 0.0786367654800415\n",
      "Epoch 15439/30000 Training Loss: 0.06652765721082687\n",
      "Epoch 15440/30000 Training Loss: 0.07499120384454727\n",
      "Epoch 15440/30000 Validation Loss: 0.06288716942071915\n",
      "Epoch 15441/30000 Training Loss: 0.06252364069223404\n",
      "Epoch 15442/30000 Training Loss: 0.07366501539945602\n",
      "Epoch 15443/30000 Training Loss: 0.07382312417030334\n",
      "Epoch 15444/30000 Training Loss: 0.05877026915550232\n",
      "Epoch 15445/30000 Training Loss: 0.10048764944076538\n",
      "Epoch 15446/30000 Training Loss: 0.0800197422504425\n",
      "Epoch 15447/30000 Training Loss: 0.06298138946294785\n",
      "Epoch 15448/30000 Training Loss: 0.07604221254587173\n",
      "Epoch 15449/30000 Training Loss: 0.07524850219488144\n",
      "Epoch 15450/30000 Training Loss: 0.06833379715681076\n",
      "Epoch 15450/30000 Validation Loss: 0.07295310497283936\n",
      "Epoch 15451/30000 Training Loss: 0.07231172919273376\n",
      "Epoch 15452/30000 Training Loss: 0.08407556265592575\n",
      "Epoch 15453/30000 Training Loss: 0.06656979769468307\n",
      "Epoch 15454/30000 Training Loss: 0.09241795539855957\n",
      "Epoch 15455/30000 Training Loss: 0.0597895085811615\n",
      "Epoch 15456/30000 Training Loss: 0.08102661371231079\n",
      "Epoch 15457/30000 Training Loss: 0.08587712049484253\n",
      "Epoch 15458/30000 Training Loss: 0.06499898433685303\n",
      "Epoch 15459/30000 Training Loss: 0.07706072181463242\n",
      "Epoch 15460/30000 Training Loss: 0.0653822124004364\n",
      "Epoch 15460/30000 Validation Loss: 0.07274877279996872\n",
      "Epoch 15461/30000 Training Loss: 0.06565315276384354\n",
      "Epoch 15462/30000 Training Loss: 0.05554158613085747\n",
      "Epoch 15463/30000 Training Loss: 0.05800419673323631\n",
      "Epoch 15464/30000 Training Loss: 0.0643344521522522\n",
      "Epoch 15465/30000 Training Loss: 0.07995570451021194\n",
      "Epoch 15466/30000 Training Loss: 0.06800717860460281\n",
      "Epoch 15467/30000 Training Loss: 0.08407456427812576\n",
      "Epoch 15468/30000 Training Loss: 0.08358826488256454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15469/30000 Training Loss: 0.07842422276735306\n",
      "Epoch 15470/30000 Training Loss: 0.09809822589159012\n",
      "Epoch 15470/30000 Validation Loss: 0.08232513815164566\n",
      "Epoch 15471/30000 Training Loss: 0.0801532194018364\n",
      "Epoch 15472/30000 Training Loss: 0.07880408316850662\n",
      "Epoch 15473/30000 Training Loss: 0.06531558185815811\n",
      "Epoch 15474/30000 Training Loss: 0.061224937438964844\n",
      "Epoch 15475/30000 Training Loss: 0.07862446457147598\n",
      "Epoch 15476/30000 Training Loss: 0.07789161056280136\n",
      "Epoch 15477/30000 Training Loss: 0.07226032018661499\n",
      "Epoch 15478/30000 Training Loss: 0.06858956813812256\n",
      "Epoch 15479/30000 Training Loss: 0.07251346856355667\n",
      "Epoch 15480/30000 Training Loss: 0.07777903228998184\n",
      "Epoch 15480/30000 Validation Loss: 0.07731913775205612\n",
      "Epoch 15481/30000 Training Loss: 0.0640084370970726\n",
      "Epoch 15482/30000 Training Loss: 0.09676595777273178\n",
      "Epoch 15483/30000 Training Loss: 0.0837203860282898\n",
      "Epoch 15484/30000 Training Loss: 0.09461447596549988\n",
      "Epoch 15485/30000 Training Loss: 0.07667142897844315\n",
      "Epoch 15486/30000 Training Loss: 0.07270985841751099\n",
      "Epoch 15487/30000 Training Loss: 0.05619296059012413\n",
      "Epoch 15488/30000 Training Loss: 0.07645345479249954\n",
      "Epoch 15489/30000 Training Loss: 0.0859694704413414\n",
      "Epoch 15490/30000 Training Loss: 0.07927247136831284\n",
      "Epoch 15490/30000 Validation Loss: 0.06779307872056961\n",
      "Epoch 15491/30000 Training Loss: 0.07350582629442215\n",
      "Epoch 15492/30000 Training Loss: 0.09033357352018356\n",
      "Epoch 15493/30000 Training Loss: 0.0696733146905899\n",
      "Epoch 15494/30000 Training Loss: 0.07098740339279175\n",
      "Epoch 15495/30000 Training Loss: 0.08690771460533142\n",
      "Epoch 15496/30000 Training Loss: 0.052730854600667953\n",
      "Epoch 15497/30000 Training Loss: 0.0753321498632431\n",
      "Epoch 15498/30000 Training Loss: 0.08686204999685287\n",
      "Epoch 15499/30000 Training Loss: 0.08043595403432846\n",
      "Epoch 15500/30000 Training Loss: 0.07064900547266006\n",
      "Epoch 15500/30000 Validation Loss: 0.0588434636592865\n",
      "Epoch 15501/30000 Training Loss: 0.07510823756456375\n",
      "Epoch 15502/30000 Training Loss: 0.08335291594266891\n",
      "Epoch 15503/30000 Training Loss: 0.08454238623380661\n",
      "Epoch 15504/30000 Training Loss: 0.07542100548744202\n",
      "Epoch 15505/30000 Training Loss: 0.06520553678274155\n",
      "Epoch 15506/30000 Training Loss: 0.07841835170984268\n",
      "Epoch 15507/30000 Training Loss: 0.10157942771911621\n",
      "Epoch 15508/30000 Training Loss: 0.0949634239077568\n",
      "Epoch 15509/30000 Training Loss: 0.05606652423739433\n",
      "Epoch 15510/30000 Training Loss: 0.0734798014163971\n",
      "Epoch 15510/30000 Validation Loss: 0.0660923644900322\n",
      "Epoch 15511/30000 Training Loss: 0.07186765223741531\n",
      "Epoch 15512/30000 Training Loss: 0.08760722726583481\n",
      "Epoch 15513/30000 Training Loss: 0.05830061063170433\n",
      "Epoch 15514/30000 Training Loss: 0.07759479433298111\n",
      "Epoch 15515/30000 Training Loss: 0.05924972891807556\n",
      "Epoch 15516/30000 Training Loss: 0.08189722895622253\n",
      "Epoch 15517/30000 Training Loss: 0.07402236759662628\n",
      "Epoch 15518/30000 Training Loss: 0.08308040350675583\n",
      "Epoch 15519/30000 Training Loss: 0.07370206713676453\n",
      "Epoch 15520/30000 Training Loss: 0.07515544444322586\n",
      "Epoch 15520/30000 Validation Loss: 0.0836375430226326\n",
      "Epoch 15521/30000 Training Loss: 0.060227539390325546\n",
      "Epoch 15522/30000 Training Loss: 0.07199167460203171\n",
      "Epoch 15523/30000 Training Loss: 0.06192141771316528\n",
      "Epoch 15524/30000 Training Loss: 0.06646376103162766\n",
      "Epoch 15525/30000 Training Loss: 0.08122321963310242\n",
      "Epoch 15526/30000 Training Loss: 0.05896298214793205\n",
      "Epoch 15527/30000 Training Loss: 0.07095635682344437\n",
      "Epoch 15528/30000 Training Loss: 0.07816264778375626\n",
      "Epoch 15529/30000 Training Loss: 0.06861256808042526\n",
      "Epoch 15530/30000 Training Loss: 0.06794624775648117\n",
      "Epoch 15530/30000 Validation Loss: 0.07991666346788406\n",
      "Epoch 15531/30000 Training Loss: 0.04999056085944176\n",
      "Epoch 15532/30000 Training Loss: 0.06356464326381683\n",
      "Epoch 15533/30000 Training Loss: 0.09411174803972244\n",
      "Epoch 15534/30000 Training Loss: 0.07248512655496597\n",
      "Epoch 15535/30000 Training Loss: 0.0832981988787651\n",
      "Epoch 15536/30000 Training Loss: 0.08396259695291519\n",
      "Epoch 15537/30000 Training Loss: 0.08724025636911392\n",
      "Epoch 15538/30000 Training Loss: 0.0758993849158287\n",
      "Epoch 15539/30000 Training Loss: 0.06971926242113113\n",
      "Epoch 15540/30000 Training Loss: 0.08350943773984909\n",
      "Epoch 15540/30000 Validation Loss: 0.0764194205403328\n",
      "Epoch 15541/30000 Training Loss: 0.07860254496335983\n",
      "Epoch 15542/30000 Training Loss: 0.08192586898803711\n",
      "Epoch 15543/30000 Training Loss: 0.059053778648376465\n",
      "Epoch 15544/30000 Training Loss: 0.07450463622808456\n",
      "Epoch 15545/30000 Training Loss: 0.06549596786499023\n",
      "Epoch 15546/30000 Training Loss: 0.0635463073849678\n",
      "Epoch 15547/30000 Training Loss: 0.10890617966651917\n",
      "Epoch 15548/30000 Training Loss: 0.07681518793106079\n",
      "Epoch 15549/30000 Training Loss: 0.06643224507570267\n",
      "Epoch 15550/30000 Training Loss: 0.06422702223062515\n",
      "Epoch 15550/30000 Validation Loss: 0.05282682180404663\n",
      "Epoch 15551/30000 Training Loss: 0.09556376188993454\n",
      "Epoch 15552/30000 Training Loss: 0.06549308449029922\n",
      "Epoch 15553/30000 Training Loss: 0.05487430468201637\n",
      "Epoch 15554/30000 Training Loss: 0.08399034291505814\n",
      "Epoch 15555/30000 Training Loss: 0.07305437326431274\n",
      "Epoch 15556/30000 Training Loss: 0.0978970006108284\n",
      "Epoch 15557/30000 Training Loss: 0.08362073451280594\n",
      "Epoch 15558/30000 Training Loss: 0.06869611889123917\n",
      "Epoch 15559/30000 Training Loss: 0.0718952938914299\n",
      "Epoch 15560/30000 Training Loss: 0.06527213007211685\n",
      "Epoch 15560/30000 Validation Loss: 0.08882877230644226\n",
      "Epoch 15561/30000 Training Loss: 0.07057706266641617\n",
      "Epoch 15562/30000 Training Loss: 0.07504219561815262\n",
      "Epoch 15563/30000 Training Loss: 0.07076896727085114\n",
      "Epoch 15564/30000 Training Loss: 0.06079167127609253\n",
      "Epoch 15565/30000 Training Loss: 0.07667967677116394\n",
      "Epoch 15566/30000 Training Loss: 0.057198598980903625\n",
      "Epoch 15567/30000 Training Loss: 0.08466553688049316\n",
      "Epoch 15568/30000 Training Loss: 0.0761629268527031\n",
      "Epoch 15569/30000 Training Loss: 0.06707591563463211\n",
      "Epoch 15570/30000 Training Loss: 0.05854317173361778\n",
      "Epoch 15570/30000 Validation Loss: 0.07251689583063126\n",
      "Epoch 15571/30000 Training Loss: 0.10420999675989151\n",
      "Epoch 15572/30000 Training Loss: 0.06987708806991577\n",
      "Epoch 15573/30000 Training Loss: 0.0963151827454567\n",
      "Epoch 15574/30000 Training Loss: 0.07600099593400955\n",
      "Epoch 15575/30000 Training Loss: 0.07023777812719345\n",
      "Epoch 15576/30000 Training Loss: 0.07372985035181046\n",
      "Epoch 15577/30000 Training Loss: 0.07728811353445053\n",
      "Epoch 15578/30000 Training Loss: 0.06491304188966751\n",
      "Epoch 15579/30000 Training Loss: 0.08217280358076096\n",
      "Epoch 15580/30000 Training Loss: 0.06194518879055977\n",
      "Epoch 15580/30000 Validation Loss: 0.0680050477385521\n",
      "Epoch 15581/30000 Training Loss: 0.06663388013839722\n",
      "Epoch 15582/30000 Training Loss: 0.06131087243556976\n",
      "Epoch 15583/30000 Training Loss: 0.08709058165550232\n",
      "Epoch 15584/30000 Training Loss: 0.0650152936577797\n",
      "Epoch 15585/30000 Training Loss: 0.0667228028178215\n",
      "Epoch 15586/30000 Training Loss: 0.08496823161840439\n",
      "Epoch 15587/30000 Training Loss: 0.07342299818992615\n",
      "Epoch 15588/30000 Training Loss: 0.0614488311111927\n",
      "Epoch 15589/30000 Training Loss: 0.08087506145238876\n",
      "Epoch 15590/30000 Training Loss: 0.07112792134284973\n",
      "Epoch 15590/30000 Validation Loss: 0.06518415361642838\n",
      "Epoch 15591/30000 Training Loss: 0.0668429359793663\n",
      "Epoch 15592/30000 Training Loss: 0.06637623906135559\n",
      "Epoch 15593/30000 Training Loss: 0.08364783972501755\n",
      "Epoch 15594/30000 Training Loss: 0.08420941233634949\n",
      "Epoch 15595/30000 Training Loss: 0.06314554065465927\n",
      "Epoch 15596/30000 Training Loss: 0.08076002448797226\n",
      "Epoch 15597/30000 Training Loss: 0.06000056862831116\n",
      "Epoch 15598/30000 Training Loss: 0.07165344804525375\n",
      "Epoch 15599/30000 Training Loss: 0.07169393450021744\n",
      "Epoch 15600/30000 Training Loss: 0.07038187235593796\n",
      "Epoch 15600/30000 Validation Loss: 0.059216927736997604\n",
      "Epoch 15601/30000 Training Loss: 0.07331749051809311\n",
      "Epoch 15602/30000 Training Loss: 0.06747909635305405\n",
      "Epoch 15603/30000 Training Loss: 0.08485177904367447\n",
      "Epoch 15604/30000 Training Loss: 0.07376963645219803\n",
      "Epoch 15605/30000 Training Loss: 0.08859535306692123\n",
      "Epoch 15606/30000 Training Loss: 0.07394683361053467\n",
      "Epoch 15607/30000 Training Loss: 0.08415517210960388\n",
      "Epoch 15608/30000 Training Loss: 0.072285957634449\n",
      "Epoch 15609/30000 Training Loss: 0.07615778595209122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15610/30000 Training Loss: 0.11538920551538467\n",
      "Epoch 15610/30000 Validation Loss: 0.08857721090316772\n",
      "Epoch 15611/30000 Training Loss: 0.05927099287509918\n",
      "Epoch 15612/30000 Training Loss: 0.09963741153478622\n",
      "Epoch 15613/30000 Training Loss: 0.07899089902639389\n",
      "Epoch 15614/30000 Training Loss: 0.0807550773024559\n",
      "Epoch 15615/30000 Training Loss: 0.10261747986078262\n",
      "Epoch 15616/30000 Training Loss: 0.06897877156734467\n",
      "Epoch 15617/30000 Training Loss: 0.08004774898290634\n",
      "Epoch 15618/30000 Training Loss: 0.06431210041046143\n",
      "Epoch 15619/30000 Training Loss: 0.08434555679559708\n",
      "Epoch 15620/30000 Training Loss: 0.07564479112625122\n",
      "Epoch 15620/30000 Validation Loss: 0.07429549843072891\n",
      "Epoch 15621/30000 Training Loss: 0.06737038493156433\n",
      "Epoch 15622/30000 Training Loss: 0.07885883003473282\n",
      "Epoch 15623/30000 Training Loss: 0.0809548944234848\n",
      "Epoch 15624/30000 Training Loss: 0.06504681706428528\n",
      "Epoch 15625/30000 Training Loss: 0.08951693773269653\n",
      "Epoch 15626/30000 Training Loss: 0.059346601366996765\n",
      "Epoch 15627/30000 Training Loss: 0.07219916582107544\n",
      "Epoch 15628/30000 Training Loss: 0.0712769404053688\n",
      "Epoch 15629/30000 Training Loss: 0.07045846432447433\n",
      "Epoch 15630/30000 Training Loss: 0.07417402416467667\n",
      "Epoch 15630/30000 Validation Loss: 0.07659386843442917\n",
      "Epoch 15631/30000 Training Loss: 0.0662103071808815\n",
      "Epoch 15632/30000 Training Loss: 0.07602596282958984\n",
      "Epoch 15633/30000 Training Loss: 0.07137005776166916\n",
      "Epoch 15634/30000 Training Loss: 0.08538701385259628\n",
      "Epoch 15635/30000 Training Loss: 0.07389374822378159\n",
      "Epoch 15636/30000 Training Loss: 0.07732915133237839\n",
      "Epoch 15637/30000 Training Loss: 0.0743069276213646\n",
      "Epoch 15638/30000 Training Loss: 0.06574303656816483\n",
      "Epoch 15639/30000 Training Loss: 0.06463437527418137\n",
      "Epoch 15640/30000 Training Loss: 0.09785354137420654\n",
      "Epoch 15640/30000 Validation Loss: 0.06395932286977768\n",
      "Epoch 15641/30000 Training Loss: 0.07789210230112076\n",
      "Epoch 15642/30000 Training Loss: 0.08103733509778976\n",
      "Epoch 15643/30000 Training Loss: 0.07444185763597488\n",
      "Epoch 15644/30000 Training Loss: 0.06570771336555481\n",
      "Epoch 15645/30000 Training Loss: 0.0746716633439064\n",
      "Epoch 15646/30000 Training Loss: 0.09297593683004379\n",
      "Epoch 15647/30000 Training Loss: 0.08319695293903351\n",
      "Epoch 15648/30000 Training Loss: 0.08088218420743942\n",
      "Epoch 15649/30000 Training Loss: 0.08226967602968216\n",
      "Epoch 15650/30000 Training Loss: 0.0682884082198143\n",
      "Epoch 15650/30000 Validation Loss: 0.08822671324014664\n",
      "Epoch 15651/30000 Training Loss: 0.08686020970344543\n",
      "Epoch 15652/30000 Training Loss: 0.08037125319242477\n",
      "Epoch 15653/30000 Training Loss: 0.0762004479765892\n",
      "Epoch 15654/30000 Training Loss: 0.06964748352766037\n",
      "Epoch 15655/30000 Training Loss: 0.06828319281339645\n",
      "Epoch 15656/30000 Training Loss: 0.06804553419351578\n",
      "Epoch 15657/30000 Training Loss: 0.05741637572646141\n",
      "Epoch 15658/30000 Training Loss: 0.10979235172271729\n",
      "Epoch 15659/30000 Training Loss: 0.07982882112264633\n",
      "Epoch 15660/30000 Training Loss: 0.06969109922647476\n",
      "Epoch 15660/30000 Validation Loss: 0.06865244358778\n",
      "Epoch 15661/30000 Training Loss: 0.07542473822832108\n",
      "Epoch 15662/30000 Training Loss: 0.06874509900808334\n",
      "Epoch 15663/30000 Training Loss: 0.052128445357084274\n",
      "Epoch 15664/30000 Training Loss: 0.07201237976551056\n",
      "Epoch 15665/30000 Training Loss: 0.0700017511844635\n",
      "Epoch 15666/30000 Training Loss: 0.06818339228630066\n",
      "Epoch 15667/30000 Training Loss: 0.07923077791929245\n",
      "Epoch 15668/30000 Training Loss: 0.07817554473876953\n",
      "Epoch 15669/30000 Training Loss: 0.07924522459506989\n",
      "Epoch 15670/30000 Training Loss: 0.05778555944561958\n",
      "Epoch 15670/30000 Validation Loss: 0.07540392875671387\n",
      "Epoch 15671/30000 Training Loss: 0.09512746334075928\n",
      "Epoch 15672/30000 Training Loss: 0.05882613733410835\n",
      "Epoch 15673/30000 Training Loss: 0.06043343245983124\n",
      "Epoch 15674/30000 Training Loss: 0.07904458045959473\n",
      "Epoch 15675/30000 Training Loss: 0.0838686153292656\n",
      "Epoch 15676/30000 Training Loss: 0.0699162408709526\n",
      "Epoch 15677/30000 Training Loss: 0.0886809229850769\n",
      "Epoch 15678/30000 Training Loss: 0.06876396387815475\n",
      "Epoch 15679/30000 Training Loss: 0.08262961357831955\n",
      "Epoch 15680/30000 Training Loss: 0.08465796709060669\n",
      "Epoch 15680/30000 Validation Loss: 0.08094306290149689\n",
      "Epoch 15681/30000 Training Loss: 0.07665315270423889\n",
      "Epoch 15682/30000 Training Loss: 0.0830296203494072\n",
      "Epoch 15683/30000 Training Loss: 0.0742119625210762\n",
      "Epoch 15684/30000 Training Loss: 0.08736255019903183\n",
      "Epoch 15685/30000 Training Loss: 0.0779578909277916\n",
      "Epoch 15686/30000 Training Loss: 0.08153143525123596\n",
      "Epoch 15687/30000 Training Loss: 0.05978850647807121\n",
      "Epoch 15688/30000 Training Loss: 0.061541300266981125\n",
      "Epoch 15689/30000 Training Loss: 0.07766014337539673\n",
      "Epoch 15690/30000 Training Loss: 0.06743866950273514\n",
      "Epoch 15690/30000 Validation Loss: 0.07675834745168686\n",
      "Epoch 15691/30000 Training Loss: 0.07072725892066956\n",
      "Epoch 15692/30000 Training Loss: 0.07923377305269241\n",
      "Epoch 15693/30000 Training Loss: 0.06594555824995041\n",
      "Epoch 15694/30000 Training Loss: 0.05304591357707977\n",
      "Epoch 15695/30000 Training Loss: 0.07944169640541077\n",
      "Epoch 15696/30000 Training Loss: 0.06732205301523209\n",
      "Epoch 15697/30000 Training Loss: 0.06208781525492668\n",
      "Epoch 15698/30000 Training Loss: 0.06873718649148941\n",
      "Epoch 15699/30000 Training Loss: 0.057834442704916\n",
      "Epoch 15700/30000 Training Loss: 0.07408823817968369\n",
      "Epoch 15700/30000 Validation Loss: 0.08025362342596054\n",
      "Epoch 15701/30000 Training Loss: 0.07175429165363312\n",
      "Epoch 15702/30000 Training Loss: 0.08114424347877502\n",
      "Epoch 15703/30000 Training Loss: 0.050863370299339294\n",
      "Epoch 15704/30000 Training Loss: 0.0807008221745491\n",
      "Epoch 15705/30000 Training Loss: 0.10776001214981079\n",
      "Epoch 15706/30000 Training Loss: 0.06748858839273453\n",
      "Epoch 15707/30000 Training Loss: 0.06604090332984924\n",
      "Epoch 15708/30000 Training Loss: 0.05639566853642464\n",
      "Epoch 15709/30000 Training Loss: 0.0680113360285759\n",
      "Epoch 15710/30000 Training Loss: 0.07375207543373108\n",
      "Epoch 15710/30000 Validation Loss: 0.1059614047408104\n",
      "Epoch 15711/30000 Training Loss: 0.08814674615859985\n",
      "Epoch 15712/30000 Training Loss: 0.0712682232260704\n",
      "Epoch 15713/30000 Training Loss: 0.06797917932271957\n",
      "Epoch 15714/30000 Training Loss: 0.09825265407562256\n",
      "Epoch 15715/30000 Training Loss: 0.07124539464712143\n",
      "Epoch 15716/30000 Training Loss: 0.07607074081897736\n",
      "Epoch 15717/30000 Training Loss: 0.06688747555017471\n",
      "Epoch 15718/30000 Training Loss: 0.07433555275201797\n",
      "Epoch 15719/30000 Training Loss: 0.07816503196954727\n",
      "Epoch 15720/30000 Training Loss: 0.07437369972467422\n",
      "Epoch 15720/30000 Validation Loss: 0.06118813157081604\n",
      "Epoch 15721/30000 Training Loss: 0.06928955763578415\n",
      "Epoch 15722/30000 Training Loss: 0.08290132135152817\n",
      "Epoch 15723/30000 Training Loss: 0.061965424567461014\n",
      "Epoch 15724/30000 Training Loss: 0.06727764010429382\n",
      "Epoch 15725/30000 Training Loss: 0.0638280138373375\n",
      "Epoch 15726/30000 Training Loss: 0.08557575941085815\n",
      "Epoch 15727/30000 Training Loss: 0.06500144302845001\n",
      "Epoch 15728/30000 Training Loss: 0.09100920706987381\n",
      "Epoch 15729/30000 Training Loss: 0.08040416240692139\n",
      "Epoch 15730/30000 Training Loss: 0.07743603736162186\n",
      "Epoch 15730/30000 Validation Loss: 0.08379117399454117\n",
      "Epoch 15731/30000 Training Loss: 0.0740085020661354\n",
      "Epoch 15732/30000 Training Loss: 0.09286916255950928\n",
      "Epoch 15733/30000 Training Loss: 0.07109386473894119\n",
      "Epoch 15734/30000 Training Loss: 0.08034253865480423\n",
      "Epoch 15735/30000 Training Loss: 0.0818498432636261\n",
      "Epoch 15736/30000 Training Loss: 0.077360600233078\n",
      "Epoch 15737/30000 Training Loss: 0.0892866775393486\n",
      "Epoch 15738/30000 Training Loss: 0.07882537692785263\n",
      "Epoch 15739/30000 Training Loss: 0.068962961435318\n",
      "Epoch 15740/30000 Training Loss: 0.07617441564798355\n",
      "Epoch 15740/30000 Validation Loss: 0.059307169169187546\n",
      "Epoch 15741/30000 Training Loss: 0.07146605849266052\n",
      "Epoch 15742/30000 Training Loss: 0.0751214399933815\n",
      "Epoch 15743/30000 Training Loss: 0.06863120943307877\n",
      "Epoch 15744/30000 Training Loss: 0.08251740783452988\n",
      "Epoch 15745/30000 Training Loss: 0.06098143383860588\n",
      "Epoch 15746/30000 Training Loss: 0.057134583592414856\n",
      "Epoch 15747/30000 Training Loss: 0.09891155362129211\n",
      "Epoch 15748/30000 Training Loss: 0.07331541180610657\n",
      "Epoch 15749/30000 Training Loss: 0.092464379966259\n",
      "Epoch 15750/30000 Training Loss: 0.0693678930401802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15750/30000 Validation Loss: 0.07283138483762741\n",
      "Epoch 15751/30000 Training Loss: 0.08666238188743591\n",
      "Epoch 15752/30000 Training Loss: 0.07369384914636612\n",
      "Epoch 15753/30000 Training Loss: 0.07103120535612106\n",
      "Epoch 15754/30000 Training Loss: 0.0639229491353035\n",
      "Epoch 15755/30000 Training Loss: 0.07186026871204376\n",
      "Epoch 15756/30000 Training Loss: 0.07232511043548584\n",
      "Epoch 15757/30000 Training Loss: 0.06829928606748581\n",
      "Epoch 15758/30000 Training Loss: 0.07968493551015854\n",
      "Epoch 15759/30000 Training Loss: 0.07707535475492477\n",
      "Epoch 15760/30000 Training Loss: 0.08713197708129883\n",
      "Epoch 15760/30000 Validation Loss: 0.07740914076566696\n",
      "Epoch 15761/30000 Training Loss: 0.0891481339931488\n",
      "Epoch 15762/30000 Training Loss: 0.06757085770368576\n",
      "Epoch 15763/30000 Training Loss: 0.08319473266601562\n",
      "Epoch 15764/30000 Training Loss: 0.06274241209030151\n",
      "Epoch 15765/30000 Training Loss: 0.10674303770065308\n",
      "Epoch 15766/30000 Training Loss: 0.061568498611450195\n",
      "Epoch 15767/30000 Training Loss: 0.061612147837877274\n",
      "Epoch 15768/30000 Training Loss: 0.05922402814030647\n",
      "Epoch 15769/30000 Training Loss: 0.06188606098294258\n",
      "Epoch 15770/30000 Training Loss: 0.07754883170127869\n",
      "Epoch 15770/30000 Validation Loss: 0.07760193198919296\n",
      "Epoch 15771/30000 Training Loss: 0.09447146207094193\n",
      "Epoch 15772/30000 Training Loss: 0.07918102294206619\n",
      "Epoch 15773/30000 Training Loss: 0.06412369757890701\n",
      "Epoch 15774/30000 Training Loss: 0.06859719753265381\n",
      "Epoch 15775/30000 Training Loss: 0.08440407365560532\n",
      "Epoch 15776/30000 Training Loss: 0.08090220391750336\n",
      "Epoch 15777/30000 Training Loss: 0.06965570896863937\n",
      "Epoch 15778/30000 Training Loss: 0.086861252784729\n",
      "Epoch 15779/30000 Training Loss: 0.0654149055480957\n",
      "Epoch 15780/30000 Training Loss: 0.0703325942158699\n",
      "Epoch 15780/30000 Validation Loss: 0.06563384085893631\n",
      "Epoch 15781/30000 Training Loss: 0.0772194191813469\n",
      "Epoch 15782/30000 Training Loss: 0.072613924741745\n",
      "Epoch 15783/30000 Training Loss: 0.0677984431385994\n",
      "Epoch 15784/30000 Training Loss: 0.060147225856781006\n",
      "Epoch 15785/30000 Training Loss: 0.06145399808883667\n",
      "Epoch 15786/30000 Training Loss: 0.07916593551635742\n",
      "Epoch 15787/30000 Training Loss: 0.07779842615127563\n",
      "Epoch 15788/30000 Training Loss: 0.07585310935974121\n",
      "Epoch 15789/30000 Training Loss: 0.07775136828422546\n",
      "Epoch 15790/30000 Training Loss: 0.09256207942962646\n",
      "Epoch 15790/30000 Validation Loss: 0.05345222353935242\n",
      "Epoch 15791/30000 Training Loss: 0.06747845560312271\n",
      "Epoch 15792/30000 Training Loss: 0.07221654802560806\n",
      "Epoch 15793/30000 Training Loss: 0.08499521017074585\n",
      "Epoch 15794/30000 Training Loss: 0.06623268127441406\n",
      "Epoch 15795/30000 Training Loss: 0.06493173539638519\n",
      "Epoch 15796/30000 Training Loss: 0.058581024408340454\n",
      "Epoch 15797/30000 Training Loss: 0.07303480058908463\n",
      "Epoch 15798/30000 Training Loss: 0.06613529473543167\n",
      "Epoch 15799/30000 Training Loss: 0.062061697244644165\n",
      "Epoch 15800/30000 Training Loss: 0.07543410360813141\n",
      "Epoch 15800/30000 Validation Loss: 0.06280281394720078\n",
      "Epoch 15801/30000 Training Loss: 0.07230288535356522\n",
      "Epoch 15802/30000 Training Loss: 0.06788083910942078\n",
      "Epoch 15803/30000 Training Loss: 0.06289175152778625\n",
      "Epoch 15804/30000 Training Loss: 0.06746406853199005\n",
      "Epoch 15805/30000 Training Loss: 0.07544706016778946\n",
      "Epoch 15806/30000 Training Loss: 0.08678260445594788\n",
      "Epoch 15807/30000 Training Loss: 0.07194303721189499\n",
      "Epoch 15808/30000 Training Loss: 0.07400289177894592\n",
      "Epoch 15809/30000 Training Loss: 0.06834486126899719\n",
      "Epoch 15810/30000 Training Loss: 0.07705104351043701\n",
      "Epoch 15810/30000 Validation Loss: 0.07076077163219452\n",
      "Epoch 15811/30000 Training Loss: 0.06417269259691238\n",
      "Epoch 15812/30000 Training Loss: 0.06882075220346451\n",
      "Epoch 15813/30000 Training Loss: 0.0687665268778801\n",
      "Epoch 15814/30000 Training Loss: 0.07875049114227295\n",
      "Epoch 15815/30000 Training Loss: 0.050045982003211975\n",
      "Epoch 15816/30000 Training Loss: 0.07721876353025436\n",
      "Epoch 15817/30000 Training Loss: 0.09353524446487427\n",
      "Epoch 15818/30000 Training Loss: 0.0760645866394043\n",
      "Epoch 15819/30000 Training Loss: 0.08670052886009216\n",
      "Epoch 15820/30000 Training Loss: 0.07342676818370819\n",
      "Epoch 15820/30000 Validation Loss: 0.09180312603712082\n",
      "Epoch 15821/30000 Training Loss: 0.08612266182899475\n",
      "Epoch 15822/30000 Training Loss: 0.07316053658723831\n",
      "Epoch 15823/30000 Training Loss: 0.07323113828897476\n",
      "Epoch 15824/30000 Training Loss: 0.07083139568567276\n",
      "Epoch 15825/30000 Training Loss: 0.06952511519193649\n",
      "Epoch 15826/30000 Training Loss: 0.07146460562944412\n",
      "Epoch 15827/30000 Training Loss: 0.07773817330598831\n",
      "Epoch 15828/30000 Training Loss: 0.07255788147449493\n",
      "Epoch 15829/30000 Training Loss: 0.07616206258535385\n",
      "Epoch 15830/30000 Training Loss: 0.06753312796354294\n",
      "Epoch 15830/30000 Validation Loss: 0.08718213438987732\n",
      "Epoch 15831/30000 Training Loss: 0.06098957359790802\n",
      "Epoch 15832/30000 Training Loss: 0.08261216431856155\n",
      "Epoch 15833/30000 Training Loss: 0.09104651212692261\n",
      "Epoch 15834/30000 Training Loss: 0.06348172575235367\n",
      "Epoch 15835/30000 Training Loss: 0.08240360021591187\n",
      "Epoch 15836/30000 Training Loss: 0.07468542456626892\n",
      "Epoch 15837/30000 Training Loss: 0.07849399000406265\n",
      "Epoch 15838/30000 Training Loss: 0.07342324405908585\n",
      "Epoch 15839/30000 Training Loss: 0.07744819670915604\n",
      "Epoch 15840/30000 Training Loss: 0.07071610540151596\n",
      "Epoch 15840/30000 Validation Loss: 0.0690779909491539\n",
      "Epoch 15841/30000 Training Loss: 0.07509855180978775\n",
      "Epoch 15842/30000 Training Loss: 0.075135238468647\n",
      "Epoch 15843/30000 Training Loss: 0.06899578124284744\n",
      "Epoch 15844/30000 Training Loss: 0.0786200761795044\n",
      "Epoch 15845/30000 Training Loss: 0.08173177391290665\n",
      "Epoch 15846/30000 Training Loss: 0.07079116255044937\n",
      "Epoch 15847/30000 Training Loss: 0.08848723769187927\n",
      "Epoch 15848/30000 Training Loss: 0.07800086587667465\n",
      "Epoch 15849/30000 Training Loss: 0.056932222098112106\n",
      "Epoch 15850/30000 Training Loss: 0.07491722702980042\n",
      "Epoch 15850/30000 Validation Loss: 0.08673117309808731\n",
      "Epoch 15851/30000 Training Loss: 0.07571932673454285\n",
      "Epoch 15852/30000 Training Loss: 0.09330757707357407\n",
      "Epoch 15853/30000 Training Loss: 0.07341008633375168\n",
      "Epoch 15854/30000 Training Loss: 0.07788023352622986\n",
      "Epoch 15855/30000 Training Loss: 0.0845324695110321\n",
      "Epoch 15856/30000 Training Loss: 0.05878118798136711\n",
      "Epoch 15857/30000 Training Loss: 0.06674257665872574\n",
      "Epoch 15858/30000 Training Loss: 0.09002089500427246\n",
      "Epoch 15859/30000 Training Loss: 0.05474591627717018\n",
      "Epoch 15860/30000 Training Loss: 0.08427580446004868\n",
      "Epoch 15860/30000 Validation Loss: 0.07035095244646072\n",
      "Epoch 15861/30000 Training Loss: 0.07428305596113205\n",
      "Epoch 15862/30000 Training Loss: 0.06844434887170792\n",
      "Epoch 15863/30000 Training Loss: 0.09551166743040085\n",
      "Epoch 15864/30000 Training Loss: 0.06662430614233017\n",
      "Epoch 15865/30000 Training Loss: 0.07064469903707504\n",
      "Epoch 15866/30000 Training Loss: 0.07007188349962234\n",
      "Epoch 15867/30000 Training Loss: 0.11168306320905685\n",
      "Epoch 15868/30000 Training Loss: 0.07776262611150742\n",
      "Epoch 15869/30000 Training Loss: 0.06592682749032974\n",
      "Epoch 15870/30000 Training Loss: 0.10718261450529099\n",
      "Epoch 15870/30000 Validation Loss: 0.06570663303136826\n",
      "Epoch 15871/30000 Training Loss: 0.06968335807323456\n",
      "Epoch 15872/30000 Training Loss: 0.05516644939780235\n",
      "Epoch 15873/30000 Training Loss: 0.05702201649546623\n",
      "Epoch 15874/30000 Training Loss: 0.07106409221887589\n",
      "Epoch 15875/30000 Training Loss: 0.07436952739953995\n",
      "Epoch 15876/30000 Training Loss: 0.07495047152042389\n",
      "Epoch 15877/30000 Training Loss: 0.05545906722545624\n",
      "Epoch 15878/30000 Training Loss: 0.0634363517165184\n",
      "Epoch 15879/30000 Training Loss: 0.09180436283349991\n",
      "Epoch 15880/30000 Training Loss: 0.0678003579378128\n",
      "Epoch 15880/30000 Validation Loss: 0.07029351592063904\n",
      "Epoch 15881/30000 Training Loss: 0.08707375079393387\n",
      "Epoch 15882/30000 Training Loss: 0.07873954623937607\n",
      "Epoch 15883/30000 Training Loss: 0.07957029342651367\n",
      "Epoch 15884/30000 Training Loss: 0.07108151912689209\n",
      "Epoch 15885/30000 Training Loss: 0.09134211391210556\n",
      "Epoch 15886/30000 Training Loss: 0.07716048508882523\n",
      "Epoch 15887/30000 Training Loss: 0.06765944510698318\n",
      "Epoch 15888/30000 Training Loss: 0.09734194725751877\n",
      "Epoch 15889/30000 Training Loss: 0.05143512412905693\n",
      "Epoch 15890/30000 Training Loss: 0.06241345405578613\n",
      "Epoch 15890/30000 Validation Loss: 0.07492297142744064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15891/30000 Training Loss: 0.06212016940116882\n",
      "Epoch 15892/30000 Training Loss: 0.09549015015363693\n",
      "Epoch 15893/30000 Training Loss: 0.07713630795478821\n",
      "Epoch 15894/30000 Training Loss: 0.048395778983831406\n",
      "Epoch 15895/30000 Training Loss: 0.08997633308172226\n",
      "Epoch 15896/30000 Training Loss: 0.05489509180188179\n",
      "Epoch 15897/30000 Training Loss: 0.0834893211722374\n",
      "Epoch 15898/30000 Training Loss: 0.06669542193412781\n",
      "Epoch 15899/30000 Training Loss: 0.07487844675779343\n",
      "Epoch 15900/30000 Training Loss: 0.07687617093324661\n",
      "Epoch 15900/30000 Validation Loss: 0.06870468705892563\n",
      "Epoch 15901/30000 Training Loss: 0.058514554053545\n",
      "Epoch 15902/30000 Training Loss: 0.07721395045518875\n",
      "Epoch 15903/30000 Training Loss: 0.06227567419409752\n",
      "Epoch 15904/30000 Training Loss: 0.09439390897750854\n",
      "Epoch 15905/30000 Training Loss: 0.0683448314666748\n",
      "Epoch 15906/30000 Training Loss: 0.08643188327550888\n",
      "Epoch 15907/30000 Training Loss: 0.0703151598572731\n",
      "Epoch 15908/30000 Training Loss: 0.08934571593999863\n",
      "Epoch 15909/30000 Training Loss: 0.07781752198934555\n",
      "Epoch 15910/30000 Training Loss: 0.07864788174629211\n",
      "Epoch 15910/30000 Validation Loss: 0.07892297953367233\n",
      "Epoch 15911/30000 Training Loss: 0.07318910956382751\n",
      "Epoch 15912/30000 Training Loss: 0.08702336996793747\n",
      "Epoch 15913/30000 Training Loss: 0.07541600614786148\n",
      "Epoch 15914/30000 Training Loss: 0.08982769399881363\n",
      "Epoch 15915/30000 Training Loss: 0.0670533999800682\n",
      "Epoch 15916/30000 Training Loss: 0.07948089390993118\n",
      "Epoch 15917/30000 Training Loss: 0.07329340279102325\n",
      "Epoch 15918/30000 Training Loss: 0.06271997094154358\n",
      "Epoch 15919/30000 Training Loss: 0.08161139488220215\n",
      "Epoch 15920/30000 Training Loss: 0.09607687592506409\n",
      "Epoch 15920/30000 Validation Loss: 0.09488484263420105\n",
      "Epoch 15921/30000 Training Loss: 0.06395512074232101\n",
      "Epoch 15922/30000 Training Loss: 0.06965813785791397\n",
      "Epoch 15923/30000 Training Loss: 0.06401538848876953\n",
      "Epoch 15924/30000 Training Loss: 0.07999911159276962\n",
      "Epoch 15925/30000 Training Loss: 0.09640374779701233\n",
      "Epoch 15926/30000 Training Loss: 0.0700535997748375\n",
      "Epoch 15927/30000 Training Loss: 0.07259825617074966\n",
      "Epoch 15928/30000 Training Loss: 0.0713038370013237\n",
      "Epoch 15929/30000 Training Loss: 0.07621560245752335\n",
      "Epoch 15930/30000 Training Loss: 0.060818370431661606\n",
      "Epoch 15930/30000 Validation Loss: 0.07500749826431274\n",
      "Epoch 15931/30000 Training Loss: 0.07729455083608627\n",
      "Epoch 15932/30000 Training Loss: 0.07133632898330688\n",
      "Epoch 15933/30000 Training Loss: 0.05142982304096222\n",
      "Epoch 15934/30000 Training Loss: 0.0811445340514183\n",
      "Epoch 15935/30000 Training Loss: 0.07223931699991226\n",
      "Epoch 15936/30000 Training Loss: 0.06841087341308594\n",
      "Epoch 15937/30000 Training Loss: 0.082021065056324\n",
      "Epoch 15938/30000 Training Loss: 0.06308812648057938\n",
      "Epoch 15939/30000 Training Loss: 0.06624585390090942\n",
      "Epoch 15940/30000 Training Loss: 0.06281053274869919\n",
      "Epoch 15940/30000 Validation Loss: 0.08265003561973572\n",
      "Epoch 15941/30000 Training Loss: 0.06868536025285721\n",
      "Epoch 15942/30000 Training Loss: 0.05245693400502205\n",
      "Epoch 15943/30000 Training Loss: 0.07878571003675461\n",
      "Epoch 15944/30000 Training Loss: 0.05787454545497894\n",
      "Epoch 15945/30000 Training Loss: 0.06862723082304001\n",
      "Epoch 15946/30000 Training Loss: 0.06814393401145935\n",
      "Epoch 15947/30000 Training Loss: 0.08398875594139099\n",
      "Epoch 15948/30000 Training Loss: 0.08499464392662048\n",
      "Epoch 15949/30000 Training Loss: 0.06839209049940109\n",
      "Epoch 15950/30000 Training Loss: 0.09104469418525696\n",
      "Epoch 15950/30000 Validation Loss: 0.0930386409163475\n",
      "Epoch 15951/30000 Training Loss: 0.06415615975856781\n",
      "Epoch 15952/30000 Training Loss: 0.0816277489066124\n",
      "Epoch 15953/30000 Training Loss: 0.08014488965272903\n",
      "Epoch 15954/30000 Training Loss: 0.0747571811079979\n",
      "Epoch 15955/30000 Training Loss: 0.05569573864340782\n",
      "Epoch 15956/30000 Training Loss: 0.07166506350040436\n",
      "Epoch 15957/30000 Training Loss: 0.06401457637548447\n",
      "Epoch 15958/30000 Training Loss: 0.08177593350410461\n",
      "Epoch 15959/30000 Training Loss: 0.07053986191749573\n",
      "Epoch 15960/30000 Training Loss: 0.07715637236833572\n",
      "Epoch 15960/30000 Validation Loss: 0.08607637137174606\n",
      "Epoch 15961/30000 Training Loss: 0.06703741103410721\n",
      "Epoch 15962/30000 Training Loss: 0.06381691247224808\n",
      "Epoch 15963/30000 Training Loss: 0.09142556041479111\n",
      "Epoch 15964/30000 Training Loss: 0.05799790844321251\n",
      "Epoch 15965/30000 Training Loss: 0.06953832507133484\n",
      "Epoch 15966/30000 Training Loss: 0.06274988502264023\n",
      "Epoch 15967/30000 Training Loss: 0.07161517441272736\n",
      "Epoch 15968/30000 Training Loss: 0.07441747188568115\n",
      "Epoch 15969/30000 Training Loss: 0.056435469537973404\n",
      "Epoch 15970/30000 Training Loss: 0.07972832769155502\n",
      "Epoch 15970/30000 Validation Loss: 0.0824785828590393\n",
      "Epoch 15971/30000 Training Loss: 0.07900985330343246\n",
      "Epoch 15972/30000 Training Loss: 0.06966965645551682\n",
      "Epoch 15973/30000 Training Loss: 0.08299627900123596\n",
      "Epoch 15974/30000 Training Loss: 0.05930560827255249\n",
      "Epoch 15975/30000 Training Loss: 0.05748441815376282\n",
      "Epoch 15976/30000 Training Loss: 0.09340333193540573\n",
      "Epoch 15977/30000 Training Loss: 0.07751565426588058\n",
      "Epoch 15978/30000 Training Loss: 0.0641552284359932\n",
      "Epoch 15979/30000 Training Loss: 0.08165330439805984\n",
      "Epoch 15980/30000 Training Loss: 0.07307302206754684\n",
      "Epoch 15980/30000 Validation Loss: 0.08762858062982559\n",
      "Epoch 15981/30000 Training Loss: 0.06264408677816391\n",
      "Epoch 15982/30000 Training Loss: 0.07834876328706741\n",
      "Epoch 15983/30000 Training Loss: 0.07409367710351944\n",
      "Epoch 15984/30000 Training Loss: 0.08032148331403732\n",
      "Epoch 15985/30000 Training Loss: 0.06324201822280884\n",
      "Epoch 15986/30000 Training Loss: 0.09740453958511353\n",
      "Epoch 15987/30000 Training Loss: 0.073572538793087\n",
      "Epoch 15988/30000 Training Loss: 0.06899729371070862\n",
      "Epoch 15989/30000 Training Loss: 0.06945985555648804\n",
      "Epoch 15990/30000 Training Loss: 0.05672653391957283\n",
      "Epoch 15990/30000 Validation Loss: 0.0880788266658783\n",
      "Epoch 15991/30000 Training Loss: 0.07228710502386093\n",
      "Epoch 15992/30000 Training Loss: 0.08115631341934204\n",
      "Epoch 15993/30000 Training Loss: 0.07988769561052322\n",
      "Epoch 15994/30000 Training Loss: 0.066317118704319\n",
      "Epoch 15995/30000 Training Loss: 0.07170918583869934\n",
      "Epoch 15996/30000 Training Loss: 0.07419726252555847\n",
      "Epoch 15997/30000 Training Loss: 0.06679414957761765\n",
      "Epoch 15998/30000 Training Loss: 0.05350701883435249\n",
      "Epoch 15999/30000 Training Loss: 0.06820017844438553\n",
      "Epoch 16000/30000 Training Loss: 0.0983896553516388\n",
      "Epoch 16000/30000 Validation Loss: 0.07222046703100204\n",
      "Epoch 16001/30000 Training Loss: 0.0745149552822113\n",
      "Epoch 16002/30000 Training Loss: 0.08764322847127914\n",
      "Epoch 16003/30000 Training Loss: 0.06615530699491501\n",
      "Epoch 16004/30000 Training Loss: 0.05978739261627197\n",
      "Epoch 16005/30000 Training Loss: 0.07468516379594803\n",
      "Epoch 16006/30000 Training Loss: 0.0776243731379509\n",
      "Epoch 16007/30000 Training Loss: 0.06137794256210327\n",
      "Epoch 16008/30000 Training Loss: 0.07723716646432877\n",
      "Epoch 16009/30000 Training Loss: 0.08085953444242477\n",
      "Epoch 16010/30000 Training Loss: 0.06897798180580139\n",
      "Epoch 16010/30000 Validation Loss: 0.06700564920902252\n",
      "Epoch 16011/30000 Training Loss: 0.0809008851647377\n",
      "Epoch 16012/30000 Training Loss: 0.07670026272535324\n",
      "Epoch 16013/30000 Training Loss: 0.07269276678562164\n",
      "Epoch 16014/30000 Training Loss: 0.08484842628240585\n",
      "Epoch 16015/30000 Training Loss: 0.07710393518209457\n",
      "Epoch 16016/30000 Training Loss: 0.0684487596154213\n",
      "Epoch 16017/30000 Training Loss: 0.08229895681142807\n",
      "Epoch 16018/30000 Training Loss: 0.0754924789071083\n",
      "Epoch 16019/30000 Training Loss: 0.07269664853811264\n",
      "Epoch 16020/30000 Training Loss: 0.058424681425094604\n",
      "Epoch 16020/30000 Validation Loss: 0.09260698407888412\n",
      "Epoch 16021/30000 Training Loss: 0.06346210837364197\n",
      "Epoch 16022/30000 Training Loss: 0.08903513103723526\n",
      "Epoch 16023/30000 Training Loss: 0.0572856105864048\n",
      "Epoch 16024/30000 Training Loss: 0.07511500269174576\n",
      "Epoch 16025/30000 Training Loss: 0.0737968310713768\n",
      "Epoch 16026/30000 Training Loss: 0.08253062516450882\n",
      "Epoch 16027/30000 Training Loss: 0.05762101337313652\n",
      "Epoch 16028/30000 Training Loss: 0.06565984338521957\n",
      "Epoch 16029/30000 Training Loss: 0.05447333678603172\n",
      "Epoch 16030/30000 Training Loss: 0.08330690115690231\n",
      "Epoch 16030/30000 Validation Loss: 0.0742911770939827\n",
      "Epoch 16031/30000 Training Loss: 0.0890207290649414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16032/30000 Training Loss: 0.07756052166223526\n",
      "Epoch 16033/30000 Training Loss: 0.07221585512161255\n",
      "Epoch 16034/30000 Training Loss: 0.07593805342912674\n",
      "Epoch 16035/30000 Training Loss: 0.06308455020189285\n",
      "Epoch 16036/30000 Training Loss: 0.09418738633394241\n",
      "Epoch 16037/30000 Training Loss: 0.08522635698318481\n",
      "Epoch 16038/30000 Training Loss: 0.06561750173568726\n",
      "Epoch 16039/30000 Training Loss: 0.07718030363321304\n",
      "Epoch 16040/30000 Training Loss: 0.08939731866121292\n",
      "Epoch 16040/30000 Validation Loss: 0.062286004424095154\n",
      "Epoch 16041/30000 Training Loss: 0.08047109842300415\n",
      "Epoch 16042/30000 Training Loss: 0.08419791609048843\n",
      "Epoch 16043/30000 Training Loss: 0.06462190300226212\n",
      "Epoch 16044/30000 Training Loss: 0.06407458335161209\n",
      "Epoch 16045/30000 Training Loss: 0.05519585311412811\n",
      "Epoch 16046/30000 Training Loss: 0.07007306069135666\n",
      "Epoch 16047/30000 Training Loss: 0.08872580528259277\n",
      "Epoch 16048/30000 Training Loss: 0.07160532474517822\n",
      "Epoch 16049/30000 Training Loss: 0.07122572511434555\n",
      "Epoch 16050/30000 Training Loss: 0.09739165753126144\n",
      "Epoch 16050/30000 Validation Loss: 0.08039607852697372\n",
      "Epoch 16051/30000 Training Loss: 0.07192741334438324\n",
      "Epoch 16052/30000 Training Loss: 0.06372696906328201\n",
      "Epoch 16053/30000 Training Loss: 0.07287705689668655\n",
      "Epoch 16054/30000 Training Loss: 0.08610987663269043\n",
      "Epoch 16055/30000 Training Loss: 0.07915859669446945\n",
      "Epoch 16056/30000 Training Loss: 0.0797673836350441\n",
      "Epoch 16057/30000 Training Loss: 0.08709361404180527\n",
      "Epoch 16058/30000 Training Loss: 0.06530068069696426\n",
      "Epoch 16059/30000 Training Loss: 0.09221493452787399\n",
      "Epoch 16060/30000 Training Loss: 0.06197359040379524\n",
      "Epoch 16060/30000 Validation Loss: 0.07174551486968994\n",
      "Epoch 16061/30000 Training Loss: 0.062971331179142\n",
      "Epoch 16062/30000 Training Loss: 0.06207742169499397\n",
      "Epoch 16063/30000 Training Loss: 0.07950080186128616\n",
      "Epoch 16064/30000 Training Loss: 0.07869917154312134\n",
      "Epoch 16065/30000 Training Loss: 0.09872082620859146\n",
      "Epoch 16066/30000 Training Loss: 0.07641344517469406\n",
      "Epoch 16067/30000 Training Loss: 0.08610185980796814\n",
      "Epoch 16068/30000 Training Loss: 0.08703908324241638\n",
      "Epoch 16069/30000 Training Loss: 0.062187690287828445\n",
      "Epoch 16070/30000 Training Loss: 0.07459000498056412\n",
      "Epoch 16070/30000 Validation Loss: 0.07715319842100143\n",
      "Epoch 16071/30000 Training Loss: 0.06353612244129181\n",
      "Epoch 16072/30000 Training Loss: 0.056834254413843155\n",
      "Epoch 16073/30000 Training Loss: 0.07089284807443619\n",
      "Epoch 16074/30000 Training Loss: 0.08406247943639755\n",
      "Epoch 16075/30000 Training Loss: 0.09009229391813278\n",
      "Epoch 16076/30000 Training Loss: 0.06984107941389084\n",
      "Epoch 16077/30000 Training Loss: 0.06668586283922195\n",
      "Epoch 16078/30000 Training Loss: 0.061818912625312805\n",
      "Epoch 16079/30000 Training Loss: 0.05881698057055473\n",
      "Epoch 16080/30000 Training Loss: 0.09058485180139542\n",
      "Epoch 16080/30000 Validation Loss: 0.07332552224397659\n",
      "Epoch 16081/30000 Training Loss: 0.06905577331781387\n",
      "Epoch 16082/30000 Training Loss: 0.0856192335486412\n",
      "Epoch 16083/30000 Training Loss: 0.05566875636577606\n",
      "Epoch 16084/30000 Training Loss: 0.0802631601691246\n",
      "Epoch 16085/30000 Training Loss: 0.056993354111909866\n",
      "Epoch 16086/30000 Training Loss: 0.07397433370351791\n",
      "Epoch 16087/30000 Training Loss: 0.0672520324587822\n",
      "Epoch 16088/30000 Training Loss: 0.11839631199836731\n",
      "Epoch 16089/30000 Training Loss: 0.06972616165876389\n",
      "Epoch 16090/30000 Training Loss: 0.07400663942098618\n",
      "Epoch 16090/30000 Validation Loss: 0.06012469530105591\n",
      "Epoch 16091/30000 Training Loss: 0.07918617874383926\n",
      "Epoch 16092/30000 Training Loss: 0.07198724895715714\n",
      "Epoch 16093/30000 Training Loss: 0.07409200072288513\n",
      "Epoch 16094/30000 Training Loss: 0.09938263893127441\n",
      "Epoch 16095/30000 Training Loss: 0.09005572646856308\n",
      "Epoch 16096/30000 Training Loss: 0.07383131235837936\n",
      "Epoch 16097/30000 Training Loss: 0.07009568810462952\n",
      "Epoch 16098/30000 Training Loss: 0.06182225048542023\n",
      "Epoch 16099/30000 Training Loss: 0.06864067167043686\n",
      "Epoch 16100/30000 Training Loss: 0.08996903151273727\n",
      "Epoch 16100/30000 Validation Loss: 0.08138411492109299\n",
      "Epoch 16101/30000 Training Loss: 0.07191356271505356\n",
      "Epoch 16102/30000 Training Loss: 0.07208746671676636\n",
      "Epoch 16103/30000 Training Loss: 0.08628591895103455\n",
      "Epoch 16104/30000 Training Loss: 0.07872302085161209\n",
      "Epoch 16105/30000 Training Loss: 0.06812455505132675\n",
      "Epoch 16106/30000 Training Loss: 0.0753295049071312\n",
      "Epoch 16107/30000 Training Loss: 0.05989633873105049\n",
      "Epoch 16108/30000 Training Loss: 0.07302949577569962\n",
      "Epoch 16109/30000 Training Loss: 0.08238116651773453\n",
      "Epoch 16110/30000 Training Loss: 0.08299203962087631\n",
      "Epoch 16110/30000 Validation Loss: 0.07895606011152267\n",
      "Epoch 16111/30000 Training Loss: 0.05956123769283295\n",
      "Epoch 16112/30000 Training Loss: 0.050474267452955246\n",
      "Epoch 16113/30000 Training Loss: 0.0672590509057045\n",
      "Epoch 16114/30000 Training Loss: 0.0592934675514698\n",
      "Epoch 16115/30000 Training Loss: 0.05973418056964874\n",
      "Epoch 16116/30000 Training Loss: 0.07441243529319763\n",
      "Epoch 16117/30000 Training Loss: 0.09878077358007431\n",
      "Epoch 16118/30000 Training Loss: 0.0713328868150711\n",
      "Epoch 16119/30000 Training Loss: 0.0691143050789833\n",
      "Epoch 16120/30000 Training Loss: 0.0698341503739357\n",
      "Epoch 16120/30000 Validation Loss: 0.06634081900119781\n",
      "Epoch 16121/30000 Training Loss: 0.05720466747879982\n",
      "Epoch 16122/30000 Training Loss: 0.07210046797990799\n",
      "Epoch 16123/30000 Training Loss: 0.07027003914117813\n",
      "Epoch 16124/30000 Training Loss: 0.08572570234537125\n",
      "Epoch 16125/30000 Training Loss: 0.09010254591703415\n",
      "Epoch 16126/30000 Training Loss: 0.08031346648931503\n",
      "Epoch 16127/30000 Training Loss: 0.06076425313949585\n",
      "Epoch 16128/30000 Training Loss: 0.08733421564102173\n",
      "Epoch 16129/30000 Training Loss: 0.06412824243307114\n",
      "Epoch 16130/30000 Training Loss: 0.07496524602174759\n",
      "Epoch 16130/30000 Validation Loss: 0.07955848425626755\n",
      "Epoch 16131/30000 Training Loss: 0.07080120593309402\n",
      "Epoch 16132/30000 Training Loss: 0.08360115438699722\n",
      "Epoch 16133/30000 Training Loss: 0.08027005195617676\n",
      "Epoch 16134/30000 Training Loss: 0.06493506580591202\n",
      "Epoch 16135/30000 Training Loss: 0.09277850389480591\n",
      "Epoch 16136/30000 Training Loss: 0.05159362033009529\n",
      "Epoch 16137/30000 Training Loss: 0.07884963601827621\n",
      "Epoch 16138/30000 Training Loss: 0.06198566034436226\n",
      "Epoch 16139/30000 Training Loss: 0.08830175548791885\n",
      "Epoch 16140/30000 Training Loss: 0.06726895272731781\n",
      "Epoch 16140/30000 Validation Loss: 0.08096497505903244\n",
      "Epoch 16141/30000 Training Loss: 0.06255345791578293\n",
      "Epoch 16142/30000 Training Loss: 0.07945949584245682\n",
      "Epoch 16143/30000 Training Loss: 0.06964176148176193\n",
      "Epoch 16144/30000 Training Loss: 0.08844245225191116\n",
      "Epoch 16145/30000 Training Loss: 0.07010570168495178\n",
      "Epoch 16146/30000 Training Loss: 0.08156948536634445\n",
      "Epoch 16147/30000 Training Loss: 0.07499054819345474\n",
      "Epoch 16148/30000 Training Loss: 0.07346224784851074\n",
      "Epoch 16149/30000 Training Loss: 0.09131491184234619\n",
      "Epoch 16150/30000 Training Loss: 0.07432442158460617\n",
      "Epoch 16150/30000 Validation Loss: 0.09096720814704895\n",
      "Epoch 16151/30000 Training Loss: 0.07700563222169876\n",
      "Epoch 16152/30000 Training Loss: 0.07465066015720367\n",
      "Epoch 16153/30000 Training Loss: 0.07001253217458725\n",
      "Epoch 16154/30000 Training Loss: 0.06932777166366577\n",
      "Epoch 16155/30000 Training Loss: 0.0735829696059227\n",
      "Epoch 16156/30000 Training Loss: 0.076494000852108\n",
      "Epoch 16157/30000 Training Loss: 0.07455717772245407\n",
      "Epoch 16158/30000 Training Loss: 0.05735747516155243\n",
      "Epoch 16159/30000 Training Loss: 0.05882441624999046\n",
      "Epoch 16160/30000 Training Loss: 0.07553284615278244\n",
      "Epoch 16160/30000 Validation Loss: 0.08624320477247238\n",
      "Epoch 16161/30000 Training Loss: 0.06858164817094803\n",
      "Epoch 16162/30000 Training Loss: 0.07808641344308853\n",
      "Epoch 16163/30000 Training Loss: 0.07677503675222397\n",
      "Epoch 16164/30000 Training Loss: 0.07674466818571091\n",
      "Epoch 16165/30000 Training Loss: 0.06479625403881073\n",
      "Epoch 16166/30000 Training Loss: 0.05817876383662224\n",
      "Epoch 16167/30000 Training Loss: 0.08405939489603043\n",
      "Epoch 16168/30000 Training Loss: 0.06594831496477127\n",
      "Epoch 16169/30000 Training Loss: 0.07185107469558716\n",
      "Epoch 16170/30000 Training Loss: 0.052227932959795\n",
      "Epoch 16170/30000 Validation Loss: 0.0695439800620079\n",
      "Epoch 16171/30000 Training Loss: 0.08884718269109726\n",
      "Epoch 16172/30000 Training Loss: 0.08030638843774796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16173/30000 Training Loss: 0.09911724925041199\n",
      "Epoch 16174/30000 Training Loss: 0.07524145394563675\n",
      "Epoch 16175/30000 Training Loss: 0.08047094941139221\n",
      "Epoch 16176/30000 Training Loss: 0.0756029263138771\n",
      "Epoch 16177/30000 Training Loss: 0.06979420781135559\n",
      "Epoch 16178/30000 Training Loss: 0.0764925554394722\n",
      "Epoch 16179/30000 Training Loss: 0.07045117765665054\n",
      "Epoch 16180/30000 Training Loss: 0.09428530186414719\n",
      "Epoch 16180/30000 Validation Loss: 0.07013384997844696\n",
      "Epoch 16181/30000 Training Loss: 0.07920005917549133\n",
      "Epoch 16182/30000 Training Loss: 0.08773157000541687\n",
      "Epoch 16183/30000 Training Loss: 0.07308712601661682\n",
      "Epoch 16184/30000 Training Loss: 0.07201069593429565\n",
      "Epoch 16185/30000 Training Loss: 0.06971561163663864\n",
      "Epoch 16186/30000 Training Loss: 0.06773404031991959\n",
      "Epoch 16187/30000 Training Loss: 0.07098089903593063\n",
      "Epoch 16188/30000 Training Loss: 0.068067766726017\n",
      "Epoch 16189/30000 Training Loss: 0.0810103714466095\n",
      "Epoch 16190/30000 Training Loss: 0.06503056734800339\n",
      "Epoch 16190/30000 Validation Loss: 0.07434498518705368\n",
      "Epoch 16191/30000 Training Loss: 0.08316438645124435\n",
      "Epoch 16192/30000 Training Loss: 0.07422882318496704\n",
      "Epoch 16193/30000 Training Loss: 0.06713332235813141\n",
      "Epoch 16194/30000 Training Loss: 0.07421904057264328\n",
      "Epoch 16195/30000 Training Loss: 0.06882400810718536\n",
      "Epoch 16196/30000 Training Loss: 0.06953494995832443\n",
      "Epoch 16197/30000 Training Loss: 0.07724592089653015\n",
      "Epoch 16198/30000 Training Loss: 0.06874352693557739\n",
      "Epoch 16199/30000 Training Loss: 0.06999044865369797\n",
      "Epoch 16200/30000 Training Loss: 0.06771823018789291\n",
      "Epoch 16200/30000 Validation Loss: 0.07315126806497574\n",
      "Epoch 16201/30000 Training Loss: 0.05813823267817497\n",
      "Epoch 16202/30000 Training Loss: 0.08067179471254349\n",
      "Epoch 16203/30000 Training Loss: 0.0928502306342125\n",
      "Epoch 16204/30000 Training Loss: 0.07088067382574081\n",
      "Epoch 16205/30000 Training Loss: 0.07044145464897156\n",
      "Epoch 16206/30000 Training Loss: 0.08234330266714096\n",
      "Epoch 16207/30000 Training Loss: 0.06209568679332733\n",
      "Epoch 16208/30000 Training Loss: 0.054536256939172745\n",
      "Epoch 16209/30000 Training Loss: 0.06808975338935852\n",
      "Epoch 16210/30000 Training Loss: 0.0631299614906311\n",
      "Epoch 16210/30000 Validation Loss: 0.06738432496786118\n",
      "Epoch 16211/30000 Training Loss: 0.07660671323537827\n",
      "Epoch 16212/30000 Training Loss: 0.06603968143463135\n",
      "Epoch 16213/30000 Training Loss: 0.0695449635386467\n",
      "Epoch 16214/30000 Training Loss: 0.06309753656387329\n",
      "Epoch 16215/30000 Training Loss: 0.08009996265172958\n",
      "Epoch 16216/30000 Training Loss: 0.09162777662277222\n",
      "Epoch 16217/30000 Training Loss: 0.07960466295480728\n",
      "Epoch 16218/30000 Training Loss: 0.07611563801765442\n",
      "Epoch 16219/30000 Training Loss: 0.07164338231086731\n",
      "Epoch 16220/30000 Training Loss: 0.08175142854452133\n",
      "Epoch 16220/30000 Validation Loss: 0.07786007970571518\n",
      "Epoch 16221/30000 Training Loss: 0.06479085236787796\n",
      "Epoch 16222/30000 Training Loss: 0.07281908392906189\n",
      "Epoch 16223/30000 Training Loss: 0.10483481734991074\n",
      "Epoch 16224/30000 Training Loss: 0.065610870718956\n",
      "Epoch 16225/30000 Training Loss: 0.06477341800928116\n",
      "Epoch 16226/30000 Training Loss: 0.07208550721406937\n",
      "Epoch 16227/30000 Training Loss: 0.07971031218767166\n",
      "Epoch 16228/30000 Training Loss: 0.08599120378494263\n",
      "Epoch 16229/30000 Training Loss: 0.06406975537538528\n",
      "Epoch 16230/30000 Training Loss: 0.08245941996574402\n",
      "Epoch 16230/30000 Validation Loss: 0.07856370508670807\n",
      "Epoch 16231/30000 Training Loss: 0.06548237800598145\n",
      "Epoch 16232/30000 Training Loss: 0.08263357728719711\n",
      "Epoch 16233/30000 Training Loss: 0.073649100959301\n",
      "Epoch 16234/30000 Training Loss: 0.08040343225002289\n",
      "Epoch 16235/30000 Training Loss: 0.07630928605794907\n",
      "Epoch 16236/30000 Training Loss: 0.06723781675100327\n",
      "Epoch 16237/30000 Training Loss: 0.060714706778526306\n",
      "Epoch 16238/30000 Training Loss: 0.06599727272987366\n",
      "Epoch 16239/30000 Training Loss: 0.10009568929672241\n",
      "Epoch 16240/30000 Training Loss: 0.0858224406838417\n",
      "Epoch 16240/30000 Validation Loss: 0.08563479036092758\n",
      "Epoch 16241/30000 Training Loss: 0.09349565953016281\n",
      "Epoch 16242/30000 Training Loss: 0.08213448524475098\n",
      "Epoch 16243/30000 Training Loss: 0.078453928232193\n",
      "Epoch 16244/30000 Training Loss: 0.0739668682217598\n",
      "Epoch 16245/30000 Training Loss: 0.06590226292610168\n",
      "Epoch 16246/30000 Training Loss: 0.0756782591342926\n",
      "Epoch 16247/30000 Training Loss: 0.0737374797463417\n",
      "Epoch 16248/30000 Training Loss: 0.06024956703186035\n",
      "Epoch 16249/30000 Training Loss: 0.08751168847084045\n",
      "Epoch 16250/30000 Training Loss: 0.07935409992933273\n",
      "Epoch 16250/30000 Validation Loss: 0.09094309061765671\n",
      "Epoch 16251/30000 Training Loss: 0.09025231003761292\n",
      "Epoch 16252/30000 Training Loss: 0.06338373571634293\n",
      "Epoch 16253/30000 Training Loss: 0.08540342003107071\n",
      "Epoch 16254/30000 Training Loss: 0.06222144886851311\n",
      "Epoch 16255/30000 Training Loss: 0.06439121812582016\n",
      "Epoch 16256/30000 Training Loss: 0.0737394168972969\n",
      "Epoch 16257/30000 Training Loss: 0.062354136258363724\n",
      "Epoch 16258/30000 Training Loss: 0.07680243998765945\n",
      "Epoch 16259/30000 Training Loss: 0.08949027210474014\n",
      "Epoch 16260/30000 Training Loss: 0.0857313945889473\n",
      "Epoch 16260/30000 Validation Loss: 0.08496177941560745\n",
      "Epoch 16261/30000 Training Loss: 0.06242698058485985\n",
      "Epoch 16262/30000 Training Loss: 0.08153755217790604\n",
      "Epoch 16263/30000 Training Loss: 0.08257739245891571\n",
      "Epoch 16264/30000 Training Loss: 0.09457217901945114\n",
      "Epoch 16265/30000 Training Loss: 0.07751383632421494\n",
      "Epoch 16266/30000 Training Loss: 0.0794629231095314\n",
      "Epoch 16267/30000 Training Loss: 0.06216853857040405\n",
      "Epoch 16268/30000 Training Loss: 0.05904606357216835\n",
      "Epoch 16269/30000 Training Loss: 0.06204409524798393\n",
      "Epoch 16270/30000 Training Loss: 0.09501438587903976\n",
      "Epoch 16270/30000 Validation Loss: 0.06861227005720139\n",
      "Epoch 16271/30000 Training Loss: 0.06831709295511246\n",
      "Epoch 16272/30000 Training Loss: 0.0710655227303505\n",
      "Epoch 16273/30000 Training Loss: 0.07470746338367462\n",
      "Epoch 16274/30000 Training Loss: 0.06312870979309082\n",
      "Epoch 16275/30000 Training Loss: 0.08077079802751541\n",
      "Epoch 16276/30000 Training Loss: 0.0702439695596695\n",
      "Epoch 16277/30000 Training Loss: 0.08293411880731583\n",
      "Epoch 16278/30000 Training Loss: 0.08584535121917725\n",
      "Epoch 16279/30000 Training Loss: 0.09691127389669418\n",
      "Epoch 16280/30000 Training Loss: 0.06973504275083542\n",
      "Epoch 16280/30000 Validation Loss: 0.06455393880605698\n",
      "Epoch 16281/30000 Training Loss: 0.062188953161239624\n",
      "Epoch 16282/30000 Training Loss: 0.07497359812259674\n",
      "Epoch 16283/30000 Training Loss: 0.06531140953302383\n",
      "Epoch 16284/30000 Training Loss: 0.07089795172214508\n",
      "Epoch 16285/30000 Training Loss: 0.0761844590306282\n",
      "Epoch 16286/30000 Training Loss: 0.06388034671545029\n",
      "Epoch 16287/30000 Training Loss: 0.08980276435613632\n",
      "Epoch 16288/30000 Training Loss: 0.06925169378519058\n",
      "Epoch 16289/30000 Training Loss: 0.06600772589445114\n",
      "Epoch 16290/30000 Training Loss: 0.07530808448791504\n",
      "Epoch 16290/30000 Validation Loss: 0.08112666010856628\n",
      "Epoch 16291/30000 Training Loss: 0.06805200129747391\n",
      "Epoch 16292/30000 Training Loss: 0.07627739757299423\n",
      "Epoch 16293/30000 Training Loss: 0.07547777146100998\n",
      "Epoch 16294/30000 Training Loss: 0.06957313418388367\n",
      "Epoch 16295/30000 Training Loss: 0.07475284487009048\n",
      "Epoch 16296/30000 Training Loss: 0.07841860502958298\n",
      "Epoch 16297/30000 Training Loss: 0.09125532954931259\n",
      "Epoch 16298/30000 Training Loss: 0.06821117550134659\n",
      "Epoch 16299/30000 Training Loss: 0.09636842459440231\n",
      "Epoch 16300/30000 Training Loss: 0.07430433481931686\n",
      "Epoch 16300/30000 Validation Loss: 0.08465418964624405\n",
      "Epoch 16301/30000 Training Loss: 0.0769122838973999\n",
      "Epoch 16302/30000 Training Loss: 0.07319074869155884\n",
      "Epoch 16303/30000 Training Loss: 0.06804155558347702\n",
      "Epoch 16304/30000 Training Loss: 0.06720646470785141\n",
      "Epoch 16305/30000 Training Loss: 0.09089665859937668\n",
      "Epoch 16306/30000 Training Loss: 0.06029440090060234\n",
      "Epoch 16307/30000 Training Loss: 0.08028947561979294\n",
      "Epoch 16308/30000 Training Loss: 0.06746753305196762\n",
      "Epoch 16309/30000 Training Loss: 0.09868886321783066\n",
      "Epoch 16310/30000 Training Loss: 0.059865206480026245\n",
      "Epoch 16310/30000 Validation Loss: 0.060874536633491516\n",
      "Epoch 16311/30000 Training Loss: 0.08496228605508804\n",
      "Epoch 16312/30000 Training Loss: 0.06821314245462418\n",
      "Epoch 16313/30000 Training Loss: 0.058606475591659546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16314/30000 Training Loss: 0.05753999948501587\n",
      "Epoch 16315/30000 Training Loss: 0.08022815734148026\n",
      "Epoch 16316/30000 Training Loss: 0.07167574763298035\n",
      "Epoch 16317/30000 Training Loss: 0.06562582403421402\n",
      "Epoch 16318/30000 Training Loss: 0.06564037501811981\n",
      "Epoch 16319/30000 Training Loss: 0.058216795325279236\n",
      "Epoch 16320/30000 Training Loss: 0.09090743213891983\n",
      "Epoch 16320/30000 Validation Loss: 0.06915903836488724\n",
      "Epoch 16321/30000 Training Loss: 0.07695391774177551\n",
      "Epoch 16322/30000 Training Loss: 0.07065705209970474\n",
      "Epoch 16323/30000 Training Loss: 0.09065625816583633\n",
      "Epoch 16324/30000 Training Loss: 0.05550701543688774\n",
      "Epoch 16325/30000 Training Loss: 0.07849142700433731\n",
      "Epoch 16326/30000 Training Loss: 0.07522682100534439\n",
      "Epoch 16327/30000 Training Loss: 0.06595898419618607\n",
      "Epoch 16328/30000 Training Loss: 0.06756473332643509\n",
      "Epoch 16329/30000 Training Loss: 0.07460209727287292\n",
      "Epoch 16330/30000 Training Loss: 0.07796219736337662\n",
      "Epoch 16330/30000 Validation Loss: 0.06186215206980705\n",
      "Epoch 16331/30000 Training Loss: 0.08973712474107742\n",
      "Epoch 16332/30000 Training Loss: 0.059402089565992355\n",
      "Epoch 16333/30000 Training Loss: 0.06894045323133469\n",
      "Epoch 16334/30000 Training Loss: 0.07227083295583725\n",
      "Epoch 16335/30000 Training Loss: 0.07823161035776138\n",
      "Epoch 16336/30000 Training Loss: 0.07215794920921326\n",
      "Epoch 16337/30000 Training Loss: 0.0937880203127861\n",
      "Epoch 16338/30000 Training Loss: 0.07248977571725845\n",
      "Epoch 16339/30000 Training Loss: 0.07790231704711914\n",
      "Epoch 16340/30000 Training Loss: 0.07677220553159714\n",
      "Epoch 16340/30000 Validation Loss: 0.08117345720529556\n",
      "Epoch 16341/30000 Training Loss: 0.06560323387384415\n",
      "Epoch 16342/30000 Training Loss: 0.07591572403907776\n",
      "Epoch 16343/30000 Training Loss: 0.05899209901690483\n",
      "Epoch 16344/30000 Training Loss: 0.07224179059267044\n",
      "Epoch 16345/30000 Training Loss: 0.06396546214818954\n",
      "Epoch 16346/30000 Training Loss: 0.08489479869604111\n",
      "Epoch 16347/30000 Training Loss: 0.0633629560470581\n",
      "Epoch 16348/30000 Training Loss: 0.06360835582017899\n",
      "Epoch 16349/30000 Training Loss: 0.09363333135843277\n",
      "Epoch 16350/30000 Training Loss: 0.08332441747188568\n",
      "Epoch 16350/30000 Validation Loss: 0.0629999041557312\n",
      "Epoch 16351/30000 Training Loss: 0.07848332077264786\n",
      "Epoch 16352/30000 Training Loss: 0.09517272561788559\n",
      "Epoch 16353/30000 Training Loss: 0.09151029586791992\n",
      "Epoch 16354/30000 Training Loss: 0.06820449978113174\n",
      "Epoch 16355/30000 Training Loss: 0.0780053362250328\n",
      "Epoch 16356/30000 Training Loss: 0.06756854802370071\n",
      "Epoch 16357/30000 Training Loss: 0.06771552562713623\n",
      "Epoch 16358/30000 Training Loss: 0.06435161083936691\n",
      "Epoch 16359/30000 Training Loss: 0.06833939999341965\n",
      "Epoch 16360/30000 Training Loss: 0.06086995080113411\n",
      "Epoch 16360/30000 Validation Loss: 0.07402525097131729\n",
      "Epoch 16361/30000 Training Loss: 0.07475641369819641\n",
      "Epoch 16362/30000 Training Loss: 0.07391827553510666\n",
      "Epoch 16363/30000 Training Loss: 0.05961472913622856\n",
      "Epoch 16364/30000 Training Loss: 0.0748581662774086\n",
      "Epoch 16365/30000 Training Loss: 0.09064391255378723\n",
      "Epoch 16366/30000 Training Loss: 0.06854229420423508\n",
      "Epoch 16367/30000 Training Loss: 0.09191638231277466\n",
      "Epoch 16368/30000 Training Loss: 0.0800054669380188\n",
      "Epoch 16369/30000 Training Loss: 0.07498788088560104\n",
      "Epoch 16370/30000 Training Loss: 0.05239148065447807\n",
      "Epoch 16370/30000 Validation Loss: 0.08355969935655594\n",
      "Epoch 16371/30000 Training Loss: 0.07753405719995499\n",
      "Epoch 16372/30000 Training Loss: 0.06151042506098747\n",
      "Epoch 16373/30000 Training Loss: 0.08174046874046326\n",
      "Epoch 16374/30000 Training Loss: 0.06201895698904991\n",
      "Epoch 16375/30000 Training Loss: 0.056410614401102066\n",
      "Epoch 16376/30000 Training Loss: 0.08401545137166977\n",
      "Epoch 16377/30000 Training Loss: 0.09295808523893356\n",
      "Epoch 16378/30000 Training Loss: 0.07753226906061172\n",
      "Epoch 16379/30000 Training Loss: 0.07521366328001022\n",
      "Epoch 16380/30000 Training Loss: 0.07886073738336563\n",
      "Epoch 16380/30000 Validation Loss: 0.07140520215034485\n",
      "Epoch 16381/30000 Training Loss: 0.0692908838391304\n",
      "Epoch 16382/30000 Training Loss: 0.07659807056188583\n",
      "Epoch 16383/30000 Training Loss: 0.0808933675289154\n",
      "Epoch 16384/30000 Training Loss: 0.0556558258831501\n",
      "Epoch 16385/30000 Training Loss: 0.07735093683004379\n",
      "Epoch 16386/30000 Training Loss: 0.07177882641553879\n",
      "Epoch 16387/30000 Training Loss: 0.06864369660615921\n",
      "Epoch 16388/30000 Training Loss: 0.05990130826830864\n",
      "Epoch 16389/30000 Training Loss: 0.07050737738609314\n",
      "Epoch 16390/30000 Training Loss: 0.07467640191316605\n",
      "Epoch 16390/30000 Validation Loss: 0.0765165463089943\n",
      "Epoch 16391/30000 Training Loss: 0.08329730480909348\n",
      "Epoch 16392/30000 Training Loss: 0.07181666046380997\n",
      "Epoch 16393/30000 Training Loss: 0.07764454931020737\n",
      "Epoch 16394/30000 Training Loss: 0.09823793917894363\n",
      "Epoch 16395/30000 Training Loss: 0.07402356714010239\n",
      "Epoch 16396/30000 Training Loss: 0.06994408369064331\n",
      "Epoch 16397/30000 Training Loss: 0.07875968515872955\n",
      "Epoch 16398/30000 Training Loss: 0.07741481810808182\n",
      "Epoch 16399/30000 Training Loss: 0.06810262054204941\n",
      "Epoch 16400/30000 Training Loss: 0.07559620589017868\n",
      "Epoch 16400/30000 Validation Loss: 0.07766666263341904\n",
      "Epoch 16401/30000 Training Loss: 0.06900908797979355\n",
      "Epoch 16402/30000 Training Loss: 0.0701642856001854\n",
      "Epoch 16403/30000 Training Loss: 0.06920979171991348\n",
      "Epoch 16404/30000 Training Loss: 0.05203821882605553\n",
      "Epoch 16405/30000 Training Loss: 0.057177066802978516\n",
      "Epoch 16406/30000 Training Loss: 0.08261281251907349\n",
      "Epoch 16407/30000 Training Loss: 0.08929929882287979\n",
      "Epoch 16408/30000 Training Loss: 0.0840315893292427\n",
      "Epoch 16409/30000 Training Loss: 0.06283228099346161\n",
      "Epoch 16410/30000 Training Loss: 0.08959764242172241\n",
      "Epoch 16410/30000 Validation Loss: 0.07703172415494919\n",
      "Epoch 16411/30000 Training Loss: 0.06068019941449165\n",
      "Epoch 16412/30000 Training Loss: 0.0933544710278511\n",
      "Epoch 16413/30000 Training Loss: 0.07925134897232056\n",
      "Epoch 16414/30000 Training Loss: 0.07271561771631241\n",
      "Epoch 16415/30000 Training Loss: 0.07609555870294571\n",
      "Epoch 16416/30000 Training Loss: 0.07927333563566208\n",
      "Epoch 16417/30000 Training Loss: 0.082611083984375\n",
      "Epoch 16418/30000 Training Loss: 0.07645976543426514\n",
      "Epoch 16419/30000 Training Loss: 0.0648944154381752\n",
      "Epoch 16420/30000 Training Loss: 0.07706277817487717\n",
      "Epoch 16420/30000 Validation Loss: 0.07518760114908218\n",
      "Epoch 16421/30000 Training Loss: 0.05825325846672058\n",
      "Epoch 16422/30000 Training Loss: 0.07300189137458801\n",
      "Epoch 16423/30000 Training Loss: 0.07192764431238174\n",
      "Epoch 16424/30000 Training Loss: 0.05998009070754051\n",
      "Epoch 16425/30000 Training Loss: 0.06961816549301147\n",
      "Epoch 16426/30000 Training Loss: 0.06669965386390686\n",
      "Epoch 16427/30000 Training Loss: 0.08237982541322708\n",
      "Epoch 16428/30000 Training Loss: 0.0906519666314125\n",
      "Epoch 16429/30000 Training Loss: 0.06811785697937012\n",
      "Epoch 16430/30000 Training Loss: 0.07392521947622299\n",
      "Epoch 16430/30000 Validation Loss: 0.07303107529878616\n",
      "Epoch 16431/30000 Training Loss: 0.06336823850870132\n",
      "Epoch 16432/30000 Training Loss: 0.09287742525339127\n",
      "Epoch 16433/30000 Training Loss: 0.08072920888662338\n",
      "Epoch 16434/30000 Training Loss: 0.0869404673576355\n",
      "Epoch 16435/30000 Training Loss: 0.07292861491441727\n",
      "Epoch 16436/30000 Training Loss: 0.061950478702783585\n",
      "Epoch 16437/30000 Training Loss: 0.08209017664194107\n",
      "Epoch 16438/30000 Training Loss: 0.08186736702919006\n",
      "Epoch 16439/30000 Training Loss: 0.06727632135152817\n",
      "Epoch 16440/30000 Training Loss: 0.06665787845849991\n",
      "Epoch 16440/30000 Validation Loss: 0.08789873868227005\n",
      "Epoch 16441/30000 Training Loss: 0.06667967140674591\n",
      "Epoch 16442/30000 Training Loss: 0.0702093243598938\n",
      "Epoch 16443/30000 Training Loss: 0.05566926673054695\n",
      "Epoch 16444/30000 Training Loss: 0.07076605409383774\n",
      "Epoch 16445/30000 Training Loss: 0.0792340561747551\n",
      "Epoch 16446/30000 Training Loss: 0.08639657497406006\n",
      "Epoch 16447/30000 Training Loss: 0.09318678826093674\n",
      "Epoch 16448/30000 Training Loss: 0.07913336902856827\n",
      "Epoch 16449/30000 Training Loss: 0.07570002228021622\n",
      "Epoch 16450/30000 Training Loss: 0.10104554891586304\n",
      "Epoch 16450/30000 Validation Loss: 0.07097256183624268\n",
      "Epoch 16451/30000 Training Loss: 0.06485533714294434\n",
      "Epoch 16452/30000 Training Loss: 0.055873751640319824\n",
      "Epoch 16453/30000 Training Loss: 0.08249431103467941\n",
      "Epoch 16454/30000 Training Loss: 0.07078861445188522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16455/30000 Training Loss: 0.08039691299200058\n",
      "Epoch 16456/30000 Training Loss: 0.0667407438158989\n",
      "Epoch 16457/30000 Training Loss: 0.06587675213813782\n",
      "Epoch 16458/30000 Training Loss: 0.06144735589623451\n",
      "Epoch 16459/30000 Training Loss: 0.08122222870588303\n",
      "Epoch 16460/30000 Training Loss: 0.07404124736785889\n",
      "Epoch 16460/30000 Validation Loss: 0.07202431559562683\n",
      "Epoch 16461/30000 Training Loss: 0.06676191836595535\n",
      "Epoch 16462/30000 Training Loss: 0.07666421681642532\n",
      "Epoch 16463/30000 Training Loss: 0.07950279861688614\n",
      "Epoch 16464/30000 Training Loss: 0.054404016584157944\n",
      "Epoch 16465/30000 Training Loss: 0.07190712541341782\n",
      "Epoch 16466/30000 Training Loss: 0.06191712245345116\n",
      "Epoch 16467/30000 Training Loss: 0.08216273039579391\n",
      "Epoch 16468/30000 Training Loss: 0.07621949166059494\n",
      "Epoch 16469/30000 Training Loss: 0.07374521344900131\n",
      "Epoch 16470/30000 Training Loss: 0.07493820786476135\n",
      "Epoch 16470/30000 Validation Loss: 0.0915246307849884\n",
      "Epoch 16471/30000 Training Loss: 0.07295367121696472\n",
      "Epoch 16472/30000 Training Loss: 0.07817048579454422\n",
      "Epoch 16473/30000 Training Loss: 0.060693442821502686\n",
      "Epoch 16474/30000 Training Loss: 0.06756362318992615\n",
      "Epoch 16475/30000 Training Loss: 0.0594295971095562\n",
      "Epoch 16476/30000 Training Loss: 0.08711722493171692\n",
      "Epoch 16477/30000 Training Loss: 0.07755051553249359\n",
      "Epoch 16478/30000 Training Loss: 0.05761101841926575\n",
      "Epoch 16479/30000 Training Loss: 0.0778135135769844\n",
      "Epoch 16480/30000 Training Loss: 0.06759948283433914\n",
      "Epoch 16480/30000 Validation Loss: 0.09678012877702713\n",
      "Epoch 16481/30000 Training Loss: 0.06964436173439026\n",
      "Epoch 16482/30000 Training Loss: 0.09945980459451675\n",
      "Epoch 16483/30000 Training Loss: 0.09266658872365952\n",
      "Epoch 16484/30000 Training Loss: 0.07642555981874466\n",
      "Epoch 16485/30000 Training Loss: 0.08108571916818619\n",
      "Epoch 16486/30000 Training Loss: 0.06582725793123245\n",
      "Epoch 16487/30000 Training Loss: 0.060418251901865005\n",
      "Epoch 16488/30000 Training Loss: 0.07298070937395096\n",
      "Epoch 16489/30000 Training Loss: 0.0731256976723671\n",
      "Epoch 16490/30000 Training Loss: 0.08067920058965683\n",
      "Epoch 16490/30000 Validation Loss: 0.06726387143135071\n",
      "Epoch 16491/30000 Training Loss: 0.06165764853358269\n",
      "Epoch 16492/30000 Training Loss: 0.07249323278665543\n",
      "Epoch 16493/30000 Training Loss: 0.09551310539245605\n",
      "Epoch 16494/30000 Training Loss: 0.06408295035362244\n",
      "Epoch 16495/30000 Training Loss: 0.06169729307293892\n",
      "Epoch 16496/30000 Training Loss: 0.07516119629144669\n",
      "Epoch 16497/30000 Training Loss: 0.07518003135919571\n",
      "Epoch 16498/30000 Training Loss: 0.07703477889299393\n",
      "Epoch 16499/30000 Training Loss: 0.06520196050405502\n",
      "Epoch 16500/30000 Training Loss: 0.08759564906358719\n",
      "Epoch 16500/30000 Validation Loss: 0.09466042369604111\n",
      "Epoch 16501/30000 Training Loss: 0.07660775631666183\n",
      "Epoch 16502/30000 Training Loss: 0.08082488924264908\n",
      "Epoch 16503/30000 Training Loss: 0.07504294067621231\n",
      "Epoch 16504/30000 Training Loss: 0.06462790817022324\n",
      "Epoch 16505/30000 Training Loss: 0.07237634062767029\n",
      "Epoch 16506/30000 Training Loss: 0.08367254585027695\n",
      "Epoch 16507/30000 Training Loss: 0.06782898306846619\n",
      "Epoch 16508/30000 Training Loss: 0.0741090402007103\n",
      "Epoch 16509/30000 Training Loss: 0.06888100504875183\n",
      "Epoch 16510/30000 Training Loss: 0.06122560799121857\n",
      "Epoch 16510/30000 Validation Loss: 0.055282872170209885\n",
      "Epoch 16511/30000 Training Loss: 0.06261798739433289\n",
      "Epoch 16512/30000 Training Loss: 0.09824744611978531\n",
      "Epoch 16513/30000 Training Loss: 0.07444903999567032\n",
      "Epoch 16514/30000 Training Loss: 0.06967155635356903\n",
      "Epoch 16515/30000 Training Loss: 0.08823438733816147\n",
      "Epoch 16516/30000 Training Loss: 0.06360917538404465\n",
      "Epoch 16517/30000 Training Loss: 0.08961233496665955\n",
      "Epoch 16518/30000 Training Loss: 0.07007811963558197\n",
      "Epoch 16519/30000 Training Loss: 0.07522737979888916\n",
      "Epoch 16520/30000 Training Loss: 0.09728336334228516\n",
      "Epoch 16520/30000 Validation Loss: 0.06716779619455338\n",
      "Epoch 16521/30000 Training Loss: 0.07007453590631485\n",
      "Epoch 16522/30000 Training Loss: 0.07638197392225266\n",
      "Epoch 16523/30000 Training Loss: 0.058299023658037186\n",
      "Epoch 16524/30000 Training Loss: 0.08231978863477707\n",
      "Epoch 16525/30000 Training Loss: 0.08075222373008728\n",
      "Epoch 16526/30000 Training Loss: 0.06015824154019356\n",
      "Epoch 16527/30000 Training Loss: 0.09229204803705215\n",
      "Epoch 16528/30000 Training Loss: 0.07478898763656616\n",
      "Epoch 16529/30000 Training Loss: 0.073767751455307\n",
      "Epoch 16530/30000 Training Loss: 0.08927243947982788\n",
      "Epoch 16530/30000 Validation Loss: 0.0702691301703453\n",
      "Epoch 16531/30000 Training Loss: 0.06276785582304001\n",
      "Epoch 16532/30000 Training Loss: 0.06817395240068436\n",
      "Epoch 16533/30000 Training Loss: 0.08600219339132309\n",
      "Epoch 16534/30000 Training Loss: 0.0628981962800026\n",
      "Epoch 16535/30000 Training Loss: 0.05966635048389435\n",
      "Epoch 16536/30000 Training Loss: 0.06501903384923935\n",
      "Epoch 16537/30000 Training Loss: 0.08225025981664658\n",
      "Epoch 16538/30000 Training Loss: 0.0685378685593605\n",
      "Epoch 16539/30000 Training Loss: 0.0873185396194458\n",
      "Epoch 16540/30000 Training Loss: 0.06453588604927063\n",
      "Epoch 16540/30000 Validation Loss: 0.07821585237979889\n",
      "Epoch 16541/30000 Training Loss: 0.07497154921293259\n",
      "Epoch 16542/30000 Training Loss: 0.07294873148202896\n",
      "Epoch 16543/30000 Training Loss: 0.0756605789065361\n",
      "Epoch 16544/30000 Training Loss: 0.06784132868051529\n",
      "Epoch 16545/30000 Training Loss: 0.05832470953464508\n",
      "Epoch 16546/30000 Training Loss: 0.08367625623941422\n",
      "Epoch 16547/30000 Training Loss: 0.06876762956380844\n",
      "Epoch 16548/30000 Training Loss: 0.06096714735031128\n",
      "Epoch 16549/30000 Training Loss: 0.06640461832284927\n",
      "Epoch 16550/30000 Training Loss: 0.10049421340227127\n",
      "Epoch 16550/30000 Validation Loss: 0.07393128424882889\n",
      "Epoch 16551/30000 Training Loss: 0.07960812002420425\n",
      "Epoch 16552/30000 Training Loss: 0.06741795688867569\n",
      "Epoch 16553/30000 Training Loss: 0.07206148654222488\n",
      "Epoch 16554/30000 Training Loss: 0.07853229343891144\n",
      "Epoch 16555/30000 Training Loss: 0.08200659602880478\n",
      "Epoch 16556/30000 Training Loss: 0.08343043178319931\n",
      "Epoch 16557/30000 Training Loss: 0.07122471928596497\n",
      "Epoch 16558/30000 Training Loss: 0.08201301842927933\n",
      "Epoch 16559/30000 Training Loss: 0.06589938700199127\n",
      "Epoch 16560/30000 Training Loss: 0.06747410446405411\n",
      "Epoch 16560/30000 Validation Loss: 0.10115044564008713\n",
      "Epoch 16561/30000 Training Loss: 0.06803112477064133\n",
      "Epoch 16562/30000 Training Loss: 0.07422469556331635\n",
      "Epoch 16563/30000 Training Loss: 0.07383500039577484\n",
      "Epoch 16564/30000 Training Loss: 0.07703623175621033\n",
      "Epoch 16565/30000 Training Loss: 0.08470674604177475\n",
      "Epoch 16566/30000 Training Loss: 0.06795772910118103\n",
      "Epoch 16567/30000 Training Loss: 0.07241586595773697\n",
      "Epoch 16568/30000 Training Loss: 0.06373361498117447\n",
      "Epoch 16569/30000 Training Loss: 0.057683467864990234\n",
      "Epoch 16570/30000 Training Loss: 0.10682809352874756\n",
      "Epoch 16570/30000 Validation Loss: 0.0735580325126648\n",
      "Epoch 16571/30000 Training Loss: 0.07845482975244522\n",
      "Epoch 16572/30000 Training Loss: 0.09026926010847092\n",
      "Epoch 16573/30000 Training Loss: 0.0880793035030365\n",
      "Epoch 16574/30000 Training Loss: 0.0805245041847229\n",
      "Epoch 16575/30000 Training Loss: 0.05466543510556221\n",
      "Epoch 16576/30000 Training Loss: 0.06220690533518791\n",
      "Epoch 16577/30000 Training Loss: 0.06402226537466049\n",
      "Epoch 16578/30000 Training Loss: 0.06229710578918457\n",
      "Epoch 16579/30000 Training Loss: 0.09944994002580643\n",
      "Epoch 16580/30000 Training Loss: 0.07981729507446289\n",
      "Epoch 16580/30000 Validation Loss: 0.06433210521936417\n",
      "Epoch 16581/30000 Training Loss: 0.06819278746843338\n",
      "Epoch 16582/30000 Training Loss: 0.05598210170865059\n",
      "Epoch 16583/30000 Training Loss: 0.07986781001091003\n",
      "Epoch 16584/30000 Training Loss: 0.07313422113656998\n",
      "Epoch 16585/30000 Training Loss: 0.09980166703462601\n",
      "Epoch 16586/30000 Training Loss: 0.07424011081457138\n",
      "Epoch 16587/30000 Training Loss: 0.08503907918930054\n",
      "Epoch 16588/30000 Training Loss: 0.09940154105424881\n",
      "Epoch 16589/30000 Training Loss: 0.08564900606870651\n",
      "Epoch 16590/30000 Training Loss: 0.0537627674639225\n",
      "Epoch 16590/30000 Validation Loss: 0.07754149287939072\n",
      "Epoch 16591/30000 Training Loss: 0.05833795294165611\n",
      "Epoch 16592/30000 Training Loss: 0.08556943386793137\n",
      "Epoch 16593/30000 Training Loss: 0.06497332453727722\n",
      "Epoch 16594/30000 Training Loss: 0.07653104513883591\n",
      "Epoch 16595/30000 Training Loss: 0.0746883675456047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16596/30000 Training Loss: 0.08047478646039963\n",
      "Epoch 16597/30000 Training Loss: 0.07201887667179108\n",
      "Epoch 16598/30000 Training Loss: 0.07361318916082382\n",
      "Epoch 16599/30000 Training Loss: 0.07455577701330185\n",
      "Epoch 16600/30000 Training Loss: 0.07356143742799759\n",
      "Epoch 16600/30000 Validation Loss: 0.0692858174443245\n",
      "Epoch 16601/30000 Training Loss: 0.058794405311346054\n",
      "Epoch 16602/30000 Training Loss: 0.07523805648088455\n",
      "Epoch 16603/30000 Training Loss: 0.07161449640989304\n",
      "Epoch 16604/30000 Training Loss: 0.05422795191407204\n",
      "Epoch 16605/30000 Training Loss: 0.055394966155290604\n",
      "Epoch 16606/30000 Training Loss: 0.07152605056762695\n",
      "Epoch 16607/30000 Training Loss: 0.07507642358541489\n",
      "Epoch 16608/30000 Training Loss: 0.06960556656122208\n",
      "Epoch 16609/30000 Training Loss: 0.06781140714883804\n",
      "Epoch 16610/30000 Training Loss: 0.08108487725257874\n",
      "Epoch 16610/30000 Validation Loss: 0.08038029819726944\n",
      "Epoch 16611/30000 Training Loss: 0.06364437192678452\n",
      "Epoch 16612/30000 Training Loss: 0.08521950989961624\n",
      "Epoch 16613/30000 Training Loss: 0.08053858578205109\n",
      "Epoch 16614/30000 Training Loss: 0.06084759160876274\n",
      "Epoch 16615/30000 Training Loss: 0.06279648840427399\n",
      "Epoch 16616/30000 Training Loss: 0.06048441305756569\n",
      "Epoch 16617/30000 Training Loss: 0.07119491696357727\n",
      "Epoch 16618/30000 Training Loss: 0.07193639874458313\n",
      "Epoch 16619/30000 Training Loss: 0.07249989360570908\n",
      "Epoch 16620/30000 Training Loss: 0.05822819471359253\n",
      "Epoch 16620/30000 Validation Loss: 0.09586858749389648\n",
      "Epoch 16621/30000 Training Loss: 0.07671770453453064\n",
      "Epoch 16622/30000 Training Loss: 0.07376202195882797\n",
      "Epoch 16623/30000 Training Loss: 0.07816176861524582\n",
      "Epoch 16624/30000 Training Loss: 0.05762280151247978\n",
      "Epoch 16625/30000 Training Loss: 0.05964461341500282\n",
      "Epoch 16626/30000 Training Loss: 0.07627909630537033\n",
      "Epoch 16627/30000 Training Loss: 0.07034161686897278\n",
      "Epoch 16628/30000 Training Loss: 0.06593929976224899\n",
      "Epoch 16629/30000 Training Loss: 0.06717344373464584\n",
      "Epoch 16630/30000 Training Loss: 0.06358177214860916\n",
      "Epoch 16630/30000 Validation Loss: 0.09180211275815964\n",
      "Epoch 16631/30000 Training Loss: 0.07447239756584167\n",
      "Epoch 16632/30000 Training Loss: 0.07926259189844131\n",
      "Epoch 16633/30000 Training Loss: 0.07186079025268555\n",
      "Epoch 16634/30000 Training Loss: 0.0673569068312645\n",
      "Epoch 16635/30000 Training Loss: 0.06621376425027847\n",
      "Epoch 16636/30000 Training Loss: 0.06256154179573059\n",
      "Epoch 16637/30000 Training Loss: 0.06410050392150879\n",
      "Epoch 16638/30000 Training Loss: 0.057893723249435425\n",
      "Epoch 16639/30000 Training Loss: 0.08268726617097855\n",
      "Epoch 16640/30000 Training Loss: 0.06085417792201042\n",
      "Epoch 16640/30000 Validation Loss: 0.07482832670211792\n",
      "Epoch 16641/30000 Training Loss: 0.0688624382019043\n",
      "Epoch 16642/30000 Training Loss: 0.07656291872262955\n",
      "Epoch 16643/30000 Training Loss: 0.07070382684469223\n",
      "Epoch 16644/30000 Training Loss: 0.08482829481363297\n",
      "Epoch 16645/30000 Training Loss: 0.0694882944226265\n",
      "Epoch 16646/30000 Training Loss: 0.08884575217962265\n",
      "Epoch 16647/30000 Training Loss: 0.07155463099479675\n",
      "Epoch 16648/30000 Training Loss: 0.05904088541865349\n",
      "Epoch 16649/30000 Training Loss: 0.06851113587617874\n",
      "Epoch 16650/30000 Training Loss: 0.08568332344293594\n",
      "Epoch 16650/30000 Validation Loss: 0.09632109850645065\n",
      "Epoch 16651/30000 Training Loss: 0.05601939186453819\n",
      "Epoch 16652/30000 Training Loss: 0.08761029690504074\n",
      "Epoch 16653/30000 Training Loss: 0.08184602856636047\n",
      "Epoch 16654/30000 Training Loss: 0.06939537078142166\n",
      "Epoch 16655/30000 Training Loss: 0.06021834909915924\n",
      "Epoch 16656/30000 Training Loss: 0.10256794095039368\n",
      "Epoch 16657/30000 Training Loss: 0.06412681192159653\n",
      "Epoch 16658/30000 Training Loss: 0.07851362228393555\n",
      "Epoch 16659/30000 Training Loss: 0.09199734777212143\n",
      "Epoch 16660/30000 Training Loss: 0.07195878028869629\n",
      "Epoch 16660/30000 Validation Loss: 0.06311160326004028\n",
      "Epoch 16661/30000 Training Loss: 0.07877977937459946\n",
      "Epoch 16662/30000 Training Loss: 0.05919767916202545\n",
      "Epoch 16663/30000 Training Loss: 0.07682766765356064\n",
      "Epoch 16664/30000 Training Loss: 0.06697680801153183\n",
      "Epoch 16665/30000 Training Loss: 0.08429312705993652\n",
      "Epoch 16666/30000 Training Loss: 0.07111867517232895\n",
      "Epoch 16667/30000 Training Loss: 0.0744134709239006\n",
      "Epoch 16668/30000 Training Loss: 0.09290599077939987\n",
      "Epoch 16669/30000 Training Loss: 0.07012518495321274\n",
      "Epoch 16670/30000 Training Loss: 0.07669001072645187\n",
      "Epoch 16670/30000 Validation Loss: 0.06677524000406265\n",
      "Epoch 16671/30000 Training Loss: 0.0799207016825676\n",
      "Epoch 16672/30000 Training Loss: 0.08707576245069504\n",
      "Epoch 16673/30000 Training Loss: 0.06845446676015854\n",
      "Epoch 16674/30000 Training Loss: 0.08915217965841293\n",
      "Epoch 16675/30000 Training Loss: 0.06041986122727394\n",
      "Epoch 16676/30000 Training Loss: 0.09094849228858948\n",
      "Epoch 16677/30000 Training Loss: 0.07355726510286331\n",
      "Epoch 16678/30000 Training Loss: 0.0704960897564888\n",
      "Epoch 16679/30000 Training Loss: 0.06359415501356125\n",
      "Epoch 16680/30000 Training Loss: 0.06137687340378761\n",
      "Epoch 16680/30000 Validation Loss: 0.07458948343992233\n",
      "Epoch 16681/30000 Training Loss: 0.0867791697382927\n",
      "Epoch 16682/30000 Training Loss: 0.05688886344432831\n",
      "Epoch 16683/30000 Training Loss: 0.07760203629732132\n",
      "Epoch 16684/30000 Training Loss: 0.06416796892881393\n",
      "Epoch 16685/30000 Training Loss: 0.0519932359457016\n",
      "Epoch 16686/30000 Training Loss: 0.0702734887599945\n",
      "Epoch 16687/30000 Training Loss: 0.08435508608818054\n",
      "Epoch 16688/30000 Training Loss: 0.06856203824281693\n",
      "Epoch 16689/30000 Training Loss: 0.07518178969621658\n",
      "Epoch 16690/30000 Training Loss: 0.0709749385714531\n",
      "Epoch 16690/30000 Validation Loss: 0.0720340684056282\n",
      "Epoch 16691/30000 Training Loss: 0.08819053322076797\n",
      "Epoch 16692/30000 Training Loss: 0.07507466524839401\n",
      "Epoch 16693/30000 Training Loss: 0.07646983116865158\n",
      "Epoch 16694/30000 Training Loss: 0.07244671136140823\n",
      "Epoch 16695/30000 Training Loss: 0.0648060217499733\n",
      "Epoch 16696/30000 Training Loss: 0.07446140795946121\n",
      "Epoch 16697/30000 Training Loss: 0.08083941787481308\n",
      "Epoch 16698/30000 Training Loss: 0.07106845825910568\n",
      "Epoch 16699/30000 Training Loss: 0.08146891742944717\n",
      "Epoch 16700/30000 Training Loss: 0.07356712967157364\n",
      "Epoch 16700/30000 Validation Loss: 0.08312514424324036\n",
      "Epoch 16701/30000 Training Loss: 0.05982678756117821\n",
      "Epoch 16702/30000 Training Loss: 0.0839606299996376\n",
      "Epoch 16703/30000 Training Loss: 0.08403214812278748\n",
      "Epoch 16704/30000 Training Loss: 0.09332969784736633\n",
      "Epoch 16705/30000 Training Loss: 0.07708669453859329\n",
      "Epoch 16706/30000 Training Loss: 0.07995180040597916\n",
      "Epoch 16707/30000 Training Loss: 0.04941899701952934\n",
      "Epoch 16708/30000 Training Loss: 0.08286618441343307\n",
      "Epoch 16709/30000 Training Loss: 0.07402033358812332\n",
      "Epoch 16710/30000 Training Loss: 0.07423976808786392\n",
      "Epoch 16710/30000 Validation Loss: 0.07093875855207443\n",
      "Epoch 16711/30000 Training Loss: 0.085210420191288\n",
      "Epoch 16712/30000 Training Loss: 0.07031380385160446\n",
      "Epoch 16713/30000 Training Loss: 0.09171203523874283\n",
      "Epoch 16714/30000 Training Loss: 0.08172964304685593\n",
      "Epoch 16715/30000 Training Loss: 0.07268539816141129\n",
      "Epoch 16716/30000 Training Loss: 0.0751628428697586\n",
      "Epoch 16717/30000 Training Loss: 0.08691039681434631\n",
      "Epoch 16718/30000 Training Loss: 0.06357141584157944\n",
      "Epoch 16719/30000 Training Loss: 0.07428817451000214\n",
      "Epoch 16720/30000 Training Loss: 0.07448991388082504\n",
      "Epoch 16720/30000 Validation Loss: 0.0749192014336586\n",
      "Epoch 16721/30000 Training Loss: 0.06769222021102905\n",
      "Epoch 16722/30000 Training Loss: 0.07806187123060226\n",
      "Epoch 16723/30000 Training Loss: 0.06228673458099365\n",
      "Epoch 16724/30000 Training Loss: 0.06929953396320343\n",
      "Epoch 16725/30000 Training Loss: 0.09409189969301224\n",
      "Epoch 16726/30000 Training Loss: 0.06658750772476196\n",
      "Epoch 16727/30000 Training Loss: 0.07305821776390076\n",
      "Epoch 16728/30000 Training Loss: 0.07249090820550919\n",
      "Epoch 16729/30000 Training Loss: 0.06699242442846298\n",
      "Epoch 16730/30000 Training Loss: 0.07751363515853882\n",
      "Epoch 16730/30000 Validation Loss: 0.08634809404611588\n",
      "Epoch 16731/30000 Training Loss: 0.07648075371980667\n",
      "Epoch 16732/30000 Training Loss: 0.07270291447639465\n",
      "Epoch 16733/30000 Training Loss: 0.08725716918706894\n",
      "Epoch 16734/30000 Training Loss: 0.0902593731880188\n",
      "Epoch 16735/30000 Training Loss: 0.07239076495170593\n",
      "Epoch 16736/30000 Training Loss: 0.09099499136209488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16737/30000 Training Loss: 0.09140563756227493\n",
      "Epoch 16738/30000 Training Loss: 0.0664684846997261\n",
      "Epoch 16739/30000 Training Loss: 0.07215797156095505\n",
      "Epoch 16740/30000 Training Loss: 0.07593682408332825\n",
      "Epoch 16740/30000 Validation Loss: 0.05734535679221153\n",
      "Epoch 16741/30000 Training Loss: 0.09799137711524963\n",
      "Epoch 16742/30000 Training Loss: 0.06332162022590637\n",
      "Epoch 16743/30000 Training Loss: 0.06661981344223022\n",
      "Epoch 16744/30000 Training Loss: 0.07709744572639465\n",
      "Epoch 16745/30000 Training Loss: 0.0642012283205986\n",
      "Epoch 16746/30000 Training Loss: 0.0679703950881958\n",
      "Epoch 16747/30000 Training Loss: 0.07718679308891296\n",
      "Epoch 16748/30000 Training Loss: 0.0607302300632\n",
      "Epoch 16749/30000 Training Loss: 0.07987579703330994\n",
      "Epoch 16750/30000 Training Loss: 0.0785890519618988\n",
      "Epoch 16750/30000 Validation Loss: 0.07110030204057693\n",
      "Epoch 16751/30000 Training Loss: 0.08263127505779266\n",
      "Epoch 16752/30000 Training Loss: 0.06344253569841385\n",
      "Epoch 16753/30000 Training Loss: 0.08063223958015442\n",
      "Epoch 16754/30000 Training Loss: 0.07981611043214798\n",
      "Epoch 16755/30000 Training Loss: 0.05753374472260475\n",
      "Epoch 16756/30000 Training Loss: 0.06518613547086716\n",
      "Epoch 16757/30000 Training Loss: 0.05021760240197182\n",
      "Epoch 16758/30000 Training Loss: 0.07842009514570236\n",
      "Epoch 16759/30000 Training Loss: 0.06454844772815704\n",
      "Epoch 16760/30000 Training Loss: 0.0625695288181305\n",
      "Epoch 16760/30000 Validation Loss: 0.0882752314209938\n",
      "Epoch 16761/30000 Training Loss: 0.08729905635118484\n",
      "Epoch 16762/30000 Training Loss: 0.09378401190042496\n",
      "Epoch 16763/30000 Training Loss: 0.054479897022247314\n",
      "Epoch 16764/30000 Training Loss: 0.08565748482942581\n",
      "Epoch 16765/30000 Training Loss: 0.07527255266904831\n",
      "Epoch 16766/30000 Training Loss: 0.05742059275507927\n",
      "Epoch 16767/30000 Training Loss: 0.07568051666021347\n",
      "Epoch 16768/30000 Training Loss: 0.06969394534826279\n",
      "Epoch 16769/30000 Training Loss: 0.09964340925216675\n",
      "Epoch 16770/30000 Training Loss: 0.06437298655509949\n",
      "Epoch 16770/30000 Validation Loss: 0.07459226995706558\n",
      "Epoch 16771/30000 Training Loss: 0.0690954253077507\n",
      "Epoch 16772/30000 Training Loss: 0.0717429518699646\n",
      "Epoch 16773/30000 Training Loss: 0.06840866804122925\n",
      "Epoch 16774/30000 Training Loss: 0.08087846636772156\n",
      "Epoch 16775/30000 Training Loss: 0.05027928575873375\n",
      "Epoch 16776/30000 Training Loss: 0.0823194682598114\n",
      "Epoch 16777/30000 Training Loss: 0.04881750047206879\n",
      "Epoch 16778/30000 Training Loss: 0.06001724302768707\n",
      "Epoch 16779/30000 Training Loss: 0.060580115765333176\n",
      "Epoch 16780/30000 Training Loss: 0.08391845971345901\n",
      "Epoch 16780/30000 Validation Loss: 0.0879611074924469\n",
      "Epoch 16781/30000 Training Loss: 0.0739230290055275\n",
      "Epoch 16782/30000 Training Loss: 0.07091157883405685\n",
      "Epoch 16783/30000 Training Loss: 0.0692533329129219\n",
      "Epoch 16784/30000 Training Loss: 0.06980415433645248\n",
      "Epoch 16785/30000 Training Loss: 0.09892010688781738\n",
      "Epoch 16786/30000 Training Loss: 0.06803184002637863\n",
      "Epoch 16787/30000 Training Loss: 0.05732965096831322\n",
      "Epoch 16788/30000 Training Loss: 0.06262297183275223\n",
      "Epoch 16789/30000 Training Loss: 0.07697144150733948\n",
      "Epoch 16790/30000 Training Loss: 0.07552351802587509\n",
      "Epoch 16790/30000 Validation Loss: 0.08500680327415466\n",
      "Epoch 16791/30000 Training Loss: 0.0845656469464302\n",
      "Epoch 16792/30000 Training Loss: 0.10189700871706009\n",
      "Epoch 16793/30000 Training Loss: 0.08297182619571686\n",
      "Epoch 16794/30000 Training Loss: 0.09083785861730576\n",
      "Epoch 16795/30000 Training Loss: 0.05775229260325432\n",
      "Epoch 16796/30000 Training Loss: 0.07325679808855057\n",
      "Epoch 16797/30000 Training Loss: 0.05873372033238411\n",
      "Epoch 16798/30000 Training Loss: 0.06124318763613701\n",
      "Epoch 16799/30000 Training Loss: 0.07631806284189224\n",
      "Epoch 16800/30000 Training Loss: 0.06168777868151665\n",
      "Epoch 16800/30000 Validation Loss: 0.07045867294073105\n",
      "Epoch 16801/30000 Training Loss: 0.07233694940805435\n",
      "Epoch 16802/30000 Training Loss: 0.07752294093370438\n",
      "Epoch 16803/30000 Training Loss: 0.07246685028076172\n",
      "Epoch 16804/30000 Training Loss: 0.07395226508378983\n",
      "Epoch 16805/30000 Training Loss: 0.04797611013054848\n",
      "Epoch 16806/30000 Training Loss: 0.08735746890306473\n",
      "Epoch 16807/30000 Training Loss: 0.061524659395217896\n",
      "Epoch 16808/30000 Training Loss: 0.06480909138917923\n",
      "Epoch 16809/30000 Training Loss: 0.067058265209198\n",
      "Epoch 16810/30000 Training Loss: 0.07137367129325867\n",
      "Epoch 16810/30000 Validation Loss: 0.07851282507181168\n",
      "Epoch 16811/30000 Training Loss: 0.0709805116057396\n",
      "Epoch 16812/30000 Training Loss: 0.08261952549219131\n",
      "Epoch 16813/30000 Training Loss: 0.07784587144851685\n",
      "Epoch 16814/30000 Training Loss: 0.07010556757450104\n",
      "Epoch 16815/30000 Training Loss: 0.07798098772764206\n",
      "Epoch 16816/30000 Training Loss: 0.06372863799333572\n",
      "Epoch 16817/30000 Training Loss: 0.079350046813488\n",
      "Epoch 16818/30000 Training Loss: 0.08119233697652817\n",
      "Epoch 16819/30000 Training Loss: 0.07329393178224564\n",
      "Epoch 16820/30000 Training Loss: 0.09285757690668106\n",
      "Epoch 16820/30000 Validation Loss: 0.08250399678945541\n",
      "Epoch 16821/30000 Training Loss: 0.07000000029802322\n",
      "Epoch 16822/30000 Training Loss: 0.08349382132291794\n",
      "Epoch 16823/30000 Training Loss: 0.07968158274888992\n",
      "Epoch 16824/30000 Training Loss: 0.07045725733041763\n",
      "Epoch 16825/30000 Training Loss: 0.0676024928689003\n",
      "Epoch 16826/30000 Training Loss: 0.06145414337515831\n",
      "Epoch 16827/30000 Training Loss: 0.08249519020318985\n",
      "Epoch 16828/30000 Training Loss: 0.07167299091815948\n",
      "Epoch 16829/30000 Training Loss: 0.07065243273973465\n",
      "Epoch 16830/30000 Training Loss: 0.06355444341897964\n",
      "Epoch 16830/30000 Validation Loss: 0.05527601018548012\n",
      "Epoch 16831/30000 Training Loss: 0.05373698100447655\n",
      "Epoch 16832/30000 Training Loss: 0.06798160076141357\n",
      "Epoch 16833/30000 Training Loss: 0.0785570964217186\n",
      "Epoch 16834/30000 Training Loss: 0.08726400882005692\n",
      "Epoch 16835/30000 Training Loss: 0.07804261893033981\n",
      "Epoch 16836/30000 Training Loss: 0.06678364425897598\n",
      "Epoch 16837/30000 Training Loss: 0.07140053063631058\n",
      "Epoch 16838/30000 Training Loss: 0.06867331266403198\n",
      "Epoch 16839/30000 Training Loss: 0.0791391134262085\n",
      "Epoch 16840/30000 Training Loss: 0.06173178553581238\n",
      "Epoch 16840/30000 Validation Loss: 0.08419928699731827\n",
      "Epoch 16841/30000 Training Loss: 0.08965297788381577\n",
      "Epoch 16842/30000 Training Loss: 0.06518610566854477\n",
      "Epoch 16843/30000 Training Loss: 0.08555198460817337\n",
      "Epoch 16844/30000 Training Loss: 0.07920204848051071\n",
      "Epoch 16845/30000 Training Loss: 0.08759673684835434\n",
      "Epoch 16846/30000 Training Loss: 0.09216628223657608\n",
      "Epoch 16847/30000 Training Loss: 0.06513004750013351\n",
      "Epoch 16848/30000 Training Loss: 0.09084341675043106\n",
      "Epoch 16849/30000 Training Loss: 0.06985623389482498\n",
      "Epoch 16850/30000 Training Loss: 0.0840870812535286\n",
      "Epoch 16850/30000 Validation Loss: 0.08126819133758545\n",
      "Epoch 16851/30000 Training Loss: 0.06276801228523254\n",
      "Epoch 16852/30000 Training Loss: 0.08423032611608505\n",
      "Epoch 16853/30000 Training Loss: 0.07210849970579147\n",
      "Epoch 16854/30000 Training Loss: 0.07728490233421326\n",
      "Epoch 16855/30000 Training Loss: 0.07766299694776535\n",
      "Epoch 16856/30000 Training Loss: 0.07432069629430771\n",
      "Epoch 16857/30000 Training Loss: 0.06960143893957138\n",
      "Epoch 16858/30000 Training Loss: 0.08734261989593506\n",
      "Epoch 16859/30000 Training Loss: 0.06519931554794312\n",
      "Epoch 16860/30000 Training Loss: 0.08810678124427795\n",
      "Epoch 16860/30000 Validation Loss: 0.0657092034816742\n",
      "Epoch 16861/30000 Training Loss: 0.06384245306253433\n",
      "Epoch 16862/30000 Training Loss: 0.06667990237474442\n",
      "Epoch 16863/30000 Training Loss: 0.07309038192033768\n",
      "Epoch 16864/30000 Training Loss: 0.07703384011983871\n",
      "Epoch 16865/30000 Training Loss: 0.0696808397769928\n",
      "Epoch 16866/30000 Training Loss: 0.06739306449890137\n",
      "Epoch 16867/30000 Training Loss: 0.06216851994395256\n",
      "Epoch 16868/30000 Training Loss: 0.07244930416345596\n",
      "Epoch 16869/30000 Training Loss: 0.06455665081739426\n",
      "Epoch 16870/30000 Training Loss: 0.08546581119298935\n",
      "Epoch 16870/30000 Validation Loss: 0.06073324382305145\n",
      "Epoch 16871/30000 Training Loss: 0.07158355414867401\n",
      "Epoch 16872/30000 Training Loss: 0.058508992195129395\n",
      "Epoch 16873/30000 Training Loss: 0.0705370232462883\n",
      "Epoch 16874/30000 Training Loss: 0.07084440439939499\n",
      "Epoch 16875/30000 Training Loss: 0.0827239379286766\n",
      "Epoch 16876/30000 Training Loss: 0.05917027220129967\n",
      "Epoch 16877/30000 Training Loss: 0.09950622171163559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16878/30000 Training Loss: 0.07479730993509293\n",
      "Epoch 16879/30000 Training Loss: 0.0677296444773674\n",
      "Epoch 16880/30000 Training Loss: 0.07913130521774292\n",
      "Epoch 16880/30000 Validation Loss: 0.08072695136070251\n",
      "Epoch 16881/30000 Training Loss: 0.09194978326559067\n",
      "Epoch 16882/30000 Training Loss: 0.0757158026099205\n",
      "Epoch 16883/30000 Training Loss: 0.0490105040371418\n",
      "Epoch 16884/30000 Training Loss: 0.07406976819038391\n",
      "Epoch 16885/30000 Training Loss: 0.06634604930877686\n",
      "Epoch 16886/30000 Training Loss: 0.07228022068738937\n",
      "Epoch 16887/30000 Training Loss: 0.07665673643350601\n",
      "Epoch 16888/30000 Training Loss: 0.0738091990351677\n",
      "Epoch 16889/30000 Training Loss: 0.06739767640829086\n",
      "Epoch 16890/30000 Training Loss: 0.0644381195306778\n",
      "Epoch 16890/30000 Validation Loss: 0.0775715708732605\n",
      "Epoch 16891/30000 Training Loss: 0.08014211058616638\n",
      "Epoch 16892/30000 Training Loss: 0.06309085339307785\n",
      "Epoch 16893/30000 Training Loss: 0.07601994276046753\n",
      "Epoch 16894/30000 Training Loss: 0.057836007326841354\n",
      "Epoch 16895/30000 Training Loss: 0.0836649164557457\n",
      "Epoch 16896/30000 Training Loss: 0.07291451841592789\n",
      "Epoch 16897/30000 Training Loss: 0.06599416583776474\n",
      "Epoch 16898/30000 Training Loss: 0.06229319050908089\n",
      "Epoch 16899/30000 Training Loss: 0.06722196936607361\n",
      "Epoch 16900/30000 Training Loss: 0.07302670925855637\n",
      "Epoch 16900/30000 Validation Loss: 0.09439730644226074\n",
      "Epoch 16901/30000 Training Loss: 0.07151824980974197\n",
      "Epoch 16902/30000 Training Loss: 0.060514554381370544\n",
      "Epoch 16903/30000 Training Loss: 0.06898737698793411\n",
      "Epoch 16904/30000 Training Loss: 0.06795072555541992\n",
      "Epoch 16905/30000 Training Loss: 0.06722612679004669\n",
      "Epoch 16906/30000 Training Loss: 0.07629680633544922\n",
      "Epoch 16907/30000 Training Loss: 0.050297632813453674\n",
      "Epoch 16908/30000 Training Loss: 0.06722988933324814\n",
      "Epoch 16909/30000 Training Loss: 0.05565948411822319\n",
      "Epoch 16910/30000 Training Loss: 0.06741324067115784\n",
      "Epoch 16910/30000 Validation Loss: 0.07008634507656097\n",
      "Epoch 16911/30000 Training Loss: 0.0696554109454155\n",
      "Epoch 16912/30000 Training Loss: 0.07152485847473145\n",
      "Epoch 16913/30000 Training Loss: 0.08998184651136398\n",
      "Epoch 16914/30000 Training Loss: 0.06292059272527695\n",
      "Epoch 16915/30000 Training Loss: 0.05605708435177803\n",
      "Epoch 16916/30000 Training Loss: 0.06893791258335114\n",
      "Epoch 16917/30000 Training Loss: 0.0727287158370018\n",
      "Epoch 16918/30000 Training Loss: 0.07734056562185287\n",
      "Epoch 16919/30000 Training Loss: 0.07578215003013611\n",
      "Epoch 16920/30000 Training Loss: 0.06437162309885025\n",
      "Epoch 16920/30000 Validation Loss: 0.08083450049161911\n",
      "Epoch 16921/30000 Training Loss: 0.064124695956707\n",
      "Epoch 16922/30000 Training Loss: 0.07258300483226776\n",
      "Epoch 16923/30000 Training Loss: 0.06419570744037628\n",
      "Epoch 16924/30000 Training Loss: 0.06333782523870468\n",
      "Epoch 16925/30000 Training Loss: 0.0735909715294838\n",
      "Epoch 16926/30000 Training Loss: 0.07291466742753983\n",
      "Epoch 16927/30000 Training Loss: 0.07071360945701599\n",
      "Epoch 16928/30000 Training Loss: 0.07168639451265335\n",
      "Epoch 16929/30000 Training Loss: 0.060384269803762436\n",
      "Epoch 16930/30000 Training Loss: 0.0711190328001976\n",
      "Epoch 16930/30000 Validation Loss: 0.08195250481367111\n",
      "Epoch 16931/30000 Training Loss: 0.07124854624271393\n",
      "Epoch 16932/30000 Training Loss: 0.06481271237134933\n",
      "Epoch 16933/30000 Training Loss: 0.07724351435899734\n",
      "Epoch 16934/30000 Training Loss: 0.07643591612577438\n",
      "Epoch 16935/30000 Training Loss: 0.05852832272648811\n",
      "Epoch 16936/30000 Training Loss: 0.09250485152006149\n",
      "Epoch 16937/30000 Training Loss: 0.07522765547037125\n",
      "Epoch 16938/30000 Training Loss: 0.06583195924758911\n",
      "Epoch 16939/30000 Training Loss: 0.07999235391616821\n",
      "Epoch 16940/30000 Training Loss: 0.08543131500482559\n",
      "Epoch 16940/30000 Validation Loss: 0.07890059798955917\n",
      "Epoch 16941/30000 Training Loss: 0.0686739906668663\n",
      "Epoch 16942/30000 Training Loss: 0.06396009773015976\n",
      "Epoch 16943/30000 Training Loss: 0.09398838877677917\n",
      "Epoch 16944/30000 Training Loss: 0.061162497848272324\n",
      "Epoch 16945/30000 Training Loss: 0.094493068754673\n",
      "Epoch 16946/30000 Training Loss: 0.07946605235338211\n",
      "Epoch 16947/30000 Training Loss: 0.06349625438451767\n",
      "Epoch 16948/30000 Training Loss: 0.05302984640002251\n",
      "Epoch 16949/30000 Training Loss: 0.06608960032463074\n",
      "Epoch 16950/30000 Training Loss: 0.07078243046998978\n",
      "Epoch 16950/30000 Validation Loss: 0.06428561359643936\n",
      "Epoch 16951/30000 Training Loss: 0.0863373875617981\n",
      "Epoch 16952/30000 Training Loss: 0.0910419449210167\n",
      "Epoch 16953/30000 Training Loss: 0.0922124907374382\n",
      "Epoch 16954/30000 Training Loss: 0.07868116348981857\n",
      "Epoch 16955/30000 Training Loss: 0.059042174369096756\n",
      "Epoch 16956/30000 Training Loss: 0.09589189291000366\n",
      "Epoch 16957/30000 Training Loss: 0.08861953020095825\n",
      "Epoch 16958/30000 Training Loss: 0.07641054689884186\n",
      "Epoch 16959/30000 Training Loss: 0.08886028081178665\n",
      "Epoch 16960/30000 Training Loss: 0.06388088315725327\n",
      "Epoch 16960/30000 Validation Loss: 0.0662573054432869\n",
      "Epoch 16961/30000 Training Loss: 0.06622572988271713\n",
      "Epoch 16962/30000 Training Loss: 0.057713452726602554\n",
      "Epoch 16963/30000 Training Loss: 0.07401030510663986\n",
      "Epoch 16964/30000 Training Loss: 0.08235016465187073\n",
      "Epoch 16965/30000 Training Loss: 0.06742233037948608\n",
      "Epoch 16966/30000 Training Loss: 0.08675462752580643\n",
      "Epoch 16967/30000 Training Loss: 0.075454942882061\n",
      "Epoch 16968/30000 Training Loss: 0.07358252257108688\n",
      "Epoch 16969/30000 Training Loss: 0.07457167655229568\n",
      "Epoch 16970/30000 Training Loss: 0.06893997639417648\n",
      "Epoch 16970/30000 Validation Loss: 0.08031325787305832\n",
      "Epoch 16971/30000 Training Loss: 0.08488180488348007\n",
      "Epoch 16972/30000 Training Loss: 0.05864052101969719\n",
      "Epoch 16973/30000 Training Loss: 0.06974130868911743\n",
      "Epoch 16974/30000 Training Loss: 0.053107697516679764\n",
      "Epoch 16975/30000 Training Loss: 0.07279805094003677\n",
      "Epoch 16976/30000 Training Loss: 0.06793177127838135\n",
      "Epoch 16977/30000 Training Loss: 0.06517750024795532\n",
      "Epoch 16978/30000 Training Loss: 0.06945531070232391\n",
      "Epoch 16979/30000 Training Loss: 0.07403028011322021\n",
      "Epoch 16980/30000 Training Loss: 0.0905143991112709\n",
      "Epoch 16980/30000 Validation Loss: 0.09581676870584488\n",
      "Epoch 16981/30000 Training Loss: 0.06708795577287674\n",
      "Epoch 16982/30000 Training Loss: 0.08679026365280151\n",
      "Epoch 16983/30000 Training Loss: 0.07413452118635178\n",
      "Epoch 16984/30000 Training Loss: 0.06585831940174103\n",
      "Epoch 16985/30000 Training Loss: 0.07782843708992004\n",
      "Epoch 16986/30000 Training Loss: 0.067983478307724\n",
      "Epoch 16987/30000 Training Loss: 0.07659035176038742\n",
      "Epoch 16988/30000 Training Loss: 0.084130197763443\n",
      "Epoch 16989/30000 Training Loss: 0.06719622015953064\n",
      "Epoch 16990/30000 Training Loss: 0.06653699278831482\n",
      "Epoch 16990/30000 Validation Loss: 0.07867413014173508\n",
      "Epoch 16991/30000 Training Loss: 0.09704210609197617\n",
      "Epoch 16992/30000 Training Loss: 0.0619521327316761\n",
      "Epoch 16993/30000 Training Loss: 0.07874936610460281\n",
      "Epoch 16994/30000 Training Loss: 0.080437071621418\n",
      "Epoch 16995/30000 Training Loss: 0.07644394040107727\n",
      "Epoch 16996/30000 Training Loss: 0.05910072848200798\n",
      "Epoch 16997/30000 Training Loss: 0.0593806616961956\n",
      "Epoch 16998/30000 Training Loss: 0.0752728059887886\n",
      "Epoch 16999/30000 Training Loss: 0.06657925248146057\n",
      "Epoch 17000/30000 Training Loss: 0.0789799913764\n",
      "Epoch 17000/30000 Validation Loss: 0.06449097394943237\n",
      "Epoch 17001/30000 Training Loss: 0.07495898753404617\n",
      "Epoch 17002/30000 Training Loss: 0.07623883336782455\n",
      "Epoch 17003/30000 Training Loss: 0.07933462411165237\n",
      "Epoch 17004/30000 Training Loss: 0.07540770620107651\n",
      "Epoch 17005/30000 Training Loss: 0.06705828756093979\n",
      "Epoch 17006/30000 Training Loss: 0.06725713610649109\n",
      "Epoch 17007/30000 Training Loss: 0.069923035800457\n",
      "Epoch 17008/30000 Training Loss: 0.07351159304380417\n",
      "Epoch 17009/30000 Training Loss: 0.07632488757371902\n",
      "Epoch 17010/30000 Training Loss: 0.0730629488825798\n",
      "Epoch 17010/30000 Validation Loss: 0.06912776082754135\n",
      "Epoch 17011/30000 Training Loss: 0.07603245228528976\n",
      "Epoch 17012/30000 Training Loss: 0.060999851673841476\n",
      "Epoch 17013/30000 Training Loss: 0.06855576485395432\n",
      "Epoch 17014/30000 Training Loss: 0.07657485455274582\n",
      "Epoch 17015/30000 Training Loss: 0.061337485909461975\n",
      "Epoch 17016/30000 Training Loss: 0.0709492415189743\n",
      "Epoch 17017/30000 Training Loss: 0.07750406861305237\n",
      "Epoch 17018/30000 Training Loss: 0.06427664309740067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17019/30000 Training Loss: 0.06483035534620285\n",
      "Epoch 17020/30000 Training Loss: 0.06446833163499832\n",
      "Epoch 17020/30000 Validation Loss: 0.06632748991250992\n",
      "Epoch 17021/30000 Training Loss: 0.06931620091199875\n",
      "Epoch 17022/30000 Training Loss: 0.10049409419298172\n",
      "Epoch 17023/30000 Training Loss: 0.07637128233909607\n",
      "Epoch 17024/30000 Training Loss: 0.07908076792955399\n",
      "Epoch 17025/30000 Training Loss: 0.06635361164808273\n",
      "Epoch 17026/30000 Training Loss: 0.11008608341217041\n",
      "Epoch 17027/30000 Training Loss: 0.06485746055841446\n",
      "Epoch 17028/30000 Training Loss: 0.06983204931020737\n",
      "Epoch 17029/30000 Training Loss: 0.07375527173280716\n",
      "Epoch 17030/30000 Training Loss: 0.08977127820253372\n",
      "Epoch 17030/30000 Validation Loss: 0.07572080940008163\n",
      "Epoch 17031/30000 Training Loss: 0.07859788835048676\n",
      "Epoch 17032/30000 Training Loss: 0.08881578594446182\n",
      "Epoch 17033/30000 Training Loss: 0.06167624890804291\n",
      "Epoch 17034/30000 Training Loss: 0.06618013232946396\n",
      "Epoch 17035/30000 Training Loss: 0.08030112832784653\n",
      "Epoch 17036/30000 Training Loss: 0.06647447496652603\n",
      "Epoch 17037/30000 Training Loss: 0.10307107120752335\n",
      "Epoch 17038/30000 Training Loss: 0.07331446558237076\n",
      "Epoch 17039/30000 Training Loss: 0.0668373703956604\n",
      "Epoch 17040/30000 Training Loss: 0.05993235111236572\n",
      "Epoch 17040/30000 Validation Loss: 0.06957469880580902\n",
      "Epoch 17041/30000 Training Loss: 0.06544256955385208\n",
      "Epoch 17042/30000 Training Loss: 0.09444548934698105\n",
      "Epoch 17043/30000 Training Loss: 0.09708324074745178\n",
      "Epoch 17044/30000 Training Loss: 0.07425091415643692\n",
      "Epoch 17045/30000 Training Loss: 0.07547955214977264\n",
      "Epoch 17046/30000 Training Loss: 0.08262553066015244\n",
      "Epoch 17047/30000 Training Loss: 0.06538226455450058\n",
      "Epoch 17048/30000 Training Loss: 0.07448171824216843\n",
      "Epoch 17049/30000 Training Loss: 0.0723460465669632\n",
      "Epoch 17050/30000 Training Loss: 0.05480046942830086\n",
      "Epoch 17050/30000 Validation Loss: 0.06525994092226028\n",
      "Epoch 17051/30000 Training Loss: 0.059836145490407944\n",
      "Epoch 17052/30000 Training Loss: 0.06898847967386246\n",
      "Epoch 17053/30000 Training Loss: 0.060905903577804565\n",
      "Epoch 17054/30000 Training Loss: 0.08513551950454712\n",
      "Epoch 17055/30000 Training Loss: 0.09992281347513199\n",
      "Epoch 17056/30000 Training Loss: 0.06576001644134521\n",
      "Epoch 17057/30000 Training Loss: 0.07782015204429626\n",
      "Epoch 17058/30000 Training Loss: 0.06996908038854599\n",
      "Epoch 17059/30000 Training Loss: 0.06698184460401535\n",
      "Epoch 17060/30000 Training Loss: 0.08814536780118942\n",
      "Epoch 17060/30000 Validation Loss: 0.08709516376256943\n",
      "Epoch 17061/30000 Training Loss: 0.056773364543914795\n",
      "Epoch 17062/30000 Training Loss: 0.07782400399446487\n",
      "Epoch 17063/30000 Training Loss: 0.06971425563097\n",
      "Epoch 17064/30000 Training Loss: 0.08473831415176392\n",
      "Epoch 17065/30000 Training Loss: 0.09136033803224564\n",
      "Epoch 17066/30000 Training Loss: 0.06227976083755493\n",
      "Epoch 17067/30000 Training Loss: 0.08363288640975952\n",
      "Epoch 17068/30000 Training Loss: 0.06360039860010147\n",
      "Epoch 17069/30000 Training Loss: 0.07935444265604019\n",
      "Epoch 17070/30000 Training Loss: 0.07746968418359756\n",
      "Epoch 17070/30000 Validation Loss: 0.06538740545511246\n",
      "Epoch 17071/30000 Training Loss: 0.07055876404047012\n",
      "Epoch 17072/30000 Training Loss: 0.07227339595556259\n",
      "Epoch 17073/30000 Training Loss: 0.0920986458659172\n",
      "Epoch 17074/30000 Training Loss: 0.058027882128953934\n",
      "Epoch 17075/30000 Training Loss: 0.06925737112760544\n",
      "Epoch 17076/30000 Training Loss: 0.06163409352302551\n",
      "Epoch 17077/30000 Training Loss: 0.06781283020973206\n",
      "Epoch 17078/30000 Training Loss: 0.057253628969192505\n",
      "Epoch 17079/30000 Training Loss: 0.05673171952366829\n",
      "Epoch 17080/30000 Training Loss: 0.07604652643203735\n",
      "Epoch 17080/30000 Validation Loss: 0.083152174949646\n",
      "Epoch 17081/30000 Training Loss: 0.05419531464576721\n",
      "Epoch 17082/30000 Training Loss: 0.06551125645637512\n",
      "Epoch 17083/30000 Training Loss: 0.07064465433359146\n",
      "Epoch 17084/30000 Training Loss: 0.07709652185440063\n",
      "Epoch 17085/30000 Training Loss: 0.07203102111816406\n",
      "Epoch 17086/30000 Training Loss: 0.06364519894123077\n",
      "Epoch 17087/30000 Training Loss: 0.07613581418991089\n",
      "Epoch 17088/30000 Training Loss: 0.0724017545580864\n",
      "Epoch 17089/30000 Training Loss: 0.06898549944162369\n",
      "Epoch 17090/30000 Training Loss: 0.09587622433900833\n",
      "Epoch 17090/30000 Validation Loss: 0.07837403565645218\n",
      "Epoch 17091/30000 Training Loss: 0.06633800268173218\n",
      "Epoch 17092/30000 Training Loss: 0.06930921226739883\n",
      "Epoch 17093/30000 Training Loss: 0.08965197950601578\n",
      "Epoch 17094/30000 Training Loss: 0.07372357696294785\n",
      "Epoch 17095/30000 Training Loss: 0.06878677755594254\n",
      "Epoch 17096/30000 Training Loss: 0.07050389051437378\n",
      "Epoch 17097/30000 Training Loss: 0.0838281512260437\n",
      "Epoch 17098/30000 Training Loss: 0.06819771975278854\n",
      "Epoch 17099/30000 Training Loss: 0.0640069767832756\n",
      "Epoch 17100/30000 Training Loss: 0.061066124588251114\n",
      "Epoch 17100/30000 Validation Loss: 0.047859787940979004\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.047859787940979004<=============\n",
      "Epoch 17101/30000 Training Loss: 0.06396802514791489\n",
      "Epoch 17102/30000 Training Loss: 0.07492529600858688\n",
      "Epoch 17103/30000 Training Loss: 0.06761068850755692\n",
      "Epoch 17104/30000 Training Loss: 0.07547435164451599\n",
      "Epoch 17105/30000 Training Loss: 0.06480652093887329\n",
      "Epoch 17106/30000 Training Loss: 0.08083204180002213\n",
      "Epoch 17107/30000 Training Loss: 0.07464665174484253\n",
      "Epoch 17108/30000 Training Loss: 0.06328114122152328\n",
      "Epoch 17109/30000 Training Loss: 0.0696420818567276\n",
      "Epoch 17110/30000 Training Loss: 0.07317225635051727\n",
      "Epoch 17110/30000 Validation Loss: 0.07567398995161057\n",
      "Epoch 17111/30000 Training Loss: 0.08746335655450821\n",
      "Epoch 17112/30000 Training Loss: 0.06892833113670349\n",
      "Epoch 17113/30000 Training Loss: 0.07851097732782364\n",
      "Epoch 17114/30000 Training Loss: 0.07104779034852982\n",
      "Epoch 17115/30000 Training Loss: 0.09164002537727356\n",
      "Epoch 17116/30000 Training Loss: 0.06383385509252548\n",
      "Epoch 17117/30000 Training Loss: 0.06808190047740936\n",
      "Epoch 17118/30000 Training Loss: 0.05805313587188721\n",
      "Epoch 17119/30000 Training Loss: 0.07112213224172592\n",
      "Epoch 17120/30000 Training Loss: 0.0772782638669014\n",
      "Epoch 17120/30000 Validation Loss: 0.06397683173418045\n",
      "Epoch 17121/30000 Training Loss: 0.08467990905046463\n",
      "Epoch 17122/30000 Training Loss: 0.06616344302892685\n",
      "Epoch 17123/30000 Training Loss: 0.09497380256652832\n",
      "Epoch 17124/30000 Training Loss: 0.06197885796427727\n",
      "Epoch 17125/30000 Training Loss: 0.06738840043544769\n",
      "Epoch 17126/30000 Training Loss: 0.08250824362039566\n",
      "Epoch 17127/30000 Training Loss: 0.08336194604635239\n",
      "Epoch 17128/30000 Training Loss: 0.06081883981823921\n",
      "Epoch 17129/30000 Training Loss: 0.08503308892250061\n",
      "Epoch 17130/30000 Training Loss: 0.07469476014375687\n",
      "Epoch 17130/30000 Validation Loss: 0.08031033724546432\n",
      "Epoch 17131/30000 Training Loss: 0.06031406298279762\n",
      "Epoch 17132/30000 Training Loss: 0.06262999027967453\n",
      "Epoch 17133/30000 Training Loss: 0.06044159457087517\n",
      "Epoch 17134/30000 Training Loss: 0.07147563248872757\n",
      "Epoch 17135/30000 Training Loss: 0.06421855837106705\n",
      "Epoch 17136/30000 Training Loss: 0.07437541335821152\n",
      "Epoch 17137/30000 Training Loss: 0.07651850581169128\n",
      "Epoch 17138/30000 Training Loss: 0.0763450562953949\n",
      "Epoch 17139/30000 Training Loss: 0.09520718455314636\n",
      "Epoch 17140/30000 Training Loss: 0.07908298820257187\n",
      "Epoch 17140/30000 Validation Loss: 0.08737915009260178\n",
      "Epoch 17141/30000 Training Loss: 0.09151231497526169\n",
      "Epoch 17142/30000 Training Loss: 0.08250182867050171\n",
      "Epoch 17143/30000 Training Loss: 0.08117633312940598\n",
      "Epoch 17144/30000 Training Loss: 0.08230853825807571\n",
      "Epoch 17145/30000 Training Loss: 0.07831793278455734\n",
      "Epoch 17146/30000 Training Loss: 0.06726624816656113\n",
      "Epoch 17147/30000 Training Loss: 0.06422116607427597\n",
      "Epoch 17148/30000 Training Loss: 0.06414195895195007\n",
      "Epoch 17149/30000 Training Loss: 0.058712150901556015\n",
      "Epoch 17150/30000 Training Loss: 0.08698219060897827\n",
      "Epoch 17150/30000 Validation Loss: 0.0851631686091423\n",
      "Epoch 17151/30000 Training Loss: 0.06685131043195724\n",
      "Epoch 17152/30000 Training Loss: 0.07555076479911804\n",
      "Epoch 17153/30000 Training Loss: 0.0854915976524353\n",
      "Epoch 17154/30000 Training Loss: 0.09444519877433777\n",
      "Epoch 17155/30000 Training Loss: 0.07093609869480133\n",
      "Epoch 17156/30000 Training Loss: 0.07136925309896469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17157/30000 Training Loss: 0.06574322283267975\n",
      "Epoch 17158/30000 Training Loss: 0.09055694937705994\n",
      "Epoch 17159/30000 Training Loss: 0.08721774071455002\n",
      "Epoch 17160/30000 Training Loss: 0.10518863052129745\n",
      "Epoch 17160/30000 Validation Loss: 0.08865997940301895\n",
      "Epoch 17161/30000 Training Loss: 0.07374080270528793\n",
      "Epoch 17162/30000 Training Loss: 0.06169499084353447\n",
      "Epoch 17163/30000 Training Loss: 0.059737641364336014\n",
      "Epoch 17164/30000 Training Loss: 0.07133161276578903\n",
      "Epoch 17165/30000 Training Loss: 0.06368700414896011\n",
      "Epoch 17166/30000 Training Loss: 0.08225834369659424\n",
      "Epoch 17167/30000 Training Loss: 0.07286272197961807\n",
      "Epoch 17168/30000 Training Loss: 0.057659048587083817\n",
      "Epoch 17169/30000 Training Loss: 0.05629425868391991\n",
      "Epoch 17170/30000 Training Loss: 0.06927067786455154\n",
      "Epoch 17170/30000 Validation Loss: 0.07107959687709808\n",
      "Epoch 17171/30000 Training Loss: 0.08132818341255188\n",
      "Epoch 17172/30000 Training Loss: 0.054376959800720215\n",
      "Epoch 17173/30000 Training Loss: 0.07733993977308273\n",
      "Epoch 17174/30000 Training Loss: 0.06891962140798569\n",
      "Epoch 17175/30000 Training Loss: 0.06756144762039185\n",
      "Epoch 17176/30000 Training Loss: 0.06872863322496414\n",
      "Epoch 17177/30000 Training Loss: 0.07710977643728256\n",
      "Epoch 17178/30000 Training Loss: 0.07751481980085373\n",
      "Epoch 17179/30000 Training Loss: 0.06766325235366821\n",
      "Epoch 17180/30000 Training Loss: 0.08481266349554062\n",
      "Epoch 17180/30000 Validation Loss: 0.08652445673942566\n",
      "Epoch 17181/30000 Training Loss: 0.08117181807756424\n",
      "Epoch 17182/30000 Training Loss: 0.0754111111164093\n",
      "Epoch 17183/30000 Training Loss: 0.07034588605165482\n",
      "Epoch 17184/30000 Training Loss: 0.06250638514757156\n",
      "Epoch 17185/30000 Training Loss: 0.07139691710472107\n",
      "Epoch 17186/30000 Training Loss: 0.07322075963020325\n",
      "Epoch 17187/30000 Training Loss: 0.059947092086076736\n",
      "Epoch 17188/30000 Training Loss: 0.07786047458648682\n",
      "Epoch 17189/30000 Training Loss: 0.08712170273065567\n",
      "Epoch 17190/30000 Training Loss: 0.08298566192388535\n",
      "Epoch 17190/30000 Validation Loss: 0.0767756924033165\n",
      "Epoch 17191/30000 Training Loss: 0.0857437252998352\n",
      "Epoch 17192/30000 Training Loss: 0.0551600344479084\n",
      "Epoch 17193/30000 Training Loss: 0.0777476355433464\n",
      "Epoch 17194/30000 Training Loss: 0.06107271835207939\n",
      "Epoch 17195/30000 Training Loss: 0.07247785478830338\n",
      "Epoch 17196/30000 Training Loss: 0.061729345470666885\n",
      "Epoch 17197/30000 Training Loss: 0.08082780987024307\n",
      "Epoch 17198/30000 Training Loss: 0.045289892703294754\n",
      "Epoch 17199/30000 Training Loss: 0.06684718281030655\n",
      "Epoch 17200/30000 Training Loss: 0.06301804631948471\n",
      "Epoch 17200/30000 Validation Loss: 0.08441474288702011\n",
      "Epoch 17201/30000 Training Loss: 0.084091417491436\n",
      "Epoch 17202/30000 Training Loss: 0.08351149410009384\n",
      "Epoch 17203/30000 Training Loss: 0.062037210911512375\n",
      "Epoch 17204/30000 Training Loss: 0.0688142254948616\n",
      "Epoch 17205/30000 Training Loss: 0.06969506293535233\n",
      "Epoch 17206/30000 Training Loss: 0.05958786606788635\n",
      "Epoch 17207/30000 Training Loss: 0.07869889587163925\n",
      "Epoch 17208/30000 Training Loss: 0.0698404386639595\n",
      "Epoch 17209/30000 Training Loss: 0.07867950946092606\n",
      "Epoch 17210/30000 Training Loss: 0.07450071722269058\n",
      "Epoch 17210/30000 Validation Loss: 0.09734243899583817\n",
      "Epoch 17211/30000 Training Loss: 0.08625664561986923\n",
      "Epoch 17212/30000 Training Loss: 0.07349270582199097\n",
      "Epoch 17213/30000 Training Loss: 0.08551120012998581\n",
      "Epoch 17214/30000 Training Loss: 0.07116792351007462\n",
      "Epoch 17215/30000 Training Loss: 0.060364846140146255\n",
      "Epoch 17216/30000 Training Loss: 0.06828145682811737\n",
      "Epoch 17217/30000 Training Loss: 0.07077615708112717\n",
      "Epoch 17218/30000 Training Loss: 0.0794491171836853\n",
      "Epoch 17219/30000 Training Loss: 0.08383812755346298\n",
      "Epoch 17220/30000 Training Loss: 0.08127471059560776\n",
      "Epoch 17220/30000 Validation Loss: 0.07608812302350998\n",
      "Epoch 17221/30000 Training Loss: 0.05885845050215721\n",
      "Epoch 17222/30000 Training Loss: 0.07086790353059769\n",
      "Epoch 17223/30000 Training Loss: 0.0673801526427269\n",
      "Epoch 17224/30000 Training Loss: 0.0840080976486206\n",
      "Epoch 17225/30000 Training Loss: 0.07885280251502991\n",
      "Epoch 17226/30000 Training Loss: 0.06589977443218231\n",
      "Epoch 17227/30000 Training Loss: 0.06612396240234375\n",
      "Epoch 17228/30000 Training Loss: 0.06637801975011826\n",
      "Epoch 17229/30000 Training Loss: 0.08672318607568741\n",
      "Epoch 17230/30000 Training Loss: 0.08844128251075745\n",
      "Epoch 17230/30000 Validation Loss: 0.07388950139284134\n",
      "Epoch 17231/30000 Training Loss: 0.07001560926437378\n",
      "Epoch 17232/30000 Training Loss: 0.05623788759112358\n",
      "Epoch 17233/30000 Training Loss: 0.0559469573199749\n",
      "Epoch 17234/30000 Training Loss: 0.054350245743989944\n",
      "Epoch 17235/30000 Training Loss: 0.07788333296775818\n",
      "Epoch 17236/30000 Training Loss: 0.07762458175420761\n",
      "Epoch 17237/30000 Training Loss: 0.06857246160507202\n",
      "Epoch 17238/30000 Training Loss: 0.09150240570306778\n",
      "Epoch 17239/30000 Training Loss: 0.07271625846624374\n",
      "Epoch 17240/30000 Training Loss: 0.07248923927545547\n",
      "Epoch 17240/30000 Validation Loss: 0.08079209923744202\n",
      "Epoch 17241/30000 Training Loss: 0.06937732547521591\n",
      "Epoch 17242/30000 Training Loss: 0.08428096771240234\n",
      "Epoch 17243/30000 Training Loss: 0.06576982140541077\n",
      "Epoch 17244/30000 Training Loss: 0.08252636343240738\n",
      "Epoch 17245/30000 Training Loss: 0.06610766798257828\n",
      "Epoch 17246/30000 Training Loss: 0.06656644493341446\n",
      "Epoch 17247/30000 Training Loss: 0.07262936979532242\n",
      "Epoch 17248/30000 Training Loss: 0.0748816430568695\n",
      "Epoch 17249/30000 Training Loss: 0.07118627429008484\n",
      "Epoch 17250/30000 Training Loss: 0.07848656922578812\n",
      "Epoch 17250/30000 Validation Loss: 0.05814911425113678\n",
      "Epoch 17251/30000 Training Loss: 0.0641268938779831\n",
      "Epoch 17252/30000 Training Loss: 0.05896846577525139\n",
      "Epoch 17253/30000 Training Loss: 0.08686814457178116\n",
      "Epoch 17254/30000 Training Loss: 0.05869250372052193\n",
      "Epoch 17255/30000 Training Loss: 0.05753668025135994\n",
      "Epoch 17256/30000 Training Loss: 0.09111994504928589\n",
      "Epoch 17257/30000 Training Loss: 0.10104174166917801\n",
      "Epoch 17258/30000 Training Loss: 0.06577852368354797\n",
      "Epoch 17259/30000 Training Loss: 0.087154321372509\n",
      "Epoch 17260/30000 Training Loss: 0.09247580915689468\n",
      "Epoch 17260/30000 Validation Loss: 0.06608004122972488\n",
      "Epoch 17261/30000 Training Loss: 0.08604904264211655\n",
      "Epoch 17262/30000 Training Loss: 0.07350263744592667\n",
      "Epoch 17263/30000 Training Loss: 0.09942551702260971\n",
      "Epoch 17264/30000 Training Loss: 0.06571821123361588\n",
      "Epoch 17265/30000 Training Loss: 0.08009961247444153\n",
      "Epoch 17266/30000 Training Loss: 0.07493244856595993\n",
      "Epoch 17267/30000 Training Loss: 0.06075485423207283\n",
      "Epoch 17268/30000 Training Loss: 0.06042258068919182\n",
      "Epoch 17269/30000 Training Loss: 0.07949266582727432\n",
      "Epoch 17270/30000 Training Loss: 0.07048588246107101\n",
      "Epoch 17270/30000 Validation Loss: 0.07013001292943954\n",
      "Epoch 17271/30000 Training Loss: 0.07575079053640366\n",
      "Epoch 17272/30000 Training Loss: 0.0756869688630104\n",
      "Epoch 17273/30000 Training Loss: 0.06247301027178764\n",
      "Epoch 17274/30000 Training Loss: 0.06349826604127884\n",
      "Epoch 17275/30000 Training Loss: 0.07408192753791809\n",
      "Epoch 17276/30000 Training Loss: 0.0726759135723114\n",
      "Epoch 17277/30000 Training Loss: 0.06921166926622391\n",
      "Epoch 17278/30000 Training Loss: 0.0764528140425682\n",
      "Epoch 17279/30000 Training Loss: 0.07577463239431381\n",
      "Epoch 17280/30000 Training Loss: 0.07438066601753235\n",
      "Epoch 17280/30000 Validation Loss: 0.08670899271965027\n",
      "Epoch 17281/30000 Training Loss: 0.0726979523897171\n",
      "Epoch 17282/30000 Training Loss: 0.07476773858070374\n",
      "Epoch 17283/30000 Training Loss: 0.06671597808599472\n",
      "Epoch 17284/30000 Training Loss: 0.08681299537420273\n",
      "Epoch 17285/30000 Training Loss: 0.0702870786190033\n",
      "Epoch 17286/30000 Training Loss: 0.05900844931602478\n",
      "Epoch 17287/30000 Training Loss: 0.08308194577693939\n",
      "Epoch 17288/30000 Training Loss: 0.08181409537792206\n",
      "Epoch 17289/30000 Training Loss: 0.06588245183229446\n",
      "Epoch 17290/30000 Training Loss: 0.08479585498571396\n",
      "Epoch 17290/30000 Validation Loss: 0.099957175552845\n",
      "Epoch 17291/30000 Training Loss: 0.0696023479104042\n",
      "Epoch 17292/30000 Training Loss: 0.055054258555173874\n",
      "Epoch 17293/30000 Training Loss: 0.07591638714075089\n",
      "Epoch 17294/30000 Training Loss: 0.06273774057626724\n",
      "Epoch 17295/30000 Training Loss: 0.06188225746154785\n",
      "Epoch 17296/30000 Training Loss: 0.07339026778936386\n",
      "Epoch 17297/30000 Training Loss: 0.08015742152929306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17298/30000 Training Loss: 0.07455522567033768\n",
      "Epoch 17299/30000 Training Loss: 0.07854632288217545\n",
      "Epoch 17300/30000 Training Loss: 0.08255841583013535\n",
      "Epoch 17300/30000 Validation Loss: 0.0845256820321083\n",
      "Epoch 17301/30000 Training Loss: 0.060552988201379776\n",
      "Epoch 17302/30000 Training Loss: 0.06741455942392349\n",
      "Epoch 17303/30000 Training Loss: 0.0866275429725647\n",
      "Epoch 17304/30000 Training Loss: 0.06597354263067245\n",
      "Epoch 17305/30000 Training Loss: 0.06779148429632187\n",
      "Epoch 17306/30000 Training Loss: 0.09074311703443527\n",
      "Epoch 17307/30000 Training Loss: 0.07757296413183212\n",
      "Epoch 17308/30000 Training Loss: 0.09322480112314224\n",
      "Epoch 17309/30000 Training Loss: 0.058471422642469406\n",
      "Epoch 17310/30000 Training Loss: 0.09239723533391953\n",
      "Epoch 17310/30000 Validation Loss: 0.07003588229417801\n",
      "Epoch 17311/30000 Training Loss: 0.08213347941637039\n",
      "Epoch 17312/30000 Training Loss: 0.07289038598537445\n",
      "Epoch 17313/30000 Training Loss: 0.0689622089266777\n",
      "Epoch 17314/30000 Training Loss: 0.07439731806516647\n",
      "Epoch 17315/30000 Training Loss: 0.0642109140753746\n",
      "Epoch 17316/30000 Training Loss: 0.07823377102613449\n",
      "Epoch 17317/30000 Training Loss: 0.06782577186822891\n",
      "Epoch 17318/30000 Training Loss: 0.08015983551740646\n",
      "Epoch 17319/30000 Training Loss: 0.062308382242918015\n",
      "Epoch 17320/30000 Training Loss: 0.06880863755941391\n",
      "Epoch 17320/30000 Validation Loss: 0.0717177540063858\n",
      "Epoch 17321/30000 Training Loss: 0.06210358813405037\n",
      "Epoch 17322/30000 Training Loss: 0.05174742266535759\n",
      "Epoch 17323/30000 Training Loss: 0.05579841136932373\n",
      "Epoch 17324/30000 Training Loss: 0.12861846387386322\n",
      "Epoch 17325/30000 Training Loss: 0.06410817056894302\n",
      "Epoch 17326/30000 Training Loss: 0.10359057784080505\n",
      "Epoch 17327/30000 Training Loss: 0.08462781459093094\n",
      "Epoch 17328/30000 Training Loss: 0.0913468524813652\n",
      "Epoch 17329/30000 Training Loss: 0.07005030661821365\n",
      "Epoch 17330/30000 Training Loss: 0.07222196459770203\n",
      "Epoch 17330/30000 Validation Loss: 0.07025551050901413\n",
      "Epoch 17331/30000 Training Loss: 0.07334678620100021\n",
      "Epoch 17332/30000 Training Loss: 0.06265509873628616\n",
      "Epoch 17333/30000 Training Loss: 0.06785987317562103\n",
      "Epoch 17334/30000 Training Loss: 0.07243504375219345\n",
      "Epoch 17335/30000 Training Loss: 0.06646734476089478\n",
      "Epoch 17336/30000 Training Loss: 0.06590186804533005\n",
      "Epoch 17337/30000 Training Loss: 0.07225167006254196\n",
      "Epoch 17338/30000 Training Loss: 0.0634651929140091\n",
      "Epoch 17339/30000 Training Loss: 0.05917598679661751\n",
      "Epoch 17340/30000 Training Loss: 0.07152912765741348\n",
      "Epoch 17340/30000 Validation Loss: 0.06178949773311615\n",
      "Epoch 17341/30000 Training Loss: 0.09201326221227646\n",
      "Epoch 17342/30000 Training Loss: 0.055223267525434494\n",
      "Epoch 17343/30000 Training Loss: 0.08315020799636841\n",
      "Epoch 17344/30000 Training Loss: 0.07515004277229309\n",
      "Epoch 17345/30000 Training Loss: 0.06970614194869995\n",
      "Epoch 17346/30000 Training Loss: 0.07823086529970169\n",
      "Epoch 17347/30000 Training Loss: 0.07327702641487122\n",
      "Epoch 17348/30000 Training Loss: 0.0950542688369751\n",
      "Epoch 17349/30000 Training Loss: 0.08710790425539017\n",
      "Epoch 17350/30000 Training Loss: 0.0710274875164032\n",
      "Epoch 17350/30000 Validation Loss: 0.07066042721271515\n",
      "Epoch 17351/30000 Training Loss: 0.09809046983718872\n",
      "Epoch 17352/30000 Training Loss: 0.061186760663986206\n",
      "Epoch 17353/30000 Training Loss: 0.06345313787460327\n",
      "Epoch 17354/30000 Training Loss: 0.049469996243715286\n",
      "Epoch 17355/30000 Training Loss: 0.07022154331207275\n",
      "Epoch 17356/30000 Training Loss: 0.07774416357278824\n",
      "Epoch 17357/30000 Training Loss: 0.07730105519294739\n",
      "Epoch 17358/30000 Training Loss: 0.07137131690979004\n",
      "Epoch 17359/30000 Training Loss: 0.046872012317180634\n",
      "Epoch 17360/30000 Training Loss: 0.06909164786338806\n",
      "Epoch 17360/30000 Validation Loss: 0.06618651747703552\n",
      "Epoch 17361/30000 Training Loss: 0.057267870754003525\n",
      "Epoch 17362/30000 Training Loss: 0.07915301620960236\n",
      "Epoch 17363/30000 Training Loss: 0.07865659147500992\n",
      "Epoch 17364/30000 Training Loss: 0.09407355636358261\n",
      "Epoch 17365/30000 Training Loss: 0.08877316862344742\n",
      "Epoch 17366/30000 Training Loss: 0.07008450478315353\n",
      "Epoch 17367/30000 Training Loss: 0.06967334449291229\n",
      "Epoch 17368/30000 Training Loss: 0.07028929889202118\n",
      "Epoch 17369/30000 Training Loss: 0.09210613369941711\n",
      "Epoch 17370/30000 Training Loss: 0.08111593872308731\n",
      "Epoch 17370/30000 Validation Loss: 0.08129691332578659\n",
      "Epoch 17371/30000 Training Loss: 0.062239453196525574\n",
      "Epoch 17372/30000 Training Loss: 0.07233860343694687\n",
      "Epoch 17373/30000 Training Loss: 0.06530196219682693\n",
      "Epoch 17374/30000 Training Loss: 0.07745944708585739\n",
      "Epoch 17375/30000 Training Loss: 0.08300677686929703\n",
      "Epoch 17376/30000 Training Loss: 0.06365877389907837\n",
      "Epoch 17377/30000 Training Loss: 0.08623611927032471\n",
      "Epoch 17378/30000 Training Loss: 0.07741571962833405\n",
      "Epoch 17379/30000 Training Loss: 0.08449134975671768\n",
      "Epoch 17380/30000 Training Loss: 0.07038025557994843\n",
      "Epoch 17380/30000 Validation Loss: 0.0517263226211071\n",
      "Epoch 17381/30000 Training Loss: 0.09247676283121109\n",
      "Epoch 17382/30000 Training Loss: 0.09551489353179932\n",
      "Epoch 17383/30000 Training Loss: 0.06548445671796799\n",
      "Epoch 17384/30000 Training Loss: 0.0761827826499939\n",
      "Epoch 17385/30000 Training Loss: 0.07998446375131607\n",
      "Epoch 17386/30000 Training Loss: 0.07634460926055908\n",
      "Epoch 17387/30000 Training Loss: 0.07199131697416306\n",
      "Epoch 17388/30000 Training Loss: 0.08836086839437485\n",
      "Epoch 17389/30000 Training Loss: 0.06965118646621704\n",
      "Epoch 17390/30000 Training Loss: 0.0704570934176445\n",
      "Epoch 17390/30000 Validation Loss: 0.0804993212223053\n",
      "Epoch 17391/30000 Training Loss: 0.07252001017332077\n",
      "Epoch 17392/30000 Training Loss: 0.06925103813409805\n",
      "Epoch 17393/30000 Training Loss: 0.08381136506795883\n",
      "Epoch 17394/30000 Training Loss: 0.09066500514745712\n",
      "Epoch 17395/30000 Training Loss: 0.0727255642414093\n",
      "Epoch 17396/30000 Training Loss: 0.07205431908369064\n",
      "Epoch 17397/30000 Training Loss: 0.06016146019101143\n",
      "Epoch 17398/30000 Training Loss: 0.08764704316854477\n",
      "Epoch 17399/30000 Training Loss: 0.08642533421516418\n",
      "Epoch 17400/30000 Training Loss: 0.06717813014984131\n",
      "Epoch 17400/30000 Validation Loss: 0.06675795465707779\n",
      "Epoch 17401/30000 Training Loss: 0.061467260122299194\n",
      "Epoch 17402/30000 Training Loss: 0.05449998006224632\n",
      "Epoch 17403/30000 Training Loss: 0.07093151658773422\n",
      "Epoch 17404/30000 Training Loss: 0.068865567445755\n",
      "Epoch 17405/30000 Training Loss: 0.08317380398511887\n",
      "Epoch 17406/30000 Training Loss: 0.08110427856445312\n",
      "Epoch 17407/30000 Training Loss: 0.07082203030586243\n",
      "Epoch 17408/30000 Training Loss: 0.07902111858129501\n",
      "Epoch 17409/30000 Training Loss: 0.060159724205732346\n",
      "Epoch 17410/30000 Training Loss: 0.08593162894248962\n",
      "Epoch 17410/30000 Validation Loss: 0.061723459511995316\n",
      "Epoch 17411/30000 Training Loss: 0.07891618460416794\n",
      "Epoch 17412/30000 Training Loss: 0.05321189761161804\n",
      "Epoch 17413/30000 Training Loss: 0.060027826577425\n",
      "Epoch 17414/30000 Training Loss: 0.07024198770523071\n",
      "Epoch 17415/30000 Training Loss: 0.0753118023276329\n",
      "Epoch 17416/30000 Training Loss: 0.07329566031694412\n",
      "Epoch 17417/30000 Training Loss: 0.07553031295537949\n",
      "Epoch 17418/30000 Training Loss: 0.07560186833143234\n",
      "Epoch 17419/30000 Training Loss: 0.0685153529047966\n",
      "Epoch 17420/30000 Training Loss: 0.07971075177192688\n",
      "Epoch 17420/30000 Validation Loss: 0.07689660042524338\n",
      "Epoch 17421/30000 Training Loss: 0.07355769723653793\n",
      "Epoch 17422/30000 Training Loss: 0.06814277917146683\n",
      "Epoch 17423/30000 Training Loss: 0.08717366307973862\n",
      "Epoch 17424/30000 Training Loss: 0.0898563340306282\n",
      "Epoch 17425/30000 Training Loss: 0.07114282250404358\n",
      "Epoch 17426/30000 Training Loss: 0.07607252150774002\n",
      "Epoch 17427/30000 Training Loss: 0.06953635066747665\n",
      "Epoch 17428/30000 Training Loss: 0.07978103309869766\n",
      "Epoch 17429/30000 Training Loss: 0.06919495016336441\n",
      "Epoch 17430/30000 Training Loss: 0.06448245793581009\n",
      "Epoch 17430/30000 Validation Loss: 0.07618413120508194\n",
      "Epoch 17431/30000 Training Loss: 0.07957590371370316\n",
      "Epoch 17432/30000 Training Loss: 0.06363829970359802\n",
      "Epoch 17433/30000 Training Loss: 0.06914138048887253\n",
      "Epoch 17434/30000 Training Loss: 0.08186819404363632\n",
      "Epoch 17435/30000 Training Loss: 0.07900187373161316\n",
      "Epoch 17436/30000 Training Loss: 0.06396184116601944\n",
      "Epoch 17437/30000 Training Loss: 0.07599511742591858\n",
      "Epoch 17438/30000 Training Loss: 0.08595273643732071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17439/30000 Training Loss: 0.06488580256700516\n",
      "Epoch 17440/30000 Training Loss: 0.0702638030052185\n",
      "Epoch 17440/30000 Validation Loss: 0.06319043040275574\n",
      "Epoch 17441/30000 Training Loss: 0.05734436586499214\n",
      "Epoch 17442/30000 Training Loss: 0.05398683249950409\n",
      "Epoch 17443/30000 Training Loss: 0.054477352648973465\n",
      "Epoch 17444/30000 Training Loss: 0.07663533091545105\n",
      "Epoch 17445/30000 Training Loss: 0.052901580929756165\n",
      "Epoch 17446/30000 Training Loss: 0.07912170141935349\n",
      "Epoch 17447/30000 Training Loss: 0.05467754974961281\n",
      "Epoch 17448/30000 Training Loss: 0.07131249457597733\n",
      "Epoch 17449/30000 Training Loss: 0.06863055378198624\n",
      "Epoch 17450/30000 Training Loss: 0.07996609061956406\n",
      "Epoch 17450/30000 Validation Loss: 0.058748483657836914\n",
      "Epoch 17451/30000 Training Loss: 0.06455663591623306\n",
      "Epoch 17452/30000 Training Loss: 0.06876536458730698\n",
      "Epoch 17453/30000 Training Loss: 0.06517764180898666\n",
      "Epoch 17454/30000 Training Loss: 0.07565867900848389\n",
      "Epoch 17455/30000 Training Loss: 0.09258564561605453\n",
      "Epoch 17456/30000 Training Loss: 0.08066504448652267\n",
      "Epoch 17457/30000 Training Loss: 0.07250934094190598\n",
      "Epoch 17458/30000 Training Loss: 0.08482953161001205\n",
      "Epoch 17459/30000 Training Loss: 0.06017187237739563\n",
      "Epoch 17460/30000 Training Loss: 0.08389785140752792\n",
      "Epoch 17460/30000 Validation Loss: 0.06239458918571472\n",
      "Epoch 17461/30000 Training Loss: 0.07618851959705353\n",
      "Epoch 17462/30000 Training Loss: 0.07171819359064102\n",
      "Epoch 17463/30000 Training Loss: 0.07064556330442429\n",
      "Epoch 17464/30000 Training Loss: 0.08693858981132507\n",
      "Epoch 17465/30000 Training Loss: 0.08177468925714493\n",
      "Epoch 17466/30000 Training Loss: 0.08360123634338379\n",
      "Epoch 17467/30000 Training Loss: 0.057921718806028366\n",
      "Epoch 17468/30000 Training Loss: 0.08923995494842529\n",
      "Epoch 17469/30000 Training Loss: 0.06517807394266129\n",
      "Epoch 17470/30000 Training Loss: 0.06046445295214653\n",
      "Epoch 17470/30000 Validation Loss: 0.06377177685499191\n",
      "Epoch 17471/30000 Training Loss: 0.06176529452204704\n",
      "Epoch 17472/30000 Training Loss: 0.08413699269294739\n",
      "Epoch 17473/30000 Training Loss: 0.06315280497074127\n",
      "Epoch 17474/30000 Training Loss: 0.06904525309801102\n",
      "Epoch 17475/30000 Training Loss: 0.0910995677113533\n",
      "Epoch 17476/30000 Training Loss: 0.08920776098966599\n",
      "Epoch 17477/30000 Training Loss: 0.061064463108778\n",
      "Epoch 17478/30000 Training Loss: 0.07455597072839737\n",
      "Epoch 17479/30000 Training Loss: 0.07310599833726883\n",
      "Epoch 17480/30000 Training Loss: 0.07082504779100418\n",
      "Epoch 17480/30000 Validation Loss: 0.06410067528486252\n",
      "Epoch 17481/30000 Training Loss: 0.0856410562992096\n",
      "Epoch 17482/30000 Training Loss: 0.09468057006597519\n",
      "Epoch 17483/30000 Training Loss: 0.06160295382142067\n",
      "Epoch 17484/30000 Training Loss: 0.06539896875619888\n",
      "Epoch 17485/30000 Training Loss: 0.07976031303405762\n",
      "Epoch 17486/30000 Training Loss: 0.07737627625465393\n",
      "Epoch 17487/30000 Training Loss: 0.07726028561592102\n",
      "Epoch 17488/30000 Training Loss: 0.07304324954748154\n",
      "Epoch 17489/30000 Training Loss: 0.053543705493211746\n",
      "Epoch 17490/30000 Training Loss: 0.06182755157351494\n",
      "Epoch 17490/30000 Validation Loss: 0.07831305265426636\n",
      "Epoch 17491/30000 Training Loss: 0.0834084153175354\n",
      "Epoch 17492/30000 Training Loss: 0.09057586640119553\n",
      "Epoch 17493/30000 Training Loss: 0.07515374571084976\n",
      "Epoch 17494/30000 Training Loss: 0.08111360669136047\n",
      "Epoch 17495/30000 Training Loss: 0.06474807858467102\n",
      "Epoch 17496/30000 Training Loss: 0.07061167806386948\n",
      "Epoch 17497/30000 Training Loss: 0.09093106538057327\n",
      "Epoch 17498/30000 Training Loss: 0.07661232352256775\n",
      "Epoch 17499/30000 Training Loss: 0.06054146960377693\n",
      "Epoch 17500/30000 Training Loss: 0.09417597204446793\n",
      "Epoch 17500/30000 Validation Loss: 0.09022805094718933\n",
      "Epoch 17501/30000 Training Loss: 0.09116359800100327\n",
      "Epoch 17502/30000 Training Loss: 0.08058784157037735\n",
      "Epoch 17503/30000 Training Loss: 0.06166393682360649\n",
      "Epoch 17504/30000 Training Loss: 0.07581985741853714\n",
      "Epoch 17505/30000 Training Loss: 0.07993387430906296\n",
      "Epoch 17506/30000 Training Loss: 0.05914623662829399\n",
      "Epoch 17507/30000 Training Loss: 0.08194496482610703\n",
      "Epoch 17508/30000 Training Loss: 0.07422325760126114\n",
      "Epoch 17509/30000 Training Loss: 0.05762002244591713\n",
      "Epoch 17510/30000 Training Loss: 0.07046271860599518\n",
      "Epoch 17510/30000 Validation Loss: 0.06705541908740997\n",
      "Epoch 17511/30000 Training Loss: 0.06082133948802948\n",
      "Epoch 17512/30000 Training Loss: 0.07963037490844727\n",
      "Epoch 17513/30000 Training Loss: 0.08792329579591751\n",
      "Epoch 17514/30000 Training Loss: 0.09264516085386276\n",
      "Epoch 17515/30000 Training Loss: 0.053787123411893845\n",
      "Epoch 17516/30000 Training Loss: 0.07827422022819519\n",
      "Epoch 17517/30000 Training Loss: 0.09060368686914444\n",
      "Epoch 17518/30000 Training Loss: 0.08371328562498093\n",
      "Epoch 17519/30000 Training Loss: 0.0744829773902893\n",
      "Epoch 17520/30000 Training Loss: 0.11542657762765884\n",
      "Epoch 17520/30000 Validation Loss: 0.0744691863656044\n",
      "Epoch 17521/30000 Training Loss: 0.0760343000292778\n",
      "Epoch 17522/30000 Training Loss: 0.0826306864619255\n",
      "Epoch 17523/30000 Training Loss: 0.04754266142845154\n",
      "Epoch 17524/30000 Training Loss: 0.07312745600938797\n",
      "Epoch 17525/30000 Training Loss: 0.07523764669895172\n",
      "Epoch 17526/30000 Training Loss: 0.08497273921966553\n",
      "Epoch 17527/30000 Training Loss: 0.057738687843084335\n",
      "Epoch 17528/30000 Training Loss: 0.05889762565493584\n",
      "Epoch 17529/30000 Training Loss: 0.06939008831977844\n",
      "Epoch 17530/30000 Training Loss: 0.06627654284238815\n",
      "Epoch 17530/30000 Validation Loss: 0.07311021536588669\n",
      "Epoch 17531/30000 Training Loss: 0.09068534523248672\n",
      "Epoch 17532/30000 Training Loss: 0.0845770463347435\n",
      "Epoch 17533/30000 Training Loss: 0.08946230262517929\n",
      "Epoch 17534/30000 Training Loss: 0.0727500393986702\n",
      "Epoch 17535/30000 Training Loss: 0.08599130064249039\n",
      "Epoch 17536/30000 Training Loss: 0.07525303214788437\n",
      "Epoch 17537/30000 Training Loss: 0.07765168696641922\n",
      "Epoch 17538/30000 Training Loss: 0.06269129365682602\n",
      "Epoch 17539/30000 Training Loss: 0.06842870265245438\n",
      "Epoch 17540/30000 Training Loss: 0.058452848345041275\n",
      "Epoch 17540/30000 Validation Loss: 0.05703895166516304\n",
      "Epoch 17541/30000 Training Loss: 0.06634104251861572\n",
      "Epoch 17542/30000 Training Loss: 0.05607031658291817\n",
      "Epoch 17543/30000 Training Loss: 0.06465604156255722\n",
      "Epoch 17544/30000 Training Loss: 0.0761638954281807\n",
      "Epoch 17545/30000 Training Loss: 0.06061020493507385\n",
      "Epoch 17546/30000 Training Loss: 0.07478790730237961\n",
      "Epoch 17547/30000 Training Loss: 0.08815619349479675\n",
      "Epoch 17548/30000 Training Loss: 0.05769576504826546\n",
      "Epoch 17549/30000 Training Loss: 0.06604843586683273\n",
      "Epoch 17550/30000 Training Loss: 0.08113469928503036\n",
      "Epoch 17550/30000 Validation Loss: 0.08570336550474167\n",
      "Epoch 17551/30000 Training Loss: 0.07248731702566147\n",
      "Epoch 17552/30000 Training Loss: 0.07490888237953186\n",
      "Epoch 17553/30000 Training Loss: 0.07777055352926254\n",
      "Epoch 17554/30000 Training Loss: 0.06679678708314896\n",
      "Epoch 17555/30000 Training Loss: 0.06587965041399002\n",
      "Epoch 17556/30000 Training Loss: 0.07805489003658295\n",
      "Epoch 17557/30000 Training Loss: 0.07048452645540237\n",
      "Epoch 17558/30000 Training Loss: 0.07023505866527557\n",
      "Epoch 17559/30000 Training Loss: 0.06586479395627975\n",
      "Epoch 17560/30000 Training Loss: 0.07469999045133591\n",
      "Epoch 17560/30000 Validation Loss: 0.06543752551078796\n",
      "Epoch 17561/30000 Training Loss: 0.05966946855187416\n",
      "Epoch 17562/30000 Training Loss: 0.09204757213592529\n",
      "Epoch 17563/30000 Training Loss: 0.06924092024564743\n",
      "Epoch 17564/30000 Training Loss: 0.08890416473150253\n",
      "Epoch 17565/30000 Training Loss: 0.07657787948846817\n",
      "Epoch 17566/30000 Training Loss: 0.07977873086929321\n",
      "Epoch 17567/30000 Training Loss: 0.05630214512348175\n",
      "Epoch 17568/30000 Training Loss: 0.07454416900873184\n",
      "Epoch 17569/30000 Training Loss: 0.06798089295625687\n",
      "Epoch 17570/30000 Training Loss: 0.08235246688127518\n",
      "Epoch 17570/30000 Validation Loss: 0.07223858684301376\n",
      "Epoch 17571/30000 Training Loss: 0.07109343260526657\n",
      "Epoch 17572/30000 Training Loss: 0.07306701689958572\n",
      "Epoch 17573/30000 Training Loss: 0.067206971347332\n",
      "Epoch 17574/30000 Training Loss: 0.08577221632003784\n",
      "Epoch 17575/30000 Training Loss: 0.10971254855394363\n",
      "Epoch 17576/30000 Training Loss: 0.06522545963525772\n",
      "Epoch 17577/30000 Training Loss: 0.07999604195356369\n",
      "Epoch 17578/30000 Training Loss: 0.0809873715043068\n",
      "Epoch 17579/30000 Training Loss: 0.09893932193517685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17580/30000 Training Loss: 0.061797380447387695\n",
      "Epoch 17580/30000 Validation Loss: 0.06434360146522522\n",
      "Epoch 17581/30000 Training Loss: 0.06538998335599899\n",
      "Epoch 17582/30000 Training Loss: 0.07859919965267181\n",
      "Epoch 17583/30000 Training Loss: 0.05744506046175957\n",
      "Epoch 17584/30000 Training Loss: 0.08347619324922562\n",
      "Epoch 17585/30000 Training Loss: 0.08216948062181473\n",
      "Epoch 17586/30000 Training Loss: 0.06986626237630844\n",
      "Epoch 17587/30000 Training Loss: 0.07285749912261963\n",
      "Epoch 17588/30000 Training Loss: 0.08749616891145706\n",
      "Epoch 17589/30000 Training Loss: 0.06480355560779572\n",
      "Epoch 17590/30000 Training Loss: 0.06542176008224487\n",
      "Epoch 17590/30000 Validation Loss: 0.10006196051836014\n",
      "Epoch 17591/30000 Training Loss: 0.06269186735153198\n",
      "Epoch 17592/30000 Training Loss: 0.09247883409261703\n",
      "Epoch 17593/30000 Training Loss: 0.07512404769659042\n",
      "Epoch 17594/30000 Training Loss: 0.06541531533002853\n",
      "Epoch 17595/30000 Training Loss: 0.06312157958745956\n",
      "Epoch 17596/30000 Training Loss: 0.06424429267644882\n",
      "Epoch 17597/30000 Training Loss: 0.07987051457166672\n",
      "Epoch 17598/30000 Training Loss: 0.06341970711946487\n",
      "Epoch 17599/30000 Training Loss: 0.09019765257835388\n",
      "Epoch 17600/30000 Training Loss: 0.059439584612846375\n",
      "Epoch 17600/30000 Validation Loss: 0.07296816259622574\n",
      "Epoch 17601/30000 Training Loss: 0.07589804381132126\n",
      "Epoch 17602/30000 Training Loss: 0.09366791695356369\n",
      "Epoch 17603/30000 Training Loss: 0.06113307550549507\n",
      "Epoch 17604/30000 Training Loss: 0.08311020582914352\n",
      "Epoch 17605/30000 Training Loss: 0.06847364455461502\n",
      "Epoch 17606/30000 Training Loss: 0.06879212707281113\n",
      "Epoch 17607/30000 Training Loss: 0.07343592494726181\n",
      "Epoch 17608/30000 Training Loss: 0.06147133186459541\n",
      "Epoch 17609/30000 Training Loss: 0.06207460165023804\n",
      "Epoch 17610/30000 Training Loss: 0.0849016010761261\n",
      "Epoch 17610/30000 Validation Loss: 0.06318055093288422\n",
      "Epoch 17611/30000 Training Loss: 0.0794445276260376\n",
      "Epoch 17612/30000 Training Loss: 0.08351298421621323\n",
      "Epoch 17613/30000 Training Loss: 0.07376565039157867\n",
      "Epoch 17614/30000 Training Loss: 0.08938642591238022\n",
      "Epoch 17615/30000 Training Loss: 0.07661056518554688\n",
      "Epoch 17616/30000 Training Loss: 0.06609521061182022\n",
      "Epoch 17617/30000 Training Loss: 0.06907732039690018\n",
      "Epoch 17618/30000 Training Loss: 0.0854094848036766\n",
      "Epoch 17619/30000 Training Loss: 0.06712175160646439\n",
      "Epoch 17620/30000 Training Loss: 0.08197688311338425\n",
      "Epoch 17620/30000 Validation Loss: 0.06858588010072708\n",
      "Epoch 17621/30000 Training Loss: 0.06318840384483337\n",
      "Epoch 17622/30000 Training Loss: 0.0706762745976448\n",
      "Epoch 17623/30000 Training Loss: 0.09715408086776733\n",
      "Epoch 17624/30000 Training Loss: 0.08027032762765884\n",
      "Epoch 17625/30000 Training Loss: 0.0716499537229538\n",
      "Epoch 17626/30000 Training Loss: 0.05783716216683388\n",
      "Epoch 17627/30000 Training Loss: 0.08354663103818893\n",
      "Epoch 17628/30000 Training Loss: 0.09515255689620972\n",
      "Epoch 17629/30000 Training Loss: 0.06637561321258545\n",
      "Epoch 17630/30000 Training Loss: 0.0826655700802803\n",
      "Epoch 17630/30000 Validation Loss: 0.06284818053245544\n",
      "Epoch 17631/30000 Training Loss: 0.063011534512043\n",
      "Epoch 17632/30000 Training Loss: 0.08700031042098999\n",
      "Epoch 17633/30000 Training Loss: 0.06951751559972763\n",
      "Epoch 17634/30000 Training Loss: 0.08004417270421982\n",
      "Epoch 17635/30000 Training Loss: 0.06986594945192337\n",
      "Epoch 17636/30000 Training Loss: 0.08119495958089828\n",
      "Epoch 17637/30000 Training Loss: 0.06917065382003784\n",
      "Epoch 17638/30000 Training Loss: 0.07676023989915848\n",
      "Epoch 17639/30000 Training Loss: 0.07657105475664139\n",
      "Epoch 17640/30000 Training Loss: 0.0761803612112999\n",
      "Epoch 17640/30000 Validation Loss: 0.08529991656541824\n",
      "Epoch 17641/30000 Training Loss: 0.09103143960237503\n",
      "Epoch 17642/30000 Training Loss: 0.07115250080823898\n",
      "Epoch 17643/30000 Training Loss: 0.0535535030066967\n",
      "Epoch 17644/30000 Training Loss: 0.06261975318193436\n",
      "Epoch 17645/30000 Training Loss: 0.08388320356607437\n",
      "Epoch 17646/30000 Training Loss: 0.06915613263845444\n",
      "Epoch 17647/30000 Training Loss: 0.07868561893701553\n",
      "Epoch 17648/30000 Training Loss: 0.06479911506175995\n",
      "Epoch 17649/30000 Training Loss: 0.08490780740976334\n",
      "Epoch 17650/30000 Training Loss: 0.05997629836201668\n",
      "Epoch 17650/30000 Validation Loss: 0.07634454220533371\n",
      "Epoch 17651/30000 Training Loss: 0.06535575538873672\n",
      "Epoch 17652/30000 Training Loss: 0.08124644309282303\n",
      "Epoch 17653/30000 Training Loss: 0.07125556468963623\n",
      "Epoch 17654/30000 Training Loss: 0.07286000996828079\n",
      "Epoch 17655/30000 Training Loss: 0.07714113593101501\n",
      "Epoch 17656/30000 Training Loss: 0.08250223100185394\n",
      "Epoch 17657/30000 Training Loss: 0.08253112435340881\n",
      "Epoch 17658/30000 Training Loss: 0.06134948134422302\n",
      "Epoch 17659/30000 Training Loss: 0.0640329048037529\n",
      "Epoch 17660/30000 Training Loss: 0.06805950403213501\n",
      "Epoch 17660/30000 Validation Loss: 0.07362460345029831\n",
      "Epoch 17661/30000 Training Loss: 0.07697585225105286\n",
      "Epoch 17662/30000 Training Loss: 0.07620013505220413\n",
      "Epoch 17663/30000 Training Loss: 0.05833934247493744\n",
      "Epoch 17664/30000 Training Loss: 0.06635063141584396\n",
      "Epoch 17665/30000 Training Loss: 0.07664436101913452\n",
      "Epoch 17666/30000 Training Loss: 0.07911938428878784\n",
      "Epoch 17667/30000 Training Loss: 0.060341399163007736\n",
      "Epoch 17668/30000 Training Loss: 0.06846977025270462\n",
      "Epoch 17669/30000 Training Loss: 0.07563835382461548\n",
      "Epoch 17670/30000 Training Loss: 0.06423857808113098\n",
      "Epoch 17670/30000 Validation Loss: 0.0687318965792656\n",
      "Epoch 17671/30000 Training Loss: 0.07215295732021332\n",
      "Epoch 17672/30000 Training Loss: 0.08621647208929062\n",
      "Epoch 17673/30000 Training Loss: 0.07232097536325455\n",
      "Epoch 17674/30000 Training Loss: 0.08534280210733414\n",
      "Epoch 17675/30000 Training Loss: 0.06846799701452255\n",
      "Epoch 17676/30000 Training Loss: 0.06479605287313461\n",
      "Epoch 17677/30000 Training Loss: 0.08459919691085815\n",
      "Epoch 17678/30000 Training Loss: 0.06993359327316284\n",
      "Epoch 17679/30000 Training Loss: 0.08190426230430603\n",
      "Epoch 17680/30000 Training Loss: 0.07961607724428177\n",
      "Epoch 17680/30000 Validation Loss: 0.07684313505887985\n",
      "Epoch 17681/30000 Training Loss: 0.08874884992837906\n",
      "Epoch 17682/30000 Training Loss: 0.06646034121513367\n",
      "Epoch 17683/30000 Training Loss: 0.07720216363668442\n",
      "Epoch 17684/30000 Training Loss: 0.06234468147158623\n",
      "Epoch 17685/30000 Training Loss: 0.07102112472057343\n",
      "Epoch 17686/30000 Training Loss: 0.061858098953962326\n",
      "Epoch 17687/30000 Training Loss: 0.08245915174484253\n",
      "Epoch 17688/30000 Training Loss: 0.05705483630299568\n",
      "Epoch 17689/30000 Training Loss: 0.06261765211820602\n",
      "Epoch 17690/30000 Training Loss: 0.06263332813978195\n",
      "Epoch 17690/30000 Validation Loss: 0.08801662176847458\n",
      "Epoch 17691/30000 Training Loss: 0.09712281078100204\n",
      "Epoch 17692/30000 Training Loss: 0.0680529847741127\n",
      "Epoch 17693/30000 Training Loss: 0.06520844995975494\n",
      "Epoch 17694/30000 Training Loss: 0.06551576405763626\n",
      "Epoch 17695/30000 Training Loss: 0.05918717011809349\n",
      "Epoch 17696/30000 Training Loss: 0.09701046347618103\n",
      "Epoch 17697/30000 Training Loss: 0.06915803253650665\n",
      "Epoch 17698/30000 Training Loss: 0.08618540316820145\n",
      "Epoch 17699/30000 Training Loss: 0.0601932518184185\n",
      "Epoch 17700/30000 Training Loss: 0.07870655506849289\n",
      "Epoch 17700/30000 Validation Loss: 0.06812425702810287\n",
      "Epoch 17701/30000 Training Loss: 0.05940740182995796\n",
      "Epoch 17702/30000 Training Loss: 0.0669083222746849\n",
      "Epoch 17703/30000 Training Loss: 0.07406295835971832\n",
      "Epoch 17704/30000 Training Loss: 0.06985598802566528\n",
      "Epoch 17705/30000 Training Loss: 0.06352207064628601\n",
      "Epoch 17706/30000 Training Loss: 0.09392473101615906\n",
      "Epoch 17707/30000 Training Loss: 0.0983319953083992\n",
      "Epoch 17708/30000 Training Loss: 0.0657542496919632\n",
      "Epoch 17709/30000 Training Loss: 0.06596776098012924\n",
      "Epoch 17710/30000 Training Loss: 0.08081521838903427\n",
      "Epoch 17710/30000 Validation Loss: 0.07752425223588943\n",
      "Epoch 17711/30000 Training Loss: 0.08197589218616486\n",
      "Epoch 17712/30000 Training Loss: 0.08474141359329224\n",
      "Epoch 17713/30000 Training Loss: 0.06161870062351227\n",
      "Epoch 17714/30000 Training Loss: 0.08224178850650787\n",
      "Epoch 17715/30000 Training Loss: 0.06949024647474289\n",
      "Epoch 17716/30000 Training Loss: 0.06672468036413193\n",
      "Epoch 17717/30000 Training Loss: 0.07820483297109604\n",
      "Epoch 17718/30000 Training Loss: 0.07039212435483932\n",
      "Epoch 17719/30000 Training Loss: 0.07979300618171692\n",
      "Epoch 17720/30000 Training Loss: 0.07530364394187927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17720/30000 Validation Loss: 0.060661811381578445\n",
      "Epoch 17721/30000 Training Loss: 0.0721053034067154\n",
      "Epoch 17722/30000 Training Loss: 0.08612939715385437\n",
      "Epoch 17723/30000 Training Loss: 0.05521972477436066\n",
      "Epoch 17724/30000 Training Loss: 0.07933488488197327\n",
      "Epoch 17725/30000 Training Loss: 0.07263150811195374\n",
      "Epoch 17726/30000 Training Loss: 0.07981511205434799\n",
      "Epoch 17727/30000 Training Loss: 0.08740296959877014\n",
      "Epoch 17728/30000 Training Loss: 0.09920024871826172\n",
      "Epoch 17729/30000 Training Loss: 0.0793469175696373\n",
      "Epoch 17730/30000 Training Loss: 0.05694388225674629\n",
      "Epoch 17730/30000 Validation Loss: 0.05377774313092232\n",
      "Epoch 17731/30000 Training Loss: 0.06975158303976059\n",
      "Epoch 17732/30000 Training Loss: 0.0646163746714592\n",
      "Epoch 17733/30000 Training Loss: 0.0751202180981636\n",
      "Epoch 17734/30000 Training Loss: 0.05986003205180168\n",
      "Epoch 17735/30000 Training Loss: 0.07095199823379517\n",
      "Epoch 17736/30000 Training Loss: 0.07797911763191223\n",
      "Epoch 17737/30000 Training Loss: 0.05946905538439751\n",
      "Epoch 17738/30000 Training Loss: 0.06872063130140305\n",
      "Epoch 17739/30000 Training Loss: 0.07398252934217453\n",
      "Epoch 17740/30000 Training Loss: 0.09581241011619568\n",
      "Epoch 17740/30000 Validation Loss: 0.10166939347982407\n",
      "Epoch 17741/30000 Training Loss: 0.09982796758413315\n",
      "Epoch 17742/30000 Training Loss: 0.08677829056978226\n",
      "Epoch 17743/30000 Training Loss: 0.06803936511278152\n",
      "Epoch 17744/30000 Training Loss: 0.06764019280672073\n",
      "Epoch 17745/30000 Training Loss: 0.06382907181978226\n",
      "Epoch 17746/30000 Training Loss: 0.06955335289239883\n",
      "Epoch 17747/30000 Training Loss: 0.06216080114245415\n",
      "Epoch 17748/30000 Training Loss: 0.07687573879957199\n",
      "Epoch 17749/30000 Training Loss: 0.0691743716597557\n",
      "Epoch 17750/30000 Training Loss: 0.06633993238210678\n",
      "Epoch 17750/30000 Validation Loss: 0.07523792237043381\n",
      "Epoch 17751/30000 Training Loss: 0.06366349011659622\n",
      "Epoch 17752/30000 Training Loss: 0.06279981881380081\n",
      "Epoch 17753/30000 Training Loss: 0.084759421646595\n",
      "Epoch 17754/30000 Training Loss: 0.06226927042007446\n",
      "Epoch 17755/30000 Training Loss: 0.08475253731012344\n",
      "Epoch 17756/30000 Training Loss: 0.0721597746014595\n",
      "Epoch 17757/30000 Training Loss: 0.07619404047727585\n",
      "Epoch 17758/30000 Training Loss: 0.0591922402381897\n",
      "Epoch 17759/30000 Training Loss: 0.08599379658699036\n",
      "Epoch 17760/30000 Training Loss: 0.07216133922338486\n",
      "Epoch 17760/30000 Validation Loss: 0.09664424508810043\n",
      "Epoch 17761/30000 Training Loss: 0.08272967487573624\n",
      "Epoch 17762/30000 Training Loss: 0.07393566519021988\n",
      "Epoch 17763/30000 Training Loss: 0.0959639921784401\n",
      "Epoch 17764/30000 Training Loss: 0.06614413857460022\n",
      "Epoch 17765/30000 Training Loss: 0.067287378013134\n",
      "Epoch 17766/30000 Training Loss: 0.08231496065855026\n",
      "Epoch 17767/30000 Training Loss: 0.05033751204609871\n",
      "Epoch 17768/30000 Training Loss: 0.06792324036359787\n",
      "Epoch 17769/30000 Training Loss: 0.08348182588815689\n",
      "Epoch 17770/30000 Training Loss: 0.06511261314153671\n",
      "Epoch 17770/30000 Validation Loss: 0.07245930284261703\n",
      "Epoch 17771/30000 Training Loss: 0.09308552742004395\n",
      "Epoch 17772/30000 Training Loss: 0.07441230118274689\n",
      "Epoch 17773/30000 Training Loss: 0.08407392352819443\n",
      "Epoch 17774/30000 Training Loss: 0.055822521448135376\n",
      "Epoch 17775/30000 Training Loss: 0.06881830096244812\n",
      "Epoch 17776/30000 Training Loss: 0.07506266236305237\n",
      "Epoch 17777/30000 Training Loss: 0.08160857111215591\n",
      "Epoch 17778/30000 Training Loss: 0.06761905550956726\n",
      "Epoch 17779/30000 Training Loss: 0.08984991163015366\n",
      "Epoch 17780/30000 Training Loss: 0.07702536135911942\n",
      "Epoch 17780/30000 Validation Loss: 0.07956483960151672\n",
      "Epoch 17781/30000 Training Loss: 0.08924240618944168\n",
      "Epoch 17782/30000 Training Loss: 0.08702734112739563\n",
      "Epoch 17783/30000 Training Loss: 0.0662924125790596\n",
      "Epoch 17784/30000 Training Loss: 0.08971592038869858\n",
      "Epoch 17785/30000 Training Loss: 0.08477986603975296\n",
      "Epoch 17786/30000 Training Loss: 0.08004433661699295\n",
      "Epoch 17787/30000 Training Loss: 0.07810162752866745\n",
      "Epoch 17788/30000 Training Loss: 0.07565748691558838\n",
      "Epoch 17789/30000 Training Loss: 0.10603942722082138\n",
      "Epoch 17790/30000 Training Loss: 0.0764148011803627\n",
      "Epoch 17790/30000 Validation Loss: 0.0689765140414238\n",
      "Epoch 17791/30000 Training Loss: 0.06618419289588928\n",
      "Epoch 17792/30000 Training Loss: 0.06529746204614639\n",
      "Epoch 17793/30000 Training Loss: 0.06373541802167892\n",
      "Epoch 17794/30000 Training Loss: 0.05737621709704399\n",
      "Epoch 17795/30000 Training Loss: 0.06179742142558098\n",
      "Epoch 17796/30000 Training Loss: 0.07404789328575134\n",
      "Epoch 17797/30000 Training Loss: 0.06601553410291672\n",
      "Epoch 17798/30000 Training Loss: 0.06387066841125488\n",
      "Epoch 17799/30000 Training Loss: 0.08318424224853516\n",
      "Epoch 17800/30000 Training Loss: 0.08212167769670486\n",
      "Epoch 17800/30000 Validation Loss: 0.0776277557015419\n",
      "Epoch 17801/30000 Training Loss: 0.08795718103647232\n",
      "Epoch 17802/30000 Training Loss: 0.06603206694126129\n",
      "Epoch 17803/30000 Training Loss: 0.0728393942117691\n",
      "Epoch 17804/30000 Training Loss: 0.07147254794836044\n",
      "Epoch 17805/30000 Training Loss: 0.06834850460290909\n",
      "Epoch 17806/30000 Training Loss: 0.0689755529165268\n",
      "Epoch 17807/30000 Training Loss: 0.08303339034318924\n",
      "Epoch 17808/30000 Training Loss: 0.07241662591695786\n",
      "Epoch 17809/30000 Training Loss: 0.07396969199180603\n",
      "Epoch 17810/30000 Training Loss: 0.08528226613998413\n",
      "Epoch 17810/30000 Validation Loss: 0.06635672599077225\n",
      "Epoch 17811/30000 Training Loss: 0.06588342040777206\n",
      "Epoch 17812/30000 Training Loss: 0.07593553513288498\n",
      "Epoch 17813/30000 Training Loss: 0.0631960928440094\n",
      "Epoch 17814/30000 Training Loss: 0.06719367951154709\n",
      "Epoch 17815/30000 Training Loss: 0.06216038763523102\n",
      "Epoch 17816/30000 Training Loss: 0.07444728165864944\n",
      "Epoch 17817/30000 Training Loss: 0.06357680261135101\n",
      "Epoch 17818/30000 Training Loss: 0.09031780809164047\n",
      "Epoch 17819/30000 Training Loss: 0.07924666255712509\n",
      "Epoch 17820/30000 Training Loss: 0.08709929138422012\n",
      "Epoch 17820/30000 Validation Loss: 0.061598632484674454\n",
      "Epoch 17821/30000 Training Loss: 0.06213890388607979\n",
      "Epoch 17822/30000 Training Loss: 0.06245501711964607\n",
      "Epoch 17823/30000 Training Loss: 0.06125979125499725\n",
      "Epoch 17824/30000 Training Loss: 0.07007387280464172\n",
      "Epoch 17825/30000 Training Loss: 0.07704957574605942\n",
      "Epoch 17826/30000 Training Loss: 0.08631601929664612\n",
      "Epoch 17827/30000 Training Loss: 0.05946258828043938\n",
      "Epoch 17828/30000 Training Loss: 0.07619636505842209\n",
      "Epoch 17829/30000 Training Loss: 0.08396172523498535\n",
      "Epoch 17830/30000 Training Loss: 0.06185082718729973\n",
      "Epoch 17830/30000 Validation Loss: 0.06652349978685379\n",
      "Epoch 17831/30000 Training Loss: 0.0633513331413269\n",
      "Epoch 17832/30000 Training Loss: 0.0767088234424591\n",
      "Epoch 17833/30000 Training Loss: 0.05809897184371948\n",
      "Epoch 17834/30000 Training Loss: 0.07107681035995483\n",
      "Epoch 17835/30000 Training Loss: 0.06556972861289978\n",
      "Epoch 17836/30000 Training Loss: 0.07853129506111145\n",
      "Epoch 17837/30000 Training Loss: 0.06850158423185349\n",
      "Epoch 17838/30000 Training Loss: 0.08043905347585678\n",
      "Epoch 17839/30000 Training Loss: 0.08180423825979233\n",
      "Epoch 17840/30000 Training Loss: 0.06407446414232254\n",
      "Epoch 17840/30000 Validation Loss: 0.076444111764431\n",
      "Epoch 17841/30000 Training Loss: 0.08245852589607239\n",
      "Epoch 17842/30000 Training Loss: 0.060417305678129196\n",
      "Epoch 17843/30000 Training Loss: 0.07383688539266586\n",
      "Epoch 17844/30000 Training Loss: 0.07832077145576477\n",
      "Epoch 17845/30000 Training Loss: 0.0542682409286499\n",
      "Epoch 17846/30000 Training Loss: 0.06694024056196213\n",
      "Epoch 17847/30000 Training Loss: 0.07120455801486969\n",
      "Epoch 17848/30000 Training Loss: 0.07234132289886475\n",
      "Epoch 17849/30000 Training Loss: 0.07897397875785828\n",
      "Epoch 17850/30000 Training Loss: 0.06782501935958862\n",
      "Epoch 17850/30000 Validation Loss: 0.07143694907426834\n",
      "Epoch 17851/30000 Training Loss: 0.08071595430374146\n",
      "Epoch 17852/30000 Training Loss: 0.07534418255090714\n",
      "Epoch 17853/30000 Training Loss: 0.06321241706609726\n",
      "Epoch 17854/30000 Training Loss: 0.07206550985574722\n",
      "Epoch 17855/30000 Training Loss: 0.090931236743927\n",
      "Epoch 17856/30000 Training Loss: 0.07344397902488708\n",
      "Epoch 17857/30000 Training Loss: 0.06376486271619797\n",
      "Epoch 17858/30000 Training Loss: 0.07786063104867935\n",
      "Epoch 17859/30000 Training Loss: 0.0966697558760643\n",
      "Epoch 17860/30000 Training Loss: 0.07426942139863968\n",
      "Epoch 17860/30000 Validation Loss: 0.058346182107925415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17861/30000 Training Loss: 0.07002817839384079\n",
      "Epoch 17862/30000 Training Loss: 0.07370826601982117\n",
      "Epoch 17863/30000 Training Loss: 0.05800405517220497\n",
      "Epoch 17864/30000 Training Loss: 0.05127626284956932\n",
      "Epoch 17865/30000 Training Loss: 0.0641019269824028\n",
      "Epoch 17866/30000 Training Loss: 0.06553708761930466\n",
      "Epoch 17867/30000 Training Loss: 0.07404476404190063\n",
      "Epoch 17868/30000 Training Loss: 0.0648513063788414\n",
      "Epoch 17869/30000 Training Loss: 0.07436554878950119\n",
      "Epoch 17870/30000 Training Loss: 0.07233317941427231\n",
      "Epoch 17870/30000 Validation Loss: 0.08660420775413513\n",
      "Epoch 17871/30000 Training Loss: 0.07102871686220169\n",
      "Epoch 17872/30000 Training Loss: 0.06476645916700363\n",
      "Epoch 17873/30000 Training Loss: 0.07788380980491638\n",
      "Epoch 17874/30000 Training Loss: 0.06756515800952911\n",
      "Epoch 17875/30000 Training Loss: 0.07693025469779968\n",
      "Epoch 17876/30000 Training Loss: 0.06047859787940979\n",
      "Epoch 17877/30000 Training Loss: 0.06922369450330734\n",
      "Epoch 17878/30000 Training Loss: 0.06811825186014175\n",
      "Epoch 17879/30000 Training Loss: 0.06873752921819687\n",
      "Epoch 17880/30000 Training Loss: 0.06709765642881393\n",
      "Epoch 17880/30000 Validation Loss: 0.0937112495303154\n",
      "Epoch 17881/30000 Training Loss: 0.08312749117612839\n",
      "Epoch 17882/30000 Training Loss: 0.06916310638189316\n",
      "Epoch 17883/30000 Training Loss: 0.057399023324251175\n",
      "Epoch 17884/30000 Training Loss: 0.07385757565498352\n",
      "Epoch 17885/30000 Training Loss: 0.0580054335296154\n",
      "Epoch 17886/30000 Training Loss: 0.07306141406297684\n",
      "Epoch 17887/30000 Training Loss: 0.068671815097332\n",
      "Epoch 17888/30000 Training Loss: 0.06955129653215408\n",
      "Epoch 17889/30000 Training Loss: 0.08632242679595947\n",
      "Epoch 17890/30000 Training Loss: 0.06505588442087173\n",
      "Epoch 17890/30000 Validation Loss: 0.07959502935409546\n",
      "Epoch 17891/30000 Training Loss: 0.08458408713340759\n",
      "Epoch 17892/30000 Training Loss: 0.06559505313634872\n",
      "Epoch 17893/30000 Training Loss: 0.0817750096321106\n",
      "Epoch 17894/30000 Training Loss: 0.07411708682775497\n",
      "Epoch 17895/30000 Training Loss: 0.049307215958833694\n",
      "Epoch 17896/30000 Training Loss: 0.06684079021215439\n",
      "Epoch 17897/30000 Training Loss: 0.07322244346141815\n",
      "Epoch 17898/30000 Training Loss: 0.08754775673151016\n",
      "Epoch 17899/30000 Training Loss: 0.08165434002876282\n",
      "Epoch 17900/30000 Training Loss: 0.06845744699239731\n",
      "Epoch 17900/30000 Validation Loss: 0.0577220655977726\n",
      "Epoch 17901/30000 Training Loss: 0.06719880551099777\n",
      "Epoch 17902/30000 Training Loss: 0.06461735814809799\n",
      "Epoch 17903/30000 Training Loss: 0.08513326197862625\n",
      "Epoch 17904/30000 Training Loss: 0.08097277581691742\n",
      "Epoch 17905/30000 Training Loss: 0.06538436561822891\n",
      "Epoch 17906/30000 Training Loss: 0.07437075674533844\n",
      "Epoch 17907/30000 Training Loss: 0.07105116546154022\n",
      "Epoch 17908/30000 Training Loss: 0.07721836119890213\n",
      "Epoch 17909/30000 Training Loss: 0.07926302403211594\n",
      "Epoch 17910/30000 Training Loss: 0.05757268890738487\n",
      "Epoch 17910/30000 Validation Loss: 0.06271184980869293\n",
      "Epoch 17911/30000 Training Loss: 0.0706636905670166\n",
      "Epoch 17912/30000 Training Loss: 0.0816011130809784\n",
      "Epoch 17913/30000 Training Loss: 0.07786749303340912\n",
      "Epoch 17914/30000 Training Loss: 0.07986653596162796\n",
      "Epoch 17915/30000 Training Loss: 0.0904156044125557\n",
      "Epoch 17916/30000 Training Loss: 0.05528267100453377\n",
      "Epoch 17917/30000 Training Loss: 0.07890906184911728\n",
      "Epoch 17918/30000 Training Loss: 0.08640354871749878\n",
      "Epoch 17919/30000 Training Loss: 0.07151315361261368\n",
      "Epoch 17920/30000 Training Loss: 0.059629350900650024\n",
      "Epoch 17920/30000 Validation Loss: 0.07244015485048294\n",
      "Epoch 17921/30000 Training Loss: 0.07328113168478012\n",
      "Epoch 17922/30000 Training Loss: 0.06615262478590012\n",
      "Epoch 17923/30000 Training Loss: 0.05709995701909065\n",
      "Epoch 17924/30000 Training Loss: 0.06480957567691803\n",
      "Epoch 17925/30000 Training Loss: 0.07640614360570908\n",
      "Epoch 17926/30000 Training Loss: 0.08627727627754211\n",
      "Epoch 17927/30000 Training Loss: 0.06485684961080551\n",
      "Epoch 17928/30000 Training Loss: 0.09040850400924683\n",
      "Epoch 17929/30000 Training Loss: 0.0681997612118721\n",
      "Epoch 17930/30000 Training Loss: 0.07833240181207657\n",
      "Epoch 17930/30000 Validation Loss: 0.07173682004213333\n",
      "Epoch 17931/30000 Training Loss: 0.0893736481666565\n",
      "Epoch 17932/30000 Training Loss: 0.09019598364830017\n",
      "Epoch 17933/30000 Training Loss: 0.06987481564283371\n",
      "Epoch 17934/30000 Training Loss: 0.06693301349878311\n",
      "Epoch 17935/30000 Training Loss: 0.07980663329362869\n",
      "Epoch 17936/30000 Training Loss: 0.07293420284986496\n",
      "Epoch 17937/30000 Training Loss: 0.08443842083215714\n",
      "Epoch 17938/30000 Training Loss: 0.06533872336149216\n",
      "Epoch 17939/30000 Training Loss: 0.08524361997842789\n",
      "Epoch 17940/30000 Training Loss: 0.07838261127471924\n",
      "Epoch 17940/30000 Validation Loss: 0.08250606805086136\n",
      "Epoch 17941/30000 Training Loss: 0.06460500508546829\n",
      "Epoch 17942/30000 Training Loss: 0.07587937265634537\n",
      "Epoch 17943/30000 Training Loss: 0.0695270299911499\n",
      "Epoch 17944/30000 Training Loss: 0.06384376436471939\n",
      "Epoch 17945/30000 Training Loss: 0.0668085515499115\n",
      "Epoch 17946/30000 Training Loss: 0.06690690666437149\n",
      "Epoch 17947/30000 Training Loss: 0.08110330253839493\n",
      "Epoch 17948/30000 Training Loss: 0.06705431640148163\n",
      "Epoch 17949/30000 Training Loss: 0.0712171420454979\n",
      "Epoch 17950/30000 Training Loss: 0.05211606249213219\n",
      "Epoch 17950/30000 Validation Loss: 0.07314605265855789\n",
      "Epoch 17951/30000 Training Loss: 0.0650339424610138\n",
      "Epoch 17952/30000 Training Loss: 0.09805022925138474\n",
      "Epoch 17953/30000 Training Loss: 0.06414800137281418\n",
      "Epoch 17954/30000 Training Loss: 0.06750286370515823\n",
      "Epoch 17955/30000 Training Loss: 0.06385982781648636\n",
      "Epoch 17956/30000 Training Loss: 0.07603278756141663\n",
      "Epoch 17957/30000 Training Loss: 0.07235825061798096\n",
      "Epoch 17958/30000 Training Loss: 0.06587287038564682\n",
      "Epoch 17959/30000 Training Loss: 0.05552424490451813\n",
      "Epoch 17960/30000 Training Loss: 0.08531811088323593\n",
      "Epoch 17960/30000 Validation Loss: 0.0852510929107666\n",
      "Epoch 17961/30000 Training Loss: 0.07091179490089417\n",
      "Epoch 17962/30000 Training Loss: 0.0702708289027214\n",
      "Epoch 17963/30000 Training Loss: 0.06876683980226517\n",
      "Epoch 17964/30000 Training Loss: 0.07304614037275314\n",
      "Epoch 17965/30000 Training Loss: 0.06853673607110977\n",
      "Epoch 17966/30000 Training Loss: 0.08026701211929321\n",
      "Epoch 17967/30000 Training Loss: 0.07429909706115723\n",
      "Epoch 17968/30000 Training Loss: 0.07577765733003616\n",
      "Epoch 17969/30000 Training Loss: 0.0704040601849556\n",
      "Epoch 17970/30000 Training Loss: 0.08251159638166428\n",
      "Epoch 17970/30000 Validation Loss: 0.06344462186098099\n",
      "Epoch 17971/30000 Training Loss: 0.07085030525922775\n",
      "Epoch 17972/30000 Training Loss: 0.06215083226561546\n",
      "Epoch 17973/30000 Training Loss: 0.06524994969367981\n",
      "Epoch 17974/30000 Training Loss: 0.0627312883734703\n",
      "Epoch 17975/30000 Training Loss: 0.07070066779851913\n",
      "Epoch 17976/30000 Training Loss: 0.07548874616622925\n",
      "Epoch 17977/30000 Training Loss: 0.0680084228515625\n",
      "Epoch 17978/30000 Training Loss: 0.0899827852845192\n",
      "Epoch 17979/30000 Training Loss: 0.06548679620027542\n",
      "Epoch 17980/30000 Training Loss: 0.06040458008646965\n",
      "Epoch 17980/30000 Validation Loss: 0.07275038957595825\n",
      "Epoch 17981/30000 Training Loss: 0.08638127893209457\n",
      "Epoch 17982/30000 Training Loss: 0.09262868016958237\n",
      "Epoch 17983/30000 Training Loss: 0.07109623402357101\n",
      "Epoch 17984/30000 Training Loss: 0.07382585853338242\n",
      "Epoch 17985/30000 Training Loss: 0.07304958254098892\n",
      "Epoch 17986/30000 Training Loss: 0.05827463045716286\n",
      "Epoch 17987/30000 Training Loss: 0.06632652133703232\n",
      "Epoch 17988/30000 Training Loss: 0.0858844742178917\n",
      "Epoch 17989/30000 Training Loss: 0.08390706032514572\n",
      "Epoch 17990/30000 Training Loss: 0.0728100910782814\n",
      "Epoch 17990/30000 Validation Loss: 0.09316553920507431\n",
      "Epoch 17991/30000 Training Loss: 0.05889120697975159\n",
      "Epoch 17992/30000 Training Loss: 0.08311007171869278\n",
      "Epoch 17993/30000 Training Loss: 0.06511027365922928\n",
      "Epoch 17994/30000 Training Loss: 0.08612581342458725\n",
      "Epoch 17995/30000 Training Loss: 0.06519174575805664\n",
      "Epoch 17996/30000 Training Loss: 0.06767488270998001\n",
      "Epoch 17997/30000 Training Loss: 0.07766786962747574\n",
      "Epoch 17998/30000 Training Loss: 0.08788448572158813\n",
      "Epoch 17999/30000 Training Loss: 0.07659412175416946\n",
      "Epoch 18000/30000 Training Loss: 0.0671907290816307\n",
      "Epoch 18000/30000 Validation Loss: 0.07558167725801468\n",
      "Epoch 18001/30000 Training Loss: 0.05749506875872612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18002/30000 Training Loss: 0.08734685927629471\n",
      "Epoch 18003/30000 Training Loss: 0.07998886704444885\n",
      "Epoch 18004/30000 Training Loss: 0.07832213491201401\n",
      "Epoch 18005/30000 Training Loss: 0.07070088386535645\n",
      "Epoch 18006/30000 Training Loss: 0.07404894381761551\n",
      "Epoch 18007/30000 Training Loss: 0.05809149146080017\n",
      "Epoch 18008/30000 Training Loss: 0.05786137655377388\n",
      "Epoch 18009/30000 Training Loss: 0.07849390059709549\n",
      "Epoch 18010/30000 Training Loss: 0.07487080246210098\n",
      "Epoch 18010/30000 Validation Loss: 0.07757469266653061\n",
      "Epoch 18011/30000 Training Loss: 0.0659923255443573\n",
      "Epoch 18012/30000 Training Loss: 0.08175338059663773\n",
      "Epoch 18013/30000 Training Loss: 0.08096276968717575\n",
      "Epoch 18014/30000 Training Loss: 0.07935100793838501\n",
      "Epoch 18015/30000 Training Loss: 0.0814434066414833\n",
      "Epoch 18016/30000 Training Loss: 0.07270783185958862\n",
      "Epoch 18017/30000 Training Loss: 0.07731392234563828\n",
      "Epoch 18018/30000 Training Loss: 0.0702894851565361\n",
      "Epoch 18019/30000 Training Loss: 0.07767441868782043\n",
      "Epoch 18020/30000 Training Loss: 0.06628794223070145\n",
      "Epoch 18020/30000 Validation Loss: 0.08071041852235794\n",
      "Epoch 18021/30000 Training Loss: 0.06430920958518982\n",
      "Epoch 18022/30000 Training Loss: 0.08868402987718582\n",
      "Epoch 18023/30000 Training Loss: 0.07646011561155319\n",
      "Epoch 18024/30000 Training Loss: 0.06367502361536026\n",
      "Epoch 18025/30000 Training Loss: 0.08430612087249756\n",
      "Epoch 18026/30000 Training Loss: 0.059730082750320435\n",
      "Epoch 18027/30000 Training Loss: 0.07339637726545334\n",
      "Epoch 18028/30000 Training Loss: 0.078611820936203\n",
      "Epoch 18029/30000 Training Loss: 0.05839281156659126\n",
      "Epoch 18030/30000 Training Loss: 0.07634226232767105\n",
      "Epoch 18030/30000 Validation Loss: 0.06813907623291016\n",
      "Epoch 18031/30000 Training Loss: 0.0836968794465065\n",
      "Epoch 18032/30000 Training Loss: 0.06125742569565773\n",
      "Epoch 18033/30000 Training Loss: 0.07187893986701965\n",
      "Epoch 18034/30000 Training Loss: 0.0801190435886383\n",
      "Epoch 18035/30000 Training Loss: 0.06364578008651733\n",
      "Epoch 18036/30000 Training Loss: 0.08228269219398499\n",
      "Epoch 18037/30000 Training Loss: 0.07514989376068115\n",
      "Epoch 18038/30000 Training Loss: 0.06295427680015564\n",
      "Epoch 18039/30000 Training Loss: 0.07950299233198166\n",
      "Epoch 18040/30000 Training Loss: 0.05689108371734619\n",
      "Epoch 18040/30000 Validation Loss: 0.06438515335321426\n",
      "Epoch 18041/30000 Training Loss: 0.09799182415008545\n",
      "Epoch 18042/30000 Training Loss: 0.06775890290737152\n",
      "Epoch 18043/30000 Training Loss: 0.0836348906159401\n",
      "Epoch 18044/30000 Training Loss: 0.08010383695363998\n",
      "Epoch 18045/30000 Training Loss: 0.08179912716150284\n",
      "Epoch 18046/30000 Training Loss: 0.06157888099551201\n",
      "Epoch 18047/30000 Training Loss: 0.10373049229383469\n",
      "Epoch 18048/30000 Training Loss: 0.06930414587259293\n",
      "Epoch 18049/30000 Training Loss: 0.07627939432859421\n",
      "Epoch 18050/30000 Training Loss: 0.09009066224098206\n",
      "Epoch 18050/30000 Validation Loss: 0.06257878988981247\n",
      "Epoch 18051/30000 Training Loss: 0.07156413048505783\n",
      "Epoch 18052/30000 Training Loss: 0.07267412543296814\n",
      "Epoch 18053/30000 Training Loss: 0.08393487334251404\n",
      "Epoch 18054/30000 Training Loss: 0.09429734200239182\n",
      "Epoch 18055/30000 Training Loss: 0.07725486904382706\n",
      "Epoch 18056/30000 Training Loss: 0.09132969379425049\n",
      "Epoch 18057/30000 Training Loss: 0.06944651156663895\n",
      "Epoch 18058/30000 Training Loss: 0.06727730482816696\n",
      "Epoch 18059/30000 Training Loss: 0.06480544805526733\n",
      "Epoch 18060/30000 Training Loss: 0.06571952253580093\n",
      "Epoch 18060/30000 Validation Loss: 0.07285758852958679\n",
      "Epoch 18061/30000 Training Loss: 0.08630629628896713\n",
      "Epoch 18062/30000 Training Loss: 0.08013111352920532\n",
      "Epoch 18063/30000 Training Loss: 0.057272881269454956\n",
      "Epoch 18064/30000 Training Loss: 0.06429305672645569\n",
      "Epoch 18065/30000 Training Loss: 0.08616165071725845\n",
      "Epoch 18066/30000 Training Loss: 0.06512346118688583\n",
      "Epoch 18067/30000 Training Loss: 0.07698802649974823\n",
      "Epoch 18068/30000 Training Loss: 0.09813625365495682\n",
      "Epoch 18069/30000 Training Loss: 0.07642549276351929\n",
      "Epoch 18070/30000 Training Loss: 0.08784019201993942\n",
      "Epoch 18070/30000 Validation Loss: 0.06785319000482559\n",
      "Epoch 18071/30000 Training Loss: 0.05783819779753685\n",
      "Epoch 18072/30000 Training Loss: 0.06762691587209702\n",
      "Epoch 18073/30000 Training Loss: 0.06538135558366776\n",
      "Epoch 18074/30000 Training Loss: 0.0818488821387291\n",
      "Epoch 18075/30000 Training Loss: 0.07788623124361038\n",
      "Epoch 18076/30000 Training Loss: 0.091944120824337\n",
      "Epoch 18077/30000 Training Loss: 0.0919329896569252\n",
      "Epoch 18078/30000 Training Loss: 0.08939316868782043\n",
      "Epoch 18079/30000 Training Loss: 0.07424718141555786\n",
      "Epoch 18080/30000 Training Loss: 0.07127071171998978\n",
      "Epoch 18080/30000 Validation Loss: 0.07182535529136658\n",
      "Epoch 18081/30000 Training Loss: 0.08313257992267609\n",
      "Epoch 18082/30000 Training Loss: 0.08723306655883789\n",
      "Epoch 18083/30000 Training Loss: 0.08376365900039673\n",
      "Epoch 18084/30000 Training Loss: 0.06048433855175972\n",
      "Epoch 18085/30000 Training Loss: 0.08229202777147293\n",
      "Epoch 18086/30000 Training Loss: 0.07302772253751755\n",
      "Epoch 18087/30000 Training Loss: 0.060740452259778976\n",
      "Epoch 18088/30000 Training Loss: 0.09137053042650223\n",
      "Epoch 18089/30000 Training Loss: 0.07069965451955795\n",
      "Epoch 18090/30000 Training Loss: 0.0732426568865776\n",
      "Epoch 18090/30000 Validation Loss: 0.0725891962647438\n",
      "Epoch 18091/30000 Training Loss: 0.08469027280807495\n",
      "Epoch 18092/30000 Training Loss: 0.06993021070957184\n",
      "Epoch 18093/30000 Training Loss: 0.05894729867577553\n",
      "Epoch 18094/30000 Training Loss: 0.09791511297225952\n",
      "Epoch 18095/30000 Training Loss: 0.09165886789560318\n",
      "Epoch 18096/30000 Training Loss: 0.061237484216690063\n",
      "Epoch 18097/30000 Training Loss: 0.05641493201255798\n",
      "Epoch 18098/30000 Training Loss: 0.07381894439458847\n",
      "Epoch 18099/30000 Training Loss: 0.07320832461118698\n",
      "Epoch 18100/30000 Training Loss: 0.07207290083169937\n",
      "Epoch 18100/30000 Validation Loss: 0.06790392845869064\n",
      "Epoch 18101/30000 Training Loss: 0.07539815455675125\n",
      "Epoch 18102/30000 Training Loss: 0.06542255729436874\n",
      "Epoch 18103/30000 Training Loss: 0.06136475130915642\n",
      "Epoch 18104/30000 Training Loss: 0.06980101019144058\n",
      "Epoch 18105/30000 Training Loss: 0.06948234140872955\n",
      "Epoch 18106/30000 Training Loss: 0.06786912679672241\n",
      "Epoch 18107/30000 Training Loss: 0.08781567215919495\n",
      "Epoch 18108/30000 Training Loss: 0.06395667046308517\n",
      "Epoch 18109/30000 Training Loss: 0.061783567070961\n",
      "Epoch 18110/30000 Training Loss: 0.07052960246801376\n",
      "Epoch 18110/30000 Validation Loss: 0.07625787705183029\n",
      "Epoch 18111/30000 Training Loss: 0.073773093521595\n",
      "Epoch 18112/30000 Training Loss: 0.06451874226331711\n",
      "Epoch 18113/30000 Training Loss: 0.0669049546122551\n",
      "Epoch 18114/30000 Training Loss: 0.06124788522720337\n",
      "Epoch 18115/30000 Training Loss: 0.07911593466997147\n",
      "Epoch 18116/30000 Training Loss: 0.07328160852193832\n",
      "Epoch 18117/30000 Training Loss: 0.0623755156993866\n",
      "Epoch 18118/30000 Training Loss: 0.08266528695821762\n",
      "Epoch 18119/30000 Training Loss: 0.06568887084722519\n",
      "Epoch 18120/30000 Training Loss: 0.0685155838727951\n",
      "Epoch 18120/30000 Validation Loss: 0.07465378940105438\n",
      "Epoch 18121/30000 Training Loss: 0.06832580268383026\n",
      "Epoch 18122/30000 Training Loss: 0.08708138018846512\n",
      "Epoch 18123/30000 Training Loss: 0.0784301832318306\n",
      "Epoch 18124/30000 Training Loss: 0.07988858222961426\n",
      "Epoch 18125/30000 Training Loss: 0.08086889982223511\n",
      "Epoch 18126/30000 Training Loss: 0.07226100564002991\n",
      "Epoch 18127/30000 Training Loss: 0.07547248154878616\n",
      "Epoch 18128/30000 Training Loss: 0.0823226198554039\n",
      "Epoch 18129/30000 Training Loss: 0.07712172716856003\n",
      "Epoch 18130/30000 Training Loss: 0.08614170551300049\n",
      "Epoch 18130/30000 Validation Loss: 0.058320995420217514\n",
      "Epoch 18131/30000 Training Loss: 0.07659395784139633\n",
      "Epoch 18132/30000 Training Loss: 0.06330164521932602\n",
      "Epoch 18133/30000 Training Loss: 0.07624553889036179\n",
      "Epoch 18134/30000 Training Loss: 0.08269215375185013\n",
      "Epoch 18135/30000 Training Loss: 0.07435727119445801\n",
      "Epoch 18136/30000 Training Loss: 0.07322432845830917\n",
      "Epoch 18137/30000 Training Loss: 0.07211679220199585\n",
      "Epoch 18138/30000 Training Loss: 0.07473120838403702\n",
      "Epoch 18139/30000 Training Loss: 0.07283341139554977\n",
      "Epoch 18140/30000 Training Loss: 0.07891760021448135\n",
      "Epoch 18140/30000 Validation Loss: 0.07664582133293152\n",
      "Epoch 18141/30000 Training Loss: 0.08673503249883652\n",
      "Epoch 18142/30000 Training Loss: 0.08730725198984146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18143/30000 Training Loss: 0.07362411916255951\n",
      "Epoch 18144/30000 Training Loss: 0.06878085434436798\n",
      "Epoch 18145/30000 Training Loss: 0.08564289659261703\n",
      "Epoch 18146/30000 Training Loss: 0.07302484661340714\n",
      "Epoch 18147/30000 Training Loss: 0.08048897236585617\n",
      "Epoch 18148/30000 Training Loss: 0.05682248994708061\n",
      "Epoch 18149/30000 Training Loss: 0.07228848338127136\n",
      "Epoch 18150/30000 Training Loss: 0.06470752507448196\n",
      "Epoch 18150/30000 Validation Loss: 0.06194120645523071\n",
      "Epoch 18151/30000 Training Loss: 0.07462311536073685\n",
      "Epoch 18152/30000 Training Loss: 0.07816043496131897\n",
      "Epoch 18153/30000 Training Loss: 0.07080600410699844\n",
      "Epoch 18154/30000 Training Loss: 0.06720199435949326\n",
      "Epoch 18155/30000 Training Loss: 0.09468086808919907\n",
      "Epoch 18156/30000 Training Loss: 0.06907167285680771\n",
      "Epoch 18157/30000 Training Loss: 0.09963224083185196\n",
      "Epoch 18158/30000 Training Loss: 0.07272329926490784\n",
      "Epoch 18159/30000 Training Loss: 0.0666661262512207\n",
      "Epoch 18160/30000 Training Loss: 0.04904312267899513\n",
      "Epoch 18160/30000 Validation Loss: 0.09348317235708237\n",
      "Epoch 18161/30000 Training Loss: 0.0905245766043663\n",
      "Epoch 18162/30000 Training Loss: 0.06181655824184418\n",
      "Epoch 18163/30000 Training Loss: 0.0732770562171936\n",
      "Epoch 18164/30000 Training Loss: 0.07794630527496338\n",
      "Epoch 18165/30000 Training Loss: 0.07964026182889938\n",
      "Epoch 18166/30000 Training Loss: 0.06496544927358627\n",
      "Epoch 18167/30000 Training Loss: 0.05857047438621521\n",
      "Epoch 18168/30000 Training Loss: 0.0732404962182045\n",
      "Epoch 18169/30000 Training Loss: 0.0495922677218914\n",
      "Epoch 18170/30000 Training Loss: 0.06923501938581467\n",
      "Epoch 18170/30000 Validation Loss: 0.07140474766492844\n",
      "Epoch 18171/30000 Training Loss: 0.06399378180503845\n",
      "Epoch 18172/30000 Training Loss: 0.058198511600494385\n",
      "Epoch 18173/30000 Training Loss: 0.0767737478017807\n",
      "Epoch 18174/30000 Training Loss: 0.06738045066595078\n",
      "Epoch 18175/30000 Training Loss: 0.07575197517871857\n",
      "Epoch 18176/30000 Training Loss: 0.07608941197395325\n",
      "Epoch 18177/30000 Training Loss: 0.0625247210264206\n",
      "Epoch 18178/30000 Training Loss: 0.0648094117641449\n",
      "Epoch 18179/30000 Training Loss: 0.08240003138780594\n",
      "Epoch 18180/30000 Training Loss: 0.06813877820968628\n",
      "Epoch 18180/30000 Validation Loss: 0.06385122984647751\n",
      "Epoch 18181/30000 Training Loss: 0.0764702782034874\n",
      "Epoch 18182/30000 Training Loss: 0.06012551113963127\n",
      "Epoch 18183/30000 Training Loss: 0.07373997569084167\n",
      "Epoch 18184/30000 Training Loss: 0.05832235887646675\n",
      "Epoch 18185/30000 Training Loss: 0.06627476215362549\n",
      "Epoch 18186/30000 Training Loss: 0.077568419277668\n",
      "Epoch 18187/30000 Training Loss: 0.09084811806678772\n",
      "Epoch 18188/30000 Training Loss: 0.06518518179655075\n",
      "Epoch 18189/30000 Training Loss: 0.08055233210325241\n",
      "Epoch 18190/30000 Training Loss: 0.06172429397702217\n",
      "Epoch 18190/30000 Validation Loss: 0.0718880146741867\n",
      "Epoch 18191/30000 Training Loss: 0.0716560110449791\n",
      "Epoch 18192/30000 Training Loss: 0.07545092701911926\n",
      "Epoch 18193/30000 Training Loss: 0.0717129185795784\n",
      "Epoch 18194/30000 Training Loss: 0.07282046228647232\n",
      "Epoch 18195/30000 Training Loss: 0.07453051209449768\n",
      "Epoch 18196/30000 Training Loss: 0.08650758862495422\n",
      "Epoch 18197/30000 Training Loss: 0.05920163914561272\n",
      "Epoch 18198/30000 Training Loss: 0.07284102588891983\n",
      "Epoch 18199/30000 Training Loss: 0.06235288083553314\n",
      "Epoch 18200/30000 Training Loss: 0.09010686725378036\n",
      "Epoch 18200/30000 Validation Loss: 0.06622090935707092\n",
      "Epoch 18201/30000 Training Loss: 0.09906061738729477\n",
      "Epoch 18202/30000 Training Loss: 0.0750831589102745\n",
      "Epoch 18203/30000 Training Loss: 0.0764629915356636\n",
      "Epoch 18204/30000 Training Loss: 0.0843929871916771\n",
      "Epoch 18205/30000 Training Loss: 0.06860192865133286\n",
      "Epoch 18206/30000 Training Loss: 0.0675870031118393\n",
      "Epoch 18207/30000 Training Loss: 0.07903870195150375\n",
      "Epoch 18208/30000 Training Loss: 0.072004534304142\n",
      "Epoch 18209/30000 Training Loss: 0.08139350265264511\n",
      "Epoch 18210/30000 Training Loss: 0.06896232813596725\n",
      "Epoch 18210/30000 Validation Loss: 0.08969476073980331\n",
      "Epoch 18211/30000 Training Loss: 0.07739397138357162\n",
      "Epoch 18212/30000 Training Loss: 0.07922650128602982\n",
      "Epoch 18213/30000 Training Loss: 0.096269391477108\n",
      "Epoch 18214/30000 Training Loss: 0.06686960905790329\n",
      "Epoch 18215/30000 Training Loss: 0.065924733877182\n",
      "Epoch 18216/30000 Training Loss: 0.07760249823331833\n",
      "Epoch 18217/30000 Training Loss: 0.0852743610739708\n",
      "Epoch 18218/30000 Training Loss: 0.0654340535402298\n",
      "Epoch 18219/30000 Training Loss: 0.08962800353765488\n",
      "Epoch 18220/30000 Training Loss: 0.07013126462697983\n",
      "Epoch 18220/30000 Validation Loss: 0.07095059007406235\n",
      "Epoch 18221/30000 Training Loss: 0.0670749619603157\n",
      "Epoch 18222/30000 Training Loss: 0.08319845050573349\n",
      "Epoch 18223/30000 Training Loss: 0.07054232805967331\n",
      "Epoch 18224/30000 Training Loss: 0.07591912895441055\n",
      "Epoch 18225/30000 Training Loss: 0.08072100579738617\n",
      "Epoch 18226/30000 Training Loss: 0.07776319980621338\n",
      "Epoch 18227/30000 Training Loss: 0.07988948374986649\n",
      "Epoch 18228/30000 Training Loss: 0.07061240077018738\n",
      "Epoch 18229/30000 Training Loss: 0.07345085591077805\n",
      "Epoch 18230/30000 Training Loss: 0.1038137674331665\n",
      "Epoch 18230/30000 Validation Loss: 0.07171281427145004\n",
      "Epoch 18231/30000 Training Loss: 0.06737101078033447\n",
      "Epoch 18232/30000 Training Loss: 0.06178103759884834\n",
      "Epoch 18233/30000 Training Loss: 0.07593479007482529\n",
      "Epoch 18234/30000 Training Loss: 0.0830729678273201\n",
      "Epoch 18235/30000 Training Loss: 0.07823847979307175\n",
      "Epoch 18236/30000 Training Loss: 0.07142939418554306\n",
      "Epoch 18237/30000 Training Loss: 0.07219376415014267\n",
      "Epoch 18238/30000 Training Loss: 0.060172051191329956\n",
      "Epoch 18239/30000 Training Loss: 0.06980551034212112\n",
      "Epoch 18240/30000 Training Loss: 0.05582756921648979\n",
      "Epoch 18240/30000 Validation Loss: 0.0704970732331276\n",
      "Epoch 18241/30000 Training Loss: 0.06571196764707565\n",
      "Epoch 18242/30000 Training Loss: 0.06742433458566666\n",
      "Epoch 18243/30000 Training Loss: 0.07406999170780182\n",
      "Epoch 18244/30000 Training Loss: 0.08444621413946152\n",
      "Epoch 18245/30000 Training Loss: 0.0918622612953186\n",
      "Epoch 18246/30000 Training Loss: 0.07429484277963638\n",
      "Epoch 18247/30000 Training Loss: 0.06996164470911026\n",
      "Epoch 18248/30000 Training Loss: 0.061144109815359116\n",
      "Epoch 18249/30000 Training Loss: 0.06625498086214066\n",
      "Epoch 18250/30000 Training Loss: 0.07122600078582764\n",
      "Epoch 18250/30000 Validation Loss: 0.06435338407754898\n",
      "Epoch 18251/30000 Training Loss: 0.06952846050262451\n",
      "Epoch 18252/30000 Training Loss: 0.07275157421827316\n",
      "Epoch 18253/30000 Training Loss: 0.07239824533462524\n",
      "Epoch 18254/30000 Training Loss: 0.057495683431625366\n",
      "Epoch 18255/30000 Training Loss: 0.0952765941619873\n",
      "Epoch 18256/30000 Training Loss: 0.07308690994977951\n",
      "Epoch 18257/30000 Training Loss: 0.06456225365400314\n",
      "Epoch 18258/30000 Training Loss: 0.09801039099693298\n",
      "Epoch 18259/30000 Training Loss: 0.06982042640447617\n",
      "Epoch 18260/30000 Training Loss: 0.07985668629407883\n",
      "Epoch 18260/30000 Validation Loss: 0.05472499504685402\n",
      "Epoch 18261/30000 Training Loss: 0.07123517245054245\n",
      "Epoch 18262/30000 Training Loss: 0.06724344938993454\n",
      "Epoch 18263/30000 Training Loss: 0.07593954354524612\n",
      "Epoch 18264/30000 Training Loss: 0.09057895094156265\n",
      "Epoch 18265/30000 Training Loss: 0.09141958504915237\n",
      "Epoch 18266/30000 Training Loss: 0.08213463425636292\n",
      "Epoch 18267/30000 Training Loss: 0.055723000317811966\n",
      "Epoch 18268/30000 Training Loss: 0.07666665315628052\n",
      "Epoch 18269/30000 Training Loss: 0.06040719151496887\n",
      "Epoch 18270/30000 Training Loss: 0.06901400536298752\n",
      "Epoch 18270/30000 Validation Loss: 0.05597199499607086\n",
      "Epoch 18271/30000 Training Loss: 0.064723901450634\n",
      "Epoch 18272/30000 Training Loss: 0.06950885057449341\n",
      "Epoch 18273/30000 Training Loss: 0.06620779633522034\n",
      "Epoch 18274/30000 Training Loss: 0.06171800568699837\n",
      "Epoch 18275/30000 Training Loss: 0.0830090269446373\n",
      "Epoch 18276/30000 Training Loss: 0.11655479669570923\n",
      "Epoch 18277/30000 Training Loss: 0.07172880321741104\n",
      "Epoch 18278/30000 Training Loss: 0.07722342759370804\n",
      "Epoch 18279/30000 Training Loss: 0.06718119978904724\n",
      "Epoch 18280/30000 Training Loss: 0.06950253993272781\n",
      "Epoch 18280/30000 Validation Loss: 0.0891161784529686\n",
      "Epoch 18281/30000 Training Loss: 0.07833686470985413\n",
      "Epoch 18282/30000 Training Loss: 0.0691845640540123\n",
      "Epoch 18283/30000 Training Loss: 0.07171188294887543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18284/30000 Training Loss: 0.0773698166012764\n",
      "Epoch 18285/30000 Training Loss: 0.07432731986045837\n",
      "Epoch 18286/30000 Training Loss: 0.06364473700523376\n",
      "Epoch 18287/30000 Training Loss: 0.08407774567604065\n",
      "Epoch 18288/30000 Training Loss: 0.06958859413862228\n",
      "Epoch 18289/30000 Training Loss: 0.05952301248908043\n",
      "Epoch 18290/30000 Training Loss: 0.06810399889945984\n",
      "Epoch 18290/30000 Validation Loss: 0.05787433683872223\n",
      "Epoch 18291/30000 Training Loss: 0.05979730561375618\n",
      "Epoch 18292/30000 Training Loss: 0.07011250406503677\n",
      "Epoch 18293/30000 Training Loss: 0.06290131062269211\n",
      "Epoch 18294/30000 Training Loss: 0.07395108044147491\n",
      "Epoch 18295/30000 Training Loss: 0.07680332660675049\n",
      "Epoch 18296/30000 Training Loss: 0.07119674980640411\n",
      "Epoch 18297/30000 Training Loss: 0.06978770345449448\n",
      "Epoch 18298/30000 Training Loss: 0.06550765782594681\n",
      "Epoch 18299/30000 Training Loss: 0.06616569310426712\n",
      "Epoch 18300/30000 Training Loss: 0.07104802876710892\n",
      "Epoch 18300/30000 Validation Loss: 0.08207499235868454\n",
      "Epoch 18301/30000 Training Loss: 0.06219556927680969\n",
      "Epoch 18302/30000 Training Loss: 0.060629088431596756\n",
      "Epoch 18303/30000 Training Loss: 0.060219019651412964\n",
      "Epoch 18304/30000 Training Loss: 0.07420649379491806\n",
      "Epoch 18305/30000 Training Loss: 0.05877988412976265\n",
      "Epoch 18306/30000 Training Loss: 0.06036322936415672\n",
      "Epoch 18307/30000 Training Loss: 0.08307299762964249\n",
      "Epoch 18308/30000 Training Loss: 0.0641765296459198\n",
      "Epoch 18309/30000 Training Loss: 0.07642651349306107\n",
      "Epoch 18310/30000 Training Loss: 0.058232780545949936\n",
      "Epoch 18310/30000 Validation Loss: 0.06607339531183243\n",
      "Epoch 18311/30000 Training Loss: 0.06566864997148514\n",
      "Epoch 18312/30000 Training Loss: 0.08510798215866089\n",
      "Epoch 18313/30000 Training Loss: 0.08244121819734573\n",
      "Epoch 18314/30000 Training Loss: 0.07971461862325668\n",
      "Epoch 18315/30000 Training Loss: 0.08470344543457031\n",
      "Epoch 18316/30000 Training Loss: 0.08151958137750626\n",
      "Epoch 18317/30000 Training Loss: 0.06830387562513351\n",
      "Epoch 18318/30000 Training Loss: 0.07035856693983078\n",
      "Epoch 18319/30000 Training Loss: 0.09291726350784302\n",
      "Epoch 18320/30000 Training Loss: 0.07813667505979538\n",
      "Epoch 18320/30000 Validation Loss: 0.06135748326778412\n",
      "Epoch 18321/30000 Training Loss: 0.06110024079680443\n",
      "Epoch 18322/30000 Training Loss: 0.09320526570081711\n",
      "Epoch 18323/30000 Training Loss: 0.07268598675727844\n",
      "Epoch 18324/30000 Training Loss: 0.054699067026376724\n",
      "Epoch 18325/30000 Training Loss: 0.07426901906728745\n",
      "Epoch 18326/30000 Training Loss: 0.06661929935216904\n",
      "Epoch 18327/30000 Training Loss: 0.05496019497513771\n",
      "Epoch 18328/30000 Training Loss: 0.0625796988606453\n",
      "Epoch 18329/30000 Training Loss: 0.06722182035446167\n",
      "Epoch 18330/30000 Training Loss: 0.062261130660772324\n",
      "Epoch 18330/30000 Validation Loss: 0.06753721833229065\n",
      "Epoch 18331/30000 Training Loss: 0.08010541647672653\n",
      "Epoch 18332/30000 Training Loss: 0.07487786561250687\n",
      "Epoch 18333/30000 Training Loss: 0.07951024174690247\n",
      "Epoch 18334/30000 Training Loss: 0.0714515671133995\n",
      "Epoch 18335/30000 Training Loss: 0.08619344979524612\n",
      "Epoch 18336/30000 Training Loss: 0.0667777881026268\n",
      "Epoch 18337/30000 Training Loss: 0.07916270196437836\n",
      "Epoch 18338/30000 Training Loss: 0.08095089346170425\n",
      "Epoch 18339/30000 Training Loss: 0.07320239394903183\n",
      "Epoch 18340/30000 Training Loss: 0.06500884890556335\n",
      "Epoch 18340/30000 Validation Loss: 0.08266466110944748\n",
      "Epoch 18341/30000 Training Loss: 0.07930385321378708\n",
      "Epoch 18342/30000 Training Loss: 0.08244021981954575\n",
      "Epoch 18343/30000 Training Loss: 0.06122833117842674\n",
      "Epoch 18344/30000 Training Loss: 0.06414570659399033\n",
      "Epoch 18345/30000 Training Loss: 0.058939993381500244\n",
      "Epoch 18346/30000 Training Loss: 0.07378338277339935\n",
      "Epoch 18347/30000 Training Loss: 0.074517160654068\n",
      "Epoch 18348/30000 Training Loss: 0.1000860333442688\n",
      "Epoch 18349/30000 Training Loss: 0.06749277561903\n",
      "Epoch 18350/30000 Training Loss: 0.06952857971191406\n",
      "Epoch 18350/30000 Validation Loss: 0.08248213678598404\n",
      "Epoch 18351/30000 Training Loss: 0.08694297820329666\n",
      "Epoch 18352/30000 Training Loss: 0.07041061669588089\n",
      "Epoch 18353/30000 Training Loss: 0.08027763664722443\n",
      "Epoch 18354/30000 Training Loss: 0.07372253388166428\n",
      "Epoch 18355/30000 Training Loss: 0.06694680452346802\n",
      "Epoch 18356/30000 Training Loss: 0.06446566432714462\n",
      "Epoch 18357/30000 Training Loss: 0.05864090844988823\n",
      "Epoch 18358/30000 Training Loss: 0.07045727968215942\n",
      "Epoch 18359/30000 Training Loss: 0.0625908300280571\n",
      "Epoch 18360/30000 Training Loss: 0.06871995329856873\n",
      "Epoch 18360/30000 Validation Loss: 0.07046584039926529\n",
      "Epoch 18361/30000 Training Loss: 0.07722402364015579\n",
      "Epoch 18362/30000 Training Loss: 0.07477960735559464\n",
      "Epoch 18363/30000 Training Loss: 0.07638490200042725\n",
      "Epoch 18364/30000 Training Loss: 0.06991645693778992\n",
      "Epoch 18365/30000 Training Loss: 0.07435532659292221\n",
      "Epoch 18366/30000 Training Loss: 0.08185184746980667\n",
      "Epoch 18367/30000 Training Loss: 0.06902867555618286\n",
      "Epoch 18368/30000 Training Loss: 0.07612669467926025\n",
      "Epoch 18369/30000 Training Loss: 0.06872496753931046\n",
      "Epoch 18370/30000 Training Loss: 0.05721799656748772\n",
      "Epoch 18370/30000 Validation Loss: 0.06227120757102966\n",
      "Epoch 18371/30000 Training Loss: 0.055485960096120834\n",
      "Epoch 18372/30000 Training Loss: 0.08366798609495163\n",
      "Epoch 18373/30000 Training Loss: 0.06015656888484955\n",
      "Epoch 18374/30000 Training Loss: 0.07999586313962936\n",
      "Epoch 18375/30000 Training Loss: 0.06526744365692139\n",
      "Epoch 18376/30000 Training Loss: 0.07279505580663681\n",
      "Epoch 18377/30000 Training Loss: 0.06644900143146515\n",
      "Epoch 18378/30000 Training Loss: 0.0534011535346508\n",
      "Epoch 18379/30000 Training Loss: 0.07516977190971375\n",
      "Epoch 18380/30000 Training Loss: 0.058550696820020676\n",
      "Epoch 18380/30000 Validation Loss: 0.0965360477566719\n",
      "Epoch 18381/30000 Training Loss: 0.06900712102651596\n",
      "Epoch 18382/30000 Training Loss: 0.06703630089759827\n",
      "Epoch 18383/30000 Training Loss: 0.08128964900970459\n",
      "Epoch 18384/30000 Training Loss: 0.06469426304101944\n",
      "Epoch 18385/30000 Training Loss: 0.08865810185670853\n",
      "Epoch 18386/30000 Training Loss: 0.07658296078443527\n",
      "Epoch 18387/30000 Training Loss: 0.09899470955133438\n",
      "Epoch 18388/30000 Training Loss: 0.0782458484172821\n",
      "Epoch 18389/30000 Training Loss: 0.06054959073662758\n",
      "Epoch 18390/30000 Training Loss: 0.06810875982046127\n",
      "Epoch 18390/30000 Validation Loss: 0.06681086122989655\n",
      "Epoch 18391/30000 Training Loss: 0.07170827686786652\n",
      "Epoch 18392/30000 Training Loss: 0.05806998908519745\n",
      "Epoch 18393/30000 Training Loss: 0.06940986961126328\n",
      "Epoch 18394/30000 Training Loss: 0.0576288104057312\n",
      "Epoch 18395/30000 Training Loss: 0.07099651545286179\n",
      "Epoch 18396/30000 Training Loss: 0.08003773540258408\n",
      "Epoch 18397/30000 Training Loss: 0.08398552983999252\n",
      "Epoch 18398/30000 Training Loss: 0.09107988327741623\n",
      "Epoch 18399/30000 Training Loss: 0.06655444949865341\n",
      "Epoch 18400/30000 Training Loss: 0.07602342218160629\n",
      "Epoch 18400/30000 Validation Loss: 0.09597540646791458\n",
      "Epoch 18401/30000 Training Loss: 0.061106353998184204\n",
      "Epoch 18402/30000 Training Loss: 0.08555074781179428\n",
      "Epoch 18403/30000 Training Loss: 0.09746626764535904\n",
      "Epoch 18404/30000 Training Loss: 0.10160869359970093\n",
      "Epoch 18405/30000 Training Loss: 0.06382215768098831\n",
      "Epoch 18406/30000 Training Loss: 0.07626548409461975\n",
      "Epoch 18407/30000 Training Loss: 0.05699274316430092\n",
      "Epoch 18408/30000 Training Loss: 0.06499356776475906\n",
      "Epoch 18409/30000 Training Loss: 0.08363654464483261\n",
      "Epoch 18410/30000 Training Loss: 0.0618116557598114\n",
      "Epoch 18410/30000 Validation Loss: 0.07847955822944641\n",
      "Epoch 18411/30000 Training Loss: 0.10305950045585632\n",
      "Epoch 18412/30000 Training Loss: 0.08145210891962051\n",
      "Epoch 18413/30000 Training Loss: 0.08055334538221359\n",
      "Epoch 18414/30000 Training Loss: 0.07128249853849411\n",
      "Epoch 18415/30000 Training Loss: 0.07362061738967896\n",
      "Epoch 18416/30000 Training Loss: 0.08266424387693405\n",
      "Epoch 18417/30000 Training Loss: 0.06021498143672943\n",
      "Epoch 18418/30000 Training Loss: 0.07468090206384659\n",
      "Epoch 18419/30000 Training Loss: 0.06461616605520248\n",
      "Epoch 18420/30000 Training Loss: 0.07108008861541748\n",
      "Epoch 18420/30000 Validation Loss: 0.0768551304936409\n",
      "Epoch 18421/30000 Training Loss: 0.07277238368988037\n",
      "Epoch 18422/30000 Training Loss: 0.06318040937185287\n",
      "Epoch 18423/30000 Training Loss: 0.05058056488633156\n",
      "Epoch 18424/30000 Training Loss: 0.07341522723436356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18425/30000 Training Loss: 0.07417014986276627\n",
      "Epoch 18426/30000 Training Loss: 0.06691163033246994\n",
      "Epoch 18427/30000 Training Loss: 0.054494600743055344\n",
      "Epoch 18428/30000 Training Loss: 0.06496740132570267\n",
      "Epoch 18429/30000 Training Loss: 0.07773654162883759\n",
      "Epoch 18430/30000 Training Loss: 0.08595729619264603\n",
      "Epoch 18430/30000 Validation Loss: 0.0681900605559349\n",
      "Epoch 18431/30000 Training Loss: 0.08319947868585587\n",
      "Epoch 18432/30000 Training Loss: 0.06499874591827393\n",
      "Epoch 18433/30000 Training Loss: 0.07504915446043015\n",
      "Epoch 18434/30000 Training Loss: 0.09420766681432724\n",
      "Epoch 18435/30000 Training Loss: 0.08747616410255432\n",
      "Epoch 18436/30000 Training Loss: 0.06850666552782059\n",
      "Epoch 18437/30000 Training Loss: 0.05600704252719879\n",
      "Epoch 18438/30000 Training Loss: 0.08487986773252487\n",
      "Epoch 18439/30000 Training Loss: 0.045918431133031845\n",
      "Epoch 18440/30000 Training Loss: 0.08659926801919937\n",
      "Epoch 18440/30000 Validation Loss: 0.06991568952798843\n",
      "Epoch 18441/30000 Training Loss: 0.06646057218313217\n",
      "Epoch 18442/30000 Training Loss: 0.09567145258188248\n",
      "Epoch 18443/30000 Training Loss: 0.08037832379341125\n",
      "Epoch 18444/30000 Training Loss: 0.04624702036380768\n",
      "Epoch 18445/30000 Training Loss: 0.0735987201333046\n",
      "Epoch 18446/30000 Training Loss: 0.08098291605710983\n",
      "Epoch 18447/30000 Training Loss: 0.07454299181699753\n",
      "Epoch 18448/30000 Training Loss: 0.08995777368545532\n",
      "Epoch 18449/30000 Training Loss: 0.07113758474588394\n",
      "Epoch 18450/30000 Training Loss: 0.06980841606855392\n",
      "Epoch 18450/30000 Validation Loss: 0.0582628957927227\n",
      "Epoch 18451/30000 Training Loss: 0.07223685830831528\n",
      "Epoch 18452/30000 Training Loss: 0.07529819756746292\n",
      "Epoch 18453/30000 Training Loss: 0.07450711727142334\n",
      "Epoch 18454/30000 Training Loss: 0.09344031661748886\n",
      "Epoch 18455/30000 Training Loss: 0.08608636260032654\n",
      "Epoch 18456/30000 Training Loss: 0.0805177390575409\n",
      "Epoch 18457/30000 Training Loss: 0.08116894215345383\n",
      "Epoch 18458/30000 Training Loss: 0.0602276511490345\n",
      "Epoch 18459/30000 Training Loss: 0.08241564780473709\n",
      "Epoch 18460/30000 Training Loss: 0.07925970107316971\n",
      "Epoch 18460/30000 Validation Loss: 0.06490916758775711\n",
      "Epoch 18461/30000 Training Loss: 0.07430347800254822\n",
      "Epoch 18462/30000 Training Loss: 0.052803050726652145\n",
      "Epoch 18463/30000 Training Loss: 0.06791037321090698\n",
      "Epoch 18464/30000 Training Loss: 0.07281021028757095\n",
      "Epoch 18465/30000 Training Loss: 0.08007524907588959\n",
      "Epoch 18466/30000 Training Loss: 0.06960504502058029\n",
      "Epoch 18467/30000 Training Loss: 0.07144054770469666\n",
      "Epoch 18468/30000 Training Loss: 0.05592025816440582\n",
      "Epoch 18469/30000 Training Loss: 0.0750085711479187\n",
      "Epoch 18470/30000 Training Loss: 0.07901615649461746\n",
      "Epoch 18470/30000 Validation Loss: 0.07495272159576416\n",
      "Epoch 18471/30000 Training Loss: 0.08466073870658875\n",
      "Epoch 18472/30000 Training Loss: 0.0683179423213005\n",
      "Epoch 18473/30000 Training Loss: 0.07549010962247849\n",
      "Epoch 18474/30000 Training Loss: 0.07085341215133667\n",
      "Epoch 18475/30000 Training Loss: 0.07372218370437622\n",
      "Epoch 18476/30000 Training Loss: 0.07143820077180862\n",
      "Epoch 18477/30000 Training Loss: 0.07079149037599564\n",
      "Epoch 18478/30000 Training Loss: 0.05789409205317497\n",
      "Epoch 18479/30000 Training Loss: 0.08519468456506729\n",
      "Epoch 18480/30000 Training Loss: 0.07932418584823608\n",
      "Epoch 18480/30000 Validation Loss: 0.06362783908843994\n",
      "Epoch 18481/30000 Training Loss: 0.07746672630310059\n",
      "Epoch 18482/30000 Training Loss: 0.06622489541769028\n",
      "Epoch 18483/30000 Training Loss: 0.08215475082397461\n",
      "Epoch 18484/30000 Training Loss: 0.06899898499250412\n",
      "Epoch 18485/30000 Training Loss: 0.06382939219474792\n",
      "Epoch 18486/30000 Training Loss: 0.07855238765478134\n",
      "Epoch 18487/30000 Training Loss: 0.05914567410945892\n",
      "Epoch 18488/30000 Training Loss: 0.06331182271242142\n",
      "Epoch 18489/30000 Training Loss: 0.08580977469682693\n",
      "Epoch 18490/30000 Training Loss: 0.07479293644428253\n",
      "Epoch 18490/30000 Validation Loss: 0.06483086198568344\n",
      "Epoch 18491/30000 Training Loss: 0.07888378947973251\n",
      "Epoch 18492/30000 Training Loss: 0.059292182326316833\n",
      "Epoch 18493/30000 Training Loss: 0.08532559126615524\n",
      "Epoch 18494/30000 Training Loss: 0.07255074381828308\n",
      "Epoch 18495/30000 Training Loss: 0.07862383872270584\n",
      "Epoch 18496/30000 Training Loss: 0.08169117569923401\n",
      "Epoch 18497/30000 Training Loss: 0.07846304029226303\n",
      "Epoch 18498/30000 Training Loss: 0.06007862091064453\n",
      "Epoch 18499/30000 Training Loss: 0.06420458108186722\n",
      "Epoch 18500/30000 Training Loss: 0.07146402448415756\n",
      "Epoch 18500/30000 Validation Loss: 0.06027955934405327\n",
      "Epoch 18501/30000 Training Loss: 0.06590697169303894\n",
      "Epoch 18502/30000 Training Loss: 0.06311222165822983\n",
      "Epoch 18503/30000 Training Loss: 0.06781844794750214\n",
      "Epoch 18504/30000 Training Loss: 0.08239749819040298\n",
      "Epoch 18505/30000 Training Loss: 0.06757519394159317\n",
      "Epoch 18506/30000 Training Loss: 0.06234027072787285\n",
      "Epoch 18507/30000 Training Loss: 0.07243669033050537\n",
      "Epoch 18508/30000 Training Loss: 0.07611223310232162\n",
      "Epoch 18509/30000 Training Loss: 0.08716830611228943\n",
      "Epoch 18510/30000 Training Loss: 0.07377147674560547\n",
      "Epoch 18510/30000 Validation Loss: 0.06949550658464432\n",
      "Epoch 18511/30000 Training Loss: 0.07192521542310715\n",
      "Epoch 18512/30000 Training Loss: 0.08307864516973495\n",
      "Epoch 18513/30000 Training Loss: 0.06552190333604813\n",
      "Epoch 18514/30000 Training Loss: 0.06706976145505905\n",
      "Epoch 18515/30000 Training Loss: 0.07174665480852127\n",
      "Epoch 18516/30000 Training Loss: 0.0727614015340805\n",
      "Epoch 18517/30000 Training Loss: 0.07544323056936264\n",
      "Epoch 18518/30000 Training Loss: 0.07676147669553757\n",
      "Epoch 18519/30000 Training Loss: 0.07728402316570282\n",
      "Epoch 18520/30000 Training Loss: 0.08172555267810822\n",
      "Epoch 18520/30000 Validation Loss: 0.08393875509500504\n",
      "Epoch 18521/30000 Training Loss: 0.07046594470739365\n",
      "Epoch 18522/30000 Training Loss: 0.06671422719955444\n",
      "Epoch 18523/30000 Training Loss: 0.059585098177194595\n",
      "Epoch 18524/30000 Training Loss: 0.0750468373298645\n",
      "Epoch 18525/30000 Training Loss: 0.0686478391289711\n",
      "Epoch 18526/30000 Training Loss: 0.07348256558179855\n",
      "Epoch 18527/30000 Training Loss: 0.06682108342647552\n",
      "Epoch 18528/30000 Training Loss: 0.06844732910394669\n",
      "Epoch 18529/30000 Training Loss: 0.07035704702138901\n",
      "Epoch 18530/30000 Training Loss: 0.06659528613090515\n",
      "Epoch 18530/30000 Validation Loss: 0.06664282828569412\n",
      "Epoch 18531/30000 Training Loss: 0.06821346282958984\n",
      "Epoch 18532/30000 Training Loss: 0.08219759166240692\n",
      "Epoch 18533/30000 Training Loss: 0.06974884122610092\n",
      "Epoch 18534/30000 Training Loss: 0.06548254191875458\n",
      "Epoch 18535/30000 Training Loss: 0.08218761533498764\n",
      "Epoch 18536/30000 Training Loss: 0.07527155429124832\n",
      "Epoch 18537/30000 Training Loss: 0.08259756118059158\n",
      "Epoch 18538/30000 Training Loss: 0.09380415827035904\n",
      "Epoch 18539/30000 Training Loss: 0.07469486445188522\n",
      "Epoch 18540/30000 Training Loss: 0.07586533576250076\n",
      "Epoch 18540/30000 Validation Loss: 0.08691016584634781\n",
      "Epoch 18541/30000 Training Loss: 0.09019441157579422\n",
      "Epoch 18542/30000 Training Loss: 0.0975327119231224\n",
      "Epoch 18543/30000 Training Loss: 0.06318479031324387\n",
      "Epoch 18544/30000 Training Loss: 0.06560537219047546\n",
      "Epoch 18545/30000 Training Loss: 0.058737292885780334\n",
      "Epoch 18546/30000 Training Loss: 0.06823018938302994\n",
      "Epoch 18547/30000 Training Loss: 0.08345166593790054\n",
      "Epoch 18548/30000 Training Loss: 0.05573434755206108\n",
      "Epoch 18549/30000 Training Loss: 0.0859832763671875\n",
      "Epoch 18550/30000 Training Loss: 0.07941875606775284\n",
      "Epoch 18550/30000 Validation Loss: 0.07001011818647385\n",
      "Epoch 18551/30000 Training Loss: 0.07609153538942337\n",
      "Epoch 18552/30000 Training Loss: 0.07370161265134811\n",
      "Epoch 18553/30000 Training Loss: 0.06611981242895126\n",
      "Epoch 18554/30000 Training Loss: 0.0730518102645874\n",
      "Epoch 18555/30000 Training Loss: 0.06542455404996872\n",
      "Epoch 18556/30000 Training Loss: 0.07267673313617706\n",
      "Epoch 18557/30000 Training Loss: 0.07504088431596756\n",
      "Epoch 18558/30000 Training Loss: 0.06027821823954582\n",
      "Epoch 18559/30000 Training Loss: 0.06914956122636795\n",
      "Epoch 18560/30000 Training Loss: 0.06569131463766098\n",
      "Epoch 18560/30000 Validation Loss: 0.07829704135656357\n",
      "Epoch 18561/30000 Training Loss: 0.06778693944215775\n",
      "Epoch 18562/30000 Training Loss: 0.06933719664812088\n",
      "Epoch 18563/30000 Training Loss: 0.06786704063415527\n",
      "Epoch 18564/30000 Training Loss: 0.09250440448522568\n",
      "Epoch 18565/30000 Training Loss: 0.07382375746965408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18566/30000 Training Loss: 0.0703948587179184\n",
      "Epoch 18567/30000 Training Loss: 0.07242294400930405\n",
      "Epoch 18568/30000 Training Loss: 0.07717631012201309\n",
      "Epoch 18569/30000 Training Loss: 0.059407711029052734\n",
      "Epoch 18570/30000 Training Loss: 0.09309098869562149\n",
      "Epoch 18570/30000 Validation Loss: 0.08096469193696976\n",
      "Epoch 18571/30000 Training Loss: 0.07354647666215897\n",
      "Epoch 18572/30000 Training Loss: 0.06540091335773468\n",
      "Epoch 18573/30000 Training Loss: 0.06429647654294968\n",
      "Epoch 18574/30000 Training Loss: 0.06479766964912415\n",
      "Epoch 18575/30000 Training Loss: 0.06595568358898163\n",
      "Epoch 18576/30000 Training Loss: 0.0699949860572815\n",
      "Epoch 18577/30000 Training Loss: 0.07353947311639786\n",
      "Epoch 18578/30000 Training Loss: 0.1161113902926445\n",
      "Epoch 18579/30000 Training Loss: 0.07696732133626938\n",
      "Epoch 18580/30000 Training Loss: 0.07495520263910294\n",
      "Epoch 18580/30000 Validation Loss: 0.07140793651342392\n",
      "Epoch 18581/30000 Training Loss: 0.07290355116128922\n",
      "Epoch 18582/30000 Training Loss: 0.08036420494318008\n",
      "Epoch 18583/30000 Training Loss: 0.06232186779379845\n",
      "Epoch 18584/30000 Training Loss: 0.06824728101491928\n",
      "Epoch 18585/30000 Training Loss: 0.06991437077522278\n",
      "Epoch 18586/30000 Training Loss: 0.08534049987792969\n",
      "Epoch 18587/30000 Training Loss: 0.08398410677909851\n",
      "Epoch 18588/30000 Training Loss: 0.07900047302246094\n",
      "Epoch 18589/30000 Training Loss: 0.06673762947320938\n",
      "Epoch 18590/30000 Training Loss: 0.08494985103607178\n",
      "Epoch 18590/30000 Validation Loss: 0.0743839219212532\n",
      "Epoch 18591/30000 Training Loss: 0.07003290951251984\n",
      "Epoch 18592/30000 Training Loss: 0.05465475842356682\n",
      "Epoch 18593/30000 Training Loss: 0.08649828284978867\n",
      "Epoch 18594/30000 Training Loss: 0.08073937147855759\n",
      "Epoch 18595/30000 Training Loss: 0.0853046178817749\n",
      "Epoch 18596/30000 Training Loss: 0.05723557993769646\n",
      "Epoch 18597/30000 Training Loss: 0.0727052316069603\n",
      "Epoch 18598/30000 Training Loss: 0.07209930568933487\n",
      "Epoch 18599/30000 Training Loss: 0.08275598287582397\n",
      "Epoch 18600/30000 Training Loss: 0.06267275661230087\n",
      "Epoch 18600/30000 Validation Loss: 0.06393557786941528\n",
      "Epoch 18601/30000 Training Loss: 0.0803958997130394\n",
      "Epoch 18602/30000 Training Loss: 0.08551836013793945\n",
      "Epoch 18603/30000 Training Loss: 0.08989584445953369\n",
      "Epoch 18604/30000 Training Loss: 0.07488321512937546\n",
      "Epoch 18605/30000 Training Loss: 0.0917060375213623\n",
      "Epoch 18606/30000 Training Loss: 0.06537657976150513\n",
      "Epoch 18607/30000 Training Loss: 0.10000147670507431\n",
      "Epoch 18608/30000 Training Loss: 0.06870172172784805\n",
      "Epoch 18609/30000 Training Loss: 0.07784177362918854\n",
      "Epoch 18610/30000 Training Loss: 0.06518864631652832\n",
      "Epoch 18610/30000 Validation Loss: 0.07228868454694748\n",
      "Epoch 18611/30000 Training Loss: 0.06665395945310593\n",
      "Epoch 18612/30000 Training Loss: 0.06098540127277374\n",
      "Epoch 18613/30000 Training Loss: 0.09119979292154312\n",
      "Epoch 18614/30000 Training Loss: 0.06464656442403793\n",
      "Epoch 18615/30000 Training Loss: 0.06933868676424026\n",
      "Epoch 18616/30000 Training Loss: 0.08829140663146973\n",
      "Epoch 18617/30000 Training Loss: 0.07345378398895264\n",
      "Epoch 18618/30000 Training Loss: 0.0651373565196991\n",
      "Epoch 18619/30000 Training Loss: 0.07703718543052673\n",
      "Epoch 18620/30000 Training Loss: 0.06558956950902939\n",
      "Epoch 18620/30000 Validation Loss: 0.07192404568195343\n",
      "Epoch 18621/30000 Training Loss: 0.05421285703778267\n",
      "Epoch 18622/30000 Training Loss: 0.08733627200126648\n",
      "Epoch 18623/30000 Training Loss: 0.08115480095148087\n",
      "Epoch 18624/30000 Training Loss: 0.06527495384216309\n",
      "Epoch 18625/30000 Training Loss: 0.07817619293928146\n",
      "Epoch 18626/30000 Training Loss: 0.07201017439365387\n",
      "Epoch 18627/30000 Training Loss: 0.06717456132173538\n",
      "Epoch 18628/30000 Training Loss: 0.08535917848348618\n",
      "Epoch 18629/30000 Training Loss: 0.08515062183141708\n",
      "Epoch 18630/30000 Training Loss: 0.08887750655412674\n",
      "Epoch 18630/30000 Validation Loss: 0.06794201582670212\n",
      "Epoch 18631/30000 Training Loss: 0.0618889182806015\n",
      "Epoch 18632/30000 Training Loss: 0.07057182490825653\n",
      "Epoch 18633/30000 Training Loss: 0.0815739706158638\n",
      "Epoch 18634/30000 Training Loss: 0.06163319945335388\n",
      "Epoch 18635/30000 Training Loss: 0.06680434197187424\n",
      "Epoch 18636/30000 Training Loss: 0.0741400197148323\n",
      "Epoch 18637/30000 Training Loss: 0.07080549746751785\n",
      "Epoch 18638/30000 Training Loss: 0.0695330947637558\n",
      "Epoch 18639/30000 Training Loss: 0.0781945213675499\n",
      "Epoch 18640/30000 Training Loss: 0.05514386668801308\n",
      "Epoch 18640/30000 Validation Loss: 0.08161566406488419\n",
      "Epoch 18641/30000 Training Loss: 0.06536269187927246\n",
      "Epoch 18642/30000 Training Loss: 0.06573087722063065\n",
      "Epoch 18643/30000 Training Loss: 0.08364216238260269\n",
      "Epoch 18644/30000 Training Loss: 0.08335316181182861\n",
      "Epoch 18645/30000 Training Loss: 0.06816699355840683\n",
      "Epoch 18646/30000 Training Loss: 0.08727437257766724\n",
      "Epoch 18647/30000 Training Loss: 0.06131899356842041\n",
      "Epoch 18648/30000 Training Loss: 0.0519421361386776\n",
      "Epoch 18649/30000 Training Loss: 0.06823237985372543\n",
      "Epoch 18650/30000 Training Loss: 0.06464364379644394\n",
      "Epoch 18650/30000 Validation Loss: 0.08379408717155457\n",
      "Epoch 18651/30000 Training Loss: 0.07501909136772156\n",
      "Epoch 18652/30000 Training Loss: 0.07289714366197586\n",
      "Epoch 18653/30000 Training Loss: 0.09131977707147598\n",
      "Epoch 18654/30000 Training Loss: 0.06349649280309677\n",
      "Epoch 18655/30000 Training Loss: 0.06383246183395386\n",
      "Epoch 18656/30000 Training Loss: 0.06400404125452042\n",
      "Epoch 18657/30000 Training Loss: 0.07208260148763657\n",
      "Epoch 18658/30000 Training Loss: 0.07198036462068558\n",
      "Epoch 18659/30000 Training Loss: 0.07411125302314758\n",
      "Epoch 18660/30000 Training Loss: 0.07332272827625275\n",
      "Epoch 18660/30000 Validation Loss: 0.09172758460044861\n",
      "Epoch 18661/30000 Training Loss: 0.06796512752771378\n",
      "Epoch 18662/30000 Training Loss: 0.09677311033010483\n",
      "Epoch 18663/30000 Training Loss: 0.09595086425542831\n",
      "Epoch 18664/30000 Training Loss: 0.06655243039131165\n",
      "Epoch 18665/30000 Training Loss: 0.06243305280804634\n",
      "Epoch 18666/30000 Training Loss: 0.06876406073570251\n",
      "Epoch 18667/30000 Training Loss: 0.07922881096601486\n",
      "Epoch 18668/30000 Training Loss: 0.06694695353507996\n",
      "Epoch 18669/30000 Training Loss: 0.07309069484472275\n",
      "Epoch 18670/30000 Training Loss: 0.08091941475868225\n",
      "Epoch 18670/30000 Validation Loss: 0.08373803645372391\n",
      "Epoch 18671/30000 Training Loss: 0.0766838863492012\n",
      "Epoch 18672/30000 Training Loss: 0.06399987637996674\n",
      "Epoch 18673/30000 Training Loss: 0.07186931371688843\n",
      "Epoch 18674/30000 Training Loss: 0.07654324918985367\n",
      "Epoch 18675/30000 Training Loss: 0.07065572589635849\n",
      "Epoch 18676/30000 Training Loss: 0.0793602466583252\n",
      "Epoch 18677/30000 Training Loss: 0.09359219670295715\n",
      "Epoch 18678/30000 Training Loss: 0.061977971345186234\n",
      "Epoch 18679/30000 Training Loss: 0.07670117169618607\n",
      "Epoch 18680/30000 Training Loss: 0.07049872726202011\n",
      "Epoch 18680/30000 Validation Loss: 0.07880836725234985\n",
      "Epoch 18681/30000 Training Loss: 0.05781187489628792\n",
      "Epoch 18682/30000 Training Loss: 0.0743018090724945\n",
      "Epoch 18683/30000 Training Loss: 0.07409373670816422\n",
      "Epoch 18684/30000 Training Loss: 0.08156435936689377\n",
      "Epoch 18685/30000 Training Loss: 0.06847696751356125\n",
      "Epoch 18686/30000 Training Loss: 0.06541790813207626\n",
      "Epoch 18687/30000 Training Loss: 0.07897710800170898\n",
      "Epoch 18688/30000 Training Loss: 0.07323480397462845\n",
      "Epoch 18689/30000 Training Loss: 0.07280343025922775\n",
      "Epoch 18690/30000 Training Loss: 0.07580056041479111\n",
      "Epoch 18690/30000 Validation Loss: 0.06834738701581955\n",
      "Epoch 18691/30000 Training Loss: 0.07711853832006454\n",
      "Epoch 18692/30000 Training Loss: 0.0853557214140892\n",
      "Epoch 18693/30000 Training Loss: 0.06905470043420792\n",
      "Epoch 18694/30000 Training Loss: 0.1010071262717247\n",
      "Epoch 18695/30000 Training Loss: 0.056098878383636475\n",
      "Epoch 18696/30000 Training Loss: 0.06701231747865677\n",
      "Epoch 18697/30000 Training Loss: 0.08167126029729843\n",
      "Epoch 18698/30000 Training Loss: 0.07109065353870392\n",
      "Epoch 18699/30000 Training Loss: 0.06909188628196716\n",
      "Epoch 18700/30000 Training Loss: 0.07299167662858963\n",
      "Epoch 18700/30000 Validation Loss: 0.07632005959749222\n",
      "Epoch 18701/30000 Training Loss: 0.0768149271607399\n",
      "Epoch 18702/30000 Training Loss: 0.07777375727891922\n",
      "Epoch 18703/30000 Training Loss: 0.08314890414476395\n",
      "Epoch 18704/30000 Training Loss: 0.06337855011224747\n",
      "Epoch 18705/30000 Training Loss: 0.07099629193544388\n",
      "Epoch 18706/30000 Training Loss: 0.0701964721083641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18707/30000 Training Loss: 0.06668076664209366\n",
      "Epoch 18708/30000 Training Loss: 0.06705998629331589\n",
      "Epoch 18709/30000 Training Loss: 0.06694293022155762\n",
      "Epoch 18710/30000 Training Loss: 0.06295641511678696\n",
      "Epoch 18710/30000 Validation Loss: 0.06616362929344177\n",
      "Epoch 18711/30000 Training Loss: 0.08371275663375854\n",
      "Epoch 18712/30000 Training Loss: 0.06218928098678589\n",
      "Epoch 18713/30000 Training Loss: 0.06683843582868576\n",
      "Epoch 18714/30000 Training Loss: 0.06472239643335342\n",
      "Epoch 18715/30000 Training Loss: 0.08955693989992142\n",
      "Epoch 18716/30000 Training Loss: 0.05842344090342522\n",
      "Epoch 18717/30000 Training Loss: 0.07090907543897629\n",
      "Epoch 18718/30000 Training Loss: 0.06275095790624619\n",
      "Epoch 18719/30000 Training Loss: 0.06878648698329926\n",
      "Epoch 18720/30000 Training Loss: 0.06555869430303574\n",
      "Epoch 18720/30000 Validation Loss: 0.05954648181796074\n",
      "Epoch 18721/30000 Training Loss: 0.06665170192718506\n",
      "Epoch 18722/30000 Training Loss: 0.0735233798623085\n",
      "Epoch 18723/30000 Training Loss: 0.07317348569631577\n",
      "Epoch 18724/30000 Training Loss: 0.08381586521863937\n",
      "Epoch 18725/30000 Training Loss: 0.09494048357009888\n",
      "Epoch 18726/30000 Training Loss: 0.08431752771139145\n",
      "Epoch 18727/30000 Training Loss: 0.07392895966768265\n",
      "Epoch 18728/30000 Training Loss: 0.06729330867528915\n",
      "Epoch 18729/30000 Training Loss: 0.06506815552711487\n",
      "Epoch 18730/30000 Training Loss: 0.0809452161192894\n",
      "Epoch 18730/30000 Validation Loss: 0.08198043704032898\n",
      "Epoch 18731/30000 Training Loss: 0.06691623479127884\n",
      "Epoch 18732/30000 Training Loss: 0.07598525285720825\n",
      "Epoch 18733/30000 Training Loss: 0.06871359795331955\n",
      "Epoch 18734/30000 Training Loss: 0.06475311517715454\n",
      "Epoch 18735/30000 Training Loss: 0.08017614483833313\n",
      "Epoch 18736/30000 Training Loss: 0.06954529881477356\n",
      "Epoch 18737/30000 Training Loss: 0.07687919586896896\n",
      "Epoch 18738/30000 Training Loss: 0.07660869508981705\n",
      "Epoch 18739/30000 Training Loss: 0.0715019553899765\n",
      "Epoch 18740/30000 Training Loss: 0.06938668340444565\n",
      "Epoch 18740/30000 Validation Loss: 0.07886971533298492\n",
      "Epoch 18741/30000 Training Loss: 0.06288459151983261\n",
      "Epoch 18742/30000 Training Loss: 0.091490238904953\n",
      "Epoch 18743/30000 Training Loss: 0.07009219378232956\n",
      "Epoch 18744/30000 Training Loss: 0.09269535541534424\n",
      "Epoch 18745/30000 Training Loss: 0.07411843538284302\n",
      "Epoch 18746/30000 Training Loss: 0.05913644656538963\n",
      "Epoch 18747/30000 Training Loss: 0.08375168591737747\n",
      "Epoch 18748/30000 Training Loss: 0.06158198416233063\n",
      "Epoch 18749/30000 Training Loss: 0.07047468423843384\n",
      "Epoch 18750/30000 Training Loss: 0.07282599061727524\n",
      "Epoch 18750/30000 Validation Loss: 0.057253483682870865\n",
      "Epoch 18751/30000 Training Loss: 0.052494119852781296\n",
      "Epoch 18752/30000 Training Loss: 0.0801837369799614\n",
      "Epoch 18753/30000 Training Loss: 0.0631907507777214\n",
      "Epoch 18754/30000 Training Loss: 0.10058152675628662\n",
      "Epoch 18755/30000 Training Loss: 0.06246049329638481\n",
      "Epoch 18756/30000 Training Loss: 0.0700593963265419\n",
      "Epoch 18757/30000 Training Loss: 0.07847650349140167\n",
      "Epoch 18758/30000 Training Loss: 0.07905883342027664\n",
      "Epoch 18759/30000 Training Loss: 0.07170367240905762\n",
      "Epoch 18760/30000 Training Loss: 0.06696247309446335\n",
      "Epoch 18760/30000 Validation Loss: 0.06844687461853027\n",
      "Epoch 18761/30000 Training Loss: 0.07056637853384018\n",
      "Epoch 18762/30000 Training Loss: 0.07718726247549057\n",
      "Epoch 18763/30000 Training Loss: 0.08008525520563126\n",
      "Epoch 18764/30000 Training Loss: 0.06294954568147659\n",
      "Epoch 18765/30000 Training Loss: 0.0686413124203682\n",
      "Epoch 18766/30000 Training Loss: 0.08607461303472519\n",
      "Epoch 18767/30000 Training Loss: 0.08113616704940796\n",
      "Epoch 18768/30000 Training Loss: 0.08336341381072998\n",
      "Epoch 18769/30000 Training Loss: 0.0631927028298378\n",
      "Epoch 18770/30000 Training Loss: 0.09386879205703735\n",
      "Epoch 18770/30000 Validation Loss: 0.06315727531909943\n",
      "Epoch 18771/30000 Training Loss: 0.0650908350944519\n",
      "Epoch 18772/30000 Training Loss: 0.06352308392524719\n",
      "Epoch 18773/30000 Training Loss: 0.06562682241201401\n",
      "Epoch 18774/30000 Training Loss: 0.0697532519698143\n",
      "Epoch 18775/30000 Training Loss: 0.0795905813574791\n",
      "Epoch 18776/30000 Training Loss: 0.0777742937207222\n",
      "Epoch 18777/30000 Training Loss: 0.07954561710357666\n",
      "Epoch 18778/30000 Training Loss: 0.06305065751075745\n",
      "Epoch 18779/30000 Training Loss: 0.05329697206616402\n",
      "Epoch 18780/30000 Training Loss: 0.08877617120742798\n",
      "Epoch 18780/30000 Validation Loss: 0.05646871030330658\n",
      "Epoch 18781/30000 Training Loss: 0.08985517174005508\n",
      "Epoch 18782/30000 Training Loss: 0.06867023557424545\n",
      "Epoch 18783/30000 Training Loss: 0.08957269042730331\n",
      "Epoch 18784/30000 Training Loss: 0.07548053562641144\n",
      "Epoch 18785/30000 Training Loss: 0.07988600432872772\n",
      "Epoch 18786/30000 Training Loss: 0.06634201109409332\n",
      "Epoch 18787/30000 Training Loss: 0.06329779326915741\n",
      "Epoch 18788/30000 Training Loss: 0.09891483932733536\n",
      "Epoch 18789/30000 Training Loss: 0.07000453770160675\n",
      "Epoch 18790/30000 Training Loss: 0.07449851185083389\n",
      "Epoch 18790/30000 Validation Loss: 0.07602716237306595\n",
      "Epoch 18791/30000 Training Loss: 0.07019976526498795\n",
      "Epoch 18792/30000 Training Loss: 0.09224063158035278\n",
      "Epoch 18793/30000 Training Loss: 0.07399255037307739\n",
      "Epoch 18794/30000 Training Loss: 0.049283627420663834\n",
      "Epoch 18795/30000 Training Loss: 0.06597480922937393\n",
      "Epoch 18796/30000 Training Loss: 0.08061022311449051\n",
      "Epoch 18797/30000 Training Loss: 0.07629436999559402\n",
      "Epoch 18798/30000 Training Loss: 0.051079314202070236\n",
      "Epoch 18799/30000 Training Loss: 0.08546449989080429\n",
      "Epoch 18800/30000 Training Loss: 0.07707531005144119\n",
      "Epoch 18800/30000 Validation Loss: 0.06878862529993057\n",
      "Epoch 18801/30000 Training Loss: 0.07000818103551865\n",
      "Epoch 18802/30000 Training Loss: 0.06872997432947159\n",
      "Epoch 18803/30000 Training Loss: 0.07280497997999191\n",
      "Epoch 18804/30000 Training Loss: 0.06082303449511528\n",
      "Epoch 18805/30000 Training Loss: 0.061941418796777725\n",
      "Epoch 18806/30000 Training Loss: 0.061066657304763794\n",
      "Epoch 18807/30000 Training Loss: 0.0774945393204689\n",
      "Epoch 18808/30000 Training Loss: 0.05036356672644615\n",
      "Epoch 18809/30000 Training Loss: 0.08009178191423416\n",
      "Epoch 18810/30000 Training Loss: 0.06082142889499664\n",
      "Epoch 18810/30000 Validation Loss: 0.08277235180139542\n",
      "Epoch 18811/30000 Training Loss: 0.08020531386137009\n",
      "Epoch 18812/30000 Training Loss: 0.07837525755167007\n",
      "Epoch 18813/30000 Training Loss: 0.07609832286834717\n",
      "Epoch 18814/30000 Training Loss: 0.08259084075689316\n",
      "Epoch 18815/30000 Training Loss: 0.06466817855834961\n",
      "Epoch 18816/30000 Training Loss: 0.07692920416593552\n",
      "Epoch 18817/30000 Training Loss: 0.06778981536626816\n",
      "Epoch 18818/30000 Training Loss: 0.06772193312644958\n",
      "Epoch 18819/30000 Training Loss: 0.07210839539766312\n",
      "Epoch 18820/30000 Training Loss: 0.0802595317363739\n",
      "Epoch 18820/30000 Validation Loss: 0.07520607858896255\n",
      "Epoch 18821/30000 Training Loss: 0.07769554853439331\n",
      "Epoch 18822/30000 Training Loss: 0.05948522314429283\n",
      "Epoch 18823/30000 Training Loss: 0.06936502456665039\n",
      "Epoch 18824/30000 Training Loss: 0.07649918645620346\n",
      "Epoch 18825/30000 Training Loss: 0.0780155137181282\n",
      "Epoch 18826/30000 Training Loss: 0.09044831246137619\n",
      "Epoch 18827/30000 Training Loss: 0.052344728261232376\n",
      "Epoch 18828/30000 Training Loss: 0.06784675270318985\n",
      "Epoch 18829/30000 Training Loss: 0.08869928866624832\n",
      "Epoch 18830/30000 Training Loss: 0.06931553035974503\n",
      "Epoch 18830/30000 Validation Loss: 0.06055573746562004\n",
      "Epoch 18831/30000 Training Loss: 0.06572886556386948\n",
      "Epoch 18832/30000 Training Loss: 0.059145670384168625\n",
      "Epoch 18833/30000 Training Loss: 0.08043374866247177\n",
      "Epoch 18834/30000 Training Loss: 0.08650191873311996\n",
      "Epoch 18835/30000 Training Loss: 0.07321713119745255\n",
      "Epoch 18836/30000 Training Loss: 0.08088964223861694\n",
      "Epoch 18837/30000 Training Loss: 0.0683642253279686\n",
      "Epoch 18838/30000 Training Loss: 0.06518211215734482\n",
      "Epoch 18839/30000 Training Loss: 0.07400684803724289\n",
      "Epoch 18840/30000 Training Loss: 0.07155773043632507\n",
      "Epoch 18840/30000 Validation Loss: 0.08145160973072052\n",
      "Epoch 18841/30000 Training Loss: 0.08550480753183365\n",
      "Epoch 18842/30000 Training Loss: 0.0772952064871788\n",
      "Epoch 18843/30000 Training Loss: 0.07086246460676193\n",
      "Epoch 18844/30000 Training Loss: 0.0778534933924675\n",
      "Epoch 18845/30000 Training Loss: 0.08868061751127243\n",
      "Epoch 18846/30000 Training Loss: 0.07444784045219421\n",
      "Epoch 18847/30000 Training Loss: 0.08287572115659714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18848/30000 Training Loss: 0.08148670196533203\n",
      "Epoch 18849/30000 Training Loss: 0.08108597993850708\n",
      "Epoch 18850/30000 Training Loss: 0.06116684153676033\n",
      "Epoch 18850/30000 Validation Loss: 0.07750379294157028\n",
      "Epoch 18851/30000 Training Loss: 0.07446804642677307\n",
      "Epoch 18852/30000 Training Loss: 0.05666547641158104\n",
      "Epoch 18853/30000 Training Loss: 0.07835596799850464\n",
      "Epoch 18854/30000 Training Loss: 0.09599694609642029\n",
      "Epoch 18855/30000 Training Loss: 0.06449780613183975\n",
      "Epoch 18856/30000 Training Loss: 0.07233332842588425\n",
      "Epoch 18857/30000 Training Loss: 0.068928062915802\n",
      "Epoch 18858/30000 Training Loss: 0.07584371417760849\n",
      "Epoch 18859/30000 Training Loss: 0.0785263404250145\n",
      "Epoch 18860/30000 Training Loss: 0.07430758327245712\n",
      "Epoch 18860/30000 Validation Loss: 0.07571449875831604\n",
      "Epoch 18861/30000 Training Loss: 0.06430389732122421\n",
      "Epoch 18862/30000 Training Loss: 0.06315343827009201\n",
      "Epoch 18863/30000 Training Loss: 0.07551053911447525\n",
      "Epoch 18864/30000 Training Loss: 0.10297385603189468\n",
      "Epoch 18865/30000 Training Loss: 0.08578214049339294\n",
      "Epoch 18866/30000 Training Loss: 0.08151695877313614\n",
      "Epoch 18867/30000 Training Loss: 0.08303423970937729\n",
      "Epoch 18868/30000 Training Loss: 0.05327298864722252\n",
      "Epoch 18869/30000 Training Loss: 0.0750444233417511\n",
      "Epoch 18870/30000 Training Loss: 0.08121564984321594\n",
      "Epoch 18870/30000 Validation Loss: 0.06232615187764168\n",
      "Epoch 18871/30000 Training Loss: 0.06988944858312607\n",
      "Epoch 18872/30000 Training Loss: 0.09358367323875427\n",
      "Epoch 18873/30000 Training Loss: 0.08171547204256058\n",
      "Epoch 18874/30000 Training Loss: 0.06947415322065353\n",
      "Epoch 18875/30000 Training Loss: 0.08499607443809509\n",
      "Epoch 18876/30000 Training Loss: 0.06938932090997696\n",
      "Epoch 18877/30000 Training Loss: 0.06610966473817825\n",
      "Epoch 18878/30000 Training Loss: 0.07138771563768387\n",
      "Epoch 18879/30000 Training Loss: 0.06756642460823059\n",
      "Epoch 18880/30000 Training Loss: 0.08377375453710556\n",
      "Epoch 18880/30000 Validation Loss: 0.060308441519737244\n",
      "Epoch 18881/30000 Training Loss: 0.07138162851333618\n",
      "Epoch 18882/30000 Training Loss: 0.061930686235427856\n",
      "Epoch 18883/30000 Training Loss: 0.07065143436193466\n",
      "Epoch 18884/30000 Training Loss: 0.04890860244631767\n",
      "Epoch 18885/30000 Training Loss: 0.06422469764947891\n",
      "Epoch 18886/30000 Training Loss: 0.08213835954666138\n",
      "Epoch 18887/30000 Training Loss: 0.08173000067472458\n",
      "Epoch 18888/30000 Training Loss: 0.07010849565267563\n",
      "Epoch 18889/30000 Training Loss: 0.07487569004297256\n",
      "Epoch 18890/30000 Training Loss: 0.06007438898086548\n",
      "Epoch 18890/30000 Validation Loss: 0.0631568506360054\n",
      "Epoch 18891/30000 Training Loss: 0.07577510923147202\n",
      "Epoch 18892/30000 Training Loss: 0.06984414905309677\n",
      "Epoch 18893/30000 Training Loss: 0.07064739614725113\n",
      "Epoch 18894/30000 Training Loss: 0.06807544082403183\n",
      "Epoch 18895/30000 Training Loss: 0.07327451556921005\n",
      "Epoch 18896/30000 Training Loss: 0.05527673289179802\n",
      "Epoch 18897/30000 Training Loss: 0.06504034250974655\n",
      "Epoch 18898/30000 Training Loss: 0.08033832907676697\n",
      "Epoch 18899/30000 Training Loss: 0.07451841980218887\n",
      "Epoch 18900/30000 Training Loss: 0.08430403470993042\n",
      "Epoch 18900/30000 Validation Loss: 0.0742148756980896\n",
      "Epoch 18901/30000 Training Loss: 0.06292179971933365\n",
      "Epoch 18902/30000 Training Loss: 0.0714031383395195\n",
      "Epoch 18903/30000 Training Loss: 0.085589699447155\n",
      "Epoch 18904/30000 Training Loss: 0.07034000754356384\n",
      "Epoch 18905/30000 Training Loss: 0.07248900085687637\n",
      "Epoch 18906/30000 Training Loss: 0.07372123748064041\n",
      "Epoch 18907/30000 Training Loss: 0.09168243408203125\n",
      "Epoch 18908/30000 Training Loss: 0.06480512022972107\n",
      "Epoch 18909/30000 Training Loss: 0.07429943233728409\n",
      "Epoch 18910/30000 Training Loss: 0.07395291328430176\n",
      "Epoch 18910/30000 Validation Loss: 0.0793958231806755\n",
      "Epoch 18911/30000 Training Loss: 0.06743385642766953\n",
      "Epoch 18912/30000 Training Loss: 0.06729591637849808\n",
      "Epoch 18913/30000 Training Loss: 0.07056796550750732\n",
      "Epoch 18914/30000 Training Loss: 0.08163245767354965\n",
      "Epoch 18915/30000 Training Loss: 0.05764728784561157\n",
      "Epoch 18916/30000 Training Loss: 0.06847697496414185\n",
      "Epoch 18917/30000 Training Loss: 0.0903363823890686\n",
      "Epoch 18918/30000 Training Loss: 0.08375966548919678\n",
      "Epoch 18919/30000 Training Loss: 0.08544224500656128\n",
      "Epoch 18920/30000 Training Loss: 0.06439882516860962\n",
      "Epoch 18920/30000 Validation Loss: 0.08750859647989273\n",
      "Epoch 18921/30000 Training Loss: 0.09256704896688461\n",
      "Epoch 18922/30000 Training Loss: 0.06657522171735764\n",
      "Epoch 18923/30000 Training Loss: 0.08265545219182968\n",
      "Epoch 18924/30000 Training Loss: 0.07237600535154343\n",
      "Epoch 18925/30000 Training Loss: 0.054693300276994705\n",
      "Epoch 18926/30000 Training Loss: 0.08876202255487442\n",
      "Epoch 18927/30000 Training Loss: 0.06132792308926582\n",
      "Epoch 18928/30000 Training Loss: 0.06728028506040573\n",
      "Epoch 18929/30000 Training Loss: 0.05987812578678131\n",
      "Epoch 18930/30000 Training Loss: 0.07242877036333084\n",
      "Epoch 18930/30000 Validation Loss: 0.08156529068946838\n",
      "Epoch 18931/30000 Training Loss: 0.07013093680143356\n",
      "Epoch 18932/30000 Training Loss: 0.0679478570818901\n",
      "Epoch 18933/30000 Training Loss: 0.05787151679396629\n",
      "Epoch 18934/30000 Training Loss: 0.06745438277721405\n",
      "Epoch 18935/30000 Training Loss: 0.08374317735433578\n",
      "Epoch 18936/30000 Training Loss: 0.06680618226528168\n",
      "Epoch 18937/30000 Training Loss: 0.06459642946720123\n",
      "Epoch 18938/30000 Training Loss: 0.07449208199977875\n",
      "Epoch 18939/30000 Training Loss: 0.10612881183624268\n",
      "Epoch 18940/30000 Training Loss: 0.08443120867013931\n",
      "Epoch 18940/30000 Validation Loss: 0.06946275383234024\n",
      "Epoch 18941/30000 Training Loss: 0.0754164606332779\n",
      "Epoch 18942/30000 Training Loss: 0.07129326462745667\n",
      "Epoch 18943/30000 Training Loss: 0.07222012430429459\n",
      "Epoch 18944/30000 Training Loss: 0.07051023095846176\n",
      "Epoch 18945/30000 Training Loss: 0.07011733204126358\n",
      "Epoch 18946/30000 Training Loss: 0.06797593086957932\n",
      "Epoch 18947/30000 Training Loss: 0.09609431773424149\n",
      "Epoch 18948/30000 Training Loss: 0.057320818305015564\n",
      "Epoch 18949/30000 Training Loss: 0.05189155414700508\n",
      "Epoch 18950/30000 Training Loss: 0.05692950263619423\n",
      "Epoch 18950/30000 Validation Loss: 0.07978913933038712\n",
      "Epoch 18951/30000 Training Loss: 0.057011932134628296\n",
      "Epoch 18952/30000 Training Loss: 0.07146470248699188\n",
      "Epoch 18953/30000 Training Loss: 0.0802774727344513\n",
      "Epoch 18954/30000 Training Loss: 0.06665319949388504\n",
      "Epoch 18955/30000 Training Loss: 0.07770199328660965\n",
      "Epoch 18956/30000 Training Loss: 0.06481391936540604\n",
      "Epoch 18957/30000 Training Loss: 0.06089616194367409\n",
      "Epoch 18958/30000 Training Loss: 0.0654856339097023\n",
      "Epoch 18959/30000 Training Loss: 0.07690498232841492\n",
      "Epoch 18960/30000 Training Loss: 0.06110836938023567\n",
      "Epoch 18960/30000 Validation Loss: 0.08757307380437851\n",
      "Epoch 18961/30000 Training Loss: 0.06292393058538437\n",
      "Epoch 18962/30000 Training Loss: 0.05212174728512764\n",
      "Epoch 18963/30000 Training Loss: 0.06626591086387634\n",
      "Epoch 18964/30000 Training Loss: 0.07702689617872238\n",
      "Epoch 18965/30000 Training Loss: 0.06902550905942917\n",
      "Epoch 18966/30000 Training Loss: 0.08782392740249634\n",
      "Epoch 18967/30000 Training Loss: 0.07572924345731735\n",
      "Epoch 18968/30000 Training Loss: 0.07347803562879562\n",
      "Epoch 18969/30000 Training Loss: 0.07524514943361282\n",
      "Epoch 18970/30000 Training Loss: 0.06050266698002815\n",
      "Epoch 18970/30000 Validation Loss: 0.06848186254501343\n",
      "Epoch 18971/30000 Training Loss: 0.08348499983549118\n",
      "Epoch 18972/30000 Training Loss: 0.06339339166879654\n",
      "Epoch 18973/30000 Training Loss: 0.06927043944597244\n",
      "Epoch 18974/30000 Training Loss: 0.07251330465078354\n",
      "Epoch 18975/30000 Training Loss: 0.05875738337635994\n",
      "Epoch 18976/30000 Training Loss: 0.08580848574638367\n",
      "Epoch 18977/30000 Training Loss: 0.08295149356126785\n",
      "Epoch 18978/30000 Training Loss: 0.06801850348711014\n",
      "Epoch 18979/30000 Training Loss: 0.0697602704167366\n",
      "Epoch 18980/30000 Training Loss: 0.07810989767313004\n",
      "Epoch 18980/30000 Validation Loss: 0.08616926521062851\n",
      "Epoch 18981/30000 Training Loss: 0.06628043204545975\n",
      "Epoch 18982/30000 Training Loss: 0.05106329917907715\n",
      "Epoch 18983/30000 Training Loss: 0.07227642089128494\n",
      "Epoch 18984/30000 Training Loss: 0.08025997877120972\n",
      "Epoch 18985/30000 Training Loss: 0.070836640894413\n",
      "Epoch 18986/30000 Training Loss: 0.08230679482221603\n",
      "Epoch 18987/30000 Training Loss: 0.06441795080900192\n",
      "Epoch 18988/30000 Training Loss: 0.07045036554336548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18989/30000 Training Loss: 0.06899327784776688\n",
      "Epoch 18990/30000 Training Loss: 0.0650973916053772\n",
      "Epoch 18990/30000 Validation Loss: 0.08176809549331665\n",
      "Epoch 18991/30000 Training Loss: 0.060488492250442505\n",
      "Epoch 18992/30000 Training Loss: 0.06880783289670944\n",
      "Epoch 18993/30000 Training Loss: 0.06972196698188782\n",
      "Epoch 18994/30000 Training Loss: 0.06224970892071724\n",
      "Epoch 18995/30000 Training Loss: 0.07231155037879944\n",
      "Epoch 18996/30000 Training Loss: 0.07679744064807892\n",
      "Epoch 18997/30000 Training Loss: 0.07192067056894302\n",
      "Epoch 18998/30000 Training Loss: 0.08564326912164688\n",
      "Epoch 18999/30000 Training Loss: 0.06910060346126556\n",
      "Epoch 19000/30000 Training Loss: 0.05855405330657959\n",
      "Epoch 19000/30000 Validation Loss: 0.07550900429487228\n",
      "Epoch 19001/30000 Training Loss: 0.08100605010986328\n",
      "Epoch 19002/30000 Training Loss: 0.0674123540520668\n",
      "Epoch 19003/30000 Training Loss: 0.07518622279167175\n",
      "Epoch 19004/30000 Training Loss: 0.0742318406701088\n",
      "Epoch 19005/30000 Training Loss: 0.06002376601099968\n",
      "Epoch 19006/30000 Training Loss: 0.06975150853395462\n",
      "Epoch 19007/30000 Training Loss: 0.05602055788040161\n",
      "Epoch 19008/30000 Training Loss: 0.060553550720214844\n",
      "Epoch 19009/30000 Training Loss: 0.09377031773328781\n",
      "Epoch 19010/30000 Training Loss: 0.06013089045882225\n",
      "Epoch 19010/30000 Validation Loss: 0.0980868935585022\n",
      "Epoch 19011/30000 Training Loss: 0.06032551825046539\n",
      "Epoch 19012/30000 Training Loss: 0.06180913373827934\n",
      "Epoch 19013/30000 Training Loss: 0.07042577862739563\n",
      "Epoch 19014/30000 Training Loss: 0.07236186414957047\n",
      "Epoch 19015/30000 Training Loss: 0.0908280536532402\n",
      "Epoch 19016/30000 Training Loss: 0.09697829931974411\n",
      "Epoch 19017/30000 Training Loss: 0.06668947637081146\n",
      "Epoch 19018/30000 Training Loss: 0.06582915782928467\n",
      "Epoch 19019/30000 Training Loss: 0.06005984544754028\n",
      "Epoch 19020/30000 Training Loss: 0.06947215646505356\n",
      "Epoch 19020/30000 Validation Loss: 0.08065611124038696\n",
      "Epoch 19021/30000 Training Loss: 0.059822145849466324\n",
      "Epoch 19022/30000 Training Loss: 0.05967125669121742\n",
      "Epoch 19023/30000 Training Loss: 0.06814844906330109\n",
      "Epoch 19024/30000 Training Loss: 0.08565684407949448\n",
      "Epoch 19025/30000 Training Loss: 0.06931103020906448\n",
      "Epoch 19026/30000 Training Loss: 0.06756114214658737\n",
      "Epoch 19027/30000 Training Loss: 0.0675630047917366\n",
      "Epoch 19028/30000 Training Loss: 0.07951343804597855\n",
      "Epoch 19029/30000 Training Loss: 0.06521513313055038\n",
      "Epoch 19030/30000 Training Loss: 0.06433925032615662\n",
      "Epoch 19030/30000 Validation Loss: 0.08478167653083801\n",
      "Epoch 19031/30000 Training Loss: 0.06251095235347748\n",
      "Epoch 19032/30000 Training Loss: 0.08897721767425537\n",
      "Epoch 19033/30000 Training Loss: 0.05932043865323067\n",
      "Epoch 19034/30000 Training Loss: 0.0611867718398571\n",
      "Epoch 19035/30000 Training Loss: 0.05458001419901848\n",
      "Epoch 19036/30000 Training Loss: 0.067376509308815\n",
      "Epoch 19037/30000 Training Loss: 0.07503102719783783\n",
      "Epoch 19038/30000 Training Loss: 0.06784652918577194\n",
      "Epoch 19039/30000 Training Loss: 0.07403790950775146\n",
      "Epoch 19040/30000 Training Loss: 0.0756983533501625\n",
      "Epoch 19040/30000 Validation Loss: 0.07871206849813461\n",
      "Epoch 19041/30000 Training Loss: 0.0677112266421318\n",
      "Epoch 19042/30000 Training Loss: 0.07589428871870041\n",
      "Epoch 19043/30000 Training Loss: 0.07214925438165665\n",
      "Epoch 19044/30000 Training Loss: 0.05965769663453102\n",
      "Epoch 19045/30000 Training Loss: 0.06497332453727722\n",
      "Epoch 19046/30000 Training Loss: 0.08874797821044922\n",
      "Epoch 19047/30000 Training Loss: 0.07793848216533661\n",
      "Epoch 19048/30000 Training Loss: 0.06223045662045479\n",
      "Epoch 19049/30000 Training Loss: 0.06957301497459412\n",
      "Epoch 19050/30000 Training Loss: 0.0607413686811924\n",
      "Epoch 19050/30000 Validation Loss: 0.08780946582555771\n",
      "Epoch 19051/30000 Training Loss: 0.07549118995666504\n",
      "Epoch 19052/30000 Training Loss: 0.07381429523229599\n",
      "Epoch 19053/30000 Training Loss: 0.08485045284032822\n",
      "Epoch 19054/30000 Training Loss: 0.0586850680410862\n",
      "Epoch 19055/30000 Training Loss: 0.058984801173210144\n",
      "Epoch 19056/30000 Training Loss: 0.06995224952697754\n",
      "Epoch 19057/30000 Training Loss: 0.06564998626708984\n",
      "Epoch 19058/30000 Training Loss: 0.07585901767015457\n",
      "Epoch 19059/30000 Training Loss: 0.08058429509401321\n",
      "Epoch 19060/30000 Training Loss: 0.08285314589738846\n",
      "Epoch 19060/30000 Validation Loss: 0.08177752047777176\n",
      "Epoch 19061/30000 Training Loss: 0.06986608356237411\n",
      "Epoch 19062/30000 Training Loss: 0.08957451581954956\n",
      "Epoch 19063/30000 Training Loss: 0.07512670755386353\n",
      "Epoch 19064/30000 Training Loss: 0.07436657696962357\n",
      "Epoch 19065/30000 Training Loss: 0.07528983801603317\n",
      "Epoch 19066/30000 Training Loss: 0.09048941731452942\n",
      "Epoch 19067/30000 Training Loss: 0.060066092759370804\n",
      "Epoch 19068/30000 Training Loss: 0.07237955182790756\n",
      "Epoch 19069/30000 Training Loss: 0.08359893411397934\n",
      "Epoch 19070/30000 Training Loss: 0.0769791230559349\n",
      "Epoch 19070/30000 Validation Loss: 0.06282660365104675\n",
      "Epoch 19071/30000 Training Loss: 0.06422625482082367\n",
      "Epoch 19072/30000 Training Loss: 0.07546031475067139\n",
      "Epoch 19073/30000 Training Loss: 0.08661922067403793\n",
      "Epoch 19074/30000 Training Loss: 0.06450355798006058\n",
      "Epoch 19075/30000 Training Loss: 0.07981345802545547\n",
      "Epoch 19076/30000 Training Loss: 0.06599859148263931\n",
      "Epoch 19077/30000 Training Loss: 0.08398792892694473\n",
      "Epoch 19078/30000 Training Loss: 0.07180533558130264\n",
      "Epoch 19079/30000 Training Loss: 0.07524295896291733\n",
      "Epoch 19080/30000 Training Loss: 0.06936579197645187\n",
      "Epoch 19080/30000 Validation Loss: 0.0777808353304863\n",
      "Epoch 19081/30000 Training Loss: 0.07296024262905121\n",
      "Epoch 19082/30000 Training Loss: 0.08889319747686386\n",
      "Epoch 19083/30000 Training Loss: 0.07253222912549973\n",
      "Epoch 19084/30000 Training Loss: 0.08696303516626358\n",
      "Epoch 19085/30000 Training Loss: 0.06835756450891495\n",
      "Epoch 19086/30000 Training Loss: 0.05833679810166359\n",
      "Epoch 19087/30000 Training Loss: 0.0718986988067627\n",
      "Epoch 19088/30000 Training Loss: 0.07168906927108765\n",
      "Epoch 19089/30000 Training Loss: 0.06885270774364471\n",
      "Epoch 19090/30000 Training Loss: 0.07661204040050507\n",
      "Epoch 19090/30000 Validation Loss: 0.06408587843179703\n",
      "Epoch 19091/30000 Training Loss: 0.056143153458833694\n",
      "Epoch 19092/30000 Training Loss: 0.06932631880044937\n",
      "Epoch 19093/30000 Training Loss: 0.06496936082839966\n",
      "Epoch 19094/30000 Training Loss: 0.05692470446228981\n",
      "Epoch 19095/30000 Training Loss: 0.07433750480413437\n",
      "Epoch 19096/30000 Training Loss: 0.06997022032737732\n",
      "Epoch 19097/30000 Training Loss: 0.07659619301557541\n",
      "Epoch 19098/30000 Training Loss: 0.07926373928785324\n",
      "Epoch 19099/30000 Training Loss: 0.08393136411905289\n",
      "Epoch 19100/30000 Training Loss: 0.07446630299091339\n",
      "Epoch 19100/30000 Validation Loss: 0.07812448590993881\n",
      "Epoch 19101/30000 Training Loss: 0.07532382011413574\n",
      "Epoch 19102/30000 Training Loss: 0.06784519553184509\n",
      "Epoch 19103/30000 Training Loss: 0.08211406320333481\n",
      "Epoch 19104/30000 Training Loss: 0.06293792277574539\n",
      "Epoch 19105/30000 Training Loss: 0.07366415858268738\n",
      "Epoch 19106/30000 Training Loss: 0.0683044046163559\n",
      "Epoch 19107/30000 Training Loss: 0.059905022382736206\n",
      "Epoch 19108/30000 Training Loss: 0.05305905640125275\n",
      "Epoch 19109/30000 Training Loss: 0.07091520726680756\n",
      "Epoch 19110/30000 Training Loss: 0.07739147543907166\n",
      "Epoch 19110/30000 Validation Loss: 0.0699298307299614\n",
      "Epoch 19111/30000 Training Loss: 0.061730366200208664\n",
      "Epoch 19112/30000 Training Loss: 0.07383526861667633\n",
      "Epoch 19113/30000 Training Loss: 0.07709444314241409\n",
      "Epoch 19114/30000 Training Loss: 0.06393925100564957\n",
      "Epoch 19115/30000 Training Loss: 0.08591166138648987\n",
      "Epoch 19116/30000 Training Loss: 0.08196926862001419\n",
      "Epoch 19117/30000 Training Loss: 0.0763232633471489\n",
      "Epoch 19118/30000 Training Loss: 0.08506274223327637\n",
      "Epoch 19119/30000 Training Loss: 0.08208343386650085\n",
      "Epoch 19120/30000 Training Loss: 0.07475586980581284\n",
      "Epoch 19120/30000 Validation Loss: 0.08666249364614487\n",
      "Epoch 19121/30000 Training Loss: 0.05946454405784607\n",
      "Epoch 19122/30000 Training Loss: 0.05689350888133049\n",
      "Epoch 19123/30000 Training Loss: 0.05723143741488457\n",
      "Epoch 19124/30000 Training Loss: 0.06543149054050446\n",
      "Epoch 19125/30000 Training Loss: 0.07367639988660812\n",
      "Epoch 19126/30000 Training Loss: 0.0817696824669838\n",
      "Epoch 19127/30000 Training Loss: 0.06861194223165512\n",
      "Epoch 19128/30000 Training Loss: 0.08411666750907898\n",
      "Epoch 19129/30000 Training Loss: 0.06865482032299042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19130/30000 Training Loss: 0.07156786322593689\n",
      "Epoch 19130/30000 Validation Loss: 0.0714077576994896\n",
      "Epoch 19131/30000 Training Loss: 0.09066031128168106\n",
      "Epoch 19132/30000 Training Loss: 0.07044228911399841\n",
      "Epoch 19133/30000 Training Loss: 0.06962178647518158\n",
      "Epoch 19134/30000 Training Loss: 0.066701240837574\n",
      "Epoch 19135/30000 Training Loss: 0.07465572655200958\n",
      "Epoch 19136/30000 Training Loss: 0.09340959042310715\n",
      "Epoch 19137/30000 Training Loss: 0.08046406507492065\n",
      "Epoch 19138/30000 Training Loss: 0.07968070358037949\n",
      "Epoch 19139/30000 Training Loss: 0.08945105224847794\n",
      "Epoch 19140/30000 Training Loss: 0.07262889295816422\n",
      "Epoch 19140/30000 Validation Loss: 0.06822460889816284\n",
      "Epoch 19141/30000 Training Loss: 0.07065684348344803\n",
      "Epoch 19142/30000 Training Loss: 0.07415565103292465\n",
      "Epoch 19143/30000 Training Loss: 0.05641255900263786\n",
      "Epoch 19144/30000 Training Loss: 0.052838776260614395\n",
      "Epoch 19145/30000 Training Loss: 0.08605065941810608\n",
      "Epoch 19146/30000 Training Loss: 0.074102021753788\n",
      "Epoch 19147/30000 Training Loss: 0.05962652340531349\n",
      "Epoch 19148/30000 Training Loss: 0.05507626011967659\n",
      "Epoch 19149/30000 Training Loss: 0.06054109334945679\n",
      "Epoch 19150/30000 Training Loss: 0.08561301231384277\n",
      "Epoch 19150/30000 Validation Loss: 0.07320553809404373\n",
      "Epoch 19151/30000 Training Loss: 0.05629816651344299\n",
      "Epoch 19152/30000 Training Loss: 0.06564035266637802\n",
      "Epoch 19153/30000 Training Loss: 0.07451526820659637\n",
      "Epoch 19154/30000 Training Loss: 0.07655017822980881\n",
      "Epoch 19155/30000 Training Loss: 0.06661367416381836\n",
      "Epoch 19156/30000 Training Loss: 0.06920206546783447\n",
      "Epoch 19157/30000 Training Loss: 0.06398496776819229\n",
      "Epoch 19158/30000 Training Loss: 0.07697007805109024\n",
      "Epoch 19159/30000 Training Loss: 0.06504953652620316\n",
      "Epoch 19160/30000 Training Loss: 0.06687844544649124\n",
      "Epoch 19160/30000 Validation Loss: 0.0692569836974144\n",
      "Epoch 19161/30000 Training Loss: 0.08155738562345505\n",
      "Epoch 19162/30000 Training Loss: 0.0798306092619896\n",
      "Epoch 19163/30000 Training Loss: 0.0722956731915474\n",
      "Epoch 19164/30000 Training Loss: 0.06546657532453537\n",
      "Epoch 19165/30000 Training Loss: 0.07300455123186111\n",
      "Epoch 19166/30000 Training Loss: 0.07416119426488876\n",
      "Epoch 19167/30000 Training Loss: 0.07944763451814651\n",
      "Epoch 19168/30000 Training Loss: 0.07222378998994827\n",
      "Epoch 19169/30000 Training Loss: 0.06465602666139603\n",
      "Epoch 19170/30000 Training Loss: 0.05735301971435547\n",
      "Epoch 19170/30000 Validation Loss: 0.06589271873235703\n",
      "Epoch 19171/30000 Training Loss: 0.05987195298075676\n",
      "Epoch 19172/30000 Training Loss: 0.06191680207848549\n",
      "Epoch 19173/30000 Training Loss: 0.06769298762083054\n",
      "Epoch 19174/30000 Training Loss: 0.0625569224357605\n",
      "Epoch 19175/30000 Training Loss: 0.06371551007032394\n",
      "Epoch 19176/30000 Training Loss: 0.07686427980661392\n",
      "Epoch 19177/30000 Training Loss: 0.07046613842248917\n",
      "Epoch 19178/30000 Training Loss: 0.065070241689682\n",
      "Epoch 19179/30000 Training Loss: 0.06712989509105682\n",
      "Epoch 19180/30000 Training Loss: 0.07364349812269211\n",
      "Epoch 19180/30000 Validation Loss: 0.06333804875612259\n",
      "Epoch 19181/30000 Training Loss: 0.04778996482491493\n",
      "Epoch 19182/30000 Training Loss: 0.07513897866010666\n",
      "Epoch 19183/30000 Training Loss: 0.06895647943019867\n",
      "Epoch 19184/30000 Training Loss: 0.07389046996831894\n",
      "Epoch 19185/30000 Training Loss: 0.06581927090883255\n",
      "Epoch 19186/30000 Training Loss: 0.05668768659234047\n",
      "Epoch 19187/30000 Training Loss: 0.07510051131248474\n",
      "Epoch 19188/30000 Training Loss: 0.06913963705301285\n",
      "Epoch 19189/30000 Training Loss: 0.07257480174303055\n",
      "Epoch 19190/30000 Training Loss: 0.06469274312257767\n",
      "Epoch 19190/30000 Validation Loss: 0.08322570472955704\n",
      "Epoch 19191/30000 Training Loss: 0.05775520205497742\n",
      "Epoch 19192/30000 Training Loss: 0.07229653000831604\n",
      "Epoch 19193/30000 Training Loss: 0.0749645009636879\n",
      "Epoch 19194/30000 Training Loss: 0.06873390823602676\n",
      "Epoch 19195/30000 Training Loss: 0.07201036810874939\n",
      "Epoch 19196/30000 Training Loss: 0.07774325460195541\n",
      "Epoch 19197/30000 Training Loss: 0.05964803323149681\n",
      "Epoch 19198/30000 Training Loss: 0.08748719841241837\n",
      "Epoch 19199/30000 Training Loss: 0.07123266160488129\n",
      "Epoch 19200/30000 Training Loss: 0.08462349325418472\n",
      "Epoch 19200/30000 Validation Loss: 0.09315190464258194\n",
      "Epoch 19201/30000 Training Loss: 0.08059922605752945\n",
      "Epoch 19202/30000 Training Loss: 0.0708220824599266\n",
      "Epoch 19203/30000 Training Loss: 0.09619929641485214\n",
      "Epoch 19204/30000 Training Loss: 0.08348793536424637\n",
      "Epoch 19205/30000 Training Loss: 0.0726088210940361\n",
      "Epoch 19206/30000 Training Loss: 0.06066054105758667\n",
      "Epoch 19207/30000 Training Loss: 0.05426650121808052\n",
      "Epoch 19208/30000 Training Loss: 0.05329461023211479\n",
      "Epoch 19209/30000 Training Loss: 0.07828697562217712\n",
      "Epoch 19210/30000 Training Loss: 0.06077338382601738\n",
      "Epoch 19210/30000 Validation Loss: 0.07122449576854706\n",
      "Epoch 19211/30000 Training Loss: 0.06510733813047409\n",
      "Epoch 19212/30000 Training Loss: 0.07233256101608276\n",
      "Epoch 19213/30000 Training Loss: 0.07660900801420212\n",
      "Epoch 19214/30000 Training Loss: 0.06290619820356369\n",
      "Epoch 19215/30000 Training Loss: 0.0660136342048645\n",
      "Epoch 19216/30000 Training Loss: 0.08297095447778702\n",
      "Epoch 19217/30000 Training Loss: 0.057865917682647705\n",
      "Epoch 19218/30000 Training Loss: 0.09190420061349869\n",
      "Epoch 19219/30000 Training Loss: 0.06259521096944809\n",
      "Epoch 19220/30000 Training Loss: 0.06436878442764282\n",
      "Epoch 19220/30000 Validation Loss: 0.06976326555013657\n",
      "Epoch 19221/30000 Training Loss: 0.06887326389551163\n",
      "Epoch 19222/30000 Training Loss: 0.06769213825464249\n",
      "Epoch 19223/30000 Training Loss: 0.07597658038139343\n",
      "Epoch 19224/30000 Training Loss: 0.06512393802404404\n",
      "Epoch 19225/30000 Training Loss: 0.06719706952571869\n",
      "Epoch 19226/30000 Training Loss: 0.08342147618532181\n",
      "Epoch 19227/30000 Training Loss: 0.07408856600522995\n",
      "Epoch 19228/30000 Training Loss: 0.05661611631512642\n",
      "Epoch 19229/30000 Training Loss: 0.06421488523483276\n",
      "Epoch 19230/30000 Training Loss: 0.058145418763160706\n",
      "Epoch 19230/30000 Validation Loss: 0.07879182696342468\n",
      "Epoch 19231/30000 Training Loss: 0.05877073481678963\n",
      "Epoch 19232/30000 Training Loss: 0.08104830980300903\n",
      "Epoch 19233/30000 Training Loss: 0.07729289680719376\n",
      "Epoch 19234/30000 Training Loss: 0.07126442342996597\n",
      "Epoch 19235/30000 Training Loss: 0.0830802395939827\n",
      "Epoch 19236/30000 Training Loss: 0.07753457129001617\n",
      "Epoch 19237/30000 Training Loss: 0.0723700299859047\n",
      "Epoch 19238/30000 Training Loss: 0.09217851608991623\n",
      "Epoch 19239/30000 Training Loss: 0.0836792066693306\n",
      "Epoch 19240/30000 Training Loss: 0.0823327824473381\n",
      "Epoch 19240/30000 Validation Loss: 0.06585142016410828\n",
      "Epoch 19241/30000 Training Loss: 0.06778166443109512\n",
      "Epoch 19242/30000 Training Loss: 0.0777522400021553\n",
      "Epoch 19243/30000 Training Loss: 0.07914478331804276\n",
      "Epoch 19244/30000 Training Loss: 0.0647859200835228\n",
      "Epoch 19245/30000 Training Loss: 0.05720030143857002\n",
      "Epoch 19246/30000 Training Loss: 0.07369065284729004\n",
      "Epoch 19247/30000 Training Loss: 0.06462797522544861\n",
      "Epoch 19248/30000 Training Loss: 0.05882455036044121\n",
      "Epoch 19249/30000 Training Loss: 0.07606179267168045\n",
      "Epoch 19250/30000 Training Loss: 0.059071484953165054\n",
      "Epoch 19250/30000 Validation Loss: 0.08054671436548233\n",
      "Epoch 19251/30000 Training Loss: 0.08491136878728867\n",
      "Epoch 19252/30000 Training Loss: 0.0632128119468689\n",
      "Epoch 19253/30000 Training Loss: 0.05699281021952629\n",
      "Epoch 19254/30000 Training Loss: 0.05507044121623039\n",
      "Epoch 19255/30000 Training Loss: 0.08468297868967056\n",
      "Epoch 19256/30000 Training Loss: 0.06936783343553543\n",
      "Epoch 19257/30000 Training Loss: 0.07081488519906998\n",
      "Epoch 19258/30000 Training Loss: 0.08625418692827225\n",
      "Epoch 19259/30000 Training Loss: 0.059167783707380295\n",
      "Epoch 19260/30000 Training Loss: 0.07181958109140396\n",
      "Epoch 19260/30000 Validation Loss: 0.07899916917085648\n",
      "Epoch 19261/30000 Training Loss: 0.06700992584228516\n",
      "Epoch 19262/30000 Training Loss: 0.07372447103261948\n",
      "Epoch 19263/30000 Training Loss: 0.06866765767335892\n",
      "Epoch 19264/30000 Training Loss: 0.06598249822854996\n",
      "Epoch 19265/30000 Training Loss: 0.053992610424757004\n",
      "Epoch 19266/30000 Training Loss: 0.09671860188245773\n",
      "Epoch 19267/30000 Training Loss: 0.07246861606836319\n",
      "Epoch 19268/30000 Training Loss: 0.07284575700759888\n",
      "Epoch 19269/30000 Training Loss: 0.06816854327917099\n",
      "Epoch 19270/30000 Training Loss: 0.069355309009552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19270/30000 Validation Loss: 0.0777418240904808\n",
      "Epoch 19271/30000 Training Loss: 0.06854382902383804\n",
      "Epoch 19272/30000 Training Loss: 0.07429129630327225\n",
      "Epoch 19273/30000 Training Loss: 0.05675810948014259\n",
      "Epoch 19274/30000 Training Loss: 0.0777101144194603\n",
      "Epoch 19275/30000 Training Loss: 0.0757124051451683\n",
      "Epoch 19276/30000 Training Loss: 0.08403802663087845\n",
      "Epoch 19277/30000 Training Loss: 0.08721691370010376\n",
      "Epoch 19278/30000 Training Loss: 0.08239816874265671\n",
      "Epoch 19279/30000 Training Loss: 0.06162025034427643\n",
      "Epoch 19280/30000 Training Loss: 0.05568729341030121\n",
      "Epoch 19280/30000 Validation Loss: 0.06994054466485977\n",
      "Epoch 19281/30000 Training Loss: 0.07581454515457153\n",
      "Epoch 19282/30000 Training Loss: 0.05630701780319214\n",
      "Epoch 19283/30000 Training Loss: 0.07060036063194275\n",
      "Epoch 19284/30000 Training Loss: 0.07704649120569229\n",
      "Epoch 19285/30000 Training Loss: 0.06173970177769661\n",
      "Epoch 19286/30000 Training Loss: 0.06605196744203568\n",
      "Epoch 19287/30000 Training Loss: 0.08172491937875748\n",
      "Epoch 19288/30000 Training Loss: 0.07848595827817917\n",
      "Epoch 19289/30000 Training Loss: 0.0827113687992096\n",
      "Epoch 19290/30000 Training Loss: 0.07731754332780838\n",
      "Epoch 19290/30000 Validation Loss: 0.07769932597875595\n",
      "Epoch 19291/30000 Training Loss: 0.09272123128175735\n",
      "Epoch 19292/30000 Training Loss: 0.06612788885831833\n",
      "Epoch 19293/30000 Training Loss: 0.07749050855636597\n",
      "Epoch 19294/30000 Training Loss: 0.07874584943056107\n",
      "Epoch 19295/30000 Training Loss: 0.06613823026418686\n",
      "Epoch 19296/30000 Training Loss: 0.07231441885232925\n",
      "Epoch 19297/30000 Training Loss: 0.08263314515352249\n",
      "Epoch 19298/30000 Training Loss: 0.06576112657785416\n",
      "Epoch 19299/30000 Training Loss: 0.060440585017204285\n",
      "Epoch 19300/30000 Training Loss: 0.06568079441785812\n",
      "Epoch 19300/30000 Validation Loss: 0.08058105409145355\n",
      "Epoch 19301/30000 Training Loss: 0.06839986890554428\n",
      "Epoch 19302/30000 Training Loss: 0.06367423385381699\n",
      "Epoch 19303/30000 Training Loss: 0.08189564198255539\n",
      "Epoch 19304/30000 Training Loss: 0.07969046384096146\n",
      "Epoch 19305/30000 Training Loss: 0.08021774142980576\n",
      "Epoch 19306/30000 Training Loss: 0.06434623152017593\n",
      "Epoch 19307/30000 Training Loss: 0.06690783053636551\n",
      "Epoch 19308/30000 Training Loss: 0.06449533253908157\n",
      "Epoch 19309/30000 Training Loss: 0.0739293172955513\n",
      "Epoch 19310/30000 Training Loss: 0.0712161734700203\n",
      "Epoch 19310/30000 Validation Loss: 0.09740134328603745\n",
      "Epoch 19311/30000 Training Loss: 0.07205669581890106\n",
      "Epoch 19312/30000 Training Loss: 0.08252605050802231\n",
      "Epoch 19313/30000 Training Loss: 0.06287731975317001\n",
      "Epoch 19314/30000 Training Loss: 0.07807009667158127\n",
      "Epoch 19315/30000 Training Loss: 0.07461640983819962\n",
      "Epoch 19316/30000 Training Loss: 0.06988850980997086\n",
      "Epoch 19317/30000 Training Loss: 0.051057253032922745\n",
      "Epoch 19318/30000 Training Loss: 0.0871167778968811\n",
      "Epoch 19319/30000 Training Loss: 0.08359009772539139\n",
      "Epoch 19320/30000 Training Loss: 0.06762105226516724\n",
      "Epoch 19320/30000 Validation Loss: 0.06606551259756088\n",
      "Epoch 19321/30000 Training Loss: 0.06571687757968903\n",
      "Epoch 19322/30000 Training Loss: 0.09209958463907242\n",
      "Epoch 19323/30000 Training Loss: 0.07597240805625916\n",
      "Epoch 19324/30000 Training Loss: 0.06417632848024368\n",
      "Epoch 19325/30000 Training Loss: 0.06316876411437988\n",
      "Epoch 19326/30000 Training Loss: 0.079941026866436\n",
      "Epoch 19327/30000 Training Loss: 0.059827983379364014\n",
      "Epoch 19328/30000 Training Loss: 0.08021628111600876\n",
      "Epoch 19329/30000 Training Loss: 0.07170461863279343\n",
      "Epoch 19330/30000 Training Loss: 0.06886187195777893\n",
      "Epoch 19330/30000 Validation Loss: 0.08070475608110428\n",
      "Epoch 19331/30000 Training Loss: 0.07353805750608444\n",
      "Epoch 19332/30000 Training Loss: 0.07404414564371109\n",
      "Epoch 19333/30000 Training Loss: 0.06556140631437302\n",
      "Epoch 19334/30000 Training Loss: 0.07604324072599411\n",
      "Epoch 19335/30000 Training Loss: 0.07282888889312744\n",
      "Epoch 19336/30000 Training Loss: 0.06956683844327927\n",
      "Epoch 19337/30000 Training Loss: 0.08094483613967896\n",
      "Epoch 19338/30000 Training Loss: 0.06678416579961777\n",
      "Epoch 19339/30000 Training Loss: 0.0612456388771534\n",
      "Epoch 19340/30000 Training Loss: 0.06514167785644531\n",
      "Epoch 19340/30000 Validation Loss: 0.08524731546640396\n",
      "Epoch 19341/30000 Training Loss: 0.06513331830501556\n",
      "Epoch 19342/30000 Training Loss: 0.05541412532329559\n",
      "Epoch 19343/30000 Training Loss: 0.07170037925243378\n",
      "Epoch 19344/30000 Training Loss: 0.06773480027914047\n",
      "Epoch 19345/30000 Training Loss: 0.0726323351264\n",
      "Epoch 19346/30000 Training Loss: 0.07951470464468002\n",
      "Epoch 19347/30000 Training Loss: 0.053917210549116135\n",
      "Epoch 19348/30000 Training Loss: 0.05967654660344124\n",
      "Epoch 19349/30000 Training Loss: 0.07249968498945236\n",
      "Epoch 19350/30000 Training Loss: 0.07518567889928818\n",
      "Epoch 19350/30000 Validation Loss: 0.06417178362607956\n",
      "Epoch 19351/30000 Training Loss: 0.05853884294629097\n",
      "Epoch 19352/30000 Training Loss: 0.0638374537229538\n",
      "Epoch 19353/30000 Training Loss: 0.0665467381477356\n",
      "Epoch 19354/30000 Training Loss: 0.07348795980215073\n",
      "Epoch 19355/30000 Training Loss: 0.06263187527656555\n",
      "Epoch 19356/30000 Training Loss: 0.06470068544149399\n",
      "Epoch 19357/30000 Training Loss: 0.07318934798240662\n",
      "Epoch 19358/30000 Training Loss: 0.06006026268005371\n",
      "Epoch 19359/30000 Training Loss: 0.08473622053861618\n",
      "Epoch 19360/30000 Training Loss: 0.0837201401591301\n",
      "Epoch 19360/30000 Validation Loss: 0.06865596026182175\n",
      "Epoch 19361/30000 Training Loss: 0.0950990691781044\n",
      "Epoch 19362/30000 Training Loss: 0.06697037070989609\n",
      "Epoch 19363/30000 Training Loss: 0.07689100503921509\n",
      "Epoch 19364/30000 Training Loss: 0.06822668761014938\n",
      "Epoch 19365/30000 Training Loss: 0.08051231503486633\n",
      "Epoch 19366/30000 Training Loss: 0.059632379561662674\n",
      "Epoch 19367/30000 Training Loss: 0.0623108446598053\n",
      "Epoch 19368/30000 Training Loss: 0.0688723772764206\n",
      "Epoch 19369/30000 Training Loss: 0.061195194721221924\n",
      "Epoch 19370/30000 Training Loss: 0.06499037891626358\n",
      "Epoch 19370/30000 Validation Loss: 0.08562234789133072\n",
      "Epoch 19371/30000 Training Loss: 0.08806520700454712\n",
      "Epoch 19372/30000 Training Loss: 0.07672121375799179\n",
      "Epoch 19373/30000 Training Loss: 0.07686050981283188\n",
      "Epoch 19374/30000 Training Loss: 0.053110893815755844\n",
      "Epoch 19375/30000 Training Loss: 0.0832526907324791\n",
      "Epoch 19376/30000 Training Loss: 0.06095980107784271\n",
      "Epoch 19377/30000 Training Loss: 0.059462059289216995\n",
      "Epoch 19378/30000 Training Loss: 0.07622595131397247\n",
      "Epoch 19379/30000 Training Loss: 0.0743071660399437\n",
      "Epoch 19380/30000 Training Loss: 0.09047073125839233\n",
      "Epoch 19380/30000 Validation Loss: 0.08105532079935074\n",
      "Epoch 19381/30000 Training Loss: 0.0797455906867981\n",
      "Epoch 19382/30000 Training Loss: 0.07102402299642563\n",
      "Epoch 19383/30000 Training Loss: 0.09628361463546753\n",
      "Epoch 19384/30000 Training Loss: 0.06461872905492783\n",
      "Epoch 19385/30000 Training Loss: 0.07278246432542801\n",
      "Epoch 19386/30000 Training Loss: 0.07040009647607803\n",
      "Epoch 19387/30000 Training Loss: 0.07226914912462234\n",
      "Epoch 19388/30000 Training Loss: 0.06586644798517227\n",
      "Epoch 19389/30000 Training Loss: 0.0671408623456955\n",
      "Epoch 19390/30000 Training Loss: 0.06320267915725708\n",
      "Epoch 19390/30000 Validation Loss: 0.08041244000196457\n",
      "Epoch 19391/30000 Training Loss: 0.07995187491178513\n",
      "Epoch 19392/30000 Training Loss: 0.07274667173624039\n",
      "Epoch 19393/30000 Training Loss: 0.060169149190187454\n",
      "Epoch 19394/30000 Training Loss: 0.10792112350463867\n",
      "Epoch 19395/30000 Training Loss: 0.061173152178525925\n",
      "Epoch 19396/30000 Training Loss: 0.07578548043966293\n",
      "Epoch 19397/30000 Training Loss: 0.07459693402051926\n",
      "Epoch 19398/30000 Training Loss: 0.07027259469032288\n",
      "Epoch 19399/30000 Training Loss: 0.07556728273630142\n",
      "Epoch 19400/30000 Training Loss: 0.0752987265586853\n",
      "Epoch 19400/30000 Validation Loss: 0.07949811965227127\n",
      "Epoch 19401/30000 Training Loss: 0.08172469586133957\n",
      "Epoch 19402/30000 Training Loss: 0.07747769355773926\n",
      "Epoch 19403/30000 Training Loss: 0.08209586888551712\n",
      "Epoch 19404/30000 Training Loss: 0.06952692568302155\n",
      "Epoch 19405/30000 Training Loss: 0.07844357937574387\n",
      "Epoch 19406/30000 Training Loss: 0.06947285681962967\n",
      "Epoch 19407/30000 Training Loss: 0.06679898500442505\n",
      "Epoch 19408/30000 Training Loss: 0.07052353024482727\n",
      "Epoch 19409/30000 Training Loss: 0.055022746324539185\n",
      "Epoch 19410/30000 Training Loss: 0.07185900956392288\n",
      "Epoch 19410/30000 Validation Loss: 0.06186570227146149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19411/30000 Training Loss: 0.06378505378961563\n",
      "Epoch 19412/30000 Training Loss: 0.07393141090869904\n",
      "Epoch 19413/30000 Training Loss: 0.0647207722067833\n",
      "Epoch 19414/30000 Training Loss: 0.08389514684677124\n",
      "Epoch 19415/30000 Training Loss: 0.07958958297967911\n",
      "Epoch 19416/30000 Training Loss: 0.06358157098293304\n",
      "Epoch 19417/30000 Training Loss: 0.08470776677131653\n",
      "Epoch 19418/30000 Training Loss: 0.06575622409582138\n",
      "Epoch 19419/30000 Training Loss: 0.05824262276291847\n",
      "Epoch 19420/30000 Training Loss: 0.0700010359287262\n",
      "Epoch 19420/30000 Validation Loss: 0.07247195392847061\n",
      "Epoch 19421/30000 Training Loss: 0.05668628588318825\n",
      "Epoch 19422/30000 Training Loss: 0.06884972006082535\n",
      "Epoch 19423/30000 Training Loss: 0.059837695211172104\n",
      "Epoch 19424/30000 Training Loss: 0.07338174432516098\n",
      "Epoch 19425/30000 Training Loss: 0.07176396250724792\n",
      "Epoch 19426/30000 Training Loss: 0.07924003154039383\n",
      "Epoch 19427/30000 Training Loss: 0.08348806947469711\n",
      "Epoch 19428/30000 Training Loss: 0.062292054295539856\n",
      "Epoch 19429/30000 Training Loss: 0.06614499539136887\n",
      "Epoch 19430/30000 Training Loss: 0.05679989978671074\n",
      "Epoch 19430/30000 Validation Loss: 0.06646546721458435\n",
      "Epoch 19431/30000 Training Loss: 0.07501660287380219\n",
      "Epoch 19432/30000 Training Loss: 0.06792523711919785\n",
      "Epoch 19433/30000 Training Loss: 0.07628711313009262\n",
      "Epoch 19434/30000 Training Loss: 0.07905599474906921\n",
      "Epoch 19435/30000 Training Loss: 0.08551838994026184\n",
      "Epoch 19436/30000 Training Loss: 0.0758405402302742\n",
      "Epoch 19437/30000 Training Loss: 0.06077340245246887\n",
      "Epoch 19438/30000 Training Loss: 0.09197614341974258\n",
      "Epoch 19439/30000 Training Loss: 0.06653239578008652\n",
      "Epoch 19440/30000 Training Loss: 0.06202726438641548\n",
      "Epoch 19440/30000 Validation Loss: 0.08374437689781189\n",
      "Epoch 19441/30000 Training Loss: 0.06181149184703827\n",
      "Epoch 19442/30000 Training Loss: 0.06727247685194016\n",
      "Epoch 19443/30000 Training Loss: 0.05669519305229187\n",
      "Epoch 19444/30000 Training Loss: 0.06937909871339798\n",
      "Epoch 19445/30000 Training Loss: 0.08279597014188766\n",
      "Epoch 19446/30000 Training Loss: 0.07744044065475464\n",
      "Epoch 19447/30000 Training Loss: 0.09539913386106491\n",
      "Epoch 19448/30000 Training Loss: 0.07376024127006531\n",
      "Epoch 19449/30000 Training Loss: 0.060077738016843796\n",
      "Epoch 19450/30000 Training Loss: 0.07543598860502243\n",
      "Epoch 19450/30000 Validation Loss: 0.06895456463098526\n",
      "Epoch 19451/30000 Training Loss: 0.06325662136077881\n",
      "Epoch 19452/30000 Training Loss: 0.0769217237830162\n",
      "Epoch 19453/30000 Training Loss: 0.05958416685461998\n",
      "Epoch 19454/30000 Training Loss: 0.07513672858476639\n",
      "Epoch 19455/30000 Training Loss: 0.07945726066827774\n",
      "Epoch 19456/30000 Training Loss: 0.05657346546649933\n",
      "Epoch 19457/30000 Training Loss: 0.07347235828638077\n",
      "Epoch 19458/30000 Training Loss: 0.08077304065227509\n",
      "Epoch 19459/30000 Training Loss: 0.06268800795078278\n",
      "Epoch 19460/30000 Training Loss: 0.05946044996380806\n",
      "Epoch 19460/30000 Validation Loss: 0.06411965936422348\n",
      "Epoch 19461/30000 Training Loss: 0.06961789727210999\n",
      "Epoch 19462/30000 Training Loss: 0.06388872861862183\n",
      "Epoch 19463/30000 Training Loss: 0.057706043124198914\n",
      "Epoch 19464/30000 Training Loss: 0.06415289640426636\n",
      "Epoch 19465/30000 Training Loss: 0.05930863693356514\n",
      "Epoch 19466/30000 Training Loss: 0.06733433902263641\n",
      "Epoch 19467/30000 Training Loss: 0.07749265432357788\n",
      "Epoch 19468/30000 Training Loss: 0.07986991852521896\n",
      "Epoch 19469/30000 Training Loss: 0.06853675097227097\n",
      "Epoch 19470/30000 Training Loss: 0.09022492915391922\n",
      "Epoch 19470/30000 Validation Loss: 0.061117883771657944\n",
      "Epoch 19471/30000 Training Loss: 0.06729501485824585\n",
      "Epoch 19472/30000 Training Loss: 0.07706424593925476\n",
      "Epoch 19473/30000 Training Loss: 0.07404354214668274\n",
      "Epoch 19474/30000 Training Loss: 0.07793961465358734\n",
      "Epoch 19475/30000 Training Loss: 0.0671151652932167\n",
      "Epoch 19476/30000 Training Loss: 0.08640106767416\n",
      "Epoch 19477/30000 Training Loss: 0.07082071900367737\n",
      "Epoch 19478/30000 Training Loss: 0.07316318154335022\n",
      "Epoch 19479/30000 Training Loss: 0.0780048593878746\n",
      "Epoch 19480/30000 Training Loss: 0.06646723300218582\n",
      "Epoch 19480/30000 Validation Loss: 0.06777393072843552\n",
      "Epoch 19481/30000 Training Loss: 0.09275242686271667\n",
      "Epoch 19482/30000 Training Loss: 0.07925310730934143\n",
      "Epoch 19483/30000 Training Loss: 0.08500725030899048\n",
      "Epoch 19484/30000 Training Loss: 0.06399122625589371\n",
      "Epoch 19485/30000 Training Loss: 0.07718322426080704\n",
      "Epoch 19486/30000 Training Loss: 0.0618884302675724\n",
      "Epoch 19487/30000 Training Loss: 0.06568695604801178\n",
      "Epoch 19488/30000 Training Loss: 0.07273790240287781\n",
      "Epoch 19489/30000 Training Loss: 0.05125219747424126\n",
      "Epoch 19490/30000 Training Loss: 0.09451086074113846\n",
      "Epoch 19490/30000 Validation Loss: 0.06418438255786896\n",
      "Epoch 19491/30000 Training Loss: 0.07048207521438599\n",
      "Epoch 19492/30000 Training Loss: 0.05809534713625908\n",
      "Epoch 19493/30000 Training Loss: 0.05260760709643364\n",
      "Epoch 19494/30000 Training Loss: 0.10656491667032242\n",
      "Epoch 19495/30000 Training Loss: 0.05666375532746315\n",
      "Epoch 19496/30000 Training Loss: 0.06883504986763\n",
      "Epoch 19497/30000 Training Loss: 0.06841178983449936\n",
      "Epoch 19498/30000 Training Loss: 0.08864159137010574\n",
      "Epoch 19499/30000 Training Loss: 0.06234225630760193\n",
      "Epoch 19500/30000 Training Loss: 0.08354968577623367\n",
      "Epoch 19500/30000 Validation Loss: 0.06938008219003677\n",
      "Epoch 19501/30000 Training Loss: 0.07975568622350693\n",
      "Epoch 19502/30000 Training Loss: 0.056194890290498734\n",
      "Epoch 19503/30000 Training Loss: 0.06007043644785881\n",
      "Epoch 19504/30000 Training Loss: 0.07452245056629181\n",
      "Epoch 19505/30000 Training Loss: 0.07764814049005508\n",
      "Epoch 19506/30000 Training Loss: 0.0981464609503746\n",
      "Epoch 19507/30000 Training Loss: 0.09626223891973495\n",
      "Epoch 19508/30000 Training Loss: 0.08187402784824371\n",
      "Epoch 19509/30000 Training Loss: 0.08203595131635666\n",
      "Epoch 19510/30000 Training Loss: 0.09579962491989136\n",
      "Epoch 19510/30000 Validation Loss: 0.0697743371129036\n",
      "Epoch 19511/30000 Training Loss: 0.08997529745101929\n",
      "Epoch 19512/30000 Training Loss: 0.08006028085947037\n",
      "Epoch 19513/30000 Training Loss: 0.06961377710103989\n",
      "Epoch 19514/30000 Training Loss: 0.08147748559713364\n",
      "Epoch 19515/30000 Training Loss: 0.07865286618471146\n",
      "Epoch 19516/30000 Training Loss: 0.059442926198244095\n",
      "Epoch 19517/30000 Training Loss: 0.0917869433760643\n",
      "Epoch 19518/30000 Training Loss: 0.058755338191986084\n",
      "Epoch 19519/30000 Training Loss: 0.08480306714773178\n",
      "Epoch 19520/30000 Training Loss: 0.060612887144088745\n",
      "Epoch 19520/30000 Validation Loss: 0.07739803940057755\n",
      "Epoch 19521/30000 Training Loss: 0.06737122684717178\n",
      "Epoch 19522/30000 Training Loss: 0.05443437024950981\n",
      "Epoch 19523/30000 Training Loss: 0.08490084856748581\n",
      "Epoch 19524/30000 Training Loss: 0.07016492635011673\n",
      "Epoch 19525/30000 Training Loss: 0.06264691799879074\n",
      "Epoch 19526/30000 Training Loss: 0.08209624141454697\n",
      "Epoch 19527/30000 Training Loss: 0.062075357884168625\n",
      "Epoch 19528/30000 Training Loss: 0.0606166273355484\n",
      "Epoch 19529/30000 Training Loss: 0.04421806335449219\n",
      "Epoch 19530/30000 Training Loss: 0.07396633177995682\n",
      "Epoch 19530/30000 Validation Loss: 0.06585654616355896\n",
      "Epoch 19531/30000 Training Loss: 0.05287441983819008\n",
      "Epoch 19532/30000 Training Loss: 0.08961807936429977\n",
      "Epoch 19533/30000 Training Loss: 0.07037857919931412\n",
      "Epoch 19534/30000 Training Loss: 0.08979359269142151\n",
      "Epoch 19535/30000 Training Loss: 0.055714208632707596\n",
      "Epoch 19536/30000 Training Loss: 0.07062558084726334\n",
      "Epoch 19537/30000 Training Loss: 0.06393701583147049\n",
      "Epoch 19538/30000 Training Loss: 0.06613827496767044\n",
      "Epoch 19539/30000 Training Loss: 0.06767458468675613\n",
      "Epoch 19540/30000 Training Loss: 0.06185130402445793\n",
      "Epoch 19540/30000 Validation Loss: 0.06288919597864151\n",
      "Epoch 19541/30000 Training Loss: 0.07393306493759155\n",
      "Epoch 19542/30000 Training Loss: 0.07100934535264969\n",
      "Epoch 19543/30000 Training Loss: 0.06842272728681564\n",
      "Epoch 19544/30000 Training Loss: 0.07178378850221634\n",
      "Epoch 19545/30000 Training Loss: 0.05826757475733757\n",
      "Epoch 19546/30000 Training Loss: 0.06334742903709412\n",
      "Epoch 19547/30000 Training Loss: 0.07524287700653076\n",
      "Epoch 19548/30000 Training Loss: 0.0878501757979393\n",
      "Epoch 19549/30000 Training Loss: 0.06569463759660721\n",
      "Epoch 19550/30000 Training Loss: 0.05855293944478035\n",
      "Epoch 19550/30000 Validation Loss: 0.06827794760465622\n",
      "Epoch 19551/30000 Training Loss: 0.06745519489049911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19552/30000 Training Loss: 0.06653108447790146\n",
      "Epoch 19553/30000 Training Loss: 0.06460199505090714\n",
      "Epoch 19554/30000 Training Loss: 0.06756136566400528\n",
      "Epoch 19555/30000 Training Loss: 0.0580141544342041\n",
      "Epoch 19556/30000 Training Loss: 0.07681220769882202\n",
      "Epoch 19557/30000 Training Loss: 0.06268022209405899\n",
      "Epoch 19558/30000 Training Loss: 0.07335560768842697\n",
      "Epoch 19559/30000 Training Loss: 0.07424328476190567\n",
      "Epoch 19560/30000 Training Loss: 0.07540670782327652\n",
      "Epoch 19560/30000 Validation Loss: 0.0804460346698761\n",
      "Epoch 19561/30000 Training Loss: 0.08169320225715637\n",
      "Epoch 19562/30000 Training Loss: 0.07572714239358902\n",
      "Epoch 19563/30000 Training Loss: 0.04624390974640846\n",
      "Epoch 19564/30000 Training Loss: 0.07280611246824265\n",
      "Epoch 19565/30000 Training Loss: 0.05452723428606987\n",
      "Epoch 19566/30000 Training Loss: 0.057573314756155014\n",
      "Epoch 19567/30000 Training Loss: 0.07628504931926727\n",
      "Epoch 19568/30000 Training Loss: 0.06458709388971329\n",
      "Epoch 19569/30000 Training Loss: 0.08558696508407593\n",
      "Epoch 19570/30000 Training Loss: 0.07433107495307922\n",
      "Epoch 19570/30000 Validation Loss: 0.08357248455286026\n",
      "Epoch 19571/30000 Training Loss: 0.06327465921640396\n",
      "Epoch 19572/30000 Training Loss: 0.07408732920885086\n",
      "Epoch 19573/30000 Training Loss: 0.07151728123426437\n",
      "Epoch 19574/30000 Training Loss: 0.0958363488316536\n",
      "Epoch 19575/30000 Training Loss: 0.08324503898620605\n",
      "Epoch 19576/30000 Training Loss: 0.08450517058372498\n",
      "Epoch 19577/30000 Training Loss: 0.07519716769456863\n",
      "Epoch 19578/30000 Training Loss: 0.09460324048995972\n",
      "Epoch 19579/30000 Training Loss: 0.04936864972114563\n",
      "Epoch 19580/30000 Training Loss: 0.08456673473119736\n",
      "Epoch 19580/30000 Validation Loss: 0.06751853227615356\n",
      "Epoch 19581/30000 Training Loss: 0.08752354234457016\n",
      "Epoch 19582/30000 Training Loss: 0.06461963802576065\n",
      "Epoch 19583/30000 Training Loss: 0.07549526542425156\n",
      "Epoch 19584/30000 Training Loss: 0.08982712030410767\n",
      "Epoch 19585/30000 Training Loss: 0.08521831035614014\n",
      "Epoch 19586/30000 Training Loss: 0.07917921990156174\n",
      "Epoch 19587/30000 Training Loss: 0.062064170837402344\n",
      "Epoch 19588/30000 Training Loss: 0.06959596276283264\n",
      "Epoch 19589/30000 Training Loss: 0.07180412113666534\n",
      "Epoch 19590/30000 Training Loss: 0.06817138195037842\n",
      "Epoch 19590/30000 Validation Loss: 0.07225918024778366\n",
      "Epoch 19591/30000 Training Loss: 0.07481091469526291\n",
      "Epoch 19592/30000 Training Loss: 0.061084628105163574\n",
      "Epoch 19593/30000 Training Loss: 0.07126036286354065\n",
      "Epoch 19594/30000 Training Loss: 0.07739080488681793\n",
      "Epoch 19595/30000 Training Loss: 0.07084879279136658\n",
      "Epoch 19596/30000 Training Loss: 0.06559127569198608\n",
      "Epoch 19597/30000 Training Loss: 0.06313461065292358\n",
      "Epoch 19598/30000 Training Loss: 0.0736251249909401\n",
      "Epoch 19599/30000 Training Loss: 0.06355751305818558\n",
      "Epoch 19600/30000 Training Loss: 0.08525258302688599\n",
      "Epoch 19600/30000 Validation Loss: 0.08118874579668045\n",
      "Epoch 19601/30000 Training Loss: 0.05659041926264763\n",
      "Epoch 19602/30000 Training Loss: 0.06579184532165527\n",
      "Epoch 19603/30000 Training Loss: 0.07710231095552444\n",
      "Epoch 19604/30000 Training Loss: 0.0791194811463356\n",
      "Epoch 19605/30000 Training Loss: 0.05492790415883064\n",
      "Epoch 19606/30000 Training Loss: 0.06042599678039551\n",
      "Epoch 19607/30000 Training Loss: 0.07879702001810074\n",
      "Epoch 19608/30000 Training Loss: 0.05661467835307121\n",
      "Epoch 19609/30000 Training Loss: 0.0716027021408081\n",
      "Epoch 19610/30000 Training Loss: 0.06655975431203842\n",
      "Epoch 19610/30000 Validation Loss: 0.08114472776651382\n",
      "Epoch 19611/30000 Training Loss: 0.07234073430299759\n",
      "Epoch 19612/30000 Training Loss: 0.08368909358978271\n",
      "Epoch 19613/30000 Training Loss: 0.051769182085990906\n",
      "Epoch 19614/30000 Training Loss: 0.06229177489876747\n",
      "Epoch 19615/30000 Training Loss: 0.06400441378355026\n",
      "Epoch 19616/30000 Training Loss: 0.06478919833898544\n",
      "Epoch 19617/30000 Training Loss: 0.05886825546622276\n",
      "Epoch 19618/30000 Training Loss: 0.06247469782829285\n",
      "Epoch 19619/30000 Training Loss: 0.07462690025568008\n",
      "Epoch 19620/30000 Training Loss: 0.0664563849568367\n",
      "Epoch 19620/30000 Validation Loss: 0.060485463589429855\n",
      "Epoch 19621/30000 Training Loss: 0.06432675570249557\n",
      "Epoch 19622/30000 Training Loss: 0.0607396699488163\n",
      "Epoch 19623/30000 Training Loss: 0.09263419359922409\n",
      "Epoch 19624/30000 Training Loss: 0.07270128279924393\n",
      "Epoch 19625/30000 Training Loss: 0.07806971669197083\n",
      "Epoch 19626/30000 Training Loss: 0.08292286843061447\n",
      "Epoch 19627/30000 Training Loss: 0.08150606602430344\n",
      "Epoch 19628/30000 Training Loss: 0.07412237673997879\n",
      "Epoch 19629/30000 Training Loss: 0.07771933823823929\n",
      "Epoch 19630/30000 Training Loss: 0.08784735202789307\n",
      "Epoch 19630/30000 Validation Loss: 0.06948060542345047\n",
      "Epoch 19631/30000 Training Loss: 0.06347358971834183\n",
      "Epoch 19632/30000 Training Loss: 0.05314573645591736\n",
      "Epoch 19633/30000 Training Loss: 0.06770811229944229\n",
      "Epoch 19634/30000 Training Loss: 0.08239742368459702\n",
      "Epoch 19635/30000 Training Loss: 0.07391715049743652\n",
      "Epoch 19636/30000 Training Loss: 0.07584601640701294\n",
      "Epoch 19637/30000 Training Loss: 0.06790519505739212\n",
      "Epoch 19638/30000 Training Loss: 0.07194527983665466\n",
      "Epoch 19639/30000 Training Loss: 0.05213133990764618\n",
      "Epoch 19640/30000 Training Loss: 0.06985726207494736\n",
      "Epoch 19640/30000 Validation Loss: 0.06915029138326645\n",
      "Epoch 19641/30000 Training Loss: 0.051237720996141434\n",
      "Epoch 19642/30000 Training Loss: 0.06533502787351608\n",
      "Epoch 19643/30000 Training Loss: 0.07648887485265732\n",
      "Epoch 19644/30000 Training Loss: 0.06819356232881546\n",
      "Epoch 19645/30000 Training Loss: 0.06325971335172653\n",
      "Epoch 19646/30000 Training Loss: 0.07640745490789413\n",
      "Epoch 19647/30000 Training Loss: 0.08556289225816727\n",
      "Epoch 19648/30000 Training Loss: 0.06990621238946915\n",
      "Epoch 19649/30000 Training Loss: 0.07963941246271133\n",
      "Epoch 19650/30000 Training Loss: 0.06930447369813919\n",
      "Epoch 19650/30000 Validation Loss: 0.08779051899909973\n",
      "Epoch 19651/30000 Training Loss: 0.06720294803380966\n",
      "Epoch 19652/30000 Training Loss: 0.07808363437652588\n",
      "Epoch 19653/30000 Training Loss: 0.07655889540910721\n",
      "Epoch 19654/30000 Training Loss: 0.0756787434220314\n",
      "Epoch 19655/30000 Training Loss: 0.0573943667113781\n",
      "Epoch 19656/30000 Training Loss: 0.07655662298202515\n",
      "Epoch 19657/30000 Training Loss: 0.0624857135117054\n",
      "Epoch 19658/30000 Training Loss: 0.060716841369867325\n",
      "Epoch 19659/30000 Training Loss: 0.06908828020095825\n",
      "Epoch 19660/30000 Training Loss: 0.06659821420907974\n",
      "Epoch 19660/30000 Validation Loss: 0.05913762375712395\n",
      "Epoch 19661/30000 Training Loss: 0.0670139342546463\n",
      "Epoch 19662/30000 Training Loss: 0.06996432691812515\n",
      "Epoch 19663/30000 Training Loss: 0.07037738710641861\n",
      "Epoch 19664/30000 Training Loss: 0.06992961466312408\n",
      "Epoch 19665/30000 Training Loss: 0.0719422921538353\n",
      "Epoch 19666/30000 Training Loss: 0.06939776986837387\n",
      "Epoch 19667/30000 Training Loss: 0.0846298560500145\n",
      "Epoch 19668/30000 Training Loss: 0.06091894209384918\n",
      "Epoch 19669/30000 Training Loss: 0.0774301066994667\n",
      "Epoch 19670/30000 Training Loss: 0.08873002976179123\n",
      "Epoch 19670/30000 Validation Loss: 0.07454588264226913\n",
      "Epoch 19671/30000 Training Loss: 0.08423420786857605\n",
      "Epoch 19672/30000 Training Loss: 0.07035233825445175\n",
      "Epoch 19673/30000 Training Loss: 0.07772510498762131\n",
      "Epoch 19674/30000 Training Loss: 0.06955210119485855\n",
      "Epoch 19675/30000 Training Loss: 0.08587378263473511\n",
      "Epoch 19676/30000 Training Loss: 0.06489390879869461\n",
      "Epoch 19677/30000 Training Loss: 0.09654510021209717\n",
      "Epoch 19678/30000 Training Loss: 0.06899341195821762\n",
      "Epoch 19679/30000 Training Loss: 0.07056387513875961\n",
      "Epoch 19680/30000 Training Loss: 0.06397102028131485\n",
      "Epoch 19680/30000 Validation Loss: 0.07116799801588058\n",
      "Epoch 19681/30000 Training Loss: 0.06683216243982315\n",
      "Epoch 19682/30000 Training Loss: 0.05719194933772087\n",
      "Epoch 19683/30000 Training Loss: 0.08519160747528076\n",
      "Epoch 19684/30000 Training Loss: 0.10747606307268143\n",
      "Epoch 19685/30000 Training Loss: 0.07455169409513474\n",
      "Epoch 19686/30000 Training Loss: 0.0815606489777565\n",
      "Epoch 19687/30000 Training Loss: 0.07769384235143661\n",
      "Epoch 19688/30000 Training Loss: 0.07835384458303452\n",
      "Epoch 19689/30000 Training Loss: 0.07407063245773315\n",
      "Epoch 19690/30000 Training Loss: 0.06099627912044525\n",
      "Epoch 19690/30000 Validation Loss: 0.06946174800395966\n",
      "Epoch 19691/30000 Training Loss: 0.06823211908340454\n",
      "Epoch 19692/30000 Training Loss: 0.057231154292821884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19693/30000 Training Loss: 0.08612582087516785\n",
      "Epoch 19694/30000 Training Loss: 0.061782628297805786\n",
      "Epoch 19695/30000 Training Loss: 0.06739860028028488\n",
      "Epoch 19696/30000 Training Loss: 0.06199727579951286\n",
      "Epoch 19697/30000 Training Loss: 0.05614816024899483\n",
      "Epoch 19698/30000 Training Loss: 0.07033286243677139\n",
      "Epoch 19699/30000 Training Loss: 0.07350683957338333\n",
      "Epoch 19700/30000 Training Loss: 0.07854389399290085\n",
      "Epoch 19700/30000 Validation Loss: 0.06499486416578293\n",
      "Epoch 19701/30000 Training Loss: 0.07826259732246399\n",
      "Epoch 19702/30000 Training Loss: 0.06946345418691635\n",
      "Epoch 19703/30000 Training Loss: 0.07311694324016571\n",
      "Epoch 19704/30000 Training Loss: 0.06577839702367783\n",
      "Epoch 19705/30000 Training Loss: 0.10133592039346695\n",
      "Epoch 19706/30000 Training Loss: 0.05606464669108391\n",
      "Epoch 19707/30000 Training Loss: 0.08750277757644653\n",
      "Epoch 19708/30000 Training Loss: 0.04858266934752464\n",
      "Epoch 19709/30000 Training Loss: 0.06763029098510742\n",
      "Epoch 19710/30000 Training Loss: 0.06991199404001236\n",
      "Epoch 19710/30000 Validation Loss: 0.05784435197710991\n",
      "Epoch 19711/30000 Training Loss: 0.07924070954322815\n",
      "Epoch 19712/30000 Training Loss: 0.07021906971931458\n",
      "Epoch 19713/30000 Training Loss: 0.09571367502212524\n",
      "Epoch 19714/30000 Training Loss: 0.09710875153541565\n",
      "Epoch 19715/30000 Training Loss: 0.09955019503831863\n",
      "Epoch 19716/30000 Training Loss: 0.07143978774547577\n",
      "Epoch 19717/30000 Training Loss: 0.08776629716157913\n",
      "Epoch 19718/30000 Training Loss: 0.06215718388557434\n",
      "Epoch 19719/30000 Training Loss: 0.07802950590848923\n",
      "Epoch 19720/30000 Training Loss: 0.06112850829958916\n",
      "Epoch 19720/30000 Validation Loss: 0.06533171981573105\n",
      "Epoch 19721/30000 Training Loss: 0.08195465058088303\n",
      "Epoch 19722/30000 Training Loss: 0.09997948259115219\n",
      "Epoch 19723/30000 Training Loss: 0.08122915774583817\n",
      "Epoch 19724/30000 Training Loss: 0.06607495248317719\n",
      "Epoch 19725/30000 Training Loss: 0.07517442852258682\n",
      "Epoch 19726/30000 Training Loss: 0.05692865327000618\n",
      "Epoch 19727/30000 Training Loss: 0.09905624389648438\n",
      "Epoch 19728/30000 Training Loss: 0.06520158052444458\n",
      "Epoch 19729/30000 Training Loss: 0.07913505285978317\n",
      "Epoch 19730/30000 Training Loss: 0.05504212900996208\n",
      "Epoch 19730/30000 Validation Loss: 0.05220493674278259\n",
      "Epoch 19731/30000 Training Loss: 0.06832123547792435\n",
      "Epoch 19732/30000 Training Loss: 0.07637056708335876\n",
      "Epoch 19733/30000 Training Loss: 0.08142029494047165\n",
      "Epoch 19734/30000 Training Loss: 0.07361384481191635\n",
      "Epoch 19735/30000 Training Loss: 0.0705091580748558\n",
      "Epoch 19736/30000 Training Loss: 0.06360150128602982\n",
      "Epoch 19737/30000 Training Loss: 0.061881184577941895\n",
      "Epoch 19738/30000 Training Loss: 0.08647870272397995\n",
      "Epoch 19739/30000 Training Loss: 0.07685533165931702\n",
      "Epoch 19740/30000 Training Loss: 0.09290802478790283\n",
      "Epoch 19740/30000 Validation Loss: 0.07717984914779663\n",
      "Epoch 19741/30000 Training Loss: 0.08319488167762756\n",
      "Epoch 19742/30000 Training Loss: 0.07621420174837112\n",
      "Epoch 19743/30000 Training Loss: 0.07089421898126602\n",
      "Epoch 19744/30000 Training Loss: 0.06809880584478378\n",
      "Epoch 19745/30000 Training Loss: 0.0790482833981514\n",
      "Epoch 19746/30000 Training Loss: 0.05527561902999878\n",
      "Epoch 19747/30000 Training Loss: 0.07829395681619644\n",
      "Epoch 19748/30000 Training Loss: 0.06348803639411926\n",
      "Epoch 19749/30000 Training Loss: 0.08206869661808014\n",
      "Epoch 19750/30000 Training Loss: 0.0764252245426178\n",
      "Epoch 19750/30000 Validation Loss: 0.05753420665860176\n",
      "Epoch 19751/30000 Training Loss: 0.07151646167039871\n",
      "Epoch 19752/30000 Training Loss: 0.06961921602487564\n",
      "Epoch 19753/30000 Training Loss: 0.08827710151672363\n",
      "Epoch 19754/30000 Training Loss: 0.07520344108343124\n",
      "Epoch 19755/30000 Training Loss: 0.0913107767701149\n",
      "Epoch 19756/30000 Training Loss: 0.08079084753990173\n",
      "Epoch 19757/30000 Training Loss: 0.07501216232776642\n",
      "Epoch 19758/30000 Training Loss: 0.07634527236223221\n",
      "Epoch 19759/30000 Training Loss: 0.06802498549222946\n",
      "Epoch 19760/30000 Training Loss: 0.05960517004132271\n",
      "Epoch 19760/30000 Validation Loss: 0.0863405391573906\n",
      "Epoch 19761/30000 Training Loss: 0.08476665616035461\n",
      "Epoch 19762/30000 Training Loss: 0.09339801222085953\n",
      "Epoch 19763/30000 Training Loss: 0.06942103058099747\n",
      "Epoch 19764/30000 Training Loss: 0.07512690871953964\n",
      "Epoch 19765/30000 Training Loss: 0.06981401890516281\n",
      "Epoch 19766/30000 Training Loss: 0.07220902293920517\n",
      "Epoch 19767/30000 Training Loss: 0.07134833186864853\n",
      "Epoch 19768/30000 Training Loss: 0.06520938128232956\n",
      "Epoch 19769/30000 Training Loss: 0.07757075130939484\n",
      "Epoch 19770/30000 Training Loss: 0.06931138783693314\n",
      "Epoch 19770/30000 Validation Loss: 0.06178988516330719\n",
      "Epoch 19771/30000 Training Loss: 0.07195043563842773\n",
      "Epoch 19772/30000 Training Loss: 0.06637518852949142\n",
      "Epoch 19773/30000 Training Loss: 0.06318358331918716\n",
      "Epoch 19774/30000 Training Loss: 0.06356918066740036\n",
      "Epoch 19775/30000 Training Loss: 0.08269255608320236\n",
      "Epoch 19776/30000 Training Loss: 0.08712080121040344\n",
      "Epoch 19777/30000 Training Loss: 0.07163036614656448\n",
      "Epoch 19778/30000 Training Loss: 0.05595336854457855\n",
      "Epoch 19779/30000 Training Loss: 0.07120310515165329\n",
      "Epoch 19780/30000 Training Loss: 0.07360971719026566\n",
      "Epoch 19780/30000 Validation Loss: 0.05197462439537048\n",
      "Epoch 19781/30000 Training Loss: 0.06766556203365326\n",
      "Epoch 19782/30000 Training Loss: 0.06389304995536804\n",
      "Epoch 19783/30000 Training Loss: 0.08820386976003647\n",
      "Epoch 19784/30000 Training Loss: 0.09153757244348526\n",
      "Epoch 19785/30000 Training Loss: 0.08071006089448929\n",
      "Epoch 19786/30000 Training Loss: 0.06948279589414597\n",
      "Epoch 19787/30000 Training Loss: 0.07655441015958786\n",
      "Epoch 19788/30000 Training Loss: 0.08207631856203079\n",
      "Epoch 19789/30000 Training Loss: 0.051540449261665344\n",
      "Epoch 19790/30000 Training Loss: 0.06270632147789001\n",
      "Epoch 19790/30000 Validation Loss: 0.08067171275615692\n",
      "Epoch 19791/30000 Training Loss: 0.0562557578086853\n",
      "Epoch 19792/30000 Training Loss: 0.10237350314855576\n",
      "Epoch 19793/30000 Training Loss: 0.060266971588134766\n",
      "Epoch 19794/30000 Training Loss: 0.09398612380027771\n",
      "Epoch 19795/30000 Training Loss: 0.06862955540418625\n",
      "Epoch 19796/30000 Training Loss: 0.07808235287666321\n",
      "Epoch 19797/30000 Training Loss: 0.06271182745695114\n",
      "Epoch 19798/30000 Training Loss: 0.07259044796228409\n",
      "Epoch 19799/30000 Training Loss: 0.06562285870313644\n",
      "Epoch 19800/30000 Training Loss: 0.07265973836183548\n",
      "Epoch 19800/30000 Validation Loss: 0.06705264002084732\n",
      "Epoch 19801/30000 Training Loss: 0.07136977463960648\n",
      "Epoch 19802/30000 Training Loss: 0.08185437321662903\n",
      "Epoch 19803/30000 Training Loss: 0.07388057559728622\n",
      "Epoch 19804/30000 Training Loss: 0.08863008767366409\n",
      "Epoch 19805/30000 Training Loss: 0.06900620460510254\n",
      "Epoch 19806/30000 Training Loss: 0.08943530172109604\n",
      "Epoch 19807/30000 Training Loss: 0.07074538618326187\n",
      "Epoch 19808/30000 Training Loss: 0.08207034319639206\n",
      "Epoch 19809/30000 Training Loss: 0.07251051068305969\n",
      "Epoch 19810/30000 Training Loss: 0.08979567140340805\n",
      "Epoch 19810/30000 Validation Loss: 0.052726928144693375\n",
      "Epoch 19811/30000 Training Loss: 0.08884669095277786\n",
      "Epoch 19812/30000 Training Loss: 0.06640265136957169\n",
      "Epoch 19813/30000 Training Loss: 0.061564818024635315\n",
      "Epoch 19814/30000 Training Loss: 0.07701624184846878\n",
      "Epoch 19815/30000 Training Loss: 0.07345842570066452\n",
      "Epoch 19816/30000 Training Loss: 0.07834116369485855\n",
      "Epoch 19817/30000 Training Loss: 0.06960532814264297\n",
      "Epoch 19818/30000 Training Loss: 0.0723748728632927\n",
      "Epoch 19819/30000 Training Loss: 0.06966663897037506\n",
      "Epoch 19820/30000 Training Loss: 0.08353149145841599\n",
      "Epoch 19820/30000 Validation Loss: 0.06949865818023682\n",
      "Epoch 19821/30000 Training Loss: 0.06115757301449776\n",
      "Epoch 19822/30000 Training Loss: 0.09631115943193436\n",
      "Epoch 19823/30000 Training Loss: 0.05700086057186127\n",
      "Epoch 19824/30000 Training Loss: 0.06516767293214798\n",
      "Epoch 19825/30000 Training Loss: 0.06987033039331436\n",
      "Epoch 19826/30000 Training Loss: 0.05483323335647583\n",
      "Epoch 19827/30000 Training Loss: 0.09041029214859009\n",
      "Epoch 19828/30000 Training Loss: 0.0866926833987236\n",
      "Epoch 19829/30000 Training Loss: 0.06644989550113678\n",
      "Epoch 19830/30000 Training Loss: 0.06684059649705887\n",
      "Epoch 19830/30000 Validation Loss: 0.07924406975507736\n",
      "Epoch 19831/30000 Training Loss: 0.056751567870378494\n",
      "Epoch 19832/30000 Training Loss: 0.06233488395810127\n",
      "Epoch 19833/30000 Training Loss: 0.06759687513113022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19834/30000 Training Loss: 0.06671103835105896\n",
      "Epoch 19835/30000 Training Loss: 0.0655483528971672\n",
      "Epoch 19836/30000 Training Loss: 0.0543675534427166\n",
      "Epoch 19837/30000 Training Loss: 0.06261779367923737\n",
      "Epoch 19838/30000 Training Loss: 0.07115186005830765\n",
      "Epoch 19839/30000 Training Loss: 0.06315367668867111\n",
      "Epoch 19840/30000 Training Loss: 0.08621527999639511\n",
      "Epoch 19840/30000 Validation Loss: 0.06538362056016922\n",
      "Epoch 19841/30000 Training Loss: 0.09123537689447403\n",
      "Epoch 19842/30000 Training Loss: 0.07373631745576859\n",
      "Epoch 19843/30000 Training Loss: 0.07511460036039352\n",
      "Epoch 19844/30000 Training Loss: 0.061354201287031174\n",
      "Epoch 19845/30000 Training Loss: 0.06523120403289795\n",
      "Epoch 19846/30000 Training Loss: 0.05709265545010567\n",
      "Epoch 19847/30000 Training Loss: 0.06918101757764816\n",
      "Epoch 19848/30000 Training Loss: 0.07639601081609726\n",
      "Epoch 19849/30000 Training Loss: 0.06075344607234001\n",
      "Epoch 19850/30000 Training Loss: 0.07513806968927383\n",
      "Epoch 19850/30000 Validation Loss: 0.0650269091129303\n",
      "Epoch 19851/30000 Training Loss: 0.08322139084339142\n",
      "Epoch 19852/30000 Training Loss: 0.07456336170434952\n",
      "Epoch 19853/30000 Training Loss: 0.07662344723939896\n",
      "Epoch 19854/30000 Training Loss: 0.07608789205551147\n",
      "Epoch 19855/30000 Training Loss: 0.08323944360017776\n",
      "Epoch 19856/30000 Training Loss: 0.07052389532327652\n",
      "Epoch 19857/30000 Training Loss: 0.06625723093748093\n",
      "Epoch 19858/30000 Training Loss: 0.06937792152166367\n",
      "Epoch 19859/30000 Training Loss: 0.07387932389974594\n",
      "Epoch 19860/30000 Training Loss: 0.05612261965870857\n",
      "Epoch 19860/30000 Validation Loss: 0.08637315034866333\n",
      "Epoch 19861/30000 Training Loss: 0.0818651020526886\n",
      "Epoch 19862/30000 Training Loss: 0.07390867918729782\n",
      "Epoch 19863/30000 Training Loss: 0.09582655876874924\n",
      "Epoch 19864/30000 Training Loss: 0.07609280198812485\n",
      "Epoch 19865/30000 Training Loss: 0.06832868605852127\n",
      "Epoch 19866/30000 Training Loss: 0.08377989381551743\n",
      "Epoch 19867/30000 Training Loss: 0.05993596836924553\n",
      "Epoch 19868/30000 Training Loss: 0.08512391895055771\n",
      "Epoch 19869/30000 Training Loss: 0.06498313695192337\n",
      "Epoch 19870/30000 Training Loss: 0.06670921295881271\n",
      "Epoch 19870/30000 Validation Loss: 0.07417530566453934\n",
      "Epoch 19871/30000 Training Loss: 0.060340166091918945\n",
      "Epoch 19872/30000 Training Loss: 0.0758586898446083\n",
      "Epoch 19873/30000 Training Loss: 0.06896694749593735\n",
      "Epoch 19874/30000 Training Loss: 0.059246648102998734\n",
      "Epoch 19875/30000 Training Loss: 0.05960703268647194\n",
      "Epoch 19876/30000 Training Loss: 0.0867515429854393\n",
      "Epoch 19877/30000 Training Loss: 0.0650276318192482\n",
      "Epoch 19878/30000 Training Loss: 0.06266030669212341\n",
      "Epoch 19879/30000 Training Loss: 0.08188629895448685\n",
      "Epoch 19880/30000 Training Loss: 0.07040611654520035\n",
      "Epoch 19880/30000 Validation Loss: 0.06446326524019241\n",
      "Epoch 19881/30000 Training Loss: 0.06714054942131042\n",
      "Epoch 19882/30000 Training Loss: 0.0744175985455513\n",
      "Epoch 19883/30000 Training Loss: 0.062361616641283035\n",
      "Epoch 19884/30000 Training Loss: 0.07612116634845734\n",
      "Epoch 19885/30000 Training Loss: 0.05772319436073303\n",
      "Epoch 19886/30000 Training Loss: 0.08291799575090408\n",
      "Epoch 19887/30000 Training Loss: 0.06769108027219772\n",
      "Epoch 19888/30000 Training Loss: 0.07203207165002823\n",
      "Epoch 19889/30000 Training Loss: 0.07205145806074142\n",
      "Epoch 19890/30000 Training Loss: 0.06642138212919235\n",
      "Epoch 19890/30000 Validation Loss: 0.07376030832529068\n",
      "Epoch 19891/30000 Training Loss: 0.06825102120637894\n",
      "Epoch 19892/30000 Training Loss: 0.07016608864068985\n",
      "Epoch 19893/30000 Training Loss: 0.05786517634987831\n",
      "Epoch 19894/30000 Training Loss: 0.07411541789770126\n",
      "Epoch 19895/30000 Training Loss: 0.07599130272865295\n",
      "Epoch 19896/30000 Training Loss: 0.06270116567611694\n",
      "Epoch 19897/30000 Training Loss: 0.0900394394993782\n",
      "Epoch 19898/30000 Training Loss: 0.07298993319272995\n",
      "Epoch 19899/30000 Training Loss: 0.06310669332742691\n",
      "Epoch 19900/30000 Training Loss: 0.06759849935770035\n",
      "Epoch 19900/30000 Validation Loss: 0.08629564195871353\n",
      "Epoch 19901/30000 Training Loss: 0.07103627920150757\n",
      "Epoch 19902/30000 Training Loss: 0.08174031972885132\n",
      "Epoch 19903/30000 Training Loss: 0.07540255039930344\n",
      "Epoch 19904/30000 Training Loss: 0.08205186575651169\n",
      "Epoch 19905/30000 Training Loss: 0.07333006709814072\n",
      "Epoch 19906/30000 Training Loss: 0.06404774636030197\n",
      "Epoch 19907/30000 Training Loss: 0.06319985538721085\n",
      "Epoch 19908/30000 Training Loss: 0.06811919063329697\n",
      "Epoch 19909/30000 Training Loss: 0.08203249424695969\n",
      "Epoch 19910/30000 Training Loss: 0.08121869713068008\n",
      "Epoch 19910/30000 Validation Loss: 0.07355265319347382\n",
      "Epoch 19911/30000 Training Loss: 0.08430218696594238\n",
      "Epoch 19912/30000 Training Loss: 0.06785530596971512\n",
      "Epoch 19913/30000 Training Loss: 0.07327130436897278\n",
      "Epoch 19914/30000 Training Loss: 0.08667081594467163\n",
      "Epoch 19915/30000 Training Loss: 0.07076490670442581\n",
      "Epoch 19916/30000 Training Loss: 0.05461518093943596\n",
      "Epoch 19917/30000 Training Loss: 0.0592794232070446\n",
      "Epoch 19918/30000 Training Loss: 0.065699003636837\n",
      "Epoch 19919/30000 Training Loss: 0.0756218209862709\n",
      "Epoch 19920/30000 Training Loss: 0.07602435350418091\n",
      "Epoch 19920/30000 Validation Loss: 0.07798267155885696\n",
      "Epoch 19921/30000 Training Loss: 0.06612936407327652\n",
      "Epoch 19922/30000 Training Loss: 0.07343485206365585\n",
      "Epoch 19923/30000 Training Loss: 0.07938244193792343\n",
      "Epoch 19924/30000 Training Loss: 0.08556623011827469\n",
      "Epoch 19925/30000 Training Loss: 0.06750701367855072\n",
      "Epoch 19926/30000 Training Loss: 0.08879753947257996\n",
      "Epoch 19927/30000 Training Loss: 0.07295522093772888\n",
      "Epoch 19928/30000 Training Loss: 0.0551518015563488\n",
      "Epoch 19929/30000 Training Loss: 0.07891658693552017\n",
      "Epoch 19930/30000 Training Loss: 0.07671025395393372\n",
      "Epoch 19930/30000 Validation Loss: 0.0750863254070282\n",
      "Epoch 19931/30000 Training Loss: 0.0694386288523674\n",
      "Epoch 19932/30000 Training Loss: 0.07686225324869156\n",
      "Epoch 19933/30000 Training Loss: 0.07653730362653732\n",
      "Epoch 19934/30000 Training Loss: 0.08514857292175293\n",
      "Epoch 19935/30000 Training Loss: 0.07020960003137589\n",
      "Epoch 19936/30000 Training Loss: 0.08666592836380005\n",
      "Epoch 19937/30000 Training Loss: 0.05110206827521324\n",
      "Epoch 19938/30000 Training Loss: 0.0940142497420311\n",
      "Epoch 19939/30000 Training Loss: 0.058726951479911804\n",
      "Epoch 19940/30000 Training Loss: 0.06778334826231003\n",
      "Epoch 19940/30000 Validation Loss: 0.07242194563150406\n",
      "Epoch 19941/30000 Training Loss: 0.05921245738863945\n",
      "Epoch 19942/30000 Training Loss: 0.08089440315961838\n",
      "Epoch 19943/30000 Training Loss: 0.09308641403913498\n",
      "Epoch 19944/30000 Training Loss: 0.07566335052251816\n",
      "Epoch 19945/30000 Training Loss: 0.061303406953811646\n",
      "Epoch 19946/30000 Training Loss: 0.06424792855978012\n",
      "Epoch 19947/30000 Training Loss: 0.0795605480670929\n",
      "Epoch 19948/30000 Training Loss: 0.06281653791666031\n",
      "Epoch 19949/30000 Training Loss: 0.05509939789772034\n",
      "Epoch 19950/30000 Training Loss: 0.0708395466208458\n",
      "Epoch 19950/30000 Validation Loss: 0.0870404839515686\n",
      "Epoch 19951/30000 Training Loss: 0.08967408537864685\n",
      "Epoch 19952/30000 Training Loss: 0.06597628444433212\n",
      "Epoch 19953/30000 Training Loss: 0.07459230720996857\n",
      "Epoch 19954/30000 Training Loss: 0.061676472425460815\n",
      "Epoch 19955/30000 Training Loss: 0.061239052563905716\n",
      "Epoch 19956/30000 Training Loss: 0.060292478650808334\n",
      "Epoch 19957/30000 Training Loss: 0.07624838501214981\n",
      "Epoch 19958/30000 Training Loss: 0.056557636708021164\n",
      "Epoch 19959/30000 Training Loss: 0.07856520265340805\n",
      "Epoch 19960/30000 Training Loss: 0.09908336400985718\n",
      "Epoch 19960/30000 Validation Loss: 0.06559151411056519\n",
      "Epoch 19961/30000 Training Loss: 0.07333433628082275\n",
      "Epoch 19962/30000 Training Loss: 0.07382726669311523\n",
      "Epoch 19963/30000 Training Loss: 0.06664744019508362\n",
      "Epoch 19964/30000 Training Loss: 0.07477562874555588\n",
      "Epoch 19965/30000 Training Loss: 0.05639499053359032\n",
      "Epoch 19966/30000 Training Loss: 0.0676240622997284\n",
      "Epoch 19967/30000 Training Loss: 0.06217333674430847\n",
      "Epoch 19968/30000 Training Loss: 0.07046728581190109\n",
      "Epoch 19969/30000 Training Loss: 0.06293667107820511\n",
      "Epoch 19970/30000 Training Loss: 0.056223321706056595\n",
      "Epoch 19970/30000 Validation Loss: 0.060241639614105225\n",
      "Epoch 19971/30000 Training Loss: 0.070413738489151\n",
      "Epoch 19972/30000 Training Loss: 0.07079880684614182\n",
      "Epoch 19973/30000 Training Loss: 0.09300091117620468\n",
      "Epoch 19974/30000 Training Loss: 0.061555489897727966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19975/30000 Training Loss: 0.05631740391254425\n",
      "Epoch 19976/30000 Training Loss: 0.06473306566476822\n",
      "Epoch 19977/30000 Training Loss: 0.08252333849668503\n",
      "Epoch 19978/30000 Training Loss: 0.06262309104204178\n",
      "Epoch 19979/30000 Training Loss: 0.07791581749916077\n",
      "Epoch 19980/30000 Training Loss: 0.08425823599100113\n",
      "Epoch 19980/30000 Validation Loss: 0.06575555354356766\n",
      "Epoch 19981/30000 Training Loss: 0.07648755609989166\n",
      "Epoch 19982/30000 Training Loss: 0.08462130278348923\n",
      "Epoch 19983/30000 Training Loss: 0.07289933413267136\n",
      "Epoch 19984/30000 Training Loss: 0.0791970044374466\n",
      "Epoch 19985/30000 Training Loss: 0.06494025886058807\n",
      "Epoch 19986/30000 Training Loss: 0.07842689007520676\n",
      "Epoch 19987/30000 Training Loss: 0.07956021279096603\n",
      "Epoch 19988/30000 Training Loss: 0.06960812211036682\n",
      "Epoch 19989/30000 Training Loss: 0.08346690982580185\n",
      "Epoch 19990/30000 Training Loss: 0.0654965415596962\n",
      "Epoch 19990/30000 Validation Loss: 0.08584180474281311\n",
      "Epoch 19991/30000 Training Loss: 0.06279958039522171\n",
      "Epoch 19992/30000 Training Loss: 0.09003113955259323\n",
      "Epoch 19993/30000 Training Loss: 0.06678268313407898\n",
      "Epoch 19994/30000 Training Loss: 0.05668988823890686\n",
      "Epoch 19995/30000 Training Loss: 0.07280762493610382\n",
      "Epoch 19996/30000 Training Loss: 0.08204977959394455\n",
      "Epoch 19997/30000 Training Loss: 0.05892319977283478\n",
      "Epoch 19998/30000 Training Loss: 0.071832075715065\n",
      "Epoch 19999/30000 Training Loss: 0.0966418981552124\n",
      "Epoch 20000/30000 Training Loss: 0.06400545686483383\n",
      "Epoch 20000/30000 Validation Loss: 0.08232450485229492\n",
      "Epoch 20001/30000 Training Loss: 0.07586348056793213\n",
      "Epoch 20002/30000 Training Loss: 0.07472803443670273\n",
      "Epoch 20003/30000 Training Loss: 0.06223444640636444\n",
      "Epoch 20004/30000 Training Loss: 0.07118936628103256\n",
      "Epoch 20005/30000 Training Loss: 0.07892551273107529\n",
      "Epoch 20006/30000 Training Loss: 0.08717969805002213\n",
      "Epoch 20007/30000 Training Loss: 0.0771242305636406\n",
      "Epoch 20008/30000 Training Loss: 0.07293283194303513\n",
      "Epoch 20009/30000 Training Loss: 0.0764828473329544\n",
      "Epoch 20010/30000 Training Loss: 0.054889947175979614\n",
      "Epoch 20010/30000 Validation Loss: 0.06632557511329651\n",
      "Epoch 20011/30000 Training Loss: 0.0909828469157219\n",
      "Epoch 20012/30000 Training Loss: 0.05165274441242218\n",
      "Epoch 20013/30000 Training Loss: 0.07928340137004852\n",
      "Epoch 20014/30000 Training Loss: 0.05711745098233223\n",
      "Epoch 20015/30000 Training Loss: 0.06467927247285843\n",
      "Epoch 20016/30000 Training Loss: 0.05790451169013977\n",
      "Epoch 20017/30000 Training Loss: 0.07907684892416\n",
      "Epoch 20018/30000 Training Loss: 0.06325171142816544\n",
      "Epoch 20019/30000 Training Loss: 0.08323431015014648\n",
      "Epoch 20020/30000 Training Loss: 0.07164580374956131\n",
      "Epoch 20020/30000 Validation Loss: 0.07341819256544113\n",
      "Epoch 20021/30000 Training Loss: 0.0734313502907753\n",
      "Epoch 20022/30000 Training Loss: 0.06725767999887466\n",
      "Epoch 20023/30000 Training Loss: 0.07235044986009598\n",
      "Epoch 20024/30000 Training Loss: 0.07146690040826797\n",
      "Epoch 20025/30000 Training Loss: 0.07774800807237625\n",
      "Epoch 20026/30000 Training Loss: 0.0723898708820343\n",
      "Epoch 20027/30000 Training Loss: 0.05931497737765312\n",
      "Epoch 20028/30000 Training Loss: 0.07690480351448059\n",
      "Epoch 20029/30000 Training Loss: 0.07708603888750076\n",
      "Epoch 20030/30000 Training Loss: 0.055877018719911575\n",
      "Epoch 20030/30000 Validation Loss: 0.08392136543989182\n",
      "Epoch 20031/30000 Training Loss: 0.07133055478334427\n",
      "Epoch 20032/30000 Training Loss: 0.0663357600569725\n",
      "Epoch 20033/30000 Training Loss: 0.06209638714790344\n",
      "Epoch 20034/30000 Training Loss: 0.07014159113168716\n",
      "Epoch 20035/30000 Training Loss: 0.10438782721757889\n",
      "Epoch 20036/30000 Training Loss: 0.06745680421590805\n",
      "Epoch 20037/30000 Training Loss: 0.07097882777452469\n",
      "Epoch 20038/30000 Training Loss: 0.06831159442663193\n",
      "Epoch 20039/30000 Training Loss: 0.0530862957239151\n",
      "Epoch 20040/30000 Training Loss: 0.09067095071077347\n",
      "Epoch 20040/30000 Validation Loss: 0.06531553715467453\n",
      "Epoch 20041/30000 Training Loss: 0.09417112916707993\n",
      "Epoch 20042/30000 Training Loss: 0.07262369245290756\n",
      "Epoch 20043/30000 Training Loss: 0.06511963158845901\n",
      "Epoch 20044/30000 Training Loss: 0.0642915740609169\n",
      "Epoch 20045/30000 Training Loss: 0.07622049003839493\n",
      "Epoch 20046/30000 Training Loss: 0.08431190997362137\n",
      "Epoch 20047/30000 Training Loss: 0.06099584698677063\n",
      "Epoch 20048/30000 Training Loss: 0.06523218005895615\n",
      "Epoch 20049/30000 Training Loss: 0.06989803165197372\n",
      "Epoch 20050/30000 Training Loss: 0.05428532883524895\n",
      "Epoch 20050/30000 Validation Loss: 0.05444537103176117\n",
      "Epoch 20051/30000 Training Loss: 0.07765979319810867\n",
      "Epoch 20052/30000 Training Loss: 0.07548690587282181\n",
      "Epoch 20053/30000 Training Loss: 0.054323356598615646\n",
      "Epoch 20054/30000 Training Loss: 0.08275791257619858\n",
      "Epoch 20055/30000 Training Loss: 0.06919360905885696\n",
      "Epoch 20056/30000 Training Loss: 0.08033052831888199\n",
      "Epoch 20057/30000 Training Loss: 0.061546534299850464\n",
      "Epoch 20058/30000 Training Loss: 0.07080700993537903\n",
      "Epoch 20059/30000 Training Loss: 0.05202614888548851\n",
      "Epoch 20060/30000 Training Loss: 0.08226233720779419\n",
      "Epoch 20060/30000 Validation Loss: 0.07663781195878983\n",
      "Epoch 20061/30000 Training Loss: 0.08880589157342911\n",
      "Epoch 20062/30000 Training Loss: 0.07205990701913834\n",
      "Epoch 20063/30000 Training Loss: 0.05786804482340813\n",
      "Epoch 20064/30000 Training Loss: 0.06735832244157791\n",
      "Epoch 20065/30000 Training Loss: 0.06485959142446518\n",
      "Epoch 20066/30000 Training Loss: 0.07250907272100449\n",
      "Epoch 20067/30000 Training Loss: 0.062014490365982056\n",
      "Epoch 20068/30000 Training Loss: 0.09039577841758728\n",
      "Epoch 20069/30000 Training Loss: 0.08480355888605118\n",
      "Epoch 20070/30000 Training Loss: 0.0660429298877716\n",
      "Epoch 20070/30000 Validation Loss: 0.08007653802633286\n",
      "Epoch 20071/30000 Training Loss: 0.07028746604919434\n",
      "Epoch 20072/30000 Training Loss: 0.07782834023237228\n",
      "Epoch 20073/30000 Training Loss: 0.0882597267627716\n",
      "Epoch 20074/30000 Training Loss: 0.08389925956726074\n",
      "Epoch 20075/30000 Training Loss: 0.07373562455177307\n",
      "Epoch 20076/30000 Training Loss: 0.07015398889780045\n",
      "Epoch 20077/30000 Training Loss: 0.05864390730857849\n",
      "Epoch 20078/30000 Training Loss: 0.07174110412597656\n",
      "Epoch 20079/30000 Training Loss: 0.07745477557182312\n",
      "Epoch 20080/30000 Training Loss: 0.05958603695034981\n",
      "Epoch 20080/30000 Validation Loss: 0.08517513424158096\n",
      "Epoch 20081/30000 Training Loss: 0.059678394347429276\n",
      "Epoch 20082/30000 Training Loss: 0.10075131803750992\n",
      "Epoch 20083/30000 Training Loss: 0.06450573354959488\n",
      "Epoch 20084/30000 Training Loss: 0.07493256032466888\n",
      "Epoch 20085/30000 Training Loss: 0.0920291543006897\n",
      "Epoch 20086/30000 Training Loss: 0.054554011672735214\n",
      "Epoch 20087/30000 Training Loss: 0.07639727741479874\n",
      "Epoch 20088/30000 Training Loss: 0.08329480141401291\n",
      "Epoch 20089/30000 Training Loss: 0.09198063611984253\n",
      "Epoch 20090/30000 Training Loss: 0.08731047064065933\n",
      "Epoch 20090/30000 Validation Loss: 0.07065077871084213\n",
      "Epoch 20091/30000 Training Loss: 0.055192794650793076\n",
      "Epoch 20092/30000 Training Loss: 0.0776703953742981\n",
      "Epoch 20093/30000 Training Loss: 0.06853093206882477\n",
      "Epoch 20094/30000 Training Loss: 0.07359331846237183\n",
      "Epoch 20095/30000 Training Loss: 0.06614349037408829\n",
      "Epoch 20096/30000 Training Loss: 0.082757867872715\n",
      "Epoch 20097/30000 Training Loss: 0.07769123464822769\n",
      "Epoch 20098/30000 Training Loss: 0.07789469510316849\n",
      "Epoch 20099/30000 Training Loss: 0.0776607096195221\n",
      "Epoch 20100/30000 Training Loss: 0.07477059215307236\n",
      "Epoch 20100/30000 Validation Loss: 0.0772319808602333\n",
      "Epoch 20101/30000 Training Loss: 0.07716701924800873\n",
      "Epoch 20102/30000 Training Loss: 0.07480497658252716\n",
      "Epoch 20103/30000 Training Loss: 0.058838050812482834\n",
      "Epoch 20104/30000 Training Loss: 0.06440833956003189\n",
      "Epoch 20105/30000 Training Loss: 0.09196304529905319\n",
      "Epoch 20106/30000 Training Loss: 0.05955426022410393\n",
      "Epoch 20107/30000 Training Loss: 0.07473691552877426\n",
      "Epoch 20108/30000 Training Loss: 0.08849348872900009\n",
      "Epoch 20109/30000 Training Loss: 0.07727394253015518\n",
      "Epoch 20110/30000 Training Loss: 0.07482177019119263\n",
      "Epoch 20110/30000 Validation Loss: 0.0664854347705841\n",
      "Epoch 20111/30000 Training Loss: 0.07923058420419693\n",
      "Epoch 20112/30000 Training Loss: 0.06654385477304459\n",
      "Epoch 20113/30000 Training Loss: 0.08361700177192688\n",
      "Epoch 20114/30000 Training Loss: 0.058822911232709885\n",
      "Epoch 20115/30000 Training Loss: 0.06518419086933136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20116/30000 Training Loss: 0.06456789374351501\n",
      "Epoch 20117/30000 Training Loss: 0.06173156201839447\n",
      "Epoch 20118/30000 Training Loss: 0.06378578394651413\n",
      "Epoch 20119/30000 Training Loss: 0.05100671947002411\n",
      "Epoch 20120/30000 Training Loss: 0.0883406475186348\n",
      "Epoch 20120/30000 Validation Loss: 0.05852574110031128\n",
      "Epoch 20121/30000 Training Loss: 0.062080156058073044\n",
      "Epoch 20122/30000 Training Loss: 0.06733867526054382\n",
      "Epoch 20123/30000 Training Loss: 0.0825706198811531\n",
      "Epoch 20124/30000 Training Loss: 0.0737399086356163\n",
      "Epoch 20125/30000 Training Loss: 0.0763905942440033\n",
      "Epoch 20126/30000 Training Loss: 0.07038091868162155\n",
      "Epoch 20127/30000 Training Loss: 0.0676620677113533\n",
      "Epoch 20128/30000 Training Loss: 0.07480034977197647\n",
      "Epoch 20129/30000 Training Loss: 0.06190783903002739\n",
      "Epoch 20130/30000 Training Loss: 0.07582362741231918\n",
      "Epoch 20130/30000 Validation Loss: 0.08277275413274765\n",
      "Epoch 20131/30000 Training Loss: 0.06794939190149307\n",
      "Epoch 20132/30000 Training Loss: 0.07493867725133896\n",
      "Epoch 20133/30000 Training Loss: 0.07703987509012222\n",
      "Epoch 20134/30000 Training Loss: 0.0638602003455162\n",
      "Epoch 20135/30000 Training Loss: 0.06484144181013107\n",
      "Epoch 20136/30000 Training Loss: 0.07241647690534592\n",
      "Epoch 20137/30000 Training Loss: 0.08690130710601807\n",
      "Epoch 20138/30000 Training Loss: 0.06666985154151917\n",
      "Epoch 20139/30000 Training Loss: 0.05883242189884186\n",
      "Epoch 20140/30000 Training Loss: 0.07736284285783768\n",
      "Epoch 20140/30000 Validation Loss: 0.07726383209228516\n",
      "Epoch 20141/30000 Training Loss: 0.06962133944034576\n",
      "Epoch 20142/30000 Training Loss: 0.07319571822881699\n",
      "Epoch 20143/30000 Training Loss: 0.057423580437898636\n",
      "Epoch 20144/30000 Training Loss: 0.073734350502491\n",
      "Epoch 20145/30000 Training Loss: 0.08521298319101334\n",
      "Epoch 20146/30000 Training Loss: 0.07913806289434433\n",
      "Epoch 20147/30000 Training Loss: 0.07484965771436691\n",
      "Epoch 20148/30000 Training Loss: 0.0889330729842186\n",
      "Epoch 20149/30000 Training Loss: 0.04991487041115761\n",
      "Epoch 20150/30000 Training Loss: 0.06982894986867905\n",
      "Epoch 20150/30000 Validation Loss: 0.09326664358377457\n",
      "Epoch 20151/30000 Training Loss: 0.082670658826828\n",
      "Epoch 20152/30000 Training Loss: 0.06422629207372665\n",
      "Epoch 20153/30000 Training Loss: 0.074455127120018\n",
      "Epoch 20154/30000 Training Loss: 0.070989228785038\n",
      "Epoch 20155/30000 Training Loss: 0.06918814778327942\n",
      "Epoch 20156/30000 Training Loss: 0.08845392614603043\n",
      "Epoch 20157/30000 Training Loss: 0.07113371044397354\n",
      "Epoch 20158/30000 Training Loss: 0.07836214452981949\n",
      "Epoch 20159/30000 Training Loss: 0.06904133409261703\n",
      "Epoch 20160/30000 Training Loss: 0.0723256766796112\n",
      "Epoch 20160/30000 Validation Loss: 0.0712217390537262\n",
      "Epoch 20161/30000 Training Loss: 0.07387841492891312\n",
      "Epoch 20162/30000 Training Loss: 0.06383965909481049\n",
      "Epoch 20163/30000 Training Loss: 0.05892794206738472\n",
      "Epoch 20164/30000 Training Loss: 0.08577233552932739\n",
      "Epoch 20165/30000 Training Loss: 0.0586908720433712\n",
      "Epoch 20166/30000 Training Loss: 0.08803841471672058\n",
      "Epoch 20167/30000 Training Loss: 0.06375125795602798\n",
      "Epoch 20168/30000 Training Loss: 0.0729982852935791\n",
      "Epoch 20169/30000 Training Loss: 0.06305039674043655\n",
      "Epoch 20170/30000 Training Loss: 0.0738648846745491\n",
      "Epoch 20170/30000 Validation Loss: 0.08156895637512207\n",
      "Epoch 20171/30000 Training Loss: 0.0589648075401783\n",
      "Epoch 20172/30000 Training Loss: 0.0775848850607872\n",
      "Epoch 20173/30000 Training Loss: 0.08123812824487686\n",
      "Epoch 20174/30000 Training Loss: 0.07113045454025269\n",
      "Epoch 20175/30000 Training Loss: 0.07643811404705048\n",
      "Epoch 20176/30000 Training Loss: 0.07046631723642349\n",
      "Epoch 20177/30000 Training Loss: 0.06638147681951523\n",
      "Epoch 20178/30000 Training Loss: 0.06646347045898438\n",
      "Epoch 20179/30000 Training Loss: 0.07589630037546158\n",
      "Epoch 20180/30000 Training Loss: 0.0690518319606781\n",
      "Epoch 20180/30000 Validation Loss: 0.07938162237405777\n",
      "Epoch 20181/30000 Training Loss: 0.07215023785829544\n",
      "Epoch 20182/30000 Training Loss: 0.0758044570684433\n",
      "Epoch 20183/30000 Training Loss: 0.07254122942686081\n",
      "Epoch 20184/30000 Training Loss: 0.06447231769561768\n",
      "Epoch 20185/30000 Training Loss: 0.06166691705584526\n",
      "Epoch 20186/30000 Training Loss: 0.07389212399721146\n",
      "Epoch 20187/30000 Training Loss: 0.09468790143728256\n",
      "Epoch 20188/30000 Training Loss: 0.053855836391448975\n",
      "Epoch 20189/30000 Training Loss: 0.0643235296010971\n",
      "Epoch 20190/30000 Training Loss: 0.07242109626531601\n",
      "Epoch 20190/30000 Validation Loss: 0.082292340695858\n",
      "Epoch 20191/30000 Training Loss: 0.0545232780277729\n",
      "Epoch 20192/30000 Training Loss: 0.08827564865350723\n",
      "Epoch 20193/30000 Training Loss: 0.06427699327468872\n",
      "Epoch 20194/30000 Training Loss: 0.06596583873033524\n",
      "Epoch 20195/30000 Training Loss: 0.06215648353099823\n",
      "Epoch 20196/30000 Training Loss: 0.07555651664733887\n",
      "Epoch 20197/30000 Training Loss: 0.07100564986467361\n",
      "Epoch 20198/30000 Training Loss: 0.05963624641299248\n",
      "Epoch 20199/30000 Training Loss: 0.05320979282259941\n",
      "Epoch 20200/30000 Training Loss: 0.06345498561859131\n",
      "Epoch 20200/30000 Validation Loss: 0.06547192484140396\n",
      "Epoch 20201/30000 Training Loss: 0.08123548328876495\n",
      "Epoch 20202/30000 Training Loss: 0.08194145560264587\n",
      "Epoch 20203/30000 Training Loss: 0.08348309993743896\n",
      "Epoch 20204/30000 Training Loss: 0.07499779015779495\n",
      "Epoch 20205/30000 Training Loss: 0.07287050038576126\n",
      "Epoch 20206/30000 Training Loss: 0.07495862245559692\n",
      "Epoch 20207/30000 Training Loss: 0.0744488313794136\n",
      "Epoch 20208/30000 Training Loss: 0.0664442703127861\n",
      "Epoch 20209/30000 Training Loss: 0.06593518704175949\n",
      "Epoch 20210/30000 Training Loss: 0.0646175667643547\n",
      "Epoch 20210/30000 Validation Loss: 0.10231923311948776\n",
      "Epoch 20211/30000 Training Loss: 0.08432653546333313\n",
      "Epoch 20212/30000 Training Loss: 0.07599439471960068\n",
      "Epoch 20213/30000 Training Loss: 0.08478089421987534\n",
      "Epoch 20214/30000 Training Loss: 0.07681106775999069\n",
      "Epoch 20215/30000 Training Loss: 0.07135587185621262\n",
      "Epoch 20216/30000 Training Loss: 0.06797341257333755\n",
      "Epoch 20217/30000 Training Loss: 0.07461535930633545\n",
      "Epoch 20218/30000 Training Loss: 0.07496175169944763\n",
      "Epoch 20219/30000 Training Loss: 0.08026594668626785\n",
      "Epoch 20220/30000 Training Loss: 0.049215640872716904\n",
      "Epoch 20220/30000 Validation Loss: 0.06837296485900879\n",
      "Epoch 20221/30000 Training Loss: 0.07258063554763794\n",
      "Epoch 20222/30000 Training Loss: 0.08636856079101562\n",
      "Epoch 20223/30000 Training Loss: 0.07243353128433228\n",
      "Epoch 20224/30000 Training Loss: 0.06789936870336533\n",
      "Epoch 20225/30000 Training Loss: 0.0754968449473381\n",
      "Epoch 20226/30000 Training Loss: 0.0563579685986042\n",
      "Epoch 20227/30000 Training Loss: 0.07242899388074875\n",
      "Epoch 20228/30000 Training Loss: 0.07906248420476913\n",
      "Epoch 20229/30000 Training Loss: 0.06059039756655693\n",
      "Epoch 20230/30000 Training Loss: 0.058428630232810974\n",
      "Epoch 20230/30000 Validation Loss: 0.07671775668859482\n",
      "Epoch 20231/30000 Training Loss: 0.06428304314613342\n",
      "Epoch 20232/30000 Training Loss: 0.07276815176010132\n",
      "Epoch 20233/30000 Training Loss: 0.0768447294831276\n",
      "Epoch 20234/30000 Training Loss: 0.08810830116271973\n",
      "Epoch 20235/30000 Training Loss: 0.10770049691200256\n",
      "Epoch 20236/30000 Training Loss: 0.0746699795126915\n",
      "Epoch 20237/30000 Training Loss: 0.06344414502382278\n",
      "Epoch 20238/30000 Training Loss: 0.06656847149133682\n",
      "Epoch 20239/30000 Training Loss: 0.0644376203417778\n",
      "Epoch 20240/30000 Training Loss: 0.09192326664924622\n",
      "Epoch 20240/30000 Validation Loss: 0.06415044516324997\n",
      "Epoch 20241/30000 Training Loss: 0.05488559976220131\n",
      "Epoch 20242/30000 Training Loss: 0.05827254056930542\n",
      "Epoch 20243/30000 Training Loss: 0.08897080272436142\n",
      "Epoch 20244/30000 Training Loss: 0.0639968141913414\n",
      "Epoch 20245/30000 Training Loss: 0.08823704719543457\n",
      "Epoch 20246/30000 Training Loss: 0.09082547575235367\n",
      "Epoch 20247/30000 Training Loss: 0.06236377730965614\n",
      "Epoch 20248/30000 Training Loss: 0.06602772325277328\n",
      "Epoch 20249/30000 Training Loss: 0.06474116444587708\n",
      "Epoch 20250/30000 Training Loss: 0.06433544307947159\n",
      "Epoch 20250/30000 Validation Loss: 0.08927339315414429\n",
      "Epoch 20251/30000 Training Loss: 0.05604250356554985\n",
      "Epoch 20252/30000 Training Loss: 0.08910205960273743\n",
      "Epoch 20253/30000 Training Loss: 0.0764932706952095\n",
      "Epoch 20254/30000 Training Loss: 0.057670608162879944\n",
      "Epoch 20255/30000 Training Loss: 0.07297904789447784\n",
      "Epoch 20256/30000 Training Loss: 0.09188226610422134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20257/30000 Training Loss: 0.058788519352674484\n",
      "Epoch 20258/30000 Training Loss: 0.07546210289001465\n",
      "Epoch 20259/30000 Training Loss: 0.07207139581441879\n",
      "Epoch 20260/30000 Training Loss: 0.057434096932411194\n",
      "Epoch 20260/30000 Validation Loss: 0.08641939610242844\n",
      "Epoch 20261/30000 Training Loss: 0.07443013042211533\n",
      "Epoch 20262/30000 Training Loss: 0.08993270248174667\n",
      "Epoch 20263/30000 Training Loss: 0.06502586603164673\n",
      "Epoch 20264/30000 Training Loss: 0.07849166542291641\n",
      "Epoch 20265/30000 Training Loss: 0.08340194821357727\n",
      "Epoch 20266/30000 Training Loss: 0.06173918768763542\n",
      "Epoch 20267/30000 Training Loss: 0.07595960050821304\n",
      "Epoch 20268/30000 Training Loss: 0.07023809105157852\n",
      "Epoch 20269/30000 Training Loss: 0.07521773874759674\n",
      "Epoch 20270/30000 Training Loss: 0.07946750521659851\n",
      "Epoch 20270/30000 Validation Loss: 0.0654081329703331\n",
      "Epoch 20271/30000 Training Loss: 0.05836892127990723\n",
      "Epoch 20272/30000 Training Loss: 0.07117044180631638\n",
      "Epoch 20273/30000 Training Loss: 0.07926090806722641\n",
      "Epoch 20274/30000 Training Loss: 0.07874161005020142\n",
      "Epoch 20275/30000 Training Loss: 0.05902909114956856\n",
      "Epoch 20276/30000 Training Loss: 0.06174825504422188\n",
      "Epoch 20277/30000 Training Loss: 0.08049318194389343\n",
      "Epoch 20278/30000 Training Loss: 0.06890513747930527\n",
      "Epoch 20279/30000 Training Loss: 0.07234559208154678\n",
      "Epoch 20280/30000 Training Loss: 0.08469850569963455\n",
      "Epoch 20280/30000 Validation Loss: 0.075489841401577\n",
      "Epoch 20281/30000 Training Loss: 0.08658036589622498\n",
      "Epoch 20282/30000 Training Loss: 0.07648178935050964\n",
      "Epoch 20283/30000 Training Loss: 0.08489086478948593\n",
      "Epoch 20284/30000 Training Loss: 0.054901231080293655\n",
      "Epoch 20285/30000 Training Loss: 0.0741518959403038\n",
      "Epoch 20286/30000 Training Loss: 0.06260348856449127\n",
      "Epoch 20287/30000 Training Loss: 0.08559376001358032\n",
      "Epoch 20288/30000 Training Loss: 0.0704248696565628\n",
      "Epoch 20289/30000 Training Loss: 0.061771731823682785\n",
      "Epoch 20290/30000 Training Loss: 0.08161730319261551\n",
      "Epoch 20290/30000 Validation Loss: 0.06760165840387344\n",
      "Epoch 20291/30000 Training Loss: 0.07342205196619034\n",
      "Epoch 20292/30000 Training Loss: 0.06222274899482727\n",
      "Epoch 20293/30000 Training Loss: 0.06540755927562714\n",
      "Epoch 20294/30000 Training Loss: 0.059907566756010056\n",
      "Epoch 20295/30000 Training Loss: 0.07713352888822556\n",
      "Epoch 20296/30000 Training Loss: 0.06426244974136353\n",
      "Epoch 20297/30000 Training Loss: 0.07738711684942245\n",
      "Epoch 20298/30000 Training Loss: 0.07370155304670334\n",
      "Epoch 20299/30000 Training Loss: 0.08447787165641785\n",
      "Epoch 20300/30000 Training Loss: 0.06530367583036423\n",
      "Epoch 20300/30000 Validation Loss: 0.0866454467177391\n",
      "Epoch 20301/30000 Training Loss: 0.08554401248693466\n",
      "Epoch 20302/30000 Training Loss: 0.05806554853916168\n",
      "Epoch 20303/30000 Training Loss: 0.058924537152051926\n",
      "Epoch 20304/30000 Training Loss: 0.0959632396697998\n",
      "Epoch 20305/30000 Training Loss: 0.08137612789869308\n",
      "Epoch 20306/30000 Training Loss: 0.054046716541051865\n",
      "Epoch 20307/30000 Training Loss: 0.06015884503722191\n",
      "Epoch 20308/30000 Training Loss: 0.0673818364739418\n",
      "Epoch 20309/30000 Training Loss: 0.06014041230082512\n",
      "Epoch 20310/30000 Training Loss: 0.06799054145812988\n",
      "Epoch 20310/30000 Validation Loss: 0.07541748881340027\n",
      "Epoch 20311/30000 Training Loss: 0.06969908624887466\n",
      "Epoch 20312/30000 Training Loss: 0.07340659946203232\n",
      "Epoch 20313/30000 Training Loss: 0.07119158655405045\n",
      "Epoch 20314/30000 Training Loss: 0.07774261385202408\n",
      "Epoch 20315/30000 Training Loss: 0.06700026243925095\n",
      "Epoch 20316/30000 Training Loss: 0.06911002099514008\n",
      "Epoch 20317/30000 Training Loss: 0.07922057062387466\n",
      "Epoch 20318/30000 Training Loss: 0.05449545755982399\n",
      "Epoch 20319/30000 Training Loss: 0.07667823880910873\n",
      "Epoch 20320/30000 Training Loss: 0.07407588511705399\n",
      "Epoch 20320/30000 Validation Loss: 0.06632620096206665\n",
      "Epoch 20321/30000 Training Loss: 0.06168252229690552\n",
      "Epoch 20322/30000 Training Loss: 0.07324504107236862\n",
      "Epoch 20323/30000 Training Loss: 0.07074597477912903\n",
      "Epoch 20324/30000 Training Loss: 0.08874122053384781\n",
      "Epoch 20325/30000 Training Loss: 0.07191787660121918\n",
      "Epoch 20326/30000 Training Loss: 0.07007160782814026\n",
      "Epoch 20327/30000 Training Loss: 0.07325074821710587\n",
      "Epoch 20328/30000 Training Loss: 0.06858932226896286\n",
      "Epoch 20329/30000 Training Loss: 0.07071075588464737\n",
      "Epoch 20330/30000 Training Loss: 0.059326767921447754\n",
      "Epoch 20330/30000 Validation Loss: 0.06973516941070557\n",
      "Epoch 20331/30000 Training Loss: 0.07638870179653168\n",
      "Epoch 20332/30000 Training Loss: 0.07228651642799377\n",
      "Epoch 20333/30000 Training Loss: 0.06829016655683517\n",
      "Epoch 20334/30000 Training Loss: 0.06917863339185715\n",
      "Epoch 20335/30000 Training Loss: 0.06535512208938599\n",
      "Epoch 20336/30000 Training Loss: 0.062020476907491684\n",
      "Epoch 20337/30000 Training Loss: 0.06105073168873787\n",
      "Epoch 20338/30000 Training Loss: 0.05944564938545227\n",
      "Epoch 20339/30000 Training Loss: 0.06425558775663376\n",
      "Epoch 20340/30000 Training Loss: 0.08414900302886963\n",
      "Epoch 20340/30000 Validation Loss: 0.0690217912197113\n",
      "Epoch 20341/30000 Training Loss: 0.07000661641359329\n",
      "Epoch 20342/30000 Training Loss: 0.05883897468447685\n",
      "Epoch 20343/30000 Training Loss: 0.07049357891082764\n",
      "Epoch 20344/30000 Training Loss: 0.06259401887655258\n",
      "Epoch 20345/30000 Training Loss: 0.0703757181763649\n",
      "Epoch 20346/30000 Training Loss: 0.05742979049682617\n",
      "Epoch 20347/30000 Training Loss: 0.09132647514343262\n",
      "Epoch 20348/30000 Training Loss: 0.055778682231903076\n",
      "Epoch 20349/30000 Training Loss: 0.06465063244104385\n",
      "Epoch 20350/30000 Training Loss: 0.06907743215560913\n",
      "Epoch 20350/30000 Validation Loss: 0.07379648089408875\n",
      "Epoch 20351/30000 Training Loss: 0.06704279780387878\n",
      "Epoch 20352/30000 Training Loss: 0.07191886752843857\n",
      "Epoch 20353/30000 Training Loss: 0.0661541298031807\n",
      "Epoch 20354/30000 Training Loss: 0.05308064818382263\n",
      "Epoch 20355/30000 Training Loss: 0.07775673270225525\n",
      "Epoch 20356/30000 Training Loss: 0.07015145570039749\n",
      "Epoch 20357/30000 Training Loss: 0.08014794439077377\n",
      "Epoch 20358/30000 Training Loss: 0.06571374833583832\n",
      "Epoch 20359/30000 Training Loss: 0.0756569430232048\n",
      "Epoch 20360/30000 Training Loss: 0.07890234142541885\n",
      "Epoch 20360/30000 Validation Loss: 0.06695545464754105\n",
      "Epoch 20361/30000 Training Loss: 0.07201644778251648\n",
      "Epoch 20362/30000 Training Loss: 0.07522932440042496\n",
      "Epoch 20363/30000 Training Loss: 0.0667329654097557\n",
      "Epoch 20364/30000 Training Loss: 0.0754832848906517\n",
      "Epoch 20365/30000 Training Loss: 0.05774470046162605\n",
      "Epoch 20366/30000 Training Loss: 0.07518312335014343\n",
      "Epoch 20367/30000 Training Loss: 0.07180757075548172\n",
      "Epoch 20368/30000 Training Loss: 0.08643727749586105\n",
      "Epoch 20369/30000 Training Loss: 0.06859997659921646\n",
      "Epoch 20370/30000 Training Loss: 0.06820536404848099\n",
      "Epoch 20370/30000 Validation Loss: 0.06850221008062363\n",
      "Epoch 20371/30000 Training Loss: 0.06645223498344421\n",
      "Epoch 20372/30000 Training Loss: 0.06111685559153557\n",
      "Epoch 20373/30000 Training Loss: 0.04946942999958992\n",
      "Epoch 20374/30000 Training Loss: 0.0717073306441307\n",
      "Epoch 20375/30000 Training Loss: 0.08089283853769302\n",
      "Epoch 20376/30000 Training Loss: 0.0871252790093422\n",
      "Epoch 20377/30000 Training Loss: 0.0606401264667511\n",
      "Epoch 20378/30000 Training Loss: 0.06986626982688904\n",
      "Epoch 20379/30000 Training Loss: 0.06292909383773804\n",
      "Epoch 20380/30000 Training Loss: 0.06655550003051758\n",
      "Epoch 20380/30000 Validation Loss: 0.06901726871728897\n",
      "Epoch 20381/30000 Training Loss: 0.08979731053113937\n",
      "Epoch 20382/30000 Training Loss: 0.06940252333879471\n",
      "Epoch 20383/30000 Training Loss: 0.06960467994213104\n",
      "Epoch 20384/30000 Training Loss: 0.089004285633564\n",
      "Epoch 20385/30000 Training Loss: 0.0737646147608757\n",
      "Epoch 20386/30000 Training Loss: 0.06807804852724075\n",
      "Epoch 20387/30000 Training Loss: 0.06403405964374542\n",
      "Epoch 20388/30000 Training Loss: 0.06756457686424255\n",
      "Epoch 20389/30000 Training Loss: 0.060493677854537964\n",
      "Epoch 20390/30000 Training Loss: 0.061972394585609436\n",
      "Epoch 20390/30000 Validation Loss: 0.05580168962478638\n",
      "Epoch 20391/30000 Training Loss: 0.055413320660591125\n",
      "Epoch 20392/30000 Training Loss: 0.0777435153722763\n",
      "Epoch 20393/30000 Training Loss: 0.08016779273748398\n",
      "Epoch 20394/30000 Training Loss: 0.08244822174310684\n",
      "Epoch 20395/30000 Training Loss: 0.08039846271276474\n",
      "Epoch 20396/30000 Training Loss: 0.0742909237742424\n",
      "Epoch 20397/30000 Training Loss: 0.06408592313528061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20398/30000 Training Loss: 0.06366944313049316\n",
      "Epoch 20399/30000 Training Loss: 0.078647680580616\n",
      "Epoch 20400/30000 Training Loss: 0.08165840059518814\n",
      "Epoch 20400/30000 Validation Loss: 0.09280749410390854\n",
      "Epoch 20401/30000 Training Loss: 0.05978560447692871\n",
      "Epoch 20402/30000 Training Loss: 0.0640193298459053\n",
      "Epoch 20403/30000 Training Loss: 0.09185394644737244\n",
      "Epoch 20404/30000 Training Loss: 0.056533824652433395\n",
      "Epoch 20405/30000 Training Loss: 0.08940543979406357\n",
      "Epoch 20406/30000 Training Loss: 0.06035557761788368\n",
      "Epoch 20407/30000 Training Loss: 0.057966578751802444\n",
      "Epoch 20408/30000 Training Loss: 0.07001286745071411\n",
      "Epoch 20409/30000 Training Loss: 0.0856359601020813\n",
      "Epoch 20410/30000 Training Loss: 0.08169957995414734\n",
      "Epoch 20410/30000 Validation Loss: 0.06997273117303848\n",
      "Epoch 20411/30000 Training Loss: 0.0672580674290657\n",
      "Epoch 20412/30000 Training Loss: 0.07001426815986633\n",
      "Epoch 20413/30000 Training Loss: 0.05932103469967842\n",
      "Epoch 20414/30000 Training Loss: 0.09920746833086014\n",
      "Epoch 20415/30000 Training Loss: 0.09406336396932602\n",
      "Epoch 20416/30000 Training Loss: 0.0682167187333107\n",
      "Epoch 20417/30000 Training Loss: 0.06011951342225075\n",
      "Epoch 20418/30000 Training Loss: 0.06166684627532959\n",
      "Epoch 20419/30000 Training Loss: 0.06452800333499908\n",
      "Epoch 20420/30000 Training Loss: 0.09877733141183853\n",
      "Epoch 20420/30000 Validation Loss: 0.06479741632938385\n",
      "Epoch 20421/30000 Training Loss: 0.07760421186685562\n",
      "Epoch 20422/30000 Training Loss: 0.07648802548646927\n",
      "Epoch 20423/30000 Training Loss: 0.0473521463572979\n",
      "Epoch 20424/30000 Training Loss: 0.07644232362508774\n",
      "Epoch 20425/30000 Training Loss: 0.0769495740532875\n",
      "Epoch 20426/30000 Training Loss: 0.07440832257270813\n",
      "Epoch 20427/30000 Training Loss: 0.06908776611089706\n",
      "Epoch 20428/30000 Training Loss: 0.07234909385442734\n",
      "Epoch 20429/30000 Training Loss: 0.0697951391339302\n",
      "Epoch 20430/30000 Training Loss: 0.06720267981290817\n",
      "Epoch 20430/30000 Validation Loss: 0.08052881062030792\n",
      "Epoch 20431/30000 Training Loss: 0.07221979647874832\n",
      "Epoch 20432/30000 Training Loss: 0.05725645646452904\n",
      "Epoch 20433/30000 Training Loss: 0.09301390498876572\n",
      "Epoch 20434/30000 Training Loss: 0.07758162170648575\n",
      "Epoch 20435/30000 Training Loss: 0.08817429095506668\n",
      "Epoch 20436/30000 Training Loss: 0.07949349284172058\n",
      "Epoch 20437/30000 Training Loss: 0.05905742570757866\n",
      "Epoch 20438/30000 Training Loss: 0.06584512442350388\n",
      "Epoch 20439/30000 Training Loss: 0.08991692215204239\n",
      "Epoch 20440/30000 Training Loss: 0.06905082613229752\n",
      "Epoch 20440/30000 Validation Loss: 0.06980516761541367\n",
      "Epoch 20441/30000 Training Loss: 0.07756941765546799\n",
      "Epoch 20442/30000 Training Loss: 0.07923874258995056\n",
      "Epoch 20443/30000 Training Loss: 0.08329407125711441\n",
      "Epoch 20444/30000 Training Loss: 0.07497536391019821\n",
      "Epoch 20445/30000 Training Loss: 0.05930037423968315\n",
      "Epoch 20446/30000 Training Loss: 0.061250824481248856\n",
      "Epoch 20447/30000 Training Loss: 0.07755943387746811\n",
      "Epoch 20448/30000 Training Loss: 0.0688936710357666\n",
      "Epoch 20449/30000 Training Loss: 0.08931571245193481\n",
      "Epoch 20450/30000 Training Loss: 0.08063510805368423\n",
      "Epoch 20450/30000 Validation Loss: 0.07807081192731857\n",
      "Epoch 20451/30000 Training Loss: 0.062397558242082596\n",
      "Epoch 20452/30000 Training Loss: 0.08075525611639023\n",
      "Epoch 20453/30000 Training Loss: 0.06983412802219391\n",
      "Epoch 20454/30000 Training Loss: 0.07266726344823837\n",
      "Epoch 20455/30000 Training Loss: 0.05483867600560188\n",
      "Epoch 20456/30000 Training Loss: 0.06749852746725082\n",
      "Epoch 20457/30000 Training Loss: 0.06448867172002792\n",
      "Epoch 20458/30000 Training Loss: 0.05739313364028931\n",
      "Epoch 20459/30000 Training Loss: 0.055328141897916794\n",
      "Epoch 20460/30000 Training Loss: 0.07314728200435638\n",
      "Epoch 20460/30000 Validation Loss: 0.06735855340957642\n",
      "Epoch 20461/30000 Training Loss: 0.05411636829376221\n",
      "Epoch 20462/30000 Training Loss: 0.06750594824552536\n",
      "Epoch 20463/30000 Training Loss: 0.05891338363289833\n",
      "Epoch 20464/30000 Training Loss: 0.06589708477258682\n",
      "Epoch 20465/30000 Training Loss: 0.08920294046401978\n",
      "Epoch 20466/30000 Training Loss: 0.08186269551515579\n",
      "Epoch 20467/30000 Training Loss: 0.07166769355535507\n",
      "Epoch 20468/30000 Training Loss: 0.0662095844745636\n",
      "Epoch 20469/30000 Training Loss: 0.0634254515171051\n",
      "Epoch 20470/30000 Training Loss: 0.07790705561637878\n",
      "Epoch 20470/30000 Validation Loss: 0.08404701948165894\n",
      "Epoch 20471/30000 Training Loss: 0.0747564360499382\n",
      "Epoch 20472/30000 Training Loss: 0.07230634242296219\n",
      "Epoch 20473/30000 Training Loss: 0.04801816865801811\n",
      "Epoch 20474/30000 Training Loss: 0.09325078874826431\n",
      "Epoch 20475/30000 Training Loss: 0.08054623007774353\n",
      "Epoch 20476/30000 Training Loss: 0.0717085748910904\n",
      "Epoch 20477/30000 Training Loss: 0.05726827308535576\n",
      "Epoch 20478/30000 Training Loss: 0.06114080548286438\n",
      "Epoch 20479/30000 Training Loss: 0.0814816877245903\n",
      "Epoch 20480/30000 Training Loss: 0.07936284691095352\n",
      "Epoch 20480/30000 Validation Loss: 0.07781008630990982\n",
      "Epoch 20481/30000 Training Loss: 0.07286997139453888\n",
      "Epoch 20482/30000 Training Loss: 0.06936321407556534\n",
      "Epoch 20483/30000 Training Loss: 0.0603412389755249\n",
      "Epoch 20484/30000 Training Loss: 0.06632795184850693\n",
      "Epoch 20485/30000 Training Loss: 0.09246202558279037\n",
      "Epoch 20486/30000 Training Loss: 0.0827481746673584\n",
      "Epoch 20487/30000 Training Loss: 0.0577746219933033\n",
      "Epoch 20488/30000 Training Loss: 0.07573508471250534\n",
      "Epoch 20489/30000 Training Loss: 0.07626104354858398\n",
      "Epoch 20490/30000 Training Loss: 0.07057680934667587\n",
      "Epoch 20490/30000 Validation Loss: 0.0777326226234436\n",
      "Epoch 20491/30000 Training Loss: 0.06271471828222275\n",
      "Epoch 20492/30000 Training Loss: 0.07919836789369583\n",
      "Epoch 20493/30000 Training Loss: 0.05560871958732605\n",
      "Epoch 20494/30000 Training Loss: 0.062267377972602844\n",
      "Epoch 20495/30000 Training Loss: 0.05399123951792717\n",
      "Epoch 20496/30000 Training Loss: 0.08539453148841858\n",
      "Epoch 20497/30000 Training Loss: 0.07000754028558731\n",
      "Epoch 20498/30000 Training Loss: 0.062499821186065674\n",
      "Epoch 20499/30000 Training Loss: 0.06352388113737106\n",
      "Epoch 20500/30000 Training Loss: 0.06761675328016281\n",
      "Epoch 20500/30000 Validation Loss: 0.05442465469241142\n",
      "Epoch 20501/30000 Training Loss: 0.07929996401071548\n",
      "Epoch 20502/30000 Training Loss: 0.061840251088142395\n",
      "Epoch 20503/30000 Training Loss: 0.08143087476491928\n",
      "Epoch 20504/30000 Training Loss: 0.06669893860816956\n",
      "Epoch 20505/30000 Training Loss: 0.06672655045986176\n",
      "Epoch 20506/30000 Training Loss: 0.05917537212371826\n",
      "Epoch 20507/30000 Training Loss: 0.08070505410432816\n",
      "Epoch 20508/30000 Training Loss: 0.08813578635454178\n",
      "Epoch 20509/30000 Training Loss: 0.06337850540876389\n",
      "Epoch 20510/30000 Training Loss: 0.07594732195138931\n",
      "Epoch 20510/30000 Validation Loss: 0.0754503682255745\n",
      "Epoch 20511/30000 Training Loss: 0.0816015899181366\n",
      "Epoch 20512/30000 Training Loss: 0.07355931401252747\n",
      "Epoch 20513/30000 Training Loss: 0.05997118726372719\n",
      "Epoch 20514/30000 Training Loss: 0.0669161006808281\n",
      "Epoch 20515/30000 Training Loss: 0.07067445665597916\n",
      "Epoch 20516/30000 Training Loss: 0.07200144231319427\n",
      "Epoch 20517/30000 Training Loss: 0.06708832085132599\n",
      "Epoch 20518/30000 Training Loss: 0.07879944145679474\n",
      "Epoch 20519/30000 Training Loss: 0.07634119689464569\n",
      "Epoch 20520/30000 Training Loss: 0.069822758436203\n",
      "Epoch 20520/30000 Validation Loss: 0.08458145707845688\n",
      "Epoch 20521/30000 Training Loss: 0.07527120411396027\n",
      "Epoch 20522/30000 Training Loss: 0.06629740446805954\n",
      "Epoch 20523/30000 Training Loss: 0.07791035622358322\n",
      "Epoch 20524/30000 Training Loss: 0.0698462501168251\n",
      "Epoch 20525/30000 Training Loss: 0.06748829036951065\n",
      "Epoch 20526/30000 Training Loss: 0.08491344004869461\n",
      "Epoch 20527/30000 Training Loss: 0.07512170076370239\n",
      "Epoch 20528/30000 Training Loss: 0.07737612724304199\n",
      "Epoch 20529/30000 Training Loss: 0.08841124922037125\n",
      "Epoch 20530/30000 Training Loss: 0.06518339365720749\n",
      "Epoch 20530/30000 Validation Loss: 0.07735645771026611\n",
      "Epoch 20531/30000 Training Loss: 0.07737183570861816\n",
      "Epoch 20532/30000 Training Loss: 0.07912855595350266\n",
      "Epoch 20533/30000 Training Loss: 0.06436917930841446\n",
      "Epoch 20534/30000 Training Loss: 0.06216328218579292\n",
      "Epoch 20535/30000 Training Loss: 0.06790995597839355\n",
      "Epoch 20536/30000 Training Loss: 0.07275775820016861\n",
      "Epoch 20537/30000 Training Loss: 0.05405941233038902\n",
      "Epoch 20538/30000 Training Loss: 0.0702160969376564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20539/30000 Training Loss: 0.06752253323793411\n",
      "Epoch 20540/30000 Training Loss: 0.06344843655824661\n",
      "Epoch 20540/30000 Validation Loss: 0.06444334238767624\n",
      "Epoch 20541/30000 Training Loss: 0.06128045544028282\n",
      "Epoch 20542/30000 Training Loss: 0.07912945747375488\n",
      "Epoch 20543/30000 Training Loss: 0.06741079688072205\n",
      "Epoch 20544/30000 Training Loss: 0.06880145519971848\n",
      "Epoch 20545/30000 Training Loss: 0.06404600292444229\n",
      "Epoch 20546/30000 Training Loss: 0.08861354738473892\n",
      "Epoch 20547/30000 Training Loss: 0.08112756907939911\n",
      "Epoch 20548/30000 Training Loss: 0.08184980601072311\n",
      "Epoch 20549/30000 Training Loss: 0.06403090804815292\n",
      "Epoch 20550/30000 Training Loss: 0.07820519059896469\n",
      "Epoch 20550/30000 Validation Loss: 0.06481018662452698\n",
      "Epoch 20551/30000 Training Loss: 0.07234295457601547\n",
      "Epoch 20552/30000 Training Loss: 0.09848463535308838\n",
      "Epoch 20553/30000 Training Loss: 0.05761000141501427\n",
      "Epoch 20554/30000 Training Loss: 0.08076872676610947\n",
      "Epoch 20555/30000 Training Loss: 0.0700746700167656\n",
      "Epoch 20556/30000 Training Loss: 0.06907646358013153\n",
      "Epoch 20557/30000 Training Loss: 0.07915938645601273\n",
      "Epoch 20558/30000 Training Loss: 0.08202473074197769\n",
      "Epoch 20559/30000 Training Loss: 0.06652481108903885\n",
      "Epoch 20560/30000 Training Loss: 0.05638483166694641\n",
      "Epoch 20560/30000 Validation Loss: 0.06834088265895844\n",
      "Epoch 20561/30000 Training Loss: 0.06383200734853745\n",
      "Epoch 20562/30000 Training Loss: 0.07398951798677444\n",
      "Epoch 20563/30000 Training Loss: 0.08083516359329224\n",
      "Epoch 20564/30000 Training Loss: 0.07984448224306107\n",
      "Epoch 20565/30000 Training Loss: 0.09045317023992538\n",
      "Epoch 20566/30000 Training Loss: 0.08053693175315857\n",
      "Epoch 20567/30000 Training Loss: 0.07016092538833618\n",
      "Epoch 20568/30000 Training Loss: 0.08422639966011047\n",
      "Epoch 20569/30000 Training Loss: 0.06983184069395065\n",
      "Epoch 20570/30000 Training Loss: 0.06428409367799759\n",
      "Epoch 20570/30000 Validation Loss: 0.09004684537649155\n",
      "Epoch 20571/30000 Training Loss: 0.06644249707460403\n",
      "Epoch 20572/30000 Training Loss: 0.09157561510801315\n",
      "Epoch 20573/30000 Training Loss: 0.06454090774059296\n",
      "Epoch 20574/30000 Training Loss: 0.06644943356513977\n",
      "Epoch 20575/30000 Training Loss: 0.05987098440527916\n",
      "Epoch 20576/30000 Training Loss: 0.09210825711488724\n",
      "Epoch 20577/30000 Training Loss: 0.06848985701799393\n",
      "Epoch 20578/30000 Training Loss: 0.04889911040663719\n",
      "Epoch 20579/30000 Training Loss: 0.06900879740715027\n",
      "Epoch 20580/30000 Training Loss: 0.07334964722394943\n",
      "Epoch 20580/30000 Validation Loss: 0.07530126720666885\n",
      "Epoch 20581/30000 Training Loss: 0.07911305874586105\n",
      "Epoch 20582/30000 Training Loss: 0.07427141070365906\n",
      "Epoch 20583/30000 Training Loss: 0.06515825539827347\n",
      "Epoch 20584/30000 Training Loss: 0.08354976028203964\n",
      "Epoch 20585/30000 Training Loss: 0.06030368432402611\n",
      "Epoch 20586/30000 Training Loss: 0.06468378752470016\n",
      "Epoch 20587/30000 Training Loss: 0.06559314578771591\n",
      "Epoch 20588/30000 Training Loss: 0.08377325534820557\n",
      "Epoch 20589/30000 Training Loss: 0.08199987560510635\n",
      "Epoch 20590/30000 Training Loss: 0.08039509505033493\n",
      "Epoch 20590/30000 Validation Loss: 0.07300526648759842\n",
      "Epoch 20591/30000 Training Loss: 0.07989335060119629\n",
      "Epoch 20592/30000 Training Loss: 0.06383884698152542\n",
      "Epoch 20593/30000 Training Loss: 0.05589401349425316\n",
      "Epoch 20594/30000 Training Loss: 0.0695020779967308\n",
      "Epoch 20595/30000 Training Loss: 0.07921076565980911\n",
      "Epoch 20596/30000 Training Loss: 0.06779617816209793\n",
      "Epoch 20597/30000 Training Loss: 0.06353116780519485\n",
      "Epoch 20598/30000 Training Loss: 0.05970747396349907\n",
      "Epoch 20599/30000 Training Loss: 0.06519812345504761\n",
      "Epoch 20600/30000 Training Loss: 0.08987484127283096\n",
      "Epoch 20600/30000 Validation Loss: 0.09113645553588867\n",
      "Epoch 20601/30000 Training Loss: 0.06651335209608078\n",
      "Epoch 20602/30000 Training Loss: 0.08521240949630737\n",
      "Epoch 20603/30000 Training Loss: 0.07329016178846359\n",
      "Epoch 20604/30000 Training Loss: 0.06388254463672638\n",
      "Epoch 20605/30000 Training Loss: 0.07540488243103027\n",
      "Epoch 20606/30000 Training Loss: 0.056144773960113525\n",
      "Epoch 20607/30000 Training Loss: 0.06231415644288063\n",
      "Epoch 20608/30000 Training Loss: 0.060673490166664124\n",
      "Epoch 20609/30000 Training Loss: 0.0543394535779953\n",
      "Epoch 20610/30000 Training Loss: 0.055885594338178635\n",
      "Epoch 20610/30000 Validation Loss: 0.08296539634466171\n",
      "Epoch 20611/30000 Training Loss: 0.06516694277524948\n",
      "Epoch 20612/30000 Training Loss: 0.07285173237323761\n",
      "Epoch 20613/30000 Training Loss: 0.06469705700874329\n",
      "Epoch 20614/30000 Training Loss: 0.04634168744087219\n",
      "Epoch 20615/30000 Training Loss: 0.07105173915624619\n",
      "Epoch 20616/30000 Training Loss: 0.05994076654314995\n",
      "Epoch 20617/30000 Training Loss: 0.07250171154737473\n",
      "Epoch 20618/30000 Training Loss: 0.06242826581001282\n",
      "Epoch 20619/30000 Training Loss: 0.09746304899454117\n",
      "Epoch 20620/30000 Training Loss: 0.07140891999006271\n",
      "Epoch 20620/30000 Validation Loss: 0.0637751892209053\n",
      "Epoch 20621/30000 Training Loss: 0.07570794969797134\n",
      "Epoch 20622/30000 Training Loss: 0.07095586508512497\n",
      "Epoch 20623/30000 Training Loss: 0.05431845784187317\n",
      "Epoch 20624/30000 Training Loss: 0.061202067881822586\n",
      "Epoch 20625/30000 Training Loss: 0.07271117717027664\n",
      "Epoch 20626/30000 Training Loss: 0.06377245485782623\n",
      "Epoch 20627/30000 Training Loss: 0.0778375044465065\n",
      "Epoch 20628/30000 Training Loss: 0.059900593012571335\n",
      "Epoch 20629/30000 Training Loss: 0.06857559829950333\n",
      "Epoch 20630/30000 Training Loss: 0.07745363563299179\n",
      "Epoch 20630/30000 Validation Loss: 0.07623007148504257\n",
      "Epoch 20631/30000 Training Loss: 0.054937735199928284\n",
      "Epoch 20632/30000 Training Loss: 0.07647425681352615\n",
      "Epoch 20633/30000 Training Loss: 0.06468868255615234\n",
      "Epoch 20634/30000 Training Loss: 0.07198024541139603\n",
      "Epoch 20635/30000 Training Loss: 0.06283333152532578\n",
      "Epoch 20636/30000 Training Loss: 0.05683373287320137\n",
      "Epoch 20637/30000 Training Loss: 0.07162254303693771\n",
      "Epoch 20638/30000 Training Loss: 0.070221908390522\n",
      "Epoch 20639/30000 Training Loss: 0.07059940695762634\n",
      "Epoch 20640/30000 Training Loss: 0.056335389614105225\n",
      "Epoch 20640/30000 Validation Loss: 0.06618618220090866\n",
      "Epoch 20641/30000 Training Loss: 0.06847864389419556\n",
      "Epoch 20642/30000 Training Loss: 0.08537224680185318\n",
      "Epoch 20643/30000 Training Loss: 0.0622190423309803\n",
      "Epoch 20644/30000 Training Loss: 0.07283372431993484\n",
      "Epoch 20645/30000 Training Loss: 0.08651631325483322\n",
      "Epoch 20646/30000 Training Loss: 0.0815429836511612\n",
      "Epoch 20647/30000 Training Loss: 0.06740906089544296\n",
      "Epoch 20648/30000 Training Loss: 0.0678124949336052\n",
      "Epoch 20649/30000 Training Loss: 0.068115234375\n",
      "Epoch 20650/30000 Training Loss: 0.06797005981206894\n",
      "Epoch 20650/30000 Validation Loss: 0.07123783230781555\n",
      "Epoch 20651/30000 Training Loss: 0.06937123090028763\n",
      "Epoch 20652/30000 Training Loss: 0.05796973407268524\n",
      "Epoch 20653/30000 Training Loss: 0.07862042635679245\n",
      "Epoch 20654/30000 Training Loss: 0.07610919326543808\n",
      "Epoch 20655/30000 Training Loss: 0.07997549325227737\n",
      "Epoch 20656/30000 Training Loss: 0.06386921554803848\n",
      "Epoch 20657/30000 Training Loss: 0.06794586032629013\n",
      "Epoch 20658/30000 Training Loss: 0.07093816250562668\n",
      "Epoch 20659/30000 Training Loss: 0.07444917410612106\n",
      "Epoch 20660/30000 Training Loss: 0.07265090197324753\n",
      "Epoch 20660/30000 Validation Loss: 0.06203258037567139\n",
      "Epoch 20661/30000 Training Loss: 0.06571147590875626\n",
      "Epoch 20662/30000 Training Loss: 0.0671219751238823\n",
      "Epoch 20663/30000 Training Loss: 0.08273587375879288\n",
      "Epoch 20664/30000 Training Loss: 0.0667308047413826\n",
      "Epoch 20665/30000 Training Loss: 0.06576632708311081\n",
      "Epoch 20666/30000 Training Loss: 0.07879552990198135\n",
      "Epoch 20667/30000 Training Loss: 0.05919741094112396\n",
      "Epoch 20668/30000 Training Loss: 0.06731214374303818\n",
      "Epoch 20669/30000 Training Loss: 0.07298532873392105\n",
      "Epoch 20670/30000 Training Loss: 0.09026140719652176\n",
      "Epoch 20670/30000 Validation Loss: 0.0716865062713623\n",
      "Epoch 20671/30000 Training Loss: 0.0969231054186821\n",
      "Epoch 20672/30000 Training Loss: 0.05839600786566734\n",
      "Epoch 20673/30000 Training Loss: 0.06211479380726814\n",
      "Epoch 20674/30000 Training Loss: 0.0564710795879364\n",
      "Epoch 20675/30000 Training Loss: 0.07815206050872803\n",
      "Epoch 20676/30000 Training Loss: 0.05885237455368042\n",
      "Epoch 20677/30000 Training Loss: 0.07989192754030228\n",
      "Epoch 20678/30000 Training Loss: 0.06622450798749924\n",
      "Epoch 20679/30000 Training Loss: 0.08773433417081833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20680/30000 Training Loss: 0.0725507065653801\n",
      "Epoch 20680/30000 Validation Loss: 0.09199065715074539\n",
      "Epoch 20681/30000 Training Loss: 0.08626099675893784\n",
      "Epoch 20682/30000 Training Loss: 0.061448946595191956\n",
      "Epoch 20683/30000 Training Loss: 0.07975734025239944\n",
      "Epoch 20684/30000 Training Loss: 0.08766833692789078\n",
      "Epoch 20685/30000 Training Loss: 0.07757028192281723\n",
      "Epoch 20686/30000 Training Loss: 0.09692617505788803\n",
      "Epoch 20687/30000 Training Loss: 0.060641225427389145\n",
      "Epoch 20688/30000 Training Loss: 0.0735328420996666\n",
      "Epoch 20689/30000 Training Loss: 0.0623350590467453\n",
      "Epoch 20690/30000 Training Loss: 0.0595509298145771\n",
      "Epoch 20690/30000 Validation Loss: 0.08514294028282166\n",
      "Epoch 20691/30000 Training Loss: 0.06897913664579391\n",
      "Epoch 20692/30000 Training Loss: 0.06653047353029251\n",
      "Epoch 20693/30000 Training Loss: 0.09542012959718704\n",
      "Epoch 20694/30000 Training Loss: 0.07547492533922195\n",
      "Epoch 20695/30000 Training Loss: 0.06750772148370743\n",
      "Epoch 20696/30000 Training Loss: 0.0710187554359436\n",
      "Epoch 20697/30000 Training Loss: 0.08772626519203186\n",
      "Epoch 20698/30000 Training Loss: 0.06430032104253769\n",
      "Epoch 20699/30000 Training Loss: 0.06579209119081497\n",
      "Epoch 20700/30000 Training Loss: 0.06143501400947571\n",
      "Epoch 20700/30000 Validation Loss: 0.06556611508131027\n",
      "Epoch 20701/30000 Training Loss: 0.07219185680150986\n",
      "Epoch 20702/30000 Training Loss: 0.05526593327522278\n",
      "Epoch 20703/30000 Training Loss: 0.06817763298749924\n",
      "Epoch 20704/30000 Training Loss: 0.06724292039871216\n",
      "Epoch 20705/30000 Training Loss: 0.10197924822568893\n",
      "Epoch 20706/30000 Training Loss: 0.07109037786722183\n",
      "Epoch 20707/30000 Training Loss: 0.06364601105451584\n",
      "Epoch 20708/30000 Training Loss: 0.07328179478645325\n",
      "Epoch 20709/30000 Training Loss: 0.08389710634946823\n",
      "Epoch 20710/30000 Training Loss: 0.055707573890686035\n",
      "Epoch 20710/30000 Validation Loss: 0.08570054173469543\n",
      "Epoch 20711/30000 Training Loss: 0.09481054544448853\n",
      "Epoch 20712/30000 Training Loss: 0.06781049817800522\n",
      "Epoch 20713/30000 Training Loss: 0.07813069224357605\n",
      "Epoch 20714/30000 Training Loss: 0.06814617663621902\n",
      "Epoch 20715/30000 Training Loss: 0.081851065158844\n",
      "Epoch 20716/30000 Training Loss: 0.06930967420339584\n",
      "Epoch 20717/30000 Training Loss: 0.09350121766328812\n",
      "Epoch 20718/30000 Training Loss: 0.07282222807407379\n",
      "Epoch 20719/30000 Training Loss: 0.062014926224946976\n",
      "Epoch 20720/30000 Training Loss: 0.07376140356063843\n",
      "Epoch 20720/30000 Validation Loss: 0.06434009224176407\n",
      "Epoch 20721/30000 Training Loss: 0.08597578853368759\n",
      "Epoch 20722/30000 Training Loss: 0.07155162841081619\n",
      "Epoch 20723/30000 Training Loss: 0.07474537938833237\n",
      "Epoch 20724/30000 Training Loss: 0.05037375167012215\n",
      "Epoch 20725/30000 Training Loss: 0.07154155522584915\n",
      "Epoch 20726/30000 Training Loss: 0.05381731688976288\n",
      "Epoch 20727/30000 Training Loss: 0.07910242676734924\n",
      "Epoch 20728/30000 Training Loss: 0.07218264788389206\n",
      "Epoch 20729/30000 Training Loss: 0.07437620311975479\n",
      "Epoch 20730/30000 Training Loss: 0.07514341920614243\n",
      "Epoch 20730/30000 Validation Loss: 0.059271931648254395\n",
      "Epoch 20731/30000 Training Loss: 0.06990260630846024\n",
      "Epoch 20732/30000 Training Loss: 0.0709911361336708\n",
      "Epoch 20733/30000 Training Loss: 0.09160452336072922\n",
      "Epoch 20734/30000 Training Loss: 0.0778161808848381\n",
      "Epoch 20735/30000 Training Loss: 0.0713493824005127\n",
      "Epoch 20736/30000 Training Loss: 0.08854275196790695\n",
      "Epoch 20737/30000 Training Loss: 0.089378722012043\n",
      "Epoch 20738/30000 Training Loss: 0.08142626285552979\n",
      "Epoch 20739/30000 Training Loss: 0.06253386288881302\n",
      "Epoch 20740/30000 Training Loss: 0.0701812282204628\n",
      "Epoch 20740/30000 Validation Loss: 0.06701412796974182\n",
      "Epoch 20741/30000 Training Loss: 0.07771509885787964\n",
      "Epoch 20742/30000 Training Loss: 0.09338679164648056\n",
      "Epoch 20743/30000 Training Loss: 0.06923316419124603\n",
      "Epoch 20744/30000 Training Loss: 0.07432911545038223\n",
      "Epoch 20745/30000 Training Loss: 0.06248626485466957\n",
      "Epoch 20746/30000 Training Loss: 0.07205671817064285\n",
      "Epoch 20747/30000 Training Loss: 0.06127053499221802\n",
      "Epoch 20748/30000 Training Loss: 0.07503427565097809\n",
      "Epoch 20749/30000 Training Loss: 0.0801953449845314\n",
      "Epoch 20750/30000 Training Loss: 0.06852605193853378\n",
      "Epoch 20750/30000 Validation Loss: 0.06322018057107925\n",
      "Epoch 20751/30000 Training Loss: 0.07369624823331833\n",
      "Epoch 20752/30000 Training Loss: 0.07335033267736435\n",
      "Epoch 20753/30000 Training Loss: 0.06631201505661011\n",
      "Epoch 20754/30000 Training Loss: 0.07066157460212708\n",
      "Epoch 20755/30000 Training Loss: 0.0974048599600792\n",
      "Epoch 20756/30000 Training Loss: 0.08679350465536118\n",
      "Epoch 20757/30000 Training Loss: 0.06678260117769241\n",
      "Epoch 20758/30000 Training Loss: 0.0870312750339508\n",
      "Epoch 20759/30000 Training Loss: 0.053052663803100586\n",
      "Epoch 20760/30000 Training Loss: 0.07405788451433182\n",
      "Epoch 20760/30000 Validation Loss: 0.06585148721933365\n",
      "Epoch 20761/30000 Training Loss: 0.07674761116504669\n",
      "Epoch 20762/30000 Training Loss: 0.09007477760314941\n",
      "Epoch 20763/30000 Training Loss: 0.06842002272605896\n",
      "Epoch 20764/30000 Training Loss: 0.06564076244831085\n",
      "Epoch 20765/30000 Training Loss: 0.10797697305679321\n",
      "Epoch 20766/30000 Training Loss: 0.057979196310043335\n",
      "Epoch 20767/30000 Training Loss: 0.06342519074678421\n",
      "Epoch 20768/30000 Training Loss: 0.07880755513906479\n",
      "Epoch 20769/30000 Training Loss: 0.07368860393762589\n",
      "Epoch 20770/30000 Training Loss: 0.07399827986955643\n",
      "Epoch 20770/30000 Validation Loss: 0.08559472113847733\n",
      "Epoch 20771/30000 Training Loss: 0.07891764491796494\n",
      "Epoch 20772/30000 Training Loss: 0.08377651125192642\n",
      "Epoch 20773/30000 Training Loss: 0.09242258220911026\n",
      "Epoch 20774/30000 Training Loss: 0.0969160795211792\n",
      "Epoch 20775/30000 Training Loss: 0.07549112290143967\n",
      "Epoch 20776/30000 Training Loss: 0.048181235790252686\n",
      "Epoch 20777/30000 Training Loss: 0.07874324917793274\n",
      "Epoch 20778/30000 Training Loss: 0.06414880603551865\n",
      "Epoch 20779/30000 Training Loss: 0.07378988713026047\n",
      "Epoch 20780/30000 Training Loss: 0.08245565742254257\n",
      "Epoch 20780/30000 Validation Loss: 0.06884502619504929\n",
      "Epoch 20781/30000 Training Loss: 0.05602693185210228\n",
      "Epoch 20782/30000 Training Loss: 0.06265441328287125\n",
      "Epoch 20783/30000 Training Loss: 0.05860142037272453\n",
      "Epoch 20784/30000 Training Loss: 0.07030245661735535\n",
      "Epoch 20785/30000 Training Loss: 0.05554940178990364\n",
      "Epoch 20786/30000 Training Loss: 0.07764392346143723\n",
      "Epoch 20787/30000 Training Loss: 0.07171415537595749\n",
      "Epoch 20788/30000 Training Loss: 0.0733962133526802\n",
      "Epoch 20789/30000 Training Loss: 0.07416346669197083\n",
      "Epoch 20790/30000 Training Loss: 0.07377137988805771\n",
      "Epoch 20790/30000 Validation Loss: 0.08714313060045242\n",
      "Epoch 20791/30000 Training Loss: 0.09127222746610641\n",
      "Epoch 20792/30000 Training Loss: 0.0697498694062233\n",
      "Epoch 20793/30000 Training Loss: 0.08439033478498459\n",
      "Epoch 20794/30000 Training Loss: 0.0647590309381485\n",
      "Epoch 20795/30000 Training Loss: 0.07927533984184265\n",
      "Epoch 20796/30000 Training Loss: 0.06977397948503494\n",
      "Epoch 20797/30000 Training Loss: 0.06236866116523743\n",
      "Epoch 20798/30000 Training Loss: 0.07853693515062332\n",
      "Epoch 20799/30000 Training Loss: 0.07052689045667648\n",
      "Epoch 20800/30000 Training Loss: 0.09142015129327774\n",
      "Epoch 20800/30000 Validation Loss: 0.06869826465845108\n",
      "Epoch 20801/30000 Training Loss: 0.05261242017149925\n",
      "Epoch 20802/30000 Training Loss: 0.07580149918794632\n",
      "Epoch 20803/30000 Training Loss: 0.08019235730171204\n",
      "Epoch 20804/30000 Training Loss: 0.061503518372774124\n",
      "Epoch 20805/30000 Training Loss: 0.08691055327653885\n",
      "Epoch 20806/30000 Training Loss: 0.09291073679924011\n",
      "Epoch 20807/30000 Training Loss: 0.08586045354604721\n",
      "Epoch 20808/30000 Training Loss: 0.07078772038221359\n",
      "Epoch 20809/30000 Training Loss: 0.07806260138750076\n",
      "Epoch 20810/30000 Training Loss: 0.06921025365591049\n",
      "Epoch 20810/30000 Validation Loss: 0.07742749899625778\n",
      "Epoch 20811/30000 Training Loss: 0.06132073700428009\n",
      "Epoch 20812/30000 Training Loss: 0.05795316770672798\n",
      "Epoch 20813/30000 Training Loss: 0.06975451111793518\n",
      "Epoch 20814/30000 Training Loss: 0.06749853491783142\n",
      "Epoch 20815/30000 Training Loss: 0.0641598328948021\n",
      "Epoch 20816/30000 Training Loss: 0.054888322949409485\n",
      "Epoch 20817/30000 Training Loss: 0.06784095615148544\n",
      "Epoch 20818/30000 Training Loss: 0.06314778327941895\n",
      "Epoch 20819/30000 Training Loss: 0.06216819956898689\n",
      "Epoch 20820/30000 Training Loss: 0.06429938226938248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20820/30000 Validation Loss: 0.07428941130638123\n",
      "Epoch 20821/30000 Training Loss: 0.09289068728685379\n",
      "Epoch 20822/30000 Training Loss: 0.07637228071689606\n",
      "Epoch 20823/30000 Training Loss: 0.07748723775148392\n",
      "Epoch 20824/30000 Training Loss: 0.06989457458257675\n",
      "Epoch 20825/30000 Training Loss: 0.06742260605096817\n",
      "Epoch 20826/30000 Training Loss: 0.059066835790872574\n",
      "Epoch 20827/30000 Training Loss: 0.05498260259628296\n",
      "Epoch 20828/30000 Training Loss: 0.06057388707995415\n",
      "Epoch 20829/30000 Training Loss: 0.07658267021179199\n",
      "Epoch 20830/30000 Training Loss: 0.07387945801019669\n",
      "Epoch 20830/30000 Validation Loss: 0.07801088690757751\n",
      "Epoch 20831/30000 Training Loss: 0.05919994041323662\n",
      "Epoch 20832/30000 Training Loss: 0.08395896106958389\n",
      "Epoch 20833/30000 Training Loss: 0.07290976494550705\n",
      "Epoch 20834/30000 Training Loss: 0.06299898773431778\n",
      "Epoch 20835/30000 Training Loss: 0.06742323189973831\n",
      "Epoch 20836/30000 Training Loss: 0.05543358251452446\n",
      "Epoch 20837/30000 Training Loss: 0.07505723834037781\n",
      "Epoch 20838/30000 Training Loss: 0.05628145858645439\n",
      "Epoch 20839/30000 Training Loss: 0.06675557047128677\n",
      "Epoch 20840/30000 Training Loss: 0.08653131127357483\n",
      "Epoch 20840/30000 Validation Loss: 0.06297917664051056\n",
      "Epoch 20841/30000 Training Loss: 0.06532664597034454\n",
      "Epoch 20842/30000 Training Loss: 0.06862624734640121\n",
      "Epoch 20843/30000 Training Loss: 0.10157830268144608\n",
      "Epoch 20844/30000 Training Loss: 0.06795185804367065\n",
      "Epoch 20845/30000 Training Loss: 0.06481630355119705\n",
      "Epoch 20846/30000 Training Loss: 0.10059124231338501\n",
      "Epoch 20847/30000 Training Loss: 0.08345499634742737\n",
      "Epoch 20848/30000 Training Loss: 0.06032093986868858\n",
      "Epoch 20849/30000 Training Loss: 0.0828997790813446\n",
      "Epoch 20850/30000 Training Loss: 0.06626313179731369\n",
      "Epoch 20850/30000 Validation Loss: 0.06954867392778397\n",
      "Epoch 20851/30000 Training Loss: 0.056422483175992966\n",
      "Epoch 20852/30000 Training Loss: 0.08258053660392761\n",
      "Epoch 20853/30000 Training Loss: 0.08409034460783005\n",
      "Epoch 20854/30000 Training Loss: 0.07630771398544312\n",
      "Epoch 20855/30000 Training Loss: 0.0835951641201973\n",
      "Epoch 20856/30000 Training Loss: 0.06806675344705582\n",
      "Epoch 20857/30000 Training Loss: 0.05962146818637848\n",
      "Epoch 20858/30000 Training Loss: 0.07150115817785263\n",
      "Epoch 20859/30000 Training Loss: 0.06502538919448853\n",
      "Epoch 20860/30000 Training Loss: 0.07615248113870621\n",
      "Epoch 20860/30000 Validation Loss: 0.06699763238430023\n",
      "Epoch 20861/30000 Training Loss: 0.0777660682797432\n",
      "Epoch 20862/30000 Training Loss: 0.08857056498527527\n",
      "Epoch 20863/30000 Training Loss: 0.0650741457939148\n",
      "Epoch 20864/30000 Training Loss: 0.06540676951408386\n",
      "Epoch 20865/30000 Training Loss: 0.07771603763103485\n",
      "Epoch 20866/30000 Training Loss: 0.07072659581899643\n",
      "Epoch 20867/30000 Training Loss: 0.07946323603391647\n",
      "Epoch 20868/30000 Training Loss: 0.08218181878328323\n",
      "Epoch 20869/30000 Training Loss: 0.07365918904542923\n",
      "Epoch 20870/30000 Training Loss: 0.0717908963561058\n",
      "Epoch 20870/30000 Validation Loss: 0.07060708105564117\n",
      "Epoch 20871/30000 Training Loss: 0.06062251329421997\n",
      "Epoch 20872/30000 Training Loss: 0.08129245787858963\n",
      "Epoch 20873/30000 Training Loss: 0.09059059619903564\n",
      "Epoch 20874/30000 Training Loss: 0.06417635828256607\n",
      "Epoch 20875/30000 Training Loss: 0.06667513400316238\n",
      "Epoch 20876/30000 Training Loss: 0.09670301526784897\n",
      "Epoch 20877/30000 Training Loss: 0.0857534185051918\n",
      "Epoch 20878/30000 Training Loss: 0.07588981837034225\n",
      "Epoch 20879/30000 Training Loss: 0.08336291462182999\n",
      "Epoch 20880/30000 Training Loss: 0.07308172434568405\n",
      "Epoch 20880/30000 Validation Loss: 0.0756814181804657\n",
      "Epoch 20881/30000 Training Loss: 0.06471767276525497\n",
      "Epoch 20882/30000 Training Loss: 0.06356534361839294\n",
      "Epoch 20883/30000 Training Loss: 0.06594106554985046\n",
      "Epoch 20884/30000 Training Loss: 0.06822467595338821\n",
      "Epoch 20885/30000 Training Loss: 0.0659669041633606\n",
      "Epoch 20886/30000 Training Loss: 0.06802598387002945\n",
      "Epoch 20887/30000 Training Loss: 0.06964904814958572\n",
      "Epoch 20888/30000 Training Loss: 0.07431244105100632\n",
      "Epoch 20889/30000 Training Loss: 0.07098895311355591\n",
      "Epoch 20890/30000 Training Loss: 0.07447060942649841\n",
      "Epoch 20890/30000 Validation Loss: 0.06359726935625076\n",
      "Epoch 20891/30000 Training Loss: 0.08385750651359558\n",
      "Epoch 20892/30000 Training Loss: 0.07462096959352493\n",
      "Epoch 20893/30000 Training Loss: 0.07538782805204391\n",
      "Epoch 20894/30000 Training Loss: 0.0775982216000557\n",
      "Epoch 20895/30000 Training Loss: 0.07568743079900742\n",
      "Epoch 20896/30000 Training Loss: 0.07997582852840424\n",
      "Epoch 20897/30000 Training Loss: 0.07587692886590958\n",
      "Epoch 20898/30000 Training Loss: 0.061992693692445755\n",
      "Epoch 20899/30000 Training Loss: 0.05868978425860405\n",
      "Epoch 20900/30000 Training Loss: 0.06837689876556396\n",
      "Epoch 20900/30000 Validation Loss: 0.06494579464197159\n",
      "Epoch 20901/30000 Training Loss: 0.07895761728286743\n",
      "Epoch 20902/30000 Training Loss: 0.06974802166223526\n",
      "Epoch 20903/30000 Training Loss: 0.060020823031663895\n",
      "Epoch 20904/30000 Training Loss: 0.07468670606613159\n",
      "Epoch 20905/30000 Training Loss: 0.07096926122903824\n",
      "Epoch 20906/30000 Training Loss: 0.06903908401727676\n",
      "Epoch 20907/30000 Training Loss: 0.07886053621768951\n",
      "Epoch 20908/30000 Training Loss: 0.0666508823633194\n",
      "Epoch 20909/30000 Training Loss: 0.07112951576709747\n",
      "Epoch 20910/30000 Training Loss: 0.0697568729519844\n",
      "Epoch 20910/30000 Validation Loss: 0.06833410263061523\n",
      "Epoch 20911/30000 Training Loss: 0.07656396180391312\n",
      "Epoch 20912/30000 Training Loss: 0.07683338224887848\n",
      "Epoch 20913/30000 Training Loss: 0.07003989070653915\n",
      "Epoch 20914/30000 Training Loss: 0.07257436960935593\n",
      "Epoch 20915/30000 Training Loss: 0.057677317410707474\n",
      "Epoch 20916/30000 Training Loss: 0.06449311226606369\n",
      "Epoch 20917/30000 Training Loss: 0.06814678013324738\n",
      "Epoch 20918/30000 Training Loss: 0.05249719321727753\n",
      "Epoch 20919/30000 Training Loss: 0.05623611807823181\n",
      "Epoch 20920/30000 Training Loss: 0.09340548515319824\n",
      "Epoch 20920/30000 Validation Loss: 0.06750136613845825\n",
      "Epoch 20921/30000 Training Loss: 0.06673929840326309\n",
      "Epoch 20922/30000 Training Loss: 0.06687526404857635\n",
      "Epoch 20923/30000 Training Loss: 0.06541288644075394\n",
      "Epoch 20924/30000 Training Loss: 0.1160375103354454\n",
      "Epoch 20925/30000 Training Loss: 0.06516462564468384\n",
      "Epoch 20926/30000 Training Loss: 0.0612398236989975\n",
      "Epoch 20927/30000 Training Loss: 0.08321360498666763\n",
      "Epoch 20928/30000 Training Loss: 0.0709790289402008\n",
      "Epoch 20929/30000 Training Loss: 0.056421589106321335\n",
      "Epoch 20930/30000 Training Loss: 0.04986327514052391\n",
      "Epoch 20930/30000 Validation Loss: 0.06730019301176071\n",
      "Epoch 20931/30000 Training Loss: 0.08170320838689804\n",
      "Epoch 20932/30000 Training Loss: 0.06765338778495789\n",
      "Epoch 20933/30000 Training Loss: 0.06176447495818138\n",
      "Epoch 20934/30000 Training Loss: 0.0722956582903862\n",
      "Epoch 20935/30000 Training Loss: 0.061215925961732864\n",
      "Epoch 20936/30000 Training Loss: 0.07420029491186142\n",
      "Epoch 20937/30000 Training Loss: 0.07218898832798004\n",
      "Epoch 20938/30000 Training Loss: 0.07449480146169662\n",
      "Epoch 20939/30000 Training Loss: 0.07199961692094803\n",
      "Epoch 20940/30000 Training Loss: 0.08072688430547714\n",
      "Epoch 20940/30000 Validation Loss: 0.08275967836380005\n",
      "Epoch 20941/30000 Training Loss: 0.06204432249069214\n",
      "Epoch 20942/30000 Training Loss: 0.06773865967988968\n",
      "Epoch 20943/30000 Training Loss: 0.08142777532339096\n",
      "Epoch 20944/30000 Training Loss: 0.06460079550743103\n",
      "Epoch 20945/30000 Training Loss: 0.06943469494581223\n",
      "Epoch 20946/30000 Training Loss: 0.07620149850845337\n",
      "Epoch 20947/30000 Training Loss: 0.06517494469881058\n",
      "Epoch 20948/30000 Training Loss: 0.07348179072141647\n",
      "Epoch 20949/30000 Training Loss: 0.048868533223867416\n",
      "Epoch 20950/30000 Training Loss: 0.05545651912689209\n",
      "Epoch 20950/30000 Validation Loss: 0.06937969475984573\n",
      "Epoch 20951/30000 Training Loss: 0.08185694366693497\n",
      "Epoch 20952/30000 Training Loss: 0.06951864808797836\n",
      "Epoch 20953/30000 Training Loss: 0.06820222735404968\n",
      "Epoch 20954/30000 Training Loss: 0.07886016368865967\n",
      "Epoch 20955/30000 Training Loss: 0.07157798856496811\n",
      "Epoch 20956/30000 Training Loss: 0.06444104760885239\n",
      "Epoch 20957/30000 Training Loss: 0.07545310258865356\n",
      "Epoch 20958/30000 Training Loss: 0.0699252113699913\n",
      "Epoch 20959/30000 Training Loss: 0.06849735230207443\n",
      "Epoch 20960/30000 Training Loss: 0.0709916353225708\n",
      "Epoch 20960/30000 Validation Loss: 0.06856999546289444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20961/30000 Training Loss: 0.06906172633171082\n",
      "Epoch 20962/30000 Training Loss: 0.08622197061777115\n",
      "Epoch 20963/30000 Training Loss: 0.062436867505311966\n",
      "Epoch 20964/30000 Training Loss: 0.0691075399518013\n",
      "Epoch 20965/30000 Training Loss: 0.07302571088075638\n",
      "Epoch 20966/30000 Training Loss: 0.08173664659261703\n",
      "Epoch 20967/30000 Training Loss: 0.06317969411611557\n",
      "Epoch 20968/30000 Training Loss: 0.06847740709781647\n",
      "Epoch 20969/30000 Training Loss: 0.07982327044010162\n",
      "Epoch 20970/30000 Training Loss: 0.08301707357168198\n",
      "Epoch 20970/30000 Validation Loss: 0.05991481617093086\n",
      "Epoch 20971/30000 Training Loss: 0.07493933290243149\n",
      "Epoch 20972/30000 Training Loss: 0.06728797405958176\n",
      "Epoch 20973/30000 Training Loss: 0.08875339478254318\n",
      "Epoch 20974/30000 Training Loss: 0.05428406223654747\n",
      "Epoch 20975/30000 Training Loss: 0.08123782277107239\n",
      "Epoch 20976/30000 Training Loss: 0.05329855903983116\n",
      "Epoch 20977/30000 Training Loss: 0.08474138379096985\n",
      "Epoch 20978/30000 Training Loss: 0.049882348626852036\n",
      "Epoch 20979/30000 Training Loss: 0.07193801552057266\n",
      "Epoch 20980/30000 Training Loss: 0.0767403319478035\n",
      "Epoch 20980/30000 Validation Loss: 0.06080659106373787\n",
      "Epoch 20981/30000 Training Loss: 0.07542797178030014\n",
      "Epoch 20982/30000 Training Loss: 0.10229481011629105\n",
      "Epoch 20983/30000 Training Loss: 0.0626482143998146\n",
      "Epoch 20984/30000 Training Loss: 0.06897617131471634\n",
      "Epoch 20985/30000 Training Loss: 0.07270008325576782\n",
      "Epoch 20986/30000 Training Loss: 0.06760317832231522\n",
      "Epoch 20987/30000 Training Loss: 0.0694083496928215\n",
      "Epoch 20988/30000 Training Loss: 0.08465971797704697\n",
      "Epoch 20989/30000 Training Loss: 0.07642557471990585\n",
      "Epoch 20990/30000 Training Loss: 0.087748222053051\n",
      "Epoch 20990/30000 Validation Loss: 0.08031406253576279\n",
      "Epoch 20991/30000 Training Loss: 0.06139928102493286\n",
      "Epoch 20992/30000 Training Loss: 0.08743861317634583\n",
      "Epoch 20993/30000 Training Loss: 0.07305336743593216\n",
      "Epoch 20994/30000 Training Loss: 0.07608067244291306\n",
      "Epoch 20995/30000 Training Loss: 0.06927450746297836\n",
      "Epoch 20996/30000 Training Loss: 0.09168895334005356\n",
      "Epoch 20997/30000 Training Loss: 0.06301646679639816\n",
      "Epoch 20998/30000 Training Loss: 0.06173316016793251\n",
      "Epoch 20999/30000 Training Loss: 0.08855441212654114\n",
      "Epoch 21000/30000 Training Loss: 0.07675465941429138\n",
      "Epoch 21000/30000 Validation Loss: 0.06775825470685959\n",
      "Epoch 21001/30000 Training Loss: 0.06553294509649277\n",
      "Epoch 21002/30000 Training Loss: 0.07347381860017776\n",
      "Epoch 21003/30000 Training Loss: 0.0618782602250576\n",
      "Epoch 21004/30000 Training Loss: 0.07432588189840317\n",
      "Epoch 21005/30000 Training Loss: 0.07985392212867737\n",
      "Epoch 21006/30000 Training Loss: 0.06101273000240326\n",
      "Epoch 21007/30000 Training Loss: 0.08260003477334976\n",
      "Epoch 21008/30000 Training Loss: 0.0643550381064415\n",
      "Epoch 21009/30000 Training Loss: 0.07337545603513718\n",
      "Epoch 21010/30000 Training Loss: 0.07493049651384354\n",
      "Epoch 21010/30000 Validation Loss: 0.06480440497398376\n",
      "Epoch 21011/30000 Training Loss: 0.07509244233369827\n",
      "Epoch 21012/30000 Training Loss: 0.08796340227127075\n",
      "Epoch 21013/30000 Training Loss: 0.062415216118097305\n",
      "Epoch 21014/30000 Training Loss: 0.06180758401751518\n",
      "Epoch 21015/30000 Training Loss: 0.0823080912232399\n",
      "Epoch 21016/30000 Training Loss: 0.06944603472948074\n",
      "Epoch 21017/30000 Training Loss: 0.0827261283993721\n",
      "Epoch 21018/30000 Training Loss: 0.07478135079145432\n",
      "Epoch 21019/30000 Training Loss: 0.09385345131158829\n",
      "Epoch 21020/30000 Training Loss: 0.06718458980321884\n",
      "Epoch 21020/30000 Validation Loss: 0.0641871988773346\n",
      "Epoch 21021/30000 Training Loss: 0.06726544350385666\n",
      "Epoch 21022/30000 Training Loss: 0.06083546206355095\n",
      "Epoch 21023/30000 Training Loss: 0.06767485290765762\n",
      "Epoch 21024/30000 Training Loss: 0.08450547605752945\n",
      "Epoch 21025/30000 Training Loss: 0.07948702573776245\n",
      "Epoch 21026/30000 Training Loss: 0.08445964008569717\n",
      "Epoch 21027/30000 Training Loss: 0.0729999914765358\n",
      "Epoch 21028/30000 Training Loss: 0.07144101709127426\n",
      "Epoch 21029/30000 Training Loss: 0.057172417640686035\n",
      "Epoch 21030/30000 Training Loss: 0.0683654174208641\n",
      "Epoch 21030/30000 Validation Loss: 0.06524830311536789\n",
      "Epoch 21031/30000 Training Loss: 0.07468250393867493\n",
      "Epoch 21032/30000 Training Loss: 0.09604933857917786\n",
      "Epoch 21033/30000 Training Loss: 0.08448874205350876\n",
      "Epoch 21034/30000 Training Loss: 0.0689784362912178\n",
      "Epoch 21035/30000 Training Loss: 0.07861340790987015\n",
      "Epoch 21036/30000 Training Loss: 0.05912884697318077\n",
      "Epoch 21037/30000 Training Loss: 0.0823824480175972\n",
      "Epoch 21038/30000 Training Loss: 0.0631793886423111\n",
      "Epoch 21039/30000 Training Loss: 0.10536324232816696\n",
      "Epoch 21040/30000 Training Loss: 0.06643243134021759\n",
      "Epoch 21040/30000 Validation Loss: 0.0756654441356659\n",
      "Epoch 21041/30000 Training Loss: 0.06963470578193665\n",
      "Epoch 21042/30000 Training Loss: 0.06704925745725632\n",
      "Epoch 21043/30000 Training Loss: 0.0806720182299614\n",
      "Epoch 21044/30000 Training Loss: 0.07465585321187973\n",
      "Epoch 21045/30000 Training Loss: 0.07595239579677582\n",
      "Epoch 21046/30000 Training Loss: 0.07091102749109268\n",
      "Epoch 21047/30000 Training Loss: 0.07269260287284851\n",
      "Epoch 21048/30000 Training Loss: 0.06145717203617096\n",
      "Epoch 21049/30000 Training Loss: 0.06962593644857407\n",
      "Epoch 21050/30000 Training Loss: 0.061181437224149704\n",
      "Epoch 21050/30000 Validation Loss: 0.08115794509649277\n",
      "Epoch 21051/30000 Training Loss: 0.07309320569038391\n",
      "Epoch 21052/30000 Training Loss: 0.07203209400177002\n",
      "Epoch 21053/30000 Training Loss: 0.059295494109392166\n",
      "Epoch 21054/30000 Training Loss: 0.06490189582109451\n",
      "Epoch 21055/30000 Training Loss: 0.07935186475515366\n",
      "Epoch 21056/30000 Training Loss: 0.07099441438913345\n",
      "Epoch 21057/30000 Training Loss: 0.0685727521777153\n",
      "Epoch 21058/30000 Training Loss: 0.09118802100419998\n",
      "Epoch 21059/30000 Training Loss: 0.05812534689903259\n",
      "Epoch 21060/30000 Training Loss: 0.07040636241436005\n",
      "Epoch 21060/30000 Validation Loss: 0.0781242623925209\n",
      "Epoch 21061/30000 Training Loss: 0.05892070755362511\n",
      "Epoch 21062/30000 Training Loss: 0.06653176993131638\n",
      "Epoch 21063/30000 Training Loss: 0.06819837540388107\n",
      "Epoch 21064/30000 Training Loss: 0.07394921034574509\n",
      "Epoch 21065/30000 Training Loss: 0.08128177374601364\n",
      "Epoch 21066/30000 Training Loss: 0.06718277186155319\n",
      "Epoch 21067/30000 Training Loss: 0.05538724362850189\n",
      "Epoch 21068/30000 Training Loss: 0.06277961283922195\n",
      "Epoch 21069/30000 Training Loss: 0.05248318240046501\n",
      "Epoch 21070/30000 Training Loss: 0.059237416833639145\n",
      "Epoch 21070/30000 Validation Loss: 0.07188457995653152\n",
      "Epoch 21071/30000 Training Loss: 0.07770583033561707\n",
      "Epoch 21072/30000 Training Loss: 0.08345162868499756\n",
      "Epoch 21073/30000 Training Loss: 0.09128269553184509\n",
      "Epoch 21074/30000 Training Loss: 0.08806147426366806\n",
      "Epoch 21075/30000 Training Loss: 0.0855245366692543\n",
      "Epoch 21076/30000 Training Loss: 0.08109407871961594\n",
      "Epoch 21077/30000 Training Loss: 0.0719960406422615\n",
      "Epoch 21078/30000 Training Loss: 0.055369436740875244\n",
      "Epoch 21079/30000 Training Loss: 0.07881643623113632\n",
      "Epoch 21080/30000 Training Loss: 0.07501692324876785\n",
      "Epoch 21080/30000 Validation Loss: 0.06424438953399658\n",
      "Epoch 21081/30000 Training Loss: 0.08345961570739746\n",
      "Epoch 21082/30000 Training Loss: 0.06970975548028946\n",
      "Epoch 21083/30000 Training Loss: 0.0676700547337532\n",
      "Epoch 21084/30000 Training Loss: 0.07248097658157349\n",
      "Epoch 21085/30000 Training Loss: 0.06954754143953323\n",
      "Epoch 21086/30000 Training Loss: 0.07368863373994827\n",
      "Epoch 21087/30000 Training Loss: 0.06833446025848389\n",
      "Epoch 21088/30000 Training Loss: 0.05891669914126396\n",
      "Epoch 21089/30000 Training Loss: 0.06145351007580757\n",
      "Epoch 21090/30000 Training Loss: 0.06492187827825546\n",
      "Epoch 21090/30000 Validation Loss: 0.07113205641508102\n",
      "Epoch 21091/30000 Training Loss: 0.08842305094003677\n",
      "Epoch 21092/30000 Training Loss: 0.06723561137914658\n",
      "Epoch 21093/30000 Training Loss: 0.0749499574303627\n",
      "Epoch 21094/30000 Training Loss: 0.050829093903303146\n",
      "Epoch 21095/30000 Training Loss: 0.0624358169734478\n",
      "Epoch 21096/30000 Training Loss: 0.0645480528473854\n",
      "Epoch 21097/30000 Training Loss: 0.04848472774028778\n",
      "Epoch 21098/30000 Training Loss: 0.0764714702963829\n",
      "Epoch 21099/30000 Training Loss: 0.057276561856269836\n",
      "Epoch 21100/30000 Training Loss: 0.08516079187393188\n",
      "Epoch 21100/30000 Validation Loss: 0.07890952378511429\n",
      "Epoch 21101/30000 Training Loss: 0.07813987880945206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21102/30000 Training Loss: 0.07383221387863159\n",
      "Epoch 21103/30000 Training Loss: 0.06383391469717026\n",
      "Epoch 21104/30000 Training Loss: 0.09044009447097778\n",
      "Epoch 21105/30000 Training Loss: 0.06562072038650513\n",
      "Epoch 21106/30000 Training Loss: 0.05372634902596474\n",
      "Epoch 21107/30000 Training Loss: 0.057317256927490234\n",
      "Epoch 21108/30000 Training Loss: 0.07549090683460236\n",
      "Epoch 21109/30000 Training Loss: 0.08309754729270935\n",
      "Epoch 21110/30000 Training Loss: 0.059569139033555984\n",
      "Epoch 21110/30000 Validation Loss: 0.07040821015834808\n",
      "Epoch 21111/30000 Training Loss: 0.06116626784205437\n",
      "Epoch 21112/30000 Training Loss: 0.053465042263269424\n",
      "Epoch 21113/30000 Training Loss: 0.06746596097946167\n",
      "Epoch 21114/30000 Training Loss: 0.08805590867996216\n",
      "Epoch 21115/30000 Training Loss: 0.07501676678657532\n",
      "Epoch 21116/30000 Training Loss: 0.05947406589984894\n",
      "Epoch 21117/30000 Training Loss: 0.053934380412101746\n",
      "Epoch 21118/30000 Training Loss: 0.06535592675209045\n",
      "Epoch 21119/30000 Training Loss: 0.06944010406732559\n",
      "Epoch 21120/30000 Training Loss: 0.08410017937421799\n",
      "Epoch 21120/30000 Validation Loss: 0.06419812887907028\n",
      "Epoch 21121/30000 Training Loss: 0.06552771478891373\n",
      "Epoch 21122/30000 Training Loss: 0.09115733951330185\n",
      "Epoch 21123/30000 Training Loss: 0.06508923321962357\n",
      "Epoch 21124/30000 Training Loss: 0.08016461879014969\n",
      "Epoch 21125/30000 Training Loss: 0.07123629003763199\n",
      "Epoch 21126/30000 Training Loss: 0.06211315095424652\n",
      "Epoch 21127/30000 Training Loss: 0.07183065265417099\n",
      "Epoch 21128/30000 Training Loss: 0.06137123703956604\n",
      "Epoch 21129/30000 Training Loss: 0.06315794587135315\n",
      "Epoch 21130/30000 Training Loss: 0.0693260207772255\n",
      "Epoch 21130/30000 Validation Loss: 0.07136530429124832\n",
      "Epoch 21131/30000 Training Loss: 0.05001131072640419\n",
      "Epoch 21132/30000 Training Loss: 0.06777111440896988\n",
      "Epoch 21133/30000 Training Loss: 0.0765422061085701\n",
      "Epoch 21134/30000 Training Loss: 0.06323172897100449\n",
      "Epoch 21135/30000 Training Loss: 0.07141562551259995\n",
      "Epoch 21136/30000 Training Loss: 0.0847601369023323\n",
      "Epoch 21137/30000 Training Loss: 0.06396260112524033\n",
      "Epoch 21138/30000 Training Loss: 0.06954478472471237\n",
      "Epoch 21139/30000 Training Loss: 0.07154382020235062\n",
      "Epoch 21140/30000 Training Loss: 0.05127646401524544\n",
      "Epoch 21140/30000 Validation Loss: 0.06755121797323227\n",
      "Epoch 21141/30000 Training Loss: 0.07723075896501541\n",
      "Epoch 21142/30000 Training Loss: 0.08660287410020828\n",
      "Epoch 21143/30000 Training Loss: 0.08191689848899841\n",
      "Epoch 21144/30000 Training Loss: 0.08649381250143051\n",
      "Epoch 21145/30000 Training Loss: 0.07536523789167404\n",
      "Epoch 21146/30000 Training Loss: 0.08120059221982956\n",
      "Epoch 21147/30000 Training Loss: 0.06598300486803055\n",
      "Epoch 21148/30000 Training Loss: 0.05088427662849426\n",
      "Epoch 21149/30000 Training Loss: 0.07499262690544128\n",
      "Epoch 21150/30000 Training Loss: 0.07178554683923721\n",
      "Epoch 21150/30000 Validation Loss: 0.05477866530418396\n",
      "Epoch 21151/30000 Training Loss: 0.07579432427883148\n",
      "Epoch 21152/30000 Training Loss: 0.07135239988565445\n",
      "Epoch 21153/30000 Training Loss: 0.08694923669099808\n",
      "Epoch 21154/30000 Training Loss: 0.06988724321126938\n",
      "Epoch 21155/30000 Training Loss: 0.07022935897111893\n",
      "Epoch 21156/30000 Training Loss: 0.08307702094316483\n",
      "Epoch 21157/30000 Training Loss: 0.09122452884912491\n",
      "Epoch 21158/30000 Training Loss: 0.0701707974076271\n",
      "Epoch 21159/30000 Training Loss: 0.05484187602996826\n",
      "Epoch 21160/30000 Training Loss: 0.0730457529425621\n",
      "Epoch 21160/30000 Validation Loss: 0.05968804284930229\n",
      "Epoch 21161/30000 Training Loss: 0.0816240981221199\n",
      "Epoch 21162/30000 Training Loss: 0.07259049266576767\n",
      "Epoch 21163/30000 Training Loss: 0.07416011393070221\n",
      "Epoch 21164/30000 Training Loss: 0.054418984800577164\n",
      "Epoch 21165/30000 Training Loss: 0.07385226339101791\n",
      "Epoch 21166/30000 Training Loss: 0.07968220114707947\n",
      "Epoch 21167/30000 Training Loss: 0.09219255298376083\n",
      "Epoch 21168/30000 Training Loss: 0.06609418243169785\n",
      "Epoch 21169/30000 Training Loss: 0.056945931166410446\n",
      "Epoch 21170/30000 Training Loss: 0.09064198285341263\n",
      "Epoch 21170/30000 Validation Loss: 0.07030084729194641\n",
      "Epoch 21171/30000 Training Loss: 0.05332301929593086\n",
      "Epoch 21172/30000 Training Loss: 0.0714462473988533\n",
      "Epoch 21173/30000 Training Loss: 0.07746297866106033\n",
      "Epoch 21174/30000 Training Loss: 0.0843941792845726\n",
      "Epoch 21175/30000 Training Loss: 0.07853122800588608\n",
      "Epoch 21176/30000 Training Loss: 0.07983548194169998\n",
      "Epoch 21177/30000 Training Loss: 0.046527981758117676\n",
      "Epoch 21178/30000 Training Loss: 0.07168528437614441\n",
      "Epoch 21179/30000 Training Loss: 0.07725536078214645\n",
      "Epoch 21180/30000 Training Loss: 0.05015088990330696\n",
      "Epoch 21180/30000 Validation Loss: 0.055116865783929825\n",
      "Epoch 21181/30000 Training Loss: 0.08400002121925354\n",
      "Epoch 21182/30000 Training Loss: 0.06521619111299515\n",
      "Epoch 21183/30000 Training Loss: 0.09865859150886536\n",
      "Epoch 21184/30000 Training Loss: 0.05610339716076851\n",
      "Epoch 21185/30000 Training Loss: 0.06915178149938583\n",
      "Epoch 21186/30000 Training Loss: 0.08123866468667984\n",
      "Epoch 21187/30000 Training Loss: 0.054202258586883545\n",
      "Epoch 21188/30000 Training Loss: 0.06701195240020752\n",
      "Epoch 21189/30000 Training Loss: 0.08683008700609207\n",
      "Epoch 21190/30000 Training Loss: 0.08183842152357101\n",
      "Epoch 21190/30000 Validation Loss: 0.07430537790060043\n",
      "Epoch 21191/30000 Training Loss: 0.0755835548043251\n",
      "Epoch 21192/30000 Training Loss: 0.06448913365602493\n",
      "Epoch 21193/30000 Training Loss: 0.08045746386051178\n",
      "Epoch 21194/30000 Training Loss: 0.08267863839864731\n",
      "Epoch 21195/30000 Training Loss: 0.06545593589544296\n",
      "Epoch 21196/30000 Training Loss: 0.05291556194424629\n",
      "Epoch 21197/30000 Training Loss: 0.06453930586576462\n",
      "Epoch 21198/30000 Training Loss: 0.07209178060293198\n",
      "Epoch 21199/30000 Training Loss: 0.06165081262588501\n",
      "Epoch 21200/30000 Training Loss: 0.07267006486654282\n",
      "Epoch 21200/30000 Validation Loss: 0.07730099558830261\n",
      "Epoch 21201/30000 Training Loss: 0.06420215219259262\n",
      "Epoch 21202/30000 Training Loss: 0.05291561409831047\n",
      "Epoch 21203/30000 Training Loss: 0.061293184757232666\n",
      "Epoch 21204/30000 Training Loss: 0.0692991092801094\n",
      "Epoch 21205/30000 Training Loss: 0.07513899356126785\n",
      "Epoch 21206/30000 Training Loss: 0.054816290736198425\n",
      "Epoch 21207/30000 Training Loss: 0.06567910313606262\n",
      "Epoch 21208/30000 Training Loss: 0.07126201689243317\n",
      "Epoch 21209/30000 Training Loss: 0.08466291427612305\n",
      "Epoch 21210/30000 Training Loss: 0.07802232354879379\n",
      "Epoch 21210/30000 Validation Loss: 0.07052434235811234\n",
      "Epoch 21211/30000 Training Loss: 0.05262473598122597\n",
      "Epoch 21212/30000 Training Loss: 0.08300771564245224\n",
      "Epoch 21213/30000 Training Loss: 0.07537364214658737\n",
      "Epoch 21214/30000 Training Loss: 0.0659068152308464\n",
      "Epoch 21215/30000 Training Loss: 0.07003128528594971\n",
      "Epoch 21216/30000 Training Loss: 0.06940928846597672\n",
      "Epoch 21217/30000 Training Loss: 0.06937786936759949\n",
      "Epoch 21218/30000 Training Loss: 0.06561186909675598\n",
      "Epoch 21219/30000 Training Loss: 0.08465081453323364\n",
      "Epoch 21220/30000 Training Loss: 0.059286754578351974\n",
      "Epoch 21220/30000 Validation Loss: 0.06884705275297165\n",
      "Epoch 21221/30000 Training Loss: 0.0636688694357872\n",
      "Epoch 21222/30000 Training Loss: 0.08130091428756714\n",
      "Epoch 21223/30000 Training Loss: 0.07972025871276855\n",
      "Epoch 21224/30000 Training Loss: 0.08174356073141098\n",
      "Epoch 21225/30000 Training Loss: 0.0689477026462555\n",
      "Epoch 21226/30000 Training Loss: 0.0634801909327507\n",
      "Epoch 21227/30000 Training Loss: 0.08100763708353043\n",
      "Epoch 21228/30000 Training Loss: 0.0805886909365654\n",
      "Epoch 21229/30000 Training Loss: 0.07155977934598923\n",
      "Epoch 21230/30000 Training Loss: 0.07590057700872421\n",
      "Epoch 21230/30000 Validation Loss: 0.06766656786203384\n",
      "Epoch 21231/30000 Training Loss: 0.0705001950263977\n",
      "Epoch 21232/30000 Training Loss: 0.06767672300338745\n",
      "Epoch 21233/30000 Training Loss: 0.08200816065073013\n",
      "Epoch 21234/30000 Training Loss: 0.06863372772932053\n",
      "Epoch 21235/30000 Training Loss: 0.07985898107290268\n",
      "Epoch 21236/30000 Training Loss: 0.08511548489332199\n",
      "Epoch 21237/30000 Training Loss: 0.06504717469215393\n",
      "Epoch 21238/30000 Training Loss: 0.07003312557935715\n",
      "Epoch 21239/30000 Training Loss: 0.07462170720100403\n",
      "Epoch 21240/30000 Training Loss: 0.07957438379526138\n",
      "Epoch 21240/30000 Validation Loss: 0.08913495391607285\n",
      "Epoch 21241/30000 Training Loss: 0.08128267526626587\n",
      "Epoch 21242/30000 Training Loss: 0.07615874707698822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21243/30000 Training Loss: 0.0895366296172142\n",
      "Epoch 21244/30000 Training Loss: 0.0952177420258522\n",
      "Epoch 21245/30000 Training Loss: 0.05648675188422203\n",
      "Epoch 21246/30000 Training Loss: 0.060546983033418655\n",
      "Epoch 21247/30000 Training Loss: 0.061185579746961594\n",
      "Epoch 21248/30000 Training Loss: 0.07076957821846008\n",
      "Epoch 21249/30000 Training Loss: 0.05847654119133949\n",
      "Epoch 21250/30000 Training Loss: 0.06400647014379501\n",
      "Epoch 21250/30000 Validation Loss: 0.0974082425236702\n",
      "Epoch 21251/30000 Training Loss: 0.06848901510238647\n",
      "Epoch 21252/30000 Training Loss: 0.06765446066856384\n",
      "Epoch 21253/30000 Training Loss: 0.07517314702272415\n",
      "Epoch 21254/30000 Training Loss: 0.0715823769569397\n",
      "Epoch 21255/30000 Training Loss: 0.06674336642026901\n",
      "Epoch 21256/30000 Training Loss: 0.04704967141151428\n",
      "Epoch 21257/30000 Training Loss: 0.06113560497760773\n",
      "Epoch 21258/30000 Training Loss: 0.08552521467208862\n",
      "Epoch 21259/30000 Training Loss: 0.06903847306966782\n",
      "Epoch 21260/30000 Training Loss: 0.09288854151964188\n",
      "Epoch 21260/30000 Validation Loss: 0.07049347460269928\n",
      "Epoch 21261/30000 Training Loss: 0.07345914095640182\n",
      "Epoch 21262/30000 Training Loss: 0.06845904141664505\n",
      "Epoch 21263/30000 Training Loss: 0.05123795196413994\n",
      "Epoch 21264/30000 Training Loss: 0.0726488009095192\n",
      "Epoch 21265/30000 Training Loss: 0.06083732470870018\n",
      "Epoch 21266/30000 Training Loss: 0.06967835128307343\n",
      "Epoch 21267/30000 Training Loss: 0.08185584098100662\n",
      "Epoch 21268/30000 Training Loss: 0.08239107578992844\n",
      "Epoch 21269/30000 Training Loss: 0.08209488540887833\n",
      "Epoch 21270/30000 Training Loss: 0.059878479689359665\n",
      "Epoch 21270/30000 Validation Loss: 0.07362154871225357\n",
      "Epoch 21271/30000 Training Loss: 0.0670219287276268\n",
      "Epoch 21272/30000 Training Loss: 0.09125041216611862\n",
      "Epoch 21273/30000 Training Loss: 0.06253547221422195\n",
      "Epoch 21274/30000 Training Loss: 0.07072757929563522\n",
      "Epoch 21275/30000 Training Loss: 0.05950622633099556\n",
      "Epoch 21276/30000 Training Loss: 0.07767637819051743\n",
      "Epoch 21277/30000 Training Loss: 0.0631764605641365\n",
      "Epoch 21278/30000 Training Loss: 0.0697150006890297\n",
      "Epoch 21279/30000 Training Loss: 0.06425797939300537\n",
      "Epoch 21280/30000 Training Loss: 0.0809132382273674\n",
      "Epoch 21280/30000 Validation Loss: 0.07198264449834824\n",
      "Epoch 21281/30000 Training Loss: 0.07354231178760529\n",
      "Epoch 21282/30000 Training Loss: 0.05551508441567421\n",
      "Epoch 21283/30000 Training Loss: 0.07494717091321945\n",
      "Epoch 21284/30000 Training Loss: 0.08257006853818893\n",
      "Epoch 21285/30000 Training Loss: 0.07529277354478836\n",
      "Epoch 21286/30000 Training Loss: 0.06681863218545914\n",
      "Epoch 21287/30000 Training Loss: 0.06718876957893372\n",
      "Epoch 21288/30000 Training Loss: 0.06273216754198074\n",
      "Epoch 21289/30000 Training Loss: 0.07904862612485886\n",
      "Epoch 21290/30000 Training Loss: 0.06933488696813583\n",
      "Epoch 21290/30000 Validation Loss: 0.07630852609872818\n",
      "Epoch 21291/30000 Training Loss: 0.07422176748514175\n",
      "Epoch 21292/30000 Training Loss: 0.06547164916992188\n",
      "Epoch 21293/30000 Training Loss: 0.06106477975845337\n",
      "Epoch 21294/30000 Training Loss: 0.06791842728853226\n",
      "Epoch 21295/30000 Training Loss: 0.06886213272809982\n",
      "Epoch 21296/30000 Training Loss: 0.07558146864175797\n",
      "Epoch 21297/30000 Training Loss: 0.07207471877336502\n",
      "Epoch 21298/30000 Training Loss: 0.06875023245811462\n",
      "Epoch 21299/30000 Training Loss: 0.07149749994277954\n",
      "Epoch 21300/30000 Training Loss: 0.05578729510307312\n",
      "Epoch 21300/30000 Validation Loss: 0.07937637716531754\n",
      "Epoch 21301/30000 Training Loss: 0.05247555673122406\n",
      "Epoch 21302/30000 Training Loss: 0.09932327270507812\n",
      "Epoch 21303/30000 Training Loss: 0.08847663551568985\n",
      "Epoch 21304/30000 Training Loss: 0.07572349160909653\n",
      "Epoch 21305/30000 Training Loss: 0.0804005041718483\n",
      "Epoch 21306/30000 Training Loss: 0.0671226978302002\n",
      "Epoch 21307/30000 Training Loss: 0.07746165245771408\n",
      "Epoch 21308/30000 Training Loss: 0.09969112277030945\n",
      "Epoch 21309/30000 Training Loss: 0.08274370431900024\n",
      "Epoch 21310/30000 Training Loss: 0.07204849272966385\n",
      "Epoch 21310/30000 Validation Loss: 0.06683183461427689\n",
      "Epoch 21311/30000 Training Loss: 0.0722208321094513\n",
      "Epoch 21312/30000 Training Loss: 0.05542529746890068\n",
      "Epoch 21313/30000 Training Loss: 0.07013384252786636\n",
      "Epoch 21314/30000 Training Loss: 0.060499537736177444\n",
      "Epoch 21315/30000 Training Loss: 0.0664929524064064\n",
      "Epoch 21316/30000 Training Loss: 0.07904034107923508\n",
      "Epoch 21317/30000 Training Loss: 0.07215174287557602\n",
      "Epoch 21318/30000 Training Loss: 0.07381411641836166\n",
      "Epoch 21319/30000 Training Loss: 0.06896241754293442\n",
      "Epoch 21320/30000 Training Loss: 0.08394529670476913\n",
      "Epoch 21320/30000 Validation Loss: 0.06435656547546387\n",
      "Epoch 21321/30000 Training Loss: 0.07150521874427795\n",
      "Epoch 21322/30000 Training Loss: 0.07228020578622818\n",
      "Epoch 21323/30000 Training Loss: 0.06390710920095444\n",
      "Epoch 21324/30000 Training Loss: 0.05208579823374748\n",
      "Epoch 21325/30000 Training Loss: 0.08796433359384537\n",
      "Epoch 21326/30000 Training Loss: 0.058640409260988235\n",
      "Epoch 21327/30000 Training Loss: 0.08595693111419678\n",
      "Epoch 21328/30000 Training Loss: 0.06288563460111618\n",
      "Epoch 21329/30000 Training Loss: 0.07508141547441483\n",
      "Epoch 21330/30000 Training Loss: 0.06955885887145996\n",
      "Epoch 21330/30000 Validation Loss: 0.0680711418390274\n",
      "Epoch 21331/30000 Training Loss: 0.07936201244592667\n",
      "Epoch 21332/30000 Training Loss: 0.09319005161523819\n",
      "Epoch 21333/30000 Training Loss: 0.07658926397562027\n",
      "Epoch 21334/30000 Training Loss: 0.06574265658855438\n",
      "Epoch 21335/30000 Training Loss: 0.07152900844812393\n",
      "Epoch 21336/30000 Training Loss: 0.08444296568632126\n",
      "Epoch 21337/30000 Training Loss: 0.08617862313985825\n",
      "Epoch 21338/30000 Training Loss: 0.07284704595804214\n",
      "Epoch 21339/30000 Training Loss: 0.06534264236688614\n",
      "Epoch 21340/30000 Training Loss: 0.062454864382743835\n",
      "Epoch 21340/30000 Validation Loss: 0.06421075016260147\n",
      "Epoch 21341/30000 Training Loss: 0.05013890191912651\n",
      "Epoch 21342/30000 Training Loss: 0.06508851051330566\n",
      "Epoch 21343/30000 Training Loss: 0.05705076456069946\n",
      "Epoch 21344/30000 Training Loss: 0.09664805978536606\n",
      "Epoch 21345/30000 Training Loss: 0.06712589412927628\n",
      "Epoch 21346/30000 Training Loss: 0.07962726801633835\n",
      "Epoch 21347/30000 Training Loss: 0.07098707556724548\n",
      "Epoch 21348/30000 Training Loss: 0.06734652817249298\n",
      "Epoch 21349/30000 Training Loss: 0.07892805337905884\n",
      "Epoch 21350/30000 Training Loss: 0.08266659080982208\n",
      "Epoch 21350/30000 Validation Loss: 0.06537363678216934\n",
      "Epoch 21351/30000 Training Loss: 0.05199718847870827\n",
      "Epoch 21352/30000 Training Loss: 0.07141420990228653\n",
      "Epoch 21353/30000 Training Loss: 0.06093014404177666\n",
      "Epoch 21354/30000 Training Loss: 0.07548972964286804\n",
      "Epoch 21355/30000 Training Loss: 0.0618518702685833\n",
      "Epoch 21356/30000 Training Loss: 0.06891585141420364\n",
      "Epoch 21357/30000 Training Loss: 0.06719030439853668\n",
      "Epoch 21358/30000 Training Loss: 0.06644066423177719\n",
      "Epoch 21359/30000 Training Loss: 0.07466665655374527\n",
      "Epoch 21360/30000 Training Loss: 0.0646834746003151\n",
      "Epoch 21360/30000 Validation Loss: 0.08362818509340286\n",
      "Epoch 21361/30000 Training Loss: 0.05702030286192894\n",
      "Epoch 21362/30000 Training Loss: 0.09741038829088211\n",
      "Epoch 21363/30000 Training Loss: 0.08587341755628586\n",
      "Epoch 21364/30000 Training Loss: 0.07809016108512878\n",
      "Epoch 21365/30000 Training Loss: 0.08231601864099503\n",
      "Epoch 21366/30000 Training Loss: 0.06295068562030792\n",
      "Epoch 21367/30000 Training Loss: 0.07591430097818375\n",
      "Epoch 21368/30000 Training Loss: 0.06258579343557358\n",
      "Epoch 21369/30000 Training Loss: 0.057564105838537216\n",
      "Epoch 21370/30000 Training Loss: 0.07233072072267532\n",
      "Epoch 21370/30000 Validation Loss: 0.07923324406147003\n",
      "Epoch 21371/30000 Training Loss: 0.056755200028419495\n",
      "Epoch 21372/30000 Training Loss: 0.07088324427604675\n",
      "Epoch 21373/30000 Training Loss: 0.0712813213467598\n",
      "Epoch 21374/30000 Training Loss: 0.07369035482406616\n",
      "Epoch 21375/30000 Training Loss: 0.07203172892332077\n",
      "Epoch 21376/30000 Training Loss: 0.0767720639705658\n",
      "Epoch 21377/30000 Training Loss: 0.0717109739780426\n",
      "Epoch 21378/30000 Training Loss: 0.08782444149255753\n",
      "Epoch 21379/30000 Training Loss: 0.07494426518678665\n",
      "Epoch 21380/30000 Training Loss: 0.058187052607536316\n",
      "Epoch 21380/30000 Validation Loss: 0.08228257298469543\n",
      "Epoch 21381/30000 Training Loss: 0.06091594696044922\n",
      "Epoch 21382/30000 Training Loss: 0.08380325883626938\n",
      "Epoch 21383/30000 Training Loss: 0.10417642444372177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21384/30000 Training Loss: 0.06907664239406586\n",
      "Epoch 21385/30000 Training Loss: 0.06452895700931549\n",
      "Epoch 21386/30000 Training Loss: 0.0928797647356987\n",
      "Epoch 21387/30000 Training Loss: 0.06570341438055038\n",
      "Epoch 21388/30000 Training Loss: 0.06569588929414749\n",
      "Epoch 21389/30000 Training Loss: 0.06622704863548279\n",
      "Epoch 21390/30000 Training Loss: 0.06615285575389862\n",
      "Epoch 21390/30000 Validation Loss: 0.07215218991041183\n",
      "Epoch 21391/30000 Training Loss: 0.0637935996055603\n",
      "Epoch 21392/30000 Training Loss: 0.056923478841781616\n",
      "Epoch 21393/30000 Training Loss: 0.060090821236371994\n",
      "Epoch 21394/30000 Training Loss: 0.052195340394973755\n",
      "Epoch 21395/30000 Training Loss: 0.07299622893333435\n",
      "Epoch 21396/30000 Training Loss: 0.08310099691152573\n",
      "Epoch 21397/30000 Training Loss: 0.06550543755292892\n",
      "Epoch 21398/30000 Training Loss: 0.05999957397580147\n",
      "Epoch 21399/30000 Training Loss: 0.06521180272102356\n",
      "Epoch 21400/30000 Training Loss: 0.05812067165970802\n",
      "Epoch 21400/30000 Validation Loss: 0.07722622901201248\n",
      "Epoch 21401/30000 Training Loss: 0.07565871626138687\n",
      "Epoch 21402/30000 Training Loss: 0.09440814703702927\n",
      "Epoch 21403/30000 Training Loss: 0.06001819297671318\n",
      "Epoch 21404/30000 Training Loss: 0.07218065112829208\n",
      "Epoch 21405/30000 Training Loss: 0.06464140117168427\n",
      "Epoch 21406/30000 Training Loss: 0.07973784953355789\n",
      "Epoch 21407/30000 Training Loss: 0.0639524757862091\n",
      "Epoch 21408/30000 Training Loss: 0.07229578495025635\n",
      "Epoch 21409/30000 Training Loss: 0.09627670049667358\n",
      "Epoch 21410/30000 Training Loss: 0.06187857314944267\n",
      "Epoch 21410/30000 Validation Loss: 0.07461771368980408\n",
      "Epoch 21411/30000 Training Loss: 0.07836554199457169\n",
      "Epoch 21412/30000 Training Loss: 0.07242671400308609\n",
      "Epoch 21413/30000 Training Loss: 0.07605131715536118\n",
      "Epoch 21414/30000 Training Loss: 0.07313455641269684\n",
      "Epoch 21415/30000 Training Loss: 0.06186420097947121\n",
      "Epoch 21416/30000 Training Loss: 0.05276046320796013\n",
      "Epoch 21417/30000 Training Loss: 0.0781913474202156\n",
      "Epoch 21418/30000 Training Loss: 0.059531327337026596\n",
      "Epoch 21419/30000 Training Loss: 0.0668579712510109\n",
      "Epoch 21420/30000 Training Loss: 0.07309811562299728\n",
      "Epoch 21420/30000 Validation Loss: 0.07106494158506393\n",
      "Epoch 21421/30000 Training Loss: 0.06592050939798355\n",
      "Epoch 21422/30000 Training Loss: 0.07176775485277176\n",
      "Epoch 21423/30000 Training Loss: 0.06144978478550911\n",
      "Epoch 21424/30000 Training Loss: 0.05566033348441124\n",
      "Epoch 21425/30000 Training Loss: 0.0829692929983139\n",
      "Epoch 21426/30000 Training Loss: 0.0786813423037529\n",
      "Epoch 21427/30000 Training Loss: 0.06322316080331802\n",
      "Epoch 21428/30000 Training Loss: 0.05916362628340721\n",
      "Epoch 21429/30000 Training Loss: 0.07783126085996628\n",
      "Epoch 21430/30000 Training Loss: 0.08192163705825806\n",
      "Epoch 21430/30000 Validation Loss: 0.06556611508131027\n",
      "Epoch 21431/30000 Training Loss: 0.07221857458353043\n",
      "Epoch 21432/30000 Training Loss: 0.05973267927765846\n",
      "Epoch 21433/30000 Training Loss: 0.055558253079652786\n",
      "Epoch 21434/30000 Training Loss: 0.06951847672462463\n",
      "Epoch 21435/30000 Training Loss: 0.08475177735090256\n",
      "Epoch 21436/30000 Training Loss: 0.09573512524366379\n",
      "Epoch 21437/30000 Training Loss: 0.07139497995376587\n",
      "Epoch 21438/30000 Training Loss: 0.06971890479326248\n",
      "Epoch 21439/30000 Training Loss: 0.06503968685865402\n",
      "Epoch 21440/30000 Training Loss: 0.0672794058918953\n",
      "Epoch 21440/30000 Validation Loss: 0.08041062206029892\n",
      "Epoch 21441/30000 Training Loss: 0.08059561997652054\n",
      "Epoch 21442/30000 Training Loss: 0.06949032098054886\n",
      "Epoch 21443/30000 Training Loss: 0.06834866851568222\n",
      "Epoch 21444/30000 Training Loss: 0.05411095917224884\n",
      "Epoch 21445/30000 Training Loss: 0.0714593157172203\n",
      "Epoch 21446/30000 Training Loss: 0.09071243554353714\n",
      "Epoch 21447/30000 Training Loss: 0.07026347517967224\n",
      "Epoch 21448/30000 Training Loss: 0.08416804671287537\n",
      "Epoch 21449/30000 Training Loss: 0.06600537151098251\n",
      "Epoch 21450/30000 Training Loss: 0.06905268877744675\n",
      "Epoch 21450/30000 Validation Loss: 0.07108920067548752\n",
      "Epoch 21451/30000 Training Loss: 0.0705626830458641\n",
      "Epoch 21452/30000 Training Loss: 0.06673204898834229\n",
      "Epoch 21453/30000 Training Loss: 0.07342854142189026\n",
      "Epoch 21454/30000 Training Loss: 0.0783170685172081\n",
      "Epoch 21455/30000 Training Loss: 0.07236522436141968\n",
      "Epoch 21456/30000 Training Loss: 0.06316009908914566\n",
      "Epoch 21457/30000 Training Loss: 0.07861334085464478\n",
      "Epoch 21458/30000 Training Loss: 0.06879974156618118\n",
      "Epoch 21459/30000 Training Loss: 0.08439403772354126\n",
      "Epoch 21460/30000 Training Loss: 0.07062686234712601\n",
      "Epoch 21460/30000 Validation Loss: 0.06092110276222229\n",
      "Epoch 21461/30000 Training Loss: 0.09029475599527359\n",
      "Epoch 21462/30000 Training Loss: 0.06829250603914261\n",
      "Epoch 21463/30000 Training Loss: 0.07827157527208328\n",
      "Epoch 21464/30000 Training Loss: 0.06084328517317772\n",
      "Epoch 21465/30000 Training Loss: 0.0717303454875946\n",
      "Epoch 21466/30000 Training Loss: 0.07108521461486816\n",
      "Epoch 21467/30000 Training Loss: 0.06703851372003555\n",
      "Epoch 21468/30000 Training Loss: 0.06571879237890244\n",
      "Epoch 21469/30000 Training Loss: 0.05362047627568245\n",
      "Epoch 21470/30000 Training Loss: 0.06088680401444435\n",
      "Epoch 21470/30000 Validation Loss: 0.07556495815515518\n",
      "Epoch 21471/30000 Training Loss: 0.08856034278869629\n",
      "Epoch 21472/30000 Training Loss: 0.06940040737390518\n",
      "Epoch 21473/30000 Training Loss: 0.06002984941005707\n",
      "Epoch 21474/30000 Training Loss: 0.07300259917974472\n",
      "Epoch 21475/30000 Training Loss: 0.0579683892428875\n",
      "Epoch 21476/30000 Training Loss: 0.07571573555469513\n",
      "Epoch 21477/30000 Training Loss: 0.07289241999387741\n",
      "Epoch 21478/30000 Training Loss: 0.08401817828416824\n",
      "Epoch 21479/30000 Training Loss: 0.07899592071771622\n",
      "Epoch 21480/30000 Training Loss: 0.08945146203041077\n",
      "Epoch 21480/30000 Validation Loss: 0.0669335350394249\n",
      "Epoch 21481/30000 Training Loss: 0.06667938083410263\n",
      "Epoch 21482/30000 Training Loss: 0.07445674389600754\n",
      "Epoch 21483/30000 Training Loss: 0.07528414577245712\n",
      "Epoch 21484/30000 Training Loss: 0.0704488679766655\n",
      "Epoch 21485/30000 Training Loss: 0.07924263924360275\n",
      "Epoch 21486/30000 Training Loss: 0.07547608762979507\n",
      "Epoch 21487/30000 Training Loss: 0.07381632179021835\n",
      "Epoch 21488/30000 Training Loss: 0.06371565163135529\n",
      "Epoch 21489/30000 Training Loss: 0.06259097903966904\n",
      "Epoch 21490/30000 Training Loss: 0.05957403779029846\n",
      "Epoch 21490/30000 Validation Loss: 0.05516059324145317\n",
      "Epoch 21491/30000 Training Loss: 0.055753882974386215\n",
      "Epoch 21492/30000 Training Loss: 0.0851222351193428\n",
      "Epoch 21493/30000 Training Loss: 0.08105600625276566\n",
      "Epoch 21494/30000 Training Loss: 0.06422185152769089\n",
      "Epoch 21495/30000 Training Loss: 0.08009764552116394\n",
      "Epoch 21496/30000 Training Loss: 0.07039319723844528\n",
      "Epoch 21497/30000 Training Loss: 0.06814586371183395\n",
      "Epoch 21498/30000 Training Loss: 0.07018724828958511\n",
      "Epoch 21499/30000 Training Loss: 0.08556593209505081\n",
      "Epoch 21500/30000 Training Loss: 0.077680803835392\n",
      "Epoch 21500/30000 Validation Loss: 0.06724098324775696\n",
      "Epoch 21501/30000 Training Loss: 0.07694648206233978\n",
      "Epoch 21502/30000 Training Loss: 0.06318005174398422\n",
      "Epoch 21503/30000 Training Loss: 0.06480558961629868\n",
      "Epoch 21504/30000 Training Loss: 0.07035868614912033\n",
      "Epoch 21505/30000 Training Loss: 0.07024331390857697\n",
      "Epoch 21506/30000 Training Loss: 0.07674342393875122\n",
      "Epoch 21507/30000 Training Loss: 0.08055511862039566\n",
      "Epoch 21508/30000 Training Loss: 0.06469223648309708\n",
      "Epoch 21509/30000 Training Loss: 0.06024618819355965\n",
      "Epoch 21510/30000 Training Loss: 0.07091846317052841\n",
      "Epoch 21510/30000 Validation Loss: 0.06204672157764435\n",
      "Epoch 21511/30000 Training Loss: 0.07629755139350891\n",
      "Epoch 21512/30000 Training Loss: 0.05540521442890167\n",
      "Epoch 21513/30000 Training Loss: 0.08012518286705017\n",
      "Epoch 21514/30000 Training Loss: 0.07417405396699905\n",
      "Epoch 21515/30000 Training Loss: 0.06317669153213501\n",
      "Epoch 21516/30000 Training Loss: 0.055860210210084915\n",
      "Epoch 21517/30000 Training Loss: 0.058537423610687256\n",
      "Epoch 21518/30000 Training Loss: 0.0663975402712822\n",
      "Epoch 21519/30000 Training Loss: 0.06835594028234482\n",
      "Epoch 21520/30000 Training Loss: 0.0770438089966774\n",
      "Epoch 21520/30000 Validation Loss: 0.07445941120386124\n",
      "Epoch 21521/30000 Training Loss: 0.07890846580266953\n",
      "Epoch 21522/30000 Training Loss: 0.06484951823949814\n",
      "Epoch 21523/30000 Training Loss: 0.07840355485677719\n",
      "Epoch 21524/30000 Training Loss: 0.0689135268330574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21525/30000 Training Loss: 0.06301788240671158\n",
      "Epoch 21526/30000 Training Loss: 0.08054964989423752\n",
      "Epoch 21527/30000 Training Loss: 0.06891448050737381\n",
      "Epoch 21528/30000 Training Loss: 0.06589857488870621\n",
      "Epoch 21529/30000 Training Loss: 0.06425962597131729\n",
      "Epoch 21530/30000 Training Loss: 0.07248056679964066\n",
      "Epoch 21530/30000 Validation Loss: 0.06475820392370224\n",
      "Epoch 21531/30000 Training Loss: 0.08312731236219406\n",
      "Epoch 21532/30000 Training Loss: 0.04391050711274147\n",
      "Epoch 21533/30000 Training Loss: 0.06186037138104439\n",
      "Epoch 21534/30000 Training Loss: 0.06867220252752304\n",
      "Epoch 21535/30000 Training Loss: 0.07865682244300842\n",
      "Epoch 21536/30000 Training Loss: 0.06780295819044113\n",
      "Epoch 21537/30000 Training Loss: 0.07678011059761047\n",
      "Epoch 21538/30000 Training Loss: 0.07617197185754776\n",
      "Epoch 21539/30000 Training Loss: 0.06305676698684692\n",
      "Epoch 21540/30000 Training Loss: 0.07638020068407059\n",
      "Epoch 21540/30000 Validation Loss: 0.05744903162121773\n",
      "Epoch 21541/30000 Training Loss: 0.06473976373672485\n",
      "Epoch 21542/30000 Training Loss: 0.060525696724653244\n",
      "Epoch 21543/30000 Training Loss: 0.08380556106567383\n",
      "Epoch 21544/30000 Training Loss: 0.05048438906669617\n",
      "Epoch 21545/30000 Training Loss: 0.07551278173923492\n",
      "Epoch 21546/30000 Training Loss: 0.06084143742918968\n",
      "Epoch 21547/30000 Training Loss: 0.0716433897614479\n",
      "Epoch 21548/30000 Training Loss: 0.06698843091726303\n",
      "Epoch 21549/30000 Training Loss: 0.05072671175003052\n",
      "Epoch 21550/30000 Training Loss: 0.07492648810148239\n",
      "Epoch 21550/30000 Validation Loss: 0.06050708517432213\n",
      "Epoch 21551/30000 Training Loss: 0.09704822301864624\n",
      "Epoch 21552/30000 Training Loss: 0.06461822241544724\n",
      "Epoch 21553/30000 Training Loss: 0.08353783935308456\n",
      "Epoch 21554/30000 Training Loss: 0.059103235602378845\n",
      "Epoch 21555/30000 Training Loss: 0.0650162324309349\n",
      "Epoch 21556/30000 Training Loss: 0.06934285908937454\n",
      "Epoch 21557/30000 Training Loss: 0.06641367077827454\n",
      "Epoch 21558/30000 Training Loss: 0.06305617839097977\n",
      "Epoch 21559/30000 Training Loss: 0.062315985560417175\n",
      "Epoch 21560/30000 Training Loss: 0.08066975325345993\n",
      "Epoch 21560/30000 Validation Loss: 0.07757806032896042\n",
      "Epoch 21561/30000 Training Loss: 0.08832169324159622\n",
      "Epoch 21562/30000 Training Loss: 0.07900597900152206\n",
      "Epoch 21563/30000 Training Loss: 0.06713569164276123\n",
      "Epoch 21564/30000 Training Loss: 0.08342032879590988\n",
      "Epoch 21565/30000 Training Loss: 0.0585562102496624\n",
      "Epoch 21566/30000 Training Loss: 0.06293559074401855\n",
      "Epoch 21567/30000 Training Loss: 0.0733172595500946\n",
      "Epoch 21568/30000 Training Loss: 0.08635600656270981\n",
      "Epoch 21569/30000 Training Loss: 0.0739406868815422\n",
      "Epoch 21570/30000 Training Loss: 0.07296300679445267\n",
      "Epoch 21570/30000 Validation Loss: 0.060943204909563065\n",
      "Epoch 21571/30000 Training Loss: 0.08042161911725998\n",
      "Epoch 21572/30000 Training Loss: 0.07936733216047287\n",
      "Epoch 21573/30000 Training Loss: 0.0769113227725029\n",
      "Epoch 21574/30000 Training Loss: 0.06337427347898483\n",
      "Epoch 21575/30000 Training Loss: 0.09274496883153915\n",
      "Epoch 21576/30000 Training Loss: 0.06840381771326065\n",
      "Epoch 21577/30000 Training Loss: 0.08028963953256607\n",
      "Epoch 21578/30000 Training Loss: 0.07385066896677017\n",
      "Epoch 21579/30000 Training Loss: 0.07173901051282883\n",
      "Epoch 21580/30000 Training Loss: 0.06073050573468208\n",
      "Epoch 21580/30000 Validation Loss: 0.06084899976849556\n",
      "Epoch 21581/30000 Training Loss: 0.08242615312337875\n",
      "Epoch 21582/30000 Training Loss: 0.07859262824058533\n",
      "Epoch 21583/30000 Training Loss: 0.07840577512979507\n",
      "Epoch 21584/30000 Training Loss: 0.04968330264091492\n",
      "Epoch 21585/30000 Training Loss: 0.0790465772151947\n",
      "Epoch 21586/30000 Training Loss: 0.10302319377660751\n",
      "Epoch 21587/30000 Training Loss: 0.06736830621957779\n",
      "Epoch 21588/30000 Training Loss: 0.06334217637777328\n",
      "Epoch 21589/30000 Training Loss: 0.07844165712594986\n",
      "Epoch 21590/30000 Training Loss: 0.06778943538665771\n",
      "Epoch 21590/30000 Validation Loss: 0.07141947746276855\n",
      "Epoch 21591/30000 Training Loss: 0.06778047233819962\n",
      "Epoch 21592/30000 Training Loss: 0.07113050669431686\n",
      "Epoch 21593/30000 Training Loss: 0.07248114794492722\n",
      "Epoch 21594/30000 Training Loss: 0.057730864733457565\n",
      "Epoch 21595/30000 Training Loss: 0.07435020059347153\n",
      "Epoch 21596/30000 Training Loss: 0.07412290573120117\n",
      "Epoch 21597/30000 Training Loss: 0.08230113238096237\n",
      "Epoch 21598/30000 Training Loss: 0.07362239807844162\n",
      "Epoch 21599/30000 Training Loss: 0.0766424611210823\n",
      "Epoch 21600/30000 Training Loss: 0.06875142455101013\n",
      "Epoch 21600/30000 Validation Loss: 0.06268555670976639\n",
      "Epoch 21601/30000 Training Loss: 0.07161165028810501\n",
      "Epoch 21602/30000 Training Loss: 0.0701868087053299\n",
      "Epoch 21603/30000 Training Loss: 0.06459834426641464\n",
      "Epoch 21604/30000 Training Loss: 0.06592122465372086\n",
      "Epoch 21605/30000 Training Loss: 0.07487063854932785\n",
      "Epoch 21606/30000 Training Loss: 0.06421684473752975\n",
      "Epoch 21607/30000 Training Loss: 0.07945653796195984\n",
      "Epoch 21608/30000 Training Loss: 0.05217645689845085\n",
      "Epoch 21609/30000 Training Loss: 0.08524414896965027\n",
      "Epoch 21610/30000 Training Loss: 0.06592553853988647\n",
      "Epoch 21610/30000 Validation Loss: 0.07014460861682892\n",
      "Epoch 21611/30000 Training Loss: 0.06769129633903503\n",
      "Epoch 21612/30000 Training Loss: 0.07448866218328476\n",
      "Epoch 21613/30000 Training Loss: 0.060440611094236374\n",
      "Epoch 21614/30000 Training Loss: 0.065296970307827\n",
      "Epoch 21615/30000 Training Loss: 0.042644381523132324\n",
      "Epoch 21616/30000 Training Loss: 0.06941860169172287\n",
      "Epoch 21617/30000 Training Loss: 0.07273916155099869\n",
      "Epoch 21618/30000 Training Loss: 0.0714680626988411\n",
      "Epoch 21619/30000 Training Loss: 0.06797315925359726\n",
      "Epoch 21620/30000 Training Loss: 0.06548161059617996\n",
      "Epoch 21620/30000 Validation Loss: 0.07921428233385086\n",
      "Epoch 21621/30000 Training Loss: 0.0704839900135994\n",
      "Epoch 21622/30000 Training Loss: 0.06531546264886856\n",
      "Epoch 21623/30000 Training Loss: 0.06327974051237106\n",
      "Epoch 21624/30000 Training Loss: 0.05831591412425041\n",
      "Epoch 21625/30000 Training Loss: 0.06458478420972824\n",
      "Epoch 21626/30000 Training Loss: 0.05276854336261749\n",
      "Epoch 21627/30000 Training Loss: 0.07525341957807541\n",
      "Epoch 21628/30000 Training Loss: 0.06181718781590462\n",
      "Epoch 21629/30000 Training Loss: 0.07109607011079788\n",
      "Epoch 21630/30000 Training Loss: 0.07129797339439392\n",
      "Epoch 21630/30000 Validation Loss: 0.06973898410797119\n",
      "Epoch 21631/30000 Training Loss: 0.08773627877235413\n",
      "Epoch 21632/30000 Training Loss: 0.05680824816226959\n",
      "Epoch 21633/30000 Training Loss: 0.06759781390428543\n",
      "Epoch 21634/30000 Training Loss: 0.06330874562263489\n",
      "Epoch 21635/30000 Training Loss: 0.08304014056921005\n",
      "Epoch 21636/30000 Training Loss: 0.06889673322439194\n",
      "Epoch 21637/30000 Training Loss: 0.07880941778421402\n",
      "Epoch 21638/30000 Training Loss: 0.06474313884973526\n",
      "Epoch 21639/30000 Training Loss: 0.0747385323047638\n",
      "Epoch 21640/30000 Training Loss: 0.07113348692655563\n",
      "Epoch 21640/30000 Validation Loss: 0.06524239480495453\n",
      "Epoch 21641/30000 Training Loss: 0.0535050630569458\n",
      "Epoch 21642/30000 Training Loss: 0.07708871364593506\n",
      "Epoch 21643/30000 Training Loss: 0.06518256664276123\n",
      "Epoch 21644/30000 Training Loss: 0.08065158873796463\n",
      "Epoch 21645/30000 Training Loss: 0.07229918241500854\n",
      "Epoch 21646/30000 Training Loss: 0.07481054216623306\n",
      "Epoch 21647/30000 Training Loss: 0.06513101607561111\n",
      "Epoch 21648/30000 Training Loss: 0.08340396732091904\n",
      "Epoch 21649/30000 Training Loss: 0.05059865117073059\n",
      "Epoch 21650/30000 Training Loss: 0.06275101751089096\n",
      "Epoch 21650/30000 Validation Loss: 0.07690855115652084\n",
      "Epoch 21651/30000 Training Loss: 0.07806076854467392\n",
      "Epoch 21652/30000 Training Loss: 0.04962671920657158\n",
      "Epoch 21653/30000 Training Loss: 0.08350307494401932\n",
      "Epoch 21654/30000 Training Loss: 0.07146108895540237\n",
      "Epoch 21655/30000 Training Loss: 0.06508830934762955\n",
      "Epoch 21656/30000 Training Loss: 0.06408198922872543\n",
      "Epoch 21657/30000 Training Loss: 0.09016195684671402\n",
      "Epoch 21658/30000 Training Loss: 0.0639929473400116\n",
      "Epoch 21659/30000 Training Loss: 0.0642576590180397\n",
      "Epoch 21660/30000 Training Loss: 0.0693133994936943\n",
      "Epoch 21660/30000 Validation Loss: 0.07550818473100662\n",
      "Epoch 21661/30000 Training Loss: 0.08149737119674683\n",
      "Epoch 21662/30000 Training Loss: 0.0792469009757042\n",
      "Epoch 21663/30000 Training Loss: 0.0521286278963089\n",
      "Epoch 21664/30000 Training Loss: 0.0828699991106987\n",
      "Epoch 21665/30000 Training Loss: 0.070993572473526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21666/30000 Training Loss: 0.06759709864854813\n",
      "Epoch 21667/30000 Training Loss: 0.06756825000047684\n",
      "Epoch 21668/30000 Training Loss: 0.08280918747186661\n",
      "Epoch 21669/30000 Training Loss: 0.05924993380904198\n",
      "Epoch 21670/30000 Training Loss: 0.07249032706022263\n",
      "Epoch 21670/30000 Validation Loss: 0.08570095151662827\n",
      "Epoch 21671/30000 Training Loss: 0.07824012637138367\n",
      "Epoch 21672/30000 Training Loss: 0.05519704893231392\n",
      "Epoch 21673/30000 Training Loss: 0.057300399988889694\n",
      "Epoch 21674/30000 Training Loss: 0.068927101790905\n",
      "Epoch 21675/30000 Training Loss: 0.06200229004025459\n",
      "Epoch 21676/30000 Training Loss: 0.08911404758691788\n",
      "Epoch 21677/30000 Training Loss: 0.055620547384023666\n",
      "Epoch 21678/30000 Training Loss: 0.05257434770464897\n",
      "Epoch 21679/30000 Training Loss: 0.06883791834115982\n",
      "Epoch 21680/30000 Training Loss: 0.06431522965431213\n",
      "Epoch 21680/30000 Validation Loss: 0.06894686073064804\n",
      "Epoch 21681/30000 Training Loss: 0.07238509505987167\n",
      "Epoch 21682/30000 Training Loss: 0.07744399458169937\n",
      "Epoch 21683/30000 Training Loss: 0.06192447617650032\n",
      "Epoch 21684/30000 Training Loss: 0.07419002056121826\n",
      "Epoch 21685/30000 Training Loss: 0.08473217487335205\n",
      "Epoch 21686/30000 Training Loss: 0.06314555555582047\n",
      "Epoch 21687/30000 Training Loss: 0.0793764516711235\n",
      "Epoch 21688/30000 Training Loss: 0.07942094653844833\n",
      "Epoch 21689/30000 Training Loss: 0.06486289948225021\n",
      "Epoch 21690/30000 Training Loss: 0.06979243457317352\n",
      "Epoch 21690/30000 Validation Loss: 0.07449764758348465\n",
      "Epoch 21691/30000 Training Loss: 0.08950085192918777\n",
      "Epoch 21692/30000 Training Loss: 0.07029005140066147\n",
      "Epoch 21693/30000 Training Loss: 0.07789402455091476\n",
      "Epoch 21694/30000 Training Loss: 0.052973899990320206\n",
      "Epoch 21695/30000 Training Loss: 0.07365398108959198\n",
      "Epoch 21696/30000 Training Loss: 0.08098731935024261\n",
      "Epoch 21697/30000 Training Loss: 0.07393113523721695\n",
      "Epoch 21698/30000 Training Loss: 0.08245643228292465\n",
      "Epoch 21699/30000 Training Loss: 0.042250461876392365\n",
      "Epoch 21700/30000 Training Loss: 0.062471698969602585\n",
      "Epoch 21700/30000 Validation Loss: 0.06042501702904701\n",
      "Epoch 21701/30000 Training Loss: 0.058278415352106094\n",
      "Epoch 21702/30000 Training Loss: 0.08146744966506958\n",
      "Epoch 21703/30000 Training Loss: 0.06361345201730728\n",
      "Epoch 21704/30000 Training Loss: 0.08114472776651382\n",
      "Epoch 21705/30000 Training Loss: 0.05927029252052307\n",
      "Epoch 21706/30000 Training Loss: 0.07340788096189499\n",
      "Epoch 21707/30000 Training Loss: 0.08091133832931519\n",
      "Epoch 21708/30000 Training Loss: 0.06474027037620544\n",
      "Epoch 21709/30000 Training Loss: 0.07481280714273453\n",
      "Epoch 21710/30000 Training Loss: 0.07791858166456223\n",
      "Epoch 21710/30000 Validation Loss: 0.06519605219364166\n",
      "Epoch 21711/30000 Training Loss: 0.05634918808937073\n",
      "Epoch 21712/30000 Training Loss: 0.07060720771551132\n",
      "Epoch 21713/30000 Training Loss: 0.06612808257341385\n",
      "Epoch 21714/30000 Training Loss: 0.06763409078121185\n",
      "Epoch 21715/30000 Training Loss: 0.06148843467235565\n",
      "Epoch 21716/30000 Training Loss: 0.05754660442471504\n",
      "Epoch 21717/30000 Training Loss: 0.0767114982008934\n",
      "Epoch 21718/30000 Training Loss: 0.07574307173490524\n",
      "Epoch 21719/30000 Training Loss: 0.07193928956985474\n",
      "Epoch 21720/30000 Training Loss: 0.07002320140600204\n",
      "Epoch 21720/30000 Validation Loss: 0.07418876886367798\n",
      "Epoch 21721/30000 Training Loss: 0.0690927505493164\n",
      "Epoch 21722/30000 Training Loss: 0.04865635558962822\n",
      "Epoch 21723/30000 Training Loss: 0.05722011998295784\n",
      "Epoch 21724/30000 Training Loss: 0.07121500372886658\n",
      "Epoch 21725/30000 Training Loss: 0.07616760581731796\n",
      "Epoch 21726/30000 Training Loss: 0.06426157802343369\n",
      "Epoch 21727/30000 Training Loss: 0.06626642495393753\n",
      "Epoch 21728/30000 Training Loss: 0.056839898228645325\n",
      "Epoch 21729/30000 Training Loss: 0.06991633027791977\n",
      "Epoch 21730/30000 Training Loss: 0.055643230676651\n",
      "Epoch 21730/30000 Validation Loss: 0.0835171639919281\n",
      "Epoch 21731/30000 Training Loss: 0.06162278354167938\n",
      "Epoch 21732/30000 Training Loss: 0.07812219858169556\n",
      "Epoch 21733/30000 Training Loss: 0.05632825195789337\n",
      "Epoch 21734/30000 Training Loss: 0.08370482921600342\n",
      "Epoch 21735/30000 Training Loss: 0.05349789559841156\n",
      "Epoch 21736/30000 Training Loss: 0.06151922419667244\n",
      "Epoch 21737/30000 Training Loss: 0.06371871381998062\n",
      "Epoch 21738/30000 Training Loss: 0.06222720071673393\n",
      "Epoch 21739/30000 Training Loss: 0.0832333192229271\n",
      "Epoch 21740/30000 Training Loss: 0.05760281905531883\n",
      "Epoch 21740/30000 Validation Loss: 0.07977306842803955\n",
      "Epoch 21741/30000 Training Loss: 0.06549809128046036\n",
      "Epoch 21742/30000 Training Loss: 0.05471291020512581\n",
      "Epoch 21743/30000 Training Loss: 0.07365533709526062\n",
      "Epoch 21744/30000 Training Loss: 0.07708190381526947\n",
      "Epoch 21745/30000 Training Loss: 0.06615066528320312\n",
      "Epoch 21746/30000 Training Loss: 0.07288093119859695\n",
      "Epoch 21747/30000 Training Loss: 0.06387083977460861\n",
      "Epoch 21748/30000 Training Loss: 0.07750781625509262\n",
      "Epoch 21749/30000 Training Loss: 0.05660561844706535\n",
      "Epoch 21750/30000 Training Loss: 0.07699549198150635\n",
      "Epoch 21750/30000 Validation Loss: 0.08454661816358566\n",
      "Epoch 21751/30000 Training Loss: 0.08228212594985962\n",
      "Epoch 21752/30000 Training Loss: 0.06508266925811768\n",
      "Epoch 21753/30000 Training Loss: 0.07402912527322769\n",
      "Epoch 21754/30000 Training Loss: 0.08069619536399841\n",
      "Epoch 21755/30000 Training Loss: 0.0698542669415474\n",
      "Epoch 21756/30000 Training Loss: 0.07865265756845474\n",
      "Epoch 21757/30000 Training Loss: 0.06279339641332626\n",
      "Epoch 21758/30000 Training Loss: 0.05679774284362793\n",
      "Epoch 21759/30000 Training Loss: 0.07570692151784897\n",
      "Epoch 21760/30000 Training Loss: 0.08258897811174393\n",
      "Epoch 21760/30000 Validation Loss: 0.07173792272806168\n",
      "Epoch 21761/30000 Training Loss: 0.06633713096380234\n",
      "Epoch 21762/30000 Training Loss: 0.06098087131977081\n",
      "Epoch 21763/30000 Training Loss: 0.07421276718378067\n",
      "Epoch 21764/30000 Training Loss: 0.08118433505296707\n",
      "Epoch 21765/30000 Training Loss: 0.05407220125198364\n",
      "Epoch 21766/30000 Training Loss: 0.07845005393028259\n",
      "Epoch 21767/30000 Training Loss: 0.09359804540872574\n",
      "Epoch 21768/30000 Training Loss: 0.09039902687072754\n",
      "Epoch 21769/30000 Training Loss: 0.07473382353782654\n",
      "Epoch 21770/30000 Training Loss: 0.08158629387617111\n",
      "Epoch 21770/30000 Validation Loss: 0.08831757307052612\n",
      "Epoch 21771/30000 Training Loss: 0.09180717915296555\n",
      "Epoch 21772/30000 Training Loss: 0.08147043734788895\n",
      "Epoch 21773/30000 Training Loss: 0.09158468246459961\n",
      "Epoch 21774/30000 Training Loss: 0.09289994090795517\n",
      "Epoch 21775/30000 Training Loss: 0.06762445718050003\n",
      "Epoch 21776/30000 Training Loss: 0.06764408946037292\n",
      "Epoch 21777/30000 Training Loss: 0.08118811994791031\n",
      "Epoch 21778/30000 Training Loss: 0.06747577339410782\n",
      "Epoch 21779/30000 Training Loss: 0.07106871157884598\n",
      "Epoch 21780/30000 Training Loss: 0.08398864418268204\n",
      "Epoch 21780/30000 Validation Loss: 0.07558500021696091\n",
      "Epoch 21781/30000 Training Loss: 0.08089488744735718\n",
      "Epoch 21782/30000 Training Loss: 0.0624137818813324\n",
      "Epoch 21783/30000 Training Loss: 0.0651598647236824\n",
      "Epoch 21784/30000 Training Loss: 0.05480281636118889\n",
      "Epoch 21785/30000 Training Loss: 0.06724684685468674\n",
      "Epoch 21786/30000 Training Loss: 0.05685349181294441\n",
      "Epoch 21787/30000 Training Loss: 0.07060382515192032\n",
      "Epoch 21788/30000 Training Loss: 0.07233471423387527\n",
      "Epoch 21789/30000 Training Loss: 0.06764928251504898\n",
      "Epoch 21790/30000 Training Loss: 0.07562366127967834\n",
      "Epoch 21790/30000 Validation Loss: 0.07528188824653625\n",
      "Epoch 21791/30000 Training Loss: 0.05807038024067879\n",
      "Epoch 21792/30000 Training Loss: 0.09884419292211533\n",
      "Epoch 21793/30000 Training Loss: 0.0681767463684082\n",
      "Epoch 21794/30000 Training Loss: 0.07209429889917374\n",
      "Epoch 21795/30000 Training Loss: 0.06405413895845413\n",
      "Epoch 21796/30000 Training Loss: 0.08265051245689392\n",
      "Epoch 21797/30000 Training Loss: 0.08153869956731796\n",
      "Epoch 21798/30000 Training Loss: 0.07938791066408157\n",
      "Epoch 21799/30000 Training Loss: 0.07487950474023819\n",
      "Epoch 21800/30000 Training Loss: 0.06727909296751022\n",
      "Epoch 21800/30000 Validation Loss: 0.06533090770244598\n",
      "Epoch 21801/30000 Training Loss: 0.0652315765619278\n",
      "Epoch 21802/30000 Training Loss: 0.07953471690416336\n",
      "Epoch 21803/30000 Training Loss: 0.04849666357040405\n",
      "Epoch 21804/30000 Training Loss: 0.06024381145834923\n",
      "Epoch 21805/30000 Training Loss: 0.06565762311220169\n",
      "Epoch 21806/30000 Training Loss: 0.06252805143594742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21807/30000 Training Loss: 0.0726773738861084\n",
      "Epoch 21808/30000 Training Loss: 0.07406655699014664\n",
      "Epoch 21809/30000 Training Loss: 0.0695594847202301\n",
      "Epoch 21810/30000 Training Loss: 0.08235481381416321\n",
      "Epoch 21810/30000 Validation Loss: 0.07014749199151993\n",
      "Epoch 21811/30000 Training Loss: 0.053090859204530716\n",
      "Epoch 21812/30000 Training Loss: 0.047263581305742264\n",
      "Epoch 21813/30000 Training Loss: 0.06589057296514511\n",
      "Epoch 21814/30000 Training Loss: 0.07957181334495544\n",
      "Epoch 21815/30000 Training Loss: 0.06650514900684357\n",
      "Epoch 21816/30000 Training Loss: 0.05772804841399193\n",
      "Epoch 21817/30000 Training Loss: 0.04952206090092659\n",
      "Epoch 21818/30000 Training Loss: 0.07720131427049637\n",
      "Epoch 21819/30000 Training Loss: 0.061343997716903687\n",
      "Epoch 21820/30000 Training Loss: 0.0686061903834343\n",
      "Epoch 21820/30000 Validation Loss: 0.07058250904083252\n",
      "Epoch 21821/30000 Training Loss: 0.08254634588956833\n",
      "Epoch 21822/30000 Training Loss: 0.06484732776880264\n",
      "Epoch 21823/30000 Training Loss: 0.06777257472276688\n",
      "Epoch 21824/30000 Training Loss: 0.09596248716115952\n",
      "Epoch 21825/30000 Training Loss: 0.08403602987527847\n",
      "Epoch 21826/30000 Training Loss: 0.049821510910987854\n",
      "Epoch 21827/30000 Training Loss: 0.05338260903954506\n",
      "Epoch 21828/30000 Training Loss: 0.07183116674423218\n",
      "Epoch 21829/30000 Training Loss: 0.06131249666213989\n",
      "Epoch 21830/30000 Training Loss: 0.06212250515818596\n",
      "Epoch 21830/30000 Validation Loss: 0.07702185958623886\n",
      "Epoch 21831/30000 Training Loss: 0.0765741690993309\n",
      "Epoch 21832/30000 Training Loss: 0.055196698755025864\n",
      "Epoch 21833/30000 Training Loss: 0.07891187816858292\n",
      "Epoch 21834/30000 Training Loss: 0.06907284259796143\n",
      "Epoch 21835/30000 Training Loss: 0.07626911997795105\n",
      "Epoch 21836/30000 Training Loss: 0.07189588993787766\n",
      "Epoch 21837/30000 Training Loss: 0.09417015314102173\n",
      "Epoch 21838/30000 Training Loss: 0.07614689320325851\n",
      "Epoch 21839/30000 Training Loss: 0.05904390290379524\n",
      "Epoch 21840/30000 Training Loss: 0.08002665638923645\n",
      "Epoch 21840/30000 Validation Loss: 0.07495709508657455\n",
      "Epoch 21841/30000 Training Loss: 0.06743103265762329\n",
      "Epoch 21842/30000 Training Loss: 0.07803130894899368\n",
      "Epoch 21843/30000 Training Loss: 0.06815249472856522\n",
      "Epoch 21844/30000 Training Loss: 0.0518234521150589\n",
      "Epoch 21845/30000 Training Loss: 0.06689850986003876\n",
      "Epoch 21846/30000 Training Loss: 0.07002746313810349\n",
      "Epoch 21847/30000 Training Loss: 0.06228600814938545\n",
      "Epoch 21848/30000 Training Loss: 0.08259999006986618\n",
      "Epoch 21849/30000 Training Loss: 0.05508008599281311\n",
      "Epoch 21850/30000 Training Loss: 0.08102750778198242\n",
      "Epoch 21850/30000 Validation Loss: 0.056105028837919235\n",
      "Epoch 21851/30000 Training Loss: 0.08348837494850159\n",
      "Epoch 21852/30000 Training Loss: 0.08427441120147705\n",
      "Epoch 21853/30000 Training Loss: 0.09181684255599976\n",
      "Epoch 21854/30000 Training Loss: 0.06834419816732407\n",
      "Epoch 21855/30000 Training Loss: 0.08579650521278381\n",
      "Epoch 21856/30000 Training Loss: 0.05542951822280884\n",
      "Epoch 21857/30000 Training Loss: 0.06825418025255203\n",
      "Epoch 21858/30000 Training Loss: 0.09598157554864883\n",
      "Epoch 21859/30000 Training Loss: 0.09136835485696793\n",
      "Epoch 21860/30000 Training Loss: 0.09111106395721436\n",
      "Epoch 21860/30000 Validation Loss: 0.08345667272806168\n",
      "Epoch 21861/30000 Training Loss: 0.06747154891490936\n",
      "Epoch 21862/30000 Training Loss: 0.08268073201179504\n",
      "Epoch 21863/30000 Training Loss: 0.06742697954177856\n",
      "Epoch 21864/30000 Training Loss: 0.0725303366780281\n",
      "Epoch 21865/30000 Training Loss: 0.06718869507312775\n",
      "Epoch 21866/30000 Training Loss: 0.08038153499364853\n",
      "Epoch 21867/30000 Training Loss: 0.06300342082977295\n",
      "Epoch 21868/30000 Training Loss: 0.07004103809595108\n",
      "Epoch 21869/30000 Training Loss: 0.08651629090309143\n",
      "Epoch 21870/30000 Training Loss: 0.059729915112257004\n",
      "Epoch 21870/30000 Validation Loss: 0.06543812155723572\n",
      "Epoch 21871/30000 Training Loss: 0.08359017223119736\n",
      "Epoch 21872/30000 Training Loss: 0.09120363742113113\n",
      "Epoch 21873/30000 Training Loss: 0.06864256411790848\n",
      "Epoch 21874/30000 Training Loss: 0.05914564058184624\n",
      "Epoch 21875/30000 Training Loss: 0.07117821276187897\n",
      "Epoch 21876/30000 Training Loss: 0.0731341540813446\n",
      "Epoch 21877/30000 Training Loss: 0.07374542206525803\n",
      "Epoch 21878/30000 Training Loss: 0.05475324019789696\n",
      "Epoch 21879/30000 Training Loss: 0.07392621040344238\n",
      "Epoch 21880/30000 Training Loss: 0.06813215464353561\n",
      "Epoch 21880/30000 Validation Loss: 0.05846129730343819\n",
      "Epoch 21881/30000 Training Loss: 0.05831317976117134\n",
      "Epoch 21882/30000 Training Loss: 0.05855288729071617\n",
      "Epoch 21883/30000 Training Loss: 0.09110420197248459\n",
      "Epoch 21884/30000 Training Loss: 0.05785759910941124\n",
      "Epoch 21885/30000 Training Loss: 0.07496871799230576\n",
      "Epoch 21886/30000 Training Loss: 0.064540795981884\n",
      "Epoch 21887/30000 Training Loss: 0.07127036899328232\n",
      "Epoch 21888/30000 Training Loss: 0.0520944781601429\n",
      "Epoch 21889/30000 Training Loss: 0.08121418207883835\n",
      "Epoch 21890/30000 Training Loss: 0.07208385318517685\n",
      "Epoch 21890/30000 Validation Loss: 0.07464292645454407\n",
      "Epoch 21891/30000 Training Loss: 0.08564874529838562\n",
      "Epoch 21892/30000 Training Loss: 0.08752455562353134\n",
      "Epoch 21893/30000 Training Loss: 0.05930672213435173\n",
      "Epoch 21894/30000 Training Loss: 0.07513932138681412\n",
      "Epoch 21895/30000 Training Loss: 0.058538395911455154\n",
      "Epoch 21896/30000 Training Loss: 0.06078118085861206\n",
      "Epoch 21897/30000 Training Loss: 0.07481100410223007\n",
      "Epoch 21898/30000 Training Loss: 0.06329864263534546\n",
      "Epoch 21899/30000 Training Loss: 0.06665904819965363\n",
      "Epoch 21900/30000 Training Loss: 0.07431316375732422\n",
      "Epoch 21900/30000 Validation Loss: 0.06452184170484543\n",
      "Epoch 21901/30000 Training Loss: 0.09136825054883957\n",
      "Epoch 21902/30000 Training Loss: 0.06041119992733002\n",
      "Epoch 21903/30000 Training Loss: 0.07338590174913406\n",
      "Epoch 21904/30000 Training Loss: 0.07224223762750626\n",
      "Epoch 21905/30000 Training Loss: 0.06094843149185181\n",
      "Epoch 21906/30000 Training Loss: 0.08731428533792496\n",
      "Epoch 21907/30000 Training Loss: 0.05635040998458862\n",
      "Epoch 21908/30000 Training Loss: 0.09036403894424438\n",
      "Epoch 21909/30000 Training Loss: 0.060566116124391556\n",
      "Epoch 21910/30000 Training Loss: 0.08419577032327652\n",
      "Epoch 21910/30000 Validation Loss: 0.06480174511671066\n",
      "Epoch 21911/30000 Training Loss: 0.06318207085132599\n",
      "Epoch 21912/30000 Training Loss: 0.06356985121965408\n",
      "Epoch 21913/30000 Training Loss: 0.06861113756895065\n",
      "Epoch 21914/30000 Training Loss: 0.08197023719549179\n",
      "Epoch 21915/30000 Training Loss: 0.073639877140522\n",
      "Epoch 21916/30000 Training Loss: 0.072157122194767\n",
      "Epoch 21917/30000 Training Loss: 0.07927895337343216\n",
      "Epoch 21918/30000 Training Loss: 0.07973852008581161\n",
      "Epoch 21919/30000 Training Loss: 0.07660764455795288\n",
      "Epoch 21920/30000 Training Loss: 0.06649186462163925\n",
      "Epoch 21920/30000 Validation Loss: 0.0780816376209259\n",
      "Epoch 21921/30000 Training Loss: 0.07938824594020844\n",
      "Epoch 21922/30000 Training Loss: 0.0600428581237793\n",
      "Epoch 21923/30000 Training Loss: 0.08211065083742142\n",
      "Epoch 21924/30000 Training Loss: 0.08412611484527588\n",
      "Epoch 21925/30000 Training Loss: 0.08160858601331711\n",
      "Epoch 21926/30000 Training Loss: 0.05607563629746437\n",
      "Epoch 21927/30000 Training Loss: 0.06349862366914749\n",
      "Epoch 21928/30000 Training Loss: 0.08697008341550827\n",
      "Epoch 21929/30000 Training Loss: 0.07283740490674973\n",
      "Epoch 21930/30000 Training Loss: 0.07873859256505966\n",
      "Epoch 21930/30000 Validation Loss: 0.06662344187498093\n",
      "Epoch 21931/30000 Training Loss: 0.07852396368980408\n",
      "Epoch 21932/30000 Training Loss: 0.095393605530262\n",
      "Epoch 21933/30000 Training Loss: 0.06293849647045135\n",
      "Epoch 21934/30000 Training Loss: 0.0679667592048645\n",
      "Epoch 21935/30000 Training Loss: 0.06508421897888184\n",
      "Epoch 21936/30000 Training Loss: 0.06862180680036545\n",
      "Epoch 21937/30000 Training Loss: 0.06598574668169022\n",
      "Epoch 21938/30000 Training Loss: 0.06473590433597565\n",
      "Epoch 21939/30000 Training Loss: 0.06866704672574997\n",
      "Epoch 21940/30000 Training Loss: 0.0688941702246666\n",
      "Epoch 21940/30000 Validation Loss: 0.06375908106565475\n",
      "Epoch 21941/30000 Training Loss: 0.08521906286478043\n",
      "Epoch 21942/30000 Training Loss: 0.07833586633205414\n",
      "Epoch 21943/30000 Training Loss: 0.054148633033037186\n",
      "Epoch 21944/30000 Training Loss: 0.06502027064561844\n",
      "Epoch 21945/30000 Training Loss: 0.06762989610433578\n",
      "Epoch 21946/30000 Training Loss: 0.07304179668426514\n",
      "Epoch 21947/30000 Training Loss: 0.07321197539567947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21948/30000 Training Loss: 0.05671742558479309\n",
      "Epoch 21949/30000 Training Loss: 0.06753315031528473\n",
      "Epoch 21950/30000 Training Loss: 0.057262491434812546\n",
      "Epoch 21950/30000 Validation Loss: 0.07795549929141998\n",
      "Epoch 21951/30000 Training Loss: 0.07799005508422852\n",
      "Epoch 21952/30000 Training Loss: 0.08311614394187927\n",
      "Epoch 21953/30000 Training Loss: 0.06127573922276497\n",
      "Epoch 21954/30000 Training Loss: 0.07866919785737991\n",
      "Epoch 21955/30000 Training Loss: 0.0665523111820221\n",
      "Epoch 21956/30000 Training Loss: 0.07614485174417496\n",
      "Epoch 21957/30000 Training Loss: 0.07003014534711838\n",
      "Epoch 21958/30000 Training Loss: 0.07866866141557693\n",
      "Epoch 21959/30000 Training Loss: 0.0507165789604187\n",
      "Epoch 21960/30000 Training Loss: 0.07409750670194626\n",
      "Epoch 21960/30000 Validation Loss: 0.0727652758359909\n",
      "Epoch 21961/30000 Training Loss: 0.06364337354898453\n",
      "Epoch 21962/30000 Training Loss: 0.06856376677751541\n",
      "Epoch 21963/30000 Training Loss: 0.08490810543298721\n",
      "Epoch 21964/30000 Training Loss: 0.08192872256040573\n",
      "Epoch 21965/30000 Training Loss: 0.05771562457084656\n",
      "Epoch 21966/30000 Training Loss: 0.08543426543474197\n",
      "Epoch 21967/30000 Training Loss: 0.06612125784158707\n",
      "Epoch 21968/30000 Training Loss: 0.054926797747612\n",
      "Epoch 21969/30000 Training Loss: 0.06796959042549133\n",
      "Epoch 21970/30000 Training Loss: 0.07771679759025574\n",
      "Epoch 21970/30000 Validation Loss: 0.06921802461147308\n",
      "Epoch 21971/30000 Training Loss: 0.084234319627285\n",
      "Epoch 21972/30000 Training Loss: 0.07733381539583206\n",
      "Epoch 21973/30000 Training Loss: 0.07332408428192139\n",
      "Epoch 21974/30000 Training Loss: 0.07581659406423569\n",
      "Epoch 21975/30000 Training Loss: 0.07548651844263077\n",
      "Epoch 21976/30000 Training Loss: 0.08145224303007126\n",
      "Epoch 21977/30000 Training Loss: 0.0839628279209137\n",
      "Epoch 21978/30000 Training Loss: 0.074893057346344\n",
      "Epoch 21979/30000 Training Loss: 0.09247195720672607\n",
      "Epoch 21980/30000 Training Loss: 0.07797884196043015\n",
      "Epoch 21980/30000 Validation Loss: 0.07088359445333481\n",
      "Epoch 21981/30000 Training Loss: 0.08486127108335495\n",
      "Epoch 21982/30000 Training Loss: 0.07422420382499695\n",
      "Epoch 21983/30000 Training Loss: 0.06840898841619492\n",
      "Epoch 21984/30000 Training Loss: 0.0506555549800396\n",
      "Epoch 21985/30000 Training Loss: 0.06290624290704727\n",
      "Epoch 21986/30000 Training Loss: 0.067921482026577\n",
      "Epoch 21987/30000 Training Loss: 0.07959926873445511\n",
      "Epoch 21988/30000 Training Loss: 0.0687231793999672\n",
      "Epoch 21989/30000 Training Loss: 0.0628746971487999\n",
      "Epoch 21990/30000 Training Loss: 0.05918988958001137\n",
      "Epoch 21990/30000 Validation Loss: 0.07203008979558945\n",
      "Epoch 21991/30000 Training Loss: 0.06250634789466858\n",
      "Epoch 21992/30000 Training Loss: 0.08837836235761642\n",
      "Epoch 21993/30000 Training Loss: 0.07970910519361496\n",
      "Epoch 21994/30000 Training Loss: 0.06540041416883469\n",
      "Epoch 21995/30000 Training Loss: 0.09426533430814743\n",
      "Epoch 21996/30000 Training Loss: 0.04802535101771355\n",
      "Epoch 21997/30000 Training Loss: 0.06422071158885956\n",
      "Epoch 21998/30000 Training Loss: 0.081328384578228\n",
      "Epoch 21999/30000 Training Loss: 0.07437604665756226\n",
      "Epoch 22000/30000 Training Loss: 0.07659632712602615\n",
      "Epoch 22000/30000 Validation Loss: 0.07818802446126938\n",
      "Epoch 22001/30000 Training Loss: 0.07380426675081253\n",
      "Epoch 22002/30000 Training Loss: 0.07339267432689667\n",
      "Epoch 22003/30000 Training Loss: 0.06293269991874695\n",
      "Epoch 22004/30000 Training Loss: 0.08505111932754517\n",
      "Epoch 22005/30000 Training Loss: 0.06878116726875305\n",
      "Epoch 22006/30000 Training Loss: 0.06655902415513992\n",
      "Epoch 22007/30000 Training Loss: 0.07767046242952347\n",
      "Epoch 22008/30000 Training Loss: 0.08710283041000366\n",
      "Epoch 22009/30000 Training Loss: 0.09050432592630386\n",
      "Epoch 22010/30000 Training Loss: 0.08355260640382767\n",
      "Epoch 22010/30000 Validation Loss: 0.05308813974261284\n",
      "Epoch 22011/30000 Training Loss: 0.05837758257985115\n",
      "Epoch 22012/30000 Training Loss: 0.09013783931732178\n",
      "Epoch 22013/30000 Training Loss: 0.08943524211645126\n",
      "Epoch 22014/30000 Training Loss: 0.06606236100196838\n",
      "Epoch 22015/30000 Training Loss: 0.05404222011566162\n",
      "Epoch 22016/30000 Training Loss: 0.06989739090204239\n",
      "Epoch 22017/30000 Training Loss: 0.07315804809331894\n",
      "Epoch 22018/30000 Training Loss: 0.08760792762041092\n",
      "Epoch 22019/30000 Training Loss: 0.09180735796689987\n",
      "Epoch 22020/30000 Training Loss: 0.07727247476577759\n",
      "Epoch 22020/30000 Validation Loss: 0.08090586960315704\n",
      "Epoch 22021/30000 Training Loss: 0.09051836282014847\n",
      "Epoch 22022/30000 Training Loss: 0.07463500648736954\n",
      "Epoch 22023/30000 Training Loss: 0.09374075382947922\n",
      "Epoch 22024/30000 Training Loss: 0.06816459447145462\n",
      "Epoch 22025/30000 Training Loss: 0.06742524355649948\n",
      "Epoch 22026/30000 Training Loss: 0.05804656073451042\n",
      "Epoch 22027/30000 Training Loss: 0.06128672882914543\n",
      "Epoch 22028/30000 Training Loss: 0.07671801000833511\n",
      "Epoch 22029/30000 Training Loss: 0.058583784848451614\n",
      "Epoch 22030/30000 Training Loss: 0.059463854879140854\n",
      "Epoch 22030/30000 Validation Loss: 0.08464684337377548\n",
      "Epoch 22031/30000 Training Loss: 0.06396166235208511\n",
      "Epoch 22032/30000 Training Loss: 0.07456240057945251\n",
      "Epoch 22033/30000 Training Loss: 0.07418376952409744\n",
      "Epoch 22034/30000 Training Loss: 0.060580432415008545\n",
      "Epoch 22035/30000 Training Loss: 0.07853605598211288\n",
      "Epoch 22036/30000 Training Loss: 0.06995802372694016\n",
      "Epoch 22037/30000 Training Loss: 0.06194048747420311\n",
      "Epoch 22038/30000 Training Loss: 0.08683633804321289\n",
      "Epoch 22039/30000 Training Loss: 0.07164590805768967\n",
      "Epoch 22040/30000 Training Loss: 0.05261087417602539\n",
      "Epoch 22040/30000 Validation Loss: 0.06869322061538696\n",
      "Epoch 22041/30000 Training Loss: 0.05856889486312866\n",
      "Epoch 22042/30000 Training Loss: 0.06567205488681793\n",
      "Epoch 22043/30000 Training Loss: 0.07442709803581238\n",
      "Epoch 22044/30000 Training Loss: 0.08345788717269897\n",
      "Epoch 22045/30000 Training Loss: 0.10237407684326172\n",
      "Epoch 22046/30000 Training Loss: 0.062206462025642395\n",
      "Epoch 22047/30000 Training Loss: 0.09171665459871292\n",
      "Epoch 22048/30000 Training Loss: 0.05979407951235771\n",
      "Epoch 22049/30000 Training Loss: 0.06173418089747429\n",
      "Epoch 22050/30000 Training Loss: 0.065007284283638\n",
      "Epoch 22050/30000 Validation Loss: 0.08603938668966293\n",
      "Epoch 22051/30000 Training Loss: 0.06917615234851837\n",
      "Epoch 22052/30000 Training Loss: 0.06700262427330017\n",
      "Epoch 22053/30000 Training Loss: 0.08556824922561646\n",
      "Epoch 22054/30000 Training Loss: 0.08115115761756897\n",
      "Epoch 22055/30000 Training Loss: 0.065358467400074\n",
      "Epoch 22056/30000 Training Loss: 0.052313655614852905\n",
      "Epoch 22057/30000 Training Loss: 0.08012191206216812\n",
      "Epoch 22058/30000 Training Loss: 0.08309192955493927\n",
      "Epoch 22059/30000 Training Loss: 0.05366452410817146\n",
      "Epoch 22060/30000 Training Loss: 0.07429597526788712\n",
      "Epoch 22060/30000 Validation Loss: 0.06888679414987564\n",
      "Epoch 22061/30000 Training Loss: 0.07082924246788025\n",
      "Epoch 22062/30000 Training Loss: 0.06884599477052689\n",
      "Epoch 22063/30000 Training Loss: 0.05500657856464386\n",
      "Epoch 22064/30000 Training Loss: 0.07274466007947922\n",
      "Epoch 22065/30000 Training Loss: 0.0810336098074913\n",
      "Epoch 22066/30000 Training Loss: 0.07723631709814072\n",
      "Epoch 22067/30000 Training Loss: 0.08232326060533524\n",
      "Epoch 22068/30000 Training Loss: 0.08180323988199234\n",
      "Epoch 22069/30000 Training Loss: 0.06648430973291397\n",
      "Epoch 22070/30000 Training Loss: 0.05396527051925659\n",
      "Epoch 22070/30000 Validation Loss: 0.07047226279973984\n",
      "Epoch 22071/30000 Training Loss: 0.07771053910255432\n",
      "Epoch 22072/30000 Training Loss: 0.06643009185791016\n",
      "Epoch 22073/30000 Training Loss: 0.07834295928478241\n",
      "Epoch 22074/30000 Training Loss: 0.060393620282411575\n",
      "Epoch 22075/30000 Training Loss: 0.07821481674909592\n",
      "Epoch 22076/30000 Training Loss: 0.06290168315172195\n",
      "Epoch 22077/30000 Training Loss: 0.06895896047353745\n",
      "Epoch 22078/30000 Training Loss: 0.07896561175584793\n",
      "Epoch 22079/30000 Training Loss: 0.061847392469644547\n",
      "Epoch 22080/30000 Training Loss: 0.061771005392074585\n",
      "Epoch 22080/30000 Validation Loss: 0.0682177022099495\n",
      "Epoch 22081/30000 Training Loss: 0.07902271300554276\n",
      "Epoch 22082/30000 Training Loss: 0.05480465665459633\n",
      "Epoch 22083/30000 Training Loss: 0.07159429788589478\n",
      "Epoch 22084/30000 Training Loss: 0.05897679924964905\n",
      "Epoch 22085/30000 Training Loss: 0.056765731424093246\n",
      "Epoch 22086/30000 Training Loss: 0.06746517866849899\n",
      "Epoch 22087/30000 Training Loss: 0.06779235601425171\n",
      "Epoch 22088/30000 Training Loss: 0.052682001143693924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22089/30000 Training Loss: 0.06694740056991577\n",
      "Epoch 22090/30000 Training Loss: 0.07880106568336487\n",
      "Epoch 22090/30000 Validation Loss: 0.06904851645231247\n",
      "Epoch 22091/30000 Training Loss: 0.06779581308364868\n",
      "Epoch 22092/30000 Training Loss: 0.06392254680395126\n",
      "Epoch 22093/30000 Training Loss: 0.08213575184345245\n",
      "Epoch 22094/30000 Training Loss: 0.07588222622871399\n",
      "Epoch 22095/30000 Training Loss: 0.05564501881599426\n",
      "Epoch 22096/30000 Training Loss: 0.06630747765302658\n",
      "Epoch 22097/30000 Training Loss: 0.09264171868562698\n",
      "Epoch 22098/30000 Training Loss: 0.06721784919500351\n",
      "Epoch 22099/30000 Training Loss: 0.07867234945297241\n",
      "Epoch 22100/30000 Training Loss: 0.07017772644758224\n",
      "Epoch 22100/30000 Validation Loss: 0.06849824637174606\n",
      "Epoch 22101/30000 Training Loss: 0.07463269680738449\n",
      "Epoch 22102/30000 Training Loss: 0.07601425796747208\n",
      "Epoch 22103/30000 Training Loss: 0.07562441378831863\n",
      "Epoch 22104/30000 Training Loss: 0.07181084901094437\n",
      "Epoch 22105/30000 Training Loss: 0.061694253236055374\n",
      "Epoch 22106/30000 Training Loss: 0.080213762819767\n",
      "Epoch 22107/30000 Training Loss: 0.06472883373498917\n",
      "Epoch 22108/30000 Training Loss: 0.08385872840881348\n",
      "Epoch 22109/30000 Training Loss: 0.08078471571207047\n",
      "Epoch 22110/30000 Training Loss: 0.06850624084472656\n",
      "Epoch 22110/30000 Validation Loss: 0.0658118724822998\n",
      "Epoch 22111/30000 Training Loss: 0.08372464030981064\n",
      "Epoch 22112/30000 Training Loss: 0.06663555651903152\n",
      "Epoch 22113/30000 Training Loss: 0.06839832663536072\n",
      "Epoch 22114/30000 Training Loss: 0.06139644980430603\n",
      "Epoch 22115/30000 Training Loss: 0.0631403848528862\n",
      "Epoch 22116/30000 Training Loss: 0.07633514702320099\n",
      "Epoch 22117/30000 Training Loss: 0.06710714846849442\n",
      "Epoch 22118/30000 Training Loss: 0.07078389078378677\n",
      "Epoch 22119/30000 Training Loss: 0.0653461217880249\n",
      "Epoch 22120/30000 Training Loss: 0.0817112997174263\n",
      "Epoch 22120/30000 Validation Loss: 0.061049479991197586\n",
      "Epoch 22121/30000 Training Loss: 0.06508730351924896\n",
      "Epoch 22122/30000 Training Loss: 0.07379583269357681\n",
      "Epoch 22123/30000 Training Loss: 0.06076664850115776\n",
      "Epoch 22124/30000 Training Loss: 0.06148291006684303\n",
      "Epoch 22125/30000 Training Loss: 0.07174082845449448\n",
      "Epoch 22126/30000 Training Loss: 0.08760445564985275\n",
      "Epoch 22127/30000 Training Loss: 0.08397328853607178\n",
      "Epoch 22128/30000 Training Loss: 0.07126768678426743\n",
      "Epoch 22129/30000 Training Loss: 0.08821278810501099\n",
      "Epoch 22130/30000 Training Loss: 0.0872575119137764\n",
      "Epoch 22130/30000 Validation Loss: 0.07070083171129227\n",
      "Epoch 22131/30000 Training Loss: 0.06588446348905563\n",
      "Epoch 22132/30000 Training Loss: 0.06384588032960892\n",
      "Epoch 22133/30000 Training Loss: 0.06339056044816971\n",
      "Epoch 22134/30000 Training Loss: 0.06141035631299019\n",
      "Epoch 22135/30000 Training Loss: 0.06706901639699936\n",
      "Epoch 22136/30000 Training Loss: 0.07070144265890121\n",
      "Epoch 22137/30000 Training Loss: 0.07093676179647446\n",
      "Epoch 22138/30000 Training Loss: 0.08267024904489517\n",
      "Epoch 22139/30000 Training Loss: 0.06004777178168297\n",
      "Epoch 22140/30000 Training Loss: 0.0720672532916069\n",
      "Epoch 22140/30000 Validation Loss: 0.06434137374162674\n",
      "Epoch 22141/30000 Training Loss: 0.06892596930265427\n",
      "Epoch 22142/30000 Training Loss: 0.06880360096693039\n",
      "Epoch 22143/30000 Training Loss: 0.07141699641942978\n",
      "Epoch 22144/30000 Training Loss: 0.07362417131662369\n",
      "Epoch 22145/30000 Training Loss: 0.07775920629501343\n",
      "Epoch 22146/30000 Training Loss: 0.0706922858953476\n",
      "Epoch 22147/30000 Training Loss: 0.07290785759687424\n",
      "Epoch 22148/30000 Training Loss: 0.06618115305900574\n",
      "Epoch 22149/30000 Training Loss: 0.0718989297747612\n",
      "Epoch 22150/30000 Training Loss: 0.059113889932632446\n",
      "Epoch 22150/30000 Validation Loss: 0.08526120334863663\n",
      "Epoch 22151/30000 Training Loss: 0.0740167424082756\n",
      "Epoch 22152/30000 Training Loss: 0.06752030551433563\n",
      "Epoch 22153/30000 Training Loss: 0.07017705589532852\n",
      "Epoch 22154/30000 Training Loss: 0.07747294753789902\n",
      "Epoch 22155/30000 Training Loss: 0.07125743478536606\n",
      "Epoch 22156/30000 Training Loss: 0.07365744560956955\n",
      "Epoch 22157/30000 Training Loss: 0.07392802089452744\n",
      "Epoch 22158/30000 Training Loss: 0.06372672319412231\n",
      "Epoch 22159/30000 Training Loss: 0.06411266326904297\n",
      "Epoch 22160/30000 Training Loss: 0.07860779017210007\n",
      "Epoch 22160/30000 Validation Loss: 0.07277867943048477\n",
      "Epoch 22161/30000 Training Loss: 0.07337972521781921\n",
      "Epoch 22162/30000 Training Loss: 0.07102406769990921\n",
      "Epoch 22163/30000 Training Loss: 0.06650841981172562\n",
      "Epoch 22164/30000 Training Loss: 0.06583118438720703\n",
      "Epoch 22165/30000 Training Loss: 0.07445601373910904\n",
      "Epoch 22166/30000 Training Loss: 0.09000509977340698\n",
      "Epoch 22167/30000 Training Loss: 0.08634532243013382\n",
      "Epoch 22168/30000 Training Loss: 0.08519648760557175\n",
      "Epoch 22169/30000 Training Loss: 0.05536564067006111\n",
      "Epoch 22170/30000 Training Loss: 0.06740892678499222\n",
      "Epoch 22170/30000 Validation Loss: 0.07182949781417847\n",
      "Epoch 22171/30000 Training Loss: 0.07656217366456985\n",
      "Epoch 22172/30000 Training Loss: 0.06442896276712418\n",
      "Epoch 22173/30000 Training Loss: 0.06730946898460388\n",
      "Epoch 22174/30000 Training Loss: 0.060662005096673965\n",
      "Epoch 22175/30000 Training Loss: 0.07638073712587357\n",
      "Epoch 22176/30000 Training Loss: 0.07780209928750992\n",
      "Epoch 22177/30000 Training Loss: 0.07189217209815979\n",
      "Epoch 22178/30000 Training Loss: 0.06203740835189819\n",
      "Epoch 22179/30000 Training Loss: 0.0760197639465332\n",
      "Epoch 22180/30000 Training Loss: 0.06407465785741806\n",
      "Epoch 22180/30000 Validation Loss: 0.09153226763010025\n",
      "Epoch 22181/30000 Training Loss: 0.07033588737249374\n",
      "Epoch 22182/30000 Training Loss: 0.054287344217300415\n",
      "Epoch 22183/30000 Training Loss: 0.0964682325720787\n",
      "Epoch 22184/30000 Training Loss: 0.07192977517843246\n",
      "Epoch 22185/30000 Training Loss: 0.07249213010072708\n",
      "Epoch 22186/30000 Training Loss: 0.06158756837248802\n",
      "Epoch 22187/30000 Training Loss: 0.07140097767114639\n",
      "Epoch 22188/30000 Training Loss: 0.05843024328351021\n",
      "Epoch 22189/30000 Training Loss: 0.09163439273834229\n",
      "Epoch 22190/30000 Training Loss: 0.07380110025405884\n",
      "Epoch 22190/30000 Validation Loss: 0.0875963345170021\n",
      "Epoch 22191/30000 Training Loss: 0.06021399796009064\n",
      "Epoch 22192/30000 Training Loss: 0.06686880439519882\n",
      "Epoch 22193/30000 Training Loss: 0.07062214612960815\n",
      "Epoch 22194/30000 Training Loss: 0.08121371269226074\n",
      "Epoch 22195/30000 Training Loss: 0.05632789060473442\n",
      "Epoch 22196/30000 Training Loss: 0.06286036968231201\n",
      "Epoch 22197/30000 Training Loss: 0.059850577265024185\n",
      "Epoch 22198/30000 Training Loss: 0.06724569201469421\n",
      "Epoch 22199/30000 Training Loss: 0.06418868154287338\n",
      "Epoch 22200/30000 Training Loss: 0.06288941949605942\n",
      "Epoch 22200/30000 Validation Loss: 0.08429243415594101\n",
      "Epoch 22201/30000 Training Loss: 0.07438669353723526\n",
      "Epoch 22202/30000 Training Loss: 0.07095837593078613\n",
      "Epoch 22203/30000 Training Loss: 0.05952656269073486\n",
      "Epoch 22204/30000 Training Loss: 0.05687698349356651\n",
      "Epoch 22205/30000 Training Loss: 0.07562520354986191\n",
      "Epoch 22206/30000 Training Loss: 0.06786251813173294\n",
      "Epoch 22207/30000 Training Loss: 0.06254037469625473\n",
      "Epoch 22208/30000 Training Loss: 0.07789459079504013\n",
      "Epoch 22209/30000 Training Loss: 0.06811339408159256\n",
      "Epoch 22210/30000 Training Loss: 0.08469018340110779\n",
      "Epoch 22210/30000 Validation Loss: 0.06902114301919937\n",
      "Epoch 22211/30000 Training Loss: 0.06417613476514816\n",
      "Epoch 22212/30000 Training Loss: 0.06962046027183533\n",
      "Epoch 22213/30000 Training Loss: 0.07345060259103775\n",
      "Epoch 22214/30000 Training Loss: 0.08522951602935791\n",
      "Epoch 22215/30000 Training Loss: 0.07043533772230148\n",
      "Epoch 22216/30000 Training Loss: 0.07639110833406448\n",
      "Epoch 22217/30000 Training Loss: 0.06418273597955704\n",
      "Epoch 22218/30000 Training Loss: 0.05616381764411926\n",
      "Epoch 22219/30000 Training Loss: 0.08057598024606705\n",
      "Epoch 22220/30000 Training Loss: 0.06467548757791519\n",
      "Epoch 22220/30000 Validation Loss: 0.08004256337881088\n",
      "Epoch 22221/30000 Training Loss: 0.07023585587739944\n",
      "Epoch 22222/30000 Training Loss: 0.07387823611497879\n",
      "Epoch 22223/30000 Training Loss: 0.053592633455991745\n",
      "Epoch 22224/30000 Training Loss: 0.0662926658987999\n",
      "Epoch 22225/30000 Training Loss: 0.06679271161556244\n",
      "Epoch 22226/30000 Training Loss: 0.08020404726266861\n",
      "Epoch 22227/30000 Training Loss: 0.08674674481153488\n",
      "Epoch 22228/30000 Training Loss: 0.07160211354494095\n",
      "Epoch 22229/30000 Training Loss: 0.061187367886304855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22230/30000 Training Loss: 0.07562282681465149\n",
      "Epoch 22230/30000 Validation Loss: 0.06806259602308273\n",
      "Epoch 22231/30000 Training Loss: 0.07203012704849243\n",
      "Epoch 22232/30000 Training Loss: 0.06591474264860153\n",
      "Epoch 22233/30000 Training Loss: 0.06349190324544907\n",
      "Epoch 22234/30000 Training Loss: 0.06448081135749817\n",
      "Epoch 22235/30000 Training Loss: 0.08265779167413712\n",
      "Epoch 22236/30000 Training Loss: 0.06386549770832062\n",
      "Epoch 22237/30000 Training Loss: 0.07604149729013443\n",
      "Epoch 22238/30000 Training Loss: 0.07033932954072952\n",
      "Epoch 22239/30000 Training Loss: 0.06984236091375351\n",
      "Epoch 22240/30000 Training Loss: 0.06252719461917877\n",
      "Epoch 22240/30000 Validation Loss: 0.07198798656463623\n",
      "Epoch 22241/30000 Training Loss: 0.0766976922750473\n",
      "Epoch 22242/30000 Training Loss: 0.09023324400186539\n",
      "Epoch 22243/30000 Training Loss: 0.08306896686553955\n",
      "Epoch 22244/30000 Training Loss: 0.07071501761674881\n",
      "Epoch 22245/30000 Training Loss: 0.07726887613534927\n",
      "Epoch 22246/30000 Training Loss: 0.07660369575023651\n",
      "Epoch 22247/30000 Training Loss: 0.060364048928022385\n",
      "Epoch 22248/30000 Training Loss: 0.06537159532308578\n",
      "Epoch 22249/30000 Training Loss: 0.07901210337877274\n",
      "Epoch 22250/30000 Training Loss: 0.05680800974369049\n",
      "Epoch 22250/30000 Validation Loss: 0.06234835460782051\n",
      "Epoch 22251/30000 Training Loss: 0.0538519062101841\n",
      "Epoch 22252/30000 Training Loss: 0.0619317889213562\n",
      "Epoch 22253/30000 Training Loss: 0.08489647507667542\n",
      "Epoch 22254/30000 Training Loss: 0.07289961725473404\n",
      "Epoch 22255/30000 Training Loss: 0.05479046702384949\n",
      "Epoch 22256/30000 Training Loss: 0.07640094310045242\n",
      "Epoch 22257/30000 Training Loss: 0.07556357979774475\n",
      "Epoch 22258/30000 Training Loss: 0.08501449972391129\n",
      "Epoch 22259/30000 Training Loss: 0.05964803323149681\n",
      "Epoch 22260/30000 Training Loss: 0.06504116207361221\n",
      "Epoch 22260/30000 Validation Loss: 0.06811784952878952\n",
      "Epoch 22261/30000 Training Loss: 0.07469707727432251\n",
      "Epoch 22262/30000 Training Loss: 0.06179872527718544\n",
      "Epoch 22263/30000 Training Loss: 0.06678339093923569\n",
      "Epoch 22264/30000 Training Loss: 0.06997496634721756\n",
      "Epoch 22265/30000 Training Loss: 0.061050593852996826\n",
      "Epoch 22266/30000 Training Loss: 0.08681610226631165\n",
      "Epoch 22267/30000 Training Loss: 0.05544839799404144\n",
      "Epoch 22268/30000 Training Loss: 0.07333456724882126\n",
      "Epoch 22269/30000 Training Loss: 0.07858313620090485\n",
      "Epoch 22270/30000 Training Loss: 0.07815530151128769\n",
      "Epoch 22270/30000 Validation Loss: 0.08527874946594238\n",
      "Epoch 22271/30000 Training Loss: 0.07999106496572495\n",
      "Epoch 22272/30000 Training Loss: 0.06400809437036514\n",
      "Epoch 22273/30000 Training Loss: 0.07076168805360794\n",
      "Epoch 22274/30000 Training Loss: 0.0843452587723732\n",
      "Epoch 22275/30000 Training Loss: 0.07252460718154907\n",
      "Epoch 22276/30000 Training Loss: 0.05705760791897774\n",
      "Epoch 22277/30000 Training Loss: 0.08966243267059326\n",
      "Epoch 22278/30000 Training Loss: 0.07298928499221802\n",
      "Epoch 22279/30000 Training Loss: 0.08323727548122406\n",
      "Epoch 22280/30000 Training Loss: 0.0725066065788269\n",
      "Epoch 22280/30000 Validation Loss: 0.07760893553495407\n",
      "Epoch 22281/30000 Training Loss: 0.0778554379940033\n",
      "Epoch 22282/30000 Training Loss: 0.07503096014261246\n",
      "Epoch 22283/30000 Training Loss: 0.06859293580055237\n",
      "Epoch 22284/30000 Training Loss: 0.0598028190433979\n",
      "Epoch 22285/30000 Training Loss: 0.0886969193816185\n",
      "Epoch 22286/30000 Training Loss: 0.06734608858823776\n",
      "Epoch 22287/30000 Training Loss: 0.06398238986730576\n",
      "Epoch 22288/30000 Training Loss: 0.05278586968779564\n",
      "Epoch 22289/30000 Training Loss: 0.06362035125494003\n",
      "Epoch 22290/30000 Training Loss: 0.0713777169585228\n",
      "Epoch 22290/30000 Validation Loss: 0.06739936023950577\n",
      "Epoch 22291/30000 Training Loss: 0.0640101432800293\n",
      "Epoch 22292/30000 Training Loss: 0.0799386203289032\n",
      "Epoch 22293/30000 Training Loss: 0.07026573270559311\n",
      "Epoch 22294/30000 Training Loss: 0.05257915332913399\n",
      "Epoch 22295/30000 Training Loss: 0.07355739921331406\n",
      "Epoch 22296/30000 Training Loss: 0.05686765909194946\n",
      "Epoch 22297/30000 Training Loss: 0.07910008728504181\n",
      "Epoch 22298/30000 Training Loss: 0.060046952217817307\n",
      "Epoch 22299/30000 Training Loss: 0.06538406759500504\n",
      "Epoch 22300/30000 Training Loss: 0.09930416941642761\n",
      "Epoch 22300/30000 Validation Loss: 0.07224033027887344\n",
      "Epoch 22301/30000 Training Loss: 0.07847221940755844\n",
      "Epoch 22302/30000 Training Loss: 0.07346847653388977\n",
      "Epoch 22303/30000 Training Loss: 0.0583665706217289\n",
      "Epoch 22304/30000 Training Loss: 0.06108567491173744\n",
      "Epoch 22305/30000 Training Loss: 0.0627698302268982\n",
      "Epoch 22306/30000 Training Loss: 0.0669374018907547\n",
      "Epoch 22307/30000 Training Loss: 0.07496970891952515\n",
      "Epoch 22308/30000 Training Loss: 0.06859853863716125\n",
      "Epoch 22309/30000 Training Loss: 0.06948721408843994\n",
      "Epoch 22310/30000 Training Loss: 0.0835983157157898\n",
      "Epoch 22310/30000 Validation Loss: 0.07651207596063614\n",
      "Epoch 22311/30000 Training Loss: 0.07552894949913025\n",
      "Epoch 22312/30000 Training Loss: 0.08657094836235046\n",
      "Epoch 22313/30000 Training Loss: 0.050150346010923386\n",
      "Epoch 22314/30000 Training Loss: 0.07594341039657593\n",
      "Epoch 22315/30000 Training Loss: 0.10208819061517715\n",
      "Epoch 22316/30000 Training Loss: 0.0839110016822815\n",
      "Epoch 22317/30000 Training Loss: 0.07233467698097229\n",
      "Epoch 22318/30000 Training Loss: 0.06649855524301529\n",
      "Epoch 22319/30000 Training Loss: 0.06968456506729126\n",
      "Epoch 22320/30000 Training Loss: 0.07016684114933014\n",
      "Epoch 22320/30000 Validation Loss: 0.07090170681476593\n",
      "Epoch 22321/30000 Training Loss: 0.08208826929330826\n",
      "Epoch 22322/30000 Training Loss: 0.0530744306743145\n",
      "Epoch 22323/30000 Training Loss: 0.07332666963338852\n",
      "Epoch 22324/30000 Training Loss: 0.06636430323123932\n",
      "Epoch 22325/30000 Training Loss: 0.06365250796079636\n",
      "Epoch 22326/30000 Training Loss: 0.06836256384849548\n",
      "Epoch 22327/30000 Training Loss: 0.06632427871227264\n",
      "Epoch 22328/30000 Training Loss: 0.08788642287254333\n",
      "Epoch 22329/30000 Training Loss: 0.07094735652208328\n",
      "Epoch 22330/30000 Training Loss: 0.06444589048624039\n",
      "Epoch 22330/30000 Validation Loss: 0.07159563153982162\n",
      "Epoch 22331/30000 Training Loss: 0.0637439712882042\n",
      "Epoch 22332/30000 Training Loss: 0.06710106134414673\n",
      "Epoch 22333/30000 Training Loss: 0.07395405322313309\n",
      "Epoch 22334/30000 Training Loss: 0.0635390430688858\n",
      "Epoch 22335/30000 Training Loss: 0.05669473111629486\n",
      "Epoch 22336/30000 Training Loss: 0.060114774852991104\n",
      "Epoch 22337/30000 Training Loss: 0.07672848552465439\n",
      "Epoch 22338/30000 Training Loss: 0.0700330063700676\n",
      "Epoch 22339/30000 Training Loss: 0.08087385445833206\n",
      "Epoch 22340/30000 Training Loss: 0.06680747121572495\n",
      "Epoch 22340/30000 Validation Loss: 0.06629437953233719\n",
      "Epoch 22341/30000 Training Loss: 0.0776934027671814\n",
      "Epoch 22342/30000 Training Loss: 0.07590501755475998\n",
      "Epoch 22343/30000 Training Loss: 0.07286560535430908\n",
      "Epoch 22344/30000 Training Loss: 0.06731847673654556\n",
      "Epoch 22345/30000 Training Loss: 0.07134532928466797\n",
      "Epoch 22346/30000 Training Loss: 0.05296986922621727\n",
      "Epoch 22347/30000 Training Loss: 0.06326215714216232\n",
      "Epoch 22348/30000 Training Loss: 0.0710495337843895\n",
      "Epoch 22349/30000 Training Loss: 0.07049401104450226\n",
      "Epoch 22350/30000 Training Loss: 0.07402325421571732\n",
      "Epoch 22350/30000 Validation Loss: 0.050126414746046066\n",
      "Epoch 22351/30000 Training Loss: 0.08139457553625107\n",
      "Epoch 22352/30000 Training Loss: 0.08012264221906662\n",
      "Epoch 22353/30000 Training Loss: 0.06541433185338974\n",
      "Epoch 22354/30000 Training Loss: 0.062384288758039474\n",
      "Epoch 22355/30000 Training Loss: 0.0705859437584877\n",
      "Epoch 22356/30000 Training Loss: 0.05620185658335686\n",
      "Epoch 22357/30000 Training Loss: 0.07145548611879349\n",
      "Epoch 22358/30000 Training Loss: 0.07814116030931473\n",
      "Epoch 22359/30000 Training Loss: 0.059327125549316406\n",
      "Epoch 22360/30000 Training Loss: 0.07148894667625427\n",
      "Epoch 22360/30000 Validation Loss: 0.06529156118631363\n",
      "Epoch 22361/30000 Training Loss: 0.0584014467895031\n",
      "Epoch 22362/30000 Training Loss: 0.09226509183645248\n",
      "Epoch 22363/30000 Training Loss: 0.060006868094205856\n",
      "Epoch 22364/30000 Training Loss: 0.059136126190423965\n",
      "Epoch 22365/30000 Training Loss: 0.07371652871370316\n",
      "Epoch 22366/30000 Training Loss: 0.05958865210413933\n",
      "Epoch 22367/30000 Training Loss: 0.07138631492853165\n",
      "Epoch 22368/30000 Training Loss: 0.08401861041784286\n",
      "Epoch 22369/30000 Training Loss: 0.04957057908177376\n",
      "Epoch 22370/30000 Training Loss: 0.06722596287727356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22370/30000 Validation Loss: 0.06143568456172943\n",
      "Epoch 22371/30000 Training Loss: 0.07439429312944412\n",
      "Epoch 22372/30000 Training Loss: 0.07166340202093124\n",
      "Epoch 22373/30000 Training Loss: 0.06652175635099411\n",
      "Epoch 22374/30000 Training Loss: 0.07880576699972153\n",
      "Epoch 22375/30000 Training Loss: 0.06757555156946182\n",
      "Epoch 22376/30000 Training Loss: 0.06664418429136276\n",
      "Epoch 22377/30000 Training Loss: 0.06797794252634048\n",
      "Epoch 22378/30000 Training Loss: 0.059469133615493774\n",
      "Epoch 22379/30000 Training Loss: 0.06411164253950119\n",
      "Epoch 22380/30000 Training Loss: 0.07731827348470688\n",
      "Epoch 22380/30000 Validation Loss: 0.07666877657175064\n",
      "Epoch 22381/30000 Training Loss: 0.07147983461618423\n",
      "Epoch 22382/30000 Training Loss: 0.057911068201065063\n",
      "Epoch 22383/30000 Training Loss: 0.0722295492887497\n",
      "Epoch 22384/30000 Training Loss: 0.06976219266653061\n",
      "Epoch 22385/30000 Training Loss: 0.07351665943861008\n",
      "Epoch 22386/30000 Training Loss: 0.07263661175966263\n",
      "Epoch 22387/30000 Training Loss: 0.06335250288248062\n",
      "Epoch 22388/30000 Training Loss: 0.076516292989254\n",
      "Epoch 22389/30000 Training Loss: 0.06394504010677338\n",
      "Epoch 22390/30000 Training Loss: 0.07191449403762817\n",
      "Epoch 22390/30000 Validation Loss: 0.06119078770279884\n",
      "Epoch 22391/30000 Training Loss: 0.07798874378204346\n",
      "Epoch 22392/30000 Training Loss: 0.06848978251218796\n",
      "Epoch 22393/30000 Training Loss: 0.07151772826910019\n",
      "Epoch 22394/30000 Training Loss: 0.08217106014490128\n",
      "Epoch 22395/30000 Training Loss: 0.0612286776304245\n",
      "Epoch 22396/30000 Training Loss: 0.05851783975958824\n",
      "Epoch 22397/30000 Training Loss: 0.07453089952468872\n",
      "Epoch 22398/30000 Training Loss: 0.06757697463035583\n",
      "Epoch 22399/30000 Training Loss: 0.06447215378284454\n",
      "Epoch 22400/30000 Training Loss: 0.0729285255074501\n",
      "Epoch 22400/30000 Validation Loss: 0.0731356143951416\n",
      "Epoch 22401/30000 Training Loss: 0.07376107573509216\n",
      "Epoch 22402/30000 Training Loss: 0.07965066283941269\n",
      "Epoch 22403/30000 Training Loss: 0.059303563088178635\n",
      "Epoch 22404/30000 Training Loss: 0.07327008992433548\n",
      "Epoch 22405/30000 Training Loss: 0.09165668487548828\n",
      "Epoch 22406/30000 Training Loss: 0.06488051265478134\n",
      "Epoch 22407/30000 Training Loss: 0.08340442925691605\n",
      "Epoch 22408/30000 Training Loss: 0.07996562123298645\n",
      "Epoch 22409/30000 Training Loss: 0.07764548808336258\n",
      "Epoch 22410/30000 Training Loss: 0.07295655459165573\n",
      "Epoch 22410/30000 Validation Loss: 0.07942318171262741\n",
      "Epoch 22411/30000 Training Loss: 0.06280583888292313\n",
      "Epoch 22412/30000 Training Loss: 0.06413532048463821\n",
      "Epoch 22413/30000 Training Loss: 0.09179854393005371\n",
      "Epoch 22414/30000 Training Loss: 0.06900748610496521\n",
      "Epoch 22415/30000 Training Loss: 0.08045043796300888\n",
      "Epoch 22416/30000 Training Loss: 0.07344845682382584\n",
      "Epoch 22417/30000 Training Loss: 0.07574998587369919\n",
      "Epoch 22418/30000 Training Loss: 0.07957044988870621\n",
      "Epoch 22419/30000 Training Loss: 0.07522160559892654\n",
      "Epoch 22420/30000 Training Loss: 0.07506340742111206\n",
      "Epoch 22420/30000 Validation Loss: 0.06349357217550278\n",
      "Epoch 22421/30000 Training Loss: 0.07714829593896866\n",
      "Epoch 22422/30000 Training Loss: 0.07616765052080154\n",
      "Epoch 22423/30000 Training Loss: 0.08481241017580032\n",
      "Epoch 22424/30000 Training Loss: 0.05460028350353241\n",
      "Epoch 22425/30000 Training Loss: 0.06830517202615738\n",
      "Epoch 22426/30000 Training Loss: 0.08161553740501404\n",
      "Epoch 22427/30000 Training Loss: 0.08659271150827408\n",
      "Epoch 22428/30000 Training Loss: 0.07709603756666183\n",
      "Epoch 22429/30000 Training Loss: 0.07508552819490433\n",
      "Epoch 22430/30000 Training Loss: 0.07451889663934708\n",
      "Epoch 22430/30000 Validation Loss: 0.0642053484916687\n",
      "Epoch 22431/30000 Training Loss: 0.06300004571676254\n",
      "Epoch 22432/30000 Training Loss: 0.07945362478494644\n",
      "Epoch 22433/30000 Training Loss: 0.05975770950317383\n",
      "Epoch 22434/30000 Training Loss: 0.08172056078910828\n",
      "Epoch 22435/30000 Training Loss: 0.0690184235572815\n",
      "Epoch 22436/30000 Training Loss: 0.07354513555765152\n",
      "Epoch 22437/30000 Training Loss: 0.0767008438706398\n",
      "Epoch 22438/30000 Training Loss: 0.07995771616697311\n",
      "Epoch 22439/30000 Training Loss: 0.07925590127706528\n",
      "Epoch 22440/30000 Training Loss: 0.0757979229092598\n",
      "Epoch 22440/30000 Validation Loss: 0.0604952909052372\n",
      "Epoch 22441/30000 Training Loss: 0.054433632642030716\n",
      "Epoch 22442/30000 Training Loss: 0.09943032264709473\n",
      "Epoch 22443/30000 Training Loss: 0.0650910809636116\n",
      "Epoch 22444/30000 Training Loss: 0.061668749898672104\n",
      "Epoch 22445/30000 Training Loss: 0.08648935705423355\n",
      "Epoch 22446/30000 Training Loss: 0.056591957807540894\n",
      "Epoch 22447/30000 Training Loss: 0.07093077898025513\n",
      "Epoch 22448/30000 Training Loss: 0.0638008639216423\n",
      "Epoch 22449/30000 Training Loss: 0.08417501300573349\n",
      "Epoch 22450/30000 Training Loss: 0.09015098959207535\n",
      "Epoch 22450/30000 Validation Loss: 0.0766887292265892\n",
      "Epoch 22451/30000 Training Loss: 0.09997871518135071\n",
      "Epoch 22452/30000 Training Loss: 0.07701171189546585\n",
      "Epoch 22453/30000 Training Loss: 0.06824872642755508\n",
      "Epoch 22454/30000 Training Loss: 0.056583940982818604\n",
      "Epoch 22455/30000 Training Loss: 0.07067343592643738\n",
      "Epoch 22456/30000 Training Loss: 0.06116529926657677\n",
      "Epoch 22457/30000 Training Loss: 0.06533005833625793\n",
      "Epoch 22458/30000 Training Loss: 0.07852818816900253\n",
      "Epoch 22459/30000 Training Loss: 0.08803985267877579\n",
      "Epoch 22460/30000 Training Loss: 0.08823952823877335\n",
      "Epoch 22460/30000 Validation Loss: 0.06522970646619797\n",
      "Epoch 22461/30000 Training Loss: 0.06745393574237823\n",
      "Epoch 22462/30000 Training Loss: 0.09219592809677124\n",
      "Epoch 22463/30000 Training Loss: 0.07421746104955673\n",
      "Epoch 22464/30000 Training Loss: 0.06662747263908386\n",
      "Epoch 22465/30000 Training Loss: 0.07501113414764404\n",
      "Epoch 22466/30000 Training Loss: 0.05577841401100159\n",
      "Epoch 22467/30000 Training Loss: 0.0770839974284172\n",
      "Epoch 22468/30000 Training Loss: 0.0858064666390419\n",
      "Epoch 22469/30000 Training Loss: 0.061115819960832596\n",
      "Epoch 22470/30000 Training Loss: 0.058024633675813675\n",
      "Epoch 22470/30000 Validation Loss: 0.07594802975654602\n",
      "Epoch 22471/30000 Training Loss: 0.06441781669855118\n",
      "Epoch 22472/30000 Training Loss: 0.05760999396443367\n",
      "Epoch 22473/30000 Training Loss: 0.08234098553657532\n",
      "Epoch 22474/30000 Training Loss: 0.055944789201021194\n",
      "Epoch 22475/30000 Training Loss: 0.06226770952343941\n",
      "Epoch 22476/30000 Training Loss: 0.06662631779909134\n",
      "Epoch 22477/30000 Training Loss: 0.06014900282025337\n",
      "Epoch 22478/30000 Training Loss: 0.07557174563407898\n",
      "Epoch 22479/30000 Training Loss: 0.0734550952911377\n",
      "Epoch 22480/30000 Training Loss: 0.08266735076904297\n",
      "Epoch 22480/30000 Validation Loss: 0.07053806632757187\n",
      "Epoch 22481/30000 Training Loss: 0.08589348196983337\n",
      "Epoch 22482/30000 Training Loss: 0.058639515191316605\n",
      "Epoch 22483/30000 Training Loss: 0.06282616406679153\n",
      "Epoch 22484/30000 Training Loss: 0.0634746253490448\n",
      "Epoch 22485/30000 Training Loss: 0.05274517834186554\n",
      "Epoch 22486/30000 Training Loss: 0.06283337622880936\n",
      "Epoch 22487/30000 Training Loss: 0.0925847515463829\n",
      "Epoch 22488/30000 Training Loss: 0.06287077814340591\n",
      "Epoch 22489/30000 Training Loss: 0.05929635837674141\n",
      "Epoch 22490/30000 Training Loss: 0.07141300290822983\n",
      "Epoch 22490/30000 Validation Loss: 0.08100537210702896\n",
      "Epoch 22491/30000 Training Loss: 0.06095599755644798\n",
      "Epoch 22492/30000 Training Loss: 0.06501574069261551\n",
      "Epoch 22493/30000 Training Loss: 0.06196404621005058\n",
      "Epoch 22494/30000 Training Loss: 0.06932410597801208\n",
      "Epoch 22495/30000 Training Loss: 0.0737699493765831\n",
      "Epoch 22496/30000 Training Loss: 0.05951143801212311\n",
      "Epoch 22497/30000 Training Loss: 0.08316730707883835\n",
      "Epoch 22498/30000 Training Loss: 0.0853709876537323\n",
      "Epoch 22499/30000 Training Loss: 0.07741013169288635\n",
      "Epoch 22500/30000 Training Loss: 0.06097221001982689\n",
      "Epoch 22500/30000 Validation Loss: 0.06682727485895157\n",
      "Epoch 22501/30000 Training Loss: 0.08973721414804459\n",
      "Epoch 22502/30000 Training Loss: 0.06840384751558304\n",
      "Epoch 22503/30000 Training Loss: 0.07136263698339462\n",
      "Epoch 22504/30000 Training Loss: 0.08488722890615463\n",
      "Epoch 22505/30000 Training Loss: 0.055301692336797714\n",
      "Epoch 22506/30000 Training Loss: 0.06389304995536804\n",
      "Epoch 22507/30000 Training Loss: 0.06268834322690964\n",
      "Epoch 22508/30000 Training Loss: 0.05771799013018608\n",
      "Epoch 22509/30000 Training Loss: 0.06954794377088547\n",
      "Epoch 22510/30000 Training Loss: 0.06761305779218674\n",
      "Epoch 22510/30000 Validation Loss: 0.06659398972988129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22511/30000 Training Loss: 0.06328848749399185\n",
      "Epoch 22512/30000 Training Loss: 0.07506205886602402\n",
      "Epoch 22513/30000 Training Loss: 0.0857049822807312\n",
      "Epoch 22514/30000 Training Loss: 0.05335872992873192\n",
      "Epoch 22515/30000 Training Loss: 0.06964672356843948\n",
      "Epoch 22516/30000 Training Loss: 0.08315172046422958\n",
      "Epoch 22517/30000 Training Loss: 0.051691338419914246\n",
      "Epoch 22518/30000 Training Loss: 0.06376712024211884\n",
      "Epoch 22519/30000 Training Loss: 0.0770605280995369\n",
      "Epoch 22520/30000 Training Loss: 0.08567079156637192\n",
      "Epoch 22520/30000 Validation Loss: 0.07191916555166245\n",
      "Epoch 22521/30000 Training Loss: 0.08425071090459824\n",
      "Epoch 22522/30000 Training Loss: 0.08558899164199829\n",
      "Epoch 22523/30000 Training Loss: 0.06010981276631355\n",
      "Epoch 22524/30000 Training Loss: 0.05317656323313713\n",
      "Epoch 22525/30000 Training Loss: 0.06685008853673935\n",
      "Epoch 22526/30000 Training Loss: 0.06204644963145256\n",
      "Epoch 22527/30000 Training Loss: 0.08554895967245102\n",
      "Epoch 22528/30000 Training Loss: 0.06882203370332718\n",
      "Epoch 22529/30000 Training Loss: 0.07405619323253632\n",
      "Epoch 22530/30000 Training Loss: 0.06182311847805977\n",
      "Epoch 22530/30000 Validation Loss: 0.06051264703273773\n",
      "Epoch 22531/30000 Training Loss: 0.07254188507795334\n",
      "Epoch 22532/30000 Training Loss: 0.08534463495016098\n",
      "Epoch 22533/30000 Training Loss: 0.05652722716331482\n",
      "Epoch 22534/30000 Training Loss: 0.07378259301185608\n",
      "Epoch 22535/30000 Training Loss: 0.07665177434682846\n",
      "Epoch 22536/30000 Training Loss: 0.07582662999629974\n",
      "Epoch 22537/30000 Training Loss: 0.07501243054866791\n",
      "Epoch 22538/30000 Training Loss: 0.057908620685338974\n",
      "Epoch 22539/30000 Training Loss: 0.05534407123923302\n",
      "Epoch 22540/30000 Training Loss: 0.06622809916734695\n",
      "Epoch 22540/30000 Validation Loss: 0.07019532471895218\n",
      "Epoch 22541/30000 Training Loss: 0.07249950617551804\n",
      "Epoch 22542/30000 Training Loss: 0.07553305476903915\n",
      "Epoch 22543/30000 Training Loss: 0.05878119543194771\n",
      "Epoch 22544/30000 Training Loss: 0.08308523148298264\n",
      "Epoch 22545/30000 Training Loss: 0.06358126550912857\n",
      "Epoch 22546/30000 Training Loss: 0.07179441303014755\n",
      "Epoch 22547/30000 Training Loss: 0.07164078950881958\n",
      "Epoch 22548/30000 Training Loss: 0.0791194960474968\n",
      "Epoch 22549/30000 Training Loss: 0.07057694345712662\n",
      "Epoch 22550/30000 Training Loss: 0.10590764135122299\n",
      "Epoch 22550/30000 Validation Loss: 0.0874948725104332\n",
      "Epoch 22551/30000 Training Loss: 0.059318315237760544\n",
      "Epoch 22552/30000 Training Loss: 0.06404741108417511\n",
      "Epoch 22553/30000 Training Loss: 0.06665492057800293\n",
      "Epoch 22554/30000 Training Loss: 0.07072009146213531\n",
      "Epoch 22555/30000 Training Loss: 0.07767704129219055\n",
      "Epoch 22556/30000 Training Loss: 0.08130024373531342\n",
      "Epoch 22557/30000 Training Loss: 0.07209987938404083\n",
      "Epoch 22558/30000 Training Loss: 0.07468528300523758\n",
      "Epoch 22559/30000 Training Loss: 0.07400613278150558\n",
      "Epoch 22560/30000 Training Loss: 0.0871608629822731\n",
      "Epoch 22560/30000 Validation Loss: 0.07083014398813248\n",
      "Epoch 22561/30000 Training Loss: 0.0522797591984272\n",
      "Epoch 22562/30000 Training Loss: 0.08205077052116394\n",
      "Epoch 22563/30000 Training Loss: 0.0729835033416748\n",
      "Epoch 22564/30000 Training Loss: 0.08174631744623184\n",
      "Epoch 22565/30000 Training Loss: 0.06990539282560349\n",
      "Epoch 22566/30000 Training Loss: 0.061029303818941116\n",
      "Epoch 22567/30000 Training Loss: 0.07119234651327133\n",
      "Epoch 22568/30000 Training Loss: 0.055899057537317276\n",
      "Epoch 22569/30000 Training Loss: 0.08403400331735611\n",
      "Epoch 22570/30000 Training Loss: 0.07766153663396835\n",
      "Epoch 22570/30000 Validation Loss: 0.07303566485643387\n",
      "Epoch 22571/30000 Training Loss: 0.06770244985818863\n",
      "Epoch 22572/30000 Training Loss: 0.05756794288754463\n",
      "Epoch 22573/30000 Training Loss: 0.06584060192108154\n",
      "Epoch 22574/30000 Training Loss: 0.06412739306688309\n",
      "Epoch 22575/30000 Training Loss: 0.06875866651535034\n",
      "Epoch 22576/30000 Training Loss: 0.05642961338162422\n",
      "Epoch 22577/30000 Training Loss: 0.07287511229515076\n",
      "Epoch 22578/30000 Training Loss: 0.07937183231115341\n",
      "Epoch 22579/30000 Training Loss: 0.06759712845087051\n",
      "Epoch 22580/30000 Training Loss: 0.057667579501867294\n",
      "Epoch 22580/30000 Validation Loss: 0.08964019268751144\n",
      "Epoch 22581/30000 Training Loss: 0.0680636465549469\n",
      "Epoch 22582/30000 Training Loss: 0.05442296341061592\n",
      "Epoch 22583/30000 Training Loss: 0.06734485179185867\n",
      "Epoch 22584/30000 Training Loss: 0.05978472903370857\n",
      "Epoch 22585/30000 Training Loss: 0.07583091408014297\n",
      "Epoch 22586/30000 Training Loss: 0.05990232899785042\n",
      "Epoch 22587/30000 Training Loss: 0.06635839492082596\n",
      "Epoch 22588/30000 Training Loss: 0.05473845824599266\n",
      "Epoch 22589/30000 Training Loss: 0.07223282009363174\n",
      "Epoch 22590/30000 Training Loss: 0.07192779332399368\n",
      "Epoch 22590/30000 Validation Loss: 0.07243543118238449\n",
      "Epoch 22591/30000 Training Loss: 0.062413353472948074\n",
      "Epoch 22592/30000 Training Loss: 0.06356068700551987\n",
      "Epoch 22593/30000 Training Loss: 0.04802538454532623\n",
      "Epoch 22594/30000 Training Loss: 0.06575757265090942\n",
      "Epoch 22595/30000 Training Loss: 0.07864407449960709\n",
      "Epoch 22596/30000 Training Loss: 0.07850533723831177\n",
      "Epoch 22597/30000 Training Loss: 0.05887536704540253\n",
      "Epoch 22598/30000 Training Loss: 0.07969606667757034\n",
      "Epoch 22599/30000 Training Loss: 0.07121339440345764\n",
      "Epoch 22600/30000 Training Loss: 0.05726676061749458\n",
      "Epoch 22600/30000 Validation Loss: 0.07803917676210403\n",
      "Epoch 22601/30000 Training Loss: 0.06503543257713318\n",
      "Epoch 22602/30000 Training Loss: 0.05934173986315727\n",
      "Epoch 22603/30000 Training Loss: 0.09183260798454285\n",
      "Epoch 22604/30000 Training Loss: 0.072902612388134\n",
      "Epoch 22605/30000 Training Loss: 0.06848100572824478\n",
      "Epoch 22606/30000 Training Loss: 0.09307575970888138\n",
      "Epoch 22607/30000 Training Loss: 0.05520591512322426\n",
      "Epoch 22608/30000 Training Loss: 0.08194746822118759\n",
      "Epoch 22609/30000 Training Loss: 0.07947827130556107\n",
      "Epoch 22610/30000 Training Loss: 0.07131313532590866\n",
      "Epoch 22610/30000 Validation Loss: 0.07922350615262985\n",
      "Epoch 22611/30000 Training Loss: 0.06944658607244492\n",
      "Epoch 22612/30000 Training Loss: 0.06677913665771484\n",
      "Epoch 22613/30000 Training Loss: 0.06854566186666489\n",
      "Epoch 22614/30000 Training Loss: 0.06944575160741806\n",
      "Epoch 22615/30000 Training Loss: 0.07686925679445267\n",
      "Epoch 22616/30000 Training Loss: 0.07376927137374878\n",
      "Epoch 22617/30000 Training Loss: 0.06327705830335617\n",
      "Epoch 22618/30000 Training Loss: 0.06225663423538208\n",
      "Epoch 22619/30000 Training Loss: 0.07208839058876038\n",
      "Epoch 22620/30000 Training Loss: 0.0686209425330162\n",
      "Epoch 22620/30000 Validation Loss: 0.07395824044942856\n",
      "Epoch 22621/30000 Training Loss: 0.08597678691148758\n",
      "Epoch 22622/30000 Training Loss: 0.0771908387541771\n",
      "Epoch 22623/30000 Training Loss: 0.06122672185301781\n",
      "Epoch 22624/30000 Training Loss: 0.06717511266469955\n",
      "Epoch 22625/30000 Training Loss: 0.05403304472565651\n",
      "Epoch 22626/30000 Training Loss: 0.07297313958406448\n",
      "Epoch 22627/30000 Training Loss: 0.08017223328351974\n",
      "Epoch 22628/30000 Training Loss: 0.07192080467939377\n",
      "Epoch 22629/30000 Training Loss: 0.05887414887547493\n",
      "Epoch 22630/30000 Training Loss: 0.061308603733778\n",
      "Epoch 22630/30000 Validation Loss: 0.07071607559919357\n",
      "Epoch 22631/30000 Training Loss: 0.06472630053758621\n",
      "Epoch 22632/30000 Training Loss: 0.06966298073530197\n",
      "Epoch 22633/30000 Training Loss: 0.0747971311211586\n",
      "Epoch 22634/30000 Training Loss: 0.07057441771030426\n",
      "Epoch 22635/30000 Training Loss: 0.07715173810720444\n",
      "Epoch 22636/30000 Training Loss: 0.06829023361206055\n",
      "Epoch 22637/30000 Training Loss: 0.06702287495136261\n",
      "Epoch 22638/30000 Training Loss: 0.0784665048122406\n",
      "Epoch 22639/30000 Training Loss: 0.07416847348213196\n",
      "Epoch 22640/30000 Training Loss: 0.07071954011917114\n",
      "Epoch 22640/30000 Validation Loss: 0.0640728697180748\n",
      "Epoch 22641/30000 Training Loss: 0.06416061520576477\n",
      "Epoch 22642/30000 Training Loss: 0.05601583793759346\n",
      "Epoch 22643/30000 Training Loss: 0.05103260651230812\n",
      "Epoch 22644/30000 Training Loss: 0.0813458263874054\n",
      "Epoch 22645/30000 Training Loss: 0.06854632496833801\n",
      "Epoch 22646/30000 Training Loss: 0.07502072304487228\n",
      "Epoch 22647/30000 Training Loss: 0.05172353982925415\n",
      "Epoch 22648/30000 Training Loss: 0.08505628257989883\n",
      "Epoch 22649/30000 Training Loss: 0.08637222647666931\n",
      "Epoch 22650/30000 Training Loss: 0.0720871314406395\n",
      "Epoch 22650/30000 Validation Loss: 0.08504534512758255\n",
      "Epoch 22651/30000 Training Loss: 0.06754083931446075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22652/30000 Training Loss: 0.08562642335891724\n",
      "Epoch 22653/30000 Training Loss: 0.07275926321744919\n",
      "Epoch 22654/30000 Training Loss: 0.06513661891222\n",
      "Epoch 22655/30000 Training Loss: 0.0708591490983963\n",
      "Epoch 22656/30000 Training Loss: 0.07110965251922607\n",
      "Epoch 22657/30000 Training Loss: 0.06428835541009903\n",
      "Epoch 22658/30000 Training Loss: 0.06591450423002243\n",
      "Epoch 22659/30000 Training Loss: 0.06411942094564438\n",
      "Epoch 22660/30000 Training Loss: 0.07452739775180817\n",
      "Epoch 22660/30000 Validation Loss: 0.06711671501398087\n",
      "Epoch 22661/30000 Training Loss: 0.07369642704725266\n",
      "Epoch 22662/30000 Training Loss: 0.07598995417356491\n",
      "Epoch 22663/30000 Training Loss: 0.0692991092801094\n",
      "Epoch 22664/30000 Training Loss: 0.06385978311300278\n",
      "Epoch 22665/30000 Training Loss: 0.059843603521585464\n",
      "Epoch 22666/30000 Training Loss: 0.0677710548043251\n",
      "Epoch 22667/30000 Training Loss: 0.07401756197214127\n",
      "Epoch 22668/30000 Training Loss: 0.07184562087059021\n",
      "Epoch 22669/30000 Training Loss: 0.06865842640399933\n",
      "Epoch 22670/30000 Training Loss: 0.0676705539226532\n",
      "Epoch 22670/30000 Validation Loss: 0.059065934270620346\n",
      "Epoch 22671/30000 Training Loss: 0.07162422686815262\n",
      "Epoch 22672/30000 Training Loss: 0.07657264918088913\n",
      "Epoch 22673/30000 Training Loss: 0.06460846215486526\n",
      "Epoch 22674/30000 Training Loss: 0.06513787060976028\n",
      "Epoch 22675/30000 Training Loss: 0.07334201782941818\n",
      "Epoch 22676/30000 Training Loss: 0.06500830501317978\n",
      "Epoch 22677/30000 Training Loss: 0.0716232880949974\n",
      "Epoch 22678/30000 Training Loss: 0.06361192464828491\n",
      "Epoch 22679/30000 Training Loss: 0.06044724956154823\n",
      "Epoch 22680/30000 Training Loss: 0.07684285193681717\n",
      "Epoch 22680/30000 Validation Loss: 0.062217261642217636\n",
      "Epoch 22681/30000 Training Loss: 0.0686795711517334\n",
      "Epoch 22682/30000 Training Loss: 0.0850166380405426\n",
      "Epoch 22683/30000 Training Loss: 0.07317928224802017\n",
      "Epoch 22684/30000 Training Loss: 0.06335635483264923\n",
      "Epoch 22685/30000 Training Loss: 0.06300654262304306\n",
      "Epoch 22686/30000 Training Loss: 0.08553982526063919\n",
      "Epoch 22687/30000 Training Loss: 0.054618898779153824\n",
      "Epoch 22688/30000 Training Loss: 0.0900161936879158\n",
      "Epoch 22689/30000 Training Loss: 0.06791598349809647\n",
      "Epoch 22690/30000 Training Loss: 0.061578620225191116\n",
      "Epoch 22690/30000 Validation Loss: 0.06323950737714767\n",
      "Epoch 22691/30000 Training Loss: 0.08932942897081375\n",
      "Epoch 22692/30000 Training Loss: 0.06249837204813957\n",
      "Epoch 22693/30000 Training Loss: 0.08776430040597916\n",
      "Epoch 22694/30000 Training Loss: 0.06142900884151459\n",
      "Epoch 22695/30000 Training Loss: 0.07164175063371658\n",
      "Epoch 22696/30000 Training Loss: 0.07094849646091461\n",
      "Epoch 22697/30000 Training Loss: 0.05575524643063545\n",
      "Epoch 22698/30000 Training Loss: 0.050534892827272415\n",
      "Epoch 22699/30000 Training Loss: 0.08235707134008408\n",
      "Epoch 22700/30000 Training Loss: 0.06837549805641174\n",
      "Epoch 22700/30000 Validation Loss: 0.07669896632432938\n",
      "Epoch 22701/30000 Training Loss: 0.07682998478412628\n",
      "Epoch 22702/30000 Training Loss: 0.06253641843795776\n",
      "Epoch 22703/30000 Training Loss: 0.06876987963914871\n",
      "Epoch 22704/30000 Training Loss: 0.07336565852165222\n",
      "Epoch 22705/30000 Training Loss: 0.06961330026388168\n",
      "Epoch 22706/30000 Training Loss: 0.08668481558561325\n",
      "Epoch 22707/30000 Training Loss: 0.0630899965763092\n",
      "Epoch 22708/30000 Training Loss: 0.057005900889635086\n",
      "Epoch 22709/30000 Training Loss: 0.05302327498793602\n",
      "Epoch 22710/30000 Training Loss: 0.05405591055750847\n",
      "Epoch 22710/30000 Validation Loss: 0.075251005589962\n",
      "Epoch 22711/30000 Training Loss: 0.06246606633067131\n",
      "Epoch 22712/30000 Training Loss: 0.07635209709405899\n",
      "Epoch 22713/30000 Training Loss: 0.06297742575407028\n",
      "Epoch 22714/30000 Training Loss: 0.07204619795084\n",
      "Epoch 22715/30000 Training Loss: 0.0681244507431984\n",
      "Epoch 22716/30000 Training Loss: 0.07715339213609695\n",
      "Epoch 22717/30000 Training Loss: 0.07598502933979034\n",
      "Epoch 22718/30000 Training Loss: 0.06235456466674805\n",
      "Epoch 22719/30000 Training Loss: 0.07320045679807663\n",
      "Epoch 22720/30000 Training Loss: 0.05553561821579933\n",
      "Epoch 22720/30000 Validation Loss: 0.07189766317605972\n",
      "Epoch 22721/30000 Training Loss: 0.082563616335392\n",
      "Epoch 22722/30000 Training Loss: 0.0720386728644371\n",
      "Epoch 22723/30000 Training Loss: 0.072153739631176\n",
      "Epoch 22724/30000 Training Loss: 0.07605955749750137\n",
      "Epoch 22725/30000 Training Loss: 0.08266919106245041\n",
      "Epoch 22726/30000 Training Loss: 0.07405663281679153\n",
      "Epoch 22727/30000 Training Loss: 0.06635380536317825\n",
      "Epoch 22728/30000 Training Loss: 0.05338014289736748\n",
      "Epoch 22729/30000 Training Loss: 0.05851275846362114\n",
      "Epoch 22730/30000 Training Loss: 0.06477368623018265\n",
      "Epoch 22730/30000 Validation Loss: 0.06360425800085068\n",
      "Epoch 22731/30000 Training Loss: 0.06474679708480835\n",
      "Epoch 22732/30000 Training Loss: 0.0693906843662262\n",
      "Epoch 22733/30000 Training Loss: 0.07880149781703949\n",
      "Epoch 22734/30000 Training Loss: 0.06356271356344223\n",
      "Epoch 22735/30000 Training Loss: 0.07614970952272415\n",
      "Epoch 22736/30000 Training Loss: 0.06427469849586487\n",
      "Epoch 22737/30000 Training Loss: 0.05070368945598602\n",
      "Epoch 22738/30000 Training Loss: 0.0628950372338295\n",
      "Epoch 22739/30000 Training Loss: 0.05932992324233055\n",
      "Epoch 22740/30000 Training Loss: 0.06570574641227722\n",
      "Epoch 22740/30000 Validation Loss: 0.058873698115348816\n",
      "Epoch 22741/30000 Training Loss: 0.06527476757764816\n",
      "Epoch 22742/30000 Training Loss: 0.07594325393438339\n",
      "Epoch 22743/30000 Training Loss: 0.07616874575614929\n",
      "Epoch 22744/30000 Training Loss: 0.07280733436346054\n",
      "Epoch 22745/30000 Training Loss: 0.07666417956352234\n",
      "Epoch 22746/30000 Training Loss: 0.06192043796181679\n",
      "Epoch 22747/30000 Training Loss: 0.05765025317668915\n",
      "Epoch 22748/30000 Training Loss: 0.05424654483795166\n",
      "Epoch 22749/30000 Training Loss: 0.07971528172492981\n",
      "Epoch 22750/30000 Training Loss: 0.07293098419904709\n",
      "Epoch 22750/30000 Validation Loss: 0.1004033088684082\n",
      "Epoch 22751/30000 Training Loss: 0.07296256721019745\n",
      "Epoch 22752/30000 Training Loss: 0.09113553166389465\n",
      "Epoch 22753/30000 Training Loss: 0.06898680329322815\n",
      "Epoch 22754/30000 Training Loss: 0.07481757551431656\n",
      "Epoch 22755/30000 Training Loss: 0.07190931588411331\n",
      "Epoch 22756/30000 Training Loss: 0.09593119472265244\n",
      "Epoch 22757/30000 Training Loss: 0.05357930064201355\n",
      "Epoch 22758/30000 Training Loss: 0.0686160996556282\n",
      "Epoch 22759/30000 Training Loss: 0.06158624216914177\n",
      "Epoch 22760/30000 Training Loss: 0.05975307524204254\n",
      "Epoch 22760/30000 Validation Loss: 0.05502612888813019\n",
      "Epoch 22761/30000 Training Loss: 0.0670553669333458\n",
      "Epoch 22762/30000 Training Loss: 0.07638301700353622\n",
      "Epoch 22763/30000 Training Loss: 0.06528187543153763\n",
      "Epoch 22764/30000 Training Loss: 0.06105521321296692\n",
      "Epoch 22765/30000 Training Loss: 0.05744289979338646\n",
      "Epoch 22766/30000 Training Loss: 0.06765493750572205\n",
      "Epoch 22767/30000 Training Loss: 0.0668211504817009\n",
      "Epoch 22768/30000 Training Loss: 0.06224125251173973\n",
      "Epoch 22769/30000 Training Loss: 0.08395149558782578\n",
      "Epoch 22770/30000 Training Loss: 0.070607990026474\n",
      "Epoch 22770/30000 Validation Loss: 0.06551352143287659\n",
      "Epoch 22771/30000 Training Loss: 0.06940218061208725\n",
      "Epoch 22772/30000 Training Loss: 0.058626171201467514\n",
      "Epoch 22773/30000 Training Loss: 0.06273224204778671\n",
      "Epoch 22774/30000 Training Loss: 0.06750661134719849\n",
      "Epoch 22775/30000 Training Loss: 0.07054579257965088\n",
      "Epoch 22776/30000 Training Loss: 0.09149458259344101\n",
      "Epoch 22777/30000 Training Loss: 0.04988805949687958\n",
      "Epoch 22778/30000 Training Loss: 0.07194312661886215\n",
      "Epoch 22779/30000 Training Loss: 0.05600801482796669\n",
      "Epoch 22780/30000 Training Loss: 0.07206297665834427\n",
      "Epoch 22780/30000 Validation Loss: 0.09174883365631104\n",
      "Epoch 22781/30000 Training Loss: 0.07346341013908386\n",
      "Epoch 22782/30000 Training Loss: 0.10312158614397049\n",
      "Epoch 22783/30000 Training Loss: 0.08166080713272095\n",
      "Epoch 22784/30000 Training Loss: 0.07389622181653976\n",
      "Epoch 22785/30000 Training Loss: 0.05879391357302666\n",
      "Epoch 22786/30000 Training Loss: 0.05876113474369049\n",
      "Epoch 22787/30000 Training Loss: 0.06650169938802719\n",
      "Epoch 22788/30000 Training Loss: 0.06992214173078537\n",
      "Epoch 22789/30000 Training Loss: 0.07751179486513138\n",
      "Epoch 22790/30000 Training Loss: 0.06776060909032822\n",
      "Epoch 22790/30000 Validation Loss: 0.06525284796953201\n",
      "Epoch 22791/30000 Training Loss: 0.05485957860946655\n",
      "Epoch 22792/30000 Training Loss: 0.06788914650678635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22793/30000 Training Loss: 0.06399809569120407\n",
      "Epoch 22794/30000 Training Loss: 0.061558496206998825\n",
      "Epoch 22795/30000 Training Loss: 0.09030557423830032\n",
      "Epoch 22796/30000 Training Loss: 0.06158233806490898\n",
      "Epoch 22797/30000 Training Loss: 0.07044442743062973\n",
      "Epoch 22798/30000 Training Loss: 0.06741312891244888\n",
      "Epoch 22799/30000 Training Loss: 0.055645767599344254\n",
      "Epoch 22800/30000 Training Loss: 0.0650707483291626\n",
      "Epoch 22800/30000 Validation Loss: 0.06644109636545181\n",
      "Epoch 22801/30000 Training Loss: 0.055504798889160156\n",
      "Epoch 22802/30000 Training Loss: 0.06483892351388931\n",
      "Epoch 22803/30000 Training Loss: 0.07967106252908707\n",
      "Epoch 22804/30000 Training Loss: 0.07945246994495392\n",
      "Epoch 22805/30000 Training Loss: 0.0835791826248169\n",
      "Epoch 22806/30000 Training Loss: 0.08077450096607208\n",
      "Epoch 22807/30000 Training Loss: 0.07315337657928467\n",
      "Epoch 22808/30000 Training Loss: 0.0642734095454216\n",
      "Epoch 22809/30000 Training Loss: 0.058955151587724686\n",
      "Epoch 22810/30000 Training Loss: 0.06979719549417496\n",
      "Epoch 22810/30000 Validation Loss: 0.059169039130210876\n",
      "Epoch 22811/30000 Training Loss: 0.06226008012890816\n",
      "Epoch 22812/30000 Training Loss: 0.06633163243532181\n",
      "Epoch 22813/30000 Training Loss: 0.08264006674289703\n",
      "Epoch 22814/30000 Training Loss: 0.05371759459376335\n",
      "Epoch 22815/30000 Training Loss: 0.06581658124923706\n",
      "Epoch 22816/30000 Training Loss: 0.052837420254945755\n",
      "Epoch 22817/30000 Training Loss: 0.06646552681922913\n",
      "Epoch 22818/30000 Training Loss: 0.060570526868104935\n",
      "Epoch 22819/30000 Training Loss: 0.06606683880090714\n",
      "Epoch 22820/30000 Training Loss: 0.0641554519534111\n",
      "Epoch 22820/30000 Validation Loss: 0.05641995742917061\n",
      "Epoch 22821/30000 Training Loss: 0.09410220384597778\n",
      "Epoch 22822/30000 Training Loss: 0.0895448550581932\n",
      "Epoch 22823/30000 Training Loss: 0.061327606439590454\n",
      "Epoch 22824/30000 Training Loss: 0.07782043516635895\n",
      "Epoch 22825/30000 Training Loss: 0.07902127504348755\n",
      "Epoch 22826/30000 Training Loss: 0.07266941666603088\n",
      "Epoch 22827/30000 Training Loss: 0.07835178077220917\n",
      "Epoch 22828/30000 Training Loss: 0.06399298459291458\n",
      "Epoch 22829/30000 Training Loss: 0.05910815671086311\n",
      "Epoch 22830/30000 Training Loss: 0.07764467597007751\n",
      "Epoch 22830/30000 Validation Loss: 0.061023861169815063\n",
      "Epoch 22831/30000 Training Loss: 0.07342473417520523\n",
      "Epoch 22832/30000 Training Loss: 0.05716579034924507\n",
      "Epoch 22833/30000 Training Loss: 0.06806198507547379\n",
      "Epoch 22834/30000 Training Loss: 0.06617724150419235\n",
      "Epoch 22835/30000 Training Loss: 0.06308946758508682\n",
      "Epoch 22836/30000 Training Loss: 0.06882812082767487\n",
      "Epoch 22837/30000 Training Loss: 0.06485672295093536\n",
      "Epoch 22838/30000 Training Loss: 0.05681706592440605\n",
      "Epoch 22839/30000 Training Loss: 0.0940062627196312\n",
      "Epoch 22840/30000 Training Loss: 0.05292530357837677\n",
      "Epoch 22840/30000 Validation Loss: 0.06915915757417679\n",
      "Epoch 22841/30000 Training Loss: 0.08793649822473526\n",
      "Epoch 22842/30000 Training Loss: 0.08992534875869751\n",
      "Epoch 22843/30000 Training Loss: 0.04980647936463356\n",
      "Epoch 22844/30000 Training Loss: 0.06781581789255142\n",
      "Epoch 22845/30000 Training Loss: 0.07813435047864914\n",
      "Epoch 22846/30000 Training Loss: 0.06570572406053543\n",
      "Epoch 22847/30000 Training Loss: 0.07208174467086792\n",
      "Epoch 22848/30000 Training Loss: 0.07113391160964966\n",
      "Epoch 22849/30000 Training Loss: 0.0837397575378418\n",
      "Epoch 22850/30000 Training Loss: 0.06114478036761284\n",
      "Epoch 22850/30000 Validation Loss: 0.08248439431190491\n",
      "Epoch 22851/30000 Training Loss: 0.0652070865035057\n",
      "Epoch 22852/30000 Training Loss: 0.0844096913933754\n",
      "Epoch 22853/30000 Training Loss: 0.07033699005842209\n",
      "Epoch 22854/30000 Training Loss: 0.07193449139595032\n",
      "Epoch 22855/30000 Training Loss: 0.06282306462526321\n",
      "Epoch 22856/30000 Training Loss: 0.06459001451730728\n",
      "Epoch 22857/30000 Training Loss: 0.05777808651328087\n",
      "Epoch 22858/30000 Training Loss: 0.0677911564707756\n",
      "Epoch 22859/30000 Training Loss: 0.07470202445983887\n",
      "Epoch 22860/30000 Training Loss: 0.06549444794654846\n",
      "Epoch 22860/30000 Validation Loss: 0.08084287494421005\n",
      "Epoch 22861/30000 Training Loss: 0.07349253445863724\n",
      "Epoch 22862/30000 Training Loss: 0.09610772132873535\n",
      "Epoch 22863/30000 Training Loss: 0.08789218217134476\n",
      "Epoch 22864/30000 Training Loss: 0.07131092995405197\n",
      "Epoch 22865/30000 Training Loss: 0.05977661535143852\n",
      "Epoch 22866/30000 Training Loss: 0.08289440721273422\n",
      "Epoch 22867/30000 Training Loss: 0.07714604586362839\n",
      "Epoch 22868/30000 Training Loss: 0.0677381083369255\n",
      "Epoch 22869/30000 Training Loss: 0.05388908088207245\n",
      "Epoch 22870/30000 Training Loss: 0.07705270498991013\n",
      "Epoch 22870/30000 Validation Loss: 0.08242403715848923\n",
      "Epoch 22871/30000 Training Loss: 0.06402841210365295\n",
      "Epoch 22872/30000 Training Loss: 0.05419089272618294\n",
      "Epoch 22873/30000 Training Loss: 0.06524351239204407\n",
      "Epoch 22874/30000 Training Loss: 0.06681652367115021\n",
      "Epoch 22875/30000 Training Loss: 0.06346144527196884\n",
      "Epoch 22876/30000 Training Loss: 0.06646842509508133\n",
      "Epoch 22877/30000 Training Loss: 0.08397579193115234\n",
      "Epoch 22878/30000 Training Loss: 0.08041518926620483\n",
      "Epoch 22879/30000 Training Loss: 0.08876325935125351\n",
      "Epoch 22880/30000 Training Loss: 0.05045006051659584\n",
      "Epoch 22880/30000 Validation Loss: 0.060095202177762985\n",
      "Epoch 22881/30000 Training Loss: 0.06524952501058578\n",
      "Epoch 22882/30000 Training Loss: 0.06646240502595901\n",
      "Epoch 22883/30000 Training Loss: 0.08118744939565659\n",
      "Epoch 22884/30000 Training Loss: 0.05960150063037872\n",
      "Epoch 22885/30000 Training Loss: 0.061810631304979324\n",
      "Epoch 22886/30000 Training Loss: 0.06315793842077255\n",
      "Epoch 22887/30000 Training Loss: 0.07645171880722046\n",
      "Epoch 22888/30000 Training Loss: 0.054569825530052185\n",
      "Epoch 22889/30000 Training Loss: 0.07476183027029037\n",
      "Epoch 22890/30000 Training Loss: 0.055947449058294296\n",
      "Epoch 22890/30000 Validation Loss: 0.07060448825359344\n",
      "Epoch 22891/30000 Training Loss: 0.08531195670366287\n",
      "Epoch 22892/30000 Training Loss: 0.05901084840297699\n",
      "Epoch 22893/30000 Training Loss: 0.06507766991853714\n",
      "Epoch 22894/30000 Training Loss: 0.06292051821947098\n",
      "Epoch 22895/30000 Training Loss: 0.06920918077230453\n",
      "Epoch 22896/30000 Training Loss: 0.06252752989530563\n",
      "Epoch 22897/30000 Training Loss: 0.06881356239318848\n",
      "Epoch 22898/30000 Training Loss: 0.05759308859705925\n",
      "Epoch 22899/30000 Training Loss: 0.06923414021730423\n",
      "Epoch 22900/30000 Training Loss: 0.07177802920341492\n",
      "Epoch 22900/30000 Validation Loss: 0.06994155049324036\n",
      "Epoch 22901/30000 Training Loss: 0.08185479789972305\n",
      "Epoch 22902/30000 Training Loss: 0.07774144411087036\n",
      "Epoch 22903/30000 Training Loss: 0.08803531527519226\n",
      "Epoch 22904/30000 Training Loss: 0.07107826322317123\n",
      "Epoch 22905/30000 Training Loss: 0.06884368509054184\n",
      "Epoch 22906/30000 Training Loss: 0.07852412015199661\n",
      "Epoch 22907/30000 Training Loss: 0.0767621174454689\n",
      "Epoch 22908/30000 Training Loss: 0.07151611149311066\n",
      "Epoch 22909/30000 Training Loss: 0.05989299342036247\n",
      "Epoch 22910/30000 Training Loss: 0.09063098579645157\n",
      "Epoch 22910/30000 Validation Loss: 0.07779619097709656\n",
      "Epoch 22911/30000 Training Loss: 0.05776980519294739\n",
      "Epoch 22912/30000 Training Loss: 0.08817990869283676\n",
      "Epoch 22913/30000 Training Loss: 0.07605612277984619\n",
      "Epoch 22914/30000 Training Loss: 0.06302189081907272\n",
      "Epoch 22915/30000 Training Loss: 0.062199074774980545\n",
      "Epoch 22916/30000 Training Loss: 0.08372068405151367\n",
      "Epoch 22917/30000 Training Loss: 0.06453641504049301\n",
      "Epoch 22918/30000 Training Loss: 0.07477209717035294\n",
      "Epoch 22919/30000 Training Loss: 0.07159759849309921\n",
      "Epoch 22920/30000 Training Loss: 0.05056697502732277\n",
      "Epoch 22920/30000 Validation Loss: 0.06761521846055984\n",
      "Epoch 22921/30000 Training Loss: 0.08311010152101517\n",
      "Epoch 22922/30000 Training Loss: 0.05922890827059746\n",
      "Epoch 22923/30000 Training Loss: 0.06510099768638611\n",
      "Epoch 22924/30000 Training Loss: 0.04839066043496132\n",
      "Epoch 22925/30000 Training Loss: 0.0665738508105278\n",
      "Epoch 22926/30000 Training Loss: 0.07213911414146423\n",
      "Epoch 22927/30000 Training Loss: 0.0789593979716301\n",
      "Epoch 22928/30000 Training Loss: 0.06250082701444626\n",
      "Epoch 22929/30000 Training Loss: 0.052671413868665695\n",
      "Epoch 22930/30000 Training Loss: 0.07294443994760513\n",
      "Epoch 22930/30000 Validation Loss: 0.06163156032562256\n",
      "Epoch 22931/30000 Training Loss: 0.05572229251265526\n",
      "Epoch 22932/30000 Training Loss: 0.0858142152428627\n",
      "Epoch 22933/30000 Training Loss: 0.0743316188454628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22934/30000 Training Loss: 0.061596035957336426\n",
      "Epoch 22935/30000 Training Loss: 0.07573626190423965\n",
      "Epoch 22936/30000 Training Loss: 0.06361409276723862\n",
      "Epoch 22937/30000 Training Loss: 0.07195927947759628\n",
      "Epoch 22938/30000 Training Loss: 0.06923162937164307\n",
      "Epoch 22939/30000 Training Loss: 0.06173050403594971\n",
      "Epoch 22940/30000 Training Loss: 0.06394269317388535\n",
      "Epoch 22940/30000 Validation Loss: 0.06765947490930557\n",
      "Epoch 22941/30000 Training Loss: 0.07558762282133102\n",
      "Epoch 22942/30000 Training Loss: 0.07016968727111816\n",
      "Epoch 22943/30000 Training Loss: 0.08826162666082382\n",
      "Epoch 22944/30000 Training Loss: 0.08751425892114639\n",
      "Epoch 22945/30000 Training Loss: 0.08113465458154678\n",
      "Epoch 22946/30000 Training Loss: 0.0665941908955574\n",
      "Epoch 22947/30000 Training Loss: 0.0669291689991951\n",
      "Epoch 22948/30000 Training Loss: 0.06763900071382523\n",
      "Epoch 22949/30000 Training Loss: 0.061228860169649124\n",
      "Epoch 22950/30000 Training Loss: 0.09747926145792007\n",
      "Epoch 22950/30000 Validation Loss: 0.06352434307336807\n",
      "Epoch 22951/30000 Training Loss: 0.07424289733171463\n",
      "Epoch 22952/30000 Training Loss: 0.053081635385751724\n",
      "Epoch 22953/30000 Training Loss: 0.06103302910923958\n",
      "Epoch 22954/30000 Training Loss: 0.07743378728628159\n",
      "Epoch 22955/30000 Training Loss: 0.07968995720148087\n",
      "Epoch 22956/30000 Training Loss: 0.07613556832075119\n",
      "Epoch 22957/30000 Training Loss: 0.068088598549366\n",
      "Epoch 22958/30000 Training Loss: 0.0783279612660408\n",
      "Epoch 22959/30000 Training Loss: 0.07671164721250534\n",
      "Epoch 22960/30000 Training Loss: 0.07876056432723999\n",
      "Epoch 22960/30000 Validation Loss: 0.05295678600668907\n",
      "Epoch 22961/30000 Training Loss: 0.06381196528673172\n",
      "Epoch 22962/30000 Training Loss: 0.07951130717992783\n",
      "Epoch 22963/30000 Training Loss: 0.06075969338417053\n",
      "Epoch 22964/30000 Training Loss: 0.07891106605529785\n",
      "Epoch 22965/30000 Training Loss: 0.06867482513189316\n",
      "Epoch 22966/30000 Training Loss: 0.0717410072684288\n",
      "Epoch 22967/30000 Training Loss: 0.08194277435541153\n",
      "Epoch 22968/30000 Training Loss: 0.07413167506456375\n",
      "Epoch 22969/30000 Training Loss: 0.0819408968091011\n",
      "Epoch 22970/30000 Training Loss: 0.09305637329816818\n",
      "Epoch 22970/30000 Validation Loss: 0.06912118196487427\n",
      "Epoch 22971/30000 Training Loss: 0.08505968004465103\n",
      "Epoch 22972/30000 Training Loss: 0.052217040210962296\n",
      "Epoch 22973/30000 Training Loss: 0.08622395992279053\n",
      "Epoch 22974/30000 Training Loss: 0.07318621128797531\n",
      "Epoch 22975/30000 Training Loss: 0.07523456960916519\n",
      "Epoch 22976/30000 Training Loss: 0.09616287797689438\n",
      "Epoch 22977/30000 Training Loss: 0.09280253201723099\n",
      "Epoch 22978/30000 Training Loss: 0.08386191725730896\n",
      "Epoch 22979/30000 Training Loss: 0.06941917538642883\n",
      "Epoch 22980/30000 Training Loss: 0.06410785764455795\n",
      "Epoch 22980/30000 Validation Loss: 0.07296258956193924\n",
      "Epoch 22981/30000 Training Loss: 0.08397889137268066\n",
      "Epoch 22982/30000 Training Loss: 0.07213864475488663\n",
      "Epoch 22983/30000 Training Loss: 0.05961597338318825\n",
      "Epoch 22984/30000 Training Loss: 0.0659947469830513\n",
      "Epoch 22985/30000 Training Loss: 0.0735408142209053\n",
      "Epoch 22986/30000 Training Loss: 0.060302767902612686\n",
      "Epoch 22987/30000 Training Loss: 0.10488984733819962\n",
      "Epoch 22988/30000 Training Loss: 0.053874671459198\n",
      "Epoch 22989/30000 Training Loss: 0.07883413881063461\n",
      "Epoch 22990/30000 Training Loss: 0.05464978888630867\n",
      "Epoch 22990/30000 Validation Loss: 0.06845308840274811\n",
      "Epoch 22991/30000 Training Loss: 0.08396045118570328\n",
      "Epoch 22992/30000 Training Loss: 0.06462814658880234\n",
      "Epoch 22993/30000 Training Loss: 0.07899070531129837\n",
      "Epoch 22994/30000 Training Loss: 0.07757047563791275\n",
      "Epoch 22995/30000 Training Loss: 0.08236102014780045\n",
      "Epoch 22996/30000 Training Loss: 0.06288798898458481\n",
      "Epoch 22997/30000 Training Loss: 0.0707440972328186\n",
      "Epoch 22998/30000 Training Loss: 0.06063081696629524\n",
      "Epoch 22999/30000 Training Loss: 0.07863364368677139\n",
      "Epoch 23000/30000 Training Loss: 0.08406006544828415\n",
      "Epoch 23000/30000 Validation Loss: 0.06118902564048767\n",
      "Epoch 23001/30000 Training Loss: 0.05804932117462158\n",
      "Epoch 23002/30000 Training Loss: 0.06994331628084183\n",
      "Epoch 23003/30000 Training Loss: 0.06980423629283905\n",
      "Epoch 23004/30000 Training Loss: 0.08267202973365784\n",
      "Epoch 23005/30000 Training Loss: 0.07487186044454575\n",
      "Epoch 23006/30000 Training Loss: 0.06729672104120255\n",
      "Epoch 23007/30000 Training Loss: 0.07820656150579453\n",
      "Epoch 23008/30000 Training Loss: 0.0713469535112381\n",
      "Epoch 23009/30000 Training Loss: 0.06491466611623764\n",
      "Epoch 23010/30000 Training Loss: 0.07389209419488907\n",
      "Epoch 23010/30000 Validation Loss: 0.061495888978242874\n",
      "Epoch 23011/30000 Training Loss: 0.07445918023586273\n",
      "Epoch 23012/30000 Training Loss: 0.06611142307519913\n",
      "Epoch 23013/30000 Training Loss: 0.07854458689689636\n",
      "Epoch 23014/30000 Training Loss: 0.06146365404129028\n",
      "Epoch 23015/30000 Training Loss: 0.07193254679441452\n",
      "Epoch 23016/30000 Training Loss: 0.08151593804359436\n",
      "Epoch 23017/30000 Training Loss: 0.08535785228013992\n",
      "Epoch 23018/30000 Training Loss: 0.06672237813472748\n",
      "Epoch 23019/30000 Training Loss: 0.06968367099761963\n",
      "Epoch 23020/30000 Training Loss: 0.054060980677604675\n",
      "Epoch 23020/30000 Validation Loss: 0.07135311514139175\n",
      "Epoch 23021/30000 Training Loss: 0.058802735060453415\n",
      "Epoch 23022/30000 Training Loss: 0.062126848846673965\n",
      "Epoch 23023/30000 Training Loss: 0.0812983363866806\n",
      "Epoch 23024/30000 Training Loss: 0.06183084473013878\n",
      "Epoch 23025/30000 Training Loss: 0.0713912844657898\n",
      "Epoch 23026/30000 Training Loss: 0.08683512359857559\n",
      "Epoch 23027/30000 Training Loss: 0.06191806495189667\n",
      "Epoch 23028/30000 Training Loss: 0.0728253647685051\n",
      "Epoch 23029/30000 Training Loss: 0.05045650526881218\n",
      "Epoch 23030/30000 Training Loss: 0.07673513144254684\n",
      "Epoch 23030/30000 Validation Loss: 0.06297639012336731\n",
      "Epoch 23031/30000 Training Loss: 0.08227082341909409\n",
      "Epoch 23032/30000 Training Loss: 0.061495259404182434\n",
      "Epoch 23033/30000 Training Loss: 0.055992379784584045\n",
      "Epoch 23034/30000 Training Loss: 0.05981127545237541\n",
      "Epoch 23035/30000 Training Loss: 0.060340460389852524\n",
      "Epoch 23036/30000 Training Loss: 0.06339938938617706\n",
      "Epoch 23037/30000 Training Loss: 0.0803307518362999\n",
      "Epoch 23038/30000 Training Loss: 0.06183195114135742\n",
      "Epoch 23039/30000 Training Loss: 0.06588263064622879\n",
      "Epoch 23040/30000 Training Loss: 0.07805488258600235\n",
      "Epoch 23040/30000 Validation Loss: 0.08467086404561996\n",
      "Epoch 23041/30000 Training Loss: 0.07119324803352356\n",
      "Epoch 23042/30000 Training Loss: 0.06156260892748833\n",
      "Epoch 23043/30000 Training Loss: 0.08380317687988281\n",
      "Epoch 23044/30000 Training Loss: 0.06766361743211746\n",
      "Epoch 23045/30000 Training Loss: 0.06540226191282272\n",
      "Epoch 23046/30000 Training Loss: 0.06517190486192703\n",
      "Epoch 23047/30000 Training Loss: 0.07265710085630417\n",
      "Epoch 23048/30000 Training Loss: 0.07764946669340134\n",
      "Epoch 23049/30000 Training Loss: 0.07821319252252579\n",
      "Epoch 23050/30000 Training Loss: 0.05277814343571663\n",
      "Epoch 23050/30000 Validation Loss: 0.05779200792312622\n",
      "Epoch 23051/30000 Training Loss: 0.08241354674100876\n",
      "Epoch 23052/30000 Training Loss: 0.08663966506719589\n",
      "Epoch 23053/30000 Training Loss: 0.06591510772705078\n",
      "Epoch 23054/30000 Training Loss: 0.06210631504654884\n",
      "Epoch 23055/30000 Training Loss: 0.06610842794179916\n",
      "Epoch 23056/30000 Training Loss: 0.07574788480997086\n",
      "Epoch 23057/30000 Training Loss: 0.05802352353930473\n",
      "Epoch 23058/30000 Training Loss: 0.06180363893508911\n",
      "Epoch 23059/30000 Training Loss: 0.07472360879182816\n",
      "Epoch 23060/30000 Training Loss: 0.07096069306135178\n",
      "Epoch 23060/30000 Validation Loss: 0.07489656656980515\n",
      "Epoch 23061/30000 Training Loss: 0.06255701184272766\n",
      "Epoch 23062/30000 Training Loss: 0.07471873611211777\n",
      "Epoch 23063/30000 Training Loss: 0.058053016662597656\n",
      "Epoch 23064/30000 Training Loss: 0.06770806759595871\n",
      "Epoch 23065/30000 Training Loss: 0.07461386173963547\n",
      "Epoch 23066/30000 Training Loss: 0.07865960150957108\n",
      "Epoch 23067/30000 Training Loss: 0.07552242279052734\n",
      "Epoch 23068/30000 Training Loss: 0.0713626965880394\n",
      "Epoch 23069/30000 Training Loss: 0.06423763185739517\n",
      "Epoch 23070/30000 Training Loss: 0.06815396994352341\n",
      "Epoch 23070/30000 Validation Loss: 0.0841655433177948\n",
      "Epoch 23071/30000 Training Loss: 0.06173129752278328\n",
      "Epoch 23072/30000 Training Loss: 0.06299728900194168\n",
      "Epoch 23073/30000 Training Loss: 0.05986377224326134\n",
      "Epoch 23074/30000 Training Loss: 0.06895072013139725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23075/30000 Training Loss: 0.05828668177127838\n",
      "Epoch 23076/30000 Training Loss: 0.05586157739162445\n",
      "Epoch 23077/30000 Training Loss: 0.0956936851143837\n",
      "Epoch 23078/30000 Training Loss: 0.06601429730653763\n",
      "Epoch 23079/30000 Training Loss: 0.07101885229349136\n",
      "Epoch 23080/30000 Training Loss: 0.08054229617118835\n",
      "Epoch 23080/30000 Validation Loss: 0.08720240741968155\n",
      "Epoch 23081/30000 Training Loss: 0.07097335904836655\n",
      "Epoch 23082/30000 Training Loss: 0.06929797679185867\n",
      "Epoch 23083/30000 Training Loss: 0.0832948163151741\n",
      "Epoch 23084/30000 Training Loss: 0.05584863945841789\n",
      "Epoch 23085/30000 Training Loss: 0.08841127157211304\n",
      "Epoch 23086/30000 Training Loss: 0.06449807435274124\n",
      "Epoch 23087/30000 Training Loss: 0.08186784386634827\n",
      "Epoch 23088/30000 Training Loss: 0.06955117732286453\n",
      "Epoch 23089/30000 Training Loss: 0.08078109472990036\n",
      "Epoch 23090/30000 Training Loss: 0.06563812494277954\n",
      "Epoch 23090/30000 Validation Loss: 0.07509476691484451\n",
      "Epoch 23091/30000 Training Loss: 0.07073742151260376\n",
      "Epoch 23092/30000 Training Loss: 0.06544771790504456\n",
      "Epoch 23093/30000 Training Loss: 0.05801914259791374\n",
      "Epoch 23094/30000 Training Loss: 0.06196489930152893\n",
      "Epoch 23095/30000 Training Loss: 0.07797450572252274\n",
      "Epoch 23096/30000 Training Loss: 0.06260770559310913\n",
      "Epoch 23097/30000 Training Loss: 0.0747680515050888\n",
      "Epoch 23098/30000 Training Loss: 0.05640898272395134\n",
      "Epoch 23099/30000 Training Loss: 0.060394302010536194\n",
      "Epoch 23100/30000 Training Loss: 0.08816620707511902\n",
      "Epoch 23100/30000 Validation Loss: 0.07843295484781265\n",
      "Epoch 23101/30000 Training Loss: 0.06806889176368713\n",
      "Epoch 23102/30000 Training Loss: 0.07463663816452026\n",
      "Epoch 23103/30000 Training Loss: 0.06705773621797562\n",
      "Epoch 23104/30000 Training Loss: 0.06374498456716537\n",
      "Epoch 23105/30000 Training Loss: 0.06770787388086319\n",
      "Epoch 23106/30000 Training Loss: 0.07863777130842209\n",
      "Epoch 23107/30000 Training Loss: 0.06709340214729309\n",
      "Epoch 23108/30000 Training Loss: 0.08258839696645737\n",
      "Epoch 23109/30000 Training Loss: 0.0784088745713234\n",
      "Epoch 23110/30000 Training Loss: 0.07871443778276443\n",
      "Epoch 23110/30000 Validation Loss: 0.08472201973199844\n",
      "Epoch 23111/30000 Training Loss: 0.07406935095787048\n",
      "Epoch 23112/30000 Training Loss: 0.08224931359291077\n",
      "Epoch 23113/30000 Training Loss: 0.06573794037103653\n",
      "Epoch 23114/30000 Training Loss: 0.07430564612150192\n",
      "Epoch 23115/30000 Training Loss: 0.08435400575399399\n",
      "Epoch 23116/30000 Training Loss: 0.05658851936459541\n",
      "Epoch 23117/30000 Training Loss: 0.08197338134050369\n",
      "Epoch 23118/30000 Training Loss: 0.05699147656559944\n",
      "Epoch 23119/30000 Training Loss: 0.05579252168536186\n",
      "Epoch 23120/30000 Training Loss: 0.06551050394773483\n",
      "Epoch 23120/30000 Validation Loss: 0.06813127547502518\n",
      "Epoch 23121/30000 Training Loss: 0.07461532205343246\n",
      "Epoch 23122/30000 Training Loss: 0.07862811535596848\n",
      "Epoch 23123/30000 Training Loss: 0.08523574471473694\n",
      "Epoch 23124/30000 Training Loss: 0.06396671384572983\n",
      "Epoch 23125/30000 Training Loss: 0.0673297867178917\n",
      "Epoch 23126/30000 Training Loss: 0.05921320244669914\n",
      "Epoch 23127/30000 Training Loss: 0.05849034711718559\n",
      "Epoch 23128/30000 Training Loss: 0.0825292095541954\n",
      "Epoch 23129/30000 Training Loss: 0.06185980513691902\n",
      "Epoch 23130/30000 Training Loss: 0.061920445412397385\n",
      "Epoch 23130/30000 Validation Loss: 0.06928441673517227\n",
      "Epoch 23131/30000 Training Loss: 0.0643719956278801\n",
      "Epoch 23132/30000 Training Loss: 0.072507344186306\n",
      "Epoch 23133/30000 Training Loss: 0.0664278119802475\n",
      "Epoch 23134/30000 Training Loss: 0.07457518577575684\n",
      "Epoch 23135/30000 Training Loss: 0.0653090626001358\n",
      "Epoch 23136/30000 Training Loss: 0.06920298933982849\n",
      "Epoch 23137/30000 Training Loss: 0.09022670984268188\n",
      "Epoch 23138/30000 Training Loss: 0.06192244216799736\n",
      "Epoch 23139/30000 Training Loss: 0.06911161541938782\n",
      "Epoch 23140/30000 Training Loss: 0.06261928379535675\n",
      "Epoch 23140/30000 Validation Loss: 0.07825929671525955\n",
      "Epoch 23141/30000 Training Loss: 0.06504640728235245\n",
      "Epoch 23142/30000 Training Loss: 0.06495298445224762\n",
      "Epoch 23143/30000 Training Loss: 0.062088459730148315\n",
      "Epoch 23144/30000 Training Loss: 0.06718799471855164\n",
      "Epoch 23145/30000 Training Loss: 0.0758344754576683\n",
      "Epoch 23146/30000 Training Loss: 0.06446301937103271\n",
      "Epoch 23147/30000 Training Loss: 0.07602501660585403\n",
      "Epoch 23148/30000 Training Loss: 0.06555354595184326\n",
      "Epoch 23149/30000 Training Loss: 0.0766913965344429\n",
      "Epoch 23150/30000 Training Loss: 0.0692509338259697\n",
      "Epoch 23150/30000 Validation Loss: 0.05857622250914574\n",
      "Epoch 23151/30000 Training Loss: 0.07367848604917526\n",
      "Epoch 23152/30000 Training Loss: 0.07005193084478378\n",
      "Epoch 23153/30000 Training Loss: 0.0697331354022026\n",
      "Epoch 23154/30000 Training Loss: 0.06448712944984436\n",
      "Epoch 23155/30000 Training Loss: 0.05016930028796196\n",
      "Epoch 23156/30000 Training Loss: 0.06606355309486389\n",
      "Epoch 23157/30000 Training Loss: 0.07661819458007812\n",
      "Epoch 23158/30000 Training Loss: 0.0684690773487091\n",
      "Epoch 23159/30000 Training Loss: 0.06260352581739426\n",
      "Epoch 23160/30000 Training Loss: 0.07997622340917587\n",
      "Epoch 23160/30000 Validation Loss: 0.09671562165021896\n",
      "Epoch 23161/30000 Training Loss: 0.053214818239212036\n",
      "Epoch 23162/30000 Training Loss: 0.08667058497667313\n",
      "Epoch 23163/30000 Training Loss: 0.06629882007837296\n",
      "Epoch 23164/30000 Training Loss: 0.06437772512435913\n",
      "Epoch 23165/30000 Training Loss: 0.0700308158993721\n",
      "Epoch 23166/30000 Training Loss: 0.0672629252076149\n",
      "Epoch 23167/30000 Training Loss: 0.07586172968149185\n",
      "Epoch 23168/30000 Training Loss: 0.06906947493553162\n",
      "Epoch 23169/30000 Training Loss: 0.08718780428171158\n",
      "Epoch 23170/30000 Training Loss: 0.07481948286294937\n",
      "Epoch 23170/30000 Validation Loss: 0.07540517300367355\n",
      "Epoch 23171/30000 Training Loss: 0.0681244283914566\n",
      "Epoch 23172/30000 Training Loss: 0.05789366364479065\n",
      "Epoch 23173/30000 Training Loss: 0.06734036654233932\n",
      "Epoch 23174/30000 Training Loss: 0.07779711484909058\n",
      "Epoch 23175/30000 Training Loss: 0.06814085692167282\n",
      "Epoch 23176/30000 Training Loss: 0.08549455553293228\n",
      "Epoch 23177/30000 Training Loss: 0.08365098387002945\n",
      "Epoch 23178/30000 Training Loss: 0.06349518895149231\n",
      "Epoch 23179/30000 Training Loss: 0.062197256833314896\n",
      "Epoch 23180/30000 Training Loss: 0.060459911823272705\n",
      "Epoch 23180/30000 Validation Loss: 0.07341659069061279\n",
      "Epoch 23181/30000 Training Loss: 0.0848011001944542\n",
      "Epoch 23182/30000 Training Loss: 0.10130252689123154\n",
      "Epoch 23183/30000 Training Loss: 0.06282379478216171\n",
      "Epoch 23184/30000 Training Loss: 0.06675931811332703\n",
      "Epoch 23185/30000 Training Loss: 0.08697814494371414\n",
      "Epoch 23186/30000 Training Loss: 0.07400447875261307\n",
      "Epoch 23187/30000 Training Loss: 0.06088325008749962\n",
      "Epoch 23188/30000 Training Loss: 0.059094201773405075\n",
      "Epoch 23189/30000 Training Loss: 0.06592479348182678\n",
      "Epoch 23190/30000 Training Loss: 0.057965587824583054\n",
      "Epoch 23190/30000 Validation Loss: 0.06708309054374695\n",
      "Epoch 23191/30000 Training Loss: 0.06575866788625717\n",
      "Epoch 23192/30000 Training Loss: 0.051361650228500366\n",
      "Epoch 23193/30000 Training Loss: 0.0645795539021492\n",
      "Epoch 23194/30000 Training Loss: 0.06375202536582947\n",
      "Epoch 23195/30000 Training Loss: 0.05416110157966614\n",
      "Epoch 23196/30000 Training Loss: 0.10100718587636948\n",
      "Epoch 23197/30000 Training Loss: 0.07343398779630661\n",
      "Epoch 23198/30000 Training Loss: 0.0775248259305954\n",
      "Epoch 23199/30000 Training Loss: 0.0720762312412262\n",
      "Epoch 23200/30000 Training Loss: 0.08094406127929688\n",
      "Epoch 23200/30000 Validation Loss: 0.06923303753137589\n",
      "Epoch 23201/30000 Training Loss: 0.07263486832380295\n",
      "Epoch 23202/30000 Training Loss: 0.09207359701395035\n",
      "Epoch 23203/30000 Training Loss: 0.07014480233192444\n",
      "Epoch 23204/30000 Training Loss: 0.07711166888475418\n",
      "Epoch 23205/30000 Training Loss: 0.06703582406044006\n",
      "Epoch 23206/30000 Training Loss: 0.07479029148817062\n",
      "Epoch 23207/30000 Training Loss: 0.06432043761014938\n",
      "Epoch 23208/30000 Training Loss: 0.07202503830194473\n",
      "Epoch 23209/30000 Training Loss: 0.07708307355642319\n",
      "Epoch 23210/30000 Training Loss: 0.06764750927686691\n",
      "Epoch 23210/30000 Validation Loss: 0.055843252688646317\n",
      "Epoch 23211/30000 Training Loss: 0.07295794039964676\n",
      "Epoch 23212/30000 Training Loss: 0.06901340931653976\n",
      "Epoch 23213/30000 Training Loss: 0.06799246370792389\n",
      "Epoch 23214/30000 Training Loss: 0.08660119771957397\n",
      "Epoch 23215/30000 Training Loss: 0.09155526012182236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23216/30000 Training Loss: 0.07125713676214218\n",
      "Epoch 23217/30000 Training Loss: 0.09133341163396835\n",
      "Epoch 23218/30000 Training Loss: 0.06913570314645767\n",
      "Epoch 23219/30000 Training Loss: 0.05264728143811226\n",
      "Epoch 23220/30000 Training Loss: 0.07052091509103775\n",
      "Epoch 23220/30000 Validation Loss: 0.05928465351462364\n",
      "Epoch 23221/30000 Training Loss: 0.06830514222383499\n",
      "Epoch 23222/30000 Training Loss: 0.0670679435133934\n",
      "Epoch 23223/30000 Training Loss: 0.08811374753713608\n",
      "Epoch 23224/30000 Training Loss: 0.06286446005105972\n",
      "Epoch 23225/30000 Training Loss: 0.06820181012153625\n",
      "Epoch 23226/30000 Training Loss: 0.07304482907056808\n",
      "Epoch 23227/30000 Training Loss: 0.06106824800372124\n",
      "Epoch 23228/30000 Training Loss: 0.08030559867620468\n",
      "Epoch 23229/30000 Training Loss: 0.07058468461036682\n",
      "Epoch 23230/30000 Training Loss: 0.07044512778520584\n",
      "Epoch 23230/30000 Validation Loss: 0.06024752929806709\n",
      "Epoch 23231/30000 Training Loss: 0.0705469474196434\n",
      "Epoch 23232/30000 Training Loss: 0.07254036515951157\n",
      "Epoch 23233/30000 Training Loss: 0.07209423929452896\n",
      "Epoch 23234/30000 Training Loss: 0.06276527792215347\n",
      "Epoch 23235/30000 Training Loss: 0.07792580872774124\n",
      "Epoch 23236/30000 Training Loss: 0.06113702058792114\n",
      "Epoch 23237/30000 Training Loss: 0.06929071247577667\n",
      "Epoch 23238/30000 Training Loss: 0.07353577762842178\n",
      "Epoch 23239/30000 Training Loss: 0.057320158928632736\n",
      "Epoch 23240/30000 Training Loss: 0.06834200769662857\n",
      "Epoch 23240/30000 Validation Loss: 0.08741988986730576\n",
      "Epoch 23241/30000 Training Loss: 0.06035616993904114\n",
      "Epoch 23242/30000 Training Loss: 0.08068373799324036\n",
      "Epoch 23243/30000 Training Loss: 0.06055380403995514\n",
      "Epoch 23244/30000 Training Loss: 0.09440898150205612\n",
      "Epoch 23245/30000 Training Loss: 0.07743851840496063\n",
      "Epoch 23246/30000 Training Loss: 0.08040156960487366\n",
      "Epoch 23247/30000 Training Loss: 0.06480563431978226\n",
      "Epoch 23248/30000 Training Loss: 0.07362896203994751\n",
      "Epoch 23249/30000 Training Loss: 0.07316266745328903\n",
      "Epoch 23250/30000 Training Loss: 0.08093270659446716\n",
      "Epoch 23250/30000 Validation Loss: 0.06664452701807022\n",
      "Epoch 23251/30000 Training Loss: 0.07936757057905197\n",
      "Epoch 23252/30000 Training Loss: 0.055226534605026245\n",
      "Epoch 23253/30000 Training Loss: 0.08023501187562943\n",
      "Epoch 23254/30000 Training Loss: 0.06514870375394821\n",
      "Epoch 23255/30000 Training Loss: 0.07356131821870804\n",
      "Epoch 23256/30000 Training Loss: 0.06331980973482132\n",
      "Epoch 23257/30000 Training Loss: 0.07269585877656937\n",
      "Epoch 23258/30000 Training Loss: 0.07425499707460403\n",
      "Epoch 23259/30000 Training Loss: 0.07794561237096786\n",
      "Epoch 23260/30000 Training Loss: 0.06912948191165924\n",
      "Epoch 23260/30000 Validation Loss: 0.06650102883577347\n",
      "Epoch 23261/30000 Training Loss: 0.06196314096450806\n",
      "Epoch 23262/30000 Training Loss: 0.062433868646621704\n",
      "Epoch 23263/30000 Training Loss: 0.08477010577917099\n",
      "Epoch 23264/30000 Training Loss: 0.062065888196229935\n",
      "Epoch 23265/30000 Training Loss: 0.0719384178519249\n",
      "Epoch 23266/30000 Training Loss: 0.07297233492136002\n",
      "Epoch 23267/30000 Training Loss: 0.056998495012521744\n",
      "Epoch 23268/30000 Training Loss: 0.0714293047785759\n",
      "Epoch 23269/30000 Training Loss: 0.058006733655929565\n",
      "Epoch 23270/30000 Training Loss: 0.06694065779447556\n",
      "Epoch 23270/30000 Validation Loss: 0.06400371342897415\n",
      "Epoch 23271/30000 Training Loss: 0.08605685085058212\n",
      "Epoch 23272/30000 Training Loss: 0.09037405252456665\n",
      "Epoch 23273/30000 Training Loss: 0.07084325700998306\n",
      "Epoch 23274/30000 Training Loss: 0.06641364842653275\n",
      "Epoch 23275/30000 Training Loss: 0.09337303787469864\n",
      "Epoch 23276/30000 Training Loss: 0.054715316742658615\n",
      "Epoch 23277/30000 Training Loss: 0.06460670381784439\n",
      "Epoch 23278/30000 Training Loss: 0.0661940947175026\n",
      "Epoch 23279/30000 Training Loss: 0.07191585749387741\n",
      "Epoch 23280/30000 Training Loss: 0.06484123319387436\n",
      "Epoch 23280/30000 Validation Loss: 0.08258646726608276\n",
      "Epoch 23281/30000 Training Loss: 0.07294469326734543\n",
      "Epoch 23282/30000 Training Loss: 0.046956609934568405\n",
      "Epoch 23283/30000 Training Loss: 0.08438286185264587\n",
      "Epoch 23284/30000 Training Loss: 0.05145980045199394\n",
      "Epoch 23285/30000 Training Loss: 0.0670805349946022\n",
      "Epoch 23286/30000 Training Loss: 0.07721084356307983\n",
      "Epoch 23287/30000 Training Loss: 0.06398747116327286\n",
      "Epoch 23288/30000 Training Loss: 0.06590940058231354\n",
      "Epoch 23289/30000 Training Loss: 0.055375147610902786\n",
      "Epoch 23290/30000 Training Loss: 0.07739753276109695\n",
      "Epoch 23290/30000 Validation Loss: 0.08557974547147751\n",
      "Epoch 23291/30000 Training Loss: 0.07700275629758835\n",
      "Epoch 23292/30000 Training Loss: 0.07669821381568909\n",
      "Epoch 23293/30000 Training Loss: 0.06809661537408829\n",
      "Epoch 23294/30000 Training Loss: 0.07092306017875671\n",
      "Epoch 23295/30000 Training Loss: 0.07774989306926727\n",
      "Epoch 23296/30000 Training Loss: 0.061638493090867996\n",
      "Epoch 23297/30000 Training Loss: 0.058556001633405685\n",
      "Epoch 23298/30000 Training Loss: 0.05698223412036896\n",
      "Epoch 23299/30000 Training Loss: 0.07368241995573044\n",
      "Epoch 23300/30000 Training Loss: 0.07722566276788712\n",
      "Epoch 23300/30000 Validation Loss: 0.07596839219331741\n",
      "Epoch 23301/30000 Training Loss: 0.08083855360746384\n",
      "Epoch 23302/30000 Training Loss: 0.06782557815313339\n",
      "Epoch 23303/30000 Training Loss: 0.07112719118595123\n",
      "Epoch 23304/30000 Training Loss: 0.07548592239618301\n",
      "Epoch 23305/30000 Training Loss: 0.07393413037061691\n",
      "Epoch 23306/30000 Training Loss: 0.10022499412298203\n",
      "Epoch 23307/30000 Training Loss: 0.07407161593437195\n",
      "Epoch 23308/30000 Training Loss: 0.052750904113054276\n",
      "Epoch 23309/30000 Training Loss: 0.06421244144439697\n",
      "Epoch 23310/30000 Training Loss: 0.07770150154829025\n",
      "Epoch 23310/30000 Validation Loss: 0.06779709458351135\n",
      "Epoch 23311/30000 Training Loss: 0.06080125272274017\n",
      "Epoch 23312/30000 Training Loss: 0.07254749536514282\n",
      "Epoch 23313/30000 Training Loss: 0.07579324394464493\n",
      "Epoch 23314/30000 Training Loss: 0.07046238332986832\n",
      "Epoch 23315/30000 Training Loss: 0.07697377353906631\n",
      "Epoch 23316/30000 Training Loss: 0.08214481920003891\n",
      "Epoch 23317/30000 Training Loss: 0.07585141807794571\n",
      "Epoch 23318/30000 Training Loss: 0.05629860237240791\n",
      "Epoch 23319/30000 Training Loss: 0.0637987032532692\n",
      "Epoch 23320/30000 Training Loss: 0.07076499611139297\n",
      "Epoch 23320/30000 Validation Loss: 0.07840176671743393\n",
      "Epoch 23321/30000 Training Loss: 0.06806022673845291\n",
      "Epoch 23322/30000 Training Loss: 0.07394681125879288\n",
      "Epoch 23323/30000 Training Loss: 0.07502540946006775\n",
      "Epoch 23324/30000 Training Loss: 0.08067479729652405\n",
      "Epoch 23325/30000 Training Loss: 0.06818368285894394\n",
      "Epoch 23326/30000 Training Loss: 0.0662539079785347\n",
      "Epoch 23327/30000 Training Loss: 0.05720595642924309\n",
      "Epoch 23328/30000 Training Loss: 0.07622983306646347\n",
      "Epoch 23329/30000 Training Loss: 0.07296795397996902\n",
      "Epoch 23330/30000 Training Loss: 0.09139484167098999\n",
      "Epoch 23330/30000 Validation Loss: 0.05211948975920677\n",
      "Epoch 23331/30000 Training Loss: 0.0734393373131752\n",
      "Epoch 23332/30000 Training Loss: 0.07014298439025879\n",
      "Epoch 23333/30000 Training Loss: 0.10242176055908203\n",
      "Epoch 23334/30000 Training Loss: 0.062276020646095276\n",
      "Epoch 23335/30000 Training Loss: 0.06584881991147995\n",
      "Epoch 23336/30000 Training Loss: 0.07783759385347366\n",
      "Epoch 23337/30000 Training Loss: 0.07715398818254471\n",
      "Epoch 23338/30000 Training Loss: 0.06968062371015549\n",
      "Epoch 23339/30000 Training Loss: 0.06385574489831924\n",
      "Epoch 23340/30000 Training Loss: 0.07178474217653275\n",
      "Epoch 23340/30000 Validation Loss: 0.07272806763648987\n",
      "Epoch 23341/30000 Training Loss: 0.06892801076173782\n",
      "Epoch 23342/30000 Training Loss: 0.056671466678380966\n",
      "Epoch 23343/30000 Training Loss: 0.08012374490499496\n",
      "Epoch 23344/30000 Training Loss: 0.09248407930135727\n",
      "Epoch 23345/30000 Training Loss: 0.0814998522400856\n",
      "Epoch 23346/30000 Training Loss: 0.06289006024599075\n",
      "Epoch 23347/30000 Training Loss: 0.06549358367919922\n",
      "Epoch 23348/30000 Training Loss: 0.06608766317367554\n",
      "Epoch 23349/30000 Training Loss: 0.06682973355054855\n",
      "Epoch 23350/30000 Training Loss: 0.0638248547911644\n",
      "Epoch 23350/30000 Validation Loss: 0.06990041583776474\n",
      "Epoch 23351/30000 Training Loss: 0.06436596065759659\n",
      "Epoch 23352/30000 Training Loss: 0.0628889799118042\n",
      "Epoch 23353/30000 Training Loss: 0.07489341497421265\n",
      "Epoch 23354/30000 Training Loss: 0.05837439373135567\n",
      "Epoch 23355/30000 Training Loss: 0.07761743664741516\n",
      "Epoch 23356/30000 Training Loss: 0.09904438257217407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23357/30000 Training Loss: 0.07837606221437454\n",
      "Epoch 23358/30000 Training Loss: 0.08818075805902481\n",
      "Epoch 23359/30000 Training Loss: 0.0811450183391571\n",
      "Epoch 23360/30000 Training Loss: 0.0695829913020134\n",
      "Epoch 23360/30000 Validation Loss: 0.08328171819448471\n",
      "Epoch 23361/30000 Training Loss: 0.07747097313404083\n",
      "Epoch 23362/30000 Training Loss: 0.06199353560805321\n",
      "Epoch 23363/30000 Training Loss: 0.06489933282136917\n",
      "Epoch 23364/30000 Training Loss: 0.09411483258008957\n",
      "Epoch 23365/30000 Training Loss: 0.07439882308244705\n",
      "Epoch 23366/30000 Training Loss: 0.06586755067110062\n",
      "Epoch 23367/30000 Training Loss: 0.07562307268381119\n",
      "Epoch 23368/30000 Training Loss: 0.071005679666996\n",
      "Epoch 23369/30000 Training Loss: 0.061941519379615784\n",
      "Epoch 23370/30000 Training Loss: 0.06626728922128677\n",
      "Epoch 23370/30000 Validation Loss: 0.08315053582191467\n",
      "Epoch 23371/30000 Training Loss: 0.07581951469182968\n",
      "Epoch 23372/30000 Training Loss: 0.052476704120635986\n",
      "Epoch 23373/30000 Training Loss: 0.07036878913640976\n",
      "Epoch 23374/30000 Training Loss: 0.07324910908937454\n",
      "Epoch 23375/30000 Training Loss: 0.06899795681238174\n",
      "Epoch 23376/30000 Training Loss: 0.06443311274051666\n",
      "Epoch 23377/30000 Training Loss: 0.0781245157122612\n",
      "Epoch 23378/30000 Training Loss: 0.07511203736066818\n",
      "Epoch 23379/30000 Training Loss: 0.0705510824918747\n",
      "Epoch 23380/30000 Training Loss: 0.07765039801597595\n",
      "Epoch 23380/30000 Validation Loss: 0.07014823704957962\n",
      "Epoch 23381/30000 Training Loss: 0.06581506878137589\n",
      "Epoch 23382/30000 Training Loss: 0.07239631563425064\n",
      "Epoch 23383/30000 Training Loss: 0.06844117492437363\n",
      "Epoch 23384/30000 Training Loss: 0.06771315634250641\n",
      "Epoch 23385/30000 Training Loss: 0.08542179316282272\n",
      "Epoch 23386/30000 Training Loss: 0.06677398830652237\n",
      "Epoch 23387/30000 Training Loss: 0.09373738616704941\n",
      "Epoch 23388/30000 Training Loss: 0.07371291518211365\n",
      "Epoch 23389/30000 Training Loss: 0.06487032771110535\n",
      "Epoch 23390/30000 Training Loss: 0.0697641596198082\n",
      "Epoch 23390/30000 Validation Loss: 0.05309265851974487\n",
      "Epoch 23391/30000 Training Loss: 0.07048671692609787\n",
      "Epoch 23392/30000 Training Loss: 0.08809221535921097\n",
      "Epoch 23393/30000 Training Loss: 0.06228562071919441\n",
      "Epoch 23394/30000 Training Loss: 0.06321137398481369\n",
      "Epoch 23395/30000 Training Loss: 0.08913424611091614\n",
      "Epoch 23396/30000 Training Loss: 0.07445666193962097\n",
      "Epoch 23397/30000 Training Loss: 0.08095300197601318\n",
      "Epoch 23398/30000 Training Loss: 0.0710294172167778\n",
      "Epoch 23399/30000 Training Loss: 0.08087379485368729\n",
      "Epoch 23400/30000 Training Loss: 0.08153034001588821\n",
      "Epoch 23400/30000 Validation Loss: 0.0669475719332695\n",
      "Epoch 23401/30000 Training Loss: 0.08107227087020874\n",
      "Epoch 23402/30000 Training Loss: 0.07340986281633377\n",
      "Epoch 23403/30000 Training Loss: 0.08637833595275879\n",
      "Epoch 23404/30000 Training Loss: 0.0626995638012886\n",
      "Epoch 23405/30000 Training Loss: 0.0681702271103859\n",
      "Epoch 23406/30000 Training Loss: 0.07388841360807419\n",
      "Epoch 23407/30000 Training Loss: 0.06276009231805801\n",
      "Epoch 23408/30000 Training Loss: 0.06882249563932419\n",
      "Epoch 23409/30000 Training Loss: 0.06962024420499802\n",
      "Epoch 23410/30000 Training Loss: 0.07238638401031494\n",
      "Epoch 23410/30000 Validation Loss: 0.0742863118648529\n",
      "Epoch 23411/30000 Training Loss: 0.07848146557807922\n",
      "Epoch 23412/30000 Training Loss: 0.06678193062543869\n",
      "Epoch 23413/30000 Training Loss: 0.07266160100698471\n",
      "Epoch 23414/30000 Training Loss: 0.07492369413375854\n",
      "Epoch 23415/30000 Training Loss: 0.06304407119750977\n",
      "Epoch 23416/30000 Training Loss: 0.06869019567966461\n",
      "Epoch 23417/30000 Training Loss: 0.05241512134671211\n",
      "Epoch 23418/30000 Training Loss: 0.055985625833272934\n",
      "Epoch 23419/30000 Training Loss: 0.07480273395776749\n",
      "Epoch 23420/30000 Training Loss: 0.064612977206707\n",
      "Epoch 23420/30000 Validation Loss: 0.05420351028442383\n",
      "Epoch 23421/30000 Training Loss: 0.07383277267217636\n",
      "Epoch 23422/30000 Training Loss: 0.06216754391789436\n",
      "Epoch 23423/30000 Training Loss: 0.08761953562498093\n",
      "Epoch 23424/30000 Training Loss: 0.06341449171304703\n",
      "Epoch 23425/30000 Training Loss: 0.07776274532079697\n",
      "Epoch 23426/30000 Training Loss: 0.04876841977238655\n",
      "Epoch 23427/30000 Training Loss: 0.07840912789106369\n",
      "Epoch 23428/30000 Training Loss: 0.07043573260307312\n",
      "Epoch 23429/30000 Training Loss: 0.0700732097029686\n",
      "Epoch 23430/30000 Training Loss: 0.08173349499702454\n",
      "Epoch 23430/30000 Validation Loss: 0.062388405203819275\n",
      "Epoch 23431/30000 Training Loss: 0.06043674424290657\n",
      "Epoch 23432/30000 Training Loss: 0.06304352730512619\n",
      "Epoch 23433/30000 Training Loss: 0.0852535143494606\n",
      "Epoch 23434/30000 Training Loss: 0.07242757081985474\n",
      "Epoch 23435/30000 Training Loss: 0.05903086066246033\n",
      "Epoch 23436/30000 Training Loss: 0.08953186124563217\n",
      "Epoch 23437/30000 Training Loss: 0.08788137882947922\n",
      "Epoch 23438/30000 Training Loss: 0.07369952648878098\n",
      "Epoch 23439/30000 Training Loss: 0.07517410069704056\n",
      "Epoch 23440/30000 Training Loss: 0.05921000614762306\n",
      "Epoch 23440/30000 Validation Loss: 0.08007016777992249\n",
      "Epoch 23441/30000 Training Loss: 0.06809926778078079\n",
      "Epoch 23442/30000 Training Loss: 0.07737232744693756\n",
      "Epoch 23443/30000 Training Loss: 0.07689856737852097\n",
      "Epoch 23444/30000 Training Loss: 0.06562865525484085\n",
      "Epoch 23445/30000 Training Loss: 0.07388060539960861\n",
      "Epoch 23446/30000 Training Loss: 0.08502297848463058\n",
      "Epoch 23447/30000 Training Loss: 0.07498981058597565\n",
      "Epoch 23448/30000 Training Loss: 0.057633478194475174\n",
      "Epoch 23449/30000 Training Loss: 0.08088365197181702\n",
      "Epoch 23450/30000 Training Loss: 0.07071086764335632\n",
      "Epoch 23450/30000 Validation Loss: 0.06393712013959885\n",
      "Epoch 23451/30000 Training Loss: 0.06210000813007355\n",
      "Epoch 23452/30000 Training Loss: 0.07769259810447693\n",
      "Epoch 23453/30000 Training Loss: 0.08163636922836304\n",
      "Epoch 23454/30000 Training Loss: 0.06638552993535995\n",
      "Epoch 23455/30000 Training Loss: 0.09492429345846176\n",
      "Epoch 23456/30000 Training Loss: 0.0617157518863678\n",
      "Epoch 23457/30000 Training Loss: 0.05786902830004692\n",
      "Epoch 23458/30000 Training Loss: 0.06689100712537766\n",
      "Epoch 23459/30000 Training Loss: 0.09072494506835938\n",
      "Epoch 23460/30000 Training Loss: 0.0766969546675682\n",
      "Epoch 23460/30000 Validation Loss: 0.0717674195766449\n",
      "Epoch 23461/30000 Training Loss: 0.07637014985084534\n",
      "Epoch 23462/30000 Training Loss: 0.08201562613248825\n",
      "Epoch 23463/30000 Training Loss: 0.06396108865737915\n",
      "Epoch 23464/30000 Training Loss: 0.06142732873558998\n",
      "Epoch 23465/30000 Training Loss: 0.07274345308542252\n",
      "Epoch 23466/30000 Training Loss: 0.09008327126502991\n",
      "Epoch 23467/30000 Training Loss: 0.06023447588086128\n",
      "Epoch 23468/30000 Training Loss: 0.07615918666124344\n",
      "Epoch 23469/30000 Training Loss: 0.07170504331588745\n",
      "Epoch 23470/30000 Training Loss: 0.07061535865068436\n",
      "Epoch 23470/30000 Validation Loss: 0.07822535187005997\n",
      "Epoch 23471/30000 Training Loss: 0.060540616512298584\n",
      "Epoch 23472/30000 Training Loss: 0.05838088318705559\n",
      "Epoch 23473/30000 Training Loss: 0.07394383102655411\n",
      "Epoch 23474/30000 Training Loss: 0.05726594105362892\n",
      "Epoch 23475/30000 Training Loss: 0.07959497720003128\n",
      "Epoch 23476/30000 Training Loss: 0.06696446239948273\n",
      "Epoch 23477/30000 Training Loss: 0.07580801844596863\n",
      "Epoch 23478/30000 Training Loss: 0.08618703484535217\n",
      "Epoch 23479/30000 Training Loss: 0.06811873614788055\n",
      "Epoch 23480/30000 Training Loss: 0.07376987487077713\n",
      "Epoch 23480/30000 Validation Loss: 0.07512283325195312\n",
      "Epoch 23481/30000 Training Loss: 0.05809663236141205\n",
      "Epoch 23482/30000 Training Loss: 0.07311221957206726\n",
      "Epoch 23483/30000 Training Loss: 0.06985796988010406\n",
      "Epoch 23484/30000 Training Loss: 0.07206732779741287\n",
      "Epoch 23485/30000 Training Loss: 0.0733751729130745\n",
      "Epoch 23486/30000 Training Loss: 0.0682934895157814\n",
      "Epoch 23487/30000 Training Loss: 0.08801300078630447\n",
      "Epoch 23488/30000 Training Loss: 0.06522241979837418\n",
      "Epoch 23489/30000 Training Loss: 0.05247032642364502\n",
      "Epoch 23490/30000 Training Loss: 0.10031113773584366\n",
      "Epoch 23490/30000 Validation Loss: 0.07047855108976364\n",
      "Epoch 23491/30000 Training Loss: 0.05968348681926727\n",
      "Epoch 23492/30000 Training Loss: 0.06915359199047089\n",
      "Epoch 23493/30000 Training Loss: 0.09958985447883606\n",
      "Epoch 23494/30000 Training Loss: 0.0786752924323082\n",
      "Epoch 23495/30000 Training Loss: 0.0746150016784668\n",
      "Epoch 23496/30000 Training Loss: 0.06609556823968887\n",
      "Epoch 23497/30000 Training Loss: 0.08174105733633041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23498/30000 Training Loss: 0.07375553250312805\n",
      "Epoch 23499/30000 Training Loss: 0.06827142834663391\n",
      "Epoch 23500/30000 Training Loss: 0.06735268980264664\n",
      "Epoch 23500/30000 Validation Loss: 0.08940262347459793\n",
      "Epoch 23501/30000 Training Loss: 0.0734088346362114\n",
      "Epoch 23502/30000 Training Loss: 0.095810167491436\n",
      "Epoch 23503/30000 Training Loss: 0.07655998319387436\n",
      "Epoch 23504/30000 Training Loss: 0.06699645519256592\n",
      "Epoch 23505/30000 Training Loss: 0.06915254145860672\n",
      "Epoch 23506/30000 Training Loss: 0.06593360751867294\n",
      "Epoch 23507/30000 Training Loss: 0.10039489716291428\n",
      "Epoch 23508/30000 Training Loss: 0.06650742143392563\n",
      "Epoch 23509/30000 Training Loss: 0.06800762563943863\n",
      "Epoch 23510/30000 Training Loss: 0.06944651901721954\n",
      "Epoch 23510/30000 Validation Loss: 0.06239217519760132\n",
      "Epoch 23511/30000 Training Loss: 0.05414998531341553\n",
      "Epoch 23512/30000 Training Loss: 0.052654217928647995\n",
      "Epoch 23513/30000 Training Loss: 0.058278638869524\n",
      "Epoch 23514/30000 Training Loss: 0.07030854374170303\n",
      "Epoch 23515/30000 Training Loss: 0.08112332969903946\n",
      "Epoch 23516/30000 Training Loss: 0.06542856246232986\n",
      "Epoch 23517/30000 Training Loss: 0.07176505029201508\n",
      "Epoch 23518/30000 Training Loss: 0.05916571989655495\n",
      "Epoch 23519/30000 Training Loss: 0.06146126613020897\n",
      "Epoch 23520/30000 Training Loss: 0.08260468393564224\n",
      "Epoch 23520/30000 Validation Loss: 0.06892284005880356\n",
      "Epoch 23521/30000 Training Loss: 0.06190504506230354\n",
      "Epoch 23522/30000 Training Loss: 0.06963557749986649\n",
      "Epoch 23523/30000 Training Loss: 0.07687127590179443\n",
      "Epoch 23524/30000 Training Loss: 0.06526520848274231\n",
      "Epoch 23525/30000 Training Loss: 0.07307371497154236\n",
      "Epoch 23526/30000 Training Loss: 0.06980288028717041\n",
      "Epoch 23527/30000 Training Loss: 0.07406457513570786\n",
      "Epoch 23528/30000 Training Loss: 0.05646364763379097\n",
      "Epoch 23529/30000 Training Loss: 0.07682391256093979\n",
      "Epoch 23530/30000 Training Loss: 0.0655394196510315\n",
      "Epoch 23530/30000 Validation Loss: 0.06731661409139633\n",
      "Epoch 23531/30000 Training Loss: 0.07018271833658218\n",
      "Epoch 23532/30000 Training Loss: 0.09159386157989502\n",
      "Epoch 23533/30000 Training Loss: 0.07965027540922165\n",
      "Epoch 23534/30000 Training Loss: 0.0824800506234169\n",
      "Epoch 23535/30000 Training Loss: 0.07673752307891846\n",
      "Epoch 23536/30000 Training Loss: 0.08676017075777054\n",
      "Epoch 23537/30000 Training Loss: 0.06321364641189575\n",
      "Epoch 23538/30000 Training Loss: 0.06283670663833618\n",
      "Epoch 23539/30000 Training Loss: 0.0586036741733551\n",
      "Epoch 23540/30000 Training Loss: 0.06817730516195297\n",
      "Epoch 23540/30000 Validation Loss: 0.08952254801988602\n",
      "Epoch 23541/30000 Training Loss: 0.07065247744321823\n",
      "Epoch 23542/30000 Training Loss: 0.06883066892623901\n",
      "Epoch 23543/30000 Training Loss: 0.06438373774290085\n",
      "Epoch 23544/30000 Training Loss: 0.08993726223707199\n",
      "Epoch 23545/30000 Training Loss: 0.07793789356946945\n",
      "Epoch 23546/30000 Training Loss: 0.06009264662861824\n",
      "Epoch 23547/30000 Training Loss: 0.06478757411241531\n",
      "Epoch 23548/30000 Training Loss: 0.0679074227809906\n",
      "Epoch 23549/30000 Training Loss: 0.05450648441910744\n",
      "Epoch 23550/30000 Training Loss: 0.05991272255778313\n",
      "Epoch 23550/30000 Validation Loss: 0.06867466121912003\n",
      "Epoch 23551/30000 Training Loss: 0.05387299135327339\n",
      "Epoch 23552/30000 Training Loss: 0.08090905845165253\n",
      "Epoch 23553/30000 Training Loss: 0.07395396381616592\n",
      "Epoch 23554/30000 Training Loss: 0.06329089403152466\n",
      "Epoch 23555/30000 Training Loss: 0.08538960665464401\n",
      "Epoch 23556/30000 Training Loss: 0.09034312516450882\n",
      "Epoch 23557/30000 Training Loss: 0.058684419840574265\n",
      "Epoch 23558/30000 Training Loss: 0.07793646305799484\n",
      "Epoch 23559/30000 Training Loss: 0.057946812361478806\n",
      "Epoch 23560/30000 Training Loss: 0.07284050434827805\n",
      "Epoch 23560/30000 Validation Loss: 0.07761845737695694\n",
      "Epoch 23561/30000 Training Loss: 0.07046645879745483\n",
      "Epoch 23562/30000 Training Loss: 0.0773276537656784\n",
      "Epoch 23563/30000 Training Loss: 0.07238224148750305\n",
      "Epoch 23564/30000 Training Loss: 0.09481393545866013\n",
      "Epoch 23565/30000 Training Loss: 0.0676671713590622\n",
      "Epoch 23566/30000 Training Loss: 0.06244386360049248\n",
      "Epoch 23567/30000 Training Loss: 0.0709226205945015\n",
      "Epoch 23568/30000 Training Loss: 0.06329244375228882\n",
      "Epoch 23569/30000 Training Loss: 0.06527424603700638\n",
      "Epoch 23570/30000 Training Loss: 0.06902032345533371\n",
      "Epoch 23570/30000 Validation Loss: 0.07535163313150406\n",
      "Epoch 23571/30000 Training Loss: 0.06282676756381989\n",
      "Epoch 23572/30000 Training Loss: 0.06832867115736008\n",
      "Epoch 23573/30000 Training Loss: 0.062499839812517166\n",
      "Epoch 23574/30000 Training Loss: 0.06596700102090836\n",
      "Epoch 23575/30000 Training Loss: 0.07676878571510315\n",
      "Epoch 23576/30000 Training Loss: 0.062151145190000534\n",
      "Epoch 23577/30000 Training Loss: 0.06228557601571083\n",
      "Epoch 23578/30000 Training Loss: 0.09279004484415054\n",
      "Epoch 23579/30000 Training Loss: 0.08030491322278976\n",
      "Epoch 23580/30000 Training Loss: 0.052386537194252014\n",
      "Epoch 23580/30000 Validation Loss: 0.06820186227560043\n",
      "Epoch 23581/30000 Training Loss: 0.06634886562824249\n",
      "Epoch 23582/30000 Training Loss: 0.0696399137377739\n",
      "Epoch 23583/30000 Training Loss: 0.07579245418310165\n",
      "Epoch 23584/30000 Training Loss: 0.06785397976636887\n",
      "Epoch 23585/30000 Training Loss: 0.05417663976550102\n",
      "Epoch 23586/30000 Training Loss: 0.08201181143522263\n",
      "Epoch 23587/30000 Training Loss: 0.06617743521928787\n",
      "Epoch 23588/30000 Training Loss: 0.0625564381480217\n",
      "Epoch 23589/30000 Training Loss: 0.07426725327968597\n",
      "Epoch 23590/30000 Training Loss: 0.0630682036280632\n",
      "Epoch 23590/30000 Validation Loss: 0.10313057899475098\n",
      "Epoch 23591/30000 Training Loss: 0.0609523244202137\n",
      "Epoch 23592/30000 Training Loss: 0.09309697896242142\n",
      "Epoch 23593/30000 Training Loss: 0.06320837140083313\n",
      "Epoch 23594/30000 Training Loss: 0.06473241746425629\n",
      "Epoch 23595/30000 Training Loss: 0.0925038680434227\n",
      "Epoch 23596/30000 Training Loss: 0.06825213134288788\n",
      "Epoch 23597/30000 Training Loss: 0.07512539625167847\n",
      "Epoch 23598/30000 Training Loss: 0.05668564513325691\n",
      "Epoch 23599/30000 Training Loss: 0.06379368901252747\n",
      "Epoch 23600/30000 Training Loss: 0.07002344727516174\n",
      "Epoch 23600/30000 Validation Loss: 0.06767674535512924\n",
      "Epoch 23601/30000 Training Loss: 0.06705998629331589\n",
      "Epoch 23602/30000 Training Loss: 0.06799602508544922\n",
      "Epoch 23603/30000 Training Loss: 0.05649774149060249\n",
      "Epoch 23604/30000 Training Loss: 0.07842016965150833\n",
      "Epoch 23605/30000 Training Loss: 0.06397533416748047\n",
      "Epoch 23606/30000 Training Loss: 0.09168878197669983\n",
      "Epoch 23607/30000 Training Loss: 0.07180952280759811\n",
      "Epoch 23608/30000 Training Loss: 0.06426777690649033\n",
      "Epoch 23609/30000 Training Loss: 0.06244496628642082\n",
      "Epoch 23610/30000 Training Loss: 0.0608733706176281\n",
      "Epoch 23610/30000 Validation Loss: 0.06361609697341919\n",
      "Epoch 23611/30000 Training Loss: 0.055547405034303665\n",
      "Epoch 23612/30000 Training Loss: 0.07032382488250732\n",
      "Epoch 23613/30000 Training Loss: 0.0621064119040966\n",
      "Epoch 23614/30000 Training Loss: 0.055946409702301025\n",
      "Epoch 23615/30000 Training Loss: 0.0690048560500145\n",
      "Epoch 23616/30000 Training Loss: 0.0597604475915432\n",
      "Epoch 23617/30000 Training Loss: 0.07828504592180252\n",
      "Epoch 23618/30000 Training Loss: 0.06058366969227791\n",
      "Epoch 23619/30000 Training Loss: 0.08798643201589584\n",
      "Epoch 23620/30000 Training Loss: 0.06075320020318031\n",
      "Epoch 23620/30000 Validation Loss: 0.05804559215903282\n",
      "Epoch 23621/30000 Training Loss: 0.07306928187608719\n",
      "Epoch 23622/30000 Training Loss: 0.06464162468910217\n",
      "Epoch 23623/30000 Training Loss: 0.07305318117141724\n",
      "Epoch 23624/30000 Training Loss: 0.06969356536865234\n",
      "Epoch 23625/30000 Training Loss: 0.09078847616910934\n",
      "Epoch 23626/30000 Training Loss: 0.06786372512578964\n",
      "Epoch 23627/30000 Training Loss: 0.06312013417482376\n",
      "Epoch 23628/30000 Training Loss: 0.07935355603694916\n",
      "Epoch 23629/30000 Training Loss: 0.0626516044139862\n",
      "Epoch 23630/30000 Training Loss: 0.06986752897500992\n",
      "Epoch 23630/30000 Validation Loss: 0.06427769362926483\n",
      "Epoch 23631/30000 Training Loss: 0.091792531311512\n",
      "Epoch 23632/30000 Training Loss: 0.06442075222730637\n",
      "Epoch 23633/30000 Training Loss: 0.07006826996803284\n",
      "Epoch 23634/30000 Training Loss: 0.07545360922813416\n",
      "Epoch 23635/30000 Training Loss: 0.0664520412683487\n",
      "Epoch 23636/30000 Training Loss: 0.07590632885694504\n",
      "Epoch 23637/30000 Training Loss: 0.05554505065083504\n",
      "Epoch 23638/30000 Training Loss: 0.07533824443817139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23639/30000 Training Loss: 0.07331947237253189\n",
      "Epoch 23640/30000 Training Loss: 0.06074655055999756\n",
      "Epoch 23640/30000 Validation Loss: 0.07429427653551102\n",
      "Epoch 23641/30000 Training Loss: 0.0656789019703865\n",
      "Epoch 23642/30000 Training Loss: 0.09048452973365784\n",
      "Epoch 23643/30000 Training Loss: 0.077097088098526\n",
      "Epoch 23644/30000 Training Loss: 0.08179495483636856\n",
      "Epoch 23645/30000 Training Loss: 0.07919918745756149\n",
      "Epoch 23646/30000 Training Loss: 0.08133189380168915\n",
      "Epoch 23647/30000 Training Loss: 0.058120403438806534\n",
      "Epoch 23648/30000 Training Loss: 0.07209598273038864\n",
      "Epoch 23649/30000 Training Loss: 0.06414753943681717\n",
      "Epoch 23650/30000 Training Loss: 0.08292631059885025\n",
      "Epoch 23650/30000 Validation Loss: 0.07548552751541138\n",
      "Epoch 23651/30000 Training Loss: 0.06757792830467224\n",
      "Epoch 23652/30000 Training Loss: 0.08240113407373428\n",
      "Epoch 23653/30000 Training Loss: 0.05278145149350166\n",
      "Epoch 23654/30000 Training Loss: 0.06115299090743065\n",
      "Epoch 23655/30000 Training Loss: 0.059876155108213425\n",
      "Epoch 23656/30000 Training Loss: 0.06040870025753975\n",
      "Epoch 23657/30000 Training Loss: 0.06576257944107056\n",
      "Epoch 23658/30000 Training Loss: 0.07553840428590775\n",
      "Epoch 23659/30000 Training Loss: 0.07173842936754227\n",
      "Epoch 23660/30000 Training Loss: 0.08246444910764694\n",
      "Epoch 23660/30000 Validation Loss: 0.06339982897043228\n",
      "Epoch 23661/30000 Training Loss: 0.07859189063310623\n",
      "Epoch 23662/30000 Training Loss: 0.07854720950126648\n",
      "Epoch 23663/30000 Training Loss: 0.06323929876089096\n",
      "Epoch 23664/30000 Training Loss: 0.06824377924203873\n",
      "Epoch 23665/30000 Training Loss: 0.07757224887609482\n",
      "Epoch 23666/30000 Training Loss: 0.06017027422785759\n",
      "Epoch 23667/30000 Training Loss: 0.07154204696416855\n",
      "Epoch 23668/30000 Training Loss: 0.056891992688179016\n",
      "Epoch 23669/30000 Training Loss: 0.05737295746803284\n",
      "Epoch 23670/30000 Training Loss: 0.09800118207931519\n",
      "Epoch 23670/30000 Validation Loss: 0.06573302298784256\n",
      "Epoch 23671/30000 Training Loss: 0.06348623335361481\n",
      "Epoch 23672/30000 Training Loss: 0.06707045435905457\n",
      "Epoch 23673/30000 Training Loss: 0.06041198968887329\n",
      "Epoch 23674/30000 Training Loss: 0.06984275579452515\n",
      "Epoch 23675/30000 Training Loss: 0.08181453496217728\n",
      "Epoch 23676/30000 Training Loss: 0.07779427617788315\n",
      "Epoch 23677/30000 Training Loss: 0.0655759945511818\n",
      "Epoch 23678/30000 Training Loss: 0.06159447506070137\n",
      "Epoch 23679/30000 Training Loss: 0.0609515905380249\n",
      "Epoch 23680/30000 Training Loss: 0.06542093306779861\n",
      "Epoch 23680/30000 Validation Loss: 0.0701063722372055\n",
      "Epoch 23681/30000 Training Loss: 0.07117407023906708\n",
      "Epoch 23682/30000 Training Loss: 0.06170428916811943\n",
      "Epoch 23683/30000 Training Loss: 0.07479748874902725\n",
      "Epoch 23684/30000 Training Loss: 0.06362421065568924\n",
      "Epoch 23685/30000 Training Loss: 0.08012103289365768\n",
      "Epoch 23686/30000 Training Loss: 0.0728435292840004\n",
      "Epoch 23687/30000 Training Loss: 0.05962100625038147\n",
      "Epoch 23688/30000 Training Loss: 0.06114083528518677\n",
      "Epoch 23689/30000 Training Loss: 0.10158262401819229\n",
      "Epoch 23690/30000 Training Loss: 0.069687619805336\n",
      "Epoch 23690/30000 Validation Loss: 0.06730332225561142\n",
      "Epoch 23691/30000 Training Loss: 0.08257504552602768\n",
      "Epoch 23692/30000 Training Loss: 0.06438278406858444\n",
      "Epoch 23693/30000 Training Loss: 0.051457542926073074\n",
      "Epoch 23694/30000 Training Loss: 0.07492440938949585\n",
      "Epoch 23695/30000 Training Loss: 0.07292111217975616\n",
      "Epoch 23696/30000 Training Loss: 0.05088518559932709\n",
      "Epoch 23697/30000 Training Loss: 0.07730378955602646\n",
      "Epoch 23698/30000 Training Loss: 0.07468963414430618\n",
      "Epoch 23699/30000 Training Loss: 0.07168776541948318\n",
      "Epoch 23700/30000 Training Loss: 0.08310392498970032\n",
      "Epoch 23700/30000 Validation Loss: 0.08018568158149719\n",
      "Epoch 23701/30000 Training Loss: 0.0772894099354744\n",
      "Epoch 23702/30000 Training Loss: 0.07371952384710312\n",
      "Epoch 23703/30000 Training Loss: 0.07314524799585342\n",
      "Epoch 23704/30000 Training Loss: 0.06900202482938766\n",
      "Epoch 23705/30000 Training Loss: 0.07449215650558472\n",
      "Epoch 23706/30000 Training Loss: 0.06621309369802475\n",
      "Epoch 23707/30000 Training Loss: 0.06567200273275375\n",
      "Epoch 23708/30000 Training Loss: 0.10330323129892349\n",
      "Epoch 23709/30000 Training Loss: 0.07251442223787308\n",
      "Epoch 23710/30000 Training Loss: 0.04647968336939812\n",
      "Epoch 23710/30000 Validation Loss: 0.07618337124586105\n",
      "Epoch 23711/30000 Training Loss: 0.057473719120025635\n",
      "Epoch 23712/30000 Training Loss: 0.07043647021055222\n",
      "Epoch 23713/30000 Training Loss: 0.08247299492359161\n",
      "Epoch 23714/30000 Training Loss: 0.06795758754014969\n",
      "Epoch 23715/30000 Training Loss: 0.07616455107927322\n",
      "Epoch 23716/30000 Training Loss: 0.05441422387957573\n",
      "Epoch 23717/30000 Training Loss: 0.05560733377933502\n",
      "Epoch 23718/30000 Training Loss: 0.07260186225175858\n",
      "Epoch 23719/30000 Training Loss: 0.06432966142892838\n",
      "Epoch 23720/30000 Training Loss: 0.09911204129457474\n",
      "Epoch 23720/30000 Validation Loss: 0.07431292533874512\n",
      "Epoch 23721/30000 Training Loss: 0.07077499479055405\n",
      "Epoch 23722/30000 Training Loss: 0.05778995156288147\n",
      "Epoch 23723/30000 Training Loss: 0.06800776720046997\n",
      "Epoch 23724/30000 Training Loss: 0.05872620642185211\n",
      "Epoch 23725/30000 Training Loss: 0.07269321382045746\n",
      "Epoch 23726/30000 Training Loss: 0.05805578827857971\n",
      "Epoch 23727/30000 Training Loss: 0.07525646686553955\n",
      "Epoch 23728/30000 Training Loss: 0.06316594034433365\n",
      "Epoch 23729/30000 Training Loss: 0.07077372074127197\n",
      "Epoch 23730/30000 Training Loss: 0.078037790954113\n",
      "Epoch 23730/30000 Validation Loss: 0.08196414262056351\n",
      "Epoch 23731/30000 Training Loss: 0.06385213136672974\n",
      "Epoch 23732/30000 Training Loss: 0.05224578455090523\n",
      "Epoch 23733/30000 Training Loss: 0.060249458998441696\n",
      "Epoch 23734/30000 Training Loss: 0.11506903171539307\n",
      "Epoch 23735/30000 Training Loss: 0.06431016325950623\n",
      "Epoch 23736/30000 Training Loss: 0.06359919160604477\n",
      "Epoch 23737/30000 Training Loss: 0.07484780997037888\n",
      "Epoch 23738/30000 Training Loss: 0.06566468626260757\n",
      "Epoch 23739/30000 Training Loss: 0.07817059755325317\n",
      "Epoch 23740/30000 Training Loss: 0.06701088696718216\n",
      "Epoch 23740/30000 Validation Loss: 0.09112635254859924\n",
      "Epoch 23741/30000 Training Loss: 0.0841016173362732\n",
      "Epoch 23742/30000 Training Loss: 0.06022491678595543\n",
      "Epoch 23743/30000 Training Loss: 0.07961954921483994\n",
      "Epoch 23744/30000 Training Loss: 0.07524016499519348\n",
      "Epoch 23745/30000 Training Loss: 0.07863006740808487\n",
      "Epoch 23746/30000 Training Loss: 0.06970115005970001\n",
      "Epoch 23747/30000 Training Loss: 0.07096376270055771\n",
      "Epoch 23748/30000 Training Loss: 0.07789769768714905\n",
      "Epoch 23749/30000 Training Loss: 0.06277432292699814\n",
      "Epoch 23750/30000 Training Loss: 0.06718374788761139\n",
      "Epoch 23750/30000 Validation Loss: 0.06948592513799667\n",
      "Epoch 23751/30000 Training Loss: 0.06434737145900726\n",
      "Epoch 23752/30000 Training Loss: 0.05995228886604309\n",
      "Epoch 23753/30000 Training Loss: 0.05054223909974098\n",
      "Epoch 23754/30000 Training Loss: 0.06419462710618973\n",
      "Epoch 23755/30000 Training Loss: 0.0785059928894043\n",
      "Epoch 23756/30000 Training Loss: 0.0785936713218689\n",
      "Epoch 23757/30000 Training Loss: 0.05761463940143585\n",
      "Epoch 23758/30000 Training Loss: 0.06515577435493469\n",
      "Epoch 23759/30000 Training Loss: 0.07366728782653809\n",
      "Epoch 23760/30000 Training Loss: 0.05980857089161873\n",
      "Epoch 23760/30000 Validation Loss: 0.07139397412538528\n",
      "Epoch 23761/30000 Training Loss: 0.06790831685066223\n",
      "Epoch 23762/30000 Training Loss: 0.07256804406642914\n",
      "Epoch 23763/30000 Training Loss: 0.07985362410545349\n",
      "Epoch 23764/30000 Training Loss: 0.06658939272165298\n",
      "Epoch 23765/30000 Training Loss: 0.06461535394191742\n",
      "Epoch 23766/30000 Training Loss: 0.06698980182409286\n",
      "Epoch 23767/30000 Training Loss: 0.0731620118021965\n",
      "Epoch 23768/30000 Training Loss: 0.07350222766399384\n",
      "Epoch 23769/30000 Training Loss: 0.08042863756418228\n",
      "Epoch 23770/30000 Training Loss: 0.0538964569568634\n",
      "Epoch 23770/30000 Validation Loss: 0.07113125175237656\n",
      "Epoch 23771/30000 Training Loss: 0.06091315671801567\n",
      "Epoch 23772/30000 Training Loss: 0.06858900189399719\n",
      "Epoch 23773/30000 Training Loss: 0.09968049079179764\n",
      "Epoch 23774/30000 Training Loss: 0.06260332465171814\n",
      "Epoch 23775/30000 Training Loss: 0.06019077077507973\n",
      "Epoch 23776/30000 Training Loss: 0.07302553951740265\n",
      "Epoch 23777/30000 Training Loss: 0.07929497957229614\n",
      "Epoch 23778/30000 Training Loss: 0.07884322851896286\n",
      "Epoch 23779/30000 Training Loss: 0.08482825756072998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23780/30000 Training Loss: 0.06610681861639023\n",
      "Epoch 23780/30000 Validation Loss: 0.06672001630067825\n",
      "Epoch 23781/30000 Training Loss: 0.058872710913419724\n",
      "Epoch 23782/30000 Training Loss: 0.06355931609869003\n",
      "Epoch 23783/30000 Training Loss: 0.07139880210161209\n",
      "Epoch 23784/30000 Training Loss: 0.08701638132333755\n",
      "Epoch 23785/30000 Training Loss: 0.0648389682173729\n",
      "Epoch 23786/30000 Training Loss: 0.0776350274682045\n",
      "Epoch 23787/30000 Training Loss: 0.0712340697646141\n",
      "Epoch 23788/30000 Training Loss: 0.06667596846818924\n",
      "Epoch 23789/30000 Training Loss: 0.06174309179186821\n",
      "Epoch 23790/30000 Training Loss: 0.07742825895547867\n",
      "Epoch 23790/30000 Validation Loss: 0.06657342612743378\n",
      "Epoch 23791/30000 Training Loss: 0.06181542947888374\n",
      "Epoch 23792/30000 Training Loss: 0.06575978547334671\n",
      "Epoch 23793/30000 Training Loss: 0.06565745919942856\n",
      "Epoch 23794/30000 Training Loss: 0.055061209946870804\n",
      "Epoch 23795/30000 Training Loss: 0.06939291208982468\n",
      "Epoch 23796/30000 Training Loss: 0.06929682940244675\n",
      "Epoch 23797/30000 Training Loss: 0.06558528542518616\n",
      "Epoch 23798/30000 Training Loss: 0.05872689560055733\n",
      "Epoch 23799/30000 Training Loss: 0.06451334059238434\n",
      "Epoch 23800/30000 Training Loss: 0.07316429167985916\n",
      "Epoch 23800/30000 Validation Loss: 0.05575552582740784\n",
      "Epoch 23801/30000 Training Loss: 0.06357209384441376\n",
      "Epoch 23802/30000 Training Loss: 0.08308294415473938\n",
      "Epoch 23803/30000 Training Loss: 0.05563170090317726\n",
      "Epoch 23804/30000 Training Loss: 0.06768367439508438\n",
      "Epoch 23805/30000 Training Loss: 0.07309889048337936\n",
      "Epoch 23806/30000 Training Loss: 0.07773313671350479\n",
      "Epoch 23807/30000 Training Loss: 0.07120099663734436\n",
      "Epoch 23808/30000 Training Loss: 0.07753194123506546\n",
      "Epoch 23809/30000 Training Loss: 0.08181177824735641\n",
      "Epoch 23810/30000 Training Loss: 0.07854229211807251\n",
      "Epoch 23810/30000 Validation Loss: 0.06984877586364746\n",
      "Epoch 23811/30000 Training Loss: 0.05312490835785866\n",
      "Epoch 23812/30000 Training Loss: 0.07864860445261002\n",
      "Epoch 23813/30000 Training Loss: 0.06849479675292969\n",
      "Epoch 23814/30000 Training Loss: 0.06469113379716873\n",
      "Epoch 23815/30000 Training Loss: 0.08167179673910141\n",
      "Epoch 23816/30000 Training Loss: 0.06558992713689804\n",
      "Epoch 23817/30000 Training Loss: 0.0651058629155159\n",
      "Epoch 23818/30000 Training Loss: 0.08030671626329422\n",
      "Epoch 23819/30000 Training Loss: 0.06532496213912964\n",
      "Epoch 23820/30000 Training Loss: 0.08120603114366531\n",
      "Epoch 23820/30000 Validation Loss: 0.08183180540800095\n",
      "Epoch 23821/30000 Training Loss: 0.07420936971902847\n",
      "Epoch 23822/30000 Training Loss: 0.05252053961157799\n",
      "Epoch 23823/30000 Training Loss: 0.0803256630897522\n",
      "Epoch 23824/30000 Training Loss: 0.06577622890472412\n",
      "Epoch 23825/30000 Training Loss: 0.0557703860104084\n",
      "Epoch 23826/30000 Training Loss: 0.08311855792999268\n",
      "Epoch 23827/30000 Training Loss: 0.07894328236579895\n",
      "Epoch 23828/30000 Training Loss: 0.0942688062787056\n",
      "Epoch 23829/30000 Training Loss: 0.07143803685903549\n",
      "Epoch 23830/30000 Training Loss: 0.06841570883989334\n",
      "Epoch 23830/30000 Validation Loss: 0.07474956661462784\n",
      "Epoch 23831/30000 Training Loss: 0.0676831603050232\n",
      "Epoch 23832/30000 Training Loss: 0.05815506353974342\n",
      "Epoch 23833/30000 Training Loss: 0.06371109187602997\n",
      "Epoch 23834/30000 Training Loss: 0.07969554513692856\n",
      "Epoch 23835/30000 Training Loss: 0.06278642266988754\n",
      "Epoch 23836/30000 Training Loss: 0.07645157724618912\n",
      "Epoch 23837/30000 Training Loss: 0.08023783564567566\n",
      "Epoch 23838/30000 Training Loss: 0.07267805188894272\n",
      "Epoch 23839/30000 Training Loss: 0.07418953627347946\n",
      "Epoch 23840/30000 Training Loss: 0.07927033305168152\n",
      "Epoch 23840/30000 Validation Loss: 0.0619136206805706\n",
      "Epoch 23841/30000 Training Loss: 0.0739985853433609\n",
      "Epoch 23842/30000 Training Loss: 0.05875854194164276\n",
      "Epoch 23843/30000 Training Loss: 0.06519809365272522\n",
      "Epoch 23844/30000 Training Loss: 0.06623301655054092\n",
      "Epoch 23845/30000 Training Loss: 0.07036621868610382\n",
      "Epoch 23846/30000 Training Loss: 0.06503736227750778\n",
      "Epoch 23847/30000 Training Loss: 0.07454954087734222\n",
      "Epoch 23848/30000 Training Loss: 0.05739198252558708\n",
      "Epoch 23849/30000 Training Loss: 0.06662570685148239\n",
      "Epoch 23850/30000 Training Loss: 0.05722479894757271\n",
      "Epoch 23850/30000 Validation Loss: 0.08194088190793991\n",
      "Epoch 23851/30000 Training Loss: 0.06715516000986099\n",
      "Epoch 23852/30000 Training Loss: 0.06825766712427139\n",
      "Epoch 23853/30000 Training Loss: 0.07051387429237366\n",
      "Epoch 23854/30000 Training Loss: 0.05855186656117439\n",
      "Epoch 23855/30000 Training Loss: 0.09389569610357285\n",
      "Epoch 23856/30000 Training Loss: 0.08531024307012558\n",
      "Epoch 23857/30000 Training Loss: 0.07245593518018723\n",
      "Epoch 23858/30000 Training Loss: 0.07616331428289413\n",
      "Epoch 23859/30000 Training Loss: 0.0655258372426033\n",
      "Epoch 23860/30000 Training Loss: 0.07488641887903214\n",
      "Epoch 23860/30000 Validation Loss: 0.06463142484426498\n",
      "Epoch 23861/30000 Training Loss: 0.0672801211476326\n",
      "Epoch 23862/30000 Training Loss: 0.0658230409026146\n",
      "Epoch 23863/30000 Training Loss: 0.06646168231964111\n",
      "Epoch 23864/30000 Training Loss: 0.055854395031929016\n",
      "Epoch 23865/30000 Training Loss: 0.07779348641633987\n",
      "Epoch 23866/30000 Training Loss: 0.07333556562662125\n",
      "Epoch 23867/30000 Training Loss: 0.08653666824102402\n",
      "Epoch 23868/30000 Training Loss: 0.07279898226261139\n",
      "Epoch 23869/30000 Training Loss: 0.04972356557846069\n",
      "Epoch 23870/30000 Training Loss: 0.06831683963537216\n",
      "Epoch 23870/30000 Validation Loss: 0.06789694726467133\n",
      "Epoch 23871/30000 Training Loss: 0.06609264016151428\n",
      "Epoch 23872/30000 Training Loss: 0.08272673934698105\n",
      "Epoch 23873/30000 Training Loss: 0.06367602199316025\n",
      "Epoch 23874/30000 Training Loss: 0.10077769309282303\n",
      "Epoch 23875/30000 Training Loss: 0.07005121558904648\n",
      "Epoch 23876/30000 Training Loss: 0.0638287365436554\n",
      "Epoch 23877/30000 Training Loss: 0.08057862520217896\n",
      "Epoch 23878/30000 Training Loss: 0.07003919780254364\n",
      "Epoch 23879/30000 Training Loss: 0.06958217173814774\n",
      "Epoch 23880/30000 Training Loss: 0.0693889707326889\n",
      "Epoch 23880/30000 Validation Loss: 0.05465972051024437\n",
      "Epoch 23881/30000 Training Loss: 0.06152934953570366\n",
      "Epoch 23882/30000 Training Loss: 0.06116766855120659\n",
      "Epoch 23883/30000 Training Loss: 0.05556810274720192\n",
      "Epoch 23884/30000 Training Loss: 0.07047560065984726\n",
      "Epoch 23885/30000 Training Loss: 0.05863503739237785\n",
      "Epoch 23886/30000 Training Loss: 0.06845059245824814\n",
      "Epoch 23887/30000 Training Loss: 0.08252572268247604\n",
      "Epoch 23888/30000 Training Loss: 0.07133933156728745\n",
      "Epoch 23889/30000 Training Loss: 0.07728666812181473\n",
      "Epoch 23890/30000 Training Loss: 0.06747422367334366\n",
      "Epoch 23890/30000 Validation Loss: 0.06014391779899597\n",
      "Epoch 23891/30000 Training Loss: 0.08661005645990372\n",
      "Epoch 23892/30000 Training Loss: 0.07998210191726685\n",
      "Epoch 23893/30000 Training Loss: 0.07225877046585083\n",
      "Epoch 23894/30000 Training Loss: 0.050414055585861206\n",
      "Epoch 23895/30000 Training Loss: 0.06021232530474663\n",
      "Epoch 23896/30000 Training Loss: 0.06834744662046432\n",
      "Epoch 23897/30000 Training Loss: 0.06184880807995796\n",
      "Epoch 23898/30000 Training Loss: 0.08912024646997452\n",
      "Epoch 23899/30000 Training Loss: 0.06778398156166077\n",
      "Epoch 23900/30000 Training Loss: 0.06485126167535782\n",
      "Epoch 23900/30000 Validation Loss: 0.07418762892484665\n",
      "Epoch 23901/30000 Training Loss: 0.07424286752939224\n",
      "Epoch 23902/30000 Training Loss: 0.06095978245139122\n",
      "Epoch 23903/30000 Training Loss: 0.06206698343157768\n",
      "Epoch 23904/30000 Training Loss: 0.0822138711810112\n",
      "Epoch 23905/30000 Training Loss: 0.05852505564689636\n",
      "Epoch 23906/30000 Training Loss: 0.06492334604263306\n",
      "Epoch 23907/30000 Training Loss: 0.06743630021810532\n",
      "Epoch 23908/30000 Training Loss: 0.05243527889251709\n",
      "Epoch 23909/30000 Training Loss: 0.057717833667993546\n",
      "Epoch 23910/30000 Training Loss: 0.06393861025571823\n",
      "Epoch 23910/30000 Validation Loss: 0.06022832170128822\n",
      "Epoch 23911/30000 Training Loss: 0.06990330666303635\n",
      "Epoch 23912/30000 Training Loss: 0.07792406529188156\n",
      "Epoch 23913/30000 Training Loss: 0.0698050707578659\n",
      "Epoch 23914/30000 Training Loss: 0.05913512036204338\n",
      "Epoch 23915/30000 Training Loss: 0.06911850720643997\n",
      "Epoch 23916/30000 Training Loss: 0.06696167588233948\n",
      "Epoch 23917/30000 Training Loss: 0.06493816524744034\n",
      "Epoch 23918/30000 Training Loss: 0.0666256994009018\n",
      "Epoch 23919/30000 Training Loss: 0.057329099625349045\n",
      "Epoch 23920/30000 Training Loss: 0.06868011504411697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23920/30000 Validation Loss: 0.06188260391354561\n",
      "Epoch 23921/30000 Training Loss: 0.09710264205932617\n",
      "Epoch 23922/30000 Training Loss: 0.07215318083763123\n",
      "Epoch 23923/30000 Training Loss: 0.06574998050928116\n",
      "Epoch 23924/30000 Training Loss: 0.05764107033610344\n",
      "Epoch 23925/30000 Training Loss: 0.07004527747631073\n",
      "Epoch 23926/30000 Training Loss: 0.07659929245710373\n",
      "Epoch 23927/30000 Training Loss: 0.07225630432367325\n",
      "Epoch 23928/30000 Training Loss: 0.07487031817436218\n",
      "Epoch 23929/30000 Training Loss: 0.060502585023641586\n",
      "Epoch 23930/30000 Training Loss: 0.07715347409248352\n",
      "Epoch 23930/30000 Validation Loss: 0.04146156460046768\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04146156460046768<=============\n",
      "Epoch 23931/30000 Training Loss: 0.069126196205616\n",
      "Epoch 23932/30000 Training Loss: 0.06710547208786011\n",
      "Epoch 23933/30000 Training Loss: 0.08091545850038528\n",
      "Epoch 23934/30000 Training Loss: 0.06390846520662308\n",
      "Epoch 23935/30000 Training Loss: 0.0726996511220932\n",
      "Epoch 23936/30000 Training Loss: 0.07047528773546219\n",
      "Epoch 23937/30000 Training Loss: 0.07159259170293808\n",
      "Epoch 23938/30000 Training Loss: 0.07602822780609131\n",
      "Epoch 23939/30000 Training Loss: 0.06808627396821976\n",
      "Epoch 23940/30000 Training Loss: 0.061584457755088806\n",
      "Epoch 23940/30000 Validation Loss: 0.06696383655071259\n",
      "Epoch 23941/30000 Training Loss: 0.07359514385461807\n",
      "Epoch 23942/30000 Training Loss: 0.07401509582996368\n",
      "Epoch 23943/30000 Training Loss: 0.059669435024261475\n",
      "Epoch 23944/30000 Training Loss: 0.08485522866249084\n",
      "Epoch 23945/30000 Training Loss: 0.06500213593244553\n",
      "Epoch 23946/30000 Training Loss: 0.06511814147233963\n",
      "Epoch 23947/30000 Training Loss: 0.08043808490037918\n",
      "Epoch 23948/30000 Training Loss: 0.06197318434715271\n",
      "Epoch 23949/30000 Training Loss: 0.07096637040376663\n",
      "Epoch 23950/30000 Training Loss: 0.08441362529993057\n",
      "Epoch 23950/30000 Validation Loss: 0.07560236006975174\n",
      "Epoch 23951/30000 Training Loss: 0.0718807727098465\n",
      "Epoch 23952/30000 Training Loss: 0.07076781988143921\n",
      "Epoch 23953/30000 Training Loss: 0.0716128870844841\n",
      "Epoch 23954/30000 Training Loss: 0.056918635964393616\n",
      "Epoch 23955/30000 Training Loss: 0.05933089926838875\n",
      "Epoch 23956/30000 Training Loss: 0.057512152940034866\n",
      "Epoch 23957/30000 Training Loss: 0.056001435965299606\n",
      "Epoch 23958/30000 Training Loss: 0.08224797993898392\n",
      "Epoch 23959/30000 Training Loss: 0.07560237497091293\n",
      "Epoch 23960/30000 Training Loss: 0.0786176547408104\n",
      "Epoch 23960/30000 Validation Loss: 0.08537593483924866\n",
      "Epoch 23961/30000 Training Loss: 0.06154019758105278\n",
      "Epoch 23962/30000 Training Loss: 0.07293929904699326\n",
      "Epoch 23963/30000 Training Loss: 0.057201821357011795\n",
      "Epoch 23964/30000 Training Loss: 0.04884842038154602\n",
      "Epoch 23965/30000 Training Loss: 0.05494503304362297\n",
      "Epoch 23966/30000 Training Loss: 0.07019095867872238\n",
      "Epoch 23967/30000 Training Loss: 0.0829029306769371\n",
      "Epoch 23968/30000 Training Loss: 0.08778616040945053\n",
      "Epoch 23969/30000 Training Loss: 0.06626317650079727\n",
      "Epoch 23970/30000 Training Loss: 0.10006768256425858\n",
      "Epoch 23970/30000 Validation Loss: 0.07196678966283798\n",
      "Epoch 23971/30000 Training Loss: 0.06379086524248123\n",
      "Epoch 23972/30000 Training Loss: 0.06777065992355347\n",
      "Epoch 23973/30000 Training Loss: 0.07123453170061111\n",
      "Epoch 23974/30000 Training Loss: 0.05562971904873848\n",
      "Epoch 23975/30000 Training Loss: 0.07298937439918518\n",
      "Epoch 23976/30000 Training Loss: 0.06684719771146774\n",
      "Epoch 23977/30000 Training Loss: 0.07101740688085556\n",
      "Epoch 23978/30000 Training Loss: 0.07799198478460312\n",
      "Epoch 23979/30000 Training Loss: 0.058641787618398666\n",
      "Epoch 23980/30000 Training Loss: 0.05906802415847778\n",
      "Epoch 23980/30000 Validation Loss: 0.08747822791337967\n",
      "Epoch 23981/30000 Training Loss: 0.05634928122162819\n",
      "Epoch 23982/30000 Training Loss: 0.06716538220643997\n",
      "Epoch 23983/30000 Training Loss: 0.0581618957221508\n",
      "Epoch 23984/30000 Training Loss: 0.06470389664173126\n",
      "Epoch 23985/30000 Training Loss: 0.06324566155672073\n",
      "Epoch 23986/30000 Training Loss: 0.06738152354955673\n",
      "Epoch 23987/30000 Training Loss: 0.05853886529803276\n",
      "Epoch 23988/30000 Training Loss: 0.06966198235750198\n",
      "Epoch 23989/30000 Training Loss: 0.08061718940734863\n",
      "Epoch 23990/30000 Training Loss: 0.06778102368116379\n",
      "Epoch 23990/30000 Validation Loss: 0.06673259288072586\n",
      "Epoch 23991/30000 Training Loss: 0.07547057420015335\n",
      "Epoch 23992/30000 Training Loss: 0.06633037328720093\n",
      "Epoch 23993/30000 Training Loss: 0.08353995531797409\n",
      "Epoch 23994/30000 Training Loss: 0.06273377686738968\n",
      "Epoch 23995/30000 Training Loss: 0.07405473291873932\n",
      "Epoch 23996/30000 Training Loss: 0.05646245554089546\n",
      "Epoch 23997/30000 Training Loss: 0.06371632218360901\n",
      "Epoch 23998/30000 Training Loss: 0.06462084501981735\n",
      "Epoch 23999/30000 Training Loss: 0.0754837691783905\n",
      "Epoch 24000/30000 Training Loss: 0.060817211866378784\n",
      "Epoch 24000/30000 Validation Loss: 0.060235898941755295\n",
      "Epoch 24001/30000 Training Loss: 0.08038002252578735\n",
      "Epoch 24002/30000 Training Loss: 0.06940943747758865\n",
      "Epoch 24003/30000 Training Loss: 0.06313061714172363\n",
      "Epoch 24004/30000 Training Loss: 0.06577587127685547\n",
      "Epoch 24005/30000 Training Loss: 0.06175469234585762\n",
      "Epoch 24006/30000 Training Loss: 0.0754847303032875\n",
      "Epoch 24007/30000 Training Loss: 0.07714370638132095\n",
      "Epoch 24008/30000 Training Loss: 0.06991814821958542\n",
      "Epoch 24009/30000 Training Loss: 0.05382024124264717\n",
      "Epoch 24010/30000 Training Loss: 0.08185136318206787\n",
      "Epoch 24010/30000 Validation Loss: 0.06358271092176437\n",
      "Epoch 24011/30000 Training Loss: 0.07504158467054367\n",
      "Epoch 24012/30000 Training Loss: 0.08490403741598129\n",
      "Epoch 24013/30000 Training Loss: 0.06868220120668411\n",
      "Epoch 24014/30000 Training Loss: 0.06533405184745789\n",
      "Epoch 24015/30000 Training Loss: 0.07674650102853775\n",
      "Epoch 24016/30000 Training Loss: 0.06613879650831223\n",
      "Epoch 24017/30000 Training Loss: 0.08385691046714783\n",
      "Epoch 24018/30000 Training Loss: 0.062388647347688675\n",
      "Epoch 24019/30000 Training Loss: 0.06268947571516037\n",
      "Epoch 24020/30000 Training Loss: 0.08631535619497299\n",
      "Epoch 24020/30000 Validation Loss: 0.07231350988149643\n",
      "Epoch 24021/30000 Training Loss: 0.07396169751882553\n",
      "Epoch 24022/30000 Training Loss: 0.06545152515172958\n",
      "Epoch 24023/30000 Training Loss: 0.07505229115486145\n",
      "Epoch 24024/30000 Training Loss: 0.06268342584371567\n",
      "Epoch 24025/30000 Training Loss: 0.05783355236053467\n",
      "Epoch 24026/30000 Training Loss: 0.057378482073545456\n",
      "Epoch 24027/30000 Training Loss: 0.09069022536277771\n",
      "Epoch 24028/30000 Training Loss: 0.07261203974485397\n",
      "Epoch 24029/30000 Training Loss: 0.06910840421915054\n",
      "Epoch 24030/30000 Training Loss: 0.07250890135765076\n",
      "Epoch 24030/30000 Validation Loss: 0.0739268958568573\n",
      "Epoch 24031/30000 Training Loss: 0.06268034875392914\n",
      "Epoch 24032/30000 Training Loss: 0.08009815216064453\n",
      "Epoch 24033/30000 Training Loss: 0.05868268385529518\n",
      "Epoch 24034/30000 Training Loss: 0.07008599489927292\n",
      "Epoch 24035/30000 Training Loss: 0.05844475328922272\n",
      "Epoch 24036/30000 Training Loss: 0.05502859875559807\n",
      "Epoch 24037/30000 Training Loss: 0.06296514719724655\n",
      "Epoch 24038/30000 Training Loss: 0.07465674728155136\n",
      "Epoch 24039/30000 Training Loss: 0.06718524545431137\n",
      "Epoch 24040/30000 Training Loss: 0.060142457485198975\n",
      "Epoch 24040/30000 Validation Loss: 0.06456669420003891\n",
      "Epoch 24041/30000 Training Loss: 0.06711925566196442\n",
      "Epoch 24042/30000 Training Loss: 0.09254000335931778\n",
      "Epoch 24043/30000 Training Loss: 0.07070503383874893\n",
      "Epoch 24044/30000 Training Loss: 0.06509320437908173\n",
      "Epoch 24045/30000 Training Loss: 0.07441926747560501\n",
      "Epoch 24046/30000 Training Loss: 0.06795237213373184\n",
      "Epoch 24047/30000 Training Loss: 0.06513003259897232\n",
      "Epoch 24048/30000 Training Loss: 0.058821290731430054\n",
      "Epoch 24049/30000 Training Loss: 0.07201465219259262\n",
      "Epoch 24050/30000 Training Loss: 0.08590323477983475\n",
      "Epoch 24050/30000 Validation Loss: 0.08279843628406525\n",
      "Epoch 24051/30000 Training Loss: 0.0713021382689476\n",
      "Epoch 24052/30000 Training Loss: 0.08057525008916855\n",
      "Epoch 24053/30000 Training Loss: 0.07013682276010513\n",
      "Epoch 24054/30000 Training Loss: 0.08179715275764465\n",
      "Epoch 24055/30000 Training Loss: 0.0736519992351532\n",
      "Epoch 24056/30000 Training Loss: 0.06194564700126648\n",
      "Epoch 24057/30000 Training Loss: 0.06475221365690231\n",
      "Epoch 24058/30000 Training Loss: 0.0789070725440979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24059/30000 Training Loss: 0.06765740364789963\n",
      "Epoch 24060/30000 Training Loss: 0.07667795568704605\n",
      "Epoch 24060/30000 Validation Loss: 0.06480322778224945\n",
      "Epoch 24061/30000 Training Loss: 0.07795610278844833\n",
      "Epoch 24062/30000 Training Loss: 0.07831975072622299\n",
      "Epoch 24063/30000 Training Loss: 0.09158137440681458\n",
      "Epoch 24064/30000 Training Loss: 0.07539356499910355\n",
      "Epoch 24065/30000 Training Loss: 0.06622185558080673\n",
      "Epoch 24066/30000 Training Loss: 0.07065068185329437\n",
      "Epoch 24067/30000 Training Loss: 0.0703081414103508\n",
      "Epoch 24068/30000 Training Loss: 0.06578469276428223\n",
      "Epoch 24069/30000 Training Loss: 0.08598408848047256\n",
      "Epoch 24070/30000 Training Loss: 0.05780906602740288\n",
      "Epoch 24070/30000 Validation Loss: 0.0948600172996521\n",
      "Epoch 24071/30000 Training Loss: 0.05296333134174347\n",
      "Epoch 24072/30000 Training Loss: 0.07173766940832138\n",
      "Epoch 24073/30000 Training Loss: 0.05317525938153267\n",
      "Epoch 24074/30000 Training Loss: 0.06890454888343811\n",
      "Epoch 24075/30000 Training Loss: 0.06375007331371307\n",
      "Epoch 24076/30000 Training Loss: 0.07988763600587845\n",
      "Epoch 24077/30000 Training Loss: 0.08276032656431198\n",
      "Epoch 24078/30000 Training Loss: 0.07932522892951965\n",
      "Epoch 24079/30000 Training Loss: 0.07073988765478134\n",
      "Epoch 24080/30000 Training Loss: 0.07134125381708145\n",
      "Epoch 24080/30000 Validation Loss: 0.07972237467765808\n",
      "Epoch 24081/30000 Training Loss: 0.08062862604856491\n",
      "Epoch 24082/30000 Training Loss: 0.07242676615715027\n",
      "Epoch 24083/30000 Training Loss: 0.05875801667571068\n",
      "Epoch 24084/30000 Training Loss: 0.07949704676866531\n",
      "Epoch 24085/30000 Training Loss: 0.06963728368282318\n",
      "Epoch 24086/30000 Training Loss: 0.06136317923665047\n",
      "Epoch 24087/30000 Training Loss: 0.08091788738965988\n",
      "Epoch 24088/30000 Training Loss: 0.07373922318220139\n",
      "Epoch 24089/30000 Training Loss: 0.09407097101211548\n",
      "Epoch 24090/30000 Training Loss: 0.057194847613573074\n",
      "Epoch 24090/30000 Validation Loss: 0.09991905093193054\n",
      "Epoch 24091/30000 Training Loss: 0.07566239684820175\n",
      "Epoch 24092/30000 Training Loss: 0.09338811039924622\n",
      "Epoch 24093/30000 Training Loss: 0.09099485725164413\n",
      "Epoch 24094/30000 Training Loss: 0.08506884425878525\n",
      "Epoch 24095/30000 Training Loss: 0.06674420833587646\n",
      "Epoch 24096/30000 Training Loss: 0.07812384516000748\n",
      "Epoch 24097/30000 Training Loss: 0.07753738015890121\n",
      "Epoch 24098/30000 Training Loss: 0.07339366525411606\n",
      "Epoch 24099/30000 Training Loss: 0.06982714682817459\n",
      "Epoch 24100/30000 Training Loss: 0.07139960676431656\n",
      "Epoch 24100/30000 Validation Loss: 0.06513150036334991\n",
      "Epoch 24101/30000 Training Loss: 0.07815473526716232\n",
      "Epoch 24102/30000 Training Loss: 0.06293720752000809\n",
      "Epoch 24103/30000 Training Loss: 0.06866668909788132\n",
      "Epoch 24104/30000 Training Loss: 0.07301200181245804\n",
      "Epoch 24105/30000 Training Loss: 0.06810857355594635\n",
      "Epoch 24106/30000 Training Loss: 0.05921204760670662\n",
      "Epoch 24107/30000 Training Loss: 0.07016495615243912\n",
      "Epoch 24108/30000 Training Loss: 0.0694170892238617\n",
      "Epoch 24109/30000 Training Loss: 0.06832420080900192\n",
      "Epoch 24110/30000 Training Loss: 0.07921057939529419\n",
      "Epoch 24110/30000 Validation Loss: 0.08513132482767105\n",
      "Epoch 24111/30000 Training Loss: 0.07073075324296951\n",
      "Epoch 24112/30000 Training Loss: 0.06467566639184952\n",
      "Epoch 24113/30000 Training Loss: 0.06359922140836716\n",
      "Epoch 24114/30000 Training Loss: 0.06739439070224762\n",
      "Epoch 24115/30000 Training Loss: 0.08124914020299911\n",
      "Epoch 24116/30000 Training Loss: 0.05767549201846123\n",
      "Epoch 24117/30000 Training Loss: 0.0739746242761612\n",
      "Epoch 24118/30000 Training Loss: 0.07165470719337463\n",
      "Epoch 24119/30000 Training Loss: 0.06000050902366638\n",
      "Epoch 24120/30000 Training Loss: 0.06780152767896652\n",
      "Epoch 24120/30000 Validation Loss: 0.07105406373739243\n",
      "Epoch 24121/30000 Training Loss: 0.059366416186094284\n",
      "Epoch 24122/30000 Training Loss: 0.07209090143442154\n",
      "Epoch 24123/30000 Training Loss: 0.06352263689041138\n",
      "Epoch 24124/30000 Training Loss: 0.07804752141237259\n",
      "Epoch 24125/30000 Training Loss: 0.06012247875332832\n",
      "Epoch 24126/30000 Training Loss: 0.07524839043617249\n",
      "Epoch 24127/30000 Training Loss: 0.07453780621290207\n",
      "Epoch 24128/30000 Training Loss: 0.08295588940382004\n",
      "Epoch 24129/30000 Training Loss: 0.07700568437576294\n",
      "Epoch 24130/30000 Training Loss: 0.05385274067521095\n",
      "Epoch 24130/30000 Validation Loss: 0.06549683958292007\n",
      "Epoch 24131/30000 Training Loss: 0.07420758157968521\n",
      "Epoch 24132/30000 Training Loss: 0.08288800716400146\n",
      "Epoch 24133/30000 Training Loss: 0.06604542583227158\n",
      "Epoch 24134/30000 Training Loss: 0.07968644052743912\n",
      "Epoch 24135/30000 Training Loss: 0.053571660071611404\n",
      "Epoch 24136/30000 Training Loss: 0.06121189519762993\n",
      "Epoch 24137/30000 Training Loss: 0.0855880081653595\n",
      "Epoch 24138/30000 Training Loss: 0.07405108958482742\n",
      "Epoch 24139/30000 Training Loss: 0.06551484018564224\n",
      "Epoch 24140/30000 Training Loss: 0.07885602861642838\n",
      "Epoch 24140/30000 Validation Loss: 0.06568630039691925\n",
      "Epoch 24141/30000 Training Loss: 0.07346288114786148\n",
      "Epoch 24142/30000 Training Loss: 0.059439580887556076\n",
      "Epoch 24143/30000 Training Loss: 0.07140430063009262\n",
      "Epoch 24144/30000 Training Loss: 0.04565548524260521\n",
      "Epoch 24145/30000 Training Loss: 0.07465662807226181\n",
      "Epoch 24146/30000 Training Loss: 0.07462915033102036\n",
      "Epoch 24147/30000 Training Loss: 0.06319347023963928\n",
      "Epoch 24148/30000 Training Loss: 0.06218963861465454\n",
      "Epoch 24149/30000 Training Loss: 0.07501727342605591\n",
      "Epoch 24150/30000 Training Loss: 0.07064240425825119\n",
      "Epoch 24150/30000 Validation Loss: 0.07450851052999496\n",
      "Epoch 24151/30000 Training Loss: 0.07317686825990677\n",
      "Epoch 24152/30000 Training Loss: 0.08779584616422653\n",
      "Epoch 24153/30000 Training Loss: 0.08261743187904358\n",
      "Epoch 24154/30000 Training Loss: 0.07540596276521683\n",
      "Epoch 24155/30000 Training Loss: 0.07127325981855392\n",
      "Epoch 24156/30000 Training Loss: 0.07275494933128357\n",
      "Epoch 24157/30000 Training Loss: 0.07526715844869614\n",
      "Epoch 24158/30000 Training Loss: 0.0747472420334816\n",
      "Epoch 24159/30000 Training Loss: 0.08426177501678467\n",
      "Epoch 24160/30000 Training Loss: 0.06381983309984207\n",
      "Epoch 24160/30000 Validation Loss: 0.07358944416046143\n",
      "Epoch 24161/30000 Training Loss: 0.05558069422841072\n",
      "Epoch 24162/30000 Training Loss: 0.06903531402349472\n",
      "Epoch 24163/30000 Training Loss: 0.06408872455358505\n",
      "Epoch 24164/30000 Training Loss: 0.07843288034200668\n",
      "Epoch 24165/30000 Training Loss: 0.06853893399238586\n",
      "Epoch 24166/30000 Training Loss: 0.07707185298204422\n",
      "Epoch 24167/30000 Training Loss: 0.06543763726949692\n",
      "Epoch 24168/30000 Training Loss: 0.08676602691411972\n",
      "Epoch 24169/30000 Training Loss: 0.075033999979496\n",
      "Epoch 24170/30000 Training Loss: 0.07375715672969818\n",
      "Epoch 24170/30000 Validation Loss: 0.07164984196424484\n",
      "Epoch 24171/30000 Training Loss: 0.046013932675123215\n",
      "Epoch 24172/30000 Training Loss: 0.07678251713514328\n",
      "Epoch 24173/30000 Training Loss: 0.06295070052146912\n",
      "Epoch 24174/30000 Training Loss: 0.06018717214465141\n",
      "Epoch 24175/30000 Training Loss: 0.09417331963777542\n",
      "Epoch 24176/30000 Training Loss: 0.0663682222366333\n",
      "Epoch 24177/30000 Training Loss: 0.06237956881523132\n",
      "Epoch 24178/30000 Training Loss: 0.08128061890602112\n",
      "Epoch 24179/30000 Training Loss: 0.07183387130498886\n",
      "Epoch 24180/30000 Training Loss: 0.06259426474571228\n",
      "Epoch 24180/30000 Validation Loss: 0.08607538789510727\n",
      "Epoch 24181/30000 Training Loss: 0.06341903656721115\n",
      "Epoch 24182/30000 Training Loss: 0.06174413487315178\n",
      "Epoch 24183/30000 Training Loss: 0.06003682315349579\n",
      "Epoch 24184/30000 Training Loss: 0.06519148498773575\n",
      "Epoch 24185/30000 Training Loss: 0.06807026267051697\n",
      "Epoch 24186/30000 Training Loss: 0.06682509183883667\n",
      "Epoch 24187/30000 Training Loss: 0.061823949217796326\n",
      "Epoch 24188/30000 Training Loss: 0.06062840297818184\n",
      "Epoch 24189/30000 Training Loss: 0.06563380360603333\n",
      "Epoch 24190/30000 Training Loss: 0.0703626200556755\n",
      "Epoch 24190/30000 Validation Loss: 0.08026237040758133\n",
      "Epoch 24191/30000 Training Loss: 0.06751564890146255\n",
      "Epoch 24192/30000 Training Loss: 0.07960955053567886\n",
      "Epoch 24193/30000 Training Loss: 0.07947173714637756\n",
      "Epoch 24194/30000 Training Loss: 0.05976201966404915\n",
      "Epoch 24195/30000 Training Loss: 0.0769479051232338\n",
      "Epoch 24196/30000 Training Loss: 0.0640745460987091\n",
      "Epoch 24197/30000 Training Loss: 0.06752390414476395\n",
      "Epoch 24198/30000 Training Loss: 0.07951933890581131\n",
      "Epoch 24199/30000 Training Loss: 0.05978582799434662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24200/30000 Training Loss: 0.06519042700529099\n",
      "Epoch 24200/30000 Validation Loss: 0.05831587314605713\n",
      "Epoch 24201/30000 Training Loss: 0.0791977047920227\n",
      "Epoch 24202/30000 Training Loss: 0.07830092310905457\n",
      "Epoch 24203/30000 Training Loss: 0.07380268722772598\n",
      "Epoch 24204/30000 Training Loss: 0.060161858797073364\n",
      "Epoch 24205/30000 Training Loss: 0.0740768238902092\n",
      "Epoch 24206/30000 Training Loss: 0.07926542311906815\n",
      "Epoch 24207/30000 Training Loss: 0.06962139159440994\n",
      "Epoch 24208/30000 Training Loss: 0.0777960792183876\n",
      "Epoch 24209/30000 Training Loss: 0.06346967816352844\n",
      "Epoch 24210/30000 Training Loss: 0.06402647495269775\n",
      "Epoch 24210/30000 Validation Loss: 0.06345932930707932\n",
      "Epoch 24211/30000 Training Loss: 0.05617281794548035\n",
      "Epoch 24212/30000 Training Loss: 0.05486191809177399\n",
      "Epoch 24213/30000 Training Loss: 0.08140923827886581\n",
      "Epoch 24214/30000 Training Loss: 0.06965450197458267\n",
      "Epoch 24215/30000 Training Loss: 0.05887672305107117\n",
      "Epoch 24216/30000 Training Loss: 0.06834588199853897\n",
      "Epoch 24217/30000 Training Loss: 0.07184891402721405\n",
      "Epoch 24218/30000 Training Loss: 0.06730794161558151\n",
      "Epoch 24219/30000 Training Loss: 0.07115349918603897\n",
      "Epoch 24220/30000 Training Loss: 0.0898166298866272\n",
      "Epoch 24220/30000 Validation Loss: 0.07354318350553513\n",
      "Epoch 24221/30000 Training Loss: 0.06888540834188461\n",
      "Epoch 24222/30000 Training Loss: 0.04914345219731331\n",
      "Epoch 24223/30000 Training Loss: 0.06761320680379868\n",
      "Epoch 24224/30000 Training Loss: 0.0752357468008995\n",
      "Epoch 24225/30000 Training Loss: 0.05940471962094307\n",
      "Epoch 24226/30000 Training Loss: 0.07456333190202713\n",
      "Epoch 24227/30000 Training Loss: 0.0894104614853859\n",
      "Epoch 24228/30000 Training Loss: 0.07228585332632065\n",
      "Epoch 24229/30000 Training Loss: 0.07158634811639786\n",
      "Epoch 24230/30000 Training Loss: 0.07682348042726517\n",
      "Epoch 24230/30000 Validation Loss: 0.06334996223449707\n",
      "Epoch 24231/30000 Training Loss: 0.05543975904583931\n",
      "Epoch 24232/30000 Training Loss: 0.08577871322631836\n",
      "Epoch 24233/30000 Training Loss: 0.05957034230232239\n",
      "Epoch 24234/30000 Training Loss: 0.060873616486787796\n",
      "Epoch 24235/30000 Training Loss: 0.060090571641922\n",
      "Epoch 24236/30000 Training Loss: 0.07272914052009583\n",
      "Epoch 24237/30000 Training Loss: 0.06912849098443985\n",
      "Epoch 24238/30000 Training Loss: 0.0629701241850853\n",
      "Epoch 24239/30000 Training Loss: 0.06225739046931267\n",
      "Epoch 24240/30000 Training Loss: 0.060417309403419495\n",
      "Epoch 24240/30000 Validation Loss: 0.07226254791021347\n",
      "Epoch 24241/30000 Training Loss: 0.07076919078826904\n",
      "Epoch 24242/30000 Training Loss: 0.0671052634716034\n",
      "Epoch 24243/30000 Training Loss: 0.06525949388742447\n",
      "Epoch 24244/30000 Training Loss: 0.054933417588472366\n",
      "Epoch 24245/30000 Training Loss: 0.07392953336238861\n",
      "Epoch 24246/30000 Training Loss: 0.06172521412372589\n",
      "Epoch 24247/30000 Training Loss: 0.08068645745515823\n",
      "Epoch 24248/30000 Training Loss: 0.06493774801492691\n",
      "Epoch 24249/30000 Training Loss: 0.0808907151222229\n",
      "Epoch 24250/30000 Training Loss: 0.06449737399816513\n",
      "Epoch 24250/30000 Validation Loss: 0.07851093262434006\n",
      "Epoch 24251/30000 Training Loss: 0.09060674905776978\n",
      "Epoch 24252/30000 Training Loss: 0.06646024435758591\n",
      "Epoch 24253/30000 Training Loss: 0.07186076790094376\n",
      "Epoch 24254/30000 Training Loss: 0.09788557142019272\n",
      "Epoch 24255/30000 Training Loss: 0.06308499723672867\n",
      "Epoch 24256/30000 Training Loss: 0.07099501043558121\n",
      "Epoch 24257/30000 Training Loss: 0.06934946030378342\n",
      "Epoch 24258/30000 Training Loss: 0.05002487823367119\n",
      "Epoch 24259/30000 Training Loss: 0.0610513873398304\n",
      "Epoch 24260/30000 Training Loss: 0.08495786041021347\n",
      "Epoch 24260/30000 Validation Loss: 0.0598919503390789\n",
      "Epoch 24261/30000 Training Loss: 0.05936955288052559\n",
      "Epoch 24262/30000 Training Loss: 0.06883490085601807\n",
      "Epoch 24263/30000 Training Loss: 0.07839284092187881\n",
      "Epoch 24264/30000 Training Loss: 0.06394129246473312\n",
      "Epoch 24265/30000 Training Loss: 0.07033547013998032\n",
      "Epoch 24266/30000 Training Loss: 0.050963785499334335\n",
      "Epoch 24267/30000 Training Loss: 0.09659517556428909\n",
      "Epoch 24268/30000 Training Loss: 0.05801903083920479\n",
      "Epoch 24269/30000 Training Loss: 0.06827735155820847\n",
      "Epoch 24270/30000 Training Loss: 0.07343355566263199\n",
      "Epoch 24270/30000 Validation Loss: 0.06677796691656113\n",
      "Epoch 24271/30000 Training Loss: 0.06037900969386101\n",
      "Epoch 24272/30000 Training Loss: 0.07178945094347\n",
      "Epoch 24273/30000 Training Loss: 0.07231131196022034\n",
      "Epoch 24274/30000 Training Loss: 0.06927961111068726\n",
      "Epoch 24275/30000 Training Loss: 0.059930503368377686\n",
      "Epoch 24276/30000 Training Loss: 0.07796907424926758\n",
      "Epoch 24277/30000 Training Loss: 0.0634273812174797\n",
      "Epoch 24278/30000 Training Loss: 0.057059068232774734\n",
      "Epoch 24279/30000 Training Loss: 0.06172652170062065\n",
      "Epoch 24280/30000 Training Loss: 0.06065252423286438\n",
      "Epoch 24280/30000 Validation Loss: 0.0704607143998146\n",
      "Epoch 24281/30000 Training Loss: 0.06049386039376259\n",
      "Epoch 24282/30000 Training Loss: 0.061972517520189285\n",
      "Epoch 24283/30000 Training Loss: 0.08299753814935684\n",
      "Epoch 24284/30000 Training Loss: 0.06676630675792694\n",
      "Epoch 24285/30000 Training Loss: 0.0796918049454689\n",
      "Epoch 24286/30000 Training Loss: 0.0686311349272728\n",
      "Epoch 24287/30000 Training Loss: 0.08885538578033447\n",
      "Epoch 24288/30000 Training Loss: 0.07454141229391098\n",
      "Epoch 24289/30000 Training Loss: 0.07018782943487167\n",
      "Epoch 24290/30000 Training Loss: 0.08565270900726318\n",
      "Epoch 24290/30000 Validation Loss: 0.08229556679725647\n",
      "Epoch 24291/30000 Training Loss: 0.07590068131685257\n",
      "Epoch 24292/30000 Training Loss: 0.07949281483888626\n",
      "Epoch 24293/30000 Training Loss: 0.054261211305856705\n",
      "Epoch 24294/30000 Training Loss: 0.07560934871435165\n",
      "Epoch 24295/30000 Training Loss: 0.07629669457674026\n",
      "Epoch 24296/30000 Training Loss: 0.056164950132369995\n",
      "Epoch 24297/30000 Training Loss: 0.07008419185876846\n",
      "Epoch 24298/30000 Training Loss: 0.0707719475030899\n",
      "Epoch 24299/30000 Training Loss: 0.058049142360687256\n",
      "Epoch 24300/30000 Training Loss: 0.08356929570436478\n",
      "Epoch 24300/30000 Validation Loss: 0.07349731773138046\n",
      "Epoch 24301/30000 Training Loss: 0.06985987722873688\n",
      "Epoch 24302/30000 Training Loss: 0.06552130728960037\n",
      "Epoch 24303/30000 Training Loss: 0.06993845850229263\n",
      "Epoch 24304/30000 Training Loss: 0.0663314238190651\n",
      "Epoch 24305/30000 Training Loss: 0.07219585031270981\n",
      "Epoch 24306/30000 Training Loss: 0.06797675043344498\n",
      "Epoch 24307/30000 Training Loss: 0.06575723737478256\n",
      "Epoch 24308/30000 Training Loss: 0.07820896059274673\n",
      "Epoch 24309/30000 Training Loss: 0.06327193975448608\n",
      "Epoch 24310/30000 Training Loss: 0.07365290075540543\n",
      "Epoch 24310/30000 Validation Loss: 0.06775904446840286\n",
      "Epoch 24311/30000 Training Loss: 0.07728391140699387\n",
      "Epoch 24312/30000 Training Loss: 0.06357037276029587\n",
      "Epoch 24313/30000 Training Loss: 0.06302804499864578\n",
      "Epoch 24314/30000 Training Loss: 0.09344150871038437\n",
      "Epoch 24315/30000 Training Loss: 0.06772986054420471\n",
      "Epoch 24316/30000 Training Loss: 0.0527133047580719\n",
      "Epoch 24317/30000 Training Loss: 0.06241144239902496\n",
      "Epoch 24318/30000 Training Loss: 0.05268624797463417\n",
      "Epoch 24319/30000 Training Loss: 0.0697561576962471\n",
      "Epoch 24320/30000 Training Loss: 0.07175201922655106\n",
      "Epoch 24320/30000 Validation Loss: 0.06474880874156952\n",
      "Epoch 24321/30000 Training Loss: 0.06465675681829453\n",
      "Epoch 24322/30000 Training Loss: 0.06882632523775101\n",
      "Epoch 24323/30000 Training Loss: 0.08090832084417343\n",
      "Epoch 24324/30000 Training Loss: 0.06667432934045792\n",
      "Epoch 24325/30000 Training Loss: 0.05909924581646919\n",
      "Epoch 24326/30000 Training Loss: 0.07130204886198044\n",
      "Epoch 24327/30000 Training Loss: 0.05230331793427467\n",
      "Epoch 24328/30000 Training Loss: 0.08766833692789078\n",
      "Epoch 24329/30000 Training Loss: 0.0672956183552742\n",
      "Epoch 24330/30000 Training Loss: 0.07297051697969437\n",
      "Epoch 24330/30000 Validation Loss: 0.08108177781105042\n",
      "Epoch 24331/30000 Training Loss: 0.07502282410860062\n",
      "Epoch 24332/30000 Training Loss: 0.06634611636400223\n",
      "Epoch 24333/30000 Training Loss: 0.06601588428020477\n",
      "Epoch 24334/30000 Training Loss: 0.07000339776277542\n",
      "Epoch 24335/30000 Training Loss: 0.07859388738870621\n",
      "Epoch 24336/30000 Training Loss: 0.08706124871969223\n",
      "Epoch 24337/30000 Training Loss: 0.06956005096435547\n",
      "Epoch 24338/30000 Training Loss: 0.07397536188364029\n",
      "Epoch 24339/30000 Training Loss: 0.08389031887054443\n",
      "Epoch 24340/30000 Training Loss: 0.0761680081486702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24340/30000 Validation Loss: 0.0643078163266182\n",
      "Epoch 24341/30000 Training Loss: 0.06109576299786568\n",
      "Epoch 24342/30000 Training Loss: 0.0787554606795311\n",
      "Epoch 24343/30000 Training Loss: 0.06024011969566345\n",
      "Epoch 24344/30000 Training Loss: 0.07393565773963928\n",
      "Epoch 24345/30000 Training Loss: 0.09272197633981705\n",
      "Epoch 24346/30000 Training Loss: 0.06570649147033691\n",
      "Epoch 24347/30000 Training Loss: 0.0693497285246849\n",
      "Epoch 24348/30000 Training Loss: 0.0784347727894783\n",
      "Epoch 24349/30000 Training Loss: 0.07082491368055344\n",
      "Epoch 24350/30000 Training Loss: 0.04631233215332031\n",
      "Epoch 24350/30000 Validation Loss: 0.0736660286784172\n",
      "Epoch 24351/30000 Training Loss: 0.07441810518503189\n",
      "Epoch 24352/30000 Training Loss: 0.059407781809568405\n",
      "Epoch 24353/30000 Training Loss: 0.06937519460916519\n",
      "Epoch 24354/30000 Training Loss: 0.06851771473884583\n",
      "Epoch 24355/30000 Training Loss: 0.06424065679311752\n",
      "Epoch 24356/30000 Training Loss: 0.08775168657302856\n",
      "Epoch 24357/30000 Training Loss: 0.07649609446525574\n",
      "Epoch 24358/30000 Training Loss: 0.06106661260128021\n",
      "Epoch 24359/30000 Training Loss: 0.07595498114824295\n",
      "Epoch 24360/30000 Training Loss: 0.06315606832504272\n",
      "Epoch 24360/30000 Validation Loss: 0.0599680095911026\n",
      "Epoch 24361/30000 Training Loss: 0.0648110881447792\n",
      "Epoch 24362/30000 Training Loss: 0.06438673287630081\n",
      "Epoch 24363/30000 Training Loss: 0.07370007783174515\n",
      "Epoch 24364/30000 Training Loss: 0.07579220831394196\n",
      "Epoch 24365/30000 Training Loss: 0.06829893589019775\n",
      "Epoch 24366/30000 Training Loss: 0.06770328432321548\n",
      "Epoch 24367/30000 Training Loss: 0.06988102942705154\n",
      "Epoch 24368/30000 Training Loss: 0.06068691611289978\n",
      "Epoch 24369/30000 Training Loss: 0.06701869517564774\n",
      "Epoch 24370/30000 Training Loss: 0.07669345289468765\n",
      "Epoch 24370/30000 Validation Loss: 0.06947571784257889\n",
      "Epoch 24371/30000 Training Loss: 0.06747671961784363\n",
      "Epoch 24372/30000 Training Loss: 0.0829685628414154\n",
      "Epoch 24373/30000 Training Loss: 0.06150801479816437\n",
      "Epoch 24374/30000 Training Loss: 0.05741089582443237\n",
      "Epoch 24375/30000 Training Loss: 0.07323982566595078\n",
      "Epoch 24376/30000 Training Loss: 0.05467526987195015\n",
      "Epoch 24377/30000 Training Loss: 0.07184714823961258\n",
      "Epoch 24378/30000 Training Loss: 0.05729271098971367\n",
      "Epoch 24379/30000 Training Loss: 0.08735015988349915\n",
      "Epoch 24380/30000 Training Loss: 0.07918500155210495\n",
      "Epoch 24380/30000 Validation Loss: 0.054351165890693665\n",
      "Epoch 24381/30000 Training Loss: 0.07526451349258423\n",
      "Epoch 24382/30000 Training Loss: 0.07329241186380386\n",
      "Epoch 24383/30000 Training Loss: 0.06372755020856857\n",
      "Epoch 24384/30000 Training Loss: 0.07152745127677917\n",
      "Epoch 24385/30000 Training Loss: 0.06329121440649033\n",
      "Epoch 24386/30000 Training Loss: 0.061721038073301315\n",
      "Epoch 24387/30000 Training Loss: 0.056642889976501465\n",
      "Epoch 24388/30000 Training Loss: 0.07092610001564026\n",
      "Epoch 24389/30000 Training Loss: 0.07736404985189438\n",
      "Epoch 24390/30000 Training Loss: 0.056631192564964294\n",
      "Epoch 24390/30000 Validation Loss: 0.09008719772100449\n",
      "Epoch 24391/30000 Training Loss: 0.06704817712306976\n",
      "Epoch 24392/30000 Training Loss: 0.06529665738344193\n",
      "Epoch 24393/30000 Training Loss: 0.059467073529958725\n",
      "Epoch 24394/30000 Training Loss: 0.07073252648115158\n",
      "Epoch 24395/30000 Training Loss: 0.052401456981897354\n",
      "Epoch 24396/30000 Training Loss: 0.053968895226716995\n",
      "Epoch 24397/30000 Training Loss: 0.06371930986642838\n",
      "Epoch 24398/30000 Training Loss: 0.05980619788169861\n",
      "Epoch 24399/30000 Training Loss: 0.08111534267663956\n",
      "Epoch 24400/30000 Training Loss: 0.05465059354901314\n",
      "Epoch 24400/30000 Validation Loss: 0.04811494052410126\n",
      "Epoch 24401/30000 Training Loss: 0.07399368286132812\n",
      "Epoch 24402/30000 Training Loss: 0.051495518535375595\n",
      "Epoch 24403/30000 Training Loss: 0.059334322810173035\n",
      "Epoch 24404/30000 Training Loss: 0.07441557943820953\n",
      "Epoch 24405/30000 Training Loss: 0.07453381270170212\n",
      "Epoch 24406/30000 Training Loss: 0.07056397944688797\n",
      "Epoch 24407/30000 Training Loss: 0.08305399864912033\n",
      "Epoch 24408/30000 Training Loss: 0.061761319637298584\n",
      "Epoch 24409/30000 Training Loss: 0.07223272323608398\n",
      "Epoch 24410/30000 Training Loss: 0.07498852163553238\n",
      "Epoch 24410/30000 Validation Loss: 0.06081893667578697\n",
      "Epoch 24411/30000 Training Loss: 0.07731115072965622\n",
      "Epoch 24412/30000 Training Loss: 0.06581758707761765\n",
      "Epoch 24413/30000 Training Loss: 0.07617884129285812\n",
      "Epoch 24414/30000 Training Loss: 0.05128978565335274\n",
      "Epoch 24415/30000 Training Loss: 0.09643087536096573\n",
      "Epoch 24416/30000 Training Loss: 0.0637759268283844\n",
      "Epoch 24417/30000 Training Loss: 0.05770307406783104\n",
      "Epoch 24418/30000 Training Loss: 0.07118216156959534\n",
      "Epoch 24419/30000 Training Loss: 0.05715828016400337\n",
      "Epoch 24420/30000 Training Loss: 0.06515739113092422\n",
      "Epoch 24420/30000 Validation Loss: 0.07527004927396774\n",
      "Epoch 24421/30000 Training Loss: 0.06641330569982529\n",
      "Epoch 24422/30000 Training Loss: 0.0806461051106453\n",
      "Epoch 24423/30000 Training Loss: 0.08805481344461441\n",
      "Epoch 24424/30000 Training Loss: 0.07522889226675034\n",
      "Epoch 24425/30000 Training Loss: 0.07761842012405396\n",
      "Epoch 24426/30000 Training Loss: 0.07723888754844666\n",
      "Epoch 24427/30000 Training Loss: 0.062341123819351196\n",
      "Epoch 24428/30000 Training Loss: 0.08342338353395462\n",
      "Epoch 24429/30000 Training Loss: 0.07738801836967468\n",
      "Epoch 24430/30000 Training Loss: 0.07016926258802414\n",
      "Epoch 24430/30000 Validation Loss: 0.08420329540967941\n",
      "Epoch 24431/30000 Training Loss: 0.06740355491638184\n",
      "Epoch 24432/30000 Training Loss: 0.06295370310544968\n",
      "Epoch 24433/30000 Training Loss: 0.05915432050824165\n",
      "Epoch 24434/30000 Training Loss: 0.06739788502454758\n",
      "Epoch 24435/30000 Training Loss: 0.07486652582883835\n",
      "Epoch 24436/30000 Training Loss: 0.07838798314332962\n",
      "Epoch 24437/30000 Training Loss: 0.06586620211601257\n",
      "Epoch 24438/30000 Training Loss: 0.07497261464595795\n",
      "Epoch 24439/30000 Training Loss: 0.06249654293060303\n",
      "Epoch 24440/30000 Training Loss: 0.05949375033378601\n",
      "Epoch 24440/30000 Validation Loss: 0.07229865342378616\n",
      "Epoch 24441/30000 Training Loss: 0.08333083242177963\n",
      "Epoch 24442/30000 Training Loss: 0.06579471379518509\n",
      "Epoch 24443/30000 Training Loss: 0.07668209820985794\n",
      "Epoch 24444/30000 Training Loss: 0.0613425076007843\n",
      "Epoch 24445/30000 Training Loss: 0.07336612790822983\n",
      "Epoch 24446/30000 Training Loss: 0.060029324144124985\n",
      "Epoch 24447/30000 Training Loss: 0.0747671127319336\n",
      "Epoch 24448/30000 Training Loss: 0.07269341498613358\n",
      "Epoch 24449/30000 Training Loss: 0.07987646758556366\n",
      "Epoch 24450/30000 Training Loss: 0.05626219883561134\n",
      "Epoch 24450/30000 Validation Loss: 0.060903847217559814\n",
      "Epoch 24451/30000 Training Loss: 0.058961838483810425\n",
      "Epoch 24452/30000 Training Loss: 0.069305419921875\n",
      "Epoch 24453/30000 Training Loss: 0.06520354002714157\n",
      "Epoch 24454/30000 Training Loss: 0.061304088681936264\n",
      "Epoch 24455/30000 Training Loss: 0.07238759845495224\n",
      "Epoch 24456/30000 Training Loss: 0.06277164816856384\n",
      "Epoch 24457/30000 Training Loss: 0.05801859498023987\n",
      "Epoch 24458/30000 Training Loss: 0.07378973811864853\n",
      "Epoch 24459/30000 Training Loss: 0.0687350407242775\n",
      "Epoch 24460/30000 Training Loss: 0.07730109989643097\n",
      "Epoch 24460/30000 Validation Loss: 0.07950597256422043\n",
      "Epoch 24461/30000 Training Loss: 0.0665193498134613\n",
      "Epoch 24462/30000 Training Loss: 0.06071772053837776\n",
      "Epoch 24463/30000 Training Loss: 0.06341785192489624\n",
      "Epoch 24464/30000 Training Loss: 0.0615430511534214\n",
      "Epoch 24465/30000 Training Loss: 0.07435644418001175\n",
      "Epoch 24466/30000 Training Loss: 0.061592038720846176\n",
      "Epoch 24467/30000 Training Loss: 0.05116482079029083\n",
      "Epoch 24468/30000 Training Loss: 0.07077804952859879\n",
      "Epoch 24469/30000 Training Loss: 0.07536300271749496\n",
      "Epoch 24470/30000 Training Loss: 0.06435561180114746\n",
      "Epoch 24470/30000 Validation Loss: 0.060088083148002625\n",
      "Epoch 24471/30000 Training Loss: 0.0535278357565403\n",
      "Epoch 24472/30000 Training Loss: 0.06887953728437424\n",
      "Epoch 24473/30000 Training Loss: 0.052759215235710144\n",
      "Epoch 24474/30000 Training Loss: 0.05961312726140022\n",
      "Epoch 24475/30000 Training Loss: 0.07180904597043991\n",
      "Epoch 24476/30000 Training Loss: 0.06418636441230774\n",
      "Epoch 24477/30000 Training Loss: 0.06863116472959518\n",
      "Epoch 24478/30000 Training Loss: 0.07246346026659012\n",
      "Epoch 24479/30000 Training Loss: 0.06901638954877853\n",
      "Epoch 24480/30000 Training Loss: 0.0681002214550972\n",
      "Epoch 24480/30000 Validation Loss: 0.07520698755979538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24481/30000 Training Loss: 0.0693478137254715\n",
      "Epoch 24482/30000 Training Loss: 0.054251778870821\n",
      "Epoch 24483/30000 Training Loss: 0.06804422289133072\n",
      "Epoch 24484/30000 Training Loss: 0.08790820837020874\n",
      "Epoch 24485/30000 Training Loss: 0.0698128417134285\n",
      "Epoch 24486/30000 Training Loss: 0.05562340095639229\n",
      "Epoch 24487/30000 Training Loss: 0.0706644281744957\n",
      "Epoch 24488/30000 Training Loss: 0.07217711210250854\n",
      "Epoch 24489/30000 Training Loss: 0.07190221548080444\n",
      "Epoch 24490/30000 Training Loss: 0.09185093641281128\n",
      "Epoch 24490/30000 Validation Loss: 0.06880995631217957\n",
      "Epoch 24491/30000 Training Loss: 0.07636778801679611\n",
      "Epoch 24492/30000 Training Loss: 0.05592117831110954\n",
      "Epoch 24493/30000 Training Loss: 0.06892391294240952\n",
      "Epoch 24494/30000 Training Loss: 0.0688168928027153\n",
      "Epoch 24495/30000 Training Loss: 0.06756287813186646\n",
      "Epoch 24496/30000 Training Loss: 0.08763426542282104\n",
      "Epoch 24497/30000 Training Loss: 0.09215684980154037\n",
      "Epoch 24498/30000 Training Loss: 0.061712294816970825\n",
      "Epoch 24499/30000 Training Loss: 0.08803359419107437\n",
      "Epoch 24500/30000 Training Loss: 0.08052802830934525\n",
      "Epoch 24500/30000 Validation Loss: 0.08444175124168396\n",
      "Epoch 24501/30000 Training Loss: 0.05510551854968071\n",
      "Epoch 24502/30000 Training Loss: 0.08552130311727524\n",
      "Epoch 24503/30000 Training Loss: 0.05507533624768257\n",
      "Epoch 24504/30000 Training Loss: 0.07696818560361862\n",
      "Epoch 24505/30000 Training Loss: 0.054977383464574814\n",
      "Epoch 24506/30000 Training Loss: 0.06490810215473175\n",
      "Epoch 24507/30000 Training Loss: 0.07553768903017044\n",
      "Epoch 24508/30000 Training Loss: 0.09115595370531082\n",
      "Epoch 24509/30000 Training Loss: 0.07398644834756851\n",
      "Epoch 24510/30000 Training Loss: 0.06399407237768173\n",
      "Epoch 24510/30000 Validation Loss: 0.06333822011947632\n",
      "Epoch 24511/30000 Training Loss: 0.07028902322053909\n",
      "Epoch 24512/30000 Training Loss: 0.09190445393323898\n",
      "Epoch 24513/30000 Training Loss: 0.09961684793233871\n",
      "Epoch 24514/30000 Training Loss: 0.06301643699407578\n",
      "Epoch 24515/30000 Training Loss: 0.071062833070755\n",
      "Epoch 24516/30000 Training Loss: 0.08327437192201614\n",
      "Epoch 24517/30000 Training Loss: 0.0786169245839119\n",
      "Epoch 24518/30000 Training Loss: 0.08221086859703064\n",
      "Epoch 24519/30000 Training Loss: 0.07876405119895935\n",
      "Epoch 24520/30000 Training Loss: 0.06834423542022705\n",
      "Epoch 24520/30000 Validation Loss: 0.0649401918053627\n",
      "Epoch 24521/30000 Training Loss: 0.0608421266078949\n",
      "Epoch 24522/30000 Training Loss: 0.0858096182346344\n",
      "Epoch 24523/30000 Training Loss: 0.07511097192764282\n",
      "Epoch 24524/30000 Training Loss: 0.08032549172639847\n",
      "Epoch 24525/30000 Training Loss: 0.06458867341279984\n",
      "Epoch 24526/30000 Training Loss: 0.05834190174937248\n",
      "Epoch 24527/30000 Training Loss: 0.07396727055311203\n",
      "Epoch 24528/30000 Training Loss: 0.06736260652542114\n",
      "Epoch 24529/30000 Training Loss: 0.07136830687522888\n",
      "Epoch 24530/30000 Training Loss: 0.06396644562482834\n",
      "Epoch 24530/30000 Validation Loss: 0.07608262449502945\n",
      "Epoch 24531/30000 Training Loss: 0.079072006046772\n",
      "Epoch 24532/30000 Training Loss: 0.07077271491289139\n",
      "Epoch 24533/30000 Training Loss: 0.06260152161121368\n",
      "Epoch 24534/30000 Training Loss: 0.06366056948900223\n",
      "Epoch 24535/30000 Training Loss: 0.0768093541264534\n",
      "Epoch 24536/30000 Training Loss: 0.09111535549163818\n",
      "Epoch 24537/30000 Training Loss: 0.09903762489557266\n",
      "Epoch 24538/30000 Training Loss: 0.07676627486944199\n",
      "Epoch 24539/30000 Training Loss: 0.059647370129823685\n",
      "Epoch 24540/30000 Training Loss: 0.07092653214931488\n",
      "Epoch 24540/30000 Validation Loss: 0.07806044071912766\n",
      "Epoch 24541/30000 Training Loss: 0.06023532152175903\n",
      "Epoch 24542/30000 Training Loss: 0.06367103010416031\n",
      "Epoch 24543/30000 Training Loss: 0.061935409903526306\n",
      "Epoch 24544/30000 Training Loss: 0.08946896344423294\n",
      "Epoch 24545/30000 Training Loss: 0.08688489347696304\n",
      "Epoch 24546/30000 Training Loss: 0.07272997498512268\n",
      "Epoch 24547/30000 Training Loss: 0.08031975477933884\n",
      "Epoch 24548/30000 Training Loss: 0.07113046199083328\n",
      "Epoch 24549/30000 Training Loss: 0.08259057253599167\n",
      "Epoch 24550/30000 Training Loss: 0.0804378092288971\n",
      "Epoch 24550/30000 Validation Loss: 0.06536679714918137\n",
      "Epoch 24551/30000 Training Loss: 0.08559510111808777\n",
      "Epoch 24552/30000 Training Loss: 0.06641819328069687\n",
      "Epoch 24553/30000 Training Loss: 0.06728795915842056\n",
      "Epoch 24554/30000 Training Loss: 0.07616642862558365\n",
      "Epoch 24555/30000 Training Loss: 0.07552134990692139\n",
      "Epoch 24556/30000 Training Loss: 0.06901571154594421\n",
      "Epoch 24557/30000 Training Loss: 0.06560292094945908\n",
      "Epoch 24558/30000 Training Loss: 0.05970299243927002\n",
      "Epoch 24559/30000 Training Loss: 0.061940956860780716\n",
      "Epoch 24560/30000 Training Loss: 0.07631335407495499\n",
      "Epoch 24560/30000 Validation Loss: 0.0641290619969368\n",
      "Epoch 24561/30000 Training Loss: 0.08278586715459824\n",
      "Epoch 24562/30000 Training Loss: 0.06976579129695892\n",
      "Epoch 24563/30000 Training Loss: 0.0754719004034996\n",
      "Epoch 24564/30000 Training Loss: 0.08367013931274414\n",
      "Epoch 24565/30000 Training Loss: 0.07945135235786438\n",
      "Epoch 24566/30000 Training Loss: 0.051465149968862534\n",
      "Epoch 24567/30000 Training Loss: 0.06517737358808517\n",
      "Epoch 24568/30000 Training Loss: 0.06621576100587845\n",
      "Epoch 24569/30000 Training Loss: 0.07701580971479416\n",
      "Epoch 24570/30000 Training Loss: 0.06830006837844849\n",
      "Epoch 24570/30000 Validation Loss: 0.056599389761686325\n",
      "Epoch 24571/30000 Training Loss: 0.07563575357198715\n",
      "Epoch 24572/30000 Training Loss: 0.07758699357509613\n",
      "Epoch 24573/30000 Training Loss: 0.06359771639108658\n",
      "Epoch 24574/30000 Training Loss: 0.048368941992521286\n",
      "Epoch 24575/30000 Training Loss: 0.08616867661476135\n",
      "Epoch 24576/30000 Training Loss: 0.05407136306166649\n",
      "Epoch 24577/30000 Training Loss: 0.0781092420220375\n",
      "Epoch 24578/30000 Training Loss: 0.06864511221647263\n",
      "Epoch 24579/30000 Training Loss: 0.08201294392347336\n",
      "Epoch 24580/30000 Training Loss: 0.05705638602375984\n",
      "Epoch 24580/30000 Validation Loss: 0.05791432783007622\n",
      "Epoch 24581/30000 Training Loss: 0.07073984295129776\n",
      "Epoch 24582/30000 Training Loss: 0.06383439898490906\n",
      "Epoch 24583/30000 Training Loss: 0.08138386905193329\n",
      "Epoch 24584/30000 Training Loss: 0.07332254201173782\n",
      "Epoch 24585/30000 Training Loss: 0.06686265766620636\n",
      "Epoch 24586/30000 Training Loss: 0.08521810919046402\n",
      "Epoch 24587/30000 Training Loss: 0.07528437674045563\n",
      "Epoch 24588/30000 Training Loss: 0.06667803972959518\n",
      "Epoch 24589/30000 Training Loss: 0.08730202168226242\n",
      "Epoch 24590/30000 Training Loss: 0.07803115993738174\n",
      "Epoch 24590/30000 Validation Loss: 0.08126659691333771\n",
      "Epoch 24591/30000 Training Loss: 0.08974971622228622\n",
      "Epoch 24592/30000 Training Loss: 0.08012247830629349\n",
      "Epoch 24593/30000 Training Loss: 0.07633452862501144\n",
      "Epoch 24594/30000 Training Loss: 0.07721533626317978\n",
      "Epoch 24595/30000 Training Loss: 0.07542043179273605\n",
      "Epoch 24596/30000 Training Loss: 0.06711918860673904\n",
      "Epoch 24597/30000 Training Loss: 0.0631958469748497\n",
      "Epoch 24598/30000 Training Loss: 0.08790100365877151\n",
      "Epoch 24599/30000 Training Loss: 0.07464434951543808\n",
      "Epoch 24600/30000 Training Loss: 0.0574144683778286\n",
      "Epoch 24600/30000 Validation Loss: 0.07039409130811691\n",
      "Epoch 24601/30000 Training Loss: 0.0806446447968483\n",
      "Epoch 24602/30000 Training Loss: 0.06868348270654678\n",
      "Epoch 24603/30000 Training Loss: 0.06405293196439743\n",
      "Epoch 24604/30000 Training Loss: 0.08015232533216476\n",
      "Epoch 24605/30000 Training Loss: 0.06972423195838928\n",
      "Epoch 24606/30000 Training Loss: 0.07113415002822876\n",
      "Epoch 24607/30000 Training Loss: 0.07243376970291138\n",
      "Epoch 24608/30000 Training Loss: 0.08660998940467834\n",
      "Epoch 24609/30000 Training Loss: 0.07649034261703491\n",
      "Epoch 24610/30000 Training Loss: 0.07004401832818985\n",
      "Epoch 24610/30000 Validation Loss: 0.08239882439374924\n",
      "Epoch 24611/30000 Training Loss: 0.07497722655534744\n",
      "Epoch 24612/30000 Training Loss: 0.07608349621295929\n",
      "Epoch 24613/30000 Training Loss: 0.09091826528310776\n",
      "Epoch 24614/30000 Training Loss: 0.06635887175798416\n",
      "Epoch 24615/30000 Training Loss: 0.05358054116368294\n",
      "Epoch 24616/30000 Training Loss: 0.057577114552259445\n",
      "Epoch 24617/30000 Training Loss: 0.06666315346956253\n",
      "Epoch 24618/30000 Training Loss: 0.08388965576887131\n",
      "Epoch 24619/30000 Training Loss: 0.06887972354888916\n",
      "Epoch 24620/30000 Training Loss: 0.08330629765987396\n",
      "Epoch 24620/30000 Validation Loss: 0.05694815516471863\n",
      "Epoch 24621/30000 Training Loss: 0.08505678176879883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24622/30000 Training Loss: 0.0577516108751297\n",
      "Epoch 24623/30000 Training Loss: 0.06896831840276718\n",
      "Epoch 24624/30000 Training Loss: 0.0769953727722168\n",
      "Epoch 24625/30000 Training Loss: 0.0765165314078331\n",
      "Epoch 24626/30000 Training Loss: 0.07429352402687073\n",
      "Epoch 24627/30000 Training Loss: 0.061050642281770706\n",
      "Epoch 24628/30000 Training Loss: 0.06344973295927048\n",
      "Epoch 24629/30000 Training Loss: 0.08273300528526306\n",
      "Epoch 24630/30000 Training Loss: 0.06954795122146606\n",
      "Epoch 24630/30000 Validation Loss: 0.07227969914674759\n",
      "Epoch 24631/30000 Training Loss: 0.08432245254516602\n",
      "Epoch 24632/30000 Training Loss: 0.07353037595748901\n",
      "Epoch 24633/30000 Training Loss: 0.08475691825151443\n",
      "Epoch 24634/30000 Training Loss: 0.06309370696544647\n",
      "Epoch 24635/30000 Training Loss: 0.06724514812231064\n",
      "Epoch 24636/30000 Training Loss: 0.0605362206697464\n",
      "Epoch 24637/30000 Training Loss: 0.07274410128593445\n",
      "Epoch 24638/30000 Training Loss: 0.058430638164281845\n",
      "Epoch 24639/30000 Training Loss: 0.0798959955573082\n",
      "Epoch 24640/30000 Training Loss: 0.07356545329093933\n",
      "Epoch 24640/30000 Validation Loss: 0.06982763856649399\n",
      "Epoch 24641/30000 Training Loss: 0.05396933853626251\n",
      "Epoch 24642/30000 Training Loss: 0.058385442942380905\n",
      "Epoch 24643/30000 Training Loss: 0.07691063731908798\n",
      "Epoch 24644/30000 Training Loss: 0.060301098972558975\n",
      "Epoch 24645/30000 Training Loss: 0.07122030109167099\n",
      "Epoch 24646/30000 Training Loss: 0.062213629484176636\n",
      "Epoch 24647/30000 Training Loss: 0.0711294487118721\n",
      "Epoch 24648/30000 Training Loss: 0.07243385165929794\n",
      "Epoch 24649/30000 Training Loss: 0.07084909081459045\n",
      "Epoch 24650/30000 Training Loss: 0.07584269344806671\n",
      "Epoch 24650/30000 Validation Loss: 0.08030390739440918\n",
      "Epoch 24651/30000 Training Loss: 0.0690508708357811\n",
      "Epoch 24652/30000 Training Loss: 0.07394282519817352\n",
      "Epoch 24653/30000 Training Loss: 0.06141563132405281\n",
      "Epoch 24654/30000 Training Loss: 0.07028016448020935\n",
      "Epoch 24655/30000 Training Loss: 0.0683496966958046\n",
      "Epoch 24656/30000 Training Loss: 0.06713899970054626\n",
      "Epoch 24657/30000 Training Loss: 0.08452842384576797\n",
      "Epoch 24658/30000 Training Loss: 0.05499972403049469\n",
      "Epoch 24659/30000 Training Loss: 0.05935308709740639\n",
      "Epoch 24660/30000 Training Loss: 0.08190324157476425\n",
      "Epoch 24660/30000 Validation Loss: 0.07883944362401962\n",
      "Epoch 24661/30000 Training Loss: 0.05691629648208618\n",
      "Epoch 24662/30000 Training Loss: 0.05627962574362755\n",
      "Epoch 24663/30000 Training Loss: 0.08406505733728409\n",
      "Epoch 24664/30000 Training Loss: 0.08031698316335678\n",
      "Epoch 24665/30000 Training Loss: 0.06418471783399582\n",
      "Epoch 24666/30000 Training Loss: 0.07086453586816788\n",
      "Epoch 24667/30000 Training Loss: 0.08353026956319809\n",
      "Epoch 24668/30000 Training Loss: 0.052032727748155594\n",
      "Epoch 24669/30000 Training Loss: 0.07018939405679703\n",
      "Epoch 24670/30000 Training Loss: 0.061268400400877\n",
      "Epoch 24670/30000 Validation Loss: 0.082087941467762\n",
      "Epoch 24671/30000 Training Loss: 0.07192631810903549\n",
      "Epoch 24672/30000 Training Loss: 0.05375738441944122\n",
      "Epoch 24673/30000 Training Loss: 0.06964841485023499\n",
      "Epoch 24674/30000 Training Loss: 0.08083011955022812\n",
      "Epoch 24675/30000 Training Loss: 0.058431174606084824\n",
      "Epoch 24676/30000 Training Loss: 0.054269228130578995\n",
      "Epoch 24677/30000 Training Loss: 0.05855380371212959\n",
      "Epoch 24678/30000 Training Loss: 0.06703691184520721\n",
      "Epoch 24679/30000 Training Loss: 0.06509682536125183\n",
      "Epoch 24680/30000 Training Loss: 0.07950941473245621\n",
      "Epoch 24680/30000 Validation Loss: 0.08370248228311539\n",
      "Epoch 24681/30000 Training Loss: 0.05999327823519707\n",
      "Epoch 24682/30000 Training Loss: 0.0731169581413269\n",
      "Epoch 24683/30000 Training Loss: 0.08295191824436188\n",
      "Epoch 24684/30000 Training Loss: 0.06163580343127251\n",
      "Epoch 24685/30000 Training Loss: 0.07957962155342102\n",
      "Epoch 24686/30000 Training Loss: 0.05928381159901619\n",
      "Epoch 24687/30000 Training Loss: 0.07245967537164688\n",
      "Epoch 24688/30000 Training Loss: 0.06041516736149788\n",
      "Epoch 24689/30000 Training Loss: 0.07675575464963913\n",
      "Epoch 24690/30000 Training Loss: 0.0828472301363945\n",
      "Epoch 24690/30000 Validation Loss: 0.07329384237527847\n",
      "Epoch 24691/30000 Training Loss: 0.08110534399747849\n",
      "Epoch 24692/30000 Training Loss: 0.06331589818000793\n",
      "Epoch 24693/30000 Training Loss: 0.08086531609296799\n",
      "Epoch 24694/30000 Training Loss: 0.07010146230459213\n",
      "Epoch 24695/30000 Training Loss: 0.0731276199221611\n",
      "Epoch 24696/30000 Training Loss: 0.07817033678293228\n",
      "Epoch 24697/30000 Training Loss: 0.05694201588630676\n",
      "Epoch 24698/30000 Training Loss: 0.05871947482228279\n",
      "Epoch 24699/30000 Training Loss: 0.056104231625795364\n",
      "Epoch 24700/30000 Training Loss: 0.06546557694673538\n",
      "Epoch 24700/30000 Validation Loss: 0.06364919990301132\n",
      "Epoch 24701/30000 Training Loss: 0.07917026430368423\n",
      "Epoch 24702/30000 Training Loss: 0.0802324190735817\n",
      "Epoch 24703/30000 Training Loss: 0.052133187651634216\n",
      "Epoch 24704/30000 Training Loss: 0.06822564452886581\n",
      "Epoch 24705/30000 Training Loss: 0.06147050857543945\n",
      "Epoch 24706/30000 Training Loss: 0.06349136680364609\n",
      "Epoch 24707/30000 Training Loss: 0.06461400538682938\n",
      "Epoch 24708/30000 Training Loss: 0.05582244321703911\n",
      "Epoch 24709/30000 Training Loss: 0.05897912383079529\n",
      "Epoch 24710/30000 Training Loss: 0.06484278291463852\n",
      "Epoch 24710/30000 Validation Loss: 0.07325487583875656\n",
      "Epoch 24711/30000 Training Loss: 0.08920162916183472\n",
      "Epoch 24712/30000 Training Loss: 0.06170549616217613\n",
      "Epoch 24713/30000 Training Loss: 0.08675848692655563\n",
      "Epoch 24714/30000 Training Loss: 0.07680798321962357\n",
      "Epoch 24715/30000 Training Loss: 0.06870933622121811\n",
      "Epoch 24716/30000 Training Loss: 0.07280978560447693\n",
      "Epoch 24717/30000 Training Loss: 0.06765028834342957\n",
      "Epoch 24718/30000 Training Loss: 0.05471387505531311\n",
      "Epoch 24719/30000 Training Loss: 0.0938665047287941\n",
      "Epoch 24720/30000 Training Loss: 0.07847344875335693\n",
      "Epoch 24720/30000 Validation Loss: 0.06661081314086914\n",
      "Epoch 24721/30000 Training Loss: 0.08717742562294006\n",
      "Epoch 24722/30000 Training Loss: 0.06119124963879585\n",
      "Epoch 24723/30000 Training Loss: 0.05009368062019348\n",
      "Epoch 24724/30000 Training Loss: 0.07505834847688675\n",
      "Epoch 24725/30000 Training Loss: 0.0659530982375145\n",
      "Epoch 24726/30000 Training Loss: 0.06482484191656113\n",
      "Epoch 24727/30000 Training Loss: 0.07225077599287033\n",
      "Epoch 24728/30000 Training Loss: 0.06934753805398941\n",
      "Epoch 24729/30000 Training Loss: 0.0703468918800354\n",
      "Epoch 24730/30000 Training Loss: 0.0798008143901825\n",
      "Epoch 24730/30000 Validation Loss: 0.07079747319221497\n",
      "Epoch 24731/30000 Training Loss: 0.07255954295396805\n",
      "Epoch 24732/30000 Training Loss: 0.07820969074964523\n",
      "Epoch 24733/30000 Training Loss: 0.07290469110012054\n",
      "Epoch 24734/30000 Training Loss: 0.07128307968378067\n",
      "Epoch 24735/30000 Training Loss: 0.07558730989694595\n",
      "Epoch 24736/30000 Training Loss: 0.08578958362340927\n",
      "Epoch 24737/30000 Training Loss: 0.079278863966465\n",
      "Epoch 24738/30000 Training Loss: 0.06986086815595627\n",
      "Epoch 24739/30000 Training Loss: 0.07512415945529938\n",
      "Epoch 24740/30000 Training Loss: 0.07270928472280502\n",
      "Epoch 24740/30000 Validation Loss: 0.06150231882929802\n",
      "Epoch 24741/30000 Training Loss: 0.053593557327985764\n",
      "Epoch 24742/30000 Training Loss: 0.06851165741682053\n",
      "Epoch 24743/30000 Training Loss: 0.06803199648857117\n",
      "Epoch 24744/30000 Training Loss: 0.07670844346284866\n",
      "Epoch 24745/30000 Training Loss: 0.08363965153694153\n",
      "Epoch 24746/30000 Training Loss: 0.07806143164634705\n",
      "Epoch 24747/30000 Training Loss: 0.06382777541875839\n",
      "Epoch 24748/30000 Training Loss: 0.08612867444753647\n",
      "Epoch 24749/30000 Training Loss: 0.0910617932677269\n",
      "Epoch 24750/30000 Training Loss: 0.0792742371559143\n",
      "Epoch 24750/30000 Validation Loss: 0.07519669085741043\n",
      "Epoch 24751/30000 Training Loss: 0.07045403867959976\n",
      "Epoch 24752/30000 Training Loss: 0.05480446293950081\n",
      "Epoch 24753/30000 Training Loss: 0.05803096666932106\n",
      "Epoch 24754/30000 Training Loss: 0.07503131777048111\n",
      "Epoch 24755/30000 Training Loss: 0.07415265589952469\n",
      "Epoch 24756/30000 Training Loss: 0.06070948764681816\n",
      "Epoch 24757/30000 Training Loss: 0.06980082392692566\n",
      "Epoch 24758/30000 Training Loss: 0.06373190879821777\n",
      "Epoch 24759/30000 Training Loss: 0.057573091238737106\n",
      "Epoch 24760/30000 Training Loss: 0.06243965029716492\n",
      "Epoch 24760/30000 Validation Loss: 0.08030404150485992\n",
      "Epoch 24761/30000 Training Loss: 0.07617664337158203\n",
      "Epoch 24762/30000 Training Loss: 0.08165130019187927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24763/30000 Training Loss: 0.06687160581350327\n",
      "Epoch 24764/30000 Training Loss: 0.05351991578936577\n",
      "Epoch 24765/30000 Training Loss: 0.06875085830688477\n",
      "Epoch 24766/30000 Training Loss: 0.08421777933835983\n",
      "Epoch 24767/30000 Training Loss: 0.05249403417110443\n",
      "Epoch 24768/30000 Training Loss: 0.0837266743183136\n",
      "Epoch 24769/30000 Training Loss: 0.0885998010635376\n",
      "Epoch 24770/30000 Training Loss: 0.07576528191566467\n",
      "Epoch 24770/30000 Validation Loss: 0.06611273437738419\n",
      "Epoch 24771/30000 Training Loss: 0.05495159700512886\n",
      "Epoch 24772/30000 Training Loss: 0.06164273992180824\n",
      "Epoch 24773/30000 Training Loss: 0.0696806088089943\n",
      "Epoch 24774/30000 Training Loss: 0.06201445683836937\n",
      "Epoch 24775/30000 Training Loss: 0.07702378183603287\n",
      "Epoch 24776/30000 Training Loss: 0.059103820472955704\n",
      "Epoch 24777/30000 Training Loss: 0.05799713730812073\n",
      "Epoch 24778/30000 Training Loss: 0.06185423955321312\n",
      "Epoch 24779/30000 Training Loss: 0.08434145897626877\n",
      "Epoch 24780/30000 Training Loss: 0.05544896796345711\n",
      "Epoch 24780/30000 Validation Loss: 0.07516501098871231\n",
      "Epoch 24781/30000 Training Loss: 0.07208399474620819\n",
      "Epoch 24782/30000 Training Loss: 0.07163477689027786\n",
      "Epoch 24783/30000 Training Loss: 0.10004901140928268\n",
      "Epoch 24784/30000 Training Loss: 0.06300225108861923\n",
      "Epoch 24785/30000 Training Loss: 0.05978613719344139\n",
      "Epoch 24786/30000 Training Loss: 0.10927381366491318\n",
      "Epoch 24787/30000 Training Loss: 0.06643108278512955\n",
      "Epoch 24788/30000 Training Loss: 0.07115649431943893\n",
      "Epoch 24789/30000 Training Loss: 0.06795913726091385\n",
      "Epoch 24790/30000 Training Loss: 0.06182476878166199\n",
      "Epoch 24790/30000 Validation Loss: 0.07519537955522537\n",
      "Epoch 24791/30000 Training Loss: 0.05446504056453705\n",
      "Epoch 24792/30000 Training Loss: 0.06525837630033493\n",
      "Epoch 24793/30000 Training Loss: 0.07738672196865082\n",
      "Epoch 24794/30000 Training Loss: 0.06265751272439957\n",
      "Epoch 24795/30000 Training Loss: 0.0555708110332489\n",
      "Epoch 24796/30000 Training Loss: 0.08506583422422409\n",
      "Epoch 24797/30000 Training Loss: 0.06299733370542526\n",
      "Epoch 24798/30000 Training Loss: 0.06905317306518555\n",
      "Epoch 24799/30000 Training Loss: 0.05946953222155571\n",
      "Epoch 24800/30000 Training Loss: 0.07250452786684036\n",
      "Epoch 24800/30000 Validation Loss: 0.06546180695295334\n",
      "Epoch 24801/30000 Training Loss: 0.06148817762732506\n",
      "Epoch 24802/30000 Training Loss: 0.05813418701291084\n",
      "Epoch 24803/30000 Training Loss: 0.07009047269821167\n",
      "Epoch 24804/30000 Training Loss: 0.06552320718765259\n",
      "Epoch 24805/30000 Training Loss: 0.07917685061693192\n",
      "Epoch 24806/30000 Training Loss: 0.05528661608695984\n",
      "Epoch 24807/30000 Training Loss: 0.055412787944078445\n",
      "Epoch 24808/30000 Training Loss: 0.08067015558481216\n",
      "Epoch 24809/30000 Training Loss: 0.06365406513214111\n",
      "Epoch 24810/30000 Training Loss: 0.07299431413412094\n",
      "Epoch 24810/30000 Validation Loss: 0.07405876368284225\n",
      "Epoch 24811/30000 Training Loss: 0.07602044194936752\n",
      "Epoch 24812/30000 Training Loss: 0.06068335846066475\n",
      "Epoch 24813/30000 Training Loss: 0.07478275895118713\n",
      "Epoch 24814/30000 Training Loss: 0.07593591511249542\n",
      "Epoch 24815/30000 Training Loss: 0.07305771112442017\n",
      "Epoch 24816/30000 Training Loss: 0.07673409581184387\n",
      "Epoch 24817/30000 Training Loss: 0.06196150183677673\n",
      "Epoch 24818/30000 Training Loss: 0.0676926001906395\n",
      "Epoch 24819/30000 Training Loss: 0.08042549341917038\n",
      "Epoch 24820/30000 Training Loss: 0.05386556684970856\n",
      "Epoch 24820/30000 Validation Loss: 0.06839869171380997\n",
      "Epoch 24821/30000 Training Loss: 0.04977002367377281\n",
      "Epoch 24822/30000 Training Loss: 0.07468942552804947\n",
      "Epoch 24823/30000 Training Loss: 0.0831199511885643\n",
      "Epoch 24824/30000 Training Loss: 0.07801951467990875\n",
      "Epoch 24825/30000 Training Loss: 0.0661114752292633\n",
      "Epoch 24826/30000 Training Loss: 0.07543324679136276\n",
      "Epoch 24827/30000 Training Loss: 0.07331795245409012\n",
      "Epoch 24828/30000 Training Loss: 0.06987544149160385\n",
      "Epoch 24829/30000 Training Loss: 0.07381457835435867\n",
      "Epoch 24830/30000 Training Loss: 0.08520915359258652\n",
      "Epoch 24830/30000 Validation Loss: 0.07527359575033188\n",
      "Epoch 24831/30000 Training Loss: 0.06112055107951164\n",
      "Epoch 24832/30000 Training Loss: 0.06348999589681625\n",
      "Epoch 24833/30000 Training Loss: 0.06886187195777893\n",
      "Epoch 24834/30000 Training Loss: 0.07048287987709045\n",
      "Epoch 24835/30000 Training Loss: 0.08599568158388138\n",
      "Epoch 24836/30000 Training Loss: 0.08225911855697632\n",
      "Epoch 24837/30000 Training Loss: 0.08481007814407349\n",
      "Epoch 24838/30000 Training Loss: 0.06136119365692139\n",
      "Epoch 24839/30000 Training Loss: 0.07068843394517899\n",
      "Epoch 24840/30000 Training Loss: 0.07239273935556412\n",
      "Epoch 24840/30000 Validation Loss: 0.0737089142203331\n",
      "Epoch 24841/30000 Training Loss: 0.07374277710914612\n",
      "Epoch 24842/30000 Training Loss: 0.06675467640161514\n",
      "Epoch 24843/30000 Training Loss: 0.07554320991039276\n",
      "Epoch 24844/30000 Training Loss: 0.06788618117570877\n",
      "Epoch 24845/30000 Training Loss: 0.08864299207925797\n",
      "Epoch 24846/30000 Training Loss: 0.058464910835027695\n",
      "Epoch 24847/30000 Training Loss: 0.0855480208992958\n",
      "Epoch 24848/30000 Training Loss: 0.07109245657920837\n",
      "Epoch 24849/30000 Training Loss: 0.06475938111543655\n",
      "Epoch 24850/30000 Training Loss: 0.07358264178037643\n",
      "Epoch 24850/30000 Validation Loss: 0.06560619920492172\n",
      "Epoch 24851/30000 Training Loss: 0.061600249260663986\n",
      "Epoch 24852/30000 Training Loss: 0.08766428381204605\n",
      "Epoch 24853/30000 Training Loss: 0.06683249026536942\n",
      "Epoch 24854/30000 Training Loss: 0.07441172748804092\n",
      "Epoch 24855/30000 Training Loss: 0.0714622288942337\n",
      "Epoch 24856/30000 Training Loss: 0.09466919302940369\n",
      "Epoch 24857/30000 Training Loss: 0.07022657245397568\n",
      "Epoch 24858/30000 Training Loss: 0.1019609272480011\n",
      "Epoch 24859/30000 Training Loss: 0.05647518113255501\n",
      "Epoch 24860/30000 Training Loss: 0.0631101131439209\n",
      "Epoch 24860/30000 Validation Loss: 0.0680258646607399\n",
      "Epoch 24861/30000 Training Loss: 0.06318102031946182\n",
      "Epoch 24862/30000 Training Loss: 0.06330500543117523\n",
      "Epoch 24863/30000 Training Loss: 0.07558991760015488\n",
      "Epoch 24864/30000 Training Loss: 0.0735810399055481\n",
      "Epoch 24865/30000 Training Loss: 0.05903911590576172\n",
      "Epoch 24866/30000 Training Loss: 0.06600725650787354\n",
      "Epoch 24867/30000 Training Loss: 0.06488622725009918\n",
      "Epoch 24868/30000 Training Loss: 0.06216558814048767\n",
      "Epoch 24869/30000 Training Loss: 0.07988245040178299\n",
      "Epoch 24870/30000 Training Loss: 0.07197815179824829\n",
      "Epoch 24870/30000 Validation Loss: 0.0761609673500061\n",
      "Epoch 24871/30000 Training Loss: 0.05590656399726868\n",
      "Epoch 24872/30000 Training Loss: 0.06284693628549576\n",
      "Epoch 24873/30000 Training Loss: 0.0730927512049675\n",
      "Epoch 24874/30000 Training Loss: 0.06450260430574417\n",
      "Epoch 24875/30000 Training Loss: 0.07539213448762894\n",
      "Epoch 24876/30000 Training Loss: 0.10221307724714279\n",
      "Epoch 24877/30000 Training Loss: 0.05435368791222572\n",
      "Epoch 24878/30000 Training Loss: 0.06583268195390701\n",
      "Epoch 24879/30000 Training Loss: 0.062143247574567795\n",
      "Epoch 24880/30000 Training Loss: 0.06054605916142464\n",
      "Epoch 24880/30000 Validation Loss: 0.07299233227968216\n",
      "Epoch 24881/30000 Training Loss: 0.054934341460466385\n",
      "Epoch 24882/30000 Training Loss: 0.0642629861831665\n",
      "Epoch 24883/30000 Training Loss: 0.06702498346567154\n",
      "Epoch 24884/30000 Training Loss: 0.08167565613985062\n",
      "Epoch 24885/30000 Training Loss: 0.07426963001489639\n",
      "Epoch 24886/30000 Training Loss: 0.07990756630897522\n",
      "Epoch 24887/30000 Training Loss: 0.0689205601811409\n",
      "Epoch 24888/30000 Training Loss: 0.04966217279434204\n",
      "Epoch 24889/30000 Training Loss: 0.07648303359746933\n",
      "Epoch 24890/30000 Training Loss: 0.06909948587417603\n",
      "Epoch 24890/30000 Validation Loss: 0.06722891330718994\n",
      "Epoch 24891/30000 Training Loss: 0.06278475373983383\n",
      "Epoch 24892/30000 Training Loss: 0.07109114527702332\n",
      "Epoch 24893/30000 Training Loss: 0.08296970278024673\n",
      "Epoch 24894/30000 Training Loss: 0.08033565431833267\n",
      "Epoch 24895/30000 Training Loss: 0.06211058422923088\n",
      "Epoch 24896/30000 Training Loss: 0.0703810602426529\n",
      "Epoch 24897/30000 Training Loss: 0.06772071868181229\n",
      "Epoch 24898/30000 Training Loss: 0.0659143477678299\n",
      "Epoch 24899/30000 Training Loss: 0.06952842324972153\n",
      "Epoch 24900/30000 Training Loss: 0.05618534982204437\n",
      "Epoch 24900/30000 Validation Loss: 0.0752403736114502\n",
      "Epoch 24901/30000 Training Loss: 0.05601762607693672\n",
      "Epoch 24902/30000 Training Loss: 0.08373650163412094\n",
      "Epoch 24903/30000 Training Loss: 0.07572191208600998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24904/30000 Training Loss: 0.06916797161102295\n",
      "Epoch 24905/30000 Training Loss: 0.07449841499328613\n",
      "Epoch 24906/30000 Training Loss: 0.0837705135345459\n",
      "Epoch 24907/30000 Training Loss: 0.07234912365674973\n",
      "Epoch 24908/30000 Training Loss: 0.076100192964077\n",
      "Epoch 24909/30000 Training Loss: 0.07649566978216171\n",
      "Epoch 24910/30000 Training Loss: 0.07799869030714035\n",
      "Epoch 24910/30000 Validation Loss: 0.06689822673797607\n",
      "Epoch 24911/30000 Training Loss: 0.059928104281425476\n",
      "Epoch 24912/30000 Training Loss: 0.09369269758462906\n",
      "Epoch 24913/30000 Training Loss: 0.0684351697564125\n",
      "Epoch 24914/30000 Training Loss: 0.053909119218587875\n",
      "Epoch 24915/30000 Training Loss: 0.058662768453359604\n",
      "Epoch 24916/30000 Training Loss: 0.07913913577795029\n",
      "Epoch 24917/30000 Training Loss: 0.07196246832609177\n",
      "Epoch 24918/30000 Training Loss: 0.05604970455169678\n",
      "Epoch 24919/30000 Training Loss: 0.0643886998295784\n",
      "Epoch 24920/30000 Training Loss: 0.07121419161558151\n",
      "Epoch 24920/30000 Validation Loss: 0.08432325720787048\n",
      "Epoch 24921/30000 Training Loss: 0.08781832456588745\n",
      "Epoch 24922/30000 Training Loss: 0.07543381303548813\n",
      "Epoch 24923/30000 Training Loss: 0.08443746715784073\n",
      "Epoch 24924/30000 Training Loss: 0.06856248527765274\n",
      "Epoch 24925/30000 Training Loss: 0.07626534253358841\n",
      "Epoch 24926/30000 Training Loss: 0.06275995820760727\n",
      "Epoch 24927/30000 Training Loss: 0.06710895150899887\n",
      "Epoch 24928/30000 Training Loss: 0.0628795400261879\n",
      "Epoch 24929/30000 Training Loss: 0.0613483190536499\n",
      "Epoch 24930/30000 Training Loss: 0.07164298743009567\n",
      "Epoch 24930/30000 Validation Loss: 0.06515367329120636\n",
      "Epoch 24931/30000 Training Loss: 0.06837594509124756\n",
      "Epoch 24932/30000 Training Loss: 0.0719103142619133\n",
      "Epoch 24933/30000 Training Loss: 0.06924999505281448\n",
      "Epoch 24934/30000 Training Loss: 0.060178592801094055\n",
      "Epoch 24935/30000 Training Loss: 0.08190175890922546\n",
      "Epoch 24936/30000 Training Loss: 0.06114659085869789\n",
      "Epoch 24937/30000 Training Loss: 0.0806010439991951\n",
      "Epoch 24938/30000 Training Loss: 0.05923926830291748\n",
      "Epoch 24939/30000 Training Loss: 0.07640822976827621\n",
      "Epoch 24940/30000 Training Loss: 0.07291355729103088\n",
      "Epoch 24940/30000 Validation Loss: 0.06146622821688652\n",
      "Epoch 24941/30000 Training Loss: 0.06965047866106033\n",
      "Epoch 24942/30000 Training Loss: 0.055593978613615036\n",
      "Epoch 24943/30000 Training Loss: 0.07677900791168213\n",
      "Epoch 24944/30000 Training Loss: 0.06128089502453804\n",
      "Epoch 24945/30000 Training Loss: 0.07615780085325241\n",
      "Epoch 24946/30000 Training Loss: 0.06467967480421066\n",
      "Epoch 24947/30000 Training Loss: 0.06079990789294243\n",
      "Epoch 24948/30000 Training Loss: 0.09124786406755447\n",
      "Epoch 24949/30000 Training Loss: 0.054085660725831985\n",
      "Epoch 24950/30000 Training Loss: 0.06350669264793396\n",
      "Epoch 24950/30000 Validation Loss: 0.06307803839445114\n",
      "Epoch 24951/30000 Training Loss: 0.06642689555883408\n",
      "Epoch 24952/30000 Training Loss: 0.08011230826377869\n",
      "Epoch 24953/30000 Training Loss: 0.07842544466257095\n",
      "Epoch 24954/30000 Training Loss: 0.0649806410074234\n",
      "Epoch 24955/30000 Training Loss: 0.05783378705382347\n",
      "Epoch 24956/30000 Training Loss: 0.051228880882263184\n",
      "Epoch 24957/30000 Training Loss: 0.06881038099527359\n",
      "Epoch 24958/30000 Training Loss: 0.07265427708625793\n",
      "Epoch 24959/30000 Training Loss: 0.08480420708656311\n",
      "Epoch 24960/30000 Training Loss: 0.08477427810430527\n",
      "Epoch 24960/30000 Validation Loss: 0.060116712003946304\n",
      "Epoch 24961/30000 Training Loss: 0.06353362649679184\n",
      "Epoch 24962/30000 Training Loss: 0.07330247759819031\n",
      "Epoch 24963/30000 Training Loss: 0.055346567183732986\n",
      "Epoch 24964/30000 Training Loss: 0.0762447640299797\n",
      "Epoch 24965/30000 Training Loss: 0.08193664252758026\n",
      "Epoch 24966/30000 Training Loss: 0.06283485144376755\n",
      "Epoch 24967/30000 Training Loss: 0.061204999685287476\n",
      "Epoch 24968/30000 Training Loss: 0.0625685378909111\n",
      "Epoch 24969/30000 Training Loss: 0.080109141767025\n",
      "Epoch 24970/30000 Training Loss: 0.06850963085889816\n",
      "Epoch 24970/30000 Validation Loss: 0.06321894377470016\n",
      "Epoch 24971/30000 Training Loss: 0.06913582235574722\n",
      "Epoch 24972/30000 Training Loss: 0.07334154099225998\n",
      "Epoch 24973/30000 Training Loss: 0.06840676069259644\n",
      "Epoch 24974/30000 Training Loss: 0.06427750736474991\n",
      "Epoch 24975/30000 Training Loss: 0.05790835618972778\n",
      "Epoch 24976/30000 Training Loss: 0.06917030364274979\n",
      "Epoch 24977/30000 Training Loss: 0.05262547358870506\n",
      "Epoch 24978/30000 Training Loss: 0.06712061166763306\n",
      "Epoch 24979/30000 Training Loss: 0.059579864144325256\n",
      "Epoch 24980/30000 Training Loss: 0.07921383529901505\n",
      "Epoch 24980/30000 Validation Loss: 0.06454592198133469\n",
      "Epoch 24981/30000 Training Loss: 0.06026121973991394\n",
      "Epoch 24982/30000 Training Loss: 0.0760338082909584\n",
      "Epoch 24983/30000 Training Loss: 0.06176872178912163\n",
      "Epoch 24984/30000 Training Loss: 0.054863717406988144\n",
      "Epoch 24985/30000 Training Loss: 0.07981367409229279\n",
      "Epoch 24986/30000 Training Loss: 0.05721675232052803\n",
      "Epoch 24987/30000 Training Loss: 0.05001281201839447\n",
      "Epoch 24988/30000 Training Loss: 0.05951074883341789\n",
      "Epoch 24989/30000 Training Loss: 0.06027901545166969\n",
      "Epoch 24990/30000 Training Loss: 0.0509549044072628\n",
      "Epoch 24990/30000 Validation Loss: 0.06683147698640823\n",
      "Epoch 24991/30000 Training Loss: 0.06307018548250198\n",
      "Epoch 24992/30000 Training Loss: 0.06817678362131119\n",
      "Epoch 24993/30000 Training Loss: 0.06084170937538147\n",
      "Epoch 24994/30000 Training Loss: 0.07253529876470566\n",
      "Epoch 24995/30000 Training Loss: 0.07500198483467102\n",
      "Epoch 24996/30000 Training Loss: 0.07626757025718689\n",
      "Epoch 24997/30000 Training Loss: 0.05838733911514282\n",
      "Epoch 24998/30000 Training Loss: 0.07445254921913147\n",
      "Epoch 24999/30000 Training Loss: 0.08335278183221817\n",
      "Epoch 25000/30000 Training Loss: 0.0687800869345665\n",
      "Epoch 25000/30000 Validation Loss: 0.054765600711107254\n",
      "Epoch 25001/30000 Training Loss: 0.08636829257011414\n",
      "Epoch 25002/30000 Training Loss: 0.055745337158441544\n",
      "Epoch 25003/30000 Training Loss: 0.07338231056928635\n",
      "Epoch 25004/30000 Training Loss: 0.06018128991127014\n",
      "Epoch 25005/30000 Training Loss: 0.08762485533952713\n",
      "Epoch 25006/30000 Training Loss: 0.0832303836941719\n",
      "Epoch 25007/30000 Training Loss: 0.05842243507504463\n",
      "Epoch 25008/30000 Training Loss: 0.07699906826019287\n",
      "Epoch 25009/30000 Training Loss: 0.0855262503027916\n",
      "Epoch 25010/30000 Training Loss: 0.06231735646724701\n",
      "Epoch 25010/30000 Validation Loss: 0.08242879807949066\n",
      "Epoch 25011/30000 Training Loss: 0.08269477635622025\n",
      "Epoch 25012/30000 Training Loss: 0.07214225083589554\n",
      "Epoch 25013/30000 Training Loss: 0.06739363819360733\n",
      "Epoch 25014/30000 Training Loss: 0.06952033191919327\n",
      "Epoch 25015/30000 Training Loss: 0.06081115081906319\n",
      "Epoch 25016/30000 Training Loss: 0.0709972083568573\n",
      "Epoch 25017/30000 Training Loss: 0.06744033098220825\n",
      "Epoch 25018/30000 Training Loss: 0.0633096918463707\n",
      "Epoch 25019/30000 Training Loss: 0.0881776437163353\n",
      "Epoch 25020/30000 Training Loss: 0.07158411294221878\n",
      "Epoch 25020/30000 Validation Loss: 0.06110002100467682\n",
      "Epoch 25021/30000 Training Loss: 0.07067453861236572\n",
      "Epoch 25022/30000 Training Loss: 0.05212298408150673\n",
      "Epoch 25023/30000 Training Loss: 0.0772961899638176\n",
      "Epoch 25024/30000 Training Loss: 0.06251338869333267\n",
      "Epoch 25025/30000 Training Loss: 0.078732430934906\n",
      "Epoch 25026/30000 Training Loss: 0.06880723685026169\n",
      "Epoch 25027/30000 Training Loss: 0.09021349996328354\n",
      "Epoch 25028/30000 Training Loss: 0.06785305589437485\n",
      "Epoch 25029/30000 Training Loss: 0.06112822890281677\n",
      "Epoch 25030/30000 Training Loss: 0.06867244094610214\n",
      "Epoch 25030/30000 Validation Loss: 0.0824667438864708\n",
      "Epoch 25031/30000 Training Loss: 0.05294737592339516\n",
      "Epoch 25032/30000 Training Loss: 0.06363102793693542\n",
      "Epoch 25033/30000 Training Loss: 0.07350028306245804\n",
      "Epoch 25034/30000 Training Loss: 0.08371875435113907\n",
      "Epoch 25035/30000 Training Loss: 0.06405380368232727\n",
      "Epoch 25036/30000 Training Loss: 0.07192325592041016\n",
      "Epoch 25037/30000 Training Loss: 0.06726246327161789\n",
      "Epoch 25038/30000 Training Loss: 0.06485133618116379\n",
      "Epoch 25039/30000 Training Loss: 0.06940630823373795\n",
      "Epoch 25040/30000 Training Loss: 0.06942055374383926\n",
      "Epoch 25040/30000 Validation Loss: 0.048865076154470444\n",
      "Epoch 25041/30000 Training Loss: 0.06929780542850494\n",
      "Epoch 25042/30000 Training Loss: 0.06946269422769547\n",
      "Epoch 25043/30000 Training Loss: 0.06402382999658585\n",
      "Epoch 25044/30000 Training Loss: 0.07189317792654037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25045/30000 Training Loss: 0.06349945813417435\n",
      "Epoch 25046/30000 Training Loss: 0.07083051651716232\n",
      "Epoch 25047/30000 Training Loss: 0.0739390179514885\n",
      "Epoch 25048/30000 Training Loss: 0.05872799828648567\n",
      "Epoch 25049/30000 Training Loss: 0.08745113015174866\n",
      "Epoch 25050/30000 Training Loss: 0.06053196266293526\n",
      "Epoch 25050/30000 Validation Loss: 0.06874553114175797\n",
      "Epoch 25051/30000 Training Loss: 0.0645587369799614\n",
      "Epoch 25052/30000 Training Loss: 0.07215070724487305\n",
      "Epoch 25053/30000 Training Loss: 0.09130474925041199\n",
      "Epoch 25054/30000 Training Loss: 0.08111199736595154\n",
      "Epoch 25055/30000 Training Loss: 0.059212684631347656\n",
      "Epoch 25056/30000 Training Loss: 0.06683757156133652\n",
      "Epoch 25057/30000 Training Loss: 0.08161160349845886\n",
      "Epoch 25058/30000 Training Loss: 0.05756629630923271\n",
      "Epoch 25059/30000 Training Loss: 0.06705205887556076\n",
      "Epoch 25060/30000 Training Loss: 0.08041862398386002\n",
      "Epoch 25060/30000 Validation Loss: 0.05730414390563965\n",
      "Epoch 25061/30000 Training Loss: 0.06862518191337585\n",
      "Epoch 25062/30000 Training Loss: 0.08790896087884903\n",
      "Epoch 25063/30000 Training Loss: 0.08901730924844742\n",
      "Epoch 25064/30000 Training Loss: 0.06849709898233414\n",
      "Epoch 25065/30000 Training Loss: 0.08366727083921432\n",
      "Epoch 25066/30000 Training Loss: 0.06848840415477753\n",
      "Epoch 25067/30000 Training Loss: 0.06442911177873611\n",
      "Epoch 25068/30000 Training Loss: 0.08528878539800644\n",
      "Epoch 25069/30000 Training Loss: 0.052972663193941116\n",
      "Epoch 25070/30000 Training Loss: 0.06457046419382095\n",
      "Epoch 25070/30000 Validation Loss: 0.08980651944875717\n",
      "Epoch 25071/30000 Training Loss: 0.06663331389427185\n",
      "Epoch 25072/30000 Training Loss: 0.05374014005064964\n",
      "Epoch 25073/30000 Training Loss: 0.0625094398856163\n",
      "Epoch 25074/30000 Training Loss: 0.05840010568499565\n",
      "Epoch 25075/30000 Training Loss: 0.0679786205291748\n",
      "Epoch 25076/30000 Training Loss: 0.06865791231393814\n",
      "Epoch 25077/30000 Training Loss: 0.06904439628124237\n",
      "Epoch 25078/30000 Training Loss: 0.07092747092247009\n",
      "Epoch 25079/30000 Training Loss: 0.07061132043600082\n",
      "Epoch 25080/30000 Training Loss: 0.06067265197634697\n",
      "Epoch 25080/30000 Validation Loss: 0.047451213002204895\n",
      "Epoch 25081/30000 Training Loss: 0.07680641859769821\n",
      "Epoch 25082/30000 Training Loss: 0.07113171368837357\n",
      "Epoch 25083/30000 Training Loss: 0.060093507170677185\n",
      "Epoch 25084/30000 Training Loss: 0.07041843235492706\n",
      "Epoch 25085/30000 Training Loss: 0.082023024559021\n",
      "Epoch 25086/30000 Training Loss: 0.07181175798177719\n",
      "Epoch 25087/30000 Training Loss: 0.07312703877687454\n",
      "Epoch 25088/30000 Training Loss: 0.06803280860185623\n",
      "Epoch 25089/30000 Training Loss: 0.07028480619192123\n",
      "Epoch 25090/30000 Training Loss: 0.07576537132263184\n",
      "Epoch 25090/30000 Validation Loss: 0.075936459004879\n",
      "Epoch 25091/30000 Training Loss: 0.07032972574234009\n",
      "Epoch 25092/30000 Training Loss: 0.08304234594106674\n",
      "Epoch 25093/30000 Training Loss: 0.06967393308877945\n",
      "Epoch 25094/30000 Training Loss: 0.09053119271993637\n",
      "Epoch 25095/30000 Training Loss: 0.09535107761621475\n",
      "Epoch 25096/30000 Training Loss: 0.06896094232797623\n",
      "Epoch 25097/30000 Training Loss: 0.06818587332963943\n",
      "Epoch 25098/30000 Training Loss: 0.055649805814027786\n",
      "Epoch 25099/30000 Training Loss: 0.07051745057106018\n",
      "Epoch 25100/30000 Training Loss: 0.08029382675886154\n",
      "Epoch 25100/30000 Validation Loss: 0.05690747871994972\n",
      "Epoch 25101/30000 Training Loss: 0.0734851136803627\n",
      "Epoch 25102/30000 Training Loss: 0.10254258662462234\n",
      "Epoch 25103/30000 Training Loss: 0.05476490780711174\n",
      "Epoch 25104/30000 Training Loss: 0.06611717492341995\n",
      "Epoch 25105/30000 Training Loss: 0.06157417222857475\n",
      "Epoch 25106/30000 Training Loss: 0.0878930613398552\n",
      "Epoch 25107/30000 Training Loss: 0.06964518874883652\n",
      "Epoch 25108/30000 Training Loss: 0.07288724184036255\n",
      "Epoch 25109/30000 Training Loss: 0.07507070153951645\n",
      "Epoch 25110/30000 Training Loss: 0.07344478368759155\n",
      "Epoch 25110/30000 Validation Loss: 0.08551442623138428\n",
      "Epoch 25111/30000 Training Loss: 0.06594189256429672\n",
      "Epoch 25112/30000 Training Loss: 0.09241623431444168\n",
      "Epoch 25113/30000 Training Loss: 0.0648854598402977\n",
      "Epoch 25114/30000 Training Loss: 0.08094432950019836\n",
      "Epoch 25115/30000 Training Loss: 0.07955985516309738\n",
      "Epoch 25116/30000 Training Loss: 0.06427553296089172\n",
      "Epoch 25117/30000 Training Loss: 0.09465736150741577\n",
      "Epoch 25118/30000 Training Loss: 0.06018901988863945\n",
      "Epoch 25119/30000 Training Loss: 0.08316396921873093\n",
      "Epoch 25120/30000 Training Loss: 0.06889871507883072\n",
      "Epoch 25120/30000 Validation Loss: 0.10481616109609604\n",
      "Epoch 25121/30000 Training Loss: 0.06722988188266754\n",
      "Epoch 25122/30000 Training Loss: 0.05573949217796326\n",
      "Epoch 25123/30000 Training Loss: 0.05782422423362732\n",
      "Epoch 25124/30000 Training Loss: 0.08008257299661636\n",
      "Epoch 25125/30000 Training Loss: 0.07822670787572861\n",
      "Epoch 25126/30000 Training Loss: 0.08457833528518677\n",
      "Epoch 25127/30000 Training Loss: 0.07685267180204391\n",
      "Epoch 25128/30000 Training Loss: 0.08270270377397537\n",
      "Epoch 25129/30000 Training Loss: 0.08055257797241211\n",
      "Epoch 25130/30000 Training Loss: 0.0809805691242218\n",
      "Epoch 25130/30000 Validation Loss: 0.0949142649769783\n",
      "Epoch 25131/30000 Training Loss: 0.054144952446222305\n",
      "Epoch 25132/30000 Training Loss: 0.08042129129171371\n",
      "Epoch 25133/30000 Training Loss: 0.05951256677508354\n",
      "Epoch 25134/30000 Training Loss: 0.06822633743286133\n",
      "Epoch 25135/30000 Training Loss: 0.08028203248977661\n",
      "Epoch 25136/30000 Training Loss: 0.050256624817848206\n",
      "Epoch 25137/30000 Training Loss: 0.05886055529117584\n",
      "Epoch 25138/30000 Training Loss: 0.06329557299613953\n",
      "Epoch 25139/30000 Training Loss: 0.08617031574249268\n",
      "Epoch 25140/30000 Training Loss: 0.07085119932889938\n",
      "Epoch 25140/30000 Validation Loss: 0.06758832186460495\n",
      "Epoch 25141/30000 Training Loss: 0.09094005078077316\n",
      "Epoch 25142/30000 Training Loss: 0.0949442982673645\n",
      "Epoch 25143/30000 Training Loss: 0.06224989518523216\n",
      "Epoch 25144/30000 Training Loss: 0.07376977801322937\n",
      "Epoch 25145/30000 Training Loss: 0.06022049859166145\n",
      "Epoch 25146/30000 Training Loss: 0.09404429793357849\n",
      "Epoch 25147/30000 Training Loss: 0.09698911756277084\n",
      "Epoch 25148/30000 Training Loss: 0.05773374065756798\n",
      "Epoch 25149/30000 Training Loss: 0.06058948114514351\n",
      "Epoch 25150/30000 Training Loss: 0.06569191068410873\n",
      "Epoch 25150/30000 Validation Loss: 0.06862889975309372\n",
      "Epoch 25151/30000 Training Loss: 0.08627110719680786\n",
      "Epoch 25152/30000 Training Loss: 0.06897985190153122\n",
      "Epoch 25153/30000 Training Loss: 0.07756020873785019\n",
      "Epoch 25154/30000 Training Loss: 0.07135000824928284\n",
      "Epoch 25155/30000 Training Loss: 0.06849496811628342\n",
      "Epoch 25156/30000 Training Loss: 0.06367215514183044\n",
      "Epoch 25157/30000 Training Loss: 0.07431800663471222\n",
      "Epoch 25158/30000 Training Loss: 0.05898074805736542\n",
      "Epoch 25159/30000 Training Loss: 0.08665835857391357\n",
      "Epoch 25160/30000 Training Loss: 0.06717395037412643\n",
      "Epoch 25160/30000 Validation Loss: 0.0806618183851242\n",
      "Epoch 25161/30000 Training Loss: 0.06257375329732895\n",
      "Epoch 25162/30000 Training Loss: 0.08089039474725723\n",
      "Epoch 25163/30000 Training Loss: 0.08584214001893997\n",
      "Epoch 25164/30000 Training Loss: 0.05916161835193634\n",
      "Epoch 25165/30000 Training Loss: 0.0684303492307663\n",
      "Epoch 25166/30000 Training Loss: 0.07138070464134216\n",
      "Epoch 25167/30000 Training Loss: 0.07331319898366928\n",
      "Epoch 25168/30000 Training Loss: 0.0688479021191597\n",
      "Epoch 25169/30000 Training Loss: 0.09098216146230698\n",
      "Epoch 25170/30000 Training Loss: 0.08465049415826797\n",
      "Epoch 25170/30000 Validation Loss: 0.05459216237068176\n",
      "Epoch 25171/30000 Training Loss: 0.06141090765595436\n",
      "Epoch 25172/30000 Training Loss: 0.08462537080049515\n",
      "Epoch 25173/30000 Training Loss: 0.08641799539327621\n",
      "Epoch 25174/30000 Training Loss: 0.07963521033525467\n",
      "Epoch 25175/30000 Training Loss: 0.06832852214574814\n",
      "Epoch 25176/30000 Training Loss: 0.06589357554912567\n",
      "Epoch 25177/30000 Training Loss: 0.0893741175532341\n",
      "Epoch 25178/30000 Training Loss: 0.06086732819676399\n",
      "Epoch 25179/30000 Training Loss: 0.06471496820449829\n",
      "Epoch 25180/30000 Training Loss: 0.0787644311785698\n",
      "Epoch 25180/30000 Validation Loss: 0.07947655767202377\n",
      "Epoch 25181/30000 Training Loss: 0.06433654576539993\n",
      "Epoch 25182/30000 Training Loss: 0.06318357586860657\n",
      "Epoch 25183/30000 Training Loss: 0.07833150029182434\n",
      "Epoch 25184/30000 Training Loss: 0.06088751554489136\n",
      "Epoch 25185/30000 Training Loss: 0.07087383419275284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25186/30000 Training Loss: 0.06760547310113907\n",
      "Epoch 25187/30000 Training Loss: 0.0973043143749237\n",
      "Epoch 25188/30000 Training Loss: 0.08707409352064133\n",
      "Epoch 25189/30000 Training Loss: 0.06936392188072205\n",
      "Epoch 25190/30000 Training Loss: 0.05158756300806999\n",
      "Epoch 25190/30000 Validation Loss: 0.07046771794557571\n",
      "Epoch 25191/30000 Training Loss: 0.06858175992965698\n",
      "Epoch 25192/30000 Training Loss: 0.06508348137140274\n",
      "Epoch 25193/30000 Training Loss: 0.08469828218221664\n",
      "Epoch 25194/30000 Training Loss: 0.07425793260335922\n",
      "Epoch 25195/30000 Training Loss: 0.057157959789037704\n",
      "Epoch 25196/30000 Training Loss: 0.06457676738500595\n",
      "Epoch 25197/30000 Training Loss: 0.07246822863817215\n",
      "Epoch 25198/30000 Training Loss: 0.06337963044643402\n",
      "Epoch 25199/30000 Training Loss: 0.05727115273475647\n",
      "Epoch 25200/30000 Training Loss: 0.07901981472969055\n",
      "Epoch 25200/30000 Validation Loss: 0.0754801407456398\n",
      "Epoch 25201/30000 Training Loss: 0.06535828113555908\n",
      "Epoch 25202/30000 Training Loss: 0.07422762364149094\n",
      "Epoch 25203/30000 Training Loss: 0.07435441762208939\n",
      "Epoch 25204/30000 Training Loss: 0.07162033766508102\n",
      "Epoch 25205/30000 Training Loss: 0.059509679675102234\n",
      "Epoch 25206/30000 Training Loss: 0.076229028403759\n",
      "Epoch 25207/30000 Training Loss: 0.06483585387468338\n",
      "Epoch 25208/30000 Training Loss: 0.06238359212875366\n",
      "Epoch 25209/30000 Training Loss: 0.07545536011457443\n",
      "Epoch 25210/30000 Training Loss: 0.08604923635721207\n",
      "Epoch 25210/30000 Validation Loss: 0.07058613747358322\n",
      "Epoch 25211/30000 Training Loss: 0.0684938132762909\n",
      "Epoch 25212/30000 Training Loss: 0.0743289366364479\n",
      "Epoch 25213/30000 Training Loss: 0.08203792572021484\n",
      "Epoch 25214/30000 Training Loss: 0.0694948062300682\n",
      "Epoch 25215/30000 Training Loss: 0.07812941819429398\n",
      "Epoch 25216/30000 Training Loss: 0.06705991178750992\n",
      "Epoch 25217/30000 Training Loss: 0.0754246935248375\n",
      "Epoch 25218/30000 Training Loss: 0.05740591883659363\n",
      "Epoch 25219/30000 Training Loss: 0.07147732377052307\n",
      "Epoch 25220/30000 Training Loss: 0.05356465280056\n",
      "Epoch 25220/30000 Validation Loss: 0.08884970098733902\n",
      "Epoch 25221/30000 Training Loss: 0.06591974943876266\n",
      "Epoch 25222/30000 Training Loss: 0.07544130831956863\n",
      "Epoch 25223/30000 Training Loss: 0.07359690219163895\n",
      "Epoch 25224/30000 Training Loss: 0.06780899316072464\n",
      "Epoch 25225/30000 Training Loss: 0.05113746598362923\n",
      "Epoch 25226/30000 Training Loss: 0.07211234420537949\n",
      "Epoch 25227/30000 Training Loss: 0.06366918236017227\n",
      "Epoch 25228/30000 Training Loss: 0.05475660040974617\n",
      "Epoch 25229/30000 Training Loss: 0.07029634714126587\n",
      "Epoch 25230/30000 Training Loss: 0.07091260701417923\n",
      "Epoch 25230/30000 Validation Loss: 0.07670122385025024\n",
      "Epoch 25231/30000 Training Loss: 0.06812789291143417\n",
      "Epoch 25232/30000 Training Loss: 0.08465927839279175\n",
      "Epoch 25233/30000 Training Loss: 0.06238512322306633\n",
      "Epoch 25234/30000 Training Loss: 0.06764430552721024\n",
      "Epoch 25235/30000 Training Loss: 0.07727038860321045\n",
      "Epoch 25236/30000 Training Loss: 0.058812856674194336\n",
      "Epoch 25237/30000 Training Loss: 0.09033157676458359\n",
      "Epoch 25238/30000 Training Loss: 0.05790242552757263\n",
      "Epoch 25239/30000 Training Loss: 0.0640978142619133\n",
      "Epoch 25240/30000 Training Loss: 0.07666274905204773\n",
      "Epoch 25240/30000 Validation Loss: 0.05933239683508873\n",
      "Epoch 25241/30000 Training Loss: 0.05773436650633812\n",
      "Epoch 25242/30000 Training Loss: 0.06059885025024414\n",
      "Epoch 25243/30000 Training Loss: 0.0716896653175354\n",
      "Epoch 25244/30000 Training Loss: 0.05381419137120247\n",
      "Epoch 25245/30000 Training Loss: 0.06609293818473816\n",
      "Epoch 25246/30000 Training Loss: 0.06628463417291641\n",
      "Epoch 25247/30000 Training Loss: 0.07043560594320297\n",
      "Epoch 25248/30000 Training Loss: 0.06707144528627396\n",
      "Epoch 25249/30000 Training Loss: 0.0757184624671936\n",
      "Epoch 25250/30000 Training Loss: 0.07553642243146896\n",
      "Epoch 25250/30000 Validation Loss: 0.07079487293958664\n",
      "Epoch 25251/30000 Training Loss: 0.05980445444583893\n",
      "Epoch 25252/30000 Training Loss: 0.08081018179655075\n",
      "Epoch 25253/30000 Training Loss: 0.06420383602380753\n",
      "Epoch 25254/30000 Training Loss: 0.05885555222630501\n",
      "Epoch 25255/30000 Training Loss: 0.0707722082734108\n",
      "Epoch 25256/30000 Training Loss: 0.06304676085710526\n",
      "Epoch 25257/30000 Training Loss: 0.06650390475988388\n",
      "Epoch 25258/30000 Training Loss: 0.07073617726564407\n",
      "Epoch 25259/30000 Training Loss: 0.07677248865365982\n",
      "Epoch 25260/30000 Training Loss: 0.07498521357774734\n",
      "Epoch 25260/30000 Validation Loss: 0.06900163739919662\n",
      "Epoch 25261/30000 Training Loss: 0.05765486881136894\n",
      "Epoch 25262/30000 Training Loss: 0.0644427016377449\n",
      "Epoch 25263/30000 Training Loss: 0.07116612046957016\n",
      "Epoch 25264/30000 Training Loss: 0.06518149375915527\n",
      "Epoch 25265/30000 Training Loss: 0.07177474349737167\n",
      "Epoch 25266/30000 Training Loss: 0.07092171162366867\n",
      "Epoch 25267/30000 Training Loss: 0.05926046893000603\n",
      "Epoch 25268/30000 Training Loss: 0.07370587438344955\n",
      "Epoch 25269/30000 Training Loss: 0.06182047724723816\n",
      "Epoch 25270/30000 Training Loss: 0.06069345772266388\n",
      "Epoch 25270/30000 Validation Loss: 0.07956505566835403\n",
      "Epoch 25271/30000 Training Loss: 0.07775194197893143\n",
      "Epoch 25272/30000 Training Loss: 0.0648607686161995\n",
      "Epoch 25273/30000 Training Loss: 0.08252101391553879\n",
      "Epoch 25274/30000 Training Loss: 0.07537534087896347\n",
      "Epoch 25275/30000 Training Loss: 0.07536933571100235\n",
      "Epoch 25276/30000 Training Loss: 0.07546190172433853\n",
      "Epoch 25277/30000 Training Loss: 0.06395207345485687\n",
      "Epoch 25278/30000 Training Loss: 0.06920856237411499\n",
      "Epoch 25279/30000 Training Loss: 0.08486267179250717\n",
      "Epoch 25280/30000 Training Loss: 0.08257735520601273\n",
      "Epoch 25280/30000 Validation Loss: 0.08424223214387894\n",
      "Epoch 25281/30000 Training Loss: 0.06839238852262497\n",
      "Epoch 25282/30000 Training Loss: 0.07421606034040451\n",
      "Epoch 25283/30000 Training Loss: 0.07408026605844498\n",
      "Epoch 25284/30000 Training Loss: 0.049786221235990524\n",
      "Epoch 25285/30000 Training Loss: 0.06304092705249786\n",
      "Epoch 25286/30000 Training Loss: 0.06693056225776672\n",
      "Epoch 25287/30000 Training Loss: 0.07139724493026733\n",
      "Epoch 25288/30000 Training Loss: 0.08078258484601974\n",
      "Epoch 25289/30000 Training Loss: 0.06078292801976204\n",
      "Epoch 25290/30000 Training Loss: 0.06319528818130493\n",
      "Epoch 25290/30000 Validation Loss: 0.08403003215789795\n",
      "Epoch 25291/30000 Training Loss: 0.06625617295503616\n",
      "Epoch 25292/30000 Training Loss: 0.06965747475624084\n",
      "Epoch 25293/30000 Training Loss: 0.07144350558519363\n",
      "Epoch 25294/30000 Training Loss: 0.07465288788080215\n",
      "Epoch 25295/30000 Training Loss: 0.06313128769397736\n",
      "Epoch 25296/30000 Training Loss: 0.05832110717892647\n",
      "Epoch 25297/30000 Training Loss: 0.0722571387887001\n",
      "Epoch 25298/30000 Training Loss: 0.07546916604042053\n",
      "Epoch 25299/30000 Training Loss: 0.06057059392333031\n",
      "Epoch 25300/30000 Training Loss: 0.07474934309720993\n",
      "Epoch 25300/30000 Validation Loss: 0.07246001809835434\n",
      "Epoch 25301/30000 Training Loss: 0.07728757709264755\n",
      "Epoch 25302/30000 Training Loss: 0.06018620729446411\n",
      "Epoch 25303/30000 Training Loss: 0.0566067099571228\n",
      "Epoch 25304/30000 Training Loss: 0.0695890411734581\n",
      "Epoch 25305/30000 Training Loss: 0.06551895290613174\n",
      "Epoch 25306/30000 Training Loss: 0.07630228251218796\n",
      "Epoch 25307/30000 Training Loss: 0.06879761070013046\n",
      "Epoch 25308/30000 Training Loss: 0.06584595143795013\n",
      "Epoch 25309/30000 Training Loss: 0.09400952607393265\n",
      "Epoch 25310/30000 Training Loss: 0.07330691069364548\n",
      "Epoch 25310/30000 Validation Loss: 0.0651981458067894\n",
      "Epoch 25311/30000 Training Loss: 0.06048626825213432\n",
      "Epoch 25312/30000 Training Loss: 0.10871826857328415\n",
      "Epoch 25313/30000 Training Loss: 0.05989605188369751\n",
      "Epoch 25314/30000 Training Loss: 0.07132536917924881\n",
      "Epoch 25315/30000 Training Loss: 0.06530164927244186\n",
      "Epoch 25316/30000 Training Loss: 0.06440936774015427\n",
      "Epoch 25317/30000 Training Loss: 0.06011878326535225\n",
      "Epoch 25318/30000 Training Loss: 0.051459651440382004\n",
      "Epoch 25319/30000 Training Loss: 0.08321712911128998\n",
      "Epoch 25320/30000 Training Loss: 0.07174044847488403\n",
      "Epoch 25320/30000 Validation Loss: 0.06272241473197937\n",
      "Epoch 25321/30000 Training Loss: 0.08475559949874878\n",
      "Epoch 25322/30000 Training Loss: 0.07361206412315369\n",
      "Epoch 25323/30000 Training Loss: 0.06563400477170944\n",
      "Epoch 25324/30000 Training Loss: 0.07099229842424393\n",
      "Epoch 25325/30000 Training Loss: 0.0495980829000473\n",
      "Epoch 25326/30000 Training Loss: 0.057378631085157394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25327/30000 Training Loss: 0.07565206289291382\n",
      "Epoch 25328/30000 Training Loss: 0.06766439229249954\n",
      "Epoch 25329/30000 Training Loss: 0.0600416474044323\n",
      "Epoch 25330/30000 Training Loss: 0.07626733928918839\n",
      "Epoch 25330/30000 Validation Loss: 0.0706876888871193\n",
      "Epoch 25331/30000 Training Loss: 0.057151149958372116\n",
      "Epoch 25332/30000 Training Loss: 0.07972905039787292\n",
      "Epoch 25333/30000 Training Loss: 0.06401509791612625\n",
      "Epoch 25334/30000 Training Loss: 0.06031492352485657\n",
      "Epoch 25335/30000 Training Loss: 0.08382910490036011\n",
      "Epoch 25336/30000 Training Loss: 0.07630269974470139\n",
      "Epoch 25337/30000 Training Loss: 0.07279448956251144\n",
      "Epoch 25338/30000 Training Loss: 0.0669422373175621\n",
      "Epoch 25339/30000 Training Loss: 0.06836680322885513\n",
      "Epoch 25340/30000 Training Loss: 0.058704957365989685\n",
      "Epoch 25340/30000 Validation Loss: 0.0676208958029747\n",
      "Epoch 25341/30000 Training Loss: 0.05383820831775665\n",
      "Epoch 25342/30000 Training Loss: 0.06828249990940094\n",
      "Epoch 25343/30000 Training Loss: 0.0662127360701561\n",
      "Epoch 25344/30000 Training Loss: 0.07559272646903992\n",
      "Epoch 25345/30000 Training Loss: 0.060505885630846024\n",
      "Epoch 25346/30000 Training Loss: 0.060213685035705566\n",
      "Epoch 25347/30000 Training Loss: 0.08192455023527145\n",
      "Epoch 25348/30000 Training Loss: 0.08942482620477676\n",
      "Epoch 25349/30000 Training Loss: 0.09191039204597473\n",
      "Epoch 25350/30000 Training Loss: 0.06231267377734184\n",
      "Epoch 25350/30000 Validation Loss: 0.05990004539489746\n",
      "Epoch 25351/30000 Training Loss: 0.08397286385297775\n",
      "Epoch 25352/30000 Training Loss: 0.0804833173751831\n",
      "Epoch 25353/30000 Training Loss: 0.05476571246981621\n",
      "Epoch 25354/30000 Training Loss: 0.07873568683862686\n",
      "Epoch 25355/30000 Training Loss: 0.08351254463195801\n",
      "Epoch 25356/30000 Training Loss: 0.09062165766954422\n",
      "Epoch 25357/30000 Training Loss: 0.06419111043214798\n",
      "Epoch 25358/30000 Training Loss: 0.06610050052404404\n",
      "Epoch 25359/30000 Training Loss: 0.07033983618021011\n",
      "Epoch 25360/30000 Training Loss: 0.08461365103721619\n",
      "Epoch 25360/30000 Validation Loss: 0.06827139854431152\n",
      "Epoch 25361/30000 Training Loss: 0.06668569147586823\n",
      "Epoch 25362/30000 Training Loss: 0.060040444135665894\n",
      "Epoch 25363/30000 Training Loss: 0.08113789558410645\n",
      "Epoch 25364/30000 Training Loss: 0.07335274666547775\n",
      "Epoch 25365/30000 Training Loss: 0.06529036164283752\n",
      "Epoch 25366/30000 Training Loss: 0.06960422545671463\n",
      "Epoch 25367/30000 Training Loss: 0.07065015286207199\n",
      "Epoch 25368/30000 Training Loss: 0.07399149239063263\n",
      "Epoch 25369/30000 Training Loss: 0.0743449404835701\n",
      "Epoch 25370/30000 Training Loss: 0.0646572932600975\n",
      "Epoch 25370/30000 Validation Loss: 0.0733647421002388\n",
      "Epoch 25371/30000 Training Loss: 0.07283594459295273\n",
      "Epoch 25372/30000 Training Loss: 0.06164823845028877\n",
      "Epoch 25373/30000 Training Loss: 0.06531178206205368\n",
      "Epoch 25374/30000 Training Loss: 0.06933343410491943\n",
      "Epoch 25375/30000 Training Loss: 0.08838363736867905\n",
      "Epoch 25376/30000 Training Loss: 0.055990204215049744\n",
      "Epoch 25377/30000 Training Loss: 0.07138216495513916\n",
      "Epoch 25378/30000 Training Loss: 0.06814491003751755\n",
      "Epoch 25379/30000 Training Loss: 0.0626528263092041\n",
      "Epoch 25380/30000 Training Loss: 0.07227405160665512\n",
      "Epoch 25380/30000 Validation Loss: 0.07516849040985107\n",
      "Epoch 25381/30000 Training Loss: 0.07279080152511597\n",
      "Epoch 25382/30000 Training Loss: 0.04676850512623787\n",
      "Epoch 25383/30000 Training Loss: 0.06090434268116951\n",
      "Epoch 25384/30000 Training Loss: 0.07830507308244705\n",
      "Epoch 25385/30000 Training Loss: 0.07773575186729431\n",
      "Epoch 25386/30000 Training Loss: 0.07305998355150223\n",
      "Epoch 25387/30000 Training Loss: 0.09437095373868942\n",
      "Epoch 25388/30000 Training Loss: 0.05422040820121765\n",
      "Epoch 25389/30000 Training Loss: 0.062358204275369644\n",
      "Epoch 25390/30000 Training Loss: 0.06225131079554558\n",
      "Epoch 25390/30000 Validation Loss: 0.0650067999958992\n",
      "Epoch 25391/30000 Training Loss: 0.06774166226387024\n",
      "Epoch 25392/30000 Training Loss: 0.07061076164245605\n",
      "Epoch 25393/30000 Training Loss: 0.06911583989858627\n",
      "Epoch 25394/30000 Training Loss: 0.08055124431848526\n",
      "Epoch 25395/30000 Training Loss: 0.06385841220617294\n",
      "Epoch 25396/30000 Training Loss: 0.07865053415298462\n",
      "Epoch 25397/30000 Training Loss: 0.07211730629205704\n",
      "Epoch 25398/30000 Training Loss: 0.07142872363328934\n",
      "Epoch 25399/30000 Training Loss: 0.07986665517091751\n",
      "Epoch 25400/30000 Training Loss: 0.06905368715524673\n",
      "Epoch 25400/30000 Validation Loss: 0.07005184143781662\n",
      "Epoch 25401/30000 Training Loss: 0.06076556444168091\n",
      "Epoch 25402/30000 Training Loss: 0.07594694197177887\n",
      "Epoch 25403/30000 Training Loss: 0.07062069326639175\n",
      "Epoch 25404/30000 Training Loss: 0.06926046311855316\n",
      "Epoch 25405/30000 Training Loss: 0.07731964439153671\n",
      "Epoch 25406/30000 Training Loss: 0.06967576593160629\n",
      "Epoch 25407/30000 Training Loss: 0.06624253839254379\n",
      "Epoch 25408/30000 Training Loss: 0.07045073062181473\n",
      "Epoch 25409/30000 Training Loss: 0.06401241570711136\n",
      "Epoch 25410/30000 Training Loss: 0.053676802664995193\n",
      "Epoch 25410/30000 Validation Loss: 0.07195977121591568\n",
      "Epoch 25411/30000 Training Loss: 0.07252748310565948\n",
      "Epoch 25412/30000 Training Loss: 0.05551961064338684\n",
      "Epoch 25413/30000 Training Loss: 0.08603354543447495\n",
      "Epoch 25414/30000 Training Loss: 0.07531396299600601\n",
      "Epoch 25415/30000 Training Loss: 0.0613279826939106\n",
      "Epoch 25416/30000 Training Loss: 0.07696732878684998\n",
      "Epoch 25417/30000 Training Loss: 0.0734066441655159\n",
      "Epoch 25418/30000 Training Loss: 0.060640450567007065\n",
      "Epoch 25419/30000 Training Loss: 0.08288128674030304\n",
      "Epoch 25420/30000 Training Loss: 0.07761064171791077\n",
      "Epoch 25420/30000 Validation Loss: 0.09685498476028442\n",
      "Epoch 25421/30000 Training Loss: 0.06869232654571533\n",
      "Epoch 25422/30000 Training Loss: 0.08028694242238998\n",
      "Epoch 25423/30000 Training Loss: 0.058819156140089035\n",
      "Epoch 25424/30000 Training Loss: 0.08130627125501633\n",
      "Epoch 25425/30000 Training Loss: 0.06906118988990784\n",
      "Epoch 25426/30000 Training Loss: 0.05938186123967171\n",
      "Epoch 25427/30000 Training Loss: 0.07887362688779831\n",
      "Epoch 25428/30000 Training Loss: 0.06379693001508713\n",
      "Epoch 25429/30000 Training Loss: 0.05509181693196297\n",
      "Epoch 25430/30000 Training Loss: 0.0850304588675499\n",
      "Epoch 25430/30000 Validation Loss: 0.07742006331682205\n",
      "Epoch 25431/30000 Training Loss: 0.0684942826628685\n",
      "Epoch 25432/30000 Training Loss: 0.0730535015463829\n",
      "Epoch 25433/30000 Training Loss: 0.07035934180021286\n",
      "Epoch 25434/30000 Training Loss: 0.06583323329687119\n",
      "Epoch 25435/30000 Training Loss: 0.06513626128435135\n",
      "Epoch 25436/30000 Training Loss: 0.07619623094797134\n",
      "Epoch 25437/30000 Training Loss: 0.05840761587023735\n",
      "Epoch 25438/30000 Training Loss: 0.07434427738189697\n",
      "Epoch 25439/30000 Training Loss: 0.05865125358104706\n",
      "Epoch 25440/30000 Training Loss: 0.06405449658632278\n",
      "Epoch 25440/30000 Validation Loss: 0.05538560450077057\n",
      "Epoch 25441/30000 Training Loss: 0.0840267539024353\n",
      "Epoch 25442/30000 Training Loss: 0.06535755842924118\n",
      "Epoch 25443/30000 Training Loss: 0.07219088822603226\n",
      "Epoch 25444/30000 Training Loss: 0.08351382613182068\n",
      "Epoch 25445/30000 Training Loss: 0.08334343880414963\n",
      "Epoch 25446/30000 Training Loss: 0.08060391992330551\n",
      "Epoch 25447/30000 Training Loss: 0.05683237686753273\n",
      "Epoch 25448/30000 Training Loss: 0.07617374509572983\n",
      "Epoch 25449/30000 Training Loss: 0.07975995540618896\n",
      "Epoch 25450/30000 Training Loss: 0.0695357546210289\n",
      "Epoch 25450/30000 Validation Loss: 0.053787872195243835\n",
      "Epoch 25451/30000 Training Loss: 0.05699940025806427\n",
      "Epoch 25452/30000 Training Loss: 0.04856522008776665\n",
      "Epoch 25453/30000 Training Loss: 0.07482627034187317\n",
      "Epoch 25454/30000 Training Loss: 0.07729452103376389\n",
      "Epoch 25455/30000 Training Loss: 0.07385659217834473\n",
      "Epoch 25456/30000 Training Loss: 0.05454157292842865\n",
      "Epoch 25457/30000 Training Loss: 0.08781177550554276\n",
      "Epoch 25458/30000 Training Loss: 0.07193008810281754\n",
      "Epoch 25459/30000 Training Loss: 0.05235603079199791\n",
      "Epoch 25460/30000 Training Loss: 0.06925659626722336\n",
      "Epoch 25460/30000 Validation Loss: 0.06245168671011925\n",
      "Epoch 25461/30000 Training Loss: 0.06140384078025818\n",
      "Epoch 25462/30000 Training Loss: 0.07443318516016006\n",
      "Epoch 25463/30000 Training Loss: 0.06560593843460083\n",
      "Epoch 25464/30000 Training Loss: 0.08046319335699081\n",
      "Epoch 25465/30000 Training Loss: 0.06776216626167297\n",
      "Epoch 25466/30000 Training Loss: 0.0614425428211689\n",
      "Epoch 25467/30000 Training Loss: 0.05598825216293335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25468/30000 Training Loss: 0.06820493936538696\n",
      "Epoch 25469/30000 Training Loss: 0.08273240178823471\n",
      "Epoch 25470/30000 Training Loss: 0.07777459174394608\n",
      "Epoch 25470/30000 Validation Loss: 0.06698427349328995\n",
      "Epoch 25471/30000 Training Loss: 0.08154039084911346\n",
      "Epoch 25472/30000 Training Loss: 0.07965319603681564\n",
      "Epoch 25473/30000 Training Loss: 0.0609176941215992\n",
      "Epoch 25474/30000 Training Loss: 0.0899379625916481\n",
      "Epoch 25475/30000 Training Loss: 0.06501644849777222\n",
      "Epoch 25476/30000 Training Loss: 0.07158470898866653\n",
      "Epoch 25477/30000 Training Loss: 0.06700034439563751\n",
      "Epoch 25478/30000 Training Loss: 0.06845003366470337\n",
      "Epoch 25479/30000 Training Loss: 0.08678913116455078\n",
      "Epoch 25480/30000 Training Loss: 0.0885247215628624\n",
      "Epoch 25480/30000 Validation Loss: 0.07167477160692215\n",
      "Epoch 25481/30000 Training Loss: 0.06687256693840027\n",
      "Epoch 25482/30000 Training Loss: 0.07384871691465378\n",
      "Epoch 25483/30000 Training Loss: 0.06383071839809418\n",
      "Epoch 25484/30000 Training Loss: 0.08218487352132797\n",
      "Epoch 25485/30000 Training Loss: 0.07278299331665039\n",
      "Epoch 25486/30000 Training Loss: 0.07287513464689255\n",
      "Epoch 25487/30000 Training Loss: 0.05374281480908394\n",
      "Epoch 25488/30000 Training Loss: 0.06176357343792915\n",
      "Epoch 25489/30000 Training Loss: 0.09146726876497269\n",
      "Epoch 25490/30000 Training Loss: 0.07557070255279541\n",
      "Epoch 25490/30000 Validation Loss: 0.07160097360610962\n",
      "Epoch 25491/30000 Training Loss: 0.08410846441984177\n",
      "Epoch 25492/30000 Training Loss: 0.05947147682309151\n",
      "Epoch 25493/30000 Training Loss: 0.0687982439994812\n",
      "Epoch 25494/30000 Training Loss: 0.0800921618938446\n",
      "Epoch 25495/30000 Training Loss: 0.0691480040550232\n",
      "Epoch 25496/30000 Training Loss: 0.05939634516835213\n",
      "Epoch 25497/30000 Training Loss: 0.07591859251260757\n",
      "Epoch 25498/30000 Training Loss: 0.0750080943107605\n",
      "Epoch 25499/30000 Training Loss: 0.07017100602388382\n",
      "Epoch 25500/30000 Training Loss: 0.0610571950674057\n",
      "Epoch 25500/30000 Validation Loss: 0.062021274119615555\n",
      "Epoch 25501/30000 Training Loss: 0.049368083477020264\n",
      "Epoch 25502/30000 Training Loss: 0.07875962555408478\n",
      "Epoch 25503/30000 Training Loss: 0.07135051488876343\n",
      "Epoch 25504/30000 Training Loss: 0.05644087865948677\n",
      "Epoch 25505/30000 Training Loss: 0.0763830840587616\n",
      "Epoch 25506/30000 Training Loss: 0.06142617389559746\n",
      "Epoch 25507/30000 Training Loss: 0.06682919710874557\n",
      "Epoch 25508/30000 Training Loss: 0.054744262248277664\n",
      "Epoch 25509/30000 Training Loss: 0.07202392816543579\n",
      "Epoch 25510/30000 Training Loss: 0.06038727983832359\n",
      "Epoch 25510/30000 Validation Loss: 0.07175447791814804\n",
      "Epoch 25511/30000 Training Loss: 0.056929659098386765\n",
      "Epoch 25512/30000 Training Loss: 0.057891327887773514\n",
      "Epoch 25513/30000 Training Loss: 0.06172625347971916\n",
      "Epoch 25514/30000 Training Loss: 0.07418011873960495\n",
      "Epoch 25515/30000 Training Loss: 0.0671687051653862\n",
      "Epoch 25516/30000 Training Loss: 0.05166313052177429\n",
      "Epoch 25517/30000 Training Loss: 0.057934198528528214\n",
      "Epoch 25518/30000 Training Loss: 0.06413065642118454\n",
      "Epoch 25519/30000 Training Loss: 0.07349402457475662\n",
      "Epoch 25520/30000 Training Loss: 0.06679817289113998\n",
      "Epoch 25520/30000 Validation Loss: 0.08134254068136215\n",
      "Epoch 25521/30000 Training Loss: 0.07317019253969193\n",
      "Epoch 25522/30000 Training Loss: 0.06693577766418457\n",
      "Epoch 25523/30000 Training Loss: 0.07256799191236496\n",
      "Epoch 25524/30000 Training Loss: 0.059776853770017624\n",
      "Epoch 25525/30000 Training Loss: 0.07661417871713638\n",
      "Epoch 25526/30000 Training Loss: 0.0687701478600502\n",
      "Epoch 25527/30000 Training Loss: 0.06615936756134033\n",
      "Epoch 25528/30000 Training Loss: 0.08361434191465378\n",
      "Epoch 25529/30000 Training Loss: 0.06783431768417358\n",
      "Epoch 25530/30000 Training Loss: 0.08173560351133347\n",
      "Epoch 25530/30000 Validation Loss: 0.0691271498799324\n",
      "Epoch 25531/30000 Training Loss: 0.0635090097784996\n",
      "Epoch 25532/30000 Training Loss: 0.06450601667165756\n",
      "Epoch 25533/30000 Training Loss: 0.07201376557350159\n",
      "Epoch 25534/30000 Training Loss: 0.07316634058952332\n",
      "Epoch 25535/30000 Training Loss: 0.07266279309988022\n",
      "Epoch 25536/30000 Training Loss: 0.06913019716739655\n",
      "Epoch 25537/30000 Training Loss: 0.08501248806715012\n",
      "Epoch 25538/30000 Training Loss: 0.0721825584769249\n",
      "Epoch 25539/30000 Training Loss: 0.0646655485033989\n",
      "Epoch 25540/30000 Training Loss: 0.0675191879272461\n",
      "Epoch 25540/30000 Validation Loss: 0.07323863357305527\n",
      "Epoch 25541/30000 Training Loss: 0.0757853165268898\n",
      "Epoch 25542/30000 Training Loss: 0.09244219213724136\n",
      "Epoch 25543/30000 Training Loss: 0.08025804907083511\n",
      "Epoch 25544/30000 Training Loss: 0.061074066907167435\n",
      "Epoch 25545/30000 Training Loss: 0.06508973240852356\n",
      "Epoch 25546/30000 Training Loss: 0.0686623677611351\n",
      "Epoch 25547/30000 Training Loss: 0.07497865706682205\n",
      "Epoch 25548/30000 Training Loss: 0.06647119671106339\n",
      "Epoch 25549/30000 Training Loss: 0.05899462103843689\n",
      "Epoch 25550/30000 Training Loss: 0.055894020944833755\n",
      "Epoch 25550/30000 Validation Loss: 0.06922558695077896\n",
      "Epoch 25551/30000 Training Loss: 0.06488042324781418\n",
      "Epoch 25552/30000 Training Loss: 0.04937693104147911\n",
      "Epoch 25553/30000 Training Loss: 0.06391168385744095\n",
      "Epoch 25554/30000 Training Loss: 0.055792633444070816\n",
      "Epoch 25555/30000 Training Loss: 0.07927460223436356\n",
      "Epoch 25556/30000 Training Loss: 0.05648570880293846\n",
      "Epoch 25557/30000 Training Loss: 0.05691329762339592\n",
      "Epoch 25558/30000 Training Loss: 0.05695076659321785\n",
      "Epoch 25559/30000 Training Loss: 0.06019622087478638\n",
      "Epoch 25560/30000 Training Loss: 0.0708271786570549\n",
      "Epoch 25560/30000 Validation Loss: 0.0730246976017952\n",
      "Epoch 25561/30000 Training Loss: 0.06797510385513306\n",
      "Epoch 25562/30000 Training Loss: 0.07444536685943604\n",
      "Epoch 25563/30000 Training Loss: 0.06067930534482002\n",
      "Epoch 25564/30000 Training Loss: 0.06554005295038223\n",
      "Epoch 25565/30000 Training Loss: 0.06720194220542908\n",
      "Epoch 25566/30000 Training Loss: 0.06401579827070236\n",
      "Epoch 25567/30000 Training Loss: 0.05745494365692139\n",
      "Epoch 25568/30000 Training Loss: 0.0556735061109066\n",
      "Epoch 25569/30000 Training Loss: 0.07370542734861374\n",
      "Epoch 25570/30000 Training Loss: 0.07659118622541428\n",
      "Epoch 25570/30000 Validation Loss: 0.07478100061416626\n",
      "Epoch 25571/30000 Training Loss: 0.07620099931955338\n",
      "Epoch 25572/30000 Training Loss: 0.07353033125400543\n",
      "Epoch 25573/30000 Training Loss: 0.05208921805024147\n",
      "Epoch 25574/30000 Training Loss: 0.07217887789011002\n",
      "Epoch 25575/30000 Training Loss: 0.08662337809801102\n",
      "Epoch 25576/30000 Training Loss: 0.08012521266937256\n",
      "Epoch 25577/30000 Training Loss: 0.06284875422716141\n",
      "Epoch 25578/30000 Training Loss: 0.06793876737356186\n",
      "Epoch 25579/30000 Training Loss: 0.061282772570848465\n",
      "Epoch 25580/30000 Training Loss: 0.06773007661104202\n",
      "Epoch 25580/30000 Validation Loss: 0.07308261841535568\n",
      "Epoch 25581/30000 Training Loss: 0.08756708353757858\n",
      "Epoch 25582/30000 Training Loss: 0.08329464495182037\n",
      "Epoch 25583/30000 Training Loss: 0.08776450157165527\n",
      "Epoch 25584/30000 Training Loss: 0.08430535346269608\n",
      "Epoch 25585/30000 Training Loss: 0.06330433487892151\n",
      "Epoch 25586/30000 Training Loss: 0.08289758116006851\n",
      "Epoch 25587/30000 Training Loss: 0.0814826563000679\n",
      "Epoch 25588/30000 Training Loss: 0.05747706815600395\n",
      "Epoch 25589/30000 Training Loss: 0.07641807943582535\n",
      "Epoch 25590/30000 Training Loss: 0.07003137469291687\n",
      "Epoch 25590/30000 Validation Loss: 0.07516563683748245\n",
      "Epoch 25591/30000 Training Loss: 0.06715961545705795\n",
      "Epoch 25592/30000 Training Loss: 0.05826367437839508\n",
      "Epoch 25593/30000 Training Loss: 0.08409067988395691\n",
      "Epoch 25594/30000 Training Loss: 0.07606199383735657\n",
      "Epoch 25595/30000 Training Loss: 0.06958877295255661\n",
      "Epoch 25596/30000 Training Loss: 0.056444212794303894\n",
      "Epoch 25597/30000 Training Loss: 0.058173973113298416\n",
      "Epoch 25598/30000 Training Loss: 0.05048239603638649\n",
      "Epoch 25599/30000 Training Loss: 0.08156531304121017\n",
      "Epoch 25600/30000 Training Loss: 0.07517172396183014\n",
      "Epoch 25600/30000 Validation Loss: 0.07867167145013809\n",
      "Epoch 25601/30000 Training Loss: 0.08926435559988022\n",
      "Epoch 25602/30000 Training Loss: 0.05182856321334839\n",
      "Epoch 25603/30000 Training Loss: 0.06046272814273834\n",
      "Epoch 25604/30000 Training Loss: 0.04938238859176636\n",
      "Epoch 25605/30000 Training Loss: 0.05335969850420952\n",
      "Epoch 25606/30000 Training Loss: 0.06762120127677917\n",
      "Epoch 25607/30000 Training Loss: 0.0722564086318016\n",
      "Epoch 25608/30000 Training Loss: 0.06245226040482521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25609/30000 Training Loss: 0.06977538019418716\n",
      "Epoch 25610/30000 Training Loss: 0.06797151267528534\n",
      "Epoch 25610/30000 Validation Loss: 0.06726890057325363\n",
      "Epoch 25611/30000 Training Loss: 0.0714046061038971\n",
      "Epoch 25612/30000 Training Loss: 0.07539661973714828\n",
      "Epoch 25613/30000 Training Loss: 0.06812389194965363\n",
      "Epoch 25614/30000 Training Loss: 0.06162024661898613\n",
      "Epoch 25615/30000 Training Loss: 0.07444142550230026\n",
      "Epoch 25616/30000 Training Loss: 0.0881725326180458\n",
      "Epoch 25617/30000 Training Loss: 0.06269150227308273\n",
      "Epoch 25618/30000 Training Loss: 0.06419694423675537\n",
      "Epoch 25619/30000 Training Loss: 0.06562299281358719\n",
      "Epoch 25620/30000 Training Loss: 0.06729169934988022\n",
      "Epoch 25620/30000 Validation Loss: 0.07048838585615158\n",
      "Epoch 25621/30000 Training Loss: 0.06461315602064133\n",
      "Epoch 25622/30000 Training Loss: 0.06532769650220871\n",
      "Epoch 25623/30000 Training Loss: 0.07055791467428207\n",
      "Epoch 25624/30000 Training Loss: 0.07342660427093506\n",
      "Epoch 25625/30000 Training Loss: 0.07329528778791428\n",
      "Epoch 25626/30000 Training Loss: 0.07820723205804825\n",
      "Epoch 25627/30000 Training Loss: 0.07241401076316833\n",
      "Epoch 25628/30000 Training Loss: 0.06319015473127365\n",
      "Epoch 25629/30000 Training Loss: 0.07449016720056534\n",
      "Epoch 25630/30000 Training Loss: 0.07030311226844788\n",
      "Epoch 25630/30000 Validation Loss: 0.07249365001916885\n",
      "Epoch 25631/30000 Training Loss: 0.07040494680404663\n",
      "Epoch 25632/30000 Training Loss: 0.08841872960329056\n",
      "Epoch 25633/30000 Training Loss: 0.05787009000778198\n",
      "Epoch 25634/30000 Training Loss: 0.0660143569111824\n",
      "Epoch 25635/30000 Training Loss: 0.059231508523225784\n",
      "Epoch 25636/30000 Training Loss: 0.09375738352537155\n",
      "Epoch 25637/30000 Training Loss: 0.06141277030110359\n",
      "Epoch 25638/30000 Training Loss: 0.07044783979654312\n",
      "Epoch 25639/30000 Training Loss: 0.07756635546684265\n",
      "Epoch 25640/30000 Training Loss: 0.07188105583190918\n",
      "Epoch 25640/30000 Validation Loss: 0.0768468976020813\n",
      "Epoch 25641/30000 Training Loss: 0.06721276044845581\n",
      "Epoch 25642/30000 Training Loss: 0.060134973376989365\n",
      "Epoch 25643/30000 Training Loss: 0.06497139483690262\n",
      "Epoch 25644/30000 Training Loss: 0.08060797303915024\n",
      "Epoch 25645/30000 Training Loss: 0.07884076982736588\n",
      "Epoch 25646/30000 Training Loss: 0.08234808593988419\n",
      "Epoch 25647/30000 Training Loss: 0.06758922338485718\n",
      "Epoch 25648/30000 Training Loss: 0.06429552286863327\n",
      "Epoch 25649/30000 Training Loss: 0.08521594852209091\n",
      "Epoch 25650/30000 Training Loss: 0.06198678910732269\n",
      "Epoch 25650/30000 Validation Loss: 0.0533326081931591\n",
      "Epoch 25651/30000 Training Loss: 0.06266546994447708\n",
      "Epoch 25652/30000 Training Loss: 0.0643378272652626\n",
      "Epoch 25653/30000 Training Loss: 0.07099275290966034\n",
      "Epoch 25654/30000 Training Loss: 0.058153580874204636\n",
      "Epoch 25655/30000 Training Loss: 0.06968186050653458\n",
      "Epoch 25656/30000 Training Loss: 0.05695458874106407\n",
      "Epoch 25657/30000 Training Loss: 0.05953555926680565\n",
      "Epoch 25658/30000 Training Loss: 0.05549393594264984\n",
      "Epoch 25659/30000 Training Loss: 0.07734456658363342\n",
      "Epoch 25660/30000 Training Loss: 0.06042938306927681\n",
      "Epoch 25660/30000 Validation Loss: 0.07070177048444748\n",
      "Epoch 25661/30000 Training Loss: 0.06500589847564697\n",
      "Epoch 25662/30000 Training Loss: 0.05993594601750374\n",
      "Epoch 25663/30000 Training Loss: 0.06104506924748421\n",
      "Epoch 25664/30000 Training Loss: 0.07079955190420151\n",
      "Epoch 25665/30000 Training Loss: 0.05029640719294548\n",
      "Epoch 25666/30000 Training Loss: 0.06695792078971863\n",
      "Epoch 25667/30000 Training Loss: 0.08090609312057495\n",
      "Epoch 25668/30000 Training Loss: 0.07533646374940872\n",
      "Epoch 25669/30000 Training Loss: 0.07236737757921219\n",
      "Epoch 25670/30000 Training Loss: 0.06763862818479538\n",
      "Epoch 25670/30000 Validation Loss: 0.060211341828107834\n",
      "Epoch 25671/30000 Training Loss: 0.07199891656637192\n",
      "Epoch 25672/30000 Training Loss: 0.06782113760709763\n",
      "Epoch 25673/30000 Training Loss: 0.08029964566230774\n",
      "Epoch 25674/30000 Training Loss: 0.08229099959135056\n",
      "Epoch 25675/30000 Training Loss: 0.06945938616991043\n",
      "Epoch 25676/30000 Training Loss: 0.08972964435815811\n",
      "Epoch 25677/30000 Training Loss: 0.07150042802095413\n",
      "Epoch 25678/30000 Training Loss: 0.06701148301362991\n",
      "Epoch 25679/30000 Training Loss: 0.07922416180372238\n",
      "Epoch 25680/30000 Training Loss: 0.07859639078378677\n",
      "Epoch 25680/30000 Validation Loss: 0.07320316880941391\n",
      "Epoch 25681/30000 Training Loss: 0.0656253844499588\n",
      "Epoch 25682/30000 Training Loss: 0.058181583881378174\n",
      "Epoch 25683/30000 Training Loss: 0.05928667262196541\n",
      "Epoch 25684/30000 Training Loss: 0.06858374923467636\n",
      "Epoch 25685/30000 Training Loss: 0.08585762977600098\n",
      "Epoch 25686/30000 Training Loss: 0.06014041602611542\n",
      "Epoch 25687/30000 Training Loss: 0.07006684690713882\n",
      "Epoch 25688/30000 Training Loss: 0.05699289217591286\n",
      "Epoch 25689/30000 Training Loss: 0.0682477056980133\n",
      "Epoch 25690/30000 Training Loss: 0.0817316547036171\n",
      "Epoch 25690/30000 Validation Loss: 0.0662335678935051\n",
      "Epoch 25691/30000 Training Loss: 0.06325749307870865\n",
      "Epoch 25692/30000 Training Loss: 0.07135670632123947\n",
      "Epoch 25693/30000 Training Loss: 0.07568157464265823\n",
      "Epoch 25694/30000 Training Loss: 0.08138734102249146\n",
      "Epoch 25695/30000 Training Loss: 0.06590292602777481\n",
      "Epoch 25696/30000 Training Loss: 0.07373235374689102\n",
      "Epoch 25697/30000 Training Loss: 0.06963945180177689\n",
      "Epoch 25698/30000 Training Loss: 0.06984352320432663\n",
      "Epoch 25699/30000 Training Loss: 0.07517698407173157\n",
      "Epoch 25700/30000 Training Loss: 0.07289781421422958\n",
      "Epoch 25700/30000 Validation Loss: 0.07225435227155685\n",
      "Epoch 25701/30000 Training Loss: 0.050192225724458694\n",
      "Epoch 25702/30000 Training Loss: 0.08825641870498657\n",
      "Epoch 25703/30000 Training Loss: 0.06339873373508453\n",
      "Epoch 25704/30000 Training Loss: 0.08465269953012466\n",
      "Epoch 25705/30000 Training Loss: 0.07563487440347672\n",
      "Epoch 25706/30000 Training Loss: 0.08597003668546677\n",
      "Epoch 25707/30000 Training Loss: 0.06399746984243393\n",
      "Epoch 25708/30000 Training Loss: 0.07689879089593887\n",
      "Epoch 25709/30000 Training Loss: 0.06613819301128387\n",
      "Epoch 25710/30000 Training Loss: 0.06173032894730568\n",
      "Epoch 25710/30000 Validation Loss: 0.06705057621002197\n",
      "Epoch 25711/30000 Training Loss: 0.06937864422798157\n",
      "Epoch 25712/30000 Training Loss: 0.07298103719949722\n",
      "Epoch 25713/30000 Training Loss: 0.06669456511735916\n",
      "Epoch 25714/30000 Training Loss: 0.053502652794122696\n",
      "Epoch 25715/30000 Training Loss: 0.07921444624662399\n",
      "Epoch 25716/30000 Training Loss: 0.06350340694189072\n",
      "Epoch 25717/30000 Training Loss: 0.06105056405067444\n",
      "Epoch 25718/30000 Training Loss: 0.06317981332540512\n",
      "Epoch 25719/30000 Training Loss: 0.06816618889570236\n",
      "Epoch 25720/30000 Training Loss: 0.05753372982144356\n",
      "Epoch 25720/30000 Validation Loss: 0.07480693608522415\n",
      "Epoch 25721/30000 Training Loss: 0.08347663283348083\n",
      "Epoch 25722/30000 Training Loss: 0.07880492508411407\n",
      "Epoch 25723/30000 Training Loss: 0.07049950212240219\n",
      "Epoch 25724/30000 Training Loss: 0.06838975101709366\n",
      "Epoch 25725/30000 Training Loss: 0.07431197911500931\n",
      "Epoch 25726/30000 Training Loss: 0.08108067512512207\n",
      "Epoch 25727/30000 Training Loss: 0.05178504064679146\n",
      "Epoch 25728/30000 Training Loss: 0.07213127613067627\n",
      "Epoch 25729/30000 Training Loss: 0.07084160298109055\n",
      "Epoch 25730/30000 Training Loss: 0.08002972602844238\n",
      "Epoch 25730/30000 Validation Loss: 0.055510472506284714\n",
      "Epoch 25731/30000 Training Loss: 0.05281443893909454\n",
      "Epoch 25732/30000 Training Loss: 0.05106667801737785\n",
      "Epoch 25733/30000 Training Loss: 0.05516761541366577\n",
      "Epoch 25734/30000 Training Loss: 0.05853256583213806\n",
      "Epoch 25735/30000 Training Loss: 0.09528756141662598\n",
      "Epoch 25736/30000 Training Loss: 0.06028395891189575\n",
      "Epoch 25737/30000 Training Loss: 0.06680654734373093\n",
      "Epoch 25738/30000 Training Loss: 0.06709682196378708\n",
      "Epoch 25739/30000 Training Loss: 0.056925784796476364\n",
      "Epoch 25740/30000 Training Loss: 0.0825158879160881\n",
      "Epoch 25740/30000 Validation Loss: 0.07239049673080444\n",
      "Epoch 25741/30000 Training Loss: 0.06438497453927994\n",
      "Epoch 25742/30000 Training Loss: 0.10087154060602188\n",
      "Epoch 25743/30000 Training Loss: 0.08065900206565857\n",
      "Epoch 25744/30000 Training Loss: 0.08543912321329117\n",
      "Epoch 25745/30000 Training Loss: 0.07224909216165543\n",
      "Epoch 25746/30000 Training Loss: 0.05661192536354065\n",
      "Epoch 25747/30000 Training Loss: 0.08408832550048828\n",
      "Epoch 25748/30000 Training Loss: 0.09046008437871933\n",
      "Epoch 25749/30000 Training Loss: 0.07009715586900711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25750/30000 Training Loss: 0.0881216749548912\n",
      "Epoch 25750/30000 Validation Loss: 0.07512006908655167\n",
      "Epoch 25751/30000 Training Loss: 0.0736285150051117\n",
      "Epoch 25752/30000 Training Loss: 0.07960771769285202\n",
      "Epoch 25753/30000 Training Loss: 0.0674193724989891\n",
      "Epoch 25754/30000 Training Loss: 0.060500454157590866\n",
      "Epoch 25755/30000 Training Loss: 0.0675998404622078\n",
      "Epoch 25756/30000 Training Loss: 0.06910400837659836\n",
      "Epoch 25757/30000 Training Loss: 0.0501168854534626\n",
      "Epoch 25758/30000 Training Loss: 0.05961422249674797\n",
      "Epoch 25759/30000 Training Loss: 0.07446185499429703\n",
      "Epoch 25760/30000 Training Loss: 0.07902274280786514\n",
      "Epoch 25760/30000 Validation Loss: 0.06390642374753952\n",
      "Epoch 25761/30000 Training Loss: 0.0663074180483818\n",
      "Epoch 25762/30000 Training Loss: 0.06188085675239563\n",
      "Epoch 25763/30000 Training Loss: 0.05617077276110649\n",
      "Epoch 25764/30000 Training Loss: 0.07504827529191971\n",
      "Epoch 25765/30000 Training Loss: 0.09090456366539001\n",
      "Epoch 25766/30000 Training Loss: 0.07730603218078613\n",
      "Epoch 25767/30000 Training Loss: 0.06977478414773941\n",
      "Epoch 25768/30000 Training Loss: 0.06474830955266953\n",
      "Epoch 25769/30000 Training Loss: 0.07331880927085876\n",
      "Epoch 25770/30000 Training Loss: 0.07272522896528244\n",
      "Epoch 25770/30000 Validation Loss: 0.06726346164941788\n",
      "Epoch 25771/30000 Training Loss: 0.06598252058029175\n",
      "Epoch 25772/30000 Training Loss: 0.05645555257797241\n",
      "Epoch 25773/30000 Training Loss: 0.051276206970214844\n",
      "Epoch 25774/30000 Training Loss: 0.07041434198617935\n",
      "Epoch 25775/30000 Training Loss: 0.06010918691754341\n",
      "Epoch 25776/30000 Training Loss: 0.08168163150548935\n",
      "Epoch 25777/30000 Training Loss: 0.07695163041353226\n",
      "Epoch 25778/30000 Training Loss: 0.07143548130989075\n",
      "Epoch 25779/30000 Training Loss: 0.062117595225572586\n",
      "Epoch 25780/30000 Training Loss: 0.050200968980789185\n",
      "Epoch 25780/30000 Validation Loss: 0.07169017940759659\n",
      "Epoch 25781/30000 Training Loss: 0.08265898376703262\n",
      "Epoch 25782/30000 Training Loss: 0.06510122120380402\n",
      "Epoch 25783/30000 Training Loss: 0.06336072832345963\n",
      "Epoch 25784/30000 Training Loss: 0.07197294384241104\n",
      "Epoch 25785/30000 Training Loss: 0.07635928690433502\n",
      "Epoch 25786/30000 Training Loss: 0.06695606559515\n",
      "Epoch 25787/30000 Training Loss: 0.07445366680622101\n",
      "Epoch 25788/30000 Training Loss: 0.05721583962440491\n",
      "Epoch 25789/30000 Training Loss: 0.06920767575502396\n",
      "Epoch 25790/30000 Training Loss: 0.06380701810121536\n",
      "Epoch 25790/30000 Validation Loss: 0.059351980686187744\n",
      "Epoch 25791/30000 Training Loss: 0.0824941098690033\n",
      "Epoch 25792/30000 Training Loss: 0.06404345482587814\n",
      "Epoch 25793/30000 Training Loss: 0.0684828981757164\n",
      "Epoch 25794/30000 Training Loss: 0.06252027302980423\n",
      "Epoch 25795/30000 Training Loss: 0.0851888582110405\n",
      "Epoch 25796/30000 Training Loss: 0.0788595899939537\n",
      "Epoch 25797/30000 Training Loss: 0.06871714442968369\n",
      "Epoch 25798/30000 Training Loss: 0.0734056904911995\n",
      "Epoch 25799/30000 Training Loss: 0.060069989413022995\n",
      "Epoch 25800/30000 Training Loss: 0.07174650579690933\n",
      "Epoch 25800/30000 Validation Loss: 0.06706955283880234\n",
      "Epoch 25801/30000 Training Loss: 0.07320643216371536\n",
      "Epoch 25802/30000 Training Loss: 0.07678287476301193\n",
      "Epoch 25803/30000 Training Loss: 0.06315641105175018\n",
      "Epoch 25804/30000 Training Loss: 0.054972101002931595\n",
      "Epoch 25805/30000 Training Loss: 0.07452991604804993\n",
      "Epoch 25806/30000 Training Loss: 0.07874021679162979\n",
      "Epoch 25807/30000 Training Loss: 0.09489860385656357\n",
      "Epoch 25808/30000 Training Loss: 0.08028139919042587\n",
      "Epoch 25809/30000 Training Loss: 0.07139290124177933\n",
      "Epoch 25810/30000 Training Loss: 0.06725678592920303\n",
      "Epoch 25810/30000 Validation Loss: 0.07104922086000443\n",
      "Epoch 25811/30000 Training Loss: 0.06790152937173843\n",
      "Epoch 25812/30000 Training Loss: 0.056155234575271606\n",
      "Epoch 25813/30000 Training Loss: 0.08462607860565186\n",
      "Epoch 25814/30000 Training Loss: 0.06047757342457771\n",
      "Epoch 25815/30000 Training Loss: 0.06914947181940079\n",
      "Epoch 25816/30000 Training Loss: 0.060689885169267654\n",
      "Epoch 25817/30000 Training Loss: 0.060120582580566406\n",
      "Epoch 25818/30000 Training Loss: 0.061522483825683594\n",
      "Epoch 25819/30000 Training Loss: 0.07386147230863571\n",
      "Epoch 25820/30000 Training Loss: 0.0744858980178833\n",
      "Epoch 25820/30000 Validation Loss: 0.07286270707845688\n",
      "Epoch 25821/30000 Training Loss: 0.0654306635260582\n",
      "Epoch 25822/30000 Training Loss: 0.07625839859247208\n",
      "Epoch 25823/30000 Training Loss: 0.05789949372410774\n",
      "Epoch 25824/30000 Training Loss: 0.05474378541111946\n",
      "Epoch 25825/30000 Training Loss: 0.08151455968618393\n",
      "Epoch 25826/30000 Training Loss: 0.07011941075325012\n",
      "Epoch 25827/30000 Training Loss: 0.06078386679291725\n",
      "Epoch 25828/30000 Training Loss: 0.09044602513313293\n",
      "Epoch 25829/30000 Training Loss: 0.06749490648508072\n",
      "Epoch 25830/30000 Training Loss: 0.070077084004879\n",
      "Epoch 25830/30000 Validation Loss: 0.05379614233970642\n",
      "Epoch 25831/30000 Training Loss: 0.06615901738405228\n",
      "Epoch 25832/30000 Training Loss: 0.06615506857633591\n",
      "Epoch 25833/30000 Training Loss: 0.06787783652544022\n",
      "Epoch 25834/30000 Training Loss: 0.08440760523080826\n",
      "Epoch 25835/30000 Training Loss: 0.07556464523077011\n",
      "Epoch 25836/30000 Training Loss: 0.05469149351119995\n",
      "Epoch 25837/30000 Training Loss: 0.06339814513921738\n",
      "Epoch 25838/30000 Training Loss: 0.06701302528381348\n",
      "Epoch 25839/30000 Training Loss: 0.07211238890886307\n",
      "Epoch 25840/30000 Training Loss: 0.08379197865724564\n",
      "Epoch 25840/30000 Validation Loss: 0.050863299518823624\n",
      "Epoch 25841/30000 Training Loss: 0.06847355514764786\n",
      "Epoch 25842/30000 Training Loss: 0.06604092568159103\n",
      "Epoch 25843/30000 Training Loss: 0.08545401692390442\n",
      "Epoch 25844/30000 Training Loss: 0.05628015100955963\n",
      "Epoch 25845/30000 Training Loss: 0.08464040607213974\n",
      "Epoch 25846/30000 Training Loss: 0.07465656846761703\n",
      "Epoch 25847/30000 Training Loss: 0.07344520837068558\n",
      "Epoch 25848/30000 Training Loss: 0.07193558663129807\n",
      "Epoch 25849/30000 Training Loss: 0.07385363429784775\n",
      "Epoch 25850/30000 Training Loss: 0.05749409273266792\n",
      "Epoch 25850/30000 Validation Loss: 0.07784980535507202\n",
      "Epoch 25851/30000 Training Loss: 0.07326831668615341\n",
      "Epoch 25852/30000 Training Loss: 0.05536525323987007\n",
      "Epoch 25853/30000 Training Loss: 0.06772726029157639\n",
      "Epoch 25854/30000 Training Loss: 0.06555677950382233\n",
      "Epoch 25855/30000 Training Loss: 0.05016115680336952\n",
      "Epoch 25856/30000 Training Loss: 0.08165570348501205\n",
      "Epoch 25857/30000 Training Loss: 0.06430071592330933\n",
      "Epoch 25858/30000 Training Loss: 0.09590160846710205\n",
      "Epoch 25859/30000 Training Loss: 0.062457453459501266\n",
      "Epoch 25860/30000 Training Loss: 0.06834157556295395\n",
      "Epoch 25860/30000 Validation Loss: 0.053366780281066895\n",
      "Epoch 25861/30000 Training Loss: 0.06704384833574295\n",
      "Epoch 25862/30000 Training Loss: 0.06368888169527054\n",
      "Epoch 25863/30000 Training Loss: 0.06443356722593307\n",
      "Epoch 25864/30000 Training Loss: 0.05626695230603218\n",
      "Epoch 25865/30000 Training Loss: 0.06125064566731453\n",
      "Epoch 25866/30000 Training Loss: 0.056066010147333145\n",
      "Epoch 25867/30000 Training Loss: 0.07081935554742813\n",
      "Epoch 25868/30000 Training Loss: 0.07737282663583755\n",
      "Epoch 25869/30000 Training Loss: 0.0655420795083046\n",
      "Epoch 25870/30000 Training Loss: 0.09081486612558365\n",
      "Epoch 25870/30000 Validation Loss: 0.06320828199386597\n",
      "Epoch 25871/30000 Training Loss: 0.08512149006128311\n",
      "Epoch 25872/30000 Training Loss: 0.06148582324385643\n",
      "Epoch 25873/30000 Training Loss: 0.06154286861419678\n",
      "Epoch 25874/30000 Training Loss: 0.07187993079423904\n",
      "Epoch 25875/30000 Training Loss: 0.06856611371040344\n",
      "Epoch 25876/30000 Training Loss: 0.06813123822212219\n",
      "Epoch 25877/30000 Training Loss: 0.06092889606952667\n",
      "Epoch 25878/30000 Training Loss: 0.05210452899336815\n",
      "Epoch 25879/30000 Training Loss: 0.06210009753704071\n",
      "Epoch 25880/30000 Training Loss: 0.049433279782533646\n",
      "Epoch 25880/30000 Validation Loss: 0.07417265325784683\n",
      "Epoch 25881/30000 Training Loss: 0.06594374775886536\n",
      "Epoch 25882/30000 Training Loss: 0.059881895780563354\n",
      "Epoch 25883/30000 Training Loss: 0.06747271865606308\n",
      "Epoch 25884/30000 Training Loss: 0.059513915330171585\n",
      "Epoch 25885/30000 Training Loss: 0.0641140341758728\n",
      "Epoch 25886/30000 Training Loss: 0.07018418610095978\n",
      "Epoch 25887/30000 Training Loss: 0.058658063411712646\n",
      "Epoch 25888/30000 Training Loss: 0.04767140746116638\n",
      "Epoch 25889/30000 Training Loss: 0.057668864727020264\n",
      "Epoch 25890/30000 Training Loss: 0.0720691978931427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25890/30000 Validation Loss: 0.05621302127838135\n",
      "Epoch 25891/30000 Training Loss: 0.060085728764534\n",
      "Epoch 25892/30000 Training Loss: 0.06706447154283524\n",
      "Epoch 25893/30000 Training Loss: 0.06613615155220032\n",
      "Epoch 25894/30000 Training Loss: 0.06681934744119644\n",
      "Epoch 25895/30000 Training Loss: 0.06499213725328445\n",
      "Epoch 25896/30000 Training Loss: 0.06421273946762085\n",
      "Epoch 25897/30000 Training Loss: 0.0709507167339325\n",
      "Epoch 25898/30000 Training Loss: 0.06554893404245377\n",
      "Epoch 25899/30000 Training Loss: 0.059192050248384476\n",
      "Epoch 25900/30000 Training Loss: 0.056037787348032\n",
      "Epoch 25900/30000 Validation Loss: 0.08809616416692734\n",
      "Epoch 25901/30000 Training Loss: 0.0825507715344429\n",
      "Epoch 25902/30000 Training Loss: 0.05851330980658531\n",
      "Epoch 25903/30000 Training Loss: 0.07113344222307205\n",
      "Epoch 25904/30000 Training Loss: 0.0636226162314415\n",
      "Epoch 25905/30000 Training Loss: 0.05814368650317192\n",
      "Epoch 25906/30000 Training Loss: 0.05831434205174446\n",
      "Epoch 25907/30000 Training Loss: 0.06966491788625717\n",
      "Epoch 25908/30000 Training Loss: 0.060806870460510254\n",
      "Epoch 25909/30000 Training Loss: 0.06620559841394424\n",
      "Epoch 25910/30000 Training Loss: 0.07259839028120041\n",
      "Epoch 25910/30000 Validation Loss: 0.06883995980024338\n",
      "Epoch 25911/30000 Training Loss: 0.07170641422271729\n",
      "Epoch 25912/30000 Training Loss: 0.06758468598127365\n",
      "Epoch 25913/30000 Training Loss: 0.06605878472328186\n",
      "Epoch 25914/30000 Training Loss: 0.05973948910832405\n",
      "Epoch 25915/30000 Training Loss: 0.06282592564821243\n",
      "Epoch 25916/30000 Training Loss: 0.07223566621541977\n",
      "Epoch 25917/30000 Training Loss: 0.06497278064489365\n",
      "Epoch 25918/30000 Training Loss: 0.04743291810154915\n",
      "Epoch 25919/30000 Training Loss: 0.05782270431518555\n",
      "Epoch 25920/30000 Training Loss: 0.07175151258707047\n",
      "Epoch 25920/30000 Validation Loss: 0.06397996842861176\n",
      "Epoch 25921/30000 Training Loss: 0.06038851663470268\n",
      "Epoch 25922/30000 Training Loss: 0.0666988343000412\n",
      "Epoch 25923/30000 Training Loss: 0.07333394885063171\n",
      "Epoch 25924/30000 Training Loss: 0.055262643843889236\n",
      "Epoch 25925/30000 Training Loss: 0.053585320711135864\n",
      "Epoch 25926/30000 Training Loss: 0.06516557931900024\n",
      "Epoch 25927/30000 Training Loss: 0.07618086040019989\n",
      "Epoch 25928/30000 Training Loss: 0.06198014318943024\n",
      "Epoch 25929/30000 Training Loss: 0.07611491531133652\n",
      "Epoch 25930/30000 Training Loss: 0.0878901407122612\n",
      "Epoch 25930/30000 Validation Loss: 0.07216313481330872\n",
      "Epoch 25931/30000 Training Loss: 0.06885073333978653\n",
      "Epoch 25932/30000 Training Loss: 0.07628747075796127\n",
      "Epoch 25933/30000 Training Loss: 0.08672723174095154\n",
      "Epoch 25934/30000 Training Loss: 0.06718811392784119\n",
      "Epoch 25935/30000 Training Loss: 0.06146254017949104\n",
      "Epoch 25936/30000 Training Loss: 0.07680518925189972\n",
      "Epoch 25937/30000 Training Loss: 0.07754374295473099\n",
      "Epoch 25938/30000 Training Loss: 0.0551464818418026\n",
      "Epoch 25939/30000 Training Loss: 0.06699308753013611\n",
      "Epoch 25940/30000 Training Loss: 0.06750670075416565\n",
      "Epoch 25940/30000 Validation Loss: 0.05239671841263771\n",
      "Epoch 25941/30000 Training Loss: 0.06554406881332397\n",
      "Epoch 25942/30000 Training Loss: 0.09635383635759354\n",
      "Epoch 25943/30000 Training Loss: 0.08818323165178299\n",
      "Epoch 25944/30000 Training Loss: 0.07426366955041885\n",
      "Epoch 25945/30000 Training Loss: 0.07594536989927292\n",
      "Epoch 25946/30000 Training Loss: 0.06437555700540543\n",
      "Epoch 25947/30000 Training Loss: 0.07466719299554825\n",
      "Epoch 25948/30000 Training Loss: 0.06689488887786865\n",
      "Epoch 25949/30000 Training Loss: 0.0687226876616478\n",
      "Epoch 25950/30000 Training Loss: 0.09198492020368576\n",
      "Epoch 25950/30000 Validation Loss: 0.07623085379600525\n",
      "Epoch 25951/30000 Training Loss: 0.06188759580254555\n",
      "Epoch 25952/30000 Training Loss: 0.07495875656604767\n",
      "Epoch 25953/30000 Training Loss: 0.06351882964372635\n",
      "Epoch 25954/30000 Training Loss: 0.07130684703588486\n",
      "Epoch 25955/30000 Training Loss: 0.06278451532125473\n",
      "Epoch 25956/30000 Training Loss: 0.08597340434789658\n",
      "Epoch 25957/30000 Training Loss: 0.07550130784511566\n",
      "Epoch 25958/30000 Training Loss: 0.054708924144506454\n",
      "Epoch 25959/30000 Training Loss: 0.07297708839178085\n",
      "Epoch 25960/30000 Training Loss: 0.07047861069440842\n",
      "Epoch 25960/30000 Validation Loss: 0.06966914981603622\n",
      "Epoch 25961/30000 Training Loss: 0.06205146387219429\n",
      "Epoch 25962/30000 Training Loss: 0.05216539278626442\n",
      "Epoch 25963/30000 Training Loss: 0.0740196704864502\n",
      "Epoch 25964/30000 Training Loss: 0.057917628437280655\n",
      "Epoch 25965/30000 Training Loss: 0.06789066642522812\n",
      "Epoch 25966/30000 Training Loss: 0.059932857751846313\n",
      "Epoch 25967/30000 Training Loss: 0.04847157001495361\n",
      "Epoch 25968/30000 Training Loss: 0.07371840626001358\n",
      "Epoch 25969/30000 Training Loss: 0.061737436801195145\n",
      "Epoch 25970/30000 Training Loss: 0.06465139985084534\n",
      "Epoch 25970/30000 Validation Loss: 0.07880300283432007\n",
      "Epoch 25971/30000 Training Loss: 0.06312250345945358\n",
      "Epoch 25972/30000 Training Loss: 0.08618836849927902\n",
      "Epoch 25973/30000 Training Loss: 0.07974659651517868\n",
      "Epoch 25974/30000 Training Loss: 0.06024882197380066\n",
      "Epoch 25975/30000 Training Loss: 0.07354097813367844\n",
      "Epoch 25976/30000 Training Loss: 0.06500318646430969\n",
      "Epoch 25977/30000 Training Loss: 0.060213565826416016\n",
      "Epoch 25978/30000 Training Loss: 0.06862103939056396\n",
      "Epoch 25979/30000 Training Loss: 0.06138922646641731\n",
      "Epoch 25980/30000 Training Loss: 0.07891645282506943\n",
      "Epoch 25980/30000 Validation Loss: 0.07627930492162704\n",
      "Epoch 25981/30000 Training Loss: 0.06722446531057358\n",
      "Epoch 25982/30000 Training Loss: 0.07248380035161972\n",
      "Epoch 25983/30000 Training Loss: 0.07525572925806046\n",
      "Epoch 25984/30000 Training Loss: 0.06934871524572372\n",
      "Epoch 25985/30000 Training Loss: 0.06994658708572388\n",
      "Epoch 25986/30000 Training Loss: 0.06553060561418533\n",
      "Epoch 25987/30000 Training Loss: 0.07461071014404297\n",
      "Epoch 25988/30000 Training Loss: 0.07506858557462692\n",
      "Epoch 25989/30000 Training Loss: 0.06517426669597626\n",
      "Epoch 25990/30000 Training Loss: 0.06905587762594223\n",
      "Epoch 25990/30000 Validation Loss: 0.07471156120300293\n",
      "Epoch 25991/30000 Training Loss: 0.06358110159635544\n",
      "Epoch 25992/30000 Training Loss: 0.06392395496368408\n",
      "Epoch 25993/30000 Training Loss: 0.07577487081289291\n",
      "Epoch 25994/30000 Training Loss: 0.06747543811798096\n",
      "Epoch 25995/30000 Training Loss: 0.08743223547935486\n",
      "Epoch 25996/30000 Training Loss: 0.07982944697141647\n",
      "Epoch 25997/30000 Training Loss: 0.0822247788310051\n",
      "Epoch 25998/30000 Training Loss: 0.06092235445976257\n",
      "Epoch 25999/30000 Training Loss: 0.057439107447862625\n",
      "Epoch 26000/30000 Training Loss: 0.0688306987285614\n",
      "Epoch 26000/30000 Validation Loss: 0.07681039720773697\n",
      "Epoch 26001/30000 Training Loss: 0.05439592897891998\n",
      "Epoch 26002/30000 Training Loss: 0.07988223433494568\n",
      "Epoch 26003/30000 Training Loss: 0.05233216658234596\n",
      "Epoch 26004/30000 Training Loss: 0.08112579584121704\n",
      "Epoch 26005/30000 Training Loss: 0.07025545835494995\n",
      "Epoch 26006/30000 Training Loss: 0.06311418861150742\n",
      "Epoch 26007/30000 Training Loss: 0.08102782070636749\n",
      "Epoch 26008/30000 Training Loss: 0.06184150278568268\n",
      "Epoch 26009/30000 Training Loss: 0.06396356970071793\n",
      "Epoch 26010/30000 Training Loss: 0.07395480573177338\n",
      "Epoch 26010/30000 Validation Loss: 0.06537248194217682\n",
      "Epoch 26011/30000 Training Loss: 0.07955236732959747\n",
      "Epoch 26012/30000 Training Loss: 0.06172720715403557\n",
      "Epoch 26013/30000 Training Loss: 0.0652727410197258\n",
      "Epoch 26014/30000 Training Loss: 0.07100985944271088\n",
      "Epoch 26015/30000 Training Loss: 0.06876232475042343\n",
      "Epoch 26016/30000 Training Loss: 0.07627106457948685\n",
      "Epoch 26017/30000 Training Loss: 0.09149431437253952\n",
      "Epoch 26018/30000 Training Loss: 0.05228051543235779\n",
      "Epoch 26019/30000 Training Loss: 0.07924171537160873\n",
      "Epoch 26020/30000 Training Loss: 0.07010918110609055\n",
      "Epoch 26020/30000 Validation Loss: 0.06961935013532639\n",
      "Epoch 26021/30000 Training Loss: 0.06903236359357834\n",
      "Epoch 26022/30000 Training Loss: 0.06366293877363205\n",
      "Epoch 26023/30000 Training Loss: 0.05601410195231438\n",
      "Epoch 26024/30000 Training Loss: 0.06421022117137909\n",
      "Epoch 26025/30000 Training Loss: 0.08363088220357895\n",
      "Epoch 26026/30000 Training Loss: 0.07539155334234238\n",
      "Epoch 26027/30000 Training Loss: 0.0796828642487526\n",
      "Epoch 26028/30000 Training Loss: 0.06624175608158112\n",
      "Epoch 26029/30000 Training Loss: 0.05804025009274483\n",
      "Epoch 26030/30000 Training Loss: 0.07338040322065353\n",
      "Epoch 26030/30000 Validation Loss: 0.08423937112092972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26031/30000 Training Loss: 0.06711845844984055\n",
      "Epoch 26032/30000 Training Loss: 0.056364793330430984\n",
      "Epoch 26033/30000 Training Loss: 0.08696398138999939\n",
      "Epoch 26034/30000 Training Loss: 0.06968756765127182\n",
      "Epoch 26035/30000 Training Loss: 0.06205855682492256\n",
      "Epoch 26036/30000 Training Loss: 0.0804022029042244\n",
      "Epoch 26037/30000 Training Loss: 0.06763754040002823\n",
      "Epoch 26038/30000 Training Loss: 0.058996737003326416\n",
      "Epoch 26039/30000 Training Loss: 0.04199402779340744\n",
      "Epoch 26040/30000 Training Loss: 0.05702394247055054\n",
      "Epoch 26040/30000 Validation Loss: 0.0661124661564827\n",
      "Epoch 26041/30000 Training Loss: 0.049792394042015076\n",
      "Epoch 26042/30000 Training Loss: 0.09781714528799057\n",
      "Epoch 26043/30000 Training Loss: 0.0751420184969902\n",
      "Epoch 26044/30000 Training Loss: 0.07440602034330368\n",
      "Epoch 26045/30000 Training Loss: 0.060324400663375854\n",
      "Epoch 26046/30000 Training Loss: 0.07326369732618332\n",
      "Epoch 26047/30000 Training Loss: 0.07502224296331406\n",
      "Epoch 26048/30000 Training Loss: 0.07330869883298874\n",
      "Epoch 26049/30000 Training Loss: 0.07260798662900925\n",
      "Epoch 26050/30000 Training Loss: 0.08685186505317688\n",
      "Epoch 26050/30000 Validation Loss: 0.07937345653772354\n",
      "Epoch 26051/30000 Training Loss: 0.0590217299759388\n",
      "Epoch 26052/30000 Training Loss: 0.0682489275932312\n",
      "Epoch 26053/30000 Training Loss: 0.06510830670595169\n",
      "Epoch 26054/30000 Training Loss: 0.07685134559869766\n",
      "Epoch 26055/30000 Training Loss: 0.05240793153643608\n",
      "Epoch 26056/30000 Training Loss: 0.07604417949914932\n",
      "Epoch 26057/30000 Training Loss: 0.060078054666519165\n",
      "Epoch 26058/30000 Training Loss: 0.063599593937397\n",
      "Epoch 26059/30000 Training Loss: 0.0788348838686943\n",
      "Epoch 26060/30000 Training Loss: 0.07131115347146988\n",
      "Epoch 26060/30000 Validation Loss: 0.06438600271940231\n",
      "Epoch 26061/30000 Training Loss: 0.05564194917678833\n",
      "Epoch 26062/30000 Training Loss: 0.08058212697505951\n",
      "Epoch 26063/30000 Training Loss: 0.05986694619059563\n",
      "Epoch 26064/30000 Training Loss: 0.07280480861663818\n",
      "Epoch 26065/30000 Training Loss: 0.05857746675610542\n",
      "Epoch 26066/30000 Training Loss: 0.06353143602609634\n",
      "Epoch 26067/30000 Training Loss: 0.06754240393638611\n",
      "Epoch 26068/30000 Training Loss: 0.0758906677365303\n",
      "Epoch 26069/30000 Training Loss: 0.05868658795952797\n",
      "Epoch 26070/30000 Training Loss: 0.09395483881235123\n",
      "Epoch 26070/30000 Validation Loss: 0.05889495089650154\n",
      "Epoch 26071/30000 Training Loss: 0.07354170083999634\n",
      "Epoch 26072/30000 Training Loss: 0.05220287665724754\n",
      "Epoch 26073/30000 Training Loss: 0.050786226987838745\n",
      "Epoch 26074/30000 Training Loss: 0.07324416190385818\n",
      "Epoch 26075/30000 Training Loss: 0.0656118392944336\n",
      "Epoch 26076/30000 Training Loss: 0.07477427273988724\n",
      "Epoch 26077/30000 Training Loss: 0.05336418375372887\n",
      "Epoch 26078/30000 Training Loss: 0.060672830790281296\n",
      "Epoch 26079/30000 Training Loss: 0.07622065395116806\n",
      "Epoch 26080/30000 Training Loss: 0.07032258063554764\n",
      "Epoch 26080/30000 Validation Loss: 0.07673899084329605\n",
      "Epoch 26081/30000 Training Loss: 0.059874486178159714\n",
      "Epoch 26082/30000 Training Loss: 0.06463932245969772\n",
      "Epoch 26083/30000 Training Loss: 0.06344389915466309\n",
      "Epoch 26084/30000 Training Loss: 0.06230379641056061\n",
      "Epoch 26085/30000 Training Loss: 0.09103947877883911\n",
      "Epoch 26086/30000 Training Loss: 0.10307637602090836\n",
      "Epoch 26087/30000 Training Loss: 0.06701651960611343\n",
      "Epoch 26088/30000 Training Loss: 0.06643928587436676\n",
      "Epoch 26089/30000 Training Loss: 0.0700652152299881\n",
      "Epoch 26090/30000 Training Loss: 0.0733848437666893\n",
      "Epoch 26090/30000 Validation Loss: 0.059491727501153946\n",
      "Epoch 26091/30000 Training Loss: 0.0752621591091156\n",
      "Epoch 26092/30000 Training Loss: 0.07539505511522293\n",
      "Epoch 26093/30000 Training Loss: 0.0624459944665432\n",
      "Epoch 26094/30000 Training Loss: 0.07193120568990707\n",
      "Epoch 26095/30000 Training Loss: 0.05935713276267052\n",
      "Epoch 26096/30000 Training Loss: 0.08387159556150436\n",
      "Epoch 26097/30000 Training Loss: 0.07033614814281464\n",
      "Epoch 26098/30000 Training Loss: 0.07837966084480286\n",
      "Epoch 26099/30000 Training Loss: 0.06268364936113358\n",
      "Epoch 26100/30000 Training Loss: 0.052713435143232346\n",
      "Epoch 26100/30000 Validation Loss: 0.08854914456605911\n",
      "Epoch 26101/30000 Training Loss: 0.07403676956892014\n",
      "Epoch 26102/30000 Training Loss: 0.0765867680311203\n",
      "Epoch 26103/30000 Training Loss: 0.06935697048902512\n",
      "Epoch 26104/30000 Training Loss: 0.06995037198066711\n",
      "Epoch 26105/30000 Training Loss: 0.06089312955737114\n",
      "Epoch 26106/30000 Training Loss: 0.06734149903059006\n",
      "Epoch 26107/30000 Training Loss: 0.07676351815462112\n",
      "Epoch 26108/30000 Training Loss: 0.0696796104311943\n",
      "Epoch 26109/30000 Training Loss: 0.07609482854604721\n",
      "Epoch 26110/30000 Training Loss: 0.08749379962682724\n",
      "Epoch 26110/30000 Validation Loss: 0.08860176801681519\n",
      "Epoch 26111/30000 Training Loss: 0.06367091089487076\n",
      "Epoch 26112/30000 Training Loss: 0.05901646614074707\n",
      "Epoch 26113/30000 Training Loss: 0.05859878659248352\n",
      "Epoch 26114/30000 Training Loss: 0.07646258920431137\n",
      "Epoch 26115/30000 Training Loss: 0.07554543763399124\n",
      "Epoch 26116/30000 Training Loss: 0.08436962962150574\n",
      "Epoch 26117/30000 Training Loss: 0.0734512209892273\n",
      "Epoch 26118/30000 Training Loss: 0.07480999082326889\n",
      "Epoch 26119/30000 Training Loss: 0.06598114222288132\n",
      "Epoch 26120/30000 Training Loss: 0.0763435959815979\n",
      "Epoch 26120/30000 Validation Loss: 0.07649710029363632\n",
      "Epoch 26121/30000 Training Loss: 0.0665760338306427\n",
      "Epoch 26122/30000 Training Loss: 0.0713057890534401\n",
      "Epoch 26123/30000 Training Loss: 0.07859638333320618\n",
      "Epoch 26124/30000 Training Loss: 0.06283486634492874\n",
      "Epoch 26125/30000 Training Loss: 0.06314293295145035\n",
      "Epoch 26126/30000 Training Loss: 0.06616557389497757\n",
      "Epoch 26127/30000 Training Loss: 0.06612281501293182\n",
      "Epoch 26128/30000 Training Loss: 0.08117661625146866\n",
      "Epoch 26129/30000 Training Loss: 0.05518028140068054\n",
      "Epoch 26130/30000 Training Loss: 0.07166247814893723\n",
      "Epoch 26130/30000 Validation Loss: 0.08327490091323853\n",
      "Epoch 26131/30000 Training Loss: 0.07086619734764099\n",
      "Epoch 26132/30000 Training Loss: 0.0755217969417572\n",
      "Epoch 26133/30000 Training Loss: 0.059473831206560135\n",
      "Epoch 26134/30000 Training Loss: 0.07040659338235855\n",
      "Epoch 26135/30000 Training Loss: 0.05111643299460411\n",
      "Epoch 26136/30000 Training Loss: 0.07593721151351929\n",
      "Epoch 26137/30000 Training Loss: 0.06738018989562988\n",
      "Epoch 26138/30000 Training Loss: 0.065682552754879\n",
      "Epoch 26139/30000 Training Loss: 0.0803733840584755\n",
      "Epoch 26140/30000 Training Loss: 0.061396270990371704\n",
      "Epoch 26140/30000 Validation Loss: 0.059144701808691025\n",
      "Epoch 26141/30000 Training Loss: 0.06324071437120438\n",
      "Epoch 26142/30000 Training Loss: 0.06403055042028427\n",
      "Epoch 26143/30000 Training Loss: 0.06668061763048172\n",
      "Epoch 26144/30000 Training Loss: 0.06476789712905884\n",
      "Epoch 26145/30000 Training Loss: 0.05536370351910591\n",
      "Epoch 26146/30000 Training Loss: 0.06894803792238235\n",
      "Epoch 26147/30000 Training Loss: 0.08172702044248581\n",
      "Epoch 26148/30000 Training Loss: 0.08302170038223267\n",
      "Epoch 26149/30000 Training Loss: 0.07864208519458771\n",
      "Epoch 26150/30000 Training Loss: 0.06923971325159073\n",
      "Epoch 26150/30000 Validation Loss: 0.08680584281682968\n",
      "Epoch 26151/30000 Training Loss: 0.057967428117990494\n",
      "Epoch 26152/30000 Training Loss: 0.06670338660478592\n",
      "Epoch 26153/30000 Training Loss: 0.08977999538183212\n",
      "Epoch 26154/30000 Training Loss: 0.06587785482406616\n",
      "Epoch 26155/30000 Training Loss: 0.07288648933172226\n",
      "Epoch 26156/30000 Training Loss: 0.09044814109802246\n",
      "Epoch 26157/30000 Training Loss: 0.06626716256141663\n",
      "Epoch 26158/30000 Training Loss: 0.07126516103744507\n",
      "Epoch 26159/30000 Training Loss: 0.06173352524638176\n",
      "Epoch 26160/30000 Training Loss: 0.07207410782575607\n",
      "Epoch 26160/30000 Validation Loss: 0.0611431784927845\n",
      "Epoch 26161/30000 Training Loss: 0.072357177734375\n",
      "Epoch 26162/30000 Training Loss: 0.0655163899064064\n",
      "Epoch 26163/30000 Training Loss: 0.07568875700235367\n",
      "Epoch 26164/30000 Training Loss: 0.05662114545702934\n",
      "Epoch 26165/30000 Training Loss: 0.07564006000757217\n",
      "Epoch 26166/30000 Training Loss: 0.06406620889902115\n",
      "Epoch 26167/30000 Training Loss: 0.05520717427134514\n",
      "Epoch 26168/30000 Training Loss: 0.05217614397406578\n",
      "Epoch 26169/30000 Training Loss: 0.0580003522336483\n",
      "Epoch 26170/30000 Training Loss: 0.06349319964647293\n",
      "Epoch 26170/30000 Validation Loss: 0.07067926973104477\n",
      "Epoch 26171/30000 Training Loss: 0.08281717449426651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26172/30000 Training Loss: 0.09129870682954788\n",
      "Epoch 26173/30000 Training Loss: 0.08712473511695862\n",
      "Epoch 26174/30000 Training Loss: 0.05635850504040718\n",
      "Epoch 26175/30000 Training Loss: 0.0693061351776123\n",
      "Epoch 26176/30000 Training Loss: 0.06943550705909729\n",
      "Epoch 26177/30000 Training Loss: 0.06756177544593811\n",
      "Epoch 26178/30000 Training Loss: 0.05214259400963783\n",
      "Epoch 26179/30000 Training Loss: 0.0667564794421196\n",
      "Epoch 26180/30000 Training Loss: 0.08832704275846481\n",
      "Epoch 26180/30000 Validation Loss: 0.07092564553022385\n",
      "Epoch 26181/30000 Training Loss: 0.06410028040409088\n",
      "Epoch 26182/30000 Training Loss: 0.07803118228912354\n",
      "Epoch 26183/30000 Training Loss: 0.051035672426223755\n",
      "Epoch 26184/30000 Training Loss: 0.06878015398979187\n",
      "Epoch 26185/30000 Training Loss: 0.06770728528499603\n",
      "Epoch 26186/30000 Training Loss: 0.059233203530311584\n",
      "Epoch 26187/30000 Training Loss: 0.07837652415037155\n",
      "Epoch 26188/30000 Training Loss: 0.07066170126199722\n",
      "Epoch 26189/30000 Training Loss: 0.05488741025328636\n",
      "Epoch 26190/30000 Training Loss: 0.07003084570169449\n",
      "Epoch 26190/30000 Validation Loss: 0.0635804757475853\n",
      "Epoch 26191/30000 Training Loss: 0.07278233766555786\n",
      "Epoch 26192/30000 Training Loss: 0.07905793935060501\n",
      "Epoch 26193/30000 Training Loss: 0.06019943952560425\n",
      "Epoch 26194/30000 Training Loss: 0.056863922625780106\n",
      "Epoch 26195/30000 Training Loss: 0.07153654843568802\n",
      "Epoch 26196/30000 Training Loss: 0.053531110286712646\n",
      "Epoch 26197/30000 Training Loss: 0.07807999104261398\n",
      "Epoch 26198/30000 Training Loss: 0.07163075357675552\n",
      "Epoch 26199/30000 Training Loss: 0.08032246679067612\n",
      "Epoch 26200/30000 Training Loss: 0.09985246509313583\n",
      "Epoch 26200/30000 Validation Loss: 0.0693260207772255\n",
      "Epoch 26201/30000 Training Loss: 0.059731852263212204\n",
      "Epoch 26202/30000 Training Loss: 0.08678123354911804\n",
      "Epoch 26203/30000 Training Loss: 0.08331746608018875\n",
      "Epoch 26204/30000 Training Loss: 0.0732104703783989\n",
      "Epoch 26205/30000 Training Loss: 0.0660032257437706\n",
      "Epoch 26206/30000 Training Loss: 0.06996545940637589\n",
      "Epoch 26207/30000 Training Loss: 0.06178739294409752\n",
      "Epoch 26208/30000 Training Loss: 0.06037076190114021\n",
      "Epoch 26209/30000 Training Loss: 0.061032067984342575\n",
      "Epoch 26210/30000 Training Loss: 0.06350845098495483\n",
      "Epoch 26210/30000 Validation Loss: 0.07176119834184647\n",
      "Epoch 26211/30000 Training Loss: 0.07825221121311188\n",
      "Epoch 26212/30000 Training Loss: 0.07837570458650589\n",
      "Epoch 26213/30000 Training Loss: 0.06956867128610611\n",
      "Epoch 26214/30000 Training Loss: 0.0642537847161293\n",
      "Epoch 26215/30000 Training Loss: 0.07797863334417343\n",
      "Epoch 26216/30000 Training Loss: 0.06817600876092911\n",
      "Epoch 26217/30000 Training Loss: 0.0697779580950737\n",
      "Epoch 26218/30000 Training Loss: 0.0557435117661953\n",
      "Epoch 26219/30000 Training Loss: 0.06182412430644035\n",
      "Epoch 26220/30000 Training Loss: 0.09827807545661926\n",
      "Epoch 26220/30000 Validation Loss: 0.06041647866368294\n",
      "Epoch 26221/30000 Training Loss: 0.05909750983119011\n",
      "Epoch 26222/30000 Training Loss: 0.06158949062228203\n",
      "Epoch 26223/30000 Training Loss: 0.0653083324432373\n",
      "Epoch 26224/30000 Training Loss: 0.07237183302640915\n",
      "Epoch 26225/30000 Training Loss: 0.08072058111429214\n",
      "Epoch 26226/30000 Training Loss: 0.08054082840681076\n",
      "Epoch 26227/30000 Training Loss: 0.06889230757951736\n",
      "Epoch 26228/30000 Training Loss: 0.06102690473198891\n",
      "Epoch 26229/30000 Training Loss: 0.06886176019906998\n",
      "Epoch 26230/30000 Training Loss: 0.07683540135622025\n",
      "Epoch 26230/30000 Validation Loss: 0.06975673884153366\n",
      "Epoch 26231/30000 Training Loss: 0.07752182334661484\n",
      "Epoch 26232/30000 Training Loss: 0.08573559671640396\n",
      "Epoch 26233/30000 Training Loss: 0.07318677008152008\n",
      "Epoch 26234/30000 Training Loss: 0.05586560443043709\n",
      "Epoch 26235/30000 Training Loss: 0.06345763057470322\n",
      "Epoch 26236/30000 Training Loss: 0.06948317587375641\n",
      "Epoch 26237/30000 Training Loss: 0.06110462546348572\n",
      "Epoch 26238/30000 Training Loss: 0.07289876788854599\n",
      "Epoch 26239/30000 Training Loss: 0.06931284070014954\n",
      "Epoch 26240/30000 Training Loss: 0.06485822051763535\n",
      "Epoch 26240/30000 Validation Loss: 0.08462926000356674\n",
      "Epoch 26241/30000 Training Loss: 0.07944905012845993\n",
      "Epoch 26242/30000 Training Loss: 0.06232045218348503\n",
      "Epoch 26243/30000 Training Loss: 0.05409427359700203\n",
      "Epoch 26244/30000 Training Loss: 0.07127484679222107\n",
      "Epoch 26245/30000 Training Loss: 0.06436903029680252\n",
      "Epoch 26246/30000 Training Loss: 0.07081828266382217\n",
      "Epoch 26247/30000 Training Loss: 0.06885698437690735\n",
      "Epoch 26248/30000 Training Loss: 0.08153985440731049\n",
      "Epoch 26249/30000 Training Loss: 0.06979451328516006\n",
      "Epoch 26250/30000 Training Loss: 0.07804625481367111\n",
      "Epoch 26250/30000 Validation Loss: 0.08466855436563492\n",
      "Epoch 26251/30000 Training Loss: 0.056170906871557236\n",
      "Epoch 26252/30000 Training Loss: 0.07460259646177292\n",
      "Epoch 26253/30000 Training Loss: 0.0758795514702797\n",
      "Epoch 26254/30000 Training Loss: 0.06481090933084488\n",
      "Epoch 26255/30000 Training Loss: 0.0738237127661705\n",
      "Epoch 26256/30000 Training Loss: 0.07366252690553665\n",
      "Epoch 26257/30000 Training Loss: 0.07704427093267441\n",
      "Epoch 26258/30000 Training Loss: 0.08661026507616043\n",
      "Epoch 26259/30000 Training Loss: 0.06314406543970108\n",
      "Epoch 26260/30000 Training Loss: 0.06364753097295761\n",
      "Epoch 26260/30000 Validation Loss: 0.07299557328224182\n",
      "Epoch 26261/30000 Training Loss: 0.07921744138002396\n",
      "Epoch 26262/30000 Training Loss: 0.06601830571889877\n",
      "Epoch 26263/30000 Training Loss: 0.08068124204874039\n",
      "Epoch 26264/30000 Training Loss: 0.07865062355995178\n",
      "Epoch 26265/30000 Training Loss: 0.08122847229242325\n",
      "Epoch 26266/30000 Training Loss: 0.07670852541923523\n",
      "Epoch 26267/30000 Training Loss: 0.07982946187257767\n",
      "Epoch 26268/30000 Training Loss: 0.07052570581436157\n",
      "Epoch 26269/30000 Training Loss: 0.060096073895692825\n",
      "Epoch 26270/30000 Training Loss: 0.049413904547691345\n",
      "Epoch 26270/30000 Validation Loss: 0.08862461894750595\n",
      "Epoch 26271/30000 Training Loss: 0.06534574925899506\n",
      "Epoch 26272/30000 Training Loss: 0.06811728328466415\n",
      "Epoch 26273/30000 Training Loss: 0.09353414922952652\n",
      "Epoch 26274/30000 Training Loss: 0.0732826367020607\n",
      "Epoch 26275/30000 Training Loss: 0.07085379213094711\n",
      "Epoch 26276/30000 Training Loss: 0.060347508639097214\n",
      "Epoch 26277/30000 Training Loss: 0.08118361979722977\n",
      "Epoch 26278/30000 Training Loss: 0.07834316045045853\n",
      "Epoch 26279/30000 Training Loss: 0.07878272980451584\n",
      "Epoch 26280/30000 Training Loss: 0.06512177735567093\n",
      "Epoch 26280/30000 Validation Loss: 0.05545571818947792\n",
      "Epoch 26281/30000 Training Loss: 0.06262306123971939\n",
      "Epoch 26282/30000 Training Loss: 0.0815243124961853\n",
      "Epoch 26283/30000 Training Loss: 0.0650072917342186\n",
      "Epoch 26284/30000 Training Loss: 0.0641803964972496\n",
      "Epoch 26285/30000 Training Loss: 0.05509909987449646\n",
      "Epoch 26286/30000 Training Loss: 0.07427312433719635\n",
      "Epoch 26287/30000 Training Loss: 0.06597629934549332\n",
      "Epoch 26288/30000 Training Loss: 0.059958163648843765\n",
      "Epoch 26289/30000 Training Loss: 0.06450620293617249\n",
      "Epoch 26290/30000 Training Loss: 0.06947382539510727\n",
      "Epoch 26290/30000 Validation Loss: 0.07230469584465027\n",
      "Epoch 26291/30000 Training Loss: 0.060520365834236145\n",
      "Epoch 26292/30000 Training Loss: 0.07793400436639786\n",
      "Epoch 26293/30000 Training Loss: 0.0730878934264183\n",
      "Epoch 26294/30000 Training Loss: 0.07970734685659409\n",
      "Epoch 26295/30000 Training Loss: 0.06537304073572159\n",
      "Epoch 26296/30000 Training Loss: 0.06419991701841354\n",
      "Epoch 26297/30000 Training Loss: 0.06225991249084473\n",
      "Epoch 26298/30000 Training Loss: 0.07618100941181183\n",
      "Epoch 26299/30000 Training Loss: 0.06758645921945572\n",
      "Epoch 26300/30000 Training Loss: 0.05947074294090271\n",
      "Epoch 26300/30000 Validation Loss: 0.08325721323490143\n",
      "Epoch 26301/30000 Training Loss: 0.06428611278533936\n",
      "Epoch 26302/30000 Training Loss: 0.06481935828924179\n",
      "Epoch 26303/30000 Training Loss: 0.06679338216781616\n",
      "Epoch 26304/30000 Training Loss: 0.08347371965646744\n",
      "Epoch 26305/30000 Training Loss: 0.06258518248796463\n",
      "Epoch 26306/30000 Training Loss: 0.06433268636465073\n",
      "Epoch 26307/30000 Training Loss: 0.0860859751701355\n",
      "Epoch 26308/30000 Training Loss: 0.06683479994535446\n",
      "Epoch 26309/30000 Training Loss: 0.07661055028438568\n",
      "Epoch 26310/30000 Training Loss: 0.07761115580797195\n",
      "Epoch 26310/30000 Validation Loss: 0.04878358915448189\n",
      "Epoch 26311/30000 Training Loss: 0.04899054765701294\n",
      "Epoch 26312/30000 Training Loss: 0.06268493086099625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26313/30000 Training Loss: 0.059083376079797745\n",
      "Epoch 26314/30000 Training Loss: 0.06146618351340294\n",
      "Epoch 26315/30000 Training Loss: 0.06009794399142265\n",
      "Epoch 26316/30000 Training Loss: 0.06917885690927505\n",
      "Epoch 26317/30000 Training Loss: 0.05824669077992439\n",
      "Epoch 26318/30000 Training Loss: 0.062443867325782776\n",
      "Epoch 26319/30000 Training Loss: 0.053109776228666306\n",
      "Epoch 26320/30000 Training Loss: 0.07708755135536194\n",
      "Epoch 26320/30000 Validation Loss: 0.09375006705522537\n",
      "Epoch 26321/30000 Training Loss: 0.06583019345998764\n",
      "Epoch 26322/30000 Training Loss: 0.07441198080778122\n",
      "Epoch 26323/30000 Training Loss: 0.08719611167907715\n",
      "Epoch 26324/30000 Training Loss: 0.05687394738197327\n",
      "Epoch 26325/30000 Training Loss: 0.07200918346643448\n",
      "Epoch 26326/30000 Training Loss: 0.07640442997217178\n",
      "Epoch 26327/30000 Training Loss: 0.07430102676153183\n",
      "Epoch 26328/30000 Training Loss: 0.06820564717054367\n",
      "Epoch 26329/30000 Training Loss: 0.07539879530668259\n",
      "Epoch 26330/30000 Training Loss: 0.05242932215332985\n",
      "Epoch 26330/30000 Validation Loss: 0.06042295694351196\n",
      "Epoch 26331/30000 Training Loss: 0.08116709440946579\n",
      "Epoch 26332/30000 Training Loss: 0.06042488291859627\n",
      "Epoch 26333/30000 Training Loss: 0.07859081774950027\n",
      "Epoch 26334/30000 Training Loss: 0.0697852373123169\n",
      "Epoch 26335/30000 Training Loss: 0.07127635180950165\n",
      "Epoch 26336/30000 Training Loss: 0.06493949890136719\n",
      "Epoch 26337/30000 Training Loss: 0.0991283431649208\n",
      "Epoch 26338/30000 Training Loss: 0.07543458789587021\n",
      "Epoch 26339/30000 Training Loss: 0.07652518898248672\n",
      "Epoch 26340/30000 Training Loss: 0.0772494375705719\n",
      "Epoch 26340/30000 Validation Loss: 0.05786995589733124\n",
      "Epoch 26341/30000 Training Loss: 0.057184215635061264\n",
      "Epoch 26342/30000 Training Loss: 0.06010768190026283\n",
      "Epoch 26343/30000 Training Loss: 0.05682792142033577\n",
      "Epoch 26344/30000 Training Loss: 0.05785249546170235\n",
      "Epoch 26345/30000 Training Loss: 0.08195844292640686\n",
      "Epoch 26346/30000 Training Loss: 0.0683029294013977\n",
      "Epoch 26347/30000 Training Loss: 0.07209206372499466\n",
      "Epoch 26348/30000 Training Loss: 0.06578247994184494\n",
      "Epoch 26349/30000 Training Loss: 0.06736646592617035\n",
      "Epoch 26350/30000 Training Loss: 0.08206193894147873\n",
      "Epoch 26350/30000 Validation Loss: 0.06606105715036392\n",
      "Epoch 26351/30000 Training Loss: 0.06476637721061707\n",
      "Epoch 26352/30000 Training Loss: 0.08107531815767288\n",
      "Epoch 26353/30000 Training Loss: 0.06414967775344849\n",
      "Epoch 26354/30000 Training Loss: 0.07153350859880447\n",
      "Epoch 26355/30000 Training Loss: 0.06412885338068008\n",
      "Epoch 26356/30000 Training Loss: 0.09576766937971115\n",
      "Epoch 26357/30000 Training Loss: 0.0657387375831604\n",
      "Epoch 26358/30000 Training Loss: 0.07400118559598923\n",
      "Epoch 26359/30000 Training Loss: 0.05958859622478485\n",
      "Epoch 26360/30000 Training Loss: 0.07328496128320694\n",
      "Epoch 26360/30000 Validation Loss: 0.054019391536712646\n",
      "Epoch 26361/30000 Training Loss: 0.06948087364435196\n",
      "Epoch 26362/30000 Training Loss: 0.06214672699570656\n",
      "Epoch 26363/30000 Training Loss: 0.05873556807637215\n",
      "Epoch 26364/30000 Training Loss: 0.05319058895111084\n",
      "Epoch 26365/30000 Training Loss: 0.06236840412020683\n",
      "Epoch 26366/30000 Training Loss: 0.05919384956359863\n",
      "Epoch 26367/30000 Training Loss: 0.08265460282564163\n",
      "Epoch 26368/30000 Training Loss: 0.08182228356599808\n",
      "Epoch 26369/30000 Training Loss: 0.08278899639844894\n",
      "Epoch 26370/30000 Training Loss: 0.07032906264066696\n",
      "Epoch 26370/30000 Validation Loss: 0.065435029566288\n",
      "Epoch 26371/30000 Training Loss: 0.07664024829864502\n",
      "Epoch 26372/30000 Training Loss: 0.09145594388246536\n",
      "Epoch 26373/30000 Training Loss: 0.08155526965856552\n",
      "Epoch 26374/30000 Training Loss: 0.04930591955780983\n",
      "Epoch 26375/30000 Training Loss: 0.07286036759614944\n",
      "Epoch 26376/30000 Training Loss: 0.07665124535560608\n",
      "Epoch 26377/30000 Training Loss: 0.08774568885564804\n",
      "Epoch 26378/30000 Training Loss: 0.07248588651418686\n",
      "Epoch 26379/30000 Training Loss: 0.07353222370147705\n",
      "Epoch 26380/30000 Training Loss: 0.08202164620161057\n",
      "Epoch 26380/30000 Validation Loss: 0.0868242010474205\n",
      "Epoch 26381/30000 Training Loss: 0.062087129801511765\n",
      "Epoch 26382/30000 Training Loss: 0.06052081659436226\n",
      "Epoch 26383/30000 Training Loss: 0.07100216299295425\n",
      "Epoch 26384/30000 Training Loss: 0.06006447598338127\n",
      "Epoch 26385/30000 Training Loss: 0.06548457592725754\n",
      "Epoch 26386/30000 Training Loss: 0.05762801691889763\n",
      "Epoch 26387/30000 Training Loss: 0.08961037546396255\n",
      "Epoch 26388/30000 Training Loss: 0.06286310404539108\n",
      "Epoch 26389/30000 Training Loss: 0.051712121814489365\n",
      "Epoch 26390/30000 Training Loss: 0.0918726921081543\n",
      "Epoch 26390/30000 Validation Loss: 0.06920554488897324\n",
      "Epoch 26391/30000 Training Loss: 0.07432863861322403\n",
      "Epoch 26392/30000 Training Loss: 0.06731437891721725\n",
      "Epoch 26393/30000 Training Loss: 0.06197727844119072\n",
      "Epoch 26394/30000 Training Loss: 0.05762272700667381\n",
      "Epoch 26395/30000 Training Loss: 0.06385122239589691\n",
      "Epoch 26396/30000 Training Loss: 0.0630384162068367\n",
      "Epoch 26397/30000 Training Loss: 0.06603188067674637\n",
      "Epoch 26398/30000 Training Loss: 0.0750693529844284\n",
      "Epoch 26399/30000 Training Loss: 0.06860183924436569\n",
      "Epoch 26400/30000 Training Loss: 0.0665612667798996\n",
      "Epoch 26400/30000 Validation Loss: 0.06478329747915268\n",
      "Epoch 26401/30000 Training Loss: 0.06857171654701233\n",
      "Epoch 26402/30000 Training Loss: 0.08284618705511093\n",
      "Epoch 26403/30000 Training Loss: 0.060246724635362625\n",
      "Epoch 26404/30000 Training Loss: 0.07789058238267899\n",
      "Epoch 26405/30000 Training Loss: 0.07122287899255753\n",
      "Epoch 26406/30000 Training Loss: 0.06000607833266258\n",
      "Epoch 26407/30000 Training Loss: 0.07143775373697281\n",
      "Epoch 26408/30000 Training Loss: 0.05828316882252693\n",
      "Epoch 26409/30000 Training Loss: 0.06675387918949127\n",
      "Epoch 26410/30000 Training Loss: 0.06668058782815933\n",
      "Epoch 26410/30000 Validation Loss: 0.06435330957174301\n",
      "Epoch 26411/30000 Training Loss: 0.07308239489793777\n",
      "Epoch 26412/30000 Training Loss: 0.07923918217420578\n",
      "Epoch 26413/30000 Training Loss: 0.061017002910375595\n",
      "Epoch 26414/30000 Training Loss: 0.07486697286367416\n",
      "Epoch 26415/30000 Training Loss: 0.08126974105834961\n",
      "Epoch 26416/30000 Training Loss: 0.07923831790685654\n",
      "Epoch 26417/30000 Training Loss: 0.07422631978988647\n",
      "Epoch 26418/30000 Training Loss: 0.06612741202116013\n",
      "Epoch 26419/30000 Training Loss: 0.07779563218355179\n",
      "Epoch 26420/30000 Training Loss: 0.06340762972831726\n",
      "Epoch 26420/30000 Validation Loss: 0.0724506676197052\n",
      "Epoch 26421/30000 Training Loss: 0.08915424346923828\n",
      "Epoch 26422/30000 Training Loss: 0.0624469555914402\n",
      "Epoch 26423/30000 Training Loss: 0.07202252745628357\n",
      "Epoch 26424/30000 Training Loss: 0.08194874972105026\n",
      "Epoch 26425/30000 Training Loss: 0.06351029872894287\n",
      "Epoch 26426/30000 Training Loss: 0.0740882083773613\n",
      "Epoch 26427/30000 Training Loss: 0.05474965646862984\n",
      "Epoch 26428/30000 Training Loss: 0.07074526697397232\n",
      "Epoch 26429/30000 Training Loss: 0.072865329682827\n",
      "Epoch 26430/30000 Training Loss: 0.0539885051548481\n",
      "Epoch 26430/30000 Validation Loss: 0.08751649409532547\n",
      "Epoch 26431/30000 Training Loss: 0.07792457938194275\n",
      "Epoch 26432/30000 Training Loss: 0.07009124010801315\n",
      "Epoch 26433/30000 Training Loss: 0.08893126994371414\n",
      "Epoch 26434/30000 Training Loss: 0.07897605001926422\n",
      "Epoch 26435/30000 Training Loss: 0.059843238443136215\n",
      "Epoch 26436/30000 Training Loss: 0.08192724734544754\n",
      "Epoch 26437/30000 Training Loss: 0.07284616678953171\n",
      "Epoch 26438/30000 Training Loss: 0.0689789354801178\n",
      "Epoch 26439/30000 Training Loss: 0.08036297559738159\n",
      "Epoch 26440/30000 Training Loss: 0.060347139835357666\n",
      "Epoch 26440/30000 Validation Loss: 0.070503830909729\n",
      "Epoch 26441/30000 Training Loss: 0.06250523775815964\n",
      "Epoch 26442/30000 Training Loss: 0.06582677364349365\n",
      "Epoch 26443/30000 Training Loss: 0.084040068089962\n",
      "Epoch 26444/30000 Training Loss: 0.08369817584753036\n",
      "Epoch 26445/30000 Training Loss: 0.06776504963636398\n",
      "Epoch 26446/30000 Training Loss: 0.09464684128761292\n",
      "Epoch 26447/30000 Training Loss: 0.06114901974797249\n",
      "Epoch 26448/30000 Training Loss: 0.08009042590856552\n",
      "Epoch 26449/30000 Training Loss: 0.07348177582025528\n",
      "Epoch 26450/30000 Training Loss: 0.05956714227795601\n",
      "Epoch 26450/30000 Validation Loss: 0.09320064634084702\n",
      "Epoch 26451/30000 Training Loss: 0.07720180600881577\n",
      "Epoch 26452/30000 Training Loss: 0.07592297345399857\n",
      "Epoch 26453/30000 Training Loss: 0.08283036202192307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26454/30000 Training Loss: 0.07635056972503662\n",
      "Epoch 26455/30000 Training Loss: 0.07174672931432724\n",
      "Epoch 26456/30000 Training Loss: 0.06293197721242905\n",
      "Epoch 26457/30000 Training Loss: 0.07071930915117264\n",
      "Epoch 26458/30000 Training Loss: 0.07693351060152054\n",
      "Epoch 26459/30000 Training Loss: 0.058770742267370224\n",
      "Epoch 26460/30000 Training Loss: 0.06661199033260345\n",
      "Epoch 26460/30000 Validation Loss: 0.07500490546226501\n",
      "Epoch 26461/30000 Training Loss: 0.0663481131196022\n",
      "Epoch 26462/30000 Training Loss: 0.07227443903684616\n",
      "Epoch 26463/30000 Training Loss: 0.05464974045753479\n",
      "Epoch 26464/30000 Training Loss: 0.07280043512582779\n",
      "Epoch 26465/30000 Training Loss: 0.08234021067619324\n",
      "Epoch 26466/30000 Training Loss: 0.06620284169912338\n",
      "Epoch 26467/30000 Training Loss: 0.05788639560341835\n",
      "Epoch 26468/30000 Training Loss: 0.07348164916038513\n",
      "Epoch 26469/30000 Training Loss: 0.06100921332836151\n",
      "Epoch 26470/30000 Training Loss: 0.07475927472114563\n",
      "Epoch 26470/30000 Validation Loss: 0.06333080679178238\n",
      "Epoch 26471/30000 Training Loss: 0.06743646413087845\n",
      "Epoch 26472/30000 Training Loss: 0.08999025076627731\n",
      "Epoch 26473/30000 Training Loss: 0.07104501873254776\n",
      "Epoch 26474/30000 Training Loss: 0.054973382502794266\n",
      "Epoch 26475/30000 Training Loss: 0.07405833899974823\n",
      "Epoch 26476/30000 Training Loss: 0.05816268548369408\n",
      "Epoch 26477/30000 Training Loss: 0.05543351545929909\n",
      "Epoch 26478/30000 Training Loss: 0.06998386979103088\n",
      "Epoch 26479/30000 Training Loss: 0.06263947486877441\n",
      "Epoch 26480/30000 Training Loss: 0.08509781956672668\n",
      "Epoch 26480/30000 Validation Loss: 0.06635823100805283\n",
      "Epoch 26481/30000 Training Loss: 0.09991458058357239\n",
      "Epoch 26482/30000 Training Loss: 0.08151504397392273\n",
      "Epoch 26483/30000 Training Loss: 0.06790658086538315\n",
      "Epoch 26484/30000 Training Loss: 0.08446940034627914\n",
      "Epoch 26485/30000 Training Loss: 0.0722259059548378\n",
      "Epoch 26486/30000 Training Loss: 0.05735625699162483\n",
      "Epoch 26487/30000 Training Loss: 0.06233428791165352\n",
      "Epoch 26488/30000 Training Loss: 0.0662955492734909\n",
      "Epoch 26489/30000 Training Loss: 0.0646011009812355\n",
      "Epoch 26490/30000 Training Loss: 0.0646018236875534\n",
      "Epoch 26490/30000 Validation Loss: 0.0686536654829979\n",
      "Epoch 26491/30000 Training Loss: 0.07663992047309875\n",
      "Epoch 26492/30000 Training Loss: 0.07291669398546219\n",
      "Epoch 26493/30000 Training Loss: 0.0720260962843895\n",
      "Epoch 26494/30000 Training Loss: 0.06514749675989151\n",
      "Epoch 26495/30000 Training Loss: 0.0735430121421814\n",
      "Epoch 26496/30000 Training Loss: 0.06550539284944534\n",
      "Epoch 26497/30000 Training Loss: 0.07569482177495956\n",
      "Epoch 26498/30000 Training Loss: 0.0960836187005043\n",
      "Epoch 26499/30000 Training Loss: 0.084067203104496\n",
      "Epoch 26500/30000 Training Loss: 0.08312554657459259\n",
      "Epoch 26500/30000 Validation Loss: 0.06856673955917358\n",
      "Epoch 26501/30000 Training Loss: 0.06797253340482712\n",
      "Epoch 26502/30000 Training Loss: 0.06053122878074646\n",
      "Epoch 26503/30000 Training Loss: 0.0763385072350502\n",
      "Epoch 26504/30000 Training Loss: 0.06894153356552124\n",
      "Epoch 26505/30000 Training Loss: 0.06717965006828308\n",
      "Epoch 26506/30000 Training Loss: 0.07556848227977753\n",
      "Epoch 26507/30000 Training Loss: 0.05503864213824272\n",
      "Epoch 26508/30000 Training Loss: 0.06572521477937698\n",
      "Epoch 26509/30000 Training Loss: 0.0939197912812233\n",
      "Epoch 26510/30000 Training Loss: 0.07521367073059082\n",
      "Epoch 26510/30000 Validation Loss: 0.0710170641541481\n",
      "Epoch 26511/30000 Training Loss: 0.06543330103158951\n",
      "Epoch 26512/30000 Training Loss: 0.07743490487337112\n",
      "Epoch 26513/30000 Training Loss: 0.059329260140657425\n",
      "Epoch 26514/30000 Training Loss: 0.05744784697890282\n",
      "Epoch 26515/30000 Training Loss: 0.06274602562189102\n",
      "Epoch 26516/30000 Training Loss: 0.06590529531240463\n",
      "Epoch 26517/30000 Training Loss: 0.0750763788819313\n",
      "Epoch 26518/30000 Training Loss: 0.06406981498003006\n",
      "Epoch 26519/30000 Training Loss: 0.05133701488375664\n",
      "Epoch 26520/30000 Training Loss: 0.05841904506087303\n",
      "Epoch 26520/30000 Validation Loss: 0.07196212559938431\n",
      "Epoch 26521/30000 Training Loss: 0.08079075068235397\n",
      "Epoch 26522/30000 Training Loss: 0.0680011436343193\n",
      "Epoch 26523/30000 Training Loss: 0.06729944795370102\n",
      "Epoch 26524/30000 Training Loss: 0.057710424065589905\n",
      "Epoch 26525/30000 Training Loss: 0.06770995259284973\n",
      "Epoch 26526/30000 Training Loss: 0.06698990613222122\n",
      "Epoch 26527/30000 Training Loss: 0.07709943503141403\n",
      "Epoch 26528/30000 Training Loss: 0.07154075056314468\n",
      "Epoch 26529/30000 Training Loss: 0.07538846135139465\n",
      "Epoch 26530/30000 Training Loss: 0.07267849892377853\n",
      "Epoch 26530/30000 Validation Loss: 0.07477327436208725\n",
      "Epoch 26531/30000 Training Loss: 0.06605292111635208\n",
      "Epoch 26532/30000 Training Loss: 0.06470274925231934\n",
      "Epoch 26533/30000 Training Loss: 0.06883465498685837\n",
      "Epoch 26534/30000 Training Loss: 0.07587455213069916\n",
      "Epoch 26535/30000 Training Loss: 0.06586515158414841\n",
      "Epoch 26536/30000 Training Loss: 0.0724833682179451\n",
      "Epoch 26537/30000 Training Loss: 0.06539896130561829\n",
      "Epoch 26538/30000 Training Loss: 0.05811963602900505\n",
      "Epoch 26539/30000 Training Loss: 0.07951700687408447\n",
      "Epoch 26540/30000 Training Loss: 0.07978228479623795\n",
      "Epoch 26540/30000 Validation Loss: 0.06565319746732712\n",
      "Epoch 26541/30000 Training Loss: 0.07607414573431015\n",
      "Epoch 26542/30000 Training Loss: 0.08883389085531235\n",
      "Epoch 26543/30000 Training Loss: 0.05952804163098335\n",
      "Epoch 26544/30000 Training Loss: 0.0728035643696785\n",
      "Epoch 26545/30000 Training Loss: 0.0645892396569252\n",
      "Epoch 26546/30000 Training Loss: 0.05767607316374779\n",
      "Epoch 26547/30000 Training Loss: 0.0644393041729927\n",
      "Epoch 26548/30000 Training Loss: 0.06937813758850098\n",
      "Epoch 26549/30000 Training Loss: 0.06750267744064331\n",
      "Epoch 26550/30000 Training Loss: 0.07573647052049637\n",
      "Epoch 26550/30000 Validation Loss: 0.08466243743896484\n",
      "Epoch 26551/30000 Training Loss: 0.06599757075309753\n",
      "Epoch 26552/30000 Training Loss: 0.06859719753265381\n",
      "Epoch 26553/30000 Training Loss: 0.05935249105095863\n",
      "Epoch 26554/30000 Training Loss: 0.06770844012498856\n",
      "Epoch 26555/30000 Training Loss: 0.08755514025688171\n",
      "Epoch 26556/30000 Training Loss: 0.05481179431080818\n",
      "Epoch 26557/30000 Training Loss: 0.05543803051114082\n",
      "Epoch 26558/30000 Training Loss: 0.058703016489744186\n",
      "Epoch 26559/30000 Training Loss: 0.08424904197454453\n",
      "Epoch 26560/30000 Training Loss: 0.0808805301785469\n",
      "Epoch 26560/30000 Validation Loss: 0.051647260785102844\n",
      "Epoch 26561/30000 Training Loss: 0.06841744482517242\n",
      "Epoch 26562/30000 Training Loss: 0.06403704732656479\n",
      "Epoch 26563/30000 Training Loss: 0.06912778317928314\n",
      "Epoch 26564/30000 Training Loss: 0.07667506486177444\n",
      "Epoch 26565/30000 Training Loss: 0.06228160858154297\n",
      "Epoch 26566/30000 Training Loss: 0.07229877263307571\n",
      "Epoch 26567/30000 Training Loss: 0.06249837204813957\n",
      "Epoch 26568/30000 Training Loss: 0.08395575731992722\n",
      "Epoch 26569/30000 Training Loss: 0.05236571654677391\n",
      "Epoch 26570/30000 Training Loss: 0.05601566657423973\n",
      "Epoch 26570/30000 Validation Loss: 0.08851206302642822\n",
      "Epoch 26571/30000 Training Loss: 0.0688011571764946\n",
      "Epoch 26572/30000 Training Loss: 0.06926427036523819\n",
      "Epoch 26573/30000 Training Loss: 0.06198428198695183\n",
      "Epoch 26574/30000 Training Loss: 0.06640350073575974\n",
      "Epoch 26575/30000 Training Loss: 0.07124760001897812\n",
      "Epoch 26576/30000 Training Loss: 0.07240574806928635\n",
      "Epoch 26577/30000 Training Loss: 0.07380806654691696\n",
      "Epoch 26578/30000 Training Loss: 0.07240694016218185\n",
      "Epoch 26579/30000 Training Loss: 0.060674384236335754\n",
      "Epoch 26580/30000 Training Loss: 0.060064446181058884\n",
      "Epoch 26580/30000 Validation Loss: 0.07443919777870178\n",
      "Epoch 26581/30000 Training Loss: 0.07952949404716492\n",
      "Epoch 26582/30000 Training Loss: 0.07622445374727249\n",
      "Epoch 26583/30000 Training Loss: 0.060686901211738586\n",
      "Epoch 26584/30000 Training Loss: 0.06377068907022476\n",
      "Epoch 26585/30000 Training Loss: 0.06926026940345764\n",
      "Epoch 26586/30000 Training Loss: 0.05766835808753967\n",
      "Epoch 26587/30000 Training Loss: 0.08053407818078995\n",
      "Epoch 26588/30000 Training Loss: 0.07208497077226639\n",
      "Epoch 26589/30000 Training Loss: 0.060986146330833435\n",
      "Epoch 26590/30000 Training Loss: 0.0690963938832283\n",
      "Epoch 26590/30000 Validation Loss: 0.05210206285119057\n",
      "Epoch 26591/30000 Training Loss: 0.07535945624113083\n",
      "Epoch 26592/30000 Training Loss: 0.061995942145586014\n",
      "Epoch 26593/30000 Training Loss: 0.07448220998048782\n",
      "Epoch 26594/30000 Training Loss: 0.07824862003326416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26595/30000 Training Loss: 0.07241106033325195\n",
      "Epoch 26596/30000 Training Loss: 0.049809712916612625\n",
      "Epoch 26597/30000 Training Loss: 0.05770595371723175\n",
      "Epoch 26598/30000 Training Loss: 0.07214593142271042\n",
      "Epoch 26599/30000 Training Loss: 0.07691224664449692\n",
      "Epoch 26600/30000 Training Loss: 0.07451274991035461\n",
      "Epoch 26600/30000 Validation Loss: 0.0700019970536232\n",
      "Epoch 26601/30000 Training Loss: 0.07814404368400574\n",
      "Epoch 26602/30000 Training Loss: 0.06899524480104446\n",
      "Epoch 26603/30000 Training Loss: 0.05899234488606453\n",
      "Epoch 26604/30000 Training Loss: 0.06834270060062408\n",
      "Epoch 26605/30000 Training Loss: 0.06313234567642212\n",
      "Epoch 26606/30000 Training Loss: 0.06256816536188126\n",
      "Epoch 26607/30000 Training Loss: 0.06922009587287903\n",
      "Epoch 26608/30000 Training Loss: 0.06310626864433289\n",
      "Epoch 26609/30000 Training Loss: 0.06984486430883408\n",
      "Epoch 26610/30000 Training Loss: 0.07225427031517029\n",
      "Epoch 26610/30000 Validation Loss: 0.061760272830724716\n",
      "Epoch 26611/30000 Training Loss: 0.06814305484294891\n",
      "Epoch 26612/30000 Training Loss: 0.08154375106096268\n",
      "Epoch 26613/30000 Training Loss: 0.07386704534292221\n",
      "Epoch 26614/30000 Training Loss: 0.07148159295320511\n",
      "Epoch 26615/30000 Training Loss: 0.06650435924530029\n",
      "Epoch 26616/30000 Training Loss: 0.05967501178383827\n",
      "Epoch 26617/30000 Training Loss: 0.09599073976278305\n",
      "Epoch 26618/30000 Training Loss: 0.07418764382600784\n",
      "Epoch 26619/30000 Training Loss: 0.08547719568014145\n",
      "Epoch 26620/30000 Training Loss: 0.07569532841444016\n",
      "Epoch 26620/30000 Validation Loss: 0.06502195447683334\n",
      "Epoch 26621/30000 Training Loss: 0.06305265426635742\n",
      "Epoch 26622/30000 Training Loss: 0.0618460476398468\n",
      "Epoch 26623/30000 Training Loss: 0.07858851552009583\n",
      "Epoch 26624/30000 Training Loss: 0.06354647129774094\n",
      "Epoch 26625/30000 Training Loss: 0.060572344809770584\n",
      "Epoch 26626/30000 Training Loss: 0.06618855148553848\n",
      "Epoch 26627/30000 Training Loss: 0.0625157430768013\n",
      "Epoch 26628/30000 Training Loss: 0.055204689502716064\n",
      "Epoch 26629/30000 Training Loss: 0.056732311844825745\n",
      "Epoch 26630/30000 Training Loss: 0.05675714835524559\n",
      "Epoch 26630/30000 Validation Loss: 0.08664479106664658\n",
      "Epoch 26631/30000 Training Loss: 0.06075117364525795\n",
      "Epoch 26632/30000 Training Loss: 0.0800345167517662\n",
      "Epoch 26633/30000 Training Loss: 0.06905180960893631\n",
      "Epoch 26634/30000 Training Loss: 0.0653064027428627\n",
      "Epoch 26635/30000 Training Loss: 0.06120329722762108\n",
      "Epoch 26636/30000 Training Loss: 0.07341333478689194\n",
      "Epoch 26637/30000 Training Loss: 0.07729540020227432\n",
      "Epoch 26638/30000 Training Loss: 0.06981115788221359\n",
      "Epoch 26639/30000 Training Loss: 0.07121596485376358\n",
      "Epoch 26640/30000 Training Loss: 0.08185714483261108\n",
      "Epoch 26640/30000 Validation Loss: 0.056909624487161636\n",
      "Epoch 26641/30000 Training Loss: 0.07300129532814026\n",
      "Epoch 26642/30000 Training Loss: 0.07613576203584671\n",
      "Epoch 26643/30000 Training Loss: 0.07285944372415543\n",
      "Epoch 26644/30000 Training Loss: 0.07664699107408524\n",
      "Epoch 26645/30000 Training Loss: 0.07555469125509262\n",
      "Epoch 26646/30000 Training Loss: 0.05674778297543526\n",
      "Epoch 26647/30000 Training Loss: 0.05369959771633148\n",
      "Epoch 26648/30000 Training Loss: 0.08183705061674118\n",
      "Epoch 26649/30000 Training Loss: 0.05418820306658745\n",
      "Epoch 26650/30000 Training Loss: 0.06496648490428925\n",
      "Epoch 26650/30000 Validation Loss: 0.06774341315031052\n",
      "Epoch 26651/30000 Training Loss: 0.05896587669849396\n",
      "Epoch 26652/30000 Training Loss: 0.06328234821557999\n",
      "Epoch 26653/30000 Training Loss: 0.06840604543685913\n",
      "Epoch 26654/30000 Training Loss: 0.07802455872297287\n",
      "Epoch 26655/30000 Training Loss: 0.08179214596748352\n",
      "Epoch 26656/30000 Training Loss: 0.09582030773162842\n",
      "Epoch 26657/30000 Training Loss: 0.06954141706228256\n",
      "Epoch 26658/30000 Training Loss: 0.08527747541666031\n",
      "Epoch 26659/30000 Training Loss: 0.05705702677369118\n",
      "Epoch 26660/30000 Training Loss: 0.08878225088119507\n",
      "Epoch 26660/30000 Validation Loss: 0.06529245525598526\n",
      "Epoch 26661/30000 Training Loss: 0.0783991739153862\n",
      "Epoch 26662/30000 Training Loss: 0.07159505039453506\n",
      "Epoch 26663/30000 Training Loss: 0.07367512583732605\n",
      "Epoch 26664/30000 Training Loss: 0.08369190245866776\n",
      "Epoch 26665/30000 Training Loss: 0.05790494754910469\n",
      "Epoch 26666/30000 Training Loss: 0.07913729548454285\n",
      "Epoch 26667/30000 Training Loss: 0.0693734884262085\n",
      "Epoch 26668/30000 Training Loss: 0.08295188099145889\n",
      "Epoch 26669/30000 Training Loss: 0.055642131716012955\n",
      "Epoch 26670/30000 Training Loss: 0.06929367035627365\n",
      "Epoch 26670/30000 Validation Loss: 0.08151201903820038\n",
      "Epoch 26671/30000 Training Loss: 0.06177155300974846\n",
      "Epoch 26672/30000 Training Loss: 0.06371519714593887\n",
      "Epoch 26673/30000 Training Loss: 0.06740555912256241\n",
      "Epoch 26674/30000 Training Loss: 0.06231720373034477\n",
      "Epoch 26675/30000 Training Loss: 0.0672919824719429\n",
      "Epoch 26676/30000 Training Loss: 0.08621729165315628\n",
      "Epoch 26677/30000 Training Loss: 0.07934097200632095\n",
      "Epoch 26678/30000 Training Loss: 0.07079290598630905\n",
      "Epoch 26679/30000 Training Loss: 0.08465078473091125\n",
      "Epoch 26680/30000 Training Loss: 0.06182866171002388\n",
      "Epoch 26680/30000 Validation Loss: 0.06733334809541702\n",
      "Epoch 26681/30000 Training Loss: 0.06794927269220352\n",
      "Epoch 26682/30000 Training Loss: 0.07203013449907303\n",
      "Epoch 26683/30000 Training Loss: 0.08294596523046494\n",
      "Epoch 26684/30000 Training Loss: 0.07693325728178024\n",
      "Epoch 26685/30000 Training Loss: 0.06220686808228493\n",
      "Epoch 26686/30000 Training Loss: 0.07690509408712387\n",
      "Epoch 26687/30000 Training Loss: 0.06296678632497787\n",
      "Epoch 26688/30000 Training Loss: 0.05805596709251404\n",
      "Epoch 26689/30000 Training Loss: 0.060721248388290405\n",
      "Epoch 26690/30000 Training Loss: 0.06353578716516495\n",
      "Epoch 26690/30000 Validation Loss: 0.06911615282297134\n",
      "Epoch 26691/30000 Training Loss: 0.06873270869255066\n",
      "Epoch 26692/30000 Training Loss: 0.0717947855591774\n",
      "Epoch 26693/30000 Training Loss: 0.07135946303606033\n",
      "Epoch 26694/30000 Training Loss: 0.07709398120641708\n",
      "Epoch 26695/30000 Training Loss: 0.06212181970477104\n",
      "Epoch 26696/30000 Training Loss: 0.06205970048904419\n",
      "Epoch 26697/30000 Training Loss: 0.06444204598665237\n",
      "Epoch 26698/30000 Training Loss: 0.06586253643035889\n",
      "Epoch 26699/30000 Training Loss: 0.07136236131191254\n",
      "Epoch 26700/30000 Training Loss: 0.05836021900177002\n",
      "Epoch 26700/30000 Validation Loss: 0.07480800151824951\n",
      "Epoch 26701/30000 Training Loss: 0.06638526171445847\n",
      "Epoch 26702/30000 Training Loss: 0.06926438212394714\n",
      "Epoch 26703/30000 Training Loss: 0.06556659936904907\n",
      "Epoch 26704/30000 Training Loss: 0.06685465574264526\n",
      "Epoch 26705/30000 Training Loss: 0.06970936059951782\n",
      "Epoch 26706/30000 Training Loss: 0.09163900464773178\n",
      "Epoch 26707/30000 Training Loss: 0.08237779885530472\n",
      "Epoch 26708/30000 Training Loss: 0.07077687233686447\n",
      "Epoch 26709/30000 Training Loss: 0.06852684915065765\n",
      "Epoch 26710/30000 Training Loss: 0.06569080799818039\n",
      "Epoch 26710/30000 Validation Loss: 0.07200703769922256\n",
      "Epoch 26711/30000 Training Loss: 0.07080531120300293\n",
      "Epoch 26712/30000 Training Loss: 0.07910480350255966\n",
      "Epoch 26713/30000 Training Loss: 0.08168504387140274\n",
      "Epoch 26714/30000 Training Loss: 0.06646385788917542\n",
      "Epoch 26715/30000 Training Loss: 0.05549578741192818\n",
      "Epoch 26716/30000 Training Loss: 0.08228733390569687\n",
      "Epoch 26717/30000 Training Loss: 0.08058558404445648\n",
      "Epoch 26718/30000 Training Loss: 0.0743783712387085\n",
      "Epoch 26719/30000 Training Loss: 0.06488867104053497\n",
      "Epoch 26720/30000 Training Loss: 0.06851307302713394\n",
      "Epoch 26720/30000 Validation Loss: 0.06431598961353302\n",
      "Epoch 26721/30000 Training Loss: 0.07335232943296432\n",
      "Epoch 26722/30000 Training Loss: 0.07277371734380722\n",
      "Epoch 26723/30000 Training Loss: 0.061862338334321976\n",
      "Epoch 26724/30000 Training Loss: 0.07952588051557541\n",
      "Epoch 26725/30000 Training Loss: 0.06974998861551285\n",
      "Epoch 26726/30000 Training Loss: 0.0597866028547287\n",
      "Epoch 26727/30000 Training Loss: 0.0633772686123848\n",
      "Epoch 26728/30000 Training Loss: 0.07007084041833878\n",
      "Epoch 26729/30000 Training Loss: 0.08084850758314133\n",
      "Epoch 26730/30000 Training Loss: 0.059340327978134155\n",
      "Epoch 26730/30000 Validation Loss: 0.08012037724256516\n",
      "Epoch 26731/30000 Training Loss: 0.0669199749827385\n",
      "Epoch 26732/30000 Training Loss: 0.09380286186933517\n",
      "Epoch 26733/30000 Training Loss: 0.07140631228685379\n",
      "Epoch 26734/30000 Training Loss: 0.07887475937604904\n",
      "Epoch 26735/30000 Training Loss: 0.06092393770813942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26736/30000 Training Loss: 0.06194179877638817\n",
      "Epoch 26737/30000 Training Loss: 0.08353877067565918\n",
      "Epoch 26738/30000 Training Loss: 0.06299268454313278\n",
      "Epoch 26739/30000 Training Loss: 0.05301377549767494\n",
      "Epoch 26740/30000 Training Loss: 0.0665009617805481\n",
      "Epoch 26740/30000 Validation Loss: 0.07160202413797379\n",
      "Epoch 26741/30000 Training Loss: 0.07081665843725204\n",
      "Epoch 26742/30000 Training Loss: 0.07694528251886368\n",
      "Epoch 26743/30000 Training Loss: 0.06817208975553513\n",
      "Epoch 26744/30000 Training Loss: 0.052081864327192307\n",
      "Epoch 26745/30000 Training Loss: 0.08254335075616837\n",
      "Epoch 26746/30000 Training Loss: 0.0583643801510334\n",
      "Epoch 26747/30000 Training Loss: 0.06379721313714981\n",
      "Epoch 26748/30000 Training Loss: 0.07211557775735855\n",
      "Epoch 26749/30000 Training Loss: 0.06605150550603867\n",
      "Epoch 26750/30000 Training Loss: 0.08174233883619308\n",
      "Epoch 26750/30000 Validation Loss: 0.06254271417856216\n",
      "Epoch 26751/30000 Training Loss: 0.06593681126832962\n",
      "Epoch 26752/30000 Training Loss: 0.06371059268712997\n",
      "Epoch 26753/30000 Training Loss: 0.0725715160369873\n",
      "Epoch 26754/30000 Training Loss: 0.06351984292268753\n",
      "Epoch 26755/30000 Training Loss: 0.0764889270067215\n",
      "Epoch 26756/30000 Training Loss: 0.08794905990362167\n",
      "Epoch 26757/30000 Training Loss: 0.08386694639921188\n",
      "Epoch 26758/30000 Training Loss: 0.0768597349524498\n",
      "Epoch 26759/30000 Training Loss: 0.09035781770944595\n",
      "Epoch 26760/30000 Training Loss: 0.05405174568295479\n",
      "Epoch 26760/30000 Validation Loss: 0.06394781917333603\n",
      "Epoch 26761/30000 Training Loss: 0.0750792995095253\n",
      "Epoch 26762/30000 Training Loss: 0.06028144061565399\n",
      "Epoch 26763/30000 Training Loss: 0.08182039111852646\n",
      "Epoch 26764/30000 Training Loss: 0.08268312364816666\n",
      "Epoch 26765/30000 Training Loss: 0.07976296544075012\n",
      "Epoch 26766/30000 Training Loss: 0.07004382461309433\n",
      "Epoch 26767/30000 Training Loss: 0.06179915368556976\n",
      "Epoch 26768/30000 Training Loss: 0.07640531659126282\n",
      "Epoch 26769/30000 Training Loss: 0.06264177709817886\n",
      "Epoch 26770/30000 Training Loss: 0.08022897690534592\n",
      "Epoch 26770/30000 Validation Loss: 0.08904621750116348\n",
      "Epoch 26771/30000 Training Loss: 0.067381352186203\n",
      "Epoch 26772/30000 Training Loss: 0.063131183385849\n",
      "Epoch 26773/30000 Training Loss: 0.07264003902673721\n",
      "Epoch 26774/30000 Training Loss: 0.0701502338051796\n",
      "Epoch 26775/30000 Training Loss: 0.07886344194412231\n",
      "Epoch 26776/30000 Training Loss: 0.060580670833587646\n",
      "Epoch 26777/30000 Training Loss: 0.07476228475570679\n",
      "Epoch 26778/30000 Training Loss: 0.07394897192716599\n",
      "Epoch 26779/30000 Training Loss: 0.07208648324012756\n",
      "Epoch 26780/30000 Training Loss: 0.06093050539493561\n",
      "Epoch 26780/30000 Validation Loss: 0.09127829223871231\n",
      "Epoch 26781/30000 Training Loss: 0.0729198157787323\n",
      "Epoch 26782/30000 Training Loss: 0.0650629922747612\n",
      "Epoch 26783/30000 Training Loss: 0.0697496086359024\n",
      "Epoch 26784/30000 Training Loss: 0.05730177462100983\n",
      "Epoch 26785/30000 Training Loss: 0.0504160076379776\n",
      "Epoch 26786/30000 Training Loss: 0.06425455957651138\n",
      "Epoch 26787/30000 Training Loss: 0.06989308446645737\n",
      "Epoch 26788/30000 Training Loss: 0.07523179799318314\n",
      "Epoch 26789/30000 Training Loss: 0.05362724885344505\n",
      "Epoch 26790/30000 Training Loss: 0.0738503634929657\n",
      "Epoch 26790/30000 Validation Loss: 0.08620620518922806\n",
      "Epoch 26791/30000 Training Loss: 0.1050800085067749\n",
      "Epoch 26792/30000 Training Loss: 0.07559473067522049\n",
      "Epoch 26793/30000 Training Loss: 0.07189881056547165\n",
      "Epoch 26794/30000 Training Loss: 0.06749901920557022\n",
      "Epoch 26795/30000 Training Loss: 0.07723764330148697\n",
      "Epoch 26796/30000 Training Loss: 0.07718829065561295\n",
      "Epoch 26797/30000 Training Loss: 0.08893410116434097\n",
      "Epoch 26798/30000 Training Loss: 0.0852246806025505\n",
      "Epoch 26799/30000 Training Loss: 0.062467217445373535\n",
      "Epoch 26800/30000 Training Loss: 0.07002205401659012\n",
      "Epoch 26800/30000 Validation Loss: 0.09783432632684708\n",
      "Epoch 26801/30000 Training Loss: 0.07838832587003708\n",
      "Epoch 26802/30000 Training Loss: 0.06343083828687668\n",
      "Epoch 26803/30000 Training Loss: 0.06594082713127136\n",
      "Epoch 26804/30000 Training Loss: 0.07412687689065933\n",
      "Epoch 26805/30000 Training Loss: 0.07269816100597382\n",
      "Epoch 26806/30000 Training Loss: 0.06601037830114365\n",
      "Epoch 26807/30000 Training Loss: 0.07445325702428818\n",
      "Epoch 26808/30000 Training Loss: 0.07429269701242447\n",
      "Epoch 26809/30000 Training Loss: 0.07019082456827164\n",
      "Epoch 26810/30000 Training Loss: 0.06855076551437378\n",
      "Epoch 26810/30000 Validation Loss: 0.0659044161438942\n",
      "Epoch 26811/30000 Training Loss: 0.06780778616666794\n",
      "Epoch 26812/30000 Training Loss: 0.05282619968056679\n",
      "Epoch 26813/30000 Training Loss: 0.09177518635988235\n",
      "Epoch 26814/30000 Training Loss: 0.06126208230853081\n",
      "Epoch 26815/30000 Training Loss: 0.06667978316545486\n",
      "Epoch 26816/30000 Training Loss: 0.06729783862829208\n",
      "Epoch 26817/30000 Training Loss: 0.056681230664253235\n",
      "Epoch 26818/30000 Training Loss: 0.08231377601623535\n",
      "Epoch 26819/30000 Training Loss: 0.06505870819091797\n",
      "Epoch 26820/30000 Training Loss: 0.06636936217546463\n",
      "Epoch 26820/30000 Validation Loss: 0.08646032214164734\n",
      "Epoch 26821/30000 Training Loss: 0.08888817578554153\n",
      "Epoch 26822/30000 Training Loss: 0.08284785598516464\n",
      "Epoch 26823/30000 Training Loss: 0.058384478092193604\n",
      "Epoch 26824/30000 Training Loss: 0.06683093309402466\n",
      "Epoch 26825/30000 Training Loss: 0.05541355907917023\n",
      "Epoch 26826/30000 Training Loss: 0.07877111434936523\n",
      "Epoch 26827/30000 Training Loss: 0.06090526282787323\n",
      "Epoch 26828/30000 Training Loss: 0.05917845293879509\n",
      "Epoch 26829/30000 Training Loss: 0.07416980713605881\n",
      "Epoch 26830/30000 Training Loss: 0.07826787233352661\n",
      "Epoch 26830/30000 Validation Loss: 0.06467442214488983\n",
      "Epoch 26831/30000 Training Loss: 0.07020137459039688\n",
      "Epoch 26832/30000 Training Loss: 0.06817609071731567\n",
      "Epoch 26833/30000 Training Loss: 0.07856929302215576\n",
      "Epoch 26834/30000 Training Loss: 0.06831898540258408\n",
      "Epoch 26835/30000 Training Loss: 0.06283595412969589\n",
      "Epoch 26836/30000 Training Loss: 0.0755016952753067\n",
      "Epoch 26837/30000 Training Loss: 0.07376504689455032\n",
      "Epoch 26838/30000 Training Loss: 0.060784440487623215\n",
      "Epoch 26839/30000 Training Loss: 0.08923076838254929\n",
      "Epoch 26840/30000 Training Loss: 0.0575312115252018\n",
      "Epoch 26840/30000 Validation Loss: 0.051859039813280106\n",
      "Epoch 26841/30000 Training Loss: 0.07837557792663574\n",
      "Epoch 26842/30000 Training Loss: 0.07227423042058945\n",
      "Epoch 26843/30000 Training Loss: 0.050981879234313965\n",
      "Epoch 26844/30000 Training Loss: 0.09880167990922928\n",
      "Epoch 26845/30000 Training Loss: 0.07342711836099625\n",
      "Epoch 26846/30000 Training Loss: 0.06790552288293839\n",
      "Epoch 26847/30000 Training Loss: 0.05211300775408745\n",
      "Epoch 26848/30000 Training Loss: 0.054978594183921814\n",
      "Epoch 26849/30000 Training Loss: 0.07873212546110153\n",
      "Epoch 26850/30000 Training Loss: 0.07739710062742233\n",
      "Epoch 26850/30000 Validation Loss: 0.06299422681331635\n",
      "Epoch 26851/30000 Training Loss: 0.06027977541089058\n",
      "Epoch 26852/30000 Training Loss: 0.06864694505929947\n",
      "Epoch 26853/30000 Training Loss: 0.0800279900431633\n",
      "Epoch 26854/30000 Training Loss: 0.07375117391347885\n",
      "Epoch 26855/30000 Training Loss: 0.07611749321222305\n",
      "Epoch 26856/30000 Training Loss: 0.06515409797430038\n",
      "Epoch 26857/30000 Training Loss: 0.08206465095281601\n",
      "Epoch 26858/30000 Training Loss: 0.05804910138249397\n",
      "Epoch 26859/30000 Training Loss: 0.0684100091457367\n",
      "Epoch 26860/30000 Training Loss: 0.0777282640337944\n",
      "Epoch 26860/30000 Validation Loss: 0.06192556023597717\n",
      "Epoch 26861/30000 Training Loss: 0.0833907499909401\n",
      "Epoch 26862/30000 Training Loss: 0.06903940439224243\n",
      "Epoch 26863/30000 Training Loss: 0.10174044221639633\n",
      "Epoch 26864/30000 Training Loss: 0.05003121867775917\n",
      "Epoch 26865/30000 Training Loss: 0.05315259099006653\n",
      "Epoch 26866/30000 Training Loss: 0.06246979907155037\n",
      "Epoch 26867/30000 Training Loss: 0.07195761799812317\n",
      "Epoch 26868/30000 Training Loss: 0.05612829327583313\n",
      "Epoch 26869/30000 Training Loss: 0.05610884353518486\n",
      "Epoch 26870/30000 Training Loss: 0.07817110419273376\n",
      "Epoch 26870/30000 Validation Loss: 0.07508822530508041\n",
      "Epoch 26871/30000 Training Loss: 0.07632079720497131\n",
      "Epoch 26872/30000 Training Loss: 0.057451874017715454\n",
      "Epoch 26873/30000 Training Loss: 0.07221586257219315\n",
      "Epoch 26874/30000 Training Loss: 0.08314309269189835\n",
      "Epoch 26875/30000 Training Loss: 0.0713236853480339\n",
      "Epoch 26876/30000 Training Loss: 0.08568789809942245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26877/30000 Training Loss: 0.0767742395401001\n",
      "Epoch 26878/30000 Training Loss: 0.06170623376965523\n",
      "Epoch 26879/30000 Training Loss: 0.0614667646586895\n",
      "Epoch 26880/30000 Training Loss: 0.07255705446004868\n",
      "Epoch 26880/30000 Validation Loss: 0.07277575880289078\n",
      "Epoch 26881/30000 Training Loss: 0.08126956224441528\n",
      "Epoch 26882/30000 Training Loss: 0.07007674127817154\n",
      "Epoch 26883/30000 Training Loss: 0.06553858518600464\n",
      "Epoch 26884/30000 Training Loss: 0.07922748476266861\n",
      "Epoch 26885/30000 Training Loss: 0.07370499521493912\n",
      "Epoch 26886/30000 Training Loss: 0.06980019062757492\n",
      "Epoch 26887/30000 Training Loss: 0.07279572635889053\n",
      "Epoch 26888/30000 Training Loss: 0.07121581584215164\n",
      "Epoch 26889/30000 Training Loss: 0.07080566883087158\n",
      "Epoch 26890/30000 Training Loss: 0.06836283206939697\n",
      "Epoch 26890/30000 Validation Loss: 0.08035675436258316\n",
      "Epoch 26891/30000 Training Loss: 0.085856594145298\n",
      "Epoch 26892/30000 Training Loss: 0.0912381187081337\n",
      "Epoch 26893/30000 Training Loss: 0.08547908812761307\n",
      "Epoch 26894/30000 Training Loss: 0.07254528254270554\n",
      "Epoch 26895/30000 Training Loss: 0.06590070575475693\n",
      "Epoch 26896/30000 Training Loss: 0.07566166669130325\n",
      "Epoch 26897/30000 Training Loss: 0.053049053996801376\n",
      "Epoch 26898/30000 Training Loss: 0.07444529980421066\n",
      "Epoch 26899/30000 Training Loss: 0.07436134666204453\n",
      "Epoch 26900/30000 Training Loss: 0.0559987835586071\n",
      "Epoch 26900/30000 Validation Loss: 0.065043605864048\n",
      "Epoch 26901/30000 Training Loss: 0.06783144921064377\n",
      "Epoch 26902/30000 Training Loss: 0.07593568414449692\n",
      "Epoch 26903/30000 Training Loss: 0.09164846688508987\n",
      "Epoch 26904/30000 Training Loss: 0.06883076578378677\n",
      "Epoch 26905/30000 Training Loss: 0.07986415922641754\n",
      "Epoch 26906/30000 Training Loss: 0.05837150290608406\n",
      "Epoch 26907/30000 Training Loss: 0.07333481311798096\n",
      "Epoch 26908/30000 Training Loss: 0.05476643517613411\n",
      "Epoch 26909/30000 Training Loss: 0.0734671875834465\n",
      "Epoch 26910/30000 Training Loss: 0.07361932843923569\n",
      "Epoch 26910/30000 Validation Loss: 0.06536681205034256\n",
      "Epoch 26911/30000 Training Loss: 0.06809831410646439\n",
      "Epoch 26912/30000 Training Loss: 0.058386702090501785\n",
      "Epoch 26913/30000 Training Loss: 0.07512611150741577\n",
      "Epoch 26914/30000 Training Loss: 0.08742719888687134\n",
      "Epoch 26915/30000 Training Loss: 0.08332575112581253\n",
      "Epoch 26916/30000 Training Loss: 0.077366404235363\n",
      "Epoch 26917/30000 Training Loss: 0.05342082306742668\n",
      "Epoch 26918/30000 Training Loss: 0.06027519330382347\n",
      "Epoch 26919/30000 Training Loss: 0.09873843193054199\n",
      "Epoch 26920/30000 Training Loss: 0.07272640615701675\n",
      "Epoch 26920/30000 Validation Loss: 0.07633259892463684\n",
      "Epoch 26921/30000 Training Loss: 0.06348296254873276\n",
      "Epoch 26922/30000 Training Loss: 0.059302881360054016\n",
      "Epoch 26923/30000 Training Loss: 0.08241797238588333\n",
      "Epoch 26924/30000 Training Loss: 0.07014534622430801\n",
      "Epoch 26925/30000 Training Loss: 0.07383381575345993\n",
      "Epoch 26926/30000 Training Loss: 0.07381193339824677\n",
      "Epoch 26927/30000 Training Loss: 0.06150789558887482\n",
      "Epoch 26928/30000 Training Loss: 0.06196412816643715\n",
      "Epoch 26929/30000 Training Loss: 0.051131341606378555\n",
      "Epoch 26930/30000 Training Loss: 0.0634593665599823\n",
      "Epoch 26930/30000 Validation Loss: 0.058931007981300354\n",
      "Epoch 26931/30000 Training Loss: 0.07753711938858032\n",
      "Epoch 26932/30000 Training Loss: 0.06868038326501846\n",
      "Epoch 26933/30000 Training Loss: 0.06988812237977982\n",
      "Epoch 26934/30000 Training Loss: 0.07655901461839676\n",
      "Epoch 26935/30000 Training Loss: 0.08341728895902634\n",
      "Epoch 26936/30000 Training Loss: 0.05191338062286377\n",
      "Epoch 26937/30000 Training Loss: 0.06922652572393417\n",
      "Epoch 26938/30000 Training Loss: 0.06834178417921066\n",
      "Epoch 26939/30000 Training Loss: 0.06816408783197403\n",
      "Epoch 26940/30000 Training Loss: 0.060120806097984314\n",
      "Epoch 26940/30000 Validation Loss: 0.06603207439184189\n",
      "Epoch 26941/30000 Training Loss: 0.05428856238722801\n",
      "Epoch 26942/30000 Training Loss: 0.061846643686294556\n",
      "Epoch 26943/30000 Training Loss: 0.08971825987100601\n",
      "Epoch 26944/30000 Training Loss: 0.07743271440267563\n",
      "Epoch 26945/30000 Training Loss: 0.07611146569252014\n",
      "Epoch 26946/30000 Training Loss: 0.05006851628422737\n",
      "Epoch 26947/30000 Training Loss: 0.05593114718794823\n",
      "Epoch 26948/30000 Training Loss: 0.05156653746962547\n",
      "Epoch 26949/30000 Training Loss: 0.06224741041660309\n",
      "Epoch 26950/30000 Training Loss: 0.06141131743788719\n",
      "Epoch 26950/30000 Validation Loss: 0.05848473310470581\n",
      "Epoch 26951/30000 Training Loss: 0.07175222784280777\n",
      "Epoch 26952/30000 Training Loss: 0.05880506709218025\n",
      "Epoch 26953/30000 Training Loss: 0.08835256099700928\n",
      "Epoch 26954/30000 Training Loss: 0.058202147483825684\n",
      "Epoch 26955/30000 Training Loss: 0.09078031778335571\n",
      "Epoch 26956/30000 Training Loss: 0.07109752297401428\n",
      "Epoch 26957/30000 Training Loss: 0.08692280203104019\n",
      "Epoch 26958/30000 Training Loss: 0.07565870881080627\n",
      "Epoch 26959/30000 Training Loss: 0.06973075866699219\n",
      "Epoch 26960/30000 Training Loss: 0.08913680166006088\n",
      "Epoch 26960/30000 Validation Loss: 0.052284132689237595\n",
      "Epoch 26961/30000 Training Loss: 0.06844926625490189\n",
      "Epoch 26962/30000 Training Loss: 0.06296727061271667\n",
      "Epoch 26963/30000 Training Loss: 0.07036643475294113\n",
      "Epoch 26964/30000 Training Loss: 0.05732671916484833\n",
      "Epoch 26965/30000 Training Loss: 0.05235844850540161\n",
      "Epoch 26966/30000 Training Loss: 0.0688924714922905\n",
      "Epoch 26967/30000 Training Loss: 0.08342952281236649\n",
      "Epoch 26968/30000 Training Loss: 0.07207933068275452\n",
      "Epoch 26969/30000 Training Loss: 0.060792285948991776\n",
      "Epoch 26970/30000 Training Loss: 0.06069347634911537\n",
      "Epoch 26970/30000 Validation Loss: 0.06794658303260803\n",
      "Epoch 26971/30000 Training Loss: 0.059142619371414185\n",
      "Epoch 26972/30000 Training Loss: 0.060692574828863144\n",
      "Epoch 26973/30000 Training Loss: 0.07035576552152634\n",
      "Epoch 26974/30000 Training Loss: 0.06645910441875458\n",
      "Epoch 26975/30000 Training Loss: 0.07378645986318588\n",
      "Epoch 26976/30000 Training Loss: 0.0585433654487133\n",
      "Epoch 26977/30000 Training Loss: 0.07136810570955276\n",
      "Epoch 26978/30000 Training Loss: 0.07229689508676529\n",
      "Epoch 26979/30000 Training Loss: 0.05763429403305054\n",
      "Epoch 26980/30000 Training Loss: 0.05929766595363617\n",
      "Epoch 26980/30000 Validation Loss: 0.0724039301276207\n",
      "Epoch 26981/30000 Training Loss: 0.076322041451931\n",
      "Epoch 26982/30000 Training Loss: 0.08430835604667664\n",
      "Epoch 26983/30000 Training Loss: 0.07089566439390182\n",
      "Epoch 26984/30000 Training Loss: 0.0750027447938919\n",
      "Epoch 26985/30000 Training Loss: 0.06724750250577927\n",
      "Epoch 26986/30000 Training Loss: 0.059495192021131516\n",
      "Epoch 26987/30000 Training Loss: 0.07733658701181412\n",
      "Epoch 26988/30000 Training Loss: 0.06955837458372116\n",
      "Epoch 26989/30000 Training Loss: 0.09045974165201187\n",
      "Epoch 26990/30000 Training Loss: 0.06188671663403511\n",
      "Epoch 26990/30000 Validation Loss: 0.07991208881139755\n",
      "Epoch 26991/30000 Training Loss: 0.07408375293016434\n",
      "Epoch 26992/30000 Training Loss: 0.07647120952606201\n",
      "Epoch 26993/30000 Training Loss: 0.08517175167798996\n",
      "Epoch 26994/30000 Training Loss: 0.0763433575630188\n",
      "Epoch 26995/30000 Training Loss: 0.07727613300085068\n",
      "Epoch 26996/30000 Training Loss: 0.0811392143368721\n",
      "Epoch 26997/30000 Training Loss: 0.05803040787577629\n",
      "Epoch 26998/30000 Training Loss: 0.07623284310102463\n",
      "Epoch 26999/30000 Training Loss: 0.06920864433050156\n",
      "Epoch 27000/30000 Training Loss: 0.09514248371124268\n",
      "Epoch 27000/30000 Validation Loss: 0.06315720826387405\n",
      "Epoch 27001/30000 Training Loss: 0.05355767533183098\n",
      "Epoch 27002/30000 Training Loss: 0.05287357047200203\n",
      "Epoch 27003/30000 Training Loss: 0.061109092086553574\n",
      "Epoch 27004/30000 Training Loss: 0.0537477545440197\n",
      "Epoch 27005/30000 Training Loss: 0.05777457356452942\n",
      "Epoch 27006/30000 Training Loss: 0.07249055057764053\n",
      "Epoch 27007/30000 Training Loss: 0.07386952638626099\n",
      "Epoch 27008/30000 Training Loss: 0.059392306953668594\n",
      "Epoch 27009/30000 Training Loss: 0.07077938318252563\n",
      "Epoch 27010/30000 Training Loss: 0.0621139220893383\n",
      "Epoch 27010/30000 Validation Loss: 0.06930654495954514\n",
      "Epoch 27011/30000 Training Loss: 0.06254785507917404\n",
      "Epoch 27012/30000 Training Loss: 0.09410104900598526\n",
      "Epoch 27013/30000 Training Loss: 0.06910034269094467\n",
      "Epoch 27014/30000 Training Loss: 0.07282688468694687\n",
      "Epoch 27015/30000 Training Loss: 0.06686088442802429\n",
      "Epoch 27016/30000 Training Loss: 0.05667206645011902\n",
      "Epoch 27017/30000 Training Loss: 0.08330594748258591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27018/30000 Training Loss: 0.0768381878733635\n",
      "Epoch 27019/30000 Training Loss: 0.06342317909002304\n",
      "Epoch 27020/30000 Training Loss: 0.06472589075565338\n",
      "Epoch 27020/30000 Validation Loss: 0.05826031044125557\n",
      "Epoch 27021/30000 Training Loss: 0.07816057652235031\n",
      "Epoch 27022/30000 Training Loss: 0.06635165959596634\n",
      "Epoch 27023/30000 Training Loss: 0.06417132169008255\n",
      "Epoch 27024/30000 Training Loss: 0.05814358964562416\n",
      "Epoch 27025/30000 Training Loss: 0.07963087409734726\n",
      "Epoch 27026/30000 Training Loss: 0.0792061910033226\n",
      "Epoch 27027/30000 Training Loss: 0.0892321765422821\n",
      "Epoch 27028/30000 Training Loss: 0.07655077427625656\n",
      "Epoch 27029/30000 Training Loss: 0.05907433107495308\n",
      "Epoch 27030/30000 Training Loss: 0.06362345069646835\n",
      "Epoch 27030/30000 Validation Loss: 0.07329403609037399\n",
      "Epoch 27031/30000 Training Loss: 0.0535987913608551\n",
      "Epoch 27032/30000 Training Loss: 0.05165192112326622\n",
      "Epoch 27033/30000 Training Loss: 0.0630607083439827\n",
      "Epoch 27034/30000 Training Loss: 0.08192525058984756\n",
      "Epoch 27035/30000 Training Loss: 0.07162249088287354\n",
      "Epoch 27036/30000 Training Loss: 0.07868295907974243\n",
      "Epoch 27037/30000 Training Loss: 0.08735907822847366\n",
      "Epoch 27038/30000 Training Loss: 0.08744920045137405\n",
      "Epoch 27039/30000 Training Loss: 0.0763758197426796\n",
      "Epoch 27040/30000 Training Loss: 0.06356773525476456\n",
      "Epoch 27040/30000 Validation Loss: 0.07937748730182648\n",
      "Epoch 27041/30000 Training Loss: 0.09346136450767517\n",
      "Epoch 27042/30000 Training Loss: 0.05767374113202095\n",
      "Epoch 27043/30000 Training Loss: 0.07311151921749115\n",
      "Epoch 27044/30000 Training Loss: 0.08162959665060043\n",
      "Epoch 27045/30000 Training Loss: 0.09019884467124939\n",
      "Epoch 27046/30000 Training Loss: 0.06956558674573898\n",
      "Epoch 27047/30000 Training Loss: 0.0550151951611042\n",
      "Epoch 27048/30000 Training Loss: 0.06478095799684525\n",
      "Epoch 27049/30000 Training Loss: 0.067158542573452\n",
      "Epoch 27050/30000 Training Loss: 0.08234753459692001\n",
      "Epoch 27050/30000 Validation Loss: 0.07206810265779495\n",
      "Epoch 27051/30000 Training Loss: 0.05491606891155243\n",
      "Epoch 27052/30000 Training Loss: 0.08278226852416992\n",
      "Epoch 27053/30000 Training Loss: 0.06931193172931671\n",
      "Epoch 27054/30000 Training Loss: 0.05400915816426277\n",
      "Epoch 27055/30000 Training Loss: 0.06261631101369858\n",
      "Epoch 27056/30000 Training Loss: 0.06146100163459778\n",
      "Epoch 27057/30000 Training Loss: 0.062485963106155396\n",
      "Epoch 27058/30000 Training Loss: 0.07117541879415512\n",
      "Epoch 27059/30000 Training Loss: 0.08032706379890442\n",
      "Epoch 27060/30000 Training Loss: 0.04829967021942139\n",
      "Epoch 27060/30000 Validation Loss: 0.06762021780014038\n",
      "Epoch 27061/30000 Training Loss: 0.07289674878120422\n",
      "Epoch 27062/30000 Training Loss: 0.06623483449220657\n",
      "Epoch 27063/30000 Training Loss: 0.0843677893280983\n",
      "Epoch 27064/30000 Training Loss: 0.06689473241567612\n",
      "Epoch 27065/30000 Training Loss: 0.0825619325041771\n",
      "Epoch 27066/30000 Training Loss: 0.06255742907524109\n",
      "Epoch 27067/30000 Training Loss: 0.07018131762742996\n",
      "Epoch 27068/30000 Training Loss: 0.06254292279481888\n",
      "Epoch 27069/30000 Training Loss: 0.058517973870038986\n",
      "Epoch 27070/30000 Training Loss: 0.057595085352659225\n",
      "Epoch 27070/30000 Validation Loss: 0.062025975435972214\n",
      "Epoch 27071/30000 Training Loss: 0.06628236919641495\n",
      "Epoch 27072/30000 Training Loss: 0.07970937341451645\n",
      "Epoch 27073/30000 Training Loss: 0.08961894363164902\n",
      "Epoch 27074/30000 Training Loss: 0.054258693009614944\n",
      "Epoch 27075/30000 Training Loss: 0.085565485060215\n",
      "Epoch 27076/30000 Training Loss: 0.07627422362565994\n",
      "Epoch 27077/30000 Training Loss: 0.07303261011838913\n",
      "Epoch 27078/30000 Training Loss: 0.07400042563676834\n",
      "Epoch 27079/30000 Training Loss: 0.08158902078866959\n",
      "Epoch 27080/30000 Training Loss: 0.059419143944978714\n",
      "Epoch 27080/30000 Validation Loss: 0.10663404315710068\n",
      "Epoch 27081/30000 Training Loss: 0.06125766038894653\n",
      "Epoch 27082/30000 Training Loss: 0.07249319553375244\n",
      "Epoch 27083/30000 Training Loss: 0.06891432404518127\n",
      "Epoch 27084/30000 Training Loss: 0.07099059224128723\n",
      "Epoch 27085/30000 Training Loss: 0.06443647295236588\n",
      "Epoch 27086/30000 Training Loss: 0.06424680352210999\n",
      "Epoch 27087/30000 Training Loss: 0.06681960076093674\n",
      "Epoch 27088/30000 Training Loss: 0.07355887442827225\n",
      "Epoch 27089/30000 Training Loss: 0.07470156997442245\n",
      "Epoch 27090/30000 Training Loss: 0.07146751135587692\n",
      "Epoch 27090/30000 Validation Loss: 0.05543428286910057\n",
      "Epoch 27091/30000 Training Loss: 0.0798659399151802\n",
      "Epoch 27092/30000 Training Loss: 0.06890828162431717\n",
      "Epoch 27093/30000 Training Loss: 0.06859687715768814\n",
      "Epoch 27094/30000 Training Loss: 0.07454250007867813\n",
      "Epoch 27095/30000 Training Loss: 0.06699520349502563\n",
      "Epoch 27096/30000 Training Loss: 0.08255068212747574\n",
      "Epoch 27097/30000 Training Loss: 0.06545836478471756\n",
      "Epoch 27098/30000 Training Loss: 0.06793302297592163\n",
      "Epoch 27099/30000 Training Loss: 0.07938068360090256\n",
      "Epoch 27100/30000 Training Loss: 0.06570479273796082\n",
      "Epoch 27100/30000 Validation Loss: 0.07412894815206528\n",
      "Epoch 27101/30000 Training Loss: 0.07631270587444305\n",
      "Epoch 27102/30000 Training Loss: 0.06978828459978104\n",
      "Epoch 27103/30000 Training Loss: 0.054115984588861465\n",
      "Epoch 27104/30000 Training Loss: 0.064825139939785\n",
      "Epoch 27105/30000 Training Loss: 0.06259376555681229\n",
      "Epoch 27106/30000 Training Loss: 0.06734835356473923\n",
      "Epoch 27107/30000 Training Loss: 0.07284451276063919\n",
      "Epoch 27108/30000 Training Loss: 0.07768843322992325\n",
      "Epoch 27109/30000 Training Loss: 0.07151167839765549\n",
      "Epoch 27110/30000 Training Loss: 0.06743228435516357\n",
      "Epoch 27110/30000 Validation Loss: 0.06983355432748795\n",
      "Epoch 27111/30000 Training Loss: 0.0740523636341095\n",
      "Epoch 27112/30000 Training Loss: 0.07516569644212723\n",
      "Epoch 27113/30000 Training Loss: 0.054161738604307175\n",
      "Epoch 27114/30000 Training Loss: 0.05217643454670906\n",
      "Epoch 27115/30000 Training Loss: 0.08448449522256851\n",
      "Epoch 27116/30000 Training Loss: 0.09554028511047363\n",
      "Epoch 27117/30000 Training Loss: 0.06444386392831802\n",
      "Epoch 27118/30000 Training Loss: 0.07554086297750473\n",
      "Epoch 27119/30000 Training Loss: 0.07730364799499512\n",
      "Epoch 27120/30000 Training Loss: 0.06646030396223068\n",
      "Epoch 27120/30000 Validation Loss: 0.07373049110174179\n",
      "Epoch 27121/30000 Training Loss: 0.06687618046998978\n",
      "Epoch 27122/30000 Training Loss: 0.056088730692863464\n",
      "Epoch 27123/30000 Training Loss: 0.08485725522041321\n",
      "Epoch 27124/30000 Training Loss: 0.048680514097213745\n",
      "Epoch 27125/30000 Training Loss: 0.08319677412509918\n",
      "Epoch 27126/30000 Training Loss: 0.06512045115232468\n",
      "Epoch 27127/30000 Training Loss: 0.056663576513528824\n",
      "Epoch 27128/30000 Training Loss: 0.0709284320473671\n",
      "Epoch 27129/30000 Training Loss: 0.07883349806070328\n",
      "Epoch 27130/30000 Training Loss: 0.06524302065372467\n",
      "Epoch 27130/30000 Validation Loss: 0.061778534203767776\n",
      "Epoch 27131/30000 Training Loss: 0.058592572808265686\n",
      "Epoch 27132/30000 Training Loss: 0.06363394111394882\n",
      "Epoch 27133/30000 Training Loss: 0.06103022024035454\n",
      "Epoch 27134/30000 Training Loss: 0.07616743445396423\n",
      "Epoch 27135/30000 Training Loss: 0.0720599889755249\n",
      "Epoch 27136/30000 Training Loss: 0.05572519823908806\n",
      "Epoch 27137/30000 Training Loss: 0.07652825117111206\n",
      "Epoch 27138/30000 Training Loss: 0.054317791014909744\n",
      "Epoch 27139/30000 Training Loss: 0.0778399333357811\n",
      "Epoch 27140/30000 Training Loss: 0.05810209736227989\n",
      "Epoch 27140/30000 Validation Loss: 0.07971960306167603\n",
      "Epoch 27141/30000 Training Loss: 0.0642106905579567\n",
      "Epoch 27142/30000 Training Loss: 0.06768131256103516\n",
      "Epoch 27143/30000 Training Loss: 0.07766375690698624\n",
      "Epoch 27144/30000 Training Loss: 0.06962037086486816\n",
      "Epoch 27145/30000 Training Loss: 0.06723982095718384\n",
      "Epoch 27146/30000 Training Loss: 0.07946114242076874\n",
      "Epoch 27147/30000 Training Loss: 0.0726250484585762\n",
      "Epoch 27148/30000 Training Loss: 0.07278839498758316\n",
      "Epoch 27149/30000 Training Loss: 0.07099534571170807\n",
      "Epoch 27150/30000 Training Loss: 0.06242598965764046\n",
      "Epoch 27150/30000 Validation Loss: 0.06304877996444702\n",
      "Epoch 27151/30000 Training Loss: 0.04971843957901001\n",
      "Epoch 27152/30000 Training Loss: 0.07462114840745926\n",
      "Epoch 27153/30000 Training Loss: 0.06773529201745987\n",
      "Epoch 27154/30000 Training Loss: 0.063141830265522\n",
      "Epoch 27155/30000 Training Loss: 0.054519861936569214\n",
      "Epoch 27156/30000 Training Loss: 0.05647161975502968\n",
      "Epoch 27157/30000 Training Loss: 0.07348459213972092\n",
      "Epoch 27158/30000 Training Loss: 0.050186555832624435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27159/30000 Training Loss: 0.07695639878511429\n",
      "Epoch 27160/30000 Training Loss: 0.07443850487470627\n",
      "Epoch 27160/30000 Validation Loss: 0.0848359763622284\n",
      "Epoch 27161/30000 Training Loss: 0.06925299018621445\n",
      "Epoch 27162/30000 Training Loss: 0.06791562587022781\n",
      "Epoch 27163/30000 Training Loss: 0.08146490901708603\n",
      "Epoch 27164/30000 Training Loss: 0.06558229774236679\n",
      "Epoch 27165/30000 Training Loss: 0.08577940613031387\n",
      "Epoch 27166/30000 Training Loss: 0.07219512015581131\n",
      "Epoch 27167/30000 Training Loss: 0.06079557165503502\n",
      "Epoch 27168/30000 Training Loss: 0.06285592913627625\n",
      "Epoch 27169/30000 Training Loss: 0.06490079313516617\n",
      "Epoch 27170/30000 Training Loss: 0.06730546802282333\n",
      "Epoch 27170/30000 Validation Loss: 0.07895610481500626\n",
      "Epoch 27171/30000 Training Loss: 0.08302351087331772\n",
      "Epoch 27172/30000 Training Loss: 0.07198493927717209\n",
      "Epoch 27173/30000 Training Loss: 0.07344359904527664\n",
      "Epoch 27174/30000 Training Loss: 0.07516304403543472\n",
      "Epoch 27175/30000 Training Loss: 0.07017575949430466\n",
      "Epoch 27176/30000 Training Loss: 0.06968554854393005\n",
      "Epoch 27177/30000 Training Loss: 0.06196194887161255\n",
      "Epoch 27178/30000 Training Loss: 0.07233699411153793\n",
      "Epoch 27179/30000 Training Loss: 0.0638481006026268\n",
      "Epoch 27180/30000 Training Loss: 0.06266439706087112\n",
      "Epoch 27180/30000 Validation Loss: 0.060031596571207047\n",
      "Epoch 27181/30000 Training Loss: 0.0818353220820427\n",
      "Epoch 27182/30000 Training Loss: 0.0594504289329052\n",
      "Epoch 27183/30000 Training Loss: 0.07334470748901367\n",
      "Epoch 27184/30000 Training Loss: 0.062288474291563034\n",
      "Epoch 27185/30000 Training Loss: 0.06716076284646988\n",
      "Epoch 27186/30000 Training Loss: 0.07727474719285965\n",
      "Epoch 27187/30000 Training Loss: 0.08638253062963486\n",
      "Epoch 27188/30000 Training Loss: 0.05620703101158142\n",
      "Epoch 27189/30000 Training Loss: 0.08307958394289017\n",
      "Epoch 27190/30000 Training Loss: 0.06322861462831497\n",
      "Epoch 27190/30000 Validation Loss: 0.05777667090296745\n",
      "Epoch 27191/30000 Training Loss: 0.056850533932447433\n",
      "Epoch 27192/30000 Training Loss: 0.0870651975274086\n",
      "Epoch 27193/30000 Training Loss: 0.06756138056516647\n",
      "Epoch 27194/30000 Training Loss: 0.07318738102912903\n",
      "Epoch 27195/30000 Training Loss: 0.079536072909832\n",
      "Epoch 27196/30000 Training Loss: 0.07762042433023453\n",
      "Epoch 27197/30000 Training Loss: 0.060636382550001144\n",
      "Epoch 27198/30000 Training Loss: 0.06356719136238098\n",
      "Epoch 27199/30000 Training Loss: 0.08335784077644348\n",
      "Epoch 27200/30000 Training Loss: 0.07754343003034592\n",
      "Epoch 27200/30000 Validation Loss: 0.06902563571929932\n",
      "Epoch 27201/30000 Training Loss: 0.07000013440847397\n",
      "Epoch 27202/30000 Training Loss: 0.07027024775743484\n",
      "Epoch 27203/30000 Training Loss: 0.08023881912231445\n",
      "Epoch 27204/30000 Training Loss: 0.07758435606956482\n",
      "Epoch 27205/30000 Training Loss: 0.06453276425600052\n",
      "Epoch 27206/30000 Training Loss: 0.06942520290613174\n",
      "Epoch 27207/30000 Training Loss: 0.06568769365549088\n",
      "Epoch 27208/30000 Training Loss: 0.05778185650706291\n",
      "Epoch 27209/30000 Training Loss: 0.059234414249658585\n",
      "Epoch 27210/30000 Training Loss: 0.08842676877975464\n",
      "Epoch 27210/30000 Validation Loss: 0.08275943249464035\n",
      "Epoch 27211/30000 Training Loss: 0.06037427857518196\n",
      "Epoch 27212/30000 Training Loss: 0.06747212260961533\n",
      "Epoch 27213/30000 Training Loss: 0.07854823023080826\n",
      "Epoch 27214/30000 Training Loss: 0.07364186644554138\n",
      "Epoch 27215/30000 Training Loss: 0.068405382335186\n",
      "Epoch 27216/30000 Training Loss: 0.06809370964765549\n",
      "Epoch 27217/30000 Training Loss: 0.09850195795297623\n",
      "Epoch 27218/30000 Training Loss: 0.08208132535219193\n",
      "Epoch 27219/30000 Training Loss: 0.07157706469297409\n",
      "Epoch 27220/30000 Training Loss: 0.07447013258934021\n",
      "Epoch 27220/30000 Validation Loss: 0.06873533874750137\n",
      "Epoch 27221/30000 Training Loss: 0.0781811848282814\n",
      "Epoch 27222/30000 Training Loss: 0.06395229697227478\n",
      "Epoch 27223/30000 Training Loss: 0.07270625233650208\n",
      "Epoch 27224/30000 Training Loss: 0.06899227946996689\n",
      "Epoch 27225/30000 Training Loss: 0.0894758403301239\n",
      "Epoch 27226/30000 Training Loss: 0.06817705929279327\n",
      "Epoch 27227/30000 Training Loss: 0.06099948659539223\n",
      "Epoch 27228/30000 Training Loss: 0.06245201826095581\n",
      "Epoch 27229/30000 Training Loss: 0.06237666308879852\n",
      "Epoch 27230/30000 Training Loss: 0.06439714878797531\n",
      "Epoch 27230/30000 Validation Loss: 0.07234043627977371\n",
      "Epoch 27231/30000 Training Loss: 0.07629173994064331\n",
      "Epoch 27232/30000 Training Loss: 0.0899108275771141\n",
      "Epoch 27233/30000 Training Loss: 0.07178305089473724\n",
      "Epoch 27234/30000 Training Loss: 0.06970304995775223\n",
      "Epoch 27235/30000 Training Loss: 0.06044996902346611\n",
      "Epoch 27236/30000 Training Loss: 0.06166507676243782\n",
      "Epoch 27237/30000 Training Loss: 0.06560051441192627\n",
      "Epoch 27238/30000 Training Loss: 0.0602310486137867\n",
      "Epoch 27239/30000 Training Loss: 0.07605702430009842\n",
      "Epoch 27240/30000 Training Loss: 0.07791910320520401\n",
      "Epoch 27240/30000 Validation Loss: 0.083119235932827\n",
      "Epoch 27241/30000 Training Loss: 0.07968977838754654\n",
      "Epoch 27242/30000 Training Loss: 0.06769119948148727\n",
      "Epoch 27243/30000 Training Loss: 0.08687152713537216\n",
      "Epoch 27244/30000 Training Loss: 0.06948095560073853\n",
      "Epoch 27245/30000 Training Loss: 0.07997357100248337\n",
      "Epoch 27246/30000 Training Loss: 0.07270962744951248\n",
      "Epoch 27247/30000 Training Loss: 0.05801049992442131\n",
      "Epoch 27248/30000 Training Loss: 0.06305410712957382\n",
      "Epoch 27249/30000 Training Loss: 0.07152220606803894\n",
      "Epoch 27250/30000 Training Loss: 0.06353688985109329\n",
      "Epoch 27250/30000 Validation Loss: 0.08543935418128967\n",
      "Epoch 27251/30000 Training Loss: 0.07097466289997101\n",
      "Epoch 27252/30000 Training Loss: 0.06980861723423004\n",
      "Epoch 27253/30000 Training Loss: 0.06890558451414108\n",
      "Epoch 27254/30000 Training Loss: 0.06026419997215271\n",
      "Epoch 27255/30000 Training Loss: 0.06486081331968307\n",
      "Epoch 27256/30000 Training Loss: 0.07572486251592636\n",
      "Epoch 27257/30000 Training Loss: 0.07586100697517395\n",
      "Epoch 27258/30000 Training Loss: 0.0741862952709198\n",
      "Epoch 27259/30000 Training Loss: 0.07409902662038803\n",
      "Epoch 27260/30000 Training Loss: 0.05917152762413025\n",
      "Epoch 27260/30000 Validation Loss: 0.06496027112007141\n",
      "Epoch 27261/30000 Training Loss: 0.06537694483995438\n",
      "Epoch 27262/30000 Training Loss: 0.09116283804178238\n",
      "Epoch 27263/30000 Training Loss: 0.0723029300570488\n",
      "Epoch 27264/30000 Training Loss: 0.06716474890708923\n",
      "Epoch 27265/30000 Training Loss: 0.069038026034832\n",
      "Epoch 27266/30000 Training Loss: 0.07472439110279083\n",
      "Epoch 27267/30000 Training Loss: 0.07184163480997086\n",
      "Epoch 27268/30000 Training Loss: 0.09297382086515427\n",
      "Epoch 27269/30000 Training Loss: 0.06662546843290329\n",
      "Epoch 27270/30000 Training Loss: 0.06173861399292946\n",
      "Epoch 27270/30000 Validation Loss: 0.07246030122041702\n",
      "Epoch 27271/30000 Training Loss: 0.057678502053022385\n",
      "Epoch 27272/30000 Training Loss: 0.05665367469191551\n",
      "Epoch 27273/30000 Training Loss: 0.06303364783525467\n",
      "Epoch 27274/30000 Training Loss: 0.06448555737733841\n",
      "Epoch 27275/30000 Training Loss: 0.06481903791427612\n",
      "Epoch 27276/30000 Training Loss: 0.060942064970731735\n",
      "Epoch 27277/30000 Training Loss: 0.07215845584869385\n",
      "Epoch 27278/30000 Training Loss: 0.1054123267531395\n",
      "Epoch 27279/30000 Training Loss: 0.061841439455747604\n",
      "Epoch 27280/30000 Training Loss: 0.07493192702531815\n",
      "Epoch 27280/30000 Validation Loss: 0.06583591550588608\n",
      "Epoch 27281/30000 Training Loss: 0.05446534976363182\n",
      "Epoch 27282/30000 Training Loss: 0.07777537405490875\n",
      "Epoch 27283/30000 Training Loss: 0.0523035041987896\n",
      "Epoch 27284/30000 Training Loss: 0.06379175931215286\n",
      "Epoch 27285/30000 Training Loss: 0.06828516721725464\n",
      "Epoch 27286/30000 Training Loss: 0.06198388338088989\n",
      "Epoch 27287/30000 Training Loss: 0.07993251830339432\n",
      "Epoch 27288/30000 Training Loss: 0.06551488488912582\n",
      "Epoch 27289/30000 Training Loss: 0.060365352779626846\n",
      "Epoch 27290/30000 Training Loss: 0.06893009692430496\n",
      "Epoch 27290/30000 Validation Loss: 0.07014387100934982\n",
      "Epoch 27291/30000 Training Loss: 0.07757070660591125\n",
      "Epoch 27292/30000 Training Loss: 0.057527441531419754\n",
      "Epoch 27293/30000 Training Loss: 0.052736878395080566\n",
      "Epoch 27294/30000 Training Loss: 0.06796462088823318\n",
      "Epoch 27295/30000 Training Loss: 0.055027857422828674\n",
      "Epoch 27296/30000 Training Loss: 0.08520150184631348\n",
      "Epoch 27297/30000 Training Loss: 0.0606573186814785\n",
      "Epoch 27298/30000 Training Loss: 0.0550544448196888\n",
      "Epoch 27299/30000 Training Loss: 0.0652410089969635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27300/30000 Training Loss: 0.0706372931599617\n",
      "Epoch 27300/30000 Validation Loss: 0.07365730404853821\n",
      "Epoch 27301/30000 Training Loss: 0.078196220099926\n",
      "Epoch 27302/30000 Training Loss: 0.06783143430948257\n",
      "Epoch 27303/30000 Training Loss: 0.06421177834272385\n",
      "Epoch 27304/30000 Training Loss: 0.06018248572945595\n",
      "Epoch 27305/30000 Training Loss: 0.08137744665145874\n",
      "Epoch 27306/30000 Training Loss: 0.06828584522008896\n",
      "Epoch 27307/30000 Training Loss: 0.07530847936868668\n",
      "Epoch 27308/30000 Training Loss: 0.06292592734098434\n",
      "Epoch 27309/30000 Training Loss: 0.06976953148841858\n",
      "Epoch 27310/30000 Training Loss: 0.056312739849090576\n",
      "Epoch 27310/30000 Validation Loss: 0.06931149214506149\n",
      "Epoch 27311/30000 Training Loss: 0.07395730167627335\n",
      "Epoch 27312/30000 Training Loss: 0.06386011838912964\n",
      "Epoch 27313/30000 Training Loss: 0.09098293632268906\n",
      "Epoch 27314/30000 Training Loss: 0.05702237784862518\n",
      "Epoch 27315/30000 Training Loss: 0.07395514100790024\n",
      "Epoch 27316/30000 Training Loss: 0.07388066500425339\n",
      "Epoch 27317/30000 Training Loss: 0.07947299629449844\n",
      "Epoch 27318/30000 Training Loss: 0.07207920402288437\n",
      "Epoch 27319/30000 Training Loss: 0.07176462560892105\n",
      "Epoch 27320/30000 Training Loss: 0.056195732206106186\n",
      "Epoch 27320/30000 Validation Loss: 0.09833186864852905\n",
      "Epoch 27321/30000 Training Loss: 0.0839666798710823\n",
      "Epoch 27322/30000 Training Loss: 0.069026879966259\n",
      "Epoch 27323/30000 Training Loss: 0.05981016531586647\n",
      "Epoch 27324/30000 Training Loss: 0.07225865125656128\n",
      "Epoch 27325/30000 Training Loss: 0.06913549453020096\n",
      "Epoch 27326/30000 Training Loss: 0.08991170674562454\n",
      "Epoch 27327/30000 Training Loss: 0.05725885555148125\n",
      "Epoch 27328/30000 Training Loss: 0.05177457258105278\n",
      "Epoch 27329/30000 Training Loss: 0.08831629157066345\n",
      "Epoch 27330/30000 Training Loss: 0.08074632287025452\n",
      "Epoch 27330/30000 Validation Loss: 0.07448334246873856\n",
      "Epoch 27331/30000 Training Loss: 0.07895665615797043\n",
      "Epoch 27332/30000 Training Loss: 0.05760757252573967\n",
      "Epoch 27333/30000 Training Loss: 0.07180529832839966\n",
      "Epoch 27334/30000 Training Loss: 0.07009600847959518\n",
      "Epoch 27335/30000 Training Loss: 0.08144495636224747\n",
      "Epoch 27336/30000 Training Loss: 0.067998506128788\n",
      "Epoch 27337/30000 Training Loss: 0.06174961104989052\n",
      "Epoch 27338/30000 Training Loss: 0.0579969622194767\n",
      "Epoch 27339/30000 Training Loss: 0.0727795958518982\n",
      "Epoch 27340/30000 Training Loss: 0.07088208198547363\n",
      "Epoch 27340/30000 Validation Loss: 0.0752556249499321\n",
      "Epoch 27341/30000 Training Loss: 0.05773268640041351\n",
      "Epoch 27342/30000 Training Loss: 0.07705432921648026\n",
      "Epoch 27343/30000 Training Loss: 0.07921388000249863\n",
      "Epoch 27344/30000 Training Loss: 0.06241193786263466\n",
      "Epoch 27345/30000 Training Loss: 0.0677960142493248\n",
      "Epoch 27346/30000 Training Loss: 0.06708233803510666\n",
      "Epoch 27347/30000 Training Loss: 0.07668918371200562\n",
      "Epoch 27348/30000 Training Loss: 0.06336843967437744\n",
      "Epoch 27349/30000 Training Loss: 0.06783155351877213\n",
      "Epoch 27350/30000 Training Loss: 0.09018460661172867\n",
      "Epoch 27350/30000 Validation Loss: 0.0723314881324768\n",
      "Epoch 27351/30000 Training Loss: 0.05637633800506592\n",
      "Epoch 27352/30000 Training Loss: 0.06272340565919876\n",
      "Epoch 27353/30000 Training Loss: 0.0731278657913208\n",
      "Epoch 27354/30000 Training Loss: 0.08457083255052567\n",
      "Epoch 27355/30000 Training Loss: 0.07869940996170044\n",
      "Epoch 27356/30000 Training Loss: 0.07331735640764236\n",
      "Epoch 27357/30000 Training Loss: 0.06776995211839676\n",
      "Epoch 27358/30000 Training Loss: 0.07895750552415848\n",
      "Epoch 27359/30000 Training Loss: 0.05890911445021629\n",
      "Epoch 27360/30000 Training Loss: 0.06508120894432068\n",
      "Epoch 27360/30000 Validation Loss: 0.07791426032781601\n",
      "Epoch 27361/30000 Training Loss: 0.05312689021229744\n",
      "Epoch 27362/30000 Training Loss: 0.07004152983427048\n",
      "Epoch 27363/30000 Training Loss: 0.08382714539766312\n",
      "Epoch 27364/30000 Training Loss: 0.08575896173715591\n",
      "Epoch 27365/30000 Training Loss: 0.08293271064758301\n",
      "Epoch 27366/30000 Training Loss: 0.08095962554216385\n",
      "Epoch 27367/30000 Training Loss: 0.06841319054365158\n",
      "Epoch 27368/30000 Training Loss: 0.05091556906700134\n",
      "Epoch 27369/30000 Training Loss: 0.05228182673454285\n",
      "Epoch 27370/30000 Training Loss: 0.07349324226379395\n",
      "Epoch 27370/30000 Validation Loss: 0.06454017013311386\n",
      "Epoch 27371/30000 Training Loss: 0.06407741457223892\n",
      "Epoch 27372/30000 Training Loss: 0.07158345729112625\n",
      "Epoch 27373/30000 Training Loss: 0.07152511179447174\n",
      "Epoch 27374/30000 Training Loss: 0.07108110934495926\n",
      "Epoch 27375/30000 Training Loss: 0.06662534922361374\n",
      "Epoch 27376/30000 Training Loss: 0.06809508055448532\n",
      "Epoch 27377/30000 Training Loss: 0.07938337326049805\n",
      "Epoch 27378/30000 Training Loss: 0.07548045367002487\n",
      "Epoch 27379/30000 Training Loss: 0.09481164067983627\n",
      "Epoch 27380/30000 Training Loss: 0.0882560983300209\n",
      "Epoch 27380/30000 Validation Loss: 0.07820894569158554\n",
      "Epoch 27381/30000 Training Loss: 0.05360434576869011\n",
      "Epoch 27382/30000 Training Loss: 0.07262024283409119\n",
      "Epoch 27383/30000 Training Loss: 0.08593674749135971\n",
      "Epoch 27384/30000 Training Loss: 0.05636638030409813\n",
      "Epoch 27385/30000 Training Loss: 0.07490753382444382\n",
      "Epoch 27386/30000 Training Loss: 0.06920195370912552\n",
      "Epoch 27387/30000 Training Loss: 0.07592645287513733\n",
      "Epoch 27388/30000 Training Loss: 0.05298851802945137\n",
      "Epoch 27389/30000 Training Loss: 0.09030169248580933\n",
      "Epoch 27390/30000 Training Loss: 0.0663316622376442\n",
      "Epoch 27390/30000 Validation Loss: 0.06706754863262177\n",
      "Epoch 27391/30000 Training Loss: 0.08169612288475037\n",
      "Epoch 27392/30000 Training Loss: 0.07188505679368973\n",
      "Epoch 27393/30000 Training Loss: 0.06685226410627365\n",
      "Epoch 27394/30000 Training Loss: 0.058300551027059555\n",
      "Epoch 27395/30000 Training Loss: 0.06596241146326065\n",
      "Epoch 27396/30000 Training Loss: 0.06624672561883926\n",
      "Epoch 27397/30000 Training Loss: 0.06572596728801727\n",
      "Epoch 27398/30000 Training Loss: 0.08857318013906479\n",
      "Epoch 27399/30000 Training Loss: 0.06464479118585587\n",
      "Epoch 27400/30000 Training Loss: 0.05726766586303711\n",
      "Epoch 27400/30000 Validation Loss: 0.06339040398597717\n",
      "Epoch 27401/30000 Training Loss: 0.07308325916528702\n",
      "Epoch 27402/30000 Training Loss: 0.0740649625658989\n",
      "Epoch 27403/30000 Training Loss: 0.06635920703411102\n",
      "Epoch 27404/30000 Training Loss: 0.07319973409175873\n",
      "Epoch 27405/30000 Training Loss: 0.08604315668344498\n",
      "Epoch 27406/30000 Training Loss: 0.060225486755371094\n",
      "Epoch 27407/30000 Training Loss: 0.06370803713798523\n",
      "Epoch 27408/30000 Training Loss: 0.06435636430978775\n",
      "Epoch 27409/30000 Training Loss: 0.048152197152376175\n",
      "Epoch 27410/30000 Training Loss: 0.05487392842769623\n",
      "Epoch 27410/30000 Validation Loss: 0.07316530495882034\n",
      "Epoch 27411/30000 Training Loss: 0.07159792631864548\n",
      "Epoch 27412/30000 Training Loss: 0.08950696140527725\n",
      "Epoch 27413/30000 Training Loss: 0.062281787395477295\n",
      "Epoch 27414/30000 Training Loss: 0.0782119557261467\n",
      "Epoch 27415/30000 Training Loss: 0.06610780209302902\n",
      "Epoch 27416/30000 Training Loss: 0.05700283125042915\n",
      "Epoch 27417/30000 Training Loss: 0.08499311655759811\n",
      "Epoch 27418/30000 Training Loss: 0.05569504573941231\n",
      "Epoch 27419/30000 Training Loss: 0.05779567360877991\n",
      "Epoch 27420/30000 Training Loss: 0.08938365429639816\n",
      "Epoch 27420/30000 Validation Loss: 0.08106211572885513\n",
      "Epoch 27421/30000 Training Loss: 0.06481144577264786\n",
      "Epoch 27422/30000 Training Loss: 0.062059059739112854\n",
      "Epoch 27423/30000 Training Loss: 0.0784640684723854\n",
      "Epoch 27424/30000 Training Loss: 0.07608910650014877\n",
      "Epoch 27425/30000 Training Loss: 0.06269997358322144\n",
      "Epoch 27426/30000 Training Loss: 0.05432786047458649\n",
      "Epoch 27427/30000 Training Loss: 0.06885511428117752\n",
      "Epoch 27428/30000 Training Loss: 0.059990424662828445\n",
      "Epoch 27429/30000 Training Loss: 0.0780949518084526\n",
      "Epoch 27430/30000 Training Loss: 0.08314921706914902\n",
      "Epoch 27430/30000 Validation Loss: 0.0648685097694397\n",
      "Epoch 27431/30000 Training Loss: 0.05345660820603371\n",
      "Epoch 27432/30000 Training Loss: 0.05985620990395546\n",
      "Epoch 27433/30000 Training Loss: 0.061051588505506516\n",
      "Epoch 27434/30000 Training Loss: 0.06066218391060829\n",
      "Epoch 27435/30000 Training Loss: 0.0760500431060791\n",
      "Epoch 27436/30000 Training Loss: 0.04877700284123421\n",
      "Epoch 27437/30000 Training Loss: 0.07407525926828384\n",
      "Epoch 27438/30000 Training Loss: 0.06553509086370468\n",
      "Epoch 27439/30000 Training Loss: 0.06379098445177078\n",
      "Epoch 27440/30000 Training Loss: 0.06166769564151764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27440/30000 Validation Loss: 0.07649066299200058\n",
      "Epoch 27441/30000 Training Loss: 0.07216405123472214\n",
      "Epoch 27442/30000 Training Loss: 0.08450528234243393\n",
      "Epoch 27443/30000 Training Loss: 0.06507550925016403\n",
      "Epoch 27444/30000 Training Loss: 0.09005739539861679\n",
      "Epoch 27445/30000 Training Loss: 0.07083430886268616\n",
      "Epoch 27446/30000 Training Loss: 0.06871387362480164\n",
      "Epoch 27447/30000 Training Loss: 0.057758528739213943\n",
      "Epoch 27448/30000 Training Loss: 0.0998513475060463\n",
      "Epoch 27449/30000 Training Loss: 0.06390361487865448\n",
      "Epoch 27450/30000 Training Loss: 0.06687609106302261\n",
      "Epoch 27450/30000 Validation Loss: 0.07191718369722366\n",
      "Epoch 27451/30000 Training Loss: 0.06840114295482635\n",
      "Epoch 27452/30000 Training Loss: 0.0693778321146965\n",
      "Epoch 27453/30000 Training Loss: 0.07243452221155167\n",
      "Epoch 27454/30000 Training Loss: 0.06342156231403351\n",
      "Epoch 27455/30000 Training Loss: 0.07251891493797302\n",
      "Epoch 27456/30000 Training Loss: 0.08203313499689102\n",
      "Epoch 27457/30000 Training Loss: 0.05672537162899971\n",
      "Epoch 27458/30000 Training Loss: 0.06979747861623764\n",
      "Epoch 27459/30000 Training Loss: 0.06384488940238953\n",
      "Epoch 27460/30000 Training Loss: 0.08109673112630844\n",
      "Epoch 27460/30000 Validation Loss: 0.06196916103363037\n",
      "Epoch 27461/30000 Training Loss: 0.07164641469717026\n",
      "Epoch 27462/30000 Training Loss: 0.06424818933010101\n",
      "Epoch 27463/30000 Training Loss: 0.0887225791811943\n",
      "Epoch 27464/30000 Training Loss: 0.05484338477253914\n",
      "Epoch 27465/30000 Training Loss: 0.07048710435628891\n",
      "Epoch 27466/30000 Training Loss: 0.06441276520490646\n",
      "Epoch 27467/30000 Training Loss: 0.06933527439832687\n",
      "Epoch 27468/30000 Training Loss: 0.07450009137392044\n",
      "Epoch 27469/30000 Training Loss: 0.06262848526239395\n",
      "Epoch 27470/30000 Training Loss: 0.08056037873029709\n",
      "Epoch 27470/30000 Validation Loss: 0.07766302675008774\n",
      "Epoch 27471/30000 Training Loss: 0.09618530422449112\n",
      "Epoch 27472/30000 Training Loss: 0.07204706221818924\n",
      "Epoch 27473/30000 Training Loss: 0.0700833722949028\n",
      "Epoch 27474/30000 Training Loss: 0.07981152087450027\n",
      "Epoch 27475/30000 Training Loss: 0.08977784961462021\n",
      "Epoch 27476/30000 Training Loss: 0.07522710412740707\n",
      "Epoch 27477/30000 Training Loss: 0.056504588574171066\n",
      "Epoch 27478/30000 Training Loss: 0.07229255139827728\n",
      "Epoch 27479/30000 Training Loss: 0.08664098381996155\n",
      "Epoch 27480/30000 Training Loss: 0.06187383458018303\n",
      "Epoch 27480/30000 Validation Loss: 0.0729285478591919\n",
      "Epoch 27481/30000 Training Loss: 0.07769298553466797\n",
      "Epoch 27482/30000 Training Loss: 0.08112169057130814\n",
      "Epoch 27483/30000 Training Loss: 0.08481570333242416\n",
      "Epoch 27484/30000 Training Loss: 0.058408867567777634\n",
      "Epoch 27485/30000 Training Loss: 0.058428239077329636\n",
      "Epoch 27486/30000 Training Loss: 0.08543194085359573\n",
      "Epoch 27487/30000 Training Loss: 0.09593811631202698\n",
      "Epoch 27488/30000 Training Loss: 0.06378952413797379\n",
      "Epoch 27489/30000 Training Loss: 0.05396561324596405\n",
      "Epoch 27490/30000 Training Loss: 0.08095861971378326\n",
      "Epoch 27490/30000 Validation Loss: 0.09087201952934265\n",
      "Epoch 27491/30000 Training Loss: 0.08383500576019287\n",
      "Epoch 27492/30000 Training Loss: 0.0741722360253334\n",
      "Epoch 27493/30000 Training Loss: 0.06608884781599045\n",
      "Epoch 27494/30000 Training Loss: 0.06334798038005829\n",
      "Epoch 27495/30000 Training Loss: 0.06470005959272385\n",
      "Epoch 27496/30000 Training Loss: 0.07330133765935898\n",
      "Epoch 27497/30000 Training Loss: 0.06853808462619781\n",
      "Epoch 27498/30000 Training Loss: 0.07766437530517578\n",
      "Epoch 27499/30000 Training Loss: 0.05886800214648247\n",
      "Epoch 27500/30000 Training Loss: 0.07491137832403183\n",
      "Epoch 27500/30000 Validation Loss: 0.07537544518709183\n",
      "Epoch 27501/30000 Training Loss: 0.06457307189702988\n",
      "Epoch 27502/30000 Training Loss: 0.06323672086000443\n",
      "Epoch 27503/30000 Training Loss: 0.05672150477766991\n",
      "Epoch 27504/30000 Training Loss: 0.05282765254378319\n",
      "Epoch 27505/30000 Training Loss: 0.0580914206802845\n",
      "Epoch 27506/30000 Training Loss: 0.09125470370054245\n",
      "Epoch 27507/30000 Training Loss: 0.06964809447526932\n",
      "Epoch 27508/30000 Training Loss: 0.0717368796467781\n",
      "Epoch 27509/30000 Training Loss: 0.05685944855213165\n",
      "Epoch 27510/30000 Training Loss: 0.06010723114013672\n",
      "Epoch 27510/30000 Validation Loss: 0.07285255938768387\n",
      "Epoch 27511/30000 Training Loss: 0.06488186866044998\n",
      "Epoch 27512/30000 Training Loss: 0.0729082003235817\n",
      "Epoch 27513/30000 Training Loss: 0.07580193877220154\n",
      "Epoch 27514/30000 Training Loss: 0.07078644633293152\n",
      "Epoch 27515/30000 Training Loss: 0.08277540653944016\n",
      "Epoch 27516/30000 Training Loss: 0.06742527335882187\n",
      "Epoch 27517/30000 Training Loss: 0.06913947314023972\n",
      "Epoch 27518/30000 Training Loss: 0.07908844202756882\n",
      "Epoch 27519/30000 Training Loss: 0.059378936886787415\n",
      "Epoch 27520/30000 Training Loss: 0.05618545413017273\n",
      "Epoch 27520/30000 Validation Loss: 0.06809008866548538\n",
      "Epoch 27521/30000 Training Loss: 0.06991147249937057\n",
      "Epoch 27522/30000 Training Loss: 0.07656504958868027\n",
      "Epoch 27523/30000 Training Loss: 0.06467775255441666\n",
      "Epoch 27524/30000 Training Loss: 0.07901645451784134\n",
      "Epoch 27525/30000 Training Loss: 0.05880582332611084\n",
      "Epoch 27526/30000 Training Loss: 0.07830420881509781\n",
      "Epoch 27527/30000 Training Loss: 0.06940626353025436\n",
      "Epoch 27528/30000 Training Loss: 0.06308024376630783\n",
      "Epoch 27529/30000 Training Loss: 0.06163179874420166\n",
      "Epoch 27530/30000 Training Loss: 0.05688627436757088\n",
      "Epoch 27530/30000 Validation Loss: 0.06903498619794846\n",
      "Epoch 27531/30000 Training Loss: 0.06808415800333023\n",
      "Epoch 27532/30000 Training Loss: 0.06092662736773491\n",
      "Epoch 27533/30000 Training Loss: 0.06683546304702759\n",
      "Epoch 27534/30000 Training Loss: 0.06834567338228226\n",
      "Epoch 27535/30000 Training Loss: 0.058271002024412155\n",
      "Epoch 27536/30000 Training Loss: 0.0762992799282074\n",
      "Epoch 27537/30000 Training Loss: 0.06466060876846313\n",
      "Epoch 27538/30000 Training Loss: 0.06165728345513344\n",
      "Epoch 27539/30000 Training Loss: 0.08410432934761047\n",
      "Epoch 27540/30000 Training Loss: 0.06971260160207748\n",
      "Epoch 27540/30000 Validation Loss: 0.061694491654634476\n",
      "Epoch 27541/30000 Training Loss: 0.05603400990366936\n",
      "Epoch 27542/30000 Training Loss: 0.064034603536129\n",
      "Epoch 27543/30000 Training Loss: 0.07374701648950577\n",
      "Epoch 27544/30000 Training Loss: 0.07973601669073105\n",
      "Epoch 27545/30000 Training Loss: 0.0858813226222992\n",
      "Epoch 27546/30000 Training Loss: 0.07605055719614029\n",
      "Epoch 27547/30000 Training Loss: 0.09253737330436707\n",
      "Epoch 27548/30000 Training Loss: 0.0556899718940258\n",
      "Epoch 27549/30000 Training Loss: 0.07504870742559433\n",
      "Epoch 27550/30000 Training Loss: 0.08596885204315186\n",
      "Epoch 27550/30000 Validation Loss: 0.0688878521323204\n",
      "Epoch 27551/30000 Training Loss: 0.07782841473817825\n",
      "Epoch 27552/30000 Training Loss: 0.07278997451066971\n",
      "Epoch 27553/30000 Training Loss: 0.06648223847150803\n",
      "Epoch 27554/30000 Training Loss: 0.07749366760253906\n",
      "Epoch 27555/30000 Training Loss: 0.061446547508239746\n",
      "Epoch 27556/30000 Training Loss: 0.06771385669708252\n",
      "Epoch 27557/30000 Training Loss: 0.07308817654848099\n",
      "Epoch 27558/30000 Training Loss: 0.06605654209852219\n",
      "Epoch 27559/30000 Training Loss: 0.06257451325654984\n",
      "Epoch 27560/30000 Training Loss: 0.055861275643110275\n",
      "Epoch 27560/30000 Validation Loss: 0.09174976497888565\n",
      "Epoch 27561/30000 Training Loss: 0.0768967941403389\n",
      "Epoch 27562/30000 Training Loss: 0.05263720825314522\n",
      "Epoch 27563/30000 Training Loss: 0.06046903505921364\n",
      "Epoch 27564/30000 Training Loss: 0.07465704530477524\n",
      "Epoch 27565/30000 Training Loss: 0.07407594472169876\n",
      "Epoch 27566/30000 Training Loss: 0.05607360973954201\n",
      "Epoch 27567/30000 Training Loss: 0.07909560948610306\n",
      "Epoch 27568/30000 Training Loss: 0.06964447349309921\n",
      "Epoch 27569/30000 Training Loss: 0.07924585789442062\n",
      "Epoch 27570/30000 Training Loss: 0.0647483840584755\n",
      "Epoch 27570/30000 Validation Loss: 0.06689704954624176\n",
      "Epoch 27571/30000 Training Loss: 0.05617417395114899\n",
      "Epoch 27572/30000 Training Loss: 0.07210568338632584\n",
      "Epoch 27573/30000 Training Loss: 0.05997760221362114\n",
      "Epoch 27574/30000 Training Loss: 0.06794223934412003\n",
      "Epoch 27575/30000 Training Loss: 0.07540922611951828\n",
      "Epoch 27576/30000 Training Loss: 0.07416728138923645\n",
      "Epoch 27577/30000 Training Loss: 0.06867057085037231\n",
      "Epoch 27578/30000 Training Loss: 0.07346540689468384\n",
      "Epoch 27579/30000 Training Loss: 0.07077313959598541\n",
      "Epoch 27580/30000 Training Loss: 0.0741037055850029\n",
      "Epoch 27580/30000 Validation Loss: 0.06256148219108582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27581/30000 Training Loss: 0.06488428264856339\n",
      "Epoch 27582/30000 Training Loss: 0.058023255318403244\n",
      "Epoch 27583/30000 Training Loss: 0.08410528302192688\n",
      "Epoch 27584/30000 Training Loss: 0.06952334940433502\n",
      "Epoch 27585/30000 Training Loss: 0.0718379095196724\n",
      "Epoch 27586/30000 Training Loss: 0.06911418586969376\n",
      "Epoch 27587/30000 Training Loss: 0.07076127082109451\n",
      "Epoch 27588/30000 Training Loss: 0.06735178828239441\n",
      "Epoch 27589/30000 Training Loss: 0.07649893313646317\n",
      "Epoch 27590/30000 Training Loss: 0.07455271482467651\n",
      "Epoch 27590/30000 Validation Loss: 0.06799168139696121\n",
      "Epoch 27591/30000 Training Loss: 0.07166589051485062\n",
      "Epoch 27592/30000 Training Loss: 0.05359892174601555\n",
      "Epoch 27593/30000 Training Loss: 0.08879917860031128\n",
      "Epoch 27594/30000 Training Loss: 0.06520313769578934\n",
      "Epoch 27595/30000 Training Loss: 0.060651738196611404\n",
      "Epoch 27596/30000 Training Loss: 0.06798277795314789\n",
      "Epoch 27597/30000 Training Loss: 0.07440191507339478\n",
      "Epoch 27598/30000 Training Loss: 0.06715808063745499\n",
      "Epoch 27599/30000 Training Loss: 0.06076503172516823\n",
      "Epoch 27600/30000 Training Loss: 0.08318684250116348\n",
      "Epoch 27600/30000 Validation Loss: 0.07865584641695023\n",
      "Epoch 27601/30000 Training Loss: 0.07081260532140732\n",
      "Epoch 27602/30000 Training Loss: 0.06648442149162292\n",
      "Epoch 27603/30000 Training Loss: 0.07008820027112961\n",
      "Epoch 27604/30000 Training Loss: 0.06538095325231552\n",
      "Epoch 27605/30000 Training Loss: 0.06183714047074318\n",
      "Epoch 27606/30000 Training Loss: 0.06593824923038483\n",
      "Epoch 27607/30000 Training Loss: 0.05739089846611023\n",
      "Epoch 27608/30000 Training Loss: 0.050298284739255905\n",
      "Epoch 27609/30000 Training Loss: 0.06536691635847092\n",
      "Epoch 27610/30000 Training Loss: 0.06959909945726395\n",
      "Epoch 27610/30000 Validation Loss: 0.07303658872842789\n",
      "Epoch 27611/30000 Training Loss: 0.05056220293045044\n",
      "Epoch 27612/30000 Training Loss: 0.0655350461602211\n",
      "Epoch 27613/30000 Training Loss: 0.05646410956978798\n",
      "Epoch 27614/30000 Training Loss: 0.07385345548391342\n",
      "Epoch 27615/30000 Training Loss: 0.08041060715913773\n",
      "Epoch 27616/30000 Training Loss: 0.051574092358350754\n",
      "Epoch 27617/30000 Training Loss: 0.05930786207318306\n",
      "Epoch 27618/30000 Training Loss: 0.07609566301107407\n",
      "Epoch 27619/30000 Training Loss: 0.07287167757749557\n",
      "Epoch 27620/30000 Training Loss: 0.0690942034125328\n",
      "Epoch 27620/30000 Validation Loss: 0.08871272951364517\n",
      "Epoch 27621/30000 Training Loss: 0.05388147756457329\n",
      "Epoch 27622/30000 Training Loss: 0.05886278674006462\n",
      "Epoch 27623/30000 Training Loss: 0.07256675511598587\n",
      "Epoch 27624/30000 Training Loss: 0.08509113639593124\n",
      "Epoch 27625/30000 Training Loss: 0.07296855002641678\n",
      "Epoch 27626/30000 Training Loss: 0.06825242191553116\n",
      "Epoch 27627/30000 Training Loss: 0.08347120881080627\n",
      "Epoch 27628/30000 Training Loss: 0.05178184434771538\n",
      "Epoch 27629/30000 Training Loss: 0.05597573518753052\n",
      "Epoch 27630/30000 Training Loss: 0.08143389970064163\n",
      "Epoch 27630/30000 Validation Loss: 0.09481602907180786\n",
      "Epoch 27631/30000 Training Loss: 0.06232614442706108\n",
      "Epoch 27632/30000 Training Loss: 0.058226004242897034\n",
      "Epoch 27633/30000 Training Loss: 0.06283906102180481\n",
      "Epoch 27634/30000 Training Loss: 0.053448308259248734\n",
      "Epoch 27635/30000 Training Loss: 0.10606186836957932\n",
      "Epoch 27636/30000 Training Loss: 0.0721995159983635\n",
      "Epoch 27637/30000 Training Loss: 0.0668690875172615\n",
      "Epoch 27638/30000 Training Loss: 0.06805282831192017\n",
      "Epoch 27639/30000 Training Loss: 0.05649581551551819\n",
      "Epoch 27640/30000 Training Loss: 0.08271623402833939\n",
      "Epoch 27640/30000 Validation Loss: 0.07105604559183121\n",
      "Epoch 27641/30000 Training Loss: 0.08712369948625565\n",
      "Epoch 27642/30000 Training Loss: 0.0798429474234581\n",
      "Epoch 27643/30000 Training Loss: 0.06813264638185501\n",
      "Epoch 27644/30000 Training Loss: 0.08381766825914383\n",
      "Epoch 27645/30000 Training Loss: 0.048178721219301224\n",
      "Epoch 27646/30000 Training Loss: 0.06282829493284225\n",
      "Epoch 27647/30000 Training Loss: 0.06425056606531143\n",
      "Epoch 27648/30000 Training Loss: 0.053252775222063065\n",
      "Epoch 27649/30000 Training Loss: 0.053919468075037\n",
      "Epoch 27650/30000 Training Loss: 0.054828450083732605\n",
      "Epoch 27650/30000 Validation Loss: 0.06568661332130432\n",
      "Epoch 27651/30000 Training Loss: 0.08583300560712814\n",
      "Epoch 27652/30000 Training Loss: 0.0569385290145874\n",
      "Epoch 27653/30000 Training Loss: 0.08308228850364685\n",
      "Epoch 27654/30000 Training Loss: 0.0600828118622303\n",
      "Epoch 27655/30000 Training Loss: 0.06111548840999603\n",
      "Epoch 27656/30000 Training Loss: 0.08386433869600296\n",
      "Epoch 27657/30000 Training Loss: 0.07157539576292038\n",
      "Epoch 27658/30000 Training Loss: 0.05440351739525795\n",
      "Epoch 27659/30000 Training Loss: 0.0828593373298645\n",
      "Epoch 27660/30000 Training Loss: 0.058143626898527145\n",
      "Epoch 27660/30000 Validation Loss: 0.0772901102900505\n",
      "Epoch 27661/30000 Training Loss: 0.07696715742349625\n",
      "Epoch 27662/30000 Training Loss: 0.06728244572877884\n",
      "Epoch 27663/30000 Training Loss: 0.061586033552885056\n",
      "Epoch 27664/30000 Training Loss: 0.06633996218442917\n",
      "Epoch 27665/30000 Training Loss: 0.05844351649284363\n",
      "Epoch 27666/30000 Training Loss: 0.07883381843566895\n",
      "Epoch 27667/30000 Training Loss: 0.067844919860363\n",
      "Epoch 27668/30000 Training Loss: 0.06914155930280685\n",
      "Epoch 27669/30000 Training Loss: 0.06346944719552994\n",
      "Epoch 27670/30000 Training Loss: 0.06918732076883316\n",
      "Epoch 27670/30000 Validation Loss: 0.051099080592393875\n",
      "Epoch 27671/30000 Training Loss: 0.056928589940071106\n",
      "Epoch 27672/30000 Training Loss: 0.07318688184022903\n",
      "Epoch 27673/30000 Training Loss: 0.07445631176233292\n",
      "Epoch 27674/30000 Training Loss: 0.0645684227347374\n",
      "Epoch 27675/30000 Training Loss: 0.06400329619646072\n",
      "Epoch 27676/30000 Training Loss: 0.06326454132795334\n",
      "Epoch 27677/30000 Training Loss: 0.06737468391656876\n",
      "Epoch 27678/30000 Training Loss: 0.0703422948718071\n",
      "Epoch 27679/30000 Training Loss: 0.07323187589645386\n",
      "Epoch 27680/30000 Training Loss: 0.05887216702103615\n",
      "Epoch 27680/30000 Validation Loss: 0.07517337054014206\n",
      "Epoch 27681/30000 Training Loss: 0.0495331734418869\n",
      "Epoch 27682/30000 Training Loss: 0.07074624300003052\n",
      "Epoch 27683/30000 Training Loss: 0.07525093108415604\n",
      "Epoch 27684/30000 Training Loss: 0.08550649881362915\n",
      "Epoch 27685/30000 Training Loss: 0.05742469057440758\n",
      "Epoch 27686/30000 Training Loss: 0.05873796343803406\n",
      "Epoch 27687/30000 Training Loss: 0.06025298312306404\n",
      "Epoch 27688/30000 Training Loss: 0.057326626032590866\n",
      "Epoch 27689/30000 Training Loss: 0.08425462990999222\n",
      "Epoch 27690/30000 Training Loss: 0.06152484938502312\n",
      "Epoch 27690/30000 Validation Loss: 0.07873161882162094\n",
      "Epoch 27691/30000 Training Loss: 0.06686411052942276\n",
      "Epoch 27692/30000 Training Loss: 0.07225385308265686\n",
      "Epoch 27693/30000 Training Loss: 0.06402130424976349\n",
      "Epoch 27694/30000 Training Loss: 0.0615551732480526\n",
      "Epoch 27695/30000 Training Loss: 0.06799992173910141\n",
      "Epoch 27696/30000 Training Loss: 0.07586243003606796\n",
      "Epoch 27697/30000 Training Loss: 0.06345794349908829\n",
      "Epoch 27698/30000 Training Loss: 0.04858756065368652\n",
      "Epoch 27699/30000 Training Loss: 0.06332463026046753\n",
      "Epoch 27700/30000 Training Loss: 0.09421948343515396\n",
      "Epoch 27700/30000 Validation Loss: 0.07518329471349716\n",
      "Epoch 27701/30000 Training Loss: 0.05607113242149353\n",
      "Epoch 27702/30000 Training Loss: 0.07523016631603241\n",
      "Epoch 27703/30000 Training Loss: 0.06399758905172348\n",
      "Epoch 27704/30000 Training Loss: 0.06832209974527359\n",
      "Epoch 27705/30000 Training Loss: 0.06354418396949768\n",
      "Epoch 27706/30000 Training Loss: 0.05418996140360832\n",
      "Epoch 27707/30000 Training Loss: 0.07295756787061691\n",
      "Epoch 27708/30000 Training Loss: 0.08841588348150253\n",
      "Epoch 27709/30000 Training Loss: 0.05257086828351021\n",
      "Epoch 27710/30000 Training Loss: 0.06881407648324966\n",
      "Epoch 27710/30000 Validation Loss: 0.05774552747607231\n",
      "Epoch 27711/30000 Training Loss: 0.05442497134208679\n",
      "Epoch 27712/30000 Training Loss: 0.05961930751800537\n",
      "Epoch 27713/30000 Training Loss: 0.07652147859334946\n",
      "Epoch 27714/30000 Training Loss: 0.06925598531961441\n",
      "Epoch 27715/30000 Training Loss: 0.05106807127594948\n",
      "Epoch 27716/30000 Training Loss: 0.057538677006959915\n",
      "Epoch 27717/30000 Training Loss: 0.0613267682492733\n",
      "Epoch 27718/30000 Training Loss: 0.06842010468244553\n",
      "Epoch 27719/30000 Training Loss: 0.07216226309537888\n",
      "Epoch 27720/30000 Training Loss: 0.07450031489133835\n",
      "Epoch 27720/30000 Validation Loss: 0.06552641838788986\n",
      "Epoch 27721/30000 Training Loss: 0.06012947857379913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27722/30000 Training Loss: 0.05548720061779022\n",
      "Epoch 27723/30000 Training Loss: 0.06875903159379959\n",
      "Epoch 27724/30000 Training Loss: 0.06810630857944489\n",
      "Epoch 27725/30000 Training Loss: 0.06361066550016403\n",
      "Epoch 27726/30000 Training Loss: 0.08414403349161148\n",
      "Epoch 27727/30000 Training Loss: 0.07106518745422363\n",
      "Epoch 27728/30000 Training Loss: 0.0724649429321289\n",
      "Epoch 27729/30000 Training Loss: 0.08145731687545776\n",
      "Epoch 27730/30000 Training Loss: 0.06788229942321777\n",
      "Epoch 27730/30000 Validation Loss: 0.07422813773155212\n",
      "Epoch 27731/30000 Training Loss: 0.06516680866479874\n",
      "Epoch 27732/30000 Training Loss: 0.062234848737716675\n",
      "Epoch 27733/30000 Training Loss: 0.07244975119829178\n",
      "Epoch 27734/30000 Training Loss: 0.07394897192716599\n",
      "Epoch 27735/30000 Training Loss: 0.10460706800222397\n",
      "Epoch 27736/30000 Training Loss: 0.05980078503489494\n",
      "Epoch 27737/30000 Training Loss: 0.07741593569517136\n",
      "Epoch 27738/30000 Training Loss: 0.06027163565158844\n",
      "Epoch 27739/30000 Training Loss: 0.08234774321317673\n",
      "Epoch 27740/30000 Training Loss: 0.09657546132802963\n",
      "Epoch 27740/30000 Validation Loss: 0.07781660556793213\n",
      "Epoch 27741/30000 Training Loss: 0.07465629279613495\n",
      "Epoch 27742/30000 Training Loss: 0.06030966714024544\n",
      "Epoch 27743/30000 Training Loss: 0.06066475436091423\n",
      "Epoch 27744/30000 Training Loss: 0.06209560111165047\n",
      "Epoch 27745/30000 Training Loss: 0.08289188146591187\n",
      "Epoch 27746/30000 Training Loss: 0.07679154723882675\n",
      "Epoch 27747/30000 Training Loss: 0.08063269406557083\n",
      "Epoch 27748/30000 Training Loss: 0.06714656949043274\n",
      "Epoch 27749/30000 Training Loss: 0.069814033806324\n",
      "Epoch 27750/30000 Training Loss: 0.058481063693761826\n",
      "Epoch 27750/30000 Validation Loss: 0.06277339160442352\n",
      "Epoch 27751/30000 Training Loss: 0.06462839990854263\n",
      "Epoch 27752/30000 Training Loss: 0.0683530941605568\n",
      "Epoch 27753/30000 Training Loss: 0.07038774341344833\n",
      "Epoch 27754/30000 Training Loss: 0.07332763075828552\n",
      "Epoch 27755/30000 Training Loss: 0.08672384172677994\n",
      "Epoch 27756/30000 Training Loss: 0.07563628256320953\n",
      "Epoch 27757/30000 Training Loss: 0.06684479862451553\n",
      "Epoch 27758/30000 Training Loss: 0.06768029928207397\n",
      "Epoch 27759/30000 Training Loss: 0.06301403790712357\n",
      "Epoch 27760/30000 Training Loss: 0.05781049653887749\n",
      "Epoch 27760/30000 Validation Loss: 0.059470754116773605\n",
      "Epoch 27761/30000 Training Loss: 0.07490278035402298\n",
      "Epoch 27762/30000 Training Loss: 0.056051816791296005\n",
      "Epoch 27763/30000 Training Loss: 0.0571846179664135\n",
      "Epoch 27764/30000 Training Loss: 0.08118041604757309\n",
      "Epoch 27765/30000 Training Loss: 0.06417614221572876\n",
      "Epoch 27766/30000 Training Loss: 0.05334514006972313\n",
      "Epoch 27767/30000 Training Loss: 0.05971108749508858\n",
      "Epoch 27768/30000 Training Loss: 0.08021777868270874\n",
      "Epoch 27769/30000 Training Loss: 0.04976131021976471\n",
      "Epoch 27770/30000 Training Loss: 0.06933438777923584\n",
      "Epoch 27770/30000 Validation Loss: 0.06255590170621872\n",
      "Epoch 27771/30000 Training Loss: 0.06468422710895538\n",
      "Epoch 27772/30000 Training Loss: 0.06451687216758728\n",
      "Epoch 27773/30000 Training Loss: 0.07582014799118042\n",
      "Epoch 27774/30000 Training Loss: 0.0772932693362236\n",
      "Epoch 27775/30000 Training Loss: 0.06403301656246185\n",
      "Epoch 27776/30000 Training Loss: 0.07409090548753738\n",
      "Epoch 27777/30000 Training Loss: 0.07266304641962051\n",
      "Epoch 27778/30000 Training Loss: 0.06639113277196884\n",
      "Epoch 27779/30000 Training Loss: 0.06314736604690552\n",
      "Epoch 27780/30000 Training Loss: 0.06643647700548172\n",
      "Epoch 27780/30000 Validation Loss: 0.07947011291980743\n",
      "Epoch 27781/30000 Training Loss: 0.07452710717916489\n",
      "Epoch 27782/30000 Training Loss: 0.09269807487726212\n",
      "Epoch 27783/30000 Training Loss: 0.060572732239961624\n",
      "Epoch 27784/30000 Training Loss: 0.0630410835146904\n",
      "Epoch 27785/30000 Training Loss: 0.060593198984861374\n",
      "Epoch 27786/30000 Training Loss: 0.09044036269187927\n",
      "Epoch 27787/30000 Training Loss: 0.07716698199510574\n",
      "Epoch 27788/30000 Training Loss: 0.06155182793736458\n",
      "Epoch 27789/30000 Training Loss: 0.05464593693614006\n",
      "Epoch 27790/30000 Training Loss: 0.0778195932507515\n",
      "Epoch 27790/30000 Validation Loss: 0.059699948877096176\n",
      "Epoch 27791/30000 Training Loss: 0.06572043895721436\n",
      "Epoch 27792/30000 Training Loss: 0.06439440697431564\n",
      "Epoch 27793/30000 Training Loss: 0.07476809620857239\n",
      "Epoch 27794/30000 Training Loss: 0.06410063058137894\n",
      "Epoch 27795/30000 Training Loss: 0.06105620786547661\n",
      "Epoch 27796/30000 Training Loss: 0.0609094612300396\n",
      "Epoch 27797/30000 Training Loss: 0.08131568878889084\n",
      "Epoch 27798/30000 Training Loss: 0.06098318099975586\n",
      "Epoch 27799/30000 Training Loss: 0.06691747903823853\n",
      "Epoch 27800/30000 Training Loss: 0.06787652522325516\n",
      "Epoch 27800/30000 Validation Loss: 0.0723356083035469\n",
      "Epoch 27801/30000 Training Loss: 0.07758831977844238\n",
      "Epoch 27802/30000 Training Loss: 0.07262397557497025\n",
      "Epoch 27803/30000 Training Loss: 0.06480173021554947\n",
      "Epoch 27804/30000 Training Loss: 0.07215999811887741\n",
      "Epoch 27805/30000 Training Loss: 0.05673002079129219\n",
      "Epoch 27806/30000 Training Loss: 0.08354312926530838\n",
      "Epoch 27807/30000 Training Loss: 0.06093373894691467\n",
      "Epoch 27808/30000 Training Loss: 0.07549456506967545\n",
      "Epoch 27809/30000 Training Loss: 0.0647745355963707\n",
      "Epoch 27810/30000 Training Loss: 0.06428883969783783\n",
      "Epoch 27810/30000 Validation Loss: 0.07110363990068436\n",
      "Epoch 27811/30000 Training Loss: 0.05690687894821167\n",
      "Epoch 27812/30000 Training Loss: 0.06526153534650803\n",
      "Epoch 27813/30000 Training Loss: 0.07325152307748795\n",
      "Epoch 27814/30000 Training Loss: 0.08048731088638306\n",
      "Epoch 27815/30000 Training Loss: 0.0558578260242939\n",
      "Epoch 27816/30000 Training Loss: 0.08318960666656494\n",
      "Epoch 27817/30000 Training Loss: 0.08270182460546494\n",
      "Epoch 27818/30000 Training Loss: 0.06994835287332535\n",
      "Epoch 27819/30000 Training Loss: 0.056263700127601624\n",
      "Epoch 27820/30000 Training Loss: 0.07723899930715561\n",
      "Epoch 27820/30000 Validation Loss: 0.06593039631843567\n",
      "Epoch 27821/30000 Training Loss: 0.0711107924580574\n",
      "Epoch 27822/30000 Training Loss: 0.07090754061937332\n",
      "Epoch 27823/30000 Training Loss: 0.0777478739619255\n",
      "Epoch 27824/30000 Training Loss: 0.07640308141708374\n",
      "Epoch 27825/30000 Training Loss: 0.07188639044761658\n",
      "Epoch 27826/30000 Training Loss: 0.07246627658605576\n",
      "Epoch 27827/30000 Training Loss: 0.05794763192534447\n",
      "Epoch 27828/30000 Training Loss: 0.06527363508939743\n",
      "Epoch 27829/30000 Training Loss: 0.06407468765974045\n",
      "Epoch 27830/30000 Training Loss: 0.0665535107254982\n",
      "Epoch 27830/30000 Validation Loss: 0.06558427959680557\n",
      "Epoch 27831/30000 Training Loss: 0.06567056477069855\n",
      "Epoch 27832/30000 Training Loss: 0.06364757567644119\n",
      "Epoch 27833/30000 Training Loss: 0.0718098059296608\n",
      "Epoch 27834/30000 Training Loss: 0.06858619302511215\n",
      "Epoch 27835/30000 Training Loss: 0.06325235217809677\n",
      "Epoch 27836/30000 Training Loss: 0.06373026221990585\n",
      "Epoch 27837/30000 Training Loss: 0.0541866272687912\n",
      "Epoch 27838/30000 Training Loss: 0.08254770189523697\n",
      "Epoch 27839/30000 Training Loss: 0.07562718540430069\n",
      "Epoch 27840/30000 Training Loss: 0.059757500886917114\n",
      "Epoch 27840/30000 Validation Loss: 0.07193627208471298\n",
      "Epoch 27841/30000 Training Loss: 0.08845718950033188\n",
      "Epoch 27842/30000 Training Loss: 0.06403637677431107\n",
      "Epoch 27843/30000 Training Loss: 0.0778050646185875\n",
      "Epoch 27844/30000 Training Loss: 0.09895366430282593\n",
      "Epoch 27845/30000 Training Loss: 0.08083454519510269\n",
      "Epoch 27846/30000 Training Loss: 0.08508079499006271\n",
      "Epoch 27847/30000 Training Loss: 0.06151080131530762\n",
      "Epoch 27848/30000 Training Loss: 0.07678326219320297\n",
      "Epoch 27849/30000 Training Loss: 0.06785812973976135\n",
      "Epoch 27850/30000 Training Loss: 0.06427663564682007\n",
      "Epoch 27850/30000 Validation Loss: 0.06156262382864952\n",
      "Epoch 27851/30000 Training Loss: 0.07685352116823196\n",
      "Epoch 27852/30000 Training Loss: 0.0645531490445137\n",
      "Epoch 27853/30000 Training Loss: 0.055706799030303955\n",
      "Epoch 27854/30000 Training Loss: 0.0643649697303772\n",
      "Epoch 27855/30000 Training Loss: 0.059627026319503784\n",
      "Epoch 27856/30000 Training Loss: 0.06984633207321167\n",
      "Epoch 27857/30000 Training Loss: 0.074932761490345\n",
      "Epoch 27858/30000 Training Loss: 0.06302333623170853\n",
      "Epoch 27859/30000 Training Loss: 0.09624793380498886\n",
      "Epoch 27860/30000 Training Loss: 0.0819944441318512\n",
      "Epoch 27860/30000 Validation Loss: 0.06000588834285736\n",
      "Epoch 27861/30000 Training Loss: 0.06107465550303459\n",
      "Epoch 27862/30000 Training Loss: 0.05649484321475029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27863/30000 Training Loss: 0.06690436601638794\n",
      "Epoch 27864/30000 Training Loss: 0.07079261541366577\n",
      "Epoch 27865/30000 Training Loss: 0.06435742229223251\n",
      "Epoch 27866/30000 Training Loss: 0.07214968651533127\n",
      "Epoch 27867/30000 Training Loss: 0.060570452362298965\n",
      "Epoch 27868/30000 Training Loss: 0.061444252729415894\n",
      "Epoch 27869/30000 Training Loss: 0.07890819758176804\n",
      "Epoch 27870/30000 Training Loss: 0.06611011177301407\n",
      "Epoch 27870/30000 Validation Loss: 0.06006699427962303\n",
      "Epoch 27871/30000 Training Loss: 0.05876253917813301\n",
      "Epoch 27872/30000 Training Loss: 0.06649821251630783\n",
      "Epoch 27873/30000 Training Loss: 0.08963638544082642\n",
      "Epoch 27874/30000 Training Loss: 0.05338304117321968\n",
      "Epoch 27875/30000 Training Loss: 0.07135527580976486\n",
      "Epoch 27876/30000 Training Loss: 0.08461824804544449\n",
      "Epoch 27877/30000 Training Loss: 0.062445688992738724\n",
      "Epoch 27878/30000 Training Loss: 0.06455235928297043\n",
      "Epoch 27879/30000 Training Loss: 0.06257975846529007\n",
      "Epoch 27880/30000 Training Loss: 0.0678839161992073\n",
      "Epoch 27880/30000 Validation Loss: 0.06780584901571274\n",
      "Epoch 27881/30000 Training Loss: 0.07012217491865158\n",
      "Epoch 27882/30000 Training Loss: 0.0833846926689148\n",
      "Epoch 27883/30000 Training Loss: 0.06659268587827682\n",
      "Epoch 27884/30000 Training Loss: 0.07814127951860428\n",
      "Epoch 27885/30000 Training Loss: 0.06718814373016357\n",
      "Epoch 27886/30000 Training Loss: 0.06070900335907936\n",
      "Epoch 27887/30000 Training Loss: 0.06645357608795166\n",
      "Epoch 27888/30000 Training Loss: 0.08040360361337662\n",
      "Epoch 27889/30000 Training Loss: 0.05251792073249817\n",
      "Epoch 27890/30000 Training Loss: 0.07545524090528488\n",
      "Epoch 27890/30000 Validation Loss: 0.04809199273586273\n",
      "Epoch 27891/30000 Training Loss: 0.07602159678936005\n",
      "Epoch 27892/30000 Training Loss: 0.0661495253443718\n",
      "Epoch 27893/30000 Training Loss: 0.08393640071153641\n",
      "Epoch 27894/30000 Training Loss: 0.07358062267303467\n",
      "Epoch 27895/30000 Training Loss: 0.06384261697530746\n",
      "Epoch 27896/30000 Training Loss: 0.062178533524274826\n",
      "Epoch 27897/30000 Training Loss: 0.053111448884010315\n",
      "Epoch 27898/30000 Training Loss: 0.06371592730283737\n",
      "Epoch 27899/30000 Training Loss: 0.09521174430847168\n",
      "Epoch 27900/30000 Training Loss: 0.056072335690259933\n",
      "Epoch 27900/30000 Validation Loss: 0.05790974199771881\n",
      "Epoch 27901/30000 Training Loss: 0.07203172892332077\n",
      "Epoch 27902/30000 Training Loss: 0.07253298908472061\n",
      "Epoch 27903/30000 Training Loss: 0.06242631748318672\n",
      "Epoch 27904/30000 Training Loss: 0.0669582188129425\n",
      "Epoch 27905/30000 Training Loss: 0.07968434691429138\n",
      "Epoch 27906/30000 Training Loss: 0.0812108963727951\n",
      "Epoch 27907/30000 Training Loss: 0.07623586803674698\n",
      "Epoch 27908/30000 Training Loss: 0.07057461887598038\n",
      "Epoch 27909/30000 Training Loss: 0.05240592360496521\n",
      "Epoch 27910/30000 Training Loss: 0.07102721929550171\n",
      "Epoch 27910/30000 Validation Loss: 0.07175277173519135\n",
      "Epoch 27911/30000 Training Loss: 0.05896565318107605\n",
      "Epoch 27912/30000 Training Loss: 0.06487145274877548\n",
      "Epoch 27913/30000 Training Loss: 0.06047039106488228\n",
      "Epoch 27914/30000 Training Loss: 0.09049775451421738\n",
      "Epoch 27915/30000 Training Loss: 0.06774038076400757\n",
      "Epoch 27916/30000 Training Loss: 0.06205977872014046\n",
      "Epoch 27917/30000 Training Loss: 0.062136825174093246\n",
      "Epoch 27918/30000 Training Loss: 0.0939079001545906\n",
      "Epoch 27919/30000 Training Loss: 0.06797664612531662\n",
      "Epoch 27920/30000 Training Loss: 0.0706486627459526\n",
      "Epoch 27920/30000 Validation Loss: 0.06758088618516922\n",
      "Epoch 27921/30000 Training Loss: 0.08058079332113266\n",
      "Epoch 27922/30000 Training Loss: 0.08708018064498901\n",
      "Epoch 27923/30000 Training Loss: 0.06677994877099991\n",
      "Epoch 27924/30000 Training Loss: 0.06428122520446777\n",
      "Epoch 27925/30000 Training Loss: 0.05492936447262764\n",
      "Epoch 27926/30000 Training Loss: 0.08518002182245255\n",
      "Epoch 27927/30000 Training Loss: 0.06329140067100525\n",
      "Epoch 27928/30000 Training Loss: 0.07806674391031265\n",
      "Epoch 27929/30000 Training Loss: 0.08314069360494614\n",
      "Epoch 27930/30000 Training Loss: 0.06285064667463303\n",
      "Epoch 27930/30000 Validation Loss: 0.05919802561402321\n",
      "Epoch 27931/30000 Training Loss: 0.06728150695562363\n",
      "Epoch 27932/30000 Training Loss: 0.07678309082984924\n",
      "Epoch 27933/30000 Training Loss: 0.07116672396659851\n",
      "Epoch 27934/30000 Training Loss: 0.07821547240018845\n",
      "Epoch 27935/30000 Training Loss: 0.07030028849840164\n",
      "Epoch 27936/30000 Training Loss: 0.06255156546831131\n",
      "Epoch 27937/30000 Training Loss: 0.0742422416806221\n",
      "Epoch 27938/30000 Training Loss: 0.06875887513160706\n",
      "Epoch 27939/30000 Training Loss: 0.061925992369651794\n",
      "Epoch 27940/30000 Training Loss: 0.07552088052034378\n",
      "Epoch 27940/30000 Validation Loss: 0.045545268803834915\n",
      "Epoch 27941/30000 Training Loss: 0.06591866165399551\n",
      "Epoch 27942/30000 Training Loss: 0.0774155929684639\n",
      "Epoch 27943/30000 Training Loss: 0.06448454409837723\n",
      "Epoch 27944/30000 Training Loss: 0.07824903726577759\n",
      "Epoch 27945/30000 Training Loss: 0.09637653827667236\n",
      "Epoch 27946/30000 Training Loss: 0.07396810501813889\n",
      "Epoch 27947/30000 Training Loss: 0.06800197809934616\n",
      "Epoch 27948/30000 Training Loss: 0.06547467410564423\n",
      "Epoch 27949/30000 Training Loss: 0.07516717910766602\n",
      "Epoch 27950/30000 Training Loss: 0.05694422498345375\n",
      "Epoch 27950/30000 Validation Loss: 0.059099841862916946\n",
      "Epoch 27951/30000 Training Loss: 0.05786705017089844\n",
      "Epoch 27952/30000 Training Loss: 0.07793528586626053\n",
      "Epoch 27953/30000 Training Loss: 0.06639602035284042\n",
      "Epoch 27954/30000 Training Loss: 0.07830283790826797\n",
      "Epoch 27955/30000 Training Loss: 0.06264690309762955\n",
      "Epoch 27956/30000 Training Loss: 0.07222936302423477\n",
      "Epoch 27957/30000 Training Loss: 0.07547754794359207\n",
      "Epoch 27958/30000 Training Loss: 0.061801303178071976\n",
      "Epoch 27959/30000 Training Loss: 0.06958193331956863\n",
      "Epoch 27960/30000 Training Loss: 0.07470452040433884\n",
      "Epoch 27960/30000 Validation Loss: 0.07880278676748276\n",
      "Epoch 27961/30000 Training Loss: 0.08031081408262253\n",
      "Epoch 27962/30000 Training Loss: 0.05659491941332817\n",
      "Epoch 27963/30000 Training Loss: 0.07382945716381073\n",
      "Epoch 27964/30000 Training Loss: 0.06083494424819946\n",
      "Epoch 27965/30000 Training Loss: 0.0787864625453949\n",
      "Epoch 27966/30000 Training Loss: 0.06919164210557938\n",
      "Epoch 27967/30000 Training Loss: 0.05937863513827324\n",
      "Epoch 27968/30000 Training Loss: 0.06795772165060043\n",
      "Epoch 27969/30000 Training Loss: 0.07879674434661865\n",
      "Epoch 27970/30000 Training Loss: 0.08632853627204895\n",
      "Epoch 27970/30000 Validation Loss: 0.07715832442045212\n",
      "Epoch 27971/30000 Training Loss: 0.07151064276695251\n",
      "Epoch 27972/30000 Training Loss: 0.06336159259080887\n",
      "Epoch 27973/30000 Training Loss: 0.053914155811071396\n",
      "Epoch 27974/30000 Training Loss: 0.07552475482225418\n",
      "Epoch 27975/30000 Training Loss: 0.08568064123392105\n",
      "Epoch 27976/30000 Training Loss: 0.0875500813126564\n",
      "Epoch 27977/30000 Training Loss: 0.06103423610329628\n",
      "Epoch 27978/30000 Training Loss: 0.060983624309301376\n",
      "Epoch 27979/30000 Training Loss: 0.07373685389757156\n",
      "Epoch 27980/30000 Training Loss: 0.056617364287376404\n",
      "Epoch 27980/30000 Validation Loss: 0.08479080349206924\n",
      "Epoch 27981/30000 Training Loss: 0.05916980281472206\n",
      "Epoch 27982/30000 Training Loss: 0.06841889768838882\n",
      "Epoch 27983/30000 Training Loss: 0.05411548539996147\n",
      "Epoch 27984/30000 Training Loss: 0.08489660173654556\n",
      "Epoch 27985/30000 Training Loss: 0.0730065330862999\n",
      "Epoch 27986/30000 Training Loss: 0.05250076949596405\n",
      "Epoch 27987/30000 Training Loss: 0.0647544413805008\n",
      "Epoch 27988/30000 Training Loss: 0.06958826631307602\n",
      "Epoch 27989/30000 Training Loss: 0.07159969210624695\n",
      "Epoch 27990/30000 Training Loss: 0.0592944510281086\n",
      "Epoch 27990/30000 Validation Loss: 0.05598969757556915\n",
      "Epoch 27991/30000 Training Loss: 0.07796180993318558\n",
      "Epoch 27992/30000 Training Loss: 0.0774456188082695\n",
      "Epoch 27993/30000 Training Loss: 0.07248008251190186\n",
      "Epoch 27994/30000 Training Loss: 0.08024672418832779\n",
      "Epoch 27995/30000 Training Loss: 0.0728098675608635\n",
      "Epoch 27996/30000 Training Loss: 0.05916474387049675\n",
      "Epoch 27997/30000 Training Loss: 0.06921056658029556\n",
      "Epoch 27998/30000 Training Loss: 0.06182393431663513\n",
      "Epoch 27999/30000 Training Loss: 0.05894353985786438\n",
      "Epoch 28000/30000 Training Loss: 0.0677751675248146\n",
      "Epoch 28000/30000 Validation Loss: 0.08925271779298782\n",
      "Epoch 28001/30000 Training Loss: 0.07275784015655518\n",
      "Epoch 28002/30000 Training Loss: 0.07469642162322998\n",
      "Epoch 28003/30000 Training Loss: 0.07959704101085663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28004/30000 Training Loss: 0.056023139506578445\n",
      "Epoch 28005/30000 Training Loss: 0.06488820910453796\n",
      "Epoch 28006/30000 Training Loss: 0.06026211380958557\n",
      "Epoch 28007/30000 Training Loss: 0.07278034836053848\n",
      "Epoch 28008/30000 Training Loss: 0.05956936255097389\n",
      "Epoch 28009/30000 Training Loss: 0.06708481907844543\n",
      "Epoch 28010/30000 Training Loss: 0.07849854975938797\n",
      "Epoch 28010/30000 Validation Loss: 0.05696556344628334\n",
      "Epoch 28011/30000 Training Loss: 0.053783971816301346\n",
      "Epoch 28012/30000 Training Loss: 0.08103431016206741\n",
      "Epoch 28013/30000 Training Loss: 0.05268292501568794\n",
      "Epoch 28014/30000 Training Loss: 0.05632004141807556\n",
      "Epoch 28015/30000 Training Loss: 0.07984299212694168\n",
      "Epoch 28016/30000 Training Loss: 0.06165935471653938\n",
      "Epoch 28017/30000 Training Loss: 0.08125745505094528\n",
      "Epoch 28018/30000 Training Loss: 0.06887436658143997\n",
      "Epoch 28019/30000 Training Loss: 0.06606568396091461\n",
      "Epoch 28020/30000 Training Loss: 0.06363800913095474\n",
      "Epoch 28020/30000 Validation Loss: 0.08907932043075562\n",
      "Epoch 28021/30000 Training Loss: 0.09764629602432251\n",
      "Epoch 28022/30000 Training Loss: 0.05423043295741081\n",
      "Epoch 28023/30000 Training Loss: 0.07651882618665695\n",
      "Epoch 28024/30000 Training Loss: 0.07362059503793716\n",
      "Epoch 28025/30000 Training Loss: 0.06445243954658508\n",
      "Epoch 28026/30000 Training Loss: 0.0700642541050911\n",
      "Epoch 28027/30000 Training Loss: 0.06433499604463577\n",
      "Epoch 28028/30000 Training Loss: 0.09908095747232437\n",
      "Epoch 28029/30000 Training Loss: 0.08601653575897217\n",
      "Epoch 28030/30000 Training Loss: 0.0704478845000267\n",
      "Epoch 28030/30000 Validation Loss: 0.0893675684928894\n",
      "Epoch 28031/30000 Training Loss: 0.06088376045227051\n",
      "Epoch 28032/30000 Training Loss: 0.06564752757549286\n",
      "Epoch 28033/30000 Training Loss: 0.07063943892717361\n",
      "Epoch 28034/30000 Training Loss: 0.06783216446638107\n",
      "Epoch 28035/30000 Training Loss: 0.07904469221830368\n",
      "Epoch 28036/30000 Training Loss: 0.06920835375785828\n",
      "Epoch 28037/30000 Training Loss: 0.07457975298166275\n",
      "Epoch 28038/30000 Training Loss: 0.071573406457901\n",
      "Epoch 28039/30000 Training Loss: 0.06692848354578018\n",
      "Epoch 28040/30000 Training Loss: 0.06658606976270676\n",
      "Epoch 28040/30000 Validation Loss: 0.07598049193620682\n",
      "Epoch 28041/30000 Training Loss: 0.0685669407248497\n",
      "Epoch 28042/30000 Training Loss: 0.07327403128147125\n",
      "Epoch 28043/30000 Training Loss: 0.059795260429382324\n",
      "Epoch 28044/30000 Training Loss: 0.08225653320550919\n",
      "Epoch 28045/30000 Training Loss: 0.06762824207544327\n",
      "Epoch 28046/30000 Training Loss: 0.06491797417402267\n",
      "Epoch 28047/30000 Training Loss: 0.06630198657512665\n",
      "Epoch 28048/30000 Training Loss: 0.07916303724050522\n",
      "Epoch 28049/30000 Training Loss: 0.06312686949968338\n",
      "Epoch 28050/30000 Training Loss: 0.06265660375356674\n",
      "Epoch 28050/30000 Validation Loss: 0.06650648266077042\n",
      "Epoch 28051/30000 Training Loss: 0.08166942745447159\n",
      "Epoch 28052/30000 Training Loss: 0.08172773569822311\n",
      "Epoch 28053/30000 Training Loss: 0.08276423811912537\n",
      "Epoch 28054/30000 Training Loss: 0.057314854115247726\n",
      "Epoch 28055/30000 Training Loss: 0.06795316189527512\n",
      "Epoch 28056/30000 Training Loss: 0.05877848342061043\n",
      "Epoch 28057/30000 Training Loss: 0.06718761473894119\n",
      "Epoch 28058/30000 Training Loss: 0.06402359157800674\n",
      "Epoch 28059/30000 Training Loss: 0.055236298590898514\n",
      "Epoch 28060/30000 Training Loss: 0.08082428574562073\n",
      "Epoch 28060/30000 Validation Loss: 0.0663202628493309\n",
      "Epoch 28061/30000 Training Loss: 0.0831337720155716\n",
      "Epoch 28062/30000 Training Loss: 0.06976820528507233\n",
      "Epoch 28063/30000 Training Loss: 0.060098808258771896\n",
      "Epoch 28064/30000 Training Loss: 0.05867229774594307\n",
      "Epoch 28065/30000 Training Loss: 0.060361672192811966\n",
      "Epoch 28066/30000 Training Loss: 0.05446957051753998\n",
      "Epoch 28067/30000 Training Loss: 0.06110406294465065\n",
      "Epoch 28068/30000 Training Loss: 0.0584164559841156\n",
      "Epoch 28069/30000 Training Loss: 0.07172083854675293\n",
      "Epoch 28070/30000 Training Loss: 0.07031752914190292\n",
      "Epoch 28070/30000 Validation Loss: 0.06383908540010452\n",
      "Epoch 28071/30000 Training Loss: 0.06000375375151634\n",
      "Epoch 28072/30000 Training Loss: 0.07030946016311646\n",
      "Epoch 28073/30000 Training Loss: 0.07329031825065613\n",
      "Epoch 28074/30000 Training Loss: 0.0680646076798439\n",
      "Epoch 28075/30000 Training Loss: 0.06851909309625626\n",
      "Epoch 28076/30000 Training Loss: 0.059285372495651245\n",
      "Epoch 28077/30000 Training Loss: 0.06580629199743271\n",
      "Epoch 28078/30000 Training Loss: 0.07189648598432541\n",
      "Epoch 28079/30000 Training Loss: 0.07909465581178665\n",
      "Epoch 28080/30000 Training Loss: 0.07624590396881104\n",
      "Epoch 28080/30000 Validation Loss: 0.06823000311851501\n",
      "Epoch 28081/30000 Training Loss: 0.06523352116346359\n",
      "Epoch 28082/30000 Training Loss: 0.0659184455871582\n",
      "Epoch 28083/30000 Training Loss: 0.05626377463340759\n",
      "Epoch 28084/30000 Training Loss: 0.06453094631433487\n",
      "Epoch 28085/30000 Training Loss: 0.06404564529657364\n",
      "Epoch 28086/30000 Training Loss: 0.059379126876592636\n",
      "Epoch 28087/30000 Training Loss: 0.07288813591003418\n",
      "Epoch 28088/30000 Training Loss: 0.06744388490915298\n",
      "Epoch 28089/30000 Training Loss: 0.056792471557855606\n",
      "Epoch 28090/30000 Training Loss: 0.058599408715963364\n",
      "Epoch 28090/30000 Validation Loss: 0.05412952974438667\n",
      "Epoch 28091/30000 Training Loss: 0.08315790444612503\n",
      "Epoch 28092/30000 Training Loss: 0.07968667894601822\n",
      "Epoch 28093/30000 Training Loss: 0.058215003460645676\n",
      "Epoch 28094/30000 Training Loss: 0.060686707496643066\n",
      "Epoch 28095/30000 Training Loss: 0.05913383141160011\n",
      "Epoch 28096/30000 Training Loss: 0.08593056350946426\n",
      "Epoch 28097/30000 Training Loss: 0.07438100874423981\n",
      "Epoch 28098/30000 Training Loss: 0.06367035210132599\n",
      "Epoch 28099/30000 Training Loss: 0.06494460999965668\n",
      "Epoch 28100/30000 Training Loss: 0.05354127660393715\n",
      "Epoch 28100/30000 Validation Loss: 0.06664719432592392\n",
      "Epoch 28101/30000 Training Loss: 0.07683879137039185\n",
      "Epoch 28102/30000 Training Loss: 0.07052762061357498\n",
      "Epoch 28103/30000 Training Loss: 0.0708339735865593\n",
      "Epoch 28104/30000 Training Loss: 0.06535690277814865\n",
      "Epoch 28105/30000 Training Loss: 0.0648038312792778\n",
      "Epoch 28106/30000 Training Loss: 0.07636049389839172\n",
      "Epoch 28107/30000 Training Loss: 0.08239380270242691\n",
      "Epoch 28108/30000 Training Loss: 0.07203374058008194\n",
      "Epoch 28109/30000 Training Loss: 0.07134950906038284\n",
      "Epoch 28110/30000 Training Loss: 0.06656784564256668\n",
      "Epoch 28110/30000 Validation Loss: 0.05476727709174156\n",
      "Epoch 28111/30000 Training Loss: 0.0774175301194191\n",
      "Epoch 28112/30000 Training Loss: 0.059344273060560226\n",
      "Epoch 28113/30000 Training Loss: 0.06370556354522705\n",
      "Epoch 28114/30000 Training Loss: 0.06409561634063721\n",
      "Epoch 28115/30000 Training Loss: 0.06844532489776611\n",
      "Epoch 28116/30000 Training Loss: 0.059276193380355835\n",
      "Epoch 28117/30000 Training Loss: 0.057449858635663986\n",
      "Epoch 28118/30000 Training Loss: 0.06463257223367691\n",
      "Epoch 28119/30000 Training Loss: 0.06239216402173042\n",
      "Epoch 28120/30000 Training Loss: 0.059316474944353104\n",
      "Epoch 28120/30000 Validation Loss: 0.07467912882566452\n",
      "Epoch 28121/30000 Training Loss: 0.054737966507673264\n",
      "Epoch 28122/30000 Training Loss: 0.06362180411815643\n",
      "Epoch 28123/30000 Training Loss: 0.07925456017255783\n",
      "Epoch 28124/30000 Training Loss: 0.06982734799385071\n",
      "Epoch 28125/30000 Training Loss: 0.08258635550737381\n",
      "Epoch 28126/30000 Training Loss: 0.08219835162162781\n",
      "Epoch 28127/30000 Training Loss: 0.07083778083324432\n",
      "Epoch 28128/30000 Training Loss: 0.05282953754067421\n",
      "Epoch 28129/30000 Training Loss: 0.06819204241037369\n",
      "Epoch 28130/30000 Training Loss: 0.08851218968629837\n",
      "Epoch 28130/30000 Validation Loss: 0.07882308214902878\n",
      "Epoch 28131/30000 Training Loss: 0.08750688284635544\n",
      "Epoch 28132/30000 Training Loss: 0.05595599487423897\n",
      "Epoch 28133/30000 Training Loss: 0.05407048389315605\n",
      "Epoch 28134/30000 Training Loss: 0.0543927364051342\n",
      "Epoch 28135/30000 Training Loss: 0.07497432082891464\n",
      "Epoch 28136/30000 Training Loss: 0.052286580204963684\n",
      "Epoch 28137/30000 Training Loss: 0.08825754374265671\n",
      "Epoch 28138/30000 Training Loss: 0.059789761900901794\n",
      "Epoch 28139/30000 Training Loss: 0.062470853328704834\n",
      "Epoch 28140/30000 Training Loss: 0.057836394757032394\n",
      "Epoch 28140/30000 Validation Loss: 0.08753302693367004\n",
      "Epoch 28141/30000 Training Loss: 0.06456323713064194\n",
      "Epoch 28142/30000 Training Loss: 0.08048061281442642\n",
      "Epoch 28143/30000 Training Loss: 0.07222936302423477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28144/30000 Training Loss: 0.07613968104124069\n",
      "Epoch 28145/30000 Training Loss: 0.06914912909269333\n",
      "Epoch 28146/30000 Training Loss: 0.06574215739965439\n",
      "Epoch 28147/30000 Training Loss: 0.09700848907232285\n",
      "Epoch 28148/30000 Training Loss: 0.06978923827409744\n",
      "Epoch 28149/30000 Training Loss: 0.06956600397825241\n",
      "Epoch 28150/30000 Training Loss: 0.05947187915444374\n",
      "Epoch 28150/30000 Validation Loss: 0.06117253005504608\n",
      "Epoch 28151/30000 Training Loss: 0.0758824273943901\n",
      "Epoch 28152/30000 Training Loss: 0.05971335247159004\n",
      "Epoch 28153/30000 Training Loss: 0.05483328178524971\n",
      "Epoch 28154/30000 Training Loss: 0.06831075996160507\n",
      "Epoch 28155/30000 Training Loss: 0.06653215736150742\n",
      "Epoch 28156/30000 Training Loss: 0.07251599431037903\n",
      "Epoch 28157/30000 Training Loss: 0.071137934923172\n",
      "Epoch 28158/30000 Training Loss: 0.08625400811433792\n",
      "Epoch 28159/30000 Training Loss: 0.07283636182546616\n",
      "Epoch 28160/30000 Training Loss: 0.06310035288333893\n",
      "Epoch 28160/30000 Validation Loss: 0.0660129189491272\n",
      "Epoch 28161/30000 Training Loss: 0.08514384180307388\n",
      "Epoch 28162/30000 Training Loss: 0.04882635548710823\n",
      "Epoch 28163/30000 Training Loss: 0.07387738674879074\n",
      "Epoch 28164/30000 Training Loss: 0.059970200061798096\n",
      "Epoch 28165/30000 Training Loss: 0.05549256131052971\n",
      "Epoch 28166/30000 Training Loss: 0.06900442391633987\n",
      "Epoch 28167/30000 Training Loss: 0.07435571402311325\n",
      "Epoch 28168/30000 Training Loss: 0.06968669593334198\n",
      "Epoch 28169/30000 Training Loss: 0.05589943751692772\n",
      "Epoch 28170/30000 Training Loss: 0.050895605236291885\n",
      "Epoch 28170/30000 Validation Loss: 0.0752873420715332\n",
      "Epoch 28171/30000 Training Loss: 0.06167471781373024\n",
      "Epoch 28172/30000 Training Loss: 0.08440136909484863\n",
      "Epoch 28173/30000 Training Loss: 0.054351259022951126\n",
      "Epoch 28174/30000 Training Loss: 0.08308769762516022\n",
      "Epoch 28175/30000 Training Loss: 0.058367226272821426\n",
      "Epoch 28176/30000 Training Loss: 0.07220884412527084\n",
      "Epoch 28177/30000 Training Loss: 0.07510272413492203\n",
      "Epoch 28178/30000 Training Loss: 0.07441060990095139\n",
      "Epoch 28179/30000 Training Loss: 0.05880849435925484\n",
      "Epoch 28180/30000 Training Loss: 0.05861852690577507\n",
      "Epoch 28180/30000 Validation Loss: 0.07018635421991348\n",
      "Epoch 28181/30000 Training Loss: 0.09336260706186295\n",
      "Epoch 28182/30000 Training Loss: 0.0654381513595581\n",
      "Epoch 28183/30000 Training Loss: 0.06511928886175156\n",
      "Epoch 28184/30000 Training Loss: 0.07679253071546555\n",
      "Epoch 28185/30000 Training Loss: 0.07670798897743225\n",
      "Epoch 28186/30000 Training Loss: 0.06803029775619507\n",
      "Epoch 28187/30000 Training Loss: 0.06724802404642105\n",
      "Epoch 28188/30000 Training Loss: 0.0519135482609272\n",
      "Epoch 28189/30000 Training Loss: 0.0747506394982338\n",
      "Epoch 28190/30000 Training Loss: 0.08604515343904495\n",
      "Epoch 28190/30000 Validation Loss: 0.07446911185979843\n",
      "Epoch 28191/30000 Training Loss: 0.06361580640077591\n",
      "Epoch 28192/30000 Training Loss: 0.05740925669670105\n",
      "Epoch 28193/30000 Training Loss: 0.083550363779068\n",
      "Epoch 28194/30000 Training Loss: 0.06727602332830429\n",
      "Epoch 28195/30000 Training Loss: 0.08695908635854721\n",
      "Epoch 28196/30000 Training Loss: 0.072669118642807\n",
      "Epoch 28197/30000 Training Loss: 0.0850425586104393\n",
      "Epoch 28198/30000 Training Loss: 0.07958204299211502\n",
      "Epoch 28199/30000 Training Loss: 0.07352990657091141\n",
      "Epoch 28200/30000 Training Loss: 0.08563382178544998\n",
      "Epoch 28200/30000 Validation Loss: 0.07215563952922821\n",
      "Epoch 28201/30000 Training Loss: 0.06663543730974197\n",
      "Epoch 28202/30000 Training Loss: 0.07333820313215256\n",
      "Epoch 28203/30000 Training Loss: 0.07383810728788376\n",
      "Epoch 28204/30000 Training Loss: 0.057844310998916626\n",
      "Epoch 28205/30000 Training Loss: 0.058968473225831985\n",
      "Epoch 28206/30000 Training Loss: 0.07101741433143616\n",
      "Epoch 28207/30000 Training Loss: 0.05819854140281677\n",
      "Epoch 28208/30000 Training Loss: 0.08103803545236588\n",
      "Epoch 28209/30000 Training Loss: 0.05295909196138382\n",
      "Epoch 28210/30000 Training Loss: 0.06324923038482666\n",
      "Epoch 28210/30000 Validation Loss: 0.05963994935154915\n",
      "Epoch 28211/30000 Training Loss: 0.0553772933781147\n",
      "Epoch 28212/30000 Training Loss: 0.0669202208518982\n",
      "Epoch 28213/30000 Training Loss: 0.07734892517328262\n",
      "Epoch 28214/30000 Training Loss: 0.0882890596985817\n",
      "Epoch 28215/30000 Training Loss: 0.05583307147026062\n",
      "Epoch 28216/30000 Training Loss: 0.08068756014108658\n",
      "Epoch 28217/30000 Training Loss: 0.06746798008680344\n",
      "Epoch 28218/30000 Training Loss: 0.057222116738557816\n",
      "Epoch 28219/30000 Training Loss: 0.06876275688409805\n",
      "Epoch 28220/30000 Training Loss: 0.09536972641944885\n",
      "Epoch 28220/30000 Validation Loss: 0.08051720261573792\n",
      "Epoch 28221/30000 Training Loss: 0.054780881851911545\n",
      "Epoch 28222/30000 Training Loss: 0.060684431344270706\n",
      "Epoch 28223/30000 Training Loss: 0.07127895951271057\n",
      "Epoch 28224/30000 Training Loss: 0.058258261531591415\n",
      "Epoch 28225/30000 Training Loss: 0.06536581367254257\n",
      "Epoch 28226/30000 Training Loss: 0.07971229404211044\n",
      "Epoch 28227/30000 Training Loss: 0.06375663727521896\n",
      "Epoch 28228/30000 Training Loss: 0.06985336542129517\n",
      "Epoch 28229/30000 Training Loss: 0.06846995651721954\n",
      "Epoch 28230/30000 Training Loss: 0.06549805402755737\n",
      "Epoch 28230/30000 Validation Loss: 0.05947837233543396\n",
      "Epoch 28231/30000 Training Loss: 0.07926806062459946\n",
      "Epoch 28232/30000 Training Loss: 0.08132759481668472\n",
      "Epoch 28233/30000 Training Loss: 0.05292915925383568\n",
      "Epoch 28234/30000 Training Loss: 0.06755255907773972\n",
      "Epoch 28235/30000 Training Loss: 0.06486393511295319\n",
      "Epoch 28236/30000 Training Loss: 0.07036638259887695\n",
      "Epoch 28237/30000 Training Loss: 0.07105953246355057\n",
      "Epoch 28238/30000 Training Loss: 0.07572288066148758\n",
      "Epoch 28239/30000 Training Loss: 0.06503516435623169\n",
      "Epoch 28240/30000 Training Loss: 0.07585851103067398\n",
      "Epoch 28240/30000 Validation Loss: 0.062386032193899155\n",
      "Epoch 28241/30000 Training Loss: 0.08844529837369919\n",
      "Epoch 28242/30000 Training Loss: 0.06463364511728287\n",
      "Epoch 28243/30000 Training Loss: 0.0693124532699585\n",
      "Epoch 28244/30000 Training Loss: 0.07009730488061905\n",
      "Epoch 28245/30000 Training Loss: 0.06619835644960403\n",
      "Epoch 28246/30000 Training Loss: 0.06476771086454391\n",
      "Epoch 28247/30000 Training Loss: 0.07462725788354874\n",
      "Epoch 28248/30000 Training Loss: 0.06278989464044571\n",
      "Epoch 28249/30000 Training Loss: 0.08316072076559067\n",
      "Epoch 28250/30000 Training Loss: 0.09287556260824203\n",
      "Epoch 28250/30000 Validation Loss: 0.07332361489534378\n",
      "Epoch 28251/30000 Training Loss: 0.08302990347146988\n",
      "Epoch 28252/30000 Training Loss: 0.06465457379817963\n",
      "Epoch 28253/30000 Training Loss: 0.0702323392033577\n",
      "Epoch 28254/30000 Training Loss: 0.062328364700078964\n",
      "Epoch 28255/30000 Training Loss: 0.06790496408939362\n",
      "Epoch 28256/30000 Training Loss: 0.07369668781757355\n",
      "Epoch 28257/30000 Training Loss: 0.07342198491096497\n",
      "Epoch 28258/30000 Training Loss: 0.07599855214357376\n",
      "Epoch 28259/30000 Training Loss: 0.07275035232305527\n",
      "Epoch 28260/30000 Training Loss: 0.07940015196800232\n",
      "Epoch 28260/30000 Validation Loss: 0.06764418631792068\n",
      "Epoch 28261/30000 Training Loss: 0.07460146397352219\n",
      "Epoch 28262/30000 Training Loss: 0.06389070302248001\n",
      "Epoch 28263/30000 Training Loss: 0.04754028841853142\n",
      "Epoch 28264/30000 Training Loss: 0.05753950774669647\n",
      "Epoch 28265/30000 Training Loss: 0.060474321246147156\n",
      "Epoch 28266/30000 Training Loss: 0.07443907111883163\n",
      "Epoch 28267/30000 Training Loss: 0.059446435421705246\n",
      "Epoch 28268/30000 Training Loss: 0.06465267390012741\n",
      "Epoch 28269/30000 Training Loss: 0.055223267525434494\n",
      "Epoch 28270/30000 Training Loss: 0.08718249946832657\n",
      "Epoch 28270/30000 Validation Loss: 0.06487695127725601\n",
      "Epoch 28271/30000 Training Loss: 0.0771569013595581\n",
      "Epoch 28272/30000 Training Loss: 0.07911017537117004\n",
      "Epoch 28273/30000 Training Loss: 0.07019185274839401\n",
      "Epoch 28274/30000 Training Loss: 0.0738275945186615\n",
      "Epoch 28275/30000 Training Loss: 0.06719005107879639\n",
      "Epoch 28276/30000 Training Loss: 0.0744677260518074\n",
      "Epoch 28277/30000 Training Loss: 0.06511038541793823\n",
      "Epoch 28278/30000 Training Loss: 0.058506399393081665\n",
      "Epoch 28279/30000 Training Loss: 0.06538773328065872\n",
      "Epoch 28280/30000 Training Loss: 0.07616402953863144\n",
      "Epoch 28280/30000 Validation Loss: 0.062041640281677246\n",
      "Epoch 28281/30000 Training Loss: 0.06178775057196617\n",
      "Epoch 28282/30000 Training Loss: 0.05876007676124573\n",
      "Epoch 28283/30000 Training Loss: 0.06687614321708679\n",
      "Epoch 28284/30000 Training Loss: 0.08011048287153244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28285/30000 Training Loss: 0.0619843564927578\n",
      "Epoch 28286/30000 Training Loss: 0.07057078927755356\n",
      "Epoch 28287/30000 Training Loss: 0.07729852199554443\n",
      "Epoch 28288/30000 Training Loss: 0.07358066737651825\n",
      "Epoch 28289/30000 Training Loss: 0.06030398607254028\n",
      "Epoch 28290/30000 Training Loss: 0.0625709742307663\n",
      "Epoch 28290/30000 Validation Loss: 0.060982927680015564\n",
      "Epoch 28291/30000 Training Loss: 0.08320463448762894\n",
      "Epoch 28292/30000 Training Loss: 0.06774181127548218\n",
      "Epoch 28293/30000 Training Loss: 0.07051578164100647\n",
      "Epoch 28294/30000 Training Loss: 0.08434301614761353\n",
      "Epoch 28295/30000 Training Loss: 0.06576588749885559\n",
      "Epoch 28296/30000 Training Loss: 0.07553499191999435\n",
      "Epoch 28297/30000 Training Loss: 0.0726737231016159\n",
      "Epoch 28298/30000 Training Loss: 0.07739762216806412\n",
      "Epoch 28299/30000 Training Loss: 0.07199186831712723\n",
      "Epoch 28300/30000 Training Loss: 0.07480964809656143\n",
      "Epoch 28300/30000 Validation Loss: 0.07535690069198608\n",
      "Epoch 28301/30000 Training Loss: 0.07663308829069138\n",
      "Epoch 28302/30000 Training Loss: 0.08067202568054199\n",
      "Epoch 28303/30000 Training Loss: 0.05549046769738197\n",
      "Epoch 28304/30000 Training Loss: 0.07252776622772217\n",
      "Epoch 28305/30000 Training Loss: 0.061044007539749146\n",
      "Epoch 28306/30000 Training Loss: 0.08345333486795425\n",
      "Epoch 28307/30000 Training Loss: 0.053937703371047974\n",
      "Epoch 28308/30000 Training Loss: 0.08127833157777786\n",
      "Epoch 28309/30000 Training Loss: 0.06364186853170395\n",
      "Epoch 28310/30000 Training Loss: 0.06178423389792442\n",
      "Epoch 28310/30000 Validation Loss: 0.06158290430903435\n",
      "Epoch 28311/30000 Training Loss: 0.0628001019358635\n",
      "Epoch 28312/30000 Training Loss: 0.07434918731451035\n",
      "Epoch 28313/30000 Training Loss: 0.05066361650824547\n",
      "Epoch 28314/30000 Training Loss: 0.08419644832611084\n",
      "Epoch 28315/30000 Training Loss: 0.06989515572786331\n",
      "Epoch 28316/30000 Training Loss: 0.07244875282049179\n",
      "Epoch 28317/30000 Training Loss: 0.07217538356781006\n",
      "Epoch 28318/30000 Training Loss: 0.0880649983882904\n",
      "Epoch 28319/30000 Training Loss: 0.08200839906930923\n",
      "Epoch 28320/30000 Training Loss: 0.08353728801012039\n",
      "Epoch 28320/30000 Validation Loss: 0.08028309792280197\n",
      "Epoch 28321/30000 Training Loss: 0.06776288151741028\n",
      "Epoch 28322/30000 Training Loss: 0.06307868659496307\n",
      "Epoch 28323/30000 Training Loss: 0.057151127606630325\n",
      "Epoch 28324/30000 Training Loss: 0.058885861188173294\n",
      "Epoch 28325/30000 Training Loss: 0.05874347314238548\n",
      "Epoch 28326/30000 Training Loss: 0.06524402648210526\n",
      "Epoch 28327/30000 Training Loss: 0.06716008484363556\n",
      "Epoch 28328/30000 Training Loss: 0.07994681596755981\n",
      "Epoch 28329/30000 Training Loss: 0.07777979969978333\n",
      "Epoch 28330/30000 Training Loss: 0.06413988023996353\n",
      "Epoch 28330/30000 Validation Loss: 0.06822356581687927\n",
      "Epoch 28331/30000 Training Loss: 0.07873164862394333\n",
      "Epoch 28332/30000 Training Loss: 0.0560929961502552\n",
      "Epoch 28333/30000 Training Loss: 0.06447245925664902\n",
      "Epoch 28334/30000 Training Loss: 0.06245948374271393\n",
      "Epoch 28335/30000 Training Loss: 0.06916558742523193\n",
      "Epoch 28336/30000 Training Loss: 0.05900345742702484\n",
      "Epoch 28337/30000 Training Loss: 0.09321198612451553\n",
      "Epoch 28338/30000 Training Loss: 0.056655555963516235\n",
      "Epoch 28339/30000 Training Loss: 0.08420637995004654\n",
      "Epoch 28340/30000 Training Loss: 0.06165849044919014\n",
      "Epoch 28340/30000 Validation Loss: 0.07757307589054108\n",
      "Epoch 28341/30000 Training Loss: 0.07564987987279892\n",
      "Epoch 28342/30000 Training Loss: 0.07635225355625153\n",
      "Epoch 28343/30000 Training Loss: 0.06896565109491348\n",
      "Epoch 28344/30000 Training Loss: 0.07430914789438248\n",
      "Epoch 28345/30000 Training Loss: 0.06083838641643524\n",
      "Epoch 28346/30000 Training Loss: 0.06981194764375687\n",
      "Epoch 28347/30000 Training Loss: 0.09099385142326355\n",
      "Epoch 28348/30000 Training Loss: 0.07535625249147415\n",
      "Epoch 28349/30000 Training Loss: 0.057403143495321274\n",
      "Epoch 28350/30000 Training Loss: 0.08551647514104843\n",
      "Epoch 28350/30000 Validation Loss: 0.06826893985271454\n",
      "Epoch 28351/30000 Training Loss: 0.06564115732908249\n",
      "Epoch 28352/30000 Training Loss: 0.08374956995248795\n",
      "Epoch 28353/30000 Training Loss: 0.07381406426429749\n",
      "Epoch 28354/30000 Training Loss: 0.05457593500614166\n",
      "Epoch 28355/30000 Training Loss: 0.05979496240615845\n",
      "Epoch 28356/30000 Training Loss: 0.05572271719574928\n",
      "Epoch 28357/30000 Training Loss: 0.10023894906044006\n",
      "Epoch 28358/30000 Training Loss: 0.07432340830564499\n",
      "Epoch 28359/30000 Training Loss: 0.05409076809883118\n",
      "Epoch 28360/30000 Training Loss: 0.07659085839986801\n",
      "Epoch 28360/30000 Validation Loss: 0.06490350514650345\n",
      "Epoch 28361/30000 Training Loss: 0.06832230091094971\n",
      "Epoch 28362/30000 Training Loss: 0.06830301135778427\n",
      "Epoch 28363/30000 Training Loss: 0.07149641960859299\n",
      "Epoch 28364/30000 Training Loss: 0.059120167046785355\n",
      "Epoch 28365/30000 Training Loss: 0.06877905875444412\n",
      "Epoch 28366/30000 Training Loss: 0.058907970786094666\n",
      "Epoch 28367/30000 Training Loss: 0.06674519926309586\n",
      "Epoch 28368/30000 Training Loss: 0.08093757927417755\n",
      "Epoch 28369/30000 Training Loss: 0.07893702387809753\n",
      "Epoch 28370/30000 Training Loss: 0.0628943145275116\n",
      "Epoch 28370/30000 Validation Loss: 0.07199367135763168\n",
      "Epoch 28371/30000 Training Loss: 0.07034077495336533\n",
      "Epoch 28372/30000 Training Loss: 0.08267146348953247\n",
      "Epoch 28373/30000 Training Loss: 0.07341606169939041\n",
      "Epoch 28374/30000 Training Loss: 0.06179167330265045\n",
      "Epoch 28375/30000 Training Loss: 0.06384971737861633\n",
      "Epoch 28376/30000 Training Loss: 0.053960684686899185\n",
      "Epoch 28377/30000 Training Loss: 0.05941450595855713\n",
      "Epoch 28378/30000 Training Loss: 0.07208885252475739\n",
      "Epoch 28379/30000 Training Loss: 0.07802090048789978\n",
      "Epoch 28380/30000 Training Loss: 0.07123784720897675\n",
      "Epoch 28380/30000 Validation Loss: 0.07342827320098877\n",
      "Epoch 28381/30000 Training Loss: 0.05827636644244194\n",
      "Epoch 28382/30000 Training Loss: 0.07888638973236084\n",
      "Epoch 28383/30000 Training Loss: 0.06407461315393448\n",
      "Epoch 28384/30000 Training Loss: 0.07522282749414444\n",
      "Epoch 28385/30000 Training Loss: 0.06589696556329727\n",
      "Epoch 28386/30000 Training Loss: 0.06601344048976898\n",
      "Epoch 28387/30000 Training Loss: 0.07203375548124313\n",
      "Epoch 28388/30000 Training Loss: 0.06545145064592361\n",
      "Epoch 28389/30000 Training Loss: 0.06070640683174133\n",
      "Epoch 28390/30000 Training Loss: 0.07552728801965714\n",
      "Epoch 28390/30000 Validation Loss: 0.0739913061261177\n",
      "Epoch 28391/30000 Training Loss: 0.07513464242219925\n",
      "Epoch 28392/30000 Training Loss: 0.05384630337357521\n",
      "Epoch 28393/30000 Training Loss: 0.07630516588687897\n",
      "Epoch 28394/30000 Training Loss: 0.08559548854827881\n",
      "Epoch 28395/30000 Training Loss: 0.09012050181627274\n",
      "Epoch 28396/30000 Training Loss: 0.06702422350645065\n",
      "Epoch 28397/30000 Training Loss: 0.07407132536172867\n",
      "Epoch 28398/30000 Training Loss: 0.08400359749794006\n",
      "Epoch 28399/30000 Training Loss: 0.0843244269490242\n",
      "Epoch 28400/30000 Training Loss: 0.06482666730880737\n",
      "Epoch 28400/30000 Validation Loss: 0.05200699344277382\n",
      "Epoch 28401/30000 Training Loss: 0.07539116591215134\n",
      "Epoch 28402/30000 Training Loss: 0.07486853003501892\n",
      "Epoch 28403/30000 Training Loss: 0.06516977399587631\n",
      "Epoch 28404/30000 Training Loss: 0.06617898494005203\n",
      "Epoch 28405/30000 Training Loss: 0.06535836309194565\n",
      "Epoch 28406/30000 Training Loss: 0.06859183311462402\n",
      "Epoch 28407/30000 Training Loss: 0.07192665338516235\n",
      "Epoch 28408/30000 Training Loss: 0.07478231191635132\n",
      "Epoch 28409/30000 Training Loss: 0.061120420694351196\n",
      "Epoch 28410/30000 Training Loss: 0.07688089460134506\n",
      "Epoch 28410/30000 Validation Loss: 0.09025105088949203\n",
      "Epoch 28411/30000 Training Loss: 0.0654570683836937\n",
      "Epoch 28412/30000 Training Loss: 0.09236418455839157\n",
      "Epoch 28413/30000 Training Loss: 0.06178842857480049\n",
      "Epoch 28414/30000 Training Loss: 0.07445327192544937\n",
      "Epoch 28415/30000 Training Loss: 0.07764741033315659\n",
      "Epoch 28416/30000 Training Loss: 0.06324970722198486\n",
      "Epoch 28417/30000 Training Loss: 0.055566225200891495\n",
      "Epoch 28418/30000 Training Loss: 0.06468861550092697\n",
      "Epoch 28419/30000 Training Loss: 0.07850254327058792\n",
      "Epoch 28420/30000 Training Loss: 0.06871619075536728\n",
      "Epoch 28420/30000 Validation Loss: 0.07658874243497849\n",
      "Epoch 28421/30000 Training Loss: 0.07237092405557632\n",
      "Epoch 28422/30000 Training Loss: 0.062339603900909424\n",
      "Epoch 28423/30000 Training Loss: 0.07155042141675949\n",
      "Epoch 28424/30000 Training Loss: 0.0766635462641716\n",
      "Epoch 28425/30000 Training Loss: 0.06357812136411667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28426/30000 Training Loss: 0.0500485934317112\n",
      "Epoch 28427/30000 Training Loss: 0.050321538001298904\n",
      "Epoch 28428/30000 Training Loss: 0.07132577151060104\n",
      "Epoch 28429/30000 Training Loss: 0.062017425894737244\n",
      "Epoch 28430/30000 Training Loss: 0.08279021084308624\n",
      "Epoch 28430/30000 Validation Loss: 0.06423517316579819\n",
      "Epoch 28431/30000 Training Loss: 0.08201867341995239\n",
      "Epoch 28432/30000 Training Loss: 0.08610286563634872\n",
      "Epoch 28433/30000 Training Loss: 0.08673888444900513\n",
      "Epoch 28434/30000 Training Loss: 0.07966095209121704\n",
      "Epoch 28435/30000 Training Loss: 0.061866775155067444\n",
      "Epoch 28436/30000 Training Loss: 0.07951764762401581\n",
      "Epoch 28437/30000 Training Loss: 0.05554787442088127\n",
      "Epoch 28438/30000 Training Loss: 0.07513123005628586\n",
      "Epoch 28439/30000 Training Loss: 0.05643989518284798\n",
      "Epoch 28440/30000 Training Loss: 0.07833719998598099\n",
      "Epoch 28440/30000 Validation Loss: 0.07563058286905289\n",
      "Epoch 28441/30000 Training Loss: 0.06943277269601822\n",
      "Epoch 28442/30000 Training Loss: 0.06730236858129501\n",
      "Epoch 28443/30000 Training Loss: 0.07620584219694138\n",
      "Epoch 28444/30000 Training Loss: 0.07324245572090149\n",
      "Epoch 28445/30000 Training Loss: 0.0619690865278244\n",
      "Epoch 28446/30000 Training Loss: 0.06366575509309769\n",
      "Epoch 28447/30000 Training Loss: 0.08087832480669022\n",
      "Epoch 28448/30000 Training Loss: 0.06762554496526718\n",
      "Epoch 28449/30000 Training Loss: 0.06496541947126389\n",
      "Epoch 28450/30000 Training Loss: 0.0769144743680954\n",
      "Epoch 28450/30000 Validation Loss: 0.059675171971321106\n",
      "Epoch 28451/30000 Training Loss: 0.07103566080331802\n",
      "Epoch 28452/30000 Training Loss: 0.08026837557554245\n",
      "Epoch 28453/30000 Training Loss: 0.07279352843761444\n",
      "Epoch 28454/30000 Training Loss: 0.05361185967922211\n",
      "Epoch 28455/30000 Training Loss: 0.06287491321563721\n",
      "Epoch 28456/30000 Training Loss: 0.08374809473752975\n",
      "Epoch 28457/30000 Training Loss: 0.07430186122655869\n",
      "Epoch 28458/30000 Training Loss: 0.08141572028398514\n",
      "Epoch 28459/30000 Training Loss: 0.08136669546365738\n",
      "Epoch 28460/30000 Training Loss: 0.08098963648080826\n",
      "Epoch 28460/30000 Validation Loss: 0.05535277724266052\n",
      "Epoch 28461/30000 Training Loss: 0.06782981008291245\n",
      "Epoch 28462/30000 Training Loss: 0.07713828980922699\n",
      "Epoch 28463/30000 Training Loss: 0.05633829906582832\n",
      "Epoch 28464/30000 Training Loss: 0.08049843460321426\n",
      "Epoch 28465/30000 Training Loss: 0.08950608968734741\n",
      "Epoch 28466/30000 Training Loss: 0.06642762571573257\n",
      "Epoch 28467/30000 Training Loss: 0.08128779381513596\n",
      "Epoch 28468/30000 Training Loss: 0.09531352669000626\n",
      "Epoch 28469/30000 Training Loss: 0.05032092705368996\n",
      "Epoch 28470/30000 Training Loss: 0.0551871694624424\n",
      "Epoch 28470/30000 Validation Loss: 0.07628854364156723\n",
      "Epoch 28471/30000 Training Loss: 0.07646261900663376\n",
      "Epoch 28472/30000 Training Loss: 0.06338689476251602\n",
      "Epoch 28473/30000 Training Loss: 0.07298512756824493\n",
      "Epoch 28474/30000 Training Loss: 0.0768362283706665\n",
      "Epoch 28475/30000 Training Loss: 0.05569938197731972\n",
      "Epoch 28476/30000 Training Loss: 0.06973210722208023\n",
      "Epoch 28477/30000 Training Loss: 0.07317323237657547\n",
      "Epoch 28478/30000 Training Loss: 0.06954308599233627\n",
      "Epoch 28479/30000 Training Loss: 0.08462940901517868\n",
      "Epoch 28480/30000 Training Loss: 0.06834398210048676\n",
      "Epoch 28480/30000 Validation Loss: 0.0752943605184555\n",
      "Epoch 28481/30000 Training Loss: 0.07253295928239822\n",
      "Epoch 28482/30000 Training Loss: 0.07525608688592911\n",
      "Epoch 28483/30000 Training Loss: 0.06341493874788284\n",
      "Epoch 28484/30000 Training Loss: 0.06352200359106064\n",
      "Epoch 28485/30000 Training Loss: 0.08575256913900375\n",
      "Epoch 28486/30000 Training Loss: 0.07010450959205627\n",
      "Epoch 28487/30000 Training Loss: 0.06068648025393486\n",
      "Epoch 28488/30000 Training Loss: 0.06473874300718307\n",
      "Epoch 28489/30000 Training Loss: 0.06668440252542496\n",
      "Epoch 28490/30000 Training Loss: 0.07488003373146057\n",
      "Epoch 28490/30000 Validation Loss: 0.0633462443947792\n",
      "Epoch 28491/30000 Training Loss: 0.07371991127729416\n",
      "Epoch 28492/30000 Training Loss: 0.07286367565393448\n",
      "Epoch 28493/30000 Training Loss: 0.0693601593375206\n",
      "Epoch 28494/30000 Training Loss: 0.05843536555767059\n",
      "Epoch 28495/30000 Training Loss: 0.06667713075876236\n",
      "Epoch 28496/30000 Training Loss: 0.06320717930793762\n",
      "Epoch 28497/30000 Training Loss: 0.06628131866455078\n",
      "Epoch 28498/30000 Training Loss: 0.0788891464471817\n",
      "Epoch 28499/30000 Training Loss: 0.06338571757078171\n",
      "Epoch 28500/30000 Training Loss: 0.06083439290523529\n",
      "Epoch 28500/30000 Validation Loss: 0.08998733758926392\n",
      "Epoch 28501/30000 Training Loss: 0.0703505352139473\n",
      "Epoch 28502/30000 Training Loss: 0.06580522656440735\n",
      "Epoch 28503/30000 Training Loss: 0.05611639842391014\n",
      "Epoch 28504/30000 Training Loss: 0.06698815524578094\n",
      "Epoch 28505/30000 Training Loss: 0.06718125194311142\n",
      "Epoch 28506/30000 Training Loss: 0.08949878066778183\n",
      "Epoch 28507/30000 Training Loss: 0.07568343728780746\n",
      "Epoch 28508/30000 Training Loss: 0.06708481162786484\n",
      "Epoch 28509/30000 Training Loss: 0.06975934654474258\n",
      "Epoch 28510/30000 Training Loss: 0.07386718690395355\n",
      "Epoch 28510/30000 Validation Loss: 0.0859515443444252\n",
      "Epoch 28511/30000 Training Loss: 0.06856023520231247\n",
      "Epoch 28512/30000 Training Loss: 0.06811696290969849\n",
      "Epoch 28513/30000 Training Loss: 0.07053610682487488\n",
      "Epoch 28514/30000 Training Loss: 0.07595362514257431\n",
      "Epoch 28515/30000 Training Loss: 0.08290860801935196\n",
      "Epoch 28516/30000 Training Loss: 0.0568213015794754\n",
      "Epoch 28517/30000 Training Loss: 0.06334590166807175\n",
      "Epoch 28518/30000 Training Loss: 0.07389766722917557\n",
      "Epoch 28519/30000 Training Loss: 0.07519474625587463\n",
      "Epoch 28520/30000 Training Loss: 0.07745613902807236\n",
      "Epoch 28520/30000 Validation Loss: 0.10055939108133316\n",
      "Epoch 28521/30000 Training Loss: 0.05842923745512962\n",
      "Epoch 28522/30000 Training Loss: 0.09030503779649734\n",
      "Epoch 28523/30000 Training Loss: 0.07648692280054092\n",
      "Epoch 28524/30000 Training Loss: 0.057317495346069336\n",
      "Epoch 28525/30000 Training Loss: 0.0765993595123291\n",
      "Epoch 28526/30000 Training Loss: 0.06237129867076874\n",
      "Epoch 28527/30000 Training Loss: 0.07265910506248474\n",
      "Epoch 28528/30000 Training Loss: 0.08072639256715775\n",
      "Epoch 28529/30000 Training Loss: 0.06479975581169128\n",
      "Epoch 28530/30000 Training Loss: 0.07821734994649887\n",
      "Epoch 28530/30000 Validation Loss: 0.08010304719209671\n",
      "Epoch 28531/30000 Training Loss: 0.085837222635746\n",
      "Epoch 28532/30000 Training Loss: 0.07882561534643173\n",
      "Epoch 28533/30000 Training Loss: 0.058394599705934525\n",
      "Epoch 28534/30000 Training Loss: 0.07208612561225891\n",
      "Epoch 28535/30000 Training Loss: 0.07193908840417862\n",
      "Epoch 28536/30000 Training Loss: 0.07174751907587051\n",
      "Epoch 28537/30000 Training Loss: 0.06206126883625984\n",
      "Epoch 28538/30000 Training Loss: 0.09472006559371948\n",
      "Epoch 28539/30000 Training Loss: 0.06808897107839584\n",
      "Epoch 28540/30000 Training Loss: 0.07842608541250229\n",
      "Epoch 28540/30000 Validation Loss: 0.062165338546037674\n",
      "Epoch 28541/30000 Training Loss: 0.06892548501491547\n",
      "Epoch 28542/30000 Training Loss: 0.07093369215726852\n",
      "Epoch 28543/30000 Training Loss: 0.06152142211794853\n",
      "Epoch 28544/30000 Training Loss: 0.05984845757484436\n",
      "Epoch 28545/30000 Training Loss: 0.06665167957544327\n",
      "Epoch 28546/30000 Training Loss: 0.06693696975708008\n",
      "Epoch 28547/30000 Training Loss: 0.06962940841913223\n",
      "Epoch 28548/30000 Training Loss: 0.08634117990732193\n",
      "Epoch 28549/30000 Training Loss: 0.08007685840129852\n",
      "Epoch 28550/30000 Training Loss: 0.059634193778038025\n",
      "Epoch 28550/30000 Validation Loss: 0.05582934617996216\n",
      "Epoch 28551/30000 Training Loss: 0.061179447919130325\n",
      "Epoch 28552/30000 Training Loss: 0.06834254413843155\n",
      "Epoch 28553/30000 Training Loss: 0.06512167304754257\n",
      "Epoch 28554/30000 Training Loss: 0.06808274239301682\n",
      "Epoch 28555/30000 Training Loss: 0.06034291163086891\n",
      "Epoch 28556/30000 Training Loss: 0.07169454544782639\n",
      "Epoch 28557/30000 Training Loss: 0.06190454959869385\n",
      "Epoch 28558/30000 Training Loss: 0.0871024802327156\n",
      "Epoch 28559/30000 Training Loss: 0.07378655672073364\n",
      "Epoch 28560/30000 Training Loss: 0.08222506195306778\n",
      "Epoch 28560/30000 Validation Loss: 0.06899447739124298\n",
      "Epoch 28561/30000 Training Loss: 0.07749244570732117\n",
      "Epoch 28562/30000 Training Loss: 0.07847558706998825\n",
      "Epoch 28563/30000 Training Loss: 0.07054024934768677\n",
      "Epoch 28564/30000 Training Loss: 0.08644639700651169\n",
      "Epoch 28565/30000 Training Loss: 0.05718033015727997\n",
      "Epoch 28566/30000 Training Loss: 0.061057787388563156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28567/30000 Training Loss: 0.05720524117350578\n",
      "Epoch 28568/30000 Training Loss: 0.07510645687580109\n",
      "Epoch 28569/30000 Training Loss: 0.059531569480895996\n",
      "Epoch 28570/30000 Training Loss: 0.06456214189529419\n",
      "Epoch 28570/30000 Validation Loss: 0.04672154784202576\n",
      "Epoch 28571/30000 Training Loss: 0.0656028687953949\n",
      "Epoch 28572/30000 Training Loss: 0.08234990388154984\n",
      "Epoch 28573/30000 Training Loss: 0.05548867955803871\n",
      "Epoch 28574/30000 Training Loss: 0.08580074459314346\n",
      "Epoch 28575/30000 Training Loss: 0.07984274625778198\n",
      "Epoch 28576/30000 Training Loss: 0.06470111757516861\n",
      "Epoch 28577/30000 Training Loss: 0.08508062362670898\n",
      "Epoch 28578/30000 Training Loss: 0.07828018814325333\n",
      "Epoch 28579/30000 Training Loss: 0.08587068319320679\n",
      "Epoch 28580/30000 Training Loss: 0.07893741130828857\n",
      "Epoch 28580/30000 Validation Loss: 0.0937218889594078\n",
      "Epoch 28581/30000 Training Loss: 0.06321755796670914\n",
      "Epoch 28582/30000 Training Loss: 0.061273083090782166\n",
      "Epoch 28583/30000 Training Loss: 0.053565461188554764\n",
      "Epoch 28584/30000 Training Loss: 0.06582693010568619\n",
      "Epoch 28585/30000 Training Loss: 0.07498922944068909\n",
      "Epoch 28586/30000 Training Loss: 0.06274934858083725\n",
      "Epoch 28587/30000 Training Loss: 0.08535481244325638\n",
      "Epoch 28588/30000 Training Loss: 0.08056672662496567\n",
      "Epoch 28589/30000 Training Loss: 0.07164295017719269\n",
      "Epoch 28590/30000 Training Loss: 0.075539730489254\n",
      "Epoch 28590/30000 Validation Loss: 0.0794876292347908\n",
      "Epoch 28591/30000 Training Loss: 0.08400777727365494\n",
      "Epoch 28592/30000 Training Loss: 0.06842724233865738\n",
      "Epoch 28593/30000 Training Loss: 0.060743510723114014\n",
      "Epoch 28594/30000 Training Loss: 0.07306190580129623\n",
      "Epoch 28595/30000 Training Loss: 0.061962563544511795\n",
      "Epoch 28596/30000 Training Loss: 0.06609541922807693\n",
      "Epoch 28597/30000 Training Loss: 0.051571231335401535\n",
      "Epoch 28598/30000 Training Loss: 0.0590420663356781\n",
      "Epoch 28599/30000 Training Loss: 0.0686693862080574\n",
      "Epoch 28600/30000 Training Loss: 0.07770010083913803\n",
      "Epoch 28600/30000 Validation Loss: 0.06363020092248917\n",
      "Epoch 28601/30000 Training Loss: 0.06166375055909157\n",
      "Epoch 28602/30000 Training Loss: 0.07000581175088882\n",
      "Epoch 28603/30000 Training Loss: 0.09731245785951614\n",
      "Epoch 28604/30000 Training Loss: 0.0684095248579979\n",
      "Epoch 28605/30000 Training Loss: 0.08189866691827774\n",
      "Epoch 28606/30000 Training Loss: 0.06706663221120834\n",
      "Epoch 28607/30000 Training Loss: 0.06561186909675598\n",
      "Epoch 28608/30000 Training Loss: 0.07162212580442429\n",
      "Epoch 28609/30000 Training Loss: 0.07114968448877335\n",
      "Epoch 28610/30000 Training Loss: 0.06686855107545853\n",
      "Epoch 28610/30000 Validation Loss: 0.09727270156145096\n",
      "Epoch 28611/30000 Training Loss: 0.08758027106523514\n",
      "Epoch 28612/30000 Training Loss: 0.06907576322555542\n",
      "Epoch 28613/30000 Training Loss: 0.06499647349119186\n",
      "Epoch 28614/30000 Training Loss: 0.07501943409442902\n",
      "Epoch 28615/30000 Training Loss: 0.06757503002882004\n",
      "Epoch 28616/30000 Training Loss: 0.06791771203279495\n",
      "Epoch 28617/30000 Training Loss: 0.05009967461228371\n",
      "Epoch 28618/30000 Training Loss: 0.08476119488477707\n",
      "Epoch 28619/30000 Training Loss: 0.05486070737242699\n",
      "Epoch 28620/30000 Training Loss: 0.06400450319051743\n",
      "Epoch 28620/30000 Validation Loss: 0.08479664474725723\n",
      "Epoch 28621/30000 Training Loss: 0.0769023522734642\n",
      "Epoch 28622/30000 Training Loss: 0.06051263213157654\n",
      "Epoch 28623/30000 Training Loss: 0.07702759653329849\n",
      "Epoch 28624/30000 Training Loss: 0.06664387136697769\n",
      "Epoch 28625/30000 Training Loss: 0.07242028415203094\n",
      "Epoch 28626/30000 Training Loss: 0.06321533769369125\n",
      "Epoch 28627/30000 Training Loss: 0.059280794113874435\n",
      "Epoch 28628/30000 Training Loss: 0.05622251704335213\n",
      "Epoch 28629/30000 Training Loss: 0.08554372191429138\n",
      "Epoch 28630/30000 Training Loss: 0.07767537236213684\n",
      "Epoch 28630/30000 Validation Loss: 0.06333426386117935\n",
      "Epoch 28631/30000 Training Loss: 0.07246177643537521\n",
      "Epoch 28632/30000 Training Loss: 0.08874789625406265\n",
      "Epoch 28633/30000 Training Loss: 0.08085794001817703\n",
      "Epoch 28634/30000 Training Loss: 0.06815484166145325\n",
      "Epoch 28635/30000 Training Loss: 0.054422106593847275\n",
      "Epoch 28636/30000 Training Loss: 0.0661846324801445\n",
      "Epoch 28637/30000 Training Loss: 0.0738445520401001\n",
      "Epoch 28638/30000 Training Loss: 0.06613709777593613\n",
      "Epoch 28639/30000 Training Loss: 0.08790155500173569\n",
      "Epoch 28640/30000 Training Loss: 0.07250004261732101\n",
      "Epoch 28640/30000 Validation Loss: 0.07094822824001312\n",
      "Epoch 28641/30000 Training Loss: 0.06717175245285034\n",
      "Epoch 28642/30000 Training Loss: 0.06195387244224548\n",
      "Epoch 28643/30000 Training Loss: 0.05919843539595604\n",
      "Epoch 28644/30000 Training Loss: 0.07959982007741928\n",
      "Epoch 28645/30000 Training Loss: 0.06974384933710098\n",
      "Epoch 28646/30000 Training Loss: 0.0670430138707161\n",
      "Epoch 28647/30000 Training Loss: 0.06425642967224121\n",
      "Epoch 28648/30000 Training Loss: 0.08151301741600037\n",
      "Epoch 28649/30000 Training Loss: 0.06294041126966476\n",
      "Epoch 28650/30000 Training Loss: 0.062466900795698166\n",
      "Epoch 28650/30000 Validation Loss: 0.06809956580400467\n",
      "Epoch 28651/30000 Training Loss: 0.07199970632791519\n",
      "Epoch 28652/30000 Training Loss: 0.08180475980043411\n",
      "Epoch 28653/30000 Training Loss: 0.05992782115936279\n",
      "Epoch 28654/30000 Training Loss: 0.06568408757448196\n",
      "Epoch 28655/30000 Training Loss: 0.08110054582357407\n",
      "Epoch 28656/30000 Training Loss: 0.06227995455265045\n",
      "Epoch 28657/30000 Training Loss: 0.0750049501657486\n",
      "Epoch 28658/30000 Training Loss: 0.076223224401474\n",
      "Epoch 28659/30000 Training Loss: 0.07460841536521912\n",
      "Epoch 28660/30000 Training Loss: 0.06535117328166962\n",
      "Epoch 28660/30000 Validation Loss: 0.07015890628099442\n",
      "Epoch 28661/30000 Training Loss: 0.06252613663673401\n",
      "Epoch 28662/30000 Training Loss: 0.07938787341117859\n",
      "Epoch 28663/30000 Training Loss: 0.05344773456454277\n",
      "Epoch 28664/30000 Training Loss: 0.05987449362874031\n",
      "Epoch 28665/30000 Training Loss: 0.07431063055992126\n",
      "Epoch 28666/30000 Training Loss: 0.06412885338068008\n",
      "Epoch 28667/30000 Training Loss: 0.05277177691459656\n",
      "Epoch 28668/30000 Training Loss: 0.07225237041711807\n",
      "Epoch 28669/30000 Training Loss: 0.0966312363743782\n",
      "Epoch 28670/30000 Training Loss: 0.06352290511131287\n",
      "Epoch 28670/30000 Validation Loss: 0.07257014513015747\n",
      "Epoch 28671/30000 Training Loss: 0.06502214074134827\n",
      "Epoch 28672/30000 Training Loss: 0.07855809479951859\n",
      "Epoch 28673/30000 Training Loss: 0.06220082566142082\n",
      "Epoch 28674/30000 Training Loss: 0.05627458170056343\n",
      "Epoch 28675/30000 Training Loss: 0.07497677206993103\n",
      "Epoch 28676/30000 Training Loss: 0.07370863109827042\n",
      "Epoch 28677/30000 Training Loss: 0.05781745910644531\n",
      "Epoch 28678/30000 Training Loss: 0.08932340890169144\n",
      "Epoch 28679/30000 Training Loss: 0.06354300677776337\n",
      "Epoch 28680/30000 Training Loss: 0.06493091583251953\n",
      "Epoch 28680/30000 Validation Loss: 0.0783405676484108\n",
      "Epoch 28681/30000 Training Loss: 0.05857938528060913\n",
      "Epoch 28682/30000 Training Loss: 0.07333910465240479\n",
      "Epoch 28683/30000 Training Loss: 0.06726857274770737\n",
      "Epoch 28684/30000 Training Loss: 0.061743393540382385\n",
      "Epoch 28685/30000 Training Loss: 0.0781412348151207\n",
      "Epoch 28686/30000 Training Loss: 0.0743822455406189\n",
      "Epoch 28687/30000 Training Loss: 0.07099252194166183\n",
      "Epoch 28688/30000 Training Loss: 0.07273548096418381\n",
      "Epoch 28689/30000 Training Loss: 0.08342549949884415\n",
      "Epoch 28690/30000 Training Loss: 0.06790166348218918\n",
      "Epoch 28690/30000 Validation Loss: 0.06339097768068314\n",
      "Epoch 28691/30000 Training Loss: 0.07812688499689102\n",
      "Epoch 28692/30000 Training Loss: 0.0759270116686821\n",
      "Epoch 28693/30000 Training Loss: 0.06599154323339462\n",
      "Epoch 28694/30000 Training Loss: 0.05638279393315315\n",
      "Epoch 28695/30000 Training Loss: 0.0825112834572792\n",
      "Epoch 28696/30000 Training Loss: 0.07094836980104446\n",
      "Epoch 28697/30000 Training Loss: 0.08429943770170212\n",
      "Epoch 28698/30000 Training Loss: 0.06693404912948608\n",
      "Epoch 28699/30000 Training Loss: 0.08594298362731934\n",
      "Epoch 28700/30000 Training Loss: 0.08187945932149887\n",
      "Epoch 28700/30000 Validation Loss: 0.0668419674038887\n",
      "Epoch 28701/30000 Training Loss: 0.0609879195690155\n",
      "Epoch 28702/30000 Training Loss: 0.059182971715927124\n",
      "Epoch 28703/30000 Training Loss: 0.09196987748146057\n",
      "Epoch 28704/30000 Training Loss: 0.0839117243885994\n",
      "Epoch 28705/30000 Training Loss: 0.05856260657310486\n",
      "Epoch 28706/30000 Training Loss: 0.0592714361846447\n",
      "Epoch 28707/30000 Training Loss: 0.060977157205343246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28708/30000 Training Loss: 0.05938252806663513\n",
      "Epoch 28709/30000 Training Loss: 0.08946225047111511\n",
      "Epoch 28710/30000 Training Loss: 0.07309205830097198\n",
      "Epoch 28710/30000 Validation Loss: 0.08370146155357361\n",
      "Epoch 28711/30000 Training Loss: 0.07684146612882614\n",
      "Epoch 28712/30000 Training Loss: 0.06685132533311844\n",
      "Epoch 28713/30000 Training Loss: 0.05637243390083313\n",
      "Epoch 28714/30000 Training Loss: 0.06289749592542648\n",
      "Epoch 28715/30000 Training Loss: 0.09138552099466324\n",
      "Epoch 28716/30000 Training Loss: 0.0641002207994461\n",
      "Epoch 28717/30000 Training Loss: 0.05926724150776863\n",
      "Epoch 28718/30000 Training Loss: 0.07539664208889008\n",
      "Epoch 28719/30000 Training Loss: 0.0793200358748436\n",
      "Epoch 28720/30000 Training Loss: 0.09089759737253189\n",
      "Epoch 28720/30000 Validation Loss: 0.082973413169384\n",
      "Epoch 28721/30000 Training Loss: 0.0672585517168045\n",
      "Epoch 28722/30000 Training Loss: 0.06912035495042801\n",
      "Epoch 28723/30000 Training Loss: 0.06348998099565506\n",
      "Epoch 28724/30000 Training Loss: 0.058396678417921066\n",
      "Epoch 28725/30000 Training Loss: 0.08066079020500183\n",
      "Epoch 28726/30000 Training Loss: 0.057714566588401794\n",
      "Epoch 28727/30000 Training Loss: 0.07490634173154831\n",
      "Epoch 28728/30000 Training Loss: 0.0658363625407219\n",
      "Epoch 28729/30000 Training Loss: 0.07129064947366714\n",
      "Epoch 28730/30000 Training Loss: 0.06462268531322479\n",
      "Epoch 28730/30000 Validation Loss: 0.07327120751142502\n",
      "Epoch 28731/30000 Training Loss: 0.0697791799902916\n",
      "Epoch 28732/30000 Training Loss: 0.05617423728108406\n",
      "Epoch 28733/30000 Training Loss: 0.06528329104185104\n",
      "Epoch 28734/30000 Training Loss: 0.07460248470306396\n",
      "Epoch 28735/30000 Training Loss: 0.0532405860722065\n",
      "Epoch 28736/30000 Training Loss: 0.06637075543403625\n",
      "Epoch 28737/30000 Training Loss: 0.08371987193822861\n",
      "Epoch 28738/30000 Training Loss: 0.06770497560501099\n",
      "Epoch 28739/30000 Training Loss: 0.10064690560102463\n",
      "Epoch 28740/30000 Training Loss: 0.05489576980471611\n",
      "Epoch 28740/30000 Validation Loss: 0.0715756043791771\n",
      "Epoch 28741/30000 Training Loss: 0.08310559391975403\n",
      "Epoch 28742/30000 Training Loss: 0.06618344038724899\n",
      "Epoch 28743/30000 Training Loss: 0.06909380108118057\n",
      "Epoch 28744/30000 Training Loss: 0.06578486412763596\n",
      "Epoch 28745/30000 Training Loss: 0.08326420933008194\n",
      "Epoch 28746/30000 Training Loss: 0.06865987181663513\n",
      "Epoch 28747/30000 Training Loss: 0.0686088278889656\n",
      "Epoch 28748/30000 Training Loss: 0.07358833402395248\n",
      "Epoch 28749/30000 Training Loss: 0.06370660662651062\n",
      "Epoch 28750/30000 Training Loss: 0.061486970633268356\n",
      "Epoch 28750/30000 Validation Loss: 0.08468016982078552\n",
      "Epoch 28751/30000 Training Loss: 0.07534682750701904\n",
      "Epoch 28752/30000 Training Loss: 0.08968895673751831\n",
      "Epoch 28753/30000 Training Loss: 0.08089704811573029\n",
      "Epoch 28754/30000 Training Loss: 0.06335194408893585\n",
      "Epoch 28755/30000 Training Loss: 0.08496584743261337\n",
      "Epoch 28756/30000 Training Loss: 0.07572309672832489\n",
      "Epoch 28757/30000 Training Loss: 0.065220907330513\n",
      "Epoch 28758/30000 Training Loss: 0.08491217344999313\n",
      "Epoch 28759/30000 Training Loss: 0.0626288577914238\n",
      "Epoch 28760/30000 Training Loss: 0.07970616221427917\n",
      "Epoch 28760/30000 Validation Loss: 0.06777673959732056\n",
      "Epoch 28761/30000 Training Loss: 0.09248629957437515\n",
      "Epoch 28762/30000 Training Loss: 0.06028289720416069\n",
      "Epoch 28763/30000 Training Loss: 0.06379693001508713\n",
      "Epoch 28764/30000 Training Loss: 0.066129170358181\n",
      "Epoch 28765/30000 Training Loss: 0.060674116015434265\n",
      "Epoch 28766/30000 Training Loss: 0.05349106714129448\n",
      "Epoch 28767/30000 Training Loss: 0.06564026325941086\n",
      "Epoch 28768/30000 Training Loss: 0.0853056013584137\n",
      "Epoch 28769/30000 Training Loss: 0.053868453949689865\n",
      "Epoch 28770/30000 Training Loss: 0.06648973375558853\n",
      "Epoch 28770/30000 Validation Loss: 0.06484147161245346\n",
      "Epoch 28771/30000 Training Loss: 0.06392255425453186\n",
      "Epoch 28772/30000 Training Loss: 0.05460883677005768\n",
      "Epoch 28773/30000 Training Loss: 0.05258624628186226\n",
      "Epoch 28774/30000 Training Loss: 0.06412184238433838\n",
      "Epoch 28775/30000 Training Loss: 0.07275516539812088\n",
      "Epoch 28776/30000 Training Loss: 0.06640545278787613\n",
      "Epoch 28777/30000 Training Loss: 0.05963259935379028\n",
      "Epoch 28778/30000 Training Loss: 0.06788432598114014\n",
      "Epoch 28779/30000 Training Loss: 0.051889047026634216\n",
      "Epoch 28780/30000 Training Loss: 0.05857786536216736\n",
      "Epoch 28780/30000 Validation Loss: 0.09680292010307312\n",
      "Epoch 28781/30000 Training Loss: 0.07124140858650208\n",
      "Epoch 28782/30000 Training Loss: 0.07328592985868454\n",
      "Epoch 28783/30000 Training Loss: 0.059653040021657944\n",
      "Epoch 28784/30000 Training Loss: 0.06374210119247437\n",
      "Epoch 28785/30000 Training Loss: 0.059733230620622635\n",
      "Epoch 28786/30000 Training Loss: 0.06239163503050804\n",
      "Epoch 28787/30000 Training Loss: 0.08486553281545639\n",
      "Epoch 28788/30000 Training Loss: 0.07032722234725952\n",
      "Epoch 28789/30000 Training Loss: 0.07385457307100296\n",
      "Epoch 28790/30000 Training Loss: 0.0663505420088768\n",
      "Epoch 28790/30000 Validation Loss: 0.08340474218130112\n",
      "Epoch 28791/30000 Training Loss: 0.08231298625469208\n",
      "Epoch 28792/30000 Training Loss: 0.061740607023239136\n",
      "Epoch 28793/30000 Training Loss: 0.0832134410738945\n",
      "Epoch 28794/30000 Training Loss: 0.05960876867175102\n",
      "Epoch 28795/30000 Training Loss: 0.06451103091239929\n",
      "Epoch 28796/30000 Training Loss: 0.07476162910461426\n",
      "Epoch 28797/30000 Training Loss: 0.08211378008127213\n",
      "Epoch 28798/30000 Training Loss: 0.0700707882642746\n",
      "Epoch 28799/30000 Training Loss: 0.060187339782714844\n",
      "Epoch 28800/30000 Training Loss: 0.06793370097875595\n",
      "Epoch 28800/30000 Validation Loss: 0.06273204833269119\n",
      "Epoch 28801/30000 Training Loss: 0.05809139087796211\n",
      "Epoch 28802/30000 Training Loss: 0.07014751434326172\n",
      "Epoch 28803/30000 Training Loss: 0.06024995073676109\n",
      "Epoch 28804/30000 Training Loss: 0.06441305577754974\n",
      "Epoch 28805/30000 Training Loss: 0.06214599683880806\n",
      "Epoch 28806/30000 Training Loss: 0.07251141220331192\n",
      "Epoch 28807/30000 Training Loss: 0.06173345446586609\n",
      "Epoch 28808/30000 Training Loss: 0.06991346925497055\n",
      "Epoch 28809/30000 Training Loss: 0.06683706492185593\n",
      "Epoch 28810/30000 Training Loss: 0.06942996382713318\n",
      "Epoch 28810/30000 Validation Loss: 0.05639711022377014\n",
      "Epoch 28811/30000 Training Loss: 0.07646747678518295\n",
      "Epoch 28812/30000 Training Loss: 0.059010203927755356\n",
      "Epoch 28813/30000 Training Loss: 0.06797847896814346\n",
      "Epoch 28814/30000 Training Loss: 0.07544556260108948\n",
      "Epoch 28815/30000 Training Loss: 0.05401287600398064\n",
      "Epoch 28816/30000 Training Loss: 0.07071053981781006\n",
      "Epoch 28817/30000 Training Loss: 0.07211923599243164\n",
      "Epoch 28818/30000 Training Loss: 0.08638425916433334\n",
      "Epoch 28819/30000 Training Loss: 0.06549616903066635\n",
      "Epoch 28820/30000 Training Loss: 0.09402503818273544\n",
      "Epoch 28820/30000 Validation Loss: 0.07879668474197388\n",
      "Epoch 28821/30000 Training Loss: 0.06178395077586174\n",
      "Epoch 28822/30000 Training Loss: 0.0670388787984848\n",
      "Epoch 28823/30000 Training Loss: 0.0632316917181015\n",
      "Epoch 28824/30000 Training Loss: 0.05945025011897087\n",
      "Epoch 28825/30000 Training Loss: 0.07168085128068924\n",
      "Epoch 28826/30000 Training Loss: 0.07271704822778702\n",
      "Epoch 28827/30000 Training Loss: 0.07620327919721603\n",
      "Epoch 28828/30000 Training Loss: 0.07982943207025528\n",
      "Epoch 28829/30000 Training Loss: 0.06703656166791916\n",
      "Epoch 28830/30000 Training Loss: 0.08327185362577438\n",
      "Epoch 28830/30000 Validation Loss: 0.10359875112771988\n",
      "Epoch 28831/30000 Training Loss: 0.06587442755699158\n",
      "Epoch 28832/30000 Training Loss: 0.054688796401023865\n",
      "Epoch 28833/30000 Training Loss: 0.07849603146314621\n",
      "Epoch 28834/30000 Training Loss: 0.09896639734506607\n",
      "Epoch 28835/30000 Training Loss: 0.06983699649572372\n",
      "Epoch 28836/30000 Training Loss: 0.09707228094339371\n",
      "Epoch 28837/30000 Training Loss: 0.08244649320840836\n",
      "Epoch 28838/30000 Training Loss: 0.07147767394781113\n",
      "Epoch 28839/30000 Training Loss: 0.07849866896867752\n",
      "Epoch 28840/30000 Training Loss: 0.0632440522313118\n",
      "Epoch 28840/30000 Validation Loss: 0.05880962312221527\n",
      "Epoch 28841/30000 Training Loss: 0.061001766473054886\n",
      "Epoch 28842/30000 Training Loss: 0.0769934430718422\n",
      "Epoch 28843/30000 Training Loss: 0.05830470100045204\n",
      "Epoch 28844/30000 Training Loss: 0.06296347826719284\n",
      "Epoch 28845/30000 Training Loss: 0.054017603397369385\n",
      "Epoch 28846/30000 Training Loss: 0.08549856394529343\n",
      "Epoch 28847/30000 Training Loss: 0.07727807760238647\n",
      "Epoch 28848/30000 Training Loss: 0.0740610882639885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28849/30000 Training Loss: 0.06694941222667694\n",
      "Epoch 28850/30000 Training Loss: 0.051687825471162796\n",
      "Epoch 28850/30000 Validation Loss: 0.07207357883453369\n",
      "Epoch 28851/30000 Training Loss: 0.05978183075785637\n",
      "Epoch 28852/30000 Training Loss: 0.08893077820539474\n",
      "Epoch 28853/30000 Training Loss: 0.07956472784280777\n",
      "Epoch 28854/30000 Training Loss: 0.05884658917784691\n",
      "Epoch 28855/30000 Training Loss: 0.08374281972646713\n",
      "Epoch 28856/30000 Training Loss: 0.06789091974496841\n",
      "Epoch 28857/30000 Training Loss: 0.045945633202791214\n",
      "Epoch 28858/30000 Training Loss: 0.06559432297945023\n",
      "Epoch 28859/30000 Training Loss: 0.06906651705503464\n",
      "Epoch 28860/30000 Training Loss: 0.06305018812417984\n",
      "Epoch 28860/30000 Validation Loss: 0.06120910868048668\n",
      "Epoch 28861/30000 Training Loss: 0.06293914467096329\n",
      "Epoch 28862/30000 Training Loss: 0.07144839316606522\n",
      "Epoch 28863/30000 Training Loss: 0.06861177831888199\n",
      "Epoch 28864/30000 Training Loss: 0.0755508542060852\n",
      "Epoch 28865/30000 Training Loss: 0.06102091446518898\n",
      "Epoch 28866/30000 Training Loss: 0.06950734555721283\n",
      "Epoch 28867/30000 Training Loss: 0.07630092650651932\n",
      "Epoch 28868/30000 Training Loss: 0.05222121253609657\n",
      "Epoch 28869/30000 Training Loss: 0.0651111900806427\n",
      "Epoch 28870/30000 Training Loss: 0.06608856469392776\n",
      "Epoch 28870/30000 Validation Loss: 0.06681208312511444\n",
      "Epoch 28871/30000 Training Loss: 0.06533896178007126\n",
      "Epoch 28872/30000 Training Loss: 0.09773602336645126\n",
      "Epoch 28873/30000 Training Loss: 0.06024222448468208\n",
      "Epoch 28874/30000 Training Loss: 0.06277734786272049\n",
      "Epoch 28875/30000 Training Loss: 0.07065378874540329\n",
      "Epoch 28876/30000 Training Loss: 0.05867047980427742\n",
      "Epoch 28877/30000 Training Loss: 0.06673538684844971\n",
      "Epoch 28878/30000 Training Loss: 0.06227951869368553\n",
      "Epoch 28879/30000 Training Loss: 0.08374615758657455\n",
      "Epoch 28880/30000 Training Loss: 0.06568245589733124\n",
      "Epoch 28880/30000 Validation Loss: 0.07057362794876099\n",
      "Epoch 28881/30000 Training Loss: 0.068718321621418\n",
      "Epoch 28882/30000 Training Loss: 0.06309053301811218\n",
      "Epoch 28883/30000 Training Loss: 0.056781280785799026\n",
      "Epoch 28884/30000 Training Loss: 0.08496148139238358\n",
      "Epoch 28885/30000 Training Loss: 0.06605866551399231\n",
      "Epoch 28886/30000 Training Loss: 0.06437786668539047\n",
      "Epoch 28887/30000 Training Loss: 0.07425972074270248\n",
      "Epoch 28888/30000 Training Loss: 0.05821606144309044\n",
      "Epoch 28889/30000 Training Loss: 0.07033591717481613\n",
      "Epoch 28890/30000 Training Loss: 0.07755961269140244\n",
      "Epoch 28890/30000 Validation Loss: 0.06876090168952942\n",
      "Epoch 28891/30000 Training Loss: 0.07759924978017807\n",
      "Epoch 28892/30000 Training Loss: 0.09236618131399155\n",
      "Epoch 28893/30000 Training Loss: 0.069517120718956\n",
      "Epoch 28894/30000 Training Loss: 0.07310763746500015\n",
      "Epoch 28895/30000 Training Loss: 0.0682598426938057\n",
      "Epoch 28896/30000 Training Loss: 0.07243753224611282\n",
      "Epoch 28897/30000 Training Loss: 0.05074151232838631\n",
      "Epoch 28898/30000 Training Loss: 0.08953019976615906\n",
      "Epoch 28899/30000 Training Loss: 0.06994259357452393\n",
      "Epoch 28900/30000 Training Loss: 0.06300689280033112\n",
      "Epoch 28900/30000 Validation Loss: 0.07178067415952682\n",
      "Epoch 28901/30000 Training Loss: 0.0764724537730217\n",
      "Epoch 28902/30000 Training Loss: 0.08497073501348495\n",
      "Epoch 28903/30000 Training Loss: 0.0757913812994957\n",
      "Epoch 28904/30000 Training Loss: 0.05305362865328789\n",
      "Epoch 28905/30000 Training Loss: 0.06128570809960365\n",
      "Epoch 28906/30000 Training Loss: 0.06114702299237251\n",
      "Epoch 28907/30000 Training Loss: 0.05572275444865227\n",
      "Epoch 28908/30000 Training Loss: 0.07501297444105148\n",
      "Epoch 28909/30000 Training Loss: 0.06910257786512375\n",
      "Epoch 28910/30000 Training Loss: 0.08798101544380188\n",
      "Epoch 28910/30000 Validation Loss: 0.06851101666688919\n",
      "Epoch 28911/30000 Training Loss: 0.0467737652361393\n",
      "Epoch 28912/30000 Training Loss: 0.059290409088134766\n",
      "Epoch 28913/30000 Training Loss: 0.07206708192825317\n",
      "Epoch 28914/30000 Training Loss: 0.0629529356956482\n",
      "Epoch 28915/30000 Training Loss: 0.09282561391592026\n",
      "Epoch 28916/30000 Training Loss: 0.06768804788589478\n",
      "Epoch 28917/30000 Training Loss: 0.06345745921134949\n",
      "Epoch 28918/30000 Training Loss: 0.05886340141296387\n",
      "Epoch 28919/30000 Training Loss: 0.09891161322593689\n",
      "Epoch 28920/30000 Training Loss: 0.08249899744987488\n",
      "Epoch 28920/30000 Validation Loss: 0.06839185208082199\n",
      "Epoch 28921/30000 Training Loss: 0.06313817203044891\n",
      "Epoch 28922/30000 Training Loss: 0.0635627880692482\n",
      "Epoch 28923/30000 Training Loss: 0.06922116130590439\n",
      "Epoch 28924/30000 Training Loss: 0.07719061523675919\n",
      "Epoch 28925/30000 Training Loss: 0.07263243198394775\n",
      "Epoch 28926/30000 Training Loss: 0.08168189972639084\n",
      "Epoch 28927/30000 Training Loss: 0.07074431329965591\n",
      "Epoch 28928/30000 Training Loss: 0.07651317119598389\n",
      "Epoch 28929/30000 Training Loss: 0.06387097388505936\n",
      "Epoch 28930/30000 Training Loss: 0.07132285833358765\n",
      "Epoch 28930/30000 Validation Loss: 0.08058983832597733\n",
      "Epoch 28931/30000 Training Loss: 0.09111211448907852\n",
      "Epoch 28932/30000 Training Loss: 0.06672587990760803\n",
      "Epoch 28933/30000 Training Loss: 0.06782928109169006\n",
      "Epoch 28934/30000 Training Loss: 0.06785008311271667\n",
      "Epoch 28935/30000 Training Loss: 0.07042761892080307\n",
      "Epoch 28936/30000 Training Loss: 0.05619972571730614\n",
      "Epoch 28937/30000 Training Loss: 0.06054973602294922\n",
      "Epoch 28938/30000 Training Loss: 0.06783229112625122\n",
      "Epoch 28939/30000 Training Loss: 0.06924527138471603\n",
      "Epoch 28940/30000 Training Loss: 0.06753440946340561\n",
      "Epoch 28940/30000 Validation Loss: 0.06591594964265823\n",
      "Epoch 28941/30000 Training Loss: 0.06863810867071152\n",
      "Epoch 28942/30000 Training Loss: 0.06208851933479309\n",
      "Epoch 28943/30000 Training Loss: 0.06825005263090134\n",
      "Epoch 28944/30000 Training Loss: 0.08380480855703354\n",
      "Epoch 28945/30000 Training Loss: 0.08160430938005447\n",
      "Epoch 28946/30000 Training Loss: 0.06008315086364746\n",
      "Epoch 28947/30000 Training Loss: 0.05310148373246193\n",
      "Epoch 28948/30000 Training Loss: 0.07292439788579941\n",
      "Epoch 28949/30000 Training Loss: 0.07329051941633224\n",
      "Epoch 28950/30000 Training Loss: 0.07489223033189774\n",
      "Epoch 28950/30000 Validation Loss: 0.06304392963647842\n",
      "Epoch 28951/30000 Training Loss: 0.07759854197502136\n",
      "Epoch 28952/30000 Training Loss: 0.08260823041200638\n",
      "Epoch 28953/30000 Training Loss: 0.046891480684280396\n",
      "Epoch 28954/30000 Training Loss: 0.07787299901247025\n",
      "Epoch 28955/30000 Training Loss: 0.06328346580266953\n",
      "Epoch 28956/30000 Training Loss: 0.057874321937561035\n",
      "Epoch 28957/30000 Training Loss: 0.08470162004232407\n",
      "Epoch 28958/30000 Training Loss: 0.06164832040667534\n",
      "Epoch 28959/30000 Training Loss: 0.06831149011850357\n",
      "Epoch 28960/30000 Training Loss: 0.06754090636968613\n",
      "Epoch 28960/30000 Validation Loss: 0.07326895743608475\n",
      "Epoch 28961/30000 Training Loss: 0.07498941570520401\n",
      "Epoch 28962/30000 Training Loss: 0.05249828100204468\n",
      "Epoch 28963/30000 Training Loss: 0.04992792382836342\n",
      "Epoch 28964/30000 Training Loss: 0.0564437173306942\n",
      "Epoch 28965/30000 Training Loss: 0.05878428742289543\n",
      "Epoch 28966/30000 Training Loss: 0.05752294138073921\n",
      "Epoch 28967/30000 Training Loss: 0.07516356557607651\n",
      "Epoch 28968/30000 Training Loss: 0.08903155475854874\n",
      "Epoch 28969/30000 Training Loss: 0.07009857892990112\n",
      "Epoch 28970/30000 Training Loss: 0.08341968804597855\n",
      "Epoch 28970/30000 Validation Loss: 0.08817759901285172\n",
      "Epoch 28971/30000 Training Loss: 0.0641351044178009\n",
      "Epoch 28972/30000 Training Loss: 0.07936102896928787\n",
      "Epoch 28973/30000 Training Loss: 0.07114017754793167\n",
      "Epoch 28974/30000 Training Loss: 0.05902142822742462\n",
      "Epoch 28975/30000 Training Loss: 0.06297656893730164\n",
      "Epoch 28976/30000 Training Loss: 0.05913357064127922\n",
      "Epoch 28977/30000 Training Loss: 0.0766511783003807\n",
      "Epoch 28978/30000 Training Loss: 0.07556653767824173\n",
      "Epoch 28979/30000 Training Loss: 0.05834155157208443\n",
      "Epoch 28980/30000 Training Loss: 0.06848602741956711\n",
      "Epoch 28980/30000 Validation Loss: 0.060673970729112625\n",
      "Epoch 28981/30000 Training Loss: 0.0755220428109169\n",
      "Epoch 28982/30000 Training Loss: 0.07438149303197861\n",
      "Epoch 28983/30000 Training Loss: 0.06642244011163712\n",
      "Epoch 28984/30000 Training Loss: 0.07978557795286179\n",
      "Epoch 28985/30000 Training Loss: 0.07960780709981918\n",
      "Epoch 28986/30000 Training Loss: 0.07823918014764786\n",
      "Epoch 28987/30000 Training Loss: 0.05477474257349968\n",
      "Epoch 28988/30000 Training Loss: 0.07985645532608032\n",
      "Epoch 28989/30000 Training Loss: 0.05485015735030174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28990/30000 Training Loss: 0.07536756992340088\n",
      "Epoch 28990/30000 Validation Loss: 0.08744951337575912\n",
      "Epoch 28991/30000 Training Loss: 0.08840598911046982\n",
      "Epoch 28992/30000 Training Loss: 0.08888688683509827\n",
      "Epoch 28993/30000 Training Loss: 0.0538240484893322\n",
      "Epoch 28994/30000 Training Loss: 0.0789744183421135\n",
      "Epoch 28995/30000 Training Loss: 0.06181344389915466\n",
      "Epoch 28996/30000 Training Loss: 0.06306766718626022\n",
      "Epoch 28997/30000 Training Loss: 0.06236987188458443\n",
      "Epoch 28998/30000 Training Loss: 0.05738407373428345\n",
      "Epoch 28999/30000 Training Loss: 0.06752196699380875\n",
      "Epoch 29000/30000 Training Loss: 0.06765605509281158\n",
      "Epoch 29000/30000 Validation Loss: 0.07421939820051193\n",
      "Epoch 29001/30000 Training Loss: 0.06230620667338371\n",
      "Epoch 29002/30000 Training Loss: 0.07665311545133591\n",
      "Epoch 29003/30000 Training Loss: 0.06850478798151016\n",
      "Epoch 29004/30000 Training Loss: 0.06990393996238708\n",
      "Epoch 29005/30000 Training Loss: 0.06388500332832336\n",
      "Epoch 29006/30000 Training Loss: 0.0617704875767231\n",
      "Epoch 29007/30000 Training Loss: 0.06186933442950249\n",
      "Epoch 29008/30000 Training Loss: 0.07562016695737839\n",
      "Epoch 29009/30000 Training Loss: 0.05884985998272896\n",
      "Epoch 29010/30000 Training Loss: 0.052922654896974564\n",
      "Epoch 29010/30000 Validation Loss: 0.0567769818007946\n",
      "Epoch 29011/30000 Training Loss: 0.07195524126291275\n",
      "Epoch 29012/30000 Training Loss: 0.06532827764749527\n",
      "Epoch 29013/30000 Training Loss: 0.06130022928118706\n",
      "Epoch 29014/30000 Training Loss: 0.05972491577267647\n",
      "Epoch 29015/30000 Training Loss: 0.06560208648443222\n",
      "Epoch 29016/30000 Training Loss: 0.06108454242348671\n",
      "Epoch 29017/30000 Training Loss: 0.05173883214592934\n",
      "Epoch 29018/30000 Training Loss: 0.06886085867881775\n",
      "Epoch 29019/30000 Training Loss: 0.055206310003995895\n",
      "Epoch 29020/30000 Training Loss: 0.088539719581604\n",
      "Epoch 29020/30000 Validation Loss: 0.053845033049583435\n",
      "Epoch 29021/30000 Training Loss: 0.07827185839414597\n",
      "Epoch 29022/30000 Training Loss: 0.06885427981615067\n",
      "Epoch 29023/30000 Training Loss: 0.05664485692977905\n",
      "Epoch 29024/30000 Training Loss: 0.05695013329386711\n",
      "Epoch 29025/30000 Training Loss: 0.07199283689260483\n",
      "Epoch 29026/30000 Training Loss: 0.060317739844322205\n",
      "Epoch 29027/30000 Training Loss: 0.06326805800199509\n",
      "Epoch 29028/30000 Training Loss: 0.06199738383293152\n",
      "Epoch 29029/30000 Training Loss: 0.07703817635774612\n",
      "Epoch 29030/30000 Training Loss: 0.06918945908546448\n",
      "Epoch 29030/30000 Validation Loss: 0.09324246644973755\n",
      "Epoch 29031/30000 Training Loss: 0.06461620330810547\n",
      "Epoch 29032/30000 Training Loss: 0.06430038809776306\n",
      "Epoch 29033/30000 Training Loss: 0.06295391172170639\n",
      "Epoch 29034/30000 Training Loss: 0.049613106995821\n",
      "Epoch 29035/30000 Training Loss: 0.0647515282034874\n",
      "Epoch 29036/30000 Training Loss: 0.0713643953204155\n",
      "Epoch 29037/30000 Training Loss: 0.061228010803461075\n",
      "Epoch 29038/30000 Training Loss: 0.0677855834364891\n",
      "Epoch 29039/30000 Training Loss: 0.06184583902359009\n",
      "Epoch 29040/30000 Training Loss: 0.08660677075386047\n",
      "Epoch 29040/30000 Validation Loss: 0.06249404326081276\n",
      "Epoch 29041/30000 Training Loss: 0.0733487606048584\n",
      "Epoch 29042/30000 Training Loss: 0.07346702367067337\n",
      "Epoch 29043/30000 Training Loss: 0.055613283067941666\n",
      "Epoch 29044/30000 Training Loss: 0.06929104775190353\n",
      "Epoch 29045/30000 Training Loss: 0.05355698987841606\n",
      "Epoch 29046/30000 Training Loss: 0.08803877979516983\n",
      "Epoch 29047/30000 Training Loss: 0.06722437590360641\n",
      "Epoch 29048/30000 Training Loss: 0.09329810738563538\n",
      "Epoch 29049/30000 Training Loss: 0.07335067540407181\n",
      "Epoch 29050/30000 Training Loss: 0.07367067784070969\n",
      "Epoch 29050/30000 Validation Loss: 0.06829512864351273\n",
      "Epoch 29051/30000 Training Loss: 0.07058045268058777\n",
      "Epoch 29052/30000 Training Loss: 0.06887236982584\n",
      "Epoch 29053/30000 Training Loss: 0.07954161614179611\n",
      "Epoch 29054/30000 Training Loss: 0.06540754437446594\n",
      "Epoch 29055/30000 Training Loss: 0.09009769558906555\n",
      "Epoch 29056/30000 Training Loss: 0.0931360051035881\n",
      "Epoch 29057/30000 Training Loss: 0.05983443185687065\n",
      "Epoch 29058/30000 Training Loss: 0.0828414335846901\n",
      "Epoch 29059/30000 Training Loss: 0.06856712698936462\n",
      "Epoch 29060/30000 Training Loss: 0.08067977428436279\n",
      "Epoch 29060/30000 Validation Loss: 0.05468599870800972\n",
      "Epoch 29061/30000 Training Loss: 0.07226994633674622\n",
      "Epoch 29062/30000 Training Loss: 0.07681082934141159\n",
      "Epoch 29063/30000 Training Loss: 0.06949897855520248\n",
      "Epoch 29064/30000 Training Loss: 0.08010261505842209\n",
      "Epoch 29065/30000 Training Loss: 0.05293309688568115\n",
      "Epoch 29066/30000 Training Loss: 0.08743958920240402\n",
      "Epoch 29067/30000 Training Loss: 0.07490412145853043\n",
      "Epoch 29068/30000 Training Loss: 0.0658780112862587\n",
      "Epoch 29069/30000 Training Loss: 0.08364061266183853\n",
      "Epoch 29070/30000 Training Loss: 0.07193697988986969\n",
      "Epoch 29070/30000 Validation Loss: 0.05888482928276062\n",
      "Epoch 29071/30000 Training Loss: 0.05121511220932007\n",
      "Epoch 29072/30000 Training Loss: 0.07375476509332657\n",
      "Epoch 29073/30000 Training Loss: 0.08594394475221634\n",
      "Epoch 29074/30000 Training Loss: 0.09603586792945862\n",
      "Epoch 29075/30000 Training Loss: 0.06898999959230423\n",
      "Epoch 29076/30000 Training Loss: 0.06660308688879013\n",
      "Epoch 29077/30000 Training Loss: 0.08041450381278992\n",
      "Epoch 29078/30000 Training Loss: 0.07692175358533859\n",
      "Epoch 29079/30000 Training Loss: 0.06441695243120193\n",
      "Epoch 29080/30000 Training Loss: 0.08895615488290787\n",
      "Epoch 29080/30000 Validation Loss: 0.05960748717188835\n",
      "Epoch 29081/30000 Training Loss: 0.07134675979614258\n",
      "Epoch 29082/30000 Training Loss: 0.07671917229890823\n",
      "Epoch 29083/30000 Training Loss: 0.08129353076219559\n",
      "Epoch 29084/30000 Training Loss: 0.07766813039779663\n",
      "Epoch 29085/30000 Training Loss: 0.0828976035118103\n",
      "Epoch 29086/30000 Training Loss: 0.06852741539478302\n",
      "Epoch 29087/30000 Training Loss: 0.06680213660001755\n",
      "Epoch 29088/30000 Training Loss: 0.07084158807992935\n",
      "Epoch 29089/30000 Training Loss: 0.06529561430215836\n",
      "Epoch 29090/30000 Training Loss: 0.0702284649014473\n",
      "Epoch 29090/30000 Validation Loss: 0.06128537282347679\n",
      "Epoch 29091/30000 Training Loss: 0.07494046539068222\n",
      "Epoch 29092/30000 Training Loss: 0.06508957594633102\n",
      "Epoch 29093/30000 Training Loss: 0.06426110118627548\n",
      "Epoch 29094/30000 Training Loss: 0.08897507935762405\n",
      "Epoch 29095/30000 Training Loss: 0.07590202987194061\n",
      "Epoch 29096/30000 Training Loss: 0.05862690880894661\n",
      "Epoch 29097/30000 Training Loss: 0.06298612058162689\n",
      "Epoch 29098/30000 Training Loss: 0.05277909338474274\n",
      "Epoch 29099/30000 Training Loss: 0.06108774617314339\n",
      "Epoch 29100/30000 Training Loss: 0.07525493949651718\n",
      "Epoch 29100/30000 Validation Loss: 0.0580897219479084\n",
      "Epoch 29101/30000 Training Loss: 0.05431871488690376\n",
      "Epoch 29102/30000 Training Loss: 0.06642404943704605\n",
      "Epoch 29103/30000 Training Loss: 0.06266789883375168\n",
      "Epoch 29104/30000 Training Loss: 0.06150628253817558\n",
      "Epoch 29105/30000 Training Loss: 0.05826813355088234\n",
      "Epoch 29106/30000 Training Loss: 0.07218680530786514\n",
      "Epoch 29107/30000 Training Loss: 0.07306355237960815\n",
      "Epoch 29108/30000 Training Loss: 0.08482885360717773\n",
      "Epoch 29109/30000 Training Loss: 0.06720437854528427\n",
      "Epoch 29110/30000 Training Loss: 0.06670375913381577\n",
      "Epoch 29110/30000 Validation Loss: 0.06511124223470688\n",
      "Epoch 29111/30000 Training Loss: 0.05307800695300102\n",
      "Epoch 29112/30000 Training Loss: 0.04870356619358063\n",
      "Epoch 29113/30000 Training Loss: 0.07875049114227295\n",
      "Epoch 29114/30000 Training Loss: 0.09597545117139816\n",
      "Epoch 29115/30000 Training Loss: 0.0833183228969574\n",
      "Epoch 29116/30000 Training Loss: 0.07040046900510788\n",
      "Epoch 29117/30000 Training Loss: 0.05765891075134277\n",
      "Epoch 29118/30000 Training Loss: 0.07713642716407776\n",
      "Epoch 29119/30000 Training Loss: 0.07782905548810959\n",
      "Epoch 29120/30000 Training Loss: 0.05329633876681328\n",
      "Epoch 29120/30000 Validation Loss: 0.08391553908586502\n",
      "Epoch 29121/30000 Training Loss: 0.06441769748926163\n",
      "Epoch 29122/30000 Training Loss: 0.07255233079195023\n",
      "Epoch 29123/30000 Training Loss: 0.08425956964492798\n",
      "Epoch 29124/30000 Training Loss: 0.06920397281646729\n",
      "Epoch 29125/30000 Training Loss: 0.07936365157365799\n",
      "Epoch 29126/30000 Training Loss: 0.06960704177618027\n",
      "Epoch 29127/30000 Training Loss: 0.05947967991232872\n",
      "Epoch 29128/30000 Training Loss: 0.0554116815328598\n",
      "Epoch 29129/30000 Training Loss: 0.08139017969369888\n",
      "Epoch 29130/30000 Training Loss: 0.06740135699510574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29130/30000 Validation Loss: 0.08732655644416809\n",
      "Epoch 29131/30000 Training Loss: 0.06256235390901566\n",
      "Epoch 29132/30000 Training Loss: 0.06695463508367538\n",
      "Epoch 29133/30000 Training Loss: 0.07963746786117554\n",
      "Epoch 29134/30000 Training Loss: 0.06387630850076675\n",
      "Epoch 29135/30000 Training Loss: 0.04770197346806526\n",
      "Epoch 29136/30000 Training Loss: 0.07014746218919754\n",
      "Epoch 29137/30000 Training Loss: 0.06949815154075623\n",
      "Epoch 29138/30000 Training Loss: 0.08110194653272629\n",
      "Epoch 29139/30000 Training Loss: 0.06263420730829239\n",
      "Epoch 29140/30000 Training Loss: 0.05634958669543266\n",
      "Epoch 29140/30000 Validation Loss: 0.10471167415380478\n",
      "Epoch 29141/30000 Training Loss: 0.06624838709831238\n",
      "Epoch 29142/30000 Training Loss: 0.04836728051304817\n",
      "Epoch 29143/30000 Training Loss: 0.06266147643327713\n",
      "Epoch 29144/30000 Training Loss: 0.0735279992222786\n",
      "Epoch 29145/30000 Training Loss: 0.06726302951574326\n",
      "Epoch 29146/30000 Training Loss: 0.0686207041144371\n",
      "Epoch 29147/30000 Training Loss: 0.05825607106089592\n",
      "Epoch 29148/30000 Training Loss: 0.06932716816663742\n",
      "Epoch 29149/30000 Training Loss: 0.08188162744045258\n",
      "Epoch 29150/30000 Training Loss: 0.07484886795282364\n",
      "Epoch 29150/30000 Validation Loss: 0.06066081300377846\n",
      "Epoch 29151/30000 Training Loss: 0.07375930994749069\n",
      "Epoch 29152/30000 Training Loss: 0.06397774070501328\n",
      "Epoch 29153/30000 Training Loss: 0.05174805596470833\n",
      "Epoch 29154/30000 Training Loss: 0.07002834975719452\n",
      "Epoch 29155/30000 Training Loss: 0.07149188965559006\n",
      "Epoch 29156/30000 Training Loss: 0.06973956525325775\n",
      "Epoch 29157/30000 Training Loss: 0.08800987154245377\n",
      "Epoch 29158/30000 Training Loss: 0.06900575757026672\n",
      "Epoch 29159/30000 Training Loss: 0.07584946602582932\n",
      "Epoch 29160/30000 Training Loss: 0.0847201943397522\n",
      "Epoch 29160/30000 Validation Loss: 0.06372383236885071\n",
      "Epoch 29161/30000 Training Loss: 0.054140448570251465\n",
      "Epoch 29162/30000 Training Loss: 0.06857260316610336\n",
      "Epoch 29163/30000 Training Loss: 0.06128406524658203\n",
      "Epoch 29164/30000 Training Loss: 0.06812620162963867\n",
      "Epoch 29165/30000 Training Loss: 0.07879546284675598\n",
      "Epoch 29166/30000 Training Loss: 0.07139109075069427\n",
      "Epoch 29167/30000 Training Loss: 0.05491435527801514\n",
      "Epoch 29168/30000 Training Loss: 0.07463225722312927\n",
      "Epoch 29169/30000 Training Loss: 0.06651818752288818\n",
      "Epoch 29170/30000 Training Loss: 0.07069113850593567\n",
      "Epoch 29170/30000 Validation Loss: 0.07296866178512573\n",
      "Epoch 29171/30000 Training Loss: 0.06291016936302185\n",
      "Epoch 29172/30000 Training Loss: 0.07814055681228638\n",
      "Epoch 29173/30000 Training Loss: 0.06828657537698746\n",
      "Epoch 29174/30000 Training Loss: 0.054242346435785294\n",
      "Epoch 29175/30000 Training Loss: 0.060628682374954224\n",
      "Epoch 29176/30000 Training Loss: 0.05836408957839012\n",
      "Epoch 29177/30000 Training Loss: 0.0797121524810791\n",
      "Epoch 29178/30000 Training Loss: 0.05080883204936981\n",
      "Epoch 29179/30000 Training Loss: 0.06235544756054878\n",
      "Epoch 29180/30000 Training Loss: 0.06330152601003647\n",
      "Epoch 29180/30000 Validation Loss: 0.06566517055034637\n",
      "Epoch 29181/30000 Training Loss: 0.06417777389287949\n",
      "Epoch 29182/30000 Training Loss: 0.06494717299938202\n",
      "Epoch 29183/30000 Training Loss: 0.0649913102388382\n",
      "Epoch 29184/30000 Training Loss: 0.06282710283994675\n",
      "Epoch 29185/30000 Training Loss: 0.06276259571313858\n",
      "Epoch 29186/30000 Training Loss: 0.06473466753959656\n",
      "Epoch 29187/30000 Training Loss: 0.07290156930685043\n",
      "Epoch 29188/30000 Training Loss: 0.0601203590631485\n",
      "Epoch 29189/30000 Training Loss: 0.06568419933319092\n",
      "Epoch 29190/30000 Training Loss: 0.06985044479370117\n",
      "Epoch 29190/30000 Validation Loss: 0.0751185193657875\n",
      "Epoch 29191/30000 Training Loss: 0.07810141891241074\n",
      "Epoch 29192/30000 Training Loss: 0.08182795345783234\n",
      "Epoch 29193/30000 Training Loss: 0.07930763810873032\n",
      "Epoch 29194/30000 Training Loss: 0.062201667577028275\n",
      "Epoch 29195/30000 Training Loss: 0.0608416311442852\n",
      "Epoch 29196/30000 Training Loss: 0.07803037017583847\n",
      "Epoch 29197/30000 Training Loss: 0.06986149400472641\n",
      "Epoch 29198/30000 Training Loss: 0.08270261436700821\n",
      "Epoch 29199/30000 Training Loss: 0.06247994303703308\n",
      "Epoch 29200/30000 Training Loss: 0.07288122922182083\n",
      "Epoch 29200/30000 Validation Loss: 0.06504876166582108\n",
      "Epoch 29201/30000 Training Loss: 0.09084832668304443\n",
      "Epoch 29202/30000 Training Loss: 0.07932674884796143\n",
      "Epoch 29203/30000 Training Loss: 0.06837687641382217\n",
      "Epoch 29204/30000 Training Loss: 0.08581292629241943\n",
      "Epoch 29205/30000 Training Loss: 0.06838709115982056\n",
      "Epoch 29206/30000 Training Loss: 0.06352090835571289\n",
      "Epoch 29207/30000 Training Loss: 0.07276707887649536\n",
      "Epoch 29208/30000 Training Loss: 0.06697381287813187\n",
      "Epoch 29209/30000 Training Loss: 0.0752122774720192\n",
      "Epoch 29210/30000 Training Loss: 0.06228256598114967\n",
      "Epoch 29210/30000 Validation Loss: 0.05773451551795006\n",
      "Epoch 29211/30000 Training Loss: 0.06937349587678909\n",
      "Epoch 29212/30000 Training Loss: 0.050346601754426956\n",
      "Epoch 29213/30000 Training Loss: 0.07977816462516785\n",
      "Epoch 29214/30000 Training Loss: 0.08090691268444061\n",
      "Epoch 29215/30000 Training Loss: 0.07722948491573334\n",
      "Epoch 29216/30000 Training Loss: 0.07921650260686874\n",
      "Epoch 29217/30000 Training Loss: 0.07824400067329407\n",
      "Epoch 29218/30000 Training Loss: 0.051659468561410904\n",
      "Epoch 29219/30000 Training Loss: 0.06721916049718857\n",
      "Epoch 29220/30000 Training Loss: 0.0695205107331276\n",
      "Epoch 29220/30000 Validation Loss: 0.06793021410703659\n",
      "Epoch 29221/30000 Training Loss: 0.054897695779800415\n",
      "Epoch 29222/30000 Training Loss: 0.07906431704759598\n",
      "Epoch 29223/30000 Training Loss: 0.06136084720492363\n",
      "Epoch 29224/30000 Training Loss: 0.07613911479711533\n",
      "Epoch 29225/30000 Training Loss: 0.06377550959587097\n",
      "Epoch 29226/30000 Training Loss: 0.06372318416833878\n",
      "Epoch 29227/30000 Training Loss: 0.05216386914253235\n",
      "Epoch 29228/30000 Training Loss: 0.06800951808691025\n",
      "Epoch 29229/30000 Training Loss: 0.07133562117815018\n",
      "Epoch 29230/30000 Training Loss: 0.08052066713571548\n",
      "Epoch 29230/30000 Validation Loss: 0.08044765144586563\n",
      "Epoch 29231/30000 Training Loss: 0.07128816097974777\n",
      "Epoch 29232/30000 Training Loss: 0.06904193013906479\n",
      "Epoch 29233/30000 Training Loss: 0.061658550053834915\n",
      "Epoch 29234/30000 Training Loss: 0.06544864922761917\n",
      "Epoch 29235/30000 Training Loss: 0.056263070553541183\n",
      "Epoch 29236/30000 Training Loss: 0.06543954461812973\n",
      "Epoch 29237/30000 Training Loss: 0.070011205971241\n",
      "Epoch 29238/30000 Training Loss: 0.07199504971504211\n",
      "Epoch 29239/30000 Training Loss: 0.07123586535453796\n",
      "Epoch 29240/30000 Training Loss: 0.0731988176703453\n",
      "Epoch 29240/30000 Validation Loss: 0.07412692904472351\n",
      "Epoch 29241/30000 Training Loss: 0.08052855730056763\n",
      "Epoch 29242/30000 Training Loss: 0.05487373098731041\n",
      "Epoch 29243/30000 Training Loss: 0.06677690148353577\n",
      "Epoch 29244/30000 Training Loss: 0.06315848231315613\n",
      "Epoch 29245/30000 Training Loss: 0.06632277369499207\n",
      "Epoch 29246/30000 Training Loss: 0.07676037400960922\n",
      "Epoch 29247/30000 Training Loss: 0.052879054099321365\n",
      "Epoch 29248/30000 Training Loss: 0.06649664789438248\n",
      "Epoch 29249/30000 Training Loss: 0.04960444197058678\n",
      "Epoch 29250/30000 Training Loss: 0.06889352947473526\n",
      "Epoch 29250/30000 Validation Loss: 0.08316250890493393\n",
      "Epoch 29251/30000 Training Loss: 0.05591273680329323\n",
      "Epoch 29252/30000 Training Loss: 0.07486061006784439\n",
      "Epoch 29253/30000 Training Loss: 0.059712279587984085\n",
      "Epoch 29254/30000 Training Loss: 0.06239354610443115\n",
      "Epoch 29255/30000 Training Loss: 0.07449094206094742\n",
      "Epoch 29256/30000 Training Loss: 0.07822980731725693\n",
      "Epoch 29257/30000 Training Loss: 0.06511428207159042\n",
      "Epoch 29258/30000 Training Loss: 0.06817876547574997\n",
      "Epoch 29259/30000 Training Loss: 0.07190876454114914\n",
      "Epoch 29260/30000 Training Loss: 0.06219290569424629\n",
      "Epoch 29260/30000 Validation Loss: 0.08277870714664459\n",
      "Epoch 29261/30000 Training Loss: 0.0683007463812828\n",
      "Epoch 29262/30000 Training Loss: 0.0796862319111824\n",
      "Epoch 29263/30000 Training Loss: 0.07264932245016098\n",
      "Epoch 29264/30000 Training Loss: 0.07327448576688766\n",
      "Epoch 29265/30000 Training Loss: 0.06326857954263687\n",
      "Epoch 29266/30000 Training Loss: 0.0810672715306282\n",
      "Epoch 29267/30000 Training Loss: 0.05924864485859871\n",
      "Epoch 29268/30000 Training Loss: 0.08053675293922424\n",
      "Epoch 29269/30000 Training Loss: 0.053497422486543655\n",
      "Epoch 29270/30000 Training Loss: 0.06219964846968651\n",
      "Epoch 29270/30000 Validation Loss: 0.056693051010370255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29271/30000 Training Loss: 0.07982631772756577\n",
      "Epoch 29272/30000 Training Loss: 0.06139988824725151\n",
      "Epoch 29273/30000 Training Loss: 0.047579508274793625\n",
      "Epoch 29274/30000 Training Loss: 0.05092291906476021\n",
      "Epoch 29275/30000 Training Loss: 0.07259351015090942\n",
      "Epoch 29276/30000 Training Loss: 0.08099772781133652\n",
      "Epoch 29277/30000 Training Loss: 0.06355264782905579\n",
      "Epoch 29278/30000 Training Loss: 0.06840605288743973\n",
      "Epoch 29279/30000 Training Loss: 0.0793539360165596\n",
      "Epoch 29280/30000 Training Loss: 0.07899221032857895\n",
      "Epoch 29280/30000 Validation Loss: 0.08504758030176163\n",
      "Epoch 29281/30000 Training Loss: 0.08935628086328506\n",
      "Epoch 29282/30000 Training Loss: 0.06827408820390701\n",
      "Epoch 29283/30000 Training Loss: 0.06206849217414856\n",
      "Epoch 29284/30000 Training Loss: 0.055002253502607346\n",
      "Epoch 29285/30000 Training Loss: 0.05809352919459343\n",
      "Epoch 29286/30000 Training Loss: 0.08071673661470413\n",
      "Epoch 29287/30000 Training Loss: 0.06111663579940796\n",
      "Epoch 29288/30000 Training Loss: 0.05916741490364075\n",
      "Epoch 29289/30000 Training Loss: 0.061320558190345764\n",
      "Epoch 29290/30000 Training Loss: 0.07897485047578812\n",
      "Epoch 29290/30000 Validation Loss: 0.07671058177947998\n",
      "Epoch 29291/30000 Training Loss: 0.08542252331972122\n",
      "Epoch 29292/30000 Training Loss: 0.08053212612867355\n",
      "Epoch 29293/30000 Training Loss: 0.08700668811798096\n",
      "Epoch 29294/30000 Training Loss: 0.054518334567546844\n",
      "Epoch 29295/30000 Training Loss: 0.07945620268583298\n",
      "Epoch 29296/30000 Training Loss: 0.06858295202255249\n",
      "Epoch 29297/30000 Training Loss: 0.07500485330820084\n",
      "Epoch 29298/30000 Training Loss: 0.06356090307235718\n",
      "Epoch 29299/30000 Training Loss: 0.09329154342412949\n",
      "Epoch 29300/30000 Training Loss: 0.057288169860839844\n",
      "Epoch 29300/30000 Validation Loss: 0.06489413231611252\n",
      "Epoch 29301/30000 Training Loss: 0.07729217410087585\n",
      "Epoch 29302/30000 Training Loss: 0.06126425042748451\n",
      "Epoch 29303/30000 Training Loss: 0.058294814079999924\n",
      "Epoch 29304/30000 Training Loss: 0.05911864712834358\n",
      "Epoch 29305/30000 Training Loss: 0.07556062936782837\n",
      "Epoch 29306/30000 Training Loss: 0.05759475752711296\n",
      "Epoch 29307/30000 Training Loss: 0.0747218206524849\n",
      "Epoch 29308/30000 Training Loss: 0.06441550701856613\n",
      "Epoch 29309/30000 Training Loss: 0.07723984122276306\n",
      "Epoch 29310/30000 Training Loss: 0.055711086839437485\n",
      "Epoch 29310/30000 Validation Loss: 0.07068728655576706\n",
      "Epoch 29311/30000 Training Loss: 0.07969513535499573\n",
      "Epoch 29312/30000 Training Loss: 0.06454428285360336\n",
      "Epoch 29313/30000 Training Loss: 0.07319460809230804\n",
      "Epoch 29314/30000 Training Loss: 0.06673681735992432\n",
      "Epoch 29315/30000 Training Loss: 0.06574946641921997\n",
      "Epoch 29316/30000 Training Loss: 0.0853460431098938\n",
      "Epoch 29317/30000 Training Loss: 0.06937555223703384\n",
      "Epoch 29318/30000 Training Loss: 0.0750245451927185\n",
      "Epoch 29319/30000 Training Loss: 0.06532847136259079\n",
      "Epoch 29320/30000 Training Loss: 0.0752025917172432\n",
      "Epoch 29320/30000 Validation Loss: 0.06343651562929153\n",
      "Epoch 29321/30000 Training Loss: 0.0747658833861351\n",
      "Epoch 29322/30000 Training Loss: 0.05913351848721504\n",
      "Epoch 29323/30000 Training Loss: 0.053986888378858566\n",
      "Epoch 29324/30000 Training Loss: 0.062327608466148376\n",
      "Epoch 29325/30000 Training Loss: 0.07415857166051865\n",
      "Epoch 29326/30000 Training Loss: 0.07651656121015549\n",
      "Epoch 29327/30000 Training Loss: 0.06550274044275284\n",
      "Epoch 29328/30000 Training Loss: 0.06547702103853226\n",
      "Epoch 29329/30000 Training Loss: 0.048823919147253036\n",
      "Epoch 29330/30000 Training Loss: 0.05850885435938835\n",
      "Epoch 29330/30000 Validation Loss: 0.050559867173433304\n",
      "Epoch 29331/30000 Training Loss: 0.07223602384328842\n",
      "Epoch 29332/30000 Training Loss: 0.04578860476613045\n",
      "Epoch 29333/30000 Training Loss: 0.061621177941560745\n",
      "Epoch 29334/30000 Training Loss: 0.06538218259811401\n",
      "Epoch 29335/30000 Training Loss: 0.06666170805692673\n",
      "Epoch 29336/30000 Training Loss: 0.07821496576070786\n",
      "Epoch 29337/30000 Training Loss: 0.05887271836400032\n",
      "Epoch 29338/30000 Training Loss: 0.06981807202100754\n",
      "Epoch 29339/30000 Training Loss: 0.0580986924469471\n",
      "Epoch 29340/30000 Training Loss: 0.06239088252186775\n",
      "Epoch 29340/30000 Validation Loss: 0.06351891160011292\n",
      "Epoch 29341/30000 Training Loss: 0.08023298531770706\n",
      "Epoch 29342/30000 Training Loss: 0.060495611280202866\n",
      "Epoch 29343/30000 Training Loss: 0.09665104001760483\n",
      "Epoch 29344/30000 Training Loss: 0.07326547056436539\n",
      "Epoch 29345/30000 Training Loss: 0.07656199485063553\n",
      "Epoch 29346/30000 Training Loss: 0.0782756432890892\n",
      "Epoch 29347/30000 Training Loss: 0.06678956001996994\n",
      "Epoch 29348/30000 Training Loss: 0.0552356131374836\n",
      "Epoch 29349/30000 Training Loss: 0.07490066438913345\n",
      "Epoch 29350/30000 Training Loss: 0.06526297330856323\n",
      "Epoch 29350/30000 Validation Loss: 0.053728897124528885\n",
      "Epoch 29351/30000 Training Loss: 0.07836974412202835\n",
      "Epoch 29352/30000 Training Loss: 0.059299420565366745\n",
      "Epoch 29353/30000 Training Loss: 0.05000388249754906\n",
      "Epoch 29354/30000 Training Loss: 0.07097344100475311\n",
      "Epoch 29355/30000 Training Loss: 0.07870059460401535\n",
      "Epoch 29356/30000 Training Loss: 0.06849053502082825\n",
      "Epoch 29357/30000 Training Loss: 0.06378517299890518\n",
      "Epoch 29358/30000 Training Loss: 0.04956740513443947\n",
      "Epoch 29359/30000 Training Loss: 0.10631097108125687\n",
      "Epoch 29360/30000 Training Loss: 0.08203640580177307\n",
      "Epoch 29360/30000 Validation Loss: 0.06059302017092705\n",
      "Epoch 29361/30000 Training Loss: 0.06643550843000412\n",
      "Epoch 29362/30000 Training Loss: 0.060756370425224304\n",
      "Epoch 29363/30000 Training Loss: 0.06977681070566177\n",
      "Epoch 29364/30000 Training Loss: 0.06532561033964157\n",
      "Epoch 29365/30000 Training Loss: 0.08396199345588684\n",
      "Epoch 29366/30000 Training Loss: 0.07701250910758972\n",
      "Epoch 29367/30000 Training Loss: 0.05050460621714592\n",
      "Epoch 29368/30000 Training Loss: 0.0752735286951065\n",
      "Epoch 29369/30000 Training Loss: 0.09070214629173279\n",
      "Epoch 29370/30000 Training Loss: 0.07156158238649368\n",
      "Epoch 29370/30000 Validation Loss: 0.06571030616760254\n",
      "Epoch 29371/30000 Training Loss: 0.059335317462682724\n",
      "Epoch 29372/30000 Training Loss: 0.0692535936832428\n",
      "Epoch 29373/30000 Training Loss: 0.06866684556007385\n",
      "Epoch 29374/30000 Training Loss: 0.06053439900279045\n",
      "Epoch 29375/30000 Training Loss: 0.07281538099050522\n",
      "Epoch 29376/30000 Training Loss: 0.06654081493616104\n",
      "Epoch 29377/30000 Training Loss: 0.06416954845190048\n",
      "Epoch 29378/30000 Training Loss: 0.05550137162208557\n",
      "Epoch 29379/30000 Training Loss: 0.06015528738498688\n",
      "Epoch 29380/30000 Training Loss: 0.06981786340475082\n",
      "Epoch 29380/30000 Validation Loss: 0.06205086410045624\n",
      "Epoch 29381/30000 Training Loss: 0.0554119236767292\n",
      "Epoch 29382/30000 Training Loss: 0.06490323692560196\n",
      "Epoch 29383/30000 Training Loss: 0.05839132145047188\n",
      "Epoch 29384/30000 Training Loss: 0.06920214742422104\n",
      "Epoch 29385/30000 Training Loss: 0.060157518833875656\n",
      "Epoch 29386/30000 Training Loss: 0.04935187101364136\n",
      "Epoch 29387/30000 Training Loss: 0.0671568512916565\n",
      "Epoch 29388/30000 Training Loss: 0.06917691975831985\n",
      "Epoch 29389/30000 Training Loss: 0.08827835321426392\n",
      "Epoch 29390/30000 Training Loss: 0.05558426305651665\n",
      "Epoch 29390/30000 Validation Loss: 0.0643281564116478\n",
      "Epoch 29391/30000 Training Loss: 0.07276273518800735\n",
      "Epoch 29392/30000 Training Loss: 0.08403881639242172\n",
      "Epoch 29393/30000 Training Loss: 0.07330762594938278\n",
      "Epoch 29394/30000 Training Loss: 0.06230616569519043\n",
      "Epoch 29395/30000 Training Loss: 0.05707152560353279\n",
      "Epoch 29396/30000 Training Loss: 0.07575968652963638\n",
      "Epoch 29397/30000 Training Loss: 0.06048613786697388\n",
      "Epoch 29398/30000 Training Loss: 0.07574048638343811\n",
      "Epoch 29399/30000 Training Loss: 0.07895758748054504\n",
      "Epoch 29400/30000 Training Loss: 0.08287867158651352\n",
      "Epoch 29400/30000 Validation Loss: 0.08911781758069992\n",
      "Epoch 29401/30000 Training Loss: 0.06510518491268158\n",
      "Epoch 29402/30000 Training Loss: 0.07565012574195862\n",
      "Epoch 29403/30000 Training Loss: 0.07947743684053421\n",
      "Epoch 29404/30000 Training Loss: 0.05595758557319641\n",
      "Epoch 29405/30000 Training Loss: 0.0740523561835289\n",
      "Epoch 29406/30000 Training Loss: 0.04699103161692619\n",
      "Epoch 29407/30000 Training Loss: 0.07692532241344452\n",
      "Epoch 29408/30000 Training Loss: 0.06753907352685928\n",
      "Epoch 29409/30000 Training Loss: 0.06209242343902588\n",
      "Epoch 29410/30000 Training Loss: 0.08044447749853134\n",
      "Epoch 29410/30000 Validation Loss: 0.06339360028505325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29411/30000 Training Loss: 0.08844638615846634\n",
      "Epoch 29412/30000 Training Loss: 0.07605748623609543\n",
      "Epoch 29413/30000 Training Loss: 0.07031793147325516\n",
      "Epoch 29414/30000 Training Loss: 0.07325994223356247\n",
      "Epoch 29415/30000 Training Loss: 0.0705869197845459\n",
      "Epoch 29416/30000 Training Loss: 0.06544135510921478\n",
      "Epoch 29417/30000 Training Loss: 0.06272672861814499\n",
      "Epoch 29418/30000 Training Loss: 0.08519405871629715\n",
      "Epoch 29419/30000 Training Loss: 0.062292929738759995\n",
      "Epoch 29420/30000 Training Loss: 0.08137050271034241\n",
      "Epoch 29420/30000 Validation Loss: 0.06941104680299759\n",
      "Epoch 29421/30000 Training Loss: 0.053806859999895096\n",
      "Epoch 29422/30000 Training Loss: 0.08170471340417862\n",
      "Epoch 29423/30000 Training Loss: 0.05778460204601288\n",
      "Epoch 29424/30000 Training Loss: 0.05662136152386665\n",
      "Epoch 29425/30000 Training Loss: 0.06619114428758621\n",
      "Epoch 29426/30000 Training Loss: 0.0807393491268158\n",
      "Epoch 29427/30000 Training Loss: 0.07748473435640335\n",
      "Epoch 29428/30000 Training Loss: 0.0692322626709938\n",
      "Epoch 29429/30000 Training Loss: 0.0906776562333107\n",
      "Epoch 29430/30000 Training Loss: 0.06340018659830093\n",
      "Epoch 29430/30000 Validation Loss: 0.06624598056077957\n",
      "Epoch 29431/30000 Training Loss: 0.07658526301383972\n",
      "Epoch 29432/30000 Training Loss: 0.08415641635656357\n",
      "Epoch 29433/30000 Training Loss: 0.05421251058578491\n",
      "Epoch 29434/30000 Training Loss: 0.06501594185829163\n",
      "Epoch 29435/30000 Training Loss: 0.09263211488723755\n",
      "Epoch 29436/30000 Training Loss: 0.06653963774442673\n",
      "Epoch 29437/30000 Training Loss: 0.0745861604809761\n",
      "Epoch 29438/30000 Training Loss: 0.08528408408164978\n",
      "Epoch 29439/30000 Training Loss: 0.05621492862701416\n",
      "Epoch 29440/30000 Training Loss: 0.07379414886236191\n",
      "Epoch 29440/30000 Validation Loss: 0.078299880027771\n",
      "Epoch 29441/30000 Training Loss: 0.0794806182384491\n",
      "Epoch 29442/30000 Training Loss: 0.09306156635284424\n",
      "Epoch 29443/30000 Training Loss: 0.050651226192712784\n",
      "Epoch 29444/30000 Training Loss: 0.06963063031435013\n",
      "Epoch 29445/30000 Training Loss: 0.0762694776058197\n",
      "Epoch 29446/30000 Training Loss: 0.07803992182016373\n",
      "Epoch 29447/30000 Training Loss: 0.08920449763536453\n",
      "Epoch 29448/30000 Training Loss: 0.07708034664392471\n",
      "Epoch 29449/30000 Training Loss: 0.06937123090028763\n",
      "Epoch 29450/30000 Training Loss: 0.05795794352889061\n",
      "Epoch 29450/30000 Validation Loss: 0.05777345970273018\n",
      "Epoch 29451/30000 Training Loss: 0.0467563159763813\n",
      "Epoch 29452/30000 Training Loss: 0.08021347969770432\n",
      "Epoch 29453/30000 Training Loss: 0.08989760279655457\n",
      "Epoch 29454/30000 Training Loss: 0.08891936391592026\n",
      "Epoch 29455/30000 Training Loss: 0.07249181717634201\n",
      "Epoch 29456/30000 Training Loss: 0.06015988066792488\n",
      "Epoch 29457/30000 Training Loss: 0.06883961707353592\n",
      "Epoch 29458/30000 Training Loss: 0.05637456104159355\n",
      "Epoch 29459/30000 Training Loss: 0.07321569323539734\n",
      "Epoch 29460/30000 Training Loss: 0.06197170540690422\n",
      "Epoch 29460/30000 Validation Loss: 0.06026624143123627\n",
      "Epoch 29461/30000 Training Loss: 0.07150569558143616\n",
      "Epoch 29462/30000 Training Loss: 0.05444839596748352\n",
      "Epoch 29463/30000 Training Loss: 0.0624539740383625\n",
      "Epoch 29464/30000 Training Loss: 0.07464699447154999\n",
      "Epoch 29465/30000 Training Loss: 0.07674924284219742\n",
      "Epoch 29466/30000 Training Loss: 0.0685141384601593\n",
      "Epoch 29467/30000 Training Loss: 0.0811111330986023\n",
      "Epoch 29468/30000 Training Loss: 0.081907719373703\n",
      "Epoch 29469/30000 Training Loss: 0.06707771867513657\n",
      "Epoch 29470/30000 Training Loss: 0.06720522791147232\n",
      "Epoch 29470/30000 Validation Loss: 0.08995374292135239\n",
      "Epoch 29471/30000 Training Loss: 0.07406298071146011\n",
      "Epoch 29472/30000 Training Loss: 0.06512251496315002\n",
      "Epoch 29473/30000 Training Loss: 0.05918164178729057\n",
      "Epoch 29474/30000 Training Loss: 0.06247088313102722\n",
      "Epoch 29475/30000 Training Loss: 0.07537446171045303\n",
      "Epoch 29476/30000 Training Loss: 0.053147222846746445\n",
      "Epoch 29477/30000 Training Loss: 0.06668796390295029\n",
      "Epoch 29478/30000 Training Loss: 0.06774510443210602\n",
      "Epoch 29479/30000 Training Loss: 0.06619294732809067\n",
      "Epoch 29480/30000 Training Loss: 0.08365094661712646\n",
      "Epoch 29480/30000 Validation Loss: 0.06689730286598206\n",
      "Epoch 29481/30000 Training Loss: 0.07188212126493454\n",
      "Epoch 29482/30000 Training Loss: 0.07875186949968338\n",
      "Epoch 29483/30000 Training Loss: 0.0635204091668129\n",
      "Epoch 29484/30000 Training Loss: 0.06760594248771667\n",
      "Epoch 29485/30000 Training Loss: 0.08167914301156998\n",
      "Epoch 29486/30000 Training Loss: 0.0905890092253685\n",
      "Epoch 29487/30000 Training Loss: 0.06851498037576675\n",
      "Epoch 29488/30000 Training Loss: 0.07370677590370178\n",
      "Epoch 29489/30000 Training Loss: 0.056864332407712936\n",
      "Epoch 29490/30000 Training Loss: 0.05853806436061859\n",
      "Epoch 29490/30000 Validation Loss: 0.08624476194381714\n",
      "Epoch 29491/30000 Training Loss: 0.06795378774404526\n",
      "Epoch 29492/30000 Training Loss: 0.07917129993438721\n",
      "Epoch 29493/30000 Training Loss: 0.06330141425132751\n",
      "Epoch 29494/30000 Training Loss: 0.07791593670845032\n",
      "Epoch 29495/30000 Training Loss: 0.06906258314847946\n",
      "Epoch 29496/30000 Training Loss: 0.06758841872215271\n",
      "Epoch 29497/30000 Training Loss: 0.08625414222478867\n",
      "Epoch 29498/30000 Training Loss: 0.06068798899650574\n",
      "Epoch 29499/30000 Training Loss: 0.08458580821752548\n",
      "Epoch 29500/30000 Training Loss: 0.0695660188794136\n",
      "Epoch 29500/30000 Validation Loss: 0.09146776795387268\n",
      "Epoch 29501/30000 Training Loss: 0.07633186876773834\n",
      "Epoch 29502/30000 Training Loss: 0.05943853780627251\n",
      "Epoch 29503/30000 Training Loss: 0.07591008394956589\n",
      "Epoch 29504/30000 Training Loss: 0.07456891238689423\n",
      "Epoch 29505/30000 Training Loss: 0.07841894775629044\n",
      "Epoch 29506/30000 Training Loss: 0.06812229752540588\n",
      "Epoch 29507/30000 Training Loss: 0.06972039490938187\n",
      "Epoch 29508/30000 Training Loss: 0.06625533103942871\n",
      "Epoch 29509/30000 Training Loss: 0.07867923378944397\n",
      "Epoch 29510/30000 Training Loss: 0.0646970197558403\n",
      "Epoch 29510/30000 Validation Loss: 0.059909235686063766\n",
      "Epoch 29511/30000 Training Loss: 0.06921202689409256\n",
      "Epoch 29512/30000 Training Loss: 0.06526822596788406\n",
      "Epoch 29513/30000 Training Loss: 0.05837482213973999\n",
      "Epoch 29514/30000 Training Loss: 0.08465111255645752\n",
      "Epoch 29515/30000 Training Loss: 0.06903642416000366\n",
      "Epoch 29516/30000 Training Loss: 0.06804129481315613\n",
      "Epoch 29517/30000 Training Loss: 0.06852242350578308\n",
      "Epoch 29518/30000 Training Loss: 0.0646577998995781\n",
      "Epoch 29519/30000 Training Loss: 0.06272778660058975\n",
      "Epoch 29520/30000 Training Loss: 0.08052629977464676\n",
      "Epoch 29520/30000 Validation Loss: 0.06630238145589828\n",
      "Epoch 29521/30000 Training Loss: 0.058989912271499634\n",
      "Epoch 29522/30000 Training Loss: 0.05987037345767021\n",
      "Epoch 29523/30000 Training Loss: 0.061151135712862015\n",
      "Epoch 29524/30000 Training Loss: 0.0753064677119255\n",
      "Epoch 29525/30000 Training Loss: 0.07013209909200668\n",
      "Epoch 29526/30000 Training Loss: 0.07653756439685822\n",
      "Epoch 29527/30000 Training Loss: 0.06467265635728836\n",
      "Epoch 29528/30000 Training Loss: 0.06418173760175705\n",
      "Epoch 29529/30000 Training Loss: 0.06866153329610825\n",
      "Epoch 29530/30000 Training Loss: 0.0671539306640625\n",
      "Epoch 29530/30000 Validation Loss: 0.07728235423564911\n",
      "Epoch 29531/30000 Training Loss: 0.06051979586482048\n",
      "Epoch 29532/30000 Training Loss: 0.0812557116150856\n",
      "Epoch 29533/30000 Training Loss: 0.06987445801496506\n",
      "Epoch 29534/30000 Training Loss: 0.05799498036503792\n",
      "Epoch 29535/30000 Training Loss: 0.054689209908246994\n",
      "Epoch 29536/30000 Training Loss: 0.09104835242033005\n",
      "Epoch 29537/30000 Training Loss: 0.07079873234033585\n",
      "Epoch 29538/30000 Training Loss: 0.06199139729142189\n",
      "Epoch 29539/30000 Training Loss: 0.0819592997431755\n",
      "Epoch 29540/30000 Training Loss: 0.08271985501050949\n",
      "Epoch 29540/30000 Validation Loss: 0.07263974100351334\n",
      "Epoch 29541/30000 Training Loss: 0.062466442584991455\n",
      "Epoch 29542/30000 Training Loss: 0.07844451069831848\n",
      "Epoch 29543/30000 Training Loss: 0.06134742870926857\n",
      "Epoch 29544/30000 Training Loss: 0.07654684036970139\n",
      "Epoch 29545/30000 Training Loss: 0.06224020943045616\n",
      "Epoch 29546/30000 Training Loss: 0.07364080846309662\n",
      "Epoch 29547/30000 Training Loss: 0.06751076132059097\n",
      "Epoch 29548/30000 Training Loss: 0.06468244642019272\n",
      "Epoch 29549/30000 Training Loss: 0.07852699607610703\n",
      "Epoch 29550/30000 Training Loss: 0.06972069293260574\n",
      "Epoch 29550/30000 Validation Loss: 0.06986240297555923\n",
      "Epoch 29551/30000 Training Loss: 0.07554107159376144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29552/30000 Training Loss: 0.059606846421957016\n",
      "Epoch 29553/30000 Training Loss: 0.08076890558004379\n",
      "Epoch 29554/30000 Training Loss: 0.05545759201049805\n",
      "Epoch 29555/30000 Training Loss: 0.05834592506289482\n",
      "Epoch 29556/30000 Training Loss: 0.05754968523979187\n",
      "Epoch 29557/30000 Training Loss: 0.06709971278905869\n",
      "Epoch 29558/30000 Training Loss: 0.06664503365755081\n",
      "Epoch 29559/30000 Training Loss: 0.06685107201337814\n",
      "Epoch 29560/30000 Training Loss: 0.06898230314254761\n",
      "Epoch 29560/30000 Validation Loss: 0.056900691241025925\n",
      "Epoch 29561/30000 Training Loss: 0.0747489482164383\n",
      "Epoch 29562/30000 Training Loss: 0.08035110682249069\n",
      "Epoch 29563/30000 Training Loss: 0.07297917455434799\n",
      "Epoch 29564/30000 Training Loss: 0.06404594331979752\n",
      "Epoch 29565/30000 Training Loss: 0.07064498215913773\n",
      "Epoch 29566/30000 Training Loss: 0.07631554454565048\n",
      "Epoch 29567/30000 Training Loss: 0.06114937737584114\n",
      "Epoch 29568/30000 Training Loss: 0.0922192633152008\n",
      "Epoch 29569/30000 Training Loss: 0.06392767280340195\n",
      "Epoch 29570/30000 Training Loss: 0.07447091490030289\n",
      "Epoch 29570/30000 Validation Loss: 0.05465904250741005\n",
      "Epoch 29571/30000 Training Loss: 0.0640992522239685\n",
      "Epoch 29572/30000 Training Loss: 0.07465598732233047\n",
      "Epoch 29573/30000 Training Loss: 0.061643194407224655\n",
      "Epoch 29574/30000 Training Loss: 0.0641530379652977\n",
      "Epoch 29575/30000 Training Loss: 0.07668881863355637\n",
      "Epoch 29576/30000 Training Loss: 0.06984689831733704\n",
      "Epoch 29577/30000 Training Loss: 0.05757194757461548\n",
      "Epoch 29578/30000 Training Loss: 0.0948801264166832\n",
      "Epoch 29579/30000 Training Loss: 0.056942641735076904\n",
      "Epoch 29580/30000 Training Loss: 0.07034186273813248\n",
      "Epoch 29580/30000 Validation Loss: 0.07506849616765976\n",
      "Epoch 29581/30000 Training Loss: 0.09825193881988525\n",
      "Epoch 29582/30000 Training Loss: 0.06264446675777435\n",
      "Epoch 29583/30000 Training Loss: 0.07231462746858597\n",
      "Epoch 29584/30000 Training Loss: 0.07934803515672684\n",
      "Epoch 29585/30000 Training Loss: 0.06977164000272751\n",
      "Epoch 29586/30000 Training Loss: 0.06655232608318329\n",
      "Epoch 29587/30000 Training Loss: 0.06646032631397247\n",
      "Epoch 29588/30000 Training Loss: 0.06037614122033119\n",
      "Epoch 29589/30000 Training Loss: 0.08529943227767944\n",
      "Epoch 29590/30000 Training Loss: 0.06186490133404732\n",
      "Epoch 29590/30000 Validation Loss: 0.07014919072389603\n",
      "Epoch 29591/30000 Training Loss: 0.063112773001194\n",
      "Epoch 29592/30000 Training Loss: 0.06443190574645996\n",
      "Epoch 29593/30000 Training Loss: 0.07400068640708923\n",
      "Epoch 29594/30000 Training Loss: 0.07335802167654037\n",
      "Epoch 29595/30000 Training Loss: 0.06660250574350357\n",
      "Epoch 29596/30000 Training Loss: 0.04978443682193756\n",
      "Epoch 29597/30000 Training Loss: 0.06275495141744614\n",
      "Epoch 29598/30000 Training Loss: 0.05832787975668907\n",
      "Epoch 29599/30000 Training Loss: 0.07094167917966843\n",
      "Epoch 29600/30000 Training Loss: 0.08376940339803696\n",
      "Epoch 29600/30000 Validation Loss: 0.09275513887405396\n",
      "Epoch 29601/30000 Training Loss: 0.06674567610025406\n",
      "Epoch 29602/30000 Training Loss: 0.059265490621328354\n",
      "Epoch 29603/30000 Training Loss: 0.0636909231543541\n",
      "Epoch 29604/30000 Training Loss: 0.062305841594934464\n",
      "Epoch 29605/30000 Training Loss: 0.06946834921836853\n",
      "Epoch 29606/30000 Training Loss: 0.0664832592010498\n",
      "Epoch 29607/30000 Training Loss: 0.07262109965085983\n",
      "Epoch 29608/30000 Training Loss: 0.06019739806652069\n",
      "Epoch 29609/30000 Training Loss: 0.05972324684262276\n",
      "Epoch 29610/30000 Training Loss: 0.061573486775159836\n",
      "Epoch 29610/30000 Validation Loss: 0.07357420772314072\n",
      "Epoch 29611/30000 Training Loss: 0.07385832816362381\n",
      "Epoch 29612/30000 Training Loss: 0.05953548476099968\n",
      "Epoch 29613/30000 Training Loss: 0.09577161073684692\n",
      "Epoch 29614/30000 Training Loss: 0.06566748768091202\n",
      "Epoch 29615/30000 Training Loss: 0.0638178214430809\n",
      "Epoch 29616/30000 Training Loss: 0.07922274619340897\n",
      "Epoch 29617/30000 Training Loss: 0.07635215669870377\n",
      "Epoch 29618/30000 Training Loss: 0.09218183904886246\n",
      "Epoch 29619/30000 Training Loss: 0.08054512739181519\n",
      "Epoch 29620/30000 Training Loss: 0.07355231791734695\n",
      "Epoch 29620/30000 Validation Loss: 0.06581167131662369\n",
      "Epoch 29621/30000 Training Loss: 0.04964180663228035\n",
      "Epoch 29622/30000 Training Loss: 0.08664458990097046\n",
      "Epoch 29623/30000 Training Loss: 0.06705842167139053\n",
      "Epoch 29624/30000 Training Loss: 0.07424940168857574\n",
      "Epoch 29625/30000 Training Loss: 0.0657266154885292\n",
      "Epoch 29626/30000 Training Loss: 0.08565432578325272\n",
      "Epoch 29627/30000 Training Loss: 0.0742717832326889\n",
      "Epoch 29628/30000 Training Loss: 0.07166405767202377\n",
      "Epoch 29629/30000 Training Loss: 0.05567200854420662\n",
      "Epoch 29630/30000 Training Loss: 0.061141569167375565\n",
      "Epoch 29630/30000 Validation Loss: 0.06389792263507843\n",
      "Epoch 29631/30000 Training Loss: 0.08328958600759506\n",
      "Epoch 29632/30000 Training Loss: 0.06800133734941483\n",
      "Epoch 29633/30000 Training Loss: 0.08161824196577072\n",
      "Epoch 29634/30000 Training Loss: 0.06777413189411163\n",
      "Epoch 29635/30000 Training Loss: 0.07538197189569473\n",
      "Epoch 29636/30000 Training Loss: 0.049312081187963486\n",
      "Epoch 29637/30000 Training Loss: 0.061343640089035034\n",
      "Epoch 29638/30000 Training Loss: 0.05472439154982567\n",
      "Epoch 29639/30000 Training Loss: 0.06788302212953568\n",
      "Epoch 29640/30000 Training Loss: 0.06563178449869156\n",
      "Epoch 29640/30000 Validation Loss: 0.07121329754590988\n",
      "Epoch 29641/30000 Training Loss: 0.054514482617378235\n",
      "Epoch 29642/30000 Training Loss: 0.0858563780784607\n",
      "Epoch 29643/30000 Training Loss: 0.06148815155029297\n",
      "Epoch 29644/30000 Training Loss: 0.0612429641187191\n",
      "Epoch 29645/30000 Training Loss: 0.0652993842959404\n",
      "Epoch 29646/30000 Training Loss: 0.06569451093673706\n",
      "Epoch 29647/30000 Training Loss: 0.07151991873979568\n",
      "Epoch 29648/30000 Training Loss: 0.08496987819671631\n",
      "Epoch 29649/30000 Training Loss: 0.07101302593946457\n",
      "Epoch 29650/30000 Training Loss: 0.06714465469121933\n",
      "Epoch 29650/30000 Validation Loss: 0.06570664048194885\n",
      "Epoch 29651/30000 Training Loss: 0.058571815490722656\n",
      "Epoch 29652/30000 Training Loss: 0.0898083969950676\n",
      "Epoch 29653/30000 Training Loss: 0.06447311490774155\n",
      "Epoch 29654/30000 Training Loss: 0.04741093888878822\n",
      "Epoch 29655/30000 Training Loss: 0.08022883534431458\n",
      "Epoch 29656/30000 Training Loss: 0.0570586733520031\n",
      "Epoch 29657/30000 Training Loss: 0.06682171672582626\n",
      "Epoch 29658/30000 Training Loss: 0.050616923719644547\n",
      "Epoch 29659/30000 Training Loss: 0.0702667310833931\n",
      "Epoch 29660/30000 Training Loss: 0.05874304100871086\n",
      "Epoch 29660/30000 Validation Loss: 0.05087864398956299\n",
      "Epoch 29661/30000 Training Loss: 0.06833247095346451\n",
      "Epoch 29662/30000 Training Loss: 0.06606542319059372\n",
      "Epoch 29663/30000 Training Loss: 0.06937825679779053\n",
      "Epoch 29664/30000 Training Loss: 0.06873572617769241\n",
      "Epoch 29665/30000 Training Loss: 0.07346908003091812\n",
      "Epoch 29666/30000 Training Loss: 0.0701138973236084\n",
      "Epoch 29667/30000 Training Loss: 0.07334239035844803\n",
      "Epoch 29668/30000 Training Loss: 0.0646933913230896\n",
      "Epoch 29669/30000 Training Loss: 0.06618348509073257\n",
      "Epoch 29670/30000 Training Loss: 0.08184654265642166\n",
      "Epoch 29670/30000 Validation Loss: 0.06250561028718948\n",
      "Epoch 29671/30000 Training Loss: 0.06426110863685608\n",
      "Epoch 29672/30000 Training Loss: 0.08411648869514465\n",
      "Epoch 29673/30000 Training Loss: 0.07553528249263763\n",
      "Epoch 29674/30000 Training Loss: 0.07896962016820908\n",
      "Epoch 29675/30000 Training Loss: 0.06689981371164322\n",
      "Epoch 29676/30000 Training Loss: 0.07095395773649216\n",
      "Epoch 29677/30000 Training Loss: 0.05394122004508972\n",
      "Epoch 29678/30000 Training Loss: 0.05906818434596062\n",
      "Epoch 29679/30000 Training Loss: 0.07424694299697876\n",
      "Epoch 29680/30000 Training Loss: 0.06803634017705917\n",
      "Epoch 29680/30000 Validation Loss: 0.06336610019207001\n",
      "Epoch 29681/30000 Training Loss: 0.08427760750055313\n",
      "Epoch 29682/30000 Training Loss: 0.07053577154874802\n",
      "Epoch 29683/30000 Training Loss: 0.06931998580694199\n",
      "Epoch 29684/30000 Training Loss: 0.07525578886270523\n",
      "Epoch 29685/30000 Training Loss: 0.05760626122355461\n",
      "Epoch 29686/30000 Training Loss: 0.056700557470321655\n",
      "Epoch 29687/30000 Training Loss: 0.096193827688694\n",
      "Epoch 29688/30000 Training Loss: 0.06858757883310318\n",
      "Epoch 29689/30000 Training Loss: 0.08702531456947327\n",
      "Epoch 29690/30000 Training Loss: 0.06834422796964645\n",
      "Epoch 29690/30000 Validation Loss: 0.07772526890039444\n",
      "Epoch 29691/30000 Training Loss: 0.06695892661809921\n",
      "Epoch 29692/30000 Training Loss: 0.08351512998342514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29693/30000 Training Loss: 0.05481107905507088\n",
      "Epoch 29694/30000 Training Loss: 0.07411172986030579\n",
      "Epoch 29695/30000 Training Loss: 0.06775624305009842\n",
      "Epoch 29696/30000 Training Loss: 0.05792106315493584\n",
      "Epoch 29697/30000 Training Loss: 0.0665382444858551\n",
      "Epoch 29698/30000 Training Loss: 0.059959590435028076\n",
      "Epoch 29699/30000 Training Loss: 0.08537384122610092\n",
      "Epoch 29700/30000 Training Loss: 0.0601501502096653\n",
      "Epoch 29700/30000 Validation Loss: 0.05788363888859749\n",
      "Epoch 29701/30000 Training Loss: 0.06944718956947327\n",
      "Epoch 29702/30000 Training Loss: 0.06732375174760818\n",
      "Epoch 29703/30000 Training Loss: 0.07497125118970871\n",
      "Epoch 29704/30000 Training Loss: 0.06444492191076279\n",
      "Epoch 29705/30000 Training Loss: 0.07348491996526718\n",
      "Epoch 29706/30000 Training Loss: 0.08090364933013916\n",
      "Epoch 29707/30000 Training Loss: 0.06191723421216011\n",
      "Epoch 29708/30000 Training Loss: 0.07836636155843735\n",
      "Epoch 29709/30000 Training Loss: 0.07100367546081543\n",
      "Epoch 29710/30000 Training Loss: 0.06851973384618759\n",
      "Epoch 29710/30000 Validation Loss: 0.08168908208608627\n",
      "Epoch 29711/30000 Training Loss: 0.0747264102101326\n",
      "Epoch 29712/30000 Training Loss: 0.06721486896276474\n",
      "Epoch 29713/30000 Training Loss: 0.06926466524600983\n",
      "Epoch 29714/30000 Training Loss: 0.08317092806100845\n",
      "Epoch 29715/30000 Training Loss: 0.08746979385614395\n",
      "Epoch 29716/30000 Training Loss: 0.060702186077833176\n",
      "Epoch 29717/30000 Training Loss: 0.0767304003238678\n",
      "Epoch 29718/30000 Training Loss: 0.07644640654325485\n",
      "Epoch 29719/30000 Training Loss: 0.07396602630615234\n",
      "Epoch 29720/30000 Training Loss: 0.04946492239832878\n",
      "Epoch 29720/30000 Validation Loss: 0.07465014606714249\n",
      "Epoch 29721/30000 Training Loss: 0.07043483108282089\n",
      "Epoch 29722/30000 Training Loss: 0.055411938577890396\n",
      "Epoch 29723/30000 Training Loss: 0.06753382831811905\n",
      "Epoch 29724/30000 Training Loss: 0.07363266497850418\n",
      "Epoch 29725/30000 Training Loss: 0.06962721794843674\n",
      "Epoch 29726/30000 Training Loss: 0.0717753916978836\n",
      "Epoch 29727/30000 Training Loss: 0.062095049768686295\n",
      "Epoch 29728/30000 Training Loss: 0.07513562589883804\n",
      "Epoch 29729/30000 Training Loss: 0.0642710030078888\n",
      "Epoch 29730/30000 Training Loss: 0.06507998704910278\n",
      "Epoch 29730/30000 Validation Loss: 0.07028676569461823\n",
      "Epoch 29731/30000 Training Loss: 0.08039834350347519\n",
      "Epoch 29732/30000 Training Loss: 0.05332187935709953\n",
      "Epoch 29733/30000 Training Loss: 0.08092954009771347\n",
      "Epoch 29734/30000 Training Loss: 0.061019573360681534\n",
      "Epoch 29735/30000 Training Loss: 0.06679844111204147\n",
      "Epoch 29736/30000 Training Loss: 0.07591287046670914\n",
      "Epoch 29737/30000 Training Loss: 0.07570508867502213\n",
      "Epoch 29738/30000 Training Loss: 0.07658279687166214\n",
      "Epoch 29739/30000 Training Loss: 0.06861737370491028\n",
      "Epoch 29740/30000 Training Loss: 0.051332324743270874\n",
      "Epoch 29740/30000 Validation Loss: 0.05944615602493286\n",
      "Epoch 29741/30000 Training Loss: 0.06554687023162842\n",
      "Epoch 29742/30000 Training Loss: 0.08139993995428085\n",
      "Epoch 29743/30000 Training Loss: 0.09855369478464127\n",
      "Epoch 29744/30000 Training Loss: 0.06270184367895126\n",
      "Epoch 29745/30000 Training Loss: 0.082207091152668\n",
      "Epoch 29746/30000 Training Loss: 0.08009541779756546\n",
      "Epoch 29747/30000 Training Loss: 0.06179875507950783\n",
      "Epoch 29748/30000 Training Loss: 0.05325062572956085\n",
      "Epoch 29749/30000 Training Loss: 0.0643402710556984\n",
      "Epoch 29750/30000 Training Loss: 0.0754576101899147\n",
      "Epoch 29750/30000 Validation Loss: 0.07345451414585114\n",
      "Epoch 29751/30000 Training Loss: 0.06062250956892967\n",
      "Epoch 29752/30000 Training Loss: 0.08139823377132416\n",
      "Epoch 29753/30000 Training Loss: 0.06554719060659409\n",
      "Epoch 29754/30000 Training Loss: 0.06243910267949104\n",
      "Epoch 29755/30000 Training Loss: 0.06444757431745529\n",
      "Epoch 29756/30000 Training Loss: 0.060789212584495544\n",
      "Epoch 29757/30000 Training Loss: 0.059111520648002625\n",
      "Epoch 29758/30000 Training Loss: 0.0660928338766098\n",
      "Epoch 29759/30000 Training Loss: 0.0809226855635643\n",
      "Epoch 29760/30000 Training Loss: 0.06109154596924782\n",
      "Epoch 29760/30000 Validation Loss: 0.08099871873855591\n",
      "Epoch 29761/30000 Training Loss: 0.08690614253282547\n",
      "Epoch 29762/30000 Training Loss: 0.058852747082710266\n",
      "Epoch 29763/30000 Training Loss: 0.0699705258011818\n",
      "Epoch 29764/30000 Training Loss: 0.07937686890363693\n",
      "Epoch 29765/30000 Training Loss: 0.07320196181535721\n",
      "Epoch 29766/30000 Training Loss: 0.061780836433172226\n",
      "Epoch 29767/30000 Training Loss: 0.08400104194879532\n",
      "Epoch 29768/30000 Training Loss: 0.06458816677331924\n",
      "Epoch 29769/30000 Training Loss: 0.06724882125854492\n",
      "Epoch 29770/30000 Training Loss: 0.07276276499032974\n",
      "Epoch 29770/30000 Validation Loss: 0.07025980204343796\n",
      "Epoch 29771/30000 Training Loss: 0.06993170827627182\n",
      "Epoch 29772/30000 Training Loss: 0.07049978524446487\n",
      "Epoch 29773/30000 Training Loss: 0.05302448198199272\n",
      "Epoch 29774/30000 Training Loss: 0.06877458095550537\n",
      "Epoch 29775/30000 Training Loss: 0.08379670232534409\n",
      "Epoch 29776/30000 Training Loss: 0.0937165841460228\n",
      "Epoch 29777/30000 Training Loss: 0.06534584611654282\n",
      "Epoch 29778/30000 Training Loss: 0.07997287064790726\n",
      "Epoch 29779/30000 Training Loss: 0.04985425993800163\n",
      "Epoch 29780/30000 Training Loss: 0.058893006294965744\n",
      "Epoch 29780/30000 Validation Loss: 0.09202537685632706\n",
      "Epoch 29781/30000 Training Loss: 0.06270328164100647\n",
      "Epoch 29782/30000 Training Loss: 0.06533657014369965\n",
      "Epoch 29783/30000 Training Loss: 0.057207126170396805\n",
      "Epoch 29784/30000 Training Loss: 0.07564727216959\n",
      "Epoch 29785/30000 Training Loss: 0.07238296419382095\n",
      "Epoch 29786/30000 Training Loss: 0.06485793739557266\n",
      "Epoch 29787/30000 Training Loss: 0.06592492014169693\n",
      "Epoch 29788/30000 Training Loss: 0.0690016821026802\n",
      "Epoch 29789/30000 Training Loss: 0.0711233839392662\n",
      "Epoch 29790/30000 Training Loss: 0.07430487126111984\n",
      "Epoch 29790/30000 Validation Loss: 0.06271352618932724\n",
      "Epoch 29791/30000 Training Loss: 0.05160417780280113\n",
      "Epoch 29792/30000 Training Loss: 0.06411435455083847\n",
      "Epoch 29793/30000 Training Loss: 0.07263501733541489\n",
      "Epoch 29794/30000 Training Loss: 0.06208242103457451\n",
      "Epoch 29795/30000 Training Loss: 0.060272786766290665\n",
      "Epoch 29796/30000 Training Loss: 0.07005567103624344\n",
      "Epoch 29797/30000 Training Loss: 0.05452755093574524\n",
      "Epoch 29798/30000 Training Loss: 0.05743539705872536\n",
      "Epoch 29799/30000 Training Loss: 0.06375978142023087\n",
      "Epoch 29800/30000 Training Loss: 0.07240121811628342\n",
      "Epoch 29800/30000 Validation Loss: 0.09346240758895874\n",
      "Epoch 29801/30000 Training Loss: 0.06876642256975174\n",
      "Epoch 29802/30000 Training Loss: 0.07697930932044983\n",
      "Epoch 29803/30000 Training Loss: 0.0821758583188057\n",
      "Epoch 29804/30000 Training Loss: 0.07804638147354126\n",
      "Epoch 29805/30000 Training Loss: 0.08222121745347977\n",
      "Epoch 29806/30000 Training Loss: 0.07044798880815506\n",
      "Epoch 29807/30000 Training Loss: 0.057780783623456955\n",
      "Epoch 29808/30000 Training Loss: 0.0577579140663147\n",
      "Epoch 29809/30000 Training Loss: 0.06818286329507828\n",
      "Epoch 29810/30000 Training Loss: 0.0749005451798439\n",
      "Epoch 29810/30000 Validation Loss: 0.06415858119726181\n",
      "Epoch 29811/30000 Training Loss: 0.06213222071528435\n",
      "Epoch 29812/30000 Training Loss: 0.05889468267560005\n",
      "Epoch 29813/30000 Training Loss: 0.06496302038431168\n",
      "Epoch 29814/30000 Training Loss: 0.088869608938694\n",
      "Epoch 29815/30000 Training Loss: 0.06287946552038193\n",
      "Epoch 29816/30000 Training Loss: 0.08154668658971786\n",
      "Epoch 29817/30000 Training Loss: 0.06333529204130173\n",
      "Epoch 29818/30000 Training Loss: 0.05682972073554993\n",
      "Epoch 29819/30000 Training Loss: 0.06293335556983948\n",
      "Epoch 29820/30000 Training Loss: 0.07714802771806717\n",
      "Epoch 29820/30000 Validation Loss: 0.06752505153417587\n",
      "Epoch 29821/30000 Training Loss: 0.0953511968255043\n",
      "Epoch 29822/30000 Training Loss: 0.07412301003932953\n",
      "Epoch 29823/30000 Training Loss: 0.0690685510635376\n",
      "Epoch 29824/30000 Training Loss: 0.07704395055770874\n",
      "Epoch 29825/30000 Training Loss: 0.05188388004899025\n",
      "Epoch 29826/30000 Training Loss: 0.07902229577302933\n",
      "Epoch 29827/30000 Training Loss: 0.05745222046971321\n",
      "Epoch 29828/30000 Training Loss: 0.06129484251141548\n",
      "Epoch 29829/30000 Training Loss: 0.07496526837348938\n",
      "Epoch 29830/30000 Training Loss: 0.07269128412008286\n",
      "Epoch 29830/30000 Validation Loss: 0.08126912266016006\n",
      "Epoch 29831/30000 Training Loss: 0.062860868871212\n",
      "Epoch 29832/30000 Training Loss: 0.05248069390654564\n",
      "Epoch 29833/30000 Training Loss: 0.07799151539802551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29834/30000 Training Loss: 0.07264196127653122\n",
      "Epoch 29835/30000 Training Loss: 0.06639423966407776\n",
      "Epoch 29836/30000 Training Loss: 0.06093986704945564\n",
      "Epoch 29837/30000 Training Loss: 0.06779066473245621\n",
      "Epoch 29838/30000 Training Loss: 0.06769602000713348\n",
      "Epoch 29839/30000 Training Loss: 0.06103569641709328\n",
      "Epoch 29840/30000 Training Loss: 0.07499050348997116\n",
      "Epoch 29840/30000 Validation Loss: 0.07180424779653549\n",
      "Epoch 29841/30000 Training Loss: 0.09301304817199707\n",
      "Epoch 29842/30000 Training Loss: 0.06276995688676834\n",
      "Epoch 29843/30000 Training Loss: 0.06812797486782074\n",
      "Epoch 29844/30000 Training Loss: 0.05937481299042702\n",
      "Epoch 29845/30000 Training Loss: 0.07628358155488968\n",
      "Epoch 29846/30000 Training Loss: 0.07363473623991013\n",
      "Epoch 29847/30000 Training Loss: 0.07992953807115555\n",
      "Epoch 29848/30000 Training Loss: 0.06606351584196091\n",
      "Epoch 29849/30000 Training Loss: 0.07115941494703293\n",
      "Epoch 29850/30000 Training Loss: 0.07700146734714508\n",
      "Epoch 29850/30000 Validation Loss: 0.07688958942890167\n",
      "Epoch 29851/30000 Training Loss: 0.06233949586749077\n",
      "Epoch 29852/30000 Training Loss: 0.06230170652270317\n",
      "Epoch 29853/30000 Training Loss: 0.07032414525747299\n",
      "Epoch 29854/30000 Training Loss: 0.06997644901275635\n",
      "Epoch 29855/30000 Training Loss: 0.07619572430849075\n",
      "Epoch 29856/30000 Training Loss: 0.061696331948041916\n",
      "Epoch 29857/30000 Training Loss: 0.05429567024111748\n",
      "Epoch 29858/30000 Training Loss: 0.055654462426900864\n",
      "Epoch 29859/30000 Training Loss: 0.0873972475528717\n",
      "Epoch 29860/30000 Training Loss: 0.05822989344596863\n",
      "Epoch 29860/30000 Validation Loss: 0.08890103548765182\n",
      "Epoch 29861/30000 Training Loss: 0.07531416416168213\n",
      "Epoch 29862/30000 Training Loss: 0.052351921796798706\n",
      "Epoch 29863/30000 Training Loss: 0.06962475925683975\n",
      "Epoch 29864/30000 Training Loss: 0.06461433321237564\n",
      "Epoch 29865/30000 Training Loss: 0.06633459776639938\n",
      "Epoch 29866/30000 Training Loss: 0.07744813710451126\n",
      "Epoch 29867/30000 Training Loss: 0.07845137268304825\n",
      "Epoch 29868/30000 Training Loss: 0.07476230710744858\n",
      "Epoch 29869/30000 Training Loss: 0.06279352307319641\n",
      "Epoch 29870/30000 Training Loss: 0.0710996761918068\n",
      "Epoch 29870/30000 Validation Loss: 0.05395805835723877\n",
      "Epoch 29871/30000 Training Loss: 0.0657508596777916\n",
      "Epoch 29872/30000 Training Loss: 0.09096894413232803\n",
      "Epoch 29873/30000 Training Loss: 0.06607014685869217\n",
      "Epoch 29874/30000 Training Loss: 0.09651395678520203\n",
      "Epoch 29875/30000 Training Loss: 0.06356224417686462\n",
      "Epoch 29876/30000 Training Loss: 0.059994470328092575\n",
      "Epoch 29877/30000 Training Loss: 0.08343882113695145\n",
      "Epoch 29878/30000 Training Loss: 0.07419383525848389\n",
      "Epoch 29879/30000 Training Loss: 0.06020145118236542\n",
      "Epoch 29880/30000 Training Loss: 0.06151694059371948\n",
      "Epoch 29880/30000 Validation Loss: 0.07745394855737686\n",
      "Epoch 29881/30000 Training Loss: 0.0623881071805954\n",
      "Epoch 29882/30000 Training Loss: 0.07445583492517471\n",
      "Epoch 29883/30000 Training Loss: 0.06124007701873779\n",
      "Epoch 29884/30000 Training Loss: 0.05508410930633545\n",
      "Epoch 29885/30000 Training Loss: 0.07632254809141159\n",
      "Epoch 29886/30000 Training Loss: 0.059804219752550125\n",
      "Epoch 29887/30000 Training Loss: 0.06856992095708847\n",
      "Epoch 29888/30000 Training Loss: 0.05721136927604675\n",
      "Epoch 29889/30000 Training Loss: 0.07351899147033691\n",
      "Epoch 29890/30000 Training Loss: 0.07512974739074707\n",
      "Epoch 29890/30000 Validation Loss: 0.058715373277664185\n",
      "Epoch 29891/30000 Training Loss: 0.0450456477701664\n",
      "Epoch 29892/30000 Training Loss: 0.07411634922027588\n",
      "Epoch 29893/30000 Training Loss: 0.06878910213708878\n",
      "Epoch 29894/30000 Training Loss: 0.07228245586156845\n",
      "Epoch 29895/30000 Training Loss: 0.07815597206354141\n",
      "Epoch 29896/30000 Training Loss: 0.05394149199128151\n",
      "Epoch 29897/30000 Training Loss: 0.059250909835100174\n",
      "Epoch 29898/30000 Training Loss: 0.06538067758083344\n",
      "Epoch 29899/30000 Training Loss: 0.06396038085222244\n",
      "Epoch 29900/30000 Training Loss: 0.06074465811252594\n",
      "Epoch 29900/30000 Validation Loss: 0.08352446556091309\n",
      "Epoch 29901/30000 Training Loss: 0.06841348856687546\n",
      "Epoch 29902/30000 Training Loss: 0.08097001910209656\n",
      "Epoch 29903/30000 Training Loss: 0.07224845141172409\n",
      "Epoch 29904/30000 Training Loss: 0.06932402402162552\n",
      "Epoch 29905/30000 Training Loss: 0.06444715708494186\n",
      "Epoch 29906/30000 Training Loss: 0.05759739875793457\n",
      "Epoch 29907/30000 Training Loss: 0.06153889372944832\n",
      "Epoch 29908/30000 Training Loss: 0.06588233262300491\n",
      "Epoch 29909/30000 Training Loss: 0.06516719609498978\n",
      "Epoch 29910/30000 Training Loss: 0.06707173585891724\n",
      "Epoch 29910/30000 Validation Loss: 0.06912629306316376\n",
      "Epoch 29911/30000 Training Loss: 0.07595378905534744\n",
      "Epoch 29912/30000 Training Loss: 0.06945904344320297\n",
      "Epoch 29913/30000 Training Loss: 0.06370655447244644\n",
      "Epoch 29914/30000 Training Loss: 0.052105654031038284\n",
      "Epoch 29915/30000 Training Loss: 0.08041789382696152\n",
      "Epoch 29916/30000 Training Loss: 0.07291704416275024\n",
      "Epoch 29917/30000 Training Loss: 0.06597890704870224\n",
      "Epoch 29918/30000 Training Loss: 0.08860913664102554\n",
      "Epoch 29919/30000 Training Loss: 0.07321705669164658\n",
      "Epoch 29920/30000 Training Loss: 0.05882878974080086\n",
      "Epoch 29920/30000 Validation Loss: 0.0723237618803978\n",
      "Epoch 29921/30000 Training Loss: 0.09956518560647964\n",
      "Epoch 29922/30000 Training Loss: 0.06698638945817947\n",
      "Epoch 29923/30000 Training Loss: 0.07682017236948013\n",
      "Epoch 29924/30000 Training Loss: 0.06621959060430527\n",
      "Epoch 29925/30000 Training Loss: 0.06252036988735199\n",
      "Epoch 29926/30000 Training Loss: 0.06742378324270248\n",
      "Epoch 29927/30000 Training Loss: 0.07606825977563858\n",
      "Epoch 29928/30000 Training Loss: 0.059989530593156815\n",
      "Epoch 29929/30000 Training Loss: 0.07426174730062485\n",
      "Epoch 29930/30000 Training Loss: 0.07855281233787537\n",
      "Epoch 29930/30000 Validation Loss: 0.06460444629192352\n",
      "Epoch 29931/30000 Training Loss: 0.07260743528604507\n",
      "Epoch 29932/30000 Training Loss: 0.054378900676965714\n",
      "Epoch 29933/30000 Training Loss: 0.07394751906394958\n",
      "Epoch 29934/30000 Training Loss: 0.0616401731967926\n",
      "Epoch 29935/30000 Training Loss: 0.06331955641508102\n",
      "Epoch 29936/30000 Training Loss: 0.07088792324066162\n",
      "Epoch 29937/30000 Training Loss: 0.06824168562889099\n",
      "Epoch 29938/30000 Training Loss: 0.07611668854951859\n",
      "Epoch 29939/30000 Training Loss: 0.0693359449505806\n",
      "Epoch 29940/30000 Training Loss: 0.0794127956032753\n",
      "Epoch 29940/30000 Validation Loss: 0.06558740139007568\n",
      "Epoch 29941/30000 Training Loss: 0.06452181190252304\n",
      "Epoch 29942/30000 Training Loss: 0.08151379227638245\n",
      "Epoch 29943/30000 Training Loss: 0.06617753952741623\n",
      "Epoch 29944/30000 Training Loss: 0.07943282276391983\n",
      "Epoch 29945/30000 Training Loss: 0.07162321358919144\n",
      "Epoch 29946/30000 Training Loss: 0.061480533331632614\n",
      "Epoch 29947/30000 Training Loss: 0.08985882252454758\n",
      "Epoch 29948/30000 Training Loss: 0.0796564444899559\n",
      "Epoch 29949/30000 Training Loss: 0.08963470906019211\n",
      "Epoch 29950/30000 Training Loss: 0.06413237750530243\n",
      "Epoch 29950/30000 Validation Loss: 0.07535690814256668\n",
      "Epoch 29951/30000 Training Loss: 0.06136747822165489\n",
      "Epoch 29952/30000 Training Loss: 0.07350722700357437\n",
      "Epoch 29953/30000 Training Loss: 0.07176902145147324\n",
      "Epoch 29954/30000 Training Loss: 0.0610521174967289\n",
      "Epoch 29955/30000 Training Loss: 0.05703182891011238\n",
      "Epoch 29956/30000 Training Loss: 0.050960689783096313\n",
      "Epoch 29957/30000 Training Loss: 0.07797867804765701\n",
      "Epoch 29958/30000 Training Loss: 0.0671888217329979\n",
      "Epoch 29959/30000 Training Loss: 0.05753278732299805\n",
      "Epoch 29960/30000 Training Loss: 0.08160418272018433\n",
      "Epoch 29960/30000 Validation Loss: 0.06295444816350937\n",
      "Epoch 29961/30000 Training Loss: 0.07887089252471924\n",
      "Epoch 29962/30000 Training Loss: 0.07925178110599518\n",
      "Epoch 29963/30000 Training Loss: 0.07746488600969315\n",
      "Epoch 29964/30000 Training Loss: 0.06565374881029129\n",
      "Epoch 29965/30000 Training Loss: 0.08883268386125565\n",
      "Epoch 29966/30000 Training Loss: 0.0753396525979042\n",
      "Epoch 29967/30000 Training Loss: 0.054121434688568115\n",
      "Epoch 29968/30000 Training Loss: 0.05516928434371948\n",
      "Epoch 29969/30000 Training Loss: 0.06575149297714233\n",
      "Epoch 29970/30000 Training Loss: 0.06351236253976822\n",
      "Epoch 29970/30000 Validation Loss: 0.07685071229934692\n",
      "Epoch 29971/30000 Training Loss: 0.06465379148721695\n",
      "Epoch 29972/30000 Training Loss: 0.06956585496664047\n",
      "Epoch 29973/30000 Training Loss: 0.055942002683877945\n",
      "Epoch 29974/30000 Training Loss: 0.07648826390504837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29975/30000 Training Loss: 0.07692773640155792\n",
      "Epoch 29976/30000 Training Loss: 0.08890943974256516\n",
      "Epoch 29977/30000 Training Loss: 0.06429499387741089\n",
      "Epoch 29978/30000 Training Loss: 0.07122113555669785\n",
      "Epoch 29979/30000 Training Loss: 0.06363467127084732\n",
      "Epoch 29980/30000 Training Loss: 0.07840152829885483\n",
      "Epoch 29980/30000 Validation Loss: 0.06785768270492554\n",
      "Epoch 29981/30000 Training Loss: 0.07128793001174927\n",
      "Epoch 29982/30000 Training Loss: 0.06643220782279968\n",
      "Epoch 29983/30000 Training Loss: 0.09301088005304337\n",
      "Epoch 29984/30000 Training Loss: 0.07620496302843094\n",
      "Epoch 29985/30000 Training Loss: 0.0679331123828888\n",
      "Epoch 29986/30000 Training Loss: 0.06313331425189972\n",
      "Epoch 29987/30000 Training Loss: 0.07106880098581314\n",
      "Epoch 29988/30000 Training Loss: 0.09112560749053955\n",
      "Epoch 29989/30000 Training Loss: 0.06787580996751785\n",
      "Epoch 29990/30000 Training Loss: 0.06422186642885208\n",
      "Epoch 29990/30000 Validation Loss: 0.07221904397010803\n",
      "Epoch 29991/30000 Training Loss: 0.0642237663269043\n",
      "Epoch 29992/30000 Training Loss: 0.060237620025873184\n",
      "Epoch 29993/30000 Training Loss: 0.08896686881780624\n",
      "Epoch 29994/30000 Training Loss: 0.05988536402583122\n",
      "Epoch 29995/30000 Training Loss: 0.06306879967451096\n",
      "Epoch 29996/30000 Training Loss: 0.07261649519205093\n",
      "Epoch 29997/30000 Training Loss: 0.060715217143297195\n",
      "Epoch 29998/30000 Training Loss: 0.0639599934220314\n",
      "Epoch 29999/30000 Training Loss: 0.06736467778682709\n",
      "Epoch 30000/30000 Validation Loss: 0.05971850827336311\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "model.to(dist_util.dev())\n",
    "\n",
    "train_loss, val_loss = TrainLoop(\n",
    "                            model=model,\n",
    "                            diffusion=diffusion,\n",
    "                            data=data,\n",
    "                            batch_size=batch_size,\n",
    "                            microbatch=microbatch,\n",
    "                            lr=lr,\n",
    "                            ema_rate=ema_rate,\n",
    "                            schedule_sampler=schedule_sampler,\n",
    "                            weight_decay=weight_decay,\n",
    "                            epochs=epochs,\n",
    "                            eval_data=val,\n",
    "                            eval_interval=eval_interval,\n",
    "                            warm_up_steps=500,\n",
    "                            use_llrd=True,\n",
    "                            llrd_rate=0.9\n",
    "                        ).run_loop()\n",
    "\n",
    "dt = datetime.now().strftime(\"%m%d\")\n",
    "pickle.dump(model, open(f\"models/{dt}/final_model_df{diffusion_steps}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fa9c7-84c8-48bd-b368-318df3f0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_fp = f'models/0308/model_best_epoch_23930_min_val_loss_0.02459999918937683.pkl'\n",
    "with open(best_model_fp, 'rb') as handle:\n",
    "    best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_2000 = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 2000),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3e232-af43-44f3-8acb-ec31f1b0bd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " [\"my lord, i claim your gift, my due by promise, for which your honour and your faith is pawn'd, the earldom of hereford and the moveables the which you promised i should possess.\", 'i am a bastard too, i love bastards i am a bastard begot, bastard instructed, bastard in mind, bastard in valour, in every thing illegitimate. one bear will not bite another,'] ['stanley, look to your wife, if she convey letters to richmond, you shall answer it.', \"and wherefore should one bastard? take heed, the quarrel's most ominous to us if the son of a whore fight for a whore, he tempts judgment farewell, bastard.\"]\n",
      "RAM used: 6068.92 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 4\n",
      "})\n",
      "RAM used: 6068.92 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3275193a4c7848eda9cb48300596c997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 4\n",
      "})\n",
      "### tokenized_datasets...example [2, 105, 203, 10, 31, 2385, 135, 1820, 10, 105, 2280, 193, 1823, 10, 115, 260, 135, 487, 85, 135, 676, 121, 3876, 9, 26, 10, 78, 11495, 94, 3787, 85, 78, 11788, 78, 260, 89, 3808, 31, 305, 1872, 12, 3]\n",
      "RAM used: 6068.98 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d6bd0f42a74b5790baef918784be48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 6068.98 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087a4cb31dc749288617b30f8426e72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 4\n",
      "}) padded dataset\n",
      "RAM used: 6069.00 MB\n",
      "RAM used: 6069.00 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcefe70093e4569bd3e65a6d7e16d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion_2000, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128, \n",
    "                                                           show_intermediate_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] my lord, i claim your gift, my due by promise, for which your honour and your faith is pawn'd, the earldom of hereford and the moveables the which you promised i should possess. [SEP] [SEP]\",\n",
       " '[CLS] i am a bastard too, i love bastards i am a bastard begot, bastard instructed, bastard in mind, bastard in valour, in every thing illegitimate. one bear will not bite another, [SEP] [SEP]',\n",
       " \"[CLS] my lord,'tis the pondering of life's meaning that doth occupy my thoughts most gravely. [SEP] [SEP]\",\n",
       " '[CLS] call her forth to me [SEP] [SEP]']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d9184761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] so your to be him king in? [SEP]',\n",
       " '[CLS] by sorry well to be to to, or to the good. [SEP]',\n",
       " '[CLS] for your one to a christian. [SEP]',\n",
       " '[CLS] how hark! [SEP]']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81787a-cde2-40ea-ae5b-2d3c66e84c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
