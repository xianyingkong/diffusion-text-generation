{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a01d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 16:32:59.076686: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 16:32:59.076753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 16:32:59.078465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 16:32:59.089465: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 16:33:00.639125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from model_arch.run_train import *\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from model_arch.train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from model_arch.tokenizer import load_tokenizer, load_model_emb\n",
    "from model_arch.sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=30\n",
    "val_batch_size=30\n",
    "microbatch=10\n",
    "epochs=30_000\n",
    "eval_interval=10\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=1000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=10275679\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data_dir='data'\n",
    "data_player_dir='data/with_player'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer('shakespeare', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30268, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30268"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['this is the very false gallop of verses why do you infect yourself with them?', 'and fair men call for grace. aaron will have his soul black like his face.'] ['peace, you dull fool! i found them on a tree.', 'o, here i lift this one hand up to heaven, and bow this feeble ruin to the earth if any power pities wretched tears, to that i call! what, wilt thou kneel with me? do, then, dear heart,']\n",
      "RAM used: 718.34 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 32531\n",
      "})\n",
      "RAM used: 737.28 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34549fbc7fb143a0b42d64bac865088a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/32531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 32531\n",
      "})\n",
      "### tokenized_datasets...example [2, 138, 121, 78, 476, 880, 9871, 94, 5833, 329, 144, 89, 2960, 955, 131, 266, 22, 3]\n",
      "RAM used: 776.12 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb381dc161c44008a77641efee9e3f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/32531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 811.85 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e8c66a4f7046faaafe71db9a8686ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/32531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 32531\n",
      "}) padded dataset\n",
      "RAM used: 877.51 MB\n",
      "RAM used: 877.51 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"kind o' thing than a fool and yet i would not be thee, nuncle, thou hast pared thy wit o' both sides, and left nothing i' the middle here comes one o' the parings.\", 'yes, by saint patrick, but there is, horatio, and much offence too. touching this vision here, it is an honest ghost, that let me tell you for your desire to know what is between us,'] [\"how now, daughter! what makes that frontlet on? methinks you are too much of late i' the frown.\", \"o'ermaster 't as you may. and now, good friends, as you are friends, scholars and soldiers, give me one poor request.\"]\n",
      "RAM used: 840.76 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 8123\n",
      "})\n",
      "RAM used: 840.76 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f45c97921c47dcbe7739a3df0aeb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/8123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 8123\n",
      "})\n",
      "### tokenized_datasets...example [2, 714, 37, 9, 749, 243, 23, 550, 85, 317, 31, 241, 125, 100, 208, 10, 6206, 10, 136, 469, 23187, 180, 646, 37, 9, 579, 3437, 10, 85, 1044, 549, 31, 9, 78, 5789, 237, 571, 295, 37, 9, 78, 14784, 12, 3]\n",
      "RAM used: 846.08 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20626be96f22465d9a9c44600de99baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/8123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 859.91 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36677861f3ab4f92af0aedc4bb84e9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/8123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 8123\n",
      "}) padded dataset\n",
      "RAM used: 874.72 MB\n",
      "RAM used: 874.72 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=val_batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight, # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192508"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1883bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# from transformers import BertConfig\n",
    "\n",
    "# config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# for layer in model.input_transformers.layer[-1:]:\n",
    "#     for module in layer.modules():\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()\n",
    "#         elif isinstance(module, nn.Embedding):\n",
    "#             module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "#             if module.padding_idx is not None:\n",
    "#                 module.weight.data[module.padding_idx].zero_()\n",
    "#         elif isinstance(module, nn.LayerNorm):\n",
    "#             module.bias.data.zero_()\n",
    "#             module.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 0.0001\n",
      "name: lm_head.bias, lr: 0.0001\n",
      "name: time_embed.0.weight, lr: 0.0001\n",
      "name: time_embed.0.bias, lr: 0.0001\n",
      "name: time_embed.2.weight, lr: 0.0001\n",
      "name: time_embed.2.bias, lr: 0.0001\n",
      "name: input_up_proj.0.weight, lr: 0.0001\n",
      "name: input_up_proj.0.bias, lr: 0.0001\n",
      "name: input_up_proj.2.weight, lr: 0.0001\n",
      "name: input_up_proj.2.bias, lr: 0.0001\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 0.00011111111111111112\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 0.0001234567901234568\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 0.00013717421124828533\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 0.00015241579027587258\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 0.00016935087808430286\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 0.00018816764231589206\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 0.00020907515812876895\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 0.00023230573125418772\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.0002581174791713197\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00028679719907924407\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.0003186635545324934\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.00035407061614721485\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.00035407061614721485\n",
      "name: position_embeddings.weight, lr: 0.0003934117957191276\n",
      "name: LayerNorm.weight, lr: 0.0003934117957191276\n",
      "name: LayerNorm.bias, lr: 0.0003934117957191276\n",
      "name: output_down_proj.0.weight, lr: 0.0003934117957191276\n",
      "name: output_down_proj.0.bias, lr: 0.0003934117957191276\n",
      "name: output_down_proj.2.weight, lr: 0.0003934117957191276\n",
      "name: output_down_proj.2.bias, lr: 0.0003934117957191276\n",
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n",
      "Epoch 0/30000 Training Loss: 0.9905311465263367\n",
      "Epoch 0/30000 Validation Loss: 0.9975106120109558\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.9975106120109558<=============\n",
      "Epoch 1/30000 Training Loss: 0.9798384308815002\n",
      "Epoch 2/30000 Training Loss: 0.9917788505554199\n",
      "Epoch 3/30000 Training Loss: 0.9897893071174622\n",
      "Epoch 4/30000 Training Loss: 0.9831383228302002\n",
      "Epoch 5/30000 Training Loss: 0.9645359516143799\n",
      "Epoch 6/30000 Training Loss: 0.9492614269256592\n",
      "Epoch 7/30000 Training Loss: 0.932619571685791\n",
      "Epoch 8/30000 Training Loss: 0.9134400486946106\n",
      "Epoch 9/30000 Training Loss: 0.9072536826133728\n",
      "Epoch 10/30000 Training Loss: 0.9031212329864502\n",
      "Epoch 10/30000 Validation Loss: 0.8546254634857178\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.8546254634857178<=============\n",
      "Epoch 11/30000 Training Loss: 0.8615453243255615\n",
      "Epoch 12/30000 Training Loss: 0.8615904450416565\n",
      "Epoch 13/30000 Training Loss: 0.8038876056671143\n",
      "Epoch 14/30000 Training Loss: 0.7840509414672852\n",
      "Epoch 15/30000 Training Loss: 0.7746918797492981\n",
      "Epoch 16/30000 Training Loss: 0.7387543320655823\n",
      "Epoch 17/30000 Training Loss: 0.7567963004112244\n",
      "Epoch 18/30000 Training Loss: 0.764523446559906\n",
      "Epoch 19/30000 Training Loss: 0.7323889136314392\n",
      "Epoch 20/30000 Training Loss: 0.6841219067573547\n",
      "Epoch 20/30000 Validation Loss: 0.7038933634757996\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.7038933634757996<=============\n",
      "Epoch 21/30000 Training Loss: 0.7189437747001648\n",
      "Epoch 22/30000 Training Loss: 0.6995804905891418\n",
      "Epoch 23/30000 Training Loss: 0.6559562683105469\n",
      "Epoch 24/30000 Training Loss: 0.5889356136322021\n",
      "Epoch 25/30000 Training Loss: 0.6313129663467407\n",
      "Epoch 26/30000 Training Loss: 0.6473814845085144\n",
      "Epoch 27/30000 Training Loss: 0.6192075610160828\n",
      "Epoch 28/30000 Training Loss: 0.6333627104759216\n",
      "Epoch 29/30000 Training Loss: 0.6035627126693726\n",
      "Epoch 30/30000 Training Loss: 0.6327913403511047\n",
      "Epoch 30/30000 Validation Loss: 0.6528738737106323\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.6528738737106323<=============\n",
      "Epoch 31/30000 Training Loss: 0.5980174541473389\n",
      "Epoch 32/30000 Training Loss: 0.6172124743461609\n",
      "Epoch 33/30000 Training Loss: 0.5812382698059082\n",
      "Epoch 34/30000 Training Loss: 0.5978503227233887\n",
      "Epoch 35/30000 Training Loss: 0.5312039256095886\n",
      "Epoch 36/30000 Training Loss: 0.6185150146484375\n",
      "Epoch 37/30000 Training Loss: 0.6098126769065857\n",
      "Epoch 38/30000 Training Loss: 0.5761844515800476\n",
      "Epoch 39/30000 Training Loss: 0.6879581809043884\n",
      "Epoch 40/30000 Training Loss: 0.6767714619636536\n",
      "Epoch 40/30000 Validation Loss: 0.6081438660621643\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.6081438660621643<=============\n",
      "Epoch 41/30000 Training Loss: 0.5967269539833069\n",
      "Epoch 42/30000 Training Loss: 0.6164881587028503\n",
      "Epoch 43/30000 Training Loss: 0.6047589778900146\n",
      "Epoch 44/30000 Training Loss: 0.6248247623443604\n",
      "Epoch 45/30000 Training Loss: 0.63299959897995\n",
      "Epoch 46/30000 Training Loss: 0.6403571963310242\n",
      "Epoch 47/30000 Training Loss: 0.7238773703575134\n",
      "Epoch 48/30000 Training Loss: 0.6765623092651367\n",
      "Epoch 49/30000 Training Loss: 0.5938349366188049\n",
      "Epoch 50/30000 Training Loss: 0.6550996899604797\n",
      "Epoch 50/30000 Validation Loss: 0.6163735389709473\n",
      "Epoch 51/30000 Training Loss: 0.6467140316963196\n",
      "Epoch 52/30000 Training Loss: 0.6131730675697327\n",
      "Epoch 53/30000 Training Loss: 0.5730627179145813\n",
      "Epoch 54/30000 Training Loss: 0.6151946187019348\n",
      "Epoch 55/30000 Training Loss: 0.5752306580543518\n",
      "Epoch 56/30000 Training Loss: 0.6098802089691162\n",
      "Epoch 57/30000 Training Loss: 0.5718526840209961\n",
      "Epoch 58/30000 Training Loss: 0.5160233974456787\n",
      "Epoch 59/30000 Training Loss: 0.5129356980323792\n",
      "Epoch 60/30000 Training Loss: 0.5362064838409424\n",
      "Epoch 60/30000 Validation Loss: 0.5096642374992371\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.5096642374992371<=============\n",
      "Epoch 61/30000 Training Loss: 0.5370368361473083\n",
      "Epoch 62/30000 Training Loss: 0.5114956498146057\n",
      "Epoch 63/30000 Training Loss: 0.4909108579158783\n",
      "Epoch 64/30000 Training Loss: 0.5590401291847229\n",
      "Epoch 65/30000 Training Loss: 0.4849201738834381\n",
      "Epoch 66/30000 Training Loss: 0.5088014602661133\n",
      "Epoch 67/30000 Training Loss: 0.4463130533695221\n",
      "Epoch 68/30000 Training Loss: 0.4742487370967865\n",
      "Epoch 69/30000 Training Loss: 0.4471571743488312\n",
      "Epoch 70/30000 Training Loss: 0.430086225271225\n",
      "Epoch 70/30000 Validation Loss: 0.49714645743370056\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.49714645743370056<=============\n",
      "Epoch 71/30000 Training Loss: 0.4992181956768036\n",
      "Epoch 72/30000 Training Loss: 0.43266066908836365\n",
      "Epoch 73/30000 Training Loss: 0.41079041361808777\n",
      "Epoch 74/30000 Training Loss: 0.5144224762916565\n",
      "Epoch 75/30000 Training Loss: 0.4570469856262207\n",
      "Epoch 76/30000 Training Loss: 0.4231744110584259\n",
      "Epoch 77/30000 Training Loss: 0.45866501331329346\n",
      "Epoch 78/30000 Training Loss: 0.4420419931411743\n",
      "Epoch 79/30000 Training Loss: 0.41728806495666504\n",
      "Epoch 80/30000 Training Loss: 0.4381929636001587\n",
      "Epoch 80/30000 Validation Loss: 0.37491747736930847\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.37491747736930847<=============\n",
      "Epoch 81/30000 Training Loss: 0.48785170912742615\n",
      "Epoch 82/30000 Training Loss: 0.4275471866130829\n",
      "Epoch 83/30000 Training Loss: 0.4289083480834961\n",
      "Epoch 84/30000 Training Loss: 0.3521108627319336\n",
      "Epoch 85/30000 Training Loss: 0.42656993865966797\n",
      "Epoch 86/30000 Training Loss: 0.38742661476135254\n",
      "Epoch 87/30000 Training Loss: 0.3887554109096527\n",
      "Epoch 88/30000 Training Loss: 0.45550093054771423\n",
      "Epoch 89/30000 Training Loss: 0.3919711112976074\n",
      "Epoch 90/30000 Training Loss: 0.3940955698490143\n",
      "Epoch 90/30000 Validation Loss: 0.4469326436519623\n",
      "Epoch 91/30000 Training Loss: 0.4261641502380371\n",
      "Epoch 92/30000 Training Loss: 0.40298351645469666\n",
      "Epoch 93/30000 Training Loss: 0.4118800163269043\n",
      "Epoch 94/30000 Training Loss: 0.42758575081825256\n",
      "Epoch 95/30000 Training Loss: 0.4071086645126343\n",
      "Epoch 96/30000 Training Loss: 0.4617733061313629\n",
      "Epoch 97/30000 Training Loss: 0.4752085208892822\n",
      "Epoch 98/30000 Training Loss: 0.4606187045574188\n",
      "Epoch 99/30000 Training Loss: 0.3956810235977173\n",
      "Epoch 100/30000 Training Loss: 0.3967258036136627\n",
      "Epoch 100/30000 Validation Loss: 0.41982805728912354\n",
      "Epoch 101/30000 Training Loss: 0.4102764129638672\n",
      "Epoch 102/30000 Training Loss: 0.41834354400634766\n",
      "Epoch 103/30000 Training Loss: 0.37389492988586426\n",
      "Epoch 104/30000 Training Loss: 0.4210801124572754\n",
      "Epoch 105/30000 Training Loss: 0.44920405745506287\n",
      "Epoch 106/30000 Training Loss: 0.3916459381580353\n",
      "Epoch 107/30000 Training Loss: 0.4035470485687256\n",
      "Epoch 108/30000 Training Loss: 0.41580307483673096\n",
      "Epoch 109/30000 Training Loss: 0.3518298864364624\n",
      "Epoch 110/30000 Training Loss: 0.375949889421463\n",
      "Epoch 110/30000 Validation Loss: 0.4146556854248047\n",
      "Epoch 111/30000 Training Loss: 0.3784027099609375\n",
      "Epoch 112/30000 Training Loss: 0.39102303981781006\n",
      "Epoch 113/30000 Training Loss: 0.35148656368255615\n",
      "Epoch 114/30000 Training Loss: 0.3267459571361542\n",
      "Epoch 115/30000 Training Loss: 0.3778816759586334\n",
      "Epoch 116/30000 Training Loss: 0.36533161997795105\n",
      "Epoch 117/30000 Training Loss: 0.3549672067165375\n",
      "Epoch 118/30000 Training Loss: 0.3978765904903412\n",
      "Epoch 119/30000 Training Loss: 0.3847767412662506\n",
      "Epoch 120/30000 Training Loss: 0.3461538553237915\n",
      "Epoch 120/30000 Validation Loss: 0.3326159417629242\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.3326159417629242<=============\n",
      "Epoch 121/30000 Training Loss: 0.3777974545955658\n",
      "Epoch 122/30000 Training Loss: 0.36947765946388245\n",
      "Epoch 123/30000 Training Loss: 0.40561679005622864\n",
      "Epoch 124/30000 Training Loss: 0.3409661054611206\n",
      "Epoch 125/30000 Training Loss: 0.37416645884513855\n",
      "Epoch 126/30000 Training Loss: 0.3127821981906891\n",
      "Epoch 127/30000 Training Loss: 0.37192901968955994\n",
      "Epoch 128/30000 Training Loss: 0.33266791701316833\n",
      "Epoch 129/30000 Training Loss: 0.3693755865097046\n",
      "Epoch 130/30000 Training Loss: 0.33122459053993225\n",
      "Epoch 130/30000 Validation Loss: 0.39604732394218445\n",
      "Epoch 131/30000 Training Loss: 0.42224860191345215\n",
      "Epoch 132/30000 Training Loss: 0.3869903087615967\n",
      "Epoch 133/30000 Training Loss: 0.39515557885169983\n",
      "Epoch 134/30000 Training Loss: 0.3721056878566742\n",
      "Epoch 135/30000 Training Loss: 0.37828460335731506\n",
      "Epoch 136/30000 Training Loss: 0.3603827655315399\n",
      "Epoch 137/30000 Training Loss: 0.414743572473526\n",
      "Epoch 138/30000 Training Loss: 0.3542845547199249\n",
      "Epoch 139/30000 Training Loss: 0.34124669432640076\n",
      "Epoch 140/30000 Training Loss: 0.4219132363796234\n",
      "Epoch 140/30000 Validation Loss: 0.36152663826942444\n",
      "Epoch 141/30000 Training Loss: 0.3693338632583618\n",
      "Epoch 142/30000 Training Loss: 0.3614595830440521\n",
      "Epoch 143/30000 Training Loss: 0.3407081067562103\n",
      "Epoch 144/30000 Training Loss: 0.3038907051086426\n",
      "Epoch 145/30000 Training Loss: 0.3559558093547821\n",
      "Epoch 146/30000 Training Loss: 0.30793848633766174\n",
      "Epoch 147/30000 Training Loss: 0.3463341295719147\n",
      "Epoch 148/30000 Training Loss: 0.3319171369075775\n",
      "Epoch 149/30000 Training Loss: 0.29978880286216736\n",
      "Epoch 150/30000 Training Loss: 0.33640027046203613\n",
      "Epoch 150/30000 Validation Loss: 0.3356912136077881\n",
      "Epoch 151/30000 Training Loss: 0.340077668428421\n",
      "Epoch 152/30000 Training Loss: 0.32712116837501526\n",
      "Epoch 153/30000 Training Loss: 0.3564532697200775\n",
      "Epoch 154/30000 Training Loss: 0.33574172854423523\n",
      "Epoch 155/30000 Training Loss: 0.2981373965740204\n",
      "Epoch 156/30000 Training Loss: 0.32096609473228455\n",
      "Epoch 157/30000 Training Loss: 0.3183231055736542\n",
      "Epoch 158/30000 Training Loss: 0.37178799510002136\n",
      "Epoch 159/30000 Training Loss: 0.3825364112854004\n",
      "Epoch 160/30000 Training Loss: 0.33277785778045654\n",
      "Epoch 160/30000 Validation Loss: 0.29763466119766235\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.29763466119766235<=============\n",
      "Epoch 161/30000 Training Loss: 0.28796640038490295\n",
      "Epoch 162/30000 Training Loss: 0.3420890271663666\n",
      "Epoch 163/30000 Training Loss: 0.30482837557792664\n",
      "Epoch 164/30000 Training Loss: 0.2973780035972595\n",
      "Epoch 165/30000 Training Loss: 0.29499515891075134\n",
      "Epoch 166/30000 Training Loss: 0.2876169681549072\n",
      "Epoch 167/30000 Training Loss: 0.28119680285453796\n",
      "Epoch 168/30000 Training Loss: 0.2682599723339081\n",
      "Epoch 169/30000 Training Loss: 0.3223609924316406\n",
      "Epoch 170/30000 Training Loss: 0.3029179573059082\n",
      "Epoch 170/30000 Validation Loss: 0.2978563606739044\n",
      "Epoch 171/30000 Training Loss: 0.29196181893348694\n",
      "Epoch 172/30000 Training Loss: 0.31806623935699463\n",
      "Epoch 173/30000 Training Loss: 0.28919023275375366\n",
      "Epoch 174/30000 Training Loss: 0.2876453399658203\n",
      "Epoch 175/30000 Training Loss: 0.3050305247306824\n",
      "Epoch 176/30000 Training Loss: 0.278967946767807\n",
      "Epoch 177/30000 Training Loss: 0.27582886815071106\n",
      "Epoch 178/30000 Training Loss: 0.24581606686115265\n",
      "Epoch 179/30000 Training Loss: 0.26358896493911743\n",
      "Epoch 180/30000 Training Loss: 0.3017001152038574\n",
      "Epoch 180/30000 Validation Loss: 0.3093710243701935\n",
      "Epoch 181/30000 Training Loss: 0.2930316627025604\n",
      "Epoch 182/30000 Training Loss: 0.259754478931427\n",
      "Epoch 183/30000 Training Loss: 0.29594218730926514\n",
      "Epoch 184/30000 Training Loss: 0.2950374186038971\n",
      "Epoch 185/30000 Training Loss: 0.2955068349838257\n",
      "Epoch 186/30000 Training Loss: 0.3100607693195343\n",
      "Epoch 187/30000 Training Loss: 0.29885241389274597\n",
      "Epoch 188/30000 Training Loss: 0.2564101219177246\n",
      "Epoch 189/30000 Training Loss: 0.2457486391067505\n",
      "Epoch 190/30000 Training Loss: 0.24036937952041626\n",
      "Epoch 190/30000 Validation Loss: 0.27539655566215515\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.27539655566215515<=============\n",
      "Epoch 191/30000 Training Loss: 0.2816213071346283\n",
      "Epoch 192/30000 Training Loss: 0.3096510171890259\n",
      "Epoch 193/30000 Training Loss: 0.2660054862499237\n",
      "Epoch 194/30000 Training Loss: 0.27858641743659973\n",
      "Epoch 195/30000 Training Loss: 0.28756505250930786\n",
      "Epoch 196/30000 Training Loss: 0.2703218162059784\n",
      "Epoch 197/30000 Training Loss: 0.2623591721057892\n",
      "Epoch 198/30000 Training Loss: 0.23315829038619995\n",
      "Epoch 199/30000 Training Loss: 0.2873704135417938\n",
      "Epoch 200/30000 Training Loss: 0.2526223957538605\n",
      "Epoch 200/30000 Validation Loss: 0.23358769714832306\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.23358769714832306<=============\n",
      "Epoch 201/30000 Training Loss: 0.24879513680934906\n",
      "Epoch 202/30000 Training Loss: 0.2767089307308197\n",
      "Epoch 203/30000 Training Loss: 0.21750353276729584\n",
      "Epoch 204/30000 Training Loss: 0.27156659960746765\n",
      "Epoch 205/30000 Training Loss: 0.2548840641975403\n",
      "Epoch 206/30000 Training Loss: 0.26111850142478943\n",
      "Epoch 207/30000 Training Loss: 0.23664464056491852\n",
      "Epoch 208/30000 Training Loss: 0.28637897968292236\n",
      "Epoch 209/30000 Training Loss: 0.23257696628570557\n",
      "Epoch 210/30000 Training Loss: 0.27322590351104736\n",
      "Epoch 210/30000 Validation Loss: 0.2792690098285675\n",
      "Epoch 211/30000 Training Loss: 0.2474604994058609\n",
      "Epoch 212/30000 Training Loss: 0.2420061081647873\n",
      "Epoch 213/30000 Training Loss: 0.2629775404930115\n",
      "Epoch 214/30000 Training Loss: 0.22606785595417023\n",
      "Epoch 215/30000 Training Loss: 0.2695036232471466\n",
      "Epoch 216/30000 Training Loss: 0.23098038136959076\n",
      "Epoch 217/30000 Training Loss: 0.2467298060655594\n",
      "Epoch 218/30000 Training Loss: 0.27027374505996704\n",
      "Epoch 219/30000 Training Loss: 0.24707262217998505\n",
      "Epoch 220/30000 Training Loss: 0.2427726536989212\n",
      "Epoch 220/30000 Validation Loss: 0.291393905878067\n",
      "Epoch 221/30000 Training Loss: 0.22853262722492218\n",
      "Epoch 222/30000 Training Loss: 0.2564408779144287\n",
      "Epoch 223/30000 Training Loss: 0.2640465795993805\n",
      "Epoch 224/30000 Training Loss: 0.243691086769104\n",
      "Epoch 225/30000 Training Loss: 0.26709362864494324\n",
      "Epoch 226/30000 Training Loss: 0.22858624160289764\n",
      "Epoch 227/30000 Training Loss: 0.2410786896944046\n",
      "Epoch 228/30000 Training Loss: 0.2631347179412842\n",
      "Epoch 229/30000 Training Loss: 0.22322414815425873\n",
      "Epoch 230/30000 Training Loss: 0.2334694117307663\n",
      "Epoch 230/30000 Validation Loss: 0.25389376282691956\n",
      "Epoch 231/30000 Training Loss: 0.248138427734375\n",
      "Epoch 232/30000 Training Loss: 0.25080248713493347\n",
      "Epoch 233/30000 Training Loss: 0.2325146645307541\n",
      "Epoch 234/30000 Training Loss: 0.2271483987569809\n",
      "Epoch 235/30000 Training Loss: 0.19147761166095734\n",
      "Epoch 236/30000 Training Loss: 0.20582444965839386\n",
      "Epoch 237/30000 Training Loss: 0.24251361191272736\n",
      "Epoch 238/30000 Training Loss: 0.2678745985031128\n",
      "Epoch 239/30000 Training Loss: 0.23034214973449707\n",
      "Epoch 240/30000 Training Loss: 0.23291657865047455\n",
      "Epoch 240/30000 Validation Loss: 0.22761547565460205\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.22761547565460205<=============\n",
      "Epoch 241/30000 Training Loss: 0.22435657680034637\n",
      "Epoch 242/30000 Training Loss: 0.26090526580810547\n",
      "Epoch 243/30000 Training Loss: 0.22913758456707\n",
      "Epoch 244/30000 Training Loss: 0.20517174899578094\n",
      "Epoch 245/30000 Training Loss: 0.2279832810163498\n",
      "Epoch 246/30000 Training Loss: 0.2578200101852417\n",
      "Epoch 247/30000 Training Loss: 0.21874552965164185\n",
      "Epoch 248/30000 Training Loss: 0.2034064084291458\n",
      "Epoch 249/30000 Training Loss: 0.2387913018465042\n",
      "Epoch 250/30000 Training Loss: 0.22585143148899078\n",
      "Epoch 250/30000 Validation Loss: 0.20312488079071045\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.20312488079071045<=============\n",
      "Epoch 251/30000 Training Loss: 0.1930783987045288\n",
      "Epoch 252/30000 Training Loss: 0.23122406005859375\n",
      "Epoch 253/30000 Training Loss: 0.22913497686386108\n",
      "Epoch 254/30000 Training Loss: 0.234413743019104\n",
      "Epoch 255/30000 Training Loss: 0.22081327438354492\n",
      "Epoch 256/30000 Training Loss: 0.21571005880832672\n",
      "Epoch 257/30000 Training Loss: 0.1863570213317871\n",
      "Epoch 258/30000 Training Loss: 0.2495705634355545\n",
      "Epoch 259/30000 Training Loss: 0.18831686675548553\n",
      "Epoch 260/30000 Training Loss: 0.20961935818195343\n",
      "Epoch 260/30000 Validation Loss: 0.21413899958133698\n",
      "Epoch 261/30000 Training Loss: 0.22921836376190186\n",
      "Epoch 262/30000 Training Loss: 0.24841193854808807\n",
      "Epoch 263/30000 Training Loss: 0.22053559124469757\n",
      "Epoch 264/30000 Training Loss: 0.2259768396615982\n",
      "Epoch 265/30000 Training Loss: 0.20916323363780975\n",
      "Epoch 266/30000 Training Loss: 0.23138605058193207\n",
      "Epoch 267/30000 Training Loss: 0.22541582584381104\n",
      "Epoch 268/30000 Training Loss: 0.204454705119133\n",
      "Epoch 269/30000 Training Loss: 0.21347813308238983\n",
      "Epoch 270/30000 Training Loss: 0.1965423971414566\n",
      "Epoch 270/30000 Validation Loss: 0.20201517641544342\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.20201517641544342<=============\n",
      "Epoch 271/30000 Training Loss: 0.18682748079299927\n",
      "Epoch 272/30000 Training Loss: 0.19836564362049103\n",
      "Epoch 273/30000 Training Loss: 0.19953231513500214\n",
      "Epoch 274/30000 Training Loss: 0.19651784002780914\n",
      "Epoch 275/30000 Training Loss: 0.18736141920089722\n",
      "Epoch 276/30000 Training Loss: 0.20373259484767914\n",
      "Epoch 277/30000 Training Loss: 0.21237830817699432\n",
      "Epoch 278/30000 Training Loss: 0.21886193752288818\n",
      "Epoch 279/30000 Training Loss: 0.22157657146453857\n",
      "Epoch 280/30000 Training Loss: 0.1963513344526291\n",
      "Epoch 280/30000 Validation Loss: 0.19579178094863892\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.19579178094863892<=============\n",
      "Epoch 281/30000 Training Loss: 0.22166942059993744\n",
      "Epoch 282/30000 Training Loss: 0.2149486094713211\n",
      "Epoch 283/30000 Training Loss: 0.23042957484722137\n",
      "Epoch 284/30000 Training Loss: 0.17937584221363068\n",
      "Epoch 285/30000 Training Loss: 0.19831682741641998\n",
      "Epoch 286/30000 Training Loss: 0.195811465382576\n",
      "Epoch 287/30000 Training Loss: 0.16839337348937988\n",
      "Epoch 288/30000 Training Loss: 0.1953682154417038\n",
      "Epoch 289/30000 Training Loss: 0.22012044489383698\n",
      "Epoch 290/30000 Training Loss: 0.1835833042860031\n",
      "Epoch 290/30000 Validation Loss: 0.21225686371326447\n",
      "Epoch 291/30000 Training Loss: 0.1906222552061081\n",
      "Epoch 292/30000 Training Loss: 0.19987249374389648\n",
      "Epoch 293/30000 Training Loss: 0.21600431203842163\n",
      "Epoch 294/30000 Training Loss: 0.20905685424804688\n",
      "Epoch 295/30000 Training Loss: 0.21233749389648438\n",
      "Epoch 296/30000 Training Loss: 0.20226015150547028\n",
      "Epoch 297/30000 Training Loss: 0.23335953056812286\n",
      "Epoch 298/30000 Training Loss: 0.1939762979745865\n",
      "Epoch 299/30000 Training Loss: 0.19953183829784393\n",
      "Epoch 300/30000 Training Loss: 0.194525346159935\n",
      "Epoch 300/30000 Validation Loss: 0.2101767510175705\n",
      "Epoch 301/30000 Training Loss: 0.17543445527553558\n",
      "Epoch 302/30000 Training Loss: 0.2260405421257019\n",
      "Epoch 303/30000 Training Loss: 0.18707901239395142\n",
      "Epoch 304/30000 Training Loss: 0.19187413156032562\n",
      "Epoch 305/30000 Training Loss: 0.18972395360469818\n",
      "Epoch 306/30000 Training Loss: 0.1807878613471985\n",
      "Epoch 307/30000 Training Loss: 0.16779929399490356\n",
      "Epoch 308/30000 Training Loss: 0.197739839553833\n",
      "Epoch 309/30000 Training Loss: 0.19020499289035797\n",
      "Epoch 310/30000 Training Loss: 0.20235216617584229\n",
      "Epoch 310/30000 Validation Loss: 0.2203957587480545\n",
      "Epoch 311/30000 Training Loss: 0.19462589919567108\n",
      "Epoch 312/30000 Training Loss: 0.2429666966199875\n",
      "Epoch 313/30000 Training Loss: 0.20691144466400146\n",
      "Epoch 314/30000 Training Loss: 0.1873413324356079\n",
      "Epoch 315/30000 Training Loss: 0.1618509590625763\n",
      "Epoch 316/30000 Training Loss: 0.20531077682971954\n",
      "Epoch 317/30000 Training Loss: 0.210314080119133\n",
      "Epoch 318/30000 Training Loss: 0.17952609062194824\n",
      "Epoch 319/30000 Training Loss: 0.20159901678562164\n",
      "Epoch 320/30000 Training Loss: 0.196946918964386\n",
      "Epoch 320/30000 Validation Loss: 0.18156997859477997\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.18156997859477997<=============\n",
      "Epoch 321/30000 Training Loss: 0.19192935526371002\n",
      "Epoch 322/30000 Training Loss: 0.18254749476909637\n",
      "Epoch 323/30000 Training Loss: 0.20870327949523926\n",
      "Epoch 324/30000 Training Loss: 0.18779760599136353\n",
      "Epoch 325/30000 Training Loss: 0.20836739242076874\n",
      "Epoch 326/30000 Training Loss: 0.1744387149810791\n",
      "Epoch 327/30000 Training Loss: 0.18532037734985352\n",
      "Epoch 328/30000 Training Loss: 0.20468644797801971\n",
      "Epoch 329/30000 Training Loss: 0.16400277614593506\n",
      "Epoch 330/30000 Training Loss: 0.17549459636211395\n",
      "Epoch 330/30000 Validation Loss: 0.15537218749523163\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.15537218749523163<=============\n",
      "Epoch 331/30000 Training Loss: 0.2192217856645584\n",
      "Epoch 332/30000 Training Loss: 0.18457277119159698\n",
      "Epoch 333/30000 Training Loss: 0.2035774439573288\n",
      "Epoch 334/30000 Training Loss: 0.16659899055957794\n",
      "Epoch 335/30000 Training Loss: 0.17393426597118378\n",
      "Epoch 336/30000 Training Loss: 0.19333119690418243\n",
      "Epoch 337/30000 Training Loss: 0.16623324155807495\n",
      "Epoch 338/30000 Training Loss: 0.21279971301555634\n",
      "Epoch 339/30000 Training Loss: 0.1506885439157486\n",
      "Epoch 340/30000 Training Loss: 0.1731174737215042\n",
      "Epoch 340/30000 Validation Loss: 0.19463805854320526\n",
      "Epoch 341/30000 Training Loss: 0.149739608168602\n",
      "Epoch 342/30000 Training Loss: 0.15462054312229156\n",
      "Epoch 343/30000 Training Loss: 0.17560459673404694\n",
      "Epoch 344/30000 Training Loss: 0.15168625116348267\n",
      "Epoch 345/30000 Training Loss: 0.20145010948181152\n",
      "Epoch 346/30000 Training Loss: 0.16974389553070068\n",
      "Epoch 347/30000 Training Loss: 0.20151519775390625\n",
      "Epoch 348/30000 Training Loss: 0.157936692237854\n",
      "Epoch 349/30000 Training Loss: 0.15770649909973145\n",
      "Epoch 350/30000 Training Loss: 0.17735524475574493\n",
      "Epoch 350/30000 Validation Loss: 0.16567635536193848\n",
      "Epoch 351/30000 Training Loss: 0.19997994601726532\n",
      "Epoch 352/30000 Training Loss: 0.15115128457546234\n",
      "Epoch 353/30000 Training Loss: 0.15901809930801392\n",
      "Epoch 354/30000 Training Loss: 0.18776746094226837\n",
      "Epoch 355/30000 Training Loss: 0.16574649512767792\n",
      "Epoch 356/30000 Training Loss: 0.16947458684444427\n",
      "Epoch 357/30000 Training Loss: 0.20357976853847504\n",
      "Epoch 358/30000 Training Loss: 0.1801437884569168\n",
      "Epoch 359/30000 Training Loss: 0.1552136093378067\n",
      "Epoch 360/30000 Training Loss: 0.16397327184677124\n",
      "Epoch 360/30000 Validation Loss: 0.17597687244415283\n",
      "Epoch 361/30000 Training Loss: 0.15168434381484985\n",
      "Epoch 362/30000 Training Loss: 0.19670264422893524\n",
      "Epoch 363/30000 Training Loss: 0.12824691832065582\n",
      "Epoch 364/30000 Training Loss: 0.1871403008699417\n",
      "Epoch 365/30000 Training Loss: 0.17445974051952362\n",
      "Epoch 366/30000 Training Loss: 0.181528702378273\n",
      "Epoch 367/30000 Training Loss: 0.17857415974140167\n",
      "Epoch 368/30000 Training Loss: 0.18409286439418793\n",
      "Epoch 369/30000 Training Loss: 0.17013131082057953\n",
      "Epoch 370/30000 Training Loss: 0.16050434112548828\n",
      "Epoch 370/30000 Validation Loss: 0.226246640086174\n",
      "Epoch 371/30000 Training Loss: 0.16722746193408966\n",
      "Epoch 372/30000 Training Loss: 0.17917793989181519\n",
      "Epoch 373/30000 Training Loss: 0.16939593851566315\n",
      "Epoch 374/30000 Training Loss: 0.1678590625524521\n",
      "Epoch 375/30000 Training Loss: 0.16562151908874512\n",
      "Epoch 376/30000 Training Loss: 0.1635991334915161\n",
      "Epoch 377/30000 Training Loss: 0.19306820631027222\n",
      "Epoch 378/30000 Training Loss: 0.1788075715303421\n",
      "Epoch 379/30000 Training Loss: 0.18334956467151642\n",
      "Epoch 380/30000 Training Loss: 0.16917766630649567\n",
      "Epoch 380/30000 Validation Loss: 0.1621846705675125\n",
      "Epoch 381/30000 Training Loss: 0.18164384365081787\n",
      "Epoch 382/30000 Training Loss: 0.16308081150054932\n",
      "Epoch 383/30000 Training Loss: 0.16249734163284302\n",
      "Epoch 384/30000 Training Loss: 0.1895122528076172\n",
      "Epoch 385/30000 Training Loss: 0.13749875128269196\n",
      "Epoch 386/30000 Training Loss: 0.16878777742385864\n",
      "Epoch 387/30000 Training Loss: 0.1672513335943222\n",
      "Epoch 388/30000 Training Loss: 0.1530463844537735\n",
      "Epoch 389/30000 Training Loss: 0.1704339236021042\n",
      "Epoch 390/30000 Training Loss: 0.1798311471939087\n",
      "Epoch 390/30000 Validation Loss: 0.17948468029499054\n",
      "Epoch 391/30000 Training Loss: 0.15317629277706146\n",
      "Epoch 392/30000 Training Loss: 0.1714559942483902\n",
      "Epoch 393/30000 Training Loss: 0.17932391166687012\n",
      "Epoch 394/30000 Training Loss: 0.18645411729812622\n",
      "Epoch 395/30000 Training Loss: 0.15082396566867828\n",
      "Epoch 396/30000 Training Loss: 0.19682204723358154\n",
      "Epoch 397/30000 Training Loss: 0.18047499656677246\n",
      "Epoch 398/30000 Training Loss: 0.16137966513633728\n",
      "Epoch 399/30000 Training Loss: 0.1922520399093628\n",
      "Epoch 400/30000 Training Loss: 0.15189774334430695\n",
      "Epoch 400/30000 Validation Loss: 0.14973215758800507\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.14973215758800507<=============\n",
      "Epoch 401/30000 Training Loss: 0.1975194811820984\n",
      "Epoch 402/30000 Training Loss: 0.1865644007921219\n",
      "Epoch 403/30000 Training Loss: 0.19993460178375244\n",
      "Epoch 404/30000 Training Loss: 0.16440528631210327\n",
      "Epoch 405/30000 Training Loss: 0.157938614487648\n",
      "Epoch 406/30000 Training Loss: 0.16565515100955963\n",
      "Epoch 407/30000 Training Loss: 0.12377645820379257\n",
      "Epoch 408/30000 Training Loss: 0.19178269803524017\n",
      "Epoch 409/30000 Training Loss: 0.14368538558483124\n",
      "Epoch 410/30000 Training Loss: 0.15592728555202484\n",
      "Epoch 410/30000 Validation Loss: 0.14771437644958496\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.14771437644958496<=============\n",
      "Epoch 411/30000 Training Loss: 0.17870663106441498\n",
      "Epoch 412/30000 Training Loss: 0.15519939363002777\n",
      "Epoch 413/30000 Training Loss: 0.18153928220272064\n",
      "Epoch 414/30000 Training Loss: 0.13821233808994293\n",
      "Epoch 415/30000 Training Loss: 0.1485460102558136\n",
      "Epoch 416/30000 Training Loss: 0.18729738891124725\n",
      "Epoch 417/30000 Training Loss: 0.16625608503818512\n",
      "Epoch 418/30000 Training Loss: 0.17084340751171112\n",
      "Epoch 419/30000 Training Loss: 0.14312900602817535\n",
      "Epoch 420/30000 Training Loss: 0.194560706615448\n",
      "Epoch 420/30000 Validation Loss: 0.16932469606399536\n",
      "Epoch 421/30000 Training Loss: 0.16467343270778656\n",
      "Epoch 422/30000 Training Loss: 0.18772940337657928\n",
      "Epoch 423/30000 Training Loss: 0.15471915900707245\n",
      "Epoch 424/30000 Training Loss: 0.18343855440616608\n",
      "Epoch 425/30000 Training Loss: 0.189358189702034\n",
      "Epoch 426/30000 Training Loss: 0.14916537702083588\n",
      "Epoch 427/30000 Training Loss: 0.1590486615896225\n",
      "Epoch 428/30000 Training Loss: 0.15784774720668793\n",
      "Epoch 429/30000 Training Loss: 0.17087607085704803\n",
      "Epoch 430/30000 Training Loss: 0.1752193719148636\n",
      "Epoch 430/30000 Validation Loss: 0.16599015891551971\n",
      "Epoch 431/30000 Training Loss: 0.12691374123096466\n",
      "Epoch 432/30000 Training Loss: 0.13659311830997467\n",
      "Epoch 433/30000 Training Loss: 0.13800877332687378\n",
      "Epoch 434/30000 Training Loss: 0.1441090852022171\n",
      "Epoch 435/30000 Training Loss: 0.17469726502895355\n",
      "Epoch 436/30000 Training Loss: 0.16869038343429565\n",
      "Epoch 437/30000 Training Loss: 0.15441520512104034\n",
      "Epoch 438/30000 Training Loss: 0.1707916259765625\n",
      "Epoch 439/30000 Training Loss: 0.16579343378543854\n",
      "Epoch 440/30000 Training Loss: 0.15532593429088593\n",
      "Epoch 440/30000 Validation Loss: 0.1481732726097107\n",
      "Epoch 441/30000 Training Loss: 0.1628633737564087\n",
      "Epoch 442/30000 Training Loss: 0.161504864692688\n",
      "Epoch 443/30000 Training Loss: 0.1743011623620987\n",
      "Epoch 444/30000 Training Loss: 0.14441245794296265\n",
      "Epoch 445/30000 Training Loss: 0.1602010726928711\n",
      "Epoch 446/30000 Training Loss: 0.13864393532276154\n",
      "Epoch 447/30000 Training Loss: 0.14045611023902893\n",
      "Epoch 448/30000 Training Loss: 0.14355219900608063\n",
      "Epoch 449/30000 Training Loss: 0.16202448308467865\n",
      "Epoch 450/30000 Training Loss: 0.16679047048091888\n",
      "Epoch 450/30000 Validation Loss: 0.1791583150625229\n",
      "Epoch 451/30000 Training Loss: 0.19056056439876556\n",
      "Epoch 452/30000 Training Loss: 0.14912307262420654\n",
      "Epoch 453/30000 Training Loss: 0.16372060775756836\n",
      "Epoch 454/30000 Training Loss: 0.16765058040618896\n",
      "Epoch 455/30000 Training Loss: 0.1801546812057495\n",
      "Epoch 456/30000 Training Loss: 0.1555127054452896\n",
      "Epoch 457/30000 Training Loss: 0.17062409222126007\n",
      "Epoch 458/30000 Training Loss: 0.16748863458633423\n",
      "Epoch 459/30000 Training Loss: 0.1811104267835617\n",
      "Epoch 460/30000 Training Loss: 0.14192356169223785\n",
      "Epoch 460/30000 Validation Loss: 0.15535129606723785\n",
      "Epoch 461/30000 Training Loss: 0.15079450607299805\n",
      "Epoch 462/30000 Training Loss: 0.15417619049549103\n",
      "Epoch 463/30000 Training Loss: 0.16079463064670563\n",
      "Epoch 464/30000 Training Loss: 0.1283702701330185\n",
      "Epoch 465/30000 Training Loss: 0.15687520802021027\n",
      "Epoch 466/30000 Training Loss: 0.16133242845535278\n",
      "Epoch 467/30000 Training Loss: 0.14771588146686554\n",
      "Epoch 468/30000 Training Loss: 0.15233010053634644\n",
      "Epoch 469/30000 Training Loss: 0.13970161974430084\n",
      "Epoch 470/30000 Training Loss: 0.12890945374965668\n",
      "Epoch 470/30000 Validation Loss: 0.16354985535144806\n",
      "Epoch 471/30000 Training Loss: 0.1577596217393875\n",
      "Epoch 472/30000 Training Loss: 0.14412708580493927\n",
      "Epoch 473/30000 Training Loss: 0.12439683824777603\n",
      "Epoch 474/30000 Training Loss: 0.15491868555545807\n",
      "Epoch 475/30000 Training Loss: 0.15534573793411255\n",
      "Epoch 476/30000 Training Loss: 0.16480213403701782\n",
      "Epoch 477/30000 Training Loss: 0.14387346804141998\n",
      "Epoch 478/30000 Training Loss: 0.14297795295715332\n",
      "Epoch 479/30000 Training Loss: 0.1328083574771881\n",
      "Epoch 480/30000 Training Loss: 0.11695799231529236\n",
      "Epoch 480/30000 Validation Loss: 0.14820897579193115\n",
      "Epoch 481/30000 Training Loss: 0.1744995266199112\n",
      "Epoch 482/30000 Training Loss: 0.11603763699531555\n",
      "Epoch 483/30000 Training Loss: 0.1213788390159607\n",
      "Epoch 484/30000 Training Loss: 0.10872539132833481\n",
      "Epoch 485/30000 Training Loss: 0.1320372223854065\n",
      "Epoch 486/30000 Training Loss: 0.1387130171060562\n",
      "Epoch 487/30000 Training Loss: 0.1836625337600708\n",
      "Epoch 488/30000 Training Loss: 0.12921999394893646\n",
      "Epoch 489/30000 Training Loss: 0.16846495866775513\n",
      "Epoch 490/30000 Training Loss: 0.1666659563779831\n",
      "Epoch 490/30000 Validation Loss: 0.12654277682304382\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.12654277682304382<=============\n",
      "Epoch 491/30000 Training Loss: 0.12172075361013412\n",
      "Epoch 492/30000 Training Loss: 0.1301078349351883\n",
      "Epoch 493/30000 Training Loss: 0.130360409617424\n",
      "Epoch 494/30000 Training Loss: 0.1563551276922226\n",
      "Epoch 495/30000 Training Loss: 0.137558251619339\n",
      "Epoch 496/30000 Training Loss: 0.12264017015695572\n",
      "Epoch 497/30000 Training Loss: 0.1295483112335205\n",
      "Epoch 498/30000 Training Loss: 0.11486303806304932\n",
      "Epoch 499/30000 Training Loss: 0.16856102645397186\n",
      "Epoch 500/30000 Training Loss: 0.15987621247768402\n",
      "Epoch 500/30000 Validation Loss: 0.1501084715127945\n",
      "Epoch 501/30000 Training Loss: 0.13887496292591095\n",
      "Epoch 502/30000 Training Loss: 0.18258143961429596\n",
      "Epoch 503/30000 Training Loss: 0.1341874599456787\n",
      "Epoch 504/30000 Training Loss: 0.10872262716293335\n",
      "Epoch 505/30000 Training Loss: 0.14644403755664825\n",
      "Epoch 506/30000 Training Loss: 0.12427269667387009\n",
      "Epoch 507/30000 Training Loss: 0.1451823115348816\n",
      "Epoch 508/30000 Training Loss: 0.15040095150470734\n",
      "Epoch 509/30000 Training Loss: 0.14197023212909698\n",
      "Epoch 510/30000 Training Loss: 0.16067257523536682\n",
      "Epoch 510/30000 Validation Loss: 0.15437103807926178\n",
      "Epoch 511/30000 Training Loss: 0.1347639113664627\n",
      "Epoch 512/30000 Training Loss: 0.14376874268054962\n",
      "Epoch 513/30000 Training Loss: 0.1208864226937294\n",
      "Epoch 514/30000 Training Loss: 0.15110208094120026\n",
      "Epoch 515/30000 Training Loss: 0.15485763549804688\n",
      "Epoch 516/30000 Training Loss: 0.14831729233264923\n",
      "Epoch 517/30000 Training Loss: 0.12859350442886353\n",
      "Epoch 518/30000 Training Loss: 0.1216387152671814\n",
      "Epoch 519/30000 Training Loss: 0.13509786128997803\n",
      "Epoch 520/30000 Training Loss: 0.1516634225845337\n",
      "Epoch 520/30000 Validation Loss: 0.14872834086418152\n",
      "Epoch 521/30000 Training Loss: 0.1937745064496994\n",
      "Epoch 522/30000 Training Loss: 0.14510329067707062\n",
      "Epoch 523/30000 Training Loss: 0.12904582917690277\n",
      "Epoch 524/30000 Training Loss: 0.15463308990001678\n",
      "Epoch 525/30000 Training Loss: 0.12989138066768646\n",
      "Epoch 526/30000 Training Loss: 0.13881686329841614\n",
      "Epoch 527/30000 Training Loss: 0.15918037295341492\n",
      "Epoch 528/30000 Training Loss: 0.14409072697162628\n",
      "Epoch 529/30000 Training Loss: 0.15169881284236908\n",
      "Epoch 530/30000 Training Loss: 0.14566174149513245\n",
      "Epoch 530/30000 Validation Loss: 0.14395278692245483\n",
      "Epoch 531/30000 Training Loss: 0.17060579359531403\n",
      "Epoch 532/30000 Training Loss: 0.12905600666999817\n",
      "Epoch 533/30000 Training Loss: 0.1337141990661621\n",
      "Epoch 534/30000 Training Loss: 0.15842245519161224\n",
      "Epoch 535/30000 Training Loss: 0.14812405407428741\n",
      "Epoch 536/30000 Training Loss: 0.1596454530954361\n",
      "Epoch 537/30000 Training Loss: 0.14098212122917175\n",
      "Epoch 538/30000 Training Loss: 0.15669983625411987\n",
      "Epoch 539/30000 Training Loss: 0.13600462675094604\n",
      "Epoch 540/30000 Training Loss: 0.1258704513311386\n",
      "Epoch 540/30000 Validation Loss: 0.14740760624408722\n",
      "Epoch 541/30000 Training Loss: 0.16276587545871735\n",
      "Epoch 542/30000 Training Loss: 0.14478889107704163\n",
      "Epoch 543/30000 Training Loss: 0.140812948346138\n",
      "Epoch 544/30000 Training Loss: 0.1494726538658142\n",
      "Epoch 545/30000 Training Loss: 0.1664903163909912\n",
      "Epoch 546/30000 Training Loss: 0.10601416230201721\n",
      "Epoch 547/30000 Training Loss: 0.14870871603488922\n",
      "Epoch 548/30000 Training Loss: 0.1603754758834839\n",
      "Epoch 549/30000 Training Loss: 0.12238163501024246\n",
      "Epoch 550/30000 Training Loss: 0.14698351919651031\n",
      "Epoch 550/30000 Validation Loss: 0.1486671268939972\n",
      "Epoch 551/30000 Training Loss: 0.16594259440898895\n",
      "Epoch 552/30000 Training Loss: 0.14291037619113922\n",
      "Epoch 553/30000 Training Loss: 0.14678578078746796\n",
      "Epoch 554/30000 Training Loss: 0.13302181661128998\n",
      "Epoch 555/30000 Training Loss: 0.14470797777175903\n",
      "Epoch 556/30000 Training Loss: 0.11262023448944092\n",
      "Epoch 557/30000 Training Loss: 0.14007097482681274\n",
      "Epoch 558/30000 Training Loss: 0.144332617521286\n",
      "Epoch 559/30000 Training Loss: 0.17196817696094513\n",
      "Epoch 560/30000 Training Loss: 0.13994784653186798\n",
      "Epoch 560/30000 Validation Loss: 0.14667360484600067\n",
      "Epoch 561/30000 Training Loss: 0.1301914006471634\n",
      "Epoch 562/30000 Training Loss: 0.11319946497678757\n",
      "Epoch 563/30000 Training Loss: 0.10061917454004288\n",
      "Epoch 564/30000 Training Loss: 0.13813114166259766\n",
      "Epoch 565/30000 Training Loss: 0.10709062963724136\n",
      "Epoch 566/30000 Training Loss: 0.12744949758052826\n",
      "Epoch 567/30000 Training Loss: 0.13894794881343842\n",
      "Epoch 568/30000 Training Loss: 0.13110163807868958\n",
      "Epoch 569/30000 Training Loss: 0.12203719466924667\n",
      "Epoch 570/30000 Training Loss: 0.12487169355154037\n",
      "Epoch 570/30000 Validation Loss: 0.09975842386484146\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09975842386484146<=============\n",
      "Epoch 571/30000 Training Loss: 0.1561989188194275\n",
      "Epoch 572/30000 Training Loss: 0.14279107749462128\n",
      "Epoch 573/30000 Training Loss: 0.13455933332443237\n",
      "Epoch 574/30000 Training Loss: 0.12765853106975555\n",
      "Epoch 575/30000 Training Loss: 0.12696801126003265\n",
      "Epoch 576/30000 Training Loss: 0.1179719939827919\n",
      "Epoch 577/30000 Training Loss: 0.1418292373418808\n",
      "Epoch 578/30000 Training Loss: 0.12975020706653595\n",
      "Epoch 579/30000 Training Loss: 0.14169873297214508\n",
      "Epoch 580/30000 Training Loss: 0.12989304959774017\n",
      "Epoch 580/30000 Validation Loss: 0.14266255497932434\n",
      "Epoch 581/30000 Training Loss: 0.14067228138446808\n",
      "Epoch 582/30000 Training Loss: 0.1329912692308426\n",
      "Epoch 583/30000 Training Loss: 0.15456454455852509\n",
      "Epoch 584/30000 Training Loss: 0.11828207224607468\n",
      "Epoch 585/30000 Training Loss: 0.13284431397914886\n",
      "Epoch 586/30000 Training Loss: 0.10653404146432877\n",
      "Epoch 587/30000 Training Loss: 0.141129732131958\n",
      "Epoch 588/30000 Training Loss: 0.11973196268081665\n",
      "Epoch 589/30000 Training Loss: 0.13169986009597778\n",
      "Epoch 590/30000 Training Loss: 0.12929005920886993\n",
      "Epoch 590/30000 Validation Loss: 0.14743275940418243\n",
      "Epoch 591/30000 Training Loss: 0.12931658327579498\n",
      "Epoch 592/30000 Training Loss: 0.11836415529251099\n",
      "Epoch 593/30000 Training Loss: 0.14039234817028046\n",
      "Epoch 594/30000 Training Loss: 0.14607661962509155\n",
      "Epoch 595/30000 Training Loss: 0.13079313933849335\n",
      "Epoch 596/30000 Training Loss: 0.10238320380449295\n",
      "Epoch 597/30000 Training Loss: 0.11613955348730087\n",
      "Epoch 598/30000 Training Loss: 0.12594249844551086\n",
      "Epoch 599/30000 Training Loss: 0.1155788004398346\n",
      "Epoch 600/30000 Training Loss: 0.1604442149400711\n",
      "Epoch 600/30000 Validation Loss: 0.13015355169773102\n",
      "Epoch 601/30000 Training Loss: 0.11731881648302078\n",
      "Epoch 602/30000 Training Loss: 0.1258126050233841\n",
      "Epoch 603/30000 Training Loss: 0.11168713122606277\n",
      "Epoch 604/30000 Training Loss: 0.13206733763217926\n",
      "Epoch 605/30000 Training Loss: 0.13695426285266876\n",
      "Epoch 606/30000 Training Loss: 0.12247125059366226\n",
      "Epoch 607/30000 Training Loss: 0.12311301380395889\n",
      "Epoch 608/30000 Training Loss: 0.11635002493858337\n",
      "Epoch 609/30000 Training Loss: 0.14251600205898285\n",
      "Epoch 610/30000 Training Loss: 0.128640815615654\n",
      "Epoch 610/30000 Validation Loss: 0.1318725347518921\n",
      "Epoch 611/30000 Training Loss: 0.1165994182229042\n",
      "Epoch 612/30000 Training Loss: 0.12093519419431686\n",
      "Epoch 613/30000 Training Loss: 0.10199200361967087\n",
      "Epoch 614/30000 Training Loss: 0.14015230536460876\n",
      "Epoch 615/30000 Training Loss: 0.11483808606863022\n",
      "Epoch 616/30000 Training Loss: 0.16270439326763153\n",
      "Epoch 617/30000 Training Loss: 0.15091367065906525\n",
      "Epoch 618/30000 Training Loss: 0.14803457260131836\n",
      "Epoch 619/30000 Training Loss: 0.1066712737083435\n",
      "Epoch 620/30000 Training Loss: 0.14792658388614655\n",
      "Epoch 620/30000 Validation Loss: 0.11499649286270142\n",
      "Epoch 621/30000 Training Loss: 0.13397549092769623\n",
      "Epoch 622/30000 Training Loss: 0.10160381346940994\n",
      "Epoch 623/30000 Training Loss: 0.12935160100460052\n",
      "Epoch 624/30000 Training Loss: 0.11247646808624268\n",
      "Epoch 625/30000 Training Loss: 0.1371883898973465\n",
      "Epoch 626/30000 Training Loss: 0.14270727336406708\n",
      "Epoch 627/30000 Training Loss: 0.15143676102161407\n",
      "Epoch 628/30000 Training Loss: 0.17243020236492157\n",
      "Epoch 629/30000 Training Loss: 0.13060863316059113\n",
      "Epoch 630/30000 Training Loss: 0.12471690028905869\n",
      "Epoch 630/30000 Validation Loss: 0.14594179391860962\n",
      "Epoch 631/30000 Training Loss: 0.1210469976067543\n",
      "Epoch 632/30000 Training Loss: 0.13362784683704376\n",
      "Epoch 633/30000 Training Loss: 0.11810263991355896\n",
      "Epoch 634/30000 Training Loss: 0.12383800745010376\n",
      "Epoch 635/30000 Training Loss: 0.11384323984384537\n",
      "Epoch 636/30000 Training Loss: 0.12160351872444153\n",
      "Epoch 637/30000 Training Loss: 0.10287433862686157\n",
      "Epoch 638/30000 Training Loss: 0.1562025099992752\n",
      "Epoch 639/30000 Training Loss: 0.1329813152551651\n",
      "Epoch 640/30000 Training Loss: 0.11478579044342041\n",
      "Epoch 640/30000 Validation Loss: 0.15210022032260895\n",
      "Epoch 641/30000 Training Loss: 0.11798566579818726\n",
      "Epoch 642/30000 Training Loss: 0.1179976835846901\n",
      "Epoch 643/30000 Training Loss: 0.12191235274076462\n",
      "Epoch 644/30000 Training Loss: 0.1048702672123909\n",
      "Epoch 645/30000 Training Loss: 0.15145593881607056\n",
      "Epoch 646/30000 Training Loss: 0.14397843182086945\n",
      "Epoch 647/30000 Training Loss: 0.12257740646600723\n",
      "Epoch 648/30000 Training Loss: 0.1124974861741066\n",
      "Epoch 649/30000 Training Loss: 0.13361856341362\n",
      "Epoch 650/30000 Training Loss: 0.11496022343635559\n",
      "Epoch 650/30000 Validation Loss: 0.09966634958982468\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09966634958982468<=============\n",
      "Epoch 651/30000 Training Loss: 0.11137761920690536\n",
      "Epoch 652/30000 Training Loss: 0.15448524057865143\n",
      "Epoch 653/30000 Training Loss: 0.11412128061056137\n",
      "Epoch 654/30000 Training Loss: 0.10925456881523132\n",
      "Epoch 655/30000 Training Loss: 0.13658513128757477\n",
      "Epoch 656/30000 Training Loss: 0.12424663454294205\n",
      "Epoch 657/30000 Training Loss: 0.1177477017045021\n",
      "Epoch 658/30000 Training Loss: 0.1225552186369896\n",
      "Epoch 659/30000 Training Loss: 0.12212652713060379\n",
      "Epoch 660/30000 Training Loss: 0.13965971767902374\n",
      "Epoch 660/30000 Validation Loss: 0.12925614416599274\n",
      "Epoch 661/30000 Training Loss: 0.12280907481908798\n",
      "Epoch 662/30000 Training Loss: 0.13040022552013397\n",
      "Epoch 663/30000 Training Loss: 0.11701798439025879\n",
      "Epoch 664/30000 Training Loss: 0.1081126257777214\n",
      "Epoch 665/30000 Training Loss: 0.12685178220272064\n",
      "Epoch 666/30000 Training Loss: 0.12141495198011398\n",
      "Epoch 667/30000 Training Loss: 0.12969261407852173\n",
      "Epoch 668/30000 Training Loss: 0.12999124825000763\n",
      "Epoch 669/30000 Training Loss: 0.11163115501403809\n",
      "Epoch 670/30000 Training Loss: 0.10930828005075455\n",
      "Epoch 670/30000 Validation Loss: 0.11855723708868027\n",
      "Epoch 671/30000 Training Loss: 0.13718555867671967\n",
      "Epoch 672/30000 Training Loss: 0.11115813255310059\n",
      "Epoch 673/30000 Training Loss: 0.1212991252541542\n",
      "Epoch 674/30000 Training Loss: 0.1440075933933258\n",
      "Epoch 675/30000 Training Loss: 0.14956122636795044\n",
      "Epoch 676/30000 Training Loss: 0.14240916073322296\n",
      "Epoch 677/30000 Training Loss: 0.1325882524251938\n",
      "Epoch 678/30000 Training Loss: 0.12238263338804245\n",
      "Epoch 679/30000 Training Loss: 0.11421173810958862\n",
      "Epoch 680/30000 Training Loss: 0.12284937500953674\n",
      "Epoch 680/30000 Validation Loss: 0.13694114983081818\n",
      "Epoch 681/30000 Training Loss: 0.1229388490319252\n",
      "Epoch 682/30000 Training Loss: 0.11842906475067139\n",
      "Epoch 683/30000 Training Loss: 0.138866126537323\n",
      "Epoch 684/30000 Training Loss: 0.14080041646957397\n",
      "Epoch 685/30000 Training Loss: 0.12531054019927979\n",
      "Epoch 686/30000 Training Loss: 0.12103398889303207\n",
      "Epoch 687/30000 Training Loss: 0.1282905489206314\n",
      "Epoch 688/30000 Training Loss: 0.13740575313568115\n",
      "Epoch 689/30000 Training Loss: 0.1534299999475479\n",
      "Epoch 690/30000 Training Loss: 0.11243630200624466\n",
      "Epoch 690/30000 Validation Loss: 0.1090848445892334\n",
      "Epoch 691/30000 Training Loss: 0.12275582551956177\n",
      "Epoch 692/30000 Training Loss: 0.19091665744781494\n",
      "Epoch 693/30000 Training Loss: 0.12992684543132782\n",
      "Epoch 694/30000 Training Loss: 0.12131288647651672\n",
      "Epoch 695/30000 Training Loss: 0.1315540373325348\n",
      "Epoch 696/30000 Training Loss: 0.09866663813591003\n",
      "Epoch 697/30000 Training Loss: 0.12430515140295029\n",
      "Epoch 698/30000 Training Loss: 0.14400596916675568\n",
      "Epoch 699/30000 Training Loss: 0.12120275944471359\n",
      "Epoch 700/30000 Training Loss: 0.11838217824697495\n",
      "Epoch 700/30000 Validation Loss: 0.12522011995315552\n",
      "Epoch 701/30000 Training Loss: 0.11550164222717285\n",
      "Epoch 702/30000 Training Loss: 0.10979247838258743\n",
      "Epoch 703/30000 Training Loss: 0.12178037315607071\n",
      "Epoch 704/30000 Training Loss: 0.10972711443901062\n",
      "Epoch 705/30000 Training Loss: 0.14333991706371307\n",
      "Epoch 706/30000 Training Loss: 0.11417446285486221\n",
      "Epoch 707/30000 Training Loss: 0.12467101961374283\n",
      "Epoch 708/30000 Training Loss: 0.1007223129272461\n",
      "Epoch 709/30000 Training Loss: 0.1188332736492157\n",
      "Epoch 710/30000 Training Loss: 0.11911074072122574\n",
      "Epoch 710/30000 Validation Loss: 0.13405567407608032\n",
      "Epoch 711/30000 Training Loss: 0.11798599362373352\n",
      "Epoch 712/30000 Training Loss: 0.11174815893173218\n",
      "Epoch 713/30000 Training Loss: 0.14119088649749756\n",
      "Epoch 714/30000 Training Loss: 0.12496218830347061\n",
      "Epoch 715/30000 Training Loss: 0.11069154739379883\n",
      "Epoch 716/30000 Training Loss: 0.13755665719509125\n",
      "Epoch 717/30000 Training Loss: 0.12480399012565613\n",
      "Epoch 718/30000 Training Loss: 0.12788234651088715\n",
      "Epoch 719/30000 Training Loss: 0.161674365401268\n",
      "Epoch 720/30000 Training Loss: 0.11487576365470886\n",
      "Epoch 720/30000 Validation Loss: 0.1355651170015335\n",
      "Epoch 721/30000 Training Loss: 0.12478122115135193\n",
      "Epoch 722/30000 Training Loss: 0.13429607450962067\n",
      "Epoch 723/30000 Training Loss: 0.1524152308702469\n",
      "Epoch 724/30000 Training Loss: 0.1275450885295868\n",
      "Epoch 725/30000 Training Loss: 0.125824436545372\n",
      "Epoch 726/30000 Training Loss: 0.12217766046524048\n",
      "Epoch 727/30000 Training Loss: 0.12210925668478012\n",
      "Epoch 728/30000 Training Loss: 0.11888331174850464\n",
      "Epoch 729/30000 Training Loss: 0.13414128124713898\n",
      "Epoch 730/30000 Training Loss: 0.12269753217697144\n",
      "Epoch 730/30000 Validation Loss: 0.14776049554347992\n",
      "Epoch 731/30000 Training Loss: 0.12618394196033478\n",
      "Epoch 732/30000 Training Loss: 0.13677184283733368\n",
      "Epoch 733/30000 Training Loss: 0.09024098515510559\n",
      "Epoch 734/30000 Training Loss: 0.13990771770477295\n",
      "Epoch 735/30000 Training Loss: 0.13819415867328644\n",
      "Epoch 736/30000 Training Loss: 0.11779176443815231\n",
      "Epoch 737/30000 Training Loss: 0.11873198300600052\n",
      "Epoch 738/30000 Training Loss: 0.1317315697669983\n",
      "Epoch 739/30000 Training Loss: 0.10891427844762802\n",
      "Epoch 740/30000 Training Loss: 0.09482365846633911\n",
      "Epoch 740/30000 Validation Loss: 0.13549505174160004\n",
      "Epoch 741/30000 Training Loss: 0.11387117952108383\n",
      "Epoch 742/30000 Training Loss: 0.10637027770280838\n",
      "Epoch 743/30000 Training Loss: 0.10850253701210022\n",
      "Epoch 744/30000 Training Loss: 0.1174238920211792\n",
      "Epoch 745/30000 Training Loss: 0.09956353157758713\n",
      "Epoch 746/30000 Training Loss: 0.09847211092710495\n",
      "Epoch 747/30000 Training Loss: 0.13307571411132812\n",
      "Epoch 748/30000 Training Loss: 0.11626026779413223\n",
      "Epoch 749/30000 Training Loss: 0.12512879073619843\n",
      "Epoch 750/30000 Training Loss: 0.11762439459562302\n",
      "Epoch 750/30000 Validation Loss: 0.10542455315589905\n",
      "Epoch 751/30000 Training Loss: 0.12757046520709991\n",
      "Epoch 752/30000 Training Loss: 0.11924052983522415\n",
      "Epoch 753/30000 Training Loss: 0.11469841003417969\n",
      "Epoch 754/30000 Training Loss: 0.11899826675653458\n",
      "Epoch 755/30000 Training Loss: 0.10365083813667297\n",
      "Epoch 756/30000 Training Loss: 0.12273066490888596\n",
      "Epoch 757/30000 Training Loss: 0.15040360391139984\n",
      "Epoch 758/30000 Training Loss: 0.14830486476421356\n",
      "Epoch 759/30000 Training Loss: 0.12177705764770508\n",
      "Epoch 760/30000 Training Loss: 0.13571399450302124\n",
      "Epoch 760/30000 Validation Loss: 0.09300124645233154\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09300124645233154<=============\n",
      "Epoch 761/30000 Training Loss: 0.12252355366945267\n",
      "Epoch 762/30000 Training Loss: 0.13000993430614471\n",
      "Epoch 763/30000 Training Loss: 0.10053735971450806\n",
      "Epoch 764/30000 Training Loss: 0.12931396067142487\n",
      "Epoch 765/30000 Training Loss: 0.1298198699951172\n",
      "Epoch 766/30000 Training Loss: 0.11082140356302261\n",
      "Epoch 767/30000 Training Loss: 0.11877521127462387\n",
      "Epoch 768/30000 Training Loss: 0.17338688671588898\n",
      "Epoch 769/30000 Training Loss: 0.10663854330778122\n",
      "Epoch 770/30000 Training Loss: 0.13044552505016327\n",
      "Epoch 770/30000 Validation Loss: 0.11531451344490051\n",
      "Epoch 771/30000 Training Loss: 0.12770463526248932\n",
      "Epoch 772/30000 Training Loss: 0.10907157510519028\n",
      "Epoch 773/30000 Training Loss: 0.1251193881034851\n",
      "Epoch 774/30000 Training Loss: 0.11863106489181519\n",
      "Epoch 775/30000 Training Loss: 0.11985248327255249\n",
      "Epoch 776/30000 Training Loss: 0.08982553333044052\n",
      "Epoch 777/30000 Training Loss: 0.11745303124189377\n",
      "Epoch 778/30000 Training Loss: 0.11764468997716904\n",
      "Epoch 779/30000 Training Loss: 0.09059974551200867\n",
      "Epoch 780/30000 Training Loss: 0.11674704402685165\n",
      "Epoch 780/30000 Validation Loss: 0.11498696357011795\n",
      "Epoch 781/30000 Training Loss: 0.11025474220514297\n",
      "Epoch 782/30000 Training Loss: 0.14088977873325348\n",
      "Epoch 783/30000 Training Loss: 0.11562842130661011\n",
      "Epoch 784/30000 Training Loss: 0.09254622459411621\n",
      "Epoch 785/30000 Training Loss: 0.1287171095609665\n",
      "Epoch 786/30000 Training Loss: 0.10339043289422989\n",
      "Epoch 787/30000 Training Loss: 0.1442171335220337\n",
      "Epoch 788/30000 Training Loss: 0.1021844744682312\n",
      "Epoch 789/30000 Training Loss: 0.11816402524709702\n",
      "Epoch 790/30000 Training Loss: 0.13185591995716095\n",
      "Epoch 790/30000 Validation Loss: 0.1498219221830368\n",
      "Epoch 791/30000 Training Loss: 0.11210889369249344\n",
      "Epoch 792/30000 Training Loss: 0.09574565291404724\n",
      "Epoch 793/30000 Training Loss: 0.13939978182315826\n",
      "Epoch 794/30000 Training Loss: 0.12485913187265396\n",
      "Epoch 795/30000 Training Loss: 0.12014273554086685\n",
      "Epoch 796/30000 Training Loss: 0.12279834598302841\n",
      "Epoch 797/30000 Training Loss: 0.12984608113765717\n",
      "Epoch 798/30000 Training Loss: 0.15448082983493805\n",
      "Epoch 799/30000 Training Loss: 0.12453410774469376\n",
      "Epoch 800/30000 Training Loss: 0.10896814614534378\n",
      "Epoch 800/30000 Validation Loss: 0.11826298385858536\n",
      "Epoch 801/30000 Training Loss: 0.12641195952892303\n",
      "Epoch 802/30000 Training Loss: 0.13118787109851837\n",
      "Epoch 803/30000 Training Loss: 0.10123944282531738\n",
      "Epoch 804/30000 Training Loss: 0.13243074715137482\n",
      "Epoch 805/30000 Training Loss: 0.1353725790977478\n",
      "Epoch 806/30000 Training Loss: 0.11298120766878128\n",
      "Epoch 807/30000 Training Loss: 0.11083296686410904\n",
      "Epoch 808/30000 Training Loss: 0.11125350743532181\n",
      "Epoch 809/30000 Training Loss: 0.122370146214962\n",
      "Epoch 810/30000 Training Loss: 0.13828247785568237\n",
      "Epoch 810/30000 Validation Loss: 0.13024991750717163\n",
      "Epoch 811/30000 Training Loss: 0.11092958599328995\n",
      "Epoch 812/30000 Training Loss: 0.11213227361440659\n",
      "Epoch 813/30000 Training Loss: 0.09879330545663834\n",
      "Epoch 814/30000 Training Loss: 0.09867899864912033\n",
      "Epoch 815/30000 Training Loss: 0.1451987326145172\n",
      "Epoch 816/30000 Training Loss: 0.10446599870920181\n",
      "Epoch 817/30000 Training Loss: 0.13064415752887726\n",
      "Epoch 818/30000 Training Loss: 0.12139488011598587\n",
      "Epoch 819/30000 Training Loss: 0.12322895973920822\n",
      "Epoch 820/30000 Training Loss: 0.1004883423447609\n",
      "Epoch 820/30000 Validation Loss: 0.11865776777267456\n",
      "Epoch 821/30000 Training Loss: 0.13123558461666107\n",
      "Epoch 822/30000 Training Loss: 0.11699726432561874\n",
      "Epoch 823/30000 Training Loss: 0.11715590208768845\n",
      "Epoch 824/30000 Training Loss: 0.09091033786535263\n",
      "Epoch 825/30000 Training Loss: 0.12480881065130234\n",
      "Epoch 826/30000 Training Loss: 0.09141785651445389\n",
      "Epoch 827/30000 Training Loss: 0.11377429962158203\n",
      "Epoch 828/30000 Training Loss: 0.10977903753519058\n",
      "Epoch 829/30000 Training Loss: 0.10809257626533508\n",
      "Epoch 830/30000 Training Loss: 0.11273563653230667\n",
      "Epoch 830/30000 Validation Loss: 0.10600241273641586\n",
      "Epoch 831/30000 Training Loss: 0.14166991412639618\n",
      "Epoch 832/30000 Training Loss: 0.12057828158140182\n",
      "Epoch 833/30000 Training Loss: 0.12228085845708847\n",
      "Epoch 834/30000 Training Loss: 0.10050340741872787\n",
      "Epoch 835/30000 Training Loss: 0.09958847612142563\n",
      "Epoch 836/30000 Training Loss: 0.1366492062807083\n",
      "Epoch 837/30000 Training Loss: 0.09432309865951538\n",
      "Epoch 838/30000 Training Loss: 0.11508005112409592\n",
      "Epoch 839/30000 Training Loss: 0.09400829672813416\n",
      "Epoch 840/30000 Training Loss: 0.13280387222766876\n",
      "Epoch 840/30000 Validation Loss: 0.09793581813573837\n",
      "Epoch 841/30000 Training Loss: 0.13753579556941986\n",
      "Epoch 842/30000 Training Loss: 0.11359446495771408\n",
      "Epoch 843/30000 Training Loss: 0.11461180448532104\n",
      "Epoch 844/30000 Training Loss: 0.13204719126224518\n",
      "Epoch 845/30000 Training Loss: 0.10266423225402832\n",
      "Epoch 846/30000 Training Loss: 0.14381276071071625\n",
      "Epoch 847/30000 Training Loss: 0.11352628469467163\n",
      "Epoch 848/30000 Training Loss: 0.1250333935022354\n",
      "Epoch 849/30000 Training Loss: 0.10552168637514114\n",
      "Epoch 850/30000 Training Loss: 0.14663377404212952\n",
      "Epoch 850/30000 Validation Loss: 0.09806591272354126\n",
      "Epoch 851/30000 Training Loss: 0.12717628479003906\n",
      "Epoch 852/30000 Training Loss: 0.11591299623250961\n",
      "Epoch 853/30000 Training Loss: 0.09439324587583542\n",
      "Epoch 854/30000 Training Loss: 0.09922102838754654\n",
      "Epoch 855/30000 Training Loss: 0.12138409167528152\n",
      "Epoch 856/30000 Training Loss: 0.12350106239318848\n",
      "Epoch 857/30000 Training Loss: 0.11307093501091003\n",
      "Epoch 858/30000 Training Loss: 0.09943646192550659\n",
      "Epoch 859/30000 Training Loss: 0.13995841145515442\n",
      "Epoch 860/30000 Training Loss: 0.10698915272951126\n",
      "Epoch 860/30000 Validation Loss: 0.1196196898818016\n",
      "Epoch 861/30000 Training Loss: 0.09844318777322769\n",
      "Epoch 862/30000 Training Loss: 0.12185269594192505\n",
      "Epoch 863/30000 Training Loss: 0.11815494298934937\n",
      "Epoch 864/30000 Training Loss: 0.1081627830862999\n",
      "Epoch 865/30000 Training Loss: 0.11633589118719101\n",
      "Epoch 866/30000 Training Loss: 0.1037878766655922\n",
      "Epoch 867/30000 Training Loss: 0.11781417578458786\n",
      "Epoch 868/30000 Training Loss: 0.11459872871637344\n",
      "Epoch 869/30000 Training Loss: 0.1232224702835083\n",
      "Epoch 870/30000 Training Loss: 0.12287700176239014\n",
      "Epoch 870/30000 Validation Loss: 0.11518970131874084\n",
      "Epoch 871/30000 Training Loss: 0.07849320769309998\n",
      "Epoch 872/30000 Training Loss: 0.10923761129379272\n",
      "Epoch 873/30000 Training Loss: 0.12939061224460602\n",
      "Epoch 874/30000 Training Loss: 0.10983524471521378\n",
      "Epoch 875/30000 Training Loss: 0.1232120469212532\n",
      "Epoch 876/30000 Training Loss: 0.11309128999710083\n",
      "Epoch 877/30000 Training Loss: 0.12015429884195328\n",
      "Epoch 878/30000 Training Loss: 0.10865416377782822\n",
      "Epoch 879/30000 Training Loss: 0.10852334648370743\n",
      "Epoch 880/30000 Training Loss: 0.11469972133636475\n",
      "Epoch 880/30000 Validation Loss: 0.12155493348836899\n",
      "Epoch 881/30000 Training Loss: 0.11434672027826309\n",
      "Epoch 882/30000 Training Loss: 0.09366639703512192\n",
      "Epoch 883/30000 Training Loss: 0.12710882723331451\n",
      "Epoch 884/30000 Training Loss: 0.0926981046795845\n",
      "Epoch 885/30000 Training Loss: 0.12765653431415558\n",
      "Epoch 886/30000 Training Loss: 0.08786531537771225\n",
      "Epoch 887/30000 Training Loss: 0.09920749813318253\n",
      "Epoch 888/30000 Training Loss: 0.08114892989397049\n",
      "Epoch 889/30000 Training Loss: 0.11782020330429077\n",
      "Epoch 890/30000 Training Loss: 0.14141353964805603\n",
      "Epoch 890/30000 Validation Loss: 0.12144643068313599\n",
      "Epoch 891/30000 Training Loss: 0.11476811021566391\n",
      "Epoch 892/30000 Training Loss: 0.11195961385965347\n",
      "Epoch 893/30000 Training Loss: 0.1022343710064888\n",
      "Epoch 894/30000 Training Loss: 0.08318374305963516\n",
      "Epoch 895/30000 Training Loss: 0.11426997929811478\n",
      "Epoch 896/30000 Training Loss: 0.12944386899471283\n",
      "Epoch 897/30000 Training Loss: 0.10665751248598099\n",
      "Epoch 898/30000 Training Loss: 0.13230295479297638\n",
      "Epoch 899/30000 Training Loss: 0.09810785204172134\n",
      "Epoch 900/30000 Training Loss: 0.09694977849721909\n",
      "Epoch 900/30000 Validation Loss: 0.12298282235860825\n",
      "Epoch 901/30000 Training Loss: 0.10683593153953552\n",
      "Epoch 902/30000 Training Loss: 0.09073793143033981\n",
      "Epoch 903/30000 Training Loss: 0.12306830286979675\n",
      "Epoch 904/30000 Training Loss: 0.1304500848054886\n",
      "Epoch 905/30000 Training Loss: 0.09991401433944702\n",
      "Epoch 906/30000 Training Loss: 0.1420658826828003\n",
      "Epoch 907/30000 Training Loss: 0.0956825390458107\n",
      "Epoch 908/30000 Training Loss: 0.10099554061889648\n",
      "Epoch 909/30000 Training Loss: 0.11717307567596436\n",
      "Epoch 910/30000 Training Loss: 0.09500988572835922\n",
      "Epoch 910/30000 Validation Loss: 0.09299667924642563\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.09299667924642563<=============\n",
      "Epoch 911/30000 Training Loss: 0.1198430061340332\n",
      "Epoch 912/30000 Training Loss: 0.09805852174758911\n",
      "Epoch 913/30000 Training Loss: 0.1220044493675232\n",
      "Epoch 914/30000 Training Loss: 0.10097690671682358\n",
      "Epoch 915/30000 Training Loss: 0.129072904586792\n",
      "Epoch 916/30000 Training Loss: 0.10144621133804321\n",
      "Epoch 917/30000 Training Loss: 0.10907480120658875\n",
      "Epoch 918/30000 Training Loss: 0.12562315165996552\n",
      "Epoch 919/30000 Training Loss: 0.10514890402555466\n",
      "Epoch 920/30000 Training Loss: 0.11312499642372131\n",
      "Epoch 920/30000 Validation Loss: 0.13426916301250458\n",
      "Epoch 921/30000 Training Loss: 0.11072716861963272\n",
      "Epoch 922/30000 Training Loss: 0.11336018890142441\n",
      "Epoch 923/30000 Training Loss: 0.09989143162965775\n",
      "Epoch 924/30000 Training Loss: 0.09923377633094788\n",
      "Epoch 925/30000 Training Loss: 0.08490858227014542\n",
      "Epoch 926/30000 Training Loss: 0.10734093934297562\n",
      "Epoch 927/30000 Training Loss: 0.11475289613008499\n",
      "Epoch 928/30000 Training Loss: 0.12844784557819366\n",
      "Epoch 929/30000 Training Loss: 0.09767067432403564\n",
      "Epoch 930/30000 Training Loss: 0.111756831407547\n",
      "Epoch 930/30000 Validation Loss: 0.14198023080825806\n",
      "Epoch 931/30000 Training Loss: 0.11744609475135803\n",
      "Epoch 932/30000 Training Loss: 0.08599945157766342\n",
      "Epoch 933/30000 Training Loss: 0.10383991152048111\n",
      "Epoch 934/30000 Training Loss: 0.09793886542320251\n",
      "Epoch 935/30000 Training Loss: 0.1114688441157341\n",
      "Epoch 936/30000 Training Loss: 0.10636051744222641\n",
      "Epoch 937/30000 Training Loss: 0.10935818403959274\n",
      "Epoch 938/30000 Training Loss: 0.11814316362142563\n",
      "Epoch 939/30000 Training Loss: 0.09578701853752136\n",
      "Epoch 940/30000 Training Loss: 0.12595532834529877\n",
      "Epoch 940/30000 Validation Loss: 0.10504563897848129\n",
      "Epoch 941/30000 Training Loss: 0.09667424112558365\n",
      "Epoch 942/30000 Training Loss: 0.1283797174692154\n",
      "Epoch 943/30000 Training Loss: 0.09100814908742905\n",
      "Epoch 944/30000 Training Loss: 0.09568019956350327\n",
      "Epoch 945/30000 Training Loss: 0.11303571611642838\n",
      "Epoch 946/30000 Training Loss: 0.10401890426874161\n",
      "Epoch 947/30000 Training Loss: 0.1353253722190857\n",
      "Epoch 948/30000 Training Loss: 0.1046418845653534\n",
      "Epoch 949/30000 Training Loss: 0.14579211175441742\n",
      "Epoch 950/30000 Training Loss: 0.10194152593612671\n",
      "Epoch 950/30000 Validation Loss: 0.10856323689222336\n",
      "Epoch 951/30000 Training Loss: 0.09308155626058578\n",
      "Epoch 952/30000 Training Loss: 0.11620619148015976\n",
      "Epoch 953/30000 Training Loss: 0.09650426357984543\n",
      "Epoch 954/30000 Training Loss: 0.11309187859296799\n",
      "Epoch 955/30000 Training Loss: 0.13171084225177765\n",
      "Epoch 956/30000 Training Loss: 0.1295849233865738\n",
      "Epoch 957/30000 Training Loss: 0.0850532129406929\n",
      "Epoch 958/30000 Training Loss: 0.13155747950077057\n",
      "Epoch 959/30000 Training Loss: 0.1097036600112915\n",
      "Epoch 960/30000 Training Loss: 0.11121636629104614\n",
      "Epoch 960/30000 Validation Loss: 0.11698704957962036\n",
      "Epoch 961/30000 Training Loss: 0.1163289025425911\n",
      "Epoch 962/30000 Training Loss: 0.11501151323318481\n",
      "Epoch 963/30000 Training Loss: 0.09306303411722183\n",
      "Epoch 964/30000 Training Loss: 0.11015018075704575\n",
      "Epoch 965/30000 Training Loss: 0.10629010945558548\n",
      "Epoch 966/30000 Training Loss: 0.11577684432268143\n",
      "Epoch 967/30000 Training Loss: 0.10326104611158371\n",
      "Epoch 968/30000 Training Loss: 0.10768171399831772\n",
      "Epoch 969/30000 Training Loss: 0.09424680471420288\n",
      "Epoch 970/30000 Training Loss: 0.10584897547960281\n",
      "Epoch 970/30000 Validation Loss: 0.11113494634628296\n",
      "Epoch 971/30000 Training Loss: 0.10453889518976212\n",
      "Epoch 972/30000 Training Loss: 0.10080668330192566\n",
      "Epoch 973/30000 Training Loss: 0.10805302858352661\n",
      "Epoch 974/30000 Training Loss: 0.10618119686841965\n",
      "Epoch 975/30000 Training Loss: 0.11965586990118027\n",
      "Epoch 976/30000 Training Loss: 0.10700154304504395\n",
      "Epoch 977/30000 Training Loss: 0.12646602094173431\n",
      "Epoch 978/30000 Training Loss: 0.1067967638373375\n",
      "Epoch 979/30000 Training Loss: 0.09622988104820251\n",
      "Epoch 980/30000 Training Loss: 0.09466128796339035\n",
      "Epoch 980/30000 Validation Loss: 0.08396363258361816\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08396363258361816<=============\n",
      "Epoch 981/30000 Training Loss: 0.13473773002624512\n",
      "Epoch 982/30000 Training Loss: 0.1166776642203331\n",
      "Epoch 983/30000 Training Loss: 0.12533356249332428\n",
      "Epoch 984/30000 Training Loss: 0.1006731390953064\n",
      "Epoch 985/30000 Training Loss: 0.10782819986343384\n",
      "Epoch 986/30000 Training Loss: 0.1029723659157753\n",
      "Epoch 987/30000 Training Loss: 0.1101810410618782\n",
      "Epoch 988/30000 Training Loss: 0.13222302496433258\n",
      "Epoch 989/30000 Training Loss: 0.10264012217521667\n",
      "Epoch 990/30000 Training Loss: 0.10041424632072449\n",
      "Epoch 990/30000 Validation Loss: 0.11245447397232056\n",
      "Epoch 991/30000 Training Loss: 0.110617995262146\n",
      "Epoch 992/30000 Training Loss: 0.12065499275922775\n",
      "Epoch 993/30000 Training Loss: 0.10624376684427261\n",
      "Epoch 994/30000 Training Loss: 0.10513518005609512\n",
      "Epoch 995/30000 Training Loss: 0.11788501590490341\n",
      "Epoch 996/30000 Training Loss: 0.1033315435051918\n",
      "Epoch 997/30000 Training Loss: 0.11880362033843994\n",
      "Epoch 998/30000 Training Loss: 0.11795958876609802\n",
      "Epoch 999/30000 Training Loss: 0.12614451348781586\n",
      "Epoch 1000/30000 Training Loss: 0.13242872059345245\n",
      "Epoch 1000/30000 Validation Loss: 0.08450132608413696\n",
      "Epoch 1001/30000 Training Loss: 0.1097746416926384\n",
      "Epoch 1002/30000 Training Loss: 0.10393313318490982\n",
      "Epoch 1003/30000 Training Loss: 0.10971689224243164\n",
      "Epoch 1004/30000 Training Loss: 0.11084987968206406\n",
      "Epoch 1005/30000 Training Loss: 0.10850026458501816\n",
      "Epoch 1006/30000 Training Loss: 0.09832581132650375\n",
      "Epoch 1007/30000 Training Loss: 0.09611291438341141\n",
      "Epoch 1008/30000 Training Loss: 0.09130477160215378\n",
      "Epoch 1009/30000 Training Loss: 0.1105370819568634\n",
      "Epoch 1010/30000 Training Loss: 0.09745529294013977\n",
      "Epoch 1010/30000 Validation Loss: 0.10449644178152084\n",
      "Epoch 1011/30000 Training Loss: 0.07597172260284424\n",
      "Epoch 1012/30000 Training Loss: 0.11041318625211716\n",
      "Epoch 1013/30000 Training Loss: 0.10101597756147385\n",
      "Epoch 1014/30000 Training Loss: 0.09886709600687027\n",
      "Epoch 1015/30000 Training Loss: 0.1112142875790596\n",
      "Epoch 1016/30000 Training Loss: 0.113218754529953\n",
      "Epoch 1017/30000 Training Loss: 0.10928044468164444\n",
      "Epoch 1018/30000 Training Loss: 0.11016464233398438\n",
      "Epoch 1019/30000 Training Loss: 0.082111656665802\n",
      "Epoch 1020/30000 Training Loss: 0.11355964094400406\n",
      "Epoch 1020/30000 Validation Loss: 0.13694548606872559\n",
      "Epoch 1021/30000 Training Loss: 0.12724006175994873\n",
      "Epoch 1022/30000 Training Loss: 0.10553697496652603\n",
      "Epoch 1023/30000 Training Loss: 0.12528559565544128\n",
      "Epoch 1024/30000 Training Loss: 0.10850480943918228\n",
      "Epoch 1025/30000 Training Loss: 0.1255669742822647\n",
      "Epoch 1026/30000 Training Loss: 0.10685086250305176\n",
      "Epoch 1027/30000 Training Loss: 0.10594186186790466\n",
      "Epoch 1028/30000 Training Loss: 0.128886416554451\n",
      "Epoch 1029/30000 Training Loss: 0.10469230264425278\n",
      "Epoch 1030/30000 Training Loss: 0.10799839347600937\n",
      "Epoch 1030/30000 Validation Loss: 0.11727619916200638\n",
      "Epoch 1031/30000 Training Loss: 0.10451414436101913\n",
      "Epoch 1032/30000 Training Loss: 0.11336646229028702\n",
      "Epoch 1033/30000 Training Loss: 0.14591707289218903\n",
      "Epoch 1034/30000 Training Loss: 0.09386024624109268\n",
      "Epoch 1035/30000 Training Loss: 0.12339478731155396\n",
      "Epoch 1036/30000 Training Loss: 0.12553167343139648\n",
      "Epoch 1037/30000 Training Loss: 0.11503870040178299\n",
      "Epoch 1038/30000 Training Loss: 0.10210296511650085\n",
      "Epoch 1039/30000 Training Loss: 0.09761068969964981\n",
      "Epoch 1040/30000 Training Loss: 0.09542662650346756\n",
      "Epoch 1040/30000 Validation Loss: 0.10515718907117844\n",
      "Epoch 1041/30000 Training Loss: 0.13566000759601593\n",
      "Epoch 1042/30000 Training Loss: 0.10653400421142578\n",
      "Epoch 1043/30000 Training Loss: 0.12005513906478882\n",
      "Epoch 1044/30000 Training Loss: 0.09905422478914261\n",
      "Epoch 1045/30000 Training Loss: 0.10997924953699112\n",
      "Epoch 1046/30000 Training Loss: 0.10796476155519485\n",
      "Epoch 1047/30000 Training Loss: 0.08852258324623108\n",
      "Epoch 1048/30000 Training Loss: 0.08030643314123154\n",
      "Epoch 1049/30000 Training Loss: 0.08979520946741104\n",
      "Epoch 1050/30000 Training Loss: 0.09666106849908829\n",
      "Epoch 1050/30000 Validation Loss: 0.09194960445165634\n",
      "Epoch 1051/30000 Training Loss: 0.11257511377334595\n",
      "Epoch 1052/30000 Training Loss: 0.12971995770931244\n",
      "Epoch 1053/30000 Training Loss: 0.11097443103790283\n",
      "Epoch 1054/30000 Training Loss: 0.10245833545923233\n",
      "Epoch 1055/30000 Training Loss: 0.09254895895719528\n",
      "Epoch 1056/30000 Training Loss: 0.11083913594484329\n",
      "Epoch 1057/30000 Training Loss: 0.08856338262557983\n",
      "Epoch 1058/30000 Training Loss: 0.12182947993278503\n",
      "Epoch 1059/30000 Training Loss: 0.08454232662916183\n",
      "Epoch 1060/30000 Training Loss: 0.11466865986585617\n",
      "Epoch 1060/30000 Validation Loss: 0.09311363101005554\n",
      "Epoch 1061/30000 Training Loss: 0.1144368126988411\n",
      "Epoch 1062/30000 Training Loss: 0.09248141199350357\n",
      "Epoch 1063/30000 Training Loss: 0.11308625340461731\n",
      "Epoch 1064/30000 Training Loss: 0.08261863142251968\n",
      "Epoch 1065/30000 Training Loss: 0.10851099342107773\n",
      "Epoch 1066/30000 Training Loss: 0.09868060797452927\n",
      "Epoch 1067/30000 Training Loss: 0.0817866101861\n",
      "Epoch 1068/30000 Training Loss: 0.09108945727348328\n",
      "Epoch 1069/30000 Training Loss: 0.12097319960594177\n",
      "Epoch 1070/30000 Training Loss: 0.12035509198904037\n",
      "Epoch 1070/30000 Validation Loss: 0.11723915487527847\n",
      "Epoch 1071/30000 Training Loss: 0.10362014919519424\n",
      "Epoch 1072/30000 Training Loss: 0.10985938459634781\n",
      "Epoch 1073/30000 Training Loss: 0.09069686383008957\n",
      "Epoch 1074/30000 Training Loss: 0.0942041352391243\n",
      "Epoch 1075/30000 Training Loss: 0.08370685577392578\n",
      "Epoch 1076/30000 Training Loss: 0.11241564899682999\n",
      "Epoch 1077/30000 Training Loss: 0.09699058532714844\n",
      "Epoch 1078/30000 Training Loss: 0.12118007987737656\n",
      "Epoch 1079/30000 Training Loss: 0.07191897183656693\n",
      "Epoch 1080/30000 Training Loss: 0.08999869227409363\n",
      "Epoch 1080/30000 Validation Loss: 0.11546897888183594\n",
      "Epoch 1081/30000 Training Loss: 0.07872343808412552\n",
      "Epoch 1082/30000 Training Loss: 0.12294980883598328\n",
      "Epoch 1083/30000 Training Loss: 0.10217069834470749\n",
      "Epoch 1084/30000 Training Loss: 0.06992237269878387\n",
      "Epoch 1085/30000 Training Loss: 0.11676344275474548\n",
      "Epoch 1086/30000 Training Loss: 0.07613480091094971\n",
      "Epoch 1087/30000 Training Loss: 0.10659202188253403\n",
      "Epoch 1088/30000 Training Loss: 0.10857584327459335\n",
      "Epoch 1089/30000 Training Loss: 0.1394120156764984\n",
      "Epoch 1090/30000 Training Loss: 0.09205448627471924\n",
      "Epoch 1090/30000 Validation Loss: 0.09070395678281784\n",
      "Epoch 1091/30000 Training Loss: 0.08748636394739151\n",
      "Epoch 1092/30000 Training Loss: 0.13035552203655243\n",
      "Epoch 1093/30000 Training Loss: 0.09913122653961182\n",
      "Epoch 1094/30000 Training Loss: 0.10061714798212051\n",
      "Epoch 1095/30000 Training Loss: 0.1202472671866417\n",
      "Epoch 1096/30000 Training Loss: 0.08112967014312744\n",
      "Epoch 1097/30000 Training Loss: 0.1354246586561203\n",
      "Epoch 1098/30000 Training Loss: 0.09197299927473068\n",
      "Epoch 1099/30000 Training Loss: 0.12042494863271713\n",
      "Epoch 1100/30000 Training Loss: 0.09165775775909424\n",
      "Epoch 1100/30000 Validation Loss: 0.10493588447570801\n",
      "Epoch 1101/30000 Training Loss: 0.1011737659573555\n",
      "Epoch 1102/30000 Training Loss: 0.09811926633119583\n",
      "Epoch 1103/30000 Training Loss: 0.1338677853345871\n",
      "Epoch 1104/30000 Training Loss: 0.11221518367528915\n",
      "Epoch 1105/30000 Training Loss: 0.08931723237037659\n",
      "Epoch 1106/30000 Training Loss: 0.12809310853481293\n",
      "Epoch 1107/30000 Training Loss: 0.10648199915885925\n",
      "Epoch 1108/30000 Training Loss: 0.09732910245656967\n",
      "Epoch 1109/30000 Training Loss: 0.12058943510055542\n",
      "Epoch 1110/30000 Training Loss: 0.1341257244348526\n",
      "Epoch 1110/30000 Validation Loss: 0.1404150277376175\n",
      "Epoch 1111/30000 Training Loss: 0.10786888748407364\n",
      "Epoch 1112/30000 Training Loss: 0.09393081068992615\n",
      "Epoch 1113/30000 Training Loss: 0.0896729826927185\n",
      "Epoch 1114/30000 Training Loss: 0.10102730989456177\n",
      "Epoch 1115/30000 Training Loss: 0.1104392483830452\n",
      "Epoch 1116/30000 Training Loss: 0.12000974267721176\n",
      "Epoch 1117/30000 Training Loss: 0.12241960316896439\n",
      "Epoch 1118/30000 Training Loss: 0.07642307132482529\n",
      "Epoch 1119/30000 Training Loss: 0.10342324525117874\n",
      "Epoch 1120/30000 Training Loss: 0.08352136611938477\n",
      "Epoch 1120/30000 Validation Loss: 0.08227580040693283\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08227580040693283<=============\n",
      "Epoch 1121/30000 Training Loss: 0.09909889101982117\n",
      "Epoch 1122/30000 Training Loss: 0.111028753221035\n",
      "Epoch 1123/30000 Training Loss: 0.08454021066427231\n",
      "Epoch 1124/30000 Training Loss: 0.10020173341035843\n",
      "Epoch 1125/30000 Training Loss: 0.10394806414842606\n",
      "Epoch 1126/30000 Training Loss: 0.11630696058273315\n",
      "Epoch 1127/30000 Training Loss: 0.08403632789850235\n",
      "Epoch 1128/30000 Training Loss: 0.12079083919525146\n",
      "Epoch 1129/30000 Training Loss: 0.07804559916257858\n",
      "Epoch 1130/30000 Training Loss: 0.09776922315359116\n",
      "Epoch 1130/30000 Validation Loss: 0.12010487169027328\n",
      "Epoch 1131/30000 Training Loss: 0.10978338867425919\n",
      "Epoch 1132/30000 Training Loss: 0.12381043285131454\n",
      "Epoch 1133/30000 Training Loss: 0.13427402079105377\n",
      "Epoch 1134/30000 Training Loss: 0.09759163856506348\n",
      "Epoch 1135/30000 Training Loss: 0.11427178233861923\n",
      "Epoch 1136/30000 Training Loss: 0.10022372752428055\n",
      "Epoch 1137/30000 Training Loss: 0.10878141969442368\n",
      "Epoch 1138/30000 Training Loss: 0.09842872619628906\n",
      "Epoch 1139/30000 Training Loss: 0.11050406843423843\n",
      "Epoch 1140/30000 Training Loss: 0.10658606141805649\n",
      "Epoch 1140/30000 Validation Loss: 0.11227325350046158\n",
      "Epoch 1141/30000 Training Loss: 0.1312587559223175\n",
      "Epoch 1142/30000 Training Loss: 0.08475927263498306\n",
      "Epoch 1143/30000 Training Loss: 0.09328695386648178\n",
      "Epoch 1144/30000 Training Loss: 0.10496745258569717\n",
      "Epoch 1145/30000 Training Loss: 0.09579811245203018\n",
      "Epoch 1146/30000 Training Loss: 0.10953819751739502\n",
      "Epoch 1147/30000 Training Loss: 0.09729320555925369\n",
      "Epoch 1148/30000 Training Loss: 0.10152322053909302\n",
      "Epoch 1149/30000 Training Loss: 0.14141471683979034\n",
      "Epoch 1150/30000 Training Loss: 0.10846055299043655\n",
      "Epoch 1150/30000 Validation Loss: 0.10465186089277267\n",
      "Epoch 1151/30000 Training Loss: 0.11213237792253494\n",
      "Epoch 1152/30000 Training Loss: 0.09050122648477554\n",
      "Epoch 1153/30000 Training Loss: 0.13379226624965668\n",
      "Epoch 1154/30000 Training Loss: 0.10079919546842575\n",
      "Epoch 1155/30000 Training Loss: 0.11302163451910019\n",
      "Epoch 1156/30000 Training Loss: 0.11573942750692368\n",
      "Epoch 1157/30000 Training Loss: 0.08671493083238602\n",
      "Epoch 1158/30000 Training Loss: 0.08131036162376404\n",
      "Epoch 1159/30000 Training Loss: 0.11119874566793442\n",
      "Epoch 1160/30000 Training Loss: 0.08098956197500229\n",
      "Epoch 1160/30000 Validation Loss: 0.11308044195175171\n",
      "Epoch 1161/30000 Training Loss: 0.09930003434419632\n",
      "Epoch 1162/30000 Training Loss: 0.11882016807794571\n",
      "Epoch 1163/30000 Training Loss: 0.11387097835540771\n",
      "Epoch 1164/30000 Training Loss: 0.10873791575431824\n",
      "Epoch 1165/30000 Training Loss: 0.10123845189809799\n",
      "Epoch 1166/30000 Training Loss: 0.10728558152914047\n",
      "Epoch 1167/30000 Training Loss: 0.10815795511007309\n",
      "Epoch 1168/30000 Training Loss: 0.09786321967840195\n",
      "Epoch 1169/30000 Training Loss: 0.09458097070455551\n",
      "Epoch 1170/30000 Training Loss: 0.10134503990411758\n",
      "Epoch 1170/30000 Validation Loss: 0.09227774292230606\n",
      "Epoch 1171/30000 Training Loss: 0.08672789484262466\n",
      "Epoch 1172/30000 Training Loss: 0.09200679510831833\n",
      "Epoch 1173/30000 Training Loss: 0.10529953241348267\n",
      "Epoch 1174/30000 Training Loss: 0.0856708288192749\n",
      "Epoch 1175/30000 Training Loss: 0.10107570886611938\n",
      "Epoch 1176/30000 Training Loss: 0.09802465885877609\n",
      "Epoch 1177/30000 Training Loss: 0.11263994127511978\n",
      "Epoch 1178/30000 Training Loss: 0.0994546040892601\n",
      "Epoch 1179/30000 Training Loss: 0.1078294888138771\n",
      "Epoch 1180/30000 Training Loss: 0.09521260857582092\n",
      "Epoch 1180/30000 Validation Loss: 0.09340714663267136\n",
      "Epoch 1181/30000 Training Loss: 0.1236700639128685\n",
      "Epoch 1182/30000 Training Loss: 0.10552028566598892\n",
      "Epoch 1183/30000 Training Loss: 0.12250178307294846\n",
      "Epoch 1184/30000 Training Loss: 0.10894153267145157\n",
      "Epoch 1185/30000 Training Loss: 0.12021854519844055\n",
      "Epoch 1186/30000 Training Loss: 0.12301886081695557\n",
      "Epoch 1187/30000 Training Loss: 0.11505887657403946\n",
      "Epoch 1188/30000 Training Loss: 0.1007656380534172\n",
      "Epoch 1189/30000 Training Loss: 0.09671439975500107\n",
      "Epoch 1190/30000 Training Loss: 0.09695293754339218\n",
      "Epoch 1190/30000 Validation Loss: 0.09472114592790604\n",
      "Epoch 1191/30000 Training Loss: 0.11317167431116104\n",
      "Epoch 1192/30000 Training Loss: 0.11069247871637344\n",
      "Epoch 1193/30000 Training Loss: 0.10653432458639145\n",
      "Epoch 1194/30000 Training Loss: 0.08147837966680527\n",
      "Epoch 1195/30000 Training Loss: 0.12089095264673233\n",
      "Epoch 1196/30000 Training Loss: 0.09145311266183853\n",
      "Epoch 1197/30000 Training Loss: 0.10738348960876465\n",
      "Epoch 1198/30000 Training Loss: 0.08640829473733902\n",
      "Epoch 1199/30000 Training Loss: 0.09201091527938843\n",
      "Epoch 1200/30000 Training Loss: 0.09521088004112244\n",
      "Epoch 1200/30000 Validation Loss: 0.0843103751540184\n",
      "Epoch 1201/30000 Training Loss: 0.0928407832980156\n",
      "Epoch 1202/30000 Training Loss: 0.08502232283353806\n",
      "Epoch 1203/30000 Training Loss: 0.1082368716597557\n",
      "Epoch 1204/30000 Training Loss: 0.13264888525009155\n",
      "Epoch 1205/30000 Training Loss: 0.09619592875242233\n",
      "Epoch 1206/30000 Training Loss: 0.09429935365915298\n",
      "Epoch 1207/30000 Training Loss: 0.0791792944073677\n",
      "Epoch 1208/30000 Training Loss: 0.1110067144036293\n",
      "Epoch 1209/30000 Training Loss: 0.09608208388090134\n",
      "Epoch 1210/30000 Training Loss: 0.1344924420118332\n",
      "Epoch 1210/30000 Validation Loss: 0.10765031725168228\n",
      "Epoch 1211/30000 Training Loss: 0.09308630973100662\n",
      "Epoch 1212/30000 Training Loss: 0.1042020246386528\n",
      "Epoch 1213/30000 Training Loss: 0.09042318910360336\n",
      "Epoch 1214/30000 Training Loss: 0.10595620423555374\n",
      "Epoch 1215/30000 Training Loss: 0.09477806091308594\n",
      "Epoch 1216/30000 Training Loss: 0.1473381668329239\n",
      "Epoch 1217/30000 Training Loss: 0.10831359773874283\n",
      "Epoch 1218/30000 Training Loss: 0.09701720625162125\n",
      "Epoch 1219/30000 Training Loss: 0.10757923871278763\n",
      "Epoch 1220/30000 Training Loss: 0.09322170168161392\n",
      "Epoch 1220/30000 Validation Loss: 0.09685387462377548\n",
      "Epoch 1221/30000 Training Loss: 0.10524282604455948\n",
      "Epoch 1222/30000 Training Loss: 0.14245717227458954\n",
      "Epoch 1223/30000 Training Loss: 0.10468290001153946\n",
      "Epoch 1224/30000 Training Loss: 0.10014554858207703\n",
      "Epoch 1225/30000 Training Loss: 0.10731867700815201\n",
      "Epoch 1226/30000 Training Loss: 0.11258681863546371\n",
      "Epoch 1227/30000 Training Loss: 0.13103137910366058\n",
      "Epoch 1228/30000 Training Loss: 0.09903093427419662\n",
      "Epoch 1229/30000 Training Loss: 0.08918308466672897\n",
      "Epoch 1230/30000 Training Loss: 0.08916088938713074\n",
      "Epoch 1230/30000 Validation Loss: 0.09796015173196793\n",
      "Epoch 1231/30000 Training Loss: 0.09588459879159927\n",
      "Epoch 1232/30000 Training Loss: 0.11037618666887283\n",
      "Epoch 1233/30000 Training Loss: 0.09462254494428635\n",
      "Epoch 1234/30000 Training Loss: 0.10439292341470718\n",
      "Epoch 1235/30000 Training Loss: 0.10074708610773087\n",
      "Epoch 1236/30000 Training Loss: 0.09607704728841782\n",
      "Epoch 1237/30000 Training Loss: 0.1098882332444191\n",
      "Epoch 1238/30000 Training Loss: 0.0980978012084961\n",
      "Epoch 1239/30000 Training Loss: 0.1390455961227417\n",
      "Epoch 1240/30000 Training Loss: 0.1004069447517395\n",
      "Epoch 1240/30000 Validation Loss: 0.10759180039167404\n",
      "Epoch 1241/30000 Training Loss: 0.08268872648477554\n",
      "Epoch 1242/30000 Training Loss: 0.10006150603294373\n",
      "Epoch 1243/30000 Training Loss: 0.1278841197490692\n",
      "Epoch 1244/30000 Training Loss: 0.12047012895345688\n",
      "Epoch 1245/30000 Training Loss: 0.14351226389408112\n",
      "Epoch 1246/30000 Training Loss: 0.11445868015289307\n",
      "Epoch 1247/30000 Training Loss: 0.1323404461145401\n",
      "Epoch 1248/30000 Training Loss: 0.09514091163873672\n",
      "Epoch 1249/30000 Training Loss: 0.10166708379983902\n",
      "Epoch 1250/30000 Training Loss: 0.09504873305559158\n",
      "Epoch 1250/30000 Validation Loss: 0.09876837581396103\n",
      "Epoch 1251/30000 Training Loss: 0.11175539344549179\n",
      "Epoch 1252/30000 Training Loss: 0.09924165159463882\n",
      "Epoch 1253/30000 Training Loss: 0.08179645240306854\n",
      "Epoch 1254/30000 Training Loss: 0.11329539865255356\n",
      "Epoch 1255/30000 Training Loss: 0.10788482427597046\n",
      "Epoch 1256/30000 Training Loss: 0.09155184030532837\n",
      "Epoch 1257/30000 Training Loss: 0.10525190830230713\n",
      "Epoch 1258/30000 Training Loss: 0.1058628186583519\n",
      "Epoch 1259/30000 Training Loss: 0.09770945459604263\n",
      "Epoch 1260/30000 Training Loss: 0.09565196186304092\n",
      "Epoch 1260/30000 Validation Loss: 0.09696849435567856\n",
      "Epoch 1261/30000 Training Loss: 0.08037076145410538\n",
      "Epoch 1262/30000 Training Loss: 0.10418415069580078\n",
      "Epoch 1263/30000 Training Loss: 0.08655920624732971\n",
      "Epoch 1264/30000 Training Loss: 0.09409946948289871\n",
      "Epoch 1265/30000 Training Loss: 0.09805181622505188\n",
      "Epoch 1266/30000 Training Loss: 0.10116392374038696\n",
      "Epoch 1267/30000 Training Loss: 0.08352785557508469\n",
      "Epoch 1268/30000 Training Loss: 0.1131957545876503\n",
      "Epoch 1269/30000 Training Loss: 0.0954211950302124\n",
      "Epoch 1270/30000 Training Loss: 0.12902140617370605\n",
      "Epoch 1270/30000 Validation Loss: 0.09933388233184814\n",
      "Epoch 1271/30000 Training Loss: 0.09619668871164322\n",
      "Epoch 1272/30000 Training Loss: 0.12009286135435104\n",
      "Epoch 1273/30000 Training Loss: 0.11130806058645248\n",
      "Epoch 1274/30000 Training Loss: 0.0931171104311943\n",
      "Epoch 1275/30000 Training Loss: 0.09362032264471054\n",
      "Epoch 1276/30000 Training Loss: 0.1224348321557045\n",
      "Epoch 1277/30000 Training Loss: 0.10291159152984619\n",
      "Epoch 1278/30000 Training Loss: 0.09947290271520615\n",
      "Epoch 1279/30000 Training Loss: 0.0998476967215538\n",
      "Epoch 1280/30000 Training Loss: 0.09203662723302841\n",
      "Epoch 1280/30000 Validation Loss: 0.08986389636993408\n",
      "Epoch 1281/30000 Training Loss: 0.11333570629358292\n",
      "Epoch 1282/30000 Training Loss: 0.09662551432847977\n",
      "Epoch 1283/30000 Training Loss: 0.10563153028488159\n",
      "Epoch 1284/30000 Training Loss: 0.11207113415002823\n",
      "Epoch 1285/30000 Training Loss: 0.09464007616043091\n",
      "Epoch 1286/30000 Training Loss: 0.09803468734025955\n",
      "Epoch 1287/30000 Training Loss: 0.10553622245788574\n",
      "Epoch 1288/30000 Training Loss: 0.08374619483947754\n",
      "Epoch 1289/30000 Training Loss: 0.09839267283678055\n",
      "Epoch 1290/30000 Training Loss: 0.08299994468688965\n",
      "Epoch 1290/30000 Validation Loss: 0.11927458643913269\n",
      "Epoch 1291/30000 Training Loss: 0.08199610561132431\n",
      "Epoch 1292/30000 Training Loss: 0.11508654803037643\n",
      "Epoch 1293/30000 Training Loss: 0.10871341824531555\n",
      "Epoch 1294/30000 Training Loss: 0.08233840763568878\n",
      "Epoch 1295/30000 Training Loss: 0.11601496487855911\n",
      "Epoch 1296/30000 Training Loss: 0.09916868805885315\n",
      "Epoch 1297/30000 Training Loss: 0.08996471017599106\n",
      "Epoch 1298/30000 Training Loss: 0.07429517060518265\n",
      "Epoch 1299/30000 Training Loss: 0.10106302052736282\n",
      "Epoch 1300/30000 Training Loss: 0.12560927867889404\n",
      "Epoch 1300/30000 Validation Loss: 0.0910433754324913\n",
      "Epoch 1301/30000 Training Loss: 0.10210394114255905\n",
      "Epoch 1302/30000 Training Loss: 0.11129677295684814\n",
      "Epoch 1303/30000 Training Loss: 0.0904078260064125\n",
      "Epoch 1304/30000 Training Loss: 0.10021386295557022\n",
      "Epoch 1305/30000 Training Loss: 0.11481072753667831\n",
      "Epoch 1306/30000 Training Loss: 0.09574192762374878\n",
      "Epoch 1307/30000 Training Loss: 0.09880729764699936\n",
      "Epoch 1308/30000 Training Loss: 0.1167767345905304\n",
      "Epoch 1309/30000 Training Loss: 0.1170017346739769\n",
      "Epoch 1310/30000 Training Loss: 0.11080194264650345\n",
      "Epoch 1310/30000 Validation Loss: 0.09208569675683975\n",
      "Epoch 1311/30000 Training Loss: 0.0951572060585022\n",
      "Epoch 1312/30000 Training Loss: 0.08998414129018784\n",
      "Epoch 1313/30000 Training Loss: 0.1283441185951233\n",
      "Epoch 1314/30000 Training Loss: 0.10051891952753067\n",
      "Epoch 1315/30000 Training Loss: 0.1062551736831665\n",
      "Epoch 1316/30000 Training Loss: 0.09649341553449631\n",
      "Epoch 1317/30000 Training Loss: 0.09628212451934814\n",
      "Epoch 1318/30000 Training Loss: 0.10198121517896652\n",
      "Epoch 1319/30000 Training Loss: 0.09352341294288635\n",
      "Epoch 1320/30000 Training Loss: 0.09040381759405136\n",
      "Epoch 1320/30000 Validation Loss: 0.08033936470746994\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08033936470746994<=============\n",
      "Epoch 1321/30000 Training Loss: 0.08093810826539993\n",
      "Epoch 1322/30000 Training Loss: 0.11573351174592972\n",
      "Epoch 1323/30000 Training Loss: 0.08865106850862503\n",
      "Epoch 1324/30000 Training Loss: 0.11566946655511856\n",
      "Epoch 1325/30000 Training Loss: 0.09862520545721054\n",
      "Epoch 1326/30000 Training Loss: 0.11715336889028549\n",
      "Epoch 1327/30000 Training Loss: 0.0955800712108612\n",
      "Epoch 1328/30000 Training Loss: 0.10026359558105469\n",
      "Epoch 1329/30000 Training Loss: 0.08010245114564896\n",
      "Epoch 1330/30000 Training Loss: 0.09389405697584152\n",
      "Epoch 1330/30000 Validation Loss: 0.1202002763748169\n",
      "Epoch 1331/30000 Training Loss: 0.10524088144302368\n",
      "Epoch 1332/30000 Training Loss: 0.10375398397445679\n",
      "Epoch 1333/30000 Training Loss: 0.09547308832406998\n",
      "Epoch 1334/30000 Training Loss: 0.10178344696760178\n",
      "Epoch 1335/30000 Training Loss: 0.09053868055343628\n",
      "Epoch 1336/30000 Training Loss: 0.08168498426675797\n",
      "Epoch 1337/30000 Training Loss: 0.11003411561250687\n",
      "Epoch 1338/30000 Training Loss: 0.0946270227432251\n",
      "Epoch 1339/30000 Training Loss: 0.10563027858734131\n",
      "Epoch 1340/30000 Training Loss: 0.11195176094770432\n",
      "Epoch 1340/30000 Validation Loss: 0.07651573419570923\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07651573419570923<=============\n",
      "Epoch 1341/30000 Training Loss: 0.09366916865110397\n",
      "Epoch 1342/30000 Training Loss: 0.08504918217658997\n",
      "Epoch 1343/30000 Training Loss: 0.09991300106048584\n",
      "Epoch 1344/30000 Training Loss: 0.09326273202896118\n",
      "Epoch 1345/30000 Training Loss: 0.10951472073793411\n",
      "Epoch 1346/30000 Training Loss: 0.10598722845315933\n",
      "Epoch 1347/30000 Training Loss: 0.12290369719266891\n",
      "Epoch 1348/30000 Training Loss: 0.09401307255029678\n",
      "Epoch 1349/30000 Training Loss: 0.11584489792585373\n",
      "Epoch 1350/30000 Training Loss: 0.09847443550825119\n",
      "Epoch 1350/30000 Validation Loss: 0.091129831969738\n",
      "Epoch 1351/30000 Training Loss: 0.08806497603654861\n",
      "Epoch 1352/30000 Training Loss: 0.11671806126832962\n",
      "Epoch 1353/30000 Training Loss: 0.11992084234952927\n",
      "Epoch 1354/30000 Training Loss: 0.11737042665481567\n",
      "Epoch 1355/30000 Training Loss: 0.08431705087423325\n",
      "Epoch 1356/30000 Training Loss: 0.08819524198770523\n",
      "Epoch 1357/30000 Training Loss: 0.10265017300844193\n",
      "Epoch 1358/30000 Training Loss: 0.09163016825914383\n",
      "Epoch 1359/30000 Training Loss: 0.07917284965515137\n",
      "Epoch 1360/30000 Training Loss: 0.09757576137781143\n",
      "Epoch 1360/30000 Validation Loss: 0.10590904951095581\n",
      "Epoch 1361/30000 Training Loss: 0.08693776279687881\n",
      "Epoch 1362/30000 Training Loss: 0.09950771927833557\n",
      "Epoch 1363/30000 Training Loss: 0.10607907921075821\n",
      "Epoch 1364/30000 Training Loss: 0.09653214365243912\n",
      "Epoch 1365/30000 Training Loss: 0.12370116263628006\n",
      "Epoch 1366/30000 Training Loss: 0.1049814224243164\n",
      "Epoch 1367/30000 Training Loss: 0.11182713508605957\n",
      "Epoch 1368/30000 Training Loss: 0.10688912868499756\n",
      "Epoch 1369/30000 Training Loss: 0.09070396423339844\n",
      "Epoch 1370/30000 Training Loss: 0.0888589397072792\n",
      "Epoch 1370/30000 Validation Loss: 0.08423575758934021\n",
      "Epoch 1371/30000 Training Loss: 0.09244176000356674\n",
      "Epoch 1372/30000 Training Loss: 0.0839330181479454\n",
      "Epoch 1373/30000 Training Loss: 0.11043095588684082\n",
      "Epoch 1374/30000 Training Loss: 0.10652503371238708\n",
      "Epoch 1375/30000 Training Loss: 0.08905130624771118\n",
      "Epoch 1376/30000 Training Loss: 0.08747085183858871\n",
      "Epoch 1377/30000 Training Loss: 0.10553538799285889\n",
      "Epoch 1378/30000 Training Loss: 0.0988430306315422\n",
      "Epoch 1379/30000 Training Loss: 0.09563261270523071\n",
      "Epoch 1380/30000 Training Loss: 0.1088135838508606\n",
      "Epoch 1380/30000 Validation Loss: 0.12217769026756287\n",
      "Epoch 1381/30000 Training Loss: 0.1108606681227684\n",
      "Epoch 1382/30000 Training Loss: 0.13232068717479706\n",
      "Epoch 1383/30000 Training Loss: 0.08863063901662827\n",
      "Epoch 1384/30000 Training Loss: 0.08815836906433105\n",
      "Epoch 1385/30000 Training Loss: 0.09224975109100342\n",
      "Epoch 1386/30000 Training Loss: 0.07489315420389175\n",
      "Epoch 1387/30000 Training Loss: 0.10793918371200562\n",
      "Epoch 1388/30000 Training Loss: 0.09860847145318985\n",
      "Epoch 1389/30000 Training Loss: 0.08917891979217529\n",
      "Epoch 1390/30000 Training Loss: 0.09070590138435364\n",
      "Epoch 1390/30000 Validation Loss: 0.1149013414978981\n",
      "Epoch 1391/30000 Training Loss: 0.09010428935289383\n",
      "Epoch 1392/30000 Training Loss: 0.09115932136774063\n",
      "Epoch 1393/30000 Training Loss: 0.12352317571640015\n",
      "Epoch 1394/30000 Training Loss: 0.09292320162057877\n",
      "Epoch 1395/30000 Training Loss: 0.08818527311086655\n",
      "Epoch 1396/30000 Training Loss: 0.08642981201410294\n",
      "Epoch 1397/30000 Training Loss: 0.10457423329353333\n",
      "Epoch 1398/30000 Training Loss: 0.11471274495124817\n",
      "Epoch 1399/30000 Training Loss: 0.08296824246644974\n",
      "Epoch 1400/30000 Training Loss: 0.07602875679731369\n",
      "Epoch 1400/30000 Validation Loss: 0.10376244783401489\n",
      "Epoch 1401/30000 Training Loss: 0.09530868381261826\n",
      "Epoch 1402/30000 Training Loss: 0.11537715047597885\n",
      "Epoch 1403/30000 Training Loss: 0.12363157421350479\n",
      "Epoch 1404/30000 Training Loss: 0.13038413226604462\n",
      "Epoch 1405/30000 Training Loss: 0.09903331845998764\n",
      "Epoch 1406/30000 Training Loss: 0.09098345041275024\n",
      "Epoch 1407/30000 Training Loss: 0.09917348623275757\n",
      "Epoch 1408/30000 Training Loss: 0.10721597075462341\n",
      "Epoch 1409/30000 Training Loss: 0.10468535870313644\n",
      "Epoch 1410/30000 Training Loss: 0.08156182616949081\n",
      "Epoch 1410/30000 Validation Loss: 0.0984511449933052\n",
      "Epoch 1411/30000 Training Loss: 0.11215761303901672\n",
      "Epoch 1412/30000 Training Loss: 0.11792991310358047\n",
      "Epoch 1413/30000 Training Loss: 0.11356977373361588\n",
      "Epoch 1414/30000 Training Loss: 0.11142457276582718\n",
      "Epoch 1415/30000 Training Loss: 0.06976592540740967\n",
      "Epoch 1416/30000 Training Loss: 0.09536096453666687\n",
      "Epoch 1417/30000 Training Loss: 0.10025504976511002\n",
      "Epoch 1418/30000 Training Loss: 0.1016223356127739\n",
      "Epoch 1419/30000 Training Loss: 0.08908125758171082\n",
      "Epoch 1420/30000 Training Loss: 0.118143729865551\n",
      "Epoch 1420/30000 Validation Loss: 0.12326887249946594\n",
      "Epoch 1421/30000 Training Loss: 0.10828540474176407\n",
      "Epoch 1422/30000 Training Loss: 0.09803357720375061\n",
      "Epoch 1423/30000 Training Loss: 0.10287607461214066\n",
      "Epoch 1424/30000 Training Loss: 0.07597196847200394\n",
      "Epoch 1425/30000 Training Loss: 0.09247538447380066\n",
      "Epoch 1426/30000 Training Loss: 0.09459017962217331\n",
      "Epoch 1427/30000 Training Loss: 0.08859530091285706\n",
      "Epoch 1428/30000 Training Loss: 0.1070469543337822\n",
      "Epoch 1429/30000 Training Loss: 0.09679193049669266\n",
      "Epoch 1430/30000 Training Loss: 0.08073581010103226\n",
      "Epoch 1430/30000 Validation Loss: 0.10280846804380417\n",
      "Epoch 1431/30000 Training Loss: 0.11721258610486984\n",
      "Epoch 1432/30000 Training Loss: 0.11093538999557495\n",
      "Epoch 1433/30000 Training Loss: 0.10242966562509537\n",
      "Epoch 1434/30000 Training Loss: 0.08208399266004562\n",
      "Epoch 1435/30000 Training Loss: 0.08729004114866257\n",
      "Epoch 1436/30000 Training Loss: 0.11815860867500305\n",
      "Epoch 1437/30000 Training Loss: 0.09204425662755966\n",
      "Epoch 1438/30000 Training Loss: 0.07763335853815079\n",
      "Epoch 1439/30000 Training Loss: 0.11999624967575073\n",
      "Epoch 1440/30000 Training Loss: 0.09165912866592407\n",
      "Epoch 1440/30000 Validation Loss: 0.09986428171396255\n",
      "Epoch 1441/30000 Training Loss: 0.08039706945419312\n",
      "Epoch 1442/30000 Training Loss: 0.1241491511464119\n",
      "Epoch 1443/30000 Training Loss: 0.12052986025810242\n",
      "Epoch 1444/30000 Training Loss: 0.09957456588745117\n",
      "Epoch 1445/30000 Training Loss: 0.09203767776489258\n",
      "Epoch 1446/30000 Training Loss: 0.1102980449795723\n",
      "Epoch 1447/30000 Training Loss: 0.12110332399606705\n",
      "Epoch 1448/30000 Training Loss: 0.06799989193677902\n",
      "Epoch 1449/30000 Training Loss: 0.1058170273900032\n",
      "Epoch 1450/30000 Training Loss: 0.10620377212762833\n",
      "Epoch 1450/30000 Validation Loss: 0.07531612366437912\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07531612366437912<=============\n",
      "Epoch 1451/30000 Training Loss: 0.1248338595032692\n",
      "Epoch 1452/30000 Training Loss: 0.09065858274698257\n",
      "Epoch 1453/30000 Training Loss: 0.09378627687692642\n",
      "Epoch 1454/30000 Training Loss: 0.08780518919229507\n",
      "Epoch 1455/30000 Training Loss: 0.09914771467447281\n",
      "Epoch 1456/30000 Training Loss: 0.09594472497701645\n",
      "Epoch 1457/30000 Training Loss: 0.09683427959680557\n",
      "Epoch 1458/30000 Training Loss: 0.0914405807852745\n",
      "Epoch 1459/30000 Training Loss: 0.09044277667999268\n",
      "Epoch 1460/30000 Training Loss: 0.09342750161886215\n",
      "Epoch 1460/30000 Validation Loss: 0.08861441165208817\n",
      "Epoch 1461/30000 Training Loss: 0.0953904464840889\n",
      "Epoch 1462/30000 Training Loss: 0.09959480911493301\n",
      "Epoch 1463/30000 Training Loss: 0.09082254022359848\n",
      "Epoch 1464/30000 Training Loss: 0.08370263129472733\n",
      "Epoch 1465/30000 Training Loss: 0.0952845886349678\n",
      "Epoch 1466/30000 Training Loss: 0.13835905492305756\n",
      "Epoch 1467/30000 Training Loss: 0.07562511414289474\n",
      "Epoch 1468/30000 Training Loss: 0.09732106328010559\n",
      "Epoch 1469/30000 Training Loss: 0.1158984899520874\n",
      "Epoch 1470/30000 Training Loss: 0.07000686973333359\n",
      "Epoch 1470/30000 Validation Loss: 0.10600346326828003\n",
      "Epoch 1471/30000 Training Loss: 0.13611431419849396\n",
      "Epoch 1472/30000 Training Loss: 0.07699471712112427\n",
      "Epoch 1473/30000 Training Loss: 0.098158098757267\n",
      "Epoch 1474/30000 Training Loss: 0.1040213331580162\n",
      "Epoch 1475/30000 Training Loss: 0.08429839462041855\n",
      "Epoch 1476/30000 Training Loss: 0.10354509949684143\n",
      "Epoch 1477/30000 Training Loss: 0.09811071306467056\n",
      "Epoch 1478/30000 Training Loss: 0.08910572528839111\n",
      "Epoch 1479/30000 Training Loss: 0.09476003795862198\n",
      "Epoch 1480/30000 Training Loss: 0.09198296070098877\n",
      "Epoch 1480/30000 Validation Loss: 0.11339402198791504\n",
      "Epoch 1481/30000 Training Loss: 0.08190389722585678\n",
      "Epoch 1482/30000 Training Loss: 0.11852268129587173\n",
      "Epoch 1483/30000 Training Loss: 0.08530678600072861\n",
      "Epoch 1484/30000 Training Loss: 0.12987862527370453\n",
      "Epoch 1485/30000 Training Loss: 0.08691477030515671\n",
      "Epoch 1486/30000 Training Loss: 0.10732445865869522\n",
      "Epoch 1487/30000 Training Loss: 0.10288598388433456\n",
      "Epoch 1488/30000 Training Loss: 0.09363508224487305\n",
      "Epoch 1489/30000 Training Loss: 0.11114585399627686\n",
      "Epoch 1490/30000 Training Loss: 0.08968716114759445\n",
      "Epoch 1490/30000 Validation Loss: 0.09858471155166626\n",
      "Epoch 1491/30000 Training Loss: 0.12274709343910217\n",
      "Epoch 1492/30000 Training Loss: 0.0638543963432312\n",
      "Epoch 1493/30000 Training Loss: 0.10114413499832153\n",
      "Epoch 1494/30000 Training Loss: 0.0982123613357544\n",
      "Epoch 1495/30000 Training Loss: 0.09937459975481033\n",
      "Epoch 1496/30000 Training Loss: 0.13148058950901031\n",
      "Epoch 1497/30000 Training Loss: 0.08910218626260757\n",
      "Epoch 1498/30000 Training Loss: 0.11720233410596848\n",
      "Epoch 1499/30000 Training Loss: 0.0870145931839943\n",
      "Epoch 1500/30000 Training Loss: 0.09178972989320755\n",
      "Epoch 1500/30000 Validation Loss: 0.08287695050239563\n",
      "Epoch 1501/30000 Training Loss: 0.08952350169420242\n",
      "Epoch 1502/30000 Training Loss: 0.08482497930526733\n",
      "Epoch 1503/30000 Training Loss: 0.10087858885526657\n",
      "Epoch 1504/30000 Training Loss: 0.09154834598302841\n",
      "Epoch 1505/30000 Training Loss: 0.11297141760587692\n",
      "Epoch 1506/30000 Training Loss: 0.09066758304834366\n",
      "Epoch 1507/30000 Training Loss: 0.09392627328634262\n",
      "Epoch 1508/30000 Training Loss: 0.0737098753452301\n",
      "Epoch 1509/30000 Training Loss: 0.11757174879312515\n",
      "Epoch 1510/30000 Training Loss: 0.09764453768730164\n",
      "Epoch 1510/30000 Validation Loss: 0.11061017960309982\n",
      "Epoch 1511/30000 Training Loss: 0.11022140830755234\n",
      "Epoch 1512/30000 Training Loss: 0.08681914955377579\n",
      "Epoch 1513/30000 Training Loss: 0.08635643124580383\n",
      "Epoch 1514/30000 Training Loss: 0.10632497072219849\n",
      "Epoch 1515/30000 Training Loss: 0.09802696853876114\n",
      "Epoch 1516/30000 Training Loss: 0.08925484865903854\n",
      "Epoch 1517/30000 Training Loss: 0.08294401317834854\n",
      "Epoch 1518/30000 Training Loss: 0.09508999437093735\n",
      "Epoch 1519/30000 Training Loss: 0.07724332809448242\n",
      "Epoch 1520/30000 Training Loss: 0.07792932540178299\n",
      "Epoch 1520/30000 Validation Loss: 0.0994245782494545\n",
      "Epoch 1521/30000 Training Loss: 0.10390833020210266\n",
      "Epoch 1522/30000 Training Loss: 0.07844891399145126\n",
      "Epoch 1523/30000 Training Loss: 0.099109947681427\n",
      "Epoch 1524/30000 Training Loss: 0.08724650740623474\n",
      "Epoch 1525/30000 Training Loss: 0.12121317535638809\n",
      "Epoch 1526/30000 Training Loss: 0.11141427606344223\n",
      "Epoch 1527/30000 Training Loss: 0.10768016427755356\n",
      "Epoch 1528/30000 Training Loss: 0.08890379220247269\n",
      "Epoch 1529/30000 Training Loss: 0.08536279946565628\n",
      "Epoch 1530/30000 Training Loss: 0.08984675258398056\n",
      "Epoch 1530/30000 Validation Loss: 0.08356612920761108\n",
      "Epoch 1531/30000 Training Loss: 0.08937907218933105\n",
      "Epoch 1532/30000 Training Loss: 0.10159257054328918\n",
      "Epoch 1533/30000 Training Loss: 0.0813385471701622\n",
      "Epoch 1534/30000 Training Loss: 0.10709929466247559\n",
      "Epoch 1535/30000 Training Loss: 0.09109574556350708\n",
      "Epoch 1536/30000 Training Loss: 0.09124986082315445\n",
      "Epoch 1537/30000 Training Loss: 0.10304241627454758\n",
      "Epoch 1538/30000 Training Loss: 0.09233734756708145\n",
      "Epoch 1539/30000 Training Loss: 0.10387880355119705\n",
      "Epoch 1540/30000 Training Loss: 0.097675621509552\n",
      "Epoch 1540/30000 Validation Loss: 0.09193132072687149\n",
      "Epoch 1541/30000 Training Loss: 0.11333641409873962\n",
      "Epoch 1542/30000 Training Loss: 0.10545212030410767\n",
      "Epoch 1543/30000 Training Loss: 0.10788979381322861\n",
      "Epoch 1544/30000 Training Loss: 0.11098317056894302\n",
      "Epoch 1545/30000 Training Loss: 0.10098972916603088\n",
      "Epoch 1546/30000 Training Loss: 0.09203856438398361\n",
      "Epoch 1547/30000 Training Loss: 0.09280387312173843\n",
      "Epoch 1548/30000 Training Loss: 0.08763802796602249\n",
      "Epoch 1549/30000 Training Loss: 0.09315326064825058\n",
      "Epoch 1550/30000 Training Loss: 0.08687662333250046\n",
      "Epoch 1550/30000 Validation Loss: 0.09175559133291245\n",
      "Epoch 1551/30000 Training Loss: 0.12579017877578735\n",
      "Epoch 1552/30000 Training Loss: 0.07818055897951126\n",
      "Epoch 1553/30000 Training Loss: 0.1147707924246788\n",
      "Epoch 1554/30000 Training Loss: 0.10360010713338852\n",
      "Epoch 1555/30000 Training Loss: 0.07684396952390671\n",
      "Epoch 1556/30000 Training Loss: 0.10699038952589035\n",
      "Epoch 1557/30000 Training Loss: 0.10617057234048843\n",
      "Epoch 1558/30000 Training Loss: 0.0874151661992073\n",
      "Epoch 1559/30000 Training Loss: 0.10238758474588394\n",
      "Epoch 1560/30000 Training Loss: 0.11902868747711182\n",
      "Epoch 1560/30000 Validation Loss: 0.11655449867248535\n",
      "Epoch 1561/30000 Training Loss: 0.09786874800920486\n",
      "Epoch 1562/30000 Training Loss: 0.09346291422843933\n",
      "Epoch 1563/30000 Training Loss: 0.09191491454839706\n",
      "Epoch 1564/30000 Training Loss: 0.08978179097175598\n",
      "Epoch 1565/30000 Training Loss: 0.08485408872365952\n",
      "Epoch 1566/30000 Training Loss: 0.0950506329536438\n",
      "Epoch 1567/30000 Training Loss: 0.07328569144010544\n",
      "Epoch 1568/30000 Training Loss: 0.08166994899511337\n",
      "Epoch 1569/30000 Training Loss: 0.06744080781936646\n",
      "Epoch 1570/30000 Training Loss: 0.10081008821725845\n",
      "Epoch 1570/30000 Validation Loss: 0.09210827946662903\n",
      "Epoch 1571/30000 Training Loss: 0.09386005997657776\n",
      "Epoch 1572/30000 Training Loss: 0.11388307809829712\n",
      "Epoch 1573/30000 Training Loss: 0.08682575076818466\n",
      "Epoch 1574/30000 Training Loss: 0.10096865892410278\n",
      "Epoch 1575/30000 Training Loss: 0.11037490516901016\n",
      "Epoch 1576/30000 Training Loss: 0.10081914067268372\n",
      "Epoch 1577/30000 Training Loss: 0.10965364426374435\n",
      "Epoch 1578/30000 Training Loss: 0.08606261759996414\n",
      "Epoch 1579/30000 Training Loss: 0.0827459767460823\n",
      "Epoch 1580/30000 Training Loss: 0.09540524333715439\n",
      "Epoch 1580/30000 Validation Loss: 0.10334515571594238\n",
      "Epoch 1581/30000 Training Loss: 0.08908335119485855\n",
      "Epoch 1582/30000 Training Loss: 0.08175995200872421\n",
      "Epoch 1583/30000 Training Loss: 0.07622294872999191\n",
      "Epoch 1584/30000 Training Loss: 0.09403812885284424\n",
      "Epoch 1585/30000 Training Loss: 0.09904675930738449\n",
      "Epoch 1586/30000 Training Loss: 0.0850972905755043\n",
      "Epoch 1587/30000 Training Loss: 0.0844285786151886\n",
      "Epoch 1588/30000 Training Loss: 0.08602755516767502\n",
      "Epoch 1589/30000 Training Loss: 0.06834129989147186\n",
      "Epoch 1590/30000 Training Loss: 0.10238803178071976\n",
      "Epoch 1590/30000 Validation Loss: 0.08840437978506088\n",
      "Epoch 1591/30000 Training Loss: 0.08458877354860306\n",
      "Epoch 1592/30000 Training Loss: 0.0873013511300087\n",
      "Epoch 1593/30000 Training Loss: 0.11335050314664841\n",
      "Epoch 1594/30000 Training Loss: 0.07880530506372452\n",
      "Epoch 1595/30000 Training Loss: 0.1011359691619873\n",
      "Epoch 1596/30000 Training Loss: 0.10684176534414291\n",
      "Epoch 1597/30000 Training Loss: 0.09866180270910263\n",
      "Epoch 1598/30000 Training Loss: 0.08887525647878647\n",
      "Epoch 1599/30000 Training Loss: 0.10790088772773743\n",
      "Epoch 1600/30000 Training Loss: 0.10206449031829834\n",
      "Epoch 1600/30000 Validation Loss: 0.10731372982263565\n",
      "Epoch 1601/30000 Training Loss: 0.10662981122732162\n",
      "Epoch 1602/30000 Training Loss: 0.0964735820889473\n",
      "Epoch 1603/30000 Training Loss: 0.09429574757814407\n",
      "Epoch 1604/30000 Training Loss: 0.11355987936258316\n",
      "Epoch 1605/30000 Training Loss: 0.07621581107378006\n",
      "Epoch 1606/30000 Training Loss: 0.10177281498908997\n",
      "Epoch 1607/30000 Training Loss: 0.09050961583852768\n",
      "Epoch 1608/30000 Training Loss: 0.09521733969449997\n",
      "Epoch 1609/30000 Training Loss: 0.12073910236358643\n",
      "Epoch 1610/30000 Training Loss: 0.0701446458697319\n",
      "Epoch 1610/30000 Validation Loss: 0.10356265306472778\n",
      "Epoch 1611/30000 Training Loss: 0.1178835853934288\n",
      "Epoch 1612/30000 Training Loss: 0.10328783839941025\n",
      "Epoch 1613/30000 Training Loss: 0.103431336581707\n",
      "Epoch 1614/30000 Training Loss: 0.10070336610078812\n",
      "Epoch 1615/30000 Training Loss: 0.08722320199012756\n",
      "Epoch 1616/30000 Training Loss: 0.1124432310461998\n",
      "Epoch 1617/30000 Training Loss: 0.07433712482452393\n",
      "Epoch 1618/30000 Training Loss: 0.06634314358234406\n",
      "Epoch 1619/30000 Training Loss: 0.12138143926858902\n",
      "Epoch 1620/30000 Training Loss: 0.1032516285777092\n",
      "Epoch 1620/30000 Validation Loss: 0.12485634535551071\n",
      "Epoch 1621/30000 Training Loss: 0.0949319377541542\n",
      "Epoch 1622/30000 Training Loss: 0.08830329030752182\n",
      "Epoch 1623/30000 Training Loss: 0.11529874056577682\n",
      "Epoch 1624/30000 Training Loss: 0.10107805579900742\n",
      "Epoch 1625/30000 Training Loss: 0.11500361561775208\n",
      "Epoch 1626/30000 Training Loss: 0.11116457730531693\n",
      "Epoch 1627/30000 Training Loss: 0.11777197569608688\n",
      "Epoch 1628/30000 Training Loss: 0.1135341003537178\n",
      "Epoch 1629/30000 Training Loss: 0.08460398763418198\n",
      "Epoch 1630/30000 Training Loss: 0.11653649806976318\n",
      "Epoch 1630/30000 Validation Loss: 0.11554751545190811\n",
      "Epoch 1631/30000 Training Loss: 0.09871479123830795\n",
      "Epoch 1632/30000 Training Loss: 0.09764337539672852\n",
      "Epoch 1633/30000 Training Loss: 0.13575197756290436\n",
      "Epoch 1634/30000 Training Loss: 0.0752093642950058\n",
      "Epoch 1635/30000 Training Loss: 0.09810686111450195\n",
      "Epoch 1636/30000 Training Loss: 0.08656057715415955\n",
      "Epoch 1637/30000 Training Loss: 0.11318621784448624\n",
      "Epoch 1638/30000 Training Loss: 0.09517639875411987\n",
      "Epoch 1639/30000 Training Loss: 0.09210175275802612\n",
      "Epoch 1640/30000 Training Loss: 0.08494964241981506\n",
      "Epoch 1640/30000 Validation Loss: 0.11602220684289932\n",
      "Epoch 1641/30000 Training Loss: 0.077753446996212\n",
      "Epoch 1642/30000 Training Loss: 0.1064615324139595\n",
      "Epoch 1643/30000 Training Loss: 0.09299159049987793\n",
      "Epoch 1644/30000 Training Loss: 0.08959085494279861\n",
      "Epoch 1645/30000 Training Loss: 0.1028456911444664\n",
      "Epoch 1646/30000 Training Loss: 0.08842990547418594\n",
      "Epoch 1647/30000 Training Loss: 0.09025668352842331\n",
      "Epoch 1648/30000 Training Loss: 0.0689735934138298\n",
      "Epoch 1649/30000 Training Loss: 0.10081732273101807\n",
      "Epoch 1650/30000 Training Loss: 0.07864858955144882\n",
      "Epoch 1650/30000 Validation Loss: 0.08379960060119629\n",
      "Epoch 1651/30000 Training Loss: 0.10145175457000732\n",
      "Epoch 1652/30000 Training Loss: 0.07901260256767273\n",
      "Epoch 1653/30000 Training Loss: 0.09604885429143906\n",
      "Epoch 1654/30000 Training Loss: 0.09296879172325134\n",
      "Epoch 1655/30000 Training Loss: 0.09384423494338989\n",
      "Epoch 1656/30000 Training Loss: 0.11198264360427856\n",
      "Epoch 1657/30000 Training Loss: 0.10360176116228104\n",
      "Epoch 1658/30000 Training Loss: 0.0960182324051857\n",
      "Epoch 1659/30000 Training Loss: 0.11317165940999985\n",
      "Epoch 1660/30000 Training Loss: 0.09020185470581055\n",
      "Epoch 1660/30000 Validation Loss: 0.0945117399096489\n",
      "Epoch 1661/30000 Training Loss: 0.0674370750784874\n",
      "Epoch 1662/30000 Training Loss: 0.1040080115199089\n",
      "Epoch 1663/30000 Training Loss: 0.09373375028371811\n",
      "Epoch 1664/30000 Training Loss: 0.10472848266363144\n",
      "Epoch 1665/30000 Training Loss: 0.11382689327001572\n",
      "Epoch 1666/30000 Training Loss: 0.11653891950845718\n",
      "Epoch 1667/30000 Training Loss: 0.08207666128873825\n",
      "Epoch 1668/30000 Training Loss: 0.08371793478727341\n",
      "Epoch 1669/30000 Training Loss: 0.09849488735198975\n",
      "Epoch 1670/30000 Training Loss: 0.0770517885684967\n",
      "Epoch 1670/30000 Validation Loss: 0.10654530674219131\n",
      "Epoch 1671/30000 Training Loss: 0.09155773371458054\n",
      "Epoch 1672/30000 Training Loss: 0.09728319197893143\n",
      "Epoch 1673/30000 Training Loss: 0.08376544713973999\n",
      "Epoch 1674/30000 Training Loss: 0.09648194164037704\n",
      "Epoch 1675/30000 Training Loss: 0.07451867312192917\n",
      "Epoch 1676/30000 Training Loss: 0.09125315397977829\n",
      "Epoch 1677/30000 Training Loss: 0.09340100735425949\n",
      "Epoch 1678/30000 Training Loss: 0.08772137761116028\n",
      "Epoch 1679/30000 Training Loss: 0.09848671406507492\n",
      "Epoch 1680/30000 Training Loss: 0.08366596698760986\n",
      "Epoch 1680/30000 Validation Loss: 0.08354517072439194\n",
      "Epoch 1681/30000 Training Loss: 0.10472345352172852\n",
      "Epoch 1682/30000 Training Loss: 0.07967784255743027\n",
      "Epoch 1683/30000 Training Loss: 0.0900636538863182\n",
      "Epoch 1684/30000 Training Loss: 0.10267835855484009\n",
      "Epoch 1685/30000 Training Loss: 0.09414788335561752\n",
      "Epoch 1686/30000 Training Loss: 0.0860411524772644\n",
      "Epoch 1687/30000 Training Loss: 0.09419989585876465\n",
      "Epoch 1688/30000 Training Loss: 0.07397421449422836\n",
      "Epoch 1689/30000 Training Loss: 0.08683665841817856\n",
      "Epoch 1690/30000 Training Loss: 0.09500027447938919\n",
      "Epoch 1690/30000 Validation Loss: 0.09743592888116837\n",
      "Epoch 1691/30000 Training Loss: 0.10219018906354904\n",
      "Epoch 1692/30000 Training Loss: 0.07915591448545456\n",
      "Epoch 1693/30000 Training Loss: 0.08810906857252121\n",
      "Epoch 1694/30000 Training Loss: 0.07173728942871094\n",
      "Epoch 1695/30000 Training Loss: 0.10323313623666763\n",
      "Epoch 1696/30000 Training Loss: 0.09562323242425919\n",
      "Epoch 1697/30000 Training Loss: 0.09299203008413315\n",
      "Epoch 1698/30000 Training Loss: 0.0795297771692276\n",
      "Epoch 1699/30000 Training Loss: 0.0975179672241211\n",
      "Epoch 1700/30000 Training Loss: 0.10541462153196335\n",
      "Epoch 1700/30000 Validation Loss: 0.0964740738272667\n",
      "Epoch 1701/30000 Training Loss: 0.1014961302280426\n",
      "Epoch 1702/30000 Training Loss: 0.08123588562011719\n",
      "Epoch 1703/30000 Training Loss: 0.09483977407217026\n",
      "Epoch 1704/30000 Training Loss: 0.08809541910886765\n",
      "Epoch 1705/30000 Training Loss: 0.1077909842133522\n",
      "Epoch 1706/30000 Training Loss: 0.1074732318520546\n",
      "Epoch 1707/30000 Training Loss: 0.07709544152021408\n",
      "Epoch 1708/30000 Training Loss: 0.09975462406873703\n",
      "Epoch 1709/30000 Training Loss: 0.10751441866159439\n",
      "Epoch 1710/30000 Training Loss: 0.08452875167131424\n",
      "Epoch 1710/30000 Validation Loss: 0.11130962520837784\n",
      "Epoch 1711/30000 Training Loss: 0.11749882251024246\n",
      "Epoch 1712/30000 Training Loss: 0.10259505361318588\n",
      "Epoch 1713/30000 Training Loss: 0.11703565716743469\n",
      "Epoch 1714/30000 Training Loss: 0.11415505409240723\n",
      "Epoch 1715/30000 Training Loss: 0.07920730859041214\n",
      "Epoch 1716/30000 Training Loss: 0.08761747926473618\n",
      "Epoch 1717/30000 Training Loss: 0.09941712766885757\n",
      "Epoch 1718/30000 Training Loss: 0.08353113383054733\n",
      "Epoch 1719/30000 Training Loss: 0.08785360306501389\n",
      "Epoch 1720/30000 Training Loss: 0.08543767780065536\n",
      "Epoch 1720/30000 Validation Loss: 0.09457194805145264\n",
      "Epoch 1721/30000 Training Loss: 0.09369461983442307\n",
      "Epoch 1722/30000 Training Loss: 0.09281941503286362\n",
      "Epoch 1723/30000 Training Loss: 0.09282311052083969\n",
      "Epoch 1724/30000 Training Loss: 0.09432532638311386\n",
      "Epoch 1725/30000 Training Loss: 0.07935913652181625\n",
      "Epoch 1726/30000 Training Loss: 0.07414290308952332\n",
      "Epoch 1727/30000 Training Loss: 0.09644577652215958\n",
      "Epoch 1728/30000 Training Loss: 0.09006127715110779\n",
      "Epoch 1729/30000 Training Loss: 0.0782567635178566\n",
      "Epoch 1730/30000 Training Loss: 0.09722968935966492\n",
      "Epoch 1730/30000 Validation Loss: 0.12227264791727066\n",
      "Epoch 1731/30000 Training Loss: 0.08742514997720718\n",
      "Epoch 1732/30000 Training Loss: 0.07906505465507507\n",
      "Epoch 1733/30000 Training Loss: 0.0980977937579155\n",
      "Epoch 1734/30000 Training Loss: 0.09906937927007675\n",
      "Epoch 1735/30000 Training Loss: 0.07814687490463257\n",
      "Epoch 1736/30000 Training Loss: 0.08594974875450134\n",
      "Epoch 1737/30000 Training Loss: 0.10899347066879272\n",
      "Epoch 1738/30000 Training Loss: 0.09943550080060959\n",
      "Epoch 1739/30000 Training Loss: 0.10525421053171158\n",
      "Epoch 1740/30000 Training Loss: 0.09351495653390884\n",
      "Epoch 1740/30000 Validation Loss: 0.07610680907964706\n",
      "Epoch 1741/30000 Training Loss: 0.07745018601417542\n",
      "Epoch 1742/30000 Training Loss: 0.0869092047214508\n",
      "Epoch 1743/30000 Training Loss: 0.10088074952363968\n",
      "Epoch 1744/30000 Training Loss: 0.08715110272169113\n",
      "Epoch 1745/30000 Training Loss: 0.08305004984140396\n",
      "Epoch 1746/30000 Training Loss: 0.06777576357126236\n",
      "Epoch 1747/30000 Training Loss: 0.08192264288663864\n",
      "Epoch 1748/30000 Training Loss: 0.10519150644540787\n",
      "Epoch 1749/30000 Training Loss: 0.09562359005212784\n",
      "Epoch 1750/30000 Training Loss: 0.1019183024764061\n",
      "Epoch 1750/30000 Validation Loss: 0.0956028401851654\n",
      "Epoch 1751/30000 Training Loss: 0.1156303659081459\n",
      "Epoch 1752/30000 Training Loss: 0.1093149185180664\n",
      "Epoch 1753/30000 Training Loss: 0.11084853857755661\n",
      "Epoch 1754/30000 Training Loss: 0.08754350990056992\n",
      "Epoch 1755/30000 Training Loss: 0.08764674514532089\n",
      "Epoch 1756/30000 Training Loss: 0.10269569605588913\n",
      "Epoch 1757/30000 Training Loss: 0.08831077814102173\n",
      "Epoch 1758/30000 Training Loss: 0.09162082523107529\n",
      "Epoch 1759/30000 Training Loss: 0.09496617317199707\n",
      "Epoch 1760/30000 Training Loss: 0.09338497370481491\n",
      "Epoch 1760/30000 Validation Loss: 0.08285632729530334\n",
      "Epoch 1761/30000 Training Loss: 0.08972117304801941\n",
      "Epoch 1762/30000 Training Loss: 0.1030600443482399\n",
      "Epoch 1763/30000 Training Loss: 0.08592021465301514\n",
      "Epoch 1764/30000 Training Loss: 0.08766409009695053\n",
      "Epoch 1765/30000 Training Loss: 0.07737291604280472\n",
      "Epoch 1766/30000 Training Loss: 0.10259959846735\n",
      "Epoch 1767/30000 Training Loss: 0.09199566394090652\n",
      "Epoch 1768/30000 Training Loss: 0.09410446882247925\n",
      "Epoch 1769/30000 Training Loss: 0.08992421627044678\n",
      "Epoch 1770/30000 Training Loss: 0.09955690056085587\n",
      "Epoch 1770/30000 Validation Loss: 0.08899423480033875\n",
      "Epoch 1771/30000 Training Loss: 0.10902292281389236\n",
      "Epoch 1772/30000 Training Loss: 0.09768912941217422\n",
      "Epoch 1773/30000 Training Loss: 0.10332191735506058\n",
      "Epoch 1774/30000 Training Loss: 0.10996315628290176\n",
      "Epoch 1775/30000 Training Loss: 0.08823808282613754\n",
      "Epoch 1776/30000 Training Loss: 0.09531539678573608\n",
      "Epoch 1777/30000 Training Loss: 0.07706622034311295\n",
      "Epoch 1778/30000 Training Loss: 0.12026157230138779\n",
      "Epoch 1779/30000 Training Loss: 0.09905285388231277\n",
      "Epoch 1780/30000 Training Loss: 0.10080062597990036\n",
      "Epoch 1780/30000 Validation Loss: 0.10610103607177734\n",
      "Epoch 1781/30000 Training Loss: 0.08129609376192093\n",
      "Epoch 1782/30000 Training Loss: 0.09557675570249557\n",
      "Epoch 1783/30000 Training Loss: 0.09934661537408829\n",
      "Epoch 1784/30000 Training Loss: 0.07979915291070938\n",
      "Epoch 1785/30000 Training Loss: 0.08546528965234756\n",
      "Epoch 1786/30000 Training Loss: 0.09849335998296738\n",
      "Epoch 1787/30000 Training Loss: 0.09721998125314713\n",
      "Epoch 1788/30000 Training Loss: 0.09075094014406204\n",
      "Epoch 1789/30000 Training Loss: 0.09417945146560669\n",
      "Epoch 1790/30000 Training Loss: 0.1130853220820427\n",
      "Epoch 1790/30000 Validation Loss: 0.10272576659917831\n",
      "Epoch 1791/30000 Training Loss: 0.07724658399820328\n",
      "Epoch 1792/30000 Training Loss: 0.08584194630384445\n",
      "Epoch 1793/30000 Training Loss: 0.07801977545022964\n",
      "Epoch 1794/30000 Training Loss: 0.0958348736166954\n",
      "Epoch 1795/30000 Training Loss: 0.12419348955154419\n",
      "Epoch 1796/30000 Training Loss: 0.08145608752965927\n",
      "Epoch 1797/30000 Training Loss: 0.07582642883062363\n",
      "Epoch 1798/30000 Training Loss: 0.0984540805220604\n",
      "Epoch 1799/30000 Training Loss: 0.1058281660079956\n",
      "Epoch 1800/30000 Training Loss: 0.09499192237854004\n",
      "Epoch 1800/30000 Validation Loss: 0.08098047971725464\n",
      "Epoch 1801/30000 Training Loss: 0.09875068068504333\n",
      "Epoch 1802/30000 Training Loss: 0.08213137835264206\n",
      "Epoch 1803/30000 Training Loss: 0.09618216007947922\n",
      "Epoch 1804/30000 Training Loss: 0.10944115370512009\n",
      "Epoch 1805/30000 Training Loss: 0.08488407731056213\n",
      "Epoch 1806/30000 Training Loss: 0.11516992002725601\n",
      "Epoch 1807/30000 Training Loss: 0.09542129188776016\n",
      "Epoch 1808/30000 Training Loss: 0.07256124168634415\n",
      "Epoch 1809/30000 Training Loss: 0.11553042382001877\n",
      "Epoch 1810/30000 Training Loss: 0.10471653938293457\n",
      "Epoch 1810/30000 Validation Loss: 0.07589937001466751\n",
      "Epoch 1811/30000 Training Loss: 0.1051841601729393\n",
      "Epoch 1812/30000 Training Loss: 0.09188725799322128\n",
      "Epoch 1813/30000 Training Loss: 0.08155695348978043\n",
      "Epoch 1814/30000 Training Loss: 0.09747201204299927\n",
      "Epoch 1815/30000 Training Loss: 0.0726332738995552\n",
      "Epoch 1816/30000 Training Loss: 0.07869721204042435\n",
      "Epoch 1817/30000 Training Loss: 0.0916128158569336\n",
      "Epoch 1818/30000 Training Loss: 0.08618519455194473\n",
      "Epoch 1819/30000 Training Loss: 0.08493456989526749\n",
      "Epoch 1820/30000 Training Loss: 0.11720182746648788\n",
      "Epoch 1820/30000 Validation Loss: 0.09290396422147751\n",
      "Epoch 1821/30000 Training Loss: 0.0756421685218811\n",
      "Epoch 1822/30000 Training Loss: 0.10524801164865494\n",
      "Epoch 1823/30000 Training Loss: 0.08751720935106277\n",
      "Epoch 1824/30000 Training Loss: 0.11390753835439682\n",
      "Epoch 1825/30000 Training Loss: 0.09645954519510269\n",
      "Epoch 1826/30000 Training Loss: 0.08962198346853256\n",
      "Epoch 1827/30000 Training Loss: 0.10301816463470459\n",
      "Epoch 1828/30000 Training Loss: 0.075495146214962\n",
      "Epoch 1829/30000 Training Loss: 0.10998024791479111\n",
      "Epoch 1830/30000 Training Loss: 0.08345583826303482\n",
      "Epoch 1830/30000 Validation Loss: 0.07462630420923233\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07462630420923233<=============\n",
      "Epoch 1831/30000 Training Loss: 0.0866987481713295\n",
      "Epoch 1832/30000 Training Loss: 0.11839846521615982\n",
      "Epoch 1833/30000 Training Loss: 0.09161975234746933\n",
      "Epoch 1834/30000 Training Loss: 0.09854420274496078\n",
      "Epoch 1835/30000 Training Loss: 0.11001074314117432\n",
      "Epoch 1836/30000 Training Loss: 0.11577349901199341\n",
      "Epoch 1837/30000 Training Loss: 0.10412648320198059\n",
      "Epoch 1838/30000 Training Loss: 0.09660086780786514\n",
      "Epoch 1839/30000 Training Loss: 0.0695062205195427\n",
      "Epoch 1840/30000 Training Loss: 0.08981584757566452\n",
      "Epoch 1840/30000 Validation Loss: 0.08985207229852676\n",
      "Epoch 1841/30000 Training Loss: 0.07014721632003784\n",
      "Epoch 1842/30000 Training Loss: 0.0985959991812706\n",
      "Epoch 1843/30000 Training Loss: 0.08838263154029846\n",
      "Epoch 1844/30000 Training Loss: 0.0849856436252594\n",
      "Epoch 1845/30000 Training Loss: 0.10939755290746689\n",
      "Epoch 1846/30000 Training Loss: 0.09238210320472717\n",
      "Epoch 1847/30000 Training Loss: 0.10089802742004395\n",
      "Epoch 1848/30000 Training Loss: 0.08753586560487747\n",
      "Epoch 1849/30000 Training Loss: 0.11016283184289932\n",
      "Epoch 1850/30000 Training Loss: 0.08995748311281204\n",
      "Epoch 1850/30000 Validation Loss: 0.07526320219039917\n",
      "Epoch 1851/30000 Training Loss: 0.0711810514330864\n",
      "Epoch 1852/30000 Training Loss: 0.0933220386505127\n",
      "Epoch 1853/30000 Training Loss: 0.1144227683544159\n",
      "Epoch 1854/30000 Training Loss: 0.0968407616019249\n",
      "Epoch 1855/30000 Training Loss: 0.09187097102403641\n",
      "Epoch 1856/30000 Training Loss: 0.09481089562177658\n",
      "Epoch 1857/30000 Training Loss: 0.07129471749067307\n",
      "Epoch 1858/30000 Training Loss: 0.09783849120140076\n",
      "Epoch 1859/30000 Training Loss: 0.09099019318819046\n",
      "Epoch 1860/30000 Training Loss: 0.09722384810447693\n",
      "Epoch 1860/30000 Validation Loss: 0.10735373944044113\n",
      "Epoch 1861/30000 Training Loss: 0.09827176481485367\n",
      "Epoch 1862/30000 Training Loss: 0.0757596492767334\n",
      "Epoch 1863/30000 Training Loss: 0.08205928653478622\n",
      "Epoch 1864/30000 Training Loss: 0.0840783342719078\n",
      "Epoch 1865/30000 Training Loss: 0.09834276884794235\n",
      "Epoch 1866/30000 Training Loss: 0.11017211526632309\n",
      "Epoch 1867/30000 Training Loss: 0.10544411092996597\n",
      "Epoch 1868/30000 Training Loss: 0.07367589324712753\n",
      "Epoch 1869/30000 Training Loss: 0.07507488876581192\n",
      "Epoch 1870/30000 Training Loss: 0.08321386575698853\n",
      "Epoch 1870/30000 Validation Loss: 0.07367243617773056\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07367243617773056<=============\n",
      "Epoch 1871/30000 Training Loss: 0.09413722902536392\n",
      "Epoch 1872/30000 Training Loss: 0.10891183465719223\n",
      "Epoch 1873/30000 Training Loss: 0.08782994747161865\n",
      "Epoch 1874/30000 Training Loss: 0.09449680894613266\n",
      "Epoch 1875/30000 Training Loss: 0.09340092539787292\n",
      "Epoch 1876/30000 Training Loss: 0.09344974160194397\n",
      "Epoch 1877/30000 Training Loss: 0.08814498037099838\n",
      "Epoch 1878/30000 Training Loss: 0.1080549955368042\n",
      "Epoch 1879/30000 Training Loss: 0.09083407372236252\n",
      "Epoch 1880/30000 Training Loss: 0.07699386030435562\n",
      "Epoch 1880/30000 Validation Loss: 0.07210328429937363\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07210328429937363<=============\n",
      "Epoch 1881/30000 Training Loss: 0.09082144498825073\n",
      "Epoch 1882/30000 Training Loss: 0.10134629160165787\n",
      "Epoch 1883/30000 Training Loss: 0.09852021932601929\n",
      "Epoch 1884/30000 Training Loss: 0.081367626786232\n",
      "Epoch 1885/30000 Training Loss: 0.08913758397102356\n",
      "Epoch 1886/30000 Training Loss: 0.10466209799051285\n",
      "Epoch 1887/30000 Training Loss: 0.08816781640052795\n",
      "Epoch 1888/30000 Training Loss: 0.08781462162733078\n",
      "Epoch 1889/30000 Training Loss: 0.09416339546442032\n",
      "Epoch 1890/30000 Training Loss: 0.09128955751657486\n",
      "Epoch 1890/30000 Validation Loss: 0.08147069811820984\n",
      "Epoch 1891/30000 Training Loss: 0.07993769645690918\n",
      "Epoch 1892/30000 Training Loss: 0.08276841789484024\n",
      "Epoch 1893/30000 Training Loss: 0.07262108474969864\n",
      "Epoch 1894/30000 Training Loss: 0.10628309845924377\n",
      "Epoch 1895/30000 Training Loss: 0.0842890739440918\n",
      "Epoch 1896/30000 Training Loss: 0.0916258692741394\n",
      "Epoch 1897/30000 Training Loss: 0.08547761291265488\n",
      "Epoch 1898/30000 Training Loss: 0.1226506233215332\n",
      "Epoch 1899/30000 Training Loss: 0.08214155584573746\n",
      "Epoch 1900/30000 Training Loss: 0.0880846381187439\n",
      "Epoch 1900/30000 Validation Loss: 0.09465666860342026\n",
      "Epoch 1901/30000 Training Loss: 0.06762783974409103\n",
      "Epoch 1902/30000 Training Loss: 0.10590344667434692\n",
      "Epoch 1903/30000 Training Loss: 0.08417155593633652\n",
      "Epoch 1904/30000 Training Loss: 0.10510998219251633\n",
      "Epoch 1905/30000 Training Loss: 0.11497699469327927\n",
      "Epoch 1906/30000 Training Loss: 0.08906065672636032\n",
      "Epoch 1907/30000 Training Loss: 0.10377363115549088\n",
      "Epoch 1908/30000 Training Loss: 0.08614236116409302\n",
      "Epoch 1909/30000 Training Loss: 0.07118651270866394\n",
      "Epoch 1910/30000 Training Loss: 0.07728460431098938\n",
      "Epoch 1910/30000 Validation Loss: 0.10047260671854019\n",
      "Epoch 1911/30000 Training Loss: 0.08176105469465256\n",
      "Epoch 1912/30000 Training Loss: 0.08939865231513977\n",
      "Epoch 1913/30000 Training Loss: 0.07044776529073715\n",
      "Epoch 1914/30000 Training Loss: 0.08347419649362564\n",
      "Epoch 1915/30000 Training Loss: 0.094937264919281\n",
      "Epoch 1916/30000 Training Loss: 0.09710109978914261\n",
      "Epoch 1917/30000 Training Loss: 0.09448639303445816\n",
      "Epoch 1918/30000 Training Loss: 0.06683063507080078\n",
      "Epoch 1919/30000 Training Loss: 0.10223276168107986\n",
      "Epoch 1920/30000 Training Loss: 0.07935943454504013\n",
      "Epoch 1920/30000 Validation Loss: 0.07783146947622299\n",
      "Epoch 1921/30000 Training Loss: 0.10251142829656601\n",
      "Epoch 1922/30000 Training Loss: 0.07633260637521744\n",
      "Epoch 1923/30000 Training Loss: 0.07999544590711594\n",
      "Epoch 1924/30000 Training Loss: 0.09555798023939133\n",
      "Epoch 1925/30000 Training Loss: 0.09666279703378677\n",
      "Epoch 1926/30000 Training Loss: 0.08217891305685043\n",
      "Epoch 1927/30000 Training Loss: 0.09196744114160538\n",
      "Epoch 1928/30000 Training Loss: 0.07640481740236282\n",
      "Epoch 1929/30000 Training Loss: 0.0851236879825592\n",
      "Epoch 1930/30000 Training Loss: 0.09284182637929916\n",
      "Epoch 1930/30000 Validation Loss: 0.08958184719085693\n",
      "Epoch 1931/30000 Training Loss: 0.09904700517654419\n",
      "Epoch 1932/30000 Training Loss: 0.09003034979104996\n",
      "Epoch 1933/30000 Training Loss: 0.08714956045150757\n",
      "Epoch 1934/30000 Training Loss: 0.07254882901906967\n",
      "Epoch 1935/30000 Training Loss: 0.11284774541854858\n",
      "Epoch 1936/30000 Training Loss: 0.1047104001045227\n",
      "Epoch 1937/30000 Training Loss: 0.08552217483520508\n",
      "Epoch 1938/30000 Training Loss: 0.09367219358682632\n",
      "Epoch 1939/30000 Training Loss: 0.08299616724252701\n",
      "Epoch 1940/30000 Training Loss: 0.08162066340446472\n",
      "Epoch 1940/30000 Validation Loss: 0.08098126202821732\n",
      "Epoch 1941/30000 Training Loss: 0.09375133365392685\n",
      "Epoch 1942/30000 Training Loss: 0.09427043795585632\n",
      "Epoch 1943/30000 Training Loss: 0.09606587886810303\n",
      "Epoch 1944/30000 Training Loss: 0.09980735182762146\n",
      "Epoch 1945/30000 Training Loss: 0.08284614235162735\n",
      "Epoch 1946/30000 Training Loss: 0.07068151235580444\n",
      "Epoch 1947/30000 Training Loss: 0.08771417289972305\n",
      "Epoch 1948/30000 Training Loss: 0.09453079104423523\n",
      "Epoch 1949/30000 Training Loss: 0.09965696185827255\n",
      "Epoch 1950/30000 Training Loss: 0.10269030928611755\n",
      "Epoch 1950/30000 Validation Loss: 0.07092378288507462\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07092378288507462<=============\n",
      "Epoch 1951/30000 Training Loss: 0.08310779929161072\n",
      "Epoch 1952/30000 Training Loss: 0.10369873046875\n",
      "Epoch 1953/30000 Training Loss: 0.1084609404206276\n",
      "Epoch 1954/30000 Training Loss: 0.0937262549996376\n",
      "Epoch 1955/30000 Training Loss: 0.11009097099304199\n",
      "Epoch 1956/30000 Training Loss: 0.07194725424051285\n",
      "Epoch 1957/30000 Training Loss: 0.07898146659135818\n",
      "Epoch 1958/30000 Training Loss: 0.09324867278337479\n",
      "Epoch 1959/30000 Training Loss: 0.09105449914932251\n",
      "Epoch 1960/30000 Training Loss: 0.09199518710374832\n",
      "Epoch 1960/30000 Validation Loss: 0.06875511258840561\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06875511258840561<=============\n",
      "Epoch 1961/30000 Training Loss: 0.08568784594535828\n",
      "Epoch 1962/30000 Training Loss: 0.08701866865158081\n",
      "Epoch 1963/30000 Training Loss: 0.10154219716787338\n",
      "Epoch 1964/30000 Training Loss: 0.1325654536485672\n",
      "Epoch 1965/30000 Training Loss: 0.07131396234035492\n",
      "Epoch 1966/30000 Training Loss: 0.07022207975387573\n",
      "Epoch 1967/30000 Training Loss: 0.09596019983291626\n",
      "Epoch 1968/30000 Training Loss: 0.06616677343845367\n",
      "Epoch 1969/30000 Training Loss: 0.09907909482717514\n",
      "Epoch 1970/30000 Training Loss: 0.09690067917108536\n",
      "Epoch 1970/30000 Validation Loss: 0.10153260082006454\n",
      "Epoch 1971/30000 Training Loss: 0.08264046162366867\n",
      "Epoch 1972/30000 Training Loss: 0.06766492873430252\n",
      "Epoch 1973/30000 Training Loss: 0.07082602381706238\n",
      "Epoch 1974/30000 Training Loss: 0.07166997343301773\n",
      "Epoch 1975/30000 Training Loss: 0.11929434537887573\n",
      "Epoch 1976/30000 Training Loss: 0.11309659481048584\n",
      "Epoch 1977/30000 Training Loss: 0.11282503604888916\n",
      "Epoch 1978/30000 Training Loss: 0.07597609609365463\n",
      "Epoch 1979/30000 Training Loss: 0.08150485903024673\n",
      "Epoch 1980/30000 Training Loss: 0.10225404053926468\n",
      "Epoch 1980/30000 Validation Loss: 0.08609654754400253\n",
      "Epoch 1981/30000 Training Loss: 0.09970500320196152\n",
      "Epoch 1982/30000 Training Loss: 0.08140962570905685\n",
      "Epoch 1983/30000 Training Loss: 0.09870617836713791\n",
      "Epoch 1984/30000 Training Loss: 0.08683488517999649\n",
      "Epoch 1985/30000 Training Loss: 0.08586630970239639\n",
      "Epoch 1986/30000 Training Loss: 0.10416562110185623\n",
      "Epoch 1987/30000 Training Loss: 0.08223900198936462\n",
      "Epoch 1988/30000 Training Loss: 0.10482340306043625\n",
      "Epoch 1989/30000 Training Loss: 0.12548334896564484\n",
      "Epoch 1990/30000 Training Loss: 0.0874025747179985\n",
      "Epoch 1990/30000 Validation Loss: 0.09877070784568787\n",
      "Epoch 1991/30000 Training Loss: 0.08663851022720337\n",
      "Epoch 1992/30000 Training Loss: 0.08101421594619751\n",
      "Epoch 1993/30000 Training Loss: 0.08852231502532959\n",
      "Epoch 1994/30000 Training Loss: 0.08334577828645706\n",
      "Epoch 1995/30000 Training Loss: 0.06947118788957596\n",
      "Epoch 1996/30000 Training Loss: 0.08444983512163162\n",
      "Epoch 1997/30000 Training Loss: 0.07105749845504761\n",
      "Epoch 1998/30000 Training Loss: 0.08727944642305374\n",
      "Epoch 1999/30000 Training Loss: 0.09185704588890076\n",
      "Epoch 2000/30000 Training Loss: 0.07788204401731491\n",
      "Epoch 2000/30000 Validation Loss: 0.08916708081960678\n",
      "Epoch 2001/30000 Training Loss: 0.10505139827728271\n",
      "Epoch 2002/30000 Training Loss: 0.09218230098485947\n",
      "Epoch 2003/30000 Training Loss: 0.08706533908843994\n",
      "Epoch 2004/30000 Training Loss: 0.08553514629602432\n",
      "Epoch 2005/30000 Training Loss: 0.09079285711050034\n",
      "Epoch 2006/30000 Training Loss: 0.08351671695709229\n",
      "Epoch 2007/30000 Training Loss: 0.12983962893486023\n",
      "Epoch 2008/30000 Training Loss: 0.0915779247879982\n",
      "Epoch 2009/30000 Training Loss: 0.09478923678398132\n",
      "Epoch 2010/30000 Training Loss: 0.07794126868247986\n",
      "Epoch 2010/30000 Validation Loss: 0.07536715269088745\n",
      "Epoch 2011/30000 Training Loss: 0.09707308560609818\n",
      "Epoch 2012/30000 Training Loss: 0.07776667922735214\n",
      "Epoch 2013/30000 Training Loss: 0.10165209323167801\n",
      "Epoch 2014/30000 Training Loss: 0.07624080032110214\n",
      "Epoch 2015/30000 Training Loss: 0.10607656091451645\n",
      "Epoch 2016/30000 Training Loss: 0.09618739038705826\n",
      "Epoch 2017/30000 Training Loss: 0.06239362061023712\n",
      "Epoch 2018/30000 Training Loss: 0.09474541991949081\n",
      "Epoch 2019/30000 Training Loss: 0.09464257210493088\n",
      "Epoch 2020/30000 Training Loss: 0.11016055196523666\n",
      "Epoch 2020/30000 Validation Loss: 0.08416064828634262\n",
      "Epoch 2021/30000 Training Loss: 0.11204840987920761\n",
      "Epoch 2022/30000 Training Loss: 0.07033395767211914\n",
      "Epoch 2023/30000 Training Loss: 0.0823773443698883\n",
      "Epoch 2024/30000 Training Loss: 0.0748685896396637\n",
      "Epoch 2025/30000 Training Loss: 0.09884495288133621\n",
      "Epoch 2026/30000 Training Loss: 0.08613892644643784\n",
      "Epoch 2027/30000 Training Loss: 0.08353009074926376\n",
      "Epoch 2028/30000 Training Loss: 0.0731092169880867\n",
      "Epoch 2029/30000 Training Loss: 0.09316138178110123\n",
      "Epoch 2030/30000 Training Loss: 0.09807521849870682\n",
      "Epoch 2030/30000 Validation Loss: 0.0707310289144516\n",
      "Epoch 2031/30000 Training Loss: 0.09953480213880539\n",
      "Epoch 2032/30000 Training Loss: 0.10120155662298203\n",
      "Epoch 2033/30000 Training Loss: 0.10457343608140945\n",
      "Epoch 2034/30000 Training Loss: 0.12379948049783707\n",
      "Epoch 2035/30000 Training Loss: 0.0754493698477745\n",
      "Epoch 2036/30000 Training Loss: 0.09814082831144333\n",
      "Epoch 2037/30000 Training Loss: 0.10090551525354385\n",
      "Epoch 2038/30000 Training Loss: 0.09183799475431442\n",
      "Epoch 2039/30000 Training Loss: 0.10089647769927979\n",
      "Epoch 2040/30000 Training Loss: 0.10140257328748703\n",
      "Epoch 2040/30000 Validation Loss: 0.11239445209503174\n",
      "Epoch 2041/30000 Training Loss: 0.10691952705383301\n",
      "Epoch 2042/30000 Training Loss: 0.10189163684844971\n",
      "Epoch 2043/30000 Training Loss: 0.08181381970643997\n",
      "Epoch 2044/30000 Training Loss: 0.10622543096542358\n",
      "Epoch 2045/30000 Training Loss: 0.08377780765295029\n",
      "Epoch 2046/30000 Training Loss: 0.0742754116654396\n",
      "Epoch 2047/30000 Training Loss: 0.09994536638259888\n",
      "Epoch 2048/30000 Training Loss: 0.06699958443641663\n",
      "Epoch 2049/30000 Training Loss: 0.0840030089020729\n",
      "Epoch 2050/30000 Training Loss: 0.09525842219591141\n",
      "Epoch 2050/30000 Validation Loss: 0.07835081964731216\n",
      "Epoch 2051/30000 Training Loss: 0.10962659120559692\n",
      "Epoch 2052/30000 Training Loss: 0.10009235888719559\n",
      "Epoch 2053/30000 Training Loss: 0.08536461740732193\n",
      "Epoch 2054/30000 Training Loss: 0.09366432577371597\n",
      "Epoch 2055/30000 Training Loss: 0.1029997244477272\n",
      "Epoch 2056/30000 Training Loss: 0.13142073154449463\n",
      "Epoch 2057/30000 Training Loss: 0.10394614934921265\n",
      "Epoch 2058/30000 Training Loss: 0.09669774770736694\n",
      "Epoch 2059/30000 Training Loss: 0.0903112068772316\n",
      "Epoch 2060/30000 Training Loss: 0.08958613872528076\n",
      "Epoch 2060/30000 Validation Loss: 0.09040194749832153\n",
      "Epoch 2061/30000 Training Loss: 0.08474931120872498\n",
      "Epoch 2062/30000 Training Loss: 0.08002132922410965\n",
      "Epoch 2063/30000 Training Loss: 0.0832480862736702\n",
      "Epoch 2064/30000 Training Loss: 0.08779439330101013\n",
      "Epoch 2065/30000 Training Loss: 0.082852303981781\n",
      "Epoch 2066/30000 Training Loss: 0.09773820638656616\n",
      "Epoch 2067/30000 Training Loss: 0.09954366832971573\n",
      "Epoch 2068/30000 Training Loss: 0.09423013776540756\n",
      "Epoch 2069/30000 Training Loss: 0.08825788646936417\n",
      "Epoch 2070/30000 Training Loss: 0.09058654308319092\n",
      "Epoch 2070/30000 Validation Loss: 0.08354046195745468\n",
      "Epoch 2071/30000 Training Loss: 0.08519285917282104\n",
      "Epoch 2072/30000 Training Loss: 0.08699751645326614\n",
      "Epoch 2073/30000 Training Loss: 0.13067398965358734\n",
      "Epoch 2074/30000 Training Loss: 0.0821586474776268\n",
      "Epoch 2075/30000 Training Loss: 0.08166790008544922\n",
      "Epoch 2076/30000 Training Loss: 0.10214591026306152\n",
      "Epoch 2077/30000 Training Loss: 0.08357136696577072\n",
      "Epoch 2078/30000 Training Loss: 0.09540104866027832\n",
      "Epoch 2079/30000 Training Loss: 0.09380334615707397\n",
      "Epoch 2080/30000 Training Loss: 0.08736120909452438\n",
      "Epoch 2080/30000 Validation Loss: 0.0848253145813942\n",
      "Epoch 2081/30000 Training Loss: 0.0787384882569313\n",
      "Epoch 2082/30000 Training Loss: 0.10509120672941208\n",
      "Epoch 2083/30000 Training Loss: 0.07047519832849503\n",
      "Epoch 2084/30000 Training Loss: 0.08655339479446411\n",
      "Epoch 2085/30000 Training Loss: 0.08243625611066818\n",
      "Epoch 2086/30000 Training Loss: 0.10260381549596786\n",
      "Epoch 2087/30000 Training Loss: 0.0823436751961708\n",
      "Epoch 2088/30000 Training Loss: 0.09982284158468246\n",
      "Epoch 2089/30000 Training Loss: 0.09176742285490036\n",
      "Epoch 2090/30000 Training Loss: 0.08700387924909592\n",
      "Epoch 2090/30000 Validation Loss: 0.07895886898040771\n",
      "Epoch 2091/30000 Training Loss: 0.1112574115395546\n",
      "Epoch 2092/30000 Training Loss: 0.06889104098081589\n",
      "Epoch 2093/30000 Training Loss: 0.07845043390989304\n",
      "Epoch 2094/30000 Training Loss: 0.0716773048043251\n",
      "Epoch 2095/30000 Training Loss: 0.0786326602101326\n",
      "Epoch 2096/30000 Training Loss: 0.08712875843048096\n",
      "Epoch 2097/30000 Training Loss: 0.09932021051645279\n",
      "Epoch 2098/30000 Training Loss: 0.07565716654062271\n",
      "Epoch 2099/30000 Training Loss: 0.06592101603746414\n",
      "Epoch 2100/30000 Training Loss: 0.09793785959482193\n",
      "Epoch 2100/30000 Validation Loss: 0.08993172645568848\n",
      "Epoch 2101/30000 Training Loss: 0.1052451804280281\n",
      "Epoch 2102/30000 Training Loss: 0.08509510010480881\n",
      "Epoch 2103/30000 Training Loss: 0.08550675958395004\n",
      "Epoch 2104/30000 Training Loss: 0.10645586252212524\n",
      "Epoch 2105/30000 Training Loss: 0.10286808013916016\n",
      "Epoch 2106/30000 Training Loss: 0.09965390712022781\n",
      "Epoch 2107/30000 Training Loss: 0.08247245103120804\n",
      "Epoch 2108/30000 Training Loss: 0.09883648157119751\n",
      "Epoch 2109/30000 Training Loss: 0.09775394201278687\n",
      "Epoch 2110/30000 Training Loss: 0.11112462729215622\n",
      "Epoch 2110/30000 Validation Loss: 0.08736581355333328\n",
      "Epoch 2111/30000 Training Loss: 0.06843940168619156\n",
      "Epoch 2112/30000 Training Loss: 0.11152531951665878\n",
      "Epoch 2113/30000 Training Loss: 0.08541786670684814\n",
      "Epoch 2114/30000 Training Loss: 0.09092005342245102\n",
      "Epoch 2115/30000 Training Loss: 0.07951602339744568\n",
      "Epoch 2116/30000 Training Loss: 0.08155837655067444\n",
      "Epoch 2117/30000 Training Loss: 0.09257248044013977\n",
      "Epoch 2118/30000 Training Loss: 0.10017860680818558\n",
      "Epoch 2119/30000 Training Loss: 0.07363902777433395\n",
      "Epoch 2120/30000 Training Loss: 0.08729112148284912\n",
      "Epoch 2120/30000 Validation Loss: 0.08867158740758896\n",
      "Epoch 2121/30000 Training Loss: 0.0896286889910698\n",
      "Epoch 2122/30000 Training Loss: 0.09096407145261765\n",
      "Epoch 2123/30000 Training Loss: 0.08238110691308975\n",
      "Epoch 2124/30000 Training Loss: 0.07910971343517303\n",
      "Epoch 2125/30000 Training Loss: 0.07766017317771912\n",
      "Epoch 2126/30000 Training Loss: 0.1001594066619873\n",
      "Epoch 2127/30000 Training Loss: 0.09773644059896469\n",
      "Epoch 2128/30000 Training Loss: 0.10886821895837784\n",
      "Epoch 2129/30000 Training Loss: 0.06851985305547714\n",
      "Epoch 2130/30000 Training Loss: 0.0976022258400917\n",
      "Epoch 2130/30000 Validation Loss: 0.0864412784576416\n",
      "Epoch 2131/30000 Training Loss: 0.09949309378862381\n",
      "Epoch 2132/30000 Training Loss: 0.08898966759443283\n",
      "Epoch 2133/30000 Training Loss: 0.08442315459251404\n",
      "Epoch 2134/30000 Training Loss: 0.08562221378087997\n",
      "Epoch 2135/30000 Training Loss: 0.08823534101247787\n",
      "Epoch 2136/30000 Training Loss: 0.08362563699483871\n",
      "Epoch 2137/30000 Training Loss: 0.07851652055978775\n",
      "Epoch 2138/30000 Training Loss: 0.10458242893218994\n",
      "Epoch 2139/30000 Training Loss: 0.0867789015173912\n",
      "Epoch 2140/30000 Training Loss: 0.08427643030881882\n",
      "Epoch 2140/30000 Validation Loss: 0.09948182106018066\n",
      "Epoch 2141/30000 Training Loss: 0.10685589909553528\n",
      "Epoch 2142/30000 Training Loss: 0.07444078475236893\n",
      "Epoch 2143/30000 Training Loss: 0.07313384860754013\n",
      "Epoch 2144/30000 Training Loss: 0.08977974206209183\n",
      "Epoch 2145/30000 Training Loss: 0.07836008816957474\n",
      "Epoch 2146/30000 Training Loss: 0.08825492858886719\n",
      "Epoch 2147/30000 Training Loss: 0.1059732437133789\n",
      "Epoch 2148/30000 Training Loss: 0.09064391255378723\n",
      "Epoch 2149/30000 Training Loss: 0.0889301672577858\n",
      "Epoch 2150/30000 Training Loss: 0.09026366472244263\n",
      "Epoch 2150/30000 Validation Loss: 0.09082940220832825\n",
      "Epoch 2151/30000 Training Loss: 0.07540888339281082\n",
      "Epoch 2152/30000 Training Loss: 0.08370982855558395\n",
      "Epoch 2153/30000 Training Loss: 0.10007854551076889\n",
      "Epoch 2154/30000 Training Loss: 0.10729683190584183\n",
      "Epoch 2155/30000 Training Loss: 0.09652653336524963\n",
      "Epoch 2156/30000 Training Loss: 0.09393636137247086\n",
      "Epoch 2157/30000 Training Loss: 0.09893975406885147\n",
      "Epoch 2158/30000 Training Loss: 0.09761053323745728\n",
      "Epoch 2159/30000 Training Loss: 0.0685083195567131\n",
      "Epoch 2160/30000 Training Loss: 0.07918544858694077\n",
      "Epoch 2160/30000 Validation Loss: 0.0870024785399437\n",
      "Epoch 2161/30000 Training Loss: 0.08433156460523605\n",
      "Epoch 2162/30000 Training Loss: 0.08927580714225769\n",
      "Epoch 2163/30000 Training Loss: 0.09400644153356552\n",
      "Epoch 2164/30000 Training Loss: 0.06841988861560822\n",
      "Epoch 2165/30000 Training Loss: 0.07939186692237854\n",
      "Epoch 2166/30000 Training Loss: 0.07744345813989639\n",
      "Epoch 2167/30000 Training Loss: 0.09257898479700089\n",
      "Epoch 2168/30000 Training Loss: 0.10897607356309891\n",
      "Epoch 2169/30000 Training Loss: 0.06412415206432343\n",
      "Epoch 2170/30000 Training Loss: 0.07146177440881729\n",
      "Epoch 2170/30000 Validation Loss: 0.10053237527608871\n",
      "Epoch 2171/30000 Training Loss: 0.07710172981023788\n",
      "Epoch 2172/30000 Training Loss: 0.09696630388498306\n",
      "Epoch 2173/30000 Training Loss: 0.0874013677239418\n",
      "Epoch 2174/30000 Training Loss: 0.12388795614242554\n",
      "Epoch 2175/30000 Training Loss: 0.08945196121931076\n",
      "Epoch 2176/30000 Training Loss: 0.09651578217744827\n",
      "Epoch 2177/30000 Training Loss: 0.08861235529184341\n",
      "Epoch 2178/30000 Training Loss: 0.08466701954603195\n",
      "Epoch 2179/30000 Training Loss: 0.10088374465703964\n",
      "Epoch 2180/30000 Training Loss: 0.10459315031766891\n",
      "Epoch 2180/30000 Validation Loss: 0.0933917835354805\n",
      "Epoch 2181/30000 Training Loss: 0.09242046624422073\n",
      "Epoch 2182/30000 Training Loss: 0.09303917735815048\n",
      "Epoch 2183/30000 Training Loss: 0.06540512293577194\n",
      "Epoch 2184/30000 Training Loss: 0.0816740095615387\n",
      "Epoch 2185/30000 Training Loss: 0.09351136535406113\n",
      "Epoch 2186/30000 Training Loss: 0.08676817268133163\n",
      "Epoch 2187/30000 Training Loss: 0.09545972943305969\n",
      "Epoch 2188/30000 Training Loss: 0.10472410172224045\n",
      "Epoch 2189/30000 Training Loss: 0.081554114818573\n",
      "Epoch 2190/30000 Training Loss: 0.07681333273649216\n",
      "Epoch 2190/30000 Validation Loss: 0.09987395256757736\n",
      "Epoch 2191/30000 Training Loss: 0.08569842576980591\n",
      "Epoch 2192/30000 Training Loss: 0.07652337104082108\n",
      "Epoch 2193/30000 Training Loss: 0.08866021037101746\n",
      "Epoch 2194/30000 Training Loss: 0.09034685045480728\n",
      "Epoch 2195/30000 Training Loss: 0.10154974460601807\n",
      "Epoch 2196/30000 Training Loss: 0.08052398264408112\n",
      "Epoch 2197/30000 Training Loss: 0.0639437809586525\n",
      "Epoch 2198/30000 Training Loss: 0.0939568355679512\n",
      "Epoch 2199/30000 Training Loss: 0.08754211664199829\n",
      "Epoch 2200/30000 Training Loss: 0.09775152802467346\n",
      "Epoch 2200/30000 Validation Loss: 0.08129861205816269\n",
      "Epoch 2201/30000 Training Loss: 0.09827927500009537\n",
      "Epoch 2202/30000 Training Loss: 0.09616366773843765\n",
      "Epoch 2203/30000 Training Loss: 0.05713960528373718\n",
      "Epoch 2204/30000 Training Loss: 0.10620642453432083\n",
      "Epoch 2205/30000 Training Loss: 0.07901116460561752\n",
      "Epoch 2206/30000 Training Loss: 0.11434494704008102\n",
      "Epoch 2207/30000 Training Loss: 0.08631742000579834\n",
      "Epoch 2208/30000 Training Loss: 0.07157861441373825\n",
      "Epoch 2209/30000 Training Loss: 0.08975216001272202\n",
      "Epoch 2210/30000 Training Loss: 0.0983477234840393\n",
      "Epoch 2210/30000 Validation Loss: 0.11870799213647842\n",
      "Epoch 2211/30000 Training Loss: 0.07894255965948105\n",
      "Epoch 2212/30000 Training Loss: 0.07560523599386215\n",
      "Epoch 2213/30000 Training Loss: 0.06794256716966629\n",
      "Epoch 2214/30000 Training Loss: 0.06181294098496437\n",
      "Epoch 2215/30000 Training Loss: 0.13598336279392242\n",
      "Epoch 2216/30000 Training Loss: 0.13140475749969482\n",
      "Epoch 2217/30000 Training Loss: 0.10476336628198624\n",
      "Epoch 2218/30000 Training Loss: 0.10940779000520706\n",
      "Epoch 2219/30000 Training Loss: 0.08978227525949478\n",
      "Epoch 2220/30000 Training Loss: 0.10899096727371216\n",
      "Epoch 2220/30000 Validation Loss: 0.07809869199991226\n",
      "Epoch 2221/30000 Training Loss: 0.12224116176366806\n",
      "Epoch 2222/30000 Training Loss: 0.09643372148275375\n",
      "Epoch 2223/30000 Training Loss: 0.0663614273071289\n",
      "Epoch 2224/30000 Training Loss: 0.07784059643745422\n",
      "Epoch 2225/30000 Training Loss: 0.08967193961143494\n",
      "Epoch 2226/30000 Training Loss: 0.0867827758193016\n",
      "Epoch 2227/30000 Training Loss: 0.0932149812579155\n",
      "Epoch 2228/30000 Training Loss: 0.08273693919181824\n",
      "Epoch 2229/30000 Training Loss: 0.07326828688383102\n",
      "Epoch 2230/30000 Training Loss: 0.09426557272672653\n",
      "Epoch 2230/30000 Validation Loss: 0.09767129272222519\n",
      "Epoch 2231/30000 Training Loss: 0.09990578144788742\n",
      "Epoch 2232/30000 Training Loss: 0.0926174744963646\n",
      "Epoch 2233/30000 Training Loss: 0.07818976789712906\n",
      "Epoch 2234/30000 Training Loss: 0.08275382220745087\n",
      "Epoch 2235/30000 Training Loss: 0.09444588422775269\n",
      "Epoch 2236/30000 Training Loss: 0.08551069349050522\n",
      "Epoch 2237/30000 Training Loss: 0.07022593915462494\n",
      "Epoch 2238/30000 Training Loss: 0.09540679305791855\n",
      "Epoch 2239/30000 Training Loss: 0.08872640132904053\n",
      "Epoch 2240/30000 Training Loss: 0.07981033623218536\n",
      "Epoch 2240/30000 Validation Loss: 0.0772211030125618\n",
      "Epoch 2241/30000 Training Loss: 0.0889308750629425\n",
      "Epoch 2242/30000 Training Loss: 0.08736053854227066\n",
      "Epoch 2243/30000 Training Loss: 0.07611801475286484\n",
      "Epoch 2244/30000 Training Loss: 0.10765721648931503\n",
      "Epoch 2245/30000 Training Loss: 0.06946934759616852\n",
      "Epoch 2246/30000 Training Loss: 0.07274913787841797\n",
      "Epoch 2247/30000 Training Loss: 0.07646175473928452\n",
      "Epoch 2248/30000 Training Loss: 0.09897593408823013\n",
      "Epoch 2249/30000 Training Loss: 0.09871359914541245\n",
      "Epoch 2250/30000 Training Loss: 0.08231334388256073\n",
      "Epoch 2250/30000 Validation Loss: 0.07303154468536377\n",
      "Epoch 2251/30000 Training Loss: 0.09523597359657288\n",
      "Epoch 2252/30000 Training Loss: 0.08096989244222641\n",
      "Epoch 2253/30000 Training Loss: 0.07413946837186813\n",
      "Epoch 2254/30000 Training Loss: 0.09740561246871948\n",
      "Epoch 2255/30000 Training Loss: 0.10658153146505356\n",
      "Epoch 2256/30000 Training Loss: 0.07135166972875595\n",
      "Epoch 2257/30000 Training Loss: 0.0844278559088707\n",
      "Epoch 2258/30000 Training Loss: 0.08181621134281158\n",
      "Epoch 2259/30000 Training Loss: 0.10441199690103531\n",
      "Epoch 2260/30000 Training Loss: 0.07827543467283249\n",
      "Epoch 2260/30000 Validation Loss: 0.08428213000297546\n",
      "Epoch 2261/30000 Training Loss: 0.07879191637039185\n",
      "Epoch 2262/30000 Training Loss: 0.10349980741739273\n",
      "Epoch 2263/30000 Training Loss: 0.09185445308685303\n",
      "Epoch 2264/30000 Training Loss: 0.09388776868581772\n",
      "Epoch 2265/30000 Training Loss: 0.0902315005660057\n",
      "Epoch 2266/30000 Training Loss: 0.1096472516655922\n",
      "Epoch 2267/30000 Training Loss: 0.09240889549255371\n",
      "Epoch 2268/30000 Training Loss: 0.09295959025621414\n",
      "Epoch 2269/30000 Training Loss: 0.09026686102151871\n",
      "Epoch 2270/30000 Training Loss: 0.08753400295972824\n",
      "Epoch 2270/30000 Validation Loss: 0.10165011137723923\n",
      "Epoch 2271/30000 Training Loss: 0.0799950435757637\n",
      "Epoch 2272/30000 Training Loss: 0.08853864669799805\n",
      "Epoch 2273/30000 Training Loss: 0.07660096138715744\n",
      "Epoch 2274/30000 Training Loss: 0.11100370436906815\n",
      "Epoch 2275/30000 Training Loss: 0.08482078462839127\n",
      "Epoch 2276/30000 Training Loss: 0.09254878759384155\n",
      "Epoch 2277/30000 Training Loss: 0.09106341749429703\n",
      "Epoch 2278/30000 Training Loss: 0.08155813813209534\n",
      "Epoch 2279/30000 Training Loss: 0.07124850153923035\n",
      "Epoch 2280/30000 Training Loss: 0.09172922372817993\n",
      "Epoch 2280/30000 Validation Loss: 0.07622624188661575\n",
      "Epoch 2281/30000 Training Loss: 0.09199231117963791\n",
      "Epoch 2282/30000 Training Loss: 0.07993987202644348\n",
      "Epoch 2283/30000 Training Loss: 0.08941052109003067\n",
      "Epoch 2284/30000 Training Loss: 0.07420701533555984\n",
      "Epoch 2285/30000 Training Loss: 0.08912283182144165\n",
      "Epoch 2286/30000 Training Loss: 0.08708503097295761\n",
      "Epoch 2287/30000 Training Loss: 0.08079373836517334\n",
      "Epoch 2288/30000 Training Loss: 0.10772264748811722\n",
      "Epoch 2289/30000 Training Loss: 0.09969735145568848\n",
      "Epoch 2290/30000 Training Loss: 0.07722310721874237\n",
      "Epoch 2290/30000 Validation Loss: 0.09332546591758728\n",
      "Epoch 2291/30000 Training Loss: 0.09197928756475449\n",
      "Epoch 2292/30000 Training Loss: 0.0886433944106102\n",
      "Epoch 2293/30000 Training Loss: 0.09502100199460983\n",
      "Epoch 2294/30000 Training Loss: 0.0856405571103096\n",
      "Epoch 2295/30000 Training Loss: 0.07216927409172058\n",
      "Epoch 2296/30000 Training Loss: 0.0773555338382721\n",
      "Epoch 2297/30000 Training Loss: 0.08389449119567871\n",
      "Epoch 2298/30000 Training Loss: 0.07675886899232864\n",
      "Epoch 2299/30000 Training Loss: 0.08994940668344498\n",
      "Epoch 2300/30000 Training Loss: 0.08689717203378677\n",
      "Epoch 2300/30000 Validation Loss: 0.09386574476957321\n",
      "Epoch 2301/30000 Training Loss: 0.08883295208215714\n",
      "Epoch 2302/30000 Training Loss: 0.10598509758710861\n",
      "Epoch 2303/30000 Training Loss: 0.09468378871679306\n",
      "Epoch 2304/30000 Training Loss: 0.07507592439651489\n",
      "Epoch 2305/30000 Training Loss: 0.09018702059984207\n",
      "Epoch 2306/30000 Training Loss: 0.10127011686563492\n",
      "Epoch 2307/30000 Training Loss: 0.11451316624879837\n",
      "Epoch 2308/30000 Training Loss: 0.10068286210298538\n",
      "Epoch 2309/30000 Training Loss: 0.0831659808754921\n",
      "Epoch 2310/30000 Training Loss: 0.09867540746927261\n",
      "Epoch 2310/30000 Validation Loss: 0.08080991357564926\n",
      "Epoch 2311/30000 Training Loss: 0.0728067085146904\n",
      "Epoch 2312/30000 Training Loss: 0.10053186863660812\n",
      "Epoch 2313/30000 Training Loss: 0.08821424096822739\n",
      "Epoch 2314/30000 Training Loss: 0.08404141664505005\n",
      "Epoch 2315/30000 Training Loss: 0.10034061223268509\n",
      "Epoch 2316/30000 Training Loss: 0.08773574978113174\n",
      "Epoch 2317/30000 Training Loss: 0.08803308755159378\n",
      "Epoch 2318/30000 Training Loss: 0.09838370233774185\n",
      "Epoch 2319/30000 Training Loss: 0.0640779659152031\n",
      "Epoch 2320/30000 Training Loss: 0.080849289894104\n",
      "Epoch 2320/30000 Validation Loss: 0.08322015404701233\n",
      "Epoch 2321/30000 Training Loss: 0.07381784915924072\n",
      "Epoch 2322/30000 Training Loss: 0.10630598664283752\n",
      "Epoch 2323/30000 Training Loss: 0.09305713325738907\n",
      "Epoch 2324/30000 Training Loss: 0.10871555656194687\n",
      "Epoch 2325/30000 Training Loss: 0.0878552719950676\n",
      "Epoch 2326/30000 Training Loss: 0.1122051477432251\n",
      "Epoch 2327/30000 Training Loss: 0.09602237492799759\n",
      "Epoch 2328/30000 Training Loss: 0.09970428794622421\n",
      "Epoch 2329/30000 Training Loss: 0.11042480915784836\n",
      "Epoch 2330/30000 Training Loss: 0.08873545378446579\n",
      "Epoch 2330/30000 Validation Loss: 0.09751038998365402\n",
      "Epoch 2331/30000 Training Loss: 0.09282054752111435\n",
      "Epoch 2332/30000 Training Loss: 0.11674373596906662\n",
      "Epoch 2333/30000 Training Loss: 0.09057270735502243\n",
      "Epoch 2334/30000 Training Loss: 0.07612700015306473\n",
      "Epoch 2335/30000 Training Loss: 0.08765818923711777\n",
      "Epoch 2336/30000 Training Loss: 0.07915452122688293\n",
      "Epoch 2337/30000 Training Loss: 0.06812279671430588\n",
      "Epoch 2338/30000 Training Loss: 0.08725956827402115\n",
      "Epoch 2339/30000 Training Loss: 0.10334502905607224\n",
      "Epoch 2340/30000 Training Loss: 0.0857158675789833\n",
      "Epoch 2340/30000 Validation Loss: 0.10061722993850708\n",
      "Epoch 2341/30000 Training Loss: 0.07388114929199219\n",
      "Epoch 2342/30000 Training Loss: 0.11398973315954208\n",
      "Epoch 2343/30000 Training Loss: 0.10578397661447525\n",
      "Epoch 2344/30000 Training Loss: 0.08153558522462845\n",
      "Epoch 2345/30000 Training Loss: 0.11308155208826065\n",
      "Epoch 2346/30000 Training Loss: 0.0891619548201561\n",
      "Epoch 2347/30000 Training Loss: 0.076097272336483\n",
      "Epoch 2348/30000 Training Loss: 0.08817977458238602\n",
      "Epoch 2349/30000 Training Loss: 0.07251203060150146\n",
      "Epoch 2350/30000 Training Loss: 0.09847982972860336\n",
      "Epoch 2350/30000 Validation Loss: 0.0868803858757019\n",
      "Epoch 2351/30000 Training Loss: 0.0806506797671318\n",
      "Epoch 2352/30000 Training Loss: 0.08819488435983658\n",
      "Epoch 2353/30000 Training Loss: 0.0885339304804802\n",
      "Epoch 2354/30000 Training Loss: 0.10349208116531372\n",
      "Epoch 2355/30000 Training Loss: 0.09903371334075928\n",
      "Epoch 2356/30000 Training Loss: 0.09708762168884277\n",
      "Epoch 2357/30000 Training Loss: 0.08815596252679825\n",
      "Epoch 2358/30000 Training Loss: 0.09820755571126938\n",
      "Epoch 2359/30000 Training Loss: 0.07560696452856064\n",
      "Epoch 2360/30000 Training Loss: 0.07118295878171921\n",
      "Epoch 2360/30000 Validation Loss: 0.06706377863883972\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06706377863883972<=============\n",
      "Epoch 2361/30000 Training Loss: 0.09013543277978897\n",
      "Epoch 2362/30000 Training Loss: 0.12000694125890732\n",
      "Epoch 2363/30000 Training Loss: 0.07159379869699478\n",
      "Epoch 2364/30000 Training Loss: 0.07057594507932663\n",
      "Epoch 2365/30000 Training Loss: 0.0809326022863388\n",
      "Epoch 2366/30000 Training Loss: 0.09354660660028458\n",
      "Epoch 2367/30000 Training Loss: 0.07659471780061722\n",
      "Epoch 2368/30000 Training Loss: 0.08502770215272903\n",
      "Epoch 2369/30000 Training Loss: 0.08813100308179855\n",
      "Epoch 2370/30000 Training Loss: 0.0854300931096077\n",
      "Epoch 2370/30000 Validation Loss: 0.09482567757368088\n",
      "Epoch 2371/30000 Training Loss: 0.10829248279333115\n",
      "Epoch 2372/30000 Training Loss: 0.11521586030721664\n",
      "Epoch 2373/30000 Training Loss: 0.05828699469566345\n",
      "Epoch 2374/30000 Training Loss: 0.07995332032442093\n",
      "Epoch 2375/30000 Training Loss: 0.09525123983621597\n",
      "Epoch 2376/30000 Training Loss: 0.08552690595388412\n",
      "Epoch 2377/30000 Training Loss: 0.07559575885534286\n",
      "Epoch 2378/30000 Training Loss: 0.10359877347946167\n",
      "Epoch 2379/30000 Training Loss: 0.0965326726436615\n",
      "Epoch 2380/30000 Training Loss: 0.08776994794607162\n",
      "Epoch 2380/30000 Validation Loss: 0.10003602504730225\n",
      "Epoch 2381/30000 Training Loss: 0.08230119198560715\n",
      "Epoch 2382/30000 Training Loss: 0.07427235692739487\n",
      "Epoch 2383/30000 Training Loss: 0.08860749751329422\n",
      "Epoch 2384/30000 Training Loss: 0.114429771900177\n",
      "Epoch 2385/30000 Training Loss: 0.09897493571043015\n",
      "Epoch 2386/30000 Training Loss: 0.0785633847117424\n",
      "Epoch 2387/30000 Training Loss: 0.07959481328725815\n",
      "Epoch 2388/30000 Training Loss: 0.07468173652887344\n",
      "Epoch 2389/30000 Training Loss: 0.10859682410955429\n",
      "Epoch 2390/30000 Training Loss: 0.09652554988861084\n",
      "Epoch 2390/30000 Validation Loss: 0.1132000982761383\n",
      "Epoch 2391/30000 Training Loss: 0.07911034673452377\n",
      "Epoch 2392/30000 Training Loss: 0.0877508744597435\n",
      "Epoch 2393/30000 Training Loss: 0.09175338596105576\n",
      "Epoch 2394/30000 Training Loss: 0.10253407806158066\n",
      "Epoch 2395/30000 Training Loss: 0.08808711171150208\n",
      "Epoch 2396/30000 Training Loss: 0.09281077235937119\n",
      "Epoch 2397/30000 Training Loss: 0.07239115238189697\n",
      "Epoch 2398/30000 Training Loss: 0.096669502556324\n",
      "Epoch 2399/30000 Training Loss: 0.07877025008201599\n",
      "Epoch 2400/30000 Training Loss: 0.07737497240304947\n",
      "Epoch 2400/30000 Validation Loss: 0.10065782070159912\n",
      "Epoch 2401/30000 Training Loss: 0.10834648460149765\n",
      "Epoch 2402/30000 Training Loss: 0.08374864608049393\n",
      "Epoch 2403/30000 Training Loss: 0.08828128129243851\n",
      "Epoch 2404/30000 Training Loss: 0.08261971920728683\n",
      "Epoch 2405/30000 Training Loss: 0.086587093770504\n",
      "Epoch 2406/30000 Training Loss: 0.06885877251625061\n",
      "Epoch 2407/30000 Training Loss: 0.0976426899433136\n",
      "Epoch 2408/30000 Training Loss: 0.09512448310852051\n",
      "Epoch 2409/30000 Training Loss: 0.08072149008512497\n",
      "Epoch 2410/30000 Training Loss: 0.06962008774280548\n",
      "Epoch 2410/30000 Validation Loss: 0.11785709112882614\n",
      "Epoch 2411/30000 Training Loss: 0.10420221835374832\n",
      "Epoch 2412/30000 Training Loss: 0.09088963270187378\n",
      "Epoch 2413/30000 Training Loss: 0.08619911223649979\n",
      "Epoch 2414/30000 Training Loss: 0.07968322932720184\n",
      "Epoch 2415/30000 Training Loss: 0.10485026985406876\n",
      "Epoch 2416/30000 Training Loss: 0.07348182052373886\n",
      "Epoch 2417/30000 Training Loss: 0.07850804924964905\n",
      "Epoch 2418/30000 Training Loss: 0.07680442184209824\n",
      "Epoch 2419/30000 Training Loss: 0.09499315172433853\n",
      "Epoch 2420/30000 Training Loss: 0.09325773268938065\n",
      "Epoch 2420/30000 Validation Loss: 0.09910400956869125\n",
      "Epoch 2421/30000 Training Loss: 0.07479893416166306\n",
      "Epoch 2422/30000 Training Loss: 0.08237651735544205\n",
      "Epoch 2423/30000 Training Loss: 0.07557670027017593\n",
      "Epoch 2424/30000 Training Loss: 0.08279022574424744\n",
      "Epoch 2425/30000 Training Loss: 0.09155956655740738\n",
      "Epoch 2426/30000 Training Loss: 0.0925021693110466\n",
      "Epoch 2427/30000 Training Loss: 0.08578559011220932\n",
      "Epoch 2428/30000 Training Loss: 0.1008082702755928\n",
      "Epoch 2429/30000 Training Loss: 0.07906998693943024\n",
      "Epoch 2430/30000 Training Loss: 0.08013220876455307\n",
      "Epoch 2430/30000 Validation Loss: 0.08685523271560669\n",
      "Epoch 2431/30000 Training Loss: 0.09385663270950317\n",
      "Epoch 2432/30000 Training Loss: 0.09422215074300766\n",
      "Epoch 2433/30000 Training Loss: 0.09866733103990555\n",
      "Epoch 2434/30000 Training Loss: 0.10310328006744385\n",
      "Epoch 2435/30000 Training Loss: 0.07978951185941696\n",
      "Epoch 2436/30000 Training Loss: 0.06576938927173615\n",
      "Epoch 2437/30000 Training Loss: 0.10098832845687866\n",
      "Epoch 2438/30000 Training Loss: 0.08299034833908081\n",
      "Epoch 2439/30000 Training Loss: 0.09118852764368057\n",
      "Epoch 2440/30000 Training Loss: 0.10059040784835815\n",
      "Epoch 2440/30000 Validation Loss: 0.0761515200138092\n",
      "Epoch 2441/30000 Training Loss: 0.08514588326215744\n",
      "Epoch 2442/30000 Training Loss: 0.06382142752408981\n",
      "Epoch 2443/30000 Training Loss: 0.09289810061454773\n",
      "Epoch 2444/30000 Training Loss: 0.09358804672956467\n",
      "Epoch 2445/30000 Training Loss: 0.08187083154916763\n",
      "Epoch 2446/30000 Training Loss: 0.07751001417636871\n",
      "Epoch 2447/30000 Training Loss: 0.09409809112548828\n",
      "Epoch 2448/30000 Training Loss: 0.09714699536561966\n",
      "Epoch 2449/30000 Training Loss: 0.08557427674531937\n",
      "Epoch 2450/30000 Training Loss: 0.11701711267232895\n",
      "Epoch 2450/30000 Validation Loss: 0.05782772973179817\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05782772973179817<=============\n",
      "Epoch 2451/30000 Training Loss: 0.10251104086637497\n",
      "Epoch 2452/30000 Training Loss: 0.09322579950094223\n",
      "Epoch 2453/30000 Training Loss: 0.11820840835571289\n",
      "Epoch 2454/30000 Training Loss: 0.0632130354642868\n",
      "Epoch 2455/30000 Training Loss: 0.07569268345832825\n",
      "Epoch 2456/30000 Training Loss: 0.09276005625724792\n",
      "Epoch 2457/30000 Training Loss: 0.06574896723031998\n",
      "Epoch 2458/30000 Training Loss: 0.08712965250015259\n",
      "Epoch 2459/30000 Training Loss: 0.08682478219270706\n",
      "Epoch 2460/30000 Training Loss: 0.11189067363739014\n",
      "Epoch 2460/30000 Validation Loss: 0.09378022700548172\n",
      "Epoch 2461/30000 Training Loss: 0.08944948762655258\n",
      "Epoch 2462/30000 Training Loss: 0.09797525405883789\n",
      "Epoch 2463/30000 Training Loss: 0.11093328148126602\n",
      "Epoch 2464/30000 Training Loss: 0.0874684751033783\n",
      "Epoch 2465/30000 Training Loss: 0.07509786635637283\n",
      "Epoch 2466/30000 Training Loss: 0.07787513732910156\n",
      "Epoch 2467/30000 Training Loss: 0.08286342769861221\n",
      "Epoch 2468/30000 Training Loss: 0.09364978224039078\n",
      "Epoch 2469/30000 Training Loss: 0.07160047441720963\n",
      "Epoch 2470/30000 Training Loss: 0.09710348397493362\n",
      "Epoch 2470/30000 Validation Loss: 0.09159734845161438\n",
      "Epoch 2471/30000 Training Loss: 0.07395387440919876\n",
      "Epoch 2472/30000 Training Loss: 0.09191414713859558\n",
      "Epoch 2473/30000 Training Loss: 0.08430366963148117\n",
      "Epoch 2474/30000 Training Loss: 0.07101716846227646\n",
      "Epoch 2475/30000 Training Loss: 0.09727019816637039\n",
      "Epoch 2476/30000 Training Loss: 0.07765063643455505\n",
      "Epoch 2477/30000 Training Loss: 0.08354050666093826\n",
      "Epoch 2478/30000 Training Loss: 0.08270341157913208\n",
      "Epoch 2479/30000 Training Loss: 0.09856849163770676\n",
      "Epoch 2480/30000 Training Loss: 0.09459789842367172\n",
      "Epoch 2480/30000 Validation Loss: 0.09930489212274551\n",
      "Epoch 2481/30000 Training Loss: 0.09966740012168884\n",
      "Epoch 2482/30000 Training Loss: 0.13689884543418884\n",
      "Epoch 2483/30000 Training Loss: 0.09952438622713089\n",
      "Epoch 2484/30000 Training Loss: 0.10541952401399612\n",
      "Epoch 2485/30000 Training Loss: 0.0734705999493599\n",
      "Epoch 2486/30000 Training Loss: 0.08491126447916031\n",
      "Epoch 2487/30000 Training Loss: 0.10288199782371521\n",
      "Epoch 2488/30000 Training Loss: 0.07774049788713455\n",
      "Epoch 2489/30000 Training Loss: 0.10201811790466309\n",
      "Epoch 2490/30000 Training Loss: 0.09767419099807739\n",
      "Epoch 2490/30000 Validation Loss: 0.06306130439043045\n",
      "Epoch 2491/30000 Training Loss: 0.0943429172039032\n",
      "Epoch 2492/30000 Training Loss: 0.10569336265325546\n",
      "Epoch 2493/30000 Training Loss: 0.10408564656972885\n",
      "Epoch 2494/30000 Training Loss: 0.0767136737704277\n",
      "Epoch 2495/30000 Training Loss: 0.09715742617845535\n",
      "Epoch 2496/30000 Training Loss: 0.08452102541923523\n",
      "Epoch 2497/30000 Training Loss: 0.0751652792096138\n",
      "Epoch 2498/30000 Training Loss: 0.10664712637662888\n",
      "Epoch 2499/30000 Training Loss: 0.07873431593179703\n",
      "Epoch 2500/30000 Training Loss: 0.08883761614561081\n",
      "Epoch 2500/30000 Validation Loss: 0.11412877589464188\n",
      "Epoch 2501/30000 Training Loss: 0.08127289265394211\n",
      "Epoch 2502/30000 Training Loss: 0.09370102733373642\n",
      "Epoch 2503/30000 Training Loss: 0.10316657274961472\n",
      "Epoch 2504/30000 Training Loss: 0.07225758582353592\n",
      "Epoch 2505/30000 Training Loss: 0.0779414176940918\n",
      "Epoch 2506/30000 Training Loss: 0.09619730710983276\n",
      "Epoch 2507/30000 Training Loss: 0.07377775758504868\n",
      "Epoch 2508/30000 Training Loss: 0.10312989354133606\n",
      "Epoch 2509/30000 Training Loss: 0.06385339796543121\n",
      "Epoch 2510/30000 Training Loss: 0.09314007312059402\n",
      "Epoch 2510/30000 Validation Loss: 0.08065535873174667\n",
      "Epoch 2511/30000 Training Loss: 0.056376367807388306\n",
      "Epoch 2512/30000 Training Loss: 0.07813604921102524\n",
      "Epoch 2513/30000 Training Loss: 0.09437359124422073\n",
      "Epoch 2514/30000 Training Loss: 0.07880958169698715\n",
      "Epoch 2515/30000 Training Loss: 0.08711722493171692\n",
      "Epoch 2516/30000 Training Loss: 0.10820218175649643\n",
      "Epoch 2517/30000 Training Loss: 0.07630922645330429\n",
      "Epoch 2518/30000 Training Loss: 0.08244241774082184\n",
      "Epoch 2519/30000 Training Loss: 0.07488439232110977\n",
      "Epoch 2520/30000 Training Loss: 0.11758512258529663\n",
      "Epoch 2520/30000 Validation Loss: 0.07495515793561935\n",
      "Epoch 2521/30000 Training Loss: 0.08908405154943466\n",
      "Epoch 2522/30000 Training Loss: 0.081507109105587\n",
      "Epoch 2523/30000 Training Loss: 0.0889253243803978\n",
      "Epoch 2524/30000 Training Loss: 0.0867513045668602\n",
      "Epoch 2525/30000 Training Loss: 0.07898885756731033\n",
      "Epoch 2526/30000 Training Loss: 0.08545904606580734\n",
      "Epoch 2527/30000 Training Loss: 0.10518866032361984\n",
      "Epoch 2528/30000 Training Loss: 0.09874383360147476\n",
      "Epoch 2529/30000 Training Loss: 0.0669030174612999\n",
      "Epoch 2530/30000 Training Loss: 0.11028271913528442\n",
      "Epoch 2530/30000 Validation Loss: 0.07935328036546707\n",
      "Epoch 2531/30000 Training Loss: 0.08991370350122452\n",
      "Epoch 2532/30000 Training Loss: 0.08172740787267685\n",
      "Epoch 2533/30000 Training Loss: 0.07155060023069382\n",
      "Epoch 2534/30000 Training Loss: 0.08931205421686172\n",
      "Epoch 2535/30000 Training Loss: 0.07147856801748276\n",
      "Epoch 2536/30000 Training Loss: 0.09236526489257812\n",
      "Epoch 2537/30000 Training Loss: 0.10763037204742432\n",
      "Epoch 2538/30000 Training Loss: 0.08609464764595032\n",
      "Epoch 2539/30000 Training Loss: 0.08008858561515808\n",
      "Epoch 2540/30000 Training Loss: 0.07630911469459534\n",
      "Epoch 2540/30000 Validation Loss: 0.09704885631799698\n",
      "Epoch 2541/30000 Training Loss: 0.0942184329032898\n",
      "Epoch 2542/30000 Training Loss: 0.09506934136152267\n",
      "Epoch 2543/30000 Training Loss: 0.0959547683596611\n",
      "Epoch 2544/30000 Training Loss: 0.08171354979276657\n",
      "Epoch 2545/30000 Training Loss: 0.10232120752334595\n",
      "Epoch 2546/30000 Training Loss: 0.07113919407129288\n",
      "Epoch 2547/30000 Training Loss: 0.10256747156381607\n",
      "Epoch 2548/30000 Training Loss: 0.08828741312026978\n",
      "Epoch 2549/30000 Training Loss: 0.0783892571926117\n",
      "Epoch 2550/30000 Training Loss: 0.07962889224290848\n",
      "Epoch 2550/30000 Validation Loss: 0.07881776243448257\n",
      "Epoch 2551/30000 Training Loss: 0.09091120958328247\n",
      "Epoch 2552/30000 Training Loss: 0.08201111108064651\n",
      "Epoch 2553/30000 Training Loss: 0.09075329452753067\n",
      "Epoch 2554/30000 Training Loss: 0.08793536573648453\n",
      "Epoch 2555/30000 Training Loss: 0.08276499062776566\n",
      "Epoch 2556/30000 Training Loss: 0.060310136526823044\n",
      "Epoch 2557/30000 Training Loss: 0.06879761070013046\n",
      "Epoch 2558/30000 Training Loss: 0.07778038829565048\n",
      "Epoch 2559/30000 Training Loss: 0.08949974924325943\n",
      "Epoch 2560/30000 Training Loss: 0.08220180124044418\n",
      "Epoch 2560/30000 Validation Loss: 0.0793849304318428\n",
      "Epoch 2561/30000 Training Loss: 0.08934137970209122\n",
      "Epoch 2562/30000 Training Loss: 0.10006777197122574\n",
      "Epoch 2563/30000 Training Loss: 0.08985646814107895\n",
      "Epoch 2564/30000 Training Loss: 0.08248133212327957\n",
      "Epoch 2565/30000 Training Loss: 0.07142040878534317\n",
      "Epoch 2566/30000 Training Loss: 0.07680179923772812\n",
      "Epoch 2567/30000 Training Loss: 0.09154041856527328\n",
      "Epoch 2568/30000 Training Loss: 0.08296047896146774\n",
      "Epoch 2569/30000 Training Loss: 0.10560449957847595\n",
      "Epoch 2570/30000 Training Loss: 0.0971909835934639\n",
      "Epoch 2570/30000 Validation Loss: 0.06302722543478012\n",
      "Epoch 2571/30000 Training Loss: 0.09836790710687637\n",
      "Epoch 2572/30000 Training Loss: 0.07400405406951904\n",
      "Epoch 2573/30000 Training Loss: 0.07772845029830933\n",
      "Epoch 2574/30000 Training Loss: 0.10479261726140976\n",
      "Epoch 2575/30000 Training Loss: 0.07572527974843979\n",
      "Epoch 2576/30000 Training Loss: 0.09926421195268631\n",
      "Epoch 2577/30000 Training Loss: 0.06606653332710266\n",
      "Epoch 2578/30000 Training Loss: 0.09908944368362427\n",
      "Epoch 2579/30000 Training Loss: 0.08537379652261734\n",
      "Epoch 2580/30000 Training Loss: 0.08391038328409195\n",
      "Epoch 2580/30000 Validation Loss: 0.0819750651717186\n",
      "Epoch 2581/30000 Training Loss: 0.08337131887674332\n",
      "Epoch 2582/30000 Training Loss: 0.08186065405607224\n",
      "Epoch 2583/30000 Training Loss: 0.08513152599334717\n",
      "Epoch 2584/30000 Training Loss: 0.06410707533359528\n",
      "Epoch 2585/30000 Training Loss: 0.08483856171369553\n",
      "Epoch 2586/30000 Training Loss: 0.0917162075638771\n",
      "Epoch 2587/30000 Training Loss: 0.09723782539367676\n",
      "Epoch 2588/30000 Training Loss: 0.096993088722229\n",
      "Epoch 2589/30000 Training Loss: 0.08062649518251419\n",
      "Epoch 2590/30000 Training Loss: 0.11190083622932434\n",
      "Epoch 2590/30000 Validation Loss: 0.08483517915010452\n",
      "Epoch 2591/30000 Training Loss: 0.0873018130660057\n",
      "Epoch 2592/30000 Training Loss: 0.09704819321632385\n",
      "Epoch 2593/30000 Training Loss: 0.07567496597766876\n",
      "Epoch 2594/30000 Training Loss: 0.09743357449769974\n",
      "Epoch 2595/30000 Training Loss: 0.1084635928273201\n",
      "Epoch 2596/30000 Training Loss: 0.07345035672187805\n",
      "Epoch 2597/30000 Training Loss: 0.09321942180395126\n",
      "Epoch 2598/30000 Training Loss: 0.07554019242525101\n",
      "Epoch 2599/30000 Training Loss: 0.10706125944852829\n",
      "Epoch 2600/30000 Training Loss: 0.10779234766960144\n",
      "Epoch 2600/30000 Validation Loss: 0.09525644779205322\n",
      "Epoch 2601/30000 Training Loss: 0.07160242646932602\n",
      "Epoch 2602/30000 Training Loss: 0.09129662066698074\n",
      "Epoch 2603/30000 Training Loss: 0.08720072358846664\n",
      "Epoch 2604/30000 Training Loss: 0.08087349683046341\n",
      "Epoch 2605/30000 Training Loss: 0.07474076747894287\n",
      "Epoch 2606/30000 Training Loss: 0.09772541373968124\n",
      "Epoch 2607/30000 Training Loss: 0.07575962692499161\n",
      "Epoch 2608/30000 Training Loss: 0.08908184617757797\n",
      "Epoch 2609/30000 Training Loss: 0.07189323753118515\n",
      "Epoch 2610/30000 Training Loss: 0.09720740467309952\n",
      "Epoch 2610/30000 Validation Loss: 0.09587264060974121\n",
      "Epoch 2611/30000 Training Loss: 0.08363962918519974\n",
      "Epoch 2612/30000 Training Loss: 0.08162898570299149\n",
      "Epoch 2613/30000 Training Loss: 0.10527358204126358\n",
      "Epoch 2614/30000 Training Loss: 0.0878218412399292\n",
      "Epoch 2615/30000 Training Loss: 0.07621344923973083\n",
      "Epoch 2616/30000 Training Loss: 0.09508498758077621\n",
      "Epoch 2617/30000 Training Loss: 0.08948957175016403\n",
      "Epoch 2618/30000 Training Loss: 0.0850675106048584\n",
      "Epoch 2619/30000 Training Loss: 0.08809202909469604\n",
      "Epoch 2620/30000 Training Loss: 0.08379122614860535\n",
      "Epoch 2620/30000 Validation Loss: 0.09223312139511108\n",
      "Epoch 2621/30000 Training Loss: 0.0746413990855217\n",
      "Epoch 2622/30000 Training Loss: 0.08315424621105194\n",
      "Epoch 2623/30000 Training Loss: 0.0670204684138298\n",
      "Epoch 2624/30000 Training Loss: 0.0753331258893013\n",
      "Epoch 2625/30000 Training Loss: 0.08726563304662704\n",
      "Epoch 2626/30000 Training Loss: 0.0800725594162941\n",
      "Epoch 2627/30000 Training Loss: 0.10330913215875626\n",
      "Epoch 2628/30000 Training Loss: 0.10427210479974747\n",
      "Epoch 2629/30000 Training Loss: 0.10830724239349365\n",
      "Epoch 2630/30000 Training Loss: 0.09210275858640671\n",
      "Epoch 2630/30000 Validation Loss: 0.07106930762529373\n",
      "Epoch 2631/30000 Training Loss: 0.07949575036764145\n",
      "Epoch 2632/30000 Training Loss: 0.09809733182191849\n",
      "Epoch 2633/30000 Training Loss: 0.10351059585809708\n",
      "Epoch 2634/30000 Training Loss: 0.07953982800245285\n",
      "Epoch 2635/30000 Training Loss: 0.0903657078742981\n",
      "Epoch 2636/30000 Training Loss: 0.07767334580421448\n",
      "Epoch 2637/30000 Training Loss: 0.07925590872764587\n",
      "Epoch 2638/30000 Training Loss: 0.11575529724359512\n",
      "Epoch 2639/30000 Training Loss: 0.06762856245040894\n",
      "Epoch 2640/30000 Training Loss: 0.06134885549545288\n",
      "Epoch 2640/30000 Validation Loss: 0.07948006689548492\n",
      "Epoch 2641/30000 Training Loss: 0.08817718178033829\n",
      "Epoch 2642/30000 Training Loss: 0.1265624612569809\n",
      "Epoch 2643/30000 Training Loss: 0.0740363597869873\n",
      "Epoch 2644/30000 Training Loss: 0.10462304949760437\n",
      "Epoch 2645/30000 Training Loss: 0.09359914809465408\n",
      "Epoch 2646/30000 Training Loss: 0.08693116903305054\n",
      "Epoch 2647/30000 Training Loss: 0.08040828257799149\n",
      "Epoch 2648/30000 Training Loss: 0.08828053623437881\n",
      "Epoch 2649/30000 Training Loss: 0.1048479899764061\n",
      "Epoch 2650/30000 Training Loss: 0.07233292609453201\n",
      "Epoch 2650/30000 Validation Loss: 0.10202217102050781\n",
      "Epoch 2651/30000 Training Loss: 0.07421201467514038\n",
      "Epoch 2652/30000 Training Loss: 0.0622289814054966\n",
      "Epoch 2653/30000 Training Loss: 0.07424143701791763\n",
      "Epoch 2654/30000 Training Loss: 0.054625142365694046\n",
      "Epoch 2655/30000 Training Loss: 0.08194058388471603\n",
      "Epoch 2656/30000 Training Loss: 0.08819744735956192\n",
      "Epoch 2657/30000 Training Loss: 0.09879513829946518\n",
      "Epoch 2658/30000 Training Loss: 0.0714292898774147\n",
      "Epoch 2659/30000 Training Loss: 0.09512567520141602\n",
      "Epoch 2660/30000 Training Loss: 0.10500360280275345\n",
      "Epoch 2660/30000 Validation Loss: 0.09022727608680725\n",
      "Epoch 2661/30000 Training Loss: 0.07506883889436722\n",
      "Epoch 2662/30000 Training Loss: 0.08197801560163498\n",
      "Epoch 2663/30000 Training Loss: 0.08254163712263107\n",
      "Epoch 2664/30000 Training Loss: 0.10680142790079117\n",
      "Epoch 2665/30000 Training Loss: 0.09328162670135498\n",
      "Epoch 2666/30000 Training Loss: 0.08904219418764114\n",
      "Epoch 2667/30000 Training Loss: 0.06917482614517212\n",
      "Epoch 2668/30000 Training Loss: 0.06501971930265427\n",
      "Epoch 2669/30000 Training Loss: 0.09788957238197327\n",
      "Epoch 2670/30000 Training Loss: 0.07205754518508911\n",
      "Epoch 2670/30000 Validation Loss: 0.08655175566673279\n",
      "Epoch 2671/30000 Training Loss: 0.07247711718082428\n",
      "Epoch 2672/30000 Training Loss: 0.08169467002153397\n",
      "Epoch 2673/30000 Training Loss: 0.07129734009504318\n",
      "Epoch 2674/30000 Training Loss: 0.05948106572031975\n",
      "Epoch 2675/30000 Training Loss: 0.08074159175157547\n",
      "Epoch 2676/30000 Training Loss: 0.09498307853937149\n",
      "Epoch 2677/30000 Training Loss: 0.07679349184036255\n",
      "Epoch 2678/30000 Training Loss: 0.08523142337799072\n",
      "Epoch 2679/30000 Training Loss: 0.07821226865053177\n",
      "Epoch 2680/30000 Training Loss: 0.10174372047185898\n",
      "Epoch 2680/30000 Validation Loss: 0.10163279622793198\n",
      "Epoch 2681/30000 Training Loss: 0.07834511250257492\n",
      "Epoch 2682/30000 Training Loss: 0.086966373026371\n",
      "Epoch 2683/30000 Training Loss: 0.08345606923103333\n",
      "Epoch 2684/30000 Training Loss: 0.08466444164514542\n",
      "Epoch 2685/30000 Training Loss: 0.09550172090530396\n",
      "Epoch 2686/30000 Training Loss: 0.09632674604654312\n",
      "Epoch 2687/30000 Training Loss: 0.08679205924272537\n",
      "Epoch 2688/30000 Training Loss: 0.07796133309602737\n",
      "Epoch 2689/30000 Training Loss: 0.08389228582382202\n",
      "Epoch 2690/30000 Training Loss: 0.07463550567626953\n",
      "Epoch 2690/30000 Validation Loss: 0.0848425105214119\n",
      "Epoch 2691/30000 Training Loss: 0.08008285611867905\n",
      "Epoch 2692/30000 Training Loss: 0.0912911519408226\n",
      "Epoch 2693/30000 Training Loss: 0.058456916362047195\n",
      "Epoch 2694/30000 Training Loss: 0.08613651990890503\n",
      "Epoch 2695/30000 Training Loss: 0.08821900933980942\n",
      "Epoch 2696/30000 Training Loss: 0.09530233591794968\n",
      "Epoch 2697/30000 Training Loss: 0.09416533261537552\n",
      "Epoch 2698/30000 Training Loss: 0.07740738242864609\n",
      "Epoch 2699/30000 Training Loss: 0.07257235795259476\n",
      "Epoch 2700/30000 Training Loss: 0.07633081078529358\n",
      "Epoch 2700/30000 Validation Loss: 0.10059847682714462\n",
      "Epoch 2701/30000 Training Loss: 0.09200336784124374\n",
      "Epoch 2702/30000 Training Loss: 0.09205031394958496\n",
      "Epoch 2703/30000 Training Loss: 0.07138732820749283\n",
      "Epoch 2704/30000 Training Loss: 0.08166400343179703\n",
      "Epoch 2705/30000 Training Loss: 0.09178084135055542\n",
      "Epoch 2706/30000 Training Loss: 0.07351488620042801\n",
      "Epoch 2707/30000 Training Loss: 0.08986648172140121\n",
      "Epoch 2708/30000 Training Loss: 0.09305103868246078\n",
      "Epoch 2709/30000 Training Loss: 0.0708565041422844\n",
      "Epoch 2710/30000 Training Loss: 0.07385566830635071\n",
      "Epoch 2710/30000 Validation Loss: 0.08931103348731995\n",
      "Epoch 2711/30000 Training Loss: 0.07992009073495865\n",
      "Epoch 2712/30000 Training Loss: 0.07371365278959274\n",
      "Epoch 2713/30000 Training Loss: 0.08068590611219406\n",
      "Epoch 2714/30000 Training Loss: 0.07329010963439941\n",
      "Epoch 2715/30000 Training Loss: 0.0858248695731163\n",
      "Epoch 2716/30000 Training Loss: 0.09217024594545364\n",
      "Epoch 2717/30000 Training Loss: 0.07408282905817032\n",
      "Epoch 2718/30000 Training Loss: 0.06556036323308945\n",
      "Epoch 2719/30000 Training Loss: 0.06781125068664551\n",
      "Epoch 2720/30000 Training Loss: 0.07758589833974838\n",
      "Epoch 2720/30000 Validation Loss: 0.09063425660133362\n",
      "Epoch 2721/30000 Training Loss: 0.10078132152557373\n",
      "Epoch 2722/30000 Training Loss: 0.11330137401819229\n",
      "Epoch 2723/30000 Training Loss: 0.08996085077524185\n",
      "Epoch 2724/30000 Training Loss: 0.08473196625709534\n",
      "Epoch 2725/30000 Training Loss: 0.06791925430297852\n",
      "Epoch 2726/30000 Training Loss: 0.0868501365184784\n",
      "Epoch 2727/30000 Training Loss: 0.11933296918869019\n",
      "Epoch 2728/30000 Training Loss: 0.09716638177633286\n",
      "Epoch 2729/30000 Training Loss: 0.10526173561811447\n",
      "Epoch 2730/30000 Training Loss: 0.08294404298067093\n",
      "Epoch 2730/30000 Validation Loss: 0.08522839099168777\n",
      "Epoch 2731/30000 Training Loss: 0.084089495241642\n",
      "Epoch 2732/30000 Training Loss: 0.08318120241165161\n",
      "Epoch 2733/30000 Training Loss: 0.07301673293113708\n",
      "Epoch 2734/30000 Training Loss: 0.11524160951375961\n",
      "Epoch 2735/30000 Training Loss: 0.0825493335723877\n",
      "Epoch 2736/30000 Training Loss: 0.08808115869760513\n",
      "Epoch 2737/30000 Training Loss: 0.08388593047857285\n",
      "Epoch 2738/30000 Training Loss: 0.1071246862411499\n",
      "Epoch 2739/30000 Training Loss: 0.06742750853300095\n",
      "Epoch 2740/30000 Training Loss: 0.0731528177857399\n",
      "Epoch 2740/30000 Validation Loss: 0.06648331880569458\n",
      "Epoch 2741/30000 Training Loss: 0.08358568698167801\n",
      "Epoch 2742/30000 Training Loss: 0.07583320140838623\n",
      "Epoch 2743/30000 Training Loss: 0.08989451080560684\n",
      "Epoch 2744/30000 Training Loss: 0.0942455530166626\n",
      "Epoch 2745/30000 Training Loss: 0.08982320874929428\n",
      "Epoch 2746/30000 Training Loss: 0.07711667567491531\n",
      "Epoch 2747/30000 Training Loss: 0.07625744491815567\n",
      "Epoch 2748/30000 Training Loss: 0.08829855918884277\n",
      "Epoch 2749/30000 Training Loss: 0.10191327333450317\n",
      "Epoch 2750/30000 Training Loss: 0.07513146847486496\n",
      "Epoch 2750/30000 Validation Loss: 0.09429524093866348\n",
      "Epoch 2751/30000 Training Loss: 0.09693142026662827\n",
      "Epoch 2752/30000 Training Loss: 0.07646400481462479\n",
      "Epoch 2753/30000 Training Loss: 0.08050570636987686\n",
      "Epoch 2754/30000 Training Loss: 0.08490125089883804\n",
      "Epoch 2755/30000 Training Loss: 0.08048520982265472\n",
      "Epoch 2756/30000 Training Loss: 0.09103333204984665\n",
      "Epoch 2757/30000 Training Loss: 0.10183925181627274\n",
      "Epoch 2758/30000 Training Loss: 0.05949283763766289\n",
      "Epoch 2759/30000 Training Loss: 0.06484384089708328\n",
      "Epoch 2760/30000 Training Loss: 0.08032511919736862\n",
      "Epoch 2760/30000 Validation Loss: 0.06627897173166275\n",
      "Epoch 2761/30000 Training Loss: 0.07057065516710281\n",
      "Epoch 2762/30000 Training Loss: 0.08283340930938721\n",
      "Epoch 2763/30000 Training Loss: 0.0900215208530426\n",
      "Epoch 2764/30000 Training Loss: 0.07992824167013168\n",
      "Epoch 2765/30000 Training Loss: 0.08630182594060898\n",
      "Epoch 2766/30000 Training Loss: 0.07167980819940567\n",
      "Epoch 2767/30000 Training Loss: 0.08229541033506393\n",
      "Epoch 2768/30000 Training Loss: 0.07814710587263107\n",
      "Epoch 2769/30000 Training Loss: 0.0819297656416893\n",
      "Epoch 2770/30000 Training Loss: 0.10406719893217087\n",
      "Epoch 2770/30000 Validation Loss: 0.09920670837163925\n",
      "Epoch 2771/30000 Training Loss: 0.08310963213443756\n",
      "Epoch 2772/30000 Training Loss: 0.10651310533285141\n",
      "Epoch 2773/30000 Training Loss: 0.07964880019426346\n",
      "Epoch 2774/30000 Training Loss: 0.07516493648290634\n",
      "Epoch 2775/30000 Training Loss: 0.08939570933580399\n",
      "Epoch 2776/30000 Training Loss: 0.09099508076906204\n",
      "Epoch 2777/30000 Training Loss: 0.06913524866104126\n",
      "Epoch 2778/30000 Training Loss: 0.083929143846035\n",
      "Epoch 2779/30000 Training Loss: 0.10811034590005875\n",
      "Epoch 2780/30000 Training Loss: 0.08434396237134933\n",
      "Epoch 2780/30000 Validation Loss: 0.08529311418533325\n",
      "Epoch 2781/30000 Training Loss: 0.07876858860254288\n",
      "Epoch 2782/30000 Training Loss: 0.0864008292555809\n",
      "Epoch 2783/30000 Training Loss: 0.07142717391252518\n",
      "Epoch 2784/30000 Training Loss: 0.08339235931634903\n",
      "Epoch 2785/30000 Training Loss: 0.1033751368522644\n",
      "Epoch 2786/30000 Training Loss: 0.10134933143854141\n",
      "Epoch 2787/30000 Training Loss: 0.09397509694099426\n",
      "Epoch 2788/30000 Training Loss: 0.08153224736452103\n",
      "Epoch 2789/30000 Training Loss: 0.07039793580770493\n",
      "Epoch 2790/30000 Training Loss: 0.09973756223917007\n",
      "Epoch 2790/30000 Validation Loss: 0.07004918158054352\n",
      "Epoch 2791/30000 Training Loss: 0.09534410387277603\n",
      "Epoch 2792/30000 Training Loss: 0.07221280783414841\n",
      "Epoch 2793/30000 Training Loss: 0.0817563608288765\n",
      "Epoch 2794/30000 Training Loss: 0.09571778774261475\n",
      "Epoch 2795/30000 Training Loss: 0.07466758042573929\n",
      "Epoch 2796/30000 Training Loss: 0.07019778341054916\n",
      "Epoch 2797/30000 Training Loss: 0.10480905324220657\n",
      "Epoch 2798/30000 Training Loss: 0.11904382705688477\n",
      "Epoch 2799/30000 Training Loss: 0.09193658828735352\n",
      "Epoch 2800/30000 Training Loss: 0.0958658829331398\n",
      "Epoch 2800/30000 Validation Loss: 0.0985814705491066\n",
      "Epoch 2801/30000 Training Loss: 0.059469107538461685\n",
      "Epoch 2802/30000 Training Loss: 0.09466451406478882\n",
      "Epoch 2803/30000 Training Loss: 0.060008201748132706\n",
      "Epoch 2804/30000 Training Loss: 0.0938873291015625\n",
      "Epoch 2805/30000 Training Loss: 0.07720677554607391\n",
      "Epoch 2806/30000 Training Loss: 0.07832261919975281\n",
      "Epoch 2807/30000 Training Loss: 0.08176884800195694\n",
      "Epoch 2808/30000 Training Loss: 0.06872093677520752\n",
      "Epoch 2809/30000 Training Loss: 0.09842628240585327\n",
      "Epoch 2810/30000 Training Loss: 0.06063101813197136\n",
      "Epoch 2810/30000 Validation Loss: 0.07796313613653183\n",
      "Epoch 2811/30000 Training Loss: 0.06973814964294434\n",
      "Epoch 2812/30000 Training Loss: 0.08162710815668106\n",
      "Epoch 2813/30000 Training Loss: 0.08137210458517075\n",
      "Epoch 2814/30000 Training Loss: 0.06369484215974808\n",
      "Epoch 2815/30000 Training Loss: 0.11527635902166367\n",
      "Epoch 2816/30000 Training Loss: 0.08600752800703049\n",
      "Epoch 2817/30000 Training Loss: 0.07058745622634888\n",
      "Epoch 2818/30000 Training Loss: 0.0624251514673233\n",
      "Epoch 2819/30000 Training Loss: 0.09105197340250015\n",
      "Epoch 2820/30000 Training Loss: 0.09644765406847\n",
      "Epoch 2820/30000 Validation Loss: 0.08123695105314255\n",
      "Epoch 2821/30000 Training Loss: 0.09557905048131943\n",
      "Epoch 2822/30000 Training Loss: 0.08813517540693283\n",
      "Epoch 2823/30000 Training Loss: 0.09605472534894943\n",
      "Epoch 2824/30000 Training Loss: 0.09067117422819138\n",
      "Epoch 2825/30000 Training Loss: 0.09144612401723862\n",
      "Epoch 2826/30000 Training Loss: 0.08549768477678299\n",
      "Epoch 2827/30000 Training Loss: 0.1190628707408905\n",
      "Epoch 2828/30000 Training Loss: 0.08864191919565201\n",
      "Epoch 2829/30000 Training Loss: 0.0695825070142746\n",
      "Epoch 2830/30000 Training Loss: 0.0863528847694397\n",
      "Epoch 2830/30000 Validation Loss: 0.08747408539056778\n",
      "Epoch 2831/30000 Training Loss: 0.10148528963327408\n",
      "Epoch 2832/30000 Training Loss: 0.0815335065126419\n",
      "Epoch 2833/30000 Training Loss: 0.0844455286860466\n",
      "Epoch 2834/30000 Training Loss: 0.0809388980269432\n",
      "Epoch 2835/30000 Training Loss: 0.09912734478712082\n",
      "Epoch 2836/30000 Training Loss: 0.09138832241296768\n",
      "Epoch 2837/30000 Training Loss: 0.09388937801122665\n",
      "Epoch 2838/30000 Training Loss: 0.08097944408655167\n",
      "Epoch 2839/30000 Training Loss: 0.0739627480506897\n",
      "Epoch 2840/30000 Training Loss: 0.10406133532524109\n",
      "Epoch 2840/30000 Validation Loss: 0.09220985323190689\n",
      "Epoch 2841/30000 Training Loss: 0.08389896899461746\n",
      "Epoch 2842/30000 Training Loss: 0.06805487722158432\n",
      "Epoch 2843/30000 Training Loss: 0.07722549885511398\n",
      "Epoch 2844/30000 Training Loss: 0.08112456649541855\n",
      "Epoch 2845/30000 Training Loss: 0.09435973316431046\n",
      "Epoch 2846/30000 Training Loss: 0.08364829421043396\n",
      "Epoch 2847/30000 Training Loss: 0.07410158962011337\n",
      "Epoch 2848/30000 Training Loss: 0.09193376451730728\n",
      "Epoch 2849/30000 Training Loss: 0.08771374076604843\n",
      "Epoch 2850/30000 Training Loss: 0.097109355032444\n",
      "Epoch 2850/30000 Validation Loss: 0.07336347550153732\n",
      "Epoch 2851/30000 Training Loss: 0.08497198671102524\n",
      "Epoch 2852/30000 Training Loss: 0.08439762145280838\n",
      "Epoch 2853/30000 Training Loss: 0.10185534507036209\n",
      "Epoch 2854/30000 Training Loss: 0.1137462630867958\n",
      "Epoch 2855/30000 Training Loss: 0.09635666757822037\n",
      "Epoch 2856/30000 Training Loss: 0.08696288615465164\n",
      "Epoch 2857/30000 Training Loss: 0.06716277450323105\n",
      "Epoch 2858/30000 Training Loss: 0.11121752858161926\n",
      "Epoch 2859/30000 Training Loss: 0.0885089859366417\n",
      "Epoch 2860/30000 Training Loss: 0.07577697932720184\n",
      "Epoch 2860/30000 Validation Loss: 0.09783893078565598\n",
      "Epoch 2861/30000 Training Loss: 0.09502355009317398\n",
      "Epoch 2862/30000 Training Loss: 0.11762174963951111\n",
      "Epoch 2863/30000 Training Loss: 0.11553836613893509\n",
      "Epoch 2864/30000 Training Loss: 0.09393062442541122\n",
      "Epoch 2865/30000 Training Loss: 0.1052808165550232\n",
      "Epoch 2866/30000 Training Loss: 0.0667671486735344\n",
      "Epoch 2867/30000 Training Loss: 0.10663308948278427\n",
      "Epoch 2868/30000 Training Loss: 0.08686461299657822\n",
      "Epoch 2869/30000 Training Loss: 0.05867091938853264\n",
      "Epoch 2870/30000 Training Loss: 0.09436267614364624\n",
      "Epoch 2870/30000 Validation Loss: 0.09470149129629135\n",
      "Epoch 2871/30000 Training Loss: 0.07396632432937622\n",
      "Epoch 2872/30000 Training Loss: 0.0705450028181076\n",
      "Epoch 2873/30000 Training Loss: 0.07639705389738083\n",
      "Epoch 2874/30000 Training Loss: 0.08583929389715195\n",
      "Epoch 2875/30000 Training Loss: 0.10367941856384277\n",
      "Epoch 2876/30000 Training Loss: 0.06532353907823563\n",
      "Epoch 2877/30000 Training Loss: 0.08372244983911514\n",
      "Epoch 2878/30000 Training Loss: 0.0680735781788826\n",
      "Epoch 2879/30000 Training Loss: 0.08261556923389435\n",
      "Epoch 2880/30000 Training Loss: 0.1067710742354393\n",
      "Epoch 2880/30000 Validation Loss: 0.08461859077215195\n",
      "Epoch 2881/30000 Training Loss: 0.07052957266569138\n",
      "Epoch 2882/30000 Training Loss: 0.07613999396562576\n",
      "Epoch 2883/30000 Training Loss: 0.08665410429239273\n",
      "Epoch 2884/30000 Training Loss: 0.0893595814704895\n",
      "Epoch 2885/30000 Training Loss: 0.0834532380104065\n",
      "Epoch 2886/30000 Training Loss: 0.11105325818061829\n",
      "Epoch 2887/30000 Training Loss: 0.07741758227348328\n",
      "Epoch 2888/30000 Training Loss: 0.09327790141105652\n",
      "Epoch 2889/30000 Training Loss: 0.09311874955892563\n",
      "Epoch 2890/30000 Training Loss: 0.07649993151426315\n",
      "Epoch 2890/30000 Validation Loss: 0.07711314409971237\n",
      "Epoch 2891/30000 Training Loss: 0.08631619811058044\n",
      "Epoch 2892/30000 Training Loss: 0.07862541079521179\n",
      "Epoch 2893/30000 Training Loss: 0.07543712109327316\n",
      "Epoch 2894/30000 Training Loss: 0.07900306582450867\n",
      "Epoch 2895/30000 Training Loss: 0.0701543316245079\n",
      "Epoch 2896/30000 Training Loss: 0.08741423487663269\n",
      "Epoch 2897/30000 Training Loss: 0.09931051731109619\n",
      "Epoch 2898/30000 Training Loss: 0.09382831305265427\n",
      "Epoch 2899/30000 Training Loss: 0.10868868976831436\n",
      "Epoch 2900/30000 Training Loss: 0.07291626930236816\n",
      "Epoch 2900/30000 Validation Loss: 0.07345452904701233\n",
      "Epoch 2901/30000 Training Loss: 0.09481488913297653\n",
      "Epoch 2902/30000 Training Loss: 0.09271939843893051\n",
      "Epoch 2903/30000 Training Loss: 0.08402799814939499\n",
      "Epoch 2904/30000 Training Loss: 0.09973610192537308\n",
      "Epoch 2905/30000 Training Loss: 0.09001663327217102\n",
      "Epoch 2906/30000 Training Loss: 0.0818370133638382\n",
      "Epoch 2907/30000 Training Loss: 0.07708819210529327\n",
      "Epoch 2908/30000 Training Loss: 0.08337771147489548\n",
      "Epoch 2909/30000 Training Loss: 0.09380784630775452\n",
      "Epoch 2910/30000 Training Loss: 0.08923707157373428\n",
      "Epoch 2910/30000 Validation Loss: 0.07322108000516891\n",
      "Epoch 2911/30000 Training Loss: 0.0843612477183342\n",
      "Epoch 2912/30000 Training Loss: 0.0715138167142868\n",
      "Epoch 2913/30000 Training Loss: 0.07041565328836441\n",
      "Epoch 2914/30000 Training Loss: 0.08591442555189133\n",
      "Epoch 2915/30000 Training Loss: 0.07447709888219833\n",
      "Epoch 2916/30000 Training Loss: 0.08030814677476883\n",
      "Epoch 2917/30000 Training Loss: 0.09918683022260666\n",
      "Epoch 2918/30000 Training Loss: 0.10121417045593262\n",
      "Epoch 2919/30000 Training Loss: 0.08204071968793869\n",
      "Epoch 2920/30000 Training Loss: 0.08445024490356445\n",
      "Epoch 2920/30000 Validation Loss: 0.08459635823965073\n",
      "Epoch 2921/30000 Training Loss: 0.09851095825433731\n",
      "Epoch 2922/30000 Training Loss: 0.07802584022283554\n",
      "Epoch 2923/30000 Training Loss: 0.08864378184080124\n",
      "Epoch 2924/30000 Training Loss: 0.07631576806306839\n",
      "Epoch 2925/30000 Training Loss: 0.1034555733203888\n",
      "Epoch 2926/30000 Training Loss: 0.07432340830564499\n",
      "Epoch 2927/30000 Training Loss: 0.08306852728128433\n",
      "Epoch 2928/30000 Training Loss: 0.07625055313110352\n",
      "Epoch 2929/30000 Training Loss: 0.12540322542190552\n",
      "Epoch 2930/30000 Training Loss: 0.07564987242221832\n",
      "Epoch 2930/30000 Validation Loss: 0.09945980459451675\n",
      "Epoch 2931/30000 Training Loss: 0.07528197020292282\n",
      "Epoch 2932/30000 Training Loss: 0.08131826668977737\n",
      "Epoch 2933/30000 Training Loss: 0.08604323863983154\n",
      "Epoch 2934/30000 Training Loss: 0.09775619953870773\n",
      "Epoch 2935/30000 Training Loss: 0.07679165154695511\n",
      "Epoch 2936/30000 Training Loss: 0.07204558700323105\n",
      "Epoch 2937/30000 Training Loss: 0.08079005032777786\n",
      "Epoch 2938/30000 Training Loss: 0.11519304662942886\n",
      "Epoch 2939/30000 Training Loss: 0.10458364337682724\n",
      "Epoch 2940/30000 Training Loss: 0.10035989433526993\n",
      "Epoch 2940/30000 Validation Loss: 0.0800059512257576\n",
      "Epoch 2941/30000 Training Loss: 0.08299063891172409\n",
      "Epoch 2942/30000 Training Loss: 0.06963791698217392\n",
      "Epoch 2943/30000 Training Loss: 0.07394993305206299\n",
      "Epoch 2944/30000 Training Loss: 0.09320753812789917\n",
      "Epoch 2945/30000 Training Loss: 0.08641789108514786\n",
      "Epoch 2946/30000 Training Loss: 0.08320953696966171\n",
      "Epoch 2947/30000 Training Loss: 0.06529071927070618\n",
      "Epoch 2948/30000 Training Loss: 0.08296837657690048\n",
      "Epoch 2949/30000 Training Loss: 0.08133330196142197\n",
      "Epoch 2950/30000 Training Loss: 0.07990727573633194\n",
      "Epoch 2950/30000 Validation Loss: 0.08659031242132187\n",
      "Epoch 2951/30000 Training Loss: 0.08947250247001648\n",
      "Epoch 2952/30000 Training Loss: 0.07434815913438797\n",
      "Epoch 2953/30000 Training Loss: 0.0671083927154541\n",
      "Epoch 2954/30000 Training Loss: 0.0683063268661499\n",
      "Epoch 2955/30000 Training Loss: 0.07606460899114609\n",
      "Epoch 2956/30000 Training Loss: 0.07830379158258438\n",
      "Epoch 2957/30000 Training Loss: 0.09316436201334\n",
      "Epoch 2958/30000 Training Loss: 0.07257617264986038\n",
      "Epoch 2959/30000 Training Loss: 0.10555273294448853\n",
      "Epoch 2960/30000 Training Loss: 0.08401782065629959\n",
      "Epoch 2960/30000 Validation Loss: 0.0990828201174736\n",
      "Epoch 2961/30000 Training Loss: 0.07457927614450455\n",
      "Epoch 2962/30000 Training Loss: 0.07158298045396805\n",
      "Epoch 2963/30000 Training Loss: 0.08808218687772751\n",
      "Epoch 2964/30000 Training Loss: 0.08747593313455582\n",
      "Epoch 2965/30000 Training Loss: 0.07862987369298935\n",
      "Epoch 2966/30000 Training Loss: 0.07955221086740494\n",
      "Epoch 2967/30000 Training Loss: 0.0789237692952156\n",
      "Epoch 2968/30000 Training Loss: 0.10155981779098511\n",
      "Epoch 2969/30000 Training Loss: 0.06054562330245972\n",
      "Epoch 2970/30000 Training Loss: 0.07116255909204483\n",
      "Epoch 2970/30000 Validation Loss: 0.07937979698181152\n",
      "Epoch 2971/30000 Training Loss: 0.09366622567176819\n",
      "Epoch 2972/30000 Training Loss: 0.077120840549469\n",
      "Epoch 2973/30000 Training Loss: 0.0641283318400383\n",
      "Epoch 2974/30000 Training Loss: 0.07558020204305649\n",
      "Epoch 2975/30000 Training Loss: 0.08504718542098999\n",
      "Epoch 2976/30000 Training Loss: 0.07785394787788391\n",
      "Epoch 2977/30000 Training Loss: 0.08263042569160461\n",
      "Epoch 2978/30000 Training Loss: 0.06815791130065918\n",
      "Epoch 2979/30000 Training Loss: 0.07447846978902817\n",
      "Epoch 2980/30000 Training Loss: 0.10682013630867004\n",
      "Epoch 2980/30000 Validation Loss: 0.06831043213605881\n",
      "Epoch 2981/30000 Training Loss: 0.08520521968603134\n",
      "Epoch 2982/30000 Training Loss: 0.08741522580385208\n",
      "Epoch 2983/30000 Training Loss: 0.06739496439695358\n",
      "Epoch 2984/30000 Training Loss: 0.054959069937467575\n",
      "Epoch 2985/30000 Training Loss: 0.11047843843698502\n",
      "Epoch 2986/30000 Training Loss: 0.06990151852369308\n",
      "Epoch 2987/30000 Training Loss: 0.1140122041106224\n",
      "Epoch 2988/30000 Training Loss: 0.060003846883773804\n",
      "Epoch 2989/30000 Training Loss: 0.08437389135360718\n",
      "Epoch 2990/30000 Training Loss: 0.06796015799045563\n",
      "Epoch 2990/30000 Validation Loss: 0.06008114293217659\n",
      "Epoch 2991/30000 Training Loss: 0.1108492985367775\n",
      "Epoch 2992/30000 Training Loss: 0.07146964222192764\n",
      "Epoch 2993/30000 Training Loss: 0.07037021964788437\n",
      "Epoch 2994/30000 Training Loss: 0.0710664615035057\n",
      "Epoch 2995/30000 Training Loss: 0.09049540758132935\n",
      "Epoch 2996/30000 Training Loss: 0.08737888187170029\n",
      "Epoch 2997/30000 Training Loss: 0.08974379301071167\n",
      "Epoch 2998/30000 Training Loss: 0.07934869825839996\n",
      "Epoch 2999/30000 Training Loss: 0.08296423405408859\n",
      "Epoch 3000/30000 Training Loss: 0.07133394479751587\n",
      "Epoch 3000/30000 Validation Loss: 0.092127226293087\n",
      "Epoch 3001/30000 Training Loss: 0.07604938000440598\n",
      "Epoch 3002/30000 Training Loss: 0.08750921487808228\n",
      "Epoch 3003/30000 Training Loss: 0.07091110199689865\n",
      "Epoch 3004/30000 Training Loss: 0.07457516342401505\n",
      "Epoch 3005/30000 Training Loss: 0.09882166981697083\n",
      "Epoch 3006/30000 Training Loss: 0.10585849732160568\n",
      "Epoch 3007/30000 Training Loss: 0.09360892325639725\n",
      "Epoch 3008/30000 Training Loss: 0.06906630843877792\n",
      "Epoch 3009/30000 Training Loss: 0.0755336806178093\n",
      "Epoch 3010/30000 Training Loss: 0.09389068931341171\n",
      "Epoch 3010/30000 Validation Loss: 0.10555905103683472\n",
      "Epoch 3011/30000 Training Loss: 0.08832147717475891\n",
      "Epoch 3012/30000 Training Loss: 0.10615301132202148\n",
      "Epoch 3013/30000 Training Loss: 0.07693427056074142\n",
      "Epoch 3014/30000 Training Loss: 0.07959838956594467\n",
      "Epoch 3015/30000 Training Loss: 0.0884389579296112\n",
      "Epoch 3016/30000 Training Loss: 0.10858210176229477\n",
      "Epoch 3017/30000 Training Loss: 0.08656781911849976\n",
      "Epoch 3018/30000 Training Loss: 0.07917555421590805\n",
      "Epoch 3019/30000 Training Loss: 0.07413392513990402\n",
      "Epoch 3020/30000 Training Loss: 0.11290958523750305\n",
      "Epoch 3020/30000 Validation Loss: 0.09228134155273438\n",
      "Epoch 3021/30000 Training Loss: 0.08715309947729111\n",
      "Epoch 3022/30000 Training Loss: 0.08917390555143356\n",
      "Epoch 3023/30000 Training Loss: 0.08063913136720657\n",
      "Epoch 3024/30000 Training Loss: 0.07472848147153854\n",
      "Epoch 3025/30000 Training Loss: 0.08437352627515793\n",
      "Epoch 3026/30000 Training Loss: 0.09188675135374069\n",
      "Epoch 3027/30000 Training Loss: 0.08557624369859695\n",
      "Epoch 3028/30000 Training Loss: 0.06721518188714981\n",
      "Epoch 3029/30000 Training Loss: 0.09175568073987961\n",
      "Epoch 3030/30000 Training Loss: 0.07354827970266342\n",
      "Epoch 3030/30000 Validation Loss: 0.07258119434118271\n",
      "Epoch 3031/30000 Training Loss: 0.06605514138936996\n",
      "Epoch 3032/30000 Training Loss: 0.09365721791982651\n",
      "Epoch 3033/30000 Training Loss: 0.08486238867044449\n",
      "Epoch 3034/30000 Training Loss: 0.10182548314332962\n",
      "Epoch 3035/30000 Training Loss: 0.09318574517965317\n",
      "Epoch 3036/30000 Training Loss: 0.09856724739074707\n",
      "Epoch 3037/30000 Training Loss: 0.09291688352823257\n",
      "Epoch 3038/30000 Training Loss: 0.07800743728876114\n",
      "Epoch 3039/30000 Training Loss: 0.08260830491781235\n",
      "Epoch 3040/30000 Training Loss: 0.0953376516699791\n",
      "Epoch 3040/30000 Validation Loss: 0.08157840371131897\n",
      "Epoch 3041/30000 Training Loss: 0.05431210622191429\n",
      "Epoch 3042/30000 Training Loss: 0.07578284293413162\n",
      "Epoch 3043/30000 Training Loss: 0.08680462837219238\n",
      "Epoch 3044/30000 Training Loss: 0.10785045474767685\n",
      "Epoch 3045/30000 Training Loss: 0.09435942023992538\n",
      "Epoch 3046/30000 Training Loss: 0.08328597992658615\n",
      "Epoch 3047/30000 Training Loss: 0.08458107709884644\n",
      "Epoch 3048/30000 Training Loss: 0.10024800151586533\n",
      "Epoch 3049/30000 Training Loss: 0.08051607757806778\n",
      "Epoch 3050/30000 Training Loss: 0.07476985454559326\n",
      "Epoch 3050/30000 Validation Loss: 0.09536910057067871\n",
      "Epoch 3051/30000 Training Loss: 0.0747990533709526\n",
      "Epoch 3052/30000 Training Loss: 0.07993921637535095\n",
      "Epoch 3053/30000 Training Loss: 0.07955881953239441\n",
      "Epoch 3054/30000 Training Loss: 0.07399310916662216\n",
      "Epoch 3055/30000 Training Loss: 0.09587031602859497\n",
      "Epoch 3056/30000 Training Loss: 0.06606603413820267\n",
      "Epoch 3057/30000 Training Loss: 0.08641266077756882\n",
      "Epoch 3058/30000 Training Loss: 0.06524660438299179\n",
      "Epoch 3059/30000 Training Loss: 0.06850408762693405\n",
      "Epoch 3060/30000 Training Loss: 0.09485574811697006\n",
      "Epoch 3060/30000 Validation Loss: 0.10628799349069595\n",
      "Epoch 3061/30000 Training Loss: 0.08313406258821487\n",
      "Epoch 3062/30000 Training Loss: 0.08610057830810547\n",
      "Epoch 3063/30000 Training Loss: 0.053755637258291245\n",
      "Epoch 3064/30000 Training Loss: 0.06718821078538895\n",
      "Epoch 3065/30000 Training Loss: 0.09129089117050171\n",
      "Epoch 3066/30000 Training Loss: 0.08774291723966599\n",
      "Epoch 3067/30000 Training Loss: 0.0646006166934967\n",
      "Epoch 3068/30000 Training Loss: 0.10091590881347656\n",
      "Epoch 3069/30000 Training Loss: 0.06969199329614639\n",
      "Epoch 3070/30000 Training Loss: 0.08542406558990479\n",
      "Epoch 3070/30000 Validation Loss: 0.1215595230460167\n",
      "Epoch 3071/30000 Training Loss: 0.07491525262594223\n",
      "Epoch 3072/30000 Training Loss: 0.07280021905899048\n",
      "Epoch 3073/30000 Training Loss: 0.09158921241760254\n",
      "Epoch 3074/30000 Training Loss: 0.09202287346124649\n",
      "Epoch 3075/30000 Training Loss: 0.0889010801911354\n",
      "Epoch 3076/30000 Training Loss: 0.08257253468036652\n",
      "Epoch 3077/30000 Training Loss: 0.07269658893346786\n",
      "Epoch 3078/30000 Training Loss: 0.07025831937789917\n",
      "Epoch 3079/30000 Training Loss: 0.07441413402557373\n",
      "Epoch 3080/30000 Training Loss: 0.05981928110122681\n",
      "Epoch 3080/30000 Validation Loss: 0.10637298971414566\n",
      "Epoch 3081/30000 Training Loss: 0.09110981971025467\n",
      "Epoch 3082/30000 Training Loss: 0.07032433152198792\n",
      "Epoch 3083/30000 Training Loss: 0.10183116048574448\n",
      "Epoch 3084/30000 Training Loss: 0.07880525290966034\n",
      "Epoch 3085/30000 Training Loss: 0.0754704549908638\n",
      "Epoch 3086/30000 Training Loss: 0.09716381877660751\n",
      "Epoch 3087/30000 Training Loss: 0.07639855891466141\n",
      "Epoch 3088/30000 Training Loss: 0.08686994761228561\n",
      "Epoch 3089/30000 Training Loss: 0.07795211672782898\n",
      "Epoch 3090/30000 Training Loss: 0.08933857828378677\n",
      "Epoch 3090/30000 Validation Loss: 0.07896286249160767\n",
      "Epoch 3091/30000 Training Loss: 0.11697351932525635\n",
      "Epoch 3092/30000 Training Loss: 0.10148235410451889\n",
      "Epoch 3093/30000 Training Loss: 0.07483959197998047\n",
      "Epoch 3094/30000 Training Loss: 0.08748926967382431\n",
      "Epoch 3095/30000 Training Loss: 0.07182925939559937\n",
      "Epoch 3096/30000 Training Loss: 0.07907959073781967\n",
      "Epoch 3097/30000 Training Loss: 0.08401934057474136\n",
      "Epoch 3098/30000 Training Loss: 0.09088090062141418\n",
      "Epoch 3099/30000 Training Loss: 0.0698872059583664\n",
      "Epoch 3100/30000 Training Loss: 0.06628961116075516\n",
      "Epoch 3100/30000 Validation Loss: 0.08697241544723511\n",
      "Epoch 3101/30000 Training Loss: 0.07462168484926224\n",
      "Epoch 3102/30000 Training Loss: 0.0846845880150795\n",
      "Epoch 3103/30000 Training Loss: 0.07800717651844025\n",
      "Epoch 3104/30000 Training Loss: 0.09067555516958237\n",
      "Epoch 3105/30000 Training Loss: 0.10022029280662537\n",
      "Epoch 3106/30000 Training Loss: 0.07846751064062119\n",
      "Epoch 3107/30000 Training Loss: 0.0848015621304512\n",
      "Epoch 3108/30000 Training Loss: 0.07808514684438705\n",
      "Epoch 3109/30000 Training Loss: 0.08925965428352356\n",
      "Epoch 3110/30000 Training Loss: 0.07235724478960037\n",
      "Epoch 3110/30000 Validation Loss: 0.08580908924341202\n",
      "Epoch 3111/30000 Training Loss: 0.06819825619459152\n",
      "Epoch 3112/30000 Training Loss: 0.08909603953361511\n",
      "Epoch 3113/30000 Training Loss: 0.07357507944107056\n",
      "Epoch 3114/30000 Training Loss: 0.08972802013158798\n",
      "Epoch 3115/30000 Training Loss: 0.08763935416936874\n",
      "Epoch 3116/30000 Training Loss: 0.0839661955833435\n",
      "Epoch 3117/30000 Training Loss: 0.08812499046325684\n",
      "Epoch 3118/30000 Training Loss: 0.08223879337310791\n",
      "Epoch 3119/30000 Training Loss: 0.11036831140518188\n",
      "Epoch 3120/30000 Training Loss: 0.08721200376749039\n",
      "Epoch 3120/30000 Validation Loss: 0.07941347360610962\n",
      "Epoch 3121/30000 Training Loss: 0.08108095824718475\n",
      "Epoch 3122/30000 Training Loss: 0.12139779329299927\n",
      "Epoch 3123/30000 Training Loss: 0.11545151472091675\n",
      "Epoch 3124/30000 Training Loss: 0.09851348400115967\n",
      "Epoch 3125/30000 Training Loss: 0.1009652391076088\n",
      "Epoch 3126/30000 Training Loss: 0.08467340469360352\n",
      "Epoch 3127/30000 Training Loss: 0.08024144172668457\n",
      "Epoch 3128/30000 Training Loss: 0.10502535104751587\n",
      "Epoch 3129/30000 Training Loss: 0.08173251897096634\n",
      "Epoch 3130/30000 Training Loss: 0.08425134420394897\n",
      "Epoch 3130/30000 Validation Loss: 0.10108134895563126\n",
      "Epoch 3131/30000 Training Loss: 0.09247791767120361\n",
      "Epoch 3132/30000 Training Loss: 0.07875881344079971\n",
      "Epoch 3133/30000 Training Loss: 0.08214019984006882\n",
      "Epoch 3134/30000 Training Loss: 0.06477474421262741\n",
      "Epoch 3135/30000 Training Loss: 0.082073874771595\n",
      "Epoch 3136/30000 Training Loss: 0.07800676673650742\n",
      "Epoch 3137/30000 Training Loss: 0.09046802669763565\n",
      "Epoch 3138/30000 Training Loss: 0.07507658749818802\n",
      "Epoch 3139/30000 Training Loss: 0.08306815475225449\n",
      "Epoch 3140/30000 Training Loss: 0.07580335438251495\n",
      "Epoch 3140/30000 Validation Loss: 0.09009983390569687\n",
      "Epoch 3141/30000 Training Loss: 0.07990754395723343\n",
      "Epoch 3142/30000 Training Loss: 0.08341612666845322\n",
      "Epoch 3143/30000 Training Loss: 0.082705557346344\n",
      "Epoch 3144/30000 Training Loss: 0.07357289642095566\n",
      "Epoch 3145/30000 Training Loss: 0.0963398814201355\n",
      "Epoch 3146/30000 Training Loss: 0.06812111288309097\n",
      "Epoch 3147/30000 Training Loss: 0.11238973587751389\n",
      "Epoch 3148/30000 Training Loss: 0.10069391131401062\n",
      "Epoch 3149/30000 Training Loss: 0.08064379543066025\n",
      "Epoch 3150/30000 Training Loss: 0.08531889319419861\n",
      "Epoch 3150/30000 Validation Loss: 0.07256213575601578\n",
      "Epoch 3151/30000 Training Loss: 0.10633599013090134\n",
      "Epoch 3152/30000 Training Loss: 0.09623315930366516\n",
      "Epoch 3153/30000 Training Loss: 0.07470253109931946\n",
      "Epoch 3154/30000 Training Loss: 0.07485560327768326\n",
      "Epoch 3155/30000 Training Loss: 0.06696144491434097\n",
      "Epoch 3156/30000 Training Loss: 0.07842792570590973\n",
      "Epoch 3157/30000 Training Loss: 0.08108266443014145\n",
      "Epoch 3158/30000 Training Loss: 0.10524296015501022\n",
      "Epoch 3159/30000 Training Loss: 0.07153042405843735\n",
      "Epoch 3160/30000 Training Loss: 0.07583589106798172\n",
      "Epoch 3160/30000 Validation Loss: 0.0901079997420311\n",
      "Epoch 3161/30000 Training Loss: 0.08226015418767929\n",
      "Epoch 3162/30000 Training Loss: 0.06373197585344315\n",
      "Epoch 3163/30000 Training Loss: 0.08794531971216202\n",
      "Epoch 3164/30000 Training Loss: 0.08459589630365372\n",
      "Epoch 3165/30000 Training Loss: 0.08380090445280075\n",
      "Epoch 3166/30000 Training Loss: 0.08202711492776871\n",
      "Epoch 3167/30000 Training Loss: 0.08987605571746826\n",
      "Epoch 3168/30000 Training Loss: 0.07070430368185043\n",
      "Epoch 3169/30000 Training Loss: 0.08970114588737488\n",
      "Epoch 3170/30000 Training Loss: 0.09081098437309265\n",
      "Epoch 3170/30000 Validation Loss: 0.07530590146780014\n",
      "Epoch 3171/30000 Training Loss: 0.09333145618438721\n",
      "Epoch 3172/30000 Training Loss: 0.07840230315923691\n",
      "Epoch 3173/30000 Training Loss: 0.08199810236692429\n",
      "Epoch 3174/30000 Training Loss: 0.07336927205324173\n",
      "Epoch 3175/30000 Training Loss: 0.0882720872759819\n",
      "Epoch 3176/30000 Training Loss: 0.1157214418053627\n",
      "Epoch 3177/30000 Training Loss: 0.074805848300457\n",
      "Epoch 3178/30000 Training Loss: 0.08159010857343674\n",
      "Epoch 3179/30000 Training Loss: 0.07390262931585312\n",
      "Epoch 3180/30000 Training Loss: 0.0734022855758667\n",
      "Epoch 3180/30000 Validation Loss: 0.09137510508298874\n",
      "Epoch 3181/30000 Training Loss: 0.05657677352428436\n",
      "Epoch 3182/30000 Training Loss: 0.0878700390458107\n",
      "Epoch 3183/30000 Training Loss: 0.08361329883337021\n",
      "Epoch 3184/30000 Training Loss: 0.08701371401548386\n",
      "Epoch 3185/30000 Training Loss: 0.06468810886144638\n",
      "Epoch 3186/30000 Training Loss: 0.07430823892354965\n",
      "Epoch 3187/30000 Training Loss: 0.07018036395311356\n",
      "Epoch 3188/30000 Training Loss: 0.0798308476805687\n",
      "Epoch 3189/30000 Training Loss: 0.10142744332551956\n",
      "Epoch 3190/30000 Training Loss: 0.08429861068725586\n",
      "Epoch 3190/30000 Validation Loss: 0.07889872044324875\n",
      "Epoch 3191/30000 Training Loss: 0.07973741739988327\n",
      "Epoch 3192/30000 Training Loss: 0.0909968838095665\n",
      "Epoch 3193/30000 Training Loss: 0.09415402263402939\n",
      "Epoch 3194/30000 Training Loss: 0.09764454513788223\n",
      "Epoch 3195/30000 Training Loss: 0.08059391379356384\n",
      "Epoch 3196/30000 Training Loss: 0.06925151497125626\n",
      "Epoch 3197/30000 Training Loss: 0.08760989457368851\n",
      "Epoch 3198/30000 Training Loss: 0.08635548502206802\n",
      "Epoch 3199/30000 Training Loss: 0.06801390647888184\n",
      "Epoch 3200/30000 Training Loss: 0.08988039940595627\n",
      "Epoch 3200/30000 Validation Loss: 0.08516561985015869\n",
      "Epoch 3201/30000 Training Loss: 0.0853029415011406\n",
      "Epoch 3202/30000 Training Loss: 0.10375984758138657\n",
      "Epoch 3203/30000 Training Loss: 0.08132389187812805\n",
      "Epoch 3204/30000 Training Loss: 0.06374163180589676\n",
      "Epoch 3205/30000 Training Loss: 0.07343589514493942\n",
      "Epoch 3206/30000 Training Loss: 0.10224627703428268\n",
      "Epoch 3207/30000 Training Loss: 0.10273750871419907\n",
      "Epoch 3208/30000 Training Loss: 0.07136885821819305\n",
      "Epoch 3209/30000 Training Loss: 0.07719432562589645\n",
      "Epoch 3210/30000 Training Loss: 0.06535801291465759\n",
      "Epoch 3210/30000 Validation Loss: 0.11835896968841553\n",
      "Epoch 3211/30000 Training Loss: 0.0686618983745575\n",
      "Epoch 3212/30000 Training Loss: 0.10987062007188797\n",
      "Epoch 3213/30000 Training Loss: 0.0991469994187355\n",
      "Epoch 3214/30000 Training Loss: 0.0736614391207695\n",
      "Epoch 3215/30000 Training Loss: 0.07608944177627563\n",
      "Epoch 3216/30000 Training Loss: 0.09681699424982071\n",
      "Epoch 3217/30000 Training Loss: 0.0757659301161766\n",
      "Epoch 3218/30000 Training Loss: 0.06704865396022797\n",
      "Epoch 3219/30000 Training Loss: 0.07760581374168396\n",
      "Epoch 3220/30000 Training Loss: 0.07103065401315689\n",
      "Epoch 3220/30000 Validation Loss: 0.08659379929304123\n",
      "Epoch 3221/30000 Training Loss: 0.08563733845949173\n",
      "Epoch 3222/30000 Training Loss: 0.10673880577087402\n",
      "Epoch 3223/30000 Training Loss: 0.07991299033164978\n",
      "Epoch 3224/30000 Training Loss: 0.0661543682217598\n",
      "Epoch 3225/30000 Training Loss: 0.06529644131660461\n",
      "Epoch 3226/30000 Training Loss: 0.07883347570896149\n",
      "Epoch 3227/30000 Training Loss: 0.06610872596502304\n",
      "Epoch 3228/30000 Training Loss: 0.094155453145504\n",
      "Epoch 3229/30000 Training Loss: 0.08538036793470383\n",
      "Epoch 3230/30000 Training Loss: 0.09978270530700684\n",
      "Epoch 3230/30000 Validation Loss: 0.08406024426221848\n",
      "Epoch 3231/30000 Training Loss: 0.07562346756458282\n",
      "Epoch 3232/30000 Training Loss: 0.10704255849123001\n",
      "Epoch 3233/30000 Training Loss: 0.06277177482843399\n",
      "Epoch 3234/30000 Training Loss: 0.08937958627939224\n",
      "Epoch 3235/30000 Training Loss: 0.07649575173854828\n",
      "Epoch 3236/30000 Training Loss: 0.06414861232042313\n",
      "Epoch 3237/30000 Training Loss: 0.07827397435903549\n",
      "Epoch 3238/30000 Training Loss: 0.08573838323354721\n",
      "Epoch 3239/30000 Training Loss: 0.09784729033708572\n",
      "Epoch 3240/30000 Training Loss: 0.1130199208855629\n",
      "Epoch 3240/30000 Validation Loss: 0.07569930702447891\n",
      "Epoch 3241/30000 Training Loss: 0.08604126423597336\n",
      "Epoch 3242/30000 Training Loss: 0.08872977644205093\n",
      "Epoch 3243/30000 Training Loss: 0.07194829732179642\n",
      "Epoch 3244/30000 Training Loss: 0.08527780324220657\n",
      "Epoch 3245/30000 Training Loss: 0.09002711623907089\n",
      "Epoch 3246/30000 Training Loss: 0.10045231133699417\n",
      "Epoch 3247/30000 Training Loss: 0.08160995692014694\n",
      "Epoch 3248/30000 Training Loss: 0.07190703600645065\n",
      "Epoch 3249/30000 Training Loss: 0.06892773509025574\n",
      "Epoch 3250/30000 Training Loss: 0.07582706958055496\n",
      "Epoch 3250/30000 Validation Loss: 0.10024473816156387\n",
      "Epoch 3251/30000 Training Loss: 0.07706247270107269\n",
      "Epoch 3252/30000 Training Loss: 0.08596218377351761\n",
      "Epoch 3253/30000 Training Loss: 0.08404053002595901\n",
      "Epoch 3254/30000 Training Loss: 0.04393340274691582\n",
      "Epoch 3255/30000 Training Loss: 0.06809203326702118\n",
      "Epoch 3256/30000 Training Loss: 0.08694341778755188\n",
      "Epoch 3257/30000 Training Loss: 0.10372019559144974\n",
      "Epoch 3258/30000 Training Loss: 0.08180573582649231\n",
      "Epoch 3259/30000 Training Loss: 0.10274115949869156\n",
      "Epoch 3260/30000 Training Loss: 0.057648915797472\n",
      "Epoch 3260/30000 Validation Loss: 0.10386732220649719\n",
      "Epoch 3261/30000 Training Loss: 0.06661466509103775\n",
      "Epoch 3262/30000 Training Loss: 0.07046028226613998\n",
      "Epoch 3263/30000 Training Loss: 0.078873410820961\n",
      "Epoch 3264/30000 Training Loss: 0.09696274995803833\n",
      "Epoch 3265/30000 Training Loss: 0.08895518630743027\n",
      "Epoch 3266/30000 Training Loss: 0.06218741834163666\n",
      "Epoch 3267/30000 Training Loss: 0.09723753482103348\n",
      "Epoch 3268/30000 Training Loss: 0.07528870552778244\n",
      "Epoch 3269/30000 Training Loss: 0.08115141838788986\n",
      "Epoch 3270/30000 Training Loss: 0.07334541529417038\n",
      "Epoch 3270/30000 Validation Loss: 0.10251429677009583\n",
      "Epoch 3271/30000 Training Loss: 0.08539064973592758\n",
      "Epoch 3272/30000 Training Loss: 0.07854774594306946\n",
      "Epoch 3273/30000 Training Loss: 0.08507630974054337\n",
      "Epoch 3274/30000 Training Loss: 0.07710041850805283\n",
      "Epoch 3275/30000 Training Loss: 0.09013378620147705\n",
      "Epoch 3276/30000 Training Loss: 0.08434563130140305\n",
      "Epoch 3277/30000 Training Loss: 0.08440122753381729\n",
      "Epoch 3278/30000 Training Loss: 0.09469569474458694\n",
      "Epoch 3279/30000 Training Loss: 0.07733208686113358\n",
      "Epoch 3280/30000 Training Loss: 0.07928034663200378\n",
      "Epoch 3280/30000 Validation Loss: 0.05698945000767708\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05698945000767708<=============\n",
      "Epoch 3281/30000 Training Loss: 0.0883956030011177\n",
      "Epoch 3282/30000 Training Loss: 0.05916399881243706\n",
      "Epoch 3283/30000 Training Loss: 0.07167831063270569\n",
      "Epoch 3284/30000 Training Loss: 0.07741773873567581\n",
      "Epoch 3285/30000 Training Loss: 0.07271768897771835\n",
      "Epoch 3286/30000 Training Loss: 0.06842000037431717\n",
      "Epoch 3287/30000 Training Loss: 0.07168703526258469\n",
      "Epoch 3288/30000 Training Loss: 0.05213455483317375\n",
      "Epoch 3289/30000 Training Loss: 0.09634900838136673\n",
      "Epoch 3290/30000 Training Loss: 0.06453267484903336\n",
      "Epoch 3290/30000 Validation Loss: 0.0732099637389183\n",
      "Epoch 3291/30000 Training Loss: 0.06431878358125687\n",
      "Epoch 3292/30000 Training Loss: 0.06399205327033997\n",
      "Epoch 3293/30000 Training Loss: 0.07013943046331406\n",
      "Epoch 3294/30000 Training Loss: 0.09658879786729813\n",
      "Epoch 3295/30000 Training Loss: 0.10108540207147598\n",
      "Epoch 3296/30000 Training Loss: 0.06251591444015503\n",
      "Epoch 3297/30000 Training Loss: 0.07425990700721741\n",
      "Epoch 3298/30000 Training Loss: 0.07576595991849899\n",
      "Epoch 3299/30000 Training Loss: 0.09661271423101425\n",
      "Epoch 3300/30000 Training Loss: 0.09267451614141464\n",
      "Epoch 3300/30000 Validation Loss: 0.07418394088745117\n",
      "Epoch 3301/30000 Training Loss: 0.10642224550247192\n",
      "Epoch 3302/30000 Training Loss: 0.10377643257379532\n",
      "Epoch 3303/30000 Training Loss: 0.09825599193572998\n",
      "Epoch 3304/30000 Training Loss: 0.0864349827170372\n",
      "Epoch 3305/30000 Training Loss: 0.08608078956604004\n",
      "Epoch 3306/30000 Training Loss: 0.090018630027771\n",
      "Epoch 3307/30000 Training Loss: 0.09374719858169556\n",
      "Epoch 3308/30000 Training Loss: 0.06288814544677734\n",
      "Epoch 3309/30000 Training Loss: 0.08112990856170654\n",
      "Epoch 3310/30000 Training Loss: 0.07323556393384933\n",
      "Epoch 3310/30000 Validation Loss: 0.08398041874170303\n",
      "Epoch 3311/30000 Training Loss: 0.07602199912071228\n",
      "Epoch 3312/30000 Training Loss: 0.06646706908941269\n",
      "Epoch 3313/30000 Training Loss: 0.08835244923830032\n",
      "Epoch 3314/30000 Training Loss: 0.08264134079217911\n",
      "Epoch 3315/30000 Training Loss: 0.08252333849668503\n",
      "Epoch 3316/30000 Training Loss: 0.10193721204996109\n",
      "Epoch 3317/30000 Training Loss: 0.07767465710639954\n",
      "Epoch 3318/30000 Training Loss: 0.06665229052305222\n",
      "Epoch 3319/30000 Training Loss: 0.1054360643029213\n",
      "Epoch 3320/30000 Training Loss: 0.07369034737348557\n",
      "Epoch 3320/30000 Validation Loss: 0.08214861899614334\n",
      "Epoch 3321/30000 Training Loss: 0.07470521330833435\n",
      "Epoch 3322/30000 Training Loss: 0.09332102537155151\n",
      "Epoch 3323/30000 Training Loss: 0.08732780069112778\n",
      "Epoch 3324/30000 Training Loss: 0.08701970428228378\n",
      "Epoch 3325/30000 Training Loss: 0.07750073820352554\n",
      "Epoch 3326/30000 Training Loss: 0.11654043197631836\n",
      "Epoch 3327/30000 Training Loss: 0.07477149367332458\n",
      "Epoch 3328/30000 Training Loss: 0.08120466023683548\n",
      "Epoch 3329/30000 Training Loss: 0.10192934423685074\n",
      "Epoch 3330/30000 Training Loss: 0.07728972285985947\n",
      "Epoch 3330/30000 Validation Loss: 0.07619583606719971\n",
      "Epoch 3331/30000 Training Loss: 0.09661892801523209\n",
      "Epoch 3332/30000 Training Loss: 0.08260199427604675\n",
      "Epoch 3333/30000 Training Loss: 0.09213438630104065\n",
      "Epoch 3334/30000 Training Loss: 0.08431559056043625\n",
      "Epoch 3335/30000 Training Loss: 0.07075655460357666\n",
      "Epoch 3336/30000 Training Loss: 0.09723186492919922\n",
      "Epoch 3337/30000 Training Loss: 0.09034839272499084\n",
      "Epoch 3338/30000 Training Loss: 0.09727521985769272\n",
      "Epoch 3339/30000 Training Loss: 0.08224627375602722\n",
      "Epoch 3340/30000 Training Loss: 0.08450277894735336\n",
      "Epoch 3340/30000 Validation Loss: 0.09984010457992554\n",
      "Epoch 3341/30000 Training Loss: 0.07537863403558731\n",
      "Epoch 3342/30000 Training Loss: 0.06978878378868103\n",
      "Epoch 3343/30000 Training Loss: 0.07050986588001251\n",
      "Epoch 3344/30000 Training Loss: 0.08718999475240707\n",
      "Epoch 3345/30000 Training Loss: 0.0759790763258934\n",
      "Epoch 3346/30000 Training Loss: 0.07466619461774826\n",
      "Epoch 3347/30000 Training Loss: 0.08934544771909714\n",
      "Epoch 3348/30000 Training Loss: 0.07617215067148209\n",
      "Epoch 3349/30000 Training Loss: 0.08096577972173691\n",
      "Epoch 3350/30000 Training Loss: 0.09014914184808731\n",
      "Epoch 3350/30000 Validation Loss: 0.07763317227363586\n",
      "Epoch 3351/30000 Training Loss: 0.09393197298049927\n",
      "Epoch 3352/30000 Training Loss: 0.08878978341817856\n",
      "Epoch 3353/30000 Training Loss: 0.0810374915599823\n",
      "Epoch 3354/30000 Training Loss: 0.0811191201210022\n",
      "Epoch 3355/30000 Training Loss: 0.08659187704324722\n",
      "Epoch 3356/30000 Training Loss: 0.09400156885385513\n",
      "Epoch 3357/30000 Training Loss: 0.08216650038957596\n",
      "Epoch 3358/30000 Training Loss: 0.08149918168783188\n",
      "Epoch 3359/30000 Training Loss: 0.08101121336221695\n",
      "Epoch 3360/30000 Training Loss: 0.06555533409118652\n",
      "Epoch 3360/30000 Validation Loss: 0.06103217229247093\n",
      "Epoch 3361/30000 Training Loss: 0.09317169338464737\n",
      "Epoch 3362/30000 Training Loss: 0.06892824918031693\n",
      "Epoch 3363/30000 Training Loss: 0.10886004567146301\n",
      "Epoch 3364/30000 Training Loss: 0.0842159166932106\n",
      "Epoch 3365/30000 Training Loss: 0.09150278568267822\n",
      "Epoch 3366/30000 Training Loss: 0.08165924996137619\n",
      "Epoch 3367/30000 Training Loss: 0.06750517338514328\n",
      "Epoch 3368/30000 Training Loss: 0.08224555104970932\n",
      "Epoch 3369/30000 Training Loss: 0.0658365786075592\n",
      "Epoch 3370/30000 Training Loss: 0.05950583145022392\n",
      "Epoch 3370/30000 Validation Loss: 0.07368674874305725\n",
      "Epoch 3371/30000 Training Loss: 0.08171238750219345\n",
      "Epoch 3372/30000 Training Loss: 0.08514940738677979\n",
      "Epoch 3373/30000 Training Loss: 0.07150773704051971\n",
      "Epoch 3374/30000 Training Loss: 0.09438807517290115\n",
      "Epoch 3375/30000 Training Loss: 0.10818170756101608\n",
      "Epoch 3376/30000 Training Loss: 0.07153531908988953\n",
      "Epoch 3377/30000 Training Loss: 0.06473055481910706\n",
      "Epoch 3378/30000 Training Loss: 0.09719002991914749\n",
      "Epoch 3379/30000 Training Loss: 0.06680463999509811\n",
      "Epoch 3380/30000 Training Loss: 0.08527693897485733\n",
      "Epoch 3380/30000 Validation Loss: 0.06939072161912918\n",
      "Epoch 3381/30000 Training Loss: 0.08015631884336472\n",
      "Epoch 3382/30000 Training Loss: 0.07996386289596558\n",
      "Epoch 3383/30000 Training Loss: 0.06208285689353943\n",
      "Epoch 3384/30000 Training Loss: 0.08586999028921127\n",
      "Epoch 3385/30000 Training Loss: 0.0628613829612732\n",
      "Epoch 3386/30000 Training Loss: 0.09283032268285751\n",
      "Epoch 3387/30000 Training Loss: 0.08395848423242569\n",
      "Epoch 3388/30000 Training Loss: 0.07903186976909637\n",
      "Epoch 3389/30000 Training Loss: 0.07184001803398132\n",
      "Epoch 3390/30000 Training Loss: 0.0716029480099678\n",
      "Epoch 3390/30000 Validation Loss: 0.11764860898256302\n",
      "Epoch 3391/30000 Training Loss: 0.09859415143728256\n",
      "Epoch 3392/30000 Training Loss: 0.09290648251771927\n",
      "Epoch 3393/30000 Training Loss: 0.07242605835199356\n",
      "Epoch 3394/30000 Training Loss: 0.07116594165563583\n",
      "Epoch 3395/30000 Training Loss: 0.10843149572610855\n",
      "Epoch 3396/30000 Training Loss: 0.08675578236579895\n",
      "Epoch 3397/30000 Training Loss: 0.08695319294929504\n",
      "Epoch 3398/30000 Training Loss: 0.09173009544610977\n",
      "Epoch 3399/30000 Training Loss: 0.07869533449411392\n",
      "Epoch 3400/30000 Training Loss: 0.08765456825494766\n",
      "Epoch 3400/30000 Validation Loss: 0.06561864167451859\n",
      "Epoch 3401/30000 Training Loss: 0.06240898370742798\n",
      "Epoch 3402/30000 Training Loss: 0.09030896425247192\n",
      "Epoch 3403/30000 Training Loss: 0.08413996547460556\n",
      "Epoch 3404/30000 Training Loss: 0.07835099846124649\n",
      "Epoch 3405/30000 Training Loss: 0.08704698830842972\n",
      "Epoch 3406/30000 Training Loss: 0.07053595036268234\n",
      "Epoch 3407/30000 Training Loss: 0.10146940499544144\n",
      "Epoch 3408/30000 Training Loss: 0.07731806486845016\n",
      "Epoch 3409/30000 Training Loss: 0.07085611671209335\n",
      "Epoch 3410/30000 Training Loss: 0.06820964813232422\n",
      "Epoch 3410/30000 Validation Loss: 0.06169688701629639\n",
      "Epoch 3411/30000 Training Loss: 0.06603743880987167\n",
      "Epoch 3412/30000 Training Loss: 0.07253289967775345\n",
      "Epoch 3413/30000 Training Loss: 0.09922724962234497\n",
      "Epoch 3414/30000 Training Loss: 0.09670523554086685\n",
      "Epoch 3415/30000 Training Loss: 0.09980624914169312\n",
      "Epoch 3416/30000 Training Loss: 0.07374314218759537\n",
      "Epoch 3417/30000 Training Loss: 0.08557423949241638\n",
      "Epoch 3418/30000 Training Loss: 0.08985975384712219\n",
      "Epoch 3419/30000 Training Loss: 0.07822167873382568\n",
      "Epoch 3420/30000 Training Loss: 0.08076059818267822\n",
      "Epoch 3420/30000 Validation Loss: 0.08070307224988937\n",
      "Epoch 3421/30000 Training Loss: 0.07858756929636002\n",
      "Epoch 3422/30000 Training Loss: 0.10424679517745972\n",
      "Epoch 3423/30000 Training Loss: 0.06786876171827316\n",
      "Epoch 3424/30000 Training Loss: 0.08236708492040634\n",
      "Epoch 3425/30000 Training Loss: 0.07494174689054489\n",
      "Epoch 3426/30000 Training Loss: 0.06733065098524094\n",
      "Epoch 3427/30000 Training Loss: 0.08198238909244537\n",
      "Epoch 3428/30000 Training Loss: 0.07290569692850113\n",
      "Epoch 3429/30000 Training Loss: 0.0695500299334526\n",
      "Epoch 3430/30000 Training Loss: 0.0975349172949791\n",
      "Epoch 3430/30000 Validation Loss: 0.08440402150154114\n",
      "Epoch 3431/30000 Training Loss: 0.07169914990663528\n",
      "Epoch 3432/30000 Training Loss: 0.08993083983659744\n",
      "Epoch 3433/30000 Training Loss: 0.08214891701936722\n",
      "Epoch 3434/30000 Training Loss: 0.07957281917333603\n",
      "Epoch 3435/30000 Training Loss: 0.08981522172689438\n",
      "Epoch 3436/30000 Training Loss: 0.10204582661390305\n",
      "Epoch 3437/30000 Training Loss: 0.06256071478128433\n",
      "Epoch 3438/30000 Training Loss: 0.09805648773908615\n",
      "Epoch 3439/30000 Training Loss: 0.08659350872039795\n",
      "Epoch 3440/30000 Training Loss: 0.09817109256982803\n",
      "Epoch 3440/30000 Validation Loss: 0.08909744024276733\n",
      "Epoch 3441/30000 Training Loss: 0.09015881270170212\n",
      "Epoch 3442/30000 Training Loss: 0.06782349199056625\n",
      "Epoch 3443/30000 Training Loss: 0.07282823324203491\n",
      "Epoch 3444/30000 Training Loss: 0.08035968989133835\n",
      "Epoch 3445/30000 Training Loss: 0.08866313099861145\n",
      "Epoch 3446/30000 Training Loss: 0.0975899025797844\n",
      "Epoch 3447/30000 Training Loss: 0.11586674302816391\n",
      "Epoch 3448/30000 Training Loss: 0.06904841959476471\n",
      "Epoch 3449/30000 Training Loss: 0.09660790115594864\n",
      "Epoch 3450/30000 Training Loss: 0.09064185619354248\n",
      "Epoch 3450/30000 Validation Loss: 0.08392956107854843\n",
      "Epoch 3451/30000 Training Loss: 0.08428996801376343\n",
      "Epoch 3452/30000 Training Loss: 0.08667775988578796\n",
      "Epoch 3453/30000 Training Loss: 0.06319732218980789\n",
      "Epoch 3454/30000 Training Loss: 0.09800108522176743\n",
      "Epoch 3455/30000 Training Loss: 0.07342032343149185\n",
      "Epoch 3456/30000 Training Loss: 0.06776712089776993\n",
      "Epoch 3457/30000 Training Loss: 0.07201062887907028\n",
      "Epoch 3458/30000 Training Loss: 0.06317886710166931\n",
      "Epoch 3459/30000 Training Loss: 0.0740920677781105\n",
      "Epoch 3460/30000 Training Loss: 0.08048287034034729\n",
      "Epoch 3460/30000 Validation Loss: 0.07713616639375687\n",
      "Epoch 3461/30000 Training Loss: 0.08500520139932632\n",
      "Epoch 3462/30000 Training Loss: 0.09003730863332748\n",
      "Epoch 3463/30000 Training Loss: 0.09823060035705566\n",
      "Epoch 3464/30000 Training Loss: 0.08661755174398422\n",
      "Epoch 3465/30000 Training Loss: 0.10867943614721298\n",
      "Epoch 3466/30000 Training Loss: 0.07452598959207535\n",
      "Epoch 3467/30000 Training Loss: 0.06756747514009476\n",
      "Epoch 3468/30000 Training Loss: 0.08033677190542221\n",
      "Epoch 3469/30000 Training Loss: 0.07692895084619522\n",
      "Epoch 3470/30000 Training Loss: 0.08991221338510513\n",
      "Epoch 3470/30000 Validation Loss: 0.06779421120882034\n",
      "Epoch 3471/30000 Training Loss: 0.06061847135424614\n",
      "Epoch 3472/30000 Training Loss: 0.09473550319671631\n",
      "Epoch 3473/30000 Training Loss: 0.09788715094327927\n",
      "Epoch 3474/30000 Training Loss: 0.08628642559051514\n",
      "Epoch 3475/30000 Training Loss: 0.08716221898794174\n",
      "Epoch 3476/30000 Training Loss: 0.06531737744808197\n",
      "Epoch 3477/30000 Training Loss: 0.11690526455640793\n",
      "Epoch 3478/30000 Training Loss: 0.07970704138278961\n",
      "Epoch 3479/30000 Training Loss: 0.09552746266126633\n",
      "Epoch 3480/30000 Training Loss: 0.07520129531621933\n",
      "Epoch 3480/30000 Validation Loss: 0.06994710117578506\n",
      "Epoch 3481/30000 Training Loss: 0.10745477676391602\n",
      "Epoch 3482/30000 Training Loss: 0.08159562945365906\n",
      "Epoch 3483/30000 Training Loss: 0.09833565354347229\n",
      "Epoch 3484/30000 Training Loss: 0.08032724261283875\n",
      "Epoch 3485/30000 Training Loss: 0.0832534208893776\n",
      "Epoch 3486/30000 Training Loss: 0.07239329069852829\n",
      "Epoch 3487/30000 Training Loss: 0.099022276699543\n",
      "Epoch 3488/30000 Training Loss: 0.08120051771402359\n",
      "Epoch 3489/30000 Training Loss: 0.08069709688425064\n",
      "Epoch 3490/30000 Training Loss: 0.08269752562046051\n",
      "Epoch 3490/30000 Validation Loss: 0.08376670628786087\n",
      "Epoch 3491/30000 Training Loss: 0.06154557690024376\n",
      "Epoch 3492/30000 Training Loss: 0.11063050478696823\n",
      "Epoch 3493/30000 Training Loss: 0.060531944036483765\n",
      "Epoch 3494/30000 Training Loss: 0.08712935447692871\n",
      "Epoch 3495/30000 Training Loss: 0.07355397194623947\n",
      "Epoch 3496/30000 Training Loss: 0.09748264402151108\n",
      "Epoch 3497/30000 Training Loss: 0.08041337877511978\n",
      "Epoch 3498/30000 Training Loss: 0.07198431342840195\n",
      "Epoch 3499/30000 Training Loss: 0.0725853368639946\n",
      "Epoch 3500/30000 Training Loss: 0.07428868114948273\n",
      "Epoch 3500/30000 Validation Loss: 0.09614282846450806\n",
      "Epoch 3501/30000 Training Loss: 0.07389519363641739\n",
      "Epoch 3502/30000 Training Loss: 0.08864664286375046\n",
      "Epoch 3503/30000 Training Loss: 0.07696452736854553\n",
      "Epoch 3504/30000 Training Loss: 0.10525355488061905\n",
      "Epoch 3505/30000 Training Loss: 0.10379060357809067\n",
      "Epoch 3506/30000 Training Loss: 0.07810028642416\n",
      "Epoch 3507/30000 Training Loss: 0.07370695471763611\n",
      "Epoch 3508/30000 Training Loss: 0.06302276253700256\n",
      "Epoch 3509/30000 Training Loss: 0.08690973371267319\n",
      "Epoch 3510/30000 Training Loss: 0.09470004588365555\n",
      "Epoch 3510/30000 Validation Loss: 0.10678640007972717\n",
      "Epoch 3511/30000 Training Loss: 0.0846499428153038\n",
      "Epoch 3512/30000 Training Loss: 0.07972989231348038\n",
      "Epoch 3513/30000 Training Loss: 0.09379126876592636\n",
      "Epoch 3514/30000 Training Loss: 0.05898298695683479\n",
      "Epoch 3515/30000 Training Loss: 0.0922057256102562\n",
      "Epoch 3516/30000 Training Loss: 0.09091377258300781\n",
      "Epoch 3517/30000 Training Loss: 0.09873265027999878\n",
      "Epoch 3518/30000 Training Loss: 0.09635904431343079\n",
      "Epoch 3519/30000 Training Loss: 0.10219664126634598\n",
      "Epoch 3520/30000 Training Loss: 0.07715541869401932\n",
      "Epoch 3520/30000 Validation Loss: 0.0752905085682869\n",
      "Epoch 3521/30000 Training Loss: 0.09397175163030624\n",
      "Epoch 3522/30000 Training Loss: 0.07829681783914566\n",
      "Epoch 3523/30000 Training Loss: 0.07246232032775879\n",
      "Epoch 3524/30000 Training Loss: 0.07774302363395691\n",
      "Epoch 3525/30000 Training Loss: 0.08873152732849121\n",
      "Epoch 3526/30000 Training Loss: 0.08861557394266129\n",
      "Epoch 3527/30000 Training Loss: 0.07682079821825027\n",
      "Epoch 3528/30000 Training Loss: 0.07147172093391418\n",
      "Epoch 3529/30000 Training Loss: 0.08410757780075073\n",
      "Epoch 3530/30000 Training Loss: 0.08123530447483063\n",
      "Epoch 3530/30000 Validation Loss: 0.07992467284202576\n",
      "Epoch 3531/30000 Training Loss: 0.07947853952646255\n",
      "Epoch 3532/30000 Training Loss: 0.088166244328022\n",
      "Epoch 3533/30000 Training Loss: 0.08316918462514877\n",
      "Epoch 3534/30000 Training Loss: 0.07566361874341965\n",
      "Epoch 3535/30000 Training Loss: 0.09642855077981949\n",
      "Epoch 3536/30000 Training Loss: 0.07974041253328323\n",
      "Epoch 3537/30000 Training Loss: 0.0897650346159935\n",
      "Epoch 3538/30000 Training Loss: 0.08404938131570816\n",
      "Epoch 3539/30000 Training Loss: 0.0776686742901802\n",
      "Epoch 3540/30000 Training Loss: 0.08851698040962219\n",
      "Epoch 3540/30000 Validation Loss: 0.09518023580312729\n",
      "Epoch 3541/30000 Training Loss: 0.08003724366426468\n",
      "Epoch 3542/30000 Training Loss: 0.09071013331413269\n",
      "Epoch 3543/30000 Training Loss: 0.08116362243890762\n",
      "Epoch 3544/30000 Training Loss: 0.08544345945119858\n",
      "Epoch 3545/30000 Training Loss: 0.09052997082471848\n",
      "Epoch 3546/30000 Training Loss: 0.07175416499376297\n",
      "Epoch 3547/30000 Training Loss: 0.08229772001504898\n",
      "Epoch 3548/30000 Training Loss: 0.06897418946027756\n",
      "Epoch 3549/30000 Training Loss: 0.07061445713043213\n",
      "Epoch 3550/30000 Training Loss: 0.1268012672662735\n",
      "Epoch 3550/30000 Validation Loss: 0.07398011535406113\n",
      "Epoch 3551/30000 Training Loss: 0.09129666537046432\n",
      "Epoch 3552/30000 Training Loss: 0.06591221690177917\n",
      "Epoch 3553/30000 Training Loss: 0.09408401697874069\n",
      "Epoch 3554/30000 Training Loss: 0.09427386522293091\n",
      "Epoch 3555/30000 Training Loss: 0.08811137825250626\n",
      "Epoch 3556/30000 Training Loss: 0.07604371756315231\n",
      "Epoch 3557/30000 Training Loss: 0.0851067304611206\n",
      "Epoch 3558/30000 Training Loss: 0.07957912981510162\n",
      "Epoch 3559/30000 Training Loss: 0.06849076598882675\n",
      "Epoch 3560/30000 Training Loss: 0.07604102045297623\n",
      "Epoch 3560/30000 Validation Loss: 0.11181097477674484\n",
      "Epoch 3561/30000 Training Loss: 0.08031103760004044\n",
      "Epoch 3562/30000 Training Loss: 0.07693133503198624\n",
      "Epoch 3563/30000 Training Loss: 0.08808716386556625\n",
      "Epoch 3564/30000 Training Loss: 0.08875241875648499\n",
      "Epoch 3565/30000 Training Loss: 0.08311887830495834\n",
      "Epoch 3566/30000 Training Loss: 0.07459713518619537\n",
      "Epoch 3567/30000 Training Loss: 0.0922987088561058\n",
      "Epoch 3568/30000 Training Loss: 0.08505859971046448\n",
      "Epoch 3569/30000 Training Loss: 0.08117488771677017\n",
      "Epoch 3570/30000 Training Loss: 0.07639860361814499\n",
      "Epoch 3570/30000 Validation Loss: 0.10041943937540054\n",
      "Epoch 3571/30000 Training Loss: 0.07193347811698914\n",
      "Epoch 3572/30000 Training Loss: 0.06493854522705078\n",
      "Epoch 3573/30000 Training Loss: 0.08811523765325546\n",
      "Epoch 3574/30000 Training Loss: 0.09771526604890823\n",
      "Epoch 3575/30000 Training Loss: 0.0728796198964119\n",
      "Epoch 3576/30000 Training Loss: 0.0798702985048294\n",
      "Epoch 3577/30000 Training Loss: 0.09362917393445969\n",
      "Epoch 3578/30000 Training Loss: 0.09593740105628967\n",
      "Epoch 3579/30000 Training Loss: 0.0838932916522026\n",
      "Epoch 3580/30000 Training Loss: 0.07920338958501816\n",
      "Epoch 3580/30000 Validation Loss: 0.09744733572006226\n",
      "Epoch 3581/30000 Training Loss: 0.07746124267578125\n",
      "Epoch 3582/30000 Training Loss: 0.08063911646604538\n",
      "Epoch 3583/30000 Training Loss: 0.11544711142778397\n",
      "Epoch 3584/30000 Training Loss: 0.06596407294273376\n",
      "Epoch 3585/30000 Training Loss: 0.104106105864048\n",
      "Epoch 3586/30000 Training Loss: 0.07378831505775452\n",
      "Epoch 3587/30000 Training Loss: 0.08953849226236343\n",
      "Epoch 3588/30000 Training Loss: 0.07811659574508667\n",
      "Epoch 3589/30000 Training Loss: 0.056524455547332764\n",
      "Epoch 3590/30000 Training Loss: 0.07153509557247162\n",
      "Epoch 3590/30000 Validation Loss: 0.10745477676391602\n",
      "Epoch 3591/30000 Training Loss: 0.07458123564720154\n",
      "Epoch 3592/30000 Training Loss: 0.08550862222909927\n",
      "Epoch 3593/30000 Training Loss: 0.07044195383787155\n",
      "Epoch 3594/30000 Training Loss: 0.06899412721395493\n",
      "Epoch 3595/30000 Training Loss: 0.07398410886526108\n",
      "Epoch 3596/30000 Training Loss: 0.0762760117650032\n",
      "Epoch 3597/30000 Training Loss: 0.06363442540168762\n",
      "Epoch 3598/30000 Training Loss: 0.08625780791044235\n",
      "Epoch 3599/30000 Training Loss: 0.08784281462430954\n",
      "Epoch 3600/30000 Training Loss: 0.07619109749794006\n",
      "Epoch 3600/30000 Validation Loss: 0.09153395146131516\n",
      "Epoch 3601/30000 Training Loss: 0.07155200093984604\n",
      "Epoch 3602/30000 Training Loss: 0.06659767031669617\n",
      "Epoch 3603/30000 Training Loss: 0.09580302983522415\n",
      "Epoch 3604/30000 Training Loss: 0.06642390042543411\n",
      "Epoch 3605/30000 Training Loss: 0.09824731945991516\n",
      "Epoch 3606/30000 Training Loss: 0.10763183236122131\n",
      "Epoch 3607/30000 Training Loss: 0.08021178841590881\n",
      "Epoch 3608/30000 Training Loss: 0.09123193472623825\n",
      "Epoch 3609/30000 Training Loss: 0.07771224528551102\n",
      "Epoch 3610/30000 Training Loss: 0.07100186496973038\n",
      "Epoch 3610/30000 Validation Loss: 0.09688220173120499\n",
      "Epoch 3611/30000 Training Loss: 0.06265362352132797\n",
      "Epoch 3612/30000 Training Loss: 0.1087246909737587\n",
      "Epoch 3613/30000 Training Loss: 0.0837581679224968\n",
      "Epoch 3614/30000 Training Loss: 0.07362444698810577\n",
      "Epoch 3615/30000 Training Loss: 0.07063723355531693\n",
      "Epoch 3616/30000 Training Loss: 0.08461228013038635\n",
      "Epoch 3617/30000 Training Loss: 0.07171063125133514\n",
      "Epoch 3618/30000 Training Loss: 0.06331511586904526\n",
      "Epoch 3619/30000 Training Loss: 0.09109154343605042\n",
      "Epoch 3620/30000 Training Loss: 0.06867650896310806\n",
      "Epoch 3620/30000 Validation Loss: 0.09270676970481873\n",
      "Epoch 3621/30000 Training Loss: 0.09448205679655075\n",
      "Epoch 3622/30000 Training Loss: 0.08193717151880264\n",
      "Epoch 3623/30000 Training Loss: 0.10120431333780289\n",
      "Epoch 3624/30000 Training Loss: 0.07898631691932678\n",
      "Epoch 3625/30000 Training Loss: 0.07436472922563553\n",
      "Epoch 3626/30000 Training Loss: 0.08213385194540024\n",
      "Epoch 3627/30000 Training Loss: 0.09103120118379593\n",
      "Epoch 3628/30000 Training Loss: 0.08252989500761032\n",
      "Epoch 3629/30000 Training Loss: 0.08160907030105591\n",
      "Epoch 3630/30000 Training Loss: 0.0736992210149765\n",
      "Epoch 3630/30000 Validation Loss: 0.09054073691368103\n",
      "Epoch 3631/30000 Training Loss: 0.06952810287475586\n",
      "Epoch 3632/30000 Training Loss: 0.0812494084239006\n",
      "Epoch 3633/30000 Training Loss: 0.07223577052354813\n",
      "Epoch 3634/30000 Training Loss: 0.07036976516246796\n",
      "Epoch 3635/30000 Training Loss: 0.08578592538833618\n",
      "Epoch 3636/30000 Training Loss: 0.09754589945077896\n",
      "Epoch 3637/30000 Training Loss: 0.0650920420885086\n",
      "Epoch 3638/30000 Training Loss: 0.07658416032791138\n",
      "Epoch 3639/30000 Training Loss: 0.08687520027160645\n",
      "Epoch 3640/30000 Training Loss: 0.07246281951665878\n",
      "Epoch 3640/30000 Validation Loss: 0.09143853932619095\n",
      "Epoch 3641/30000 Training Loss: 0.07985590398311615\n",
      "Epoch 3642/30000 Training Loss: 0.08251336961984634\n",
      "Epoch 3643/30000 Training Loss: 0.06365642696619034\n",
      "Epoch 3644/30000 Training Loss: 0.08375188708305359\n",
      "Epoch 3645/30000 Training Loss: 0.08962586522102356\n",
      "Epoch 3646/30000 Training Loss: 0.086764395236969\n",
      "Epoch 3647/30000 Training Loss: 0.08226621896028519\n",
      "Epoch 3648/30000 Training Loss: 0.10048366338014603\n",
      "Epoch 3649/30000 Training Loss: 0.0953853502869606\n",
      "Epoch 3650/30000 Training Loss: 0.08326580375432968\n",
      "Epoch 3650/30000 Validation Loss: 0.08701426535844803\n",
      "Epoch 3651/30000 Training Loss: 0.09965473413467407\n",
      "Epoch 3652/30000 Training Loss: 0.07672540098428726\n",
      "Epoch 3653/30000 Training Loss: 0.08626297861337662\n",
      "Epoch 3654/30000 Training Loss: 0.09044506400823593\n",
      "Epoch 3655/30000 Training Loss: 0.08984118700027466\n",
      "Epoch 3656/30000 Training Loss: 0.11567848920822144\n",
      "Epoch 3657/30000 Training Loss: 0.07826986908912659\n",
      "Epoch 3658/30000 Training Loss: 0.08023291826248169\n",
      "Epoch 3659/30000 Training Loss: 0.07992351800203323\n",
      "Epoch 3660/30000 Training Loss: 0.06560349464416504\n",
      "Epoch 3660/30000 Validation Loss: 0.07473201304674149\n",
      "Epoch 3661/30000 Training Loss: 0.08266828209161758\n",
      "Epoch 3662/30000 Training Loss: 0.05813969299197197\n",
      "Epoch 3663/30000 Training Loss: 0.10061176866292953\n",
      "Epoch 3664/30000 Training Loss: 0.06552841514348984\n",
      "Epoch 3665/30000 Training Loss: 0.06535537540912628\n",
      "Epoch 3666/30000 Training Loss: 0.07116413116455078\n",
      "Epoch 3667/30000 Training Loss: 0.09175997227430344\n",
      "Epoch 3668/30000 Training Loss: 0.09196940809488297\n",
      "Epoch 3669/30000 Training Loss: 0.07642221450805664\n",
      "Epoch 3670/30000 Training Loss: 0.10660294443368912\n",
      "Epoch 3670/30000 Validation Loss: 0.08144082874059677\n",
      "Epoch 3671/30000 Training Loss: 0.06850585341453552\n",
      "Epoch 3672/30000 Training Loss: 0.08887363225221634\n",
      "Epoch 3673/30000 Training Loss: 0.07020647078752518\n",
      "Epoch 3674/30000 Training Loss: 0.086017906665802\n",
      "Epoch 3675/30000 Training Loss: 0.08257641643285751\n",
      "Epoch 3676/30000 Training Loss: 0.08920777589082718\n",
      "Epoch 3677/30000 Training Loss: 0.09055761247873306\n",
      "Epoch 3678/30000 Training Loss: 0.06973794847726822\n",
      "Epoch 3679/30000 Training Loss: 0.09184473752975464\n",
      "Epoch 3680/30000 Training Loss: 0.09677930921316147\n",
      "Epoch 3680/30000 Validation Loss: 0.07291515916585922\n",
      "Epoch 3681/30000 Training Loss: 0.0918666198849678\n",
      "Epoch 3682/30000 Training Loss: 0.076517254114151\n",
      "Epoch 3683/30000 Training Loss: 0.07919049263000488\n",
      "Epoch 3684/30000 Training Loss: 0.08272957801818848\n",
      "Epoch 3685/30000 Training Loss: 0.08827970176935196\n",
      "Epoch 3686/30000 Training Loss: 0.07653605192899704\n",
      "Epoch 3687/30000 Training Loss: 0.07009252160787582\n",
      "Epoch 3688/30000 Training Loss: 0.07307101786136627\n",
      "Epoch 3689/30000 Training Loss: 0.07418275624513626\n",
      "Epoch 3690/30000 Training Loss: 0.0734381154179573\n",
      "Epoch 3690/30000 Validation Loss: 0.059733521193265915\n",
      "Epoch 3691/30000 Training Loss: 0.08063876628875732\n",
      "Epoch 3692/30000 Training Loss: 0.07302705198526382\n",
      "Epoch 3693/30000 Training Loss: 0.09533414989709854\n",
      "Epoch 3694/30000 Training Loss: 0.07724645733833313\n",
      "Epoch 3695/30000 Training Loss: 0.0887734591960907\n",
      "Epoch 3696/30000 Training Loss: 0.08484109491109848\n",
      "Epoch 3697/30000 Training Loss: 0.0871896967291832\n",
      "Epoch 3698/30000 Training Loss: 0.09004328399896622\n",
      "Epoch 3699/30000 Training Loss: 0.08159054070711136\n",
      "Epoch 3700/30000 Training Loss: 0.10314839333295822\n",
      "Epoch 3700/30000 Validation Loss: 0.06007353961467743\n",
      "Epoch 3701/30000 Training Loss: 0.057249393314123154\n",
      "Epoch 3702/30000 Training Loss: 0.08666492253541946\n",
      "Epoch 3703/30000 Training Loss: 0.06937777996063232\n",
      "Epoch 3704/30000 Training Loss: 0.07349929213523865\n",
      "Epoch 3705/30000 Training Loss: 0.07625110447406769\n",
      "Epoch 3706/30000 Training Loss: 0.07058148831129074\n",
      "Epoch 3707/30000 Training Loss: 0.07864487916231155\n",
      "Epoch 3708/30000 Training Loss: 0.08283868432044983\n",
      "Epoch 3709/30000 Training Loss: 0.08981885761022568\n",
      "Epoch 3710/30000 Training Loss: 0.07677378505468369\n",
      "Epoch 3710/30000 Validation Loss: 0.07173442095518112\n",
      "Epoch 3711/30000 Training Loss: 0.08278369903564453\n",
      "Epoch 3712/30000 Training Loss: 0.0621861070394516\n",
      "Epoch 3713/30000 Training Loss: 0.08980777114629745\n",
      "Epoch 3714/30000 Training Loss: 0.10310345143079758\n",
      "Epoch 3715/30000 Training Loss: 0.09823579341173172\n",
      "Epoch 3716/30000 Training Loss: 0.08036476373672485\n",
      "Epoch 3717/30000 Training Loss: 0.05382457375526428\n",
      "Epoch 3718/30000 Training Loss: 0.08932133764028549\n",
      "Epoch 3719/30000 Training Loss: 0.0671985000371933\n",
      "Epoch 3720/30000 Training Loss: 0.08367208391427994\n",
      "Epoch 3720/30000 Validation Loss: 0.07637875527143478\n",
      "Epoch 3721/30000 Training Loss: 0.08854049444198608\n",
      "Epoch 3722/30000 Training Loss: 0.07419689744710922\n",
      "Epoch 3723/30000 Training Loss: 0.07534316927194595\n",
      "Epoch 3724/30000 Training Loss: 0.09466806799173355\n",
      "Epoch 3725/30000 Training Loss: 0.08516400307416916\n",
      "Epoch 3726/30000 Training Loss: 0.07090141624212265\n",
      "Epoch 3727/30000 Training Loss: 0.07461994141340256\n",
      "Epoch 3728/30000 Training Loss: 0.07454515248537064\n",
      "Epoch 3729/30000 Training Loss: 0.07587385177612305\n",
      "Epoch 3730/30000 Training Loss: 0.07824946194887161\n",
      "Epoch 3730/30000 Validation Loss: 0.08912193775177002\n",
      "Epoch 3731/30000 Training Loss: 0.07919678092002869\n",
      "Epoch 3732/30000 Training Loss: 0.08460468053817749\n",
      "Epoch 3733/30000 Training Loss: 0.09475711733102798\n",
      "Epoch 3734/30000 Training Loss: 0.0985138937830925\n",
      "Epoch 3735/30000 Training Loss: 0.0847153440117836\n",
      "Epoch 3736/30000 Training Loss: 0.10877006500959396\n",
      "Epoch 3737/30000 Training Loss: 0.058874428272247314\n",
      "Epoch 3738/30000 Training Loss: 0.07427877187728882\n",
      "Epoch 3739/30000 Training Loss: 0.07227037101984024\n",
      "Epoch 3740/30000 Training Loss: 0.09538974612951279\n",
      "Epoch 3740/30000 Validation Loss: 0.09079995006322861\n",
      "Epoch 3741/30000 Training Loss: 0.10310694575309753\n",
      "Epoch 3742/30000 Training Loss: 0.07658872753381729\n",
      "Epoch 3743/30000 Training Loss: 0.0877453163266182\n",
      "Epoch 3744/30000 Training Loss: 0.09409534931182861\n",
      "Epoch 3745/30000 Training Loss: 0.07572903484106064\n",
      "Epoch 3746/30000 Training Loss: 0.0776609554886818\n",
      "Epoch 3747/30000 Training Loss: 0.07324996590614319\n",
      "Epoch 3748/30000 Training Loss: 0.08346801996231079\n",
      "Epoch 3749/30000 Training Loss: 0.10240837186574936\n",
      "Epoch 3750/30000 Training Loss: 0.06896886974573135\n",
      "Epoch 3750/30000 Validation Loss: 0.07118767499923706\n",
      "Epoch 3751/30000 Training Loss: 0.08458653837442398\n",
      "Epoch 3752/30000 Training Loss: 0.07300584763288498\n",
      "Epoch 3753/30000 Training Loss: 0.07309719175100327\n",
      "Epoch 3754/30000 Training Loss: 0.09497008472681046\n",
      "Epoch 3755/30000 Training Loss: 0.0808865949511528\n",
      "Epoch 3756/30000 Training Loss: 0.08421739190816879\n",
      "Epoch 3757/30000 Training Loss: 0.06805448234081268\n",
      "Epoch 3758/30000 Training Loss: 0.06437418609857559\n",
      "Epoch 3759/30000 Training Loss: 0.06990277022123337\n",
      "Epoch 3760/30000 Training Loss: 0.08343533426523209\n",
      "Epoch 3760/30000 Validation Loss: 0.06667696684598923\n",
      "Epoch 3761/30000 Training Loss: 0.06785278767347336\n",
      "Epoch 3762/30000 Training Loss: 0.07395065575838089\n",
      "Epoch 3763/30000 Training Loss: 0.07469884306192398\n",
      "Epoch 3764/30000 Training Loss: 0.06651085615158081\n",
      "Epoch 3765/30000 Training Loss: 0.08996645361185074\n",
      "Epoch 3766/30000 Training Loss: 0.07700900733470917\n",
      "Epoch 3767/30000 Training Loss: 0.09687743335962296\n",
      "Epoch 3768/30000 Training Loss: 0.0701800286769867\n",
      "Epoch 3769/30000 Training Loss: 0.0895223617553711\n",
      "Epoch 3770/30000 Training Loss: 0.08832518011331558\n",
      "Epoch 3770/30000 Validation Loss: 0.08747487515211105\n",
      "Epoch 3771/30000 Training Loss: 0.100718192756176\n",
      "Epoch 3772/30000 Training Loss: 0.06487768143415451\n",
      "Epoch 3773/30000 Training Loss: 0.0996885597705841\n",
      "Epoch 3774/30000 Training Loss: 0.10030904412269592\n",
      "Epoch 3775/30000 Training Loss: 0.06112067028880119\n",
      "Epoch 3776/30000 Training Loss: 0.09707605093717575\n",
      "Epoch 3777/30000 Training Loss: 0.07374174892902374\n",
      "Epoch 3778/30000 Training Loss: 0.06405407935380936\n",
      "Epoch 3779/30000 Training Loss: 0.10123875737190247\n",
      "Epoch 3780/30000 Training Loss: 0.07319837808609009\n",
      "Epoch 3780/30000 Validation Loss: 0.10047324746847153\n",
      "Epoch 3781/30000 Training Loss: 0.0999453142285347\n",
      "Epoch 3782/30000 Training Loss: 0.08071177452802658\n",
      "Epoch 3783/30000 Training Loss: 0.07573475688695908\n",
      "Epoch 3784/30000 Training Loss: 0.09186014533042908\n",
      "Epoch 3785/30000 Training Loss: 0.084494948387146\n",
      "Epoch 3786/30000 Training Loss: 0.10699053853750229\n",
      "Epoch 3787/30000 Training Loss: 0.08589563518762589\n",
      "Epoch 3788/30000 Training Loss: 0.07211358100175858\n",
      "Epoch 3789/30000 Training Loss: 0.08406845480203629\n",
      "Epoch 3790/30000 Training Loss: 0.06904718279838562\n",
      "Epoch 3790/30000 Validation Loss: 0.07295846939086914\n",
      "Epoch 3791/30000 Training Loss: 0.0677163377404213\n",
      "Epoch 3792/30000 Training Loss: 0.08488922566175461\n",
      "Epoch 3793/30000 Training Loss: 0.073480524122715\n",
      "Epoch 3794/30000 Training Loss: 0.0807899758219719\n",
      "Epoch 3795/30000 Training Loss: 0.08207333832979202\n",
      "Epoch 3796/30000 Training Loss: 0.0753711462020874\n",
      "Epoch 3797/30000 Training Loss: 0.09440916776657104\n",
      "Epoch 3798/30000 Training Loss: 0.07906445115804672\n",
      "Epoch 3799/30000 Training Loss: 0.0822364017367363\n",
      "Epoch 3800/30000 Training Loss: 0.0845181867480278\n",
      "Epoch 3800/30000 Validation Loss: 0.0815528854727745\n",
      "Epoch 3801/30000 Training Loss: 0.07149127125740051\n",
      "Epoch 3802/30000 Training Loss: 0.08439480513334274\n",
      "Epoch 3803/30000 Training Loss: 0.08784424513578415\n",
      "Epoch 3804/30000 Training Loss: 0.06832260638475418\n",
      "Epoch 3805/30000 Training Loss: 0.07709335535764694\n",
      "Epoch 3806/30000 Training Loss: 0.09054461121559143\n",
      "Epoch 3807/30000 Training Loss: 0.09363075345754623\n",
      "Epoch 3808/30000 Training Loss: 0.07445808500051498\n",
      "Epoch 3809/30000 Training Loss: 0.08777221292257309\n",
      "Epoch 3810/30000 Training Loss: 0.06066332757472992\n",
      "Epoch 3810/30000 Validation Loss: 0.09745698422193527\n",
      "Epoch 3811/30000 Training Loss: 0.061004314571619034\n",
      "Epoch 3812/30000 Training Loss: 0.08472707867622375\n",
      "Epoch 3813/30000 Training Loss: 0.09321298450231552\n",
      "Epoch 3814/30000 Training Loss: 0.08727624267339706\n",
      "Epoch 3815/30000 Training Loss: 0.08504084497690201\n",
      "Epoch 3816/30000 Training Loss: 0.06469767540693283\n",
      "Epoch 3817/30000 Training Loss: 0.07162042707204819\n",
      "Epoch 3818/30000 Training Loss: 0.07151994854211807\n",
      "Epoch 3819/30000 Training Loss: 0.07695887237787247\n",
      "Epoch 3820/30000 Training Loss: 0.08422958850860596\n",
      "Epoch 3820/30000 Validation Loss: 0.07007744163274765\n",
      "Epoch 3821/30000 Training Loss: 0.09324628114700317\n",
      "Epoch 3822/30000 Training Loss: 0.058911655098199844\n",
      "Epoch 3823/30000 Training Loss: 0.07317095249891281\n",
      "Epoch 3824/30000 Training Loss: 0.0563267357647419\n",
      "Epoch 3825/30000 Training Loss: 0.06816074252128601\n",
      "Epoch 3826/30000 Training Loss: 0.09182298183441162\n",
      "Epoch 3827/30000 Training Loss: 0.07825277000665665\n",
      "Epoch 3828/30000 Training Loss: 0.06909644603729248\n",
      "Epoch 3829/30000 Training Loss: 0.07788083702325821\n",
      "Epoch 3830/30000 Training Loss: 0.0849149227142334\n",
      "Epoch 3830/30000 Validation Loss: 0.08044013381004333\n",
      "Epoch 3831/30000 Training Loss: 0.06436121463775635\n",
      "Epoch 3832/30000 Training Loss: 0.09558437019586563\n",
      "Epoch 3833/30000 Training Loss: 0.07790622860193253\n",
      "Epoch 3834/30000 Training Loss: 0.08785855770111084\n",
      "Epoch 3835/30000 Training Loss: 0.06802091002464294\n",
      "Epoch 3836/30000 Training Loss: 0.08503436297178268\n",
      "Epoch 3837/30000 Training Loss: 0.06079798936843872\n",
      "Epoch 3838/30000 Training Loss: 0.10200542211532593\n",
      "Epoch 3839/30000 Training Loss: 0.06811868399381638\n",
      "Epoch 3840/30000 Training Loss: 0.07827136665582657\n",
      "Epoch 3840/30000 Validation Loss: 0.11350834369659424\n",
      "Epoch 3841/30000 Training Loss: 0.07542037218809128\n",
      "Epoch 3842/30000 Training Loss: 0.0985662117600441\n",
      "Epoch 3843/30000 Training Loss: 0.07801968604326248\n",
      "Epoch 3844/30000 Training Loss: 0.08092059940099716\n",
      "Epoch 3845/30000 Training Loss: 0.07288423925638199\n",
      "Epoch 3846/30000 Training Loss: 0.07181703299283981\n",
      "Epoch 3847/30000 Training Loss: 0.08858724683523178\n",
      "Epoch 3848/30000 Training Loss: 0.09675809741020203\n",
      "Epoch 3849/30000 Training Loss: 0.08784008026123047\n",
      "Epoch 3850/30000 Training Loss: 0.08773990720510483\n",
      "Epoch 3850/30000 Validation Loss: 0.07404335588216782\n",
      "Epoch 3851/30000 Training Loss: 0.0760350301861763\n",
      "Epoch 3852/30000 Training Loss: 0.06589070707559586\n",
      "Epoch 3853/30000 Training Loss: 0.07384385913610458\n",
      "Epoch 3854/30000 Training Loss: 0.07273975014686584\n",
      "Epoch 3855/30000 Training Loss: 0.09108096361160278\n",
      "Epoch 3856/30000 Training Loss: 0.07940587401390076\n",
      "Epoch 3857/30000 Training Loss: 0.07417117804288864\n",
      "Epoch 3858/30000 Training Loss: 0.0680776834487915\n",
      "Epoch 3859/30000 Training Loss: 0.09379931539297104\n",
      "Epoch 3860/30000 Training Loss: 0.09254705160856247\n",
      "Epoch 3860/30000 Validation Loss: 0.08037400990724564\n",
      "Epoch 3861/30000 Training Loss: 0.07582921534776688\n",
      "Epoch 3862/30000 Training Loss: 0.06370320171117783\n",
      "Epoch 3863/30000 Training Loss: 0.07600780576467514\n",
      "Epoch 3864/30000 Training Loss: 0.06332089751958847\n",
      "Epoch 3865/30000 Training Loss: 0.07907145470380783\n",
      "Epoch 3866/30000 Training Loss: 0.07230592519044876\n",
      "Epoch 3867/30000 Training Loss: 0.08126982301473618\n",
      "Epoch 3868/30000 Training Loss: 0.06653277575969696\n",
      "Epoch 3869/30000 Training Loss: 0.08290604501962662\n",
      "Epoch 3870/30000 Training Loss: 0.08478745073080063\n",
      "Epoch 3870/30000 Validation Loss: 0.08399585634469986\n",
      "Epoch 3871/30000 Training Loss: 0.07658462971448898\n",
      "Epoch 3872/30000 Training Loss: 0.06544496864080429\n",
      "Epoch 3873/30000 Training Loss: 0.09506887197494507\n",
      "Epoch 3874/30000 Training Loss: 0.09679421037435532\n",
      "Epoch 3875/30000 Training Loss: 0.09551182389259338\n",
      "Epoch 3876/30000 Training Loss: 0.0811593309044838\n",
      "Epoch 3877/30000 Training Loss: 0.09193875640630722\n",
      "Epoch 3878/30000 Training Loss: 0.06258806586265564\n",
      "Epoch 3879/30000 Training Loss: 0.07058817148208618\n",
      "Epoch 3880/30000 Training Loss: 0.09384290128946304\n",
      "Epoch 3880/30000 Validation Loss: 0.07323072105646133\n",
      "Epoch 3881/30000 Training Loss: 0.08045514672994614\n",
      "Epoch 3882/30000 Training Loss: 0.09932851046323776\n",
      "Epoch 3883/30000 Training Loss: 0.06312105804681778\n",
      "Epoch 3884/30000 Training Loss: 0.06327248364686966\n",
      "Epoch 3885/30000 Training Loss: 0.09130886197090149\n",
      "Epoch 3886/30000 Training Loss: 0.08126255124807358\n",
      "Epoch 3887/30000 Training Loss: 0.06680992990732193\n",
      "Epoch 3888/30000 Training Loss: 0.07520677894353867\n",
      "Epoch 3889/30000 Training Loss: 0.06805922836065292\n",
      "Epoch 3890/30000 Training Loss: 0.08339307457208633\n",
      "Epoch 3890/30000 Validation Loss: 0.09070869535207748\n",
      "Epoch 3891/30000 Training Loss: 0.06446302682161331\n",
      "Epoch 3892/30000 Training Loss: 0.08249523490667343\n",
      "Epoch 3893/30000 Training Loss: 0.08300174027681351\n",
      "Epoch 3894/30000 Training Loss: 0.0825752317905426\n",
      "Epoch 3895/30000 Training Loss: 0.0702766701579094\n",
      "Epoch 3896/30000 Training Loss: 0.07851166278123856\n",
      "Epoch 3897/30000 Training Loss: 0.0735788643360138\n",
      "Epoch 3898/30000 Training Loss: 0.09324133396148682\n",
      "Epoch 3899/30000 Training Loss: 0.05436742305755615\n",
      "Epoch 3900/30000 Training Loss: 0.06923209875822067\n",
      "Epoch 3900/30000 Validation Loss: 0.08221594244241714\n",
      "Epoch 3901/30000 Training Loss: 0.08073919266462326\n",
      "Epoch 3902/30000 Training Loss: 0.07073891907930374\n",
      "Epoch 3903/30000 Training Loss: 0.07797790318727493\n",
      "Epoch 3904/30000 Training Loss: 0.07673751562833786\n",
      "Epoch 3905/30000 Training Loss: 0.06736179441213608\n",
      "Epoch 3906/30000 Training Loss: 0.08510548621416092\n",
      "Epoch 3907/30000 Training Loss: 0.0740862786769867\n",
      "Epoch 3908/30000 Training Loss: 0.07593873888254166\n",
      "Epoch 3909/30000 Training Loss: 0.07160482555627823\n",
      "Epoch 3910/30000 Training Loss: 0.08534546941518784\n",
      "Epoch 3910/30000 Validation Loss: 0.06514901667833328\n",
      "Epoch 3911/30000 Training Loss: 0.07576632499694824\n",
      "Epoch 3912/30000 Training Loss: 0.09270862489938736\n",
      "Epoch 3913/30000 Training Loss: 0.08039465546607971\n",
      "Epoch 3914/30000 Training Loss: 0.07144240289926529\n",
      "Epoch 3915/30000 Training Loss: 0.07854409515857697\n",
      "Epoch 3916/30000 Training Loss: 0.09041255712509155\n",
      "Epoch 3917/30000 Training Loss: 0.07048922032117844\n",
      "Epoch 3918/30000 Training Loss: 0.0994669571518898\n",
      "Epoch 3919/30000 Training Loss: 0.06659851223230362\n",
      "Epoch 3920/30000 Training Loss: 0.07926387339830399\n",
      "Epoch 3920/30000 Validation Loss: 0.08545184880495071\n",
      "Epoch 3921/30000 Training Loss: 0.07650206238031387\n",
      "Epoch 3922/30000 Training Loss: 0.0702420324087143\n",
      "Epoch 3923/30000 Training Loss: 0.09769302606582642\n",
      "Epoch 3924/30000 Training Loss: 0.07248031347990036\n",
      "Epoch 3925/30000 Training Loss: 0.0969831570982933\n",
      "Epoch 3926/30000 Training Loss: 0.07919609546661377\n",
      "Epoch 3927/30000 Training Loss: 0.08657345175743103\n",
      "Epoch 3928/30000 Training Loss: 0.08749420195817947\n",
      "Epoch 3929/30000 Training Loss: 0.09513986855745316\n",
      "Epoch 3930/30000 Training Loss: 0.062490254640579224\n",
      "Epoch 3930/30000 Validation Loss: 0.07827664166688919\n",
      "Epoch 3931/30000 Training Loss: 0.08304084092378616\n",
      "Epoch 3932/30000 Training Loss: 0.11109369993209839\n",
      "Epoch 3933/30000 Training Loss: 0.08452708274126053\n",
      "Epoch 3934/30000 Training Loss: 0.0902620181441307\n",
      "Epoch 3935/30000 Training Loss: 0.0723595917224884\n",
      "Epoch 3936/30000 Training Loss: 0.08546098321676254\n",
      "Epoch 3937/30000 Training Loss: 0.08224248141050339\n",
      "Epoch 3938/30000 Training Loss: 0.06935921311378479\n",
      "Epoch 3939/30000 Training Loss: 0.08029910176992416\n",
      "Epoch 3940/30000 Training Loss: 0.0767885148525238\n",
      "Epoch 3940/30000 Validation Loss: 0.07065768539905548\n",
      "Epoch 3941/30000 Training Loss: 0.06818845123052597\n",
      "Epoch 3942/30000 Training Loss: 0.07636761665344238\n",
      "Epoch 3943/30000 Training Loss: 0.0870288610458374\n",
      "Epoch 3944/30000 Training Loss: 0.10789644718170166\n",
      "Epoch 3945/30000 Training Loss: 0.06887835264205933\n",
      "Epoch 3946/30000 Training Loss: 0.08905263990163803\n",
      "Epoch 3947/30000 Training Loss: 0.08075761795043945\n",
      "Epoch 3948/30000 Training Loss: 0.10091999173164368\n",
      "Epoch 3949/30000 Training Loss: 0.06719281524419785\n",
      "Epoch 3950/30000 Training Loss: 0.10295506566762924\n",
      "Epoch 3950/30000 Validation Loss: 0.07513167709112167\n",
      "Epoch 3951/30000 Training Loss: 0.07950089871883392\n",
      "Epoch 3952/30000 Training Loss: 0.06175240874290466\n",
      "Epoch 3953/30000 Training Loss: 0.08691048622131348\n",
      "Epoch 3954/30000 Training Loss: 0.06256372481584549\n",
      "Epoch 3955/30000 Training Loss: 0.09840544313192368\n",
      "Epoch 3956/30000 Training Loss: 0.10708039253950119\n",
      "Epoch 3957/30000 Training Loss: 0.08858401328325272\n",
      "Epoch 3958/30000 Training Loss: 0.07237866520881653\n",
      "Epoch 3959/30000 Training Loss: 0.0647125169634819\n",
      "Epoch 3960/30000 Training Loss: 0.08371751755475998\n",
      "Epoch 3960/30000 Validation Loss: 0.09075161069631577\n",
      "Epoch 3961/30000 Training Loss: 0.06500229239463806\n",
      "Epoch 3962/30000 Training Loss: 0.10922837257385254\n",
      "Epoch 3963/30000 Training Loss: 0.07641828060150146\n",
      "Epoch 3964/30000 Training Loss: 0.06744172424077988\n",
      "Epoch 3965/30000 Training Loss: 0.07258135825395584\n",
      "Epoch 3966/30000 Training Loss: 0.07975728064775467\n",
      "Epoch 3967/30000 Training Loss: 0.07093586772680283\n",
      "Epoch 3968/30000 Training Loss: 0.08343922346830368\n",
      "Epoch 3969/30000 Training Loss: 0.0884188637137413\n",
      "Epoch 3970/30000 Training Loss: 0.08614766597747803\n",
      "Epoch 3970/30000 Validation Loss: 0.07876274734735489\n",
      "Epoch 3971/30000 Training Loss: 0.0799083262681961\n",
      "Epoch 3972/30000 Training Loss: 0.0836394652724266\n",
      "Epoch 3973/30000 Training Loss: 0.08505710959434509\n",
      "Epoch 3974/30000 Training Loss: 0.08393663913011551\n",
      "Epoch 3975/30000 Training Loss: 0.07756705582141876\n",
      "Epoch 3976/30000 Training Loss: 0.09319011121988297\n",
      "Epoch 3977/30000 Training Loss: 0.09507647156715393\n",
      "Epoch 3978/30000 Training Loss: 0.07040224969387054\n",
      "Epoch 3979/30000 Training Loss: 0.0772821307182312\n",
      "Epoch 3980/30000 Training Loss: 0.07816901803016663\n",
      "Epoch 3980/30000 Validation Loss: 0.07162830233573914\n",
      "Epoch 3981/30000 Training Loss: 0.06551917642354965\n",
      "Epoch 3982/30000 Training Loss: 0.07528723031282425\n",
      "Epoch 3983/30000 Training Loss: 0.07496003061532974\n",
      "Epoch 3984/30000 Training Loss: 0.0854586660861969\n",
      "Epoch 3985/30000 Training Loss: 0.08009584993124008\n",
      "Epoch 3986/30000 Training Loss: 0.09077206254005432\n",
      "Epoch 3987/30000 Training Loss: 0.08073785156011581\n",
      "Epoch 3988/30000 Training Loss: 0.09490537643432617\n",
      "Epoch 3989/30000 Training Loss: 0.09690950065851212\n",
      "Epoch 3990/30000 Training Loss: 0.08996039628982544\n",
      "Epoch 3990/30000 Validation Loss: 0.07881990820169449\n",
      "Epoch 3991/30000 Training Loss: 0.06276065111160278\n",
      "Epoch 3992/30000 Training Loss: 0.086105577647686\n",
      "Epoch 3993/30000 Training Loss: 0.0844753086566925\n",
      "Epoch 3994/30000 Training Loss: 0.0838543102145195\n",
      "Epoch 3995/30000 Training Loss: 0.07471267133951187\n",
      "Epoch 3996/30000 Training Loss: 0.0758146122097969\n",
      "Epoch 3997/30000 Training Loss: 0.1055128201842308\n",
      "Epoch 3998/30000 Training Loss: 0.06596244126558304\n",
      "Epoch 3999/30000 Training Loss: 0.07036453485488892\n",
      "Epoch 4000/30000 Training Loss: 0.08485303074121475\n",
      "Epoch 4000/30000 Validation Loss: 0.0660054087638855\n",
      "Epoch 4001/30000 Training Loss: 0.079366035759449\n",
      "Epoch 4002/30000 Training Loss: 0.08500862121582031\n",
      "Epoch 4003/30000 Training Loss: 0.06472783535718918\n",
      "Epoch 4004/30000 Training Loss: 0.08269230276346207\n",
      "Epoch 4005/30000 Training Loss: 0.07325030118227005\n",
      "Epoch 4006/30000 Training Loss: 0.10079243779182434\n",
      "Epoch 4007/30000 Training Loss: 0.06296899914741516\n",
      "Epoch 4008/30000 Training Loss: 0.07316749542951584\n",
      "Epoch 4009/30000 Training Loss: 0.06791356950998306\n",
      "Epoch 4010/30000 Training Loss: 0.09555327147245407\n",
      "Epoch 4010/30000 Validation Loss: 0.07572397589683533\n",
      "Epoch 4011/30000 Training Loss: 0.07876460999250412\n",
      "Epoch 4012/30000 Training Loss: 0.08886925131082535\n",
      "Epoch 4013/30000 Training Loss: 0.07876195758581161\n",
      "Epoch 4014/30000 Training Loss: 0.06209149956703186\n",
      "Epoch 4015/30000 Training Loss: 0.09973090142011642\n",
      "Epoch 4016/30000 Training Loss: 0.08533793687820435\n",
      "Epoch 4017/30000 Training Loss: 0.09372717142105103\n",
      "Epoch 4018/30000 Training Loss: 0.08977481722831726\n",
      "Epoch 4019/30000 Training Loss: 0.06866297870874405\n",
      "Epoch 4020/30000 Training Loss: 0.07834642380475998\n",
      "Epoch 4020/30000 Validation Loss: 0.08152757585048676\n",
      "Epoch 4021/30000 Training Loss: 0.07006973028182983\n",
      "Epoch 4022/30000 Training Loss: 0.08775144070386887\n",
      "Epoch 4023/30000 Training Loss: 0.06877776980400085\n",
      "Epoch 4024/30000 Training Loss: 0.10160770267248154\n",
      "Epoch 4025/30000 Training Loss: 0.08780699223279953\n",
      "Epoch 4026/30000 Training Loss: 0.07819805294275284\n",
      "Epoch 4027/30000 Training Loss: 0.06759156286716461\n",
      "Epoch 4028/30000 Training Loss: 0.07803013920783997\n",
      "Epoch 4029/30000 Training Loss: 0.06355925649404526\n",
      "Epoch 4030/30000 Training Loss: 0.08972393721342087\n",
      "Epoch 4030/30000 Validation Loss: 0.0865403488278389\n",
      "Epoch 4031/30000 Training Loss: 0.08078107982873917\n",
      "Epoch 4032/30000 Training Loss: 0.08143633604049683\n",
      "Epoch 4033/30000 Training Loss: 0.08833250403404236\n",
      "Epoch 4034/30000 Training Loss: 0.08913958817720413\n",
      "Epoch 4035/30000 Training Loss: 0.06523545831441879\n",
      "Epoch 4036/30000 Training Loss: 0.0824771299958229\n",
      "Epoch 4037/30000 Training Loss: 0.10435434430837631\n",
      "Epoch 4038/30000 Training Loss: 0.07080024480819702\n",
      "Epoch 4039/30000 Training Loss: 0.06095061078667641\n",
      "Epoch 4040/30000 Training Loss: 0.07789960503578186\n",
      "Epoch 4040/30000 Validation Loss: 0.08863814920186996\n",
      "Epoch 4041/30000 Training Loss: 0.07843828946352005\n",
      "Epoch 4042/30000 Training Loss: 0.09069835394620895\n",
      "Epoch 4043/30000 Training Loss: 0.08284495025873184\n",
      "Epoch 4044/30000 Training Loss: 0.09720557928085327\n",
      "Epoch 4045/30000 Training Loss: 0.08619702607393265\n",
      "Epoch 4046/30000 Training Loss: 0.06740835309028625\n",
      "Epoch 4047/30000 Training Loss: 0.06903671473264694\n",
      "Epoch 4048/30000 Training Loss: 0.0827087014913559\n",
      "Epoch 4049/30000 Training Loss: 0.07050446420907974\n",
      "Epoch 4050/30000 Training Loss: 0.06913100183010101\n",
      "Epoch 4050/30000 Validation Loss: 0.08840316534042358\n",
      "Epoch 4051/30000 Training Loss: 0.0877801775932312\n",
      "Epoch 4052/30000 Training Loss: 0.08433427661657333\n",
      "Epoch 4053/30000 Training Loss: 0.06904473155736923\n",
      "Epoch 4054/30000 Training Loss: 0.05787195637822151\n",
      "Epoch 4055/30000 Training Loss: 0.0679943636059761\n",
      "Epoch 4056/30000 Training Loss: 0.08949882537126541\n",
      "Epoch 4057/30000 Training Loss: 0.07471125572919846\n",
      "Epoch 4058/30000 Training Loss: 0.0706106498837471\n",
      "Epoch 4059/30000 Training Loss: 0.11229684948921204\n",
      "Epoch 4060/30000 Training Loss: 0.08476323634386063\n",
      "Epoch 4060/30000 Validation Loss: 0.07135000079870224\n",
      "Epoch 4061/30000 Training Loss: 0.08216338604688644\n",
      "Epoch 4062/30000 Training Loss: 0.07712829858064651\n",
      "Epoch 4063/30000 Training Loss: 0.06388625502586365\n",
      "Epoch 4064/30000 Training Loss: 0.08682183176279068\n",
      "Epoch 4065/30000 Training Loss: 0.1160375252366066\n",
      "Epoch 4066/30000 Training Loss: 0.08096522837877274\n",
      "Epoch 4067/30000 Training Loss: 0.07106661796569824\n",
      "Epoch 4068/30000 Training Loss: 0.08880455046892166\n",
      "Epoch 4069/30000 Training Loss: 0.08764385432004929\n",
      "Epoch 4070/30000 Training Loss: 0.09063827991485596\n",
      "Epoch 4070/30000 Validation Loss: 0.07897298783063889\n",
      "Epoch 4071/30000 Training Loss: 0.06550664454698563\n",
      "Epoch 4072/30000 Training Loss: 0.08541321009397507\n",
      "Epoch 4073/30000 Training Loss: 0.07916364818811417\n",
      "Epoch 4074/30000 Training Loss: 0.08313260227441788\n",
      "Epoch 4075/30000 Training Loss: 0.07291775196790695\n",
      "Epoch 4076/30000 Training Loss: 0.08741048723459244\n",
      "Epoch 4077/30000 Training Loss: 0.07418356090784073\n",
      "Epoch 4078/30000 Training Loss: 0.09069962054491043\n",
      "Epoch 4079/30000 Training Loss: 0.061306700110435486\n",
      "Epoch 4080/30000 Training Loss: 0.0631197839975357\n",
      "Epoch 4080/30000 Validation Loss: 0.08448567986488342\n",
      "Epoch 4081/30000 Training Loss: 0.06875262409448624\n",
      "Epoch 4082/30000 Training Loss: 0.07718371599912643\n",
      "Epoch 4083/30000 Training Loss: 0.08393251895904541\n",
      "Epoch 4084/30000 Training Loss: 0.08322545886039734\n",
      "Epoch 4085/30000 Training Loss: 0.08515951782464981\n",
      "Epoch 4086/30000 Training Loss: 0.0981668010354042\n",
      "Epoch 4087/30000 Training Loss: 0.06874370574951172\n",
      "Epoch 4088/30000 Training Loss: 0.06043560430407524\n",
      "Epoch 4089/30000 Training Loss: 0.08206552267074585\n",
      "Epoch 4090/30000 Training Loss: 0.062451209872961044\n",
      "Epoch 4090/30000 Validation Loss: 0.09486506134271622\n",
      "Epoch 4091/30000 Training Loss: 0.09096431732177734\n",
      "Epoch 4092/30000 Training Loss: 0.06155290827155113\n",
      "Epoch 4093/30000 Training Loss: 0.07995215803384781\n",
      "Epoch 4094/30000 Training Loss: 0.06285920739173889\n",
      "Epoch 4095/30000 Training Loss: 0.11989674717187881\n",
      "Epoch 4096/30000 Training Loss: 0.1041523739695549\n",
      "Epoch 4097/30000 Training Loss: 0.08728078752756119\n",
      "Epoch 4098/30000 Training Loss: 0.0651393011212349\n",
      "Epoch 4099/30000 Training Loss: 0.06375693529844284\n",
      "Epoch 4100/30000 Training Loss: 0.10212496668100357\n",
      "Epoch 4100/30000 Validation Loss: 0.09537167102098465\n",
      "Epoch 4101/30000 Training Loss: 0.09497782588005066\n",
      "Epoch 4102/30000 Training Loss: 0.06530890613794327\n",
      "Epoch 4103/30000 Training Loss: 0.069889597594738\n",
      "Epoch 4104/30000 Training Loss: 0.09551810473203659\n",
      "Epoch 4105/30000 Training Loss: 0.07054264098405838\n",
      "Epoch 4106/30000 Training Loss: 0.0913606584072113\n",
      "Epoch 4107/30000 Training Loss: 0.10305380821228027\n",
      "Epoch 4108/30000 Training Loss: 0.06254496425390244\n",
      "Epoch 4109/30000 Training Loss: 0.0706467553973198\n",
      "Epoch 4110/30000 Training Loss: 0.0788763239979744\n",
      "Epoch 4110/30000 Validation Loss: 0.07902602106332779\n",
      "Epoch 4111/30000 Training Loss: 0.06526845693588257\n",
      "Epoch 4112/30000 Training Loss: 0.07644283771514893\n",
      "Epoch 4113/30000 Training Loss: 0.06490984559059143\n",
      "Epoch 4114/30000 Training Loss: 0.09896742552518845\n",
      "Epoch 4115/30000 Training Loss: 0.06786932796239853\n",
      "Epoch 4116/30000 Training Loss: 0.07336920499801636\n",
      "Epoch 4117/30000 Training Loss: 0.08021146059036255\n",
      "Epoch 4118/30000 Training Loss: 0.07069537043571472\n",
      "Epoch 4119/30000 Training Loss: 0.0859188437461853\n",
      "Epoch 4120/30000 Training Loss: 0.09967003017663956\n",
      "Epoch 4120/30000 Validation Loss: 0.0746515616774559\n",
      "Epoch 4121/30000 Training Loss: 0.09754516929388046\n",
      "Epoch 4122/30000 Training Loss: 0.08192402869462967\n",
      "Epoch 4123/30000 Training Loss: 0.06167798861861229\n",
      "Epoch 4124/30000 Training Loss: 0.07746893912553787\n",
      "Epoch 4125/30000 Training Loss: 0.09192243963479996\n",
      "Epoch 4126/30000 Training Loss: 0.08588459342718124\n",
      "Epoch 4127/30000 Training Loss: 0.06028557941317558\n",
      "Epoch 4128/30000 Training Loss: 0.07716792821884155\n",
      "Epoch 4129/30000 Training Loss: 0.08561436086893082\n",
      "Epoch 4130/30000 Training Loss: 0.1024598702788353\n",
      "Epoch 4130/30000 Validation Loss: 0.07444453239440918\n",
      "Epoch 4131/30000 Training Loss: 0.08229568600654602\n",
      "Epoch 4132/30000 Training Loss: 0.0846831426024437\n",
      "Epoch 4133/30000 Training Loss: 0.08452126383781433\n",
      "Epoch 4134/30000 Training Loss: 0.10665285587310791\n",
      "Epoch 4135/30000 Training Loss: 0.09738316386938095\n",
      "Epoch 4136/30000 Training Loss: 0.07738545536994934\n",
      "Epoch 4137/30000 Training Loss: 0.06624244898557663\n",
      "Epoch 4138/30000 Training Loss: 0.09225054830312729\n",
      "Epoch 4139/30000 Training Loss: 0.08521992713212967\n",
      "Epoch 4140/30000 Training Loss: 0.0930488184094429\n",
      "Epoch 4140/30000 Validation Loss: 0.08290424197912216\n",
      "Epoch 4141/30000 Training Loss: 0.07082903385162354\n",
      "Epoch 4142/30000 Training Loss: 0.060952138155698776\n",
      "Epoch 4143/30000 Training Loss: 0.07627493888139725\n",
      "Epoch 4144/30000 Training Loss: 0.07346995919942856\n",
      "Epoch 4145/30000 Training Loss: 0.08359310775995255\n",
      "Epoch 4146/30000 Training Loss: 0.09461130946874619\n",
      "Epoch 4147/30000 Training Loss: 0.10447170585393906\n",
      "Epoch 4148/30000 Training Loss: 0.0845605656504631\n",
      "Epoch 4149/30000 Training Loss: 0.0661398395895958\n",
      "Epoch 4150/30000 Training Loss: 0.07664280384778976\n",
      "Epoch 4150/30000 Validation Loss: 0.0841711238026619\n",
      "Epoch 4151/30000 Training Loss: 0.10595149546861649\n",
      "Epoch 4152/30000 Training Loss: 0.06264685094356537\n",
      "Epoch 4153/30000 Training Loss: 0.10163145512342453\n",
      "Epoch 4154/30000 Training Loss: 0.06189088523387909\n",
      "Epoch 4155/30000 Training Loss: 0.07141318917274475\n",
      "Epoch 4156/30000 Training Loss: 0.07202497124671936\n",
      "Epoch 4157/30000 Training Loss: 0.059905678033828735\n",
      "Epoch 4158/30000 Training Loss: 0.08339327573776245\n",
      "Epoch 4159/30000 Training Loss: 0.09856102615594864\n",
      "Epoch 4160/30000 Training Loss: 0.10227357596158981\n",
      "Epoch 4160/30000 Validation Loss: 0.07015655189752579\n",
      "Epoch 4161/30000 Training Loss: 0.08034329861402512\n",
      "Epoch 4162/30000 Training Loss: 0.06757191568613052\n",
      "Epoch 4163/30000 Training Loss: 0.09113989025354385\n",
      "Epoch 4164/30000 Training Loss: 0.08414444327354431\n",
      "Epoch 4165/30000 Training Loss: 0.0882851779460907\n",
      "Epoch 4166/30000 Training Loss: 0.09226997941732407\n",
      "Epoch 4167/30000 Training Loss: 0.06919949501752853\n",
      "Epoch 4168/30000 Training Loss: 0.09663105010986328\n",
      "Epoch 4169/30000 Training Loss: 0.08742311596870422\n",
      "Epoch 4170/30000 Training Loss: 0.10334950685501099\n",
      "Epoch 4170/30000 Validation Loss: 0.07461429387331009\n",
      "Epoch 4171/30000 Training Loss: 0.08650568127632141\n",
      "Epoch 4172/30000 Training Loss: 0.07717756181955338\n",
      "Epoch 4173/30000 Training Loss: 0.0859607681632042\n",
      "Epoch 4174/30000 Training Loss: 0.08188673108816147\n",
      "Epoch 4175/30000 Training Loss: 0.0871102586388588\n",
      "Epoch 4176/30000 Training Loss: 0.08652373403310776\n",
      "Epoch 4177/30000 Training Loss: 0.05836043134331703\n",
      "Epoch 4178/30000 Training Loss: 0.07382043451070786\n",
      "Epoch 4179/30000 Training Loss: 0.05811071768403053\n",
      "Epoch 4180/30000 Training Loss: 0.0750015452504158\n",
      "Epoch 4180/30000 Validation Loss: 0.09012732654809952\n",
      "Epoch 4181/30000 Training Loss: 0.08461139351129532\n",
      "Epoch 4182/30000 Training Loss: 0.09763625264167786\n",
      "Epoch 4183/30000 Training Loss: 0.08192357420921326\n",
      "Epoch 4184/30000 Training Loss: 0.06584268063306808\n",
      "Epoch 4185/30000 Training Loss: 0.06781400740146637\n",
      "Epoch 4186/30000 Training Loss: 0.0710739865899086\n",
      "Epoch 4187/30000 Training Loss: 0.06333199143409729\n",
      "Epoch 4188/30000 Training Loss: 0.07712648063898087\n",
      "Epoch 4189/30000 Training Loss: 0.07465788722038269\n",
      "Epoch 4190/30000 Training Loss: 0.08819767087697983\n",
      "Epoch 4190/30000 Validation Loss: 0.07296023517847061\n",
      "Epoch 4191/30000 Training Loss: 0.08322551846504211\n",
      "Epoch 4192/30000 Training Loss: 0.06246432289481163\n",
      "Epoch 4193/30000 Training Loss: 0.09714169055223465\n",
      "Epoch 4194/30000 Training Loss: 0.096415214240551\n",
      "Epoch 4195/30000 Training Loss: 0.0766761377453804\n",
      "Epoch 4196/30000 Training Loss: 0.06408434361219406\n",
      "Epoch 4197/30000 Training Loss: 0.08419276028871536\n",
      "Epoch 4198/30000 Training Loss: 0.08025941997766495\n",
      "Epoch 4199/30000 Training Loss: 0.07149354368448257\n",
      "Epoch 4200/30000 Training Loss: 0.07758703082799911\n",
      "Epoch 4200/30000 Validation Loss: 0.0667693018913269\n",
      "Epoch 4201/30000 Training Loss: 0.08180264383554459\n",
      "Epoch 4202/30000 Training Loss: 0.06877896189689636\n",
      "Epoch 4203/30000 Training Loss: 0.06977903842926025\n",
      "Epoch 4204/30000 Training Loss: 0.11959478259086609\n",
      "Epoch 4205/30000 Training Loss: 0.08107688277959824\n",
      "Epoch 4206/30000 Training Loss: 0.0735679492354393\n",
      "Epoch 4207/30000 Training Loss: 0.08091818541288376\n",
      "Epoch 4208/30000 Training Loss: 0.07379721850156784\n",
      "Epoch 4209/30000 Training Loss: 0.07903067022562027\n",
      "Epoch 4210/30000 Training Loss: 0.07965948432683945\n",
      "Epoch 4210/30000 Validation Loss: 0.09223269671201706\n",
      "Epoch 4211/30000 Training Loss: 0.09908578544855118\n",
      "Epoch 4212/30000 Training Loss: 0.06404217332601547\n",
      "Epoch 4213/30000 Training Loss: 0.08014734834432602\n",
      "Epoch 4214/30000 Training Loss: 0.06721382588148117\n",
      "Epoch 4215/30000 Training Loss: 0.0826270654797554\n",
      "Epoch 4216/30000 Training Loss: 0.06758090108633041\n",
      "Epoch 4217/30000 Training Loss: 0.0785113051533699\n",
      "Epoch 4218/30000 Training Loss: 0.07627952843904495\n",
      "Epoch 4219/30000 Training Loss: 0.05778081715106964\n",
      "Epoch 4220/30000 Training Loss: 0.05763999745249748\n",
      "Epoch 4220/30000 Validation Loss: 0.07875379920005798\n",
      "Epoch 4221/30000 Training Loss: 0.07789860665798187\n",
      "Epoch 4222/30000 Training Loss: 0.07897359132766724\n",
      "Epoch 4223/30000 Training Loss: 0.07217445224523544\n",
      "Epoch 4224/30000 Training Loss: 0.08347504585981369\n",
      "Epoch 4225/30000 Training Loss: 0.06568152457475662\n",
      "Epoch 4226/30000 Training Loss: 0.08013146370649338\n",
      "Epoch 4227/30000 Training Loss: 0.08587253093719482\n",
      "Epoch 4228/30000 Training Loss: 0.09419862180948257\n",
      "Epoch 4229/30000 Training Loss: 0.07112497091293335\n",
      "Epoch 4230/30000 Training Loss: 0.06324915587902069\n",
      "Epoch 4230/30000 Validation Loss: 0.06384392827749252\n",
      "Epoch 4231/30000 Training Loss: 0.08672719448804855\n",
      "Epoch 4232/30000 Training Loss: 0.09042022377252579\n",
      "Epoch 4233/30000 Training Loss: 0.06022325158119202\n",
      "Epoch 4234/30000 Training Loss: 0.061600521206855774\n",
      "Epoch 4235/30000 Training Loss: 0.06618252396583557\n",
      "Epoch 4236/30000 Training Loss: 0.05673536658287048\n",
      "Epoch 4237/30000 Training Loss: 0.09342864155769348\n",
      "Epoch 4238/30000 Training Loss: 0.06726830452680588\n",
      "Epoch 4239/30000 Training Loss: 0.08859135955572128\n",
      "Epoch 4240/30000 Training Loss: 0.09957725554704666\n",
      "Epoch 4240/30000 Validation Loss: 0.06360121816396713\n",
      "Epoch 4241/30000 Training Loss: 0.0721394494175911\n",
      "Epoch 4242/30000 Training Loss: 0.0798272117972374\n",
      "Epoch 4243/30000 Training Loss: 0.10383105278015137\n",
      "Epoch 4244/30000 Training Loss: 0.07284942269325256\n",
      "Epoch 4245/30000 Training Loss: 0.08555519580841064\n",
      "Epoch 4246/30000 Training Loss: 0.10462454706430435\n",
      "Epoch 4247/30000 Training Loss: 0.07384029775857925\n",
      "Epoch 4248/30000 Training Loss: 0.06858867406845093\n",
      "Epoch 4249/30000 Training Loss: 0.06981830298900604\n",
      "Epoch 4250/30000 Training Loss: 0.07551798969507217\n",
      "Epoch 4250/30000 Validation Loss: 0.06284814327955246\n",
      "Epoch 4251/30000 Training Loss: 0.08934793621301651\n",
      "Epoch 4252/30000 Training Loss: 0.0678761824965477\n",
      "Epoch 4253/30000 Training Loss: 0.06569952517747879\n",
      "Epoch 4254/30000 Training Loss: 0.07685586810112\n",
      "Epoch 4255/30000 Training Loss: 0.06524017453193665\n",
      "Epoch 4256/30000 Training Loss: 0.1116483211517334\n",
      "Epoch 4257/30000 Training Loss: 0.06578827649354935\n",
      "Epoch 4258/30000 Training Loss: 0.06348884105682373\n",
      "Epoch 4259/30000 Training Loss: 0.07365954667329788\n",
      "Epoch 4260/30000 Training Loss: 0.0972917303442955\n",
      "Epoch 4260/30000 Validation Loss: 0.07318904250860214\n",
      "Epoch 4261/30000 Training Loss: 0.09595225006341934\n",
      "Epoch 4262/30000 Training Loss: 0.06457359343767166\n",
      "Epoch 4263/30000 Training Loss: 0.06330087780952454\n",
      "Epoch 4264/30000 Training Loss: 0.0675630047917366\n",
      "Epoch 4265/30000 Training Loss: 0.08757048100233078\n",
      "Epoch 4266/30000 Training Loss: 0.05370812490582466\n",
      "Epoch 4267/30000 Training Loss: 0.06115793064236641\n",
      "Epoch 4268/30000 Training Loss: 0.07345884293317795\n",
      "Epoch 4269/30000 Training Loss: 0.07662332057952881\n",
      "Epoch 4270/30000 Training Loss: 0.09419131278991699\n",
      "Epoch 4270/30000 Validation Loss: 0.0827564224600792\n",
      "Epoch 4271/30000 Training Loss: 0.08451075106859207\n",
      "Epoch 4272/30000 Training Loss: 0.06921003758907318\n",
      "Epoch 4273/30000 Training Loss: 0.07054079324007034\n",
      "Epoch 4274/30000 Training Loss: 0.06420943140983582\n",
      "Epoch 4275/30000 Training Loss: 0.0784342959523201\n",
      "Epoch 4276/30000 Training Loss: 0.0639900341629982\n",
      "Epoch 4277/30000 Training Loss: 0.08752990514039993\n",
      "Epoch 4278/30000 Training Loss: 0.08979327231645584\n",
      "Epoch 4279/30000 Training Loss: 0.09025587886571884\n",
      "Epoch 4280/30000 Training Loss: 0.0771302580833435\n",
      "Epoch 4280/30000 Validation Loss: 0.07483462244272232\n",
      "Epoch 4281/30000 Training Loss: 0.07124567031860352\n",
      "Epoch 4282/30000 Training Loss: 0.08332569897174835\n",
      "Epoch 4283/30000 Training Loss: 0.07196556776762009\n",
      "Epoch 4284/30000 Training Loss: 0.06959401816129684\n",
      "Epoch 4285/30000 Training Loss: 0.08259176462888718\n",
      "Epoch 4286/30000 Training Loss: 0.06189993396401405\n",
      "Epoch 4287/30000 Training Loss: 0.10476094484329224\n",
      "Epoch 4288/30000 Training Loss: 0.09673464298248291\n",
      "Epoch 4289/30000 Training Loss: 0.06777439266443253\n",
      "Epoch 4290/30000 Training Loss: 0.07540195435285568\n",
      "Epoch 4290/30000 Validation Loss: 0.0890164002776146\n",
      "Epoch 4291/30000 Training Loss: 0.08577445149421692\n",
      "Epoch 4292/30000 Training Loss: 0.06999752670526505\n",
      "Epoch 4293/30000 Training Loss: 0.08308318257331848\n",
      "Epoch 4294/30000 Training Loss: 0.08271118253469467\n",
      "Epoch 4295/30000 Training Loss: 0.08348920196294785\n",
      "Epoch 4296/30000 Training Loss: 0.10272234678268433\n",
      "Epoch 4297/30000 Training Loss: 0.07414329051971436\n",
      "Epoch 4298/30000 Training Loss: 0.09158841520547867\n",
      "Epoch 4299/30000 Training Loss: 0.06923378258943558\n",
      "Epoch 4300/30000 Training Loss: 0.07281878590583801\n",
      "Epoch 4300/30000 Validation Loss: 0.06239272654056549\n",
      "Epoch 4301/30000 Training Loss: 0.0797361209988594\n",
      "Epoch 4302/30000 Training Loss: 0.06525614857673645\n",
      "Epoch 4303/30000 Training Loss: 0.07763192057609558\n",
      "Epoch 4304/30000 Training Loss: 0.09098219126462936\n",
      "Epoch 4305/30000 Training Loss: 0.06429403275251389\n",
      "Epoch 4306/30000 Training Loss: 0.09480971097946167\n",
      "Epoch 4307/30000 Training Loss: 0.09959856420755386\n",
      "Epoch 4308/30000 Training Loss: 0.10464426130056381\n",
      "Epoch 4309/30000 Training Loss: 0.07183859497308731\n",
      "Epoch 4310/30000 Training Loss: 0.07094985991716385\n",
      "Epoch 4310/30000 Validation Loss: 0.08759700506925583\n",
      "Epoch 4311/30000 Training Loss: 0.11662264913320541\n",
      "Epoch 4312/30000 Training Loss: 0.07039333134889603\n",
      "Epoch 4313/30000 Training Loss: 0.09343834966421127\n",
      "Epoch 4314/30000 Training Loss: 0.07717368006706238\n",
      "Epoch 4315/30000 Training Loss: 0.07459206134080887\n",
      "Epoch 4316/30000 Training Loss: 0.08761843293905258\n",
      "Epoch 4317/30000 Training Loss: 0.08078611642122269\n",
      "Epoch 4318/30000 Training Loss: 0.07373878359794617\n",
      "Epoch 4319/30000 Training Loss: 0.09100973606109619\n",
      "Epoch 4320/30000 Training Loss: 0.08662565797567368\n",
      "Epoch 4320/30000 Validation Loss: 0.07830366492271423\n",
      "Epoch 4321/30000 Training Loss: 0.08667268604040146\n",
      "Epoch 4322/30000 Training Loss: 0.07054241746664047\n",
      "Epoch 4323/30000 Training Loss: 0.092445969581604\n",
      "Epoch 4324/30000 Training Loss: 0.08258674293756485\n",
      "Epoch 4325/30000 Training Loss: 0.06872159987688065\n",
      "Epoch 4326/30000 Training Loss: 0.06160804256796837\n",
      "Epoch 4327/30000 Training Loss: 0.06035294756293297\n",
      "Epoch 4328/30000 Training Loss: 0.08760564774274826\n",
      "Epoch 4329/30000 Training Loss: 0.055985402315855026\n",
      "Epoch 4330/30000 Training Loss: 0.07508229464292526\n",
      "Epoch 4330/30000 Validation Loss: 0.09178841859102249\n",
      "Epoch 4331/30000 Training Loss: 0.08692321926355362\n",
      "Epoch 4332/30000 Training Loss: 0.08425593376159668\n",
      "Epoch 4333/30000 Training Loss: 0.0807529017329216\n",
      "Epoch 4334/30000 Training Loss: 0.060776472091674805\n",
      "Epoch 4335/30000 Training Loss: 0.061983320862054825\n",
      "Epoch 4336/30000 Training Loss: 0.06510009616613388\n",
      "Epoch 4337/30000 Training Loss: 0.06823858618736267\n",
      "Epoch 4338/30000 Training Loss: 0.07112014293670654\n",
      "Epoch 4339/30000 Training Loss: 0.0426655188202858\n",
      "Epoch 4340/30000 Training Loss: 0.08413967490196228\n",
      "Epoch 4340/30000 Validation Loss: 0.08940277248620987\n",
      "Epoch 4341/30000 Training Loss: 0.06618037074804306\n",
      "Epoch 4342/30000 Training Loss: 0.10172318667173386\n",
      "Epoch 4343/30000 Training Loss: 0.07510486245155334\n",
      "Epoch 4344/30000 Training Loss: 0.08896482735872269\n",
      "Epoch 4345/30000 Training Loss: 0.07283128052949905\n",
      "Epoch 4346/30000 Training Loss: 0.08076538145542145\n",
      "Epoch 4347/30000 Training Loss: 0.09278374910354614\n",
      "Epoch 4348/30000 Training Loss: 0.06844797730445862\n",
      "Epoch 4349/30000 Training Loss: 0.08365704864263535\n",
      "Epoch 4350/30000 Training Loss: 0.08318915218114853\n",
      "Epoch 4350/30000 Validation Loss: 0.08557069301605225\n",
      "Epoch 4351/30000 Training Loss: 0.07440266758203506\n",
      "Epoch 4352/30000 Training Loss: 0.08788886666297913\n",
      "Epoch 4353/30000 Training Loss: 0.054254237562417984\n",
      "Epoch 4354/30000 Training Loss: 0.07227859646081924\n",
      "Epoch 4355/30000 Training Loss: 0.07204056531190872\n",
      "Epoch 4356/30000 Training Loss: 0.0695483461022377\n",
      "Epoch 4357/30000 Training Loss: 0.07615581154823303\n",
      "Epoch 4358/30000 Training Loss: 0.06423208117485046\n",
      "Epoch 4359/30000 Training Loss: 0.08338280767202377\n",
      "Epoch 4360/30000 Training Loss: 0.07660456746816635\n",
      "Epoch 4360/30000 Validation Loss: 0.07151389867067337\n",
      "Epoch 4361/30000 Training Loss: 0.07503270357847214\n",
      "Epoch 4362/30000 Training Loss: 0.07429300993680954\n",
      "Epoch 4363/30000 Training Loss: 0.08295450359582901\n",
      "Epoch 4364/30000 Training Loss: 0.05828048661351204\n",
      "Epoch 4365/30000 Training Loss: 0.07630123943090439\n",
      "Epoch 4366/30000 Training Loss: 0.09255903959274292\n",
      "Epoch 4367/30000 Training Loss: 0.06800823658704758\n",
      "Epoch 4368/30000 Training Loss: 0.0689261257648468\n",
      "Epoch 4369/30000 Training Loss: 0.06770787388086319\n",
      "Epoch 4370/30000 Training Loss: 0.08426201343536377\n",
      "Epoch 4370/30000 Validation Loss: 0.08039408177137375\n",
      "Epoch 4371/30000 Training Loss: 0.07260864973068237\n",
      "Epoch 4372/30000 Training Loss: 0.09644191712141037\n",
      "Epoch 4373/30000 Training Loss: 0.06380423903465271\n",
      "Epoch 4374/30000 Training Loss: 0.09794463962316513\n",
      "Epoch 4375/30000 Training Loss: 0.04975856468081474\n",
      "Epoch 4376/30000 Training Loss: 0.08396483212709427\n",
      "Epoch 4377/30000 Training Loss: 0.07558926939964294\n",
      "Epoch 4378/30000 Training Loss: 0.06814542412757874\n",
      "Epoch 4379/30000 Training Loss: 0.05934678390622139\n",
      "Epoch 4380/30000 Training Loss: 0.08475720882415771\n",
      "Epoch 4380/30000 Validation Loss: 0.06584035605192184\n",
      "Epoch 4381/30000 Training Loss: 0.08709073811769485\n",
      "Epoch 4382/30000 Training Loss: 0.07442626357078552\n",
      "Epoch 4383/30000 Training Loss: 0.06800814718008041\n",
      "Epoch 4384/30000 Training Loss: 0.06738341599702835\n",
      "Epoch 4385/30000 Training Loss: 0.10100453346967697\n",
      "Epoch 4386/30000 Training Loss: 0.08446834236383438\n",
      "Epoch 4387/30000 Training Loss: 0.07320930063724518\n",
      "Epoch 4388/30000 Training Loss: 0.0614042766392231\n",
      "Epoch 4389/30000 Training Loss: 0.07219857722520828\n",
      "Epoch 4390/30000 Training Loss: 0.08891239762306213\n",
      "Epoch 4390/30000 Validation Loss: 0.07415079325437546\n",
      "Epoch 4391/30000 Training Loss: 0.07850843667984009\n",
      "Epoch 4392/30000 Training Loss: 0.07282049208879471\n",
      "Epoch 4393/30000 Training Loss: 0.06243528425693512\n",
      "Epoch 4394/30000 Training Loss: 0.0834977924823761\n",
      "Epoch 4395/30000 Training Loss: 0.07273833453655243\n",
      "Epoch 4396/30000 Training Loss: 0.07776329666376114\n",
      "Epoch 4397/30000 Training Loss: 0.07115494459867477\n",
      "Epoch 4398/30000 Training Loss: 0.07906263321638107\n",
      "Epoch 4399/30000 Training Loss: 0.06431431323289871\n",
      "Epoch 4400/30000 Training Loss: 0.0875091552734375\n",
      "Epoch 4400/30000 Validation Loss: 0.08409064263105392\n",
      "Epoch 4401/30000 Training Loss: 0.08899444341659546\n",
      "Epoch 9440/30000 Training Loss: 0.07768521457910538\n",
      "Epoch 9440/30000 Validation Loss: 0.05858827009797096\n",
      "Epoch 9441/30000 Training Loss: 0.08291543275117874\n",
      "Epoch 9442/30000 Training Loss: 0.07948306947946548\n",
      "Epoch 9443/30000 Training Loss: 0.0845305323600769\n",
      "Epoch 9444/30000 Training Loss: 0.0708475187420845\n",
      "Epoch 9445/30000 Training Loss: 0.07410622388124466\n",
      "Epoch 9446/30000 Training Loss: 0.05716180428862572\n",
      "Epoch 9447/30000 Training Loss: 0.06320801377296448\n",
      "Epoch 9448/30000 Training Loss: 0.08715973049402237\n",
      "Epoch 9449/30000 Training Loss: 0.09062358736991882\n",
      "Epoch 9450/30000 Training Loss: 0.06605922430753708\n",
      "Epoch 9450/30000 Validation Loss: 0.06805006414651871\n",
      "Epoch 9451/30000 Training Loss: 0.07681096345186234\n",
      "Epoch 9452/30000 Training Loss: 0.05337383225560188\n",
      "Epoch 9453/30000 Training Loss: 0.07219990342855453\n",
      "Epoch 9454/30000 Training Loss: 0.06734862923622131\n",
      "Epoch 9455/30000 Training Loss: 0.06843248754739761\n",
      "Epoch 9456/30000 Training Loss: 0.0720391646027565\n",
      "Epoch 9457/30000 Training Loss: 0.06997551769018173\n",
      "Epoch 9458/30000 Training Loss: 0.06847139447927475\n",
      "Epoch 9459/30000 Training Loss: 0.0634092465043068\n",
      "Epoch 9460/30000 Training Loss: 0.06226566433906555\n",
      "Epoch 9460/30000 Validation Loss: 0.08523645251989365\n",
      "Epoch 9461/30000 Training Loss: 0.08391249179840088\n",
      "Epoch 9462/30000 Training Loss: 0.07982187718153\n",
      "Epoch 9463/30000 Training Loss: 0.05711420252919197\n",
      "Epoch 9464/30000 Training Loss: 0.0666315108537674\n",
      "Epoch 9465/30000 Training Loss: 0.060459207743406296\n",
      "Epoch 9466/30000 Training Loss: 0.07536997646093369\n",
      "Epoch 9467/30000 Training Loss: 0.06892416626214981\n",
      "Epoch 9468/30000 Training Loss: 0.07359502464532852\n",
      "Epoch 9469/30000 Training Loss: 0.05878357216715813\n",
      "Epoch 9470/30000 Training Loss: 0.05367737635970116\n",
      "Epoch 9470/30000 Validation Loss: 0.06693436950445175\n",
      "Epoch 9471/30000 Training Loss: 0.055547941476106644\n",
      "Epoch 9472/30000 Training Loss: 0.0715857520699501\n",
      "Epoch 9473/30000 Training Loss: 0.10834521800279617\n",
      "Epoch 9474/30000 Training Loss: 0.06419320404529572\n",
      "Epoch 9475/30000 Training Loss: 0.07600492984056473\n",
      "Epoch 9476/30000 Training Loss: 0.05978934466838837\n",
      "Epoch 9477/30000 Training Loss: 0.06936254352331161\n",
      "Epoch 9478/30000 Training Loss: 0.0967065617442131\n",
      "Epoch 9479/30000 Training Loss: 0.06829053163528442\n",
      "Epoch 9480/30000 Training Loss: 0.06777605414390564\n",
      "Epoch 9480/30000 Validation Loss: 0.0693756490945816\n",
      "Epoch 9481/30000 Training Loss: 0.07550101727247238\n",
      "Epoch 9482/30000 Training Loss: 0.07816700637340546\n",
      "Epoch 9483/30000 Training Loss: 0.0724332258105278\n",
      "Epoch 9484/30000 Training Loss: 0.07277461141347885\n",
      "Epoch 9485/30000 Training Loss: 0.060312554240226746\n",
      "Epoch 9486/30000 Training Loss: 0.08109834045171738\n",
      "Epoch 9487/30000 Training Loss: 0.07157787680625916\n",
      "Epoch 9488/30000 Training Loss: 0.06852748245000839\n",
      "Epoch 9489/30000 Training Loss: 0.0720159113407135\n",
      "Epoch 9490/30000 Training Loss: 0.09999959915876389\n",
      "Epoch 9490/30000 Validation Loss: 0.04981256648898125\n",
      "Epoch 9491/30000 Training Loss: 0.06944123655557632\n",
      "Epoch 9492/30000 Training Loss: 0.06720033288002014\n",
      "Epoch 9493/30000 Training Loss: 0.06280582398176193\n",
      "Epoch 9494/30000 Training Loss: 0.07003173977136612\n",
      "Epoch 9495/30000 Training Loss: 0.0793004035949707\n",
      "Epoch 9496/30000 Training Loss: 0.08475717157125473\n",
      "Epoch 9497/30000 Training Loss: 0.07953161746263504\n",
      "Epoch 9498/30000 Training Loss: 0.07955015450716019\n",
      "Epoch 9499/30000 Training Loss: 0.07408356666564941\n",
      "Epoch 9500/30000 Training Loss: 0.062001973390579224\n",
      "Epoch 9500/30000 Validation Loss: 0.07727319002151489\n",
      "Epoch 9501/30000 Training Loss: 0.06731132417917252\n",
      "Epoch 9502/30000 Training Loss: 0.0597844161093235\n",
      "Epoch 9503/30000 Training Loss: 0.07206666469573975\n",
      "Epoch 9504/30000 Training Loss: 0.06428589671850204\n",
      "Epoch 9505/30000 Training Loss: 0.06638789921998978\n",
      "Epoch 9506/30000 Training Loss: 0.058796390891075134\n",
      "Epoch 9507/30000 Training Loss: 0.08949769288301468\n",
      "Epoch 9508/30000 Training Loss: 0.07750735431909561\n",
      "Epoch 9509/30000 Training Loss: 0.07495766878128052\n",
      "Epoch 9510/30000 Training Loss: 0.06783012300729752\n",
      "Epoch 9510/30000 Validation Loss: 0.06431987881660461\n",
      "Epoch 9511/30000 Training Loss: 0.080343097448349\n",
      "Epoch 9512/30000 Training Loss: 0.07653602212667465\n",
      "Epoch 9513/30000 Training Loss: 0.05092156305909157\n",
      "Epoch 9514/30000 Training Loss: 0.0860387310385704\n",
      "Epoch 9515/30000 Training Loss: 0.04948434606194496\n",
      "Epoch 9516/30000 Training Loss: 0.06305865198373795\n",
      "Epoch 9517/30000 Training Loss: 0.05819049850106239\n",
      "Epoch 9518/30000 Training Loss: 0.06900006532669067\n",
      "Epoch 9519/30000 Training Loss: 0.07010867446660995\n",
      "Epoch 9520/30000 Training Loss: 0.08236069232225418\n",
      "Epoch 9520/30000 Validation Loss: 0.07921437174081802\n",
      "Epoch 9521/30000 Training Loss: 0.06446215510368347\n",
      "Epoch 9522/30000 Training Loss: 0.07003550976514816\n",
      "Epoch 9523/30000 Training Loss: 0.059812068939208984\n",
      "Epoch 9524/30000 Training Loss: 0.08002854138612747\n",
      "Epoch 9525/30000 Training Loss: 0.08503884822130203\n",
      "Epoch 9526/30000 Training Loss: 0.07998977601528168\n",
      "Epoch 9527/30000 Training Loss: 0.07824680209159851\n",
      "Epoch 9528/30000 Training Loss: 0.05094996094703674\n",
      "Epoch 9529/30000 Training Loss: 0.08541003614664078\n",
      "Epoch 9530/30000 Training Loss: 0.07195831090211868\n",
      "Epoch 9530/30000 Validation Loss: 0.06912852823734283\n",
      "Epoch 9531/30000 Training Loss: 0.06196069344878197\n",
      "Epoch 9532/30000 Training Loss: 0.08026004582643509\n",
      "Epoch 9533/30000 Training Loss: 0.0710538849234581\n",
      "Epoch 9534/30000 Training Loss: 0.07352621108293533\n",
      "Epoch 9535/30000 Training Loss: 0.06453733146190643\n",
      "Epoch 9536/30000 Training Loss: 0.085942842066288\n",
      "Epoch 9537/30000 Training Loss: 0.07481455057859421\n",
      "Epoch 9538/30000 Training Loss: 0.06755715608596802\n",
      "Epoch 9539/30000 Training Loss: 0.06454277038574219\n",
      "Epoch 9540/30000 Training Loss: 0.06776613742113113\n",
      "Epoch 9540/30000 Validation Loss: 0.07199626415967941\n",
      "Epoch 9541/30000 Training Loss: 0.05277889966964722\n",
      "Epoch 9542/30000 Training Loss: 0.06131675839424133\n",
      "Epoch 9543/30000 Training Loss: 0.048336710780858994\n",
      "Epoch 9544/30000 Training Loss: 0.0787329450249672\n",
      "Epoch 9545/30000 Training Loss: 0.08193118125200272\n",
      "Epoch 9546/30000 Training Loss: 0.07749561220407486\n",
      "Epoch 9547/30000 Training Loss: 0.08414173126220703\n",
      "Epoch 9548/30000 Training Loss: 0.0620882473886013\n",
      "Epoch 9549/30000 Training Loss: 0.05998413637280464\n",
      "Epoch 9550/30000 Training Loss: 0.06898940354585648\n",
      "Epoch 9550/30000 Validation Loss: 0.07352930307388306\n",
      "Epoch 9551/30000 Training Loss: 0.060446206480264664\n",
      "Epoch 9552/30000 Training Loss: 0.06618402153253555\n",
      "Epoch 9553/30000 Training Loss: 0.05281563475728035\n",
      "Epoch 9554/30000 Training Loss: 0.10518995672464371\n",
      "Epoch 9555/30000 Training Loss: 0.06897474080324173\n",
      "Epoch 9556/30000 Training Loss: 0.08051871508359909\n",
      "Epoch 9557/30000 Training Loss: 0.06690656393766403\n",
      "Epoch 9558/30000 Training Loss: 0.05660693719983101\n",
      "Epoch 9559/30000 Training Loss: 0.08631088584661484\n",
      "Epoch 9560/30000 Training Loss: 0.06627192348241806\n",
      "Epoch 9560/30000 Validation Loss: 0.06482601165771484\n",
      "Epoch 9561/30000 Training Loss: 0.07545257359743118\n",
      "Epoch 9562/30000 Training Loss: 0.061421800404787064\n",
      "Epoch 9563/30000 Training Loss: 0.05890391394495964\n",
      "Epoch 9564/30000 Training Loss: 0.07763538509607315\n",
      "Epoch 9565/30000 Training Loss: 0.09622925519943237\n",
      "Epoch 9566/30000 Training Loss: 0.07740980386734009\n",
      "Epoch 9567/30000 Training Loss: 0.054279133677482605\n",
      "Epoch 9568/30000 Training Loss: 0.059943001717329025\n",
      "Epoch 9569/30000 Training Loss: 0.07092585414648056\n",
      "Epoch 9570/30000 Training Loss: 0.0780877023935318\n",
      "Epoch 9570/30000 Validation Loss: 0.08424186706542969\n",
      "Epoch 9571/30000 Training Loss: 0.07171059399843216\n",
      "Epoch 9572/30000 Training Loss: 0.06636420637369156\n",
      "Epoch 9573/30000 Training Loss: 0.058294445276260376\n",
      "Epoch 9574/30000 Training Loss: 0.05065573379397392\n",
      "Epoch 9575/30000 Training Loss: 0.061240628361701965\n",
      "Epoch 9576/30000 Training Loss: 0.08359485864639282\n",
      "Epoch 9577/30000 Training Loss: 0.06777998059988022\n",
      "Epoch 9578/30000 Training Loss: 0.06410741060972214\n",
      "Epoch 9579/30000 Training Loss: 0.057892028242349625\n",
      "Epoch 9580/30000 Training Loss: 0.07190455496311188\n",
      "Epoch 9580/30000 Validation Loss: 0.048551708459854126\n",
      "Epoch 9581/30000 Training Loss: 0.05779506266117096\n",
      "Epoch 9582/30000 Training Loss: 0.06980044394731522\n",
      "Epoch 9583/30000 Training Loss: 0.06107425317168236\n",
      "Epoch 9584/30000 Training Loss: 0.06404503434896469\n",
      "Epoch 9585/30000 Training Loss: 0.0848543718457222\n",
      "Epoch 9586/30000 Training Loss: 0.0712229311466217\n",
      "Epoch 9587/30000 Training Loss: 0.06374650448560715\n",
      "Epoch 9588/30000 Training Loss: 0.07351881265640259\n",
      "Epoch 9589/30000 Training Loss: 0.09370020776987076\n",
      "Epoch 9590/30000 Training Loss: 0.05800483748316765\n",
      "Epoch 9590/30000 Validation Loss: 0.0714002251625061\n",
      "Epoch 9591/30000 Training Loss: 0.06541174650192261\n",
      "Epoch 9592/30000 Training Loss: 0.05588732659816742\n",
      "Epoch 9593/30000 Training Loss: 0.07265850901603699\n",
      "Epoch 9594/30000 Training Loss: 0.07402907311916351\n",
      "Epoch 9595/30000 Training Loss: 0.058381516486406326\n",
      "Epoch 9596/30000 Training Loss: 0.060492005199193954\n",
      "Epoch 9597/30000 Training Loss: 0.06276380270719528\n",
      "Epoch 9598/30000 Training Loss: 0.08276476711034775\n",
      "Epoch 9599/30000 Training Loss: 0.10141219943761826\n",
      "Epoch 9600/30000 Training Loss: 0.08602365851402283\n",
      "Epoch 9600/30000 Validation Loss: 0.05424458906054497\n",
      "Epoch 9601/30000 Training Loss: 0.0632057711482048\n",
      "Epoch 9602/30000 Training Loss: 0.07272214442491531\n",
      "Epoch 9603/30000 Training Loss: 0.06462075561285019\n",
      "Epoch 9604/30000 Training Loss: 0.07095178216695786\n",
      "Epoch 9605/30000 Training Loss: 0.05552435293793678\n",
      "Epoch 9606/30000 Training Loss: 0.06775005906820297\n",
      "Epoch 9607/30000 Training Loss: 0.06445745378732681\n",
      "Epoch 9608/30000 Training Loss: 0.08572695404291153\n",
      "Epoch 9609/30000 Training Loss: 0.05833875760436058\n",
      "Epoch 9610/30000 Training Loss: 0.06562637537717819\n",
      "Epoch 9610/30000 Validation Loss: 0.07172776013612747\n",
      "Epoch 9611/30000 Training Loss: 0.05265486612915993\n",
      "Epoch 9612/30000 Training Loss: 0.0655406191945076\n",
      "Epoch 9613/30000 Training Loss: 0.05469344183802605\n",
      "Epoch 9614/30000 Training Loss: 0.0640646293759346\n",
      "Epoch 9615/30000 Training Loss: 0.08323512226343155\n",
      "Epoch 9616/30000 Training Loss: 0.0651472881436348\n",
      "Epoch 9617/30000 Training Loss: 0.05826741084456444\n",
      "Epoch 9618/30000 Training Loss: 0.10053354501724243\n",
      "Epoch 9619/30000 Training Loss: 0.07037559896707535\n",
      "Epoch 9620/30000 Training Loss: 0.0600939579308033\n",
      "Epoch 9620/30000 Validation Loss: 0.05258730426430702\n",
      "Epoch 9621/30000 Training Loss: 0.06434746831655502\n",
      "Epoch 9622/30000 Training Loss: 0.07073793560266495\n",
      "Epoch 9623/30000 Training Loss: 0.06412460654973984\n",
      "Epoch 9624/30000 Training Loss: 0.07435260713100433\n",
      "Epoch 9625/30000 Training Loss: 0.05389426276087761\n",
      "Epoch 9626/30000 Training Loss: 0.054855331778526306\n",
      "Epoch 9627/30000 Training Loss: 0.07871360331773758\n",
      "Epoch 9628/30000 Training Loss: 0.06250458210706711\n",
      "Epoch 9629/30000 Training Loss: 0.06917806714773178\n",
      "Epoch 9630/30000 Training Loss: 0.05063087120652199\n",
      "Epoch 9630/30000 Validation Loss: 0.06777603924274445\n",
      "Epoch 9631/30000 Training Loss: 0.05243028327822685\n",
      "Epoch 9632/30000 Training Loss: 0.08799304813146591\n",
      "Epoch 9633/30000 Training Loss: 0.06141610071063042\n",
      "Epoch 9634/30000 Training Loss: 0.07672309130430222\n",
      "Epoch 9635/30000 Training Loss: 0.07142339646816254\n",
      "Epoch 9636/30000 Training Loss: 0.07341913133859634\n",
      "Epoch 9637/30000 Training Loss: 0.06730737537145615\n",
      "Epoch 9638/30000 Training Loss: 0.06127482280135155\n",
      "Epoch 9639/30000 Training Loss: 0.06597796827554703\n",
      "Epoch 9640/30000 Training Loss: 0.09573548287153244\n",
      "Epoch 9640/30000 Validation Loss: 0.08885722607374191\n",
      "Epoch 9641/30000 Training Loss: 0.07811637967824936\n",
      "Epoch 9642/30000 Training Loss: 0.05523287132382393\n",
      "Epoch 9643/30000 Training Loss: 0.05467452108860016\n",
      "Epoch 9644/30000 Training Loss: 0.04816363379359245\n",
      "Epoch 9645/30000 Training Loss: 0.10214029997587204\n",
      "Epoch 9646/30000 Training Loss: 0.0794157013297081\n",
      "Epoch 9647/30000 Training Loss: 0.06097282096743584\n",
      "Epoch 9648/30000 Training Loss: 0.06992622464895248\n",
      "Epoch 9649/30000 Training Loss: 0.0669391006231308\n",
      "Epoch 9650/30000 Training Loss: 0.0689183697104454\n",
      "Epoch 9650/30000 Validation Loss: 0.052985865622758865\n",
      "Epoch 9651/30000 Training Loss: 0.07074946910142899\n",
      "Epoch 9652/30000 Training Loss: 0.05670161172747612\n",
      "Epoch 9653/30000 Training Loss: 0.055040422827005386\n",
      "Epoch 9654/30000 Training Loss: 0.06608543545007706\n",
      "Epoch 9655/30000 Training Loss: 0.06567717343568802\n",
      "Epoch 9656/30000 Training Loss: 0.08652425557374954\n",
      "Epoch 9657/30000 Training Loss: 0.07626507431268692\n",
      "Epoch 9658/30000 Training Loss: 0.08163337409496307\n",
      "Epoch 9659/30000 Training Loss: 0.07136905938386917\n",
      "Epoch 9660/30000 Training Loss: 0.054256245493888855\n",
      "Epoch 9660/30000 Validation Loss: 0.06236189231276512\n",
      "Epoch 9661/30000 Training Loss: 0.04911747947335243\n",
      "Epoch 9662/30000 Training Loss: 0.06750208139419556\n",
      "Epoch 9663/30000 Training Loss: 0.06827330589294434\n",
      "Epoch 9664/30000 Training Loss: 0.1073935404419899\n",
      "Epoch 9665/30000 Training Loss: 0.05475419759750366\n",
      "Epoch 9666/30000 Training Loss: 0.08704468607902527\n",
      "Epoch 9667/30000 Training Loss: 0.06535255163908005\n",
      "Epoch 9668/30000 Training Loss: 0.07701090723276138\n",
      "Epoch 9669/30000 Training Loss: 0.06630789488554001\n",
      "Epoch 9670/30000 Training Loss: 0.08754387497901917\n",
      "Epoch 9670/30000 Validation Loss: 0.06292470544576645\n",
      "Epoch 9671/30000 Training Loss: 0.05433526635169983\n",
      "Epoch 9672/30000 Training Loss: 0.05670403316617012\n",
      "Epoch 9673/30000 Training Loss: 0.055212631821632385\n",
      "Epoch 9674/30000 Training Loss: 0.07934630662202835\n",
      "Epoch 9675/30000 Training Loss: 0.06793683022260666\n",
      "Epoch 9676/30000 Training Loss: 0.06693085283041\n",
      "Epoch 9677/30000 Training Loss: 0.07807285338640213\n",
      "Epoch 9678/30000 Training Loss: 0.06038768216967583\n",
      "Epoch 9679/30000 Training Loss: 0.0769830271601677\n",
      "Epoch 9680/30000 Training Loss: 0.06959682703018188\n",
      "Epoch 9680/30000 Validation Loss: 0.08039496093988419\n",
      "Epoch 9681/30000 Training Loss: 0.07956986874341965\n",
      "Epoch 9682/30000 Training Loss: 0.06075320765376091\n",
      "Epoch 9683/30000 Training Loss: 0.04782676324248314\n",
      "Epoch 9684/30000 Training Loss: 0.06462065130472183\n",
      "Epoch 9685/30000 Training Loss: 0.06508109718561172\n",
      "Epoch 9686/30000 Training Loss: 0.07234504073858261\n",
      "Epoch 9687/30000 Training Loss: 0.07515975087881088\n",
      "Epoch 9688/30000 Training Loss: 0.05853322520852089\n",
      "Epoch 9689/30000 Training Loss: 0.05737492814660072\n",
      "Epoch 9690/30000 Training Loss: 0.06635714322328568\n",
      "Epoch 9690/30000 Validation Loss: 0.07244862616062164\n",
      "Epoch 9691/30000 Training Loss: 0.052185337990522385\n",
      "Epoch 9692/30000 Training Loss: 0.08272280544042587\n",
      "Epoch 9693/30000 Training Loss: 0.0638934075832367\n",
      "Epoch 9694/30000 Training Loss: 0.06994575262069702\n",
      "Epoch 9695/30000 Training Loss: 0.07142101973295212\n",
      "Epoch 9696/30000 Training Loss: 0.058900102972984314\n",
      "Epoch 9697/30000 Training Loss: 0.07481304556131363\n",
      "Epoch 9698/30000 Training Loss: 0.06332822889089584\n",
      "Epoch 9699/30000 Training Loss: 0.07733744382858276\n",
      "Epoch 9700/30000 Training Loss: 0.0775124728679657\n",
      "Epoch 9700/30000 Validation Loss: 0.09210765361785889\n",
      "Epoch 9701/30000 Training Loss: 0.08808831125497818\n",
      "Epoch 9702/30000 Training Loss: 0.06764630228281021\n",
      "Epoch 9703/30000 Training Loss: 0.0696013793349266\n",
      "Epoch 9704/30000 Training Loss: 0.07302859425544739\n",
      "Epoch 9705/30000 Training Loss: 0.0853416845202446\n",
      "Epoch 9706/30000 Training Loss: 0.07204733043909073\n",
      "Epoch 9707/30000 Training Loss: 0.06789174675941467\n",
      "Epoch 9708/30000 Training Loss: 0.05800854042172432\n",
      "Epoch 9709/30000 Training Loss: 0.06729885190725327\n",
      "Epoch 9710/30000 Training Loss: 0.05420063063502312\n",
      "Epoch 9710/30000 Validation Loss: 0.06005613133311272\n",
      "Epoch 9711/30000 Training Loss: 0.08242861181497574\n",
      "Epoch 9712/30000 Training Loss: 0.060275208204984665\n",
      "Epoch 9713/30000 Training Loss: 0.0710889920592308\n",
      "Epoch 9714/30000 Training Loss: 0.07196707278490067\n",
      "Epoch 9715/30000 Training Loss: 0.084998719394207\n",
      "Epoch 9716/30000 Training Loss: 0.07714829593896866\n",
      "Epoch 9717/30000 Training Loss: 0.0550600104033947\n",
      "Epoch 9718/30000 Training Loss: 0.06919968873262405\n",
      "Epoch 9719/30000 Training Loss: 0.05935145542025566\n",
      "Epoch 9720/30000 Training Loss: 0.07878312468528748\n",
      "Epoch 9720/30000 Validation Loss: 0.09278037399053574\n",
      "Epoch 9721/30000 Training Loss: 0.05177786946296692\n",
      "Epoch 9722/30000 Training Loss: 0.07406338304281235\n",
      "Epoch 9723/30000 Training Loss: 0.08441940695047379\n",
      "Epoch 9724/30000 Training Loss: 0.05688021704554558\n",
      "Epoch 9725/30000 Training Loss: 0.06251297146081924\n",
      "Epoch 9726/30000 Training Loss: 0.09009157866239548\n",
      "Epoch 9727/30000 Training Loss: 0.06333208829164505\n",
      "Epoch 9728/30000 Training Loss: 0.04935142397880554\n",
      "Epoch 9729/30000 Training Loss: 0.06752435117959976\n",
      "Epoch 9730/30000 Training Loss: 0.06387617439031601\n",
      "Epoch 9730/30000 Validation Loss: 0.09028907865285873\n",
      "Epoch 9731/30000 Training Loss: 0.09348896890878677\n",
      "Epoch 9732/30000 Training Loss: 0.07233835011720657\n",
      "Epoch 9733/30000 Training Loss: 0.09554877877235413\n",
      "Epoch 9734/30000 Training Loss: 0.08335565775632858\n",
      "Epoch 9735/30000 Training Loss: 0.0727103129029274\n",
      "Epoch 9736/30000 Training Loss: 0.0776706412434578\n",
      "Epoch 9737/30000 Training Loss: 0.06475064158439636\n",
      "Epoch 9738/30000 Training Loss: 0.06269410997629166\n",
      "Epoch 9739/30000 Training Loss: 0.07906101644039154\n",
      "Epoch 9740/30000 Training Loss: 0.06651489436626434\n",
      "Epoch 9740/30000 Validation Loss: 0.07699688524007797\n",
      "Epoch 9741/30000 Training Loss: 0.08285840600728989\n",
      "Epoch 9742/30000 Training Loss: 0.07647933065891266\n",
      "Epoch 9743/30000 Training Loss: 0.06303127855062485\n",
      "Epoch 9744/30000 Training Loss: 0.07116066664457321\n",
      "Epoch 9745/30000 Training Loss: 0.07280916720628738\n",
      "Epoch 9746/30000 Training Loss: 0.05621258541941643\n",
      "Epoch 9747/30000 Training Loss: 0.05659909546375275\n",
      "Epoch 9748/30000 Training Loss: 0.06388899683952332\n",
      "Epoch 9749/30000 Training Loss: 0.0952499583363533\n",
      "Epoch 9750/30000 Training Loss: 0.08507013320922852\n",
      "Epoch 9750/30000 Validation Loss: 0.077573262155056\n",
      "Epoch 9751/30000 Training Loss: 0.08957865089178085\n",
      "Epoch 9752/30000 Training Loss: 0.0748986229300499\n",
      "Epoch 9753/30000 Training Loss: 0.05932379886507988\n",
      "Epoch 9754/30000 Training Loss: 0.07017339020967484\n",
      "Epoch 9755/30000 Training Loss: 0.0912628248333931\n",
      "Epoch 9756/30000 Training Loss: 0.06362830847501755\n",
      "Epoch 9757/30000 Training Loss: 0.05141581594944\n",
      "Epoch 9758/30000 Training Loss: 0.07693710923194885\n",
      "Epoch 9759/30000 Training Loss: 0.06692751497030258\n",
      "Epoch 9760/30000 Training Loss: 0.06987852603197098\n",
      "Epoch 9760/30000 Validation Loss: 0.061219826340675354\n",
      "Epoch 9761/30000 Training Loss: 0.06736353039741516\n",
      "Epoch 9762/30000 Training Loss: 0.051014456897974014\n",
      "Epoch 9763/30000 Training Loss: 0.0938933864235878\n",
      "Epoch 9764/30000 Training Loss: 0.07078473269939423\n",
      "Epoch 9765/30000 Training Loss: 0.08718004077672958\n",
      "Epoch 9766/30000 Training Loss: 0.06559682637453079\n",
      "Epoch 9767/30000 Training Loss: 0.057985108345746994\n",
      "Epoch 9768/30000 Training Loss: 0.08938761800527573\n",
      "Epoch 9769/30000 Training Loss: 0.07403800636529922\n",
      "Epoch 9770/30000 Training Loss: 0.07133132219314575\n",
      "Epoch 9770/30000 Validation Loss: 0.0720251202583313\n",
      "Epoch 9771/30000 Training Loss: 0.05863362178206444\n",
      "Epoch 9772/30000 Training Loss: 0.06270811706781387\n",
      "Epoch 9773/30000 Training Loss: 0.06825719773769379\n",
      "Epoch 9774/30000 Training Loss: 0.0715053603053093\n",
      "Epoch 9775/30000 Training Loss: 0.07228792458772659\n",
      "Epoch 9776/30000 Training Loss: 0.06215318664908409\n",
      "Epoch 9777/30000 Training Loss: 0.07545549422502518\n",
      "Epoch 9778/30000 Training Loss: 0.05070921406149864\n",
      "Epoch 9779/30000 Training Loss: 0.09121542423963547\n",
      "Epoch 9780/30000 Training Loss: 0.0791996419429779\n",
      "Epoch 9780/30000 Validation Loss: 0.07592186331748962\n",
      "Epoch 9781/30000 Training Loss: 0.055735234171152115\n",
      "Epoch 9782/30000 Training Loss: 0.06606590747833252\n",
      "Epoch 9783/30000 Training Loss: 0.06870283931493759\n",
      "Epoch 9784/30000 Training Loss: 0.08274722844362259\n",
      "Epoch 9785/30000 Training Loss: 0.06648368388414383\n",
      "Epoch 9786/30000 Training Loss: 0.06753605604171753\n",
      "Epoch 9787/30000 Training Loss: 0.07431300729513168\n",
      "Epoch 9788/30000 Training Loss: 0.09060734510421753\n",
      "Epoch 9789/30000 Training Loss: 0.069278784096241\n",
      "Epoch 9790/30000 Training Loss: 0.05453189089894295\n",
      "Epoch 9790/30000 Validation Loss: 0.08111365884542465\n",
      "Epoch 9791/30000 Training Loss: 0.06261938810348511\n",
      "Epoch 9792/30000 Training Loss: 0.06702212244272232\n",
      "Epoch 9793/30000 Training Loss: 0.06910789757966995\n",
      "Epoch 9794/30000 Training Loss: 0.06270599365234375\n",
      "Epoch 9795/30000 Training Loss: 0.06880391389131546\n",
      "Epoch 9796/30000 Training Loss: 0.06908523291349411\n",
      "Epoch 9797/30000 Training Loss: 0.0668729841709137\n",
      "Epoch 9798/30000 Training Loss: 0.06809724122285843\n",
      "Epoch 9799/30000 Training Loss: 0.08920317888259888\n",
      "Epoch 9800/30000 Training Loss: 0.060883015394210815\n",
      "Epoch 9800/30000 Validation Loss: 0.0855715200304985\n",
      "Epoch 9801/30000 Training Loss: 0.07382044196128845\n",
      "Epoch 9802/30000 Training Loss: 0.06615444272756577\n",
      "Epoch 9803/30000 Training Loss: 0.05073719844222069\n",
      "Epoch 9804/30000 Training Loss: 0.07306618243455887\n",
      "Epoch 9805/30000 Training Loss: 0.10456516593694687\n",
      "Epoch 9806/30000 Training Loss: 0.059513043612241745\n",
      "Epoch 9807/30000 Training Loss: 0.05427147075533867\n",
      "Epoch 9808/30000 Training Loss: 0.07089785486459732\n",
      "Epoch 9809/30000 Training Loss: 0.057583507150411606\n",
      "Epoch 9810/30000 Training Loss: 0.06923016905784607\n",
      "Epoch 9810/30000 Validation Loss: 0.08332279324531555\n",
      "Epoch 9811/30000 Training Loss: 0.08826840668916702\n",
      "Epoch 9812/30000 Training Loss: 0.0760750100016594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_135/754797905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_loss, val_loss = TrainLoop(\n\u001b[0m\u001b[1;32m      6\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                             \u001b[0mdiffusion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/train.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m         ):\n\u001b[1;32m    125\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mbatch_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/train.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/train.py\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, batch, cond)\u001b[0m\n\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLossAwareSampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    762\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[1;32m    763\u001b[0m         \u001b[0;31m# print('called training_losses')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_xstart_from_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses_seq2seq\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mtT_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmean_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_mean\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mdecoder_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_discrete_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# embedding regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nll\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_discrete_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out_x_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# x_0->model_out_x_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# assert (model.lm_head.weight == model.word_embedding.weight).all()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/gaussian_diffusion.py\u001b[0m in \u001b[0;36m_token_discrete_loss\u001b[0;34m(self, x_t, get_logits, input_ids, mask, truncate, t)\u001b[0m\n\u001b[1;32m    486\u001b[0m         '''\n\u001b[1;32m    487\u001b[0m         \u001b[0mreshaped_x_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_x_t\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# bsz, seqlen, vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mdecoder_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/diffusion-text-generation/model_arch/transformer.py\u001b[0m in \u001b[0;36mget_logits\u001b[0;34m(self, hidden_repr)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# standard cosine similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtext_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "model.to(dist_util.dev())\n",
    "\n",
    "train_loss, val_loss = TrainLoop(\n",
    "                            model=model,\n",
    "                            diffusion=diffusion,\n",
    "                            data=data,\n",
    "                            batch_size=batch_size,\n",
    "                            microbatch=microbatch,\n",
    "                            lr=lr,\n",
    "                            ema_rate=ema_rate,\n",
    "                            schedule_sampler=schedule_sampler,\n",
    "                            weight_decay=weight_decay,\n",
    "                            epochs=epochs,\n",
    "                            eval_data=val,\n",
    "                            eval_interval=eval_interval,\n",
    "                            warm_up_steps=500,\n",
    "                            use_llrd=True,\n",
    "                            llrd_rate=0.9\n",
    "                        ).run_loop()\n",
    "\n",
    "dt = datetime.now().strftime(\"%m%d\")\n",
    "pickle.dump(model, open(f\"models/{dt}/final_model_df{diffusion_steps}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fa9c7-84c8-48bd-b368-318df3f0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_fp = f'models/results/model_df2000_best_epoch_26150_min_val_loss_0.030700000002980232.pkl'\n",
    "# with open(best_model_fp, 'rb') as handle:\n",
    "#     best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_2000 = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 2000),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3e232-af43-44f3-8acb-ec31f1b0bd99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " [\"My lord, 'tis the pondering of life's meaning that doth occupy my thoughts most gravely.\", 'call her forth to me.'] ['', '']\n",
      "RAM used: 3236.95 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 5\n",
      "})\n",
      "RAM used: 3236.95 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a254597cdf904d55bfc4dc22d6aff80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 5\n",
      "})\n",
      "### tokenized_datasets...example [2, 105, 203, 10, 9, 334, 78, 14472, 102, 94, 451, 9, 41, 2679, 110, 457, 19200, 105, 1079, 367, 23844, 12, 3]\n",
      "RAM used: 3238.46 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888a754ab16e499e9af4f2bc17b5f8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3238.47 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b167c6b834854d4ba819b64589ebe3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 5\n",
      "}) padded dataset\n",
      "RAM used: 3238.51 MB\n",
      "RAM used: 3238.51 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33754f65e55d4e63aaa3e79ffff47201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion_2000, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128, \n",
    "                                                           show_intermediate_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] my lord,'tis the pondering of life's meaning that doth occupy my thoughts most gravely. [SEP] [SEP]\",\n",
       " '[CLS] call her forth to me. [SEP] [SEP]',\n",
       " '[CLS] what is your will? [SEP] [SEP]',\n",
       " '[CLS] hark, fair sarah! [SEP] [SEP]',\n",
       " \"[CLS] what dost thou have planned for the morrow's reprieve? [SEP] [SEP]\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9184761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] my lord. [SEP]',\n",
       " '[CLS] she so from a reason now, to wife my wife the wife man no from a most him come the wife [SEP]',\n",
       " '[CLS] come to his thing your wife me. [SEP]',\n",
       " '[CLS] o, the of pretty, in touch,. [SEP]',\n",
       " '[CLS] who lord, she them that. [SEP]']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81787a-cde2-40ea-ae5b-2d3c66e84c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
