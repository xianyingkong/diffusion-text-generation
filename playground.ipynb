{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a01d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_train import create_model_and_diffusion\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from tokenizer import load_tokenizer, load_model_emb\n",
    "from sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=20\n",
    "microbatch=10\n",
    "epochs=30_000\n",
    "eval_interval=100\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=2000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=102\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data_dir='data'\n",
    "data_player_dir='data/with_player'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('shakespeare_plays', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30268, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30268"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['o hell! what have we here? a carrion death, within whose empty eye there is a written scroll!', 'and his disciples only envy at, ye blew the fire that burns ye now have at ye! enter king,'] [\"i'll read the writing. all that glitters is not gold, often have you heard that told\", 'frowning on them, takes his seat']\n",
      "RAM used: 2657.00 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "RAM used: 2670.58 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcb541969a1431d82bd380027f46049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "### tokenized_datasets...example [2, 37, 1300, 6, 164, 150, 133, 237, 22, 23, 7135, 432, 10, 906, 569, 3066, 756, 210, 121, 23, 4180, 7422, 6, 3]\n",
      "RAM used: 2714.87 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ba1ffaa97a418a9353621056c4eca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2745.44 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d88f52163d458386ca5c77a40ea25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 48627\n",
      "}) padded dataset\n",
      "RAM used: 2836.12 MB\n",
      "RAM used: 2836.12 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"petruchio is my name, antonio's son, a man well known throughout all italy.\", 'the matter is to me, sir, as concerning jaquenetta. the manner of it is,'] ['i know him well you are welcome for his sake.', 'i was taken with the manner.']\n",
      "RAM used: 2798.12 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "RAM used: 2798.12 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a72fd5e1d04677812a6de099366bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "### tokenized_datasets...example [2, 3886, 121, 105, 520, 10, 2546, 9, 41, 478, 10, 23, 211, 254, 1233, 9840, 187, 4043, 12, 3]\n",
      "RAM used: 2804.71 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aba3a1631b24205b5ed46ddc45dbdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2819.98 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db5c475469340fd9dcbd75cb680a9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 12147\n",
      "}) padded dataset\n",
      "RAM used: 2837.61 MB\n",
      "RAM used: 2837.61 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(30268, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=30268, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )\n",
    "\n",
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192508"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 0.0001\n",
      "name: lm_head.bias, lr: 0.0001\n",
      "name: time_embed.0.weight, lr: 0.0001\n",
      "name: time_embed.0.bias, lr: 0.0001\n",
      "name: time_embed.2.weight, lr: 0.0001\n",
      "name: time_embed.2.bias, lr: 0.0001\n",
      "name: input_up_proj.0.weight, lr: 0.0001\n",
      "name: input_up_proj.0.bias, lr: 0.0001\n",
      "name: input_up_proj.2.weight, lr: 0.0001\n",
      "name: input_up_proj.2.bias, lr: 0.0001\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.000112817809950197\n",
      "name: position_embeddings.weight, lr: 0.00011395738378807778\n",
      "name: LayerNorm.weight, lr: 0.00011395738378807778\n",
      "name: LayerNorm.bias, lr: 0.00011395738378807778\n",
      "name: output_down_proj.0.weight, lr: 0.00011395738378807778\n",
      "name: output_down_proj.0.bias, lr: 0.00011395738378807778\n",
      "name: output_down_proj.2.weight, lr: 0.00011395738378807778\n",
      "name: output_down_proj.2.bias, lr: 0.00011395738378807778\n",
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n",
      "Epoch 1/30000 Training Loss: 1.1960620880126953\n",
      "Epoch 2/30000 Training Loss: 1.1986987590789795\n",
      "Epoch 3/30000 Training Loss: 1.1942803859710693\n",
      "Epoch 4/30000 Training Loss: 1.1921192407608032\n",
      "Epoch 5/30000 Training Loss: 1.1917610168457031\n",
      "Epoch 6/30000 Training Loss: 1.1797977685928345\n",
      "Epoch 7/30000 Training Loss: 1.1714781522750854\n",
      "Epoch 8/30000 Training Loss: 1.1640989780426025\n",
      "Epoch 9/30000 Training Loss: 1.1473033428192139\n",
      "Epoch 10/30000 Training Loss: 1.1356630325317383\n",
      "Epoch 11/30000 Training Loss: 1.1178592443466187\n",
      "Epoch 12/30000 Training Loss: 1.1065053939819336\n",
      "Epoch 13/30000 Training Loss: 1.0856940746307373\n",
      "Epoch 14/30000 Training Loss: 1.064578652381897\n",
      "Epoch 15/30000 Training Loss: 1.0511670112609863\n",
      "Epoch 16/30000 Training Loss: 1.0286777019500732\n",
      "Epoch 17/30000 Training Loss: 1.0047500133514404\n",
      "Epoch 18/30000 Training Loss: 0.9839529991149902\n",
      "Epoch 19/30000 Training Loss: 0.970119059085846\n",
      "Epoch 20/30000 Training Loss: 0.9377853870391846\n",
      "Epoch 21/30000 Training Loss: 0.9062467813491821\n",
      "Epoch 22/30000 Training Loss: 0.8936604261398315\n",
      "Epoch 23/30000 Training Loss: 0.8662049770355225\n",
      "Epoch 24/30000 Training Loss: 0.8464294672012329\n",
      "Epoch 25/30000 Training Loss: 0.8301979303359985\n",
      "Epoch 26/30000 Training Loss: 0.7913105487823486\n",
      "Epoch 27/30000 Training Loss: 0.7690510153770447\n",
      "Epoch 28/30000 Training Loss: 0.7590354681015015\n",
      "Epoch 29/30000 Training Loss: 0.7377820014953613\n",
      "Epoch 30/30000 Training Loss: 0.7315154075622559\n",
      "Epoch 31/30000 Training Loss: 0.7271511554718018\n",
      "Epoch 32/30000 Training Loss: 0.6881808042526245\n",
      "Epoch 33/30000 Training Loss: 0.6538013815879822\n",
      "Epoch 34/30000 Training Loss: 0.6738131046295166\n",
      "Epoch 35/30000 Training Loss: 0.6551072001457214\n",
      "Epoch 36/30000 Training Loss: 0.6432257890701294\n",
      "Epoch 37/30000 Training Loss: 0.6198878288269043\n",
      "Epoch 38/30000 Training Loss: 0.6120221614837646\n",
      "Epoch 39/30000 Training Loss: 0.6275086998939514\n",
      "Epoch 40/30000 Training Loss: 0.6368765830993652\n",
      "Epoch 41/30000 Training Loss: 0.6100599765777588\n",
      "Epoch 42/30000 Training Loss: 0.568395733833313\n",
      "Epoch 43/30000 Training Loss: 0.5811017751693726\n",
      "Epoch 44/30000 Training Loss: 0.5675331354141235\n",
      "Epoch 45/30000 Training Loss: 0.5860152244567871\n",
      "Epoch 46/30000 Training Loss: 0.5809703469276428\n",
      "Epoch 47/30000 Training Loss: 0.5550506711006165\n",
      "Epoch 48/30000 Training Loss: 0.5666651725769043\n",
      "Epoch 49/30000 Training Loss: 0.5486868023872375\n",
      "Epoch 50/30000 Training Loss: 0.571609377861023\n",
      "Epoch 51/30000 Training Loss: 0.572226881980896\n",
      "Epoch 52/30000 Training Loss: 0.5489662885665894\n",
      "Epoch 53/30000 Training Loss: 0.554851770401001\n",
      "Epoch 54/30000 Training Loss: 0.5401816964149475\n",
      "Epoch 55/30000 Training Loss: 0.5268640518188477\n",
      "Epoch 56/30000 Training Loss: 0.5468295812606812\n",
      "Epoch 57/30000 Training Loss: 0.5298045873641968\n",
      "Epoch 58/30000 Training Loss: 0.5454474687576294\n",
      "Epoch 59/30000 Training Loss: 0.5593158006668091\n",
      "Epoch 60/30000 Training Loss: 0.5340662002563477\n",
      "Epoch 61/30000 Training Loss: 0.5430231690406799\n",
      "Epoch 62/30000 Training Loss: 0.5327667593955994\n",
      "Epoch 63/30000 Training Loss: 0.5009147524833679\n",
      "Epoch 64/30000 Training Loss: 0.5388891100883484\n",
      "Epoch 65/30000 Training Loss: 0.5543081760406494\n",
      "Epoch 66/30000 Training Loss: 0.5167059898376465\n",
      "Epoch 67/30000 Training Loss: 0.5152516961097717\n",
      "Epoch 68/30000 Training Loss: 0.5244541168212891\n",
      "Epoch 69/30000 Training Loss: 0.5613499879837036\n",
      "Epoch 70/30000 Training Loss: 0.517228364944458\n",
      "Epoch 71/30000 Training Loss: 0.5005124807357788\n",
      "Epoch 72/30000 Training Loss: 0.5542588233947754\n",
      "Epoch 73/30000 Training Loss: 0.501573920249939\n",
      "Epoch 74/30000 Training Loss: 0.5516923666000366\n",
      "Epoch 75/30000 Training Loss: 0.4991297423839569\n",
      "Epoch 76/30000 Training Loss: 0.5334743857383728\n",
      "Epoch 77/30000 Training Loss: 0.516472339630127\n",
      "Epoch 78/30000 Training Loss: 0.5232129096984863\n",
      "Epoch 79/30000 Training Loss: 0.5639282464981079\n",
      "Epoch 80/30000 Training Loss: 0.5569104552268982\n",
      "Epoch 81/30000 Training Loss: 0.5513701438903809\n",
      "Epoch 82/30000 Training Loss: 0.5445849299430847\n",
      "Epoch 83/30000 Training Loss: 0.5159658789634705\n",
      "Epoch 84/30000 Training Loss: 0.5132524371147156\n",
      "Epoch 85/30000 Training Loss: 0.5467804670333862\n",
      "Epoch 86/30000 Training Loss: 0.5520956516265869\n",
      "Epoch 87/30000 Training Loss: 0.5350439548492432\n",
      "Epoch 88/30000 Training Loss: 0.486587792634964\n",
      "Epoch 89/30000 Training Loss: 0.5505601167678833\n",
      "Epoch 90/30000 Training Loss: 0.5105676651000977\n",
      "Epoch 91/30000 Training Loss: 0.5283130407333374\n",
      "Epoch 92/30000 Training Loss: 0.5386536121368408\n",
      "Epoch 93/30000 Training Loss: 0.5398673415184021\n",
      "Epoch 94/30000 Training Loss: 0.521226167678833\n",
      "Epoch 95/30000 Training Loss: 0.5666586756706238\n",
      "Epoch 96/30000 Training Loss: 0.5133384466171265\n",
      "Epoch 97/30000 Training Loss: 0.5091781616210938\n",
      "Epoch 98/30000 Training Loss: 0.503135085105896\n",
      "Epoch 99/30000 Training Loss: 0.5185592174530029\n",
      "Epoch 100/30000 Training Loss: 0.5077349543571472\n",
      "Epoch 100/30000 Validation Loss: 0.5334790349006653\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.5334790349006653<=============\n",
      "Epoch 101/30000 Training Loss: 0.5024475455284119\n",
      "Epoch 102/30000 Training Loss: 0.5105360746383667\n",
      "Epoch 103/30000 Training Loss: 0.4914554953575134\n",
      "Epoch 104/30000 Training Loss: 0.5112332105636597\n",
      "Epoch 105/30000 Training Loss: 0.521155834197998\n",
      "Epoch 106/30000 Training Loss: 0.4546918272972107\n",
      "Epoch 107/30000 Training Loss: 0.5017281174659729\n",
      "Epoch 108/30000 Training Loss: 0.46055591106414795\n",
      "Epoch 109/30000 Training Loss: 0.41930949687957764\n",
      "Epoch 110/30000 Training Loss: 0.40899136662483215\n",
      "Epoch 111/30000 Training Loss: 0.4131719470024109\n",
      "Epoch 112/30000 Training Loss: 0.41224128007888794\n",
      "Epoch 113/30000 Training Loss: 0.39694640040397644\n",
      "Epoch 114/30000 Training Loss: 0.4112899899482727\n",
      "Epoch 115/30000 Training Loss: 0.4072071611881256\n",
      "Epoch 116/30000 Training Loss: 0.3790191411972046\n",
      "Epoch 117/30000 Training Loss: 0.37141889333724976\n",
      "Epoch 118/30000 Training Loss: 0.36687809228897095\n",
      "Epoch 119/30000 Training Loss: 0.3357720375061035\n",
      "Epoch 120/30000 Training Loss: 0.3159172236919403\n",
      "Epoch 121/30000 Training Loss: 0.3547062873840332\n",
      "Epoch 122/30000 Training Loss: 0.34687119722366333\n",
      "Epoch 123/30000 Training Loss: 0.3238948583602905\n",
      "Epoch 124/30000 Training Loss: 0.33564844727516174\n",
      "Epoch 125/30000 Training Loss: 0.2907866835594177\n",
      "Epoch 126/30000 Training Loss: 0.31342771649360657\n",
      "Epoch 127/30000 Training Loss: 0.33605194091796875\n",
      "Epoch 128/30000 Training Loss: 0.31801825761795044\n",
      "Epoch 129/30000 Training Loss: 0.2902500331401825\n",
      "Epoch 130/30000 Training Loss: 0.2762947082519531\n",
      "Epoch 131/30000 Training Loss: 0.3045331835746765\n",
      "Epoch 132/30000 Training Loss: 0.3218020498752594\n",
      "Epoch 133/30000 Training Loss: 0.301613986492157\n",
      "Epoch 134/30000 Training Loss: 0.33647459745407104\n",
      "Epoch 135/30000 Training Loss: 0.28145408630371094\n",
      "Epoch 136/30000 Training Loss: 0.31196874380111694\n",
      "Epoch 137/30000 Training Loss: 0.30877572298049927\n",
      "Epoch 138/30000 Training Loss: 0.29672443866729736\n",
      "Epoch 139/30000 Training Loss: 0.24446634948253632\n",
      "Epoch 140/30000 Training Loss: 0.3338766098022461\n",
      "Epoch 141/30000 Training Loss: 0.29193025827407837\n",
      "Epoch 142/30000 Training Loss: 0.31492140889167786\n",
      "Epoch 143/30000 Training Loss: 0.31809642910957336\n",
      "Epoch 144/30000 Training Loss: 0.29227304458618164\n",
      "Epoch 145/30000 Training Loss: 0.28415173292160034\n",
      "Epoch 146/30000 Training Loss: 0.31440114974975586\n",
      "Epoch 147/30000 Training Loss: 0.3200906217098236\n",
      "Epoch 148/30000 Training Loss: 0.3190670609474182\n",
      "Epoch 149/30000 Training Loss: 0.2947659194469452\n",
      "Epoch 150/30000 Training Loss: 0.2996593117713928\n",
      "Epoch 151/30000 Training Loss: 0.31741052865982056\n",
      "Epoch 152/30000 Training Loss: 0.27258554100990295\n",
      "Epoch 153/30000 Training Loss: 0.29504328966140747\n",
      "Epoch 154/30000 Training Loss: 0.2989201545715332\n",
      "Epoch 155/30000 Training Loss: 0.28479552268981934\n",
      "Epoch 156/30000 Training Loss: 0.2900938391685486\n",
      "Epoch 157/30000 Training Loss: 0.3034546673297882\n",
      "Epoch 158/30000 Training Loss: 0.28681886196136475\n",
      "Epoch 159/30000 Training Loss: 0.27816975116729736\n",
      "Epoch 160/30000 Training Loss: 0.30399245023727417\n",
      "Epoch 161/30000 Training Loss: 0.29906991124153137\n",
      "Epoch 162/30000 Training Loss: 0.3075404167175293\n",
      "Epoch 163/30000 Training Loss: 0.266208291053772\n",
      "Epoch 164/30000 Training Loss: 0.3244985342025757\n",
      "Epoch 165/30000 Training Loss: 0.29519888758659363\n",
      "Epoch 166/30000 Training Loss: 0.2976064383983612\n",
      "Epoch 167/30000 Training Loss: 0.3104356825351715\n",
      "Epoch 168/30000 Training Loss: 0.3201749324798584\n",
      "Epoch 169/30000 Training Loss: 0.31408703327178955\n",
      "Epoch 170/30000 Training Loss: 0.28817957639694214\n",
      "Epoch 171/30000 Training Loss: 0.29649120569229126\n",
      "Epoch 172/30000 Training Loss: 0.3034832179546356\n",
      "Epoch 173/30000 Training Loss: 0.27299606800079346\n",
      "Epoch 174/30000 Training Loss: 0.2845408320426941\n",
      "Epoch 175/30000 Training Loss: 0.26578715443611145\n",
      "Epoch 176/30000 Training Loss: 0.32627105712890625\n",
      "Epoch 177/30000 Training Loss: 0.29039081931114197\n",
      "Epoch 178/30000 Training Loss: 0.316251277923584\n",
      "Epoch 179/30000 Training Loss: 0.284295916557312\n",
      "Epoch 180/30000 Training Loss: 0.3196849822998047\n",
      "Epoch 181/30000 Training Loss: 0.30112752318382263\n",
      "Epoch 182/30000 Training Loss: 0.29881513118743896\n",
      "Epoch 183/30000 Training Loss: 0.31614893674850464\n",
      "Epoch 184/30000 Training Loss: 0.2702265977859497\n",
      "Epoch 185/30000 Training Loss: 0.30017292499542236\n",
      "Epoch 186/30000 Training Loss: 0.2819036841392517\n",
      "Epoch 187/30000 Training Loss: 0.2837318480014801\n",
      "Epoch 188/30000 Training Loss: 0.2924003303050995\n",
      "Epoch 189/30000 Training Loss: 0.29144197702407837\n",
      "Epoch 190/30000 Training Loss: 0.2804170846939087\n",
      "Epoch 191/30000 Training Loss: 0.2776232957839966\n",
      "Epoch 192/30000 Training Loss: 0.2885119318962097\n",
      "Epoch 193/30000 Training Loss: 0.2620719075202942\n",
      "Epoch 194/30000 Training Loss: 0.29863786697387695\n",
      "Epoch 195/30000 Training Loss: 0.27159571647644043\n",
      "Epoch 196/30000 Training Loss: 0.26925259828567505\n",
      "Epoch 197/30000 Training Loss: 0.26534903049468994\n",
      "Epoch 198/30000 Training Loss: 0.2668062448501587\n",
      "Epoch 199/30000 Training Loss: 0.267118901014328\n",
      "Epoch 200/30000 Training Loss: 0.2519369125366211\n",
      "Epoch 200/30000 Validation Loss: 0.30578723549842834\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.30578723549842834<=============\n",
      "Epoch 201/30000 Training Loss: 0.23995265364646912\n",
      "Epoch 202/30000 Training Loss: 0.2547798454761505\n",
      "Epoch 203/30000 Training Loss: 0.2359233796596527\n",
      "Epoch 204/30000 Training Loss: 0.27065980434417725\n",
      "Epoch 205/30000 Training Loss: 0.2545071840286255\n",
      "Epoch 206/30000 Training Loss: 0.2712301015853882\n",
      "Epoch 207/30000 Training Loss: 0.27091020345687866\n",
      "Epoch 208/30000 Training Loss: 0.2675883173942566\n",
      "Epoch 209/30000 Training Loss: 0.2426554560661316\n",
      "Epoch 210/30000 Training Loss: 0.24946343898773193\n",
      "Epoch 211/30000 Training Loss: 0.24457210302352905\n",
      "Epoch 212/30000 Training Loss: 0.268149197101593\n",
      "Epoch 213/30000 Training Loss: 0.23990094661712646\n",
      "Epoch 214/30000 Training Loss: 0.23743140697479248\n",
      "Epoch 215/30000 Training Loss: 0.2449505627155304\n",
      "Epoch 216/30000 Training Loss: 0.26242291927337646\n",
      "Epoch 217/30000 Training Loss: 0.24288520216941833\n",
      "Epoch 218/30000 Training Loss: 0.23600786924362183\n",
      "Epoch 219/30000 Training Loss: 0.24442315101623535\n",
      "Epoch 220/30000 Training Loss: 0.23942112922668457\n",
      "Epoch 221/30000 Training Loss: 0.22971415519714355\n",
      "Epoch 222/30000 Training Loss: 0.29181724786758423\n",
      "Epoch 223/30000 Training Loss: 0.23850984871387482\n",
      "Epoch 224/30000 Training Loss: 0.2456396520137787\n",
      "Epoch 225/30000 Training Loss: 0.25502076745033264\n",
      "Epoch 226/30000 Training Loss: 0.24396821856498718\n",
      "Epoch 227/30000 Training Loss: 0.22510048747062683\n",
      "Epoch 228/30000 Training Loss: 0.23718568682670593\n",
      "Epoch 229/30000 Training Loss: 0.23520267009735107\n",
      "Epoch 230/30000 Training Loss: 0.23342761397361755\n",
      "Epoch 231/30000 Training Loss: 0.2437307983636856\n",
      "Epoch 232/30000 Training Loss: 0.22860756516456604\n",
      "Epoch 233/30000 Training Loss: 0.25642621517181396\n",
      "Epoch 234/30000 Training Loss: 0.22746513783931732\n",
      "Epoch 235/30000 Training Loss: 0.24653227627277374\n",
      "Epoch 236/30000 Training Loss: 0.22801651060581207\n",
      "Epoch 237/30000 Training Loss: 0.2342538833618164\n",
      "Epoch 238/30000 Training Loss: 0.2236708402633667\n",
      "Epoch 239/30000 Training Loss: 0.2337188720703125\n",
      "Epoch 240/30000 Training Loss: 0.23987776041030884\n",
      "Epoch 241/30000 Training Loss: 0.2373420149087906\n",
      "Epoch 242/30000 Training Loss: 0.25813791155815125\n",
      "Epoch 243/30000 Training Loss: 0.24457643926143646\n",
      "Epoch 244/30000 Training Loss: 0.23715192079544067\n",
      "Epoch 245/30000 Training Loss: 0.25246289372444153\n",
      "Epoch 246/30000 Training Loss: 0.2183479517698288\n",
      "Epoch 247/30000 Training Loss: 0.24432450532913208\n",
      "Epoch 248/30000 Training Loss: 0.2205219715833664\n",
      "Epoch 249/30000 Training Loss: 0.22271859645843506\n",
      "Epoch 250/30000 Training Loss: 0.232871413230896\n",
      "Epoch 251/30000 Training Loss: 0.20169144868850708\n",
      "Epoch 252/30000 Training Loss: 0.22227060794830322\n",
      "Epoch 253/30000 Training Loss: 0.24361573159694672\n",
      "Epoch 254/30000 Training Loss: 0.20620301365852356\n",
      "Epoch 255/30000 Training Loss: 0.20824800431728363\n",
      "Epoch 256/30000 Training Loss: 0.22566290199756622\n",
      "Epoch 257/30000 Training Loss: 0.21581974625587463\n",
      "Epoch 258/30000 Training Loss: 0.21474333107471466\n",
      "Epoch 259/30000 Training Loss: 0.21224379539489746\n",
      "Epoch 260/30000 Training Loss: 0.22864478826522827\n",
      "Epoch 261/30000 Training Loss: 0.20995569229125977\n",
      "Epoch 262/30000 Training Loss: 0.20133617520332336\n",
      "Epoch 263/30000 Training Loss: 0.24106386303901672\n",
      "Epoch 264/30000 Training Loss: 0.20377841591835022\n",
      "Epoch 265/30000 Training Loss: 0.2150673270225525\n",
      "Epoch 266/30000 Training Loss: 0.21924354135990143\n",
      "Epoch 267/30000 Training Loss: 0.1950235664844513\n",
      "Epoch 268/30000 Training Loss: 0.20140153169631958\n",
      "Epoch 269/30000 Training Loss: 0.21501797437667847\n",
      "Epoch 270/30000 Training Loss: 0.20140725374221802\n",
      "Epoch 271/30000 Training Loss: 0.2291252315044403\n",
      "Epoch 272/30000 Training Loss: 0.20558741688728333\n",
      "Epoch 273/30000 Training Loss: 0.21445617079734802\n",
      "Epoch 274/30000 Training Loss: 0.2234971523284912\n",
      "Epoch 275/30000 Training Loss: 0.21178779006004333\n",
      "Epoch 276/30000 Training Loss: 0.2278963029384613\n",
      "Epoch 277/30000 Training Loss: 0.19916489720344543\n",
      "Epoch 278/30000 Training Loss: 0.2169725000858307\n",
      "Epoch 279/30000 Training Loss: 0.18560081720352173\n",
      "Epoch 280/30000 Training Loss: 0.21337318420410156\n",
      "Epoch 281/30000 Training Loss: 0.19055873155593872\n",
      "Epoch 282/30000 Training Loss: 0.22162404656410217\n",
      "Epoch 283/30000 Training Loss: 0.20399484038352966\n",
      "Epoch 284/30000 Training Loss: 0.18752485513687134\n",
      "Epoch 285/30000 Training Loss: 0.19737638533115387\n",
      "Epoch 286/30000 Training Loss: 0.19237203896045685\n",
      "Epoch 287/30000 Training Loss: 0.22496503591537476\n",
      "Epoch 288/30000 Training Loss: 0.2116641402244568\n",
      "Epoch 289/30000 Training Loss: 0.1861760914325714\n",
      "Epoch 290/30000 Training Loss: 0.19967500865459442\n",
      "Epoch 291/30000 Training Loss: 0.18946444988250732\n",
      "Epoch 292/30000 Training Loss: 0.20656433701515198\n",
      "Epoch 293/30000 Training Loss: 0.18518778681755066\n",
      "Epoch 294/30000 Training Loss: 0.1592092216014862\n",
      "Epoch 295/30000 Training Loss: 0.19349165260791779\n",
      "Epoch 296/30000 Training Loss: 0.18366947770118713\n",
      "Epoch 297/30000 Training Loss: 0.18917886912822723\n",
      "Epoch 298/30000 Training Loss: 0.19827565550804138\n",
      "Epoch 299/30000 Training Loss: 0.20140232145786285\n",
      "Epoch 300/30000 Training Loss: 0.18168696761131287\n",
      "Epoch 300/30000 Validation Loss: 0.19535434246063232\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.19535434246063232<=============\n",
      "Epoch 301/30000 Training Loss: 0.19744765758514404\n",
      "Epoch 302/30000 Training Loss: 0.2038307636976242\n",
      "Epoch 303/30000 Training Loss: 0.19934037327766418\n",
      "Epoch 304/30000 Training Loss: 0.18999725580215454\n",
      "Epoch 305/30000 Training Loss: 0.1895286589860916\n",
      "Epoch 306/30000 Training Loss: 0.1828785240650177\n",
      "Epoch 307/30000 Training Loss: 0.1850174367427826\n",
      "Epoch 308/30000 Training Loss: 0.1633485108613968\n",
      "Epoch 309/30000 Training Loss: 0.20277376472949982\n",
      "Epoch 310/30000 Training Loss: 0.17572641372680664\n",
      "Epoch 311/30000 Training Loss: 0.17873281240463257\n",
      "Epoch 312/30000 Training Loss: 0.1732095181941986\n",
      "Epoch 313/30000 Training Loss: 0.18330813944339752\n",
      "Epoch 314/30000 Training Loss: 0.17907977104187012\n",
      "Epoch 315/30000 Training Loss: 0.18782606720924377\n",
      "Epoch 316/30000 Training Loss: 0.19153816998004913\n",
      "Epoch 317/30000 Training Loss: 0.17498593032360077\n",
      "Epoch 318/30000 Training Loss: 0.20696260035037994\n",
      "Epoch 319/30000 Training Loss: 0.1815282702445984\n",
      "Epoch 320/30000 Training Loss: 0.16757960617542267\n",
      "Epoch 321/30000 Training Loss: 0.17234179377555847\n",
      "Epoch 322/30000 Training Loss: 0.16841642558574677\n",
      "Epoch 323/30000 Training Loss: 0.20008859038352966\n",
      "Epoch 324/30000 Training Loss: 0.1680191308259964\n",
      "Epoch 325/30000 Training Loss: 0.1783943772315979\n",
      "Epoch 326/30000 Training Loss: 0.20576629042625427\n",
      "Epoch 327/30000 Training Loss: 0.1789305955171585\n",
      "Epoch 328/30000 Training Loss: 0.16813892126083374\n",
      "Epoch 329/30000 Training Loss: 0.1892106831073761\n",
      "Epoch 330/30000 Training Loss: 0.19115419685840607\n",
      "Epoch 331/30000 Training Loss: 0.18733429908752441\n",
      "Epoch 332/30000 Training Loss: 0.1597568392753601\n",
      "Epoch 333/30000 Training Loss: 0.16404464840888977\n",
      "Epoch 334/30000 Training Loss: 0.18495121598243713\n",
      "Epoch 335/30000 Training Loss: 0.18530985713005066\n",
      "Epoch 336/30000 Training Loss: 0.1728614717721939\n",
      "Epoch 337/30000 Training Loss: 0.1645556539297104\n",
      "Epoch 338/30000 Training Loss: 0.16890645027160645\n",
      "Epoch 339/30000 Training Loss: 0.16743403673171997\n",
      "Epoch 340/30000 Training Loss: 0.15461373329162598\n",
      "Epoch 341/30000 Training Loss: 0.17190709710121155\n",
      "Epoch 342/30000 Training Loss: 0.18032293021678925\n",
      "Epoch 343/30000 Training Loss: 0.17035417258739471\n",
      "Epoch 344/30000 Training Loss: 0.19134369492530823\n",
      "Epoch 345/30000 Training Loss: 0.15275973081588745\n",
      "Epoch 346/30000 Training Loss: 0.17472153902053833\n",
      "Epoch 347/30000 Training Loss: 0.17591020464897156\n",
      "Epoch 348/30000 Training Loss: 0.17966118454933167\n",
      "Epoch 349/30000 Training Loss: 0.18902836740016937\n",
      "Epoch 350/30000 Training Loss: 0.18141397833824158\n",
      "Epoch 351/30000 Training Loss: 0.18786141276359558\n",
      "Epoch 352/30000 Training Loss: 0.1732093095779419\n",
      "Epoch 353/30000 Training Loss: 0.16284409165382385\n",
      "Epoch 354/30000 Training Loss: 0.1775166243314743\n",
      "Epoch 355/30000 Training Loss: 0.15910734236240387\n",
      "Epoch 356/30000 Training Loss: 0.191221684217453\n",
      "Epoch 357/30000 Training Loss: 0.1791413128376007\n",
      "Epoch 358/30000 Training Loss: 0.17253920435905457\n",
      "Epoch 359/30000 Training Loss: 0.16844087839126587\n",
      "Epoch 360/30000 Training Loss: 0.1652851104736328\n",
      "Epoch 361/30000 Training Loss: 0.16975650191307068\n",
      "Epoch 362/30000 Training Loss: 0.14682239294052124\n",
      "Epoch 363/30000 Training Loss: 0.16782796382904053\n",
      "Epoch 364/30000 Training Loss: 0.15541419386863708\n",
      "Epoch 365/30000 Training Loss: 0.16885176301002502\n",
      "Epoch 366/30000 Training Loss: 0.1530713438987732\n",
      "Epoch 367/30000 Training Loss: 0.15463611483573914\n",
      "Epoch 368/30000 Training Loss: 0.1607043445110321\n",
      "Epoch 369/30000 Training Loss: 0.14978453516960144\n",
      "Epoch 370/30000 Training Loss: 0.15184536576271057\n",
      "Epoch 371/30000 Training Loss: 0.1610589325428009\n",
      "Epoch 372/30000 Training Loss: 0.15143561363220215\n",
      "Epoch 373/30000 Training Loss: 0.1396365761756897\n",
      "Epoch 374/30000 Training Loss: 0.16534945368766785\n",
      "Epoch 375/30000 Training Loss: 0.15142332017421722\n",
      "Epoch 376/30000 Training Loss: 0.16123345494270325\n",
      "Epoch 377/30000 Training Loss: 0.15110129117965698\n",
      "Epoch 378/30000 Training Loss: 0.14899136126041412\n",
      "Epoch 379/30000 Training Loss: 0.15738610923290253\n",
      "Epoch 380/30000 Training Loss: 0.18027706444263458\n",
      "Epoch 381/30000 Training Loss: 0.14843431115150452\n",
      "Epoch 382/30000 Training Loss: 0.17221416532993317\n",
      "Epoch 383/30000 Training Loss: 0.14626723527908325\n",
      "Epoch 384/30000 Training Loss: 0.1678883284330368\n",
      "Epoch 385/30000 Training Loss: 0.17359322309494019\n",
      "Epoch 386/30000 Training Loss: 0.12537869811058044\n",
      "Epoch 387/30000 Training Loss: 0.1735389232635498\n",
      "Epoch 388/30000 Training Loss: 0.1751742660999298\n",
      "Epoch 389/30000 Training Loss: 0.1576097160577774\n",
      "Epoch 390/30000 Training Loss: 0.150779128074646\n",
      "Epoch 391/30000 Training Loss: 0.16034772992134094\n",
      "Epoch 392/30000 Training Loss: 0.13516050577163696\n",
      "Epoch 393/30000 Training Loss: 0.15762487053871155\n",
      "Epoch 394/30000 Training Loss: 0.14170566201210022\n",
      "Epoch 395/30000 Training Loss: 0.14956796169281006\n",
      "Epoch 396/30000 Training Loss: 0.14887794852256775\n",
      "Epoch 397/30000 Training Loss: 0.1377720683813095\n",
      "Epoch 398/30000 Training Loss: 0.13347336649894714\n",
      "Epoch 399/30000 Training Loss: 0.16646365821361542\n",
      "Epoch 400/30000 Training Loss: 0.15761584043502808\n",
      "Epoch 400/30000 Validation Loss: 0.14299675822257996\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.14299675822257996<=============\n",
      "Epoch 401/30000 Training Loss: 0.14156362414360046\n",
      "Epoch 402/30000 Training Loss: 0.15418480336666107\n",
      "Epoch 403/30000 Training Loss: 0.1446329951286316\n",
      "Epoch 404/30000 Training Loss: 0.15366917848587036\n",
      "Epoch 405/30000 Training Loss: 0.1410524845123291\n",
      "Epoch 406/30000 Training Loss: 0.16579127311706543\n",
      "Epoch 407/30000 Training Loss: 0.15248823165893555\n",
      "Epoch 408/30000 Training Loss: 0.14734308421611786\n",
      "Epoch 409/30000 Training Loss: 0.1614638715982437\n",
      "Epoch 410/30000 Training Loss: 0.13658957183361053\n",
      "Epoch 411/30000 Training Loss: 0.14697235822677612\n",
      "Epoch 412/30000 Training Loss: 0.13882790505886078\n",
      "Epoch 413/30000 Training Loss: 0.14825133979320526\n",
      "Epoch 414/30000 Training Loss: 0.13487491011619568\n",
      "Epoch 415/30000 Training Loss: 0.1425861418247223\n",
      "Epoch 416/30000 Training Loss: 0.13243752717971802\n",
      "Epoch 417/30000 Training Loss: 0.14787238836288452\n",
      "Epoch 418/30000 Training Loss: 0.13506832718849182\n",
      "Epoch 419/30000 Training Loss: 0.14156045019626617\n",
      "Epoch 420/30000 Training Loss: 0.15223035216331482\n",
      "Epoch 421/30000 Training Loss: 0.15261778235435486\n",
      "Epoch 422/30000 Training Loss: 0.13793998956680298\n",
      "Epoch 423/30000 Training Loss: 0.15227051079273224\n",
      "Epoch 424/30000 Training Loss: 0.1600252389907837\n",
      "Epoch 425/30000 Training Loss: 0.15723231434822083\n",
      "Epoch 426/30000 Training Loss: 0.14508084952831268\n",
      "Epoch 427/30000 Training Loss: 0.131562739610672\n",
      "Epoch 428/30000 Training Loss: 0.14646369218826294\n",
      "Epoch 429/30000 Training Loss: 0.15293416380882263\n",
      "Epoch 430/30000 Training Loss: 0.14261800050735474\n",
      "Epoch 431/30000 Training Loss: 0.13696031272411346\n",
      "Epoch 432/30000 Training Loss: 0.12565390765666962\n",
      "Epoch 433/30000 Training Loss: 0.16239556670188904\n",
      "Epoch 434/30000 Training Loss: 0.14977431297302246\n",
      "Epoch 435/30000 Training Loss: 0.13840441405773163\n",
      "Epoch 436/30000 Training Loss: 0.1415419578552246\n",
      "Epoch 437/30000 Training Loss: 0.12424588203430176\n",
      "Epoch 438/30000 Training Loss: 0.1312953680753708\n",
      "Epoch 439/30000 Training Loss: 0.1292983740568161\n",
      "Epoch 440/30000 Training Loss: 0.139239102602005\n",
      "Epoch 441/30000 Training Loss: 0.15783976018428802\n",
      "Epoch 442/30000 Training Loss: 0.1363132745027542\n",
      "Epoch 443/30000 Training Loss: 0.1233731210231781\n",
      "Epoch 444/30000 Training Loss: 0.12651364505290985\n",
      "Epoch 445/30000 Training Loss: 0.1451660394668579\n",
      "Epoch 446/30000 Training Loss: 0.12051954865455627\n",
      "Epoch 447/30000 Training Loss: 0.1306886374950409\n",
      "Epoch 448/30000 Training Loss: 0.14372774958610535\n",
      "Epoch 449/30000 Training Loss: 0.16470907628536224\n",
      "Epoch 450/30000 Training Loss: 0.14165842533111572\n",
      "Epoch 451/30000 Training Loss: 0.1515120565891266\n",
      "Epoch 452/30000 Training Loss: 0.14889764785766602\n",
      "Epoch 453/30000 Training Loss: 0.14110064506530762\n",
      "Epoch 454/30000 Training Loss: 0.1341189444065094\n",
      "Epoch 455/30000 Training Loss: 0.12452230602502823\n",
      "Epoch 456/30000 Training Loss: 0.12475713342428207\n",
      "Epoch 457/30000 Training Loss: 0.13313432037830353\n",
      "Epoch 458/30000 Training Loss: 0.1330680549144745\n",
      "Epoch 459/30000 Training Loss: 0.1318381428718567\n",
      "Epoch 460/30000 Training Loss: 0.13153493404388428\n",
      "Epoch 461/30000 Training Loss: 0.1302328109741211\n",
      "Epoch 462/30000 Training Loss: 0.14072605967521667\n",
      "Epoch 463/30000 Training Loss: 0.14929330348968506\n",
      "Epoch 464/30000 Training Loss: 0.13748499751091003\n",
      "Epoch 465/30000 Training Loss: 0.13595107197761536\n",
      "Epoch 466/30000 Training Loss: 0.14060449600219727\n",
      "Epoch 467/30000 Training Loss: 0.14401042461395264\n",
      "Epoch 468/30000 Training Loss: 0.13848917186260223\n",
      "Epoch 469/30000 Training Loss: 0.11703614890575409\n",
      "Epoch 470/30000 Training Loss: 0.15391004085540771\n",
      "Epoch 471/30000 Training Loss: 0.13350030779838562\n",
      "Epoch 472/30000 Training Loss: 0.13243868947029114\n",
      "Epoch 473/30000 Training Loss: 0.1514706313610077\n",
      "Epoch 474/30000 Training Loss: 0.1261308640241623\n",
      "Epoch 475/30000 Training Loss: 0.1484946757555008\n",
      "Epoch 476/30000 Training Loss: 0.12523600459098816\n",
      "Epoch 477/30000 Training Loss: 0.13837355375289917\n",
      "Epoch 478/30000 Training Loss: 0.15961186587810516\n",
      "Epoch 479/30000 Training Loss: 0.12950226664543152\n",
      "Epoch 480/30000 Training Loss: 0.14466001093387604\n",
      "Epoch 481/30000 Training Loss: 0.1449412703514099\n",
      "Epoch 482/30000 Training Loss: 0.12120583653450012\n",
      "Epoch 483/30000 Training Loss: 0.14520670473575592\n",
      "Epoch 484/30000 Training Loss: 0.13139058649539948\n",
      "Epoch 485/30000 Training Loss: 0.1380254030227661\n",
      "Epoch 486/30000 Training Loss: 0.13199643790721893\n",
      "Epoch 487/30000 Training Loss: 0.1222095787525177\n",
      "Epoch 488/30000 Training Loss: 0.13163867592811584\n",
      "Epoch 489/30000 Training Loss: 0.12210763990879059\n",
      "Epoch 490/30000 Training Loss: 0.13216134905815125\n",
      "Epoch 491/30000 Training Loss: 0.1332603394985199\n",
      "Epoch 492/30000 Training Loss: 0.14944902062416077\n",
      "Epoch 493/30000 Training Loss: 0.13207313418388367\n",
      "Epoch 494/30000 Training Loss: 0.13445456326007843\n",
      "Epoch 495/30000 Training Loss: 0.14437851309776306\n",
      "Epoch 496/30000 Training Loss: 0.15232577919960022\n",
      "Epoch 497/30000 Training Loss: 0.1408367156982422\n",
      "Epoch 498/30000 Training Loss: 0.11684706807136536\n",
      "Epoch 499/30000 Training Loss: 0.1298903226852417\n",
      "Epoch 500/30000 Training Loss: 0.12137101590633392\n",
      "Epoch 500/30000 Validation Loss: 0.15626423060894012\n",
      "Epoch 501/30000 Training Loss: 0.14862635731697083\n",
      "Epoch 502/30000 Training Loss: 0.11597321182489395\n",
      "Epoch 503/30000 Training Loss: 0.12377642095088959\n",
      "Epoch 504/30000 Training Loss: 0.12339729070663452\n",
      "Epoch 505/30000 Training Loss: 0.11244605481624603\n",
      "Epoch 506/30000 Training Loss: 0.10896701365709305\n",
      "Epoch 507/30000 Training Loss: 0.10472892224788666\n",
      "Epoch 508/30000 Training Loss: 0.1298440843820572\n",
      "Epoch 509/30000 Training Loss: 0.13411341607570648\n",
      "Epoch 510/30000 Training Loss: 0.13598978519439697\n",
      "Epoch 511/30000 Training Loss: 0.1275237500667572\n",
      "Epoch 512/30000 Training Loss: 0.11208394169807434\n",
      "Epoch 513/30000 Training Loss: 0.12029629945755005\n",
      "Epoch 514/30000 Training Loss: 0.12658081948757172\n",
      "Epoch 515/30000 Training Loss: 0.15473775565624237\n",
      "Epoch 516/30000 Training Loss: 0.14139659702777863\n",
      "Epoch 517/30000 Training Loss: 0.13214561343193054\n",
      "Epoch 518/30000 Training Loss: 0.11597050726413727\n",
      "Epoch 519/30000 Training Loss: 0.13100969791412354\n",
      "Epoch 520/30000 Training Loss: 0.1289266049861908\n",
      "Epoch 521/30000 Training Loss: 0.14106446504592896\n",
      "Epoch 522/30000 Training Loss: 0.12483467161655426\n",
      "Epoch 523/30000 Training Loss: 0.1272784322500229\n",
      "Epoch 524/30000 Training Loss: 0.12384513020515442\n",
      "Epoch 525/30000 Training Loss: 0.13717594742774963\n",
      "Epoch 526/30000 Training Loss: 0.15320904552936554\n",
      "Epoch 527/30000 Training Loss: 0.14252878725528717\n",
      "Epoch 528/30000 Training Loss: 0.11769221723079681\n",
      "Epoch 529/30000 Training Loss: 0.13863933086395264\n",
      "Epoch 530/30000 Training Loss: 0.13304534554481506\n",
      "Epoch 531/30000 Training Loss: 0.13539698719978333\n",
      "Epoch 532/30000 Training Loss: 0.11797510087490082\n",
      "Epoch 533/30000 Training Loss: 0.1364842653274536\n",
      "Epoch 534/30000 Training Loss: 0.11587806791067123\n",
      "Epoch 535/30000 Training Loss: 0.10868136584758759\n",
      "Epoch 536/30000 Training Loss: 0.1271073967218399\n",
      "Epoch 537/30000 Training Loss: 0.11883591115474701\n",
      "Epoch 538/30000 Training Loss: 0.11730948090553284\n",
      "Epoch 539/30000 Training Loss: 0.15326321125030518\n",
      "Epoch 540/30000 Training Loss: 0.11375544965267181\n",
      "Epoch 541/30000 Training Loss: 0.12592767179012299\n",
      "Epoch 542/30000 Training Loss: 0.11248473078012466\n",
      "Epoch 543/30000 Training Loss: 0.13589301705360413\n",
      "Epoch 544/30000 Training Loss: 0.11617255955934525\n",
      "Epoch 545/30000 Training Loss: 0.14853526651859283\n",
      "Epoch 546/30000 Training Loss: 0.1244330108165741\n",
      "Epoch 547/30000 Training Loss: 0.11971739679574966\n",
      "Epoch 548/30000 Training Loss: 0.12586328387260437\n",
      "Epoch 549/30000 Training Loss: 0.11388812959194183\n",
      "Epoch 550/30000 Training Loss: 0.11687703430652618\n",
      "Epoch 551/30000 Training Loss: 0.1193622499704361\n",
      "Epoch 552/30000 Training Loss: 0.11045952886343002\n",
      "Epoch 553/30000 Training Loss: 0.1236536055803299\n",
      "Epoch 554/30000 Training Loss: 0.11926326155662537\n",
      "Epoch 555/30000 Training Loss: 0.11844268441200256\n",
      "Epoch 556/30000 Training Loss: 0.1284048855304718\n",
      "Epoch 557/30000 Training Loss: 0.09823605418205261\n",
      "Epoch 558/30000 Training Loss: 0.10784487426280975\n",
      "Epoch 559/30000 Training Loss: 0.11698837578296661\n",
      "Epoch 560/30000 Training Loss: 0.1131623238325119\n",
      "Epoch 561/30000 Training Loss: 0.1264238953590393\n",
      "Epoch 562/30000 Training Loss: 0.12765471637248993\n",
      "Epoch 563/30000 Training Loss: 0.12045268714427948\n",
      "Epoch 564/30000 Training Loss: 0.1386461853981018\n",
      "Epoch 565/30000 Training Loss: 0.11324819922447205\n",
      "Epoch 566/30000 Training Loss: 0.149673193693161\n",
      "Epoch 567/30000 Training Loss: 0.10910658538341522\n",
      "Epoch 568/30000 Training Loss: 0.10714887082576752\n",
      "Epoch 569/30000 Training Loss: 0.133344829082489\n",
      "Epoch 570/30000 Training Loss: 0.11165837198495865\n",
      "Epoch 571/30000 Training Loss: 0.13514772057533264\n",
      "Epoch 572/30000 Training Loss: 0.12567217648029327\n",
      "Epoch 573/30000 Training Loss: 0.12499929964542389\n",
      "Epoch 574/30000 Training Loss: 0.1268898844718933\n",
      "Epoch 575/30000 Training Loss: 0.10613178461790085\n",
      "Epoch 576/30000 Training Loss: 0.11184346675872803\n",
      "Epoch 577/30000 Training Loss: 0.11607728898525238\n",
      "Epoch 578/30000 Training Loss: 0.12814536690711975\n",
      "Epoch 579/30000 Training Loss: 0.10493574291467667\n",
      "Epoch 580/30000 Training Loss: 0.09933032095432281\n",
      "Epoch 581/30000 Training Loss: 0.11015231907367706\n",
      "Epoch 582/30000 Training Loss: 0.12735387682914734\n",
      "Epoch 583/30000 Training Loss: 0.11350790411233902\n",
      "Epoch 584/30000 Training Loss: 0.11329665035009384\n",
      "Epoch 585/30000 Training Loss: 0.1255943775177002\n",
      "Epoch 586/30000 Training Loss: 0.09024468064308167\n",
      "Epoch 587/30000 Training Loss: 0.08814098685979843\n",
      "Epoch 588/30000 Training Loss: 0.10080625116825104\n",
      "Epoch 589/30000 Training Loss: 0.12285678088665009\n",
      "Epoch 590/30000 Training Loss: 0.12056379020214081\n",
      "Epoch 591/30000 Training Loss: 0.12215429544448853\n",
      "Epoch 592/30000 Training Loss: 0.11228856444358826\n",
      "Epoch 593/30000 Training Loss: 0.11108462512493134\n",
      "Epoch 594/30000 Training Loss: 0.09699230641126633\n",
      "Epoch 595/30000 Training Loss: 0.11542166769504547\n",
      "Epoch 596/30000 Training Loss: 0.10888232290744781\n",
      "Epoch 597/30000 Training Loss: 0.12429612874984741\n",
      "Epoch 598/30000 Training Loss: 0.10037001967430115\n",
      "Epoch 599/30000 Training Loss: 0.11785146594047546\n",
      "Epoch 600/30000 Training Loss: 0.11241685599088669\n",
      "Epoch 600/30000 Validation Loss: 0.12028198689222336\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.12028198689222336<=============\n",
      "Epoch 601/30000 Training Loss: 0.10392389446496964\n",
      "Epoch 602/30000 Training Loss: 0.1208476722240448\n",
      "Epoch 603/30000 Training Loss: 0.10333894938230515\n",
      "Epoch 604/30000 Training Loss: 0.11714817583560944\n",
      "Epoch 605/30000 Training Loss: 0.12059511244297028\n",
      "Epoch 606/30000 Training Loss: 0.11917699873447418\n",
      "Epoch 607/30000 Training Loss: 0.12415079772472382\n",
      "Epoch 608/30000 Training Loss: 0.11850479245185852\n",
      "Epoch 609/30000 Training Loss: 0.10800562798976898\n",
      "Epoch 610/30000 Training Loss: 0.10589922219514847\n",
      "Epoch 611/30000 Training Loss: 0.12093722820281982\n",
      "Epoch 612/30000 Training Loss: 0.11876434087753296\n",
      "Epoch 613/30000 Training Loss: 0.10488897562026978\n",
      "Epoch 614/30000 Training Loss: 0.10531178116798401\n",
      "Epoch 615/30000 Training Loss: 0.09191061556339264\n",
      "Epoch 616/30000 Training Loss: 0.11563056707382202\n",
      "Epoch 617/30000 Training Loss: 0.11246498674154282\n",
      "Epoch 618/30000 Training Loss: 0.10416840016841888\n",
      "Epoch 619/30000 Training Loss: 0.10452964901924133\n",
      "Epoch 620/30000 Training Loss: 0.09851039946079254\n",
      "Epoch 621/30000 Training Loss: 0.10459604859352112\n",
      "Epoch 622/30000 Training Loss: 0.09902332723140717\n",
      "Epoch 623/30000 Training Loss: 0.11181247979402542\n",
      "Epoch 624/30000 Training Loss: 0.10486967861652374\n",
      "Epoch 625/30000 Training Loss: 0.10885916650295258\n",
      "Epoch 626/30000 Training Loss: 0.08831994235515594\n",
      "Epoch 627/30000 Training Loss: 0.12859180569648743\n",
      "Epoch 628/30000 Training Loss: 0.10711175203323364\n",
      "Epoch 629/30000 Training Loss: 0.10134261846542358\n",
      "Epoch 630/30000 Training Loss: 0.1284950077533722\n",
      "Epoch 631/30000 Training Loss: 0.10184638202190399\n",
      "Epoch 632/30000 Training Loss: 0.1262447088956833\n",
      "Epoch 633/30000 Training Loss: 0.118531733751297\n",
      "Epoch 634/30000 Training Loss: 0.10613566637039185\n",
      "Epoch 635/30000 Training Loss: 0.11316397786140442\n",
      "Epoch 636/30000 Training Loss: 0.11636952310800552\n",
      "Epoch 637/30000 Training Loss: 0.11750514805316925\n",
      "Epoch 638/30000 Training Loss: 0.1023699939250946\n",
      "Epoch 639/30000 Training Loss: 0.11358295381069183\n",
      "Epoch 640/30000 Training Loss: 0.1071966290473938\n",
      "Epoch 641/30000 Training Loss: 0.12401904165744781\n",
      "Epoch 642/30000 Training Loss: 0.11305530369281769\n",
      "Epoch 643/30000 Training Loss: 0.09954650700092316\n",
      "Epoch 644/30000 Training Loss: 0.11143818497657776\n",
      "Epoch 645/30000 Training Loss: 0.10051430016756058\n",
      "Epoch 646/30000 Training Loss: 0.1000048816204071\n",
      "Epoch 647/30000 Training Loss: 0.09680218249559402\n",
      "Epoch 648/30000 Training Loss: 0.12766054272651672\n",
      "Epoch 649/30000 Training Loss: 0.10083235800266266\n",
      "Epoch 650/30000 Training Loss: 0.10206416249275208\n",
      "Epoch 651/30000 Training Loss: 0.10206516087055206\n",
      "Epoch 652/30000 Training Loss: 0.12592554092407227\n",
      "Epoch 653/30000 Training Loss: 0.11593654751777649\n",
      "Epoch 654/30000 Training Loss: 0.11842954158782959\n",
      "Epoch 655/30000 Training Loss: 0.12265855073928833\n",
      "Epoch 656/30000 Training Loss: 0.108544260263443\n",
      "Epoch 657/30000 Training Loss: 0.11037716269493103\n",
      "Epoch 658/30000 Training Loss: 0.10574677586555481\n",
      "Epoch 659/30000 Training Loss: 0.11156420409679413\n",
      "Epoch 660/30000 Training Loss: 0.10420908778905869\n",
      "Epoch 661/30000 Training Loss: 0.10043796896934509\n",
      "Epoch 662/30000 Training Loss: 0.12611925601959229\n",
      "Epoch 663/30000 Training Loss: 0.12531711161136627\n",
      "Epoch 664/30000 Training Loss: 0.09164387732744217\n",
      "Epoch 665/30000 Training Loss: 0.11077913641929626\n",
      "Epoch 666/30000 Training Loss: 0.11591913551092148\n",
      "Epoch 667/30000 Training Loss: 0.09858562052249908\n",
      "Epoch 668/30000 Training Loss: 0.10625793039798737\n",
      "Epoch 669/30000 Training Loss: 0.11256065964698792\n",
      "Epoch 670/30000 Training Loss: 0.11213873326778412\n",
      "Epoch 671/30000 Training Loss: 0.10611400008201599\n",
      "Epoch 672/30000 Training Loss: 0.11316973716020584\n",
      "Epoch 673/30000 Training Loss: 0.11589368432760239\n",
      "Epoch 674/30000 Training Loss: 0.12433858215808868\n",
      "Epoch 675/30000 Training Loss: 0.11303573101758957\n",
      "Epoch 676/30000 Training Loss: 0.10484115779399872\n",
      "Epoch 677/30000 Training Loss: 0.09244784712791443\n",
      "Epoch 678/30000 Training Loss: 0.09863466024398804\n",
      "Epoch 679/30000 Training Loss: 0.10563194751739502\n",
      "Epoch 680/30000 Training Loss: 0.097463458776474\n",
      "Epoch 681/30000 Training Loss: 0.10398513078689575\n",
      "Epoch 682/30000 Training Loss: 0.10474659502506256\n",
      "Epoch 683/30000 Training Loss: 0.10867857933044434\n",
      "Epoch 684/30000 Training Loss: 0.11277977377176285\n",
      "Epoch 685/30000 Training Loss: 0.11037229001522064\n",
      "Epoch 686/30000 Training Loss: 0.09275403618812561\n",
      "Epoch 687/30000 Training Loss: 0.10567555576562881\n",
      "Epoch 688/30000 Training Loss: 0.0951714739203453\n",
      "Epoch 689/30000 Training Loss: 0.1059373989701271\n",
      "Epoch 690/30000 Training Loss: 0.10218749940395355\n",
      "Epoch 691/30000 Training Loss: 0.10519289970397949\n",
      "Epoch 692/30000 Training Loss: 0.11130008101463318\n",
      "Epoch 693/30000 Training Loss: 0.09959200024604797\n",
      "Epoch 694/30000 Training Loss: 0.08136574923992157\n",
      "Epoch 695/30000 Training Loss: 0.11536378413438797\n",
      "Epoch 696/30000 Training Loss: 0.1007901132106781\n",
      "Epoch 697/30000 Training Loss: 0.09958213567733765\n",
      "Epoch 698/30000 Training Loss: 0.10615029186010361\n",
      "Epoch 699/30000 Training Loss: 0.11622636020183563\n",
      "Epoch 700/30000 Training Loss: 0.11065651476383209\n",
      "Epoch 700/30000 Validation Loss: 0.11079959571361542\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.11079959571361542<=============\n",
      "Epoch 701/30000 Training Loss: 0.09314486384391785\n",
      "Epoch 702/30000 Training Loss: 0.08552561700344086\n",
      "Epoch 703/30000 Training Loss: 0.11137010157108307\n",
      "Epoch 704/30000 Training Loss: 0.11846992373466492\n",
      "Epoch 705/30000 Training Loss: 0.11150380223989487\n",
      "Epoch 706/30000 Training Loss: 0.11506889760494232\n",
      "Epoch 707/30000 Training Loss: 0.09855236113071442\n",
      "Epoch 708/30000 Training Loss: 0.09885798394680023\n",
      "Epoch 709/30000 Training Loss: 0.10777735710144043\n",
      "Epoch 710/30000 Training Loss: 0.10136551409959793\n",
      "Epoch 711/30000 Training Loss: 0.09698210656642914\n",
      "Epoch 712/30000 Training Loss: 0.11206864565610886\n",
      "Epoch 713/30000 Training Loss: 0.1043982282280922\n",
      "Epoch 714/30000 Training Loss: 0.10109400749206543\n",
      "Epoch 715/30000 Training Loss: 0.0847768783569336\n",
      "Epoch 716/30000 Training Loss: 0.1060670018196106\n",
      "Epoch 717/30000 Training Loss: 0.10429108887910843\n",
      "Epoch 718/30000 Training Loss: 0.097669318318367\n",
      "Epoch 719/30000 Training Loss: 0.10388556122779846\n",
      "Epoch 720/30000 Training Loss: 0.09817548096179962\n",
      "Epoch 721/30000 Training Loss: 0.10291078686714172\n",
      "Epoch 722/30000 Training Loss: 0.09609270095825195\n",
      "Epoch 723/30000 Training Loss: 0.12418877333402634\n",
      "Epoch 724/30000 Training Loss: 0.09564119577407837\n",
      "Epoch 725/30000 Training Loss: 0.08752892166376114\n",
      "Epoch 726/30000 Training Loss: 0.08380962908267975\n",
      "Epoch 727/30000 Training Loss: 0.10955601185560226\n",
      "Epoch 728/30000 Training Loss: 0.0965408980846405\n",
      "Epoch 729/30000 Training Loss: 0.11050660908222198\n",
      "Epoch 730/30000 Training Loss: 0.10479255020618439\n",
      "Epoch 731/30000 Training Loss: 0.10486330091953278\n",
      "Epoch 732/30000 Training Loss: 0.08857280761003494\n",
      "Epoch 733/30000 Training Loss: 0.10122771561145782\n",
      "Epoch 734/30000 Training Loss: 0.11183655261993408\n",
      "Epoch 735/30000 Training Loss: 0.09622757136821747\n",
      "Epoch 736/30000 Training Loss: 0.08263171464204788\n",
      "Epoch 737/30000 Training Loss: 0.11112652719020844\n",
      "Epoch 738/30000 Training Loss: 0.09313564002513885\n",
      "Epoch 739/30000 Training Loss: 0.10593082010746002\n",
      "Epoch 740/30000 Training Loss: 0.09680237621068954\n",
      "Epoch 741/30000 Training Loss: 0.10435885190963745\n",
      "Epoch 742/30000 Training Loss: 0.1174267828464508\n",
      "Epoch 743/30000 Training Loss: 0.11100764572620392\n",
      "Epoch 744/30000 Training Loss: 0.11986750364303589\n",
      "Epoch 745/30000 Training Loss: 0.0861826241016388\n",
      "Epoch 746/30000 Training Loss: 0.08373534679412842\n",
      "Epoch 747/30000 Training Loss: 0.09818147122859955\n",
      "Epoch 748/30000 Training Loss: 0.09230926632881165\n",
      "Epoch 749/30000 Training Loss: 0.10253357887268066\n",
      "Epoch 750/30000 Training Loss: 0.10451661050319672\n",
      "Epoch 751/30000 Training Loss: 0.08209232985973358\n",
      "Epoch 752/30000 Training Loss: 0.10048790276050568\n",
      "Epoch 753/30000 Training Loss: 0.10166578739881516\n",
      "Epoch 754/30000 Training Loss: 0.09308471530675888\n",
      "Epoch 755/30000 Training Loss: 0.08944080024957657\n",
      "Epoch 756/30000 Training Loss: 0.09266012907028198\n",
      "Epoch 757/30000 Training Loss: 0.11948616802692413\n",
      "Epoch 758/30000 Training Loss: 0.08766650408506393\n",
      "Epoch 759/30000 Training Loss: 0.0961349606513977\n",
      "Epoch 760/30000 Training Loss: 0.09179360419511795\n",
      "Epoch 761/30000 Training Loss: 0.10099522769451141\n",
      "Epoch 762/30000 Training Loss: 0.11168111860752106\n",
      "Epoch 763/30000 Training Loss: 0.1052718311548233\n",
      "Epoch 764/30000 Training Loss: 0.08535383641719818\n",
      "Epoch 765/30000 Training Loss: 0.0891648530960083\n",
      "Epoch 766/30000 Training Loss: 0.10907134413719177\n",
      "Epoch 767/30000 Training Loss: 0.10106108337640762\n",
      "Epoch 768/30000 Training Loss: 0.09181055426597595\n",
      "Epoch 769/30000 Training Loss: 0.09316402673721313\n",
      "Epoch 770/30000 Training Loss: 0.0985717624425888\n",
      "Epoch 771/30000 Training Loss: 0.10011868923902512\n",
      "Epoch 772/30000 Training Loss: 0.08685645461082458\n",
      "Epoch 773/30000 Training Loss: 0.11159132421016693\n",
      "Epoch 774/30000 Training Loss: 0.1067853718996048\n",
      "Epoch 775/30000 Training Loss: 0.10847098380327225\n",
      "Epoch 776/30000 Training Loss: 0.0775376558303833\n",
      "Epoch 777/30000 Training Loss: 0.08292552083730698\n",
      "Epoch 778/30000 Training Loss: 0.07545579224824905\n",
      "Epoch 779/30000 Training Loss: 0.09970829635858536\n",
      "Epoch 780/30000 Training Loss: 0.11134213209152222\n",
      "Epoch 781/30000 Training Loss: 0.10738155990839005\n",
      "Epoch 782/30000 Training Loss: 0.0969809740781784\n",
      "Epoch 783/30000 Training Loss: 0.11477169394493103\n",
      "Epoch 784/30000 Training Loss: 0.08293981850147247\n",
      "Epoch 785/30000 Training Loss: 0.0960245430469513\n",
      "Epoch 786/30000 Training Loss: 0.08656798303127289\n",
      "Epoch 787/30000 Training Loss: 0.0918818786740303\n",
      "Epoch 788/30000 Training Loss: 0.08632099628448486\n",
      "Epoch 789/30000 Training Loss: 0.09079378843307495\n",
      "Epoch 790/30000 Training Loss: 0.09603998064994812\n",
      "Epoch 791/30000 Training Loss: 0.09717309474945068\n",
      "Epoch 792/30000 Training Loss: 0.0981602817773819\n",
      "Epoch 793/30000 Training Loss: 0.11304129660129547\n",
      "Epoch 794/30000 Training Loss: 0.1027606725692749\n",
      "Epoch 795/30000 Training Loss: 0.09336744248867035\n",
      "Epoch 796/30000 Training Loss: 0.08891163766384125\n",
      "Epoch 797/30000 Training Loss: 0.0953645184636116\n",
      "Epoch 798/30000 Training Loss: 0.11596624553203583\n",
      "Epoch 799/30000 Training Loss: 0.11019313335418701\n",
      "Epoch 800/30000 Training Loss: 0.10415259003639221\n",
      "Epoch 800/30000 Validation Loss: 0.0984039455652237\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0984039455652237<=============\n",
      "Epoch 801/30000 Training Loss: 0.09203171730041504\n",
      "Epoch 802/30000 Training Loss: 0.10078773647546768\n",
      "Epoch 803/30000 Training Loss: 0.1061871126294136\n",
      "Epoch 804/30000 Training Loss: 0.10838115215301514\n",
      "Epoch 805/30000 Training Loss: 0.10311819612979889\n",
      "Epoch 806/30000 Training Loss: 0.08784584701061249\n",
      "Epoch 807/30000 Training Loss: 0.09388339519500732\n",
      "Epoch 808/30000 Training Loss: 0.10926233232021332\n",
      "Epoch 809/30000 Training Loss: 0.1186685562133789\n",
      "Epoch 810/30000 Training Loss: 0.09156952798366547\n",
      "Epoch 811/30000 Training Loss: 0.09007087349891663\n",
      "Epoch 812/30000 Training Loss: 0.11645589768886566\n",
      "Epoch 813/30000 Training Loss: 0.10225750505924225\n",
      "Epoch 814/30000 Training Loss: 0.09515857696533203\n",
      "Epoch 815/30000 Training Loss: 0.11332486569881439\n",
      "Epoch 816/30000 Training Loss: 0.0983230322599411\n",
      "Epoch 817/30000 Training Loss: 0.10619701445102692\n",
      "Epoch 818/30000 Training Loss: 0.10048970580101013\n",
      "Epoch 819/30000 Training Loss: 0.08600208163261414\n",
      "Epoch 820/30000 Training Loss: 0.08568544685840607\n",
      "Epoch 821/30000 Training Loss: 0.11083591729402542\n",
      "Epoch 822/30000 Training Loss: 0.0950440838932991\n",
      "Epoch 823/30000 Training Loss: 0.0862654447555542\n",
      "Epoch 824/30000 Training Loss: 0.08569493889808655\n",
      "Epoch 825/30000 Training Loss: 0.09253884851932526\n",
      "Epoch 826/30000 Training Loss: 0.1107102781534195\n",
      "Epoch 827/30000 Training Loss: 0.11334384977817535\n",
      "Epoch 828/30000 Training Loss: 0.10230481624603271\n",
      "Epoch 829/30000 Training Loss: 0.10453993082046509\n",
      "Epoch 830/30000 Training Loss: 0.09457706660032272\n",
      "Epoch 831/30000 Training Loss: 0.08177098631858826\n",
      "Epoch 832/30000 Training Loss: 0.1025972068309784\n",
      "Epoch 833/30000 Training Loss: 0.1080867275595665\n",
      "Epoch 834/30000 Training Loss: 0.0998319536447525\n",
      "Epoch 835/30000 Training Loss: 0.09021185338497162\n",
      "Epoch 836/30000 Training Loss: 0.08285645395517349\n",
      "Epoch 837/30000 Training Loss: 0.07912473380565643\n",
      "Epoch 838/30000 Training Loss: 0.08330947905778885\n",
      "Epoch 839/30000 Training Loss: 0.09551426768302917\n",
      "Epoch 840/30000 Training Loss: 0.07927189022302628\n",
      "Epoch 841/30000 Training Loss: 0.11678189039230347\n",
      "Epoch 842/30000 Training Loss: 0.09458386152982712\n",
      "Epoch 843/30000 Training Loss: 0.09341078996658325\n",
      "Epoch 844/30000 Training Loss: 0.09242285788059235\n",
      "Epoch 845/30000 Training Loss: 0.09937144815921783\n",
      "Epoch 846/30000 Training Loss: 0.08586589992046356\n",
      "Epoch 847/30000 Training Loss: 0.0968402773141861\n",
      "Epoch 848/30000 Training Loss: 0.08349978923797607\n",
      "Epoch 849/30000 Training Loss: 0.11468438059091568\n",
      "Epoch 850/30000 Training Loss: 0.08708661794662476\n",
      "Epoch 851/30000 Training Loss: 0.09647327661514282\n",
      "Epoch 852/30000 Training Loss: 0.08047658205032349\n",
      "Epoch 853/30000 Training Loss: 0.10511431843042374\n",
      "Epoch 854/30000 Training Loss: 0.1043502539396286\n",
      "Epoch 855/30000 Training Loss: 0.10125339031219482\n",
      "Epoch 856/30000 Training Loss: 0.09961295127868652\n",
      "Epoch 857/30000 Training Loss: 0.1061018630862236\n",
      "Epoch 858/30000 Training Loss: 0.10825762897729874\n",
      "Epoch 859/30000 Training Loss: 0.07453903555870056\n",
      "Epoch 860/30000 Training Loss: 0.0888054370880127\n",
      "Epoch 861/30000 Training Loss: 0.0834457278251648\n",
      "Epoch 862/30000 Training Loss: 0.08762701600790024\n",
      "Epoch 863/30000 Training Loss: 0.08408966660499573\n",
      "Epoch 864/30000 Training Loss: 0.08198045939207077\n",
      "Epoch 865/30000 Training Loss: 0.0821601003408432\n",
      "Epoch 866/30000 Training Loss: 0.10452196002006531\n",
      "Epoch 867/30000 Training Loss: 0.1007593423128128\n",
      "Epoch 868/30000 Training Loss: 0.08300982415676117\n",
      "Epoch 869/30000 Training Loss: 0.08665934950113297\n",
      "Epoch 870/30000 Training Loss: 0.09495130181312561\n",
      "Epoch 871/30000 Training Loss: 0.09484447538852692\n",
      "Epoch 872/30000 Training Loss: 0.09280633926391602\n",
      "Epoch 873/30000 Training Loss: 0.10410124808549881\n",
      "Epoch 874/30000 Training Loss: 0.10933978110551834\n",
      "Epoch 875/30000 Training Loss: 0.08073654025793076\n",
      "Epoch 876/30000 Training Loss: 0.09401561319828033\n",
      "Epoch 877/30000 Training Loss: 0.11566275358200073\n",
      "Epoch 878/30000 Training Loss: 0.0899905115365982\n",
      "Epoch 879/30000 Training Loss: 0.10250638425350189\n",
      "Epoch 880/30000 Training Loss: 0.08957851678133011\n",
      "Epoch 881/30000 Training Loss: 0.09034108370542526\n",
      "Epoch 882/30000 Training Loss: 0.10363712906837463\n",
      "Epoch 883/30000 Training Loss: 0.07341563701629639\n",
      "Epoch 884/30000 Training Loss: 0.09317260980606079\n",
      "Epoch 885/30000 Training Loss: 0.09047795832157135\n",
      "Epoch 886/30000 Training Loss: 0.09068955481052399\n",
      "Epoch 887/30000 Training Loss: 0.10505221784114838\n",
      "Epoch 888/30000 Training Loss: 0.07068943977355957\n",
      "Epoch 889/30000 Training Loss: 0.09322245419025421\n",
      "Epoch 890/30000 Training Loss: 0.08992978185415268\n",
      "Epoch 891/30000 Training Loss: 0.08056093007326126\n",
      "Epoch 892/30000 Training Loss: 0.08716531842947006\n",
      "Epoch 893/30000 Training Loss: 0.08896821737289429\n",
      "Epoch 894/30000 Training Loss: 0.08370241522789001\n",
      "Epoch 895/30000 Training Loss: 0.07957185804843903\n",
      "Epoch 896/30000 Training Loss: 0.08086439967155457\n",
      "Epoch 897/30000 Training Loss: 0.09454536437988281\n",
      "Epoch 898/30000 Training Loss: 0.11327601224184036\n",
      "Epoch 899/30000 Training Loss: 0.08924880623817444\n",
      "Epoch 900/30000 Training Loss: 0.08656667917966843\n",
      "Epoch 900/30000 Validation Loss: 0.08117809891700745\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.08117809891700745<=============\n",
      "Epoch 901/30000 Training Loss: 0.09153959155082703\n",
      "Epoch 902/30000 Training Loss: 0.08681833744049072\n",
      "Epoch 903/30000 Training Loss: 0.10718512535095215\n",
      "Epoch 904/30000 Training Loss: 0.1122383251786232\n",
      "Epoch 905/30000 Training Loss: 0.08545873314142227\n",
      "Epoch 906/30000 Training Loss: 0.10558421164751053\n",
      "Epoch 907/30000 Training Loss: 0.09788504242897034\n",
      "Epoch 908/30000 Training Loss: 0.08437968045473099\n",
      "Epoch 909/30000 Training Loss: 0.08147957175970078\n",
      "Epoch 910/30000 Training Loss: 0.07635216414928436\n",
      "Epoch 911/30000 Training Loss: 0.08813462406396866\n",
      "Epoch 912/30000 Training Loss: 0.10545839369297028\n",
      "Epoch 913/30000 Training Loss: 0.09244804084300995\n",
      "Epoch 914/30000 Training Loss: 0.08769817650318146\n",
      "Epoch 915/30000 Training Loss: 0.08949557691812515\n",
      "Epoch 916/30000 Training Loss: 0.08328329771757126\n",
      "Epoch 917/30000 Training Loss: 0.0921383947134018\n",
      "Epoch 918/30000 Training Loss: 0.09583669900894165\n",
      "Epoch 919/30000 Training Loss: 0.10158824920654297\n",
      "Epoch 920/30000 Training Loss: 0.09183892607688904\n",
      "Epoch 921/30000 Training Loss: 0.08954977989196777\n",
      "Epoch 922/30000 Training Loss: 0.10440696030855179\n",
      "Epoch 923/30000 Training Loss: 0.09019650518894196\n",
      "Epoch 924/30000 Training Loss: 0.08133125305175781\n",
      "Epoch 925/30000 Training Loss: 0.09743629395961761\n",
      "Epoch 926/30000 Training Loss: 0.07076755166053772\n",
      "Epoch 927/30000 Training Loss: 0.11544736474752426\n",
      "Epoch 928/30000 Training Loss: 0.09534533321857452\n",
      "Epoch 929/30000 Training Loss: 0.0705433040857315\n",
      "Epoch 930/30000 Training Loss: 0.08983297646045685\n",
      "Epoch 931/30000 Training Loss: 0.08662544190883636\n",
      "Epoch 932/30000 Training Loss: 0.10184797644615173\n",
      "Epoch 933/30000 Training Loss: 0.09043361991643906\n",
      "Epoch 934/30000 Training Loss: 0.09673766791820526\n",
      "Epoch 935/30000 Training Loss: 0.10235412418842316\n",
      "Epoch 936/30000 Training Loss: 0.08378803730010986\n",
      "Epoch 937/30000 Training Loss: 0.09743823111057281\n",
      "Epoch 938/30000 Training Loss: 0.09428952634334564\n",
      "Epoch 939/30000 Training Loss: 0.09402284026145935\n",
      "Epoch 940/30000 Training Loss: 0.0824851244688034\n",
      "Epoch 941/30000 Training Loss: 0.09263187646865845\n",
      "Epoch 942/30000 Training Loss: 0.08642629534006119\n",
      "Epoch 943/30000 Training Loss: 0.10531571507453918\n",
      "Epoch 944/30000 Training Loss: 0.07727831602096558\n",
      "Epoch 945/30000 Training Loss: 0.08989040553569794\n",
      "Epoch 946/30000 Training Loss: 0.08948187530040741\n",
      "Epoch 947/30000 Training Loss: 0.09678344428539276\n",
      "Epoch 948/30000 Training Loss: 0.09408018738031387\n",
      "Epoch 949/30000 Training Loss: 0.10014039278030396\n",
      "Epoch 950/30000 Training Loss: 0.10724376142024994\n",
      "Epoch 951/30000 Training Loss: 0.10686799138784409\n",
      "Epoch 952/30000 Training Loss: 0.10059398412704468\n",
      "Epoch 953/30000 Training Loss: 0.09894051402807236\n",
      "Epoch 954/30000 Training Loss: 0.10275189578533173\n",
      "Epoch 955/30000 Training Loss: 0.08708973973989487\n",
      "Epoch 956/30000 Training Loss: 0.07910795509815216\n",
      "Epoch 957/30000 Training Loss: 0.10052622109651566\n",
      "Epoch 958/30000 Training Loss: 0.07935886085033417\n",
      "Epoch 959/30000 Training Loss: 0.09154235571622849\n",
      "Epoch 960/30000 Training Loss: 0.09508644789457321\n",
      "Epoch 961/30000 Training Loss: 0.09704665094614029\n",
      "Epoch 962/30000 Training Loss: 0.08953409641981125\n",
      "Epoch 963/30000 Training Loss: 0.0812210962176323\n",
      "Epoch 964/30000 Training Loss: 0.07534852623939514\n",
      "Epoch 965/30000 Training Loss: 0.06815382838249207\n",
      "Epoch 966/30000 Training Loss: 0.08880919218063354\n",
      "Epoch 967/30000 Training Loss: 0.08873920142650604\n",
      "Epoch 968/30000 Training Loss: 0.09403704106807709\n",
      "Epoch 969/30000 Training Loss: 0.09850063174962997\n",
      "Epoch 970/30000 Training Loss: 0.07589200884103775\n",
      "Epoch 971/30000 Training Loss: 0.08173468708992004\n",
      "Epoch 972/30000 Training Loss: 0.10881157219409943\n",
      "Epoch 973/30000 Training Loss: 0.09339770674705505\n",
      "Epoch 974/30000 Training Loss: 0.07174406945705414\n",
      "Epoch 975/30000 Training Loss: 0.08102580159902573\n",
      "Epoch 976/30000 Training Loss: 0.10372187197208405\n",
      "Epoch 977/30000 Training Loss: 0.09035368263721466\n",
      "Epoch 978/30000 Training Loss: 0.09845460951328278\n",
      "Epoch 979/30000 Training Loss: 0.08620503544807434\n",
      "Epoch 980/30000 Training Loss: 0.094375841319561\n",
      "Epoch 981/30000 Training Loss: 0.07821967452764511\n",
      "Epoch 982/30000 Training Loss: 0.07341369986534119\n",
      "Epoch 983/30000 Training Loss: 0.09491373598575592\n",
      "Epoch 984/30000 Training Loss: 0.07668693363666534\n",
      "Epoch 985/30000 Training Loss: 0.09212224185466766\n",
      "Epoch 986/30000 Training Loss: 0.09980027377605438\n",
      "Epoch 987/30000 Training Loss: 0.08176687359809875\n",
      "Epoch 988/30000 Training Loss: 0.08043159544467926\n",
      "Epoch 989/30000 Training Loss: 0.07483075559139252\n",
      "Epoch 990/30000 Training Loss: 0.08977197110652924\n",
      "Epoch 991/30000 Training Loss: 0.09206655621528625\n",
      "Epoch 992/30000 Training Loss: 0.09426308423280716\n",
      "Epoch 993/30000 Training Loss: 0.09359865635633469\n",
      "Epoch 994/30000 Training Loss: 0.09608113765716553\n",
      "Epoch 995/30000 Training Loss: 0.0867069661617279\n",
      "Epoch 996/30000 Training Loss: 0.10222744941711426\n",
      "Epoch 997/30000 Training Loss: 0.07578372955322266\n",
      "Epoch 998/30000 Training Loss: 0.09320390224456787\n",
      "Epoch 999/30000 Training Loss: 0.10075664520263672\n",
      "Epoch 1000/30000 Training Loss: 0.08987162262201309\n",
      "Epoch 1000/30000 Validation Loss: 0.08386814594268799\n",
      "Epoch 1001/30000 Training Loss: 0.09292685985565186\n",
      "Epoch 1002/30000 Training Loss: 0.08699068427085876\n",
      "Epoch 1003/30000 Training Loss: 0.07619315385818481\n",
      "Epoch 1004/30000 Training Loss: 0.0864046961069107\n",
      "Epoch 1005/30000 Training Loss: 0.07946839928627014\n",
      "Epoch 1006/30000 Training Loss: 0.08985813707113266\n",
      "Epoch 1007/30000 Training Loss: 0.09382130205631256\n",
      "Epoch 1008/30000 Training Loss: 0.0960872545838356\n",
      "Epoch 1009/30000 Training Loss: 0.08619324862957001\n",
      "Epoch 1010/30000 Training Loss: 0.08515815436840057\n",
      "Epoch 1011/30000 Training Loss: 0.09290628135204315\n",
      "Epoch 1012/30000 Training Loss: 0.11374000459909439\n",
      "Epoch 1013/30000 Training Loss: 0.09112782031297684\n",
      "Epoch 1014/30000 Training Loss: 0.07025155425071716\n",
      "Epoch 1015/30000 Training Loss: 0.08786477148532867\n",
      "Epoch 1016/30000 Training Loss: 0.08532808721065521\n",
      "Epoch 1017/30000 Training Loss: 0.0918809026479721\n",
      "Epoch 1018/30000 Training Loss: 0.0694391131401062\n",
      "Epoch 1019/30000 Training Loss: 0.09629882872104645\n",
      "Epoch 1020/30000 Training Loss: 0.06526622176170349\n",
      "Epoch 1021/30000 Training Loss: 0.08291871845722198\n",
      "Epoch 1022/30000 Training Loss: 0.08001509308815002\n",
      "Epoch 1023/30000 Training Loss: 0.09372610598802567\n",
      "Epoch 1024/30000 Training Loss: 0.10611489415168762\n",
      "Epoch 1025/30000 Training Loss: 0.09810003638267517\n",
      "Epoch 1026/30000 Training Loss: 0.08437446504831314\n",
      "Epoch 1027/30000 Training Loss: 0.08803984522819519\n",
      "Epoch 1028/30000 Training Loss: 0.09159690141677856\n",
      "Epoch 1029/30000 Training Loss: 0.09179583191871643\n",
      "Epoch 1030/30000 Training Loss: 0.07590825855731964\n",
      "Epoch 1031/30000 Training Loss: 0.08673910796642303\n",
      "Epoch 1032/30000 Training Loss: 0.07638145983219147\n",
      "Epoch 1033/30000 Training Loss: 0.0791327953338623\n",
      "Epoch 1034/30000 Training Loss: 0.08875251561403275\n",
      "Epoch 1035/30000 Training Loss: 0.09191638231277466\n",
      "Epoch 1036/30000 Training Loss: 0.09294728189706802\n",
      "Epoch 1037/30000 Training Loss: 0.0794048011302948\n",
      "Epoch 1038/30000 Training Loss: 0.07861427962779999\n",
      "Epoch 1039/30000 Training Loss: 0.07815544307231903\n",
      "Epoch 1040/30000 Training Loss: 0.06831938028335571\n",
      "Epoch 1041/30000 Training Loss: 0.08681602776050568\n",
      "Epoch 1042/30000 Training Loss: 0.05909664183855057\n",
      "Epoch 1043/30000 Training Loss: 0.09603656083345413\n",
      "Epoch 1044/30000 Training Loss: 0.08297687768936157\n",
      "Epoch 1045/30000 Training Loss: 0.0750875324010849\n",
      "Epoch 1046/30000 Training Loss: 0.07145467400550842\n",
      "Epoch 1047/30000 Training Loss: 0.09339414536952972\n",
      "Epoch 1048/30000 Training Loss: 0.09000980854034424\n",
      "Epoch 1049/30000 Training Loss: 0.09063594788312912\n",
      "Epoch 1050/30000 Training Loss: 0.08278681337833405\n",
      "Epoch 1051/30000 Training Loss: 0.08929795771837234\n",
      "Epoch 1052/30000 Training Loss: 0.09447341412305832\n",
      "Epoch 1053/30000 Training Loss: 0.11052508652210236\n",
      "Epoch 1054/30000 Training Loss: 0.07093177735805511\n",
      "Epoch 1055/30000 Training Loss: 0.09337078034877777\n",
      "Epoch 1056/30000 Training Loss: 0.07481934130191803\n",
      "Epoch 1057/30000 Training Loss: 0.0728483647108078\n",
      "Epoch 1058/30000 Training Loss: 0.08319973945617676\n",
      "Epoch 1059/30000 Training Loss: 0.09527911245822906\n",
      "Epoch 1060/30000 Training Loss: 0.09340046346187592\n",
      "Epoch 1061/30000 Training Loss: 0.07113775610923767\n",
      "Epoch 1062/30000 Training Loss: 0.07686568051576614\n",
      "Epoch 1063/30000 Training Loss: 0.08561433851718903\n",
      "Epoch 1064/30000 Training Loss: 0.06941980123519897\n",
      "Epoch 1065/30000 Training Loss: 0.10787305235862732\n",
      "Epoch 1066/30000 Training Loss: 0.08112432062625885\n",
      "Epoch 1067/30000 Training Loss: 0.07135095447301865\n",
      "Epoch 1068/30000 Training Loss: 0.07694177329540253\n",
      "Epoch 1069/30000 Training Loss: 0.10284902155399323\n",
      "Epoch 1070/30000 Training Loss: 0.06309928745031357\n",
      "Epoch 1071/30000 Training Loss: 0.08672982454299927\n",
      "Epoch 1072/30000 Training Loss: 0.08809243887662888\n",
      "Epoch 1073/30000 Training Loss: 0.08058531582355499\n",
      "Epoch 1074/30000 Training Loss: 0.0840030163526535\n",
      "Epoch 1075/30000 Training Loss: 0.07528351247310638\n",
      "Epoch 1076/30000 Training Loss: 0.1090516448020935\n",
      "Epoch 1077/30000 Training Loss: 0.06571309268474579\n",
      "Epoch 1078/30000 Training Loss: 0.10120010375976562\n",
      "Epoch 1079/30000 Training Loss: 0.07617255300283432\n",
      "Epoch 1080/30000 Training Loss: 0.09761124104261398\n",
      "Epoch 1081/30000 Training Loss: 0.09042862057685852\n",
      "Epoch 1082/30000 Training Loss: 0.0821089893579483\n",
      "Epoch 1083/30000 Training Loss: 0.08008340746164322\n",
      "Epoch 1084/30000 Training Loss: 0.10801663994789124\n",
      "Epoch 1085/30000 Training Loss: 0.07150055468082428\n",
      "Epoch 1086/30000 Training Loss: 0.10689768195152283\n",
      "Epoch 1087/30000 Training Loss: 0.09262659400701523\n",
      "Epoch 1088/30000 Training Loss: 0.07379481941461563\n",
      "Epoch 1089/30000 Training Loss: 0.08336347341537476\n",
      "Epoch 1090/30000 Training Loss: 0.06012438237667084\n",
      "Epoch 1091/30000 Training Loss: 0.08258499205112457\n",
      "Epoch 1092/30000 Training Loss: 0.09537748992443085\n",
      "Epoch 1093/30000 Training Loss: 0.09010981023311615\n",
      "Epoch 1094/30000 Training Loss: 0.08281245827674866\n",
      "Epoch 1095/30000 Training Loss: 0.08864033967256546\n",
      "Epoch 1096/30000 Training Loss: 0.07470864057540894\n",
      "Epoch 1097/30000 Training Loss: 0.06103542074561119\n",
      "Epoch 1098/30000 Training Loss: 0.0943530723452568\n",
      "Epoch 1099/30000 Training Loss: 0.08088681846857071\n",
      "Epoch 1100/30000 Training Loss: 0.09090719372034073\n",
      "Epoch 1100/30000 Validation Loss: 0.08614421635866165\n",
      "Epoch 1101/30000 Training Loss: 0.0784590095281601\n",
      "Epoch 1102/30000 Training Loss: 0.09884513914585114\n",
      "Epoch 1103/30000 Training Loss: 0.07059507817029953\n",
      "Epoch 1104/30000 Training Loss: 0.08794352412223816\n",
      "Epoch 1105/30000 Training Loss: 0.08734381198883057\n",
      "Epoch 1106/30000 Training Loss: 0.08622362464666367\n",
      "Epoch 1107/30000 Training Loss: 0.09003935754299164\n",
      "Epoch 1108/30000 Training Loss: 0.08809413015842438\n",
      "Epoch 1109/30000 Training Loss: 0.06890974938869476\n",
      "Epoch 1110/30000 Training Loss: 0.0786113291978836\n",
      "Epoch 1111/30000 Training Loss: 0.0879882425069809\n",
      "Epoch 1112/30000 Training Loss: 0.08378319442272186\n",
      "Epoch 1113/30000 Training Loss: 0.09179012477397919\n",
      "Epoch 1114/30000 Training Loss: 0.0804092064499855\n",
      "Epoch 1115/30000 Training Loss: 0.09425120055675507\n",
      "Epoch 1116/30000 Training Loss: 0.09531108289957047\n",
      "Epoch 1117/30000 Training Loss: 0.09812797605991364\n",
      "Epoch 1118/30000 Training Loss: 0.07718796283006668\n",
      "Epoch 1119/30000 Training Loss: 0.07301507890224457\n",
      "Epoch 1120/30000 Training Loss: 0.0908391922712326\n",
      "Epoch 1121/30000 Training Loss: 0.09302275627851486\n",
      "Epoch 1122/30000 Training Loss: 0.0790085718035698\n",
      "Epoch 1123/30000 Training Loss: 0.07872478663921356\n",
      "Epoch 1124/30000 Training Loss: 0.07036493718624115\n",
      "Epoch 1125/30000 Training Loss: 0.08976612985134125\n",
      "Epoch 1126/30000 Training Loss: 0.0957157090306282\n",
      "Epoch 1127/30000 Training Loss: 0.07770461589097977\n",
      "Epoch 1128/30000 Training Loss: 0.06650242209434509\n",
      "Epoch 1129/30000 Training Loss: 0.10126602649688721\n",
      "Epoch 1130/30000 Training Loss: 0.08724841475486755\n",
      "Epoch 1131/30000 Training Loss: 0.08740261942148209\n",
      "Epoch 1132/30000 Training Loss: 0.07947464287281036\n",
      "Epoch 1133/30000 Training Loss: 0.10453203320503235\n",
      "Epoch 1134/30000 Training Loss: 0.09703472256660461\n",
      "Epoch 1135/30000 Training Loss: 0.07312457263469696\n",
      "Epoch 1136/30000 Training Loss: 0.07755666226148605\n",
      "Epoch 1137/30000 Training Loss: 0.08020009100437164\n",
      "Epoch 1138/30000 Training Loss: 0.07898789644241333\n",
      "Epoch 1139/30000 Training Loss: 0.08680523931980133\n",
      "Epoch 1140/30000 Training Loss: 0.06638973206281662\n",
      "Epoch 1141/30000 Training Loss: 0.08586975932121277\n",
      "Epoch 1142/30000 Training Loss: 0.0889340490102768\n",
      "Epoch 1143/30000 Training Loss: 0.09398800134658813\n",
      "Epoch 1144/30000 Training Loss: 0.07836072891950607\n",
      "Epoch 1145/30000 Training Loss: 0.08319880068302155\n",
      "Epoch 1146/30000 Training Loss: 0.07104054093360901\n",
      "Epoch 1147/30000 Training Loss: 0.0883980467915535\n",
      "Epoch 1148/30000 Training Loss: 0.08027278631925583\n",
      "Epoch 1149/30000 Training Loss: 0.07832974195480347\n",
      "Epoch 1150/30000 Training Loss: 0.0846741795539856\n",
      "Epoch 1151/30000 Training Loss: 0.08251233398914337\n",
      "Epoch 1152/30000 Training Loss: 0.08321566134691238\n",
      "Epoch 1153/30000 Training Loss: 0.08580765128135681\n",
      "Epoch 1154/30000 Training Loss: 0.08145125210285187\n",
      "Epoch 1155/30000 Training Loss: 0.07407566159963608\n",
      "Epoch 1156/30000 Training Loss: 0.08507277816534042\n",
      "Epoch 1157/30000 Training Loss: 0.07337482273578644\n",
      "Epoch 1158/30000 Training Loss: 0.0907004326581955\n",
      "Epoch 1159/30000 Training Loss: 0.07204188406467438\n",
      "Epoch 1160/30000 Training Loss: 0.07791079580783844\n",
      "Epoch 1161/30000 Training Loss: 0.07267719507217407\n",
      "Epoch 1162/30000 Training Loss: 0.07265008240938187\n",
      "Epoch 1163/30000 Training Loss: 0.07372604310512543\n",
      "Epoch 1164/30000 Training Loss: 0.09351260215044022\n",
      "Epoch 1165/30000 Training Loss: 0.086331307888031\n",
      "Epoch 1166/30000 Training Loss: 0.0821206271648407\n",
      "Epoch 1167/30000 Training Loss: 0.09072256833314896\n",
      "Epoch 1168/30000 Training Loss: 0.09516564756631851\n",
      "Epoch 1169/30000 Training Loss: 0.07792091369628906\n",
      "Epoch 1170/30000 Training Loss: 0.0730179101228714\n",
      "Epoch 1171/30000 Training Loss: 0.06694523990154266\n",
      "Epoch 1172/30000 Training Loss: 0.0703953430056572\n",
      "Epoch 1173/30000 Training Loss: 0.08014731109142303\n",
      "Epoch 1174/30000 Training Loss: 0.09069198369979858\n",
      "Epoch 1175/30000 Training Loss: 0.09402747452259064\n",
      "Epoch 1176/30000 Training Loss: 0.10036986321210861\n",
      "Epoch 1177/30000 Training Loss: 0.0876157283782959\n",
      "Epoch 1178/30000 Training Loss: 0.08404677361249924\n",
      "Epoch 1179/30000 Training Loss: 0.08216626942157745\n",
      "Epoch 1180/30000 Training Loss: 0.06236553192138672\n",
      "Epoch 1181/30000 Training Loss: 0.07412132620811462\n",
      "Epoch 1182/30000 Training Loss: 0.08150334656238556\n",
      "Epoch 1183/30000 Training Loss: 0.0833917111158371\n",
      "Epoch 1184/30000 Training Loss: 0.08658503741025925\n",
      "Epoch 1185/30000 Training Loss: 0.06748692691326141\n",
      "Epoch 1186/30000 Training Loss: 0.07972604036331177\n",
      "Epoch 1187/30000 Training Loss: 0.08937665820121765\n",
      "Epoch 1188/30000 Training Loss: 0.08562968671321869\n",
      "Epoch 1189/30000 Training Loss: 0.07794643938541412\n",
      "Epoch 1190/30000 Training Loss: 0.07450716942548752\n",
      "Epoch 1191/30000 Training Loss: 0.07513796538114548\n",
      "Epoch 1192/30000 Training Loss: 0.06902863830327988\n",
      "Epoch 1193/30000 Training Loss: 0.08131501823663712\n",
      "Epoch 1194/30000 Training Loss: 0.07108946144580841\n",
      "Epoch 1195/30000 Training Loss: 0.06580181419849396\n",
      "Epoch 1196/30000 Training Loss: 0.07502720504999161\n",
      "Epoch 1197/30000 Training Loss: 0.07640507072210312\n",
      "Epoch 1198/30000 Training Loss: 0.0802055150270462\n",
      "Epoch 1199/30000 Training Loss: 0.08230219781398773\n",
      "Epoch 1200/30000 Training Loss: 0.09152624756097794\n",
      "Epoch 1200/30000 Validation Loss: 0.07294706255197525\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.07294706255197525<=============\n",
      "Epoch 1201/30000 Training Loss: 0.08496280759572983\n",
      "Epoch 1202/30000 Training Loss: 0.08308757841587067\n",
      "Epoch 1203/30000 Training Loss: 0.0833459421992302\n",
      "Epoch 1204/30000 Training Loss: 0.07660019397735596\n",
      "Epoch 1205/30000 Training Loss: 0.09053967893123627\n",
      "Epoch 1206/30000 Training Loss: 0.06867899745702744\n",
      "Epoch 1207/30000 Training Loss: 0.0965714305639267\n",
      "Epoch 1208/30000 Training Loss: 0.0799407809972763\n",
      "Epoch 1209/30000 Training Loss: 0.07327689230442047\n",
      "Epoch 1210/30000 Training Loss: 0.08198437094688416\n",
      "Epoch 1211/30000 Training Loss: 0.07163852453231812\n",
      "Epoch 1212/30000 Training Loss: 0.09187003970146179\n",
      "Epoch 1213/30000 Training Loss: 0.0793161690235138\n",
      "Epoch 1214/30000 Training Loss: 0.0675598531961441\n",
      "Epoch 1215/30000 Training Loss: 0.0918838381767273\n",
      "Epoch 1216/30000 Training Loss: 0.08561845123767853\n",
      "Epoch 1217/30000 Training Loss: 0.06827833503484726\n",
      "Epoch 1218/30000 Training Loss: 0.07966893911361694\n",
      "Epoch 1219/30000 Training Loss: 0.08153031766414642\n",
      "Epoch 1220/30000 Training Loss: 0.07226917147636414\n",
      "Epoch 1221/30000 Training Loss: 0.07118870317935944\n",
      "Epoch 1222/30000 Training Loss: 0.06503129005432129\n",
      "Epoch 1223/30000 Training Loss: 0.07606148719787598\n",
      "Epoch 1224/30000 Training Loss: 0.06073589622974396\n",
      "Epoch 1225/30000 Training Loss: 0.09698228538036346\n",
      "Epoch 1226/30000 Training Loss: 0.07908125221729279\n",
      "Epoch 1227/30000 Training Loss: 0.08115213364362717\n",
      "Epoch 1228/30000 Training Loss: 0.08775752037763596\n",
      "Epoch 1229/30000 Training Loss: 0.10228854417800903\n",
      "Epoch 1230/30000 Training Loss: 0.07527874410152435\n",
      "Epoch 1231/30000 Training Loss: 0.08669799566268921\n",
      "Epoch 1232/30000 Training Loss: 0.08016292750835419\n",
      "Epoch 1233/30000 Training Loss: 0.0735069289803505\n",
      "Epoch 1234/30000 Training Loss: 0.07485142350196838\n",
      "Epoch 1235/30000 Training Loss: 0.07523387670516968\n",
      "Epoch 1236/30000 Training Loss: 0.08496682345867157\n",
      "Epoch 1237/30000 Training Loss: 0.09223438799381256\n",
      "Epoch 1238/30000 Training Loss: 0.08226169645786285\n",
      "Epoch 1239/30000 Training Loss: 0.06447198987007141\n",
      "Epoch 1240/30000 Training Loss: 0.10636058449745178\n",
      "Epoch 1241/30000 Training Loss: 0.07929614186286926\n",
      "Epoch 1242/30000 Training Loss: 0.0804789811372757\n",
      "Epoch 1243/30000 Training Loss: 0.08110729604959488\n",
      "Epoch 1244/30000 Training Loss: 0.07609142363071442\n",
      "Epoch 1245/30000 Training Loss: 0.08131342381238937\n",
      "Epoch 1246/30000 Training Loss: 0.0844220370054245\n",
      "Epoch 1247/30000 Training Loss: 0.057992178946733475\n",
      "Epoch 1248/30000 Training Loss: 0.06988260895013809\n",
      "Epoch 1249/30000 Training Loss: 0.06383539736270905\n",
      "Epoch 1250/30000 Training Loss: 0.07505098730325699\n",
      "Epoch 1251/30000 Training Loss: 0.07554571330547333\n",
      "Epoch 1252/30000 Training Loss: 0.08130961656570435\n",
      "Epoch 1253/30000 Training Loss: 0.08148093521595001\n",
      "Epoch 1254/30000 Training Loss: 0.07480508834123611\n",
      "Epoch 1255/30000 Training Loss: 0.06659214943647385\n",
      "Epoch 1256/30000 Training Loss: 0.09014655649662018\n",
      "Epoch 1257/30000 Training Loss: 0.09581685811281204\n",
      "Epoch 1258/30000 Training Loss: 0.07139552384614944\n",
      "Epoch 1259/30000 Training Loss: 0.08801445364952087\n",
      "Epoch 1260/30000 Training Loss: 0.09082777798175812\n",
      "Epoch 1261/30000 Training Loss: 0.0703439712524414\n",
      "Epoch 1262/30000 Training Loss: 0.07747289538383484\n",
      "Epoch 1263/30000 Training Loss: 0.06752483546733856\n",
      "Epoch 1264/30000 Training Loss: 0.08497060090303421\n",
      "Epoch 1265/30000 Training Loss: 0.06995377689599991\n",
      "Epoch 1266/30000 Training Loss: 0.08131726831197739\n",
      "Epoch 1267/30000 Training Loss: 0.08552691340446472\n",
      "Epoch 1268/30000 Training Loss: 0.08408694714307785\n",
      "Epoch 1269/30000 Training Loss: 0.0738522857427597\n",
      "Epoch 1270/30000 Training Loss: 0.07167207449674606\n",
      "Epoch 1271/30000 Training Loss: 0.06789187341928482\n",
      "Epoch 1272/30000 Training Loss: 0.09619759768247604\n",
      "Epoch 1273/30000 Training Loss: 0.07070662081241608\n",
      "Epoch 1274/30000 Training Loss: 0.08329412341117859\n",
      "Epoch 1275/30000 Training Loss: 0.08147092163562775\n",
      "Epoch 1276/30000 Training Loss: 0.08504387736320496\n",
      "Epoch 1277/30000 Training Loss: 0.07522754371166229\n",
      "Epoch 1278/30000 Training Loss: 0.0864742249250412\n",
      "Epoch 1279/30000 Training Loss: 0.07928158342838287\n",
      "Epoch 1280/30000 Training Loss: 0.09752192348241806\n",
      "Epoch 1281/30000 Training Loss: 0.07024048268795013\n",
      "Epoch 1282/30000 Training Loss: 0.06712684780359268\n",
      "Epoch 1283/30000 Training Loss: 0.07402946054935455\n",
      "Epoch 1284/30000 Training Loss: 0.08411604911088943\n",
      "Epoch 1285/30000 Training Loss: 0.08115163445472717\n",
      "Epoch 1286/30000 Training Loss: 0.07528405636548996\n",
      "Epoch 1287/30000 Training Loss: 0.07936057448387146\n",
      "Epoch 1288/30000 Training Loss: 0.09286642074584961\n",
      "Epoch 1289/30000 Training Loss: 0.07286520302295685\n",
      "Epoch 1290/30000 Training Loss: 0.08407430350780487\n",
      "Epoch 1291/30000 Training Loss: 0.06234988570213318\n",
      "Epoch 1292/30000 Training Loss: 0.08020463585853577\n",
      "Epoch 1293/30000 Training Loss: 0.08034797757863998\n",
      "Epoch 1294/30000 Training Loss: 0.07779940217733383\n",
      "Epoch 1295/30000 Training Loss: 0.08100882172584534\n",
      "Epoch 1296/30000 Training Loss: 0.08240702003240585\n",
      "Epoch 1297/30000 Training Loss: 0.08536281436681747\n",
      "Epoch 1298/30000 Training Loss: 0.07423675060272217\n",
      "Epoch 1299/30000 Training Loss: 0.07240457087755203\n",
      "Epoch 1300/30000 Training Loss: 0.06043000519275665\n",
      "Epoch 1300/30000 Validation Loss: 0.06957554817199707\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06957554817199707<=============\n",
      "Epoch 1301/30000 Training Loss: 0.07504423707723618\n",
      "Epoch 1302/30000 Training Loss: 0.07639685273170471\n",
      "Epoch 1303/30000 Training Loss: 0.07250411808490753\n",
      "Epoch 1304/30000 Training Loss: 0.07294248044490814\n",
      "Epoch 1305/30000 Training Loss: 0.0919070690870285\n",
      "Epoch 1306/30000 Training Loss: 0.07350093126296997\n",
      "Epoch 1307/30000 Training Loss: 0.08308349549770355\n",
      "Epoch 1308/30000 Training Loss: 0.07636307924985886\n",
      "Epoch 1309/30000 Training Loss: 0.0704130083322525\n",
      "Epoch 1310/30000 Training Loss: 0.0864250585436821\n",
      "Epoch 1311/30000 Training Loss: 0.08395243436098099\n",
      "Epoch 1312/30000 Training Loss: 0.09053825587034225\n",
      "Epoch 1313/30000 Training Loss: 0.08964225649833679\n",
      "Epoch 1314/30000 Training Loss: 0.09266511350870132\n",
      "Epoch 1315/30000 Training Loss: 0.07233111560344696\n",
      "Epoch 1316/30000 Training Loss: 0.09232433140277863\n",
      "Epoch 1317/30000 Training Loss: 0.0804050862789154\n",
      "Epoch 1318/30000 Training Loss: 0.07930000871419907\n",
      "Epoch 1319/30000 Training Loss: 0.07339385151863098\n",
      "Epoch 1320/30000 Training Loss: 0.07217294722795486\n",
      "Epoch 1321/30000 Training Loss: 0.07205568253993988\n",
      "Epoch 1322/30000 Training Loss: 0.0659073069691658\n",
      "Epoch 1323/30000 Training Loss: 0.0703345537185669\n",
      "Epoch 1324/30000 Training Loss: 0.06980292499065399\n",
      "Epoch 1325/30000 Training Loss: 0.10019026696681976\n",
      "Epoch 1326/30000 Training Loss: 0.06664689630270004\n",
      "Epoch 1327/30000 Training Loss: 0.0786031037569046\n",
      "Epoch 1328/30000 Training Loss: 0.07615584880113602\n",
      "Epoch 1329/30000 Training Loss: 0.08202987909317017\n",
      "Epoch 1330/30000 Training Loss: 0.08403301239013672\n",
      "Epoch 1331/30000 Training Loss: 0.09961946308612823\n",
      "Epoch 1332/30000 Training Loss: 0.06356868147850037\n",
      "Epoch 1333/30000 Training Loss: 0.08028880506753922\n",
      "Epoch 1334/30000 Training Loss: 0.06894047558307648\n",
      "Epoch 1335/30000 Training Loss: 0.09116329252719879\n",
      "Epoch 1336/30000 Training Loss: 0.07780744135379791\n",
      "Epoch 1337/30000 Training Loss: 0.07216210663318634\n",
      "Epoch 1338/30000 Training Loss: 0.07047078013420105\n",
      "Epoch 1339/30000 Training Loss: 0.0752374604344368\n",
      "Epoch 1340/30000 Training Loss: 0.0651850625872612\n",
      "Epoch 1341/30000 Training Loss: 0.06238071620464325\n",
      "Epoch 1342/30000 Training Loss: 0.09523184597492218\n",
      "Epoch 1343/30000 Training Loss: 0.07309311628341675\n",
      "Epoch 1344/30000 Training Loss: 0.07394301146268845\n",
      "Epoch 1345/30000 Training Loss: 0.06694342941045761\n",
      "Epoch 1346/30000 Training Loss: 0.08285319805145264\n",
      "Epoch 1347/30000 Training Loss: 0.0774240791797638\n",
      "Epoch 1348/30000 Training Loss: 0.07234765589237213\n",
      "Epoch 1349/30000 Training Loss: 0.075156569480896\n",
      "Epoch 1350/30000 Training Loss: 0.07155424356460571\n",
      "Epoch 1351/30000 Training Loss: 0.06069743633270264\n",
      "Epoch 1352/30000 Training Loss: 0.06745931506156921\n",
      "Epoch 1353/30000 Training Loss: 0.08041471242904663\n",
      "Epoch 1354/30000 Training Loss: 0.06517188996076584\n",
      "Epoch 1355/30000 Training Loss: 0.07808979600667953\n",
      "Epoch 1356/30000 Training Loss: 0.0755554810166359\n",
      "Epoch 1357/30000 Training Loss: 0.07010309398174286\n",
      "Epoch 1358/30000 Training Loss: 0.08380280435085297\n",
      "Epoch 1359/30000 Training Loss: 0.07770232856273651\n",
      "Epoch 1360/30000 Training Loss: 0.08916395157575607\n",
      "Epoch 1361/30000 Training Loss: 0.07350567728281021\n",
      "Epoch 1362/30000 Training Loss: 0.07636690139770508\n",
      "Epoch 1363/30000 Training Loss: 0.07281093299388885\n",
      "Epoch 1364/30000 Training Loss: 0.09367718547582626\n",
      "Epoch 1365/30000 Training Loss: 0.08624156564474106\n",
      "Epoch 1366/30000 Training Loss: 0.09006132185459137\n",
      "Epoch 1367/30000 Training Loss: 0.07653863728046417\n",
      "Epoch 1368/30000 Training Loss: 0.1010795384645462\n",
      "Epoch 1369/30000 Training Loss: 0.07360856235027313\n",
      "Epoch 1370/30000 Training Loss: 0.09084287285804749\n",
      "Epoch 1371/30000 Training Loss: 0.07083779573440552\n",
      "Epoch 1372/30000 Training Loss: 0.09583902359008789\n",
      "Epoch 1373/30000 Training Loss: 0.06694604456424713\n",
      "Epoch 1374/30000 Training Loss: 0.07688401639461517\n",
      "Epoch 1375/30000 Training Loss: 0.08081823587417603\n",
      "Epoch 1376/30000 Training Loss: 0.07492858171463013\n",
      "Epoch 1377/30000 Training Loss: 0.08526193350553513\n",
      "Epoch 1378/30000 Training Loss: 0.07634136080741882\n",
      "Epoch 1379/30000 Training Loss: 0.07049539685249329\n",
      "Epoch 1380/30000 Training Loss: 0.08541551232337952\n",
      "Epoch 1381/30000 Training Loss: 0.07356241345405579\n",
      "Epoch 1382/30000 Training Loss: 0.063484326004982\n",
      "Epoch 1383/30000 Training Loss: 0.06498033553361893\n",
      "Epoch 1384/30000 Training Loss: 0.07928980886936188\n",
      "Epoch 1385/30000 Training Loss: 0.07174298912286758\n",
      "Epoch 1386/30000 Training Loss: 0.07797682285308838\n",
      "Epoch 1387/30000 Training Loss: 0.0598655641078949\n",
      "Epoch 1388/30000 Training Loss: 0.09989973902702332\n",
      "Epoch 1389/30000 Training Loss: 0.07452399283647537\n",
      "Epoch 1390/30000 Training Loss: 0.09083157032728195\n",
      "Epoch 1391/30000 Training Loss: 0.06867295503616333\n",
      "Epoch 1392/30000 Training Loss: 0.08929780125617981\n",
      "Epoch 1393/30000 Training Loss: 0.08506910502910614\n",
      "Epoch 1394/30000 Training Loss: 0.069819375872612\n",
      "Epoch 1395/30000 Training Loss: 0.09374727308750153\n",
      "Epoch 1396/30000 Training Loss: 0.05654284358024597\n",
      "Epoch 1397/30000 Training Loss: 0.07160787284374237\n",
      "Epoch 1398/30000 Training Loss: 0.06591729819774628\n",
      "Epoch 1399/30000 Training Loss: 0.07842768728733063\n",
      "Epoch 1400/30000 Training Loss: 0.0752778947353363\n",
      "Epoch 1400/30000 Validation Loss: 0.07539153099060059\n",
      "Epoch 1401/30000 Training Loss: 0.05638998746871948\n",
      "Epoch 1402/30000 Training Loss: 0.07808429002761841\n",
      "Epoch 1403/30000 Training Loss: 0.08720776438713074\n",
      "Epoch 1404/30000 Training Loss: 0.08769641816616058\n",
      "Epoch 1405/30000 Training Loss: 0.07435422390699387\n",
      "Epoch 1406/30000 Training Loss: 0.07020734250545502\n",
      "Epoch 1407/30000 Training Loss: 0.07751035690307617\n",
      "Epoch 1408/30000 Training Loss: 0.06304440647363663\n",
      "Epoch 1409/30000 Training Loss: 0.07184512913227081\n",
      "Epoch 1410/30000 Training Loss: 0.0858505591750145\n",
      "Epoch 1411/30000 Training Loss: 0.07902297377586365\n",
      "Epoch 1412/30000 Training Loss: 0.09505248069763184\n",
      "Epoch 1413/30000 Training Loss: 0.08239537477493286\n",
      "Epoch 1414/30000 Training Loss: 0.06917175650596619\n",
      "Epoch 1415/30000 Training Loss: 0.06934110075235367\n",
      "Epoch 1416/30000 Training Loss: 0.08691074699163437\n",
      "Epoch 1417/30000 Training Loss: 0.06610730290412903\n",
      "Epoch 1418/30000 Training Loss: 0.09182325005531311\n",
      "Epoch 1419/30000 Training Loss: 0.08920647948980331\n",
      "Epoch 1420/30000 Training Loss: 0.07593479007482529\n",
      "Epoch 1421/30000 Training Loss: 0.08955328166484833\n",
      "Epoch 1422/30000 Training Loss: 0.08957651257514954\n",
      "Epoch 1423/30000 Training Loss: 0.07696932554244995\n",
      "Epoch 1424/30000 Training Loss: 0.09037034213542938\n",
      "Epoch 1425/30000 Training Loss: 0.08688122779130936\n",
      "Epoch 1426/30000 Training Loss: 0.06915245205163956\n",
      "Epoch 1427/30000 Training Loss: 0.08612152189016342\n",
      "Epoch 1428/30000 Training Loss: 0.09039472043514252\n",
      "Epoch 1429/30000 Training Loss: 0.07726317644119263\n",
      "Epoch 1430/30000 Training Loss: 0.09503383934497833\n",
      "Epoch 1431/30000 Training Loss: 0.06517080962657928\n",
      "Epoch 1432/30000 Training Loss: 0.0852811262011528\n",
      "Epoch 1433/30000 Training Loss: 0.06830480694770813\n",
      "Epoch 1434/30000 Training Loss: 0.07811372727155685\n",
      "Epoch 1435/30000 Training Loss: 0.07600128650665283\n",
      "Epoch 1436/30000 Training Loss: 0.07908779382705688\n",
      "Epoch 1437/30000 Training Loss: 0.07167252898216248\n",
      "Epoch 1438/30000 Training Loss: 0.07702994346618652\n",
      "Epoch 1439/30000 Training Loss: 0.0836908221244812\n",
      "Epoch 1440/30000 Training Loss: 0.06492108106613159\n",
      "Epoch 1441/30000 Training Loss: 0.08826486766338348\n",
      "Epoch 1442/30000 Training Loss: 0.08860750496387482\n",
      "Epoch 1443/30000 Training Loss: 0.07384496927261353\n",
      "Epoch 1444/30000 Training Loss: 0.07704596221446991\n",
      "Epoch 1445/30000 Training Loss: 0.09540034830570221\n",
      "Epoch 1446/30000 Training Loss: 0.0601695217192173\n",
      "Epoch 1447/30000 Training Loss: 0.0926295816898346\n",
      "Epoch 1448/30000 Training Loss: 0.07855051755905151\n",
      "Epoch 1449/30000 Training Loss: 0.07982754707336426\n",
      "Epoch 1450/30000 Training Loss: 0.08368708938360214\n",
      "Epoch 1451/30000 Training Loss: 0.08460178971290588\n",
      "Epoch 1452/30000 Training Loss: 0.07447393983602524\n",
      "Epoch 1453/30000 Training Loss: 0.06352977454662323\n",
      "Epoch 1454/30000 Training Loss: 0.0807054340839386\n",
      "Epoch 1455/30000 Training Loss: 0.0729399025440216\n",
      "Epoch 1456/30000 Training Loss: 0.07150588184595108\n",
      "Epoch 1457/30000 Training Loss: 0.08596155047416687\n",
      "Epoch 1458/30000 Training Loss: 0.07397650182247162\n",
      "Epoch 1459/30000 Training Loss: 0.08079000562429428\n",
      "Epoch 1460/30000 Training Loss: 0.07291977107524872\n",
      "Epoch 1461/30000 Training Loss: 0.06728646904230118\n",
      "Epoch 1462/30000 Training Loss: 0.0852457731962204\n",
      "Epoch 1463/30000 Training Loss: 0.08539539575576782\n",
      "Epoch 1464/30000 Training Loss: 0.06884682178497314\n",
      "Epoch 1465/30000 Training Loss: 0.06902346014976501\n",
      "Epoch 1466/30000 Training Loss: 0.07245457172393799\n",
      "Epoch 1467/30000 Training Loss: 0.0843377485871315\n",
      "Epoch 1468/30000 Training Loss: 0.07523515075445175\n",
      "Epoch 1469/30000 Training Loss: 0.07159516960382462\n",
      "Epoch 1470/30000 Training Loss: 0.09856738150119781\n",
      "Epoch 1471/30000 Training Loss: 0.060585200786590576\n",
      "Epoch 1472/30000 Training Loss: 0.08424624800682068\n",
      "Epoch 1473/30000 Training Loss: 0.08218717575073242\n",
      "Epoch 1474/30000 Training Loss: 0.07161487638950348\n",
      "Epoch 1475/30000 Training Loss: 0.07320409268140793\n",
      "Epoch 1476/30000 Training Loss: 0.07790778577327728\n",
      "Epoch 1477/30000 Training Loss: 0.06393703073263168\n",
      "Epoch 1478/30000 Training Loss: 0.06085999310016632\n",
      "Epoch 1479/30000 Training Loss: 0.08372047543525696\n",
      "Epoch 1480/30000 Training Loss: 0.06502588093280792\n",
      "Epoch 1481/30000 Training Loss: 0.07930448651313782\n",
      "Epoch 1482/30000 Training Loss: 0.07062487304210663\n",
      "Epoch 1483/30000 Training Loss: 0.08121447265148163\n",
      "Epoch 1484/30000 Training Loss: 0.08608925342559814\n",
      "Epoch 1485/30000 Training Loss: 0.08708938956260681\n",
      "Epoch 1486/30000 Training Loss: 0.05982336401939392\n",
      "Epoch 1487/30000 Training Loss: 0.08235238492488861\n",
      "Epoch 1488/30000 Training Loss: 0.08559496700763702\n",
      "Epoch 1489/30000 Training Loss: 0.07981417328119278\n",
      "Epoch 1490/30000 Training Loss: 0.07748688757419586\n",
      "Epoch 1491/30000 Training Loss: 0.06924470514059067\n",
      "Epoch 1492/30000 Training Loss: 0.07350049912929535\n",
      "Epoch 1493/30000 Training Loss: 0.08463846147060394\n",
      "Epoch 1494/30000 Training Loss: 0.07627353072166443\n",
      "Epoch 1495/30000 Training Loss: 0.07045856863260269\n",
      "Epoch 1496/30000 Training Loss: 0.07958667725324631\n",
      "Epoch 1497/30000 Training Loss: 0.07515445351600647\n",
      "Epoch 1498/30000 Training Loss: 0.06568112969398499\n",
      "Epoch 1499/30000 Training Loss: 0.07832103967666626\n",
      "Epoch 1500/30000 Training Loss: 0.07671540230512619\n",
      "Epoch 1500/30000 Validation Loss: 0.07175338268280029\n",
      "Epoch 1501/30000 Training Loss: 0.1040697917342186\n",
      "Epoch 1502/30000 Training Loss: 0.06241587549448013\n",
      "Epoch 1503/30000 Training Loss: 0.07692036032676697\n",
      "Epoch 1504/30000 Training Loss: 0.07324486970901489\n",
      "Epoch 1505/30000 Training Loss: 0.07646944373846054\n",
      "Epoch 1506/30000 Training Loss: 0.06661657243967056\n",
      "Epoch 1507/30000 Training Loss: 0.05860450118780136\n",
      "Epoch 1508/30000 Training Loss: 0.08442138880491257\n",
      "Epoch 1509/30000 Training Loss: 0.07926540076732635\n",
      "Epoch 1510/30000 Training Loss: 0.058166325092315674\n",
      "Epoch 1511/30000 Training Loss: 0.062324583530426025\n",
      "Epoch 1512/30000 Training Loss: 0.06487541645765305\n",
      "Epoch 1513/30000 Training Loss: 0.09133996069431305\n",
      "Epoch 1514/30000 Training Loss: 0.07674406468868256\n",
      "Epoch 1515/30000 Training Loss: 0.07997637987136841\n",
      "Epoch 1516/30000 Training Loss: 0.07963912189006805\n",
      "Epoch 1517/30000 Training Loss: 0.07636740803718567\n",
      "Epoch 1518/30000 Training Loss: 0.08440250158309937\n",
      "Epoch 1519/30000 Training Loss: 0.06649292260408401\n",
      "Epoch 1520/30000 Training Loss: 0.06599579006433487\n",
      "Epoch 1521/30000 Training Loss: 0.08554203808307648\n",
      "Epoch 1522/30000 Training Loss: 0.08661720901727676\n",
      "Epoch 1523/30000 Training Loss: 0.07132614403963089\n",
      "Epoch 1524/30000 Training Loss: 0.07641736418008804\n",
      "Epoch 1525/30000 Training Loss: 0.08789277076721191\n",
      "Epoch 1526/30000 Training Loss: 0.07096801698207855\n",
      "Epoch 1527/30000 Training Loss: 0.08162941038608551\n",
      "Epoch 1528/30000 Training Loss: 0.07418444007635117\n",
      "Epoch 1529/30000 Training Loss: 0.07209226489067078\n",
      "Epoch 1530/30000 Training Loss: 0.08749639242887497\n",
      "Epoch 1531/30000 Training Loss: 0.06259122490882874\n",
      "Epoch 1532/30000 Training Loss: 0.08291415125131607\n",
      "Epoch 1533/30000 Training Loss: 0.08065737783908844\n",
      "Epoch 1534/30000 Training Loss: 0.0691806972026825\n",
      "Epoch 1535/30000 Training Loss: 0.07549476623535156\n",
      "Epoch 1536/30000 Training Loss: 0.07399094104766846\n",
      "Epoch 1537/30000 Training Loss: 0.05937585234642029\n",
      "Epoch 1538/30000 Training Loss: 0.06723064184188843\n",
      "Epoch 1539/30000 Training Loss: 0.08548863232135773\n",
      "Epoch 1540/30000 Training Loss: 0.0738336518406868\n",
      "Epoch 1541/30000 Training Loss: 0.06978367269039154\n",
      "Epoch 1542/30000 Training Loss: 0.09123768657445908\n",
      "Epoch 1543/30000 Training Loss: 0.06523381918668747\n",
      "Epoch 1544/30000 Training Loss: 0.08108485490083694\n",
      "Epoch 1545/30000 Training Loss: 0.06860724836587906\n",
      "Epoch 1546/30000 Training Loss: 0.07518364489078522\n",
      "Epoch 1547/30000 Training Loss: 0.09136281907558441\n",
      "Epoch 1548/30000 Training Loss: 0.08320446312427521\n",
      "Epoch 1549/30000 Training Loss: 0.05676547437906265\n",
      "Epoch 1550/30000 Training Loss: 0.05899751931428909\n",
      "Epoch 1551/30000 Training Loss: 0.06630107760429382\n",
      "Epoch 1552/30000 Training Loss: 0.07482445240020752\n",
      "Epoch 1553/30000 Training Loss: 0.08977602422237396\n",
      "Epoch 1554/30000 Training Loss: 0.06637546420097351\n",
      "Epoch 1555/30000 Training Loss: 0.08846127986907959\n",
      "Epoch 1556/30000 Training Loss: 0.058041565120220184\n",
      "Epoch 1557/30000 Training Loss: 0.09317506849765778\n",
      "Epoch 1558/30000 Training Loss: 0.05815638601779938\n",
      "Epoch 1559/30000 Training Loss: 0.0753404051065445\n",
      "Epoch 1560/30000 Training Loss: 0.08579204976558685\n",
      "Epoch 1561/30000 Training Loss: 0.05425695329904556\n",
      "Epoch 1562/30000 Training Loss: 0.06248362362384796\n",
      "Epoch 1563/30000 Training Loss: 0.07997909188270569\n",
      "Epoch 1564/30000 Training Loss: 0.07446032017469406\n",
      "Epoch 1565/30000 Training Loss: 0.07207400351762772\n",
      "Epoch 1566/30000 Training Loss: 0.0656370222568512\n",
      "Epoch 1567/30000 Training Loss: 0.06889235228300095\n",
      "Epoch 1568/30000 Training Loss: 0.07208618521690369\n",
      "Epoch 1569/30000 Training Loss: 0.06475500762462616\n",
      "Epoch 1570/30000 Training Loss: 0.07970553636550903\n",
      "Epoch 1571/30000 Training Loss: 0.062001876533031464\n",
      "Epoch 1572/30000 Training Loss: 0.08549916744232178\n",
      "Epoch 1573/30000 Training Loss: 0.07326111942529678\n",
      "Epoch 1574/30000 Training Loss: 0.07025454193353653\n",
      "Epoch 1575/30000 Training Loss: 0.07492738962173462\n",
      "Epoch 1576/30000 Training Loss: 0.06506334245204926\n",
      "Epoch 1577/30000 Training Loss: 0.07187297940254211\n",
      "Epoch 1578/30000 Training Loss: 0.07268216460943222\n",
      "Epoch 1579/30000 Training Loss: 0.0611778125166893\n",
      "Epoch 1580/30000 Training Loss: 0.06838039308786392\n",
      "Epoch 1581/30000 Training Loss: 0.06625401973724365\n",
      "Epoch 1582/30000 Training Loss: 0.05090826004743576\n",
      "Epoch 1583/30000 Training Loss: 0.07260579615831375\n",
      "Epoch 1584/30000 Training Loss: 0.07704410701990128\n",
      "Epoch 1585/30000 Training Loss: 0.08098942041397095\n",
      "Epoch 1586/30000 Training Loss: 0.07158233970403671\n",
      "Epoch 1587/30000 Training Loss: 0.05353657156229019\n",
      "Epoch 1588/30000 Training Loss: 0.08578804135322571\n",
      "Epoch 1589/30000 Training Loss: 0.06436139345169067\n",
      "Epoch 1590/30000 Training Loss: 0.07445240020751953\n",
      "Epoch 1591/30000 Training Loss: 0.07763659954071045\n",
      "Epoch 1592/30000 Training Loss: 0.06860233843326569\n",
      "Epoch 1593/30000 Training Loss: 0.07790626585483551\n",
      "Epoch 1594/30000 Training Loss: 0.08148902654647827\n",
      "Epoch 1595/30000 Training Loss: 0.08023740351200104\n",
      "Epoch 1596/30000 Training Loss: 0.06240777671337128\n",
      "Epoch 1597/30000 Training Loss: 0.06589756906032562\n",
      "Epoch 1598/30000 Training Loss: 0.07589507102966309\n",
      "Epoch 1599/30000 Training Loss: 0.07987207174301147\n",
      "Epoch 1600/30000 Training Loss: 0.058384962379932404\n",
      "Epoch 1600/30000 Validation Loss: 0.06713502109050751\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06713502109050751<=============\n",
      "Epoch 1601/30000 Training Loss: 0.0820169597864151\n",
      "Epoch 1602/30000 Training Loss: 0.06266721338033676\n",
      "Epoch 1603/30000 Training Loss: 0.09599585831165314\n",
      "Epoch 1604/30000 Training Loss: 0.0663655698299408\n",
      "Epoch 1605/30000 Training Loss: 0.0737672820687294\n",
      "Epoch 1606/30000 Training Loss: 0.07430887222290039\n",
      "Epoch 1607/30000 Training Loss: 0.08572152256965637\n",
      "Epoch 1608/30000 Training Loss: 0.069405198097229\n",
      "Epoch 1609/30000 Training Loss: 0.06882719695568085\n",
      "Epoch 1610/30000 Training Loss: 0.0695805698633194\n",
      "Epoch 1611/30000 Training Loss: 0.08059336245059967\n",
      "Epoch 1612/30000 Training Loss: 0.055132120847702026\n",
      "Epoch 1613/30000 Training Loss: 0.05991329252719879\n",
      "Epoch 1614/30000 Training Loss: 0.08127319812774658\n",
      "Epoch 1615/30000 Training Loss: 0.07315650582313538\n",
      "Epoch 1616/30000 Training Loss: 0.07810944318771362\n",
      "Epoch 1617/30000 Training Loss: 0.07354960590600967\n",
      "Epoch 1618/30000 Training Loss: 0.06685258448123932\n",
      "Epoch 1619/30000 Training Loss: 0.0828610509634018\n",
      "Epoch 1620/30000 Training Loss: 0.06208924949169159\n",
      "Epoch 1621/30000 Training Loss: 0.06614767014980316\n",
      "Epoch 1622/30000 Training Loss: 0.09543770551681519\n",
      "Epoch 1623/30000 Training Loss: 0.08180476725101471\n",
      "Epoch 1624/30000 Training Loss: 0.08081899583339691\n",
      "Epoch 1625/30000 Training Loss: 0.07731654495000839\n",
      "Epoch 1626/30000 Training Loss: 0.06696934252977371\n",
      "Epoch 1627/30000 Training Loss: 0.07990284264087677\n",
      "Epoch 1628/30000 Training Loss: 0.08276889473199844\n",
      "Epoch 1629/30000 Training Loss: 0.06622108817100525\n",
      "Epoch 1630/30000 Training Loss: 0.08016882091760635\n",
      "Epoch 1631/30000 Training Loss: 0.08669424057006836\n",
      "Epoch 1632/30000 Training Loss: 0.07645495235919952\n",
      "Epoch 1633/30000 Training Loss: 0.07106661796569824\n",
      "Epoch 1634/30000 Training Loss: 0.05869216471910477\n",
      "Epoch 1635/30000 Training Loss: 0.0738597884774208\n",
      "Epoch 1636/30000 Training Loss: 0.061046965420246124\n",
      "Epoch 1637/30000 Training Loss: 0.06720590591430664\n",
      "Epoch 1638/30000 Training Loss: 0.0728861540555954\n",
      "Epoch 1639/30000 Training Loss: 0.0819365605711937\n",
      "Epoch 1640/30000 Training Loss: 0.08428733050823212\n",
      "Epoch 1641/30000 Training Loss: 0.07452990114688873\n",
      "Epoch 1642/30000 Training Loss: 0.08873601257801056\n",
      "Epoch 1643/30000 Training Loss: 0.07446976006031036\n",
      "Epoch 1644/30000 Training Loss: 0.06554459780454636\n",
      "Epoch 1645/30000 Training Loss: 0.07838903367519379\n",
      "Epoch 1646/30000 Training Loss: 0.06396545469760895\n",
      "Epoch 1647/30000 Training Loss: 0.07749917358160019\n",
      "Epoch 1648/30000 Training Loss: 0.08129718899726868\n",
      "Epoch 1649/30000 Training Loss: 0.07728676497936249\n",
      "Epoch 1650/30000 Training Loss: 0.06929408758878708\n",
      "Epoch 1651/30000 Training Loss: 0.06710556149482727\n",
      "Epoch 1652/30000 Training Loss: 0.07842081785202026\n",
      "Epoch 1653/30000 Training Loss: 0.0772450864315033\n",
      "Epoch 1654/30000 Training Loss: 0.06815922260284424\n",
      "Epoch 1655/30000 Training Loss: 0.07726848870515823\n",
      "Epoch 1656/30000 Training Loss: 0.08685941249132156\n",
      "Epoch 1657/30000 Training Loss: 0.06201642006635666\n",
      "Epoch 1658/30000 Training Loss: 0.07206462323665619\n",
      "Epoch 1659/30000 Training Loss: 0.055709727108478546\n",
      "Epoch 1660/30000 Training Loss: 0.0712643414735794\n",
      "Epoch 1661/30000 Training Loss: 0.05715430900454521\n",
      "Epoch 1662/30000 Training Loss: 0.08798881620168686\n",
      "Epoch 1663/30000 Training Loss: 0.07488608360290527\n",
      "Epoch 1664/30000 Training Loss: 0.08691920340061188\n",
      "Epoch 1665/30000 Training Loss: 0.07726863026618958\n",
      "Epoch 1666/30000 Training Loss: 0.06579531729221344\n",
      "Epoch 1667/30000 Training Loss: 0.06689520180225372\n",
      "Epoch 1668/30000 Training Loss: 0.06672795116901398\n",
      "Epoch 1669/30000 Training Loss: 0.061672016978263855\n",
      "Epoch 1670/30000 Training Loss: 0.08558154106140137\n",
      "Epoch 1671/30000 Training Loss: 0.057329028844833374\n",
      "Epoch 1672/30000 Training Loss: 0.0679909884929657\n",
      "Epoch 1673/30000 Training Loss: 0.057989031076431274\n",
      "Epoch 1674/30000 Training Loss: 0.07655470818281174\n",
      "Epoch 1675/30000 Training Loss: 0.0933314859867096\n",
      "Epoch 1676/30000 Training Loss: 0.06178523972630501\n",
      "Epoch 1677/30000 Training Loss: 0.08453860878944397\n",
      "Epoch 1678/30000 Training Loss: 0.057841308414936066\n",
      "Epoch 1679/30000 Training Loss: 0.07241347432136536\n",
      "Epoch 1680/30000 Training Loss: 0.06320668756961823\n",
      "Epoch 1681/30000 Training Loss: 0.08274288475513458\n",
      "Epoch 1682/30000 Training Loss: 0.06224051117897034\n",
      "Epoch 1683/30000 Training Loss: 0.06434489041566849\n",
      "Epoch 1684/30000 Training Loss: 0.09233235567808151\n",
      "Epoch 1685/30000 Training Loss: 0.0733308270573616\n",
      "Epoch 1686/30000 Training Loss: 0.08123547583818436\n",
      "Epoch 1687/30000 Training Loss: 0.06769263744354248\n",
      "Epoch 1688/30000 Training Loss: 0.07841484993696213\n",
      "Epoch 1689/30000 Training Loss: 0.06402115523815155\n",
      "Epoch 1690/30000 Training Loss: 0.0907202810049057\n",
      "Epoch 1691/30000 Training Loss: 0.06528045982122421\n",
      "Epoch 1692/30000 Training Loss: 0.05853730067610741\n",
      "Epoch 1693/30000 Training Loss: 0.08211055397987366\n",
      "Epoch 1694/30000 Training Loss: 0.06870242953300476\n",
      "Epoch 1695/30000 Training Loss: 0.08237922936677933\n",
      "Epoch 1696/30000 Training Loss: 0.06913469731807709\n",
      "Epoch 1697/30000 Training Loss: 0.0725318044424057\n",
      "Epoch 1698/30000 Training Loss: 0.060363925993442535\n",
      "Epoch 1699/30000 Training Loss: 0.07041138410568237\n",
      "Epoch 1700/30000 Training Loss: 0.06571027636528015\n",
      "Epoch 1700/30000 Validation Loss: 0.07853631675243378\n",
      "Epoch 1701/30000 Training Loss: 0.06676065921783447\n",
      "Epoch 1702/30000 Training Loss: 0.07306484133005142\n",
      "Epoch 1703/30000 Training Loss: 0.08468876779079437\n",
      "Epoch 1704/30000 Training Loss: 0.06455009430646896\n",
      "Epoch 1705/30000 Training Loss: 0.08667248487472534\n",
      "Epoch 1706/30000 Training Loss: 0.0702148824930191\n",
      "Epoch 1707/30000 Training Loss: 0.06022084131836891\n",
      "Epoch 1708/30000 Training Loss: 0.07806611806154251\n",
      "Epoch 1709/30000 Training Loss: 0.07903076708316803\n",
      "Epoch 1710/30000 Training Loss: 0.050931669771671295\n",
      "Epoch 1711/30000 Training Loss: 0.08151503652334213\n",
      "Epoch 1712/30000 Training Loss: 0.07077284902334213\n",
      "Epoch 1713/30000 Training Loss: 0.08093953877687454\n",
      "Epoch 1714/30000 Training Loss: 0.07133008539676666\n",
      "Epoch 1715/30000 Training Loss: 0.06046731770038605\n",
      "Epoch 1716/30000 Training Loss: 0.07201926410198212\n",
      "Epoch 1717/30000 Training Loss: 0.060251981019973755\n",
      "Epoch 1718/30000 Training Loss: 0.06580392271280289\n",
      "Epoch 1719/30000 Training Loss: 0.07889371365308762\n",
      "Epoch 1720/30000 Training Loss: 0.062202565371990204\n",
      "Epoch 1721/30000 Training Loss: 0.06302829086780548\n",
      "Epoch 1722/30000 Training Loss: 0.06783484667539597\n",
      "Epoch 1723/30000 Training Loss: 0.05923028290271759\n",
      "Epoch 1724/30000 Training Loss: 0.06294307112693787\n",
      "Epoch 1725/30000 Training Loss: 0.07216425240039825\n",
      "Epoch 1726/30000 Training Loss: 0.0771409422159195\n",
      "Epoch 1727/30000 Training Loss: 0.07766524702310562\n",
      "Epoch 1728/30000 Training Loss: 0.08585107326507568\n",
      "Epoch 1729/30000 Training Loss: 0.0564090758562088\n",
      "Epoch 1730/30000 Training Loss: 0.06420041620731354\n",
      "Epoch 1731/30000 Training Loss: 0.06853355467319489\n",
      "Epoch 1732/30000 Training Loss: 0.0912446528673172\n",
      "Epoch 1733/30000 Training Loss: 0.06753315776586533\n",
      "Epoch 1734/30000 Training Loss: 0.06308244913816452\n",
      "Epoch 1735/30000 Training Loss: 0.0710977092385292\n",
      "Epoch 1736/30000 Training Loss: 0.06888629496097565\n",
      "Epoch 1737/30000 Training Loss: 0.07153019309043884\n",
      "Epoch 1738/30000 Training Loss: 0.08219392597675323\n",
      "Epoch 1739/30000 Training Loss: 0.06784408539533615\n",
      "Epoch 1740/30000 Training Loss: 0.0711960643529892\n",
      "Epoch 1741/30000 Training Loss: 0.08915038406848907\n",
      "Epoch 1742/30000 Training Loss: 0.07336241006851196\n",
      "Epoch 1743/30000 Training Loss: 0.07993610203266144\n",
      "Epoch 1744/30000 Training Loss: 0.06389231234788895\n",
      "Epoch 1745/30000 Training Loss: 0.07035661488771439\n",
      "Epoch 1746/30000 Training Loss: 0.06164845824241638\n",
      "Epoch 1747/30000 Training Loss: 0.06996402889490128\n",
      "Epoch 1748/30000 Training Loss: 0.08316413313150406\n",
      "Epoch 1749/30000 Training Loss: 0.07515660673379898\n",
      "Epoch 1750/30000 Training Loss: 0.08016304671764374\n",
      "Epoch 1751/30000 Training Loss: 0.09064202755689621\n",
      "Epoch 1752/30000 Training Loss: 0.06364646553993225\n",
      "Epoch 1753/30000 Training Loss: 0.07274257391691208\n",
      "Epoch 1754/30000 Training Loss: 0.06578180193901062\n",
      "Epoch 1755/30000 Training Loss: 0.06413460522890091\n",
      "Epoch 1756/30000 Training Loss: 0.07042170315980911\n",
      "Epoch 1757/30000 Training Loss: 0.07170498371124268\n",
      "Epoch 1758/30000 Training Loss: 0.06200162321329117\n",
      "Epoch 1759/30000 Training Loss: 0.06376191973686218\n",
      "Epoch 1760/30000 Training Loss: 0.07169689238071442\n",
      "Epoch 1761/30000 Training Loss: 0.07663512229919434\n",
      "Epoch 1762/30000 Training Loss: 0.07333973050117493\n",
      "Epoch 1763/30000 Training Loss: 0.06740973889827728\n",
      "Epoch 1764/30000 Training Loss: 0.06876444071531296\n",
      "Epoch 1765/30000 Training Loss: 0.08256692439317703\n",
      "Epoch 1766/30000 Training Loss: 0.08564308285713196\n",
      "Epoch 1767/30000 Training Loss: 0.06586968153715134\n",
      "Epoch 1768/30000 Training Loss: 0.05985920876264572\n",
      "Epoch 1769/30000 Training Loss: 0.060884784907102585\n",
      "Epoch 1770/30000 Training Loss: 0.06101532280445099\n",
      "Epoch 1771/30000 Training Loss: 0.05512344837188721\n",
      "Epoch 1772/30000 Training Loss: 0.07348023355007172\n",
      "Epoch 1773/30000 Training Loss: 0.05723953992128372\n",
      "Epoch 1774/30000 Training Loss: 0.06820282340049744\n",
      "Epoch 1775/30000 Training Loss: 0.07859495282173157\n",
      "Epoch 1776/30000 Training Loss: 0.08356627076864243\n",
      "Epoch 1777/30000 Training Loss: 0.06765450537204742\n",
      "Epoch 1778/30000 Training Loss: 0.06452342867851257\n",
      "Epoch 1779/30000 Training Loss: 0.08515254408121109\n",
      "Epoch 1780/30000 Training Loss: 0.06776926666498184\n",
      "Epoch 1781/30000 Training Loss: 0.06404154002666473\n",
      "Epoch 1782/30000 Training Loss: 0.07948896288871765\n",
      "Epoch 1783/30000 Training Loss: 0.0656946450471878\n",
      "Epoch 1784/30000 Training Loss: 0.0790245532989502\n",
      "Epoch 1785/30000 Training Loss: 0.058788761496543884\n",
      "Epoch 1786/30000 Training Loss: 0.06591571122407913\n",
      "Epoch 1787/30000 Training Loss: 0.06547003984451294\n",
      "Epoch 1788/30000 Training Loss: 0.08699411153793335\n",
      "Epoch 1789/30000 Training Loss: 0.06751415133476257\n",
      "Epoch 1790/30000 Training Loss: 0.08004379272460938\n",
      "Epoch 1791/30000 Training Loss: 0.06511449813842773\n",
      "Epoch 1792/30000 Training Loss: 0.07687098532915115\n",
      "Epoch 1793/30000 Training Loss: 0.0867229551076889\n",
      "Epoch 1794/30000 Training Loss: 0.05594483017921448\n",
      "Epoch 1795/30000 Training Loss: 0.07468356192111969\n",
      "Epoch 1796/30000 Training Loss: 0.0783916711807251\n",
      "Epoch 1797/30000 Training Loss: 0.06542228162288666\n",
      "Epoch 1798/30000 Training Loss: 0.05859905481338501\n",
      "Epoch 1799/30000 Training Loss: 0.07089446485042572\n",
      "Epoch 1800/30000 Training Loss: 0.06924185901880264\n",
      "Epoch 1800/30000 Validation Loss: 0.09345072507858276\n",
      "Epoch 1801/30000 Training Loss: 0.06768070161342621\n",
      "Epoch 1802/30000 Training Loss: 0.07976870238780975\n",
      "Epoch 1803/30000 Training Loss: 0.06465543806552887\n",
      "Epoch 1804/30000 Training Loss: 0.07240791618824005\n",
      "Epoch 1805/30000 Training Loss: 0.05557548254728317\n",
      "Epoch 1806/30000 Training Loss: 0.07881420850753784\n",
      "Epoch 1807/30000 Training Loss: 0.06281618773937225\n",
      "Epoch 1808/30000 Training Loss: 0.06745517253875732\n",
      "Epoch 1809/30000 Training Loss: 0.0721663236618042\n",
      "Epoch 1810/30000 Training Loss: 0.07387541234493256\n",
      "Epoch 1811/30000 Training Loss: 0.06761236488819122\n",
      "Epoch 1812/30000 Training Loss: 0.08600521087646484\n",
      "Epoch 1813/30000 Training Loss: 0.06171894073486328\n",
      "Epoch 1814/30000 Training Loss: 0.07809799909591675\n",
      "Epoch 1815/30000 Training Loss: 0.06980839371681213\n",
      "Epoch 1816/30000 Training Loss: 0.08061045408248901\n",
      "Epoch 1817/30000 Training Loss: 0.0628223717212677\n",
      "Epoch 1818/30000 Training Loss: 0.06255844980478287\n",
      "Epoch 1819/30000 Training Loss: 0.07442569732666016\n",
      "Epoch 1820/30000 Training Loss: 0.07352356612682343\n",
      "Epoch 1821/30000 Training Loss: 0.06861633062362671\n",
      "Epoch 1822/30000 Training Loss: 0.07384894788265228\n",
      "Epoch 1823/30000 Training Loss: 0.0649915337562561\n",
      "Epoch 1824/30000 Training Loss: 0.08341274410486221\n",
      "Epoch 1825/30000 Training Loss: 0.09695810824632645\n",
      "Epoch 1826/30000 Training Loss: 0.06907223165035248\n",
      "Epoch 1827/30000 Training Loss: 0.0646413043141365\n",
      "Epoch 1828/30000 Training Loss: 0.06784384697675705\n",
      "Epoch 1829/30000 Training Loss: 0.06909915804862976\n",
      "Epoch 1830/30000 Training Loss: 0.0713852047920227\n",
      "Epoch 1831/30000 Training Loss: 0.06478501856327057\n",
      "Epoch 1832/30000 Training Loss: 0.06585216522216797\n",
      "Epoch 1833/30000 Training Loss: 0.06721703708171844\n",
      "Epoch 1834/30000 Training Loss: 0.08323627710342407\n",
      "Epoch 1835/30000 Training Loss: 0.07506005465984344\n",
      "Epoch 1836/30000 Training Loss: 0.06492426246404648\n",
      "Epoch 1837/30000 Training Loss: 0.07072136551141739\n",
      "Epoch 1838/30000 Training Loss: 0.05604869872331619\n",
      "Epoch 1839/30000 Training Loss: 0.06673440337181091\n",
      "Epoch 1840/30000 Training Loss: 0.0763520747423172\n",
      "Epoch 1841/30000 Training Loss: 0.07362872362136841\n",
      "Epoch 1842/30000 Training Loss: 0.07963664084672928\n",
      "Epoch 1843/30000 Training Loss: 0.08138149976730347\n",
      "Epoch 1844/30000 Training Loss: 0.0602538101375103\n",
      "Epoch 1845/30000 Training Loss: 0.0749925822019577\n",
      "Epoch 1846/30000 Training Loss: 0.057437777519226074\n",
      "Epoch 1847/30000 Training Loss: 0.056902870535850525\n",
      "Epoch 1848/30000 Training Loss: 0.06724889576435089\n",
      "Epoch 1849/30000 Training Loss: 0.0842161774635315\n",
      "Epoch 1850/30000 Training Loss: 0.07140136510133743\n",
      "Epoch 1851/30000 Training Loss: 0.07916931062936783\n",
      "Epoch 1852/30000 Training Loss: 0.06429258733987808\n",
      "Epoch 1853/30000 Training Loss: 0.07701662182807922\n",
      "Epoch 1854/30000 Training Loss: 0.06589598953723907\n",
      "Epoch 1855/30000 Training Loss: 0.0853864997625351\n",
      "Epoch 1856/30000 Training Loss: 0.0679803192615509\n",
      "Epoch 1857/30000 Training Loss: 0.06558354943990707\n",
      "Epoch 1858/30000 Training Loss: 0.07929135859012604\n",
      "Epoch 1859/30000 Training Loss: 0.06565585732460022\n",
      "Epoch 1860/30000 Training Loss: 0.07370971888303757\n",
      "Epoch 1861/30000 Training Loss: 0.06683870404958725\n",
      "Epoch 1862/30000 Training Loss: 0.06497019529342651\n",
      "Epoch 1863/30000 Training Loss: 0.0658835619688034\n",
      "Epoch 1864/30000 Training Loss: 0.0774279534816742\n",
      "Epoch 1865/30000 Training Loss: 0.0657493844628334\n",
      "Epoch 1866/30000 Training Loss: 0.08195256441831589\n",
      "Epoch 1867/30000 Training Loss: 0.06539519131183624\n",
      "Epoch 1868/30000 Training Loss: 0.07461989670991898\n",
      "Epoch 1869/30000 Training Loss: 0.08167588710784912\n",
      "Epoch 1870/30000 Training Loss: 0.06392519921064377\n",
      "Epoch 1871/30000 Training Loss: 0.06493020802736282\n",
      "Epoch 1872/30000 Training Loss: 0.05894738435745239\n",
      "Epoch 1873/30000 Training Loss: 0.06991863995790482\n",
      "Epoch 1874/30000 Training Loss: 0.0859832838177681\n",
      "Epoch 1875/30000 Training Loss: 0.07042664289474487\n",
      "Epoch 1876/30000 Training Loss: 0.06220141425728798\n",
      "Epoch 1877/30000 Training Loss: 0.10052813589572906\n",
      "Epoch 1878/30000 Training Loss: 0.06291240453720093\n",
      "Epoch 1879/30000 Training Loss: 0.07852480560541153\n",
      "Epoch 1880/30000 Training Loss: 0.06950759142637253\n",
      "Epoch 1881/30000 Training Loss: 0.08518080413341522\n",
      "Epoch 1882/30000 Training Loss: 0.09642249345779419\n",
      "Epoch 1883/30000 Training Loss: 0.06269165873527527\n",
      "Epoch 1884/30000 Training Loss: 0.06384064257144928\n",
      "Epoch 1885/30000 Training Loss: 0.07232996076345444\n",
      "Epoch 1886/30000 Training Loss: 0.07825560122728348\n",
      "Epoch 1887/30000 Training Loss: 0.08136577904224396\n",
      "Epoch 1888/30000 Training Loss: 0.058281391859054565\n",
      "Epoch 1889/30000 Training Loss: 0.06503106653690338\n",
      "Epoch 1890/30000 Training Loss: 0.08295293897390366\n",
      "Epoch 1891/30000 Training Loss: 0.08108022063970566\n",
      "Epoch 1892/30000 Training Loss: 0.056204624474048615\n",
      "Epoch 1893/30000 Training Loss: 0.06125064566731453\n",
      "Epoch 1894/30000 Training Loss: 0.06892664730548859\n",
      "Epoch 1895/30000 Training Loss: 0.07609984278678894\n",
      "Epoch 1896/30000 Training Loss: 0.059591904282569885\n",
      "Epoch 1897/30000 Training Loss: 0.0832185298204422\n",
      "Epoch 1898/30000 Training Loss: 0.06734231859445572\n",
      "Epoch 1899/30000 Training Loss: 0.06933076679706573\n",
      "Epoch 1900/30000 Training Loss: 0.07462907582521439\n",
      "Epoch 1900/30000 Validation Loss: 0.074924036860466\n",
      "Epoch 1901/30000 Training Loss: 0.07319582253694534\n",
      "Epoch 1902/30000 Training Loss: 0.07016368955373764\n",
      "Epoch 1903/30000 Training Loss: 0.07262486219406128\n",
      "Epoch 1904/30000 Training Loss: 0.055464692413806915\n",
      "Epoch 1905/30000 Training Loss: 0.0642700120806694\n",
      "Epoch 1906/30000 Training Loss: 0.06368560343980789\n",
      "Epoch 1907/30000 Training Loss: 0.08888254314661026\n",
      "Epoch 1908/30000 Training Loss: 0.07874754816293716\n",
      "Epoch 1909/30000 Training Loss: 0.060980141162872314\n",
      "Epoch 1910/30000 Training Loss: 0.07859474420547485\n",
      "Epoch 1911/30000 Training Loss: 0.08087428659200668\n",
      "Epoch 1912/30000 Training Loss: 0.06919314712285995\n",
      "Epoch 1913/30000 Training Loss: 0.08048217743635178\n",
      "Epoch 1914/30000 Training Loss: 0.07419034838676453\n",
      "Epoch 1915/30000 Training Loss: 0.0749717503786087\n",
      "Epoch 1916/30000 Training Loss: 0.06316186487674713\n",
      "Epoch 1917/30000 Training Loss: 0.07392928004264832\n",
      "Epoch 1918/30000 Training Loss: 0.06427733600139618\n",
      "Epoch 1919/30000 Training Loss: 0.06827197968959808\n",
      "Epoch 1920/30000 Training Loss: 0.08346286416053772\n",
      "Epoch 1921/30000 Training Loss: 0.08345384150743484\n",
      "Epoch 1922/30000 Training Loss: 0.06784482300281525\n",
      "Epoch 1923/30000 Training Loss: 0.0606561154127121\n",
      "Epoch 1924/30000 Training Loss: 0.07169029861688614\n",
      "Epoch 1925/30000 Training Loss: 0.05789356306195259\n",
      "Epoch 1926/30000 Training Loss: 0.06730519235134125\n",
      "Epoch 1927/30000 Training Loss: 0.049584824591875076\n",
      "Epoch 1928/30000 Training Loss: 0.07308691740036011\n",
      "Epoch 1929/30000 Training Loss: 0.06912273913621902\n",
      "Epoch 1930/30000 Training Loss: 0.055377401411533356\n",
      "Epoch 1931/30000 Training Loss: 0.07999944686889648\n",
      "Epoch 1932/30000 Training Loss: 0.06255055963993073\n",
      "Epoch 1933/30000 Training Loss: 0.05583680421113968\n",
      "Epoch 1934/30000 Training Loss: 0.06979458779096603\n",
      "Epoch 1935/30000 Training Loss: 0.055210039019584656\n",
      "Epoch 1936/30000 Training Loss: 0.07708792388439178\n",
      "Epoch 1937/30000 Training Loss: 0.0755399763584137\n",
      "Epoch 1938/30000 Training Loss: 0.07007622718811035\n",
      "Epoch 1939/30000 Training Loss: 0.0716291069984436\n",
      "Epoch 1940/30000 Training Loss: 0.07313401252031326\n",
      "Epoch 1941/30000 Training Loss: 0.0776081383228302\n",
      "Epoch 1942/30000 Training Loss: 0.05908476561307907\n",
      "Epoch 1943/30000 Training Loss: 0.06828558444976807\n",
      "Epoch 1944/30000 Training Loss: 0.07723703980445862\n",
      "Epoch 1945/30000 Training Loss: 0.07765845954418182\n",
      "Epoch 1946/30000 Training Loss: 0.07087139785289764\n",
      "Epoch 1947/30000 Training Loss: 0.0637301355600357\n",
      "Epoch 1948/30000 Training Loss: 0.06345020979642868\n",
      "Epoch 1949/30000 Training Loss: 0.07471849024295807\n",
      "Epoch 1950/30000 Training Loss: 0.08913926780223846\n",
      "Epoch 1951/30000 Training Loss: 0.07039210200309753\n",
      "Epoch 1952/30000 Training Loss: 0.07144400477409363\n",
      "Epoch 1953/30000 Training Loss: 0.06181081011891365\n",
      "Epoch 1954/30000 Training Loss: 0.07011093199253082\n",
      "Epoch 1955/30000 Training Loss: 0.06834977865219116\n",
      "Epoch 1956/30000 Training Loss: 0.07485150545835495\n",
      "Epoch 1957/30000 Training Loss: 0.0780651643872261\n",
      "Epoch 1958/30000 Training Loss: 0.07408934831619263\n",
      "Epoch 1959/30000 Training Loss: 0.0811505913734436\n",
      "Epoch 1960/30000 Training Loss: 0.06702779233455658\n",
      "Epoch 1961/30000 Training Loss: 0.0799335390329361\n",
      "Epoch 1962/30000 Training Loss: 0.0647125169634819\n",
      "Epoch 1963/30000 Training Loss: 0.07887964695692062\n",
      "Epoch 1964/30000 Training Loss: 0.06943759322166443\n",
      "Epoch 1965/30000 Training Loss: 0.06249067932367325\n",
      "Epoch 1966/30000 Training Loss: 0.06097271293401718\n",
      "Epoch 1967/30000 Training Loss: 0.08931363373994827\n",
      "Epoch 1968/30000 Training Loss: 0.07364832609891891\n",
      "Epoch 1969/30000 Training Loss: 0.06472638249397278\n",
      "Epoch 1970/30000 Training Loss: 0.06730286777019501\n",
      "Epoch 1971/30000 Training Loss: 0.07275119423866272\n",
      "Epoch 1972/30000 Training Loss: 0.0890238881111145\n",
      "Epoch 1973/30000 Training Loss: 0.06368705630302429\n",
      "Epoch 1974/30000 Training Loss: 0.06381021440029144\n",
      "Epoch 1975/30000 Training Loss: 0.07076169550418854\n",
      "Epoch 1976/30000 Training Loss: 0.05173814296722412\n",
      "Epoch 1977/30000 Training Loss: 0.05480555444955826\n",
      "Epoch 1978/30000 Training Loss: 0.06587143242359161\n",
      "Epoch 1979/30000 Training Loss: 0.06304515153169632\n",
      "Epoch 1980/30000 Training Loss: 0.08353686332702637\n",
      "Epoch 1981/30000 Training Loss: 0.09297318756580353\n",
      "Epoch 1982/30000 Training Loss: 0.06565843522548676\n",
      "Epoch 1983/30000 Training Loss: 0.06057161092758179\n",
      "Epoch 1984/30000 Training Loss: 0.06448891013860703\n",
      "Epoch 1985/30000 Training Loss: 0.05237632244825363\n",
      "Epoch 1986/30000 Training Loss: 0.060281507670879364\n",
      "Epoch 1987/30000 Training Loss: 0.08123500645160675\n",
      "Epoch 1988/30000 Training Loss: 0.058808498084545135\n",
      "Epoch 1989/30000 Training Loss: 0.07814736664295197\n",
      "Epoch 1990/30000 Training Loss: 0.08253790438175201\n",
      "Epoch 1991/30000 Training Loss: 0.06399724632501602\n",
      "Epoch 1992/30000 Training Loss: 0.06720957159996033\n",
      "Epoch 1993/30000 Training Loss: 0.08202101290225983\n",
      "Epoch 1994/30000 Training Loss: 0.061677686870098114\n",
      "Epoch 1995/30000 Training Loss: 0.07818993180990219\n",
      "Epoch 1996/30000 Training Loss: 0.07995232194662094\n",
      "Epoch 1997/30000 Training Loss: 0.06997938454151154\n",
      "Epoch 1998/30000 Training Loss: 0.08148445188999176\n",
      "Epoch 1999/30000 Training Loss: 0.07741466909646988\n",
      "Epoch 2000/30000 Training Loss: 0.0597279891371727\n",
      "Epoch 2000/30000 Validation Loss: 0.06479248404502869\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06479248404502869<=============\n",
      "Epoch 2001/30000 Training Loss: 0.06295675039291382\n",
      "Epoch 2002/30000 Training Loss: 0.05830647796392441\n",
      "Epoch 2003/30000 Training Loss: 0.06458880007266998\n",
      "Epoch 2004/30000 Training Loss: 0.07195169478654861\n",
      "Epoch 2005/30000 Training Loss: 0.052256856113672256\n",
      "Epoch 2006/30000 Training Loss: 0.05997423827648163\n",
      "Epoch 2007/30000 Training Loss: 0.08037823438644409\n",
      "Epoch 2008/30000 Training Loss: 0.0645291730761528\n",
      "Epoch 2009/30000 Training Loss: 0.07815413922071457\n",
      "Epoch 2010/30000 Training Loss: 0.06392519176006317\n",
      "Epoch 2011/30000 Training Loss: 0.07377365231513977\n",
      "Epoch 2012/30000 Training Loss: 0.05799820274114609\n",
      "Epoch 2013/30000 Training Loss: 0.06731261312961578\n",
      "Epoch 2014/30000 Training Loss: 0.06950938701629639\n",
      "Epoch 2015/30000 Training Loss: 0.06849846243858337\n",
      "Epoch 2016/30000 Training Loss: 0.06792211532592773\n",
      "Epoch 2017/30000 Training Loss: 0.08687108755111694\n",
      "Epoch 2018/30000 Training Loss: 0.05525333434343338\n",
      "Epoch 2019/30000 Training Loss: 0.0785868689417839\n",
      "Epoch 2020/30000 Training Loss: 0.0780070349574089\n",
      "Epoch 2021/30000 Training Loss: 0.06481866538524628\n",
      "Epoch 2022/30000 Training Loss: 0.06839723140001297\n",
      "Epoch 2023/30000 Training Loss: 0.07815569639205933\n",
      "Epoch 2024/30000 Training Loss: 0.06913361698389053\n",
      "Epoch 2025/30000 Training Loss: 0.07932206988334656\n",
      "Epoch 2026/30000 Training Loss: 0.06610065698623657\n",
      "Epoch 2027/30000 Training Loss: 0.06501229107379913\n",
      "Epoch 2028/30000 Training Loss: 0.052112773060798645\n",
      "Epoch 2029/30000 Training Loss: 0.07715978473424911\n",
      "Epoch 2030/30000 Training Loss: 0.0655173510313034\n",
      "Epoch 2031/30000 Training Loss: 0.06287666410207748\n",
      "Epoch 2032/30000 Training Loss: 0.06608346849679947\n",
      "Epoch 2033/30000 Training Loss: 0.0645747184753418\n",
      "Epoch 2034/30000 Training Loss: 0.0734390988945961\n",
      "Epoch 2035/30000 Training Loss: 0.05771762505173683\n",
      "Epoch 2036/30000 Training Loss: 0.07595422863960266\n",
      "Epoch 2037/30000 Training Loss: 0.062295593321323395\n",
      "Epoch 2038/30000 Training Loss: 0.07335160672664642\n",
      "Epoch 2039/30000 Training Loss: 0.06530826538801193\n",
      "Epoch 2040/30000 Training Loss: 0.07995210587978363\n",
      "Epoch 2041/30000 Training Loss: 0.06025669723749161\n",
      "Epoch 2042/30000 Training Loss: 0.06600086390972137\n",
      "Epoch 2043/30000 Training Loss: 0.060394853353500366\n",
      "Epoch 2044/30000 Training Loss: 0.08199310302734375\n",
      "Epoch 2045/30000 Training Loss: 0.06460385769605637\n",
      "Epoch 2046/30000 Training Loss: 0.06500445306301117\n",
      "Epoch 2047/30000 Training Loss: 0.062059588730335236\n",
      "Epoch 2048/30000 Training Loss: 0.07132080942392349\n",
      "Epoch 2049/30000 Training Loss: 0.08465006947517395\n",
      "Epoch 2050/30000 Training Loss: 0.07394049316644669\n",
      "Epoch 2051/30000 Training Loss: 0.06471528112888336\n",
      "Epoch 2052/30000 Training Loss: 0.06734836101531982\n",
      "Epoch 2053/30000 Training Loss: 0.05359194427728653\n",
      "Epoch 2054/30000 Training Loss: 0.07392910122871399\n",
      "Epoch 2055/30000 Training Loss: 0.0730166882276535\n",
      "Epoch 2056/30000 Training Loss: 0.0678357258439064\n",
      "Epoch 2057/30000 Training Loss: 0.07989427447319031\n",
      "Epoch 2058/30000 Training Loss: 0.061167821288108826\n",
      "Epoch 2059/30000 Training Loss: 0.07051302492618561\n",
      "Epoch 2060/30000 Training Loss: 0.06878058612346649\n",
      "Epoch 2061/30000 Training Loss: 0.07735179364681244\n",
      "Epoch 2062/30000 Training Loss: 0.06513012945652008\n",
      "Epoch 2063/30000 Training Loss: 0.05842036008834839\n",
      "Epoch 2064/30000 Training Loss: 0.05588188022375107\n",
      "Epoch 2065/30000 Training Loss: 0.056301891803741455\n",
      "Epoch 2066/30000 Training Loss: 0.06364606320858002\n",
      "Epoch 2067/30000 Training Loss: 0.06982173025608063\n",
      "Epoch 2068/30000 Training Loss: 0.06763644516468048\n",
      "Epoch 2069/30000 Training Loss: 0.07219824939966202\n",
      "Epoch 2070/30000 Training Loss: 0.06690485030412674\n",
      "Epoch 2071/30000 Training Loss: 0.06833140552043915\n",
      "Epoch 2072/30000 Training Loss: 0.06996917724609375\n",
      "Epoch 2073/30000 Training Loss: 0.06496268510818481\n",
      "Epoch 2074/30000 Training Loss: 0.05508369952440262\n",
      "Epoch 2075/30000 Training Loss: 0.0933389812707901\n",
      "Epoch 2076/30000 Training Loss: 0.07848867774009705\n",
      "Epoch 2077/30000 Training Loss: 0.07345518469810486\n",
      "Epoch 2078/30000 Training Loss: 0.08267084509134293\n",
      "Epoch 2079/30000 Training Loss: 0.06886228173971176\n",
      "Epoch 2080/30000 Training Loss: 0.07661940157413483\n",
      "Epoch 2081/30000 Training Loss: 0.05379924178123474\n",
      "Epoch 2082/30000 Training Loss: 0.0717950165271759\n",
      "Epoch 2083/30000 Training Loss: 0.06846029311418533\n",
      "Epoch 2084/30000 Training Loss: 0.06606068462133408\n",
      "Epoch 2085/30000 Training Loss: 0.0744260847568512\n",
      "Epoch 2086/30000 Training Loss: 0.05410635471343994\n",
      "Epoch 2087/30000 Training Loss: 0.07869721949100494\n",
      "Epoch 2088/30000 Training Loss: 0.06409172713756561\n",
      "Epoch 2089/30000 Training Loss: 0.06916622817516327\n",
      "Epoch 2090/30000 Training Loss: 0.057183846831321716\n",
      "Epoch 2091/30000 Training Loss: 0.04948894679546356\n",
      "Epoch 2092/30000 Training Loss: 0.07347597926855087\n",
      "Epoch 2093/30000 Training Loss: 0.0677768662571907\n",
      "Epoch 2094/30000 Training Loss: 0.06902210414409637\n",
      "Epoch 2095/30000 Training Loss: 0.08019302785396576\n",
      "Epoch 2096/30000 Training Loss: 0.06974554061889648\n",
      "Epoch 2097/30000 Training Loss: 0.07886995375156403\n",
      "Epoch 2098/30000 Training Loss: 0.07394169270992279\n",
      "Epoch 2099/30000 Training Loss: 0.06205663084983826\n",
      "Epoch 2100/30000 Training Loss: 0.06822333484888077\n",
      "Epoch 2100/30000 Validation Loss: 0.07992497831583023\n",
      "Epoch 2101/30000 Training Loss: 0.051563262939453125\n",
      "Epoch 2102/30000 Training Loss: 0.08038388937711716\n",
      "Epoch 2103/30000 Training Loss: 0.047798529267311096\n",
      "Epoch 2104/30000 Training Loss: 0.06836336106061935\n",
      "Epoch 2105/30000 Training Loss: 0.07126042991876602\n",
      "Epoch 2106/30000 Training Loss: 0.08886776864528656\n",
      "Epoch 2107/30000 Training Loss: 0.07603317499160767\n",
      "Epoch 2108/30000 Training Loss: 0.06675512343645096\n",
      "Epoch 2109/30000 Training Loss: 0.07004223763942719\n",
      "Epoch 2110/30000 Training Loss: 0.06834132224321365\n",
      "Epoch 2111/30000 Training Loss: 0.0705469399690628\n",
      "Epoch 2112/30000 Training Loss: 0.06528594344854355\n",
      "Epoch 2113/30000 Training Loss: 0.0753515213727951\n",
      "Epoch 2114/30000 Training Loss: 0.07937070727348328\n",
      "Epoch 2115/30000 Training Loss: 0.0668332651257515\n",
      "Epoch 2116/30000 Training Loss: 0.06472254544496536\n",
      "Epoch 2117/30000 Training Loss: 0.06457027047872543\n",
      "Epoch 2118/30000 Training Loss: 0.08543777465820312\n",
      "Epoch 2119/30000 Training Loss: 0.05835309624671936\n",
      "Epoch 2120/30000 Training Loss: 0.07737350463867188\n",
      "Epoch 2121/30000 Training Loss: 0.057932935655117035\n",
      "Epoch 2122/30000 Training Loss: 0.06776288896799088\n",
      "Epoch 2123/30000 Training Loss: 0.05768151953816414\n",
      "Epoch 2124/30000 Training Loss: 0.07555215060710907\n",
      "Epoch 2125/30000 Training Loss: 0.06094449758529663\n",
      "Epoch 2126/30000 Training Loss: 0.07238572835922241\n",
      "Epoch 2127/30000 Training Loss: 0.06807784736156464\n",
      "Epoch 2128/30000 Training Loss: 0.07547952234745026\n",
      "Epoch 2129/30000 Training Loss: 0.06953608244657516\n",
      "Epoch 2130/30000 Training Loss: 0.057471513748168945\n",
      "Epoch 2131/30000 Training Loss: 0.06833939254283905\n",
      "Epoch 2132/30000 Training Loss: 0.07603991031646729\n",
      "Epoch 2133/30000 Training Loss: 0.0633476972579956\n",
      "Epoch 2134/30000 Training Loss: 0.062183596193790436\n",
      "Epoch 2135/30000 Training Loss: 0.07125762104988098\n",
      "Epoch 2136/30000 Training Loss: 0.06910787522792816\n",
      "Epoch 2137/30000 Training Loss: 0.05831833556294441\n",
      "Epoch 2138/30000 Training Loss: 0.056210391223430634\n",
      "Epoch 2139/30000 Training Loss: 0.07984817773103714\n",
      "Epoch 2140/30000 Training Loss: 0.06313831359148026\n",
      "Epoch 2141/30000 Training Loss: 0.05579717829823494\n",
      "Epoch 2142/30000 Training Loss: 0.05605337396264076\n",
      "Epoch 2143/30000 Training Loss: 0.0584389790892601\n",
      "Epoch 2144/30000 Training Loss: 0.0511622354388237\n",
      "Epoch 2145/30000 Training Loss: 0.06638909876346588\n",
      "Epoch 2146/30000 Training Loss: 0.05927898734807968\n",
      "Epoch 2147/30000 Training Loss: 0.057167887687683105\n",
      "Epoch 2148/30000 Training Loss: 0.06259442865848541\n",
      "Epoch 2149/30000 Training Loss: 0.06826139986515045\n",
      "Epoch 2150/30000 Training Loss: 0.0776178315281868\n",
      "Epoch 2151/30000 Training Loss: 0.08525198698043823\n",
      "Epoch 2152/30000 Training Loss: 0.05597372353076935\n",
      "Epoch 2153/30000 Training Loss: 0.05445194989442825\n",
      "Epoch 2154/30000 Training Loss: 0.07626835256814957\n",
      "Epoch 2155/30000 Training Loss: 0.0670332983136177\n",
      "Epoch 2156/30000 Training Loss: 0.08448614180088043\n",
      "Epoch 2157/30000 Training Loss: 0.05460216850042343\n",
      "Epoch 2158/30000 Training Loss: 0.06414595991373062\n",
      "Epoch 2159/30000 Training Loss: 0.0607350617647171\n",
      "Epoch 2160/30000 Training Loss: 0.051882870495319366\n",
      "Epoch 2161/30000 Training Loss: 0.07964374125003815\n",
      "Epoch 2162/30000 Training Loss: 0.06797094643115997\n",
      "Epoch 2163/30000 Training Loss: 0.07902347296476364\n",
      "Epoch 2164/30000 Training Loss: 0.07052155584096909\n",
      "Epoch 2165/30000 Training Loss: 0.06166388466954231\n",
      "Epoch 2166/30000 Training Loss: 0.08011769503355026\n",
      "Epoch 2167/30000 Training Loss: 0.0605083629488945\n",
      "Epoch 2168/30000 Training Loss: 0.07728886604309082\n",
      "Epoch 2169/30000 Training Loss: 0.06208325922489166\n",
      "Epoch 2170/30000 Training Loss: 0.06753966212272644\n",
      "Epoch 2171/30000 Training Loss: 0.08455520868301392\n",
      "Epoch 2172/30000 Training Loss: 0.07041984796524048\n",
      "Epoch 2173/30000 Training Loss: 0.07085132598876953\n",
      "Epoch 2174/30000 Training Loss: 0.07548952847719193\n",
      "Epoch 2175/30000 Training Loss: 0.06191360950469971\n",
      "Epoch 2176/30000 Training Loss: 0.06375323235988617\n",
      "Epoch 2177/30000 Training Loss: 0.07802661508321762\n",
      "Epoch 2178/30000 Training Loss: 0.06604710221290588\n",
      "Epoch 2179/30000 Training Loss: 0.062384650111198425\n",
      "Epoch 2180/30000 Training Loss: 0.07094573974609375\n",
      "Epoch 2181/30000 Training Loss: 0.054679591208696365\n",
      "Epoch 2182/30000 Training Loss: 0.06406405568122864\n",
      "Epoch 2183/30000 Training Loss: 0.06575178354978561\n",
      "Epoch 2184/30000 Training Loss: 0.0663856714963913\n",
      "Epoch 2185/30000 Training Loss: 0.059564877301454544\n",
      "Epoch 2186/30000 Training Loss: 0.051411665976047516\n",
      "Epoch 2187/30000 Training Loss: 0.06364303827285767\n",
      "Epoch 2188/30000 Training Loss: 0.06805277615785599\n",
      "Epoch 2189/30000 Training Loss: 0.055763669312000275\n",
      "Epoch 2190/30000 Training Loss: 0.07916615158319473\n",
      "Epoch 2191/30000 Training Loss: 0.06705397367477417\n",
      "Epoch 2192/30000 Training Loss: 0.07810463011264801\n",
      "Epoch 2193/30000 Training Loss: 0.06209909915924072\n",
      "Epoch 2194/30000 Training Loss: 0.06632787734270096\n",
      "Epoch 2195/30000 Training Loss: 0.06047371029853821\n",
      "Epoch 2196/30000 Training Loss: 0.06679680943489075\n",
      "Epoch 2197/30000 Training Loss: 0.07147787511348724\n",
      "Epoch 2198/30000 Training Loss: 0.07242647558450699\n",
      "Epoch 2199/30000 Training Loss: 0.07634176313877106\n",
      "Epoch 2200/30000 Training Loss: 0.08289367705583572\n",
      "Epoch 2200/30000 Validation Loss: 0.06963872909545898\n",
      "Epoch 2201/30000 Training Loss: 0.06152183562517166\n",
      "Epoch 2202/30000 Training Loss: 0.06281839311122894\n",
      "Epoch 2203/30000 Training Loss: 0.06230555847287178\n",
      "Epoch 2204/30000 Training Loss: 0.06748898327350616\n",
      "Epoch 2205/30000 Training Loss: 0.06980384141206741\n",
      "Epoch 2206/30000 Training Loss: 0.07726055383682251\n",
      "Epoch 2207/30000 Training Loss: 0.05821322277188301\n",
      "Epoch 2208/30000 Training Loss: 0.07527793943881989\n",
      "Epoch 2209/30000 Training Loss: 0.05289134755730629\n",
      "Epoch 2210/30000 Training Loss: 0.07561045140028\n",
      "Epoch 2211/30000 Training Loss: 0.06474035978317261\n",
      "Epoch 2212/30000 Training Loss: 0.057156436145305634\n",
      "Epoch 2213/30000 Training Loss: 0.09329874813556671\n",
      "Epoch 2214/30000 Training Loss: 0.0516149178147316\n",
      "Epoch 2215/30000 Training Loss: 0.07037253677845001\n",
      "Epoch 2216/30000 Training Loss: 0.07921675592660904\n",
      "Epoch 2217/30000 Training Loss: 0.08069337904453278\n",
      "Epoch 2218/30000 Training Loss: 0.06409571319818497\n",
      "Epoch 2219/30000 Training Loss: 0.04606550931930542\n",
      "Epoch 2220/30000 Training Loss: 0.0658925324678421\n",
      "Epoch 2221/30000 Training Loss: 0.06408123672008514\n",
      "Epoch 2222/30000 Training Loss: 0.06747698783874512\n",
      "Epoch 2223/30000 Training Loss: 0.05831559747457504\n",
      "Epoch 2224/30000 Training Loss: 0.07313472032546997\n",
      "Epoch 2225/30000 Training Loss: 0.057263754308223724\n",
      "Epoch 2226/30000 Training Loss: 0.06035637483000755\n",
      "Epoch 2227/30000 Training Loss: 0.08119194209575653\n",
      "Epoch 2228/30000 Training Loss: 0.08007834106683731\n",
      "Epoch 2229/30000 Training Loss: 0.07287174463272095\n",
      "Epoch 2230/30000 Training Loss: 0.07447196543216705\n",
      "Epoch 2231/30000 Training Loss: 0.06715217977762222\n",
      "Epoch 2232/30000 Training Loss: 0.0712970644235611\n",
      "Epoch 2233/30000 Training Loss: 0.057618819177150726\n",
      "Epoch 2234/30000 Training Loss: 0.061142995953559875\n",
      "Epoch 2235/30000 Training Loss: 0.06912554800510406\n",
      "Epoch 2236/30000 Training Loss: 0.04995062202215195\n",
      "Epoch 2237/30000 Training Loss: 0.055549394339323044\n",
      "Epoch 2238/30000 Training Loss: 0.07335872948169708\n",
      "Epoch 2239/30000 Training Loss: 0.06611941009759903\n",
      "Epoch 2240/30000 Training Loss: 0.06695825606584549\n",
      "Epoch 2241/30000 Training Loss: 0.0672149509191513\n",
      "Epoch 2242/30000 Training Loss: 0.07405215501785278\n",
      "Epoch 2243/30000 Training Loss: 0.061433084309101105\n",
      "Epoch 2244/30000 Training Loss: 0.06500519067049026\n",
      "Epoch 2245/30000 Training Loss: 0.05515294522047043\n",
      "Epoch 2246/30000 Training Loss: 0.07254444807767868\n",
      "Epoch 2247/30000 Training Loss: 0.05466039851307869\n",
      "Epoch 2248/30000 Training Loss: 0.07344534993171692\n",
      "Epoch 2249/30000 Training Loss: 0.08468584716320038\n",
      "Epoch 2250/30000 Training Loss: 0.08233514428138733\n",
      "Epoch 2251/30000 Training Loss: 0.07007824629545212\n",
      "Epoch 2252/30000 Training Loss: 0.06458310782909393\n",
      "Epoch 2253/30000 Training Loss: 0.065159872174263\n",
      "Epoch 2254/30000 Training Loss: 0.05881423503160477\n",
      "Epoch 2255/30000 Training Loss: 0.07046860456466675\n",
      "Epoch 2256/30000 Training Loss: 0.06123945116996765\n",
      "Epoch 2257/30000 Training Loss: 0.07299770414829254\n",
      "Epoch 2258/30000 Training Loss: 0.05763838440179825\n",
      "Epoch 2259/30000 Training Loss: 0.06218109279870987\n",
      "Epoch 2260/30000 Training Loss: 0.07354626059532166\n",
      "Epoch 2261/30000 Training Loss: 0.06521908938884735\n",
      "Epoch 2262/30000 Training Loss: 0.08016835153102875\n",
      "Epoch 2263/30000 Training Loss: 0.07301560789346695\n",
      "Epoch 2264/30000 Training Loss: 0.06800208985805511\n",
      "Epoch 2265/30000 Training Loss: 0.059895358979701996\n",
      "Epoch 2266/30000 Training Loss: 0.06884416192770004\n",
      "Epoch 2267/30000 Training Loss: 0.07842252403497696\n",
      "Epoch 2268/30000 Training Loss: 0.060797352343797684\n",
      "Epoch 2269/30000 Training Loss: 0.0557212233543396\n",
      "Epoch 2270/30000 Training Loss: 0.06794354319572449\n",
      "Epoch 2271/30000 Training Loss: 0.06503187119960785\n",
      "Epoch 2272/30000 Training Loss: 0.06374610215425491\n",
      "Epoch 2273/30000 Training Loss: 0.061015114188194275\n",
      "Epoch 2274/30000 Training Loss: 0.05762527883052826\n",
      "Epoch 2275/30000 Training Loss: 0.07403385639190674\n",
      "Epoch 2276/30000 Training Loss: 0.07243306934833527\n",
      "Epoch 2277/30000 Training Loss: 0.07041681557893753\n",
      "Epoch 2278/30000 Training Loss: 0.07220806181430817\n",
      "Epoch 2279/30000 Training Loss: 0.06389624625444412\n",
      "Epoch 2280/30000 Training Loss: 0.06869539618492126\n",
      "Epoch 2281/30000 Training Loss: 0.06341105699539185\n",
      "Epoch 2282/30000 Training Loss: 0.07105977833271027\n",
      "Epoch 2283/30000 Training Loss: 0.06921078264713287\n",
      "Epoch 2284/30000 Training Loss: 0.07740539312362671\n",
      "Epoch 2285/30000 Training Loss: 0.06372116506099701\n",
      "Epoch 2286/30000 Training Loss: 0.07334389537572861\n",
      "Epoch 2287/30000 Training Loss: 0.06908778101205826\n",
      "Epoch 2288/30000 Training Loss: 0.0551297552883625\n",
      "Epoch 2289/30000 Training Loss: 0.05441474914550781\n",
      "Epoch 2290/30000 Training Loss: 0.07112116366624832\n",
      "Epoch 2291/30000 Training Loss: 0.06594018638134003\n",
      "Epoch 2292/30000 Training Loss: 0.07866363227367401\n",
      "Epoch 2293/30000 Training Loss: 0.057749368250370026\n",
      "Epoch 2294/30000 Training Loss: 0.06845289468765259\n",
      "Epoch 2295/30000 Training Loss: 0.07203638553619385\n",
      "Epoch 2296/30000 Training Loss: 0.07136958837509155\n",
      "Epoch 2297/30000 Training Loss: 0.05688611418008804\n",
      "Epoch 2298/30000 Training Loss: 0.07244759798049927\n",
      "Epoch 2299/30000 Training Loss: 0.08280839771032333\n",
      "Epoch 2300/30000 Training Loss: 0.07711370289325714\n",
      "Epoch 2300/30000 Validation Loss: 0.052928000688552856\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.052928000688552856<=============\n",
      "Epoch 2301/30000 Training Loss: 0.05808877572417259\n",
      "Epoch 2302/30000 Training Loss: 0.06055626645684242\n",
      "Epoch 2303/30000 Training Loss: 0.06454869359731674\n",
      "Epoch 2304/30000 Training Loss: 0.06647038459777832\n",
      "Epoch 2305/30000 Training Loss: 0.051597870886325836\n",
      "Epoch 2306/30000 Training Loss: 0.06616373360157013\n",
      "Epoch 2307/30000 Training Loss: 0.08518306165933609\n",
      "Epoch 2308/30000 Training Loss: 0.05427095293998718\n",
      "Epoch 2309/30000 Training Loss: 0.054821789264678955\n",
      "Epoch 2310/30000 Training Loss: 0.06847849488258362\n",
      "Epoch 2311/30000 Training Loss: 0.05212544649839401\n",
      "Epoch 2312/30000 Training Loss: 0.06682631373405457\n",
      "Epoch 2313/30000 Training Loss: 0.06938448548316956\n",
      "Epoch 2314/30000 Training Loss: 0.07508774101734161\n",
      "Epoch 2315/30000 Training Loss: 0.07444218546152115\n",
      "Epoch 2316/30000 Training Loss: 0.07303586602210999\n",
      "Epoch 2317/30000 Training Loss: 0.07077724486589432\n",
      "Epoch 2318/30000 Training Loss: 0.08207973837852478\n",
      "Epoch 2319/30000 Training Loss: 0.07127651572227478\n",
      "Epoch 2320/30000 Training Loss: 0.07125528156757355\n",
      "Epoch 2321/30000 Training Loss: 0.07106637954711914\n",
      "Epoch 2322/30000 Training Loss: 0.07087752223014832\n",
      "Epoch 2323/30000 Training Loss: 0.07225926220417023\n",
      "Epoch 2324/30000 Training Loss: 0.06729919463396072\n",
      "Epoch 2325/30000 Training Loss: 0.07745489478111267\n",
      "Epoch 2326/30000 Training Loss: 0.0665210634469986\n",
      "Epoch 2327/30000 Training Loss: 0.06704845279455185\n",
      "Epoch 2328/30000 Training Loss: 0.06845973432064056\n",
      "Epoch 2329/30000 Training Loss: 0.06806330382823944\n",
      "Epoch 2330/30000 Training Loss: 0.08068662881851196\n",
      "Epoch 2331/30000 Training Loss: 0.05567735433578491\n",
      "Epoch 2332/30000 Training Loss: 0.08209174871444702\n",
      "Epoch 2333/30000 Training Loss: 0.07781687378883362\n",
      "Epoch 2334/30000 Training Loss: 0.04664061218500137\n",
      "Epoch 2335/30000 Training Loss: 0.05423247069120407\n",
      "Epoch 2336/30000 Training Loss: 0.07554549723863602\n",
      "Epoch 2337/30000 Training Loss: 0.05476392060518265\n",
      "Epoch 2338/30000 Training Loss: 0.06265147030353546\n",
      "Epoch 2339/30000 Training Loss: 0.06548506021499634\n",
      "Epoch 2340/30000 Training Loss: 0.06903117150068283\n",
      "Epoch 2341/30000 Training Loss: 0.0573444589972496\n",
      "Epoch 2342/30000 Training Loss: 0.07512064278125763\n",
      "Epoch 2343/30000 Training Loss: 0.06595160812139511\n",
      "Epoch 2344/30000 Training Loss: 0.06656400859355927\n",
      "Epoch 2345/30000 Training Loss: 0.07222974300384521\n",
      "Epoch 2346/30000 Training Loss: 0.07001957297325134\n",
      "Epoch 2347/30000 Training Loss: 0.06135326623916626\n",
      "Epoch 2348/30000 Training Loss: 0.06092296913266182\n",
      "Epoch 2349/30000 Training Loss: 0.08753502368927002\n",
      "Epoch 2350/30000 Training Loss: 0.055698513984680176\n",
      "Epoch 2351/30000 Training Loss: 0.0674506425857544\n",
      "Epoch 2352/30000 Training Loss: 0.060685284435749054\n",
      "Epoch 2353/30000 Training Loss: 0.08424890041351318\n",
      "Epoch 2354/30000 Training Loss: 0.055981822311878204\n",
      "Epoch 2355/30000 Training Loss: 0.05431925505399704\n",
      "Epoch 2356/30000 Training Loss: 0.056869566440582275\n",
      "Epoch 2357/30000 Training Loss: 0.05326049029827118\n",
      "Epoch 2358/30000 Training Loss: 0.053270675241947174\n",
      "Epoch 2359/30000 Training Loss: 0.06165938824415207\n",
      "Epoch 2360/30000 Training Loss: 0.07292137295007706\n",
      "Epoch 2361/30000 Training Loss: 0.08120506256818771\n",
      "Epoch 2362/30000 Training Loss: 0.06086276099085808\n",
      "Epoch 2363/30000 Training Loss: 0.05082463473081589\n",
      "Epoch 2364/30000 Training Loss: 0.08436709642410278\n",
      "Epoch 2365/30000 Training Loss: 0.07452406734228134\n",
      "Epoch 2366/30000 Training Loss: 0.06688129901885986\n",
      "Epoch 2367/30000 Training Loss: 0.07279669493436813\n",
      "Epoch 2368/30000 Training Loss: 0.057810425758361816\n",
      "Epoch 2369/30000 Training Loss: 0.06020490080118179\n",
      "Epoch 2370/30000 Training Loss: 0.07109158486127853\n",
      "Epoch 2371/30000 Training Loss: 0.06752565503120422\n",
      "Epoch 2372/30000 Training Loss: 0.08336624503135681\n",
      "Epoch 2373/30000 Training Loss: 0.06262072920799255\n",
      "Epoch 2374/30000 Training Loss: 0.06947219371795654\n",
      "Epoch 2375/30000 Training Loss: 0.07598869502544403\n",
      "Epoch 2376/30000 Training Loss: 0.06436917185783386\n",
      "Epoch 2377/30000 Training Loss: 0.05817113071680069\n",
      "Epoch 2378/30000 Training Loss: 0.07912620902061462\n",
      "Epoch 2379/30000 Training Loss: 0.06925594061613083\n",
      "Epoch 2380/30000 Training Loss: 0.06723567098379135\n",
      "Epoch 2381/30000 Training Loss: 0.07793845236301422\n",
      "Epoch 2382/30000 Training Loss: 0.0597400888800621\n",
      "Epoch 2383/30000 Training Loss: 0.05630524456501007\n",
      "Epoch 2384/30000 Training Loss: 0.0655718520283699\n",
      "Epoch 2385/30000 Training Loss: 0.06243452429771423\n",
      "Epoch 2386/30000 Training Loss: 0.060970455408096313\n",
      "Epoch 2387/30000 Training Loss: 0.05646991729736328\n",
      "Epoch 2388/30000 Training Loss: 0.059599392116069794\n",
      "Epoch 2389/30000 Training Loss: 0.07060418277978897\n",
      "Epoch 2390/30000 Training Loss: 0.07175258547067642\n",
      "Epoch 2391/30000 Training Loss: 0.05162986367940903\n",
      "Epoch 2392/30000 Training Loss: 0.07410846650600433\n",
      "Epoch 2393/30000 Training Loss: 0.06606337428092957\n",
      "Epoch 2394/30000 Training Loss: 0.07128722965717316\n",
      "Epoch 2395/30000 Training Loss: 0.0707247257232666\n",
      "Epoch 2396/30000 Training Loss: 0.06165129691362381\n",
      "Epoch 2397/30000 Training Loss: 0.05811712518334389\n",
      "Epoch 2398/30000 Training Loss: 0.06907475739717484\n",
      "Epoch 2399/30000 Training Loss: 0.05370339751243591\n",
      "Epoch 2400/30000 Training Loss: 0.06143979728221893\n",
      "Epoch 2400/30000 Validation Loss: 0.05456673353910446\n",
      "Epoch 2401/30000 Training Loss: 0.07370442152023315\n",
      "Epoch 2402/30000 Training Loss: 0.06773777306079865\n",
      "Epoch 2403/30000 Training Loss: 0.06570157408714294\n",
      "Epoch 2404/30000 Training Loss: 0.05566983297467232\n",
      "Epoch 2405/30000 Training Loss: 0.06401976943016052\n",
      "Epoch 2406/30000 Training Loss: 0.0589413195848465\n",
      "Epoch 2407/30000 Training Loss: 0.06168578192591667\n",
      "Epoch 2408/30000 Training Loss: 0.06282702088356018\n",
      "Epoch 2409/30000 Training Loss: 0.05758857727050781\n",
      "Epoch 2410/30000 Training Loss: 0.05747584253549576\n",
      "Epoch 2411/30000 Training Loss: 0.058487314730882645\n",
      "Epoch 2412/30000 Training Loss: 0.06866662204265594\n",
      "Epoch 2413/30000 Training Loss: 0.0864829495549202\n",
      "Epoch 2414/30000 Training Loss: 0.07339450716972351\n",
      "Epoch 2415/30000 Training Loss: 0.06789621710777283\n",
      "Epoch 2416/30000 Training Loss: 0.07546348124742508\n",
      "Epoch 2417/30000 Training Loss: 0.07166982442140579\n",
      "Epoch 2418/30000 Training Loss: 0.06782348453998566\n",
      "Epoch 2419/30000 Training Loss: 0.07826894521713257\n",
      "Epoch 2420/30000 Training Loss: 0.07677207887172699\n",
      "Epoch 2421/30000 Training Loss: 0.0670560970902443\n",
      "Epoch 2422/30000 Training Loss: 0.07290942221879959\n",
      "Epoch 2423/30000 Training Loss: 0.0587134063243866\n",
      "Epoch 2424/30000 Training Loss: 0.06504665315151215\n",
      "Epoch 2425/30000 Training Loss: 0.06511756777763367\n",
      "Epoch 2426/30000 Training Loss: 0.04965417459607124\n",
      "Epoch 2427/30000 Training Loss: 0.061510249972343445\n",
      "Epoch 2428/30000 Training Loss: 0.07486613839864731\n",
      "Epoch 2429/30000 Training Loss: 0.08873939514160156\n",
      "Epoch 2430/30000 Training Loss: 0.0647466629743576\n",
      "Epoch 2431/30000 Training Loss: 0.058549292385578156\n",
      "Epoch 2432/30000 Training Loss: 0.07383929193019867\n",
      "Epoch 2433/30000 Training Loss: 0.05813683569431305\n",
      "Epoch 2434/30000 Training Loss: 0.05717814341187477\n",
      "Epoch 2435/30000 Training Loss: 0.07468324899673462\n",
      "Epoch 2436/30000 Training Loss: 0.06855565309524536\n",
      "Epoch 2437/30000 Training Loss: 0.06479209661483765\n",
      "Epoch 2438/30000 Training Loss: 0.07438047230243683\n",
      "Epoch 2439/30000 Training Loss: 0.07773914188146591\n",
      "Epoch 2440/30000 Training Loss: 0.06769578158855438\n",
      "Epoch 2441/30000 Training Loss: 0.057600051164627075\n",
      "Epoch 2442/30000 Training Loss: 0.07292622327804565\n",
      "Epoch 2443/30000 Training Loss: 0.06393057852983475\n",
      "Epoch 2444/30000 Training Loss: 0.06374938786029816\n",
      "Epoch 2445/30000 Training Loss: 0.06300002336502075\n",
      "Epoch 2446/30000 Training Loss: 0.05374116450548172\n",
      "Epoch 2447/30000 Training Loss: 0.07972236722707748\n",
      "Epoch 2448/30000 Training Loss: 0.06187380105257034\n",
      "Epoch 2449/30000 Training Loss: 0.06624049693346024\n",
      "Epoch 2450/30000 Training Loss: 0.06520964205265045\n",
      "Epoch 2451/30000 Training Loss: 0.07654768228530884\n",
      "Epoch 2452/30000 Training Loss: 0.08040347695350647\n",
      "Epoch 2453/30000 Training Loss: 0.062484290450811386\n",
      "Epoch 2454/30000 Training Loss: 0.06393313407897949\n",
      "Epoch 2455/30000 Training Loss: 0.06462759524583817\n",
      "Epoch 2456/30000 Training Loss: 0.06850980222225189\n",
      "Epoch 2457/30000 Training Loss: 0.06966638565063477\n",
      "Epoch 2458/30000 Training Loss: 0.050625480711460114\n",
      "Epoch 2459/30000 Training Loss: 0.07543538510799408\n",
      "Epoch 2460/30000 Training Loss: 0.06901726126670837\n",
      "Epoch 2461/30000 Training Loss: 0.07251216471195221\n",
      "Epoch 2462/30000 Training Loss: 0.06339573115110397\n",
      "Epoch 2463/30000 Training Loss: 0.06645141541957855\n",
      "Epoch 2464/30000 Training Loss: 0.06127122789621353\n",
      "Epoch 2465/30000 Training Loss: 0.058306097984313965\n",
      "Epoch 2466/30000 Training Loss: 0.05865826457738876\n",
      "Epoch 2467/30000 Training Loss: 0.06808533519506454\n",
      "Epoch 2468/30000 Training Loss: 0.07152993977069855\n",
      "Epoch 2469/30000 Training Loss: 0.06851459294557571\n",
      "Epoch 2470/30000 Training Loss: 0.07053232192993164\n",
      "Epoch 2471/30000 Training Loss: 0.07212172448635101\n",
      "Epoch 2472/30000 Training Loss: 0.06921961903572083\n",
      "Epoch 2473/30000 Training Loss: 0.06163260340690613\n",
      "Epoch 2474/30000 Training Loss: 0.0516282320022583\n",
      "Epoch 2475/30000 Training Loss: 0.059224583208560944\n",
      "Epoch 2476/30000 Training Loss: 0.06539887934923172\n",
      "Epoch 2477/30000 Training Loss: 0.06694482266902924\n",
      "Epoch 2478/30000 Training Loss: 0.05716010928153992\n",
      "Epoch 2479/30000 Training Loss: 0.0670347809791565\n",
      "Epoch 2480/30000 Training Loss: 0.05549902468919754\n",
      "Epoch 2481/30000 Training Loss: 0.07080455124378204\n",
      "Epoch 2482/30000 Training Loss: 0.05253750830888748\n",
      "Epoch 2483/30000 Training Loss: 0.060889020562171936\n",
      "Epoch 2484/30000 Training Loss: 0.058353275060653687\n",
      "Epoch 2485/30000 Training Loss: 0.06880493462085724\n",
      "Epoch 2486/30000 Training Loss: 0.06783479452133179\n",
      "Epoch 2487/30000 Training Loss: 0.059376686811447144\n",
      "Epoch 2488/30000 Training Loss: 0.06760099530220032\n",
      "Epoch 2489/30000 Training Loss: 0.06125333905220032\n",
      "Epoch 2490/30000 Training Loss: 0.05552596598863602\n",
      "Epoch 2491/30000 Training Loss: 0.07002660632133484\n",
      "Epoch 2492/30000 Training Loss: 0.06497550755739212\n",
      "Epoch 2493/30000 Training Loss: 0.06449054181575775\n",
      "Epoch 2494/30000 Training Loss: 0.06472168117761612\n",
      "Epoch 2495/30000 Training Loss: 0.04991045594215393\n",
      "Epoch 2496/30000 Training Loss: 0.0690038800239563\n",
      "Epoch 2497/30000 Training Loss: 0.059238433837890625\n",
      "Epoch 2498/30000 Training Loss: 0.06045892834663391\n",
      "Epoch 2499/30000 Training Loss: 0.06182483583688736\n",
      "Epoch 2500/30000 Training Loss: 0.0735587626695633\n",
      "Epoch 2500/30000 Validation Loss: 0.0766894668340683\n",
      "Epoch 2501/30000 Training Loss: 0.07065597921609879\n",
      "Epoch 2502/30000 Training Loss: 0.06348647177219391\n",
      "Epoch 2503/30000 Training Loss: 0.071586474776268\n",
      "Epoch 2504/30000 Training Loss: 0.07011819630861282\n",
      "Epoch 2505/30000 Training Loss: 0.08140003681182861\n",
      "Epoch 2506/30000 Training Loss: 0.0655687153339386\n",
      "Epoch 2507/30000 Training Loss: 0.04783070087432861\n",
      "Epoch 2508/30000 Training Loss: 0.05967866629362106\n",
      "Epoch 2509/30000 Training Loss: 0.06208084896206856\n",
      "Epoch 2510/30000 Training Loss: 0.05538415163755417\n",
      "Epoch 2511/30000 Training Loss: 0.06766529381275177\n",
      "Epoch 2512/30000 Training Loss: 0.07438664138317108\n",
      "Epoch 2513/30000 Training Loss: 0.07460376620292664\n",
      "Epoch 2514/30000 Training Loss: 0.0730191022157669\n",
      "Epoch 2515/30000 Training Loss: 0.06194140017032623\n",
      "Epoch 2516/30000 Training Loss: 0.06185528263449669\n",
      "Epoch 2517/30000 Training Loss: 0.07273492217063904\n",
      "Epoch 2518/30000 Training Loss: 0.07922620326280594\n",
      "Epoch 2519/30000 Training Loss: 0.055459074676036835\n",
      "Epoch 2520/30000 Training Loss: 0.06021649390459061\n",
      "Epoch 2521/30000 Training Loss: 0.06440705806016922\n",
      "Epoch 2522/30000 Training Loss: 0.04362165555357933\n",
      "Epoch 2523/30000 Training Loss: 0.06875674426555634\n",
      "Epoch 2524/30000 Training Loss: 0.06874136626720428\n",
      "Epoch 2525/30000 Training Loss: 0.06647045910358429\n",
      "Epoch 2526/30000 Training Loss: 0.06393374502658844\n",
      "Epoch 2527/30000 Training Loss: 0.0774625763297081\n",
      "Epoch 2528/30000 Training Loss: 0.05516159534454346\n",
      "Epoch 2529/30000 Training Loss: 0.05085482448339462\n",
      "Epoch 2530/30000 Training Loss: 0.06306123733520508\n",
      "Epoch 2531/30000 Training Loss: 0.049791328608989716\n",
      "Epoch 2532/30000 Training Loss: 0.058098867535591125\n",
      "Epoch 2533/30000 Training Loss: 0.06517519801855087\n",
      "Epoch 2534/30000 Training Loss: 0.0788300633430481\n",
      "Epoch 2535/30000 Training Loss: 0.06482844799757004\n",
      "Epoch 2536/30000 Training Loss: 0.07158826291561127\n",
      "Epoch 2537/30000 Training Loss: 0.06516771763563156\n",
      "Epoch 2538/30000 Training Loss: 0.05927914381027222\n",
      "Epoch 2539/30000 Training Loss: 0.061735235154628754\n",
      "Epoch 2540/30000 Training Loss: 0.07217788696289062\n",
      "Epoch 2541/30000 Training Loss: 0.05942457914352417\n",
      "Epoch 2542/30000 Training Loss: 0.06640210747718811\n",
      "Epoch 2543/30000 Training Loss: 0.06295640766620636\n",
      "Epoch 2544/30000 Training Loss: 0.06998025625944138\n",
      "Epoch 2545/30000 Training Loss: 0.07514654099941254\n",
      "Epoch 2546/30000 Training Loss: 0.07108592987060547\n",
      "Epoch 2547/30000 Training Loss: 0.07629919052124023\n",
      "Epoch 2548/30000 Training Loss: 0.05996069312095642\n",
      "Epoch 2549/30000 Training Loss: 0.0755605697631836\n",
      "Epoch 2550/30000 Training Loss: 0.07655046880245209\n",
      "Epoch 2551/30000 Training Loss: 0.07446132600307465\n",
      "Epoch 2552/30000 Training Loss: 0.07004562020301819\n",
      "Epoch 2553/30000 Training Loss: 0.06131190061569214\n",
      "Epoch 2554/30000 Training Loss: 0.07725825905799866\n",
      "Epoch 2555/30000 Training Loss: 0.05587910860776901\n",
      "Epoch 2556/30000 Training Loss: 0.07541904598474503\n",
      "Epoch 2557/30000 Training Loss: 0.06508005410432816\n",
      "Epoch 2558/30000 Training Loss: 0.06322158873081207\n",
      "Epoch 2559/30000 Training Loss: 0.059694305062294006\n",
      "Epoch 2560/30000 Training Loss: 0.06191569194197655\n",
      "Epoch 2561/30000 Training Loss: 0.06461140513420105\n",
      "Epoch 2562/30000 Training Loss: 0.05638527125120163\n",
      "Epoch 2563/30000 Training Loss: 0.08100000023841858\n",
      "Epoch 2564/30000 Training Loss: 0.07190380990505219\n",
      "Epoch 2565/30000 Training Loss: 0.061504069715738297\n",
      "Epoch 2566/30000 Training Loss: 0.09103064984083176\n",
      "Epoch 2567/30000 Training Loss: 0.05164187029004097\n",
      "Epoch 2568/30000 Training Loss: 0.05937262624502182\n",
      "Epoch 2569/30000 Training Loss: 0.05019417032599449\n",
      "Epoch 2570/30000 Training Loss: 0.06290307641029358\n",
      "Epoch 2571/30000 Training Loss: 0.05381416529417038\n",
      "Epoch 2572/30000 Training Loss: 0.07243910431861877\n",
      "Epoch 2573/30000 Training Loss: 0.0570593997836113\n",
      "Epoch 2574/30000 Training Loss: 0.07041746377944946\n",
      "Epoch 2575/30000 Training Loss: 0.067314013838768\n",
      "Epoch 2576/30000 Training Loss: 0.07671518623828888\n",
      "Epoch 2577/30000 Training Loss: 0.05681302025914192\n",
      "Epoch 2578/30000 Training Loss: 0.061084892600774765\n",
      "Epoch 2579/30000 Training Loss: 0.07488454878330231\n",
      "Epoch 2580/30000 Training Loss: 0.062478117644786835\n",
      "Epoch 2581/30000 Training Loss: 0.07324347645044327\n",
      "Epoch 2582/30000 Training Loss: 0.05577770620584488\n",
      "Epoch 2583/30000 Training Loss: 0.0696493461728096\n",
      "Epoch 2584/30000 Training Loss: 0.05542493611574173\n",
      "Epoch 2585/30000 Training Loss: 0.06365258246660233\n",
      "Epoch 2586/30000 Training Loss: 0.06070604920387268\n",
      "Epoch 2587/30000 Training Loss: 0.06587135046720505\n",
      "Epoch 2588/30000 Training Loss: 0.06879763305187225\n",
      "Epoch 2589/30000 Training Loss: 0.057997286319732666\n",
      "Epoch 2590/30000 Training Loss: 0.06579672545194626\n",
      "Epoch 2591/30000 Training Loss: 0.04888182133436203\n",
      "Epoch 2592/30000 Training Loss: 0.07798507809638977\n",
      "Epoch 2593/30000 Training Loss: 0.06491853296756744\n",
      "Epoch 2594/30000 Training Loss: 0.06642144173383713\n",
      "Epoch 2595/30000 Training Loss: 0.04657798260450363\n",
      "Epoch 2596/30000 Training Loss: 0.07219575345516205\n",
      "Epoch 2597/30000 Training Loss: 0.059090644121170044\n",
      "Epoch 2598/30000 Training Loss: 0.06259828060865402\n",
      "Epoch 2599/30000 Training Loss: 0.059291549026966095\n",
      "Epoch 2600/30000 Training Loss: 0.08133834600448608\n",
      "Epoch 2600/30000 Validation Loss: 0.05582254379987717\n",
      "Epoch 2601/30000 Training Loss: 0.07250356674194336\n",
      "Epoch 2602/30000 Training Loss: 0.05342555418610573\n",
      "Epoch 2603/30000 Training Loss: 0.06860733032226562\n",
      "Epoch 2604/30000 Training Loss: 0.08041593432426453\n",
      "Epoch 2605/30000 Training Loss: 0.057086147367954254\n",
      "Epoch 2606/30000 Training Loss: 0.0650261715054512\n",
      "Epoch 2607/30000 Training Loss: 0.05801001563668251\n",
      "Epoch 2608/30000 Training Loss: 0.06202017515897751\n",
      "Epoch 2609/30000 Training Loss: 0.0587223656475544\n",
      "Epoch 2610/30000 Training Loss: 0.060121819376945496\n",
      "Epoch 2611/30000 Training Loss: 0.05125416815280914\n",
      "Epoch 2612/30000 Training Loss: 0.06798555701971054\n",
      "Epoch 2613/30000 Training Loss: 0.061655521392822266\n",
      "Epoch 2614/30000 Training Loss: 0.06747870147228241\n",
      "Epoch 2615/30000 Training Loss: 0.08711377531290054\n",
      "Epoch 2616/30000 Training Loss: 0.0708574503660202\n",
      "Epoch 2617/30000 Training Loss: 0.07161423563957214\n",
      "Epoch 2618/30000 Training Loss: 0.054146476089954376\n",
      "Epoch 2619/30000 Training Loss: 0.05509413033723831\n",
      "Epoch 2620/30000 Training Loss: 0.07666239142417908\n",
      "Epoch 2621/30000 Training Loss: 0.06917127966880798\n",
      "Epoch 2622/30000 Training Loss: 0.06915292143821716\n",
      "Epoch 2623/30000 Training Loss: 0.0530327744781971\n",
      "Epoch 2624/30000 Training Loss: 0.08227117359638214\n",
      "Epoch 2625/30000 Training Loss: 0.05255483090877533\n",
      "Epoch 2626/30000 Training Loss: 0.06698521226644516\n",
      "Epoch 2627/30000 Training Loss: 0.06225338578224182\n",
      "Epoch 2628/30000 Training Loss: 0.07992613315582275\n",
      "Epoch 2629/30000 Training Loss: 0.056629396975040436\n",
      "Epoch 2630/30000 Training Loss: 0.05815822631120682\n",
      "Epoch 2631/30000 Training Loss: 0.06288471817970276\n",
      "Epoch 2632/30000 Training Loss: 0.07133173197507858\n",
      "Epoch 2633/30000 Training Loss: 0.059319738298654556\n",
      "Epoch 2634/30000 Training Loss: 0.06207305192947388\n",
      "Epoch 2635/30000 Training Loss: 0.05102652311325073\n",
      "Epoch 2636/30000 Training Loss: 0.05547076463699341\n",
      "Epoch 2637/30000 Training Loss: 0.06968119740486145\n",
      "Epoch 2638/30000 Training Loss: 0.08858150243759155\n",
      "Epoch 2639/30000 Training Loss: 0.0684395432472229\n",
      "Epoch 2640/30000 Training Loss: 0.06133601441979408\n",
      "Epoch 2641/30000 Training Loss: 0.06550978124141693\n",
      "Epoch 2642/30000 Training Loss: 0.05139242485165596\n",
      "Epoch 2643/30000 Training Loss: 0.05507334694266319\n",
      "Epoch 2644/30000 Training Loss: 0.08192181587219238\n",
      "Epoch 2645/30000 Training Loss: 0.07451775670051575\n",
      "Epoch 2646/30000 Training Loss: 0.06499636918306351\n",
      "Epoch 2647/30000 Training Loss: 0.06664154678583145\n",
      "Epoch 2648/30000 Training Loss: 0.05386225879192352\n",
      "Epoch 2649/30000 Training Loss: 0.05983126163482666\n",
      "Epoch 2650/30000 Training Loss: 0.06381218135356903\n",
      "Epoch 2651/30000 Training Loss: 0.05498941242694855\n",
      "Epoch 2652/30000 Training Loss: 0.06287195533514023\n",
      "Epoch 2653/30000 Training Loss: 0.051450468599796295\n",
      "Epoch 2654/30000 Training Loss: 0.07802528142929077\n",
      "Epoch 2655/30000 Training Loss: 0.061622828245162964\n",
      "Epoch 2656/30000 Training Loss: 0.06316959112882614\n",
      "Epoch 2657/30000 Training Loss: 0.06322174519300461\n",
      "Epoch 2658/30000 Training Loss: 0.049341119825839996\n",
      "Epoch 2659/30000 Training Loss: 0.051049232482910156\n",
      "Epoch 2660/30000 Training Loss: 0.05762282758951187\n",
      "Epoch 2661/30000 Training Loss: 0.05443286895751953\n",
      "Epoch 2662/30000 Training Loss: 0.07337895035743713\n",
      "Epoch 2663/30000 Training Loss: 0.05626584589481354\n",
      "Epoch 2664/30000 Training Loss: 0.05600522458553314\n",
      "Epoch 2665/30000 Training Loss: 0.06330616772174835\n",
      "Epoch 2666/30000 Training Loss: 0.06404560804367065\n",
      "Epoch 2667/30000 Training Loss: 0.08388601988554001\n",
      "Epoch 2668/30000 Training Loss: 0.06540504097938538\n",
      "Epoch 2669/30000 Training Loss: 0.0575452521443367\n",
      "Epoch 2670/30000 Training Loss: 0.05955938249826431\n",
      "Epoch 2671/30000 Training Loss: 0.06407681852579117\n",
      "Epoch 2672/30000 Training Loss: 0.07587204873561859\n",
      "Epoch 2673/30000 Training Loss: 0.06207223981618881\n",
      "Epoch 2674/30000 Training Loss: 0.07286371290683746\n",
      "Epoch 2675/30000 Training Loss: 0.07066801190376282\n",
      "Epoch 2676/30000 Training Loss: 0.05342945456504822\n",
      "Epoch 2677/30000 Training Loss: 0.07639409601688385\n",
      "Epoch 2678/30000 Training Loss: 0.07249422371387482\n",
      "Epoch 2679/30000 Training Loss: 0.06728370487689972\n",
      "Epoch 2680/30000 Training Loss: 0.06319339573383331\n",
      "Epoch 2681/30000 Training Loss: 0.06350540369749069\n",
      "Epoch 2682/30000 Training Loss: 0.05630192905664444\n",
      "Epoch 2683/30000 Training Loss: 0.05538197606801987\n",
      "Epoch 2684/30000 Training Loss: 0.06303860247135162\n",
      "Epoch 2685/30000 Training Loss: 0.059336259961128235\n",
      "Epoch 2686/30000 Training Loss: 0.05482703819870949\n",
      "Epoch 2687/30000 Training Loss: 0.049306537955999374\n",
      "Epoch 2688/30000 Training Loss: 0.05881096422672272\n",
      "Epoch 2689/30000 Training Loss: 0.054258331656455994\n",
      "Epoch 2690/30000 Training Loss: 0.06536126881837845\n",
      "Epoch 2691/30000 Training Loss: 0.06846870481967926\n",
      "Epoch 2692/30000 Training Loss: 0.08359856903553009\n",
      "Epoch 2693/30000 Training Loss: 0.0639122724533081\n",
      "Epoch 2694/30000 Training Loss: 0.05719836801290512\n",
      "Epoch 2695/30000 Training Loss: 0.0701688677072525\n",
      "Epoch 2696/30000 Training Loss: 0.052786193788051605\n",
      "Epoch 2697/30000 Training Loss: 0.07555723190307617\n",
      "Epoch 2698/30000 Training Loss: 0.048430778086185455\n",
      "Epoch 2699/30000 Training Loss: 0.04676838219165802\n",
      "Epoch 2700/30000 Training Loss: 0.05659813433885574\n",
      "Epoch 2700/30000 Validation Loss: 0.06765931844711304\n",
      "Epoch 2701/30000 Training Loss: 0.05786498636007309\n",
      "Epoch 2702/30000 Training Loss: 0.052875958383083344\n",
      "Epoch 2703/30000 Training Loss: 0.08629631996154785\n",
      "Epoch 2704/30000 Training Loss: 0.05906124413013458\n",
      "Epoch 2705/30000 Training Loss: 0.07505669444799423\n",
      "Epoch 2706/30000 Training Loss: 0.07101880013942719\n",
      "Epoch 2707/30000 Training Loss: 0.07682564854621887\n",
      "Epoch 2708/30000 Training Loss: 0.07574877887964249\n",
      "Epoch 2709/30000 Training Loss: 0.06296996027231216\n",
      "Epoch 2710/30000 Training Loss: 0.07582728564739227\n",
      "Epoch 2711/30000 Training Loss: 0.06857036054134369\n",
      "Epoch 2712/30000 Training Loss: 0.0622282400727272\n",
      "Epoch 2713/30000 Training Loss: 0.07028032839298248\n",
      "Epoch 2714/30000 Training Loss: 0.0641779750585556\n",
      "Epoch 2715/30000 Training Loss: 0.050945185124874115\n",
      "Epoch 2716/30000 Training Loss: 0.08293407410383224\n",
      "Epoch 2717/30000 Training Loss: 0.06976218521595001\n",
      "Epoch 2718/30000 Training Loss: 0.06127282977104187\n",
      "Epoch 2719/30000 Training Loss: 0.07119301706552505\n",
      "Epoch 2720/30000 Training Loss: 0.05940522253513336\n",
      "Epoch 2721/30000 Training Loss: 0.05705948919057846\n",
      "Epoch 2722/30000 Training Loss: 0.05515991896390915\n",
      "Epoch 2723/30000 Training Loss: 0.05489136278629303\n",
      "Epoch 2724/30000 Training Loss: 0.05796871334314346\n",
      "Epoch 2725/30000 Training Loss: 0.057212017476558685\n",
      "Epoch 2726/30000 Training Loss: 0.06052302569150925\n",
      "Epoch 2727/30000 Training Loss: 0.07565657794475555\n",
      "Epoch 2728/30000 Training Loss: 0.04210105165839195\n",
      "Epoch 2729/30000 Training Loss: 0.05876348912715912\n",
      "Epoch 2730/30000 Training Loss: 0.06455451250076294\n",
      "Epoch 2731/30000 Training Loss: 0.05961592495441437\n",
      "Epoch 2732/30000 Training Loss: 0.049330875277519226\n",
      "Epoch 2733/30000 Training Loss: 0.05674780160188675\n",
      "Epoch 2734/30000 Training Loss: 0.0674658715724945\n",
      "Epoch 2735/30000 Training Loss: 0.06335312128067017\n",
      "Epoch 2736/30000 Training Loss: 0.05704629421234131\n",
      "Epoch 2737/30000 Training Loss: 0.05103427916765213\n",
      "Epoch 2738/30000 Training Loss: 0.05454016849398613\n",
      "Epoch 2739/30000 Training Loss: 0.073666512966156\n",
      "Epoch 2740/30000 Training Loss: 0.06087775155901909\n",
      "Epoch 2741/30000 Training Loss: 0.046681858599185944\n",
      "Epoch 2742/30000 Training Loss: 0.053478751331567764\n",
      "Epoch 2743/30000 Training Loss: 0.06681729853153229\n",
      "Epoch 2744/30000 Training Loss: 0.06739193201065063\n",
      "Epoch 2745/30000 Training Loss: 0.06817407160997391\n",
      "Epoch 2746/30000 Training Loss: 0.052845824509859085\n",
      "Epoch 2747/30000 Training Loss: 0.05558963119983673\n",
      "Epoch 2748/30000 Training Loss: 0.07917166501283646\n",
      "Epoch 2749/30000 Training Loss: 0.05808783322572708\n",
      "Epoch 2750/30000 Training Loss: 0.06855636835098267\n",
      "Epoch 2751/30000 Training Loss: 0.06915293633937836\n",
      "Epoch 2752/30000 Training Loss: 0.06166169419884682\n",
      "Epoch 2753/30000 Training Loss: 0.057338640093803406\n",
      "Epoch 2754/30000 Training Loss: 0.07597791403532028\n",
      "Epoch 2755/30000 Training Loss: 0.06691080331802368\n",
      "Epoch 2756/30000 Training Loss: 0.060267508029937744\n",
      "Epoch 2757/30000 Training Loss: 0.06497061252593994\n",
      "Epoch 2758/30000 Training Loss: 0.07183758169412613\n",
      "Epoch 2759/30000 Training Loss: 0.0627150759100914\n",
      "Epoch 2760/30000 Training Loss: 0.05310478061437607\n",
      "Epoch 2761/30000 Training Loss: 0.05243577063083649\n",
      "Epoch 2762/30000 Training Loss: 0.05916132777929306\n",
      "Epoch 2763/30000 Training Loss: 0.07334625720977783\n",
      "Epoch 2764/30000 Training Loss: 0.06206463277339935\n",
      "Epoch 2765/30000 Training Loss: 0.07191626727581024\n",
      "Epoch 2766/30000 Training Loss: 0.06449691951274872\n",
      "Epoch 2767/30000 Training Loss: 0.07281692326068878\n",
      "Epoch 2768/30000 Training Loss: 0.07167938351631165\n",
      "Epoch 2769/30000 Training Loss: 0.06589939445257187\n",
      "Epoch 2770/30000 Training Loss: 0.07339882105588913\n",
      "Epoch 2771/30000 Training Loss: 0.05597848445177078\n",
      "Epoch 2772/30000 Training Loss: 0.06928817182779312\n",
      "Epoch 2773/30000 Training Loss: 0.07741609215736389\n",
      "Epoch 2774/30000 Training Loss: 0.08258356153964996\n",
      "Epoch 2775/30000 Training Loss: 0.06700552999973297\n",
      "Epoch 2776/30000 Training Loss: 0.05791592597961426\n",
      "Epoch 2777/30000 Training Loss: 0.05933420732617378\n",
      "Epoch 2778/30000 Training Loss: 0.06648069620132446\n",
      "Epoch 2779/30000 Training Loss: 0.06508641690015793\n",
      "Epoch 2780/30000 Training Loss: 0.061213213950395584\n",
      "Epoch 2781/30000 Training Loss: 0.07354908436536789\n",
      "Epoch 2782/30000 Training Loss: 0.0725615918636322\n",
      "Epoch 2783/30000 Training Loss: 0.07778485119342804\n",
      "Epoch 2784/30000 Training Loss: 0.05897215008735657\n",
      "Epoch 2785/30000 Training Loss: 0.06021905690431595\n",
      "Epoch 2786/30000 Training Loss: 0.08082040399312973\n",
      "Epoch 2787/30000 Training Loss: 0.06434156745672226\n",
      "Epoch 2788/30000 Training Loss: 0.05960170924663544\n",
      "Epoch 2789/30000 Training Loss: 0.08319561928510666\n",
      "Epoch 2790/30000 Training Loss: 0.0809154063463211\n",
      "Epoch 2791/30000 Training Loss: 0.06790777295827866\n",
      "Epoch 2792/30000 Training Loss: 0.05957024544477463\n",
      "Epoch 2793/30000 Training Loss: 0.0837319865822792\n",
      "Epoch 2794/30000 Training Loss: 0.0722428411245346\n",
      "Epoch 2795/30000 Training Loss: 0.05641473829746246\n",
      "Epoch 2796/30000 Training Loss: 0.060327593237161636\n",
      "Epoch 2797/30000 Training Loss: 0.055341631174087524\n",
      "Epoch 2798/30000 Training Loss: 0.06890329718589783\n",
      "Epoch 2799/30000 Training Loss: 0.06092394143342972\n",
      "Epoch 2800/30000 Training Loss: 0.06045114994049072\n",
      "Epoch 2800/30000 Validation Loss: 0.08449884504079819\n",
      "Epoch 2801/30000 Training Loss: 0.048314981162548065\n",
      "Epoch 2802/30000 Training Loss: 0.06488531827926636\n",
      "Epoch 2803/30000 Training Loss: 0.06807179749011993\n",
      "Epoch 2804/30000 Training Loss: 0.06040588766336441\n",
      "Epoch 2805/30000 Training Loss: 0.06127776950597763\n",
      "Epoch 2806/30000 Training Loss: 0.07028356939554214\n",
      "Epoch 2807/30000 Training Loss: 0.08730439096689224\n",
      "Epoch 2808/30000 Training Loss: 0.06568369269371033\n",
      "Epoch 2809/30000 Training Loss: 0.06524646282196045\n",
      "Epoch 2810/30000 Training Loss: 0.05872039124369621\n",
      "Epoch 2811/30000 Training Loss: 0.07149506360292435\n",
      "Epoch 2812/30000 Training Loss: 0.05184130743145943\n",
      "Epoch 2813/30000 Training Loss: 0.06490833312273026\n",
      "Epoch 2814/30000 Training Loss: 0.05676758289337158\n",
      "Epoch 2815/30000 Training Loss: 0.06298524141311646\n",
      "Epoch 2816/30000 Training Loss: 0.06786321103572845\n",
      "Epoch 2817/30000 Training Loss: 0.07240663468837738\n",
      "Epoch 2818/30000 Training Loss: 0.057027846574783325\n",
      "Epoch 2819/30000 Training Loss: 0.06546061486005783\n",
      "Epoch 2820/30000 Training Loss: 0.0843244343996048\n",
      "Epoch 2821/30000 Training Loss: 0.0716308206319809\n",
      "Epoch 2822/30000 Training Loss: 0.056979190558195114\n",
      "Epoch 2823/30000 Training Loss: 0.0798865333199501\n",
      "Epoch 2824/30000 Training Loss: 0.05273878574371338\n",
      "Epoch 2825/30000 Training Loss: 0.061274539679288864\n",
      "Epoch 2826/30000 Training Loss: 0.06578722596168518\n",
      "Epoch 2827/30000 Training Loss: 0.05708690732717514\n",
      "Epoch 2828/30000 Training Loss: 0.07794564962387085\n",
      "Epoch 2829/30000 Training Loss: 0.057506658136844635\n",
      "Epoch 2830/30000 Training Loss: 0.0653461217880249\n",
      "Epoch 2831/30000 Training Loss: 0.0651809498667717\n",
      "Epoch 2832/30000 Training Loss: 0.0696517825126648\n",
      "Epoch 2833/30000 Training Loss: 0.06892126798629761\n",
      "Epoch 2834/30000 Training Loss: 0.05731165409088135\n",
      "Epoch 2835/30000 Training Loss: 0.06077725440263748\n",
      "Epoch 2836/30000 Training Loss: 0.05130303278565407\n",
      "Epoch 2837/30000 Training Loss: 0.06178143620491028\n",
      "Epoch 2838/30000 Training Loss: 0.06984216719865799\n",
      "Epoch 2839/30000 Training Loss: 0.06642226874828339\n",
      "Epoch 2840/30000 Training Loss: 0.07394199818372726\n",
      "Epoch 2841/30000 Training Loss: 0.07300539314746857\n",
      "Epoch 2842/30000 Training Loss: 0.05680245906114578\n",
      "Epoch 2843/30000 Training Loss: 0.07369376718997955\n",
      "Epoch 2844/30000 Training Loss: 0.0777963250875473\n",
      "Epoch 2845/30000 Training Loss: 0.06169315427541733\n",
      "Epoch 2846/30000 Training Loss: 0.07287098467350006\n",
      "Epoch 2847/30000 Training Loss: 0.05464797839522362\n",
      "Epoch 2848/30000 Training Loss: 0.045439012348651886\n",
      "Epoch 2849/30000 Training Loss: 0.06629978865385056\n",
      "Epoch 2850/30000 Training Loss: 0.054860398173332214\n",
      "Epoch 2851/30000 Training Loss: 0.05795419216156006\n",
      "Epoch 2852/30000 Training Loss: 0.06085311248898506\n",
      "Epoch 2853/30000 Training Loss: 0.07038282603025436\n",
      "Epoch 2854/30000 Training Loss: 0.05617727339267731\n",
      "Epoch 2855/30000 Training Loss: 0.06553986668586731\n",
      "Epoch 2856/30000 Training Loss: 0.06376596540212631\n",
      "Epoch 2857/30000 Training Loss: 0.07170421630144119\n",
      "Epoch 2858/30000 Training Loss: 0.05451781302690506\n",
      "Epoch 2859/30000 Training Loss: 0.0645056813955307\n",
      "Epoch 2860/30000 Training Loss: 0.06584041565656662\n",
      "Epoch 2861/30000 Training Loss: 0.09037359803915024\n",
      "Epoch 2862/30000 Training Loss: 0.05418195202946663\n",
      "Epoch 2863/30000 Training Loss: 0.08466055989265442\n",
      "Epoch 2864/30000 Training Loss: 0.05620471388101578\n",
      "Epoch 2865/30000 Training Loss: 0.07308751344680786\n",
      "Epoch 2866/30000 Training Loss: 0.0746021419763565\n",
      "Epoch 2867/30000 Training Loss: 0.0696987733244896\n",
      "Epoch 2868/30000 Training Loss: 0.07438904792070389\n",
      "Epoch 2869/30000 Training Loss: 0.05814100801944733\n",
      "Epoch 2870/30000 Training Loss: 0.05796101316809654\n",
      "Epoch 2871/30000 Training Loss: 0.06316692382097244\n",
      "Epoch 2872/30000 Training Loss: 0.059363048523664474\n",
      "Epoch 2873/30000 Training Loss: 0.06337309628725052\n",
      "Epoch 2874/30000 Training Loss: 0.056457869708538055\n",
      "Epoch 2875/30000 Training Loss: 0.05278102308511734\n",
      "Epoch 2876/30000 Training Loss: 0.060994476079940796\n",
      "Epoch 2877/30000 Training Loss: 0.05813441425561905\n",
      "Epoch 2878/30000 Training Loss: 0.07204151153564453\n",
      "Epoch 2879/30000 Training Loss: 0.05819860100746155\n",
      "Epoch 2880/30000 Training Loss: 0.06300124526023865\n",
      "Epoch 2881/30000 Training Loss: 0.06310215592384338\n",
      "Epoch 2882/30000 Training Loss: 0.0626872107386589\n",
      "Epoch 2883/30000 Training Loss: 0.06987257301807404\n",
      "Epoch 2884/30000 Training Loss: 0.07260197401046753\n",
      "Epoch 2885/30000 Training Loss: 0.06431453675031662\n",
      "Epoch 2886/30000 Training Loss: 0.055450715124607086\n",
      "Epoch 2887/30000 Training Loss: 0.07150791585445404\n",
      "Epoch 2888/30000 Training Loss: 0.06303952634334564\n",
      "Epoch 2889/30000 Training Loss: 0.07745301723480225\n",
      "Epoch 2890/30000 Training Loss: 0.04858066886663437\n",
      "Epoch 2891/30000 Training Loss: 0.049968965351581573\n",
      "Epoch 2892/30000 Training Loss: 0.06873993575572968\n",
      "Epoch 2893/30000 Training Loss: 0.04618430882692337\n",
      "Epoch 2894/30000 Training Loss: 0.05438274145126343\n",
      "Epoch 2895/30000 Training Loss: 0.08071114122867584\n",
      "Epoch 2896/30000 Training Loss: 0.062392428517341614\n",
      "Epoch 2897/30000 Training Loss: 0.07606307417154312\n",
      "Epoch 2898/30000 Training Loss: 0.07793773710727692\n",
      "Epoch 2899/30000 Training Loss: 0.058023419231176376\n",
      "Epoch 2900/30000 Training Loss: 0.07497988641262054\n",
      "Epoch 2900/30000 Validation Loss: 0.05767647176980972\n",
      "Epoch 2901/30000 Training Loss: 0.0644889771938324\n",
      "Epoch 2902/30000 Training Loss: 0.05532050132751465\n",
      "Epoch 2903/30000 Training Loss: 0.0717490166425705\n",
      "Epoch 2904/30000 Training Loss: 0.05804325267672539\n",
      "Epoch 2905/30000 Training Loss: 0.07281216979026794\n",
      "Epoch 2906/30000 Training Loss: 0.06326726824045181\n",
      "Epoch 2907/30000 Training Loss: 0.06478995084762573\n",
      "Epoch 2908/30000 Training Loss: 0.05130897834897041\n",
      "Epoch 2909/30000 Training Loss: 0.05612263083457947\n",
      "Epoch 2910/30000 Training Loss: 0.05465797334909439\n",
      "Epoch 2911/30000 Training Loss: 0.057785697281360626\n",
      "Epoch 2912/30000 Training Loss: 0.05819921940565109\n",
      "Epoch 2913/30000 Training Loss: 0.07564263790845871\n",
      "Epoch 2914/30000 Training Loss: 0.06118278205394745\n",
      "Epoch 2915/30000 Training Loss: 0.07440461218357086\n",
      "Epoch 2916/30000 Training Loss: 0.07592017948627472\n",
      "Epoch 2917/30000 Training Loss: 0.04759693890810013\n",
      "Epoch 2918/30000 Training Loss: 0.07656224071979523\n",
      "Epoch 2919/30000 Training Loss: 0.06310799717903137\n",
      "Epoch 2920/30000 Training Loss: 0.0632518082857132\n",
      "Epoch 2921/30000 Training Loss: 0.06004653871059418\n",
      "Epoch 2922/30000 Training Loss: 0.06102684140205383\n",
      "Epoch 2923/30000 Training Loss: 0.05362578108906746\n",
      "Epoch 2924/30000 Training Loss: 0.0683891773223877\n",
      "Epoch 2925/30000 Training Loss: 0.08078226447105408\n",
      "Epoch 2926/30000 Training Loss: 0.06067432463169098\n",
      "Epoch 2927/30000 Training Loss: 0.06246483325958252\n",
      "Epoch 2928/30000 Training Loss: 0.07293686270713806\n",
      "Epoch 2929/30000 Training Loss: 0.07091866433620453\n",
      "Epoch 2930/30000 Training Loss: 0.04552989453077316\n",
      "Epoch 2931/30000 Training Loss: 0.05626695975661278\n",
      "Epoch 2932/30000 Training Loss: 0.06728163361549377\n",
      "Epoch 2933/30000 Training Loss: 0.05939041078090668\n",
      "Epoch 2934/30000 Training Loss: 0.06441523134708405\n",
      "Epoch 2935/30000 Training Loss: 0.04802462458610535\n",
      "Epoch 2936/30000 Training Loss: 0.06526144593954086\n",
      "Epoch 2937/30000 Training Loss: 0.058786895126104355\n",
      "Epoch 2938/30000 Training Loss: 0.06066364422440529\n",
      "Epoch 2939/30000 Training Loss: 0.045862067490816116\n",
      "Epoch 2940/30000 Training Loss: 0.06062587350606918\n",
      "Epoch 2941/30000 Training Loss: 0.07353173196315765\n",
      "Epoch 2942/30000 Training Loss: 0.06156172603368759\n",
      "Epoch 2943/30000 Training Loss: 0.05572574958205223\n",
      "Epoch 2944/30000 Training Loss: 0.06159120798110962\n",
      "Epoch 2945/30000 Training Loss: 0.0659603625535965\n",
      "Epoch 2946/30000 Training Loss: 0.06087927147746086\n",
      "Epoch 2947/30000 Training Loss: 0.07686753571033478\n",
      "Epoch 2948/30000 Training Loss: 0.06981375813484192\n",
      "Epoch 2949/30000 Training Loss: 0.06804157793521881\n",
      "Epoch 2950/30000 Training Loss: 0.06264583766460419\n",
      "Epoch 2951/30000 Training Loss: 0.05048852413892746\n",
      "Epoch 2952/30000 Training Loss: 0.05966562032699585\n",
      "Epoch 2953/30000 Training Loss: 0.06061236560344696\n",
      "Epoch 2954/30000 Training Loss: 0.06968161463737488\n",
      "Epoch 2955/30000 Training Loss: 0.071858711540699\n",
      "Epoch 2956/30000 Training Loss: 0.05894269049167633\n",
      "Epoch 2957/30000 Training Loss: 0.06622542440891266\n",
      "Epoch 2958/30000 Training Loss: 0.06373558938503265\n",
      "Epoch 2959/30000 Training Loss: 0.05259668827056885\n",
      "Epoch 2960/30000 Training Loss: 0.0686485692858696\n",
      "Epoch 2961/30000 Training Loss: 0.04655119776725769\n",
      "Epoch 2962/30000 Training Loss: 0.05642381310462952\n",
      "Epoch 2963/30000 Training Loss: 0.06055298447608948\n",
      "Epoch 2964/30000 Training Loss: 0.05436119809746742\n",
      "Epoch 2965/30000 Training Loss: 0.05701294541358948\n",
      "Epoch 2966/30000 Training Loss: 0.06601695716381073\n",
      "Epoch 2967/30000 Training Loss: 0.04949251189827919\n",
      "Epoch 2968/30000 Training Loss: 0.05943521112203598\n",
      "Epoch 2969/30000 Training Loss: 0.058977313339710236\n",
      "Epoch 2970/30000 Training Loss: 0.058135684579610825\n",
      "Epoch 2971/30000 Training Loss: 0.05907753109931946\n",
      "Epoch 2972/30000 Training Loss: 0.06270735710859299\n",
      "Epoch 2973/30000 Training Loss: 0.04940169304609299\n",
      "Epoch 2974/30000 Training Loss: 0.05354594439268112\n",
      "Epoch 2975/30000 Training Loss: 0.061567023396492004\n",
      "Epoch 2976/30000 Training Loss: 0.06527083367109299\n",
      "Epoch 2977/30000 Training Loss: 0.047807253897190094\n",
      "Epoch 2978/30000 Training Loss: 0.047544196248054504\n",
      "Epoch 2979/30000 Training Loss: 0.08342099189758301\n",
      "Epoch 2980/30000 Training Loss: 0.056964460760354996\n",
      "Epoch 2981/30000 Training Loss: 0.0687299519777298\n",
      "Epoch 2982/30000 Training Loss: 0.07614083588123322\n",
      "Epoch 2983/30000 Training Loss: 0.05890480428934097\n",
      "Epoch 2984/30000 Training Loss: 0.04540135711431503\n",
      "Epoch 2985/30000 Training Loss: 0.06694246083498001\n",
      "Epoch 2986/30000 Training Loss: 0.06249692291021347\n",
      "Epoch 2987/30000 Training Loss: 0.05661030486226082\n",
      "Epoch 2988/30000 Training Loss: 0.06604155898094177\n",
      "Epoch 2989/30000 Training Loss: 0.05533596873283386\n",
      "Epoch 2990/30000 Training Loss: 0.05854833871126175\n",
      "Epoch 2991/30000 Training Loss: 0.04755966365337372\n",
      "Epoch 2992/30000 Training Loss: 0.07732069492340088\n",
      "Epoch 2993/30000 Training Loss: 0.06952007859945297\n",
      "Epoch 2994/30000 Training Loss: 0.06327256560325623\n",
      "Epoch 2995/30000 Training Loss: 0.05164910852909088\n",
      "Epoch 2996/30000 Training Loss: 0.05987969785928726\n",
      "Epoch 2997/30000 Training Loss: 0.0550970695912838\n",
      "Epoch 2998/30000 Training Loss: 0.05352894961833954\n",
      "Epoch 2999/30000 Training Loss: 0.05326846241950989\n",
      "Epoch 3000/30000 Training Loss: 0.05869439244270325\n",
      "Epoch 3000/30000 Validation Loss: 0.07195663452148438\n",
      "Epoch 3001/30000 Training Loss: 0.05873965471982956\n",
      "Epoch 3002/30000 Training Loss: 0.0666106790304184\n",
      "Epoch 3003/30000 Training Loss: 0.054874174296855927\n",
      "Epoch 3004/30000 Training Loss: 0.0738866925239563\n",
      "Epoch 3005/30000 Training Loss: 0.06723594665527344\n",
      "Epoch 3006/30000 Training Loss: 0.060299161821603775\n",
      "Epoch 3007/30000 Training Loss: 0.05908661335706711\n",
      "Epoch 3008/30000 Training Loss: 0.06383968889713287\n",
      "Epoch 3009/30000 Training Loss: 0.06263341754674911\n",
      "Epoch 3010/30000 Training Loss: 0.059573695063591\n",
      "Epoch 3011/30000 Training Loss: 0.06345588713884354\n",
      "Epoch 3012/30000 Training Loss: 0.05853872001171112\n",
      "Epoch 3013/30000 Training Loss: 0.06119510531425476\n",
      "Epoch 3014/30000 Training Loss: 0.0682486742734909\n",
      "Epoch 3015/30000 Training Loss: 0.07874292135238647\n",
      "Epoch 3016/30000 Training Loss: 0.05350033938884735\n",
      "Epoch 3017/30000 Training Loss: 0.0549047626554966\n",
      "Epoch 3018/30000 Training Loss: 0.053902000188827515\n",
      "Epoch 3019/30000 Training Loss: 0.054461147636175156\n",
      "Epoch 3020/30000 Training Loss: 0.05632748082280159\n",
      "Epoch 3021/30000 Training Loss: 0.049500443041324615\n",
      "Epoch 3022/30000 Training Loss: 0.05308310687541962\n",
      "Epoch 3023/30000 Training Loss: 0.06055314093828201\n",
      "Epoch 3024/30000 Training Loss: 0.07407490909099579\n",
      "Epoch 3025/30000 Training Loss: 0.05821467936038971\n",
      "Epoch 3026/30000 Training Loss: 0.07116779685020447\n",
      "Epoch 3027/30000 Training Loss: 0.0667797401547432\n",
      "Epoch 3028/30000 Training Loss: 0.0508822537958622\n",
      "Epoch 3029/30000 Training Loss: 0.05177650600671768\n",
      "Epoch 3030/30000 Training Loss: 0.05244386941194534\n",
      "Epoch 3031/30000 Training Loss: 0.06617933511734009\n",
      "Epoch 3032/30000 Training Loss: 0.07307498157024384\n",
      "Epoch 3033/30000 Training Loss: 0.051836512982845306\n",
      "Epoch 3034/30000 Training Loss: 0.0744609460234642\n",
      "Epoch 3035/30000 Training Loss: 0.05475415289402008\n",
      "Epoch 3036/30000 Training Loss: 0.06377458572387695\n",
      "Epoch 3037/30000 Training Loss: 0.06366387754678726\n",
      "Epoch 3038/30000 Training Loss: 0.07686290889978409\n",
      "Epoch 3039/30000 Training Loss: 0.07050475478172302\n",
      "Epoch 3040/30000 Training Loss: 0.06822052597999573\n",
      "Epoch 3041/30000 Training Loss: 0.05302722007036209\n",
      "Epoch 3042/30000 Training Loss: 0.06376825273036957\n",
      "Epoch 3043/30000 Training Loss: 0.061333008110523224\n",
      "Epoch 3044/30000 Training Loss: 0.0765213593840599\n",
      "Epoch 3045/30000 Training Loss: 0.05531780421733856\n",
      "Epoch 3046/30000 Training Loss: 0.06378576904535294\n",
      "Epoch 3047/30000 Training Loss: 0.05801698565483093\n",
      "Epoch 3048/30000 Training Loss: 0.07358838617801666\n",
      "Epoch 3049/30000 Training Loss: 0.06321272253990173\n",
      "Epoch 3050/30000 Training Loss: 0.06639283895492554\n",
      "Epoch 3051/30000 Training Loss: 0.06732165068387985\n",
      "Epoch 3052/30000 Training Loss: 0.04902459308505058\n",
      "Epoch 3053/30000 Training Loss: 0.06135590374469757\n",
      "Epoch 3054/30000 Training Loss: 0.07013644278049469\n",
      "Epoch 3055/30000 Training Loss: 0.06415429711341858\n",
      "Epoch 3056/30000 Training Loss: 0.054212022572755814\n",
      "Epoch 3057/30000 Training Loss: 0.061519451439380646\n",
      "Epoch 3058/30000 Training Loss: 0.059098631143569946\n",
      "Epoch 3059/30000 Training Loss: 0.07889765501022339\n",
      "Epoch 3060/30000 Training Loss: 0.07587703317403793\n",
      "Epoch 3061/30000 Training Loss: 0.0488477498292923\n",
      "Epoch 3062/30000 Training Loss: 0.061935797333717346\n",
      "Epoch 3063/30000 Training Loss: 0.06480386108160019\n",
      "Epoch 3064/30000 Training Loss: 0.06890869140625\n",
      "Epoch 3065/30000 Training Loss: 0.052905917167663574\n",
      "Epoch 3066/30000 Training Loss: 0.04644156992435455\n",
      "Epoch 3067/30000 Training Loss: 0.06393802165985107\n",
      "Epoch 3068/30000 Training Loss: 0.06056338921189308\n",
      "Epoch 3069/30000 Training Loss: 0.060430288314819336\n",
      "Epoch 3070/30000 Training Loss: 0.052526116371154785\n",
      "Epoch 3071/30000 Training Loss: 0.07262855023145676\n",
      "Epoch 3072/30000 Training Loss: 0.07064607739448547\n",
      "Epoch 3073/30000 Training Loss: 0.055471256375312805\n",
      "Epoch 3074/30000 Training Loss: 0.06919129192829132\n",
      "Epoch 3075/30000 Training Loss: 0.06066317856311798\n",
      "Epoch 3076/30000 Training Loss: 0.05832754820585251\n",
      "Epoch 3077/30000 Training Loss: 0.059743814170360565\n",
      "Epoch 3078/30000 Training Loss: 0.0785694420337677\n",
      "Epoch 3079/30000 Training Loss: 0.052341870963573456\n",
      "Epoch 3080/30000 Training Loss: 0.07440638542175293\n",
      "Epoch 3081/30000 Training Loss: 0.06459091603755951\n",
      "Epoch 3082/30000 Training Loss: 0.04941326379776001\n",
      "Epoch 3083/30000 Training Loss: 0.057138193398714066\n",
      "Epoch 3084/30000 Training Loss: 0.05351906269788742\n",
      "Epoch 3085/30000 Training Loss: 0.06208888441324234\n",
      "Epoch 3086/30000 Training Loss: 0.06807060539722443\n",
      "Epoch 3087/30000 Training Loss: 0.0630316510796547\n",
      "Epoch 3088/30000 Training Loss: 0.052542489022016525\n",
      "Epoch 3089/30000 Training Loss: 0.06885924190282822\n",
      "Epoch 3090/30000 Training Loss: 0.06460137665271759\n",
      "Epoch 3091/30000 Training Loss: 0.05618281662464142\n",
      "Epoch 3092/30000 Training Loss: 0.05976228415966034\n",
      "Epoch 3093/30000 Training Loss: 0.048985958099365234\n",
      "Epoch 3094/30000 Training Loss: 0.05833013355731964\n",
      "Epoch 3095/30000 Training Loss: 0.06509389728307724\n",
      "Epoch 3096/30000 Training Loss: 0.0461001992225647\n",
      "Epoch 3097/30000 Training Loss: 0.06106070056557655\n",
      "Epoch 3098/30000 Training Loss: 0.07201838493347168\n",
      "Epoch 3099/30000 Training Loss: 0.06511467695236206\n",
      "Epoch 3100/30000 Training Loss: 0.0545801967382431\n",
      "Epoch 3100/30000 Validation Loss: 0.0463113933801651\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0463113933801651<=============\n",
      "Epoch 3101/30000 Training Loss: 0.06224770098924637\n",
      "Epoch 3102/30000 Training Loss: 0.06273086369037628\n",
      "Epoch 3103/30000 Training Loss: 0.05502937361598015\n",
      "Epoch 3104/30000 Training Loss: 0.06043922156095505\n",
      "Epoch 3105/30000 Training Loss: 0.062294088304042816\n",
      "Epoch 3106/30000 Training Loss: 0.05507214739918709\n",
      "Epoch 3107/30000 Training Loss: 0.05457247048616409\n",
      "Epoch 3108/30000 Training Loss: 0.0632801279425621\n",
      "Epoch 3109/30000 Training Loss: 0.0569237619638443\n",
      "Epoch 3110/30000 Training Loss: 0.058688730001449585\n",
      "Epoch 3111/30000 Training Loss: 0.0749453529715538\n",
      "Epoch 3112/30000 Training Loss: 0.05647507309913635\n",
      "Epoch 3113/30000 Training Loss: 0.06308921426534653\n",
      "Epoch 3114/30000 Training Loss: 0.07708767056465149\n",
      "Epoch 3115/30000 Training Loss: 0.06817521154880524\n",
      "Epoch 3116/30000 Training Loss: 0.0599319152534008\n",
      "Epoch 3117/30000 Training Loss: 0.061815112829208374\n",
      "Epoch 3118/30000 Training Loss: 0.06253261119127274\n",
      "Epoch 3119/30000 Training Loss: 0.05560803413391113\n",
      "Epoch 3120/30000 Training Loss: 0.05883210152387619\n",
      "Epoch 3121/30000 Training Loss: 0.07609014213085175\n",
      "Epoch 3122/30000 Training Loss: 0.06688101589679718\n",
      "Epoch 3123/30000 Training Loss: 0.050575338304042816\n",
      "Epoch 3124/30000 Training Loss: 0.07769308984279633\n",
      "Epoch 3125/30000 Training Loss: 0.06574539840221405\n",
      "Epoch 3126/30000 Training Loss: 0.05109292268753052\n",
      "Epoch 3127/30000 Training Loss: 0.07248938828706741\n",
      "Epoch 3128/30000 Training Loss: 0.05828384682536125\n",
      "Epoch 3129/30000 Training Loss: 0.06739433109760284\n",
      "Epoch 3130/30000 Training Loss: 0.05976805090904236\n",
      "Epoch 3131/30000 Training Loss: 0.05697783827781677\n",
      "Epoch 3132/30000 Training Loss: 0.05131953954696655\n",
      "Epoch 3133/30000 Training Loss: 0.05769279971718788\n",
      "Epoch 3134/30000 Training Loss: 0.04428015649318695\n",
      "Epoch 3135/30000 Training Loss: 0.059807293117046356\n",
      "Epoch 3136/30000 Training Loss: 0.058279626071453094\n",
      "Epoch 3137/30000 Training Loss: 0.07299911975860596\n",
      "Epoch 3138/30000 Training Loss: 0.07629320025444031\n",
      "Epoch 3139/30000 Training Loss: 0.05869736522436142\n",
      "Epoch 3140/30000 Training Loss: 0.05712113156914711\n",
      "Epoch 3141/30000 Training Loss: 0.050571367144584656\n",
      "Epoch 3142/30000 Training Loss: 0.056915003806352615\n",
      "Epoch 3143/30000 Training Loss: 0.06833516061306\n",
      "Epoch 3144/30000 Training Loss: 0.0549490749835968\n",
      "Epoch 3145/30000 Training Loss: 0.056580737233161926\n",
      "Epoch 3146/30000 Training Loss: 0.08089852333068848\n",
      "Epoch 3147/30000 Training Loss: 0.05279184505343437\n",
      "Epoch 3148/30000 Training Loss: 0.07303126156330109\n",
      "Epoch 3149/30000 Training Loss: 0.05565003305673599\n",
      "Epoch 3150/30000 Training Loss: 0.06718664616346359\n",
      "Epoch 3151/30000 Training Loss: 0.07549789547920227\n",
      "Epoch 3152/30000 Training Loss: 0.04553021490573883\n",
      "Epoch 3153/30000 Training Loss: 0.048755303025245667\n",
      "Epoch 3154/30000 Training Loss: 0.06011858955025673\n",
      "Epoch 3155/30000 Training Loss: 0.07201363891363144\n",
      "Epoch 3156/30000 Training Loss: 0.0890665054321289\n",
      "Epoch 3157/30000 Training Loss: 0.06121883541345596\n",
      "Epoch 3158/30000 Training Loss: 0.07035752385854721\n",
      "Epoch 3159/30000 Training Loss: 0.06942413002252579\n",
      "Epoch 3160/30000 Training Loss: 0.06378567218780518\n",
      "Epoch 3161/30000 Training Loss: 0.06286762654781342\n",
      "Epoch 3162/30000 Training Loss: 0.06261041760444641\n",
      "Epoch 3163/30000 Training Loss: 0.060992173850536346\n",
      "Epoch 3164/30000 Training Loss: 0.057930875569581985\n",
      "Epoch 3165/30000 Training Loss: 0.06539750844240189\n",
      "Epoch 3166/30000 Training Loss: 0.06668417155742645\n",
      "Epoch 3167/30000 Training Loss: 0.04846499115228653\n",
      "Epoch 3168/30000 Training Loss: 0.059689659625291824\n",
      "Epoch 3169/30000 Training Loss: 0.06312324851751328\n",
      "Epoch 3170/30000 Training Loss: 0.053927231580019\n",
      "Epoch 3171/30000 Training Loss: 0.055231038480997086\n",
      "Epoch 3172/30000 Training Loss: 0.0673113688826561\n",
      "Epoch 3173/30000 Training Loss: 0.07383042573928833\n",
      "Epoch 3174/30000 Training Loss: 0.06650819629430771\n",
      "Epoch 3175/30000 Training Loss: 0.06024335324764252\n",
      "Epoch 3176/30000 Training Loss: 0.069094717502594\n",
      "Epoch 3177/30000 Training Loss: 0.06348167359828949\n",
      "Epoch 3178/30000 Training Loss: 0.06409894675016403\n",
      "Epoch 3179/30000 Training Loss: 0.07749482989311218\n",
      "Epoch 3180/30000 Training Loss: 0.06714425981044769\n",
      "Epoch 3181/30000 Training Loss: 0.06357845664024353\n",
      "Epoch 3182/30000 Training Loss: 0.0624164417386055\n",
      "Epoch 3183/30000 Training Loss: 0.051285259425640106\n",
      "Epoch 3184/30000 Training Loss: 0.056425683200359344\n",
      "Epoch 3185/30000 Training Loss: 0.06574143469333649\n",
      "Epoch 3186/30000 Training Loss: 0.0647505596280098\n",
      "Epoch 3187/30000 Training Loss: 0.051064956933259964\n",
      "Epoch 3188/30000 Training Loss: 0.059317655861377716\n",
      "Epoch 3189/30000 Training Loss: 0.07572078704833984\n",
      "Epoch 3190/30000 Training Loss: 0.07038634270429611\n",
      "Epoch 3191/30000 Training Loss: 0.06203985959291458\n",
      "Epoch 3192/30000 Training Loss: 0.05457816272974014\n",
      "Epoch 3193/30000 Training Loss: 0.06594152003526688\n",
      "Epoch 3194/30000 Training Loss: 0.055719781666994095\n",
      "Epoch 3195/30000 Training Loss: 0.06315204501152039\n",
      "Epoch 3196/30000 Training Loss: 0.06664961576461792\n",
      "Epoch 3197/30000 Training Loss: 0.06005198508501053\n",
      "Epoch 3198/30000 Training Loss: 0.06646326929330826\n",
      "Epoch 3199/30000 Training Loss: 0.06142722815275192\n",
      "Epoch 3200/30000 Training Loss: 0.06514836847782135\n",
      "Epoch 3200/30000 Validation Loss: 0.059350524097681046\n",
      "Epoch 3201/30000 Training Loss: 0.05512436479330063\n",
      "Epoch 3202/30000 Training Loss: 0.06033730506896973\n",
      "Epoch 3203/30000 Training Loss: 0.059393201023340225\n",
      "Epoch 3204/30000 Training Loss: 0.05465016886591911\n",
      "Epoch 3205/30000 Training Loss: 0.07036729156970978\n",
      "Epoch 3206/30000 Training Loss: 0.0511070154607296\n",
      "Epoch 3207/30000 Training Loss: 0.08478027582168579\n",
      "Epoch 3208/30000 Training Loss: 0.06064664572477341\n",
      "Epoch 3209/30000 Training Loss: 0.049140915274620056\n",
      "Epoch 3210/30000 Training Loss: 0.06614384055137634\n",
      "Epoch 3211/30000 Training Loss: 0.06622333079576492\n",
      "Epoch 3212/30000 Training Loss: 0.07897447049617767\n",
      "Epoch 3213/30000 Training Loss: 0.07189644128084183\n",
      "Epoch 3214/30000 Training Loss: 0.06816361099481583\n",
      "Epoch 3215/30000 Training Loss: 0.04916565865278244\n",
      "Epoch 3216/30000 Training Loss: 0.061754051595926285\n",
      "Epoch 3217/30000 Training Loss: 0.05557267367839813\n",
      "Epoch 3218/30000 Training Loss: 0.06410357356071472\n",
      "Epoch 3219/30000 Training Loss: 0.06003042310476303\n",
      "Epoch 3220/30000 Training Loss: 0.06427716463804245\n",
      "Epoch 3221/30000 Training Loss: 0.05980416387319565\n",
      "Epoch 3222/30000 Training Loss: 0.05888833850622177\n",
      "Epoch 3223/30000 Training Loss: 0.05565822869539261\n",
      "Epoch 3224/30000 Training Loss: 0.07136419415473938\n",
      "Epoch 3225/30000 Training Loss: 0.06900361180305481\n",
      "Epoch 3226/30000 Training Loss: 0.06151759624481201\n",
      "Epoch 3227/30000 Training Loss: 0.0651269406080246\n",
      "Epoch 3228/30000 Training Loss: 0.04637372866272926\n",
      "Epoch 3229/30000 Training Loss: 0.05791466683149338\n",
      "Epoch 3230/30000 Training Loss: 0.052655965089797974\n",
      "Epoch 3231/30000 Training Loss: 0.06084176152944565\n",
      "Epoch 3232/30000 Training Loss: 0.06002803146839142\n",
      "Epoch 3233/30000 Training Loss: 0.06734132766723633\n",
      "Epoch 3234/30000 Training Loss: 0.06258798390626907\n",
      "Epoch 3235/30000 Training Loss: 0.053862474858760834\n",
      "Epoch 3236/30000 Training Loss: 0.06363377720117569\n",
      "Epoch 3237/30000 Training Loss: 0.0654083713889122\n",
      "Epoch 3238/30000 Training Loss: 0.07394059002399445\n",
      "Epoch 3239/30000 Training Loss: 0.05952942371368408\n",
      "Epoch 3240/30000 Training Loss: 0.05447446554899216\n",
      "Epoch 3241/30000 Training Loss: 0.060943230986595154\n",
      "Epoch 3242/30000 Training Loss: 0.0668993666768074\n",
      "Epoch 3243/30000 Training Loss: 0.07418611645698547\n",
      "Epoch 3244/30000 Training Loss: 0.06464774906635284\n",
      "Epoch 3245/30000 Training Loss: 0.051650017499923706\n",
      "Epoch 3246/30000 Training Loss: 0.06503672897815704\n",
      "Epoch 3247/30000 Training Loss: 0.0844983458518982\n",
      "Epoch 3248/30000 Training Loss: 0.04506856948137283\n",
      "Epoch 3249/30000 Training Loss: 0.07514491677284241\n",
      "Epoch 3250/30000 Training Loss: 0.06793390959501266\n",
      "Epoch 3251/30000 Training Loss: 0.06720270216464996\n",
      "Epoch 3252/30000 Training Loss: 0.044819705188274384\n",
      "Epoch 3253/30000 Training Loss: 0.07084450125694275\n",
      "Epoch 3254/30000 Training Loss: 0.06721612066030502\n",
      "Epoch 3255/30000 Training Loss: 0.05928702652454376\n",
      "Epoch 3256/30000 Training Loss: 0.05988607555627823\n",
      "Epoch 3257/30000 Training Loss: 0.052968233823776245\n",
      "Epoch 3258/30000 Training Loss: 0.058760471642017365\n",
      "Epoch 3259/30000 Training Loss: 0.05647967755794525\n",
      "Epoch 3260/30000 Training Loss: 0.06225666403770447\n",
      "Epoch 3261/30000 Training Loss: 0.07742363214492798\n",
      "Epoch 3262/30000 Training Loss: 0.07125376164913177\n",
      "Epoch 3263/30000 Training Loss: 0.063617043197155\n",
      "Epoch 3264/30000 Training Loss: 0.0693872943520546\n",
      "Epoch 3265/30000 Training Loss: 0.06147325783967972\n",
      "Epoch 3266/30000 Training Loss: 0.07271607220172882\n",
      "Epoch 3267/30000 Training Loss: 0.05656972900032997\n",
      "Epoch 3268/30000 Training Loss: 0.061636023223400116\n",
      "Epoch 3269/30000 Training Loss: 0.0629168301820755\n",
      "Epoch 3270/30000 Training Loss: 0.0495142824947834\n",
      "Epoch 3271/30000 Training Loss: 0.05206530913710594\n",
      "Epoch 3272/30000 Training Loss: 0.0721169263124466\n",
      "Epoch 3273/30000 Training Loss: 0.07102873921394348\n",
      "Epoch 3274/30000 Training Loss: 0.0579405277967453\n",
      "Epoch 3275/30000 Training Loss: 0.060239024460315704\n",
      "Epoch 3276/30000 Training Loss: 0.06704457849264145\n",
      "Epoch 3277/30000 Training Loss: 0.05960514396429062\n",
      "Epoch 3278/30000 Training Loss: 0.0540122464299202\n",
      "Epoch 3279/30000 Training Loss: 0.0683533102273941\n",
      "Epoch 3280/30000 Training Loss: 0.04930425062775612\n",
      "Epoch 3281/30000 Training Loss: 0.06335717439651489\n",
      "Epoch 3282/30000 Training Loss: 0.06282630562782288\n",
      "Epoch 3283/30000 Training Loss: 0.06128218024969101\n",
      "Epoch 3284/30000 Training Loss: 0.06774245947599411\n",
      "Epoch 3285/30000 Training Loss: 0.07153318077325821\n",
      "Epoch 3286/30000 Training Loss: 0.060300312936306\n",
      "Epoch 3287/30000 Training Loss: 0.05292597413063049\n",
      "Epoch 3288/30000 Training Loss: 0.055462636053562164\n",
      "Epoch 3289/30000 Training Loss: 0.08142305910587311\n",
      "Epoch 3290/30000 Training Loss: 0.077531598508358\n",
      "Epoch 3291/30000 Training Loss: 0.05760063976049423\n",
      "Epoch 3292/30000 Training Loss: 0.055023279041051865\n",
      "Epoch 3293/30000 Training Loss: 0.044571757316589355\n",
      "Epoch 3294/30000 Training Loss: 0.05295739322900772\n",
      "Epoch 3295/30000 Training Loss: 0.0725989043712616\n",
      "Epoch 3296/30000 Training Loss: 0.053477752953767776\n",
      "Epoch 3297/30000 Training Loss: 0.060088712722063065\n",
      "Epoch 3298/30000 Training Loss: 0.06831154227256775\n",
      "Epoch 3299/30000 Training Loss: 0.07165941596031189\n",
      "Epoch 3300/30000 Training Loss: 0.052914757281541824\n",
      "Epoch 3300/30000 Validation Loss: 0.07187710702419281\n",
      "Epoch 3301/30000 Training Loss: 0.06251781433820724\n",
      "Epoch 3302/30000 Training Loss: 0.05806548893451691\n",
      "Epoch 3303/30000 Training Loss: 0.06363807618618011\n",
      "Epoch 3304/30000 Training Loss: 0.05072878301143646\n",
      "Epoch 3305/30000 Training Loss: 0.06215484440326691\n",
      "Epoch 3306/30000 Training Loss: 0.08471176028251648\n",
      "Epoch 3307/30000 Training Loss: 0.05691681057214737\n",
      "Epoch 3308/30000 Training Loss: 0.05936073511838913\n",
      "Epoch 3309/30000 Training Loss: 0.07026691734790802\n",
      "Epoch 3310/30000 Training Loss: 0.05925750732421875\n",
      "Epoch 3311/30000 Training Loss: 0.07400844246149063\n",
      "Epoch 3312/30000 Training Loss: 0.051365241408348083\n",
      "Epoch 3313/30000 Training Loss: 0.05444616079330444\n",
      "Epoch 3314/30000 Training Loss: 0.07096182554960251\n",
      "Epoch 3315/30000 Training Loss: 0.055062197148799896\n",
      "Epoch 3316/30000 Training Loss: 0.05382893979549408\n",
      "Epoch 3317/30000 Training Loss: 0.05740395188331604\n",
      "Epoch 3318/30000 Training Loss: 0.06341877579689026\n",
      "Epoch 3319/30000 Training Loss: 0.06066843122243881\n",
      "Epoch 3320/30000 Training Loss: 0.05530812591314316\n",
      "Epoch 3321/30000 Training Loss: 0.04409938305616379\n",
      "Epoch 3322/30000 Training Loss: 0.06117433309555054\n",
      "Epoch 3323/30000 Training Loss: 0.06323506683111191\n",
      "Epoch 3324/30000 Training Loss: 0.05697457119822502\n",
      "Epoch 3325/30000 Training Loss: 0.0628393217921257\n",
      "Epoch 3326/30000 Training Loss: 0.046869125217199326\n",
      "Epoch 3327/30000 Training Loss: 0.0529521182179451\n",
      "Epoch 3328/30000 Training Loss: 0.0698055699467659\n",
      "Epoch 3329/30000 Training Loss: 0.05860838294029236\n",
      "Epoch 3330/30000 Training Loss: 0.07174546271562576\n",
      "Epoch 3331/30000 Training Loss: 0.05190429836511612\n",
      "Epoch 3332/30000 Training Loss: 0.06697632372379303\n",
      "Epoch 3333/30000 Training Loss: 0.05950269103050232\n",
      "Epoch 3334/30000 Training Loss: 0.056083664298057556\n",
      "Epoch 3335/30000 Training Loss: 0.05969347432255745\n",
      "Epoch 3336/30000 Training Loss: 0.05650048702955246\n",
      "Epoch 3337/30000 Training Loss: 0.06588801741600037\n",
      "Epoch 3338/30000 Training Loss: 0.03954918310046196\n",
      "Epoch 3339/30000 Training Loss: 0.06253592669963837\n",
      "Epoch 3340/30000 Training Loss: 0.05397111922502518\n",
      "Epoch 3341/30000 Training Loss: 0.0633174479007721\n",
      "Epoch 3342/30000 Training Loss: 0.055563874542713165\n",
      "Epoch 3343/30000 Training Loss: 0.06074035167694092\n",
      "Epoch 3344/30000 Training Loss: 0.06705634295940399\n",
      "Epoch 3345/30000 Training Loss: 0.058364104479551315\n",
      "Epoch 3346/30000 Training Loss: 0.0595439076423645\n",
      "Epoch 3347/30000 Training Loss: 0.06372007727622986\n",
      "Epoch 3348/30000 Training Loss: 0.06265869736671448\n",
      "Epoch 3349/30000 Training Loss: 0.07915525138378143\n",
      "Epoch 3350/30000 Training Loss: 0.06893973052501678\n",
      "Epoch 3351/30000 Training Loss: 0.06599602848291397\n",
      "Epoch 3352/30000 Training Loss: 0.048756398260593414\n",
      "Epoch 3353/30000 Training Loss: 0.06537969410419464\n",
      "Epoch 3354/30000 Training Loss: 0.058509327471256256\n",
      "Epoch 3355/30000 Training Loss: 0.06675808131694794\n",
      "Epoch 3356/30000 Training Loss: 0.05679482966661453\n",
      "Epoch 3357/30000 Training Loss: 0.06628420948982239\n",
      "Epoch 3358/30000 Training Loss: 0.07128287851810455\n",
      "Epoch 3359/30000 Training Loss: 0.06774753332138062\n",
      "Epoch 3360/30000 Training Loss: 0.06433630734682083\n",
      "Epoch 3361/30000 Training Loss: 0.07207149267196655\n",
      "Epoch 3362/30000 Training Loss: 0.056227073073387146\n",
      "Epoch 3363/30000 Training Loss: 0.062100328505039215\n",
      "Epoch 3364/30000 Training Loss: 0.0636090561747551\n",
      "Epoch 3365/30000 Training Loss: 0.060346417129039764\n",
      "Epoch 3366/30000 Training Loss: 0.07299560308456421\n",
      "Epoch 3367/30000 Training Loss: 0.0547332763671875\n",
      "Epoch 3368/30000 Training Loss: 0.06627340614795685\n",
      "Epoch 3369/30000 Training Loss: 0.06160437688231468\n",
      "Epoch 3370/30000 Training Loss: 0.058418333530426025\n",
      "Epoch 3371/30000 Training Loss: 0.06035187095403671\n",
      "Epoch 3372/30000 Training Loss: 0.0482814759016037\n",
      "Epoch 3373/30000 Training Loss: 0.051368579268455505\n",
      "Epoch 3374/30000 Training Loss: 0.06438487768173218\n",
      "Epoch 3375/30000 Training Loss: 0.06835035234689713\n",
      "Epoch 3376/30000 Training Loss: 0.04789535701274872\n",
      "Epoch 3377/30000 Training Loss: 0.059353187680244446\n",
      "Epoch 3378/30000 Training Loss: 0.06699732691049576\n",
      "Epoch 3379/30000 Training Loss: 0.05209262669086456\n",
      "Epoch 3380/30000 Training Loss: 0.05054779350757599\n",
      "Epoch 3381/30000 Training Loss: 0.07531890273094177\n",
      "Epoch 3382/30000 Training Loss: 0.05404523015022278\n",
      "Epoch 3383/30000 Training Loss: 0.05261952057480812\n",
      "Epoch 3384/30000 Training Loss: 0.06448409706354141\n",
      "Epoch 3385/30000 Training Loss: 0.0783131867647171\n",
      "Epoch 3386/30000 Training Loss: 0.07262630760669708\n",
      "Epoch 3387/30000 Training Loss: 0.06874484568834305\n",
      "Epoch 3388/30000 Training Loss: 0.07157061994075775\n",
      "Epoch 3389/30000 Training Loss: 0.07453594356775284\n",
      "Epoch 3390/30000 Training Loss: 0.05488399416208267\n",
      "Epoch 3391/30000 Training Loss: 0.06360501796007156\n",
      "Epoch 3392/30000 Training Loss: 0.055323660373687744\n",
      "Epoch 3393/30000 Training Loss: 0.07393424957990646\n",
      "Epoch 3394/30000 Training Loss: 0.04647254943847656\n",
      "Epoch 3395/30000 Training Loss: 0.06756699085235596\n",
      "Epoch 3396/30000 Training Loss: 0.05352858081459999\n",
      "Epoch 3397/30000 Training Loss: 0.04860277473926544\n",
      "Epoch 3398/30000 Training Loss: 0.0686047226190567\n",
      "Epoch 3399/30000 Training Loss: 0.04834774136543274\n",
      "Epoch 3400/30000 Training Loss: 0.06895822286605835\n",
      "Epoch 3400/30000 Validation Loss: 0.05671984702348709\n",
      "Epoch 3401/30000 Training Loss: 0.07775773108005524\n",
      "Epoch 3402/30000 Training Loss: 0.058348383754491806\n",
      "Epoch 3403/30000 Training Loss: 0.07766591757535934\n",
      "Epoch 3404/30000 Training Loss: 0.08767436444759369\n",
      "Epoch 3405/30000 Training Loss: 0.07086043804883957\n",
      "Epoch 3406/30000 Training Loss: 0.044115275144577026\n",
      "Epoch 3407/30000 Training Loss: 0.05940784513950348\n",
      "Epoch 3408/30000 Training Loss: 0.05638720840215683\n",
      "Epoch 3409/30000 Training Loss: 0.06936195492744446\n",
      "Epoch 3410/30000 Training Loss: 0.06590196490287781\n",
      "Epoch 3411/30000 Training Loss: 0.05450533330440521\n",
      "Epoch 3412/30000 Training Loss: 0.04438593238592148\n",
      "Epoch 3413/30000 Training Loss: 0.048987019807100296\n",
      "Epoch 3414/30000 Training Loss: 0.05298062786459923\n",
      "Epoch 3415/30000 Training Loss: 0.048927322030067444\n",
      "Epoch 3416/30000 Training Loss: 0.058915190398693085\n",
      "Epoch 3417/30000 Training Loss: 0.06465999037027359\n",
      "Epoch 3418/30000 Training Loss: 0.06844231486320496\n",
      "Epoch 3419/30000 Training Loss: 0.0653369128704071\n",
      "Epoch 3420/30000 Training Loss: 0.06257990002632141\n",
      "Epoch 3421/30000 Training Loss: 0.05773237347602844\n",
      "Epoch 3422/30000 Training Loss: 0.056818924844264984\n",
      "Epoch 3423/30000 Training Loss: 0.053817018866539\n",
      "Epoch 3424/30000 Training Loss: 0.05619443953037262\n",
      "Epoch 3425/30000 Training Loss: 0.07029146701097488\n",
      "Epoch 3426/30000 Training Loss: 0.07317008078098297\n",
      "Epoch 3427/30000 Training Loss: 0.05507390946149826\n",
      "Epoch 3428/30000 Training Loss: 0.04917112737894058\n",
      "Epoch 3429/30000 Training Loss: 0.059017762541770935\n",
      "Epoch 3430/30000 Training Loss: 0.07632865011692047\n",
      "Epoch 3431/30000 Training Loss: 0.06543358415365219\n",
      "Epoch 3432/30000 Training Loss: 0.05501507967710495\n",
      "Epoch 3433/30000 Training Loss: 0.042545925825834274\n",
      "Epoch 3434/30000 Training Loss: 0.06507444381713867\n",
      "Epoch 3435/30000 Training Loss: 0.06007005274295807\n",
      "Epoch 3436/30000 Training Loss: 0.072580985724926\n",
      "Epoch 3437/30000 Training Loss: 0.07062972337007523\n",
      "Epoch 3438/30000 Training Loss: 0.07989100366830826\n",
      "Epoch 3439/30000 Training Loss: 0.061160385608673096\n",
      "Epoch 3440/30000 Training Loss: 0.06239153444766998\n",
      "Epoch 3441/30000 Training Loss: 0.04760151356458664\n",
      "Epoch 3442/30000 Training Loss: 0.045634910464286804\n",
      "Epoch 3443/30000 Training Loss: 0.06568533927202225\n",
      "Epoch 3444/30000 Training Loss: 0.06974554061889648\n",
      "Epoch 3445/30000 Training Loss: 0.054235056042671204\n",
      "Epoch 3446/30000 Training Loss: 0.06543052941560745\n",
      "Epoch 3447/30000 Training Loss: 0.05734875053167343\n",
      "Epoch 3448/30000 Training Loss: 0.04519066959619522\n",
      "Epoch 3449/30000 Training Loss: 0.052665017545223236\n",
      "Epoch 3450/30000 Training Loss: 0.05578561872243881\n",
      "Epoch 3451/30000 Training Loss: 0.06450013816356659\n",
      "Epoch 3452/30000 Training Loss: 0.06381474435329437\n",
      "Epoch 3453/30000 Training Loss: 0.054295673966407776\n",
      "Epoch 3454/30000 Training Loss: 0.05113048106431961\n",
      "Epoch 3455/30000 Training Loss: 0.0670233890414238\n",
      "Epoch 3456/30000 Training Loss: 0.06328128278255463\n",
      "Epoch 3457/30000 Training Loss: 0.06760105490684509\n",
      "Epoch 3458/30000 Training Loss: 0.06035073846578598\n",
      "Epoch 3459/30000 Training Loss: 0.06851870566606522\n",
      "Epoch 3460/30000 Training Loss: 0.06606031954288483\n",
      "Epoch 3461/30000 Training Loss: 0.05707123130559921\n",
      "Epoch 3462/30000 Training Loss: 0.04871504753828049\n",
      "Epoch 3463/30000 Training Loss: 0.04894686117768288\n",
      "Epoch 3464/30000 Training Loss: 0.04835682362318039\n",
      "Epoch 3465/30000 Training Loss: 0.04961060732603073\n",
      "Epoch 3466/30000 Training Loss: 0.052436817437410355\n",
      "Epoch 3467/30000 Training Loss: 0.07100842893123627\n",
      "Epoch 3468/30000 Training Loss: 0.05717488378286362\n",
      "Epoch 3469/30000 Training Loss: 0.07395043969154358\n",
      "Epoch 3470/30000 Training Loss: 0.06072322279214859\n",
      "Epoch 3471/30000 Training Loss: 0.05443887412548065\n",
      "Epoch 3472/30000 Training Loss: 0.056678421795368195\n",
      "Epoch 3473/30000 Training Loss: 0.07155469059944153\n",
      "Epoch 3474/30000 Training Loss: 0.04685354232788086\n",
      "Epoch 3475/30000 Training Loss: 0.06352603435516357\n",
      "Epoch 3476/30000 Training Loss: 0.07013147324323654\n",
      "Epoch 3477/30000 Training Loss: 0.05890145152807236\n",
      "Epoch 3478/30000 Training Loss: 0.05513909459114075\n",
      "Epoch 3479/30000 Training Loss: 0.05452384054660797\n",
      "Epoch 3480/30000 Training Loss: 0.062355875968933105\n",
      "Epoch 3481/30000 Training Loss: 0.05766734853386879\n",
      "Epoch 3482/30000 Training Loss: 0.07330746203660965\n",
      "Epoch 3483/30000 Training Loss: 0.07095202058553696\n",
      "Epoch 3484/30000 Training Loss: 0.06940381228923798\n",
      "Epoch 3485/30000 Training Loss: 0.06373101472854614\n",
      "Epoch 3486/30000 Training Loss: 0.05456075817346573\n",
      "Epoch 3487/30000 Training Loss: 0.06808275729417801\n",
      "Epoch 3488/30000 Training Loss: 0.05814320594072342\n",
      "Epoch 3489/30000 Training Loss: 0.06860602647066116\n",
      "Epoch 3490/30000 Training Loss: 0.058015625923871994\n",
      "Epoch 3491/30000 Training Loss: 0.05794856697320938\n",
      "Epoch 3492/30000 Training Loss: 0.06441354751586914\n",
      "Epoch 3493/30000 Training Loss: 0.06492432951927185\n",
      "Epoch 3494/30000 Training Loss: 0.07925067842006683\n",
      "Epoch 3495/30000 Training Loss: 0.05945906043052673\n",
      "Epoch 3496/30000 Training Loss: 0.062256328761577606\n",
      "Epoch 3497/30000 Training Loss: 0.06777750700712204\n",
      "Epoch 3498/30000 Training Loss: 0.05647414177656174\n",
      "Epoch 3499/30000 Training Loss: 0.0491061732172966\n",
      "Epoch 3500/30000 Training Loss: 0.049677662551403046\n",
      "Epoch 3500/30000 Validation Loss: 0.046978242695331573\n",
      "Epoch 3501/30000 Training Loss: 0.07289451360702515\n",
      "Epoch 3502/30000 Training Loss: 0.056952059268951416\n",
      "Epoch 3503/30000 Training Loss: 0.06571290642023087\n",
      "Epoch 3504/30000 Training Loss: 0.0657912939786911\n",
      "Epoch 3505/30000 Training Loss: 0.056037064641714096\n",
      "Epoch 3506/30000 Training Loss: 0.05990506336092949\n",
      "Epoch 3507/30000 Training Loss: 0.06459876894950867\n",
      "Epoch 3508/30000 Training Loss: 0.06786617636680603\n",
      "Epoch 3509/30000 Training Loss: 0.057506710290908813\n",
      "Epoch 3510/30000 Training Loss: 0.06503766030073166\n",
      "Epoch 3511/30000 Training Loss: 0.07192376255989075\n",
      "Epoch 3512/30000 Training Loss: 0.07461850345134735\n",
      "Epoch 3513/30000 Training Loss: 0.06818166375160217\n",
      "Epoch 3514/30000 Training Loss: 0.05601244419813156\n",
      "Epoch 3515/30000 Training Loss: 0.04995296150445938\n",
      "Epoch 3516/30000 Training Loss: 0.07020054757595062\n",
      "Epoch 3517/30000 Training Loss: 0.06430520117282867\n",
      "Epoch 3518/30000 Training Loss: 0.047772474586963654\n",
      "Epoch 3519/30000 Training Loss: 0.06541728973388672\n",
      "Epoch 3520/30000 Training Loss: 0.06180190294981003\n",
      "Epoch 3521/30000 Training Loss: 0.06567947566509247\n",
      "Epoch 3522/30000 Training Loss: 0.04785667732357979\n",
      "Epoch 3523/30000 Training Loss: 0.07059623301029205\n",
      "Epoch 3524/30000 Training Loss: 0.07261979579925537\n",
      "Epoch 3525/30000 Training Loss: 0.05882585793733597\n",
      "Epoch 3526/30000 Training Loss: 0.06531251966953278\n",
      "Epoch 3527/30000 Training Loss: 0.0626545175909996\n",
      "Epoch 3528/30000 Training Loss: 0.05068524181842804\n",
      "Epoch 3529/30000 Training Loss: 0.04906367510557175\n",
      "Epoch 3530/30000 Training Loss: 0.05844924598932266\n",
      "Epoch 3531/30000 Training Loss: 0.07170151174068451\n",
      "Epoch 3532/30000 Training Loss: 0.06256938725709915\n",
      "Epoch 3533/30000 Training Loss: 0.05290721356868744\n",
      "Epoch 3534/30000 Training Loss: 0.05727824568748474\n",
      "Epoch 3535/30000 Training Loss: 0.05701867491006851\n",
      "Epoch 3536/30000 Training Loss: 0.05502593517303467\n",
      "Epoch 3537/30000 Training Loss: 0.07278770208358765\n",
      "Epoch 3538/30000 Training Loss: 0.0538368746638298\n",
      "Epoch 3539/30000 Training Loss: 0.056204430758953094\n",
      "Epoch 3540/30000 Training Loss: 0.04517367109656334\n",
      "Epoch 3541/30000 Training Loss: 0.06793107837438583\n",
      "Epoch 3542/30000 Training Loss: 0.05740486830472946\n",
      "Epoch 3543/30000 Training Loss: 0.06550218164920807\n",
      "Epoch 3544/30000 Training Loss: 0.06474624574184418\n",
      "Epoch 3545/30000 Training Loss: 0.06117688864469528\n",
      "Epoch 3546/30000 Training Loss: 0.04676297679543495\n",
      "Epoch 3547/30000 Training Loss: 0.07532362639904022\n",
      "Epoch 3548/30000 Training Loss: 0.06183673441410065\n",
      "Epoch 3549/30000 Training Loss: 0.06783423572778702\n",
      "Epoch 3550/30000 Training Loss: 0.06732293963432312\n",
      "Epoch 3551/30000 Training Loss: 0.06155593320727348\n",
      "Epoch 3552/30000 Training Loss: 0.06052469462156296\n",
      "Epoch 3553/30000 Training Loss: 0.0687914788722992\n",
      "Epoch 3554/30000 Training Loss: 0.05549738556146622\n",
      "Epoch 3555/30000 Training Loss: 0.06396621465682983\n",
      "Epoch 3556/30000 Training Loss: 0.04890154302120209\n",
      "Epoch 3557/30000 Training Loss: 0.060346849262714386\n",
      "Epoch 3558/30000 Training Loss: 0.0708799660205841\n",
      "Epoch 3559/30000 Training Loss: 0.04666847735643387\n",
      "Epoch 3560/30000 Training Loss: 0.05291657894849777\n",
      "Epoch 3561/30000 Training Loss: 0.05289778113365173\n",
      "Epoch 3562/30000 Training Loss: 0.06861888617277145\n",
      "Epoch 3563/30000 Training Loss: 0.07222027331590652\n",
      "Epoch 3564/30000 Training Loss: 0.0667702704668045\n",
      "Epoch 3565/30000 Training Loss: 0.051417574286460876\n",
      "Epoch 3566/30000 Training Loss: 0.0616874098777771\n",
      "Epoch 3567/30000 Training Loss: 0.05518953502178192\n",
      "Epoch 3568/30000 Training Loss: 0.05027925968170166\n",
      "Epoch 3569/30000 Training Loss: 0.06620714068412781\n",
      "Epoch 3570/30000 Training Loss: 0.060331400483846664\n",
      "Epoch 3571/30000 Training Loss: 0.05570894479751587\n",
      "Epoch 3572/30000 Training Loss: 0.0468289814889431\n",
      "Epoch 3573/30000 Training Loss: 0.0684000551700592\n",
      "Epoch 3574/30000 Training Loss: 0.05981344357132912\n",
      "Epoch 3575/30000 Training Loss: 0.058664530515670776\n",
      "Epoch 3576/30000 Training Loss: 0.06932254135608673\n",
      "Epoch 3577/30000 Training Loss: 0.07113103568553925\n",
      "Epoch 3578/30000 Training Loss: 0.05564423277974129\n",
      "Epoch 3579/30000 Training Loss: 0.06166376173496246\n",
      "Epoch 3580/30000 Training Loss: 0.05987255275249481\n",
      "Epoch 3581/30000 Training Loss: 0.04840640723705292\n",
      "Epoch 3582/30000 Training Loss: 0.056672103703022\n",
      "Epoch 3583/30000 Training Loss: 0.06185091659426689\n",
      "Epoch 3584/30000 Training Loss: 0.0505315326154232\n",
      "Epoch 3585/30000 Training Loss: 0.06213536858558655\n",
      "Epoch 3586/30000 Training Loss: 0.06280194967985153\n",
      "Epoch 3587/30000 Training Loss: 0.0564892441034317\n",
      "Epoch 3588/30000 Training Loss: 0.05538929998874664\n",
      "Epoch 3589/30000 Training Loss: 0.05544763803482056\n",
      "Epoch 3590/30000 Training Loss: 0.06347358226776123\n",
      "Epoch 3591/30000 Training Loss: 0.05278453975915909\n",
      "Epoch 3592/30000 Training Loss: 0.05753282085061073\n",
      "Epoch 3593/30000 Training Loss: 0.06374888867139816\n",
      "Epoch 3594/30000 Training Loss: 0.06761866062879562\n",
      "Epoch 3595/30000 Training Loss: 0.05462059751152992\n",
      "Epoch 3596/30000 Training Loss: 0.06558187305927277\n",
      "Epoch 3597/30000 Training Loss: 0.04347824305295944\n",
      "Epoch 3598/30000 Training Loss: 0.0793052613735199\n",
      "Epoch 3599/30000 Training Loss: 0.060613520443439484\n",
      "Epoch 3600/30000 Training Loss: 0.06544840335845947\n",
      "Epoch 3600/30000 Validation Loss: 0.07232040911912918\n",
      "Epoch 3601/30000 Training Loss: 0.04761366918683052\n",
      "Epoch 3602/30000 Training Loss: 0.08045238256454468\n",
      "Epoch 3603/30000 Training Loss: 0.053913991898298264\n",
      "Epoch 3604/30000 Training Loss: 0.06286104768514633\n",
      "Epoch 3605/30000 Training Loss: 0.04951886087656021\n",
      "Epoch 3606/30000 Training Loss: 0.06692859530448914\n",
      "Epoch 3607/30000 Training Loss: 0.06044292822480202\n",
      "Epoch 3608/30000 Training Loss: 0.06400332599878311\n",
      "Epoch 3609/30000 Training Loss: 0.05353577435016632\n",
      "Epoch 3610/30000 Training Loss: 0.06255857646465302\n",
      "Epoch 3611/30000 Training Loss: 0.06598937511444092\n",
      "Epoch 3612/30000 Training Loss: 0.061909228563308716\n",
      "Epoch 3613/30000 Training Loss: 0.05034199357032776\n",
      "Epoch 3614/30000 Training Loss: 0.06150992587208748\n",
      "Epoch 3615/30000 Training Loss: 0.04713187739253044\n",
      "Epoch 3616/30000 Training Loss: 0.06884108483791351\n",
      "Epoch 3617/30000 Training Loss: 0.05211031809449196\n",
      "Epoch 3618/30000 Training Loss: 0.06514394283294678\n",
      "Epoch 3619/30000 Training Loss: 0.0750347226858139\n",
      "Epoch 3620/30000 Training Loss: 0.06533640623092651\n",
      "Epoch 3621/30000 Training Loss: 0.04553478956222534\n",
      "Epoch 3622/30000 Training Loss: 0.06747251749038696\n",
      "Epoch 3623/30000 Training Loss: 0.06247188150882721\n",
      "Epoch 3624/30000 Training Loss: 0.04666747897863388\n",
      "Epoch 3625/30000 Training Loss: 0.04773586988449097\n",
      "Epoch 3626/30000 Training Loss: 0.05777425691485405\n",
      "Epoch 3627/30000 Training Loss: 0.06091764196753502\n",
      "Epoch 3628/30000 Training Loss: 0.05825931578874588\n",
      "Epoch 3629/30000 Training Loss: 0.07054587453603745\n",
      "Epoch 3630/30000 Training Loss: 0.04951293766498566\n",
      "Epoch 3631/30000 Training Loss: 0.04780280962586403\n",
      "Epoch 3632/30000 Training Loss: 0.0694379135966301\n",
      "Epoch 3633/30000 Training Loss: 0.06102935969829559\n",
      "Epoch 3634/30000 Training Loss: 0.05771481990814209\n",
      "Epoch 3635/30000 Training Loss: 0.05417744070291519\n",
      "Epoch 3636/30000 Training Loss: 0.07124869525432587\n",
      "Epoch 3637/30000 Training Loss: 0.06935805082321167\n",
      "Epoch 3638/30000 Training Loss: 0.05105093866586685\n",
      "Epoch 3639/30000 Training Loss: 0.06427125632762909\n",
      "Epoch 3640/30000 Training Loss: 0.07533620297908783\n",
      "Epoch 3641/30000 Training Loss: 0.04462598264217377\n",
      "Epoch 3642/30000 Training Loss: 0.048801809549331665\n",
      "Epoch 3643/30000 Training Loss: 0.059285979717969894\n",
      "Epoch 3644/30000 Training Loss: 0.08006870746612549\n",
      "Epoch 3645/30000 Training Loss: 0.051377810537815094\n",
      "Epoch 3646/30000 Training Loss: 0.06801975518465042\n",
      "Epoch 3647/30000 Training Loss: 0.06154918670654297\n",
      "Epoch 3648/30000 Training Loss: 0.04627513140439987\n",
      "Epoch 3649/30000 Training Loss: 0.05586915463209152\n",
      "Epoch 3650/30000 Training Loss: 0.0743982195854187\n",
      "Epoch 3651/30000 Training Loss: 0.04727887362241745\n",
      "Epoch 3652/30000 Training Loss: 0.061211466789245605\n",
      "Epoch 3653/30000 Training Loss: 0.04556678608059883\n",
      "Epoch 3654/30000 Training Loss: 0.05713922530412674\n",
      "Epoch 3655/30000 Training Loss: 0.05914794281125069\n",
      "Epoch 3656/30000 Training Loss: 0.06607335805892944\n",
      "Epoch 3657/30000 Training Loss: 0.05627398192882538\n",
      "Epoch 3658/30000 Training Loss: 0.06188785284757614\n",
      "Epoch 3659/30000 Training Loss: 0.06272030621767044\n",
      "Epoch 3660/30000 Training Loss: 0.06339941173791885\n",
      "Epoch 3661/30000 Training Loss: 0.06196659058332443\n",
      "Epoch 3662/30000 Training Loss: 0.07576483488082886\n",
      "Epoch 3663/30000 Training Loss: 0.05986905097961426\n",
      "Epoch 3664/30000 Training Loss: 0.06122415512800217\n",
      "Epoch 3665/30000 Training Loss: 0.05803607404232025\n",
      "Epoch 3666/30000 Training Loss: 0.054751984775066376\n",
      "Epoch 3667/30000 Training Loss: 0.05214560776948929\n",
      "Epoch 3668/30000 Training Loss: 0.05822683125734329\n",
      "Epoch 3669/30000 Training Loss: 0.05347520112991333\n",
      "Epoch 3670/30000 Training Loss: 0.07431285083293915\n",
      "Epoch 3671/30000 Training Loss: 0.04737512394785881\n",
      "Epoch 3672/30000 Training Loss: 0.0631105899810791\n",
      "Epoch 3673/30000 Training Loss: 0.05490231141448021\n",
      "Epoch 3674/30000 Training Loss: 0.0626799687743187\n",
      "Epoch 3675/30000 Training Loss: 0.04585873335599899\n",
      "Epoch 3676/30000 Training Loss: 0.047309160232543945\n",
      "Epoch 3677/30000 Training Loss: 0.05837585777044296\n",
      "Epoch 3678/30000 Training Loss: 0.0663481056690216\n",
      "Epoch 3679/30000 Training Loss: 0.06389264762401581\n",
      "Epoch 3680/30000 Training Loss: 0.06972998380661011\n",
      "Epoch 3681/30000 Training Loss: 0.054560236632823944\n",
      "Epoch 3682/30000 Training Loss: 0.05430071800947189\n",
      "Epoch 3683/30000 Training Loss: 0.05184241384267807\n",
      "Epoch 3684/30000 Training Loss: 0.04836062341928482\n",
      "Epoch 3685/30000 Training Loss: 0.06533713638782501\n",
      "Epoch 3686/30000 Training Loss: 0.06838412582874298\n",
      "Epoch 3687/30000 Training Loss: 0.05719103664159775\n",
      "Epoch 3688/30000 Training Loss: 0.051938746124506\n",
      "Epoch 3689/30000 Training Loss: 0.06149128079414368\n",
      "Epoch 3690/30000 Training Loss: 0.059955209493637085\n",
      "Epoch 3691/30000 Training Loss: 0.07147795706987381\n",
      "Epoch 3692/30000 Training Loss: 0.05113140493631363\n",
      "Epoch 3693/30000 Training Loss: 0.061737868934869766\n",
      "Epoch 3694/30000 Training Loss: 0.07018927484750748\n",
      "Epoch 3695/30000 Training Loss: 0.0732608363032341\n",
      "Epoch 3696/30000 Training Loss: 0.06877753883600235\n",
      "Epoch 3697/30000 Training Loss: 0.05505704507231712\n",
      "Epoch 3698/30000 Training Loss: 0.07362506538629532\n",
      "Epoch 3699/30000 Training Loss: 0.053893234580755234\n",
      "Epoch 3700/30000 Training Loss: 0.056095294654369354\n",
      "Epoch 3700/30000 Validation Loss: 0.06352262943983078\n",
      "Epoch 3701/30000 Training Loss: 0.04854900762438774\n",
      "Epoch 3702/30000 Training Loss: 0.06166312098503113\n",
      "Epoch 3703/30000 Training Loss: 0.06392665207386017\n",
      "Epoch 3704/30000 Training Loss: 0.05085012689232826\n",
      "Epoch 3705/30000 Training Loss: 0.049652308225631714\n",
      "Epoch 3706/30000 Training Loss: 0.0498993918299675\n",
      "Epoch 3707/30000 Training Loss: 0.07693105936050415\n",
      "Epoch 3708/30000 Training Loss: 0.058159373700618744\n",
      "Epoch 3709/30000 Training Loss: 0.06361968070268631\n",
      "Epoch 3710/30000 Training Loss: 0.05746602267026901\n",
      "Epoch 3711/30000 Training Loss: 0.05777464807033539\n",
      "Epoch 3712/30000 Training Loss: 0.07169472426176071\n",
      "Epoch 3713/30000 Training Loss: 0.06500381231307983\n",
      "Epoch 3714/30000 Training Loss: 0.057661205530166626\n",
      "Epoch 3715/30000 Training Loss: 0.06256140023469925\n",
      "Epoch 3716/30000 Training Loss: 0.04948621615767479\n",
      "Epoch 3717/30000 Training Loss: 0.06947989016771317\n",
      "Epoch 3718/30000 Training Loss: 0.06343601644039154\n",
      "Epoch 3719/30000 Training Loss: 0.0652284175157547\n",
      "Epoch 3720/30000 Training Loss: 0.06681037694215775\n",
      "Epoch 3721/30000 Training Loss: 0.061901211738586426\n",
      "Epoch 3722/30000 Training Loss: 0.06519525498151779\n",
      "Epoch 3723/30000 Training Loss: 0.054932523518800735\n",
      "Epoch 3724/30000 Training Loss: 0.06099000573158264\n",
      "Epoch 3725/30000 Training Loss: 0.048499591648578644\n",
      "Epoch 3726/30000 Training Loss: 0.08026805520057678\n",
      "Epoch 3727/30000 Training Loss: 0.06685294210910797\n",
      "Epoch 3728/30000 Training Loss: 0.060909830033779144\n",
      "Epoch 3729/30000 Training Loss: 0.08028892427682877\n",
      "Epoch 3730/30000 Training Loss: 0.06112860143184662\n",
      "Epoch 3731/30000 Training Loss: 0.06730404496192932\n",
      "Epoch 3732/30000 Training Loss: 0.05402451008558273\n",
      "Epoch 3733/30000 Training Loss: 0.06587408483028412\n",
      "Epoch 3734/30000 Training Loss: 0.057059869170188904\n",
      "Epoch 3735/30000 Training Loss: 0.07111957669258118\n",
      "Epoch 3736/30000 Training Loss: 0.053583741188049316\n",
      "Epoch 3737/30000 Training Loss: 0.07302936911582947\n",
      "Epoch 3738/30000 Training Loss: 0.06866394728422165\n",
      "Epoch 3739/30000 Training Loss: 0.062475282698869705\n",
      "Epoch 3740/30000 Training Loss: 0.043121613562107086\n",
      "Epoch 3741/30000 Training Loss: 0.07325178384780884\n",
      "Epoch 3742/30000 Training Loss: 0.062131598591804504\n",
      "Epoch 3743/30000 Training Loss: 0.05123516172170639\n",
      "Epoch 3744/30000 Training Loss: 0.06910660862922668\n",
      "Epoch 3745/30000 Training Loss: 0.08280373364686966\n",
      "Epoch 3746/30000 Training Loss: 0.07365341484546661\n",
      "Epoch 3747/30000 Training Loss: 0.06812497228384018\n",
      "Epoch 3748/30000 Training Loss: 0.07056595385074615\n",
      "Epoch 3749/30000 Training Loss: 0.06336113810539246\n",
      "Epoch 3750/30000 Training Loss: 0.06011006981134415\n",
      "Epoch 3751/30000 Training Loss: 0.065080426633358\n",
      "Epoch 3752/30000 Training Loss: 0.057088810950517654\n",
      "Epoch 3753/30000 Training Loss: 0.0568232536315918\n",
      "Epoch 3754/30000 Training Loss: 0.05275610089302063\n",
      "Epoch 3755/30000 Training Loss: 0.06002777814865112\n",
      "Epoch 3756/30000 Training Loss: 0.05105094611644745\n",
      "Epoch 3757/30000 Training Loss: 0.07582873106002808\n",
      "Epoch 3758/30000 Training Loss: 0.05649111047387123\n",
      "Epoch 3759/30000 Training Loss: 0.04985327273607254\n",
      "Epoch 3760/30000 Training Loss: 0.06670193374156952\n",
      "Epoch 3761/30000 Training Loss: 0.05874628201127052\n",
      "Epoch 3762/30000 Training Loss: 0.07477372139692307\n",
      "Epoch 3763/30000 Training Loss: 0.07203987240791321\n",
      "Epoch 3764/30000 Training Loss: 0.05404118448495865\n",
      "Epoch 3765/30000 Training Loss: 0.06271688640117645\n",
      "Epoch 3766/30000 Training Loss: 0.07491129636764526\n",
      "Epoch 3767/30000 Training Loss: 0.04453899711370468\n",
      "Epoch 3768/30000 Training Loss: 0.06306744366884232\n",
      "Epoch 3769/30000 Training Loss: 0.062422920018434525\n",
      "Epoch 3770/30000 Training Loss: 0.06132635474205017\n",
      "Epoch 3771/30000 Training Loss: 0.05826921761035919\n",
      "Epoch 3772/30000 Training Loss: 0.05447009205818176\n",
      "Epoch 3773/30000 Training Loss: 0.05056843161582947\n",
      "Epoch 3774/30000 Training Loss: 0.0484277680516243\n",
      "Epoch 3775/30000 Training Loss: 0.06530045717954636\n",
      "Epoch 3776/30000 Training Loss: 0.0734838992357254\n",
      "Epoch 3777/30000 Training Loss: 0.06256763637065887\n",
      "Epoch 3778/30000 Training Loss: 0.046424444764852524\n",
      "Epoch 3779/30000 Training Loss: 0.05380485951900482\n",
      "Epoch 3780/30000 Training Loss: 0.05525904893875122\n",
      "Epoch 3781/30000 Training Loss: 0.0606803297996521\n",
      "Epoch 3782/30000 Training Loss: 0.05711930990219116\n",
      "Epoch 3783/30000 Training Loss: 0.05008349567651749\n",
      "Epoch 3784/30000 Training Loss: 0.05795551463961601\n",
      "Epoch 3785/30000 Training Loss: 0.06141112744808197\n",
      "Epoch 3786/30000 Training Loss: 0.06076580286026001\n",
      "Epoch 3787/30000 Training Loss: 0.08297406136989594\n",
      "Epoch 3788/30000 Training Loss: 0.059079792350530624\n",
      "Epoch 3789/30000 Training Loss: 0.06931474804878235\n",
      "Epoch 3790/30000 Training Loss: 0.05202416703104973\n",
      "Epoch 3791/30000 Training Loss: 0.05882622301578522\n",
      "Epoch 3792/30000 Training Loss: 0.06706173717975616\n",
      "Epoch 3793/30000 Training Loss: 0.05593129247426987\n",
      "Epoch 3794/30000 Training Loss: 0.05726177990436554\n",
      "Epoch 3795/30000 Training Loss: 0.05899415910243988\n",
      "Epoch 3796/30000 Training Loss: 0.08282757550477982\n",
      "Epoch 3797/30000 Training Loss: 0.06691744178533554\n",
      "Epoch 3798/30000 Training Loss: 0.06796036660671234\n",
      "Epoch 3799/30000 Training Loss: 0.05620327964425087\n",
      "Epoch 3800/30000 Training Loss: 0.05110712721943855\n",
      "Epoch 3800/30000 Validation Loss: 0.06338454782962799\n",
      "Epoch 3801/30000 Training Loss: 0.059250570833683014\n",
      "Epoch 3802/30000 Training Loss: 0.06230384111404419\n",
      "Epoch 3803/30000 Training Loss: 0.07059909403324127\n",
      "Epoch 3804/30000 Training Loss: 0.06335707753896713\n",
      "Epoch 3805/30000 Training Loss: 0.06596295535564423\n",
      "Epoch 3806/30000 Training Loss: 0.06029629334807396\n",
      "Epoch 3807/30000 Training Loss: 0.048433490097522736\n",
      "Epoch 3808/30000 Training Loss: 0.05448291450738907\n",
      "Epoch 3809/30000 Training Loss: 0.06462685763835907\n",
      "Epoch 3810/30000 Training Loss: 0.05494134500622749\n",
      "Epoch 3811/30000 Training Loss: 0.07471805810928345\n",
      "Epoch 3812/30000 Training Loss: 0.06754720211029053\n",
      "Epoch 3813/30000 Training Loss: 0.042917586863040924\n",
      "Epoch 3814/30000 Training Loss: 0.05268898606300354\n",
      "Epoch 3815/30000 Training Loss: 0.055125460028648376\n",
      "Epoch 3816/30000 Training Loss: 0.06696692109107971\n",
      "Epoch 3817/30000 Training Loss: 0.07500074803829193\n",
      "Epoch 3818/30000 Training Loss: 0.0567634291946888\n",
      "Epoch 3819/30000 Training Loss: 0.05728401988744736\n",
      "Epoch 3820/30000 Training Loss: 0.05182679742574692\n",
      "Epoch 3821/30000 Training Loss: 0.06327690184116364\n",
      "Epoch 3822/30000 Training Loss: 0.05784758925437927\n",
      "Epoch 3823/30000 Training Loss: 0.08029016852378845\n",
      "Epoch 3824/30000 Training Loss: 0.07035364210605621\n",
      "Epoch 3825/30000 Training Loss: 0.06443756818771362\n",
      "Epoch 3826/30000 Training Loss: 0.06967632472515106\n",
      "Epoch 3827/30000 Training Loss: 0.0836559385061264\n",
      "Epoch 3828/30000 Training Loss: 0.04793207347393036\n",
      "Epoch 3829/30000 Training Loss: 0.07013844698667526\n",
      "Epoch 3830/30000 Training Loss: 0.04835844039916992\n",
      "Epoch 3831/30000 Training Loss: 0.060467302799224854\n",
      "Epoch 3832/30000 Training Loss: 0.05466892570257187\n",
      "Epoch 3833/30000 Training Loss: 0.04823962226510048\n",
      "Epoch 3834/30000 Training Loss: 0.07114614546298981\n",
      "Epoch 3835/30000 Training Loss: 0.06069251894950867\n",
      "Epoch 3836/30000 Training Loss: 0.06546441465616226\n",
      "Epoch 3837/30000 Training Loss: 0.07212892174720764\n",
      "Epoch 3838/30000 Training Loss: 0.07218322157859802\n",
      "Epoch 3839/30000 Training Loss: 0.047020621597766876\n",
      "Epoch 3840/30000 Training Loss: 0.0572449155151844\n",
      "Epoch 3841/30000 Training Loss: 0.05054678022861481\n",
      "Epoch 3842/30000 Training Loss: 0.07613086700439453\n",
      "Epoch 3843/30000 Training Loss: 0.06773693114519119\n",
      "Epoch 3844/30000 Training Loss: 0.06650844216346741\n",
      "Epoch 3845/30000 Training Loss: 0.0715370923280716\n",
      "Epoch 3846/30000 Training Loss: 0.06264380365610123\n",
      "Epoch 3847/30000 Training Loss: 0.05637139081954956\n",
      "Epoch 3848/30000 Training Loss: 0.07415354251861572\n",
      "Epoch 3849/30000 Training Loss: 0.06760035455226898\n",
      "Epoch 3850/30000 Training Loss: 0.06737716495990753\n",
      "Epoch 3851/30000 Training Loss: 0.05308283120393753\n",
      "Epoch 3852/30000 Training Loss: 0.06872864812612534\n",
      "Epoch 3853/30000 Training Loss: 0.06858731806278229\n",
      "Epoch 3854/30000 Training Loss: 0.056910187005996704\n",
      "Epoch 3855/30000 Training Loss: 0.05120908468961716\n",
      "Epoch 3856/30000 Training Loss: 0.04362921416759491\n",
      "Epoch 3857/30000 Training Loss: 0.055744048207998276\n",
      "Epoch 3858/30000 Training Loss: 0.06447257101535797\n",
      "Epoch 3859/30000 Training Loss: 0.05505495145916939\n",
      "Epoch 3860/30000 Training Loss: 0.06921136379241943\n",
      "Epoch 3861/30000 Training Loss: 0.04792797565460205\n",
      "Epoch 3862/30000 Training Loss: 0.07940436899662018\n",
      "Epoch 3863/30000 Training Loss: 0.051313336938619614\n",
      "Epoch 3864/30000 Training Loss: 0.07001049816608429\n",
      "Epoch 3865/30000 Training Loss: 0.056675203144550323\n",
      "Epoch 3866/30000 Training Loss: 0.06504012644290924\n",
      "Epoch 3867/30000 Training Loss: 0.0483391135931015\n",
      "Epoch 3868/30000 Training Loss: 0.055627524852752686\n",
      "Epoch 3869/30000 Training Loss: 0.05736439302563667\n",
      "Epoch 3870/30000 Training Loss: 0.05633342266082764\n",
      "Epoch 3871/30000 Training Loss: 0.046203337609767914\n",
      "Epoch 3872/30000 Training Loss: 0.04344417154788971\n",
      "Epoch 3873/30000 Training Loss: 0.06674415618181229\n",
      "Epoch 3874/30000 Training Loss: 0.06931793689727783\n",
      "Epoch 3875/30000 Training Loss: 0.054338548332452774\n",
      "Epoch 3876/30000 Training Loss: 0.06536591798067093\n",
      "Epoch 3877/30000 Training Loss: 0.05173461139202118\n",
      "Epoch 3878/30000 Training Loss: 0.06058238446712494\n",
      "Epoch 3879/30000 Training Loss: 0.04751599580049515\n",
      "Epoch 3880/30000 Training Loss: 0.07648065686225891\n",
      "Epoch 3881/30000 Training Loss: 0.0661853775382042\n",
      "Epoch 3882/30000 Training Loss: 0.057810038328170776\n",
      "Epoch 3883/30000 Training Loss: 0.06527281552553177\n",
      "Epoch 3884/30000 Training Loss: 0.05287669971585274\n",
      "Epoch 3885/30000 Training Loss: 0.06951471418142319\n",
      "Epoch 3886/30000 Training Loss: 0.06705307960510254\n",
      "Epoch 3887/30000 Training Loss: 0.07050198316574097\n",
      "Epoch 3888/30000 Training Loss: 0.055154599249362946\n",
      "Epoch 3889/30000 Training Loss: 0.07495181262493134\n",
      "Epoch 3890/30000 Training Loss: 0.06539570540189743\n",
      "Epoch 3891/30000 Training Loss: 0.05292278900742531\n",
      "Epoch 3892/30000 Training Loss: 0.05734359100461006\n",
      "Epoch 3893/30000 Training Loss: 0.06950866430997849\n",
      "Epoch 3894/30000 Training Loss: 0.060040183365345\n",
      "Epoch 3895/30000 Training Loss: 0.05151506885886192\n",
      "Epoch 3896/30000 Training Loss: 0.07332439720630646\n",
      "Epoch 3897/30000 Training Loss: 0.06826187670230865\n",
      "Epoch 3898/30000 Training Loss: 0.047926127910614014\n",
      "Epoch 3899/30000 Training Loss: 0.07642783224582672\n",
      "Epoch 3900/30000 Training Loss: 0.060057997703552246\n",
      "Epoch 3900/30000 Validation Loss: 0.0651385709643364\n",
      "Epoch 3901/30000 Training Loss: 0.0450490266084671\n",
      "Epoch 3902/30000 Training Loss: 0.073293037712574\n",
      "Epoch 3903/30000 Training Loss: 0.06051549315452576\n",
      "Epoch 3904/30000 Training Loss: 0.06790623068809509\n",
      "Epoch 3905/30000 Training Loss: 0.0500272735953331\n",
      "Epoch 3906/30000 Training Loss: 0.07112321257591248\n",
      "Epoch 3907/30000 Training Loss: 0.059413738548755646\n",
      "Epoch 3908/30000 Training Loss: 0.06898055970668793\n",
      "Epoch 3909/30000 Training Loss: 0.055157292634248734\n",
      "Epoch 3910/30000 Training Loss: 0.04585552215576172\n",
      "Epoch 3911/30000 Training Loss: 0.06016834080219269\n",
      "Epoch 3912/30000 Training Loss: 0.05392465740442276\n",
      "Epoch 3913/30000 Training Loss: 0.05882447957992554\n",
      "Epoch 3914/30000 Training Loss: 0.05071377754211426\n",
      "Epoch 3915/30000 Training Loss: 0.05312911421060562\n",
      "Epoch 3916/30000 Training Loss: 0.06936711817979813\n",
      "Epoch 3917/30000 Training Loss: 0.053250327706336975\n",
      "Epoch 3918/30000 Training Loss: 0.053925544023513794\n",
      "Epoch 3919/30000 Training Loss: 0.04714353382587433\n",
      "Epoch 3920/30000 Training Loss: 0.06499370187520981\n",
      "Epoch 3921/30000 Training Loss: 0.06873977184295654\n",
      "Epoch 3922/30000 Training Loss: 0.07037588208913803\n",
      "Epoch 3923/30000 Training Loss: 0.04764628782868385\n",
      "Epoch 3924/30000 Training Loss: 0.05336904525756836\n",
      "Epoch 3925/30000 Training Loss: 0.06706620007753372\n",
      "Epoch 3926/30000 Training Loss: 0.06073274463415146\n",
      "Epoch 3927/30000 Training Loss: 0.05930476635694504\n",
      "Epoch 3928/30000 Training Loss: 0.07339368760585785\n",
      "Epoch 3929/30000 Training Loss: 0.06089574843645096\n",
      "Epoch 3930/30000 Training Loss: 0.052896175533533096\n",
      "Epoch 3931/30000 Training Loss: 0.05196136236190796\n",
      "Epoch 3932/30000 Training Loss: 0.06230034679174423\n",
      "Epoch 3933/30000 Training Loss: 0.05847354233264923\n",
      "Epoch 3934/30000 Training Loss: 0.06528826802968979\n",
      "Epoch 3935/30000 Training Loss: 0.06343725323677063\n",
      "Epoch 3936/30000 Training Loss: 0.07421571761369705\n",
      "Epoch 3937/30000 Training Loss: 0.07707968354225159\n",
      "Epoch 3938/30000 Training Loss: 0.056565217673778534\n",
      "Epoch 3939/30000 Training Loss: 0.06266292184591293\n",
      "Epoch 3940/30000 Training Loss: 0.047856882214546204\n",
      "Epoch 3941/30000 Training Loss: 0.05064374953508377\n",
      "Epoch 3942/30000 Training Loss: 0.06374576687812805\n",
      "Epoch 3943/30000 Training Loss: 0.04983782023191452\n",
      "Epoch 3944/30000 Training Loss: 0.05861751735210419\n",
      "Epoch 3945/30000 Training Loss: 0.04680998623371124\n",
      "Epoch 3946/30000 Training Loss: 0.06599589437246323\n",
      "Epoch 3947/30000 Training Loss: 0.059154875576496124\n",
      "Epoch 3948/30000 Training Loss: 0.04939844459295273\n",
      "Epoch 3949/30000 Training Loss: 0.06747979670763016\n",
      "Epoch 3950/30000 Training Loss: 0.05979437008500099\n",
      "Epoch 3951/30000 Training Loss: 0.043168433010578156\n",
      "Epoch 3952/30000 Training Loss: 0.04098612070083618\n",
      "Epoch 3953/30000 Training Loss: 0.04675975814461708\n",
      "Epoch 3954/30000 Training Loss: 0.072868712246418\n",
      "Epoch 3955/30000 Training Loss: 0.06847377121448517\n",
      "Epoch 3956/30000 Training Loss: 0.0632576122879982\n",
      "Epoch 3957/30000 Training Loss: 0.0741262435913086\n",
      "Epoch 3958/30000 Training Loss: 0.05688207596540451\n",
      "Epoch 3959/30000 Training Loss: 0.062099747359752655\n",
      "Epoch 3960/30000 Training Loss: 0.06847642362117767\n",
      "Epoch 3961/30000 Training Loss: 0.0674092099070549\n",
      "Epoch 3962/30000 Training Loss: 0.06649759411811829\n",
      "Epoch 3963/30000 Training Loss: 0.049308400601148605\n",
      "Epoch 3964/30000 Training Loss: 0.06211426854133606\n",
      "Epoch 3965/30000 Training Loss: 0.051509782671928406\n",
      "Epoch 3966/30000 Training Loss: 0.06402841955423355\n",
      "Epoch 3967/30000 Training Loss: 0.056737206876277924\n",
      "Epoch 3968/30000 Training Loss: 0.06423722207546234\n",
      "Epoch 3969/30000 Training Loss: 0.07020328938961029\n",
      "Epoch 3970/30000 Training Loss: 0.0553596168756485\n",
      "Epoch 3971/30000 Training Loss: 0.051842592656612396\n",
      "Epoch 3972/30000 Training Loss: 0.06013672798871994\n",
      "Epoch 3973/30000 Training Loss: 0.06656625121831894\n",
      "Epoch 3974/30000 Training Loss: 0.06149756908416748\n",
      "Epoch 3975/30000 Training Loss: 0.06165711581707001\n",
      "Epoch 3976/30000 Training Loss: 0.06240455061197281\n",
      "Epoch 3977/30000 Training Loss: 0.06462313234806061\n",
      "Epoch 3978/30000 Training Loss: 0.05504314601421356\n",
      "Epoch 3979/30000 Training Loss: 0.057954706251621246\n",
      "Epoch 3980/30000 Training Loss: 0.058395594358444214\n",
      "Epoch 3981/30000 Training Loss: 0.04774019494652748\n",
      "Epoch 3982/30000 Training Loss: 0.05712480843067169\n",
      "Epoch 3983/30000 Training Loss: 0.06771691143512726\n",
      "Epoch 3984/30000 Training Loss: 0.06495401263237\n",
      "Epoch 3985/30000 Training Loss: 0.0481809601187706\n",
      "Epoch 3986/30000 Training Loss: 0.05826568603515625\n",
      "Epoch 3987/30000 Training Loss: 0.06636731326580048\n",
      "Epoch 3988/30000 Training Loss: 0.06955790519714355\n",
      "Epoch 3989/30000 Training Loss: 0.07093968987464905\n",
      "Epoch 3990/30000 Training Loss: 0.054707176983356476\n",
      "Epoch 3991/30000 Training Loss: 0.055482130497694016\n",
      "Epoch 3992/30000 Training Loss: 0.061586469411849976\n",
      "Epoch 3993/30000 Training Loss: 0.06222546100616455\n",
      "Epoch 3994/30000 Training Loss: 0.043996766209602356\n",
      "Epoch 3995/30000 Training Loss: 0.058534394949674606\n",
      "Epoch 3996/30000 Training Loss: 0.05939807742834091\n",
      "Epoch 3997/30000 Training Loss: 0.04896251857280731\n",
      "Epoch 3998/30000 Training Loss: 0.0555013008415699\n",
      "Epoch 3999/30000 Training Loss: 0.07144224643707275\n",
      "Epoch 4000/30000 Training Loss: 0.056561946868896484\n",
      "Epoch 4000/30000 Validation Loss: 0.05116011202335358\n",
      "Epoch 4001/30000 Training Loss: 0.044670913368463516\n",
      "Epoch 4002/30000 Training Loss: 0.04942324757575989\n",
      "Epoch 4003/30000 Training Loss: 0.07700054347515106\n",
      "Epoch 4004/30000 Training Loss: 0.0652175173163414\n",
      "Epoch 4005/30000 Training Loss: 0.0514526441693306\n",
      "Epoch 4006/30000 Training Loss: 0.05539616942405701\n",
      "Epoch 4007/30000 Training Loss: 0.05783340334892273\n",
      "Epoch 4008/30000 Training Loss: 0.06072971969842911\n",
      "Epoch 4009/30000 Training Loss: 0.06827519834041595\n",
      "Epoch 4010/30000 Training Loss: 0.06407734006643295\n",
      "Epoch 4011/30000 Training Loss: 0.03973149508237839\n",
      "Epoch 4012/30000 Training Loss: 0.051952116191387177\n",
      "Epoch 4013/30000 Training Loss: 0.05100935325026512\n",
      "Epoch 4014/30000 Training Loss: 0.054950423538684845\n",
      "Epoch 4015/30000 Training Loss: 0.0790894404053688\n",
      "Epoch 4016/30000 Training Loss: 0.04678382724523544\n",
      "Epoch 4017/30000 Training Loss: 0.07359527796506882\n",
      "Epoch 4018/30000 Training Loss: 0.05630020797252655\n",
      "Epoch 4019/30000 Training Loss: 0.04715082049369812\n",
      "Epoch 4020/30000 Training Loss: 0.05298342555761337\n",
      "Epoch 4021/30000 Training Loss: 0.06678806990385056\n",
      "Epoch 4022/30000 Training Loss: 0.05800734460353851\n",
      "Epoch 4023/30000 Training Loss: 0.04974013566970825\n",
      "Epoch 4024/30000 Training Loss: 0.05687922239303589\n",
      "Epoch 4025/30000 Training Loss: 0.055186476558446884\n",
      "Epoch 4026/30000 Training Loss: 0.07417169958353043\n",
      "Epoch 4027/30000 Training Loss: 0.07346761226654053\n",
      "Epoch 4028/30000 Training Loss: 0.0487055629491806\n",
      "Epoch 4029/30000 Training Loss: 0.04825238883495331\n",
      "Epoch 4030/30000 Training Loss: 0.053397081792354584\n",
      "Epoch 4031/30000 Training Loss: 0.0656733363866806\n",
      "Epoch 4032/30000 Training Loss: 0.052864618599414825\n",
      "Epoch 4033/30000 Training Loss: 0.05956956744194031\n",
      "Epoch 4034/30000 Training Loss: 0.05475347489118576\n",
      "Epoch 4035/30000 Training Loss: 0.060985811054706573\n",
      "Epoch 4036/30000 Training Loss: 0.06179294362664223\n",
      "Epoch 4037/30000 Training Loss: 0.05652984231710434\n",
      "Epoch 4038/30000 Training Loss: 0.06561550498008728\n",
      "Epoch 4039/30000 Training Loss: 0.0537828728556633\n",
      "Epoch 4040/30000 Training Loss: 0.05202708765864372\n",
      "Epoch 4041/30000 Training Loss: 0.07294486463069916\n",
      "Epoch 4042/30000 Training Loss: 0.06731608510017395\n",
      "Epoch 4043/30000 Training Loss: 0.06345365196466446\n",
      "Epoch 4044/30000 Training Loss: 0.06836142390966415\n",
      "Epoch 4045/30000 Training Loss: 0.041783906519412994\n",
      "Epoch 4046/30000 Training Loss: 0.06849947571754456\n",
      "Epoch 4047/30000 Training Loss: 0.059392258524894714\n",
      "Epoch 4048/30000 Training Loss: 0.05812343209981918\n",
      "Epoch 4049/30000 Training Loss: 0.0529651939868927\n",
      "Epoch 4050/30000 Training Loss: 0.06057429313659668\n",
      "Epoch 4051/30000 Training Loss: 0.06283033639192581\n",
      "Epoch 4052/30000 Training Loss: 0.05383249372243881\n",
      "Epoch 4053/30000 Training Loss: 0.04826612398028374\n",
      "Epoch 4054/30000 Training Loss: 0.04795385152101517\n",
      "Epoch 4055/30000 Training Loss: 0.05598577857017517\n",
      "Epoch 4056/30000 Training Loss: 0.0637860968708992\n",
      "Epoch 4057/30000 Training Loss: 0.057578541338443756\n",
      "Epoch 4058/30000 Training Loss: 0.06123592332005501\n",
      "Epoch 4059/30000 Training Loss: 0.06485679745674133\n",
      "Epoch 4060/30000 Training Loss: 0.08048854768276215\n",
      "Epoch 4061/30000 Training Loss: 0.05130719020962715\n",
      "Epoch 4062/30000 Training Loss: 0.06009533256292343\n",
      "Epoch 4063/30000 Training Loss: 0.07138065993785858\n",
      "Epoch 4064/30000 Training Loss: 0.04385841637849808\n",
      "Epoch 4065/30000 Training Loss: 0.05017987638711929\n",
      "Epoch 4066/30000 Training Loss: 0.04876285791397095\n",
      "Epoch 4067/30000 Training Loss: 0.04525499790906906\n",
      "Epoch 4068/30000 Training Loss: 0.05108349025249481\n",
      "Epoch 4069/30000 Training Loss: 0.05386289209127426\n",
      "Epoch 4070/30000 Training Loss: 0.055160120129585266\n",
      "Epoch 4071/30000 Training Loss: 0.05271279439330101\n",
      "Epoch 4072/30000 Training Loss: 0.05853363126516342\n",
      "Epoch 4073/30000 Training Loss: 0.061990994960069656\n",
      "Epoch 4074/30000 Training Loss: 0.06713055819272995\n",
      "Epoch 4075/30000 Training Loss: 0.0544809028506279\n",
      "Epoch 4076/30000 Training Loss: 0.06531772017478943\n",
      "Epoch 4077/30000 Training Loss: 0.06673520803451538\n",
      "Epoch 4078/30000 Training Loss: 0.049347203224897385\n",
      "Epoch 4079/30000 Training Loss: 0.06627511978149414\n",
      "Epoch 4080/30000 Training Loss: 0.06327740848064423\n",
      "Epoch 4081/30000 Training Loss: 0.04543033987283707\n",
      "Epoch 4082/30000 Training Loss: 0.07603606581687927\n",
      "Epoch 4083/30000 Training Loss: 0.049598321318626404\n",
      "Epoch 4084/30000 Training Loss: 0.06605198979377747\n",
      "Epoch 4085/30000 Training Loss: 0.05782962217926979\n",
      "Epoch 4086/30000 Training Loss: 0.06512034684419632\n",
      "Epoch 4087/30000 Training Loss: 0.059170082211494446\n",
      "Epoch 4088/30000 Training Loss: 0.06645490229129791\n",
      "Epoch 4089/30000 Training Loss: 0.05994682013988495\n",
      "Epoch 4090/30000 Training Loss: 0.0507962591946125\n",
      "Epoch 4091/30000 Training Loss: 0.06332911550998688\n",
      "Epoch 4092/30000 Training Loss: 0.06513085961341858\n",
      "Epoch 4093/30000 Training Loss: 0.04983457177877426\n",
      "Epoch 4094/30000 Training Loss: 0.06091158092021942\n",
      "Epoch 4095/30000 Training Loss: 0.06302861869335175\n",
      "Epoch 4096/30000 Training Loss: 0.05704129859805107\n",
      "Epoch 4097/30000 Training Loss: 0.05786019191145897\n",
      "Epoch 4098/30000 Training Loss: 0.06726137548685074\n",
      "Epoch 4099/30000 Training Loss: 0.05801505222916603\n",
      "Epoch 4100/30000 Training Loss: 0.0625583827495575\n",
      "Epoch 4100/30000 Validation Loss: 0.054712630808353424\n",
      "Epoch 4101/30000 Training Loss: 0.04664675146341324\n",
      "Epoch 4102/30000 Training Loss: 0.06596120446920395\n",
      "Epoch 4103/30000 Training Loss: 0.09512631595134735\n",
      "Epoch 4104/30000 Training Loss: 0.05734969675540924\n",
      "Epoch 4105/30000 Training Loss: 0.05859406292438507\n",
      "Epoch 4106/30000 Training Loss: 0.06931871175765991\n",
      "Epoch 4107/30000 Training Loss: 0.06351138651371002\n",
      "Epoch 4108/30000 Training Loss: 0.0661512091755867\n",
      "Epoch 4109/30000 Training Loss: 0.06514791399240494\n",
      "Epoch 4110/30000 Training Loss: 0.06251299381256104\n",
      "Epoch 4111/30000 Training Loss: 0.05825033783912659\n",
      "Epoch 4112/30000 Training Loss: 0.06548036634922028\n",
      "Epoch 4113/30000 Training Loss: 0.06234704703092575\n",
      "Epoch 4114/30000 Training Loss: 0.04558814316987991\n",
      "Epoch 4115/30000 Training Loss: 0.06062527745962143\n",
      "Epoch 4116/30000 Training Loss: 0.05108422785997391\n",
      "Epoch 4117/30000 Training Loss: 0.06420226395130157\n",
      "Epoch 4118/30000 Training Loss: 0.06351250410079956\n",
      "Epoch 4119/30000 Training Loss: 0.0468631312251091\n",
      "Epoch 4120/30000 Training Loss: 0.06411577761173248\n",
      "Epoch 4121/30000 Training Loss: 0.05933777615427971\n",
      "Epoch 4122/30000 Training Loss: 0.0753081887960434\n",
      "Epoch 4123/30000 Training Loss: 0.05986351519823074\n",
      "Epoch 4124/30000 Training Loss: 0.04120258241891861\n",
      "Epoch 4125/30000 Training Loss: 0.07008126378059387\n",
      "Epoch 4126/30000 Training Loss: 0.04524754360318184\n",
      "Epoch 4127/30000 Training Loss: 0.0623653382062912\n",
      "Epoch 4128/30000 Training Loss: 0.06662105023860931\n",
      "Epoch 4129/30000 Training Loss: 0.06791748851537704\n",
      "Epoch 4130/30000 Training Loss: 0.05395791679620743\n",
      "Epoch 4131/30000 Training Loss: 0.06508889049291611\n",
      "Epoch 4132/30000 Training Loss: 0.06767434626817703\n",
      "Epoch 4133/30000 Training Loss: 0.0582568384706974\n",
      "Epoch 4134/30000 Training Loss: 0.064842090010643\n",
      "Epoch 4135/30000 Training Loss: 0.07671183347702026\n",
      "Epoch 4136/30000 Training Loss: 0.047886233776807785\n",
      "Epoch 4137/30000 Training Loss: 0.08356211334466934\n",
      "Epoch 4138/30000 Training Loss: 0.05989403277635574\n",
      "Epoch 4139/30000 Training Loss: 0.06542079895734787\n",
      "Epoch 4140/30000 Training Loss: 0.06329017877578735\n",
      "Epoch 4141/30000 Training Loss: 0.05150750279426575\n",
      "Epoch 4142/30000 Training Loss: 0.05100993812084198\n",
      "Epoch 4143/30000 Training Loss: 0.07695850729942322\n",
      "Epoch 4144/30000 Training Loss: 0.0516979917883873\n",
      "Epoch 4145/30000 Training Loss: 0.05601920187473297\n",
      "Epoch 4146/30000 Training Loss: 0.05640733987092972\n",
      "Epoch 4147/30000 Training Loss: 0.06308010965585709\n",
      "Epoch 4148/30000 Training Loss: 0.07333585619926453\n",
      "Epoch 4149/30000 Training Loss: 0.049997277557849884\n",
      "Epoch 4150/30000 Training Loss: 0.05883530527353287\n",
      "Epoch 4151/30000 Training Loss: 0.0571141242980957\n",
      "Epoch 4152/30000 Training Loss: 0.058782197535037994\n",
      "Epoch 4153/30000 Training Loss: 0.05500795319676399\n",
      "Epoch 4154/30000 Training Loss: 0.05171813443303108\n",
      "Epoch 4155/30000 Training Loss: 0.05011509731411934\n",
      "Epoch 4156/30000 Training Loss: 0.06811611354351044\n",
      "Epoch 4157/30000 Training Loss: 0.0643332302570343\n",
      "Epoch 4158/30000 Training Loss: 0.06390015780925751\n",
      "Epoch 4159/30000 Training Loss: 0.048520151525735855\n",
      "Epoch 4160/30000 Training Loss: 0.06725582480430603\n",
      "Epoch 4161/30000 Training Loss: 0.07174919545650482\n",
      "Epoch 4162/30000 Training Loss: 0.053691454231739044\n",
      "Epoch 4163/30000 Training Loss: 0.06455743312835693\n",
      "Epoch 4164/30000 Training Loss: 0.06850037723779678\n",
      "Epoch 4165/30000 Training Loss: 0.05958685278892517\n",
      "Epoch 4166/30000 Training Loss: 0.04456432908773422\n",
      "Epoch 4167/30000 Training Loss: 0.06471195816993713\n",
      "Epoch 4168/30000 Training Loss: 0.046845223754644394\n",
      "Epoch 4169/30000 Training Loss: 0.05829720199108124\n",
      "Epoch 4170/30000 Training Loss: 0.06945354491472244\n",
      "Epoch 4171/30000 Training Loss: 0.0707915723323822\n",
      "Epoch 4172/30000 Training Loss: 0.06515766680240631\n",
      "Epoch 4173/30000 Training Loss: 0.06125561147928238\n",
      "Epoch 4174/30000 Training Loss: 0.05933196470141411\n",
      "Epoch 4175/30000 Training Loss: 0.05011582374572754\n",
      "Epoch 4176/30000 Training Loss: 0.0643109530210495\n",
      "Epoch 4177/30000 Training Loss: 0.05821599066257477\n",
      "Epoch 4178/30000 Training Loss: 0.0531434640288353\n",
      "Epoch 4179/30000 Training Loss: 0.04635366052389145\n",
      "Epoch 4180/30000 Training Loss: 0.06897653639316559\n",
      "Epoch 4181/30000 Training Loss: 0.06924240291118622\n",
      "Epoch 4182/30000 Training Loss: 0.04455120116472244\n",
      "Epoch 4183/30000 Training Loss: 0.03972755745053291\n",
      "Epoch 4184/30000 Training Loss: 0.0549808070063591\n",
      "Epoch 4185/30000 Training Loss: 0.07037679851055145\n",
      "Epoch 4186/30000 Training Loss: 0.049651384353637695\n",
      "Epoch 4187/30000 Training Loss: 0.04474613070487976\n",
      "Epoch 4188/30000 Training Loss: 0.05353474244475365\n",
      "Epoch 4189/30000 Training Loss: 0.0728389099240303\n",
      "Epoch 4190/30000 Training Loss: 0.08011378347873688\n",
      "Epoch 4191/30000 Training Loss: 0.05538706108927727\n",
      "Epoch 4192/30000 Training Loss: 0.04952327162027359\n",
      "Epoch 4193/30000 Training Loss: 0.04813480004668236\n",
      "Epoch 4194/30000 Training Loss: 0.044177256524562836\n",
      "Epoch 4195/30000 Training Loss: 0.06844079494476318\n",
      "Epoch 4196/30000 Training Loss: 0.059319332242012024\n",
      "Epoch 4197/30000 Training Loss: 0.07205784320831299\n",
      "Epoch 4198/30000 Training Loss: 0.0670943558216095\n",
      "Epoch 4199/30000 Training Loss: 0.057579804211854935\n",
      "Epoch 4200/30000 Training Loss: 0.04838607460260391\n",
      "Epoch 4200/30000 Validation Loss: 0.05382248014211655\n",
      "Epoch 4201/30000 Training Loss: 0.050836190581321716\n",
      "Epoch 4202/30000 Training Loss: 0.06170382350683212\n",
      "Epoch 4203/30000 Training Loss: 0.04916404187679291\n",
      "Epoch 4204/30000 Training Loss: 0.047697022557258606\n",
      "Epoch 4205/30000 Training Loss: 0.046478740870952606\n",
      "Epoch 4206/30000 Training Loss: 0.06340210139751434\n",
      "Epoch 4207/30000 Training Loss: 0.05344183370471001\n",
      "Epoch 4208/30000 Training Loss: 0.06029120832681656\n",
      "Epoch 4209/30000 Training Loss: 0.06420984864234924\n",
      "Epoch 4210/30000 Training Loss: 0.05710172653198242\n",
      "Epoch 4211/30000 Training Loss: 0.05758308991789818\n",
      "Epoch 4212/30000 Training Loss: 0.061155304312705994\n",
      "Epoch 4213/30000 Training Loss: 0.05668559670448303\n",
      "Epoch 4214/30000 Training Loss: 0.05335156247019768\n",
      "Epoch 4215/30000 Training Loss: 0.061518579721450806\n",
      "Epoch 4216/30000 Training Loss: 0.04806090146303177\n",
      "Epoch 4217/30000 Training Loss: 0.06018804758787155\n",
      "Epoch 4218/30000 Training Loss: 0.058665867894887924\n",
      "Epoch 4219/30000 Training Loss: 0.0706963837146759\n",
      "Epoch 4220/30000 Training Loss: 0.06284917145967484\n",
      "Epoch 4221/30000 Training Loss: 0.06600349396467209\n",
      "Epoch 4222/30000 Training Loss: 0.072580486536026\n",
      "Epoch 4223/30000 Training Loss: 0.0575011745095253\n",
      "Epoch 4224/30000 Training Loss: 0.03756987303495407\n",
      "Epoch 4225/30000 Training Loss: 0.06195858120918274\n",
      "Epoch 4226/30000 Training Loss: 0.0543096587061882\n",
      "Epoch 4227/30000 Training Loss: 0.04991741478443146\n",
      "Epoch 4228/30000 Training Loss: 0.07300741970539093\n",
      "Epoch 4229/30000 Training Loss: 0.05800290405750275\n",
      "Epoch 4230/30000 Training Loss: 0.05733538419008255\n",
      "Epoch 4231/30000 Training Loss: 0.05371016263961792\n",
      "Epoch 4232/30000 Training Loss: 0.05599043145775795\n",
      "Epoch 4233/30000 Training Loss: 0.06274633854627609\n",
      "Epoch 4234/30000 Training Loss: 0.06199422478675842\n",
      "Epoch 4235/30000 Training Loss: 0.05290926247835159\n",
      "Epoch 4236/30000 Training Loss: 0.06356817483901978\n",
      "Epoch 4237/30000 Training Loss: 0.058933719992637634\n",
      "Epoch 4238/30000 Training Loss: 0.04754873365163803\n",
      "Epoch 4239/30000 Training Loss: 0.061665575951337814\n",
      "Epoch 4240/30000 Training Loss: 0.059281669557094574\n",
      "Epoch 4241/30000 Training Loss: 0.065800741314888\n",
      "Epoch 4242/30000 Training Loss: 0.06263252347707748\n",
      "Epoch 4243/30000 Training Loss: 0.06723497062921524\n",
      "Epoch 4244/30000 Training Loss: 0.05473483353853226\n",
      "Epoch 4245/30000 Training Loss: 0.05591011047363281\n",
      "Epoch 4246/30000 Training Loss: 0.052985455840826035\n",
      "Epoch 4247/30000 Training Loss: 0.050840020179748535\n",
      "Epoch 4248/30000 Training Loss: 0.06390423327684402\n",
      "Epoch 4249/30000 Training Loss: 0.0605582669377327\n",
      "Epoch 4250/30000 Training Loss: 0.06187601760029793\n",
      "Epoch 4251/30000 Training Loss: 0.051529042422771454\n",
      "Epoch 4252/30000 Training Loss: 0.05304791033267975\n",
      "Epoch 4253/30000 Training Loss: 0.053462814539670944\n",
      "Epoch 4254/30000 Training Loss: 0.05659695342183113\n",
      "Epoch 4255/30000 Training Loss: 0.06562655419111252\n",
      "Epoch 4256/30000 Training Loss: 0.07208167761564255\n",
      "Epoch 4257/30000 Training Loss: 0.04483281821012497\n",
      "Epoch 4258/30000 Training Loss: 0.053592272102832794\n",
      "Epoch 4259/30000 Training Loss: 0.049931950867176056\n",
      "Epoch 4260/30000 Training Loss: 0.05903810262680054\n",
      "Epoch 4261/30000 Training Loss: 0.051328547298908234\n",
      "Epoch 4262/30000 Training Loss: 0.06127024441957474\n",
      "Epoch 4263/30000 Training Loss: 0.053884267807006836\n",
      "Epoch 4264/30000 Training Loss: 0.06809203326702118\n",
      "Epoch 4265/30000 Training Loss: 0.059855952858924866\n",
      "Epoch 4266/30000 Training Loss: 0.05630486086010933\n",
      "Epoch 4267/30000 Training Loss: 0.07236849516630173\n",
      "Epoch 4268/30000 Training Loss: 0.05901923030614853\n",
      "Epoch 4269/30000 Training Loss: 0.05643826723098755\n",
      "Epoch 4270/30000 Training Loss: 0.06753120571374893\n",
      "Epoch 4271/30000 Training Loss: 0.0499153658747673\n",
      "Epoch 4272/30000 Training Loss: 0.049779389053583145\n",
      "Epoch 4273/30000 Training Loss: 0.050734587013721466\n",
      "Epoch 4274/30000 Training Loss: 0.05497601628303528\n",
      "Epoch 4275/30000 Training Loss: 0.052364449948072433\n",
      "Epoch 4276/30000 Training Loss: 0.04953697696328163\n",
      "Epoch 4277/30000 Training Loss: 0.07596111297607422\n",
      "Epoch 4278/30000 Training Loss: 0.05740121379494667\n",
      "Epoch 4279/30000 Training Loss: 0.045736998319625854\n",
      "Epoch 4280/30000 Training Loss: 0.05369812250137329\n",
      "Epoch 4281/30000 Training Loss: 0.04888572543859482\n",
      "Epoch 4282/30000 Training Loss: 0.055510759353637695\n",
      "Epoch 4283/30000 Training Loss: 0.06268617510795593\n",
      "Epoch 4284/30000 Training Loss: 0.056403763592243195\n",
      "Epoch 4285/30000 Training Loss: 0.0613308846950531\n",
      "Epoch 4286/30000 Training Loss: 0.04653023183345795\n",
      "Epoch 4287/30000 Training Loss: 0.05680060759186745\n",
      "Epoch 4288/30000 Training Loss: 0.05829918384552002\n",
      "Epoch 4289/30000 Training Loss: 0.06086970120668411\n",
      "Epoch 4290/30000 Training Loss: 0.0671241357922554\n",
      "Epoch 4291/30000 Training Loss: 0.040078967809677124\n",
      "Epoch 4292/30000 Training Loss: 0.05178452283143997\n",
      "Epoch 4293/30000 Training Loss: 0.06136948987841606\n",
      "Epoch 4294/30000 Training Loss: 0.059988267719745636\n",
      "Epoch 4295/30000 Training Loss: 0.05608704686164856\n",
      "Epoch 4296/30000 Training Loss: 0.06832177191972733\n",
      "Epoch 4297/30000 Training Loss: 0.051442861557006836\n",
      "Epoch 4298/30000 Training Loss: 0.072477325797081\n",
      "Epoch 4299/30000 Training Loss: 0.06052015721797943\n",
      "Epoch 4300/30000 Training Loss: 0.055550895631313324\n",
      "Epoch 4300/30000 Validation Loss: 0.06578561663627625\n",
      "Epoch 4301/30000 Training Loss: 0.048104315996170044\n",
      "Epoch 4302/30000 Training Loss: 0.07004863768815994\n",
      "Epoch 4303/30000 Training Loss: 0.05160687491297722\n",
      "Epoch 4304/30000 Training Loss: 0.0490209199488163\n",
      "Epoch 4305/30000 Training Loss: 0.0638081356883049\n",
      "Epoch 4306/30000 Training Loss: 0.05721975862979889\n",
      "Epoch 4307/30000 Training Loss: 0.05488697439432144\n",
      "Epoch 4308/30000 Training Loss: 0.043538495898246765\n",
      "Epoch 4309/30000 Training Loss: 0.057986367493867874\n",
      "Epoch 4310/30000 Training Loss: 0.06799840927124023\n",
      "Epoch 4311/30000 Training Loss: 0.05843905359506607\n",
      "Epoch 4312/30000 Training Loss: 0.061918970197439194\n",
      "Epoch 4313/30000 Training Loss: 0.05461585521697998\n",
      "Epoch 4314/30000 Training Loss: 0.059289947152137756\n",
      "Epoch 4315/30000 Training Loss: 0.05974298715591431\n",
      "Epoch 4316/30000 Training Loss: 0.07012586295604706\n",
      "Epoch 4317/30000 Training Loss: 0.0571611225605011\n",
      "Epoch 4318/30000 Training Loss: 0.060702867805957794\n",
      "Epoch 4319/30000 Training Loss: 0.06115759164094925\n",
      "Epoch 4320/30000 Training Loss: 0.05054468289017677\n",
      "Epoch 4321/30000 Training Loss: 0.05653361976146698\n",
      "Epoch 4322/30000 Training Loss: 0.05155149847269058\n",
      "Epoch 4323/30000 Training Loss: 0.060615718364715576\n",
      "Epoch 4324/30000 Training Loss: 0.06573157757520676\n",
      "Epoch 4325/30000 Training Loss: 0.05031280219554901\n",
      "Epoch 4326/30000 Training Loss: 0.05323837697505951\n",
      "Epoch 4327/30000 Training Loss: 0.07867303490638733\n",
      "Epoch 4328/30000 Training Loss: 0.05971977114677429\n",
      "Epoch 4329/30000 Training Loss: 0.05161966383457184\n",
      "Epoch 4330/30000 Training Loss: 0.06428144127130508\n",
      "Epoch 4331/30000 Training Loss: 0.07435107976198196\n",
      "Epoch 4332/30000 Training Loss: 0.058146338909864426\n",
      "Epoch 4333/30000 Training Loss: 0.04964347928762436\n",
      "Epoch 4334/30000 Training Loss: 0.0563824325799942\n",
      "Epoch 4335/30000 Training Loss: 0.051919810473918915\n",
      "Epoch 4336/30000 Training Loss: 0.05351030081510544\n",
      "Epoch 4337/30000 Training Loss: 0.054020121693611145\n",
      "Epoch 4338/30000 Training Loss: 0.08423305302858353\n",
      "Epoch 4339/30000 Training Loss: 0.044230662286281586\n",
      "Epoch 4340/30000 Training Loss: 0.06676578521728516\n",
      "Epoch 4341/30000 Training Loss: 0.05695167928934097\n",
      "Epoch 4342/30000 Training Loss: 0.06529530882835388\n",
      "Epoch 4343/30000 Training Loss: 0.05991252511739731\n",
      "Epoch 4344/30000 Training Loss: 0.05959978699684143\n",
      "Epoch 4345/30000 Training Loss: 0.06228414922952652\n",
      "Epoch 4346/30000 Training Loss: 0.07571925222873688\n",
      "Epoch 4347/30000 Training Loss: 0.049442291259765625\n",
      "Epoch 4348/30000 Training Loss: 0.04622511565685272\n",
      "Epoch 4349/30000 Training Loss: 0.05890437215566635\n",
      "Epoch 4350/30000 Training Loss: 0.06508275866508484\n",
      "Epoch 4351/30000 Training Loss: 0.045731861144304276\n",
      "Epoch 4352/30000 Training Loss: 0.06249400973320007\n",
      "Epoch 4353/30000 Training Loss: 0.05881774425506592\n",
      "Epoch 4354/30000 Training Loss: 0.060913726687431335\n",
      "Epoch 4355/30000 Training Loss: 0.05298272892832756\n",
      "Epoch 4356/30000 Training Loss: 0.04439245164394379\n",
      "Epoch 4357/30000 Training Loss: 0.06093671917915344\n",
      "Epoch 4358/30000 Training Loss: 0.055992159992456436\n",
      "Epoch 4359/30000 Training Loss: 0.055711779743433\n",
      "Epoch 4360/30000 Training Loss: 0.054432764649391174\n",
      "Epoch 4361/30000 Training Loss: 0.056156255304813385\n",
      "Epoch 4362/30000 Training Loss: 0.04686521366238594\n",
      "Epoch 4363/30000 Training Loss: 0.06315986812114716\n",
      "Epoch 4364/30000 Training Loss: 0.050226546823978424\n",
      "Epoch 4365/30000 Training Loss: 0.04988094046711922\n",
      "Epoch 4366/30000 Training Loss: 0.05140484496951103\n",
      "Epoch 4367/30000 Training Loss: 0.05370013415813446\n",
      "Epoch 4368/30000 Training Loss: 0.057858288288116455\n",
      "Epoch 4369/30000 Training Loss: 0.07420546561479568\n",
      "Epoch 4370/30000 Training Loss: 0.05299067497253418\n",
      "Epoch 4371/30000 Training Loss: 0.06301191449165344\n",
      "Epoch 4372/30000 Training Loss: 0.05640614777803421\n",
      "Epoch 4373/30000 Training Loss: 0.0630795955657959\n",
      "Epoch 4374/30000 Training Loss: 0.05942171812057495\n",
      "Epoch 4375/30000 Training Loss: 0.055085521191358566\n",
      "Epoch 4376/30000 Training Loss: 0.052913419902324677\n",
      "Epoch 4377/30000 Training Loss: 0.04193711280822754\n",
      "Epoch 4378/30000 Training Loss: 0.06978807598352432\n",
      "Epoch 4379/30000 Training Loss: 0.05976766347885132\n",
      "Epoch 4380/30000 Training Loss: 0.06665052473545074\n",
      "Epoch 4381/30000 Training Loss: 0.0634254664182663\n",
      "Epoch 4382/30000 Training Loss: 0.06414233148097992\n",
      "Epoch 4383/30000 Training Loss: 0.0706588625907898\n",
      "Epoch 4384/30000 Training Loss: 0.06656590849161148\n",
      "Epoch 4385/30000 Training Loss: 0.0471901074051857\n",
      "Epoch 4386/30000 Training Loss: 0.05689838528633118\n",
      "Epoch 4387/30000 Training Loss: 0.07959190011024475\n",
      "Epoch 4388/30000 Training Loss: 0.06068282574415207\n",
      "Epoch 4389/30000 Training Loss: 0.05167178064584732\n",
      "Epoch 4390/30000 Training Loss: 0.05220840126276016\n",
      "Epoch 4391/30000 Training Loss: 0.061054080724716187\n",
      "Epoch 4392/30000 Training Loss: 0.04561817646026611\n",
      "Epoch 4393/30000 Training Loss: 0.04409123957157135\n",
      "Epoch 4394/30000 Training Loss: 0.05810820311307907\n",
      "Epoch 4395/30000 Training Loss: 0.07509184628725052\n",
      "Epoch 4396/30000 Training Loss: 0.06219853460788727\n",
      "Epoch 4397/30000 Training Loss: 0.049876779317855835\n",
      "Epoch 4398/30000 Training Loss: 0.04351755976676941\n",
      "Epoch 4399/30000 Training Loss: 0.06264621764421463\n",
      "Epoch 4400/30000 Training Loss: 0.06321902573108673\n",
      "Epoch 4400/30000 Validation Loss: 0.07468648254871368\n",
      "Epoch 4401/30000 Training Loss: 0.05846814066171646\n",
      "Epoch 4402/30000 Training Loss: 0.06039203703403473\n",
      "Epoch 4403/30000 Training Loss: 0.05629388242959976\n",
      "Epoch 4404/30000 Training Loss: 0.06093699112534523\n",
      "Epoch 4405/30000 Training Loss: 0.0566728450357914\n",
      "Epoch 4406/30000 Training Loss: 0.04328278452157974\n",
      "Epoch 4407/30000 Training Loss: 0.06009583920240402\n",
      "Epoch 4408/30000 Training Loss: 0.047378137707710266\n",
      "Epoch 4409/30000 Training Loss: 0.05079355090856552\n",
      "Epoch 4410/30000 Training Loss: 0.05373392999172211\n",
      "Epoch 4411/30000 Training Loss: 0.05513136833906174\n",
      "Epoch 4412/30000 Training Loss: 0.056817807257175446\n",
      "Epoch 4413/30000 Training Loss: 0.06949771195650101\n",
      "Epoch 4414/30000 Training Loss: 0.03894022852182388\n",
      "Epoch 4415/30000 Training Loss: 0.06310287117958069\n",
      "Epoch 4416/30000 Training Loss: 0.05935852974653244\n",
      "Epoch 4417/30000 Training Loss: 0.04976917803287506\n",
      "Epoch 4418/30000 Training Loss: 0.05812656879425049\n",
      "Epoch 4419/30000 Training Loss: 0.07181225717067719\n",
      "Epoch 4420/30000 Training Loss: 0.0517176017165184\n",
      "Epoch 4421/30000 Training Loss: 0.07964184135198593\n",
      "Epoch 4422/30000 Training Loss: 0.06377658247947693\n",
      "Epoch 4423/30000 Training Loss: 0.08279877156019211\n",
      "Epoch 4424/30000 Training Loss: 0.056247152388095856\n",
      "Epoch 4425/30000 Training Loss: 0.06796838343143463\n",
      "Epoch 4426/30000 Training Loss: 0.06355427205562592\n",
      "Epoch 4427/30000 Training Loss: 0.05495164170861244\n",
      "Epoch 4428/30000 Training Loss: 0.05548293888568878\n",
      "Epoch 4429/30000 Training Loss: 0.05704547464847565\n",
      "Epoch 4430/30000 Training Loss: 0.06357499957084656\n",
      "Epoch 4431/30000 Training Loss: 0.06679908186197281\n",
      "Epoch 4432/30000 Training Loss: 0.06931917369365692\n",
      "Epoch 4433/30000 Training Loss: 0.0698595866560936\n",
      "Epoch 4434/30000 Training Loss: 0.04324394837021828\n",
      "Epoch 4435/30000 Training Loss: 0.05655774846673012\n",
      "Epoch 4436/30000 Training Loss: 0.07268989086151123\n",
      "Epoch 4437/30000 Training Loss: 0.04841473326086998\n",
      "Epoch 4438/30000 Training Loss: 0.06432178616523743\n",
      "Epoch 4439/30000 Training Loss: 0.05017530173063278\n",
      "Epoch 4440/30000 Training Loss: 0.05590525269508362\n",
      "Epoch 4441/30000 Training Loss: 0.07481350004673004\n",
      "Epoch 4442/30000 Training Loss: 0.05735351890325546\n",
      "Epoch 4443/30000 Training Loss: 0.04947328940033913\n",
      "Epoch 4444/30000 Training Loss: 0.047100648283958435\n",
      "Epoch 4445/30000 Training Loss: 0.0582662932574749\n",
      "Epoch 4446/30000 Training Loss: 0.07967153191566467\n",
      "Epoch 4447/30000 Training Loss: 0.056704871356487274\n",
      "Epoch 4448/30000 Training Loss: 0.04160730540752411\n",
      "Epoch 4449/30000 Training Loss: 0.06594210863113403\n",
      "Epoch 4450/30000 Training Loss: 0.05111285299062729\n",
      "Epoch 4451/30000 Training Loss: 0.05810236185789108\n",
      "Epoch 4452/30000 Training Loss: 0.07246918976306915\n",
      "Epoch 4453/30000 Training Loss: 0.059576280415058136\n",
      "Epoch 4454/30000 Training Loss: 0.051981985569000244\n",
      "Epoch 4455/30000 Training Loss: 0.06177089363336563\n",
      "Epoch 4456/30000 Training Loss: 0.06437398493289948\n",
      "Epoch 4457/30000 Training Loss: 0.06078013777732849\n",
      "Epoch 4458/30000 Training Loss: 0.05031850188970566\n",
      "Epoch 4459/30000 Training Loss: 0.04762038215994835\n",
      "Epoch 4460/30000 Training Loss: 0.06486880034208298\n",
      "Epoch 4461/30000 Training Loss: 0.054484933614730835\n",
      "Epoch 4462/30000 Training Loss: 0.04800330474972725\n",
      "Epoch 4463/30000 Training Loss: 0.04528503865003586\n",
      "Epoch 4464/30000 Training Loss: 0.06988771259784698\n",
      "Epoch 4465/30000 Training Loss: 0.052768513560295105\n",
      "Epoch 4466/30000 Training Loss: 0.0516030378639698\n",
      "Epoch 4467/30000 Training Loss: 0.043128736317157745\n",
      "Epoch 4468/30000 Training Loss: 0.056986503303050995\n",
      "Epoch 4469/30000 Training Loss: 0.05399036779999733\n",
      "Epoch 4470/30000 Training Loss: 0.054112985730171204\n",
      "Epoch 4471/30000 Training Loss: 0.06221849098801613\n",
      "Epoch 4472/30000 Training Loss: 0.06108468770980835\n",
      "Epoch 4473/30000 Training Loss: 0.0595313161611557\n",
      "Epoch 4474/30000 Training Loss: 0.06148507446050644\n",
      "Epoch 4475/30000 Training Loss: 0.05683949589729309\n",
      "Epoch 4476/30000 Training Loss: 0.07369112223386765\n",
      "Epoch 4477/30000 Training Loss: 0.05929909646511078\n",
      "Epoch 4478/30000 Training Loss: 0.07589899748563766\n",
      "Epoch 4479/30000 Training Loss: 0.054593734443187714\n",
      "Epoch 4480/30000 Training Loss: 0.062434978783130646\n",
      "Epoch 4481/30000 Training Loss: 0.06219549849629402\n",
      "Epoch 4482/30000 Training Loss: 0.061525292694568634\n",
      "Epoch 4483/30000 Training Loss: 0.07755309343338013\n",
      "Epoch 4484/30000 Training Loss: 0.061930812895298004\n",
      "Epoch 4485/30000 Training Loss: 0.04017248749732971\n",
      "Epoch 4486/30000 Training Loss: 0.041757382452487946\n",
      "Epoch 4487/30000 Training Loss: 0.07507364451885223\n",
      "Epoch 4488/30000 Training Loss: 0.057655446231365204\n",
      "Epoch 4489/30000 Training Loss: 0.04991696774959564\n",
      "Epoch 4490/30000 Training Loss: 0.06117480248212814\n",
      "Epoch 4491/30000 Training Loss: 0.07182861864566803\n",
      "Epoch 4492/30000 Training Loss: 0.06569507718086243\n",
      "Epoch 4493/30000 Training Loss: 0.07330532371997833\n",
      "Epoch 4494/30000 Training Loss: 0.0524972528219223\n",
      "Epoch 4495/30000 Training Loss: 0.0498429611325264\n",
      "Epoch 4496/30000 Training Loss: 0.06336735934019089\n",
      "Epoch 4497/30000 Training Loss: 0.045070797204971313\n",
      "Epoch 4498/30000 Training Loss: 0.054377079010009766\n",
      "Epoch 4499/30000 Training Loss: 0.05312814563512802\n",
      "Epoch 4500/30000 Training Loss: 0.06437993794679642\n",
      "Epoch 4500/30000 Validation Loss: 0.06274343281984329\n",
      "Epoch 4501/30000 Training Loss: 0.04397910088300705\n",
      "Epoch 4502/30000 Training Loss: 0.0462743416428566\n",
      "Epoch 4503/30000 Training Loss: 0.06859402358531952\n",
      "Epoch 4504/30000 Training Loss: 0.04807727411389351\n",
      "Epoch 4505/30000 Training Loss: 0.046430349349975586\n",
      "Epoch 4506/30000 Training Loss: 0.07111500203609467\n",
      "Epoch 4507/30000 Training Loss: 0.07124775648117065\n",
      "Epoch 4508/30000 Training Loss: 0.05357431247830391\n",
      "Epoch 4509/30000 Training Loss: 0.05567244067788124\n",
      "Epoch 4510/30000 Training Loss: 0.05888432264328003\n",
      "Epoch 4511/30000 Training Loss: 0.053372904658317566\n",
      "Epoch 4512/30000 Training Loss: 0.06188594177365303\n",
      "Epoch 4513/30000 Training Loss: 0.042375437915325165\n",
      "Epoch 4514/30000 Training Loss: 0.0534263476729393\n",
      "Epoch 4515/30000 Training Loss: 0.050886038690805435\n",
      "Epoch 4516/30000 Training Loss: 0.0658741220831871\n",
      "Epoch 4517/30000 Training Loss: 0.060347311198711395\n",
      "Epoch 4518/30000 Training Loss: 0.05288659408688545\n",
      "Epoch 4519/30000 Training Loss: 0.06284977495670319\n",
      "Epoch 4520/30000 Training Loss: 0.054411597549915314\n",
      "Epoch 4521/30000 Training Loss: 0.05108948051929474\n",
      "Epoch 4522/30000 Training Loss: 0.05492996424436569\n",
      "Epoch 4523/30000 Training Loss: 0.07012520730495453\n",
      "Epoch 4524/30000 Training Loss: 0.05693161487579346\n",
      "Epoch 4525/30000 Training Loss: 0.05353425443172455\n",
      "Epoch 4526/30000 Training Loss: 0.04664141684770584\n",
      "Epoch 4527/30000 Training Loss: 0.04631051421165466\n",
      "Epoch 4528/30000 Training Loss: 0.05496038496494293\n",
      "Epoch 4529/30000 Training Loss: 0.050616417080163956\n",
      "Epoch 4530/30000 Training Loss: 0.05856868997216225\n",
      "Epoch 4531/30000 Training Loss: 0.06244179606437683\n",
      "Epoch 4532/30000 Training Loss: 0.06428968161344528\n",
      "Epoch 4533/30000 Training Loss: 0.03699015825986862\n",
      "Epoch 4534/30000 Training Loss: 0.05418192595243454\n",
      "Epoch 4535/30000 Training Loss: 0.05384397879242897\n",
      "Epoch 4536/30000 Training Loss: 0.0698029175400734\n",
      "Epoch 4537/30000 Training Loss: 0.04261995106935501\n",
      "Epoch 4538/30000 Training Loss: 0.08472445607185364\n",
      "Epoch 4539/30000 Training Loss: 0.05498885363340378\n",
      "Epoch 4540/30000 Training Loss: 0.062232606112957\n",
      "Epoch 4541/30000 Training Loss: 0.0473792739212513\n",
      "Epoch 4542/30000 Training Loss: 0.05784600228071213\n",
      "Epoch 4543/30000 Training Loss: 0.04864216595888138\n",
      "Epoch 4544/30000 Training Loss: 0.0548555850982666\n",
      "Epoch 4545/30000 Training Loss: 0.06676293909549713\n",
      "Epoch 4546/30000 Training Loss: 0.05782691389322281\n",
      "Epoch 4547/30000 Training Loss: 0.05675314739346504\n",
      "Epoch 4548/30000 Training Loss: 0.05715905874967575\n",
      "Epoch 4549/30000 Training Loss: 0.06397132575511932\n",
      "Epoch 4550/30000 Training Loss: 0.06538743525743484\n",
      "Epoch 4551/30000 Training Loss: 0.054466087371110916\n",
      "Epoch 4552/30000 Training Loss: 0.05219527333974838\n",
      "Epoch 4553/30000 Training Loss: 0.08134324848651886\n",
      "Epoch 4554/30000 Training Loss: 0.060455985367298126\n",
      "Epoch 4555/30000 Training Loss: 0.05270305275917053\n",
      "Epoch 4556/30000 Training Loss: 0.040448181331157684\n",
      "Epoch 4557/30000 Training Loss: 0.06276312470436096\n",
      "Epoch 4558/30000 Training Loss: 0.057758960872888565\n",
      "Epoch 4559/30000 Training Loss: 0.07336732745170593\n",
      "Epoch 4560/30000 Training Loss: 0.06330963969230652\n",
      "Epoch 4561/30000 Training Loss: 0.07425186038017273\n",
      "Epoch 4562/30000 Training Loss: 0.043881237506866455\n",
      "Epoch 4563/30000 Training Loss: 0.045057281851768494\n",
      "Epoch 4564/30000 Training Loss: 0.053157199174165726\n",
      "Epoch 4565/30000 Training Loss: 0.07203592360019684\n",
      "Epoch 4566/30000 Training Loss: 0.0668492540717125\n",
      "Epoch 4567/30000 Training Loss: 0.0480470284819603\n",
      "Epoch 4568/30000 Training Loss: 0.05575881153345108\n",
      "Epoch 4569/30000 Training Loss: 0.0570138543844223\n",
      "Epoch 4570/30000 Training Loss: 0.060695212334394455\n",
      "Epoch 4571/30000 Training Loss: 0.054732538759708405\n",
      "Epoch 4572/30000 Training Loss: 0.06911425292491913\n",
      "Epoch 4573/30000 Training Loss: 0.047863103449344635\n",
      "Epoch 4574/30000 Training Loss: 0.05471671372652054\n",
      "Epoch 4575/30000 Training Loss: 0.0672091543674469\n",
      "Epoch 4576/30000 Training Loss: 0.046253275126218796\n",
      "Epoch 4577/30000 Training Loss: 0.048910290002822876\n",
      "Epoch 4578/30000 Training Loss: 0.051398906856775284\n",
      "Epoch 4579/30000 Training Loss: 0.06028566509485245\n",
      "Epoch 4580/30000 Training Loss: 0.06432417035102844\n",
      "Epoch 4581/30000 Training Loss: 0.06188705563545227\n",
      "Epoch 4582/30000 Training Loss: 0.0681452676653862\n",
      "Epoch 4583/30000 Training Loss: 0.07737921178340912\n",
      "Epoch 4584/30000 Training Loss: 0.06539439409971237\n",
      "Epoch 4585/30000 Training Loss: 0.06699641048908234\n",
      "Epoch 4586/30000 Training Loss: 0.06447026878595352\n",
      "Epoch 4587/30000 Training Loss: 0.06873972713947296\n",
      "Epoch 4588/30000 Training Loss: 0.06491470336914062\n",
      "Epoch 4589/30000 Training Loss: 0.05343586951494217\n",
      "Epoch 4590/30000 Training Loss: 0.05700882524251938\n",
      "Epoch 4591/30000 Training Loss: 0.048029109835624695\n",
      "Epoch 4592/30000 Training Loss: 0.06892726570367813\n",
      "Epoch 4593/30000 Training Loss: 0.07346324622631073\n",
      "Epoch 4594/30000 Training Loss: 0.07044366002082825\n",
      "Epoch 4595/30000 Training Loss: 0.04474436864256859\n",
      "Epoch 4596/30000 Training Loss: 0.07335798442363739\n",
      "Epoch 4597/30000 Training Loss: 0.05991741269826889\n",
      "Epoch 4598/30000 Training Loss: 0.06931155920028687\n",
      "Epoch 4599/30000 Training Loss: 0.05727638304233551\n",
      "Epoch 4600/30000 Training Loss: 0.06771209836006165\n",
      "Epoch 4600/30000 Validation Loss: 0.053644463419914246\n",
      "Epoch 4601/30000 Training Loss: 0.05354716628789902\n",
      "Epoch 4602/30000 Training Loss: 0.04323761910200119\n",
      "Epoch 4603/30000 Training Loss: 0.05205630883574486\n",
      "Epoch 4604/30000 Training Loss: 0.07110253721475601\n",
      "Epoch 4605/30000 Training Loss: 0.04534819722175598\n",
      "Epoch 4606/30000 Training Loss: 0.05821792036294937\n",
      "Epoch 4607/30000 Training Loss: 0.06410852074623108\n",
      "Epoch 4608/30000 Training Loss: 0.05234755575656891\n",
      "Epoch 4609/30000 Training Loss: 0.056281089782714844\n",
      "Epoch 4610/30000 Training Loss: 0.03981547802686691\n",
      "Epoch 4611/30000 Training Loss: 0.054013486951589584\n",
      "Epoch 4612/30000 Training Loss: 0.06420308351516724\n",
      "Epoch 4613/30000 Training Loss: 0.04536709561944008\n",
      "Epoch 4614/30000 Training Loss: 0.06699825823307037\n",
      "Epoch 4615/30000 Training Loss: 0.07522724568843842\n",
      "Epoch 4616/30000 Training Loss: 0.06747063994407654\n",
      "Epoch 4617/30000 Training Loss: 0.054186172783374786\n",
      "Epoch 4618/30000 Training Loss: 0.061486609280109406\n",
      "Epoch 4619/30000 Training Loss: 0.06955494731664658\n",
      "Epoch 4620/30000 Training Loss: 0.0433070994913578\n",
      "Epoch 4621/30000 Training Loss: 0.049256306141614914\n",
      "Epoch 4622/30000 Training Loss: 0.05149140581488609\n",
      "Epoch 4623/30000 Training Loss: 0.06054328754544258\n",
      "Epoch 4624/30000 Training Loss: 0.06421463191509247\n",
      "Epoch 4625/30000 Training Loss: 0.056714706122875214\n",
      "Epoch 4626/30000 Training Loss: 0.05224379152059555\n",
      "Epoch 4627/30000 Training Loss: 0.06328007578849792\n",
      "Epoch 4628/30000 Training Loss: 0.05066751688718796\n",
      "Epoch 4629/30000 Training Loss: 0.055461324751377106\n",
      "Epoch 4630/30000 Training Loss: 0.062287837266922\n",
      "Epoch 4631/30000 Training Loss: 0.05366279557347298\n",
      "Epoch 4632/30000 Training Loss: 0.04938843846321106\n",
      "Epoch 4633/30000 Training Loss: 0.06247737258672714\n",
      "Epoch 4634/30000 Training Loss: 0.05951476842164993\n",
      "Epoch 4635/30000 Training Loss: 0.04542628675699234\n",
      "Epoch 4636/30000 Training Loss: 0.0672186017036438\n",
      "Epoch 4637/30000 Training Loss: 0.052120886743068695\n",
      "Epoch 4638/30000 Training Loss: 0.055007390677928925\n",
      "Epoch 4639/30000 Training Loss: 0.061153896152973175\n",
      "Epoch 4640/30000 Training Loss: 0.05942073464393616\n",
      "Epoch 4641/30000 Training Loss: 0.041248805820941925\n",
      "Epoch 4642/30000 Training Loss: 0.08470185101032257\n",
      "Epoch 4643/30000 Training Loss: 0.05785604566335678\n",
      "Epoch 4644/30000 Training Loss: 0.049125850200653076\n",
      "Epoch 4645/30000 Training Loss: 0.07651568204164505\n",
      "Epoch 4646/30000 Training Loss: 0.03998102247714996\n",
      "Epoch 4647/30000 Training Loss: 0.0638515055179596\n",
      "Epoch 4648/30000 Training Loss: 0.05192119628190994\n",
      "Epoch 4649/30000 Training Loss: 0.05805287882685661\n",
      "Epoch 4650/30000 Training Loss: 0.04391578584909439\n",
      "Epoch 4651/30000 Training Loss: 0.04500354081392288\n",
      "Epoch 4652/30000 Training Loss: 0.04644002392888069\n",
      "Epoch 4653/30000 Training Loss: 0.059440985321998596\n",
      "Epoch 4654/30000 Training Loss: 0.06396457552909851\n",
      "Epoch 4655/30000 Training Loss: 0.055646494030952454\n",
      "Epoch 4656/30000 Training Loss: 0.06885446608066559\n",
      "Epoch 4657/30000 Training Loss: 0.03784226253628731\n",
      "Epoch 4658/30000 Training Loss: 0.049645453691482544\n",
      "Epoch 4659/30000 Training Loss: 0.06515102088451385\n",
      "Epoch 4660/30000 Training Loss: 0.06237465888261795\n",
      "Epoch 4661/30000 Training Loss: 0.06519030779600143\n",
      "Epoch 4662/30000 Training Loss: 0.04779757186770439\n",
      "Epoch 4663/30000 Training Loss: 0.04668703302741051\n",
      "Epoch 4664/30000 Training Loss: 0.04725012928247452\n",
      "Epoch 4665/30000 Training Loss: 0.04537586867809296\n",
      "Epoch 4666/30000 Training Loss: 0.05989324301481247\n",
      "Epoch 4667/30000 Training Loss: 0.05062590539455414\n",
      "Epoch 4668/30000 Training Loss: 0.05319119989871979\n",
      "Epoch 4669/30000 Training Loss: 0.0704287737607956\n",
      "Epoch 4670/30000 Training Loss: 0.04642663896083832\n",
      "Epoch 4671/30000 Training Loss: 0.0677328109741211\n",
      "Epoch 4672/30000 Training Loss: 0.054643191397190094\n",
      "Epoch 4673/30000 Training Loss: 0.04976210743188858\n",
      "Epoch 4674/30000 Training Loss: 0.056648604571819305\n",
      "Epoch 4675/30000 Training Loss: 0.05696243792772293\n",
      "Epoch 4676/30000 Training Loss: 0.05591834336519241\n",
      "Epoch 4677/30000 Training Loss: 0.04806321859359741\n",
      "Epoch 4678/30000 Training Loss: 0.06383302807807922\n",
      "Epoch 4679/30000 Training Loss: 0.05530630797147751\n",
      "Epoch 4680/30000 Training Loss: 0.051294609904289246\n",
      "Epoch 4681/30000 Training Loss: 0.0762614980340004\n",
      "Epoch 4682/30000 Training Loss: 0.061579421162605286\n",
      "Epoch 4683/30000 Training Loss: 0.05441150814294815\n",
      "Epoch 4684/30000 Training Loss: 0.06411625444889069\n",
      "Epoch 4685/30000 Training Loss: 0.06682457029819489\n",
      "Epoch 4686/30000 Training Loss: 0.06371575593948364\n",
      "Epoch 4687/30000 Training Loss: 0.06105981022119522\n",
      "Epoch 4688/30000 Training Loss: 0.04548557102680206\n",
      "Epoch 4689/30000 Training Loss: 0.08522333204746246\n",
      "Epoch 4690/30000 Training Loss: 0.0468449592590332\n",
      "Epoch 4691/30000 Training Loss: 0.06356681138277054\n",
      "Epoch 4692/30000 Training Loss: 0.04565226286649704\n",
      "Epoch 4693/30000 Training Loss: 0.05583062395453453\n",
      "Epoch 4694/30000 Training Loss: 0.053062111139297485\n",
      "Epoch 4695/30000 Training Loss: 0.07401679456233978\n",
      "Epoch 4696/30000 Training Loss: 0.05882313475012779\n",
      "Epoch 4697/30000 Training Loss: 0.061930589377880096\n",
      "Epoch 4698/30000 Training Loss: 0.061503976583480835\n",
      "Epoch 4699/30000 Training Loss: 0.04879465699195862\n",
      "Epoch 4700/30000 Training Loss: 0.06512041389942169\n",
      "Epoch 4700/30000 Validation Loss: 0.0458274781703949\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0458274781703949<=============\n",
      "Epoch 4701/30000 Training Loss: 0.05371005833148956\n",
      "Epoch 4702/30000 Training Loss: 0.05310971662402153\n",
      "Epoch 4703/30000 Training Loss: 0.04665762558579445\n",
      "Epoch 4704/30000 Training Loss: 0.06658155471086502\n",
      "Epoch 4705/30000 Training Loss: 0.04170961678028107\n",
      "Epoch 4706/30000 Training Loss: 0.061441775411367416\n",
      "Epoch 4707/30000 Training Loss: 0.06951217353343964\n",
      "Epoch 4708/30000 Training Loss: 0.049267180263996124\n",
      "Epoch 4709/30000 Training Loss: 0.05278865247964859\n",
      "Epoch 4710/30000 Training Loss: 0.052535705268383026\n",
      "Epoch 4711/30000 Training Loss: 0.0588742271065712\n",
      "Epoch 4712/30000 Training Loss: 0.0648585855960846\n",
      "Epoch 4713/30000 Training Loss: 0.06373700499534607\n",
      "Epoch 4714/30000 Training Loss: 0.04734707623720169\n",
      "Epoch 4715/30000 Training Loss: 0.06036489084362984\n",
      "Epoch 4716/30000 Training Loss: 0.06226407364010811\n",
      "Epoch 4717/30000 Training Loss: 0.06402517855167389\n",
      "Epoch 4718/30000 Training Loss: 0.056359805166721344\n",
      "Epoch 4719/30000 Training Loss: 0.05634952336549759\n",
      "Epoch 4720/30000 Training Loss: 0.04473501071333885\n",
      "Epoch 4721/30000 Training Loss: 0.056472208350896835\n",
      "Epoch 4722/30000 Training Loss: 0.07519963383674622\n",
      "Epoch 4723/30000 Training Loss: 0.06106800585985184\n",
      "Epoch 4724/30000 Training Loss: 0.06190941482782364\n",
      "Epoch 4725/30000 Training Loss: 0.051227666437625885\n",
      "Epoch 4726/30000 Training Loss: 0.06642133742570877\n",
      "Epoch 4727/30000 Training Loss: 0.0627342090010643\n",
      "Epoch 4728/30000 Training Loss: 0.053431835025548935\n",
      "Epoch 4729/30000 Training Loss: 0.06390733271837234\n",
      "Epoch 4730/30000 Training Loss: 0.05862000212073326\n",
      "Epoch 4731/30000 Training Loss: 0.04951106756925583\n",
      "Epoch 4732/30000 Training Loss: 0.05603025108575821\n",
      "Epoch 4733/30000 Training Loss: 0.05064288526773453\n",
      "Epoch 4734/30000 Training Loss: 0.055584508925676346\n",
      "Epoch 4735/30000 Training Loss: 0.08203114569187164\n",
      "Epoch 4736/30000 Training Loss: 0.059886060655117035\n",
      "Epoch 4737/30000 Training Loss: 0.05009051412343979\n",
      "Epoch 4738/30000 Training Loss: 0.060023583471775055\n",
      "Epoch 4739/30000 Training Loss: 0.06049303337931633\n",
      "Epoch 4740/30000 Training Loss: 0.051805950701236725\n",
      "Epoch 4741/30000 Training Loss: 0.04655461385846138\n",
      "Epoch 4742/30000 Training Loss: 0.06778339296579361\n",
      "Epoch 4743/30000 Training Loss: 0.05280851572751999\n",
      "Epoch 4744/30000 Training Loss: 0.05725153908133507\n",
      "Epoch 4745/30000 Training Loss: 0.05808594822883606\n",
      "Epoch 4746/30000 Training Loss: 0.057997070252895355\n",
      "Epoch 4747/30000 Training Loss: 0.04910506308078766\n",
      "Epoch 4748/30000 Training Loss: 0.07491222023963928\n",
      "Epoch 4749/30000 Training Loss: 0.05611944571137428\n",
      "Epoch 4750/30000 Training Loss: 0.06952206045389175\n",
      "Epoch 4751/30000 Training Loss: 0.06289945542812347\n",
      "Epoch 4752/30000 Training Loss: 0.07920975983142853\n",
      "Epoch 4753/30000 Training Loss: 0.052339598536491394\n",
      "Epoch 4754/30000 Training Loss: 0.0612776055932045\n",
      "Epoch 4755/30000 Training Loss: 0.057638078927993774\n",
      "Epoch 4756/30000 Training Loss: 0.07058800756931305\n",
      "Epoch 4757/30000 Training Loss: 0.048851676285266876\n",
      "Epoch 4758/30000 Training Loss: 0.05300150439143181\n",
      "Epoch 4759/30000 Training Loss: 0.04650270193815231\n",
      "Epoch 4760/30000 Training Loss: 0.061218515038490295\n",
      "Epoch 4761/30000 Training Loss: 0.05606139823794365\n",
      "Epoch 4762/30000 Training Loss: 0.06008724123239517\n",
      "Epoch 4763/30000 Training Loss: 0.04083918035030365\n",
      "Epoch 4764/30000 Training Loss: 0.05880843847990036\n",
      "Epoch 4765/30000 Training Loss: 0.050103817135095596\n",
      "Epoch 4766/30000 Training Loss: 0.039857037365436554\n",
      "Epoch 4767/30000 Training Loss: 0.052603960037231445\n",
      "Epoch 4768/30000 Training Loss: 0.051258206367492676\n",
      "Epoch 4769/30000 Training Loss: 0.05274324119091034\n",
      "Epoch 4770/30000 Training Loss: 0.0461777001619339\n",
      "Epoch 4771/30000 Training Loss: 0.06119949370622635\n",
      "Epoch 4772/30000 Training Loss: 0.05494294315576553\n",
      "Epoch 4773/30000 Training Loss: 0.05749092251062393\n",
      "Epoch 4774/30000 Training Loss: 0.06487580388784409\n",
      "Epoch 4775/30000 Training Loss: 0.06670504808425903\n",
      "Epoch 4776/30000 Training Loss: 0.05718943849205971\n",
      "Epoch 4777/30000 Training Loss: 0.04479020833969116\n",
      "Epoch 4778/30000 Training Loss: 0.06059475243091583\n",
      "Epoch 4779/30000 Training Loss: 0.056652314960956573\n",
      "Epoch 4780/30000 Training Loss: 0.053116410970687866\n",
      "Epoch 4781/30000 Training Loss: 0.06428059935569763\n",
      "Epoch 4782/30000 Training Loss: 0.05743592232465744\n",
      "Epoch 4783/30000 Training Loss: 0.05178289860486984\n",
      "Epoch 4784/30000 Training Loss: 0.05646196007728577\n",
      "Epoch 4785/30000 Training Loss: 0.05932643637061119\n",
      "Epoch 4786/30000 Training Loss: 0.048638030886650085\n",
      "Epoch 4787/30000 Training Loss: 0.07116974890232086\n",
      "Epoch 4788/30000 Training Loss: 0.06875231862068176\n",
      "Epoch 4789/30000 Training Loss: 0.04103890433907509\n",
      "Epoch 4790/30000 Training Loss: 0.06274651736021042\n",
      "Epoch 4791/30000 Training Loss: 0.046090513467788696\n",
      "Epoch 4792/30000 Training Loss: 0.056352920830249786\n",
      "Epoch 4793/30000 Training Loss: 0.05849185585975647\n",
      "Epoch 4794/30000 Training Loss: 0.050457824021577835\n",
      "Epoch 4795/30000 Training Loss: 0.03923557698726654\n",
      "Epoch 4796/30000 Training Loss: 0.04689313471317291\n",
      "Epoch 4797/30000 Training Loss: 0.054000984877347946\n",
      "Epoch 4798/30000 Training Loss: 0.06379779428243637\n",
      "Epoch 4799/30000 Training Loss: 0.05399858206510544\n",
      "Epoch 4800/30000 Training Loss: 0.05156184732913971\n",
      "Epoch 4800/30000 Validation Loss: 0.05616284906864166\n",
      "Epoch 4801/30000 Training Loss: 0.05299237743020058\n",
      "Epoch 4802/30000 Training Loss: 0.060487933456897736\n",
      "Epoch 4803/30000 Training Loss: 0.05660562217235565\n",
      "Epoch 4804/30000 Training Loss: 0.04488078132271767\n",
      "Epoch 4805/30000 Training Loss: 0.04859387129545212\n",
      "Epoch 4806/30000 Training Loss: 0.057526733726263046\n",
      "Epoch 4807/30000 Training Loss: 0.07186074554920197\n",
      "Epoch 4808/30000 Training Loss: 0.0619044154882431\n",
      "Epoch 4809/30000 Training Loss: 0.05134576931595802\n",
      "Epoch 4810/30000 Training Loss: 0.057728830724954605\n",
      "Epoch 4811/30000 Training Loss: 0.08002990484237671\n",
      "Epoch 4812/30000 Training Loss: 0.060874372720718384\n",
      "Epoch 4813/30000 Training Loss: 0.0662473738193512\n",
      "Epoch 4814/30000 Training Loss: 0.047500595450401306\n",
      "Epoch 4815/30000 Training Loss: 0.04354598745703697\n",
      "Epoch 4816/30000 Training Loss: 0.05210204795002937\n",
      "Epoch 4817/30000 Training Loss: 0.04210348054766655\n",
      "Epoch 4818/30000 Training Loss: 0.04409678280353546\n",
      "Epoch 4819/30000 Training Loss: 0.06040024012327194\n",
      "Epoch 4820/30000 Training Loss: 0.06556469947099686\n",
      "Epoch 4821/30000 Training Loss: 0.05161040276288986\n",
      "Epoch 4822/30000 Training Loss: 0.05075544863939285\n",
      "Epoch 4823/30000 Training Loss: 0.06655295193195343\n",
      "Epoch 4824/30000 Training Loss: 0.05777183920145035\n",
      "Epoch 4825/30000 Training Loss: 0.06558053940534592\n",
      "Epoch 4826/30000 Training Loss: 0.053877219557762146\n",
      "Epoch 4827/30000 Training Loss: 0.05243513733148575\n",
      "Epoch 4828/30000 Training Loss: 0.05408312380313873\n",
      "Epoch 4829/30000 Training Loss: 0.05277179554104805\n",
      "Epoch 4830/30000 Training Loss: 0.07392286509275436\n",
      "Epoch 4831/30000 Training Loss: 0.06288474053144455\n",
      "Epoch 4832/30000 Training Loss: 0.055057600140571594\n",
      "Epoch 4833/30000 Training Loss: 0.049477193504571915\n",
      "Epoch 4834/30000 Training Loss: 0.0530259795486927\n",
      "Epoch 4835/30000 Training Loss: 0.04692243039608002\n",
      "Epoch 4836/30000 Training Loss: 0.046294741332530975\n",
      "Epoch 4837/30000 Training Loss: 0.05314897373318672\n",
      "Epoch 4838/30000 Training Loss: 0.03655273839831352\n",
      "Epoch 4839/30000 Training Loss: 0.0630556121468544\n",
      "Epoch 4840/30000 Training Loss: 0.06283098459243774\n",
      "Epoch 4841/30000 Training Loss: 0.04857625067234039\n",
      "Epoch 4842/30000 Training Loss: 0.047896772623062134\n",
      "Epoch 4843/30000 Training Loss: 0.043606728315353394\n",
      "Epoch 4844/30000 Training Loss: 0.0666392371058464\n",
      "Epoch 4845/30000 Training Loss: 0.04954494163393974\n",
      "Epoch 4846/30000 Training Loss: 0.049586888402700424\n",
      "Epoch 4847/30000 Training Loss: 0.05286094918847084\n",
      "Epoch 4848/30000 Training Loss: 0.04271143302321434\n",
      "Epoch 4849/30000 Training Loss: 0.056411974132061005\n",
      "Epoch 4850/30000 Training Loss: 0.06975633651018143\n",
      "Epoch 4851/30000 Training Loss: 0.05302371084690094\n",
      "Epoch 4852/30000 Training Loss: 0.06876303255558014\n",
      "Epoch 4853/30000 Training Loss: 0.035802554339170456\n",
      "Epoch 4854/30000 Training Loss: 0.054221637547016144\n",
      "Epoch 4855/30000 Training Loss: 0.04710456356406212\n",
      "Epoch 4856/30000 Training Loss: 0.051860515028238297\n",
      "Epoch 4857/30000 Training Loss: 0.05908066779375076\n",
      "Epoch 4858/30000 Training Loss: 0.047268882393836975\n",
      "Epoch 4859/30000 Training Loss: 0.051830410957336426\n",
      "Epoch 4860/30000 Training Loss: 0.06862528622150421\n",
      "Epoch 4861/30000 Training Loss: 0.058548059314489365\n",
      "Epoch 4862/30000 Training Loss: 0.07313226908445358\n",
      "Epoch 4863/30000 Training Loss: 0.05717265605926514\n",
      "Epoch 4864/30000 Training Loss: 0.0591663233935833\n",
      "Epoch 4865/30000 Training Loss: 0.06978148967027664\n",
      "Epoch 4866/30000 Training Loss: 0.04070349037647247\n",
      "Epoch 4867/30000 Training Loss: 0.05435749888420105\n",
      "Epoch 4868/30000 Training Loss: 0.07332560420036316\n",
      "Epoch 4869/30000 Training Loss: 0.04692841321229935\n",
      "Epoch 4870/30000 Training Loss: 0.06617818772792816\n",
      "Epoch 4871/30000 Training Loss: 0.07001644372940063\n",
      "Epoch 4872/30000 Training Loss: 0.048679985105991364\n",
      "Epoch 4873/30000 Training Loss: 0.05347379297018051\n",
      "Epoch 4874/30000 Training Loss: 0.03790241479873657\n",
      "Epoch 4875/30000 Training Loss: 0.04598233848810196\n",
      "Epoch 4876/30000 Training Loss: 0.05250220000743866\n",
      "Epoch 4877/30000 Training Loss: 0.05466291680932045\n",
      "Epoch 4878/30000 Training Loss: 0.04990631341934204\n",
      "Epoch 4879/30000 Training Loss: 0.06145373731851578\n",
      "Epoch 4880/30000 Training Loss: 0.06013847887516022\n",
      "Epoch 4881/30000 Training Loss: 0.0527123287320137\n",
      "Epoch 4882/30000 Training Loss: 0.061737798154354095\n",
      "Epoch 4883/30000 Training Loss: 0.06062040477991104\n",
      "Epoch 4884/30000 Training Loss: 0.05173172801733017\n",
      "Epoch 4885/30000 Training Loss: 0.06585261970758438\n",
      "Epoch 4886/30000 Training Loss: 0.057773932814598083\n",
      "Epoch 4887/30000 Training Loss: 0.061564553529024124\n",
      "Epoch 4888/30000 Training Loss: 0.05458492413163185\n",
      "Epoch 4889/30000 Training Loss: 0.04365772381424904\n",
      "Epoch 4890/30000 Training Loss: 0.05361788719892502\n",
      "Epoch 4891/30000 Training Loss: 0.05427432805299759\n",
      "Epoch 4892/30000 Training Loss: 0.06521721929311752\n",
      "Epoch 4893/30000 Training Loss: 0.06431017071008682\n",
      "Epoch 4894/30000 Training Loss: 0.06195705384016037\n",
      "Epoch 4895/30000 Training Loss: 0.05641379952430725\n",
      "Epoch 4896/30000 Training Loss: 0.051033735275268555\n",
      "Epoch 4897/30000 Training Loss: 0.05143296718597412\n",
      "Epoch 4898/30000 Training Loss: 0.051151394844055176\n",
      "Epoch 4899/30000 Training Loss: 0.07044461369514465\n",
      "Epoch 4900/30000 Training Loss: 0.07132259756326675\n",
      "Epoch 4900/30000 Validation Loss: 0.05759267508983612\n",
      "Epoch 4901/30000 Training Loss: 0.05158843845129013\n",
      "Epoch 4902/30000 Training Loss: 0.06708009541034698\n",
      "Epoch 4903/30000 Training Loss: 0.061509355902671814\n",
      "Epoch 4904/30000 Training Loss: 0.06666742265224457\n",
      "Epoch 4905/30000 Training Loss: 0.06289578229188919\n",
      "Epoch 4906/30000 Training Loss: 0.03296525031328201\n",
      "Epoch 4907/30000 Training Loss: 0.04177886247634888\n",
      "Epoch 4908/30000 Training Loss: 0.05205077305436134\n",
      "Epoch 4909/30000 Training Loss: 0.05040815845131874\n",
      "Epoch 4910/30000 Training Loss: 0.06682614237070084\n",
      "Epoch 4911/30000 Training Loss: 0.053586579859256744\n",
      "Epoch 4912/30000 Training Loss: 0.06415718793869019\n",
      "Epoch 4913/30000 Training Loss: 0.0602484866976738\n",
      "Epoch 4914/30000 Training Loss: 0.06256222724914551\n",
      "Epoch 4915/30000 Training Loss: 0.06052906811237335\n",
      "Epoch 4916/30000 Training Loss: 0.04489058256149292\n",
      "Epoch 4917/30000 Training Loss: 0.06069990247488022\n",
      "Epoch 4918/30000 Training Loss: 0.0408443957567215\n",
      "Epoch 4919/30000 Training Loss: 0.036010488867759705\n",
      "Epoch 4920/30000 Training Loss: 0.055249039083719254\n",
      "Epoch 4921/30000 Training Loss: 0.05843234062194824\n",
      "Epoch 4922/30000 Training Loss: 0.060234591364860535\n",
      "Epoch 4923/30000 Training Loss: 0.04830601066350937\n",
      "Epoch 4924/30000 Training Loss: 0.057432420551776886\n",
      "Epoch 4925/30000 Training Loss: 0.05286470800638199\n",
      "Epoch 4926/30000 Training Loss: 0.0507950633764267\n",
      "Epoch 4927/30000 Training Loss: 0.04434172809123993\n",
      "Epoch 4928/30000 Training Loss: 0.05237220227718353\n",
      "Epoch 4929/30000 Training Loss: 0.0683978796005249\n",
      "Epoch 4930/30000 Training Loss: 0.058657653629779816\n",
      "Epoch 4931/30000 Training Loss: 0.04948197305202484\n",
      "Epoch 4932/30000 Training Loss: 0.06666606664657593\n",
      "Epoch 4933/30000 Training Loss: 0.06228227540850639\n",
      "Epoch 4934/30000 Training Loss: 0.07375868409872055\n",
      "Epoch 4935/30000 Training Loss: 0.05785796046257019\n",
      "Epoch 4936/30000 Training Loss: 0.05589944124221802\n",
      "Epoch 4937/30000 Training Loss: 0.042771898210048676\n",
      "Epoch 4938/30000 Training Loss: 0.07372002303600311\n",
      "Epoch 4939/30000 Training Loss: 0.056423429399728775\n",
      "Epoch 4940/30000 Training Loss: 0.05421452969312668\n",
      "Epoch 4941/30000 Training Loss: 0.04928547888994217\n",
      "Epoch 4942/30000 Training Loss: 0.0524105504155159\n",
      "Epoch 4943/30000 Training Loss: 0.046089667826890945\n",
      "Epoch 4944/30000 Training Loss: 0.048203084617853165\n",
      "Epoch 4945/30000 Training Loss: 0.05546712502837181\n",
      "Epoch 4946/30000 Training Loss: 0.06826175004243851\n",
      "Epoch 4947/30000 Training Loss: 0.04997742548584938\n",
      "Epoch 4948/30000 Training Loss: 0.04782405123114586\n",
      "Epoch 4949/30000 Training Loss: 0.054552920162677765\n",
      "Epoch 4950/30000 Training Loss: 0.05470944568514824\n",
      "Epoch 4951/30000 Training Loss: 0.05047271400690079\n",
      "Epoch 4952/30000 Training Loss: 0.048110440373420715\n",
      "Epoch 4953/30000 Training Loss: 0.056813471019268036\n",
      "Epoch 4954/30000 Training Loss: 0.049682147800922394\n",
      "Epoch 4955/30000 Training Loss: 0.05431473255157471\n",
      "Epoch 4956/30000 Training Loss: 0.05831712484359741\n",
      "Epoch 4957/30000 Training Loss: 0.06921876221895218\n",
      "Epoch 4958/30000 Training Loss: 0.05112852901220322\n",
      "Epoch 4959/30000 Training Loss: 0.05821024253964424\n",
      "Epoch 4960/30000 Training Loss: 0.05301237851381302\n",
      "Epoch 4961/30000 Training Loss: 0.0535857155919075\n",
      "Epoch 4962/30000 Training Loss: 0.05420484393835068\n",
      "Epoch 4963/30000 Training Loss: 0.05169073864817619\n",
      "Epoch 4964/30000 Training Loss: 0.059773512184619904\n",
      "Epoch 4965/30000 Training Loss: 0.05018552765250206\n",
      "Epoch 4966/30000 Training Loss: 0.0572696179151535\n",
      "Epoch 4967/30000 Training Loss: 0.047756873071193695\n",
      "Epoch 4968/30000 Training Loss: 0.07010680437088013\n",
      "Epoch 4969/30000 Training Loss: 0.07230295985937119\n",
      "Epoch 4970/30000 Training Loss: 0.06390787661075592\n",
      "Epoch 4971/30000 Training Loss: 0.05640442669391632\n",
      "Epoch 4972/30000 Training Loss: 0.04725591838359833\n",
      "Epoch 4973/30000 Training Loss: 0.07171985507011414\n",
      "Epoch 4974/30000 Training Loss: 0.04905511811375618\n",
      "Epoch 4975/30000 Training Loss: 0.04487719386816025\n",
      "Epoch 4976/30000 Training Loss: 0.06915825605392456\n",
      "Epoch 4977/30000 Training Loss: 0.06996581703424454\n",
      "Epoch 4978/30000 Training Loss: 0.05818343162536621\n",
      "Epoch 4979/30000 Training Loss: 0.0544077605009079\n",
      "Epoch 4980/30000 Training Loss: 0.06217183917760849\n",
      "Epoch 4981/30000 Training Loss: 0.045599475502967834\n",
      "Epoch 4982/30000 Training Loss: 0.06263169646263123\n",
      "Epoch 4983/30000 Training Loss: 0.05382979288697243\n",
      "Epoch 4984/30000 Training Loss: 0.0516427718102932\n",
      "Epoch 4985/30000 Training Loss: 0.045281216502189636\n",
      "Epoch 4986/30000 Training Loss: 0.08813829720020294\n",
      "Epoch 4987/30000 Training Loss: 0.06496752798557281\n",
      "Epoch 4988/30000 Training Loss: 0.06711462140083313\n",
      "Epoch 4989/30000 Training Loss: 0.0521758496761322\n",
      "Epoch 4990/30000 Training Loss: 0.06172777712345123\n",
      "Epoch 4991/30000 Training Loss: 0.06461465358734131\n",
      "Epoch 4992/30000 Training Loss: 0.05837317556142807\n",
      "Epoch 4993/30000 Training Loss: 0.03799408674240112\n",
      "Epoch 4994/30000 Training Loss: 0.04987113177776337\n",
      "Epoch 4995/30000 Training Loss: 0.054421618580818176\n",
      "Epoch 4996/30000 Training Loss: 0.05996520817279816\n",
      "Epoch 4997/30000 Training Loss: 0.05911083519458771\n",
      "Epoch 4998/30000 Training Loss: 0.049666136503219604\n",
      "Epoch 4999/30000 Training Loss: 0.06125645712018013\n",
      "Epoch 5000/30000 Training Loss: 0.07132652401924133\n",
      "Epoch 5000/30000 Validation Loss: 0.05883294343948364\n",
      "Epoch 5001/30000 Training Loss: 0.050560444593429565\n",
      "Epoch 5002/30000 Training Loss: 0.05194272845983505\n",
      "Epoch 5003/30000 Training Loss: 0.060084421187639236\n",
      "Epoch 5004/30000 Training Loss: 0.0571981780230999\n",
      "Epoch 5005/30000 Training Loss: 0.05824555456638336\n",
      "Epoch 5006/30000 Training Loss: 0.05390652269124985\n",
      "Epoch 5007/30000 Training Loss: 0.053384628146886826\n",
      "Epoch 5008/30000 Training Loss: 0.05337344482541084\n",
      "Epoch 5009/30000 Training Loss: 0.058012403547763824\n",
      "Epoch 5010/30000 Training Loss: 0.049699559807777405\n",
      "Epoch 5011/30000 Training Loss: 0.057159025222063065\n",
      "Epoch 5012/30000 Training Loss: 0.06044425815343857\n",
      "Epoch 5013/30000 Training Loss: 0.049125075340270996\n",
      "Epoch 5014/30000 Training Loss: 0.04804808646440506\n",
      "Epoch 5015/30000 Training Loss: 0.049479998648166656\n",
      "Epoch 5016/30000 Training Loss: 0.04451466351747513\n",
      "Epoch 5017/30000 Training Loss: 0.05190283805131912\n",
      "Epoch 5018/30000 Training Loss: 0.06311224400997162\n",
      "Epoch 5019/30000 Training Loss: 0.059781670570373535\n",
      "Epoch 5020/30000 Training Loss: 0.0641622245311737\n",
      "Epoch 5021/30000 Training Loss: 0.0703577995300293\n",
      "Epoch 5022/30000 Training Loss: 0.045813802629709244\n",
      "Epoch 5023/30000 Training Loss: 0.052367135882377625\n",
      "Epoch 5024/30000 Training Loss: 0.07027804851531982\n",
      "Epoch 5025/30000 Training Loss: 0.05932319164276123\n",
      "Epoch 5026/30000 Training Loss: 0.05031537264585495\n",
      "Epoch 5027/30000 Training Loss: 0.04632900282740593\n",
      "Epoch 5028/30000 Training Loss: 0.07791551947593689\n",
      "Epoch 5029/30000 Training Loss: 0.052323661744594574\n",
      "Epoch 5030/30000 Training Loss: 0.0561627596616745\n",
      "Epoch 5031/30000 Training Loss: 0.07084837555885315\n",
      "Epoch 5032/30000 Training Loss: 0.06008833646774292\n",
      "Epoch 5033/30000 Training Loss: 0.054813891649246216\n",
      "Epoch 5034/30000 Training Loss: 0.05119362846016884\n",
      "Epoch 5035/30000 Training Loss: 0.050978921353816986\n",
      "Epoch 5036/30000 Training Loss: 0.04457671195268631\n",
      "Epoch 5037/30000 Training Loss: 0.05581220984458923\n",
      "Epoch 5038/30000 Training Loss: 0.062135592103004456\n",
      "Epoch 5039/30000 Training Loss: 0.04656883329153061\n",
      "Epoch 5040/30000 Training Loss: 0.0694710984826088\n",
      "Epoch 5041/30000 Training Loss: 0.05869414657354355\n",
      "Epoch 5042/30000 Training Loss: 0.04686632379889488\n",
      "Epoch 5043/30000 Training Loss: 0.05053595080971718\n",
      "Epoch 5044/30000 Training Loss: 0.054256074130535126\n",
      "Epoch 5045/30000 Training Loss: 0.057552143931388855\n",
      "Epoch 5046/30000 Training Loss: 0.044276997447013855\n",
      "Epoch 5047/30000 Training Loss: 0.06381654739379883\n",
      "Epoch 5048/30000 Training Loss: 0.06096111983060837\n",
      "Epoch 5049/30000 Training Loss: 0.06176143139600754\n",
      "Epoch 5050/30000 Training Loss: 0.04503075033426285\n",
      "Epoch 5051/30000 Training Loss: 0.05375368148088455\n",
      "Epoch 5052/30000 Training Loss: 0.06950105726718903\n",
      "Epoch 5053/30000 Training Loss: 0.06123798340559006\n",
      "Epoch 5054/30000 Training Loss: 0.04797849804162979\n",
      "Epoch 5055/30000 Training Loss: 0.04906639829277992\n",
      "Epoch 5056/30000 Training Loss: 0.07320176810026169\n",
      "Epoch 5057/30000 Training Loss: 0.05029388517141342\n",
      "Epoch 5058/30000 Training Loss: 0.06626273691654205\n",
      "Epoch 5059/30000 Training Loss: 0.0506269671022892\n",
      "Epoch 5060/30000 Training Loss: 0.06965748965740204\n",
      "Epoch 5061/30000 Training Loss: 0.05644191801548004\n",
      "Epoch 5062/30000 Training Loss: 0.08005671203136444\n",
      "Epoch 5063/30000 Training Loss: 0.056161392480134964\n",
      "Epoch 5064/30000 Training Loss: 0.038852326571941376\n",
      "Epoch 5065/30000 Training Loss: 0.05414065346121788\n",
      "Epoch 5066/30000 Training Loss: 0.051155440509319305\n",
      "Epoch 5067/30000 Training Loss: 0.04098149761557579\n",
      "Epoch 5068/30000 Training Loss: 0.05664276331663132\n",
      "Epoch 5069/30000 Training Loss: 0.05846906453371048\n",
      "Epoch 5070/30000 Training Loss: 0.06443110853433609\n",
      "Epoch 5071/30000 Training Loss: 0.05065183341503143\n",
      "Epoch 5072/30000 Training Loss: 0.04928138852119446\n",
      "Epoch 5073/30000 Training Loss: 0.052634529769420624\n",
      "Epoch 5074/30000 Training Loss: 0.06159657984972\n",
      "Epoch 5075/30000 Training Loss: 0.05897991359233856\n",
      "Epoch 5076/30000 Training Loss: 0.07262250781059265\n",
      "Epoch 5077/30000 Training Loss: 0.045386627316474915\n",
      "Epoch 5078/30000 Training Loss: 0.044156044721603394\n",
      "Epoch 5079/30000 Training Loss: 0.05356340482831001\n",
      "Epoch 5080/30000 Training Loss: 0.06549552083015442\n",
      "Epoch 5081/30000 Training Loss: 0.04763580858707428\n",
      "Epoch 5082/30000 Training Loss: 0.06445826590061188\n",
      "Epoch 5083/30000 Training Loss: 0.05768156051635742\n",
      "Epoch 5084/30000 Training Loss: 0.0572710745036602\n",
      "Epoch 5085/30000 Training Loss: 0.05350886285305023\n",
      "Epoch 5086/30000 Training Loss: 0.05998581647872925\n",
      "Epoch 5087/30000 Training Loss: 0.0505247488617897\n",
      "Epoch 5088/30000 Training Loss: 0.06075981259346008\n",
      "Epoch 5089/30000 Training Loss: 0.05284857004880905\n",
      "Epoch 5090/30000 Training Loss: 0.06064978986978531\n",
      "Epoch 5091/30000 Training Loss: 0.05487065762281418\n",
      "Epoch 5092/30000 Training Loss: 0.060790568590164185\n",
      "Epoch 5093/30000 Training Loss: 0.048734020441770554\n",
      "Epoch 5094/30000 Training Loss: 0.06307670474052429\n",
      "Epoch 5095/30000 Training Loss: 0.0558614656329155\n",
      "Epoch 5096/30000 Training Loss: 0.047151170670986176\n",
      "Epoch 5097/30000 Training Loss: 0.05920591205358505\n",
      "Epoch 5098/30000 Training Loss: 0.052672673016786575\n",
      "Epoch 5099/30000 Training Loss: 0.05515497922897339\n",
      "Epoch 5100/30000 Training Loss: 0.06105443835258484\n",
      "Epoch 5100/30000 Validation Loss: 0.04421418905258179\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04421418905258179<=============\n",
      "Epoch 5101/30000 Training Loss: 0.058406271040439606\n",
      "Epoch 5102/30000 Training Loss: 0.05284465104341507\n",
      "Epoch 5103/30000 Training Loss: 0.06059470400214195\n",
      "Epoch 5104/30000 Training Loss: 0.060352884232997894\n",
      "Epoch 5105/30000 Training Loss: 0.06554502248764038\n",
      "Epoch 5106/30000 Training Loss: 0.0474918931722641\n",
      "Epoch 5107/30000 Training Loss: 0.056492120027542114\n",
      "Epoch 5108/30000 Training Loss: 0.06403853744268417\n",
      "Epoch 5109/30000 Training Loss: 0.06870785355567932\n",
      "Epoch 5110/30000 Training Loss: 0.04242816939949989\n",
      "Epoch 5111/30000 Training Loss: 0.052735764533281326\n",
      "Epoch 5112/30000 Training Loss: 0.04077894985675812\n",
      "Epoch 5113/30000 Training Loss: 0.05085596814751625\n",
      "Epoch 5114/30000 Training Loss: 0.06295984983444214\n",
      "Epoch 5115/30000 Training Loss: 0.059862926602363586\n",
      "Epoch 5116/30000 Training Loss: 0.041374996304512024\n",
      "Epoch 5117/30000 Training Loss: 0.048328325152397156\n",
      "Epoch 5118/30000 Training Loss: 0.05746103823184967\n",
      "Epoch 5119/30000 Training Loss: 0.046907175332307816\n",
      "Epoch 5120/30000 Training Loss: 0.05629401281476021\n",
      "Epoch 5121/30000 Training Loss: 0.07024259865283966\n",
      "Epoch 5122/30000 Training Loss: 0.06838825345039368\n",
      "Epoch 5123/30000 Training Loss: 0.050448521971702576\n",
      "Epoch 5124/30000 Training Loss: 0.0555255264043808\n",
      "Epoch 5125/30000 Training Loss: 0.046696655452251434\n",
      "Epoch 5126/30000 Training Loss: 0.04293704777956009\n",
      "Epoch 5127/30000 Training Loss: 0.055953413248062134\n",
      "Epoch 5128/30000 Training Loss: 0.05007968097925186\n",
      "Epoch 5129/30000 Training Loss: 0.05808181315660477\n",
      "Epoch 5130/30000 Training Loss: 0.060679733753204346\n",
      "Epoch 5131/30000 Training Loss: 0.06253986060619354\n",
      "Epoch 5132/30000 Training Loss: 0.04849873483181\n",
      "Epoch 5133/30000 Training Loss: 0.056639328598976135\n",
      "Epoch 5134/30000 Training Loss: 0.06317923963069916\n",
      "Epoch 5135/30000 Training Loss: 0.06670113652944565\n",
      "Epoch 5136/30000 Training Loss: 0.05751568078994751\n",
      "Epoch 5137/30000 Training Loss: 0.059862587600946426\n",
      "Epoch 5138/30000 Training Loss: 0.051257237792015076\n",
      "Epoch 5139/30000 Training Loss: 0.05607685446739197\n",
      "Epoch 5140/30000 Training Loss: 0.058343760669231415\n",
      "Epoch 5141/30000 Training Loss: 0.06270129233598709\n",
      "Epoch 5142/30000 Training Loss: 0.04988637939095497\n",
      "Epoch 5143/30000 Training Loss: 0.06279236078262329\n",
      "Epoch 5144/30000 Training Loss: 0.06237685680389404\n",
      "Epoch 5145/30000 Training Loss: 0.05620381981134415\n",
      "Epoch 5146/30000 Training Loss: 0.05981649458408356\n",
      "Epoch 5147/30000 Training Loss: 0.055318497121334076\n",
      "Epoch 5148/30000 Training Loss: 0.05948049947619438\n",
      "Epoch 5149/30000 Training Loss: 0.05611046031117439\n",
      "Epoch 5150/30000 Training Loss: 0.059620924293994904\n",
      "Epoch 5151/30000 Training Loss: 0.06137925386428833\n",
      "Epoch 5152/30000 Training Loss: 0.05648263543844223\n",
      "Epoch 5153/30000 Training Loss: 0.044567182660102844\n",
      "Epoch 5154/30000 Training Loss: 0.04950210079550743\n",
      "Epoch 5155/30000 Training Loss: 0.05490708723664284\n",
      "Epoch 5156/30000 Training Loss: 0.06178531050682068\n",
      "Epoch 5157/30000 Training Loss: 0.048719678074121475\n",
      "Epoch 5158/30000 Training Loss: 0.043518878519535065\n",
      "Epoch 5159/30000 Training Loss: 0.06996987760066986\n",
      "Epoch 5160/30000 Training Loss: 0.054721295833587646\n",
      "Epoch 5161/30000 Training Loss: 0.04999915137887001\n",
      "Epoch 5162/30000 Training Loss: 0.06349585205316544\n",
      "Epoch 5163/30000 Training Loss: 0.05371294915676117\n",
      "Epoch 5164/30000 Training Loss: 0.06511524319648743\n",
      "Epoch 5165/30000 Training Loss: 0.0671774297952652\n",
      "Epoch 5166/30000 Training Loss: 0.05445488542318344\n",
      "Epoch 5167/30000 Training Loss: 0.06872230768203735\n",
      "Epoch 5168/30000 Training Loss: 0.05872352421283722\n",
      "Epoch 5169/30000 Training Loss: 0.0368182510137558\n",
      "Epoch 5170/30000 Training Loss: 0.06636248528957367\n",
      "Epoch 5171/30000 Training Loss: 0.06492931395769119\n",
      "Epoch 5172/30000 Training Loss: 0.051726333796978\n",
      "Epoch 5173/30000 Training Loss: 0.06179715692996979\n",
      "Epoch 5174/30000 Training Loss: 0.04640030115842819\n",
      "Epoch 5175/30000 Training Loss: 0.050891149789094925\n",
      "Epoch 5176/30000 Training Loss: 0.04396211355924606\n",
      "Epoch 5177/30000 Training Loss: 0.06558500975370407\n",
      "Epoch 5178/30000 Training Loss: 0.04969467595219612\n",
      "Epoch 5179/30000 Training Loss: 0.07378453016281128\n",
      "Epoch 5180/30000 Training Loss: 0.04312960430979729\n",
      "Epoch 5181/30000 Training Loss: 0.048014603555202484\n",
      "Epoch 5182/30000 Training Loss: 0.05713528394699097\n",
      "Epoch 5183/30000 Training Loss: 0.051255881786346436\n",
      "Epoch 5184/30000 Training Loss: 0.05735721439123154\n",
      "Epoch 5185/30000 Training Loss: 0.0742468535900116\n",
      "Epoch 5186/30000 Training Loss: 0.050068870186805725\n",
      "Epoch 5187/30000 Training Loss: 0.0718393623828888\n",
      "Epoch 5188/30000 Training Loss: 0.05025387555360794\n",
      "Epoch 5189/30000 Training Loss: 0.047173984348773956\n",
      "Epoch 5190/30000 Training Loss: 0.07933981716632843\n",
      "Epoch 5191/30000 Training Loss: 0.041823409497737885\n",
      "Epoch 5192/30000 Training Loss: 0.0498652458190918\n",
      "Epoch 5193/30000 Training Loss: 0.06587845087051392\n",
      "Epoch 5194/30000 Training Loss: 0.04313812404870987\n",
      "Epoch 5195/30000 Training Loss: 0.08018898218870163\n",
      "Epoch 5196/30000 Training Loss: 0.04526261240243912\n",
      "Epoch 5197/30000 Training Loss: 0.05864710733294487\n",
      "Epoch 5198/30000 Training Loss: 0.06696882098913193\n",
      "Epoch 5199/30000 Training Loss: 0.06135756894946098\n",
      "Epoch 5200/30000 Training Loss: 0.05527929961681366\n",
      "Epoch 5200/30000 Validation Loss: 0.059930771589279175\n",
      "Epoch 5201/30000 Training Loss: 0.044223420321941376\n",
      "Epoch 5202/30000 Training Loss: 0.0503743439912796\n",
      "Epoch 5203/30000 Training Loss: 0.05534014850854874\n",
      "Epoch 5204/30000 Training Loss: 0.03446783125400543\n",
      "Epoch 5205/30000 Training Loss: 0.04581496864557266\n",
      "Epoch 5206/30000 Training Loss: 0.05884220823645592\n",
      "Epoch 5207/30000 Training Loss: 0.05713631212711334\n",
      "Epoch 5208/30000 Training Loss: 0.06776760518550873\n",
      "Epoch 5209/30000 Training Loss: 0.04553123936057091\n",
      "Epoch 5210/30000 Training Loss: 0.05417002737522125\n",
      "Epoch 5211/30000 Training Loss: 0.06574360281229019\n",
      "Epoch 5212/30000 Training Loss: 0.06821688264608383\n",
      "Epoch 5213/30000 Training Loss: 0.0754801481962204\n",
      "Epoch 5214/30000 Training Loss: 0.07108889520168304\n",
      "Epoch 5215/30000 Training Loss: 0.05629989504814148\n",
      "Epoch 5216/30000 Training Loss: 0.05660782754421234\n",
      "Epoch 5217/30000 Training Loss: 0.055380068719387054\n",
      "Epoch 5218/30000 Training Loss: 0.05045604705810547\n",
      "Epoch 5219/30000 Training Loss: 0.0411003902554512\n",
      "Epoch 5220/30000 Training Loss: 0.08062175661325455\n",
      "Epoch 5221/30000 Training Loss: 0.06687051802873611\n",
      "Epoch 5222/30000 Training Loss: 0.04804679751396179\n",
      "Epoch 5223/30000 Training Loss: 0.05289389565587044\n",
      "Epoch 5224/30000 Training Loss: 0.055504824966192245\n",
      "Epoch 5225/30000 Training Loss: 0.04783042520284653\n",
      "Epoch 5226/30000 Training Loss: 0.04173537343740463\n",
      "Epoch 5227/30000 Training Loss: 0.04402387514710426\n",
      "Epoch 5228/30000 Training Loss: 0.048220932483673096\n",
      "Epoch 5229/30000 Training Loss: 0.05177226662635803\n",
      "Epoch 5230/30000 Training Loss: 0.04926401749253273\n",
      "Epoch 5231/30000 Training Loss: 0.041145067662000656\n",
      "Epoch 5232/30000 Training Loss: 0.045365214347839355\n",
      "Epoch 5233/30000 Training Loss: 0.05386082082986832\n",
      "Epoch 5234/30000 Training Loss: 0.05327044427394867\n",
      "Epoch 5235/30000 Training Loss: 0.058614592999219894\n",
      "Epoch 5236/30000 Training Loss: 0.04560651257634163\n",
      "Epoch 5237/30000 Training Loss: 0.06229400634765625\n",
      "Epoch 5238/30000 Training Loss: 0.05007733404636383\n",
      "Epoch 5239/30000 Training Loss: 0.04090213030576706\n",
      "Epoch 5240/30000 Training Loss: 0.058638256043195724\n",
      "Epoch 5241/30000 Training Loss: 0.052157722413539886\n",
      "Epoch 5242/30000 Training Loss: 0.0539785772562027\n",
      "Epoch 5243/30000 Training Loss: 0.06486111134290695\n",
      "Epoch 5244/30000 Training Loss: 0.05672123283147812\n",
      "Epoch 5245/30000 Training Loss: 0.057367824018001556\n",
      "Epoch 5246/30000 Training Loss: 0.059412550181150436\n",
      "Epoch 5247/30000 Training Loss: 0.06493934988975525\n",
      "Epoch 5248/30000 Training Loss: 0.04966890811920166\n",
      "Epoch 5249/30000 Training Loss: 0.05260254815220833\n",
      "Epoch 5250/30000 Training Loss: 0.04685758799314499\n",
      "Epoch 5251/30000 Training Loss: 0.061477214097976685\n",
      "Epoch 5252/30000 Training Loss: 0.05128346383571625\n",
      "Epoch 5253/30000 Training Loss: 0.051689501851797104\n",
      "Epoch 5254/30000 Training Loss: 0.06531867384910583\n",
      "Epoch 5255/30000 Training Loss: 0.07188903540372849\n",
      "Epoch 5256/30000 Training Loss: 0.04179590940475464\n",
      "Epoch 5257/30000 Training Loss: 0.07022809237241745\n",
      "Epoch 5258/30000 Training Loss: 0.05839928239583969\n",
      "Epoch 5259/30000 Training Loss: 0.05355352163314819\n",
      "Epoch 5260/30000 Training Loss: 0.053753845393657684\n",
      "Epoch 5261/30000 Training Loss: 0.049511946737766266\n",
      "Epoch 5262/30000 Training Loss: 0.04808861017227173\n",
      "Epoch 5263/30000 Training Loss: 0.06576181203126907\n",
      "Epoch 5264/30000 Training Loss: 0.06845596432685852\n",
      "Epoch 5265/30000 Training Loss: 0.047227758914232254\n",
      "Epoch 5266/30000 Training Loss: 0.05167768895626068\n",
      "Epoch 5267/30000 Training Loss: 0.05051439628005028\n",
      "Epoch 5268/30000 Training Loss: 0.06155011057853699\n",
      "Epoch 5269/30000 Training Loss: 0.05825269967317581\n",
      "Epoch 5270/30000 Training Loss: 0.06265801936388016\n",
      "Epoch 5271/30000 Training Loss: 0.0506645143032074\n",
      "Epoch 5272/30000 Training Loss: 0.05666409805417061\n",
      "Epoch 5273/30000 Training Loss: 0.0619342140853405\n",
      "Epoch 5274/30000 Training Loss: 0.06684444844722748\n",
      "Epoch 5275/30000 Training Loss: 0.039222486317157745\n",
      "Epoch 5276/30000 Training Loss: 0.050399184226989746\n",
      "Epoch 5277/30000 Training Loss: 0.053641460835933685\n",
      "Epoch 5278/30000 Training Loss: 0.05676813796162605\n",
      "Epoch 5279/30000 Training Loss: 0.05160261690616608\n",
      "Epoch 5280/30000 Training Loss: 0.04458530992269516\n",
      "Epoch 5281/30000 Training Loss: 0.06597553938627243\n",
      "Epoch 5282/30000 Training Loss: 0.063144251704216\n",
      "Epoch 5283/30000 Training Loss: 0.05007357895374298\n",
      "Epoch 5284/30000 Training Loss: 0.05145394429564476\n",
      "Epoch 5285/30000 Training Loss: 0.05488184839487076\n",
      "Epoch 5286/30000 Training Loss: 0.0567946583032608\n",
      "Epoch 5287/30000 Training Loss: 0.07319052517414093\n",
      "Epoch 5288/30000 Training Loss: 0.055404771119356155\n",
      "Epoch 5289/30000 Training Loss: 0.0643153190612793\n",
      "Epoch 5290/30000 Training Loss: 0.04191938415169716\n",
      "Epoch 5291/30000 Training Loss: 0.059353120625019073\n",
      "Epoch 5292/30000 Training Loss: 0.06077929213643074\n",
      "Epoch 5293/30000 Training Loss: 0.06770340353250504\n",
      "Epoch 5294/30000 Training Loss: 0.04993705451488495\n",
      "Epoch 5295/30000 Training Loss: 0.05307752639055252\n",
      "Epoch 5296/30000 Training Loss: 0.046793095767498016\n",
      "Epoch 5297/30000 Training Loss: 0.05266260355710983\n",
      "Epoch 5298/30000 Training Loss: 0.051159389317035675\n",
      "Epoch 5299/30000 Training Loss: 0.057501181960105896\n",
      "Epoch 5300/30000 Training Loss: 0.060454268008470535\n",
      "Epoch 5300/30000 Validation Loss: 0.04721313714981079\n",
      "Epoch 5301/30000 Training Loss: 0.052229143679142\n",
      "Epoch 5302/30000 Training Loss: 0.0390186533331871\n",
      "Epoch 5303/30000 Training Loss: 0.039014704525470734\n",
      "Epoch 5304/30000 Training Loss: 0.05989496409893036\n",
      "Epoch 5305/30000 Training Loss: 0.0663178563117981\n",
      "Epoch 5306/30000 Training Loss: 0.05385460704565048\n",
      "Epoch 5307/30000 Training Loss: 0.052794553339481354\n",
      "Epoch 5308/30000 Training Loss: 0.05831354856491089\n",
      "Epoch 5309/30000 Training Loss: 0.04591570794582367\n",
      "Epoch 5310/30000 Training Loss: 0.042418286204338074\n",
      "Epoch 5311/30000 Training Loss: 0.059194415807724\n",
      "Epoch 5312/30000 Training Loss: 0.04801663011312485\n",
      "Epoch 5313/30000 Training Loss: 0.059978336095809937\n",
      "Epoch 5314/30000 Training Loss: 0.060082681477069855\n",
      "Epoch 5315/30000 Training Loss: 0.06342419236898422\n",
      "Epoch 5316/30000 Training Loss: 0.04992075264453888\n",
      "Epoch 5317/30000 Training Loss: 0.058610618114471436\n",
      "Epoch 5318/30000 Training Loss: 0.060083694756031036\n",
      "Epoch 5319/30000 Training Loss: 0.052053630352020264\n",
      "Epoch 5320/30000 Training Loss: 0.053529806435108185\n",
      "Epoch 5321/30000 Training Loss: 0.055366914719343185\n",
      "Epoch 5322/30000 Training Loss: 0.06019800901412964\n",
      "Epoch 5323/30000 Training Loss: 0.06852184236049652\n",
      "Epoch 5324/30000 Training Loss: 0.05349361151456833\n",
      "Epoch 5325/30000 Training Loss: 0.04836553335189819\n",
      "Epoch 5326/30000 Training Loss: 0.05540579929947853\n",
      "Epoch 5327/30000 Training Loss: 0.06956226378679276\n",
      "Epoch 5328/30000 Training Loss: 0.06551468372344971\n",
      "Epoch 5329/30000 Training Loss: 0.04583214223384857\n",
      "Epoch 5330/30000 Training Loss: 0.060165733098983765\n",
      "Epoch 5331/30000 Training Loss: 0.0529763326048851\n",
      "Epoch 5332/30000 Training Loss: 0.05549827218055725\n",
      "Epoch 5333/30000 Training Loss: 0.04870688170194626\n",
      "Epoch 5334/30000 Training Loss: 0.05897581949830055\n",
      "Epoch 5335/30000 Training Loss: 0.04185272008180618\n",
      "Epoch 5336/30000 Training Loss: 0.054144471883773804\n",
      "Epoch 5337/30000 Training Loss: 0.05233994871377945\n",
      "Epoch 5338/30000 Training Loss: 0.053963497281074524\n",
      "Epoch 5339/30000 Training Loss: 0.05260264128446579\n",
      "Epoch 5340/30000 Training Loss: 0.04641035944223404\n",
      "Epoch 5341/30000 Training Loss: 0.055349789559841156\n",
      "Epoch 5342/30000 Training Loss: 0.08150830864906311\n",
      "Epoch 5343/30000 Training Loss: 0.05630633234977722\n",
      "Epoch 5344/30000 Training Loss: 0.049413636326789856\n",
      "Epoch 5345/30000 Training Loss: 0.05653767287731171\n",
      "Epoch 5346/30000 Training Loss: 0.058869995176792145\n",
      "Epoch 5347/30000 Training Loss: 0.05435953289270401\n",
      "Epoch 5348/30000 Training Loss: 0.06056640297174454\n",
      "Epoch 5349/30000 Training Loss: 0.07482413947582245\n",
      "Epoch 5350/30000 Training Loss: 0.05733903497457504\n",
      "Epoch 5351/30000 Training Loss: 0.054054081439971924\n",
      "Epoch 5352/30000 Training Loss: 0.057736076414585114\n",
      "Epoch 5353/30000 Training Loss: 0.0554494634270668\n",
      "Epoch 5354/30000 Training Loss: 0.0625908300280571\n",
      "Epoch 5355/30000 Training Loss: 0.03920940309762955\n",
      "Epoch 5356/30000 Training Loss: 0.06557230651378632\n",
      "Epoch 5357/30000 Training Loss: 0.05695134401321411\n",
      "Epoch 5358/30000 Training Loss: 0.06782730668783188\n",
      "Epoch 5359/30000 Training Loss: 0.060340963304042816\n",
      "Epoch 5360/30000 Training Loss: 0.07136429846286774\n",
      "Epoch 5361/30000 Training Loss: 0.07108753174543381\n",
      "Epoch 5362/30000 Training Loss: 0.05614985525608063\n",
      "Epoch 5363/30000 Training Loss: 0.06888973712921143\n",
      "Epoch 5364/30000 Training Loss: 0.0653344988822937\n",
      "Epoch 5365/30000 Training Loss: 0.05785241723060608\n",
      "Epoch 5366/30000 Training Loss: 0.05324530228972435\n",
      "Epoch 5367/30000 Training Loss: 0.05565503239631653\n",
      "Epoch 5368/30000 Training Loss: 0.054287225008010864\n",
      "Epoch 5369/30000 Training Loss: 0.04318635165691376\n",
      "Epoch 5370/30000 Training Loss: 0.06608687341213226\n",
      "Epoch 5371/30000 Training Loss: 0.05637951195240021\n",
      "Epoch 5372/30000 Training Loss: 0.04734836891293526\n",
      "Epoch 5373/30000 Training Loss: 0.05250348895788193\n",
      "Epoch 5374/30000 Training Loss: 0.0607919879257679\n",
      "Epoch 5375/30000 Training Loss: 0.045006025582551956\n",
      "Epoch 5376/30000 Training Loss: 0.05060109496116638\n",
      "Epoch 5377/30000 Training Loss: 0.05263233184814453\n",
      "Epoch 5378/30000 Training Loss: 0.047784894704818726\n",
      "Epoch 5379/30000 Training Loss: 0.065015509724617\n",
      "Epoch 5380/30000 Training Loss: 0.041265811771154404\n",
      "Epoch 5381/30000 Training Loss: 0.06665698438882828\n",
      "Epoch 5382/30000 Training Loss: 0.05099979788064957\n",
      "Epoch 5383/30000 Training Loss: 0.04250704497098923\n",
      "Epoch 5384/30000 Training Loss: 0.049065832048654556\n",
      "Epoch 5385/30000 Training Loss: 0.06167524307966232\n",
      "Epoch 5386/30000 Training Loss: 0.052764102816581726\n",
      "Epoch 5387/30000 Training Loss: 0.053237445652484894\n",
      "Epoch 5388/30000 Training Loss: 0.051400888711214066\n",
      "Epoch 5389/30000 Training Loss: 0.05418369546532631\n",
      "Epoch 5390/30000 Training Loss: 0.05449237674474716\n",
      "Epoch 5391/30000 Training Loss: 0.06013726443052292\n",
      "Epoch 5392/30000 Training Loss: 0.045182861387729645\n",
      "Epoch 5393/30000 Training Loss: 0.050456322729587555\n",
      "Epoch 5394/30000 Training Loss: 0.05195443704724312\n",
      "Epoch 5395/30000 Training Loss: 0.0658332109451294\n",
      "Epoch 5396/30000 Training Loss: 0.05517372116446495\n",
      "Epoch 5397/30000 Training Loss: 0.06399784237146378\n",
      "Epoch 5398/30000 Training Loss: 0.06461416184902191\n",
      "Epoch 5399/30000 Training Loss: 0.05127664655447006\n",
      "Epoch 5400/30000 Training Loss: 0.04121647775173187\n",
      "Epoch 5400/30000 Validation Loss: 0.05117499828338623\n",
      "Epoch 5401/30000 Training Loss: 0.057220444083213806\n",
      "Epoch 5402/30000 Training Loss: 0.06132562458515167\n",
      "Epoch 5403/30000 Training Loss: 0.07919955253601074\n",
      "Epoch 5404/30000 Training Loss: 0.050142303109169006\n",
      "Epoch 5405/30000 Training Loss: 0.049097031354904175\n",
      "Epoch 5406/30000 Training Loss: 0.04276648908853531\n",
      "Epoch 5407/30000 Training Loss: 0.05201500654220581\n",
      "Epoch 5408/30000 Training Loss: 0.0525975227355957\n",
      "Epoch 5409/30000 Training Loss: 0.0663454532623291\n",
      "Epoch 5410/30000 Training Loss: 0.04373975098133087\n",
      "Epoch 5411/30000 Training Loss: 0.046891406178474426\n",
      "Epoch 5412/30000 Training Loss: 0.04727454110980034\n",
      "Epoch 5413/30000 Training Loss: 0.06307147443294525\n",
      "Epoch 5414/30000 Training Loss: 0.053154028952121735\n",
      "Epoch 5415/30000 Training Loss: 0.05004747211933136\n",
      "Epoch 5416/30000 Training Loss: 0.06306958943605423\n",
      "Epoch 5417/30000 Training Loss: 0.0623675137758255\n",
      "Epoch 5418/30000 Training Loss: 0.05896498262882233\n",
      "Epoch 5419/30000 Training Loss: 0.051512740552425385\n",
      "Epoch 5420/30000 Training Loss: 0.052357304841279984\n",
      "Epoch 5421/30000 Training Loss: 0.05648516118526459\n",
      "Epoch 5422/30000 Training Loss: 0.05950780212879181\n",
      "Epoch 5423/30000 Training Loss: 0.05956880748271942\n",
      "Epoch 5424/30000 Training Loss: 0.05334518849849701\n",
      "Epoch 5425/30000 Training Loss: 0.06021993234753609\n",
      "Epoch 5426/30000 Training Loss: 0.061682358384132385\n",
      "Epoch 5427/30000 Training Loss: 0.054521046578884125\n",
      "Epoch 5428/30000 Training Loss: 0.06243062764406204\n",
      "Epoch 5429/30000 Training Loss: 0.05311004817485809\n",
      "Epoch 5430/30000 Training Loss: 0.06218773126602173\n",
      "Epoch 5431/30000 Training Loss: 0.05059213191270828\n",
      "Epoch 5432/30000 Training Loss: 0.059651896357536316\n",
      "Epoch 5433/30000 Training Loss: 0.05089697614312172\n",
      "Epoch 5434/30000 Training Loss: 0.06278600543737411\n",
      "Epoch 5435/30000 Training Loss: 0.0466609001159668\n",
      "Epoch 5436/30000 Training Loss: 0.07190340757369995\n",
      "Epoch 5437/30000 Training Loss: 0.05445536971092224\n",
      "Epoch 5438/30000 Training Loss: 0.06581820547580719\n",
      "Epoch 5439/30000 Training Loss: 0.05688963830471039\n",
      "Epoch 5440/30000 Training Loss: 0.07001087069511414\n",
      "Epoch 5441/30000 Training Loss: 0.052570890635252\n",
      "Epoch 5442/30000 Training Loss: 0.04198033735156059\n",
      "Epoch 5443/30000 Training Loss: 0.05468372255563736\n",
      "Epoch 5444/30000 Training Loss: 0.06160512566566467\n",
      "Epoch 5445/30000 Training Loss: 0.055956676602363586\n",
      "Epoch 5446/30000 Training Loss: 0.06889774650335312\n",
      "Epoch 5447/30000 Training Loss: 0.0528506375849247\n",
      "Epoch 5448/30000 Training Loss: 0.052754539996385574\n",
      "Epoch 5449/30000 Training Loss: 0.05439923331141472\n",
      "Epoch 5450/30000 Training Loss: 0.04324308782815933\n",
      "Epoch 5451/30000 Training Loss: 0.04503859579563141\n",
      "Epoch 5452/30000 Training Loss: 0.039795055985450745\n",
      "Epoch 5453/30000 Training Loss: 0.0634622722864151\n",
      "Epoch 5454/30000 Training Loss: 0.04627782478928566\n",
      "Epoch 5455/30000 Training Loss: 0.057895347476005554\n",
      "Epoch 5456/30000 Training Loss: 0.05159652978181839\n",
      "Epoch 5457/30000 Training Loss: 0.05395566672086716\n",
      "Epoch 5458/30000 Training Loss: 0.04784759506583214\n",
      "Epoch 5459/30000 Training Loss: 0.0564577616751194\n",
      "Epoch 5460/30000 Training Loss: 0.07111643999814987\n",
      "Epoch 5461/30000 Training Loss: 0.04768865555524826\n",
      "Epoch 5462/30000 Training Loss: 0.04080384969711304\n",
      "Epoch 5463/30000 Training Loss: 0.049595270305871964\n",
      "Epoch 5464/30000 Training Loss: 0.06255710124969482\n",
      "Epoch 5465/30000 Training Loss: 0.05676110088825226\n",
      "Epoch 5466/30000 Training Loss: 0.06759586930274963\n",
      "Epoch 5467/30000 Training Loss: 0.06194248050451279\n",
      "Epoch 5468/30000 Training Loss: 0.054427411407232285\n",
      "Epoch 5469/30000 Training Loss: 0.06710975617170334\n",
      "Epoch 5470/30000 Training Loss: 0.05473092943429947\n",
      "Epoch 5471/30000 Training Loss: 0.059154488146305084\n",
      "Epoch 5472/30000 Training Loss: 0.0578555092215538\n",
      "Epoch 5473/30000 Training Loss: 0.05931656435132027\n",
      "Epoch 5474/30000 Training Loss: 0.04948359355330467\n",
      "Epoch 5475/30000 Training Loss: 0.05981665849685669\n",
      "Epoch 5476/30000 Training Loss: 0.05527929589152336\n",
      "Epoch 5477/30000 Training Loss: 0.06151407212018967\n",
      "Epoch 5478/30000 Training Loss: 0.056028641760349274\n",
      "Epoch 5479/30000 Training Loss: 0.06743104755878448\n",
      "Epoch 5480/30000 Training Loss: 0.06276378780603409\n",
      "Epoch 5481/30000 Training Loss: 0.07080542296171188\n",
      "Epoch 5482/30000 Training Loss: 0.050391945987939835\n",
      "Epoch 5483/30000 Training Loss: 0.0488433837890625\n",
      "Epoch 5484/30000 Training Loss: 0.051727741956710815\n",
      "Epoch 5485/30000 Training Loss: 0.05595602095127106\n",
      "Epoch 5486/30000 Training Loss: 0.055272623896598816\n",
      "Epoch 5487/30000 Training Loss: 0.06649494171142578\n",
      "Epoch 5488/30000 Training Loss: 0.04598868265748024\n",
      "Epoch 5489/30000 Training Loss: 0.05024342983961105\n",
      "Epoch 5490/30000 Training Loss: 0.047717563807964325\n",
      "Epoch 5491/30000 Training Loss: 0.05366193503141403\n",
      "Epoch 5492/30000 Training Loss: 0.05884786695241928\n",
      "Epoch 5493/30000 Training Loss: 0.04270616173744202\n",
      "Epoch 5494/30000 Training Loss: 0.06683123111724854\n",
      "Epoch 5495/30000 Training Loss: 0.05705581605434418\n",
      "Epoch 5496/30000 Training Loss: 0.045019663870334625\n",
      "Epoch 5497/30000 Training Loss: 0.0527057871222496\n",
      "Epoch 5498/30000 Training Loss: 0.05954552814364433\n",
      "Epoch 5499/30000 Training Loss: 0.0683184266090393\n",
      "Epoch 5500/30000 Training Loss: 0.07290734350681305\n",
      "Epoch 5500/30000 Validation Loss: 0.057628169655799866\n",
      "Epoch 5501/30000 Training Loss: 0.056589577347040176\n",
      "Epoch 5502/30000 Training Loss: 0.0567469596862793\n",
      "Epoch 5503/30000 Training Loss: 0.0661468356847763\n",
      "Epoch 5504/30000 Training Loss: 0.05060593783855438\n",
      "Epoch 5505/30000 Training Loss: 0.06152809411287308\n",
      "Epoch 5506/30000 Training Loss: 0.0641021728515625\n",
      "Epoch 5507/30000 Training Loss: 0.05693139508366585\n",
      "Epoch 5508/30000 Training Loss: 0.06376409530639648\n",
      "Epoch 5509/30000 Training Loss: 0.04560812562704086\n",
      "Epoch 5510/30000 Training Loss: 0.06403948366641998\n",
      "Epoch 5511/30000 Training Loss: 0.05136513710021973\n",
      "Epoch 5512/30000 Training Loss: 0.055342212319374084\n",
      "Epoch 5513/30000 Training Loss: 0.06889353692531586\n",
      "Epoch 5514/30000 Training Loss: 0.0504702590405941\n",
      "Epoch 5515/30000 Training Loss: 0.05435096472501755\n",
      "Epoch 5516/30000 Training Loss: 0.057729028165340424\n",
      "Epoch 5517/30000 Training Loss: 0.05429845303297043\n",
      "Epoch 5518/30000 Training Loss: 0.05579996109008789\n",
      "Epoch 5519/30000 Training Loss: 0.07996153831481934\n",
      "Epoch 5520/30000 Training Loss: 0.041689127683639526\n",
      "Epoch 5521/30000 Training Loss: 0.06294351071119308\n",
      "Epoch 5522/30000 Training Loss: 0.05724462494254112\n",
      "Epoch 5523/30000 Training Loss: 0.047427523881196976\n",
      "Epoch 5524/30000 Training Loss: 0.06361523270606995\n",
      "Epoch 5525/30000 Training Loss: 0.053540248423814774\n",
      "Epoch 5526/30000 Training Loss: 0.06570571660995483\n",
      "Epoch 5527/30000 Training Loss: 0.08278501033782959\n",
      "Epoch 5528/30000 Training Loss: 0.05510181561112404\n",
      "Epoch 5529/30000 Training Loss: 0.04545137286186218\n",
      "Epoch 5530/30000 Training Loss: 0.03965000435709953\n",
      "Epoch 5531/30000 Training Loss: 0.05424817278981209\n",
      "Epoch 5532/30000 Training Loss: 0.051094964146614075\n",
      "Epoch 5533/30000 Training Loss: 0.057448919862508774\n",
      "Epoch 5534/30000 Training Loss: 0.05974148213863373\n",
      "Epoch 5535/30000 Training Loss: 0.05311399698257446\n",
      "Epoch 5536/30000 Training Loss: 0.04846387356519699\n",
      "Epoch 5537/30000 Training Loss: 0.05817285180091858\n",
      "Epoch 5538/30000 Training Loss: 0.047789111733436584\n",
      "Epoch 5539/30000 Training Loss: 0.061580508947372437\n",
      "Epoch 5540/30000 Training Loss: 0.06728214025497437\n",
      "Epoch 5541/30000 Training Loss: 0.04662495106458664\n",
      "Epoch 5542/30000 Training Loss: 0.057175930589437485\n",
      "Epoch 5543/30000 Training Loss: 0.05636930838227272\n",
      "Epoch 5544/30000 Training Loss: 0.049291279166936874\n",
      "Epoch 5545/30000 Training Loss: 0.045037783682346344\n",
      "Epoch 5546/30000 Training Loss: 0.09256630390882492\n",
      "Epoch 5547/30000 Training Loss: 0.054363913834095\n",
      "Epoch 5548/30000 Training Loss: 0.05933944135904312\n",
      "Epoch 5549/30000 Training Loss: 0.06825955212116241\n",
      "Epoch 5550/30000 Training Loss: 0.06296665221452713\n",
      "Epoch 5551/30000 Training Loss: 0.05578913539648056\n",
      "Epoch 5552/30000 Training Loss: 0.07029059529304504\n",
      "Epoch 5553/30000 Training Loss: 0.07679310441017151\n",
      "Epoch 5554/30000 Training Loss: 0.0531434528529644\n",
      "Epoch 5555/30000 Training Loss: 0.04481876641511917\n",
      "Epoch 5556/30000 Training Loss: 0.0450914241373539\n",
      "Epoch 5557/30000 Training Loss: 0.04630105942487717\n",
      "Epoch 5558/30000 Training Loss: 0.044008005410432816\n",
      "Epoch 5559/30000 Training Loss: 0.06553959846496582\n",
      "Epoch 5560/30000 Training Loss: 0.05734559893608093\n",
      "Epoch 5561/30000 Training Loss: 0.06096599996089935\n",
      "Epoch 5562/30000 Training Loss: 0.061092860996723175\n",
      "Epoch 5563/30000 Training Loss: 0.04618154838681221\n",
      "Epoch 5564/30000 Training Loss: 0.06033118814229965\n",
      "Epoch 5565/30000 Training Loss: 0.044982701539993286\n",
      "Epoch 5566/30000 Training Loss: 0.04510010778903961\n",
      "Epoch 5567/30000 Training Loss: 0.057326383888721466\n",
      "Epoch 5568/30000 Training Loss: 0.06795984506607056\n",
      "Epoch 5569/30000 Training Loss: 0.05810420215129852\n",
      "Epoch 5570/30000 Training Loss: 0.056906163692474365\n",
      "Epoch 5571/30000 Training Loss: 0.04460575431585312\n",
      "Epoch 5572/30000 Training Loss: 0.056146927177906036\n",
      "Epoch 5573/30000 Training Loss: 0.05330580845475197\n",
      "Epoch 5574/30000 Training Loss: 0.0513443723320961\n",
      "Epoch 5575/30000 Training Loss: 0.0488281324505806\n",
      "Epoch 5576/30000 Training Loss: 0.05612945556640625\n",
      "Epoch 5577/30000 Training Loss: 0.06692367047071457\n",
      "Epoch 5578/30000 Training Loss: 0.051821500062942505\n",
      "Epoch 5579/30000 Training Loss: 0.04128342494368553\n",
      "Epoch 5580/30000 Training Loss: 0.07171287387609482\n",
      "Epoch 5581/30000 Training Loss: 0.06555699557065964\n",
      "Epoch 5582/30000 Training Loss: 0.06208813935518265\n",
      "Epoch 5583/30000 Training Loss: 0.05970010906457901\n",
      "Epoch 5584/30000 Training Loss: 0.07366520166397095\n",
      "Epoch 5585/30000 Training Loss: 0.05500928312540054\n",
      "Epoch 5586/30000 Training Loss: 0.04182900860905647\n",
      "Epoch 5587/30000 Training Loss: 0.05204709991812706\n",
      "Epoch 5588/30000 Training Loss: 0.05786135047674179\n",
      "Epoch 5589/30000 Training Loss: 0.06743248552083969\n",
      "Epoch 5590/30000 Training Loss: 0.060348980128765106\n",
      "Epoch 5591/30000 Training Loss: 0.050282567739486694\n",
      "Epoch 5592/30000 Training Loss: 0.052936144173145294\n",
      "Epoch 5593/30000 Training Loss: 0.04914824664592743\n",
      "Epoch 5594/30000 Training Loss: 0.04641110077500343\n",
      "Epoch 5595/30000 Training Loss: 0.05950261652469635\n",
      "Epoch 5596/30000 Training Loss: 0.05261722207069397\n",
      "Epoch 5597/30000 Training Loss: 0.05655216425657272\n",
      "Epoch 5598/30000 Training Loss: 0.048987895250320435\n",
      "Epoch 5599/30000 Training Loss: 0.04780655354261398\n",
      "Epoch 5600/30000 Training Loss: 0.043832018971443176\n",
      "Epoch 5600/30000 Validation Loss: 0.05289673060178757\n",
      "Epoch 5601/30000 Training Loss: 0.06329316645860672\n",
      "Epoch 5602/30000 Training Loss: 0.04233521223068237\n",
      "Epoch 5603/30000 Training Loss: 0.0494646392762661\n",
      "Epoch 5604/30000 Training Loss: 0.05591066926717758\n",
      "Epoch 5605/30000 Training Loss: 0.06610601395368576\n",
      "Epoch 5606/30000 Training Loss: 0.04594307392835617\n",
      "Epoch 5607/30000 Training Loss: 0.05658905953168869\n",
      "Epoch 5608/30000 Training Loss: 0.07266288250684738\n",
      "Epoch 5609/30000 Training Loss: 0.07508055120706558\n",
      "Epoch 5610/30000 Training Loss: 0.0460326112806797\n",
      "Epoch 5611/30000 Training Loss: 0.05011192709207535\n",
      "Epoch 5612/30000 Training Loss: 0.046380702406167984\n",
      "Epoch 5613/30000 Training Loss: 0.06561275571584702\n",
      "Epoch 5614/30000 Training Loss: 0.05269033834338188\n",
      "Epoch 5615/30000 Training Loss: 0.04171481728553772\n",
      "Epoch 5616/30000 Training Loss: 0.05937628448009491\n",
      "Epoch 5617/30000 Training Loss: 0.06186964362859726\n",
      "Epoch 5618/30000 Training Loss: 0.04630901664495468\n",
      "Epoch 5619/30000 Training Loss: 0.0496339350938797\n",
      "Epoch 5620/30000 Training Loss: 0.061298537999391556\n",
      "Epoch 5621/30000 Training Loss: 0.0690629631280899\n",
      "Epoch 5622/30000 Training Loss: 0.054198022931814194\n",
      "Epoch 5623/30000 Training Loss: 0.07590454071760178\n",
      "Epoch 5624/30000 Training Loss: 0.05658814311027527\n",
      "Epoch 5625/30000 Training Loss: 0.04830928146839142\n",
      "Epoch 5626/30000 Training Loss: 0.05226598680019379\n",
      "Epoch 5627/30000 Training Loss: 0.05250006914138794\n",
      "Epoch 5628/30000 Training Loss: 0.050049036741256714\n",
      "Epoch 5629/30000 Training Loss: 0.054506972432136536\n",
      "Epoch 5630/30000 Training Loss: 0.06315519660711288\n",
      "Epoch 5631/30000 Training Loss: 0.062033019959926605\n",
      "Epoch 5632/30000 Training Loss: 0.0500563345849514\n",
      "Epoch 5633/30000 Training Loss: 0.0597166083753109\n",
      "Epoch 5634/30000 Training Loss: 0.06619247049093246\n",
      "Epoch 5635/30000 Training Loss: 0.06206770986318588\n",
      "Epoch 5636/30000 Training Loss: 0.07179591804742813\n",
      "Epoch 5637/30000 Training Loss: 0.0739365965127945\n",
      "Epoch 5638/30000 Training Loss: 0.05436321720480919\n",
      "Epoch 5639/30000 Training Loss: 0.04070340096950531\n",
      "Epoch 5640/30000 Training Loss: 0.04863273352384567\n",
      "Epoch 5641/30000 Training Loss: 0.05947710573673248\n",
      "Epoch 5642/30000 Training Loss: 0.039517611265182495\n",
      "Epoch 5643/30000 Training Loss: 0.05178019404411316\n",
      "Epoch 5644/30000 Training Loss: 0.06021891534328461\n",
      "Epoch 5645/30000 Training Loss: 0.04478669911623001\n",
      "Epoch 5646/30000 Training Loss: 0.06660164892673492\n",
      "Epoch 5647/30000 Training Loss: 0.05114242434501648\n",
      "Epoch 5648/30000 Training Loss: 0.05232527107000351\n",
      "Epoch 5649/30000 Training Loss: 0.04494519904255867\n",
      "Epoch 5650/30000 Training Loss: 0.0467769056558609\n",
      "Epoch 5651/30000 Training Loss: 0.06818234175443649\n",
      "Epoch 5652/30000 Training Loss: 0.04663969576358795\n",
      "Epoch 5653/30000 Training Loss: 0.05131518468260765\n",
      "Epoch 5654/30000 Training Loss: 0.060287993401288986\n",
      "Epoch 5655/30000 Training Loss: 0.06192527338862419\n",
      "Epoch 5656/30000 Training Loss: 0.0672423243522644\n",
      "Epoch 5657/30000 Training Loss: 0.05366437882184982\n",
      "Epoch 5658/30000 Training Loss: 0.05125511437654495\n",
      "Epoch 5659/30000 Training Loss: 0.061963457614183426\n",
      "Epoch 5660/30000 Training Loss: 0.0521063432097435\n",
      "Epoch 5661/30000 Training Loss: 0.05622567981481552\n",
      "Epoch 5662/30000 Training Loss: 0.047205884009599686\n",
      "Epoch 5663/30000 Training Loss: 0.052701614797115326\n",
      "Epoch 5664/30000 Training Loss: 0.05578567832708359\n",
      "Epoch 5665/30000 Training Loss: 0.04557187855243683\n",
      "Epoch 5666/30000 Training Loss: 0.05253118276596069\n",
      "Epoch 5667/30000 Training Loss: 0.056220341473817825\n",
      "Epoch 5668/30000 Training Loss: 0.0634307935833931\n",
      "Epoch 5669/30000 Training Loss: 0.06517686694860458\n",
      "Epoch 5670/30000 Training Loss: 0.05494725704193115\n",
      "Epoch 5671/30000 Training Loss: 0.04374242573976517\n",
      "Epoch 5672/30000 Training Loss: 0.04902096092700958\n",
      "Epoch 5673/30000 Training Loss: 0.04511988162994385\n",
      "Epoch 5674/30000 Training Loss: 0.05904442071914673\n",
      "Epoch 5675/30000 Training Loss: 0.05334734171628952\n",
      "Epoch 5676/30000 Training Loss: 0.061899520456790924\n",
      "Epoch 5677/30000 Training Loss: 0.05375944823026657\n",
      "Epoch 5678/30000 Training Loss: 0.05104917660355568\n",
      "Epoch 5679/30000 Training Loss: 0.0697794035077095\n",
      "Epoch 5680/30000 Training Loss: 0.04691639170050621\n",
      "Epoch 5681/30000 Training Loss: 0.062014080584049225\n",
      "Epoch 5682/30000 Training Loss: 0.04400835558772087\n",
      "Epoch 5683/30000 Training Loss: 0.05136604607105255\n",
      "Epoch 5684/30000 Training Loss: 0.056558527052402496\n",
      "Epoch 5685/30000 Training Loss: 0.07699131965637207\n",
      "Epoch 5686/30000 Training Loss: 0.05857386440038681\n",
      "Epoch 5687/30000 Training Loss: 0.04764998331665993\n",
      "Epoch 5688/30000 Training Loss: 0.05597417801618576\n",
      "Epoch 5689/30000 Training Loss: 0.04754845052957535\n",
      "Epoch 5690/30000 Training Loss: 0.049357205629348755\n",
      "Epoch 5691/30000 Training Loss: 0.05067526921629906\n",
      "Epoch 5692/30000 Training Loss: 0.06395953893661499\n",
      "Epoch 5693/30000 Training Loss: 0.054708242416381836\n",
      "Epoch 5694/30000 Training Loss: 0.05555224418640137\n",
      "Epoch 5695/30000 Training Loss: 0.06998324394226074\n",
      "Epoch 5696/30000 Training Loss: 0.06738278269767761\n",
      "Epoch 5697/30000 Training Loss: 0.07098716497421265\n",
      "Epoch 5698/30000 Training Loss: 0.0706218034029007\n",
      "Epoch 5699/30000 Training Loss: 0.06056322157382965\n",
      "Epoch 5700/30000 Training Loss: 0.04709695279598236\n",
      "Epoch 5700/30000 Validation Loss: 0.06750205159187317\n",
      "Epoch 5701/30000 Training Loss: 0.050237201154232025\n",
      "Epoch 5702/30000 Training Loss: 0.04192113131284714\n",
      "Epoch 5703/30000 Training Loss: 0.056666404008865356\n",
      "Epoch 5704/30000 Training Loss: 0.05086587369441986\n",
      "Epoch 5705/30000 Training Loss: 0.06858912110328674\n",
      "Epoch 5706/30000 Training Loss: 0.058275189250707626\n",
      "Epoch 5707/30000 Training Loss: 0.06175406649708748\n",
      "Epoch 5708/30000 Training Loss: 0.06129136681556702\n",
      "Epoch 5709/30000 Training Loss: 0.040490806102752686\n",
      "Epoch 5710/30000 Training Loss: 0.0461164265871048\n",
      "Epoch 5711/30000 Training Loss: 0.07062828540802002\n",
      "Epoch 5712/30000 Training Loss: 0.053638629615306854\n",
      "Epoch 5713/30000 Training Loss: 0.05507129430770874\n",
      "Epoch 5714/30000 Training Loss: 0.06826400756835938\n",
      "Epoch 5715/30000 Training Loss: 0.05593915283679962\n",
      "Epoch 5716/30000 Training Loss: 0.05795717239379883\n",
      "Epoch 5717/30000 Training Loss: 0.07358025759458542\n",
      "Epoch 5718/30000 Training Loss: 0.039104655385017395\n",
      "Epoch 5719/30000 Training Loss: 0.05002836138010025\n",
      "Epoch 5720/30000 Training Loss: 0.04283062368631363\n",
      "Epoch 5721/30000 Training Loss: 0.06080637127161026\n",
      "Epoch 5722/30000 Training Loss: 0.05239567160606384\n",
      "Epoch 5723/30000 Training Loss: 0.056959956884384155\n",
      "Epoch 5724/30000 Training Loss: 0.047182098031044006\n",
      "Epoch 5725/30000 Training Loss: 0.054429564625024796\n",
      "Epoch 5726/30000 Training Loss: 0.05556748807430267\n",
      "Epoch 5727/30000 Training Loss: 0.06261758506298065\n",
      "Epoch 5728/30000 Training Loss: 0.05504543334245682\n",
      "Epoch 5729/30000 Training Loss: 0.04692921042442322\n",
      "Epoch 5730/30000 Training Loss: 0.049915995448827744\n",
      "Epoch 5731/30000 Training Loss: 0.05061851814389229\n",
      "Epoch 5732/30000 Training Loss: 0.055364564061164856\n",
      "Epoch 5733/30000 Training Loss: 0.06090063601732254\n",
      "Epoch 5734/30000 Training Loss: 0.06585657596588135\n",
      "Epoch 5735/30000 Training Loss: 0.05571611598134041\n",
      "Epoch 5736/30000 Training Loss: 0.05906296521425247\n",
      "Epoch 5737/30000 Training Loss: 0.07008790224790573\n",
      "Epoch 5738/30000 Training Loss: 0.056001342833042145\n",
      "Epoch 5739/30000 Training Loss: 0.04545005038380623\n",
      "Epoch 5740/30000 Training Loss: 0.06607595086097717\n",
      "Epoch 5741/30000 Training Loss: 0.05293441191315651\n",
      "Epoch 5742/30000 Training Loss: 0.05001073703169823\n",
      "Epoch 5743/30000 Training Loss: 0.06564682722091675\n",
      "Epoch 5744/30000 Training Loss: 0.05394846200942993\n",
      "Epoch 5745/30000 Training Loss: 0.05538085848093033\n",
      "Epoch 5746/30000 Training Loss: 0.055920764803886414\n",
      "Epoch 5747/30000 Training Loss: 0.04340413957834244\n",
      "Epoch 5748/30000 Training Loss: 0.0476333387196064\n",
      "Epoch 5749/30000 Training Loss: 0.0517372228205204\n",
      "Epoch 5750/30000 Training Loss: 0.06471964716911316\n",
      "Epoch 5751/30000 Training Loss: 0.06898240745067596\n",
      "Epoch 5752/30000 Training Loss: 0.05030693858861923\n",
      "Epoch 5753/30000 Training Loss: 0.07143349945545197\n",
      "Epoch 5754/30000 Training Loss: 0.05001327395439148\n",
      "Epoch 5755/30000 Training Loss: 0.05544199049472809\n",
      "Epoch 5756/30000 Training Loss: 0.05288267135620117\n",
      "Epoch 5757/30000 Training Loss: 0.055744096636772156\n",
      "Epoch 5758/30000 Training Loss: 0.0469769611954689\n",
      "Epoch 5759/30000 Training Loss: 0.05888880044221878\n",
      "Epoch 5760/30000 Training Loss: 0.0559537410736084\n",
      "Epoch 5761/30000 Training Loss: 0.048026539385318756\n",
      "Epoch 5762/30000 Training Loss: 0.05755184590816498\n",
      "Epoch 5763/30000 Training Loss: 0.052728019654750824\n",
      "Epoch 5764/30000 Training Loss: 0.060543715953826904\n",
      "Epoch 5765/30000 Training Loss: 0.08060650527477264\n",
      "Epoch 5766/30000 Training Loss: 0.04714806377887726\n",
      "Epoch 5767/30000 Training Loss: 0.05203739553689957\n",
      "Epoch 5768/30000 Training Loss: 0.058032553642988205\n",
      "Epoch 5769/30000 Training Loss: 0.061774589121341705\n",
      "Epoch 5770/30000 Training Loss: 0.05539151281118393\n",
      "Epoch 5771/30000 Training Loss: 0.04488980025053024\n",
      "Epoch 5772/30000 Training Loss: 0.05616077408194542\n",
      "Epoch 5773/30000 Training Loss: 0.04137244075536728\n",
      "Epoch 5774/30000 Training Loss: 0.04038073122501373\n",
      "Epoch 5775/30000 Training Loss: 0.05606662482023239\n",
      "Epoch 5776/30000 Training Loss: 0.058378666639328\n",
      "Epoch 5777/30000 Training Loss: 0.04553104192018509\n",
      "Epoch 5778/30000 Training Loss: 0.050146885216236115\n",
      "Epoch 5779/30000 Training Loss: 0.07473675161600113\n",
      "Epoch 5780/30000 Training Loss: 0.04626775532960892\n",
      "Epoch 5781/30000 Training Loss: 0.061880528926849365\n",
      "Epoch 5782/30000 Training Loss: 0.056250739842653275\n",
      "Epoch 5783/30000 Training Loss: 0.0578792467713356\n",
      "Epoch 5784/30000 Training Loss: 0.042812198400497437\n",
      "Epoch 5785/30000 Training Loss: 0.0482809841632843\n",
      "Epoch 5786/30000 Training Loss: 0.061467379331588745\n",
      "Epoch 5787/30000 Training Loss: 0.05733116716146469\n",
      "Epoch 5788/30000 Training Loss: 0.04434589669108391\n",
      "Epoch 5789/30000 Training Loss: 0.0656544491648674\n",
      "Epoch 5790/30000 Training Loss: 0.056771934032440186\n",
      "Epoch 5791/30000 Training Loss: 0.06158781796693802\n",
      "Epoch 5792/30000 Training Loss: 0.057989947497844696\n",
      "Epoch 5793/30000 Training Loss: 0.05355939641594887\n",
      "Epoch 5794/30000 Training Loss: 0.059654731303453445\n",
      "Epoch 5795/30000 Training Loss: 0.050333090126514435\n",
      "Epoch 5796/30000 Training Loss: 0.046065784990787506\n",
      "Epoch 5797/30000 Training Loss: 0.05875769257545471\n",
      "Epoch 5798/30000 Training Loss: 0.05107314884662628\n",
      "Epoch 5799/30000 Training Loss: 0.053518183529376984\n",
      "Epoch 5800/30000 Training Loss: 0.07231774926185608\n",
      "Epoch 5800/30000 Validation Loss: 0.06195596978068352\n",
      "Epoch 5801/30000 Training Loss: 0.04689154773950577\n",
      "Epoch 5802/30000 Training Loss: 0.04654508829116821\n",
      "Epoch 5803/30000 Training Loss: 0.05750305950641632\n",
      "Epoch 5804/30000 Training Loss: 0.05667342245578766\n",
      "Epoch 5805/30000 Training Loss: 0.04821368306875229\n",
      "Epoch 5806/30000 Training Loss: 0.05810919031500816\n",
      "Epoch 5807/30000 Training Loss: 0.061237722635269165\n",
      "Epoch 5808/30000 Training Loss: 0.06112196296453476\n",
      "Epoch 5809/30000 Training Loss: 0.05789179727435112\n",
      "Epoch 5810/30000 Training Loss: 0.055276550352573395\n",
      "Epoch 5811/30000 Training Loss: 0.06645183265209198\n",
      "Epoch 5812/30000 Training Loss: 0.047528576105833054\n",
      "Epoch 5813/30000 Training Loss: 0.06448434293270111\n",
      "Epoch 5814/30000 Training Loss: 0.05650486797094345\n",
      "Epoch 5815/30000 Training Loss: 0.06145577132701874\n",
      "Epoch 5816/30000 Training Loss: 0.05876573920249939\n",
      "Epoch 5817/30000 Training Loss: 0.047337085008621216\n",
      "Epoch 5818/30000 Training Loss: 0.0648021325469017\n",
      "Epoch 5819/30000 Training Loss: 0.05409979075193405\n",
      "Epoch 5820/30000 Training Loss: 0.049313388764858246\n",
      "Epoch 5821/30000 Training Loss: 0.05534869804978371\n",
      "Epoch 5822/30000 Training Loss: 0.0499996691942215\n",
      "Epoch 5823/30000 Training Loss: 0.05362027883529663\n",
      "Epoch 5824/30000 Training Loss: 0.07010800391435623\n",
      "Epoch 5825/30000 Training Loss: 0.07069318741559982\n",
      "Epoch 5826/30000 Training Loss: 0.057736001908779144\n",
      "Epoch 5827/30000 Training Loss: 0.0563591867685318\n",
      "Epoch 5828/30000 Training Loss: 0.07096869498491287\n",
      "Epoch 5829/30000 Training Loss: 0.05510060489177704\n",
      "Epoch 5830/30000 Training Loss: 0.04688308387994766\n",
      "Epoch 5831/30000 Training Loss: 0.05557050555944443\n",
      "Epoch 5832/30000 Training Loss: 0.04419407993555069\n",
      "Epoch 5833/30000 Training Loss: 0.05993043631315231\n",
      "Epoch 5834/30000 Training Loss: 0.051547273993492126\n",
      "Epoch 5835/30000 Training Loss: 0.05854157358407974\n",
      "Epoch 5836/30000 Training Loss: 0.05202288180589676\n",
      "Epoch 5837/30000 Training Loss: 0.06429389864206314\n",
      "Epoch 5838/30000 Training Loss: 0.05394018068909645\n",
      "Epoch 5839/30000 Training Loss: 0.05999355763196945\n",
      "Epoch 5840/30000 Training Loss: 0.050481535494327545\n",
      "Epoch 5841/30000 Training Loss: 0.039763785898685455\n",
      "Epoch 5842/30000 Training Loss: 0.06203876808285713\n",
      "Epoch 5843/30000 Training Loss: 0.07274322956800461\n",
      "Epoch 5844/30000 Training Loss: 0.04752137511968613\n",
      "Epoch 5845/30000 Training Loss: 0.04315365105867386\n",
      "Epoch 5846/30000 Training Loss: 0.03772086650133133\n",
      "Epoch 5847/30000 Training Loss: 0.07526883482933044\n",
      "Epoch 5848/30000 Training Loss: 0.05624525621533394\n",
      "Epoch 5849/30000 Training Loss: 0.06079166755080223\n",
      "Epoch 5850/30000 Training Loss: 0.06309816986322403\n",
      "Epoch 5851/30000 Training Loss: 0.05119737982749939\n",
      "Epoch 5852/30000 Training Loss: 0.05303584411740303\n",
      "Epoch 5853/30000 Training Loss: 0.044078558683395386\n",
      "Epoch 5854/30000 Training Loss: 0.05657646059989929\n",
      "Epoch 5855/30000 Training Loss: 0.05977236479520798\n",
      "Epoch 5856/30000 Training Loss: 0.05262841284275055\n",
      "Epoch 5857/30000 Training Loss: 0.05585144832730293\n",
      "Epoch 5858/30000 Training Loss: 0.05961248278617859\n",
      "Epoch 5859/30000 Training Loss: 0.05139191448688507\n",
      "Epoch 5860/30000 Training Loss: 0.06659851968288422\n",
      "Epoch 5861/30000 Training Loss: 0.06161566078662872\n",
      "Epoch 5862/30000 Training Loss: 0.056809864938259125\n",
      "Epoch 5863/30000 Training Loss: 0.060121841728687286\n",
      "Epoch 5864/30000 Training Loss: 0.05011460557579994\n",
      "Epoch 5865/30000 Training Loss: 0.04489738121628761\n",
      "Epoch 5866/30000 Training Loss: 0.058428600430488586\n",
      "Epoch 5867/30000 Training Loss: 0.05462712049484253\n",
      "Epoch 5868/30000 Training Loss: 0.058645136654376984\n",
      "Epoch 5869/30000 Training Loss: 0.05755479261279106\n",
      "Epoch 5870/30000 Training Loss: 0.05682629346847534\n",
      "Epoch 5871/30000 Training Loss: 0.047981858253479004\n",
      "Epoch 5872/30000 Training Loss: 0.07357320934534073\n",
      "Epoch 5873/30000 Training Loss: 0.05143902450799942\n",
      "Epoch 5874/30000 Training Loss: 0.049919940531253815\n",
      "Epoch 5875/30000 Training Loss: 0.05821940302848816\n",
      "Epoch 5876/30000 Training Loss: 0.046367205679416656\n",
      "Epoch 5877/30000 Training Loss: 0.06035665422677994\n",
      "Epoch 5878/30000 Training Loss: 0.03622915595769882\n",
      "Epoch 5879/30000 Training Loss: 0.041476018726825714\n",
      "Epoch 5880/30000 Training Loss: 0.042038336396217346\n",
      "Epoch 5881/30000 Training Loss: 0.058454252779483795\n",
      "Epoch 5882/30000 Training Loss: 0.0531531423330307\n",
      "Epoch 5883/30000 Training Loss: 0.05302733927965164\n",
      "Epoch 5884/30000 Training Loss: 0.062180425971746445\n",
      "Epoch 5885/30000 Training Loss: 0.039962343871593475\n",
      "Epoch 5886/30000 Training Loss: 0.04193197563290596\n",
      "Epoch 5887/30000 Training Loss: 0.08849530667066574\n",
      "Epoch 5888/30000 Training Loss: 0.05637147277593613\n",
      "Epoch 5889/30000 Training Loss: 0.06993108242750168\n",
      "Epoch 5890/30000 Training Loss: 0.05946178734302521\n",
      "Epoch 5891/30000 Training Loss: 0.07049326598644257\n",
      "Epoch 5892/30000 Training Loss: 0.06069055199623108\n",
      "Epoch 5893/30000 Training Loss: 0.0613371878862381\n",
      "Epoch 5894/30000 Training Loss: 0.041496217250823975\n",
      "Epoch 5895/30000 Training Loss: 0.04579376429319382\n",
      "Epoch 5896/30000 Training Loss: 0.05145719647407532\n",
      "Epoch 5897/30000 Training Loss: 0.05714229494333267\n",
      "Epoch 5898/30000 Training Loss: 0.05915021896362305\n",
      "Epoch 5899/30000 Training Loss: 0.058307066559791565\n",
      "Epoch 5900/30000 Training Loss: 0.05572284758090973\n",
      "Epoch 5900/30000 Validation Loss: 0.05276625230908394\n",
      "Epoch 5901/30000 Training Loss: 0.06772450357675552\n",
      "Epoch 5902/30000 Training Loss: 0.05108027160167694\n",
      "Epoch 5903/30000 Training Loss: 0.0419132299721241\n",
      "Epoch 5904/30000 Training Loss: 0.06911084055900574\n",
      "Epoch 5905/30000 Training Loss: 0.06116965040564537\n",
      "Epoch 5906/30000 Training Loss: 0.0678485631942749\n",
      "Epoch 5907/30000 Training Loss: 0.05847448110580444\n",
      "Epoch 5908/30000 Training Loss: 0.06579265743494034\n",
      "Epoch 5909/30000 Training Loss: 0.05252338945865631\n",
      "Epoch 5910/30000 Training Loss: 0.04794876277446747\n",
      "Epoch 5911/30000 Training Loss: 0.04392015561461449\n",
      "Epoch 5912/30000 Training Loss: 0.06765231490135193\n",
      "Epoch 5913/30000 Training Loss: 0.05072366073727608\n",
      "Epoch 5914/30000 Training Loss: 0.0594274178147316\n",
      "Epoch 5915/30000 Training Loss: 0.06068585067987442\n",
      "Epoch 5916/30000 Training Loss: 0.05013604462146759\n",
      "Epoch 5917/30000 Training Loss: 0.06998072564601898\n",
      "Epoch 5918/30000 Training Loss: 0.04942081868648529\n",
      "Epoch 5919/30000 Training Loss: 0.05141637474298477\n",
      "Epoch 5920/30000 Training Loss: 0.0620163157582283\n",
      "Epoch 5921/30000 Training Loss: 0.061326779425144196\n",
      "Epoch 5922/30000 Training Loss: 0.04223300889134407\n",
      "Epoch 5923/30000 Training Loss: 0.05578990280628204\n",
      "Epoch 5924/30000 Training Loss: 0.058627016842365265\n",
      "Epoch 5925/30000 Training Loss: 0.05372826009988785\n",
      "Epoch 5926/30000 Training Loss: 0.04860647767782211\n",
      "Epoch 5927/30000 Training Loss: 0.047300830483436584\n",
      "Epoch 5928/30000 Training Loss: 0.03829080983996391\n",
      "Epoch 5929/30000 Training Loss: 0.07303765416145325\n",
      "Epoch 5930/30000 Training Loss: 0.0671139732003212\n",
      "Epoch 5931/30000 Training Loss: 0.05315591022372246\n",
      "Epoch 5932/30000 Training Loss: 0.05385042726993561\n",
      "Epoch 5933/30000 Training Loss: 0.06671024858951569\n",
      "Epoch 5934/30000 Training Loss: 0.04935962334275246\n",
      "Epoch 5935/30000 Training Loss: 0.06799987703561783\n",
      "Epoch 5936/30000 Training Loss: 0.05524212121963501\n",
      "Epoch 5937/30000 Training Loss: 0.06604409217834473\n",
      "Epoch 5938/30000 Training Loss: 0.052654899656772614\n",
      "Epoch 5939/30000 Training Loss: 0.05552965775132179\n",
      "Epoch 5940/30000 Training Loss: 0.058350685983896255\n",
      "Epoch 5941/30000 Training Loss: 0.047909919172525406\n",
      "Epoch 5942/30000 Training Loss: 0.06514851748943329\n",
      "Epoch 5943/30000 Training Loss: 0.05293881893157959\n",
      "Epoch 5944/30000 Training Loss: 0.05617028474807739\n",
      "Epoch 5945/30000 Training Loss: 0.055560752749443054\n",
      "Epoch 5946/30000 Training Loss: 0.05635788291692734\n",
      "Epoch 5947/30000 Training Loss: 0.05421307682991028\n",
      "Epoch 5948/30000 Training Loss: 0.05771101638674736\n",
      "Epoch 5949/30000 Training Loss: 0.0701555609703064\n",
      "Epoch 5950/30000 Training Loss: 0.05001388490200043\n",
      "Epoch 5951/30000 Training Loss: 0.0558384507894516\n",
      "Epoch 5952/30000 Training Loss: 0.05056517571210861\n",
      "Epoch 5953/30000 Training Loss: 0.07215692102909088\n",
      "Epoch 5954/30000 Training Loss: 0.04409773647785187\n",
      "Epoch 5955/30000 Training Loss: 0.08091554790735245\n",
      "Epoch 5956/30000 Training Loss: 0.05980151891708374\n",
      "Epoch 5957/30000 Training Loss: 0.05426777899265289\n",
      "Epoch 5958/30000 Training Loss: 0.043699897825717926\n",
      "Epoch 5959/30000 Training Loss: 0.04772554337978363\n",
      "Epoch 5960/30000 Training Loss: 0.06075729802250862\n",
      "Epoch 5961/30000 Training Loss: 0.04668855667114258\n",
      "Epoch 5962/30000 Training Loss: 0.04640254005789757\n",
      "Epoch 5963/30000 Training Loss: 0.06098005920648575\n",
      "Epoch 5964/30000 Training Loss: 0.05893959477543831\n",
      "Epoch 5965/30000 Training Loss: 0.04883565008640289\n",
      "Epoch 5966/30000 Training Loss: 0.045688726007938385\n",
      "Epoch 5967/30000 Training Loss: 0.04210139065980911\n",
      "Epoch 5968/30000 Training Loss: 0.05070682242512703\n",
      "Epoch 5969/30000 Training Loss: 0.0514330118894577\n",
      "Epoch 5970/30000 Training Loss: 0.04835085570812225\n",
      "Epoch 5971/30000 Training Loss: 0.056396692991256714\n",
      "Epoch 5972/30000 Training Loss: 0.054690491408109665\n",
      "Epoch 5973/30000 Training Loss: 0.048950664699077606\n",
      "Epoch 5974/30000 Training Loss: 0.05283915251493454\n",
      "Epoch 5975/30000 Training Loss: 0.05181329697370529\n",
      "Epoch 5976/30000 Training Loss: 0.04299045354127884\n",
      "Epoch 5977/30000 Training Loss: 0.04990828409790993\n",
      "Epoch 5978/30000 Training Loss: 0.05332033336162567\n",
      "Epoch 5979/30000 Training Loss: 0.05249203369021416\n",
      "Epoch 5980/30000 Training Loss: 0.05910687893629074\n",
      "Epoch 5981/30000 Training Loss: 0.051047228276729584\n",
      "Epoch 5982/30000 Training Loss: 0.05052470415830612\n",
      "Epoch 5983/30000 Training Loss: 0.059390682727098465\n",
      "Epoch 5984/30000 Training Loss: 0.04762113094329834\n",
      "Epoch 5985/30000 Training Loss: 0.056505393236875534\n",
      "Epoch 5986/30000 Training Loss: 0.04400983825325966\n",
      "Epoch 5987/30000 Training Loss: 0.0550696924328804\n",
      "Epoch 5988/30000 Training Loss: 0.05258797109127045\n",
      "Epoch 5989/30000 Training Loss: 0.06614995002746582\n",
      "Epoch 5990/30000 Training Loss: 0.047425344586372375\n",
      "Epoch 5991/30000 Training Loss: 0.06740564107894897\n",
      "Epoch 5992/30000 Training Loss: 0.0485885813832283\n",
      "Epoch 5993/30000 Training Loss: 0.048969827592372894\n",
      "Epoch 5994/30000 Training Loss: 0.047922030091285706\n",
      "Epoch 5995/30000 Training Loss: 0.058063752949237823\n",
      "Epoch 5996/30000 Training Loss: 0.06126044690608978\n",
      "Epoch 5997/30000 Training Loss: 0.04313172772526741\n",
      "Epoch 5998/30000 Training Loss: 0.06280068308115005\n",
      "Epoch 5999/30000 Training Loss: 0.049189243465662\n",
      "Epoch 6000/30000 Training Loss: 0.05893322080373764\n",
      "Epoch 6000/30000 Validation Loss: 0.06018301844596863\n",
      "Epoch 6001/30000 Training Loss: 0.05260075256228447\n",
      "Epoch 6002/30000 Training Loss: 0.0505005307495594\n",
      "Epoch 6003/30000 Training Loss: 0.058057259768247604\n",
      "Epoch 6004/30000 Training Loss: 0.04556398093700409\n",
      "Epoch 6005/30000 Training Loss: 0.04899938777089119\n",
      "Epoch 6006/30000 Training Loss: 0.05802896246314049\n",
      "Epoch 6007/30000 Training Loss: 0.05983797833323479\n",
      "Epoch 6008/30000 Training Loss: 0.060652561485767365\n",
      "Epoch 6009/30000 Training Loss: 0.05673827975988388\n",
      "Epoch 6010/30000 Training Loss: 0.049518465995788574\n",
      "Epoch 6011/30000 Training Loss: 0.042929381132125854\n",
      "Epoch 6012/30000 Training Loss: 0.06663960218429565\n",
      "Epoch 6013/30000 Training Loss: 0.06499186158180237\n",
      "Epoch 6014/30000 Training Loss: 0.051114145666360855\n",
      "Epoch 6015/30000 Training Loss: 0.04388650506734848\n",
      "Epoch 6016/30000 Training Loss: 0.07297681272029877\n",
      "Epoch 6017/30000 Training Loss: 0.05342470109462738\n",
      "Epoch 6018/30000 Training Loss: 0.04435797780752182\n",
      "Epoch 6019/30000 Training Loss: 0.05564627796411514\n",
      "Epoch 6020/30000 Training Loss: 0.058131009340286255\n",
      "Epoch 6021/30000 Training Loss: 0.05146058648824692\n",
      "Epoch 6022/30000 Training Loss: 0.0567002035677433\n",
      "Epoch 6023/30000 Training Loss: 0.05888717621564865\n",
      "Epoch 6024/30000 Training Loss: 0.054432835429906845\n",
      "Epoch 6025/30000 Training Loss: 0.0443253293633461\n",
      "Epoch 6026/30000 Training Loss: 0.05756233632564545\n",
      "Epoch 6027/30000 Training Loss: 0.04772976413369179\n",
      "Epoch 6028/30000 Training Loss: 0.05437571555376053\n",
      "Epoch 6029/30000 Training Loss: 0.0417015478014946\n",
      "Epoch 6030/30000 Training Loss: 0.06900399923324585\n",
      "Epoch 6031/30000 Training Loss: 0.05807066708803177\n",
      "Epoch 6032/30000 Training Loss: 0.05104713514447212\n",
      "Epoch 6033/30000 Training Loss: 0.05716486647725105\n",
      "Epoch 6034/30000 Training Loss: 0.04428405687212944\n",
      "Epoch 6035/30000 Training Loss: 0.050909623503685\n",
      "Epoch 6036/30000 Training Loss: 0.0472232848405838\n",
      "Epoch 6037/30000 Training Loss: 0.05080926790833473\n",
      "Epoch 6038/30000 Training Loss: 0.051187656819820404\n",
      "Epoch 6039/30000 Training Loss: 0.04374432563781738\n",
      "Epoch 6040/30000 Training Loss: 0.04033226519823074\n",
      "Epoch 6041/30000 Training Loss: 0.0517379455268383\n",
      "Epoch 6042/30000 Training Loss: 0.05879700928926468\n",
      "Epoch 6043/30000 Training Loss: 0.05508572235703468\n",
      "Epoch 6044/30000 Training Loss: 0.06137582287192345\n",
      "Epoch 6045/30000 Training Loss: 0.06059785187244415\n",
      "Epoch 6046/30000 Training Loss: 0.07199253141880035\n",
      "Epoch 6047/30000 Training Loss: 0.05091753602027893\n",
      "Epoch 6048/30000 Training Loss: 0.059725649654865265\n",
      "Epoch 6049/30000 Training Loss: 0.03944926708936691\n",
      "Epoch 6050/30000 Training Loss: 0.06551633775234222\n",
      "Epoch 6051/30000 Training Loss: 0.04193367063999176\n",
      "Epoch 6052/30000 Training Loss: 0.06705307960510254\n",
      "Epoch 6053/30000 Training Loss: 0.046651870012283325\n",
      "Epoch 6054/30000 Training Loss: 0.048067327588796616\n",
      "Epoch 6055/30000 Training Loss: 0.04958152770996094\n",
      "Epoch 6056/30000 Training Loss: 0.039509136229753494\n",
      "Epoch 6057/30000 Training Loss: 0.07763588428497314\n",
      "Epoch 6058/30000 Training Loss: 0.057570286095142365\n",
      "Epoch 6059/30000 Training Loss: 0.04803985357284546\n",
      "Epoch 6060/30000 Training Loss: 0.04092387109994888\n",
      "Epoch 6061/30000 Training Loss: 0.056031495332717896\n",
      "Epoch 6062/30000 Training Loss: 0.0588703453540802\n",
      "Epoch 6063/30000 Training Loss: 0.043526533991098404\n",
      "Epoch 6064/30000 Training Loss: 0.054222121834754944\n",
      "Epoch 6065/30000 Training Loss: 0.04562562704086304\n",
      "Epoch 6066/30000 Training Loss: 0.0582590326666832\n",
      "Epoch 6067/30000 Training Loss: 0.06062254309654236\n",
      "Epoch 6068/30000 Training Loss: 0.042761124670505524\n",
      "Epoch 6069/30000 Training Loss: 0.05692606419324875\n",
      "Epoch 6070/30000 Training Loss: 0.046876829117536545\n",
      "Epoch 6071/30000 Training Loss: 0.06840480864048004\n",
      "Epoch 6072/30000 Training Loss: 0.06741541624069214\n",
      "Epoch 6073/30000 Training Loss: 0.04619723558425903\n",
      "Epoch 6074/30000 Training Loss: 0.0653863474726677\n",
      "Epoch 6075/30000 Training Loss: 0.0554606169462204\n",
      "Epoch 6076/30000 Training Loss: 0.058874428272247314\n",
      "Epoch 6077/30000 Training Loss: 0.05699171870946884\n",
      "Epoch 6078/30000 Training Loss: 0.049127139151096344\n",
      "Epoch 6079/30000 Training Loss: 0.060080111026763916\n",
      "Epoch 6080/30000 Training Loss: 0.06434224545955658\n",
      "Epoch 6081/30000 Training Loss: 0.052368611097335815\n",
      "Epoch 6082/30000 Training Loss: 0.05896839499473572\n",
      "Epoch 6083/30000 Training Loss: 0.05799371004104614\n",
      "Epoch 6084/30000 Training Loss: 0.04080081358551979\n",
      "Epoch 6085/30000 Training Loss: 0.05894586071372032\n",
      "Epoch 6086/30000 Training Loss: 0.05124475061893463\n",
      "Epoch 6087/30000 Training Loss: 0.03918113559484482\n",
      "Epoch 6088/30000 Training Loss: 0.04725997522473335\n",
      "Epoch 6089/30000 Training Loss: 0.06321795284748077\n",
      "Epoch 6090/30000 Training Loss: 0.05005571246147156\n",
      "Epoch 6091/30000 Training Loss: 0.04535694792866707\n",
      "Epoch 6092/30000 Training Loss: 0.05322260409593582\n",
      "Epoch 6093/30000 Training Loss: 0.043904032558202744\n",
      "Epoch 6094/30000 Training Loss: 0.04897451773285866\n",
      "Epoch 6095/30000 Training Loss: 0.04620647802948952\n",
      "Epoch 6096/30000 Training Loss: 0.043926019221544266\n",
      "Epoch 6097/30000 Training Loss: 0.059809353202581406\n",
      "Epoch 6098/30000 Training Loss: 0.07039594650268555\n",
      "Epoch 6099/30000 Training Loss: 0.061664894223213196\n",
      "Epoch 6100/30000 Training Loss: 0.058717697858810425\n",
      "Epoch 6100/30000 Validation Loss: 0.0428028404712677\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0428028404712677<=============\n",
      "Epoch 6101/30000 Training Loss: 0.055741086602211\n",
      "Epoch 6102/30000 Training Loss: 0.05278918892145157\n",
      "Epoch 6103/30000 Training Loss: 0.04967682808637619\n",
      "Epoch 6104/30000 Training Loss: 0.05478913336992264\n",
      "Epoch 6105/30000 Training Loss: 0.052660852670669556\n",
      "Epoch 6106/30000 Training Loss: 0.05107972025871277\n",
      "Epoch 6107/30000 Training Loss: 0.047867946326732635\n",
      "Epoch 6108/30000 Training Loss: 0.039587896317243576\n",
      "Epoch 6109/30000 Training Loss: 0.043825503438711166\n",
      "Epoch 6110/30000 Training Loss: 0.06880177557468414\n",
      "Epoch 6111/30000 Training Loss: 0.06332086771726608\n",
      "Epoch 6112/30000 Training Loss: 0.05705011636018753\n",
      "Epoch 6113/30000 Training Loss: 0.06027790531516075\n",
      "Epoch 6114/30000 Training Loss: 0.05814402922987938\n",
      "Epoch 6115/30000 Training Loss: 0.058420322835445404\n",
      "Epoch 6116/30000 Training Loss: 0.050298046320676804\n",
      "Epoch 6117/30000 Training Loss: 0.04772424325346947\n",
      "Epoch 6118/30000 Training Loss: 0.04827632009983063\n",
      "Epoch 6119/30000 Training Loss: 0.048780180513858795\n",
      "Epoch 6120/30000 Training Loss: 0.06601890921592712\n",
      "Epoch 6121/30000 Training Loss: 0.06416942924261093\n",
      "Epoch 6122/30000 Training Loss: 0.04997904598712921\n",
      "Epoch 6123/30000 Training Loss: 0.06067582964897156\n",
      "Epoch 6124/30000 Training Loss: 0.06773000210523605\n",
      "Epoch 6125/30000 Training Loss: 0.054856076836586\n",
      "Epoch 6126/30000 Training Loss: 0.06942684948444366\n",
      "Epoch 6127/30000 Training Loss: 0.03756507486104965\n",
      "Epoch 6128/30000 Training Loss: 0.05344870686531067\n",
      "Epoch 6129/30000 Training Loss: 0.04120098054409027\n",
      "Epoch 6130/30000 Training Loss: 0.06256122887134552\n",
      "Epoch 6131/30000 Training Loss: 0.05334906280040741\n",
      "Epoch 6132/30000 Training Loss: 0.06280463933944702\n",
      "Epoch 6133/30000 Training Loss: 0.048398371785879135\n",
      "Epoch 6134/30000 Training Loss: 0.04448963701725006\n",
      "Epoch 6135/30000 Training Loss: 0.049387380480766296\n",
      "Epoch 6136/30000 Training Loss: 0.0450562983751297\n",
      "Epoch 6137/30000 Training Loss: 0.04758620634675026\n",
      "Epoch 6138/30000 Training Loss: 0.06644228100776672\n",
      "Epoch 6139/30000 Training Loss: 0.05397351086139679\n",
      "Epoch 6140/30000 Training Loss: 0.0645633265376091\n",
      "Epoch 6141/30000 Training Loss: 0.05255600064992905\n",
      "Epoch 6142/30000 Training Loss: 0.08220616728067398\n",
      "Epoch 6143/30000 Training Loss: 0.04857129603624344\n",
      "Epoch 6144/30000 Training Loss: 0.05457065999507904\n",
      "Epoch 6145/30000 Training Loss: 0.049407146871089935\n",
      "Epoch 6146/30000 Training Loss: 0.05110146105289459\n",
      "Epoch 6147/30000 Training Loss: 0.04277954250574112\n",
      "Epoch 6148/30000 Training Loss: 0.06312176585197449\n",
      "Epoch 6149/30000 Training Loss: 0.058918192982673645\n",
      "Epoch 6150/30000 Training Loss: 0.043491754680871964\n",
      "Epoch 6151/30000 Training Loss: 0.06250271201133728\n",
      "Epoch 6152/30000 Training Loss: 0.045533571392297745\n",
      "Epoch 6153/30000 Training Loss: 0.0453668087720871\n",
      "Epoch 6154/30000 Training Loss: 0.058639202266931534\n",
      "Epoch 6155/30000 Training Loss: 0.04628979414701462\n",
      "Epoch 6156/30000 Training Loss: 0.06521090865135193\n",
      "Epoch 6157/30000 Training Loss: 0.046748191118240356\n",
      "Epoch 6158/30000 Training Loss: 0.059538450092077255\n",
      "Epoch 6159/30000 Training Loss: 0.06808286905288696\n",
      "Epoch 6160/30000 Training Loss: 0.059153683483600616\n",
      "Epoch 6161/30000 Training Loss: 0.06795824319124222\n",
      "Epoch 6162/30000 Training Loss: 0.05993826314806938\n",
      "Epoch 6163/30000 Training Loss: 0.07152526080608368\n",
      "Epoch 6164/30000 Training Loss: 0.06475357711315155\n",
      "Epoch 6165/30000 Training Loss: 0.06509214639663696\n",
      "Epoch 6166/30000 Training Loss: 0.04009569436311722\n",
      "Epoch 6167/30000 Training Loss: 0.04441559314727783\n",
      "Epoch 6168/30000 Training Loss: 0.05418330803513527\n",
      "Epoch 6169/30000 Training Loss: 0.04681459069252014\n",
      "Epoch 6170/30000 Training Loss: 0.06929219514131546\n",
      "Epoch 6171/30000 Training Loss: 0.05309460312128067\n",
      "Epoch 6172/30000 Training Loss: 0.058511700481176376\n",
      "Epoch 6173/30000 Training Loss: 0.04370987415313721\n",
      "Epoch 6174/30000 Training Loss: 0.06356438994407654\n",
      "Epoch 6175/30000 Training Loss: 0.05881061404943466\n",
      "Epoch 6176/30000 Training Loss: 0.06643020361661911\n",
      "Epoch 6177/30000 Training Loss: 0.05566290020942688\n",
      "Epoch 6178/30000 Training Loss: 0.05750804394483566\n",
      "Epoch 6179/30000 Training Loss: 0.06856092810630798\n",
      "Epoch 6180/30000 Training Loss: 0.050401538610458374\n",
      "Epoch 6181/30000 Training Loss: 0.06058850884437561\n",
      "Epoch 6182/30000 Training Loss: 0.0490226149559021\n",
      "Epoch 6183/30000 Training Loss: 0.06743215024471283\n",
      "Epoch 6184/30000 Training Loss: 0.05098440870642662\n",
      "Epoch 6185/30000 Training Loss: 0.04170691967010498\n",
      "Epoch 6186/30000 Training Loss: 0.058729805052280426\n",
      "Epoch 6187/30000 Training Loss: 0.06147404760122299\n",
      "Epoch 6188/30000 Training Loss: 0.04630333557724953\n",
      "Epoch 6189/30000 Training Loss: 0.06617695838212967\n",
      "Epoch 6190/30000 Training Loss: 0.050813622772693634\n",
      "Epoch 6191/30000 Training Loss: 0.058380432426929474\n",
      "Epoch 6192/30000 Training Loss: 0.04089093208312988\n",
      "Epoch 6193/30000 Training Loss: 0.07409372925758362\n",
      "Epoch 6194/30000 Training Loss: 0.05573653802275658\n",
      "Epoch 6195/30000 Training Loss: 0.05280420184135437\n",
      "Epoch 6196/30000 Training Loss: 0.069941446185112\n",
      "Epoch 6197/30000 Training Loss: 0.05706452578306198\n",
      "Epoch 6198/30000 Training Loss: 0.06637962907552719\n",
      "Epoch 6199/30000 Training Loss: 0.05931028351187706\n",
      "Epoch 6200/30000 Training Loss: 0.05644912272691727\n",
      "Epoch 6200/30000 Validation Loss: 0.04508893936872482\n",
      "Epoch 6201/30000 Training Loss: 0.05840098857879639\n",
      "Epoch 6202/30000 Training Loss: 0.05356041342020035\n",
      "Epoch 6203/30000 Training Loss: 0.06279014050960541\n",
      "Epoch 6204/30000 Training Loss: 0.045074477791786194\n",
      "Epoch 6205/30000 Training Loss: 0.052745163440704346\n",
      "Epoch 6206/30000 Training Loss: 0.051098961383104324\n",
      "Epoch 6207/30000 Training Loss: 0.05714556202292442\n",
      "Epoch 6208/30000 Training Loss: 0.05945032462477684\n",
      "Epoch 6209/30000 Training Loss: 0.06463110446929932\n",
      "Epoch 6210/30000 Training Loss: 0.0504145547747612\n",
      "Epoch 6211/30000 Training Loss: 0.054625943303108215\n",
      "Epoch 6212/30000 Training Loss: 0.06163919344544411\n",
      "Epoch 6213/30000 Training Loss: 0.06227125972509384\n",
      "Epoch 6214/30000 Training Loss: 0.050839297473430634\n",
      "Epoch 6215/30000 Training Loss: 0.051484495401382446\n",
      "Epoch 6216/30000 Training Loss: 0.05050218850374222\n",
      "Epoch 6217/30000 Training Loss: 0.0515902042388916\n",
      "Epoch 6218/30000 Training Loss: 0.0635487288236618\n",
      "Epoch 6219/30000 Training Loss: 0.05546744167804718\n",
      "Epoch 6220/30000 Training Loss: 0.04953397810459137\n",
      "Epoch 6221/30000 Training Loss: 0.059383220970630646\n",
      "Epoch 6222/30000 Training Loss: 0.04838866740465164\n",
      "Epoch 6223/30000 Training Loss: 0.0445612296462059\n",
      "Epoch 6224/30000 Training Loss: 0.05848179757595062\n",
      "Epoch 6225/30000 Training Loss: 0.03450553119182587\n",
      "Epoch 6226/30000 Training Loss: 0.045821014791727066\n",
      "Epoch 6227/30000 Training Loss: 0.05083759129047394\n",
      "Epoch 6228/30000 Training Loss: 0.05384465306997299\n",
      "Epoch 6229/30000 Training Loss: 0.03946421295404434\n",
      "Epoch 6230/30000 Training Loss: 0.061269111931324005\n",
      "Epoch 6231/30000 Training Loss: 0.05623496696352959\n",
      "Epoch 6232/30000 Training Loss: 0.061068251729011536\n",
      "Epoch 6233/30000 Training Loss: 0.07354060560464859\n",
      "Epoch 6234/30000 Training Loss: 0.05858378857374191\n",
      "Epoch 6235/30000 Training Loss: 0.06547299027442932\n",
      "Epoch 6236/30000 Training Loss: 0.05257319286465645\n",
      "Epoch 6237/30000 Training Loss: 0.05167816951870918\n",
      "Epoch 6238/30000 Training Loss: 0.040768660604953766\n",
      "Epoch 6239/30000 Training Loss: 0.05541050061583519\n",
      "Epoch 6240/30000 Training Loss: 0.05604296177625656\n",
      "Epoch 6241/30000 Training Loss: 0.07609764486551285\n",
      "Epoch 6242/30000 Training Loss: 0.04480898380279541\n",
      "Epoch 6243/30000 Training Loss: 0.07043323665857315\n",
      "Epoch 6244/30000 Training Loss: 0.06227827072143555\n",
      "Epoch 6245/30000 Training Loss: 0.05182760953903198\n",
      "Epoch 6246/30000 Training Loss: 0.06366048753261566\n",
      "Epoch 6247/30000 Training Loss: 0.04662724584341049\n",
      "Epoch 6248/30000 Training Loss: 0.046415120363235474\n",
      "Epoch 6249/30000 Training Loss: 0.05567402392625809\n",
      "Epoch 6250/30000 Training Loss: 0.05675506591796875\n",
      "Epoch 6251/30000 Training Loss: 0.04747527092695236\n",
      "Epoch 6252/30000 Training Loss: 0.05968347191810608\n",
      "Epoch 6253/30000 Training Loss: 0.06008431315422058\n",
      "Epoch 6254/30000 Training Loss: 0.06138334050774574\n",
      "Epoch 6255/30000 Training Loss: 0.07527495175600052\n",
      "Epoch 6256/30000 Training Loss: 0.05554330348968506\n",
      "Epoch 6257/30000 Training Loss: 0.05819028615951538\n",
      "Epoch 6258/30000 Training Loss: 0.05974539369344711\n",
      "Epoch 6259/30000 Training Loss: 0.04975244030356407\n",
      "Epoch 6260/30000 Training Loss: 0.04521871358156204\n",
      "Epoch 6261/30000 Training Loss: 0.06047435849905014\n",
      "Epoch 6262/30000 Training Loss: 0.06146072596311569\n",
      "Epoch 6263/30000 Training Loss: 0.04901012033224106\n",
      "Epoch 6264/30000 Training Loss: 0.05257537215948105\n",
      "Epoch 6265/30000 Training Loss: 0.04720170050859451\n",
      "Epoch 6266/30000 Training Loss: 0.061076149344444275\n",
      "Epoch 6267/30000 Training Loss: 0.04273970425128937\n",
      "Epoch 6268/30000 Training Loss: 0.05206035077571869\n",
      "Epoch 6269/30000 Training Loss: 0.07769939303398132\n",
      "Epoch 6270/30000 Training Loss: 0.05239500850439072\n",
      "Epoch 6271/30000 Training Loss: 0.05603483319282532\n",
      "Epoch 6272/30000 Training Loss: 0.055717986077070236\n",
      "Epoch 6273/30000 Training Loss: 0.043452709913253784\n",
      "Epoch 6274/30000 Training Loss: 0.05741645395755768\n",
      "Epoch 6275/30000 Training Loss: 0.04807714372873306\n",
      "Epoch 6276/30000 Training Loss: 0.05548017844557762\n",
      "Epoch 6277/30000 Training Loss: 0.05095205456018448\n",
      "Epoch 6278/30000 Training Loss: 0.06030121073126793\n",
      "Epoch 6279/30000 Training Loss: 0.04423878341913223\n",
      "Epoch 6280/30000 Training Loss: 0.05248045176267624\n",
      "Epoch 6281/30000 Training Loss: 0.05967004597187042\n",
      "Epoch 6282/30000 Training Loss: 0.06783322989940643\n",
      "Epoch 6283/30000 Training Loss: 0.06186024844646454\n",
      "Epoch 6284/30000 Training Loss: 0.05452525615692139\n",
      "Epoch 6285/30000 Training Loss: 0.0442022979259491\n",
      "Epoch 6286/30000 Training Loss: 0.060465604066848755\n",
      "Epoch 6287/30000 Training Loss: 0.047374751418828964\n",
      "Epoch 6288/30000 Training Loss: 0.038396380841732025\n",
      "Epoch 6289/30000 Training Loss: 0.06824629008769989\n",
      "Epoch 6290/30000 Training Loss: 0.04932384192943573\n",
      "Epoch 6291/30000 Training Loss: 0.07812093198299408\n",
      "Epoch 6292/30000 Training Loss: 0.05166500434279442\n",
      "Epoch 6293/30000 Training Loss: 0.06096808239817619\n",
      "Epoch 6294/30000 Training Loss: 0.054034896194934845\n",
      "Epoch 6295/30000 Training Loss: 0.04097665101289749\n",
      "Epoch 6296/30000 Training Loss: 0.06069761514663696\n",
      "Epoch 6297/30000 Training Loss: 0.05395350605249405\n",
      "Epoch 6298/30000 Training Loss: 0.0447421669960022\n",
      "Epoch 6299/30000 Training Loss: 0.04984210431575775\n",
      "Epoch 6300/30000 Training Loss: 0.05743379145860672\n",
      "Epoch 6300/30000 Validation Loss: 0.05216298997402191\n",
      "Epoch 6301/30000 Training Loss: 0.06306184083223343\n",
      "Epoch 6302/30000 Training Loss: 0.05631472170352936\n",
      "Epoch 6303/30000 Training Loss: 0.053119804710149765\n",
      "Epoch 6304/30000 Training Loss: 0.042551688849925995\n",
      "Epoch 6305/30000 Training Loss: 0.06428421288728714\n",
      "Epoch 6306/30000 Training Loss: 0.0652436763048172\n",
      "Epoch 6307/30000 Training Loss: 0.03356868773698807\n",
      "Epoch 6308/30000 Training Loss: 0.04846351221203804\n",
      "Epoch 6309/30000 Training Loss: 0.057025015354156494\n",
      "Epoch 6310/30000 Training Loss: 0.04600255936384201\n",
      "Epoch 6311/30000 Training Loss: 0.04756486415863037\n",
      "Epoch 6312/30000 Training Loss: 0.07196871936321259\n",
      "Epoch 6313/30000 Training Loss: 0.04560037702322006\n",
      "Epoch 6314/30000 Training Loss: 0.04915503412485123\n",
      "Epoch 6315/30000 Training Loss: 0.0590515062212944\n",
      "Epoch 6316/30000 Training Loss: 0.06500671803951263\n",
      "Epoch 6317/30000 Training Loss: 0.05563751980662346\n",
      "Epoch 6318/30000 Training Loss: 0.06783491373062134\n",
      "Epoch 6319/30000 Training Loss: 0.05148234963417053\n",
      "Epoch 6320/30000 Training Loss: 0.05660170316696167\n",
      "Epoch 6321/30000 Training Loss: 0.046875447034835815\n",
      "Epoch 6322/30000 Training Loss: 0.046874918043613434\n",
      "Epoch 6323/30000 Training Loss: 0.07549786567687988\n",
      "Epoch 6324/30000 Training Loss: 0.053862474858760834\n",
      "Epoch 6325/30000 Training Loss: 0.05276261270046234\n",
      "Epoch 6326/30000 Training Loss: 0.050379976630210876\n",
      "Epoch 6327/30000 Training Loss: 0.06199569255113602\n",
      "Epoch 6328/30000 Training Loss: 0.04607052728533745\n",
      "Epoch 6329/30000 Training Loss: 0.05919193848967552\n",
      "Epoch 6330/30000 Training Loss: 0.047499626874923706\n",
      "Epoch 6331/30000 Training Loss: 0.06452172249555588\n",
      "Epoch 6332/30000 Training Loss: 0.05893459543585777\n",
      "Epoch 6333/30000 Training Loss: 0.041932299733161926\n",
      "Epoch 6334/30000 Training Loss: 0.055135875940322876\n",
      "Epoch 6335/30000 Training Loss: 0.05129478871822357\n",
      "Epoch 6336/30000 Training Loss: 0.057639602571725845\n",
      "Epoch 6337/30000 Training Loss: 0.04460133612155914\n",
      "Epoch 6338/30000 Training Loss: 0.04194962605834007\n",
      "Epoch 6339/30000 Training Loss: 0.05216208100318909\n",
      "Epoch 6340/30000 Training Loss: 0.04235484451055527\n",
      "Epoch 6341/30000 Training Loss: 0.049305349588394165\n",
      "Epoch 6342/30000 Training Loss: 0.04189930111169815\n",
      "Epoch 6343/30000 Training Loss: 0.05614662170410156\n",
      "Epoch 6344/30000 Training Loss: 0.044672444462776184\n",
      "Epoch 6345/30000 Training Loss: 0.06506896764039993\n",
      "Epoch 6346/30000 Training Loss: 0.061675116419792175\n",
      "Epoch 6347/30000 Training Loss: 0.06179097294807434\n",
      "Epoch 6348/30000 Training Loss: 0.06208839267492294\n",
      "Epoch 6349/30000 Training Loss: 0.051300689578056335\n",
      "Epoch 6350/30000 Training Loss: 0.053103767335414886\n",
      "Epoch 6351/30000 Training Loss: 0.06251451373100281\n",
      "Epoch 6352/30000 Training Loss: 0.05307702720165253\n",
      "Epoch 6353/30000 Training Loss: 0.05420627444982529\n",
      "Epoch 6354/30000 Training Loss: 0.05747944861650467\n",
      "Epoch 6355/30000 Training Loss: 0.05551494285464287\n",
      "Epoch 6356/30000 Training Loss: 0.0564139150083065\n",
      "Epoch 6357/30000 Training Loss: 0.0704931914806366\n",
      "Epoch 6358/30000 Training Loss: 0.06840996444225311\n",
      "Epoch 6359/30000 Training Loss: 0.049021802842617035\n",
      "Epoch 6360/30000 Training Loss: 0.04616679251194\n",
      "Epoch 6361/30000 Training Loss: 0.045887213200330734\n",
      "Epoch 6362/30000 Training Loss: 0.0421338714659214\n",
      "Epoch 6363/30000 Training Loss: 0.05663798004388809\n",
      "Epoch 6364/30000 Training Loss: 0.057313404977321625\n",
      "Epoch 6365/30000 Training Loss: 0.06299003958702087\n",
      "Epoch 6366/30000 Training Loss: 0.03837234154343605\n",
      "Epoch 6367/30000 Training Loss: 0.054458439350128174\n",
      "Epoch 6368/30000 Training Loss: 0.05733954533934593\n",
      "Epoch 6369/30000 Training Loss: 0.05127014219760895\n",
      "Epoch 6370/30000 Training Loss: 0.06339322030544281\n",
      "Epoch 6371/30000 Training Loss: 0.05291105434298515\n",
      "Epoch 6372/30000 Training Loss: 0.04164738953113556\n",
      "Epoch 6373/30000 Training Loss: 0.04727752506732941\n",
      "Epoch 6374/30000 Training Loss: 0.05285380780696869\n",
      "Epoch 6375/30000 Training Loss: 0.05501614138484001\n",
      "Epoch 6376/30000 Training Loss: 0.034758225083351135\n",
      "Epoch 6377/30000 Training Loss: 0.05637025088071823\n",
      "Epoch 6378/30000 Training Loss: 0.07412123680114746\n",
      "Epoch 6379/30000 Training Loss: 0.05367642268538475\n",
      "Epoch 6380/30000 Training Loss: 0.05882207304239273\n",
      "Epoch 6381/30000 Training Loss: 0.043606601655483246\n",
      "Epoch 6382/30000 Training Loss: 0.04190313071012497\n",
      "Epoch 6383/30000 Training Loss: 0.048538945615291595\n",
      "Epoch 6384/30000 Training Loss: 0.04430699720978737\n",
      "Epoch 6385/30000 Training Loss: 0.0655730813741684\n",
      "Epoch 6386/30000 Training Loss: 0.04791133850812912\n",
      "Epoch 6387/30000 Training Loss: 0.0608949139714241\n",
      "Epoch 6388/30000 Training Loss: 0.051476988941431046\n",
      "Epoch 6389/30000 Training Loss: 0.051437973976135254\n",
      "Epoch 6390/30000 Training Loss: 0.06424024701118469\n",
      "Epoch 6391/30000 Training Loss: 0.06804455816745758\n",
      "Epoch 6392/30000 Training Loss: 0.05194845795631409\n",
      "Epoch 6393/30000 Training Loss: 0.05531838908791542\n",
      "Epoch 6394/30000 Training Loss: 0.05309934914112091\n",
      "Epoch 6395/30000 Training Loss: 0.04806319251656532\n",
      "Epoch 6396/30000 Training Loss: 0.06159675866365433\n",
      "Epoch 6397/30000 Training Loss: 0.054379258304834366\n",
      "Epoch 6398/30000 Training Loss: 0.06178361922502518\n",
      "Epoch 6399/30000 Training Loss: 0.06278462707996368\n",
      "Epoch 6400/30000 Training Loss: 0.06452736258506775\n",
      "Epoch 6400/30000 Validation Loss: 0.07295872271060944\n",
      "Epoch 6401/30000 Training Loss: 0.04632270336151123\n",
      "Epoch 6402/30000 Training Loss: 0.04316986724734306\n",
      "Epoch 6403/30000 Training Loss: 0.04857548326253891\n",
      "Epoch 6404/30000 Training Loss: 0.06840275228023529\n",
      "Epoch 6405/30000 Training Loss: 0.06574730575084686\n",
      "Epoch 6406/30000 Training Loss: 0.05567314475774765\n",
      "Epoch 6407/30000 Training Loss: 0.07405325770378113\n",
      "Epoch 6408/30000 Training Loss: 0.049858398735523224\n",
      "Epoch 6409/30000 Training Loss: 0.04574053734540939\n",
      "Epoch 6410/30000 Training Loss: 0.056868866086006165\n",
      "Epoch 6411/30000 Training Loss: 0.05404166132211685\n",
      "Epoch 6412/30000 Training Loss: 0.06646083295345306\n",
      "Epoch 6413/30000 Training Loss: 0.04842164367437363\n",
      "Epoch 6414/30000 Training Loss: 0.04974425584077835\n",
      "Epoch 6415/30000 Training Loss: 0.053672902286052704\n",
      "Epoch 6416/30000 Training Loss: 0.049844060093164444\n",
      "Epoch 6417/30000 Training Loss: 0.05087355151772499\n",
      "Epoch 6418/30000 Training Loss: 0.05950898677110672\n",
      "Epoch 6419/30000 Training Loss: 0.07284167408943176\n",
      "Epoch 6420/30000 Training Loss: 0.05820474028587341\n",
      "Epoch 6421/30000 Training Loss: 0.05489359050989151\n",
      "Epoch 6422/30000 Training Loss: 0.054693419486284256\n",
      "Epoch 6423/30000 Training Loss: 0.06867824494838715\n",
      "Epoch 6424/30000 Training Loss: 0.04676849767565727\n",
      "Epoch 6425/30000 Training Loss: 0.04459914192557335\n",
      "Epoch 6426/30000 Training Loss: 0.05595147982239723\n",
      "Epoch 6427/30000 Training Loss: 0.05865352600812912\n",
      "Epoch 6428/30000 Training Loss: 0.06333150714635849\n",
      "Epoch 6429/30000 Training Loss: 0.045561403036117554\n",
      "Epoch 6430/30000 Training Loss: 0.050138942897319794\n",
      "Epoch 6431/30000 Training Loss: 0.05717136710882187\n",
      "Epoch 6432/30000 Training Loss: 0.0451517216861248\n",
      "Epoch 6433/30000 Training Loss: 0.06771816313266754\n",
      "Epoch 6434/30000 Training Loss: 0.05185918137431145\n",
      "Epoch 6435/30000 Training Loss: 0.048642441630363464\n",
      "Epoch 6436/30000 Training Loss: 0.044670671224594116\n",
      "Epoch 6437/30000 Training Loss: 0.0514216348528862\n",
      "Epoch 6438/30000 Training Loss: 0.061883002519607544\n",
      "Epoch 6439/30000 Training Loss: 0.0746016874909401\n",
      "Epoch 6440/30000 Training Loss: 0.044175710529088974\n",
      "Epoch 6441/30000 Training Loss: 0.049650102853775024\n",
      "Epoch 6442/30000 Training Loss: 0.06381672620773315\n",
      "Epoch 6443/30000 Training Loss: 0.04661210626363754\n",
      "Epoch 6444/30000 Training Loss: 0.05332738533616066\n",
      "Epoch 6445/30000 Training Loss: 0.05228966474533081\n",
      "Epoch 6446/30000 Training Loss: 0.052098289132118225\n",
      "Epoch 6447/30000 Training Loss: 0.05168818682432175\n",
      "Epoch 6448/30000 Training Loss: 0.05649547651410103\n",
      "Epoch 6449/30000 Training Loss: 0.07257072627544403\n",
      "Epoch 6450/30000 Training Loss: 0.06021355837583542\n",
      "Epoch 6451/30000 Training Loss: 0.039669983088970184\n",
      "Epoch 6452/30000 Training Loss: 0.04602222889661789\n",
      "Epoch 6453/30000 Training Loss: 0.041430097073316574\n",
      "Epoch 6454/30000 Training Loss: 0.05910489708185196\n",
      "Epoch 6455/30000 Training Loss: 0.05058126151561737\n",
      "Epoch 6456/30000 Training Loss: 0.052180953323841095\n",
      "Epoch 6457/30000 Training Loss: 0.04379137605428696\n",
      "Epoch 6458/30000 Training Loss: 0.05864876136183739\n",
      "Epoch 6459/30000 Training Loss: 0.05421527475118637\n",
      "Epoch 6460/30000 Training Loss: 0.05126723647117615\n",
      "Epoch 6461/30000 Training Loss: 0.04648889973759651\n",
      "Epoch 6462/30000 Training Loss: 0.04901288077235222\n",
      "Epoch 6463/30000 Training Loss: 0.05045771598815918\n",
      "Epoch 6464/30000 Training Loss: 0.041420675814151764\n",
      "Epoch 6465/30000 Training Loss: 0.04867616295814514\n",
      "Epoch 6466/30000 Training Loss: 0.0530378520488739\n",
      "Epoch 6467/30000 Training Loss: 0.04503914713859558\n",
      "Epoch 6468/30000 Training Loss: 0.042863629758358\n",
      "Epoch 6469/30000 Training Loss: 0.05217328667640686\n",
      "Epoch 6470/30000 Training Loss: 0.051208123564720154\n",
      "Epoch 6471/30000 Training Loss: 0.05673746019601822\n",
      "Epoch 6472/30000 Training Loss: 0.05430852994322777\n",
      "Epoch 6473/30000 Training Loss: 0.045228417962789536\n",
      "Epoch 6474/30000 Training Loss: 0.05778338015079498\n",
      "Epoch 6475/30000 Training Loss: 0.046543970704078674\n",
      "Epoch 6476/30000 Training Loss: 0.03779003024101257\n",
      "Epoch 6477/30000 Training Loss: 0.059372372925281525\n",
      "Epoch 6478/30000 Training Loss: 0.06758998334407806\n",
      "Epoch 6479/30000 Training Loss: 0.057376667857170105\n",
      "Epoch 6480/30000 Training Loss: 0.06115918606519699\n",
      "Epoch 6481/30000 Training Loss: 0.05430211126804352\n",
      "Epoch 6482/30000 Training Loss: 0.054847601801157\n",
      "Epoch 6483/30000 Training Loss: 0.05243830755352974\n",
      "Epoch 6484/30000 Training Loss: 0.039340581744909286\n",
      "Epoch 6485/30000 Training Loss: 0.03931521624326706\n",
      "Epoch 6486/30000 Training Loss: 0.04240402206778526\n",
      "Epoch 6487/30000 Training Loss: 0.04747091978788376\n",
      "Epoch 6488/30000 Training Loss: 0.0668768361210823\n",
      "Epoch 6489/30000 Training Loss: 0.04666585475206375\n",
      "Epoch 6490/30000 Training Loss: 0.058496154844760895\n",
      "Epoch 6491/30000 Training Loss: 0.06299903243780136\n",
      "Epoch 6492/30000 Training Loss: 0.05274873599410057\n",
      "Epoch 6493/30000 Training Loss: 0.05401992052793503\n",
      "Epoch 6494/30000 Training Loss: 0.04267198592424393\n",
      "Epoch 6495/30000 Training Loss: 0.059830572456121445\n",
      "Epoch 6496/30000 Training Loss: 0.06277044117450714\n",
      "Epoch 6497/30000 Training Loss: 0.04819561913609505\n",
      "Epoch 6498/30000 Training Loss: 0.05129344016313553\n",
      "Epoch 6499/30000 Training Loss: 0.04541078954935074\n",
      "Epoch 6500/30000 Training Loss: 0.04997885599732399\n",
      "Epoch 6500/30000 Validation Loss: 0.048569969832897186\n",
      "Epoch 6501/30000 Training Loss: 0.04476500302553177\n",
      "Epoch 6502/30000 Training Loss: 0.04598294198513031\n",
      "Epoch 6503/30000 Training Loss: 0.061020925641059875\n",
      "Epoch 6504/30000 Training Loss: 0.04897061735391617\n",
      "Epoch 6505/30000 Training Loss: 0.045770563185214996\n",
      "Epoch 6506/30000 Training Loss: 0.07192506641149521\n",
      "Epoch 6507/30000 Training Loss: 0.05331684648990631\n",
      "Epoch 6508/30000 Training Loss: 0.05313831940293312\n",
      "Epoch 6509/30000 Training Loss: 0.05577453225851059\n",
      "Epoch 6510/30000 Training Loss: 0.05310646444559097\n",
      "Epoch 6511/30000 Training Loss: 0.047815944999456406\n",
      "Epoch 6512/30000 Training Loss: 0.05714350938796997\n",
      "Epoch 6513/30000 Training Loss: 0.05095680058002472\n",
      "Epoch 6514/30000 Training Loss: 0.05112915486097336\n",
      "Epoch 6515/30000 Training Loss: 0.06539661437273026\n",
      "Epoch 6516/30000 Training Loss: 0.054324302822351456\n",
      "Epoch 6517/30000 Training Loss: 0.0616864413022995\n",
      "Epoch 6518/30000 Training Loss: 0.05726417154073715\n",
      "Epoch 6519/30000 Training Loss: 0.06897055357694626\n",
      "Epoch 6520/30000 Training Loss: 0.07096461951732635\n",
      "Epoch 6521/30000 Training Loss: 0.04967570677399635\n",
      "Epoch 6522/30000 Training Loss: 0.05050612613558769\n",
      "Epoch 6523/30000 Training Loss: 0.048412300646305084\n",
      "Epoch 6524/30000 Training Loss: 0.04835502430796623\n",
      "Epoch 6525/30000 Training Loss: 0.05209837108850479\n",
      "Epoch 6526/30000 Training Loss: 0.05944220349192619\n",
      "Epoch 6527/30000 Training Loss: 0.06034553796052933\n",
      "Epoch 6528/30000 Training Loss: 0.04156314581632614\n",
      "Epoch 6529/30000 Training Loss: 0.06051677092909813\n",
      "Epoch 6530/30000 Training Loss: 0.051123905926942825\n",
      "Epoch 6531/30000 Training Loss: 0.051456794142723083\n",
      "Epoch 6532/30000 Training Loss: 0.05775190144777298\n",
      "Epoch 6533/30000 Training Loss: 0.037705548107624054\n",
      "Epoch 6534/30000 Training Loss: 0.03930618613958359\n",
      "Epoch 6535/30000 Training Loss: 0.060399554669857025\n",
      "Epoch 6536/30000 Training Loss: 0.044288016855716705\n",
      "Epoch 6537/30000 Training Loss: 0.047576501965522766\n",
      "Epoch 6538/30000 Training Loss: 0.03924822062253952\n",
      "Epoch 6539/30000 Training Loss: 0.06127609312534332\n",
      "Epoch 6540/30000 Training Loss: 0.060504354536533356\n",
      "Epoch 6541/30000 Training Loss: 0.05847366526722908\n",
      "Epoch 6542/30000 Training Loss: 0.05651387572288513\n",
      "Epoch 6543/30000 Training Loss: 0.06534796208143234\n",
      "Epoch 6544/30000 Training Loss: 0.04516899958252907\n",
      "Epoch 6545/30000 Training Loss: 0.04597483575344086\n",
      "Epoch 6546/30000 Training Loss: 0.04752644523978233\n",
      "Epoch 6547/30000 Training Loss: 0.06280122697353363\n",
      "Epoch 6548/30000 Training Loss: 0.042370207607746124\n",
      "Epoch 6549/30000 Training Loss: 0.056747592985630035\n",
      "Epoch 6550/30000 Training Loss: 0.04775461554527283\n",
      "Epoch 6551/30000 Training Loss: 0.05821000412106514\n",
      "Epoch 6552/30000 Training Loss: 0.05551064386963844\n",
      "Epoch 6553/30000 Training Loss: 0.05687442421913147\n",
      "Epoch 6554/30000 Training Loss: 0.06881003826856613\n",
      "Epoch 6555/30000 Training Loss: 0.04522726684808731\n",
      "Epoch 6556/30000 Training Loss: 0.05681329965591431\n",
      "Epoch 6557/30000 Training Loss: 0.05472208559513092\n",
      "Epoch 6558/30000 Training Loss: 0.0406496524810791\n",
      "Epoch 6559/30000 Training Loss: 0.03824980556964874\n",
      "Epoch 6560/30000 Training Loss: 0.04901069402694702\n",
      "Epoch 6561/30000 Training Loss: 0.07048803567886353\n",
      "Epoch 6562/30000 Training Loss: 0.050565510988235474\n",
      "Epoch 6563/30000 Training Loss: 0.06393563002347946\n",
      "Epoch 6564/30000 Training Loss: 0.05780455842614174\n",
      "Epoch 6565/30000 Training Loss: 0.06065216660499573\n",
      "Epoch 6566/30000 Training Loss: 0.06550543010234833\n",
      "Epoch 6567/30000 Training Loss: 0.06911568343639374\n",
      "Epoch 6568/30000 Training Loss: 0.05183248221874237\n",
      "Epoch 6569/30000 Training Loss: 0.04696153849363327\n",
      "Epoch 6570/30000 Training Loss: 0.04730997234582901\n",
      "Epoch 6571/30000 Training Loss: 0.03998158127069473\n",
      "Epoch 6572/30000 Training Loss: 0.044416643679142\n",
      "Epoch 6573/30000 Training Loss: 0.04452796280384064\n",
      "Epoch 6574/30000 Training Loss: 0.04864618927240372\n",
      "Epoch 6575/30000 Training Loss: 0.05597841739654541\n",
      "Epoch 6576/30000 Training Loss: 0.04344455152750015\n",
      "Epoch 6577/30000 Training Loss: 0.049298129975795746\n",
      "Epoch 6578/30000 Training Loss: 0.06499763578176498\n",
      "Epoch 6579/30000 Training Loss: 0.06181717291474342\n",
      "Epoch 6580/30000 Training Loss: 0.04696435108780861\n",
      "Epoch 6581/30000 Training Loss: 0.04370839148759842\n",
      "Epoch 6582/30000 Training Loss: 0.04478686675429344\n",
      "Epoch 6583/30000 Training Loss: 0.05523373931646347\n",
      "Epoch 6584/30000 Training Loss: 0.04106789827346802\n",
      "Epoch 6585/30000 Training Loss: 0.039034634828567505\n",
      "Epoch 6586/30000 Training Loss: 0.06095445156097412\n",
      "Epoch 6587/30000 Training Loss: 0.05746474117040634\n",
      "Epoch 6588/30000 Training Loss: 0.04054360091686249\n",
      "Epoch 6589/30000 Training Loss: 0.05548250675201416\n",
      "Epoch 6590/30000 Training Loss: 0.045716363936662674\n",
      "Epoch 6591/30000 Training Loss: 0.0516352541744709\n",
      "Epoch 6592/30000 Training Loss: 0.052719585597515106\n",
      "Epoch 6593/30000 Training Loss: 0.044068075716495514\n",
      "Epoch 6594/30000 Training Loss: 0.04080282524228096\n",
      "Epoch 6595/30000 Training Loss: 0.051866356283426285\n",
      "Epoch 6596/30000 Training Loss: 0.06668365746736526\n",
      "Epoch 6597/30000 Training Loss: 0.053075388073921204\n",
      "Epoch 6598/30000 Training Loss: 0.05630042031407356\n",
      "Epoch 6599/30000 Training Loss: 0.04939942806959152\n",
      "Epoch 6600/30000 Training Loss: 0.051113955676555634\n",
      "Epoch 6600/30000 Validation Loss: 0.04762915521860123\n",
      "Epoch 6601/30000 Training Loss: 0.059267960488796234\n",
      "Epoch 6602/30000 Training Loss: 0.06751509755849838\n",
      "Epoch 6603/30000 Training Loss: 0.053097181022167206\n",
      "Epoch 6604/30000 Training Loss: 0.05732061713933945\n",
      "Epoch 6605/30000 Training Loss: 0.05567257106304169\n",
      "Epoch 6606/30000 Training Loss: 0.05057630315423012\n",
      "Epoch 6607/30000 Training Loss: 0.05091049522161484\n",
      "Epoch 6608/30000 Training Loss: 0.045009806752204895\n",
      "Epoch 6609/30000 Training Loss: 0.059678591787815094\n",
      "Epoch 6610/30000 Training Loss: 0.04952877759933472\n",
      "Epoch 6611/30000 Training Loss: 0.04561920091509819\n",
      "Epoch 6612/30000 Training Loss: 0.06390247493982315\n",
      "Epoch 6613/30000 Training Loss: 0.07278352975845337\n",
      "Epoch 6614/30000 Training Loss: 0.039285507053136826\n",
      "Epoch 6615/30000 Training Loss: 0.05232766270637512\n",
      "Epoch 6616/30000 Training Loss: 0.04034644365310669\n",
      "Epoch 6617/30000 Training Loss: 0.0557357594370842\n",
      "Epoch 6618/30000 Training Loss: 0.04276290908455849\n",
      "Epoch 6619/30000 Training Loss: 0.04804489389061928\n",
      "Epoch 6620/30000 Training Loss: 0.046659596264362335\n",
      "Epoch 6621/30000 Training Loss: 0.0401126965880394\n",
      "Epoch 6622/30000 Training Loss: 0.05385152995586395\n",
      "Epoch 6623/30000 Training Loss: 0.04843728989362717\n",
      "Epoch 6624/30000 Training Loss: 0.044155508279800415\n",
      "Epoch 6625/30000 Training Loss: 0.051595740020275116\n",
      "Epoch 6626/30000 Training Loss: 0.05647260695695877\n",
      "Epoch 6627/30000 Training Loss: 0.07880769670009613\n",
      "Epoch 6628/30000 Training Loss: 0.05397126451134682\n",
      "Epoch 6629/30000 Training Loss: 0.05404164642095566\n",
      "Epoch 6630/30000 Training Loss: 0.04387647658586502\n",
      "Epoch 6631/30000 Training Loss: 0.04052460938692093\n",
      "Epoch 6632/30000 Training Loss: 0.03715698793530464\n",
      "Epoch 6633/30000 Training Loss: 0.04773049056529999\n",
      "Epoch 6634/30000 Training Loss: 0.056248538196086884\n",
      "Epoch 6635/30000 Training Loss: 0.05164766684174538\n",
      "Epoch 6636/30000 Training Loss: 0.03835228458046913\n",
      "Epoch 6637/30000 Training Loss: 0.04552649334073067\n",
      "Epoch 6638/30000 Training Loss: 0.05020434036850929\n",
      "Epoch 6639/30000 Training Loss: 0.06706676632165909\n",
      "Epoch 6640/30000 Training Loss: 0.055394239723682404\n",
      "Epoch 6641/30000 Training Loss: 0.05930129438638687\n",
      "Epoch 6642/30000 Training Loss: 0.06264151632785797\n",
      "Epoch 6643/30000 Training Loss: 0.06035281717777252\n",
      "Epoch 6644/30000 Training Loss: 0.04959000647068024\n",
      "Epoch 6645/30000 Training Loss: 0.05404127761721611\n",
      "Epoch 6646/30000 Training Loss: 0.06435896456241608\n",
      "Epoch 6647/30000 Training Loss: 0.05057916045188904\n",
      "Epoch 6648/30000 Training Loss: 0.0624135285615921\n",
      "Epoch 6649/30000 Training Loss: 0.050553761422634125\n",
      "Epoch 6650/30000 Training Loss: 0.04338272660970688\n",
      "Epoch 6651/30000 Training Loss: 0.05285458266735077\n",
      "Epoch 6652/30000 Training Loss: 0.03968093916773796\n",
      "Epoch 6653/30000 Training Loss: 0.05662119761109352\n",
      "Epoch 6654/30000 Training Loss: 0.05859476327896118\n",
      "Epoch 6655/30000 Training Loss: 0.06364288181066513\n",
      "Epoch 6656/30000 Training Loss: 0.0580054335296154\n",
      "Epoch 6657/30000 Training Loss: 0.0585835799574852\n",
      "Epoch 6658/30000 Training Loss: 0.04187065362930298\n",
      "Epoch 6659/30000 Training Loss: 0.07303430885076523\n",
      "Epoch 6660/30000 Training Loss: 0.05246608704328537\n",
      "Epoch 6661/30000 Training Loss: 0.05872885882854462\n",
      "Epoch 6662/30000 Training Loss: 0.06611721217632294\n",
      "Epoch 6663/30000 Training Loss: 0.04676290974020958\n",
      "Epoch 6664/30000 Training Loss: 0.06205519288778305\n",
      "Epoch 6665/30000 Training Loss: 0.056812457740306854\n",
      "Epoch 6666/30000 Training Loss: 0.050138793885707855\n",
      "Epoch 6667/30000 Training Loss: 0.058798760175704956\n",
      "Epoch 6668/30000 Training Loss: 0.048913873732089996\n",
      "Epoch 6669/30000 Training Loss: 0.06225615739822388\n",
      "Epoch 6670/30000 Training Loss: 0.04621062055230141\n",
      "Epoch 6671/30000 Training Loss: 0.043560709804296494\n",
      "Epoch 6672/30000 Training Loss: 0.05251174420118332\n",
      "Epoch 6673/30000 Training Loss: 0.06564532965421677\n",
      "Epoch 6674/30000 Training Loss: 0.0494622066617012\n",
      "Epoch 6675/30000 Training Loss: 0.06442676484584808\n",
      "Epoch 6676/30000 Training Loss: 0.0679117888212204\n",
      "Epoch 6677/30000 Training Loss: 0.047903768718242645\n",
      "Epoch 6678/30000 Training Loss: 0.048276811838150024\n",
      "Epoch 6679/30000 Training Loss: 0.06388688832521439\n",
      "Epoch 6680/30000 Training Loss: 0.06822662055492401\n",
      "Epoch 6681/30000 Training Loss: 0.04221104085445404\n",
      "Epoch 6682/30000 Training Loss: 0.06395293772220612\n",
      "Epoch 6683/30000 Training Loss: 0.05901531130075455\n",
      "Epoch 6684/30000 Training Loss: 0.05219908803701401\n",
      "Epoch 6685/30000 Training Loss: 0.07227158546447754\n",
      "Epoch 6686/30000 Training Loss: 0.045051272958517075\n",
      "Epoch 6687/30000 Training Loss: 0.05415026843547821\n",
      "Epoch 6688/30000 Training Loss: 0.050117529928684235\n",
      "Epoch 6689/30000 Training Loss: 0.05798247829079628\n",
      "Epoch 6690/30000 Training Loss: 0.04983041435480118\n",
      "Epoch 6691/30000 Training Loss: 0.06066901981830597\n",
      "Epoch 6692/30000 Training Loss: 0.06111506372690201\n",
      "Epoch 6693/30000 Training Loss: 0.046090491116046906\n",
      "Epoch 6694/30000 Training Loss: 0.05623968318104744\n",
      "Epoch 6695/30000 Training Loss: 0.060380175709724426\n",
      "Epoch 6696/30000 Training Loss: 0.06358935683965683\n",
      "Epoch 6697/30000 Training Loss: 0.05659199506044388\n",
      "Epoch 6698/30000 Training Loss: 0.06008100509643555\n",
      "Epoch 6699/30000 Training Loss: 0.06197352707386017\n",
      "Epoch 6700/30000 Training Loss: 0.05215873941779137\n",
      "Epoch 6700/30000 Validation Loss: 0.046580225229263306\n",
      "Epoch 6701/30000 Training Loss: 0.05059921368956566\n",
      "Epoch 6702/30000 Training Loss: 0.05570808798074722\n",
      "Epoch 6703/30000 Training Loss: 0.05228393152356148\n",
      "Epoch 6704/30000 Training Loss: 0.06557998806238174\n",
      "Epoch 6705/30000 Training Loss: 0.06748616695404053\n",
      "Epoch 6706/30000 Training Loss: 0.05350061506032944\n",
      "Epoch 6707/30000 Training Loss: 0.06565714627504349\n",
      "Epoch 6708/30000 Training Loss: 0.052975259721279144\n",
      "Epoch 6709/30000 Training Loss: 0.060296617448329926\n",
      "Epoch 6710/30000 Training Loss: 0.04965071380138397\n",
      "Epoch 6711/30000 Training Loss: 0.05166038125753403\n",
      "Epoch 6712/30000 Training Loss: 0.054244592785835266\n",
      "Epoch 6713/30000 Training Loss: 0.04929254204034805\n",
      "Epoch 6714/30000 Training Loss: 0.047029316425323486\n",
      "Epoch 6715/30000 Training Loss: 0.060289088636636734\n",
      "Epoch 6716/30000 Training Loss: 0.047700099647045135\n",
      "Epoch 6717/30000 Training Loss: 0.04727620258927345\n",
      "Epoch 6718/30000 Training Loss: 0.05812098830938339\n",
      "Epoch 6719/30000 Training Loss: 0.05776922404766083\n",
      "Epoch 6720/30000 Training Loss: 0.049527473747730255\n",
      "Epoch 6721/30000 Training Loss: 0.04925503954291344\n",
      "Epoch 6722/30000 Training Loss: 0.05318814516067505\n",
      "Epoch 6723/30000 Training Loss: 0.04562923684716225\n",
      "Epoch 6724/30000 Training Loss: 0.04996846616268158\n",
      "Epoch 6725/30000 Training Loss: 0.04913001134991646\n",
      "Epoch 6726/30000 Training Loss: 0.04042921960353851\n",
      "Epoch 6727/30000 Training Loss: 0.04340919852256775\n",
      "Epoch 6728/30000 Training Loss: 0.054296087473630905\n",
      "Epoch 6729/30000 Training Loss: 0.04400887340307236\n",
      "Epoch 6730/30000 Training Loss: 0.051165491342544556\n",
      "Epoch 6731/30000 Training Loss: 0.05362289398908615\n",
      "Epoch 6732/30000 Training Loss: 0.04275818169116974\n",
      "Epoch 6733/30000 Training Loss: 0.0765308141708374\n",
      "Epoch 6734/30000 Training Loss: 0.06854191422462463\n",
      "Epoch 6735/30000 Training Loss: 0.06258612871170044\n",
      "Epoch 6736/30000 Training Loss: 0.05054428428411484\n",
      "Epoch 6737/30000 Training Loss: 0.0425129160284996\n",
      "Epoch 6738/30000 Training Loss: 0.06589755415916443\n",
      "Epoch 6739/30000 Training Loss: 0.043270718306303024\n",
      "Epoch 6740/30000 Training Loss: 0.04011956602334976\n",
      "Epoch 6741/30000 Training Loss: 0.054461993277072906\n",
      "Epoch 6742/30000 Training Loss: 0.06581862270832062\n",
      "Epoch 6743/30000 Training Loss: 0.04466909170150757\n",
      "Epoch 6744/30000 Training Loss: 0.06870207190513611\n",
      "Epoch 6745/30000 Training Loss: 0.04798101633787155\n",
      "Epoch 6746/30000 Training Loss: 0.04653231054544449\n",
      "Epoch 6747/30000 Training Loss: 0.06120557337999344\n",
      "Epoch 6748/30000 Training Loss: 0.038383759558200836\n",
      "Epoch 6749/30000 Training Loss: 0.066976398229599\n",
      "Epoch 6750/30000 Training Loss: 0.040051791816949844\n",
      "Epoch 6751/30000 Training Loss: 0.07271349430084229\n",
      "Epoch 6752/30000 Training Loss: 0.061818987131118774\n",
      "Epoch 6753/30000 Training Loss: 0.058063872158527374\n",
      "Epoch 6754/30000 Training Loss: 0.04712142422795296\n",
      "Epoch 6755/30000 Training Loss: 0.05806438624858856\n",
      "Epoch 6756/30000 Training Loss: 0.05688805133104324\n",
      "Epoch 6757/30000 Training Loss: 0.044609956443309784\n",
      "Epoch 6758/30000 Training Loss: 0.04413914307951927\n",
      "Epoch 6759/30000 Training Loss: 0.05627203360199928\n",
      "Epoch 6760/30000 Training Loss: 0.06394623219966888\n",
      "Epoch 6761/30000 Training Loss: 0.06004331633448601\n",
      "Epoch 6762/30000 Training Loss: 0.05507194995880127\n",
      "Epoch 6763/30000 Training Loss: 0.06227138638496399\n",
      "Epoch 6764/30000 Training Loss: 0.05167696624994278\n",
      "Epoch 6765/30000 Training Loss: 0.05249811336398125\n",
      "Epoch 6766/30000 Training Loss: 0.051678866147994995\n",
      "Epoch 6767/30000 Training Loss: 0.04780331254005432\n",
      "Epoch 6768/30000 Training Loss: 0.040849652141332626\n",
      "Epoch 6769/30000 Training Loss: 0.04505559056997299\n",
      "Epoch 6770/30000 Training Loss: 0.060041964054107666\n",
      "Epoch 6771/30000 Training Loss: 0.03891358897089958\n",
      "Epoch 6772/30000 Training Loss: 0.06625726819038391\n",
      "Epoch 6773/30000 Training Loss: 0.06607595086097717\n",
      "Epoch 6774/30000 Training Loss: 0.05757042020559311\n",
      "Epoch 6775/30000 Training Loss: 0.06436724960803986\n",
      "Epoch 6776/30000 Training Loss: 0.07538563013076782\n",
      "Epoch 6777/30000 Training Loss: 0.03988659381866455\n",
      "Epoch 6778/30000 Training Loss: 0.06773874163627625\n",
      "Epoch 6779/30000 Training Loss: 0.05845401808619499\n",
      "Epoch 6780/30000 Training Loss: 0.06324639916419983\n",
      "Epoch 6781/30000 Training Loss: 0.05170813947916031\n",
      "Epoch 6782/30000 Training Loss: 0.06304246187210083\n",
      "Epoch 6783/30000 Training Loss: 0.045613646507263184\n",
      "Epoch 6784/30000 Training Loss: 0.057053834199905396\n",
      "Epoch 6785/30000 Training Loss: 0.05516726151108742\n",
      "Epoch 6786/30000 Training Loss: 0.043117500841617584\n",
      "Epoch 6787/30000 Training Loss: 0.06520175188779831\n",
      "Epoch 6788/30000 Training Loss: 0.06442265212535858\n",
      "Epoch 6789/30000 Training Loss: 0.07756779342889786\n",
      "Epoch 6790/30000 Training Loss: 0.0651378482580185\n",
      "Epoch 6791/30000 Training Loss: 0.047462254762649536\n",
      "Epoch 6792/30000 Training Loss: 0.05215989053249359\n",
      "Epoch 6793/30000 Training Loss: 0.04824896901845932\n",
      "Epoch 6794/30000 Training Loss: 0.058999523520469666\n",
      "Epoch 6795/30000 Training Loss: 0.06164117902517319\n",
      "Epoch 6796/30000 Training Loss: 0.061657536774873734\n",
      "Epoch 6797/30000 Training Loss: 0.04094766080379486\n",
      "Epoch 6798/30000 Training Loss: 0.04660137742757797\n",
      "Epoch 6799/30000 Training Loss: 0.04642488807439804\n",
      "Epoch 6800/30000 Training Loss: 0.051114700734615326\n",
      "Epoch 6800/30000 Validation Loss: 0.05014072358608246\n",
      "Epoch 6801/30000 Training Loss: 0.04348480701446533\n",
      "Epoch 6802/30000 Training Loss: 0.05806493014097214\n",
      "Epoch 6803/30000 Training Loss: 0.05723913758993149\n",
      "Epoch 6804/30000 Training Loss: 0.05204777792096138\n",
      "Epoch 6805/30000 Training Loss: 0.04938635975122452\n",
      "Epoch 6806/30000 Training Loss: 0.05680873245000839\n",
      "Epoch 6807/30000 Training Loss: 0.04875630512833595\n",
      "Epoch 6808/30000 Training Loss: 0.051763854920864105\n",
      "Epoch 6809/30000 Training Loss: 0.06007472425699234\n",
      "Epoch 6810/30000 Training Loss: 0.04688546806573868\n",
      "Epoch 6811/30000 Training Loss: 0.04848878085613251\n",
      "Epoch 6812/30000 Training Loss: 0.05018818750977516\n",
      "Epoch 6813/30000 Training Loss: 0.056691739708185196\n",
      "Epoch 6814/30000 Training Loss: 0.05771062150597572\n",
      "Epoch 6815/30000 Training Loss: 0.06319769471883774\n",
      "Epoch 6816/30000 Training Loss: 0.0636247843503952\n",
      "Epoch 6817/30000 Training Loss: 0.051285672932863235\n",
      "Epoch 6818/30000 Training Loss: 0.0609610453248024\n",
      "Epoch 6819/30000 Training Loss: 0.05105500668287277\n",
      "Epoch 6820/30000 Training Loss: 0.045398641377687454\n",
      "Epoch 6821/30000 Training Loss: 0.05843431130051613\n",
      "Epoch 6822/30000 Training Loss: 0.052463848143815994\n",
      "Epoch 6823/30000 Training Loss: 0.06818538904190063\n",
      "Epoch 6824/30000 Training Loss: 0.041700467467308044\n",
      "Epoch 6825/30000 Training Loss: 0.05695100873708725\n",
      "Epoch 6826/30000 Training Loss: 0.05585344135761261\n",
      "Epoch 6827/30000 Training Loss: 0.04840618371963501\n",
      "Epoch 6828/30000 Training Loss: 0.05022115260362625\n",
      "Epoch 6829/30000 Training Loss: 0.05960248410701752\n",
      "Epoch 6830/30000 Training Loss: 0.045415688306093216\n",
      "Epoch 6831/30000 Training Loss: 0.07652399688959122\n",
      "Epoch 6832/30000 Training Loss: 0.06200651079416275\n",
      "Epoch 6833/30000 Training Loss: 0.03922098129987717\n",
      "Epoch 6834/30000 Training Loss: 0.05003805831074715\n",
      "Epoch 6835/30000 Training Loss: 0.0411536805331707\n",
      "Epoch 6836/30000 Training Loss: 0.05319898575544357\n",
      "Epoch 6837/30000 Training Loss: 0.06584475934505463\n",
      "Epoch 6838/30000 Training Loss: 0.04413831606507301\n",
      "Epoch 6839/30000 Training Loss: 0.049458496272563934\n",
      "Epoch 6840/30000 Training Loss: 0.05530855804681778\n",
      "Epoch 6841/30000 Training Loss: 0.05751030147075653\n",
      "Epoch 6842/30000 Training Loss: 0.05196809768676758\n",
      "Epoch 6843/30000 Training Loss: 0.06356584280729294\n",
      "Epoch 6844/30000 Training Loss: 0.05411524325609207\n",
      "Epoch 6845/30000 Training Loss: 0.06643025577068329\n",
      "Epoch 6846/30000 Training Loss: 0.04005049914121628\n",
      "Epoch 6847/30000 Training Loss: 0.05589237064123154\n",
      "Epoch 6848/30000 Training Loss: 0.06504281610250473\n",
      "Epoch 6849/30000 Training Loss: 0.03574107587337494\n",
      "Epoch 6850/30000 Training Loss: 0.04787846654653549\n",
      "Epoch 6851/30000 Training Loss: 0.06743445992469788\n",
      "Epoch 6852/30000 Training Loss: 0.04598512500524521\n",
      "Epoch 6853/30000 Training Loss: 0.04659168794751167\n",
      "Epoch 6854/30000 Training Loss: 0.04485081881284714\n",
      "Epoch 6855/30000 Training Loss: 0.05586634948849678\n",
      "Epoch 6856/30000 Training Loss: 0.05831587314605713\n",
      "Epoch 6857/30000 Training Loss: 0.042972005903720856\n",
      "Epoch 6858/30000 Training Loss: 0.049393974244594574\n",
      "Epoch 6859/30000 Training Loss: 0.04925680160522461\n",
      "Epoch 6860/30000 Training Loss: 0.050018034875392914\n",
      "Epoch 6861/30000 Training Loss: 0.057813555002212524\n",
      "Epoch 6862/30000 Training Loss: 0.05816769599914551\n",
      "Epoch 6863/30000 Training Loss: 0.047834157943725586\n",
      "Epoch 6864/30000 Training Loss: 0.04670226573944092\n",
      "Epoch 6865/30000 Training Loss: 0.05509442090988159\n",
      "Epoch 6866/30000 Training Loss: 0.04445362836122513\n",
      "Epoch 6867/30000 Training Loss: 0.04481447860598564\n",
      "Epoch 6868/30000 Training Loss: 0.05794759839773178\n",
      "Epoch 6869/30000 Training Loss: 0.04439733177423477\n",
      "Epoch 6870/30000 Training Loss: 0.04263126850128174\n",
      "Epoch 6871/30000 Training Loss: 0.06253426522016525\n",
      "Epoch 6872/30000 Training Loss: 0.03824871778488159\n",
      "Epoch 6873/30000 Training Loss: 0.05164431780576706\n",
      "Epoch 6874/30000 Training Loss: 0.04502597451210022\n",
      "Epoch 6875/30000 Training Loss: 0.04525993764400482\n",
      "Epoch 6876/30000 Training Loss: 0.050968050956726074\n",
      "Epoch 6877/30000 Training Loss: 0.06124015897512436\n",
      "Epoch 6878/30000 Training Loss: 0.050399452447891235\n",
      "Epoch 6879/30000 Training Loss: 0.048387981951236725\n",
      "Epoch 6880/30000 Training Loss: 0.04492630064487457\n",
      "Epoch 6881/30000 Training Loss: 0.06216295436024666\n",
      "Epoch 6882/30000 Training Loss: 0.04469076544046402\n",
      "Epoch 6883/30000 Training Loss: 0.04511556401848793\n",
      "Epoch 6884/30000 Training Loss: 0.05131521821022034\n",
      "Epoch 6885/30000 Training Loss: 0.07242567092180252\n",
      "Epoch 6886/30000 Training Loss: 0.0678032636642456\n",
      "Epoch 6887/30000 Training Loss: 0.0441514328122139\n",
      "Epoch 6888/30000 Training Loss: 0.05212143436074257\n",
      "Epoch 6889/30000 Training Loss: 0.06457966566085815\n",
      "Epoch 6890/30000 Training Loss: 0.05334530025720596\n",
      "Epoch 6891/30000 Training Loss: 0.04928546026349068\n",
      "Epoch 6892/30000 Training Loss: 0.046655986458063126\n",
      "Epoch 6893/30000 Training Loss: 0.0671202689409256\n",
      "Epoch 6894/30000 Training Loss: 0.05670308321714401\n",
      "Epoch 6895/30000 Training Loss: 0.042796000838279724\n",
      "Epoch 6896/30000 Training Loss: 0.05370994657278061\n",
      "Epoch 6897/30000 Training Loss: 0.0515485443174839\n",
      "Epoch 6898/30000 Training Loss: 0.04202135652303696\n",
      "Epoch 6899/30000 Training Loss: 0.04402594268321991\n",
      "Epoch 6900/30000 Training Loss: 0.0605151429772377\n",
      "Epoch 6900/30000 Validation Loss: 0.05620303004980087\n",
      "Epoch 6901/30000 Training Loss: 0.05059140920639038\n",
      "Epoch 6902/30000 Training Loss: 0.05072646588087082\n",
      "Epoch 6903/30000 Training Loss: 0.053954802453517914\n",
      "Epoch 6904/30000 Training Loss: 0.05695720762014389\n",
      "Epoch 6905/30000 Training Loss: 0.053876202553510666\n",
      "Epoch 6906/30000 Training Loss: 0.05932588875293732\n",
      "Epoch 6907/30000 Training Loss: 0.052960097789764404\n",
      "Epoch 6908/30000 Training Loss: 0.05404302477836609\n",
      "Epoch 6909/30000 Training Loss: 0.05908143147826195\n",
      "Epoch 6910/30000 Training Loss: 0.056879252195358276\n",
      "Epoch 6911/30000 Training Loss: 0.05785348266363144\n",
      "Epoch 6912/30000 Training Loss: 0.06209577992558479\n",
      "Epoch 6913/30000 Training Loss: 0.07835710793733597\n",
      "Epoch 6914/30000 Training Loss: 0.05681252107024193\n",
      "Epoch 6915/30000 Training Loss: 0.059249065816402435\n",
      "Epoch 6916/30000 Training Loss: 0.059996604919433594\n",
      "Epoch 6917/30000 Training Loss: 0.0580122172832489\n",
      "Epoch 6918/30000 Training Loss: 0.042960748076438904\n",
      "Epoch 6919/30000 Training Loss: 0.05322112888097763\n",
      "Epoch 6920/30000 Training Loss: 0.056789278984069824\n",
      "Epoch 6921/30000 Training Loss: 0.05339539796113968\n",
      "Epoch 6922/30000 Training Loss: 0.04387705773115158\n",
      "Epoch 6923/30000 Training Loss: 0.04335901886224747\n",
      "Epoch 6924/30000 Training Loss: 0.04747699946165085\n",
      "Epoch 6925/30000 Training Loss: 0.04719959571957588\n",
      "Epoch 6926/30000 Training Loss: 0.05447612330317497\n",
      "Epoch 6927/30000 Training Loss: 0.044037654995918274\n",
      "Epoch 6928/30000 Training Loss: 0.05283566564321518\n",
      "Epoch 6929/30000 Training Loss: 0.045671213418245316\n",
      "Epoch 6930/30000 Training Loss: 0.05853059142827988\n",
      "Epoch 6931/30000 Training Loss: 0.056844450533390045\n",
      "Epoch 6932/30000 Training Loss: 0.05091327056288719\n",
      "Epoch 6933/30000 Training Loss: 0.05866135656833649\n",
      "Epoch 6934/30000 Training Loss: 0.059829436242580414\n",
      "Epoch 6935/30000 Training Loss: 0.047620341181755066\n",
      "Epoch 6936/30000 Training Loss: 0.061094291508197784\n",
      "Epoch 6937/30000 Training Loss: 0.053659506142139435\n",
      "Epoch 6938/30000 Training Loss: 0.06078435480594635\n",
      "Epoch 6939/30000 Training Loss: 0.05079108104109764\n",
      "Epoch 6940/30000 Training Loss: 0.04347512498497963\n",
      "Epoch 6941/30000 Training Loss: 0.05327483266592026\n",
      "Epoch 6942/30000 Training Loss: 0.05247770622372627\n",
      "Epoch 6943/30000 Training Loss: 0.046452976763248444\n",
      "Epoch 6944/30000 Training Loss: 0.06326379626989365\n",
      "Epoch 6945/30000 Training Loss: 0.05124330893158913\n",
      "Epoch 6946/30000 Training Loss: 0.050470586866140366\n",
      "Epoch 6947/30000 Training Loss: 0.04821154102683067\n",
      "Epoch 6948/30000 Training Loss: 0.06081392616033554\n",
      "Epoch 6949/30000 Training Loss: 0.05581044405698776\n",
      "Epoch 6950/30000 Training Loss: 0.03674330562353134\n",
      "Epoch 6951/30000 Training Loss: 0.05835503339767456\n",
      "Epoch 6952/30000 Training Loss: 0.05627646669745445\n",
      "Epoch 6953/30000 Training Loss: 0.05093293637037277\n",
      "Epoch 6954/30000 Training Loss: 0.07554298639297485\n",
      "Epoch 6955/30000 Training Loss: 0.05168069899082184\n",
      "Epoch 6956/30000 Training Loss: 0.046583548188209534\n",
      "Epoch 6957/30000 Training Loss: 0.061806149780750275\n",
      "Epoch 6958/30000 Training Loss: 0.07027069479227066\n",
      "Epoch 6959/30000 Training Loss: 0.0714576467871666\n",
      "Epoch 6960/30000 Training Loss: 0.06540491431951523\n",
      "Epoch 6961/30000 Training Loss: 0.05070088058710098\n",
      "Epoch 6962/30000 Training Loss: 0.05511990189552307\n",
      "Epoch 6963/30000 Training Loss: 0.059688884764909744\n",
      "Epoch 6964/30000 Training Loss: 0.03742484375834465\n",
      "Epoch 6965/30000 Training Loss: 0.049937885254621506\n",
      "Epoch 6966/30000 Training Loss: 0.06208653002977371\n",
      "Epoch 6967/30000 Training Loss: 0.03956643491983414\n",
      "Epoch 6968/30000 Training Loss: 0.05001591145992279\n",
      "Epoch 6969/30000 Training Loss: 0.04729529470205307\n",
      "Epoch 6970/30000 Training Loss: 0.05083705112338066\n",
      "Epoch 6971/30000 Training Loss: 0.05069774016737938\n",
      "Epoch 6972/30000 Training Loss: 0.053911395370960236\n",
      "Epoch 6973/30000 Training Loss: 0.04910126328468323\n",
      "Epoch 6974/30000 Training Loss: 0.05134803056716919\n",
      "Epoch 6975/30000 Training Loss: 0.05461766570806503\n",
      "Epoch 6976/30000 Training Loss: 0.06534437835216522\n",
      "Epoch 6977/30000 Training Loss: 0.048081301152706146\n",
      "Epoch 6978/30000 Training Loss: 0.0602605864405632\n",
      "Epoch 6979/30000 Training Loss: 0.04553711786866188\n",
      "Epoch 6980/30000 Training Loss: 0.058969613164663315\n",
      "Epoch 6981/30000 Training Loss: 0.041162289679050446\n",
      "Epoch 6982/30000 Training Loss: 0.058216292411088943\n",
      "Epoch 6983/30000 Training Loss: 0.053624339401721954\n",
      "Epoch 6984/30000 Training Loss: 0.06968928128480911\n",
      "Epoch 6985/30000 Training Loss: 0.062960684299469\n",
      "Epoch 6986/30000 Training Loss: 0.05241498351097107\n",
      "Epoch 6987/30000 Training Loss: 0.0408647283911705\n",
      "Epoch 6988/30000 Training Loss: 0.06293555349111557\n",
      "Epoch 6989/30000 Training Loss: 0.0612972192466259\n",
      "Epoch 6990/30000 Training Loss: 0.05014166980981827\n",
      "Epoch 6991/30000 Training Loss: 0.05922764912247658\n",
      "Epoch 6992/30000 Training Loss: 0.06858867406845093\n",
      "Epoch 6993/30000 Training Loss: 0.06289103627204895\n",
      "Epoch 6994/30000 Training Loss: 0.056307412683963776\n",
      "Epoch 6995/30000 Training Loss: 0.04542875662446022\n",
      "Epoch 6996/30000 Training Loss: 0.05870255082845688\n",
      "Epoch 6997/30000 Training Loss: 0.047239869832992554\n",
      "Epoch 6998/30000 Training Loss: 0.04374288022518158\n",
      "Epoch 6999/30000 Training Loss: 0.06381645798683167\n",
      "Epoch 7000/30000 Training Loss: 0.048251718282699585\n",
      "Epoch 7000/30000 Validation Loss: 0.06215311586856842\n",
      "Epoch 7001/30000 Training Loss: 0.06405102461576462\n",
      "Epoch 7002/30000 Training Loss: 0.058658480644226074\n",
      "Epoch 7003/30000 Training Loss: 0.0456228107213974\n",
      "Epoch 7004/30000 Training Loss: 0.03670111298561096\n",
      "Epoch 7005/30000 Training Loss: 0.05154235288500786\n",
      "Epoch 7006/30000 Training Loss: 0.07573498785495758\n",
      "Epoch 7007/30000 Training Loss: 0.05122128501534462\n",
      "Epoch 7008/30000 Training Loss: 0.051294565200805664\n",
      "Epoch 7009/30000 Training Loss: 0.06369063258171082\n",
      "Epoch 7010/30000 Training Loss: 0.053965020924806595\n",
      "Epoch 7011/30000 Training Loss: 0.05903884768486023\n",
      "Epoch 7012/30000 Training Loss: 0.0406247079372406\n",
      "Epoch 7013/30000 Training Loss: 0.040561333298683167\n",
      "Epoch 7014/30000 Training Loss: 0.06097729504108429\n",
      "Epoch 7015/30000 Training Loss: 0.05881745368242264\n",
      "Epoch 7016/30000 Training Loss: 0.06183195859193802\n",
      "Epoch 7017/30000 Training Loss: 0.06301561743021011\n",
      "Epoch 7018/30000 Training Loss: 0.05157719552516937\n",
      "Epoch 7019/30000 Training Loss: 0.047888822853565216\n",
      "Epoch 7020/30000 Training Loss: 0.058573588728904724\n",
      "Epoch 7021/30000 Training Loss: 0.04360884055495262\n",
      "Epoch 7022/30000 Training Loss: 0.06689374148845673\n",
      "Epoch 7023/30000 Training Loss: 0.048028718680143356\n",
      "Epoch 7024/30000 Training Loss: 0.04415225237607956\n",
      "Epoch 7025/30000 Training Loss: 0.06311224400997162\n",
      "Epoch 7026/30000 Training Loss: 0.055292241275310516\n",
      "Epoch 7027/30000 Training Loss: 0.05157740041613579\n",
      "Epoch 7028/30000 Training Loss: 0.050534285604953766\n",
      "Epoch 7029/30000 Training Loss: 0.04806641861796379\n",
      "Epoch 7030/30000 Training Loss: 0.038111090660095215\n",
      "Epoch 7031/30000 Training Loss: 0.052115559577941895\n",
      "Epoch 7032/30000 Training Loss: 0.07419896125793457\n",
      "Epoch 7033/30000 Training Loss: 0.046927742660045624\n",
      "Epoch 7034/30000 Training Loss: 0.05324860289692879\n",
      "Epoch 7035/30000 Training Loss: 0.05402187258005142\n",
      "Epoch 7036/30000 Training Loss: 0.0481657050549984\n",
      "Epoch 7037/30000 Training Loss: 0.05278743803501129\n",
      "Epoch 7038/30000 Training Loss: 0.05013961344957352\n",
      "Epoch 7039/30000 Training Loss: 0.0398273691534996\n",
      "Epoch 7040/30000 Training Loss: 0.04386039823293686\n",
      "Epoch 7041/30000 Training Loss: 0.053629450500011444\n",
      "Epoch 7042/30000 Training Loss: 0.04359475523233414\n",
      "Epoch 7043/30000 Training Loss: 0.04198865965008736\n",
      "Epoch 7044/30000 Training Loss: 0.05374514311552048\n",
      "Epoch 7045/30000 Training Loss: 0.04392921179533005\n",
      "Epoch 7046/30000 Training Loss: 0.05594303086400032\n",
      "Epoch 7047/30000 Training Loss: 0.047206804156303406\n",
      "Epoch 7048/30000 Training Loss: 0.061493419110774994\n",
      "Epoch 7049/30000 Training Loss: 0.056529898196458817\n",
      "Epoch 7050/30000 Training Loss: 0.05106406658887863\n",
      "Epoch 7051/30000 Training Loss: 0.049337759613990784\n",
      "Epoch 7052/30000 Training Loss: 0.04363902658224106\n",
      "Epoch 7053/30000 Training Loss: 0.04954152554273605\n",
      "Epoch 7054/30000 Training Loss: 0.043901801109313965\n",
      "Epoch 7055/30000 Training Loss: 0.05752938985824585\n",
      "Epoch 7056/30000 Training Loss: 0.05484951287508011\n",
      "Epoch 7057/30000 Training Loss: 0.047647874802351\n",
      "Epoch 7058/30000 Training Loss: 0.05913496017456055\n",
      "Epoch 7059/30000 Training Loss: 0.05508577078580856\n",
      "Epoch 7060/30000 Training Loss: 0.052197765558958054\n",
      "Epoch 7061/30000 Training Loss: 0.04635997489094734\n",
      "Epoch 7062/30000 Training Loss: 0.05758043751120567\n",
      "Epoch 7063/30000 Training Loss: 0.04785141348838806\n",
      "Epoch 7064/30000 Training Loss: 0.06137771159410477\n",
      "Epoch 7065/30000 Training Loss: 0.061471909284591675\n",
      "Epoch 7066/30000 Training Loss: 0.0446825847029686\n",
      "Epoch 7067/30000 Training Loss: 0.049283213913440704\n",
      "Epoch 7068/30000 Training Loss: 0.04303578659892082\n",
      "Epoch 7069/30000 Training Loss: 0.053906362503767014\n",
      "Epoch 7070/30000 Training Loss: 0.05978945642709732\n",
      "Epoch 7071/30000 Training Loss: 0.046097707003355026\n",
      "Epoch 7072/30000 Training Loss: 0.05777772516012192\n",
      "Epoch 7073/30000 Training Loss: 0.04921925067901611\n",
      "Epoch 7074/30000 Training Loss: 0.06497939676046371\n",
      "Epoch 7075/30000 Training Loss: 0.04412270337343216\n",
      "Epoch 7076/30000 Training Loss: 0.037419285625219345\n",
      "Epoch 7077/30000 Training Loss: 0.06755812466144562\n",
      "Epoch 7078/30000 Training Loss: 0.048657916486263275\n",
      "Epoch 7079/30000 Training Loss: 0.054417647421360016\n",
      "Epoch 7080/30000 Training Loss: 0.053754210472106934\n",
      "Epoch 7081/30000 Training Loss: 0.04630424827337265\n",
      "Epoch 7082/30000 Training Loss: 0.048618853092193604\n",
      "Epoch 7083/30000 Training Loss: 0.05003061518073082\n",
      "Epoch 7084/30000 Training Loss: 0.05890733748674393\n",
      "Epoch 7085/30000 Training Loss: 0.05323799327015877\n",
      "Epoch 7086/30000 Training Loss: 0.05042753741145134\n",
      "Epoch 7087/30000 Training Loss: 0.05730234831571579\n",
      "Epoch 7088/30000 Training Loss: 0.051512010395526886\n",
      "Epoch 7089/30000 Training Loss: 0.06734563410282135\n",
      "Epoch 7090/30000 Training Loss: 0.04857854172587395\n",
      "Epoch 7091/30000 Training Loss: 0.06183166056871414\n",
      "Epoch 7092/30000 Training Loss: 0.0724351555109024\n",
      "Epoch 7093/30000 Training Loss: 0.0431034192442894\n",
      "Epoch 7094/30000 Training Loss: 0.058493755757808685\n",
      "Epoch 7095/30000 Training Loss: 0.05214231461286545\n",
      "Epoch 7096/30000 Training Loss: 0.03695393353700638\n",
      "Epoch 7097/30000 Training Loss: 0.06303324550390244\n",
      "Epoch 7098/30000 Training Loss: 0.051055558025836945\n",
      "Epoch 7099/30000 Training Loss: 0.049346357583999634\n",
      "Epoch 7100/30000 Training Loss: 0.072625532746315\n",
      "Epoch 7100/30000 Validation Loss: 0.04978179559111595\n",
      "Epoch 7101/30000 Training Loss: 0.06075136363506317\n",
      "Epoch 7102/30000 Training Loss: 0.056151650846004486\n",
      "Epoch 7103/30000 Training Loss: 0.06791148334741592\n",
      "Epoch 7104/30000 Training Loss: 0.04406934231519699\n",
      "Epoch 7105/30000 Training Loss: 0.05301856994628906\n",
      "Epoch 7106/30000 Training Loss: 0.0681319311261177\n",
      "Epoch 7107/30000 Training Loss: 0.04993704706430435\n",
      "Epoch 7108/30000 Training Loss: 0.05469230189919472\n",
      "Epoch 7109/30000 Training Loss: 0.047900207340717316\n",
      "Epoch 7110/30000 Training Loss: 0.05996015667915344\n",
      "Epoch 7111/30000 Training Loss: 0.04289653152227402\n",
      "Epoch 7112/30000 Training Loss: 0.05082101374864578\n",
      "Epoch 7113/30000 Training Loss: 0.056892503052949905\n",
      "Epoch 7114/30000 Training Loss: 0.06092873215675354\n",
      "Epoch 7115/30000 Training Loss: 0.043010517954826355\n",
      "Epoch 7116/30000 Training Loss: 0.05511562526226044\n",
      "Epoch 7117/30000 Training Loss: 0.06413638591766357\n",
      "Epoch 7118/30000 Training Loss: 0.05439608544111252\n",
      "Epoch 7119/30000 Training Loss: 0.060974229127168655\n",
      "Epoch 7120/30000 Training Loss: 0.05268027260899544\n",
      "Epoch 7121/30000 Training Loss: 0.04882969707250595\n",
      "Epoch 7122/30000 Training Loss: 0.04967391863465309\n",
      "Epoch 7123/30000 Training Loss: 0.04210851714015007\n",
      "Epoch 7124/30000 Training Loss: 0.04440177604556084\n",
      "Epoch 7125/30000 Training Loss: 0.049120381474494934\n",
      "Epoch 7126/30000 Training Loss: 0.050104834139347076\n",
      "Epoch 7127/30000 Training Loss: 0.04959646239876747\n",
      "Epoch 7128/30000 Training Loss: 0.07206174731254578\n",
      "Epoch 7129/30000 Training Loss: 0.057829245924949646\n",
      "Epoch 7130/30000 Training Loss: 0.04980212450027466\n",
      "Epoch 7131/30000 Training Loss: 0.048923470079898834\n",
      "Epoch 7132/30000 Training Loss: 0.04175813868641853\n",
      "Epoch 7133/30000 Training Loss: 0.05081194266676903\n",
      "Epoch 7134/30000 Training Loss: 0.07037847489118576\n",
      "Epoch 7135/30000 Training Loss: 0.06352341920137405\n",
      "Epoch 7136/30000 Training Loss: 0.04997013881802559\n",
      "Epoch 7137/30000 Training Loss: 0.05589643865823746\n",
      "Epoch 7138/30000 Training Loss: 0.05194048210978508\n",
      "Epoch 7139/30000 Training Loss: 0.06550423800945282\n",
      "Epoch 7140/30000 Training Loss: 0.0641527771949768\n",
      "Epoch 7141/30000 Training Loss: 0.069607213139534\n",
      "Epoch 7142/30000 Training Loss: 0.036576129496097565\n",
      "Epoch 7143/30000 Training Loss: 0.05838973820209503\n",
      "Epoch 7144/30000 Training Loss: 0.04558861628174782\n",
      "Epoch 7145/30000 Training Loss: 0.051622942090034485\n",
      "Epoch 7146/30000 Training Loss: 0.04412863403558731\n",
      "Epoch 7147/30000 Training Loss: 0.046534597873687744\n",
      "Epoch 7148/30000 Training Loss: 0.04106491059064865\n",
      "Epoch 7149/30000 Training Loss: 0.053503669798374176\n",
      "Epoch 7150/30000 Training Loss: 0.05517490208148956\n",
      "Epoch 7151/30000 Training Loss: 0.0395282618701458\n",
      "Epoch 7152/30000 Training Loss: 0.04832816123962402\n",
      "Epoch 7153/30000 Training Loss: 0.04764372110366821\n",
      "Epoch 7154/30000 Training Loss: 0.06170783191919327\n",
      "Epoch 7155/30000 Training Loss: 0.06287597119808197\n",
      "Epoch 7156/30000 Training Loss: 0.05765560269355774\n",
      "Epoch 7157/30000 Training Loss: 0.03542014956474304\n",
      "Epoch 7158/30000 Training Loss: 0.05859886854887009\n",
      "Epoch 7159/30000 Training Loss: 0.059494499117136\n",
      "Epoch 7160/30000 Training Loss: 0.04542616382241249\n",
      "Epoch 7161/30000 Training Loss: 0.0615018755197525\n",
      "Epoch 7162/30000 Training Loss: 0.0562158077955246\n",
      "Epoch 7163/30000 Training Loss: 0.048983924090862274\n",
      "Epoch 7164/30000 Training Loss: 0.05046272650361061\n",
      "Epoch 7165/30000 Training Loss: 0.057736415416002274\n",
      "Epoch 7166/30000 Training Loss: 0.07216145098209381\n",
      "Epoch 7167/30000 Training Loss: 0.05094718560576439\n",
      "Epoch 7168/30000 Training Loss: 0.04678385704755783\n",
      "Epoch 7169/30000 Training Loss: 0.03919440507888794\n",
      "Epoch 7170/30000 Training Loss: 0.04483095556497574\n",
      "Epoch 7171/30000 Training Loss: 0.05779695138335228\n",
      "Epoch 7172/30000 Training Loss: 0.05972803384065628\n",
      "Epoch 7173/30000 Training Loss: 0.05985897779464722\n",
      "Epoch 7174/30000 Training Loss: 0.06952179223299026\n",
      "Epoch 7175/30000 Training Loss: 0.05305422842502594\n",
      "Epoch 7176/30000 Training Loss: 0.051347918808460236\n",
      "Epoch 7177/30000 Training Loss: 0.04438416287302971\n",
      "Epoch 7178/30000 Training Loss: 0.07462532073259354\n",
      "Epoch 7179/30000 Training Loss: 0.043467529118061066\n",
      "Epoch 7180/30000 Training Loss: 0.03792744502425194\n",
      "Epoch 7181/30000 Training Loss: 0.0449400320649147\n",
      "Epoch 7182/30000 Training Loss: 0.03865572065114975\n",
      "Epoch 7183/30000 Training Loss: 0.0608106330037117\n",
      "Epoch 7184/30000 Training Loss: 0.07307317107915878\n",
      "Epoch 7185/30000 Training Loss: 0.04675537347793579\n",
      "Epoch 7186/30000 Training Loss: 0.043762676417827606\n",
      "Epoch 7187/30000 Training Loss: 0.06224998086690903\n",
      "Epoch 7188/30000 Training Loss: 0.058265991508960724\n",
      "Epoch 7189/30000 Training Loss: 0.05881655216217041\n",
      "Epoch 7190/30000 Training Loss: 0.05090774968266487\n",
      "Epoch 7191/30000 Training Loss: 0.03707115352153778\n",
      "Epoch 7192/30000 Training Loss: 0.05702657252550125\n",
      "Epoch 7193/30000 Training Loss: 0.0626392588019371\n",
      "Epoch 7194/30000 Training Loss: 0.0633433535695076\n",
      "Epoch 7195/30000 Training Loss: 0.06397099792957306\n",
      "Epoch 7196/30000 Training Loss: 0.0557873472571373\n",
      "Epoch 7197/30000 Training Loss: 0.04703690856695175\n",
      "Epoch 7198/30000 Training Loss: 0.04200197383761406\n",
      "Epoch 7199/30000 Training Loss: 0.04178299009799957\n",
      "Epoch 7200/30000 Training Loss: 0.06134537607431412\n",
      "Epoch 7200/30000 Validation Loss: 0.046180032193660736\n",
      "Epoch 7201/30000 Training Loss: 0.04752764850854874\n",
      "Epoch 7202/30000 Training Loss: 0.0524575412273407\n",
      "Epoch 7203/30000 Training Loss: 0.0439932644367218\n",
      "Epoch 7204/30000 Training Loss: 0.06919476389884949\n",
      "Epoch 7205/30000 Training Loss: 0.04811987280845642\n",
      "Epoch 7206/30000 Training Loss: 0.06100329011678696\n",
      "Epoch 7207/30000 Training Loss: 0.040658846497535706\n",
      "Epoch 7208/30000 Training Loss: 0.05678578466176987\n",
      "Epoch 7209/30000 Training Loss: 0.055381692945957184\n",
      "Epoch 7210/30000 Training Loss: 0.05474681779742241\n",
      "Epoch 7211/30000 Training Loss: 0.055572472512722015\n",
      "Epoch 7212/30000 Training Loss: 0.059853389859199524\n",
      "Epoch 7213/30000 Training Loss: 0.046464353799819946\n",
      "Epoch 7214/30000 Training Loss: 0.05190642923116684\n",
      "Epoch 7215/30000 Training Loss: 0.06408695876598358\n",
      "Epoch 7216/30000 Training Loss: 0.04120080918073654\n",
      "Epoch 7217/30000 Training Loss: 0.07503437250852585\n",
      "Epoch 7218/30000 Training Loss: 0.051168397068977356\n",
      "Epoch 7219/30000 Training Loss: 0.04397626966238022\n",
      "Epoch 7220/30000 Training Loss: 0.04479040578007698\n",
      "Epoch 7221/30000 Training Loss: 0.05462416261434555\n",
      "Epoch 7222/30000 Training Loss: 0.05980735272169113\n",
      "Epoch 7223/30000 Training Loss: 0.04454536736011505\n",
      "Epoch 7224/30000 Training Loss: 0.06893335282802582\n",
      "Epoch 7225/30000 Training Loss: 0.05996501073241234\n",
      "Epoch 7226/30000 Training Loss: 0.055987078696489334\n",
      "Epoch 7227/30000 Training Loss: 0.03348325565457344\n",
      "Epoch 7228/30000 Training Loss: 0.04329191893339157\n",
      "Epoch 7229/30000 Training Loss: 0.05410177260637283\n",
      "Epoch 7230/30000 Training Loss: 0.061984457075595856\n",
      "Epoch 7231/30000 Training Loss: 0.051915545016527176\n",
      "Epoch 7232/30000 Training Loss: 0.05394019931554794\n",
      "Epoch 7233/30000 Training Loss: 0.029731128364801407\n",
      "Epoch 7234/30000 Training Loss: 0.06546923518180847\n",
      "Epoch 7235/30000 Training Loss: 0.05463328957557678\n",
      "Epoch 7236/30000 Training Loss: 0.04741812124848366\n",
      "Epoch 7237/30000 Training Loss: 0.06073812022805214\n",
      "Epoch 7238/30000 Training Loss: 0.0581541508436203\n",
      "Epoch 7239/30000 Training Loss: 0.055809203535318375\n",
      "Epoch 7240/30000 Training Loss: 0.049523089081048965\n",
      "Epoch 7241/30000 Training Loss: 0.0571700781583786\n",
      "Epoch 7242/30000 Training Loss: 0.04660952836275101\n",
      "Epoch 7243/30000 Training Loss: 0.0688326433300972\n",
      "Epoch 7244/30000 Training Loss: 0.04532428830862045\n",
      "Epoch 7245/30000 Training Loss: 0.04991088807582855\n",
      "Epoch 7246/30000 Training Loss: 0.05367113649845123\n",
      "Epoch 7247/30000 Training Loss: 0.04692336171865463\n",
      "Epoch 7248/30000 Training Loss: 0.05302530527114868\n",
      "Epoch 7249/30000 Training Loss: 0.04893076792359352\n",
      "Epoch 7250/30000 Training Loss: 0.038500599563121796\n",
      "Epoch 7251/30000 Training Loss: 0.057852402329444885\n",
      "Epoch 7252/30000 Training Loss: 0.0512341745197773\n",
      "Epoch 7253/30000 Training Loss: 0.048096008598804474\n",
      "Epoch 7254/30000 Training Loss: 0.06483232975006104\n",
      "Epoch 7255/30000 Training Loss: 0.05083972215652466\n",
      "Epoch 7256/30000 Training Loss: 0.049747534096241\n",
      "Epoch 7257/30000 Training Loss: 0.05087544023990631\n",
      "Epoch 7258/30000 Training Loss: 0.043842483311891556\n",
      "Epoch 7259/30000 Training Loss: 0.05002276599407196\n",
      "Epoch 7260/30000 Training Loss: 0.05084223672747612\n",
      "Epoch 7261/30000 Training Loss: 0.047984808683395386\n",
      "Epoch 7262/30000 Training Loss: 0.06126254051923752\n",
      "Epoch 7263/30000 Training Loss: 0.05172252655029297\n",
      "Epoch 7264/30000 Training Loss: 0.06355735659599304\n",
      "Epoch 7265/30000 Training Loss: 0.046671707183122635\n",
      "Epoch 7266/30000 Training Loss: 0.05117085948586464\n",
      "Epoch 7267/30000 Training Loss: 0.044787876307964325\n",
      "Epoch 7268/30000 Training Loss: 0.041000835597515106\n",
      "Epoch 7269/30000 Training Loss: 0.04733129218220711\n",
      "Epoch 7270/30000 Training Loss: 0.037927672266960144\n",
      "Epoch 7271/30000 Training Loss: 0.05216280370950699\n",
      "Epoch 7272/30000 Training Loss: 0.05355009436607361\n",
      "Epoch 7273/30000 Training Loss: 0.04127541929483414\n",
      "Epoch 7274/30000 Training Loss: 0.05340708792209625\n",
      "Epoch 7275/30000 Training Loss: 0.04112929478287697\n",
      "Epoch 7276/30000 Training Loss: 0.05121111869812012\n",
      "Epoch 7277/30000 Training Loss: 0.0473107285797596\n",
      "Epoch 7278/30000 Training Loss: 0.047600194811820984\n",
      "Epoch 7279/30000 Training Loss: 0.06109337881207466\n",
      "Epoch 7280/30000 Training Loss: 0.0555112287402153\n",
      "Epoch 7281/30000 Training Loss: 0.05671607702970505\n",
      "Epoch 7282/30000 Training Loss: 0.05168361961841583\n",
      "Epoch 7283/30000 Training Loss: 0.06846760213375092\n",
      "Epoch 7284/30000 Training Loss: 0.04849337041378021\n",
      "Epoch 7285/30000 Training Loss: 0.04792178422212601\n",
      "Epoch 7286/30000 Training Loss: 0.05760740488767624\n",
      "Epoch 7287/30000 Training Loss: 0.0465608611702919\n",
      "Epoch 7288/30000 Training Loss: 0.0678757056593895\n",
      "Epoch 7289/30000 Training Loss: 0.05883006006479263\n",
      "Epoch 7290/30000 Training Loss: 0.06022743135690689\n",
      "Epoch 7291/30000 Training Loss: 0.04055245220661163\n",
      "Epoch 7292/30000 Training Loss: 0.060264624655246735\n",
      "Epoch 7293/30000 Training Loss: 0.06295203417539597\n",
      "Epoch 7294/30000 Training Loss: 0.047692760825157166\n",
      "Epoch 7295/30000 Training Loss: 0.05105932429432869\n",
      "Epoch 7296/30000 Training Loss: 0.06148625537753105\n",
      "Epoch 7297/30000 Training Loss: 0.05112750828266144\n",
      "Epoch 7298/30000 Training Loss: 0.042891375720500946\n",
      "Epoch 7299/30000 Training Loss: 0.042952053248882294\n",
      "Epoch 7300/30000 Training Loss: 0.05896177515387535\n",
      "Epoch 7300/30000 Validation Loss: 0.05253119394183159\n",
      "Epoch 7301/30000 Training Loss: 0.055805716663599014\n",
      "Epoch 7302/30000 Training Loss: 0.059368886053562164\n",
      "Epoch 7303/30000 Training Loss: 0.05157475918531418\n",
      "Epoch 7304/30000 Training Loss: 0.0396895669400692\n",
      "Epoch 7305/30000 Training Loss: 0.03924306482076645\n",
      "Epoch 7306/30000 Training Loss: 0.061125755310058594\n",
      "Epoch 7307/30000 Training Loss: 0.057311512529850006\n",
      "Epoch 7308/30000 Training Loss: 0.047966279089450836\n",
      "Epoch 7309/30000 Training Loss: 0.040768977254629135\n",
      "Epoch 7310/30000 Training Loss: 0.0544796958565712\n",
      "Epoch 7311/30000 Training Loss: 0.051382310688495636\n",
      "Epoch 7312/30000 Training Loss: 0.057083845138549805\n",
      "Epoch 7313/30000 Training Loss: 0.04253172129392624\n",
      "Epoch 7314/30000 Training Loss: 0.05024372786283493\n",
      "Epoch 7315/30000 Training Loss: 0.04618768021464348\n",
      "Epoch 7316/30000 Training Loss: 0.07953055202960968\n",
      "Epoch 7317/30000 Training Loss: 0.06975996494293213\n",
      "Epoch 7318/30000 Training Loss: 0.05331116542220116\n",
      "Epoch 7319/30000 Training Loss: 0.051887303590774536\n",
      "Epoch 7320/30000 Training Loss: 0.05050622671842575\n",
      "Epoch 7321/30000 Training Loss: 0.061635132879018784\n",
      "Epoch 7322/30000 Training Loss: 0.047624584287405014\n",
      "Epoch 7323/30000 Training Loss: 0.0484234094619751\n",
      "Epoch 7324/30000 Training Loss: 0.04364223778247833\n",
      "Epoch 7325/30000 Training Loss: 0.06043985113501549\n",
      "Epoch 7326/30000 Training Loss: 0.05972418561577797\n",
      "Epoch 7327/30000 Training Loss: 0.048297490924596786\n",
      "Epoch 7328/30000 Training Loss: 0.05938643962144852\n",
      "Epoch 7329/30000 Training Loss: 0.04522670805454254\n",
      "Epoch 7330/30000 Training Loss: 0.05662446841597557\n",
      "Epoch 7331/30000 Training Loss: 0.06791015714406967\n",
      "Epoch 7332/30000 Training Loss: 0.046894997358322144\n",
      "Epoch 7333/30000 Training Loss: 0.05260109156370163\n",
      "Epoch 7334/30000 Training Loss: 0.04573813080787659\n",
      "Epoch 7335/30000 Training Loss: 0.04075770080089569\n",
      "Epoch 7336/30000 Training Loss: 0.07180092483758926\n",
      "Epoch 7337/30000 Training Loss: 0.046047355979681015\n",
      "Epoch 7338/30000 Training Loss: 0.03424544632434845\n",
      "Epoch 7339/30000 Training Loss: 0.06202000379562378\n",
      "Epoch 7340/30000 Training Loss: 0.05499689280986786\n",
      "Epoch 7341/30000 Training Loss: 0.05367062985897064\n",
      "Epoch 7342/30000 Training Loss: 0.06166854500770569\n",
      "Epoch 7343/30000 Training Loss: 0.04916328191757202\n",
      "Epoch 7344/30000 Training Loss: 0.042790815234184265\n",
      "Epoch 7345/30000 Training Loss: 0.04896465688943863\n",
      "Epoch 7346/30000 Training Loss: 0.06458117067813873\n",
      "Epoch 7347/30000 Training Loss: 0.05179501697421074\n",
      "Epoch 7348/30000 Training Loss: 0.05269419401884079\n",
      "Epoch 7349/30000 Training Loss: 0.0424380749464035\n",
      "Epoch 7350/30000 Training Loss: 0.0573524534702301\n",
      "Epoch 7351/30000 Training Loss: 0.03630194067955017\n",
      "Epoch 7352/30000 Training Loss: 0.05717472732067108\n",
      "Epoch 7353/30000 Training Loss: 0.06176652014255524\n",
      "Epoch 7354/30000 Training Loss: 0.041234999895095825\n",
      "Epoch 7355/30000 Training Loss: 0.06579741090536118\n",
      "Epoch 7356/30000 Training Loss: 0.04442159831523895\n",
      "Epoch 7357/30000 Training Loss: 0.05503222718834877\n",
      "Epoch 7358/30000 Training Loss: 0.0521661713719368\n",
      "Epoch 7359/30000 Training Loss: 0.04939908906817436\n",
      "Epoch 7360/30000 Training Loss: 0.05473052337765694\n",
      "Epoch 7361/30000 Training Loss: 0.054782576858997345\n",
      "Epoch 7362/30000 Training Loss: 0.058854322880506516\n",
      "Epoch 7363/30000 Training Loss: 0.046268582344055176\n",
      "Epoch 7364/30000 Training Loss: 0.05564238131046295\n",
      "Epoch 7365/30000 Training Loss: 0.04812178760766983\n",
      "Epoch 7366/30000 Training Loss: 0.0602545291185379\n",
      "Epoch 7367/30000 Training Loss: 0.046363674104213715\n",
      "Epoch 7368/30000 Training Loss: 0.057994235306978226\n",
      "Epoch 7369/30000 Training Loss: 0.055740632116794586\n",
      "Epoch 7370/30000 Training Loss: 0.05821533501148224\n",
      "Epoch 7371/30000 Training Loss: 0.04427552968263626\n",
      "Epoch 7372/30000 Training Loss: 0.06231076270341873\n",
      "Epoch 7373/30000 Training Loss: 0.057214558124542236\n",
      "Epoch 7374/30000 Training Loss: 0.04576297849416733\n",
      "Epoch 7375/30000 Training Loss: 0.051876001060009\n",
      "Epoch 7376/30000 Training Loss: 0.0722101628780365\n",
      "Epoch 7377/30000 Training Loss: 0.05226763337850571\n",
      "Epoch 7378/30000 Training Loss: 0.050473470240831375\n",
      "Epoch 7379/30000 Training Loss: 0.04711923003196716\n",
      "Epoch 7380/30000 Training Loss: 0.062463801354169846\n",
      "Epoch 7381/30000 Training Loss: 0.046905696392059326\n",
      "Epoch 7382/30000 Training Loss: 0.0632271021604538\n",
      "Epoch 7383/30000 Training Loss: 0.04351021721959114\n",
      "Epoch 7384/30000 Training Loss: 0.06639928370714188\n",
      "Epoch 7385/30000 Training Loss: 0.03715953603386879\n",
      "Epoch 7386/30000 Training Loss: 0.061726778745651245\n",
      "Epoch 7387/30000 Training Loss: 0.04228769987821579\n",
      "Epoch 7388/30000 Training Loss: 0.04740625619888306\n",
      "Epoch 7389/30000 Training Loss: 0.05653282627463341\n",
      "Epoch 7390/30000 Training Loss: 0.04977619647979736\n",
      "Epoch 7391/30000 Training Loss: 0.06251175701618195\n",
      "Epoch 7392/30000 Training Loss: 0.058645591139793396\n",
      "Epoch 7393/30000 Training Loss: 0.053878303617239\n",
      "Epoch 7394/30000 Training Loss: 0.043899692595005035\n",
      "Epoch 7395/30000 Training Loss: 0.053129494190216064\n",
      "Epoch 7396/30000 Training Loss: 0.05325264111161232\n",
      "Epoch 7397/30000 Training Loss: 0.0486934520304203\n",
      "Epoch 7398/30000 Training Loss: 0.05494970828294754\n",
      "Epoch 7399/30000 Training Loss: 0.04413015767931938\n",
      "Epoch 7400/30000 Training Loss: 0.05974985659122467\n",
      "Epoch 7400/30000 Validation Loss: 0.06522165983915329\n",
      "Epoch 7401/30000 Training Loss: 0.06949331611394882\n",
      "Epoch 7402/30000 Training Loss: 0.0437912717461586\n",
      "Epoch 7403/30000 Training Loss: 0.053963545709848404\n",
      "Epoch 7404/30000 Training Loss: 0.04774990677833557\n",
      "Epoch 7405/30000 Training Loss: 0.0583522655069828\n",
      "Epoch 7406/30000 Training Loss: 0.08011040091514587\n",
      "Epoch 7407/30000 Training Loss: 0.043933235108852386\n",
      "Epoch 7408/30000 Training Loss: 0.051900289952754974\n",
      "Epoch 7409/30000 Training Loss: 0.06404951214790344\n",
      "Epoch 7410/30000 Training Loss: 0.04913198947906494\n",
      "Epoch 7411/30000 Training Loss: 0.05636253207921982\n",
      "Epoch 7412/30000 Training Loss: 0.05984152853488922\n",
      "Epoch 7413/30000 Training Loss: 0.055807359516620636\n",
      "Epoch 7414/30000 Training Loss: 0.05210127308964729\n",
      "Epoch 7415/30000 Training Loss: 0.06078768149018288\n",
      "Epoch 7416/30000 Training Loss: 0.052190713584423065\n",
      "Epoch 7417/30000 Training Loss: 0.05182782933115959\n",
      "Epoch 7418/30000 Training Loss: 0.06916123628616333\n",
      "Epoch 7419/30000 Training Loss: 0.06236475706100464\n",
      "Epoch 7420/30000 Training Loss: 0.05904195457696915\n",
      "Epoch 7421/30000 Training Loss: 0.05849436670541763\n",
      "Epoch 7422/30000 Training Loss: 0.0556052103638649\n",
      "Epoch 7423/30000 Training Loss: 0.05125133693218231\n",
      "Epoch 7424/30000 Training Loss: 0.06327088922262192\n",
      "Epoch 7425/30000 Training Loss: 0.05601303651928902\n",
      "Epoch 7426/30000 Training Loss: 0.04759139567613602\n",
      "Epoch 7427/30000 Training Loss: 0.06320281326770782\n",
      "Epoch 7428/30000 Training Loss: 0.0547807402908802\n",
      "Epoch 7429/30000 Training Loss: 0.054603949189186096\n",
      "Epoch 7430/30000 Training Loss: 0.04364529997110367\n",
      "Epoch 7431/30000 Training Loss: 0.059060584753751755\n",
      "Epoch 7432/30000 Training Loss: 0.051689453423023224\n",
      "Epoch 7433/30000 Training Loss: 0.049878720194101334\n",
      "Epoch 7434/30000 Training Loss: 0.05652248114347458\n",
      "Epoch 7435/30000 Training Loss: 0.03845437616109848\n",
      "Epoch 7436/30000 Training Loss: 0.05751467123627663\n",
      "Epoch 7437/30000 Training Loss: 0.0363958477973938\n",
      "Epoch 7438/30000 Training Loss: 0.0656425729393959\n",
      "Epoch 7439/30000 Training Loss: 0.05530353635549545\n",
      "Epoch 7440/30000 Training Loss: 0.04654114693403244\n",
      "Epoch 7441/30000 Training Loss: 0.042504824697971344\n",
      "Epoch 7442/30000 Training Loss: 0.04843627288937569\n",
      "Epoch 7443/30000 Training Loss: 0.06599384546279907\n",
      "Epoch 7444/30000 Training Loss: 0.05248764157295227\n",
      "Epoch 7445/30000 Training Loss: 0.07633721828460693\n",
      "Epoch 7446/30000 Training Loss: 0.054840654134750366\n",
      "Epoch 7447/30000 Training Loss: 0.05709276348352432\n",
      "Epoch 7448/30000 Training Loss: 0.060048315674066544\n",
      "Epoch 7449/30000 Training Loss: 0.05895014852285385\n",
      "Epoch 7450/30000 Training Loss: 0.06031252443790436\n",
      "Epoch 7451/30000 Training Loss: 0.05286756157875061\n",
      "Epoch 7452/30000 Training Loss: 0.03911665827035904\n",
      "Epoch 7453/30000 Training Loss: 0.066024050116539\n",
      "Epoch 7454/30000 Training Loss: 0.04659373313188553\n",
      "Epoch 7455/30000 Training Loss: 0.05758722871541977\n",
      "Epoch 7456/30000 Training Loss: 0.05347400903701782\n",
      "Epoch 7457/30000 Training Loss: 0.060067638754844666\n",
      "Epoch 7458/30000 Training Loss: 0.06439697742462158\n",
      "Epoch 7459/30000 Training Loss: 0.06016096472740173\n",
      "Epoch 7460/30000 Training Loss: 0.05442022532224655\n",
      "Epoch 7461/30000 Training Loss: 0.04605711251497269\n",
      "Epoch 7462/30000 Training Loss: 0.038712278008461\n",
      "Epoch 7463/30000 Training Loss: 0.05381312221288681\n",
      "Epoch 7464/30000 Training Loss: 0.06726473569869995\n",
      "Epoch 7465/30000 Training Loss: 0.06062494218349457\n",
      "Epoch 7466/30000 Training Loss: 0.04804272949695587\n",
      "Epoch 7467/30000 Training Loss: 0.0485808327794075\n",
      "Epoch 7468/30000 Training Loss: 0.05389782786369324\n",
      "Epoch 7469/30000 Training Loss: 0.04450998455286026\n",
      "Epoch 7470/30000 Training Loss: 0.04236514866352081\n",
      "Epoch 7471/30000 Training Loss: 0.04358893632888794\n",
      "Epoch 7472/30000 Training Loss: 0.061937347054481506\n",
      "Epoch 7473/30000 Training Loss: 0.04863356053829193\n",
      "Epoch 7474/30000 Training Loss: 0.06155678257346153\n",
      "Epoch 7475/30000 Training Loss: 0.0598285049200058\n",
      "Epoch 7476/30000 Training Loss: 0.06824538111686707\n",
      "Epoch 7477/30000 Training Loss: 0.06026837229728699\n",
      "Epoch 7478/30000 Training Loss: 0.05558580160140991\n",
      "Epoch 7479/30000 Training Loss: 0.05029008165001869\n",
      "Epoch 7480/30000 Training Loss: 0.04858286678791046\n",
      "Epoch 7481/30000 Training Loss: 0.05582241341471672\n",
      "Epoch 7482/30000 Training Loss: 0.0584176704287529\n",
      "Epoch 7483/30000 Training Loss: 0.05269116908311844\n",
      "Epoch 7484/30000 Training Loss: 0.043299153447151184\n",
      "Epoch 7485/30000 Training Loss: 0.052602820098400116\n",
      "Epoch 7486/30000 Training Loss: 0.04781735688447952\n",
      "Epoch 7487/30000 Training Loss: 0.047949258238077164\n",
      "Epoch 7488/30000 Training Loss: 0.06391000747680664\n",
      "Epoch 7489/30000 Training Loss: 0.04514468088746071\n",
      "Epoch 7490/30000 Training Loss: 0.05113978683948517\n",
      "Epoch 7491/30000 Training Loss: 0.06567792594432831\n",
      "Epoch 7492/30000 Training Loss: 0.06382539868354797\n",
      "Epoch 7493/30000 Training Loss: 0.061344750225543976\n",
      "Epoch 7494/30000 Training Loss: 0.056303881108760834\n",
      "Epoch 7495/30000 Training Loss: 0.06453950703144073\n",
      "Epoch 7496/30000 Training Loss: 0.05186796188354492\n",
      "Epoch 7497/30000 Training Loss: 0.053025804460048676\n",
      "Epoch 7498/30000 Training Loss: 0.05724254250526428\n",
      "Epoch 7499/30000 Training Loss: 0.042154449969530106\n",
      "Epoch 7500/30000 Training Loss: 0.06171344220638275\n",
      "Epoch 7500/30000 Validation Loss: 0.054749999195337296\n",
      "Epoch 7501/30000 Training Loss: 0.04445614665746689\n",
      "Epoch 7502/30000 Training Loss: 0.05436303839087486\n",
      "Epoch 7503/30000 Training Loss: 0.04749619960784912\n",
      "Epoch 7504/30000 Training Loss: 0.04115913808345795\n",
      "Epoch 7505/30000 Training Loss: 0.052377838641405106\n",
      "Epoch 7506/30000 Training Loss: 0.049612611532211304\n",
      "Epoch 7507/30000 Training Loss: 0.0429101437330246\n",
      "Epoch 7508/30000 Training Loss: 0.05597611144185066\n",
      "Epoch 7509/30000 Training Loss: 0.04806015267968178\n",
      "Epoch 7510/30000 Training Loss: 0.05374302715063095\n",
      "Epoch 7511/30000 Training Loss: 0.055885180830955505\n",
      "Epoch 7512/30000 Training Loss: 0.057106077671051025\n",
      "Epoch 7513/30000 Training Loss: 0.045496467500925064\n",
      "Epoch 7514/30000 Training Loss: 0.040262237191200256\n",
      "Epoch 7515/30000 Training Loss: 0.05134476721286774\n",
      "Epoch 7516/30000 Training Loss: 0.07027848809957504\n",
      "Epoch 7517/30000 Training Loss: 0.05145867168903351\n",
      "Epoch 7518/30000 Training Loss: 0.061663009226322174\n",
      "Epoch 7519/30000 Training Loss: 0.04784741997718811\n",
      "Epoch 7520/30000 Training Loss: 0.04575570672750473\n",
      "Epoch 7521/30000 Training Loss: 0.06998506188392639\n",
      "Epoch 7522/30000 Training Loss: 0.0532606802880764\n",
      "Epoch 7523/30000 Training Loss: 0.04878975450992584\n",
      "Epoch 7524/30000 Training Loss: 0.04352866858243942\n",
      "Epoch 7525/30000 Training Loss: 0.04390963539481163\n",
      "Epoch 7526/30000 Training Loss: 0.05449660122394562\n",
      "Epoch 7527/30000 Training Loss: 0.05097170174121857\n",
      "Epoch 7528/30000 Training Loss: 0.04994988441467285\n",
      "Epoch 7529/30000 Training Loss: 0.060215871781110764\n",
      "Epoch 7530/30000 Training Loss: 0.059485770761966705\n",
      "Epoch 7531/30000 Training Loss: 0.04366994649171829\n",
      "Epoch 7532/30000 Training Loss: 0.045549921691417694\n",
      "Epoch 7533/30000 Training Loss: 0.04545893520116806\n",
      "Epoch 7534/30000 Training Loss: 0.05746879056096077\n",
      "Epoch 7535/30000 Training Loss: 0.059808775782585144\n",
      "Epoch 7536/30000 Training Loss: 0.057138994336128235\n",
      "Epoch 7537/30000 Training Loss: 0.05529981479048729\n",
      "Epoch 7538/30000 Training Loss: 0.04942901432514191\n",
      "Epoch 7539/30000 Training Loss: 0.055505894124507904\n",
      "Epoch 7540/30000 Training Loss: 0.059492457658052444\n",
      "Epoch 7541/30000 Training Loss: 0.05625157803297043\n",
      "Epoch 7542/30000 Training Loss: 0.05191497877240181\n",
      "Epoch 7543/30000 Training Loss: 0.05844385176897049\n",
      "Epoch 7544/30000 Training Loss: 0.044586800038814545\n",
      "Epoch 7545/30000 Training Loss: 0.043289974331855774\n",
      "Epoch 7546/30000 Training Loss: 0.04784703254699707\n",
      "Epoch 7547/30000 Training Loss: 0.03413064405322075\n",
      "Epoch 7548/30000 Training Loss: 0.05713802948594093\n",
      "Epoch 7549/30000 Training Loss: 0.06947165727615356\n",
      "Epoch 7550/30000 Training Loss: 0.04629237949848175\n",
      "Epoch 7551/30000 Training Loss: 0.04351784661412239\n",
      "Epoch 7552/30000 Training Loss: 0.04286491870880127\n",
      "Epoch 7553/30000 Training Loss: 0.05620887130498886\n",
      "Epoch 7554/30000 Training Loss: 0.04858490079641342\n",
      "Epoch 7555/30000 Training Loss: 0.04671678692102432\n",
      "Epoch 7556/30000 Training Loss: 0.06239312142133713\n",
      "Epoch 7557/30000 Training Loss: 0.04490872472524643\n",
      "Epoch 7558/30000 Training Loss: 0.048564791679382324\n",
      "Epoch 7559/30000 Training Loss: 0.061653345823287964\n",
      "Epoch 7560/30000 Training Loss: 0.04363091289997101\n",
      "Epoch 7561/30000 Training Loss: 0.04644181206822395\n",
      "Epoch 7562/30000 Training Loss: 0.049529753625392914\n",
      "Epoch 7563/30000 Training Loss: 0.05170602351427078\n",
      "Epoch 7564/30000 Training Loss: 0.050091929733753204\n",
      "Epoch 7565/30000 Training Loss: 0.0593026727437973\n",
      "Epoch 7566/30000 Training Loss: 0.04556342586874962\n",
      "Epoch 7567/30000 Training Loss: 0.06340739876031876\n",
      "Epoch 7568/30000 Training Loss: 0.06263371556997299\n",
      "Epoch 7569/30000 Training Loss: 0.048500046133995056\n",
      "Epoch 7570/30000 Training Loss: 0.06088201701641083\n",
      "Epoch 7571/30000 Training Loss: 0.03968074172735214\n",
      "Epoch 7572/30000 Training Loss: 0.058336980640888214\n",
      "Epoch 7573/30000 Training Loss: 0.045316439121961594\n",
      "Epoch 7574/30000 Training Loss: 0.06353211402893066\n",
      "Epoch 7575/30000 Training Loss: 0.046125464141368866\n",
      "Epoch 7576/30000 Training Loss: 0.05593745410442352\n",
      "Epoch 7577/30000 Training Loss: 0.045791514217853546\n",
      "Epoch 7578/30000 Training Loss: 0.06158801168203354\n",
      "Epoch 7579/30000 Training Loss: 0.059010107070207596\n",
      "Epoch 7580/30000 Training Loss: 0.06573036313056946\n",
      "Epoch 7581/30000 Training Loss: 0.04429168254137039\n",
      "Epoch 7582/30000 Training Loss: 0.05624496191740036\n",
      "Epoch 7583/30000 Training Loss: 0.04272136837244034\n",
      "Epoch 7584/30000 Training Loss: 0.06550311297178268\n",
      "Epoch 7585/30000 Training Loss: 0.04859049618244171\n",
      "Epoch 7586/30000 Training Loss: 0.05183042213320732\n",
      "Epoch 7587/30000 Training Loss: 0.04672198370099068\n",
      "Epoch 7588/30000 Training Loss: 0.05885437875986099\n",
      "Epoch 7589/30000 Training Loss: 0.05382631719112396\n",
      "Epoch 7590/30000 Training Loss: 0.0561012402176857\n",
      "Epoch 7591/30000 Training Loss: 0.045773573219776154\n",
      "Epoch 7592/30000 Training Loss: 0.04054120182991028\n",
      "Epoch 7593/30000 Training Loss: 0.06051824614405632\n",
      "Epoch 7594/30000 Training Loss: 0.0651712492108345\n",
      "Epoch 7595/30000 Training Loss: 0.044472165405750275\n",
      "Epoch 7596/30000 Training Loss: 0.05792786180973053\n",
      "Epoch 7597/30000 Training Loss: 0.08663858473300934\n",
      "Epoch 7598/30000 Training Loss: 0.046342745423316956\n",
      "Epoch 7599/30000 Training Loss: 0.07488793134689331\n",
      "Epoch 7600/30000 Training Loss: 0.07795116305351257\n",
      "Epoch 7600/30000 Validation Loss: 0.06000966578722\n",
      "Epoch 7601/30000 Training Loss: 0.047467608004808426\n",
      "Epoch 7602/30000 Training Loss: 0.05168534070253372\n",
      "Epoch 7603/30000 Training Loss: 0.04598980396986008\n",
      "Epoch 7604/30000 Training Loss: 0.062017474323511124\n",
      "Epoch 7605/30000 Training Loss: 0.056619442999362946\n",
      "Epoch 7606/30000 Training Loss: 0.04484795778989792\n",
      "Epoch 7607/30000 Training Loss: 0.055972255766391754\n",
      "Epoch 7608/30000 Training Loss: 0.0386749729514122\n",
      "Epoch 7609/30000 Training Loss: 0.03910617157816887\n",
      "Epoch 7610/30000 Training Loss: 0.04182959720492363\n",
      "Epoch 7611/30000 Training Loss: 0.04752460867166519\n",
      "Epoch 7612/30000 Training Loss: 0.06293759495019913\n",
      "Epoch 7613/30000 Training Loss: 0.051572464406490326\n",
      "Epoch 7614/30000 Training Loss: 0.05842788517475128\n",
      "Epoch 7615/30000 Training Loss: 0.043842267245054245\n",
      "Epoch 7616/30000 Training Loss: 0.06398680806159973\n",
      "Epoch 7617/30000 Training Loss: 0.04575064033269882\n",
      "Epoch 7618/30000 Training Loss: 0.04550230875611305\n",
      "Epoch 7619/30000 Training Loss: 0.06339256465435028\n",
      "Epoch 7620/30000 Training Loss: 0.04292289912700653\n",
      "Epoch 7621/30000 Training Loss: 0.04527423530817032\n",
      "Epoch 7622/30000 Training Loss: 0.06580296903848648\n",
      "Epoch 7623/30000 Training Loss: 0.04802616685628891\n",
      "Epoch 7624/30000 Training Loss: 0.03819940239191055\n",
      "Epoch 7625/30000 Training Loss: 0.054322049021720886\n",
      "Epoch 7626/30000 Training Loss: 0.05558774620294571\n",
      "Epoch 7627/30000 Training Loss: 0.049898214638233185\n",
      "Epoch 7628/30000 Training Loss: 0.04572928696870804\n",
      "Epoch 7629/30000 Training Loss: 0.061066001653671265\n",
      "Epoch 7630/30000 Training Loss: 0.05202735215425491\n",
      "Epoch 7631/30000 Training Loss: 0.061442166566848755\n",
      "Epoch 7632/30000 Training Loss: 0.0608021542429924\n",
      "Epoch 7633/30000 Training Loss: 0.0507865846157074\n",
      "Epoch 7634/30000 Training Loss: 0.058282554149627686\n",
      "Epoch 7635/30000 Training Loss: 0.057578928768634796\n",
      "Epoch 7636/30000 Training Loss: 0.04185284674167633\n",
      "Epoch 7637/30000 Training Loss: 0.05559304356575012\n",
      "Epoch 7638/30000 Training Loss: 0.06651435792446136\n",
      "Epoch 7639/30000 Training Loss: 0.04360131546854973\n",
      "Epoch 7640/30000 Training Loss: 0.05975378304719925\n",
      "Epoch 7641/30000 Training Loss: 0.05082591623067856\n",
      "Epoch 7642/30000 Training Loss: 0.05137607455253601\n",
      "Epoch 7643/30000 Training Loss: 0.06700367480516434\n",
      "Epoch 7644/30000 Training Loss: 0.053853683173656464\n",
      "Epoch 7645/30000 Training Loss: 0.049455106258392334\n",
      "Epoch 7646/30000 Training Loss: 0.05663733929395676\n",
      "Epoch 7647/30000 Training Loss: 0.05217143893241882\n",
      "Epoch 7648/30000 Training Loss: 0.047503601759672165\n",
      "Epoch 7649/30000 Training Loss: 0.047912776470184326\n",
      "Epoch 7650/30000 Training Loss: 0.0447106808423996\n",
      "Epoch 7651/30000 Training Loss: 0.0550333596765995\n",
      "Epoch 7652/30000 Training Loss: 0.050324711948633194\n",
      "Epoch 7653/30000 Training Loss: 0.06963115185499191\n",
      "Epoch 7654/30000 Training Loss: 0.05329266935586929\n",
      "Epoch 7655/30000 Training Loss: 0.050454623997211456\n",
      "Epoch 7656/30000 Training Loss: 0.049109358340501785\n",
      "Epoch 7657/30000 Training Loss: 0.048763860017061234\n",
      "Epoch 7658/30000 Training Loss: 0.04256497323513031\n",
      "Epoch 7659/30000 Training Loss: 0.05553009361028671\n",
      "Epoch 7660/30000 Training Loss: 0.05528884008526802\n",
      "Epoch 7661/30000 Training Loss: 0.06824914366006851\n",
      "Epoch 7662/30000 Training Loss: 0.04943237453699112\n",
      "Epoch 7663/30000 Training Loss: 0.057180583477020264\n",
      "Epoch 7664/30000 Training Loss: 0.045357413589954376\n",
      "Epoch 7665/30000 Training Loss: 0.04309319704771042\n",
      "Epoch 7666/30000 Training Loss: 0.05130178853869438\n",
      "Epoch 7667/30000 Training Loss: 0.057163454592227936\n",
      "Epoch 7668/30000 Training Loss: 0.045507870614528656\n",
      "Epoch 7669/30000 Training Loss: 0.036569759249687195\n",
      "Epoch 7670/30000 Training Loss: 0.049039777368307114\n",
      "Epoch 7671/30000 Training Loss: 0.05830596387386322\n",
      "Epoch 7672/30000 Training Loss: 0.06342250108718872\n",
      "Epoch 7673/30000 Training Loss: 0.04440216347575188\n",
      "Epoch 7674/30000 Training Loss: 0.05256029963493347\n",
      "Epoch 7675/30000 Training Loss: 0.05486954003572464\n",
      "Epoch 7676/30000 Training Loss: 0.049683213233947754\n",
      "Epoch 7677/30000 Training Loss: 0.04403818026185036\n",
      "Epoch 7678/30000 Training Loss: 0.05710636079311371\n",
      "Epoch 7679/30000 Training Loss: 0.040818266570568085\n",
      "Epoch 7680/30000 Training Loss: 0.060630373656749725\n",
      "Epoch 7681/30000 Training Loss: 0.050019748508930206\n",
      "Epoch 7682/30000 Training Loss: 0.0405040867626667\n",
      "Epoch 7683/30000 Training Loss: 0.06544310599565506\n",
      "Epoch 7684/30000 Training Loss: 0.0541568323969841\n",
      "Epoch 7685/30000 Training Loss: 0.054182033985853195\n",
      "Epoch 7686/30000 Training Loss: 0.044375382363796234\n",
      "Epoch 7687/30000 Training Loss: 0.0699794739484787\n",
      "Epoch 7688/30000 Training Loss: 0.04519322142004967\n",
      "Epoch 7689/30000 Training Loss: 0.06568685919046402\n",
      "Epoch 7690/30000 Training Loss: 0.046570099890232086\n",
      "Epoch 7691/30000 Training Loss: 0.06510092318058014\n",
      "Epoch 7692/30000 Training Loss: 0.0583660751581192\n",
      "Epoch 7693/30000 Training Loss: 0.041135627776384354\n",
      "Epoch 7694/30000 Training Loss: 0.04302079230546951\n",
      "Epoch 7695/30000 Training Loss: 0.050586678087711334\n",
      "Epoch 7696/30000 Training Loss: 0.0730132907629013\n",
      "Epoch 7697/30000 Training Loss: 0.05867940932512283\n",
      "Epoch 7698/30000 Training Loss: 0.07386589050292969\n",
      "Epoch 7699/30000 Training Loss: 0.0411231555044651\n",
      "Epoch 7700/30000 Training Loss: 0.04450622946023941\n",
      "Epoch 7700/30000 Validation Loss: 0.053447846323251724\n",
      "Epoch 7701/30000 Training Loss: 0.04742777347564697\n",
      "Epoch 7702/30000 Training Loss: 0.05824917554855347\n",
      "Epoch 7703/30000 Training Loss: 0.04656556248664856\n",
      "Epoch 7704/30000 Training Loss: 0.05770992115139961\n",
      "Epoch 7705/30000 Training Loss: 0.061344362795352936\n",
      "Epoch 7706/30000 Training Loss: 0.05404125154018402\n",
      "Epoch 7707/30000 Training Loss: 0.05131766200065613\n",
      "Epoch 7708/30000 Training Loss: 0.05506201833486557\n",
      "Epoch 7709/30000 Training Loss: 0.04906497150659561\n",
      "Epoch 7710/30000 Training Loss: 0.04939442127943039\n",
      "Epoch 7711/30000 Training Loss: 0.05056309700012207\n",
      "Epoch 7712/30000 Training Loss: 0.04488435760140419\n",
      "Epoch 7713/30000 Training Loss: 0.053923025727272034\n",
      "Epoch 7714/30000 Training Loss: 0.05378209426999092\n",
      "Epoch 7715/30000 Training Loss: 0.04869559407234192\n",
      "Epoch 7716/30000 Training Loss: 0.04569277912378311\n",
      "Epoch 7717/30000 Training Loss: 0.052260298281908035\n",
      "Epoch 7718/30000 Training Loss: 0.051242951303720474\n",
      "Epoch 7719/30000 Training Loss: 0.06910402327775955\n",
      "Epoch 7720/30000 Training Loss: 0.04832249879837036\n",
      "Epoch 7721/30000 Training Loss: 0.0630984753370285\n",
      "Epoch 7722/30000 Training Loss: 0.05646132677793503\n",
      "Epoch 7723/30000 Training Loss: 0.04073988273739815\n",
      "Epoch 7724/30000 Training Loss: 0.05022070184350014\n",
      "Epoch 7725/30000 Training Loss: 0.054603323340415955\n",
      "Epoch 7726/30000 Training Loss: 0.05843806266784668\n",
      "Epoch 7727/30000 Training Loss: 0.05894923210144043\n",
      "Epoch 7728/30000 Training Loss: 0.05548882856965065\n",
      "Epoch 7729/30000 Training Loss: 0.05066124349832535\n",
      "Epoch 7730/30000 Training Loss: 0.06031951308250427\n",
      "Epoch 7731/30000 Training Loss: 0.061867423355579376\n",
      "Epoch 7732/30000 Training Loss: 0.05590750277042389\n",
      "Epoch 7733/30000 Training Loss: 0.05394328758120537\n",
      "Epoch 7734/30000 Training Loss: 0.056560590863227844\n",
      "Epoch 7735/30000 Training Loss: 0.044012922793626785\n",
      "Epoch 7736/30000 Training Loss: 0.051564641296863556\n",
      "Epoch 7737/30000 Training Loss: 0.06826873123645782\n",
      "Epoch 7738/30000 Training Loss: 0.04240245372056961\n",
      "Epoch 7739/30000 Training Loss: 0.04484107345342636\n",
      "Epoch 7740/30000 Training Loss: 0.03880378603935242\n",
      "Epoch 7741/30000 Training Loss: 0.04982224851846695\n",
      "Epoch 7742/30000 Training Loss: 0.05291057378053665\n",
      "Epoch 7743/30000 Training Loss: 0.05126951262354851\n",
      "Epoch 7744/30000 Training Loss: 0.03151124343276024\n",
      "Epoch 7745/30000 Training Loss: 0.05695285648107529\n",
      "Epoch 7746/30000 Training Loss: 0.048644695430994034\n",
      "Epoch 7747/30000 Training Loss: 0.04656285047531128\n",
      "Epoch 7748/30000 Training Loss: 0.048449061810970306\n",
      "Epoch 7749/30000 Training Loss: 0.059613317251205444\n",
      "Epoch 7750/30000 Training Loss: 0.0671241283416748\n",
      "Epoch 7751/30000 Training Loss: 0.049211978912353516\n",
      "Epoch 7752/30000 Training Loss: 0.040355559438467026\n",
      "Epoch 7753/30000 Training Loss: 0.04506032168865204\n",
      "Epoch 7754/30000 Training Loss: 0.04233452305197716\n",
      "Epoch 7755/30000 Training Loss: 0.05804404616355896\n",
      "Epoch 7756/30000 Training Loss: 0.05887102708220482\n",
      "Epoch 7757/30000 Training Loss: 0.041613925248384476\n",
      "Epoch 7758/30000 Training Loss: 0.05379641801118851\n",
      "Epoch 7759/30000 Training Loss: 0.05992065370082855\n",
      "Epoch 7760/30000 Training Loss: 0.04038124904036522\n",
      "Epoch 7761/30000 Training Loss: 0.041967108845710754\n",
      "Epoch 7762/30000 Training Loss: 0.05214124917984009\n",
      "Epoch 7763/30000 Training Loss: 0.05521808937191963\n",
      "Epoch 7764/30000 Training Loss: 0.06304094195365906\n",
      "Epoch 7765/30000 Training Loss: 0.047075305134058\n",
      "Epoch 7766/30000 Training Loss: 0.047442540526390076\n",
      "Epoch 7767/30000 Training Loss: 0.057974763214588165\n",
      "Epoch 7768/30000 Training Loss: 0.04918643459677696\n",
      "Epoch 7769/30000 Training Loss: 0.05866748094558716\n",
      "Epoch 7770/30000 Training Loss: 0.06127919256687164\n",
      "Epoch 7771/30000 Training Loss: 0.04981529712677002\n",
      "Epoch 7772/30000 Training Loss: 0.048234641551971436\n",
      "Epoch 7773/30000 Training Loss: 0.049095310270786285\n",
      "Epoch 7774/30000 Training Loss: 0.04470405355095863\n",
      "Epoch 7775/30000 Training Loss: 0.07030952721834183\n",
      "Epoch 7776/30000 Training Loss: 0.060906969010829926\n",
      "Epoch 7777/30000 Training Loss: 0.05445636808872223\n",
      "Epoch 7778/30000 Training Loss: 0.050515156239271164\n",
      "Epoch 7779/30000 Training Loss: 0.062068674713373184\n",
      "Epoch 7780/30000 Training Loss: 0.041869185864925385\n",
      "Epoch 7781/30000 Training Loss: 0.0675167441368103\n",
      "Epoch 7782/30000 Training Loss: 0.04857536405324936\n",
      "Epoch 7783/30000 Training Loss: 0.05206295847892761\n",
      "Epoch 7784/30000 Training Loss: 0.04884449020028114\n",
      "Epoch 7785/30000 Training Loss: 0.04957001656293869\n",
      "Epoch 7786/30000 Training Loss: 0.04758046939969063\n",
      "Epoch 7787/30000 Training Loss: 0.054478321224451065\n",
      "Epoch 7788/30000 Training Loss: 0.06028170883655548\n",
      "Epoch 7789/30000 Training Loss: 0.03828645125031471\n",
      "Epoch 7790/30000 Training Loss: 0.05282428488135338\n",
      "Epoch 7791/30000 Training Loss: 0.04842207208275795\n",
      "Epoch 7792/30000 Training Loss: 0.07572667300701141\n",
      "Epoch 7793/30000 Training Loss: 0.044604528695344925\n",
      "Epoch 7794/30000 Training Loss: 0.04902546852827072\n",
      "Epoch 7795/30000 Training Loss: 0.06679099798202515\n",
      "Epoch 7796/30000 Training Loss: 0.06257718056440353\n",
      "Epoch 7797/30000 Training Loss: 0.052438534796237946\n",
      "Epoch 7798/30000 Training Loss: 0.0414266400039196\n",
      "Epoch 7799/30000 Training Loss: 0.057131052017211914\n",
      "Epoch 7800/30000 Training Loss: 0.03911067545413971\n",
      "Epoch 7800/30000 Validation Loss: 0.06463861465454102\n",
      "Epoch 7801/30000 Training Loss: 0.06737513095140457\n",
      "Epoch 7802/30000 Training Loss: 0.055985160171985626\n",
      "Epoch 7803/30000 Training Loss: 0.044250864535570145\n",
      "Epoch 7804/30000 Training Loss: 0.04216254875063896\n",
      "Epoch 7805/30000 Training Loss: 0.0600118450820446\n",
      "Epoch 7806/30000 Training Loss: 0.05214551091194153\n",
      "Epoch 7807/30000 Training Loss: 0.05503365397453308\n",
      "Epoch 7808/30000 Training Loss: 0.03863506391644478\n",
      "Epoch 7809/30000 Training Loss: 0.050944916903972626\n",
      "Epoch 7810/30000 Training Loss: 0.047264598309993744\n",
      "Epoch 7811/30000 Training Loss: 0.050536997616291046\n",
      "Epoch 7812/30000 Training Loss: 0.040209271013736725\n",
      "Epoch 7813/30000 Training Loss: 0.05804402381181717\n",
      "Epoch 7814/30000 Training Loss: 0.0516207218170166\n",
      "Epoch 7815/30000 Training Loss: 0.05502256006002426\n",
      "Epoch 7816/30000 Training Loss: 0.04868672043085098\n",
      "Epoch 7817/30000 Training Loss: 0.05166105926036835\n",
      "Epoch 7818/30000 Training Loss: 0.04301169514656067\n",
      "Epoch 7819/30000 Training Loss: 0.04690215736627579\n",
      "Epoch 7820/30000 Training Loss: 0.05166513845324516\n",
      "Epoch 7821/30000 Training Loss: 0.04869658499956131\n",
      "Epoch 7822/30000 Training Loss: 0.0542592778801918\n",
      "Epoch 7823/30000 Training Loss: 0.04461353272199631\n",
      "Epoch 7824/30000 Training Loss: 0.05053490027785301\n",
      "Epoch 7825/30000 Training Loss: 0.05450664460659027\n",
      "Epoch 7826/30000 Training Loss: 0.039725206792354584\n",
      "Epoch 7827/30000 Training Loss: 0.050715960562229156\n",
      "Epoch 7828/30000 Training Loss: 0.03843175619840622\n",
      "Epoch 7829/30000 Training Loss: 0.04542181268334389\n",
      "Epoch 7830/30000 Training Loss: 0.04162832349538803\n",
      "Epoch 7831/30000 Training Loss: 0.050551943480968475\n",
      "Epoch 7832/30000 Training Loss: 0.05645991861820221\n",
      "Epoch 7833/30000 Training Loss: 0.05632651224732399\n",
      "Epoch 7834/30000 Training Loss: 0.04488659277558327\n",
      "Epoch 7835/30000 Training Loss: 0.054899897426366806\n",
      "Epoch 7836/30000 Training Loss: 0.04536901414394379\n",
      "Epoch 7837/30000 Training Loss: 0.05964459478855133\n",
      "Epoch 7838/30000 Training Loss: 0.046198055148124695\n",
      "Epoch 7839/30000 Training Loss: 0.061896663159132004\n",
      "Epoch 7840/30000 Training Loss: 0.05743350088596344\n",
      "Epoch 7841/30000 Training Loss: 0.05303915590047836\n",
      "Epoch 7842/30000 Training Loss: 0.05413639917969704\n",
      "Epoch 7843/30000 Training Loss: 0.054729804396629333\n",
      "Epoch 7844/30000 Training Loss: 0.04382580518722534\n",
      "Epoch 7845/30000 Training Loss: 0.054490670561790466\n",
      "Epoch 7846/30000 Training Loss: 0.05189375951886177\n",
      "Epoch 7847/30000 Training Loss: 0.044184841215610504\n",
      "Epoch 7848/30000 Training Loss: 0.05654772371053696\n",
      "Epoch 7849/30000 Training Loss: 0.07269909977912903\n",
      "Epoch 7850/30000 Training Loss: 0.048489879816770554\n",
      "Epoch 7851/30000 Training Loss: 0.05950549244880676\n",
      "Epoch 7852/30000 Training Loss: 0.05387159436941147\n",
      "Epoch 7853/30000 Training Loss: 0.04743828624486923\n",
      "Epoch 7854/30000 Training Loss: 0.0402437299489975\n",
      "Epoch 7855/30000 Training Loss: 0.04342017322778702\n",
      "Epoch 7856/30000 Training Loss: 0.06495998799800873\n",
      "Epoch 7857/30000 Training Loss: 0.04624708369374275\n",
      "Epoch 7858/30000 Training Loss: 0.05189715325832367\n",
      "Epoch 7859/30000 Training Loss: 0.03951399773359299\n",
      "Epoch 7860/30000 Training Loss: 0.05697845667600632\n",
      "Epoch 7861/30000 Training Loss: 0.04165850579738617\n",
      "Epoch 7862/30000 Training Loss: 0.052806928753852844\n",
      "Epoch 7863/30000 Training Loss: 0.058294232934713364\n",
      "Epoch 7864/30000 Training Loss: 0.04520200937986374\n",
      "Epoch 7865/30000 Training Loss: 0.057105712592601776\n",
      "Epoch 7866/30000 Training Loss: 0.0422668382525444\n",
      "Epoch 7867/30000 Training Loss: 0.03392871469259262\n",
      "Epoch 7868/30000 Training Loss: 0.05677497759461403\n",
      "Epoch 7869/30000 Training Loss: 0.04084256291389465\n",
      "Epoch 7870/30000 Training Loss: 0.062453851103782654\n",
      "Epoch 7871/30000 Training Loss: 0.04874701052904129\n",
      "Epoch 7872/30000 Training Loss: 0.05185405910015106\n",
      "Epoch 7873/30000 Training Loss: 0.05778251588344574\n",
      "Epoch 7874/30000 Training Loss: 0.04830695316195488\n",
      "Epoch 7875/30000 Training Loss: 0.05289314314723015\n",
      "Epoch 7876/30000 Training Loss: 0.04676981270313263\n",
      "Epoch 7877/30000 Training Loss: 0.042061470448970795\n",
      "Epoch 7878/30000 Training Loss: 0.043317027390003204\n",
      "Epoch 7879/30000 Training Loss: 0.0473136231303215\n",
      "Epoch 7880/30000 Training Loss: 0.04876669496297836\n",
      "Epoch 7881/30000 Training Loss: 0.04380345344543457\n",
      "Epoch 7882/30000 Training Loss: 0.05705438181757927\n",
      "Epoch 7883/30000 Training Loss: 0.05918242782354355\n",
      "Epoch 7884/30000 Training Loss: 0.04568474367260933\n",
      "Epoch 7885/30000 Training Loss: 0.059184275567531586\n",
      "Epoch 7886/30000 Training Loss: 0.04087845981121063\n",
      "Epoch 7887/30000 Training Loss: 0.05994907766580582\n",
      "Epoch 7888/30000 Training Loss: 0.0650886669754982\n",
      "Epoch 7889/30000 Training Loss: 0.05877022072672844\n",
      "Epoch 7890/30000 Training Loss: 0.04820021614432335\n",
      "Epoch 7891/30000 Training Loss: 0.05176550894975662\n",
      "Epoch 7892/30000 Training Loss: 0.0638662800192833\n",
      "Epoch 7893/30000 Training Loss: 0.042994365096092224\n",
      "Epoch 7894/30000 Training Loss: 0.038459550589323044\n",
      "Epoch 7895/30000 Training Loss: 0.06311113387346268\n",
      "Epoch 7896/30000 Training Loss: 0.0424952358007431\n",
      "Epoch 7897/30000 Training Loss: 0.05820946395397186\n",
      "Epoch 7898/30000 Training Loss: 0.0667780339717865\n",
      "Epoch 7899/30000 Training Loss: 0.0506497323513031\n",
      "Epoch 7900/30000 Training Loss: 0.052082352340221405\n",
      "Epoch 7900/30000 Validation Loss: 0.05593198910355568\n",
      "Epoch 7901/30000 Training Loss: 0.05309204012155533\n",
      "Epoch 7902/30000 Training Loss: 0.04796361178159714\n",
      "Epoch 7903/30000 Training Loss: 0.07789203524589539\n",
      "Epoch 7904/30000 Training Loss: 0.047565311193466187\n",
      "Epoch 7905/30000 Training Loss: 0.040283165872097015\n",
      "Epoch 7906/30000 Training Loss: 0.039592813700437546\n",
      "Epoch 7907/30000 Training Loss: 0.05453048646450043\n",
      "Epoch 7908/30000 Training Loss: 0.043303534388542175\n",
      "Epoch 7909/30000 Training Loss: 0.054506439715623856\n",
      "Epoch 7910/30000 Training Loss: 0.04879892244935036\n",
      "Epoch 7911/30000 Training Loss: 0.05566244572401047\n",
      "Epoch 7912/30000 Training Loss: 0.04625755921006203\n",
      "Epoch 7913/30000 Training Loss: 0.044728778302669525\n",
      "Epoch 7914/30000 Training Loss: 0.058490775525569916\n",
      "Epoch 7915/30000 Training Loss: 0.06429032236337662\n",
      "Epoch 7916/30000 Training Loss: 0.04471893236041069\n",
      "Epoch 7917/30000 Training Loss: 0.05350556969642639\n",
      "Epoch 7918/30000 Training Loss: 0.05222656950354576\n",
      "Epoch 7919/30000 Training Loss: 0.06136595457792282\n",
      "Epoch 7920/30000 Training Loss: 0.04474197328090668\n",
      "Epoch 7921/30000 Training Loss: 0.0452091321349144\n",
      "Epoch 7922/30000 Training Loss: 0.04205217584967613\n",
      "Epoch 7923/30000 Training Loss: 0.05553983524441719\n",
      "Epoch 7924/30000 Training Loss: 0.052716709673404694\n",
      "Epoch 7925/30000 Training Loss: 0.04716283082962036\n",
      "Epoch 7926/30000 Training Loss: 0.049377165734767914\n",
      "Epoch 7927/30000 Training Loss: 0.04528793692588806\n",
      "Epoch 7928/30000 Training Loss: 0.057949818670749664\n",
      "Epoch 7929/30000 Training Loss: 0.060412339866161346\n",
      "Epoch 7930/30000 Training Loss: 0.05394583195447922\n",
      "Epoch 7931/30000 Training Loss: 0.044885631650686264\n",
      "Epoch 7932/30000 Training Loss: 0.05492796003818512\n",
      "Epoch 7933/30000 Training Loss: 0.06863116472959518\n",
      "Epoch 7934/30000 Training Loss: 0.04348022863268852\n",
      "Epoch 7935/30000 Training Loss: 0.04634265974164009\n",
      "Epoch 7936/30000 Training Loss: 0.05976264551281929\n",
      "Epoch 7937/30000 Training Loss: 0.05691025033593178\n",
      "Epoch 7938/30000 Training Loss: 0.044829994440078735\n",
      "Epoch 7939/30000 Training Loss: 0.06685232371091843\n",
      "Epoch 7940/30000 Training Loss: 0.0653395801782608\n",
      "Epoch 7941/30000 Training Loss: 0.0538845993578434\n",
      "Epoch 7942/30000 Training Loss: 0.05549578368663788\n",
      "Epoch 7943/30000 Training Loss: 0.04170006886124611\n",
      "Epoch 7944/30000 Training Loss: 0.06002247333526611\n",
      "Epoch 7945/30000 Training Loss: 0.04270050302147865\n",
      "Epoch 7946/30000 Training Loss: 0.046021342277526855\n",
      "Epoch 7947/30000 Training Loss: 0.05682406947016716\n",
      "Epoch 7948/30000 Training Loss: 0.04740554839372635\n",
      "Epoch 7949/30000 Training Loss: 0.051688164472579956\n",
      "Epoch 7950/30000 Training Loss: 0.050939127802848816\n",
      "Epoch 7951/30000 Training Loss: 0.047842226922512054\n",
      "Epoch 7952/30000 Training Loss: 0.05038702115416527\n",
      "Epoch 7953/30000 Training Loss: 0.08076450228691101\n",
      "Epoch 7954/30000 Training Loss: 0.045878659933805466\n",
      "Epoch 7955/30000 Training Loss: 0.044275104999542236\n",
      "Epoch 7956/30000 Training Loss: 0.04414180666208267\n",
      "Epoch 7957/30000 Training Loss: 0.053508859127759933\n",
      "Epoch 7958/30000 Training Loss: 0.07246565073728561\n",
      "Epoch 7959/30000 Training Loss: 0.05280611664056778\n",
      "Epoch 7960/30000 Training Loss: 0.0406891331076622\n",
      "Epoch 7961/30000 Training Loss: 0.0488777682185173\n",
      "Epoch 7962/30000 Training Loss: 0.05195164307951927\n",
      "Epoch 7963/30000 Training Loss: 0.04378552734851837\n",
      "Epoch 7964/30000 Training Loss: 0.035300254821777344\n",
      "Epoch 7965/30000 Training Loss: 0.048860982060432434\n",
      "Epoch 7966/30000 Training Loss: 0.05376531556248665\n",
      "Epoch 7967/30000 Training Loss: 0.05576710030436516\n",
      "Epoch 7968/30000 Training Loss: 0.05542634800076485\n",
      "Epoch 7969/30000 Training Loss: 0.04563581943511963\n",
      "Epoch 7970/30000 Training Loss: 0.05003974586725235\n",
      "Epoch 7971/30000 Training Loss: 0.05601700395345688\n",
      "Epoch 7972/30000 Training Loss: 0.04572436586022377\n",
      "Epoch 7973/30000 Training Loss: 0.04845532029867172\n",
      "Epoch 7974/30000 Training Loss: 0.046732932329177856\n",
      "Epoch 7975/30000 Training Loss: 0.06217755377292633\n",
      "Epoch 7976/30000 Training Loss: 0.03625073656439781\n",
      "Epoch 7977/30000 Training Loss: 0.04685865342617035\n",
      "Epoch 7978/30000 Training Loss: 0.06601905822753906\n",
      "Epoch 7979/30000 Training Loss: 0.04428107291460037\n",
      "Epoch 7980/30000 Training Loss: 0.04108496382832527\n",
      "Epoch 7981/30000 Training Loss: 0.04871688038110733\n",
      "Epoch 7982/30000 Training Loss: 0.05031168833374977\n",
      "Epoch 7983/30000 Training Loss: 0.04174385219812393\n",
      "Epoch 7984/30000 Training Loss: 0.05178871750831604\n",
      "Epoch 7985/30000 Training Loss: 0.061462655663490295\n",
      "Epoch 7986/30000 Training Loss: 0.05179188400506973\n",
      "Epoch 7987/30000 Training Loss: 0.04708555340766907\n",
      "Epoch 7988/30000 Training Loss: 0.04652068763971329\n",
      "Epoch 7989/30000 Training Loss: 0.05417080223560333\n",
      "Epoch 7990/30000 Training Loss: 0.06430214643478394\n",
      "Epoch 7991/30000 Training Loss: 0.05754227191209793\n",
      "Epoch 7992/30000 Training Loss: 0.04100017249584198\n",
      "Epoch 7993/30000 Training Loss: 0.05601128563284874\n",
      "Epoch 7994/30000 Training Loss: 0.05017220228910446\n",
      "Epoch 7995/30000 Training Loss: 0.05143554508686066\n",
      "Epoch 7996/30000 Training Loss: 0.06811532378196716\n",
      "Epoch 7997/30000 Training Loss: 0.05387895181775093\n",
      "Epoch 7998/30000 Training Loss: 0.037190672010183334\n",
      "Epoch 7999/30000 Training Loss: 0.05756383389234543\n",
      "Epoch 8000/30000 Training Loss: 0.055205509066581726\n",
      "Epoch 8000/30000 Validation Loss: 0.05768715217709541\n",
      "Epoch 8001/30000 Training Loss: 0.04524813964962959\n",
      "Epoch 8002/30000 Training Loss: 0.06562792509794235\n",
      "Epoch 8003/30000 Training Loss: 0.03709360957145691\n",
      "Epoch 8004/30000 Training Loss: 0.04597821086645126\n",
      "Epoch 8005/30000 Training Loss: 0.059890858829021454\n",
      "Epoch 8006/30000 Training Loss: 0.045911166816949844\n",
      "Epoch 8007/30000 Training Loss: 0.043653469532728195\n",
      "Epoch 8008/30000 Training Loss: 0.054117947816848755\n",
      "Epoch 8009/30000 Training Loss: 0.04852889105677605\n",
      "Epoch 8010/30000 Training Loss: 0.047804564237594604\n",
      "Epoch 8011/30000 Training Loss: 0.049432191997766495\n",
      "Epoch 8012/30000 Training Loss: 0.03988078236579895\n",
      "Epoch 8013/30000 Training Loss: 0.05976565182209015\n",
      "Epoch 8014/30000 Training Loss: 0.055811453610658646\n",
      "Epoch 8015/30000 Training Loss: 0.04431584104895592\n",
      "Epoch 8016/30000 Training Loss: 0.04108079522848129\n",
      "Epoch 8017/30000 Training Loss: 0.04614762216806412\n",
      "Epoch 8018/30000 Training Loss: 0.050001829862594604\n",
      "Epoch 8019/30000 Training Loss: 0.05518800765275955\n",
      "Epoch 8020/30000 Training Loss: 0.04964324086904526\n",
      "Epoch 8021/30000 Training Loss: 0.05208732932806015\n",
      "Epoch 8022/30000 Training Loss: 0.05095192790031433\n",
      "Epoch 8023/30000 Training Loss: 0.04451030120253563\n",
      "Epoch 8024/30000 Training Loss: 0.04578187316656113\n",
      "Epoch 8025/30000 Training Loss: 0.07376205921173096\n",
      "Epoch 8026/30000 Training Loss: 0.05156310647726059\n",
      "Epoch 8027/30000 Training Loss: 0.03724365681409836\n",
      "Epoch 8028/30000 Training Loss: 0.054996393620967865\n",
      "Epoch 8029/30000 Training Loss: 0.05401337146759033\n",
      "Epoch 8030/30000 Training Loss: 0.05476463586091995\n",
      "Epoch 8031/30000 Training Loss: 0.05678439140319824\n",
      "Epoch 8032/30000 Training Loss: 0.05018334090709686\n",
      "Epoch 8033/30000 Training Loss: 0.04618142172694206\n",
      "Epoch 8034/30000 Training Loss: 0.051602110266685486\n",
      "Epoch 8035/30000 Training Loss: 0.06152056157588959\n",
      "Epoch 8036/30000 Training Loss: 0.0527205765247345\n",
      "Epoch 8037/30000 Training Loss: 0.05516716092824936\n",
      "Epoch 8038/30000 Training Loss: 0.06252972781658173\n",
      "Epoch 8039/30000 Training Loss: 0.07594078779220581\n",
      "Epoch 8040/30000 Training Loss: 0.06137480586767197\n",
      "Epoch 8041/30000 Training Loss: 0.04811125993728638\n",
      "Epoch 8042/30000 Training Loss: 0.041975706815719604\n",
      "Epoch 8043/30000 Training Loss: 0.03914904594421387\n",
      "Epoch 8044/30000 Training Loss: 0.04496733844280243\n",
      "Epoch 8045/30000 Training Loss: 0.04694058373570442\n",
      "Epoch 8046/30000 Training Loss: 0.0518789067864418\n",
      "Epoch 8047/30000 Training Loss: 0.04469478875398636\n",
      "Epoch 8048/30000 Training Loss: 0.04632312059402466\n",
      "Epoch 8049/30000 Training Loss: 0.045222409069538116\n",
      "Epoch 8050/30000 Training Loss: 0.05934116244316101\n",
      "Epoch 8051/30000 Training Loss: 0.0492311455309391\n",
      "Epoch 8052/30000 Training Loss: 0.050369586795568466\n",
      "Epoch 8053/30000 Training Loss: 0.054796092212200165\n",
      "Epoch 8054/30000 Training Loss: 0.04926258698105812\n",
      "Epoch 8055/30000 Training Loss: 0.055491700768470764\n",
      "Epoch 8056/30000 Training Loss: 0.048533856868743896\n",
      "Epoch 8057/30000 Training Loss: 0.036140941083431244\n",
      "Epoch 8058/30000 Training Loss: 0.0626419335603714\n",
      "Epoch 8059/30000 Training Loss: 0.060709841549396515\n",
      "Epoch 8060/30000 Training Loss: 0.049654651433229446\n",
      "Epoch 8061/30000 Training Loss: 0.05410231277346611\n",
      "Epoch 8062/30000 Training Loss: 0.05838368833065033\n",
      "Epoch 8063/30000 Training Loss: 0.0514056459069252\n",
      "Epoch 8064/30000 Training Loss: 0.05701916292309761\n",
      "Epoch 8065/30000 Training Loss: 0.051289863884449005\n",
      "Epoch 8066/30000 Training Loss: 0.05503313988447189\n",
      "Epoch 8067/30000 Training Loss: 0.04892878234386444\n",
      "Epoch 8068/30000 Training Loss: 0.05232105404138565\n",
      "Epoch 8069/30000 Training Loss: 0.04369557648897171\n",
      "Epoch 8070/30000 Training Loss: 0.0483957938849926\n",
      "Epoch 8071/30000 Training Loss: 0.06301210075616837\n",
      "Epoch 8072/30000 Training Loss: 0.054474443197250366\n",
      "Epoch 8073/30000 Training Loss: 0.04672399163246155\n",
      "Epoch 8074/30000 Training Loss: 0.029789160937070847\n",
      "Epoch 8075/30000 Training Loss: 0.05185513198375702\n",
      "Epoch 8076/30000 Training Loss: 0.0457037128508091\n",
      "Epoch 8077/30000 Training Loss: 0.05437476187944412\n",
      "Epoch 8078/30000 Training Loss: 0.046660251915454865\n",
      "Epoch 8079/30000 Training Loss: 0.05881770700216293\n",
      "Epoch 8080/30000 Training Loss: 0.06049950420856476\n",
      "Epoch 8081/30000 Training Loss: 0.049582552164793015\n",
      "Epoch 8082/30000 Training Loss: 0.05980280041694641\n",
      "Epoch 8083/30000 Training Loss: 0.05516098439693451\n",
      "Epoch 8084/30000 Training Loss: 0.05359819903969765\n",
      "Epoch 8085/30000 Training Loss: 0.03390879929065704\n",
      "Epoch 8086/30000 Training Loss: 0.0469626858830452\n",
      "Epoch 8087/30000 Training Loss: 0.0602833516895771\n",
      "Epoch 8088/30000 Training Loss: 0.06318587064743042\n",
      "Epoch 8089/30000 Training Loss: 0.054221898317337036\n",
      "Epoch 8090/30000 Training Loss: 0.05856494605541229\n",
      "Epoch 8091/30000 Training Loss: 0.05130375921726227\n",
      "Epoch 8092/30000 Training Loss: 0.04057028144598007\n",
      "Epoch 8093/30000 Training Loss: 0.05470266938209534\n",
      "Epoch 8094/30000 Training Loss: 0.0589732900261879\n",
      "Epoch 8095/30000 Training Loss: 0.06519884616136551\n",
      "Epoch 8096/30000 Training Loss: 0.06723950803279877\n",
      "Epoch 8097/30000 Training Loss: 0.040060874074697495\n",
      "Epoch 8098/30000 Training Loss: 0.047072798013687134\n",
      "Epoch 8099/30000 Training Loss: 0.04694146290421486\n",
      "Epoch 8100/30000 Training Loss: 0.05409928411245346\n",
      "Epoch 8100/30000 Validation Loss: 0.06565457582473755\n",
      "Epoch 8101/30000 Training Loss: 0.05550139397382736\n",
      "Epoch 8102/30000 Training Loss: 0.0573793388903141\n",
      "Epoch 8103/30000 Training Loss: 0.07642056047916412\n",
      "Epoch 8104/30000 Training Loss: 0.045871246606111526\n",
      "Epoch 8105/30000 Training Loss: 0.06405416131019592\n",
      "Epoch 8106/30000 Training Loss: 0.04701092839241028\n",
      "Epoch 8107/30000 Training Loss: 0.03717735409736633\n",
      "Epoch 8108/30000 Training Loss: 0.05679333209991455\n",
      "Epoch 8109/30000 Training Loss: 0.06643478572368622\n",
      "Epoch 8110/30000 Training Loss: 0.04213392734527588\n",
      "Epoch 8111/30000 Training Loss: 0.06374097615480423\n",
      "Epoch 8112/30000 Training Loss: 0.04730266332626343\n",
      "Epoch 8113/30000 Training Loss: 0.04315922409296036\n",
      "Epoch 8114/30000 Training Loss: 0.047551143914461136\n",
      "Epoch 8115/30000 Training Loss: 0.0594264417886734\n",
      "Epoch 8116/30000 Training Loss: 0.039709825068712234\n",
      "Epoch 8117/30000 Training Loss: 0.06824452430009842\n",
      "Epoch 8118/30000 Training Loss: 0.05178872495889664\n",
      "Epoch 8119/30000 Training Loss: 0.04926695674657822\n",
      "Epoch 8120/30000 Training Loss: 0.06570596247911453\n",
      "Epoch 8121/30000 Training Loss: 0.04561160132288933\n",
      "Epoch 8122/30000 Training Loss: 0.04280612990260124\n",
      "Epoch 8123/30000 Training Loss: 0.05056814104318619\n",
      "Epoch 8124/30000 Training Loss: 0.05964942276477814\n",
      "Epoch 8125/30000 Training Loss: 0.04754950851202011\n",
      "Epoch 8126/30000 Training Loss: 0.058030761778354645\n",
      "Epoch 8127/30000 Training Loss: 0.03864401578903198\n",
      "Epoch 8128/30000 Training Loss: 0.05565491318702698\n",
      "Epoch 8129/30000 Training Loss: 0.07462088763713837\n",
      "Epoch 8130/30000 Training Loss: 0.07035929709672928\n",
      "Epoch 8131/30000 Training Loss: 0.04348631948232651\n",
      "Epoch 8132/30000 Training Loss: 0.04025863856077194\n",
      "Epoch 8133/30000 Training Loss: 0.04430299252271652\n",
      "Epoch 8134/30000 Training Loss: 0.039865538477897644\n",
      "Epoch 8135/30000 Training Loss: 0.04588913172483444\n",
      "Epoch 8136/30000 Training Loss: 0.0350537970662117\n",
      "Epoch 8137/30000 Training Loss: 0.03924757242202759\n",
      "Epoch 8138/30000 Training Loss: 0.04859528690576553\n",
      "Epoch 8139/30000 Training Loss: 0.055996477603912354\n",
      "Epoch 8140/30000 Training Loss: 0.058408401906490326\n",
      "Epoch 8141/30000 Training Loss: 0.05442041903734207\n",
      "Epoch 8142/30000 Training Loss: 0.0622745081782341\n",
      "Epoch 8143/30000 Training Loss: 0.05647796392440796\n",
      "Epoch 8144/30000 Training Loss: 0.038366224616765976\n",
      "Epoch 8145/30000 Training Loss: 0.052484579384326935\n",
      "Epoch 8146/30000 Training Loss: 0.05264858901500702\n",
      "Epoch 8147/30000 Training Loss: 0.03818945214152336\n",
      "Epoch 8148/30000 Training Loss: 0.043344657868146896\n",
      "Epoch 8149/30000 Training Loss: 0.06164662539958954\n",
      "Epoch 8150/30000 Training Loss: 0.05409692972898483\n",
      "Epoch 8151/30000 Training Loss: 0.046259816735982895\n",
      "Epoch 8152/30000 Training Loss: 0.06145138293504715\n",
      "Epoch 8153/30000 Training Loss: 0.0752742737531662\n",
      "Epoch 8154/30000 Training Loss: 0.06175684556365013\n",
      "Epoch 8155/30000 Training Loss: 0.06379096955060959\n",
      "Epoch 8156/30000 Training Loss: 0.05231650918722153\n",
      "Epoch 8157/30000 Training Loss: 0.051397569477558136\n",
      "Epoch 8158/30000 Training Loss: 0.03876451030373573\n",
      "Epoch 8159/30000 Training Loss: 0.053551122546195984\n",
      "Epoch 8160/30000 Training Loss: 0.05981053411960602\n",
      "Epoch 8161/30000 Training Loss: 0.05222063511610031\n",
      "Epoch 8162/30000 Training Loss: 0.05754758045077324\n",
      "Epoch 8163/30000 Training Loss: 0.045957159250974655\n",
      "Epoch 8164/30000 Training Loss: 0.05248340964317322\n",
      "Epoch 8165/30000 Training Loss: 0.04069481045007706\n",
      "Epoch 8166/30000 Training Loss: 0.05069924145936966\n",
      "Epoch 8167/30000 Training Loss: 0.05954641103744507\n",
      "Epoch 8168/30000 Training Loss: 0.055834874510765076\n",
      "Epoch 8169/30000 Training Loss: 0.056955352425575256\n",
      "Epoch 8170/30000 Training Loss: 0.051398977637290955\n",
      "Epoch 8171/30000 Training Loss: 0.034941427409648895\n",
      "Epoch 8172/30000 Training Loss: 0.03919726237654686\n",
      "Epoch 8173/30000 Training Loss: 0.04327382147312164\n",
      "Epoch 8174/30000 Training Loss: 0.05163393169641495\n",
      "Epoch 8175/30000 Training Loss: 0.051341257989406586\n",
      "Epoch 8176/30000 Training Loss: 0.04795476421713829\n",
      "Epoch 8177/30000 Training Loss: 0.05631574988365173\n",
      "Epoch 8178/30000 Training Loss: 0.05466577410697937\n",
      "Epoch 8179/30000 Training Loss: 0.039107613265514374\n",
      "Epoch 8180/30000 Training Loss: 0.05847570300102234\n",
      "Epoch 8181/30000 Training Loss: 0.0632101446390152\n",
      "Epoch 8182/30000 Training Loss: 0.050161413848400116\n",
      "Epoch 8183/30000 Training Loss: 0.056432221084833145\n",
      "Epoch 8184/30000 Training Loss: 0.06399251520633698\n",
      "Epoch 8185/30000 Training Loss: 0.05585850775241852\n",
      "Epoch 8186/30000 Training Loss: 0.06000123172998428\n",
      "Epoch 8187/30000 Training Loss: 0.07186853885650635\n",
      "Epoch 8188/30000 Training Loss: 0.052289627492427826\n",
      "Epoch 8189/30000 Training Loss: 0.05332332104444504\n",
      "Epoch 8190/30000 Training Loss: 0.04740113019943237\n",
      "Epoch 8191/30000 Training Loss: 0.056339241564273834\n",
      "Epoch 8192/30000 Training Loss: 0.05473390221595764\n",
      "Epoch 8193/30000 Training Loss: 0.05821436643600464\n",
      "Epoch 8194/30000 Training Loss: 0.05048350244760513\n",
      "Epoch 8195/30000 Training Loss: 0.05242159962654114\n",
      "Epoch 8196/30000 Training Loss: 0.0356486514210701\n",
      "Epoch 8197/30000 Training Loss: 0.045554019510746\n",
      "Epoch 8198/30000 Training Loss: 0.04370089992880821\n",
      "Epoch 8199/30000 Training Loss: 0.06721416115760803\n",
      "Epoch 8200/30000 Training Loss: 0.05538792908191681\n",
      "Epoch 8200/30000 Validation Loss: 0.05084651708602905\n",
      "Epoch 8201/30000 Training Loss: 0.06538334488868713\n",
      "Epoch 8202/30000 Training Loss: 0.05016760528087616\n",
      "Epoch 8203/30000 Training Loss: 0.052349306643009186\n",
      "Epoch 8204/30000 Training Loss: 0.03875432908535004\n",
      "Epoch 8205/30000 Training Loss: 0.043079689145088196\n",
      "Epoch 8206/30000 Training Loss: 0.0316629633307457\n",
      "Epoch 8207/30000 Training Loss: 0.057418182492256165\n",
      "Epoch 8208/30000 Training Loss: 0.0681668370962143\n",
      "Epoch 8209/30000 Training Loss: 0.05777105689048767\n",
      "Epoch 8210/30000 Training Loss: 0.049819204956293106\n",
      "Epoch 8211/30000 Training Loss: 0.04545078054070473\n",
      "Epoch 8212/30000 Training Loss: 0.04412345215678215\n",
      "Epoch 8213/30000 Training Loss: 0.056790828704833984\n",
      "Epoch 8214/30000 Training Loss: 0.04497236758470535\n",
      "Epoch 8215/30000 Training Loss: 0.06518036127090454\n",
      "Epoch 8216/30000 Training Loss: 0.048824574798345566\n",
      "Epoch 8217/30000 Training Loss: 0.052387580275535583\n",
      "Epoch 8218/30000 Training Loss: 0.07250690460205078\n",
      "Epoch 8219/30000 Training Loss: 0.06046834588050842\n",
      "Epoch 8220/30000 Training Loss: 0.06363627314567566\n",
      "Epoch 8221/30000 Training Loss: 0.048395730555057526\n",
      "Epoch 8222/30000 Training Loss: 0.04345555230975151\n",
      "Epoch 8223/30000 Training Loss: 0.06089429929852486\n",
      "Epoch 8224/30000 Training Loss: 0.06157779321074486\n",
      "Epoch 8225/30000 Training Loss: 0.03917378932237625\n",
      "Epoch 8226/30000 Training Loss: 0.05047096312046051\n",
      "Epoch 8227/30000 Training Loss: 0.058909155428409576\n",
      "Epoch 8228/30000 Training Loss: 0.04666048288345337\n",
      "Epoch 8229/30000 Training Loss: 0.05624784529209137\n",
      "Epoch 8230/30000 Training Loss: 0.05080586299300194\n",
      "Epoch 8231/30000 Training Loss: 0.05918419361114502\n",
      "Epoch 8232/30000 Training Loss: 0.0530305877327919\n",
      "Epoch 8233/30000 Training Loss: 0.0479264110326767\n",
      "Epoch 8234/30000 Training Loss: 0.048601992428302765\n",
      "Epoch 8235/30000 Training Loss: 0.046480707824230194\n",
      "Epoch 8236/30000 Training Loss: 0.039713140577077866\n",
      "Epoch 8237/30000 Training Loss: 0.04946945235133171\n",
      "Epoch 8238/30000 Training Loss: 0.058680370450019836\n",
      "Epoch 8239/30000 Training Loss: 0.047916561365127563\n",
      "Epoch 8240/30000 Training Loss: 0.041965022683143616\n",
      "Epoch 8241/30000 Training Loss: 0.04199635237455368\n",
      "Epoch 8242/30000 Training Loss: 0.045962654054164886\n",
      "Epoch 8243/30000 Training Loss: 0.05951163172721863\n",
      "Epoch 8244/30000 Training Loss: 0.0463656447827816\n",
      "Epoch 8245/30000 Training Loss: 0.043978407979011536\n",
      "Epoch 8246/30000 Training Loss: 0.06613282859325409\n",
      "Epoch 8247/30000 Training Loss: 0.06771241873502731\n",
      "Epoch 8248/30000 Training Loss: 0.05850951001048088\n",
      "Epoch 8249/30000 Training Loss: 0.06134194880723953\n",
      "Epoch 8250/30000 Training Loss: 0.04467150941491127\n",
      "Epoch 8251/30000 Training Loss: 0.05729652941226959\n",
      "Epoch 8252/30000 Training Loss: 0.04806207865476608\n",
      "Epoch 8253/30000 Training Loss: 0.05076344311237335\n",
      "Epoch 8254/30000 Training Loss: 0.05154930055141449\n",
      "Epoch 8255/30000 Training Loss: 0.05945690721273422\n",
      "Epoch 8256/30000 Training Loss: 0.049121204763650894\n",
      "Epoch 8257/30000 Training Loss: 0.049664128571748734\n",
      "Epoch 8258/30000 Training Loss: 0.06602047383785248\n",
      "Epoch 8259/30000 Training Loss: 0.05191611871123314\n",
      "Epoch 8260/30000 Training Loss: 0.044248465448617935\n",
      "Epoch 8261/30000 Training Loss: 0.04151872545480728\n",
      "Epoch 8262/30000 Training Loss: 0.0498543418943882\n",
      "Epoch 8263/30000 Training Loss: 0.05224987864494324\n",
      "Epoch 8264/30000 Training Loss: 0.061461567878723145\n",
      "Epoch 8265/30000 Training Loss: 0.0563286617398262\n",
      "Epoch 8266/30000 Training Loss: 0.03980972617864609\n",
      "Epoch 8267/30000 Training Loss: 0.04840678721666336\n",
      "Epoch 8268/30000 Training Loss: 0.05573391169309616\n",
      "Epoch 8269/30000 Training Loss: 0.047914013266563416\n",
      "Epoch 8270/30000 Training Loss: 0.05742990970611572\n",
      "Epoch 8271/30000 Training Loss: 0.05445147305727005\n",
      "Epoch 8272/30000 Training Loss: 0.06671327352523804\n",
      "Epoch 8273/30000 Training Loss: 0.04692934826016426\n",
      "Epoch 8274/30000 Training Loss: 0.08962011337280273\n",
      "Epoch 8275/30000 Training Loss: 0.05468513444066048\n",
      "Epoch 8276/30000 Training Loss: 0.04347400739789009\n",
      "Epoch 8277/30000 Training Loss: 0.03352946788072586\n",
      "Epoch 8278/30000 Training Loss: 0.052403878420591354\n",
      "Epoch 8279/30000 Training Loss: 0.052833449095487595\n",
      "Epoch 8280/30000 Training Loss: 0.07217782735824585\n",
      "Epoch 8281/30000 Training Loss: 0.05180291831493378\n",
      "Epoch 8282/30000 Training Loss: 0.04552258551120758\n",
      "Epoch 8283/30000 Training Loss: 0.04674670472741127\n",
      "Epoch 8284/30000 Training Loss: 0.04459419846534729\n",
      "Epoch 8285/30000 Training Loss: 0.03984275832772255\n",
      "Epoch 8286/30000 Training Loss: 0.06141660362482071\n",
      "Epoch 8287/30000 Training Loss: 0.04338802769780159\n",
      "Epoch 8288/30000 Training Loss: 0.051414601504802704\n",
      "Epoch 8289/30000 Training Loss: 0.06266044825315475\n",
      "Epoch 8290/30000 Training Loss: 0.05240875482559204\n",
      "Epoch 8291/30000 Training Loss: 0.049841128289699554\n",
      "Epoch 8292/30000 Training Loss: 0.06152482330799103\n",
      "Epoch 8293/30000 Training Loss: 0.045453961938619614\n",
      "Epoch 8294/30000 Training Loss: 0.04976298660039902\n",
      "Epoch 8295/30000 Training Loss: 0.05265374481678009\n",
      "Epoch 8296/30000 Training Loss: 0.05682787299156189\n",
      "Epoch 8297/30000 Training Loss: 0.04798340052366257\n",
      "Epoch 8298/30000 Training Loss: 0.04461139440536499\n",
      "Epoch 8299/30000 Training Loss: 0.05243127793073654\n",
      "Epoch 8300/30000 Training Loss: 0.042162396013736725\n",
      "Epoch 8300/30000 Validation Loss: 0.05102114379405975\n",
      "Epoch 8301/30000 Training Loss: 0.06370384991168976\n",
      "Epoch 8302/30000 Training Loss: 0.049287550151348114\n",
      "Epoch 8303/30000 Training Loss: 0.04628542438149452\n",
      "Epoch 8304/30000 Training Loss: 0.05819109082221985\n",
      "Epoch 8305/30000 Training Loss: 0.04598218575119972\n",
      "Epoch 8306/30000 Training Loss: 0.04744791239500046\n",
      "Epoch 8307/30000 Training Loss: 0.044744767248630524\n",
      "Epoch 8308/30000 Training Loss: 0.05714740604162216\n",
      "Epoch 8309/30000 Training Loss: 0.05804796516895294\n",
      "Epoch 8310/30000 Training Loss: 0.0386008694767952\n",
      "Epoch 8311/30000 Training Loss: 0.04565100371837616\n",
      "Epoch 8312/30000 Training Loss: 0.05175485461950302\n",
      "Epoch 8313/30000 Training Loss: 0.05053338408470154\n",
      "Epoch 8314/30000 Training Loss: 0.05816226080060005\n",
      "Epoch 8315/30000 Training Loss: 0.05882396548986435\n",
      "Epoch 8316/30000 Training Loss: 0.0498695895075798\n",
      "Epoch 8317/30000 Training Loss: 0.04241685941815376\n",
      "Epoch 8318/30000 Training Loss: 0.04202106595039368\n",
      "Epoch 8319/30000 Training Loss: 0.045923858880996704\n",
      "Epoch 8320/30000 Training Loss: 0.04770689085125923\n",
      "Epoch 8321/30000 Training Loss: 0.059009574353694916\n",
      "Epoch 8322/30000 Training Loss: 0.045117370784282684\n",
      "Epoch 8323/30000 Training Loss: 0.06271151453256607\n",
      "Epoch 8324/30000 Training Loss: 0.06545722484588623\n",
      "Epoch 8325/30000 Training Loss: 0.045998282730579376\n",
      "Epoch 8326/30000 Training Loss: 0.045121610164642334\n",
      "Epoch 8327/30000 Training Loss: 0.05241129547357559\n",
      "Epoch 8328/30000 Training Loss: 0.04577619954943657\n",
      "Epoch 8329/30000 Training Loss: 0.056894876062870026\n",
      "Epoch 8330/30000 Training Loss: 0.06289127469062805\n",
      "Epoch 8331/30000 Training Loss: 0.05749622732400894\n",
      "Epoch 8332/30000 Training Loss: 0.04569752514362335\n",
      "Epoch 8333/30000 Training Loss: 0.05397731065750122\n",
      "Epoch 8334/30000 Training Loss: 0.06510140746831894\n",
      "Epoch 8335/30000 Training Loss: 0.058328717947006226\n",
      "Epoch 8336/30000 Training Loss: 0.06112523376941681\n",
      "Epoch 8337/30000 Training Loss: 0.04868341237306595\n",
      "Epoch 8338/30000 Training Loss: 0.056288301944732666\n",
      "Epoch 8339/30000 Training Loss: 0.051134638488292694\n",
      "Epoch 8340/30000 Training Loss: 0.0598323792219162\n",
      "Epoch 8341/30000 Training Loss: 0.04687786474823952\n",
      "Epoch 8342/30000 Training Loss: 0.04243651032447815\n",
      "Epoch 8343/30000 Training Loss: 0.05172990262508392\n",
      "Epoch 8344/30000 Training Loss: 0.055845633149147034\n",
      "Epoch 8345/30000 Training Loss: 0.06300170719623566\n",
      "Epoch 8346/30000 Training Loss: 0.053831085562705994\n",
      "Epoch 8347/30000 Training Loss: 0.05943430960178375\n",
      "Epoch 8348/30000 Training Loss: 0.04822254925966263\n",
      "Epoch 8349/30000 Training Loss: 0.07403846085071564\n",
      "Epoch 8350/30000 Training Loss: 0.046427175402641296\n",
      "Epoch 8351/30000 Training Loss: 0.053986720740795135\n",
      "Epoch 8352/30000 Training Loss: 0.055667709559202194\n",
      "Epoch 8353/30000 Training Loss: 0.05746524780988693\n",
      "Epoch 8354/30000 Training Loss: 0.04032851383090019\n",
      "Epoch 8355/30000 Training Loss: 0.04788593202829361\n",
      "Epoch 8356/30000 Training Loss: 0.05756298080086708\n",
      "Epoch 8357/30000 Training Loss: 0.05069269612431526\n",
      "Epoch 8358/30000 Training Loss: 0.055092595517635345\n",
      "Epoch 8359/30000 Training Loss: 0.04426441341638565\n",
      "Epoch 8360/30000 Training Loss: 0.047780510038137436\n",
      "Epoch 8361/30000 Training Loss: 0.0633460283279419\n",
      "Epoch 8362/30000 Training Loss: 0.045065417885780334\n",
      "Epoch 8363/30000 Training Loss: 0.052887119352817535\n",
      "Epoch 8364/30000 Training Loss: 0.03988341987133026\n",
      "Epoch 8365/30000 Training Loss: 0.06180565804243088\n",
      "Epoch 8366/30000 Training Loss: 0.04362265765666962\n",
      "Epoch 8367/30000 Training Loss: 0.04107186943292618\n",
      "Epoch 8368/30000 Training Loss: 0.053084805607795715\n",
      "Epoch 8369/30000 Training Loss: 0.05179804563522339\n",
      "Epoch 8370/30000 Training Loss: 0.047758638858795166\n",
      "Epoch 8371/30000 Training Loss: 0.049539715051651\n",
      "Epoch 8372/30000 Training Loss: 0.047104671597480774\n",
      "Epoch 8373/30000 Training Loss: 0.045132189989089966\n",
      "Epoch 8374/30000 Training Loss: 0.06026911363005638\n",
      "Epoch 8375/30000 Training Loss: 0.047904618084430695\n",
      "Epoch 8376/30000 Training Loss: 0.05650154501199722\n",
      "Epoch 8377/30000 Training Loss: 0.045889049768447876\n",
      "Epoch 8378/30000 Training Loss: 0.050403695553541183\n",
      "Epoch 8379/30000 Training Loss: 0.05086468160152435\n",
      "Epoch 8380/30000 Training Loss: 0.0434134304523468\n",
      "Epoch 8381/30000 Training Loss: 0.03726992383599281\n",
      "Epoch 8382/30000 Training Loss: 0.04870862513780594\n",
      "Epoch 8383/30000 Training Loss: 0.05092505365610123\n",
      "Epoch 8384/30000 Training Loss: 0.042307399213314056\n",
      "Epoch 8385/30000 Training Loss: 0.05654675513505936\n",
      "Epoch 8386/30000 Training Loss: 0.04100551828742027\n",
      "Epoch 8387/30000 Training Loss: 0.05790796875953674\n",
      "Epoch 8388/30000 Training Loss: 0.04822607338428497\n",
      "Epoch 8389/30000 Training Loss: 0.0530533641576767\n",
      "Epoch 8390/30000 Training Loss: 0.05041949450969696\n",
      "Epoch 8391/30000 Training Loss: 0.0668240636587143\n",
      "Epoch 8392/30000 Training Loss: 0.06407518684864044\n",
      "Epoch 8393/30000 Training Loss: 0.04659861698746681\n",
      "Epoch 8394/30000 Training Loss: 0.06717541813850403\n",
      "Epoch 8395/30000 Training Loss: 0.05682110786437988\n",
      "Epoch 8396/30000 Training Loss: 0.04482679441571236\n",
      "Epoch 8397/30000 Training Loss: 0.04826020449399948\n",
      "Epoch 8398/30000 Training Loss: 0.06625509262084961\n",
      "Epoch 8399/30000 Training Loss: 0.04679718613624573\n",
      "Epoch 8400/30000 Training Loss: 0.05735579505562782\n",
      "Epoch 8400/30000 Validation Loss: 0.05477278307080269\n",
      "Epoch 8401/30000 Training Loss: 0.05873513221740723\n",
      "Epoch 8402/30000 Training Loss: 0.0411081463098526\n",
      "Epoch 8403/30000 Training Loss: 0.04256263002753258\n",
      "Epoch 8404/30000 Training Loss: 0.048483945429325104\n",
      "Epoch 8405/30000 Training Loss: 0.049534380435943604\n",
      "Epoch 8406/30000 Training Loss: 0.058557841926813126\n",
      "Epoch 8407/30000 Training Loss: 0.05777953565120697\n",
      "Epoch 8408/30000 Training Loss: 0.05251456797122955\n",
      "Epoch 8409/30000 Training Loss: 0.06338104605674744\n",
      "Epoch 8410/30000 Training Loss: 0.05194742605090141\n",
      "Epoch 8411/30000 Training Loss: 0.0421050563454628\n",
      "Epoch 8412/30000 Training Loss: 0.05410671606659889\n",
      "Epoch 8413/30000 Training Loss: 0.06265468895435333\n",
      "Epoch 8414/30000 Training Loss: 0.04559299722313881\n",
      "Epoch 8415/30000 Training Loss: 0.050743985921144485\n",
      "Epoch 8416/30000 Training Loss: 0.06094478815793991\n",
      "Epoch 8417/30000 Training Loss: 0.039711691439151764\n",
      "Epoch 8418/30000 Training Loss: 0.060729801654815674\n",
      "Epoch 8419/30000 Training Loss: 0.04412116855382919\n",
      "Epoch 8420/30000 Training Loss: 0.05214064195752144\n",
      "Epoch 8421/30000 Training Loss: 0.05659238249063492\n",
      "Epoch 8422/30000 Training Loss: 0.047744471579790115\n",
      "Epoch 8423/30000 Training Loss: 0.047474831342697144\n",
      "Epoch 8424/30000 Training Loss: 0.04475073888897896\n",
      "Epoch 8425/30000 Training Loss: 0.05435377359390259\n",
      "Epoch 8426/30000 Training Loss: 0.05023413896560669\n",
      "Epoch 8427/30000 Training Loss: 0.05121113359928131\n",
      "Epoch 8428/30000 Training Loss: 0.05013047158718109\n",
      "Epoch 8429/30000 Training Loss: 0.04265102744102478\n",
      "Epoch 8430/30000 Training Loss: 0.05274952948093414\n",
      "Epoch 8431/30000 Training Loss: 0.07775065302848816\n",
      "Epoch 8432/30000 Training Loss: 0.06324177980422974\n",
      "Epoch 8433/30000 Training Loss: 0.056561119854450226\n",
      "Epoch 8434/30000 Training Loss: 0.050689831376075745\n",
      "Epoch 8435/30000 Training Loss: 0.04514268785715103\n",
      "Epoch 8436/30000 Training Loss: 0.05365830659866333\n",
      "Epoch 8437/30000 Training Loss: 0.05399675667285919\n",
      "Epoch 8438/30000 Training Loss: 0.053765181452035904\n",
      "Epoch 8439/30000 Training Loss: 0.0540638267993927\n",
      "Epoch 8440/30000 Training Loss: 0.05868914723396301\n",
      "Epoch 8441/30000 Training Loss: 0.04736647754907608\n",
      "Epoch 8442/30000 Training Loss: 0.06861583888530731\n",
      "Epoch 8443/30000 Training Loss: 0.04440685361623764\n",
      "Epoch 8444/30000 Training Loss: 0.056383416056632996\n",
      "Epoch 8445/30000 Training Loss: 0.048481207340955734\n",
      "Epoch 8446/30000 Training Loss: 0.04504527151584625\n",
      "Epoch 8447/30000 Training Loss: 0.05072721838951111\n",
      "Epoch 8448/30000 Training Loss: 0.04259622469544411\n",
      "Epoch 8449/30000 Training Loss: 0.05822025611996651\n",
      "Epoch 8450/30000 Training Loss: 0.058842308819293976\n",
      "Epoch 8451/30000 Training Loss: 0.05520324781537056\n",
      "Epoch 8452/30000 Training Loss: 0.062151260673999786\n",
      "Epoch 8453/30000 Training Loss: 0.05558677017688751\n",
      "Epoch 8454/30000 Training Loss: 0.04649287462234497\n",
      "Epoch 8455/30000 Training Loss: 0.03976121544837952\n",
      "Epoch 8456/30000 Training Loss: 0.049151286482810974\n",
      "Epoch 8457/30000 Training Loss: 0.054306820034980774\n",
      "Epoch 8458/30000 Training Loss: 0.05006202310323715\n",
      "Epoch 8459/30000 Training Loss: 0.059058189392089844\n",
      "Epoch 8460/30000 Training Loss: 0.05253316089510918\n",
      "Epoch 8461/30000 Training Loss: 0.06239226460456848\n",
      "Epoch 8462/30000 Training Loss: 0.0526404082775116\n",
      "Epoch 8463/30000 Training Loss: 0.038335170596838\n",
      "Epoch 8464/30000 Training Loss: 0.07034821063280106\n",
      "Epoch 8465/30000 Training Loss: 0.036849528551101685\n",
      "Epoch 8466/30000 Training Loss: 0.04686770215630531\n",
      "Epoch 8467/30000 Training Loss: 0.04088026285171509\n",
      "Epoch 8468/30000 Training Loss: 0.05277818441390991\n",
      "Epoch 8469/30000 Training Loss: 0.03586527332663536\n",
      "Epoch 8470/30000 Training Loss: 0.06647825241088867\n",
      "Epoch 8471/30000 Training Loss: 0.0729934424161911\n",
      "Epoch 8472/30000 Training Loss: 0.03890984505414963\n",
      "Epoch 8473/30000 Training Loss: 0.05644701421260834\n",
      "Epoch 8474/30000 Training Loss: 0.0423707477748394\n",
      "Epoch 8475/30000 Training Loss: 0.06407566368579865\n",
      "Epoch 8476/30000 Training Loss: 0.040046803653240204\n",
      "Epoch 8477/30000 Training Loss: 0.04213010147213936\n",
      "Epoch 8478/30000 Training Loss: 0.06030242145061493\n",
      "Epoch 8479/30000 Training Loss: 0.04104892909526825\n",
      "Epoch 8480/30000 Training Loss: 0.07854675501585007\n",
      "Epoch 8481/30000 Training Loss: 0.042823679745197296\n",
      "Epoch 8482/30000 Training Loss: 0.05266643315553665\n",
      "Epoch 8483/30000 Training Loss: 0.048892341554164886\n",
      "Epoch 8484/30000 Training Loss: 0.06416068226099014\n",
      "Epoch 8485/30000 Training Loss: 0.03694557398557663\n",
      "Epoch 8486/30000 Training Loss: 0.04712385684251785\n",
      "Epoch 8487/30000 Training Loss: 0.050458818674087524\n",
      "Epoch 8488/30000 Training Loss: 0.04141348972916603\n",
      "Epoch 8489/30000 Training Loss: 0.04130033403635025\n",
      "Epoch 8490/30000 Training Loss: 0.05560209974646568\n",
      "Epoch 8491/30000 Training Loss: 0.05160476639866829\n",
      "Epoch 8492/30000 Training Loss: 0.03397083282470703\n",
      "Epoch 8493/30000 Training Loss: 0.04159291833639145\n",
      "Epoch 8494/30000 Training Loss: 0.05082954466342926\n",
      "Epoch 8495/30000 Training Loss: 0.05662916228175163\n",
      "Epoch 8496/30000 Training Loss: 0.054786525666713715\n",
      "Epoch 8497/30000 Training Loss: 0.05001720041036606\n",
      "Epoch 8498/30000 Training Loss: 0.05156385898590088\n",
      "Epoch 8499/30000 Training Loss: 0.048209719359874725\n",
      "Epoch 8500/30000 Training Loss: 0.061047472059726715\n",
      "Epoch 8500/30000 Validation Loss: 0.04770442470908165\n",
      "Epoch 8501/30000 Training Loss: 0.04466651752591133\n",
      "Epoch 8502/30000 Training Loss: 0.05019040033221245\n",
      "Epoch 8503/30000 Training Loss: 0.05751005560159683\n",
      "Epoch 8504/30000 Training Loss: 0.04496508836746216\n",
      "Epoch 8505/30000 Training Loss: 0.06213697791099548\n",
      "Epoch 8506/30000 Training Loss: 0.05545113980770111\n",
      "Epoch 8507/30000 Training Loss: 0.04663591831922531\n",
      "Epoch 8508/30000 Training Loss: 0.05427410453557968\n",
      "Epoch 8509/30000 Training Loss: 0.06873394548892975\n",
      "Epoch 8510/30000 Training Loss: 0.041999347507953644\n",
      "Epoch 8511/30000 Training Loss: 0.06607469916343689\n",
      "Epoch 8512/30000 Training Loss: 0.05521772801876068\n",
      "Epoch 8513/30000 Training Loss: 0.050891757011413574\n",
      "Epoch 8514/30000 Training Loss: 0.04813939705491066\n",
      "Epoch 8515/30000 Training Loss: 0.05630870163440704\n",
      "Epoch 8516/30000 Training Loss: 0.03412371501326561\n",
      "Epoch 8517/30000 Training Loss: 0.048193663358688354\n",
      "Epoch 8518/30000 Training Loss: 0.04738780856132507\n",
      "Epoch 8519/30000 Training Loss: 0.06636659055948257\n",
      "Epoch 8520/30000 Training Loss: 0.03346721827983856\n",
      "Epoch 8521/30000 Training Loss: 0.05555977672338486\n",
      "Epoch 8522/30000 Training Loss: 0.05369165539741516\n",
      "Epoch 8523/30000 Training Loss: 0.060825541615486145\n",
      "Epoch 8524/30000 Training Loss: 0.056070730090141296\n",
      "Epoch 8525/30000 Training Loss: 0.051137618720531464\n",
      "Epoch 8526/30000 Training Loss: 0.050738148391246796\n",
      "Epoch 8527/30000 Training Loss: 0.05476725101470947\n",
      "Epoch 8528/30000 Training Loss: 0.054707176983356476\n",
      "Epoch 8529/30000 Training Loss: 0.04641754925251007\n",
      "Epoch 8530/30000 Training Loss: 0.0473211407661438\n",
      "Epoch 8531/30000 Training Loss: 0.038002707064151764\n",
      "Epoch 8532/30000 Training Loss: 0.055059175938367844\n",
      "Epoch 8533/30000 Training Loss: 0.04589523375034332\n",
      "Epoch 8534/30000 Training Loss: 0.06008280813694\n",
      "Epoch 8535/30000 Training Loss: 0.033423490822315216\n",
      "Epoch 8536/30000 Training Loss: 0.03738623857498169\n",
      "Epoch 8537/30000 Training Loss: 0.05469963699579239\n",
      "Epoch 8538/30000 Training Loss: 0.05272367596626282\n",
      "Epoch 8539/30000 Training Loss: 0.05272494629025459\n",
      "Epoch 8540/30000 Training Loss: 0.05093681067228317\n",
      "Epoch 8541/30000 Training Loss: 0.04725571349263191\n",
      "Epoch 8542/30000 Training Loss: 0.07208578288555145\n",
      "Epoch 8543/30000 Training Loss: 0.05255107581615448\n",
      "Epoch 8544/30000 Training Loss: 0.05822106450796127\n",
      "Epoch 8545/30000 Training Loss: 0.04640164226293564\n",
      "Epoch 8546/30000 Training Loss: 0.05084407329559326\n",
      "Epoch 8547/30000 Training Loss: 0.04907951131463051\n",
      "Epoch 8548/30000 Training Loss: 0.044505372643470764\n",
      "Epoch 8549/30000 Training Loss: 0.05396885424852371\n",
      "Epoch 8550/30000 Training Loss: 0.04056432843208313\n",
      "Epoch 8551/30000 Training Loss: 0.045506782829761505\n",
      "Epoch 8552/30000 Training Loss: 0.039991699159145355\n",
      "Epoch 8553/30000 Training Loss: 0.0566580593585968\n",
      "Epoch 8554/30000 Training Loss: 0.05275370925664902\n",
      "Epoch 8555/30000 Training Loss: 0.0516977533698082\n",
      "Epoch 8556/30000 Training Loss: 0.06783624738454819\n",
      "Epoch 8557/30000 Training Loss: 0.03729517385363579\n",
      "Epoch 8558/30000 Training Loss: 0.04643278941512108\n",
      "Epoch 8559/30000 Training Loss: 0.042175665497779846\n",
      "Epoch 8560/30000 Training Loss: 0.041529856622219086\n",
      "Epoch 8561/30000 Training Loss: 0.054970208555459976\n",
      "Epoch 8562/30000 Training Loss: 0.05317404121160507\n",
      "Epoch 8563/30000 Training Loss: 0.04365503042936325\n",
      "Epoch 8564/30000 Training Loss: 0.04829927161335945\n",
      "Epoch 8565/30000 Training Loss: 0.03579825162887573\n",
      "Epoch 8566/30000 Training Loss: 0.03981783241033554\n",
      "Epoch 8567/30000 Training Loss: 0.04856900870800018\n",
      "Epoch 8568/30000 Training Loss: 0.049794793128967285\n",
      "Epoch 8569/30000 Training Loss: 0.037213049829006195\n",
      "Epoch 8570/30000 Training Loss: 0.05838817358016968\n",
      "Epoch 8571/30000 Training Loss: 0.06950116902589798\n",
      "Epoch 8572/30000 Training Loss: 0.06040613353252411\n",
      "Epoch 8573/30000 Training Loss: 0.04467804729938507\n",
      "Epoch 8574/30000 Training Loss: 0.07435733824968338\n",
      "Epoch 8575/30000 Training Loss: 0.05329246073961258\n",
      "Epoch 8576/30000 Training Loss: 0.04808624088764191\n",
      "Epoch 8577/30000 Training Loss: 0.056902676820755005\n",
      "Epoch 8578/30000 Training Loss: 0.05267418175935745\n",
      "Epoch 8579/30000 Training Loss: 0.0470152422785759\n",
      "Epoch 8580/30000 Training Loss: 0.05420073866844177\n",
      "Epoch 8581/30000 Training Loss: 0.04623771831393242\n",
      "Epoch 8582/30000 Training Loss: 0.05389834940433502\n",
      "Epoch 8583/30000 Training Loss: 0.04853951930999756\n",
      "Epoch 8584/30000 Training Loss: 0.06549882888793945\n",
      "Epoch 8585/30000 Training Loss: 0.04942678287625313\n",
      "Epoch 8586/30000 Training Loss: 0.052095718681812286\n",
      "Epoch 8587/30000 Training Loss: 0.04419003054499626\n",
      "Epoch 8588/30000 Training Loss: 0.053715407848358154\n",
      "Epoch 8589/30000 Training Loss: 0.04638729616999626\n",
      "Epoch 8590/30000 Training Loss: 0.05257520452141762\n",
      "Epoch 8591/30000 Training Loss: 0.046823401004076004\n",
      "Epoch 8592/30000 Training Loss: 0.03921874612569809\n",
      "Epoch 8593/30000 Training Loss: 0.05604887008666992\n",
      "Epoch 8594/30000 Training Loss: 0.050956521183252335\n",
      "Epoch 8595/30000 Training Loss: 0.05418946593999863\n",
      "Epoch 8596/30000 Training Loss: 0.0574674978852272\n",
      "Epoch 8597/30000 Training Loss: 0.03961430490016937\n",
      "Epoch 8598/30000 Training Loss: 0.05023137852549553\n",
      "Epoch 8599/30000 Training Loss: 0.05722106993198395\n",
      "Epoch 8600/30000 Training Loss: 0.04667957127094269\n",
      "Epoch 8600/30000 Validation Loss: 0.03775510564446449\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03775510564446449<=============\n",
      "Epoch 8601/30000 Training Loss: 0.0387447327375412\n",
      "Epoch 8602/30000 Training Loss: 0.04260148108005524\n",
      "Epoch 8603/30000 Training Loss: 0.04746421426534653\n",
      "Epoch 8604/30000 Training Loss: 0.051970478147268295\n",
      "Epoch 8605/30000 Training Loss: 0.047090157866477966\n",
      "Epoch 8606/30000 Training Loss: 0.03538023680448532\n",
      "Epoch 8607/30000 Training Loss: 0.04166119173169136\n",
      "Epoch 8608/30000 Training Loss: 0.055785439908504486\n",
      "Epoch 8609/30000 Training Loss: 0.04793337732553482\n",
      "Epoch 8610/30000 Training Loss: 0.04943371191620827\n",
      "Epoch 8611/30000 Training Loss: 0.0548388734459877\n",
      "Epoch 8612/30000 Training Loss: 0.04941578954458237\n",
      "Epoch 8613/30000 Training Loss: 0.036812108010053635\n",
      "Epoch 8614/30000 Training Loss: 0.057887427508831024\n",
      "Epoch 8615/30000 Training Loss: 0.051876068115234375\n",
      "Epoch 8616/30000 Training Loss: 0.041279155761003494\n",
      "Epoch 8617/30000 Training Loss: 0.05312012508511543\n",
      "Epoch 8618/30000 Training Loss: 0.049194417893886566\n",
      "Epoch 8619/30000 Training Loss: 0.05384204536676407\n",
      "Epoch 8620/30000 Training Loss: 0.040632303804159164\n",
      "Epoch 8621/30000 Training Loss: 0.05363784730434418\n",
      "Epoch 8622/30000 Training Loss: 0.04960055649280548\n",
      "Epoch 8623/30000 Training Loss: 0.05704107508063316\n",
      "Epoch 8624/30000 Training Loss: 0.042182862758636475\n",
      "Epoch 8625/30000 Training Loss: 0.06079661846160889\n",
      "Epoch 8626/30000 Training Loss: 0.047531723976135254\n",
      "Epoch 8627/30000 Training Loss: 0.05263645201921463\n",
      "Epoch 8628/30000 Training Loss: 0.05388278886675835\n",
      "Epoch 8629/30000 Training Loss: 0.04641953855752945\n",
      "Epoch 8630/30000 Training Loss: 0.05811718478798866\n",
      "Epoch 8631/30000 Training Loss: 0.06067461892962456\n",
      "Epoch 8632/30000 Training Loss: 0.042616866528987885\n",
      "Epoch 8633/30000 Training Loss: 0.05622771382331848\n",
      "Epoch 8634/30000 Training Loss: 0.04179378226399422\n",
      "Epoch 8635/30000 Training Loss: 0.04606040194630623\n",
      "Epoch 8636/30000 Training Loss: 0.05505469813942909\n",
      "Epoch 8637/30000 Training Loss: 0.04531992971897125\n",
      "Epoch 8638/30000 Training Loss: 0.040554434061050415\n",
      "Epoch 8639/30000 Training Loss: 0.04513246566057205\n",
      "Epoch 8640/30000 Training Loss: 0.040606848895549774\n",
      "Epoch 8641/30000 Training Loss: 0.05102179944515228\n",
      "Epoch 8642/30000 Training Loss: 0.06076449155807495\n",
      "Epoch 8643/30000 Training Loss: 0.05204968899488449\n",
      "Epoch 8644/30000 Training Loss: 0.037553898990154266\n",
      "Epoch 8645/30000 Training Loss: 0.052275244146585464\n",
      "Epoch 8646/30000 Training Loss: 0.04193234443664551\n",
      "Epoch 8647/30000 Training Loss: 0.061791516840457916\n",
      "Epoch 8648/30000 Training Loss: 0.04069335386157036\n",
      "Epoch 8649/30000 Training Loss: 0.060093726962804794\n",
      "Epoch 8650/30000 Training Loss: 0.04571311175823212\n",
      "Epoch 8651/30000 Training Loss: 0.05290336161851883\n",
      "Epoch 8652/30000 Training Loss: 0.04151877015829086\n",
      "Epoch 8653/30000 Training Loss: 0.06483151018619537\n",
      "Epoch 8654/30000 Training Loss: 0.057637427002191544\n",
      "Epoch 8655/30000 Training Loss: 0.046479444950819016\n",
      "Epoch 8656/30000 Training Loss: 0.061208710074424744\n",
      "Epoch 8657/30000 Training Loss: 0.04261859878897667\n",
      "Epoch 8658/30000 Training Loss: 0.05122539401054382\n",
      "Epoch 8659/30000 Training Loss: 0.0458538718521595\n",
      "Epoch 8660/30000 Training Loss: 0.04558129608631134\n",
      "Epoch 8661/30000 Training Loss: 0.060904067009687424\n",
      "Epoch 8662/30000 Training Loss: 0.051994629204273224\n",
      "Epoch 8663/30000 Training Loss: 0.05714023485779762\n",
      "Epoch 8664/30000 Training Loss: 0.0539395697414875\n",
      "Epoch 8665/30000 Training Loss: 0.052183374762535095\n",
      "Epoch 8666/30000 Training Loss: 0.05866164714097977\n",
      "Epoch 8667/30000 Training Loss: 0.04552444443106651\n",
      "Epoch 8668/30000 Training Loss: 0.04944795370101929\n",
      "Epoch 8669/30000 Training Loss: 0.051244668662548065\n",
      "Epoch 8670/30000 Training Loss: 0.038340501487255096\n",
      "Epoch 8671/30000 Training Loss: 0.04179489240050316\n",
      "Epoch 8672/30000 Training Loss: 0.04591556638479233\n",
      "Epoch 8673/30000 Training Loss: 0.06361373513936996\n",
      "Epoch 8674/30000 Training Loss: 0.04405670985579491\n",
      "Epoch 8675/30000 Training Loss: 0.05555290728807449\n",
      "Epoch 8676/30000 Training Loss: 0.05479733645915985\n",
      "Epoch 8677/30000 Training Loss: 0.05556263029575348\n",
      "Epoch 8678/30000 Training Loss: 0.05838792771100998\n",
      "Epoch 8679/30000 Training Loss: 0.060512661933898926\n",
      "Epoch 8680/30000 Training Loss: 0.04413863271474838\n",
      "Epoch 8681/30000 Training Loss: 0.04122060537338257\n",
      "Epoch 8682/30000 Training Loss: 0.05438119173049927\n",
      "Epoch 8683/30000 Training Loss: 0.051335498690605164\n",
      "Epoch 8684/30000 Training Loss: 0.052604444324970245\n",
      "Epoch 8685/30000 Training Loss: 0.04305889084935188\n",
      "Epoch 8686/30000 Training Loss: 0.05370217561721802\n",
      "Epoch 8687/30000 Training Loss: 0.05102844163775444\n",
      "Epoch 8688/30000 Training Loss: 0.05438368767499924\n",
      "Epoch 8689/30000 Training Loss: 0.06120454519987106\n",
      "Epoch 8690/30000 Training Loss: 0.04876486212015152\n",
      "Epoch 8691/30000 Training Loss: 0.05485449731349945\n",
      "Epoch 8692/30000 Training Loss: 0.03751414641737938\n",
      "Epoch 8693/30000 Training Loss: 0.07015253603458405\n",
      "Epoch 8694/30000 Training Loss: 0.04644791781902313\n",
      "Epoch 8695/30000 Training Loss: 0.053783778101205826\n",
      "Epoch 8696/30000 Training Loss: 0.044722311198711395\n",
      "Epoch 8697/30000 Training Loss: 0.047419026494026184\n",
      "Epoch 8698/30000 Training Loss: 0.05849875509738922\n",
      "Epoch 8699/30000 Training Loss: 0.044694963842630386\n",
      "Epoch 8700/30000 Training Loss: 0.058484505861997604\n",
      "Epoch 8700/30000 Validation Loss: 0.06275424361228943\n",
      "Epoch 8701/30000 Training Loss: 0.07005240023136139\n",
      "Epoch 8702/30000 Training Loss: 0.0697074830532074\n",
      "Epoch 8703/30000 Training Loss: 0.037808679044246674\n",
      "Epoch 8704/30000 Training Loss: 0.046162933111190796\n",
      "Epoch 8705/30000 Training Loss: 0.051275819540023804\n",
      "Epoch 8706/30000 Training Loss: 0.053974591195583344\n",
      "Epoch 8707/30000 Training Loss: 0.0493740513920784\n",
      "Epoch 8708/30000 Training Loss: 0.06058015301823616\n",
      "Epoch 8709/30000 Training Loss: 0.056772515177726746\n",
      "Epoch 8710/30000 Training Loss: 0.05312012881040573\n",
      "Epoch 8711/30000 Training Loss: 0.040276914834976196\n",
      "Epoch 8712/30000 Training Loss: 0.08115194737911224\n",
      "Epoch 8713/30000 Training Loss: 0.05087413638830185\n",
      "Epoch 8714/30000 Training Loss: 0.06165865436196327\n",
      "Epoch 8715/30000 Training Loss: 0.06144008785486221\n",
      "Epoch 8716/30000 Training Loss: 0.05829024687409401\n",
      "Epoch 8717/30000 Training Loss: 0.04191414639353752\n",
      "Epoch 8718/30000 Training Loss: 0.053369149565696716\n",
      "Epoch 8719/30000 Training Loss: 0.0440702848136425\n",
      "Epoch 8720/30000 Training Loss: 0.04605625942349434\n",
      "Epoch 8721/30000 Training Loss: 0.05627979710698128\n",
      "Epoch 8722/30000 Training Loss: 0.05112318694591522\n",
      "Epoch 8723/30000 Training Loss: 0.06778177618980408\n",
      "Epoch 8724/30000 Training Loss: 0.05824562534689903\n",
      "Epoch 8725/30000 Training Loss: 0.05147597938776016\n",
      "Epoch 8726/30000 Training Loss: 0.04933881759643555\n",
      "Epoch 8727/30000 Training Loss: 0.03479919955134392\n",
      "Epoch 8728/30000 Training Loss: 0.0726698637008667\n",
      "Epoch 8729/30000 Training Loss: 0.056197792291641235\n",
      "Epoch 8730/30000 Training Loss: 0.06730860471725464\n",
      "Epoch 8731/30000 Training Loss: 0.06021115928888321\n",
      "Epoch 8732/30000 Training Loss: 0.045238424092531204\n",
      "Epoch 8733/30000 Training Loss: 0.07382567971944809\n",
      "Epoch 8734/30000 Training Loss: 0.04340939596295357\n",
      "Epoch 8735/30000 Training Loss: 0.04959270358085632\n",
      "Epoch 8736/30000 Training Loss: 0.062114425003528595\n",
      "Epoch 8737/30000 Training Loss: 0.05644380301237106\n",
      "Epoch 8738/30000 Training Loss: 0.05065777152776718\n",
      "Epoch 8739/30000 Training Loss: 0.056597039103507996\n",
      "Epoch 8740/30000 Training Loss: 0.05959232896566391\n",
      "Epoch 8741/30000 Training Loss: 0.047832027077674866\n",
      "Epoch 8742/30000 Training Loss: 0.04987744987010956\n",
      "Epoch 8743/30000 Training Loss: 0.04816000908613205\n",
      "Epoch 8744/30000 Training Loss: 0.05280627682805061\n",
      "Epoch 8745/30000 Training Loss: 0.05057183653116226\n",
      "Epoch 8746/30000 Training Loss: 0.05205754190683365\n",
      "Epoch 8747/30000 Training Loss: 0.07762269675731659\n",
      "Epoch 8748/30000 Training Loss: 0.04989972338080406\n",
      "Epoch 8749/30000 Training Loss: 0.057668574154376984\n",
      "Epoch 8750/30000 Training Loss: 0.063722625374794\n",
      "Epoch 8751/30000 Training Loss: 0.046716682612895966\n",
      "Epoch 8752/30000 Training Loss: 0.053008757531642914\n",
      "Epoch 8753/30000 Training Loss: 0.0468599796295166\n",
      "Epoch 8754/30000 Training Loss: 0.06127001345157623\n",
      "Epoch 8755/30000 Training Loss: 0.04666450247168541\n",
      "Epoch 8756/30000 Training Loss: 0.056191928684711456\n",
      "Epoch 8757/30000 Training Loss: 0.03889765590429306\n",
      "Epoch 8758/30000 Training Loss: 0.048075731843709946\n",
      "Epoch 8759/30000 Training Loss: 0.06689061224460602\n",
      "Epoch 8760/30000 Training Loss: 0.06673261523246765\n",
      "Epoch 8761/30000 Training Loss: 0.04870172590017319\n",
      "Epoch 8762/30000 Training Loss: 0.04590940475463867\n",
      "Epoch 8763/30000 Training Loss: 0.04561741277575493\n",
      "Epoch 8764/30000 Training Loss: 0.05280282720923424\n",
      "Epoch 8765/30000 Training Loss: 0.03756851702928543\n",
      "Epoch 8766/30000 Training Loss: 0.07215317338705063\n",
      "Epoch 8767/30000 Training Loss: 0.043419089168310165\n",
      "Epoch 8768/30000 Training Loss: 0.05566055700182915\n",
      "Epoch 8769/30000 Training Loss: 0.049596890807151794\n",
      "Epoch 8770/30000 Training Loss: 0.055219318717718124\n",
      "Epoch 8771/30000 Training Loss: 0.055015679448843\n",
      "Epoch 8772/30000 Training Loss: 0.043162375688552856\n",
      "Epoch 8773/30000 Training Loss: 0.059722840785980225\n",
      "Epoch 8774/30000 Training Loss: 0.042614299803972244\n",
      "Epoch 8775/30000 Training Loss: 0.05254923552274704\n",
      "Epoch 8776/30000 Training Loss: 0.07360605895519257\n",
      "Epoch 8777/30000 Training Loss: 0.05987173691391945\n",
      "Epoch 8778/30000 Training Loss: 0.05436354875564575\n",
      "Epoch 8779/30000 Training Loss: 0.049893662333488464\n",
      "Epoch 8780/30000 Training Loss: 0.04603239521384239\n",
      "Epoch 8781/30000 Training Loss: 0.054517947137355804\n",
      "Epoch 8782/30000 Training Loss: 0.050420038402080536\n",
      "Epoch 8783/30000 Training Loss: 0.05069342255592346\n",
      "Epoch 8784/30000 Training Loss: 0.046991799026727676\n",
      "Epoch 8785/30000 Training Loss: 0.053966306149959564\n",
      "Epoch 8786/30000 Training Loss: 0.05763820558786392\n",
      "Epoch 8787/30000 Training Loss: 0.06775692850351334\n",
      "Epoch 8788/30000 Training Loss: 0.04584777355194092\n",
      "Epoch 8789/30000 Training Loss: 0.06405256688594818\n",
      "Epoch 8790/30000 Training Loss: 0.04393487051129341\n",
      "Epoch 8791/30000 Training Loss: 0.05385245010256767\n",
      "Epoch 8792/30000 Training Loss: 0.041335903108119965\n",
      "Epoch 8793/30000 Training Loss: 0.037155456840991974\n",
      "Epoch 8794/30000 Training Loss: 0.04579158127307892\n",
      "Epoch 8795/30000 Training Loss: 0.06007694453001022\n",
      "Epoch 8796/30000 Training Loss: 0.06678727269172668\n",
      "Epoch 8797/30000 Training Loss: 0.05123347043991089\n",
      "Epoch 8798/30000 Training Loss: 0.03583195433020592\n",
      "Epoch 8799/30000 Training Loss: 0.043618179857730865\n",
      "Epoch 8800/30000 Training Loss: 0.04611219838261604\n",
      "Epoch 8800/30000 Validation Loss: 0.04324847459793091\n",
      "Epoch 8801/30000 Training Loss: 0.05848103016614914\n",
      "Epoch 8802/30000 Training Loss: 0.03807235136628151\n",
      "Epoch 8803/30000 Training Loss: 0.06704200804233551\n",
      "Epoch 8804/30000 Training Loss: 0.04991460591554642\n",
      "Epoch 8805/30000 Training Loss: 0.045326754450798035\n",
      "Epoch 8806/30000 Training Loss: 0.04388202726840973\n",
      "Epoch 8807/30000 Training Loss: 0.03420991450548172\n",
      "Epoch 8808/30000 Training Loss: 0.04445216804742813\n",
      "Epoch 8809/30000 Training Loss: 0.05033978819847107\n",
      "Epoch 8810/30000 Training Loss: 0.051705293357372284\n",
      "Epoch 8811/30000 Training Loss: 0.0507483184337616\n",
      "Epoch 8812/30000 Training Loss: 0.04250943660736084\n",
      "Epoch 8813/30000 Training Loss: 0.057604409754276276\n",
      "Epoch 8814/30000 Training Loss: 0.05877825617790222\n",
      "Epoch 8815/30000 Training Loss: 0.04832765460014343\n",
      "Epoch 8816/30000 Training Loss: 0.040871426463127136\n",
      "Epoch 8817/30000 Training Loss: 0.04612412303686142\n",
      "Epoch 8818/30000 Training Loss: 0.04373188689351082\n",
      "Epoch 8819/30000 Training Loss: 0.04847770184278488\n",
      "Epoch 8820/30000 Training Loss: 0.04238080233335495\n",
      "Epoch 8821/30000 Training Loss: 0.0620964914560318\n",
      "Epoch 8822/30000 Training Loss: 0.047535866498947144\n",
      "Epoch 8823/30000 Training Loss: 0.05293817073106766\n",
      "Epoch 8824/30000 Training Loss: 0.0435110442340374\n",
      "Epoch 8825/30000 Training Loss: 0.0526629202067852\n",
      "Epoch 8826/30000 Training Loss: 0.05142432451248169\n",
      "Epoch 8827/30000 Training Loss: 0.03752928972244263\n",
      "Epoch 8828/30000 Training Loss: 0.06340659409761429\n",
      "Epoch 8829/30000 Training Loss: 0.052385926246643066\n",
      "Epoch 8830/30000 Training Loss: 0.04933302849531174\n",
      "Epoch 8831/30000 Training Loss: 0.05329184979200363\n",
      "Epoch 8832/30000 Training Loss: 0.05038169026374817\n",
      "Epoch 8833/30000 Training Loss: 0.039030179381370544\n",
      "Epoch 8834/30000 Training Loss: 0.05082104355096817\n",
      "Epoch 8835/30000 Training Loss: 0.049350664019584656\n",
      "Epoch 8836/30000 Training Loss: 0.053681690245866776\n",
      "Epoch 8837/30000 Training Loss: 0.035123061388731\n",
      "Epoch 8838/30000 Training Loss: 0.058718256652355194\n",
      "Epoch 8839/30000 Training Loss: 0.0475543737411499\n",
      "Epoch 8840/30000 Training Loss: 0.06318303197622299\n",
      "Epoch 8841/30000 Training Loss: 0.06316231191158295\n",
      "Epoch 8842/30000 Training Loss: 0.050482526421546936\n",
      "Epoch 8843/30000 Training Loss: 0.04834631457924843\n",
      "Epoch 8844/30000 Training Loss: 0.0466257780790329\n",
      "Epoch 8845/30000 Training Loss: 0.04385582357645035\n",
      "Epoch 8846/30000 Training Loss: 0.04334525763988495\n",
      "Epoch 8847/30000 Training Loss: 0.06144636124372482\n",
      "Epoch 8848/30000 Training Loss: 0.05532100796699524\n",
      "Epoch 8849/30000 Training Loss: 0.05269419029355049\n",
      "Epoch 8850/30000 Training Loss: 0.047924309968948364\n",
      "Epoch 8851/30000 Training Loss: 0.03901500254869461\n",
      "Epoch 8852/30000 Training Loss: 0.04827947914600372\n",
      "Epoch 8853/30000 Training Loss: 0.05397831276059151\n",
      "Epoch 8854/30000 Training Loss: 0.053724780678749084\n",
      "Epoch 8855/30000 Training Loss: 0.05927048623561859\n",
      "Epoch 8856/30000 Training Loss: 0.051943402737379074\n",
      "Epoch 8857/30000 Training Loss: 0.04880170524120331\n",
      "Epoch 8858/30000 Training Loss: 0.04775958135724068\n",
      "Epoch 8859/30000 Training Loss: 0.03394465148448944\n",
      "Epoch 8860/30000 Training Loss: 0.04378238320350647\n",
      "Epoch 8861/30000 Training Loss: 0.051833994686603546\n",
      "Epoch 8862/30000 Training Loss: 0.0466984361410141\n",
      "Epoch 8863/30000 Training Loss: 0.04771827161312103\n",
      "Epoch 8864/30000 Training Loss: 0.03559750318527222\n",
      "Epoch 8865/30000 Training Loss: 0.04751919209957123\n",
      "Epoch 8866/30000 Training Loss: 0.05437015742063522\n",
      "Epoch 8867/30000 Training Loss: 0.05444130301475525\n",
      "Epoch 8868/30000 Training Loss: 0.0486021414399147\n",
      "Epoch 8869/30000 Training Loss: 0.06262350082397461\n",
      "Epoch 8870/30000 Training Loss: 0.05110184848308563\n",
      "Epoch 8871/30000 Training Loss: 0.04614529386162758\n",
      "Epoch 8872/30000 Training Loss: 0.056187309324741364\n",
      "Epoch 8873/30000 Training Loss: 0.053362250328063965\n",
      "Epoch 8874/30000 Training Loss: 0.05208776891231537\n",
      "Epoch 8875/30000 Training Loss: 0.057946763932704926\n",
      "Epoch 8876/30000 Training Loss: 0.049378231167793274\n",
      "Epoch 8877/30000 Training Loss: 0.05110235512256622\n",
      "Epoch 8878/30000 Training Loss: 0.04984673857688904\n",
      "Epoch 8879/30000 Training Loss: 0.05297032743692398\n",
      "Epoch 8880/30000 Training Loss: 0.041735537350177765\n",
      "Epoch 8881/30000 Training Loss: 0.040805548429489136\n",
      "Epoch 8882/30000 Training Loss: 0.04873047396540642\n",
      "Epoch 8883/30000 Training Loss: 0.042066872119903564\n",
      "Epoch 8884/30000 Training Loss: 0.055293500423431396\n",
      "Epoch 8885/30000 Training Loss: 0.0403006449341774\n",
      "Epoch 8886/30000 Training Loss: 0.05240585654973984\n",
      "Epoch 8887/30000 Training Loss: 0.04971436411142349\n",
      "Epoch 8888/30000 Training Loss: 0.05537709593772888\n",
      "Epoch 8889/30000 Training Loss: 0.05341963469982147\n",
      "Epoch 8890/30000 Training Loss: 0.043599486351013184\n",
      "Epoch 8891/30000 Training Loss: 0.05686742439866066\n",
      "Epoch 8892/30000 Training Loss: 0.051453642547130585\n",
      "Epoch 8893/30000 Training Loss: 0.04283595830202103\n",
      "Epoch 8894/30000 Training Loss: 0.04368530958890915\n",
      "Epoch 8895/30000 Training Loss: 0.06329453736543655\n",
      "Epoch 8896/30000 Training Loss: 0.04492839053273201\n",
      "Epoch 8897/30000 Training Loss: 0.0695081427693367\n",
      "Epoch 8898/30000 Training Loss: 0.05961962789297104\n",
      "Epoch 8899/30000 Training Loss: 0.06229986250400543\n",
      "Epoch 8900/30000 Training Loss: 0.04823509231209755\n",
      "Epoch 8900/30000 Validation Loss: 0.05402863770723343\n",
      "Epoch 8901/30000 Training Loss: 0.05547570437192917\n",
      "Epoch 8902/30000 Training Loss: 0.05510924011468887\n",
      "Epoch 8903/30000 Training Loss: 0.036076538264751434\n",
      "Epoch 8904/30000 Training Loss: 0.056352876126766205\n",
      "Epoch 8905/30000 Training Loss: 0.04448137432336807\n",
      "Epoch 8906/30000 Training Loss: 0.05448613315820694\n",
      "Epoch 8907/30000 Training Loss: 0.06239577382802963\n",
      "Epoch 8908/30000 Training Loss: 0.05281953513622284\n",
      "Epoch 8909/30000 Training Loss: 0.04400806128978729\n",
      "Epoch 8910/30000 Training Loss: 0.048029132187366486\n",
      "Epoch 8911/30000 Training Loss: 0.04321405664086342\n",
      "Epoch 8912/30000 Training Loss: 0.0465523898601532\n",
      "Epoch 8913/30000 Training Loss: 0.03891464322805405\n",
      "Epoch 8914/30000 Training Loss: 0.03990525007247925\n",
      "Epoch 8915/30000 Training Loss: 0.04231521487236023\n",
      "Epoch 8916/30000 Training Loss: 0.05916321277618408\n",
      "Epoch 8917/30000 Training Loss: 0.0666409358382225\n",
      "Epoch 8918/30000 Training Loss: 0.04920250177383423\n",
      "Epoch 8919/30000 Training Loss: 0.05042243376374245\n",
      "Epoch 8920/30000 Training Loss: 0.05327358469367027\n",
      "Epoch 8921/30000 Training Loss: 0.0402778685092926\n",
      "Epoch 8922/30000 Training Loss: 0.044963736087083817\n",
      "Epoch 8923/30000 Training Loss: 0.05619421973824501\n",
      "Epoch 8924/30000 Training Loss: 0.043224036693573\n",
      "Epoch 8925/30000 Training Loss: 0.05676618218421936\n",
      "Epoch 8926/30000 Training Loss: 0.05171883851289749\n",
      "Epoch 8927/30000 Training Loss: 0.0429639108479023\n",
      "Epoch 8928/30000 Training Loss: 0.052406832575798035\n",
      "Epoch 8929/30000 Training Loss: 0.03734806925058365\n",
      "Epoch 8930/30000 Training Loss: 0.04816511273384094\n",
      "Epoch 8931/30000 Training Loss: 0.05526788532733917\n",
      "Epoch 8932/30000 Training Loss: 0.04614585265517235\n",
      "Epoch 8933/30000 Training Loss: 0.05712877959012985\n",
      "Epoch 8934/30000 Training Loss: 0.058965180069208145\n",
      "Epoch 8935/30000 Training Loss: 0.03785985708236694\n",
      "Epoch 8936/30000 Training Loss: 0.04565570503473282\n",
      "Epoch 8937/30000 Training Loss: 0.0342344269156456\n",
      "Epoch 8938/30000 Training Loss: 0.053057171404361725\n",
      "Epoch 8939/30000 Training Loss: 0.06344684213399887\n",
      "Epoch 8940/30000 Training Loss: 0.05682256072759628\n",
      "Epoch 8941/30000 Training Loss: 0.049203820526599884\n",
      "Epoch 8942/30000 Training Loss: 0.04605686664581299\n",
      "Epoch 8943/30000 Training Loss: 0.050686802715063095\n",
      "Epoch 8944/30000 Training Loss: 0.04022641479969025\n",
      "Epoch 8945/30000 Training Loss: 0.04470815509557724\n",
      "Epoch 8946/30000 Training Loss: 0.05136595666408539\n",
      "Epoch 8947/30000 Training Loss: 0.04869147390127182\n",
      "Epoch 8948/30000 Training Loss: 0.05704266577959061\n",
      "Epoch 8949/30000 Training Loss: 0.0627676397562027\n",
      "Epoch 8950/30000 Training Loss: 0.04980144277215004\n",
      "Epoch 8951/30000 Training Loss: 0.04988306015729904\n",
      "Epoch 8952/30000 Training Loss: 0.05258342623710632\n",
      "Epoch 8953/30000 Training Loss: 0.055181264877319336\n",
      "Epoch 8954/30000 Training Loss: 0.046143949031829834\n",
      "Epoch 8955/30000 Training Loss: 0.046579860150814056\n",
      "Epoch 8956/30000 Training Loss: 0.040034424513578415\n",
      "Epoch 8957/30000 Training Loss: 0.053455181419849396\n",
      "Epoch 8958/30000 Training Loss: 0.05858789384365082\n",
      "Epoch 8959/30000 Training Loss: 0.043087705969810486\n",
      "Epoch 8960/30000 Training Loss: 0.05807485431432724\n",
      "Epoch 8961/30000 Training Loss: 0.047823112457990646\n",
      "Epoch 8962/30000 Training Loss: 0.04992471635341644\n",
      "Epoch 8963/30000 Training Loss: 0.05468547344207764\n",
      "Epoch 8964/30000 Training Loss: 0.04827345907688141\n",
      "Epoch 8965/30000 Training Loss: 0.050410863012075424\n",
      "Epoch 8966/30000 Training Loss: 0.04472070932388306\n",
      "Epoch 8967/30000 Training Loss: 0.044595904648303986\n",
      "Epoch 8968/30000 Training Loss: 0.03999500349164009\n",
      "Epoch 8969/30000 Training Loss: 0.03739990293979645\n",
      "Epoch 8970/30000 Training Loss: 0.059954896569252014\n",
      "Epoch 8971/30000 Training Loss: 0.052620306611061096\n",
      "Epoch 8972/30000 Training Loss: 0.04416387528181076\n",
      "Epoch 8973/30000 Training Loss: 0.0649862289428711\n",
      "Epoch 8974/30000 Training Loss: 0.04986707866191864\n",
      "Epoch 8975/30000 Training Loss: 0.04726187139749527\n",
      "Epoch 8976/30000 Training Loss: 0.05137362331151962\n",
      "Epoch 8977/30000 Training Loss: 0.04587757587432861\n",
      "Epoch 8978/30000 Training Loss: 0.05817526951432228\n",
      "Epoch 8979/30000 Training Loss: 0.047765329480171204\n",
      "Epoch 8980/30000 Training Loss: 0.04988652467727661\n",
      "Epoch 8981/30000 Training Loss: 0.04309776425361633\n",
      "Epoch 8982/30000 Training Loss: 0.05352471023797989\n",
      "Epoch 8983/30000 Training Loss: 0.04881618916988373\n",
      "Epoch 8984/30000 Training Loss: 0.06642021238803864\n",
      "Epoch 8985/30000 Training Loss: 0.05254128947854042\n",
      "Epoch 8986/30000 Training Loss: 0.040714383125305176\n",
      "Epoch 8987/30000 Training Loss: 0.05230690911412239\n",
      "Epoch 8988/30000 Training Loss: 0.0363830104470253\n",
      "Epoch 8989/30000 Training Loss: 0.06418771296739578\n",
      "Epoch 8990/30000 Training Loss: 0.04849451780319214\n",
      "Epoch 8991/30000 Training Loss: 0.04376912862062454\n",
      "Epoch 8992/30000 Training Loss: 0.0621027871966362\n",
      "Epoch 8993/30000 Training Loss: 0.0413132905960083\n",
      "Epoch 8994/30000 Training Loss: 0.03806294873356819\n",
      "Epoch 8995/30000 Training Loss: 0.07002954185009003\n",
      "Epoch 8996/30000 Training Loss: 0.050490591675043106\n",
      "Epoch 8997/30000 Training Loss: 0.049122411757707596\n",
      "Epoch 8998/30000 Training Loss: 0.06235619634389877\n",
      "Epoch 8999/30000 Training Loss: 0.0456668958067894\n",
      "Epoch 9000/30000 Training Loss: 0.06637624651193619\n",
      "Epoch 9000/30000 Validation Loss: 0.051833849400281906\n",
      "Epoch 9001/30000 Training Loss: 0.04447070136666298\n",
      "Epoch 9002/30000 Training Loss: 0.04080286622047424\n",
      "Epoch 9003/30000 Training Loss: 0.0442371629178524\n",
      "Epoch 9004/30000 Training Loss: 0.04838994890451431\n",
      "Epoch 9005/30000 Training Loss: 0.04300535470247269\n",
      "Epoch 9006/30000 Training Loss: 0.03220989927649498\n",
      "Epoch 9007/30000 Training Loss: 0.052461154758930206\n",
      "Epoch 9008/30000 Training Loss: 0.050189487636089325\n",
      "Epoch 9009/30000 Training Loss: 0.041644386947155\n",
      "Epoch 9010/30000 Training Loss: 0.056838102638721466\n",
      "Epoch 9011/30000 Training Loss: 0.06790294498205185\n",
      "Epoch 9012/30000 Training Loss: 0.043138500303030014\n",
      "Epoch 9013/30000 Training Loss: 0.0416850671172142\n",
      "Epoch 9014/30000 Training Loss: 0.05576910451054573\n",
      "Epoch 9015/30000 Training Loss: 0.05985955521464348\n",
      "Epoch 9016/30000 Training Loss: 0.056689247488975525\n",
      "Epoch 9017/30000 Training Loss: 0.0417453832924366\n",
      "Epoch 9018/30000 Training Loss: 0.06346805393695831\n",
      "Epoch 9019/30000 Training Loss: 0.05851593613624573\n",
      "Epoch 9020/30000 Training Loss: 0.047249987721443176\n",
      "Epoch 9021/30000 Training Loss: 0.0466989129781723\n",
      "Epoch 9022/30000 Training Loss: 0.053201932460069656\n",
      "Epoch 9023/30000 Training Loss: 0.03808033466339111\n",
      "Epoch 9024/30000 Training Loss: 0.04537113383412361\n",
      "Epoch 9025/30000 Training Loss: 0.042468465864658356\n",
      "Epoch 9026/30000 Training Loss: 0.047074928879737854\n",
      "Epoch 9027/30000 Training Loss: 0.046472951769828796\n",
      "Epoch 9028/30000 Training Loss: 0.06184401363134384\n",
      "Epoch 9029/30000 Training Loss: 0.04704803228378296\n",
      "Epoch 9030/30000 Training Loss: 0.0713387057185173\n",
      "Epoch 9031/30000 Training Loss: 0.050777532160282135\n",
      "Epoch 9032/30000 Training Loss: 0.05348645895719528\n",
      "Epoch 9033/30000 Training Loss: 0.04808654636144638\n",
      "Epoch 9034/30000 Training Loss: 0.04984757676720619\n",
      "Epoch 9035/30000 Training Loss: 0.05181831493973732\n",
      "Epoch 9036/30000 Training Loss: 0.048822902143001556\n",
      "Epoch 9037/30000 Training Loss: 0.060766614973545074\n",
      "Epoch 9038/30000 Training Loss: 0.04069800674915314\n",
      "Epoch 9039/30000 Training Loss: 0.04701695591211319\n",
      "Epoch 9040/30000 Training Loss: 0.05424102395772934\n",
      "Epoch 9041/30000 Training Loss: 0.04514976590871811\n",
      "Epoch 9042/30000 Training Loss: 0.050570547580718994\n",
      "Epoch 9043/30000 Training Loss: 0.044112175703048706\n",
      "Epoch 9044/30000 Training Loss: 0.06747377663850784\n",
      "Epoch 9045/30000 Training Loss: 0.042792774736881256\n",
      "Epoch 9046/30000 Training Loss: 0.03571179509162903\n",
      "Epoch 9047/30000 Training Loss: 0.047817252576351166\n",
      "Epoch 9048/30000 Training Loss: 0.05727565288543701\n",
      "Epoch 9049/30000 Training Loss: 0.060858890414237976\n",
      "Epoch 9050/30000 Training Loss: 0.05024849623441696\n",
      "Epoch 9051/30000 Training Loss: 0.04803425818681717\n",
      "Epoch 9052/30000 Training Loss: 0.06507894396781921\n",
      "Epoch 9053/30000 Training Loss: 0.041467562317848206\n",
      "Epoch 9054/30000 Training Loss: 0.05139758065342903\n",
      "Epoch 9055/30000 Training Loss: 0.06609056890010834\n",
      "Epoch 9056/30000 Training Loss: 0.04482783004641533\n",
      "Epoch 9057/30000 Training Loss: 0.05027737468481064\n",
      "Epoch 9058/30000 Training Loss: 0.04274348169565201\n",
      "Epoch 9059/30000 Training Loss: 0.048567213118076324\n",
      "Epoch 9060/30000 Training Loss: 0.05810743570327759\n",
      "Epoch 9061/30000 Training Loss: 0.053084276616573334\n",
      "Epoch 9062/30000 Training Loss: 0.04773113876581192\n",
      "Epoch 9063/30000 Training Loss: 0.04318469762802124\n",
      "Epoch 9064/30000 Training Loss: 0.036744557321071625\n",
      "Epoch 9065/30000 Training Loss: 0.05870753526687622\n",
      "Epoch 9066/30000 Training Loss: 0.056859590113162994\n",
      "Epoch 9067/30000 Training Loss: 0.050587330013513565\n",
      "Epoch 9068/30000 Training Loss: 0.0355253592133522\n",
      "Epoch 9069/30000 Training Loss: 0.053406357765197754\n",
      "Epoch 9070/30000 Training Loss: 0.04274857044219971\n",
      "Epoch 9071/30000 Training Loss: 0.0576556921005249\n",
      "Epoch 9072/30000 Training Loss: 0.05026998370885849\n",
      "Epoch 9073/30000 Training Loss: 0.046882182359695435\n",
      "Epoch 9074/30000 Training Loss: 0.049966633319854736\n",
      "Epoch 9075/30000 Training Loss: 0.04487676918506622\n",
      "Epoch 9076/30000 Training Loss: 0.04711207002401352\n",
      "Epoch 9077/30000 Training Loss: 0.052243463695049286\n",
      "Epoch 9078/30000 Training Loss: 0.05211755633354187\n",
      "Epoch 9079/30000 Training Loss: 0.04438832774758339\n",
      "Epoch 9080/30000 Training Loss: 0.042712483555078506\n",
      "Epoch 9081/30000 Training Loss: 0.041872233152389526\n",
      "Epoch 9082/30000 Training Loss: 0.054526180028915405\n",
      "Epoch 9083/30000 Training Loss: 0.0693276971578598\n",
      "Epoch 9084/30000 Training Loss: 0.05979800596833229\n",
      "Epoch 9085/30000 Training Loss: 0.060301654040813446\n",
      "Epoch 9086/30000 Training Loss: 0.05568765476346016\n",
      "Epoch 9087/30000 Training Loss: 0.05658514052629471\n",
      "Epoch 9088/30000 Training Loss: 0.047363609075546265\n",
      "Epoch 9089/30000 Training Loss: 0.0512523353099823\n",
      "Epoch 9090/30000 Training Loss: 0.04844347760081291\n",
      "Epoch 9091/30000 Training Loss: 0.05009494721889496\n",
      "Epoch 9092/30000 Training Loss: 0.05640508607029915\n",
      "Epoch 9093/30000 Training Loss: 0.05562017112970352\n",
      "Epoch 9094/30000 Training Loss: 0.0408439077436924\n",
      "Epoch 9095/30000 Training Loss: 0.0561637282371521\n",
      "Epoch 9096/30000 Training Loss: 0.05187821760773659\n",
      "Epoch 9097/30000 Training Loss: 0.04014960676431656\n",
      "Epoch 9098/30000 Training Loss: 0.04366191476583481\n",
      "Epoch 9099/30000 Training Loss: 0.04637249559164047\n",
      "Epoch 9100/30000 Training Loss: 0.061768390238285065\n",
      "Epoch 9100/30000 Validation Loss: 0.05649207532405853\n",
      "Epoch 9101/30000 Training Loss: 0.052620090544223785\n",
      "Epoch 9102/30000 Training Loss: 0.04317062348127365\n",
      "Epoch 9103/30000 Training Loss: 0.05719383805990219\n",
      "Epoch 9104/30000 Training Loss: 0.05013056844472885\n",
      "Epoch 9105/30000 Training Loss: 0.06172808259725571\n",
      "Epoch 9106/30000 Training Loss: 0.045894261449575424\n",
      "Epoch 9107/30000 Training Loss: 0.045209355652332306\n",
      "Epoch 9108/30000 Training Loss: 0.05465256795287132\n",
      "Epoch 9109/30000 Training Loss: 0.04507695883512497\n",
      "Epoch 9110/30000 Training Loss: 0.05396631360054016\n",
      "Epoch 9111/30000 Training Loss: 0.05097845941781998\n",
      "Epoch 9112/30000 Training Loss: 0.06642851233482361\n",
      "Epoch 9113/30000 Training Loss: 0.05588315427303314\n",
      "Epoch 9114/30000 Training Loss: 0.05349612981081009\n",
      "Epoch 9115/30000 Training Loss: 0.040080007165670395\n",
      "Epoch 9116/30000 Training Loss: 0.042774464935064316\n",
      "Epoch 9117/30000 Training Loss: 0.04723341017961502\n",
      "Epoch 9118/30000 Training Loss: 0.041745588183403015\n",
      "Epoch 9119/30000 Training Loss: 0.05323459580540657\n",
      "Epoch 9120/30000 Training Loss: 0.05348942428827286\n",
      "Epoch 9121/30000 Training Loss: 0.049062930047512054\n",
      "Epoch 9122/30000 Training Loss: 0.03777824342250824\n",
      "Epoch 9123/30000 Training Loss: 0.05470874905586243\n",
      "Epoch 9124/30000 Training Loss: 0.04557153210043907\n",
      "Epoch 9125/30000 Training Loss: 0.05577874928712845\n",
      "Epoch 9126/30000 Training Loss: 0.05142730101943016\n",
      "Epoch 9127/30000 Training Loss: 0.048647381365299225\n",
      "Epoch 9128/30000 Training Loss: 0.07425137609243393\n",
      "Epoch 9129/30000 Training Loss: 0.04528006911277771\n",
      "Epoch 9130/30000 Training Loss: 0.04924747720360756\n",
      "Epoch 9131/30000 Training Loss: 0.059773869812488556\n",
      "Epoch 9132/30000 Training Loss: 0.055807165801525116\n",
      "Epoch 9133/30000 Training Loss: 0.04041627049446106\n",
      "Epoch 9134/30000 Training Loss: 0.053565993905067444\n",
      "Epoch 9135/30000 Training Loss: 0.05707535147666931\n",
      "Epoch 9136/30000 Training Loss: 0.05611689016222954\n",
      "Epoch 9137/30000 Training Loss: 0.0628717839717865\n",
      "Epoch 9138/30000 Training Loss: 0.04663076996803284\n",
      "Epoch 9139/30000 Training Loss: 0.04068652540445328\n",
      "Epoch 9140/30000 Training Loss: 0.0508442297577858\n",
      "Epoch 9141/30000 Training Loss: 0.06309925019741058\n",
      "Epoch 9142/30000 Training Loss: 0.05753627419471741\n",
      "Epoch 9143/30000 Training Loss: 0.04776211455464363\n",
      "Epoch 9144/30000 Training Loss: 0.047486480325460434\n",
      "Epoch 9145/30000 Training Loss: 0.041885145008563995\n",
      "Epoch 9146/30000 Training Loss: 0.04558973014354706\n",
      "Epoch 9147/30000 Training Loss: 0.044623613357543945\n",
      "Epoch 9148/30000 Training Loss: 0.04897589609026909\n",
      "Epoch 9149/30000 Training Loss: 0.0559283085167408\n",
      "Epoch 9150/30000 Training Loss: 0.03651367872953415\n",
      "Epoch 9151/30000 Training Loss: 0.05372176319360733\n",
      "Epoch 9152/30000 Training Loss: 0.03948824852705002\n",
      "Epoch 9153/30000 Training Loss: 0.04728509485721588\n",
      "Epoch 9154/30000 Training Loss: 0.043797336518764496\n",
      "Epoch 9155/30000 Training Loss: 0.05737912654876709\n",
      "Epoch 9156/30000 Training Loss: 0.053281135857105255\n",
      "Epoch 9157/30000 Training Loss: 0.0493243932723999\n",
      "Epoch 9158/30000 Training Loss: 0.0338137224316597\n",
      "Epoch 9159/30000 Training Loss: 0.045587487518787384\n",
      "Epoch 9160/30000 Training Loss: 0.05156815052032471\n",
      "Epoch 9161/30000 Training Loss: 0.031169209629297256\n",
      "Epoch 9162/30000 Training Loss: 0.04002704471349716\n",
      "Epoch 9163/30000 Training Loss: 0.04718220233917236\n",
      "Epoch 9164/30000 Training Loss: 0.04078920558094978\n",
      "Epoch 9165/30000 Training Loss: 0.0653279572725296\n",
      "Epoch 9166/30000 Training Loss: 0.038360003381967545\n",
      "Epoch 9167/30000 Training Loss: 0.06221596896648407\n",
      "Epoch 9168/30000 Training Loss: 0.055553533136844635\n",
      "Epoch 9169/30000 Training Loss: 0.057510554790496826\n",
      "Epoch 9170/30000 Training Loss: 0.058293215930461884\n",
      "Epoch 9171/30000 Training Loss: 0.042137131094932556\n",
      "Epoch 9172/30000 Training Loss: 0.042217567563056946\n",
      "Epoch 9173/30000 Training Loss: 0.056040842086076736\n",
      "Epoch 9174/30000 Training Loss: 0.057445116341114044\n",
      "Epoch 9175/30000 Training Loss: 0.05664887651801109\n",
      "Epoch 9176/30000 Training Loss: 0.04218833148479462\n",
      "Epoch 9177/30000 Training Loss: 0.07025152444839478\n",
      "Epoch 9178/30000 Training Loss: 0.05334398150444031\n",
      "Epoch 9179/30000 Training Loss: 0.06391742825508118\n",
      "Epoch 9180/30000 Training Loss: 0.03477799892425537\n",
      "Epoch 9181/30000 Training Loss: 0.04396577924489975\n",
      "Epoch 9182/30000 Training Loss: 0.063636913895607\n",
      "Epoch 9183/30000 Training Loss: 0.06132198125123978\n",
      "Epoch 9184/30000 Training Loss: 0.04189816117286682\n",
      "Epoch 9185/30000 Training Loss: 0.0400010421872139\n",
      "Epoch 9186/30000 Training Loss: 0.045141156762838364\n",
      "Epoch 9187/30000 Training Loss: 0.05449860543012619\n",
      "Epoch 9188/30000 Training Loss: 0.03873426094651222\n",
      "Epoch 9189/30000 Training Loss: 0.04109978675842285\n",
      "Epoch 9190/30000 Training Loss: 0.0546901561319828\n",
      "Epoch 9191/30000 Training Loss: 0.06341597437858582\n",
      "Epoch 9192/30000 Training Loss: 0.05101277679204941\n",
      "Epoch 9193/30000 Training Loss: 0.03690515458583832\n",
      "Epoch 9194/30000 Training Loss: 0.05255916714668274\n",
      "Epoch 9195/30000 Training Loss: 0.06993749737739563\n",
      "Epoch 9196/30000 Training Loss: 0.04920096695423126\n",
      "Epoch 9197/30000 Training Loss: 0.0494319349527359\n",
      "Epoch 9198/30000 Training Loss: 0.04634156823158264\n",
      "Epoch 9199/30000 Training Loss: 0.04983096569776535\n",
      "Epoch 9200/30000 Training Loss: 0.04725760594010353\n",
      "Epoch 9200/30000 Validation Loss: 0.05775590240955353\n",
      "Epoch 9201/30000 Training Loss: 0.0437241867184639\n",
      "Epoch 9202/30000 Training Loss: 0.05702628195285797\n",
      "Epoch 9203/30000 Training Loss: 0.04981396347284317\n",
      "Epoch 9204/30000 Training Loss: 0.06011398136615753\n",
      "Epoch 9205/30000 Training Loss: 0.05062618851661682\n",
      "Epoch 9206/30000 Training Loss: 0.041801437735557556\n",
      "Epoch 9207/30000 Training Loss: 0.06269236654043198\n",
      "Epoch 9208/30000 Training Loss: 0.053437408059835434\n",
      "Epoch 9209/30000 Training Loss: 0.03602558374404907\n",
      "Epoch 9210/30000 Training Loss: 0.04466457664966583\n",
      "Epoch 9211/30000 Training Loss: 0.04990439862012863\n",
      "Epoch 9212/30000 Training Loss: 0.050895072519779205\n",
      "Epoch 9213/30000 Training Loss: 0.03723563253879547\n",
      "Epoch 9214/30000 Training Loss: 0.059258971363306046\n",
      "Epoch 9215/30000 Training Loss: 0.06339263916015625\n",
      "Epoch 9216/30000 Training Loss: 0.03694858029484749\n",
      "Epoch 9217/30000 Training Loss: 0.04448474198579788\n",
      "Epoch 9218/30000 Training Loss: 0.0440916046500206\n",
      "Epoch 9219/30000 Training Loss: 0.07128158211708069\n",
      "Epoch 9220/30000 Training Loss: 0.04521087557077408\n",
      "Epoch 9221/30000 Training Loss: 0.0634065568447113\n",
      "Epoch 9222/30000 Training Loss: 0.043002624064683914\n",
      "Epoch 9223/30000 Training Loss: 0.05077090114355087\n",
      "Epoch 9224/30000 Training Loss: 0.048991166055202484\n",
      "Epoch 9225/30000 Training Loss: 0.06193303316831589\n",
      "Epoch 9226/30000 Training Loss: 0.05500330030918121\n",
      "Epoch 9227/30000 Training Loss: 0.052371129393577576\n",
      "Epoch 9228/30000 Training Loss: 0.05270075798034668\n",
      "Epoch 9229/30000 Training Loss: 0.04040660336613655\n",
      "Epoch 9230/30000 Training Loss: 0.03985010087490082\n",
      "Epoch 9231/30000 Training Loss: 0.04176749289035797\n",
      "Epoch 9232/30000 Training Loss: 0.06089875474572182\n",
      "Epoch 9233/30000 Training Loss: 0.03601562976837158\n",
      "Epoch 9234/30000 Training Loss: 0.054650478065013885\n",
      "Epoch 9235/30000 Training Loss: 0.05301899090409279\n",
      "Epoch 9236/30000 Training Loss: 0.05758548527956009\n",
      "Epoch 9237/30000 Training Loss: 0.04821465164422989\n",
      "Epoch 9238/30000 Training Loss: 0.039348479360342026\n",
      "Epoch 9239/30000 Training Loss: 0.053180135786533356\n",
      "Epoch 9240/30000 Training Loss: 0.06997951120138168\n",
      "Epoch 9241/30000 Training Loss: 0.04115599766373634\n",
      "Epoch 9242/30000 Training Loss: 0.05513967201113701\n",
      "Epoch 9243/30000 Training Loss: 0.04890209808945656\n",
      "Epoch 9244/30000 Training Loss: 0.03956889361143112\n",
      "Epoch 9245/30000 Training Loss: 0.057669512927532196\n",
      "Epoch 9246/30000 Training Loss: 0.06399862468242645\n",
      "Epoch 9247/30000 Training Loss: 0.0623861700296402\n",
      "Epoch 9248/30000 Training Loss: 0.05752486363053322\n",
      "Epoch 9249/30000 Training Loss: 0.06976199150085449\n",
      "Epoch 9250/30000 Training Loss: 0.04117840528488159\n",
      "Epoch 9251/30000 Training Loss: 0.05031166598200798\n",
      "Epoch 9252/30000 Training Loss: 0.046660035848617554\n",
      "Epoch 9253/30000 Training Loss: 0.037710584700107574\n",
      "Epoch 9254/30000 Training Loss: 0.04747697710990906\n",
      "Epoch 9255/30000 Training Loss: 0.056670960038900375\n",
      "Epoch 9256/30000 Training Loss: 0.05614183843135834\n",
      "Epoch 9257/30000 Training Loss: 0.04659226909279823\n",
      "Epoch 9258/30000 Training Loss: 0.04972658306360245\n",
      "Epoch 9259/30000 Training Loss: 0.0469077005982399\n",
      "Epoch 9260/30000 Training Loss: 0.04234181344509125\n",
      "Epoch 9261/30000 Training Loss: 0.05405312776565552\n",
      "Epoch 9262/30000 Training Loss: 0.04418106749653816\n",
      "Epoch 9263/30000 Training Loss: 0.049942389130592346\n",
      "Epoch 9264/30000 Training Loss: 0.04136701300740242\n",
      "Epoch 9265/30000 Training Loss: 0.03664875775575638\n",
      "Epoch 9266/30000 Training Loss: 0.0570342093706131\n",
      "Epoch 9267/30000 Training Loss: 0.056831344962120056\n",
      "Epoch 9268/30000 Training Loss: 0.07034672796726227\n",
      "Epoch 9269/30000 Training Loss: 0.04871435835957527\n",
      "Epoch 9270/30000 Training Loss: 0.06422050297260284\n",
      "Epoch 9271/30000 Training Loss: 0.04154787212610245\n",
      "Epoch 9272/30000 Training Loss: 0.04101689159870148\n",
      "Epoch 9273/30000 Training Loss: 0.05979166179895401\n",
      "Epoch 9274/30000 Training Loss: 0.0507180392742157\n",
      "Epoch 9275/30000 Training Loss: 0.055028051137924194\n",
      "Epoch 9276/30000 Training Loss: 0.05258342623710632\n",
      "Epoch 9277/30000 Training Loss: 0.05620802566409111\n",
      "Epoch 9278/30000 Training Loss: 0.05150596424937248\n",
      "Epoch 9279/30000 Training Loss: 0.04817761480808258\n",
      "Epoch 9280/30000 Training Loss: 0.04611901938915253\n",
      "Epoch 9281/30000 Training Loss: 0.04016917943954468\n",
      "Epoch 9282/30000 Training Loss: 0.05143492668867111\n",
      "Epoch 9283/30000 Training Loss: 0.06171639263629913\n",
      "Epoch 9284/30000 Training Loss: 0.04621318355202675\n",
      "Epoch 9285/30000 Training Loss: 0.05277930945158005\n",
      "Epoch 9286/30000 Training Loss: 0.04271777719259262\n",
      "Epoch 9287/30000 Training Loss: 0.04801402986049652\n",
      "Epoch 9288/30000 Training Loss: 0.056315332651138306\n",
      "Epoch 9289/30000 Training Loss: 0.04412119835615158\n",
      "Epoch 9290/30000 Training Loss: 0.05415983498096466\n",
      "Epoch 9291/30000 Training Loss: 0.0608791746199131\n",
      "Epoch 9292/30000 Training Loss: 0.060849741101264954\n",
      "Epoch 9293/30000 Training Loss: 0.04982759803533554\n",
      "Epoch 9294/30000 Training Loss: 0.04807649552822113\n",
      "Epoch 9295/30000 Training Loss: 0.058114565908908844\n",
      "Epoch 9296/30000 Training Loss: 0.04886653274297714\n",
      "Epoch 9297/30000 Training Loss: 0.042064011096954346\n",
      "Epoch 9298/30000 Training Loss: 0.03778955340385437\n",
      "Epoch 9299/30000 Training Loss: 0.044462352991104126\n",
      "Epoch 9300/30000 Training Loss: 0.059358157217502594\n",
      "Epoch 9300/30000 Validation Loss: 0.05068757012486458\n",
      "Epoch 9301/30000 Training Loss: 0.04995828866958618\n",
      "Epoch 9302/30000 Training Loss: 0.058065228164196014\n",
      "Epoch 9303/30000 Training Loss: 0.04664909839630127\n",
      "Epoch 9304/30000 Training Loss: 0.0459846630692482\n",
      "Epoch 9305/30000 Training Loss: 0.04693140834569931\n",
      "Epoch 9306/30000 Training Loss: 0.04325949028134346\n",
      "Epoch 9307/30000 Training Loss: 0.04660157859325409\n",
      "Epoch 9308/30000 Training Loss: 0.05151853337883949\n",
      "Epoch 9309/30000 Training Loss: 0.04736153036355972\n",
      "Epoch 9310/30000 Training Loss: 0.046739593148231506\n",
      "Epoch 9311/30000 Training Loss: 0.059629835188388824\n",
      "Epoch 9312/30000 Training Loss: 0.04074794054031372\n",
      "Epoch 9313/30000 Training Loss: 0.03853750601410866\n",
      "Epoch 9314/30000 Training Loss: 0.039953552186489105\n",
      "Epoch 9315/30000 Training Loss: 0.05896160006523132\n",
      "Epoch 9316/30000 Training Loss: 0.05506889894604683\n",
      "Epoch 9317/30000 Training Loss: 0.053768716752529144\n",
      "Epoch 9318/30000 Training Loss: 0.05295545980334282\n",
      "Epoch 9319/30000 Training Loss: 0.04774293303489685\n",
      "Epoch 9320/30000 Training Loss: 0.050800710916519165\n",
      "Epoch 9321/30000 Training Loss: 0.05172122269868851\n",
      "Epoch 9322/30000 Training Loss: 0.04737299680709839\n",
      "Epoch 9323/30000 Training Loss: 0.053077537566423416\n",
      "Epoch 9324/30000 Training Loss: 0.04956263676285744\n",
      "Epoch 9325/30000 Training Loss: 0.061133936047554016\n",
      "Epoch 9326/30000 Training Loss: 0.052627451717853546\n",
      "Epoch 9327/30000 Training Loss: 0.03802211582660675\n",
      "Epoch 9328/30000 Training Loss: 0.048734597861766815\n",
      "Epoch 9329/30000 Training Loss: 0.05405496805906296\n",
      "Epoch 9330/30000 Training Loss: 0.041638441383838654\n",
      "Epoch 9331/30000 Training Loss: 0.03848595917224884\n",
      "Epoch 9332/30000 Training Loss: 0.05626605451107025\n",
      "Epoch 9333/30000 Training Loss: 0.05460156127810478\n",
      "Epoch 9334/30000 Training Loss: 0.058430179953575134\n",
      "Epoch 9335/30000 Training Loss: 0.04514756798744202\n",
      "Epoch 9336/30000 Training Loss: 0.05316703021526337\n",
      "Epoch 9337/30000 Training Loss: 0.047622375190258026\n",
      "Epoch 9338/30000 Training Loss: 0.04765532165765762\n",
      "Epoch 9339/30000 Training Loss: 0.05195920169353485\n",
      "Epoch 9340/30000 Training Loss: 0.05400838702917099\n",
      "Epoch 9341/30000 Training Loss: 0.045092929154634476\n",
      "Epoch 9342/30000 Training Loss: 0.06422170996665955\n",
      "Epoch 9343/30000 Training Loss: 0.036655522882938385\n",
      "Epoch 9344/30000 Training Loss: 0.05711161345243454\n",
      "Epoch 9345/30000 Training Loss: 0.06600800156593323\n",
      "Epoch 9346/30000 Training Loss: 0.07824191451072693\n",
      "Epoch 9347/30000 Training Loss: 0.07426771521568298\n",
      "Epoch 9348/30000 Training Loss: 0.04552866518497467\n",
      "Epoch 9349/30000 Training Loss: 0.056137725710868835\n",
      "Epoch 9350/30000 Training Loss: 0.04902879148721695\n",
      "Epoch 9351/30000 Training Loss: 0.06031981110572815\n",
      "Epoch 9352/30000 Training Loss: 0.04668435454368591\n",
      "Epoch 9353/30000 Training Loss: 0.06031130626797676\n",
      "Epoch 9354/30000 Training Loss: 0.04012739658355713\n",
      "Epoch 9355/30000 Training Loss: 0.0626533031463623\n",
      "Epoch 9356/30000 Training Loss: 0.0543808713555336\n",
      "Epoch 9357/30000 Training Loss: 0.05346861481666565\n",
      "Epoch 9358/30000 Training Loss: 0.04292835295200348\n",
      "Epoch 9359/30000 Training Loss: 0.04158094897866249\n",
      "Epoch 9360/30000 Training Loss: 0.053978100419044495\n",
      "Epoch 9361/30000 Training Loss: 0.039989206939935684\n",
      "Epoch 9362/30000 Training Loss: 0.04078856110572815\n",
      "Epoch 9363/30000 Training Loss: 0.05315889045596123\n",
      "Epoch 9364/30000 Training Loss: 0.06496492028236389\n",
      "Epoch 9365/30000 Training Loss: 0.03848438337445259\n",
      "Epoch 9366/30000 Training Loss: 0.058601368218660355\n",
      "Epoch 9367/30000 Training Loss: 0.040053706616163254\n",
      "Epoch 9368/30000 Training Loss: 0.05610941722989082\n",
      "Epoch 9369/30000 Training Loss: 0.034888722002506256\n",
      "Epoch 9370/30000 Training Loss: 0.05096212029457092\n",
      "Epoch 9371/30000 Training Loss: 0.05559375882148743\n",
      "Epoch 9372/30000 Training Loss: 0.047919295728206635\n",
      "Epoch 9373/30000 Training Loss: 0.05316279083490372\n",
      "Epoch 9374/30000 Training Loss: 0.0566088892519474\n",
      "Epoch 9375/30000 Training Loss: 0.05611749365925789\n",
      "Epoch 9376/30000 Training Loss: 0.049646783620119095\n",
      "Epoch 9377/30000 Training Loss: 0.042107969522476196\n",
      "Epoch 9378/30000 Training Loss: 0.05108512192964554\n",
      "Epoch 9379/30000 Training Loss: 0.058298543095588684\n",
      "Epoch 9380/30000 Training Loss: 0.04515226185321808\n",
      "Epoch 9381/30000 Training Loss: 0.05223754793405533\n",
      "Epoch 9382/30000 Training Loss: 0.04970664158463478\n",
      "Epoch 9383/30000 Training Loss: 0.056878283619880676\n",
      "Epoch 9384/30000 Training Loss: 0.051531948149204254\n",
      "Epoch 9385/30000 Training Loss: 0.044586896896362305\n",
      "Epoch 9386/30000 Training Loss: 0.04302701726555824\n",
      "Epoch 9387/30000 Training Loss: 0.05112975090742111\n",
      "Epoch 9388/30000 Training Loss: 0.04795171692967415\n",
      "Epoch 9389/30000 Training Loss: 0.0498015396296978\n",
      "Epoch 9390/30000 Training Loss: 0.035978179425001144\n",
      "Epoch 9391/30000 Training Loss: 0.04304380342364311\n",
      "Epoch 9392/30000 Training Loss: 0.06338974088430405\n",
      "Epoch 9393/30000 Training Loss: 0.04579316824674606\n",
      "Epoch 9394/30000 Training Loss: 0.07050373405218124\n",
      "Epoch 9395/30000 Training Loss: 0.05613923817873001\n",
      "Epoch 9396/30000 Training Loss: 0.043490730226039886\n",
      "Epoch 9397/30000 Training Loss: 0.046680353581905365\n",
      "Epoch 9398/30000 Training Loss: 0.07306838035583496\n",
      "Epoch 9399/30000 Training Loss: 0.035995133221149445\n",
      "Epoch 9400/30000 Training Loss: 0.046416718512773514\n",
      "Epoch 9400/30000 Validation Loss: 0.040975987911224365\n",
      "Epoch 9401/30000 Training Loss: 0.049509041011333466\n",
      "Epoch 9402/30000 Training Loss: 0.04332399368286133\n",
      "Epoch 9403/30000 Training Loss: 0.038750968873500824\n",
      "Epoch 9404/30000 Training Loss: 0.050721026957035065\n",
      "Epoch 9405/30000 Training Loss: 0.05480251833796501\n",
      "Epoch 9406/30000 Training Loss: 0.0431031696498394\n",
      "Epoch 9407/30000 Training Loss: 0.04693800210952759\n",
      "Epoch 9408/30000 Training Loss: 0.04600944742560387\n",
      "Epoch 9409/30000 Training Loss: 0.05072242021560669\n",
      "Epoch 9410/30000 Training Loss: 0.04896659776568413\n",
      "Epoch 9411/30000 Training Loss: 0.0505068376660347\n",
      "Epoch 9412/30000 Training Loss: 0.03703738749027252\n",
      "Epoch 9413/30000 Training Loss: 0.049335770308971405\n",
      "Epoch 9414/30000 Training Loss: 0.07339337468147278\n",
      "Epoch 9415/30000 Training Loss: 0.05421735346317291\n",
      "Epoch 9416/30000 Training Loss: 0.045390620827674866\n",
      "Epoch 9417/30000 Training Loss: 0.04948027804493904\n",
      "Epoch 9418/30000 Training Loss: 0.035920023918151855\n",
      "Epoch 9419/30000 Training Loss: 0.04353911429643631\n",
      "Epoch 9420/30000 Training Loss: 0.03867905214428902\n",
      "Epoch 9421/30000 Training Loss: 0.0502326525747776\n",
      "Epoch 9422/30000 Training Loss: 0.05764938145875931\n",
      "Epoch 9423/30000 Training Loss: 0.05103183165192604\n",
      "Epoch 9424/30000 Training Loss: 0.05108539015054703\n",
      "Epoch 9425/30000 Training Loss: 0.0664549469947815\n",
      "Epoch 9426/30000 Training Loss: 0.049487240612506866\n",
      "Epoch 9427/30000 Training Loss: 0.03784014284610748\n",
      "Epoch 9428/30000 Training Loss: 0.05631023645401001\n",
      "Epoch 9429/30000 Training Loss: 0.05015239119529724\n",
      "Epoch 9430/30000 Training Loss: 0.04405286908149719\n",
      "Epoch 9431/30000 Training Loss: 0.05744239687919617\n",
      "Epoch 9432/30000 Training Loss: 0.04796827584505081\n",
      "Epoch 9433/30000 Training Loss: 0.03663318604230881\n",
      "Epoch 9434/30000 Training Loss: 0.06042923405766487\n",
      "Epoch 9435/30000 Training Loss: 0.05709272623062134\n",
      "Epoch 9436/30000 Training Loss: 0.04591803252696991\n",
      "Epoch 9437/30000 Training Loss: 0.06364726275205612\n",
      "Epoch 9438/30000 Training Loss: 0.052035413682460785\n",
      "Epoch 9439/30000 Training Loss: 0.05774213373661041\n",
      "Epoch 9440/30000 Training Loss: 0.04381081461906433\n",
      "Epoch 9441/30000 Training Loss: 0.0520748645067215\n",
      "Epoch 9442/30000 Training Loss: 0.052608221769332886\n",
      "Epoch 9443/30000 Training Loss: 0.06502752006053925\n",
      "Epoch 9444/30000 Training Loss: 0.04837208241224289\n",
      "Epoch 9445/30000 Training Loss: 0.05950074642896652\n",
      "Epoch 9446/30000 Training Loss: 0.05113837495446205\n",
      "Epoch 9447/30000 Training Loss: 0.045124247670173645\n",
      "Epoch 9448/30000 Training Loss: 0.04726649820804596\n",
      "Epoch 9449/30000 Training Loss: 0.06429192423820496\n",
      "Epoch 9450/30000 Training Loss: 0.06177181005477905\n",
      "Epoch 9451/30000 Training Loss: 0.04034058377146721\n",
      "Epoch 9452/30000 Training Loss: 0.04647299274802208\n",
      "Epoch 9453/30000 Training Loss: 0.04737170785665512\n",
      "Epoch 9454/30000 Training Loss: 0.053910233080387115\n",
      "Epoch 9455/30000 Training Loss: 0.052163537591695786\n",
      "Epoch 9456/30000 Training Loss: 0.04832880198955536\n",
      "Epoch 9457/30000 Training Loss: 0.06208772957324982\n",
      "Epoch 9458/30000 Training Loss: 0.04844112694263458\n",
      "Epoch 9459/30000 Training Loss: 0.05185206979513168\n",
      "Epoch 9460/30000 Training Loss: 0.04050145298242569\n",
      "Epoch 9461/30000 Training Loss: 0.04814811050891876\n",
      "Epoch 9462/30000 Training Loss: 0.044970616698265076\n",
      "Epoch 9463/30000 Training Loss: 0.049850188195705414\n",
      "Epoch 9464/30000 Training Loss: 0.060321465134620667\n",
      "Epoch 9465/30000 Training Loss: 0.04364333301782608\n",
      "Epoch 9466/30000 Training Loss: 0.04415437579154968\n",
      "Epoch 9467/30000 Training Loss: 0.05823858454823494\n",
      "Epoch 9468/30000 Training Loss: 0.050919800996780396\n",
      "Epoch 9469/30000 Training Loss: 0.04135890305042267\n",
      "Epoch 9470/30000 Training Loss: 0.05037933960556984\n",
      "Epoch 9471/30000 Training Loss: 0.06283693015575409\n",
      "Epoch 9472/30000 Training Loss: 0.05882202833890915\n",
      "Epoch 9473/30000 Training Loss: 0.0474105179309845\n",
      "Epoch 9474/30000 Training Loss: 0.03995329886674881\n",
      "Epoch 9475/30000 Training Loss: 0.04318614304065704\n",
      "Epoch 9476/30000 Training Loss: 0.05229276791214943\n",
      "Epoch 9477/30000 Training Loss: 0.04868139326572418\n",
      "Epoch 9478/30000 Training Loss: 0.053859490901231766\n",
      "Epoch 9479/30000 Training Loss: 0.04837968945503235\n",
      "Epoch 9480/30000 Training Loss: 0.05823910981416702\n",
      "Epoch 9481/30000 Training Loss: 0.032122205942869186\n",
      "Epoch 9482/30000 Training Loss: 0.046942926943302155\n",
      "Epoch 9483/30000 Training Loss: 0.04836925119161606\n",
      "Epoch 9484/30000 Training Loss: 0.04402017220854759\n",
      "Epoch 9485/30000 Training Loss: 0.042826972901821136\n",
      "Epoch 9486/30000 Training Loss: 0.0390605702996254\n",
      "Epoch 9487/30000 Training Loss: 0.055926110595464706\n",
      "Epoch 9488/30000 Training Loss: 0.04784965515136719\n",
      "Epoch 9489/30000 Training Loss: 0.04580938443541527\n",
      "Epoch 9490/30000 Training Loss: 0.0434073880314827\n",
      "Epoch 9491/30000 Training Loss: 0.051569536328315735\n",
      "Epoch 9492/30000 Training Loss: 0.056544482707977295\n",
      "Epoch 9493/30000 Training Loss: 0.04524753987789154\n",
      "Epoch 9494/30000 Training Loss: 0.05416882038116455\n",
      "Epoch 9495/30000 Training Loss: 0.05406756326556206\n",
      "Epoch 9496/30000 Training Loss: 0.06374512612819672\n",
      "Epoch 9497/30000 Training Loss: 0.055788978934288025\n",
      "Epoch 9498/30000 Training Loss: 0.053613945841789246\n",
      "Epoch 9499/30000 Training Loss: 0.045352473855018616\n",
      "Epoch 9500/30000 Training Loss: 0.06351783126592636\n",
      "Epoch 9500/30000 Validation Loss: 0.042160406708717346\n",
      "Epoch 9501/30000 Training Loss: 0.06764273345470428\n",
      "Epoch 9502/30000 Training Loss: 0.0437089242041111\n",
      "Epoch 9503/30000 Training Loss: 0.05509864538908005\n",
      "Epoch 9504/30000 Training Loss: 0.04984091967344284\n",
      "Epoch 9505/30000 Training Loss: 0.04771390184760094\n",
      "Epoch 9506/30000 Training Loss: 0.042923275381326675\n",
      "Epoch 9507/30000 Training Loss: 0.05036813020706177\n",
      "Epoch 9508/30000 Training Loss: 0.03959563374519348\n",
      "Epoch 9509/30000 Training Loss: 0.05178350955247879\n",
      "Epoch 9510/30000 Training Loss: 0.04074181616306305\n",
      "Epoch 9511/30000 Training Loss: 0.04277661070227623\n",
      "Epoch 9512/30000 Training Loss: 0.04538217559456825\n",
      "Epoch 9513/30000 Training Loss: 0.045871514827013016\n",
      "Epoch 9514/30000 Training Loss: 0.043591104447841644\n",
      "Epoch 9515/30000 Training Loss: 0.04301761835813522\n",
      "Epoch 9516/30000 Training Loss: 0.048908527940511703\n",
      "Epoch 9517/30000 Training Loss: 0.051888756453990936\n",
      "Epoch 9518/30000 Training Loss: 0.04852178320288658\n",
      "Epoch 9519/30000 Training Loss: 0.05904404819011688\n",
      "Epoch 9520/30000 Training Loss: 0.0433320477604866\n",
      "Epoch 9521/30000 Training Loss: 0.045029520988464355\n",
      "Epoch 9522/30000 Training Loss: 0.03766297549009323\n",
      "Epoch 9523/30000 Training Loss: 0.049878641963005066\n",
      "Epoch 9524/30000 Training Loss: 0.04521855339407921\n",
      "Epoch 9525/30000 Training Loss: 0.049178048968315125\n",
      "Epoch 9526/30000 Training Loss: 0.06808146089315414\n",
      "Epoch 9527/30000 Training Loss: 0.04942899942398071\n",
      "Epoch 9528/30000 Training Loss: 0.040412042289972305\n",
      "Epoch 9529/30000 Training Loss: 0.042950838804244995\n",
      "Epoch 9530/30000 Training Loss: 0.0557984933257103\n",
      "Epoch 9531/30000 Training Loss: 0.04580587521195412\n",
      "Epoch 9532/30000 Training Loss: 0.06382151693105698\n",
      "Epoch 9533/30000 Training Loss: 0.06003984063863754\n",
      "Epoch 9534/30000 Training Loss: 0.060769129544496536\n",
      "Epoch 9535/30000 Training Loss: 0.04951934516429901\n",
      "Epoch 9536/30000 Training Loss: 0.04830893129110336\n",
      "Epoch 9537/30000 Training Loss: 0.05452026054263115\n",
      "Epoch 9538/30000 Training Loss: 0.056032367050647736\n",
      "Epoch 9539/30000 Training Loss: 0.05142214894294739\n",
      "Epoch 9540/30000 Training Loss: 0.059767767786979675\n",
      "Epoch 9541/30000 Training Loss: 0.054840657860040665\n",
      "Epoch 9542/30000 Training Loss: 0.048041194677352905\n",
      "Epoch 9543/30000 Training Loss: 0.04449532926082611\n",
      "Epoch 9544/30000 Training Loss: 0.03751426190137863\n",
      "Epoch 9545/30000 Training Loss: 0.04875355213880539\n",
      "Epoch 9546/30000 Training Loss: 0.05007447674870491\n",
      "Epoch 9547/30000 Training Loss: 0.046486809849739075\n",
      "Epoch 9548/30000 Training Loss: 0.04087824746966362\n",
      "Epoch 9549/30000 Training Loss: 0.06568855047225952\n",
      "Epoch 9550/30000 Training Loss: 0.05033460631966591\n",
      "Epoch 9551/30000 Training Loss: 0.058472052216529846\n",
      "Epoch 9552/30000 Training Loss: 0.043881066143512726\n",
      "Epoch 9553/30000 Training Loss: 0.047619275748729706\n",
      "Epoch 9554/30000 Training Loss: 0.05070381611585617\n",
      "Epoch 9555/30000 Training Loss: 0.03835997357964516\n",
      "Epoch 9556/30000 Training Loss: 0.05415184050798416\n",
      "Epoch 9557/30000 Training Loss: 0.05416068062186241\n",
      "Epoch 9558/30000 Training Loss: 0.047483891248703\n",
      "Epoch 9559/30000 Training Loss: 0.07082106173038483\n",
      "Epoch 9560/30000 Training Loss: 0.05059782788157463\n",
      "Epoch 9561/30000 Training Loss: 0.06904931366443634\n",
      "Epoch 9562/30000 Training Loss: 0.07101476192474365\n",
      "Epoch 9563/30000 Training Loss: 0.06703148782253265\n",
      "Epoch 9564/30000 Training Loss: 0.05166451260447502\n",
      "Epoch 9565/30000 Training Loss: 0.03855501487851143\n",
      "Epoch 9566/30000 Training Loss: 0.038672883063554764\n",
      "Epoch 9567/30000 Training Loss: 0.04171019047498703\n",
      "Epoch 9568/30000 Training Loss: 0.040184229612350464\n",
      "Epoch 9569/30000 Training Loss: 0.04280693829059601\n",
      "Epoch 9570/30000 Training Loss: 0.04094058275222778\n",
      "Epoch 9571/30000 Training Loss: 0.04009619355201721\n",
      "Epoch 9572/30000 Training Loss: 0.04526405781507492\n",
      "Epoch 9573/30000 Training Loss: 0.04396164417266846\n",
      "Epoch 9574/30000 Training Loss: 0.04552377760410309\n",
      "Epoch 9575/30000 Training Loss: 0.05417391657829285\n",
      "Epoch 9576/30000 Training Loss: 0.041555121541023254\n",
      "Epoch 9577/30000 Training Loss: 0.05259399861097336\n",
      "Epoch 9578/30000 Training Loss: 0.05567669868469238\n",
      "Epoch 9579/30000 Training Loss: 0.05032746493816376\n",
      "Epoch 9580/30000 Training Loss: 0.05145431309938431\n",
      "Epoch 9581/30000 Training Loss: 0.04107992723584175\n",
      "Epoch 9582/30000 Training Loss: 0.054902032017707825\n",
      "Epoch 9583/30000 Training Loss: 0.05230860784649849\n",
      "Epoch 9584/30000 Training Loss: 0.044327400624752045\n",
      "Epoch 9585/30000 Training Loss: 0.04876495525240898\n",
      "Epoch 9586/30000 Training Loss: 0.047892116010189056\n",
      "Epoch 9587/30000 Training Loss: 0.04860951006412506\n",
      "Epoch 9588/30000 Training Loss: 0.04840685427188873\n",
      "Epoch 9589/30000 Training Loss: 0.03786648064851761\n",
      "Epoch 9590/30000 Training Loss: 0.054370854049921036\n",
      "Epoch 9591/30000 Training Loss: 0.061898257583379745\n",
      "Epoch 9592/30000 Training Loss: 0.05531129240989685\n",
      "Epoch 9593/30000 Training Loss: 0.0497213676571846\n",
      "Epoch 9594/30000 Training Loss: 0.05998113006353378\n",
      "Epoch 9595/30000 Training Loss: 0.05746298283338547\n",
      "Epoch 9596/30000 Training Loss: 0.0501178503036499\n",
      "Epoch 9597/30000 Training Loss: 0.04344671219587326\n",
      "Epoch 9598/30000 Training Loss: 0.05006527900695801\n",
      "Epoch 9599/30000 Training Loss: 0.05247209966182709\n",
      "Epoch 9600/30000 Training Loss: 0.04586564749479294\n",
      "Epoch 9600/30000 Validation Loss: 0.060236990451812744\n",
      "Epoch 9601/30000 Training Loss: 0.06503281742334366\n",
      "Epoch 9602/30000 Training Loss: 0.05801566317677498\n",
      "Epoch 9603/30000 Training Loss: 0.060249216854572296\n",
      "Epoch 9604/30000 Training Loss: 0.0492129810154438\n",
      "Epoch 9605/30000 Training Loss: 0.04749235510826111\n",
      "Epoch 9606/30000 Training Loss: 0.04640740156173706\n",
      "Epoch 9607/30000 Training Loss: 0.048749249428510666\n",
      "Epoch 9608/30000 Training Loss: 0.04247237369418144\n",
      "Epoch 9609/30000 Training Loss: 0.04011855646967888\n",
      "Epoch 9610/30000 Training Loss: 0.04927241802215576\n",
      "Epoch 9611/30000 Training Loss: 0.048815105110406876\n",
      "Epoch 9612/30000 Training Loss: 0.038337286561727524\n",
      "Epoch 9613/30000 Training Loss: 0.044642817229032516\n",
      "Epoch 9614/30000 Training Loss: 0.03941601514816284\n",
      "Epoch 9615/30000 Training Loss: 0.07335321605205536\n",
      "Epoch 9616/30000 Training Loss: 0.055029548704624176\n",
      "Epoch 9617/30000 Training Loss: 0.04505392536520958\n",
      "Epoch 9618/30000 Training Loss: 0.04931309446692467\n",
      "Epoch 9619/30000 Training Loss: 0.05769190192222595\n",
      "Epoch 9620/30000 Training Loss: 0.053657740354537964\n",
      "Epoch 9621/30000 Training Loss: 0.039937544614076614\n",
      "Epoch 9622/30000 Training Loss: 0.056904226541519165\n",
      "Epoch 9623/30000 Training Loss: 0.04755808785557747\n",
      "Epoch 9624/30000 Training Loss: 0.04029406979680061\n",
      "Epoch 9625/30000 Training Loss: 0.051329463720321655\n",
      "Epoch 9626/30000 Training Loss: 0.05008535832166672\n",
      "Epoch 9627/30000 Training Loss: 0.048063233494758606\n",
      "Epoch 9628/30000 Training Loss: 0.05107714980840683\n",
      "Epoch 9629/30000 Training Loss: 0.04408133402466774\n",
      "Epoch 9630/30000 Training Loss: 0.0391659289598465\n",
      "Epoch 9631/30000 Training Loss: 0.04784034565091133\n",
      "Epoch 9632/30000 Training Loss: 0.04672931134700775\n",
      "Epoch 9633/30000 Training Loss: 0.04425667226314545\n",
      "Epoch 9634/30000 Training Loss: 0.05642978101968765\n",
      "Epoch 9635/30000 Training Loss: 0.05227375775575638\n",
      "Epoch 9636/30000 Training Loss: 0.05620661377906799\n",
      "Epoch 9637/30000 Training Loss: 0.05349918454885483\n",
      "Epoch 9638/30000 Training Loss: 0.04633273556828499\n",
      "Epoch 9639/30000 Training Loss: 0.052606694400310516\n",
      "Epoch 9640/30000 Training Loss: 0.05024569481611252\n",
      "Epoch 9641/30000 Training Loss: 0.045533496886491776\n",
      "Epoch 9642/30000 Training Loss: 0.052017536014318466\n",
      "Epoch 9643/30000 Training Loss: 0.0597475990653038\n",
      "Epoch 9644/30000 Training Loss: 0.059919774532318115\n",
      "Epoch 9645/30000 Training Loss: 0.055647045373916626\n",
      "Epoch 9646/30000 Training Loss: 0.05874137207865715\n",
      "Epoch 9647/30000 Training Loss: 0.047728583216667175\n",
      "Epoch 9648/30000 Training Loss: 0.03617168962955475\n",
      "Epoch 9649/30000 Training Loss: 0.05873747169971466\n",
      "Epoch 9650/30000 Training Loss: 0.04921703785657883\n",
      "Epoch 9651/30000 Training Loss: 0.05642819404602051\n",
      "Epoch 9652/30000 Training Loss: 0.05047871172428131\n",
      "Epoch 9653/30000 Training Loss: 0.03950026258826256\n",
      "Epoch 9654/30000 Training Loss: 0.04175224155187607\n",
      "Epoch 9655/30000 Training Loss: 0.04825785756111145\n",
      "Epoch 9656/30000 Training Loss: 0.04705050587654114\n",
      "Epoch 9657/30000 Training Loss: 0.04560571163892746\n",
      "Epoch 9658/30000 Training Loss: 0.05074843019247055\n",
      "Epoch 9659/30000 Training Loss: 0.04050188511610031\n",
      "Epoch 9660/30000 Training Loss: 0.05403582751750946\n",
      "Epoch 9661/30000 Training Loss: 0.045189741998910904\n",
      "Epoch 9662/30000 Training Loss: 0.06178837642073631\n",
      "Epoch 9663/30000 Training Loss: 0.04969833791255951\n",
      "Epoch 9664/30000 Training Loss: 0.04428209364414215\n",
      "Epoch 9665/30000 Training Loss: 0.04422377794981003\n",
      "Epoch 9666/30000 Training Loss: 0.052220419049263\n",
      "Epoch 9667/30000 Training Loss: 0.04912073165178299\n",
      "Epoch 9668/30000 Training Loss: 0.05091128498315811\n",
      "Epoch 9669/30000 Training Loss: 0.04595061019062996\n",
      "Epoch 9670/30000 Training Loss: 0.04464763402938843\n",
      "Epoch 9671/30000 Training Loss: 0.07024814933538437\n",
      "Epoch 9672/30000 Training Loss: 0.04893165081739426\n",
      "Epoch 9673/30000 Training Loss: 0.040789566934108734\n",
      "Epoch 9674/30000 Training Loss: 0.04297930747270584\n",
      "Epoch 9675/30000 Training Loss: 0.040420014411211014\n",
      "Epoch 9676/30000 Training Loss: 0.05964355170726776\n",
      "Epoch 9677/30000 Training Loss: 0.06462045013904572\n",
      "Epoch 9678/30000 Training Loss: 0.05174589157104492\n",
      "Epoch 9679/30000 Training Loss: 0.04180608689785004\n",
      "Epoch 9680/30000 Training Loss: 0.06175753474235535\n",
      "Epoch 9681/30000 Training Loss: 0.061417728662490845\n",
      "Epoch 9682/30000 Training Loss: 0.04838469997048378\n",
      "Epoch 9683/30000 Training Loss: 0.04405711591243744\n",
      "Epoch 9684/30000 Training Loss: 0.03982851654291153\n",
      "Epoch 9685/30000 Training Loss: 0.04671793431043625\n",
      "Epoch 9686/30000 Training Loss: 0.04814435914158821\n",
      "Epoch 9687/30000 Training Loss: 0.04640037193894386\n",
      "Epoch 9688/30000 Training Loss: 0.03904931619763374\n",
      "Epoch 9689/30000 Training Loss: 0.047224484384059906\n",
      "Epoch 9690/30000 Training Loss: 0.0409705676138401\n",
      "Epoch 9691/30000 Training Loss: 0.0652616024017334\n",
      "Epoch 9692/30000 Training Loss: 0.05373387783765793\n",
      "Epoch 9693/30000 Training Loss: 0.052435122430324554\n",
      "Epoch 9694/30000 Training Loss: 0.057967886328697205\n",
      "Epoch 9695/30000 Training Loss: 0.044845350086688995\n",
      "Epoch 9696/30000 Training Loss: 0.05292166396975517\n",
      "Epoch 9697/30000 Training Loss: 0.04117274284362793\n",
      "Epoch 9698/30000 Training Loss: 0.04018240049481392\n",
      "Epoch 9699/30000 Training Loss: 0.04838654771447182\n",
      "Epoch 9700/30000 Training Loss: 0.038481660187244415\n",
      "Epoch 9700/30000 Validation Loss: 0.06199638918042183\n",
      "Epoch 9701/30000 Training Loss: 0.07119870185852051\n",
      "Epoch 9702/30000 Training Loss: 0.05366586893796921\n",
      "Epoch 9703/30000 Training Loss: 0.04530942067503929\n",
      "Epoch 9704/30000 Training Loss: 0.05548004060983658\n",
      "Epoch 9705/30000 Training Loss: 0.043504197150468826\n",
      "Epoch 9706/30000 Training Loss: 0.04125497490167618\n",
      "Epoch 9707/30000 Training Loss: 0.04928191378712654\n",
      "Epoch 9708/30000 Training Loss: 0.04258871078491211\n",
      "Epoch 9709/30000 Training Loss: 0.04068666696548462\n",
      "Epoch 9710/30000 Training Loss: 0.05864839255809784\n",
      "Epoch 9711/30000 Training Loss: 0.04820500686764717\n",
      "Epoch 9712/30000 Training Loss: 0.05644530430436134\n",
      "Epoch 9713/30000 Training Loss: 0.04832974448800087\n",
      "Epoch 9714/30000 Training Loss: 0.05240295082330704\n",
      "Epoch 9715/30000 Training Loss: 0.050988901406526566\n",
      "Epoch 9716/30000 Training Loss: 0.04725326597690582\n",
      "Epoch 9717/30000 Training Loss: 0.0373556911945343\n",
      "Epoch 9718/30000 Training Loss: 0.05324007198214531\n",
      "Epoch 9719/30000 Training Loss: 0.05374462902545929\n",
      "Epoch 9720/30000 Training Loss: 0.055411115288734436\n",
      "Epoch 9721/30000 Training Loss: 0.045003149658441544\n",
      "Epoch 9722/30000 Training Loss: 0.03888419270515442\n",
      "Epoch 9723/30000 Training Loss: 0.051987238228321075\n",
      "Epoch 9724/30000 Training Loss: 0.055514104664325714\n",
      "Epoch 9725/30000 Training Loss: 0.04643376171588898\n",
      "Epoch 9726/30000 Training Loss: 0.046481817960739136\n",
      "Epoch 9727/30000 Training Loss: 0.06032745540142059\n",
      "Epoch 9728/30000 Training Loss: 0.07035638391971588\n",
      "Epoch 9729/30000 Training Loss: 0.052615225315093994\n",
      "Epoch 9730/30000 Training Loss: 0.04156140610575676\n",
      "Epoch 9731/30000 Training Loss: 0.043096669018268585\n",
      "Epoch 9732/30000 Training Loss: 0.05992253124713898\n",
      "Epoch 9733/30000 Training Loss: 0.059170424938201904\n",
      "Epoch 9734/30000 Training Loss: 0.05453236401081085\n",
      "Epoch 9735/30000 Training Loss: 0.058778323233127594\n",
      "Epoch 9736/30000 Training Loss: 0.0354655385017395\n",
      "Epoch 9737/30000 Training Loss: 0.05037444084882736\n",
      "Epoch 9738/30000 Training Loss: 0.055318381637334824\n",
      "Epoch 9739/30000 Training Loss: 0.061485469341278076\n",
      "Epoch 9740/30000 Training Loss: 0.05275736749172211\n",
      "Epoch 9741/30000 Training Loss: 0.0474785715341568\n",
      "Epoch 9742/30000 Training Loss: 0.04636278748512268\n",
      "Epoch 9743/30000 Training Loss: 0.04299377650022507\n",
      "Epoch 9744/30000 Training Loss: 0.05273640900850296\n",
      "Epoch 9745/30000 Training Loss: 0.0503179132938385\n",
      "Epoch 9746/30000 Training Loss: 0.055807314813137054\n",
      "Epoch 9747/30000 Training Loss: 0.05705505609512329\n",
      "Epoch 9748/30000 Training Loss: 0.04633980244398117\n",
      "Epoch 9749/30000 Training Loss: 0.04846944659948349\n",
      "Epoch 9750/30000 Training Loss: 0.0557527095079422\n",
      "Epoch 9751/30000 Training Loss: 0.04689996689558029\n",
      "Epoch 9752/30000 Training Loss: 0.049339957535266876\n",
      "Epoch 9753/30000 Training Loss: 0.051498644053936005\n",
      "Epoch 9754/30000 Training Loss: 0.048749297857284546\n",
      "Epoch 9755/30000 Training Loss: 0.03569412603974342\n",
      "Epoch 9756/30000 Training Loss: 0.052160028368234634\n",
      "Epoch 9757/30000 Training Loss: 0.05339892953634262\n",
      "Epoch 9758/30000 Training Loss: 0.05117902159690857\n",
      "Epoch 9759/30000 Training Loss: 0.05210862308740616\n",
      "Epoch 9760/30000 Training Loss: 0.04177669435739517\n",
      "Epoch 9761/30000 Training Loss: 0.044500552117824554\n",
      "Epoch 9762/30000 Training Loss: 0.04005845636129379\n",
      "Epoch 9763/30000 Training Loss: 0.06501896679401398\n",
      "Epoch 9764/30000 Training Loss: 0.05782829225063324\n",
      "Epoch 9765/30000 Training Loss: 0.046752989292144775\n",
      "Epoch 9766/30000 Training Loss: 0.059231847524642944\n",
      "Epoch 9767/30000 Training Loss: 0.049691639840602875\n",
      "Epoch 9768/30000 Training Loss: 0.04456310719251633\n",
      "Epoch 9769/30000 Training Loss: 0.058981627225875854\n",
      "Epoch 9770/30000 Training Loss: 0.0371304452419281\n",
      "Epoch 9771/30000 Training Loss: 0.05316902697086334\n",
      "Epoch 9772/30000 Training Loss: 0.04330138862133026\n",
      "Epoch 9773/30000 Training Loss: 0.052521444857120514\n",
      "Epoch 9774/30000 Training Loss: 0.041263844817876816\n",
      "Epoch 9775/30000 Training Loss: 0.051110874861478806\n",
      "Epoch 9776/30000 Training Loss: 0.06210034340620041\n",
      "Epoch 9777/30000 Training Loss: 0.06391175091266632\n",
      "Epoch 9778/30000 Training Loss: 0.062396496534347534\n",
      "Epoch 9779/30000 Training Loss: 0.053049493581056595\n",
      "Epoch 9780/30000 Training Loss: 0.047257669270038605\n",
      "Epoch 9781/30000 Training Loss: 0.04743950068950653\n",
      "Epoch 9782/30000 Training Loss: 0.05398586392402649\n",
      "Epoch 9783/30000 Training Loss: 0.05224839597940445\n",
      "Epoch 9784/30000 Training Loss: 0.04151636362075806\n",
      "Epoch 9785/30000 Training Loss: 0.05594705045223236\n",
      "Epoch 9786/30000 Training Loss: 0.04870960861444473\n",
      "Epoch 9787/30000 Training Loss: 0.05001622438430786\n",
      "Epoch 9788/30000 Training Loss: 0.04465729743242264\n",
      "Epoch 9789/30000 Training Loss: 0.05261559784412384\n",
      "Epoch 9790/30000 Training Loss: 0.04373328387737274\n",
      "Epoch 9791/30000 Training Loss: 0.04083070531487465\n",
      "Epoch 9792/30000 Training Loss: 0.050505708903074265\n",
      "Epoch 9793/30000 Training Loss: 0.05542168393731117\n",
      "Epoch 9794/30000 Training Loss: 0.05366470292210579\n",
      "Epoch 9795/30000 Training Loss: 0.04581238701939583\n",
      "Epoch 9796/30000 Training Loss: 0.05630914866924286\n",
      "Epoch 9797/30000 Training Loss: 0.05045432597398758\n",
      "Epoch 9798/30000 Training Loss: 0.04783719778060913\n",
      "Epoch 9799/30000 Training Loss: 0.04055119305849075\n",
      "Epoch 9800/30000 Training Loss: 0.04161766916513443\n",
      "Epoch 9800/30000 Validation Loss: 0.06376713514328003\n",
      "Epoch 9801/30000 Training Loss: 0.043380007147789\n",
      "Epoch 9802/30000 Training Loss: 0.05542081594467163\n",
      "Epoch 9803/30000 Training Loss: 0.05250811576843262\n",
      "Epoch 9804/30000 Training Loss: 0.037498001009225845\n",
      "Epoch 9805/30000 Training Loss: 0.05239231139421463\n",
      "Epoch 9806/30000 Training Loss: 0.03591224551200867\n",
      "Epoch 9807/30000 Training Loss: 0.041451361030340195\n",
      "Epoch 9808/30000 Training Loss: 0.03691340237855911\n",
      "Epoch 9809/30000 Training Loss: 0.04830747842788696\n",
      "Epoch 9810/30000 Training Loss: 0.04325146973133087\n",
      "Epoch 9811/30000 Training Loss: 0.043831244111061096\n",
      "Epoch 9812/30000 Training Loss: 0.034666016697883606\n",
      "Epoch 9813/30000 Training Loss: 0.05298248678445816\n",
      "Epoch 9814/30000 Training Loss: 0.04141765087842941\n",
      "Epoch 9815/30000 Training Loss: 0.058443326503038406\n",
      "Epoch 9816/30000 Training Loss: 0.04058673232793808\n",
      "Epoch 9817/30000 Training Loss: 0.04968404024839401\n",
      "Epoch 9818/30000 Training Loss: 0.04569145664572716\n",
      "Epoch 9819/30000 Training Loss: 0.057107970118522644\n",
      "Epoch 9820/30000 Training Loss: 0.062078773975372314\n",
      "Epoch 9821/30000 Training Loss: 0.04717074707150459\n",
      "Epoch 9822/30000 Training Loss: 0.06053660064935684\n",
      "Epoch 9823/30000 Training Loss: 0.042020589113235474\n",
      "Epoch 9824/30000 Training Loss: 0.04406130313873291\n",
      "Epoch 9825/30000 Training Loss: 0.04317222163081169\n",
      "Epoch 9826/30000 Training Loss: 0.05290215462446213\n",
      "Epoch 9827/30000 Training Loss: 0.051190100610256195\n",
      "Epoch 9828/30000 Training Loss: 0.0656002014875412\n",
      "Epoch 9829/30000 Training Loss: 0.04842185229063034\n",
      "Epoch 9830/30000 Training Loss: 0.044978946447372437\n",
      "Epoch 9831/30000 Training Loss: 0.04676876217126846\n",
      "Epoch 9832/30000 Training Loss: 0.05392584204673767\n",
      "Epoch 9833/30000 Training Loss: 0.06044936925172806\n",
      "Epoch 9834/30000 Training Loss: 0.047204844653606415\n",
      "Epoch 9835/30000 Training Loss: 0.047707170248031616\n",
      "Epoch 9836/30000 Training Loss: 0.057878270745277405\n",
      "Epoch 9837/30000 Training Loss: 0.04699838161468506\n",
      "Epoch 9838/30000 Training Loss: 0.036624304950237274\n",
      "Epoch 9839/30000 Training Loss: 0.038751401007175446\n",
      "Epoch 9840/30000 Training Loss: 0.04663049057126045\n",
      "Epoch 9841/30000 Training Loss: 0.05907569080591202\n",
      "Epoch 9842/30000 Training Loss: 0.03879588097333908\n",
      "Epoch 9843/30000 Training Loss: 0.04445015639066696\n",
      "Epoch 9844/30000 Training Loss: 0.05788131803274155\n",
      "Epoch 9845/30000 Training Loss: 0.050383780151605606\n",
      "Epoch 9846/30000 Training Loss: 0.06350485235452652\n",
      "Epoch 9847/30000 Training Loss: 0.04352884367108345\n",
      "Epoch 9848/30000 Training Loss: 0.04537756368517876\n",
      "Epoch 9849/30000 Training Loss: 0.059614911675453186\n",
      "Epoch 9850/30000 Training Loss: 0.05391820892691612\n",
      "Epoch 9851/30000 Training Loss: 0.059212587773799896\n",
      "Epoch 9852/30000 Training Loss: 0.053489573299884796\n",
      "Epoch 9853/30000 Training Loss: 0.048525191843509674\n",
      "Epoch 9854/30000 Training Loss: 0.04936828091740608\n",
      "Epoch 9855/30000 Training Loss: 0.05335870385169983\n",
      "Epoch 9856/30000 Training Loss: 0.04115764796733856\n",
      "Epoch 9857/30000 Training Loss: 0.05140427500009537\n",
      "Epoch 9858/30000 Training Loss: 0.046778395771980286\n",
      "Epoch 9859/30000 Training Loss: 0.05597199499607086\n",
      "Epoch 9860/30000 Training Loss: 0.03538830578327179\n",
      "Epoch 9861/30000 Training Loss: 0.053301289677619934\n",
      "Epoch 9862/30000 Training Loss: 0.05577775463461876\n",
      "Epoch 9863/30000 Training Loss: 0.056297749280929565\n",
      "Epoch 9864/30000 Training Loss: 0.06857762485742569\n",
      "Epoch 9865/30000 Training Loss: 0.06733019649982452\n",
      "Epoch 9866/30000 Training Loss: 0.03972721844911575\n",
      "Epoch 9867/30000 Training Loss: 0.03909502550959587\n",
      "Epoch 9868/30000 Training Loss: 0.04904741793870926\n",
      "Epoch 9869/30000 Training Loss: 0.04506004601716995\n",
      "Epoch 9870/30000 Training Loss: 0.04927081987261772\n",
      "Epoch 9871/30000 Training Loss: 0.05983545631170273\n",
      "Epoch 9872/30000 Training Loss: 0.04594872146844864\n",
      "Epoch 9873/30000 Training Loss: 0.05168187618255615\n",
      "Epoch 9874/30000 Training Loss: 0.04828415811061859\n",
      "Epoch 9875/30000 Training Loss: 0.04644976928830147\n",
      "Epoch 9876/30000 Training Loss: 0.03983107954263687\n",
      "Epoch 9877/30000 Training Loss: 0.05179006606340408\n",
      "Epoch 9878/30000 Training Loss: 0.04331974685192108\n",
      "Epoch 9879/30000 Training Loss: 0.05956009775400162\n",
      "Epoch 9880/30000 Training Loss: 0.04875803738832474\n",
      "Epoch 9881/30000 Training Loss: 0.04791375249624252\n",
      "Epoch 9882/30000 Training Loss: 0.03825792670249939\n",
      "Epoch 9883/30000 Training Loss: 0.03972801938652992\n",
      "Epoch 9884/30000 Training Loss: 0.045863665640354156\n",
      "Epoch 9885/30000 Training Loss: 0.04696878790855408\n",
      "Epoch 9886/30000 Training Loss: 0.04996955394744873\n",
      "Epoch 9887/30000 Training Loss: 0.05272112786769867\n",
      "Epoch 9888/30000 Training Loss: 0.049933500587940216\n",
      "Epoch 9889/30000 Training Loss: 0.03713501617312431\n",
      "Epoch 9890/30000 Training Loss: 0.04702487960457802\n",
      "Epoch 9891/30000 Training Loss: 0.05061502009630203\n",
      "Epoch 9892/30000 Training Loss: 0.047913458198308945\n",
      "Epoch 9893/30000 Training Loss: 0.040996719151735306\n",
      "Epoch 9894/30000 Training Loss: 0.049733344465494156\n",
      "Epoch 9895/30000 Training Loss: 0.06203566491603851\n",
      "Epoch 9896/30000 Training Loss: 0.06158214062452316\n",
      "Epoch 9897/30000 Training Loss: 0.05572132766246796\n",
      "Epoch 9898/30000 Training Loss: 0.0574314221739769\n",
      "Epoch 9899/30000 Training Loss: 0.07006306201219559\n",
      "Epoch 9900/30000 Training Loss: 0.05225124955177307\n",
      "Epoch 9900/30000 Validation Loss: 0.0481315478682518\n",
      "Epoch 9901/30000 Training Loss: 0.038895383477211\n",
      "Epoch 9902/30000 Training Loss: 0.05009739100933075\n",
      "Epoch 9903/30000 Training Loss: 0.05186806619167328\n",
      "Epoch 9904/30000 Training Loss: 0.04590848460793495\n",
      "Epoch 9905/30000 Training Loss: 0.0430445559322834\n",
      "Epoch 9906/30000 Training Loss: 0.053966209292411804\n",
      "Epoch 9907/30000 Training Loss: 0.06459701061248779\n",
      "Epoch 9908/30000 Training Loss: 0.06198924779891968\n",
      "Epoch 9909/30000 Training Loss: 0.0445486381649971\n",
      "Epoch 9910/30000 Training Loss: 0.03401627391576767\n",
      "Epoch 9911/30000 Training Loss: 0.04545368254184723\n",
      "Epoch 9912/30000 Training Loss: 0.05696671083569527\n",
      "Epoch 9913/30000 Training Loss: 0.05639658868312836\n",
      "Epoch 9914/30000 Training Loss: 0.03203269839286804\n",
      "Epoch 9915/30000 Training Loss: 0.05414274334907532\n",
      "Epoch 9916/30000 Training Loss: 0.052801962941884995\n",
      "Epoch 9917/30000 Training Loss: 0.04644003137946129\n",
      "Epoch 9918/30000 Training Loss: 0.053346309810876846\n",
      "Epoch 9919/30000 Training Loss: 0.04428866505622864\n",
      "Epoch 9920/30000 Training Loss: 0.05051472783088684\n",
      "Epoch 9921/30000 Training Loss: 0.04500149190425873\n",
      "Epoch 9922/30000 Training Loss: 0.06558327376842499\n",
      "Epoch 9923/30000 Training Loss: 0.06928720325231552\n",
      "Epoch 9924/30000 Training Loss: 0.0744447186589241\n",
      "Epoch 9925/30000 Training Loss: 0.04536867141723633\n",
      "Epoch 9926/30000 Training Loss: 0.05213530361652374\n",
      "Epoch 9927/30000 Training Loss: 0.03940258175134659\n",
      "Epoch 9928/30000 Training Loss: 0.045451074838638306\n",
      "Epoch 9929/30000 Training Loss: 0.0444367453455925\n",
      "Epoch 9930/30000 Training Loss: 0.045563310384750366\n",
      "Epoch 9931/30000 Training Loss: 0.05031699314713478\n",
      "Epoch 9932/30000 Training Loss: 0.06338947266340256\n",
      "Epoch 9933/30000 Training Loss: 0.03785645216703415\n",
      "Epoch 9934/30000 Training Loss: 0.04628375172615051\n",
      "Epoch 9935/30000 Training Loss: 0.06134169176220894\n",
      "Epoch 9936/30000 Training Loss: 0.05352093651890755\n",
      "Epoch 9937/30000 Training Loss: 0.05540016293525696\n",
      "Epoch 9938/30000 Training Loss: 0.056748613715171814\n",
      "Epoch 9939/30000 Training Loss: 0.04514077305793762\n",
      "Epoch 9940/30000 Training Loss: 0.050377413630485535\n",
      "Epoch 9941/30000 Training Loss: 0.05003850907087326\n",
      "Epoch 9942/30000 Training Loss: 0.04915022850036621\n",
      "Epoch 9943/30000 Training Loss: 0.041174277663230896\n",
      "Epoch 9944/30000 Training Loss: 0.054088518023490906\n",
      "Epoch 9945/30000 Training Loss: 0.05138804763555527\n",
      "Epoch 9946/30000 Training Loss: 0.04612862318754196\n",
      "Epoch 9947/30000 Training Loss: 0.06825296580791473\n",
      "Epoch 9948/30000 Training Loss: 0.04495029151439667\n",
      "Epoch 9949/30000 Training Loss: 0.041575171053409576\n",
      "Epoch 9950/30000 Training Loss: 0.048796337097883224\n",
      "Epoch 9951/30000 Training Loss: 0.042202919721603394\n",
      "Epoch 9952/30000 Training Loss: 0.04269475117325783\n",
      "Epoch 9953/30000 Training Loss: 0.05962303653359413\n",
      "Epoch 9954/30000 Training Loss: 0.05319029092788696\n",
      "Epoch 9955/30000 Training Loss: 0.03940127044916153\n",
      "Epoch 9956/30000 Training Loss: 0.055229656398296356\n",
      "Epoch 9957/30000 Training Loss: 0.047857850790023804\n",
      "Epoch 9958/30000 Training Loss: 0.030305396765470505\n",
      "Epoch 9959/30000 Training Loss: 0.04965759068727493\n",
      "Epoch 9960/30000 Training Loss: 0.04365266114473343\n",
      "Epoch 9961/30000 Training Loss: 0.051863960921764374\n",
      "Epoch 9962/30000 Training Loss: 0.049457091838121414\n",
      "Epoch 9963/30000 Training Loss: 0.04066035524010658\n",
      "Epoch 9964/30000 Training Loss: 0.044964421540498734\n",
      "Epoch 9965/30000 Training Loss: 0.05415666848421097\n",
      "Epoch 9966/30000 Training Loss: 0.05708436667919159\n",
      "Epoch 9967/30000 Training Loss: 0.04516308009624481\n",
      "Epoch 9968/30000 Training Loss: 0.043585360050201416\n",
      "Epoch 9969/30000 Training Loss: 0.05041194707155228\n",
      "Epoch 9970/30000 Training Loss: 0.0370105616748333\n",
      "Epoch 9971/30000 Training Loss: 0.04150870814919472\n",
      "Epoch 9972/30000 Training Loss: 0.05875958874821663\n",
      "Epoch 9973/30000 Training Loss: 0.05430476740002632\n",
      "Epoch 9974/30000 Training Loss: 0.062081143260002136\n",
      "Epoch 9975/30000 Training Loss: 0.053834788501262665\n",
      "Epoch 9976/30000 Training Loss: 0.047036103904247284\n",
      "Epoch 9977/30000 Training Loss: 0.038751907646656036\n",
      "Epoch 9978/30000 Training Loss: 0.04542900621891022\n",
      "Epoch 9979/30000 Training Loss: 0.043769944459199905\n",
      "Epoch 9980/30000 Training Loss: 0.054544441401958466\n",
      "Epoch 9981/30000 Training Loss: 0.056089166551828384\n",
      "Epoch 9982/30000 Training Loss: 0.042097918689250946\n",
      "Epoch 9983/30000 Training Loss: 0.03911595791578293\n",
      "Epoch 9984/30000 Training Loss: 0.0394086129963398\n",
      "Epoch 9985/30000 Training Loss: 0.03926055133342743\n",
      "Epoch 9986/30000 Training Loss: 0.05119707062840462\n",
      "Epoch 9987/30000 Training Loss: 0.05407921224832535\n",
      "Epoch 9988/30000 Training Loss: 0.05480668693780899\n",
      "Epoch 9989/30000 Training Loss: 0.04807721823453903\n",
      "Epoch 9990/30000 Training Loss: 0.05973292887210846\n",
      "Epoch 9991/30000 Training Loss: 0.05096230283379555\n",
      "Epoch 9992/30000 Training Loss: 0.03538483381271362\n",
      "Epoch 9993/30000 Training Loss: 0.062062349170446396\n",
      "Epoch 9994/30000 Training Loss: 0.04825112968683243\n",
      "Epoch 9995/30000 Training Loss: 0.040363609790802\n",
      "Epoch 9996/30000 Training Loss: 0.04168146848678589\n",
      "Epoch 9997/30000 Training Loss: 0.050057537853717804\n",
      "Epoch 9998/30000 Training Loss: 0.04690050333738327\n",
      "Epoch 9999/30000 Training Loss: 0.049859173595905304\n",
      "Epoch 10000/30000 Training Loss: 0.05604655295610428\n",
      "Epoch 10000/30000 Validation Loss: 0.04679899662733078\n",
      "Epoch 10001/30000 Training Loss: 0.05383295938372612\n",
      "Epoch 10002/30000 Training Loss: 0.043596021831035614\n",
      "Epoch 10003/30000 Training Loss: 0.04477308318018913\n",
      "Epoch 10004/30000 Training Loss: 0.06655947864055634\n",
      "Epoch 10005/30000 Training Loss: 0.04176654666662216\n",
      "Epoch 10006/30000 Training Loss: 0.04311449080705643\n",
      "Epoch 10007/30000 Training Loss: 0.04644826427102089\n",
      "Epoch 10008/30000 Training Loss: 0.06811175495386124\n",
      "Epoch 10009/30000 Training Loss: 0.06610217690467834\n",
      "Epoch 10010/30000 Training Loss: 0.0660916417837143\n",
      "Epoch 10011/30000 Training Loss: 0.0519014447927475\n",
      "Epoch 10012/30000 Training Loss: 0.054940365254879\n",
      "Epoch 10013/30000 Training Loss: 0.047008976340293884\n",
      "Epoch 10014/30000 Training Loss: 0.04956546425819397\n",
      "Epoch 10015/30000 Training Loss: 0.049066923558712006\n",
      "Epoch 10016/30000 Training Loss: 0.043941907584667206\n",
      "Epoch 10017/30000 Training Loss: 0.04585622251033783\n",
      "Epoch 10018/30000 Training Loss: 0.06017347052693367\n",
      "Epoch 10019/30000 Training Loss: 0.046027980744838715\n",
      "Epoch 10020/30000 Training Loss: 0.05104372277855873\n",
      "Epoch 10021/30000 Training Loss: 0.04223443940281868\n",
      "Epoch 10022/30000 Training Loss: 0.045140720903873444\n",
      "Epoch 10023/30000 Training Loss: 0.044351354241371155\n",
      "Epoch 10024/30000 Training Loss: 0.04086385667324066\n",
      "Epoch 10025/30000 Training Loss: 0.057854264974594116\n",
      "Epoch 10026/30000 Training Loss: 0.04877876117825508\n",
      "Epoch 10027/30000 Training Loss: 0.04260896518826485\n",
      "Epoch 10028/30000 Training Loss: 0.044493354856967926\n",
      "Epoch 10029/30000 Training Loss: 0.04169788584113121\n",
      "Epoch 10030/30000 Training Loss: 0.046980664134025574\n",
      "Epoch 10031/30000 Training Loss: 0.046459950506687164\n",
      "Epoch 10032/30000 Training Loss: 0.06458667665719986\n",
      "Epoch 10033/30000 Training Loss: 0.059428662061691284\n",
      "Epoch 10034/30000 Training Loss: 0.047754619270563126\n",
      "Epoch 10035/30000 Training Loss: 0.05883879214525223\n",
      "Epoch 10036/30000 Training Loss: 0.0595366507768631\n",
      "Epoch 10037/30000 Training Loss: 0.06744180619716644\n",
      "Epoch 10038/30000 Training Loss: 0.048693373799324036\n",
      "Epoch 10039/30000 Training Loss: 0.03304729610681534\n",
      "Epoch 10040/30000 Training Loss: 0.04053116589784622\n",
      "Epoch 10041/30000 Training Loss: 0.05364464223384857\n",
      "Epoch 10042/30000 Training Loss: 0.05036143958568573\n",
      "Epoch 10043/30000 Training Loss: 0.03962456434965134\n",
      "Epoch 10044/30000 Training Loss: 0.0453273206949234\n",
      "Epoch 10045/30000 Training Loss: 0.0435502752661705\n",
      "Epoch 10046/30000 Training Loss: 0.051496945321559906\n",
      "Epoch 10047/30000 Training Loss: 0.04293888062238693\n",
      "Epoch 10048/30000 Training Loss: 0.04923481121659279\n",
      "Epoch 10049/30000 Training Loss: 0.04605531692504883\n",
      "Epoch 10050/30000 Training Loss: 0.03419323265552521\n",
      "Epoch 10051/30000 Training Loss: 0.05000540614128113\n",
      "Epoch 10052/30000 Training Loss: 0.05327276885509491\n",
      "Epoch 10053/30000 Training Loss: 0.04782873019576073\n",
      "Epoch 10054/30000 Training Loss: 0.059490084648132324\n",
      "Epoch 10055/30000 Training Loss: 0.04294988512992859\n",
      "Epoch 10056/30000 Training Loss: 0.03729381412267685\n",
      "Epoch 10057/30000 Training Loss: 0.05229198560118675\n",
      "Epoch 10058/30000 Training Loss: 0.05474972724914551\n",
      "Epoch 10059/30000 Training Loss: 0.04342160001397133\n",
      "Epoch 10060/30000 Training Loss: 0.03861016780138016\n",
      "Epoch 10061/30000 Training Loss: 0.04905509203672409\n",
      "Epoch 10062/30000 Training Loss: 0.05343450605869293\n",
      "Epoch 10063/30000 Training Loss: 0.057194169610738754\n",
      "Epoch 10064/30000 Training Loss: 0.053413715213537216\n",
      "Epoch 10065/30000 Training Loss: 0.0433148555457592\n",
      "Epoch 10066/30000 Training Loss: 0.0567314550280571\n",
      "Epoch 10067/30000 Training Loss: 0.04013963043689728\n",
      "Epoch 10068/30000 Training Loss: 0.03793061152100563\n",
      "Epoch 10069/30000 Training Loss: 0.04884723573923111\n",
      "Epoch 10070/30000 Training Loss: 0.06493215262889862\n",
      "Epoch 10071/30000 Training Loss: 0.05215500295162201\n",
      "Epoch 10072/30000 Training Loss: 0.055991291999816895\n",
      "Epoch 10073/30000 Training Loss: 0.04228610172867775\n",
      "Epoch 10074/30000 Training Loss: 0.04086770862340927\n",
      "Epoch 10075/30000 Training Loss: 0.05348382145166397\n",
      "Epoch 10076/30000 Training Loss: 0.05617343634366989\n",
      "Epoch 10077/30000 Training Loss: 0.06397704035043716\n",
      "Epoch 10078/30000 Training Loss: 0.04572746902704239\n",
      "Epoch 10079/30000 Training Loss: 0.047044072300195694\n",
      "Epoch 10080/30000 Training Loss: 0.036286234855651855\n",
      "Epoch 10081/30000 Training Loss: 0.04972971975803375\n",
      "Epoch 10082/30000 Training Loss: 0.05094224959611893\n",
      "Epoch 10083/30000 Training Loss: 0.04255576804280281\n",
      "Epoch 10084/30000 Training Loss: 0.06550106406211853\n",
      "Epoch 10085/30000 Training Loss: 0.05883936583995819\n",
      "Epoch 10086/30000 Training Loss: 0.045677416026592255\n",
      "Epoch 10087/30000 Training Loss: 0.04882019758224487\n",
      "Epoch 10088/30000 Training Loss: 0.049038439989089966\n",
      "Epoch 10089/30000 Training Loss: 0.05036923661828041\n",
      "Epoch 10090/30000 Training Loss: 0.038182251155376434\n",
      "Epoch 10091/30000 Training Loss: 0.04993833601474762\n",
      "Epoch 10092/30000 Training Loss: 0.05265801399946213\n",
      "Epoch 10093/30000 Training Loss: 0.03901112824678421\n",
      "Epoch 10094/30000 Training Loss: 0.03940804302692413\n",
      "Epoch 10095/30000 Training Loss: 0.046845175325870514\n",
      "Epoch 10096/30000 Training Loss: 0.04817461967468262\n",
      "Epoch 10097/30000 Training Loss: 0.050429243594408035\n",
      "Epoch 10098/30000 Training Loss: 0.058052174746990204\n",
      "Epoch 10099/30000 Training Loss: 0.0546845868229866\n",
      "Epoch 10100/30000 Training Loss: 0.06385565549135208\n",
      "Epoch 10100/30000 Validation Loss: 0.06262460350990295\n",
      "Epoch 10101/30000 Training Loss: 0.045860905200242996\n",
      "Epoch 10102/30000 Training Loss: 0.051178693771362305\n",
      "Epoch 10103/30000 Training Loss: 0.05916254222393036\n",
      "Epoch 10104/30000 Training Loss: 0.054924845695495605\n",
      "Epoch 10105/30000 Training Loss: 0.041067369282245636\n",
      "Epoch 10106/30000 Training Loss: 0.044732727110385895\n",
      "Epoch 10107/30000 Training Loss: 0.03954188525676727\n",
      "Epoch 10108/30000 Training Loss: 0.05510079860687256\n",
      "Epoch 10109/30000 Training Loss: 0.04799646884202957\n",
      "Epoch 10110/30000 Training Loss: 0.04775083065032959\n",
      "Epoch 10111/30000 Training Loss: 0.03379927575588226\n",
      "Epoch 10112/30000 Training Loss: 0.04826391488313675\n",
      "Epoch 10113/30000 Training Loss: 0.04392089322209358\n",
      "Epoch 10114/30000 Training Loss: 0.036490797996520996\n",
      "Epoch 10115/30000 Training Loss: 0.048411644995212555\n",
      "Epoch 10116/30000 Training Loss: 0.06203018128871918\n",
      "Epoch 10117/30000 Training Loss: 0.05245523527264595\n",
      "Epoch 10118/30000 Training Loss: 0.058489829301834106\n",
      "Epoch 10119/30000 Training Loss: 0.04904216527938843\n",
      "Epoch 10120/30000 Training Loss: 0.04476077854633331\n",
      "Epoch 10121/30000 Training Loss: 0.06490737944841385\n",
      "Epoch 10122/30000 Training Loss: 0.04792679846286774\n",
      "Epoch 10123/30000 Training Loss: 0.05425029247999191\n",
      "Epoch 10124/30000 Training Loss: 0.06621235609054565\n",
      "Epoch 10125/30000 Training Loss: 0.05020272731781006\n",
      "Epoch 10126/30000 Training Loss: 0.03863642364740372\n",
      "Epoch 10127/30000 Training Loss: 0.05136575922369957\n",
      "Epoch 10128/30000 Training Loss: 0.04349394142627716\n",
      "Epoch 10129/30000 Training Loss: 0.06373909115791321\n",
      "Epoch 10130/30000 Training Loss: 0.04957908019423485\n",
      "Epoch 10131/30000 Training Loss: 0.04812057316303253\n",
      "Epoch 10132/30000 Training Loss: 0.04239314794540405\n",
      "Epoch 10133/30000 Training Loss: 0.04721087962388992\n",
      "Epoch 10134/30000 Training Loss: 0.04021083563566208\n",
      "Epoch 10135/30000 Training Loss: 0.05298338830471039\n",
      "Epoch 10136/30000 Training Loss: 0.042528145015239716\n",
      "Epoch 10137/30000 Training Loss: 0.05281747132539749\n",
      "Epoch 10138/30000 Training Loss: 0.036051295697689056\n",
      "Epoch 10139/30000 Training Loss: 0.0339752659201622\n",
      "Epoch 10140/30000 Training Loss: 0.050942838191986084\n",
      "Epoch 10141/30000 Training Loss: 0.04708237946033478\n",
      "Epoch 10142/30000 Training Loss: 0.05321089178323746\n",
      "Epoch 10143/30000 Training Loss: 0.036210790276527405\n",
      "Epoch 10144/30000 Training Loss: 0.04141635820269585\n",
      "Epoch 10145/30000 Training Loss: 0.04841921851038933\n",
      "Epoch 10146/30000 Training Loss: 0.04650071635842323\n",
      "Epoch 10147/30000 Training Loss: 0.051475975662469864\n",
      "Epoch 10148/30000 Training Loss: 0.05402233451604843\n",
      "Epoch 10149/30000 Training Loss: 0.04201015830039978\n",
      "Epoch 10150/30000 Training Loss: 0.03207512944936752\n",
      "Epoch 10151/30000 Training Loss: 0.050097301602363586\n",
      "Epoch 10152/30000 Training Loss: 0.06480083614587784\n",
      "Epoch 10153/30000 Training Loss: 0.04479955509305\n",
      "Epoch 10154/30000 Training Loss: 0.04272894933819771\n",
      "Epoch 10155/30000 Training Loss: 0.04154127463698387\n",
      "Epoch 10156/30000 Training Loss: 0.04420464485883713\n",
      "Epoch 10157/30000 Training Loss: 0.04638772830367088\n",
      "Epoch 10158/30000 Training Loss: 0.05926843732595444\n",
      "Epoch 10159/30000 Training Loss: 0.04575422778725624\n",
      "Epoch 10160/30000 Training Loss: 0.04353957995772362\n",
      "Epoch 10161/30000 Training Loss: 0.04324446991086006\n",
      "Epoch 10162/30000 Training Loss: 0.052414391189813614\n",
      "Epoch 10163/30000 Training Loss: 0.04859636723995209\n",
      "Epoch 10164/30000 Training Loss: 0.04162416234612465\n",
      "Epoch 10165/30000 Training Loss: 0.034318387508392334\n",
      "Epoch 10166/30000 Training Loss: 0.053821153938770294\n",
      "Epoch 10167/30000 Training Loss: 0.03841308504343033\n",
      "Epoch 10168/30000 Training Loss: 0.052808977663517\n",
      "Epoch 10169/30000 Training Loss: 0.0482499934732914\n",
      "Epoch 10170/30000 Training Loss: 0.05365053564310074\n",
      "Epoch 10171/30000 Training Loss: 0.042697444558143616\n",
      "Epoch 10172/30000 Training Loss: 0.04367128014564514\n",
      "Epoch 10173/30000 Training Loss: 0.04289139062166214\n",
      "Epoch 10174/30000 Training Loss: 0.05458832532167435\n",
      "Epoch 10175/30000 Training Loss: 0.04739835113286972\n",
      "Epoch 10176/30000 Training Loss: 0.047472789883613586\n",
      "Epoch 10177/30000 Training Loss: 0.05574610084295273\n",
      "Epoch 10178/30000 Training Loss: 0.05713064968585968\n",
      "Epoch 10179/30000 Training Loss: 0.044045589864254\n",
      "Epoch 10180/30000 Training Loss: 0.050245072692632675\n",
      "Epoch 10181/30000 Training Loss: 0.04351980984210968\n",
      "Epoch 10182/30000 Training Loss: 0.06021565943956375\n",
      "Epoch 10183/30000 Training Loss: 0.04580245539546013\n",
      "Epoch 10184/30000 Training Loss: 0.03540394827723503\n",
      "Epoch 10185/30000 Training Loss: 0.0371977835893631\n",
      "Epoch 10186/30000 Training Loss: 0.06179384887218475\n",
      "Epoch 10187/30000 Training Loss: 0.047892726957798004\n",
      "Epoch 10188/30000 Training Loss: 0.05454326048493385\n",
      "Epoch 10189/30000 Training Loss: 0.04974242299795151\n",
      "Epoch 10190/30000 Training Loss: 0.04470396786928177\n",
      "Epoch 10191/30000 Training Loss: 0.05945297330617905\n",
      "Epoch 10192/30000 Training Loss: 0.0502314418554306\n",
      "Epoch 10193/30000 Training Loss: 0.04156508296728134\n",
      "Epoch 10194/30000 Training Loss: 0.049926720559597015\n",
      "Epoch 10195/30000 Training Loss: 0.048589129000902176\n",
      "Epoch 10196/30000 Training Loss: 0.043787792325019836\n",
      "Epoch 10197/30000 Training Loss: 0.04227450489997864\n",
      "Epoch 10198/30000 Training Loss: 0.05997318774461746\n",
      "Epoch 10199/30000 Training Loss: 0.04866809770464897\n",
      "Epoch 10200/30000 Training Loss: 0.04398319125175476\n",
      "Epoch 10200/30000 Validation Loss: 0.04810762405395508\n",
      "Epoch 10201/30000 Training Loss: 0.047090694308280945\n",
      "Epoch 10202/30000 Training Loss: 0.050721749663352966\n",
      "Epoch 10203/30000 Training Loss: 0.03973512351512909\n",
      "Epoch 10204/30000 Training Loss: 0.056640759110450745\n",
      "Epoch 10205/30000 Training Loss: 0.03732261806726456\n",
      "Epoch 10206/30000 Training Loss: 0.047513447701931\n",
      "Epoch 10207/30000 Training Loss: 0.05269164592027664\n",
      "Epoch 10208/30000 Training Loss: 0.044476188719272614\n",
      "Epoch 10209/30000 Training Loss: 0.04825659841299057\n",
      "Epoch 10210/30000 Training Loss: 0.04327124357223511\n",
      "Epoch 10211/30000 Training Loss: 0.05203836411237717\n",
      "Epoch 10212/30000 Training Loss: 0.04859001189470291\n",
      "Epoch 10213/30000 Training Loss: 0.04908249154686928\n",
      "Epoch 10214/30000 Training Loss: 0.047769416123628616\n",
      "Epoch 10215/30000 Training Loss: 0.047945164144039154\n",
      "Epoch 10216/30000 Training Loss: 0.04194997623562813\n",
      "Epoch 10217/30000 Training Loss: 0.06718725711107254\n",
      "Epoch 10218/30000 Training Loss: 0.04446205869317055\n",
      "Epoch 10219/30000 Training Loss: 0.05294148623943329\n",
      "Epoch 10220/30000 Training Loss: 0.04625615105032921\n",
      "Epoch 10221/30000 Training Loss: 0.04648670554161072\n",
      "Epoch 10222/30000 Training Loss: 0.04126245528459549\n",
      "Epoch 10223/30000 Training Loss: 0.04749976843595505\n",
      "Epoch 10224/30000 Training Loss: 0.04489857703447342\n",
      "Epoch 10225/30000 Training Loss: 0.05370139330625534\n",
      "Epoch 10226/30000 Training Loss: 0.04112521559000015\n",
      "Epoch 10227/30000 Training Loss: 0.04008101299405098\n",
      "Epoch 10228/30000 Training Loss: 0.053725577890872955\n",
      "Epoch 10229/30000 Training Loss: 0.0517851896584034\n",
      "Epoch 10230/30000 Training Loss: 0.05313780903816223\n",
      "Epoch 10231/30000 Training Loss: 0.042610831558704376\n",
      "Epoch 10232/30000 Training Loss: 0.05653182417154312\n",
      "Epoch 10233/30000 Training Loss: 0.03770483285188675\n",
      "Epoch 10234/30000 Training Loss: 0.05167939513921738\n",
      "Epoch 10235/30000 Training Loss: 0.04129035398364067\n",
      "Epoch 10236/30000 Training Loss: 0.04390078783035278\n",
      "Epoch 10237/30000 Training Loss: 0.05572349578142166\n",
      "Epoch 10238/30000 Training Loss: 0.05214615911245346\n",
      "Epoch 10239/30000 Training Loss: 0.05041764676570892\n",
      "Epoch 10240/30000 Training Loss: 0.037281449884176254\n",
      "Epoch 10241/30000 Training Loss: 0.04763329029083252\n",
      "Epoch 10242/30000 Training Loss: 0.05262988060712814\n",
      "Epoch 10243/30000 Training Loss: 0.0526663139462471\n",
      "Epoch 10244/30000 Training Loss: 0.05852998048067093\n",
      "Epoch 10245/30000 Training Loss: 0.06397563219070435\n",
      "Epoch 10246/30000 Training Loss: 0.03816206380724907\n",
      "Epoch 10247/30000 Training Loss: 0.05761953070759773\n",
      "Epoch 10248/30000 Training Loss: 0.04053894057869911\n",
      "Epoch 10249/30000 Training Loss: 0.056678399443626404\n",
      "Epoch 10250/30000 Training Loss: 0.05525534600019455\n",
      "Epoch 10251/30000 Training Loss: 0.05097292736172676\n",
      "Epoch 10252/30000 Training Loss: 0.06332389265298843\n",
      "Epoch 10253/30000 Training Loss: 0.05615140497684479\n",
      "Epoch 10254/30000 Training Loss: 0.04998102784156799\n",
      "Epoch 10255/30000 Training Loss: 0.04270881041884422\n",
      "Epoch 10256/30000 Training Loss: 0.05032333731651306\n",
      "Epoch 10257/30000 Training Loss: 0.034888774156570435\n",
      "Epoch 10258/30000 Training Loss: 0.04856085404753685\n",
      "Epoch 10259/30000 Training Loss: 0.05718626081943512\n",
      "Epoch 10260/30000 Training Loss: 0.05113759636878967\n",
      "Epoch 10261/30000 Training Loss: 0.053691983222961426\n",
      "Epoch 10262/30000 Training Loss: 0.04521659016609192\n",
      "Epoch 10263/30000 Training Loss: 0.04072102904319763\n",
      "Epoch 10264/30000 Training Loss: 0.05621585622429848\n",
      "Epoch 10265/30000 Training Loss: 0.03936397656798363\n",
      "Epoch 10266/30000 Training Loss: 0.04799957945942879\n",
      "Epoch 10267/30000 Training Loss: 0.049020130187273026\n",
      "Epoch 10268/30000 Training Loss: 0.05955348163843155\n",
      "Epoch 10269/30000 Training Loss: 0.053542643785476685\n",
      "Epoch 10270/30000 Training Loss: 0.042672500014305115\n",
      "Epoch 10271/30000 Training Loss: 0.04991331323981285\n",
      "Epoch 10272/30000 Training Loss: 0.061808787286281586\n",
      "Epoch 10273/30000 Training Loss: 0.06927347183227539\n",
      "Epoch 10274/30000 Training Loss: 0.05638428404927254\n",
      "Epoch 10275/30000 Training Loss: 0.05367197096347809\n",
      "Epoch 10276/30000 Training Loss: 0.04790302738547325\n",
      "Epoch 10277/30000 Training Loss: 0.04150081053376198\n",
      "Epoch 10278/30000 Training Loss: 0.0673091933131218\n",
      "Epoch 10279/30000 Training Loss: 0.06366785615682602\n",
      "Epoch 10280/30000 Training Loss: 0.045222632586956024\n",
      "Epoch 10281/30000 Training Loss: 0.05241125822067261\n",
      "Epoch 10282/30000 Training Loss: 0.05605897307395935\n",
      "Epoch 10283/30000 Training Loss: 0.05080520361661911\n",
      "Epoch 10284/30000 Training Loss: 0.05547186732292175\n",
      "Epoch 10285/30000 Training Loss: 0.05765170603990555\n",
      "Epoch 10286/30000 Training Loss: 0.0437677837908268\n",
      "Epoch 10287/30000 Training Loss: 0.03529250621795654\n",
      "Epoch 10288/30000 Training Loss: 0.060183085501194\n",
      "Epoch 10289/30000 Training Loss: 0.03834798187017441\n",
      "Epoch 10290/30000 Training Loss: 0.06005101650953293\n",
      "Epoch 10291/30000 Training Loss: 0.053964368999004364\n",
      "Epoch 10292/30000 Training Loss: 0.056633077561855316\n",
      "Epoch 10293/30000 Training Loss: 0.06591124087572098\n",
      "Epoch 10294/30000 Training Loss: 0.05370432883501053\n",
      "Epoch 10295/30000 Training Loss: 0.055135250091552734\n",
      "Epoch 10296/30000 Training Loss: 0.0636591985821724\n",
      "Epoch 10297/30000 Training Loss: 0.05656713247299194\n",
      "Epoch 10298/30000 Training Loss: 0.0595364049077034\n",
      "Epoch 10299/30000 Training Loss: 0.03861059993505478\n",
      "Epoch 10300/30000 Training Loss: 0.04085437208414078\n",
      "Epoch 10300/30000 Validation Loss: 0.050022486597299576\n",
      "Epoch 10301/30000 Training Loss: 0.05360940471291542\n",
      "Epoch 10302/30000 Training Loss: 0.042204439640045166\n",
      "Epoch 10303/30000 Training Loss: 0.05221094191074371\n",
      "Epoch 10304/30000 Training Loss: 0.054123975336551666\n",
      "Epoch 10305/30000 Training Loss: 0.04600527137517929\n",
      "Epoch 10306/30000 Training Loss: 0.05466436967253685\n",
      "Epoch 10307/30000 Training Loss: 0.0475504994392395\n",
      "Epoch 10308/30000 Training Loss: 0.038288652896881104\n",
      "Epoch 10309/30000 Training Loss: 0.03809541091322899\n",
      "Epoch 10310/30000 Training Loss: 0.06135916709899902\n",
      "Epoch 10311/30000 Training Loss: 0.04141973704099655\n",
      "Epoch 10312/30000 Training Loss: 0.046439144760370255\n",
      "Epoch 10313/30000 Training Loss: 0.04380758851766586\n",
      "Epoch 10314/30000 Training Loss: 0.04599475488066673\n",
      "Epoch 10315/30000 Training Loss: 0.05649657920002937\n",
      "Epoch 10316/30000 Training Loss: 0.05344557762145996\n",
      "Epoch 10317/30000 Training Loss: 0.05981624126434326\n",
      "Epoch 10318/30000 Training Loss: 0.04614177346229553\n",
      "Epoch 10319/30000 Training Loss: 0.06919993460178375\n",
      "Epoch 10320/30000 Training Loss: 0.049089640378952026\n",
      "Epoch 10321/30000 Training Loss: 0.05058763921260834\n",
      "Epoch 10322/30000 Training Loss: 0.04216974228620529\n",
      "Epoch 10323/30000 Training Loss: 0.045986853539943695\n",
      "Epoch 10324/30000 Training Loss: 0.0509970523416996\n",
      "Epoch 10325/30000 Training Loss: 0.03517964482307434\n",
      "Epoch 10326/30000 Training Loss: 0.038790471851825714\n",
      "Epoch 10327/30000 Training Loss: 0.06272759288549423\n",
      "Epoch 10328/30000 Training Loss: 0.052268415689468384\n",
      "Epoch 10329/30000 Training Loss: 0.04557057470083237\n",
      "Epoch 10330/30000 Training Loss: 0.06986857950687408\n",
      "Epoch 10331/30000 Training Loss: 0.04821997880935669\n",
      "Epoch 10332/30000 Training Loss: 0.05657002329826355\n",
      "Epoch 10333/30000 Training Loss: 0.05271471291780472\n",
      "Epoch 10334/30000 Training Loss: 0.04635275900363922\n",
      "Epoch 10335/30000 Training Loss: 0.046458568423986435\n",
      "Epoch 10336/30000 Training Loss: 0.0439218170940876\n",
      "Epoch 10337/30000 Training Loss: 0.06784415990114212\n",
      "Epoch 10338/30000 Training Loss: 0.05997977405786514\n",
      "Epoch 10339/30000 Training Loss: 0.04379153996706009\n",
      "Epoch 10340/30000 Training Loss: 0.06245891749858856\n",
      "Epoch 10341/30000 Training Loss: 0.0496985986828804\n",
      "Epoch 10342/30000 Training Loss: 0.04932182654738426\n",
      "Epoch 10343/30000 Training Loss: 0.05070061981678009\n",
      "Epoch 10344/30000 Training Loss: 0.04034196957945824\n",
      "Epoch 10345/30000 Training Loss: 0.05239712446928024\n",
      "Epoch 10346/30000 Training Loss: 0.03699445351958275\n",
      "Epoch 10347/30000 Training Loss: 0.05735669285058975\n",
      "Epoch 10348/30000 Training Loss: 0.053355515003204346\n",
      "Epoch 10349/30000 Training Loss: 0.05769022926688194\n",
      "Epoch 10350/30000 Training Loss: 0.03524595871567726\n",
      "Epoch 10351/30000 Training Loss: 0.053889185190200806\n",
      "Epoch 10352/30000 Training Loss: 0.03871496766805649\n",
      "Epoch 10353/30000 Training Loss: 0.04148785024881363\n",
      "Epoch 10354/30000 Training Loss: 0.03672795742750168\n",
      "Epoch 10355/30000 Training Loss: 0.046263039112091064\n",
      "Epoch 10356/30000 Training Loss: 0.049095384776592255\n",
      "Epoch 10357/30000 Training Loss: 0.04074432700872421\n",
      "Epoch 10358/30000 Training Loss: 0.04674519598484039\n",
      "Epoch 10359/30000 Training Loss: 0.039785198867321014\n",
      "Epoch 10360/30000 Training Loss: 0.038816578686237335\n",
      "Epoch 10361/30000 Training Loss: 0.0347110852599144\n",
      "Epoch 10362/30000 Training Loss: 0.060328252613544464\n",
      "Epoch 10363/30000 Training Loss: 0.040985431522130966\n",
      "Epoch 10364/30000 Training Loss: 0.033855970948934555\n",
      "Epoch 10365/30000 Training Loss: 0.07029751688241959\n",
      "Epoch 10366/30000 Training Loss: 0.049634043127298355\n",
      "Epoch 10367/30000 Training Loss: 0.051329027861356735\n",
      "Epoch 10368/30000 Training Loss: 0.049057092517614365\n",
      "Epoch 10369/30000 Training Loss: 0.0514046847820282\n",
      "Epoch 10370/30000 Training Loss: 0.04649142920970917\n",
      "Epoch 10371/30000 Training Loss: 0.044814709573984146\n",
      "Epoch 10372/30000 Training Loss: 0.049385931342840195\n",
      "Epoch 10373/30000 Training Loss: 0.04966341704130173\n",
      "Epoch 10374/30000 Training Loss: 0.04711275175213814\n",
      "Epoch 10375/30000 Training Loss: 0.04745341092348099\n",
      "Epoch 10376/30000 Training Loss: 0.05466122180223465\n",
      "Epoch 10377/30000 Training Loss: 0.050818368792533875\n",
      "Epoch 10378/30000 Training Loss: 0.04608749598264694\n",
      "Epoch 10379/30000 Training Loss: 0.04120723903179169\n",
      "Epoch 10380/30000 Training Loss: 0.04936583340167999\n",
      "Epoch 10381/30000 Training Loss: 0.04698330536484718\n",
      "Epoch 10382/30000 Training Loss: 0.06376172602176666\n",
      "Epoch 10383/30000 Training Loss: 0.05263394117355347\n",
      "Epoch 10384/30000 Training Loss: 0.05454014986753464\n",
      "Epoch 10385/30000 Training Loss: 0.07018163055181503\n",
      "Epoch 10386/30000 Training Loss: 0.0450277104973793\n",
      "Epoch 10387/30000 Training Loss: 0.0468926802277565\n",
      "Epoch 10388/30000 Training Loss: 0.05084329843521118\n",
      "Epoch 10389/30000 Training Loss: 0.05682426691055298\n",
      "Epoch 10390/30000 Training Loss: 0.041514649987220764\n",
      "Epoch 10391/30000 Training Loss: 0.04000435024499893\n",
      "Epoch 10392/30000 Training Loss: 0.03920334577560425\n",
      "Epoch 10393/30000 Training Loss: 0.044953908771276474\n",
      "Epoch 10394/30000 Training Loss: 0.05089057981967926\n",
      "Epoch 10395/30000 Training Loss: 0.05465805530548096\n",
      "Epoch 10396/30000 Training Loss: 0.052511997520923615\n",
      "Epoch 10397/30000 Training Loss: 0.04147676005959511\n",
      "Epoch 10398/30000 Training Loss: 0.06084495037794113\n",
      "Epoch 10399/30000 Training Loss: 0.03898608684539795\n",
      "Epoch 10400/30000 Training Loss: 0.05481056123971939\n",
      "Epoch 10400/30000 Validation Loss: 0.03676418960094452\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03676418960094452<=============\n",
      "Epoch 10401/30000 Training Loss: 0.047883033752441406\n",
      "Epoch 10402/30000 Training Loss: 0.04740091413259506\n",
      "Epoch 10403/30000 Training Loss: 0.07278355211019516\n",
      "Epoch 10404/30000 Training Loss: 0.050364114344120026\n",
      "Epoch 10405/30000 Training Loss: 0.04593317583203316\n",
      "Epoch 10406/30000 Training Loss: 0.04776424169540405\n",
      "Epoch 10407/30000 Training Loss: 0.07920841127634048\n",
      "Epoch 10408/30000 Training Loss: 0.03861714527010918\n",
      "Epoch 10409/30000 Training Loss: 0.04194234311580658\n",
      "Epoch 10410/30000 Training Loss: 0.053478822112083435\n",
      "Epoch 10411/30000 Training Loss: 0.041960202157497406\n",
      "Epoch 10412/30000 Training Loss: 0.04250359907746315\n",
      "Epoch 10413/30000 Training Loss: 0.06016753613948822\n",
      "Epoch 10414/30000 Training Loss: 0.04615600407123566\n",
      "Epoch 10415/30000 Training Loss: 0.050222042948007584\n",
      "Epoch 10416/30000 Training Loss: 0.038902975618839264\n",
      "Epoch 10417/30000 Training Loss: 0.0637325868010521\n",
      "Epoch 10418/30000 Training Loss: 0.044251810759305954\n",
      "Epoch 10419/30000 Training Loss: 0.04518502578139305\n",
      "Epoch 10420/30000 Training Loss: 0.07078820466995239\n",
      "Epoch 10421/30000 Training Loss: 0.04296715557575226\n",
      "Epoch 10422/30000 Training Loss: 0.04331019148230553\n",
      "Epoch 10423/30000 Training Loss: 0.03185344487428665\n",
      "Epoch 10424/30000 Training Loss: 0.049012888222932816\n",
      "Epoch 10425/30000 Training Loss: 0.050112999975681305\n",
      "Epoch 10426/30000 Training Loss: 0.05561347305774689\n",
      "Epoch 10427/30000 Training Loss: 0.055311419069767\n",
      "Epoch 10428/30000 Training Loss: 0.04093562439084053\n",
      "Epoch 10429/30000 Training Loss: 0.04674592614173889\n",
      "Epoch 10430/30000 Training Loss: 0.043143030256032944\n",
      "Epoch 10431/30000 Training Loss: 0.04873671755194664\n",
      "Epoch 10432/30000 Training Loss: 0.06324882060289383\n",
      "Epoch 10433/30000 Training Loss: 0.06385865807533264\n",
      "Epoch 10434/30000 Training Loss: 0.05449812859296799\n",
      "Epoch 10435/30000 Training Loss: 0.029949501156806946\n",
      "Epoch 10436/30000 Training Loss: 0.051322825253009796\n",
      "Epoch 10437/30000 Training Loss: 0.06495614349842072\n",
      "Epoch 10438/30000 Training Loss: 0.041898299008607864\n",
      "Epoch 10439/30000 Training Loss: 0.04623326286673546\n",
      "Epoch 10440/30000 Training Loss: 0.045227061957120895\n",
      "Epoch 10441/30000 Training Loss: 0.049402207136154175\n",
      "Epoch 10442/30000 Training Loss: 0.048367734998464584\n",
      "Epoch 10443/30000 Training Loss: 0.041636139154434204\n",
      "Epoch 10444/30000 Training Loss: 0.03805974870920181\n",
      "Epoch 10445/30000 Training Loss: 0.04603642225265503\n",
      "Epoch 10446/30000 Training Loss: 0.05511033535003662\n",
      "Epoch 10447/30000 Training Loss: 0.04101642221212387\n",
      "Epoch 10448/30000 Training Loss: 0.03446640446782112\n",
      "Epoch 10449/30000 Training Loss: 0.0373089574277401\n",
      "Epoch 10450/30000 Training Loss: 0.04466099664568901\n",
      "Epoch 10451/30000 Training Loss: 0.050190333276987076\n",
      "Epoch 10452/30000 Training Loss: 0.05816451832652092\n",
      "Epoch 10453/30000 Training Loss: 0.045218657702207565\n",
      "Epoch 10454/30000 Training Loss: 0.05810530483722687\n",
      "Epoch 10455/30000 Training Loss: 0.04458528384566307\n",
      "Epoch 10456/30000 Training Loss: 0.03769347071647644\n",
      "Epoch 10457/30000 Training Loss: 0.06266556680202484\n",
      "Epoch 10458/30000 Training Loss: 0.04835166782140732\n",
      "Epoch 10459/30000 Training Loss: 0.04856710880994797\n",
      "Epoch 10460/30000 Training Loss: 0.036843858659267426\n",
      "Epoch 10461/30000 Training Loss: 0.03934899717569351\n",
      "Epoch 10462/30000 Training Loss: 0.04089285433292389\n",
      "Epoch 10463/30000 Training Loss: 0.05107474327087402\n",
      "Epoch 10464/30000 Training Loss: 0.04425046592950821\n",
      "Epoch 10465/30000 Training Loss: 0.054374366998672485\n",
      "Epoch 10466/30000 Training Loss: 0.04384631663560867\n",
      "Epoch 10467/30000 Training Loss: 0.0521639809012413\n",
      "Epoch 10468/30000 Training Loss: 0.031631965190172195\n",
      "Epoch 10469/30000 Training Loss: 0.04378871992230415\n",
      "Epoch 10470/30000 Training Loss: 0.04942893981933594\n",
      "Epoch 10471/30000 Training Loss: 0.03787901625037193\n",
      "Epoch 10472/30000 Training Loss: 0.06387953460216522\n",
      "Epoch 10473/30000 Training Loss: 0.04307160899043083\n",
      "Epoch 10474/30000 Training Loss: 0.05109461396932602\n",
      "Epoch 10475/30000 Training Loss: 0.04915805160999298\n",
      "Epoch 10476/30000 Training Loss: 0.04638468846678734\n",
      "Epoch 10477/30000 Training Loss: 0.04697248712182045\n",
      "Epoch 10478/30000 Training Loss: 0.05216877907514572\n",
      "Epoch 10479/30000 Training Loss: 0.03715863823890686\n",
      "Epoch 10480/30000 Training Loss: 0.029300594702363014\n",
      "Epoch 10481/30000 Training Loss: 0.053895264863967896\n",
      "Epoch 10482/30000 Training Loss: 0.05434145778417587\n",
      "Epoch 10483/30000 Training Loss: 0.05988100916147232\n",
      "Epoch 10484/30000 Training Loss: 0.047910913825035095\n",
      "Epoch 10485/30000 Training Loss: 0.0547012984752655\n",
      "Epoch 10486/30000 Training Loss: 0.05614592134952545\n",
      "Epoch 10487/30000 Training Loss: 0.04245705157518387\n",
      "Epoch 10488/30000 Training Loss: 0.03925854712724686\n",
      "Epoch 10489/30000 Training Loss: 0.047708455473184586\n",
      "Epoch 10490/30000 Training Loss: 0.062068141996860504\n",
      "Epoch 10491/30000 Training Loss: 0.054308757185935974\n",
      "Epoch 10492/30000 Training Loss: 0.058581359684467316\n",
      "Epoch 10493/30000 Training Loss: 0.03552282974123955\n",
      "Epoch 10494/30000 Training Loss: 0.04324707016348839\n",
      "Epoch 10495/30000 Training Loss: 0.054705098271369934\n",
      "Epoch 10496/30000 Training Loss: 0.05140335112810135\n",
      "Epoch 10497/30000 Training Loss: 0.04007760435342789\n",
      "Epoch 10498/30000 Training Loss: 0.0573875829577446\n",
      "Epoch 10499/30000 Training Loss: 0.04799666255712509\n",
      "Epoch 10500/30000 Training Loss: 0.06334799528121948\n",
      "Epoch 10500/30000 Validation Loss: 0.04215644672513008\n",
      "Epoch 10501/30000 Training Loss: 0.0477997362613678\n",
      "Epoch 10502/30000 Training Loss: 0.043177127838134766\n",
      "Epoch 10503/30000 Training Loss: 0.04874596372246742\n",
      "Epoch 10504/30000 Training Loss: 0.04776035621762276\n",
      "Epoch 10505/30000 Training Loss: 0.04203597083687782\n",
      "Epoch 10506/30000 Training Loss: 0.0417436882853508\n",
      "Epoch 10507/30000 Training Loss: 0.05926428735256195\n",
      "Epoch 10508/30000 Training Loss: 0.052791230380535126\n",
      "Epoch 10509/30000 Training Loss: 0.03366079926490784\n",
      "Epoch 10510/30000 Training Loss: 0.05015414208173752\n",
      "Epoch 10511/30000 Training Loss: 0.04890545457601547\n",
      "Epoch 10512/30000 Training Loss: 0.04684307798743248\n",
      "Epoch 10513/30000 Training Loss: 0.03751243278384209\n",
      "Epoch 10514/30000 Training Loss: 0.05220416560769081\n",
      "Epoch 10515/30000 Training Loss: 0.06313003599643707\n",
      "Epoch 10516/30000 Training Loss: 0.049747198820114136\n",
      "Epoch 10517/30000 Training Loss: 0.045091670006513596\n",
      "Epoch 10518/30000 Training Loss: 0.06891629844903946\n",
      "Epoch 10519/30000 Training Loss: 0.05381444841623306\n",
      "Epoch 10520/30000 Training Loss: 0.05511583760380745\n",
      "Epoch 10521/30000 Training Loss: 0.056320920586586\n",
      "Epoch 10522/30000 Training Loss: 0.04369919002056122\n",
      "Epoch 10523/30000 Training Loss: 0.06368056684732437\n",
      "Epoch 10524/30000 Training Loss: 0.0536922961473465\n",
      "Epoch 10525/30000 Training Loss: 0.05785997956991196\n",
      "Epoch 10526/30000 Training Loss: 0.05388754606246948\n",
      "Epoch 10527/30000 Training Loss: 0.039463263005018234\n",
      "Epoch 10528/30000 Training Loss: 0.05789158493280411\n",
      "Epoch 10529/30000 Training Loss: 0.04684478044509888\n",
      "Epoch 10530/30000 Training Loss: 0.04629076272249222\n",
      "Epoch 10531/30000 Training Loss: 0.05985286831855774\n",
      "Epoch 10532/30000 Training Loss: 0.05850603058934212\n",
      "Epoch 10533/30000 Training Loss: 0.06250650435686111\n",
      "Epoch 10534/30000 Training Loss: 0.041669584810733795\n",
      "Epoch 10535/30000 Training Loss: 0.053003743290901184\n",
      "Epoch 10536/30000 Training Loss: 0.04978228360414505\n",
      "Epoch 10537/30000 Training Loss: 0.05449076369404793\n",
      "Epoch 10538/30000 Training Loss: 0.06132318824529648\n",
      "Epoch 10539/30000 Training Loss: 0.04610968008637428\n",
      "Epoch 10540/30000 Training Loss: 0.045762982219457626\n",
      "Epoch 10541/30000 Training Loss: 0.044901877641677856\n",
      "Epoch 10542/30000 Training Loss: 0.04937940835952759\n",
      "Epoch 10543/30000 Training Loss: 0.043439559638500214\n",
      "Epoch 10544/30000 Training Loss: 0.0650009885430336\n",
      "Epoch 10545/30000 Training Loss: 0.037038084119558334\n",
      "Epoch 10546/30000 Training Loss: 0.04164647310972214\n",
      "Epoch 10547/30000 Training Loss: 0.05206214636564255\n",
      "Epoch 10548/30000 Training Loss: 0.04292534291744232\n",
      "Epoch 10549/30000 Training Loss: 0.06270238757133484\n",
      "Epoch 10550/30000 Training Loss: 0.05067645013332367\n",
      "Epoch 10551/30000 Training Loss: 0.060636408627033234\n",
      "Epoch 10552/30000 Training Loss: 0.04774220287799835\n",
      "Epoch 10553/30000 Training Loss: 0.04066167771816254\n",
      "Epoch 10554/30000 Training Loss: 0.04875848442316055\n",
      "Epoch 10555/30000 Training Loss: 0.05741399899125099\n",
      "Epoch 10556/30000 Training Loss: 0.04459194466471672\n",
      "Epoch 10557/30000 Training Loss: 0.06048773229122162\n",
      "Epoch 10558/30000 Training Loss: 0.05623142048716545\n",
      "Epoch 10559/30000 Training Loss: 0.032906532287597656\n",
      "Epoch 10560/30000 Training Loss: 0.04043509066104889\n",
      "Epoch 10561/30000 Training Loss: 0.03888394683599472\n",
      "Epoch 10562/30000 Training Loss: 0.051475927233695984\n",
      "Epoch 10563/30000 Training Loss: 0.051985129714012146\n",
      "Epoch 10564/30000 Training Loss: 0.044538043439388275\n",
      "Epoch 10565/30000 Training Loss: 0.040264733135700226\n",
      "Epoch 10566/30000 Training Loss: 0.04533980041742325\n",
      "Epoch 10567/30000 Training Loss: 0.04818078130483627\n",
      "Epoch 10568/30000 Training Loss: 0.044912856072187424\n",
      "Epoch 10569/30000 Training Loss: 0.045312196016311646\n",
      "Epoch 10570/30000 Training Loss: 0.050250180065631866\n",
      "Epoch 10571/30000 Training Loss: 0.03848981857299805\n",
      "Epoch 10572/30000 Training Loss: 0.041398804634809494\n",
      "Epoch 10573/30000 Training Loss: 0.05932678282260895\n",
      "Epoch 10574/30000 Training Loss: 0.04619762673974037\n",
      "Epoch 10575/30000 Training Loss: 0.04675368219614029\n",
      "Epoch 10576/30000 Training Loss: 0.037887971848249435\n",
      "Epoch 10577/30000 Training Loss: 0.05078759789466858\n",
      "Epoch 10578/30000 Training Loss: 0.046380672603845596\n",
      "Epoch 10579/30000 Training Loss: 0.05887920409440994\n",
      "Epoch 10580/30000 Training Loss: 0.04783985763788223\n",
      "Epoch 10581/30000 Training Loss: 0.06677277386188507\n",
      "Epoch 10582/30000 Training Loss: 0.039468660950660706\n",
      "Epoch 10583/30000 Training Loss: 0.041923776268959045\n",
      "Epoch 10584/30000 Training Loss: 0.05060124769806862\n",
      "Epoch 10585/30000 Training Loss: 0.05218607932329178\n",
      "Epoch 10586/30000 Training Loss: 0.05710051208734512\n",
      "Epoch 10587/30000 Training Loss: 0.043715327978134155\n",
      "Epoch 10588/30000 Training Loss: 0.05745827406644821\n",
      "Epoch 10589/30000 Training Loss: 0.04917546734213829\n",
      "Epoch 10590/30000 Training Loss: 0.05956817418336868\n",
      "Epoch 10591/30000 Training Loss: 0.05613800883293152\n",
      "Epoch 10592/30000 Training Loss: 0.05133499205112457\n",
      "Epoch 10593/30000 Training Loss: 0.04679601639509201\n",
      "Epoch 10594/30000 Training Loss: 0.05972795933485031\n",
      "Epoch 10595/30000 Training Loss: 0.043834127485752106\n",
      "Epoch 10596/30000 Training Loss: 0.04134234040975571\n",
      "Epoch 10597/30000 Training Loss: 0.03352413699030876\n",
      "Epoch 10598/30000 Training Loss: 0.050128817558288574\n",
      "Epoch 10599/30000 Training Loss: 0.055335573852062225\n",
      "Epoch 10600/30000 Training Loss: 0.04993915930390358\n",
      "Epoch 10600/30000 Validation Loss: 0.04801494628190994\n",
      "Epoch 10601/30000 Training Loss: 0.05240834131836891\n",
      "Epoch 10602/30000 Training Loss: 0.05443692207336426\n",
      "Epoch 10603/30000 Training Loss: 0.04451361671090126\n",
      "Epoch 10604/30000 Training Loss: 0.05220688506960869\n",
      "Epoch 10605/30000 Training Loss: 0.05524836480617523\n",
      "Epoch 10606/30000 Training Loss: 0.0638548731803894\n",
      "Epoch 10607/30000 Training Loss: 0.04511984437704086\n",
      "Epoch 10608/30000 Training Loss: 0.06676743924617767\n",
      "Epoch 10609/30000 Training Loss: 0.04099801555275917\n",
      "Epoch 10610/30000 Training Loss: 0.06892819702625275\n",
      "Epoch 10611/30000 Training Loss: 0.03469880297780037\n",
      "Epoch 10612/30000 Training Loss: 0.06436081230640411\n",
      "Epoch 10613/30000 Training Loss: 0.045360542833805084\n",
      "Epoch 10614/30000 Training Loss: 0.03189671039581299\n",
      "Epoch 10615/30000 Training Loss: 0.056690700352191925\n",
      "Epoch 10616/30000 Training Loss: 0.05422788858413696\n",
      "Epoch 10617/30000 Training Loss: 0.04965638369321823\n",
      "Epoch 10618/30000 Training Loss: 0.049739718437194824\n",
      "Epoch 10619/30000 Training Loss: 0.04732345789670944\n",
      "Epoch 10620/30000 Training Loss: 0.05667932704091072\n",
      "Epoch 10621/30000 Training Loss: 0.049371372908353806\n",
      "Epoch 10622/30000 Training Loss: 0.05655784159898758\n",
      "Epoch 10623/30000 Training Loss: 0.05279615521430969\n",
      "Epoch 10624/30000 Training Loss: 0.0554586797952652\n",
      "Epoch 10625/30000 Training Loss: 0.04326111078262329\n",
      "Epoch 10626/30000 Training Loss: 0.055763520300388336\n",
      "Epoch 10627/30000 Training Loss: 0.049158331006765366\n",
      "Epoch 10628/30000 Training Loss: 0.02970919758081436\n",
      "Epoch 10629/30000 Training Loss: 0.05287220701575279\n",
      "Epoch 10630/30000 Training Loss: 0.03730515390634537\n",
      "Epoch 10631/30000 Training Loss: 0.04760288447141647\n",
      "Epoch 10632/30000 Training Loss: 0.04904365912079811\n",
      "Epoch 10633/30000 Training Loss: 0.06588098406791687\n",
      "Epoch 10634/30000 Training Loss: 0.06364811211824417\n",
      "Epoch 10635/30000 Training Loss: 0.04341622442007065\n",
      "Epoch 10636/30000 Training Loss: 0.050047263503074646\n",
      "Epoch 10637/30000 Training Loss: 0.03877760469913483\n",
      "Epoch 10638/30000 Training Loss: 0.046882521361112595\n",
      "Epoch 10639/30000 Training Loss: 0.05974630266427994\n",
      "Epoch 10640/30000 Training Loss: 0.058032117784023285\n",
      "Epoch 10641/30000 Training Loss: 0.04831787198781967\n",
      "Epoch 10642/30000 Training Loss: 0.0411500446498394\n",
      "Epoch 10643/30000 Training Loss: 0.04568483307957649\n",
      "Epoch 10644/30000 Training Loss: 0.048743728548288345\n",
      "Epoch 10645/30000 Training Loss: 0.05725637823343277\n",
      "Epoch 10646/30000 Training Loss: 0.0527295358479023\n",
      "Epoch 10647/30000 Training Loss: 0.05858233571052551\n",
      "Epoch 10648/30000 Training Loss: 0.05680251121520996\n",
      "Epoch 10649/30000 Training Loss: 0.04178659990429878\n",
      "Epoch 10650/30000 Training Loss: 0.04591987654566765\n",
      "Epoch 10651/30000 Training Loss: 0.05409504473209381\n",
      "Epoch 10652/30000 Training Loss: 0.04802258312702179\n",
      "Epoch 10653/30000 Training Loss: 0.051092665642499924\n",
      "Epoch 10654/30000 Training Loss: 0.053958941251039505\n",
      "Epoch 10655/30000 Training Loss: 0.04932022467255592\n",
      "Epoch 10656/30000 Training Loss: 0.06847185641527176\n",
      "Epoch 10657/30000 Training Loss: 0.04880248010158539\n",
      "Epoch 10658/30000 Training Loss: 0.04753326624631882\n",
      "Epoch 10659/30000 Training Loss: 0.045767128467559814\n",
      "Epoch 10660/30000 Training Loss: 0.04944198206067085\n",
      "Epoch 10661/30000 Training Loss: 0.04820839315652847\n",
      "Epoch 10662/30000 Training Loss: 0.04342503845691681\n",
      "Epoch 10663/30000 Training Loss: 0.05896058678627014\n",
      "Epoch 10664/30000 Training Loss: 0.04629819095134735\n",
      "Epoch 10665/30000 Training Loss: 0.037674061954021454\n",
      "Epoch 10666/30000 Training Loss: 0.046126481145620346\n",
      "Epoch 10667/30000 Training Loss: 0.045117124915122986\n",
      "Epoch 10668/30000 Training Loss: 0.038201991468667984\n",
      "Epoch 10669/30000 Training Loss: 0.043804727494716644\n",
      "Epoch 10670/30000 Training Loss: 0.052386410534381866\n",
      "Epoch 10671/30000 Training Loss: 0.04645713418722153\n",
      "Epoch 10672/30000 Training Loss: 0.054201506078243256\n",
      "Epoch 10673/30000 Training Loss: 0.06028284877538681\n",
      "Epoch 10674/30000 Training Loss: 0.06140900403261185\n",
      "Epoch 10675/30000 Training Loss: 0.05610112100839615\n",
      "Epoch 10676/30000 Training Loss: 0.037643324583768845\n",
      "Epoch 10677/30000 Training Loss: 0.05259990692138672\n",
      "Epoch 10678/30000 Training Loss: 0.0620611198246479\n",
      "Epoch 10679/30000 Training Loss: 0.05523926764726639\n",
      "Epoch 10680/30000 Training Loss: 0.048077892512083054\n",
      "Epoch 10681/30000 Training Loss: 0.05615493655204773\n",
      "Epoch 10682/30000 Training Loss: 0.06211148202419281\n",
      "Epoch 10683/30000 Training Loss: 0.04969990253448486\n",
      "Epoch 10684/30000 Training Loss: 0.04150339215993881\n",
      "Epoch 10685/30000 Training Loss: 0.05943881720304489\n",
      "Epoch 10686/30000 Training Loss: 0.03705954551696777\n",
      "Epoch 10687/30000 Training Loss: 0.03733918070793152\n",
      "Epoch 10688/30000 Training Loss: 0.050276316702365875\n",
      "Epoch 10689/30000 Training Loss: 0.05742644518613815\n",
      "Epoch 10690/30000 Training Loss: 0.04878181591629982\n",
      "Epoch 10691/30000 Training Loss: 0.0627344399690628\n",
      "Epoch 10692/30000 Training Loss: 0.04398661106824875\n",
      "Epoch 10693/30000 Training Loss: 0.03558630868792534\n",
      "Epoch 10694/30000 Training Loss: 0.054882969707250595\n",
      "Epoch 10695/30000 Training Loss: 0.05851633474230766\n",
      "Epoch 10696/30000 Training Loss: 0.041344691067934036\n",
      "Epoch 10697/30000 Training Loss: 0.05321390926837921\n",
      "Epoch 10698/30000 Training Loss: 0.04869110882282257\n",
      "Epoch 10699/30000 Training Loss: 0.045570433139801025\n",
      "Epoch 10700/30000 Training Loss: 0.05209019035100937\n",
      "Epoch 10700/30000 Validation Loss: 0.0488041490316391\n",
      "Epoch 10701/30000 Training Loss: 0.05036766082048416\n",
      "Epoch 10702/30000 Training Loss: 0.03941581770777702\n",
      "Epoch 10703/30000 Training Loss: 0.037739843130111694\n",
      "Epoch 10704/30000 Training Loss: 0.03585172817111015\n",
      "Epoch 10705/30000 Training Loss: 0.04547782242298126\n",
      "Epoch 10706/30000 Training Loss: 0.0596981942653656\n",
      "Epoch 10707/30000 Training Loss: 0.0428650826215744\n",
      "Epoch 10708/30000 Training Loss: 0.04465603828430176\n",
      "Epoch 10709/30000 Training Loss: 0.0386948399245739\n",
      "Epoch 10710/30000 Training Loss: 0.03980371356010437\n",
      "Epoch 10711/30000 Training Loss: 0.04737131670117378\n",
      "Epoch 10712/30000 Training Loss: 0.03162629157304764\n",
      "Epoch 10713/30000 Training Loss: 0.04606049880385399\n",
      "Epoch 10714/30000 Training Loss: 0.05134249106049538\n",
      "Epoch 10715/30000 Training Loss: 0.05701491981744766\n",
      "Epoch 10716/30000 Training Loss: 0.045393045991659164\n",
      "Epoch 10717/30000 Training Loss: 0.03318522125482559\n",
      "Epoch 10718/30000 Training Loss: 0.05900483950972557\n",
      "Epoch 10719/30000 Training Loss: 0.06423051655292511\n",
      "Epoch 10720/30000 Training Loss: 0.03852371498942375\n",
      "Epoch 10721/30000 Training Loss: 0.0483630895614624\n",
      "Epoch 10722/30000 Training Loss: 0.044897809624671936\n",
      "Epoch 10723/30000 Training Loss: 0.04415425658226013\n",
      "Epoch 10724/30000 Training Loss: 0.0582512691617012\n",
      "Epoch 10725/30000 Training Loss: 0.04598996043205261\n",
      "Epoch 10726/30000 Training Loss: 0.05165407806634903\n",
      "Epoch 10727/30000 Training Loss: 0.0581233836710453\n",
      "Epoch 10728/30000 Training Loss: 0.041441988199949265\n",
      "Epoch 10729/30000 Training Loss: 0.053176023066043854\n",
      "Epoch 10730/30000 Training Loss: 0.05041789263486862\n",
      "Epoch 10731/30000 Training Loss: 0.045124612748622894\n",
      "Epoch 10732/30000 Training Loss: 0.03397563844919205\n",
      "Epoch 10733/30000 Training Loss: 0.05307742953300476\n",
      "Epoch 10734/30000 Training Loss: 0.05602937191724777\n",
      "Epoch 10735/30000 Training Loss: 0.04253288730978966\n",
      "Epoch 10736/30000 Training Loss: 0.04452093690633774\n",
      "Epoch 10737/30000 Training Loss: 0.05115334689617157\n",
      "Epoch 10738/30000 Training Loss: 0.04924576357007027\n",
      "Epoch 10739/30000 Training Loss: 0.04146255925297737\n",
      "Epoch 10740/30000 Training Loss: 0.06267385184764862\n",
      "Epoch 10741/30000 Training Loss: 0.05188944935798645\n",
      "Epoch 10742/30000 Training Loss: 0.035906679928302765\n",
      "Epoch 10743/30000 Training Loss: 0.051119670271873474\n",
      "Epoch 10744/30000 Training Loss: 0.04121703654527664\n",
      "Epoch 10745/30000 Training Loss: 0.04978672415018082\n",
      "Epoch 10746/30000 Training Loss: 0.045498207211494446\n",
      "Epoch 10747/30000 Training Loss: 0.05472384765744209\n",
      "Epoch 10748/30000 Training Loss: 0.05545321851968765\n",
      "Epoch 10749/30000 Training Loss: 0.0474587008357048\n",
      "Epoch 10750/30000 Training Loss: 0.04802204668521881\n",
      "Epoch 10751/30000 Training Loss: 0.053006015717983246\n",
      "Epoch 10752/30000 Training Loss: 0.055137377232313156\n",
      "Epoch 10753/30000 Training Loss: 0.04365275055170059\n",
      "Epoch 10754/30000 Training Loss: 0.0401255339384079\n",
      "Epoch 10755/30000 Training Loss: 0.04751438647508621\n",
      "Epoch 10756/30000 Training Loss: 0.07336405664682388\n",
      "Epoch 10757/30000 Training Loss: 0.05136076360940933\n",
      "Epoch 10758/30000 Training Loss: 0.05862875282764435\n",
      "Epoch 10759/30000 Training Loss: 0.045560453087091446\n",
      "Epoch 10760/30000 Training Loss: 0.043496351689100266\n",
      "Epoch 10761/30000 Training Loss: 0.04499891400337219\n",
      "Epoch 10762/30000 Training Loss: 0.05317747965455055\n",
      "Epoch 10763/30000 Training Loss: 0.03877872973680496\n",
      "Epoch 10764/30000 Training Loss: 0.05529522895812988\n",
      "Epoch 10765/30000 Training Loss: 0.04343125969171524\n",
      "Epoch 10766/30000 Training Loss: 0.06827395409345627\n",
      "Epoch 10767/30000 Training Loss: 0.04569434002041817\n",
      "Epoch 10768/30000 Training Loss: 0.04368802160024643\n",
      "Epoch 10769/30000 Training Loss: 0.056852780282497406\n",
      "Epoch 10770/30000 Training Loss: 0.04646690934896469\n",
      "Epoch 10771/30000 Training Loss: 0.04737694934010506\n",
      "Epoch 10772/30000 Training Loss: 0.0588587261736393\n",
      "Epoch 10773/30000 Training Loss: 0.04379314184188843\n",
      "Epoch 10774/30000 Training Loss: 0.05666300654411316\n",
      "Epoch 10775/30000 Training Loss: 0.04414145648479462\n",
      "Epoch 10776/30000 Training Loss: 0.047741908580064774\n",
      "Epoch 10777/30000 Training Loss: 0.03792233765125275\n",
      "Epoch 10778/30000 Training Loss: 0.049693986773490906\n",
      "Epoch 10779/30000 Training Loss: 0.04649754986166954\n",
      "Epoch 10780/30000 Training Loss: 0.04626668244600296\n",
      "Epoch 10781/30000 Training Loss: 0.047352135181427\n",
      "Epoch 10782/30000 Training Loss: 0.04525827616453171\n",
      "Epoch 10783/30000 Training Loss: 0.038714319467544556\n",
      "Epoch 10784/30000 Training Loss: 0.04878474771976471\n",
      "Epoch 10785/30000 Training Loss: 0.0438891276717186\n",
      "Epoch 10786/30000 Training Loss: 0.04188990220427513\n",
      "Epoch 10787/30000 Training Loss: 0.05235620215535164\n",
      "Epoch 10788/30000 Training Loss: 0.05185382813215256\n",
      "Epoch 10789/30000 Training Loss: 0.039746448397636414\n",
      "Epoch 10790/30000 Training Loss: 0.06107022985816002\n",
      "Epoch 10791/30000 Training Loss: 0.03698299452662468\n",
      "Epoch 10792/30000 Training Loss: 0.04080630838871002\n",
      "Epoch 10793/30000 Training Loss: 0.05659042298793793\n",
      "Epoch 10794/30000 Training Loss: 0.04355299100279808\n",
      "Epoch 10795/30000 Training Loss: 0.05522681772708893\n",
      "Epoch 10796/30000 Training Loss: 0.05149797350168228\n",
      "Epoch 10797/30000 Training Loss: 0.0440104603767395\n",
      "Epoch 10798/30000 Training Loss: 0.04361056908965111\n",
      "Epoch 10799/30000 Training Loss: 0.048151932656764984\n",
      "Epoch 10800/30000 Training Loss: 0.03804078698158264\n",
      "Epoch 10800/30000 Validation Loss: 0.05753698945045471\n",
      "Epoch 10801/30000 Training Loss: 0.05574561655521393\n",
      "Epoch 10802/30000 Training Loss: 0.06064808368682861\n",
      "Epoch 10803/30000 Training Loss: 0.06047070771455765\n",
      "Epoch 10804/30000 Training Loss: 0.039831217378377914\n",
      "Epoch 10805/30000 Training Loss: 0.04068533331155777\n",
      "Epoch 10806/30000 Training Loss: 0.050370439887046814\n",
      "Epoch 10807/30000 Training Loss: 0.06776450574398041\n",
      "Epoch 10808/30000 Training Loss: 0.05270743370056152\n",
      "Epoch 10809/30000 Training Loss: 0.05039700120687485\n",
      "Epoch 10810/30000 Training Loss: 0.05815041810274124\n",
      "Epoch 10811/30000 Training Loss: 0.0613495409488678\n",
      "Epoch 10812/30000 Training Loss: 0.048838838934898376\n",
      "Epoch 10813/30000 Training Loss: 0.05043424665927887\n",
      "Epoch 10814/30000 Training Loss: 0.04576702415943146\n",
      "Epoch 10815/30000 Training Loss: 0.03419622406363487\n",
      "Epoch 10816/30000 Training Loss: 0.0460110679268837\n",
      "Epoch 10817/30000 Training Loss: 0.07004579156637192\n",
      "Epoch 10818/30000 Training Loss: 0.04821077734231949\n",
      "Epoch 10819/30000 Training Loss: 0.06002698466181755\n",
      "Epoch 10820/30000 Training Loss: 0.04509544000029564\n",
      "Epoch 10821/30000 Training Loss: 0.04164852946996689\n",
      "Epoch 10822/30000 Training Loss: 0.04322037100791931\n",
      "Epoch 10823/30000 Training Loss: 0.054384730756282806\n",
      "Epoch 10824/30000 Training Loss: 0.045838549733161926\n",
      "Epoch 10825/30000 Training Loss: 0.040462806820869446\n",
      "Epoch 10826/30000 Training Loss: 0.044632524251937866\n",
      "Epoch 10827/30000 Training Loss: 0.045764289796352386\n",
      "Epoch 10828/30000 Training Loss: 0.048076704144477844\n",
      "Epoch 10829/30000 Training Loss: 0.0428377166390419\n",
      "Epoch 10830/30000 Training Loss: 0.055306512862443924\n",
      "Epoch 10831/30000 Training Loss: 0.052574899047613144\n",
      "Epoch 10832/30000 Training Loss: 0.050830692052841187\n",
      "Epoch 10833/30000 Training Loss: 0.050874970853328705\n",
      "Epoch 10834/30000 Training Loss: 0.0532660037279129\n",
      "Epoch 10835/30000 Training Loss: 0.054464735090732574\n",
      "Epoch 10836/30000 Training Loss: 0.040349312126636505\n",
      "Epoch 10837/30000 Training Loss: 0.0506599135696888\n",
      "Epoch 10838/30000 Training Loss: 0.04282068461179733\n",
      "Epoch 10839/30000 Training Loss: 0.04305819422006607\n",
      "Epoch 10840/30000 Training Loss: 0.05741218477487564\n",
      "Epoch 10841/30000 Training Loss: 0.05691027641296387\n",
      "Epoch 10842/30000 Training Loss: 0.048566002398729324\n",
      "Epoch 10843/30000 Training Loss: 0.0469132624566555\n",
      "Epoch 10844/30000 Training Loss: 0.059842776507139206\n",
      "Epoch 10845/30000 Training Loss: 0.058912493288517\n",
      "Epoch 10846/30000 Training Loss: 0.047139011323451996\n",
      "Epoch 10847/30000 Training Loss: 0.04385696351528168\n",
      "Epoch 10848/30000 Training Loss: 0.03998669981956482\n",
      "Epoch 10849/30000 Training Loss: 0.046327270567417145\n",
      "Epoch 10850/30000 Training Loss: 0.04798181727528572\n",
      "Epoch 10851/30000 Training Loss: 0.04984147846698761\n",
      "Epoch 10852/30000 Training Loss: 0.042706504464149475\n",
      "Epoch 10853/30000 Training Loss: 0.04171300679445267\n",
      "Epoch 10854/30000 Training Loss: 0.04831629619002342\n",
      "Epoch 10855/30000 Training Loss: 0.0560787208378315\n",
      "Epoch 10856/30000 Training Loss: 0.0497291162610054\n",
      "Epoch 10857/30000 Training Loss: 0.05320192128419876\n",
      "Epoch 10858/30000 Training Loss: 0.042481131851673126\n",
      "Epoch 10859/30000 Training Loss: 0.04767117649316788\n",
      "Epoch 10860/30000 Training Loss: 0.06060931086540222\n",
      "Epoch 10861/30000 Training Loss: 0.050969429314136505\n",
      "Epoch 10862/30000 Training Loss: 0.05209985375404358\n",
      "Epoch 10863/30000 Training Loss: 0.05708344280719757\n",
      "Epoch 10864/30000 Training Loss: 0.044994793832302094\n",
      "Epoch 10865/30000 Training Loss: 0.041668131947517395\n",
      "Epoch 10866/30000 Training Loss: 0.04697771370410919\n",
      "Epoch 10867/30000 Training Loss: 0.04382101818919182\n",
      "Epoch 10868/30000 Training Loss: 0.05056542158126831\n",
      "Epoch 10869/30000 Training Loss: 0.03442583978176117\n",
      "Epoch 10870/30000 Training Loss: 0.059114448726177216\n",
      "Epoch 10871/30000 Training Loss: 0.05699077993631363\n",
      "Epoch 10872/30000 Training Loss: 0.058213647454977036\n",
      "Epoch 10873/30000 Training Loss: 0.046315405517816544\n",
      "Epoch 10874/30000 Training Loss: 0.04505923017859459\n",
      "Epoch 10875/30000 Training Loss: 0.04688527062535286\n",
      "Epoch 10876/30000 Training Loss: 0.04604483023285866\n",
      "Epoch 10877/30000 Training Loss: 0.04921592026948929\n",
      "Epoch 10878/30000 Training Loss: 0.0597522109746933\n",
      "Epoch 10879/30000 Training Loss: 0.0442538857460022\n",
      "Epoch 10880/30000 Training Loss: 0.05168337747454643\n",
      "Epoch 10881/30000 Training Loss: 0.04727180302143097\n",
      "Epoch 10882/30000 Training Loss: 0.06752969324588776\n",
      "Epoch 10883/30000 Training Loss: 0.046359725296497345\n",
      "Epoch 10884/30000 Training Loss: 0.06221592426300049\n",
      "Epoch 10885/30000 Training Loss: 0.04231947660446167\n",
      "Epoch 10886/30000 Training Loss: 0.04434734582901001\n",
      "Epoch 10887/30000 Training Loss: 0.04761519283056259\n",
      "Epoch 10888/30000 Training Loss: 0.04214376211166382\n",
      "Epoch 10889/30000 Training Loss: 0.04524948075413704\n",
      "Epoch 10890/30000 Training Loss: 0.03881005197763443\n",
      "Epoch 10891/30000 Training Loss: 0.04442790523171425\n",
      "Epoch 10892/30000 Training Loss: 0.053484804928302765\n",
      "Epoch 10893/30000 Training Loss: 0.04916931688785553\n",
      "Epoch 10894/30000 Training Loss: 0.04545574635267258\n",
      "Epoch 10895/30000 Training Loss: 0.06245434284210205\n",
      "Epoch 10896/30000 Training Loss: 0.04612187296152115\n",
      "Epoch 10897/30000 Training Loss: 0.030756331980228424\n",
      "Epoch 10898/30000 Training Loss: 0.06629349291324615\n",
      "Epoch 10899/30000 Training Loss: 0.04733200743794441\n",
      "Epoch 10900/30000 Training Loss: 0.04683760553598404\n",
      "Epoch 10900/30000 Validation Loss: 0.04659900441765785\n",
      "Epoch 10901/30000 Training Loss: 0.0568891242146492\n",
      "Epoch 10902/30000 Training Loss: 0.059928037226200104\n",
      "Epoch 10903/30000 Training Loss: 0.04264732450246811\n",
      "Epoch 10904/30000 Training Loss: 0.03780845180153847\n",
      "Epoch 10905/30000 Training Loss: 0.041391871869564056\n",
      "Epoch 10906/30000 Training Loss: 0.06110994517803192\n",
      "Epoch 10907/30000 Training Loss: 0.03629985824227333\n",
      "Epoch 10908/30000 Training Loss: 0.043895021080970764\n",
      "Epoch 10909/30000 Training Loss: 0.06014196574687958\n",
      "Epoch 10910/30000 Training Loss: 0.04673253744840622\n",
      "Epoch 10911/30000 Training Loss: 0.06613998860120773\n",
      "Epoch 10912/30000 Training Loss: 0.06351500749588013\n",
      "Epoch 10913/30000 Training Loss: 0.051519036293029785\n",
      "Epoch 10914/30000 Training Loss: 0.04346325993537903\n",
      "Epoch 10915/30000 Training Loss: 0.037019554525613785\n",
      "Epoch 10916/30000 Training Loss: 0.05716319382190704\n",
      "Epoch 10917/30000 Training Loss: 0.051038771867752075\n",
      "Epoch 10918/30000 Training Loss: 0.04248813912272453\n",
      "Epoch 10919/30000 Training Loss: 0.05462642014026642\n",
      "Epoch 10920/30000 Training Loss: 0.043328288942575455\n",
      "Epoch 10921/30000 Training Loss: 0.043092064559459686\n",
      "Epoch 10922/30000 Training Loss: 0.041662633419036865\n",
      "Epoch 10923/30000 Training Loss: 0.039597488939762115\n",
      "Epoch 10924/30000 Training Loss: 0.04470768943428993\n",
      "Epoch 10925/30000 Training Loss: 0.03376273065805435\n",
      "Epoch 10926/30000 Training Loss: 0.05109243839979172\n",
      "Epoch 10927/30000 Training Loss: 0.042394064366817474\n",
      "Epoch 10928/30000 Training Loss: 0.0422794371843338\n",
      "Epoch 10929/30000 Training Loss: 0.040560998022556305\n",
      "Epoch 10930/30000 Training Loss: 0.07698825001716614\n",
      "Epoch 10931/30000 Training Loss: 0.0424925796687603\n",
      "Epoch 10932/30000 Training Loss: 0.03996821120381355\n",
      "Epoch 10933/30000 Training Loss: 0.05370553582906723\n",
      "Epoch 10934/30000 Training Loss: 0.04564504697918892\n",
      "Epoch 10935/30000 Training Loss: 0.050248704850673676\n",
      "Epoch 10936/30000 Training Loss: 0.049733176827430725\n",
      "Epoch 10937/30000 Training Loss: 0.05091589689254761\n",
      "Epoch 10938/30000 Training Loss: 0.04173074662685394\n",
      "Epoch 10939/30000 Training Loss: 0.07060138136148453\n",
      "Epoch 10940/30000 Training Loss: 0.04974379390478134\n",
      "Epoch 10941/30000 Training Loss: 0.05192997306585312\n",
      "Epoch 10942/30000 Training Loss: 0.054314933717250824\n",
      "Epoch 10943/30000 Training Loss: 0.048792943358421326\n",
      "Epoch 10944/30000 Training Loss: 0.04634111374616623\n",
      "Epoch 10945/30000 Training Loss: 0.05022093653678894\n",
      "Epoch 10946/30000 Training Loss: 0.043579600751399994\n",
      "Epoch 10947/30000 Training Loss: 0.05375639349222183\n",
      "Epoch 10948/30000 Training Loss: 0.060812219977378845\n",
      "Epoch 10949/30000 Training Loss: 0.04705102741718292\n",
      "Epoch 10950/30000 Training Loss: 0.039145998656749725\n",
      "Epoch 10951/30000 Training Loss: 0.03870328515768051\n",
      "Epoch 10952/30000 Training Loss: 0.047189198434352875\n",
      "Epoch 10953/30000 Training Loss: 0.048066217452287674\n",
      "Epoch 10954/30000 Training Loss: 0.04190383478999138\n",
      "Epoch 10955/30000 Training Loss: 0.04855189844965935\n",
      "Epoch 10956/30000 Training Loss: 0.04452810063958168\n",
      "Epoch 10957/30000 Training Loss: 0.05826936289668083\n",
      "Epoch 10958/30000 Training Loss: 0.045366257429122925\n",
      "Epoch 10959/30000 Training Loss: 0.053572867065668106\n",
      "Epoch 10960/30000 Training Loss: 0.044519633054733276\n",
      "Epoch 10961/30000 Training Loss: 0.04075761139392853\n",
      "Epoch 10962/30000 Training Loss: 0.046038445085287094\n",
      "Epoch 10963/30000 Training Loss: 0.04397708922624588\n",
      "Epoch 10964/30000 Training Loss: 0.06023315340280533\n",
      "Epoch 10965/30000 Training Loss: 0.045996733009815216\n",
      "Epoch 10966/30000 Training Loss: 0.04970930516719818\n",
      "Epoch 10967/30000 Training Loss: 0.048067688941955566\n",
      "Epoch 10968/30000 Training Loss: 0.05052870512008667\n",
      "Epoch 10969/30000 Training Loss: 0.06152395159006119\n",
      "Epoch 10970/30000 Training Loss: 0.059832558035850525\n",
      "Epoch 10971/30000 Training Loss: 0.04458470642566681\n",
      "Epoch 10972/30000 Training Loss: 0.054824959486722946\n",
      "Epoch 10973/30000 Training Loss: 0.05354781448841095\n",
      "Epoch 10974/30000 Training Loss: 0.046374447643756866\n",
      "Epoch 10975/30000 Training Loss: 0.0576690211892128\n",
      "Epoch 10976/30000 Training Loss: 0.04472741484642029\n",
      "Epoch 10977/30000 Training Loss: 0.040655042976140976\n",
      "Epoch 10978/30000 Training Loss: 0.04638580232858658\n",
      "Epoch 10979/30000 Training Loss: 0.0439024455845356\n",
      "Epoch 10980/30000 Training Loss: 0.03549767658114433\n",
      "Epoch 10981/30000 Training Loss: 0.052048210054636\n",
      "Epoch 10982/30000 Training Loss: 0.04702351987361908\n",
      "Epoch 10983/30000 Training Loss: 0.051068902015686035\n",
      "Epoch 10984/30000 Training Loss: 0.03839828073978424\n",
      "Epoch 10985/30000 Training Loss: 0.040412794798612595\n",
      "Epoch 10986/30000 Training Loss: 0.05783963203430176\n",
      "Epoch 10987/30000 Training Loss: 0.056758128106594086\n",
      "Epoch 10988/30000 Training Loss: 0.05472145229578018\n",
      "Epoch 10989/30000 Training Loss: 0.0415034145116806\n",
      "Epoch 10990/30000 Training Loss: 0.04089069366455078\n",
      "Epoch 10991/30000 Training Loss: 0.04364722967147827\n",
      "Epoch 10992/30000 Training Loss: 0.04605073481798172\n",
      "Epoch 10993/30000 Training Loss: 0.0355822816491127\n",
      "Epoch 10994/30000 Training Loss: 0.03838749974966049\n",
      "Epoch 10995/30000 Training Loss: 0.05765577405691147\n",
      "Epoch 10996/30000 Training Loss: 0.04197416454553604\n",
      "Epoch 10997/30000 Training Loss: 0.06370975077152252\n",
      "Epoch 10998/30000 Training Loss: 0.04755515232682228\n",
      "Epoch 10999/30000 Training Loss: 0.044185854494571686\n",
      "Epoch 11000/30000 Training Loss: 0.045422643423080444\n",
      "Epoch 11000/30000 Validation Loss: 0.05124804750084877\n",
      "Epoch 11001/30000 Training Loss: 0.04654931277036667\n",
      "Epoch 11002/30000 Training Loss: 0.05337065830826759\n",
      "Epoch 11003/30000 Training Loss: 0.0564645379781723\n",
      "Epoch 11004/30000 Training Loss: 0.04373253509402275\n",
      "Epoch 11005/30000 Training Loss: 0.05003947392106056\n",
      "Epoch 11006/30000 Training Loss: 0.043202951550483704\n",
      "Epoch 11007/30000 Training Loss: 0.044343046844005585\n",
      "Epoch 11008/30000 Training Loss: 0.05324200168251991\n",
      "Epoch 11009/30000 Training Loss: 0.04226108640432358\n",
      "Epoch 11010/30000 Training Loss: 0.05001666769385338\n",
      "Epoch 11011/30000 Training Loss: 0.044702593237161636\n",
      "Epoch 11012/30000 Training Loss: 0.06867963820695877\n",
      "Epoch 11013/30000 Training Loss: 0.04914200305938721\n",
      "Epoch 11014/30000 Training Loss: 0.042408160865306854\n",
      "Epoch 11015/30000 Training Loss: 0.03376220911741257\n",
      "Epoch 11016/30000 Training Loss: 0.0490163117647171\n",
      "Epoch 11017/30000 Training Loss: 0.04433591663837433\n",
      "Epoch 11018/30000 Training Loss: 0.05849965289235115\n",
      "Epoch 11019/30000 Training Loss: 0.04079569876194\n",
      "Epoch 11020/30000 Training Loss: 0.04528632014989853\n",
      "Epoch 11021/30000 Training Loss: 0.04045507311820984\n",
      "Epoch 11022/30000 Training Loss: 0.05747503787279129\n",
      "Epoch 11023/30000 Training Loss: 0.060765188187360764\n",
      "Epoch 11024/30000 Training Loss: 0.04165450111031532\n",
      "Epoch 11025/30000 Training Loss: 0.04406803846359253\n",
      "Epoch 11026/30000 Training Loss: 0.05260416492819786\n",
      "Epoch 11027/30000 Training Loss: 0.04832194000482559\n",
      "Epoch 11028/30000 Training Loss: 0.05042652040719986\n",
      "Epoch 11029/30000 Training Loss: 0.06422760337591171\n",
      "Epoch 11030/30000 Training Loss: 0.041581202298402786\n",
      "Epoch 11031/30000 Training Loss: 0.03937394917011261\n",
      "Epoch 11032/30000 Training Loss: 0.03899088501930237\n",
      "Epoch 11033/30000 Training Loss: 0.053873807191848755\n",
      "Epoch 11034/30000 Training Loss: 0.056369468569755554\n",
      "Epoch 11035/30000 Training Loss: 0.045272089540958405\n",
      "Epoch 11036/30000 Training Loss: 0.054951731115579605\n",
      "Epoch 11037/30000 Training Loss: 0.06114008277654648\n",
      "Epoch 11038/30000 Training Loss: 0.05308869481086731\n",
      "Epoch 11039/30000 Training Loss: 0.0532158687710762\n",
      "Epoch 11040/30000 Training Loss: 0.0422862246632576\n",
      "Epoch 11041/30000 Training Loss: 0.05519874766469002\n",
      "Epoch 11042/30000 Training Loss: 0.04995077848434448\n",
      "Epoch 11043/30000 Training Loss: 0.0601196251809597\n",
      "Epoch 11044/30000 Training Loss: 0.04570084065198898\n",
      "Epoch 11045/30000 Training Loss: 0.06206100434064865\n",
      "Epoch 11046/30000 Training Loss: 0.03965762257575989\n",
      "Epoch 11047/30000 Training Loss: 0.04788456857204437\n",
      "Epoch 11048/30000 Training Loss: 0.03172992169857025\n",
      "Epoch 11049/30000 Training Loss: 0.043681301176548004\n",
      "Epoch 11050/30000 Training Loss: 0.05527550354599953\n",
      "Epoch 11051/30000 Training Loss: 0.052752863615751266\n",
      "Epoch 11052/30000 Training Loss: 0.04595568776130676\n",
      "Epoch 11053/30000 Training Loss: 0.04309523105621338\n",
      "Epoch 11054/30000 Training Loss: 0.05034928396344185\n",
      "Epoch 11055/30000 Training Loss: 0.0313715785741806\n",
      "Epoch 11056/30000 Training Loss: 0.045088380575180054\n",
      "Epoch 11057/30000 Training Loss: 0.053920503705739975\n",
      "Epoch 11058/30000 Training Loss: 0.04976459592580795\n",
      "Epoch 11059/30000 Training Loss: 0.03729528561234474\n",
      "Epoch 11060/30000 Training Loss: 0.04668629914522171\n",
      "Epoch 11061/30000 Training Loss: 0.058734867721796036\n",
      "Epoch 11062/30000 Training Loss: 0.03259914368391037\n",
      "Epoch 11063/30000 Training Loss: 0.04751836508512497\n",
      "Epoch 11064/30000 Training Loss: 0.06719399988651276\n",
      "Epoch 11065/30000 Training Loss: 0.04798358678817749\n",
      "Epoch 11066/30000 Training Loss: 0.04779156297445297\n",
      "Epoch 11067/30000 Training Loss: 0.04349655285477638\n",
      "Epoch 11068/30000 Training Loss: 0.049568500369787216\n",
      "Epoch 11069/30000 Training Loss: 0.04461243003606796\n",
      "Epoch 11070/30000 Training Loss: 0.051817942410707474\n",
      "Epoch 11071/30000 Training Loss: 0.0461430698633194\n",
      "Epoch 11072/30000 Training Loss: 0.047558002173900604\n",
      "Epoch 11073/30000 Training Loss: 0.04692711681127548\n",
      "Epoch 11074/30000 Training Loss: 0.05915304273366928\n",
      "Epoch 11075/30000 Training Loss: 0.05124593526124954\n",
      "Epoch 11076/30000 Training Loss: 0.05668135732412338\n",
      "Epoch 11077/30000 Training Loss: 0.04044703394174576\n",
      "Epoch 11078/30000 Training Loss: 0.03615734726190567\n",
      "Epoch 11079/30000 Training Loss: 0.04142442345619202\n",
      "Epoch 11080/30000 Training Loss: 0.03734937682747841\n",
      "Epoch 11081/30000 Training Loss: 0.04152167588472366\n",
      "Epoch 11082/30000 Training Loss: 0.04724913462996483\n",
      "Epoch 11083/30000 Training Loss: 0.04860689491033554\n",
      "Epoch 11084/30000 Training Loss: 0.04915137216448784\n",
      "Epoch 11085/30000 Training Loss: 0.050380729138851166\n",
      "Epoch 11086/30000 Training Loss: 0.047849081456661224\n",
      "Epoch 11087/30000 Training Loss: 0.06631257385015488\n",
      "Epoch 11088/30000 Training Loss: 0.04134362190961838\n",
      "Epoch 11089/30000 Training Loss: 0.041677325963974\n",
      "Epoch 11090/30000 Training Loss: 0.05438578128814697\n",
      "Epoch 11091/30000 Training Loss: 0.07238941639661789\n",
      "Epoch 11092/30000 Training Loss: 0.053793635219335556\n",
      "Epoch 11093/30000 Training Loss: 0.033050697296857834\n",
      "Epoch 11094/30000 Training Loss: 0.04601937532424927\n",
      "Epoch 11095/30000 Training Loss: 0.044682033360004425\n",
      "Epoch 11096/30000 Training Loss: 0.04760713130235672\n",
      "Epoch 11097/30000 Training Loss: 0.05592045187950134\n",
      "Epoch 11098/30000 Training Loss: 0.054136279970407486\n",
      "Epoch 11099/30000 Training Loss: 0.04598115384578705\n",
      "Epoch 11100/30000 Training Loss: 0.06620617210865021\n",
      "Epoch 11100/30000 Validation Loss: 0.06151323765516281\n",
      "Epoch 11101/30000 Training Loss: 0.03800281882286072\n",
      "Epoch 11102/30000 Training Loss: 0.049270279705524445\n",
      "Epoch 11103/30000 Training Loss: 0.05283086374402046\n",
      "Epoch 11104/30000 Training Loss: 0.04698200896382332\n",
      "Epoch 11105/30000 Training Loss: 0.06525043398141861\n",
      "Epoch 11106/30000 Training Loss: 0.036230266094207764\n",
      "Epoch 11107/30000 Training Loss: 0.049500033259391785\n",
      "Epoch 11108/30000 Training Loss: 0.06045990809798241\n",
      "Epoch 11109/30000 Training Loss: 0.039883628487586975\n",
      "Epoch 11110/30000 Training Loss: 0.032427828758955\n",
      "Epoch 11111/30000 Training Loss: 0.052430495619773865\n",
      "Epoch 11112/30000 Training Loss: 0.052158113569021225\n",
      "Epoch 11113/30000 Training Loss: 0.04034453630447388\n",
      "Epoch 11114/30000 Training Loss: 0.04758969694375992\n",
      "Epoch 11115/30000 Training Loss: 0.040469393134117126\n",
      "Epoch 11116/30000 Training Loss: 0.03920917212963104\n",
      "Epoch 11117/30000 Training Loss: 0.04764696955680847\n",
      "Epoch 11118/30000 Training Loss: 0.04964154213666916\n",
      "Epoch 11119/30000 Training Loss: 0.050214916467666626\n",
      "Epoch 11120/30000 Training Loss: 0.05876605957746506\n",
      "Epoch 11121/30000 Training Loss: 0.04874879866838455\n",
      "Epoch 11122/30000 Training Loss: 0.04192861169576645\n",
      "Epoch 11123/30000 Training Loss: 0.05230218172073364\n",
      "Epoch 11124/30000 Training Loss: 0.030260365456342697\n",
      "Epoch 11125/30000 Training Loss: 0.05522593855857849\n",
      "Epoch 11126/30000 Training Loss: 0.049461692571640015\n",
      "Epoch 11127/30000 Training Loss: 0.049390166997909546\n",
      "Epoch 11128/30000 Training Loss: 0.06248375028371811\n",
      "Epoch 11129/30000 Training Loss: 0.06002651900053024\n",
      "Epoch 11130/30000 Training Loss: 0.04824903979897499\n",
      "Epoch 11131/30000 Training Loss: 0.051973823457956314\n",
      "Epoch 11132/30000 Training Loss: 0.06462959945201874\n",
      "Epoch 11133/30000 Training Loss: 0.05774763971567154\n",
      "Epoch 11134/30000 Training Loss: 0.039779745042324066\n",
      "Epoch 11135/30000 Training Loss: 0.03993086516857147\n",
      "Epoch 11136/30000 Training Loss: 0.06145649403333664\n",
      "Epoch 11137/30000 Training Loss: 0.03819689899682999\n",
      "Epoch 11138/30000 Training Loss: 0.05012483894824982\n",
      "Epoch 11139/30000 Training Loss: 0.05900079011917114\n",
      "Epoch 11140/30000 Training Loss: 0.06380251049995422\n",
      "Epoch 11141/30000 Training Loss: 0.05355588719248772\n",
      "Epoch 11142/30000 Training Loss: 0.04848194494843483\n",
      "Epoch 11143/30000 Training Loss: 0.04350905120372772\n",
      "Epoch 11144/30000 Training Loss: 0.06346716731786728\n",
      "Epoch 11145/30000 Training Loss: 0.061524003744125366\n",
      "Epoch 11146/30000 Training Loss: 0.04624786972999573\n",
      "Epoch 11147/30000 Training Loss: 0.04316023737192154\n",
      "Epoch 11148/30000 Training Loss: 0.03796718269586563\n",
      "Epoch 11149/30000 Training Loss: 0.056198664009571075\n",
      "Epoch 11150/30000 Training Loss: 0.05307847261428833\n",
      "Epoch 11151/30000 Training Loss: 0.06958520412445068\n",
      "Epoch 11152/30000 Training Loss: 0.04417578503489494\n",
      "Epoch 11153/30000 Training Loss: 0.05715750530362129\n",
      "Epoch 11154/30000 Training Loss: 0.04169786721467972\n",
      "Epoch 11155/30000 Training Loss: 0.05966327339410782\n",
      "Epoch 11156/30000 Training Loss: 0.04515979439020157\n",
      "Epoch 11157/30000 Training Loss: 0.0384560152888298\n",
      "Epoch 11158/30000 Training Loss: 0.060808215290308\n",
      "Epoch 11159/30000 Training Loss: 0.06421487033367157\n",
      "Epoch 11160/30000 Training Loss: 0.04777733236551285\n",
      "Epoch 11161/30000 Training Loss: 0.04395680874586105\n",
      "Epoch 11162/30000 Training Loss: 0.04961307346820831\n",
      "Epoch 11163/30000 Training Loss: 0.04301312565803528\n",
      "Epoch 11164/30000 Training Loss: 0.04614308103919029\n",
      "Epoch 11165/30000 Training Loss: 0.047537535429000854\n",
      "Epoch 11166/30000 Training Loss: 0.0535125806927681\n",
      "Epoch 11167/30000 Training Loss: 0.043629035353660583\n",
      "Epoch 11168/30000 Training Loss: 0.037012845277786255\n",
      "Epoch 11169/30000 Training Loss: 0.06037771329283714\n",
      "Epoch 11170/30000 Training Loss: 0.0492684468626976\n",
      "Epoch 11171/30000 Training Loss: 0.052724964916706085\n",
      "Epoch 11172/30000 Training Loss: 0.050244078040122986\n",
      "Epoch 11173/30000 Training Loss: 0.04338161647319794\n",
      "Epoch 11174/30000 Training Loss: 0.03728518262505531\n",
      "Epoch 11175/30000 Training Loss: 0.05594785511493683\n",
      "Epoch 11176/30000 Training Loss: 0.04833032935857773\n",
      "Epoch 11177/30000 Training Loss: 0.05771707370877266\n",
      "Epoch 11178/30000 Training Loss: 0.05576423183083534\n",
      "Epoch 11179/30000 Training Loss: 0.054670147597789764\n",
      "Epoch 11180/30000 Training Loss: 0.042418770492076874\n",
      "Epoch 11181/30000 Training Loss: 0.0421740785241127\n",
      "Epoch 11182/30000 Training Loss: 0.06837597489356995\n",
      "Epoch 11183/30000 Training Loss: 0.03951272740960121\n",
      "Epoch 11184/30000 Training Loss: 0.03734312206506729\n",
      "Epoch 11185/30000 Training Loss: 0.04989679902791977\n",
      "Epoch 11186/30000 Training Loss: 0.04583601653575897\n",
      "Epoch 11187/30000 Training Loss: 0.044694531708955765\n",
      "Epoch 11188/30000 Training Loss: 0.049922749400138855\n",
      "Epoch 11189/30000 Training Loss: 0.03697524964809418\n",
      "Epoch 11190/30000 Training Loss: 0.06361988931894302\n",
      "Epoch 11191/30000 Training Loss: 0.059191908687353134\n",
      "Epoch 11192/30000 Training Loss: 0.05143830552697182\n",
      "Epoch 11193/30000 Training Loss: 0.059055499732494354\n",
      "Epoch 11194/30000 Training Loss: 0.042465146631002426\n",
      "Epoch 11195/30000 Training Loss: 0.04302413761615753\n",
      "Epoch 11196/30000 Training Loss: 0.042760275304317474\n",
      "Epoch 11197/30000 Training Loss: 0.03249675780534744\n",
      "Epoch 11198/30000 Training Loss: 0.04780920594930649\n",
      "Epoch 11199/30000 Training Loss: 0.042367324233055115\n",
      "Epoch 11200/30000 Training Loss: 0.04281814396381378\n",
      "Epoch 11200/30000 Validation Loss: 0.04687172546982765\n",
      "Epoch 11201/30000 Training Loss: 0.029588285833597183\n",
      "Epoch 11202/30000 Training Loss: 0.046550530940294266\n",
      "Epoch 11203/30000 Training Loss: 0.06462197005748749\n",
      "Epoch 11204/30000 Training Loss: 0.05716143175959587\n",
      "Epoch 11205/30000 Training Loss: 0.05268145352602005\n",
      "Epoch 11206/30000 Training Loss: 0.05027460679411888\n",
      "Epoch 11207/30000 Training Loss: 0.03798285871744156\n",
      "Epoch 11208/30000 Training Loss: 0.048230141401290894\n",
      "Epoch 11209/30000 Training Loss: 0.08143667876720428\n",
      "Epoch 11210/30000 Training Loss: 0.047439657151699066\n",
      "Epoch 11211/30000 Training Loss: 0.06392279267311096\n",
      "Epoch 11212/30000 Training Loss: 0.043678589165210724\n",
      "Epoch 11213/30000 Training Loss: 0.03986980766057968\n",
      "Epoch 11214/30000 Training Loss: 0.044629793614149094\n",
      "Epoch 11215/30000 Training Loss: 0.04797336459159851\n",
      "Epoch 11216/30000 Training Loss: 0.04988639056682587\n",
      "Epoch 11217/30000 Training Loss: 0.05875404551625252\n",
      "Epoch 11218/30000 Training Loss: 0.045501984655857086\n",
      "Epoch 11219/30000 Training Loss: 0.04602796956896782\n",
      "Epoch 11220/30000 Training Loss: 0.04097147658467293\n",
      "Epoch 11221/30000 Training Loss: 0.05824096500873566\n",
      "Epoch 11222/30000 Training Loss: 0.06476717442274094\n",
      "Epoch 11223/30000 Training Loss: 0.05757556110620499\n",
      "Epoch 11224/30000 Training Loss: 0.04549526423215866\n",
      "Epoch 11225/30000 Training Loss: 0.05418241024017334\n",
      "Epoch 11226/30000 Training Loss: 0.04939514398574829\n",
      "Epoch 11227/30000 Training Loss: 0.03970731049776077\n",
      "Epoch 11228/30000 Training Loss: 0.05332554876804352\n",
      "Epoch 11229/30000 Training Loss: 0.05216372758150101\n",
      "Epoch 11230/30000 Training Loss: 0.04231398552656174\n",
      "Epoch 11231/30000 Training Loss: 0.048194266855716705\n",
      "Epoch 11232/30000 Training Loss: 0.046349383890628815\n",
      "Epoch 11233/30000 Training Loss: 0.05606165900826454\n",
      "Epoch 11234/30000 Training Loss: 0.04697989672422409\n",
      "Epoch 11235/30000 Training Loss: 0.042072050273418427\n",
      "Epoch 11236/30000 Training Loss: 0.05777435749769211\n",
      "Epoch 11237/30000 Training Loss: 0.05467367544770241\n",
      "Epoch 11238/30000 Training Loss: 0.05235134810209274\n",
      "Epoch 11239/30000 Training Loss: 0.042613591998815536\n",
      "Epoch 11240/30000 Training Loss: 0.04356987401843071\n",
      "Epoch 11241/30000 Training Loss: 0.04004010185599327\n",
      "Epoch 11242/30000 Training Loss: 0.054968178272247314\n",
      "Epoch 11243/30000 Training Loss: 0.04349007457494736\n",
      "Epoch 11244/30000 Training Loss: 0.03912941366434097\n",
      "Epoch 11245/30000 Training Loss: 0.0692807212471962\n",
      "Epoch 11246/30000 Training Loss: 0.048809587955474854\n",
      "Epoch 11247/30000 Training Loss: 0.042302824556827545\n",
      "Epoch 11248/30000 Training Loss: 0.05130269378423691\n",
      "Epoch 11249/30000 Training Loss: 0.044705525040626526\n",
      "Epoch 11250/30000 Training Loss: 0.05198676139116287\n",
      "Epoch 11251/30000 Training Loss: 0.04273319989442825\n",
      "Epoch 11252/30000 Training Loss: 0.05382118746638298\n",
      "Epoch 11253/30000 Training Loss: 0.044118791818618774\n",
      "Epoch 11254/30000 Training Loss: 0.047079622745513916\n",
      "Epoch 11255/30000 Training Loss: 0.04583723098039627\n",
      "Epoch 11256/30000 Training Loss: 0.06341113895177841\n",
      "Epoch 11257/30000 Training Loss: 0.05715610086917877\n",
      "Epoch 11258/30000 Training Loss: 0.06556356698274612\n",
      "Epoch 11259/30000 Training Loss: 0.04845837503671646\n",
      "Epoch 11260/30000 Training Loss: 0.04435413330793381\n",
      "Epoch 11261/30000 Training Loss: 0.052806831896305084\n",
      "Epoch 11262/30000 Training Loss: 0.0481685996055603\n",
      "Epoch 11263/30000 Training Loss: 0.055642738938331604\n",
      "Epoch 11264/30000 Training Loss: 0.051223404705524445\n",
      "Epoch 11265/30000 Training Loss: 0.04990895092487335\n",
      "Epoch 11266/30000 Training Loss: 0.04017540439963341\n",
      "Epoch 11267/30000 Training Loss: 0.04884098470211029\n",
      "Epoch 11268/30000 Training Loss: 0.04297850281000137\n",
      "Epoch 11269/30000 Training Loss: 0.06349850445985794\n",
      "Epoch 11270/30000 Training Loss: 0.056514911353588104\n",
      "Epoch 11271/30000 Training Loss: 0.045943550765514374\n",
      "Epoch 11272/30000 Training Loss: 0.03501904755830765\n",
      "Epoch 11273/30000 Training Loss: 0.03890305757522583\n",
      "Epoch 11274/30000 Training Loss: 0.03605901077389717\n",
      "Epoch 11275/30000 Training Loss: 0.056495629251003265\n",
      "Epoch 11276/30000 Training Loss: 0.0376790314912796\n",
      "Epoch 11277/30000 Training Loss: 0.049610063433647156\n",
      "Epoch 11278/30000 Training Loss: 0.03700786828994751\n",
      "Epoch 11279/30000 Training Loss: 0.04366841912269592\n",
      "Epoch 11280/30000 Training Loss: 0.049110956490039825\n",
      "Epoch 11281/30000 Training Loss: 0.0512695275247097\n",
      "Epoch 11282/30000 Training Loss: 0.044831983745098114\n",
      "Epoch 11283/30000 Training Loss: 0.047364458441734314\n",
      "Epoch 11284/30000 Training Loss: 0.05591735243797302\n",
      "Epoch 11285/30000 Training Loss: 0.06019756942987442\n",
      "Epoch 11286/30000 Training Loss: 0.05004515126347542\n",
      "Epoch 11287/30000 Training Loss: 0.038841500878334045\n",
      "Epoch 11288/30000 Training Loss: 0.042487531900405884\n",
      "Epoch 11289/30000 Training Loss: 0.06480325013399124\n",
      "Epoch 11290/30000 Training Loss: 0.0502275787293911\n",
      "Epoch 11291/30000 Training Loss: 0.0424201674759388\n",
      "Epoch 11292/30000 Training Loss: 0.045140184462070465\n",
      "Epoch 11293/30000 Training Loss: 0.04348576068878174\n",
      "Epoch 11294/30000 Training Loss: 0.041695401072502136\n",
      "Epoch 11295/30000 Training Loss: 0.03463631868362427\n",
      "Epoch 11296/30000 Training Loss: 0.03390229865908623\n",
      "Epoch 11297/30000 Training Loss: 0.05369650572538376\n",
      "Epoch 11298/30000 Training Loss: 0.05342154949903488\n",
      "Epoch 11299/30000 Training Loss: 0.06175945699214935\n",
      "Epoch 11300/30000 Training Loss: 0.057916589081287384\n",
      "Epoch 11300/30000 Validation Loss: 0.04307306557893753\n",
      "Epoch 11301/30000 Training Loss: 0.047808825969696045\n",
      "Epoch 11302/30000 Training Loss: 0.05500762537121773\n",
      "Epoch 11303/30000 Training Loss: 0.061696141958236694\n",
      "Epoch 11304/30000 Training Loss: 0.05214117467403412\n",
      "Epoch 11305/30000 Training Loss: 0.034451842308044434\n",
      "Epoch 11306/30000 Training Loss: 0.05101783946156502\n",
      "Epoch 11307/30000 Training Loss: 0.04756665974855423\n",
      "Epoch 11308/30000 Training Loss: 0.047685299068689346\n",
      "Epoch 11309/30000 Training Loss: 0.04689487814903259\n",
      "Epoch 11310/30000 Training Loss: 0.03650233522057533\n",
      "Epoch 11311/30000 Training Loss: 0.05629286542534828\n",
      "Epoch 11312/30000 Training Loss: 0.053889356553554535\n",
      "Epoch 11313/30000 Training Loss: 0.05569962412118912\n",
      "Epoch 11314/30000 Training Loss: 0.045208320021629333\n",
      "Epoch 11315/30000 Training Loss: 0.03688318654894829\n",
      "Epoch 11316/30000 Training Loss: 0.06493991613388062\n",
      "Epoch 11317/30000 Training Loss: 0.05176134407520294\n",
      "Epoch 11318/30000 Training Loss: 0.04018603265285492\n",
      "Epoch 11319/30000 Training Loss: 0.03980468958616257\n",
      "Epoch 11320/30000 Training Loss: 0.0383378267288208\n",
      "Epoch 11321/30000 Training Loss: 0.049613188952207565\n",
      "Epoch 11322/30000 Training Loss: 0.06254863739013672\n",
      "Epoch 11323/30000 Training Loss: 0.056893378496170044\n",
      "Epoch 11324/30000 Training Loss: 0.05007128417491913\n",
      "Epoch 11325/30000 Training Loss: 0.04060560464859009\n",
      "Epoch 11326/30000 Training Loss: 0.043854162096977234\n",
      "Epoch 11327/30000 Training Loss: 0.06833390146493912\n",
      "Epoch 11328/30000 Training Loss: 0.03854742646217346\n",
      "Epoch 11329/30000 Training Loss: 0.056737273931503296\n",
      "Epoch 11330/30000 Training Loss: 0.04085969552397728\n",
      "Epoch 11331/30000 Training Loss: 0.05228776857256889\n",
      "Epoch 11332/30000 Training Loss: 0.03914516419172287\n",
      "Epoch 11333/30000 Training Loss: 0.052221834659576416\n",
      "Epoch 11334/30000 Training Loss: 0.038517728447914124\n",
      "Epoch 11335/30000 Training Loss: 0.05017688125371933\n",
      "Epoch 11336/30000 Training Loss: 0.04691009223461151\n",
      "Epoch 11337/30000 Training Loss: 0.0529676117002964\n",
      "Epoch 11338/30000 Training Loss: 0.052418313920497894\n",
      "Epoch 11339/30000 Training Loss: 0.04699580371379852\n",
      "Epoch 11340/30000 Training Loss: 0.04033522307872772\n",
      "Epoch 11341/30000 Training Loss: 0.03840256482362747\n",
      "Epoch 11342/30000 Training Loss: 0.05395234376192093\n",
      "Epoch 11343/30000 Training Loss: 0.039184581488370895\n",
      "Epoch 11344/30000 Training Loss: 0.052777767181396484\n",
      "Epoch 11345/30000 Training Loss: 0.039653100073337555\n",
      "Epoch 11346/30000 Training Loss: 0.046343449503183365\n",
      "Epoch 11347/30000 Training Loss: 0.0388447642326355\n",
      "Epoch 11348/30000 Training Loss: 0.039032429456710815\n",
      "Epoch 11349/30000 Training Loss: 0.045128945261240005\n",
      "Epoch 11350/30000 Training Loss: 0.043491341173648834\n",
      "Epoch 11351/30000 Training Loss: 0.05259830504655838\n",
      "Epoch 11352/30000 Training Loss: 0.04113825410604477\n",
      "Epoch 11353/30000 Training Loss: 0.04614482820034027\n",
      "Epoch 11354/30000 Training Loss: 0.04037448763847351\n",
      "Epoch 11355/30000 Training Loss: 0.05378670245409012\n",
      "Epoch 11356/30000 Training Loss: 0.06531808525323868\n",
      "Epoch 11357/30000 Training Loss: 0.043248459696769714\n",
      "Epoch 11358/30000 Training Loss: 0.05036257207393646\n",
      "Epoch 11359/30000 Training Loss: 0.05849727988243103\n",
      "Epoch 11360/30000 Training Loss: 0.044109564274549484\n",
      "Epoch 11361/30000 Training Loss: 0.05158156529068947\n",
      "Epoch 11362/30000 Training Loss: 0.04831183701753616\n",
      "Epoch 11363/30000 Training Loss: 0.03536774963140488\n",
      "Epoch 11364/30000 Training Loss: 0.042673707008361816\n",
      "Epoch 11365/30000 Training Loss: 0.04094061255455017\n",
      "Epoch 11366/30000 Training Loss: 0.03264712542295456\n",
      "Epoch 11367/30000 Training Loss: 0.06252624094486237\n",
      "Epoch 11368/30000 Training Loss: 0.05712541937828064\n",
      "Epoch 11369/30000 Training Loss: 0.07231540977954865\n",
      "Epoch 11370/30000 Training Loss: 0.049462463706731796\n",
      "Epoch 11371/30000 Training Loss: 0.05317678302526474\n",
      "Epoch 11372/30000 Training Loss: 0.044734589755535126\n",
      "Epoch 11373/30000 Training Loss: 0.05921259894967079\n",
      "Epoch 11374/30000 Training Loss: 0.050056006759405136\n",
      "Epoch 11375/30000 Training Loss: 0.051556333899497986\n",
      "Epoch 11376/30000 Training Loss: 0.04498619586229324\n",
      "Epoch 11377/30000 Training Loss: 0.038577377796173096\n",
      "Epoch 11378/30000 Training Loss: 0.05022774264216423\n",
      "Epoch 11379/30000 Training Loss: 0.045304372906684875\n",
      "Epoch 11380/30000 Training Loss: 0.048985570669174194\n",
      "Epoch 11381/30000 Training Loss: 0.05579729378223419\n",
      "Epoch 11382/30000 Training Loss: 0.05019000172615051\n",
      "Epoch 11383/30000 Training Loss: 0.05963936820626259\n",
      "Epoch 11384/30000 Training Loss: 0.056727975606918335\n",
      "Epoch 11385/30000 Training Loss: 0.04626030474901199\n",
      "Epoch 11386/30000 Training Loss: 0.0435236319899559\n",
      "Epoch 11387/30000 Training Loss: 0.045570991933345795\n",
      "Epoch 11388/30000 Training Loss: 0.038831692188978195\n",
      "Epoch 11389/30000 Training Loss: 0.05451098829507828\n",
      "Epoch 11390/30000 Training Loss: 0.06760871410369873\n",
      "Epoch 11391/30000 Training Loss: 0.042311541736125946\n",
      "Epoch 11392/30000 Training Loss: 0.04693678766489029\n",
      "Epoch 11393/30000 Training Loss: 0.04946755990386009\n",
      "Epoch 11394/30000 Training Loss: 0.043832916766405106\n",
      "Epoch 11395/30000 Training Loss: 0.0598379522562027\n",
      "Epoch 11396/30000 Training Loss: 0.045907068997621536\n",
      "Epoch 11397/30000 Training Loss: 0.04589196294546127\n",
      "Epoch 11398/30000 Training Loss: 0.033804990351200104\n",
      "Epoch 11399/30000 Training Loss: 0.04731389880180359\n",
      "Epoch 11400/30000 Training Loss: 0.04918447509407997\n",
      "Epoch 11400/30000 Validation Loss: 0.05025217682123184\n",
      "Epoch 11401/30000 Training Loss: 0.04193257912993431\n",
      "Epoch 11402/30000 Training Loss: 0.0453241765499115\n",
      "Epoch 11403/30000 Training Loss: 0.04354194551706314\n",
      "Epoch 11404/30000 Training Loss: 0.03850327432155609\n",
      "Epoch 11405/30000 Training Loss: 0.05431745946407318\n",
      "Epoch 11406/30000 Training Loss: 0.05185499042272568\n",
      "Epoch 11407/30000 Training Loss: 0.04294748604297638\n",
      "Epoch 11408/30000 Training Loss: 0.04429066926240921\n",
      "Epoch 11409/30000 Training Loss: 0.05489227548241615\n",
      "Epoch 11410/30000 Training Loss: 0.047926656901836395\n",
      "Epoch 11411/30000 Training Loss: 0.03653870150446892\n",
      "Epoch 11412/30000 Training Loss: 0.03800339996814728\n",
      "Epoch 11413/30000 Training Loss: 0.04641631245613098\n",
      "Epoch 11414/30000 Training Loss: 0.04508623480796814\n",
      "Epoch 11415/30000 Training Loss: 0.056979842483997345\n",
      "Epoch 11416/30000 Training Loss: 0.061153095215559006\n",
      "Epoch 11417/30000 Training Loss: 0.04377608373761177\n",
      "Epoch 11418/30000 Training Loss: 0.03818303719162941\n",
      "Epoch 11419/30000 Training Loss: 0.04547888785600662\n",
      "Epoch 11420/30000 Training Loss: 0.043259359896183014\n",
      "Epoch 11421/30000 Training Loss: 0.0665116012096405\n",
      "Epoch 11422/30000 Training Loss: 0.03519020229578018\n",
      "Epoch 11423/30000 Training Loss: 0.05353847146034241\n",
      "Epoch 11424/30000 Training Loss: 0.042305342853069305\n",
      "Epoch 11425/30000 Training Loss: 0.049095503985881805\n",
      "Epoch 11426/30000 Training Loss: 0.04514826089143753\n",
      "Epoch 11427/30000 Training Loss: 0.05743240565061569\n",
      "Epoch 11428/30000 Training Loss: 0.04490771144628525\n",
      "Epoch 11429/30000 Training Loss: 0.04547770693898201\n",
      "Epoch 11430/30000 Training Loss: 0.047916874289512634\n",
      "Epoch 11431/30000 Training Loss: 0.07281074672937393\n",
      "Epoch 11432/30000 Training Loss: 0.05098353326320648\n",
      "Epoch 11433/30000 Training Loss: 0.045717790722846985\n",
      "Epoch 11434/30000 Training Loss: 0.04380425810813904\n",
      "Epoch 11435/30000 Training Loss: 0.047589004039764404\n",
      "Epoch 11436/30000 Training Loss: 0.05619363114237785\n",
      "Epoch 11437/30000 Training Loss: 0.05630280077457428\n",
      "Epoch 11438/30000 Training Loss: 0.04716162383556366\n",
      "Epoch 11439/30000 Training Loss: 0.05504924803972244\n",
      "Epoch 11440/30000 Training Loss: 0.046070490032434464\n",
      "Epoch 11441/30000 Training Loss: 0.06736771762371063\n",
      "Epoch 11442/30000 Training Loss: 0.04578295350074768\n",
      "Epoch 11443/30000 Training Loss: 0.05753558874130249\n",
      "Epoch 11444/30000 Training Loss: 0.04903171956539154\n",
      "Epoch 11445/30000 Training Loss: 0.04615553095936775\n",
      "Epoch 11446/30000 Training Loss: 0.04477998614311218\n",
      "Epoch 11447/30000 Training Loss: 0.04912940412759781\n",
      "Epoch 11448/30000 Training Loss: 0.03785363584756851\n",
      "Epoch 11449/30000 Training Loss: 0.04250340163707733\n",
      "Epoch 11450/30000 Training Loss: 0.04690641164779663\n",
      "Epoch 11451/30000 Training Loss: 0.05242843180894852\n",
      "Epoch 11452/30000 Training Loss: 0.04494289308786392\n",
      "Epoch 11453/30000 Training Loss: 0.0349653996527195\n",
      "Epoch 11454/30000 Training Loss: 0.0455189049243927\n",
      "Epoch 11455/30000 Training Loss: 0.05170720815658569\n",
      "Epoch 11456/30000 Training Loss: 0.04438851773738861\n",
      "Epoch 11457/30000 Training Loss: 0.0531575083732605\n",
      "Epoch 11458/30000 Training Loss: 0.04929943382740021\n",
      "Epoch 11459/30000 Training Loss: 0.04094861447811127\n",
      "Epoch 11460/30000 Training Loss: 0.03654613345861435\n",
      "Epoch 11461/30000 Training Loss: 0.05006640776991844\n",
      "Epoch 11462/30000 Training Loss: 0.06580039858818054\n",
      "Epoch 11463/30000 Training Loss: 0.037159547209739685\n",
      "Epoch 11464/30000 Training Loss: 0.054147593677043915\n",
      "Epoch 11465/30000 Training Loss: 0.05808945372700691\n",
      "Epoch 11466/30000 Training Loss: 0.057718221098184586\n",
      "Epoch 11467/30000 Training Loss: 0.04149175062775612\n",
      "Epoch 11468/30000 Training Loss: 0.0580044761300087\n",
      "Epoch 11469/30000 Training Loss: 0.04026425629854202\n",
      "Epoch 11470/30000 Training Loss: 0.04753384739160538\n",
      "Epoch 11471/30000 Training Loss: 0.0557204969227314\n",
      "Epoch 11472/30000 Training Loss: 0.03895873576402664\n",
      "Epoch 11473/30000 Training Loss: 0.05189119651913643\n",
      "Epoch 11474/30000 Training Loss: 0.05660723149776459\n",
      "Epoch 11475/30000 Training Loss: 0.041068751364946365\n",
      "Epoch 11476/30000 Training Loss: 0.06260722875595093\n",
      "Epoch 11477/30000 Training Loss: 0.04603050649166107\n",
      "Epoch 11478/30000 Training Loss: 0.0503254160284996\n",
      "Epoch 11479/30000 Training Loss: 0.04716665297746658\n",
      "Epoch 11480/30000 Training Loss: 0.056691210716962814\n",
      "Epoch 11481/30000 Training Loss: 0.056315694004297256\n",
      "Epoch 11482/30000 Training Loss: 0.06654010713100433\n",
      "Epoch 11483/30000 Training Loss: 0.03710944950580597\n",
      "Epoch 11484/30000 Training Loss: 0.0505543127655983\n",
      "Epoch 11485/30000 Training Loss: 0.05641486495733261\n",
      "Epoch 11486/30000 Training Loss: 0.052187345921993256\n",
      "Epoch 11487/30000 Training Loss: 0.05511505529284477\n",
      "Epoch 11488/30000 Training Loss: 0.0404142290353775\n",
      "Epoch 11489/30000 Training Loss: 0.038519926369190216\n",
      "Epoch 11490/30000 Training Loss: 0.06540277600288391\n",
      "Epoch 11491/30000 Training Loss: 0.054289158433675766\n",
      "Epoch 11492/30000 Training Loss: 0.04891837760806084\n",
      "Epoch 11493/30000 Training Loss: 0.05794695019721985\n",
      "Epoch 11494/30000 Training Loss: 0.03784903138875961\n",
      "Epoch 11495/30000 Training Loss: 0.03748050704598427\n",
      "Epoch 11496/30000 Training Loss: 0.03373541682958603\n",
      "Epoch 11497/30000 Training Loss: 0.04481343552470207\n",
      "Epoch 11498/30000 Training Loss: 0.04399624839425087\n",
      "Epoch 11499/30000 Training Loss: 0.046478286385536194\n",
      "Epoch 11500/30000 Training Loss: 0.04370424151420593\n",
      "Epoch 11500/30000 Validation Loss: 0.0631856918334961\n",
      "Epoch 11501/30000 Training Loss: 0.0369715690612793\n",
      "Epoch 11502/30000 Training Loss: 0.04151438921689987\n",
      "Epoch 11503/30000 Training Loss: 0.06841601431369781\n",
      "Epoch 11504/30000 Training Loss: 0.045443665236234665\n",
      "Epoch 11505/30000 Training Loss: 0.048125725239515305\n",
      "Epoch 11506/30000 Training Loss: 0.03601124882698059\n",
      "Epoch 11507/30000 Training Loss: 0.04015957564115524\n",
      "Epoch 11508/30000 Training Loss: 0.04625203460454941\n",
      "Epoch 11509/30000 Training Loss: 0.05912473425269127\n",
      "Epoch 11510/30000 Training Loss: 0.050940416753292084\n",
      "Epoch 11511/30000 Training Loss: 0.043274521827697754\n",
      "Epoch 11512/30000 Training Loss: 0.04515530541539192\n",
      "Epoch 11513/30000 Training Loss: 0.03950557857751846\n",
      "Epoch 11514/30000 Training Loss: 0.04529751092195511\n",
      "Epoch 11515/30000 Training Loss: 0.04889165610074997\n",
      "Epoch 11516/30000 Training Loss: 0.046519532799720764\n",
      "Epoch 11517/30000 Training Loss: 0.048612162470817566\n",
      "Epoch 11518/30000 Training Loss: 0.041943471878767014\n",
      "Epoch 11519/30000 Training Loss: 0.05186505243182182\n",
      "Epoch 11520/30000 Training Loss: 0.05047663301229477\n",
      "Epoch 11521/30000 Training Loss: 0.050738729536533356\n",
      "Epoch 11522/30000 Training Loss: 0.03837287425994873\n",
      "Epoch 11523/30000 Training Loss: 0.045231375843286514\n",
      "Epoch 11524/30000 Training Loss: 0.042733289301395416\n",
      "Epoch 11525/30000 Training Loss: 0.057987239211797714\n",
      "Epoch 11526/30000 Training Loss: 0.04916263371706009\n",
      "Epoch 11527/30000 Training Loss: 0.06281975656747818\n",
      "Epoch 11528/30000 Training Loss: 0.041595734655857086\n",
      "Epoch 11529/30000 Training Loss: 0.04723762720823288\n",
      "Epoch 11530/30000 Training Loss: 0.04040735587477684\n",
      "Epoch 11531/30000 Training Loss: 0.042665429413318634\n",
      "Epoch 11532/30000 Training Loss: 0.04600216820836067\n",
      "Epoch 11533/30000 Training Loss: 0.03671375289559364\n",
      "Epoch 11534/30000 Training Loss: 0.04606817290186882\n",
      "Epoch 11535/30000 Training Loss: 0.06482207775115967\n",
      "Epoch 11536/30000 Training Loss: 0.062292613089084625\n",
      "Epoch 11537/30000 Training Loss: 0.06293050199747086\n",
      "Epoch 11538/30000 Training Loss: 0.05508904159069061\n",
      "Epoch 11539/30000 Training Loss: 0.0709807351231575\n",
      "Epoch 11540/30000 Training Loss: 0.06327870488166809\n",
      "Epoch 11541/30000 Training Loss: 0.04939499497413635\n",
      "Epoch 11542/30000 Training Loss: 0.051146864891052246\n",
      "Epoch 11543/30000 Training Loss: 0.04823753982782364\n",
      "Epoch 11544/30000 Training Loss: 0.04594343900680542\n",
      "Epoch 11545/30000 Training Loss: 0.051407590508461\n",
      "Epoch 11546/30000 Training Loss: 0.04788011312484741\n",
      "Epoch 11547/30000 Training Loss: 0.03873880207538605\n",
      "Epoch 11548/30000 Training Loss: 0.07299117743968964\n",
      "Epoch 11549/30000 Training Loss: 0.03684177249670029\n",
      "Epoch 11550/30000 Training Loss: 0.056238383054733276\n",
      "Epoch 11551/30000 Training Loss: 0.05209849402308464\n",
      "Epoch 11552/30000 Training Loss: 0.04818134009838104\n",
      "Epoch 11553/30000 Training Loss: 0.05792523920536041\n",
      "Epoch 11554/30000 Training Loss: 0.04986801743507385\n",
      "Epoch 11555/30000 Training Loss: 0.05828288197517395\n",
      "Epoch 11556/30000 Training Loss: 0.04546982795000076\n",
      "Epoch 11557/30000 Training Loss: 0.03811817616224289\n",
      "Epoch 11558/30000 Training Loss: 0.04160893335938454\n",
      "Epoch 11559/30000 Training Loss: 0.05630058795213699\n",
      "Epoch 11560/30000 Training Loss: 0.05084596201777458\n",
      "Epoch 11561/30000 Training Loss: 0.04718394577503204\n",
      "Epoch 11562/30000 Training Loss: 0.05558472126722336\n",
      "Epoch 11563/30000 Training Loss: 0.05475232005119324\n",
      "Epoch 11564/30000 Training Loss: 0.053389906883239746\n",
      "Epoch 11565/30000 Training Loss: 0.04366804659366608\n",
      "Epoch 11566/30000 Training Loss: 0.04573187977075577\n",
      "Epoch 11567/30000 Training Loss: 0.04092790558934212\n",
      "Epoch 11568/30000 Training Loss: 0.05022431164979935\n",
      "Epoch 11569/30000 Training Loss: 0.04354657977819443\n",
      "Epoch 11570/30000 Training Loss: 0.0352010577917099\n",
      "Epoch 11571/30000 Training Loss: 0.04977712780237198\n",
      "Epoch 11572/30000 Training Loss: 0.06021186709403992\n",
      "Epoch 11573/30000 Training Loss: 0.04828749597072601\n",
      "Epoch 11574/30000 Training Loss: 0.05107991024851799\n",
      "Epoch 11575/30000 Training Loss: 0.04159339889883995\n",
      "Epoch 11576/30000 Training Loss: 0.043940767645835876\n",
      "Epoch 11577/30000 Training Loss: 0.04475444182753563\n",
      "Epoch 11578/30000 Training Loss: 0.04881392419338226\n",
      "Epoch 11579/30000 Training Loss: 0.04511038586497307\n",
      "Epoch 11580/30000 Training Loss: 0.05458752065896988\n",
      "Epoch 11581/30000 Training Loss: 0.04209359362721443\n",
      "Epoch 11582/30000 Training Loss: 0.05973861739039421\n",
      "Epoch 11583/30000 Training Loss: 0.06174052879214287\n",
      "Epoch 11584/30000 Training Loss: 0.035389672964811325\n",
      "Epoch 11585/30000 Training Loss: 0.040038131177425385\n",
      "Epoch 11586/30000 Training Loss: 0.05659635737538338\n",
      "Epoch 11587/30000 Training Loss: 0.047331102192401886\n",
      "Epoch 11588/30000 Training Loss: 0.04992758855223656\n",
      "Epoch 11589/30000 Training Loss: 0.048385828733444214\n",
      "Epoch 11590/30000 Training Loss: 0.046185605227947235\n",
      "Epoch 11591/30000 Training Loss: 0.059872351586818695\n",
      "Epoch 11592/30000 Training Loss: 0.06123775988817215\n",
      "Epoch 11593/30000 Training Loss: 0.05500589311122894\n",
      "Epoch 11594/30000 Training Loss: 0.05327843874692917\n",
      "Epoch 11595/30000 Training Loss: 0.04349670931696892\n",
      "Epoch 11596/30000 Training Loss: 0.052347905933856964\n",
      "Epoch 11597/30000 Training Loss: 0.05368875339627266\n",
      "Epoch 11598/30000 Training Loss: 0.05522220954298973\n",
      "Epoch 11599/30000 Training Loss: 0.04109497740864754\n",
      "Epoch 11600/30000 Training Loss: 0.039831794798374176\n",
      "Epoch 11600/30000 Validation Loss: 0.04355207085609436\n",
      "Epoch 11601/30000 Training Loss: 0.031181327998638153\n",
      "Epoch 11602/30000 Training Loss: 0.05802135914564133\n",
      "Epoch 11603/30000 Training Loss: 0.045194000005722046\n",
      "Epoch 11604/30000 Training Loss: 0.049812134355306625\n",
      "Epoch 11605/30000 Training Loss: 0.04811588302254677\n",
      "Epoch 11606/30000 Training Loss: 0.05515347793698311\n",
      "Epoch 11607/30000 Training Loss: 0.03804687783122063\n",
      "Epoch 11608/30000 Training Loss: 0.05697742477059364\n",
      "Epoch 11609/30000 Training Loss: 0.05514593422412872\n",
      "Epoch 11610/30000 Training Loss: 0.03755359724164009\n",
      "Epoch 11611/30000 Training Loss: 0.0476967915892601\n",
      "Epoch 11612/30000 Training Loss: 0.043692123144865036\n",
      "Epoch 11613/30000 Training Loss: 0.05408391356468201\n",
      "Epoch 11614/30000 Training Loss: 0.04466312378644943\n",
      "Epoch 11615/30000 Training Loss: 0.05088956654071808\n",
      "Epoch 11616/30000 Training Loss: 0.04447202384471893\n",
      "Epoch 11617/30000 Training Loss: 0.04446320980787277\n",
      "Epoch 11618/30000 Training Loss: 0.052108898758888245\n",
      "Epoch 11619/30000 Training Loss: 0.050760962069034576\n",
      "Epoch 11620/30000 Training Loss: 0.051699198782444\n",
      "Epoch 11621/30000 Training Loss: 0.046228013932704926\n",
      "Epoch 11622/30000 Training Loss: 0.03500071540474892\n",
      "Epoch 11623/30000 Training Loss: 0.040824368596076965\n",
      "Epoch 11624/30000 Training Loss: 0.04862325266003609\n",
      "Epoch 11625/30000 Training Loss: 0.055193737149238586\n",
      "Epoch 11626/30000 Training Loss: 0.04761563241481781\n",
      "Epoch 11627/30000 Training Loss: 0.05475212633609772\n",
      "Epoch 11628/30000 Training Loss: 0.0413387268781662\n",
      "Epoch 11629/30000 Training Loss: 0.059032127261161804\n",
      "Epoch 11630/30000 Training Loss: 0.05094254016876221\n",
      "Epoch 11631/30000 Training Loss: 0.05182100832462311\n",
      "Epoch 11632/30000 Training Loss: 0.03615443781018257\n",
      "Epoch 11633/30000 Training Loss: 0.04063882306218147\n",
      "Epoch 11634/30000 Training Loss: 0.05729503184556961\n",
      "Epoch 11635/30000 Training Loss: 0.032355017960071564\n",
      "Epoch 11636/30000 Training Loss: 0.058930061757564545\n",
      "Epoch 11637/30000 Training Loss: 0.05526353418827057\n",
      "Epoch 11638/30000 Training Loss: 0.04427577555179596\n",
      "Epoch 11639/30000 Training Loss: 0.05612754821777344\n",
      "Epoch 11640/30000 Training Loss: 0.04476650059223175\n",
      "Epoch 11641/30000 Training Loss: 0.03598766773939133\n",
      "Epoch 11642/30000 Training Loss: 0.051990341395139694\n",
      "Epoch 11643/30000 Training Loss: 0.05010458827018738\n",
      "Epoch 11644/30000 Training Loss: 0.053963400423526764\n",
      "Epoch 11645/30000 Training Loss: 0.03535500168800354\n",
      "Epoch 11646/30000 Training Loss: 0.038293469697237015\n",
      "Epoch 11647/30000 Training Loss: 0.05156221240758896\n",
      "Epoch 11648/30000 Training Loss: 0.044391147792339325\n",
      "Epoch 11649/30000 Training Loss: 0.058832962065935135\n",
      "Epoch 11650/30000 Training Loss: 0.04890386760234833\n",
      "Epoch 11651/30000 Training Loss: 0.053647350519895554\n",
      "Epoch 11652/30000 Training Loss: 0.04289594292640686\n",
      "Epoch 11653/30000 Training Loss: 0.040141794830560684\n",
      "Epoch 11654/30000 Training Loss: 0.048219066113233566\n",
      "Epoch 11655/30000 Training Loss: 0.03601931035518646\n",
      "Epoch 11656/30000 Training Loss: 0.05897946655750275\n",
      "Epoch 11657/30000 Training Loss: 0.04252288490533829\n",
      "Epoch 11658/30000 Training Loss: 0.05259944871068001\n",
      "Epoch 11659/30000 Training Loss: 0.0652347281575203\n",
      "Epoch 11660/30000 Training Loss: 0.05023642256855965\n",
      "Epoch 11661/30000 Training Loss: 0.05068673565983772\n",
      "Epoch 11662/30000 Training Loss: 0.052073318511247635\n",
      "Epoch 11663/30000 Training Loss: 0.04266485944390297\n",
      "Epoch 11664/30000 Training Loss: 0.048377230763435364\n",
      "Epoch 11665/30000 Training Loss: 0.05170299485325813\n",
      "Epoch 11666/30000 Training Loss: 0.04241922125220299\n",
      "Epoch 11667/30000 Training Loss: 0.04936813563108444\n",
      "Epoch 11668/30000 Training Loss: 0.04219307005405426\n",
      "Epoch 11669/30000 Training Loss: 0.05881473794579506\n",
      "Epoch 11670/30000 Training Loss: 0.03533438965678215\n",
      "Epoch 11671/30000 Training Loss: 0.047457434237003326\n",
      "Epoch 11672/30000 Training Loss: 0.06389236450195312\n",
      "Epoch 11673/30000 Training Loss: 0.05887671560049057\n",
      "Epoch 11674/30000 Training Loss: 0.04547542333602905\n",
      "Epoch 11675/30000 Training Loss: 0.05265446752309799\n",
      "Epoch 11676/30000 Training Loss: 0.04933194816112518\n",
      "Epoch 11677/30000 Training Loss: 0.05014419928193092\n",
      "Epoch 11678/30000 Training Loss: 0.05071299523115158\n",
      "Epoch 11679/30000 Training Loss: 0.04787825793027878\n",
      "Epoch 11680/30000 Training Loss: 0.05734799802303314\n",
      "Epoch 11681/30000 Training Loss: 0.0424075573682785\n",
      "Epoch 11682/30000 Training Loss: 0.04166160523891449\n",
      "Epoch 11683/30000 Training Loss: 0.05367787927389145\n",
      "Epoch 11684/30000 Training Loss: 0.04302850365638733\n",
      "Epoch 11685/30000 Training Loss: 0.0455532968044281\n",
      "Epoch 11686/30000 Training Loss: 0.06470456719398499\n",
      "Epoch 11687/30000 Training Loss: 0.04679115116596222\n",
      "Epoch 11688/30000 Training Loss: 0.03794573247432709\n",
      "Epoch 11689/30000 Training Loss: 0.04457865282893181\n",
      "Epoch 11690/30000 Training Loss: 0.03841521963477135\n",
      "Epoch 11691/30000 Training Loss: 0.03698989003896713\n",
      "Epoch 11692/30000 Training Loss: 0.048185206949710846\n",
      "Epoch 11693/30000 Training Loss: 0.04646950215101242\n",
      "Epoch 11694/30000 Training Loss: 0.044472452253103256\n",
      "Epoch 11695/30000 Training Loss: 0.0658319965004921\n",
      "Epoch 11696/30000 Training Loss: 0.0492631159722805\n",
      "Epoch 11697/30000 Training Loss: 0.05077871307730675\n",
      "Epoch 11698/30000 Training Loss: 0.03620707616209984\n",
      "Epoch 11699/30000 Training Loss: 0.0436614453792572\n",
      "Epoch 11700/30000 Training Loss: 0.07124333083629608\n",
      "Epoch 11700/30000 Validation Loss: 0.05141575634479523\n",
      "Epoch 11701/30000 Training Loss: 0.057568565011024475\n",
      "Epoch 11702/30000 Training Loss: 0.04184575378894806\n",
      "Epoch 11703/30000 Training Loss: 0.04238943010568619\n",
      "Epoch 11704/30000 Training Loss: 0.05646645277738571\n",
      "Epoch 11705/30000 Training Loss: 0.03904131427407265\n",
      "Epoch 11706/30000 Training Loss: 0.05283968150615692\n",
      "Epoch 11707/30000 Training Loss: 0.04468083754181862\n",
      "Epoch 11708/30000 Training Loss: 0.03888145089149475\n",
      "Epoch 11709/30000 Training Loss: 0.05069673806428909\n",
      "Epoch 11710/30000 Training Loss: 0.036839116364717484\n",
      "Epoch 11711/30000 Training Loss: 0.041625335812568665\n",
      "Epoch 11712/30000 Training Loss: 0.0534692257642746\n",
      "Epoch 11713/30000 Training Loss: 0.043628543615341187\n",
      "Epoch 11714/30000 Training Loss: 0.04643775150179863\n",
      "Epoch 11715/30000 Training Loss: 0.04710400104522705\n",
      "Epoch 11716/30000 Training Loss: 0.052332550287246704\n",
      "Epoch 11717/30000 Training Loss: 0.03673931956291199\n",
      "Epoch 11718/30000 Training Loss: 0.04855579510331154\n",
      "Epoch 11719/30000 Training Loss: 0.05583343654870987\n",
      "Epoch 11720/30000 Training Loss: 0.043677303940057755\n",
      "Epoch 11721/30000 Training Loss: 0.029817158356308937\n",
      "Epoch 11722/30000 Training Loss: 0.050806570798158646\n",
      "Epoch 11723/30000 Training Loss: 0.040996238589286804\n",
      "Epoch 11724/30000 Training Loss: 0.053936123847961426\n",
      "Epoch 11725/30000 Training Loss: 0.045622218400239944\n",
      "Epoch 11726/30000 Training Loss: 0.04341595619916916\n",
      "Epoch 11727/30000 Training Loss: 0.05960017442703247\n",
      "Epoch 11728/30000 Training Loss: 0.04610253870487213\n",
      "Epoch 11729/30000 Training Loss: 0.045852623879909515\n",
      "Epoch 11730/30000 Training Loss: 0.04169489070773125\n",
      "Epoch 11731/30000 Training Loss: 0.054681479930877686\n",
      "Epoch 11732/30000 Training Loss: 0.054814837872982025\n",
      "Epoch 11733/30000 Training Loss: 0.039569295942783356\n",
      "Epoch 11734/30000 Training Loss: 0.057947129011154175\n",
      "Epoch 11735/30000 Training Loss: 0.047006942331790924\n",
      "Epoch 11736/30000 Training Loss: 0.051563821732997894\n",
      "Epoch 11737/30000 Training Loss: 0.04517656937241554\n",
      "Epoch 11738/30000 Training Loss: 0.04487117379903793\n",
      "Epoch 11739/30000 Training Loss: 0.04486807435750961\n",
      "Epoch 11740/30000 Training Loss: 0.0471806526184082\n",
      "Epoch 11741/30000 Training Loss: 0.0407148152589798\n",
      "Epoch 11742/30000 Training Loss: 0.06439471989870071\n",
      "Epoch 11743/30000 Training Loss: 0.05786168575286865\n",
      "Epoch 11744/30000 Training Loss: 0.04395153746008873\n",
      "Epoch 11745/30000 Training Loss: 0.0790177434682846\n",
      "Epoch 11746/30000 Training Loss: 0.03974054008722305\n",
      "Epoch 11747/30000 Training Loss: 0.04451281204819679\n",
      "Epoch 11748/30000 Training Loss: 0.060665860772132874\n",
      "Epoch 11749/30000 Training Loss: 0.060081981122493744\n",
      "Epoch 11750/30000 Training Loss: 0.052073657512664795\n",
      "Epoch 11751/30000 Training Loss: 0.0449313148856163\n",
      "Epoch 11752/30000 Training Loss: 0.046450212597846985\n",
      "Epoch 11753/30000 Training Loss: 0.0639309212565422\n",
      "Epoch 11754/30000 Training Loss: 0.04321347177028656\n",
      "Epoch 11755/30000 Training Loss: 0.05393080413341522\n",
      "Epoch 11756/30000 Training Loss: 0.04111846536397934\n",
      "Epoch 11757/30000 Training Loss: 0.061252251267433167\n",
      "Epoch 11758/30000 Training Loss: 0.041791293770074844\n",
      "Epoch 11759/30000 Training Loss: 0.04788562282919884\n",
      "Epoch 11760/30000 Training Loss: 0.04478641599416733\n",
      "Epoch 11761/30000 Training Loss: 0.047163475304841995\n",
      "Epoch 11762/30000 Training Loss: 0.06300192326307297\n",
      "Epoch 11763/30000 Training Loss: 0.04558195918798447\n",
      "Epoch 11764/30000 Training Loss: 0.05284695327281952\n",
      "Epoch 11765/30000 Training Loss: 0.04779428616166115\n",
      "Epoch 11766/30000 Training Loss: 0.0453140065073967\n",
      "Epoch 11767/30000 Training Loss: 0.05419803410768509\n",
      "Epoch 11768/30000 Training Loss: 0.051867708563804626\n",
      "Epoch 11769/30000 Training Loss: 0.03994695842266083\n",
      "Epoch 11770/30000 Training Loss: 0.05197031423449516\n",
      "Epoch 11771/30000 Training Loss: 0.05674697831273079\n",
      "Epoch 11772/30000 Training Loss: 0.04442308843135834\n",
      "Epoch 11773/30000 Training Loss: 0.04555615037679672\n",
      "Epoch 11774/30000 Training Loss: 0.04954713582992554\n",
      "Epoch 11775/30000 Training Loss: 0.03964371979236603\n",
      "Epoch 11776/30000 Training Loss: 0.054494842886924744\n",
      "Epoch 11777/30000 Training Loss: 0.04422125220298767\n",
      "Epoch 11778/30000 Training Loss: 0.057603687047958374\n",
      "Epoch 11779/30000 Training Loss: 0.05391072481870651\n",
      "Epoch 11780/30000 Training Loss: 0.04255330562591553\n",
      "Epoch 11781/30000 Training Loss: 0.0497175008058548\n",
      "Epoch 11782/30000 Training Loss: 0.06087067723274231\n",
      "Epoch 11783/30000 Training Loss: 0.04993199557065964\n",
      "Epoch 11784/30000 Training Loss: 0.05513238161802292\n",
      "Epoch 11785/30000 Training Loss: 0.038578592240810394\n",
      "Epoch 11786/30000 Training Loss: 0.06138037145137787\n",
      "Epoch 11787/30000 Training Loss: 0.06830980628728867\n",
      "Epoch 11788/30000 Training Loss: 0.03841704875230789\n",
      "Epoch 11789/30000 Training Loss: 0.04000595211982727\n",
      "Epoch 11790/30000 Training Loss: 0.04712057113647461\n",
      "Epoch 11791/30000 Training Loss: 0.0416703075170517\n",
      "Epoch 11792/30000 Training Loss: 0.04779694229364395\n",
      "Epoch 11793/30000 Training Loss: 0.044816404581069946\n",
      "Epoch 11794/30000 Training Loss: 0.05001837760210037\n",
      "Epoch 11795/30000 Training Loss: 0.04717426374554634\n",
      "Epoch 11796/30000 Training Loss: 0.05393221229314804\n",
      "Epoch 11797/30000 Training Loss: 0.0458105206489563\n",
      "Epoch 11798/30000 Training Loss: 0.05095987021923065\n",
      "Epoch 11799/30000 Training Loss: 0.042140893638134\n",
      "Epoch 11800/30000 Training Loss: 0.05249159038066864\n",
      "Epoch 11800/30000 Validation Loss: 0.03880370035767555\n",
      "Epoch 11801/30000 Training Loss: 0.04246875271201134\n",
      "Epoch 11802/30000 Training Loss: 0.0446707122027874\n",
      "Epoch 11803/30000 Training Loss: 0.05270770564675331\n",
      "Epoch 11804/30000 Training Loss: 0.052991464734077454\n",
      "Epoch 11805/30000 Training Loss: 0.05648072808980942\n",
      "Epoch 11806/30000 Training Loss: 0.047461818903684616\n",
      "Epoch 11807/30000 Training Loss: 0.044325076043605804\n",
      "Epoch 11808/30000 Training Loss: 0.043735504150390625\n",
      "Epoch 11809/30000 Training Loss: 0.06364136934280396\n",
      "Epoch 11810/30000 Training Loss: 0.03563430532813072\n",
      "Epoch 11811/30000 Training Loss: 0.05157186836004257\n",
      "Epoch 11812/30000 Training Loss: 0.05522832274436951\n",
      "Epoch 11813/30000 Training Loss: 0.04720692336559296\n",
      "Epoch 11814/30000 Training Loss: 0.04679614305496216\n",
      "Epoch 11815/30000 Training Loss: 0.04846339672803879\n",
      "Epoch 11816/30000 Training Loss: 0.05705524981021881\n",
      "Epoch 11817/30000 Training Loss: 0.045105431228876114\n",
      "Epoch 11818/30000 Training Loss: 0.05589750409126282\n",
      "Epoch 11819/30000 Training Loss: 0.04080948606133461\n",
      "Epoch 11820/30000 Training Loss: 0.04051332175731659\n",
      "Epoch 11821/30000 Training Loss: 0.04019356518983841\n",
      "Epoch 11822/30000 Training Loss: 0.03774316981434822\n",
      "Epoch 11823/30000 Training Loss: 0.05853230133652687\n",
      "Epoch 11824/30000 Training Loss: 0.05369926244020462\n",
      "Epoch 11825/30000 Training Loss: 0.055379047989845276\n",
      "Epoch 11826/30000 Training Loss: 0.05564102530479431\n",
      "Epoch 11827/30000 Training Loss: 0.06025812774896622\n",
      "Epoch 11828/30000 Training Loss: 0.05921855568885803\n",
      "Epoch 11829/30000 Training Loss: 0.03912021964788437\n",
      "Epoch 11830/30000 Training Loss: 0.04651332274079323\n",
      "Epoch 11831/30000 Training Loss: 0.03748235106468201\n",
      "Epoch 11832/30000 Training Loss: 0.04150048643350601\n",
      "Epoch 11833/30000 Training Loss: 0.03531144931912422\n",
      "Epoch 11834/30000 Training Loss: 0.04899086058139801\n",
      "Epoch 11835/30000 Training Loss: 0.04689048230648041\n",
      "Epoch 11836/30000 Training Loss: 0.06189241632819176\n",
      "Epoch 11837/30000 Training Loss: 0.057322144508361816\n",
      "Epoch 11838/30000 Training Loss: 0.04464779794216156\n",
      "Epoch 11839/30000 Training Loss: 0.05265079438686371\n",
      "Epoch 11840/30000 Training Loss: 0.053518783301115036\n",
      "Epoch 11841/30000 Training Loss: 0.06165614724159241\n",
      "Epoch 11842/30000 Training Loss: 0.0469086691737175\n",
      "Epoch 11843/30000 Training Loss: 0.04075939208269119\n",
      "Epoch 11844/30000 Training Loss: 0.037119340151548386\n",
      "Epoch 11845/30000 Training Loss: 0.03816999867558479\n",
      "Epoch 11846/30000 Training Loss: 0.04134077578783035\n",
      "Epoch 11847/30000 Training Loss: 0.05556108430027962\n",
      "Epoch 11848/30000 Training Loss: 0.05520119518041611\n",
      "Epoch 11849/30000 Training Loss: 0.050172317773103714\n",
      "Epoch 11850/30000 Training Loss: 0.04351719841361046\n",
      "Epoch 11851/30000 Training Loss: 0.05440770834684372\n",
      "Epoch 11852/30000 Training Loss: 0.03902403637766838\n",
      "Epoch 11853/30000 Training Loss: 0.052700579166412354\n",
      "Epoch 11854/30000 Training Loss: 0.04709147289395332\n",
      "Epoch 11855/30000 Training Loss: 0.052839718759059906\n",
      "Epoch 11856/30000 Training Loss: 0.03364398330450058\n",
      "Epoch 11857/30000 Training Loss: 0.04611942544579506\n",
      "Epoch 11858/30000 Training Loss: 0.057085271924734116\n",
      "Epoch 11859/30000 Training Loss: 0.045873451977968216\n",
      "Epoch 11860/30000 Training Loss: 0.06217607110738754\n",
      "Epoch 11861/30000 Training Loss: 0.05571556091308594\n",
      "Epoch 11862/30000 Training Loss: 0.05600638687610626\n",
      "Epoch 11863/30000 Training Loss: 0.03646446019411087\n",
      "Epoch 11864/30000 Training Loss: 0.04808759689331055\n",
      "Epoch 11865/30000 Training Loss: 0.039969369769096375\n",
      "Epoch 11866/30000 Training Loss: 0.043055638670921326\n",
      "Epoch 11867/30000 Training Loss: 0.035169221460819244\n",
      "Epoch 11868/30000 Training Loss: 0.045471079647541046\n",
      "Epoch 11869/30000 Training Loss: 0.0539495125412941\n",
      "Epoch 11870/30000 Training Loss: 0.051481425762176514\n",
      "Epoch 11871/30000 Training Loss: 0.044057585299015045\n",
      "Epoch 11872/30000 Training Loss: 0.03819946199655533\n",
      "Epoch 11873/30000 Training Loss: 0.053854070603847504\n",
      "Epoch 11874/30000 Training Loss: 0.04819642752408981\n",
      "Epoch 11875/30000 Training Loss: 0.06079870089888573\n",
      "Epoch 11876/30000 Training Loss: 0.04028795287013054\n",
      "Epoch 11877/30000 Training Loss: 0.04406622424721718\n",
      "Epoch 11878/30000 Training Loss: 0.05474857985973358\n",
      "Epoch 11879/30000 Training Loss: 0.04769264534115791\n",
      "Epoch 11880/30000 Training Loss: 0.038667432963848114\n",
      "Epoch 11881/30000 Training Loss: 0.043527208268642426\n",
      "Epoch 11882/30000 Training Loss: 0.0374726727604866\n",
      "Epoch 11883/30000 Training Loss: 0.04770208150148392\n",
      "Epoch 11884/30000 Training Loss: 0.052165161818265915\n",
      "Epoch 11885/30000 Training Loss: 0.04325329139828682\n",
      "Epoch 11886/30000 Training Loss: 0.05448121577501297\n",
      "Epoch 11887/30000 Training Loss: 0.060464970767498016\n",
      "Epoch 11888/30000 Training Loss: 0.04789409041404724\n",
      "Epoch 11889/30000 Training Loss: 0.05207902193069458\n",
      "Epoch 11890/30000 Training Loss: 0.05130162090063095\n",
      "Epoch 11891/30000 Training Loss: 0.04049045592546463\n",
      "Epoch 11892/30000 Training Loss: 0.055801548063755035\n",
      "Epoch 11893/30000 Training Loss: 0.048840247094631195\n",
      "Epoch 11894/30000 Training Loss: 0.06918135285377502\n",
      "Epoch 11895/30000 Training Loss: 0.05437278002500534\n",
      "Epoch 11896/30000 Training Loss: 0.03906833380460739\n",
      "Epoch 11897/30000 Training Loss: 0.06853535026311874\n",
      "Epoch 11898/30000 Training Loss: 0.04701784998178482\n",
      "Epoch 11899/30000 Training Loss: 0.0497894287109375\n",
      "Epoch 11900/30000 Training Loss: 0.0573735386133194\n",
      "Epoch 11900/30000 Validation Loss: 0.04614949971437454\n",
      "Epoch 11901/30000 Training Loss: 0.05208317190408707\n",
      "Epoch 11902/30000 Training Loss: 0.04710739850997925\n",
      "Epoch 11903/30000 Training Loss: 0.05448753386735916\n",
      "Epoch 11904/30000 Training Loss: 0.05087004601955414\n",
      "Epoch 11905/30000 Training Loss: 0.04744929447770119\n",
      "Epoch 11906/30000 Training Loss: 0.03749766945838928\n",
      "Epoch 11907/30000 Training Loss: 0.04647190123796463\n",
      "Epoch 11908/30000 Training Loss: 0.04001925140619278\n",
      "Epoch 11909/30000 Training Loss: 0.04983677715063095\n",
      "Epoch 11910/30000 Training Loss: 0.051519572734832764\n",
      "Epoch 11911/30000 Training Loss: 0.05225697159767151\n",
      "Epoch 11912/30000 Training Loss: 0.06323623657226562\n",
      "Epoch 11913/30000 Training Loss: 0.03384637460112572\n",
      "Epoch 11914/30000 Training Loss: 0.043723247945308685\n",
      "Epoch 11915/30000 Training Loss: 0.06094196066260338\n",
      "Epoch 11916/30000 Training Loss: 0.03669470176100731\n",
      "Epoch 11917/30000 Training Loss: 0.03636791557073593\n",
      "Epoch 11918/30000 Training Loss: 0.03714354336261749\n",
      "Epoch 11919/30000 Training Loss: 0.04890444129705429\n",
      "Epoch 11920/30000 Training Loss: 0.046548210084438324\n",
      "Epoch 11921/30000 Training Loss: 0.059290580451488495\n",
      "Epoch 11922/30000 Training Loss: 0.04423149302601814\n",
      "Epoch 11923/30000 Training Loss: 0.05923078954219818\n",
      "Epoch 11924/30000 Training Loss: 0.0372336283326149\n",
      "Epoch 11925/30000 Training Loss: 0.045942455530166626\n",
      "Epoch 11926/30000 Training Loss: 0.05647982284426689\n",
      "Epoch 11927/30000 Training Loss: 0.04606056958436966\n",
      "Epoch 11928/30000 Training Loss: 0.059019818902015686\n",
      "Epoch 11929/30000 Training Loss: 0.04397997260093689\n",
      "Epoch 11930/30000 Training Loss: 0.052459053695201874\n",
      "Epoch 11931/30000 Training Loss: 0.046877846121788025\n",
      "Epoch 11932/30000 Training Loss: 0.048843495547771454\n",
      "Epoch 11933/30000 Training Loss: 0.05215521156787872\n",
      "Epoch 11934/30000 Training Loss: 0.04740390181541443\n",
      "Epoch 11935/30000 Training Loss: 0.04403345286846161\n",
      "Epoch 11936/30000 Training Loss: 0.046322889626026154\n",
      "Epoch 11937/30000 Training Loss: 0.050883449614048004\n",
      "Epoch 11938/30000 Training Loss: 0.043437130749225616\n",
      "Epoch 11939/30000 Training Loss: 0.03891307860612869\n",
      "Epoch 11940/30000 Training Loss: 0.05329279601573944\n",
      "Epoch 11941/30000 Training Loss: 0.05744509398937225\n",
      "Epoch 11942/30000 Training Loss: 0.047065019607543945\n",
      "Epoch 11943/30000 Training Loss: 0.04710112512111664\n",
      "Epoch 11944/30000 Training Loss: 0.04334709793329239\n",
      "Epoch 11945/30000 Training Loss: 0.047698989510536194\n",
      "Epoch 11946/30000 Training Loss: 0.03793010860681534\n",
      "Epoch 11947/30000 Training Loss: 0.050933837890625\n",
      "Epoch 11948/30000 Training Loss: 0.047559574246406555\n",
      "Epoch 11949/30000 Training Loss: 0.05110844224691391\n",
      "Epoch 11950/30000 Training Loss: 0.04267115145921707\n",
      "Epoch 11951/30000 Training Loss: 0.0366034060716629\n",
      "Epoch 11952/30000 Training Loss: 0.04771610349416733\n",
      "Epoch 11953/30000 Training Loss: 0.05271730199456215\n",
      "Epoch 11954/30000 Training Loss: 0.04860565811395645\n",
      "Epoch 11955/30000 Training Loss: 0.04920770227909088\n",
      "Epoch 11956/30000 Training Loss: 0.04321177303791046\n",
      "Epoch 11957/30000 Training Loss: 0.05786839500069618\n",
      "Epoch 11958/30000 Training Loss: 0.05450720340013504\n",
      "Epoch 11959/30000 Training Loss: 0.041751034557819366\n",
      "Epoch 11960/30000 Training Loss: 0.05165508762001991\n",
      "Epoch 11961/30000 Training Loss: 0.03710770234465599\n",
      "Epoch 11962/30000 Training Loss: 0.03344080224633217\n",
      "Epoch 11963/30000 Training Loss: 0.051478311419487\n",
      "Epoch 11964/30000 Training Loss: 0.05450453609228134\n",
      "Epoch 11965/30000 Training Loss: 0.04649871587753296\n",
      "Epoch 11966/30000 Training Loss: 0.044867098331451416\n",
      "Epoch 11967/30000 Training Loss: 0.0558125264942646\n",
      "Epoch 11968/30000 Training Loss: 0.049708325415849686\n",
      "Epoch 11969/30000 Training Loss: 0.040412772446870804\n",
      "Epoch 11970/30000 Training Loss: 0.042550228536129\n",
      "Epoch 11971/30000 Training Loss: 0.038393836468458176\n",
      "Epoch 11972/30000 Training Loss: 0.04409046843647957\n",
      "Epoch 11973/30000 Training Loss: 0.052247196435928345\n",
      "Epoch 11974/30000 Training Loss: 0.05530709773302078\n",
      "Epoch 11975/30000 Training Loss: 0.04325037822127342\n",
      "Epoch 11976/30000 Training Loss: 0.0343463234603405\n",
      "Epoch 11977/30000 Training Loss: 0.04863253980875015\n",
      "Epoch 11978/30000 Training Loss: 0.05257062986493111\n",
      "Epoch 11979/30000 Training Loss: 0.048150692135095596\n",
      "Epoch 11980/30000 Training Loss: 0.047134801745414734\n",
      "Epoch 11981/30000 Training Loss: 0.051138341426849365\n",
      "Epoch 11982/30000 Training Loss: 0.03926464170217514\n",
      "Epoch 11983/30000 Training Loss: 0.05561700463294983\n",
      "Epoch 11984/30000 Training Loss: 0.04042832925915718\n",
      "Epoch 11985/30000 Training Loss: 0.04597841948270798\n",
      "Epoch 11986/30000 Training Loss: 0.036230385303497314\n",
      "Epoch 11987/30000 Training Loss: 0.044995713979005814\n",
      "Epoch 11988/30000 Training Loss: 0.04059804603457451\n",
      "Epoch 11989/30000 Training Loss: 0.04448722302913666\n",
      "Epoch 11990/30000 Training Loss: 0.05519365519285202\n",
      "Epoch 11991/30000 Training Loss: 0.0462619923055172\n",
      "Epoch 11992/30000 Training Loss: 0.048073429614305496\n",
      "Epoch 11993/30000 Training Loss: 0.052393410354852676\n",
      "Epoch 11994/30000 Training Loss: 0.04835940897464752\n",
      "Epoch 11995/30000 Training Loss: 0.044103577733039856\n",
      "Epoch 11996/30000 Training Loss: 0.05116257816553116\n",
      "Epoch 11997/30000 Training Loss: 0.060399074107408524\n",
      "Epoch 11998/30000 Training Loss: 0.05201682448387146\n",
      "Epoch 11999/30000 Training Loss: 0.03680368885397911\n",
      "Epoch 12000/30000 Training Loss: 0.055626481771469116\n",
      "Epoch 12000/30000 Validation Loss: 0.05345842242240906\n",
      "Epoch 12001/30000 Training Loss: 0.039305973798036575\n",
      "Epoch 12002/30000 Training Loss: 0.049364879727363586\n",
      "Epoch 12003/30000 Training Loss: 0.0602431446313858\n",
      "Epoch 12004/30000 Training Loss: 0.05502834916114807\n",
      "Epoch 12005/30000 Training Loss: 0.06452429294586182\n",
      "Epoch 12006/30000 Training Loss: 0.0485205352306366\n",
      "Epoch 12007/30000 Training Loss: 0.048189327120780945\n",
      "Epoch 12008/30000 Training Loss: 0.041932620108127594\n",
      "Epoch 12009/30000 Training Loss: 0.04495255649089813\n",
      "Epoch 12010/30000 Training Loss: 0.04586605727672577\n",
      "Epoch 12011/30000 Training Loss: 0.055852897465229034\n",
      "Epoch 12012/30000 Training Loss: 0.04686162248253822\n",
      "Epoch 12013/30000 Training Loss: 0.037299416959285736\n",
      "Epoch 12014/30000 Training Loss: 0.04804914444684982\n",
      "Epoch 12015/30000 Training Loss: 0.041962675750255585\n",
      "Epoch 12016/30000 Training Loss: 0.04791209101676941\n",
      "Epoch 12017/30000 Training Loss: 0.04725979268550873\n",
      "Epoch 12018/30000 Training Loss: 0.032567303627729416\n",
      "Epoch 12019/30000 Training Loss: 0.049062054604291916\n",
      "Epoch 12020/30000 Training Loss: 0.04670199751853943\n",
      "Epoch 12021/30000 Training Loss: 0.04523231461644173\n",
      "Epoch 12022/30000 Training Loss: 0.04915687441825867\n",
      "Epoch 12023/30000 Training Loss: 0.04084565490484238\n",
      "Epoch 12024/30000 Training Loss: 0.05296439677476883\n",
      "Epoch 12025/30000 Training Loss: 0.04941143840551376\n",
      "Epoch 12026/30000 Training Loss: 0.05294147878885269\n",
      "Epoch 12027/30000 Training Loss: 0.049337029457092285\n",
      "Epoch 12028/30000 Training Loss: 0.03505686670541763\n",
      "Epoch 12029/30000 Training Loss: 0.06630480289459229\n",
      "Epoch 12030/30000 Training Loss: 0.05075935274362564\n",
      "Epoch 12031/30000 Training Loss: 0.04497314244508743\n",
      "Epoch 12032/30000 Training Loss: 0.04723212122917175\n",
      "Epoch 12033/30000 Training Loss: 0.05698788911104202\n",
      "Epoch 12034/30000 Training Loss: 0.04100262373685837\n",
      "Epoch 12035/30000 Training Loss: 0.047205038368701935\n",
      "Epoch 12036/30000 Training Loss: 0.04645838588476181\n",
      "Epoch 12037/30000 Training Loss: 0.05056549981236458\n",
      "Epoch 12038/30000 Training Loss: 0.04132871329784393\n",
      "Epoch 12039/30000 Training Loss: 0.04084756597876549\n",
      "Epoch 12040/30000 Training Loss: 0.050641898065805435\n",
      "Epoch 12041/30000 Training Loss: 0.053368739783763885\n",
      "Epoch 12042/30000 Training Loss: 0.052859023213386536\n",
      "Epoch 12043/30000 Training Loss: 0.050800763070583344\n",
      "Epoch 12044/30000 Training Loss: 0.05504199117422104\n",
      "Epoch 12045/30000 Training Loss: 0.0473882332444191\n",
      "Epoch 12046/30000 Training Loss: 0.06287228316068649\n",
      "Epoch 12047/30000 Training Loss: 0.0749705582857132\n",
      "Epoch 12048/30000 Training Loss: 0.055552996695041656\n",
      "Epoch 12049/30000 Training Loss: 0.04652305319905281\n",
      "Epoch 12050/30000 Training Loss: 0.04310467094182968\n",
      "Epoch 12051/30000 Training Loss: 0.047355301678180695\n",
      "Epoch 12052/30000 Training Loss: 0.061417315155267715\n",
      "Epoch 12053/30000 Training Loss: 0.046851761639118195\n",
      "Epoch 12054/30000 Training Loss: 0.04026155173778534\n",
      "Epoch 12055/30000 Training Loss: 0.03559283912181854\n",
      "Epoch 12056/30000 Training Loss: 0.044789016246795654\n",
      "Epoch 12057/30000 Training Loss: 0.03751600906252861\n",
      "Epoch 12058/30000 Training Loss: 0.058753177523612976\n",
      "Epoch 12059/30000 Training Loss: 0.05251263827085495\n",
      "Epoch 12060/30000 Training Loss: 0.05064239352941513\n",
      "Epoch 12061/30000 Training Loss: 0.03894717991352081\n",
      "Epoch 12062/30000 Training Loss: 0.05495829880237579\n",
      "Epoch 12063/30000 Training Loss: 0.050411075353622437\n",
      "Epoch 12064/30000 Training Loss: 0.04092763364315033\n",
      "Epoch 12065/30000 Training Loss: 0.055140405893325806\n",
      "Epoch 12066/30000 Training Loss: 0.05281158536672592\n",
      "Epoch 12067/30000 Training Loss: 0.04791083559393883\n",
      "Epoch 12068/30000 Training Loss: 0.045516178011894226\n",
      "Epoch 12069/30000 Training Loss: 0.04891187325119972\n",
      "Epoch 12070/30000 Training Loss: 0.049978435039520264\n",
      "Epoch 12071/30000 Training Loss: 0.06202585622668266\n",
      "Epoch 12072/30000 Training Loss: 0.053385451436042786\n",
      "Epoch 12073/30000 Training Loss: 0.043105050921440125\n",
      "Epoch 12074/30000 Training Loss: 0.050993502140045166\n",
      "Epoch 12075/30000 Training Loss: 0.06948912143707275\n",
      "Epoch 12076/30000 Training Loss: 0.06208965182304382\n",
      "Epoch 12077/30000 Training Loss: 0.05257381498813629\n",
      "Epoch 12078/30000 Training Loss: 0.034066032618284225\n",
      "Epoch 12079/30000 Training Loss: 0.04088878631591797\n",
      "Epoch 12080/30000 Training Loss: 0.04536794126033783\n",
      "Epoch 12081/30000 Training Loss: 0.04594503715634346\n",
      "Epoch 12082/30000 Training Loss: 0.04992587864398956\n",
      "Epoch 12083/30000 Training Loss: 0.051696911454200745\n",
      "Epoch 12084/30000 Training Loss: 0.045520029962062836\n",
      "Epoch 12085/30000 Training Loss: 0.04171052575111389\n",
      "Epoch 12086/30000 Training Loss: 0.041310641914606094\n",
      "Epoch 12087/30000 Training Loss: 0.04193238168954849\n",
      "Epoch 12088/30000 Training Loss: 0.05049540847539902\n",
      "Epoch 12089/30000 Training Loss: 0.05871554836630821\n",
      "Epoch 12090/30000 Training Loss: 0.0651874765753746\n",
      "Epoch 12091/30000 Training Loss: 0.030200324952602386\n",
      "Epoch 12092/30000 Training Loss: 0.04829934239387512\n",
      "Epoch 12093/30000 Training Loss: 0.05915764719247818\n",
      "Epoch 12094/30000 Training Loss: 0.05180990695953369\n",
      "Epoch 12095/30000 Training Loss: 0.04171761870384216\n",
      "Epoch 12096/30000 Training Loss: 0.03943503648042679\n",
      "Epoch 12097/30000 Training Loss: 0.03446698561310768\n",
      "Epoch 12098/30000 Training Loss: 0.05142795294523239\n",
      "Epoch 12099/30000 Training Loss: 0.0390700027346611\n",
      "Epoch 12100/30000 Training Loss: 0.05408262833952904\n",
      "Epoch 12100/30000 Validation Loss: 0.04642828553915024\n",
      "Epoch 12101/30000 Training Loss: 0.061579637229442596\n",
      "Epoch 12102/30000 Training Loss: 0.052515365183353424\n",
      "Epoch 12103/30000 Training Loss: 0.051897451281547546\n",
      "Epoch 12104/30000 Training Loss: 0.05184795707464218\n",
      "Epoch 12105/30000 Training Loss: 0.0455370768904686\n",
      "Epoch 12106/30000 Training Loss: 0.03674823045730591\n",
      "Epoch 12107/30000 Training Loss: 0.06373120844364166\n",
      "Epoch 12108/30000 Training Loss: 0.03595443069934845\n",
      "Epoch 12109/30000 Training Loss: 0.04410886764526367\n",
      "Epoch 12110/30000 Training Loss: 0.04368231073021889\n",
      "Epoch 12111/30000 Training Loss: 0.04408692941069603\n",
      "Epoch 12112/30000 Training Loss: 0.041061680763959885\n",
      "Epoch 12113/30000 Training Loss: 0.028216201812028885\n",
      "Epoch 12114/30000 Training Loss: 0.03950249031186104\n",
      "Epoch 12115/30000 Training Loss: 0.043856751173734665\n",
      "Epoch 12116/30000 Training Loss: 0.04166487604379654\n",
      "Epoch 12117/30000 Training Loss: 0.055315665900707245\n",
      "Epoch 12118/30000 Training Loss: 0.051706477999687195\n",
      "Epoch 12119/30000 Training Loss: 0.04654355347156525\n",
      "Epoch 12120/30000 Training Loss: 0.051127612590789795\n",
      "Epoch 12121/30000 Training Loss: 0.060285136103630066\n",
      "Epoch 12122/30000 Training Loss: 0.041673749685287476\n",
      "Epoch 12123/30000 Training Loss: 0.05050249397754669\n",
      "Epoch 12124/30000 Training Loss: 0.03760179877281189\n",
      "Epoch 12125/30000 Training Loss: 0.046454332768917084\n",
      "Epoch 12126/30000 Training Loss: 0.04553478956222534\n",
      "Epoch 12127/30000 Training Loss: 0.044765930622816086\n",
      "Epoch 12128/30000 Training Loss: 0.046505559235811234\n",
      "Epoch 12129/30000 Training Loss: 0.04085918888449669\n",
      "Epoch 12130/30000 Training Loss: 0.03198004886507988\n",
      "Epoch 12131/30000 Training Loss: 0.03920190781354904\n",
      "Epoch 12132/30000 Training Loss: 0.05451217666268349\n",
      "Epoch 12133/30000 Training Loss: 0.05354122072458267\n",
      "Epoch 12134/30000 Training Loss: 0.051734182983636856\n",
      "Epoch 12135/30000 Training Loss: 0.054767243564128876\n",
      "Epoch 12136/30000 Training Loss: 0.05361875146627426\n",
      "Epoch 12137/30000 Training Loss: 0.043549083173274994\n",
      "Epoch 12138/30000 Training Loss: 0.038082774728536606\n",
      "Epoch 12139/30000 Training Loss: 0.03656940534710884\n",
      "Epoch 12140/30000 Training Loss: 0.05553595349192619\n",
      "Epoch 12141/30000 Training Loss: 0.06370630860328674\n",
      "Epoch 12142/30000 Training Loss: 0.05129564553499222\n",
      "Epoch 12143/30000 Training Loss: 0.057319171726703644\n",
      "Epoch 12144/30000 Training Loss: 0.04355289041996002\n",
      "Epoch 12145/30000 Training Loss: 0.05695618316531181\n",
      "Epoch 12146/30000 Training Loss: 0.052988797426223755\n",
      "Epoch 12147/30000 Training Loss: 0.04394341632723808\n",
      "Epoch 12148/30000 Training Loss: 0.038725756108760834\n",
      "Epoch 12149/30000 Training Loss: 0.04477536678314209\n",
      "Epoch 12150/30000 Training Loss: 0.047678060829639435\n",
      "Epoch 12151/30000 Training Loss: 0.048018500208854675\n",
      "Epoch 12152/30000 Training Loss: 0.053152259439229965\n",
      "Epoch 12153/30000 Training Loss: 0.046793386340141296\n",
      "Epoch 12154/30000 Training Loss: 0.03365812450647354\n",
      "Epoch 12155/30000 Training Loss: 0.04253515973687172\n",
      "Epoch 12156/30000 Training Loss: 0.05287904292345047\n",
      "Epoch 12157/30000 Training Loss: 0.04237549751996994\n",
      "Epoch 12158/30000 Training Loss: 0.051408715546131134\n",
      "Epoch 12159/30000 Training Loss: 0.041362643241882324\n",
      "Epoch 12160/30000 Training Loss: 0.04424957185983658\n",
      "Epoch 12161/30000 Training Loss: 0.03296688199043274\n",
      "Epoch 12162/30000 Training Loss: 0.06157635897397995\n",
      "Epoch 12163/30000 Training Loss: 0.05082429200410843\n",
      "Epoch 12164/30000 Training Loss: 0.05220247060060501\n",
      "Epoch 12165/30000 Training Loss: 0.041747692972421646\n",
      "Epoch 12166/30000 Training Loss: 0.04675429314374924\n",
      "Epoch 12167/30000 Training Loss: 0.05745512619614601\n",
      "Epoch 12168/30000 Training Loss: 0.03447927534580231\n",
      "Epoch 12169/30000 Training Loss: 0.04747822880744934\n",
      "Epoch 12170/30000 Training Loss: 0.059650860726833344\n",
      "Epoch 12171/30000 Training Loss: 0.039816711097955704\n",
      "Epoch 12172/30000 Training Loss: 0.03420751169323921\n",
      "Epoch 12173/30000 Training Loss: 0.041844166815280914\n",
      "Epoch 12174/30000 Training Loss: 0.04474897310137749\n",
      "Epoch 12175/30000 Training Loss: 0.05546404421329498\n",
      "Epoch 12176/30000 Training Loss: 0.0476895272731781\n",
      "Epoch 12177/30000 Training Loss: 0.032960761338472366\n",
      "Epoch 12178/30000 Training Loss: 0.0504799410700798\n",
      "Epoch 12179/30000 Training Loss: 0.040629640221595764\n",
      "Epoch 12180/30000 Training Loss: 0.051548391580581665\n",
      "Epoch 12181/30000 Training Loss: 0.050667669624090195\n",
      "Epoch 12182/30000 Training Loss: 0.04527987167239189\n",
      "Epoch 12183/30000 Training Loss: 0.04755106568336487\n",
      "Epoch 12184/30000 Training Loss: 0.04813377559185028\n",
      "Epoch 12185/30000 Training Loss: 0.048735395073890686\n",
      "Epoch 12186/30000 Training Loss: 0.05489247664809227\n",
      "Epoch 12187/30000 Training Loss: 0.04939037561416626\n",
      "Epoch 12188/30000 Training Loss: 0.05384403467178345\n",
      "Epoch 12189/30000 Training Loss: 0.040591344237327576\n",
      "Epoch 12190/30000 Training Loss: 0.06405068933963776\n",
      "Epoch 12191/30000 Training Loss: 0.05518969148397446\n",
      "Epoch 12192/30000 Training Loss: 0.05989644303917885\n",
      "Epoch 12193/30000 Training Loss: 0.043021492660045624\n",
      "Epoch 12194/30000 Training Loss: 0.05955548584461212\n",
      "Epoch 12195/30000 Training Loss: 0.056002069264650345\n",
      "Epoch 12196/30000 Training Loss: 0.04399273544549942\n",
      "Epoch 12197/30000 Training Loss: 0.04794742539525032\n",
      "Epoch 12198/30000 Training Loss: 0.04020605608820915\n",
      "Epoch 12199/30000 Training Loss: 0.05584661662578583\n",
      "Epoch 12200/30000 Training Loss: 0.06096170097589493\n",
      "Epoch 12200/30000 Validation Loss: 0.04511765390634537\n",
      "Epoch 12201/30000 Training Loss: 0.0555158294737339\n",
      "Epoch 12202/30000 Training Loss: 0.040106598287820816\n",
      "Epoch 12203/30000 Training Loss: 0.049545060843229294\n",
      "Epoch 12204/30000 Training Loss: 0.03890124708414078\n",
      "Epoch 12205/30000 Training Loss: 0.03255258873105049\n",
      "Epoch 12206/30000 Training Loss: 0.07048101723194122\n",
      "Epoch 12207/30000 Training Loss: 0.039577968418598175\n",
      "Epoch 12208/30000 Training Loss: 0.05010196939110756\n",
      "Epoch 12209/30000 Training Loss: 0.04513752460479736\n",
      "Epoch 12210/30000 Training Loss: 0.05304581671953201\n",
      "Epoch 12211/30000 Training Loss: 0.04345162212848663\n",
      "Epoch 12212/30000 Training Loss: 0.04350246116518974\n",
      "Epoch 12213/30000 Training Loss: 0.04624349623918533\n",
      "Epoch 12214/30000 Training Loss: 0.05489417165517807\n",
      "Epoch 12215/30000 Training Loss: 0.04386243224143982\n",
      "Epoch 12216/30000 Training Loss: 0.045143336057662964\n",
      "Epoch 12217/30000 Training Loss: 0.043784864246845245\n",
      "Epoch 12218/30000 Training Loss: 0.04006798565387726\n",
      "Epoch 12219/30000 Training Loss: 0.05128248780965805\n",
      "Epoch 12220/30000 Training Loss: 0.04362409561872482\n",
      "Epoch 12221/30000 Training Loss: 0.05150871351361275\n",
      "Epoch 12222/30000 Training Loss: 0.056032340973615646\n",
      "Epoch 12223/30000 Training Loss: 0.043288491666316986\n",
      "Epoch 12224/30000 Training Loss: 0.06315372884273529\n",
      "Epoch 12225/30000 Training Loss: 0.05522476136684418\n",
      "Epoch 12226/30000 Training Loss: 0.05941925197839737\n",
      "Epoch 12227/30000 Training Loss: 0.052445635199546814\n",
      "Epoch 12228/30000 Training Loss: 0.05743306130170822\n",
      "Epoch 12229/30000 Training Loss: 0.03805720806121826\n",
      "Epoch 12230/30000 Training Loss: 0.03271544352173805\n",
      "Epoch 12231/30000 Training Loss: 0.04673082008957863\n",
      "Epoch 12232/30000 Training Loss: 0.0547250360250473\n",
      "Epoch 12233/30000 Training Loss: 0.047691650688648224\n",
      "Epoch 12234/30000 Training Loss: 0.05722658336162567\n",
      "Epoch 12235/30000 Training Loss: 0.045410916209220886\n",
      "Epoch 12236/30000 Training Loss: 0.04627919942140579\n",
      "Epoch 12237/30000 Training Loss: 0.05845847353339195\n",
      "Epoch 12238/30000 Training Loss: 0.03819279000163078\n",
      "Epoch 12239/30000 Training Loss: 0.06120411306619644\n",
      "Epoch 12240/30000 Training Loss: 0.04914030432701111\n",
      "Epoch 12241/30000 Training Loss: 0.05275840312242508\n",
      "Epoch 12242/30000 Training Loss: 0.048646531999111176\n",
      "Epoch 12243/30000 Training Loss: 0.04350534826517105\n",
      "Epoch 12244/30000 Training Loss: 0.04023604840040207\n",
      "Epoch 12245/30000 Training Loss: 0.04196399077773094\n",
      "Epoch 12246/30000 Training Loss: 0.05602321773767471\n",
      "Epoch 12247/30000 Training Loss: 0.04675637185573578\n",
      "Epoch 12248/30000 Training Loss: 0.051085520535707474\n",
      "Epoch 12249/30000 Training Loss: 0.04983861744403839\n",
      "Epoch 12250/30000 Training Loss: 0.035526517778635025\n",
      "Epoch 12251/30000 Training Loss: 0.04564255103468895\n",
      "Epoch 12252/30000 Training Loss: 0.060443341732025146\n",
      "Epoch 12253/30000 Training Loss: 0.05200560390949249\n",
      "Epoch 12254/30000 Training Loss: 0.052000075578689575\n",
      "Epoch 12255/30000 Training Loss: 0.05691374093294144\n",
      "Epoch 12256/30000 Training Loss: 0.047188036143779755\n",
      "Epoch 12257/30000 Training Loss: 0.055368632078170776\n",
      "Epoch 12258/30000 Training Loss: 0.036845915019512177\n",
      "Epoch 12259/30000 Training Loss: 0.040311798453330994\n",
      "Epoch 12260/30000 Training Loss: 0.041964057832956314\n",
      "Epoch 12261/30000 Training Loss: 0.045301299542188644\n",
      "Epoch 12262/30000 Training Loss: 0.046077072620391846\n",
      "Epoch 12263/30000 Training Loss: 0.04614325612783432\n",
      "Epoch 12264/30000 Training Loss: 0.049449726939201355\n",
      "Epoch 12265/30000 Training Loss: 0.05088510364294052\n",
      "Epoch 12266/30000 Training Loss: 0.03236917778849602\n",
      "Epoch 12267/30000 Training Loss: 0.06652382016181946\n",
      "Epoch 12268/30000 Training Loss: 0.043124713003635406\n",
      "Epoch 12269/30000 Training Loss: 0.04596380144357681\n",
      "Epoch 12270/30000 Training Loss: 0.051627017557621\n",
      "Epoch 12271/30000 Training Loss: 0.04128449782729149\n",
      "Epoch 12272/30000 Training Loss: 0.041535526514053345\n",
      "Epoch 12273/30000 Training Loss: 0.03832868114113808\n",
      "Epoch 12274/30000 Training Loss: 0.053984135389328\n",
      "Epoch 12275/30000 Training Loss: 0.04389840364456177\n",
      "Epoch 12276/30000 Training Loss: 0.04920168220996857\n",
      "Epoch 12277/30000 Training Loss: 0.046449631452560425\n",
      "Epoch 12278/30000 Training Loss: 0.04974976181983948\n",
      "Epoch 12279/30000 Training Loss: 0.03694404661655426\n",
      "Epoch 12280/30000 Training Loss: 0.04366691783070564\n",
      "Epoch 12281/30000 Training Loss: 0.051696278154850006\n",
      "Epoch 12282/30000 Training Loss: 0.055638864636421204\n",
      "Epoch 12283/30000 Training Loss: 0.06623606383800507\n",
      "Epoch 12284/30000 Training Loss: 0.04592172056436539\n",
      "Epoch 12285/30000 Training Loss: 0.043477561324834824\n",
      "Epoch 12286/30000 Training Loss: 0.047712720930576324\n",
      "Epoch 12287/30000 Training Loss: 0.0426199808716774\n",
      "Epoch 12288/30000 Training Loss: 0.05791069194674492\n",
      "Epoch 12289/30000 Training Loss: 0.048331163823604584\n",
      "Epoch 12290/30000 Training Loss: 0.04548568278551102\n",
      "Epoch 12291/30000 Training Loss: 0.04605841264128685\n",
      "Epoch 12292/30000 Training Loss: 0.04721859097480774\n",
      "Epoch 12293/30000 Training Loss: 0.03942667320370674\n",
      "Epoch 12294/30000 Training Loss: 0.05826219171285629\n",
      "Epoch 12295/30000 Training Loss: 0.04170146957039833\n",
      "Epoch 12296/30000 Training Loss: 0.04768160730600357\n",
      "Epoch 12297/30000 Training Loss: 0.04082833230495453\n",
      "Epoch 12298/30000 Training Loss: 0.046185482293367386\n",
      "Epoch 12299/30000 Training Loss: 0.03381859138607979\n",
      "Epoch 12300/30000 Training Loss: 0.06578025221824646\n",
      "Epoch 12300/30000 Validation Loss: 0.05800136178731918\n",
      "Epoch 12301/30000 Training Loss: 0.05648743733763695\n",
      "Epoch 12302/30000 Training Loss: 0.04936455562710762\n",
      "Epoch 12303/30000 Training Loss: 0.04084938392043114\n",
      "Epoch 12304/30000 Training Loss: 0.03879029303789139\n",
      "Epoch 12305/30000 Training Loss: 0.041018806397914886\n",
      "Epoch 12306/30000 Training Loss: 0.06169740855693817\n",
      "Epoch 12307/30000 Training Loss: 0.03782501444220543\n",
      "Epoch 12308/30000 Training Loss: 0.04495835676789284\n",
      "Epoch 12309/30000 Training Loss: 0.05056092143058777\n",
      "Epoch 12310/30000 Training Loss: 0.0399298220872879\n",
      "Epoch 12311/30000 Training Loss: 0.03995821624994278\n",
      "Epoch 12312/30000 Training Loss: 0.04538384824991226\n",
      "Epoch 12313/30000 Training Loss: 0.03866879269480705\n",
      "Epoch 12314/30000 Training Loss: 0.047782786190509796\n",
      "Epoch 12315/30000 Training Loss: 0.06279820203781128\n",
      "Epoch 12316/30000 Training Loss: 0.04489240050315857\n",
      "Epoch 12317/30000 Training Loss: 0.04439204931259155\n",
      "Epoch 12318/30000 Training Loss: 0.04681140184402466\n",
      "Epoch 12319/30000 Training Loss: 0.047346536070108414\n",
      "Epoch 12320/30000 Training Loss: 0.04066433012485504\n",
      "Epoch 12321/30000 Training Loss: 0.04608854278922081\n",
      "Epoch 12322/30000 Training Loss: 0.04280129447579384\n",
      "Epoch 12323/30000 Training Loss: 0.049881622195243835\n",
      "Epoch 12324/30000 Training Loss: 0.059049442410469055\n",
      "Epoch 12325/30000 Training Loss: 0.043588943779468536\n",
      "Epoch 12326/30000 Training Loss: 0.04813935235142708\n",
      "Epoch 12327/30000 Training Loss: 0.042459093034267426\n",
      "Epoch 12328/30000 Training Loss: 0.04604736343026161\n",
      "Epoch 12329/30000 Training Loss: 0.07920549809932709\n",
      "Epoch 12330/30000 Training Loss: 0.044939540326595306\n",
      "Epoch 12331/30000 Training Loss: 0.04515352100133896\n",
      "Epoch 12332/30000 Training Loss: 0.04483790695667267\n",
      "Epoch 12333/30000 Training Loss: 0.043691374361515045\n",
      "Epoch 12334/30000 Training Loss: 0.05329892039299011\n",
      "Epoch 12335/30000 Training Loss: 0.04682421684265137\n",
      "Epoch 12336/30000 Training Loss: 0.046835757791996\n",
      "Epoch 12337/30000 Training Loss: 0.05159597098827362\n",
      "Epoch 12338/30000 Training Loss: 0.04389230161905289\n",
      "Epoch 12339/30000 Training Loss: 0.04622453451156616\n",
      "Epoch 12340/30000 Training Loss: 0.06057995185256004\n",
      "Epoch 12341/30000 Training Loss: 0.047357045114040375\n",
      "Epoch 12342/30000 Training Loss: 0.05351363122463226\n",
      "Epoch 12343/30000 Training Loss: 0.06224283576011658\n",
      "Epoch 12344/30000 Training Loss: 0.03580687195062637\n",
      "Epoch 12345/30000 Training Loss: 0.0522184781730175\n",
      "Epoch 12346/30000 Training Loss: 0.037203747779130936\n",
      "Epoch 12347/30000 Training Loss: 0.04530737176537514\n",
      "Epoch 12348/30000 Training Loss: 0.03907906264066696\n",
      "Epoch 12349/30000 Training Loss: 0.0604049377143383\n",
      "Epoch 12350/30000 Training Loss: 0.043758708983659744\n",
      "Epoch 12351/30000 Training Loss: 0.05303071439266205\n",
      "Epoch 12352/30000 Training Loss: 0.04592600837349892\n",
      "Epoch 12353/30000 Training Loss: 0.03077552653849125\n",
      "Epoch 12354/30000 Training Loss: 0.04289178550243378\n",
      "Epoch 12355/30000 Training Loss: 0.04203512519598007\n",
      "Epoch 12356/30000 Training Loss: 0.053358979523181915\n",
      "Epoch 12357/30000 Training Loss: 0.042273856699466705\n",
      "Epoch 12358/30000 Training Loss: 0.050022996962070465\n",
      "Epoch 12359/30000 Training Loss: 0.05411849170923233\n",
      "Epoch 12360/30000 Training Loss: 0.05584114044904709\n",
      "Epoch 12361/30000 Training Loss: 0.059295520186424255\n",
      "Epoch 12362/30000 Training Loss: 0.04682641476392746\n",
      "Epoch 12363/30000 Training Loss: 0.036501843482255936\n",
      "Epoch 12364/30000 Training Loss: 0.04018335044384003\n",
      "Epoch 12365/30000 Training Loss: 0.043900515884160995\n",
      "Epoch 12366/30000 Training Loss: 0.06005135178565979\n",
      "Epoch 12367/30000 Training Loss: 0.04861614853143692\n",
      "Epoch 12368/30000 Training Loss: 0.05709710717201233\n",
      "Epoch 12369/30000 Training Loss: 0.039421096444129944\n",
      "Epoch 12370/30000 Training Loss: 0.051605261862277985\n",
      "Epoch 12371/30000 Training Loss: 0.038500502705574036\n",
      "Epoch 12372/30000 Training Loss: 0.04762180894613266\n",
      "Epoch 12373/30000 Training Loss: 0.04389651119709015\n",
      "Epoch 12374/30000 Training Loss: 0.06285373866558075\n",
      "Epoch 12375/30000 Training Loss: 0.04433438926935196\n",
      "Epoch 12376/30000 Training Loss: 0.04822629690170288\n",
      "Epoch 12377/30000 Training Loss: 0.04586091637611389\n",
      "Epoch 12378/30000 Training Loss: 0.04728393256664276\n",
      "Epoch 12379/30000 Training Loss: 0.04178450256586075\n",
      "Epoch 12380/30000 Training Loss: 0.05648904666304588\n",
      "Epoch 12381/30000 Training Loss: 0.03191320225596428\n",
      "Epoch 12382/30000 Training Loss: 0.07197653502225876\n",
      "Epoch 12383/30000 Training Loss: 0.04778652265667915\n",
      "Epoch 12384/30000 Training Loss: 0.0710638165473938\n",
      "Epoch 12385/30000 Training Loss: 0.04837465286254883\n",
      "Epoch 12386/30000 Training Loss: 0.03592684119939804\n",
      "Epoch 12387/30000 Training Loss: 0.049291763454675674\n",
      "Epoch 12388/30000 Training Loss: 0.04773956909775734\n",
      "Epoch 12389/30000 Training Loss: 0.04691408574581146\n",
      "Epoch 12390/30000 Training Loss: 0.04636455327272415\n",
      "Epoch 12391/30000 Training Loss: 0.04297135770320892\n",
      "Epoch 12392/30000 Training Loss: 0.03566386550664902\n",
      "Epoch 12393/30000 Training Loss: 0.045494195073843\n",
      "Epoch 12394/30000 Training Loss: 0.05505308508872986\n",
      "Epoch 12395/30000 Training Loss: 0.06077972799539566\n",
      "Epoch 12396/30000 Training Loss: 0.04827218875288963\n",
      "Epoch 12397/30000 Training Loss: 0.04273047670722008\n",
      "Epoch 12398/30000 Training Loss: 0.04902677237987518\n",
      "Epoch 12399/30000 Training Loss: 0.052814140915870667\n",
      "Epoch 12400/30000 Training Loss: 0.03832033649086952\n",
      "Epoch 12400/30000 Validation Loss: 0.03371564298868179\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03371564298868179<=============\n",
      "Epoch 12401/30000 Training Loss: 0.03960660472512245\n",
      "Epoch 12402/30000 Training Loss: 0.04038093611598015\n",
      "Epoch 12403/30000 Training Loss: 0.03914683312177658\n",
      "Epoch 12404/30000 Training Loss: 0.05232258141040802\n",
      "Epoch 12405/30000 Training Loss: 0.05463307723402977\n",
      "Epoch 12406/30000 Training Loss: 0.06788745522499084\n",
      "Epoch 12407/30000 Training Loss: 0.05328311026096344\n",
      "Epoch 12408/30000 Training Loss: 0.04959290847182274\n",
      "Epoch 12409/30000 Training Loss: 0.05320373550057411\n",
      "Epoch 12410/30000 Training Loss: 0.0534086711704731\n",
      "Epoch 12411/30000 Training Loss: 0.05074533075094223\n",
      "Epoch 12412/30000 Training Loss: 0.03017790988087654\n",
      "Epoch 12413/30000 Training Loss: 0.06222645193338394\n",
      "Epoch 12414/30000 Training Loss: 0.043367899954319\n",
      "Epoch 12415/30000 Training Loss: 0.04634371027350426\n",
      "Epoch 12416/30000 Training Loss: 0.04881194978952408\n",
      "Epoch 12417/30000 Training Loss: 0.05339856445789337\n",
      "Epoch 12418/30000 Training Loss: 0.03827676922082901\n",
      "Epoch 12419/30000 Training Loss: 0.0426565483212471\n",
      "Epoch 12420/30000 Training Loss: 0.042501769959926605\n",
      "Epoch 12421/30000 Training Loss: 0.03446437790989876\n",
      "Epoch 12422/30000 Training Loss: 0.04220107942819595\n",
      "Epoch 12423/30000 Training Loss: 0.05402818322181702\n",
      "Epoch 12424/30000 Training Loss: 0.03520670533180237\n",
      "Epoch 12425/30000 Training Loss: 0.045809246599674225\n",
      "Epoch 12426/30000 Training Loss: 0.06608249247074127\n",
      "Epoch 12427/30000 Training Loss: 0.03459049388766289\n",
      "Epoch 12428/30000 Training Loss: 0.05605686455965042\n",
      "Epoch 12429/30000 Training Loss: 0.04042145982384682\n",
      "Epoch 12430/30000 Training Loss: 0.04444289952516556\n",
      "Epoch 12431/30000 Training Loss: 0.07129327952861786\n",
      "Epoch 12432/30000 Training Loss: 0.056023821234703064\n",
      "Epoch 12433/30000 Training Loss: 0.05928496643900871\n",
      "Epoch 12434/30000 Training Loss: 0.05709303542971611\n",
      "Epoch 12435/30000 Training Loss: 0.06169562041759491\n",
      "Epoch 12436/30000 Training Loss: 0.0463859848678112\n",
      "Epoch 12437/30000 Training Loss: 0.03516406565904617\n",
      "Epoch 12438/30000 Training Loss: 0.0591229610145092\n",
      "Epoch 12439/30000 Training Loss: 0.059650734066963196\n",
      "Epoch 12440/30000 Training Loss: 0.04547061771154404\n",
      "Epoch 12441/30000 Training Loss: 0.05232171714305878\n",
      "Epoch 12442/30000 Training Loss: 0.05198892578482628\n",
      "Epoch 12443/30000 Training Loss: 0.04468200355768204\n",
      "Epoch 12444/30000 Training Loss: 0.05219884216785431\n",
      "Epoch 12445/30000 Training Loss: 0.046985987573862076\n",
      "Epoch 12446/30000 Training Loss: 0.04331839457154274\n",
      "Epoch 12447/30000 Training Loss: 0.04653341323137283\n",
      "Epoch 12448/30000 Training Loss: 0.035717666149139404\n",
      "Epoch 12449/30000 Training Loss: 0.045244745910167694\n",
      "Epoch 12450/30000 Training Loss: 0.0421723835170269\n",
      "Epoch 12451/30000 Training Loss: 0.04048394784331322\n",
      "Epoch 12452/30000 Training Loss: 0.04578199237585068\n",
      "Epoch 12453/30000 Training Loss: 0.02783375047147274\n",
      "Epoch 12454/30000 Training Loss: 0.02999958209693432\n",
      "Epoch 12455/30000 Training Loss: 0.055287305265665054\n",
      "Epoch 12456/30000 Training Loss: 0.04326196759939194\n",
      "Epoch 12457/30000 Training Loss: 0.053346727043390274\n",
      "Epoch 12458/30000 Training Loss: 0.06327658891677856\n",
      "Epoch 12459/30000 Training Loss: 0.04392307624220848\n",
      "Epoch 12460/30000 Training Loss: 0.04687165468931198\n",
      "Epoch 12461/30000 Training Loss: 0.04700211063027382\n",
      "Epoch 12462/30000 Training Loss: 0.04758657142519951\n",
      "Epoch 12463/30000 Training Loss: 0.058035314083099365\n",
      "Epoch 12464/30000 Training Loss: 0.05945156514644623\n",
      "Epoch 12465/30000 Training Loss: 0.03851327300071716\n",
      "Epoch 12466/30000 Training Loss: 0.04009002074599266\n",
      "Epoch 12467/30000 Training Loss: 0.05180041864514351\n",
      "Epoch 12468/30000 Training Loss: 0.04607025906443596\n",
      "Epoch 12469/30000 Training Loss: 0.04130585119128227\n",
      "Epoch 12470/30000 Training Loss: 0.04750779643654823\n",
      "Epoch 12471/30000 Training Loss: 0.06110432371497154\n",
      "Epoch 12472/30000 Training Loss: 0.05455593764781952\n",
      "Epoch 12473/30000 Training Loss: 0.04289679974317551\n",
      "Epoch 12474/30000 Training Loss: 0.04628727585077286\n",
      "Epoch 12475/30000 Training Loss: 0.03642343357205391\n",
      "Epoch 12476/30000 Training Loss: 0.048898231238126755\n",
      "Epoch 12477/30000 Training Loss: 0.044920533895492554\n",
      "Epoch 12478/30000 Training Loss: 0.0604812316596508\n",
      "Epoch 12479/30000 Training Loss: 0.04005083814263344\n",
      "Epoch 12480/30000 Training Loss: 0.04525415599346161\n",
      "Epoch 12481/30000 Training Loss: 0.043303195387125015\n",
      "Epoch 12482/30000 Training Loss: 0.04299129918217659\n",
      "Epoch 12483/30000 Training Loss: 0.05685107782483101\n",
      "Epoch 12484/30000 Training Loss: 0.051845911890268326\n",
      "Epoch 12485/30000 Training Loss: 0.04085971787571907\n",
      "Epoch 12486/30000 Training Loss: 0.03866029903292656\n",
      "Epoch 12487/30000 Training Loss: 0.08158168941736221\n",
      "Epoch 12488/30000 Training Loss: 0.050129484385252\n",
      "Epoch 12489/30000 Training Loss: 0.03472348302602768\n",
      "Epoch 12490/30000 Training Loss: 0.03274380788207054\n",
      "Epoch 12491/30000 Training Loss: 0.05730422958731651\n",
      "Epoch 12492/30000 Training Loss: 0.05266931653022766\n",
      "Epoch 12493/30000 Training Loss: 0.04310644045472145\n",
      "Epoch 12494/30000 Training Loss: 0.04249878227710724\n",
      "Epoch 12495/30000 Training Loss: 0.049428146332502365\n",
      "Epoch 12496/30000 Training Loss: 0.039176564663648605\n",
      "Epoch 12497/30000 Training Loss: 0.03975303843617439\n",
      "Epoch 12498/30000 Training Loss: 0.05330100655555725\n",
      "Epoch 12499/30000 Training Loss: 0.05095887929201126\n",
      "Epoch 12500/30000 Training Loss: 0.057792216539382935\n",
      "Epoch 12500/30000 Validation Loss: 0.05236554145812988\n",
      "Epoch 12501/30000 Training Loss: 0.0353064239025116\n",
      "Epoch 12502/30000 Training Loss: 0.06278011947870255\n",
      "Epoch 12503/30000 Training Loss: 0.0322711206972599\n",
      "Epoch 12504/30000 Training Loss: 0.05911090224981308\n",
      "Epoch 12505/30000 Training Loss: 0.04015052691102028\n",
      "Epoch 12506/30000 Training Loss: 0.04840391129255295\n",
      "Epoch 12507/30000 Training Loss: 0.05452665686607361\n",
      "Epoch 12508/30000 Training Loss: 0.045600056648254395\n",
      "Epoch 12509/30000 Training Loss: 0.06092965602874756\n",
      "Epoch 12510/30000 Training Loss: 0.04717497527599335\n",
      "Epoch 12511/30000 Training Loss: 0.04204554483294487\n",
      "Epoch 12512/30000 Training Loss: 0.04721269756555557\n",
      "Epoch 12513/30000 Training Loss: 0.03992438316345215\n",
      "Epoch 12514/30000 Training Loss: 0.047125816345214844\n",
      "Epoch 12515/30000 Training Loss: 0.0441402941942215\n",
      "Epoch 12516/30000 Training Loss: 0.0571916438639164\n",
      "Epoch 12517/30000 Training Loss: 0.04389830678701401\n",
      "Epoch 12518/30000 Training Loss: 0.05116710439324379\n",
      "Epoch 12519/30000 Training Loss: 0.045034632086753845\n",
      "Epoch 12520/30000 Training Loss: 0.04553773254156113\n",
      "Epoch 12521/30000 Training Loss: 0.04593890532851219\n",
      "Epoch 12522/30000 Training Loss: 0.04109510779380798\n",
      "Epoch 12523/30000 Training Loss: 0.037977319210767746\n",
      "Epoch 12524/30000 Training Loss: 0.04629712551832199\n",
      "Epoch 12525/30000 Training Loss: 0.04883553460240364\n",
      "Epoch 12526/30000 Training Loss: 0.05515611544251442\n",
      "Epoch 12527/30000 Training Loss: 0.05519437789916992\n",
      "Epoch 12528/30000 Training Loss: 0.044754430651664734\n",
      "Epoch 12529/30000 Training Loss: 0.04425513371825218\n",
      "Epoch 12530/30000 Training Loss: 0.049638498574495316\n",
      "Epoch 12531/30000 Training Loss: 0.05161783844232559\n",
      "Epoch 12532/30000 Training Loss: 0.03228214383125305\n",
      "Epoch 12533/30000 Training Loss: 0.058880530297756195\n",
      "Epoch 12534/30000 Training Loss: 0.04050077870488167\n",
      "Epoch 12535/30000 Training Loss: 0.04996100068092346\n",
      "Epoch 12536/30000 Training Loss: 0.04111748933792114\n",
      "Epoch 12537/30000 Training Loss: 0.042514730244874954\n",
      "Epoch 12538/30000 Training Loss: 0.04908078908920288\n",
      "Epoch 12539/30000 Training Loss: 0.045920807868242264\n",
      "Epoch 12540/30000 Training Loss: 0.046750057488679886\n",
      "Epoch 12541/30000 Training Loss: 0.04607236385345459\n",
      "Epoch 12542/30000 Training Loss: 0.04862198978662491\n",
      "Epoch 12543/30000 Training Loss: 0.04287007823586464\n",
      "Epoch 12544/30000 Training Loss: 0.045792706310749054\n",
      "Epoch 12545/30000 Training Loss: 0.053152114152908325\n",
      "Epoch 12546/30000 Training Loss: 0.03606082499027252\n",
      "Epoch 12547/30000 Training Loss: 0.046136096119880676\n",
      "Epoch 12548/30000 Training Loss: 0.04888652265071869\n",
      "Epoch 12549/30000 Training Loss: 0.051726728677749634\n",
      "Epoch 12550/30000 Training Loss: 0.043379783630371094\n",
      "Epoch 12551/30000 Training Loss: 0.04650160297751427\n",
      "Epoch 12552/30000 Training Loss: 0.046199262142181396\n",
      "Epoch 12553/30000 Training Loss: 0.05387873947620392\n",
      "Epoch 12554/30000 Training Loss: 0.05184931308031082\n",
      "Epoch 12555/30000 Training Loss: 0.048829976469278336\n",
      "Epoch 12556/30000 Training Loss: 0.061614640057086945\n",
      "Epoch 12557/30000 Training Loss: 0.05612847954034805\n",
      "Epoch 12558/30000 Training Loss: 0.04654460400342941\n",
      "Epoch 12559/30000 Training Loss: 0.051011450588703156\n",
      "Epoch 12560/30000 Training Loss: 0.05810727924108505\n",
      "Epoch 12561/30000 Training Loss: 0.04681294783949852\n",
      "Epoch 12562/30000 Training Loss: 0.05826404318213463\n",
      "Epoch 12563/30000 Training Loss: 0.04013943672180176\n",
      "Epoch 12564/30000 Training Loss: 0.04862797260284424\n",
      "Epoch 12565/30000 Training Loss: 0.04304840788245201\n",
      "Epoch 12566/30000 Training Loss: 0.06275089830160141\n",
      "Epoch 12567/30000 Training Loss: 0.05473729223012924\n",
      "Epoch 12568/30000 Training Loss: 0.04572103172540665\n",
      "Epoch 12569/30000 Training Loss: 0.03886018320918083\n",
      "Epoch 12570/30000 Training Loss: 0.04779282957315445\n",
      "Epoch 12571/30000 Training Loss: 0.04189140722155571\n",
      "Epoch 12572/30000 Training Loss: 0.04199383035302162\n",
      "Epoch 12573/30000 Training Loss: 0.05212443321943283\n",
      "Epoch 12574/30000 Training Loss: 0.045334771275520325\n",
      "Epoch 12575/30000 Training Loss: 0.04434662312269211\n",
      "Epoch 12576/30000 Training Loss: 0.04425815865397453\n",
      "Epoch 12577/30000 Training Loss: 0.04495725408196449\n",
      "Epoch 12578/30000 Training Loss: 0.04418141394853592\n",
      "Epoch 12579/30000 Training Loss: 0.04790247231721878\n",
      "Epoch 12580/30000 Training Loss: 0.04554019123315811\n",
      "Epoch 12581/30000 Training Loss: 0.06010344997048378\n",
      "Epoch 12582/30000 Training Loss: 0.047170333564281464\n",
      "Epoch 12583/30000 Training Loss: 0.031999371945858\n",
      "Epoch 12584/30000 Training Loss: 0.05060802027583122\n",
      "Epoch 12585/30000 Training Loss: 0.06164021044969559\n",
      "Epoch 12586/30000 Training Loss: 0.05415046215057373\n",
      "Epoch 12587/30000 Training Loss: 0.03752223402261734\n",
      "Epoch 12588/30000 Training Loss: 0.03859765827655792\n",
      "Epoch 12589/30000 Training Loss: 0.05775504559278488\n",
      "Epoch 12590/30000 Training Loss: 0.0596671923995018\n",
      "Epoch 12591/30000 Training Loss: 0.05430559813976288\n",
      "Epoch 12592/30000 Training Loss: 0.04115954786539078\n",
      "Epoch 12593/30000 Training Loss: 0.05741465091705322\n",
      "Epoch 12594/30000 Training Loss: 0.05977220833301544\n",
      "Epoch 12595/30000 Training Loss: 0.0421854667365551\n",
      "Epoch 12596/30000 Training Loss: 0.04973986744880676\n",
      "Epoch 12597/30000 Training Loss: 0.04536491632461548\n",
      "Epoch 12598/30000 Training Loss: 0.039426207542419434\n",
      "Epoch 12599/30000 Training Loss: 0.04603632539510727\n",
      "Epoch 12600/30000 Training Loss: 0.0425836443901062\n",
      "Epoch 12600/30000 Validation Loss: 0.04894950985908508\n",
      "Epoch 12601/30000 Training Loss: 0.048388101160526276\n",
      "Epoch 12602/30000 Training Loss: 0.03154478594660759\n",
      "Epoch 12603/30000 Training Loss: 0.033675599843263626\n",
      "Epoch 12604/30000 Training Loss: 0.05195172503590584\n",
      "Epoch 12605/30000 Training Loss: 0.05290874093770981\n",
      "Epoch 12606/30000 Training Loss: 0.044404856860637665\n",
      "Epoch 12607/30000 Training Loss: 0.04388591647148132\n",
      "Epoch 12608/30000 Training Loss: 0.03183889389038086\n",
      "Epoch 12609/30000 Training Loss: 0.04066510498523712\n",
      "Epoch 12610/30000 Training Loss: 0.03658653795719147\n",
      "Epoch 12611/30000 Training Loss: 0.04595448076725006\n",
      "Epoch 12612/30000 Training Loss: 0.04274749755859375\n",
      "Epoch 12613/30000 Training Loss: 0.041619665920734406\n",
      "Epoch 12614/30000 Training Loss: 0.04690127819776535\n",
      "Epoch 12615/30000 Training Loss: 0.0457659512758255\n",
      "Epoch 12616/30000 Training Loss: 0.038474589586257935\n",
      "Epoch 12617/30000 Training Loss: 0.04777507483959198\n",
      "Epoch 12618/30000 Training Loss: 0.04302053153514862\n",
      "Epoch 12619/30000 Training Loss: 0.05811828747391701\n",
      "Epoch 12620/30000 Training Loss: 0.0441378578543663\n",
      "Epoch 12621/30000 Training Loss: 0.056706566363573074\n",
      "Epoch 12622/30000 Training Loss: 0.0642128512263298\n",
      "Epoch 12623/30000 Training Loss: 0.04811841621994972\n",
      "Epoch 12624/30000 Training Loss: 0.04422536492347717\n",
      "Epoch 12625/30000 Training Loss: 0.05476801469922066\n",
      "Epoch 12626/30000 Training Loss: 0.04679231345653534\n",
      "Epoch 12627/30000 Training Loss: 0.047152239829301834\n",
      "Epoch 12628/30000 Training Loss: 0.04566764086484909\n",
      "Epoch 12629/30000 Training Loss: 0.05781450495123863\n",
      "Epoch 12630/30000 Training Loss: 0.06571284681558609\n",
      "Epoch 12631/30000 Training Loss: 0.05610860511660576\n",
      "Epoch 12632/30000 Training Loss: 0.0381837822496891\n",
      "Epoch 12633/30000 Training Loss: 0.04239159822463989\n",
      "Epoch 12634/30000 Training Loss: 0.03834037482738495\n",
      "Epoch 12635/30000 Training Loss: 0.04712167754769325\n",
      "Epoch 12636/30000 Training Loss: 0.03506806492805481\n",
      "Epoch 12637/30000 Training Loss: 0.04552143067121506\n",
      "Epoch 12638/30000 Training Loss: 0.06792937219142914\n",
      "Epoch 12639/30000 Training Loss: 0.0609317421913147\n",
      "Epoch 12640/30000 Training Loss: 0.05167192220687866\n",
      "Epoch 12641/30000 Training Loss: 0.05396810919046402\n",
      "Epoch 12642/30000 Training Loss: 0.062069639563560486\n",
      "Epoch 12643/30000 Training Loss: 0.04733792692422867\n",
      "Epoch 12644/30000 Training Loss: 0.03750089183449745\n",
      "Epoch 12645/30000 Training Loss: 0.06536104530096054\n",
      "Epoch 12646/30000 Training Loss: 0.06407881528139114\n",
      "Epoch 12647/30000 Training Loss: 0.043866273015737534\n",
      "Epoch 12648/30000 Training Loss: 0.04421541094779968\n",
      "Epoch 12649/30000 Training Loss: 0.05309518426656723\n",
      "Epoch 12650/30000 Training Loss: 0.050290338695049286\n",
      "Epoch 12651/30000 Training Loss: 0.04790390282869339\n",
      "Epoch 12652/30000 Training Loss: 0.05475466698408127\n",
      "Epoch 12653/30000 Training Loss: 0.03896406292915344\n",
      "Epoch 12654/30000 Training Loss: 0.054090097546577454\n",
      "Epoch 12655/30000 Training Loss: 0.05424769967794418\n",
      "Epoch 12656/30000 Training Loss: 0.04717817157506943\n",
      "Epoch 12657/30000 Training Loss: 0.043892789632081985\n",
      "Epoch 12658/30000 Training Loss: 0.050951115787029266\n",
      "Epoch 12659/30000 Training Loss: 0.04551248997449875\n",
      "Epoch 12660/30000 Training Loss: 0.04320031404495239\n",
      "Epoch 12661/30000 Training Loss: 0.050319842994213104\n",
      "Epoch 12662/30000 Training Loss: 0.05376812070608139\n",
      "Epoch 12663/30000 Training Loss: 0.039923347532749176\n",
      "Epoch 12664/30000 Training Loss: 0.0440891869366169\n",
      "Epoch 12665/30000 Training Loss: 0.05174881964921951\n",
      "Epoch 12666/30000 Training Loss: 0.04951992258429527\n",
      "Epoch 12667/30000 Training Loss: 0.03770119696855545\n",
      "Epoch 12668/30000 Training Loss: 0.04631088674068451\n",
      "Epoch 12669/30000 Training Loss: 0.05540980026125908\n",
      "Epoch 12670/30000 Training Loss: 0.036798443645238876\n",
      "Epoch 12671/30000 Training Loss: 0.053499795496463776\n",
      "Epoch 12672/30000 Training Loss: 0.05126208811998367\n",
      "Epoch 12673/30000 Training Loss: 0.04033549875020981\n",
      "Epoch 12674/30000 Training Loss: 0.03366861492395401\n",
      "Epoch 12675/30000 Training Loss: 0.044670142233371735\n",
      "Epoch 12676/30000 Training Loss: 0.0468016155064106\n",
      "Epoch 12677/30000 Training Loss: 0.05879989638924599\n",
      "Epoch 12678/30000 Training Loss: 0.0517784059047699\n",
      "Epoch 12679/30000 Training Loss: 0.05097569525241852\n",
      "Epoch 12680/30000 Training Loss: 0.04785819351673126\n",
      "Epoch 12681/30000 Training Loss: 0.03948391228914261\n",
      "Epoch 12682/30000 Training Loss: 0.04565846547484398\n",
      "Epoch 12683/30000 Training Loss: 0.06258606910705566\n",
      "Epoch 12684/30000 Training Loss: 0.04305893927812576\n",
      "Epoch 12685/30000 Training Loss: 0.0615960955619812\n",
      "Epoch 12686/30000 Training Loss: 0.056183822453022\n",
      "Epoch 12687/30000 Training Loss: 0.050452761352062225\n",
      "Epoch 12688/30000 Training Loss: 0.04167742654681206\n",
      "Epoch 12689/30000 Training Loss: 0.031914468854665756\n",
      "Epoch 12690/30000 Training Loss: 0.04313354194164276\n",
      "Epoch 12691/30000 Training Loss: 0.04716388136148453\n",
      "Epoch 12692/30000 Training Loss: 0.03859075531363487\n",
      "Epoch 12693/30000 Training Loss: 0.04659188538789749\n",
      "Epoch 12694/30000 Training Loss: 0.06567314267158508\n",
      "Epoch 12695/30000 Training Loss: 0.04288976266980171\n",
      "Epoch 12696/30000 Training Loss: 0.041617147624492645\n",
      "Epoch 12697/30000 Training Loss: 0.04920172691345215\n",
      "Epoch 12698/30000 Training Loss: 0.04927185922861099\n",
      "Epoch 12699/30000 Training Loss: 0.05856262892484665\n",
      "Epoch 12700/30000 Training Loss: 0.059732019901275635\n",
      "Epoch 12700/30000 Validation Loss: 0.06058963015675545\n",
      "Epoch 12701/30000 Training Loss: 0.0538034662604332\n",
      "Epoch 12702/30000 Training Loss: 0.03356587514281273\n",
      "Epoch 12703/30000 Training Loss: 0.050306208431720734\n",
      "Epoch 12704/30000 Training Loss: 0.05171685665845871\n",
      "Epoch 12705/30000 Training Loss: 0.046609848737716675\n",
      "Epoch 12706/30000 Training Loss: 0.03871173784136772\n",
      "Epoch 12707/30000 Training Loss: 0.044785626232624054\n",
      "Epoch 12708/30000 Training Loss: 0.04499903321266174\n",
      "Epoch 12709/30000 Training Loss: 0.047564685344696045\n",
      "Epoch 12710/30000 Training Loss: 0.04161222279071808\n",
      "Epoch 12711/30000 Training Loss: 0.0455513522028923\n",
      "Epoch 12712/30000 Training Loss: 0.04550869017839432\n",
      "Epoch 12713/30000 Training Loss: 0.058548394590616226\n",
      "Epoch 12714/30000 Training Loss: 0.047739461064338684\n",
      "Epoch 12715/30000 Training Loss: 0.04824826866388321\n",
      "Epoch 12716/30000 Training Loss: 0.04326416924595833\n",
      "Epoch 12717/30000 Training Loss: 0.03628281503915787\n",
      "Epoch 12718/30000 Training Loss: 0.049830272793769836\n",
      "Epoch 12719/30000 Training Loss: 0.04696477949619293\n",
      "Epoch 12720/30000 Training Loss: 0.04958725720643997\n",
      "Epoch 12721/30000 Training Loss: 0.05800199881196022\n",
      "Epoch 12722/30000 Training Loss: 0.040491536259651184\n",
      "Epoch 12723/30000 Training Loss: 0.046749599277973175\n",
      "Epoch 12724/30000 Training Loss: 0.05164700374007225\n",
      "Epoch 12725/30000 Training Loss: 0.04063788056373596\n",
      "Epoch 12726/30000 Training Loss: 0.05593755841255188\n",
      "Epoch 12727/30000 Training Loss: 0.056086465716362\n",
      "Epoch 12728/30000 Training Loss: 0.04783792048692703\n",
      "Epoch 12729/30000 Training Loss: 0.052010875195264816\n",
      "Epoch 12730/30000 Training Loss: 0.046693846583366394\n",
      "Epoch 12731/30000 Training Loss: 0.037270016968250275\n",
      "Epoch 12732/30000 Training Loss: 0.04943913593888283\n",
      "Epoch 12733/30000 Training Loss: 0.054097261279821396\n",
      "Epoch 12734/30000 Training Loss: 0.04844450205564499\n",
      "Epoch 12735/30000 Training Loss: 0.041898779571056366\n",
      "Epoch 12736/30000 Training Loss: 0.05470549315214157\n",
      "Epoch 12737/30000 Training Loss: 0.05290999263525009\n",
      "Epoch 12738/30000 Training Loss: 0.04377937316894531\n",
      "Epoch 12739/30000 Training Loss: 0.042292702943086624\n",
      "Epoch 12740/30000 Training Loss: 0.045298006385564804\n",
      "Epoch 12741/30000 Training Loss: 0.046369776129722595\n",
      "Epoch 12742/30000 Training Loss: 0.056186750531196594\n",
      "Epoch 12743/30000 Training Loss: 0.04533647373318672\n",
      "Epoch 12744/30000 Training Loss: 0.04012703895568848\n",
      "Epoch 12745/30000 Training Loss: 0.04525163769721985\n",
      "Epoch 12746/30000 Training Loss: 0.03943059593439102\n",
      "Epoch 12747/30000 Training Loss: 0.04170004278421402\n",
      "Epoch 12748/30000 Training Loss: 0.049748972058296204\n",
      "Epoch 12749/30000 Training Loss: 0.04865220561623573\n",
      "Epoch 12750/30000 Training Loss: 0.04191216826438904\n",
      "Epoch 12751/30000 Training Loss: 0.043070003390312195\n",
      "Epoch 12752/30000 Training Loss: 0.04308325797319412\n",
      "Epoch 12753/30000 Training Loss: 0.043066877871751785\n",
      "Epoch 12754/30000 Training Loss: 0.06327349692583084\n",
      "Epoch 12755/30000 Training Loss: 0.06185813248157501\n",
      "Epoch 12756/30000 Training Loss: 0.05213668942451477\n",
      "Epoch 12757/30000 Training Loss: 0.04074703902006149\n",
      "Epoch 12758/30000 Training Loss: 0.047245852649211884\n",
      "Epoch 12759/30000 Training Loss: 0.04799559712409973\n",
      "Epoch 12760/30000 Training Loss: 0.050113409757614136\n",
      "Epoch 12761/30000 Training Loss: 0.05447633937001228\n",
      "Epoch 12762/30000 Training Loss: 0.045368798077106476\n",
      "Epoch 12763/30000 Training Loss: 0.04735811427235603\n",
      "Epoch 12764/30000 Training Loss: 0.055481068789958954\n",
      "Epoch 12765/30000 Training Loss: 0.034209754317998886\n",
      "Epoch 12766/30000 Training Loss: 0.04650338739156723\n",
      "Epoch 12767/30000 Training Loss: 0.039747290313243866\n",
      "Epoch 12768/30000 Training Loss: 0.043860360980033875\n",
      "Epoch 12769/30000 Training Loss: 0.04502769187092781\n",
      "Epoch 12770/30000 Training Loss: 0.045865077525377274\n",
      "Epoch 12771/30000 Training Loss: 0.04281876981258392\n",
      "Epoch 12772/30000 Training Loss: 0.04330427944660187\n",
      "Epoch 12773/30000 Training Loss: 0.05812237784266472\n",
      "Epoch 12774/30000 Training Loss: 0.0430084764957428\n",
      "Epoch 12775/30000 Training Loss: 0.06742307543754578\n",
      "Epoch 12776/30000 Training Loss: 0.04506244510412216\n",
      "Epoch 12777/30000 Training Loss: 0.04414675384759903\n",
      "Epoch 12778/30000 Training Loss: 0.04237460345029831\n",
      "Epoch 12779/30000 Training Loss: 0.040653228759765625\n",
      "Epoch 12780/30000 Training Loss: 0.040122974663972855\n",
      "Epoch 12781/30000 Training Loss: 0.044075049459934235\n",
      "Epoch 12782/30000 Training Loss: 0.0421564057469368\n",
      "Epoch 12783/30000 Training Loss: 0.03488226607441902\n",
      "Epoch 12784/30000 Training Loss: 0.03537951409816742\n",
      "Epoch 12785/30000 Training Loss: 0.04794992879033089\n",
      "Epoch 12786/30000 Training Loss: 0.03968511521816254\n",
      "Epoch 12787/30000 Training Loss: 0.046691495925188065\n",
      "Epoch 12788/30000 Training Loss: 0.038866326212882996\n",
      "Epoch 12789/30000 Training Loss: 0.042417775839567184\n",
      "Epoch 12790/30000 Training Loss: 0.06015350669622421\n",
      "Epoch 12791/30000 Training Loss: 0.04604199156165123\n",
      "Epoch 12792/30000 Training Loss: 0.044884685426950455\n",
      "Epoch 12793/30000 Training Loss: 0.03908538073301315\n",
      "Epoch 12794/30000 Training Loss: 0.05188766121864319\n",
      "Epoch 12795/30000 Training Loss: 0.05423961207270622\n",
      "Epoch 12796/30000 Training Loss: 0.06702478975057602\n",
      "Epoch 12797/30000 Training Loss: 0.06821456551551819\n",
      "Epoch 12798/30000 Training Loss: 0.050185903906822205\n",
      "Epoch 12799/30000 Training Loss: 0.06233493983745575\n",
      "Epoch 12800/30000 Training Loss: 0.046262312680482864\n",
      "Epoch 12800/30000 Validation Loss: 0.045639410614967346\n",
      "Epoch 12801/30000 Training Loss: 0.051027365028858185\n",
      "Epoch 12802/30000 Training Loss: 0.04811514914035797\n",
      "Epoch 12803/30000 Training Loss: 0.051494404673576355\n",
      "Epoch 12804/30000 Training Loss: 0.04262455552816391\n",
      "Epoch 12805/30000 Training Loss: 0.04148548096418381\n",
      "Epoch 12806/30000 Training Loss: 0.04474738985300064\n",
      "Epoch 12807/30000 Training Loss: 0.03676622360944748\n",
      "Epoch 12808/30000 Training Loss: 0.04462762549519539\n",
      "Epoch 12809/30000 Training Loss: 0.04224587231874466\n",
      "Epoch 12810/30000 Training Loss: 0.04671599343419075\n",
      "Epoch 12811/30000 Training Loss: 0.05882759764790535\n",
      "Epoch 12812/30000 Training Loss: 0.04731936752796173\n",
      "Epoch 12813/30000 Training Loss: 0.044173430651426315\n",
      "Epoch 12814/30000 Training Loss: 0.05103718489408493\n",
      "Epoch 12815/30000 Training Loss: 0.05426487326622009\n",
      "Epoch 12816/30000 Training Loss: 0.047219257801771164\n",
      "Epoch 12817/30000 Training Loss: 0.03671909123659134\n",
      "Epoch 12818/30000 Training Loss: 0.029570549726486206\n",
      "Epoch 12819/30000 Training Loss: 0.05226731672883034\n",
      "Epoch 12820/30000 Training Loss: 0.06467088311910629\n",
      "Epoch 12821/30000 Training Loss: 0.055295445024967194\n",
      "Epoch 12822/30000 Training Loss: 0.03281990811228752\n",
      "Epoch 12823/30000 Training Loss: 0.04714246839284897\n",
      "Epoch 12824/30000 Training Loss: 0.05185952037572861\n",
      "Epoch 12825/30000 Training Loss: 0.035110317170619965\n",
      "Epoch 12826/30000 Training Loss: 0.04095170646905899\n",
      "Epoch 12827/30000 Training Loss: 0.048691801726818085\n",
      "Epoch 12828/30000 Training Loss: 0.04234222322702408\n",
      "Epoch 12829/30000 Training Loss: 0.041903793811798096\n",
      "Epoch 12830/30000 Training Loss: 0.03293726593255997\n",
      "Epoch 12831/30000 Training Loss: 0.04692806303501129\n",
      "Epoch 12832/30000 Training Loss: 0.058236971497535706\n",
      "Epoch 12833/30000 Training Loss: 0.041508324444293976\n",
      "Epoch 12834/30000 Training Loss: 0.06309307366609573\n",
      "Epoch 12835/30000 Training Loss: 0.0387449711561203\n",
      "Epoch 12836/30000 Training Loss: 0.049061745405197144\n",
      "Epoch 12837/30000 Training Loss: 0.04542996734380722\n",
      "Epoch 12838/30000 Training Loss: 0.04208841174840927\n",
      "Epoch 12839/30000 Training Loss: 0.05815567076206207\n",
      "Epoch 12840/30000 Training Loss: 0.04579731822013855\n",
      "Epoch 12841/30000 Training Loss: 0.03607788681983948\n",
      "Epoch 12842/30000 Training Loss: 0.0517791323363781\n",
      "Epoch 12843/30000 Training Loss: 0.05008698254823685\n",
      "Epoch 12844/30000 Training Loss: 0.03750641644001007\n",
      "Epoch 12845/30000 Training Loss: 0.050807978957891464\n",
      "Epoch 12846/30000 Training Loss: 0.04859774559736252\n",
      "Epoch 12847/30000 Training Loss: 0.04699384421110153\n",
      "Epoch 12848/30000 Training Loss: 0.035715557634830475\n",
      "Epoch 12849/30000 Training Loss: 0.04938783869147301\n",
      "Epoch 12850/30000 Training Loss: 0.040563564747571945\n",
      "Epoch 12851/30000 Training Loss: 0.0675492063164711\n",
      "Epoch 12852/30000 Training Loss: 0.04934976249933243\n",
      "Epoch 12853/30000 Training Loss: 0.04399890452623367\n",
      "Epoch 12854/30000 Training Loss: 0.04205363988876343\n",
      "Epoch 12855/30000 Training Loss: 0.0570068359375\n",
      "Epoch 12856/30000 Training Loss: 0.03890174999833107\n",
      "Epoch 12857/30000 Training Loss: 0.05826716125011444\n",
      "Epoch 12858/30000 Training Loss: 0.04274939000606537\n",
      "Epoch 12859/30000 Training Loss: 0.040067046880722046\n",
      "Epoch 12860/30000 Training Loss: 0.04447346180677414\n",
      "Epoch 12861/30000 Training Loss: 0.040718674659729004\n",
      "Epoch 12862/30000 Training Loss: 0.04071976989507675\n",
      "Epoch 12863/30000 Training Loss: 0.06146102398633957\n",
      "Epoch 12864/30000 Training Loss: 0.05702768638730049\n",
      "Epoch 12865/30000 Training Loss: 0.04214523360133171\n",
      "Epoch 12866/30000 Training Loss: 0.05363930016756058\n",
      "Epoch 12867/30000 Training Loss: 0.03762165457010269\n",
      "Epoch 12868/30000 Training Loss: 0.0528029166162014\n",
      "Epoch 12869/30000 Training Loss: 0.045227378606796265\n",
      "Epoch 12870/30000 Training Loss: 0.03726494684815407\n",
      "Epoch 12871/30000 Training Loss: 0.04371420294046402\n",
      "Epoch 12872/30000 Training Loss: 0.04024520143866539\n",
      "Epoch 12873/30000 Training Loss: 0.03573966398835182\n",
      "Epoch 12874/30000 Training Loss: 0.059579044580459595\n",
      "Epoch 12875/30000 Training Loss: 0.058936454355716705\n",
      "Epoch 12876/30000 Training Loss: 0.0452372282743454\n",
      "Epoch 12877/30000 Training Loss: 0.05961380898952484\n",
      "Epoch 12878/30000 Training Loss: 0.04996012896299362\n",
      "Epoch 12879/30000 Training Loss: 0.06597809493541718\n",
      "Epoch 12880/30000 Training Loss: 0.05816914886236191\n",
      "Epoch 12881/30000 Training Loss: 0.048894040286540985\n",
      "Epoch 12882/30000 Training Loss: 0.03675297275185585\n",
      "Epoch 12883/30000 Training Loss: 0.050578340888023376\n",
      "Epoch 12884/30000 Training Loss: 0.05519041419029236\n",
      "Epoch 12885/30000 Training Loss: 0.03832617774605751\n",
      "Epoch 12886/30000 Training Loss: 0.04343973845243454\n",
      "Epoch 12887/30000 Training Loss: 0.04652518033981323\n",
      "Epoch 12888/30000 Training Loss: 0.02981463074684143\n",
      "Epoch 12889/30000 Training Loss: 0.05244306102395058\n",
      "Epoch 12890/30000 Training Loss: 0.0345294214785099\n",
      "Epoch 12891/30000 Training Loss: 0.04903699457645416\n",
      "Epoch 12892/30000 Training Loss: 0.04665761440992355\n",
      "Epoch 12893/30000 Training Loss: 0.051805876195430756\n",
      "Epoch 12894/30000 Training Loss: 0.03399881720542908\n",
      "Epoch 12895/30000 Training Loss: 0.04927227646112442\n",
      "Epoch 12896/30000 Training Loss: 0.044750601053237915\n",
      "Epoch 12897/30000 Training Loss: 0.05895376577973366\n",
      "Epoch 12898/30000 Training Loss: 0.03443146497011185\n",
      "Epoch 12899/30000 Training Loss: 0.04775580018758774\n",
      "Epoch 12900/30000 Training Loss: 0.05117664858698845\n",
      "Epoch 12900/30000 Validation Loss: 0.05885345861315727\n",
      "Epoch 12901/30000 Training Loss: 0.03737898916006088\n",
      "Epoch 12902/30000 Training Loss: 0.03871775418519974\n",
      "Epoch 12903/30000 Training Loss: 0.04500669986009598\n",
      "Epoch 12904/30000 Training Loss: 0.0497569814324379\n",
      "Epoch 12905/30000 Training Loss: 0.03457633778452873\n",
      "Epoch 12906/30000 Training Loss: 0.03949960693717003\n",
      "Epoch 12907/30000 Training Loss: 0.046849727630615234\n",
      "Epoch 12908/30000 Training Loss: 0.04075583815574646\n",
      "Epoch 12909/30000 Training Loss: 0.04029814898967743\n",
      "Epoch 12910/30000 Training Loss: 0.04616669937968254\n",
      "Epoch 12911/30000 Training Loss: 0.055124156177043915\n",
      "Epoch 12912/30000 Training Loss: 0.040493398904800415\n",
      "Epoch 12913/30000 Training Loss: 0.05577372759580612\n",
      "Epoch 12914/30000 Training Loss: 0.05936415493488312\n",
      "Epoch 12915/30000 Training Loss: 0.04050321877002716\n",
      "Epoch 12916/30000 Training Loss: 0.04530040919780731\n",
      "Epoch 12917/30000 Training Loss: 0.054449938237667084\n",
      "Epoch 12918/30000 Training Loss: 0.04148028790950775\n",
      "Epoch 12919/30000 Training Loss: 0.04904322326183319\n",
      "Epoch 12920/30000 Training Loss: 0.043813783675432205\n",
      "Epoch 12921/30000 Training Loss: 0.060897164046764374\n",
      "Epoch 12922/30000 Training Loss: 0.05883079767227173\n",
      "Epoch 12923/30000 Training Loss: 0.05106624215841293\n",
      "Epoch 12924/30000 Training Loss: 0.04323992878198624\n",
      "Epoch 12925/30000 Training Loss: 0.04485177993774414\n",
      "Epoch 12926/30000 Training Loss: 0.03358592092990875\n",
      "Epoch 12927/30000 Training Loss: 0.04501393437385559\n",
      "Epoch 12928/30000 Training Loss: 0.04542522132396698\n",
      "Epoch 12929/30000 Training Loss: 0.04350001737475395\n",
      "Epoch 12930/30000 Training Loss: 0.0602683424949646\n",
      "Epoch 12931/30000 Training Loss: 0.04394659027457237\n",
      "Epoch 12932/30000 Training Loss: 0.04441889747977257\n",
      "Epoch 12933/30000 Training Loss: 0.04458208382129669\n",
      "Epoch 12934/30000 Training Loss: 0.046138614416122437\n",
      "Epoch 12935/30000 Training Loss: 0.04727790504693985\n",
      "Epoch 12936/30000 Training Loss: 0.04273393005132675\n",
      "Epoch 12937/30000 Training Loss: 0.054448164999485016\n",
      "Epoch 12938/30000 Training Loss: 0.04288516193628311\n",
      "Epoch 12939/30000 Training Loss: 0.05453706532716751\n",
      "Epoch 12940/30000 Training Loss: 0.054116666316986084\n",
      "Epoch 12941/30000 Training Loss: 0.056409742683172226\n",
      "Epoch 12942/30000 Training Loss: 0.04607142508029938\n",
      "Epoch 12943/30000 Training Loss: 0.054251596331596375\n",
      "Epoch 12944/30000 Training Loss: 0.06189949810504913\n",
      "Epoch 12945/30000 Training Loss: 0.03779207915067673\n",
      "Epoch 12946/30000 Training Loss: 0.040498584508895874\n",
      "Epoch 12947/30000 Training Loss: 0.04179229959845543\n",
      "Epoch 12948/30000 Training Loss: 0.045026689767837524\n",
      "Epoch 12949/30000 Training Loss: 0.05389799177646637\n",
      "Epoch 12950/30000 Training Loss: 0.03723970055580139\n",
      "Epoch 12951/30000 Training Loss: 0.04739375784993172\n",
      "Epoch 12952/30000 Training Loss: 0.05568030849099159\n",
      "Epoch 12953/30000 Training Loss: 0.035147905349731445\n",
      "Epoch 12954/30000 Training Loss: 0.06946777552366257\n",
      "Epoch 12955/30000 Training Loss: 0.06874171644449234\n",
      "Epoch 12956/30000 Training Loss: 0.04408714920282364\n",
      "Epoch 12957/30000 Training Loss: 0.05691041424870491\n",
      "Epoch 12958/30000 Training Loss: 0.04676195979118347\n",
      "Epoch 12959/30000 Training Loss: 0.04081452637910843\n",
      "Epoch 12960/30000 Training Loss: 0.0655251294374466\n",
      "Epoch 12961/30000 Training Loss: 0.04144735261797905\n",
      "Epoch 12962/30000 Training Loss: 0.04598453640937805\n",
      "Epoch 12963/30000 Training Loss: 0.04002458602190018\n",
      "Epoch 12964/30000 Training Loss: 0.054395295679569244\n",
      "Epoch 12965/30000 Training Loss: 0.048788417130708694\n",
      "Epoch 12966/30000 Training Loss: 0.05803769826889038\n",
      "Epoch 12967/30000 Training Loss: 0.03666915372014046\n",
      "Epoch 12968/30000 Training Loss: 0.04140390455722809\n",
      "Epoch 12969/30000 Training Loss: 0.04540199413895607\n",
      "Epoch 12970/30000 Training Loss: 0.055669307708740234\n",
      "Epoch 12971/30000 Training Loss: 0.04615170508623123\n",
      "Epoch 12972/30000 Training Loss: 0.05554184317588806\n",
      "Epoch 12973/30000 Training Loss: 0.04484598711133003\n",
      "Epoch 12974/30000 Training Loss: 0.061123475432395935\n",
      "Epoch 12975/30000 Training Loss: 0.05038286745548248\n",
      "Epoch 12976/30000 Training Loss: 0.05275171995162964\n",
      "Epoch 12977/30000 Training Loss: 0.04487922042608261\n",
      "Epoch 12978/30000 Training Loss: 0.05559346824884415\n",
      "Epoch 12979/30000 Training Loss: 0.056546472012996674\n",
      "Epoch 12980/30000 Training Loss: 0.04128039628267288\n",
      "Epoch 12981/30000 Training Loss: 0.04999934881925583\n",
      "Epoch 12982/30000 Training Loss: 0.045418255031108856\n",
      "Epoch 12983/30000 Training Loss: 0.059874504804611206\n",
      "Epoch 12984/30000 Training Loss: 0.04864887148141861\n",
      "Epoch 12985/30000 Training Loss: 0.0478333905339241\n",
      "Epoch 12986/30000 Training Loss: 0.0507243350148201\n",
      "Epoch 12987/30000 Training Loss: 0.03771446645259857\n",
      "Epoch 12988/30000 Training Loss: 0.05796933174133301\n",
      "Epoch 12989/30000 Training Loss: 0.06045597046613693\n",
      "Epoch 12990/30000 Training Loss: 0.053227078169584274\n",
      "Epoch 12991/30000 Training Loss: 0.04513376206159592\n",
      "Epoch 12992/30000 Training Loss: 0.036352381110191345\n",
      "Epoch 12993/30000 Training Loss: 0.06664509326219559\n",
      "Epoch 12994/30000 Training Loss: 0.04115116968750954\n",
      "Epoch 12995/30000 Training Loss: 0.05012357980012894\n",
      "Epoch 12996/30000 Training Loss: 0.04794438183307648\n",
      "Epoch 12997/30000 Training Loss: 0.04387468844652176\n",
      "Epoch 12998/30000 Training Loss: 0.04223307594656944\n",
      "Epoch 12999/30000 Training Loss: 0.04075733199715614\n",
      "Epoch 13000/30000 Training Loss: 0.047307901084423065\n",
      "Epoch 13000/30000 Validation Loss: 0.04793102294206619\n",
      "Epoch 13001/30000 Training Loss: 0.054397210478782654\n",
      "Epoch 13002/30000 Training Loss: 0.054384924471378326\n",
      "Epoch 13003/30000 Training Loss: 0.05691186338663101\n",
      "Epoch 13004/30000 Training Loss: 0.05047406628727913\n",
      "Epoch 13005/30000 Training Loss: 0.04630240797996521\n",
      "Epoch 13006/30000 Training Loss: 0.04465764760971069\n",
      "Epoch 13007/30000 Training Loss: 0.06152484565973282\n",
      "Epoch 13008/30000 Training Loss: 0.03710492327809334\n",
      "Epoch 13009/30000 Training Loss: 0.038444481790065765\n",
      "Epoch 13010/30000 Training Loss: 0.05411747843027115\n",
      "Epoch 13011/30000 Training Loss: 0.05389832705259323\n",
      "Epoch 13012/30000 Training Loss: 0.041463617235422134\n",
      "Epoch 13013/30000 Training Loss: 0.04540914297103882\n",
      "Epoch 13014/30000 Training Loss: 0.03372535482048988\n",
      "Epoch 13015/30000 Training Loss: 0.03922686725854874\n",
      "Epoch 13016/30000 Training Loss: 0.0489472895860672\n",
      "Epoch 13017/30000 Training Loss: 0.04583314061164856\n",
      "Epoch 13018/30000 Training Loss: 0.04092564433813095\n",
      "Epoch 13019/30000 Training Loss: 0.04609593003988266\n",
      "Epoch 13020/30000 Training Loss: 0.05393435060977936\n",
      "Epoch 13021/30000 Training Loss: 0.04513981193304062\n",
      "Epoch 13022/30000 Training Loss: 0.048636458814144135\n",
      "Epoch 13023/30000 Training Loss: 0.04326413571834564\n",
      "Epoch 13024/30000 Training Loss: 0.05995722860097885\n",
      "Epoch 13025/30000 Training Loss: 0.05665872246026993\n",
      "Epoch 13026/30000 Training Loss: 0.04591920226812363\n",
      "Epoch 13027/30000 Training Loss: 0.0537065751850605\n",
      "Epoch 13028/30000 Training Loss: 0.04290264844894409\n",
      "Epoch 13029/30000 Training Loss: 0.04269599914550781\n",
      "Epoch 13030/30000 Training Loss: 0.0699070394039154\n",
      "Epoch 13031/30000 Training Loss: 0.054344430565834045\n",
      "Epoch 13032/30000 Training Loss: 0.04302424564957619\n",
      "Epoch 13033/30000 Training Loss: 0.04580873250961304\n",
      "Epoch 13034/30000 Training Loss: 0.043341733515262604\n",
      "Epoch 13035/30000 Training Loss: 0.03430166095495224\n",
      "Epoch 13036/30000 Training Loss: 0.04722755029797554\n",
      "Epoch 13037/30000 Training Loss: 0.054726794362068176\n",
      "Epoch 13038/30000 Training Loss: 0.06022777780890465\n",
      "Epoch 13039/30000 Training Loss: 0.04492555558681488\n",
      "Epoch 13040/30000 Training Loss: 0.04722035676240921\n",
      "Epoch 13041/30000 Training Loss: 0.057482048869132996\n",
      "Epoch 13042/30000 Training Loss: 0.06324472278356552\n",
      "Epoch 13043/30000 Training Loss: 0.053146377205848694\n",
      "Epoch 13044/30000 Training Loss: 0.039460502564907074\n",
      "Epoch 13045/30000 Training Loss: 0.05730342119932175\n",
      "Epoch 13046/30000 Training Loss: 0.049569956958293915\n",
      "Epoch 13047/30000 Training Loss: 0.05243566632270813\n",
      "Epoch 13048/30000 Training Loss: 0.038492992520332336\n",
      "Epoch 13049/30000 Training Loss: 0.052480027079582214\n",
      "Epoch 13050/30000 Training Loss: 0.050101928412914276\n",
      "Epoch 13051/30000 Training Loss: 0.04833407700061798\n",
      "Epoch 13052/30000 Training Loss: 0.04379066824913025\n",
      "Epoch 13053/30000 Training Loss: 0.04761327803134918\n",
      "Epoch 13054/30000 Training Loss: 0.04357358068227768\n",
      "Epoch 13055/30000 Training Loss: 0.041689999401569366\n",
      "Epoch 13056/30000 Training Loss: 0.042699068784713745\n",
      "Epoch 13057/30000 Training Loss: 0.06528866291046143\n",
      "Epoch 13058/30000 Training Loss: 0.04527736082673073\n",
      "Epoch 13059/30000 Training Loss: 0.039020854979753494\n",
      "Epoch 13060/30000 Training Loss: 0.050917208194732666\n",
      "Epoch 13061/30000 Training Loss: 0.031582996249198914\n",
      "Epoch 13062/30000 Training Loss: 0.0548117458820343\n",
      "Epoch 13063/30000 Training Loss: 0.053913988173007965\n",
      "Epoch 13064/30000 Training Loss: 0.045074883848428726\n",
      "Epoch 13065/30000 Training Loss: 0.04974943399429321\n",
      "Epoch 13066/30000 Training Loss: 0.04428407549858093\n",
      "Epoch 13067/30000 Training Loss: 0.05143196880817413\n",
      "Epoch 13068/30000 Training Loss: 0.03525574132800102\n",
      "Epoch 13069/30000 Training Loss: 0.05129284784197807\n",
      "Epoch 13070/30000 Training Loss: 0.036964014172554016\n",
      "Epoch 13071/30000 Training Loss: 0.05887151509523392\n",
      "Epoch 13072/30000 Training Loss: 0.039034586399793625\n",
      "Epoch 13073/30000 Training Loss: 0.06076402962207794\n",
      "Epoch 13074/30000 Training Loss: 0.034722745418548584\n",
      "Epoch 13075/30000 Training Loss: 0.06491322070360184\n",
      "Epoch 13076/30000 Training Loss: 0.04415272921323776\n",
      "Epoch 13077/30000 Training Loss: 0.03390319645404816\n",
      "Epoch 13078/30000 Training Loss: 0.05264000594615936\n",
      "Epoch 13079/30000 Training Loss: 0.05105843394994736\n",
      "Epoch 13080/30000 Training Loss: 0.04147886484861374\n",
      "Epoch 13081/30000 Training Loss: 0.0421619638800621\n",
      "Epoch 13082/30000 Training Loss: 0.04798756539821625\n",
      "Epoch 13083/30000 Training Loss: 0.045201003551483154\n",
      "Epoch 13084/30000 Training Loss: 0.043822482228279114\n",
      "Epoch 13085/30000 Training Loss: 0.04372634366154671\n",
      "Epoch 13086/30000 Training Loss: 0.04930061101913452\n",
      "Epoch 13087/30000 Training Loss: 0.04218300059437752\n",
      "Epoch 13088/30000 Training Loss: 0.04113386571407318\n",
      "Epoch 13089/30000 Training Loss: 0.04636014252901077\n",
      "Epoch 13090/30000 Training Loss: 0.062143515795469284\n",
      "Epoch 13091/30000 Training Loss: 0.048671621829271317\n",
      "Epoch 13092/30000 Training Loss: 0.04275774955749512\n",
      "Epoch 13093/30000 Training Loss: 0.04134918749332428\n",
      "Epoch 13094/30000 Training Loss: 0.04321925342082977\n",
      "Epoch 13095/30000 Training Loss: 0.05147663503885269\n",
      "Epoch 13096/30000 Training Loss: 0.03700491786003113\n",
      "Epoch 13097/30000 Training Loss: 0.037837736308574677\n",
      "Epoch 13098/30000 Training Loss: 0.03536880388855934\n",
      "Epoch 13099/30000 Training Loss: 0.04554174095392227\n",
      "Epoch 13100/30000 Training Loss: 0.0537816546857357\n",
      "Epoch 13100/30000 Validation Loss: 0.043377041816711426\n",
      "Epoch 13101/30000 Training Loss: 0.05862053856253624\n",
      "Epoch 13102/30000 Training Loss: 0.044776059687137604\n",
      "Epoch 13103/30000 Training Loss: 0.03849412873387337\n",
      "Epoch 13104/30000 Training Loss: 0.03582906723022461\n",
      "Epoch 13105/30000 Training Loss: 0.035095393657684326\n",
      "Epoch 13106/30000 Training Loss: 0.043901436030864716\n",
      "Epoch 13107/30000 Training Loss: 0.043313127011060715\n",
      "Epoch 13108/30000 Training Loss: 0.06562614440917969\n",
      "Epoch 13109/30000 Training Loss: 0.05207620561122894\n",
      "Epoch 13110/30000 Training Loss: 0.044326622039079666\n",
      "Epoch 13111/30000 Training Loss: 0.05117480829358101\n",
      "Epoch 13112/30000 Training Loss: 0.0519462525844574\n",
      "Epoch 13113/30000 Training Loss: 0.05886487662792206\n",
      "Epoch 13114/30000 Training Loss: 0.0553516149520874\n",
      "Epoch 13115/30000 Training Loss: 0.03779657557606697\n",
      "Epoch 13116/30000 Training Loss: 0.03957904875278473\n",
      "Epoch 13117/30000 Training Loss: 0.052294231951236725\n",
      "Epoch 13118/30000 Training Loss: 0.0418514646589756\n",
      "Epoch 13119/30000 Training Loss: 0.053997211158275604\n",
      "Epoch 13120/30000 Training Loss: 0.05738024413585663\n",
      "Epoch 13121/30000 Training Loss: 0.061055608093738556\n",
      "Epoch 13122/30000 Training Loss: 0.047195568680763245\n",
      "Epoch 13123/30000 Training Loss: 0.04046670347452164\n",
      "Epoch 13124/30000 Training Loss: 0.04575008153915405\n",
      "Epoch 13125/30000 Training Loss: 0.052854347974061966\n",
      "Epoch 13126/30000 Training Loss: 0.048802539706230164\n",
      "Epoch 13127/30000 Training Loss: 0.041687920689582825\n",
      "Epoch 13128/30000 Training Loss: 0.04030333086848259\n",
      "Epoch 13129/30000 Training Loss: 0.057700350880622864\n",
      "Epoch 13130/30000 Training Loss: 0.047449126839637756\n",
      "Epoch 13131/30000 Training Loss: 0.04150353744626045\n",
      "Epoch 13132/30000 Training Loss: 0.044733405113220215\n",
      "Epoch 13133/30000 Training Loss: 0.05615050345659256\n",
      "Epoch 13134/30000 Training Loss: 0.052277542650699615\n",
      "Epoch 13135/30000 Training Loss: 0.05025560408830643\n",
      "Epoch 13136/30000 Training Loss: 0.041311852633953094\n",
      "Epoch 13137/30000 Training Loss: 0.03761666268110275\n",
      "Epoch 13138/30000 Training Loss: 0.06036330759525299\n",
      "Epoch 13139/30000 Training Loss: 0.042092494666576385\n",
      "Epoch 13140/30000 Training Loss: 0.053964488208293915\n",
      "Epoch 13141/30000 Training Loss: 0.04062090069055557\n",
      "Epoch 13142/30000 Training Loss: 0.04180414229631424\n",
      "Epoch 13143/30000 Training Loss: 0.04851359874010086\n",
      "Epoch 13144/30000 Training Loss: 0.042563460767269135\n",
      "Epoch 13145/30000 Training Loss: 0.05248319357633591\n",
      "Epoch 13146/30000 Training Loss: 0.042766422033309937\n",
      "Epoch 13147/30000 Training Loss: 0.05858716368675232\n",
      "Epoch 13148/30000 Training Loss: 0.040742404758930206\n",
      "Epoch 13149/30000 Training Loss: 0.028320077806711197\n",
      "Epoch 13150/30000 Training Loss: 0.035831551998853683\n",
      "Epoch 13151/30000 Training Loss: 0.05643254518508911\n",
      "Epoch 13152/30000 Training Loss: 0.04820006340742111\n",
      "Epoch 13153/30000 Training Loss: 0.04426315426826477\n",
      "Epoch 13154/30000 Training Loss: 0.06970333307981491\n",
      "Epoch 13155/30000 Training Loss: 0.04372023046016693\n",
      "Epoch 13156/30000 Training Loss: 0.05686737596988678\n",
      "Epoch 13157/30000 Training Loss: 0.03521164879202843\n",
      "Epoch 13158/30000 Training Loss: 0.042061224579811096\n",
      "Epoch 13159/30000 Training Loss: 0.042145974934101105\n",
      "Epoch 13160/30000 Training Loss: 0.05519113689661026\n",
      "Epoch 13161/30000 Training Loss: 0.04061833769083023\n",
      "Epoch 13162/30000 Training Loss: 0.05075472965836525\n",
      "Epoch 13163/30000 Training Loss: 0.05407530814409256\n",
      "Epoch 13164/30000 Training Loss: 0.042941272258758545\n",
      "Epoch 13165/30000 Training Loss: 0.04348079115152359\n",
      "Epoch 13166/30000 Training Loss: 0.04350237548351288\n",
      "Epoch 13167/30000 Training Loss: 0.05857545882463455\n",
      "Epoch 13168/30000 Training Loss: 0.051713526248931885\n",
      "Epoch 13169/30000 Training Loss: 0.054682448506355286\n",
      "Epoch 13170/30000 Training Loss: 0.05532093346118927\n",
      "Epoch 13171/30000 Training Loss: 0.030724158510565758\n",
      "Epoch 13172/30000 Training Loss: 0.049058109521865845\n",
      "Epoch 13173/30000 Training Loss: 0.05730051174759865\n",
      "Epoch 13174/30000 Training Loss: 0.048814110457897186\n",
      "Epoch 13175/30000 Training Loss: 0.04488782584667206\n",
      "Epoch 13176/30000 Training Loss: 0.04119504988193512\n",
      "Epoch 13177/30000 Training Loss: 0.05193599313497543\n",
      "Epoch 13178/30000 Training Loss: 0.04176642745733261\n",
      "Epoch 13179/30000 Training Loss: 0.03687435761094093\n",
      "Epoch 13180/30000 Training Loss: 0.04140321910381317\n",
      "Epoch 13181/30000 Training Loss: 0.040057651698589325\n",
      "Epoch 13182/30000 Training Loss: 0.033821314573287964\n",
      "Epoch 13183/30000 Training Loss: 0.03744916617870331\n",
      "Epoch 13184/30000 Training Loss: 0.040373947471380234\n",
      "Epoch 13185/30000 Training Loss: 0.06470401585102081\n",
      "Epoch 13186/30000 Training Loss: 0.047844789922237396\n",
      "Epoch 13187/30000 Training Loss: 0.045462608337402344\n",
      "Epoch 13188/30000 Training Loss: 0.05518508329987526\n",
      "Epoch 13189/30000 Training Loss: 0.056547634303569794\n",
      "Epoch 13190/30000 Training Loss: 0.05042412877082825\n",
      "Epoch 13191/30000 Training Loss: 0.04815076291561127\n",
      "Epoch 13192/30000 Training Loss: 0.04931045323610306\n",
      "Epoch 13193/30000 Training Loss: 0.04493118077516556\n",
      "Epoch 13194/30000 Training Loss: 0.04548199474811554\n",
      "Epoch 13195/30000 Training Loss: 0.04861396551132202\n",
      "Epoch 13196/30000 Training Loss: 0.04794283211231232\n",
      "Epoch 13197/30000 Training Loss: 0.0594375915825367\n",
      "Epoch 13198/30000 Training Loss: 0.046338148415088654\n",
      "Epoch 13199/30000 Training Loss: 0.04211202263832092\n",
      "Epoch 13200/30000 Training Loss: 0.04390447586774826\n",
      "Epoch 13200/30000 Validation Loss: 0.05233605206012726\n",
      "Epoch 13201/30000 Training Loss: 0.06546740233898163\n",
      "Epoch 13202/30000 Training Loss: 0.04603569209575653\n",
      "Epoch 13203/30000 Training Loss: 0.04473816230893135\n",
      "Epoch 13204/30000 Training Loss: 0.053512267768383026\n",
      "Epoch 13205/30000 Training Loss: 0.04914043843746185\n",
      "Epoch 13206/30000 Training Loss: 0.045293040573596954\n",
      "Epoch 13207/30000 Training Loss: 0.032434239983558655\n",
      "Epoch 13208/30000 Training Loss: 0.05637194216251373\n",
      "Epoch 13209/30000 Training Loss: 0.05060204863548279\n",
      "Epoch 13210/30000 Training Loss: 0.053817279636859894\n",
      "Epoch 13211/30000 Training Loss: 0.04811963066458702\n",
      "Epoch 13212/30000 Training Loss: 0.049234770238399506\n",
      "Epoch 13213/30000 Training Loss: 0.05006883293390274\n",
      "Epoch 13214/30000 Training Loss: 0.04256543517112732\n",
      "Epoch 13215/30000 Training Loss: 0.053760480135679245\n",
      "Epoch 13216/30000 Training Loss: 0.03828389197587967\n",
      "Epoch 13217/30000 Training Loss: 0.044562309980392456\n",
      "Epoch 13218/30000 Training Loss: 0.06506367027759552\n",
      "Epoch 13219/30000 Training Loss: 0.05455927550792694\n",
      "Epoch 13220/30000 Training Loss: 0.04808183014392853\n",
      "Epoch 13221/30000 Training Loss: 0.04513898119330406\n",
      "Epoch 13222/30000 Training Loss: 0.05572337657213211\n",
      "Epoch 13223/30000 Training Loss: 0.05076772719621658\n",
      "Epoch 13224/30000 Training Loss: 0.04587464779615402\n",
      "Epoch 13225/30000 Training Loss: 0.039410300552845\n",
      "Epoch 13226/30000 Training Loss: 0.04456901550292969\n",
      "Epoch 13227/30000 Training Loss: 0.06244993209838867\n",
      "Epoch 13228/30000 Training Loss: 0.06482391059398651\n",
      "Epoch 13229/30000 Training Loss: 0.05435009300708771\n",
      "Epoch 13230/30000 Training Loss: 0.05661608651280403\n",
      "Epoch 13231/30000 Training Loss: 0.03640270605683327\n",
      "Epoch 13232/30000 Training Loss: 0.05517418310046196\n",
      "Epoch 13233/30000 Training Loss: 0.046866126358509064\n",
      "Epoch 13234/30000 Training Loss: 0.04705210030078888\n",
      "Epoch 13235/30000 Training Loss: 0.05073247104883194\n",
      "Epoch 13236/30000 Training Loss: 0.06416278332471848\n",
      "Epoch 13237/30000 Training Loss: 0.03929999843239784\n",
      "Epoch 13238/30000 Training Loss: 0.057831697165966034\n",
      "Epoch 13239/30000 Training Loss: 0.04264393448829651\n",
      "Epoch 13240/30000 Training Loss: 0.05429742485284805\n",
      "Epoch 13241/30000 Training Loss: 0.06027646362781525\n",
      "Epoch 13242/30000 Training Loss: 0.03718458488583565\n",
      "Epoch 13243/30000 Training Loss: 0.039407093077898026\n",
      "Epoch 13244/30000 Training Loss: 0.05514993518590927\n",
      "Epoch 13245/30000 Training Loss: 0.06596284359693527\n",
      "Epoch 13246/30000 Training Loss: 0.04507778584957123\n",
      "Epoch 13247/30000 Training Loss: 0.04578447341918945\n",
      "Epoch 13248/30000 Training Loss: 0.05170546472072601\n",
      "Epoch 13249/30000 Training Loss: 0.04588839039206505\n",
      "Epoch 13250/30000 Training Loss: 0.042690470814704895\n",
      "Epoch 13251/30000 Training Loss: 0.05555561184883118\n",
      "Epoch 13252/30000 Training Loss: 0.07566092163324356\n",
      "Epoch 13253/30000 Training Loss: 0.05490865930914879\n",
      "Epoch 13254/30000 Training Loss: 0.0480138435959816\n",
      "Epoch 13255/30000 Training Loss: 0.04630565270781517\n",
      "Epoch 13256/30000 Training Loss: 0.04530777037143707\n",
      "Epoch 13257/30000 Training Loss: 0.03424975275993347\n",
      "Epoch 13258/30000 Training Loss: 0.06132245808839798\n",
      "Epoch 13259/30000 Training Loss: 0.049208641052246094\n",
      "Epoch 13260/30000 Training Loss: 0.04570857435464859\n",
      "Epoch 13261/30000 Training Loss: 0.04936288669705391\n",
      "Epoch 13262/30000 Training Loss: 0.03980160504579544\n",
      "Epoch 13263/30000 Training Loss: 0.030152998864650726\n",
      "Epoch 13264/30000 Training Loss: 0.06298728287220001\n",
      "Epoch 13265/30000 Training Loss: 0.040743183344602585\n",
      "Epoch 13266/30000 Training Loss: 0.048733651638031006\n",
      "Epoch 13267/30000 Training Loss: 0.05268050357699394\n",
      "Epoch 13268/30000 Training Loss: 0.04836971312761307\n",
      "Epoch 13269/30000 Training Loss: 0.050871409475803375\n",
      "Epoch 13270/30000 Training Loss: 0.045173175632953644\n",
      "Epoch 13271/30000 Training Loss: 0.05935002118349075\n",
      "Epoch 13272/30000 Training Loss: 0.0441751666367054\n",
      "Epoch 13273/30000 Training Loss: 0.04725451022386551\n",
      "Epoch 13274/30000 Training Loss: 0.04224996268749237\n",
      "Epoch 13275/30000 Training Loss: 0.046881306916475296\n",
      "Epoch 13276/30000 Training Loss: 0.052906837314367294\n",
      "Epoch 13277/30000 Training Loss: 0.04045948013663292\n",
      "Epoch 13278/30000 Training Loss: 0.06224623695015907\n",
      "Epoch 13279/30000 Training Loss: 0.035949308425188065\n",
      "Epoch 13280/30000 Training Loss: 0.039751000702381134\n",
      "Epoch 13281/30000 Training Loss: 0.04862681031227112\n",
      "Epoch 13282/30000 Training Loss: 0.03995230793952942\n",
      "Epoch 13283/30000 Training Loss: 0.04093480855226517\n",
      "Epoch 13284/30000 Training Loss: 0.043761033564805984\n",
      "Epoch 13285/30000 Training Loss: 0.04294993728399277\n",
      "Epoch 13286/30000 Training Loss: 0.055155523121356964\n",
      "Epoch 13287/30000 Training Loss: 0.0401131846010685\n",
      "Epoch 13288/30000 Training Loss: 0.05290598422288895\n",
      "Epoch 13289/30000 Training Loss: 0.03887609764933586\n",
      "Epoch 13290/30000 Training Loss: 0.037216734141111374\n",
      "Epoch 13291/30000 Training Loss: 0.04583223909139633\n",
      "Epoch 13292/30000 Training Loss: 0.03348154574632645\n",
      "Epoch 13293/30000 Training Loss: 0.051052965223789215\n",
      "Epoch 13294/30000 Training Loss: 0.05142579227685928\n",
      "Epoch 13295/30000 Training Loss: 0.05217931047081947\n",
      "Epoch 13296/30000 Training Loss: 0.043969638645648956\n",
      "Epoch 13297/30000 Training Loss: 0.040106453001499176\n",
      "Epoch 13298/30000 Training Loss: 0.045173805207014084\n",
      "Epoch 13299/30000 Training Loss: 0.04932185262441635\n",
      "Epoch 13300/30000 Training Loss: 0.049734458327293396\n",
      "Epoch 13300/30000 Validation Loss: 0.03922338783740997\n",
      "Epoch 13301/30000 Training Loss: 0.043029963970184326\n",
      "Epoch 13302/30000 Training Loss: 0.047571416944265366\n",
      "Epoch 13303/30000 Training Loss: 0.052609480917453766\n",
      "Epoch 13304/30000 Training Loss: 0.04770129546523094\n",
      "Epoch 13305/30000 Training Loss: 0.057819582521915436\n",
      "Epoch 13306/30000 Training Loss: 0.0408552885055542\n",
      "Epoch 13307/30000 Training Loss: 0.05120446905493736\n",
      "Epoch 13308/30000 Training Loss: 0.04312508553266525\n",
      "Epoch 13309/30000 Training Loss: 0.05641326680779457\n",
      "Epoch 13310/30000 Training Loss: 0.0377148762345314\n",
      "Epoch 13311/30000 Training Loss: 0.04830241948366165\n",
      "Epoch 13312/30000 Training Loss: 0.04576774686574936\n",
      "Epoch 13313/30000 Training Loss: 0.046188171952962875\n",
      "Epoch 13314/30000 Training Loss: 0.03731681779026985\n",
      "Epoch 13315/30000 Training Loss: 0.05770585685968399\n",
      "Epoch 13316/30000 Training Loss: 0.04983238875865936\n",
      "Epoch 13317/30000 Training Loss: 0.0459492951631546\n",
      "Epoch 13318/30000 Training Loss: 0.05107764154672623\n",
      "Epoch 13319/30000 Training Loss: 0.04244127497076988\n",
      "Epoch 13320/30000 Training Loss: 0.056124284863471985\n",
      "Epoch 13321/30000 Training Loss: 0.034234412014484406\n",
      "Epoch 13322/30000 Training Loss: 0.04285171627998352\n",
      "Epoch 13323/30000 Training Loss: 0.04028705134987831\n",
      "Epoch 13324/30000 Training Loss: 0.048754021525382996\n",
      "Epoch 13325/30000 Training Loss: 0.04852787032723427\n",
      "Epoch 13326/30000 Training Loss: 0.0511728897690773\n",
      "Epoch 13327/30000 Training Loss: 0.05162062123417854\n",
      "Epoch 13328/30000 Training Loss: 0.05500439926981926\n",
      "Epoch 13329/30000 Training Loss: 0.044527918100357056\n",
      "Epoch 13330/30000 Training Loss: 0.04297562688589096\n",
      "Epoch 13331/30000 Training Loss: 0.03061588481068611\n",
      "Epoch 13332/30000 Training Loss: 0.04980454221367836\n",
      "Epoch 13333/30000 Training Loss: 0.05036339536309242\n",
      "Epoch 13334/30000 Training Loss: 0.043130259960889816\n",
      "Epoch 13335/30000 Training Loss: 0.04385107755661011\n",
      "Epoch 13336/30000 Training Loss: 0.04013918340206146\n",
      "Epoch 13337/30000 Training Loss: 0.03125134855508804\n",
      "Epoch 13338/30000 Training Loss: 0.04442472383379936\n",
      "Epoch 13339/30000 Training Loss: 0.045192018151283264\n",
      "Epoch 13340/30000 Training Loss: 0.045387350022792816\n",
      "Epoch 13341/30000 Training Loss: 0.05712580680847168\n",
      "Epoch 13342/30000 Training Loss: 0.060672126710414886\n",
      "Epoch 13343/30000 Training Loss: 0.04028081148862839\n",
      "Epoch 13344/30000 Training Loss: 0.05389498174190521\n",
      "Epoch 13345/30000 Training Loss: 0.039750419557094574\n",
      "Epoch 13346/30000 Training Loss: 0.0387936495244503\n",
      "Epoch 13347/30000 Training Loss: 0.057332903146743774\n",
      "Epoch 13348/30000 Training Loss: 0.0438825786113739\n",
      "Epoch 13349/30000 Training Loss: 0.04720334708690643\n",
      "Epoch 13350/30000 Training Loss: 0.037828125059604645\n",
      "Epoch 13351/30000 Training Loss: 0.05289458483457565\n",
      "Epoch 13352/30000 Training Loss: 0.039537906646728516\n",
      "Epoch 13353/30000 Training Loss: 0.03625915199518204\n",
      "Epoch 13354/30000 Training Loss: 0.057957082986831665\n",
      "Epoch 13355/30000 Training Loss: 0.04327050596475601\n",
      "Epoch 13356/30000 Training Loss: 0.040165796875953674\n",
      "Epoch 13357/30000 Training Loss: 0.04530338943004608\n",
      "Epoch 13358/30000 Training Loss: 0.034839216619729996\n",
      "Epoch 13359/30000 Training Loss: 0.03951210528612137\n",
      "Epoch 13360/30000 Training Loss: 0.04284324496984482\n",
      "Epoch 13361/30000 Training Loss: 0.04560448229312897\n",
      "Epoch 13362/30000 Training Loss: 0.04618888348340988\n",
      "Epoch 13363/30000 Training Loss: 0.040733322501182556\n",
      "Epoch 13364/30000 Training Loss: 0.058653101325035095\n",
      "Epoch 13365/30000 Training Loss: 0.045818738639354706\n",
      "Epoch 13366/30000 Training Loss: 0.03435193747282028\n",
      "Epoch 13367/30000 Training Loss: 0.052113741636276245\n",
      "Epoch 13368/30000 Training Loss: 0.045909203588962555\n",
      "Epoch 13369/30000 Training Loss: 0.039474643766880035\n",
      "Epoch 13370/30000 Training Loss: 0.06277988851070404\n",
      "Epoch 13371/30000 Training Loss: 0.06604358553886414\n",
      "Epoch 13372/30000 Training Loss: 0.0501411110162735\n",
      "Epoch 13373/30000 Training Loss: 0.06285063177347183\n",
      "Epoch 13374/30000 Training Loss: 0.049679942429065704\n",
      "Epoch 13375/30000 Training Loss: 0.04033125936985016\n",
      "Epoch 13376/30000 Training Loss: 0.053997952491045\n",
      "Epoch 13377/30000 Training Loss: 0.04756271839141846\n",
      "Epoch 13378/30000 Training Loss: 0.04749693349003792\n",
      "Epoch 13379/30000 Training Loss: 0.054932206869125366\n",
      "Epoch 13380/30000 Training Loss: 0.05666270852088928\n",
      "Epoch 13381/30000 Training Loss: 0.05284027010202408\n",
      "Epoch 13382/30000 Training Loss: 0.038535937666893005\n",
      "Epoch 13383/30000 Training Loss: 0.04165710508823395\n",
      "Epoch 13384/30000 Training Loss: 0.047977060079574585\n",
      "Epoch 13385/30000 Training Loss: 0.052608922123909\n",
      "Epoch 13386/30000 Training Loss: 0.05156465992331505\n",
      "Epoch 13387/30000 Training Loss: 0.048365458846092224\n",
      "Epoch 13388/30000 Training Loss: 0.041767895221710205\n",
      "Epoch 13389/30000 Training Loss: 0.046781573444604874\n",
      "Epoch 13390/30000 Training Loss: 0.04238670691847801\n",
      "Epoch 13391/30000 Training Loss: 0.0629422664642334\n",
      "Epoch 13392/30000 Training Loss: 0.044224053621292114\n",
      "Epoch 13393/30000 Training Loss: 0.04257066920399666\n",
      "Epoch 13394/30000 Training Loss: 0.04137943312525749\n",
      "Epoch 13395/30000 Training Loss: 0.0362812876701355\n",
      "Epoch 13396/30000 Training Loss: 0.04282183200120926\n",
      "Epoch 13397/30000 Training Loss: 0.05036783963441849\n",
      "Epoch 13398/30000 Training Loss: 0.04797012358903885\n",
      "Epoch 13399/30000 Training Loss: 0.04933875799179077\n",
      "Epoch 13400/30000 Training Loss: 0.04861578345298767\n",
      "Epoch 13400/30000 Validation Loss: 0.06980293989181519\n",
      "Epoch 13401/30000 Training Loss: 0.04881751164793968\n",
      "Epoch 13402/30000 Training Loss: 0.06117112934589386\n",
      "Epoch 13403/30000 Training Loss: 0.04511139169335365\n",
      "Epoch 13404/30000 Training Loss: 0.03514360263943672\n",
      "Epoch 13405/30000 Training Loss: 0.05245285481214523\n",
      "Epoch 13406/30000 Training Loss: 0.050406500697135925\n",
      "Epoch 13407/30000 Training Loss: 0.04189660772681236\n",
      "Epoch 13408/30000 Training Loss: 0.05111119896173477\n",
      "Epoch 13409/30000 Training Loss: 0.03915819525718689\n",
      "Epoch 13410/30000 Training Loss: 0.04712138697504997\n",
      "Epoch 13411/30000 Training Loss: 0.05876125395298004\n",
      "Epoch 13412/30000 Training Loss: 0.0419541597366333\n",
      "Epoch 13413/30000 Training Loss: 0.06119291111826897\n",
      "Epoch 13414/30000 Training Loss: 0.05715759098529816\n",
      "Epoch 13415/30000 Training Loss: 0.04670315235853195\n",
      "Epoch 13416/30000 Training Loss: 0.04038013517856598\n",
      "Epoch 13417/30000 Training Loss: 0.0536748543381691\n",
      "Epoch 13418/30000 Training Loss: 0.04466748982667923\n",
      "Epoch 13419/30000 Training Loss: 0.0435483455657959\n",
      "Epoch 13420/30000 Training Loss: 0.045561812818050385\n",
      "Epoch 13421/30000 Training Loss: 0.04374442249536514\n",
      "Epoch 13422/30000 Training Loss: 0.047943465411663055\n",
      "Epoch 13423/30000 Training Loss: 0.03575647994875908\n",
      "Epoch 13424/30000 Training Loss: 0.045444052666425705\n",
      "Epoch 13425/30000 Training Loss: 0.05634265020489693\n",
      "Epoch 13426/30000 Training Loss: 0.05647920072078705\n",
      "Epoch 13427/30000 Training Loss: 0.044338762760162354\n",
      "Epoch 13428/30000 Training Loss: 0.049452044069767\n",
      "Epoch 13429/30000 Training Loss: 0.03942010551691055\n",
      "Epoch 13430/30000 Training Loss: 0.03251408040523529\n",
      "Epoch 13431/30000 Training Loss: 0.046852149069309235\n",
      "Epoch 13432/30000 Training Loss: 0.0464913584291935\n",
      "Epoch 13433/30000 Training Loss: 0.042940277606248856\n",
      "Epoch 13434/30000 Training Loss: 0.04797283187508583\n",
      "Epoch 13435/30000 Training Loss: 0.05264704301953316\n",
      "Epoch 13436/30000 Training Loss: 0.042182423174381256\n",
      "Epoch 13437/30000 Training Loss: 0.039225898683071136\n",
      "Epoch 13438/30000 Training Loss: 0.04811055213212967\n",
      "Epoch 13439/30000 Training Loss: 0.06132199242711067\n",
      "Epoch 13440/30000 Training Loss: 0.05568186938762665\n",
      "Epoch 13441/30000 Training Loss: 0.033735841512680054\n",
      "Epoch 13442/30000 Training Loss: 0.04776957631111145\n",
      "Epoch 13443/30000 Training Loss: 0.05078747868537903\n",
      "Epoch 13444/30000 Training Loss: 0.05739965662360191\n",
      "Epoch 13445/30000 Training Loss: 0.05374545603990555\n",
      "Epoch 13446/30000 Training Loss: 0.051447294652462006\n",
      "Epoch 13447/30000 Training Loss: 0.05547415837645531\n",
      "Epoch 13448/30000 Training Loss: 0.0467127300798893\n",
      "Epoch 13449/30000 Training Loss: 0.04731076955795288\n",
      "Epoch 13450/30000 Training Loss: 0.053631335496902466\n",
      "Epoch 13451/30000 Training Loss: 0.04420682042837143\n",
      "Epoch 13452/30000 Training Loss: 0.03851060941815376\n",
      "Epoch 13453/30000 Training Loss: 0.03264404460787773\n",
      "Epoch 13454/30000 Training Loss: 0.06121039390563965\n",
      "Epoch 13455/30000 Training Loss: 0.03820839151740074\n",
      "Epoch 13456/30000 Training Loss: 0.06497830152511597\n",
      "Epoch 13457/30000 Training Loss: 0.053771115839481354\n",
      "Epoch 13458/30000 Training Loss: 0.037842318415641785\n",
      "Epoch 13459/30000 Training Loss: 0.04214823246002197\n",
      "Epoch 13460/30000 Training Loss: 0.0412558987736702\n",
      "Epoch 13461/30000 Training Loss: 0.0489986315369606\n",
      "Epoch 13462/30000 Training Loss: 0.04872330278158188\n",
      "Epoch 13463/30000 Training Loss: 0.048968374729156494\n",
      "Epoch 13464/30000 Training Loss: 0.06024641543626785\n",
      "Epoch 13465/30000 Training Loss: 0.03844330459833145\n",
      "Epoch 13466/30000 Training Loss: 0.04299233853816986\n",
      "Epoch 13467/30000 Training Loss: 0.0629584863781929\n",
      "Epoch 13468/30000 Training Loss: 0.04767823964357376\n",
      "Epoch 13469/30000 Training Loss: 0.054949164390563965\n",
      "Epoch 13470/30000 Training Loss: 0.04729931056499481\n",
      "Epoch 13471/30000 Training Loss: 0.0427321195602417\n",
      "Epoch 13472/30000 Training Loss: 0.05604717880487442\n",
      "Epoch 13473/30000 Training Loss: 0.05149738863110542\n",
      "Epoch 13474/30000 Training Loss: 0.06078541278839111\n",
      "Epoch 13475/30000 Training Loss: 0.04920986294746399\n",
      "Epoch 13476/30000 Training Loss: 0.043166324496269226\n",
      "Epoch 13477/30000 Training Loss: 0.04984248802065849\n",
      "Epoch 13478/30000 Training Loss: 0.03817375749349594\n",
      "Epoch 13479/30000 Training Loss: 0.05477702617645264\n",
      "Epoch 13480/30000 Training Loss: 0.04032124578952789\n",
      "Epoch 13481/30000 Training Loss: 0.042053401470184326\n",
      "Epoch 13482/30000 Training Loss: 0.042874425649642944\n",
      "Epoch 13483/30000 Training Loss: 0.05127395689487457\n",
      "Epoch 13484/30000 Training Loss: 0.042221374809741974\n",
      "Epoch 13485/30000 Training Loss: 0.041721053421497345\n",
      "Epoch 13486/30000 Training Loss: 0.03967776522040367\n",
      "Epoch 13487/30000 Training Loss: 0.035929180681705475\n",
      "Epoch 13488/30000 Training Loss: 0.0482991561293602\n",
      "Epoch 13489/30000 Training Loss: 0.0400480255484581\n",
      "Epoch 13490/30000 Training Loss: 0.05134821683168411\n",
      "Epoch 13491/30000 Training Loss: 0.04893682897090912\n",
      "Epoch 13492/30000 Training Loss: 0.03590768203139305\n",
      "Epoch 13493/30000 Training Loss: 0.052304431796073914\n",
      "Epoch 13494/30000 Training Loss: 0.042637355625629425\n",
      "Epoch 13495/30000 Training Loss: 0.044127486646175385\n",
      "Epoch 13496/30000 Training Loss: 0.0588485524058342\n",
      "Epoch 13497/30000 Training Loss: 0.05633110553026199\n",
      "Epoch 13498/30000 Training Loss: 0.05095016956329346\n",
      "Epoch 13499/30000 Training Loss: 0.05926793813705444\n",
      "Epoch 13500/30000 Training Loss: 0.04449378326535225\n",
      "Epoch 13500/30000 Validation Loss: 0.05111928656697273\n",
      "Epoch 13501/30000 Training Loss: 0.04224420338869095\n",
      "Epoch 13502/30000 Training Loss: 0.04945671558380127\n",
      "Epoch 13503/30000 Training Loss: 0.03753996640443802\n",
      "Epoch 13504/30000 Training Loss: 0.05032292380928993\n",
      "Epoch 13505/30000 Training Loss: 0.03940483182668686\n",
      "Epoch 13506/30000 Training Loss: 0.04376758262515068\n",
      "Epoch 13507/30000 Training Loss: 0.05436411499977112\n",
      "Epoch 13508/30000 Training Loss: 0.047498710453510284\n",
      "Epoch 13509/30000 Training Loss: 0.04561111703515053\n",
      "Epoch 13510/30000 Training Loss: 0.051126979291439056\n",
      "Epoch 13511/30000 Training Loss: 0.04622143507003784\n",
      "Epoch 13512/30000 Training Loss: 0.06644076853990555\n",
      "Epoch 13513/30000 Training Loss: 0.05188976973295212\n",
      "Epoch 13514/30000 Training Loss: 0.06013675779104233\n",
      "Epoch 13515/30000 Training Loss: 0.05366040766239166\n",
      "Epoch 13516/30000 Training Loss: 0.03816123679280281\n",
      "Epoch 13517/30000 Training Loss: 0.059725821018218994\n",
      "Epoch 13518/30000 Training Loss: 0.046867333352565765\n",
      "Epoch 13519/30000 Training Loss: 0.0569864958524704\n",
      "Epoch 13520/30000 Training Loss: 0.05181669443845749\n",
      "Epoch 13521/30000 Training Loss: 0.04227043315768242\n",
      "Epoch 13522/30000 Training Loss: 0.031297408044338226\n",
      "Epoch 13523/30000 Training Loss: 0.04297558218240738\n",
      "Epoch 13524/30000 Training Loss: 0.03978676348924637\n",
      "Epoch 13525/30000 Training Loss: 0.04269837215542793\n",
      "Epoch 13526/30000 Training Loss: 0.048005759716033936\n",
      "Epoch 13527/30000 Training Loss: 0.03235042840242386\n",
      "Epoch 13528/30000 Training Loss: 0.03506433218717575\n",
      "Epoch 13529/30000 Training Loss: 0.0591057613492012\n",
      "Epoch 13530/30000 Training Loss: 0.049159251153469086\n",
      "Epoch 13531/30000 Training Loss: 0.06110795959830284\n",
      "Epoch 13532/30000 Training Loss: 0.04421877861022949\n",
      "Epoch 13533/30000 Training Loss: 0.05524492263793945\n",
      "Epoch 13534/30000 Training Loss: 0.05271809548139572\n",
      "Epoch 13535/30000 Training Loss: 0.04987505078315735\n",
      "Epoch 13536/30000 Training Loss: 0.04594309255480766\n",
      "Epoch 13537/30000 Training Loss: 0.05334131419658661\n",
      "Epoch 13538/30000 Training Loss: 0.04396158456802368\n",
      "Epoch 13539/30000 Training Loss: 0.05084054917097092\n",
      "Epoch 13540/30000 Training Loss: 0.047584764659404755\n",
      "Epoch 13541/30000 Training Loss: 0.038472265005111694\n",
      "Epoch 13542/30000 Training Loss: 0.036288075149059296\n",
      "Epoch 13543/30000 Training Loss: 0.04724486917257309\n",
      "Epoch 13544/30000 Training Loss: 0.038489844650030136\n",
      "Epoch 13545/30000 Training Loss: 0.051981158554553986\n",
      "Epoch 13546/30000 Training Loss: 0.06161490082740784\n",
      "Epoch 13547/30000 Training Loss: 0.04178876802325249\n",
      "Epoch 13548/30000 Training Loss: 0.03822723776102066\n",
      "Epoch 13549/30000 Training Loss: 0.035010673105716705\n",
      "Epoch 13550/30000 Training Loss: 0.04641033336520195\n",
      "Epoch 13551/30000 Training Loss: 0.06241660192608833\n",
      "Epoch 13552/30000 Training Loss: 0.05084965378046036\n",
      "Epoch 13553/30000 Training Loss: 0.053599871695041656\n",
      "Epoch 13554/30000 Training Loss: 0.06557789444923401\n",
      "Epoch 13555/30000 Training Loss: 0.05731131136417389\n",
      "Epoch 13556/30000 Training Loss: 0.03846703842282295\n",
      "Epoch 13557/30000 Training Loss: 0.05067605525255203\n",
      "Epoch 13558/30000 Training Loss: 0.05945087969303131\n",
      "Epoch 13559/30000 Training Loss: 0.04297691583633423\n",
      "Epoch 13560/30000 Training Loss: 0.04233816638588905\n",
      "Epoch 13561/30000 Training Loss: 0.04858418554067612\n",
      "Epoch 13562/30000 Training Loss: 0.044652119278907776\n",
      "Epoch 13563/30000 Training Loss: 0.05021044239401817\n",
      "Epoch 13564/30000 Training Loss: 0.0370735302567482\n",
      "Epoch 13565/30000 Training Loss: 0.05618976801633835\n",
      "Epoch 13566/30000 Training Loss: 0.04733741283416748\n",
      "Epoch 13567/30000 Training Loss: 0.027949325740337372\n",
      "Epoch 13568/30000 Training Loss: 0.04027154669165611\n",
      "Epoch 13569/30000 Training Loss: 0.03510195389389992\n",
      "Epoch 13570/30000 Training Loss: 0.042659856379032135\n",
      "Epoch 13571/30000 Training Loss: 0.044702813029289246\n",
      "Epoch 13572/30000 Training Loss: 0.06422342360019684\n",
      "Epoch 13573/30000 Training Loss: 0.05821358412504196\n",
      "Epoch 13574/30000 Training Loss: 0.04494333639740944\n",
      "Epoch 13575/30000 Training Loss: 0.0404396615922451\n",
      "Epoch 13576/30000 Training Loss: 0.04786074161529541\n",
      "Epoch 13577/30000 Training Loss: 0.04647080972790718\n",
      "Epoch 13578/30000 Training Loss: 0.06436821818351746\n",
      "Epoch 13579/30000 Training Loss: 0.06702280044555664\n",
      "Epoch 13580/30000 Training Loss: 0.05863116681575775\n",
      "Epoch 13581/30000 Training Loss: 0.04062725976109505\n",
      "Epoch 13582/30000 Training Loss: 0.05588699132204056\n",
      "Epoch 13583/30000 Training Loss: 0.058954693377017975\n",
      "Epoch 13584/30000 Training Loss: 0.05543307960033417\n",
      "Epoch 13585/30000 Training Loss: 0.06302151829004288\n",
      "Epoch 13586/30000 Training Loss: 0.02655941992998123\n",
      "Epoch 13587/30000 Training Loss: 0.03942061588168144\n",
      "Epoch 13588/30000 Training Loss: 0.05104094743728638\n",
      "Epoch 13589/30000 Training Loss: 0.051800936460494995\n",
      "Epoch 13590/30000 Training Loss: 0.05287088453769684\n",
      "Epoch 13591/30000 Training Loss: 0.028140859678387642\n",
      "Epoch 13592/30000 Training Loss: 0.04162074625492096\n",
      "Epoch 13593/30000 Training Loss: 0.04396028816699982\n",
      "Epoch 13594/30000 Training Loss: 0.05152464658021927\n",
      "Epoch 13595/30000 Training Loss: 0.04423481971025467\n",
      "Epoch 13596/30000 Training Loss: 0.04974968358874321\n",
      "Epoch 13597/30000 Training Loss: 0.048121821135282516\n",
      "Epoch 13598/30000 Training Loss: 0.04025357961654663\n",
      "Epoch 13599/30000 Training Loss: 0.04733031243085861\n",
      "Epoch 13600/30000 Training Loss: 0.054058678448200226\n",
      "Epoch 13600/30000 Validation Loss: 0.05636436492204666\n",
      "Epoch 13601/30000 Training Loss: 0.056016720831394196\n",
      "Epoch 13602/30000 Training Loss: 0.040834859013557434\n",
      "Epoch 13603/30000 Training Loss: 0.05427068471908569\n",
      "Epoch 13604/30000 Training Loss: 0.03791210055351257\n",
      "Epoch 13605/30000 Training Loss: 0.04291572421789169\n",
      "Epoch 13606/30000 Training Loss: 0.05394992232322693\n",
      "Epoch 13607/30000 Training Loss: 0.05054759606719017\n",
      "Epoch 13608/30000 Training Loss: 0.04262441769242287\n",
      "Epoch 13609/30000 Training Loss: 0.052349843084812164\n",
      "Epoch 13610/30000 Training Loss: 0.04837591201066971\n",
      "Epoch 13611/30000 Training Loss: 0.05551588907837868\n",
      "Epoch 13612/30000 Training Loss: 0.04399702697992325\n",
      "Epoch 13613/30000 Training Loss: 0.03847897797822952\n",
      "Epoch 13614/30000 Training Loss: 0.0516774021089077\n",
      "Epoch 13615/30000 Training Loss: 0.04625524580478668\n",
      "Epoch 13616/30000 Training Loss: 0.050233595073223114\n",
      "Epoch 13617/30000 Training Loss: 0.049528881907463074\n",
      "Epoch 13618/30000 Training Loss: 0.05153147876262665\n",
      "Epoch 13619/30000 Training Loss: 0.04398193210363388\n",
      "Epoch 13620/30000 Training Loss: 0.05235341191291809\n",
      "Epoch 13621/30000 Training Loss: 0.053054098039865494\n",
      "Epoch 13622/30000 Training Loss: 0.042585812509059906\n",
      "Epoch 13623/30000 Training Loss: 0.04067760705947876\n",
      "Epoch 13624/30000 Training Loss: 0.037576187402009964\n",
      "Epoch 13625/30000 Training Loss: 0.061771973967552185\n",
      "Epoch 13626/30000 Training Loss: 0.035363778471946716\n",
      "Epoch 13627/30000 Training Loss: 0.04740147665143013\n",
      "Epoch 13628/30000 Training Loss: 0.04072973504662514\n",
      "Epoch 13629/30000 Training Loss: 0.042132675647735596\n",
      "Epoch 13630/30000 Training Loss: 0.05286020040512085\n",
      "Epoch 13631/30000 Training Loss: 0.045536816120147705\n",
      "Epoch 13632/30000 Training Loss: 0.04022727161645889\n",
      "Epoch 13633/30000 Training Loss: 0.02874426543712616\n",
      "Epoch 13634/30000 Training Loss: 0.04691861942410469\n",
      "Epoch 13635/30000 Training Loss: 0.04161300137639046\n",
      "Epoch 13636/30000 Training Loss: 0.037136733531951904\n",
      "Epoch 13637/30000 Training Loss: 0.04589573293924332\n",
      "Epoch 13638/30000 Training Loss: 0.03199492394924164\n",
      "Epoch 13639/30000 Training Loss: 0.059103600680828094\n",
      "Epoch 13640/30000 Training Loss: 0.059708625078201294\n",
      "Epoch 13641/30000 Training Loss: 0.047503512352705\n",
      "Epoch 13642/30000 Training Loss: 0.05002089962363243\n",
      "Epoch 13643/30000 Training Loss: 0.0444970577955246\n",
      "Epoch 13644/30000 Training Loss: 0.046228133141994476\n",
      "Epoch 13645/30000 Training Loss: 0.037779033184051514\n",
      "Epoch 13646/30000 Training Loss: 0.05836069583892822\n",
      "Epoch 13647/30000 Training Loss: 0.06229398399591446\n",
      "Epoch 13648/30000 Training Loss: 0.045646391808986664\n",
      "Epoch 13649/30000 Training Loss: 0.04680121690034866\n",
      "Epoch 13650/30000 Training Loss: 0.05516532436013222\n",
      "Epoch 13651/30000 Training Loss: 0.031087521463632584\n",
      "Epoch 13652/30000 Training Loss: 0.04922185838222504\n",
      "Epoch 13653/30000 Training Loss: 0.045097798109054565\n",
      "Epoch 13654/30000 Training Loss: 0.05862593650817871\n",
      "Epoch 13655/30000 Training Loss: 0.04421120136976242\n",
      "Epoch 13656/30000 Training Loss: 0.06359072029590607\n",
      "Epoch 13657/30000 Training Loss: 0.04793231189250946\n",
      "Epoch 13658/30000 Training Loss: 0.038758911192417145\n",
      "Epoch 13659/30000 Training Loss: 0.05123264342546463\n",
      "Epoch 13660/30000 Training Loss: 0.03567787632346153\n",
      "Epoch 13661/30000 Training Loss: 0.05609951913356781\n",
      "Epoch 13662/30000 Training Loss: 0.032277911901474\n",
      "Epoch 13663/30000 Training Loss: 0.04827897250652313\n",
      "Epoch 13664/30000 Training Loss: 0.05978013575077057\n",
      "Epoch 13665/30000 Training Loss: 0.049467191100120544\n",
      "Epoch 13666/30000 Training Loss: 0.03570744767785072\n",
      "Epoch 13667/30000 Training Loss: 0.04849337413907051\n",
      "Epoch 13668/30000 Training Loss: 0.059408560395240784\n",
      "Epoch 13669/30000 Training Loss: 0.04286528006196022\n",
      "Epoch 13670/30000 Training Loss: 0.05022697150707245\n",
      "Epoch 13671/30000 Training Loss: 0.03555910289287567\n",
      "Epoch 13672/30000 Training Loss: 0.04144344478845596\n",
      "Epoch 13673/30000 Training Loss: 0.05700262263417244\n",
      "Epoch 13674/30000 Training Loss: 0.04856923967599869\n",
      "Epoch 13675/30000 Training Loss: 0.03599374741315842\n",
      "Epoch 13676/30000 Training Loss: 0.04470590502023697\n",
      "Epoch 13677/30000 Training Loss: 0.048772819340229034\n",
      "Epoch 13678/30000 Training Loss: 0.038317907601594925\n",
      "Epoch 13679/30000 Training Loss: 0.04805946350097656\n",
      "Epoch 13680/30000 Training Loss: 0.04793151468038559\n",
      "Epoch 13681/30000 Training Loss: 0.05010620504617691\n",
      "Epoch 13682/30000 Training Loss: 0.04883551597595215\n",
      "Epoch 13683/30000 Training Loss: 0.04746773838996887\n",
      "Epoch 13684/30000 Training Loss: 0.03828691691160202\n",
      "Epoch 13685/30000 Training Loss: 0.05531119555234909\n",
      "Epoch 13686/30000 Training Loss: 0.05214691907167435\n",
      "Epoch 13687/30000 Training Loss: 0.028663665056228638\n",
      "Epoch 13688/30000 Training Loss: 0.05322611331939697\n",
      "Epoch 13689/30000 Training Loss: 0.04567486047744751\n",
      "Epoch 13690/30000 Training Loss: 0.045270904898643494\n",
      "Epoch 13691/30000 Training Loss: 0.04616069793701172\n",
      "Epoch 13692/30000 Training Loss: 0.05388811230659485\n",
      "Epoch 13693/30000 Training Loss: 0.0612361803650856\n",
      "Epoch 13694/30000 Training Loss: 0.05990193784236908\n",
      "Epoch 13695/30000 Training Loss: 0.05036409944295883\n",
      "Epoch 13696/30000 Training Loss: 0.0507674440741539\n",
      "Epoch 13697/30000 Training Loss: 0.04824414104223251\n",
      "Epoch 13698/30000 Training Loss: 0.0379808135330677\n",
      "Epoch 13699/30000 Training Loss: 0.034168846905231476\n",
      "Epoch 13700/30000 Training Loss: 0.033189557492733\n",
      "Epoch 13700/30000 Validation Loss: 0.048702456057071686\n",
      "Epoch 13701/30000 Training Loss: 0.05385708063840866\n",
      "Epoch 13702/30000 Training Loss: 0.047191474586725235\n",
      "Epoch 13703/30000 Training Loss: 0.03866230323910713\n",
      "Epoch 13704/30000 Training Loss: 0.05154948681592941\n",
      "Epoch 13705/30000 Training Loss: 0.0663524866104126\n",
      "Epoch 13706/30000 Training Loss: 0.050209157168865204\n",
      "Epoch 13707/30000 Training Loss: 0.049559030681848526\n",
      "Epoch 13708/30000 Training Loss: 0.03277914226055145\n",
      "Epoch 13709/30000 Training Loss: 0.04969499260187149\n",
      "Epoch 13710/30000 Training Loss: 0.03347235545516014\n",
      "Epoch 13711/30000 Training Loss: 0.039673320949077606\n",
      "Epoch 13712/30000 Training Loss: 0.0445689931511879\n",
      "Epoch 13713/30000 Training Loss: 0.04811486974358559\n",
      "Epoch 13714/30000 Training Loss: 0.0550072118639946\n",
      "Epoch 13715/30000 Training Loss: 0.04104694724082947\n",
      "Epoch 13716/30000 Training Loss: 0.06482255458831787\n",
      "Epoch 13717/30000 Training Loss: 0.05164042115211487\n",
      "Epoch 13718/30000 Training Loss: 0.04992300271987915\n",
      "Epoch 13719/30000 Training Loss: 0.04997939243912697\n",
      "Epoch 13720/30000 Training Loss: 0.03982488065958023\n",
      "Epoch 13721/30000 Training Loss: 0.05553895980119705\n",
      "Epoch 13722/30000 Training Loss: 0.03710543364286423\n",
      "Epoch 13723/30000 Training Loss: 0.06500094383955002\n",
      "Epoch 13724/30000 Training Loss: 0.034700192511081696\n",
      "Epoch 13725/30000 Training Loss: 0.038536183536052704\n",
      "Epoch 13726/30000 Training Loss: 0.034654878079891205\n",
      "Epoch 13727/30000 Training Loss: 0.04371645301580429\n",
      "Epoch 13728/30000 Training Loss: 0.046131521463394165\n",
      "Epoch 13729/30000 Training Loss: 0.047568175941705704\n",
      "Epoch 13730/30000 Training Loss: 0.04659692943096161\n",
      "Epoch 13731/30000 Training Loss: 0.043258681893348694\n",
      "Epoch 13732/30000 Training Loss: 0.043513450771570206\n",
      "Epoch 13733/30000 Training Loss: 0.043959904462099075\n",
      "Epoch 13734/30000 Training Loss: 0.039858628064394\n",
      "Epoch 13735/30000 Training Loss: 0.053361743688583374\n",
      "Epoch 13736/30000 Training Loss: 0.043327994644641876\n",
      "Epoch 13737/30000 Training Loss: 0.04157916083931923\n",
      "Epoch 13738/30000 Training Loss: 0.04120688885450363\n",
      "Epoch 13739/30000 Training Loss: 0.040419504046440125\n",
      "Epoch 13740/30000 Training Loss: 0.056427404284477234\n",
      "Epoch 13741/30000 Training Loss: 0.03629792854189873\n",
      "Epoch 13742/30000 Training Loss: 0.04023493081331253\n",
      "Epoch 13743/30000 Training Loss: 0.04718654230237007\n",
      "Epoch 13744/30000 Training Loss: 0.0347367562353611\n",
      "Epoch 13745/30000 Training Loss: 0.05348841845989227\n",
      "Epoch 13746/30000 Training Loss: 0.05015532672405243\n",
      "Epoch 13747/30000 Training Loss: 0.037513986229896545\n",
      "Epoch 13748/30000 Training Loss: 0.059062302112579346\n",
      "Epoch 13749/30000 Training Loss: 0.04935251548886299\n",
      "Epoch 13750/30000 Training Loss: 0.043316323310136795\n",
      "Epoch 13751/30000 Training Loss: 0.047106169164180756\n",
      "Epoch 13752/30000 Training Loss: 0.03676623851060867\n",
      "Epoch 13753/30000 Training Loss: 0.039451032876968384\n",
      "Epoch 13754/30000 Training Loss: 0.0519198477268219\n",
      "Epoch 13755/30000 Training Loss: 0.047164738178253174\n",
      "Epoch 13756/30000 Training Loss: 0.05290347337722778\n",
      "Epoch 13757/30000 Training Loss: 0.04114825278520584\n",
      "Epoch 13758/30000 Training Loss: 0.05060141533613205\n",
      "Epoch 13759/30000 Training Loss: 0.04354456067085266\n",
      "Epoch 13760/30000 Training Loss: 0.04707507789134979\n",
      "Epoch 13761/30000 Training Loss: 0.02979099377989769\n",
      "Epoch 13762/30000 Training Loss: 0.051736608147621155\n",
      "Epoch 13763/30000 Training Loss: 0.04349059611558914\n",
      "Epoch 13764/30000 Training Loss: 0.050059814006090164\n",
      "Epoch 13765/30000 Training Loss: 0.05676426738500595\n",
      "Epoch 13766/30000 Training Loss: 0.046820808202028275\n",
      "Epoch 13767/30000 Training Loss: 0.049165427684783936\n",
      "Epoch 13768/30000 Training Loss: 0.05241333693265915\n",
      "Epoch 13769/30000 Training Loss: 0.030786503106355667\n",
      "Epoch 13770/30000 Training Loss: 0.047861456871032715\n",
      "Epoch 13771/30000 Training Loss: 0.048898845911026\n",
      "Epoch 13772/30000 Training Loss: 0.04069187864661217\n",
      "Epoch 13773/30000 Training Loss: 0.057235099375247955\n",
      "Epoch 13774/30000 Training Loss: 0.05386291444301605\n",
      "Epoch 13775/30000 Training Loss: 0.04022139310836792\n",
      "Epoch 13776/30000 Training Loss: 0.03386211767792702\n",
      "Epoch 13777/30000 Training Loss: 0.046394772827625275\n",
      "Epoch 13778/30000 Training Loss: 0.03803977370262146\n",
      "Epoch 13779/30000 Training Loss: 0.03654685989022255\n",
      "Epoch 13780/30000 Training Loss: 0.040224626660346985\n",
      "Epoch 13781/30000 Training Loss: 0.0444955974817276\n",
      "Epoch 13782/30000 Training Loss: 0.05579858273267746\n",
      "Epoch 13783/30000 Training Loss: 0.053739581257104874\n",
      "Epoch 13784/30000 Training Loss: 0.03659112751483917\n",
      "Epoch 13785/30000 Training Loss: 0.05711565166711807\n",
      "Epoch 13786/30000 Training Loss: 0.04515403136610985\n",
      "Epoch 13787/30000 Training Loss: 0.061294443905353546\n",
      "Epoch 13788/30000 Training Loss: 0.045785367488861084\n",
      "Epoch 13789/30000 Training Loss: 0.046533551067113876\n",
      "Epoch 13790/30000 Training Loss: 0.037890054285526276\n",
      "Epoch 13791/30000 Training Loss: 0.05348234623670578\n",
      "Epoch 13792/30000 Training Loss: 0.05660361051559448\n",
      "Epoch 13793/30000 Training Loss: 0.039556071162223816\n",
      "Epoch 13794/30000 Training Loss: 0.04740924388170242\n",
      "Epoch 13795/30000 Training Loss: 0.04285998269915581\n",
      "Epoch 13796/30000 Training Loss: 0.03366304188966751\n",
      "Epoch 13797/30000 Training Loss: 0.04455644637346268\n",
      "Epoch 13798/30000 Training Loss: 0.04212268441915512\n",
      "Epoch 13799/30000 Training Loss: 0.058750949800014496\n",
      "Epoch 13800/30000 Training Loss: 0.05886007100343704\n",
      "Epoch 13800/30000 Validation Loss: 0.04838559776544571\n",
      "Epoch 13801/30000 Training Loss: 0.04144454747438431\n",
      "Epoch 13802/30000 Training Loss: 0.05117254704236984\n",
      "Epoch 13803/30000 Training Loss: 0.05620633065700531\n",
      "Epoch 13804/30000 Training Loss: 0.05944922938942909\n",
      "Epoch 13805/30000 Training Loss: 0.05468577891588211\n",
      "Epoch 13806/30000 Training Loss: 0.03560938313603401\n",
      "Epoch 13807/30000 Training Loss: 0.053302355110645294\n",
      "Epoch 13808/30000 Training Loss: 0.041053272783756256\n",
      "Epoch 13809/30000 Training Loss: 0.05660582333803177\n",
      "Epoch 13810/30000 Training Loss: 0.05473042279481888\n",
      "Epoch 13811/30000 Training Loss: 0.043381571769714355\n",
      "Epoch 13812/30000 Training Loss: 0.04678650572896004\n",
      "Epoch 13813/30000 Training Loss: 0.05586446076631546\n",
      "Epoch 13814/30000 Training Loss: 0.05894249677658081\n",
      "Epoch 13815/30000 Training Loss: 0.05387480556964874\n",
      "Epoch 13816/30000 Training Loss: 0.05215725302696228\n",
      "Epoch 13817/30000 Training Loss: 0.06427852809429169\n",
      "Epoch 13818/30000 Training Loss: 0.0650884360074997\n",
      "Epoch 13819/30000 Training Loss: 0.03798263520002365\n",
      "Epoch 13820/30000 Training Loss: 0.047407764941453934\n",
      "Epoch 13821/30000 Training Loss: 0.04533854126930237\n",
      "Epoch 13822/30000 Training Loss: 0.056676626205444336\n",
      "Epoch 13823/30000 Training Loss: 0.036280907690525055\n",
      "Epoch 13824/30000 Training Loss: 0.05353495478630066\n",
      "Epoch 13825/30000 Training Loss: 0.046131834387779236\n",
      "Epoch 13826/30000 Training Loss: 0.04313069209456444\n",
      "Epoch 13827/30000 Training Loss: 0.03967773914337158\n",
      "Epoch 13828/30000 Training Loss: 0.043559182435274124\n",
      "Epoch 13829/30000 Training Loss: 0.033447638154029846\n",
      "Epoch 13830/30000 Training Loss: 0.04055097699165344\n",
      "Epoch 13831/30000 Training Loss: 0.04578326642513275\n",
      "Epoch 13832/30000 Training Loss: 0.036502987146377563\n",
      "Epoch 13833/30000 Training Loss: 0.02987167239189148\n",
      "Epoch 13834/30000 Training Loss: 0.05542927235364914\n",
      "Epoch 13835/30000 Training Loss: 0.03967362642288208\n",
      "Epoch 13836/30000 Training Loss: 0.04163426533341408\n",
      "Epoch 13837/30000 Training Loss: 0.05030661076307297\n",
      "Epoch 13838/30000 Training Loss: 0.0570964589715004\n",
      "Epoch 13839/30000 Training Loss: 0.048303186893463135\n",
      "Epoch 13840/30000 Training Loss: 0.04038003087043762\n",
      "Epoch 13841/30000 Training Loss: 0.03833574429154396\n",
      "Epoch 13842/30000 Training Loss: 0.04298197478055954\n",
      "Epoch 13843/30000 Training Loss: 0.039713215082883835\n",
      "Epoch 13844/30000 Training Loss: 0.038686029613018036\n",
      "Epoch 13845/30000 Training Loss: 0.03839583694934845\n",
      "Epoch 13846/30000 Training Loss: 0.04555666819214821\n",
      "Epoch 13847/30000 Training Loss: 0.041313156485557556\n",
      "Epoch 13848/30000 Training Loss: 0.05146261304616928\n",
      "Epoch 13849/30000 Training Loss: 0.031946614384651184\n",
      "Epoch 13850/30000 Training Loss: 0.053207285702228546\n",
      "Epoch 13851/30000 Training Loss: 0.04543057829141617\n",
      "Epoch 13852/30000 Training Loss: 0.031264714896678925\n",
      "Epoch 13853/30000 Training Loss: 0.05104491859674454\n",
      "Epoch 13854/30000 Training Loss: 0.03935183584690094\n",
      "Epoch 13855/30000 Training Loss: 0.050592854619026184\n",
      "Epoch 13856/30000 Training Loss: 0.04085948318243027\n",
      "Epoch 13857/30000 Training Loss: 0.05480106920003891\n",
      "Epoch 13858/30000 Training Loss: 0.04209766164422035\n",
      "Epoch 13859/30000 Training Loss: 0.05268742889165878\n",
      "Epoch 13860/30000 Training Loss: 0.036874257028102875\n",
      "Epoch 13861/30000 Training Loss: 0.03857767581939697\n",
      "Epoch 13862/30000 Training Loss: 0.03829684481024742\n",
      "Epoch 13863/30000 Training Loss: 0.05526658147573471\n",
      "Epoch 13864/30000 Training Loss: 0.04889591783285141\n",
      "Epoch 13865/30000 Training Loss: 0.0429418608546257\n",
      "Epoch 13866/30000 Training Loss: 0.04711408540606499\n",
      "Epoch 13867/30000 Training Loss: 0.042282864451408386\n",
      "Epoch 13868/30000 Training Loss: 0.06775383651256561\n",
      "Epoch 13869/30000 Training Loss: 0.0531214103102684\n",
      "Epoch 13870/30000 Training Loss: 0.038212139159440994\n",
      "Epoch 13871/30000 Training Loss: 0.05648842826485634\n",
      "Epoch 13872/30000 Training Loss: 0.046514544636011124\n",
      "Epoch 13873/30000 Training Loss: 0.05351092666387558\n",
      "Epoch 13874/30000 Training Loss: 0.05935024470090866\n",
      "Epoch 13875/30000 Training Loss: 0.03906884789466858\n",
      "Epoch 13876/30000 Training Loss: 0.03451181948184967\n",
      "Epoch 13877/30000 Training Loss: 0.042215216904878616\n",
      "Epoch 13878/30000 Training Loss: 0.0377826914191246\n",
      "Epoch 13879/30000 Training Loss: 0.05728385969996452\n",
      "Epoch 13880/30000 Training Loss: 0.04813338816165924\n",
      "Epoch 13881/30000 Training Loss: 0.046735912561416626\n",
      "Epoch 13882/30000 Training Loss: 0.03945841267704964\n",
      "Epoch 13883/30000 Training Loss: 0.05483098700642586\n",
      "Epoch 13884/30000 Training Loss: 0.04583768546581268\n",
      "Epoch 13885/30000 Training Loss: 0.05156078189611435\n",
      "Epoch 13886/30000 Training Loss: 0.06967388838529587\n",
      "Epoch 13887/30000 Training Loss: 0.03759358823299408\n",
      "Epoch 13888/30000 Training Loss: 0.043428100645542145\n",
      "Epoch 13889/30000 Training Loss: 0.04848909378051758\n",
      "Epoch 13890/30000 Training Loss: 0.05787862092256546\n",
      "Epoch 13891/30000 Training Loss: 0.044661328196525574\n",
      "Epoch 13892/30000 Training Loss: 0.05335278809070587\n",
      "Epoch 13893/30000 Training Loss: 0.036308854818344116\n",
      "Epoch 13894/30000 Training Loss: 0.03928259387612343\n",
      "Epoch 13895/30000 Training Loss: 0.054812997579574585\n",
      "Epoch 13896/30000 Training Loss: 0.05023801699280739\n",
      "Epoch 13897/30000 Training Loss: 0.03895699977874756\n",
      "Epoch 13898/30000 Training Loss: 0.05164072662591934\n",
      "Epoch 13899/30000 Training Loss: 0.04358309879899025\n",
      "Epoch 13900/30000 Training Loss: 0.048271141946315765\n",
      "Epoch 13900/30000 Validation Loss: 0.0500638484954834\n",
      "Epoch 13901/30000 Training Loss: 0.06668412685394287\n",
      "Epoch 13902/30000 Training Loss: 0.03375634178519249\n",
      "Epoch 13903/30000 Training Loss: 0.03629985824227333\n",
      "Epoch 13904/30000 Training Loss: 0.029804151505231857\n",
      "Epoch 13905/30000 Training Loss: 0.039436496794223785\n",
      "Epoch 13906/30000 Training Loss: 0.056437671184539795\n",
      "Epoch 13907/30000 Training Loss: 0.0425410233438015\n",
      "Epoch 13908/30000 Training Loss: 0.04603524133563042\n",
      "Epoch 13909/30000 Training Loss: 0.04614979773759842\n",
      "Epoch 13910/30000 Training Loss: 0.03409024700522423\n",
      "Epoch 13911/30000 Training Loss: 0.0426255464553833\n",
      "Epoch 13912/30000 Training Loss: 0.08047110587358475\n",
      "Epoch 13913/30000 Training Loss: 0.06702037900686264\n",
      "Epoch 13914/30000 Training Loss: 0.046696215867996216\n",
      "Epoch 13915/30000 Training Loss: 0.0427967831492424\n",
      "Epoch 13916/30000 Training Loss: 0.043430671095848083\n",
      "Epoch 13917/30000 Training Loss: 0.04507139325141907\n",
      "Epoch 13918/30000 Training Loss: 0.05387955904006958\n",
      "Epoch 13919/30000 Training Loss: 0.044949062168598175\n",
      "Epoch 13920/30000 Training Loss: 0.034242551773786545\n",
      "Epoch 13921/30000 Training Loss: 0.05959456413984299\n",
      "Epoch 13922/30000 Training Loss: 0.041415195912122726\n",
      "Epoch 13923/30000 Training Loss: 0.047230347990989685\n",
      "Epoch 13924/30000 Training Loss: 0.050235338509082794\n",
      "Epoch 13925/30000 Training Loss: 0.05548195540904999\n",
      "Epoch 13926/30000 Training Loss: 0.06121180206537247\n",
      "Epoch 13927/30000 Training Loss: 0.03961757943034172\n",
      "Epoch 13928/30000 Training Loss: 0.04639970138669014\n",
      "Epoch 13929/30000 Training Loss: 0.049779221415519714\n",
      "Epoch 13930/30000 Training Loss: 0.04156359285116196\n",
      "Epoch 13931/30000 Training Loss: 0.039208345115184784\n",
      "Epoch 13932/30000 Training Loss: 0.03704534471035004\n",
      "Epoch 13933/30000 Training Loss: 0.04512719064950943\n",
      "Epoch 13934/30000 Training Loss: 0.05759381502866745\n",
      "Epoch 13935/30000 Training Loss: 0.0647900328040123\n",
      "Epoch 13936/30000 Training Loss: 0.036554139107465744\n",
      "Epoch 13937/30000 Training Loss: 0.04549086466431618\n",
      "Epoch 13938/30000 Training Loss: 0.055564723908901215\n",
      "Epoch 13939/30000 Training Loss: 0.0583677738904953\n",
      "Epoch 13940/30000 Training Loss: 0.04178713262081146\n",
      "Epoch 13941/30000 Training Loss: 0.058821432292461395\n",
      "Epoch 13942/30000 Training Loss: 0.04450979828834534\n",
      "Epoch 13943/30000 Training Loss: 0.04805324226617813\n",
      "Epoch 13944/30000 Training Loss: 0.0516769140958786\n",
      "Epoch 13945/30000 Training Loss: 0.04679008200764656\n",
      "Epoch 13946/30000 Training Loss: 0.04442655295133591\n",
      "Epoch 13947/30000 Training Loss: 0.05627055466175079\n",
      "Epoch 13948/30000 Training Loss: 0.03444037586450577\n",
      "Epoch 13949/30000 Training Loss: 0.0393134206533432\n",
      "Epoch 13950/30000 Training Loss: 0.06208076328039169\n",
      "Epoch 13951/30000 Training Loss: 0.050362128764390945\n",
      "Epoch 13952/30000 Training Loss: 0.053887657821178436\n",
      "Epoch 13953/30000 Training Loss: 0.0400533527135849\n",
      "Epoch 13954/30000 Training Loss: 0.03407321125268936\n",
      "Epoch 13955/30000 Training Loss: 0.050004422664642334\n",
      "Epoch 13956/30000 Training Loss: 0.03615414723753929\n",
      "Epoch 13957/30000 Training Loss: 0.042822279036045074\n",
      "Epoch 13958/30000 Training Loss: 0.045912981033325195\n",
      "Epoch 13959/30000 Training Loss: 0.04490058869123459\n",
      "Epoch 13960/30000 Training Loss: 0.05403551459312439\n",
      "Epoch 13961/30000 Training Loss: 0.04685162380337715\n",
      "Epoch 13962/30000 Training Loss: 0.03491703420877457\n",
      "Epoch 13963/30000 Training Loss: 0.05088274925947189\n",
      "Epoch 13964/30000 Training Loss: 0.05155915021896362\n",
      "Epoch 13965/30000 Training Loss: 0.03758576139807701\n",
      "Epoch 13966/30000 Training Loss: 0.04498908296227455\n",
      "Epoch 13967/30000 Training Loss: 0.04952215403318405\n",
      "Epoch 13968/30000 Training Loss: 0.05331479012966156\n",
      "Epoch 13969/30000 Training Loss: 0.061187975108623505\n",
      "Epoch 13970/30000 Training Loss: 0.038099199533462524\n",
      "Epoch 13971/30000 Training Loss: 0.04411905258893967\n",
      "Epoch 13972/30000 Training Loss: 0.04780752211809158\n",
      "Epoch 13973/30000 Training Loss: 0.055319834500551224\n",
      "Epoch 13974/30000 Training Loss: 0.05375434830784798\n",
      "Epoch 13975/30000 Training Loss: 0.04471920430660248\n",
      "Epoch 13976/30000 Training Loss: 0.05045720934867859\n",
      "Epoch 13977/30000 Training Loss: 0.045192353427410126\n",
      "Epoch 13978/30000 Training Loss: 0.03857424482703209\n",
      "Epoch 13979/30000 Training Loss: 0.04546507075428963\n",
      "Epoch 13980/30000 Training Loss: 0.04360329359769821\n",
      "Epoch 13981/30000 Training Loss: 0.03615967929363251\n",
      "Epoch 13982/30000 Training Loss: 0.04898480698466301\n",
      "Epoch 13983/30000 Training Loss: 0.05021741986274719\n",
      "Epoch 13984/30000 Training Loss: 0.03902029991149902\n",
      "Epoch 13985/30000 Training Loss: 0.050382692366838455\n",
      "Epoch 13986/30000 Training Loss: 0.05832899361848831\n",
      "Epoch 13987/30000 Training Loss: 0.04372398555278778\n",
      "Epoch 13988/30000 Training Loss: 0.03775715082883835\n",
      "Epoch 13989/30000 Training Loss: 0.0499456524848938\n",
      "Epoch 13990/30000 Training Loss: 0.03633682429790497\n",
      "Epoch 13991/30000 Training Loss: 0.05277884751558304\n",
      "Epoch 13992/30000 Training Loss: 0.04171987995505333\n",
      "Epoch 13993/30000 Training Loss: 0.04955379664897919\n",
      "Epoch 13994/30000 Training Loss: 0.04177489131689072\n",
      "Epoch 13995/30000 Training Loss: 0.06050106883049011\n",
      "Epoch 13996/30000 Training Loss: 0.043707214295864105\n",
      "Epoch 13997/30000 Training Loss: 0.04754440486431122\n",
      "Epoch 13998/30000 Training Loss: 0.04075583070516586\n",
      "Epoch 13999/30000 Training Loss: 0.041347526013851166\n",
      "Epoch 14000/30000 Training Loss: 0.04868663102388382\n",
      "Epoch 14000/30000 Validation Loss: 0.03828579932451248\n",
      "Epoch 14001/30000 Training Loss: 0.05440664291381836\n",
      "Epoch 14002/30000 Training Loss: 0.03208797052502632\n",
      "Epoch 14003/30000 Training Loss: 0.03940579295158386\n",
      "Epoch 14004/30000 Training Loss: 0.04282229021191597\n",
      "Epoch 14005/30000 Training Loss: 0.05394658446311951\n",
      "Epoch 14006/30000 Training Loss: 0.0589619055390358\n",
      "Epoch 14007/30000 Training Loss: 0.03754916787147522\n",
      "Epoch 14008/30000 Training Loss: 0.04240056127309799\n",
      "Epoch 14009/30000 Training Loss: 0.04123759642243385\n",
      "Epoch 14010/30000 Training Loss: 0.04113954305648804\n",
      "Epoch 14011/30000 Training Loss: 0.04869457706809044\n",
      "Epoch 14012/30000 Training Loss: 0.031222816556692123\n",
      "Epoch 14013/30000 Training Loss: 0.05340950936079025\n",
      "Epoch 14014/30000 Training Loss: 0.04658485949039459\n",
      "Epoch 14015/30000 Training Loss: 0.03815170377492905\n",
      "Epoch 14016/30000 Training Loss: 0.03705155849456787\n",
      "Epoch 14017/30000 Training Loss: 0.04009641706943512\n",
      "Epoch 14018/30000 Training Loss: 0.05399402976036072\n",
      "Epoch 14019/30000 Training Loss: 0.05089786276221275\n",
      "Epoch 14020/30000 Training Loss: 0.04658835008740425\n",
      "Epoch 14021/30000 Training Loss: 0.04139197617769241\n",
      "Epoch 14022/30000 Training Loss: 0.050281211733818054\n",
      "Epoch 14023/30000 Training Loss: 0.060087792575359344\n",
      "Epoch 14024/30000 Training Loss: 0.044027552008628845\n",
      "Epoch 14025/30000 Training Loss: 0.03612218052148819\n",
      "Epoch 14026/30000 Training Loss: 0.042419493198394775\n",
      "Epoch 14027/30000 Training Loss: 0.048841629177331924\n",
      "Epoch 14028/30000 Training Loss: 0.046166010200977325\n",
      "Epoch 14029/30000 Training Loss: 0.058283865451812744\n",
      "Epoch 14030/30000 Training Loss: 0.04666458070278168\n",
      "Epoch 14031/30000 Training Loss: 0.03198202699422836\n",
      "Epoch 14032/30000 Training Loss: 0.038708098232746124\n",
      "Epoch 14033/30000 Training Loss: 0.05122491344809532\n",
      "Epoch 14034/30000 Training Loss: 0.04099944978952408\n",
      "Epoch 14035/30000 Training Loss: 0.040167730301618576\n",
      "Epoch 14036/30000 Training Loss: 0.038502685725688934\n",
      "Epoch 14037/30000 Training Loss: 0.062164586037397385\n",
      "Epoch 14038/30000 Training Loss: 0.04571017622947693\n",
      "Epoch 14039/30000 Training Loss: 0.05313938856124878\n",
      "Epoch 14040/30000 Training Loss: 0.03280406445264816\n",
      "Epoch 14041/30000 Training Loss: 0.05829299986362457\n",
      "Epoch 14042/30000 Training Loss: 0.045002613216638565\n",
      "Epoch 14043/30000 Training Loss: 0.04360717162489891\n",
      "Epoch 14044/30000 Training Loss: 0.02771957777440548\n",
      "Epoch 14045/30000 Training Loss: 0.0426827147603035\n",
      "Epoch 14046/30000 Training Loss: 0.045188866555690765\n",
      "Epoch 14047/30000 Training Loss: 0.04226714000105858\n",
      "Epoch 14048/30000 Training Loss: 0.03730615973472595\n",
      "Epoch 14049/30000 Training Loss: 0.04468073695898056\n",
      "Epoch 14050/30000 Training Loss: 0.03678970783948898\n",
      "Epoch 14051/30000 Training Loss: 0.046592678874731064\n",
      "Epoch 14052/30000 Training Loss: 0.043036267161369324\n",
      "Epoch 14053/30000 Training Loss: 0.03589152544736862\n",
      "Epoch 14054/30000 Training Loss: 0.0546267032623291\n",
      "Epoch 14055/30000 Training Loss: 0.03656947240233421\n",
      "Epoch 14056/30000 Training Loss: 0.041787225753068924\n",
      "Epoch 14057/30000 Training Loss: 0.052506446838378906\n",
      "Epoch 14058/30000 Training Loss: 0.037423934787511826\n",
      "Epoch 14059/30000 Training Loss: 0.042828820645809174\n",
      "Epoch 14060/30000 Training Loss: 0.04369278997182846\n",
      "Epoch 14061/30000 Training Loss: 0.044041071087121964\n",
      "Epoch 14062/30000 Training Loss: 0.04903041571378708\n",
      "Epoch 14063/30000 Training Loss: 0.03748945891857147\n",
      "Epoch 14064/30000 Training Loss: 0.04035276174545288\n",
      "Epoch 14065/30000 Training Loss: 0.04407252371311188\n",
      "Epoch 14066/30000 Training Loss: 0.052915625274181366\n",
      "Epoch 14067/30000 Training Loss: 0.036306314170360565\n",
      "Epoch 14068/30000 Training Loss: 0.05728601664304733\n",
      "Epoch 14069/30000 Training Loss: 0.04495042562484741\n",
      "Epoch 14070/30000 Training Loss: 0.05120372027158737\n",
      "Epoch 14071/30000 Training Loss: 0.04119429737329483\n",
      "Epoch 14072/30000 Training Loss: 0.0577734038233757\n",
      "Epoch 14073/30000 Training Loss: 0.04207555949687958\n",
      "Epoch 14074/30000 Training Loss: 0.05587629973888397\n",
      "Epoch 14075/30000 Training Loss: 0.0484478697180748\n",
      "Epoch 14076/30000 Training Loss: 0.038396600633859634\n",
      "Epoch 14077/30000 Training Loss: 0.04350854456424713\n",
      "Epoch 14078/30000 Training Loss: 0.04285100847482681\n",
      "Epoch 14079/30000 Training Loss: 0.0443231463432312\n",
      "Epoch 14080/30000 Training Loss: 0.048611827194690704\n",
      "Epoch 14081/30000 Training Loss: 0.03906231373548508\n",
      "Epoch 14082/30000 Training Loss: 0.035714201629161835\n",
      "Epoch 14083/30000 Training Loss: 0.032244086265563965\n",
      "Epoch 14084/30000 Training Loss: 0.05295021831989288\n",
      "Epoch 14085/30000 Training Loss: 0.04508941248059273\n",
      "Epoch 14086/30000 Training Loss: 0.045536644756793976\n",
      "Epoch 14087/30000 Training Loss: 0.041926801204681396\n",
      "Epoch 14088/30000 Training Loss: 0.04756781831383705\n",
      "Epoch 14089/30000 Training Loss: 0.04848386347293854\n",
      "Epoch 14090/30000 Training Loss: 0.04866894334554672\n",
      "Epoch 14091/30000 Training Loss: 0.05376853048801422\n",
      "Epoch 14092/30000 Training Loss: 0.060810212045907974\n",
      "Epoch 14093/30000 Training Loss: 0.04583338648080826\n",
      "Epoch 14094/30000 Training Loss: 0.05156788229942322\n",
      "Epoch 14095/30000 Training Loss: 0.039178866893053055\n",
      "Epoch 14096/30000 Training Loss: 0.04910748451948166\n",
      "Epoch 14097/30000 Training Loss: 0.032632723450660706\n",
      "Epoch 14098/30000 Training Loss: 0.049947597086429596\n",
      "Epoch 14099/30000 Training Loss: 0.04565466567873955\n",
      "Epoch 14100/30000 Training Loss: 0.06063978374004364\n",
      "Epoch 14100/30000 Validation Loss: 0.05841507762670517\n",
      "Epoch 14101/30000 Training Loss: 0.03746006265282631\n",
      "Epoch 14102/30000 Training Loss: 0.0422569140791893\n",
      "Epoch 14103/30000 Training Loss: 0.04245813190937042\n",
      "Epoch 14104/30000 Training Loss: 0.05168827623128891\n",
      "Epoch 14105/30000 Training Loss: 0.044956985861063004\n",
      "Epoch 14106/30000 Training Loss: 0.05130977928638458\n",
      "Epoch 14107/30000 Training Loss: 0.0603410080075264\n",
      "Epoch 14108/30000 Training Loss: 0.0561203807592392\n",
      "Epoch 14109/30000 Training Loss: 0.05144483968615532\n",
      "Epoch 14110/30000 Training Loss: 0.04918226599693298\n",
      "Epoch 14111/30000 Training Loss: 0.040660325437784195\n",
      "Epoch 14112/30000 Training Loss: 0.053498655557632446\n",
      "Epoch 14113/30000 Training Loss: 0.048087477684020996\n",
      "Epoch 14114/30000 Training Loss: 0.04941244423389435\n",
      "Epoch 14115/30000 Training Loss: 0.03578721359372139\n",
      "Epoch 14116/30000 Training Loss: 0.058507248759269714\n",
      "Epoch 14117/30000 Training Loss: 0.03822838142514229\n",
      "Epoch 14118/30000 Training Loss: 0.07174370437860489\n",
      "Epoch 14119/30000 Training Loss: 0.06136564910411835\n",
      "Epoch 14120/30000 Training Loss: 0.03695075958967209\n",
      "Epoch 14121/30000 Training Loss: 0.04920018091797829\n",
      "Epoch 14122/30000 Training Loss: 0.05487838014960289\n",
      "Epoch 14123/30000 Training Loss: 0.045715589076280594\n",
      "Epoch 14124/30000 Training Loss: 0.05123453959822655\n",
      "Epoch 14125/30000 Training Loss: 0.04719281941652298\n",
      "Epoch 14126/30000 Training Loss: 0.049179233610630035\n",
      "Epoch 14127/30000 Training Loss: 0.042783137410879135\n",
      "Epoch 14128/30000 Training Loss: 0.051339104771614075\n",
      "Epoch 14129/30000 Training Loss: 0.04076845571398735\n",
      "Epoch 14130/30000 Training Loss: 0.0448288768529892\n",
      "Epoch 14131/30000 Training Loss: 0.05146108567714691\n",
      "Epoch 14132/30000 Training Loss: 0.040242452174425125\n",
      "Epoch 14133/30000 Training Loss: 0.04840032011270523\n",
      "Epoch 14134/30000 Training Loss: 0.04447168484330177\n",
      "Epoch 14135/30000 Training Loss: 0.04440351575613022\n",
      "Epoch 14136/30000 Training Loss: 0.034422069787979126\n",
      "Epoch 14137/30000 Training Loss: 0.05608675628900528\n",
      "Epoch 14138/30000 Training Loss: 0.046570535749197006\n",
      "Epoch 14139/30000 Training Loss: 0.03350725397467613\n",
      "Epoch 14140/30000 Training Loss: 0.03186827898025513\n",
      "Epoch 14141/30000 Training Loss: 0.05357855558395386\n",
      "Epoch 14142/30000 Training Loss: 0.05151542276144028\n",
      "Epoch 14143/30000 Training Loss: 0.059534210711717606\n",
      "Epoch 14144/30000 Training Loss: 0.04066663980484009\n",
      "Epoch 14145/30000 Training Loss: 0.054402679204940796\n",
      "Epoch 14146/30000 Training Loss: 0.045441389083862305\n",
      "Epoch 14147/30000 Training Loss: 0.06660299003124237\n",
      "Epoch 14148/30000 Training Loss: 0.04092871770262718\n",
      "Epoch 14149/30000 Training Loss: 0.05632826313376427\n",
      "Epoch 14150/30000 Training Loss: 0.05993857607245445\n",
      "Epoch 14151/30000 Training Loss: 0.04348210245370865\n",
      "Epoch 14152/30000 Training Loss: 0.03732207417488098\n",
      "Epoch 14153/30000 Training Loss: 0.036547958850860596\n",
      "Epoch 14154/30000 Training Loss: 0.03609248623251915\n",
      "Epoch 14155/30000 Training Loss: 0.05591903254389763\n",
      "Epoch 14156/30000 Training Loss: 0.04865865409374237\n",
      "Epoch 14157/30000 Training Loss: 0.041145339608192444\n",
      "Epoch 14158/30000 Training Loss: 0.06153175234794617\n",
      "Epoch 14159/30000 Training Loss: 0.04733873903751373\n",
      "Epoch 14160/30000 Training Loss: 0.054592423141002655\n",
      "Epoch 14161/30000 Training Loss: 0.04212154448032379\n",
      "Epoch 14162/30000 Training Loss: 0.03475606068968773\n",
      "Epoch 14163/30000 Training Loss: 0.042502183467149734\n",
      "Epoch 14164/30000 Training Loss: 0.04694994539022446\n",
      "Epoch 14165/30000 Training Loss: 0.03795989230275154\n",
      "Epoch 14166/30000 Training Loss: 0.04110867530107498\n",
      "Epoch 14167/30000 Training Loss: 0.038905270397663116\n",
      "Epoch 14168/30000 Training Loss: 0.04019751027226448\n",
      "Epoch 14169/30000 Training Loss: 0.048277828842401505\n",
      "Epoch 14170/30000 Training Loss: 0.03505557030439377\n",
      "Epoch 14171/30000 Training Loss: 0.038201771676540375\n",
      "Epoch 14172/30000 Training Loss: 0.029466554522514343\n",
      "Epoch 14173/30000 Training Loss: 0.04118339717388153\n",
      "Epoch 14174/30000 Training Loss: 0.05454541742801666\n",
      "Epoch 14175/30000 Training Loss: 0.045370012521743774\n",
      "Epoch 14176/30000 Training Loss: 0.04745606333017349\n",
      "Epoch 14177/30000 Training Loss: 0.03792167082428932\n",
      "Epoch 14178/30000 Training Loss: 0.04413334280252457\n",
      "Epoch 14179/30000 Training Loss: 0.04247418791055679\n",
      "Epoch 14180/30000 Training Loss: 0.03689882904291153\n",
      "Epoch 14181/30000 Training Loss: 0.057597555220127106\n",
      "Epoch 14182/30000 Training Loss: 0.03882291167974472\n",
      "Epoch 14183/30000 Training Loss: 0.03183366358280182\n",
      "Epoch 14184/30000 Training Loss: 0.04142835736274719\n",
      "Epoch 14185/30000 Training Loss: 0.05461727827787399\n",
      "Epoch 14186/30000 Training Loss: 0.06229236349463463\n",
      "Epoch 14187/30000 Training Loss: 0.04431120306253433\n",
      "Epoch 14188/30000 Training Loss: 0.036127492785453796\n",
      "Epoch 14189/30000 Training Loss: 0.05542683228850365\n",
      "Epoch 14190/30000 Training Loss: 0.04331167787313461\n",
      "Epoch 14191/30000 Training Loss: 0.04381982982158661\n",
      "Epoch 14192/30000 Training Loss: 0.048950791358947754\n",
      "Epoch 14193/30000 Training Loss: 0.048625774681568146\n",
      "Epoch 14194/30000 Training Loss: 0.03647594898939133\n",
      "Epoch 14195/30000 Training Loss: 0.05025014281272888\n",
      "Epoch 14196/30000 Training Loss: 0.04486895352602005\n",
      "Epoch 14197/30000 Training Loss: 0.04275345802307129\n",
      "Epoch 14198/30000 Training Loss: 0.03651037812232971\n",
      "Epoch 14199/30000 Training Loss: 0.05047797039151192\n",
      "Epoch 14200/30000 Training Loss: 0.03876979276537895\n",
      "Epoch 14200/30000 Validation Loss: 0.038795921951532364\n",
      "Epoch 14201/30000 Training Loss: 0.03943195194005966\n",
      "Epoch 14202/30000 Training Loss: 0.04870876669883728\n",
      "Epoch 14203/30000 Training Loss: 0.039492249488830566\n",
      "Epoch 14204/30000 Training Loss: 0.047317568212747574\n",
      "Epoch 14205/30000 Training Loss: 0.03621777147054672\n",
      "Epoch 14206/30000 Training Loss: 0.05446062237024307\n",
      "Epoch 14207/30000 Training Loss: 0.042824022471904755\n",
      "Epoch 14208/30000 Training Loss: 0.045980751514434814\n",
      "Epoch 14209/30000 Training Loss: 0.05240989103913307\n",
      "Epoch 14210/30000 Training Loss: 0.04773249477148056\n",
      "Epoch 14211/30000 Training Loss: 0.06362561881542206\n",
      "Epoch 14212/30000 Training Loss: 0.04397515207529068\n",
      "Epoch 14213/30000 Training Loss: 0.0422942079603672\n",
      "Epoch 14214/30000 Training Loss: 0.05429215356707573\n",
      "Epoch 14215/30000 Training Loss: 0.053803905844688416\n",
      "Epoch 14216/30000 Training Loss: 0.05690864101052284\n",
      "Epoch 14217/30000 Training Loss: 0.050157465040683746\n",
      "Epoch 14218/30000 Training Loss: 0.0405644029378891\n",
      "Epoch 14219/30000 Training Loss: 0.0437982939183712\n",
      "Epoch 14220/30000 Training Loss: 0.054077379405498505\n",
      "Epoch 14221/30000 Training Loss: 0.042082279920578\n",
      "Epoch 14222/30000 Training Loss: 0.04600871354341507\n",
      "Epoch 14223/30000 Training Loss: 0.0286484993994236\n",
      "Epoch 14224/30000 Training Loss: 0.0504877045750618\n",
      "Epoch 14225/30000 Training Loss: 0.04990518093109131\n",
      "Epoch 14226/30000 Training Loss: 0.03712055832147598\n",
      "Epoch 14227/30000 Training Loss: 0.054454632103443146\n",
      "Epoch 14228/30000 Training Loss: 0.05563990771770477\n",
      "Epoch 14229/30000 Training Loss: 0.05274999141693115\n",
      "Epoch 14230/30000 Training Loss: 0.03847121447324753\n",
      "Epoch 14231/30000 Training Loss: 0.045368362218141556\n",
      "Epoch 14232/30000 Training Loss: 0.050742994993925095\n",
      "Epoch 14233/30000 Training Loss: 0.04797472804784775\n",
      "Epoch 14234/30000 Training Loss: 0.041453175246715546\n",
      "Epoch 14235/30000 Training Loss: 0.05039473623037338\n",
      "Epoch 14236/30000 Training Loss: 0.03928148001432419\n",
      "Epoch 14237/30000 Training Loss: 0.0512874573469162\n",
      "Epoch 14238/30000 Training Loss: 0.038819704204797745\n",
      "Epoch 14239/30000 Training Loss: 0.03338497877120972\n",
      "Epoch 14240/30000 Training Loss: 0.06160077825188637\n",
      "Epoch 14241/30000 Training Loss: 0.044782016426324844\n",
      "Epoch 14242/30000 Training Loss: 0.050793036818504333\n",
      "Epoch 14243/30000 Training Loss: 0.035968247801065445\n",
      "Epoch 14244/30000 Training Loss: 0.06088586896657944\n",
      "Epoch 14245/30000 Training Loss: 0.043738216161727905\n",
      "Epoch 14246/30000 Training Loss: 0.045670561492443085\n",
      "Epoch 14247/30000 Training Loss: 0.04614586383104324\n",
      "Epoch 14248/30000 Training Loss: 0.034662846475839615\n",
      "Epoch 14249/30000 Training Loss: 0.054777372628450394\n",
      "Epoch 14250/30000 Training Loss: 0.044931575655937195\n",
      "Epoch 14251/30000 Training Loss: 0.05142080783843994\n",
      "Epoch 14252/30000 Training Loss: 0.046482957899570465\n",
      "Epoch 14253/30000 Training Loss: 0.04755256325006485\n",
      "Epoch 14254/30000 Training Loss: 0.04056347534060478\n",
      "Epoch 14255/30000 Training Loss: 0.04776834696531296\n",
      "Epoch 14256/30000 Training Loss: 0.04382479190826416\n",
      "Epoch 14257/30000 Training Loss: 0.04777480661869049\n",
      "Epoch 14258/30000 Training Loss: 0.04208774119615555\n",
      "Epoch 14259/30000 Training Loss: 0.04948923736810684\n",
      "Epoch 14260/30000 Training Loss: 0.031101806089282036\n",
      "Epoch 14261/30000 Training Loss: 0.030586326494812965\n",
      "Epoch 14262/30000 Training Loss: 0.043955590575933456\n",
      "Epoch 14263/30000 Training Loss: 0.036197811365127563\n",
      "Epoch 14264/30000 Training Loss: 0.04581820219755173\n",
      "Epoch 14265/30000 Training Loss: 0.03744220361113548\n",
      "Epoch 14266/30000 Training Loss: 0.04232746362686157\n",
      "Epoch 14267/30000 Training Loss: 0.03219026327133179\n",
      "Epoch 14268/30000 Training Loss: 0.04293588548898697\n",
      "Epoch 14269/30000 Training Loss: 0.06059473752975464\n",
      "Epoch 14270/30000 Training Loss: 0.049655795097351074\n",
      "Epoch 14271/30000 Training Loss: 0.0476156547665596\n",
      "Epoch 14272/30000 Training Loss: 0.04550732672214508\n",
      "Epoch 14273/30000 Training Loss: 0.04969857633113861\n",
      "Epoch 14274/30000 Training Loss: 0.04774066060781479\n",
      "Epoch 14275/30000 Training Loss: 0.05797058343887329\n",
      "Epoch 14276/30000 Training Loss: 0.0424436517059803\n",
      "Epoch 14277/30000 Training Loss: 0.03925512358546257\n",
      "Epoch 14278/30000 Training Loss: 0.037494052201509476\n",
      "Epoch 14279/30000 Training Loss: 0.05823994055390358\n",
      "Epoch 14280/30000 Training Loss: 0.041333746165037155\n",
      "Epoch 14281/30000 Training Loss: 0.047962792217731476\n",
      "Epoch 14282/30000 Training Loss: 0.04752916097640991\n",
      "Epoch 14283/30000 Training Loss: 0.037264447659254074\n",
      "Epoch 14284/30000 Training Loss: 0.05200067162513733\n",
      "Epoch 14285/30000 Training Loss: 0.0456271655857563\n",
      "Epoch 14286/30000 Training Loss: 0.05047913268208504\n",
      "Epoch 14287/30000 Training Loss: 0.05556144565343857\n",
      "Epoch 14288/30000 Training Loss: 0.04029305279254913\n",
      "Epoch 14289/30000 Training Loss: 0.04704026132822037\n",
      "Epoch 14290/30000 Training Loss: 0.042741551995277405\n",
      "Epoch 14291/30000 Training Loss: 0.0419955849647522\n",
      "Epoch 14292/30000 Training Loss: 0.050495781004428864\n",
      "Epoch 14293/30000 Training Loss: 0.04076886922121048\n",
      "Epoch 14294/30000 Training Loss: 0.04146648943424225\n",
      "Epoch 14295/30000 Training Loss: 0.05875431001186371\n",
      "Epoch 14296/30000 Training Loss: 0.04106951504945755\n",
      "Epoch 14297/30000 Training Loss: 0.05686555802822113\n",
      "Epoch 14298/30000 Training Loss: 0.036646436899900436\n",
      "Epoch 14299/30000 Training Loss: 0.05494040250778198\n",
      "Epoch 14300/30000 Training Loss: 0.04143486171960831\n",
      "Epoch 14300/30000 Validation Loss: 0.03965198993682861\n",
      "Epoch 14301/30000 Training Loss: 0.04067461937665939\n",
      "Epoch 14302/30000 Training Loss: 0.04896005243062973\n",
      "Epoch 14303/30000 Training Loss: 0.0454128235578537\n",
      "Epoch 14304/30000 Training Loss: 0.0386059507727623\n",
      "Epoch 14305/30000 Training Loss: 0.04672598838806152\n",
      "Epoch 14306/30000 Training Loss: 0.04469425976276398\n",
      "Epoch 14307/30000 Training Loss: 0.04793220013380051\n",
      "Epoch 14308/30000 Training Loss: 0.03863473981618881\n",
      "Epoch 14309/30000 Training Loss: 0.04231209307909012\n",
      "Epoch 14310/30000 Training Loss: 0.053557053208351135\n",
      "Epoch 14311/30000 Training Loss: 0.047547101974487305\n",
      "Epoch 14312/30000 Training Loss: 0.0390140675008297\n",
      "Epoch 14313/30000 Training Loss: 0.04072478413581848\n",
      "Epoch 14314/30000 Training Loss: 0.05846545100212097\n",
      "Epoch 14315/30000 Training Loss: 0.056252621114254\n",
      "Epoch 14316/30000 Training Loss: 0.05140302702784538\n",
      "Epoch 14317/30000 Training Loss: 0.041620317846536636\n",
      "Epoch 14318/30000 Training Loss: 0.050640106201171875\n",
      "Epoch 14319/30000 Training Loss: 0.0470646470785141\n",
      "Epoch 14320/30000 Training Loss: 0.04557096213102341\n",
      "Epoch 14321/30000 Training Loss: 0.04237796366214752\n",
      "Epoch 14322/30000 Training Loss: 0.04969709739089012\n",
      "Epoch 14323/30000 Training Loss: 0.056088127195835114\n",
      "Epoch 14324/30000 Training Loss: 0.04743912070989609\n",
      "Epoch 14325/30000 Training Loss: 0.050426457077264786\n",
      "Epoch 14326/30000 Training Loss: 0.055547259747982025\n",
      "Epoch 14327/30000 Training Loss: 0.04970230162143707\n",
      "Epoch 14328/30000 Training Loss: 0.03438229113817215\n",
      "Epoch 14329/30000 Training Loss: 0.046677522361278534\n",
      "Epoch 14330/30000 Training Loss: 0.03342815116047859\n",
      "Epoch 14331/30000 Training Loss: 0.039001818746328354\n",
      "Epoch 14332/30000 Training Loss: 0.054935622960329056\n",
      "Epoch 14333/30000 Training Loss: 0.048958487808704376\n",
      "Epoch 14334/30000 Training Loss: 0.03952011466026306\n",
      "Epoch 14335/30000 Training Loss: 0.04846682399511337\n",
      "Epoch 14336/30000 Training Loss: 0.044453978538513184\n",
      "Epoch 14337/30000 Training Loss: 0.06284278631210327\n",
      "Epoch 14338/30000 Training Loss: 0.05728232488036156\n",
      "Epoch 14339/30000 Training Loss: 0.03738826513290405\n",
      "Epoch 14340/30000 Training Loss: 0.03642309829592705\n",
      "Epoch 14341/30000 Training Loss: 0.03490278124809265\n",
      "Epoch 14342/30000 Training Loss: 0.055125243961811066\n",
      "Epoch 14343/30000 Training Loss: 0.040624216198921204\n",
      "Epoch 14344/30000 Training Loss: 0.05587996542453766\n",
      "Epoch 14345/30000 Training Loss: 0.04194583743810654\n",
      "Epoch 14346/30000 Training Loss: 0.05298328772187233\n",
      "Epoch 14347/30000 Training Loss: 0.03508540242910385\n",
      "Epoch 14348/30000 Training Loss: 0.03941100835800171\n",
      "Epoch 14349/30000 Training Loss: 0.06396764516830444\n",
      "Epoch 14350/30000 Training Loss: 0.04469830542802811\n",
      "Epoch 14351/30000 Training Loss: 0.058760784566402435\n",
      "Epoch 14352/30000 Training Loss: 0.04499612748622894\n",
      "Epoch 14353/30000 Training Loss: 0.05936086177825928\n",
      "Epoch 14354/30000 Training Loss: 0.03681914508342743\n",
      "Epoch 14355/30000 Training Loss: 0.027018997818231583\n",
      "Epoch 14356/30000 Training Loss: 0.043811626732349396\n",
      "Epoch 14357/30000 Training Loss: 0.04386911541223526\n",
      "Epoch 14358/30000 Training Loss: 0.04908823221921921\n",
      "Epoch 14359/30000 Training Loss: 0.047672808170318604\n",
      "Epoch 14360/30000 Training Loss: 0.04860330745577812\n",
      "Epoch 14361/30000 Training Loss: 0.05409485846757889\n",
      "Epoch 14362/30000 Training Loss: 0.04741501063108444\n",
      "Epoch 14363/30000 Training Loss: 0.044212400913238525\n",
      "Epoch 14364/30000 Training Loss: 0.04443610832095146\n",
      "Epoch 14365/30000 Training Loss: 0.05436800420284271\n",
      "Epoch 14366/30000 Training Loss: 0.05018126964569092\n",
      "Epoch 14367/30000 Training Loss: 0.04120220988988876\n",
      "Epoch 14368/30000 Training Loss: 0.055867522954940796\n",
      "Epoch 14369/30000 Training Loss: 0.036717090755701065\n",
      "Epoch 14370/30000 Training Loss: 0.04324868321418762\n",
      "Epoch 14371/30000 Training Loss: 0.04303711652755737\n",
      "Epoch 14372/30000 Training Loss: 0.06577517092227936\n",
      "Epoch 14373/30000 Training Loss: 0.0458059087395668\n",
      "Epoch 14374/30000 Training Loss: 0.04511822387576103\n",
      "Epoch 14375/30000 Training Loss: 0.04832053929567337\n",
      "Epoch 14376/30000 Training Loss: 0.043589480221271515\n",
      "Epoch 14377/30000 Training Loss: 0.03690440580248833\n",
      "Epoch 14378/30000 Training Loss: 0.035172026604413986\n",
      "Epoch 14379/30000 Training Loss: 0.043523360043764114\n",
      "Epoch 14380/30000 Training Loss: 0.04894937202334404\n",
      "Epoch 14381/30000 Training Loss: 0.04622449725866318\n",
      "Epoch 14382/30000 Training Loss: 0.04026874899864197\n",
      "Epoch 14383/30000 Training Loss: 0.04033319652080536\n",
      "Epoch 14384/30000 Training Loss: 0.04669787362217903\n",
      "Epoch 14385/30000 Training Loss: 0.05247402563691139\n",
      "Epoch 14386/30000 Training Loss: 0.042002372443675995\n",
      "Epoch 14387/30000 Training Loss: 0.07069851458072662\n",
      "Epoch 14388/30000 Training Loss: 0.047214336693286896\n",
      "Epoch 14389/30000 Training Loss: 0.04500807076692581\n",
      "Epoch 14390/30000 Training Loss: 0.0715574324131012\n",
      "Epoch 14391/30000 Training Loss: 0.03697016462683678\n",
      "Epoch 14392/30000 Training Loss: 0.048910390585660934\n",
      "Epoch 14393/30000 Training Loss: 0.03836701065301895\n",
      "Epoch 14394/30000 Training Loss: 0.03909670561552048\n",
      "Epoch 14395/30000 Training Loss: 0.05763191357254982\n",
      "Epoch 14396/30000 Training Loss: 0.03538820520043373\n",
      "Epoch 14397/30000 Training Loss: 0.0493563711643219\n",
      "Epoch 14398/30000 Training Loss: 0.047224078327417374\n",
      "Epoch 14399/30000 Training Loss: 0.05397544428706169\n",
      "Epoch 14400/30000 Training Loss: 0.0521589070558548\n",
      "Epoch 14400/30000 Validation Loss: 0.04251956194639206\n",
      "Epoch 14401/30000 Training Loss: 0.035086024552583694\n",
      "Epoch 14402/30000 Training Loss: 0.041493456810712814\n",
      "Epoch 14403/30000 Training Loss: 0.04699227213859558\n",
      "Epoch 14404/30000 Training Loss: 0.05388817936182022\n",
      "Epoch 14405/30000 Training Loss: 0.0517859160900116\n",
      "Epoch 14406/30000 Training Loss: 0.04116251319646835\n",
      "Epoch 14407/30000 Training Loss: 0.053964659571647644\n",
      "Epoch 14408/30000 Training Loss: 0.03036654368042946\n",
      "Epoch 14409/30000 Training Loss: 0.04533025249838829\n",
      "Epoch 14410/30000 Training Loss: 0.04568369686603546\n",
      "Epoch 14411/30000 Training Loss: 0.057853154838085175\n",
      "Epoch 14412/30000 Training Loss: 0.041966430842876434\n",
      "Epoch 14413/30000 Training Loss: 0.044888559728860855\n",
      "Epoch 14414/30000 Training Loss: 0.04497230425477028\n",
      "Epoch 14415/30000 Training Loss: 0.05734643340110779\n",
      "Epoch 14416/30000 Training Loss: 0.044997282326221466\n",
      "Epoch 14417/30000 Training Loss: 0.04721802473068237\n",
      "Epoch 14418/30000 Training Loss: 0.04465269297361374\n",
      "Epoch 14419/30000 Training Loss: 0.033553943037986755\n",
      "Epoch 14420/30000 Training Loss: 0.04449714720249176\n",
      "Epoch 14421/30000 Training Loss: 0.0397738441824913\n",
      "Epoch 14422/30000 Training Loss: 0.05202122777700424\n",
      "Epoch 14423/30000 Training Loss: 0.04578803479671478\n",
      "Epoch 14424/30000 Training Loss: 0.04422485828399658\n",
      "Epoch 14425/30000 Training Loss: 0.04918798804283142\n",
      "Epoch 14426/30000 Training Loss: 0.03704436868429184\n",
      "Epoch 14427/30000 Training Loss: 0.0479547418653965\n",
      "Epoch 14428/30000 Training Loss: 0.059009794145822525\n",
      "Epoch 14429/30000 Training Loss: 0.04502236098051071\n",
      "Epoch 14430/30000 Training Loss: 0.04188010096549988\n",
      "Epoch 14431/30000 Training Loss: 0.05595014989376068\n",
      "Epoch 14432/30000 Training Loss: 0.043648961931467056\n",
      "Epoch 14433/30000 Training Loss: 0.05321192741394043\n",
      "Epoch 14434/30000 Training Loss: 0.04722422733902931\n",
      "Epoch 14435/30000 Training Loss: 0.04597579687833786\n",
      "Epoch 14436/30000 Training Loss: 0.03997795283794403\n",
      "Epoch 14437/30000 Training Loss: 0.05338505655527115\n",
      "Epoch 14438/30000 Training Loss: 0.05148245766758919\n",
      "Epoch 14439/30000 Training Loss: 0.046417780220508575\n",
      "Epoch 14440/30000 Training Loss: 0.04851807653903961\n",
      "Epoch 14441/30000 Training Loss: 0.036574263125658035\n",
      "Epoch 14442/30000 Training Loss: 0.037444643676280975\n",
      "Epoch 14443/30000 Training Loss: 0.04829372838139534\n",
      "Epoch 14444/30000 Training Loss: 0.03795705735683441\n",
      "Epoch 14445/30000 Training Loss: 0.052108436822891235\n",
      "Epoch 14446/30000 Training Loss: 0.04435201734304428\n",
      "Epoch 14447/30000 Training Loss: 0.04855182766914368\n",
      "Epoch 14448/30000 Training Loss: 0.03797032684087753\n",
      "Epoch 14449/30000 Training Loss: 0.04640190675854683\n",
      "Epoch 14450/30000 Training Loss: 0.04824458435177803\n",
      "Epoch 14451/30000 Training Loss: 0.05158307030797005\n",
      "Epoch 14452/30000 Training Loss: 0.04358566552400589\n",
      "Epoch 14453/30000 Training Loss: 0.04334022477269173\n",
      "Epoch 14454/30000 Training Loss: 0.06182544678449631\n",
      "Epoch 14455/30000 Training Loss: 0.04334896802902222\n",
      "Epoch 14456/30000 Training Loss: 0.04596415162086487\n",
      "Epoch 14457/30000 Training Loss: 0.04849349707365036\n",
      "Epoch 14458/30000 Training Loss: 0.05595535784959793\n",
      "Epoch 14459/30000 Training Loss: 0.04868803545832634\n",
      "Epoch 14460/30000 Training Loss: 0.0567324161529541\n",
      "Epoch 14461/30000 Training Loss: 0.035253532230854034\n",
      "Epoch 14462/30000 Training Loss: 0.0503096878528595\n",
      "Epoch 14463/30000 Training Loss: 0.04542364925146103\n",
      "Epoch 14464/30000 Training Loss: 0.03406685218214989\n",
      "Epoch 14465/30000 Training Loss: 0.04035511985421181\n",
      "Epoch 14466/30000 Training Loss: 0.03886587917804718\n",
      "Epoch 14467/30000 Training Loss: 0.04556938260793686\n",
      "Epoch 14468/30000 Training Loss: 0.05174902081489563\n",
      "Epoch 14469/30000 Training Loss: 0.048796601593494415\n",
      "Epoch 14470/30000 Training Loss: 0.05293164402246475\n",
      "Epoch 14471/30000 Training Loss: 0.04632571339607239\n",
      "Epoch 14472/30000 Training Loss: 0.05024803429841995\n",
      "Epoch 14473/30000 Training Loss: 0.05449698865413666\n",
      "Epoch 14474/30000 Training Loss: 0.048205189406871796\n",
      "Epoch 14475/30000 Training Loss: 0.033749017864465714\n",
      "Epoch 14476/30000 Training Loss: 0.05486511066555977\n",
      "Epoch 14477/30000 Training Loss: 0.04535059630870819\n",
      "Epoch 14478/30000 Training Loss: 0.03869016468524933\n",
      "Epoch 14479/30000 Training Loss: 0.04415879398584366\n",
      "Epoch 14480/30000 Training Loss: 0.03229718282818794\n",
      "Epoch 14481/30000 Training Loss: 0.04251360148191452\n",
      "Epoch 14482/30000 Training Loss: 0.05553877726197243\n",
      "Epoch 14483/30000 Training Loss: 0.05266107618808746\n",
      "Epoch 14484/30000 Training Loss: 0.048702649772167206\n",
      "Epoch 14485/30000 Training Loss: 0.04094250872731209\n",
      "Epoch 14486/30000 Training Loss: 0.030280379578471184\n",
      "Epoch 14487/30000 Training Loss: 0.04712536931037903\n",
      "Epoch 14488/30000 Training Loss: 0.04775000736117363\n",
      "Epoch 14489/30000 Training Loss: 0.05315563827753067\n",
      "Epoch 14490/30000 Training Loss: 0.048402026295661926\n",
      "Epoch 14491/30000 Training Loss: 0.0399356484413147\n",
      "Epoch 14492/30000 Training Loss: 0.03494209796190262\n",
      "Epoch 14493/30000 Training Loss: 0.03975323587656021\n",
      "Epoch 14494/30000 Training Loss: 0.034598659723997116\n",
      "Epoch 14495/30000 Training Loss: 0.0310659296810627\n",
      "Epoch 14496/30000 Training Loss: 0.04584936797618866\n",
      "Epoch 14497/30000 Training Loss: 0.0494236946105957\n",
      "Epoch 14498/30000 Training Loss: 0.03954353928565979\n",
      "Epoch 14499/30000 Training Loss: 0.03840199112892151\n",
      "Epoch 14500/30000 Training Loss: 0.04849283769726753\n",
      "Epoch 14500/30000 Validation Loss: 0.04112251102924347\n",
      "Epoch 14501/30000 Training Loss: 0.05178143456578255\n",
      "Epoch 14502/30000 Training Loss: 0.049114104360342026\n",
      "Epoch 14503/30000 Training Loss: 0.037974029779434204\n",
      "Epoch 14504/30000 Training Loss: 0.04614122211933136\n",
      "Epoch 14505/30000 Training Loss: 0.06474414467811584\n",
      "Epoch 14506/30000 Training Loss: 0.05956324189901352\n",
      "Epoch 14507/30000 Training Loss: 0.06352518498897552\n",
      "Epoch 14508/30000 Training Loss: 0.06473281979560852\n",
      "Epoch 14509/30000 Training Loss: 0.05847702920436859\n",
      "Epoch 14510/30000 Training Loss: 0.039599187672138214\n",
      "Epoch 14511/30000 Training Loss: 0.046191513538360596\n",
      "Epoch 14512/30000 Training Loss: 0.041255928575992584\n",
      "Epoch 14513/30000 Training Loss: 0.030353974550962448\n",
      "Epoch 14514/30000 Training Loss: 0.03768840432167053\n",
      "Epoch 14515/30000 Training Loss: 0.04985241964459419\n",
      "Epoch 14516/30000 Training Loss: 0.04282788559794426\n",
      "Epoch 14517/30000 Training Loss: 0.03668808937072754\n",
      "Epoch 14518/30000 Training Loss: 0.04852426052093506\n",
      "Epoch 14519/30000 Training Loss: 0.042471449822187424\n",
      "Epoch 14520/30000 Training Loss: 0.05998237431049347\n",
      "Epoch 14521/30000 Training Loss: 0.04439464211463928\n",
      "Epoch 14522/30000 Training Loss: 0.04103879630565643\n",
      "Epoch 14523/30000 Training Loss: 0.0426957830786705\n",
      "Epoch 14524/30000 Training Loss: 0.04923095554113388\n",
      "Epoch 14525/30000 Training Loss: 0.04291408509016037\n",
      "Epoch 14526/30000 Training Loss: 0.042818184942007065\n",
      "Epoch 14527/30000 Training Loss: 0.041947174817323685\n",
      "Epoch 14528/30000 Training Loss: 0.03332735598087311\n",
      "Epoch 14529/30000 Training Loss: 0.03748302161693573\n",
      "Epoch 14530/30000 Training Loss: 0.045245636254549026\n",
      "Epoch 14531/30000 Training Loss: 0.034694649279117584\n",
      "Epoch 14532/30000 Training Loss: 0.0436268150806427\n",
      "Epoch 14533/30000 Training Loss: 0.05361233651638031\n",
      "Epoch 14534/30000 Training Loss: 0.0489979013800621\n",
      "Epoch 14535/30000 Training Loss: 0.0580107644200325\n",
      "Epoch 14536/30000 Training Loss: 0.044410690665245056\n",
      "Epoch 14537/30000 Training Loss: 0.03585716336965561\n",
      "Epoch 14538/30000 Training Loss: 0.04791152477264404\n",
      "Epoch 14539/30000 Training Loss: 0.045597225427627563\n",
      "Epoch 14540/30000 Training Loss: 0.05162655562162399\n",
      "Epoch 14541/30000 Training Loss: 0.04406344145536423\n",
      "Epoch 14542/30000 Training Loss: 0.041546158492565155\n",
      "Epoch 14543/30000 Training Loss: 0.03385484963655472\n",
      "Epoch 14544/30000 Training Loss: 0.05617547780275345\n",
      "Epoch 14545/30000 Training Loss: 0.04459870606660843\n",
      "Epoch 14546/30000 Training Loss: 0.050413671880960464\n",
      "Epoch 14547/30000 Training Loss: 0.03569653257727623\n",
      "Epoch 14548/30000 Training Loss: 0.03821913152933121\n",
      "Epoch 14549/30000 Training Loss: 0.0581793449819088\n",
      "Epoch 14550/30000 Training Loss: 0.047674935311079025\n",
      "Epoch 14551/30000 Training Loss: 0.04004708677530289\n",
      "Epoch 14552/30000 Training Loss: 0.05408336594700813\n",
      "Epoch 14553/30000 Training Loss: 0.04959145188331604\n",
      "Epoch 14554/30000 Training Loss: 0.0456651970744133\n",
      "Epoch 14555/30000 Training Loss: 0.047944389283657074\n",
      "Epoch 14556/30000 Training Loss: 0.04828888177871704\n",
      "Epoch 14557/30000 Training Loss: 0.06391686946153641\n",
      "Epoch 14558/30000 Training Loss: 0.03842977434396744\n",
      "Epoch 14559/30000 Training Loss: 0.05460258200764656\n",
      "Epoch 14560/30000 Training Loss: 0.04475996643304825\n",
      "Epoch 14561/30000 Training Loss: 0.031132761389017105\n",
      "Epoch 14562/30000 Training Loss: 0.03254840895533562\n",
      "Epoch 14563/30000 Training Loss: 0.05003390088677406\n",
      "Epoch 14564/30000 Training Loss: 0.04167520999908447\n",
      "Epoch 14565/30000 Training Loss: 0.037960540503263474\n",
      "Epoch 14566/30000 Training Loss: 0.035880256444215775\n",
      "Epoch 14567/30000 Training Loss: 0.033938951790332794\n",
      "Epoch 14568/30000 Training Loss: 0.03898286446928978\n",
      "Epoch 14569/30000 Training Loss: 0.04516821354627609\n",
      "Epoch 14570/30000 Training Loss: 0.04026342183351517\n",
      "Epoch 14571/30000 Training Loss: 0.03473237156867981\n",
      "Epoch 14572/30000 Training Loss: 0.042311713099479675\n",
      "Epoch 14573/30000 Training Loss: 0.03751835227012634\n",
      "Epoch 14574/30000 Training Loss: 0.04296807944774628\n",
      "Epoch 14575/30000 Training Loss: 0.05830073729157448\n",
      "Epoch 14576/30000 Training Loss: 0.06441493332386017\n",
      "Epoch 14577/30000 Training Loss: 0.05446640029549599\n",
      "Epoch 14578/30000 Training Loss: 0.0473225861787796\n",
      "Epoch 14579/30000 Training Loss: 0.04799528792500496\n",
      "Epoch 14580/30000 Training Loss: 0.05457858741283417\n",
      "Epoch 14581/30000 Training Loss: 0.03212904930114746\n",
      "Epoch 14582/30000 Training Loss: 0.049946099519729614\n",
      "Epoch 14583/30000 Training Loss: 0.04243379831314087\n",
      "Epoch 14584/30000 Training Loss: 0.04874279350042343\n",
      "Epoch 14585/30000 Training Loss: 0.046361394226551056\n",
      "Epoch 14586/30000 Training Loss: 0.038633961230516434\n",
      "Epoch 14587/30000 Training Loss: 0.04432596266269684\n",
      "Epoch 14588/30000 Training Loss: 0.04902603477239609\n",
      "Epoch 14589/30000 Training Loss: 0.04896270111203194\n",
      "Epoch 14590/30000 Training Loss: 0.04338833689689636\n",
      "Epoch 14591/30000 Training Loss: 0.05034437030553818\n",
      "Epoch 14592/30000 Training Loss: 0.05412449315190315\n",
      "Epoch 14593/30000 Training Loss: 0.04423604905605316\n",
      "Epoch 14594/30000 Training Loss: 0.04124533385038376\n",
      "Epoch 14595/30000 Training Loss: 0.03129148483276367\n",
      "Epoch 14596/30000 Training Loss: 0.0760778933763504\n",
      "Epoch 14597/30000 Training Loss: 0.05291169509291649\n",
      "Epoch 14598/30000 Training Loss: 0.046967774629592896\n",
      "Epoch 14599/30000 Training Loss: 0.05614916980266571\n",
      "Epoch 14600/30000 Training Loss: 0.04018701612949371\n",
      "Epoch 14600/30000 Validation Loss: 0.061569586396217346\n",
      "Epoch 14601/30000 Training Loss: 0.03969089686870575\n",
      "Epoch 14602/30000 Training Loss: 0.04315654933452606\n",
      "Epoch 14603/30000 Training Loss: 0.044925205409526825\n",
      "Epoch 14604/30000 Training Loss: 0.055225711315870285\n",
      "Epoch 14605/30000 Training Loss: 0.04153870791196823\n",
      "Epoch 14606/30000 Training Loss: 0.04330458492040634\n",
      "Epoch 14607/30000 Training Loss: 0.04844179376959801\n",
      "Epoch 14608/30000 Training Loss: 0.04758109897375107\n",
      "Epoch 14609/30000 Training Loss: 0.04532349482178688\n",
      "Epoch 14610/30000 Training Loss: 0.04719483107328415\n",
      "Epoch 14611/30000 Training Loss: 0.06502237915992737\n",
      "Epoch 14612/30000 Training Loss: 0.04329986870288849\n",
      "Epoch 14613/30000 Training Loss: 0.0401020273566246\n",
      "Epoch 14614/30000 Training Loss: 0.03913891315460205\n",
      "Epoch 14615/30000 Training Loss: 0.04242796450853348\n",
      "Epoch 14616/30000 Training Loss: 0.04571126773953438\n",
      "Epoch 14617/30000 Training Loss: 0.0570465512573719\n",
      "Epoch 14618/30000 Training Loss: 0.055727407336235046\n",
      "Epoch 14619/30000 Training Loss: 0.03742540627717972\n",
      "Epoch 14620/30000 Training Loss: 0.04070032760500908\n",
      "Epoch 14621/30000 Training Loss: 0.050815075635910034\n",
      "Epoch 14622/30000 Training Loss: 0.051628343760967255\n",
      "Epoch 14623/30000 Training Loss: 0.06206158548593521\n",
      "Epoch 14624/30000 Training Loss: 0.048417530953884125\n",
      "Epoch 14625/30000 Training Loss: 0.04951195791363716\n",
      "Epoch 14626/30000 Training Loss: 0.03592849150300026\n",
      "Epoch 14627/30000 Training Loss: 0.05623474717140198\n",
      "Epoch 14628/30000 Training Loss: 0.032948896288871765\n",
      "Epoch 14629/30000 Training Loss: 0.05096154287457466\n",
      "Epoch 14630/30000 Training Loss: 0.050809286534786224\n",
      "Epoch 14631/30000 Training Loss: 0.047093577682971954\n",
      "Epoch 14632/30000 Training Loss: 0.0541045218706131\n",
      "Epoch 14633/30000 Training Loss: 0.04193294048309326\n",
      "Epoch 14634/30000 Training Loss: 0.039723001420497894\n",
      "Epoch 14635/30000 Training Loss: 0.0516430139541626\n",
      "Epoch 14636/30000 Training Loss: 0.04227592051029205\n",
      "Epoch 14637/30000 Training Loss: 0.055018432438373566\n",
      "Epoch 14638/30000 Training Loss: 0.045388828963041306\n",
      "Epoch 14639/30000 Training Loss: 0.04135642945766449\n",
      "Epoch 14640/30000 Training Loss: 0.04338187724351883\n",
      "Epoch 14641/30000 Training Loss: 0.03863050788640976\n",
      "Epoch 14642/30000 Training Loss: 0.05695158988237381\n",
      "Epoch 14643/30000 Training Loss: 0.044312234967947006\n",
      "Epoch 14644/30000 Training Loss: 0.036671802401542664\n",
      "Epoch 14645/30000 Training Loss: 0.04416564479470253\n",
      "Epoch 14646/30000 Training Loss: 0.03928340598940849\n",
      "Epoch 14647/30000 Training Loss: 0.032548777759075165\n",
      "Epoch 14648/30000 Training Loss: 0.03772953152656555\n",
      "Epoch 14649/30000 Training Loss: 0.032044146209955215\n",
      "Epoch 14650/30000 Training Loss: 0.039767369627952576\n",
      "Epoch 14651/30000 Training Loss: 0.03789360448718071\n",
      "Epoch 14652/30000 Training Loss: 0.03568366914987564\n",
      "Epoch 14653/30000 Training Loss: 0.05029398947954178\n",
      "Epoch 14654/30000 Training Loss: 0.038944412022829056\n",
      "Epoch 14655/30000 Training Loss: 0.04218972474336624\n",
      "Epoch 14656/30000 Training Loss: 0.032703645527362823\n",
      "Epoch 14657/30000 Training Loss: 0.048156723380088806\n",
      "Epoch 14658/30000 Training Loss: 0.039000049233436584\n",
      "Epoch 14659/30000 Training Loss: 0.03866197541356087\n",
      "Epoch 14660/30000 Training Loss: 0.054002467542886734\n",
      "Epoch 14661/30000 Training Loss: 0.04869047552347183\n",
      "Epoch 14662/30000 Training Loss: 0.03810097277164459\n",
      "Epoch 14663/30000 Training Loss: 0.041016511619091034\n",
      "Epoch 14664/30000 Training Loss: 0.06117015331983566\n",
      "Epoch 14665/30000 Training Loss: 0.044351376593112946\n",
      "Epoch 14666/30000 Training Loss: 0.043764181435108185\n",
      "Epoch 14667/30000 Training Loss: 0.038427017629146576\n",
      "Epoch 14668/30000 Training Loss: 0.049272578209638596\n",
      "Epoch 14669/30000 Training Loss: 0.037694402039051056\n",
      "Epoch 14670/30000 Training Loss: 0.04265519976615906\n",
      "Epoch 14671/30000 Training Loss: 0.04143998771905899\n",
      "Epoch 14672/30000 Training Loss: 0.04029502719640732\n",
      "Epoch 14673/30000 Training Loss: 0.05398242175579071\n",
      "Epoch 14674/30000 Training Loss: 0.04687322676181793\n",
      "Epoch 14675/30000 Training Loss: 0.06306849420070648\n",
      "Epoch 14676/30000 Training Loss: 0.03389618173241615\n",
      "Epoch 14677/30000 Training Loss: 0.04457852616906166\n",
      "Epoch 14678/30000 Training Loss: 0.04580634832382202\n",
      "Epoch 14679/30000 Training Loss: 0.03717625513672829\n",
      "Epoch 14680/30000 Training Loss: 0.040939316153526306\n",
      "Epoch 14681/30000 Training Loss: 0.039371952414512634\n",
      "Epoch 14682/30000 Training Loss: 0.03968930244445801\n",
      "Epoch 14683/30000 Training Loss: 0.03992252051830292\n",
      "Epoch 14684/30000 Training Loss: 0.0530807189643383\n",
      "Epoch 14685/30000 Training Loss: 0.04462350532412529\n",
      "Epoch 14686/30000 Training Loss: 0.04745207726955414\n",
      "Epoch 14687/30000 Training Loss: 0.07621604204177856\n",
      "Epoch 14688/30000 Training Loss: 0.03874637559056282\n",
      "Epoch 14689/30000 Training Loss: 0.03280042111873627\n",
      "Epoch 14690/30000 Training Loss: 0.0461173951625824\n",
      "Epoch 14691/30000 Training Loss: 0.05656623840332031\n",
      "Epoch 14692/30000 Training Loss: 0.06358693540096283\n",
      "Epoch 14693/30000 Training Loss: 0.03886054828763008\n",
      "Epoch 14694/30000 Training Loss: 0.047550760209560394\n",
      "Epoch 14695/30000 Training Loss: 0.03459266573190689\n",
      "Epoch 14696/30000 Training Loss: 0.06465010344982147\n",
      "Epoch 14697/30000 Training Loss: 0.040798939764499664\n",
      "Epoch 14698/30000 Training Loss: 0.041756272315979004\n",
      "Epoch 14699/30000 Training Loss: 0.05041925236582756\n",
      "Epoch 14700/30000 Training Loss: 0.03620637208223343\n",
      "Epoch 14700/30000 Validation Loss: 0.04550090432167053\n",
      "Epoch 14701/30000 Training Loss: 0.049596332013607025\n",
      "Epoch 14702/30000 Training Loss: 0.03974146023392677\n",
      "Epoch 14703/30000 Training Loss: 0.04804088920354843\n",
      "Epoch 14704/30000 Training Loss: 0.04379279538989067\n",
      "Epoch 14705/30000 Training Loss: 0.05165141820907593\n",
      "Epoch 14706/30000 Training Loss: 0.037406206130981445\n",
      "Epoch 14707/30000 Training Loss: 0.05766686052083969\n",
      "Epoch 14708/30000 Training Loss: 0.05575820803642273\n",
      "Epoch 14709/30000 Training Loss: 0.04801185801625252\n",
      "Epoch 14710/30000 Training Loss: 0.054486244916915894\n",
      "Epoch 14711/30000 Training Loss: 0.04579326510429382\n",
      "Epoch 14712/30000 Training Loss: 0.05379084497690201\n",
      "Epoch 14713/30000 Training Loss: 0.05027935281395912\n",
      "Epoch 14714/30000 Training Loss: 0.03582821413874626\n",
      "Epoch 14715/30000 Training Loss: 0.04694554954767227\n",
      "Epoch 14716/30000 Training Loss: 0.040928181260824203\n",
      "Epoch 14717/30000 Training Loss: 0.03645169362425804\n",
      "Epoch 14718/30000 Training Loss: 0.05773654952645302\n",
      "Epoch 14719/30000 Training Loss: 0.05691326782107353\n",
      "Epoch 14720/30000 Training Loss: 0.036730267107486725\n",
      "Epoch 14721/30000 Training Loss: 0.03928329423069954\n",
      "Epoch 14722/30000 Training Loss: 0.041854094713926315\n",
      "Epoch 14723/30000 Training Loss: 0.03840521723031998\n",
      "Epoch 14724/30000 Training Loss: 0.03754614293575287\n",
      "Epoch 14725/30000 Training Loss: 0.03792295977473259\n",
      "Epoch 14726/30000 Training Loss: 0.04870780184864998\n",
      "Epoch 14727/30000 Training Loss: 0.039396170526742935\n",
      "Epoch 14728/30000 Training Loss: 0.05332164466381073\n",
      "Epoch 14729/30000 Training Loss: 0.038556329905986786\n",
      "Epoch 14730/30000 Training Loss: 0.0346587672829628\n",
      "Epoch 14731/30000 Training Loss: 0.031881947070360184\n",
      "Epoch 14732/30000 Training Loss: 0.05788271129131317\n",
      "Epoch 14733/30000 Training Loss: 0.04078475385904312\n",
      "Epoch 14734/30000 Training Loss: 0.06446617841720581\n",
      "Epoch 14735/30000 Training Loss: 0.0593959242105484\n",
      "Epoch 14736/30000 Training Loss: 0.044887442141771317\n",
      "Epoch 14737/30000 Training Loss: 0.03395336493849754\n",
      "Epoch 14738/30000 Training Loss: 0.03405795991420746\n",
      "Epoch 14739/30000 Training Loss: 0.04875064268708229\n",
      "Epoch 14740/30000 Training Loss: 0.05956828594207764\n",
      "Epoch 14741/30000 Training Loss: 0.04331572353839874\n",
      "Epoch 14742/30000 Training Loss: 0.03458661958575249\n",
      "Epoch 14743/30000 Training Loss: 0.0555078387260437\n",
      "Epoch 14744/30000 Training Loss: 0.04498980939388275\n",
      "Epoch 14745/30000 Training Loss: 0.041565366089344025\n",
      "Epoch 14746/30000 Training Loss: 0.05768425762653351\n",
      "Epoch 14747/30000 Training Loss: 0.048204824328422546\n",
      "Epoch 14748/30000 Training Loss: 0.047067590057849884\n",
      "Epoch 14749/30000 Training Loss: 0.061760831624269485\n",
      "Epoch 14750/30000 Training Loss: 0.04116780310869217\n",
      "Epoch 14751/30000 Training Loss: 0.038377899676561356\n",
      "Epoch 14752/30000 Training Loss: 0.03774799779057503\n",
      "Epoch 14753/30000 Training Loss: 0.04822646081447601\n",
      "Epoch 14754/30000 Training Loss: 0.04517548531293869\n",
      "Epoch 14755/30000 Training Loss: 0.03716198354959488\n",
      "Epoch 14756/30000 Training Loss: 0.04550221562385559\n",
      "Epoch 14757/30000 Training Loss: 0.04352138563990593\n",
      "Epoch 14758/30000 Training Loss: 0.04756384342908859\n",
      "Epoch 14759/30000 Training Loss: 0.04320431500673294\n",
      "Epoch 14760/30000 Training Loss: 0.05169317498803139\n",
      "Epoch 14761/30000 Training Loss: 0.042349621653556824\n",
      "Epoch 14762/30000 Training Loss: 0.04181136190891266\n",
      "Epoch 14763/30000 Training Loss: 0.03157976269721985\n",
      "Epoch 14764/30000 Training Loss: 0.04737085849046707\n",
      "Epoch 14765/30000 Training Loss: 0.0450238361954689\n",
      "Epoch 14766/30000 Training Loss: 0.05673440545797348\n",
      "Epoch 14767/30000 Training Loss: 0.04076452553272247\n",
      "Epoch 14768/30000 Training Loss: 0.047143369913101196\n",
      "Epoch 14769/30000 Training Loss: 0.048450760543346405\n",
      "Epoch 14770/30000 Training Loss: 0.03972730040550232\n",
      "Epoch 14771/30000 Training Loss: 0.04502958804368973\n",
      "Epoch 14772/30000 Training Loss: 0.052128709852695465\n",
      "Epoch 14773/30000 Training Loss: 0.043612442910671234\n",
      "Epoch 14774/30000 Training Loss: 0.044427499175071716\n",
      "Epoch 14775/30000 Training Loss: 0.0461084246635437\n",
      "Epoch 14776/30000 Training Loss: 0.04971963167190552\n",
      "Epoch 14777/30000 Training Loss: 0.03784489631652832\n",
      "Epoch 14778/30000 Training Loss: 0.03498975932598114\n",
      "Epoch 14779/30000 Training Loss: 0.039040133357048035\n",
      "Epoch 14780/30000 Training Loss: 0.051070310175418854\n",
      "Epoch 14781/30000 Training Loss: 0.05036288499832153\n",
      "Epoch 14782/30000 Training Loss: 0.051252175122499466\n",
      "Epoch 14783/30000 Training Loss: 0.037042032927274704\n",
      "Epoch 14784/30000 Training Loss: 0.05626118183135986\n",
      "Epoch 14785/30000 Training Loss: 0.03882966935634613\n",
      "Epoch 14786/30000 Training Loss: 0.05428136885166168\n",
      "Epoch 14787/30000 Training Loss: 0.05153978615999222\n",
      "Epoch 14788/30000 Training Loss: 0.05856892839074135\n",
      "Epoch 14789/30000 Training Loss: 0.054732292890548706\n",
      "Epoch 14790/30000 Training Loss: 0.04208826273679733\n",
      "Epoch 14791/30000 Training Loss: 0.04174521937966347\n",
      "Epoch 14792/30000 Training Loss: 0.04185227304697037\n",
      "Epoch 14793/30000 Training Loss: 0.03761756420135498\n",
      "Epoch 14794/30000 Training Loss: 0.05074441432952881\n",
      "Epoch 14795/30000 Training Loss: 0.05510173738002777\n",
      "Epoch 14796/30000 Training Loss: 0.05504601076245308\n",
      "Epoch 14797/30000 Training Loss: 0.04966405779123306\n",
      "Epoch 14798/30000 Training Loss: 0.04214954376220703\n",
      "Epoch 14799/30000 Training Loss: 0.05865291506052017\n",
      "Epoch 14800/30000 Training Loss: 0.05091020464897156\n",
      "Epoch 14800/30000 Validation Loss: 0.041443370282649994\n",
      "Epoch 14801/30000 Training Loss: 0.053597405552864075\n",
      "Epoch 14802/30000 Training Loss: 0.04970717802643776\n",
      "Epoch 14803/30000 Training Loss: 0.039879195392131805\n",
      "Epoch 14804/30000 Training Loss: 0.05839822441339493\n",
      "Epoch 14805/30000 Training Loss: 0.05519251525402069\n",
      "Epoch 14806/30000 Training Loss: 0.050077326595783234\n",
      "Epoch 14807/30000 Training Loss: 0.033366963267326355\n",
      "Epoch 14808/30000 Training Loss: 0.03892176225781441\n",
      "Epoch 14809/30000 Training Loss: 0.04643755406141281\n",
      "Epoch 14810/30000 Training Loss: 0.0488298237323761\n",
      "Epoch 14811/30000 Training Loss: 0.042353738099336624\n",
      "Epoch 14812/30000 Training Loss: 0.0431450679898262\n",
      "Epoch 14813/30000 Training Loss: 0.04642388969659805\n",
      "Epoch 14814/30000 Training Loss: 0.06487387418746948\n",
      "Epoch 14815/30000 Training Loss: 0.03498286381363869\n",
      "Epoch 14816/30000 Training Loss: 0.05488261580467224\n",
      "Epoch 14817/30000 Training Loss: 0.04205489158630371\n",
      "Epoch 14818/30000 Training Loss: 0.050946712493896484\n",
      "Epoch 14819/30000 Training Loss: 0.0456119105219841\n",
      "Epoch 14820/30000 Training Loss: 0.05212943255901337\n",
      "Epoch 14821/30000 Training Loss: 0.04519886523485184\n",
      "Epoch 14822/30000 Training Loss: 0.04059161990880966\n",
      "Epoch 14823/30000 Training Loss: 0.06408338993787766\n",
      "Epoch 14824/30000 Training Loss: 0.04252159222960472\n",
      "Epoch 14825/30000 Training Loss: 0.03867926076054573\n",
      "Epoch 14826/30000 Training Loss: 0.054403625428676605\n",
      "Epoch 14827/30000 Training Loss: 0.05362436920404434\n",
      "Epoch 14828/30000 Training Loss: 0.04239243268966675\n",
      "Epoch 14829/30000 Training Loss: 0.05203813314437866\n",
      "Epoch 14830/30000 Training Loss: 0.0386962890625\n",
      "Epoch 14831/30000 Training Loss: 0.03920646011829376\n",
      "Epoch 14832/30000 Training Loss: 0.05246282368898392\n",
      "Epoch 14833/30000 Training Loss: 0.0658135786652565\n",
      "Epoch 14834/30000 Training Loss: 0.0572793185710907\n",
      "Epoch 14835/30000 Training Loss: 0.03581535443663597\n",
      "Epoch 14836/30000 Training Loss: 0.05109579861164093\n",
      "Epoch 14837/30000 Training Loss: 0.06251876056194305\n",
      "Epoch 14838/30000 Training Loss: 0.04143209010362625\n",
      "Epoch 14839/30000 Training Loss: 0.04563656449317932\n",
      "Epoch 14840/30000 Training Loss: 0.03727632015943527\n",
      "Epoch 14841/30000 Training Loss: 0.0435238741338253\n",
      "Epoch 14842/30000 Training Loss: 0.033073149621486664\n",
      "Epoch 14843/30000 Training Loss: 0.0445236898958683\n",
      "Epoch 14844/30000 Training Loss: 0.03952401131391525\n",
      "Epoch 14845/30000 Training Loss: 0.05295886471867561\n",
      "Epoch 14846/30000 Training Loss: 0.04076690226793289\n",
      "Epoch 14847/30000 Training Loss: 0.0441611222922802\n",
      "Epoch 14848/30000 Training Loss: 0.04110771045088768\n",
      "Epoch 14849/30000 Training Loss: 0.05434839427471161\n",
      "Epoch 14850/30000 Training Loss: 0.03723280131816864\n",
      "Epoch 14851/30000 Training Loss: 0.040375661104917526\n",
      "Epoch 14852/30000 Training Loss: 0.047499895095825195\n",
      "Epoch 14853/30000 Training Loss: 0.03750355914235115\n",
      "Epoch 14854/30000 Training Loss: 0.05538696050643921\n",
      "Epoch 14855/30000 Training Loss: 0.04636890068650246\n",
      "Epoch 14856/30000 Training Loss: 0.051577046513557434\n",
      "Epoch 14857/30000 Training Loss: 0.04624275118112564\n",
      "Epoch 14858/30000 Training Loss: 0.05514201521873474\n",
      "Epoch 14859/30000 Training Loss: 0.042375609278678894\n",
      "Epoch 14860/30000 Training Loss: 0.05416455119848251\n",
      "Epoch 14861/30000 Training Loss: 0.04965243861079216\n",
      "Epoch 14862/30000 Training Loss: 0.037452735006809235\n",
      "Epoch 14863/30000 Training Loss: 0.06498440355062485\n",
      "Epoch 14864/30000 Training Loss: 0.05773752182722092\n",
      "Epoch 14865/30000 Training Loss: 0.043653737753629684\n",
      "Epoch 14866/30000 Training Loss: 0.04421263933181763\n",
      "Epoch 14867/30000 Training Loss: 0.04290689900517464\n",
      "Epoch 14868/30000 Training Loss: 0.04372835531830788\n",
      "Epoch 14869/30000 Training Loss: 0.037244949489831924\n",
      "Epoch 14870/30000 Training Loss: 0.04132957383990288\n",
      "Epoch 14871/30000 Training Loss: 0.043400488793849945\n",
      "Epoch 14872/30000 Training Loss: 0.048493001610040665\n",
      "Epoch 14873/30000 Training Loss: 0.0590733177959919\n",
      "Epoch 14874/30000 Training Loss: 0.04837144911289215\n",
      "Epoch 14875/30000 Training Loss: 0.0640426054596901\n",
      "Epoch 14876/30000 Training Loss: 0.03834376856684685\n",
      "Epoch 14877/30000 Training Loss: 0.04607401788234711\n",
      "Epoch 14878/30000 Training Loss: 0.04212719947099686\n",
      "Epoch 14879/30000 Training Loss: 0.05854702740907669\n",
      "Epoch 14880/30000 Training Loss: 0.04479227960109711\n",
      "Epoch 14881/30000 Training Loss: 0.04023897647857666\n",
      "Epoch 14882/30000 Training Loss: 0.04460107907652855\n",
      "Epoch 14883/30000 Training Loss: 0.0366373211145401\n",
      "Epoch 14884/30000 Training Loss: 0.03430339694023132\n",
      "Epoch 14885/30000 Training Loss: 0.0490240678191185\n",
      "Epoch 14886/30000 Training Loss: 0.0442839078605175\n",
      "Epoch 14887/30000 Training Loss: 0.049773089587688446\n",
      "Epoch 14888/30000 Training Loss: 0.04807643964886665\n",
      "Epoch 14889/30000 Training Loss: 0.050310201942920685\n",
      "Epoch 14890/30000 Training Loss: 0.050278276205062866\n",
      "Epoch 14891/30000 Training Loss: 0.03708098828792572\n",
      "Epoch 14892/30000 Training Loss: 0.04526747763156891\n",
      "Epoch 14893/30000 Training Loss: 0.0639059990644455\n",
      "Epoch 14894/30000 Training Loss: 0.03627064451575279\n",
      "Epoch 14895/30000 Training Loss: 0.04294256865978241\n",
      "Epoch 14896/30000 Training Loss: 0.0467425212264061\n",
      "Epoch 14897/30000 Training Loss: 0.02881757915019989\n",
      "Epoch 14898/30000 Training Loss: 0.05450993403792381\n",
      "Epoch 14899/30000 Training Loss: 0.03798547387123108\n",
      "Epoch 14900/30000 Training Loss: 0.04743863642215729\n",
      "Epoch 14900/30000 Validation Loss: 0.048322632908821106\n",
      "Epoch 14901/30000 Training Loss: 0.04358886554837227\n",
      "Epoch 14902/30000 Training Loss: 0.03238992393016815\n",
      "Epoch 14903/30000 Training Loss: 0.032746363431215286\n",
      "Epoch 14904/30000 Training Loss: 0.04681289941072464\n",
      "Epoch 14905/30000 Training Loss: 0.04441370069980621\n",
      "Epoch 14906/30000 Training Loss: 0.060708172619342804\n",
      "Epoch 14907/30000 Training Loss: 0.046396441757678986\n",
      "Epoch 14908/30000 Training Loss: 0.04128492623567581\n",
      "Epoch 14909/30000 Training Loss: 0.05968210846185684\n",
      "Epoch 14910/30000 Training Loss: 0.04212205111980438\n",
      "Epoch 14911/30000 Training Loss: 0.042904339730739594\n",
      "Epoch 14912/30000 Training Loss: 0.05383313074707985\n",
      "Epoch 14913/30000 Training Loss: 0.041182637214660645\n",
      "Epoch 14914/30000 Training Loss: 0.036325231194496155\n",
      "Epoch 14915/30000 Training Loss: 0.040874991565942764\n",
      "Epoch 14916/30000 Training Loss: 0.03808104246854782\n",
      "Epoch 14917/30000 Training Loss: 0.0449691042304039\n",
      "Epoch 14918/30000 Training Loss: 0.036495208740234375\n",
      "Epoch 14919/30000 Training Loss: 0.04411168769001961\n",
      "Epoch 14920/30000 Training Loss: 0.056675221771001816\n",
      "Epoch 14921/30000 Training Loss: 0.040895625948905945\n",
      "Epoch 14922/30000 Training Loss: 0.04037744551897049\n",
      "Epoch 14923/30000 Training Loss: 0.05079835653305054\n",
      "Epoch 14924/30000 Training Loss: 0.03422055393457413\n",
      "Epoch 14925/30000 Training Loss: 0.03535570949316025\n",
      "Epoch 14926/30000 Training Loss: 0.04814743995666504\n",
      "Epoch 14927/30000 Training Loss: 0.0530984029173851\n",
      "Epoch 14928/30000 Training Loss: 0.04989921301603317\n",
      "Epoch 14929/30000 Training Loss: 0.041457220911979675\n",
      "Epoch 14930/30000 Training Loss: 0.04579731449484825\n",
      "Epoch 14931/30000 Training Loss: 0.037874605506658554\n",
      "Epoch 14932/30000 Training Loss: 0.03035440668463707\n",
      "Epoch 14933/30000 Training Loss: 0.04919290542602539\n",
      "Epoch 14934/30000 Training Loss: 0.04219202697277069\n",
      "Epoch 14935/30000 Training Loss: 0.04043307155370712\n",
      "Epoch 14936/30000 Training Loss: 0.06399533152580261\n",
      "Epoch 14937/30000 Training Loss: 0.04545648396015167\n",
      "Epoch 14938/30000 Training Loss: 0.056910689920186996\n",
      "Epoch 14939/30000 Training Loss: 0.05289137363433838\n",
      "Epoch 14940/30000 Training Loss: 0.04799438267946243\n",
      "Epoch 14941/30000 Training Loss: 0.04915511608123779\n",
      "Epoch 14942/30000 Training Loss: 0.0434923991560936\n",
      "Epoch 14943/30000 Training Loss: 0.05231233686208725\n",
      "Epoch 14944/30000 Training Loss: 0.04906390234827995\n",
      "Epoch 14945/30000 Training Loss: 0.041952505707740784\n",
      "Epoch 14946/30000 Training Loss: 0.040434274822473526\n",
      "Epoch 14947/30000 Training Loss: 0.03563544526696205\n",
      "Epoch 14948/30000 Training Loss: 0.037884730845689774\n",
      "Epoch 14949/30000 Training Loss: 0.048034392297267914\n",
      "Epoch 14950/30000 Training Loss: 0.038413889706134796\n",
      "Epoch 14951/30000 Training Loss: 0.05806683748960495\n",
      "Epoch 14952/30000 Training Loss: 0.041105832904577255\n",
      "Epoch 14953/30000 Training Loss: 0.042923200875520706\n",
      "Epoch 14954/30000 Training Loss: 0.059929657727479935\n",
      "Epoch 14955/30000 Training Loss: 0.04580122232437134\n",
      "Epoch 14956/30000 Training Loss: 0.041735198348760605\n",
      "Epoch 14957/30000 Training Loss: 0.050264306366443634\n",
      "Epoch 14958/30000 Training Loss: 0.04398646205663681\n",
      "Epoch 14959/30000 Training Loss: 0.044232483953237534\n",
      "Epoch 14960/30000 Training Loss: 0.04690627008676529\n",
      "Epoch 14961/30000 Training Loss: 0.04176594689488411\n",
      "Epoch 14962/30000 Training Loss: 0.05617240443825722\n",
      "Epoch 14963/30000 Training Loss: 0.04837924242019653\n",
      "Epoch 14964/30000 Training Loss: 0.03204067423939705\n",
      "Epoch 14965/30000 Training Loss: 0.04005245119333267\n",
      "Epoch 14966/30000 Training Loss: 0.037328656762838364\n",
      "Epoch 14967/30000 Training Loss: 0.05902145057916641\n",
      "Epoch 14968/30000 Training Loss: 0.048645079135894775\n",
      "Epoch 14969/30000 Training Loss: 0.0440952330827713\n",
      "Epoch 14970/30000 Training Loss: 0.03955204039812088\n",
      "Epoch 14971/30000 Training Loss: 0.05091064050793648\n",
      "Epoch 14972/30000 Training Loss: 0.041385773569345474\n",
      "Epoch 14973/30000 Training Loss: 0.04073939099907875\n",
      "Epoch 14974/30000 Training Loss: 0.052534446120262146\n",
      "Epoch 14975/30000 Training Loss: 0.035616565495729446\n",
      "Epoch 14976/30000 Training Loss: 0.0447169654071331\n",
      "Epoch 14977/30000 Training Loss: 0.050601597875356674\n",
      "Epoch 14978/30000 Training Loss: 0.04885781183838844\n",
      "Epoch 14979/30000 Training Loss: 0.044566597789525986\n",
      "Epoch 14980/30000 Training Loss: 0.06125786527991295\n",
      "Epoch 14981/30000 Training Loss: 0.04719148948788643\n",
      "Epoch 14982/30000 Training Loss: 0.051641061902046204\n",
      "Epoch 14983/30000 Training Loss: 0.05175374448299408\n",
      "Epoch 14984/30000 Training Loss: 0.042178407311439514\n",
      "Epoch 14985/30000 Training Loss: 0.05556066706776619\n",
      "Epoch 14986/30000 Training Loss: 0.03707116097211838\n",
      "Epoch 14987/30000 Training Loss: 0.03741605952382088\n",
      "Epoch 14988/30000 Training Loss: 0.05785978585481644\n",
      "Epoch 14989/30000 Training Loss: 0.044036492705345154\n",
      "Epoch 14990/30000 Training Loss: 0.034488581120967865\n",
      "Epoch 14991/30000 Training Loss: 0.04348890483379364\n",
      "Epoch 14992/30000 Training Loss: 0.040810611099004745\n",
      "Epoch 14993/30000 Training Loss: 0.04612261801958084\n",
      "Epoch 14994/30000 Training Loss: 0.055082038044929504\n",
      "Epoch 14995/30000 Training Loss: 0.050137028098106384\n",
      "Epoch 14996/30000 Training Loss: 0.03850572556257248\n",
      "Epoch 14997/30000 Training Loss: 0.04344560578465462\n",
      "Epoch 14998/30000 Training Loss: 0.03977997601032257\n",
      "Epoch 14999/30000 Training Loss: 0.055848367512226105\n",
      "Epoch 15000/30000 Training Loss: 0.042594004422426224\n",
      "Epoch 15000/30000 Validation Loss: 0.04079437628388405\n",
      "Epoch 15001/30000 Training Loss: 0.05271810665726662\n",
      "Epoch 15002/30000 Training Loss: 0.04115106537938118\n",
      "Epoch 15003/30000 Training Loss: 0.0350504145026207\n",
      "Epoch 15004/30000 Training Loss: 0.051995594054460526\n",
      "Epoch 15005/30000 Training Loss: 0.05270495265722275\n",
      "Epoch 15006/30000 Training Loss: 0.05192131549119949\n",
      "Epoch 15007/30000 Training Loss: 0.053323276340961456\n",
      "Epoch 15008/30000 Training Loss: 0.036831147968769073\n",
      "Epoch 15009/30000 Training Loss: 0.04848968982696533\n",
      "Epoch 15010/30000 Training Loss: 0.04545901343226433\n",
      "Epoch 15011/30000 Training Loss: 0.04262387752532959\n",
      "Epoch 15012/30000 Training Loss: 0.034501589834690094\n",
      "Epoch 15013/30000 Training Loss: 0.040481485426425934\n",
      "Epoch 15014/30000 Training Loss: 0.04381255805492401\n",
      "Epoch 15015/30000 Training Loss: 0.04856068640947342\n",
      "Epoch 15016/30000 Training Loss: 0.03964342176914215\n",
      "Epoch 15017/30000 Training Loss: 0.05365550518035889\n",
      "Epoch 15018/30000 Training Loss: 0.05207209289073944\n",
      "Epoch 15019/30000 Training Loss: 0.047960035502910614\n",
      "Epoch 15020/30000 Training Loss: 0.0422448068857193\n",
      "Epoch 15021/30000 Training Loss: 0.038963682949543\n",
      "Epoch 15022/30000 Training Loss: 0.04477238655090332\n",
      "Epoch 15023/30000 Training Loss: 0.05236505717039108\n",
      "Epoch 15024/30000 Training Loss: 0.04508044943213463\n",
      "Epoch 15025/30000 Training Loss: 0.03878248482942581\n",
      "Epoch 15026/30000 Training Loss: 0.0685492530465126\n",
      "Epoch 15027/30000 Training Loss: 0.04111200571060181\n",
      "Epoch 15028/30000 Training Loss: 0.03568952530622482\n",
      "Epoch 15029/30000 Training Loss: 0.04063621908426285\n",
      "Epoch 15030/30000 Training Loss: 0.05681998282670975\n",
      "Epoch 15031/30000 Training Loss: 0.038306377828121185\n",
      "Epoch 15032/30000 Training Loss: 0.045654334127902985\n",
      "Epoch 15033/30000 Training Loss: 0.038145385682582855\n",
      "Epoch 15034/30000 Training Loss: 0.04110604524612427\n",
      "Epoch 15035/30000 Training Loss: 0.04471305012702942\n",
      "Epoch 15036/30000 Training Loss: 0.03620985522866249\n",
      "Epoch 15037/30000 Training Loss: 0.04216695949435234\n",
      "Epoch 15038/30000 Training Loss: 0.04147544875741005\n",
      "Epoch 15039/30000 Training Loss: 0.06262864917516708\n",
      "Epoch 15040/30000 Training Loss: 0.039631832391023636\n",
      "Epoch 15041/30000 Training Loss: 0.049741242080926895\n",
      "Epoch 15042/30000 Training Loss: 0.04536578059196472\n",
      "Epoch 15043/30000 Training Loss: 0.04786491394042969\n",
      "Epoch 15044/30000 Training Loss: 0.05756800249218941\n",
      "Epoch 15045/30000 Training Loss: 0.048548053950071335\n",
      "Epoch 15046/30000 Training Loss: 0.03220982104539871\n",
      "Epoch 15047/30000 Training Loss: 0.03771406412124634\n",
      "Epoch 15048/30000 Training Loss: 0.04432211443781853\n",
      "Epoch 15049/30000 Training Loss: 0.04714910686016083\n",
      "Epoch 15050/30000 Training Loss: 0.042471565306186676\n",
      "Epoch 15051/30000 Training Loss: 0.04172145947813988\n",
      "Epoch 15052/30000 Training Loss: 0.04406009986996651\n",
      "Epoch 15053/30000 Training Loss: 0.04794708639383316\n",
      "Epoch 15054/30000 Training Loss: 0.044845081865787506\n",
      "Epoch 15055/30000 Training Loss: 0.06325732916593552\n",
      "Epoch 15056/30000 Training Loss: 0.058432437479496\n",
      "Epoch 15057/30000 Training Loss: 0.035031259059906006\n",
      "Epoch 15058/30000 Training Loss: 0.04807698726654053\n",
      "Epoch 15059/30000 Training Loss: 0.05211902782320976\n",
      "Epoch 15060/30000 Training Loss: 0.05703694373369217\n",
      "Epoch 15061/30000 Training Loss: 0.04578913748264313\n",
      "Epoch 15062/30000 Training Loss: 0.04071437567472458\n",
      "Epoch 15063/30000 Training Loss: 0.04085163772106171\n",
      "Epoch 15064/30000 Training Loss: 0.0519561730325222\n",
      "Epoch 15065/30000 Training Loss: 0.045431409031152725\n",
      "Epoch 15066/30000 Training Loss: 0.0456046536564827\n",
      "Epoch 15067/30000 Training Loss: 0.039748284965753555\n",
      "Epoch 15068/30000 Training Loss: 0.04806627333164215\n",
      "Epoch 15069/30000 Training Loss: 0.06348392367362976\n",
      "Epoch 15070/30000 Training Loss: 0.03736075758934021\n",
      "Epoch 15071/30000 Training Loss: 0.04122762382030487\n",
      "Epoch 15072/30000 Training Loss: 0.0440470352768898\n",
      "Epoch 15073/30000 Training Loss: 0.05739954859018326\n",
      "Epoch 15074/30000 Training Loss: 0.057290300726890564\n",
      "Epoch 15075/30000 Training Loss: 0.05045987665653229\n",
      "Epoch 15076/30000 Training Loss: 0.036478348076343536\n",
      "Epoch 15077/30000 Training Loss: 0.050448816269636154\n",
      "Epoch 15078/30000 Training Loss: 0.05489340052008629\n",
      "Epoch 15079/30000 Training Loss: 0.04466403275728226\n",
      "Epoch 15080/30000 Training Loss: 0.04152434319257736\n",
      "Epoch 15081/30000 Training Loss: 0.04186049848794937\n",
      "Epoch 15082/30000 Training Loss: 0.04979293793439865\n",
      "Epoch 15083/30000 Training Loss: 0.03470318019390106\n",
      "Epoch 15084/30000 Training Loss: 0.046641819179058075\n",
      "Epoch 15085/30000 Training Loss: 0.04593236744403839\n",
      "Epoch 15086/30000 Training Loss: 0.04013051465153694\n",
      "Epoch 15087/30000 Training Loss: 0.05901626497507095\n",
      "Epoch 15088/30000 Training Loss: 0.033930376172065735\n",
      "Epoch 15089/30000 Training Loss: 0.03979719430208206\n",
      "Epoch 15090/30000 Training Loss: 0.042354874312877655\n",
      "Epoch 15091/30000 Training Loss: 0.06437112390995026\n",
      "Epoch 15092/30000 Training Loss: 0.03227730095386505\n",
      "Epoch 15093/30000 Training Loss: 0.035032760351896286\n",
      "Epoch 15094/30000 Training Loss: 0.05246644467115402\n",
      "Epoch 15095/30000 Training Loss: 0.043762777000665665\n",
      "Epoch 15096/30000 Training Loss: 0.05132167041301727\n",
      "Epoch 15097/30000 Training Loss: 0.04333873838186264\n",
      "Epoch 15098/30000 Training Loss: 0.04814334213733673\n",
      "Epoch 15099/30000 Training Loss: 0.031777333468198776\n",
      "Epoch 15100/30000 Training Loss: 0.0352143868803978\n",
      "Epoch 15100/30000 Validation Loss: 0.0599873885512352\n",
      "Epoch 15101/30000 Training Loss: 0.04159197956323624\n",
      "Epoch 15102/30000 Training Loss: 0.042894236743450165\n",
      "Epoch 15103/30000 Training Loss: 0.0538436621427536\n",
      "Epoch 15104/30000 Training Loss: 0.04372995346784592\n",
      "Epoch 15105/30000 Training Loss: 0.04258159175515175\n",
      "Epoch 15106/30000 Training Loss: 0.04766732454299927\n",
      "Epoch 15107/30000 Training Loss: 0.047693319618701935\n",
      "Epoch 15108/30000 Training Loss: 0.045987568795681\n",
      "Epoch 15109/30000 Training Loss: 0.06809096783399582\n",
      "Epoch 15110/30000 Training Loss: 0.04059935361146927\n",
      "Epoch 15111/30000 Training Loss: 0.04447520896792412\n",
      "Epoch 15112/30000 Training Loss: 0.03455004096031189\n",
      "Epoch 15113/30000 Training Loss: 0.052820660173892975\n",
      "Epoch 15114/30000 Training Loss: 0.043810609728097916\n",
      "Epoch 15115/30000 Training Loss: 0.04849374666810036\n",
      "Epoch 15116/30000 Training Loss: 0.043382178992033005\n",
      "Epoch 15117/30000 Training Loss: 0.06057092547416687\n",
      "Epoch 15118/30000 Training Loss: 0.04295732080936432\n",
      "Epoch 15119/30000 Training Loss: 0.032224416732788086\n",
      "Epoch 15120/30000 Training Loss: 0.043259359896183014\n",
      "Epoch 15121/30000 Training Loss: 0.0471714623272419\n",
      "Epoch 15122/30000 Training Loss: 0.050321318209171295\n",
      "Epoch 15123/30000 Training Loss: 0.04590239375829697\n",
      "Epoch 15124/30000 Training Loss: 0.05050928145647049\n",
      "Epoch 15125/30000 Training Loss: 0.05421602725982666\n",
      "Epoch 15126/30000 Training Loss: 0.05880076438188553\n",
      "Epoch 15127/30000 Training Loss: 0.03242221847176552\n",
      "Epoch 15128/30000 Training Loss: 0.037979576736688614\n",
      "Epoch 15129/30000 Training Loss: 0.05051874369382858\n",
      "Epoch 15130/30000 Training Loss: 0.045077141374349594\n",
      "Epoch 15131/30000 Training Loss: 0.04302588850259781\n",
      "Epoch 15132/30000 Training Loss: 0.04719151183962822\n",
      "Epoch 15133/30000 Training Loss: 0.05092675983905792\n",
      "Epoch 15134/30000 Training Loss: 0.03985061123967171\n",
      "Epoch 15135/30000 Training Loss: 0.03822724521160126\n",
      "Epoch 15136/30000 Training Loss: 0.046520866453647614\n",
      "Epoch 15137/30000 Training Loss: 0.04035435616970062\n",
      "Epoch 15138/30000 Training Loss: 0.06056426092982292\n",
      "Epoch 15139/30000 Training Loss: 0.038574881851673126\n",
      "Epoch 15140/30000 Training Loss: 0.037528637796640396\n",
      "Epoch 15141/30000 Training Loss: 0.0416615828871727\n",
      "Epoch 15142/30000 Training Loss: 0.042200781404972076\n",
      "Epoch 15143/30000 Training Loss: 0.05149204656481743\n",
      "Epoch 15144/30000 Training Loss: 0.03742167353630066\n",
      "Epoch 15145/30000 Training Loss: 0.04848678410053253\n",
      "Epoch 15146/30000 Training Loss: 0.037872664630413055\n",
      "Epoch 15147/30000 Training Loss: 0.04501299560070038\n",
      "Epoch 15148/30000 Training Loss: 0.04550229758024216\n",
      "Epoch 15149/30000 Training Loss: 0.0515163391828537\n",
      "Epoch 15150/30000 Training Loss: 0.05078204721212387\n",
      "Epoch 15151/30000 Training Loss: 0.04631704092025757\n",
      "Epoch 15152/30000 Training Loss: 0.03102256916463375\n",
      "Epoch 15153/30000 Training Loss: 0.03374423459172249\n",
      "Epoch 15154/30000 Training Loss: 0.04277919605374336\n",
      "Epoch 15155/30000 Training Loss: 0.040206871926784515\n",
      "Epoch 15156/30000 Training Loss: 0.04994796961545944\n",
      "Epoch 15157/30000 Training Loss: 0.04039270803332329\n",
      "Epoch 15158/30000 Training Loss: 0.04926849156618118\n",
      "Epoch 15159/30000 Training Loss: 0.04057777300477028\n",
      "Epoch 15160/30000 Training Loss: 0.05389122664928436\n",
      "Epoch 15161/30000 Training Loss: 0.05328744649887085\n",
      "Epoch 15162/30000 Training Loss: 0.04436919093132019\n",
      "Epoch 15163/30000 Training Loss: 0.04405321180820465\n",
      "Epoch 15164/30000 Training Loss: 0.03839128091931343\n",
      "Epoch 15165/30000 Training Loss: 0.0492609441280365\n",
      "Epoch 15166/30000 Training Loss: 0.05415259301662445\n",
      "Epoch 15167/30000 Training Loss: 0.03885435312986374\n",
      "Epoch 15168/30000 Training Loss: 0.046663183718919754\n",
      "Epoch 15169/30000 Training Loss: 0.05254027247428894\n",
      "Epoch 15170/30000 Training Loss: 0.054668597877025604\n",
      "Epoch 15171/30000 Training Loss: 0.042166657745838165\n",
      "Epoch 15172/30000 Training Loss: 0.03649332374334335\n",
      "Epoch 15173/30000 Training Loss: 0.04822411388158798\n",
      "Epoch 15174/30000 Training Loss: 0.055532246828079224\n",
      "Epoch 15175/30000 Training Loss: 0.04451752454042435\n",
      "Epoch 15176/30000 Training Loss: 0.04389920085668564\n",
      "Epoch 15177/30000 Training Loss: 0.036047499626874924\n",
      "Epoch 15178/30000 Training Loss: 0.043421659618616104\n",
      "Epoch 15179/30000 Training Loss: 0.04357077181339264\n",
      "Epoch 15180/30000 Training Loss: 0.031150083988904953\n",
      "Epoch 15181/30000 Training Loss: 0.05457267910242081\n",
      "Epoch 15182/30000 Training Loss: 0.05063992738723755\n",
      "Epoch 15183/30000 Training Loss: 0.0504111684858799\n",
      "Epoch 15184/30000 Training Loss: 0.05140914022922516\n",
      "Epoch 15185/30000 Training Loss: 0.04772387817502022\n",
      "Epoch 15186/30000 Training Loss: 0.043955568224191666\n",
      "Epoch 15187/30000 Training Loss: 0.050906069576740265\n",
      "Epoch 15188/30000 Training Loss: 0.037405289709568024\n",
      "Epoch 15189/30000 Training Loss: 0.05371924489736557\n",
      "Epoch 15190/30000 Training Loss: 0.05039165914058685\n",
      "Epoch 15191/30000 Training Loss: 0.06039736419916153\n",
      "Epoch 15192/30000 Training Loss: 0.040639206767082214\n",
      "Epoch 15193/30000 Training Loss: 0.04310223460197449\n",
      "Epoch 15194/30000 Training Loss: 0.05843313783407211\n",
      "Epoch 15195/30000 Training Loss: 0.05707858130335808\n",
      "Epoch 15196/30000 Training Loss: 0.058492034673690796\n",
      "Epoch 15197/30000 Training Loss: 0.06151668727397919\n",
      "Epoch 15198/30000 Training Loss: 0.04152733460068703\n",
      "Epoch 15199/30000 Training Loss: 0.04865914583206177\n",
      "Epoch 15200/30000 Training Loss: 0.03778088837862015\n",
      "Epoch 15200/30000 Validation Loss: 0.05498599261045456\n",
      "Epoch 15201/30000 Training Loss: 0.04321504384279251\n",
      "Epoch 15202/30000 Training Loss: 0.03735661879181862\n",
      "Epoch 15203/30000 Training Loss: 0.044330328702926636\n",
      "Epoch 15204/30000 Training Loss: 0.040850333869457245\n",
      "Epoch 15205/30000 Training Loss: 0.049152977764606476\n",
      "Epoch 15206/30000 Training Loss: 0.03473959118127823\n",
      "Epoch 15207/30000 Training Loss: 0.048367440700531006\n",
      "Epoch 15208/30000 Training Loss: 0.05145006254315376\n",
      "Epoch 15209/30000 Training Loss: 0.04370201379060745\n",
      "Epoch 15210/30000 Training Loss: 0.04677999019622803\n",
      "Epoch 15211/30000 Training Loss: 0.04771331697702408\n",
      "Epoch 15212/30000 Training Loss: 0.045680053532123566\n",
      "Epoch 15213/30000 Training Loss: 0.052017442882061005\n",
      "Epoch 15214/30000 Training Loss: 0.054605454206466675\n",
      "Epoch 15215/30000 Training Loss: 0.053813885897397995\n",
      "Epoch 15216/30000 Training Loss: 0.03702547401189804\n",
      "Epoch 15217/30000 Training Loss: 0.06365669518709183\n",
      "Epoch 15218/30000 Training Loss: 0.03862348943948746\n",
      "Epoch 15219/30000 Training Loss: 0.05424988269805908\n",
      "Epoch 15220/30000 Training Loss: 0.04480908811092377\n",
      "Epoch 15221/30000 Training Loss: 0.04390295222401619\n",
      "Epoch 15222/30000 Training Loss: 0.04540880024433136\n",
      "Epoch 15223/30000 Training Loss: 0.0357949323952198\n",
      "Epoch 15224/30000 Training Loss: 0.04231531172990799\n",
      "Epoch 15225/30000 Training Loss: 0.04030975326895714\n",
      "Epoch 15226/30000 Training Loss: 0.034526363015174866\n",
      "Epoch 15227/30000 Training Loss: 0.04668385908007622\n",
      "Epoch 15228/30000 Training Loss: 0.050228752195835114\n",
      "Epoch 15229/30000 Training Loss: 0.03749019652605057\n",
      "Epoch 15230/30000 Training Loss: 0.04643997550010681\n",
      "Epoch 15231/30000 Training Loss: 0.038738977164030075\n",
      "Epoch 15232/30000 Training Loss: 0.046619340777397156\n",
      "Epoch 15233/30000 Training Loss: 0.048501625657081604\n",
      "Epoch 15234/30000 Training Loss: 0.04440276324748993\n",
      "Epoch 15235/30000 Training Loss: 0.03769165277481079\n",
      "Epoch 15236/30000 Training Loss: 0.044354092329740524\n",
      "Epoch 15237/30000 Training Loss: 0.030953478068113327\n",
      "Epoch 15238/30000 Training Loss: 0.05348443239927292\n",
      "Epoch 15239/30000 Training Loss: 0.04378775507211685\n",
      "Epoch 15240/30000 Training Loss: 0.05366305634379387\n",
      "Epoch 15241/30000 Training Loss: 0.037091214209795\n",
      "Epoch 15242/30000 Training Loss: 0.04410025477409363\n",
      "Epoch 15243/30000 Training Loss: 0.045573167502880096\n",
      "Epoch 15244/30000 Training Loss: 0.04942311346530914\n",
      "Epoch 15245/30000 Training Loss: 0.053326625376939774\n",
      "Epoch 15246/30000 Training Loss: 0.03902249038219452\n",
      "Epoch 15247/30000 Training Loss: 0.04372711852192879\n",
      "Epoch 15248/30000 Training Loss: 0.038415465503931046\n",
      "Epoch 15249/30000 Training Loss: 0.057623036205768585\n",
      "Epoch 15250/30000 Training Loss: 0.058473773300647736\n",
      "Epoch 15251/30000 Training Loss: 0.03854279965162277\n",
      "Epoch 15252/30000 Training Loss: 0.0463690385222435\n",
      "Epoch 15253/30000 Training Loss: 0.04016750305891037\n",
      "Epoch 15254/30000 Training Loss: 0.04949193075299263\n",
      "Epoch 15255/30000 Training Loss: 0.049119021743535995\n",
      "Epoch 15256/30000 Training Loss: 0.0404982715845108\n",
      "Epoch 15257/30000 Training Loss: 0.04761655256152153\n",
      "Epoch 15258/30000 Training Loss: 0.05534656345844269\n",
      "Epoch 15259/30000 Training Loss: 0.033945146948099136\n",
      "Epoch 15260/30000 Training Loss: 0.039347387850284576\n",
      "Epoch 15261/30000 Training Loss: 0.03136465698480606\n",
      "Epoch 15262/30000 Training Loss: 0.05166789889335632\n",
      "Epoch 15263/30000 Training Loss: 0.03600015118718147\n",
      "Epoch 15264/30000 Training Loss: 0.04727480188012123\n",
      "Epoch 15265/30000 Training Loss: 0.03921831399202347\n",
      "Epoch 15266/30000 Training Loss: 0.04871943220496178\n",
      "Epoch 15267/30000 Training Loss: 0.062181562185287476\n",
      "Epoch 15268/30000 Training Loss: 0.05889735743403435\n",
      "Epoch 15269/30000 Training Loss: 0.0346931517124176\n",
      "Epoch 15270/30000 Training Loss: 0.04208971932530403\n",
      "Epoch 15271/30000 Training Loss: 0.048195429146289825\n",
      "Epoch 15272/30000 Training Loss: 0.04110560566186905\n",
      "Epoch 15273/30000 Training Loss: 0.05474626272916794\n",
      "Epoch 15274/30000 Training Loss: 0.043525904417037964\n",
      "Epoch 15275/30000 Training Loss: 0.04301122575998306\n",
      "Epoch 15276/30000 Training Loss: 0.0330544151365757\n",
      "Epoch 15277/30000 Training Loss: 0.05006265640258789\n",
      "Epoch 15278/30000 Training Loss: 0.04871021956205368\n",
      "Epoch 15279/30000 Training Loss: 0.04367326945066452\n",
      "Epoch 15280/30000 Training Loss: 0.04786315932869911\n",
      "Epoch 15281/30000 Training Loss: 0.05587216094136238\n",
      "Epoch 15282/30000 Training Loss: 0.053448744118213654\n",
      "Epoch 15283/30000 Training Loss: 0.06049176678061485\n",
      "Epoch 15284/30000 Training Loss: 0.04304947331547737\n",
      "Epoch 15285/30000 Training Loss: 0.05502719432115555\n",
      "Epoch 15286/30000 Training Loss: 0.04316531866788864\n",
      "Epoch 15287/30000 Training Loss: 0.06447699666023254\n",
      "Epoch 15288/30000 Training Loss: 0.04964546114206314\n",
      "Epoch 15289/30000 Training Loss: 0.04671430587768555\n",
      "Epoch 15290/30000 Training Loss: 0.04995961859822273\n",
      "Epoch 15291/30000 Training Loss: 0.045163124799728394\n",
      "Epoch 15292/30000 Training Loss: 0.05165623873472214\n",
      "Epoch 15293/30000 Training Loss: 0.03793332725763321\n",
      "Epoch 15294/30000 Training Loss: 0.04374447092413902\n",
      "Epoch 15295/30000 Training Loss: 0.04124847799539566\n",
      "Epoch 15296/30000 Training Loss: 0.05278051272034645\n",
      "Epoch 15297/30000 Training Loss: 0.060311123728752136\n",
      "Epoch 15298/30000 Training Loss: 0.04395623505115509\n",
      "Epoch 15299/30000 Training Loss: 0.03943498432636261\n",
      "Epoch 15300/30000 Training Loss: 0.04096600040793419\n",
      "Epoch 15300/30000 Validation Loss: 0.05326724052429199\n",
      "Epoch 15301/30000 Training Loss: 0.04473603889346123\n",
      "Epoch 15302/30000 Training Loss: 0.046365752816200256\n",
      "Epoch 15303/30000 Training Loss: 0.05166354775428772\n",
      "Epoch 15304/30000 Training Loss: 0.03919006884098053\n",
      "Epoch 15305/30000 Training Loss: 0.042808063328266144\n",
      "Epoch 15306/30000 Training Loss: 0.045161448419094086\n",
      "Epoch 15307/30000 Training Loss: 0.05488048121333122\n",
      "Epoch 15308/30000 Training Loss: 0.047522176057100296\n",
      "Epoch 15309/30000 Training Loss: 0.046885162591934204\n",
      "Epoch 15310/30000 Training Loss: 0.04367384314537048\n",
      "Epoch 15311/30000 Training Loss: 0.04177303612232208\n",
      "Epoch 15312/30000 Training Loss: 0.047322168946266174\n",
      "Epoch 15313/30000 Training Loss: 0.05104101449251175\n",
      "Epoch 15314/30000 Training Loss: 0.053765151649713516\n",
      "Epoch 15315/30000 Training Loss: 0.05153099074959755\n",
      "Epoch 15316/30000 Training Loss: 0.06787855923175812\n",
      "Epoch 15317/30000 Training Loss: 0.046056292951107025\n",
      "Epoch 15318/30000 Training Loss: 0.04341035708785057\n",
      "Epoch 15319/30000 Training Loss: 0.047981563955545425\n",
      "Epoch 15320/30000 Training Loss: 0.041134245693683624\n",
      "Epoch 15321/30000 Training Loss: 0.04332128167152405\n",
      "Epoch 15322/30000 Training Loss: 0.04141755402088165\n",
      "Epoch 15323/30000 Training Loss: 0.04738861694931984\n",
      "Epoch 15324/30000 Training Loss: 0.03955142945051193\n",
      "Epoch 15325/30000 Training Loss: 0.05337654799222946\n",
      "Epoch 15326/30000 Training Loss: 0.0317625030875206\n",
      "Epoch 15327/30000 Training Loss: 0.034673407673835754\n",
      "Epoch 15328/30000 Training Loss: 0.03524886816740036\n",
      "Epoch 15329/30000 Training Loss: 0.046123579144477844\n",
      "Epoch 15330/30000 Training Loss: 0.03537365049123764\n",
      "Epoch 15331/30000 Training Loss: 0.039686236530542374\n",
      "Epoch 15332/30000 Training Loss: 0.035429947078228\n",
      "Epoch 15333/30000 Training Loss: 0.04214881360530853\n",
      "Epoch 15334/30000 Training Loss: 0.0593334324657917\n",
      "Epoch 15335/30000 Training Loss: 0.04425996541976929\n",
      "Epoch 15336/30000 Training Loss: 0.05341406911611557\n",
      "Epoch 15337/30000 Training Loss: 0.05793848633766174\n",
      "Epoch 15338/30000 Training Loss: 0.04110877588391304\n",
      "Epoch 15339/30000 Training Loss: 0.0473017543554306\n",
      "Epoch 15340/30000 Training Loss: 0.02954244799911976\n",
      "Epoch 15341/30000 Training Loss: 0.04113931581377983\n",
      "Epoch 15342/30000 Training Loss: 0.043596457690000534\n",
      "Epoch 15343/30000 Training Loss: 0.04957681894302368\n",
      "Epoch 15344/30000 Training Loss: 0.03119581565260887\n",
      "Epoch 15345/30000 Training Loss: 0.04764246195554733\n",
      "Epoch 15346/30000 Training Loss: 0.03839736059308052\n",
      "Epoch 15347/30000 Training Loss: 0.04163501784205437\n",
      "Epoch 15348/30000 Training Loss: 0.0508829727768898\n",
      "Epoch 15349/30000 Training Loss: 0.041481222957372665\n",
      "Epoch 15350/30000 Training Loss: 0.048327311873435974\n",
      "Epoch 15351/30000 Training Loss: 0.05823882669210434\n",
      "Epoch 15352/30000 Training Loss: 0.04404410719871521\n",
      "Epoch 15353/30000 Training Loss: 0.05625355243682861\n",
      "Epoch 15354/30000 Training Loss: 0.04850393533706665\n",
      "Epoch 15355/30000 Training Loss: 0.04011576622724533\n",
      "Epoch 15356/30000 Training Loss: 0.04177521914243698\n",
      "Epoch 15357/30000 Training Loss: 0.047827478498220444\n",
      "Epoch 15358/30000 Training Loss: 0.03706904500722885\n",
      "Epoch 15359/30000 Training Loss: 0.04397263750433922\n",
      "Epoch 15360/30000 Training Loss: 0.04993660748004913\n",
      "Epoch 15361/30000 Training Loss: 0.04032296687364578\n",
      "Epoch 15362/30000 Training Loss: 0.03528159484267235\n",
      "Epoch 15363/30000 Training Loss: 0.05037335306406021\n",
      "Epoch 15364/30000 Training Loss: 0.06151418387889862\n",
      "Epoch 15365/30000 Training Loss: 0.04294028878211975\n",
      "Epoch 15366/30000 Training Loss: 0.04645359516143799\n",
      "Epoch 15367/30000 Training Loss: 0.040942028164863586\n",
      "Epoch 15368/30000 Training Loss: 0.045553356409072876\n",
      "Epoch 15369/30000 Training Loss: 0.0369383841753006\n",
      "Epoch 15370/30000 Training Loss: 0.03753450885415077\n",
      "Epoch 15371/30000 Training Loss: 0.056058987975120544\n",
      "Epoch 15372/30000 Training Loss: 0.048985011875629425\n",
      "Epoch 15373/30000 Training Loss: 0.05383085831999779\n",
      "Epoch 15374/30000 Training Loss: 0.04008029028773308\n",
      "Epoch 15375/30000 Training Loss: 0.05482365936040878\n",
      "Epoch 15376/30000 Training Loss: 0.04675176739692688\n",
      "Epoch 15377/30000 Training Loss: 0.04197561740875244\n",
      "Epoch 15378/30000 Training Loss: 0.050948552787303925\n",
      "Epoch 15379/30000 Training Loss: 0.04021639749407768\n",
      "Epoch 15380/30000 Training Loss: 0.040388453751802444\n",
      "Epoch 15381/30000 Training Loss: 0.05603589117527008\n",
      "Epoch 15382/30000 Training Loss: 0.040572263300418854\n",
      "Epoch 15383/30000 Training Loss: 0.04878813773393631\n",
      "Epoch 15384/30000 Training Loss: 0.053064145147800446\n",
      "Epoch 15385/30000 Training Loss: 0.04397408664226532\n",
      "Epoch 15386/30000 Training Loss: 0.05761508643627167\n",
      "Epoch 15387/30000 Training Loss: 0.034792400896549225\n",
      "Epoch 15388/30000 Training Loss: 0.03483528271317482\n",
      "Epoch 15389/30000 Training Loss: 0.04635145515203476\n",
      "Epoch 15390/30000 Training Loss: 0.04297906905412674\n",
      "Epoch 15391/30000 Training Loss: 0.05593505874276161\n",
      "Epoch 15392/30000 Training Loss: 0.0547267384827137\n",
      "Epoch 15393/30000 Training Loss: 0.03451205790042877\n",
      "Epoch 15394/30000 Training Loss: 0.0717749074101448\n",
      "Epoch 15395/30000 Training Loss: 0.050481975078582764\n",
      "Epoch 15396/30000 Training Loss: 0.048631519079208374\n",
      "Epoch 15397/30000 Training Loss: 0.034240175038576126\n",
      "Epoch 15398/30000 Training Loss: 0.06393417716026306\n",
      "Epoch 15399/30000 Training Loss: 0.03909669816493988\n",
      "Epoch 15400/30000 Training Loss: 0.039544727653265\n",
      "Epoch 15400/30000 Validation Loss: 0.04485815390944481\n",
      "Epoch 15401/30000 Training Loss: 0.05576246231794357\n",
      "Epoch 15402/30000 Training Loss: 0.03786672651767731\n",
      "Epoch 15403/30000 Training Loss: 0.04665903002023697\n",
      "Epoch 15404/30000 Training Loss: 0.051458753645420074\n",
      "Epoch 15405/30000 Training Loss: 0.047875165939331055\n",
      "Epoch 15406/30000 Training Loss: 0.04029045253992081\n",
      "Epoch 15407/30000 Training Loss: 0.04143746197223663\n",
      "Epoch 15408/30000 Training Loss: 0.051416777074337006\n",
      "Epoch 15409/30000 Training Loss: 0.046873778104782104\n",
      "Epoch 15410/30000 Training Loss: 0.06189344823360443\n",
      "Epoch 15411/30000 Training Loss: 0.042923979461193085\n",
      "Epoch 15412/30000 Training Loss: 0.037291936576366425\n",
      "Epoch 15413/30000 Training Loss: 0.046922437846660614\n",
      "Epoch 15414/30000 Training Loss: 0.04763136804103851\n",
      "Epoch 15415/30000 Training Loss: 0.051943011581897736\n",
      "Epoch 15416/30000 Training Loss: 0.047903209924697876\n",
      "Epoch 15417/30000 Training Loss: 0.03490956127643585\n",
      "Epoch 15418/30000 Training Loss: 0.07214464992284775\n",
      "Epoch 15419/30000 Training Loss: 0.049498870968818665\n",
      "Epoch 15420/30000 Training Loss: 0.05447334796190262\n",
      "Epoch 15421/30000 Training Loss: 0.04681020975112915\n",
      "Epoch 15422/30000 Training Loss: 0.04823293909430504\n",
      "Epoch 15423/30000 Training Loss: 0.04686956852674484\n",
      "Epoch 15424/30000 Training Loss: 0.06359390169382095\n",
      "Epoch 15425/30000 Training Loss: 0.04449838027358055\n",
      "Epoch 15426/30000 Training Loss: 0.04799690470099449\n",
      "Epoch 15427/30000 Training Loss: 0.04251304641366005\n",
      "Epoch 15428/30000 Training Loss: 0.04397161304950714\n",
      "Epoch 15429/30000 Training Loss: 0.04216945916414261\n",
      "Epoch 15430/30000 Training Loss: 0.03515158221125603\n",
      "Epoch 15431/30000 Training Loss: 0.06007084995508194\n",
      "Epoch 15432/30000 Training Loss: 0.04280153661966324\n",
      "Epoch 15433/30000 Training Loss: 0.04376733675599098\n",
      "Epoch 15434/30000 Training Loss: 0.0445253849029541\n",
      "Epoch 15435/30000 Training Loss: 0.03953365236520767\n",
      "Epoch 15436/30000 Training Loss: 0.058499179780483246\n",
      "Epoch 15437/30000 Training Loss: 0.04244302585721016\n",
      "Epoch 15438/30000 Training Loss: 0.05748947709798813\n",
      "Epoch 15439/30000 Training Loss: 0.04129398241639137\n",
      "Epoch 15440/30000 Training Loss: 0.028185276314616203\n",
      "Epoch 15441/30000 Training Loss: 0.04722479730844498\n",
      "Epoch 15442/30000 Training Loss: 0.06815975904464722\n",
      "Epoch 15443/30000 Training Loss: 0.057624898850917816\n",
      "Epoch 15444/30000 Training Loss: 0.047577232122421265\n",
      "Epoch 15445/30000 Training Loss: 0.06435010582208633\n",
      "Epoch 15446/30000 Training Loss: 0.03992518037557602\n",
      "Epoch 15447/30000 Training Loss: 0.0316418893635273\n",
      "Epoch 15448/30000 Training Loss: 0.045972272753715515\n",
      "Epoch 15449/30000 Training Loss: 0.043321382254362106\n",
      "Epoch 15450/30000 Training Loss: 0.041597332805395126\n",
      "Epoch 15451/30000 Training Loss: 0.043695658445358276\n",
      "Epoch 15452/30000 Training Loss: 0.0440436489880085\n",
      "Epoch 15453/30000 Training Loss: 0.05159063637256622\n",
      "Epoch 15454/30000 Training Loss: 0.04589247331023216\n",
      "Epoch 15455/30000 Training Loss: 0.0693788006901741\n",
      "Epoch 15456/30000 Training Loss: 0.051076434552669525\n",
      "Epoch 15457/30000 Training Loss: 0.037450071424245834\n",
      "Epoch 15458/30000 Training Loss: 0.05155525356531143\n",
      "Epoch 15459/30000 Training Loss: 0.042030319571495056\n",
      "Epoch 15460/30000 Training Loss: 0.03719232976436615\n",
      "Epoch 15461/30000 Training Loss: 0.044394783675670624\n",
      "Epoch 15462/30000 Training Loss: 0.051741644740104675\n",
      "Epoch 15463/30000 Training Loss: 0.051376886665821075\n",
      "Epoch 15464/30000 Training Loss: 0.03461815416812897\n",
      "Epoch 15465/30000 Training Loss: 0.05378347635269165\n",
      "Epoch 15466/30000 Training Loss: 0.05151895061135292\n",
      "Epoch 15467/30000 Training Loss: 0.03995097428560257\n",
      "Epoch 15468/30000 Training Loss: 0.03467921167612076\n",
      "Epoch 15469/30000 Training Loss: 0.05116185545921326\n",
      "Epoch 15470/30000 Training Loss: 0.038808710873126984\n",
      "Epoch 15471/30000 Training Loss: 0.043549925088882446\n",
      "Epoch 15472/30000 Training Loss: 0.03743060305714607\n",
      "Epoch 15473/30000 Training Loss: 0.039188139140605927\n",
      "Epoch 15474/30000 Training Loss: 0.05171681195497513\n",
      "Epoch 15475/30000 Training Loss: 0.04633984714746475\n",
      "Epoch 15476/30000 Training Loss: 0.041588760912418365\n",
      "Epoch 15477/30000 Training Loss: 0.03268779441714287\n",
      "Epoch 15478/30000 Training Loss: 0.04236277937889099\n",
      "Epoch 15479/30000 Training Loss: 0.03750288486480713\n",
      "Epoch 15480/30000 Training Loss: 0.041824303567409515\n",
      "Epoch 15481/30000 Training Loss: 0.05493069440126419\n",
      "Epoch 15482/30000 Training Loss: 0.03559136018157005\n",
      "Epoch 15483/30000 Training Loss: 0.046828530728816986\n",
      "Epoch 15484/30000 Training Loss: 0.04091857373714447\n",
      "Epoch 15485/30000 Training Loss: 0.0535317063331604\n",
      "Epoch 15486/30000 Training Loss: 0.04218583554029465\n",
      "Epoch 15487/30000 Training Loss: 0.053972382098436356\n",
      "Epoch 15488/30000 Training Loss: 0.052323758602142334\n",
      "Epoch 15489/30000 Training Loss: 0.04334539175033569\n",
      "Epoch 15490/30000 Training Loss: 0.0497119277715683\n",
      "Epoch 15491/30000 Training Loss: 0.04050353914499283\n",
      "Epoch 15492/30000 Training Loss: 0.03844190388917923\n",
      "Epoch 15493/30000 Training Loss: 0.043145716190338135\n",
      "Epoch 15494/30000 Training Loss: 0.04689457267522812\n",
      "Epoch 15495/30000 Training Loss: 0.0626828521490097\n",
      "Epoch 15496/30000 Training Loss: 0.049175798892974854\n",
      "Epoch 15497/30000 Training Loss: 0.03240082785487175\n",
      "Epoch 15498/30000 Training Loss: 0.027002528309822083\n",
      "Epoch 15499/30000 Training Loss: 0.04162622615695\n",
      "Epoch 15500/30000 Training Loss: 0.03918137028813362\n",
      "Epoch 15500/30000 Validation Loss: 0.032412320375442505\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.032412320375442505<=============\n",
      "Epoch 15501/30000 Training Loss: 0.047534964978694916\n",
      "Epoch 15502/30000 Training Loss: 0.043467119336128235\n",
      "Epoch 15503/30000 Training Loss: 0.03863251954317093\n",
      "Epoch 15504/30000 Training Loss: 0.04233938455581665\n",
      "Epoch 15505/30000 Training Loss: 0.06483707576990128\n",
      "Epoch 15506/30000 Training Loss: 0.03900051489472389\n",
      "Epoch 15507/30000 Training Loss: 0.045019298791885376\n",
      "Epoch 15508/30000 Training Loss: 0.04917723685503006\n",
      "Epoch 15509/30000 Training Loss: 0.04330934211611748\n",
      "Epoch 15510/30000 Training Loss: 0.049974653869867325\n",
      "Epoch 15511/30000 Training Loss: 0.04781287908554077\n",
      "Epoch 15512/30000 Training Loss: 0.059121303260326385\n",
      "Epoch 15513/30000 Training Loss: 0.04809601604938507\n",
      "Epoch 15514/30000 Training Loss: 0.04622109979391098\n",
      "Epoch 15515/30000 Training Loss: 0.03833114355802536\n",
      "Epoch 15516/30000 Training Loss: 0.059660546481609344\n",
      "Epoch 15517/30000 Training Loss: 0.0446728952229023\n",
      "Epoch 15518/30000 Training Loss: 0.04174257814884186\n",
      "Epoch 15519/30000 Training Loss: 0.03176433593034744\n",
      "Epoch 15520/30000 Training Loss: 0.03703978657722473\n",
      "Epoch 15521/30000 Training Loss: 0.039406925439834595\n",
      "Epoch 15522/30000 Training Loss: 0.04792475700378418\n",
      "Epoch 15523/30000 Training Loss: 0.03677710145711899\n",
      "Epoch 15524/30000 Training Loss: 0.04565173387527466\n",
      "Epoch 15525/30000 Training Loss: 0.04826569929718971\n",
      "Epoch 15526/30000 Training Loss: 0.036022767424583435\n",
      "Epoch 15527/30000 Training Loss: 0.044059868901968\n",
      "Epoch 15528/30000 Training Loss: 0.04556789994239807\n",
      "Epoch 15529/30000 Training Loss: 0.04755290597677231\n",
      "Epoch 15530/30000 Training Loss: 0.048256710171699524\n",
      "Epoch 15531/30000 Training Loss: 0.04858136549592018\n",
      "Epoch 15532/30000 Training Loss: 0.037209734320640564\n",
      "Epoch 15533/30000 Training Loss: 0.0451347753405571\n",
      "Epoch 15534/30000 Training Loss: 0.048729307949543\n",
      "Epoch 15535/30000 Training Loss: 0.03941354528069496\n",
      "Epoch 15536/30000 Training Loss: 0.04403413459658623\n",
      "Epoch 15537/30000 Training Loss: 0.05043851584196091\n",
      "Epoch 15538/30000 Training Loss: 0.04555961862206459\n",
      "Epoch 15539/30000 Training Loss: 0.05928196758031845\n",
      "Epoch 15540/30000 Training Loss: 0.03455687686800957\n",
      "Epoch 15541/30000 Training Loss: 0.05770964175462723\n",
      "Epoch 15542/30000 Training Loss: 0.04691872000694275\n",
      "Epoch 15543/30000 Training Loss: 0.04674214869737625\n",
      "Epoch 15544/30000 Training Loss: 0.05439182370901108\n",
      "Epoch 15545/30000 Training Loss: 0.06536350399255753\n",
      "Epoch 15546/30000 Training Loss: 0.048819079995155334\n",
      "Epoch 15547/30000 Training Loss: 0.05642770603299141\n",
      "Epoch 15548/30000 Training Loss: 0.05145668610930443\n",
      "Epoch 15549/30000 Training Loss: 0.037464506924152374\n",
      "Epoch 15550/30000 Training Loss: 0.044992927461862564\n",
      "Epoch 15551/30000 Training Loss: 0.04359109699726105\n",
      "Epoch 15552/30000 Training Loss: 0.04387517273426056\n",
      "Epoch 15553/30000 Training Loss: 0.04793249070644379\n",
      "Epoch 15554/30000 Training Loss: 0.05385056138038635\n",
      "Epoch 15555/30000 Training Loss: 0.050544630736112595\n",
      "Epoch 15556/30000 Training Loss: 0.04779244214296341\n",
      "Epoch 15557/30000 Training Loss: 0.03721664100885391\n",
      "Epoch 15558/30000 Training Loss: 0.041768405586481094\n",
      "Epoch 15559/30000 Training Loss: 0.04596169665455818\n",
      "Epoch 15560/30000 Training Loss: 0.054775260388851166\n",
      "Epoch 15561/30000 Training Loss: 0.061948392540216446\n",
      "Epoch 15562/30000 Training Loss: 0.04634816199541092\n",
      "Epoch 15563/30000 Training Loss: 0.03733881935477257\n",
      "Epoch 15564/30000 Training Loss: 0.03698152303695679\n",
      "Epoch 15565/30000 Training Loss: 0.045650556683540344\n",
      "Epoch 15566/30000 Training Loss: 0.04247148707509041\n",
      "Epoch 15567/30000 Training Loss: 0.0620933435857296\n",
      "Epoch 15568/30000 Training Loss: 0.056437764316797256\n",
      "Epoch 15569/30000 Training Loss: 0.03964261710643768\n",
      "Epoch 15570/30000 Training Loss: 0.055634744465351105\n",
      "Epoch 15571/30000 Training Loss: 0.04158560931682587\n",
      "Epoch 15572/30000 Training Loss: 0.03482978790998459\n",
      "Epoch 15573/30000 Training Loss: 0.03982539847493172\n",
      "Epoch 15574/30000 Training Loss: 0.031041109934449196\n",
      "Epoch 15575/30000 Training Loss: 0.05338199436664581\n",
      "Epoch 15576/30000 Training Loss: 0.039450034499168396\n",
      "Epoch 15577/30000 Training Loss: 0.03556036949157715\n",
      "Epoch 15578/30000 Training Loss: 0.04204520955681801\n",
      "Epoch 15579/30000 Training Loss: 0.04238441586494446\n",
      "Epoch 15580/30000 Training Loss: 0.036428384482860565\n",
      "Epoch 15581/30000 Training Loss: 0.03283988684415817\n",
      "Epoch 15582/30000 Training Loss: 0.054941076785326004\n",
      "Epoch 15583/30000 Training Loss: 0.04596631973981857\n",
      "Epoch 15584/30000 Training Loss: 0.05298357456922531\n",
      "Epoch 15585/30000 Training Loss: 0.041838519275188446\n",
      "Epoch 15586/30000 Training Loss: 0.04611042141914368\n",
      "Epoch 15587/30000 Training Loss: 0.04625462740659714\n",
      "Epoch 15588/30000 Training Loss: 0.03613952174782753\n",
      "Epoch 15589/30000 Training Loss: 0.050749022513628006\n",
      "Epoch 15590/30000 Training Loss: 0.04306304454803467\n",
      "Epoch 15591/30000 Training Loss: 0.04712730646133423\n",
      "Epoch 15592/30000 Training Loss: 0.0344298854470253\n",
      "Epoch 15593/30000 Training Loss: 0.037495553493499756\n",
      "Epoch 15594/30000 Training Loss: 0.06851811707019806\n",
      "Epoch 15595/30000 Training Loss: 0.03792550414800644\n",
      "Epoch 15596/30000 Training Loss: 0.03712216764688492\n",
      "Epoch 15597/30000 Training Loss: 0.07275864481925964\n",
      "Epoch 15598/30000 Training Loss: 0.05189529061317444\n",
      "Epoch 15599/30000 Training Loss: 0.057075463235378265\n",
      "Epoch 15600/30000 Training Loss: 0.047414470463991165\n",
      "Epoch 15600/30000 Validation Loss: 0.04559994488954544\n",
      "Epoch 15601/30000 Training Loss: 0.04158896580338478\n",
      "Epoch 15602/30000 Training Loss: 0.05487626791000366\n",
      "Epoch 15603/30000 Training Loss: 0.03908584266901016\n",
      "Epoch 15604/30000 Training Loss: 0.04575736075639725\n",
      "Epoch 15605/30000 Training Loss: 0.057090625166893005\n",
      "Epoch 15606/30000 Training Loss: 0.02922828495502472\n",
      "Epoch 15607/30000 Training Loss: 0.04457717761397362\n",
      "Epoch 15608/30000 Training Loss: 0.04361087456345558\n",
      "Epoch 15609/30000 Training Loss: 0.05229286104440689\n",
      "Epoch 15610/30000 Training Loss: 0.044930316507816315\n",
      "Epoch 15611/30000 Training Loss: 0.04782302677631378\n",
      "Epoch 15612/30000 Training Loss: 0.041549116373062134\n",
      "Epoch 15613/30000 Training Loss: 0.038205720484256744\n",
      "Epoch 15614/30000 Training Loss: 0.02719060517847538\n",
      "Epoch 15615/30000 Training Loss: 0.04631531238555908\n",
      "Epoch 15616/30000 Training Loss: 0.07258100807666779\n",
      "Epoch 15617/30000 Training Loss: 0.04534009099006653\n",
      "Epoch 15618/30000 Training Loss: 0.03987986221909523\n",
      "Epoch 15619/30000 Training Loss: 0.05562718212604523\n",
      "Epoch 15620/30000 Training Loss: 0.05539828538894653\n",
      "Epoch 15621/30000 Training Loss: 0.037568479776382446\n",
      "Epoch 15622/30000 Training Loss: 0.04443550854921341\n",
      "Epoch 15623/30000 Training Loss: 0.03540223091840744\n",
      "Epoch 15624/30000 Training Loss: 0.050730399787425995\n",
      "Epoch 15625/30000 Training Loss: 0.04866204783320427\n",
      "Epoch 15626/30000 Training Loss: 0.0405580997467041\n",
      "Epoch 15627/30000 Training Loss: 0.04295438528060913\n",
      "Epoch 15628/30000 Training Loss: 0.03701721131801605\n",
      "Epoch 15629/30000 Training Loss: 0.04735342040657997\n",
      "Epoch 15630/30000 Training Loss: 0.03764890879392624\n",
      "Epoch 15631/30000 Training Loss: 0.04717213660478592\n",
      "Epoch 15632/30000 Training Loss: 0.0458187535405159\n",
      "Epoch 15633/30000 Training Loss: 0.061121366918087006\n",
      "Epoch 15634/30000 Training Loss: 0.03371617570519447\n",
      "Epoch 15635/30000 Training Loss: 0.04544711858034134\n",
      "Epoch 15636/30000 Training Loss: 0.0407213419675827\n",
      "Epoch 15637/30000 Training Loss: 0.05431021749973297\n",
      "Epoch 15638/30000 Training Loss: 0.03337106108665466\n",
      "Epoch 15639/30000 Training Loss: 0.035930998623371124\n",
      "Epoch 15640/30000 Training Loss: 0.046060167253017426\n",
      "Epoch 15641/30000 Training Loss: 0.05584998428821564\n",
      "Epoch 15642/30000 Training Loss: 0.03802253305912018\n",
      "Epoch 15643/30000 Training Loss: 0.052690498530864716\n",
      "Epoch 15644/30000 Training Loss: 0.04337898641824722\n",
      "Epoch 15645/30000 Training Loss: 0.046963974833488464\n",
      "Epoch 15646/30000 Training Loss: 0.04197656363248825\n",
      "Epoch 15647/30000 Training Loss: 0.05449400842189789\n",
      "Epoch 15648/30000 Training Loss: 0.04876813292503357\n",
      "Epoch 15649/30000 Training Loss: 0.039338819682598114\n",
      "Epoch 15650/30000 Training Loss: 0.04161528870463371\n",
      "Epoch 15651/30000 Training Loss: 0.0431676059961319\n",
      "Epoch 15652/30000 Training Loss: 0.04827311635017395\n",
      "Epoch 15653/30000 Training Loss: 0.05205477774143219\n",
      "Epoch 15654/30000 Training Loss: 0.05771607905626297\n",
      "Epoch 15655/30000 Training Loss: 0.03697472810745239\n",
      "Epoch 15656/30000 Training Loss: 0.04700770601630211\n",
      "Epoch 15657/30000 Training Loss: 0.06799532473087311\n",
      "Epoch 15658/30000 Training Loss: 0.049572959542274475\n",
      "Epoch 15659/30000 Training Loss: 0.04578397423028946\n",
      "Epoch 15660/30000 Training Loss: 0.06004065275192261\n",
      "Epoch 15661/30000 Training Loss: 0.03918853774666786\n",
      "Epoch 15662/30000 Training Loss: 0.03949948027729988\n",
      "Epoch 15663/30000 Training Loss: 0.04831057786941528\n",
      "Epoch 15664/30000 Training Loss: 0.05417533218860626\n",
      "Epoch 15665/30000 Training Loss: 0.05092119425535202\n",
      "Epoch 15666/30000 Training Loss: 0.055050645023584366\n",
      "Epoch 15667/30000 Training Loss: 0.05005783587694168\n",
      "Epoch 15668/30000 Training Loss: 0.04979206621646881\n",
      "Epoch 15669/30000 Training Loss: 0.03703950718045235\n",
      "Epoch 15670/30000 Training Loss: 0.051472343504428864\n",
      "Epoch 15671/30000 Training Loss: 0.06217863783240318\n",
      "Epoch 15672/30000 Training Loss: 0.04278907552361488\n",
      "Epoch 15673/30000 Training Loss: 0.0531761534512043\n",
      "Epoch 15674/30000 Training Loss: 0.04602966085076332\n",
      "Epoch 15675/30000 Training Loss: 0.040289465337991714\n",
      "Epoch 15676/30000 Training Loss: 0.05063202977180481\n",
      "Epoch 15677/30000 Training Loss: 0.039685554802417755\n",
      "Epoch 15678/30000 Training Loss: 0.053902871906757355\n",
      "Epoch 15679/30000 Training Loss: 0.058192864060401917\n",
      "Epoch 15680/30000 Training Loss: 0.0510343573987484\n",
      "Epoch 15681/30000 Training Loss: 0.0561310313642025\n",
      "Epoch 15682/30000 Training Loss: 0.04279680922627449\n",
      "Epoch 15683/30000 Training Loss: 0.051928579807281494\n",
      "Epoch 15684/30000 Training Loss: 0.056273408234119415\n",
      "Epoch 15685/30000 Training Loss: 0.04526345804333687\n",
      "Epoch 15686/30000 Training Loss: 0.0452386736869812\n",
      "Epoch 15687/30000 Training Loss: 0.04391031712293625\n",
      "Epoch 15688/30000 Training Loss: 0.03949768841266632\n",
      "Epoch 15689/30000 Training Loss: 0.04464983940124512\n",
      "Epoch 15690/30000 Training Loss: 0.059627000242471695\n",
      "Epoch 15691/30000 Training Loss: 0.056810908019542694\n",
      "Epoch 15692/30000 Training Loss: 0.03815538436174393\n",
      "Epoch 15693/30000 Training Loss: 0.038366809487342834\n",
      "Epoch 15694/30000 Training Loss: 0.052971165627241135\n",
      "Epoch 15695/30000 Training Loss: 0.04145593196153641\n",
      "Epoch 15696/30000 Training Loss: 0.06025656685233116\n",
      "Epoch 15697/30000 Training Loss: 0.03889263793826103\n",
      "Epoch 15698/30000 Training Loss: 0.041411977261304855\n",
      "Epoch 15699/30000 Training Loss: 0.043279871344566345\n",
      "Epoch 15700/30000 Training Loss: 0.0424448698759079\n",
      "Epoch 15700/30000 Validation Loss: 0.04265761002898216\n",
      "Epoch 15701/30000 Training Loss: 0.04354795813560486\n",
      "Epoch 15702/30000 Training Loss: 0.049718812108039856\n",
      "Epoch 15703/30000 Training Loss: 0.059013381600379944\n",
      "Epoch 15704/30000 Training Loss: 0.03397234529256821\n",
      "Epoch 15705/30000 Training Loss: 0.059869177639484406\n",
      "Epoch 15706/30000 Training Loss: 0.045393213629722595\n",
      "Epoch 15707/30000 Training Loss: 0.06089087575674057\n",
      "Epoch 15708/30000 Training Loss: 0.05375714972615242\n",
      "Epoch 15709/30000 Training Loss: 0.04490828514099121\n",
      "Epoch 15710/30000 Training Loss: 0.04482463747262955\n",
      "Epoch 15711/30000 Training Loss: 0.05460719019174576\n",
      "Epoch 15712/30000 Training Loss: 0.04761951044201851\n",
      "Epoch 15713/30000 Training Loss: 0.03862418606877327\n",
      "Epoch 15714/30000 Training Loss: 0.04908563196659088\n",
      "Epoch 15715/30000 Training Loss: 0.05064019560813904\n",
      "Epoch 15716/30000 Training Loss: 0.04965699464082718\n",
      "Epoch 15717/30000 Training Loss: 0.06231551617383957\n",
      "Epoch 15718/30000 Training Loss: 0.049211446195840836\n",
      "Epoch 15719/30000 Training Loss: 0.0487312451004982\n",
      "Epoch 15720/30000 Training Loss: 0.03330080211162567\n",
      "Epoch 15721/30000 Training Loss: 0.04809126257896423\n",
      "Epoch 15722/30000 Training Loss: 0.05127748101949692\n",
      "Epoch 15723/30000 Training Loss: 0.06225156784057617\n",
      "Epoch 15724/30000 Training Loss: 0.03241397440433502\n",
      "Epoch 15725/30000 Training Loss: 0.034271690994501114\n",
      "Epoch 15726/30000 Training Loss: 0.052149347960948944\n",
      "Epoch 15727/30000 Training Loss: 0.033570706844329834\n",
      "Epoch 15728/30000 Training Loss: 0.03545534610748291\n",
      "Epoch 15729/30000 Training Loss: 0.04400501027703285\n",
      "Epoch 15730/30000 Training Loss: 0.046057865023612976\n",
      "Epoch 15731/30000 Training Loss: 0.037564728409051895\n",
      "Epoch 15732/30000 Training Loss: 0.04873088374733925\n",
      "Epoch 15733/30000 Training Loss: 0.03691999614238739\n",
      "Epoch 15734/30000 Training Loss: 0.03438659384846687\n",
      "Epoch 15735/30000 Training Loss: 0.05266201123595238\n",
      "Epoch 15736/30000 Training Loss: 0.04535605385899544\n",
      "Epoch 15737/30000 Training Loss: 0.047340624034404755\n",
      "Epoch 15738/30000 Training Loss: 0.05110665410757065\n",
      "Epoch 15739/30000 Training Loss: 0.040816083550453186\n",
      "Epoch 15740/30000 Training Loss: 0.04203663021326065\n",
      "Epoch 15741/30000 Training Loss: 0.05396746098995209\n",
      "Epoch 15742/30000 Training Loss: 0.029585734009742737\n",
      "Epoch 15743/30000 Training Loss: 0.03952278196811676\n",
      "Epoch 15744/30000 Training Loss: 0.048679277300834656\n",
      "Epoch 15745/30000 Training Loss: 0.05088172107934952\n",
      "Epoch 15746/30000 Training Loss: 0.036423180252313614\n",
      "Epoch 15747/30000 Training Loss: 0.04765865206718445\n",
      "Epoch 15748/30000 Training Loss: 0.03459307551383972\n",
      "Epoch 15749/30000 Training Loss: 0.04284004122018814\n",
      "Epoch 15750/30000 Training Loss: 0.03613733500242233\n",
      "Epoch 15751/30000 Training Loss: 0.05047666281461716\n",
      "Epoch 15752/30000 Training Loss: 0.04885849729180336\n",
      "Epoch 15753/30000 Training Loss: 0.048215754330158234\n",
      "Epoch 15754/30000 Training Loss: 0.04420965909957886\n",
      "Epoch 15755/30000 Training Loss: 0.05012330412864685\n",
      "Epoch 15756/30000 Training Loss: 0.05491437017917633\n",
      "Epoch 15757/30000 Training Loss: 0.04648032784461975\n",
      "Epoch 15758/30000 Training Loss: 0.04625917598605156\n",
      "Epoch 15759/30000 Training Loss: 0.04519820958375931\n",
      "Epoch 15760/30000 Training Loss: 0.05441785231232643\n",
      "Epoch 15761/30000 Training Loss: 0.036299992352724075\n",
      "Epoch 15762/30000 Training Loss: 0.0374896265566349\n",
      "Epoch 15763/30000 Training Loss: 0.045429691672325134\n",
      "Epoch 15764/30000 Training Loss: 0.05155794322490692\n",
      "Epoch 15765/30000 Training Loss: 0.0392247810959816\n",
      "Epoch 15766/30000 Training Loss: 0.03705569729208946\n",
      "Epoch 15767/30000 Training Loss: 0.044990330934524536\n",
      "Epoch 15768/30000 Training Loss: 0.0385490283370018\n",
      "Epoch 15769/30000 Training Loss: 0.04185868054628372\n",
      "Epoch 15770/30000 Training Loss: 0.045642368495464325\n",
      "Epoch 15771/30000 Training Loss: 0.03819979727268219\n",
      "Epoch 15772/30000 Training Loss: 0.03904242813587189\n",
      "Epoch 15773/30000 Training Loss: 0.054211221635341644\n",
      "Epoch 15774/30000 Training Loss: 0.03732471540570259\n",
      "Epoch 15775/30000 Training Loss: 0.04232566058635712\n",
      "Epoch 15776/30000 Training Loss: 0.059801653027534485\n",
      "Epoch 15777/30000 Training Loss: 0.045759961009025574\n",
      "Epoch 15778/30000 Training Loss: 0.05888634920120239\n",
      "Epoch 15779/30000 Training Loss: 0.062199048697948456\n",
      "Epoch 15780/30000 Training Loss: 0.04030021280050278\n",
      "Epoch 15781/30000 Training Loss: 0.040536463260650635\n",
      "Epoch 15782/30000 Training Loss: 0.04800737649202347\n",
      "Epoch 15783/30000 Training Loss: 0.04283308982849121\n",
      "Epoch 15784/30000 Training Loss: 0.04743366688489914\n",
      "Epoch 15785/30000 Training Loss: 0.045272909104824066\n",
      "Epoch 15786/30000 Training Loss: 0.044762302190065384\n",
      "Epoch 15787/30000 Training Loss: 0.06372039765119553\n",
      "Epoch 15788/30000 Training Loss: 0.04864984378218651\n",
      "Epoch 15789/30000 Training Loss: 0.04413311183452606\n",
      "Epoch 15790/30000 Training Loss: 0.050112418830394745\n",
      "Epoch 15791/30000 Training Loss: 0.03762069717049599\n",
      "Epoch 15792/30000 Training Loss: 0.05296904966235161\n",
      "Epoch 15793/30000 Training Loss: 0.05577659606933594\n",
      "Epoch 15794/30000 Training Loss: 0.03794056177139282\n",
      "Epoch 15795/30000 Training Loss: 0.048304393887519836\n",
      "Epoch 15796/30000 Training Loss: 0.04586410149931908\n",
      "Epoch 15797/30000 Training Loss: 0.038831114768981934\n",
      "Epoch 15798/30000 Training Loss: 0.04076417535543442\n",
      "Epoch 15799/30000 Training Loss: 0.05354606732726097\n",
      "Epoch 15800/30000 Training Loss: 0.03693729266524315\n",
      "Epoch 15800/30000 Validation Loss: 0.040475308895111084\n",
      "Epoch 15801/30000 Training Loss: 0.04782074689865112\n",
      "Epoch 15802/30000 Training Loss: 0.045391567051410675\n",
      "Epoch 15803/30000 Training Loss: 0.0718974843621254\n",
      "Epoch 15804/30000 Training Loss: 0.04477578401565552\n",
      "Epoch 15805/30000 Training Loss: 0.051457133144140244\n",
      "Epoch 15806/30000 Training Loss: 0.02925712987780571\n",
      "Epoch 15807/30000 Training Loss: 0.05074428766965866\n",
      "Epoch 15808/30000 Training Loss: 0.05702165141701698\n",
      "Epoch 15809/30000 Training Loss: 0.05116608366370201\n",
      "Epoch 15810/30000 Training Loss: 0.04191356897354126\n",
      "Epoch 15811/30000 Training Loss: 0.04990240931510925\n",
      "Epoch 15812/30000 Training Loss: 0.04528516158461571\n",
      "Epoch 15813/30000 Training Loss: 0.03611154854297638\n",
      "Epoch 15814/30000 Training Loss: 0.04850148409605026\n",
      "Epoch 15815/30000 Training Loss: 0.045407578349113464\n",
      "Epoch 15816/30000 Training Loss: 0.035405632108449936\n",
      "Epoch 15817/30000 Training Loss: 0.046670787036418915\n",
      "Epoch 15818/30000 Training Loss: 0.04601888358592987\n",
      "Epoch 15819/30000 Training Loss: 0.04389118403196335\n",
      "Epoch 15820/30000 Training Loss: 0.05654418468475342\n",
      "Epoch 15821/30000 Training Loss: 0.04961322247982025\n",
      "Epoch 15822/30000 Training Loss: 0.05044451355934143\n",
      "Epoch 15823/30000 Training Loss: 0.053108636289834976\n",
      "Epoch 15824/30000 Training Loss: 0.04850421100854874\n",
      "Epoch 15825/30000 Training Loss: 0.044801436364650726\n",
      "Epoch 15826/30000 Training Loss: 0.03892059624195099\n",
      "Epoch 15827/30000 Training Loss: 0.04873361811041832\n",
      "Epoch 15828/30000 Training Loss: 0.04892977699637413\n",
      "Epoch 15829/30000 Training Loss: 0.052736736834049225\n",
      "Epoch 15830/30000 Training Loss: 0.04624007269740105\n",
      "Epoch 15831/30000 Training Loss: 0.039137907326221466\n",
      "Epoch 15832/30000 Training Loss: 0.044668398797512054\n",
      "Epoch 15833/30000 Training Loss: 0.04148209095001221\n",
      "Epoch 15834/30000 Training Loss: 0.053709205240011215\n",
      "Epoch 15835/30000 Training Loss: 0.04822998493909836\n",
      "Epoch 15836/30000 Training Loss: 0.037479884922504425\n",
      "Epoch 15837/30000 Training Loss: 0.04321214556694031\n",
      "Epoch 15838/30000 Training Loss: 0.04746215045452118\n",
      "Epoch 15839/30000 Training Loss: 0.04377913475036621\n",
      "Epoch 15840/30000 Training Loss: 0.06820487231016159\n",
      "Epoch 15841/30000 Training Loss: 0.04346230626106262\n",
      "Epoch 15842/30000 Training Loss: 0.03664952144026756\n",
      "Epoch 15843/30000 Training Loss: 0.042325496673583984\n",
      "Epoch 15844/30000 Training Loss: 0.03238619863986969\n",
      "Epoch 15845/30000 Training Loss: 0.0391194224357605\n",
      "Epoch 15846/30000 Training Loss: 0.03915748745203018\n",
      "Epoch 15847/30000 Training Loss: 0.0353657528758049\n",
      "Epoch 15848/30000 Training Loss: 0.0530582070350647\n",
      "Epoch 15849/30000 Training Loss: 0.0356387123465538\n",
      "Epoch 15850/30000 Training Loss: 0.039139874279499054\n",
      "Epoch 15851/30000 Training Loss: 0.05148473381996155\n",
      "Epoch 15852/30000 Training Loss: 0.061974816024303436\n",
      "Epoch 15853/30000 Training Loss: 0.04126858711242676\n",
      "Epoch 15854/30000 Training Loss: 0.0554947704076767\n",
      "Epoch 15855/30000 Training Loss: 0.03566281497478485\n",
      "Epoch 15856/30000 Training Loss: 0.035759132355451584\n",
      "Epoch 15857/30000 Training Loss: 0.041796740144491196\n",
      "Epoch 15858/30000 Training Loss: 0.04765398055315018\n",
      "Epoch 15859/30000 Training Loss: 0.03532665595412254\n",
      "Epoch 15860/30000 Training Loss: 0.04193982481956482\n",
      "Epoch 15861/30000 Training Loss: 0.05817165970802307\n",
      "Epoch 15862/30000 Training Loss: 0.03846759349107742\n",
      "Epoch 15863/30000 Training Loss: 0.04400685429573059\n",
      "Epoch 15864/30000 Training Loss: 0.061638087034225464\n",
      "Epoch 15865/30000 Training Loss: 0.05053526908159256\n",
      "Epoch 15866/30000 Training Loss: 0.05553862452507019\n",
      "Epoch 15867/30000 Training Loss: 0.03595957159996033\n",
      "Epoch 15868/30000 Training Loss: 0.052992187440395355\n",
      "Epoch 15869/30000 Training Loss: 0.047804296016693115\n",
      "Epoch 15870/30000 Training Loss: 0.055270105600357056\n",
      "Epoch 15871/30000 Training Loss: 0.044638492166996\n",
      "Epoch 15872/30000 Training Loss: 0.051578715443611145\n",
      "Epoch 15873/30000 Training Loss: 0.04120422154664993\n",
      "Epoch 15874/30000 Training Loss: 0.04102598503232002\n",
      "Epoch 15875/30000 Training Loss: 0.04083298146724701\n",
      "Epoch 15876/30000 Training Loss: 0.04445391148328781\n",
      "Epoch 15877/30000 Training Loss: 0.04945347458124161\n",
      "Epoch 15878/30000 Training Loss: 0.048965565860271454\n",
      "Epoch 15879/30000 Training Loss: 0.041251037269830704\n",
      "Epoch 15880/30000 Training Loss: 0.054310478270053864\n",
      "Epoch 15881/30000 Training Loss: 0.03436838090419769\n",
      "Epoch 15882/30000 Training Loss: 0.03902939334511757\n",
      "Epoch 15883/30000 Training Loss: 0.036390531808137894\n",
      "Epoch 15884/30000 Training Loss: 0.036345481872558594\n",
      "Epoch 15885/30000 Training Loss: 0.03324171528220177\n",
      "Epoch 15886/30000 Training Loss: 0.06241977959871292\n",
      "Epoch 15887/30000 Training Loss: 0.03788735345005989\n",
      "Epoch 15888/30000 Training Loss: 0.05248182639479637\n",
      "Epoch 15889/30000 Training Loss: 0.06306050717830658\n",
      "Epoch 15890/30000 Training Loss: 0.04338225722312927\n",
      "Epoch 15891/30000 Training Loss: 0.05292714387178421\n",
      "Epoch 15892/30000 Training Loss: 0.04040270671248436\n",
      "Epoch 15893/30000 Training Loss: 0.05593698471784592\n",
      "Epoch 15894/30000 Training Loss: 0.04415016993880272\n",
      "Epoch 15895/30000 Training Loss: 0.04978372901678085\n",
      "Epoch 15896/30000 Training Loss: 0.05005154013633728\n",
      "Epoch 15897/30000 Training Loss: 0.058251626789569855\n",
      "Epoch 15898/30000 Training Loss: 0.039702244102954865\n",
      "Epoch 15899/30000 Training Loss: 0.04095469415187836\n",
      "Epoch 15900/30000 Training Loss: 0.05330013483762741\n",
      "Epoch 15900/30000 Validation Loss: 0.03814181312918663\n",
      "Epoch 15901/30000 Training Loss: 0.055028725415468216\n",
      "Epoch 15902/30000 Training Loss: 0.05516307055950165\n",
      "Epoch 15903/30000 Training Loss: 0.047199562191963196\n",
      "Epoch 15904/30000 Training Loss: 0.04408960044384003\n",
      "Epoch 15905/30000 Training Loss: 0.041276901960372925\n",
      "Epoch 15906/30000 Training Loss: 0.044865213334560394\n",
      "Epoch 15907/30000 Training Loss: 0.050495266914367676\n",
      "Epoch 15908/30000 Training Loss: 0.050372734665870667\n",
      "Epoch 15909/30000 Training Loss: 0.05461723729968071\n",
      "Epoch 15910/30000 Training Loss: 0.047573380172252655\n",
      "Epoch 15911/30000 Training Loss: 0.05200277268886566\n",
      "Epoch 15912/30000 Training Loss: 0.033087506890296936\n",
      "Epoch 15913/30000 Training Loss: 0.03984692692756653\n",
      "Epoch 15914/30000 Training Loss: 0.0322687104344368\n",
      "Epoch 15915/30000 Training Loss: 0.04256116971373558\n",
      "Epoch 15916/30000 Training Loss: 0.036918386816978455\n",
      "Epoch 15917/30000 Training Loss: 0.04079161584377289\n",
      "Epoch 15918/30000 Training Loss: 0.041353076696395874\n",
      "Epoch 15919/30000 Training Loss: 0.0562184602022171\n",
      "Epoch 15920/30000 Training Loss: 0.048727139830589294\n",
      "Epoch 15921/30000 Training Loss: 0.049224093556404114\n",
      "Epoch 15922/30000 Training Loss: 0.06514744460582733\n",
      "Epoch 15923/30000 Training Loss: 0.041897956281900406\n",
      "Epoch 15924/30000 Training Loss: 0.045359332114458084\n",
      "Epoch 15925/30000 Training Loss: 0.05399482697248459\n",
      "Epoch 15926/30000 Training Loss: 0.039709147065877914\n",
      "Epoch 15927/30000 Training Loss: 0.048966340720653534\n",
      "Epoch 15928/30000 Training Loss: 0.03861818090081215\n",
      "Epoch 15929/30000 Training Loss: 0.05426891893148422\n",
      "Epoch 15930/30000 Training Loss: 0.05209093168377876\n",
      "Epoch 15931/30000 Training Loss: 0.05392437055706978\n",
      "Epoch 15932/30000 Training Loss: 0.03585801273584366\n",
      "Epoch 15933/30000 Training Loss: 0.04511794075369835\n",
      "Epoch 15934/30000 Training Loss: 0.05118214339017868\n",
      "Epoch 15935/30000 Training Loss: 0.04445335268974304\n",
      "Epoch 15936/30000 Training Loss: 0.03953632712364197\n",
      "Epoch 15937/30000 Training Loss: 0.048521772027015686\n",
      "Epoch 15938/30000 Training Loss: 0.045538268983364105\n",
      "Epoch 15939/30000 Training Loss: 0.045496635138988495\n",
      "Epoch 15940/30000 Training Loss: 0.054550714790821075\n",
      "Epoch 15941/30000 Training Loss: 0.04289624094963074\n",
      "Epoch 15942/30000 Training Loss: 0.04089076817035675\n",
      "Epoch 15943/30000 Training Loss: 0.03971826657652855\n",
      "Epoch 15944/30000 Training Loss: 0.04938468337059021\n",
      "Epoch 15945/30000 Training Loss: 0.04041910171508789\n",
      "Epoch 15946/30000 Training Loss: 0.03877316415309906\n",
      "Epoch 15947/30000 Training Loss: 0.04246346652507782\n",
      "Epoch 15948/30000 Training Loss: 0.04508440941572189\n",
      "Epoch 15949/30000 Training Loss: 0.04326317459344864\n",
      "Epoch 15950/30000 Training Loss: 0.03397602215409279\n",
      "Epoch 15951/30000 Training Loss: 0.04493330419063568\n",
      "Epoch 15952/30000 Training Loss: 0.055116187781095505\n",
      "Epoch 15953/30000 Training Loss: 0.044806018471717834\n",
      "Epoch 15954/30000 Training Loss: 0.044160522520542145\n",
      "Epoch 15955/30000 Training Loss: 0.05357731878757477\n",
      "Epoch 15956/30000 Training Loss: 0.04825032502412796\n",
      "Epoch 15957/30000 Training Loss: 0.05001161992549896\n",
      "Epoch 15958/30000 Training Loss: 0.04810065031051636\n",
      "Epoch 15959/30000 Training Loss: 0.036757949739694595\n",
      "Epoch 15960/30000 Training Loss: 0.049027591943740845\n",
      "Epoch 15961/30000 Training Loss: 0.05790571868419647\n",
      "Epoch 15962/30000 Training Loss: 0.04502906650304794\n",
      "Epoch 15963/30000 Training Loss: 0.05394596606492996\n",
      "Epoch 15964/30000 Training Loss: 0.05373210459947586\n",
      "Epoch 15965/30000 Training Loss: 0.041332345455884933\n",
      "Epoch 15966/30000 Training Loss: 0.035145726054906845\n",
      "Epoch 15967/30000 Training Loss: 0.037064503878355026\n",
      "Epoch 15968/30000 Training Loss: 0.0433831512928009\n",
      "Epoch 15969/30000 Training Loss: 0.042248718440532684\n",
      "Epoch 15970/30000 Training Loss: 0.045755598694086075\n",
      "Epoch 15971/30000 Training Loss: 0.051781199872493744\n",
      "Epoch 15972/30000 Training Loss: 0.04379729926586151\n",
      "Epoch 15973/30000 Training Loss: 0.03307173401117325\n",
      "Epoch 15974/30000 Training Loss: 0.0459698811173439\n",
      "Epoch 15975/30000 Training Loss: 0.04152889549732208\n",
      "Epoch 15976/30000 Training Loss: 0.04869874566793442\n",
      "Epoch 15977/30000 Training Loss: 0.05363241583108902\n",
      "Epoch 15978/30000 Training Loss: 0.05151329189538956\n",
      "Epoch 15979/30000 Training Loss: 0.05093713104724884\n",
      "Epoch 15980/30000 Training Loss: 0.04295901954174042\n",
      "Epoch 15981/30000 Training Loss: 0.04211990907788277\n",
      "Epoch 15982/30000 Training Loss: 0.042350977659225464\n",
      "Epoch 15983/30000 Training Loss: 0.04505852609872818\n",
      "Epoch 15984/30000 Training Loss: 0.05509313941001892\n",
      "Epoch 15985/30000 Training Loss: 0.04336874559521675\n",
      "Epoch 15986/30000 Training Loss: 0.05262095481157303\n",
      "Epoch 15987/30000 Training Loss: 0.0474039688706398\n",
      "Epoch 15988/30000 Training Loss: 0.03326031193137169\n",
      "Epoch 15989/30000 Training Loss: 0.04208195209503174\n",
      "Epoch 15990/30000 Training Loss: 0.04235149547457695\n",
      "Epoch 15991/30000 Training Loss: 0.03053177520632744\n",
      "Epoch 15992/30000 Training Loss: 0.04669187217950821\n",
      "Epoch 15993/30000 Training Loss: 0.05959023907780647\n",
      "Epoch 15994/30000 Training Loss: 0.04813165217638016\n",
      "Epoch 15995/30000 Training Loss: 0.05153971537947655\n",
      "Epoch 15996/30000 Training Loss: 0.046087637543678284\n",
      "Epoch 15997/30000 Training Loss: 0.055841609835624695\n",
      "Epoch 15998/30000 Training Loss: 0.043928489089012146\n",
      "Epoch 15999/30000 Training Loss: 0.04535015672445297\n",
      "Epoch 16000/30000 Training Loss: 0.06507724523544312\n",
      "Epoch 16000/30000 Validation Loss: 0.05243773013353348\n",
      "Epoch 16001/30000 Training Loss: 0.03763680160045624\n",
      "Epoch 16002/30000 Training Loss: 0.03990786522626877\n",
      "Epoch 16003/30000 Training Loss: 0.039943352341651917\n",
      "Epoch 16004/30000 Training Loss: 0.05119635537266731\n",
      "Epoch 16005/30000 Training Loss: 0.03874203935265541\n",
      "Epoch 16006/30000 Training Loss: 0.03821403160691261\n",
      "Epoch 16007/30000 Training Loss: 0.030673453584313393\n",
      "Epoch 16008/30000 Training Loss: 0.04618459939956665\n",
      "Epoch 16009/30000 Training Loss: 0.03969505429267883\n",
      "Epoch 16010/30000 Training Loss: 0.0464782789349556\n",
      "Epoch 16011/30000 Training Loss: 0.05198906362056732\n",
      "Epoch 16012/30000 Training Loss: 0.03688088431954384\n",
      "Epoch 16013/30000 Training Loss: 0.035091668367385864\n",
      "Epoch 16014/30000 Training Loss: 0.04972616583108902\n",
      "Epoch 16015/30000 Training Loss: 0.04778406023979187\n",
      "Epoch 16016/30000 Training Loss: 0.03736448287963867\n",
      "Epoch 16017/30000 Training Loss: 0.05064123868942261\n",
      "Epoch 16018/30000 Training Loss: 0.046754539012908936\n",
      "Epoch 16019/30000 Training Loss: 0.04546491801738739\n",
      "Epoch 16020/30000 Training Loss: 0.0493999645113945\n",
      "Epoch 16021/30000 Training Loss: 0.052617400884628296\n",
      "Epoch 16022/30000 Training Loss: 0.04344465583562851\n",
      "Epoch 16023/30000 Training Loss: 0.047997307032346725\n",
      "Epoch 16024/30000 Training Loss: 0.038212597370147705\n",
      "Epoch 16025/30000 Training Loss: 0.045253098011016846\n",
      "Epoch 16026/30000 Training Loss: 0.04772409051656723\n",
      "Epoch 16027/30000 Training Loss: 0.03742676600813866\n",
      "Epoch 16028/30000 Training Loss: 0.042990922927856445\n",
      "Epoch 16029/30000 Training Loss: 0.050275806337594986\n",
      "Epoch 16030/30000 Training Loss: 0.03905573487281799\n",
      "Epoch 16031/30000 Training Loss: 0.042347729206085205\n",
      "Epoch 16032/30000 Training Loss: 0.04329385608434677\n",
      "Epoch 16033/30000 Training Loss: 0.04579246789216995\n",
      "Epoch 16034/30000 Training Loss: 0.052450522780418396\n",
      "Epoch 16035/30000 Training Loss: 0.045032769441604614\n",
      "Epoch 16036/30000 Training Loss: 0.03396602347493172\n",
      "Epoch 16037/30000 Training Loss: 0.037796009331941605\n",
      "Epoch 16038/30000 Training Loss: 0.041228123009204865\n",
      "Epoch 16039/30000 Training Loss: 0.03543693944811821\n",
      "Epoch 16040/30000 Training Loss: 0.05409688875079155\n",
      "Epoch 16041/30000 Training Loss: 0.047293417155742645\n",
      "Epoch 16042/30000 Training Loss: 0.046322524547576904\n",
      "Epoch 16043/30000 Training Loss: 0.06481227278709412\n",
      "Epoch 16044/30000 Training Loss: 0.050409309566020966\n",
      "Epoch 16045/30000 Training Loss: 0.05088246613740921\n",
      "Epoch 16046/30000 Training Loss: 0.06258085370063782\n",
      "Epoch 16047/30000 Training Loss: 0.05291862040758133\n",
      "Epoch 16048/30000 Training Loss: 0.04650793597102165\n",
      "Epoch 16049/30000 Training Loss: 0.03657685965299606\n",
      "Epoch 16050/30000 Training Loss: 0.05938281863927841\n",
      "Epoch 16051/30000 Training Loss: 0.04471568018198013\n",
      "Epoch 16052/30000 Training Loss: 0.04943076893687248\n",
      "Epoch 16053/30000 Training Loss: 0.037027619779109955\n",
      "Epoch 16054/30000 Training Loss: 0.049970291554927826\n",
      "Epoch 16055/30000 Training Loss: 0.0554191954433918\n",
      "Epoch 16056/30000 Training Loss: 0.04685471951961517\n",
      "Epoch 16057/30000 Training Loss: 0.043514497578144073\n",
      "Epoch 16058/30000 Training Loss: 0.03361642360687256\n",
      "Epoch 16059/30000 Training Loss: 0.04094817489385605\n",
      "Epoch 16060/30000 Training Loss: 0.053006626665592194\n",
      "Epoch 16061/30000 Training Loss: 0.05573485791683197\n",
      "Epoch 16062/30000 Training Loss: 0.05530261993408203\n",
      "Epoch 16063/30000 Training Loss: 0.03420310467481613\n",
      "Epoch 16064/30000 Training Loss: 0.05346321314573288\n",
      "Epoch 16065/30000 Training Loss: 0.04369145259261131\n",
      "Epoch 16066/30000 Training Loss: 0.04691951721906662\n",
      "Epoch 16067/30000 Training Loss: 0.059314459562301636\n",
      "Epoch 16068/30000 Training Loss: 0.04874464496970177\n",
      "Epoch 16069/30000 Training Loss: 0.0518057718873024\n",
      "Epoch 16070/30000 Training Loss: 0.051501017063856125\n",
      "Epoch 16071/30000 Training Loss: 0.03627720847725868\n",
      "Epoch 16072/30000 Training Loss: 0.04747026041150093\n",
      "Epoch 16073/30000 Training Loss: 0.04552435129880905\n",
      "Epoch 16074/30000 Training Loss: 0.041654668748378754\n",
      "Epoch 16075/30000 Training Loss: 0.040969207882881165\n",
      "Epoch 16076/30000 Training Loss: 0.04505108296871185\n",
      "Epoch 16077/30000 Training Loss: 0.04567846655845642\n",
      "Epoch 16078/30000 Training Loss: 0.04254649206995964\n",
      "Epoch 16079/30000 Training Loss: 0.050943806767463684\n",
      "Epoch 16080/30000 Training Loss: 0.052516866475343704\n",
      "Epoch 16081/30000 Training Loss: 0.04999837279319763\n",
      "Epoch 16082/30000 Training Loss: 0.054008230566978455\n",
      "Epoch 16083/30000 Training Loss: 0.04241732507944107\n",
      "Epoch 16084/30000 Training Loss: 0.03648516908288002\n",
      "Epoch 16085/30000 Training Loss: 0.04965901002287865\n",
      "Epoch 16086/30000 Training Loss: 0.04005066677927971\n",
      "Epoch 16087/30000 Training Loss: 0.054834187030792236\n",
      "Epoch 16088/30000 Training Loss: 0.04668576642870903\n",
      "Epoch 16089/30000 Training Loss: 0.04133584350347519\n",
      "Epoch 16090/30000 Training Loss: 0.05292653292417526\n",
      "Epoch 16091/30000 Training Loss: 0.05195041745901108\n",
      "Epoch 16092/30000 Training Loss: 0.04002746194601059\n",
      "Epoch 16093/30000 Training Loss: 0.04669390246272087\n",
      "Epoch 16094/30000 Training Loss: 0.03551563620567322\n",
      "Epoch 16095/30000 Training Loss: 0.056682199239730835\n",
      "Epoch 16096/30000 Training Loss: 0.05823370814323425\n",
      "Epoch 16097/30000 Training Loss: 0.05069781467318535\n",
      "Epoch 16098/30000 Training Loss: 0.03340878337621689\n",
      "Epoch 16099/30000 Training Loss: 0.0320575013756752\n",
      "Epoch 16100/30000 Training Loss: 0.04572484269738197\n",
      "Epoch 16100/30000 Validation Loss: 0.05082056671380997\n",
      "Epoch 16101/30000 Training Loss: 0.03733573481440544\n",
      "Epoch 16102/30000 Training Loss: 0.04774121940135956\n",
      "Epoch 16103/30000 Training Loss: 0.0406641848385334\n",
      "Epoch 16104/30000 Training Loss: 0.03938183933496475\n",
      "Epoch 16105/30000 Training Loss: 0.04532415047287941\n",
      "Epoch 16106/30000 Training Loss: 0.047700218856334686\n",
      "Epoch 16107/30000 Training Loss: 0.05080769956111908\n",
      "Epoch 16108/30000 Training Loss: 0.03260073810815811\n",
      "Epoch 16109/30000 Training Loss: 0.04571206122636795\n",
      "Epoch 16110/30000 Training Loss: 0.043535225093364716\n",
      "Epoch 16111/30000 Training Loss: 0.04833623021841049\n",
      "Epoch 16112/30000 Training Loss: 0.03765494003891945\n",
      "Epoch 16113/30000 Training Loss: 0.0448574498295784\n",
      "Epoch 16114/30000 Training Loss: 0.04318833351135254\n",
      "Epoch 16115/30000 Training Loss: 0.047080982476472855\n",
      "Epoch 16116/30000 Training Loss: 0.04998120293021202\n",
      "Epoch 16117/30000 Training Loss: 0.04492839798331261\n",
      "Epoch 16118/30000 Training Loss: 0.058784957975149155\n",
      "Epoch 16119/30000 Training Loss: 0.041016459465026855\n",
      "Epoch 16120/30000 Training Loss: 0.05513946712017059\n",
      "Epoch 16121/30000 Training Loss: 0.04237869009375572\n",
      "Epoch 16122/30000 Training Loss: 0.04470854625105858\n",
      "Epoch 16123/30000 Training Loss: 0.033413052558898926\n",
      "Epoch 16124/30000 Training Loss: 0.043037548661231995\n",
      "Epoch 16125/30000 Training Loss: 0.03644739091396332\n",
      "Epoch 16126/30000 Training Loss: 0.04836738854646683\n",
      "Epoch 16127/30000 Training Loss: 0.04890873283147812\n",
      "Epoch 16128/30000 Training Loss: 0.051431164145469666\n",
      "Epoch 16129/30000 Training Loss: 0.0456765741109848\n",
      "Epoch 16130/30000 Training Loss: 0.04702959954738617\n",
      "Epoch 16131/30000 Training Loss: 0.04649728909134865\n",
      "Epoch 16132/30000 Training Loss: 0.050178997218608856\n",
      "Epoch 16133/30000 Training Loss: 0.058668218553066254\n",
      "Epoch 16134/30000 Training Loss: 0.054630473256111145\n",
      "Epoch 16135/30000 Training Loss: 0.032134197652339935\n",
      "Epoch 16136/30000 Training Loss: 0.03294721245765686\n",
      "Epoch 16137/30000 Training Loss: 0.04628925025463104\n",
      "Epoch 16138/30000 Training Loss: 0.04019778594374657\n",
      "Epoch 16139/30000 Training Loss: 0.04779115691781044\n",
      "Epoch 16140/30000 Training Loss: 0.05014042556285858\n",
      "Epoch 16141/30000 Training Loss: 0.033964525908231735\n",
      "Epoch 16142/30000 Training Loss: 0.03133641183376312\n",
      "Epoch 16143/30000 Training Loss: 0.0604865737259388\n",
      "Epoch 16144/30000 Training Loss: 0.035043854266405106\n",
      "Epoch 16145/30000 Training Loss: 0.04047255963087082\n",
      "Epoch 16146/30000 Training Loss: 0.03992772847414017\n",
      "Epoch 16147/30000 Training Loss: 0.041066497564315796\n",
      "Epoch 16148/30000 Training Loss: 0.052331022918224335\n",
      "Epoch 16149/30000 Training Loss: 0.048640813678503036\n",
      "Epoch 16150/30000 Training Loss: 0.042877331376075745\n",
      "Epoch 16151/30000 Training Loss: 0.04519885778427124\n",
      "Epoch 16152/30000 Training Loss: 0.06358269602060318\n",
      "Epoch 16153/30000 Training Loss: 0.04419070482254028\n",
      "Epoch 16154/30000 Training Loss: 0.045553043484687805\n",
      "Epoch 16155/30000 Training Loss: 0.07027040421962738\n",
      "Epoch 16156/30000 Training Loss: 0.05755746364593506\n",
      "Epoch 16157/30000 Training Loss: 0.04936350882053375\n",
      "Epoch 16158/30000 Training Loss: 0.039066381752491\n",
      "Epoch 16159/30000 Training Loss: 0.03349074721336365\n",
      "Epoch 16160/30000 Training Loss: 0.06543044000864029\n",
      "Epoch 16161/30000 Training Loss: 0.042555730789899826\n",
      "Epoch 16162/30000 Training Loss: 0.047630999237298965\n",
      "Epoch 16163/30000 Training Loss: 0.04861003905534744\n",
      "Epoch 16164/30000 Training Loss: 0.04595063254237175\n",
      "Epoch 16165/30000 Training Loss: 0.0630967766046524\n",
      "Epoch 16166/30000 Training Loss: 0.05690266937017441\n",
      "Epoch 16167/30000 Training Loss: 0.045251328498125076\n",
      "Epoch 16168/30000 Training Loss: 0.06366953998804092\n",
      "Epoch 16169/30000 Training Loss: 0.051430441439151764\n",
      "Epoch 16170/30000 Training Loss: 0.04842240363359451\n",
      "Epoch 16171/30000 Training Loss: 0.04813200235366821\n",
      "Epoch 16172/30000 Training Loss: 0.0513690710067749\n",
      "Epoch 16173/30000 Training Loss: 0.040315669029951096\n",
      "Epoch 16174/30000 Training Loss: 0.05481567233800888\n",
      "Epoch 16175/30000 Training Loss: 0.04507829248905182\n",
      "Epoch 16176/30000 Training Loss: 0.041866734623909\n",
      "Epoch 16177/30000 Training Loss: 0.0509362518787384\n",
      "Epoch 16178/30000 Training Loss: 0.04420708119869232\n",
      "Epoch 16179/30000 Training Loss: 0.037579216063022614\n",
      "Epoch 16180/30000 Training Loss: 0.03862999007105827\n",
      "Epoch 16181/30000 Training Loss: 0.034815989434719086\n",
      "Epoch 16182/30000 Training Loss: 0.04425808787345886\n",
      "Epoch 16183/30000 Training Loss: 0.03912469744682312\n",
      "Epoch 16184/30000 Training Loss: 0.054726772010326385\n",
      "Epoch 16185/30000 Training Loss: 0.033323876559734344\n",
      "Epoch 16186/30000 Training Loss: 0.047591209411621094\n",
      "Epoch 16187/30000 Training Loss: 0.06588765978813171\n",
      "Epoch 16188/30000 Training Loss: 0.04441055655479431\n",
      "Epoch 16189/30000 Training Loss: 0.049511946737766266\n",
      "Epoch 16190/30000 Training Loss: 0.04013631120324135\n",
      "Epoch 16191/30000 Training Loss: 0.04605960100889206\n",
      "Epoch 16192/30000 Training Loss: 0.05956455320119858\n",
      "Epoch 16193/30000 Training Loss: 0.047957953065633774\n",
      "Epoch 16194/30000 Training Loss: 0.042861443012952805\n",
      "Epoch 16195/30000 Training Loss: 0.037713684141635895\n",
      "Epoch 16196/30000 Training Loss: 0.04914131015539169\n",
      "Epoch 16197/30000 Training Loss: 0.035645611584186554\n",
      "Epoch 16198/30000 Training Loss: 0.04039355739951134\n",
      "Epoch 16199/30000 Training Loss: 0.051570553332567215\n",
      "Epoch 16200/30000 Training Loss: 0.03542790561914444\n",
      "Epoch 16200/30000 Validation Loss: 0.045264191925525665\n",
      "Epoch 16201/30000 Training Loss: 0.04801984876394272\n",
      "Epoch 16202/30000 Training Loss: 0.043190449476242065\n",
      "Epoch 16203/30000 Training Loss: 0.06621290743350983\n",
      "Epoch 16204/30000 Training Loss: 0.046440187841653824\n",
      "Epoch 16205/30000 Training Loss: 0.05647993087768555\n",
      "Epoch 16206/30000 Training Loss: 0.03715997189283371\n",
      "Epoch 16207/30000 Training Loss: 0.04619748145341873\n",
      "Epoch 16208/30000 Training Loss: 0.06111559271812439\n",
      "Epoch 16209/30000 Training Loss: 0.044279083609580994\n",
      "Epoch 16210/30000 Training Loss: 0.04563166946172714\n",
      "Epoch 16211/30000 Training Loss: 0.04757169634103775\n",
      "Epoch 16212/30000 Training Loss: 0.04558835178613663\n",
      "Epoch 16213/30000 Training Loss: 0.0471055805683136\n",
      "Epoch 16214/30000 Training Loss: 0.036774419248104095\n",
      "Epoch 16215/30000 Training Loss: 0.04584576189517975\n",
      "Epoch 16216/30000 Training Loss: 0.042226649820804596\n",
      "Epoch 16217/30000 Training Loss: 0.04073981195688248\n",
      "Epoch 16218/30000 Training Loss: 0.03728531673550606\n",
      "Epoch 16219/30000 Training Loss: 0.057704437524080276\n",
      "Epoch 16220/30000 Training Loss: 0.0482185073196888\n",
      "Epoch 16221/30000 Training Loss: 0.034574925899505615\n",
      "Epoch 16222/30000 Training Loss: 0.041210778057575226\n",
      "Epoch 16223/30000 Training Loss: 0.039609089493751526\n",
      "Epoch 16224/30000 Training Loss: 0.04887910932302475\n",
      "Epoch 16225/30000 Training Loss: 0.04634479060769081\n",
      "Epoch 16226/30000 Training Loss: 0.03508966416120529\n",
      "Epoch 16227/30000 Training Loss: 0.039288636296987534\n",
      "Epoch 16228/30000 Training Loss: 0.038278017193078995\n",
      "Epoch 16229/30000 Training Loss: 0.04022572189569473\n",
      "Epoch 16230/30000 Training Loss: 0.03819264471530914\n",
      "Epoch 16231/30000 Training Loss: 0.04147735983133316\n",
      "Epoch 16232/30000 Training Loss: 0.054946526885032654\n",
      "Epoch 16233/30000 Training Loss: 0.047456368803977966\n",
      "Epoch 16234/30000 Training Loss: 0.05108485370874405\n",
      "Epoch 16235/30000 Training Loss: 0.051929786801338196\n",
      "Epoch 16236/30000 Training Loss: 0.044784508645534515\n",
      "Epoch 16237/30000 Training Loss: 0.03764546290040016\n",
      "Epoch 16238/30000 Training Loss: 0.04615328460931778\n",
      "Epoch 16239/30000 Training Loss: 0.05901844799518585\n",
      "Epoch 16240/30000 Training Loss: 0.03786277770996094\n",
      "Epoch 16241/30000 Training Loss: 0.04597863927483559\n",
      "Epoch 16242/30000 Training Loss: 0.03836451470851898\n",
      "Epoch 16243/30000 Training Loss: 0.04285389184951782\n",
      "Epoch 16244/30000 Training Loss: 0.05016005411744118\n",
      "Epoch 16245/30000 Training Loss: 0.04644099622964859\n",
      "Epoch 16246/30000 Training Loss: 0.03532908111810684\n",
      "Epoch 16247/30000 Training Loss: 0.04975114017724991\n",
      "Epoch 16248/30000 Training Loss: 0.04864392802119255\n",
      "Epoch 16249/30000 Training Loss: 0.04097440838813782\n",
      "Epoch 16250/30000 Training Loss: 0.04594026505947113\n",
      "Epoch 16251/30000 Training Loss: 0.04965076595544815\n",
      "Epoch 16252/30000 Training Loss: 0.05377978831529617\n",
      "Epoch 16253/30000 Training Loss: 0.033053938299417496\n",
      "Epoch 16254/30000 Training Loss: 0.052763111889362335\n",
      "Epoch 16255/30000 Training Loss: 0.04759065806865692\n",
      "Epoch 16256/30000 Training Loss: 0.061426013708114624\n",
      "Epoch 16257/30000 Training Loss: 0.054573990404605865\n",
      "Epoch 16258/30000 Training Loss: 0.04196611046791077\n",
      "Epoch 16259/30000 Training Loss: 0.03478115051984787\n",
      "Epoch 16260/30000 Training Loss: 0.04132416471838951\n",
      "Epoch 16261/30000 Training Loss: 0.03823667764663696\n",
      "Epoch 16262/30000 Training Loss: 0.03998885303735733\n",
      "Epoch 16263/30000 Training Loss: 0.05225309357047081\n",
      "Epoch 16264/30000 Training Loss: 0.04933986812829971\n",
      "Epoch 16265/30000 Training Loss: 0.04017500579357147\n",
      "Epoch 16266/30000 Training Loss: 0.05153145641088486\n",
      "Epoch 16267/30000 Training Loss: 0.05455394461750984\n",
      "Epoch 16268/30000 Training Loss: 0.038055140525102615\n",
      "Epoch 16269/30000 Training Loss: 0.06430300325155258\n",
      "Epoch 16270/30000 Training Loss: 0.03748669847846031\n",
      "Epoch 16271/30000 Training Loss: 0.05745173245668411\n",
      "Epoch 16272/30000 Training Loss: 0.04450265318155289\n",
      "Epoch 16273/30000 Training Loss: 0.03211623430252075\n",
      "Epoch 16274/30000 Training Loss: 0.04135199636220932\n",
      "Epoch 16275/30000 Training Loss: 0.046490393579006195\n",
      "Epoch 16276/30000 Training Loss: 0.030523423105478287\n",
      "Epoch 16277/30000 Training Loss: 0.02926371619105339\n",
      "Epoch 16278/30000 Training Loss: 0.04466373473405838\n",
      "Epoch 16279/30000 Training Loss: 0.045597441494464874\n",
      "Epoch 16280/30000 Training Loss: 0.05653466284275055\n",
      "Epoch 16281/30000 Training Loss: 0.03570135682821274\n",
      "Epoch 16282/30000 Training Loss: 0.04723791778087616\n",
      "Epoch 16283/30000 Training Loss: 0.03536589443683624\n",
      "Epoch 16284/30000 Training Loss: 0.05030056834220886\n",
      "Epoch 16285/30000 Training Loss: 0.05530574917793274\n",
      "Epoch 16286/30000 Training Loss: 0.051138538867235184\n",
      "Epoch 16287/30000 Training Loss: 0.05155191570520401\n",
      "Epoch 16288/30000 Training Loss: 0.04002712294459343\n",
      "Epoch 16289/30000 Training Loss: 0.045777034014463425\n",
      "Epoch 16290/30000 Training Loss: 0.04622139781713486\n",
      "Epoch 16291/30000 Training Loss: 0.046167511492967606\n",
      "Epoch 16292/30000 Training Loss: 0.042591169476509094\n",
      "Epoch 16293/30000 Training Loss: 0.03378769010305405\n",
      "Epoch 16294/30000 Training Loss: 0.05580303445458412\n",
      "Epoch 16295/30000 Training Loss: 0.049074091017246246\n",
      "Epoch 16296/30000 Training Loss: 0.04915691167116165\n",
      "Epoch 16297/30000 Training Loss: 0.03986179083585739\n",
      "Epoch 16298/30000 Training Loss: 0.035585030913352966\n",
      "Epoch 16299/30000 Training Loss: 0.037621740251779556\n",
      "Epoch 16300/30000 Training Loss: 0.038810499012470245\n",
      "Epoch 16300/30000 Validation Loss: 0.041703082621097565\n",
      "Epoch 16301/30000 Training Loss: 0.03490723669528961\n",
      "Epoch 16302/30000 Training Loss: 0.048175148665905\n",
      "Epoch 16303/30000 Training Loss: 0.06774278730154037\n",
      "Epoch 16304/30000 Training Loss: 0.04499775171279907\n",
      "Epoch 16305/30000 Training Loss: 0.05015202611684799\n",
      "Epoch 16306/30000 Training Loss: 0.051961228251457214\n",
      "Epoch 16307/30000 Training Loss: 0.04289902374148369\n",
      "Epoch 16308/30000 Training Loss: 0.05120229721069336\n",
      "Epoch 16309/30000 Training Loss: 0.0395563468337059\n",
      "Epoch 16310/30000 Training Loss: 0.04839298501610756\n",
      "Epoch 16311/30000 Training Loss: 0.05850023031234741\n",
      "Epoch 16312/30000 Training Loss: 0.05614767596125603\n",
      "Epoch 16313/30000 Training Loss: 0.04347006231546402\n",
      "Epoch 16314/30000 Training Loss: 0.04181176424026489\n",
      "Epoch 16315/30000 Training Loss: 0.044258326292037964\n",
      "Epoch 16316/30000 Training Loss: 0.04299898445606232\n",
      "Epoch 16317/30000 Training Loss: 0.04120379686355591\n",
      "Epoch 16318/30000 Training Loss: 0.051357824355363846\n",
      "Epoch 16319/30000 Training Loss: 0.03909420967102051\n",
      "Epoch 16320/30000 Training Loss: 0.052749618887901306\n",
      "Epoch 16321/30000 Training Loss: 0.03259838744997978\n",
      "Epoch 16322/30000 Training Loss: 0.039945751428604126\n",
      "Epoch 16323/30000 Training Loss: 0.03564760088920593\n",
      "Epoch 16324/30000 Training Loss: 0.06779710948467255\n",
      "Epoch 16325/30000 Training Loss: 0.04356808215379715\n",
      "Epoch 16326/30000 Training Loss: 0.04673003777861595\n",
      "Epoch 16327/30000 Training Loss: 0.04879341274499893\n",
      "Epoch 16328/30000 Training Loss: 0.048412978649139404\n",
      "Epoch 16329/30000 Training Loss: 0.045803628861904144\n",
      "Epoch 16330/30000 Training Loss: 0.03913404792547226\n",
      "Epoch 16331/30000 Training Loss: 0.04314229264855385\n",
      "Epoch 16332/30000 Training Loss: 0.0478726327419281\n",
      "Epoch 16333/30000 Training Loss: 0.03807650879025459\n",
      "Epoch 16334/30000 Training Loss: 0.04679391533136368\n",
      "Epoch 16335/30000 Training Loss: 0.04039899259805679\n",
      "Epoch 16336/30000 Training Loss: 0.04918103665113449\n",
      "Epoch 16337/30000 Training Loss: 0.04954088106751442\n",
      "Epoch 16338/30000 Training Loss: 0.034767284989356995\n",
      "Epoch 16339/30000 Training Loss: 0.027509894222021103\n",
      "Epoch 16340/30000 Training Loss: 0.03484068810939789\n",
      "Epoch 16341/30000 Training Loss: 0.05072794109582901\n",
      "Epoch 16342/30000 Training Loss: 0.031233711168169975\n",
      "Epoch 16343/30000 Training Loss: 0.041635215282440186\n",
      "Epoch 16344/30000 Training Loss: 0.04553930088877678\n",
      "Epoch 16345/30000 Training Loss: 0.056852929294109344\n",
      "Epoch 16346/30000 Training Loss: 0.043665606528520584\n",
      "Epoch 16347/30000 Training Loss: 0.02819819748401642\n",
      "Epoch 16348/30000 Training Loss: 0.04883384704589844\n",
      "Epoch 16349/30000 Training Loss: 0.0532800629734993\n",
      "Epoch 16350/30000 Training Loss: 0.06328512728214264\n",
      "Epoch 16351/30000 Training Loss: 0.03814655542373657\n",
      "Epoch 16352/30000 Training Loss: 0.04118438437581062\n",
      "Epoch 16353/30000 Training Loss: 0.045769352465867996\n",
      "Epoch 16354/30000 Training Loss: 0.039065681397914886\n",
      "Epoch 16355/30000 Training Loss: 0.04903361573815346\n",
      "Epoch 16356/30000 Training Loss: 0.0418546199798584\n",
      "Epoch 16357/30000 Training Loss: 0.05135931074619293\n",
      "Epoch 16358/30000 Training Loss: 0.03250579535961151\n",
      "Epoch 16359/30000 Training Loss: 0.035810623317956924\n",
      "Epoch 16360/30000 Training Loss: 0.04403198882937431\n",
      "Epoch 16361/30000 Training Loss: 0.03634984418749809\n",
      "Epoch 16362/30000 Training Loss: 0.047251127660274506\n",
      "Epoch 16363/30000 Training Loss: 0.04397457838058472\n",
      "Epoch 16364/30000 Training Loss: 0.03533501923084259\n",
      "Epoch 16365/30000 Training Loss: 0.03126886487007141\n",
      "Epoch 16366/30000 Training Loss: 0.035307832062244415\n",
      "Epoch 16367/30000 Training Loss: 0.0537974052131176\n",
      "Epoch 16368/30000 Training Loss: 0.04763617366552353\n",
      "Epoch 16369/30000 Training Loss: 0.03890003263950348\n",
      "Epoch 16370/30000 Training Loss: 0.04029045253992081\n",
      "Epoch 16371/30000 Training Loss: 0.060803961008787155\n",
      "Epoch 16372/30000 Training Loss: 0.03905308246612549\n",
      "Epoch 16373/30000 Training Loss: 0.039552364498376846\n",
      "Epoch 16374/30000 Training Loss: 0.031564854085445404\n",
      "Epoch 16375/30000 Training Loss: 0.04045936465263367\n",
      "Epoch 16376/30000 Training Loss: 0.0648382306098938\n",
      "Epoch 16377/30000 Training Loss: 0.027299143373966217\n",
      "Epoch 16378/30000 Training Loss: 0.03850620985031128\n",
      "Epoch 16379/30000 Training Loss: 0.048514820635318756\n",
      "Epoch 16380/30000 Training Loss: 0.044028978794813156\n",
      "Epoch 16381/30000 Training Loss: 0.05281374230980873\n",
      "Epoch 16382/30000 Training Loss: 0.03441093489527702\n",
      "Epoch 16383/30000 Training Loss: 0.051923371851444244\n",
      "Epoch 16384/30000 Training Loss: 0.042713768780231476\n",
      "Epoch 16385/30000 Training Loss: 0.03616917133331299\n",
      "Epoch 16386/30000 Training Loss: 0.04073582962155342\n",
      "Epoch 16387/30000 Training Loss: 0.03279060125350952\n",
      "Epoch 16388/30000 Training Loss: 0.05035756528377533\n",
      "Epoch 16389/30000 Training Loss: 0.04326805844902992\n",
      "Epoch 16390/30000 Training Loss: 0.037465836852788925\n",
      "Epoch 16391/30000 Training Loss: 0.04004456847906113\n",
      "Epoch 16392/30000 Training Loss: 0.05090079456567764\n",
      "Epoch 16393/30000 Training Loss: 0.050342679023742676\n",
      "Epoch 16394/30000 Training Loss: 0.04067914932966232\n",
      "Epoch 16395/30000 Training Loss: 0.041931185871362686\n",
      "Epoch 16396/30000 Training Loss: 0.04396701604127884\n",
      "Epoch 16397/30000 Training Loss: 0.037207163870334625\n",
      "Epoch 16398/30000 Training Loss: 0.04087042808532715\n",
      "Epoch 16399/30000 Training Loss: 0.039500750601291656\n",
      "Epoch 16400/30000 Training Loss: 0.04007228463888168\n",
      "Epoch 16400/30000 Validation Loss: 0.04960206151008606\n",
      "Epoch 16401/30000 Training Loss: 0.057435862720012665\n",
      "Epoch 16402/30000 Training Loss: 0.03990798443555832\n",
      "Epoch 16403/30000 Training Loss: 0.051739402115345\n",
      "Epoch 16404/30000 Training Loss: 0.043358974158763885\n",
      "Epoch 16405/30000 Training Loss: 0.042571622878313065\n",
      "Epoch 16406/30000 Training Loss: 0.054830826818943024\n",
      "Epoch 16407/30000 Training Loss: 0.06244233250617981\n",
      "Epoch 16408/30000 Training Loss: 0.05634240061044693\n",
      "Epoch 16409/30000 Training Loss: 0.04930681735277176\n",
      "Epoch 16410/30000 Training Loss: 0.04891142249107361\n",
      "Epoch 16411/30000 Training Loss: 0.04371817037463188\n",
      "Epoch 16412/30000 Training Loss: 0.05802817642688751\n",
      "Epoch 16413/30000 Training Loss: 0.03446991369128227\n",
      "Epoch 16414/30000 Training Loss: 0.03864549472928047\n",
      "Epoch 16415/30000 Training Loss: 0.04269042983651161\n",
      "Epoch 16416/30000 Training Loss: 0.04790585860610008\n",
      "Epoch 16417/30000 Training Loss: 0.04986053705215454\n",
      "Epoch 16418/30000 Training Loss: 0.056229688227176666\n",
      "Epoch 16419/30000 Training Loss: 0.04653319716453552\n",
      "Epoch 16420/30000 Training Loss: 0.039881493896245956\n",
      "Epoch 16421/30000 Training Loss: 0.03709721937775612\n",
      "Epoch 16422/30000 Training Loss: 0.047678738832473755\n",
      "Epoch 16423/30000 Training Loss: 0.057068515568971634\n",
      "Epoch 16424/30000 Training Loss: 0.04597020521759987\n",
      "Epoch 16425/30000 Training Loss: 0.04216725751757622\n",
      "Epoch 16426/30000 Training Loss: 0.04183715581893921\n",
      "Epoch 16427/30000 Training Loss: 0.046874579042196274\n",
      "Epoch 16428/30000 Training Loss: 0.046020254492759705\n",
      "Epoch 16429/30000 Training Loss: 0.048286497592926025\n",
      "Epoch 16430/30000 Training Loss: 0.03608115389943123\n",
      "Epoch 16431/30000 Training Loss: 0.03243584930896759\n",
      "Epoch 16432/30000 Training Loss: 0.05205987021327019\n",
      "Epoch 16433/30000 Training Loss: 0.05686900392174721\n",
      "Epoch 16434/30000 Training Loss: 0.03459591045975685\n",
      "Epoch 16435/30000 Training Loss: 0.04045693948864937\n",
      "Epoch 16436/30000 Training Loss: 0.044232871383428574\n",
      "Epoch 16437/30000 Training Loss: 0.05266689136624336\n",
      "Epoch 16438/30000 Training Loss: 0.04248519614338875\n",
      "Epoch 16439/30000 Training Loss: 0.04176142066717148\n",
      "Epoch 16440/30000 Training Loss: 0.0402919203042984\n",
      "Epoch 16441/30000 Training Loss: 0.042075738310813904\n",
      "Epoch 16442/30000 Training Loss: 0.038460537791252136\n",
      "Epoch 16443/30000 Training Loss: 0.04441475868225098\n",
      "Epoch 16444/30000 Training Loss: 0.04561997577548027\n",
      "Epoch 16445/30000 Training Loss: 0.05463160574436188\n",
      "Epoch 16446/30000 Training Loss: 0.04664607346057892\n",
      "Epoch 16447/30000 Training Loss: 0.04378480091691017\n",
      "Epoch 16448/30000 Training Loss: 0.037086956202983856\n",
      "Epoch 16449/30000 Training Loss: 0.053421296179294586\n",
      "Epoch 16450/30000 Training Loss: 0.0591997392475605\n",
      "Epoch 16451/30000 Training Loss: 0.05818873643875122\n",
      "Epoch 16452/30000 Training Loss: 0.05339933931827545\n",
      "Epoch 16453/30000 Training Loss: 0.06129118800163269\n",
      "Epoch 16454/30000 Training Loss: 0.03974541276693344\n",
      "Epoch 16455/30000 Training Loss: 0.05810428410768509\n",
      "Epoch 16456/30000 Training Loss: 0.04570683091878891\n",
      "Epoch 16457/30000 Training Loss: 0.03754536062479019\n",
      "Epoch 16458/30000 Training Loss: 0.05067519471049309\n",
      "Epoch 16459/30000 Training Loss: 0.04416630417108536\n",
      "Epoch 16460/30000 Training Loss: 0.03604763001203537\n",
      "Epoch 16461/30000 Training Loss: 0.0601222924888134\n",
      "Epoch 16462/30000 Training Loss: 0.06287264823913574\n",
      "Epoch 16463/30000 Training Loss: 0.03083355911076069\n",
      "Epoch 16464/30000 Training Loss: 0.0446249395608902\n",
      "Epoch 16465/30000 Training Loss: 0.05509224906563759\n",
      "Epoch 16466/30000 Training Loss: 0.05055646225810051\n",
      "Epoch 16467/30000 Training Loss: 0.036164458841085434\n",
      "Epoch 16468/30000 Training Loss: 0.04091695696115494\n",
      "Epoch 16469/30000 Training Loss: 0.047025226056575775\n",
      "Epoch 16470/30000 Training Loss: 0.047696202993392944\n",
      "Epoch 16471/30000 Training Loss: 0.06503675878047943\n",
      "Epoch 16472/30000 Training Loss: 0.04057244956493378\n",
      "Epoch 16473/30000 Training Loss: 0.05800271034240723\n",
      "Epoch 16474/30000 Training Loss: 0.05560435727238655\n",
      "Epoch 16475/30000 Training Loss: 0.04559875279664993\n",
      "Epoch 16476/30000 Training Loss: 0.04345808923244476\n",
      "Epoch 16477/30000 Training Loss: 0.051391080021858215\n",
      "Epoch 16478/30000 Training Loss: 0.0417720228433609\n",
      "Epoch 16479/30000 Training Loss: 0.045823488384485245\n",
      "Epoch 16480/30000 Training Loss: 0.03912525996565819\n",
      "Epoch 16481/30000 Training Loss: 0.047805994749069214\n",
      "Epoch 16482/30000 Training Loss: 0.06251825392246246\n",
      "Epoch 16483/30000 Training Loss: 0.045471735298633575\n",
      "Epoch 16484/30000 Training Loss: 0.047730863094329834\n",
      "Epoch 16485/30000 Training Loss: 0.03556168079376221\n",
      "Epoch 16486/30000 Training Loss: 0.0262143537402153\n",
      "Epoch 16487/30000 Training Loss: 0.04683453217148781\n",
      "Epoch 16488/30000 Training Loss: 0.0408102311193943\n",
      "Epoch 16489/30000 Training Loss: 0.05164836719632149\n",
      "Epoch 16490/30000 Training Loss: 0.048844508826732635\n",
      "Epoch 16491/30000 Training Loss: 0.06295064836740494\n",
      "Epoch 16492/30000 Training Loss: 0.05659470707178116\n",
      "Epoch 16493/30000 Training Loss: 0.05169407278299332\n",
      "Epoch 16494/30000 Training Loss: 0.039366524666547775\n",
      "Epoch 16495/30000 Training Loss: 0.04636094346642494\n",
      "Epoch 16496/30000 Training Loss: 0.054351337254047394\n",
      "Epoch 16497/30000 Training Loss: 0.04885938763618469\n",
      "Epoch 16498/30000 Training Loss: 0.042117297649383545\n",
      "Epoch 16499/30000 Training Loss: 0.05243229866027832\n",
      "Epoch 16500/30000 Training Loss: 0.0461091622710228\n",
      "Epoch 16500/30000 Validation Loss: 0.037114113569259644\n",
      "Epoch 16501/30000 Training Loss: 0.03977394104003906\n",
      "Epoch 16502/30000 Training Loss: 0.0388948991894722\n",
      "Epoch 16503/30000 Training Loss: 0.048846594989299774\n",
      "Epoch 16504/30000 Training Loss: 0.04258301481604576\n",
      "Epoch 16505/30000 Training Loss: 0.03368493914604187\n",
      "Epoch 16506/30000 Training Loss: 0.05752749741077423\n",
      "Epoch 16507/30000 Training Loss: 0.040184441953897476\n",
      "Epoch 16508/30000 Training Loss: 0.04818926379084587\n",
      "Epoch 16509/30000 Training Loss: 0.046339280903339386\n",
      "Epoch 16510/30000 Training Loss: 0.04900556057691574\n",
      "Epoch 16511/30000 Training Loss: 0.046849463135004044\n",
      "Epoch 16512/30000 Training Loss: 0.04437157139182091\n",
      "Epoch 16513/30000 Training Loss: 0.05201646685600281\n",
      "Epoch 16514/30000 Training Loss: 0.050927214324474335\n",
      "Epoch 16515/30000 Training Loss: 0.05281776189804077\n",
      "Epoch 16516/30000 Training Loss: 0.047592855989933014\n",
      "Epoch 16517/30000 Training Loss: 0.04049310088157654\n",
      "Epoch 16518/30000 Training Loss: 0.04070925712585449\n",
      "Epoch 16519/30000 Training Loss: 0.04012973606586456\n",
      "Epoch 16520/30000 Training Loss: 0.040352366864681244\n",
      "Epoch 16521/30000 Training Loss: 0.05480873957276344\n",
      "Epoch 16522/30000 Training Loss: 0.0434301532804966\n",
      "Epoch 16523/30000 Training Loss: 0.05078539997339249\n",
      "Epoch 16524/30000 Training Loss: 0.05839617922902107\n",
      "Epoch 16525/30000 Training Loss: 0.04632864519953728\n",
      "Epoch 16526/30000 Training Loss: 0.048149049282073975\n",
      "Epoch 16527/30000 Training Loss: 0.048657290637493134\n",
      "Epoch 16528/30000 Training Loss: 0.04007447138428688\n",
      "Epoch 16529/30000 Training Loss: 0.029257461428642273\n",
      "Epoch 16530/30000 Training Loss: 0.05373474955558777\n",
      "Epoch 16531/30000 Training Loss: 0.054255060851573944\n",
      "Epoch 16532/30000 Training Loss: 0.06197592616081238\n",
      "Epoch 16533/30000 Training Loss: 0.04586324840784073\n",
      "Epoch 16534/30000 Training Loss: 0.05292506515979767\n",
      "Epoch 16535/30000 Training Loss: 0.04064019024372101\n",
      "Epoch 16536/30000 Training Loss: 0.037860430777072906\n",
      "Epoch 16537/30000 Training Loss: 0.03719661757349968\n",
      "Epoch 16538/30000 Training Loss: 0.03512788563966751\n",
      "Epoch 16539/30000 Training Loss: 0.0513332337141037\n",
      "Epoch 16540/30000 Training Loss: 0.037653032690286636\n",
      "Epoch 16541/30000 Training Loss: 0.040010228753089905\n",
      "Epoch 16542/30000 Training Loss: 0.05402425676584244\n",
      "Epoch 16543/30000 Training Loss: 0.046793825924396515\n",
      "Epoch 16544/30000 Training Loss: 0.05538662523031235\n",
      "Epoch 16545/30000 Training Loss: 0.03998175263404846\n",
      "Epoch 16546/30000 Training Loss: 0.03505690395832062\n",
      "Epoch 16547/30000 Training Loss: 0.05813002586364746\n",
      "Epoch 16548/30000 Training Loss: 0.05252283066511154\n",
      "Epoch 16549/30000 Training Loss: 0.0455777570605278\n",
      "Epoch 16550/30000 Training Loss: 0.04696571081876755\n",
      "Epoch 16551/30000 Training Loss: 0.0483100563287735\n",
      "Epoch 16552/30000 Training Loss: 0.035934872925281525\n",
      "Epoch 16553/30000 Training Loss: 0.04312349855899811\n",
      "Epoch 16554/30000 Training Loss: 0.053898535668849945\n",
      "Epoch 16555/30000 Training Loss: 0.0426638089120388\n",
      "Epoch 16556/30000 Training Loss: 0.04445105791091919\n",
      "Epoch 16557/30000 Training Loss: 0.04380751773715019\n",
      "Epoch 16558/30000 Training Loss: 0.037948913872241974\n",
      "Epoch 16559/30000 Training Loss: 0.045647598803043365\n",
      "Epoch 16560/30000 Training Loss: 0.04627957195043564\n",
      "Epoch 16561/30000 Training Loss: 0.034871600568294525\n",
      "Epoch 16562/30000 Training Loss: 0.05089913308620453\n",
      "Epoch 16563/30000 Training Loss: 0.04166533425450325\n",
      "Epoch 16564/30000 Training Loss: 0.04391775280237198\n",
      "Epoch 16565/30000 Training Loss: 0.03626793622970581\n",
      "Epoch 16566/30000 Training Loss: 0.03770367056131363\n",
      "Epoch 16567/30000 Training Loss: 0.043661586940288544\n",
      "Epoch 16568/30000 Training Loss: 0.03224832937121391\n",
      "Epoch 16569/30000 Training Loss: 0.041332561522722244\n",
      "Epoch 16570/30000 Training Loss: 0.03996943682432175\n",
      "Epoch 16571/30000 Training Loss: 0.04151584208011627\n",
      "Epoch 16572/30000 Training Loss: 0.06288279592990875\n",
      "Epoch 16573/30000 Training Loss: 0.05892939120531082\n",
      "Epoch 16574/30000 Training Loss: 0.03799188882112503\n",
      "Epoch 16575/30000 Training Loss: 0.04589374363422394\n",
      "Epoch 16576/30000 Training Loss: 0.03368488699197769\n",
      "Epoch 16577/30000 Training Loss: 0.035084083676338196\n",
      "Epoch 16578/30000 Training Loss: 0.06458649784326553\n",
      "Epoch 16579/30000 Training Loss: 0.052327804267406464\n",
      "Epoch 16580/30000 Training Loss: 0.04679581895470619\n",
      "Epoch 16581/30000 Training Loss: 0.04366306588053703\n",
      "Epoch 16582/30000 Training Loss: 0.047078341245651245\n",
      "Epoch 16583/30000 Training Loss: 0.04555696249008179\n",
      "Epoch 16584/30000 Training Loss: 0.05233129858970642\n",
      "Epoch 16585/30000 Training Loss: 0.03897729143500328\n",
      "Epoch 16586/30000 Training Loss: 0.04637938737869263\n",
      "Epoch 16587/30000 Training Loss: 0.03995266929268837\n",
      "Epoch 16588/30000 Training Loss: 0.056102145463228226\n",
      "Epoch 16589/30000 Training Loss: 0.047321539372205734\n",
      "Epoch 16590/30000 Training Loss: 0.034476060420274734\n",
      "Epoch 16591/30000 Training Loss: 0.034880973398685455\n",
      "Epoch 16592/30000 Training Loss: 0.04673748090863228\n",
      "Epoch 16593/30000 Training Loss: 0.0417928472161293\n",
      "Epoch 16594/30000 Training Loss: 0.039917610585689545\n",
      "Epoch 16595/30000 Training Loss: 0.03838791698217392\n",
      "Epoch 16596/30000 Training Loss: 0.048762135207653046\n",
      "Epoch 16597/30000 Training Loss: 0.034034281969070435\n",
      "Epoch 16598/30000 Training Loss: 0.04656985402107239\n",
      "Epoch 16599/30000 Training Loss: 0.04257086664438248\n",
      "Epoch 16600/30000 Training Loss: 0.038722675293684006\n",
      "Epoch 16600/30000 Validation Loss: 0.04223211854696274\n",
      "Epoch 16601/30000 Training Loss: 0.05672154575586319\n",
      "Epoch 16602/30000 Training Loss: 0.04947324097156525\n",
      "Epoch 16603/30000 Training Loss: 0.059163883328437805\n",
      "Epoch 16604/30000 Training Loss: 0.044406767934560776\n",
      "Epoch 16605/30000 Training Loss: 0.038419075310230255\n",
      "Epoch 16606/30000 Training Loss: 0.05240369588136673\n",
      "Epoch 16607/30000 Training Loss: 0.050411760807037354\n",
      "Epoch 16608/30000 Training Loss: 0.03883273899555206\n",
      "Epoch 16609/30000 Training Loss: 0.03313750401139259\n",
      "Epoch 16610/30000 Training Loss: 0.039071790874004364\n",
      "Epoch 16611/30000 Training Loss: 0.04296542704105377\n",
      "Epoch 16612/30000 Training Loss: 0.04266047105193138\n",
      "Epoch 16613/30000 Training Loss: 0.0456240177154541\n",
      "Epoch 16614/30000 Training Loss: 0.057303011417388916\n",
      "Epoch 16615/30000 Training Loss: 0.03887534141540527\n",
      "Epoch 16616/30000 Training Loss: 0.051329080015420914\n",
      "Epoch 16617/30000 Training Loss: 0.04307575896382332\n",
      "Epoch 16618/30000 Training Loss: 0.06376977264881134\n",
      "Epoch 16619/30000 Training Loss: 0.041235245764255524\n",
      "Epoch 16620/30000 Training Loss: 0.03953590244054794\n",
      "Epoch 16621/30000 Training Loss: 0.05503135919570923\n",
      "Epoch 16622/30000 Training Loss: 0.039166443049907684\n",
      "Epoch 16623/30000 Training Loss: 0.038777850568294525\n",
      "Epoch 16624/30000 Training Loss: 0.04879586771130562\n",
      "Epoch 16625/30000 Training Loss: 0.04620242863893509\n",
      "Epoch 16626/30000 Training Loss: 0.05286061018705368\n",
      "Epoch 16627/30000 Training Loss: 0.03231138363480568\n",
      "Epoch 16628/30000 Training Loss: 0.042966026812791824\n",
      "Epoch 16629/30000 Training Loss: 0.04174933582544327\n",
      "Epoch 16630/30000 Training Loss: 0.046297937631607056\n",
      "Epoch 16631/30000 Training Loss: 0.04149468243122101\n",
      "Epoch 16632/30000 Training Loss: 0.035323724150657654\n",
      "Epoch 16633/30000 Training Loss: 0.04477522522211075\n",
      "Epoch 16634/30000 Training Loss: 0.04397169500589371\n",
      "Epoch 16635/30000 Training Loss: 0.05114055424928665\n",
      "Epoch 16636/30000 Training Loss: 0.05846964940428734\n",
      "Epoch 16637/30000 Training Loss: 0.03881826251745224\n",
      "Epoch 16638/30000 Training Loss: 0.05020340159535408\n",
      "Epoch 16639/30000 Training Loss: 0.038548145443201065\n",
      "Epoch 16640/30000 Training Loss: 0.0592772513628006\n",
      "Epoch 16641/30000 Training Loss: 0.05580337345600128\n",
      "Epoch 16642/30000 Training Loss: 0.05882551521062851\n",
      "Epoch 16643/30000 Training Loss: 0.045505501329898834\n",
      "Epoch 16644/30000 Training Loss: 0.04138912633061409\n",
      "Epoch 16645/30000 Training Loss: 0.0587906613945961\n",
      "Epoch 16646/30000 Training Loss: 0.031128384172916412\n",
      "Epoch 16647/30000 Training Loss: 0.04007038101553917\n",
      "Epoch 16648/30000 Training Loss: 0.03541463986039162\n",
      "Epoch 16649/30000 Training Loss: 0.0501675084233284\n",
      "Epoch 16650/30000 Training Loss: 0.055316582322120667\n",
      "Epoch 16651/30000 Training Loss: 0.059193722903728485\n",
      "Epoch 16652/30000 Training Loss: 0.06232410669326782\n",
      "Epoch 16653/30000 Training Loss: 0.03695856034755707\n",
      "Epoch 16654/30000 Training Loss: 0.054983191192150116\n",
      "Epoch 16655/30000 Training Loss: 0.036241792142391205\n",
      "Epoch 16656/30000 Training Loss: 0.04440034553408623\n",
      "Epoch 16657/30000 Training Loss: 0.04725304991006851\n",
      "Epoch 16658/30000 Training Loss: 0.041232429444789886\n",
      "Epoch 16659/30000 Training Loss: 0.04312034696340561\n",
      "Epoch 16660/30000 Training Loss: 0.04581634700298309\n",
      "Epoch 16661/30000 Training Loss: 0.03234373778104782\n",
      "Epoch 16662/30000 Training Loss: 0.046836577355861664\n",
      "Epoch 16663/30000 Training Loss: 0.04078640416264534\n",
      "Epoch 16664/30000 Training Loss: 0.046334147453308105\n",
      "Epoch 16665/30000 Training Loss: 0.036388903856277466\n",
      "Epoch 16666/30000 Training Loss: 0.03878476470708847\n",
      "Epoch 16667/30000 Training Loss: 0.05927008390426636\n",
      "Epoch 16668/30000 Training Loss: 0.048458464443683624\n",
      "Epoch 16669/30000 Training Loss: 0.048637595027685165\n",
      "Epoch 16670/30000 Training Loss: 0.04594730958342552\n",
      "Epoch 16671/30000 Training Loss: 0.03594878688454628\n",
      "Epoch 16672/30000 Training Loss: 0.048310309648513794\n",
      "Epoch 16673/30000 Training Loss: 0.04292692989110947\n",
      "Epoch 16674/30000 Training Loss: 0.042483918368816376\n",
      "Epoch 16675/30000 Training Loss: 0.03359260410070419\n",
      "Epoch 16676/30000 Training Loss: 0.05821751058101654\n",
      "Epoch 16677/30000 Training Loss: 0.03795762360095978\n",
      "Epoch 16678/30000 Training Loss: 0.03473168611526489\n",
      "Epoch 16679/30000 Training Loss: 0.047953203320503235\n",
      "Epoch 16680/30000 Training Loss: 0.04231959581375122\n",
      "Epoch 16681/30000 Training Loss: 0.05062589794397354\n",
      "Epoch 16682/30000 Training Loss: 0.03950526937842369\n",
      "Epoch 16683/30000 Training Loss: 0.038961537182331085\n",
      "Epoch 16684/30000 Training Loss: 0.048779942095279694\n",
      "Epoch 16685/30000 Training Loss: 0.04941762238740921\n",
      "Epoch 16686/30000 Training Loss: 0.038749679923057556\n",
      "Epoch 16687/30000 Training Loss: 0.0521063432097435\n",
      "Epoch 16688/30000 Training Loss: 0.04307563602924347\n",
      "Epoch 16689/30000 Training Loss: 0.035857394337654114\n",
      "Epoch 16690/30000 Training Loss: 0.04428412765264511\n",
      "Epoch 16691/30000 Training Loss: 0.048083581030368805\n",
      "Epoch 16692/30000 Training Loss: 0.04286579415202141\n",
      "Epoch 16693/30000 Training Loss: 0.04563050717115402\n",
      "Epoch 16694/30000 Training Loss: 0.04303748905658722\n",
      "Epoch 16695/30000 Training Loss: 0.03618011996150017\n",
      "Epoch 16696/30000 Training Loss: 0.043808143585920334\n",
      "Epoch 16697/30000 Training Loss: 0.03511025756597519\n",
      "Epoch 16698/30000 Training Loss: 0.04649536311626434\n",
      "Epoch 16699/30000 Training Loss: 0.030249863862991333\n",
      "Epoch 16700/30000 Training Loss: 0.046367187052965164\n",
      "Epoch 16700/30000 Validation Loss: 0.04327164590358734\n",
      "Epoch 16701/30000 Training Loss: 0.05669261887669563\n",
      "Epoch 16702/30000 Training Loss: 0.045040063560009\n",
      "Epoch 16703/30000 Training Loss: 0.042426809668540955\n",
      "Epoch 16704/30000 Training Loss: 0.04914133623242378\n",
      "Epoch 16705/30000 Training Loss: 0.0401444248855114\n",
      "Epoch 16706/30000 Training Loss: 0.04360828548669815\n",
      "Epoch 16707/30000 Training Loss: 0.055663514882326126\n",
      "Epoch 16708/30000 Training Loss: 0.04674676060676575\n",
      "Epoch 16709/30000 Training Loss: 0.052508335560560226\n",
      "Epoch 16710/30000 Training Loss: 0.04751624912023544\n",
      "Epoch 16711/30000 Training Loss: 0.05988626182079315\n",
      "Epoch 16712/30000 Training Loss: 0.04529564455151558\n",
      "Epoch 16713/30000 Training Loss: 0.03477007523179054\n",
      "Epoch 16714/30000 Training Loss: 0.057646363973617554\n",
      "Epoch 16715/30000 Training Loss: 0.04859734699130058\n",
      "Epoch 16716/30000 Training Loss: 0.045158155262470245\n",
      "Epoch 16717/30000 Training Loss: 0.058172471821308136\n",
      "Epoch 16718/30000 Training Loss: 0.03527761995792389\n",
      "Epoch 16719/30000 Training Loss: 0.0659257099032402\n",
      "Epoch 16720/30000 Training Loss: 0.043792277574539185\n",
      "Epoch 16721/30000 Training Loss: 0.05092405527830124\n",
      "Epoch 16722/30000 Training Loss: 0.046085573732852936\n",
      "Epoch 16723/30000 Training Loss: 0.03688662126660347\n",
      "Epoch 16724/30000 Training Loss: 0.0452163890004158\n",
      "Epoch 16725/30000 Training Loss: 0.048870012164115906\n",
      "Epoch 16726/30000 Training Loss: 0.04671058431267738\n",
      "Epoch 16727/30000 Training Loss: 0.035665661096572876\n",
      "Epoch 16728/30000 Training Loss: 0.043508611619472504\n",
      "Epoch 16729/30000 Training Loss: 0.046996332705020905\n",
      "Epoch 16730/30000 Training Loss: 0.053688257932662964\n",
      "Epoch 16731/30000 Training Loss: 0.04723658412694931\n",
      "Epoch 16732/30000 Training Loss: 0.048664458096027374\n",
      "Epoch 16733/30000 Training Loss: 0.0489552840590477\n",
      "Epoch 16734/30000 Training Loss: 0.05602402985095978\n",
      "Epoch 16735/30000 Training Loss: 0.0479801669716835\n",
      "Epoch 16736/30000 Training Loss: 0.04125919193029404\n",
      "Epoch 16737/30000 Training Loss: 0.05042906478047371\n",
      "Epoch 16738/30000 Training Loss: 0.04001443088054657\n",
      "Epoch 16739/30000 Training Loss: 0.041339751332998276\n",
      "Epoch 16740/30000 Training Loss: 0.040884725749492645\n",
      "Epoch 16741/30000 Training Loss: 0.06495737284421921\n",
      "Epoch 16742/30000 Training Loss: 0.05462265387177467\n",
      "Epoch 16743/30000 Training Loss: 0.053411588072776794\n",
      "Epoch 16744/30000 Training Loss: 0.03741465136408806\n",
      "Epoch 16745/30000 Training Loss: 0.04548465460538864\n",
      "Epoch 16746/30000 Training Loss: 0.05728113651275635\n",
      "Epoch 16747/30000 Training Loss: 0.03885553032159805\n",
      "Epoch 16748/30000 Training Loss: 0.052668504416942596\n",
      "Epoch 16749/30000 Training Loss: 0.04372682049870491\n",
      "Epoch 16750/30000 Training Loss: 0.05268165469169617\n",
      "Epoch 16751/30000 Training Loss: 0.04355654865503311\n",
      "Epoch 16752/30000 Training Loss: 0.04619825631380081\n",
      "Epoch 16753/30000 Training Loss: 0.0481598824262619\n",
      "Epoch 16754/30000 Training Loss: 0.04403943195939064\n",
      "Epoch 16755/30000 Training Loss: 0.04956158250570297\n",
      "Epoch 16756/30000 Training Loss: 0.05208572745323181\n",
      "Epoch 16757/30000 Training Loss: 0.03952591121196747\n",
      "Epoch 16758/30000 Training Loss: 0.05342479795217514\n",
      "Epoch 16759/30000 Training Loss: 0.04402490705251694\n",
      "Epoch 16760/30000 Training Loss: 0.05012942850589752\n",
      "Epoch 16761/30000 Training Loss: 0.0325215682387352\n",
      "Epoch 16762/30000 Training Loss: 0.038796938955783844\n",
      "Epoch 16763/30000 Training Loss: 0.04214150831103325\n",
      "Epoch 16764/30000 Training Loss: 0.04257393628358841\n",
      "Epoch 16765/30000 Training Loss: 0.04632917046546936\n",
      "Epoch 16766/30000 Training Loss: 0.04269280284643173\n",
      "Epoch 16767/30000 Training Loss: 0.03877795487642288\n",
      "Epoch 16768/30000 Training Loss: 0.05169173330068588\n",
      "Epoch 16769/30000 Training Loss: 0.05022934824228287\n",
      "Epoch 16770/30000 Training Loss: 0.039165593683719635\n",
      "Epoch 16771/30000 Training Loss: 0.0497908890247345\n",
      "Epoch 16772/30000 Training Loss: 0.033541735261678696\n",
      "Epoch 16773/30000 Training Loss: 0.03826563060283661\n",
      "Epoch 16774/30000 Training Loss: 0.045255325734615326\n",
      "Epoch 16775/30000 Training Loss: 0.03580718860030174\n",
      "Epoch 16776/30000 Training Loss: 0.04705612361431122\n",
      "Epoch 16777/30000 Training Loss: 0.036477454006671906\n",
      "Epoch 16778/30000 Training Loss: 0.0431332141160965\n",
      "Epoch 16779/30000 Training Loss: 0.04553121328353882\n",
      "Epoch 16780/30000 Training Loss: 0.029281135648489\n",
      "Epoch 16781/30000 Training Loss: 0.049321047961711884\n",
      "Epoch 16782/30000 Training Loss: 0.037920355796813965\n",
      "Epoch 16783/30000 Training Loss: 0.049224063754081726\n",
      "Epoch 16784/30000 Training Loss: 0.05449264496564865\n",
      "Epoch 16785/30000 Training Loss: 0.050613775849342346\n",
      "Epoch 16786/30000 Training Loss: 0.04545855522155762\n",
      "Epoch 16787/30000 Training Loss: 0.06610676646232605\n",
      "Epoch 16788/30000 Training Loss: 0.05036471411585808\n",
      "Epoch 16789/30000 Training Loss: 0.04154166951775551\n",
      "Epoch 16790/30000 Training Loss: 0.041444242000579834\n",
      "Epoch 16791/30000 Training Loss: 0.049492184072732925\n",
      "Epoch 16792/30000 Training Loss: 0.06389228254556656\n",
      "Epoch 16793/30000 Training Loss: 0.05271206796169281\n",
      "Epoch 16794/30000 Training Loss: 0.03849422186613083\n",
      "Epoch 16795/30000 Training Loss: 0.0443854033946991\n",
      "Epoch 16796/30000 Training Loss: 0.048345111310482025\n",
      "Epoch 16797/30000 Training Loss: 0.0408739373087883\n",
      "Epoch 16798/30000 Training Loss: 0.05519666522741318\n",
      "Epoch 16799/30000 Training Loss: 0.0496155209839344\n",
      "Epoch 16800/30000 Training Loss: 0.04198368638753891\n",
      "Epoch 16800/30000 Validation Loss: 0.041290998458862305\n",
      "Epoch 16801/30000 Training Loss: 0.048480547964572906\n",
      "Epoch 16802/30000 Training Loss: 0.04670413210988045\n",
      "Epoch 16803/30000 Training Loss: 0.030481725931167603\n",
      "Epoch 16804/30000 Training Loss: 0.047045547515153885\n",
      "Epoch 16805/30000 Training Loss: 0.04460587725043297\n",
      "Epoch 16806/30000 Training Loss: 0.040453508496284485\n",
      "Epoch 16807/30000 Training Loss: 0.059618182480335236\n",
      "Epoch 16808/30000 Training Loss: 0.05302628502249718\n",
      "Epoch 16809/30000 Training Loss: 0.05247241631150246\n",
      "Epoch 16810/30000 Training Loss: 0.04433998838067055\n",
      "Epoch 16811/30000 Training Loss: 0.030276138335466385\n",
      "Epoch 16812/30000 Training Loss: 0.04242707043886185\n",
      "Epoch 16813/30000 Training Loss: 0.042622655630111694\n",
      "Epoch 16814/30000 Training Loss: 0.046612538397312164\n",
      "Epoch 16815/30000 Training Loss: 0.045117657631635666\n",
      "Epoch 16816/30000 Training Loss: 0.0538240447640419\n",
      "Epoch 16817/30000 Training Loss: 0.03785325586795807\n",
      "Epoch 16818/30000 Training Loss: 0.04189863055944443\n",
      "Epoch 16819/30000 Training Loss: 0.05177043378353119\n",
      "Epoch 16820/30000 Training Loss: 0.06326878815889359\n",
      "Epoch 16821/30000 Training Loss: 0.04216286540031433\n",
      "Epoch 16822/30000 Training Loss: 0.0568830668926239\n",
      "Epoch 16823/30000 Training Loss: 0.03842128813266754\n",
      "Epoch 16824/30000 Training Loss: 0.03508787229657173\n",
      "Epoch 16825/30000 Training Loss: 0.03857932984828949\n",
      "Epoch 16826/30000 Training Loss: 0.05594243109226227\n",
      "Epoch 16827/30000 Training Loss: 0.04458095505833626\n",
      "Epoch 16828/30000 Training Loss: 0.0480513721704483\n",
      "Epoch 16829/30000 Training Loss: 0.056770868599414825\n",
      "Epoch 16830/30000 Training Loss: 0.05186161771416664\n",
      "Epoch 16831/30000 Training Loss: 0.053886812180280685\n",
      "Epoch 16832/30000 Training Loss: 0.03486759960651398\n",
      "Epoch 16833/30000 Training Loss: 0.05172854661941528\n",
      "Epoch 16834/30000 Training Loss: 0.03869050741195679\n",
      "Epoch 16835/30000 Training Loss: 0.03863565996289253\n",
      "Epoch 16836/30000 Training Loss: 0.04003807157278061\n",
      "Epoch 16837/30000 Training Loss: 0.04375535994768143\n",
      "Epoch 16838/30000 Training Loss: 0.04533158242702484\n",
      "Epoch 16839/30000 Training Loss: 0.03713612258434296\n",
      "Epoch 16840/30000 Training Loss: 0.041824132204055786\n",
      "Epoch 16841/30000 Training Loss: 0.04492351412773132\n",
      "Epoch 16842/30000 Training Loss: 0.040570713579654694\n",
      "Epoch 16843/30000 Training Loss: 0.04939405620098114\n",
      "Epoch 16844/30000 Training Loss: 0.04106610268354416\n",
      "Epoch 16845/30000 Training Loss: 0.043555598706007004\n",
      "Epoch 16846/30000 Training Loss: 0.051870934665203094\n",
      "Epoch 16847/30000 Training Loss: 0.0639546662569046\n",
      "Epoch 16848/30000 Training Loss: 0.03598976880311966\n",
      "Epoch 16849/30000 Training Loss: 0.06390281021595001\n",
      "Epoch 16850/30000 Training Loss: 0.053666748106479645\n",
      "Epoch 16851/30000 Training Loss: 0.028230994939804077\n",
      "Epoch 16852/30000 Training Loss: 0.032939501106739044\n",
      "Epoch 16853/30000 Training Loss: 0.056359514594078064\n",
      "Epoch 16854/30000 Training Loss: 0.06491487473249435\n",
      "Epoch 16855/30000 Training Loss: 0.053884346038103104\n",
      "Epoch 16856/30000 Training Loss: 0.03989359736442566\n",
      "Epoch 16857/30000 Training Loss: 0.058942485600709915\n",
      "Epoch 16858/30000 Training Loss: 0.04401349276304245\n",
      "Epoch 16859/30000 Training Loss: 0.0414016917347908\n",
      "Epoch 16860/30000 Training Loss: 0.04855410009622574\n",
      "Epoch 16861/30000 Training Loss: 0.03699984773993492\n",
      "Epoch 16862/30000 Training Loss: 0.04100778326392174\n",
      "Epoch 16863/30000 Training Loss: 0.03690603747963905\n",
      "Epoch 16864/30000 Training Loss: 0.04637521505355835\n",
      "Epoch 16865/30000 Training Loss: 0.039038173854351044\n",
      "Epoch 16866/30000 Training Loss: 0.03641431778669357\n",
      "Epoch 16867/30000 Training Loss: 0.05378403514623642\n",
      "Epoch 16868/30000 Training Loss: 0.03306758031249046\n",
      "Epoch 16869/30000 Training Loss: 0.04468874633312225\n",
      "Epoch 16870/30000 Training Loss: 0.03233221545815468\n",
      "Epoch 16871/30000 Training Loss: 0.03611847385764122\n",
      "Epoch 16872/30000 Training Loss: 0.0416296124458313\n",
      "Epoch 16873/30000 Training Loss: 0.04787519574165344\n",
      "Epoch 16874/30000 Training Loss: 0.03486759960651398\n",
      "Epoch 16875/30000 Training Loss: 0.050665851682424545\n",
      "Epoch 16876/30000 Training Loss: 0.04717869311571121\n",
      "Epoch 16877/30000 Training Loss: 0.053346142172813416\n",
      "Epoch 16878/30000 Training Loss: 0.04123913124203682\n",
      "Epoch 16879/30000 Training Loss: 0.04064609110355377\n",
      "Epoch 16880/30000 Training Loss: 0.03012465313076973\n",
      "Epoch 16881/30000 Training Loss: 0.0407656654715538\n",
      "Epoch 16882/30000 Training Loss: 0.043776921927928925\n",
      "Epoch 16883/30000 Training Loss: 0.046146146953105927\n",
      "Epoch 16884/30000 Training Loss: 0.040846407413482666\n",
      "Epoch 16885/30000 Training Loss: 0.042013175785541534\n",
      "Epoch 16886/30000 Training Loss: 0.05146236717700958\n",
      "Epoch 16887/30000 Training Loss: 0.03993644192814827\n",
      "Epoch 16888/30000 Training Loss: 0.04000852629542351\n",
      "Epoch 16889/30000 Training Loss: 0.05955694243311882\n",
      "Epoch 16890/30000 Training Loss: 0.045711930841207504\n",
      "Epoch 16891/30000 Training Loss: 0.04940197616815567\n",
      "Epoch 16892/30000 Training Loss: 0.0443083792924881\n",
      "Epoch 16893/30000 Training Loss: 0.03875775635242462\n",
      "Epoch 16894/30000 Training Loss: 0.048166051506996155\n",
      "Epoch 16895/30000 Training Loss: 0.04599901661276817\n",
      "Epoch 16896/30000 Training Loss: 0.04273054376244545\n",
      "Epoch 16897/30000 Training Loss: 0.04445044696331024\n",
      "Epoch 16898/30000 Training Loss: 0.0291909147053957\n",
      "Epoch 16899/30000 Training Loss: 0.04781022667884827\n",
      "Epoch 16900/30000 Training Loss: 0.03332142159342766\n",
      "Epoch 16900/30000 Validation Loss: 0.04220251739025116\n",
      "Epoch 16901/30000 Training Loss: 0.0393463596701622\n",
      "Epoch 16902/30000 Training Loss: 0.04320942983031273\n",
      "Epoch 16903/30000 Training Loss: 0.056816935539245605\n",
      "Epoch 16904/30000 Training Loss: 0.043553054332733154\n",
      "Epoch 16905/30000 Training Loss: 0.04250558465719223\n",
      "Epoch 16906/30000 Training Loss: 0.03849940001964569\n",
      "Epoch 16907/30000 Training Loss: 0.05702512338757515\n",
      "Epoch 16908/30000 Training Loss: 0.0507233589887619\n",
      "Epoch 16909/30000 Training Loss: 0.03600156679749489\n",
      "Epoch 16910/30000 Training Loss: 0.04396386444568634\n",
      "Epoch 16911/30000 Training Loss: 0.04889519512653351\n",
      "Epoch 16912/30000 Training Loss: 0.0783575102686882\n",
      "Epoch 16913/30000 Training Loss: 0.04699409380555153\n",
      "Epoch 16914/30000 Training Loss: 0.04958302527666092\n",
      "Epoch 16915/30000 Training Loss: 0.0474550724029541\n",
      "Epoch 16916/30000 Training Loss: 0.047697894275188446\n",
      "Epoch 16917/30000 Training Loss: 0.044641364365816116\n",
      "Epoch 16918/30000 Training Loss: 0.032493848353624344\n",
      "Epoch 16919/30000 Training Loss: 0.03626544773578644\n",
      "Epoch 16920/30000 Training Loss: 0.0686880573630333\n",
      "Epoch 16921/30000 Training Loss: 0.0387522354722023\n",
      "Epoch 16922/30000 Training Loss: 0.047551099210977554\n",
      "Epoch 16923/30000 Training Loss: 0.04782329127192497\n",
      "Epoch 16924/30000 Training Loss: 0.03572015464305878\n",
      "Epoch 16925/30000 Training Loss: 0.05823306366801262\n",
      "Epoch 16926/30000 Training Loss: 0.05006266012787819\n",
      "Epoch 16927/30000 Training Loss: 0.04048138111829758\n",
      "Epoch 16928/30000 Training Loss: 0.0509638674557209\n",
      "Epoch 16929/30000 Training Loss: 0.04520522058010101\n",
      "Epoch 16930/30000 Training Loss: 0.05068156495690346\n",
      "Epoch 16931/30000 Training Loss: 0.03783351182937622\n",
      "Epoch 16932/30000 Training Loss: 0.03715714439749718\n",
      "Epoch 16933/30000 Training Loss: 0.041776083409786224\n",
      "Epoch 16934/30000 Training Loss: 0.04542647302150726\n",
      "Epoch 16935/30000 Training Loss: 0.04432161524891853\n",
      "Epoch 16936/30000 Training Loss: 0.03557482361793518\n",
      "Epoch 16937/30000 Training Loss: 0.060808807611465454\n",
      "Epoch 16938/30000 Training Loss: 0.05371471121907234\n",
      "Epoch 16939/30000 Training Loss: 0.05003856495022774\n",
      "Epoch 16940/30000 Training Loss: 0.047080814838409424\n",
      "Epoch 16941/30000 Training Loss: 0.04362963140010834\n",
      "Epoch 16942/30000 Training Loss: 0.04088843613862991\n",
      "Epoch 16943/30000 Training Loss: 0.057594574987888336\n",
      "Epoch 16944/30000 Training Loss: 0.04184715077280998\n",
      "Epoch 16945/30000 Training Loss: 0.04804103076457977\n",
      "Epoch 16946/30000 Training Loss: 0.04106849431991577\n",
      "Epoch 16947/30000 Training Loss: 0.047496773302555084\n",
      "Epoch 16948/30000 Training Loss: 0.044066525995731354\n",
      "Epoch 16949/30000 Training Loss: 0.04608677327632904\n",
      "Epoch 16950/30000 Training Loss: 0.04511711001396179\n",
      "Epoch 16951/30000 Training Loss: 0.04670628532767296\n",
      "Epoch 16952/30000 Training Loss: 0.04953978210687637\n",
      "Epoch 16953/30000 Training Loss: 0.057452842593193054\n",
      "Epoch 16954/30000 Training Loss: 0.05168592184782028\n",
      "Epoch 16955/30000 Training Loss: 0.027908803895115852\n",
      "Epoch 16956/30000 Training Loss: 0.032984405755996704\n",
      "Epoch 16957/30000 Training Loss: 0.049913108348846436\n",
      "Epoch 16958/30000 Training Loss: 0.0544472336769104\n",
      "Epoch 16959/30000 Training Loss: 0.036922112107276917\n",
      "Epoch 16960/30000 Training Loss: 0.03858930617570877\n",
      "Epoch 16961/30000 Training Loss: 0.04609077423810959\n",
      "Epoch 16962/30000 Training Loss: 0.041333138942718506\n",
      "Epoch 16963/30000 Training Loss: 0.04437423497438431\n",
      "Epoch 16964/30000 Training Loss: 0.04003752022981644\n",
      "Epoch 16965/30000 Training Loss: 0.06490430235862732\n",
      "Epoch 16966/30000 Training Loss: 0.04030178487300873\n",
      "Epoch 16967/30000 Training Loss: 0.036213383078575134\n",
      "Epoch 16968/30000 Training Loss: 0.05775142461061478\n",
      "Epoch 16969/30000 Training Loss: 0.04595843702554703\n",
      "Epoch 16970/30000 Training Loss: 0.04586222395300865\n",
      "Epoch 16971/30000 Training Loss: 0.04595807194709778\n",
      "Epoch 16972/30000 Training Loss: 0.03754451870918274\n",
      "Epoch 16973/30000 Training Loss: 0.053173311054706573\n",
      "Epoch 16974/30000 Training Loss: 0.044104695320129395\n",
      "Epoch 16975/30000 Training Loss: 0.059048473834991455\n",
      "Epoch 16976/30000 Training Loss: 0.047310177236795425\n",
      "Epoch 16977/30000 Training Loss: 0.03740568459033966\n",
      "Epoch 16978/30000 Training Loss: 0.03891613334417343\n",
      "Epoch 16979/30000 Training Loss: 0.04416457563638687\n",
      "Epoch 16980/30000 Training Loss: 0.04143448919057846\n",
      "Epoch 16981/30000 Training Loss: 0.0536227747797966\n",
      "Epoch 16982/30000 Training Loss: 0.04223170131444931\n",
      "Epoch 16983/30000 Training Loss: 0.05338910594582558\n",
      "Epoch 16984/30000 Training Loss: 0.04634410887956619\n",
      "Epoch 16985/30000 Training Loss: 0.04218077287077904\n",
      "Epoch 16986/30000 Training Loss: 0.0323094017803669\n",
      "Epoch 16987/30000 Training Loss: 0.041945673525333405\n",
      "Epoch 16988/30000 Training Loss: 0.036291562020778656\n",
      "Epoch 16989/30000 Training Loss: 0.036804936826229095\n",
      "Epoch 16990/30000 Training Loss: 0.05218428373336792\n",
      "Epoch 16991/30000 Training Loss: 0.04822959005832672\n",
      "Epoch 16992/30000 Training Loss: 0.04890455678105354\n",
      "Epoch 16993/30000 Training Loss: 0.053609371185302734\n",
      "Epoch 16994/30000 Training Loss: 0.0512051060795784\n",
      "Epoch 16995/30000 Training Loss: 0.04225366935133934\n",
      "Epoch 16996/30000 Training Loss: 0.04122162610292435\n",
      "Epoch 16997/30000 Training Loss: 0.04737429693341255\n",
      "Epoch 16998/30000 Training Loss: 0.03245662897825241\n",
      "Epoch 16999/30000 Training Loss: 0.03708932176232338\n",
      "Epoch 17000/30000 Training Loss: 0.05431535840034485\n",
      "Epoch 17000/30000 Validation Loss: 0.03839145600795746\n",
      "Epoch 17001/30000 Training Loss: 0.03806184232234955\n",
      "Epoch 17002/30000 Training Loss: 0.03245902061462402\n",
      "Epoch 17003/30000 Training Loss: 0.04848930239677429\n",
      "Epoch 17004/30000 Training Loss: 0.05486937239766121\n",
      "Epoch 17005/30000 Training Loss: 0.04608345776796341\n",
      "Epoch 17006/30000 Training Loss: 0.03992948681116104\n",
      "Epoch 17007/30000 Training Loss: 0.054586492478847504\n",
      "Epoch 17008/30000 Training Loss: 0.052747875452041626\n",
      "Epoch 17009/30000 Training Loss: 0.05843358114361763\n",
      "Epoch 17010/30000 Training Loss: 0.035380519926548004\n",
      "Epoch 17011/30000 Training Loss: 0.04170912876725197\n",
      "Epoch 17012/30000 Training Loss: 0.03861662745475769\n",
      "Epoch 17013/30000 Training Loss: 0.031033432111144066\n",
      "Epoch 17014/30000 Training Loss: 0.042117536067962646\n",
      "Epoch 17015/30000 Training Loss: 0.04357672482728958\n",
      "Epoch 17016/30000 Training Loss: 0.04205986112356186\n",
      "Epoch 17017/30000 Training Loss: 0.043768614530563354\n",
      "Epoch 17018/30000 Training Loss: 0.045096494257450104\n",
      "Epoch 17019/30000 Training Loss: 0.039103664457798004\n",
      "Epoch 17020/30000 Training Loss: 0.037399839609861374\n",
      "Epoch 17021/30000 Training Loss: 0.06319113820791245\n",
      "Epoch 17022/30000 Training Loss: 0.04759199917316437\n",
      "Epoch 17023/30000 Training Loss: 0.03959435969591141\n",
      "Epoch 17024/30000 Training Loss: 0.05315016955137253\n",
      "Epoch 17025/30000 Training Loss: 0.04936975985765457\n",
      "Epoch 17026/30000 Training Loss: 0.03241300582885742\n",
      "Epoch 17027/30000 Training Loss: 0.05184406042098999\n",
      "Epoch 17028/30000 Training Loss: 0.045486100018024445\n",
      "Epoch 17029/30000 Training Loss: 0.04569888859987259\n",
      "Epoch 17030/30000 Training Loss: 0.05585528537631035\n",
      "Epoch 17031/30000 Training Loss: 0.04873870313167572\n",
      "Epoch 17032/30000 Training Loss: 0.036472197622060776\n",
      "Epoch 17033/30000 Training Loss: 0.044243182986974716\n",
      "Epoch 17034/30000 Training Loss: 0.038907866925001144\n",
      "Epoch 17035/30000 Training Loss: 0.03615016117691994\n",
      "Epoch 17036/30000 Training Loss: 0.04764505848288536\n",
      "Epoch 17037/30000 Training Loss: 0.03969959914684296\n",
      "Epoch 17038/30000 Training Loss: 0.037810467183589935\n",
      "Epoch 17039/30000 Training Loss: 0.04204527661204338\n",
      "Epoch 17040/30000 Training Loss: 0.04351804032921791\n",
      "Epoch 17041/30000 Training Loss: 0.046699408441782\n",
      "Epoch 17042/30000 Training Loss: 0.06539329886436462\n",
      "Epoch 17043/30000 Training Loss: 0.06246070936322212\n",
      "Epoch 17044/30000 Training Loss: 0.03909016400575638\n",
      "Epoch 17045/30000 Training Loss: 0.05041026324033737\n",
      "Epoch 17046/30000 Training Loss: 0.05103350430727005\n",
      "Epoch 17047/30000 Training Loss: 0.03934699296951294\n",
      "Epoch 17048/30000 Training Loss: 0.030396342277526855\n",
      "Epoch 17049/30000 Training Loss: 0.05717207491397858\n",
      "Epoch 17050/30000 Training Loss: 0.03556418418884277\n",
      "Epoch 17051/30000 Training Loss: 0.03556368499994278\n",
      "Epoch 17052/30000 Training Loss: 0.06270751357078552\n",
      "Epoch 17053/30000 Training Loss: 0.04392887279391289\n",
      "Epoch 17054/30000 Training Loss: 0.0469108447432518\n",
      "Epoch 17055/30000 Training Loss: 0.03519664704799652\n",
      "Epoch 17056/30000 Training Loss: 0.04373874515295029\n",
      "Epoch 17057/30000 Training Loss: 0.04377439618110657\n",
      "Epoch 17058/30000 Training Loss: 0.04048723354935646\n",
      "Epoch 17059/30000 Training Loss: 0.046733707189559937\n",
      "Epoch 17060/30000 Training Loss: 0.0368531309068203\n",
      "Epoch 17061/30000 Training Loss: 0.037836864590644836\n",
      "Epoch 17062/30000 Training Loss: 0.054951705038547516\n",
      "Epoch 17063/30000 Training Loss: 0.05468752980232239\n",
      "Epoch 17064/30000 Training Loss: 0.04745647311210632\n",
      "Epoch 17065/30000 Training Loss: 0.05002161115407944\n",
      "Epoch 17066/30000 Training Loss: 0.04670250788331032\n",
      "Epoch 17067/30000 Training Loss: 0.04280157387256622\n",
      "Epoch 17068/30000 Training Loss: 0.043606966733932495\n",
      "Epoch 17069/30000 Training Loss: 0.04955995827913284\n",
      "Epoch 17070/30000 Training Loss: 0.04134935885667801\n",
      "Epoch 17071/30000 Training Loss: 0.05609989911317825\n",
      "Epoch 17072/30000 Training Loss: 0.04146774113178253\n",
      "Epoch 17073/30000 Training Loss: 0.05104116350412369\n",
      "Epoch 17074/30000 Training Loss: 0.043151404708623886\n",
      "Epoch 17075/30000 Training Loss: 0.04502261430025101\n",
      "Epoch 17076/30000 Training Loss: 0.04306289553642273\n",
      "Epoch 17077/30000 Training Loss: 0.04374245926737785\n",
      "Epoch 17078/30000 Training Loss: 0.03053773194551468\n",
      "Epoch 17079/30000 Training Loss: 0.041531987488269806\n",
      "Epoch 17080/30000 Training Loss: 0.04319942742586136\n",
      "Epoch 17081/30000 Training Loss: 0.036797020584344864\n",
      "Epoch 17082/30000 Training Loss: 0.04149216413497925\n",
      "Epoch 17083/30000 Training Loss: 0.037986088544130325\n",
      "Epoch 17084/30000 Training Loss: 0.04774033650755882\n",
      "Epoch 17085/30000 Training Loss: 0.039262548089027405\n",
      "Epoch 17086/30000 Training Loss: 0.04587732255458832\n",
      "Epoch 17087/30000 Training Loss: 0.04200863838195801\n",
      "Epoch 17088/30000 Training Loss: 0.03192942962050438\n",
      "Epoch 17089/30000 Training Loss: 0.043312620371580124\n",
      "Epoch 17090/30000 Training Loss: 0.03864461928606033\n",
      "Epoch 17091/30000 Training Loss: 0.03696773573756218\n",
      "Epoch 17092/30000 Training Loss: 0.042420174926519394\n",
      "Epoch 17093/30000 Training Loss: 0.05184570327401161\n",
      "Epoch 17094/30000 Training Loss: 0.046143658459186554\n",
      "Epoch 17095/30000 Training Loss: 0.05028374493122101\n",
      "Epoch 17096/30000 Training Loss: 0.05852469429373741\n",
      "Epoch 17097/30000 Training Loss: 0.055273860692977905\n",
      "Epoch 17098/30000 Training Loss: 0.040677912533283234\n",
      "Epoch 17099/30000 Training Loss: 0.04397495090961456\n",
      "Epoch 17100/30000 Training Loss: 0.035657789558172226\n",
      "Epoch 17100/30000 Validation Loss: 0.03598427772521973\n",
      "Epoch 17101/30000 Training Loss: 0.03828094154596329\n",
      "Epoch 17102/30000 Training Loss: 0.03485240787267685\n",
      "Epoch 17103/30000 Training Loss: 0.04642871022224426\n",
      "Epoch 17104/30000 Training Loss: 0.048377249389886856\n",
      "Epoch 17105/30000 Training Loss: 0.05843270570039749\n",
      "Epoch 17106/30000 Training Loss: 0.055593471974134445\n",
      "Epoch 17107/30000 Training Loss: 0.051216669380664825\n",
      "Epoch 17108/30000 Training Loss: 0.05991128087043762\n",
      "Epoch 17109/30000 Training Loss: 0.050054822117090225\n",
      "Epoch 17110/30000 Training Loss: 0.051230691373348236\n",
      "Epoch 17111/30000 Training Loss: 0.041823677718639374\n",
      "Epoch 17112/30000 Training Loss: 0.05161319673061371\n",
      "Epoch 17113/30000 Training Loss: 0.04678046703338623\n",
      "Epoch 17114/30000 Training Loss: 0.04198944568634033\n",
      "Epoch 17115/30000 Training Loss: 0.041710264980793\n",
      "Epoch 17116/30000 Training Loss: 0.05549349635839462\n",
      "Epoch 17117/30000 Training Loss: 0.05138976871967316\n",
      "Epoch 17118/30000 Training Loss: 0.043112266808748245\n",
      "Epoch 17119/30000 Training Loss: 0.043830931186676025\n",
      "Epoch 17120/30000 Training Loss: 0.0364956371486187\n",
      "Epoch 17121/30000 Training Loss: 0.037809908390045166\n",
      "Epoch 17122/30000 Training Loss: 0.039831146597862244\n",
      "Epoch 17123/30000 Training Loss: 0.038908377289772034\n",
      "Epoch 17124/30000 Training Loss: 0.0440187007188797\n",
      "Epoch 17125/30000 Training Loss: 0.03092959150671959\n",
      "Epoch 17126/30000 Training Loss: 0.06450755149126053\n",
      "Epoch 17127/30000 Training Loss: 0.03606683760881424\n",
      "Epoch 17128/30000 Training Loss: 0.05632910132408142\n",
      "Epoch 17129/30000 Training Loss: 0.059365659952163696\n",
      "Epoch 17130/30000 Training Loss: 0.029059384018182755\n",
      "Epoch 17131/30000 Training Loss: 0.07205957919359207\n",
      "Epoch 17132/30000 Training Loss: 0.05775094032287598\n",
      "Epoch 17133/30000 Training Loss: 0.03215888515114784\n",
      "Epoch 17134/30000 Training Loss: 0.04522208124399185\n",
      "Epoch 17135/30000 Training Loss: 0.04357225075364113\n",
      "Epoch 17136/30000 Training Loss: 0.03735974803566933\n",
      "Epoch 17137/30000 Training Loss: 0.046218618750572205\n",
      "Epoch 17138/30000 Training Loss: 0.04542692005634308\n",
      "Epoch 17139/30000 Training Loss: 0.05023703724145889\n",
      "Epoch 17140/30000 Training Loss: 0.048597149550914764\n",
      "Epoch 17141/30000 Training Loss: 0.05534391850233078\n",
      "Epoch 17142/30000 Training Loss: 0.05002064257860184\n",
      "Epoch 17143/30000 Training Loss: 0.04592658579349518\n",
      "Epoch 17144/30000 Training Loss: 0.05848924070596695\n",
      "Epoch 17145/30000 Training Loss: 0.03743209317326546\n",
      "Epoch 17146/30000 Training Loss: 0.04129679873585701\n",
      "Epoch 17147/30000 Training Loss: 0.03322535380721092\n",
      "Epoch 17148/30000 Training Loss: 0.047646619379520416\n",
      "Epoch 17149/30000 Training Loss: 0.04217343032360077\n",
      "Epoch 17150/30000 Training Loss: 0.04646742343902588\n",
      "Epoch 17151/30000 Training Loss: 0.04451199993491173\n",
      "Epoch 17152/30000 Training Loss: 0.04262816160917282\n",
      "Epoch 17153/30000 Training Loss: 0.039560724049806595\n",
      "Epoch 17154/30000 Training Loss: 0.037609994411468506\n",
      "Epoch 17155/30000 Training Loss: 0.051701318472623825\n",
      "Epoch 17156/30000 Training Loss: 0.044783495366573334\n",
      "Epoch 17157/30000 Training Loss: 0.04232785105705261\n",
      "Epoch 17158/30000 Training Loss: 0.04501569643616676\n",
      "Epoch 17159/30000 Training Loss: 0.0377567782998085\n",
      "Epoch 17160/30000 Training Loss: 0.04235285520553589\n",
      "Epoch 17161/30000 Training Loss: 0.03948292136192322\n",
      "Epoch 17162/30000 Training Loss: 0.0312331672757864\n",
      "Epoch 17163/30000 Training Loss: 0.04698675125837326\n",
      "Epoch 17164/30000 Training Loss: 0.055455707013607025\n",
      "Epoch 17165/30000 Training Loss: 0.04766938462853432\n",
      "Epoch 17166/30000 Training Loss: 0.03769691288471222\n",
      "Epoch 17167/30000 Training Loss: 0.037055157124996185\n",
      "Epoch 17168/30000 Training Loss: 0.04794697463512421\n",
      "Epoch 17169/30000 Training Loss: 0.032627757638692856\n",
      "Epoch 17170/30000 Training Loss: 0.03244167938828468\n",
      "Epoch 17171/30000 Training Loss: 0.04988791048526764\n",
      "Epoch 17172/30000 Training Loss: 0.04316893219947815\n",
      "Epoch 17173/30000 Training Loss: 0.037184204906225204\n",
      "Epoch 17174/30000 Training Loss: 0.049633994698524475\n",
      "Epoch 17175/30000 Training Loss: 0.05018489807844162\n",
      "Epoch 17176/30000 Training Loss: 0.04795670509338379\n",
      "Epoch 17177/30000 Training Loss: 0.04246871545910835\n",
      "Epoch 17178/30000 Training Loss: 0.04519300162792206\n",
      "Epoch 17179/30000 Training Loss: 0.07533778250217438\n",
      "Epoch 17180/30000 Training Loss: 0.04395485669374466\n",
      "Epoch 17181/30000 Training Loss: 0.053966861218214035\n",
      "Epoch 17182/30000 Training Loss: 0.056546736508607864\n",
      "Epoch 17183/30000 Training Loss: 0.04783176630735397\n",
      "Epoch 17184/30000 Training Loss: 0.0606359988451004\n",
      "Epoch 17185/30000 Training Loss: 0.05477473884820938\n",
      "Epoch 17186/30000 Training Loss: 0.04831406846642494\n",
      "Epoch 17187/30000 Training Loss: 0.040552832186222076\n",
      "Epoch 17188/30000 Training Loss: 0.05269535258412361\n",
      "Epoch 17189/30000 Training Loss: 0.04666997864842415\n",
      "Epoch 17190/30000 Training Loss: 0.05045229196548462\n",
      "Epoch 17191/30000 Training Loss: 0.03749306499958038\n",
      "Epoch 17192/30000 Training Loss: 0.035683464258909225\n",
      "Epoch 17193/30000 Training Loss: 0.04507768154144287\n",
      "Epoch 17194/30000 Training Loss: 0.03834322839975357\n",
      "Epoch 17195/30000 Training Loss: 0.048932913690805435\n",
      "Epoch 17196/30000 Training Loss: 0.04600135236978531\n",
      "Epoch 17197/30000 Training Loss: 0.04445049911737442\n",
      "Epoch 17198/30000 Training Loss: 0.040344350039958954\n",
      "Epoch 17199/30000 Training Loss: 0.042805735021829605\n",
      "Epoch 17200/30000 Training Loss: 0.062285520136356354\n",
      "Epoch 17200/30000 Validation Loss: 0.044450026005506516\n",
      "Epoch 17201/30000 Training Loss: 0.05809372663497925\n",
      "Epoch 17202/30000 Training Loss: 0.04925449937582016\n",
      "Epoch 17203/30000 Training Loss: 0.04018693417310715\n",
      "Epoch 17204/30000 Training Loss: 0.04632106050848961\n",
      "Epoch 17205/30000 Training Loss: 0.03419577702879906\n",
      "Epoch 17206/30000 Training Loss: 0.04660247638821602\n",
      "Epoch 17207/30000 Training Loss: 0.039869893342256546\n",
      "Epoch 17208/30000 Training Loss: 0.053074195981025696\n",
      "Epoch 17209/30000 Training Loss: 0.04305683821439743\n",
      "Epoch 17210/30000 Training Loss: 0.04069356620311737\n",
      "Epoch 17211/30000 Training Loss: 0.04901015758514404\n",
      "Epoch 17212/30000 Training Loss: 0.038771726191043854\n",
      "Epoch 17213/30000 Training Loss: 0.05571718141436577\n",
      "Epoch 17214/30000 Training Loss: 0.04252932220697403\n",
      "Epoch 17215/30000 Training Loss: 0.05668880045413971\n",
      "Epoch 17216/30000 Training Loss: 0.05833844095468521\n",
      "Epoch 17217/30000 Training Loss: 0.04412417113780975\n",
      "Epoch 17218/30000 Training Loss: 0.04800418019294739\n",
      "Epoch 17219/30000 Training Loss: 0.05715418606996536\n",
      "Epoch 17220/30000 Training Loss: 0.04159596189856529\n",
      "Epoch 17221/30000 Training Loss: 0.03227265551686287\n",
      "Epoch 17222/30000 Training Loss: 0.0379437655210495\n",
      "Epoch 17223/30000 Training Loss: 0.060259632766246796\n",
      "Epoch 17224/30000 Training Loss: 0.04308360442519188\n",
      "Epoch 17225/30000 Training Loss: 0.03712030500173569\n",
      "Epoch 17226/30000 Training Loss: 0.04954982548952103\n",
      "Epoch 17227/30000 Training Loss: 0.04208092391490936\n",
      "Epoch 17228/30000 Training Loss: 0.042171403765678406\n",
      "Epoch 17229/30000 Training Loss: 0.04744134098291397\n",
      "Epoch 17230/30000 Training Loss: 0.044152308255434036\n",
      "Epoch 17231/30000 Training Loss: 0.031969230622053146\n",
      "Epoch 17232/30000 Training Loss: 0.0509619265794754\n",
      "Epoch 17233/30000 Training Loss: 0.054739080369472504\n",
      "Epoch 17234/30000 Training Loss: 0.0416378378868103\n",
      "Epoch 17235/30000 Training Loss: 0.04293995350599289\n",
      "Epoch 17236/30000 Training Loss: 0.06031769514083862\n",
      "Epoch 17237/30000 Training Loss: 0.04463093727827072\n",
      "Epoch 17238/30000 Training Loss: 0.05630705505609512\n",
      "Epoch 17239/30000 Training Loss: 0.044336266815662384\n",
      "Epoch 17240/30000 Training Loss: 0.05742885172367096\n",
      "Epoch 17241/30000 Training Loss: 0.03579072654247284\n",
      "Epoch 17242/30000 Training Loss: 0.045923106372356415\n",
      "Epoch 17243/30000 Training Loss: 0.0685306116938591\n",
      "Epoch 17244/30000 Training Loss: 0.03104250878095627\n",
      "Epoch 17245/30000 Training Loss: 0.03343382105231285\n",
      "Epoch 17246/30000 Training Loss: 0.051214102655649185\n",
      "Epoch 17247/30000 Training Loss: 0.04376344382762909\n",
      "Epoch 17248/30000 Training Loss: 0.03911704570055008\n",
      "Epoch 17249/30000 Training Loss: 0.047791413962841034\n",
      "Epoch 17250/30000 Training Loss: 0.05505389720201492\n",
      "Epoch 17251/30000 Training Loss: 0.032258424907922745\n",
      "Epoch 17252/30000 Training Loss: 0.04319261386990547\n",
      "Epoch 17253/30000 Training Loss: 0.03521081060171127\n",
      "Epoch 17254/30000 Training Loss: 0.041927263140678406\n",
      "Epoch 17255/30000 Training Loss: 0.05390436202287674\n",
      "Epoch 17256/30000 Training Loss: 0.03730194270610809\n",
      "Epoch 17257/30000 Training Loss: 0.04203030467033386\n",
      "Epoch 17258/30000 Training Loss: 0.04026525467634201\n",
      "Epoch 17259/30000 Training Loss: 0.042353153228759766\n",
      "Epoch 17260/30000 Training Loss: 0.041088663041591644\n",
      "Epoch 17261/30000 Training Loss: 0.054430387914180756\n",
      "Epoch 17262/30000 Training Loss: 0.03135136142373085\n",
      "Epoch 17263/30000 Training Loss: 0.04300076514482498\n",
      "Epoch 17264/30000 Training Loss: 0.04236917197704315\n",
      "Epoch 17265/30000 Training Loss: 0.03838752955198288\n",
      "Epoch 17266/30000 Training Loss: 0.03294334560632706\n",
      "Epoch 17267/30000 Training Loss: 0.04694555327296257\n",
      "Epoch 17268/30000 Training Loss: 0.05405513942241669\n",
      "Epoch 17269/30000 Training Loss: 0.050105948001146317\n",
      "Epoch 17270/30000 Training Loss: 0.03617723658680916\n",
      "Epoch 17271/30000 Training Loss: 0.06112058460712433\n",
      "Epoch 17272/30000 Training Loss: 0.03754515200853348\n",
      "Epoch 17273/30000 Training Loss: 0.04791303724050522\n",
      "Epoch 17274/30000 Training Loss: 0.03879447281360626\n",
      "Epoch 17275/30000 Training Loss: 0.04744381457567215\n",
      "Epoch 17276/30000 Training Loss: 0.04046693444252014\n",
      "Epoch 17277/30000 Training Loss: 0.04588935896754265\n",
      "Epoch 17278/30000 Training Loss: 0.03748983144760132\n",
      "Epoch 17279/30000 Training Loss: 0.028753627091646194\n",
      "Epoch 17280/30000 Training Loss: 0.03984932228922844\n",
      "Epoch 17281/30000 Training Loss: 0.042491789907217026\n",
      "Epoch 17282/30000 Training Loss: 0.041374512016773224\n",
      "Epoch 17283/30000 Training Loss: 0.046369101852178574\n",
      "Epoch 17284/30000 Training Loss: 0.0402185283601284\n",
      "Epoch 17285/30000 Training Loss: 0.05081329122185707\n",
      "Epoch 17286/30000 Training Loss: 0.04108033329248428\n",
      "Epoch 17287/30000 Training Loss: 0.043009839951992035\n",
      "Epoch 17288/30000 Training Loss: 0.04630080237984657\n",
      "Epoch 17289/30000 Training Loss: 0.030767139047384262\n",
      "Epoch 17290/30000 Training Loss: 0.038511741906404495\n",
      "Epoch 17291/30000 Training Loss: 0.027018427848815918\n",
      "Epoch 17292/30000 Training Loss: 0.03525257855653763\n",
      "Epoch 17293/30000 Training Loss: 0.03647544980049133\n",
      "Epoch 17294/30000 Training Loss: 0.03268911689519882\n",
      "Epoch 17295/30000 Training Loss: 0.05896487832069397\n",
      "Epoch 17296/30000 Training Loss: 0.04722239822149277\n",
      "Epoch 17297/30000 Training Loss: 0.04157405346632004\n",
      "Epoch 17298/30000 Training Loss: 0.05045871436595917\n",
      "Epoch 17299/30000 Training Loss: 0.045685216784477234\n",
      "Epoch 17300/30000 Training Loss: 0.05728145316243172\n",
      "Epoch 17300/30000 Validation Loss: 0.0408269464969635\n",
      "Epoch 17301/30000 Training Loss: 0.03706591576337814\n",
      "Epoch 17302/30000 Training Loss: 0.049737654626369476\n",
      "Epoch 17303/30000 Training Loss: 0.032357584685087204\n",
      "Epoch 17304/30000 Training Loss: 0.059230491518974304\n",
      "Epoch 17305/30000 Training Loss: 0.0529848113656044\n",
      "Epoch 17306/30000 Training Loss: 0.04215216264128685\n",
      "Epoch 17307/30000 Training Loss: 0.042776595801115036\n",
      "Epoch 17308/30000 Training Loss: 0.036444731056690216\n",
      "Epoch 17309/30000 Training Loss: 0.057998958975076675\n",
      "Epoch 17310/30000 Training Loss: 0.03606483340263367\n",
      "Epoch 17311/30000 Training Loss: 0.05419851094484329\n",
      "Epoch 17312/30000 Training Loss: 0.04168923944234848\n",
      "Epoch 17313/30000 Training Loss: 0.040233686566352844\n",
      "Epoch 17314/30000 Training Loss: 0.03507870063185692\n",
      "Epoch 17315/30000 Training Loss: 0.04374130815267563\n",
      "Epoch 17316/30000 Training Loss: 0.046553537249565125\n",
      "Epoch 17317/30000 Training Loss: 0.03455628454685211\n",
      "Epoch 17318/30000 Training Loss: 0.04233303666114807\n",
      "Epoch 17319/30000 Training Loss: 0.037195511162281036\n",
      "Epoch 17320/30000 Training Loss: 0.05738002806901932\n",
      "Epoch 17321/30000 Training Loss: 0.04495652765035629\n",
      "Epoch 17322/30000 Training Loss: 0.04378197342157364\n",
      "Epoch 17323/30000 Training Loss: 0.04175589233636856\n",
      "Epoch 17324/30000 Training Loss: 0.03661957383155823\n",
      "Epoch 17325/30000 Training Loss: 0.04564389958977699\n",
      "Epoch 17326/30000 Training Loss: 0.040938131511211395\n",
      "Epoch 17327/30000 Training Loss: 0.06230201572179794\n",
      "Epoch 17328/30000 Training Loss: 0.04142868518829346\n",
      "Epoch 17329/30000 Training Loss: 0.04580722004175186\n",
      "Epoch 17330/30000 Training Loss: 0.0439438596367836\n",
      "Epoch 17331/30000 Training Loss: 0.0437406450510025\n",
      "Epoch 17332/30000 Training Loss: 0.0463474839925766\n",
      "Epoch 17333/30000 Training Loss: 0.05027841031551361\n",
      "Epoch 17334/30000 Training Loss: 0.04242192208766937\n",
      "Epoch 17335/30000 Training Loss: 0.037011973559856415\n",
      "Epoch 17336/30000 Training Loss: 0.04232334345579147\n",
      "Epoch 17337/30000 Training Loss: 0.04609902575612068\n",
      "Epoch 17338/30000 Training Loss: 0.04467782750725746\n",
      "Epoch 17339/30000 Training Loss: 0.03390815109014511\n",
      "Epoch 17340/30000 Training Loss: 0.04721342772245407\n",
      "Epoch 17341/30000 Training Loss: 0.042129017412662506\n",
      "Epoch 17342/30000 Training Loss: 0.06011587381362915\n",
      "Epoch 17343/30000 Training Loss: 0.03650720790028572\n",
      "Epoch 17344/30000 Training Loss: 0.039434876292943954\n",
      "Epoch 17345/30000 Training Loss: 0.04785881191492081\n",
      "Epoch 17346/30000 Training Loss: 0.03844207525253296\n",
      "Epoch 17347/30000 Training Loss: 0.052880190312862396\n",
      "Epoch 17348/30000 Training Loss: 0.038673385977745056\n",
      "Epoch 17349/30000 Training Loss: 0.03556349501013756\n",
      "Epoch 17350/30000 Training Loss: 0.04863304644823074\n",
      "Epoch 17351/30000 Training Loss: 0.05339168384671211\n",
      "Epoch 17352/30000 Training Loss: 0.03243120014667511\n",
      "Epoch 17353/30000 Training Loss: 0.050623323768377304\n",
      "Epoch 17354/30000 Training Loss: 0.03727000951766968\n",
      "Epoch 17355/30000 Training Loss: 0.03514014929533005\n",
      "Epoch 17356/30000 Training Loss: 0.05235173553228378\n",
      "Epoch 17357/30000 Training Loss: 0.041738711297512054\n",
      "Epoch 17358/30000 Training Loss: 0.04995150864124298\n",
      "Epoch 17359/30000 Training Loss: 0.05432048812508583\n",
      "Epoch 17360/30000 Training Loss: 0.036981552839279175\n",
      "Epoch 17361/30000 Training Loss: 0.04320799931883812\n",
      "Epoch 17362/30000 Training Loss: 0.04258112236857414\n",
      "Epoch 17363/30000 Training Loss: 0.0349394828081131\n",
      "Epoch 17364/30000 Training Loss: 0.04808355122804642\n",
      "Epoch 17365/30000 Training Loss: 0.04423476755619049\n",
      "Epoch 17366/30000 Training Loss: 0.042043060064315796\n",
      "Epoch 17367/30000 Training Loss: 0.04515884071588516\n",
      "Epoch 17368/30000 Training Loss: 0.06117404252290726\n",
      "Epoch 17369/30000 Training Loss: 0.030905915424227715\n",
      "Epoch 17370/30000 Training Loss: 0.05270084738731384\n",
      "Epoch 17371/30000 Training Loss: 0.04717482253909111\n",
      "Epoch 17372/30000 Training Loss: 0.048448145389556885\n",
      "Epoch 17373/30000 Training Loss: 0.05565981566905975\n",
      "Epoch 17374/30000 Training Loss: 0.053152356296777725\n",
      "Epoch 17375/30000 Training Loss: 0.06578247994184494\n",
      "Epoch 17376/30000 Training Loss: 0.04619646444916725\n",
      "Epoch 17377/30000 Training Loss: 0.06054774671792984\n",
      "Epoch 17378/30000 Training Loss: 0.037782520055770874\n",
      "Epoch 17379/30000 Training Loss: 0.02685275301337242\n",
      "Epoch 17380/30000 Training Loss: 0.062144793570041656\n",
      "Epoch 17381/30000 Training Loss: 0.050400182604789734\n",
      "Epoch 17382/30000 Training Loss: 0.060676783323287964\n",
      "Epoch 17383/30000 Training Loss: 0.04927307739853859\n",
      "Epoch 17384/30000 Training Loss: 0.050840191543102264\n",
      "Epoch 17385/30000 Training Loss: 0.045671768486499786\n",
      "Epoch 17386/30000 Training Loss: 0.052118852734565735\n",
      "Epoch 17387/30000 Training Loss: 0.03990119695663452\n",
      "Epoch 17388/30000 Training Loss: 0.04998920112848282\n",
      "Epoch 17389/30000 Training Loss: 0.047640133649110794\n",
      "Epoch 17390/30000 Training Loss: 0.03591081127524376\n",
      "Epoch 17391/30000 Training Loss: 0.047801148146390915\n",
      "Epoch 17392/30000 Training Loss: 0.04415210708975792\n",
      "Epoch 17393/30000 Training Loss: 0.05050170421600342\n",
      "Epoch 17394/30000 Training Loss: 0.05123361945152283\n",
      "Epoch 17395/30000 Training Loss: 0.031542833894491196\n",
      "Epoch 17396/30000 Training Loss: 0.043625522404909134\n",
      "Epoch 17397/30000 Training Loss: 0.06321437656879425\n",
      "Epoch 17398/30000 Training Loss: 0.04005550220608711\n",
      "Epoch 17399/30000 Training Loss: 0.05196014791727066\n",
      "Epoch 17400/30000 Training Loss: 0.06508941948413849\n",
      "Epoch 17400/30000 Validation Loss: 0.04407738894224167\n",
      "Epoch 17401/30000 Training Loss: 0.05150286853313446\n",
      "Epoch 17402/30000 Training Loss: 0.02982296422123909\n",
      "Epoch 17403/30000 Training Loss: 0.04547922685742378\n",
      "Epoch 17404/30000 Training Loss: 0.04435579851269722\n",
      "Epoch 17405/30000 Training Loss: 0.04753579571843147\n",
      "Epoch 17406/30000 Training Loss: 0.040358543395996094\n",
      "Epoch 17407/30000 Training Loss: 0.03440633416175842\n",
      "Epoch 17408/30000 Training Loss: 0.05621153861284256\n",
      "Epoch 17409/30000 Training Loss: 0.051716603338718414\n",
      "Epoch 17410/30000 Training Loss: 0.03741031885147095\n",
      "Epoch 17411/30000 Training Loss: 0.04650253430008888\n",
      "Epoch 17412/30000 Training Loss: 0.0401344820857048\n",
      "Epoch 17413/30000 Training Loss: 0.04428139328956604\n",
      "Epoch 17414/30000 Training Loss: 0.04748748242855072\n",
      "Epoch 17415/30000 Training Loss: 0.054089970886707306\n",
      "Epoch 17416/30000 Training Loss: 0.044625118374824524\n",
      "Epoch 17417/30000 Training Loss: 0.04259123653173447\n",
      "Epoch 17418/30000 Training Loss: 0.05586254224181175\n",
      "Epoch 17419/30000 Training Loss: 0.041945282369852066\n",
      "Epoch 17420/30000 Training Loss: 0.058403994888067245\n",
      "Epoch 17421/30000 Training Loss: 0.05020252242684364\n",
      "Epoch 17422/30000 Training Loss: 0.04540884867310524\n",
      "Epoch 17423/30000 Training Loss: 0.04983139783143997\n",
      "Epoch 17424/30000 Training Loss: 0.05419024080038071\n",
      "Epoch 17425/30000 Training Loss: 0.03448154777288437\n",
      "Epoch 17426/30000 Training Loss: 0.0481816902756691\n",
      "Epoch 17427/30000 Training Loss: 0.05936415493488312\n",
      "Epoch 17428/30000 Training Loss: 0.040727004408836365\n",
      "Epoch 17429/30000 Training Loss: 0.04847398027777672\n",
      "Epoch 17430/30000 Training Loss: 0.04284006357192993\n",
      "Epoch 17431/30000 Training Loss: 0.047207243740558624\n",
      "Epoch 17432/30000 Training Loss: 0.04635061323642731\n",
      "Epoch 17433/30000 Training Loss: 0.046874962747097015\n",
      "Epoch 17434/30000 Training Loss: 0.031903889030218124\n",
      "Epoch 17435/30000 Training Loss: 0.04015934839844704\n",
      "Epoch 17436/30000 Training Loss: 0.04215511307120323\n",
      "Epoch 17437/30000 Training Loss: 0.03628358617424965\n",
      "Epoch 17438/30000 Training Loss: 0.03359101712703705\n",
      "Epoch 17439/30000 Training Loss: 0.03819679841399193\n",
      "Epoch 17440/30000 Training Loss: 0.032630804926157\n",
      "Epoch 17441/30000 Training Loss: 0.046037569642066956\n",
      "Epoch 17442/30000 Training Loss: 0.03486539050936699\n",
      "Epoch 17443/30000 Training Loss: 0.036053575575351715\n",
      "Epoch 17444/30000 Training Loss: 0.04428841546177864\n",
      "Epoch 17445/30000 Training Loss: 0.04623018950223923\n",
      "Epoch 17446/30000 Training Loss: 0.03521481156349182\n",
      "Epoch 17447/30000 Training Loss: 0.04603612422943115\n",
      "Epoch 17448/30000 Training Loss: 0.0574595108628273\n",
      "Epoch 17449/30000 Training Loss: 0.05746524780988693\n",
      "Epoch 17450/30000 Training Loss: 0.039227958768606186\n",
      "Epoch 17451/30000 Training Loss: 0.044384654611349106\n",
      "Epoch 17452/30000 Training Loss: 0.049843162298202515\n",
      "Epoch 17453/30000 Training Loss: 0.0435028001666069\n",
      "Epoch 17454/30000 Training Loss: 0.04647437855601311\n",
      "Epoch 17455/30000 Training Loss: 0.04772696644067764\n",
      "Epoch 17456/30000 Training Loss: 0.04310782253742218\n",
      "Epoch 17457/30000 Training Loss: 0.03427648916840553\n",
      "Epoch 17458/30000 Training Loss: 0.051060572266578674\n",
      "Epoch 17459/30000 Training Loss: 0.04845232889056206\n",
      "Epoch 17460/30000 Training Loss: 0.05598872900009155\n",
      "Epoch 17461/30000 Training Loss: 0.035564497113227844\n",
      "Epoch 17462/30000 Training Loss: 0.04531693458557129\n",
      "Epoch 17463/30000 Training Loss: 0.04524282366037369\n",
      "Epoch 17464/30000 Training Loss: 0.05953073501586914\n",
      "Epoch 17465/30000 Training Loss: 0.050686437636613846\n",
      "Epoch 17466/30000 Training Loss: 0.04593110457062721\n",
      "Epoch 17467/30000 Training Loss: 0.04161437600851059\n",
      "Epoch 17468/30000 Training Loss: 0.06113588809967041\n",
      "Epoch 17469/30000 Training Loss: 0.044841013848781586\n",
      "Epoch 17470/30000 Training Loss: 0.053686536848545074\n",
      "Epoch 17471/30000 Training Loss: 0.056362297385931015\n",
      "Epoch 17472/30000 Training Loss: 0.04746416211128235\n",
      "Epoch 17473/30000 Training Loss: 0.04206749051809311\n",
      "Epoch 17474/30000 Training Loss: 0.047120727598667145\n",
      "Epoch 17475/30000 Training Loss: 0.03877909108996391\n",
      "Epoch 17476/30000 Training Loss: 0.05243728682398796\n",
      "Epoch 17477/30000 Training Loss: 0.04816027730703354\n",
      "Epoch 17478/30000 Training Loss: 0.05496454983949661\n",
      "Epoch 17479/30000 Training Loss: 0.048308100551366806\n",
      "Epoch 17480/30000 Training Loss: 0.04756489396095276\n",
      "Epoch 17481/30000 Training Loss: 0.03656059503555298\n",
      "Epoch 17482/30000 Training Loss: 0.06810589879751205\n",
      "Epoch 17483/30000 Training Loss: 0.05560300499200821\n",
      "Epoch 17484/30000 Training Loss: 0.05228668823838234\n",
      "Epoch 17485/30000 Training Loss: 0.049509964883327484\n",
      "Epoch 17486/30000 Training Loss: 0.05767256021499634\n",
      "Epoch 17487/30000 Training Loss: 0.032893382012844086\n",
      "Epoch 17488/30000 Training Loss: 0.047701120376586914\n",
      "Epoch 17489/30000 Training Loss: 0.039643026888370514\n",
      "Epoch 17490/30000 Training Loss: 0.046572357416152954\n",
      "Epoch 17491/30000 Training Loss: 0.0413697212934494\n",
      "Epoch 17492/30000 Training Loss: 0.046423912048339844\n",
      "Epoch 17493/30000 Training Loss: 0.05707380175590515\n",
      "Epoch 17494/30000 Training Loss: 0.042102232575416565\n",
      "Epoch 17495/30000 Training Loss: 0.052855588495731354\n",
      "Epoch 17496/30000 Training Loss: 0.02971506118774414\n",
      "Epoch 17497/30000 Training Loss: 0.04901565983891487\n",
      "Epoch 17498/30000 Training Loss: 0.043565258383750916\n",
      "Epoch 17499/30000 Training Loss: 0.03820841759443283\n",
      "Epoch 17500/30000 Training Loss: 0.03009919449687004\n",
      "Epoch 17500/30000 Validation Loss: 0.05192902684211731\n",
      "Epoch 17501/30000 Training Loss: 0.04038334637880325\n",
      "Epoch 17502/30000 Training Loss: 0.05844435095787048\n",
      "Epoch 17503/30000 Training Loss: 0.04204101487994194\n",
      "Epoch 17504/30000 Training Loss: 0.061119578778743744\n",
      "Epoch 17505/30000 Training Loss: 0.04786165803670883\n",
      "Epoch 17506/30000 Training Loss: 0.0455441027879715\n",
      "Epoch 17507/30000 Training Loss: 0.04185614362359047\n",
      "Epoch 17508/30000 Training Loss: 0.04512912780046463\n",
      "Epoch 17509/30000 Training Loss: 0.0501663014292717\n",
      "Epoch 17510/30000 Training Loss: 0.043374769389629364\n",
      "Epoch 17511/30000 Training Loss: 0.039917077869176865\n",
      "Epoch 17512/30000 Training Loss: 0.04520198702812195\n",
      "Epoch 17513/30000 Training Loss: 0.043485306203365326\n",
      "Epoch 17514/30000 Training Loss: 0.043819963932037354\n",
      "Epoch 17515/30000 Training Loss: 0.04167761281132698\n",
      "Epoch 17516/30000 Training Loss: 0.048209138214588165\n",
      "Epoch 17517/30000 Training Loss: 0.055772215127944946\n",
      "Epoch 17518/30000 Training Loss: 0.05105692893266678\n",
      "Epoch 17519/30000 Training Loss: 0.05657828226685524\n",
      "Epoch 17520/30000 Training Loss: 0.0493297353386879\n",
      "Epoch 17521/30000 Training Loss: 0.047991685569286346\n",
      "Epoch 17522/30000 Training Loss: 0.03770677000284195\n",
      "Epoch 17523/30000 Training Loss: 0.04234584420919418\n",
      "Epoch 17524/30000 Training Loss: 0.04404780641198158\n",
      "Epoch 17525/30000 Training Loss: 0.08039246499538422\n",
      "Epoch 17526/30000 Training Loss: 0.039413031190633774\n",
      "Epoch 17527/30000 Training Loss: 0.04845442250370979\n",
      "Epoch 17528/30000 Training Loss: 0.04400899261236191\n",
      "Epoch 17529/30000 Training Loss: 0.03342558071017265\n",
      "Epoch 17530/30000 Training Loss: 0.0430062860250473\n",
      "Epoch 17531/30000 Training Loss: 0.045157596468925476\n",
      "Epoch 17532/30000 Training Loss: 0.04814609885215759\n",
      "Epoch 17533/30000 Training Loss: 0.04779800772666931\n",
      "Epoch 17534/30000 Training Loss: 0.053115054965019226\n",
      "Epoch 17535/30000 Training Loss: 0.033784572035074234\n",
      "Epoch 17536/30000 Training Loss: 0.04831985384225845\n",
      "Epoch 17537/30000 Training Loss: 0.06612417101860046\n",
      "Epoch 17538/30000 Training Loss: 0.02885684370994568\n",
      "Epoch 17539/30000 Training Loss: 0.052078843116760254\n",
      "Epoch 17540/30000 Training Loss: 0.05697844922542572\n",
      "Epoch 17541/30000 Training Loss: 0.03385335952043533\n",
      "Epoch 17542/30000 Training Loss: 0.03531774878501892\n",
      "Epoch 17543/30000 Training Loss: 0.034491755068302155\n",
      "Epoch 17544/30000 Training Loss: 0.045838791877031326\n",
      "Epoch 17545/30000 Training Loss: 0.045775171369314194\n",
      "Epoch 17546/30000 Training Loss: 0.04386631026864052\n",
      "Epoch 17547/30000 Training Loss: 0.044164471328258514\n",
      "Epoch 17548/30000 Training Loss: 0.04211794212460518\n",
      "Epoch 17549/30000 Training Loss: 0.052364740520715714\n",
      "Epoch 17550/30000 Training Loss: 0.05277106165885925\n",
      "Epoch 17551/30000 Training Loss: 0.05185941979289055\n",
      "Epoch 17552/30000 Training Loss: 0.04774666577577591\n",
      "Epoch 17553/30000 Training Loss: 0.04191426560282707\n",
      "Epoch 17554/30000 Training Loss: 0.035121850669384\n",
      "Epoch 17555/30000 Training Loss: 0.04749749228358269\n",
      "Epoch 17556/30000 Training Loss: 0.04777609556913376\n",
      "Epoch 17557/30000 Training Loss: 0.04366860166192055\n",
      "Epoch 17558/30000 Training Loss: 0.049277760088443756\n",
      "Epoch 17559/30000 Training Loss: 0.03567035496234894\n",
      "Epoch 17560/30000 Training Loss: 0.0562596470117569\n",
      "Epoch 17561/30000 Training Loss: 0.03603934496641159\n",
      "Epoch 17562/30000 Training Loss: 0.03813605755567551\n",
      "Epoch 17563/30000 Training Loss: 0.04911430925130844\n",
      "Epoch 17564/30000 Training Loss: 0.04723126068711281\n",
      "Epoch 17565/30000 Training Loss: 0.04129297286272049\n",
      "Epoch 17566/30000 Training Loss: 0.041796207427978516\n",
      "Epoch 17567/30000 Training Loss: 0.043833304196596146\n",
      "Epoch 17568/30000 Training Loss: 0.047015849500894547\n",
      "Epoch 17569/30000 Training Loss: 0.030849821865558624\n",
      "Epoch 17570/30000 Training Loss: 0.03771847486495972\n",
      "Epoch 17571/30000 Training Loss: 0.049316201359033585\n",
      "Epoch 17572/30000 Training Loss: 0.061123326420784\n",
      "Epoch 17573/30000 Training Loss: 0.06606819480657578\n",
      "Epoch 17574/30000 Training Loss: 0.03676917403936386\n",
      "Epoch 17575/30000 Training Loss: 0.05551602691411972\n",
      "Epoch 17576/30000 Training Loss: 0.038526635617017746\n",
      "Epoch 17577/30000 Training Loss: 0.048753999173641205\n",
      "Epoch 17578/30000 Training Loss: 0.03985578194260597\n",
      "Epoch 17579/30000 Training Loss: 0.029853669926524162\n",
      "Epoch 17580/30000 Training Loss: 0.0484003871679306\n",
      "Epoch 17581/30000 Training Loss: 0.05057583749294281\n",
      "Epoch 17582/30000 Training Loss: 0.04464132338762283\n",
      "Epoch 17583/30000 Training Loss: 0.033414509147405624\n",
      "Epoch 17584/30000 Training Loss: 0.036496445536613464\n",
      "Epoch 17585/30000 Training Loss: 0.04602766036987305\n",
      "Epoch 17586/30000 Training Loss: 0.03724036365747452\n",
      "Epoch 17587/30000 Training Loss: 0.04295695200562477\n",
      "Epoch 17588/30000 Training Loss: 0.03860300034284592\n",
      "Epoch 17589/30000 Training Loss: 0.04076819121837616\n",
      "Epoch 17590/30000 Training Loss: 0.050047121942043304\n",
      "Epoch 17591/30000 Training Loss: 0.06630030274391174\n",
      "Epoch 17592/30000 Training Loss: 0.03627261519432068\n",
      "Epoch 17593/30000 Training Loss: 0.04371291399002075\n",
      "Epoch 17594/30000 Training Loss: 0.060575321316719055\n",
      "Epoch 17595/30000 Training Loss: 0.042696453630924225\n",
      "Epoch 17596/30000 Training Loss: 0.04362819343805313\n",
      "Epoch 17597/30000 Training Loss: 0.04793909937143326\n",
      "Epoch 17598/30000 Training Loss: 0.060127079486846924\n",
      "Epoch 17599/30000 Training Loss: 0.03634472191333771\n",
      "Epoch 17600/30000 Training Loss: 0.050799258053302765\n",
      "Epoch 17600/30000 Validation Loss: 0.04473014920949936\n",
      "Epoch 17601/30000 Training Loss: 0.03640848770737648\n",
      "Epoch 17602/30000 Training Loss: 0.040768012404441833\n",
      "Epoch 17603/30000 Training Loss: 0.04234553873538971\n",
      "Epoch 17604/30000 Training Loss: 0.058653220534324646\n",
      "Epoch 17605/30000 Training Loss: 0.04830846190452576\n",
      "Epoch 17606/30000 Training Loss: 0.04622228443622589\n",
      "Epoch 17607/30000 Training Loss: 0.05167992785573006\n",
      "Epoch 17608/30000 Training Loss: 0.061252109706401825\n",
      "Epoch 17609/30000 Training Loss: 0.048169076442718506\n",
      "Epoch 17610/30000 Training Loss: 0.031585443764925\n",
      "Epoch 17611/30000 Training Loss: 0.04359748587012291\n",
      "Epoch 17612/30000 Training Loss: 0.03920646756887436\n",
      "Epoch 17613/30000 Training Loss: 0.040037382394075394\n",
      "Epoch 17614/30000 Training Loss: 0.04093087464570999\n",
      "Epoch 17615/30000 Training Loss: 0.051061443984508514\n",
      "Epoch 17616/30000 Training Loss: 0.06165589392185211\n",
      "Epoch 17617/30000 Training Loss: 0.05402389168739319\n",
      "Epoch 17618/30000 Training Loss: 0.04384910687804222\n",
      "Epoch 17619/30000 Training Loss: 0.03902316838502884\n",
      "Epoch 17620/30000 Training Loss: 0.06156480312347412\n",
      "Epoch 17621/30000 Training Loss: 0.04378992319107056\n",
      "Epoch 17622/30000 Training Loss: 0.055934954434633255\n",
      "Epoch 17623/30000 Training Loss: 0.04333323612809181\n",
      "Epoch 17624/30000 Training Loss: 0.036746859550476074\n",
      "Epoch 17625/30000 Training Loss: 0.059834547340869904\n",
      "Epoch 17626/30000 Training Loss: 0.04508841410279274\n",
      "Epoch 17627/30000 Training Loss: 0.03858306631445885\n",
      "Epoch 17628/30000 Training Loss: 0.04896785691380501\n",
      "Epoch 17629/30000 Training Loss: 0.038898106664419174\n",
      "Epoch 17630/30000 Training Loss: 0.05110256373882294\n",
      "Epoch 17631/30000 Training Loss: 0.04927387833595276\n",
      "Epoch 17632/30000 Training Loss: 0.04248710721731186\n",
      "Epoch 17633/30000 Training Loss: 0.05081940442323685\n",
      "Epoch 17634/30000 Training Loss: 0.043390996754169464\n",
      "Epoch 17635/30000 Training Loss: 0.041834212839603424\n",
      "Epoch 17636/30000 Training Loss: 0.056246332824230194\n",
      "Epoch 17637/30000 Training Loss: 0.0452532097697258\n",
      "Epoch 17638/30000 Training Loss: 0.03472384065389633\n",
      "Epoch 17639/30000 Training Loss: 0.03242221847176552\n",
      "Epoch 17640/30000 Training Loss: 0.04443451762199402\n",
      "Epoch 17641/30000 Training Loss: 0.04116620123386383\n",
      "Epoch 17642/30000 Training Loss: 0.03740984946489334\n",
      "Epoch 17643/30000 Training Loss: 0.03912236541509628\n",
      "Epoch 17644/30000 Training Loss: 0.05055110156536102\n",
      "Epoch 17645/30000 Training Loss: 0.04842427000403404\n",
      "Epoch 17646/30000 Training Loss: 0.04123909771442413\n",
      "Epoch 17647/30000 Training Loss: 0.04008127748966217\n",
      "Epoch 17648/30000 Training Loss: 0.04771453142166138\n",
      "Epoch 17649/30000 Training Loss: 0.033362701535224915\n",
      "Epoch 17650/30000 Training Loss: 0.03695335239171982\n",
      "Epoch 17651/30000 Training Loss: 0.06019381806254387\n",
      "Epoch 17652/30000 Training Loss: 0.03772281855344772\n",
      "Epoch 17653/30000 Training Loss: 0.04757741838693619\n",
      "Epoch 17654/30000 Training Loss: 0.0445258803665638\n",
      "Epoch 17655/30000 Training Loss: 0.04570566490292549\n",
      "Epoch 17656/30000 Training Loss: 0.0512508824467659\n",
      "Epoch 17657/30000 Training Loss: 0.04653473198413849\n",
      "Epoch 17658/30000 Training Loss: 0.03513999283313751\n",
      "Epoch 17659/30000 Training Loss: 0.039834290742874146\n",
      "Epoch 17660/30000 Training Loss: 0.04923607409000397\n",
      "Epoch 17661/30000 Training Loss: 0.04494442418217659\n",
      "Epoch 17662/30000 Training Loss: 0.04720804840326309\n",
      "Epoch 17663/30000 Training Loss: 0.03415607661008835\n",
      "Epoch 17664/30000 Training Loss: 0.05007246881723404\n",
      "Epoch 17665/30000 Training Loss: 0.04022440314292908\n",
      "Epoch 17666/30000 Training Loss: 0.04254115745425224\n",
      "Epoch 17667/30000 Training Loss: 0.035867247730493546\n",
      "Epoch 17668/30000 Training Loss: 0.05718737840652466\n",
      "Epoch 17669/30000 Training Loss: 0.04396231472492218\n",
      "Epoch 17670/30000 Training Loss: 0.03720004856586456\n",
      "Epoch 17671/30000 Training Loss: 0.05242231488227844\n",
      "Epoch 17672/30000 Training Loss: 0.0369407944381237\n",
      "Epoch 17673/30000 Training Loss: 0.05190017446875572\n",
      "Epoch 17674/30000 Training Loss: 0.031610868871212006\n",
      "Epoch 17675/30000 Training Loss: 0.05205401033163071\n",
      "Epoch 17676/30000 Training Loss: 0.03387477248907089\n",
      "Epoch 17677/30000 Training Loss: 0.0478794164955616\n",
      "Epoch 17678/30000 Training Loss: 0.04144503176212311\n",
      "Epoch 17679/30000 Training Loss: 0.03720153495669365\n",
      "Epoch 17680/30000 Training Loss: 0.03285756707191467\n",
      "Epoch 17681/30000 Training Loss: 0.04579981043934822\n",
      "Epoch 17682/30000 Training Loss: 0.04060180485248566\n",
      "Epoch 17683/30000 Training Loss: 0.05665102228522301\n",
      "Epoch 17684/30000 Training Loss: 0.05697757378220558\n",
      "Epoch 17685/30000 Training Loss: 0.04311809688806534\n",
      "Epoch 17686/30000 Training Loss: 0.05603682994842529\n",
      "Epoch 17687/30000 Training Loss: 0.03866163641214371\n",
      "Epoch 17688/30000 Training Loss: 0.04093868285417557\n",
      "Epoch 17689/30000 Training Loss: 0.05438649281859398\n",
      "Epoch 17690/30000 Training Loss: 0.03805821016430855\n",
      "Epoch 17691/30000 Training Loss: 0.05639255791902542\n",
      "Epoch 17692/30000 Training Loss: 0.04325300455093384\n",
      "Epoch 17693/30000 Training Loss: 0.04671306163072586\n",
      "Epoch 17694/30000 Training Loss: 0.04636625573039055\n",
      "Epoch 17695/30000 Training Loss: 0.03611670061945915\n",
      "Epoch 17696/30000 Training Loss: 0.04828280210494995\n",
      "Epoch 17697/30000 Training Loss: 0.045690491795539856\n",
      "Epoch 17698/30000 Training Loss: 0.05017692223191261\n",
      "Epoch 17699/30000 Training Loss: 0.04769524931907654\n",
      "Epoch 17700/30000 Training Loss: 0.04280678927898407\n",
      "Epoch 17700/30000 Validation Loss: 0.06228701397776604\n",
      "Epoch 17701/30000 Training Loss: 0.02964729070663452\n",
      "Epoch 17702/30000 Training Loss: 0.04087766259908676\n",
      "Epoch 17703/30000 Training Loss: 0.04175154119729996\n",
      "Epoch 17704/30000 Training Loss: 0.03396572172641754\n",
      "Epoch 17705/30000 Training Loss: 0.040056440979242325\n",
      "Epoch 17706/30000 Training Loss: 0.03418322280049324\n",
      "Epoch 17707/30000 Training Loss: 0.04048977047204971\n",
      "Epoch 17708/30000 Training Loss: 0.038880348205566406\n",
      "Epoch 17709/30000 Training Loss: 0.04482034593820572\n",
      "Epoch 17710/30000 Training Loss: 0.06259205937385559\n",
      "Epoch 17711/30000 Training Loss: 0.03671959787607193\n",
      "Epoch 17712/30000 Training Loss: 0.041069068014621735\n",
      "Epoch 17713/30000 Training Loss: 0.06448525935411453\n",
      "Epoch 17714/30000 Training Loss: 0.044767603278160095\n",
      "Epoch 17715/30000 Training Loss: 0.040059614926576614\n",
      "Epoch 17716/30000 Training Loss: 0.05867507681250572\n",
      "Epoch 17717/30000 Training Loss: 0.0383492037653923\n",
      "Epoch 17718/30000 Training Loss: 0.035095635801553726\n",
      "Epoch 17719/30000 Training Loss: 0.04899077117443085\n",
      "Epoch 17720/30000 Training Loss: 0.03093918412923813\n",
      "Epoch 17721/30000 Training Loss: 0.04114273563027382\n",
      "Epoch 17722/30000 Training Loss: 0.050809018313884735\n",
      "Epoch 17723/30000 Training Loss: 0.0417134128510952\n",
      "Epoch 17724/30000 Training Loss: 0.038746178150177\n",
      "Epoch 17725/30000 Training Loss: 0.034203581511974335\n",
      "Epoch 17726/30000 Training Loss: 0.05705451965332031\n",
      "Epoch 17727/30000 Training Loss: 0.06017538160085678\n",
      "Epoch 17728/30000 Training Loss: 0.049197711050510406\n",
      "Epoch 17729/30000 Training Loss: 0.06795446574687958\n",
      "Epoch 17730/30000 Training Loss: 0.04177664965391159\n",
      "Epoch 17731/30000 Training Loss: 0.03425019979476929\n",
      "Epoch 17732/30000 Training Loss: 0.04372827708721161\n",
      "Epoch 17733/30000 Training Loss: 0.04442214220762253\n",
      "Epoch 17734/30000 Training Loss: 0.049127884209156036\n",
      "Epoch 17735/30000 Training Loss: 0.03463509678840637\n",
      "Epoch 17736/30000 Training Loss: 0.0473124161362648\n",
      "Epoch 17737/30000 Training Loss: 0.03580520674586296\n",
      "Epoch 17738/30000 Training Loss: 0.045287515968084335\n",
      "Epoch 17739/30000 Training Loss: 0.0582025945186615\n",
      "Epoch 17740/30000 Training Loss: 0.05549998953938484\n",
      "Epoch 17741/30000 Training Loss: 0.03912216052412987\n",
      "Epoch 17742/30000 Training Loss: 0.046772539615631104\n",
      "Epoch 17743/30000 Training Loss: 0.05534813925623894\n",
      "Epoch 17744/30000 Training Loss: 0.05179733783006668\n",
      "Epoch 17745/30000 Training Loss: 0.0514083094894886\n",
      "Epoch 17746/30000 Training Loss: 0.04562406986951828\n",
      "Epoch 17747/30000 Training Loss: 0.04155190289020538\n",
      "Epoch 17748/30000 Training Loss: 0.0496366024017334\n",
      "Epoch 17749/30000 Training Loss: 0.03884078934788704\n",
      "Epoch 17750/30000 Training Loss: 0.04135386645793915\n",
      "Epoch 17751/30000 Training Loss: 0.04815736413002014\n",
      "Epoch 17752/30000 Training Loss: 0.044731736183166504\n",
      "Epoch 17753/30000 Training Loss: 0.04875310882925987\n",
      "Epoch 17754/30000 Training Loss: 0.04905523359775543\n",
      "Epoch 17755/30000 Training Loss: 0.045942652970552444\n",
      "Epoch 17756/30000 Training Loss: 0.047710008919239044\n",
      "Epoch 17757/30000 Training Loss: 0.03352025896310806\n",
      "Epoch 17758/30000 Training Loss: 0.04371748864650726\n",
      "Epoch 17759/30000 Training Loss: 0.035418204963207245\n",
      "Epoch 17760/30000 Training Loss: 0.035312991589307785\n",
      "Epoch 17761/30000 Training Loss: 0.05024904012680054\n",
      "Epoch 17762/30000 Training Loss: 0.0294463150203228\n",
      "Epoch 17763/30000 Training Loss: 0.0342220738530159\n",
      "Epoch 17764/30000 Training Loss: 0.046143144369125366\n",
      "Epoch 17765/30000 Training Loss: 0.04517496004700661\n",
      "Epoch 17766/30000 Training Loss: 0.04419173672795296\n",
      "Epoch 17767/30000 Training Loss: 0.05297328531742096\n",
      "Epoch 17768/30000 Training Loss: 0.05462116003036499\n",
      "Epoch 17769/30000 Training Loss: 0.04515080526471138\n",
      "Epoch 17770/30000 Training Loss: 0.039584122598171234\n",
      "Epoch 17771/30000 Training Loss: 0.037541463971138\n",
      "Epoch 17772/30000 Training Loss: 0.04703839123249054\n",
      "Epoch 17773/30000 Training Loss: 0.0475638173520565\n",
      "Epoch 17774/30000 Training Loss: 0.05770602077245712\n",
      "Epoch 17775/30000 Training Loss: 0.03442923724651337\n",
      "Epoch 17776/30000 Training Loss: 0.04043853282928467\n",
      "Epoch 17777/30000 Training Loss: 0.04622877389192581\n",
      "Epoch 17778/30000 Training Loss: 0.05177071690559387\n",
      "Epoch 17779/30000 Training Loss: 0.04696756973862648\n",
      "Epoch 17780/30000 Training Loss: 0.04289146512746811\n",
      "Epoch 17781/30000 Training Loss: 0.04732290655374527\n",
      "Epoch 17782/30000 Training Loss: 0.0373474657535553\n",
      "Epoch 17783/30000 Training Loss: 0.0444205179810524\n",
      "Epoch 17784/30000 Training Loss: 0.04053515940904617\n",
      "Epoch 17785/30000 Training Loss: 0.04025617241859436\n",
      "Epoch 17786/30000 Training Loss: 0.048276472836732864\n",
      "Epoch 17787/30000 Training Loss: 0.04291463643312454\n",
      "Epoch 17788/30000 Training Loss: 0.055032920092344284\n",
      "Epoch 17789/30000 Training Loss: 0.03897549957036972\n",
      "Epoch 17790/30000 Training Loss: 0.05647147446870804\n",
      "Epoch 17791/30000 Training Loss: 0.03986605256795883\n",
      "Epoch 17792/30000 Training Loss: 0.037922464311122894\n",
      "Epoch 17793/30000 Training Loss: 0.05036152899265289\n",
      "Epoch 17794/30000 Training Loss: 0.042688969522714615\n",
      "Epoch 17795/30000 Training Loss: 0.039037592709064484\n",
      "Epoch 17796/30000 Training Loss: 0.042727239429950714\n",
      "Epoch 17797/30000 Training Loss: 0.0446297712624073\n",
      "Epoch 17798/30000 Training Loss: 0.04986869543790817\n",
      "Epoch 17799/30000 Training Loss: 0.039696138352155685\n",
      "Epoch 17800/30000 Training Loss: 0.034558799117803574\n",
      "Epoch 17800/30000 Validation Loss: 0.04915136843919754\n",
      "Epoch 17801/30000 Training Loss: 0.04164077341556549\n",
      "Epoch 17802/30000 Training Loss: 0.04266754537820816\n",
      "Epoch 17803/30000 Training Loss: 0.04286067932844162\n",
      "Epoch 17804/30000 Training Loss: 0.040198009461164474\n",
      "Epoch 17805/30000 Training Loss: 0.051695503294467926\n",
      "Epoch 17806/30000 Training Loss: 0.05162113904953003\n",
      "Epoch 17807/30000 Training Loss: 0.04740660637617111\n",
      "Epoch 17808/30000 Training Loss: 0.0443800650537014\n",
      "Epoch 17809/30000 Training Loss: 0.039243098348379135\n",
      "Epoch 17810/30000 Training Loss: 0.04898504167795181\n",
      "Epoch 17811/30000 Training Loss: 0.05115935951471329\n",
      "Epoch 17812/30000 Training Loss: 0.04119290038943291\n",
      "Epoch 17813/30000 Training Loss: 0.03867678344249725\n",
      "Epoch 17814/30000 Training Loss: 0.04160258546471596\n",
      "Epoch 17815/30000 Training Loss: 0.06719920039176941\n",
      "Epoch 17816/30000 Training Loss: 0.03969961404800415\n",
      "Epoch 17817/30000 Training Loss: 0.053034707903862\n",
      "Epoch 17818/30000 Training Loss: 0.04755707457661629\n",
      "Epoch 17819/30000 Training Loss: 0.049345143139362335\n",
      "Epoch 17820/30000 Training Loss: 0.03595085069537163\n",
      "Epoch 17821/30000 Training Loss: 0.0501040555536747\n",
      "Epoch 17822/30000 Training Loss: 0.045453935861587524\n",
      "Epoch 17823/30000 Training Loss: 0.03845750540494919\n",
      "Epoch 17824/30000 Training Loss: 0.04931225627660751\n",
      "Epoch 17825/30000 Training Loss: 0.046517275273799896\n",
      "Epoch 17826/30000 Training Loss: 0.047285933047533035\n",
      "Epoch 17827/30000 Training Loss: 0.03882497549057007\n",
      "Epoch 17828/30000 Training Loss: 0.03978420048952103\n",
      "Epoch 17829/30000 Training Loss: 0.04794549569487572\n",
      "Epoch 17830/30000 Training Loss: 0.046362556517124176\n",
      "Epoch 17831/30000 Training Loss: 0.03902347758412361\n",
      "Epoch 17832/30000 Training Loss: 0.04735155031085014\n",
      "Epoch 17833/30000 Training Loss: 0.04637560248374939\n",
      "Epoch 17834/30000 Training Loss: 0.033299677073955536\n",
      "Epoch 17835/30000 Training Loss: 0.044261034578084946\n",
      "Epoch 17836/30000 Training Loss: 0.038866713643074036\n",
      "Epoch 17837/30000 Training Loss: 0.039634183049201965\n",
      "Epoch 17838/30000 Training Loss: 0.04367370158433914\n",
      "Epoch 17839/30000 Training Loss: 0.05029510706663132\n",
      "Epoch 17840/30000 Training Loss: 0.042620524764060974\n",
      "Epoch 17841/30000 Training Loss: 0.051770977675914764\n",
      "Epoch 17842/30000 Training Loss: 0.034415360540151596\n",
      "Epoch 17843/30000 Training Loss: 0.03348560631275177\n",
      "Epoch 17844/30000 Training Loss: 0.041401125490665436\n",
      "Epoch 17845/30000 Training Loss: 0.04555906355381012\n",
      "Epoch 17846/30000 Training Loss: 0.0363917201757431\n",
      "Epoch 17847/30000 Training Loss: 0.04527841880917549\n",
      "Epoch 17848/30000 Training Loss: 0.042859021574258804\n",
      "Epoch 17849/30000 Training Loss: 0.041127271950244904\n",
      "Epoch 17850/30000 Training Loss: 0.052016597241163254\n",
      "Epoch 17851/30000 Training Loss: 0.03854231908917427\n",
      "Epoch 17852/30000 Training Loss: 0.04664534330368042\n",
      "Epoch 17853/30000 Training Loss: 0.05296298861503601\n",
      "Epoch 17854/30000 Training Loss: 0.03806626796722412\n",
      "Epoch 17855/30000 Training Loss: 0.03431735560297966\n",
      "Epoch 17856/30000 Training Loss: 0.038488153368234634\n",
      "Epoch 17857/30000 Training Loss: 0.04581223800778389\n",
      "Epoch 17858/30000 Training Loss: 0.04662635549902916\n",
      "Epoch 17859/30000 Training Loss: 0.039741337299346924\n",
      "Epoch 17860/30000 Training Loss: 0.04066018760204315\n",
      "Epoch 17861/30000 Training Loss: 0.04492902383208275\n",
      "Epoch 17862/30000 Training Loss: 0.036509498953819275\n",
      "Epoch 17863/30000 Training Loss: 0.03792833536863327\n",
      "Epoch 17864/30000 Training Loss: 0.04357420280575752\n",
      "Epoch 17865/30000 Training Loss: 0.03576353192329407\n",
      "Epoch 17866/30000 Training Loss: 0.03561405837535858\n",
      "Epoch 17867/30000 Training Loss: 0.048217955976724625\n",
      "Epoch 17868/30000 Training Loss: 0.03771400451660156\n",
      "Epoch 17869/30000 Training Loss: 0.04803473502397537\n",
      "Epoch 17870/30000 Training Loss: 0.04517461359500885\n",
      "Epoch 17871/30000 Training Loss: 0.04356960207223892\n",
      "Epoch 17872/30000 Training Loss: 0.042972370982170105\n",
      "Epoch 17873/30000 Training Loss: 0.056276530027389526\n",
      "Epoch 17874/30000 Training Loss: 0.04398168995976448\n",
      "Epoch 17875/30000 Training Loss: 0.04499945789575577\n",
      "Epoch 17876/30000 Training Loss: 0.03686179965734482\n",
      "Epoch 17877/30000 Training Loss: 0.03361204266548157\n",
      "Epoch 17878/30000 Training Loss: 0.03738574683666229\n",
      "Epoch 17879/30000 Training Loss: 0.05359388887882233\n",
      "Epoch 17880/30000 Training Loss: 0.049997635185718536\n",
      "Epoch 17881/30000 Training Loss: 0.036442842334508896\n",
      "Epoch 17882/30000 Training Loss: 0.05039519816637039\n",
      "Epoch 17883/30000 Training Loss: 0.03660188987851143\n",
      "Epoch 17884/30000 Training Loss: 0.043324992060661316\n",
      "Epoch 17885/30000 Training Loss: 0.041367776691913605\n",
      "Epoch 17886/30000 Training Loss: 0.03550751507282257\n",
      "Epoch 17887/30000 Training Loss: 0.06504569947719574\n",
      "Epoch 17888/30000 Training Loss: 0.047311123460531235\n",
      "Epoch 17889/30000 Training Loss: 0.042509451508522034\n",
      "Epoch 17890/30000 Training Loss: 0.047829315066337585\n",
      "Epoch 17891/30000 Training Loss: 0.043566785752773285\n",
      "Epoch 17892/30000 Training Loss: 0.03558444231748581\n",
      "Epoch 17893/30000 Training Loss: 0.04568307474255562\n",
      "Epoch 17894/30000 Training Loss: 0.053508348762989044\n",
      "Epoch 17895/30000 Training Loss: 0.03722240775823593\n",
      "Epoch 17896/30000 Training Loss: 0.04689352214336395\n",
      "Epoch 17897/30000 Training Loss: 0.03883259743452072\n",
      "Epoch 17898/30000 Training Loss: 0.042312681674957275\n",
      "Epoch 17899/30000 Training Loss: 0.0471782386302948\n",
      "Epoch 17900/30000 Training Loss: 0.052698973566293716\n",
      "Epoch 17900/30000 Validation Loss: 0.03620230033993721\n",
      "Epoch 17901/30000 Training Loss: 0.03559010475873947\n",
      "Epoch 17902/30000 Training Loss: 0.042044028639793396\n",
      "Epoch 17903/30000 Training Loss: 0.04165541008114815\n",
      "Epoch 17904/30000 Training Loss: 0.035332661122083664\n",
      "Epoch 17905/30000 Training Loss: 0.05175821855664253\n",
      "Epoch 17906/30000 Training Loss: 0.04639260098338127\n",
      "Epoch 17907/30000 Training Loss: 0.03626538813114166\n",
      "Epoch 17908/30000 Training Loss: 0.045762479305267334\n",
      "Epoch 17909/30000 Training Loss: 0.046573206782341\n",
      "Epoch 17910/30000 Training Loss: 0.045918919146060944\n",
      "Epoch 17911/30000 Training Loss: 0.04172525554895401\n",
      "Epoch 17912/30000 Training Loss: 0.041669249534606934\n",
      "Epoch 17913/30000 Training Loss: 0.049998894333839417\n",
      "Epoch 17914/30000 Training Loss: 0.04568862169981003\n",
      "Epoch 17915/30000 Training Loss: 0.05138833820819855\n",
      "Epoch 17916/30000 Training Loss: 0.038996752351522446\n",
      "Epoch 17917/30000 Training Loss: 0.04603799432516098\n",
      "Epoch 17918/30000 Training Loss: 0.03471854329109192\n",
      "Epoch 17919/30000 Training Loss: 0.0386909656226635\n",
      "Epoch 17920/30000 Training Loss: 0.04232415929436684\n",
      "Epoch 17921/30000 Training Loss: 0.0316561758518219\n",
      "Epoch 17922/30000 Training Loss: 0.04345519840717316\n",
      "Epoch 17923/30000 Training Loss: 0.04705342277884483\n",
      "Epoch 17924/30000 Training Loss: 0.03454142063856125\n",
      "Epoch 17925/30000 Training Loss: 0.04430388659238815\n",
      "Epoch 17926/30000 Training Loss: 0.044789887964725494\n",
      "Epoch 17927/30000 Training Loss: 0.05852827429771423\n",
      "Epoch 17928/30000 Training Loss: 0.04059843719005585\n",
      "Epoch 17929/30000 Training Loss: 0.04974885284900665\n",
      "Epoch 17930/30000 Training Loss: 0.03892245888710022\n",
      "Epoch 17931/30000 Training Loss: 0.043852195143699646\n",
      "Epoch 17932/30000 Training Loss: 0.06286225467920303\n",
      "Epoch 17933/30000 Training Loss: 0.0514843687415123\n",
      "Epoch 17934/30000 Training Loss: 0.028128046542406082\n",
      "Epoch 17935/30000 Training Loss: 0.04431764781475067\n",
      "Epoch 17936/30000 Training Loss: 0.04020541161298752\n",
      "Epoch 17937/30000 Training Loss: 0.04955420643091202\n",
      "Epoch 17938/30000 Training Loss: 0.031343549489974976\n",
      "Epoch 17939/30000 Training Loss: 0.049440521746873856\n",
      "Epoch 17940/30000 Training Loss: 0.04885789752006531\n",
      "Epoch 17941/30000 Training Loss: 0.05317959189414978\n",
      "Epoch 17942/30000 Training Loss: 0.05275208503007889\n",
      "Epoch 17943/30000 Training Loss: 0.03321918472647667\n",
      "Epoch 17944/30000 Training Loss: 0.042927272617816925\n",
      "Epoch 17945/30000 Training Loss: 0.04605984687805176\n",
      "Epoch 17946/30000 Training Loss: 0.056178584694862366\n",
      "Epoch 17947/30000 Training Loss: 0.0385354645550251\n",
      "Epoch 17948/30000 Training Loss: 0.0346708707511425\n",
      "Epoch 17949/30000 Training Loss: 0.054304853081703186\n",
      "Epoch 17950/30000 Training Loss: 0.034796878695487976\n",
      "Epoch 17951/30000 Training Loss: 0.0563969612121582\n",
      "Epoch 17952/30000 Training Loss: 0.0497749038040638\n",
      "Epoch 17953/30000 Training Loss: 0.03677485138177872\n",
      "Epoch 17954/30000 Training Loss: 0.039443276822566986\n",
      "Epoch 17955/30000 Training Loss: 0.03749598562717438\n",
      "Epoch 17956/30000 Training Loss: 0.04068581014871597\n",
      "Epoch 17957/30000 Training Loss: 0.04222167283296585\n",
      "Epoch 17958/30000 Training Loss: 0.05790017545223236\n",
      "Epoch 17959/30000 Training Loss: 0.05710165947675705\n",
      "Epoch 17960/30000 Training Loss: 0.05284184589982033\n",
      "Epoch 17961/30000 Training Loss: 0.04394805431365967\n",
      "Epoch 17962/30000 Training Loss: 0.03957948833703995\n",
      "Epoch 17963/30000 Training Loss: 0.03575696796178818\n",
      "Epoch 17964/30000 Training Loss: 0.02748117782175541\n",
      "Epoch 17965/30000 Training Loss: 0.037750959396362305\n",
      "Epoch 17966/30000 Training Loss: 0.05535631254315376\n",
      "Epoch 17967/30000 Training Loss: 0.04708816111087799\n",
      "Epoch 17968/30000 Training Loss: 0.03807615488767624\n",
      "Epoch 17969/30000 Training Loss: 0.04856808856129646\n",
      "Epoch 17970/30000 Training Loss: 0.03657214716076851\n",
      "Epoch 17971/30000 Training Loss: 0.04441726207733154\n",
      "Epoch 17972/30000 Training Loss: 0.03771745786070824\n",
      "Epoch 17973/30000 Training Loss: 0.05468755215406418\n",
      "Epoch 17974/30000 Training Loss: 0.04391244798898697\n",
      "Epoch 17975/30000 Training Loss: 0.046660736203193665\n",
      "Epoch 17976/30000 Training Loss: 0.03357769921422005\n",
      "Epoch 17977/30000 Training Loss: 0.040224891155958176\n",
      "Epoch 17978/30000 Training Loss: 0.04791204631328583\n",
      "Epoch 17979/30000 Training Loss: 0.045863598585128784\n",
      "Epoch 17980/30000 Training Loss: 0.04660170525312424\n",
      "Epoch 17981/30000 Training Loss: 0.04298992455005646\n",
      "Epoch 17982/30000 Training Loss: 0.04186403751373291\n",
      "Epoch 17983/30000 Training Loss: 0.035318996757268906\n",
      "Epoch 17984/30000 Training Loss: 0.0608426108956337\n",
      "Epoch 17985/30000 Training Loss: 0.05402560532093048\n",
      "Epoch 17986/30000 Training Loss: 0.04502702131867409\n",
      "Epoch 17987/30000 Training Loss: 0.0341106653213501\n",
      "Epoch 17988/30000 Training Loss: 0.03981154412031174\n",
      "Epoch 17989/30000 Training Loss: 0.0384063795208931\n",
      "Epoch 17990/30000 Training Loss: 0.05111965537071228\n",
      "Epoch 17991/30000 Training Loss: 0.042164526879787445\n",
      "Epoch 17992/30000 Training Loss: 0.03847001492977142\n",
      "Epoch 17993/30000 Training Loss: 0.05218809098005295\n",
      "Epoch 17994/30000 Training Loss: 0.052170298993587494\n",
      "Epoch 17995/30000 Training Loss: 0.039116520434617996\n",
      "Epoch 17996/30000 Training Loss: 0.044320058077573776\n",
      "Epoch 17997/30000 Training Loss: 0.054302990436553955\n",
      "Epoch 17998/30000 Training Loss: 0.04379575699567795\n",
      "Epoch 17999/30000 Training Loss: 0.035058822482824326\n",
      "Epoch 18000/30000 Training Loss: 0.045413851737976074\n",
      "Epoch 18000/30000 Validation Loss: 0.04559963941574097\n",
      "Epoch 18001/30000 Training Loss: 0.053576137870550156\n",
      "Epoch 18002/30000 Training Loss: 0.06277751177549362\n",
      "Epoch 18003/30000 Training Loss: 0.04119133576750755\n",
      "Epoch 18004/30000 Training Loss: 0.03508610278367996\n",
      "Epoch 18005/30000 Training Loss: 0.03449697047472\n",
      "Epoch 18006/30000 Training Loss: 0.04232008382678032\n",
      "Epoch 18007/30000 Training Loss: 0.046034298837184906\n",
      "Epoch 18008/30000 Training Loss: 0.047775886952877045\n",
      "Epoch 18009/30000 Training Loss: 0.04856311157345772\n",
      "Epoch 18010/30000 Training Loss: 0.04079670459032059\n",
      "Epoch 18011/30000 Training Loss: 0.05474225804209709\n",
      "Epoch 18012/30000 Training Loss: 0.04741426557302475\n",
      "Epoch 18013/30000 Training Loss: 0.04366667568683624\n",
      "Epoch 18014/30000 Training Loss: 0.03787367045879364\n",
      "Epoch 18015/30000 Training Loss: 0.052948348224163055\n",
      "Epoch 18016/30000 Training Loss: 0.05061386525630951\n",
      "Epoch 18017/30000 Training Loss: 0.04479357600212097\n",
      "Epoch 18018/30000 Training Loss: 0.0466451533138752\n",
      "Epoch 18019/30000 Training Loss: 0.0433739498257637\n",
      "Epoch 18020/30000 Training Loss: 0.05516964569687843\n",
      "Epoch 18021/30000 Training Loss: 0.05087626725435257\n",
      "Epoch 18022/30000 Training Loss: 0.05218983069062233\n",
      "Epoch 18023/30000 Training Loss: 0.04156235605478287\n",
      "Epoch 18024/30000 Training Loss: 0.03367966040968895\n",
      "Epoch 18025/30000 Training Loss: 0.0603504553437233\n",
      "Epoch 18026/30000 Training Loss: 0.04498499631881714\n",
      "Epoch 18027/30000 Training Loss: 0.047004252672195435\n",
      "Epoch 18028/30000 Training Loss: 0.04560624063014984\n",
      "Epoch 18029/30000 Training Loss: 0.061536528170108795\n",
      "Epoch 18030/30000 Training Loss: 0.04444403573870659\n",
      "Epoch 18031/30000 Training Loss: 0.04607958719134331\n",
      "Epoch 18032/30000 Training Loss: 0.04935334622859955\n",
      "Epoch 18033/30000 Training Loss: 0.04262733459472656\n",
      "Epoch 18034/30000 Training Loss: 0.040725886821746826\n",
      "Epoch 18035/30000 Training Loss: 0.04860597848892212\n",
      "Epoch 18036/30000 Training Loss: 0.046393491327762604\n",
      "Epoch 18037/30000 Training Loss: 0.04971502721309662\n",
      "Epoch 18038/30000 Training Loss: 0.051386915147304535\n",
      "Epoch 18039/30000 Training Loss: 0.043626319617033005\n",
      "Epoch 18040/30000 Training Loss: 0.061331361532211304\n",
      "Epoch 18041/30000 Training Loss: 0.05136217176914215\n",
      "Epoch 18042/30000 Training Loss: 0.0428943857550621\n",
      "Epoch 18043/30000 Training Loss: 0.05146094411611557\n",
      "Epoch 18044/30000 Training Loss: 0.04716362804174423\n",
      "Epoch 18045/30000 Training Loss: 0.06081073731184006\n",
      "Epoch 18046/30000 Training Loss: 0.04048585519194603\n",
      "Epoch 18047/30000 Training Loss: 0.0478968620300293\n",
      "Epoch 18048/30000 Training Loss: 0.052619852125644684\n",
      "Epoch 18049/30000 Training Loss: 0.04111489653587341\n",
      "Epoch 18050/30000 Training Loss: 0.0392613522708416\n",
      "Epoch 18051/30000 Training Loss: 0.040122680366039276\n",
      "Epoch 18052/30000 Training Loss: 0.05277331918478012\n",
      "Epoch 18053/30000 Training Loss: 0.049295973032712936\n",
      "Epoch 18054/30000 Training Loss: 0.05386728793382645\n",
      "Epoch 18055/30000 Training Loss: 0.032578978687524796\n",
      "Epoch 18056/30000 Training Loss: 0.053663235157728195\n",
      "Epoch 18057/30000 Training Loss: 0.042800746858119965\n",
      "Epoch 18058/30000 Training Loss: 0.044828712940216064\n",
      "Epoch 18059/30000 Training Loss: 0.045291975140571594\n",
      "Epoch 18060/30000 Training Loss: 0.04750598222017288\n",
      "Epoch 18061/30000 Training Loss: 0.04847227409482002\n",
      "Epoch 18062/30000 Training Loss: 0.043570175766944885\n",
      "Epoch 18063/30000 Training Loss: 0.03641834110021591\n",
      "Epoch 18064/30000 Training Loss: 0.05636683106422424\n",
      "Epoch 18065/30000 Training Loss: 0.05395812541246414\n",
      "Epoch 18066/30000 Training Loss: 0.03440013527870178\n",
      "Epoch 18067/30000 Training Loss: 0.04500124603509903\n",
      "Epoch 18068/30000 Training Loss: 0.05011305212974548\n",
      "Epoch 18069/30000 Training Loss: 0.0488865002989769\n",
      "Epoch 18070/30000 Training Loss: 0.03660377115011215\n",
      "Epoch 18071/30000 Training Loss: 0.04750736057758331\n",
      "Epoch 18072/30000 Training Loss: 0.04864896088838577\n",
      "Epoch 18073/30000 Training Loss: 0.04701387882232666\n",
      "Epoch 18074/30000 Training Loss: 0.04451307654380798\n",
      "Epoch 18075/30000 Training Loss: 0.05256346985697746\n",
      "Epoch 18076/30000 Training Loss: 0.04471241682767868\n",
      "Epoch 18077/30000 Training Loss: 0.05357961729168892\n",
      "Epoch 18078/30000 Training Loss: 0.04114264249801636\n",
      "Epoch 18079/30000 Training Loss: 0.04436240345239639\n",
      "Epoch 18080/30000 Training Loss: 0.040941379964351654\n",
      "Epoch 18081/30000 Training Loss: 0.042642854154109955\n",
      "Epoch 18082/30000 Training Loss: 0.046295128762722015\n",
      "Epoch 18083/30000 Training Loss: 0.040241140872240067\n",
      "Epoch 18084/30000 Training Loss: 0.051715850830078125\n",
      "Epoch 18085/30000 Training Loss: 0.05185281112790108\n",
      "Epoch 18086/30000 Training Loss: 0.05333849787712097\n",
      "Epoch 18087/30000 Training Loss: 0.05375054478645325\n",
      "Epoch 18088/30000 Training Loss: 0.05437043309211731\n",
      "Epoch 18089/30000 Training Loss: 0.03321650251746178\n",
      "Epoch 18090/30000 Training Loss: 0.05496455729007721\n",
      "Epoch 18091/30000 Training Loss: 0.04681636765599251\n",
      "Epoch 18092/30000 Training Loss: 0.062330909073352814\n",
      "Epoch 18093/30000 Training Loss: 0.03967553377151489\n",
      "Epoch 18094/30000 Training Loss: 0.042117029428482056\n",
      "Epoch 18095/30000 Training Loss: 0.04378977417945862\n",
      "Epoch 18096/30000 Training Loss: 0.043279290199279785\n",
      "Epoch 18097/30000 Training Loss: 0.04269213229417801\n",
      "Epoch 18098/30000 Training Loss: 0.044091541320085526\n",
      "Epoch 18099/30000 Training Loss: 0.05554632470011711\n",
      "Epoch 18100/30000 Training Loss: 0.05055731534957886\n",
      "Epoch 18100/30000 Validation Loss: 0.04512057080864906\n",
      "Epoch 18101/30000 Training Loss: 0.04654956981539726\n",
      "Epoch 18102/30000 Training Loss: 0.051801398396492004\n",
      "Epoch 18103/30000 Training Loss: 0.06314390152692795\n",
      "Epoch 18104/30000 Training Loss: 0.0521547868847847\n",
      "Epoch 18105/30000 Training Loss: 0.05395421385765076\n",
      "Epoch 18106/30000 Training Loss: 0.03470660001039505\n",
      "Epoch 18107/30000 Training Loss: 0.052560947835445404\n",
      "Epoch 18108/30000 Training Loss: 0.037283845245838165\n",
      "Epoch 18109/30000 Training Loss: 0.045550618320703506\n",
      "Epoch 18110/30000 Training Loss: 0.05472385883331299\n",
      "Epoch 18111/30000 Training Loss: 0.046975813806056976\n",
      "Epoch 18112/30000 Training Loss: 0.038888052105903625\n",
      "Epoch 18113/30000 Training Loss: 0.06516195833683014\n",
      "Epoch 18114/30000 Training Loss: 0.03530946001410484\n",
      "Epoch 18115/30000 Training Loss: 0.04156547784805298\n",
      "Epoch 18116/30000 Training Loss: 0.0373646542429924\n",
      "Epoch 18117/30000 Training Loss: 0.06248793005943298\n",
      "Epoch 18118/30000 Training Loss: 0.03511422500014305\n",
      "Epoch 18119/30000 Training Loss: 0.047517381608486176\n",
      "Epoch 18120/30000 Training Loss: 0.044366903603076935\n",
      "Epoch 18121/30000 Training Loss: 0.04307921975851059\n",
      "Epoch 18122/30000 Training Loss: 0.057136170566082\n",
      "Epoch 18123/30000 Training Loss: 0.05976169556379318\n",
      "Epoch 18124/30000 Training Loss: 0.03200284019112587\n",
      "Epoch 18125/30000 Training Loss: 0.04605952650308609\n",
      "Epoch 18126/30000 Training Loss: 0.05448978394269943\n",
      "Epoch 18127/30000 Training Loss: 0.03754344582557678\n",
      "Epoch 18128/30000 Training Loss: 0.06486259400844574\n",
      "Epoch 18129/30000 Training Loss: 0.04145905002951622\n",
      "Epoch 18130/30000 Training Loss: 0.05214880779385567\n",
      "Epoch 18131/30000 Training Loss: 0.05017119646072388\n",
      "Epoch 18132/30000 Training Loss: 0.033708617091178894\n",
      "Epoch 18133/30000 Training Loss: 0.04877763241529465\n",
      "Epoch 18134/30000 Training Loss: 0.04103560000658035\n",
      "Epoch 18135/30000 Training Loss: 0.05186203122138977\n",
      "Epoch 18136/30000 Training Loss: 0.040323227643966675\n",
      "Epoch 18137/30000 Training Loss: 0.04820197820663452\n",
      "Epoch 18138/30000 Training Loss: 0.043718088418245316\n",
      "Epoch 18139/30000 Training Loss: 0.06455384939908981\n",
      "Epoch 18140/30000 Training Loss: 0.04962743818759918\n",
      "Epoch 18141/30000 Training Loss: 0.04589046910405159\n",
      "Epoch 18142/30000 Training Loss: 0.04860163480043411\n",
      "Epoch 18143/30000 Training Loss: 0.037306904792785645\n",
      "Epoch 18144/30000 Training Loss: 0.05021657422184944\n",
      "Epoch 18145/30000 Training Loss: 0.0371616929769516\n",
      "Epoch 18146/30000 Training Loss: 0.054386816918849945\n",
      "Epoch 18147/30000 Training Loss: 0.05273164436221123\n",
      "Epoch 18148/30000 Training Loss: 0.04576001316308975\n",
      "Epoch 18149/30000 Training Loss: 0.032631177455186844\n",
      "Epoch 18150/30000 Training Loss: 0.044713690876960754\n",
      "Epoch 18151/30000 Training Loss: 0.047267794609069824\n",
      "Epoch 18152/30000 Training Loss: 0.0424412339925766\n",
      "Epoch 18153/30000 Training Loss: 0.03985629230737686\n",
      "Epoch 18154/30000 Training Loss: 0.04365377128124237\n",
      "Epoch 18155/30000 Training Loss: 0.04374757781624794\n",
      "Epoch 18156/30000 Training Loss: 0.0660208985209465\n",
      "Epoch 18157/30000 Training Loss: 0.055764809250831604\n",
      "Epoch 18158/30000 Training Loss: 0.04693421721458435\n",
      "Epoch 18159/30000 Training Loss: 0.034716054797172546\n",
      "Epoch 18160/30000 Training Loss: 0.04432085156440735\n",
      "Epoch 18161/30000 Training Loss: 0.03710208088159561\n",
      "Epoch 18162/30000 Training Loss: 0.0491914227604866\n",
      "Epoch 18163/30000 Training Loss: 0.044157177209854126\n",
      "Epoch 18164/30000 Training Loss: 0.044764745980501175\n",
      "Epoch 18165/30000 Training Loss: 0.03778872266411781\n",
      "Epoch 18166/30000 Training Loss: 0.051943399012088776\n",
      "Epoch 18167/30000 Training Loss: 0.035233110189437866\n",
      "Epoch 18168/30000 Training Loss: 0.04579084366559982\n",
      "Epoch 18169/30000 Training Loss: 0.05511424317955971\n",
      "Epoch 18170/30000 Training Loss: 0.04233803600072861\n",
      "Epoch 18171/30000 Training Loss: 0.04024389013648033\n",
      "Epoch 18172/30000 Training Loss: 0.038221195340156555\n",
      "Epoch 18173/30000 Training Loss: 0.045890554785728455\n",
      "Epoch 18174/30000 Training Loss: 0.05016953870654106\n",
      "Epoch 18175/30000 Training Loss: 0.04723834991455078\n",
      "Epoch 18176/30000 Training Loss: 0.032266825437545776\n",
      "Epoch 18177/30000 Training Loss: 0.060368627309799194\n",
      "Epoch 18178/30000 Training Loss: 0.053106218576431274\n",
      "Epoch 18179/30000 Training Loss: 0.04175819829106331\n",
      "Epoch 18180/30000 Training Loss: 0.05547267198562622\n",
      "Epoch 18181/30000 Training Loss: 0.05843579024076462\n",
      "Epoch 18182/30000 Training Loss: 0.0403335765004158\n",
      "Epoch 18183/30000 Training Loss: 0.038943540304899216\n",
      "Epoch 18184/30000 Training Loss: 0.0432610958814621\n",
      "Epoch 18185/30000 Training Loss: 0.03353408724069595\n",
      "Epoch 18186/30000 Training Loss: 0.03538012504577637\n",
      "Epoch 18187/30000 Training Loss: 0.04336436465382576\n",
      "Epoch 18188/30000 Training Loss: 0.05394212156534195\n",
      "Epoch 18189/30000 Training Loss: 0.049879223108291626\n",
      "Epoch 18190/30000 Training Loss: 0.052184849977493286\n",
      "Epoch 18191/30000 Training Loss: 0.07161372154951096\n",
      "Epoch 18192/30000 Training Loss: 0.04728815704584122\n",
      "Epoch 18193/30000 Training Loss: 0.05383699759840965\n",
      "Epoch 18194/30000 Training Loss: 0.04851268231868744\n",
      "Epoch 18195/30000 Training Loss: 0.043468937277793884\n",
      "Epoch 18196/30000 Training Loss: 0.043112486600875854\n",
      "Epoch 18197/30000 Training Loss: 0.033987633883953094\n",
      "Epoch 18198/30000 Training Loss: 0.052456095814704895\n",
      "Epoch 18199/30000 Training Loss: 0.040062837302684784\n",
      "Epoch 18200/30000 Training Loss: 0.033629417419433594\n",
      "Epoch 18200/30000 Validation Loss: 0.03781973943114281\n",
      "Epoch 18201/30000 Training Loss: 0.04396369308233261\n",
      "Epoch 18202/30000 Training Loss: 0.042326103895902634\n",
      "Epoch 18203/30000 Training Loss: 0.049056097865104675\n",
      "Epoch 18204/30000 Training Loss: 0.04085559397935867\n",
      "Epoch 18205/30000 Training Loss: 0.0367385670542717\n",
      "Epoch 18206/30000 Training Loss: 0.045305997133255005\n",
      "Epoch 18207/30000 Training Loss: 0.039312757551670074\n",
      "Epoch 18208/30000 Training Loss: 0.040326688438653946\n",
      "Epoch 18209/30000 Training Loss: 0.03254982456564903\n",
      "Epoch 18210/30000 Training Loss: 0.06077918782830238\n",
      "Epoch 18211/30000 Training Loss: 0.046179503202438354\n",
      "Epoch 18212/30000 Training Loss: 0.04410896450281143\n",
      "Epoch 18213/30000 Training Loss: 0.03548698499798775\n",
      "Epoch 18214/30000 Training Loss: 0.03867153078317642\n",
      "Epoch 18215/30000 Training Loss: 0.04746832698583603\n",
      "Epoch 18216/30000 Training Loss: 0.04579021409153938\n",
      "Epoch 18217/30000 Training Loss: 0.039000265300273895\n",
      "Epoch 18218/30000 Training Loss: 0.038958098739385605\n",
      "Epoch 18219/30000 Training Loss: 0.044771067798137665\n",
      "Epoch 18220/30000 Training Loss: 0.044166453182697296\n",
      "Epoch 18221/30000 Training Loss: 0.05297939479351044\n",
      "Epoch 18222/30000 Training Loss: 0.04704777151346207\n",
      "Epoch 18223/30000 Training Loss: 0.04379061982035637\n",
      "Epoch 18224/30000 Training Loss: 0.03893492743372917\n",
      "Epoch 18225/30000 Training Loss: 0.04624965414404869\n",
      "Epoch 18226/30000 Training Loss: 0.04851503670215607\n",
      "Epoch 18227/30000 Training Loss: 0.03761068731546402\n",
      "Epoch 18228/30000 Training Loss: 0.04661102592945099\n",
      "Epoch 18229/30000 Training Loss: 0.05186629295349121\n",
      "Epoch 18230/30000 Training Loss: 0.04701339453458786\n",
      "Epoch 18231/30000 Training Loss: 0.044755034148693085\n",
      "Epoch 18232/30000 Training Loss: 0.05138406157493591\n",
      "Epoch 18233/30000 Training Loss: 0.04679066687822342\n",
      "Epoch 18234/30000 Training Loss: 0.038461826741695404\n",
      "Epoch 18235/30000 Training Loss: 0.04073234647512436\n",
      "Epoch 18236/30000 Training Loss: 0.05089104548096657\n",
      "Epoch 18237/30000 Training Loss: 0.03771970421075821\n",
      "Epoch 18238/30000 Training Loss: 0.031979069113731384\n",
      "Epoch 18239/30000 Training Loss: 0.039099663496017456\n",
      "Epoch 18240/30000 Training Loss: 0.05834877863526344\n",
      "Epoch 18241/30000 Training Loss: 0.056472763419151306\n",
      "Epoch 18242/30000 Training Loss: 0.03759792447090149\n",
      "Epoch 18243/30000 Training Loss: 0.06189197301864624\n",
      "Epoch 18244/30000 Training Loss: 0.05668644607067108\n",
      "Epoch 18245/30000 Training Loss: 0.04325559735298157\n",
      "Epoch 18246/30000 Training Loss: 0.06894752383232117\n",
      "Epoch 18247/30000 Training Loss: 0.033549871295690536\n",
      "Epoch 18248/30000 Training Loss: 0.05534619465470314\n",
      "Epoch 18249/30000 Training Loss: 0.05466265231370926\n",
      "Epoch 18250/30000 Training Loss: 0.047092802822589874\n",
      "Epoch 18251/30000 Training Loss: 0.05597429350018501\n",
      "Epoch 18252/30000 Training Loss: 0.04172547906637192\n",
      "Epoch 18253/30000 Training Loss: 0.0605623796582222\n",
      "Epoch 18254/30000 Training Loss: 0.03406260907649994\n",
      "Epoch 18255/30000 Training Loss: 0.05440482497215271\n",
      "Epoch 18256/30000 Training Loss: 0.03664778172969818\n",
      "Epoch 18257/30000 Training Loss: 0.03593742847442627\n",
      "Epoch 18258/30000 Training Loss: 0.04596923291683197\n",
      "Epoch 18259/30000 Training Loss: 0.04135097563266754\n",
      "Epoch 18260/30000 Training Loss: 0.04588397592306137\n",
      "Epoch 18261/30000 Training Loss: 0.03752831369638443\n",
      "Epoch 18262/30000 Training Loss: 0.04318995773792267\n",
      "Epoch 18263/30000 Training Loss: 0.033898092806339264\n",
      "Epoch 18264/30000 Training Loss: 0.0445760115981102\n",
      "Epoch 18265/30000 Training Loss: 0.031258441507816315\n",
      "Epoch 18266/30000 Training Loss: 0.037923697382211685\n",
      "Epoch 18267/30000 Training Loss: 0.03723348677158356\n",
      "Epoch 18268/30000 Training Loss: 0.05597010254859924\n",
      "Epoch 18269/30000 Training Loss: 0.040401116013526917\n",
      "Epoch 18270/30000 Training Loss: 0.048436686396598816\n",
      "Epoch 18271/30000 Training Loss: 0.03488858789205551\n",
      "Epoch 18272/30000 Training Loss: 0.04088299721479416\n",
      "Epoch 18273/30000 Training Loss: 0.03633087873458862\n",
      "Epoch 18274/30000 Training Loss: 0.03807378560304642\n",
      "Epoch 18275/30000 Training Loss: 0.04850371927022934\n",
      "Epoch 18276/30000 Training Loss: 0.04183891415596008\n",
      "Epoch 18277/30000 Training Loss: 0.040553249418735504\n",
      "Epoch 18278/30000 Training Loss: 0.049918290227651596\n",
      "Epoch 18279/30000 Training Loss: 0.04454292729496956\n",
      "Epoch 18280/30000 Training Loss: 0.058987412601709366\n",
      "Epoch 18281/30000 Training Loss: 0.03673866391181946\n",
      "Epoch 18282/30000 Training Loss: 0.03954470902681351\n",
      "Epoch 18283/30000 Training Loss: 0.04439632594585419\n",
      "Epoch 18284/30000 Training Loss: 0.045372430235147476\n",
      "Epoch 18285/30000 Training Loss: 0.03388725593686104\n",
      "Epoch 18286/30000 Training Loss: 0.03577350825071335\n",
      "Epoch 18287/30000 Training Loss: 0.05696412920951843\n",
      "Epoch 18288/30000 Training Loss: 0.035744283348321915\n",
      "Epoch 18289/30000 Training Loss: 0.0439532995223999\n",
      "Epoch 18290/30000 Training Loss: 0.03784889727830887\n",
      "Epoch 18291/30000 Training Loss: 0.04341641068458557\n",
      "Epoch 18292/30000 Training Loss: 0.05996725335717201\n",
      "Epoch 18293/30000 Training Loss: 0.035999298095703125\n",
      "Epoch 18294/30000 Training Loss: 0.04024809971451759\n",
      "Epoch 18295/30000 Training Loss: 0.030700121074914932\n",
      "Epoch 18296/30000 Training Loss: 0.04583386331796646\n",
      "Epoch 18297/30000 Training Loss: 0.039438724517822266\n",
      "Epoch 18298/30000 Training Loss: 0.049803562462329865\n",
      "Epoch 18299/30000 Training Loss: 0.04024363309144974\n",
      "Epoch 18300/30000 Training Loss: 0.04220385104417801\n",
      "Epoch 18300/30000 Validation Loss: 0.048338234424591064\n",
      "Epoch 18301/30000 Training Loss: 0.0369938425719738\n",
      "Epoch 18302/30000 Training Loss: 0.059051886200904846\n",
      "Epoch 18303/30000 Training Loss: 0.02856825478374958\n",
      "Epoch 18304/30000 Training Loss: 0.048153795301914215\n",
      "Epoch 18305/30000 Training Loss: 0.049438394606113434\n",
      "Epoch 18306/30000 Training Loss: 0.04660631716251373\n",
      "Epoch 18307/30000 Training Loss: 0.03436313197016716\n",
      "Epoch 18308/30000 Training Loss: 0.04520656540989876\n",
      "Epoch 18309/30000 Training Loss: 0.05289783328771591\n",
      "Epoch 18310/30000 Training Loss: 0.048191238194704056\n",
      "Epoch 18311/30000 Training Loss: 0.0487753301858902\n",
      "Epoch 18312/30000 Training Loss: 0.046138696372509\n",
      "Epoch 18313/30000 Training Loss: 0.042759936302900314\n",
      "Epoch 18314/30000 Training Loss: 0.06890171021223068\n",
      "Epoch 18315/30000 Training Loss: 0.033839475363492966\n",
      "Epoch 18316/30000 Training Loss: 0.043159425258636475\n",
      "Epoch 18317/30000 Training Loss: 0.055213965475559235\n",
      "Epoch 18318/30000 Training Loss: 0.04189605265855789\n",
      "Epoch 18319/30000 Training Loss: 0.037003885954618454\n",
      "Epoch 18320/30000 Training Loss: 0.042986057698726654\n",
      "Epoch 18321/30000 Training Loss: 0.05411412566900253\n",
      "Epoch 18322/30000 Training Loss: 0.04415362328290939\n",
      "Epoch 18323/30000 Training Loss: 0.040733322501182556\n",
      "Epoch 18324/30000 Training Loss: 0.04525327309966087\n",
      "Epoch 18325/30000 Training Loss: 0.04211326688528061\n",
      "Epoch 18326/30000 Training Loss: 0.037000030279159546\n",
      "Epoch 18327/30000 Training Loss: 0.05475381761789322\n",
      "Epoch 18328/30000 Training Loss: 0.049339015036821365\n",
      "Epoch 18329/30000 Training Loss: 0.051170606166124344\n",
      "Epoch 18330/30000 Training Loss: 0.043892890214920044\n",
      "Epoch 18331/30000 Training Loss: 0.049598146229982376\n",
      "Epoch 18332/30000 Training Loss: 0.05932377278804779\n",
      "Epoch 18333/30000 Training Loss: 0.0562809482216835\n",
      "Epoch 18334/30000 Training Loss: 0.03491200506687164\n",
      "Epoch 18335/30000 Training Loss: 0.059404946863651276\n",
      "Epoch 18336/30000 Training Loss: 0.044826336205005646\n",
      "Epoch 18337/30000 Training Loss: 0.039079926908016205\n",
      "Epoch 18338/30000 Training Loss: 0.05463922768831253\n",
      "Epoch 18339/30000 Training Loss: 0.03670743852853775\n",
      "Epoch 18340/30000 Training Loss: 0.044471316039562225\n",
      "Epoch 18341/30000 Training Loss: 0.054625049233436584\n",
      "Epoch 18342/30000 Training Loss: 0.07263592630624771\n",
      "Epoch 18343/30000 Training Loss: 0.04098096489906311\n",
      "Epoch 18344/30000 Training Loss: 0.02598317712545395\n",
      "Epoch 18345/30000 Training Loss: 0.03454959765076637\n",
      "Epoch 18346/30000 Training Loss: 0.0396592952311039\n",
      "Epoch 18347/30000 Training Loss: 0.061962585896253586\n",
      "Epoch 18348/30000 Training Loss: 0.038432106375694275\n",
      "Epoch 18349/30000 Training Loss: 0.04932504892349243\n",
      "Epoch 18350/30000 Training Loss: 0.03905067592859268\n",
      "Epoch 18351/30000 Training Loss: 0.04224155470728874\n",
      "Epoch 18352/30000 Training Loss: 0.048903897404670715\n",
      "Epoch 18353/30000 Training Loss: 0.047806575894355774\n",
      "Epoch 18354/30000 Training Loss: 0.0510563962161541\n",
      "Epoch 18355/30000 Training Loss: 0.06159905344247818\n",
      "Epoch 18356/30000 Training Loss: 0.03978589177131653\n",
      "Epoch 18357/30000 Training Loss: 0.05763644725084305\n",
      "Epoch 18358/30000 Training Loss: 0.05328075587749481\n",
      "Epoch 18359/30000 Training Loss: 0.028330761939287186\n",
      "Epoch 18360/30000 Training Loss: 0.057973720133304596\n",
      "Epoch 18361/30000 Training Loss: 0.050744690001010895\n",
      "Epoch 18362/30000 Training Loss: 0.043293148279190063\n",
      "Epoch 18363/30000 Training Loss: 0.03639747202396393\n",
      "Epoch 18364/30000 Training Loss: 0.041333142668008804\n",
      "Epoch 18365/30000 Training Loss: 0.038064971566200256\n",
      "Epoch 18366/30000 Training Loss: 0.049669332802295685\n",
      "Epoch 18367/30000 Training Loss: 0.039268601685762405\n",
      "Epoch 18368/30000 Training Loss: 0.040396589785814285\n",
      "Epoch 18369/30000 Training Loss: 0.047126468271017075\n",
      "Epoch 18370/30000 Training Loss: 0.04826059937477112\n",
      "Epoch 18371/30000 Training Loss: 0.03769379109144211\n",
      "Epoch 18372/30000 Training Loss: 0.03891380876302719\n",
      "Epoch 18373/30000 Training Loss: 0.05596286058425903\n",
      "Epoch 18374/30000 Training Loss: 0.0475597009062767\n",
      "Epoch 18375/30000 Training Loss: 0.04293055087327957\n",
      "Epoch 18376/30000 Training Loss: 0.03765353560447693\n",
      "Epoch 18377/30000 Training Loss: 0.03686670586466789\n",
      "Epoch 18378/30000 Training Loss: 0.04106949269771576\n",
      "Epoch 18379/30000 Training Loss: 0.04957974702119827\n",
      "Epoch 18380/30000 Training Loss: 0.04455878585577011\n",
      "Epoch 18381/30000 Training Loss: 0.049067866057157516\n",
      "Epoch 18382/30000 Training Loss: 0.04269947484135628\n",
      "Epoch 18383/30000 Training Loss: 0.03285301849246025\n",
      "Epoch 18384/30000 Training Loss: 0.043441981077194214\n",
      "Epoch 18385/30000 Training Loss: 0.056032195687294006\n",
      "Epoch 18386/30000 Training Loss: 0.04378388077020645\n",
      "Epoch 18387/30000 Training Loss: 0.04559075087308884\n",
      "Epoch 18388/30000 Training Loss: 0.04374373331665993\n",
      "Epoch 18389/30000 Training Loss: 0.0561792217195034\n",
      "Epoch 18390/30000 Training Loss: 0.03847435116767883\n",
      "Epoch 18391/30000 Training Loss: 0.047500092536211014\n",
      "Epoch 18392/30000 Training Loss: 0.04387858510017395\n",
      "Epoch 18393/30000 Training Loss: 0.055609285831451416\n",
      "Epoch 18394/30000 Training Loss: 0.04865541681647301\n",
      "Epoch 18395/30000 Training Loss: 0.03468757122755051\n",
      "Epoch 18396/30000 Training Loss: 0.04567835107445717\n",
      "Epoch 18397/30000 Training Loss: 0.05286184698343277\n",
      "Epoch 18398/30000 Training Loss: 0.033375758677721024\n",
      "Epoch 18399/30000 Training Loss: 0.05002608895301819\n",
      "Epoch 18400/30000 Training Loss: 0.03592783212661743\n",
      "Epoch 18400/30000 Validation Loss: 0.038241297006607056\n",
      "Epoch 18401/30000 Training Loss: 0.045317403972148895\n",
      "Epoch 18402/30000 Training Loss: 0.03430604934692383\n",
      "Epoch 18403/30000 Training Loss: 0.0362701490521431\n",
      "Epoch 18404/30000 Training Loss: 0.04020599275827408\n",
      "Epoch 18405/30000 Training Loss: 0.033398281782865524\n",
      "Epoch 18406/30000 Training Loss: 0.038699496537446976\n",
      "Epoch 18407/30000 Training Loss: 0.053930506110191345\n",
      "Epoch 18408/30000 Training Loss: 0.03527039662003517\n",
      "Epoch 18409/30000 Training Loss: 0.04332344979047775\n",
      "Epoch 18410/30000 Training Loss: 0.05231070518493652\n",
      "Epoch 18411/30000 Training Loss: 0.046147361397743225\n",
      "Epoch 18412/30000 Training Loss: 0.04241661727428436\n",
      "Epoch 18413/30000 Training Loss: 0.05065181106328964\n",
      "Epoch 18414/30000 Training Loss: 0.040383219718933105\n",
      "Epoch 18415/30000 Training Loss: 0.04439235106110573\n",
      "Epoch 18416/30000 Training Loss: 0.04451346397399902\n",
      "Epoch 18417/30000 Training Loss: 0.03473708778619766\n",
      "Epoch 18418/30000 Training Loss: 0.047173142433166504\n",
      "Epoch 18419/30000 Training Loss: 0.0429564043879509\n",
      "Epoch 18420/30000 Training Loss: 0.035861313343048096\n",
      "Epoch 18421/30000 Training Loss: 0.05591360852122307\n",
      "Epoch 18422/30000 Training Loss: 0.04809940606355667\n",
      "Epoch 18423/30000 Training Loss: 0.035715505480766296\n",
      "Epoch 18424/30000 Training Loss: 0.0411069318652153\n",
      "Epoch 18425/30000 Training Loss: 0.04749376326799393\n",
      "Epoch 18426/30000 Training Loss: 0.050170138478279114\n",
      "Epoch 18427/30000 Training Loss: 0.03408990055322647\n",
      "Epoch 18428/30000 Training Loss: 0.047012463212013245\n",
      "Epoch 18429/30000 Training Loss: 0.056204237043857574\n",
      "Epoch 18430/30000 Training Loss: 0.04238871857523918\n",
      "Epoch 18431/30000 Training Loss: 0.04835791140794754\n",
      "Epoch 18432/30000 Training Loss: 0.031708914786577225\n",
      "Epoch 18433/30000 Training Loss: 0.036839693784713745\n",
      "Epoch 18434/30000 Training Loss: 0.057902369648218155\n",
      "Epoch 18435/30000 Training Loss: 0.04092096909880638\n",
      "Epoch 18436/30000 Training Loss: 0.04084381088614464\n",
      "Epoch 18437/30000 Training Loss: 0.05907445028424263\n",
      "Epoch 18438/30000 Training Loss: 0.04776404798030853\n",
      "Epoch 18439/30000 Training Loss: 0.06834931671619415\n",
      "Epoch 18440/30000 Training Loss: 0.04045415669679642\n",
      "Epoch 18441/30000 Training Loss: 0.04679735004901886\n",
      "Epoch 18442/30000 Training Loss: 0.052371151745319366\n",
      "Epoch 18443/30000 Training Loss: 0.046384938061237335\n",
      "Epoch 18444/30000 Training Loss: 0.04575645923614502\n",
      "Epoch 18445/30000 Training Loss: 0.04054858535528183\n",
      "Epoch 18446/30000 Training Loss: 0.0508304238319397\n",
      "Epoch 18447/30000 Training Loss: 0.052109718322753906\n",
      "Epoch 18448/30000 Training Loss: 0.03275144100189209\n",
      "Epoch 18449/30000 Training Loss: 0.05050310865044594\n",
      "Epoch 18450/30000 Training Loss: 0.035662561655044556\n",
      "Epoch 18451/30000 Training Loss: 0.040041837841272354\n",
      "Epoch 18452/30000 Training Loss: 0.04350295290350914\n",
      "Epoch 18453/30000 Training Loss: 0.041098445653915405\n",
      "Epoch 18454/30000 Training Loss: 0.050952911376953125\n",
      "Epoch 18455/30000 Training Loss: 0.034329257905483246\n",
      "Epoch 18456/30000 Training Loss: 0.04370639845728874\n",
      "Epoch 18457/30000 Training Loss: 0.04335636645555496\n",
      "Epoch 18458/30000 Training Loss: 0.05165361613035202\n",
      "Epoch 18459/30000 Training Loss: 0.05608740448951721\n",
      "Epoch 18460/30000 Training Loss: 0.03671155869960785\n",
      "Epoch 18461/30000 Training Loss: 0.04701543226838112\n",
      "Epoch 18462/30000 Training Loss: 0.03718801587820053\n",
      "Epoch 18463/30000 Training Loss: 0.03728574514389038\n",
      "Epoch 18464/30000 Training Loss: 0.03201008588075638\n",
      "Epoch 18465/30000 Training Loss: 0.0405888631939888\n",
      "Epoch 18466/30000 Training Loss: 0.03152044862508774\n",
      "Epoch 18467/30000 Training Loss: 0.041749581694602966\n",
      "Epoch 18468/30000 Training Loss: 0.051702387630939484\n",
      "Epoch 18469/30000 Training Loss: 0.043197400867938995\n",
      "Epoch 18470/30000 Training Loss: 0.04811883717775345\n",
      "Epoch 18471/30000 Training Loss: 0.04730817675590515\n",
      "Epoch 18472/30000 Training Loss: 0.037225574254989624\n",
      "Epoch 18473/30000 Training Loss: 0.058877021074295044\n",
      "Epoch 18474/30000 Training Loss: 0.048755817115306854\n",
      "Epoch 18475/30000 Training Loss: 0.057461485266685486\n",
      "Epoch 18476/30000 Training Loss: 0.0582704097032547\n",
      "Epoch 18477/30000 Training Loss: 0.05004940181970596\n",
      "Epoch 18478/30000 Training Loss: 0.04381541907787323\n",
      "Epoch 18479/30000 Training Loss: 0.03859642148017883\n",
      "Epoch 18480/30000 Training Loss: 0.03694349527359009\n",
      "Epoch 18481/30000 Training Loss: 0.0386774055659771\n",
      "Epoch 18482/30000 Training Loss: 0.05378209799528122\n",
      "Epoch 18483/30000 Training Loss: 0.04996968060731888\n",
      "Epoch 18484/30000 Training Loss: 0.036490298807621\n",
      "Epoch 18485/30000 Training Loss: 0.030483927577733994\n",
      "Epoch 18486/30000 Training Loss: 0.04625024273991585\n",
      "Epoch 18487/30000 Training Loss: 0.04673812538385391\n",
      "Epoch 18488/30000 Training Loss: 0.06888094544410706\n",
      "Epoch 18489/30000 Training Loss: 0.0501960888504982\n",
      "Epoch 18490/30000 Training Loss: 0.03918766230344772\n",
      "Epoch 18491/30000 Training Loss: 0.035121746361255646\n",
      "Epoch 18492/30000 Training Loss: 0.05358947813510895\n",
      "Epoch 18493/30000 Training Loss: 0.047575466334819794\n",
      "Epoch 18494/30000 Training Loss: 0.045372769236564636\n",
      "Epoch 18495/30000 Training Loss: 0.04546630382537842\n",
      "Epoch 18496/30000 Training Loss: 0.03612646088004112\n",
      "Epoch 18497/30000 Training Loss: 0.0466904491186142\n",
      "Epoch 18498/30000 Training Loss: 0.050513774156570435\n",
      "Epoch 18499/30000 Training Loss: 0.04726840555667877\n",
      "Epoch 18500/30000 Training Loss: 0.04783002287149429\n",
      "Epoch 18500/30000 Validation Loss: 0.032241906970739365\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.032241906970739365<=============\n",
      "Epoch 18501/30000 Training Loss: 0.04954153299331665\n",
      "Epoch 18502/30000 Training Loss: 0.04411894083023071\n",
      "Epoch 18503/30000 Training Loss: 0.046976976096630096\n",
      "Epoch 18504/30000 Training Loss: 0.042308010160923004\n",
      "Epoch 18505/30000 Training Loss: 0.0519825741648674\n",
      "Epoch 18506/30000 Training Loss: 0.03712800145149231\n",
      "Epoch 18507/30000 Training Loss: 0.041863713413476944\n",
      "Epoch 18508/30000 Training Loss: 0.04713215306401253\n",
      "Epoch 18509/30000 Training Loss: 0.05192882567644119\n",
      "Epoch 18510/30000 Training Loss: 0.048638246953487396\n",
      "Epoch 18511/30000 Training Loss: 0.05066090077161789\n",
      "Epoch 18512/30000 Training Loss: 0.04653855413198471\n",
      "Epoch 18513/30000 Training Loss: 0.043883033096790314\n",
      "Epoch 18514/30000 Training Loss: 0.04870098456740379\n",
      "Epoch 18515/30000 Training Loss: 0.040778808295726776\n",
      "Epoch 18516/30000 Training Loss: 0.0490899421274662\n",
      "Epoch 18517/30000 Training Loss: 0.038113370537757874\n",
      "Epoch 18518/30000 Training Loss: 0.03875255957245827\n",
      "Epoch 18519/30000 Training Loss: 0.04405902326107025\n",
      "Epoch 18520/30000 Training Loss: 0.04933146387338638\n",
      "Epoch 18521/30000 Training Loss: 0.03763563930988312\n",
      "Epoch 18522/30000 Training Loss: 0.034877561032772064\n",
      "Epoch 18523/30000 Training Loss: 0.03795485198497772\n",
      "Epoch 18524/30000 Training Loss: 0.046325258910655975\n",
      "Epoch 18525/30000 Training Loss: 0.048714615404605865\n",
      "Epoch 18526/30000 Training Loss: 0.05304871127009392\n",
      "Epoch 18527/30000 Training Loss: 0.047022413462400436\n",
      "Epoch 18528/30000 Training Loss: 0.05130351707339287\n",
      "Epoch 18529/30000 Training Loss: 0.05163227394223213\n",
      "Epoch 18530/30000 Training Loss: 0.05070625990629196\n",
      "Epoch 18531/30000 Training Loss: 0.03222108632326126\n",
      "Epoch 18532/30000 Training Loss: 0.04881882667541504\n",
      "Epoch 18533/30000 Training Loss: 0.044026345014572144\n",
      "Epoch 18534/30000 Training Loss: 0.04021046310663223\n",
      "Epoch 18535/30000 Training Loss: 0.050262197852134705\n",
      "Epoch 18536/30000 Training Loss: 0.038828082382678986\n",
      "Epoch 18537/30000 Training Loss: 0.05127895623445511\n",
      "Epoch 18538/30000 Training Loss: 0.05018847435712814\n",
      "Epoch 18539/30000 Training Loss: 0.03650817275047302\n",
      "Epoch 18540/30000 Training Loss: 0.043768588453531265\n",
      "Epoch 18541/30000 Training Loss: 0.037830304354429245\n",
      "Epoch 18542/30000 Training Loss: 0.04510803520679474\n",
      "Epoch 18543/30000 Training Loss: 0.04577508568763733\n",
      "Epoch 18544/30000 Training Loss: 0.04743539169430733\n",
      "Epoch 18545/30000 Training Loss: 0.04199900105595589\n",
      "Epoch 18546/30000 Training Loss: 0.053713686764240265\n",
      "Epoch 18547/30000 Training Loss: 0.04824382811784744\n",
      "Epoch 18548/30000 Training Loss: 0.03661792725324631\n",
      "Epoch 18549/30000 Training Loss: 0.0486333891749382\n",
      "Epoch 18550/30000 Training Loss: 0.056762710213661194\n",
      "Epoch 18551/30000 Training Loss: 0.06854993104934692\n",
      "Epoch 18552/30000 Training Loss: 0.05403837561607361\n",
      "Epoch 18553/30000 Training Loss: 0.0381198525428772\n",
      "Epoch 18554/30000 Training Loss: 0.04806576669216156\n",
      "Epoch 18555/30000 Training Loss: 0.03052399307489395\n",
      "Epoch 18556/30000 Training Loss: 0.04765452817082405\n",
      "Epoch 18557/30000 Training Loss: 0.046965353190898895\n",
      "Epoch 18558/30000 Training Loss: 0.038043998181819916\n",
      "Epoch 18559/30000 Training Loss: 0.0440620519220829\n",
      "Epoch 18560/30000 Training Loss: 0.037202637642621994\n",
      "Epoch 18561/30000 Training Loss: 0.029382450506091118\n",
      "Epoch 18562/30000 Training Loss: 0.04164285212755203\n",
      "Epoch 18563/30000 Training Loss: 0.05753464996814728\n",
      "Epoch 18564/30000 Training Loss: 0.055766068398952484\n",
      "Epoch 18565/30000 Training Loss: 0.03657224029302597\n",
      "Epoch 18566/30000 Training Loss: 0.059329211711883545\n",
      "Epoch 18567/30000 Training Loss: 0.029775049537420273\n",
      "Epoch 18568/30000 Training Loss: 0.05133441090583801\n",
      "Epoch 18569/30000 Training Loss: 0.053687840700149536\n",
      "Epoch 18570/30000 Training Loss: 0.05619127303361893\n",
      "Epoch 18571/30000 Training Loss: 0.04981590807437897\n",
      "Epoch 18572/30000 Training Loss: 0.039613381028175354\n",
      "Epoch 18573/30000 Training Loss: 0.03566691279411316\n",
      "Epoch 18574/30000 Training Loss: 0.04000846669077873\n",
      "Epoch 18575/30000 Training Loss: 0.04837276414036751\n",
      "Epoch 18576/30000 Training Loss: 0.04530978202819824\n",
      "Epoch 18577/30000 Training Loss: 0.04442764073610306\n",
      "Epoch 18578/30000 Training Loss: 0.038765113800764084\n",
      "Epoch 18579/30000 Training Loss: 0.05123475566506386\n",
      "Epoch 18580/30000 Training Loss: 0.04193444550037384\n",
      "Epoch 18581/30000 Training Loss: 0.061177439987659454\n",
      "Epoch 18582/30000 Training Loss: 0.04240250587463379\n",
      "Epoch 18583/30000 Training Loss: 0.040021881461143494\n",
      "Epoch 18584/30000 Training Loss: 0.053891055285930634\n",
      "Epoch 18585/30000 Training Loss: 0.04840598255395889\n",
      "Epoch 18586/30000 Training Loss: 0.03622721508145332\n",
      "Epoch 18587/30000 Training Loss: 0.042052701115608215\n",
      "Epoch 18588/30000 Training Loss: 0.03538016974925995\n",
      "Epoch 18589/30000 Training Loss: 0.043606121093034744\n",
      "Epoch 18590/30000 Training Loss: 0.05042771250009537\n",
      "Epoch 18591/30000 Training Loss: 0.039945125579833984\n",
      "Epoch 18592/30000 Training Loss: 0.03808608651161194\n",
      "Epoch 18593/30000 Training Loss: 0.0466073676943779\n",
      "Epoch 18594/30000 Training Loss: 0.048285771161317825\n",
      "Epoch 18595/30000 Training Loss: 0.04704683646559715\n",
      "Epoch 18596/30000 Training Loss: 0.044719189405441284\n",
      "Epoch 18597/30000 Training Loss: 0.0569167397916317\n",
      "Epoch 18598/30000 Training Loss: 0.0484575480222702\n",
      "Epoch 18599/30000 Training Loss: 0.04656112939119339\n",
      "Epoch 18600/30000 Training Loss: 0.04788791388273239\n",
      "Epoch 18600/30000 Validation Loss: 0.05421725660562515\n",
      "Epoch 18601/30000 Training Loss: 0.04945863410830498\n",
      "Epoch 18602/30000 Training Loss: 0.03875618427991867\n",
      "Epoch 18603/30000 Training Loss: 0.04396872967481613\n",
      "Epoch 18604/30000 Training Loss: 0.044469915330410004\n",
      "Epoch 18605/30000 Training Loss: 0.0459090955555439\n",
      "Epoch 18606/30000 Training Loss: 0.041770175099372864\n",
      "Epoch 18607/30000 Training Loss: 0.05102578550577164\n",
      "Epoch 18608/30000 Training Loss: 0.04811061918735504\n",
      "Epoch 18609/30000 Training Loss: 0.05481720715761185\n",
      "Epoch 18610/30000 Training Loss: 0.04520471766591072\n",
      "Epoch 18611/30000 Training Loss: 0.0332162044942379\n",
      "Epoch 18612/30000 Training Loss: 0.03678194433450699\n",
      "Epoch 18613/30000 Training Loss: 0.04881063848733902\n",
      "Epoch 18614/30000 Training Loss: 0.041981086134910583\n",
      "Epoch 18615/30000 Training Loss: 0.03527366369962692\n",
      "Epoch 18616/30000 Training Loss: 0.03997134789824486\n",
      "Epoch 18617/30000 Training Loss: 0.037856027483940125\n",
      "Epoch 18618/30000 Training Loss: 0.05007275193929672\n",
      "Epoch 18619/30000 Training Loss: 0.04327255114912987\n",
      "Epoch 18620/30000 Training Loss: 0.036710333079099655\n",
      "Epoch 18621/30000 Training Loss: 0.04073447361588478\n",
      "Epoch 18622/30000 Training Loss: 0.045833978801965714\n",
      "Epoch 18623/30000 Training Loss: 0.05554668605327606\n",
      "Epoch 18624/30000 Training Loss: 0.034223854541778564\n",
      "Epoch 18625/30000 Training Loss: 0.05076798424124718\n",
      "Epoch 18626/30000 Training Loss: 0.048559319227933884\n",
      "Epoch 18627/30000 Training Loss: 0.040192730724811554\n",
      "Epoch 18628/30000 Training Loss: 0.03891701623797417\n",
      "Epoch 18629/30000 Training Loss: 0.03500480577349663\n",
      "Epoch 18630/30000 Training Loss: 0.05012121796607971\n",
      "Epoch 18631/30000 Training Loss: 0.045224420726299286\n",
      "Epoch 18632/30000 Training Loss: 0.03785067796707153\n",
      "Epoch 18633/30000 Training Loss: 0.043549664318561554\n",
      "Epoch 18634/30000 Training Loss: 0.059206001460552216\n",
      "Epoch 18635/30000 Training Loss: 0.05018144100904465\n",
      "Epoch 18636/30000 Training Loss: 0.04994228482246399\n",
      "Epoch 18637/30000 Training Loss: 0.038397349417209625\n",
      "Epoch 18638/30000 Training Loss: 0.043608374893665314\n",
      "Epoch 18639/30000 Training Loss: 0.0472785159945488\n",
      "Epoch 18640/30000 Training Loss: 0.03951392322778702\n",
      "Epoch 18641/30000 Training Loss: 0.04328975826501846\n",
      "Epoch 18642/30000 Training Loss: 0.047263216227293015\n",
      "Epoch 18643/30000 Training Loss: 0.0651339739561081\n",
      "Epoch 18644/30000 Training Loss: 0.045022882521152496\n",
      "Epoch 18645/30000 Training Loss: 0.05018883943557739\n",
      "Epoch 18646/30000 Training Loss: 0.0343446210026741\n",
      "Epoch 18647/30000 Training Loss: 0.051321178674697876\n",
      "Epoch 18648/30000 Training Loss: 0.041784316301345825\n",
      "Epoch 18649/30000 Training Loss: 0.04050394520163536\n",
      "Epoch 18650/30000 Training Loss: 0.038147762417793274\n",
      "Epoch 18651/30000 Training Loss: 0.059411898255348206\n",
      "Epoch 18652/30000 Training Loss: 0.05448063835501671\n",
      "Epoch 18653/30000 Training Loss: 0.03729000315070152\n",
      "Epoch 18654/30000 Training Loss: 0.047760751098394394\n",
      "Epoch 18655/30000 Training Loss: 0.0584656298160553\n",
      "Epoch 18656/30000 Training Loss: 0.04039736092090607\n",
      "Epoch 18657/30000 Training Loss: 0.03049716167151928\n",
      "Epoch 18658/30000 Training Loss: 0.055687688291072845\n",
      "Epoch 18659/30000 Training Loss: 0.032739605754613876\n",
      "Epoch 18660/30000 Training Loss: 0.04571571201086044\n",
      "Epoch 18661/30000 Training Loss: 0.043993860483169556\n",
      "Epoch 18662/30000 Training Loss: 0.04402312636375427\n",
      "Epoch 18663/30000 Training Loss: 0.04802694171667099\n",
      "Epoch 18664/30000 Training Loss: 0.05009480565786362\n",
      "Epoch 18665/30000 Training Loss: 0.04974778741598129\n",
      "Epoch 18666/30000 Training Loss: 0.05030316114425659\n",
      "Epoch 18667/30000 Training Loss: 0.03875347226858139\n",
      "Epoch 18668/30000 Training Loss: 0.03793475776910782\n",
      "Epoch 18669/30000 Training Loss: 0.05862637236714363\n",
      "Epoch 18670/30000 Training Loss: 0.03916478902101517\n",
      "Epoch 18671/30000 Training Loss: 0.04584066569805145\n",
      "Epoch 18672/30000 Training Loss: 0.055799394845962524\n",
      "Epoch 18673/30000 Training Loss: 0.053593285381793976\n",
      "Epoch 18674/30000 Training Loss: 0.04303659126162529\n",
      "Epoch 18675/30000 Training Loss: 0.04782288521528244\n",
      "Epoch 18676/30000 Training Loss: 0.05080515146255493\n",
      "Epoch 18677/30000 Training Loss: 0.040346093475818634\n",
      "Epoch 18678/30000 Training Loss: 0.0485624261200428\n",
      "Epoch 18679/30000 Training Loss: 0.048741698265075684\n",
      "Epoch 18680/30000 Training Loss: 0.05173296481370926\n",
      "Epoch 18681/30000 Training Loss: 0.043005719780921936\n",
      "Epoch 18682/30000 Training Loss: 0.053273942321538925\n",
      "Epoch 18683/30000 Training Loss: 0.03357994556427002\n",
      "Epoch 18684/30000 Training Loss: 0.04010158032178879\n",
      "Epoch 18685/30000 Training Loss: 0.040980033576488495\n",
      "Epoch 18686/30000 Training Loss: 0.043733708560466766\n",
      "Epoch 18687/30000 Training Loss: 0.03417665511369705\n",
      "Epoch 18688/30000 Training Loss: 0.03901585936546326\n",
      "Epoch 18689/30000 Training Loss: 0.05433632433414459\n",
      "Epoch 18690/30000 Training Loss: 0.03403189405798912\n",
      "Epoch 18691/30000 Training Loss: 0.049668751657009125\n",
      "Epoch 18692/30000 Training Loss: 0.035130925476551056\n",
      "Epoch 18693/30000 Training Loss: 0.030380621552467346\n",
      "Epoch 18694/30000 Training Loss: 0.039806827902793884\n",
      "Epoch 18695/30000 Training Loss: 0.048764392733573914\n",
      "Epoch 18696/30000 Training Loss: 0.04978059232234955\n",
      "Epoch 18697/30000 Training Loss: 0.037180326879024506\n",
      "Epoch 18698/30000 Training Loss: 0.04677554592490196\n",
      "Epoch 18699/30000 Training Loss: 0.03766239061951637\n",
      "Epoch 18700/30000 Training Loss: 0.049723029136657715\n",
      "Epoch 18700/30000 Validation Loss: 0.03400159627199173\n",
      "Epoch 18701/30000 Training Loss: 0.05291152372956276\n",
      "Epoch 18702/30000 Training Loss: 0.043297357857227325\n",
      "Epoch 18703/30000 Training Loss: 0.05782760679721832\n",
      "Epoch 18704/30000 Training Loss: 0.03453598544001579\n",
      "Epoch 18705/30000 Training Loss: 0.06435728818178177\n",
      "Epoch 18706/30000 Training Loss: 0.04795665293931961\n",
      "Epoch 18707/30000 Training Loss: 0.046472471207380295\n",
      "Epoch 18708/30000 Training Loss: 0.04175887629389763\n",
      "Epoch 18709/30000 Training Loss: 0.04897374287247658\n",
      "Epoch 18710/30000 Training Loss: 0.036286257207393646\n",
      "Epoch 18711/30000 Training Loss: 0.03162837401032448\n",
      "Epoch 18712/30000 Training Loss: 0.04129127785563469\n",
      "Epoch 18713/30000 Training Loss: 0.0398457869887352\n",
      "Epoch 18714/30000 Training Loss: 0.044109318405389786\n",
      "Epoch 18715/30000 Training Loss: 0.04708660766482353\n",
      "Epoch 18716/30000 Training Loss: 0.03685615211725235\n",
      "Epoch 18717/30000 Training Loss: 0.050791844725608826\n",
      "Epoch 18718/30000 Training Loss: 0.034830495715141296\n",
      "Epoch 18719/30000 Training Loss: 0.041949544101953506\n",
      "Epoch 18720/30000 Training Loss: 0.051390640437603\n",
      "Epoch 18721/30000 Training Loss: 0.04692838713526726\n",
      "Epoch 18722/30000 Training Loss: 0.04010044410824776\n",
      "Epoch 18723/30000 Training Loss: 0.05070791393518448\n",
      "Epoch 18724/30000 Training Loss: 0.046877358108758926\n",
      "Epoch 18725/30000 Training Loss: 0.04485982656478882\n",
      "Epoch 18726/30000 Training Loss: 0.049181971698999405\n",
      "Epoch 18727/30000 Training Loss: 0.04125465452671051\n",
      "Epoch 18728/30000 Training Loss: 0.052297480404376984\n",
      "Epoch 18729/30000 Training Loss: 0.04234704002737999\n",
      "Epoch 18730/30000 Training Loss: 0.045214034616947174\n",
      "Epoch 18731/30000 Training Loss: 0.04002394527196884\n",
      "Epoch 18732/30000 Training Loss: 0.03448023647069931\n",
      "Epoch 18733/30000 Training Loss: 0.05152242258191109\n",
      "Epoch 18734/30000 Training Loss: 0.03424058109521866\n",
      "Epoch 18735/30000 Training Loss: 0.04035767912864685\n",
      "Epoch 18736/30000 Training Loss: 0.03551887720823288\n",
      "Epoch 18737/30000 Training Loss: 0.04047168046236038\n",
      "Epoch 18738/30000 Training Loss: 0.03625819459557533\n",
      "Epoch 18739/30000 Training Loss: 0.05804983526468277\n",
      "Epoch 18740/30000 Training Loss: 0.038276784121990204\n",
      "Epoch 18741/30000 Training Loss: 0.041928574442863464\n",
      "Epoch 18742/30000 Training Loss: 0.03462528809905052\n",
      "Epoch 18743/30000 Training Loss: 0.04218795523047447\n",
      "Epoch 18744/30000 Training Loss: 0.040750451385974884\n",
      "Epoch 18745/30000 Training Loss: 0.041985757648944855\n",
      "Epoch 18746/30000 Training Loss: 0.04128197953104973\n",
      "Epoch 18747/30000 Training Loss: 0.04620319604873657\n",
      "Epoch 18748/30000 Training Loss: 0.03709832951426506\n",
      "Epoch 18749/30000 Training Loss: 0.03474699705839157\n",
      "Epoch 18750/30000 Training Loss: 0.05287322402000427\n",
      "Epoch 18751/30000 Training Loss: 0.04141174256801605\n",
      "Epoch 18752/30000 Training Loss: 0.04080183431506157\n",
      "Epoch 18753/30000 Training Loss: 0.04230344295501709\n",
      "Epoch 18754/30000 Training Loss: 0.0452665314078331\n",
      "Epoch 18755/30000 Training Loss: 0.03910592943429947\n",
      "Epoch 18756/30000 Training Loss: 0.03166987746953964\n",
      "Epoch 18757/30000 Training Loss: 0.045356862246990204\n",
      "Epoch 18758/30000 Training Loss: 0.053939975798130035\n",
      "Epoch 18759/30000 Training Loss: 0.04420390725135803\n",
      "Epoch 18760/30000 Training Loss: 0.03545190021395683\n",
      "Epoch 18761/30000 Training Loss: 0.04214576631784439\n",
      "Epoch 18762/30000 Training Loss: 0.04078149050474167\n",
      "Epoch 18763/30000 Training Loss: 0.04652321711182594\n",
      "Epoch 18764/30000 Training Loss: 0.05844162404537201\n",
      "Epoch 18765/30000 Training Loss: 0.04705941677093506\n",
      "Epoch 18766/30000 Training Loss: 0.04283404350280762\n",
      "Epoch 18767/30000 Training Loss: 0.05062367022037506\n",
      "Epoch 18768/30000 Training Loss: 0.04027724266052246\n",
      "Epoch 18769/30000 Training Loss: 0.0397823303937912\n",
      "Epoch 18770/30000 Training Loss: 0.039095863699913025\n",
      "Epoch 18771/30000 Training Loss: 0.039719536900520325\n",
      "Epoch 18772/30000 Training Loss: 0.062102675437927246\n",
      "Epoch 18773/30000 Training Loss: 0.06571253389120102\n",
      "Epoch 18774/30000 Training Loss: 0.04620564356446266\n",
      "Epoch 18775/30000 Training Loss: 0.042121559381484985\n",
      "Epoch 18776/30000 Training Loss: 0.03409957140684128\n",
      "Epoch 18777/30000 Training Loss: 0.05268324166536331\n",
      "Epoch 18778/30000 Training Loss: 0.032149821519851685\n",
      "Epoch 18779/30000 Training Loss: 0.04476844146847725\n",
      "Epoch 18780/30000 Training Loss: 0.036269187927246094\n",
      "Epoch 18781/30000 Training Loss: 0.036734938621520996\n",
      "Epoch 18782/30000 Training Loss: 0.0403541699051857\n",
      "Epoch 18783/30000 Training Loss: 0.03816404938697815\n",
      "Epoch 18784/30000 Training Loss: 0.039250656962394714\n",
      "Epoch 18785/30000 Training Loss: 0.04227519780397415\n",
      "Epoch 18786/30000 Training Loss: 0.045586585998535156\n",
      "Epoch 18787/30000 Training Loss: 0.04576815664768219\n",
      "Epoch 18788/30000 Training Loss: 0.04429280757904053\n",
      "Epoch 18789/30000 Training Loss: 0.05219169706106186\n",
      "Epoch 18790/30000 Training Loss: 0.04244188964366913\n",
      "Epoch 18791/30000 Training Loss: 0.04632674902677536\n",
      "Epoch 18792/30000 Training Loss: 0.026574505493044853\n",
      "Epoch 18793/30000 Training Loss: 0.029572656378149986\n",
      "Epoch 18794/30000 Training Loss: 0.04617495834827423\n",
      "Epoch 18795/30000 Training Loss: 0.03490244224667549\n",
      "Epoch 18796/30000 Training Loss: 0.04281364381313324\n",
      "Epoch 18797/30000 Training Loss: 0.038570307195186615\n",
      "Epoch 18798/30000 Training Loss: 0.033805590122938156\n",
      "Epoch 18799/30000 Training Loss: 0.04678009822964668\n",
      "Epoch 18800/30000 Training Loss: 0.064339779317379\n",
      "Epoch 18800/30000 Validation Loss: 0.058463044464588165\n",
      "Epoch 18801/30000 Training Loss: 0.05996309220790863\n",
      "Epoch 18802/30000 Training Loss: 0.050237249583005905\n",
      "Epoch 18803/30000 Training Loss: 0.04917515069246292\n",
      "Epoch 18804/30000 Training Loss: 0.03979752957820892\n",
      "Epoch 18805/30000 Training Loss: 0.04895901679992676\n",
      "Epoch 18806/30000 Training Loss: 0.03155374154448509\n",
      "Epoch 18807/30000 Training Loss: 0.030257582664489746\n",
      "Epoch 18808/30000 Training Loss: 0.043788038194179535\n",
      "Epoch 18809/30000 Training Loss: 0.04715827852487564\n",
      "Epoch 18810/30000 Training Loss: 0.04660259187221527\n",
      "Epoch 18811/30000 Training Loss: 0.060824643820524216\n",
      "Epoch 18812/30000 Training Loss: 0.03209973871707916\n",
      "Epoch 18813/30000 Training Loss: 0.04859515279531479\n",
      "Epoch 18814/30000 Training Loss: 0.04981275275349617\n",
      "Epoch 18815/30000 Training Loss: 0.05008862167596817\n",
      "Epoch 18816/30000 Training Loss: 0.04950929805636406\n",
      "Epoch 18817/30000 Training Loss: 0.045046351850032806\n",
      "Epoch 18818/30000 Training Loss: 0.04576832056045532\n",
      "Epoch 18819/30000 Training Loss: 0.04697591811418533\n",
      "Epoch 18820/30000 Training Loss: 0.051305174827575684\n",
      "Epoch 18821/30000 Training Loss: 0.058003075420856476\n",
      "Epoch 18822/30000 Training Loss: 0.04060528427362442\n",
      "Epoch 18823/30000 Training Loss: 0.04338870197534561\n",
      "Epoch 18824/30000 Training Loss: 0.04623368754982948\n",
      "Epoch 18825/30000 Training Loss: 0.04438457265496254\n",
      "Epoch 18826/30000 Training Loss: 0.05051962286233902\n",
      "Epoch 18827/30000 Training Loss: 0.046070631593465805\n",
      "Epoch 18828/30000 Training Loss: 0.03946113958954811\n",
      "Epoch 18829/30000 Training Loss: 0.04954522103071213\n",
      "Epoch 18830/30000 Training Loss: 0.03897038847208023\n",
      "Epoch 18831/30000 Training Loss: 0.06760690361261368\n",
      "Epoch 18832/30000 Training Loss: 0.040298499166965485\n",
      "Epoch 18833/30000 Training Loss: 0.04213747754693031\n",
      "Epoch 18834/30000 Training Loss: 0.04931844770908356\n",
      "Epoch 18835/30000 Training Loss: 0.05985945835709572\n",
      "Epoch 18836/30000 Training Loss: 0.04902023822069168\n",
      "Epoch 18837/30000 Training Loss: 0.040697406977415085\n",
      "Epoch 18838/30000 Training Loss: 0.0399751290678978\n",
      "Epoch 18839/30000 Training Loss: 0.049223393201828\n",
      "Epoch 18840/30000 Training Loss: 0.04373808205127716\n",
      "Epoch 18841/30000 Training Loss: 0.039287589490413666\n",
      "Epoch 18842/30000 Training Loss: 0.03758968412876129\n",
      "Epoch 18843/30000 Training Loss: 0.037093229591846466\n",
      "Epoch 18844/30000 Training Loss: 0.03816905990242958\n",
      "Epoch 18845/30000 Training Loss: 0.04043857753276825\n",
      "Epoch 18846/30000 Training Loss: 0.033065102994441986\n",
      "Epoch 18847/30000 Training Loss: 0.055789511650800705\n",
      "Epoch 18848/30000 Training Loss: 0.04245440661907196\n",
      "Epoch 18849/30000 Training Loss: 0.03879482299089432\n",
      "Epoch 18850/30000 Training Loss: 0.040093593299388885\n",
      "Epoch 18851/30000 Training Loss: 0.04030156135559082\n",
      "Epoch 18852/30000 Training Loss: 0.04588190093636513\n",
      "Epoch 18853/30000 Training Loss: 0.0401807501912117\n",
      "Epoch 18854/30000 Training Loss: 0.04318898171186447\n",
      "Epoch 18855/30000 Training Loss: 0.044728171080350876\n",
      "Epoch 18856/30000 Training Loss: 0.05023699253797531\n",
      "Epoch 18857/30000 Training Loss: 0.041602328419685364\n",
      "Epoch 18858/30000 Training Loss: 0.04124920070171356\n",
      "Epoch 18859/30000 Training Loss: 0.047523170709609985\n",
      "Epoch 18860/30000 Training Loss: 0.0444038063287735\n",
      "Epoch 18861/30000 Training Loss: 0.04287488013505936\n",
      "Epoch 18862/30000 Training Loss: 0.03790443018078804\n",
      "Epoch 18863/30000 Training Loss: 0.03964228928089142\n",
      "Epoch 18864/30000 Training Loss: 0.04779105633497238\n",
      "Epoch 18865/30000 Training Loss: 0.039392948150634766\n",
      "Epoch 18866/30000 Training Loss: 0.04916547238826752\n",
      "Epoch 18867/30000 Training Loss: 0.035736147314310074\n",
      "Epoch 18868/30000 Training Loss: 0.04941970854997635\n",
      "Epoch 18869/30000 Training Loss: 0.04938124120235443\n",
      "Epoch 18870/30000 Training Loss: 0.04535750299692154\n",
      "Epoch 18871/30000 Training Loss: 0.04196350276470184\n",
      "Epoch 18872/30000 Training Loss: 0.04178840294480324\n",
      "Epoch 18873/30000 Training Loss: 0.03716990724205971\n",
      "Epoch 18874/30000 Training Loss: 0.04254266619682312\n",
      "Epoch 18875/30000 Training Loss: 0.04336703568696976\n",
      "Epoch 18876/30000 Training Loss: 0.04271519184112549\n",
      "Epoch 18877/30000 Training Loss: 0.03905723989009857\n",
      "Epoch 18878/30000 Training Loss: 0.045098524540662766\n",
      "Epoch 18879/30000 Training Loss: 0.05874494090676308\n",
      "Epoch 18880/30000 Training Loss: 0.04480558633804321\n",
      "Epoch 18881/30000 Training Loss: 0.04625071585178375\n",
      "Epoch 18882/30000 Training Loss: 0.048145487904548645\n",
      "Epoch 18883/30000 Training Loss: 0.02957482635974884\n",
      "Epoch 18884/30000 Training Loss: 0.03941549360752106\n",
      "Epoch 18885/30000 Training Loss: 0.048760198056697845\n",
      "Epoch 18886/30000 Training Loss: 0.04075850546360016\n",
      "Epoch 18887/30000 Training Loss: 0.0413069948554039\n",
      "Epoch 18888/30000 Training Loss: 0.04076474532485008\n",
      "Epoch 18889/30000 Training Loss: 0.03215278312563896\n",
      "Epoch 18890/30000 Training Loss: 0.04245883971452713\n",
      "Epoch 18891/30000 Training Loss: 0.037046365439891815\n",
      "Epoch 18892/30000 Training Loss: 0.036618903279304504\n",
      "Epoch 18893/30000 Training Loss: 0.046827562153339386\n",
      "Epoch 18894/30000 Training Loss: 0.04291617497801781\n",
      "Epoch 18895/30000 Training Loss: 0.03984244167804718\n",
      "Epoch 18896/30000 Training Loss: 0.05251985788345337\n",
      "Epoch 18897/30000 Training Loss: 0.0500757098197937\n",
      "Epoch 18898/30000 Training Loss: 0.03865887224674225\n",
      "Epoch 18899/30000 Training Loss: 0.03781452775001526\n",
      "Epoch 18900/30000 Training Loss: 0.04855509102344513\n",
      "Epoch 18900/30000 Validation Loss: 0.03735198825597763\n",
      "Epoch 18901/30000 Training Loss: 0.03855074942111969\n",
      "Epoch 18902/30000 Training Loss: 0.053200241178274155\n",
      "Epoch 18903/30000 Training Loss: 0.058558348566293716\n",
      "Epoch 18904/30000 Training Loss: 0.03461020812392235\n",
      "Epoch 18905/30000 Training Loss: 0.04782857745885849\n",
      "Epoch 18906/30000 Training Loss: 0.04079689085483551\n",
      "Epoch 18907/30000 Training Loss: 0.05450262129306793\n",
      "Epoch 18908/30000 Training Loss: 0.0451032891869545\n",
      "Epoch 18909/30000 Training Loss: 0.060993634164333344\n",
      "Epoch 18910/30000 Training Loss: 0.057739876210689545\n",
      "Epoch 18911/30000 Training Loss: 0.053813233971595764\n",
      "Epoch 18912/30000 Training Loss: 0.0355403870344162\n",
      "Epoch 18913/30000 Training Loss: 0.04258716106414795\n",
      "Epoch 18914/30000 Training Loss: 0.05364742502570152\n",
      "Epoch 18915/30000 Training Loss: 0.04284767061471939\n",
      "Epoch 18916/30000 Training Loss: 0.04470032453536987\n",
      "Epoch 18917/30000 Training Loss: 0.04032167047262192\n",
      "Epoch 18918/30000 Training Loss: 0.05288330465555191\n",
      "Epoch 18919/30000 Training Loss: 0.05657580494880676\n",
      "Epoch 18920/30000 Training Loss: 0.04005388543009758\n",
      "Epoch 18921/30000 Training Loss: 0.03910987079143524\n",
      "Epoch 18922/30000 Training Loss: 0.03948415070772171\n",
      "Epoch 18923/30000 Training Loss: 0.05071249604225159\n",
      "Epoch 18924/30000 Training Loss: 0.037119969725608826\n",
      "Epoch 18925/30000 Training Loss: 0.043645210564136505\n",
      "Epoch 18926/30000 Training Loss: 0.052513010799884796\n",
      "Epoch 18927/30000 Training Loss: 0.04468707740306854\n",
      "Epoch 18928/30000 Training Loss: 0.0398402102291584\n",
      "Epoch 18929/30000 Training Loss: 0.03879993036389351\n",
      "Epoch 18930/30000 Training Loss: 0.04721539467573166\n",
      "Epoch 18931/30000 Training Loss: 0.04137081280350685\n",
      "Epoch 18932/30000 Training Loss: 0.03548476845026016\n",
      "Epoch 18933/30000 Training Loss: 0.05604993551969528\n",
      "Epoch 18934/30000 Training Loss: 0.04022955894470215\n",
      "Epoch 18935/30000 Training Loss: 0.045668408274650574\n",
      "Epoch 18936/30000 Training Loss: 0.03637051582336426\n",
      "Epoch 18937/30000 Training Loss: 0.049620479345321655\n",
      "Epoch 18938/30000 Training Loss: 0.07285836338996887\n",
      "Epoch 18939/30000 Training Loss: 0.04874400794506073\n",
      "Epoch 18940/30000 Training Loss: 0.05359147489070892\n",
      "Epoch 18941/30000 Training Loss: 0.06189418584108353\n",
      "Epoch 18942/30000 Training Loss: 0.03169930353760719\n",
      "Epoch 18943/30000 Training Loss: 0.047878749668598175\n",
      "Epoch 18944/30000 Training Loss: 0.038086242973804474\n",
      "Epoch 18945/30000 Training Loss: 0.054268576204776764\n",
      "Epoch 18946/30000 Training Loss: 0.032124873250722885\n",
      "Epoch 18947/30000 Training Loss: 0.050979990512132645\n",
      "Epoch 18948/30000 Training Loss: 0.04141034185886383\n",
      "Epoch 18949/30000 Training Loss: 0.04088149592280388\n",
      "Epoch 18950/30000 Training Loss: 0.043782442808151245\n",
      "Epoch 18951/30000 Training Loss: 0.03276367858052254\n",
      "Epoch 18952/30000 Training Loss: 0.0692310482263565\n",
      "Epoch 18953/30000 Training Loss: 0.05427602306008339\n",
      "Epoch 18954/30000 Training Loss: 0.04880978539586067\n",
      "Epoch 18955/30000 Training Loss: 0.05541399121284485\n",
      "Epoch 18956/30000 Training Loss: 0.034449122846126556\n",
      "Epoch 18957/30000 Training Loss: 0.06025053560733795\n",
      "Epoch 18958/30000 Training Loss: 0.04503439739346504\n",
      "Epoch 18959/30000 Training Loss: 0.044892191886901855\n",
      "Epoch 18960/30000 Training Loss: 0.05607428029179573\n",
      "Epoch 18961/30000 Training Loss: 0.049530573189258575\n",
      "Epoch 18962/30000 Training Loss: 0.06006815657019615\n",
      "Epoch 18963/30000 Training Loss: 0.05987207591533661\n",
      "Epoch 18964/30000 Training Loss: 0.042766571044921875\n",
      "Epoch 18965/30000 Training Loss: 0.04042193293571472\n",
      "Epoch 18966/30000 Training Loss: 0.038114894181489944\n",
      "Epoch 18967/30000 Training Loss: 0.0484645813703537\n",
      "Epoch 18968/30000 Training Loss: 0.03821687027812004\n",
      "Epoch 18969/30000 Training Loss: 0.04589511826634407\n",
      "Epoch 18970/30000 Training Loss: 0.055996254086494446\n",
      "Epoch 18971/30000 Training Loss: 0.050397373735904694\n",
      "Epoch 18972/30000 Training Loss: 0.03677292540669441\n",
      "Epoch 18973/30000 Training Loss: 0.039010729640722275\n",
      "Epoch 18974/30000 Training Loss: 0.05726274847984314\n",
      "Epoch 18975/30000 Training Loss: 0.04337557777762413\n",
      "Epoch 18976/30000 Training Loss: 0.04394210875034332\n",
      "Epoch 18977/30000 Training Loss: 0.034367308020591736\n",
      "Epoch 18978/30000 Training Loss: 0.04367837309837341\n",
      "Epoch 18979/30000 Training Loss: 0.04196012392640114\n",
      "Epoch 18980/30000 Training Loss: 0.04460867494344711\n",
      "Epoch 18981/30000 Training Loss: 0.05123591795563698\n",
      "Epoch 18982/30000 Training Loss: 0.054552678018808365\n",
      "Epoch 18983/30000 Training Loss: 0.043926630169153214\n",
      "Epoch 18984/30000 Training Loss: 0.04881087318062782\n",
      "Epoch 18985/30000 Training Loss: 0.05410025268793106\n",
      "Epoch 18986/30000 Training Loss: 0.05447407066822052\n",
      "Epoch 18987/30000 Training Loss: 0.04340042546391487\n",
      "Epoch 18988/30000 Training Loss: 0.04071866720914841\n",
      "Epoch 18989/30000 Training Loss: 0.042866505682468414\n",
      "Epoch 18990/30000 Training Loss: 0.04040779918432236\n",
      "Epoch 18991/30000 Training Loss: 0.0507635623216629\n",
      "Epoch 18992/30000 Training Loss: 0.046257391571998596\n",
      "Epoch 18993/30000 Training Loss: 0.038366347551345825\n",
      "Epoch 18994/30000 Training Loss: 0.037696197628974915\n",
      "Epoch 18995/30000 Training Loss: 0.0453132763504982\n",
      "Epoch 18996/30000 Training Loss: 0.055771343410015106\n",
      "Epoch 18997/30000 Training Loss: 0.04251107573509216\n",
      "Epoch 18998/30000 Training Loss: 0.05605844408273697\n",
      "Epoch 18999/30000 Training Loss: 0.046004850417375565\n",
      "Epoch 19000/30000 Training Loss: 0.034142330288887024\n",
      "Epoch 19000/30000 Validation Loss: 0.04344169795513153\n",
      "Epoch 19001/30000 Training Loss: 0.051753170788288116\n",
      "Epoch 19002/30000 Training Loss: 0.03877326101064682\n",
      "Epoch 19003/30000 Training Loss: 0.04112619906663895\n",
      "Epoch 19004/30000 Training Loss: 0.046232856810092926\n",
      "Epoch 19005/30000 Training Loss: 0.04546843469142914\n",
      "Epoch 19006/30000 Training Loss: 0.04025464877486229\n",
      "Epoch 19007/30000 Training Loss: 0.03958664834499359\n",
      "Epoch 19008/30000 Training Loss: 0.04653523117303848\n",
      "Epoch 19009/30000 Training Loss: 0.05917954444885254\n",
      "Epoch 19010/30000 Training Loss: 0.05958440154790878\n",
      "Epoch 19011/30000 Training Loss: 0.0619983971118927\n",
      "Epoch 19012/30000 Training Loss: 0.037454862147569656\n",
      "Epoch 19013/30000 Training Loss: 0.04242226481437683\n",
      "Epoch 19014/30000 Training Loss: 0.03833695501089096\n",
      "Epoch 19015/30000 Training Loss: 0.03612443059682846\n",
      "Epoch 19016/30000 Training Loss: 0.0364694744348526\n",
      "Epoch 19017/30000 Training Loss: 0.03825937211513519\n",
      "Epoch 19018/30000 Training Loss: 0.04055876284837723\n",
      "Epoch 19019/30000 Training Loss: 0.038294412195682526\n",
      "Epoch 19020/30000 Training Loss: 0.04248858988285065\n",
      "Epoch 19021/30000 Training Loss: 0.044374577701091766\n",
      "Epoch 19022/30000 Training Loss: 0.041799742728471756\n",
      "Epoch 19023/30000 Training Loss: 0.031493544578552246\n",
      "Epoch 19024/30000 Training Loss: 0.04602556303143501\n",
      "Epoch 19025/30000 Training Loss: 0.06189529597759247\n",
      "Epoch 19026/30000 Training Loss: 0.025583188980817795\n",
      "Epoch 19027/30000 Training Loss: 0.03298335149884224\n",
      "Epoch 19028/30000 Training Loss: 0.05914599075913429\n",
      "Epoch 19029/30000 Training Loss: 0.041474051773548126\n",
      "Epoch 19030/30000 Training Loss: 0.033566370606422424\n",
      "Epoch 19031/30000 Training Loss: 0.042695097625255585\n",
      "Epoch 19032/30000 Training Loss: 0.03961355984210968\n",
      "Epoch 19033/30000 Training Loss: 0.043182965368032455\n",
      "Epoch 19034/30000 Training Loss: 0.03226490318775177\n",
      "Epoch 19035/30000 Training Loss: 0.04028725624084473\n",
      "Epoch 19036/30000 Training Loss: 0.03978465870022774\n",
      "Epoch 19037/30000 Training Loss: 0.05731288716197014\n",
      "Epoch 19038/30000 Training Loss: 0.053214266896247864\n",
      "Epoch 19039/30000 Training Loss: 0.056827060878276825\n",
      "Epoch 19040/30000 Training Loss: 0.026932068169116974\n",
      "Epoch 19041/30000 Training Loss: 0.04708399251103401\n",
      "Epoch 19042/30000 Training Loss: 0.045735687017440796\n",
      "Epoch 19043/30000 Training Loss: 0.047390926629304886\n",
      "Epoch 19044/30000 Training Loss: 0.0442194864153862\n",
      "Epoch 19045/30000 Training Loss: 0.04609854519367218\n",
      "Epoch 19046/30000 Training Loss: 0.04051315411925316\n",
      "Epoch 19047/30000 Training Loss: 0.04021264240145683\n",
      "Epoch 19048/30000 Training Loss: 0.04994257166981697\n",
      "Epoch 19049/30000 Training Loss: 0.05581106245517731\n",
      "Epoch 19050/30000 Training Loss: 0.04372898489236832\n",
      "Epoch 19051/30000 Training Loss: 0.054308630526065826\n",
      "Epoch 19052/30000 Training Loss: 0.03414081037044525\n",
      "Epoch 19053/30000 Training Loss: 0.036524102091789246\n",
      "Epoch 19054/30000 Training Loss: 0.028028162196278572\n",
      "Epoch 19055/30000 Training Loss: 0.049820709973573685\n",
      "Epoch 19056/30000 Training Loss: 0.0438336506485939\n",
      "Epoch 19057/30000 Training Loss: 0.04726038873195648\n",
      "Epoch 19058/30000 Training Loss: 0.041164614260196686\n",
      "Epoch 19059/30000 Training Loss: 0.05188557505607605\n",
      "Epoch 19060/30000 Training Loss: 0.03813503310084343\n",
      "Epoch 19061/30000 Training Loss: 0.05152968317270279\n",
      "Epoch 19062/30000 Training Loss: 0.04584869369864464\n",
      "Epoch 19063/30000 Training Loss: 0.046167537569999695\n",
      "Epoch 19064/30000 Training Loss: 0.03808124363422394\n",
      "Epoch 19065/30000 Training Loss: 0.03659128397703171\n",
      "Epoch 19066/30000 Training Loss: 0.0419120118021965\n",
      "Epoch 19067/30000 Training Loss: 0.046357449144124985\n",
      "Epoch 19068/30000 Training Loss: 0.03724728152155876\n",
      "Epoch 19069/30000 Training Loss: 0.045435115694999695\n",
      "Epoch 19070/30000 Training Loss: 0.05498374253511429\n",
      "Epoch 19071/30000 Training Loss: 0.04458847641944885\n",
      "Epoch 19072/30000 Training Loss: 0.06113801151514053\n",
      "Epoch 19073/30000 Training Loss: 0.04620637744665146\n",
      "Epoch 19074/30000 Training Loss: 0.05654299631714821\n",
      "Epoch 19075/30000 Training Loss: 0.06694085896015167\n",
      "Epoch 19076/30000 Training Loss: 0.04943466559052467\n",
      "Epoch 19077/30000 Training Loss: 0.03486710041761398\n",
      "Epoch 19078/30000 Training Loss: 0.026321105659008026\n",
      "Epoch 19079/30000 Training Loss: 0.05700308084487915\n",
      "Epoch 19080/30000 Training Loss: 0.03742250055074692\n",
      "Epoch 19081/30000 Training Loss: 0.03733651340007782\n",
      "Epoch 19082/30000 Training Loss: 0.03947228938341141\n",
      "Epoch 19083/30000 Training Loss: 0.06202356889843941\n",
      "Epoch 19084/30000 Training Loss: 0.041831422597169876\n",
      "Epoch 19085/30000 Training Loss: 0.046010300517082214\n",
      "Epoch 19086/30000 Training Loss: 0.04239853471517563\n",
      "Epoch 19087/30000 Training Loss: 0.03889983147382736\n",
      "Epoch 19088/30000 Training Loss: 0.05303646996617317\n",
      "Epoch 19089/30000 Training Loss: 0.04190925508737564\n",
      "Epoch 19090/30000 Training Loss: 0.03668314591050148\n",
      "Epoch 19091/30000 Training Loss: 0.0452808253467083\n",
      "Epoch 19092/30000 Training Loss: 0.04531178995966911\n",
      "Epoch 19093/30000 Training Loss: 0.04689415544271469\n",
      "Epoch 19094/30000 Training Loss: 0.051252320408821106\n",
      "Epoch 19095/30000 Training Loss: 0.04571525752544403\n",
      "Epoch 19096/30000 Training Loss: 0.043408192694187164\n",
      "Epoch 19097/30000 Training Loss: 0.03972645476460457\n",
      "Epoch 19098/30000 Training Loss: 0.04643743485212326\n",
      "Epoch 19099/30000 Training Loss: 0.03706863522529602\n",
      "Epoch 19100/30000 Training Loss: 0.046613551676273346\n",
      "Epoch 19100/30000 Validation Loss: 0.047958001494407654\n",
      "Epoch 19101/30000 Training Loss: 0.03813033550977707\n",
      "Epoch 19102/30000 Training Loss: 0.04784884676337242\n",
      "Epoch 19103/30000 Training Loss: 0.044120386242866516\n",
      "Epoch 19104/30000 Training Loss: 0.04173918068408966\n",
      "Epoch 19105/30000 Training Loss: 0.043175823986530304\n",
      "Epoch 19106/30000 Training Loss: 0.05132830888032913\n",
      "Epoch 19107/30000 Training Loss: 0.0404023677110672\n",
      "Epoch 19108/30000 Training Loss: 0.05000312253832817\n",
      "Epoch 19109/30000 Training Loss: 0.04346756637096405\n",
      "Epoch 19110/30000 Training Loss: 0.039786696434020996\n",
      "Epoch 19111/30000 Training Loss: 0.06548745930194855\n",
      "Epoch 19112/30000 Training Loss: 0.041345298290252686\n",
      "Epoch 19113/30000 Training Loss: 0.03920101374387741\n",
      "Epoch 19114/30000 Training Loss: 0.03947329521179199\n",
      "Epoch 19115/30000 Training Loss: 0.03367605060338974\n",
      "Epoch 19116/30000 Training Loss: 0.03559616953134537\n",
      "Epoch 19117/30000 Training Loss: 0.04114692658185959\n",
      "Epoch 19118/30000 Training Loss: 0.040153101086616516\n",
      "Epoch 19119/30000 Training Loss: 0.05000265687704086\n",
      "Epoch 19120/30000 Training Loss: 0.04230254516005516\n",
      "Epoch 19121/30000 Training Loss: 0.04441027343273163\n",
      "Epoch 19122/30000 Training Loss: 0.04791378974914551\n",
      "Epoch 19123/30000 Training Loss: 0.04062949866056442\n",
      "Epoch 19124/30000 Training Loss: 0.03590259701013565\n",
      "Epoch 19125/30000 Training Loss: 0.0406867116689682\n",
      "Epoch 19126/30000 Training Loss: 0.0612221285700798\n",
      "Epoch 19127/30000 Training Loss: 0.03667401522397995\n",
      "Epoch 19128/30000 Training Loss: 0.04660128802061081\n",
      "Epoch 19129/30000 Training Loss: 0.03504689782857895\n",
      "Epoch 19130/30000 Training Loss: 0.044501446187496185\n",
      "Epoch 19131/30000 Training Loss: 0.04413909465074539\n",
      "Epoch 19132/30000 Training Loss: 0.054018642753362656\n",
      "Epoch 19133/30000 Training Loss: 0.039781518280506134\n",
      "Epoch 19134/30000 Training Loss: 0.039970315992832184\n",
      "Epoch 19135/30000 Training Loss: 0.03902195021510124\n",
      "Epoch 19136/30000 Training Loss: 0.04525509476661682\n",
      "Epoch 19137/30000 Training Loss: 0.045117009431123734\n",
      "Epoch 19138/30000 Training Loss: 0.04958115518093109\n",
      "Epoch 19139/30000 Training Loss: 0.06379878520965576\n",
      "Epoch 19140/30000 Training Loss: 0.053635645657777786\n",
      "Epoch 19141/30000 Training Loss: 0.03743511810898781\n",
      "Epoch 19142/30000 Training Loss: 0.039767369627952576\n",
      "Epoch 19143/30000 Training Loss: 0.04459289088845253\n",
      "Epoch 19144/30000 Training Loss: 0.07017956674098969\n",
      "Epoch 19145/30000 Training Loss: 0.045323483645915985\n",
      "Epoch 19146/30000 Training Loss: 0.04377444088459015\n",
      "Epoch 19147/30000 Training Loss: 0.045793913304805756\n",
      "Epoch 19148/30000 Training Loss: 0.033519916236400604\n",
      "Epoch 19149/30000 Training Loss: 0.03750721365213394\n",
      "Epoch 19150/30000 Training Loss: 0.046739645302295685\n",
      "Epoch 19151/30000 Training Loss: 0.03901909291744232\n",
      "Epoch 19152/30000 Training Loss: 0.04706800356507301\n",
      "Epoch 19153/30000 Training Loss: 0.053763434290885925\n",
      "Epoch 19154/30000 Training Loss: 0.044155824929475784\n",
      "Epoch 19155/30000 Training Loss: 0.03478790074586868\n",
      "Epoch 19156/30000 Training Loss: 0.05385376513004303\n",
      "Epoch 19157/30000 Training Loss: 0.04858176410198212\n",
      "Epoch 19158/30000 Training Loss: 0.04645054414868355\n",
      "Epoch 19159/30000 Training Loss: 0.04535961151123047\n",
      "Epoch 19160/30000 Training Loss: 0.04166143760085106\n",
      "Epoch 19161/30000 Training Loss: 0.034295693039894104\n",
      "Epoch 19162/30000 Training Loss: 0.04176533222198486\n",
      "Epoch 19163/30000 Training Loss: 0.04198170453310013\n",
      "Epoch 19164/30000 Training Loss: 0.032766446471214294\n",
      "Epoch 19165/30000 Training Loss: 0.032657232135534286\n",
      "Epoch 19166/30000 Training Loss: 0.04678398370742798\n",
      "Epoch 19167/30000 Training Loss: 0.03487345576286316\n",
      "Epoch 19168/30000 Training Loss: 0.048994630575180054\n",
      "Epoch 19169/30000 Training Loss: 0.04121451452374458\n",
      "Epoch 19170/30000 Training Loss: 0.03923676908016205\n",
      "Epoch 19171/30000 Training Loss: 0.03717287629842758\n",
      "Epoch 19172/30000 Training Loss: 0.039566121995449066\n",
      "Epoch 19173/30000 Training Loss: 0.038635723292827606\n",
      "Epoch 19174/30000 Training Loss: 0.05406336486339569\n",
      "Epoch 19175/30000 Training Loss: 0.049034200608730316\n",
      "Epoch 19176/30000 Training Loss: 0.035761699080467224\n",
      "Epoch 19177/30000 Training Loss: 0.03794984892010689\n",
      "Epoch 19178/30000 Training Loss: 0.042290981858968735\n",
      "Epoch 19179/30000 Training Loss: 0.06600528955459595\n",
      "Epoch 19180/30000 Training Loss: 0.04499327018857002\n",
      "Epoch 19181/30000 Training Loss: 0.03723406046628952\n",
      "Epoch 19182/30000 Training Loss: 0.0460677333176136\n",
      "Epoch 19183/30000 Training Loss: 0.03816801309585571\n",
      "Epoch 19184/30000 Training Loss: 0.043369658291339874\n",
      "Epoch 19185/30000 Training Loss: 0.05816485732793808\n",
      "Epoch 19186/30000 Training Loss: 0.05131908506155014\n",
      "Epoch 19187/30000 Training Loss: 0.04157339781522751\n",
      "Epoch 19188/30000 Training Loss: 0.03742334991693497\n",
      "Epoch 19189/30000 Training Loss: 0.04073239117860794\n",
      "Epoch 19190/30000 Training Loss: 0.032861173152923584\n",
      "Epoch 19191/30000 Training Loss: 0.04642195999622345\n",
      "Epoch 19192/30000 Training Loss: 0.04376424103975296\n",
      "Epoch 19193/30000 Training Loss: 0.041947364807128906\n",
      "Epoch 19194/30000 Training Loss: 0.02841239795088768\n",
      "Epoch 19195/30000 Training Loss: 0.043424587696790695\n",
      "Epoch 19196/30000 Training Loss: 0.03909176215529442\n",
      "Epoch 19197/30000 Training Loss: 0.03483542054891586\n",
      "Epoch 19198/30000 Training Loss: 0.05276043713092804\n",
      "Epoch 19199/30000 Training Loss: 0.04505601152777672\n",
      "Epoch 19200/30000 Training Loss: 0.0361604243516922\n",
      "Epoch 19200/30000 Validation Loss: 0.05989055708050728\n",
      "Epoch 19201/30000 Training Loss: 0.033095426857471466\n",
      "Epoch 19202/30000 Training Loss: 0.044469840824604034\n",
      "Epoch 19203/30000 Training Loss: 0.0376080721616745\n",
      "Epoch 19204/30000 Training Loss: 0.05139664560556412\n",
      "Epoch 19205/30000 Training Loss: 0.044589437544345856\n",
      "Epoch 19206/30000 Training Loss: 0.056098245084285736\n",
      "Epoch 19207/30000 Training Loss: 0.049714989960193634\n",
      "Epoch 19208/30000 Training Loss: 0.04394063353538513\n",
      "Epoch 19209/30000 Training Loss: 0.04793359339237213\n",
      "Epoch 19210/30000 Training Loss: 0.04461462423205376\n",
      "Epoch 19211/30000 Training Loss: 0.05380186811089516\n",
      "Epoch 19212/30000 Training Loss: 0.039331745356321335\n",
      "Epoch 19213/30000 Training Loss: 0.03718037158250809\n",
      "Epoch 19214/30000 Training Loss: 0.03225008025765419\n",
      "Epoch 19215/30000 Training Loss: 0.04111407324671745\n",
      "Epoch 19216/30000 Training Loss: 0.04660225659608841\n",
      "Epoch 19217/30000 Training Loss: 0.053271397948265076\n",
      "Epoch 19218/30000 Training Loss: 0.03141292557120323\n",
      "Epoch 19219/30000 Training Loss: 0.029758378863334656\n",
      "Epoch 19220/30000 Training Loss: 0.041736796498298645\n",
      "Epoch 19221/30000 Training Loss: 0.036397531628608704\n",
      "Epoch 19222/30000 Training Loss: 0.030728165060281754\n",
      "Epoch 19223/30000 Training Loss: 0.048620834946632385\n",
      "Epoch 19224/30000 Training Loss: 0.0525037981569767\n",
      "Epoch 19225/30000 Training Loss: 0.041581764817237854\n",
      "Epoch 19226/30000 Training Loss: 0.04482823610305786\n",
      "Epoch 19227/30000 Training Loss: 0.04285465553402901\n",
      "Epoch 19228/30000 Training Loss: 0.04059724509716034\n",
      "Epoch 19229/30000 Training Loss: 0.04244372993707657\n",
      "Epoch 19230/30000 Training Loss: 0.05134976655244827\n",
      "Epoch 19231/30000 Training Loss: 0.042735494673252106\n",
      "Epoch 19232/30000 Training Loss: 0.04001229628920555\n",
      "Epoch 19233/30000 Training Loss: 0.04855737462639809\n",
      "Epoch 19234/30000 Training Loss: 0.032113879919052124\n",
      "Epoch 19235/30000 Training Loss: 0.0413113608956337\n",
      "Epoch 19236/30000 Training Loss: 0.048355091363191605\n",
      "Epoch 19237/30000 Training Loss: 0.05500197410583496\n",
      "Epoch 19238/30000 Training Loss: 0.0418829545378685\n",
      "Epoch 19239/30000 Training Loss: 0.0369051918387413\n",
      "Epoch 19240/30000 Training Loss: 0.04921387508511543\n",
      "Epoch 19241/30000 Training Loss: 0.047337617725133896\n",
      "Epoch 19242/30000 Training Loss: 0.04259940981864929\n",
      "Epoch 19243/30000 Training Loss: 0.032013218849897385\n",
      "Epoch 19244/30000 Training Loss: 0.04076336696743965\n",
      "Epoch 19245/30000 Training Loss: 0.043530941009521484\n",
      "Epoch 19246/30000 Training Loss: 0.04578353464603424\n",
      "Epoch 19247/30000 Training Loss: 0.043341249227523804\n",
      "Epoch 19248/30000 Training Loss: 0.04629235342144966\n",
      "Epoch 19249/30000 Training Loss: 0.03779781609773636\n",
      "Epoch 19250/30000 Training Loss: 0.031329814344644547\n",
      "Epoch 19251/30000 Training Loss: 0.044068995863199234\n",
      "Epoch 19252/30000 Training Loss: 0.04380497336387634\n",
      "Epoch 19253/30000 Training Loss: 0.0386044941842556\n",
      "Epoch 19254/30000 Training Loss: 0.04916346073150635\n",
      "Epoch 19255/30000 Training Loss: 0.04918293282389641\n",
      "Epoch 19256/30000 Training Loss: 0.041441597044467926\n",
      "Epoch 19257/30000 Training Loss: 0.03686200827360153\n",
      "Epoch 19258/30000 Training Loss: 0.04200899600982666\n",
      "Epoch 19259/30000 Training Loss: 0.051022954285144806\n",
      "Epoch 19260/30000 Training Loss: 0.04589461535215378\n",
      "Epoch 19261/30000 Training Loss: 0.04721207916736603\n",
      "Epoch 19262/30000 Training Loss: 0.0486394464969635\n",
      "Epoch 19263/30000 Training Loss: 0.04523319751024246\n",
      "Epoch 19264/30000 Training Loss: 0.04270870238542557\n",
      "Epoch 19265/30000 Training Loss: 0.04038132354617119\n",
      "Epoch 19266/30000 Training Loss: 0.04212252050638199\n",
      "Epoch 19267/30000 Training Loss: 0.037514615803956985\n",
      "Epoch 19268/30000 Training Loss: 0.04284672811627388\n",
      "Epoch 19269/30000 Training Loss: 0.05635663866996765\n",
      "Epoch 19270/30000 Training Loss: 0.04262813180685043\n",
      "Epoch 19271/30000 Training Loss: 0.03802476450800896\n",
      "Epoch 19272/30000 Training Loss: 0.04361268877983093\n",
      "Epoch 19273/30000 Training Loss: 0.064231738448143\n",
      "Epoch 19274/30000 Training Loss: 0.04717950150370598\n",
      "Epoch 19275/30000 Training Loss: 0.04409623146057129\n",
      "Epoch 19276/30000 Training Loss: 0.04035455733537674\n",
      "Epoch 19277/30000 Training Loss: 0.04911506548523903\n",
      "Epoch 19278/30000 Training Loss: 0.042572930455207825\n",
      "Epoch 19279/30000 Training Loss: 0.03943222761154175\n",
      "Epoch 19280/30000 Training Loss: 0.037888940423727036\n",
      "Epoch 19281/30000 Training Loss: 0.038327161222696304\n",
      "Epoch 19282/30000 Training Loss: 0.059376053512096405\n",
      "Epoch 19283/30000 Training Loss: 0.03519119322299957\n",
      "Epoch 19284/30000 Training Loss: 0.050909969955682755\n",
      "Epoch 19285/30000 Training Loss: 0.05309504270553589\n",
      "Epoch 19286/30000 Training Loss: 0.03383622318506241\n",
      "Epoch 19287/30000 Training Loss: 0.04969567805528641\n",
      "Epoch 19288/30000 Training Loss: 0.050407763570547104\n",
      "Epoch 19289/30000 Training Loss: 0.0408988855779171\n",
      "Epoch 19290/30000 Training Loss: 0.06875775754451752\n",
      "Epoch 19291/30000 Training Loss: 0.04891209304332733\n",
      "Epoch 19292/30000 Training Loss: 0.0305340476334095\n",
      "Epoch 19293/30000 Training Loss: 0.035077158361673355\n",
      "Epoch 19294/30000 Training Loss: 0.04727888107299805\n",
      "Epoch 19295/30000 Training Loss: 0.05190823972225189\n",
      "Epoch 19296/30000 Training Loss: 0.041600391268730164\n",
      "Epoch 19297/30000 Training Loss: 0.043372444808483124\n",
      "Epoch 19298/30000 Training Loss: 0.05425020307302475\n",
      "Epoch 19299/30000 Training Loss: 0.06532260030508041\n",
      "Epoch 19300/30000 Training Loss: 0.04463508725166321\n",
      "Epoch 19300/30000 Validation Loss: 0.04862956702709198\n",
      "Epoch 19301/30000 Training Loss: 0.046824418008327484\n",
      "Epoch 19302/30000 Training Loss: 0.04333806410431862\n",
      "Epoch 19303/30000 Training Loss: 0.06191109120845795\n",
      "Epoch 19304/30000 Training Loss: 0.03624457120895386\n",
      "Epoch 19305/30000 Training Loss: 0.05516686290502548\n",
      "Epoch 19306/30000 Training Loss: 0.03624313324689865\n",
      "Epoch 19307/30000 Training Loss: 0.037205666303634644\n",
      "Epoch 19308/30000 Training Loss: 0.04268646240234375\n",
      "Epoch 19309/30000 Training Loss: 0.04343760386109352\n",
      "Epoch 19310/30000 Training Loss: 0.0453244149684906\n",
      "Epoch 19311/30000 Training Loss: 0.034582946449518204\n",
      "Epoch 19312/30000 Training Loss: 0.03755652531981468\n",
      "Epoch 19313/30000 Training Loss: 0.060694292187690735\n",
      "Epoch 19314/30000 Training Loss: 0.035209573805332184\n",
      "Epoch 19315/30000 Training Loss: 0.029746098443865776\n",
      "Epoch 19316/30000 Training Loss: 0.054532039910554886\n",
      "Epoch 19317/30000 Training Loss: 0.037633102387189865\n",
      "Epoch 19318/30000 Training Loss: 0.04207265377044678\n",
      "Epoch 19319/30000 Training Loss: 0.0429767481982708\n",
      "Epoch 19320/30000 Training Loss: 0.05600510537624359\n",
      "Epoch 19321/30000 Training Loss: 0.04135150462388992\n",
      "Epoch 19322/30000 Training Loss: 0.03569608926773071\n",
      "Epoch 19323/30000 Training Loss: 0.056118160486221313\n",
      "Epoch 19324/30000 Training Loss: 0.05423649027943611\n",
      "Epoch 19325/30000 Training Loss: 0.038599155843257904\n",
      "Epoch 19326/30000 Training Loss: 0.045610565692186356\n",
      "Epoch 19327/30000 Training Loss: 0.050482071936130524\n",
      "Epoch 19328/30000 Training Loss: 0.034828636795282364\n",
      "Epoch 19329/30000 Training Loss: 0.03784079849720001\n",
      "Epoch 19330/30000 Training Loss: 0.04511651024222374\n",
      "Epoch 19331/30000 Training Loss: 0.06699071824550629\n",
      "Epoch 19332/30000 Training Loss: 0.0362749882042408\n",
      "Epoch 19333/30000 Training Loss: 0.040673185139894485\n",
      "Epoch 19334/30000 Training Loss: 0.05673115700483322\n",
      "Epoch 19335/30000 Training Loss: 0.04725944250822067\n",
      "Epoch 19336/30000 Training Loss: 0.04868483915925026\n",
      "Epoch 19337/30000 Training Loss: 0.03980308026075363\n",
      "Epoch 19338/30000 Training Loss: 0.046976953744888306\n",
      "Epoch 19339/30000 Training Loss: 0.04607265070080757\n",
      "Epoch 19340/30000 Training Loss: 0.04794277250766754\n",
      "Epoch 19341/30000 Training Loss: 0.05272946506738663\n",
      "Epoch 19342/30000 Training Loss: 0.036435700953006744\n",
      "Epoch 19343/30000 Training Loss: 0.03667490556836128\n",
      "Epoch 19344/30000 Training Loss: 0.041907161474227905\n",
      "Epoch 19345/30000 Training Loss: 0.03673691302537918\n",
      "Epoch 19346/30000 Training Loss: 0.05153616517782211\n",
      "Epoch 19347/30000 Training Loss: 0.055774979293346405\n",
      "Epoch 19348/30000 Training Loss: 0.05482622608542442\n",
      "Epoch 19349/30000 Training Loss: 0.04324696958065033\n",
      "Epoch 19350/30000 Training Loss: 0.05244123935699463\n",
      "Epoch 19351/30000 Training Loss: 0.05093803256750107\n",
      "Epoch 19352/30000 Training Loss: 0.051919061690568924\n",
      "Epoch 19353/30000 Training Loss: 0.041711341589689255\n",
      "Epoch 19354/30000 Training Loss: 0.049277421087026596\n",
      "Epoch 19355/30000 Training Loss: 0.04954807832837105\n",
      "Epoch 19356/30000 Training Loss: 0.043471284210681915\n",
      "Epoch 19357/30000 Training Loss: 0.05906623601913452\n",
      "Epoch 19358/30000 Training Loss: 0.03951399028301239\n",
      "Epoch 19359/30000 Training Loss: 0.052868448197841644\n",
      "Epoch 19360/30000 Training Loss: 0.05510205775499344\n",
      "Epoch 19361/30000 Training Loss: 0.032474156469106674\n",
      "Epoch 19362/30000 Training Loss: 0.056481633335351944\n",
      "Epoch 19363/30000 Training Loss: 0.04563087224960327\n",
      "Epoch 19364/30000 Training Loss: 0.04689793288707733\n",
      "Epoch 19365/30000 Training Loss: 0.04333432391285896\n",
      "Epoch 19366/30000 Training Loss: 0.04069238156080246\n",
      "Epoch 19367/30000 Training Loss: 0.029389575123786926\n",
      "Epoch 19368/30000 Training Loss: 0.03467551991343498\n",
      "Epoch 19369/30000 Training Loss: 0.05976670980453491\n",
      "Epoch 19370/30000 Training Loss: 0.061898358166217804\n",
      "Epoch 19371/30000 Training Loss: 0.04377860948443413\n",
      "Epoch 19372/30000 Training Loss: 0.05683349072933197\n",
      "Epoch 19373/30000 Training Loss: 0.04801369085907936\n",
      "Epoch 19374/30000 Training Loss: 0.04475346952676773\n",
      "Epoch 19375/30000 Training Loss: 0.03346290439367294\n",
      "Epoch 19376/30000 Training Loss: 0.04790199548006058\n",
      "Epoch 19377/30000 Training Loss: 0.04922965168952942\n",
      "Epoch 19378/30000 Training Loss: 0.03963848203420639\n",
      "Epoch 19379/30000 Training Loss: 0.04916050285100937\n",
      "Epoch 19380/30000 Training Loss: 0.03792987018823624\n",
      "Epoch 19381/30000 Training Loss: 0.03908358886837959\n",
      "Epoch 19382/30000 Training Loss: 0.04450172185897827\n",
      "Epoch 19383/30000 Training Loss: 0.042502932250499725\n",
      "Epoch 19384/30000 Training Loss: 0.0563574880361557\n",
      "Epoch 19385/30000 Training Loss: 0.051691580563783646\n",
      "Epoch 19386/30000 Training Loss: 0.04302806407213211\n",
      "Epoch 19387/30000 Training Loss: 0.036158181726932526\n",
      "Epoch 19388/30000 Training Loss: 0.03313172608613968\n",
      "Epoch 19389/30000 Training Loss: 0.05218878388404846\n",
      "Epoch 19390/30000 Training Loss: 0.03458670526742935\n",
      "Epoch 19391/30000 Training Loss: 0.05285203456878662\n",
      "Epoch 19392/30000 Training Loss: 0.03641613945364952\n",
      "Epoch 19393/30000 Training Loss: 0.046818412840366364\n",
      "Epoch 19394/30000 Training Loss: 0.03513820841908455\n",
      "Epoch 19395/30000 Training Loss: 0.03755258768796921\n",
      "Epoch 19396/30000 Training Loss: 0.05115721374750137\n",
      "Epoch 19397/30000 Training Loss: 0.05493146926164627\n",
      "Epoch 19398/30000 Training Loss: 0.04565504565834999\n",
      "Epoch 19399/30000 Training Loss: 0.04012931138277054\n",
      "Epoch 19400/30000 Training Loss: 0.04948284849524498\n",
      "Epoch 19400/30000 Validation Loss: 0.049493759870529175\n",
      "Epoch 19401/30000 Training Loss: 0.028408769518136978\n",
      "Epoch 19402/30000 Training Loss: 0.034008681774139404\n",
      "Epoch 19403/30000 Training Loss: 0.043051816523075104\n",
      "Epoch 19404/30000 Training Loss: 0.04428919404745102\n",
      "Epoch 19405/30000 Training Loss: 0.045763447880744934\n",
      "Epoch 19406/30000 Training Loss: 0.04165475443005562\n",
      "Epoch 19407/30000 Training Loss: 0.03747687488794327\n",
      "Epoch 19408/30000 Training Loss: 0.055235061794519424\n",
      "Epoch 19409/30000 Training Loss: 0.04498770833015442\n",
      "Epoch 19410/30000 Training Loss: 0.0429685078561306\n",
      "Epoch 19411/30000 Training Loss: 0.04803594946861267\n",
      "Epoch 19412/30000 Training Loss: 0.04160495847463608\n",
      "Epoch 19413/30000 Training Loss: 0.05462787300348282\n",
      "Epoch 19414/30000 Training Loss: 0.05646250396966934\n",
      "Epoch 19415/30000 Training Loss: 0.042748238891363144\n",
      "Epoch 19416/30000 Training Loss: 0.04268108308315277\n",
      "Epoch 19417/30000 Training Loss: 0.053501442074775696\n",
      "Epoch 19418/30000 Training Loss: 0.04886171594262123\n",
      "Epoch 19419/30000 Training Loss: 0.046302106231451035\n",
      "Epoch 19420/30000 Training Loss: 0.035419344902038574\n",
      "Epoch 19421/30000 Training Loss: 0.04700171947479248\n",
      "Epoch 19422/30000 Training Loss: 0.04097258672118187\n",
      "Epoch 19423/30000 Training Loss: 0.03985130041837692\n",
      "Epoch 19424/30000 Training Loss: 0.04212668538093567\n",
      "Epoch 19425/30000 Training Loss: 0.04376646503806114\n",
      "Epoch 19426/30000 Training Loss: 0.043290875852108\n",
      "Epoch 19427/30000 Training Loss: 0.03860367834568024\n",
      "Epoch 19428/30000 Training Loss: 0.04824037849903107\n",
      "Epoch 19429/30000 Training Loss: 0.06024486571550369\n",
      "Epoch 19430/30000 Training Loss: 0.041656672954559326\n",
      "Epoch 19431/30000 Training Loss: 0.03552468121051788\n",
      "Epoch 19432/30000 Training Loss: 0.045321062207221985\n",
      "Epoch 19433/30000 Training Loss: 0.04547552019357681\n",
      "Epoch 19434/30000 Training Loss: 0.0388348363339901\n",
      "Epoch 19435/30000 Training Loss: 0.03579995036125183\n",
      "Epoch 19436/30000 Training Loss: 0.03525102883577347\n",
      "Epoch 19437/30000 Training Loss: 0.0433495007455349\n",
      "Epoch 19438/30000 Training Loss: 0.040139347314834595\n",
      "Epoch 19439/30000 Training Loss: 0.044786639511585236\n",
      "Epoch 19440/30000 Training Loss: 0.051587048918008804\n",
      "Epoch 19441/30000 Training Loss: 0.04034469276666641\n",
      "Epoch 19442/30000 Training Loss: 0.04570604860782623\n",
      "Epoch 19443/30000 Training Loss: 0.05504731461405754\n",
      "Epoch 19444/30000 Training Loss: 0.04708819091320038\n",
      "Epoch 19445/30000 Training Loss: 0.037666529417037964\n",
      "Epoch 19446/30000 Training Loss: 0.04917747899889946\n",
      "Epoch 19447/30000 Training Loss: 0.046670276671648026\n",
      "Epoch 19448/30000 Training Loss: 0.050168149173259735\n",
      "Epoch 19449/30000 Training Loss: 0.0447072833776474\n",
      "Epoch 19450/30000 Training Loss: 0.03634076192975044\n",
      "Epoch 19451/30000 Training Loss: 0.03616189584136009\n",
      "Epoch 19452/30000 Training Loss: 0.04687422513961792\n",
      "Epoch 19453/30000 Training Loss: 0.03493556007742882\n",
      "Epoch 19454/30000 Training Loss: 0.044746033847332\n",
      "Epoch 19455/30000 Training Loss: 0.06507959961891174\n",
      "Epoch 19456/30000 Training Loss: 0.055180393159389496\n",
      "Epoch 19457/30000 Training Loss: 0.0373198464512825\n",
      "Epoch 19458/30000 Training Loss: 0.03700051084160805\n",
      "Epoch 19459/30000 Training Loss: 0.04534957557916641\n",
      "Epoch 19460/30000 Training Loss: 0.04892318695783615\n",
      "Epoch 19461/30000 Training Loss: 0.04141231253743172\n",
      "Epoch 19462/30000 Training Loss: 0.07410217821598053\n",
      "Epoch 19463/30000 Training Loss: 0.04814741387963295\n",
      "Epoch 19464/30000 Training Loss: 0.028092768043279648\n",
      "Epoch 19465/30000 Training Loss: 0.04323497414588928\n",
      "Epoch 19466/30000 Training Loss: 0.04744039848446846\n",
      "Epoch 19467/30000 Training Loss: 0.038490936160087585\n",
      "Epoch 19468/30000 Training Loss: 0.06017643213272095\n",
      "Epoch 19469/30000 Training Loss: 0.036698661744594574\n",
      "Epoch 19470/30000 Training Loss: 0.045496921986341476\n",
      "Epoch 19471/30000 Training Loss: 0.03914244845509529\n",
      "Epoch 19472/30000 Training Loss: 0.039861951023340225\n",
      "Epoch 19473/30000 Training Loss: 0.046042900532484055\n",
      "Epoch 19474/30000 Training Loss: 0.05945710092782974\n",
      "Epoch 19475/30000 Training Loss: 0.05351324751973152\n",
      "Epoch 19476/30000 Training Loss: 0.035242415964603424\n",
      "Epoch 19477/30000 Training Loss: 0.04468199983239174\n",
      "Epoch 19478/30000 Training Loss: 0.039422180503606796\n",
      "Epoch 19479/30000 Training Loss: 0.037601396441459656\n",
      "Epoch 19480/30000 Training Loss: 0.04575887322425842\n",
      "Epoch 19481/30000 Training Loss: 0.04779352992773056\n",
      "Epoch 19482/30000 Training Loss: 0.0520271398127079\n",
      "Epoch 19483/30000 Training Loss: 0.03584790602326393\n",
      "Epoch 19484/30000 Training Loss: 0.0339067280292511\n",
      "Epoch 19485/30000 Training Loss: 0.04025283455848694\n",
      "Epoch 19486/30000 Training Loss: 0.048000138252973557\n",
      "Epoch 19487/30000 Training Loss: 0.05842668563127518\n",
      "Epoch 19488/30000 Training Loss: 0.03593924641609192\n",
      "Epoch 19489/30000 Training Loss: 0.04039987921714783\n",
      "Epoch 19490/30000 Training Loss: 0.0403115451335907\n",
      "Epoch 19491/30000 Training Loss: 0.03896457701921463\n",
      "Epoch 19492/30000 Training Loss: 0.045103780925273895\n",
      "Epoch 19493/30000 Training Loss: 0.04936910420656204\n",
      "Epoch 19494/30000 Training Loss: 0.059200868010520935\n",
      "Epoch 19495/30000 Training Loss: 0.0328512005507946\n",
      "Epoch 19496/30000 Training Loss: 0.04438731074333191\n",
      "Epoch 19497/30000 Training Loss: 0.06146520376205444\n",
      "Epoch 19498/30000 Training Loss: 0.03571183234453201\n",
      "Epoch 19499/30000 Training Loss: 0.04578812047839165\n",
      "Epoch 19500/30000 Training Loss: 0.034141093492507935\n",
      "Epoch 19500/30000 Validation Loss: 0.04272919520735741\n",
      "Epoch 19501/30000 Training Loss: 0.04120998829603195\n",
      "Epoch 19502/30000 Training Loss: 0.04111309349536896\n",
      "Epoch 19503/30000 Training Loss: 0.04226672649383545\n",
      "Epoch 19504/30000 Training Loss: 0.04042883962392807\n",
      "Epoch 19505/30000 Training Loss: 0.03485547751188278\n",
      "Epoch 19506/30000 Training Loss: 0.03865306079387665\n",
      "Epoch 19507/30000 Training Loss: 0.03985168784856796\n",
      "Epoch 19508/30000 Training Loss: 0.04039316624403\n",
      "Epoch 19509/30000 Training Loss: 0.051640577614307404\n",
      "Epoch 19510/30000 Training Loss: 0.04369240999221802\n",
      "Epoch 19511/30000 Training Loss: 0.02914082072675228\n",
      "Epoch 19512/30000 Training Loss: 0.04738307371735573\n",
      "Epoch 19513/30000 Training Loss: 0.03608708828687668\n",
      "Epoch 19514/30000 Training Loss: 0.04743209481239319\n",
      "Epoch 19515/30000 Training Loss: 0.03417281433939934\n",
      "Epoch 19516/30000 Training Loss: 0.05813156068325043\n",
      "Epoch 19517/30000 Training Loss: 0.05309460684657097\n",
      "Epoch 19518/30000 Training Loss: 0.03894437104463577\n",
      "Epoch 19519/30000 Training Loss: 0.03132740035653114\n",
      "Epoch 19520/30000 Training Loss: 0.04460081458091736\n",
      "Epoch 19521/30000 Training Loss: 0.049671098589897156\n",
      "Epoch 19522/30000 Training Loss: 0.041205041110515594\n",
      "Epoch 19523/30000 Training Loss: 0.04289725050330162\n",
      "Epoch 19524/30000 Training Loss: 0.0469600185751915\n",
      "Epoch 19525/30000 Training Loss: 0.04642163962125778\n",
      "Epoch 19526/30000 Training Loss: 0.042945366352796555\n",
      "Epoch 19527/30000 Training Loss: 0.04202090948820114\n",
      "Epoch 19528/30000 Training Loss: 0.053433045744895935\n",
      "Epoch 19529/30000 Training Loss: 0.031065326184034348\n",
      "Epoch 19530/30000 Training Loss: 0.04852931946516037\n",
      "Epoch 19531/30000 Training Loss: 0.05011283606290817\n",
      "Epoch 19532/30000 Training Loss: 0.04631323739886284\n",
      "Epoch 19533/30000 Training Loss: 0.04675421863794327\n",
      "Epoch 19534/30000 Training Loss: 0.038407035171985626\n",
      "Epoch 19535/30000 Training Loss: 0.03825750946998596\n",
      "Epoch 19536/30000 Training Loss: 0.05061619356274605\n",
      "Epoch 19537/30000 Training Loss: 0.03834366053342819\n",
      "Epoch 19538/30000 Training Loss: 0.03160647675395012\n",
      "Epoch 19539/30000 Training Loss: 0.035288259387016296\n",
      "Epoch 19540/30000 Training Loss: 0.041981782764196396\n",
      "Epoch 19541/30000 Training Loss: 0.04709593206644058\n",
      "Epoch 19542/30000 Training Loss: 0.05487898737192154\n",
      "Epoch 19543/30000 Training Loss: 0.046102095395326614\n",
      "Epoch 19544/30000 Training Loss: 0.03376546874642372\n",
      "Epoch 19545/30000 Training Loss: 0.04160163551568985\n",
      "Epoch 19546/30000 Training Loss: 0.02688128873705864\n",
      "Epoch 19547/30000 Training Loss: 0.039769068360328674\n",
      "Epoch 19548/30000 Training Loss: 0.0459844172000885\n",
      "Epoch 19549/30000 Training Loss: 0.03766781464219093\n",
      "Epoch 19550/30000 Training Loss: 0.03790511563420296\n",
      "Epoch 19551/30000 Training Loss: 0.0701231062412262\n",
      "Epoch 19552/30000 Training Loss: 0.048100292682647705\n",
      "Epoch 19553/30000 Training Loss: 0.050711534917354584\n",
      "Epoch 19554/30000 Training Loss: 0.033529240638017654\n",
      "Epoch 19555/30000 Training Loss: 0.045667119324207306\n",
      "Epoch 19556/30000 Training Loss: 0.05382596701383591\n",
      "Epoch 19557/30000 Training Loss: 0.03619038313627243\n",
      "Epoch 19558/30000 Training Loss: 0.053806185722351074\n",
      "Epoch 19559/30000 Training Loss: 0.029542770236730576\n",
      "Epoch 19560/30000 Training Loss: 0.048013389110565186\n",
      "Epoch 19561/30000 Training Loss: 0.04713726043701172\n",
      "Epoch 19562/30000 Training Loss: 0.044118378311395645\n",
      "Epoch 19563/30000 Training Loss: 0.049032025039196014\n",
      "Epoch 19564/30000 Training Loss: 0.037914298474788666\n",
      "Epoch 19565/30000 Training Loss: 0.037071242928504944\n",
      "Epoch 19566/30000 Training Loss: 0.0396241620182991\n",
      "Epoch 19567/30000 Training Loss: 0.03522849828004837\n",
      "Epoch 19568/30000 Training Loss: 0.05689018964767456\n",
      "Epoch 19569/30000 Training Loss: 0.04887263476848602\n",
      "Epoch 19570/30000 Training Loss: 0.049030013382434845\n",
      "Epoch 19571/30000 Training Loss: 0.050819315016269684\n",
      "Epoch 19572/30000 Training Loss: 0.047103554010391235\n",
      "Epoch 19573/30000 Training Loss: 0.04144320636987686\n",
      "Epoch 19574/30000 Training Loss: 0.03489715978503227\n",
      "Epoch 19575/30000 Training Loss: 0.045447465032339096\n",
      "Epoch 19576/30000 Training Loss: 0.043917156755924225\n",
      "Epoch 19577/30000 Training Loss: 0.05258934944868088\n",
      "Epoch 19578/30000 Training Loss: 0.04221179336309433\n",
      "Epoch 19579/30000 Training Loss: 0.04335065186023712\n",
      "Epoch 19580/30000 Training Loss: 0.04252146556973457\n",
      "Epoch 19581/30000 Training Loss: 0.04300646483898163\n",
      "Epoch 19582/30000 Training Loss: 0.04341289401054382\n",
      "Epoch 19583/30000 Training Loss: 0.03837554156780243\n",
      "Epoch 19584/30000 Training Loss: 0.037871286273002625\n",
      "Epoch 19585/30000 Training Loss: 0.04279569536447525\n",
      "Epoch 19586/30000 Training Loss: 0.042881645262241364\n",
      "Epoch 19587/30000 Training Loss: 0.03076213225722313\n",
      "Epoch 19588/30000 Training Loss: 0.053668104112148285\n",
      "Epoch 19589/30000 Training Loss: 0.03813741356134415\n",
      "Epoch 19590/30000 Training Loss: 0.05208272859454155\n",
      "Epoch 19591/30000 Training Loss: 0.038660090416669846\n",
      "Epoch 19592/30000 Training Loss: 0.0393863171339035\n",
      "Epoch 19593/30000 Training Loss: 0.030179139226675034\n",
      "Epoch 19594/30000 Training Loss: 0.0498572438955307\n",
      "Epoch 19595/30000 Training Loss: 0.03501083329319954\n",
      "Epoch 19596/30000 Training Loss: 0.050981078296899796\n",
      "Epoch 19597/30000 Training Loss: 0.04547294229269028\n",
      "Epoch 19598/30000 Training Loss: 0.04333788901567459\n",
      "Epoch 19599/30000 Training Loss: 0.04448014497756958\n",
      "Epoch 19600/30000 Training Loss: 0.0403134860098362\n",
      "Epoch 19600/30000 Validation Loss: 0.038241732865571976\n",
      "Epoch 19601/30000 Training Loss: 0.038748033344745636\n",
      "Epoch 19602/30000 Training Loss: 0.058902837336063385\n",
      "Epoch 19603/30000 Training Loss: 0.042250506579875946\n",
      "Epoch 19604/30000 Training Loss: 0.05230409651994705\n",
      "Epoch 19605/30000 Training Loss: 0.03913373500108719\n",
      "Epoch 19606/30000 Training Loss: 0.03788281977176666\n",
      "Epoch 19607/30000 Training Loss: 0.052327461540699005\n",
      "Epoch 19608/30000 Training Loss: 0.04810364544391632\n",
      "Epoch 19609/30000 Training Loss: 0.04424167796969414\n",
      "Epoch 19610/30000 Training Loss: 0.03868171572685242\n",
      "Epoch 19611/30000 Training Loss: 0.055498115718364716\n",
      "Epoch 19612/30000 Training Loss: 0.037634652107954025\n",
      "Epoch 19613/30000 Training Loss: 0.06673495471477509\n",
      "Epoch 19614/30000 Training Loss: 0.048919372260570526\n",
      "Epoch 19615/30000 Training Loss: 0.059301428496837616\n",
      "Epoch 19616/30000 Training Loss: 0.046971507370471954\n",
      "Epoch 19617/30000 Training Loss: 0.05604737251996994\n",
      "Epoch 19618/30000 Training Loss: 0.05404362827539444\n",
      "Epoch 19619/30000 Training Loss: 0.03150966763496399\n",
      "Epoch 19620/30000 Training Loss: 0.05780956521630287\n",
      "Epoch 19621/30000 Training Loss: 0.036186255514621735\n",
      "Epoch 19622/30000 Training Loss: 0.05276203155517578\n",
      "Epoch 19623/30000 Training Loss: 0.03588027507066727\n",
      "Epoch 19624/30000 Training Loss: 0.057109005749225616\n",
      "Epoch 19625/30000 Training Loss: 0.04120928794145584\n",
      "Epoch 19626/30000 Training Loss: 0.03569683060050011\n",
      "Epoch 19627/30000 Training Loss: 0.057381682097911835\n",
      "Epoch 19628/30000 Training Loss: 0.06067610904574394\n",
      "Epoch 19629/30000 Training Loss: 0.0453500896692276\n",
      "Epoch 19630/30000 Training Loss: 0.04332692176103592\n",
      "Epoch 19631/30000 Training Loss: 0.05219656229019165\n",
      "Epoch 19632/30000 Training Loss: 0.04325048252940178\n",
      "Epoch 19633/30000 Training Loss: 0.0415227897465229\n",
      "Epoch 19634/30000 Training Loss: 0.053894639015197754\n",
      "Epoch 19635/30000 Training Loss: 0.04455357789993286\n",
      "Epoch 19636/30000 Training Loss: 0.05427779257297516\n",
      "Epoch 19637/30000 Training Loss: 0.0496632382273674\n",
      "Epoch 19638/30000 Training Loss: 0.04279834032058716\n",
      "Epoch 19639/30000 Training Loss: 0.04144943132996559\n",
      "Epoch 19640/30000 Training Loss: 0.044740498065948486\n",
      "Epoch 19641/30000 Training Loss: 0.04354926943778992\n",
      "Epoch 19642/30000 Training Loss: 0.054528653621673584\n",
      "Epoch 19643/30000 Training Loss: 0.0373527854681015\n",
      "Epoch 19644/30000 Training Loss: 0.03964785858988762\n",
      "Epoch 19645/30000 Training Loss: 0.04266202077269554\n",
      "Epoch 19646/30000 Training Loss: 0.03808382898569107\n",
      "Epoch 19647/30000 Training Loss: 0.04129965975880623\n",
      "Epoch 19648/30000 Training Loss: 0.03684411942958832\n",
      "Epoch 19649/30000 Training Loss: 0.045610807836055756\n",
      "Epoch 19650/30000 Training Loss: 0.04051760584115982\n",
      "Epoch 19651/30000 Training Loss: 0.04339656978845596\n",
      "Epoch 19652/30000 Training Loss: 0.057645902037620544\n",
      "Epoch 19653/30000 Training Loss: 0.044608455151319504\n",
      "Epoch 19654/30000 Training Loss: 0.05354202538728714\n",
      "Epoch 19655/30000 Training Loss: 0.04803901165723801\n",
      "Epoch 19656/30000 Training Loss: 0.036971695721149445\n",
      "Epoch 19657/30000 Training Loss: 0.04642317816615105\n",
      "Epoch 19658/30000 Training Loss: 0.03444167971611023\n",
      "Epoch 19659/30000 Training Loss: 0.03739289194345474\n",
      "Epoch 19660/30000 Training Loss: 0.041880398988723755\n",
      "Epoch 19661/30000 Training Loss: 0.0597776360809803\n",
      "Epoch 19662/30000 Training Loss: 0.03903549537062645\n",
      "Epoch 19663/30000 Training Loss: 0.039650142192840576\n",
      "Epoch 19664/30000 Training Loss: 0.04442750662565231\n",
      "Epoch 19665/30000 Training Loss: 0.05133552849292755\n",
      "Epoch 19666/30000 Training Loss: 0.044230569154024124\n",
      "Epoch 19667/30000 Training Loss: 0.04459749907255173\n",
      "Epoch 19668/30000 Training Loss: 0.05436117202043533\n",
      "Epoch 19669/30000 Training Loss: 0.056389957666397095\n",
      "Epoch 19670/30000 Training Loss: 0.040895551443099976\n",
      "Epoch 19671/30000 Training Loss: 0.03189655393362045\n",
      "Epoch 19672/30000 Training Loss: 0.045271776616573334\n",
      "Epoch 19673/30000 Training Loss: 0.04543759301304817\n",
      "Epoch 19674/30000 Training Loss: 0.04424884915351868\n",
      "Epoch 19675/30000 Training Loss: 0.04233980178833008\n",
      "Epoch 19676/30000 Training Loss: 0.046495161950588226\n",
      "Epoch 19677/30000 Training Loss: 0.04431628808379173\n",
      "Epoch 19678/30000 Training Loss: 0.04583941400051117\n",
      "Epoch 19679/30000 Training Loss: 0.04138501361012459\n",
      "Epoch 19680/30000 Training Loss: 0.04795568063855171\n",
      "Epoch 19681/30000 Training Loss: 0.05634433031082153\n",
      "Epoch 19682/30000 Training Loss: 0.046556875109672546\n",
      "Epoch 19683/30000 Training Loss: 0.035893023014068604\n",
      "Epoch 19684/30000 Training Loss: 0.039711564779281616\n",
      "Epoch 19685/30000 Training Loss: 0.049200549721717834\n",
      "Epoch 19686/30000 Training Loss: 0.0567338727414608\n",
      "Epoch 19687/30000 Training Loss: 0.045617349445819855\n",
      "Epoch 19688/30000 Training Loss: 0.031042348593473434\n",
      "Epoch 19689/30000 Training Loss: 0.04470982030034065\n",
      "Epoch 19690/30000 Training Loss: 0.044521089643239975\n",
      "Epoch 19691/30000 Training Loss: 0.03775458782911301\n",
      "Epoch 19692/30000 Training Loss: 0.0441417396068573\n",
      "Epoch 19693/30000 Training Loss: 0.05545356124639511\n",
      "Epoch 19694/30000 Training Loss: 0.05057774484157562\n",
      "Epoch 19695/30000 Training Loss: 0.04608394205570221\n",
      "Epoch 19696/30000 Training Loss: 0.05242107808589935\n",
      "Epoch 19697/30000 Training Loss: 0.0349578782916069\n",
      "Epoch 19698/30000 Training Loss: 0.04032113030552864\n",
      "Epoch 19699/30000 Training Loss: 0.044067710638046265\n",
      "Epoch 19700/30000 Training Loss: 0.04982532560825348\n",
      "Epoch 19700/30000 Validation Loss: 0.0484185554087162\n",
      "Epoch 19701/30000 Training Loss: 0.04657967388629913\n",
      "Epoch 19702/30000 Training Loss: 0.039943769574165344\n",
      "Epoch 19703/30000 Training Loss: 0.03791879117488861\n",
      "Epoch 19704/30000 Training Loss: 0.04446442797780037\n",
      "Epoch 19705/30000 Training Loss: 0.06398043036460876\n",
      "Epoch 19706/30000 Training Loss: 0.05003954470157623\n",
      "Epoch 19707/30000 Training Loss: 0.042547740042209625\n",
      "Epoch 19708/30000 Training Loss: 0.057848505675792694\n",
      "Epoch 19709/30000 Training Loss: 0.04459928721189499\n",
      "Epoch 19710/30000 Training Loss: 0.04408331587910652\n",
      "Epoch 19711/30000 Training Loss: 0.0495077408850193\n",
      "Epoch 19712/30000 Training Loss: 0.041368644684553146\n",
      "Epoch 19713/30000 Training Loss: 0.04224512353539467\n",
      "Epoch 19714/30000 Training Loss: 0.05539613962173462\n",
      "Epoch 19715/30000 Training Loss: 0.041692718863487244\n",
      "Epoch 19716/30000 Training Loss: 0.03414482995867729\n",
      "Epoch 19717/30000 Training Loss: 0.03353717923164368\n",
      "Epoch 19718/30000 Training Loss: 0.04528823122382164\n",
      "Epoch 19719/30000 Training Loss: 0.051174141466617584\n",
      "Epoch 19720/30000 Training Loss: 0.04926237836480141\n",
      "Epoch 19721/30000 Training Loss: 0.050682779401540756\n",
      "Epoch 19722/30000 Training Loss: 0.04611732065677643\n",
      "Epoch 19723/30000 Training Loss: 0.0338209830224514\n",
      "Epoch 19724/30000 Training Loss: 0.05216516554355621\n",
      "Epoch 19725/30000 Training Loss: 0.05098262056708336\n",
      "Epoch 19726/30000 Training Loss: 0.043793901801109314\n",
      "Epoch 19727/30000 Training Loss: 0.04321664571762085\n",
      "Epoch 19728/30000 Training Loss: 0.039603717625141144\n",
      "Epoch 19729/30000 Training Loss: 0.04384313151240349\n",
      "Epoch 19730/30000 Training Loss: 0.042410366237163544\n",
      "Epoch 19731/30000 Training Loss: 0.043679919093847275\n",
      "Epoch 19732/30000 Training Loss: 0.056159477680921555\n",
      "Epoch 19733/30000 Training Loss: 0.030146518722176552\n",
      "Epoch 19734/30000 Training Loss: 0.04979388415813446\n",
      "Epoch 19735/30000 Training Loss: 0.03988220915198326\n",
      "Epoch 19736/30000 Training Loss: 0.04894572123885155\n",
      "Epoch 19737/30000 Training Loss: 0.03830989822745323\n",
      "Epoch 19738/30000 Training Loss: 0.05920045077800751\n",
      "Epoch 19739/30000 Training Loss: 0.050194576382637024\n",
      "Epoch 19740/30000 Training Loss: 0.0475800447165966\n",
      "Epoch 19741/30000 Training Loss: 0.050282880663871765\n",
      "Epoch 19742/30000 Training Loss: 0.03216062858700752\n",
      "Epoch 19743/30000 Training Loss: 0.053372375667095184\n",
      "Epoch 19744/30000 Training Loss: 0.03637312725186348\n",
      "Epoch 19745/30000 Training Loss: 0.05137183517217636\n",
      "Epoch 19746/30000 Training Loss: 0.03314722701907158\n",
      "Epoch 19747/30000 Training Loss: 0.02819552645087242\n",
      "Epoch 19748/30000 Training Loss: 0.06014195829629898\n",
      "Epoch 19749/30000 Training Loss: 0.04177446663379669\n",
      "Epoch 19750/30000 Training Loss: 0.04346349835395813\n",
      "Epoch 19751/30000 Training Loss: 0.05229448899626732\n",
      "Epoch 19752/30000 Training Loss: 0.05054819583892822\n",
      "Epoch 19753/30000 Training Loss: 0.034759316593408585\n",
      "Epoch 19754/30000 Training Loss: 0.04733527451753616\n",
      "Epoch 19755/30000 Training Loss: 0.04924926161766052\n",
      "Epoch 19756/30000 Training Loss: 0.03854994475841522\n",
      "Epoch 19757/30000 Training Loss: 0.03815937042236328\n",
      "Epoch 19758/30000 Training Loss: 0.037598323076963425\n",
      "Epoch 19759/30000 Training Loss: 0.04146208614110947\n",
      "Epoch 19760/30000 Training Loss: 0.04869086295366287\n",
      "Epoch 19761/30000 Training Loss: 0.03734774887561798\n",
      "Epoch 19762/30000 Training Loss: 0.038989897817373276\n",
      "Epoch 19763/30000 Training Loss: 0.05189784616231918\n",
      "Epoch 19764/30000 Training Loss: 0.03888114541769028\n",
      "Epoch 19765/30000 Training Loss: 0.048769377171993256\n",
      "Epoch 19766/30000 Training Loss: 0.04285632073879242\n",
      "Epoch 19767/30000 Training Loss: 0.03264212608337402\n",
      "Epoch 19768/30000 Training Loss: 0.044993508607149124\n",
      "Epoch 19769/30000 Training Loss: 0.04095436632633209\n",
      "Epoch 19770/30000 Training Loss: 0.05972679704427719\n",
      "Epoch 19771/30000 Training Loss: 0.032900094985961914\n",
      "Epoch 19772/30000 Training Loss: 0.04583366960287094\n",
      "Epoch 19773/30000 Training Loss: 0.04769058898091316\n",
      "Epoch 19774/30000 Training Loss: 0.048809465020895004\n",
      "Epoch 19775/30000 Training Loss: 0.04436677694320679\n",
      "Epoch 19776/30000 Training Loss: 0.04602310061454773\n",
      "Epoch 19777/30000 Training Loss: 0.032431185245513916\n",
      "Epoch 19778/30000 Training Loss: 0.04392068088054657\n",
      "Epoch 19779/30000 Training Loss: 0.047169435769319534\n",
      "Epoch 19780/30000 Training Loss: 0.041403189301490784\n",
      "Epoch 19781/30000 Training Loss: 0.04589826986193657\n",
      "Epoch 19782/30000 Training Loss: 0.04484102129936218\n",
      "Epoch 19783/30000 Training Loss: 0.05629303306341171\n",
      "Epoch 19784/30000 Training Loss: 0.04833485186100006\n",
      "Epoch 19785/30000 Training Loss: 0.04231812804937363\n",
      "Epoch 19786/30000 Training Loss: 0.04046376049518585\n",
      "Epoch 19787/30000 Training Loss: 0.04851566627621651\n",
      "Epoch 19788/30000 Training Loss: 0.0438886322081089\n",
      "Epoch 19789/30000 Training Loss: 0.03576694801449776\n",
      "Epoch 19790/30000 Training Loss: 0.03423186019062996\n",
      "Epoch 19791/30000 Training Loss: 0.050790224224328995\n",
      "Epoch 19792/30000 Training Loss: 0.05979594588279724\n",
      "Epoch 19793/30000 Training Loss: 0.038564637303352356\n",
      "Epoch 19794/30000 Training Loss: 0.04127563163638115\n",
      "Epoch 19795/30000 Training Loss: 0.03609449788928032\n",
      "Epoch 19796/30000 Training Loss: 0.033401377499103546\n",
      "Epoch 19797/30000 Training Loss: 0.04483331739902496\n",
      "Epoch 19798/30000 Training Loss: 0.060840506106615067\n",
      "Epoch 19799/30000 Training Loss: 0.03624054789543152\n",
      "Epoch 19800/30000 Training Loss: 0.06623721122741699\n",
      "Epoch 19800/30000 Validation Loss: 0.04447087645530701\n",
      "Epoch 19801/30000 Training Loss: 0.042714159935712814\n",
      "Epoch 19802/30000 Training Loss: 0.04226435720920563\n",
      "Epoch 19803/30000 Training Loss: 0.04963761195540428\n",
      "Epoch 19804/30000 Training Loss: 0.060602061450481415\n",
      "Epoch 19805/30000 Training Loss: 0.051041051745414734\n",
      "Epoch 19806/30000 Training Loss: 0.037241291254758835\n",
      "Epoch 19807/30000 Training Loss: 0.0426473394036293\n",
      "Epoch 19808/30000 Training Loss: 0.0449339784681797\n",
      "Epoch 19809/30000 Training Loss: 0.04127052426338196\n",
      "Epoch 19810/30000 Training Loss: 0.045091912150382996\n",
      "Epoch 19811/30000 Training Loss: 0.03800162672996521\n",
      "Epoch 19812/30000 Training Loss: 0.05634927377104759\n",
      "Epoch 19813/30000 Training Loss: 0.043617334216833115\n",
      "Epoch 19814/30000 Training Loss: 0.045369602739810944\n",
      "Epoch 19815/30000 Training Loss: 0.049184370785951614\n",
      "Epoch 19816/30000 Training Loss: 0.0562707856297493\n",
      "Epoch 19817/30000 Training Loss: 0.044266581535339355\n",
      "Epoch 19818/30000 Training Loss: 0.05154664069414139\n",
      "Epoch 19819/30000 Training Loss: 0.051012199372053146\n",
      "Epoch 19820/30000 Training Loss: 0.028670936822891235\n",
      "Epoch 19821/30000 Training Loss: 0.04797695577144623\n",
      "Epoch 19822/30000 Training Loss: 0.043348412960767746\n",
      "Epoch 19823/30000 Training Loss: 0.040604300796985626\n",
      "Epoch 19824/30000 Training Loss: 0.04496371001005173\n",
      "Epoch 19825/30000 Training Loss: 0.05200806260108948\n",
      "Epoch 19826/30000 Training Loss: 0.03939554840326309\n",
      "Epoch 19827/30000 Training Loss: 0.03885938599705696\n",
      "Epoch 19828/30000 Training Loss: 0.028264841064810753\n",
      "Epoch 19829/30000 Training Loss: 0.04029540345072746\n",
      "Epoch 19830/30000 Training Loss: 0.047256872057914734\n",
      "Epoch 19831/30000 Training Loss: 0.05198318511247635\n",
      "Epoch 19832/30000 Training Loss: 0.05311427265405655\n",
      "Epoch 19833/30000 Training Loss: 0.04256248474121094\n",
      "Epoch 19834/30000 Training Loss: 0.0504784882068634\n",
      "Epoch 19835/30000 Training Loss: 0.04983408749103546\n",
      "Epoch 19836/30000 Training Loss: 0.04286894574761391\n",
      "Epoch 19837/30000 Training Loss: 0.044631168246269226\n",
      "Epoch 19838/30000 Training Loss: 0.040426649153232574\n",
      "Epoch 19839/30000 Training Loss: 0.03520844876766205\n",
      "Epoch 19840/30000 Training Loss: 0.05379357188940048\n",
      "Epoch 19841/30000 Training Loss: 0.03655635565519333\n",
      "Epoch 19842/30000 Training Loss: 0.031951360404491425\n",
      "Epoch 19843/30000 Training Loss: 0.04238983988761902\n",
      "Epoch 19844/30000 Training Loss: 0.041886623948812485\n",
      "Epoch 19845/30000 Training Loss: 0.03816309943795204\n",
      "Epoch 19846/30000 Training Loss: 0.0339578278362751\n",
      "Epoch 19847/30000 Training Loss: 0.048009566962718964\n",
      "Epoch 19848/30000 Training Loss: 0.04346834868192673\n",
      "Epoch 19849/30000 Training Loss: 0.0348924845457077\n",
      "Epoch 19850/30000 Training Loss: 0.0326627679169178\n",
      "Epoch 19851/30000 Training Loss: 0.040458500385284424\n",
      "Epoch 19852/30000 Training Loss: 0.039029404520988464\n",
      "Epoch 19853/30000 Training Loss: 0.027745600789785385\n",
      "Epoch 19854/30000 Training Loss: 0.03956661373376846\n",
      "Epoch 19855/30000 Training Loss: 0.04036526009440422\n",
      "Epoch 19856/30000 Training Loss: 0.04064980894327164\n",
      "Epoch 19857/30000 Training Loss: 0.05391085147857666\n",
      "Epoch 19858/30000 Training Loss: 0.05624888464808464\n",
      "Epoch 19859/30000 Training Loss: 0.04447123408317566\n",
      "Epoch 19860/30000 Training Loss: 0.05128476023674011\n",
      "Epoch 19861/30000 Training Loss: 0.04959966242313385\n",
      "Epoch 19862/30000 Training Loss: 0.06244443356990814\n",
      "Epoch 19863/30000 Training Loss: 0.043610893189907074\n",
      "Epoch 19864/30000 Training Loss: 0.05212036892771721\n",
      "Epoch 19865/30000 Training Loss: 0.03475590795278549\n",
      "Epoch 19866/30000 Training Loss: 0.04585512727499008\n",
      "Epoch 19867/30000 Training Loss: 0.03775068745017052\n",
      "Epoch 19868/30000 Training Loss: 0.036043450236320496\n",
      "Epoch 19869/30000 Training Loss: 0.03409884124994278\n",
      "Epoch 19870/30000 Training Loss: 0.04006301239132881\n",
      "Epoch 19871/30000 Training Loss: 0.042473405599594116\n",
      "Epoch 19872/30000 Training Loss: 0.03840292990207672\n",
      "Epoch 19873/30000 Training Loss: 0.03287708759307861\n",
      "Epoch 19874/30000 Training Loss: 0.04069008678197861\n",
      "Epoch 19875/30000 Training Loss: 0.040596552193164825\n",
      "Epoch 19876/30000 Training Loss: 0.04263113811612129\n",
      "Epoch 19877/30000 Training Loss: 0.038124505430459976\n",
      "Epoch 19878/30000 Training Loss: 0.04235093668103218\n",
      "Epoch 19879/30000 Training Loss: 0.03567606583237648\n",
      "Epoch 19880/30000 Training Loss: 0.06471695005893707\n",
      "Epoch 19881/30000 Training Loss: 0.05818839371204376\n",
      "Epoch 19882/30000 Training Loss: 0.04629361629486084\n",
      "Epoch 19883/30000 Training Loss: 0.04761166870594025\n",
      "Epoch 19884/30000 Training Loss: 0.042248353362083435\n",
      "Epoch 19885/30000 Training Loss: 0.04542620852589607\n",
      "Epoch 19886/30000 Training Loss: 0.04378107190132141\n",
      "Epoch 19887/30000 Training Loss: 0.04563672095537186\n",
      "Epoch 19888/30000 Training Loss: 0.04018562287092209\n",
      "Epoch 19889/30000 Training Loss: 0.039942651987075806\n",
      "Epoch 19890/30000 Training Loss: 0.04742653667926788\n",
      "Epoch 19891/30000 Training Loss: 0.03489448502659798\n",
      "Epoch 19892/30000 Training Loss: 0.03887147456407547\n",
      "Epoch 19893/30000 Training Loss: 0.04748699814081192\n",
      "Epoch 19894/30000 Training Loss: 0.04071727395057678\n",
      "Epoch 19895/30000 Training Loss: 0.029919587075710297\n",
      "Epoch 19896/30000 Training Loss: 0.047260455787181854\n",
      "Epoch 19897/30000 Training Loss: 0.03941518813371658\n",
      "Epoch 19898/30000 Training Loss: 0.04903995245695114\n",
      "Epoch 19899/30000 Training Loss: 0.039325520396232605\n",
      "Epoch 19900/30000 Training Loss: 0.0452878437936306\n",
      "Epoch 19900/30000 Validation Loss: 0.05249428004026413\n",
      "Epoch 19901/30000 Training Loss: 0.03899937495589256\n",
      "Epoch 19902/30000 Training Loss: 0.0505560040473938\n",
      "Epoch 19903/30000 Training Loss: 0.044713594019412994\n",
      "Epoch 19904/30000 Training Loss: 0.032644227147102356\n",
      "Epoch 19905/30000 Training Loss: 0.04798157513141632\n",
      "Epoch 19906/30000 Training Loss: 0.048741668462753296\n",
      "Epoch 19907/30000 Training Loss: 0.06211986765265465\n",
      "Epoch 19908/30000 Training Loss: 0.054327256977558136\n",
      "Epoch 19909/30000 Training Loss: 0.03975558280944824\n",
      "Epoch 19910/30000 Training Loss: 0.03780703246593475\n",
      "Epoch 19911/30000 Training Loss: 0.04901144281029701\n",
      "Epoch 19912/30000 Training Loss: 0.03598814457654953\n",
      "Epoch 19913/30000 Training Loss: 0.05502006411552429\n",
      "Epoch 19914/30000 Training Loss: 0.05727887153625488\n",
      "Epoch 19915/30000 Training Loss: 0.0400308333337307\n",
      "Epoch 19916/30000 Training Loss: 0.036487288773059845\n",
      "Epoch 19917/30000 Training Loss: 0.0510854497551918\n",
      "Epoch 19918/30000 Training Loss: 0.045116864144802094\n",
      "Epoch 19919/30000 Training Loss: 0.03053196147084236\n",
      "Epoch 19920/30000 Training Loss: 0.05633711814880371\n",
      "Epoch 19921/30000 Training Loss: 0.05533765256404877\n",
      "Epoch 19922/30000 Training Loss: 0.04714348167181015\n",
      "Epoch 19923/30000 Training Loss: 0.046154171228408813\n",
      "Epoch 19924/30000 Training Loss: 0.04286547750234604\n",
      "Epoch 19925/30000 Training Loss: 0.043837301433086395\n",
      "Epoch 19926/30000 Training Loss: 0.05396334081888199\n",
      "Epoch 19927/30000 Training Loss: 0.041841864585876465\n",
      "Epoch 19928/30000 Training Loss: 0.061177801340818405\n",
      "Epoch 19929/30000 Training Loss: 0.04838714748620987\n",
      "Epoch 19930/30000 Training Loss: 0.043309032917022705\n",
      "Epoch 19931/30000 Training Loss: 0.04278334975242615\n",
      "Epoch 19932/30000 Training Loss: 0.049928922206163406\n",
      "Epoch 19933/30000 Training Loss: 0.042888812720775604\n",
      "Epoch 19934/30000 Training Loss: 0.052848611027002335\n",
      "Epoch 19935/30000 Training Loss: 0.03699326515197754\n",
      "Epoch 19936/30000 Training Loss: 0.03704503923654556\n",
      "Epoch 19937/30000 Training Loss: 0.03703407943248749\n",
      "Epoch 19938/30000 Training Loss: 0.043252915143966675\n",
      "Epoch 19939/30000 Training Loss: 0.04149005189538002\n",
      "Epoch 19940/30000 Training Loss: 0.04234618693590164\n",
      "Epoch 19941/30000 Training Loss: 0.04291320964694023\n",
      "Epoch 19942/30000 Training Loss: 0.07519891113042831\n",
      "Epoch 19943/30000 Training Loss: 0.04542703926563263\n",
      "Epoch 19944/30000 Training Loss: 0.047025930136442184\n",
      "Epoch 19945/30000 Training Loss: 0.05299855396151543\n",
      "Epoch 19946/30000 Training Loss: 0.05335702374577522\n",
      "Epoch 19947/30000 Training Loss: 0.04264846444129944\n",
      "Epoch 19948/30000 Training Loss: 0.04066251590847969\n",
      "Epoch 19949/30000 Training Loss: 0.05211058259010315\n",
      "Epoch 19950/30000 Training Loss: 0.03977671265602112\n",
      "Epoch 19951/30000 Training Loss: 0.055268295109272\n",
      "Epoch 19952/30000 Training Loss: 0.04875057935714722\n",
      "Epoch 19953/30000 Training Loss: 0.06074557453393936\n",
      "Epoch 19954/30000 Training Loss: 0.03544389083981514\n",
      "Epoch 19955/30000 Training Loss: 0.047598034143447876\n",
      "Epoch 19956/30000 Training Loss: 0.039477523416280746\n",
      "Epoch 19957/30000 Training Loss: 0.04703859984874725\n",
      "Epoch 19958/30000 Training Loss: 0.04637341573834419\n",
      "Epoch 19959/30000 Training Loss: 0.0463760569691658\n",
      "Epoch 19960/30000 Training Loss: 0.04143800213932991\n",
      "Epoch 19961/30000 Training Loss: 0.033534109592437744\n",
      "Epoch 19962/30000 Training Loss: 0.05002095550298691\n",
      "Epoch 19963/30000 Training Loss: 0.04730933904647827\n",
      "Epoch 19964/30000 Training Loss: 0.03703797981142998\n",
      "Epoch 19965/30000 Training Loss: 0.05206335708498955\n",
      "Epoch 19966/30000 Training Loss: 0.04963994771242142\n",
      "Epoch 19967/30000 Training Loss: 0.04191146045923233\n",
      "Epoch 19968/30000 Training Loss: 0.034259021282196045\n",
      "Epoch 19969/30000 Training Loss: 0.047249142080545425\n",
      "Epoch 19970/30000 Training Loss: 0.05777251720428467\n",
      "Epoch 19971/30000 Training Loss: 0.046580664813518524\n",
      "Epoch 19972/30000 Training Loss: 0.04636158049106598\n",
      "Epoch 19973/30000 Training Loss: 0.056701600551605225\n",
      "Epoch 19974/30000 Training Loss: 0.03671947494149208\n",
      "Epoch 19975/30000 Training Loss: 0.03991854935884476\n",
      "Epoch 19976/30000 Training Loss: 0.04846634343266487\n",
      "Epoch 19977/30000 Training Loss: 0.055797919631004333\n",
      "Epoch 19978/30000 Training Loss: 0.030652664601802826\n",
      "Epoch 19979/30000 Training Loss: 0.04471060633659363\n",
      "Epoch 19980/30000 Training Loss: 0.04397711530327797\n",
      "Epoch 19981/30000 Training Loss: 0.035199444741010666\n",
      "Epoch 19982/30000 Training Loss: 0.03894200176000595\n",
      "Epoch 19983/30000 Training Loss: 0.03931521624326706\n",
      "Epoch 19984/30000 Training Loss: 0.031540121883153915\n",
      "Epoch 19985/30000 Training Loss: 0.045355044305324554\n",
      "Epoch 19986/30000 Training Loss: 0.044417738914489746\n",
      "Epoch 19987/30000 Training Loss: 0.035662345588207245\n",
      "Epoch 19988/30000 Training Loss: 0.03455507755279541\n",
      "Epoch 19989/30000 Training Loss: 0.04585404321551323\n",
      "Epoch 19990/30000 Training Loss: 0.04752133786678314\n",
      "Epoch 19991/30000 Training Loss: 0.03256292641162872\n",
      "Epoch 19992/30000 Training Loss: 0.05331931263208389\n",
      "Epoch 19993/30000 Training Loss: 0.05044453218579292\n",
      "Epoch 19994/30000 Training Loss: 0.03565492853522301\n",
      "Epoch 19995/30000 Training Loss: 0.04974064230918884\n",
      "Epoch 19996/30000 Training Loss: 0.051177091896533966\n",
      "Epoch 19997/30000 Training Loss: 0.03813447803258896\n",
      "Epoch 19998/30000 Training Loss: 0.04153452813625336\n",
      "Epoch 19999/30000 Training Loss: 0.037869807332754135\n",
      "Epoch 20000/30000 Training Loss: 0.044665008783340454\n",
      "Epoch 20000/30000 Validation Loss: 0.0452614389359951\n",
      "Epoch 20001/30000 Training Loss: 0.04298677667975426\n",
      "Epoch 20002/30000 Training Loss: 0.05338896065950394\n",
      "Epoch 20003/30000 Training Loss: 0.038293782621622086\n",
      "Epoch 20004/30000 Training Loss: 0.05454248934984207\n",
      "Epoch 20005/30000 Training Loss: 0.030869465321302414\n",
      "Epoch 20006/30000 Training Loss: 0.05639645457267761\n",
      "Epoch 20007/30000 Training Loss: 0.05041342228651047\n",
      "Epoch 20008/30000 Training Loss: 0.03860144317150116\n",
      "Epoch 20009/30000 Training Loss: 0.06082466244697571\n",
      "Epoch 20010/30000 Training Loss: 0.048701003193855286\n",
      "Epoch 20011/30000 Training Loss: 0.04003818333148956\n",
      "Epoch 20012/30000 Training Loss: 0.03135748207569122\n",
      "Epoch 20013/30000 Training Loss: 0.04639217630028725\n",
      "Epoch 20014/30000 Training Loss: 0.057665422558784485\n",
      "Epoch 20015/30000 Training Loss: 0.0429997593164444\n",
      "Epoch 20016/30000 Training Loss: 0.03814626485109329\n",
      "Epoch 20017/30000 Training Loss: 0.04169248044490814\n",
      "Epoch 20018/30000 Training Loss: 0.04140951484441757\n",
      "Epoch 20019/30000 Training Loss: 0.03475343436002731\n",
      "Epoch 20020/30000 Training Loss: 0.04122964292764664\n",
      "Epoch 20021/30000 Training Loss: 0.03414139151573181\n",
      "Epoch 20022/30000 Training Loss: 0.049758441746234894\n",
      "Epoch 20023/30000 Training Loss: 0.055955369025468826\n",
      "Epoch 20024/30000 Training Loss: 0.03708123415708542\n",
      "Epoch 20025/30000 Training Loss: 0.041788145899772644\n",
      "Epoch 20026/30000 Training Loss: 0.0358990877866745\n",
      "Epoch 20027/30000 Training Loss: 0.040113478899002075\n",
      "Epoch 20028/30000 Training Loss: 0.055076874792575836\n",
      "Epoch 20029/30000 Training Loss: 0.05112413316965103\n",
      "Epoch 20030/30000 Training Loss: 0.03571097552776337\n",
      "Epoch 20031/30000 Training Loss: 0.035558316856622696\n",
      "Epoch 20032/30000 Training Loss: 0.04780871421098709\n",
      "Epoch 20033/30000 Training Loss: 0.03081393986940384\n",
      "Epoch 20034/30000 Training Loss: 0.04987449198961258\n",
      "Epoch 20035/30000 Training Loss: 0.03487081825733185\n",
      "Epoch 20036/30000 Training Loss: 0.04057083651423454\n",
      "Epoch 20037/30000 Training Loss: 0.04064992070198059\n",
      "Epoch 20038/30000 Training Loss: 0.04956502839922905\n",
      "Epoch 20039/30000 Training Loss: 0.04007675498723984\n",
      "Epoch 20040/30000 Training Loss: 0.04809919744729996\n",
      "Epoch 20041/30000 Training Loss: 0.038489699363708496\n",
      "Epoch 20042/30000 Training Loss: 0.034280795603990555\n",
      "Epoch 20043/30000 Training Loss: 0.03268982470035553\n",
      "Epoch 20044/30000 Training Loss: 0.04137963056564331\n",
      "Epoch 20045/30000 Training Loss: 0.03404603898525238\n",
      "Epoch 20046/30000 Training Loss: 0.04864806681871414\n",
      "Epoch 20047/30000 Training Loss: 0.05500422790646553\n",
      "Epoch 20048/30000 Training Loss: 0.044851019978523254\n",
      "Epoch 20049/30000 Training Loss: 0.05296047776937485\n",
      "Epoch 20050/30000 Training Loss: 0.04429370164871216\n",
      "Epoch 20051/30000 Training Loss: 0.038388703018426895\n",
      "Epoch 20052/30000 Training Loss: 0.04854486137628555\n",
      "Epoch 20053/30000 Training Loss: 0.04269185662269592\n",
      "Epoch 20054/30000 Training Loss: 0.05613165348768234\n",
      "Epoch 20055/30000 Training Loss: 0.05184010788798332\n",
      "Epoch 20056/30000 Training Loss: 0.044725243002176285\n",
      "Epoch 20057/30000 Training Loss: 0.03976553678512573\n",
      "Epoch 20058/30000 Training Loss: 0.04555079713463783\n",
      "Epoch 20059/30000 Training Loss: 0.03794822841882706\n",
      "Epoch 20060/30000 Training Loss: 0.0615977868437767\n",
      "Epoch 20061/30000 Training Loss: 0.04374225065112114\n",
      "Epoch 20062/30000 Training Loss: 0.047415539622306824\n",
      "Epoch 20063/30000 Training Loss: 0.0432819202542305\n",
      "Epoch 20064/30000 Training Loss: 0.04676340892910957\n",
      "Epoch 20065/30000 Training Loss: 0.045087363570928574\n",
      "Epoch 20066/30000 Training Loss: 0.04877963662147522\n",
      "Epoch 20067/30000 Training Loss: 0.04722807556390762\n",
      "Epoch 20068/30000 Training Loss: 0.03380022570490837\n",
      "Epoch 20069/30000 Training Loss: 0.038984183222055435\n",
      "Epoch 20070/30000 Training Loss: 0.04813201725482941\n",
      "Epoch 20071/30000 Training Loss: 0.035297613590955734\n",
      "Epoch 20072/30000 Training Loss: 0.05399176478385925\n",
      "Epoch 20073/30000 Training Loss: 0.04628579691052437\n",
      "Epoch 20074/30000 Training Loss: 0.04941561073064804\n",
      "Epoch 20075/30000 Training Loss: 0.03718589246273041\n",
      "Epoch 20076/30000 Training Loss: 0.05090517923235893\n",
      "Epoch 20077/30000 Training Loss: 0.03907454013824463\n",
      "Epoch 20078/30000 Training Loss: 0.03474246710538864\n",
      "Epoch 20079/30000 Training Loss: 0.04872529208660126\n",
      "Epoch 20080/30000 Training Loss: 0.03856182098388672\n",
      "Epoch 20081/30000 Training Loss: 0.03144532069563866\n",
      "Epoch 20082/30000 Training Loss: 0.035180944949388504\n",
      "Epoch 20083/30000 Training Loss: 0.039363957941532135\n",
      "Epoch 20084/30000 Training Loss: 0.04558148980140686\n",
      "Epoch 20085/30000 Training Loss: 0.04696858301758766\n",
      "Epoch 20086/30000 Training Loss: 0.05639385059475899\n",
      "Epoch 20087/30000 Training Loss: 0.0463358536362648\n",
      "Epoch 20088/30000 Training Loss: 0.05516481399536133\n",
      "Epoch 20089/30000 Training Loss: 0.02935752645134926\n",
      "Epoch 20090/30000 Training Loss: 0.044770218431949615\n",
      "Epoch 20091/30000 Training Loss: 0.033869802951812744\n",
      "Epoch 20092/30000 Training Loss: 0.028602376580238342\n",
      "Epoch 20093/30000 Training Loss: 0.05026811361312866\n",
      "Epoch 20094/30000 Training Loss: 0.03401755169034004\n",
      "Epoch 20095/30000 Training Loss: 0.03584873303771019\n",
      "Epoch 20096/30000 Training Loss: 0.042149946093559265\n",
      "Epoch 20097/30000 Training Loss: 0.04731690138578415\n",
      "Epoch 20098/30000 Training Loss: 0.04617886245250702\n",
      "Epoch 20099/30000 Training Loss: 0.04395093768835068\n",
      "Epoch 20100/30000 Training Loss: 0.04208878055214882\n",
      "Epoch 20100/30000 Validation Loss: 0.04103897139430046\n",
      "Epoch 20101/30000 Training Loss: 0.045422181487083435\n",
      "Epoch 20102/30000 Training Loss: 0.042143408209085464\n",
      "Epoch 20103/30000 Training Loss: 0.04362933337688446\n",
      "Epoch 20104/30000 Training Loss: 0.048844825476408005\n",
      "Epoch 20105/30000 Training Loss: 0.047332763671875\n",
      "Epoch 20106/30000 Training Loss: 0.03697098046541214\n",
      "Epoch 20107/30000 Training Loss: 0.04477716237306595\n",
      "Epoch 20108/30000 Training Loss: 0.04749920219182968\n",
      "Epoch 20109/30000 Training Loss: 0.04254681617021561\n",
      "Epoch 20110/30000 Training Loss: 0.04949700087308884\n",
      "Epoch 20111/30000 Training Loss: 0.040786489844322205\n",
      "Epoch 20112/30000 Training Loss: 0.04212804138660431\n",
      "Epoch 20113/30000 Training Loss: 0.03588346764445305\n",
      "Epoch 20114/30000 Training Loss: 0.044681329280138016\n",
      "Epoch 20115/30000 Training Loss: 0.04617977887392044\n",
      "Epoch 20116/30000 Training Loss: 0.04892292246222496\n",
      "Epoch 20117/30000 Training Loss: 0.037371642887592316\n",
      "Epoch 20118/30000 Training Loss: 0.03832928091287613\n",
      "Epoch 20119/30000 Training Loss: 0.06062722206115723\n",
      "Epoch 20120/30000 Training Loss: 0.048162516206502914\n",
      "Epoch 20121/30000 Training Loss: 0.03792842850089073\n",
      "Epoch 20122/30000 Training Loss: 0.044363684952259064\n",
      "Epoch 20123/30000 Training Loss: 0.06392894685268402\n",
      "Epoch 20124/30000 Training Loss: 0.033920690417289734\n",
      "Epoch 20125/30000 Training Loss: 0.04492957890033722\n",
      "Epoch 20126/30000 Training Loss: 0.0551823191344738\n",
      "Epoch 20127/30000 Training Loss: 0.05919444561004639\n",
      "Epoch 20128/30000 Training Loss: 0.03870964050292969\n",
      "Epoch 20129/30000 Training Loss: 0.0402679443359375\n",
      "Epoch 20130/30000 Training Loss: 0.06050406023859978\n",
      "Epoch 20131/30000 Training Loss: 0.03821903094649315\n",
      "Epoch 20132/30000 Training Loss: 0.056681521236896515\n",
      "Epoch 20133/30000 Training Loss: 0.03537368029356003\n",
      "Epoch 20134/30000 Training Loss: 0.05311989039182663\n",
      "Epoch 20135/30000 Training Loss: 0.046811293810606\n",
      "Epoch 20136/30000 Training Loss: 0.04460582882165909\n",
      "Epoch 20137/30000 Training Loss: 0.04659178853034973\n",
      "Epoch 20138/30000 Training Loss: 0.052305638790130615\n",
      "Epoch 20139/30000 Training Loss: 0.051001451909542084\n",
      "Epoch 20140/30000 Training Loss: 0.039980463683605194\n",
      "Epoch 20141/30000 Training Loss: 0.0433025062084198\n",
      "Epoch 20142/30000 Training Loss: 0.04248088598251343\n",
      "Epoch 20143/30000 Training Loss: 0.05339951068162918\n",
      "Epoch 20144/30000 Training Loss: 0.05111989378929138\n",
      "Epoch 20145/30000 Training Loss: 0.06111505627632141\n",
      "Epoch 20146/30000 Training Loss: 0.04122575372457504\n",
      "Epoch 20147/30000 Training Loss: 0.04621057212352753\n",
      "Epoch 20148/30000 Training Loss: 0.051617152988910675\n",
      "Epoch 20149/30000 Training Loss: 0.049893058836460114\n",
      "Epoch 20150/30000 Training Loss: 0.03613681346178055\n",
      "Epoch 20151/30000 Training Loss: 0.04771487042307854\n",
      "Epoch 20152/30000 Training Loss: 0.034959882497787476\n",
      "Epoch 20153/30000 Training Loss: 0.04103092849254608\n",
      "Epoch 20154/30000 Training Loss: 0.03788777440786362\n",
      "Epoch 20155/30000 Training Loss: 0.03815114498138428\n",
      "Epoch 20156/30000 Training Loss: 0.03524903208017349\n",
      "Epoch 20157/30000 Training Loss: 0.039555057883262634\n",
      "Epoch 20158/30000 Training Loss: 0.045914728194475174\n",
      "Epoch 20159/30000 Training Loss: 0.0441853329539299\n",
      "Epoch 20160/30000 Training Loss: 0.03187282010912895\n",
      "Epoch 20161/30000 Training Loss: 0.052232999354600906\n",
      "Epoch 20162/30000 Training Loss: 0.0539979413151741\n",
      "Epoch 20163/30000 Training Loss: 0.028438445180654526\n",
      "Epoch 20164/30000 Training Loss: 0.04217042028903961\n",
      "Epoch 20165/30000 Training Loss: 0.045342735946178436\n",
      "Epoch 20166/30000 Training Loss: 0.04011593759059906\n",
      "Epoch 20167/30000 Training Loss: 0.04690311849117279\n",
      "Epoch 20168/30000 Training Loss: 0.042117051780223846\n",
      "Epoch 20169/30000 Training Loss: 0.044520940631628036\n",
      "Epoch 20170/30000 Training Loss: 0.04937859624624252\n",
      "Epoch 20171/30000 Training Loss: 0.047687679529190063\n",
      "Epoch 20172/30000 Training Loss: 0.05377594381570816\n",
      "Epoch 20173/30000 Training Loss: 0.03917638212442398\n",
      "Epoch 20174/30000 Training Loss: 0.039559297263622284\n",
      "Epoch 20175/30000 Training Loss: 0.0546841099858284\n",
      "Epoch 20176/30000 Training Loss: 0.045329660177230835\n",
      "Epoch 20177/30000 Training Loss: 0.04232468456029892\n",
      "Epoch 20178/30000 Training Loss: 0.04422828182578087\n",
      "Epoch 20179/30000 Training Loss: 0.05848773568868637\n",
      "Epoch 20180/30000 Training Loss: 0.03652838245034218\n",
      "Epoch 20181/30000 Training Loss: 0.05101555213332176\n",
      "Epoch 20182/30000 Training Loss: 0.037444040179252625\n",
      "Epoch 20183/30000 Training Loss: 0.04579022154211998\n",
      "Epoch 20184/30000 Training Loss: 0.04057719185948372\n",
      "Epoch 20185/30000 Training Loss: 0.03944326564669609\n",
      "Epoch 20186/30000 Training Loss: 0.04582788050174713\n",
      "Epoch 20187/30000 Training Loss: 0.03797357901930809\n",
      "Epoch 20188/30000 Training Loss: 0.04758265241980553\n",
      "Epoch 20189/30000 Training Loss: 0.037653323262929916\n",
      "Epoch 20190/30000 Training Loss: 0.03398405760526657\n",
      "Epoch 20191/30000 Training Loss: 0.04011550545692444\n",
      "Epoch 20192/30000 Training Loss: 0.041323959827423096\n",
      "Epoch 20193/30000 Training Loss: 0.04133256524801254\n",
      "Epoch 20194/30000 Training Loss: 0.035300999879837036\n",
      "Epoch 20195/30000 Training Loss: 0.0481160506606102\n",
      "Epoch 20196/30000 Training Loss: 0.05054430663585663\n",
      "Epoch 20197/30000 Training Loss: 0.04808267205953598\n",
      "Epoch 20198/30000 Training Loss: 0.062073640525341034\n",
      "Epoch 20199/30000 Training Loss: 0.04479929804801941\n",
      "Epoch 20200/30000 Training Loss: 0.05459202080965042\n",
      "Epoch 20200/30000 Validation Loss: 0.05170879513025284\n",
      "Epoch 20201/30000 Training Loss: 0.028569936752319336\n",
      "Epoch 20202/30000 Training Loss: 0.040718041360378265\n",
      "Epoch 20203/30000 Training Loss: 0.04099946469068527\n",
      "Epoch 20204/30000 Training Loss: 0.035580649971961975\n",
      "Epoch 20205/30000 Training Loss: 0.03323443979024887\n",
      "Epoch 20206/30000 Training Loss: 0.04675283282995224\n",
      "Epoch 20207/30000 Training Loss: 0.04669811204075813\n",
      "Epoch 20208/30000 Training Loss: 0.0366736501455307\n",
      "Epoch 20209/30000 Training Loss: 0.050101328641176224\n",
      "Epoch 20210/30000 Training Loss: 0.04238675534725189\n",
      "Epoch 20211/30000 Training Loss: 0.04508202522993088\n",
      "Epoch 20212/30000 Training Loss: 0.05297271907329559\n",
      "Epoch 20213/30000 Training Loss: 0.04647698625922203\n",
      "Epoch 20214/30000 Training Loss: 0.0439113974571228\n",
      "Epoch 20215/30000 Training Loss: 0.03594405576586723\n",
      "Epoch 20216/30000 Training Loss: 0.028294090181589127\n",
      "Epoch 20217/30000 Training Loss: 0.044929083436727524\n",
      "Epoch 20218/30000 Training Loss: 0.05896446853876114\n",
      "Epoch 20219/30000 Training Loss: 0.042188528925180435\n",
      "Epoch 20220/30000 Training Loss: 0.0344243086874485\n",
      "Epoch 20221/30000 Training Loss: 0.03312181681394577\n",
      "Epoch 20222/30000 Training Loss: 0.04729051887989044\n",
      "Epoch 20223/30000 Training Loss: 0.05682092905044556\n",
      "Epoch 20224/30000 Training Loss: 0.0338253378868103\n",
      "Epoch 20225/30000 Training Loss: 0.05925820767879486\n",
      "Epoch 20226/30000 Training Loss: 0.049271922558546066\n",
      "Epoch 20227/30000 Training Loss: 0.0478774830698967\n",
      "Epoch 20228/30000 Training Loss: 0.03921819478273392\n",
      "Epoch 20229/30000 Training Loss: 0.05471271276473999\n",
      "Epoch 20230/30000 Training Loss: 0.047381702810525894\n",
      "Epoch 20231/30000 Training Loss: 0.03716323524713516\n",
      "Epoch 20232/30000 Training Loss: 0.05423211306333542\n",
      "Epoch 20233/30000 Training Loss: 0.0490545928478241\n",
      "Epoch 20234/30000 Training Loss: 0.037124328315258026\n",
      "Epoch 20235/30000 Training Loss: 0.043470315635204315\n",
      "Epoch 20236/30000 Training Loss: 0.03738837316632271\n",
      "Epoch 20237/30000 Training Loss: 0.04529181122779846\n",
      "Epoch 20238/30000 Training Loss: 0.04821498319506645\n",
      "Epoch 20239/30000 Training Loss: 0.04187706857919693\n",
      "Epoch 20240/30000 Training Loss: 0.045297760516405106\n",
      "Epoch 20241/30000 Training Loss: 0.036619462072849274\n",
      "Epoch 20242/30000 Training Loss: 0.03656746819615364\n",
      "Epoch 20243/30000 Training Loss: 0.04203517735004425\n",
      "Epoch 20244/30000 Training Loss: 0.037353988736867905\n",
      "Epoch 20245/30000 Training Loss: 0.040119968354701996\n",
      "Epoch 20246/30000 Training Loss: 0.045529112219810486\n",
      "Epoch 20247/30000 Training Loss: 0.04040661081671715\n",
      "Epoch 20248/30000 Training Loss: 0.05368936061859131\n",
      "Epoch 20249/30000 Training Loss: 0.05301784723997116\n",
      "Epoch 20250/30000 Training Loss: 0.034044768661260605\n",
      "Epoch 20251/30000 Training Loss: 0.06244712322950363\n",
      "Epoch 20252/30000 Training Loss: 0.05098900943994522\n",
      "Epoch 20253/30000 Training Loss: 0.0393696092069149\n",
      "Epoch 20254/30000 Training Loss: 0.05374711751937866\n",
      "Epoch 20255/30000 Training Loss: 0.04207829013466835\n",
      "Epoch 20256/30000 Training Loss: 0.03944234549999237\n",
      "Epoch 20257/30000 Training Loss: 0.038943927735090256\n",
      "Epoch 20258/30000 Training Loss: 0.03368186205625534\n",
      "Epoch 20259/30000 Training Loss: 0.059762176126241684\n",
      "Epoch 20260/30000 Training Loss: 0.03988393023610115\n",
      "Epoch 20261/30000 Training Loss: 0.038997583091259\n",
      "Epoch 20262/30000 Training Loss: 0.04751591756939888\n",
      "Epoch 20263/30000 Training Loss: 0.0302193034440279\n",
      "Epoch 20264/30000 Training Loss: 0.04528248682618141\n",
      "Epoch 20265/30000 Training Loss: 0.03391125798225403\n",
      "Epoch 20266/30000 Training Loss: 0.033462509512901306\n",
      "Epoch 20267/30000 Training Loss: 0.028677605092525482\n",
      "Epoch 20268/30000 Training Loss: 0.044707514345645905\n",
      "Epoch 20269/30000 Training Loss: 0.033234141767024994\n",
      "Epoch 20270/30000 Training Loss: 0.04229338467121124\n",
      "Epoch 20271/30000 Training Loss: 0.06059321016073227\n",
      "Epoch 20272/30000 Training Loss: 0.036115869879722595\n",
      "Epoch 20273/30000 Training Loss: 0.0377596914768219\n",
      "Epoch 20274/30000 Training Loss: 0.04766926169395447\n",
      "Epoch 20275/30000 Training Loss: 0.031915999948978424\n",
      "Epoch 20276/30000 Training Loss: 0.0469311848282814\n",
      "Epoch 20277/30000 Training Loss: 0.05394978076219559\n",
      "Epoch 20278/30000 Training Loss: 0.05623527616262436\n",
      "Epoch 20279/30000 Training Loss: 0.03688356280326843\n",
      "Epoch 20280/30000 Training Loss: 0.043199699372053146\n",
      "Epoch 20281/30000 Training Loss: 0.03439609706401825\n",
      "Epoch 20282/30000 Training Loss: 0.06507334113121033\n",
      "Epoch 20283/30000 Training Loss: 0.04524805396795273\n",
      "Epoch 20284/30000 Training Loss: 0.03998986631631851\n",
      "Epoch 20285/30000 Training Loss: 0.057348646223545074\n",
      "Epoch 20286/30000 Training Loss: 0.036952994763851166\n",
      "Epoch 20287/30000 Training Loss: 0.04082107171416283\n",
      "Epoch 20288/30000 Training Loss: 0.05307532474398613\n",
      "Epoch 20289/30000 Training Loss: 0.041915059089660645\n",
      "Epoch 20290/30000 Training Loss: 0.03698483482003212\n",
      "Epoch 20291/30000 Training Loss: 0.05266831815242767\n",
      "Epoch 20292/30000 Training Loss: 0.042050279676914215\n",
      "Epoch 20293/30000 Training Loss: 0.04445524513721466\n",
      "Epoch 20294/30000 Training Loss: 0.03590241074562073\n",
      "Epoch 20295/30000 Training Loss: 0.04074627533555031\n",
      "Epoch 20296/30000 Training Loss: 0.039800629019737244\n",
      "Epoch 20297/30000 Training Loss: 0.05527060478925705\n",
      "Epoch 20298/30000 Training Loss: 0.03755320981144905\n",
      "Epoch 20299/30000 Training Loss: 0.06464739888906479\n",
      "Epoch 20300/30000 Training Loss: 0.03707239031791687\n",
      "Epoch 20300/30000 Validation Loss: 0.031296148896217346\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.031296148896217346<=============\n",
      "Epoch 20301/30000 Training Loss: 0.05642631649971008\n",
      "Epoch 20302/30000 Training Loss: 0.03420014679431915\n",
      "Epoch 20303/30000 Training Loss: 0.05172654613852501\n",
      "Epoch 20304/30000 Training Loss: 0.0367928221821785\n",
      "Epoch 20305/30000 Training Loss: 0.04342091083526611\n",
      "Epoch 20306/30000 Training Loss: 0.046125397086143494\n",
      "Epoch 20307/30000 Training Loss: 0.04610060155391693\n",
      "Epoch 20308/30000 Training Loss: 0.03564012423157692\n",
      "Epoch 20309/30000 Training Loss: 0.037611812353134155\n",
      "Epoch 20310/30000 Training Loss: 0.038986653089523315\n",
      "Epoch 20311/30000 Training Loss: 0.041554100811481476\n",
      "Epoch 20312/30000 Training Loss: 0.04867337644100189\n",
      "Epoch 20313/30000 Training Loss: 0.04280887544155121\n",
      "Epoch 20314/30000 Training Loss: 0.03791721910238266\n",
      "Epoch 20315/30000 Training Loss: 0.04201316460967064\n",
      "Epoch 20316/30000 Training Loss: 0.047945138067007065\n",
      "Epoch 20317/30000 Training Loss: 0.038207046687603\n",
      "Epoch 20318/30000 Training Loss: 0.029981859028339386\n",
      "Epoch 20319/30000 Training Loss: 0.043584052473306656\n",
      "Epoch 20320/30000 Training Loss: 0.04978330433368683\n",
      "Epoch 20321/30000 Training Loss: 0.04453178122639656\n",
      "Epoch 20322/30000 Training Loss: 0.03415222465991974\n",
      "Epoch 20323/30000 Training Loss: 0.037926387041807175\n",
      "Epoch 20324/30000 Training Loss: 0.03569406643509865\n",
      "Epoch 20325/30000 Training Loss: 0.03692251816391945\n",
      "Epoch 20326/30000 Training Loss: 0.05400768667459488\n",
      "Epoch 20327/30000 Training Loss: 0.046573661267757416\n",
      "Epoch 20328/30000 Training Loss: 0.04237276688218117\n",
      "Epoch 20329/30000 Training Loss: 0.04345505312085152\n",
      "Epoch 20330/30000 Training Loss: 0.05175513029098511\n",
      "Epoch 20331/30000 Training Loss: 0.04361972212791443\n",
      "Epoch 20332/30000 Training Loss: 0.05569341033697128\n",
      "Epoch 20333/30000 Training Loss: 0.053166940808296204\n",
      "Epoch 20334/30000 Training Loss: 0.04964027553796768\n",
      "Epoch 20335/30000 Training Loss: 0.04173261299729347\n",
      "Epoch 20336/30000 Training Loss: 0.029133744537830353\n",
      "Epoch 20337/30000 Training Loss: 0.03944160416722298\n",
      "Epoch 20338/30000 Training Loss: 0.04648040980100632\n",
      "Epoch 20339/30000 Training Loss: 0.036813005805015564\n",
      "Epoch 20340/30000 Training Loss: 0.0400305911898613\n",
      "Epoch 20341/30000 Training Loss: 0.03331957012414932\n",
      "Epoch 20342/30000 Training Loss: 0.03617946058511734\n",
      "Epoch 20343/30000 Training Loss: 0.049073830246925354\n",
      "Epoch 20344/30000 Training Loss: 0.03548790514469147\n",
      "Epoch 20345/30000 Training Loss: 0.057220734655857086\n",
      "Epoch 20346/30000 Training Loss: 0.039004333317279816\n",
      "Epoch 20347/30000 Training Loss: 0.0380498543381691\n",
      "Epoch 20348/30000 Training Loss: 0.043085359036922455\n",
      "Epoch 20349/30000 Training Loss: 0.049069762229919434\n",
      "Epoch 20350/30000 Training Loss: 0.02642899751663208\n",
      "Epoch 20351/30000 Training Loss: 0.029856514185667038\n",
      "Epoch 20352/30000 Training Loss: 0.04227623715996742\n",
      "Epoch 20353/30000 Training Loss: 0.04997165873646736\n",
      "Epoch 20354/30000 Training Loss: 0.05140729248523712\n",
      "Epoch 20355/30000 Training Loss: 0.043798647820949554\n",
      "Epoch 20356/30000 Training Loss: 0.040789201855659485\n",
      "Epoch 20357/30000 Training Loss: 0.04120079055428505\n",
      "Epoch 20358/30000 Training Loss: 0.04939898103475571\n",
      "Epoch 20359/30000 Training Loss: 0.03937873989343643\n",
      "Epoch 20360/30000 Training Loss: 0.04791811853647232\n",
      "Epoch 20361/30000 Training Loss: 0.05596305429935455\n",
      "Epoch 20362/30000 Training Loss: 0.0365118682384491\n",
      "Epoch 20363/30000 Training Loss: 0.04593480005860329\n",
      "Epoch 20364/30000 Training Loss: 0.03593771904706955\n",
      "Epoch 20365/30000 Training Loss: 0.03236554563045502\n",
      "Epoch 20366/30000 Training Loss: 0.03713170066475868\n",
      "Epoch 20367/30000 Training Loss: 0.038700297474861145\n",
      "Epoch 20368/30000 Training Loss: 0.037782639265060425\n",
      "Epoch 20369/30000 Training Loss: 0.0476498082280159\n",
      "Epoch 20370/30000 Training Loss: 0.04376401752233505\n",
      "Epoch 20371/30000 Training Loss: 0.04379009082913399\n",
      "Epoch 20372/30000 Training Loss: 0.04582583159208298\n",
      "Epoch 20373/30000 Training Loss: 0.05587364733219147\n",
      "Epoch 20374/30000 Training Loss: 0.06351219117641449\n",
      "Epoch 20375/30000 Training Loss: 0.038388416171073914\n",
      "Epoch 20376/30000 Training Loss: 0.057489506900310516\n",
      "Epoch 20377/30000 Training Loss: 0.04542464762926102\n",
      "Epoch 20378/30000 Training Loss: 0.045516565442085266\n",
      "Epoch 20379/30000 Training Loss: 0.06010306254029274\n",
      "Epoch 20380/30000 Training Loss: 0.03520940616726875\n",
      "Epoch 20381/30000 Training Loss: 0.0393856056034565\n",
      "Epoch 20382/30000 Training Loss: 0.046541038900613785\n",
      "Epoch 20383/30000 Training Loss: 0.04724713787436485\n",
      "Epoch 20384/30000 Training Loss: 0.05433015525341034\n",
      "Epoch 20385/30000 Training Loss: 0.04364459589123726\n",
      "Epoch 20386/30000 Training Loss: 0.03986062854528427\n",
      "Epoch 20387/30000 Training Loss: 0.033824190497398376\n",
      "Epoch 20388/30000 Training Loss: 0.05792749673128128\n",
      "Epoch 20389/30000 Training Loss: 0.054210539907217026\n",
      "Epoch 20390/30000 Training Loss: 0.05626697093248367\n",
      "Epoch 20391/30000 Training Loss: 0.054318446666002274\n",
      "Epoch 20392/30000 Training Loss: 0.039038609713315964\n",
      "Epoch 20393/30000 Training Loss: 0.04324158653616905\n",
      "Epoch 20394/30000 Training Loss: 0.048853278160095215\n",
      "Epoch 20395/30000 Training Loss: 0.0447588749229908\n",
      "Epoch 20396/30000 Training Loss: 0.04700235277414322\n",
      "Epoch 20397/30000 Training Loss: 0.042372576892375946\n",
      "Epoch 20398/30000 Training Loss: 0.04529310762882233\n",
      "Epoch 20399/30000 Training Loss: 0.05065847560763359\n",
      "Epoch 20400/30000 Training Loss: 0.0392581969499588\n",
      "Epoch 20400/30000 Validation Loss: 0.05646386742591858\n",
      "Epoch 20401/30000 Training Loss: 0.0464501678943634\n",
      "Epoch 20402/30000 Training Loss: 0.042154133319854736\n",
      "Epoch 20403/30000 Training Loss: 0.03535042703151703\n",
      "Epoch 20404/30000 Training Loss: 0.04134476184844971\n",
      "Epoch 20405/30000 Training Loss: 0.06638791412115097\n",
      "Epoch 20406/30000 Training Loss: 0.060704536736011505\n",
      "Epoch 20407/30000 Training Loss: 0.03728669881820679\n",
      "Epoch 20408/30000 Training Loss: 0.03996913880109787\n",
      "Epoch 20409/30000 Training Loss: 0.04741227254271507\n",
      "Epoch 20410/30000 Training Loss: 0.04358901083469391\n",
      "Epoch 20411/30000 Training Loss: 0.05157919228076935\n",
      "Epoch 20412/30000 Training Loss: 0.04620109871029854\n",
      "Epoch 20413/30000 Training Loss: 0.04425951838493347\n",
      "Epoch 20414/30000 Training Loss: 0.042524129152297974\n",
      "Epoch 20415/30000 Training Loss: 0.041492730379104614\n",
      "Epoch 20416/30000 Training Loss: 0.050923094153404236\n",
      "Epoch 20417/30000 Training Loss: 0.049950823187828064\n",
      "Epoch 20418/30000 Training Loss: 0.036229297518730164\n",
      "Epoch 20419/30000 Training Loss: 0.039601750671863556\n",
      "Epoch 20420/30000 Training Loss: 0.05500650778412819\n",
      "Epoch 20421/30000 Training Loss: 0.04400691017508507\n",
      "Epoch 20422/30000 Training Loss: 0.03168755769729614\n",
      "Epoch 20423/30000 Training Loss: 0.04294861480593681\n",
      "Epoch 20424/30000 Training Loss: 0.047398053109645844\n",
      "Epoch 20425/30000 Training Loss: 0.03879614919424057\n",
      "Epoch 20426/30000 Training Loss: 0.04096522182226181\n",
      "Epoch 20427/30000 Training Loss: 0.03973788395524025\n",
      "Epoch 20428/30000 Training Loss: 0.05866082012653351\n",
      "Epoch 20429/30000 Training Loss: 0.04134315624833107\n",
      "Epoch 20430/30000 Training Loss: 0.045887041836977005\n",
      "Epoch 20431/30000 Training Loss: 0.037543609738349915\n",
      "Epoch 20432/30000 Training Loss: 0.03026944398880005\n",
      "Epoch 20433/30000 Training Loss: 0.05095124989748001\n",
      "Epoch 20434/30000 Training Loss: 0.051015615463256836\n",
      "Epoch 20435/30000 Training Loss: 0.044254906475543976\n",
      "Epoch 20436/30000 Training Loss: 0.03643190488219261\n",
      "Epoch 20437/30000 Training Loss: 0.042642995715141296\n",
      "Epoch 20438/30000 Training Loss: 0.037002988159656525\n",
      "Epoch 20439/30000 Training Loss: 0.041900426149368286\n",
      "Epoch 20440/30000 Training Loss: 0.05192529037594795\n",
      "Epoch 20441/30000 Training Loss: 0.04107523709535599\n",
      "Epoch 20442/30000 Training Loss: 0.06306791305541992\n",
      "Epoch 20443/30000 Training Loss: 0.04078131169080734\n",
      "Epoch 20444/30000 Training Loss: 0.04430553317070007\n",
      "Epoch 20445/30000 Training Loss: 0.0447101965546608\n",
      "Epoch 20446/30000 Training Loss: 0.05868306756019592\n",
      "Epoch 20447/30000 Training Loss: 0.046448733657598495\n",
      "Epoch 20448/30000 Training Loss: 0.03574109077453613\n",
      "Epoch 20449/30000 Training Loss: 0.04433051496744156\n",
      "Epoch 20450/30000 Training Loss: 0.049935393035411835\n",
      "Epoch 20451/30000 Training Loss: 0.037791427224874496\n",
      "Epoch 20452/30000 Training Loss: 0.04874125123023987\n",
      "Epoch 20453/30000 Training Loss: 0.05586230754852295\n",
      "Epoch 20454/30000 Training Loss: 0.04164872318506241\n",
      "Epoch 20455/30000 Training Loss: 0.038551487028598785\n",
      "Epoch 20456/30000 Training Loss: 0.041475579142570496\n",
      "Epoch 20457/30000 Training Loss: 0.04278233274817467\n",
      "Epoch 20458/30000 Training Loss: 0.03948003053665161\n",
      "Epoch 20459/30000 Training Loss: 0.031099043786525726\n",
      "Epoch 20460/30000 Training Loss: 0.050954777747392654\n",
      "Epoch 20461/30000 Training Loss: 0.042268116027116776\n",
      "Epoch 20462/30000 Training Loss: 0.053943708539009094\n",
      "Epoch 20463/30000 Training Loss: 0.03924012556672096\n",
      "Epoch 20464/30000 Training Loss: 0.03798272833228111\n",
      "Epoch 20465/30000 Training Loss: 0.044144392013549805\n",
      "Epoch 20466/30000 Training Loss: 0.043101031333208084\n",
      "Epoch 20467/30000 Training Loss: 0.03836336359381676\n",
      "Epoch 20468/30000 Training Loss: 0.044696658849716187\n",
      "Epoch 20469/30000 Training Loss: 0.04189825803041458\n",
      "Epoch 20470/30000 Training Loss: 0.04058945178985596\n",
      "Epoch 20471/30000 Training Loss: 0.03883693367242813\n",
      "Epoch 20472/30000 Training Loss: 0.04269389063119888\n",
      "Epoch 20473/30000 Training Loss: 0.05171157419681549\n",
      "Epoch 20474/30000 Training Loss: 0.043477535247802734\n",
      "Epoch 20475/30000 Training Loss: 0.06477471441030502\n",
      "Epoch 20476/30000 Training Loss: 0.04682145267724991\n",
      "Epoch 20477/30000 Training Loss: 0.02771029993891716\n",
      "Epoch 20478/30000 Training Loss: 0.026083461940288544\n",
      "Epoch 20479/30000 Training Loss: 0.035937417298555374\n",
      "Epoch 20480/30000 Training Loss: 0.047342389822006226\n",
      "Epoch 20481/30000 Training Loss: 0.04674753546714783\n",
      "Epoch 20482/30000 Training Loss: 0.037803590297698975\n",
      "Epoch 20483/30000 Training Loss: 0.0492304302752018\n",
      "Epoch 20484/30000 Training Loss: 0.04575153440237045\n",
      "Epoch 20485/30000 Training Loss: 0.04394520819187164\n",
      "Epoch 20486/30000 Training Loss: 0.038299545645713806\n",
      "Epoch 20487/30000 Training Loss: 0.03577839583158493\n",
      "Epoch 20488/30000 Training Loss: 0.03901311382651329\n",
      "Epoch 20489/30000 Training Loss: 0.051616620272397995\n",
      "Epoch 20490/30000 Training Loss: 0.05574401468038559\n",
      "Epoch 20491/30000 Training Loss: 0.04443218931555748\n",
      "Epoch 20492/30000 Training Loss: 0.04067864269018173\n",
      "Epoch 20493/30000 Training Loss: 0.041077177971601486\n",
      "Epoch 20494/30000 Training Loss: 0.03868997097015381\n",
      "Epoch 20495/30000 Training Loss: 0.04980353266000748\n",
      "Epoch 20496/30000 Training Loss: 0.05002860724925995\n",
      "Epoch 20497/30000 Training Loss: 0.06089183688163757\n",
      "Epoch 20498/30000 Training Loss: 0.04509637504816055\n",
      "Epoch 20499/30000 Training Loss: 0.03456324338912964\n",
      "Epoch 20500/30000 Training Loss: 0.03792794421315193\n",
      "Epoch 20500/30000 Validation Loss: 0.05206984654068947\n",
      "Epoch 20501/30000 Training Loss: 0.038936860859394073\n",
      "Epoch 20502/30000 Training Loss: 0.030979283154010773\n",
      "Epoch 20503/30000 Training Loss: 0.04466880485415459\n",
      "Epoch 20504/30000 Training Loss: 0.04026981443166733\n",
      "Epoch 20505/30000 Training Loss: 0.03490877524018288\n",
      "Epoch 20506/30000 Training Loss: 0.06268063187599182\n",
      "Epoch 20507/30000 Training Loss: 0.05487541854381561\n",
      "Epoch 20508/30000 Training Loss: 0.05724913626909256\n",
      "Epoch 20509/30000 Training Loss: 0.04236312955617905\n",
      "Epoch 20510/30000 Training Loss: 0.0440983772277832\n",
      "Epoch 20511/30000 Training Loss: 0.03850698471069336\n",
      "Epoch 20512/30000 Training Loss: 0.050486497581005096\n",
      "Epoch 20513/30000 Training Loss: 0.039144985377788544\n",
      "Epoch 20514/30000 Training Loss: 0.04569202661514282\n",
      "Epoch 20515/30000 Training Loss: 0.04037630558013916\n",
      "Epoch 20516/30000 Training Loss: 0.04943393915891647\n",
      "Epoch 20517/30000 Training Loss: 0.04649799317121506\n",
      "Epoch 20518/30000 Training Loss: 0.043703772127628326\n",
      "Epoch 20519/30000 Training Loss: 0.037408918142318726\n",
      "Epoch 20520/30000 Training Loss: 0.03676386550068855\n",
      "Epoch 20521/30000 Training Loss: 0.05394073203206062\n",
      "Epoch 20522/30000 Training Loss: 0.05859002843499184\n",
      "Epoch 20523/30000 Training Loss: 0.03379267454147339\n",
      "Epoch 20524/30000 Training Loss: 0.04542507976293564\n",
      "Epoch 20525/30000 Training Loss: 0.046013325452804565\n",
      "Epoch 20526/30000 Training Loss: 0.04300624132156372\n",
      "Epoch 20527/30000 Training Loss: 0.056154217571020126\n",
      "Epoch 20528/30000 Training Loss: 0.044196099042892456\n",
      "Epoch 20529/30000 Training Loss: 0.03376854956150055\n",
      "Epoch 20530/30000 Training Loss: 0.049580566585063934\n",
      "Epoch 20531/30000 Training Loss: 0.03277315944433212\n",
      "Epoch 20532/30000 Training Loss: 0.052896395325660706\n",
      "Epoch 20533/30000 Training Loss: 0.03403288498520851\n",
      "Epoch 20534/30000 Training Loss: 0.050427183508872986\n",
      "Epoch 20535/30000 Training Loss: 0.04969584941864014\n",
      "Epoch 20536/30000 Training Loss: 0.04148393124341965\n",
      "Epoch 20537/30000 Training Loss: 0.06413102149963379\n",
      "Epoch 20538/30000 Training Loss: 0.034643784165382385\n",
      "Epoch 20539/30000 Training Loss: 0.049644213169813156\n",
      "Epoch 20540/30000 Training Loss: 0.060426585376262665\n",
      "Epoch 20541/30000 Training Loss: 0.03729481250047684\n",
      "Epoch 20542/30000 Training Loss: 0.032500699162483215\n",
      "Epoch 20543/30000 Training Loss: 0.03481484577059746\n",
      "Epoch 20544/30000 Training Loss: 0.042233433574438095\n",
      "Epoch 20545/30000 Training Loss: 0.05187568813562393\n",
      "Epoch 20546/30000 Training Loss: 0.033275995403528214\n",
      "Epoch 20547/30000 Training Loss: 0.04738488048315048\n",
      "Epoch 20548/30000 Training Loss: 0.03708963096141815\n",
      "Epoch 20549/30000 Training Loss: 0.04306142032146454\n",
      "Epoch 20550/30000 Training Loss: 0.03206492215394974\n",
      "Epoch 20551/30000 Training Loss: 0.04944399744272232\n",
      "Epoch 20552/30000 Training Loss: 0.04329517111182213\n",
      "Epoch 20553/30000 Training Loss: 0.04206547886133194\n",
      "Epoch 20554/30000 Training Loss: 0.03955478593707085\n",
      "Epoch 20555/30000 Training Loss: 0.04116634652018547\n",
      "Epoch 20556/30000 Training Loss: 0.045212749391794205\n",
      "Epoch 20557/30000 Training Loss: 0.030435198917984962\n",
      "Epoch 20558/30000 Training Loss: 0.04096018522977829\n",
      "Epoch 20559/30000 Training Loss: 0.04329763725399971\n",
      "Epoch 20560/30000 Training Loss: 0.051777612417936325\n",
      "Epoch 20561/30000 Training Loss: 0.04542572423815727\n",
      "Epoch 20562/30000 Training Loss: 0.044267673045396805\n",
      "Epoch 20563/30000 Training Loss: 0.04009052366018295\n",
      "Epoch 20564/30000 Training Loss: 0.03635581582784653\n",
      "Epoch 20565/30000 Training Loss: 0.04260517656803131\n",
      "Epoch 20566/30000 Training Loss: 0.03430231660604477\n",
      "Epoch 20567/30000 Training Loss: 0.050165772438049316\n",
      "Epoch 20568/30000 Training Loss: 0.04886689782142639\n",
      "Epoch 20569/30000 Training Loss: 0.06538580358028412\n",
      "Epoch 20570/30000 Training Loss: 0.03887634351849556\n",
      "Epoch 20571/30000 Training Loss: 0.05402972549200058\n",
      "Epoch 20572/30000 Training Loss: 0.04609770327806473\n",
      "Epoch 20573/30000 Training Loss: 0.04643147811293602\n",
      "Epoch 20574/30000 Training Loss: 0.04943390563130379\n",
      "Epoch 20575/30000 Training Loss: 0.04329304024577141\n",
      "Epoch 20576/30000 Training Loss: 0.0333094485104084\n",
      "Epoch 20577/30000 Training Loss: 0.038715653121471405\n",
      "Epoch 20578/30000 Training Loss: 0.03553779795765877\n",
      "Epoch 20579/30000 Training Loss: 0.04584760218858719\n",
      "Epoch 20580/30000 Training Loss: 0.029094023630023003\n",
      "Epoch 20581/30000 Training Loss: 0.0324714332818985\n",
      "Epoch 20582/30000 Training Loss: 0.05115159600973129\n",
      "Epoch 20583/30000 Training Loss: 0.04354751110076904\n",
      "Epoch 20584/30000 Training Loss: 0.036400243639945984\n",
      "Epoch 20585/30000 Training Loss: 0.0440007783472538\n",
      "Epoch 20586/30000 Training Loss: 0.04793905094265938\n",
      "Epoch 20587/30000 Training Loss: 0.06484945863485336\n",
      "Epoch 20588/30000 Training Loss: 0.03643862158060074\n",
      "Epoch 20589/30000 Training Loss: 0.05015019327402115\n",
      "Epoch 20590/30000 Training Loss: 0.05548904091119766\n",
      "Epoch 20591/30000 Training Loss: 0.04334040731191635\n",
      "Epoch 20592/30000 Training Loss: 0.038524970412254333\n",
      "Epoch 20593/30000 Training Loss: 0.05418754369020462\n",
      "Epoch 20594/30000 Training Loss: 0.03912954777479172\n",
      "Epoch 20595/30000 Training Loss: 0.0425562858581543\n",
      "Epoch 20596/30000 Training Loss: 0.04301964119076729\n",
      "Epoch 20597/30000 Training Loss: 0.04912896826863289\n",
      "Epoch 20598/30000 Training Loss: 0.04321954771876335\n",
      "Epoch 20599/30000 Training Loss: 0.05013585835695267\n",
      "Epoch 20600/30000 Training Loss: 0.04945788532495499\n",
      "Epoch 20600/30000 Validation Loss: 0.043063826858997345\n",
      "Epoch 20601/30000 Training Loss: 0.05263611674308777\n",
      "Epoch 20602/30000 Training Loss: 0.055648840963840485\n",
      "Epoch 20603/30000 Training Loss: 0.046476978808641434\n",
      "Epoch 20604/30000 Training Loss: 0.040437474846839905\n",
      "Epoch 20605/30000 Training Loss: 0.05047167092561722\n",
      "Epoch 20606/30000 Training Loss: 0.04106908291578293\n",
      "Epoch 20607/30000 Training Loss: 0.05170375481247902\n",
      "Epoch 20608/30000 Training Loss: 0.04702542722225189\n",
      "Epoch 20609/30000 Training Loss: 0.06296943128108978\n",
      "Epoch 20610/30000 Training Loss: 0.04936998337507248\n",
      "Epoch 20611/30000 Training Loss: 0.04141905903816223\n",
      "Epoch 20612/30000 Training Loss: 0.03284003585577011\n",
      "Epoch 20613/30000 Training Loss: 0.05154597386717796\n",
      "Epoch 20614/30000 Training Loss: 0.05411004275083542\n",
      "Epoch 20615/30000 Training Loss: 0.034772612154483795\n",
      "Epoch 20616/30000 Training Loss: 0.05266992375254631\n",
      "Epoch 20617/30000 Training Loss: 0.04259173199534416\n",
      "Epoch 20618/30000 Training Loss: 0.04079955071210861\n",
      "Epoch 20619/30000 Training Loss: 0.04420620575547218\n",
      "Epoch 20620/30000 Training Loss: 0.040162019431591034\n",
      "Epoch 20621/30000 Training Loss: 0.042193297296762466\n",
      "Epoch 20622/30000 Training Loss: 0.03960715979337692\n",
      "Epoch 20623/30000 Training Loss: 0.03438661992549896\n",
      "Epoch 20624/30000 Training Loss: 0.048534221947193146\n",
      "Epoch 20625/30000 Training Loss: 0.0459601953625679\n",
      "Epoch 20626/30000 Training Loss: 0.05739782378077507\n",
      "Epoch 20627/30000 Training Loss: 0.03646412491798401\n",
      "Epoch 20628/30000 Training Loss: 0.04731506109237671\n",
      "Epoch 20629/30000 Training Loss: 0.04358293488621712\n",
      "Epoch 20630/30000 Training Loss: 0.03411291539669037\n",
      "Epoch 20631/30000 Training Loss: 0.041233718395233154\n",
      "Epoch 20632/30000 Training Loss: 0.03818220645189285\n",
      "Epoch 20633/30000 Training Loss: 0.03793776035308838\n",
      "Epoch 20634/30000 Training Loss: 0.035353776067495346\n",
      "Epoch 20635/30000 Training Loss: 0.03466104716062546\n",
      "Epoch 20636/30000 Training Loss: 0.03126585856080055\n",
      "Epoch 20637/30000 Training Loss: 0.04125681519508362\n",
      "Epoch 20638/30000 Training Loss: 0.04871274158358574\n",
      "Epoch 20639/30000 Training Loss: 0.06125369668006897\n",
      "Epoch 20640/30000 Training Loss: 0.04590063542127609\n",
      "Epoch 20641/30000 Training Loss: 0.048406727612018585\n",
      "Epoch 20642/30000 Training Loss: 0.05691218003630638\n",
      "Epoch 20643/30000 Training Loss: 0.045357029885053635\n",
      "Epoch 20644/30000 Training Loss: 0.046448033303022385\n",
      "Epoch 20645/30000 Training Loss: 0.040449511259794235\n",
      "Epoch 20646/30000 Training Loss: 0.03526948019862175\n",
      "Epoch 20647/30000 Training Loss: 0.031038228422403336\n",
      "Epoch 20648/30000 Training Loss: 0.035216279327869415\n",
      "Epoch 20649/30000 Training Loss: 0.04159899055957794\n",
      "Epoch 20650/30000 Training Loss: 0.029777653515338898\n",
      "Epoch 20651/30000 Training Loss: 0.034647636115550995\n",
      "Epoch 20652/30000 Training Loss: 0.03735886514186859\n",
      "Epoch 20653/30000 Training Loss: 0.03361125290393829\n",
      "Epoch 20654/30000 Training Loss: 0.049113679677248\n",
      "Epoch 20655/30000 Training Loss: 0.03547627478837967\n",
      "Epoch 20656/30000 Training Loss: 0.03485477715730667\n",
      "Epoch 20657/30000 Training Loss: 0.030287541449069977\n",
      "Epoch 20658/30000 Training Loss: 0.03177690505981445\n",
      "Epoch 20659/30000 Training Loss: 0.03663721680641174\n",
      "Epoch 20660/30000 Training Loss: 0.04010103642940521\n",
      "Epoch 20661/30000 Training Loss: 0.04821537435054779\n",
      "Epoch 20662/30000 Training Loss: 0.02923310175538063\n",
      "Epoch 20663/30000 Training Loss: 0.04631565883755684\n",
      "Epoch 20664/30000 Training Loss: 0.029808009043335915\n",
      "Epoch 20665/30000 Training Loss: 0.04611887037754059\n",
      "Epoch 20666/30000 Training Loss: 0.04200667142868042\n",
      "Epoch 20667/30000 Training Loss: 0.055733710527420044\n",
      "Epoch 20668/30000 Training Loss: 0.04470406472682953\n",
      "Epoch 20669/30000 Training Loss: 0.043053388595581055\n",
      "Epoch 20670/30000 Training Loss: 0.03925713151693344\n",
      "Epoch 20671/30000 Training Loss: 0.0559237077832222\n",
      "Epoch 20672/30000 Training Loss: 0.039447516202926636\n",
      "Epoch 20673/30000 Training Loss: 0.043111011385917664\n",
      "Epoch 20674/30000 Training Loss: 0.04192756861448288\n",
      "Epoch 20675/30000 Training Loss: 0.04200189188122749\n",
      "Epoch 20676/30000 Training Loss: 0.05991071090102196\n",
      "Epoch 20677/30000 Training Loss: 0.045192304998636246\n",
      "Epoch 20678/30000 Training Loss: 0.04157070443034172\n",
      "Epoch 20679/30000 Training Loss: 0.03870223090052605\n",
      "Epoch 20680/30000 Training Loss: 0.0566604807972908\n",
      "Epoch 20681/30000 Training Loss: 0.04733806103467941\n",
      "Epoch 20682/30000 Training Loss: 0.046698492020368576\n",
      "Epoch 20683/30000 Training Loss: 0.042755067348480225\n",
      "Epoch 20684/30000 Training Loss: 0.038129374384880066\n",
      "Epoch 20685/30000 Training Loss: 0.04870335012674332\n",
      "Epoch 20686/30000 Training Loss: 0.04632860794663429\n",
      "Epoch 20687/30000 Training Loss: 0.04417106881737709\n",
      "Epoch 20688/30000 Training Loss: 0.039284780621528625\n",
      "Epoch 20689/30000 Training Loss: 0.033048201352357864\n",
      "Epoch 20690/30000 Training Loss: 0.030291695147752762\n",
      "Epoch 20691/30000 Training Loss: 0.037218816578388214\n",
      "Epoch 20692/30000 Training Loss: 0.03689473122358322\n",
      "Epoch 20693/30000 Training Loss: 0.0389658622443676\n",
      "Epoch 20694/30000 Training Loss: 0.04406503960490227\n",
      "Epoch 20695/30000 Training Loss: 0.04748505726456642\n",
      "Epoch 20696/30000 Training Loss: 0.05064433068037033\n",
      "Epoch 20697/30000 Training Loss: 0.04004557803273201\n",
      "Epoch 20698/30000 Training Loss: 0.03365484997630119\n",
      "Epoch 20699/30000 Training Loss: 0.039567310363054276\n",
      "Epoch 20700/30000 Training Loss: 0.03370638191699982\n",
      "Epoch 20700/30000 Validation Loss: 0.04310273751616478\n",
      "Epoch 20701/30000 Training Loss: 0.04420481249690056\n",
      "Epoch 20702/30000 Training Loss: 0.039035700261592865\n",
      "Epoch 20703/30000 Training Loss: 0.03821864351630211\n",
      "Epoch 20704/30000 Training Loss: 0.047316376119852066\n",
      "Epoch 20705/30000 Training Loss: 0.04468037933111191\n",
      "Epoch 20706/30000 Training Loss: 0.043350107967853546\n",
      "Epoch 20707/30000 Training Loss: 0.04380778968334198\n",
      "Epoch 20708/30000 Training Loss: 0.03924143314361572\n",
      "Epoch 20709/30000 Training Loss: 0.037838853895664215\n",
      "Epoch 20710/30000 Training Loss: 0.05124898999929428\n",
      "Epoch 20711/30000 Training Loss: 0.054461464285850525\n",
      "Epoch 20712/30000 Training Loss: 0.051550883799791336\n",
      "Epoch 20713/30000 Training Loss: 0.04715462028980255\n",
      "Epoch 20714/30000 Training Loss: 0.04534818232059479\n",
      "Epoch 20715/30000 Training Loss: 0.04210645705461502\n",
      "Epoch 20716/30000 Training Loss: 0.045711904764175415\n",
      "Epoch 20717/30000 Training Loss: 0.04109126701951027\n",
      "Epoch 20718/30000 Training Loss: 0.04586184397339821\n",
      "Epoch 20719/30000 Training Loss: 0.03504648804664612\n",
      "Epoch 20720/30000 Training Loss: 0.03679347783327103\n",
      "Epoch 20721/30000 Training Loss: 0.05372125655412674\n",
      "Epoch 20722/30000 Training Loss: 0.037083279341459274\n",
      "Epoch 20723/30000 Training Loss: 0.04078888148069382\n",
      "Epoch 20724/30000 Training Loss: 0.05854940786957741\n",
      "Epoch 20725/30000 Training Loss: 0.042634017765522\n",
      "Epoch 20726/30000 Training Loss: 0.040137287229299545\n",
      "Epoch 20727/30000 Training Loss: 0.041641123592853546\n",
      "Epoch 20728/30000 Training Loss: 0.047566793859004974\n",
      "Epoch 20729/30000 Training Loss: 0.0437551811337471\n",
      "Epoch 20730/30000 Training Loss: 0.0523994117975235\n",
      "Epoch 20731/30000 Training Loss: 0.040343597531318665\n",
      "Epoch 20732/30000 Training Loss: 0.03494963422417641\n",
      "Epoch 20733/30000 Training Loss: 0.04251614958047867\n",
      "Epoch 20734/30000 Training Loss: 0.052102938294410706\n",
      "Epoch 20735/30000 Training Loss: 0.04928348585963249\n",
      "Epoch 20736/30000 Training Loss: 0.057852596044540405\n",
      "Epoch 20737/30000 Training Loss: 0.03544948995113373\n",
      "Epoch 20738/30000 Training Loss: 0.054544176906347275\n",
      "Epoch 20739/30000 Training Loss: 0.03625761717557907\n",
      "Epoch 20740/30000 Training Loss: 0.046896371990442276\n",
      "Epoch 20741/30000 Training Loss: 0.04328664019703865\n",
      "Epoch 20742/30000 Training Loss: 0.054718367755413055\n",
      "Epoch 20743/30000 Training Loss: 0.048769477754831314\n",
      "Epoch 20744/30000 Training Loss: 0.04409132897853851\n",
      "Epoch 20745/30000 Training Loss: 0.053735971450805664\n",
      "Epoch 20746/30000 Training Loss: 0.05366554856300354\n",
      "Epoch 20747/30000 Training Loss: 0.03476477414369583\n",
      "Epoch 20748/30000 Training Loss: 0.05822791904211044\n",
      "Epoch 20749/30000 Training Loss: 0.037453703582286835\n",
      "Epoch 20750/30000 Training Loss: 0.06437312066555023\n",
      "Epoch 20751/30000 Training Loss: 0.04199492931365967\n",
      "Epoch 20752/30000 Training Loss: 0.04324515908956528\n",
      "Epoch 20753/30000 Training Loss: 0.05636186897754669\n",
      "Epoch 20754/30000 Training Loss: 0.03735700249671936\n",
      "Epoch 20755/30000 Training Loss: 0.041193410754203796\n",
      "Epoch 20756/30000 Training Loss: 0.039204008877277374\n",
      "Epoch 20757/30000 Training Loss: 0.04418961703777313\n",
      "Epoch 20758/30000 Training Loss: 0.04084634408354759\n",
      "Epoch 20759/30000 Training Loss: 0.044352710247039795\n",
      "Epoch 20760/30000 Training Loss: 0.03900600224733353\n",
      "Epoch 20761/30000 Training Loss: 0.04889225214719772\n",
      "Epoch 20762/30000 Training Loss: 0.034605272114276886\n",
      "Epoch 20763/30000 Training Loss: 0.04153275862336159\n",
      "Epoch 20764/30000 Training Loss: 0.040152259171009064\n",
      "Epoch 20765/30000 Training Loss: 0.04197780787944794\n",
      "Epoch 20766/30000 Training Loss: 0.0446537621319294\n",
      "Epoch 20767/30000 Training Loss: 0.04625315964221954\n",
      "Epoch 20768/30000 Training Loss: 0.04887291043996811\n",
      "Epoch 20769/30000 Training Loss: 0.04621965438127518\n",
      "Epoch 20770/30000 Training Loss: 0.03663109987974167\n",
      "Epoch 20771/30000 Training Loss: 0.048439618200063705\n",
      "Epoch 20772/30000 Training Loss: 0.04576487094163895\n",
      "Epoch 20773/30000 Training Loss: 0.06384497135877609\n",
      "Epoch 20774/30000 Training Loss: 0.043464865535497665\n",
      "Epoch 20775/30000 Training Loss: 0.046227872371673584\n",
      "Epoch 20776/30000 Training Loss: 0.0328906774520874\n",
      "Epoch 20777/30000 Training Loss: 0.031676650047302246\n",
      "Epoch 20778/30000 Training Loss: 0.04108642414212227\n",
      "Epoch 20779/30000 Training Loss: 0.04185699671506882\n",
      "Epoch 20780/30000 Training Loss: 0.04692818969488144\n",
      "Epoch 20781/30000 Training Loss: 0.04911062493920326\n",
      "Epoch 20782/30000 Training Loss: 0.047476254403591156\n",
      "Epoch 20783/30000 Training Loss: 0.03720226511359215\n",
      "Epoch 20784/30000 Training Loss: 0.04197598248720169\n",
      "Epoch 20785/30000 Training Loss: 0.03613554313778877\n",
      "Epoch 20786/30000 Training Loss: 0.036974772810935974\n",
      "Epoch 20787/30000 Training Loss: 0.03380605950951576\n",
      "Epoch 20788/30000 Training Loss: 0.03747043013572693\n",
      "Epoch 20789/30000 Training Loss: 0.04279612377285957\n",
      "Epoch 20790/30000 Training Loss: 0.034864723682403564\n",
      "Epoch 20791/30000 Training Loss: 0.04757339134812355\n",
      "Epoch 20792/30000 Training Loss: 0.055587589740753174\n",
      "Epoch 20793/30000 Training Loss: 0.032162610441446304\n",
      "Epoch 20794/30000 Training Loss: 0.05381142348051071\n",
      "Epoch 20795/30000 Training Loss: 0.049203794449567795\n",
      "Epoch 20796/30000 Training Loss: 0.034077487885951996\n",
      "Epoch 20797/30000 Training Loss: 0.04947599023580551\n",
      "Epoch 20798/30000 Training Loss: 0.03965354338288307\n",
      "Epoch 20799/30000 Training Loss: 0.0423344150185585\n",
      "Epoch 20800/30000 Training Loss: 0.030274897813796997\n",
      "Epoch 20800/30000 Validation Loss: 0.04127468168735504\n",
      "Epoch 20801/30000 Training Loss: 0.05497128888964653\n",
      "Epoch 20802/30000 Training Loss: 0.04947121441364288\n",
      "Epoch 20803/30000 Training Loss: 0.03935297578573227\n",
      "Epoch 20804/30000 Training Loss: 0.05858639255166054\n",
      "Epoch 20805/30000 Training Loss: 0.03498101234436035\n",
      "Epoch 20806/30000 Training Loss: 0.03602667525410652\n",
      "Epoch 20807/30000 Training Loss: 0.036150895059108734\n",
      "Epoch 20808/30000 Training Loss: 0.03806059807538986\n",
      "Epoch 20809/30000 Training Loss: 0.042205069214105606\n",
      "Epoch 20810/30000 Training Loss: 0.040971703827381134\n",
      "Epoch 20811/30000 Training Loss: 0.04661393538117409\n",
      "Epoch 20812/30000 Training Loss: 0.040286943316459656\n",
      "Epoch 20813/30000 Training Loss: 0.037016380578279495\n",
      "Epoch 20814/30000 Training Loss: 0.04805702343583107\n",
      "Epoch 20815/30000 Training Loss: 0.048283979296684265\n",
      "Epoch 20816/30000 Training Loss: 0.04400523006916046\n",
      "Epoch 20817/30000 Training Loss: 0.04176407307386398\n",
      "Epoch 20818/30000 Training Loss: 0.03951643034815788\n",
      "Epoch 20819/30000 Training Loss: 0.037695445120334625\n",
      "Epoch 20820/30000 Training Loss: 0.04200822860002518\n",
      "Epoch 20821/30000 Training Loss: 0.03379707783460617\n",
      "Epoch 20822/30000 Training Loss: 0.052001118659973145\n",
      "Epoch 20823/30000 Training Loss: 0.04327648878097534\n",
      "Epoch 20824/30000 Training Loss: 0.04332350194454193\n",
      "Epoch 20825/30000 Training Loss: 0.054922692477703094\n",
      "Epoch 20826/30000 Training Loss: 0.038052164018154144\n",
      "Epoch 20827/30000 Training Loss: 0.04582880809903145\n",
      "Epoch 20828/30000 Training Loss: 0.037675097584724426\n",
      "Epoch 20829/30000 Training Loss: 0.04701761156320572\n",
      "Epoch 20830/30000 Training Loss: 0.03549770265817642\n",
      "Epoch 20831/30000 Training Loss: 0.04472513869404793\n",
      "Epoch 20832/30000 Training Loss: 0.05133190006017685\n",
      "Epoch 20833/30000 Training Loss: 0.062291961163282394\n",
      "Epoch 20834/30000 Training Loss: 0.044877007603645325\n",
      "Epoch 20835/30000 Training Loss: 0.0538916289806366\n",
      "Epoch 20836/30000 Training Loss: 0.059690140187740326\n",
      "Epoch 20837/30000 Training Loss: 0.043369393795728683\n",
      "Epoch 20838/30000 Training Loss: 0.042220339179039\n",
      "Epoch 20839/30000 Training Loss: 0.030824236571788788\n",
      "Epoch 20840/30000 Training Loss: 0.040229421108961105\n",
      "Epoch 20841/30000 Training Loss: 0.03784077614545822\n",
      "Epoch 20842/30000 Training Loss: 0.047043971717357635\n",
      "Epoch 20843/30000 Training Loss: 0.03939139097929001\n",
      "Epoch 20844/30000 Training Loss: 0.07093168795108795\n",
      "Epoch 20845/30000 Training Loss: 0.04656413197517395\n",
      "Epoch 20846/30000 Training Loss: 0.0413331463932991\n",
      "Epoch 20847/30000 Training Loss: 0.042190179228782654\n",
      "Epoch 20848/30000 Training Loss: 0.049718838185071945\n",
      "Epoch 20849/30000 Training Loss: 0.04735051840543747\n",
      "Epoch 20850/30000 Training Loss: 0.038392022252082825\n",
      "Epoch 20851/30000 Training Loss: 0.038917578756809235\n",
      "Epoch 20852/30000 Training Loss: 0.043864428997039795\n",
      "Epoch 20853/30000 Training Loss: 0.051932163536548615\n",
      "Epoch 20854/30000 Training Loss: 0.041440799832344055\n",
      "Epoch 20855/30000 Training Loss: 0.03950832411646843\n",
      "Epoch 20856/30000 Training Loss: 0.0400315523147583\n",
      "Epoch 20857/30000 Training Loss: 0.04764381796121597\n",
      "Epoch 20858/30000 Training Loss: 0.04060140624642372\n",
      "Epoch 20859/30000 Training Loss: 0.042813740670681\n",
      "Epoch 20860/30000 Training Loss: 0.052959103137254715\n",
      "Epoch 20861/30000 Training Loss: 0.04375478997826576\n",
      "Epoch 20862/30000 Training Loss: 0.058514416217803955\n",
      "Epoch 20863/30000 Training Loss: 0.033457301557064056\n",
      "Epoch 20864/30000 Training Loss: 0.034710176289081573\n",
      "Epoch 20865/30000 Training Loss: 0.04445252940058708\n",
      "Epoch 20866/30000 Training Loss: 0.054371438920497894\n",
      "Epoch 20867/30000 Training Loss: 0.03760802745819092\n",
      "Epoch 20868/30000 Training Loss: 0.05940145254135132\n",
      "Epoch 20869/30000 Training Loss: 0.05945364758372307\n",
      "Epoch 20870/30000 Training Loss: 0.04299335926771164\n",
      "Epoch 20871/30000 Training Loss: 0.03791732341051102\n",
      "Epoch 20872/30000 Training Loss: 0.05265989527106285\n",
      "Epoch 20873/30000 Training Loss: 0.03831997513771057\n",
      "Epoch 20874/30000 Training Loss: 0.043054766952991486\n",
      "Epoch 20875/30000 Training Loss: 0.03187597543001175\n",
      "Epoch 20876/30000 Training Loss: 0.05039314180612564\n",
      "Epoch 20877/30000 Training Loss: 0.06239953637123108\n",
      "Epoch 20878/30000 Training Loss: 0.04236568883061409\n",
      "Epoch 20879/30000 Training Loss: 0.046897538006305695\n",
      "Epoch 20880/30000 Training Loss: 0.03894273191690445\n",
      "Epoch 20881/30000 Training Loss: 0.05350456386804581\n",
      "Epoch 20882/30000 Training Loss: 0.03435324877500534\n",
      "Epoch 20883/30000 Training Loss: 0.047594837844371796\n",
      "Epoch 20884/30000 Training Loss: 0.04736143350601196\n",
      "Epoch 20885/30000 Training Loss: 0.051380082964897156\n",
      "Epoch 20886/30000 Training Loss: 0.034827277064323425\n",
      "Epoch 20887/30000 Training Loss: 0.034414805471897125\n",
      "Epoch 20888/30000 Training Loss: 0.051163047552108765\n",
      "Epoch 20889/30000 Training Loss: 0.037665244191884995\n",
      "Epoch 20890/30000 Training Loss: 0.037585243582725525\n",
      "Epoch 20891/30000 Training Loss: 0.03525260090827942\n",
      "Epoch 20892/30000 Training Loss: 0.036518644541502\n",
      "Epoch 20893/30000 Training Loss: 0.05499214679002762\n",
      "Epoch 20894/30000 Training Loss: 0.03359299898147583\n",
      "Epoch 20895/30000 Training Loss: 0.04512467235326767\n",
      "Epoch 20896/30000 Training Loss: 0.033220306038856506\n",
      "Epoch 20897/30000 Training Loss: 0.04731227457523346\n",
      "Epoch 20898/30000 Training Loss: 0.05644725635647774\n",
      "Epoch 20899/30000 Training Loss: 0.03494393825531006\n",
      "Epoch 20900/30000 Training Loss: 0.051147837191820145\n",
      "Epoch 20900/30000 Validation Loss: 0.04378730058670044\n",
      "Epoch 20901/30000 Training Loss: 0.04486298933625221\n",
      "Epoch 20902/30000 Training Loss: 0.04812254756689072\n",
      "Epoch 20903/30000 Training Loss: 0.044076915830373764\n",
      "Epoch 20904/30000 Training Loss: 0.04445738345384598\n",
      "Epoch 20905/30000 Training Loss: 0.045253969728946686\n",
      "Epoch 20906/30000 Training Loss: 0.05024949461221695\n",
      "Epoch 20907/30000 Training Loss: 0.04618554562330246\n",
      "Epoch 20908/30000 Training Loss: 0.0396493598818779\n",
      "Epoch 20909/30000 Training Loss: 0.04895760491490364\n",
      "Epoch 20910/30000 Training Loss: 0.05053240433335304\n",
      "Epoch 20911/30000 Training Loss: 0.03264518082141876\n",
      "Epoch 20912/30000 Training Loss: 0.038630686700344086\n",
      "Epoch 20913/30000 Training Loss: 0.05266135185956955\n",
      "Epoch 20914/30000 Training Loss: 0.03750801831483841\n",
      "Epoch 20915/30000 Training Loss: 0.038544945418834686\n",
      "Epoch 20916/30000 Training Loss: 0.03905105963349342\n",
      "Epoch 20917/30000 Training Loss: 0.05722567066550255\n",
      "Epoch 20918/30000 Training Loss: 0.05249829590320587\n",
      "Epoch 20919/30000 Training Loss: 0.05696916952729225\n",
      "Epoch 20920/30000 Training Loss: 0.049441590905189514\n",
      "Epoch 20921/30000 Training Loss: 0.04927678778767586\n",
      "Epoch 20922/30000 Training Loss: 0.04233340173959732\n",
      "Epoch 20923/30000 Training Loss: 0.03960539400577545\n",
      "Epoch 20924/30000 Training Loss: 0.04468168318271637\n",
      "Epoch 20925/30000 Training Loss: 0.031421586871147156\n",
      "Epoch 20926/30000 Training Loss: 0.04978044331073761\n",
      "Epoch 20927/30000 Training Loss: 0.03915989026427269\n",
      "Epoch 20928/30000 Training Loss: 0.045129209756851196\n",
      "Epoch 20929/30000 Training Loss: 0.044909827411174774\n",
      "Epoch 20930/30000 Training Loss: 0.044446032494306564\n",
      "Epoch 20931/30000 Training Loss: 0.03544685244560242\n",
      "Epoch 20932/30000 Training Loss: 0.03331742435693741\n",
      "Epoch 20933/30000 Training Loss: 0.040241409093141556\n",
      "Epoch 20934/30000 Training Loss: 0.0340217724442482\n",
      "Epoch 20935/30000 Training Loss: 0.0427115261554718\n",
      "Epoch 20936/30000 Training Loss: 0.03109731152653694\n",
      "Epoch 20937/30000 Training Loss: 0.049364812672138214\n",
      "Epoch 20938/30000 Training Loss: 0.048974957317113876\n",
      "Epoch 20939/30000 Training Loss: 0.033172857016325\n",
      "Epoch 20940/30000 Training Loss: 0.05495467036962509\n",
      "Epoch 20941/30000 Training Loss: 0.03988334536552429\n",
      "Epoch 20942/30000 Training Loss: 0.05417291820049286\n",
      "Epoch 20943/30000 Training Loss: 0.05156837776303291\n",
      "Epoch 20944/30000 Training Loss: 0.03881494700908661\n",
      "Epoch 20945/30000 Training Loss: 0.037586890161037445\n",
      "Epoch 20946/30000 Training Loss: 0.050423040986061096\n",
      "Epoch 20947/30000 Training Loss: 0.03760451078414917\n",
      "Epoch 20948/30000 Training Loss: 0.04285528510808945\n",
      "Epoch 20949/30000 Training Loss: 0.037577420473098755\n",
      "Epoch 20950/30000 Training Loss: 0.03856857866048813\n",
      "Epoch 20951/30000 Training Loss: 0.05473896861076355\n",
      "Epoch 20952/30000 Training Loss: 0.03526591509580612\n",
      "Epoch 20953/30000 Training Loss: 0.04975956678390503\n",
      "Epoch 20954/30000 Training Loss: 0.03477700427174568\n",
      "Epoch 20955/30000 Training Loss: 0.0301201269030571\n",
      "Epoch 20956/30000 Training Loss: 0.044837310910224915\n",
      "Epoch 20957/30000 Training Loss: 0.06380835175514221\n",
      "Epoch 20958/30000 Training Loss: 0.04402079060673714\n",
      "Epoch 20959/30000 Training Loss: 0.03957962244749069\n",
      "Epoch 20960/30000 Training Loss: 0.049970075488090515\n",
      "Epoch 20961/30000 Training Loss: 0.05298317223787308\n",
      "Epoch 20962/30000 Training Loss: 0.03779555857181549\n",
      "Epoch 20963/30000 Training Loss: 0.05105941742658615\n",
      "Epoch 20964/30000 Training Loss: 0.0557355061173439\n",
      "Epoch 20965/30000 Training Loss: 0.05060117319226265\n",
      "Epoch 20966/30000 Training Loss: 0.0395759753882885\n",
      "Epoch 20967/30000 Training Loss: 0.03284739702939987\n",
      "Epoch 20968/30000 Training Loss: 0.04254800081253052\n",
      "Epoch 20969/30000 Training Loss: 0.053869422525167465\n",
      "Epoch 20970/30000 Training Loss: 0.04894737899303436\n",
      "Epoch 20971/30000 Training Loss: 0.04582299292087555\n",
      "Epoch 20972/30000 Training Loss: 0.04573243483901024\n",
      "Epoch 20973/30000 Training Loss: 0.03708137944340706\n",
      "Epoch 20974/30000 Training Loss: 0.05508747696876526\n",
      "Epoch 20975/30000 Training Loss: 0.04445091262459755\n",
      "Epoch 20976/30000 Training Loss: 0.04438857361674309\n",
      "Epoch 20977/30000 Training Loss: 0.0383991003036499\n",
      "Epoch 20978/30000 Training Loss: 0.05378584936261177\n",
      "Epoch 20979/30000 Training Loss: 0.05160370096564293\n",
      "Epoch 20980/30000 Training Loss: 0.04350131005048752\n",
      "Epoch 20981/30000 Training Loss: 0.06050167977809906\n",
      "Epoch 20982/30000 Training Loss: 0.03765663504600525\n",
      "Epoch 20983/30000 Training Loss: 0.03440658375620842\n",
      "Epoch 20984/30000 Training Loss: 0.05329698324203491\n",
      "Epoch 20985/30000 Training Loss: 0.04389644414186478\n",
      "Epoch 20986/30000 Training Loss: 0.04200855642557144\n",
      "Epoch 20987/30000 Training Loss: 0.04495585337281227\n",
      "Epoch 20988/30000 Training Loss: 0.05556275695562363\n",
      "Epoch 20989/30000 Training Loss: 0.04134689271450043\n",
      "Epoch 20990/30000 Training Loss: 0.0412786528468132\n",
      "Epoch 20991/30000 Training Loss: 0.0380391925573349\n",
      "Epoch 20992/30000 Training Loss: 0.03950574994087219\n",
      "Epoch 20993/30000 Training Loss: 0.04870959371328354\n",
      "Epoch 20994/30000 Training Loss: 0.0268363356590271\n",
      "Epoch 20995/30000 Training Loss: 0.042782776057720184\n",
      "Epoch 20996/30000 Training Loss: 0.052615873515605927\n",
      "Epoch 20997/30000 Training Loss: 0.06142222881317139\n",
      "Epoch 20998/30000 Training Loss: 0.05055226385593414\n",
      "Epoch 20999/30000 Training Loss: 0.046701934188604355\n",
      "Epoch 21000/30000 Training Loss: 0.05097315460443497\n",
      "Epoch 21000/30000 Validation Loss: 0.03629285842180252\n",
      "Epoch 21001/30000 Training Loss: 0.03800103813409805\n",
      "Epoch 21002/30000 Training Loss: 0.03649381548166275\n",
      "Epoch 21003/30000 Training Loss: 0.0474863201379776\n",
      "Epoch 21004/30000 Training Loss: 0.053295642137527466\n",
      "Epoch 21005/30000 Training Loss: 0.04617093503475189\n",
      "Epoch 21006/30000 Training Loss: 0.03250165656208992\n",
      "Epoch 21007/30000 Training Loss: 0.05638878047466278\n",
      "Epoch 21008/30000 Training Loss: 0.04550422728061676\n",
      "Epoch 21009/30000 Training Loss: 0.03993445262312889\n",
      "Epoch 21010/30000 Training Loss: 0.0477326475083828\n",
      "Epoch 21011/30000 Training Loss: 0.051766976714134216\n",
      "Epoch 21012/30000 Training Loss: 0.04325149208307266\n",
      "Epoch 21013/30000 Training Loss: 0.05476166680455208\n",
      "Epoch 21014/30000 Training Loss: 0.041091807186603546\n",
      "Epoch 21015/30000 Training Loss: 0.05223267525434494\n",
      "Epoch 21016/30000 Training Loss: 0.043758027255535126\n",
      "Epoch 21017/30000 Training Loss: 0.044622860848903656\n",
      "Epoch 21018/30000 Training Loss: 0.05155280604958534\n",
      "Epoch 21019/30000 Training Loss: 0.04487011581659317\n",
      "Epoch 21020/30000 Training Loss: 0.034635115414857864\n",
      "Epoch 21021/30000 Training Loss: 0.041197989135980606\n",
      "Epoch 21022/30000 Training Loss: 0.03664608299732208\n",
      "Epoch 21023/30000 Training Loss: 0.0454978346824646\n",
      "Epoch 21024/30000 Training Loss: 0.0317927822470665\n",
      "Epoch 21025/30000 Training Loss: 0.04425172507762909\n",
      "Epoch 21026/30000 Training Loss: 0.04340915009379387\n",
      "Epoch 21027/30000 Training Loss: 0.046619266271591187\n",
      "Epoch 21028/30000 Training Loss: 0.03357689082622528\n",
      "Epoch 21029/30000 Training Loss: 0.05281984061002731\n",
      "Epoch 21030/30000 Training Loss: 0.049646176397800446\n",
      "Epoch 21031/30000 Training Loss: 0.042326919734478\n",
      "Epoch 21032/30000 Training Loss: 0.04206284135580063\n",
      "Epoch 21033/30000 Training Loss: 0.04251826927065849\n",
      "Epoch 21034/30000 Training Loss: 0.0500035434961319\n",
      "Epoch 21035/30000 Training Loss: 0.03449272736907005\n",
      "Epoch 21036/30000 Training Loss: 0.047731876373291016\n",
      "Epoch 21037/30000 Training Loss: 0.03945594280958176\n",
      "Epoch 21038/30000 Training Loss: 0.05018359795212746\n",
      "Epoch 21039/30000 Training Loss: 0.04360976815223694\n",
      "Epoch 21040/30000 Training Loss: 0.033376988023519516\n",
      "Epoch 21041/30000 Training Loss: 0.04451775550842285\n",
      "Epoch 21042/30000 Training Loss: 0.03546886146068573\n",
      "Epoch 21043/30000 Training Loss: 0.029072940349578857\n",
      "Epoch 21044/30000 Training Loss: 0.054873786866664886\n",
      "Epoch 21045/30000 Training Loss: 0.05303988978266716\n",
      "Epoch 21046/30000 Training Loss: 0.028441790491342545\n",
      "Epoch 21047/30000 Training Loss: 0.03269977122545242\n",
      "Epoch 21048/30000 Training Loss: 0.04320438951253891\n",
      "Epoch 21049/30000 Training Loss: 0.04211084544658661\n",
      "Epoch 21050/30000 Training Loss: 0.03586743772029877\n",
      "Epoch 21051/30000 Training Loss: 0.06526192277669907\n",
      "Epoch 21052/30000 Training Loss: 0.03382425755262375\n",
      "Epoch 21053/30000 Training Loss: 0.03315567597746849\n",
      "Epoch 21054/30000 Training Loss: 0.031671278178691864\n",
      "Epoch 21055/30000 Training Loss: 0.053233057260513306\n",
      "Epoch 21056/30000 Training Loss: 0.04302241653203964\n",
      "Epoch 21057/30000 Training Loss: 0.04132799804210663\n",
      "Epoch 21058/30000 Training Loss: 0.05332780256867409\n",
      "Epoch 21059/30000 Training Loss: 0.0519108772277832\n",
      "Epoch 21060/30000 Training Loss: 0.030819443985819817\n",
      "Epoch 21061/30000 Training Loss: 0.05290373042225838\n",
      "Epoch 21062/30000 Training Loss: 0.03850579634308815\n",
      "Epoch 21063/30000 Training Loss: 0.034511663019657135\n",
      "Epoch 21064/30000 Training Loss: 0.04438488185405731\n",
      "Epoch 21065/30000 Training Loss: 0.06716295331716537\n",
      "Epoch 21066/30000 Training Loss: 0.043163541704416275\n",
      "Epoch 21067/30000 Training Loss: 0.03978336602449417\n",
      "Epoch 21068/30000 Training Loss: 0.05570170283317566\n",
      "Epoch 21069/30000 Training Loss: 0.034115880727767944\n",
      "Epoch 21070/30000 Training Loss: 0.04346632957458496\n",
      "Epoch 21071/30000 Training Loss: 0.04922531545162201\n",
      "Epoch 21072/30000 Training Loss: 0.04603489115834236\n",
      "Epoch 21073/30000 Training Loss: 0.03680817410349846\n",
      "Epoch 21074/30000 Training Loss: 0.04594354331493378\n",
      "Epoch 21075/30000 Training Loss: 0.04044713079929352\n",
      "Epoch 21076/30000 Training Loss: 0.03318950906395912\n",
      "Epoch 21077/30000 Training Loss: 0.03284018859267235\n",
      "Epoch 21078/30000 Training Loss: 0.03889916092157364\n",
      "Epoch 21079/30000 Training Loss: 0.04380752891302109\n",
      "Epoch 21080/30000 Training Loss: 0.037516701966524124\n",
      "Epoch 21081/30000 Training Loss: 0.04282819479703903\n",
      "Epoch 21082/30000 Training Loss: 0.04726714268326759\n",
      "Epoch 21083/30000 Training Loss: 0.04833787679672241\n",
      "Epoch 21084/30000 Training Loss: 0.037246860563755035\n",
      "Epoch 21085/30000 Training Loss: 0.03880871832370758\n",
      "Epoch 21086/30000 Training Loss: 0.048549771308898926\n",
      "Epoch 21087/30000 Training Loss: 0.04540654271841049\n",
      "Epoch 21088/30000 Training Loss: 0.04203210026025772\n",
      "Epoch 21089/30000 Training Loss: 0.039170317351818085\n",
      "Epoch 21090/30000 Training Loss: 0.02599000185728073\n",
      "Epoch 21091/30000 Training Loss: 0.04716343805193901\n",
      "Epoch 21092/30000 Training Loss: 0.04696639999747276\n",
      "Epoch 21093/30000 Training Loss: 0.041604332625865936\n",
      "Epoch 21094/30000 Training Loss: 0.04574326053261757\n",
      "Epoch 21095/30000 Training Loss: 0.04208182916045189\n",
      "Epoch 21096/30000 Training Loss: 0.0370514914393425\n",
      "Epoch 21097/30000 Training Loss: 0.03570418432354927\n",
      "Epoch 21098/30000 Training Loss: 0.03698134794831276\n",
      "Epoch 21099/30000 Training Loss: 0.044166021049022675\n",
      "Epoch 21100/30000 Training Loss: 0.051622286438941956\n",
      "Epoch 21100/30000 Validation Loss: 0.04535031318664551\n",
      "Epoch 21101/30000 Training Loss: 0.047129470854997635\n",
      "Epoch 21102/30000 Training Loss: 0.04156750440597534\n",
      "Epoch 21103/30000 Training Loss: 0.03896566480398178\n",
      "Epoch 21104/30000 Training Loss: 0.03294079750776291\n",
      "Epoch 21105/30000 Training Loss: 0.04091337323188782\n",
      "Epoch 21106/30000 Training Loss: 0.04523646831512451\n",
      "Epoch 21107/30000 Training Loss: 0.0376947820186615\n",
      "Epoch 21108/30000 Training Loss: 0.05413110926747322\n",
      "Epoch 21109/30000 Training Loss: 0.05682225525379181\n",
      "Epoch 21110/30000 Training Loss: 0.03700341284275055\n",
      "Epoch 21111/30000 Training Loss: 0.044421132653951645\n",
      "Epoch 21112/30000 Training Loss: 0.0457804873585701\n",
      "Epoch 21113/30000 Training Loss: 0.037705134600400925\n",
      "Epoch 21114/30000 Training Loss: 0.03862250968813896\n",
      "Epoch 21115/30000 Training Loss: 0.04416542872786522\n",
      "Epoch 21116/30000 Training Loss: 0.048095159232616425\n",
      "Epoch 21117/30000 Training Loss: 0.04155988618731499\n",
      "Epoch 21118/30000 Training Loss: 0.05059213563799858\n",
      "Epoch 21119/30000 Training Loss: 0.041153475642204285\n",
      "Epoch 21120/30000 Training Loss: 0.042923226952552795\n",
      "Epoch 21121/30000 Training Loss: 0.05538518726825714\n",
      "Epoch 21122/30000 Training Loss: 0.038532041013240814\n",
      "Epoch 21123/30000 Training Loss: 0.04835882782936096\n",
      "Epoch 21124/30000 Training Loss: 0.04108254611492157\n",
      "Epoch 21125/30000 Training Loss: 0.036669373512268066\n",
      "Epoch 21126/30000 Training Loss: 0.04009797051548958\n",
      "Epoch 21127/30000 Training Loss: 0.046114832162857056\n",
      "Epoch 21128/30000 Training Loss: 0.03756332024931908\n",
      "Epoch 21129/30000 Training Loss: 0.040359482169151306\n",
      "Epoch 21130/30000 Training Loss: 0.054943036288022995\n",
      "Epoch 21131/30000 Training Loss: 0.03807968273758888\n",
      "Epoch 21132/30000 Training Loss: 0.03837044537067413\n",
      "Epoch 21133/30000 Training Loss: 0.043349552899599075\n",
      "Epoch 21134/30000 Training Loss: 0.061543360352516174\n",
      "Epoch 21135/30000 Training Loss: 0.054271239787340164\n",
      "Epoch 21136/30000 Training Loss: 0.04098152369260788\n",
      "Epoch 21137/30000 Training Loss: 0.042199622839689255\n",
      "Epoch 21138/30000 Training Loss: 0.03226609528064728\n",
      "Epoch 21139/30000 Training Loss: 0.040899939835071564\n",
      "Epoch 21140/30000 Training Loss: 0.03614829480648041\n",
      "Epoch 21141/30000 Training Loss: 0.04105116426944733\n",
      "Epoch 21142/30000 Training Loss: 0.03893277794122696\n",
      "Epoch 21143/30000 Training Loss: 0.046886004507541656\n",
      "Epoch 21144/30000 Training Loss: 0.06223911792039871\n",
      "Epoch 21145/30000 Training Loss: 0.03943582996726036\n",
      "Epoch 21146/30000 Training Loss: 0.04632610082626343\n",
      "Epoch 21147/30000 Training Loss: 0.04551747813820839\n",
      "Epoch 21148/30000 Training Loss: 0.03925924003124237\n",
      "Epoch 21149/30000 Training Loss: 0.04696515575051308\n",
      "Epoch 21150/30000 Training Loss: 0.04025063291192055\n",
      "Epoch 21151/30000 Training Loss: 0.0491638109087944\n",
      "Epoch 21152/30000 Training Loss: 0.04468482732772827\n",
      "Epoch 21153/30000 Training Loss: 0.03774810582399368\n",
      "Epoch 21154/30000 Training Loss: 0.04075922816991806\n",
      "Epoch 21155/30000 Training Loss: 0.04985996335744858\n",
      "Epoch 21156/30000 Training Loss: 0.05171158164739609\n",
      "Epoch 21157/30000 Training Loss: 0.04210098832845688\n",
      "Epoch 21158/30000 Training Loss: 0.048080407083034515\n",
      "Epoch 21159/30000 Training Loss: 0.058902159333229065\n",
      "Epoch 21160/30000 Training Loss: 0.041218921542167664\n",
      "Epoch 21161/30000 Training Loss: 0.04822986572980881\n",
      "Epoch 21162/30000 Training Loss: 0.03577493131160736\n",
      "Epoch 21163/30000 Training Loss: 0.044775448739528656\n",
      "Epoch 21164/30000 Training Loss: 0.034345805644989014\n",
      "Epoch 21165/30000 Training Loss: 0.05516330152750015\n",
      "Epoch 21166/30000 Training Loss: 0.043014660477638245\n",
      "Epoch 21167/30000 Training Loss: 0.05302821844816208\n",
      "Epoch 21168/30000 Training Loss: 0.04858297109603882\n",
      "Epoch 21169/30000 Training Loss: 0.051411330699920654\n",
      "Epoch 21170/30000 Training Loss: 0.04331821948289871\n",
      "Epoch 21171/30000 Training Loss: 0.038628410547971725\n",
      "Epoch 21172/30000 Training Loss: 0.04095238447189331\n",
      "Epoch 21173/30000 Training Loss: 0.039839006960392\n",
      "Epoch 21174/30000 Training Loss: 0.041179392486810684\n",
      "Epoch 21175/30000 Training Loss: 0.051159828901290894\n",
      "Epoch 21176/30000 Training Loss: 0.03738977015018463\n",
      "Epoch 21177/30000 Training Loss: 0.031470887362957\n",
      "Epoch 21178/30000 Training Loss: 0.04683970659971237\n",
      "Epoch 21179/30000 Training Loss: 0.04044307395815849\n",
      "Epoch 21180/30000 Training Loss: 0.048304542899131775\n",
      "Epoch 21181/30000 Training Loss: 0.03612667694687843\n",
      "Epoch 21182/30000 Training Loss: 0.051388535648584366\n",
      "Epoch 21183/30000 Training Loss: 0.037755366414785385\n",
      "Epoch 21184/30000 Training Loss: 0.04500461369752884\n",
      "Epoch 21185/30000 Training Loss: 0.038517773151397705\n",
      "Epoch 21186/30000 Training Loss: 0.03441501408815384\n",
      "Epoch 21187/30000 Training Loss: 0.038358159363269806\n",
      "Epoch 21188/30000 Training Loss: 0.04520583152770996\n",
      "Epoch 21189/30000 Training Loss: 0.05003834143280983\n",
      "Epoch 21190/30000 Training Loss: 0.03627729415893555\n",
      "Epoch 21191/30000 Training Loss: 0.04578345641493797\n",
      "Epoch 21192/30000 Training Loss: 0.03485368937253952\n",
      "Epoch 21193/30000 Training Loss: 0.05676843598484993\n",
      "Epoch 21194/30000 Training Loss: 0.04362471401691437\n",
      "Epoch 21195/30000 Training Loss: 0.03990105167031288\n",
      "Epoch 21196/30000 Training Loss: 0.036415405571460724\n",
      "Epoch 21197/30000 Training Loss: 0.03783575817942619\n",
      "Epoch 21198/30000 Training Loss: 0.049964845180511475\n",
      "Epoch 21199/30000 Training Loss: 0.03872968256473541\n",
      "Epoch 21200/30000 Training Loss: 0.045484453439712524\n",
      "Epoch 21200/30000 Validation Loss: 0.03884950280189514\n",
      "Epoch 21201/30000 Training Loss: 0.046838730573654175\n",
      "Epoch 21202/30000 Training Loss: 0.03836303949356079\n",
      "Epoch 21203/30000 Training Loss: 0.03814396262168884\n",
      "Epoch 21204/30000 Training Loss: 0.04805053025484085\n",
      "Epoch 21205/30000 Training Loss: 0.0380282998085022\n",
      "Epoch 21206/30000 Training Loss: 0.02883208356797695\n",
      "Epoch 21207/30000 Training Loss: 0.03617248684167862\n",
      "Epoch 21208/30000 Training Loss: 0.036284543573856354\n",
      "Epoch 21209/30000 Training Loss: 0.055335454642772675\n",
      "Epoch 21210/30000 Training Loss: 0.04503185302019119\n",
      "Epoch 21211/30000 Training Loss: 0.030056403949856758\n",
      "Epoch 21212/30000 Training Loss: 0.03838337957859039\n",
      "Epoch 21213/30000 Training Loss: 0.04608885198831558\n",
      "Epoch 21214/30000 Training Loss: 0.036129195243120193\n",
      "Epoch 21215/30000 Training Loss: 0.06635163724422455\n",
      "Epoch 21216/30000 Training Loss: 0.05207923427224159\n",
      "Epoch 21217/30000 Training Loss: 0.0414658859372139\n",
      "Epoch 21218/30000 Training Loss: 0.03696485608816147\n",
      "Epoch 21219/30000 Training Loss: 0.035601165145635605\n",
      "Epoch 21220/30000 Training Loss: 0.04812384396791458\n",
      "Epoch 21221/30000 Training Loss: 0.04302098602056503\n",
      "Epoch 21222/30000 Training Loss: 0.05647176504135132\n",
      "Epoch 21223/30000 Training Loss: 0.0427241176366806\n",
      "Epoch 21224/30000 Training Loss: 0.043843112885951996\n",
      "Epoch 21225/30000 Training Loss: 0.05522845685482025\n",
      "Epoch 21226/30000 Training Loss: 0.031889379024505615\n",
      "Epoch 21227/30000 Training Loss: 0.04498271644115448\n",
      "Epoch 21228/30000 Training Loss: 0.04558978229761124\n",
      "Epoch 21229/30000 Training Loss: 0.04823259264230728\n",
      "Epoch 21230/30000 Training Loss: 0.02899840474128723\n",
      "Epoch 21231/30000 Training Loss: 0.04576171934604645\n",
      "Epoch 21232/30000 Training Loss: 0.05217921361327171\n",
      "Epoch 21233/30000 Training Loss: 0.039741501212120056\n",
      "Epoch 21234/30000 Training Loss: 0.047706253826618195\n",
      "Epoch 21235/30000 Training Loss: 0.046755023300647736\n",
      "Epoch 21236/30000 Training Loss: 0.04373008757829666\n",
      "Epoch 21237/30000 Training Loss: 0.03876205533742905\n",
      "Epoch 21238/30000 Training Loss: 0.04555041342973709\n",
      "Epoch 21239/30000 Training Loss: 0.03865261748433113\n",
      "Epoch 21240/30000 Training Loss: 0.03988310694694519\n",
      "Epoch 21241/30000 Training Loss: 0.05092187225818634\n",
      "Epoch 21242/30000 Training Loss: 0.04050488770008087\n",
      "Epoch 21243/30000 Training Loss: 0.04133012890815735\n",
      "Epoch 21244/30000 Training Loss: 0.037189751863479614\n",
      "Epoch 21245/30000 Training Loss: 0.05017687380313873\n",
      "Epoch 21246/30000 Training Loss: 0.038380153477191925\n",
      "Epoch 21247/30000 Training Loss: 0.041178394109010696\n",
      "Epoch 21248/30000 Training Loss: 0.03919415548443794\n",
      "Epoch 21249/30000 Training Loss: 0.044120751321315765\n",
      "Epoch 21250/30000 Training Loss: 0.029066551476716995\n",
      "Epoch 21251/30000 Training Loss: 0.033647820353507996\n",
      "Epoch 21252/30000 Training Loss: 0.0323774591088295\n",
      "Epoch 21253/30000 Training Loss: 0.04751971364021301\n",
      "Epoch 21254/30000 Training Loss: 0.04029740393161774\n",
      "Epoch 21255/30000 Training Loss: 0.04902253299951553\n",
      "Epoch 21256/30000 Training Loss: 0.04572221636772156\n",
      "Epoch 21257/30000 Training Loss: 0.04032013565301895\n",
      "Epoch 21258/30000 Training Loss: 0.037722669541835785\n",
      "Epoch 21259/30000 Training Loss: 0.037371885031461716\n",
      "Epoch 21260/30000 Training Loss: 0.04302171990275383\n",
      "Epoch 21261/30000 Training Loss: 0.04365353286266327\n",
      "Epoch 21262/30000 Training Loss: 0.03534495830535889\n",
      "Epoch 21263/30000 Training Loss: 0.0634865015745163\n",
      "Epoch 21264/30000 Training Loss: 0.037233829498291016\n",
      "Epoch 21265/30000 Training Loss: 0.050881218165159225\n",
      "Epoch 21266/30000 Training Loss: 0.043437328189611435\n",
      "Epoch 21267/30000 Training Loss: 0.04012194275856018\n",
      "Epoch 21268/30000 Training Loss: 0.05053623020648956\n",
      "Epoch 21269/30000 Training Loss: 0.045076895505189896\n",
      "Epoch 21270/30000 Training Loss: 0.04585064947605133\n",
      "Epoch 21271/30000 Training Loss: 0.04272616654634476\n",
      "Epoch 21272/30000 Training Loss: 0.05906941741704941\n",
      "Epoch 21273/30000 Training Loss: 0.04430583119392395\n",
      "Epoch 21274/30000 Training Loss: 0.04573391377925873\n",
      "Epoch 21275/30000 Training Loss: 0.04553607478737831\n",
      "Epoch 21276/30000 Training Loss: 0.04298613220453262\n",
      "Epoch 21277/30000 Training Loss: 0.027514532208442688\n",
      "Epoch 21278/30000 Training Loss: 0.035881176590919495\n",
      "Epoch 21279/30000 Training Loss: 0.041032060980796814\n",
      "Epoch 21280/30000 Training Loss: 0.04045092314481735\n",
      "Epoch 21281/30000 Training Loss: 0.05664657801389694\n",
      "Epoch 21282/30000 Training Loss: 0.03771485388278961\n",
      "Epoch 21283/30000 Training Loss: 0.045150525867938995\n",
      "Epoch 21284/30000 Training Loss: 0.037894003093242645\n",
      "Epoch 21285/30000 Training Loss: 0.03779727593064308\n",
      "Epoch 21286/30000 Training Loss: 0.04325246810913086\n",
      "Epoch 21287/30000 Training Loss: 0.033823609352111816\n",
      "Epoch 21288/30000 Training Loss: 0.04872509837150574\n",
      "Epoch 21289/30000 Training Loss: 0.03757481276988983\n",
      "Epoch 21290/30000 Training Loss: 0.04027976095676422\n",
      "Epoch 21291/30000 Training Loss: 0.03859669715166092\n",
      "Epoch 21292/30000 Training Loss: 0.05330914258956909\n",
      "Epoch 21293/30000 Training Loss: 0.04167170077562332\n",
      "Epoch 21294/30000 Training Loss: 0.054645322263240814\n",
      "Epoch 21295/30000 Training Loss: 0.04979760944843292\n",
      "Epoch 21296/30000 Training Loss: 0.04492351412773132\n",
      "Epoch 21297/30000 Training Loss: 0.04662255197763443\n",
      "Epoch 21298/30000 Training Loss: 0.032362647354602814\n",
      "Epoch 21299/30000 Training Loss: 0.04637246951460838\n",
      "Epoch 21300/30000 Training Loss: 0.036376677453517914\n",
      "Epoch 21300/30000 Validation Loss: 0.03823136165738106\n",
      "Epoch 21301/30000 Training Loss: 0.04393438249826431\n",
      "Epoch 21302/30000 Training Loss: 0.04088215529918671\n",
      "Epoch 21303/30000 Training Loss: 0.05286461114883423\n",
      "Epoch 21304/30000 Training Loss: 0.035357534885406494\n",
      "Epoch 21305/30000 Training Loss: 0.04773109033703804\n",
      "Epoch 21306/30000 Training Loss: 0.04364602267742157\n",
      "Epoch 21307/30000 Training Loss: 0.042459674179553986\n",
      "Epoch 21308/30000 Training Loss: 0.05003953352570534\n",
      "Epoch 21309/30000 Training Loss: 0.03086688369512558\n",
      "Epoch 21310/30000 Training Loss: 0.049796730279922485\n",
      "Epoch 21311/30000 Training Loss: 0.0423426479101181\n",
      "Epoch 21312/30000 Training Loss: 0.04100792482495308\n",
      "Epoch 21313/30000 Training Loss: 0.04006722941994667\n",
      "Epoch 21314/30000 Training Loss: 0.03927856683731079\n",
      "Epoch 21315/30000 Training Loss: 0.03786244988441467\n",
      "Epoch 21316/30000 Training Loss: 0.04395189508795738\n",
      "Epoch 21317/30000 Training Loss: 0.04343978315591812\n",
      "Epoch 21318/30000 Training Loss: 0.040723927319049835\n",
      "Epoch 21319/30000 Training Loss: 0.06512639671564102\n",
      "Epoch 21320/30000 Training Loss: 0.038224488496780396\n",
      "Epoch 21321/30000 Training Loss: 0.034382592886686325\n",
      "Epoch 21322/30000 Training Loss: 0.0392264649271965\n",
      "Epoch 21323/30000 Training Loss: 0.027863450348377228\n",
      "Epoch 21324/30000 Training Loss: 0.049808163195848465\n",
      "Epoch 21325/30000 Training Loss: 0.03890009969472885\n",
      "Epoch 21326/30000 Training Loss: 0.05411401763558388\n",
      "Epoch 21327/30000 Training Loss: 0.06054481118917465\n",
      "Epoch 21328/30000 Training Loss: 0.03598283976316452\n",
      "Epoch 21329/30000 Training Loss: 0.03900863602757454\n",
      "Epoch 21330/30000 Training Loss: 0.042657122015953064\n",
      "Epoch 21331/30000 Training Loss: 0.040074046701192856\n",
      "Epoch 21332/30000 Training Loss: 0.03593365103006363\n",
      "Epoch 21333/30000 Training Loss: 0.04439792037010193\n",
      "Epoch 21334/30000 Training Loss: 0.04429532587528229\n",
      "Epoch 21335/30000 Training Loss: 0.031195227056741714\n",
      "Epoch 21336/30000 Training Loss: 0.03233136236667633\n",
      "Epoch 21337/30000 Training Loss: 0.055338647216558456\n",
      "Epoch 21338/30000 Training Loss: 0.06679476797580719\n",
      "Epoch 21339/30000 Training Loss: 0.04756586626172066\n",
      "Epoch 21340/30000 Training Loss: 0.041194722056388855\n",
      "Epoch 21341/30000 Training Loss: 0.04781506955623627\n",
      "Epoch 21342/30000 Training Loss: 0.04775293916463852\n",
      "Epoch 21343/30000 Training Loss: 0.050851814448833466\n",
      "Epoch 21344/30000 Training Loss: 0.04097988083958626\n",
      "Epoch 21345/30000 Training Loss: 0.04402953386306763\n",
      "Epoch 21346/30000 Training Loss: 0.03909729793667793\n",
      "Epoch 21347/30000 Training Loss: 0.04726826399564743\n",
      "Epoch 21348/30000 Training Loss: 0.04553614556789398\n",
      "Epoch 21349/30000 Training Loss: 0.03872954100370407\n",
      "Epoch 21350/30000 Training Loss: 0.047036293894052505\n",
      "Epoch 21351/30000 Training Loss: 0.04454132169485092\n",
      "Epoch 21352/30000 Training Loss: 0.03202253207564354\n",
      "Epoch 21353/30000 Training Loss: 0.04418112337589264\n",
      "Epoch 21354/30000 Training Loss: 0.042289137840270996\n",
      "Epoch 21355/30000 Training Loss: 0.04834815487265587\n",
      "Epoch 21356/30000 Training Loss: 0.04502888396382332\n",
      "Epoch 21357/30000 Training Loss: 0.04273900389671326\n",
      "Epoch 21358/30000 Training Loss: 0.043193016201257706\n",
      "Epoch 21359/30000 Training Loss: 0.03652091696858406\n",
      "Epoch 21360/30000 Training Loss: 0.046721719205379486\n",
      "Epoch 21361/30000 Training Loss: 0.03400631994009018\n",
      "Epoch 21362/30000 Training Loss: 0.04911606386303902\n",
      "Epoch 21363/30000 Training Loss: 0.036986738443374634\n",
      "Epoch 21364/30000 Training Loss: 0.039681192487478256\n",
      "Epoch 21365/30000 Training Loss: 0.046731360256671906\n",
      "Epoch 21366/30000 Training Loss: 0.03433682769536972\n",
      "Epoch 21367/30000 Training Loss: 0.04665519669651985\n",
      "Epoch 21368/30000 Training Loss: 0.04146876931190491\n",
      "Epoch 21369/30000 Training Loss: 0.036494556814432144\n",
      "Epoch 21370/30000 Training Loss: 0.04453638941049576\n",
      "Epoch 21371/30000 Training Loss: 0.03788679838180542\n",
      "Epoch 21372/30000 Training Loss: 0.04518686980009079\n",
      "Epoch 21373/30000 Training Loss: 0.029825570061802864\n",
      "Epoch 21374/30000 Training Loss: 0.03907110542058945\n",
      "Epoch 21375/30000 Training Loss: 0.03730269521474838\n",
      "Epoch 21376/30000 Training Loss: 0.03724588826298714\n",
      "Epoch 21377/30000 Training Loss: 0.03947651386260986\n",
      "Epoch 21378/30000 Training Loss: 0.03965403139591217\n",
      "Epoch 21379/30000 Training Loss: 0.05094657093286514\n",
      "Epoch 21380/30000 Training Loss: 0.044035643339157104\n",
      "Epoch 21381/30000 Training Loss: 0.05936836451292038\n",
      "Epoch 21382/30000 Training Loss: 0.04238279163837433\n",
      "Epoch 21383/30000 Training Loss: 0.040560878813266754\n",
      "Epoch 21384/30000 Training Loss: 0.055034250020980835\n",
      "Epoch 21385/30000 Training Loss: 0.04060925915837288\n",
      "Epoch 21386/30000 Training Loss: 0.03265833109617233\n",
      "Epoch 21387/30000 Training Loss: 0.03808745741844177\n",
      "Epoch 21388/30000 Training Loss: 0.03684374317526817\n",
      "Epoch 21389/30000 Training Loss: 0.03862079977989197\n",
      "Epoch 21390/30000 Training Loss: 0.04161223769187927\n",
      "Epoch 21391/30000 Training Loss: 0.03667439520359039\n",
      "Epoch 21392/30000 Training Loss: 0.0436360239982605\n",
      "Epoch 21393/30000 Training Loss: 0.03770223259925842\n",
      "Epoch 21394/30000 Training Loss: 0.039378661662340164\n",
      "Epoch 21395/30000 Training Loss: 0.04305505007505417\n",
      "Epoch 21396/30000 Training Loss: 0.05668002367019653\n",
      "Epoch 21397/30000 Training Loss: 0.05498237907886505\n",
      "Epoch 21398/30000 Training Loss: 0.04721955955028534\n",
      "Epoch 21399/30000 Training Loss: 0.039197687059640884\n",
      "Epoch 21400/30000 Training Loss: 0.04680922254920006\n",
      "Epoch 21400/30000 Validation Loss: 0.037220682948827744\n",
      "Epoch 21401/30000 Training Loss: 0.05185088515281677\n",
      "Epoch 21402/30000 Training Loss: 0.04740944504737854\n",
      "Epoch 21403/30000 Training Loss: 0.04587235301733017\n",
      "Epoch 21404/30000 Training Loss: 0.03478091582655907\n",
      "Epoch 21405/30000 Training Loss: 0.05118483304977417\n",
      "Epoch 21406/30000 Training Loss: 0.05705791711807251\n",
      "Epoch 21407/30000 Training Loss: 0.04554568603634834\n",
      "Epoch 21408/30000 Training Loss: 0.03616771474480629\n",
      "Epoch 21409/30000 Training Loss: 0.04205649346113205\n",
      "Epoch 21410/30000 Training Loss: 0.03696756809949875\n",
      "Epoch 21411/30000 Training Loss: 0.04147981107234955\n",
      "Epoch 21412/30000 Training Loss: 0.047905921936035156\n",
      "Epoch 21413/30000 Training Loss: 0.05946815758943558\n",
      "Epoch 21414/30000 Training Loss: 0.04643617942929268\n",
      "Epoch 21415/30000 Training Loss: 0.04337577894330025\n",
      "Epoch 21416/30000 Training Loss: 0.05111932009458542\n",
      "Epoch 21417/30000 Training Loss: 0.041827186942100525\n",
      "Epoch 21418/30000 Training Loss: 0.06140454113483429\n",
      "Epoch 21419/30000 Training Loss: 0.05517553165555\n",
      "Epoch 21420/30000 Training Loss: 0.03384494036436081\n",
      "Epoch 21421/30000 Training Loss: 0.0332597941160202\n",
      "Epoch 21422/30000 Training Loss: 0.043860338628292084\n",
      "Epoch 21423/30000 Training Loss: 0.050647199153900146\n",
      "Epoch 21424/30000 Training Loss: 0.040548935532569885\n",
      "Epoch 21425/30000 Training Loss: 0.03735615685582161\n",
      "Epoch 21426/30000 Training Loss: 0.0448404997587204\n",
      "Epoch 21427/30000 Training Loss: 0.035980939865112305\n",
      "Epoch 21428/30000 Training Loss: 0.0426023006439209\n",
      "Epoch 21429/30000 Training Loss: 0.04959026724100113\n",
      "Epoch 21430/30000 Training Loss: 0.04353742301464081\n",
      "Epoch 21431/30000 Training Loss: 0.03375881165266037\n",
      "Epoch 21432/30000 Training Loss: 0.031769149005413055\n",
      "Epoch 21433/30000 Training Loss: 0.030478626489639282\n",
      "Epoch 21434/30000 Training Loss: 0.05234619230031967\n",
      "Epoch 21435/30000 Training Loss: 0.05627136677503586\n",
      "Epoch 21436/30000 Training Loss: 0.03457951173186302\n",
      "Epoch 21437/30000 Training Loss: 0.04108232259750366\n",
      "Epoch 21438/30000 Training Loss: 0.03233470395207405\n",
      "Epoch 21439/30000 Training Loss: 0.0419071689248085\n",
      "Epoch 21440/30000 Training Loss: 0.03661585599184036\n",
      "Epoch 21441/30000 Training Loss: 0.05253589153289795\n",
      "Epoch 21442/30000 Training Loss: 0.03740217909216881\n",
      "Epoch 21443/30000 Training Loss: 0.04359250143170357\n",
      "Epoch 21444/30000 Training Loss: 0.03171141445636749\n",
      "Epoch 21445/30000 Training Loss: 0.03518911451101303\n",
      "Epoch 21446/30000 Training Loss: 0.04421984404325485\n",
      "Epoch 21447/30000 Training Loss: 0.05598682165145874\n",
      "Epoch 21448/30000 Training Loss: 0.03731878101825714\n",
      "Epoch 21449/30000 Training Loss: 0.04694002866744995\n",
      "Epoch 21450/30000 Training Loss: 0.04151099920272827\n",
      "Epoch 21451/30000 Training Loss: 0.03498987853527069\n",
      "Epoch 21452/30000 Training Loss: 0.057952359318733215\n",
      "Epoch 21453/30000 Training Loss: 0.04002876952290535\n",
      "Epoch 21454/30000 Training Loss: 0.05000275373458862\n",
      "Epoch 21455/30000 Training Loss: 0.03554486110806465\n",
      "Epoch 21456/30000 Training Loss: 0.05425875633955002\n",
      "Epoch 21457/30000 Training Loss: 0.05090320482850075\n",
      "Epoch 21458/30000 Training Loss: 0.036086730659008026\n",
      "Epoch 21459/30000 Training Loss: 0.04281887784600258\n",
      "Epoch 21460/30000 Training Loss: 0.04740592837333679\n",
      "Epoch 21461/30000 Training Loss: 0.04180169105529785\n",
      "Epoch 21462/30000 Training Loss: 0.03240834176540375\n",
      "Epoch 21463/30000 Training Loss: 0.05095851048827171\n",
      "Epoch 21464/30000 Training Loss: 0.0373091921210289\n",
      "Epoch 21465/30000 Training Loss: 0.0557326003909111\n",
      "Epoch 21466/30000 Training Loss: 0.047559380531311035\n",
      "Epoch 21467/30000 Training Loss: 0.042145565152168274\n",
      "Epoch 21468/30000 Training Loss: 0.039364635944366455\n",
      "Epoch 21469/30000 Training Loss: 0.04374612122774124\n",
      "Epoch 21470/30000 Training Loss: 0.048184946179389954\n",
      "Epoch 21471/30000 Training Loss: 0.05089277774095535\n",
      "Epoch 21472/30000 Training Loss: 0.04469838738441467\n",
      "Epoch 21473/30000 Training Loss: 0.03850468248128891\n",
      "Epoch 21474/30000 Training Loss: 0.052682291716337204\n",
      "Epoch 21475/30000 Training Loss: 0.037855710834264755\n",
      "Epoch 21476/30000 Training Loss: 0.04720407724380493\n",
      "Epoch 21477/30000 Training Loss: 0.05117953568696976\n",
      "Epoch 21478/30000 Training Loss: 0.044659338891506195\n",
      "Epoch 21479/30000 Training Loss: 0.054234474897384644\n",
      "Epoch 21480/30000 Training Loss: 0.039934903383255005\n",
      "Epoch 21481/30000 Training Loss: 0.03786202892661095\n",
      "Epoch 21482/30000 Training Loss: 0.0446363128721714\n",
      "Epoch 21483/30000 Training Loss: 0.02942308783531189\n",
      "Epoch 21484/30000 Training Loss: 0.03649919107556343\n",
      "Epoch 21485/30000 Training Loss: 0.04844793677330017\n",
      "Epoch 21486/30000 Training Loss: 0.04946047067642212\n",
      "Epoch 21487/30000 Training Loss: 0.03232166916131973\n",
      "Epoch 21488/30000 Training Loss: 0.05043986067175865\n",
      "Epoch 21489/30000 Training Loss: 0.04004409909248352\n",
      "Epoch 21490/30000 Training Loss: 0.04295194149017334\n",
      "Epoch 21491/30000 Training Loss: 0.029697705060243607\n",
      "Epoch 21492/30000 Training Loss: 0.036593105643987656\n",
      "Epoch 21493/30000 Training Loss: 0.03427846357226372\n",
      "Epoch 21494/30000 Training Loss: 0.04370424151420593\n",
      "Epoch 21495/30000 Training Loss: 0.04651481285691261\n",
      "Epoch 21496/30000 Training Loss: 0.042754415422677994\n",
      "Epoch 21497/30000 Training Loss: 0.03923170268535614\n",
      "Epoch 21498/30000 Training Loss: 0.048368241637945175\n",
      "Epoch 21499/30000 Training Loss: 0.028212066739797592\n",
      "Epoch 21500/30000 Training Loss: 0.04755662381649017\n",
      "Epoch 21500/30000 Validation Loss: 0.05008698254823685\n",
      "Epoch 21501/30000 Training Loss: 0.03314526006579399\n",
      "Epoch 21502/30000 Training Loss: 0.05371718853712082\n",
      "Epoch 21503/30000 Training Loss: 0.061561450362205505\n",
      "Epoch 21504/30000 Training Loss: 0.036516737192869186\n",
      "Epoch 21505/30000 Training Loss: 0.03825847804546356\n",
      "Epoch 21506/30000 Training Loss: 0.038164034485816956\n",
      "Epoch 21507/30000 Training Loss: 0.035095177590847015\n",
      "Epoch 21508/30000 Training Loss: 0.037647541612386703\n",
      "Epoch 21509/30000 Training Loss: 0.04487031325697899\n",
      "Epoch 21510/30000 Training Loss: 0.0355667769908905\n",
      "Epoch 21511/30000 Training Loss: 0.042240750044584274\n",
      "Epoch 21512/30000 Training Loss: 0.04701204597949982\n",
      "Epoch 21513/30000 Training Loss: 0.04851602017879486\n",
      "Epoch 21514/30000 Training Loss: 0.042000409215688705\n",
      "Epoch 21515/30000 Training Loss: 0.043750863522291183\n",
      "Epoch 21516/30000 Training Loss: 0.04627498239278793\n",
      "Epoch 21517/30000 Training Loss: 0.0381343737244606\n",
      "Epoch 21518/30000 Training Loss: 0.047404855489730835\n",
      "Epoch 21519/30000 Training Loss: 0.04173298180103302\n",
      "Epoch 21520/30000 Training Loss: 0.046227335929870605\n",
      "Epoch 21521/30000 Training Loss: 0.035611335188150406\n",
      "Epoch 21522/30000 Training Loss: 0.04606405645608902\n",
      "Epoch 21523/30000 Training Loss: 0.05219443887472153\n",
      "Epoch 21524/30000 Training Loss: 0.0441192127764225\n",
      "Epoch 21525/30000 Training Loss: 0.047753460705280304\n",
      "Epoch 21526/30000 Training Loss: 0.05097929388284683\n",
      "Epoch 21527/30000 Training Loss: 0.03214728459715843\n",
      "Epoch 21528/30000 Training Loss: 0.040314123034477234\n",
      "Epoch 21529/30000 Training Loss: 0.04216624051332474\n",
      "Epoch 21530/30000 Training Loss: 0.04760199785232544\n",
      "Epoch 21531/30000 Training Loss: 0.043806254863739014\n",
      "Epoch 21532/30000 Training Loss: 0.052669696509838104\n",
      "Epoch 21533/30000 Training Loss: 0.04070625081658363\n",
      "Epoch 21534/30000 Training Loss: 0.03966435417532921\n",
      "Epoch 21535/30000 Training Loss: 0.0564444437623024\n",
      "Epoch 21536/30000 Training Loss: 0.05012082681059837\n",
      "Epoch 21537/30000 Training Loss: 0.03891151025891304\n",
      "Epoch 21538/30000 Training Loss: 0.04339160397648811\n",
      "Epoch 21539/30000 Training Loss: 0.03891722485423088\n",
      "Epoch 21540/30000 Training Loss: 0.040618978440761566\n",
      "Epoch 21541/30000 Training Loss: 0.04604092240333557\n",
      "Epoch 21542/30000 Training Loss: 0.03740660101175308\n",
      "Epoch 21543/30000 Training Loss: 0.05138175189495087\n",
      "Epoch 21544/30000 Training Loss: 0.04513463377952576\n",
      "Epoch 21545/30000 Training Loss: 0.04753036051988602\n",
      "Epoch 21546/30000 Training Loss: 0.03232317417860031\n",
      "Epoch 21547/30000 Training Loss: 0.03833197057247162\n",
      "Epoch 21548/30000 Training Loss: 0.0453648678958416\n",
      "Epoch 21549/30000 Training Loss: 0.04904182627797127\n",
      "Epoch 21550/30000 Training Loss: 0.04591554403305054\n",
      "Epoch 21551/30000 Training Loss: 0.05177624523639679\n",
      "Epoch 21552/30000 Training Loss: 0.04102330654859543\n",
      "Epoch 21553/30000 Training Loss: 0.03992306813597679\n",
      "Epoch 21554/30000 Training Loss: 0.05306138098239899\n",
      "Epoch 21555/30000 Training Loss: 0.04996192455291748\n",
      "Epoch 21556/30000 Training Loss: 0.0446467399597168\n",
      "Epoch 21557/30000 Training Loss: 0.03818025439977646\n",
      "Epoch 21558/30000 Training Loss: 0.033689603209495544\n",
      "Epoch 21559/30000 Training Loss: 0.04118982329964638\n",
      "Epoch 21560/30000 Training Loss: 0.052592046558856964\n",
      "Epoch 21561/30000 Training Loss: 0.04631049558520317\n",
      "Epoch 21562/30000 Training Loss: 0.036365799605846405\n",
      "Epoch 21563/30000 Training Loss: 0.04722710698843002\n",
      "Epoch 21564/30000 Training Loss: 0.041369467973709106\n",
      "Epoch 21565/30000 Training Loss: 0.04194963723421097\n",
      "Epoch 21566/30000 Training Loss: 0.04582600295543671\n",
      "Epoch 21567/30000 Training Loss: 0.037802182137966156\n",
      "Epoch 21568/30000 Training Loss: 0.05673466622829437\n",
      "Epoch 21569/30000 Training Loss: 0.03164806589484215\n",
      "Epoch 21570/30000 Training Loss: 0.051833510398864746\n",
      "Epoch 21571/30000 Training Loss: 0.044785209000110626\n",
      "Epoch 21572/30000 Training Loss: 0.056151166558265686\n",
      "Epoch 21573/30000 Training Loss: 0.04070102050900459\n",
      "Epoch 21574/30000 Training Loss: 0.04196936637163162\n",
      "Epoch 21575/30000 Training Loss: 0.039290815591812134\n",
      "Epoch 21576/30000 Training Loss: 0.04329456388950348\n",
      "Epoch 21577/30000 Training Loss: 0.04158519580960274\n",
      "Epoch 21578/30000 Training Loss: 0.041155166923999786\n",
      "Epoch 21579/30000 Training Loss: 0.03534704074263573\n",
      "Epoch 21580/30000 Training Loss: 0.046573933213949203\n",
      "Epoch 21581/30000 Training Loss: 0.041546184569597244\n",
      "Epoch 21582/30000 Training Loss: 0.040007296949625015\n",
      "Epoch 21583/30000 Training Loss: 0.04456407204270363\n",
      "Epoch 21584/30000 Training Loss: 0.03627118468284607\n",
      "Epoch 21585/30000 Training Loss: 0.05156894773244858\n",
      "Epoch 21586/30000 Training Loss: 0.036687836050987244\n",
      "Epoch 21587/30000 Training Loss: 0.04641226306557655\n",
      "Epoch 21588/30000 Training Loss: 0.049383748322725296\n",
      "Epoch 21589/30000 Training Loss: 0.04099348932504654\n",
      "Epoch 21590/30000 Training Loss: 0.049746789038181305\n",
      "Epoch 21591/30000 Training Loss: 0.04269687086343765\n",
      "Epoch 21592/30000 Training Loss: 0.044987186789512634\n",
      "Epoch 21593/30000 Training Loss: 0.05376260727643967\n",
      "Epoch 21594/30000 Training Loss: 0.04993133246898651\n",
      "Epoch 21595/30000 Training Loss: 0.045090824365615845\n",
      "Epoch 21596/30000 Training Loss: 0.04190213233232498\n",
      "Epoch 21597/30000 Training Loss: 0.04681175947189331\n",
      "Epoch 21598/30000 Training Loss: 0.049426496028900146\n",
      "Epoch 21599/30000 Training Loss: 0.034755125641822815\n",
      "Epoch 21600/30000 Training Loss: 0.03938363492488861\n",
      "Epoch 21600/30000 Validation Loss: 0.03799280524253845\n",
      "Epoch 21601/30000 Training Loss: 0.042738039046525955\n",
      "Epoch 21602/30000 Training Loss: 0.04262659698724747\n",
      "Epoch 21603/30000 Training Loss: 0.054755471646785736\n",
      "Epoch 21604/30000 Training Loss: 0.03111932799220085\n",
      "Epoch 21605/30000 Training Loss: 0.043147869408130646\n",
      "Epoch 21606/30000 Training Loss: 0.04283203184604645\n",
      "Epoch 21607/30000 Training Loss: 0.05904765427112579\n",
      "Epoch 21608/30000 Training Loss: 0.04081297293305397\n",
      "Epoch 21609/30000 Training Loss: 0.036732017993927\n",
      "Epoch 21610/30000 Training Loss: 0.030114226043224335\n",
      "Epoch 21611/30000 Training Loss: 0.037123680114746094\n",
      "Epoch 21612/30000 Training Loss: 0.05265691131353378\n",
      "Epoch 21613/30000 Training Loss: 0.04937731474637985\n",
      "Epoch 21614/30000 Training Loss: 0.040114082396030426\n",
      "Epoch 21615/30000 Training Loss: 0.03993861749768257\n",
      "Epoch 21616/30000 Training Loss: 0.043599508702754974\n",
      "Epoch 21617/30000 Training Loss: 0.04878021776676178\n",
      "Epoch 21618/30000 Training Loss: 0.03325232118368149\n",
      "Epoch 21619/30000 Training Loss: 0.04614192619919777\n",
      "Epoch 21620/30000 Training Loss: 0.04900021851062775\n",
      "Epoch 21621/30000 Training Loss: 0.0431380569934845\n",
      "Epoch 21622/30000 Training Loss: 0.043958500027656555\n",
      "Epoch 21623/30000 Training Loss: 0.04837658628821373\n",
      "Epoch 21624/30000 Training Loss: 0.04822663217782974\n",
      "Epoch 21625/30000 Training Loss: 0.03456692770123482\n",
      "Epoch 21626/30000 Training Loss: 0.044049881398677826\n",
      "Epoch 21627/30000 Training Loss: 0.05016994848847389\n",
      "Epoch 21628/30000 Training Loss: 0.040819279849529266\n",
      "Epoch 21629/30000 Training Loss: 0.049166105687618256\n",
      "Epoch 21630/30000 Training Loss: 0.04549252986907959\n",
      "Epoch 21631/30000 Training Loss: 0.04285035282373428\n",
      "Epoch 21632/30000 Training Loss: 0.04363531246781349\n",
      "Epoch 21633/30000 Training Loss: 0.03851606696844101\n",
      "Epoch 21634/30000 Training Loss: 0.030958354473114014\n",
      "Epoch 21635/30000 Training Loss: 0.04169359430670738\n",
      "Epoch 21636/30000 Training Loss: 0.04019142687320709\n",
      "Epoch 21637/30000 Training Loss: 0.04723693057894707\n",
      "Epoch 21638/30000 Training Loss: 0.031232275068759918\n",
      "Epoch 21639/30000 Training Loss: 0.05374833941459656\n",
      "Epoch 21640/30000 Training Loss: 0.04743634909391403\n",
      "Epoch 21641/30000 Training Loss: 0.03815782815217972\n",
      "Epoch 21642/30000 Training Loss: 0.04706212133169174\n",
      "Epoch 21643/30000 Training Loss: 0.04460893198847771\n",
      "Epoch 21644/30000 Training Loss: 0.04882320389151573\n",
      "Epoch 21645/30000 Training Loss: 0.04765165597200394\n",
      "Epoch 21646/30000 Training Loss: 0.04090311378240585\n",
      "Epoch 21647/30000 Training Loss: 0.04268573969602585\n",
      "Epoch 21648/30000 Training Loss: 0.04533882439136505\n",
      "Epoch 21649/30000 Training Loss: 0.037968531250953674\n",
      "Epoch 21650/30000 Training Loss: 0.0469256155192852\n",
      "Epoch 21651/30000 Training Loss: 0.04970315098762512\n",
      "Epoch 21652/30000 Training Loss: 0.0343630276620388\n",
      "Epoch 21653/30000 Training Loss: 0.03602830320596695\n",
      "Epoch 21654/30000 Training Loss: 0.044668298214673996\n",
      "Epoch 21655/30000 Training Loss: 0.04268582910299301\n",
      "Epoch 21656/30000 Training Loss: 0.03666236996650696\n",
      "Epoch 21657/30000 Training Loss: 0.03499936684966087\n",
      "Epoch 21658/30000 Training Loss: 0.03747912496328354\n",
      "Epoch 21659/30000 Training Loss: 0.04016170650720596\n",
      "Epoch 21660/30000 Training Loss: 0.04289380460977554\n",
      "Epoch 21661/30000 Training Loss: 0.04200024902820587\n",
      "Epoch 21662/30000 Training Loss: 0.049806177616119385\n",
      "Epoch 21663/30000 Training Loss: 0.040195222944021225\n",
      "Epoch 21664/30000 Training Loss: 0.034998103976249695\n",
      "Epoch 21665/30000 Training Loss: 0.05625718832015991\n",
      "Epoch 21666/30000 Training Loss: 0.03577275574207306\n",
      "Epoch 21667/30000 Training Loss: 0.04635936766862869\n",
      "Epoch 21668/30000 Training Loss: 0.03624219819903374\n",
      "Epoch 21669/30000 Training Loss: 0.04247744381427765\n",
      "Epoch 21670/30000 Training Loss: 0.03342288359999657\n",
      "Epoch 21671/30000 Training Loss: 0.0403609499335289\n",
      "Epoch 21672/30000 Training Loss: 0.043846119195222855\n",
      "Epoch 21673/30000 Training Loss: 0.051829319447278976\n",
      "Epoch 21674/30000 Training Loss: 0.030566170811653137\n",
      "Epoch 21675/30000 Training Loss: 0.03419193625450134\n",
      "Epoch 21676/30000 Training Loss: 0.0459146648645401\n",
      "Epoch 21677/30000 Training Loss: 0.04373174533247948\n",
      "Epoch 21678/30000 Training Loss: 0.03829486668109894\n",
      "Epoch 21679/30000 Training Loss: 0.042786628007888794\n",
      "Epoch 21680/30000 Training Loss: 0.058123957365751266\n",
      "Epoch 21681/30000 Training Loss: 0.045906297862529755\n",
      "Epoch 21682/30000 Training Loss: 0.048444487154483795\n",
      "Epoch 21683/30000 Training Loss: 0.056006722152233124\n",
      "Epoch 21684/30000 Training Loss: 0.05248773470520973\n",
      "Epoch 21685/30000 Training Loss: 0.05539878457784653\n",
      "Epoch 21686/30000 Training Loss: 0.038361527025699615\n",
      "Epoch 21687/30000 Training Loss: 0.038705013692379\n",
      "Epoch 21688/30000 Training Loss: 0.039464011788368225\n",
      "Epoch 21689/30000 Training Loss: 0.036638155579566956\n",
      "Epoch 21690/30000 Training Loss: 0.04131550341844559\n",
      "Epoch 21691/30000 Training Loss: 0.03157548978924751\n",
      "Epoch 21692/30000 Training Loss: 0.04387164115905762\n",
      "Epoch 21693/30000 Training Loss: 0.044197507202625275\n",
      "Epoch 21694/30000 Training Loss: 0.046581193804740906\n",
      "Epoch 21695/30000 Training Loss: 0.04689948260784149\n",
      "Epoch 21696/30000 Training Loss: 0.04269418120384216\n",
      "Epoch 21697/30000 Training Loss: 0.05804084986448288\n",
      "Epoch 21698/30000 Training Loss: 0.03574204072356224\n",
      "Epoch 21699/30000 Training Loss: 0.02912595123052597\n",
      "Epoch 21700/30000 Training Loss: 0.05074464529752731\n",
      "Epoch 21700/30000 Validation Loss: 0.03345014154911041\n",
      "Epoch 21701/30000 Training Loss: 0.04281489923596382\n",
      "Epoch 21702/30000 Training Loss: 0.0455123595893383\n",
      "Epoch 21703/30000 Training Loss: 0.04723595455288887\n",
      "Epoch 21704/30000 Training Loss: 0.03992674872279167\n",
      "Epoch 21705/30000 Training Loss: 0.0493650957942009\n",
      "Epoch 21706/30000 Training Loss: 0.05110177770256996\n",
      "Epoch 21707/30000 Training Loss: 0.03782811015844345\n",
      "Epoch 21708/30000 Training Loss: 0.04756385087966919\n",
      "Epoch 21709/30000 Training Loss: 0.05245988816022873\n",
      "Epoch 21710/30000 Training Loss: 0.049308545887470245\n",
      "Epoch 21711/30000 Training Loss: 0.045652974396944046\n",
      "Epoch 21712/30000 Training Loss: 0.035414986312389374\n",
      "Epoch 21713/30000 Training Loss: 0.04553918540477753\n",
      "Epoch 21714/30000 Training Loss: 0.043412402272224426\n",
      "Epoch 21715/30000 Training Loss: 0.04527868703007698\n",
      "Epoch 21716/30000 Training Loss: 0.04324394091963768\n",
      "Epoch 21717/30000 Training Loss: 0.04367406666278839\n",
      "Epoch 21718/30000 Training Loss: 0.04904215782880783\n",
      "Epoch 21719/30000 Training Loss: 0.04921388626098633\n",
      "Epoch 21720/30000 Training Loss: 0.038567915558815\n",
      "Epoch 21721/30000 Training Loss: 0.05956897512078285\n",
      "Epoch 21722/30000 Training Loss: 0.047363437712192535\n",
      "Epoch 21723/30000 Training Loss: 0.041479237377643585\n",
      "Epoch 21724/30000 Training Loss: 0.03979792818427086\n",
      "Epoch 21725/30000 Training Loss: 0.04136054217815399\n",
      "Epoch 21726/30000 Training Loss: 0.047998055815696716\n",
      "Epoch 21727/30000 Training Loss: 0.052427854388952255\n",
      "Epoch 21728/30000 Training Loss: 0.047275178134441376\n",
      "Epoch 21729/30000 Training Loss: 0.04965975135564804\n",
      "Epoch 21730/30000 Training Loss: 0.051919419318437576\n",
      "Epoch 21731/30000 Training Loss: 0.05166617035865784\n",
      "Epoch 21732/30000 Training Loss: 0.036330461502075195\n",
      "Epoch 21733/30000 Training Loss: 0.042327992618083954\n",
      "Epoch 21734/30000 Training Loss: 0.04078339785337448\n",
      "Epoch 21735/30000 Training Loss: 0.04894738644361496\n",
      "Epoch 21736/30000 Training Loss: 0.048035185784101486\n",
      "Epoch 21737/30000 Training Loss: 0.04097805544734001\n",
      "Epoch 21738/30000 Training Loss: 0.04439137130975723\n",
      "Epoch 21739/30000 Training Loss: 0.0363362692296505\n",
      "Epoch 21740/30000 Training Loss: 0.047990523278713226\n",
      "Epoch 21741/30000 Training Loss: 0.045136310160160065\n",
      "Epoch 21742/30000 Training Loss: 0.030525393784046173\n",
      "Epoch 21743/30000 Training Loss: 0.033983439207077026\n",
      "Epoch 21744/30000 Training Loss: 0.03319995850324631\n",
      "Epoch 21745/30000 Training Loss: 0.03944060206413269\n",
      "Epoch 21746/30000 Training Loss: 0.04462366923689842\n",
      "Epoch 21747/30000 Training Loss: 0.05197259038686752\n",
      "Epoch 21748/30000 Training Loss: 0.03505084663629532\n",
      "Epoch 21749/30000 Training Loss: 0.038972802460193634\n",
      "Epoch 21750/30000 Training Loss: 0.04353979974985123\n",
      "Epoch 21751/30000 Training Loss: 0.05303526297211647\n",
      "Epoch 21752/30000 Training Loss: 0.03862803429365158\n",
      "Epoch 21753/30000 Training Loss: 0.052083663642406464\n",
      "Epoch 21754/30000 Training Loss: 0.04205230996012688\n",
      "Epoch 21755/30000 Training Loss: 0.04453512653708458\n",
      "Epoch 21756/30000 Training Loss: 0.04234329238533974\n",
      "Epoch 21757/30000 Training Loss: 0.03863247111439705\n",
      "Epoch 21758/30000 Training Loss: 0.05424647405743599\n",
      "Epoch 21759/30000 Training Loss: 0.040092773735523224\n",
      "Epoch 21760/30000 Training Loss: 0.04539145156741142\n",
      "Epoch 21761/30000 Training Loss: 0.03470256179571152\n",
      "Epoch 21762/30000 Training Loss: 0.04817144572734833\n",
      "Epoch 21763/30000 Training Loss: 0.0538816824555397\n",
      "Epoch 21764/30000 Training Loss: 0.029894743114709854\n",
      "Epoch 21765/30000 Training Loss: 0.0397297739982605\n",
      "Epoch 21766/30000 Training Loss: 0.04501377046108246\n",
      "Epoch 21767/30000 Training Loss: 0.052728887647390366\n",
      "Epoch 21768/30000 Training Loss: 0.05049828439950943\n",
      "Epoch 21769/30000 Training Loss: 0.05211654305458069\n",
      "Epoch 21770/30000 Training Loss: 0.04800625145435333\n",
      "Epoch 21771/30000 Training Loss: 0.03036121279001236\n",
      "Epoch 21772/30000 Training Loss: 0.04569680988788605\n",
      "Epoch 21773/30000 Training Loss: 0.045381948351860046\n",
      "Epoch 21774/30000 Training Loss: 0.04491955041885376\n",
      "Epoch 21775/30000 Training Loss: 0.04590437561273575\n",
      "Epoch 21776/30000 Training Loss: 0.044450584799051285\n",
      "Epoch 21777/30000 Training Loss: 0.030616741627454758\n",
      "Epoch 21778/30000 Training Loss: 0.02954229898750782\n",
      "Epoch 21779/30000 Training Loss: 0.04633866995573044\n",
      "Epoch 21780/30000 Training Loss: 0.05545313283801079\n",
      "Epoch 21781/30000 Training Loss: 0.0423850454390049\n",
      "Epoch 21782/30000 Training Loss: 0.04420728236436844\n",
      "Epoch 21783/30000 Training Loss: 0.049476154148578644\n",
      "Epoch 21784/30000 Training Loss: 0.034567199647426605\n",
      "Epoch 21785/30000 Training Loss: 0.04383658617734909\n",
      "Epoch 21786/30000 Training Loss: 0.04673495143651962\n",
      "Epoch 21787/30000 Training Loss: 0.05861741676926613\n",
      "Epoch 21788/30000 Training Loss: 0.03905286639928818\n",
      "Epoch 21789/30000 Training Loss: 0.0386953130364418\n",
      "Epoch 21790/30000 Training Loss: 0.030807625502347946\n",
      "Epoch 21791/30000 Training Loss: 0.03840728849172592\n",
      "Epoch 21792/30000 Training Loss: 0.030277155339717865\n",
      "Epoch 21793/30000 Training Loss: 0.04233242943882942\n",
      "Epoch 21794/30000 Training Loss: 0.03816758096218109\n",
      "Epoch 21795/30000 Training Loss: 0.044239334762096405\n",
      "Epoch 21796/30000 Training Loss: 0.048316314816474915\n",
      "Epoch 21797/30000 Training Loss: 0.03981618583202362\n",
      "Epoch 21798/30000 Training Loss: 0.038617976009845734\n",
      "Epoch 21799/30000 Training Loss: 0.04089335724711418\n",
      "Epoch 21800/30000 Training Loss: 0.04915152117609978\n",
      "Epoch 21800/30000 Validation Loss: 0.03012603148818016\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03012603148818016<=============\n",
      "Epoch 21801/30000 Training Loss: 0.03493843227624893\n",
      "Epoch 21802/30000 Training Loss: 0.03399363532662392\n",
      "Epoch 21803/30000 Training Loss: 0.047881707549095154\n",
      "Epoch 21804/30000 Training Loss: 0.03589283674955368\n",
      "Epoch 21805/30000 Training Loss: 0.05739249661564827\n",
      "Epoch 21806/30000 Training Loss: 0.03271082788705826\n",
      "Epoch 21807/30000 Training Loss: 0.04911276698112488\n",
      "Epoch 21808/30000 Training Loss: 0.04304787889122963\n",
      "Epoch 21809/30000 Training Loss: 0.037317462265491486\n",
      "Epoch 21810/30000 Training Loss: 0.03657188266515732\n",
      "Epoch 21811/30000 Training Loss: 0.037258461117744446\n",
      "Epoch 21812/30000 Training Loss: 0.037004560232162476\n",
      "Epoch 21813/30000 Training Loss: 0.038073085248470306\n",
      "Epoch 21814/30000 Training Loss: 0.04205118864774704\n",
      "Epoch 21815/30000 Training Loss: 0.04444095492362976\n",
      "Epoch 21816/30000 Training Loss: 0.04417049139738083\n",
      "Epoch 21817/30000 Training Loss: 0.04880627244710922\n",
      "Epoch 21818/30000 Training Loss: 0.04123253747820854\n",
      "Epoch 21819/30000 Training Loss: 0.033798106014728546\n",
      "Epoch 21820/30000 Training Loss: 0.06473682075738907\n",
      "Epoch 21821/30000 Training Loss: 0.033897459506988525\n",
      "Epoch 21822/30000 Training Loss: 0.03277571126818657\n",
      "Epoch 21823/30000 Training Loss: 0.03782390430569649\n",
      "Epoch 21824/30000 Training Loss: 0.05149457976222038\n",
      "Epoch 21825/30000 Training Loss: 0.04108577221632004\n",
      "Epoch 21826/30000 Training Loss: 0.03368200734257698\n",
      "Epoch 21827/30000 Training Loss: 0.04060853272676468\n",
      "Epoch 21828/30000 Training Loss: 0.04898557439446449\n",
      "Epoch 21829/30000 Training Loss: 0.04810212925076485\n",
      "Epoch 21830/30000 Training Loss: 0.04147866368293762\n",
      "Epoch 21831/30000 Training Loss: 0.0467577800154686\n",
      "Epoch 21832/30000 Training Loss: 0.05107274278998375\n",
      "Epoch 21833/30000 Training Loss: 0.040431033819913864\n",
      "Epoch 21834/30000 Training Loss: 0.03166794031858444\n",
      "Epoch 21835/30000 Training Loss: 0.04484780877828598\n",
      "Epoch 21836/30000 Training Loss: 0.05407910794019699\n",
      "Epoch 21837/30000 Training Loss: 0.04552267864346504\n",
      "Epoch 21838/30000 Training Loss: 0.03575838357210159\n",
      "Epoch 21839/30000 Training Loss: 0.04156704992055893\n",
      "Epoch 21840/30000 Training Loss: 0.036199260503053665\n",
      "Epoch 21841/30000 Training Loss: 0.04033400118350983\n",
      "Epoch 21842/30000 Training Loss: 0.04100693017244339\n",
      "Epoch 21843/30000 Training Loss: 0.05547858029603958\n",
      "Epoch 21844/30000 Training Loss: 0.04008273780345917\n",
      "Epoch 21845/30000 Training Loss: 0.033644385635852814\n",
      "Epoch 21846/30000 Training Loss: 0.0483938530087471\n",
      "Epoch 21847/30000 Training Loss: 0.03060530312359333\n",
      "Epoch 21848/30000 Training Loss: 0.04794304072856903\n",
      "Epoch 21849/30000 Training Loss: 0.03180248662829399\n",
      "Epoch 21850/30000 Training Loss: 0.050215646624565125\n",
      "Epoch 21851/30000 Training Loss: 0.03474640101194382\n",
      "Epoch 21852/30000 Training Loss: 0.027403179556131363\n",
      "Epoch 21853/30000 Training Loss: 0.05252234637737274\n",
      "Epoch 21854/30000 Training Loss: 0.05141960084438324\n",
      "Epoch 21855/30000 Training Loss: 0.03199179843068123\n",
      "Epoch 21856/30000 Training Loss: 0.04847431927919388\n",
      "Epoch 21857/30000 Training Loss: 0.04016312584280968\n",
      "Epoch 21858/30000 Training Loss: 0.03297705575823784\n",
      "Epoch 21859/30000 Training Loss: 0.0487503781914711\n",
      "Epoch 21860/30000 Training Loss: 0.046622518450021744\n",
      "Epoch 21861/30000 Training Loss: 0.04658213257789612\n",
      "Epoch 21862/30000 Training Loss: 0.03432672470808029\n",
      "Epoch 21863/30000 Training Loss: 0.040239520370960236\n",
      "Epoch 21864/30000 Training Loss: 0.030087709426879883\n",
      "Epoch 21865/30000 Training Loss: 0.04622342810034752\n",
      "Epoch 21866/30000 Training Loss: 0.0369524247944355\n",
      "Epoch 21867/30000 Training Loss: 0.04854389280080795\n",
      "Epoch 21868/30000 Training Loss: 0.03627648204565048\n",
      "Epoch 21869/30000 Training Loss: 0.04625271260738373\n",
      "Epoch 21870/30000 Training Loss: 0.03267892450094223\n",
      "Epoch 21871/30000 Training Loss: 0.04304869472980499\n",
      "Epoch 21872/30000 Training Loss: 0.04535061866044998\n",
      "Epoch 21873/30000 Training Loss: 0.04558853432536125\n",
      "Epoch 21874/30000 Training Loss: 0.04340888559818268\n",
      "Epoch 21875/30000 Training Loss: 0.04446132108569145\n",
      "Epoch 21876/30000 Training Loss: 0.05851076543331146\n",
      "Epoch 21877/30000 Training Loss: 0.025469612330198288\n",
      "Epoch 21878/30000 Training Loss: 0.03817174211144447\n",
      "Epoch 21879/30000 Training Loss: 0.03323919698596001\n",
      "Epoch 21880/30000 Training Loss: 0.03388486057519913\n",
      "Epoch 21881/30000 Training Loss: 0.04688512533903122\n",
      "Epoch 21882/30000 Training Loss: 0.04340875893831253\n",
      "Epoch 21883/30000 Training Loss: 0.0497155636548996\n",
      "Epoch 21884/30000 Training Loss: 0.04505295306444168\n",
      "Epoch 21885/30000 Training Loss: 0.05535802245140076\n",
      "Epoch 21886/30000 Training Loss: 0.04000106453895569\n",
      "Epoch 21887/30000 Training Loss: 0.04069201648235321\n",
      "Epoch 21888/30000 Training Loss: 0.04735787585377693\n",
      "Epoch 21889/30000 Training Loss: 0.04504482448101044\n",
      "Epoch 21890/30000 Training Loss: 0.030970482155680656\n",
      "Epoch 21891/30000 Training Loss: 0.03385315090417862\n",
      "Epoch 21892/30000 Training Loss: 0.053007081151008606\n",
      "Epoch 21893/30000 Training Loss: 0.035761501640081406\n",
      "Epoch 21894/30000 Training Loss: 0.049641162157058716\n",
      "Epoch 21895/30000 Training Loss: 0.03992597758769989\n",
      "Epoch 21896/30000 Training Loss: 0.04680437594652176\n",
      "Epoch 21897/30000 Training Loss: 0.05003272369503975\n",
      "Epoch 21898/30000 Training Loss: 0.034535057842731476\n",
      "Epoch 21899/30000 Training Loss: 0.035169925540685654\n",
      "Epoch 21900/30000 Training Loss: 0.04163222387433052\n",
      "Epoch 21900/30000 Validation Loss: 0.043599531054496765\n",
      "Epoch 21901/30000 Training Loss: 0.04376868158578873\n",
      "Epoch 21902/30000 Training Loss: 0.042503707110881805\n",
      "Epoch 21903/30000 Training Loss: 0.03807052969932556\n",
      "Epoch 21904/30000 Training Loss: 0.050683848559856415\n",
      "Epoch 21905/30000 Training Loss: 0.04621737450361252\n",
      "Epoch 21906/30000 Training Loss: 0.054239772260189056\n",
      "Epoch 21907/30000 Training Loss: 0.053136929869651794\n",
      "Epoch 21908/30000 Training Loss: 0.0529683381319046\n",
      "Epoch 21909/30000 Training Loss: 0.051259271800518036\n",
      "Epoch 21910/30000 Training Loss: 0.03198815509676933\n",
      "Epoch 21911/30000 Training Loss: 0.040390290319919586\n",
      "Epoch 21912/30000 Training Loss: 0.05854141339659691\n",
      "Epoch 21913/30000 Training Loss: 0.04577521234750748\n",
      "Epoch 21914/30000 Training Loss: 0.049788638949394226\n",
      "Epoch 21915/30000 Training Loss: 0.030170347541570663\n",
      "Epoch 21916/30000 Training Loss: 0.05874776095151901\n",
      "Epoch 21917/30000 Training Loss: 0.05605873465538025\n",
      "Epoch 21918/30000 Training Loss: 0.05066683888435364\n",
      "Epoch 21919/30000 Training Loss: 0.05349496752023697\n",
      "Epoch 21920/30000 Training Loss: 0.052099116146564484\n",
      "Epoch 21921/30000 Training Loss: 0.04858842492103577\n",
      "Epoch 21922/30000 Training Loss: 0.0356290265917778\n",
      "Epoch 21923/30000 Training Loss: 0.04905974119901657\n",
      "Epoch 21924/30000 Training Loss: 0.05054277926683426\n",
      "Epoch 21925/30000 Training Loss: 0.04682975262403488\n",
      "Epoch 21926/30000 Training Loss: 0.037767328321933746\n",
      "Epoch 21927/30000 Training Loss: 0.0493498221039772\n",
      "Epoch 21928/30000 Training Loss: 0.04972635209560394\n",
      "Epoch 21929/30000 Training Loss: 0.04883177950978279\n",
      "Epoch 21930/30000 Training Loss: 0.04199075698852539\n",
      "Epoch 21931/30000 Training Loss: 0.03633624315261841\n",
      "Epoch 21932/30000 Training Loss: 0.03469684720039368\n",
      "Epoch 21933/30000 Training Loss: 0.028840074315667152\n",
      "Epoch 21934/30000 Training Loss: 0.059220440685749054\n",
      "Epoch 21935/30000 Training Loss: 0.035510241985321045\n",
      "Epoch 21936/30000 Training Loss: 0.035202279686927795\n",
      "Epoch 21937/30000 Training Loss: 0.025501132011413574\n",
      "Epoch 21938/30000 Training Loss: 0.04564859718084335\n",
      "Epoch 21939/30000 Training Loss: 0.03241639584302902\n",
      "Epoch 21940/30000 Training Loss: 0.04351209104061127\n",
      "Epoch 21941/30000 Training Loss: 0.03858175128698349\n",
      "Epoch 21942/30000 Training Loss: 0.05021192878484726\n",
      "Epoch 21943/30000 Training Loss: 0.037136260420084\n",
      "Epoch 21944/30000 Training Loss: 0.038917358964681625\n",
      "Epoch 21945/30000 Training Loss: 0.03447963669896126\n",
      "Epoch 21946/30000 Training Loss: 0.03942352533340454\n",
      "Epoch 21947/30000 Training Loss: 0.042827583849430084\n",
      "Epoch 21948/30000 Training Loss: 0.03693884611129761\n",
      "Epoch 21949/30000 Training Loss: 0.04092952609062195\n",
      "Epoch 21950/30000 Training Loss: 0.04341931268572807\n",
      "Epoch 21951/30000 Training Loss: 0.039063021540641785\n",
      "Epoch 21952/30000 Training Loss: 0.05592263117432594\n",
      "Epoch 21953/30000 Training Loss: 0.03695898503065109\n",
      "Epoch 21954/30000 Training Loss: 0.04259200394153595\n",
      "Epoch 21955/30000 Training Loss: 0.047778502106666565\n",
      "Epoch 21956/30000 Training Loss: 0.04167149215936661\n",
      "Epoch 21957/30000 Training Loss: 0.03800810128450394\n",
      "Epoch 21958/30000 Training Loss: 0.0478929802775383\n",
      "Epoch 21959/30000 Training Loss: 0.045172594487667084\n",
      "Epoch 21960/30000 Training Loss: 0.0325617752969265\n",
      "Epoch 21961/30000 Training Loss: 0.03929504379630089\n",
      "Epoch 21962/30000 Training Loss: 0.042829014360904694\n",
      "Epoch 21963/30000 Training Loss: 0.04946789890527725\n",
      "Epoch 21964/30000 Training Loss: 0.034315165132284164\n",
      "Epoch 21965/30000 Training Loss: 0.03531336784362793\n",
      "Epoch 21966/30000 Training Loss: 0.04222652688622475\n",
      "Epoch 21967/30000 Training Loss: 0.04037337005138397\n",
      "Epoch 21968/30000 Training Loss: 0.048096731305122375\n",
      "Epoch 21969/30000 Training Loss: 0.044355928897857666\n",
      "Epoch 21970/30000 Training Loss: 0.05875589698553085\n",
      "Epoch 21971/30000 Training Loss: 0.03306357190012932\n",
      "Epoch 21972/30000 Training Loss: 0.03836202993988991\n",
      "Epoch 21973/30000 Training Loss: 0.04738021641969681\n",
      "Epoch 21974/30000 Training Loss: 0.039886850863695145\n",
      "Epoch 21975/30000 Training Loss: 0.036443404853343964\n",
      "Epoch 21976/30000 Training Loss: 0.047027986496686935\n",
      "Epoch 21977/30000 Training Loss: 0.05140291526913643\n",
      "Epoch 21978/30000 Training Loss: 0.040175244212150574\n",
      "Epoch 21979/30000 Training Loss: 0.055011577904224396\n",
      "Epoch 21980/30000 Training Loss: 0.048345811665058136\n",
      "Epoch 21981/30000 Training Loss: 0.04456450045108795\n",
      "Epoch 21982/30000 Training Loss: 0.04136860370635986\n",
      "Epoch 21983/30000 Training Loss: 0.042688965797424316\n",
      "Epoch 21984/30000 Training Loss: 0.044874902814626694\n",
      "Epoch 21985/30000 Training Loss: 0.03775347024202347\n",
      "Epoch 21986/30000 Training Loss: 0.04209589958190918\n",
      "Epoch 21987/30000 Training Loss: 0.03632205352187157\n",
      "Epoch 21988/30000 Training Loss: 0.03920542821288109\n",
      "Epoch 21989/30000 Training Loss: 0.04993899166584015\n",
      "Epoch 21990/30000 Training Loss: 0.03419079631567001\n",
      "Epoch 21991/30000 Training Loss: 0.04883583262562752\n",
      "Epoch 21992/30000 Training Loss: 0.030548125505447388\n",
      "Epoch 21993/30000 Training Loss: 0.044659361243247986\n",
      "Epoch 21994/30000 Training Loss: 0.03441799432039261\n",
      "Epoch 21995/30000 Training Loss: 0.03811783716082573\n",
      "Epoch 21996/30000 Training Loss: 0.04554721713066101\n",
      "Epoch 21997/30000 Training Loss: 0.03952067345380783\n",
      "Epoch 21998/30000 Training Loss: 0.04244345426559448\n",
      "Epoch 21999/30000 Training Loss: 0.04506174474954605\n",
      "Epoch 22000/30000 Training Loss: 0.032178495079278946\n",
      "Epoch 22000/30000 Validation Loss: 0.0560460239648819\n",
      "Epoch 22001/30000 Training Loss: 0.030640579760074615\n",
      "Epoch 22002/30000 Training Loss: 0.04600900784134865\n",
      "Epoch 22003/30000 Training Loss: 0.03975328803062439\n",
      "Epoch 22004/30000 Training Loss: 0.05486396700143814\n",
      "Epoch 22005/30000 Training Loss: 0.04673197120428085\n",
      "Epoch 22006/30000 Training Loss: 0.05176948010921478\n",
      "Epoch 22007/30000 Training Loss: 0.04099338501691818\n",
      "Epoch 22008/30000 Training Loss: 0.03617578744888306\n",
      "Epoch 22009/30000 Training Loss: 0.05057192221283913\n",
      "Epoch 22010/30000 Training Loss: 0.05877873674035072\n",
      "Epoch 22011/30000 Training Loss: 0.04187040030956268\n",
      "Epoch 22012/30000 Training Loss: 0.046817392110824585\n",
      "Epoch 22013/30000 Training Loss: 0.04443443566560745\n",
      "Epoch 22014/30000 Training Loss: 0.04346698895096779\n",
      "Epoch 22015/30000 Training Loss: 0.048314403742551804\n",
      "Epoch 22016/30000 Training Loss: 0.04012441262602806\n",
      "Epoch 22017/30000 Training Loss: 0.04483959078788757\n",
      "Epoch 22018/30000 Training Loss: 0.03670261800289154\n",
      "Epoch 22019/30000 Training Loss: 0.04986780136823654\n",
      "Epoch 22020/30000 Training Loss: 0.046659231185913086\n",
      "Epoch 22021/30000 Training Loss: 0.03841947019100189\n",
      "Epoch 22022/30000 Training Loss: 0.04590769112110138\n",
      "Epoch 22023/30000 Training Loss: 0.05005129426717758\n",
      "Epoch 22024/30000 Training Loss: 0.05233491584658623\n",
      "Epoch 22025/30000 Training Loss: 0.046531982719898224\n",
      "Epoch 22026/30000 Training Loss: 0.03987286612391472\n",
      "Epoch 22027/30000 Training Loss: 0.05009935796260834\n",
      "Epoch 22028/30000 Training Loss: 0.05081170052289963\n",
      "Epoch 22029/30000 Training Loss: 0.03853714093565941\n",
      "Epoch 22030/30000 Training Loss: 0.04770655557513237\n",
      "Epoch 22031/30000 Training Loss: 0.04477065056562424\n",
      "Epoch 22032/30000 Training Loss: 0.03148670867085457\n",
      "Epoch 22033/30000 Training Loss: 0.03329680860042572\n",
      "Epoch 22034/30000 Training Loss: 0.05973561108112335\n",
      "Epoch 22035/30000 Training Loss: 0.03852139413356781\n",
      "Epoch 22036/30000 Training Loss: 0.040642160922288895\n",
      "Epoch 22037/30000 Training Loss: 0.04903489723801613\n",
      "Epoch 22038/30000 Training Loss: 0.040754273533821106\n",
      "Epoch 22039/30000 Training Loss: 0.04704481363296509\n",
      "Epoch 22040/30000 Training Loss: 0.04231894388794899\n",
      "Epoch 22041/30000 Training Loss: 0.046395193785429\n",
      "Epoch 22042/30000 Training Loss: 0.056410446763038635\n",
      "Epoch 22043/30000 Training Loss: 0.03297119587659836\n",
      "Epoch 22044/30000 Training Loss: 0.04456843435764313\n",
      "Epoch 22045/30000 Training Loss: 0.05045043304562569\n",
      "Epoch 22046/30000 Training Loss: 0.037066686898469925\n",
      "Epoch 22047/30000 Training Loss: 0.049113571643829346\n",
      "Epoch 22048/30000 Training Loss: 0.044679127633571625\n",
      "Epoch 22049/30000 Training Loss: 0.05710167437791824\n",
      "Epoch 22050/30000 Training Loss: 0.04466193914413452\n",
      "Epoch 22051/30000 Training Loss: 0.04947514086961746\n",
      "Epoch 22052/30000 Training Loss: 0.04337220638990402\n",
      "Epoch 22053/30000 Training Loss: 0.049689698964357376\n",
      "Epoch 22054/30000 Training Loss: 0.0368615947663784\n",
      "Epoch 22055/30000 Training Loss: 0.04068029299378395\n",
      "Epoch 22056/30000 Training Loss: 0.043882403522729874\n",
      "Epoch 22057/30000 Training Loss: 0.04950634017586708\n",
      "Epoch 22058/30000 Training Loss: 0.04456814005970955\n",
      "Epoch 22059/30000 Training Loss: 0.05442085862159729\n",
      "Epoch 22060/30000 Training Loss: 0.04721519351005554\n",
      "Epoch 22061/30000 Training Loss: 0.02941715717315674\n",
      "Epoch 22062/30000 Training Loss: 0.0402214378118515\n",
      "Epoch 22063/30000 Training Loss: 0.037128996104002\n",
      "Epoch 22064/30000 Training Loss: 0.042830418795347214\n",
      "Epoch 22065/30000 Training Loss: 0.049195297062397\n",
      "Epoch 22066/30000 Training Loss: 0.040042098611593246\n",
      "Epoch 22067/30000 Training Loss: 0.03311755880713463\n",
      "Epoch 22068/30000 Training Loss: 0.04365504905581474\n",
      "Epoch 22069/30000 Training Loss: 0.043832406401634216\n",
      "Epoch 22070/30000 Training Loss: 0.03363342583179474\n",
      "Epoch 22071/30000 Training Loss: 0.05953829735517502\n",
      "Epoch 22072/30000 Training Loss: 0.053245991468429565\n",
      "Epoch 22073/30000 Training Loss: 0.03933706134557724\n",
      "Epoch 22074/30000 Training Loss: 0.04202224314212799\n",
      "Epoch 22075/30000 Training Loss: 0.03702719882130623\n",
      "Epoch 22076/30000 Training Loss: 0.03175618126988411\n",
      "Epoch 22077/30000 Training Loss: 0.04350481927394867\n",
      "Epoch 22078/30000 Training Loss: 0.03756679221987724\n",
      "Epoch 22079/30000 Training Loss: 0.03860372304916382\n",
      "Epoch 22080/30000 Training Loss: 0.0625477284193039\n",
      "Epoch 22081/30000 Training Loss: 0.05376686900854111\n",
      "Epoch 22082/30000 Training Loss: 0.04659300297498703\n",
      "Epoch 22083/30000 Training Loss: 0.047132883220911026\n",
      "Epoch 22084/30000 Training Loss: 0.0500500313937664\n",
      "Epoch 22085/30000 Training Loss: 0.04816964268684387\n",
      "Epoch 22086/30000 Training Loss: 0.041379254311323166\n",
      "Epoch 22087/30000 Training Loss: 0.044833697378635406\n",
      "Epoch 22088/30000 Training Loss: 0.035636335611343384\n",
      "Epoch 22089/30000 Training Loss: 0.03905098885297775\n",
      "Epoch 22090/30000 Training Loss: 0.0472787469625473\n",
      "Epoch 22091/30000 Training Loss: 0.045577313750982285\n",
      "Epoch 22092/30000 Training Loss: 0.04902270436286926\n",
      "Epoch 22093/30000 Training Loss: 0.053912486881017685\n",
      "Epoch 22094/30000 Training Loss: 0.04276757687330246\n",
      "Epoch 22095/30000 Training Loss: 0.04188723489642143\n",
      "Epoch 22096/30000 Training Loss: 0.03441581130027771\n",
      "Epoch 22097/30000 Training Loss: 0.02906760945916176\n",
      "Epoch 22098/30000 Training Loss: 0.0553724579513073\n",
      "Epoch 22099/30000 Training Loss: 0.0350104495882988\n",
      "Epoch 22100/30000 Training Loss: 0.041795942932367325\n",
      "Epoch 22100/30000 Validation Loss: 0.029381096363067627\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.029381096363067627<=============\n",
      "Epoch 22101/30000 Training Loss: 0.04105820879340172\n",
      "Epoch 22102/30000 Training Loss: 0.04098622500896454\n",
      "Epoch 22103/30000 Training Loss: 0.03740234300494194\n",
      "Epoch 22104/30000 Training Loss: 0.043702561408281326\n",
      "Epoch 22105/30000 Training Loss: 0.04945005103945732\n",
      "Epoch 22106/30000 Training Loss: 0.04272995889186859\n",
      "Epoch 22107/30000 Training Loss: 0.04945734143257141\n",
      "Epoch 22108/30000 Training Loss: 0.039510905742645264\n",
      "Epoch 22109/30000 Training Loss: 0.03874695673584938\n",
      "Epoch 22110/30000 Training Loss: 0.046359941363334656\n",
      "Epoch 22111/30000 Training Loss: 0.030497897416353226\n",
      "Epoch 22112/30000 Training Loss: 0.04617343097925186\n",
      "Epoch 22113/30000 Training Loss: 0.046543922275304794\n",
      "Epoch 22114/30000 Training Loss: 0.03744298592209816\n",
      "Epoch 22115/30000 Training Loss: 0.04636260122060776\n",
      "Epoch 22116/30000 Training Loss: 0.04375746101140976\n",
      "Epoch 22117/30000 Training Loss: 0.045195821672677994\n",
      "Epoch 22118/30000 Training Loss: 0.02851550281047821\n",
      "Epoch 22119/30000 Training Loss: 0.0531623475253582\n",
      "Epoch 22120/30000 Training Loss: 0.03958224505186081\n",
      "Epoch 22121/30000 Training Loss: 0.04933720454573631\n",
      "Epoch 22122/30000 Training Loss: 0.039797499775886536\n",
      "Epoch 22123/30000 Training Loss: 0.03737682104110718\n",
      "Epoch 22124/30000 Training Loss: 0.044253937900066376\n",
      "Epoch 22125/30000 Training Loss: 0.040343403816223145\n",
      "Epoch 22126/30000 Training Loss: 0.040534019470214844\n",
      "Epoch 22127/30000 Training Loss: 0.05460859835147858\n",
      "Epoch 22128/30000 Training Loss: 0.04050932452082634\n",
      "Epoch 22129/30000 Training Loss: 0.04421306401491165\n",
      "Epoch 22130/30000 Training Loss: 0.035412244498729706\n",
      "Epoch 22131/30000 Training Loss: 0.0396878644824028\n",
      "Epoch 22132/30000 Training Loss: 0.05191902071237564\n",
      "Epoch 22133/30000 Training Loss: 0.043143272399902344\n",
      "Epoch 22134/30000 Training Loss: 0.045398205518722534\n",
      "Epoch 22135/30000 Training Loss: 0.04884078726172447\n",
      "Epoch 22136/30000 Training Loss: 0.0374654121696949\n",
      "Epoch 22137/30000 Training Loss: 0.03495165333151817\n",
      "Epoch 22138/30000 Training Loss: 0.03966616839170456\n",
      "Epoch 22139/30000 Training Loss: 0.03546199947595596\n",
      "Epoch 22140/30000 Training Loss: 0.044963568449020386\n",
      "Epoch 22141/30000 Training Loss: 0.0421292707324028\n",
      "Epoch 22142/30000 Training Loss: 0.04719655215740204\n",
      "Epoch 22143/30000 Training Loss: 0.03445165604352951\n",
      "Epoch 22144/30000 Training Loss: 0.03393030911684036\n",
      "Epoch 22145/30000 Training Loss: 0.04346748813986778\n",
      "Epoch 22146/30000 Training Loss: 0.0437212735414505\n",
      "Epoch 22147/30000 Training Loss: 0.039128489792346954\n",
      "Epoch 22148/30000 Training Loss: 0.06065300107002258\n",
      "Epoch 22149/30000 Training Loss: 0.048188526183366776\n",
      "Epoch 22150/30000 Training Loss: 0.0476464107632637\n",
      "Epoch 22151/30000 Training Loss: 0.04089444875717163\n",
      "Epoch 22152/30000 Training Loss: 0.03358209878206253\n",
      "Epoch 22153/30000 Training Loss: 0.04948674142360687\n",
      "Epoch 22154/30000 Training Loss: 0.04779063165187836\n",
      "Epoch 22155/30000 Training Loss: 0.035797879099845886\n",
      "Epoch 22156/30000 Training Loss: 0.047054655849933624\n",
      "Epoch 22157/30000 Training Loss: 0.03444879502058029\n",
      "Epoch 22158/30000 Training Loss: 0.03997448459267616\n",
      "Epoch 22159/30000 Training Loss: 0.038367535918951035\n",
      "Epoch 22160/30000 Training Loss: 0.05747661739587784\n",
      "Epoch 22161/30000 Training Loss: 0.04440246522426605\n",
      "Epoch 22162/30000 Training Loss: 0.044892147183418274\n",
      "Epoch 22163/30000 Training Loss: 0.040392160415649414\n",
      "Epoch 22164/30000 Training Loss: 0.04478505626320839\n",
      "Epoch 22165/30000 Training Loss: 0.04895457625389099\n",
      "Epoch 22166/30000 Training Loss: 0.05181672424077988\n",
      "Epoch 22167/30000 Training Loss: 0.052443116903305054\n",
      "Epoch 22168/30000 Training Loss: 0.047054752707481384\n",
      "Epoch 22169/30000 Training Loss: 0.04850729554891586\n",
      "Epoch 22170/30000 Training Loss: 0.04540861025452614\n",
      "Epoch 22171/30000 Training Loss: 0.03589624911546707\n",
      "Epoch 22172/30000 Training Loss: 0.041209474205970764\n",
      "Epoch 22173/30000 Training Loss: 0.04590165615081787\n",
      "Epoch 22174/30000 Training Loss: 0.042662546038627625\n",
      "Epoch 22175/30000 Training Loss: 0.061192646622657776\n",
      "Epoch 22176/30000 Training Loss: 0.04851117357611656\n",
      "Epoch 22177/30000 Training Loss: 0.03613829240202904\n",
      "Epoch 22178/30000 Training Loss: 0.030049318447709084\n",
      "Epoch 22179/30000 Training Loss: 0.030947662889957428\n",
      "Epoch 22180/30000 Training Loss: 0.044133879244327545\n",
      "Epoch 22181/30000 Training Loss: 0.045975204557180405\n",
      "Epoch 22182/30000 Training Loss: 0.04116733372211456\n",
      "Epoch 22183/30000 Training Loss: 0.04510980844497681\n",
      "Epoch 22184/30000 Training Loss: 0.03872270882129669\n",
      "Epoch 22185/30000 Training Loss: 0.03776901215314865\n",
      "Epoch 22186/30000 Training Loss: 0.06791835278272629\n",
      "Epoch 22187/30000 Training Loss: 0.03970903158187866\n",
      "Epoch 22188/30000 Training Loss: 0.033994048833847046\n",
      "Epoch 22189/30000 Training Loss: 0.050305936485528946\n",
      "Epoch 22190/30000 Training Loss: 0.035543158650398254\n",
      "Epoch 22191/30000 Training Loss: 0.053979627788066864\n",
      "Epoch 22192/30000 Training Loss: 0.03872421383857727\n",
      "Epoch 22193/30000 Training Loss: 0.032154522836208344\n",
      "Epoch 22194/30000 Training Loss: 0.044407084584236145\n",
      "Epoch 22195/30000 Training Loss: 0.04890497773885727\n",
      "Epoch 22196/30000 Training Loss: 0.04504731297492981\n",
      "Epoch 22197/30000 Training Loss: 0.058959923684597015\n",
      "Epoch 22198/30000 Training Loss: 0.040000494569540024\n",
      "Epoch 22199/30000 Training Loss: 0.03226703777909279\n",
      "Epoch 22200/30000 Training Loss: 0.039773136377334595\n",
      "Epoch 22200/30000 Validation Loss: 0.04201888293027878\n",
      "Epoch 22201/30000 Training Loss: 0.04270249605178833\n",
      "Epoch 22202/30000 Training Loss: 0.04028192535042763\n",
      "Epoch 22203/30000 Training Loss: 0.0500967837870121\n",
      "Epoch 22204/30000 Training Loss: 0.05861068516969681\n",
      "Epoch 22205/30000 Training Loss: 0.05057799816131592\n",
      "Epoch 22206/30000 Training Loss: 0.04121580719947815\n",
      "Epoch 22207/30000 Training Loss: 0.047230541706085205\n",
      "Epoch 22208/30000 Training Loss: 0.030414998531341553\n",
      "Epoch 22209/30000 Training Loss: 0.03901410102844238\n",
      "Epoch 22210/30000 Training Loss: 0.03381160646677017\n",
      "Epoch 22211/30000 Training Loss: 0.03753424063324928\n",
      "Epoch 22212/30000 Training Loss: 0.02814071625471115\n",
      "Epoch 22213/30000 Training Loss: 0.04224960505962372\n",
      "Epoch 22214/30000 Training Loss: 0.0392519049346447\n",
      "Epoch 22215/30000 Training Loss: 0.04566531255841255\n",
      "Epoch 22216/30000 Training Loss: 0.06425919383764267\n",
      "Epoch 22217/30000 Training Loss: 0.03756333142518997\n",
      "Epoch 22218/30000 Training Loss: 0.048025377094745636\n",
      "Epoch 22219/30000 Training Loss: 0.03590003401041031\n",
      "Epoch 22220/30000 Training Loss: 0.049995094537734985\n",
      "Epoch 22221/30000 Training Loss: 0.03890151530504227\n",
      "Epoch 22222/30000 Training Loss: 0.05031846463680267\n",
      "Epoch 22223/30000 Training Loss: 0.03598174452781677\n",
      "Epoch 22224/30000 Training Loss: 0.03482282534241676\n",
      "Epoch 22225/30000 Training Loss: 0.03925086557865143\n",
      "Epoch 22226/30000 Training Loss: 0.0399978943169117\n",
      "Epoch 22227/30000 Training Loss: 0.042746204882860184\n",
      "Epoch 22228/30000 Training Loss: 0.037343405187129974\n",
      "Epoch 22229/30000 Training Loss: 0.041441746056079865\n",
      "Epoch 22230/30000 Training Loss: 0.0426623560488224\n",
      "Epoch 22231/30000 Training Loss: 0.026444371789693832\n",
      "Epoch 22232/30000 Training Loss: 0.04886648803949356\n",
      "Epoch 22233/30000 Training Loss: 0.030629929155111313\n",
      "Epoch 22234/30000 Training Loss: 0.04923383891582489\n",
      "Epoch 22235/30000 Training Loss: 0.03851774334907532\n",
      "Epoch 22236/30000 Training Loss: 0.05496368929743767\n",
      "Epoch 22237/30000 Training Loss: 0.04718450456857681\n",
      "Epoch 22238/30000 Training Loss: 0.04397901892662048\n",
      "Epoch 22239/30000 Training Loss: 0.04781000688672066\n",
      "Epoch 22240/30000 Training Loss: 0.05268450453877449\n",
      "Epoch 22241/30000 Training Loss: 0.04462975263595581\n",
      "Epoch 22242/30000 Training Loss: 0.043914977461099625\n",
      "Epoch 22243/30000 Training Loss: 0.0546923466026783\n",
      "Epoch 22244/30000 Training Loss: 0.04483054205775261\n",
      "Epoch 22245/30000 Training Loss: 0.053994692862033844\n",
      "Epoch 22246/30000 Training Loss: 0.041536595672369\n",
      "Epoch 22247/30000 Training Loss: 0.043160028755664825\n",
      "Epoch 22248/30000 Training Loss: 0.03698040544986725\n",
      "Epoch 22249/30000 Training Loss: 0.04589308798313141\n",
      "Epoch 22250/30000 Training Loss: 0.04585259035229683\n",
      "Epoch 22251/30000 Training Loss: 0.03947712108492851\n",
      "Epoch 22252/30000 Training Loss: 0.03461053967475891\n",
      "Epoch 22253/30000 Training Loss: 0.041090987622737885\n",
      "Epoch 22254/30000 Training Loss: 0.060157790780067444\n",
      "Epoch 22255/30000 Training Loss: 0.03737232834100723\n",
      "Epoch 22256/30000 Training Loss: 0.039442889392375946\n",
      "Epoch 22257/30000 Training Loss: 0.03453046455979347\n",
      "Epoch 22258/30000 Training Loss: 0.04077214375138283\n",
      "Epoch 22259/30000 Training Loss: 0.040260642766952515\n",
      "Epoch 22260/30000 Training Loss: 0.04447232931852341\n",
      "Epoch 22261/30000 Training Loss: 0.041914843022823334\n",
      "Epoch 22262/30000 Training Loss: 0.034275323152542114\n",
      "Epoch 22263/30000 Training Loss: 0.03994925320148468\n",
      "Epoch 22264/30000 Training Loss: 0.0384339913725853\n",
      "Epoch 22265/30000 Training Loss: 0.051465198397636414\n",
      "Epoch 22266/30000 Training Loss: 0.049943093210458755\n",
      "Epoch 22267/30000 Training Loss: 0.054726552218198776\n",
      "Epoch 22268/30000 Training Loss: 0.04274836927652359\n",
      "Epoch 22269/30000 Training Loss: 0.05158710852265358\n",
      "Epoch 22270/30000 Training Loss: 0.054928455501794815\n",
      "Epoch 22271/30000 Training Loss: 0.04345555230975151\n",
      "Epoch 22272/30000 Training Loss: 0.039485830813646317\n",
      "Epoch 22273/30000 Training Loss: 0.059670500457286835\n",
      "Epoch 22274/30000 Training Loss: 0.035152412950992584\n",
      "Epoch 22275/30000 Training Loss: 0.0500032939016819\n",
      "Epoch 22276/30000 Training Loss: 0.04851840063929558\n",
      "Epoch 22277/30000 Training Loss: 0.03955573961138725\n",
      "Epoch 22278/30000 Training Loss: 0.04153204336762428\n",
      "Epoch 22279/30000 Training Loss: 0.059139423072338104\n",
      "Epoch 22280/30000 Training Loss: 0.02951977401971817\n",
      "Epoch 22281/30000 Training Loss: 0.04759474843740463\n",
      "Epoch 22282/30000 Training Loss: 0.037135299295186996\n",
      "Epoch 22283/30000 Training Loss: 0.06401991844177246\n",
      "Epoch 22284/30000 Training Loss: 0.0428541824221611\n",
      "Epoch 22285/30000 Training Loss: 0.03479932248592377\n",
      "Epoch 22286/30000 Training Loss: 0.03397050499916077\n",
      "Epoch 22287/30000 Training Loss: 0.03851107507944107\n",
      "Epoch 22288/30000 Training Loss: 0.047330405563116074\n",
      "Epoch 22289/30000 Training Loss: 0.03365372493863106\n",
      "Epoch 22290/30000 Training Loss: 0.034667111933231354\n",
      "Epoch 22291/30000 Training Loss: 0.04072552174329758\n",
      "Epoch 22292/30000 Training Loss: 0.040159791707992554\n",
      "Epoch 22293/30000 Training Loss: 0.0291268453001976\n",
      "Epoch 22294/30000 Training Loss: 0.048884548246860504\n",
      "Epoch 22295/30000 Training Loss: 0.03136427700519562\n",
      "Epoch 22296/30000 Training Loss: 0.03928651288151741\n",
      "Epoch 22297/30000 Training Loss: 0.03986837714910507\n",
      "Epoch 22298/30000 Training Loss: 0.026364155113697052\n",
      "Epoch 22299/30000 Training Loss: 0.039527781307697296\n",
      "Epoch 22300/30000 Training Loss: 0.04516114294528961\n",
      "Epoch 22300/30000 Validation Loss: 0.05040612071752548\n",
      "Epoch 22301/30000 Training Loss: 0.062448304146528244\n",
      "Epoch 22302/30000 Training Loss: 0.048712942749261856\n",
      "Epoch 22303/30000 Training Loss: 0.04410053417086601\n",
      "Epoch 22304/30000 Training Loss: 0.035764794796705246\n",
      "Epoch 22305/30000 Training Loss: 0.03778228908777237\n",
      "Epoch 22306/30000 Training Loss: 0.04349243640899658\n",
      "Epoch 22307/30000 Training Loss: 0.03917940706014633\n",
      "Epoch 22308/30000 Training Loss: 0.04659264534711838\n",
      "Epoch 22309/30000 Training Loss: 0.04451492056250572\n",
      "Epoch 22310/30000 Training Loss: 0.042459819465875626\n",
      "Epoch 22311/30000 Training Loss: 0.043665170669555664\n",
      "Epoch 22312/30000 Training Loss: 0.05809000879526138\n",
      "Epoch 22313/30000 Training Loss: 0.047986455261707306\n",
      "Epoch 22314/30000 Training Loss: 0.04243136942386627\n",
      "Epoch 22315/30000 Training Loss: 0.0573723278939724\n",
      "Epoch 22316/30000 Training Loss: 0.0449780710041523\n",
      "Epoch 22317/30000 Training Loss: 0.051738839596509933\n",
      "Epoch 22318/30000 Training Loss: 0.041936617344617844\n",
      "Epoch 22319/30000 Training Loss: 0.052035585045814514\n",
      "Epoch 22320/30000 Training Loss: 0.04509199038147926\n",
      "Epoch 22321/30000 Training Loss: 0.046031273901462555\n",
      "Epoch 22322/30000 Training Loss: 0.04125348478555679\n",
      "Epoch 22323/30000 Training Loss: 0.04441078379750252\n",
      "Epoch 22324/30000 Training Loss: 0.03472791239619255\n",
      "Epoch 22325/30000 Training Loss: 0.0285350289195776\n",
      "Epoch 22326/30000 Training Loss: 0.03637930750846863\n",
      "Epoch 22327/30000 Training Loss: 0.03009377233684063\n",
      "Epoch 22328/30000 Training Loss: 0.03635242208838463\n",
      "Epoch 22329/30000 Training Loss: 0.045353963971138\n",
      "Epoch 22330/30000 Training Loss: 0.045135822147130966\n",
      "Epoch 22331/30000 Training Loss: 0.03266284987330437\n",
      "Epoch 22332/30000 Training Loss: 0.05033916234970093\n",
      "Epoch 22333/30000 Training Loss: 0.04151739925146103\n",
      "Epoch 22334/30000 Training Loss: 0.04640647768974304\n",
      "Epoch 22335/30000 Training Loss: 0.05755745619535446\n",
      "Epoch 22336/30000 Training Loss: 0.04218050464987755\n",
      "Epoch 22337/30000 Training Loss: 0.04006244242191315\n",
      "Epoch 22338/30000 Training Loss: 0.04111138731241226\n",
      "Epoch 22339/30000 Training Loss: 0.04571288079023361\n",
      "Epoch 22340/30000 Training Loss: 0.061455972492694855\n",
      "Epoch 22341/30000 Training Loss: 0.036809291690588\n",
      "Epoch 22342/30000 Training Loss: 0.04821554198861122\n",
      "Epoch 22343/30000 Training Loss: 0.04921363294124603\n",
      "Epoch 22344/30000 Training Loss: 0.038599394261837006\n",
      "Epoch 22345/30000 Training Loss: 0.046214036643505096\n",
      "Epoch 22346/30000 Training Loss: 0.03760376572608948\n",
      "Epoch 22347/30000 Training Loss: 0.03867904469370842\n",
      "Epoch 22348/30000 Training Loss: 0.03186514601111412\n",
      "Epoch 22349/30000 Training Loss: 0.038416918367147446\n",
      "Epoch 22350/30000 Training Loss: 0.0471799299120903\n",
      "Epoch 22351/30000 Training Loss: 0.04222901165485382\n",
      "Epoch 22352/30000 Training Loss: 0.03942212089896202\n",
      "Epoch 22353/30000 Training Loss: 0.04462637007236481\n",
      "Epoch 22354/30000 Training Loss: 0.041363425552845\n",
      "Epoch 22355/30000 Training Loss: 0.04211394488811493\n",
      "Epoch 22356/30000 Training Loss: 0.04850330576300621\n",
      "Epoch 22357/30000 Training Loss: 0.055598512291908264\n",
      "Epoch 22358/30000 Training Loss: 0.04828934371471405\n",
      "Epoch 22359/30000 Training Loss: 0.039059028029441833\n",
      "Epoch 22360/30000 Training Loss: 0.03588610514998436\n",
      "Epoch 22361/30000 Training Loss: 0.0435786098241806\n",
      "Epoch 22362/30000 Training Loss: 0.035281889140605927\n",
      "Epoch 22363/30000 Training Loss: 0.038819506764411926\n",
      "Epoch 22364/30000 Training Loss: 0.039873894304037094\n",
      "Epoch 22365/30000 Training Loss: 0.04330899566411972\n",
      "Epoch 22366/30000 Training Loss: 0.05959869921207428\n",
      "Epoch 22367/30000 Training Loss: 0.044042300432920456\n",
      "Epoch 22368/30000 Training Loss: 0.04504542425274849\n",
      "Epoch 22369/30000 Training Loss: 0.040764808654785156\n",
      "Epoch 22370/30000 Training Loss: 0.053295064717531204\n",
      "Epoch 22371/30000 Training Loss: 0.05540316551923752\n",
      "Epoch 22372/30000 Training Loss: 0.043505653738975525\n",
      "Epoch 22373/30000 Training Loss: 0.045981764793395996\n",
      "Epoch 22374/30000 Training Loss: 0.04995538294315338\n",
      "Epoch 22375/30000 Training Loss: 0.03454963117837906\n",
      "Epoch 22376/30000 Training Loss: 0.04700300842523575\n",
      "Epoch 22377/30000 Training Loss: 0.03338552266359329\n",
      "Epoch 22378/30000 Training Loss: 0.04308328777551651\n",
      "Epoch 22379/30000 Training Loss: 0.05516675487160683\n",
      "Epoch 22380/30000 Training Loss: 0.04907362908124924\n",
      "Epoch 22381/30000 Training Loss: 0.055047981441020966\n",
      "Epoch 22382/30000 Training Loss: 0.03336292505264282\n",
      "Epoch 22383/30000 Training Loss: 0.04562908783555031\n",
      "Epoch 22384/30000 Training Loss: 0.04578416049480438\n",
      "Epoch 22385/30000 Training Loss: 0.05236854404211044\n",
      "Epoch 22386/30000 Training Loss: 0.0336473323404789\n",
      "Epoch 22387/30000 Training Loss: 0.043548330664634705\n",
      "Epoch 22388/30000 Training Loss: 0.03990238532423973\n",
      "Epoch 22389/30000 Training Loss: 0.045921191573143005\n",
      "Epoch 22390/30000 Training Loss: 0.03823075443506241\n",
      "Epoch 22391/30000 Training Loss: 0.04585731402039528\n",
      "Epoch 22392/30000 Training Loss: 0.050930462777614594\n",
      "Epoch 22393/30000 Training Loss: 0.03446013480424881\n",
      "Epoch 22394/30000 Training Loss: 0.05168191343545914\n",
      "Epoch 22395/30000 Training Loss: 0.028687279671430588\n",
      "Epoch 22396/30000 Training Loss: 0.04152262955904007\n",
      "Epoch 22397/30000 Training Loss: 0.03997425362467766\n",
      "Epoch 22398/30000 Training Loss: 0.039180122315883636\n",
      "Epoch 22399/30000 Training Loss: 0.046057116240262985\n",
      "Epoch 22400/30000 Training Loss: 0.037057045847177505\n",
      "Epoch 22400/30000 Validation Loss: 0.025287512689828873\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.025287512689828873<=============\n",
      "Epoch 22401/30000 Training Loss: 0.05193353444337845\n",
      "Epoch 22402/30000 Training Loss: 0.04808682203292847\n",
      "Epoch 22403/30000 Training Loss: 0.04275639355182648\n",
      "Epoch 22404/30000 Training Loss: 0.03662768006324768\n",
      "Epoch 22405/30000 Training Loss: 0.05538039654493332\n",
      "Epoch 22406/30000 Training Loss: 0.03977927938103676\n",
      "Epoch 22407/30000 Training Loss: 0.03343724459409714\n",
      "Epoch 22408/30000 Training Loss: 0.03982372581958771\n",
      "Epoch 22409/30000 Training Loss: 0.04748590663075447\n",
      "Epoch 22410/30000 Training Loss: 0.048428989946842194\n",
      "Epoch 22411/30000 Training Loss: 0.035625554621219635\n",
      "Epoch 22412/30000 Training Loss: 0.035172417759895325\n",
      "Epoch 22413/30000 Training Loss: 0.03832421451807022\n",
      "Epoch 22414/30000 Training Loss: 0.04877971112728119\n",
      "Epoch 22415/30000 Training Loss: 0.04560939967632294\n",
      "Epoch 22416/30000 Training Loss: 0.056561149656772614\n",
      "Epoch 22417/30000 Training Loss: 0.040166549384593964\n",
      "Epoch 22418/30000 Training Loss: 0.04785064607858658\n",
      "Epoch 22419/30000 Training Loss: 0.03781849518418312\n",
      "Epoch 22420/30000 Training Loss: 0.04755289480090141\n",
      "Epoch 22421/30000 Training Loss: 0.04883509501814842\n",
      "Epoch 22422/30000 Training Loss: 0.046231843531131744\n",
      "Epoch 22423/30000 Training Loss: 0.03796682506799698\n",
      "Epoch 22424/30000 Training Loss: 0.042704980820417404\n",
      "Epoch 22425/30000 Training Loss: 0.04813268780708313\n",
      "Epoch 22426/30000 Training Loss: 0.052389174699783325\n",
      "Epoch 22427/30000 Training Loss: 0.03994069993495941\n",
      "Epoch 22428/30000 Training Loss: 0.03896688297390938\n",
      "Epoch 22429/30000 Training Loss: 0.047806113958358765\n",
      "Epoch 22430/30000 Training Loss: 0.0378444567322731\n",
      "Epoch 22431/30000 Training Loss: 0.0452529601752758\n",
      "Epoch 22432/30000 Training Loss: 0.052187513560056686\n",
      "Epoch 22433/30000 Training Loss: 0.035301122814416885\n",
      "Epoch 22434/30000 Training Loss: 0.035781413316726685\n",
      "Epoch 22435/30000 Training Loss: 0.041298314929008484\n",
      "Epoch 22436/30000 Training Loss: 0.042250290513038635\n",
      "Epoch 22437/30000 Training Loss: 0.03212593123316765\n",
      "Epoch 22438/30000 Training Loss: 0.04405420273542404\n",
      "Epoch 22439/30000 Training Loss: 0.04588240385055542\n",
      "Epoch 22440/30000 Training Loss: 0.05043601244688034\n",
      "Epoch 22441/30000 Training Loss: 0.061865150928497314\n",
      "Epoch 22442/30000 Training Loss: 0.041584134101867676\n",
      "Epoch 22443/30000 Training Loss: 0.05387786030769348\n",
      "Epoch 22444/30000 Training Loss: 0.038396138697862625\n",
      "Epoch 22445/30000 Training Loss: 0.029503408819437027\n",
      "Epoch 22446/30000 Training Loss: 0.0559287928044796\n",
      "Epoch 22447/30000 Training Loss: 0.0411229282617569\n",
      "Epoch 22448/30000 Training Loss: 0.04832833260297775\n",
      "Epoch 22449/30000 Training Loss: 0.029390450567007065\n",
      "Epoch 22450/30000 Training Loss: 0.056399304419755936\n",
      "Epoch 22451/30000 Training Loss: 0.03235512971878052\n",
      "Epoch 22452/30000 Training Loss: 0.04352676868438721\n",
      "Epoch 22453/30000 Training Loss: 0.04817178472876549\n",
      "Epoch 22454/30000 Training Loss: 0.041959039866924286\n",
      "Epoch 22455/30000 Training Loss: 0.05037478357553482\n",
      "Epoch 22456/30000 Training Loss: 0.03758632391691208\n",
      "Epoch 22457/30000 Training Loss: 0.04002765193581581\n",
      "Epoch 22458/30000 Training Loss: 0.02677624672651291\n",
      "Epoch 22459/30000 Training Loss: 0.049043331295251846\n",
      "Epoch 22460/30000 Training Loss: 0.04845261201262474\n",
      "Epoch 22461/30000 Training Loss: 0.049517154693603516\n",
      "Epoch 22462/30000 Training Loss: 0.047308020293712616\n",
      "Epoch 22463/30000 Training Loss: 0.03325467184185982\n",
      "Epoch 22464/30000 Training Loss: 0.05677492916584015\n",
      "Epoch 22465/30000 Training Loss: 0.04058666527271271\n",
      "Epoch 22466/30000 Training Loss: 0.04200667887926102\n",
      "Epoch 22467/30000 Training Loss: 0.037383753806352615\n",
      "Epoch 22468/30000 Training Loss: 0.04171900823712349\n",
      "Epoch 22469/30000 Training Loss: 0.04493545740842819\n",
      "Epoch 22470/30000 Training Loss: 0.0507030114531517\n",
      "Epoch 22471/30000 Training Loss: 0.04336664453148842\n",
      "Epoch 22472/30000 Training Loss: 0.038672901690006256\n",
      "Epoch 22473/30000 Training Loss: 0.043744321912527084\n",
      "Epoch 22474/30000 Training Loss: 0.040689483284950256\n",
      "Epoch 22475/30000 Training Loss: 0.03652787581086159\n",
      "Epoch 22476/30000 Training Loss: 0.030713964253664017\n",
      "Epoch 22477/30000 Training Loss: 0.03678309544920921\n",
      "Epoch 22478/30000 Training Loss: 0.04229174181818962\n",
      "Epoch 22479/30000 Training Loss: 0.041472937911748886\n",
      "Epoch 22480/30000 Training Loss: 0.043785460293293\n",
      "Epoch 22481/30000 Training Loss: 0.05337083339691162\n",
      "Epoch 22482/30000 Training Loss: 0.04068318009376526\n",
      "Epoch 22483/30000 Training Loss: 0.04256652668118477\n",
      "Epoch 22484/30000 Training Loss: 0.053477972745895386\n",
      "Epoch 22485/30000 Training Loss: 0.03897883743047714\n",
      "Epoch 22486/30000 Training Loss: 0.048503655940294266\n",
      "Epoch 22487/30000 Training Loss: 0.04286310821771622\n",
      "Epoch 22488/30000 Training Loss: 0.05508607253432274\n",
      "Epoch 22489/30000 Training Loss: 0.03704597055912018\n",
      "Epoch 22490/30000 Training Loss: 0.03706716001033783\n",
      "Epoch 22491/30000 Training Loss: 0.05892786383628845\n",
      "Epoch 22492/30000 Training Loss: 0.047446489334106445\n",
      "Epoch 22493/30000 Training Loss: 0.04377128928899765\n",
      "Epoch 22494/30000 Training Loss: 0.05245178937911987\n",
      "Epoch 22495/30000 Training Loss: 0.03902529925107956\n",
      "Epoch 22496/30000 Training Loss: 0.055851735174655914\n",
      "Epoch 22497/30000 Training Loss: 0.040972404181957245\n",
      "Epoch 22498/30000 Training Loss: 0.04099151864647865\n",
      "Epoch 22499/30000 Training Loss: 0.035586245357990265\n",
      "Epoch 22500/30000 Training Loss: 0.04771994799375534\n",
      "Epoch 22500/30000 Validation Loss: 0.05434040725231171\n",
      "Epoch 22501/30000 Training Loss: 0.03785928338766098\n",
      "Epoch 22502/30000 Training Loss: 0.029257535934448242\n",
      "Epoch 22503/30000 Training Loss: 0.04472261294722557\n",
      "Epoch 22504/30000 Training Loss: 0.04540826752781868\n",
      "Epoch 22505/30000 Training Loss: 0.04402822256088257\n",
      "Epoch 22506/30000 Training Loss: 0.057479992508888245\n",
      "Epoch 22507/30000 Training Loss: 0.04343529790639877\n",
      "Epoch 22508/30000 Training Loss: 0.03962637484073639\n",
      "Epoch 22509/30000 Training Loss: 0.04803480952978134\n",
      "Epoch 22510/30000 Training Loss: 0.042220838367938995\n",
      "Epoch 22511/30000 Training Loss: 0.05379975587129593\n",
      "Epoch 22512/30000 Training Loss: 0.044849641621112823\n",
      "Epoch 22513/30000 Training Loss: 0.044184595346450806\n",
      "Epoch 22514/30000 Training Loss: 0.02964930422604084\n",
      "Epoch 22515/30000 Training Loss: 0.047121498733758926\n",
      "Epoch 22516/30000 Training Loss: 0.04170858487486839\n",
      "Epoch 22517/30000 Training Loss: 0.03147386759519577\n",
      "Epoch 22518/30000 Training Loss: 0.05155714973807335\n",
      "Epoch 22519/30000 Training Loss: 0.03687261790037155\n",
      "Epoch 22520/30000 Training Loss: 0.051616691052913666\n",
      "Epoch 22521/30000 Training Loss: 0.0425824299454689\n",
      "Epoch 22522/30000 Training Loss: 0.04864279925823212\n",
      "Epoch 22523/30000 Training Loss: 0.05211672931909561\n",
      "Epoch 22524/30000 Training Loss: 0.03246038779616356\n",
      "Epoch 22525/30000 Training Loss: 0.04053788632154465\n",
      "Epoch 22526/30000 Training Loss: 0.04055556282401085\n",
      "Epoch 22527/30000 Training Loss: 0.046550266444683075\n",
      "Epoch 22528/30000 Training Loss: 0.04813678562641144\n",
      "Epoch 22529/30000 Training Loss: 0.03906712681055069\n",
      "Epoch 22530/30000 Training Loss: 0.034868866205215454\n",
      "Epoch 22531/30000 Training Loss: 0.04858463257551193\n",
      "Epoch 22532/30000 Training Loss: 0.05453050509095192\n",
      "Epoch 22533/30000 Training Loss: 0.0436435267329216\n",
      "Epoch 22534/30000 Training Loss: 0.03832375258207321\n",
      "Epoch 22535/30000 Training Loss: 0.03345397859811783\n",
      "Epoch 22536/30000 Training Loss: 0.04116680473089218\n",
      "Epoch 22537/30000 Training Loss: 0.052769243717193604\n",
      "Epoch 22538/30000 Training Loss: 0.03769524022936821\n",
      "Epoch 22539/30000 Training Loss: 0.04719429463148117\n",
      "Epoch 22540/30000 Training Loss: 0.050705067813396454\n",
      "Epoch 22541/30000 Training Loss: 0.038119129836559296\n",
      "Epoch 22542/30000 Training Loss: 0.053645819425582886\n",
      "Epoch 22543/30000 Training Loss: 0.045445069670677185\n",
      "Epoch 22544/30000 Training Loss: 0.04106038808822632\n",
      "Epoch 22545/30000 Training Loss: 0.045373186469078064\n",
      "Epoch 22546/30000 Training Loss: 0.036442652344703674\n",
      "Epoch 22547/30000 Training Loss: 0.03956461325287819\n",
      "Epoch 22548/30000 Training Loss: 0.049902379512786865\n",
      "Epoch 22549/30000 Training Loss: 0.049375616014003754\n",
      "Epoch 22550/30000 Training Loss: 0.04067610204219818\n",
      "Epoch 22551/30000 Training Loss: 0.047644246369600296\n",
      "Epoch 22552/30000 Training Loss: 0.046538446098566055\n",
      "Epoch 22553/30000 Training Loss: 0.04114421084523201\n",
      "Epoch 22554/30000 Training Loss: 0.04273943975567818\n",
      "Epoch 22555/30000 Training Loss: 0.04666370898485184\n",
      "Epoch 22556/30000 Training Loss: 0.03971610590815544\n",
      "Epoch 22557/30000 Training Loss: 0.04798295721411705\n",
      "Epoch 22558/30000 Training Loss: 0.04346437007188797\n",
      "Epoch 22559/30000 Training Loss: 0.04533524438738823\n",
      "Epoch 22560/30000 Training Loss: 0.04132714495062828\n",
      "Epoch 22561/30000 Training Loss: 0.04748089611530304\n",
      "Epoch 22562/30000 Training Loss: 0.04784663766622543\n",
      "Epoch 22563/30000 Training Loss: 0.04184350371360779\n",
      "Epoch 22564/30000 Training Loss: 0.04728708043694496\n",
      "Epoch 22565/30000 Training Loss: 0.03780115395784378\n",
      "Epoch 22566/30000 Training Loss: 0.030635766685009003\n",
      "Epoch 22567/30000 Training Loss: 0.061234913766384125\n",
      "Epoch 22568/30000 Training Loss: 0.03874102979898453\n",
      "Epoch 22569/30000 Training Loss: 0.05551094561815262\n",
      "Epoch 22570/30000 Training Loss: 0.052419766783714294\n",
      "Epoch 22571/30000 Training Loss: 0.04129839316010475\n",
      "Epoch 22572/30000 Training Loss: 0.05167461559176445\n",
      "Epoch 22573/30000 Training Loss: 0.044399138540029526\n",
      "Epoch 22574/30000 Training Loss: 0.04302293807268143\n",
      "Epoch 22575/30000 Training Loss: 0.04520505666732788\n",
      "Epoch 22576/30000 Training Loss: 0.03379526734352112\n",
      "Epoch 22577/30000 Training Loss: 0.04691823199391365\n",
      "Epoch 22578/30000 Training Loss: 0.04369839280843735\n",
      "Epoch 22579/30000 Training Loss: 0.04402165859937668\n",
      "Epoch 22580/30000 Training Loss: 0.05297480523586273\n",
      "Epoch 22581/30000 Training Loss: 0.03754206374287605\n",
      "Epoch 22582/30000 Training Loss: 0.03787165880203247\n",
      "Epoch 22583/30000 Training Loss: 0.04750581458210945\n",
      "Epoch 22584/30000 Training Loss: 0.038666944950819016\n",
      "Epoch 22585/30000 Training Loss: 0.034661196172237396\n",
      "Epoch 22586/30000 Training Loss: 0.03939121216535568\n",
      "Epoch 22587/30000 Training Loss: 0.06212818622589111\n",
      "Epoch 22588/30000 Training Loss: 0.03493155539035797\n",
      "Epoch 22589/30000 Training Loss: 0.042267806828022\n",
      "Epoch 22590/30000 Training Loss: 0.034342922270298004\n",
      "Epoch 22591/30000 Training Loss: 0.056375861167907715\n",
      "Epoch 22592/30000 Training Loss: 0.04584168270230293\n",
      "Epoch 22593/30000 Training Loss: 0.03912796080112457\n",
      "Epoch 22594/30000 Training Loss: 0.04264833778142929\n",
      "Epoch 22595/30000 Training Loss: 0.04234439879655838\n",
      "Epoch 22596/30000 Training Loss: 0.04450581595301628\n",
      "Epoch 22597/30000 Training Loss: 0.0552777498960495\n",
      "Epoch 22598/30000 Training Loss: 0.03586553782224655\n",
      "Epoch 22599/30000 Training Loss: 0.04146403819322586\n",
      "Epoch 22600/30000 Training Loss: 0.052339818328619\n",
      "Epoch 22600/30000 Validation Loss: 0.040181972086429596\n",
      "Epoch 22601/30000 Training Loss: 0.03410555049777031\n",
      "Epoch 22602/30000 Training Loss: 0.044980812817811966\n",
      "Epoch 22603/30000 Training Loss: 0.04725723713636398\n",
      "Epoch 22604/30000 Training Loss: 0.04191385954618454\n",
      "Epoch 22605/30000 Training Loss: 0.04312330111861229\n",
      "Epoch 22606/30000 Training Loss: 0.039830282330513\n",
      "Epoch 22607/30000 Training Loss: 0.03956839442253113\n",
      "Epoch 22608/30000 Training Loss: 0.03496331349015236\n",
      "Epoch 22609/30000 Training Loss: 0.05055396258831024\n",
      "Epoch 22610/30000 Training Loss: 0.03524510934948921\n",
      "Epoch 22611/30000 Training Loss: 0.051933057606220245\n",
      "Epoch 22612/30000 Training Loss: 0.03974410146474838\n",
      "Epoch 22613/30000 Training Loss: 0.0378735214471817\n",
      "Epoch 22614/30000 Training Loss: 0.04427523910999298\n",
      "Epoch 22615/30000 Training Loss: 0.036828070878982544\n",
      "Epoch 22616/30000 Training Loss: 0.0466434471309185\n",
      "Epoch 22617/30000 Training Loss: 0.043416861444711685\n",
      "Epoch 22618/30000 Training Loss: 0.04913611710071564\n",
      "Epoch 22619/30000 Training Loss: 0.027499228715896606\n",
      "Epoch 22620/30000 Training Loss: 0.053467702120542526\n",
      "Epoch 22621/30000 Training Loss: 0.035686515271663666\n",
      "Epoch 22622/30000 Training Loss: 0.05342418700456619\n",
      "Epoch 22623/30000 Training Loss: 0.042702339589595795\n",
      "Epoch 22624/30000 Training Loss: 0.03801028057932854\n",
      "Epoch 22625/30000 Training Loss: 0.03962106257677078\n",
      "Epoch 22626/30000 Training Loss: 0.0408712700009346\n",
      "Epoch 22627/30000 Training Loss: 0.04269259050488472\n",
      "Epoch 22628/30000 Training Loss: 0.043565183877944946\n",
      "Epoch 22629/30000 Training Loss: 0.03607027605175972\n",
      "Epoch 22630/30000 Training Loss: 0.047532226890325546\n",
      "Epoch 22631/30000 Training Loss: 0.05113416165113449\n",
      "Epoch 22632/30000 Training Loss: 0.04609900712966919\n",
      "Epoch 22633/30000 Training Loss: 0.04819958657026291\n",
      "Epoch 22634/30000 Training Loss: 0.029873177409172058\n",
      "Epoch 22635/30000 Training Loss: 0.047521669417619705\n",
      "Epoch 22636/30000 Training Loss: 0.0401034913957119\n",
      "Epoch 22637/30000 Training Loss: 0.04509308189153671\n",
      "Epoch 22638/30000 Training Loss: 0.04429499804973602\n",
      "Epoch 22639/30000 Training Loss: 0.0437675304710865\n",
      "Epoch 22640/30000 Training Loss: 0.04449599236249924\n",
      "Epoch 22641/30000 Training Loss: 0.04248466342687607\n",
      "Epoch 22642/30000 Training Loss: 0.042296089231967926\n",
      "Epoch 22643/30000 Training Loss: 0.05593603849411011\n",
      "Epoch 22644/30000 Training Loss: 0.040136754512786865\n",
      "Epoch 22645/30000 Training Loss: 0.04201357811689377\n",
      "Epoch 22646/30000 Training Loss: 0.03457656875252724\n",
      "Epoch 22647/30000 Training Loss: 0.042592961341142654\n",
      "Epoch 22648/30000 Training Loss: 0.03845199942588806\n",
      "Epoch 22649/30000 Training Loss: 0.050422631204128265\n",
      "Epoch 22650/30000 Training Loss: 0.04405916482210159\n",
      "Epoch 22651/30000 Training Loss: 0.04841822385787964\n",
      "Epoch 22652/30000 Training Loss: 0.044217318296432495\n",
      "Epoch 22653/30000 Training Loss: 0.04613092541694641\n",
      "Epoch 22654/30000 Training Loss: 0.04588194563984871\n",
      "Epoch 22655/30000 Training Loss: 0.041111961007118225\n",
      "Epoch 22656/30000 Training Loss: 0.05655324459075928\n",
      "Epoch 22657/30000 Training Loss: 0.038429901003837585\n",
      "Epoch 22658/30000 Training Loss: 0.04054053872823715\n",
      "Epoch 22659/30000 Training Loss: 0.039958469569683075\n",
      "Epoch 22660/30000 Training Loss: 0.04947267100214958\n",
      "Epoch 22661/30000 Training Loss: 0.039461880922317505\n",
      "Epoch 22662/30000 Training Loss: 0.037881039083004\n",
      "Epoch 22663/30000 Training Loss: 0.043727897107601166\n",
      "Epoch 22664/30000 Training Loss: 0.039780113846063614\n",
      "Epoch 22665/30000 Training Loss: 0.03319298475980759\n",
      "Epoch 22666/30000 Training Loss: 0.046653345227241516\n",
      "Epoch 22667/30000 Training Loss: 0.062358930706977844\n",
      "Epoch 22668/30000 Training Loss: 0.048019975423812866\n",
      "Epoch 22669/30000 Training Loss: 0.04062425717711449\n",
      "Epoch 22670/30000 Training Loss: 0.038510728627443314\n",
      "Epoch 22671/30000 Training Loss: 0.034784283488988876\n",
      "Epoch 22672/30000 Training Loss: 0.044415514916181564\n",
      "Epoch 22673/30000 Training Loss: 0.041186150163412094\n",
      "Epoch 22674/30000 Training Loss: 0.035006679594516754\n",
      "Epoch 22675/30000 Training Loss: 0.039166469126939774\n",
      "Epoch 22676/30000 Training Loss: 0.0336509570479393\n",
      "Epoch 22677/30000 Training Loss: 0.04681570827960968\n",
      "Epoch 22678/30000 Training Loss: 0.037677690386772156\n",
      "Epoch 22679/30000 Training Loss: 0.047309137880802155\n",
      "Epoch 22680/30000 Training Loss: 0.05537363886833191\n",
      "Epoch 22681/30000 Training Loss: 0.04665396735072136\n",
      "Epoch 22682/30000 Training Loss: 0.04369860887527466\n",
      "Epoch 22683/30000 Training Loss: 0.05523198843002319\n",
      "Epoch 22684/30000 Training Loss: 0.04134234040975571\n",
      "Epoch 22685/30000 Training Loss: 0.0419066846370697\n",
      "Epoch 22686/30000 Training Loss: 0.0604882538318634\n",
      "Epoch 22687/30000 Training Loss: 0.042064595967531204\n",
      "Epoch 22688/30000 Training Loss: 0.05048462003469467\n",
      "Epoch 22689/30000 Training Loss: 0.03666622191667557\n",
      "Epoch 22690/30000 Training Loss: 0.05820504575967789\n",
      "Epoch 22691/30000 Training Loss: 0.033438872545957565\n",
      "Epoch 22692/30000 Training Loss: 0.03704996034502983\n",
      "Epoch 22693/30000 Training Loss: 0.046233467757701874\n",
      "Epoch 22694/30000 Training Loss: 0.047059014439582825\n",
      "Epoch 22695/30000 Training Loss: 0.042831964790821075\n",
      "Epoch 22696/30000 Training Loss: 0.03929667919874191\n",
      "Epoch 22697/30000 Training Loss: 0.042307958006858826\n",
      "Epoch 22698/30000 Training Loss: 0.043439969420433044\n",
      "Epoch 22699/30000 Training Loss: 0.04315818473696709\n",
      "Epoch 22700/30000 Training Loss: 0.052403200417757034\n",
      "Epoch 22700/30000 Validation Loss: 0.03993266820907593\n",
      "Epoch 22701/30000 Training Loss: 0.04436758905649185\n",
      "Epoch 22702/30000 Training Loss: 0.042191535234451294\n",
      "Epoch 22703/30000 Training Loss: 0.058539360761642456\n",
      "Epoch 22704/30000 Training Loss: 0.0326877124607563\n",
      "Epoch 22705/30000 Training Loss: 0.042998041957616806\n",
      "Epoch 22706/30000 Training Loss: 0.035570625215768814\n",
      "Epoch 22707/30000 Training Loss: 0.039449770003557205\n",
      "Epoch 22708/30000 Training Loss: 0.029750138521194458\n",
      "Epoch 22709/30000 Training Loss: 0.059649690985679626\n",
      "Epoch 22710/30000 Training Loss: 0.05050332844257355\n",
      "Epoch 22711/30000 Training Loss: 0.05269763246178627\n",
      "Epoch 22712/30000 Training Loss: 0.03698647767305374\n",
      "Epoch 22713/30000 Training Loss: 0.03769781067967415\n",
      "Epoch 22714/30000 Training Loss: 0.039338767528533936\n",
      "Epoch 22715/30000 Training Loss: 0.045595090836286545\n",
      "Epoch 22716/30000 Training Loss: 0.04546986520290375\n",
      "Epoch 22717/30000 Training Loss: 0.03593595698475838\n",
      "Epoch 22718/30000 Training Loss: 0.048272378742694855\n",
      "Epoch 22719/30000 Training Loss: 0.0430462509393692\n",
      "Epoch 22720/30000 Training Loss: 0.04277850687503815\n",
      "Epoch 22721/30000 Training Loss: 0.04211033880710602\n",
      "Epoch 22722/30000 Training Loss: 0.04770611971616745\n",
      "Epoch 22723/30000 Training Loss: 0.04021165519952774\n",
      "Epoch 22724/30000 Training Loss: 0.03535968065261841\n",
      "Epoch 22725/30000 Training Loss: 0.03367290273308754\n",
      "Epoch 22726/30000 Training Loss: 0.02873481810092926\n",
      "Epoch 22727/30000 Training Loss: 0.041708994656801224\n",
      "Epoch 22728/30000 Training Loss: 0.036445558071136475\n",
      "Epoch 22729/30000 Training Loss: 0.03257749229669571\n",
      "Epoch 22730/30000 Training Loss: 0.051356181502342224\n",
      "Epoch 22731/30000 Training Loss: 0.07908114790916443\n",
      "Epoch 22732/30000 Training Loss: 0.04662775248289108\n",
      "Epoch 22733/30000 Training Loss: 0.045495983213186264\n",
      "Epoch 22734/30000 Training Loss: 0.03207576647400856\n",
      "Epoch 22735/30000 Training Loss: 0.04012136906385422\n",
      "Epoch 22736/30000 Training Loss: 0.040280070155858994\n",
      "Epoch 22737/30000 Training Loss: 0.041769713163375854\n",
      "Epoch 22738/30000 Training Loss: 0.05920968949794769\n",
      "Epoch 22739/30000 Training Loss: 0.04307413473725319\n",
      "Epoch 22740/30000 Training Loss: 0.039124295115470886\n",
      "Epoch 22741/30000 Training Loss: 0.04056725278496742\n",
      "Epoch 22742/30000 Training Loss: 0.04928252846002579\n",
      "Epoch 22743/30000 Training Loss: 0.03515440598130226\n",
      "Epoch 22744/30000 Training Loss: 0.039328433573246\n",
      "Epoch 22745/30000 Training Loss: 0.04998764395713806\n",
      "Epoch 22746/30000 Training Loss: 0.04256414994597435\n",
      "Epoch 22747/30000 Training Loss: 0.0370284728705883\n",
      "Epoch 22748/30000 Training Loss: 0.050320662558078766\n",
      "Epoch 22749/30000 Training Loss: 0.037650059908628464\n",
      "Epoch 22750/30000 Training Loss: 0.046625521034002304\n",
      "Epoch 22751/30000 Training Loss: 0.04311973601579666\n",
      "Epoch 22752/30000 Training Loss: 0.03854123130440712\n",
      "Epoch 22753/30000 Training Loss: 0.044278569519519806\n",
      "Epoch 22754/30000 Training Loss: 0.0566641129553318\n",
      "Epoch 22755/30000 Training Loss: 0.04574773460626602\n",
      "Epoch 22756/30000 Training Loss: 0.04515323042869568\n",
      "Epoch 22757/30000 Training Loss: 0.034618012607097626\n",
      "Epoch 22758/30000 Training Loss: 0.04931918531656265\n",
      "Epoch 22759/30000 Training Loss: 0.054562002420425415\n",
      "Epoch 22760/30000 Training Loss: 0.03197447955608368\n",
      "Epoch 22761/30000 Training Loss: 0.044210486114025116\n",
      "Epoch 22762/30000 Training Loss: 0.04499007761478424\n",
      "Epoch 22763/30000 Training Loss: 0.03608392924070358\n",
      "Epoch 22764/30000 Training Loss: 0.04309796914458275\n",
      "Epoch 22765/30000 Training Loss: 0.04825035482645035\n",
      "Epoch 22766/30000 Training Loss: 0.057583875954151154\n",
      "Epoch 22767/30000 Training Loss: 0.044129304587841034\n",
      "Epoch 22768/30000 Training Loss: 0.05374496802687645\n",
      "Epoch 22769/30000 Training Loss: 0.03976317495107651\n",
      "Epoch 22770/30000 Training Loss: 0.04306846112012863\n",
      "Epoch 22771/30000 Training Loss: 0.046192705631256104\n",
      "Epoch 22772/30000 Training Loss: 0.04216373339295387\n",
      "Epoch 22773/30000 Training Loss: 0.04020969197154045\n",
      "Epoch 22774/30000 Training Loss: 0.03587707132101059\n",
      "Epoch 22775/30000 Training Loss: 0.05607444792985916\n",
      "Epoch 22776/30000 Training Loss: 0.05931717902421951\n",
      "Epoch 22777/30000 Training Loss: 0.039089031517505646\n",
      "Epoch 22778/30000 Training Loss: 0.03637498617172241\n",
      "Epoch 22779/30000 Training Loss: 0.03759654983878136\n",
      "Epoch 22780/30000 Training Loss: 0.06846638023853302\n",
      "Epoch 22781/30000 Training Loss: 0.03931640461087227\n",
      "Epoch 22782/30000 Training Loss: 0.044454753398895264\n",
      "Epoch 22783/30000 Training Loss: 0.042745720595121384\n",
      "Epoch 22784/30000 Training Loss: 0.054797716438770294\n",
      "Epoch 22785/30000 Training Loss: 0.04062343388795853\n",
      "Epoch 22786/30000 Training Loss: 0.04460699111223221\n",
      "Epoch 22787/30000 Training Loss: 0.04943278431892395\n",
      "Epoch 22788/30000 Training Loss: 0.03653491288423538\n",
      "Epoch 22789/30000 Training Loss: 0.04344833642244339\n",
      "Epoch 22790/30000 Training Loss: 0.028995376080274582\n",
      "Epoch 22791/30000 Training Loss: 0.0454564169049263\n",
      "Epoch 22792/30000 Training Loss: 0.03687788546085358\n",
      "Epoch 22793/30000 Training Loss: 0.05177264288067818\n",
      "Epoch 22794/30000 Training Loss: 0.04428890347480774\n",
      "Epoch 22795/30000 Training Loss: 0.054279252886772156\n",
      "Epoch 22796/30000 Training Loss: 0.037855684757232666\n",
      "Epoch 22797/30000 Training Loss: 0.04221949726343155\n",
      "Epoch 22798/30000 Training Loss: 0.03980407118797302\n",
      "Epoch 22799/30000 Training Loss: 0.05441885441541672\n",
      "Epoch 22800/30000 Training Loss: 0.05823535472154617\n",
      "Epoch 22800/30000 Validation Loss: 0.03578659147024155\n",
      "Epoch 22801/30000 Training Loss: 0.05208420753479004\n",
      "Epoch 22802/30000 Training Loss: 0.04485468193888664\n",
      "Epoch 22803/30000 Training Loss: 0.05206963047385216\n",
      "Epoch 22804/30000 Training Loss: 0.03805311769247055\n",
      "Epoch 22805/30000 Training Loss: 0.04350128024816513\n",
      "Epoch 22806/30000 Training Loss: 0.05594572424888611\n",
      "Epoch 22807/30000 Training Loss: 0.043858569115400314\n",
      "Epoch 22808/30000 Training Loss: 0.0537625290453434\n",
      "Epoch 22809/30000 Training Loss: 0.04172900319099426\n",
      "Epoch 22810/30000 Training Loss: 0.045466531068086624\n",
      "Epoch 22811/30000 Training Loss: 0.03055329993367195\n",
      "Epoch 22812/30000 Training Loss: 0.03180866315960884\n",
      "Epoch 22813/30000 Training Loss: 0.03758746758103371\n",
      "Epoch 22814/30000 Training Loss: 0.043643832206726074\n",
      "Epoch 22815/30000 Training Loss: 0.053795844316482544\n",
      "Epoch 22816/30000 Training Loss: 0.04257972538471222\n",
      "Epoch 22817/30000 Training Loss: 0.06383762508630753\n",
      "Epoch 22818/30000 Training Loss: 0.043350085616111755\n",
      "Epoch 22819/30000 Training Loss: 0.04568343609571457\n",
      "Epoch 22820/30000 Training Loss: 0.034355126321315765\n",
      "Epoch 22821/30000 Training Loss: 0.04558730497956276\n",
      "Epoch 22822/30000 Training Loss: 0.03302403539419174\n",
      "Epoch 22823/30000 Training Loss: 0.05826783925294876\n",
      "Epoch 22824/30000 Training Loss: 0.032371025532484055\n",
      "Epoch 22825/30000 Training Loss: 0.04443120211362839\n",
      "Epoch 22826/30000 Training Loss: 0.04149084538221359\n",
      "Epoch 22827/30000 Training Loss: 0.030716102570295334\n",
      "Epoch 22828/30000 Training Loss: 0.034760795533657074\n",
      "Epoch 22829/30000 Training Loss: 0.04110847786068916\n",
      "Epoch 22830/30000 Training Loss: 0.058584656566381454\n",
      "Epoch 22831/30000 Training Loss: 0.04076450318098068\n",
      "Epoch 22832/30000 Training Loss: 0.029257267713546753\n",
      "Epoch 22833/30000 Training Loss: 0.04539428651332855\n",
      "Epoch 22834/30000 Training Loss: 0.03938678652048111\n",
      "Epoch 22835/30000 Training Loss: 0.0507538765668869\n",
      "Epoch 22836/30000 Training Loss: 0.030364569276571274\n",
      "Epoch 22837/30000 Training Loss: 0.03360143303871155\n",
      "Epoch 22838/30000 Training Loss: 0.039611443877220154\n",
      "Epoch 22839/30000 Training Loss: 0.050568126142024994\n",
      "Epoch 22840/30000 Training Loss: 0.045257702469825745\n",
      "Epoch 22841/30000 Training Loss: 0.05051880329847336\n",
      "Epoch 22842/30000 Training Loss: 0.047438301146030426\n",
      "Epoch 22843/30000 Training Loss: 0.0477069653570652\n",
      "Epoch 22844/30000 Training Loss: 0.033415984362363815\n",
      "Epoch 22845/30000 Training Loss: 0.032200973480939865\n",
      "Epoch 22846/30000 Training Loss: 0.03083290159702301\n",
      "Epoch 22847/30000 Training Loss: 0.050747115164995193\n",
      "Epoch 22848/30000 Training Loss: 0.04362647607922554\n",
      "Epoch 22849/30000 Training Loss: 0.04604218900203705\n",
      "Epoch 22850/30000 Training Loss: 0.03501655161380768\n",
      "Epoch 22851/30000 Training Loss: 0.025821346789598465\n",
      "Epoch 22852/30000 Training Loss: 0.04397314786911011\n",
      "Epoch 22853/30000 Training Loss: 0.03630220144987106\n",
      "Epoch 22854/30000 Training Loss: 0.035211920738220215\n",
      "Epoch 22855/30000 Training Loss: 0.03885224834084511\n",
      "Epoch 22856/30000 Training Loss: 0.04136976599693298\n",
      "Epoch 22857/30000 Training Loss: 0.03879556059837341\n",
      "Epoch 22858/30000 Training Loss: 0.03611093759536743\n",
      "Epoch 22859/30000 Training Loss: 0.046885766088962555\n",
      "Epoch 22860/30000 Training Loss: 0.033430736511945724\n",
      "Epoch 22861/30000 Training Loss: 0.033529605716466904\n",
      "Epoch 22862/30000 Training Loss: 0.03810866177082062\n",
      "Epoch 22863/30000 Training Loss: 0.039024803787469864\n",
      "Epoch 22864/30000 Training Loss: 0.034779708832502365\n",
      "Epoch 22865/30000 Training Loss: 0.04331862926483154\n",
      "Epoch 22866/30000 Training Loss: 0.04804130643606186\n",
      "Epoch 22867/30000 Training Loss: 0.03773203492164612\n",
      "Epoch 22868/30000 Training Loss: 0.04526481404900551\n",
      "Epoch 22869/30000 Training Loss: 0.03933224081993103\n",
      "Epoch 22870/30000 Training Loss: 0.030561154708266258\n",
      "Epoch 22871/30000 Training Loss: 0.04107978194952011\n",
      "Epoch 22872/30000 Training Loss: 0.03130316734313965\n",
      "Epoch 22873/30000 Training Loss: 0.04095340520143509\n",
      "Epoch 22874/30000 Training Loss: 0.028412599116563797\n",
      "Epoch 22875/30000 Training Loss: 0.027750153094530106\n",
      "Epoch 22876/30000 Training Loss: 0.03747944533824921\n",
      "Epoch 22877/30000 Training Loss: 0.03167283162474632\n",
      "Epoch 22878/30000 Training Loss: 0.04911475628614426\n",
      "Epoch 22879/30000 Training Loss: 0.050936199724674225\n",
      "Epoch 22880/30000 Training Loss: 0.04583673179149628\n",
      "Epoch 22881/30000 Training Loss: 0.04611089080572128\n",
      "Epoch 22882/30000 Training Loss: 0.04699825495481491\n",
      "Epoch 22883/30000 Training Loss: 0.057897672057151794\n",
      "Epoch 22884/30000 Training Loss: 0.0420442670583725\n",
      "Epoch 22885/30000 Training Loss: 0.04766540229320526\n",
      "Epoch 22886/30000 Training Loss: 0.04469572380185127\n",
      "Epoch 22887/30000 Training Loss: 0.043427467346191406\n",
      "Epoch 22888/30000 Training Loss: 0.033387087285518646\n",
      "Epoch 22889/30000 Training Loss: 0.035559095442295074\n",
      "Epoch 22890/30000 Training Loss: 0.03940948098897934\n",
      "Epoch 22891/30000 Training Loss: 0.04031465947628021\n",
      "Epoch 22892/30000 Training Loss: 0.04868583753705025\n",
      "Epoch 22893/30000 Training Loss: 0.03915959224104881\n",
      "Epoch 22894/30000 Training Loss: 0.04697052761912346\n",
      "Epoch 22895/30000 Training Loss: 0.04325747489929199\n",
      "Epoch 22896/30000 Training Loss: 0.06508186459541321\n",
      "Epoch 22897/30000 Training Loss: 0.030326923355460167\n",
      "Epoch 22898/30000 Training Loss: 0.04654814675450325\n",
      "Epoch 22899/30000 Training Loss: 0.04162945970892906\n",
      "Epoch 22900/30000 Training Loss: 0.0458911694586277\n",
      "Epoch 22900/30000 Validation Loss: 0.03074323944747448\n",
      "Epoch 22901/30000 Training Loss: 0.04958491772413254\n",
      "Epoch 22902/30000 Training Loss: 0.04235323518514633\n",
      "Epoch 22903/30000 Training Loss: 0.038912735879421234\n",
      "Epoch 22904/30000 Training Loss: 0.03919757157564163\n",
      "Epoch 22905/30000 Training Loss: 0.04287613183259964\n",
      "Epoch 22906/30000 Training Loss: 0.0429503433406353\n",
      "Epoch 22907/30000 Training Loss: 0.045660704374313354\n",
      "Epoch 22908/30000 Training Loss: 0.035888224840164185\n",
      "Epoch 22909/30000 Training Loss: 0.03991629183292389\n",
      "Epoch 22910/30000 Training Loss: 0.0373595729470253\n",
      "Epoch 22911/30000 Training Loss: 0.042144451290369034\n",
      "Epoch 22912/30000 Training Loss: 0.04529273882508278\n",
      "Epoch 22913/30000 Training Loss: 0.034433916211128235\n",
      "Epoch 22914/30000 Training Loss: 0.05254694074392319\n",
      "Epoch 22915/30000 Training Loss: 0.04744570702314377\n",
      "Epoch 22916/30000 Training Loss: 0.04034487158060074\n",
      "Epoch 22917/30000 Training Loss: 0.07014946639537811\n",
      "Epoch 22918/30000 Training Loss: 0.032763633877038956\n",
      "Epoch 22919/30000 Training Loss: 0.046132467687129974\n",
      "Epoch 22920/30000 Training Loss: 0.03828806430101395\n",
      "Epoch 22921/30000 Training Loss: 0.04357210174202919\n",
      "Epoch 22922/30000 Training Loss: 0.056281447410583496\n",
      "Epoch 22923/30000 Training Loss: 0.04065690189599991\n",
      "Epoch 22924/30000 Training Loss: 0.03763841837644577\n",
      "Epoch 22925/30000 Training Loss: 0.041302166879177094\n",
      "Epoch 22926/30000 Training Loss: 0.04377128928899765\n",
      "Epoch 22927/30000 Training Loss: 0.03533817082643509\n",
      "Epoch 22928/30000 Training Loss: 0.05154822766780853\n",
      "Epoch 22929/30000 Training Loss: 0.05013957619667053\n",
      "Epoch 22930/30000 Training Loss: 0.04787618666887283\n",
      "Epoch 22931/30000 Training Loss: 0.049940384924411774\n",
      "Epoch 22932/30000 Training Loss: 0.05072839930653572\n",
      "Epoch 22933/30000 Training Loss: 0.04262998327612877\n",
      "Epoch 22934/30000 Training Loss: 0.034690238535404205\n",
      "Epoch 22935/30000 Training Loss: 0.03711221367120743\n",
      "Epoch 22936/30000 Training Loss: 0.05710995942354202\n",
      "Epoch 22937/30000 Training Loss: 0.04277297481894493\n",
      "Epoch 22938/30000 Training Loss: 0.04953774809837341\n",
      "Epoch 22939/30000 Training Loss: 0.039541780948638916\n",
      "Epoch 22940/30000 Training Loss: 0.051228687167167664\n",
      "Epoch 22941/30000 Training Loss: 0.054839279502630234\n",
      "Epoch 22942/30000 Training Loss: 0.03583795949816704\n",
      "Epoch 22943/30000 Training Loss: 0.0363045409321785\n",
      "Epoch 22944/30000 Training Loss: 0.036451999098062515\n",
      "Epoch 22945/30000 Training Loss: 0.04249108210206032\n",
      "Epoch 22946/30000 Training Loss: 0.057244107127189636\n",
      "Epoch 22947/30000 Training Loss: 0.040392905473709106\n",
      "Epoch 22948/30000 Training Loss: 0.03923531994223595\n",
      "Epoch 22949/30000 Training Loss: 0.03614100068807602\n",
      "Epoch 22950/30000 Training Loss: 0.04621589928865433\n",
      "Epoch 22951/30000 Training Loss: 0.0435422919690609\n",
      "Epoch 22952/30000 Training Loss: 0.037066344171762466\n",
      "Epoch 22953/30000 Training Loss: 0.0492265522480011\n",
      "Epoch 22954/30000 Training Loss: 0.06323857605457306\n",
      "Epoch 22955/30000 Training Loss: 0.04246297478675842\n",
      "Epoch 22956/30000 Training Loss: 0.04156717285513878\n",
      "Epoch 22957/30000 Training Loss: 0.036210983991622925\n",
      "Epoch 22958/30000 Training Loss: 0.054855767637491226\n",
      "Epoch 22959/30000 Training Loss: 0.04631396383047104\n",
      "Epoch 22960/30000 Training Loss: 0.05324658751487732\n",
      "Epoch 22961/30000 Training Loss: 0.03062485158443451\n",
      "Epoch 22962/30000 Training Loss: 0.051632728427648544\n",
      "Epoch 22963/30000 Training Loss: 0.041221439838409424\n",
      "Epoch 22964/30000 Training Loss: 0.05350884050130844\n",
      "Epoch 22965/30000 Training Loss: 0.035699263215065\n",
      "Epoch 22966/30000 Training Loss: 0.044832997024059296\n",
      "Epoch 22967/30000 Training Loss: 0.03943625092506409\n",
      "Epoch 22968/30000 Training Loss: 0.062438733875751495\n",
      "Epoch 22969/30000 Training Loss: 0.04588184133172035\n",
      "Epoch 22970/30000 Training Loss: 0.044841378927230835\n",
      "Epoch 22971/30000 Training Loss: 0.04002542048692703\n",
      "Epoch 22972/30000 Training Loss: 0.042749740183353424\n",
      "Epoch 22973/30000 Training Loss: 0.04518629610538483\n",
      "Epoch 22974/30000 Training Loss: 0.05794273316860199\n",
      "Epoch 22975/30000 Training Loss: 0.053509101271629333\n",
      "Epoch 22976/30000 Training Loss: 0.06737936288118362\n",
      "Epoch 22977/30000 Training Loss: 0.04921237751841545\n",
      "Epoch 22978/30000 Training Loss: 0.039750926196575165\n",
      "Epoch 22979/30000 Training Loss: 0.04509585350751877\n",
      "Epoch 22980/30000 Training Loss: 0.04142983257770538\n",
      "Epoch 22981/30000 Training Loss: 0.04030749201774597\n",
      "Epoch 22982/30000 Training Loss: 0.05072346329689026\n",
      "Epoch 22983/30000 Training Loss: 0.05669044703245163\n",
      "Epoch 22984/30000 Training Loss: 0.0416366383433342\n",
      "Epoch 22985/30000 Training Loss: 0.02903931960463524\n",
      "Epoch 22986/30000 Training Loss: 0.04185042530298233\n",
      "Epoch 22987/30000 Training Loss: 0.0554414838552475\n",
      "Epoch 22988/30000 Training Loss: 0.05552152916789055\n",
      "Epoch 22989/30000 Training Loss: 0.04555286839604378\n",
      "Epoch 22990/30000 Training Loss: 0.03506900742650032\n",
      "Epoch 22991/30000 Training Loss: 0.030473757535219193\n",
      "Epoch 22992/30000 Training Loss: 0.04734182730317116\n",
      "Epoch 22993/30000 Training Loss: 0.05633363127708435\n",
      "Epoch 22994/30000 Training Loss: 0.050087910145521164\n",
      "Epoch 22995/30000 Training Loss: 0.049454737454652786\n",
      "Epoch 22996/30000 Training Loss: 0.03605417162179947\n",
      "Epoch 22997/30000 Training Loss: 0.051481038331985474\n",
      "Epoch 22998/30000 Training Loss: 0.046446941792964935\n",
      "Epoch 22999/30000 Training Loss: 0.03498289734125137\n",
      "Epoch 23000/30000 Training Loss: 0.04922037571668625\n",
      "Epoch 23000/30000 Validation Loss: 0.036739274859428406\n",
      "Epoch 23001/30000 Training Loss: 0.042869336903095245\n",
      "Epoch 23002/30000 Training Loss: 0.05280647054314613\n",
      "Epoch 23003/30000 Training Loss: 0.04550076648592949\n",
      "Epoch 23004/30000 Training Loss: 0.04572355002164841\n",
      "Epoch 23005/30000 Training Loss: 0.03723099082708359\n",
      "Epoch 23006/30000 Training Loss: 0.05043821036815643\n",
      "Epoch 23007/30000 Training Loss: 0.047783076763153076\n",
      "Epoch 23008/30000 Training Loss: 0.04057805612683296\n",
      "Epoch 23009/30000 Training Loss: 0.05092357471585274\n",
      "Epoch 23010/30000 Training Loss: 0.042327795177698135\n",
      "Epoch 23011/30000 Training Loss: 0.04720167815685272\n",
      "Epoch 23012/30000 Training Loss: 0.033003777265548706\n",
      "Epoch 23013/30000 Training Loss: 0.04569673538208008\n",
      "Epoch 23014/30000 Training Loss: 0.038273103535175323\n",
      "Epoch 23015/30000 Training Loss: 0.04299429804086685\n",
      "Epoch 23016/30000 Training Loss: 0.052214495837688446\n",
      "Epoch 23017/30000 Training Loss: 0.03473008796572685\n",
      "Epoch 23018/30000 Training Loss: 0.035834766924381256\n",
      "Epoch 23019/30000 Training Loss: 0.04154502600431442\n",
      "Epoch 23020/30000 Training Loss: 0.05083060264587402\n",
      "Epoch 23021/30000 Training Loss: 0.039590056985616684\n",
      "Epoch 23022/30000 Training Loss: 0.05203511565923691\n",
      "Epoch 23023/30000 Training Loss: 0.03516005724668503\n",
      "Epoch 23024/30000 Training Loss: 0.04476628825068474\n",
      "Epoch 23025/30000 Training Loss: 0.03297331929206848\n",
      "Epoch 23026/30000 Training Loss: 0.056729622185230255\n",
      "Epoch 23027/30000 Training Loss: 0.0446293018758297\n",
      "Epoch 23028/30000 Training Loss: 0.045174457132816315\n",
      "Epoch 23029/30000 Training Loss: 0.03554624319076538\n",
      "Epoch 23030/30000 Training Loss: 0.034958917647600174\n",
      "Epoch 23031/30000 Training Loss: 0.04665730148553848\n",
      "Epoch 23032/30000 Training Loss: 0.034226976335048676\n",
      "Epoch 23033/30000 Training Loss: 0.03751152753829956\n",
      "Epoch 23034/30000 Training Loss: 0.04703344404697418\n",
      "Epoch 23035/30000 Training Loss: 0.038993071764707565\n",
      "Epoch 23036/30000 Training Loss: 0.04510550573468208\n",
      "Epoch 23037/30000 Training Loss: 0.027094900608062744\n",
      "Epoch 23038/30000 Training Loss: 0.04055873677134514\n",
      "Epoch 23039/30000 Training Loss: 0.03696741908788681\n",
      "Epoch 23040/30000 Training Loss: 0.04956552013754845\n",
      "Epoch 23041/30000 Training Loss: 0.04077291861176491\n",
      "Epoch 23042/30000 Training Loss: 0.04066610336303711\n",
      "Epoch 23043/30000 Training Loss: 0.05523756146430969\n",
      "Epoch 23044/30000 Training Loss: 0.043025173246860504\n",
      "Epoch 23045/30000 Training Loss: 0.04232312738895416\n",
      "Epoch 23046/30000 Training Loss: 0.04269244521856308\n",
      "Epoch 23047/30000 Training Loss: 0.04627018794417381\n",
      "Epoch 23048/30000 Training Loss: 0.042141493409872055\n",
      "Epoch 23049/30000 Training Loss: 0.04053700342774391\n",
      "Epoch 23050/30000 Training Loss: 0.03634662181138992\n",
      "Epoch 23051/30000 Training Loss: 0.042028844356536865\n",
      "Epoch 23052/30000 Training Loss: 0.04388760030269623\n",
      "Epoch 23053/30000 Training Loss: 0.05075891315937042\n",
      "Epoch 23054/30000 Training Loss: 0.045544181019067764\n",
      "Epoch 23055/30000 Training Loss: 0.050077635794878006\n",
      "Epoch 23056/30000 Training Loss: 0.056726738810539246\n",
      "Epoch 23057/30000 Training Loss: 0.046316806226968765\n",
      "Epoch 23058/30000 Training Loss: 0.05586298555135727\n",
      "Epoch 23059/30000 Training Loss: 0.03912242501974106\n",
      "Epoch 23060/30000 Training Loss: 0.04310363531112671\n",
      "Epoch 23061/30000 Training Loss: 0.04504862427711487\n",
      "Epoch 23062/30000 Training Loss: 0.05229378864169121\n",
      "Epoch 23063/30000 Training Loss: 0.038769520819187164\n",
      "Epoch 23064/30000 Training Loss: 0.04589133709669113\n",
      "Epoch 23065/30000 Training Loss: 0.03649136424064636\n",
      "Epoch 23066/30000 Training Loss: 0.043473318219184875\n",
      "Epoch 23067/30000 Training Loss: 0.046784937381744385\n",
      "Epoch 23068/30000 Training Loss: 0.037428487092256546\n",
      "Epoch 23069/30000 Training Loss: 0.04044511169195175\n",
      "Epoch 23070/30000 Training Loss: 0.04853985086083412\n",
      "Epoch 23071/30000 Training Loss: 0.029822852462530136\n",
      "Epoch 23072/30000 Training Loss: 0.05977029353380203\n",
      "Epoch 23073/30000 Training Loss: 0.03438524156808853\n",
      "Epoch 23074/30000 Training Loss: 0.052539609372615814\n",
      "Epoch 23075/30000 Training Loss: 0.04159069061279297\n",
      "Epoch 23076/30000 Training Loss: 0.05800130218267441\n",
      "Epoch 23077/30000 Training Loss: 0.037270911037921906\n",
      "Epoch 23078/30000 Training Loss: 0.029400702565908432\n",
      "Epoch 23079/30000 Training Loss: 0.05178061127662659\n",
      "Epoch 23080/30000 Training Loss: 0.032300423830747604\n",
      "Epoch 23081/30000 Training Loss: 0.03946749493479729\n",
      "Epoch 23082/30000 Training Loss: 0.04217264801263809\n",
      "Epoch 23083/30000 Training Loss: 0.03117123246192932\n",
      "Epoch 23084/30000 Training Loss: 0.03924809396266937\n",
      "Epoch 23085/30000 Training Loss: 0.034100260585546494\n",
      "Epoch 23086/30000 Training Loss: 0.05489058047533035\n",
      "Epoch 23087/30000 Training Loss: 0.04050251841545105\n",
      "Epoch 23088/30000 Training Loss: 0.03316826373338699\n",
      "Epoch 23089/30000 Training Loss: 0.03729739040136337\n",
      "Epoch 23090/30000 Training Loss: 0.04588380828499794\n",
      "Epoch 23091/30000 Training Loss: 0.045291073620319366\n",
      "Epoch 23092/30000 Training Loss: 0.053190678358078\n",
      "Epoch 23093/30000 Training Loss: 0.04522399604320526\n",
      "Epoch 23094/30000 Training Loss: 0.038233429193496704\n",
      "Epoch 23095/30000 Training Loss: 0.05136067420244217\n",
      "Epoch 23096/30000 Training Loss: 0.056822117418050766\n",
      "Epoch 23097/30000 Training Loss: 0.03949974849820137\n",
      "Epoch 23098/30000 Training Loss: 0.05107676982879639\n",
      "Epoch 23099/30000 Training Loss: 0.0583358034491539\n",
      "Epoch 23100/30000 Training Loss: 0.04033609479665756\n",
      "Epoch 23100/30000 Validation Loss: 0.04963046312332153\n",
      "Epoch 23101/30000 Training Loss: 0.03343033045530319\n",
      "Epoch 23102/30000 Training Loss: 0.043634794652462006\n",
      "Epoch 23103/30000 Training Loss: 0.04202240705490112\n",
      "Epoch 23104/30000 Training Loss: 0.04882313683629036\n",
      "Epoch 23105/30000 Training Loss: 0.053835008293390274\n",
      "Epoch 23106/30000 Training Loss: 0.02983800135552883\n",
      "Epoch 23107/30000 Training Loss: 0.041112832725048065\n",
      "Epoch 23108/30000 Training Loss: 0.04078395664691925\n",
      "Epoch 23109/30000 Training Loss: 0.03447052463889122\n",
      "Epoch 23110/30000 Training Loss: 0.041358403861522675\n",
      "Epoch 23111/30000 Training Loss: 0.03880807012319565\n",
      "Epoch 23112/30000 Training Loss: 0.03789074718952179\n",
      "Epoch 23113/30000 Training Loss: 0.0550108477473259\n",
      "Epoch 23114/30000 Training Loss: 0.040978096425533295\n",
      "Epoch 23115/30000 Training Loss: 0.040463633835315704\n",
      "Epoch 23116/30000 Training Loss: 0.052870750427246094\n",
      "Epoch 23117/30000 Training Loss: 0.0466250516474247\n",
      "Epoch 23118/30000 Training Loss: 0.04121816158294678\n",
      "Epoch 23119/30000 Training Loss: 0.055190540850162506\n",
      "Epoch 23120/30000 Training Loss: 0.05389668792486191\n",
      "Epoch 23121/30000 Training Loss: 0.03162756562232971\n",
      "Epoch 23122/30000 Training Loss: 0.04709555581212044\n",
      "Epoch 23123/30000 Training Loss: 0.03210282698273659\n",
      "Epoch 23124/30000 Training Loss: 0.04385746642947197\n",
      "Epoch 23125/30000 Training Loss: 0.0348605178296566\n",
      "Epoch 23126/30000 Training Loss: 0.03601086139678955\n",
      "Epoch 23127/30000 Training Loss: 0.0413895808160305\n",
      "Epoch 23128/30000 Training Loss: 0.05432141572237015\n",
      "Epoch 23129/30000 Training Loss: 0.03401461988687515\n",
      "Epoch 23130/30000 Training Loss: 0.0445421002805233\n",
      "Epoch 23131/30000 Training Loss: 0.05770880728960037\n",
      "Epoch 23132/30000 Training Loss: 0.048642996698617935\n",
      "Epoch 23133/30000 Training Loss: 0.03808365389704704\n",
      "Epoch 23134/30000 Training Loss: 0.044375475496053696\n",
      "Epoch 23135/30000 Training Loss: 0.04516255855560303\n",
      "Epoch 23136/30000 Training Loss: 0.029762979596853256\n",
      "Epoch 23137/30000 Training Loss: 0.04179569333791733\n",
      "Epoch 23138/30000 Training Loss: 0.041528910398483276\n",
      "Epoch 23139/30000 Training Loss: 0.03998591750860214\n",
      "Epoch 23140/30000 Training Loss: 0.04077499732375145\n",
      "Epoch 23141/30000 Training Loss: 0.045742280781269073\n",
      "Epoch 23142/30000 Training Loss: 0.04499271512031555\n",
      "Epoch 23143/30000 Training Loss: 0.049418289214372635\n",
      "Epoch 23144/30000 Training Loss: 0.04479813203215599\n",
      "Epoch 23145/30000 Training Loss: 0.0425436906516552\n",
      "Epoch 23146/30000 Training Loss: 0.03567894548177719\n",
      "Epoch 23147/30000 Training Loss: 0.04488823190331459\n",
      "Epoch 23148/30000 Training Loss: 0.04143809527158737\n",
      "Epoch 23149/30000 Training Loss: 0.041902344673871994\n",
      "Epoch 23150/30000 Training Loss: 0.038686953485012054\n",
      "Epoch 23151/30000 Training Loss: 0.03328842297196388\n",
      "Epoch 23152/30000 Training Loss: 0.043989937752485275\n",
      "Epoch 23153/30000 Training Loss: 0.0361323319375515\n",
      "Epoch 23154/30000 Training Loss: 0.055389080196619034\n",
      "Epoch 23155/30000 Training Loss: 0.03975910693407059\n",
      "Epoch 23156/30000 Training Loss: 0.045841656625270844\n",
      "Epoch 23157/30000 Training Loss: 0.040608394891023636\n",
      "Epoch 23158/30000 Training Loss: 0.03763595223426819\n",
      "Epoch 23159/30000 Training Loss: 0.04201890528202057\n",
      "Epoch 23160/30000 Training Loss: 0.03801734745502472\n",
      "Epoch 23161/30000 Training Loss: 0.046869002282619476\n",
      "Epoch 23162/30000 Training Loss: 0.03276525437831879\n",
      "Epoch 23163/30000 Training Loss: 0.05500832200050354\n",
      "Epoch 23164/30000 Training Loss: 0.04492393136024475\n",
      "Epoch 23165/30000 Training Loss: 0.048262372612953186\n",
      "Epoch 23166/30000 Training Loss: 0.045233823359012604\n",
      "Epoch 23167/30000 Training Loss: 0.047416724264621735\n",
      "Epoch 23168/30000 Training Loss: 0.04517187923192978\n",
      "Epoch 23169/30000 Training Loss: 0.04202699288725853\n",
      "Epoch 23170/30000 Training Loss: 0.048920031636953354\n",
      "Epoch 23171/30000 Training Loss: 0.03492921590805054\n",
      "Epoch 23172/30000 Training Loss: 0.060907669365406036\n",
      "Epoch 23173/30000 Training Loss: 0.05084022879600525\n",
      "Epoch 23174/30000 Training Loss: 0.04223436862230301\n",
      "Epoch 23175/30000 Training Loss: 0.038545552641153336\n",
      "Epoch 23176/30000 Training Loss: 0.05140617862343788\n",
      "Epoch 23177/30000 Training Loss: 0.055219896137714386\n",
      "Epoch 23178/30000 Training Loss: 0.06086606904864311\n",
      "Epoch 23179/30000 Training Loss: 0.05061160773038864\n",
      "Epoch 23180/30000 Training Loss: 0.03805539757013321\n",
      "Epoch 23181/30000 Training Loss: 0.040579140186309814\n",
      "Epoch 23182/30000 Training Loss: 0.04966916888952255\n",
      "Epoch 23183/30000 Training Loss: 0.05579735338687897\n",
      "Epoch 23184/30000 Training Loss: 0.0435904785990715\n",
      "Epoch 23185/30000 Training Loss: 0.04699864611029625\n",
      "Epoch 23186/30000 Training Loss: 0.047841597348451614\n",
      "Epoch 23187/30000 Training Loss: 0.0366038978099823\n",
      "Epoch 23188/30000 Training Loss: 0.04205382615327835\n",
      "Epoch 23189/30000 Training Loss: 0.03974831849336624\n",
      "Epoch 23190/30000 Training Loss: 0.05296279489994049\n",
      "Epoch 23191/30000 Training Loss: 0.05344739183783531\n",
      "Epoch 23192/30000 Training Loss: 0.0310295969247818\n",
      "Epoch 23193/30000 Training Loss: 0.044863808900117874\n",
      "Epoch 23194/30000 Training Loss: 0.04976584017276764\n",
      "Epoch 23195/30000 Training Loss: 0.044196464121341705\n",
      "Epoch 23196/30000 Training Loss: 0.03911002725362778\n",
      "Epoch 23197/30000 Training Loss: 0.0396774560213089\n",
      "Epoch 23198/30000 Training Loss: 0.04016359895467758\n",
      "Epoch 23199/30000 Training Loss: 0.044560935348272324\n",
      "Epoch 23200/30000 Training Loss: 0.05212859809398651\n",
      "Epoch 23200/30000 Validation Loss: 0.04476415365934372\n",
      "Epoch 23201/30000 Training Loss: 0.036977335810661316\n",
      "Epoch 23202/30000 Training Loss: 0.03765749931335449\n",
      "Epoch 23203/30000 Training Loss: 0.04777361825108528\n",
      "Epoch 23204/30000 Training Loss: 0.032098304480314255\n",
      "Epoch 23205/30000 Training Loss: 0.04296960309147835\n",
      "Epoch 23206/30000 Training Loss: 0.049152299761772156\n",
      "Epoch 23207/30000 Training Loss: 0.04358756169676781\n",
      "Epoch 23208/30000 Training Loss: 0.04667365550994873\n",
      "Epoch 23209/30000 Training Loss: 0.04208806902170181\n",
      "Epoch 23210/30000 Training Loss: 0.03503856807947159\n",
      "Epoch 23211/30000 Training Loss: 0.0465102344751358\n",
      "Epoch 23212/30000 Training Loss: 0.0384088009595871\n",
      "Epoch 23213/30000 Training Loss: 0.043688490986824036\n",
      "Epoch 23214/30000 Training Loss: 0.036547258496284485\n",
      "Epoch 23215/30000 Training Loss: 0.05278365686535835\n",
      "Epoch 23216/30000 Training Loss: 0.03413021191954613\n",
      "Epoch 23217/30000 Training Loss: 0.029088690876960754\n",
      "Epoch 23218/30000 Training Loss: 0.041099242866039276\n",
      "Epoch 23219/30000 Training Loss: 0.04198228567838669\n",
      "Epoch 23220/30000 Training Loss: 0.03234567865729332\n",
      "Epoch 23221/30000 Training Loss: 0.039906542748212814\n",
      "Epoch 23222/30000 Training Loss: 0.034217722713947296\n",
      "Epoch 23223/30000 Training Loss: 0.049502331763505936\n",
      "Epoch 23224/30000 Training Loss: 0.03324398770928383\n",
      "Epoch 23225/30000 Training Loss: 0.045206665992736816\n",
      "Epoch 23226/30000 Training Loss: 0.0342458076775074\n",
      "Epoch 23227/30000 Training Loss: 0.038422271609306335\n",
      "Epoch 23228/30000 Training Loss: 0.04101785272359848\n",
      "Epoch 23229/30000 Training Loss: 0.05083830654621124\n",
      "Epoch 23230/30000 Training Loss: 0.04952554032206535\n",
      "Epoch 23231/30000 Training Loss: 0.04266199469566345\n",
      "Epoch 23232/30000 Training Loss: 0.04339592903852463\n",
      "Epoch 23233/30000 Training Loss: 0.04541310667991638\n",
      "Epoch 23234/30000 Training Loss: 0.04898912459611893\n",
      "Epoch 23235/30000 Training Loss: 0.04063791781663895\n",
      "Epoch 23236/30000 Training Loss: 0.04481274634599686\n",
      "Epoch 23237/30000 Training Loss: 0.05184662342071533\n",
      "Epoch 23238/30000 Training Loss: 0.028817035257816315\n",
      "Epoch 23239/30000 Training Loss: 0.026543641462922096\n",
      "Epoch 23240/30000 Training Loss: 0.03705604374408722\n",
      "Epoch 23241/30000 Training Loss: 0.057886622846126556\n",
      "Epoch 23242/30000 Training Loss: 0.034188512712717056\n",
      "Epoch 23243/30000 Training Loss: 0.04655771702528\n",
      "Epoch 23244/30000 Training Loss: 0.04981774091720581\n",
      "Epoch 23245/30000 Training Loss: 0.04329656437039375\n",
      "Epoch 23246/30000 Training Loss: 0.05662480741739273\n",
      "Epoch 23247/30000 Training Loss: 0.039627060294151306\n",
      "Epoch 23248/30000 Training Loss: 0.05128619074821472\n",
      "Epoch 23249/30000 Training Loss: 0.0472354032099247\n",
      "Epoch 23250/30000 Training Loss: 0.049352534115314484\n",
      "Epoch 23251/30000 Training Loss: 0.05172094330191612\n",
      "Epoch 23252/30000 Training Loss: 0.03871531784534454\n",
      "Epoch 23253/30000 Training Loss: 0.054678551852703094\n",
      "Epoch 23254/30000 Training Loss: 0.03597484529018402\n",
      "Epoch 23255/30000 Training Loss: 0.0408419668674469\n",
      "Epoch 23256/30000 Training Loss: 0.039224281907081604\n",
      "Epoch 23257/30000 Training Loss: 0.05177875608205795\n",
      "Epoch 23258/30000 Training Loss: 0.05711304023861885\n",
      "Epoch 23259/30000 Training Loss: 0.04720425605773926\n",
      "Epoch 23260/30000 Training Loss: 0.048288822174072266\n",
      "Epoch 23261/30000 Training Loss: 0.04902307689189911\n",
      "Epoch 23262/30000 Training Loss: 0.030216101557016373\n",
      "Epoch 23263/30000 Training Loss: 0.02655135653913021\n",
      "Epoch 23264/30000 Training Loss: 0.03476397693157196\n",
      "Epoch 23265/30000 Training Loss: 0.03972938656806946\n",
      "Epoch 23266/30000 Training Loss: 0.03675837814807892\n",
      "Epoch 23267/30000 Training Loss: 0.04115346819162369\n",
      "Epoch 23268/30000 Training Loss: 0.04200785234570503\n",
      "Epoch 23269/30000 Training Loss: 0.039192769676446915\n",
      "Epoch 23270/30000 Training Loss: 0.03443128243088722\n",
      "Epoch 23271/30000 Training Loss: 0.033817313611507416\n",
      "Epoch 23272/30000 Training Loss: 0.0411917008459568\n",
      "Epoch 23273/30000 Training Loss: 0.035563357174396515\n",
      "Epoch 23274/30000 Training Loss: 0.04065563529729843\n",
      "Epoch 23275/30000 Training Loss: 0.0440237894654274\n",
      "Epoch 23276/30000 Training Loss: 0.037819214165210724\n",
      "Epoch 23277/30000 Training Loss: 0.04391920566558838\n",
      "Epoch 23278/30000 Training Loss: 0.03777238354086876\n",
      "Epoch 23279/30000 Training Loss: 0.0424186997115612\n",
      "Epoch 23280/30000 Training Loss: 0.05073929578065872\n",
      "Epoch 23281/30000 Training Loss: 0.03878132253885269\n",
      "Epoch 23282/30000 Training Loss: 0.04273243993520737\n",
      "Epoch 23283/30000 Training Loss: 0.05620565265417099\n",
      "Epoch 23284/30000 Training Loss: 0.032598935067653656\n",
      "Epoch 23285/30000 Training Loss: 0.03832472860813141\n",
      "Epoch 23286/30000 Training Loss: 0.05440158396959305\n",
      "Epoch 23287/30000 Training Loss: 0.03244151547551155\n",
      "Epoch 23288/30000 Training Loss: 0.048094555735588074\n",
      "Epoch 23289/30000 Training Loss: 0.045501645654439926\n",
      "Epoch 23290/30000 Training Loss: 0.03226471692323685\n",
      "Epoch 23291/30000 Training Loss: 0.04476609081029892\n",
      "Epoch 23292/30000 Training Loss: 0.038160212337970734\n",
      "Epoch 23293/30000 Training Loss: 0.04173637181520462\n",
      "Epoch 23294/30000 Training Loss: 0.04060053825378418\n",
      "Epoch 23295/30000 Training Loss: 0.03780944645404816\n",
      "Epoch 23296/30000 Training Loss: 0.04092362895607948\n",
      "Epoch 23297/30000 Training Loss: 0.03608454763889313\n",
      "Epoch 23298/30000 Training Loss: 0.037879906594753265\n",
      "Epoch 23299/30000 Training Loss: 0.03651910647749901\n",
      "Epoch 23300/30000 Training Loss: 0.04926560819149017\n",
      "Epoch 23300/30000 Validation Loss: 0.06440293788909912\n",
      "Epoch 23301/30000 Training Loss: 0.040688298642635345\n",
      "Epoch 23302/30000 Training Loss: 0.04284869134426117\n",
      "Epoch 23303/30000 Training Loss: 0.04130968824028969\n",
      "Epoch 23304/30000 Training Loss: 0.06604029983282089\n",
      "Epoch 23305/30000 Training Loss: 0.03622182831168175\n",
      "Epoch 23306/30000 Training Loss: 0.04795119911432266\n",
      "Epoch 23307/30000 Training Loss: 0.04413788020610809\n",
      "Epoch 23308/30000 Training Loss: 0.047579117119312286\n",
      "Epoch 23309/30000 Training Loss: 0.04490985348820686\n",
      "Epoch 23310/30000 Training Loss: 0.05327649414539337\n",
      "Epoch 23311/30000 Training Loss: 0.042824387550354004\n",
      "Epoch 23312/30000 Training Loss: 0.04207203537225723\n",
      "Epoch 23313/30000 Training Loss: 0.042078569531440735\n",
      "Epoch 23314/30000 Training Loss: 0.03586743026971817\n",
      "Epoch 23315/30000 Training Loss: 0.051390234380960464\n",
      "Epoch 23316/30000 Training Loss: 0.051059313118457794\n",
      "Epoch 23317/30000 Training Loss: 0.03726913779973984\n",
      "Epoch 23318/30000 Training Loss: 0.0445128008723259\n",
      "Epoch 23319/30000 Training Loss: 0.039957404136657715\n",
      "Epoch 23320/30000 Training Loss: 0.05089187249541283\n",
      "Epoch 23321/30000 Training Loss: 0.029038190841674805\n",
      "Epoch 23322/30000 Training Loss: 0.04972149804234505\n",
      "Epoch 23323/30000 Training Loss: 0.04482541233301163\n",
      "Epoch 23324/30000 Training Loss: 0.04396951198577881\n",
      "Epoch 23325/30000 Training Loss: 0.04249749332666397\n",
      "Epoch 23326/30000 Training Loss: 0.03595208749175072\n",
      "Epoch 23327/30000 Training Loss: 0.046858541667461395\n",
      "Epoch 23328/30000 Training Loss: 0.03574639931321144\n",
      "Epoch 23329/30000 Training Loss: 0.06261621415615082\n",
      "Epoch 23330/30000 Training Loss: 0.044833481311798096\n",
      "Epoch 23331/30000 Training Loss: 0.042615167796611786\n",
      "Epoch 23332/30000 Training Loss: 0.03527618199586868\n",
      "Epoch 23333/30000 Training Loss: 0.04528244584798813\n",
      "Epoch 23334/30000 Training Loss: 0.04126126319169998\n",
      "Epoch 23335/30000 Training Loss: 0.0431232750415802\n",
      "Epoch 23336/30000 Training Loss: 0.08128836750984192\n",
      "Epoch 23337/30000 Training Loss: 0.041981957852840424\n",
      "Epoch 23338/30000 Training Loss: 0.036143817007541656\n",
      "Epoch 23339/30000 Training Loss: 0.05020996183156967\n",
      "Epoch 23340/30000 Training Loss: 0.0475706048309803\n",
      "Epoch 23341/30000 Training Loss: 0.06507057696580887\n",
      "Epoch 23342/30000 Training Loss: 0.03767795115709305\n",
      "Epoch 23343/30000 Training Loss: 0.04674502834677696\n",
      "Epoch 23344/30000 Training Loss: 0.03511369600892067\n",
      "Epoch 23345/30000 Training Loss: 0.04699982330203056\n",
      "Epoch 23346/30000 Training Loss: 0.045529790222644806\n",
      "Epoch 23347/30000 Training Loss: 0.052459292113780975\n",
      "Epoch 23348/30000 Training Loss: 0.05069977790117264\n",
      "Epoch 23349/30000 Training Loss: 0.038561079651117325\n",
      "Epoch 23350/30000 Training Loss: 0.04004231467843056\n",
      "Epoch 23351/30000 Training Loss: 0.06234244257211685\n",
      "Epoch 23352/30000 Training Loss: 0.030428791418671608\n",
      "Epoch 23353/30000 Training Loss: 0.04362589120864868\n",
      "Epoch 23354/30000 Training Loss: 0.04486323520541191\n",
      "Epoch 23355/30000 Training Loss: 0.036292996257543564\n",
      "Epoch 23356/30000 Training Loss: 0.053531214594841\n",
      "Epoch 23357/30000 Training Loss: 0.04778193682432175\n",
      "Epoch 23358/30000 Training Loss: 0.05407184362411499\n",
      "Epoch 23359/30000 Training Loss: 0.037075161933898926\n",
      "Epoch 23360/30000 Training Loss: 0.03740786015987396\n",
      "Epoch 23361/30000 Training Loss: 0.033815596252679825\n",
      "Epoch 23362/30000 Training Loss: 0.03332344442605972\n",
      "Epoch 23363/30000 Training Loss: 0.040888555347919464\n",
      "Epoch 23364/30000 Training Loss: 0.05358671396970749\n",
      "Epoch 23365/30000 Training Loss: 0.04131203889846802\n",
      "Epoch 23366/30000 Training Loss: 0.04840242862701416\n",
      "Epoch 23367/30000 Training Loss: 0.04096950218081474\n",
      "Epoch 23368/30000 Training Loss: 0.03752528876066208\n",
      "Epoch 23369/30000 Training Loss: 0.053260158747434616\n",
      "Epoch 23370/30000 Training Loss: 0.040804848074913025\n",
      "Epoch 23371/30000 Training Loss: 0.03511328250169754\n",
      "Epoch 23372/30000 Training Loss: 0.0489315465092659\n",
      "Epoch 23373/30000 Training Loss: 0.03778325766324997\n",
      "Epoch 23374/30000 Training Loss: 0.040533438324928284\n",
      "Epoch 23375/30000 Training Loss: 0.042672015726566315\n",
      "Epoch 23376/30000 Training Loss: 0.04317736253142357\n",
      "Epoch 23377/30000 Training Loss: 0.0400821715593338\n",
      "Epoch 23378/30000 Training Loss: 0.04170193523168564\n",
      "Epoch 23379/30000 Training Loss: 0.04053502529859543\n",
      "Epoch 23380/30000 Training Loss: 0.04247124865651131\n",
      "Epoch 23381/30000 Training Loss: 0.04238595440983772\n",
      "Epoch 23382/30000 Training Loss: 0.04060932993888855\n",
      "Epoch 23383/30000 Training Loss: 0.04489469528198242\n",
      "Epoch 23384/30000 Training Loss: 0.03976604342460632\n",
      "Epoch 23385/30000 Training Loss: 0.03738938271999359\n",
      "Epoch 23386/30000 Training Loss: 0.047985296696424484\n",
      "Epoch 23387/30000 Training Loss: 0.03770846500992775\n",
      "Epoch 23388/30000 Training Loss: 0.034987904131412506\n",
      "Epoch 23389/30000 Training Loss: 0.05359894037246704\n",
      "Epoch 23390/30000 Training Loss: 0.03527814894914627\n",
      "Epoch 23391/30000 Training Loss: 0.05243729054927826\n",
      "Epoch 23392/30000 Training Loss: 0.03767884895205498\n",
      "Epoch 23393/30000 Training Loss: 0.04741378501057625\n",
      "Epoch 23394/30000 Training Loss: 0.04208534583449364\n",
      "Epoch 23395/30000 Training Loss: 0.05040751025080681\n",
      "Epoch 23396/30000 Training Loss: 0.04593706130981445\n",
      "Epoch 23397/30000 Training Loss: 0.042969755828380585\n",
      "Epoch 23398/30000 Training Loss: 0.033544644713401794\n",
      "Epoch 23399/30000 Training Loss: 0.037149202078580856\n",
      "Epoch 23400/30000 Training Loss: 0.034483544528484344\n",
      "Epoch 23400/30000 Validation Loss: 0.04644818976521492\n",
      "Epoch 23401/30000 Training Loss: 0.04243116080760956\n",
      "Epoch 23402/30000 Training Loss: 0.041965551674366\n",
      "Epoch 23403/30000 Training Loss: 0.0514199435710907\n",
      "Epoch 23404/30000 Training Loss: 0.04607951268553734\n",
      "Epoch 23405/30000 Training Loss: 0.04945405572652817\n",
      "Epoch 23406/30000 Training Loss: 0.041882388293743134\n",
      "Epoch 23407/30000 Training Loss: 0.03826335817575455\n",
      "Epoch 23408/30000 Training Loss: 0.037674665451049805\n",
      "Epoch 23409/30000 Training Loss: 0.03824862092733383\n",
      "Epoch 23410/30000 Training Loss: 0.04505549371242523\n",
      "Epoch 23411/30000 Training Loss: 0.05908168852329254\n",
      "Epoch 23412/30000 Training Loss: 0.05447357892990112\n",
      "Epoch 23413/30000 Training Loss: 0.04535912722349167\n",
      "Epoch 23414/30000 Training Loss: 0.04580013453960419\n",
      "Epoch 23415/30000 Training Loss: 0.03939574211835861\n",
      "Epoch 23416/30000 Training Loss: 0.06180404871702194\n",
      "Epoch 23417/30000 Training Loss: 0.04924566671252251\n",
      "Epoch 23418/30000 Training Loss: 0.05183297395706177\n",
      "Epoch 23419/30000 Training Loss: 0.04159826785326004\n",
      "Epoch 23420/30000 Training Loss: 0.04178667068481445\n",
      "Epoch 23421/30000 Training Loss: 0.06660972535610199\n",
      "Epoch 23422/30000 Training Loss: 0.04399898648262024\n",
      "Epoch 23423/30000 Training Loss: 0.04421009123325348\n",
      "Epoch 23424/30000 Training Loss: 0.04130815714597702\n",
      "Epoch 23425/30000 Training Loss: 0.03680294752120972\n",
      "Epoch 23426/30000 Training Loss: 0.03822889178991318\n",
      "Epoch 23427/30000 Training Loss: 0.03362652659416199\n",
      "Epoch 23428/30000 Training Loss: 0.04644797742366791\n",
      "Epoch 23429/30000 Training Loss: 0.04451000690460205\n",
      "Epoch 23430/30000 Training Loss: 0.04449629411101341\n",
      "Epoch 23431/30000 Training Loss: 0.05668232589960098\n",
      "Epoch 23432/30000 Training Loss: 0.0470590740442276\n",
      "Epoch 23433/30000 Training Loss: 0.04084731638431549\n",
      "Epoch 23434/30000 Training Loss: 0.044373564422130585\n",
      "Epoch 23435/30000 Training Loss: 0.05149800330400467\n",
      "Epoch 23436/30000 Training Loss: 0.030680226162075996\n",
      "Epoch 23437/30000 Training Loss: 0.03631618991494179\n",
      "Epoch 23438/30000 Training Loss: 0.03535665571689606\n",
      "Epoch 23439/30000 Training Loss: 0.0468720868229866\n",
      "Epoch 23440/30000 Training Loss: 0.05286016687750816\n",
      "Epoch 23441/30000 Training Loss: 0.04420077055692673\n",
      "Epoch 23442/30000 Training Loss: 0.03456670790910721\n",
      "Epoch 23443/30000 Training Loss: 0.04044988751411438\n",
      "Epoch 23444/30000 Training Loss: 0.04192584007978439\n",
      "Epoch 23445/30000 Training Loss: 0.06402599811553955\n",
      "Epoch 23446/30000 Training Loss: 0.0323125459253788\n",
      "Epoch 23447/30000 Training Loss: 0.04200145974755287\n",
      "Epoch 23448/30000 Training Loss: 0.051428329199552536\n",
      "Epoch 23449/30000 Training Loss: 0.053827203810214996\n",
      "Epoch 23450/30000 Training Loss: 0.02832811512053013\n",
      "Epoch 23451/30000 Training Loss: 0.06944040954113007\n",
      "Epoch 23452/30000 Training Loss: 0.05008279159665108\n",
      "Epoch 23453/30000 Training Loss: 0.045543964952230453\n",
      "Epoch 23454/30000 Training Loss: 0.02951929345726967\n",
      "Epoch 23455/30000 Training Loss: 0.04645821452140808\n",
      "Epoch 23456/30000 Training Loss: 0.05298180133104324\n",
      "Epoch 23457/30000 Training Loss: 0.041658733040094376\n",
      "Epoch 23458/30000 Training Loss: 0.030496466904878616\n",
      "Epoch 23459/30000 Training Loss: 0.04874863475561142\n",
      "Epoch 23460/30000 Training Loss: 0.04304269328713417\n",
      "Epoch 23461/30000 Training Loss: 0.03668006509542465\n",
      "Epoch 23462/30000 Training Loss: 0.04936569184064865\n",
      "Epoch 23463/30000 Training Loss: 0.04742898419499397\n",
      "Epoch 23464/30000 Training Loss: 0.03391595929861069\n",
      "Epoch 23465/30000 Training Loss: 0.052233073860406876\n",
      "Epoch 23466/30000 Training Loss: 0.04596918076276779\n",
      "Epoch 23467/30000 Training Loss: 0.03806450963020325\n",
      "Epoch 23468/30000 Training Loss: 0.04269253835082054\n",
      "Epoch 23469/30000 Training Loss: 0.03984588384628296\n",
      "Epoch 23470/30000 Training Loss: 0.04093483090400696\n",
      "Epoch 23471/30000 Training Loss: 0.05261004716157913\n",
      "Epoch 23472/30000 Training Loss: 0.0399063304066658\n",
      "Epoch 23473/30000 Training Loss: 0.03512566536664963\n",
      "Epoch 23474/30000 Training Loss: 0.035380519926548004\n",
      "Epoch 23475/30000 Training Loss: 0.03450179845094681\n",
      "Epoch 23476/30000 Training Loss: 0.041467294096946716\n",
      "Epoch 23477/30000 Training Loss: 0.034883249551057816\n",
      "Epoch 23478/30000 Training Loss: 0.04239305853843689\n",
      "Epoch 23479/30000 Training Loss: 0.03686053305864334\n",
      "Epoch 23480/30000 Training Loss: 0.04432196915149689\n",
      "Epoch 23481/30000 Training Loss: 0.040352560579776764\n",
      "Epoch 23482/30000 Training Loss: 0.04299614578485489\n",
      "Epoch 23483/30000 Training Loss: 0.041728027164936066\n",
      "Epoch 23484/30000 Training Loss: 0.05099460110068321\n",
      "Epoch 23485/30000 Training Loss: 0.041804127395153046\n",
      "Epoch 23486/30000 Training Loss: 0.03762081265449524\n",
      "Epoch 23487/30000 Training Loss: 0.04219594597816467\n",
      "Epoch 23488/30000 Training Loss: 0.03423305228352547\n",
      "Epoch 23489/30000 Training Loss: 0.04183398187160492\n",
      "Epoch 23490/30000 Training Loss: 0.04327843338251114\n",
      "Epoch 23491/30000 Training Loss: 0.04727606475353241\n",
      "Epoch 23492/30000 Training Loss: 0.041130080819129944\n",
      "Epoch 23493/30000 Training Loss: 0.047466784715652466\n",
      "Epoch 23494/30000 Training Loss: 0.035151101648807526\n",
      "Epoch 23495/30000 Training Loss: 0.03547219559550285\n",
      "Epoch 23496/30000 Training Loss: 0.030721496790647507\n",
      "Epoch 23497/30000 Training Loss: 0.04689224436879158\n",
      "Epoch 23498/30000 Training Loss: 0.037840306758880615\n",
      "Epoch 23499/30000 Training Loss: 0.04164247214794159\n",
      "Epoch 23500/30000 Training Loss: 0.03943130373954773\n",
      "Epoch 23500/30000 Validation Loss: 0.05678509548306465\n",
      "Epoch 23501/30000 Training Loss: 0.034498367458581924\n",
      "Epoch 23502/30000 Training Loss: 0.043910760432481766\n",
      "Epoch 23503/30000 Training Loss: 0.03864169865846634\n",
      "Epoch 23504/30000 Training Loss: 0.05019176006317139\n",
      "Epoch 23505/30000 Training Loss: 0.0432872548699379\n",
      "Epoch 23506/30000 Training Loss: 0.05032292753458023\n",
      "Epoch 23507/30000 Training Loss: 0.04188047721982002\n",
      "Epoch 23508/30000 Training Loss: 0.04142113775014877\n",
      "Epoch 23509/30000 Training Loss: 0.03375137597322464\n",
      "Epoch 23510/30000 Training Loss: 0.05057747662067413\n",
      "Epoch 23511/30000 Training Loss: 0.05109158158302307\n",
      "Epoch 23512/30000 Training Loss: 0.04358929395675659\n",
      "Epoch 23513/30000 Training Loss: 0.040648870170116425\n",
      "Epoch 23514/30000 Training Loss: 0.031842585653066635\n",
      "Epoch 23515/30000 Training Loss: 0.053841814398765564\n",
      "Epoch 23516/30000 Training Loss: 0.040596455335617065\n",
      "Epoch 23517/30000 Training Loss: 0.03452703356742859\n",
      "Epoch 23518/30000 Training Loss: 0.038775380700826645\n",
      "Epoch 23519/30000 Training Loss: 0.04399877041578293\n",
      "Epoch 23520/30000 Training Loss: 0.04481063038110733\n",
      "Epoch 23521/30000 Training Loss: 0.0448511503636837\n",
      "Epoch 23522/30000 Training Loss: 0.03951195627450943\n",
      "Epoch 23523/30000 Training Loss: 0.032649923115968704\n",
      "Epoch 23524/30000 Training Loss: 0.0361676849424839\n",
      "Epoch 23525/30000 Training Loss: 0.039107371121644974\n",
      "Epoch 23526/30000 Training Loss: 0.037138745188713074\n",
      "Epoch 23527/30000 Training Loss: 0.0599968358874321\n",
      "Epoch 23528/30000 Training Loss: 0.03331820294260979\n",
      "Epoch 23529/30000 Training Loss: 0.04213260859251022\n",
      "Epoch 23530/30000 Training Loss: 0.04052229970693588\n",
      "Epoch 23531/30000 Training Loss: 0.039780598133802414\n",
      "Epoch 23532/30000 Training Loss: 0.04155255854129791\n",
      "Epoch 23533/30000 Training Loss: 0.037365712225437164\n",
      "Epoch 23534/30000 Training Loss: 0.038389209657907486\n",
      "Epoch 23535/30000 Training Loss: 0.03686962276697159\n",
      "Epoch 23536/30000 Training Loss: 0.03605884686112404\n",
      "Epoch 23537/30000 Training Loss: 0.05725914239883423\n",
      "Epoch 23538/30000 Training Loss: 0.04452117532491684\n",
      "Epoch 23539/30000 Training Loss: 0.04736354202032089\n",
      "Epoch 23540/30000 Training Loss: 0.055052194744348526\n",
      "Epoch 23541/30000 Training Loss: 0.05497618764638901\n",
      "Epoch 23542/30000 Training Loss: 0.04631379246711731\n",
      "Epoch 23543/30000 Training Loss: 0.055597878992557526\n",
      "Epoch 23544/30000 Training Loss: 0.04567921534180641\n",
      "Epoch 23545/30000 Training Loss: 0.03764006495475769\n",
      "Epoch 23546/30000 Training Loss: 0.03929251804947853\n",
      "Epoch 23547/30000 Training Loss: 0.045540668070316315\n",
      "Epoch 23548/30000 Training Loss: 0.034483227878808975\n",
      "Epoch 23549/30000 Training Loss: 0.041161417961120605\n",
      "Epoch 23550/30000 Training Loss: 0.04202713817358017\n",
      "Epoch 23551/30000 Training Loss: 0.03617972880601883\n",
      "Epoch 23552/30000 Training Loss: 0.03490370512008667\n",
      "Epoch 23553/30000 Training Loss: 0.03444434702396393\n",
      "Epoch 23554/30000 Training Loss: 0.040255725383758545\n",
      "Epoch 23555/30000 Training Loss: 0.053506284952163696\n",
      "Epoch 23556/30000 Training Loss: 0.04556872323155403\n",
      "Epoch 23557/30000 Training Loss: 0.03238660842180252\n",
      "Epoch 23558/30000 Training Loss: 0.038571372628211975\n",
      "Epoch 23559/30000 Training Loss: 0.02999061718583107\n",
      "Epoch 23560/30000 Training Loss: 0.03654082864522934\n",
      "Epoch 23561/30000 Training Loss: 0.03371912240982056\n",
      "Epoch 23562/30000 Training Loss: 0.04180420935153961\n",
      "Epoch 23563/30000 Training Loss: 0.04952472448348999\n",
      "Epoch 23564/30000 Training Loss: 0.026572439819574356\n",
      "Epoch 23565/30000 Training Loss: 0.039436642080545425\n",
      "Epoch 23566/30000 Training Loss: 0.03191876411437988\n",
      "Epoch 23567/30000 Training Loss: 0.04251624643802643\n",
      "Epoch 23568/30000 Training Loss: 0.036002758890390396\n",
      "Epoch 23569/30000 Training Loss: 0.042627133429050446\n",
      "Epoch 23570/30000 Training Loss: 0.04007212445139885\n",
      "Epoch 23571/30000 Training Loss: 0.049357570707798004\n",
      "Epoch 23572/30000 Training Loss: 0.04831010103225708\n",
      "Epoch 23573/30000 Training Loss: 0.04096692055463791\n",
      "Epoch 23574/30000 Training Loss: 0.0462782196700573\n",
      "Epoch 23575/30000 Training Loss: 0.04199372977018356\n",
      "Epoch 23576/30000 Training Loss: 0.05137494206428528\n",
      "Epoch 23577/30000 Training Loss: 0.036937009543180466\n",
      "Epoch 23578/30000 Training Loss: 0.04825318977236748\n",
      "Epoch 23579/30000 Training Loss: 0.036439038813114166\n",
      "Epoch 23580/30000 Training Loss: 0.04263279587030411\n",
      "Epoch 23581/30000 Training Loss: 0.04028232395648956\n",
      "Epoch 23582/30000 Training Loss: 0.033650998026132584\n",
      "Epoch 23583/30000 Training Loss: 0.050551995635032654\n",
      "Epoch 23584/30000 Training Loss: 0.03592821583151817\n",
      "Epoch 23585/30000 Training Loss: 0.048940785229206085\n",
      "Epoch 23586/30000 Training Loss: 0.043970223516225815\n",
      "Epoch 23587/30000 Training Loss: 0.038761381059885025\n",
      "Epoch 23588/30000 Training Loss: 0.03807198256254196\n",
      "Epoch 23589/30000 Training Loss: 0.0401548370718956\n",
      "Epoch 23590/30000 Training Loss: 0.02928660809993744\n",
      "Epoch 23591/30000 Training Loss: 0.04908585920929909\n",
      "Epoch 23592/30000 Training Loss: 0.04053132236003876\n",
      "Epoch 23593/30000 Training Loss: 0.04408189281821251\n",
      "Epoch 23594/30000 Training Loss: 0.040316030383110046\n",
      "Epoch 23595/30000 Training Loss: 0.07307679951190948\n",
      "Epoch 23596/30000 Training Loss: 0.0510227233171463\n",
      "Epoch 23597/30000 Training Loss: 0.04550393298268318\n",
      "Epoch 23598/30000 Training Loss: 0.039386503398418427\n",
      "Epoch 23599/30000 Training Loss: 0.05275898054242134\n",
      "Epoch 23600/30000 Training Loss: 0.04517194628715515\n",
      "Epoch 23600/30000 Validation Loss: 0.037349432706832886\n",
      "Epoch 23601/30000 Training Loss: 0.05307939648628235\n",
      "Epoch 23602/30000 Training Loss: 0.043816372752189636\n",
      "Epoch 23603/30000 Training Loss: 0.05803041160106659\n",
      "Epoch 23604/30000 Training Loss: 0.04301490634679794\n",
      "Epoch 23605/30000 Training Loss: 0.0334952212870121\n",
      "Epoch 23606/30000 Training Loss: 0.03873337805271149\n",
      "Epoch 23607/30000 Training Loss: 0.060941100120544434\n",
      "Epoch 23608/30000 Training Loss: 0.05888436734676361\n",
      "Epoch 23609/30000 Training Loss: 0.04034636169672012\n",
      "Epoch 23610/30000 Training Loss: 0.03374422341585159\n",
      "Epoch 23611/30000 Training Loss: 0.05500773340463638\n",
      "Epoch 23612/30000 Training Loss: 0.05623763054609299\n",
      "Epoch 23613/30000 Training Loss: 0.04997697472572327\n",
      "Epoch 23614/30000 Training Loss: 0.04857519641518593\n",
      "Epoch 23615/30000 Training Loss: 0.04793762415647507\n",
      "Epoch 23616/30000 Training Loss: 0.032157473266124725\n",
      "Epoch 23617/30000 Training Loss: 0.029924608767032623\n",
      "Epoch 23618/30000 Training Loss: 0.038737498223781586\n",
      "Epoch 23619/30000 Training Loss: 0.04977564141154289\n",
      "Epoch 23620/30000 Training Loss: 0.049135107547044754\n",
      "Epoch 23621/30000 Training Loss: 0.03976450487971306\n",
      "Epoch 23622/30000 Training Loss: 0.046257928013801575\n",
      "Epoch 23623/30000 Training Loss: 0.038412898778915405\n",
      "Epoch 23624/30000 Training Loss: 0.04304013401269913\n",
      "Epoch 23625/30000 Training Loss: 0.044487934559583664\n",
      "Epoch 23626/30000 Training Loss: 0.04136813431978226\n",
      "Epoch 23627/30000 Training Loss: 0.04929094761610031\n",
      "Epoch 23628/30000 Training Loss: 0.06748998910188675\n",
      "Epoch 23629/30000 Training Loss: 0.05099645256996155\n",
      "Epoch 23630/30000 Training Loss: 0.04859016835689545\n",
      "Epoch 23631/30000 Training Loss: 0.034730371087789536\n",
      "Epoch 23632/30000 Training Loss: 0.039123326539993286\n",
      "Epoch 23633/30000 Training Loss: 0.04211502894759178\n",
      "Epoch 23634/30000 Training Loss: 0.04133409261703491\n",
      "Epoch 23635/30000 Training Loss: 0.03811309114098549\n",
      "Epoch 23636/30000 Training Loss: 0.04469502717256546\n",
      "Epoch 23637/30000 Training Loss: 0.04371998459100723\n",
      "Epoch 23638/30000 Training Loss: 0.04211762920022011\n",
      "Epoch 23639/30000 Training Loss: 0.03334982320666313\n",
      "Epoch 23640/30000 Training Loss: 0.04180477187037468\n",
      "Epoch 23641/30000 Training Loss: 0.045514557510614395\n",
      "Epoch 23642/30000 Training Loss: 0.030317187309265137\n",
      "Epoch 23643/30000 Training Loss: 0.03737766295671463\n",
      "Epoch 23644/30000 Training Loss: 0.0420030802488327\n",
      "Epoch 23645/30000 Training Loss: 0.034713275730609894\n",
      "Epoch 23646/30000 Training Loss: 0.0516543909907341\n",
      "Epoch 23647/30000 Training Loss: 0.04248381406068802\n",
      "Epoch 23648/30000 Training Loss: 0.0436844527721405\n",
      "Epoch 23649/30000 Training Loss: 0.039104536175727844\n",
      "Epoch 23650/30000 Training Loss: 0.0378246083855629\n",
      "Epoch 23651/30000 Training Loss: 0.04350161924958229\n",
      "Epoch 23652/30000 Training Loss: 0.061035268008708954\n",
      "Epoch 23653/30000 Training Loss: 0.04893986135721207\n",
      "Epoch 23654/30000 Training Loss: 0.0504385270178318\n",
      "Epoch 23655/30000 Training Loss: 0.03548267111182213\n",
      "Epoch 23656/30000 Training Loss: 0.03827265277504921\n",
      "Epoch 23657/30000 Training Loss: 0.029402604326605797\n",
      "Epoch 23658/30000 Training Loss: 0.05981311947107315\n",
      "Epoch 23659/30000 Training Loss: 0.05265239626169205\n",
      "Epoch 23660/30000 Training Loss: 0.04351828247308731\n",
      "Epoch 23661/30000 Training Loss: 0.04259498044848442\n",
      "Epoch 23662/30000 Training Loss: 0.03822426125407219\n",
      "Epoch 23663/30000 Training Loss: 0.06019043177366257\n",
      "Epoch 23664/30000 Training Loss: 0.041710082441568375\n",
      "Epoch 23665/30000 Training Loss: 0.040918633341789246\n",
      "Epoch 23666/30000 Training Loss: 0.042593736201524734\n",
      "Epoch 23667/30000 Training Loss: 0.034543782472610474\n",
      "Epoch 23668/30000 Training Loss: 0.0539279542863369\n",
      "Epoch 23669/30000 Training Loss: 0.03970497474074364\n",
      "Epoch 23670/30000 Training Loss: 0.04204246401786804\n",
      "Epoch 23671/30000 Training Loss: 0.04177118092775345\n",
      "Epoch 23672/30000 Training Loss: 0.05378790944814682\n",
      "Epoch 23673/30000 Training Loss: 0.03691817820072174\n",
      "Epoch 23674/30000 Training Loss: 0.05054324120283127\n",
      "Epoch 23675/30000 Training Loss: 0.04639638215303421\n",
      "Epoch 23676/30000 Training Loss: 0.041017115116119385\n",
      "Epoch 23677/30000 Training Loss: 0.04113118723034859\n",
      "Epoch 23678/30000 Training Loss: 0.051067739725112915\n",
      "Epoch 23679/30000 Training Loss: 0.03388393670320511\n",
      "Epoch 23680/30000 Training Loss: 0.04692757874727249\n",
      "Epoch 23681/30000 Training Loss: 0.03823566436767578\n",
      "Epoch 23682/30000 Training Loss: 0.03365609049797058\n",
      "Epoch 23683/30000 Training Loss: 0.05327794700860977\n",
      "Epoch 23684/30000 Training Loss: 0.040685880929231644\n",
      "Epoch 23685/30000 Training Loss: 0.04367935657501221\n",
      "Epoch 23686/30000 Training Loss: 0.05012128874659538\n",
      "Epoch 23687/30000 Training Loss: 0.04152379557490349\n",
      "Epoch 23688/30000 Training Loss: 0.044426776468753815\n",
      "Epoch 23689/30000 Training Loss: 0.04187575727701187\n",
      "Epoch 23690/30000 Training Loss: 0.030670396983623505\n",
      "Epoch 23691/30000 Training Loss: 0.040934257209300995\n",
      "Epoch 23692/30000 Training Loss: 0.03661920875310898\n",
      "Epoch 23693/30000 Training Loss: 0.046208351850509644\n",
      "Epoch 23694/30000 Training Loss: 0.044502463191747665\n",
      "Epoch 23695/30000 Training Loss: 0.05440439656376839\n",
      "Epoch 23696/30000 Training Loss: 0.046977825462818146\n",
      "Epoch 23697/30000 Training Loss: 0.05540706217288971\n",
      "Epoch 23698/30000 Training Loss: 0.029036404564976692\n",
      "Epoch 23699/30000 Training Loss: 0.03177771344780922\n",
      "Epoch 23700/30000 Training Loss: 0.04613986611366272\n",
      "Epoch 23700/30000 Validation Loss: 0.04420149698853493\n",
      "Epoch 23701/30000 Training Loss: 0.04067324101924896\n",
      "Epoch 23702/30000 Training Loss: 0.04498463124036789\n",
      "Epoch 23703/30000 Training Loss: 0.05140639841556549\n",
      "Epoch 23704/30000 Training Loss: 0.048464320600032806\n",
      "Epoch 23705/30000 Training Loss: 0.04337979853153229\n",
      "Epoch 23706/30000 Training Loss: 0.04376157373189926\n",
      "Epoch 23707/30000 Training Loss: 0.037063926458358765\n",
      "Epoch 23708/30000 Training Loss: 0.045501917600631714\n",
      "Epoch 23709/30000 Training Loss: 0.04995575174689293\n",
      "Epoch 23710/30000 Training Loss: 0.03314370661973953\n",
      "Epoch 23711/30000 Training Loss: 0.04451578110456467\n",
      "Epoch 23712/30000 Training Loss: 0.046610426157712936\n",
      "Epoch 23713/30000 Training Loss: 0.04203988239169121\n",
      "Epoch 23714/30000 Training Loss: 0.041315268725156784\n",
      "Epoch 23715/30000 Training Loss: 0.0445040762424469\n",
      "Epoch 23716/30000 Training Loss: 0.023252684623003006\n",
      "Epoch 23717/30000 Training Loss: 0.05899053066968918\n",
      "Epoch 23718/30000 Training Loss: 0.04199320450425148\n",
      "Epoch 23719/30000 Training Loss: 0.0440736785531044\n",
      "Epoch 23720/30000 Training Loss: 0.06029805168509483\n",
      "Epoch 23721/30000 Training Loss: 0.0283720213919878\n",
      "Epoch 23722/30000 Training Loss: 0.03519667312502861\n",
      "Epoch 23723/30000 Training Loss: 0.05099796503782272\n",
      "Epoch 23724/30000 Training Loss: 0.04231707751750946\n",
      "Epoch 23725/30000 Training Loss: 0.04677067697048187\n",
      "Epoch 23726/30000 Training Loss: 0.04515582323074341\n",
      "Epoch 23727/30000 Training Loss: 0.04177091270685196\n",
      "Epoch 23728/30000 Training Loss: 0.05486418679356575\n",
      "Epoch 23729/30000 Training Loss: 0.032009828835725784\n",
      "Epoch 23730/30000 Training Loss: 0.046203285455703735\n",
      "Epoch 23731/30000 Training Loss: 0.04373961687088013\n",
      "Epoch 23732/30000 Training Loss: 0.03817294165492058\n",
      "Epoch 23733/30000 Training Loss: 0.047153979539871216\n",
      "Epoch 23734/30000 Training Loss: 0.04207480698823929\n",
      "Epoch 23735/30000 Training Loss: 0.045837510377168655\n",
      "Epoch 23736/30000 Training Loss: 0.05286840349435806\n",
      "Epoch 23737/30000 Training Loss: 0.040508560836315155\n",
      "Epoch 23738/30000 Training Loss: 0.04089188575744629\n",
      "Epoch 23739/30000 Training Loss: 0.04012076556682587\n",
      "Epoch 23740/30000 Training Loss: 0.048786912113428116\n",
      "Epoch 23741/30000 Training Loss: 0.048448145389556885\n",
      "Epoch 23742/30000 Training Loss: 0.047135934233665466\n",
      "Epoch 23743/30000 Training Loss: 0.0523071326315403\n",
      "Epoch 23744/30000 Training Loss: 0.03884880617260933\n",
      "Epoch 23745/30000 Training Loss: 0.04221099615097046\n",
      "Epoch 23746/30000 Training Loss: 0.03717263042926788\n",
      "Epoch 23747/30000 Training Loss: 0.047139398753643036\n",
      "Epoch 23748/30000 Training Loss: 0.03427570313215256\n",
      "Epoch 23749/30000 Training Loss: 0.05465155094861984\n",
      "Epoch 23750/30000 Training Loss: 0.04709378629922867\n",
      "Epoch 23751/30000 Training Loss: 0.0499061718583107\n",
      "Epoch 23752/30000 Training Loss: 0.05508029833436012\n",
      "Epoch 23753/30000 Training Loss: 0.03277873247861862\n",
      "Epoch 23754/30000 Training Loss: 0.059254057705402374\n",
      "Epoch 23755/30000 Training Loss: 0.055506207048892975\n",
      "Epoch 23756/30000 Training Loss: 0.04890052601695061\n",
      "Epoch 23757/30000 Training Loss: 0.045577578246593475\n",
      "Epoch 23758/30000 Training Loss: 0.03578127175569534\n",
      "Epoch 23759/30000 Training Loss: 0.04188136011362076\n",
      "Epoch 23760/30000 Training Loss: 0.03837314993143082\n",
      "Epoch 23761/30000 Training Loss: 0.04307766258716583\n",
      "Epoch 23762/30000 Training Loss: 0.032673969864845276\n",
      "Epoch 23763/30000 Training Loss: 0.045251697301864624\n",
      "Epoch 23764/30000 Training Loss: 0.032747358083724976\n",
      "Epoch 23765/30000 Training Loss: 0.04442490264773369\n",
      "Epoch 23766/30000 Training Loss: 0.038630880415439606\n",
      "Epoch 23767/30000 Training Loss: 0.05675463378429413\n",
      "Epoch 23768/30000 Training Loss: 0.04623205214738846\n",
      "Epoch 23769/30000 Training Loss: 0.034350477159023285\n",
      "Epoch 23770/30000 Training Loss: 0.04591507837176323\n",
      "Epoch 23771/30000 Training Loss: 0.045901816338300705\n",
      "Epoch 23772/30000 Training Loss: 0.04337756335735321\n",
      "Epoch 23773/30000 Training Loss: 0.06131747364997864\n",
      "Epoch 23774/30000 Training Loss: 0.03800216317176819\n",
      "Epoch 23775/30000 Training Loss: 0.0488387756049633\n",
      "Epoch 23776/30000 Training Loss: 0.039789289236068726\n",
      "Epoch 23777/30000 Training Loss: 0.04831727221608162\n",
      "Epoch 23778/30000 Training Loss: 0.05183303356170654\n",
      "Epoch 23779/30000 Training Loss: 0.040935445576906204\n",
      "Epoch 23780/30000 Training Loss: 0.03865828365087509\n",
      "Epoch 23781/30000 Training Loss: 0.05058170109987259\n",
      "Epoch 23782/30000 Training Loss: 0.026985347270965576\n",
      "Epoch 23783/30000 Training Loss: 0.042117372155189514\n",
      "Epoch 23784/30000 Training Loss: 0.05232430249452591\n",
      "Epoch 23785/30000 Training Loss: 0.03495844453573227\n",
      "Epoch 23786/30000 Training Loss: 0.04866226017475128\n",
      "Epoch 23787/30000 Training Loss: 0.05440811812877655\n",
      "Epoch 23788/30000 Training Loss: 0.05082608759403229\n",
      "Epoch 23789/30000 Training Loss: 0.04185087978839874\n",
      "Epoch 23790/30000 Training Loss: 0.03935416787862778\n",
      "Epoch 23791/30000 Training Loss: 0.03821498900651932\n",
      "Epoch 23792/30000 Training Loss: 0.04369833320379257\n",
      "Epoch 23793/30000 Training Loss: 0.02869398333132267\n",
      "Epoch 23794/30000 Training Loss: 0.05055444687604904\n",
      "Epoch 23795/30000 Training Loss: 0.047526054084300995\n",
      "Epoch 23796/30000 Training Loss: 0.036422714591026306\n",
      "Epoch 23797/30000 Training Loss: 0.04685300588607788\n",
      "Epoch 23798/30000 Training Loss: 0.0388902872800827\n",
      "Epoch 23799/30000 Training Loss: 0.042373284697532654\n",
      "Epoch 23800/30000 Training Loss: 0.03237666189670563\n",
      "Epoch 23800/30000 Validation Loss: 0.035639241337776184\n",
      "Epoch 23801/30000 Training Loss: 0.0504123829305172\n",
      "Epoch 23802/30000 Training Loss: 0.053861260414123535\n",
      "Epoch 23803/30000 Training Loss: 0.050942011177539825\n",
      "Epoch 23804/30000 Training Loss: 0.044743429869413376\n",
      "Epoch 23805/30000 Training Loss: 0.051956795156002045\n",
      "Epoch 23806/30000 Training Loss: 0.03839157521724701\n",
      "Epoch 23807/30000 Training Loss: 0.039051786065101624\n",
      "Epoch 23808/30000 Training Loss: 0.052101120352745056\n",
      "Epoch 23809/30000 Training Loss: 0.04002120718359947\n",
      "Epoch 23810/30000 Training Loss: 0.04079210385680199\n",
      "Epoch 23811/30000 Training Loss: 0.035435546189546585\n",
      "Epoch 23812/30000 Training Loss: 0.03692479431629181\n",
      "Epoch 23813/30000 Training Loss: 0.03899585083127022\n",
      "Epoch 23814/30000 Training Loss: 0.0629085898399353\n",
      "Epoch 23815/30000 Training Loss: 0.039547085762023926\n",
      "Epoch 23816/30000 Training Loss: 0.0572047084569931\n",
      "Epoch 23817/30000 Training Loss: 0.04698772728443146\n",
      "Epoch 23818/30000 Training Loss: 0.051199011504650116\n",
      "Epoch 23819/30000 Training Loss: 0.048777662217617035\n",
      "Epoch 23820/30000 Training Loss: 0.052522554993629456\n",
      "Epoch 23821/30000 Training Loss: 0.042230211198329926\n",
      "Epoch 23822/30000 Training Loss: 0.055235207080841064\n",
      "Epoch 23823/30000 Training Loss: 0.042826566845178604\n",
      "Epoch 23824/30000 Training Loss: 0.05020616948604584\n",
      "Epoch 23825/30000 Training Loss: 0.05545242875814438\n",
      "Epoch 23826/30000 Training Loss: 0.05865079537034035\n",
      "Epoch 23827/30000 Training Loss: 0.033736106008291245\n",
      "Epoch 23828/30000 Training Loss: 0.04200058430433273\n",
      "Epoch 23829/30000 Training Loss: 0.04453970491886139\n",
      "Epoch 23830/30000 Training Loss: 0.03654565289616585\n",
      "Epoch 23831/30000 Training Loss: 0.027947939932346344\n",
      "Epoch 23832/30000 Training Loss: 0.04804827272891998\n",
      "Epoch 23833/30000 Training Loss: 0.0399889275431633\n",
      "Epoch 23834/30000 Training Loss: 0.04776754975318909\n",
      "Epoch 23835/30000 Training Loss: 0.03519470617175102\n",
      "Epoch 23836/30000 Training Loss: 0.03633774816989899\n",
      "Epoch 23837/30000 Training Loss: 0.041549202054739\n",
      "Epoch 23838/30000 Training Loss: 0.03621061518788338\n",
      "Epoch 23839/30000 Training Loss: 0.03528212010860443\n",
      "Epoch 23840/30000 Training Loss: 0.05029094219207764\n",
      "Epoch 23841/30000 Training Loss: 0.0389750711619854\n",
      "Epoch 23842/30000 Training Loss: 0.03978336229920387\n",
      "Epoch 23843/30000 Training Loss: 0.043322086334228516\n",
      "Epoch 23844/30000 Training Loss: 0.04156005382537842\n",
      "Epoch 23845/30000 Training Loss: 0.05830781161785126\n",
      "Epoch 23846/30000 Training Loss: 0.04423252493143082\n",
      "Epoch 23847/30000 Training Loss: 0.054900262504816055\n",
      "Epoch 23848/30000 Training Loss: 0.04067403823137283\n",
      "Epoch 23849/30000 Training Loss: 0.0367073118686676\n",
      "Epoch 23850/30000 Training Loss: 0.04284030944108963\n",
      "Epoch 23851/30000 Training Loss: 0.03554191440343857\n",
      "Epoch 23852/30000 Training Loss: 0.03807531297206879\n",
      "Epoch 23853/30000 Training Loss: 0.04871881753206253\n",
      "Epoch 23854/30000 Training Loss: 0.039132259786129\n",
      "Epoch 23855/30000 Training Loss: 0.05445842444896698\n",
      "Epoch 23856/30000 Training Loss: 0.04230761528015137\n",
      "Epoch 23857/30000 Training Loss: 0.03826700523495674\n",
      "Epoch 23858/30000 Training Loss: 0.05310783535242081\n",
      "Epoch 23859/30000 Training Loss: 0.04554065316915512\n",
      "Epoch 23860/30000 Training Loss: 0.048626795411109924\n",
      "Epoch 23861/30000 Training Loss: 0.045794300734996796\n",
      "Epoch 23862/30000 Training Loss: 0.036498747766017914\n",
      "Epoch 23863/30000 Training Loss: 0.037401966750621796\n",
      "Epoch 23864/30000 Training Loss: 0.03338856250047684\n",
      "Epoch 23865/30000 Training Loss: 0.03163023293018341\n",
      "Epoch 23866/30000 Training Loss: 0.05214961618185043\n",
      "Epoch 23867/30000 Training Loss: 0.04530314356088638\n",
      "Epoch 23868/30000 Training Loss: 0.0473598837852478\n",
      "Epoch 23869/30000 Training Loss: 0.0338314063847065\n",
      "Epoch 23870/30000 Training Loss: 0.04520895704627037\n",
      "Epoch 23871/30000 Training Loss: 0.0362030491232872\n",
      "Epoch 23872/30000 Training Loss: 0.05341237783432007\n",
      "Epoch 23873/30000 Training Loss: 0.0382680743932724\n",
      "Epoch 23874/30000 Training Loss: 0.03959134221076965\n",
      "Epoch 23875/30000 Training Loss: 0.036710307002067566\n",
      "Epoch 23876/30000 Training Loss: 0.032853759825229645\n",
      "Epoch 23877/30000 Training Loss: 0.039376433938741684\n",
      "Epoch 23878/30000 Training Loss: 0.037895772606134415\n",
      "Epoch 23879/30000 Training Loss: 0.04746402055025101\n",
      "Epoch 23880/30000 Training Loss: 0.0501396581530571\n",
      "Epoch 23881/30000 Training Loss: 0.03939061611890793\n",
      "Epoch 23882/30000 Training Loss: 0.03474526107311249\n",
      "Epoch 23883/30000 Training Loss: 0.048727259039878845\n",
      "Epoch 23884/30000 Training Loss: 0.044107042253017426\n",
      "Epoch 23885/30000 Training Loss: 0.03548266738653183\n",
      "Epoch 23886/30000 Training Loss: 0.03766454756259918\n",
      "Epoch 23887/30000 Training Loss: 0.05771641805768013\n",
      "Epoch 23888/30000 Training Loss: 0.044426657259464264\n",
      "Epoch 23889/30000 Training Loss: 0.05411401391029358\n",
      "Epoch 23890/30000 Training Loss: 0.05133175104856491\n",
      "Epoch 23891/30000 Training Loss: 0.03622321039438248\n",
      "Epoch 23892/30000 Training Loss: 0.04061080887913704\n",
      "Epoch 23893/30000 Training Loss: 0.03005983494222164\n",
      "Epoch 23894/30000 Training Loss: 0.052604854106903076\n",
      "Epoch 23895/30000 Training Loss: 0.03743000701069832\n",
      "Epoch 23896/30000 Training Loss: 0.04590831696987152\n",
      "Epoch 23897/30000 Training Loss: 0.050059687346220016\n",
      "Epoch 23898/30000 Training Loss: 0.045868031680583954\n",
      "Epoch 23899/30000 Training Loss: 0.04038824141025543\n",
      "Epoch 23900/30000 Training Loss: 0.037862617522478104\n",
      "Epoch 23900/30000 Validation Loss: 0.03947223722934723\n",
      "Epoch 23901/30000 Training Loss: 0.04504285752773285\n",
      "Epoch 23902/30000 Training Loss: 0.040759529918432236\n",
      "Epoch 23903/30000 Training Loss: 0.0478634238243103\n",
      "Epoch 23904/30000 Training Loss: 0.05097992718219757\n",
      "Epoch 23905/30000 Training Loss: 0.04279457777738571\n",
      "Epoch 23906/30000 Training Loss: 0.05046919733285904\n",
      "Epoch 23907/30000 Training Loss: 0.04304507002234459\n",
      "Epoch 23908/30000 Training Loss: 0.037224069237709045\n",
      "Epoch 23909/30000 Training Loss: 0.049713388085365295\n",
      "Epoch 23910/30000 Training Loss: 0.04678541421890259\n",
      "Epoch 23911/30000 Training Loss: 0.045130275189876556\n",
      "Epoch 23912/30000 Training Loss: 0.044530048966407776\n",
      "Epoch 23913/30000 Training Loss: 0.05229166895151138\n",
      "Epoch 23914/30000 Training Loss: 0.05071302503347397\n",
      "Epoch 23915/30000 Training Loss: 0.03383073955774307\n",
      "Epoch 23916/30000 Training Loss: 0.029841143637895584\n",
      "Epoch 23917/30000 Training Loss: 0.0417235866189003\n",
      "Epoch 23918/30000 Training Loss: 0.04293866455554962\n",
      "Epoch 23919/30000 Training Loss: 0.032510772347450256\n",
      "Epoch 23920/30000 Training Loss: 0.05988316610455513\n",
      "Epoch 23921/30000 Training Loss: 0.06377508491277695\n",
      "Epoch 23922/30000 Training Loss: 0.04825016111135483\n",
      "Epoch 23923/30000 Training Loss: 0.036627303808927536\n",
      "Epoch 23924/30000 Training Loss: 0.046517014503479004\n",
      "Epoch 23925/30000 Training Loss: 0.042405299842357635\n",
      "Epoch 23926/30000 Training Loss: 0.058210812509059906\n",
      "Epoch 23927/30000 Training Loss: 0.050232838839292526\n",
      "Epoch 23928/30000 Training Loss: 0.034483738243579865\n",
      "Epoch 23929/30000 Training Loss: 0.039796531200408936\n",
      "Epoch 23930/30000 Training Loss: 0.04308721795678139\n",
      "Epoch 23931/30000 Training Loss: 0.040443964302539825\n",
      "Epoch 23932/30000 Training Loss: 0.0396273173391819\n",
      "Epoch 23933/30000 Training Loss: 0.030907973647117615\n",
      "Epoch 23934/30000 Training Loss: 0.049113549292087555\n",
      "Epoch 23935/30000 Training Loss: 0.04527533799409866\n",
      "Epoch 23936/30000 Training Loss: 0.04426082968711853\n",
      "Epoch 23937/30000 Training Loss: 0.03899458795785904\n",
      "Epoch 23938/30000 Training Loss: 0.046770572662353516\n",
      "Epoch 23939/30000 Training Loss: 0.05595077946782112\n",
      "Epoch 23940/30000 Training Loss: 0.03742491453886032\n",
      "Epoch 23941/30000 Training Loss: 0.03195594251155853\n",
      "Epoch 23942/30000 Training Loss: 0.041240666061639786\n",
      "Epoch 23943/30000 Training Loss: 0.04392228275537491\n",
      "Epoch 23944/30000 Training Loss: 0.046448372304439545\n",
      "Epoch 23945/30000 Training Loss: 0.041115351021289825\n",
      "Epoch 23946/30000 Training Loss: 0.04491112381219864\n",
      "Epoch 23947/30000 Training Loss: 0.05501995235681534\n",
      "Epoch 23948/30000 Training Loss: 0.04303506016731262\n",
      "Epoch 23949/30000 Training Loss: 0.04150481894612312\n",
      "Epoch 23950/30000 Training Loss: 0.0333762988448143\n",
      "Epoch 23951/30000 Training Loss: 0.04364345222711563\n",
      "Epoch 23952/30000 Training Loss: 0.049503058195114136\n",
      "Epoch 23953/30000 Training Loss: 0.04795096814632416\n",
      "Epoch 23954/30000 Training Loss: 0.035671547055244446\n",
      "Epoch 23955/30000 Training Loss: 0.04524509608745575\n",
      "Epoch 23956/30000 Training Loss: 0.05430467426776886\n",
      "Epoch 23957/30000 Training Loss: 0.03783419355750084\n",
      "Epoch 23958/30000 Training Loss: 0.03687219321727753\n",
      "Epoch 23959/30000 Training Loss: 0.036800503730773926\n",
      "Epoch 23960/30000 Training Loss: 0.04330529272556305\n",
      "Epoch 23961/30000 Training Loss: 0.04280722513794899\n",
      "Epoch 23962/30000 Training Loss: 0.043728332966566086\n",
      "Epoch 23963/30000 Training Loss: 0.05181724578142166\n",
      "Epoch 23964/30000 Training Loss: 0.042492881417274475\n",
      "Epoch 23965/30000 Training Loss: 0.036878012120723724\n",
      "Epoch 23966/30000 Training Loss: 0.039984896779060364\n",
      "Epoch 23967/30000 Training Loss: 0.040667030960321426\n",
      "Epoch 23968/30000 Training Loss: 0.0385265126824379\n",
      "Epoch 23969/30000 Training Loss: 0.036368634551763535\n",
      "Epoch 23970/30000 Training Loss: 0.03753641992807388\n",
      "Epoch 23971/30000 Training Loss: 0.03602362796664238\n",
      "Epoch 23972/30000 Training Loss: 0.04052208736538887\n",
      "Epoch 23973/30000 Training Loss: 0.04196910560131073\n",
      "Epoch 23974/30000 Training Loss: 0.04287147521972656\n",
      "Epoch 23975/30000 Training Loss: 0.04997866228222847\n",
      "Epoch 23976/30000 Training Loss: 0.03405293449759483\n",
      "Epoch 23977/30000 Training Loss: 0.04175430163741112\n",
      "Epoch 23978/30000 Training Loss: 0.02885366603732109\n",
      "Epoch 23979/30000 Training Loss: 0.03353032469749451\n",
      "Epoch 23980/30000 Training Loss: 0.045140743255615234\n",
      "Epoch 23981/30000 Training Loss: 0.04845636337995529\n",
      "Epoch 23982/30000 Training Loss: 0.0618705190718174\n",
      "Epoch 23983/30000 Training Loss: 0.03566566854715347\n",
      "Epoch 23984/30000 Training Loss: 0.04059484228491783\n",
      "Epoch 23985/30000 Training Loss: 0.03422042727470398\n",
      "Epoch 23986/30000 Training Loss: 0.04971678555011749\n",
      "Epoch 23987/30000 Training Loss: 0.03872983157634735\n",
      "Epoch 23988/30000 Training Loss: 0.04911637306213379\n",
      "Epoch 23989/30000 Training Loss: 0.034868285059928894\n",
      "Epoch 23990/30000 Training Loss: 0.05161936953663826\n",
      "Epoch 23991/30000 Training Loss: 0.038978978991508484\n",
      "Epoch 23992/30000 Training Loss: 0.0362761951982975\n",
      "Epoch 23993/30000 Training Loss: 0.05423205345869064\n",
      "Epoch 23994/30000 Training Loss: 0.04256509989500046\n",
      "Epoch 23995/30000 Training Loss: 0.03167816251516342\n",
      "Epoch 23996/30000 Training Loss: 0.035874899476766586\n",
      "Epoch 23997/30000 Training Loss: 0.04728137329220772\n",
      "Epoch 23998/30000 Training Loss: 0.052755434066057205\n",
      "Epoch 23999/30000 Training Loss: 0.04595364257693291\n",
      "Epoch 24000/30000 Training Loss: 0.04684003069996834\n",
      "Epoch 24000/30000 Validation Loss: 0.03544578701257706\n",
      "Epoch 24001/30000 Training Loss: 0.03222932294011116\n",
      "Epoch 24002/30000 Training Loss: 0.051149215549230576\n",
      "Epoch 24003/30000 Training Loss: 0.037862349301576614\n",
      "Epoch 24004/30000 Training Loss: 0.045287810266017914\n",
      "Epoch 24005/30000 Training Loss: 0.030971627682447433\n",
      "Epoch 24006/30000 Training Loss: 0.047775402665138245\n",
      "Epoch 24007/30000 Training Loss: 0.038254715502262115\n",
      "Epoch 24008/30000 Training Loss: 0.03737805038690567\n",
      "Epoch 24009/30000 Training Loss: 0.03719858080148697\n",
      "Epoch 24010/30000 Training Loss: 0.03857312723994255\n",
      "Epoch 24011/30000 Training Loss: 0.03338504582643509\n",
      "Epoch 24012/30000 Training Loss: 0.04130273312330246\n",
      "Epoch 24013/30000 Training Loss: 0.04638626426458359\n",
      "Epoch 24014/30000 Training Loss: 0.04155780375003815\n",
      "Epoch 24015/30000 Training Loss: 0.04370938614010811\n",
      "Epoch 24016/30000 Training Loss: 0.038415294140577316\n",
      "Epoch 24017/30000 Training Loss: 0.04374244809150696\n",
      "Epoch 24018/30000 Training Loss: 0.053511228412389755\n",
      "Epoch 24019/30000 Training Loss: 0.03950861468911171\n",
      "Epoch 24020/30000 Training Loss: 0.0455620214343071\n",
      "Epoch 24021/30000 Training Loss: 0.04145262390375137\n",
      "Epoch 24022/30000 Training Loss: 0.038563065230846405\n",
      "Epoch 24023/30000 Training Loss: 0.04054868221282959\n",
      "Epoch 24024/30000 Training Loss: 0.04962035268545151\n",
      "Epoch 24025/30000 Training Loss: 0.03340703994035721\n",
      "Epoch 24026/30000 Training Loss: 0.05982973426580429\n",
      "Epoch 24027/30000 Training Loss: 0.0326043963432312\n",
      "Epoch 24028/30000 Training Loss: 0.029935039579868317\n",
      "Epoch 24029/30000 Training Loss: 0.037390463054180145\n",
      "Epoch 24030/30000 Training Loss: 0.03870787471532822\n",
      "Epoch 24031/30000 Training Loss: 0.038999877870082855\n",
      "Epoch 24032/30000 Training Loss: 0.027280783280730247\n",
      "Epoch 24033/30000 Training Loss: 0.041082803159952164\n",
      "Epoch 24034/30000 Training Loss: 0.04365663230419159\n",
      "Epoch 24035/30000 Training Loss: 0.041762031614780426\n",
      "Epoch 24036/30000 Training Loss: 0.032200299203395844\n",
      "Epoch 24037/30000 Training Loss: 0.05053408816456795\n",
      "Epoch 24038/30000 Training Loss: 0.05671811103820801\n",
      "Epoch 24039/30000 Training Loss: 0.04596314951777458\n",
      "Epoch 24040/30000 Training Loss: 0.04124943166971207\n",
      "Epoch 24041/30000 Training Loss: 0.04749683290719986\n",
      "Epoch 24042/30000 Training Loss: 0.02978392317891121\n",
      "Epoch 24043/30000 Training Loss: 0.04199482500553131\n",
      "Epoch 24044/30000 Training Loss: 0.04941251873970032\n",
      "Epoch 24045/30000 Training Loss: 0.04528600722551346\n",
      "Epoch 24046/30000 Training Loss: 0.03725675493478775\n",
      "Epoch 24047/30000 Training Loss: 0.051799409091472626\n",
      "Epoch 24048/30000 Training Loss: 0.03819692134857178\n",
      "Epoch 24049/30000 Training Loss: 0.037753455340862274\n",
      "Epoch 24050/30000 Training Loss: 0.04739471524953842\n",
      "Epoch 24051/30000 Training Loss: 0.03588651120662689\n",
      "Epoch 24052/30000 Training Loss: 0.038338594138622284\n",
      "Epoch 24053/30000 Training Loss: 0.03459809720516205\n",
      "Epoch 24054/30000 Training Loss: 0.04672832041978836\n",
      "Epoch 24055/30000 Training Loss: 0.03514399752020836\n",
      "Epoch 24056/30000 Training Loss: 0.0634315088391304\n",
      "Epoch 24057/30000 Training Loss: 0.05341070890426636\n",
      "Epoch 24058/30000 Training Loss: 0.03970424830913544\n",
      "Epoch 24059/30000 Training Loss: 0.05049458518624306\n",
      "Epoch 24060/30000 Training Loss: 0.047285471111536026\n",
      "Epoch 24061/30000 Training Loss: 0.057284045964479446\n",
      "Epoch 24062/30000 Training Loss: 0.0406026616692543\n",
      "Epoch 24063/30000 Training Loss: 0.04109643027186394\n",
      "Epoch 24064/30000 Training Loss: 0.04484087973833084\n",
      "Epoch 24065/30000 Training Loss: 0.04699483513832092\n",
      "Epoch 24066/30000 Training Loss: 0.026829294860363007\n",
      "Epoch 24067/30000 Training Loss: 0.04088304936885834\n",
      "Epoch 24068/30000 Training Loss: 0.04355008900165558\n",
      "Epoch 24069/30000 Training Loss: 0.03920391947031021\n",
      "Epoch 24070/30000 Training Loss: 0.04465349018573761\n",
      "Epoch 24071/30000 Training Loss: 0.051382869482040405\n",
      "Epoch 24072/30000 Training Loss: 0.05190751701593399\n",
      "Epoch 24073/30000 Training Loss: 0.04318035766482353\n",
      "Epoch 24074/30000 Training Loss: 0.056162796914577484\n",
      "Epoch 24075/30000 Training Loss: 0.05635114014148712\n",
      "Epoch 24076/30000 Training Loss: 0.04478486627340317\n",
      "Epoch 24077/30000 Training Loss: 0.035926710814237595\n",
      "Epoch 24078/30000 Training Loss: 0.03984783962368965\n",
      "Epoch 24079/30000 Training Loss: 0.04291639104485512\n",
      "Epoch 24080/30000 Training Loss: 0.0473005473613739\n",
      "Epoch 24081/30000 Training Loss: 0.041352156549692154\n",
      "Epoch 24082/30000 Training Loss: 0.048169154673814774\n",
      "Epoch 24083/30000 Training Loss: 0.04601547122001648\n",
      "Epoch 24084/30000 Training Loss: 0.041399504989385605\n",
      "Epoch 24085/30000 Training Loss: 0.04513102024793625\n",
      "Epoch 24086/30000 Training Loss: 0.04484400153160095\n",
      "Epoch 24087/30000 Training Loss: 0.05049498751759529\n",
      "Epoch 24088/30000 Training Loss: 0.047167159616947174\n",
      "Epoch 24089/30000 Training Loss: 0.046391502022743225\n",
      "Epoch 24090/30000 Training Loss: 0.046480074524879456\n",
      "Epoch 24091/30000 Training Loss: 0.04262850433588028\n",
      "Epoch 24092/30000 Training Loss: 0.045943863689899445\n",
      "Epoch 24093/30000 Training Loss: 0.04987543821334839\n",
      "Epoch 24094/30000 Training Loss: 0.03546471148729324\n",
      "Epoch 24095/30000 Training Loss: 0.05892575532197952\n",
      "Epoch 24096/30000 Training Loss: 0.04212825000286102\n",
      "Epoch 24097/30000 Training Loss: 0.04232218861579895\n",
      "Epoch 24098/30000 Training Loss: 0.046096183359622955\n",
      "Epoch 24099/30000 Training Loss: 0.04053666070103645\n",
      "Epoch 24100/30000 Training Loss: 0.03829525038599968\n",
      "Epoch 24100/30000 Validation Loss: 0.03443100303411484\n",
      "Epoch 24101/30000 Training Loss: 0.059829674661159515\n",
      "Epoch 24102/30000 Training Loss: 0.03166903927922249\n",
      "Epoch 24103/30000 Training Loss: 0.04420212656259537\n",
      "Epoch 24104/30000 Training Loss: 0.048066891729831696\n",
      "Epoch 24105/30000 Training Loss: 0.042482890188694\n",
      "Epoch 24106/30000 Training Loss: 0.051470063626766205\n",
      "Epoch 24107/30000 Training Loss: 0.04342487454414368\n",
      "Epoch 24108/30000 Training Loss: 0.04548123478889465\n",
      "Epoch 24109/30000 Training Loss: 0.04984863102436066\n",
      "Epoch 24110/30000 Training Loss: 0.05495966225862503\n",
      "Epoch 24111/30000 Training Loss: 0.03822527825832367\n",
      "Epoch 24112/30000 Training Loss: 0.04664292559027672\n",
      "Epoch 24113/30000 Training Loss: 0.0521659255027771\n",
      "Epoch 24114/30000 Training Loss: 0.048683926463127136\n",
      "Epoch 24115/30000 Training Loss: 0.033971935510635376\n",
      "Epoch 24116/30000 Training Loss: 0.045712877064943314\n",
      "Epoch 24117/30000 Training Loss: 0.040948204696178436\n",
      "Epoch 24118/30000 Training Loss: 0.050753138959407806\n",
      "Epoch 24119/30000 Training Loss: 0.04207449406385422\n",
      "Epoch 24120/30000 Training Loss: 0.049638036638498306\n",
      "Epoch 24121/30000 Training Loss: 0.04854877293109894\n",
      "Epoch 24122/30000 Training Loss: 0.035075634717941284\n",
      "Epoch 24123/30000 Training Loss: 0.03390580415725708\n",
      "Epoch 24124/30000 Training Loss: 0.03387109562754631\n",
      "Epoch 24125/30000 Training Loss: 0.05031643435359001\n",
      "Epoch 24126/30000 Training Loss: 0.03314565494656563\n",
      "Epoch 24127/30000 Training Loss: 0.0775422602891922\n",
      "Epoch 24128/30000 Training Loss: 0.0445592999458313\n",
      "Epoch 24129/30000 Training Loss: 0.042714815586805344\n",
      "Epoch 24130/30000 Training Loss: 0.039704252034425735\n",
      "Epoch 24131/30000 Training Loss: 0.04162169247865677\n",
      "Epoch 24132/30000 Training Loss: 0.043755024671554565\n",
      "Epoch 24133/30000 Training Loss: 0.04160241782665253\n",
      "Epoch 24134/30000 Training Loss: 0.03771127387881279\n",
      "Epoch 24135/30000 Training Loss: 0.04166215658187866\n",
      "Epoch 24136/30000 Training Loss: 0.04404508322477341\n",
      "Epoch 24137/30000 Training Loss: 0.04305630177259445\n",
      "Epoch 24138/30000 Training Loss: 0.04519638419151306\n",
      "Epoch 24139/30000 Training Loss: 0.04296790063381195\n",
      "Epoch 24140/30000 Training Loss: 0.042430415749549866\n",
      "Epoch 24141/30000 Training Loss: 0.03715337812900543\n",
      "Epoch 24142/30000 Training Loss: 0.02694653533399105\n",
      "Epoch 24143/30000 Training Loss: 0.04788672551512718\n",
      "Epoch 24144/30000 Training Loss: 0.046785056591033936\n",
      "Epoch 24145/30000 Training Loss: 0.05615587159991264\n",
      "Epoch 24146/30000 Training Loss: 0.033361125737428665\n",
      "Epoch 24147/30000 Training Loss: 0.036877598613500595\n",
      "Epoch 24148/30000 Training Loss: 0.03289724886417389\n",
      "Epoch 24149/30000 Training Loss: 0.05160786584019661\n",
      "Epoch 24150/30000 Training Loss: 0.03067643940448761\n",
      "Epoch 24151/30000 Training Loss: 0.04573019593954086\n",
      "Epoch 24152/30000 Training Loss: 0.05042457580566406\n",
      "Epoch 24153/30000 Training Loss: 0.04673495888710022\n",
      "Epoch 24154/30000 Training Loss: 0.04403321072459221\n",
      "Epoch 24155/30000 Training Loss: 0.04497295990586281\n",
      "Epoch 24156/30000 Training Loss: 0.044274382293224335\n",
      "Epoch 24157/30000 Training Loss: 0.030824828892946243\n",
      "Epoch 24158/30000 Training Loss: 0.041657913476228714\n",
      "Epoch 24159/30000 Training Loss: 0.049414344131946564\n",
      "Epoch 24160/30000 Training Loss: 0.05240646004676819\n",
      "Epoch 24161/30000 Training Loss: 0.043307483196258545\n",
      "Epoch 24162/30000 Training Loss: 0.04331138730049133\n",
      "Epoch 24163/30000 Training Loss: 0.05118208006024361\n",
      "Epoch 24164/30000 Training Loss: 0.03298787772655487\n",
      "Epoch 24165/30000 Training Loss: 0.05359916388988495\n",
      "Epoch 24166/30000 Training Loss: 0.03341252729296684\n",
      "Epoch 24167/30000 Training Loss: 0.04207366704940796\n",
      "Epoch 24168/30000 Training Loss: 0.04560692608356476\n",
      "Epoch 24169/30000 Training Loss: 0.04461880028247833\n",
      "Epoch 24170/30000 Training Loss: 0.04612172394990921\n",
      "Epoch 24171/30000 Training Loss: 0.04176625236868858\n",
      "Epoch 24172/30000 Training Loss: 0.04061627388000488\n",
      "Epoch 24173/30000 Training Loss: 0.04075038805603981\n",
      "Epoch 24174/30000 Training Loss: 0.03390880301594734\n",
      "Epoch 24175/30000 Training Loss: 0.03546106442809105\n",
      "Epoch 24176/30000 Training Loss: 0.04168759286403656\n",
      "Epoch 24177/30000 Training Loss: 0.056100863963365555\n",
      "Epoch 24178/30000 Training Loss: 0.04169239103794098\n",
      "Epoch 24179/30000 Training Loss: 0.04256273806095123\n",
      "Epoch 24180/30000 Training Loss: 0.05509957671165466\n",
      "Epoch 24181/30000 Training Loss: 0.038366712629795074\n",
      "Epoch 24182/30000 Training Loss: 0.05355951189994812\n",
      "Epoch 24183/30000 Training Loss: 0.049126021564006805\n",
      "Epoch 24184/30000 Training Loss: 0.040771450847387314\n",
      "Epoch 24185/30000 Training Loss: 0.040971480309963226\n",
      "Epoch 24186/30000 Training Loss: 0.05481495335698128\n",
      "Epoch 24187/30000 Training Loss: 0.04708792269229889\n",
      "Epoch 24188/30000 Training Loss: 0.05534394830465317\n",
      "Epoch 24189/30000 Training Loss: 0.05294349044561386\n",
      "Epoch 24190/30000 Training Loss: 0.045799896121025085\n",
      "Epoch 24191/30000 Training Loss: 0.03913030028343201\n",
      "Epoch 24192/30000 Training Loss: 0.036163702607154846\n",
      "Epoch 24193/30000 Training Loss: 0.03191262483596802\n",
      "Epoch 24194/30000 Training Loss: 0.04041598364710808\n",
      "Epoch 24195/30000 Training Loss: 0.03832407668232918\n",
      "Epoch 24196/30000 Training Loss: 0.030262675136327744\n",
      "Epoch 24197/30000 Training Loss: 0.050810445100069046\n",
      "Epoch 24198/30000 Training Loss: 0.03517923504114151\n",
      "Epoch 24199/30000 Training Loss: 0.03192375600337982\n",
      "Epoch 24200/30000 Training Loss: 0.04492993652820587\n",
      "Epoch 24200/30000 Validation Loss: 0.06136416643857956\n",
      "Epoch 24201/30000 Training Loss: 0.044335588812828064\n",
      "Epoch 24202/30000 Training Loss: 0.036988016217947006\n",
      "Epoch 24203/30000 Training Loss: 0.044660475105047226\n",
      "Epoch 24204/30000 Training Loss: 0.035122100263834\n",
      "Epoch 24205/30000 Training Loss: 0.040329448878765106\n",
      "Epoch 24206/30000 Training Loss: 0.05549122393131256\n",
      "Epoch 24207/30000 Training Loss: 0.050184216350317\n",
      "Epoch 24208/30000 Training Loss: 0.0530383475124836\n",
      "Epoch 24209/30000 Training Loss: 0.037767358124256134\n",
      "Epoch 24210/30000 Training Loss: 0.03812151402235031\n",
      "Epoch 24211/30000 Training Loss: 0.05268675833940506\n",
      "Epoch 24212/30000 Training Loss: 0.06396428495645523\n",
      "Epoch 24213/30000 Training Loss: 0.04099106416106224\n",
      "Epoch 24214/30000 Training Loss: 0.04867834597826004\n",
      "Epoch 24215/30000 Training Loss: 0.029544875025749207\n",
      "Epoch 24216/30000 Training Loss: 0.04796528443694115\n",
      "Epoch 24217/30000 Training Loss: 0.034521739929914474\n",
      "Epoch 24218/30000 Training Loss: 0.04389680176973343\n",
      "Epoch 24219/30000 Training Loss: 0.033752523362636566\n",
      "Epoch 24220/30000 Training Loss: 0.04278076812624931\n",
      "Epoch 24221/30000 Training Loss: 0.04328648000955582\n",
      "Epoch 24222/30000 Training Loss: 0.03854987770318985\n",
      "Epoch 24223/30000 Training Loss: 0.03610982745885849\n",
      "Epoch 24224/30000 Training Loss: 0.028487958014011383\n",
      "Epoch 24225/30000 Training Loss: 0.03230215609073639\n",
      "Epoch 24226/30000 Training Loss: 0.03302869573235512\n",
      "Epoch 24227/30000 Training Loss: 0.031645145267248154\n",
      "Epoch 24228/30000 Training Loss: 0.04014737904071808\n",
      "Epoch 24229/30000 Training Loss: 0.039255157113075256\n",
      "Epoch 24230/30000 Training Loss: 0.05810587853193283\n",
      "Epoch 24231/30000 Training Loss: 0.041206374764442444\n",
      "Epoch 24232/30000 Training Loss: 0.05215426906943321\n",
      "Epoch 24233/30000 Training Loss: 0.037935540080070496\n",
      "Epoch 24234/30000 Training Loss: 0.053847786039114\n",
      "Epoch 24235/30000 Training Loss: 0.04972723498940468\n",
      "Epoch 24236/30000 Training Loss: 0.044889502227306366\n",
      "Epoch 24237/30000 Training Loss: 0.03352823853492737\n",
      "Epoch 24238/30000 Training Loss: 0.04749750345945358\n",
      "Epoch 24239/30000 Training Loss: 0.042787302285432816\n",
      "Epoch 24240/30000 Training Loss: 0.03922551870346069\n",
      "Epoch 24241/30000 Training Loss: 0.04665999114513397\n",
      "Epoch 24242/30000 Training Loss: 0.04588155820965767\n",
      "Epoch 24243/30000 Training Loss: 0.03266710788011551\n",
      "Epoch 24244/30000 Training Loss: 0.05323366820812225\n",
      "Epoch 24245/30000 Training Loss: 0.048115335404872894\n",
      "Epoch 24246/30000 Training Loss: 0.03524763882160187\n",
      "Epoch 24247/30000 Training Loss: 0.04137198626995087\n",
      "Epoch 24248/30000 Training Loss: 0.06341314315795898\n",
      "Epoch 24249/30000 Training Loss: 0.045320942997932434\n",
      "Epoch 24250/30000 Training Loss: 0.05026864632964134\n",
      "Epoch 24251/30000 Training Loss: 0.02599523961544037\n",
      "Epoch 24252/30000 Training Loss: 0.0344180203974247\n",
      "Epoch 24253/30000 Training Loss: 0.04377836361527443\n",
      "Epoch 24254/30000 Training Loss: 0.044012900441884995\n",
      "Epoch 24255/30000 Training Loss: 0.040970444679260254\n",
      "Epoch 24256/30000 Training Loss: 0.048169899731874466\n",
      "Epoch 24257/30000 Training Loss: 0.032091036438941956\n",
      "Epoch 24258/30000 Training Loss: 0.033545300364494324\n",
      "Epoch 24259/30000 Training Loss: 0.03819352015852928\n",
      "Epoch 24260/30000 Training Loss: 0.046983689069747925\n",
      "Epoch 24261/30000 Training Loss: 0.04364875704050064\n",
      "Epoch 24262/30000 Training Loss: 0.0340903103351593\n",
      "Epoch 24263/30000 Training Loss: 0.0481504388153553\n",
      "Epoch 24264/30000 Training Loss: 0.05838377773761749\n",
      "Epoch 24265/30000 Training Loss: 0.03165186196565628\n",
      "Epoch 24266/30000 Training Loss: 0.04342397674918175\n",
      "Epoch 24267/30000 Training Loss: 0.054031193256378174\n",
      "Epoch 24268/30000 Training Loss: 0.04999655485153198\n",
      "Epoch 24269/30000 Training Loss: 0.048807233572006226\n",
      "Epoch 24270/30000 Training Loss: 0.054697535932064056\n",
      "Epoch 24271/30000 Training Loss: 0.03920905292034149\n",
      "Epoch 24272/30000 Training Loss: 0.043777547776699066\n",
      "Epoch 24273/30000 Training Loss: 0.04219387099146843\n",
      "Epoch 24274/30000 Training Loss: 0.040771663188934326\n",
      "Epoch 24275/30000 Training Loss: 0.03614514321088791\n",
      "Epoch 24276/30000 Training Loss: 0.030984502285718918\n",
      "Epoch 24277/30000 Training Loss: 0.043564558029174805\n",
      "Epoch 24278/30000 Training Loss: 0.04194938391447067\n",
      "Epoch 24279/30000 Training Loss: 0.047095030546188354\n",
      "Epoch 24280/30000 Training Loss: 0.04341939091682434\n",
      "Epoch 24281/30000 Training Loss: 0.03930911421775818\n",
      "Epoch 24282/30000 Training Loss: 0.048752088099718094\n",
      "Epoch 24283/30000 Training Loss: 0.05685526505112648\n",
      "Epoch 24284/30000 Training Loss: 0.036101680248975754\n",
      "Epoch 24285/30000 Training Loss: 0.04396742954850197\n",
      "Epoch 24286/30000 Training Loss: 0.04105480760335922\n",
      "Epoch 24287/30000 Training Loss: 0.040593910962343216\n",
      "Epoch 24288/30000 Training Loss: 0.03801727294921875\n",
      "Epoch 24289/30000 Training Loss: 0.045569028705358505\n",
      "Epoch 24290/30000 Training Loss: 0.04281851649284363\n",
      "Epoch 24291/30000 Training Loss: 0.03744516894221306\n",
      "Epoch 24292/30000 Training Loss: 0.04306668043136597\n",
      "Epoch 24293/30000 Training Loss: 0.06539423763751984\n",
      "Epoch 24294/30000 Training Loss: 0.04000745713710785\n",
      "Epoch 24295/30000 Training Loss: 0.033263228833675385\n",
      "Epoch 24296/30000 Training Loss: 0.03797353431582451\n",
      "Epoch 24297/30000 Training Loss: 0.0322197824716568\n",
      "Epoch 24298/30000 Training Loss: 0.03104245662689209\n",
      "Epoch 24299/30000 Training Loss: 0.04050728306174278\n",
      "Epoch 24300/30000 Training Loss: 0.04011683166027069\n",
      "Epoch 24300/30000 Validation Loss: 0.036806266754865646\n",
      "Epoch 24301/30000 Training Loss: 0.04809456691145897\n",
      "Epoch 24302/30000 Training Loss: 0.03361329063773155\n",
      "Epoch 24303/30000 Training Loss: 0.053137894719839096\n",
      "Epoch 24304/30000 Training Loss: 0.049970753490924835\n",
      "Epoch 24305/30000 Training Loss: 0.031067460775375366\n",
      "Epoch 24306/30000 Training Loss: 0.03214719891548157\n",
      "Epoch 24307/30000 Training Loss: 0.031120993196964264\n",
      "Epoch 24308/30000 Training Loss: 0.06274591386318207\n",
      "Epoch 24309/30000 Training Loss: 0.03395506739616394\n",
      "Epoch 24310/30000 Training Loss: 0.0403236523270607\n",
      "Epoch 24311/30000 Training Loss: 0.043209921568632126\n",
      "Epoch 24312/30000 Training Loss: 0.04775531589984894\n",
      "Epoch 24313/30000 Training Loss: 0.0368097648024559\n",
      "Epoch 24314/30000 Training Loss: 0.03557569161057472\n",
      "Epoch 24315/30000 Training Loss: 0.04136381298303604\n",
      "Epoch 24316/30000 Training Loss: 0.03427427262067795\n",
      "Epoch 24317/30000 Training Loss: 0.04047461599111557\n",
      "Epoch 24318/30000 Training Loss: 0.03902962803840637\n",
      "Epoch 24319/30000 Training Loss: 0.04338880255818367\n",
      "Epoch 24320/30000 Training Loss: 0.048701610416173935\n",
      "Epoch 24321/30000 Training Loss: 0.04671677201986313\n",
      "Epoch 24322/30000 Training Loss: 0.024461261928081512\n",
      "Epoch 24323/30000 Training Loss: 0.03949581831693649\n",
      "Epoch 24324/30000 Training Loss: 0.04545776546001434\n",
      "Epoch 24325/30000 Training Loss: 0.04325941950082779\n",
      "Epoch 24326/30000 Training Loss: 0.0454217866063118\n",
      "Epoch 24327/30000 Training Loss: 0.04886234551668167\n",
      "Epoch 24328/30000 Training Loss: 0.02862178534269333\n",
      "Epoch 24329/30000 Training Loss: 0.04813405126333237\n",
      "Epoch 24330/30000 Training Loss: 0.04199064522981644\n",
      "Epoch 24331/30000 Training Loss: 0.03640562295913696\n",
      "Epoch 24332/30000 Training Loss: 0.05836586654186249\n",
      "Epoch 24333/30000 Training Loss: 0.04878741502761841\n",
      "Epoch 24334/30000 Training Loss: 0.03961462527513504\n",
      "Epoch 24335/30000 Training Loss: 0.04417579993605614\n",
      "Epoch 24336/30000 Training Loss: 0.03759462758898735\n",
      "Epoch 24337/30000 Training Loss: 0.03054683841764927\n",
      "Epoch 24338/30000 Training Loss: 0.05161700397729874\n",
      "Epoch 24339/30000 Training Loss: 0.04437565058469772\n",
      "Epoch 24340/30000 Training Loss: 0.043438997119665146\n",
      "Epoch 24341/30000 Training Loss: 0.05084174498915672\n",
      "Epoch 24342/30000 Training Loss: 0.04583529382944107\n",
      "Epoch 24343/30000 Training Loss: 0.042879752814769745\n",
      "Epoch 24344/30000 Training Loss: 0.05075003206729889\n",
      "Epoch 24345/30000 Training Loss: 0.04101908579468727\n",
      "Epoch 24346/30000 Training Loss: 0.04278525337576866\n",
      "Epoch 24347/30000 Training Loss: 0.03638280928134918\n",
      "Epoch 24348/30000 Training Loss: 0.048364609479904175\n",
      "Epoch 24349/30000 Training Loss: 0.039806924760341644\n",
      "Epoch 24350/30000 Training Loss: 0.04268428683280945\n",
      "Epoch 24351/30000 Training Loss: 0.05221909284591675\n",
      "Epoch 24352/30000 Training Loss: 0.04890613257884979\n",
      "Epoch 24353/30000 Training Loss: 0.03798606991767883\n",
      "Epoch 24354/30000 Training Loss: 0.041745301336050034\n",
      "Epoch 24355/30000 Training Loss: 0.03827185183763504\n",
      "Epoch 24356/30000 Training Loss: 0.037908315658569336\n",
      "Epoch 24357/30000 Training Loss: 0.03691020607948303\n",
      "Epoch 24358/30000 Training Loss: 0.04735935479402542\n",
      "Epoch 24359/30000 Training Loss: 0.04201141744852066\n",
      "Epoch 24360/30000 Training Loss: 0.046202339231967926\n",
      "Epoch 24361/30000 Training Loss: 0.03878229483962059\n",
      "Epoch 24362/30000 Training Loss: 0.0400361642241478\n",
      "Epoch 24363/30000 Training Loss: 0.04393475502729416\n",
      "Epoch 24364/30000 Training Loss: 0.038200266659259796\n",
      "Epoch 24365/30000 Training Loss: 0.035958513617515564\n",
      "Epoch 24366/30000 Training Loss: 0.04224727675318718\n",
      "Epoch 24367/30000 Training Loss: 0.03617604449391365\n",
      "Epoch 24368/30000 Training Loss: 0.04062981158494949\n",
      "Epoch 24369/30000 Training Loss: 0.037049394100904465\n",
      "Epoch 24370/30000 Training Loss: 0.031344011425971985\n",
      "Epoch 24371/30000 Training Loss: 0.037997402250766754\n",
      "Epoch 24372/30000 Training Loss: 0.049150411039590836\n",
      "Epoch 24373/30000 Training Loss: 0.0387391597032547\n",
      "Epoch 24374/30000 Training Loss: 0.03896479681134224\n",
      "Epoch 24375/30000 Training Loss: 0.031715184450149536\n",
      "Epoch 24376/30000 Training Loss: 0.04095713049173355\n",
      "Epoch 24377/30000 Training Loss: 0.04798850789666176\n",
      "Epoch 24378/30000 Training Loss: 0.04186031222343445\n",
      "Epoch 24379/30000 Training Loss: 0.055434904992580414\n",
      "Epoch 24380/30000 Training Loss: 0.04224837198853493\n",
      "Epoch 24381/30000 Training Loss: 0.03439117968082428\n",
      "Epoch 24382/30000 Training Loss: 0.04507164657115936\n",
      "Epoch 24383/30000 Training Loss: 0.038610029965639114\n",
      "Epoch 24384/30000 Training Loss: 0.030378859490156174\n",
      "Epoch 24385/30000 Training Loss: 0.06710685789585114\n",
      "Epoch 24386/30000 Training Loss: 0.03480393812060356\n",
      "Epoch 24387/30000 Training Loss: 0.028602585196495056\n",
      "Epoch 24388/30000 Training Loss: 0.036103278398513794\n",
      "Epoch 24389/30000 Training Loss: 0.03992864862084389\n",
      "Epoch 24390/30000 Training Loss: 0.04733949899673462\n",
      "Epoch 24391/30000 Training Loss: 0.04116144776344299\n",
      "Epoch 24392/30000 Training Loss: 0.04904371500015259\n",
      "Epoch 24393/30000 Training Loss: 0.03512640297412872\n",
      "Epoch 24394/30000 Training Loss: 0.03162701427936554\n",
      "Epoch 24395/30000 Training Loss: 0.03844461217522621\n",
      "Epoch 24396/30000 Training Loss: 0.04319130629301071\n",
      "Epoch 24397/30000 Training Loss: 0.036897554993629456\n",
      "Epoch 24398/30000 Training Loss: 0.04545293748378754\n",
      "Epoch 24399/30000 Training Loss: 0.0379815548658371\n",
      "Epoch 24400/30000 Training Loss: 0.042518578469753265\n",
      "Epoch 24400/30000 Validation Loss: 0.04223506152629852\n",
      "Epoch 24401/30000 Training Loss: 0.05071314051747322\n",
      "Epoch 24402/30000 Training Loss: 0.044462502002716064\n",
      "Epoch 24403/30000 Training Loss: 0.04103315249085426\n",
      "Epoch 24404/30000 Training Loss: 0.04888654500246048\n",
      "Epoch 24405/30000 Training Loss: 0.05664335563778877\n",
      "Epoch 24406/30000 Training Loss: 0.04615332931280136\n",
      "Epoch 24407/30000 Training Loss: 0.03541209176182747\n",
      "Epoch 24408/30000 Training Loss: 0.036803752183914185\n",
      "Epoch 24409/30000 Training Loss: 0.0476667582988739\n",
      "Epoch 24410/30000 Training Loss: 0.037800874561071396\n",
      "Epoch 24411/30000 Training Loss: 0.040232665836811066\n",
      "Epoch 24412/30000 Training Loss: 0.049421582370996475\n",
      "Epoch 24413/30000 Training Loss: 0.04163440316915512\n",
      "Epoch 24414/30000 Training Loss: 0.03493526577949524\n",
      "Epoch 24415/30000 Training Loss: 0.04891647398471832\n",
      "Epoch 24416/30000 Training Loss: 0.04344000667333603\n",
      "Epoch 24417/30000 Training Loss: 0.031675126403570175\n",
      "Epoch 24418/30000 Training Loss: 0.03088327869772911\n",
      "Epoch 24419/30000 Training Loss: 0.0543980598449707\n",
      "Epoch 24420/30000 Training Loss: 0.04580342397093773\n",
      "Epoch 24421/30000 Training Loss: 0.049460288137197495\n",
      "Epoch 24422/30000 Training Loss: 0.04951903223991394\n",
      "Epoch 24423/30000 Training Loss: 0.030694983899593353\n",
      "Epoch 24424/30000 Training Loss: 0.03989303112030029\n",
      "Epoch 24425/30000 Training Loss: 0.05304110795259476\n",
      "Epoch 24426/30000 Training Loss: 0.04194773733615875\n",
      "Epoch 24427/30000 Training Loss: 0.04177321121096611\n",
      "Epoch 24428/30000 Training Loss: 0.044752828776836395\n",
      "Epoch 24429/30000 Training Loss: 0.04682300239801407\n",
      "Epoch 24430/30000 Training Loss: 0.03394026309251785\n",
      "Epoch 24431/30000 Training Loss: 0.05278809368610382\n",
      "Epoch 24432/30000 Training Loss: 0.04477415233850479\n",
      "Epoch 24433/30000 Training Loss: 0.05016471445560455\n",
      "Epoch 24434/30000 Training Loss: 0.047314442694187164\n",
      "Epoch 24435/30000 Training Loss: 0.056272413581609726\n",
      "Epoch 24436/30000 Training Loss: 0.04991307109594345\n",
      "Epoch 24437/30000 Training Loss: 0.06421652436256409\n",
      "Epoch 24438/30000 Training Loss: 0.03691389411687851\n",
      "Epoch 24439/30000 Training Loss: 0.04484652355313301\n",
      "Epoch 24440/30000 Training Loss: 0.03225895017385483\n",
      "Epoch 24441/30000 Training Loss: 0.050813838839530945\n",
      "Epoch 24442/30000 Training Loss: 0.06051526218652725\n",
      "Epoch 24443/30000 Training Loss: 0.04206211119890213\n",
      "Epoch 24444/30000 Training Loss: 0.0618896558880806\n",
      "Epoch 24445/30000 Training Loss: 0.03777362406253815\n",
      "Epoch 24446/30000 Training Loss: 0.05563971772789955\n",
      "Epoch 24447/30000 Training Loss: 0.038571350276470184\n",
      "Epoch 24448/30000 Training Loss: 0.04079115390777588\n",
      "Epoch 24449/30000 Training Loss: 0.038708366453647614\n",
      "Epoch 24450/30000 Training Loss: 0.04254891723394394\n",
      "Epoch 24451/30000 Training Loss: 0.038295961916446686\n",
      "Epoch 24452/30000 Training Loss: 0.03788629174232483\n",
      "Epoch 24453/30000 Training Loss: 0.0510297566652298\n",
      "Epoch 24454/30000 Training Loss: 0.038270413875579834\n",
      "Epoch 24455/30000 Training Loss: 0.040530163794755936\n",
      "Epoch 24456/30000 Training Loss: 0.04925653338432312\n",
      "Epoch 24457/30000 Training Loss: 0.04933885484933853\n",
      "Epoch 24458/30000 Training Loss: 0.04792439192533493\n",
      "Epoch 24459/30000 Training Loss: 0.039984121918678284\n",
      "Epoch 24460/30000 Training Loss: 0.05495930835604668\n",
      "Epoch 24461/30000 Training Loss: 0.03198710456490517\n",
      "Epoch 24462/30000 Training Loss: 0.036763809621334076\n",
      "Epoch 24463/30000 Training Loss: 0.04590525105595589\n",
      "Epoch 24464/30000 Training Loss: 0.035157352685928345\n",
      "Epoch 24465/30000 Training Loss: 0.044700175523757935\n",
      "Epoch 24466/30000 Training Loss: 0.043433718383312225\n",
      "Epoch 24467/30000 Training Loss: 0.04952294006943703\n",
      "Epoch 24468/30000 Training Loss: 0.052492257207632065\n",
      "Epoch 24469/30000 Training Loss: 0.045318227261304855\n",
      "Epoch 24470/30000 Training Loss: 0.04912322014570236\n",
      "Epoch 24471/30000 Training Loss: 0.046579137444496155\n",
      "Epoch 24472/30000 Training Loss: 0.03453022241592407\n",
      "Epoch 24473/30000 Training Loss: 0.0365353599190712\n",
      "Epoch 24474/30000 Training Loss: 0.039444345980882645\n",
      "Epoch 24475/30000 Training Loss: 0.03577746823430061\n",
      "Epoch 24476/30000 Training Loss: 0.045401137322187424\n",
      "Epoch 24477/30000 Training Loss: 0.03892351686954498\n",
      "Epoch 24478/30000 Training Loss: 0.04125423729419708\n",
      "Epoch 24479/30000 Training Loss: 0.026545844972133636\n",
      "Epoch 24480/30000 Training Loss: 0.050209544599056244\n",
      "Epoch 24481/30000 Training Loss: 0.04602241888642311\n",
      "Epoch 24482/30000 Training Loss: 0.03600893169641495\n",
      "Epoch 24483/30000 Training Loss: 0.0721016600728035\n",
      "Epoch 24484/30000 Training Loss: 0.06486635655164719\n",
      "Epoch 24485/30000 Training Loss: 0.03783275932073593\n",
      "Epoch 24486/30000 Training Loss: 0.02759065292775631\n",
      "Epoch 24487/30000 Training Loss: 0.040565431118011475\n",
      "Epoch 24488/30000 Training Loss: 0.043605804443359375\n",
      "Epoch 24489/30000 Training Loss: 0.05742906779050827\n",
      "Epoch 24490/30000 Training Loss: 0.039085522294044495\n",
      "Epoch 24491/30000 Training Loss: 0.04227518290281296\n",
      "Epoch 24492/30000 Training Loss: 0.040379852056503296\n",
      "Epoch 24493/30000 Training Loss: 0.03831316530704498\n",
      "Epoch 24494/30000 Training Loss: 0.051299579441547394\n",
      "Epoch 24495/30000 Training Loss: 0.046264562755823135\n",
      "Epoch 24496/30000 Training Loss: 0.05207937955856323\n",
      "Epoch 24497/30000 Training Loss: 0.05281864106655121\n",
      "Epoch 24498/30000 Training Loss: 0.03797738254070282\n",
      "Epoch 24499/30000 Training Loss: 0.03909580782055855\n",
      "Epoch 24500/30000 Training Loss: 0.05104533210396767\n",
      "Epoch 24500/30000 Validation Loss: 0.03244267404079437\n",
      "Epoch 24501/30000 Training Loss: 0.05804247409105301\n",
      "Epoch 24502/30000 Training Loss: 0.036424484103918076\n",
      "Epoch 24503/30000 Training Loss: 0.036492519080638885\n",
      "Epoch 24504/30000 Training Loss: 0.047109514474868774\n",
      "Epoch 24505/30000 Training Loss: 0.034937143325805664\n",
      "Epoch 24506/30000 Training Loss: 0.05073699355125427\n",
      "Epoch 24507/30000 Training Loss: 0.03711722046136856\n",
      "Epoch 24508/30000 Training Loss: 0.04022703319787979\n",
      "Epoch 24509/30000 Training Loss: 0.03845222294330597\n",
      "Epoch 24510/30000 Training Loss: 0.04281967133283615\n",
      "Epoch 24511/30000 Training Loss: 0.032312944531440735\n",
      "Epoch 24512/30000 Training Loss: 0.03630244731903076\n",
      "Epoch 24513/30000 Training Loss: 0.02727527730166912\n",
      "Epoch 24514/30000 Training Loss: 0.040738627314567566\n",
      "Epoch 24515/30000 Training Loss: 0.0424809455871582\n",
      "Epoch 24516/30000 Training Loss: 0.05417788773775101\n",
      "Epoch 24517/30000 Training Loss: 0.0408233217895031\n",
      "Epoch 24518/30000 Training Loss: 0.03710218891501427\n",
      "Epoch 24519/30000 Training Loss: 0.05576267093420029\n",
      "Epoch 24520/30000 Training Loss: 0.026440147310495377\n",
      "Epoch 24521/30000 Training Loss: 0.03630106523633003\n",
      "Epoch 24522/30000 Training Loss: 0.0433492548763752\n",
      "Epoch 24523/30000 Training Loss: 0.037523381412029266\n",
      "Epoch 24524/30000 Training Loss: 0.03700819984078407\n",
      "Epoch 24525/30000 Training Loss: 0.0590120330452919\n",
      "Epoch 24526/30000 Training Loss: 0.04612601175904274\n",
      "Epoch 24527/30000 Training Loss: 0.04598809406161308\n",
      "Epoch 24528/30000 Training Loss: 0.050866685807704926\n",
      "Epoch 24529/30000 Training Loss: 0.044115833938121796\n",
      "Epoch 24530/30000 Training Loss: 0.04280724376440048\n",
      "Epoch 24531/30000 Training Loss: 0.0390276238322258\n",
      "Epoch 24532/30000 Training Loss: 0.054707057774066925\n",
      "Epoch 24533/30000 Training Loss: 0.038002368062734604\n",
      "Epoch 24534/30000 Training Loss: 0.042483553290367126\n",
      "Epoch 24535/30000 Training Loss: 0.06460053473711014\n",
      "Epoch 24536/30000 Training Loss: 0.03636518865823746\n",
      "Epoch 24537/30000 Training Loss: 0.05117587745189667\n",
      "Epoch 24538/30000 Training Loss: 0.05708393454551697\n",
      "Epoch 24539/30000 Training Loss: 0.04662320390343666\n",
      "Epoch 24540/30000 Training Loss: 0.05163193494081497\n",
      "Epoch 24541/30000 Training Loss: 0.03397016227245331\n",
      "Epoch 24542/30000 Training Loss: 0.04727034643292427\n",
      "Epoch 24543/30000 Training Loss: 0.04194629192352295\n",
      "Epoch 24544/30000 Training Loss: 0.05893237143754959\n",
      "Epoch 24545/30000 Training Loss: 0.03649105876684189\n",
      "Epoch 24546/30000 Training Loss: 0.04255471006035805\n",
      "Epoch 24547/30000 Training Loss: 0.056376613676548004\n",
      "Epoch 24548/30000 Training Loss: 0.04084482043981552\n",
      "Epoch 24549/30000 Training Loss: 0.05139172822237015\n",
      "Epoch 24550/30000 Training Loss: 0.04172684997320175\n",
      "Epoch 24551/30000 Training Loss: 0.0405837781727314\n",
      "Epoch 24552/30000 Training Loss: 0.04063615947961807\n",
      "Epoch 24553/30000 Training Loss: 0.04842125624418259\n",
      "Epoch 24554/30000 Training Loss: 0.03924824297428131\n",
      "Epoch 24555/30000 Training Loss: 0.04466264322400093\n",
      "Epoch 24556/30000 Training Loss: 0.046224307268857956\n",
      "Epoch 24557/30000 Training Loss: 0.0485260933637619\n",
      "Epoch 24558/30000 Training Loss: 0.052111152559518814\n",
      "Epoch 24559/30000 Training Loss: 0.043926067650318146\n",
      "Epoch 24560/30000 Training Loss: 0.04502681642770767\n",
      "Epoch 24561/30000 Training Loss: 0.042767297476530075\n",
      "Epoch 24562/30000 Training Loss: 0.05469130724668503\n",
      "Epoch 24563/30000 Training Loss: 0.03674525022506714\n",
      "Epoch 24564/30000 Training Loss: 0.04421345889568329\n",
      "Epoch 24565/30000 Training Loss: 0.04529685527086258\n",
      "Epoch 24566/30000 Training Loss: 0.03503451496362686\n",
      "Epoch 24567/30000 Training Loss: 0.04273992031812668\n",
      "Epoch 24568/30000 Training Loss: 0.04703132435679436\n",
      "Epoch 24569/30000 Training Loss: 0.0404718816280365\n",
      "Epoch 24570/30000 Training Loss: 0.04473637416958809\n",
      "Epoch 24571/30000 Training Loss: 0.03971825912594795\n",
      "Epoch 24572/30000 Training Loss: 0.04141177609562874\n",
      "Epoch 24573/30000 Training Loss: 0.028577584773302078\n",
      "Epoch 24574/30000 Training Loss: 0.03897153586149216\n",
      "Epoch 24575/30000 Training Loss: 0.03259002044796944\n",
      "Epoch 24576/30000 Training Loss: 0.03499052673578262\n",
      "Epoch 24577/30000 Training Loss: 0.030394230037927628\n",
      "Epoch 24578/30000 Training Loss: 0.04646293818950653\n",
      "Epoch 24579/30000 Training Loss: 0.04261358827352524\n",
      "Epoch 24580/30000 Training Loss: 0.05495624244213104\n",
      "Epoch 24581/30000 Training Loss: 0.038319021463394165\n",
      "Epoch 24582/30000 Training Loss: 0.03304386883974075\n",
      "Epoch 24583/30000 Training Loss: 0.04785683751106262\n",
      "Epoch 24584/30000 Training Loss: 0.041034549474716187\n",
      "Epoch 24585/30000 Training Loss: 0.03199203312397003\n",
      "Epoch 24586/30000 Training Loss: 0.055785082280635834\n",
      "Epoch 24587/30000 Training Loss: 0.03685939311981201\n",
      "Epoch 24588/30000 Training Loss: 0.03263462334871292\n",
      "Epoch 24589/30000 Training Loss: 0.03335137665271759\n",
      "Epoch 24590/30000 Training Loss: 0.046526677906513214\n",
      "Epoch 24591/30000 Training Loss: 0.04857977107167244\n",
      "Epoch 24592/30000 Training Loss: 0.04438162222504616\n",
      "Epoch 24593/30000 Training Loss: 0.04272008687257767\n",
      "Epoch 24594/30000 Training Loss: 0.04202308505773544\n",
      "Epoch 24595/30000 Training Loss: 0.04158402979373932\n",
      "Epoch 24596/30000 Training Loss: 0.04105690121650696\n",
      "Epoch 24597/30000 Training Loss: 0.03624787554144859\n",
      "Epoch 24598/30000 Training Loss: 0.03830336779356003\n",
      "Epoch 24599/30000 Training Loss: 0.045942261815071106\n",
      "Epoch 24600/30000 Training Loss: 0.05125255510210991\n",
      "Epoch 24600/30000 Validation Loss: 0.04094180464744568\n",
      "Epoch 24601/30000 Training Loss: 0.04367391765117645\n",
      "Epoch 24602/30000 Training Loss: 0.03794363513588905\n",
      "Epoch 24603/30000 Training Loss: 0.0433594211935997\n",
      "Epoch 24604/30000 Training Loss: 0.04199478402733803\n",
      "Epoch 24605/30000 Training Loss: 0.05142894387245178\n",
      "Epoch 24606/30000 Training Loss: 0.046541228890419006\n",
      "Epoch 24607/30000 Training Loss: 0.07166293263435364\n",
      "Epoch 24608/30000 Training Loss: 0.038457125425338745\n",
      "Epoch 24609/30000 Training Loss: 0.03726780042052269\n",
      "Epoch 24610/30000 Training Loss: 0.051316842436790466\n",
      "Epoch 24611/30000 Training Loss: 0.029043614864349365\n",
      "Epoch 24612/30000 Training Loss: 0.03853999823331833\n",
      "Epoch 24613/30000 Training Loss: 0.048494331538677216\n",
      "Epoch 24614/30000 Training Loss: 0.037221796810626984\n",
      "Epoch 24615/30000 Training Loss: 0.0433424636721611\n",
      "Epoch 24616/30000 Training Loss: 0.040596090257167816\n",
      "Epoch 24617/30000 Training Loss: 0.0548781156539917\n",
      "Epoch 24618/30000 Training Loss: 0.03381531313061714\n",
      "Epoch 24619/30000 Training Loss: 0.04763573408126831\n",
      "Epoch 24620/30000 Training Loss: 0.028639864176511765\n",
      "Epoch 24621/30000 Training Loss: 0.04258609935641289\n",
      "Epoch 24622/30000 Training Loss: 0.04037744551897049\n",
      "Epoch 24623/30000 Training Loss: 0.04173732548952103\n",
      "Epoch 24624/30000 Training Loss: 0.045906055718660355\n",
      "Epoch 24625/30000 Training Loss: 0.04540230333805084\n",
      "Epoch 24626/30000 Training Loss: 0.04230869561433792\n",
      "Epoch 24627/30000 Training Loss: 0.05096888169646263\n",
      "Epoch 24628/30000 Training Loss: 0.04504656046628952\n",
      "Epoch 24629/30000 Training Loss: 0.04158055782318115\n",
      "Epoch 24630/30000 Training Loss: 0.040944840759038925\n",
      "Epoch 24631/30000 Training Loss: 0.05052635073661804\n",
      "Epoch 24632/30000 Training Loss: 0.06469561904668808\n",
      "Epoch 24633/30000 Training Loss: 0.03890276700258255\n",
      "Epoch 24634/30000 Training Loss: 0.046612612903118134\n",
      "Epoch 24635/30000 Training Loss: 0.03360673040151596\n",
      "Epoch 24636/30000 Training Loss: 0.04534414783120155\n",
      "Epoch 24637/30000 Training Loss: 0.028069786727428436\n",
      "Epoch 24638/30000 Training Loss: 0.04358712583780289\n",
      "Epoch 24639/30000 Training Loss: 0.04723048210144043\n",
      "Epoch 24640/30000 Training Loss: 0.041219256818294525\n",
      "Epoch 24641/30000 Training Loss: 0.03335978463292122\n",
      "Epoch 24642/30000 Training Loss: 0.038985803723335266\n",
      "Epoch 24643/30000 Training Loss: 0.04596932232379913\n",
      "Epoch 24644/30000 Training Loss: 0.034203190356492996\n",
      "Epoch 24645/30000 Training Loss: 0.03398852422833443\n",
      "Epoch 24646/30000 Training Loss: 0.05115440487861633\n",
      "Epoch 24647/30000 Training Loss: 0.055265672504901886\n",
      "Epoch 24648/30000 Training Loss: 0.04105652868747711\n",
      "Epoch 24649/30000 Training Loss: 0.04912547022104263\n",
      "Epoch 24650/30000 Training Loss: 0.035084404051303864\n",
      "Epoch 24651/30000 Training Loss: 0.04773985221982002\n",
      "Epoch 24652/30000 Training Loss: 0.046578630805015564\n",
      "Epoch 24653/30000 Training Loss: 0.04301750659942627\n",
      "Epoch 24654/30000 Training Loss: 0.037652384489774704\n",
      "Epoch 24655/30000 Training Loss: 0.041987139731645584\n",
      "Epoch 24656/30000 Training Loss: 0.048860207200050354\n",
      "Epoch 24657/30000 Training Loss: 0.035354722291231155\n",
      "Epoch 24658/30000 Training Loss: 0.04035313054919243\n",
      "Epoch 24659/30000 Training Loss: 0.03517499566078186\n",
      "Epoch 24660/30000 Training Loss: 0.04716254398226738\n",
      "Epoch 24661/30000 Training Loss: 0.04534994065761566\n",
      "Epoch 24662/30000 Training Loss: 0.04332384839653969\n",
      "Epoch 24663/30000 Training Loss: 0.03417649492621422\n",
      "Epoch 24664/30000 Training Loss: 0.0375383123755455\n",
      "Epoch 24665/30000 Training Loss: 0.03289514780044556\n",
      "Epoch 24666/30000 Training Loss: 0.05069922283291817\n",
      "Epoch 24667/30000 Training Loss: 0.0443742536008358\n",
      "Epoch 24668/30000 Training Loss: 0.03578672185540199\n",
      "Epoch 24669/30000 Training Loss: 0.04690017178654671\n",
      "Epoch 24670/30000 Training Loss: 0.04855096712708473\n",
      "Epoch 24671/30000 Training Loss: 0.05034355819225311\n",
      "Epoch 24672/30000 Training Loss: 0.032977230846881866\n",
      "Epoch 24673/30000 Training Loss: 0.04296530783176422\n",
      "Epoch 24674/30000 Training Loss: 0.05692098289728165\n",
      "Epoch 24675/30000 Training Loss: 0.04589063301682472\n",
      "Epoch 24676/30000 Training Loss: 0.05585864186286926\n",
      "Epoch 24677/30000 Training Loss: 0.04607498645782471\n",
      "Epoch 24678/30000 Training Loss: 0.04470378905534744\n",
      "Epoch 24679/30000 Training Loss: 0.047477904707193375\n",
      "Epoch 24680/30000 Training Loss: 0.03853502869606018\n",
      "Epoch 24681/30000 Training Loss: 0.039926379919052124\n",
      "Epoch 24682/30000 Training Loss: 0.04044167697429657\n",
      "Epoch 24683/30000 Training Loss: 0.04957142472267151\n",
      "Epoch 24684/30000 Training Loss: 0.027811255306005478\n",
      "Epoch 24685/30000 Training Loss: 0.03132925182580948\n",
      "Epoch 24686/30000 Training Loss: 0.04314347356557846\n",
      "Epoch 24687/30000 Training Loss: 0.03141288086771965\n",
      "Epoch 24688/30000 Training Loss: 0.03474242240190506\n",
      "Epoch 24689/30000 Training Loss: 0.041734691709280014\n",
      "Epoch 24690/30000 Training Loss: 0.04026119410991669\n",
      "Epoch 24691/30000 Training Loss: 0.040517158806324005\n",
      "Epoch 24692/30000 Training Loss: 0.04831960052251816\n",
      "Epoch 24693/30000 Training Loss: 0.02691422402858734\n",
      "Epoch 24694/30000 Training Loss: 0.036194559186697006\n",
      "Epoch 24695/30000 Training Loss: 0.04431040585041046\n",
      "Epoch 24696/30000 Training Loss: 0.05690937489271164\n",
      "Epoch 24697/30000 Training Loss: 0.03601996973156929\n",
      "Epoch 24698/30000 Training Loss: 0.0409066379070282\n",
      "Epoch 24699/30000 Training Loss: 0.045249730348587036\n",
      "Epoch 24700/30000 Training Loss: 0.04609741270542145\n",
      "Epoch 24700/30000 Validation Loss: 0.047849636524915695\n",
      "Epoch 24701/30000 Training Loss: 0.040501855313777924\n",
      "Epoch 24702/30000 Training Loss: 0.04467006027698517\n",
      "Epoch 24703/30000 Training Loss: 0.039300672709941864\n",
      "Epoch 24704/30000 Training Loss: 0.039774924516677856\n",
      "Epoch 24705/30000 Training Loss: 0.04973825067281723\n",
      "Epoch 24706/30000 Training Loss: 0.024192335084080696\n",
      "Epoch 24707/30000 Training Loss: 0.04205012321472168\n",
      "Epoch 24708/30000 Training Loss: 0.04431343823671341\n",
      "Epoch 24709/30000 Training Loss: 0.0478227362036705\n",
      "Epoch 24710/30000 Training Loss: 0.03770507127046585\n",
      "Epoch 24711/30000 Training Loss: 0.044684335589408875\n",
      "Epoch 24712/30000 Training Loss: 0.036136411130428314\n",
      "Epoch 24713/30000 Training Loss: 0.03706927224993706\n",
      "Epoch 24714/30000 Training Loss: 0.028856460005044937\n",
      "Epoch 24715/30000 Training Loss: 0.03929849714040756\n",
      "Epoch 24716/30000 Training Loss: 0.04673932492733002\n",
      "Epoch 24717/30000 Training Loss: 0.035148561000823975\n",
      "Epoch 24718/30000 Training Loss: 0.0322563610970974\n",
      "Epoch 24719/30000 Training Loss: 0.03513207286596298\n",
      "Epoch 24720/30000 Training Loss: 0.041879802942276\n",
      "Epoch 24721/30000 Training Loss: 0.04080872982740402\n",
      "Epoch 24722/30000 Training Loss: 0.04304942488670349\n",
      "Epoch 24723/30000 Training Loss: 0.04047194495797157\n",
      "Epoch 24724/30000 Training Loss: 0.033827342092990875\n",
      "Epoch 24725/30000 Training Loss: 0.04771248996257782\n",
      "Epoch 24726/30000 Training Loss: 0.04117276519536972\n",
      "Epoch 24727/30000 Training Loss: 0.048717938363552094\n",
      "Epoch 24728/30000 Training Loss: 0.04025867581367493\n",
      "Epoch 24729/30000 Training Loss: 0.06123273819684982\n",
      "Epoch 24730/30000 Training Loss: 0.047639038413763046\n",
      "Epoch 24731/30000 Training Loss: 0.03654464706778526\n",
      "Epoch 24732/30000 Training Loss: 0.05052020400762558\n",
      "Epoch 24733/30000 Training Loss: 0.04450564831495285\n",
      "Epoch 24734/30000 Training Loss: 0.03589346632361412\n",
      "Epoch 24735/30000 Training Loss: 0.0371871143579483\n",
      "Epoch 24736/30000 Training Loss: 0.03088253363966942\n",
      "Epoch 24737/30000 Training Loss: 0.04274238273501396\n",
      "Epoch 24738/30000 Training Loss: 0.03926316648721695\n",
      "Epoch 24739/30000 Training Loss: 0.03077380359172821\n",
      "Epoch 24740/30000 Training Loss: 0.0355716347694397\n",
      "Epoch 24741/30000 Training Loss: 0.05733960121870041\n",
      "Epoch 24742/30000 Training Loss: 0.04304821416735649\n",
      "Epoch 24743/30000 Training Loss: 0.04539278894662857\n",
      "Epoch 24744/30000 Training Loss: 0.03991657868027687\n",
      "Epoch 24745/30000 Training Loss: 0.036877311766147614\n",
      "Epoch 24746/30000 Training Loss: 0.041926417499780655\n",
      "Epoch 24747/30000 Training Loss: 0.038677509874105453\n",
      "Epoch 24748/30000 Training Loss: 0.040828634053468704\n",
      "Epoch 24749/30000 Training Loss: 0.04691730812191963\n",
      "Epoch 24750/30000 Training Loss: 0.05572604760527611\n",
      "Epoch 24751/30000 Training Loss: 0.04587915167212486\n",
      "Epoch 24752/30000 Training Loss: 0.045452333986759186\n",
      "Epoch 24753/30000 Training Loss: 0.04143769294023514\n",
      "Epoch 24754/30000 Training Loss: 0.038549721240997314\n",
      "Epoch 24755/30000 Training Loss: 0.05470471456646919\n",
      "Epoch 24756/30000 Training Loss: 0.02876758947968483\n",
      "Epoch 24757/30000 Training Loss: 0.04932036250829697\n",
      "Epoch 24758/30000 Training Loss: 0.03693161904811859\n",
      "Epoch 24759/30000 Training Loss: 0.03427869826555252\n",
      "Epoch 24760/30000 Training Loss: 0.04292113333940506\n",
      "Epoch 24761/30000 Training Loss: 0.04143665358424187\n",
      "Epoch 24762/30000 Training Loss: 0.04416823014616966\n",
      "Epoch 24763/30000 Training Loss: 0.04045884311199188\n",
      "Epoch 24764/30000 Training Loss: 0.039143383502960205\n",
      "Epoch 24765/30000 Training Loss: 0.042871106415987015\n",
      "Epoch 24766/30000 Training Loss: 0.04754266515374184\n",
      "Epoch 24767/30000 Training Loss: 0.04311038926243782\n",
      "Epoch 24768/30000 Training Loss: 0.03260648623108864\n",
      "Epoch 24769/30000 Training Loss: 0.07172530889511108\n",
      "Epoch 24770/30000 Training Loss: 0.04388175159692764\n",
      "Epoch 24771/30000 Training Loss: 0.048697710037231445\n",
      "Epoch 24772/30000 Training Loss: 0.04650908708572388\n",
      "Epoch 24773/30000 Training Loss: 0.06111632287502289\n",
      "Epoch 24774/30000 Training Loss: 0.04924393072724342\n",
      "Epoch 24775/30000 Training Loss: 0.04576299712061882\n",
      "Epoch 24776/30000 Training Loss: 0.04350012168288231\n",
      "Epoch 24777/30000 Training Loss: 0.04405701905488968\n",
      "Epoch 24778/30000 Training Loss: 0.04389127343893051\n",
      "Epoch 24779/30000 Training Loss: 0.047922950237989426\n",
      "Epoch 24780/30000 Training Loss: 0.05079737678170204\n",
      "Epoch 24781/30000 Training Loss: 0.03557077422738075\n",
      "Epoch 24782/30000 Training Loss: 0.04646707698702812\n",
      "Epoch 24783/30000 Training Loss: 0.0442802719771862\n",
      "Epoch 24784/30000 Training Loss: 0.039511241018772125\n",
      "Epoch 24785/30000 Training Loss: 0.046967580914497375\n",
      "Epoch 24786/30000 Training Loss: 0.053373247385025024\n",
      "Epoch 24787/30000 Training Loss: 0.049065105617046356\n",
      "Epoch 24788/30000 Training Loss: 0.047360874712467194\n",
      "Epoch 24789/30000 Training Loss: 0.04730067402124405\n",
      "Epoch 24790/30000 Training Loss: 0.053382374346256256\n",
      "Epoch 24791/30000 Training Loss: 0.040568165481090546\n",
      "Epoch 24792/30000 Training Loss: 0.0482097826898098\n",
      "Epoch 24793/30000 Training Loss: 0.0560079962015152\n",
      "Epoch 24794/30000 Training Loss: 0.041753411293029785\n",
      "Epoch 24795/30000 Training Loss: 0.038372378796339035\n",
      "Epoch 24796/30000 Training Loss: 0.05321236699819565\n",
      "Epoch 24797/30000 Training Loss: 0.033794112503528595\n",
      "Epoch 24798/30000 Training Loss: 0.042997971177101135\n",
      "Epoch 24799/30000 Training Loss: 0.04752480983734131\n",
      "Epoch 24800/30000 Training Loss: 0.039277710020542145\n",
      "Epoch 24800/30000 Validation Loss: 0.04802577942609787\n",
      "Epoch 24801/30000 Training Loss: 0.04637450352311134\n",
      "Epoch 24802/30000 Training Loss: 0.03860415890812874\n",
      "Epoch 24803/30000 Training Loss: 0.055838264524936676\n",
      "Epoch 24804/30000 Training Loss: 0.0360216423869133\n",
      "Epoch 24805/30000 Training Loss: 0.04638981074094772\n",
      "Epoch 24806/30000 Training Loss: 0.05059703439474106\n",
      "Epoch 24807/30000 Training Loss: 0.0365309901535511\n",
      "Epoch 24808/30000 Training Loss: 0.04518858343362808\n",
      "Epoch 24809/30000 Training Loss: 0.044561583548784256\n",
      "Epoch 24810/30000 Training Loss: 0.041476938873529434\n",
      "Epoch 24811/30000 Training Loss: 0.0424029678106308\n",
      "Epoch 24812/30000 Training Loss: 0.04374530538916588\n",
      "Epoch 24813/30000 Training Loss: 0.05832195281982422\n",
      "Epoch 24814/30000 Training Loss: 0.03336116299033165\n",
      "Epoch 24815/30000 Training Loss: 0.055278293788433075\n",
      "Epoch 24816/30000 Training Loss: 0.04118112102150917\n",
      "Epoch 24817/30000 Training Loss: 0.03732609748840332\n",
      "Epoch 24818/30000 Training Loss: 0.03586259111762047\n",
      "Epoch 24819/30000 Training Loss: 0.03720971196889877\n",
      "Epoch 24820/30000 Training Loss: 0.03155957907438278\n",
      "Epoch 24821/30000 Training Loss: 0.03893475979566574\n",
      "Epoch 24822/30000 Training Loss: 0.03995766490697861\n",
      "Epoch 24823/30000 Training Loss: 0.03567209094762802\n",
      "Epoch 24824/30000 Training Loss: 0.0451836958527565\n",
      "Epoch 24825/30000 Training Loss: 0.031013283878564835\n",
      "Epoch 24826/30000 Training Loss: 0.047013647854328156\n",
      "Epoch 24827/30000 Training Loss: 0.040489763021469116\n",
      "Epoch 24828/30000 Training Loss: 0.03985872492194176\n",
      "Epoch 24829/30000 Training Loss: 0.057863749563694\n",
      "Epoch 24830/30000 Training Loss: 0.04113360494375229\n",
      "Epoch 24831/30000 Training Loss: 0.03372199088335037\n",
      "Epoch 24832/30000 Training Loss: 0.04847695678472519\n",
      "Epoch 24833/30000 Training Loss: 0.047747932374477386\n",
      "Epoch 24834/30000 Training Loss: 0.04109489545226097\n",
      "Epoch 24835/30000 Training Loss: 0.05209573358297348\n",
      "Epoch 24836/30000 Training Loss: 0.03505496680736542\n",
      "Epoch 24837/30000 Training Loss: 0.05665375292301178\n",
      "Epoch 24838/30000 Training Loss: 0.0482567623257637\n",
      "Epoch 24839/30000 Training Loss: 0.03627020865678787\n",
      "Epoch 24840/30000 Training Loss: 0.03579863905906677\n",
      "Epoch 24841/30000 Training Loss: 0.043974801898002625\n",
      "Epoch 24842/30000 Training Loss: 0.032429397106170654\n",
      "Epoch 24843/30000 Training Loss: 0.045850206166505814\n",
      "Epoch 24844/30000 Training Loss: 0.047846682369709015\n",
      "Epoch 24845/30000 Training Loss: 0.03342217952013016\n",
      "Epoch 24846/30000 Training Loss: 0.051372162997722626\n",
      "Epoch 24847/30000 Training Loss: 0.04232632368803024\n",
      "Epoch 24848/30000 Training Loss: 0.0450739823281765\n",
      "Epoch 24849/30000 Training Loss: 0.04409431666135788\n",
      "Epoch 24850/30000 Training Loss: 0.046498287469148636\n",
      "Epoch 24851/30000 Training Loss: 0.0415053516626358\n",
      "Epoch 24852/30000 Training Loss: 0.03813710808753967\n",
      "Epoch 24853/30000 Training Loss: 0.047462642192840576\n",
      "Epoch 24854/30000 Training Loss: 0.04912446439266205\n",
      "Epoch 24855/30000 Training Loss: 0.042756058275699615\n",
      "Epoch 24856/30000 Training Loss: 0.05189476162195206\n",
      "Epoch 24857/30000 Training Loss: 0.0338120199739933\n",
      "Epoch 24858/30000 Training Loss: 0.033408235758543015\n",
      "Epoch 24859/30000 Training Loss: 0.04283097758889198\n",
      "Epoch 24860/30000 Training Loss: 0.04393930733203888\n",
      "Epoch 24861/30000 Training Loss: 0.0501871332526207\n",
      "Epoch 24862/30000 Training Loss: 0.02705925516784191\n",
      "Epoch 24863/30000 Training Loss: 0.041196245700120926\n",
      "Epoch 24864/30000 Training Loss: 0.06027274578809738\n",
      "Epoch 24865/30000 Training Loss: 0.03156211972236633\n",
      "Epoch 24866/30000 Training Loss: 0.04820040985941887\n",
      "Epoch 24867/30000 Training Loss: 0.03821864724159241\n",
      "Epoch 24868/30000 Training Loss: 0.040425945073366165\n",
      "Epoch 24869/30000 Training Loss: 0.04603687673807144\n",
      "Epoch 24870/30000 Training Loss: 0.02951575443148613\n",
      "Epoch 24871/30000 Training Loss: 0.03460489213466644\n",
      "Epoch 24872/30000 Training Loss: 0.043522533029317856\n",
      "Epoch 24873/30000 Training Loss: 0.06029567867517471\n",
      "Epoch 24874/30000 Training Loss: 0.045219339430332184\n",
      "Epoch 24875/30000 Training Loss: 0.04143755882978439\n",
      "Epoch 24876/30000 Training Loss: 0.05415939539670944\n",
      "Epoch 24877/30000 Training Loss: 0.03014061599969864\n",
      "Epoch 24878/30000 Training Loss: 0.04317334666848183\n",
      "Epoch 24879/30000 Training Loss: 0.027321169152855873\n",
      "Epoch 24880/30000 Training Loss: 0.043359771370887756\n",
      "Epoch 24881/30000 Training Loss: 0.041733287274837494\n",
      "Epoch 24882/30000 Training Loss: 0.05107109248638153\n",
      "Epoch 24883/30000 Training Loss: 0.0447273775935173\n",
      "Epoch 24884/30000 Training Loss: 0.031026162207126617\n",
      "Epoch 24885/30000 Training Loss: 0.028343703597784042\n",
      "Epoch 24886/30000 Training Loss: 0.0529346838593483\n",
      "Epoch 24887/30000 Training Loss: 0.045407041907310486\n",
      "Epoch 24888/30000 Training Loss: 0.036992914974689484\n",
      "Epoch 24889/30000 Training Loss: 0.04431287199258804\n",
      "Epoch 24890/30000 Training Loss: 0.052912674844264984\n",
      "Epoch 24891/30000 Training Loss: 0.04352041333913803\n",
      "Epoch 24892/30000 Training Loss: 0.03793967142701149\n",
      "Epoch 24893/30000 Training Loss: 0.038603268563747406\n",
      "Epoch 24894/30000 Training Loss: 0.04237537458539009\n",
      "Epoch 24895/30000 Training Loss: 0.05188656225800514\n",
      "Epoch 24896/30000 Training Loss: 0.0469607338309288\n",
      "Epoch 24897/30000 Training Loss: 0.053841736167669296\n",
      "Epoch 24898/30000 Training Loss: 0.031983472406864166\n",
      "Epoch 24899/30000 Training Loss: 0.04579582065343857\n",
      "Epoch 24900/30000 Training Loss: 0.039212070405483246\n",
      "Epoch 24900/30000 Validation Loss: 0.03090774267911911\n",
      "Epoch 24901/30000 Training Loss: 0.04139653220772743\n",
      "Epoch 24902/30000 Training Loss: 0.04538240283727646\n",
      "Epoch 24903/30000 Training Loss: 0.03513812646269798\n",
      "Epoch 24904/30000 Training Loss: 0.03674550727009773\n",
      "Epoch 24905/30000 Training Loss: 0.04622448980808258\n",
      "Epoch 24906/30000 Training Loss: 0.027416469529271126\n",
      "Epoch 24907/30000 Training Loss: 0.03237680345773697\n",
      "Epoch 24908/30000 Training Loss: 0.033890534192323685\n",
      "Epoch 24909/30000 Training Loss: 0.03963073715567589\n",
      "Epoch 24910/30000 Training Loss: 0.05736289173364639\n",
      "Epoch 24911/30000 Training Loss: 0.03864999860525131\n",
      "Epoch 24912/30000 Training Loss: 0.04583878815174103\n",
      "Epoch 24913/30000 Training Loss: 0.04381944611668587\n",
      "Epoch 24914/30000 Training Loss: 0.03872906416654587\n",
      "Epoch 24915/30000 Training Loss: 0.04135637730360031\n",
      "Epoch 24916/30000 Training Loss: 0.04878538101911545\n",
      "Epoch 24917/30000 Training Loss: 0.04374244809150696\n",
      "Epoch 24918/30000 Training Loss: 0.03336453065276146\n",
      "Epoch 24919/30000 Training Loss: 0.05715979263186455\n",
      "Epoch 24920/30000 Training Loss: 0.03189632296562195\n",
      "Epoch 24921/30000 Training Loss: 0.048807114362716675\n",
      "Epoch 24922/30000 Training Loss: 0.0452393963932991\n",
      "Epoch 24923/30000 Training Loss: 0.048301029950380325\n",
      "Epoch 24924/30000 Training Loss: 0.049975231289863586\n",
      "Epoch 24925/30000 Training Loss: 0.047818925231695175\n",
      "Epoch 24926/30000 Training Loss: 0.061807822436094284\n",
      "Epoch 24927/30000 Training Loss: 0.05744936689734459\n",
      "Epoch 24928/30000 Training Loss: 0.04012846574187279\n",
      "Epoch 24929/30000 Training Loss: 0.04645087569952011\n",
      "Epoch 24930/30000 Training Loss: 0.04249902442097664\n",
      "Epoch 24931/30000 Training Loss: 0.057386577129364014\n",
      "Epoch 24932/30000 Training Loss: 0.04859834164381027\n",
      "Epoch 24933/30000 Training Loss: 0.044827546924352646\n",
      "Epoch 24934/30000 Training Loss: 0.0393875315785408\n",
      "Epoch 24935/30000 Training Loss: 0.02849676087498665\n",
      "Epoch 24936/30000 Training Loss: 0.027440384030342102\n",
      "Epoch 24937/30000 Training Loss: 0.04406348988413811\n",
      "Epoch 24938/30000 Training Loss: 0.049039967358112335\n",
      "Epoch 24939/30000 Training Loss: 0.04864911735057831\n",
      "Epoch 24940/30000 Training Loss: 0.03484386205673218\n",
      "Epoch 24941/30000 Training Loss: 0.0393853597342968\n",
      "Epoch 24942/30000 Training Loss: 0.04840738698840141\n",
      "Epoch 24943/30000 Training Loss: 0.05542968213558197\n",
      "Epoch 24944/30000 Training Loss: 0.044829610735177994\n",
      "Epoch 24945/30000 Training Loss: 0.04225866496562958\n",
      "Epoch 24946/30000 Training Loss: 0.02979767508804798\n",
      "Epoch 24947/30000 Training Loss: 0.052263204008340836\n",
      "Epoch 24948/30000 Training Loss: 0.04168786108493805\n",
      "Epoch 24949/30000 Training Loss: 0.037250857800245285\n",
      "Epoch 24950/30000 Training Loss: 0.04430721327662468\n",
      "Epoch 24951/30000 Training Loss: 0.037678107619285583\n",
      "Epoch 24952/30000 Training Loss: 0.043503303080797195\n",
      "Epoch 24953/30000 Training Loss: 0.05331043899059296\n",
      "Epoch 24954/30000 Training Loss: 0.038964249193668365\n",
      "Epoch 24955/30000 Training Loss: 0.05587412416934967\n",
      "Epoch 24956/30000 Training Loss: 0.041080281138420105\n",
      "Epoch 24957/30000 Training Loss: 0.047920890152454376\n",
      "Epoch 24958/30000 Training Loss: 0.03628629818558693\n",
      "Epoch 24959/30000 Training Loss: 0.04235235974192619\n",
      "Epoch 24960/30000 Training Loss: 0.03413264453411102\n",
      "Epoch 24961/30000 Training Loss: 0.05129187926650047\n",
      "Epoch 24962/30000 Training Loss: 0.05963626503944397\n",
      "Epoch 24963/30000 Training Loss: 0.04462728276848793\n",
      "Epoch 24964/30000 Training Loss: 0.058830685913562775\n",
      "Epoch 24965/30000 Training Loss: 0.03193492814898491\n",
      "Epoch 24966/30000 Training Loss: 0.041242554783821106\n",
      "Epoch 24967/30000 Training Loss: 0.034909963607788086\n",
      "Epoch 24968/30000 Training Loss: 0.05716522037982941\n",
      "Epoch 24969/30000 Training Loss: 0.03807670623064041\n",
      "Epoch 24970/30000 Training Loss: 0.04017198085784912\n",
      "Epoch 24971/30000 Training Loss: 0.04158611595630646\n",
      "Epoch 24972/30000 Training Loss: 0.04077565297484398\n",
      "Epoch 24973/30000 Training Loss: 0.044530950486660004\n",
      "Epoch 24974/30000 Training Loss: 0.04860861971974373\n",
      "Epoch 24975/30000 Training Loss: 0.06066622585058212\n",
      "Epoch 24976/30000 Training Loss: 0.03095148876309395\n",
      "Epoch 24977/30000 Training Loss: 0.03888387233018875\n",
      "Epoch 24978/30000 Training Loss: 0.03059387020766735\n",
      "Epoch 24979/30000 Training Loss: 0.03583192825317383\n",
      "Epoch 24980/30000 Training Loss: 0.04496631771326065\n",
      "Epoch 24981/30000 Training Loss: 0.042110975831747055\n",
      "Epoch 24982/30000 Training Loss: 0.05368839576840401\n",
      "Epoch 24983/30000 Training Loss: 0.03799550235271454\n",
      "Epoch 24984/30000 Training Loss: 0.032690804451704025\n",
      "Epoch 24985/30000 Training Loss: 0.04058415815234184\n",
      "Epoch 24986/30000 Training Loss: 0.04487404227256775\n",
      "Epoch 24987/30000 Training Loss: 0.03769870102405548\n",
      "Epoch 24988/30000 Training Loss: 0.044692277908325195\n",
      "Epoch 24989/30000 Training Loss: 0.0327942818403244\n",
      "Epoch 24990/30000 Training Loss: 0.058779116719961166\n",
      "Epoch 24991/30000 Training Loss: 0.048045817762613297\n",
      "Epoch 24992/30000 Training Loss: 0.05179062485694885\n",
      "Epoch 24993/30000 Training Loss: 0.03694616258144379\n",
      "Epoch 24994/30000 Training Loss: 0.04406595230102539\n",
      "Epoch 24995/30000 Training Loss: 0.04556754231452942\n",
      "Epoch 24996/30000 Training Loss: 0.03567761182785034\n",
      "Epoch 24997/30000 Training Loss: 0.042169418185949326\n",
      "Epoch 24998/30000 Training Loss: 0.05082235112786293\n",
      "Epoch 24999/30000 Training Loss: 0.047255028039216995\n",
      "Epoch 25000/30000 Training Loss: 0.04707658290863037\n",
      "Epoch 25000/30000 Validation Loss: 0.038198135793209076\n",
      "Epoch 25001/30000 Training Loss: 0.0434190109372139\n",
      "Epoch 25002/30000 Training Loss: 0.06231658160686493\n",
      "Epoch 25003/30000 Training Loss: 0.0490306131541729\n",
      "Epoch 25004/30000 Training Loss: 0.02964627929031849\n",
      "Epoch 25005/30000 Training Loss: 0.03937063366174698\n",
      "Epoch 25006/30000 Training Loss: 0.04125555232167244\n",
      "Epoch 25007/30000 Training Loss: 0.04983777552843094\n",
      "Epoch 25008/30000 Training Loss: 0.04088686406612396\n",
      "Epoch 25009/30000 Training Loss: 0.05343251675367355\n",
      "Epoch 25010/30000 Training Loss: 0.05029746890068054\n",
      "Epoch 25011/30000 Training Loss: 0.04002947360277176\n",
      "Epoch 25012/30000 Training Loss: 0.04753077030181885\n",
      "Epoch 25013/30000 Training Loss: 0.037244342267513275\n",
      "Epoch 25014/30000 Training Loss: 0.03293447196483612\n",
      "Epoch 25015/30000 Training Loss: 0.0457439050078392\n",
      "Epoch 25016/30000 Training Loss: 0.0415273979306221\n",
      "Epoch 25017/30000 Training Loss: 0.027321267873048782\n",
      "Epoch 25018/30000 Training Loss: 0.047139666974544525\n",
      "Epoch 25019/30000 Training Loss: 0.031048394739627838\n",
      "Epoch 25020/30000 Training Loss: 0.04571269452571869\n",
      "Epoch 25021/30000 Training Loss: 0.03826291114091873\n",
      "Epoch 25022/30000 Training Loss: 0.046062689274549484\n",
      "Epoch 25023/30000 Training Loss: 0.05274735391139984\n",
      "Epoch 25024/30000 Training Loss: 0.0442378893494606\n",
      "Epoch 25025/30000 Training Loss: 0.04252283275127411\n",
      "Epoch 25026/30000 Training Loss: 0.03745131194591522\n",
      "Epoch 25027/30000 Training Loss: 0.028986550867557526\n",
      "Epoch 25028/30000 Training Loss: 0.04609477519989014\n",
      "Epoch 25029/30000 Training Loss: 0.03269088268280029\n",
      "Epoch 25030/30000 Training Loss: 0.04526551440358162\n",
      "Epoch 25031/30000 Training Loss: 0.05265076458454132\n",
      "Epoch 25032/30000 Training Loss: 0.0408790186047554\n",
      "Epoch 25033/30000 Training Loss: 0.048527732491493225\n",
      "Epoch 25034/30000 Training Loss: 0.030250664800405502\n",
      "Epoch 25035/30000 Training Loss: 0.04943773150444031\n",
      "Epoch 25036/30000 Training Loss: 0.038543008267879486\n",
      "Epoch 25037/30000 Training Loss: 0.029673833400011063\n",
      "Epoch 25038/30000 Training Loss: 0.046346936374902725\n",
      "Epoch 25039/30000 Training Loss: 0.03373227268457413\n",
      "Epoch 25040/30000 Training Loss: 0.041778381913900375\n",
      "Epoch 25041/30000 Training Loss: 0.038660936057567596\n",
      "Epoch 25042/30000 Training Loss: 0.04700120538473129\n",
      "Epoch 25043/30000 Training Loss: 0.04778175801038742\n",
      "Epoch 25044/30000 Training Loss: 0.04952199012041092\n",
      "Epoch 25045/30000 Training Loss: 0.04200543835759163\n",
      "Epoch 25046/30000 Training Loss: 0.04333241656422615\n",
      "Epoch 25047/30000 Training Loss: 0.048772960901260376\n",
      "Epoch 25048/30000 Training Loss: 0.041851624846458435\n",
      "Epoch 25049/30000 Training Loss: 0.043411217629909515\n",
      "Epoch 25050/30000 Training Loss: 0.050729312002658844\n",
      "Epoch 25051/30000 Training Loss: 0.04911479353904724\n",
      "Epoch 25052/30000 Training Loss: 0.03808130696415901\n",
      "Epoch 25053/30000 Training Loss: 0.04127879813313484\n",
      "Epoch 25054/30000 Training Loss: 0.03736460581421852\n",
      "Epoch 25055/30000 Training Loss: 0.042258281260728836\n",
      "Epoch 25056/30000 Training Loss: 0.03781133145093918\n",
      "Epoch 25057/30000 Training Loss: 0.04268377646803856\n",
      "Epoch 25058/30000 Training Loss: 0.034184232354164124\n",
      "Epoch 25059/30000 Training Loss: 0.03719190135598183\n",
      "Epoch 25060/30000 Training Loss: 0.058142561465501785\n",
      "Epoch 25061/30000 Training Loss: 0.03723746910691261\n",
      "Epoch 25062/30000 Training Loss: 0.059513822197914124\n",
      "Epoch 25063/30000 Training Loss: 0.04166492074728012\n",
      "Epoch 25064/30000 Training Loss: 0.046657487750053406\n",
      "Epoch 25065/30000 Training Loss: 0.040379978716373444\n",
      "Epoch 25066/30000 Training Loss: 0.03520213067531586\n",
      "Epoch 25067/30000 Training Loss: 0.03858938813209534\n",
      "Epoch 25068/30000 Training Loss: 0.03377050533890724\n",
      "Epoch 25069/30000 Training Loss: 0.05052822455763817\n",
      "Epoch 25070/30000 Training Loss: 0.035953402519226074\n",
      "Epoch 25071/30000 Training Loss: 0.04371500015258789\n",
      "Epoch 25072/30000 Training Loss: 0.039269570261240005\n",
      "Epoch 25073/30000 Training Loss: 0.04472839832305908\n",
      "Epoch 25074/30000 Training Loss: 0.04008155316114426\n",
      "Epoch 25075/30000 Training Loss: 0.03785530477762222\n",
      "Epoch 25076/30000 Training Loss: 0.046185292303562164\n",
      "Epoch 25077/30000 Training Loss: 0.05052776262164116\n",
      "Epoch 25078/30000 Training Loss: 0.03316490352153778\n",
      "Epoch 25079/30000 Training Loss: 0.0528818741440773\n",
      "Epoch 25080/30000 Training Loss: 0.03455186262726784\n",
      "Epoch 25081/30000 Training Loss: 0.04436076059937477\n",
      "Epoch 25082/30000 Training Loss: 0.04720814898610115\n",
      "Epoch 25083/30000 Training Loss: 0.034211281687021255\n",
      "Epoch 25084/30000 Training Loss: 0.04714497923851013\n",
      "Epoch 25085/30000 Training Loss: 0.03554696962237358\n",
      "Epoch 25086/30000 Training Loss: 0.0464835986495018\n",
      "Epoch 25087/30000 Training Loss: 0.04367198050022125\n",
      "Epoch 25088/30000 Training Loss: 0.04329449683427811\n",
      "Epoch 25089/30000 Training Loss: 0.037332914769649506\n",
      "Epoch 25090/30000 Training Loss: 0.04648790508508682\n",
      "Epoch 25091/30000 Training Loss: 0.030565645545721054\n",
      "Epoch 25092/30000 Training Loss: 0.04730765148997307\n",
      "Epoch 25093/30000 Training Loss: 0.04421990364789963\n",
      "Epoch 25094/30000 Training Loss: 0.039077963680028915\n",
      "Epoch 25095/30000 Training Loss: 0.04588469862937927\n",
      "Epoch 25096/30000 Training Loss: 0.03556535765528679\n",
      "Epoch 25097/30000 Training Loss: 0.04316602274775505\n",
      "Epoch 25098/30000 Training Loss: 0.04558674618601799\n",
      "Epoch 25099/30000 Training Loss: 0.056868135929107666\n",
      "Epoch 25100/30000 Training Loss: 0.04426395148038864\n",
      "Epoch 25100/30000 Validation Loss: 0.037450604140758514\n",
      "Epoch 25101/30000 Training Loss: 0.04576300084590912\n",
      "Epoch 25102/30000 Training Loss: 0.03331757336854935\n",
      "Epoch 25103/30000 Training Loss: 0.040862586349248886\n",
      "Epoch 25104/30000 Training Loss: 0.04052460566163063\n",
      "Epoch 25105/30000 Training Loss: 0.04284278303384781\n",
      "Epoch 25106/30000 Training Loss: 0.05386931821703911\n",
      "Epoch 25107/30000 Training Loss: 0.054206278175115585\n",
      "Epoch 25108/30000 Training Loss: 0.043686989694833755\n",
      "Epoch 25109/30000 Training Loss: 0.04297826439142227\n",
      "Epoch 25110/30000 Training Loss: 0.03868827596306801\n",
      "Epoch 25111/30000 Training Loss: 0.044306471943855286\n",
      "Epoch 25112/30000 Training Loss: 0.04001089185476303\n",
      "Epoch 25113/30000 Training Loss: 0.044299013912677765\n",
      "Epoch 25114/30000 Training Loss: 0.04608384519815445\n",
      "Epoch 25115/30000 Training Loss: 0.04703651741147041\n",
      "Epoch 25116/30000 Training Loss: 0.052628837525844574\n",
      "Epoch 25117/30000 Training Loss: 0.049374841153621674\n",
      "Epoch 25118/30000 Training Loss: 0.039883166551589966\n",
      "Epoch 25119/30000 Training Loss: 0.04457233101129532\n",
      "Epoch 25120/30000 Training Loss: 0.05675911530852318\n",
      "Epoch 25121/30000 Training Loss: 0.036329884082078934\n",
      "Epoch 25122/30000 Training Loss: 0.043523162603378296\n",
      "Epoch 25123/30000 Training Loss: 0.041467927396297455\n",
      "Epoch 25124/30000 Training Loss: 0.04831720516085625\n",
      "Epoch 25125/30000 Training Loss: 0.03100975975394249\n",
      "Epoch 25126/30000 Training Loss: 0.05204034224152565\n",
      "Epoch 25127/30000 Training Loss: 0.05076504126191139\n",
      "Epoch 25128/30000 Training Loss: 0.03955256938934326\n",
      "Epoch 25129/30000 Training Loss: 0.04584989696741104\n",
      "Epoch 25130/30000 Training Loss: 0.036472074687480927\n",
      "Epoch 25131/30000 Training Loss: 0.05485439673066139\n",
      "Epoch 25132/30000 Training Loss: 0.042364444583654404\n",
      "Epoch 25133/30000 Training Loss: 0.04714963957667351\n",
      "Epoch 25134/30000 Training Loss: 0.052960947155952454\n",
      "Epoch 25135/30000 Training Loss: 0.040773406624794006\n",
      "Epoch 25136/30000 Training Loss: 0.04205309972167015\n",
      "Epoch 25137/30000 Training Loss: 0.04109133034944534\n",
      "Epoch 25138/30000 Training Loss: 0.059242650866508484\n",
      "Epoch 25139/30000 Training Loss: 0.04513029381632805\n",
      "Epoch 25140/30000 Training Loss: 0.034765295684337616\n",
      "Epoch 25141/30000 Training Loss: 0.049969419836997986\n",
      "Epoch 25142/30000 Training Loss: 0.04873424023389816\n",
      "Epoch 25143/30000 Training Loss: 0.04622945934534073\n",
      "Epoch 25144/30000 Training Loss: 0.035155877470970154\n",
      "Epoch 25145/30000 Training Loss: 0.03666897863149643\n",
      "Epoch 25146/30000 Training Loss: 0.0470903143286705\n",
      "Epoch 25147/30000 Training Loss: 0.042905293405056\n",
      "Epoch 25148/30000 Training Loss: 0.03485145419836044\n",
      "Epoch 25149/30000 Training Loss: 0.052264995872974396\n",
      "Epoch 25150/30000 Training Loss: 0.03884848579764366\n",
      "Epoch 25151/30000 Training Loss: 0.04236941784620285\n",
      "Epoch 25152/30000 Training Loss: 0.04197773337364197\n",
      "Epoch 25153/30000 Training Loss: 0.042425718158483505\n",
      "Epoch 25154/30000 Training Loss: 0.03641882166266441\n",
      "Epoch 25155/30000 Training Loss: 0.04533752053976059\n",
      "Epoch 25156/30000 Training Loss: 0.03442254662513733\n",
      "Epoch 25157/30000 Training Loss: 0.041343774646520615\n",
      "Epoch 25158/30000 Training Loss: 0.04053077846765518\n",
      "Epoch 25159/30000 Training Loss: 0.034066908061504364\n",
      "Epoch 25160/30000 Training Loss: 0.02870607003569603\n",
      "Epoch 25161/30000 Training Loss: 0.0401071272790432\n",
      "Epoch 25162/30000 Training Loss: 0.04948706179857254\n",
      "Epoch 25163/30000 Training Loss: 0.04474989324808121\n",
      "Epoch 25164/30000 Training Loss: 0.041898950934410095\n",
      "Epoch 25165/30000 Training Loss: 0.047310423105955124\n",
      "Epoch 25166/30000 Training Loss: 0.03784376382827759\n",
      "Epoch 25167/30000 Training Loss: 0.037679895758628845\n",
      "Epoch 25168/30000 Training Loss: 0.04530699551105499\n",
      "Epoch 25169/30000 Training Loss: 0.03802741318941116\n",
      "Epoch 25170/30000 Training Loss: 0.04423289746046066\n",
      "Epoch 25171/30000 Training Loss: 0.05212992802262306\n",
      "Epoch 25172/30000 Training Loss: 0.043754227459430695\n",
      "Epoch 25173/30000 Training Loss: 0.04252219200134277\n",
      "Epoch 25174/30000 Training Loss: 0.03430050611495972\n",
      "Epoch 25175/30000 Training Loss: 0.04217325896024704\n",
      "Epoch 25176/30000 Training Loss: 0.04322677105665207\n",
      "Epoch 25177/30000 Training Loss: 0.04926685616374016\n",
      "Epoch 25178/30000 Training Loss: 0.03144698962569237\n",
      "Epoch 25179/30000 Training Loss: 0.03587913513183594\n",
      "Epoch 25180/30000 Training Loss: 0.03812339901924133\n",
      "Epoch 25181/30000 Training Loss: 0.04398728162050247\n",
      "Epoch 25182/30000 Training Loss: 0.036704473197460175\n",
      "Epoch 25183/30000 Training Loss: 0.06551636755466461\n",
      "Epoch 25184/30000 Training Loss: 0.040941961109638214\n",
      "Epoch 25185/30000 Training Loss: 0.032265644520521164\n",
      "Epoch 25186/30000 Training Loss: 0.049945682287216187\n",
      "Epoch 25187/30000 Training Loss: 0.037422411143779755\n",
      "Epoch 25188/30000 Training Loss: 0.04933856800198555\n",
      "Epoch 25189/30000 Training Loss: 0.038787320256233215\n",
      "Epoch 25190/30000 Training Loss: 0.05230522155761719\n",
      "Epoch 25191/30000 Training Loss: 0.04294819012284279\n",
      "Epoch 25192/30000 Training Loss: 0.041651103645563126\n",
      "Epoch 25193/30000 Training Loss: 0.03497280925512314\n",
      "Epoch 25194/30000 Training Loss: 0.06107532978057861\n",
      "Epoch 25195/30000 Training Loss: 0.03704629838466644\n",
      "Epoch 25196/30000 Training Loss: 0.04041072353720665\n",
      "Epoch 25197/30000 Training Loss: 0.05008848384022713\n",
      "Epoch 25198/30000 Training Loss: 0.03680206835269928\n",
      "Epoch 25199/30000 Training Loss: 0.05206066370010376\n",
      "Epoch 25200/30000 Training Loss: 0.04791538044810295\n",
      "Epoch 25200/30000 Validation Loss: 0.03967788815498352\n",
      "Epoch 25201/30000 Training Loss: 0.040749453008174896\n",
      "Epoch 25202/30000 Training Loss: 0.05591798573732376\n",
      "Epoch 25203/30000 Training Loss: 0.030715394765138626\n",
      "Epoch 25204/30000 Training Loss: 0.041321657598018646\n",
      "Epoch 25205/30000 Training Loss: 0.049978576600551605\n",
      "Epoch 25206/30000 Training Loss: 0.04496017098426819\n",
      "Epoch 25207/30000 Training Loss: 0.04678301513195038\n",
      "Epoch 25208/30000 Training Loss: 0.04087990149855614\n",
      "Epoch 25209/30000 Training Loss: 0.041746318340301514\n",
      "Epoch 25210/30000 Training Loss: 0.03291625529527664\n",
      "Epoch 25211/30000 Training Loss: 0.06374617666006088\n",
      "Epoch 25212/30000 Training Loss: 0.05480965971946716\n",
      "Epoch 25213/30000 Training Loss: 0.038618817925453186\n",
      "Epoch 25214/30000 Training Loss: 0.03360690176486969\n",
      "Epoch 25215/30000 Training Loss: 0.042802102863788605\n",
      "Epoch 25216/30000 Training Loss: 0.03489997982978821\n",
      "Epoch 25217/30000 Training Loss: 0.044062480330467224\n",
      "Epoch 25218/30000 Training Loss: 0.04348916560411453\n",
      "Epoch 25219/30000 Training Loss: 0.03255373612046242\n",
      "Epoch 25220/30000 Training Loss: 0.04657571390271187\n",
      "Epoch 25221/30000 Training Loss: 0.036147590726614\n",
      "Epoch 25222/30000 Training Loss: 0.0368475578725338\n",
      "Epoch 25223/30000 Training Loss: 0.057020850479602814\n",
      "Epoch 25224/30000 Training Loss: 0.044706568121910095\n",
      "Epoch 25225/30000 Training Loss: 0.04283992946147919\n",
      "Epoch 25226/30000 Training Loss: 0.03925265744328499\n",
      "Epoch 25227/30000 Training Loss: 0.03822777047753334\n",
      "Epoch 25228/30000 Training Loss: 0.04058285802602768\n",
      "Epoch 25229/30000 Training Loss: 0.039509791880846024\n",
      "Epoch 25230/30000 Training Loss: 0.028944365680217743\n",
      "Epoch 25231/30000 Training Loss: 0.04954318702220917\n",
      "Epoch 25232/30000 Training Loss: 0.04019291698932648\n",
      "Epoch 25233/30000 Training Loss: 0.05252186208963394\n",
      "Epoch 25234/30000 Training Loss: 0.034706998616456985\n",
      "Epoch 25235/30000 Training Loss: 0.042582497000694275\n",
      "Epoch 25236/30000 Training Loss: 0.041717737913131714\n",
      "Epoch 25237/30000 Training Loss: 0.04843518137931824\n",
      "Epoch 25238/30000 Training Loss: 0.04698476940393448\n",
      "Epoch 25239/30000 Training Loss: 0.05016237497329712\n",
      "Epoch 25240/30000 Training Loss: 0.058263249695301056\n",
      "Epoch 25241/30000 Training Loss: 0.037302352488040924\n",
      "Epoch 25242/30000 Training Loss: 0.04338807240128517\n",
      "Epoch 25243/30000 Training Loss: 0.03664855286478996\n",
      "Epoch 25244/30000 Training Loss: 0.04079221561551094\n",
      "Epoch 25245/30000 Training Loss: 0.046803608536720276\n",
      "Epoch 25246/30000 Training Loss: 0.046041712164878845\n",
      "Epoch 25247/30000 Training Loss: 0.037480808794498444\n",
      "Epoch 25248/30000 Training Loss: 0.043899133801460266\n",
      "Epoch 25249/30000 Training Loss: 0.036725353449583054\n",
      "Epoch 25250/30000 Training Loss: 0.036397963762283325\n",
      "Epoch 25251/30000 Training Loss: 0.03505312651395798\n",
      "Epoch 25252/30000 Training Loss: 0.03259643167257309\n",
      "Epoch 25253/30000 Training Loss: 0.03818230330944061\n",
      "Epoch 25254/30000 Training Loss: 0.040869735181331635\n",
      "Epoch 25255/30000 Training Loss: 0.05089590698480606\n",
      "Epoch 25256/30000 Training Loss: 0.033902399241924286\n",
      "Epoch 25257/30000 Training Loss: 0.037945348769426346\n",
      "Epoch 25258/30000 Training Loss: 0.03750254958868027\n",
      "Epoch 25259/30000 Training Loss: 0.044786859303712845\n",
      "Epoch 25260/30000 Training Loss: 0.03928551822900772\n",
      "Epoch 25261/30000 Training Loss: 0.05317601561546326\n",
      "Epoch 25262/30000 Training Loss: 0.04241889342665672\n",
      "Epoch 25263/30000 Training Loss: 0.05193885415792465\n",
      "Epoch 25264/30000 Training Loss: 0.03857000917196274\n",
      "Epoch 25265/30000 Training Loss: 0.04896759241819382\n",
      "Epoch 25266/30000 Training Loss: 0.048060476779937744\n",
      "Epoch 25267/30000 Training Loss: 0.03803712874650955\n",
      "Epoch 25268/30000 Training Loss: 0.04706663638353348\n",
      "Epoch 25269/30000 Training Loss: 0.03808823227882385\n",
      "Epoch 25270/30000 Training Loss: 0.04809415340423584\n",
      "Epoch 25271/30000 Training Loss: 0.04480958357453346\n",
      "Epoch 25272/30000 Training Loss: 0.03534802794456482\n",
      "Epoch 25273/30000 Training Loss: 0.04031204804778099\n",
      "Epoch 25274/30000 Training Loss: 0.045393019914627075\n",
      "Epoch 25275/30000 Training Loss: 0.030853217467665672\n",
      "Epoch 25276/30000 Training Loss: 0.03407205268740654\n",
      "Epoch 25277/30000 Training Loss: 0.05120598524808884\n",
      "Epoch 25278/30000 Training Loss: 0.044056713581085205\n",
      "Epoch 25279/30000 Training Loss: 0.04172026365995407\n",
      "Epoch 25280/30000 Training Loss: 0.042560234665870667\n",
      "Epoch 25281/30000 Training Loss: 0.04224656522274017\n",
      "Epoch 25282/30000 Training Loss: 0.04170050472021103\n",
      "Epoch 25283/30000 Training Loss: 0.04020145535469055\n",
      "Epoch 25284/30000 Training Loss: 0.04664129763841629\n",
      "Epoch 25285/30000 Training Loss: 0.031995739787817\n",
      "Epoch 25286/30000 Training Loss: 0.04780232906341553\n",
      "Epoch 25287/30000 Training Loss: 0.043061140924692154\n",
      "Epoch 25288/30000 Training Loss: 0.052586015313863754\n",
      "Epoch 25289/30000 Training Loss: 0.05575628578662872\n",
      "Epoch 25290/30000 Training Loss: 0.04407726228237152\n",
      "Epoch 25291/30000 Training Loss: 0.04355965554714203\n",
      "Epoch 25292/30000 Training Loss: 0.04797988757491112\n",
      "Epoch 25293/30000 Training Loss: 0.031631842255592346\n",
      "Epoch 25294/30000 Training Loss: 0.03413207828998566\n",
      "Epoch 25295/30000 Training Loss: 0.0410594642162323\n",
      "Epoch 25296/30000 Training Loss: 0.03488924726843834\n",
      "Epoch 25297/30000 Training Loss: 0.03692605346441269\n",
      "Epoch 25298/30000 Training Loss: 0.060400594025850296\n",
      "Epoch 25299/30000 Training Loss: 0.06012412905693054\n",
      "Epoch 25300/30000 Training Loss: 0.04835899919271469\n",
      "Epoch 25300/30000 Validation Loss: 0.025672651827335358\n",
      "Epoch 25301/30000 Training Loss: 0.03607212007045746\n",
      "Epoch 25302/30000 Training Loss: 0.02848011627793312\n",
      "Epoch 25303/30000 Training Loss: 0.04553760588169098\n",
      "Epoch 25304/30000 Training Loss: 0.05221410468220711\n",
      "Epoch 25305/30000 Training Loss: 0.03890639916062355\n",
      "Epoch 25306/30000 Training Loss: 0.036942124366760254\n",
      "Epoch 25307/30000 Training Loss: 0.05441749840974808\n",
      "Epoch 25308/30000 Training Loss: 0.04604213684797287\n",
      "Epoch 25309/30000 Training Loss: 0.03692887723445892\n",
      "Epoch 25310/30000 Training Loss: 0.05571109056472778\n",
      "Epoch 25311/30000 Training Loss: 0.052421655505895615\n",
      "Epoch 25312/30000 Training Loss: 0.0517338290810585\n",
      "Epoch 25313/30000 Training Loss: 0.03435155004262924\n",
      "Epoch 25314/30000 Training Loss: 0.04555775225162506\n",
      "Epoch 25315/30000 Training Loss: 0.04003549739718437\n",
      "Epoch 25316/30000 Training Loss: 0.04515872150659561\n",
      "Epoch 25317/30000 Training Loss: 0.04481803625822067\n",
      "Epoch 25318/30000 Training Loss: 0.04362490773200989\n",
      "Epoch 25319/30000 Training Loss: 0.043073080480098724\n",
      "Epoch 25320/30000 Training Loss: 0.04743432626128197\n",
      "Epoch 25321/30000 Training Loss: 0.042321644723415375\n",
      "Epoch 25322/30000 Training Loss: 0.03899211063981056\n",
      "Epoch 25323/30000 Training Loss: 0.034496672451496124\n",
      "Epoch 25324/30000 Training Loss: 0.04318368062376976\n",
      "Epoch 25325/30000 Training Loss: 0.04911698400974274\n",
      "Epoch 25326/30000 Training Loss: 0.03731090947985649\n",
      "Epoch 25327/30000 Training Loss: 0.0537649467587471\n",
      "Epoch 25328/30000 Training Loss: 0.036464445292949677\n",
      "Epoch 25329/30000 Training Loss: 0.03733977675437927\n",
      "Epoch 25330/30000 Training Loss: 0.040661752223968506\n",
      "Epoch 25331/30000 Training Loss: 0.052342306822538376\n",
      "Epoch 25332/30000 Training Loss: 0.05114826187491417\n",
      "Epoch 25333/30000 Training Loss: 0.04262368381023407\n",
      "Epoch 25334/30000 Training Loss: 0.03638515993952751\n",
      "Epoch 25335/30000 Training Loss: 0.04363279789686203\n",
      "Epoch 25336/30000 Training Loss: 0.033756524324417114\n",
      "Epoch 25337/30000 Training Loss: 0.03683429956436157\n",
      "Epoch 25338/30000 Training Loss: 0.034348562359809875\n",
      "Epoch 25339/30000 Training Loss: 0.06278222054243088\n",
      "Epoch 25340/30000 Training Loss: 0.04149415343999863\n",
      "Epoch 25341/30000 Training Loss: 0.04618263989686966\n",
      "Epoch 25342/30000 Training Loss: 0.03315768390893936\n",
      "Epoch 25343/30000 Training Loss: 0.03754188120365143\n",
      "Epoch 25344/30000 Training Loss: 0.035942308604717255\n",
      "Epoch 25345/30000 Training Loss: 0.0437348373234272\n",
      "Epoch 25346/30000 Training Loss: 0.049906279891729355\n",
      "Epoch 25347/30000 Training Loss: 0.04891600459814072\n",
      "Epoch 25348/30000 Training Loss: 0.049080997705459595\n",
      "Epoch 25349/30000 Training Loss: 0.03625224530696869\n",
      "Epoch 25350/30000 Training Loss: 0.04061712324619293\n",
      "Epoch 25351/30000 Training Loss: 0.039459265768527985\n",
      "Epoch 25352/30000 Training Loss: 0.03515837341547012\n",
      "Epoch 25353/30000 Training Loss: 0.04809163883328438\n",
      "Epoch 25354/30000 Training Loss: 0.04108697548508644\n",
      "Epoch 25355/30000 Training Loss: 0.03538554534316063\n",
      "Epoch 25356/30000 Training Loss: 0.060456857085227966\n",
      "Epoch 25357/30000 Training Loss: 0.044906795024871826\n",
      "Epoch 25358/30000 Training Loss: 0.04253020137548447\n",
      "Epoch 25359/30000 Training Loss: 0.02919231355190277\n",
      "Epoch 25360/30000 Training Loss: 0.04421287402510643\n",
      "Epoch 25361/30000 Training Loss: 0.04472199082374573\n",
      "Epoch 25362/30000 Training Loss: 0.03740417957305908\n",
      "Epoch 25363/30000 Training Loss: 0.03887970745563507\n",
      "Epoch 25364/30000 Training Loss: 0.05283170938491821\n",
      "Epoch 25365/30000 Training Loss: 0.042685601860284805\n",
      "Epoch 25366/30000 Training Loss: 0.03740353137254715\n",
      "Epoch 25367/30000 Training Loss: 0.052846185863018036\n",
      "Epoch 25368/30000 Training Loss: 0.039412498474121094\n",
      "Epoch 25369/30000 Training Loss: 0.049380823969841\n",
      "Epoch 25370/30000 Training Loss: 0.05921335890889168\n",
      "Epoch 25371/30000 Training Loss: 0.03955071419477463\n",
      "Epoch 25372/30000 Training Loss: 0.038165196776390076\n",
      "Epoch 25373/30000 Training Loss: 0.051464926451444626\n",
      "Epoch 25374/30000 Training Loss: 0.04488559067249298\n",
      "Epoch 25375/30000 Training Loss: 0.04198796674609184\n",
      "Epoch 25376/30000 Training Loss: 0.045170411467552185\n",
      "Epoch 25377/30000 Training Loss: 0.04251021146774292\n",
      "Epoch 25378/30000 Training Loss: 0.04667424410581589\n",
      "Epoch 25379/30000 Training Loss: 0.040536362677812576\n",
      "Epoch 25380/30000 Training Loss: 0.04555216059088707\n",
      "Epoch 25381/30000 Training Loss: 0.05754835903644562\n",
      "Epoch 25382/30000 Training Loss: 0.04316514730453491\n",
      "Epoch 25383/30000 Training Loss: 0.035381920635700226\n",
      "Epoch 25384/30000 Training Loss: 0.04356706142425537\n",
      "Epoch 25385/30000 Training Loss: 0.043253093957901\n",
      "Epoch 25386/30000 Training Loss: 0.03667913004755974\n",
      "Epoch 25387/30000 Training Loss: 0.05265326797962189\n",
      "Epoch 25388/30000 Training Loss: 0.046112146228551865\n",
      "Epoch 25389/30000 Training Loss: 0.04030574858188629\n",
      "Epoch 25390/30000 Training Loss: 0.045832887291908264\n",
      "Epoch 25391/30000 Training Loss: 0.03990161791443825\n",
      "Epoch 25392/30000 Training Loss: 0.046161193400621414\n",
      "Epoch 25393/30000 Training Loss: 0.04098919779062271\n",
      "Epoch 25394/30000 Training Loss: 0.03669039160013199\n",
      "Epoch 25395/30000 Training Loss: 0.048858892172575\n",
      "Epoch 25396/30000 Training Loss: 0.0426117479801178\n",
      "Epoch 25397/30000 Training Loss: 0.0484975203871727\n",
      "Epoch 25398/30000 Training Loss: 0.044396623969078064\n",
      "Epoch 25399/30000 Training Loss: 0.03918880596756935\n",
      "Epoch 25400/30000 Training Loss: 0.04518735408782959\n",
      "Epoch 25400/30000 Validation Loss: 0.03691806644201279\n",
      "Epoch 25401/30000 Training Loss: 0.040555521845817566\n",
      "Epoch 25402/30000 Training Loss: 0.045383255928754807\n",
      "Epoch 25403/30000 Training Loss: 0.04690339416265488\n",
      "Epoch 25404/30000 Training Loss: 0.04327603802084923\n",
      "Epoch 25405/30000 Training Loss: 0.04138222336769104\n",
      "Epoch 25406/30000 Training Loss: 0.04162633419036865\n",
      "Epoch 25407/30000 Training Loss: 0.043234799057245255\n",
      "Epoch 25408/30000 Training Loss: 0.04404061660170555\n",
      "Epoch 25409/30000 Training Loss: 0.04442056268453598\n",
      "Epoch 25410/30000 Training Loss: 0.03375501185655594\n",
      "Epoch 25411/30000 Training Loss: 0.05496089905500412\n",
      "Epoch 25412/30000 Training Loss: 0.041780970990657806\n",
      "Epoch 25413/30000 Training Loss: 0.04950488358736038\n",
      "Epoch 25414/30000 Training Loss: 0.04881727695465088\n",
      "Epoch 25415/30000 Training Loss: 0.04654458537697792\n",
      "Epoch 25416/30000 Training Loss: 0.03220851346850395\n",
      "Epoch 25417/30000 Training Loss: 0.028563469648361206\n",
      "Epoch 25418/30000 Training Loss: 0.052871882915496826\n",
      "Epoch 25419/30000 Training Loss: 0.04706169664859772\n",
      "Epoch 25420/30000 Training Loss: 0.047122180461883545\n",
      "Epoch 25421/30000 Training Loss: 0.0487765371799469\n",
      "Epoch 25422/30000 Training Loss: 0.04967746138572693\n",
      "Epoch 25423/30000 Training Loss: 0.0424916073679924\n",
      "Epoch 25424/30000 Training Loss: 0.050522468984127045\n",
      "Epoch 25425/30000 Training Loss: 0.036132700741291046\n",
      "Epoch 25426/30000 Training Loss: 0.03091246262192726\n",
      "Epoch 25427/30000 Training Loss: 0.05516213923692703\n",
      "Epoch 25428/30000 Training Loss: 0.04713044315576553\n",
      "Epoch 25429/30000 Training Loss: 0.04022715240716934\n",
      "Epoch 25430/30000 Training Loss: 0.040302082896232605\n",
      "Epoch 25431/30000 Training Loss: 0.042771775275468826\n",
      "Epoch 25432/30000 Training Loss: 0.05201346054673195\n",
      "Epoch 25433/30000 Training Loss: 0.042470429092645645\n",
      "Epoch 25434/30000 Training Loss: 0.043224506080150604\n",
      "Epoch 25435/30000 Training Loss: 0.033629611134529114\n",
      "Epoch 25436/30000 Training Loss: 0.03603946790099144\n",
      "Epoch 25437/30000 Training Loss: 0.051857803016901016\n",
      "Epoch 25438/30000 Training Loss: 0.05141153559088707\n",
      "Epoch 25439/30000 Training Loss: 0.03927159681916237\n",
      "Epoch 25440/30000 Training Loss: 0.04122917726635933\n",
      "Epoch 25441/30000 Training Loss: 0.029461491852998734\n",
      "Epoch 25442/30000 Training Loss: 0.03889745473861694\n",
      "Epoch 25443/30000 Training Loss: 0.04257135093212128\n",
      "Epoch 25444/30000 Training Loss: 0.04535909742116928\n",
      "Epoch 25445/30000 Training Loss: 0.04720968008041382\n",
      "Epoch 25446/30000 Training Loss: 0.03956613689661026\n",
      "Epoch 25447/30000 Training Loss: 0.03411802649497986\n",
      "Epoch 25448/30000 Training Loss: 0.03361434116959572\n",
      "Epoch 25449/30000 Training Loss: 0.04315550625324249\n",
      "Epoch 25450/30000 Training Loss: 0.03240280970931053\n",
      "Epoch 25451/30000 Training Loss: 0.03851011022925377\n",
      "Epoch 25452/30000 Training Loss: 0.04287222772836685\n",
      "Epoch 25453/30000 Training Loss: 0.04080578684806824\n",
      "Epoch 25454/30000 Training Loss: 0.04870835319161415\n",
      "Epoch 25455/30000 Training Loss: 0.0387628972530365\n",
      "Epoch 25456/30000 Training Loss: 0.042976297438144684\n",
      "Epoch 25457/30000 Training Loss: 0.044235385954380035\n",
      "Epoch 25458/30000 Training Loss: 0.05989697575569153\n",
      "Epoch 25459/30000 Training Loss: 0.03530406579375267\n",
      "Epoch 25460/30000 Training Loss: 0.04346925765275955\n",
      "Epoch 25461/30000 Training Loss: 0.039699893444776535\n",
      "Epoch 25462/30000 Training Loss: 0.05387861281633377\n",
      "Epoch 25463/30000 Training Loss: 0.042594484984874725\n",
      "Epoch 25464/30000 Training Loss: 0.04234199598431587\n",
      "Epoch 25465/30000 Training Loss: 0.04022521898150444\n",
      "Epoch 25466/30000 Training Loss: 0.05187494307756424\n",
      "Epoch 25467/30000 Training Loss: 0.04197463393211365\n",
      "Epoch 25468/30000 Training Loss: 0.04337642341852188\n",
      "Epoch 25469/30000 Training Loss: 0.04246491193771362\n",
      "Epoch 25470/30000 Training Loss: 0.03875356167554855\n",
      "Epoch 25471/30000 Training Loss: 0.03303360193967819\n",
      "Epoch 25472/30000 Training Loss: 0.04428887367248535\n",
      "Epoch 25473/30000 Training Loss: 0.03966522216796875\n",
      "Epoch 25474/30000 Training Loss: 0.05335244536399841\n",
      "Epoch 25475/30000 Training Loss: 0.04129042848944664\n",
      "Epoch 25476/30000 Training Loss: 0.03415721654891968\n",
      "Epoch 25477/30000 Training Loss: 0.03248973935842514\n",
      "Epoch 25478/30000 Training Loss: 0.03861899673938751\n",
      "Epoch 25479/30000 Training Loss: 0.043256938457489014\n",
      "Epoch 25480/30000 Training Loss: 0.04956961050629616\n",
      "Epoch 25481/30000 Training Loss: 0.04340419918298721\n",
      "Epoch 25482/30000 Training Loss: 0.035259030759334564\n",
      "Epoch 25483/30000 Training Loss: 0.03573957830667496\n",
      "Epoch 25484/30000 Training Loss: 0.039012610912323\n",
      "Epoch 25485/30000 Training Loss: 0.04737171530723572\n",
      "Epoch 25486/30000 Training Loss: 0.043177660554647446\n",
      "Epoch 25487/30000 Training Loss: 0.05824693664908409\n",
      "Epoch 25488/30000 Training Loss: 0.03928545117378235\n",
      "Epoch 25489/30000 Training Loss: 0.04914189875125885\n",
      "Epoch 25490/30000 Training Loss: 0.05744456127285957\n",
      "Epoch 25491/30000 Training Loss: 0.03987313061952591\n",
      "Epoch 25492/30000 Training Loss: 0.03725094348192215\n",
      "Epoch 25493/30000 Training Loss: 0.04276315122842789\n",
      "Epoch 25494/30000 Training Loss: 0.03705738112330437\n",
      "Epoch 25495/30000 Training Loss: 0.03690895810723305\n",
      "Epoch 25496/30000 Training Loss: 0.03868630528450012\n",
      "Epoch 25497/30000 Training Loss: 0.03836161270737648\n",
      "Epoch 25498/30000 Training Loss: 0.04980037361383438\n",
      "Epoch 25499/30000 Training Loss: 0.05788542330265045\n",
      "Epoch 25500/30000 Training Loss: 0.045881353318691254\n",
      "Epoch 25500/30000 Validation Loss: 0.052369456738233566\n",
      "Epoch 25501/30000 Training Loss: 0.046688634902238846\n",
      "Epoch 25502/30000 Training Loss: 0.06136196106672287\n",
      "Epoch 25503/30000 Training Loss: 0.04686763882637024\n",
      "Epoch 25504/30000 Training Loss: 0.03531215712428093\n",
      "Epoch 25505/30000 Training Loss: 0.053248390555381775\n",
      "Epoch 25506/30000 Training Loss: 0.048819176852703094\n",
      "Epoch 25507/30000 Training Loss: 0.033343955874443054\n",
      "Epoch 25508/30000 Training Loss: 0.04945072904229164\n",
      "Epoch 25509/30000 Training Loss: 0.04228068143129349\n",
      "Epoch 25510/30000 Training Loss: 0.04036404937505722\n",
      "Epoch 25511/30000 Training Loss: 0.042589761316776276\n",
      "Epoch 25512/30000 Training Loss: 0.035675860941410065\n",
      "Epoch 25513/30000 Training Loss: 0.05285365507006645\n",
      "Epoch 25514/30000 Training Loss: 0.04415958374738693\n",
      "Epoch 25515/30000 Training Loss: 0.04256996512413025\n",
      "Epoch 25516/30000 Training Loss: 0.0430934764444828\n",
      "Epoch 25517/30000 Training Loss: 0.04051073640584946\n",
      "Epoch 25518/30000 Training Loss: 0.03858792036771774\n",
      "Epoch 25519/30000 Training Loss: 0.03503430262207985\n",
      "Epoch 25520/30000 Training Loss: 0.038804784417152405\n",
      "Epoch 25521/30000 Training Loss: 0.03515814244747162\n",
      "Epoch 25522/30000 Training Loss: 0.03802916780114174\n",
      "Epoch 25523/30000 Training Loss: 0.04111411049962044\n",
      "Epoch 25524/30000 Training Loss: 0.04508870095014572\n",
      "Epoch 25525/30000 Training Loss: 0.050905607640743256\n",
      "Epoch 25526/30000 Training Loss: 0.03843522444367409\n",
      "Epoch 25527/30000 Training Loss: 0.0508989654481411\n",
      "Epoch 25528/30000 Training Loss: 0.05562206357717514\n",
      "Epoch 25529/30000 Training Loss: 0.03831349313259125\n",
      "Epoch 25530/30000 Training Loss: 0.04433666169643402\n",
      "Epoch 25531/30000 Training Loss: 0.04613766819238663\n",
      "Epoch 25532/30000 Training Loss: 0.03773779422044754\n",
      "Epoch 25533/30000 Training Loss: 0.05147732049226761\n",
      "Epoch 25534/30000 Training Loss: 0.04085845500230789\n",
      "Epoch 25535/30000 Training Loss: 0.046076130121946335\n",
      "Epoch 25536/30000 Training Loss: 0.037216320633888245\n",
      "Epoch 25537/30000 Training Loss: 0.043153420090675354\n",
      "Epoch 25538/30000 Training Loss: 0.04371890798211098\n",
      "Epoch 25539/30000 Training Loss: 0.04030321165919304\n",
      "Epoch 25540/30000 Training Loss: 0.0463789626955986\n",
      "Epoch 25541/30000 Training Loss: 0.04612812399864197\n",
      "Epoch 25542/30000 Training Loss: 0.04820055887103081\n",
      "Epoch 25543/30000 Training Loss: 0.030876783654093742\n",
      "Epoch 25544/30000 Training Loss: 0.049835361540317535\n",
      "Epoch 25545/30000 Training Loss: 0.04737052321434021\n",
      "Epoch 25546/30000 Training Loss: 0.038648851215839386\n",
      "Epoch 25547/30000 Training Loss: 0.042714834213256836\n",
      "Epoch 25548/30000 Training Loss: 0.027267713099718094\n",
      "Epoch 25549/30000 Training Loss: 0.047375649213790894\n",
      "Epoch 25550/30000 Training Loss: 0.03335782140493393\n",
      "Epoch 25551/30000 Training Loss: 0.04179583117365837\n",
      "Epoch 25552/30000 Training Loss: 0.04784855991601944\n",
      "Epoch 25553/30000 Training Loss: 0.03394319489598274\n",
      "Epoch 25554/30000 Training Loss: 0.04795243591070175\n",
      "Epoch 25555/30000 Training Loss: 0.03800643980503082\n",
      "Epoch 25556/30000 Training Loss: 0.05205678939819336\n",
      "Epoch 25557/30000 Training Loss: 0.04287319630384445\n",
      "Epoch 25558/30000 Training Loss: 0.03189109265804291\n",
      "Epoch 25559/30000 Training Loss: 0.031225930899381638\n",
      "Epoch 25560/30000 Training Loss: 0.05105115473270416\n",
      "Epoch 25561/30000 Training Loss: 0.044023796916007996\n",
      "Epoch 25562/30000 Training Loss: 0.054163869470357895\n",
      "Epoch 25563/30000 Training Loss: 0.04297898709774017\n",
      "Epoch 25564/30000 Training Loss: 0.037943143397569656\n",
      "Epoch 25565/30000 Training Loss: 0.050005435943603516\n",
      "Epoch 25566/30000 Training Loss: 0.04457796365022659\n",
      "Epoch 25567/30000 Training Loss: 0.05095405504107475\n",
      "Epoch 25568/30000 Training Loss: 0.05468533933162689\n",
      "Epoch 25569/30000 Training Loss: 0.026725973933935165\n",
      "Epoch 25570/30000 Training Loss: 0.04614735394716263\n",
      "Epoch 25571/30000 Training Loss: 0.03893350437283516\n",
      "Epoch 25572/30000 Training Loss: 0.04627463221549988\n",
      "Epoch 25573/30000 Training Loss: 0.040876150131225586\n",
      "Epoch 25574/30000 Training Loss: 0.045797593891620636\n",
      "Epoch 25575/30000 Training Loss: 0.05166992172598839\n",
      "Epoch 25576/30000 Training Loss: 0.0537637360394001\n",
      "Epoch 25577/30000 Training Loss: 0.05222878232598305\n",
      "Epoch 25578/30000 Training Loss: 0.03188140690326691\n",
      "Epoch 25579/30000 Training Loss: 0.0568164587020874\n",
      "Epoch 25580/30000 Training Loss: 0.0592532716691494\n",
      "Epoch 25581/30000 Training Loss: 0.03016381710767746\n",
      "Epoch 25582/30000 Training Loss: 0.04374205321073532\n",
      "Epoch 25583/30000 Training Loss: 0.035614967346191406\n",
      "Epoch 25584/30000 Training Loss: 0.06031332537531853\n",
      "Epoch 25585/30000 Training Loss: 0.061505094170570374\n",
      "Epoch 25586/30000 Training Loss: 0.04090767726302147\n",
      "Epoch 25587/30000 Training Loss: 0.032509852200746536\n",
      "Epoch 25588/30000 Training Loss: 0.047941211611032486\n",
      "Epoch 25589/30000 Training Loss: 0.03569675236940384\n",
      "Epoch 25590/30000 Training Loss: 0.030478380620479584\n",
      "Epoch 25591/30000 Training Loss: 0.044307559728622437\n",
      "Epoch 25592/30000 Training Loss: 0.05249762535095215\n",
      "Epoch 25593/30000 Training Loss: 0.04739968478679657\n",
      "Epoch 25594/30000 Training Loss: 0.04666076600551605\n",
      "Epoch 25595/30000 Training Loss: 0.04882727563381195\n",
      "Epoch 25596/30000 Training Loss: 0.052691660821437836\n",
      "Epoch 25597/30000 Training Loss: 0.03845249116420746\n",
      "Epoch 25598/30000 Training Loss: 0.04486279934644699\n",
      "Epoch 25599/30000 Training Loss: 0.04320617765188217\n",
      "Epoch 25600/30000 Training Loss: 0.042763616889715195\n",
      "Epoch 25600/30000 Validation Loss: 0.04183164983987808\n",
      "Epoch 25601/30000 Training Loss: 0.034497879445552826\n",
      "Epoch 25602/30000 Training Loss: 0.03990452364087105\n",
      "Epoch 25603/30000 Training Loss: 0.03613762557506561\n",
      "Epoch 25604/30000 Training Loss: 0.04891730099916458\n",
      "Epoch 25605/30000 Training Loss: 0.04825340211391449\n",
      "Epoch 25606/30000 Training Loss: 0.042792048305273056\n",
      "Epoch 25607/30000 Training Loss: 0.043959055095911026\n",
      "Epoch 25608/30000 Training Loss: 0.04363762587308884\n",
      "Epoch 25609/30000 Training Loss: 0.028952138498425484\n",
      "Epoch 25610/30000 Training Loss: 0.04518773406744003\n",
      "Epoch 25611/30000 Training Loss: 0.042621590197086334\n",
      "Epoch 25612/30000 Training Loss: 0.04972056671977043\n",
      "Epoch 25613/30000 Training Loss: 0.04616040736436844\n",
      "Epoch 25614/30000 Training Loss: 0.05585767328739166\n",
      "Epoch 25615/30000 Training Loss: 0.05406155437231064\n",
      "Epoch 25616/30000 Training Loss: 0.05024699121713638\n",
      "Epoch 25617/30000 Training Loss: 0.05390091985464096\n",
      "Epoch 25618/30000 Training Loss: 0.05094970017671585\n",
      "Epoch 25619/30000 Training Loss: 0.0420849546790123\n",
      "Epoch 25620/30000 Training Loss: 0.03492347151041031\n",
      "Epoch 25621/30000 Training Loss: 0.046256937086582184\n",
      "Epoch 25622/30000 Training Loss: 0.038930073380470276\n",
      "Epoch 25623/30000 Training Loss: 0.0435873419046402\n",
      "Epoch 25624/30000 Training Loss: 0.040835268795490265\n",
      "Epoch 25625/30000 Training Loss: 0.04663844406604767\n",
      "Epoch 25626/30000 Training Loss: 0.038763005286455154\n",
      "Epoch 25627/30000 Training Loss: 0.0387425497174263\n",
      "Epoch 25628/30000 Training Loss: 0.03612115606665611\n",
      "Epoch 25629/30000 Training Loss: 0.034635115414857864\n",
      "Epoch 25630/30000 Training Loss: 0.03848564997315407\n",
      "Epoch 25631/30000 Training Loss: 0.045987166464328766\n",
      "Epoch 25632/30000 Training Loss: 0.044762834906578064\n",
      "Epoch 25633/30000 Training Loss: 0.04762216657400131\n",
      "Epoch 25634/30000 Training Loss: 0.040236856788396835\n",
      "Epoch 25635/30000 Training Loss: 0.032717011868953705\n",
      "Epoch 25636/30000 Training Loss: 0.04456745460629463\n",
      "Epoch 25637/30000 Training Loss: 0.03916306048631668\n",
      "Epoch 25638/30000 Training Loss: 0.04502290487289429\n",
      "Epoch 25639/30000 Training Loss: 0.038043513894081116\n",
      "Epoch 25640/30000 Training Loss: 0.035622067749500275\n",
      "Epoch 25641/30000 Training Loss: 0.047238968312740326\n",
      "Epoch 25642/30000 Training Loss: 0.03505031019449234\n",
      "Epoch 25643/30000 Training Loss: 0.05475243926048279\n",
      "Epoch 25644/30000 Training Loss: 0.04016374424099922\n",
      "Epoch 25645/30000 Training Loss: 0.056697916239500046\n",
      "Epoch 25646/30000 Training Loss: 0.03572995960712433\n",
      "Epoch 25647/30000 Training Loss: 0.03871767595410347\n",
      "Epoch 25648/30000 Training Loss: 0.0506274476647377\n",
      "Epoch 25649/30000 Training Loss: 0.03750120848417282\n",
      "Epoch 25650/30000 Training Loss: 0.039194606244564056\n",
      "Epoch 25651/30000 Training Loss: 0.059143394231796265\n",
      "Epoch 25652/30000 Training Loss: 0.04221434146165848\n",
      "Epoch 25653/30000 Training Loss: 0.03869544714689255\n",
      "Epoch 25654/30000 Training Loss: 0.049454689025878906\n",
      "Epoch 25655/30000 Training Loss: 0.039761558175086975\n",
      "Epoch 25656/30000 Training Loss: 0.04208465293049812\n",
      "Epoch 25657/30000 Training Loss: 0.04545468091964722\n",
      "Epoch 25658/30000 Training Loss: 0.04761286824941635\n",
      "Epoch 25659/30000 Training Loss: 0.03817364573478699\n",
      "Epoch 25660/30000 Training Loss: 0.042526185512542725\n",
      "Epoch 25661/30000 Training Loss: 0.04519486427307129\n",
      "Epoch 25662/30000 Training Loss: 0.04189813882112503\n",
      "Epoch 25663/30000 Training Loss: 0.03247681260108948\n",
      "Epoch 25664/30000 Training Loss: 0.04114639759063721\n",
      "Epoch 25665/30000 Training Loss: 0.03556780144572258\n",
      "Epoch 25666/30000 Training Loss: 0.05723797529935837\n",
      "Epoch 25667/30000 Training Loss: 0.04783812165260315\n",
      "Epoch 25668/30000 Training Loss: 0.040582891553640366\n",
      "Epoch 25669/30000 Training Loss: 0.04753522947430611\n",
      "Epoch 25670/30000 Training Loss: 0.03594023734331131\n",
      "Epoch 25671/30000 Training Loss: 0.04519397020339966\n",
      "Epoch 25672/30000 Training Loss: 0.03756716102361679\n",
      "Epoch 25673/30000 Training Loss: 0.04099489003419876\n",
      "Epoch 25674/30000 Training Loss: 0.05138783901929855\n",
      "Epoch 25675/30000 Training Loss: 0.04262284189462662\n",
      "Epoch 25676/30000 Training Loss: 0.040959373116493225\n",
      "Epoch 25677/30000 Training Loss: 0.0522051602602005\n",
      "Epoch 25678/30000 Training Loss: 0.04299895465373993\n",
      "Epoch 25679/30000 Training Loss: 0.05507761985063553\n",
      "Epoch 25680/30000 Training Loss: 0.03063041716814041\n",
      "Epoch 25681/30000 Training Loss: 0.04481026902794838\n",
      "Epoch 25682/30000 Training Loss: 0.05347342789173126\n",
      "Epoch 25683/30000 Training Loss: 0.051699183881282806\n",
      "Epoch 25684/30000 Training Loss: 0.05131298303604126\n",
      "Epoch 25685/30000 Training Loss: 0.03978916257619858\n",
      "Epoch 25686/30000 Training Loss: 0.052704788744449615\n",
      "Epoch 25687/30000 Training Loss: 0.03654961660504341\n",
      "Epoch 25688/30000 Training Loss: 0.05584683269262314\n",
      "Epoch 25689/30000 Training Loss: 0.0384061299264431\n",
      "Epoch 25690/30000 Training Loss: 0.04676473140716553\n",
      "Epoch 25691/30000 Training Loss: 0.04732026904821396\n",
      "Epoch 25692/30000 Training Loss: 0.03967815637588501\n",
      "Epoch 25693/30000 Training Loss: 0.03642769157886505\n",
      "Epoch 25694/30000 Training Loss: 0.03483308479189873\n",
      "Epoch 25695/30000 Training Loss: 0.05157957971096039\n",
      "Epoch 25696/30000 Training Loss: 0.03964966908097267\n",
      "Epoch 25697/30000 Training Loss: 0.04314962029457092\n",
      "Epoch 25698/30000 Training Loss: 0.05068885535001755\n",
      "Epoch 25699/30000 Training Loss: 0.04430677741765976\n",
      "Epoch 25700/30000 Training Loss: 0.04950081557035446\n",
      "Epoch 25700/30000 Validation Loss: 0.03957396745681763\n",
      "Epoch 25701/30000 Training Loss: 0.04902014508843422\n",
      "Epoch 25702/30000 Training Loss: 0.03326336294412613\n",
      "Epoch 25703/30000 Training Loss: 0.03779730200767517\n",
      "Epoch 25704/30000 Training Loss: 0.05255736783146858\n",
      "Epoch 25705/30000 Training Loss: 0.05453634262084961\n",
      "Epoch 25706/30000 Training Loss: 0.0509161576628685\n",
      "Epoch 25707/30000 Training Loss: 0.04539594426751137\n",
      "Epoch 25708/30000 Training Loss: 0.04230298101902008\n",
      "Epoch 25709/30000 Training Loss: 0.03505191206932068\n",
      "Epoch 25710/30000 Training Loss: 0.03960545361042023\n",
      "Epoch 25711/30000 Training Loss: 0.03974376246333122\n",
      "Epoch 25712/30000 Training Loss: 0.06610794365406036\n",
      "Epoch 25713/30000 Training Loss: 0.044258229434490204\n",
      "Epoch 25714/30000 Training Loss: 0.04362371936440468\n",
      "Epoch 25715/30000 Training Loss: 0.04452575743198395\n",
      "Epoch 25716/30000 Training Loss: 0.030614323914051056\n",
      "Epoch 25717/30000 Training Loss: 0.06094241887331009\n",
      "Epoch 25718/30000 Training Loss: 0.04016175866127014\n",
      "Epoch 25719/30000 Training Loss: 0.028718501329421997\n",
      "Epoch 25720/30000 Training Loss: 0.04414607957005501\n",
      "Epoch 25721/30000 Training Loss: 0.03770638257265091\n",
      "Epoch 25722/30000 Training Loss: 0.03597081080079079\n",
      "Epoch 25723/30000 Training Loss: 0.04157514125108719\n",
      "Epoch 25724/30000 Training Loss: 0.03599609434604645\n",
      "Epoch 25725/30000 Training Loss: 0.04602503031492233\n",
      "Epoch 25726/30000 Training Loss: 0.051651597023010254\n",
      "Epoch 25727/30000 Training Loss: 0.04999013990163803\n",
      "Epoch 25728/30000 Training Loss: 0.0499703623354435\n",
      "Epoch 25729/30000 Training Loss: 0.03392691910266876\n",
      "Epoch 25730/30000 Training Loss: 0.03591442480683327\n",
      "Epoch 25731/30000 Training Loss: 0.04269777238368988\n",
      "Epoch 25732/30000 Training Loss: 0.04734598845243454\n",
      "Epoch 25733/30000 Training Loss: 0.04961848258972168\n",
      "Epoch 25734/30000 Training Loss: 0.04286385327577591\n",
      "Epoch 25735/30000 Training Loss: 0.03439496457576752\n",
      "Epoch 25736/30000 Training Loss: 0.03815189003944397\n",
      "Epoch 25737/30000 Training Loss: 0.046045683324337006\n",
      "Epoch 25738/30000 Training Loss: 0.03317055106163025\n",
      "Epoch 25739/30000 Training Loss: 0.035103462636470795\n",
      "Epoch 25740/30000 Training Loss: 0.04854751378297806\n",
      "Epoch 25741/30000 Training Loss: 0.04450869932770729\n",
      "Epoch 25742/30000 Training Loss: 0.04683518409729004\n",
      "Epoch 25743/30000 Training Loss: 0.04626324400305748\n",
      "Epoch 25744/30000 Training Loss: 0.04088299721479416\n",
      "Epoch 25745/30000 Training Loss: 0.045704491436481476\n",
      "Epoch 25746/30000 Training Loss: 0.052378661930561066\n",
      "Epoch 25747/30000 Training Loss: 0.043999288231134415\n",
      "Epoch 25748/30000 Training Loss: 0.047909215092659\n",
      "Epoch 25749/30000 Training Loss: 0.03817535191774368\n",
      "Epoch 25750/30000 Training Loss: 0.04385033994913101\n",
      "Epoch 25751/30000 Training Loss: 0.035222090780735016\n",
      "Epoch 25752/30000 Training Loss: 0.05394598841667175\n",
      "Epoch 25753/30000 Training Loss: 0.036557070910930634\n",
      "Epoch 25754/30000 Training Loss: 0.05585787817835808\n",
      "Epoch 25755/30000 Training Loss: 0.042202651500701904\n",
      "Epoch 25756/30000 Training Loss: 0.04365875571966171\n",
      "Epoch 25757/30000 Training Loss: 0.047222502529621124\n",
      "Epoch 25758/30000 Training Loss: 0.03621799498796463\n",
      "Epoch 25759/30000 Training Loss: 0.05410449951887131\n",
      "Epoch 25760/30000 Training Loss: 0.03493073582649231\n",
      "Epoch 25761/30000 Training Loss: 0.048269737511873245\n",
      "Epoch 25762/30000 Training Loss: 0.04065092280507088\n",
      "Epoch 25763/30000 Training Loss: 0.04724428057670593\n",
      "Epoch 25764/30000 Training Loss: 0.04532574862241745\n",
      "Epoch 25765/30000 Training Loss: 0.04838890954852104\n",
      "Epoch 25766/30000 Training Loss: 0.04987911880016327\n",
      "Epoch 25767/30000 Training Loss: 0.03417022526264191\n",
      "Epoch 25768/30000 Training Loss: 0.054229918867349625\n",
      "Epoch 25769/30000 Training Loss: 0.04191305860877037\n",
      "Epoch 25770/30000 Training Loss: 0.047990743070840836\n",
      "Epoch 25771/30000 Training Loss: 0.04355438053607941\n",
      "Epoch 25772/30000 Training Loss: 0.06038606911897659\n",
      "Epoch 25773/30000 Training Loss: 0.04283808171749115\n",
      "Epoch 25774/30000 Training Loss: 0.04541707783937454\n",
      "Epoch 25775/30000 Training Loss: 0.04398275911808014\n",
      "Epoch 25776/30000 Training Loss: 0.036221541464328766\n",
      "Epoch 25777/30000 Training Loss: 0.03524582460522652\n",
      "Epoch 25778/30000 Training Loss: 0.040562570095062256\n",
      "Epoch 25779/30000 Training Loss: 0.043986715376377106\n",
      "Epoch 25780/30000 Training Loss: 0.03624026104807854\n",
      "Epoch 25781/30000 Training Loss: 0.03918758034706116\n",
      "Epoch 25782/30000 Training Loss: 0.05130555480718613\n",
      "Epoch 25783/30000 Training Loss: 0.04840824753046036\n",
      "Epoch 25784/30000 Training Loss: 0.036697085946798325\n",
      "Epoch 25785/30000 Training Loss: 0.05388501659035683\n",
      "Epoch 25786/30000 Training Loss: 0.03263348713517189\n",
      "Epoch 25787/30000 Training Loss: 0.047624289989471436\n",
      "Epoch 25788/30000 Training Loss: 0.044075388461351395\n",
      "Epoch 25789/30000 Training Loss: 0.03529515862464905\n",
      "Epoch 25790/30000 Training Loss: 0.05239323526620865\n",
      "Epoch 25791/30000 Training Loss: 0.03548354655504227\n",
      "Epoch 25792/30000 Training Loss: 0.049060702323913574\n",
      "Epoch 25793/30000 Training Loss: 0.05038723349571228\n",
      "Epoch 25794/30000 Training Loss: 0.0303119495511055\n",
      "Epoch 25795/30000 Training Loss: 0.036230042576789856\n",
      "Epoch 25796/30000 Training Loss: 0.04063553363084793\n",
      "Epoch 25797/30000 Training Loss: 0.04283063858747482\n",
      "Epoch 25798/30000 Training Loss: 0.055656373500823975\n",
      "Epoch 25799/30000 Training Loss: 0.03930135816335678\n",
      "Epoch 25800/30000 Training Loss: 0.03321336954832077\n",
      "Epoch 25800/30000 Validation Loss: 0.04933427274227142\n",
      "Epoch 25801/30000 Training Loss: 0.04428673908114433\n",
      "Epoch 25802/30000 Training Loss: 0.044540856033563614\n",
      "Epoch 25803/30000 Training Loss: 0.03435078263282776\n",
      "Epoch 25804/30000 Training Loss: 0.04560353606939316\n",
      "Epoch 25805/30000 Training Loss: 0.048182327300310135\n",
      "Epoch 25806/30000 Training Loss: 0.04152728244662285\n",
      "Epoch 25807/30000 Training Loss: 0.056053489446640015\n",
      "Epoch 25808/30000 Training Loss: 0.050111155956983566\n",
      "Epoch 25809/30000 Training Loss: 0.05296424403786659\n",
      "Epoch 25810/30000 Training Loss: 0.04103061929345131\n",
      "Epoch 25811/30000 Training Loss: 0.03354432433843613\n",
      "Epoch 25812/30000 Training Loss: 0.04314056411385536\n",
      "Epoch 25813/30000 Training Loss: 0.05226746201515198\n",
      "Epoch 25814/30000 Training Loss: 0.04870356619358063\n",
      "Epoch 25815/30000 Training Loss: 0.04798818379640579\n",
      "Epoch 25816/30000 Training Loss: 0.047755930572748184\n",
      "Epoch 25817/30000 Training Loss: 0.048601821064949036\n",
      "Epoch 25818/30000 Training Loss: 0.04508131742477417\n",
      "Epoch 25819/30000 Training Loss: 0.04360430687665939\n",
      "Epoch 25820/30000 Training Loss: 0.03160719573497772\n",
      "Epoch 25821/30000 Training Loss: 0.06602542847394943\n",
      "Epoch 25822/30000 Training Loss: 0.05071920156478882\n",
      "Epoch 25823/30000 Training Loss: 0.03661937266588211\n",
      "Epoch 25824/30000 Training Loss: 0.046358317136764526\n",
      "Epoch 25825/30000 Training Loss: 0.04874855652451515\n",
      "Epoch 25826/30000 Training Loss: 0.038332484662532806\n",
      "Epoch 25827/30000 Training Loss: 0.04672416299581528\n",
      "Epoch 25828/30000 Training Loss: 0.038651324808597565\n",
      "Epoch 25829/30000 Training Loss: 0.03637783229351044\n",
      "Epoch 25830/30000 Training Loss: 0.046761661767959595\n",
      "Epoch 25831/30000 Training Loss: 0.03452587500214577\n",
      "Epoch 25832/30000 Training Loss: 0.03416144847869873\n",
      "Epoch 25833/30000 Training Loss: 0.05018211901187897\n",
      "Epoch 25834/30000 Training Loss: 0.04813703894615173\n",
      "Epoch 25835/30000 Training Loss: 0.040786415338516235\n",
      "Epoch 25836/30000 Training Loss: 0.04478397220373154\n",
      "Epoch 25837/30000 Training Loss: 0.04150277376174927\n",
      "Epoch 25838/30000 Training Loss: 0.05375604331493378\n",
      "Epoch 25839/30000 Training Loss: 0.03477569669485092\n",
      "Epoch 25840/30000 Training Loss: 0.04319017753005028\n",
      "Epoch 25841/30000 Training Loss: 0.0331929549574852\n",
      "Epoch 25842/30000 Training Loss: 0.04061264172196388\n",
      "Epoch 25843/30000 Training Loss: 0.041046593338251114\n",
      "Epoch 25844/30000 Training Loss: 0.046928636729717255\n",
      "Epoch 25845/30000 Training Loss: 0.034103259444236755\n",
      "Epoch 25846/30000 Training Loss: 0.034504760056734085\n",
      "Epoch 25847/30000 Training Loss: 0.04832544922828674\n",
      "Epoch 25848/30000 Training Loss: 0.04007694125175476\n",
      "Epoch 25849/30000 Training Loss: 0.055989570915699005\n",
      "Epoch 25850/30000 Training Loss: 0.031035570427775383\n",
      "Epoch 25851/30000 Training Loss: 0.03964570537209511\n",
      "Epoch 25852/30000 Training Loss: 0.04573693126440048\n",
      "Epoch 25853/30000 Training Loss: 0.04215911775827408\n",
      "Epoch 25854/30000 Training Loss: 0.047428011894226074\n",
      "Epoch 25855/30000 Training Loss: 0.034585483372211456\n",
      "Epoch 25856/30000 Training Loss: 0.037069037556648254\n",
      "Epoch 25857/30000 Training Loss: 0.02812015265226364\n",
      "Epoch 25858/30000 Training Loss: 0.04125778004527092\n",
      "Epoch 25859/30000 Training Loss: 0.042041342705488205\n",
      "Epoch 25860/30000 Training Loss: 0.05500607192516327\n",
      "Epoch 25861/30000 Training Loss: 0.04561746492981911\n",
      "Epoch 25862/30000 Training Loss: 0.04132485017180443\n",
      "Epoch 25863/30000 Training Loss: 0.042040951550006866\n",
      "Epoch 25864/30000 Training Loss: 0.04599008336663246\n",
      "Epoch 25865/30000 Training Loss: 0.04780583828687668\n",
      "Epoch 25866/30000 Training Loss: 0.03527246415615082\n",
      "Epoch 25867/30000 Training Loss: 0.048456624150276184\n",
      "Epoch 25868/30000 Training Loss: 0.03962761163711548\n",
      "Epoch 25869/30000 Training Loss: 0.0341108962893486\n",
      "Epoch 25870/30000 Training Loss: 0.048014745116233826\n",
      "Epoch 25871/30000 Training Loss: 0.040421418845653534\n",
      "Epoch 25872/30000 Training Loss: 0.045226018875837326\n",
      "Epoch 25873/30000 Training Loss: 0.04433686286211014\n",
      "Epoch 25874/30000 Training Loss: 0.04632866010069847\n",
      "Epoch 25875/30000 Training Loss: 0.03155522048473358\n",
      "Epoch 25876/30000 Training Loss: 0.04279004782438278\n",
      "Epoch 25877/30000 Training Loss: 0.06565242260694504\n",
      "Epoch 25878/30000 Training Loss: 0.04290352761745453\n",
      "Epoch 25879/30000 Training Loss: 0.02920382469892502\n",
      "Epoch 25880/30000 Training Loss: 0.04093114286661148\n",
      "Epoch 25881/30000 Training Loss: 0.04343181103467941\n",
      "Epoch 25882/30000 Training Loss: 0.04127024486660957\n",
      "Epoch 25883/30000 Training Loss: 0.0527581050992012\n",
      "Epoch 25884/30000 Training Loss: 0.04463304579257965\n",
      "Epoch 25885/30000 Training Loss: 0.03879701346158981\n",
      "Epoch 25886/30000 Training Loss: 0.05197303742170334\n",
      "Epoch 25887/30000 Training Loss: 0.041019558906555176\n",
      "Epoch 25888/30000 Training Loss: 0.06218316778540611\n",
      "Epoch 25889/30000 Training Loss: 0.04213831573724747\n",
      "Epoch 25890/30000 Training Loss: 0.03775886446237564\n",
      "Epoch 25891/30000 Training Loss: 0.04963628947734833\n",
      "Epoch 25892/30000 Training Loss: 0.04164337366819382\n",
      "Epoch 25893/30000 Training Loss: 0.05669873207807541\n",
      "Epoch 25894/30000 Training Loss: 0.047208789736032486\n",
      "Epoch 25895/30000 Training Loss: 0.052024390548467636\n",
      "Epoch 25896/30000 Training Loss: 0.04709848016500473\n",
      "Epoch 25897/30000 Training Loss: 0.043628208339214325\n",
      "Epoch 25898/30000 Training Loss: 0.04692119359970093\n",
      "Epoch 25899/30000 Training Loss: 0.03324110805988312\n",
      "Epoch 25900/30000 Training Loss: 0.05245598405599594\n",
      "Epoch 25900/30000 Validation Loss: 0.03627973049879074\n",
      "Epoch 25901/30000 Training Loss: 0.0330793596804142\n",
      "Epoch 25902/30000 Training Loss: 0.04035438597202301\n",
      "Epoch 25903/30000 Training Loss: 0.05441456288099289\n",
      "Epoch 25904/30000 Training Loss: 0.042164333164691925\n",
      "Epoch 25905/30000 Training Loss: 0.04375597834587097\n",
      "Epoch 25906/30000 Training Loss: 0.047627732157707214\n",
      "Epoch 25907/30000 Training Loss: 0.03254372626543045\n",
      "Epoch 25908/30000 Training Loss: 0.0389317087829113\n",
      "Epoch 25909/30000 Training Loss: 0.049270905554294586\n",
      "Epoch 25910/30000 Training Loss: 0.04562840610742569\n",
      "Epoch 25911/30000 Training Loss: 0.05247128754854202\n",
      "Epoch 25912/30000 Training Loss: 0.03893955051898956\n",
      "Epoch 25913/30000 Training Loss: 0.04182286560535431\n",
      "Epoch 25914/30000 Training Loss: 0.04485722631216049\n",
      "Epoch 25915/30000 Training Loss: 0.038448259234428406\n",
      "Epoch 25916/30000 Training Loss: 0.028967874124646187\n",
      "Epoch 25917/30000 Training Loss: 0.04979659244418144\n",
      "Epoch 25918/30000 Training Loss: 0.04264133423566818\n",
      "Epoch 25919/30000 Training Loss: 0.04391932487487793\n",
      "Epoch 25920/30000 Training Loss: 0.04051710665225983\n",
      "Epoch 25921/30000 Training Loss: 0.04832160472869873\n",
      "Epoch 25922/30000 Training Loss: 0.051258452236652374\n",
      "Epoch 25923/30000 Training Loss: 0.041344717144966125\n",
      "Epoch 25924/30000 Training Loss: 0.03089742362499237\n",
      "Epoch 25925/30000 Training Loss: 0.040234219282865524\n",
      "Epoch 25926/30000 Training Loss: 0.04275074601173401\n",
      "Epoch 25927/30000 Training Loss: 0.050993386656045914\n",
      "Epoch 25928/30000 Training Loss: 0.03568827360868454\n",
      "Epoch 25929/30000 Training Loss: 0.04670817404985428\n",
      "Epoch 25930/30000 Training Loss: 0.05701401084661484\n",
      "Epoch 25931/30000 Training Loss: 0.044945456087589264\n",
      "Epoch 25932/30000 Training Loss: 0.045515045523643494\n",
      "Epoch 25933/30000 Training Loss: 0.04428974539041519\n",
      "Epoch 25934/30000 Training Loss: 0.044624269008636475\n",
      "Epoch 25935/30000 Training Loss: 0.033485136926174164\n",
      "Epoch 25936/30000 Training Loss: 0.04632817953824997\n",
      "Epoch 25937/30000 Training Loss: 0.04178644344210625\n",
      "Epoch 25938/30000 Training Loss: 0.041671156883239746\n",
      "Epoch 25939/30000 Training Loss: 0.05175691843032837\n",
      "Epoch 25940/30000 Training Loss: 0.045941490679979324\n",
      "Epoch 25941/30000 Training Loss: 0.03811519593000412\n",
      "Epoch 25942/30000 Training Loss: 0.03597601503133774\n",
      "Epoch 25943/30000 Training Loss: 0.05391905456781387\n",
      "Epoch 25944/30000 Training Loss: 0.03435882180929184\n",
      "Epoch 25945/30000 Training Loss: 0.0425146222114563\n",
      "Epoch 25946/30000 Training Loss: 0.034115392714738846\n",
      "Epoch 25947/30000 Training Loss: 0.0447482094168663\n",
      "Epoch 25948/30000 Training Loss: 0.04114587604999542\n",
      "Epoch 25949/30000 Training Loss: 0.04089837521314621\n",
      "Epoch 25950/30000 Training Loss: 0.04307444393634796\n",
      "Epoch 25951/30000 Training Loss: 0.04930393397808075\n",
      "Epoch 25952/30000 Training Loss: 0.042267508804798126\n",
      "Epoch 25953/30000 Training Loss: 0.03271903097629547\n",
      "Epoch 25954/30000 Training Loss: 0.028417473658919334\n",
      "Epoch 25955/30000 Training Loss: 0.034668367356061935\n",
      "Epoch 25956/30000 Training Loss: 0.04645533859729767\n",
      "Epoch 25957/30000 Training Loss: 0.041635096073150635\n",
      "Epoch 25958/30000 Training Loss: 0.04678378999233246\n",
      "Epoch 25959/30000 Training Loss: 0.046201370656490326\n",
      "Epoch 25960/30000 Training Loss: 0.04032532870769501\n",
      "Epoch 25961/30000 Training Loss: 0.042524442076683044\n",
      "Epoch 25962/30000 Training Loss: 0.04601345956325531\n",
      "Epoch 25963/30000 Training Loss: 0.04160288721323013\n",
      "Epoch 25964/30000 Training Loss: 0.03873300552368164\n",
      "Epoch 25965/30000 Training Loss: 0.0405740812420845\n",
      "Epoch 25966/30000 Training Loss: 0.046149998903274536\n",
      "Epoch 25967/30000 Training Loss: 0.04427385702729225\n",
      "Epoch 25968/30000 Training Loss: 0.05603907257318497\n",
      "Epoch 25969/30000 Training Loss: 0.03586819767951965\n",
      "Epoch 25970/30000 Training Loss: 0.03941437974572182\n",
      "Epoch 25971/30000 Training Loss: 0.04583007097244263\n",
      "Epoch 25972/30000 Training Loss: 0.04894386604428291\n",
      "Epoch 25973/30000 Training Loss: 0.044182635843753815\n",
      "Epoch 25974/30000 Training Loss: 0.04831957072019577\n",
      "Epoch 25975/30000 Training Loss: 0.040534816682338715\n",
      "Epoch 25976/30000 Training Loss: 0.034762702882289886\n",
      "Epoch 25977/30000 Training Loss: 0.04228082671761513\n",
      "Epoch 25978/30000 Training Loss: 0.05446936935186386\n",
      "Epoch 25979/30000 Training Loss: 0.05132036283612251\n",
      "Epoch 25980/30000 Training Loss: 0.0451338067650795\n",
      "Epoch 25981/30000 Training Loss: 0.04763716086745262\n",
      "Epoch 25982/30000 Training Loss: 0.048674002289772034\n",
      "Epoch 25983/30000 Training Loss: 0.045935023576021194\n",
      "Epoch 25984/30000 Training Loss: 0.038317687809467316\n",
      "Epoch 25985/30000 Training Loss: 0.039784789085388184\n",
      "Epoch 25986/30000 Training Loss: 0.03950255364179611\n",
      "Epoch 25987/30000 Training Loss: 0.03485634922981262\n",
      "Epoch 25988/30000 Training Loss: 0.05159127712249756\n",
      "Epoch 25989/30000 Training Loss: 0.04156170412898064\n",
      "Epoch 25990/30000 Training Loss: 0.044110968708992004\n",
      "Epoch 25991/30000 Training Loss: 0.035596128553152084\n",
      "Epoch 25992/30000 Training Loss: 0.04534737393260002\n",
      "Epoch 25993/30000 Training Loss: 0.04827536270022392\n",
      "Epoch 25994/30000 Training Loss: 0.0439736545085907\n",
      "Epoch 25995/30000 Training Loss: 0.049174964427948\n",
      "Epoch 25996/30000 Training Loss: 0.03747960925102234\n",
      "Epoch 25997/30000 Training Loss: 0.03209611400961876\n",
      "Epoch 25998/30000 Training Loss: 0.042263537645339966\n",
      "Epoch 25999/30000 Training Loss: 0.0603436604142189\n",
      "Epoch 26000/30000 Training Loss: 0.052566394209861755\n",
      "Epoch 26000/30000 Validation Loss: 0.02866159752011299\n",
      "Epoch 26001/30000 Training Loss: 0.030876802280545235\n",
      "Epoch 26002/30000 Training Loss: 0.04591567814350128\n",
      "Epoch 26003/30000 Training Loss: 0.04831014946103096\n",
      "Epoch 26004/30000 Training Loss: 0.043347135186195374\n",
      "Epoch 26005/30000 Training Loss: 0.03924756497144699\n",
      "Epoch 26006/30000 Training Loss: 0.048991840332746506\n",
      "Epoch 26007/30000 Training Loss: 0.04149499535560608\n",
      "Epoch 26008/30000 Training Loss: 0.05099571868777275\n",
      "Epoch 26009/30000 Training Loss: 0.031794168055057526\n",
      "Epoch 26010/30000 Training Loss: 0.04323875531554222\n",
      "Epoch 26011/30000 Training Loss: 0.03915570676326752\n",
      "Epoch 26012/30000 Training Loss: 0.029268130660057068\n",
      "Epoch 26013/30000 Training Loss: 0.052373506128787994\n",
      "Epoch 26014/30000 Training Loss: 0.03686625137925148\n",
      "Epoch 26015/30000 Training Loss: 0.032510098069906235\n",
      "Epoch 26016/30000 Training Loss: 0.051283471286296844\n",
      "Epoch 26017/30000 Training Loss: 0.04164152219891548\n",
      "Epoch 26018/30000 Training Loss: 0.035745568573474884\n",
      "Epoch 26019/30000 Training Loss: 0.04228903353214264\n",
      "Epoch 26020/30000 Training Loss: 0.04191803187131882\n",
      "Epoch 26021/30000 Training Loss: 0.04214625060558319\n",
      "Epoch 26022/30000 Training Loss: 0.04974067211151123\n",
      "Epoch 26023/30000 Training Loss: 0.05397117882966995\n",
      "Epoch 26024/30000 Training Loss: 0.05107765644788742\n",
      "Epoch 26025/30000 Training Loss: 0.04616847634315491\n",
      "Epoch 26026/30000 Training Loss: 0.02980644255876541\n",
      "Epoch 26027/30000 Training Loss: 0.04130704700946808\n",
      "Epoch 26028/30000 Training Loss: 0.05397929251194\n",
      "Epoch 26029/30000 Training Loss: 0.037400759756565094\n",
      "Epoch 26030/30000 Training Loss: 0.03943350166082382\n",
      "Epoch 26031/30000 Training Loss: 0.04534447193145752\n",
      "Epoch 26032/30000 Training Loss: 0.03213765472173691\n",
      "Epoch 26033/30000 Training Loss: 0.03951113671064377\n",
      "Epoch 26034/30000 Training Loss: 0.038565948605537415\n",
      "Epoch 26035/30000 Training Loss: 0.055436521768569946\n",
      "Epoch 26036/30000 Training Loss: 0.039231397211551666\n",
      "Epoch 26037/30000 Training Loss: 0.03667347878217697\n",
      "Epoch 26038/30000 Training Loss: 0.03622054308652878\n",
      "Epoch 26039/30000 Training Loss: 0.04783520847558975\n",
      "Epoch 26040/30000 Training Loss: 0.0415961816906929\n",
      "Epoch 26041/30000 Training Loss: 0.031679872423410416\n",
      "Epoch 26042/30000 Training Loss: 0.042787209153175354\n",
      "Epoch 26043/30000 Training Loss: 0.0367063507437706\n",
      "Epoch 26044/30000 Training Loss: 0.042136628180742264\n",
      "Epoch 26045/30000 Training Loss: 0.031006373465061188\n",
      "Epoch 26046/30000 Training Loss: 0.048888809978961945\n",
      "Epoch 26047/30000 Training Loss: 0.04320092871785164\n",
      "Epoch 26048/30000 Training Loss: 0.04984721913933754\n",
      "Epoch 26049/30000 Training Loss: 0.044723935425281525\n",
      "Epoch 26050/30000 Training Loss: 0.04605739191174507\n",
      "Epoch 26051/30000 Training Loss: 0.033770859241485596\n",
      "Epoch 26052/30000 Training Loss: 0.0398222915828228\n",
      "Epoch 26053/30000 Training Loss: 0.028350472450256348\n",
      "Epoch 26054/30000 Training Loss: 0.0432492233812809\n",
      "Epoch 26055/30000 Training Loss: 0.04215753823518753\n",
      "Epoch 26056/30000 Training Loss: 0.03476985543966293\n",
      "Epoch 26057/30000 Training Loss: 0.04387882724404335\n",
      "Epoch 26058/30000 Training Loss: 0.04016083851456642\n",
      "Epoch 26059/30000 Training Loss: 0.049720197916030884\n",
      "Epoch 26060/30000 Training Loss: 0.03307866305112839\n",
      "Epoch 26061/30000 Training Loss: 0.046956874430179596\n",
      "Epoch 26062/30000 Training Loss: 0.034530941396951675\n",
      "Epoch 26063/30000 Training Loss: 0.03555729240179062\n",
      "Epoch 26064/30000 Training Loss: 0.027339234948158264\n",
      "Epoch 26065/30000 Training Loss: 0.047949228435754776\n",
      "Epoch 26066/30000 Training Loss: 0.036524903029203415\n",
      "Epoch 26067/30000 Training Loss: 0.04272887110710144\n",
      "Epoch 26068/30000 Training Loss: 0.041620172560214996\n",
      "Epoch 26069/30000 Training Loss: 0.043885186314582825\n",
      "Epoch 26070/30000 Training Loss: 0.03389442712068558\n",
      "Epoch 26071/30000 Training Loss: 0.030039368197321892\n",
      "Epoch 26072/30000 Training Loss: 0.045540131628513336\n",
      "Epoch 26073/30000 Training Loss: 0.04651821777224541\n",
      "Epoch 26074/30000 Training Loss: 0.03550499677658081\n",
      "Epoch 26075/30000 Training Loss: 0.029507718980312347\n",
      "Epoch 26076/30000 Training Loss: 0.03187927231192589\n",
      "Epoch 26077/30000 Training Loss: 0.04216170310974121\n",
      "Epoch 26078/30000 Training Loss: 0.043284133076667786\n",
      "Epoch 26079/30000 Training Loss: 0.034107375890016556\n",
      "Epoch 26080/30000 Training Loss: 0.035554252564907074\n",
      "Epoch 26081/30000 Training Loss: 0.03442460298538208\n",
      "Epoch 26082/30000 Training Loss: 0.037375640124082565\n",
      "Epoch 26083/30000 Training Loss: 0.03390519320964813\n",
      "Epoch 26084/30000 Training Loss: 0.05715509504079819\n",
      "Epoch 26085/30000 Training Loss: 0.0508592315018177\n",
      "Epoch 26086/30000 Training Loss: 0.04054676741361618\n",
      "Epoch 26087/30000 Training Loss: 0.042365796864032745\n",
      "Epoch 26088/30000 Training Loss: 0.03218935802578926\n",
      "Epoch 26089/30000 Training Loss: 0.03336038067936897\n",
      "Epoch 26090/30000 Training Loss: 0.05145363509654999\n",
      "Epoch 26091/30000 Training Loss: 0.033464204519987106\n",
      "Epoch 26092/30000 Training Loss: 0.037878528237342834\n",
      "Epoch 26093/30000 Training Loss: 0.04060618579387665\n",
      "Epoch 26094/30000 Training Loss: 0.04263140633702278\n",
      "Epoch 26095/30000 Training Loss: 0.06353381276130676\n",
      "Epoch 26096/30000 Training Loss: 0.035239800810813904\n",
      "Epoch 26097/30000 Training Loss: 0.03937743604183197\n",
      "Epoch 26098/30000 Training Loss: 0.03672568500041962\n",
      "Epoch 26099/30000 Training Loss: 0.05137157440185547\n",
      "Epoch 26100/30000 Training Loss: 0.038682080805301666\n",
      "Epoch 26100/30000 Validation Loss: 0.04192693531513214\n",
      "Epoch 26101/30000 Training Loss: 0.05465763807296753\n",
      "Epoch 26102/30000 Training Loss: 0.043139614164829254\n",
      "Epoch 26103/30000 Training Loss: 0.03706134483218193\n",
      "Epoch 26104/30000 Training Loss: 0.041891224682331085\n",
      "Epoch 26105/30000 Training Loss: 0.04210244491696358\n",
      "Epoch 26106/30000 Training Loss: 0.044369056820869446\n",
      "Epoch 26107/30000 Training Loss: 0.053964681923389435\n",
      "Epoch 26108/30000 Training Loss: 0.04570832848548889\n",
      "Epoch 26109/30000 Training Loss: 0.03706412762403488\n",
      "Epoch 26110/30000 Training Loss: 0.05545806512236595\n",
      "Epoch 26111/30000 Training Loss: 0.036824021488428116\n",
      "Epoch 26112/30000 Training Loss: 0.043523285537958145\n",
      "Epoch 26113/30000 Training Loss: 0.038519904017448425\n",
      "Epoch 26114/30000 Training Loss: 0.037463895976543427\n",
      "Epoch 26115/30000 Training Loss: 0.04629301279783249\n",
      "Epoch 26116/30000 Training Loss: 0.04798727110028267\n",
      "Epoch 26117/30000 Training Loss: 0.047616735100746155\n",
      "Epoch 26118/30000 Training Loss: 0.0350719690322876\n",
      "Epoch 26119/30000 Training Loss: 0.04024241119623184\n",
      "Epoch 26120/30000 Training Loss: 0.035779986530542374\n",
      "Epoch 26121/30000 Training Loss: 0.042781710624694824\n",
      "Epoch 26122/30000 Training Loss: 0.028903817757964134\n",
      "Epoch 26123/30000 Training Loss: 0.032070342451334\n",
      "Epoch 26124/30000 Training Loss: 0.052519652992486954\n",
      "Epoch 26125/30000 Training Loss: 0.039116229861974716\n",
      "Epoch 26126/30000 Training Loss: 0.06225007027387619\n",
      "Epoch 26127/30000 Training Loss: 0.04490867257118225\n",
      "Epoch 26128/30000 Training Loss: 0.044017255306243896\n",
      "Epoch 26129/30000 Training Loss: 0.039601389318704605\n",
      "Epoch 26130/30000 Training Loss: 0.035781942307949066\n",
      "Epoch 26131/30000 Training Loss: 0.04710943251848221\n",
      "Epoch 26132/30000 Training Loss: 0.03352843225002289\n",
      "Epoch 26133/30000 Training Loss: 0.035764675587415695\n",
      "Epoch 26134/30000 Training Loss: 0.04487009719014168\n",
      "Epoch 26135/30000 Training Loss: 0.05067246034741402\n",
      "Epoch 26136/30000 Training Loss: 0.041073277592659\n",
      "Epoch 26137/30000 Training Loss: 0.03211648017168045\n",
      "Epoch 26138/30000 Training Loss: 0.03940485417842865\n",
      "Epoch 26139/30000 Training Loss: 0.04049190878868103\n",
      "Epoch 26140/30000 Training Loss: 0.05299058184027672\n",
      "Epoch 26141/30000 Training Loss: 0.06298786401748657\n",
      "Epoch 26142/30000 Training Loss: 0.030135471373796463\n",
      "Epoch 26143/30000 Training Loss: 0.03388984501361847\n",
      "Epoch 26144/30000 Training Loss: 0.054649803787469864\n",
      "Epoch 26145/30000 Training Loss: 0.041156038641929626\n",
      "Epoch 26146/30000 Training Loss: 0.030821651220321655\n",
      "Epoch 26147/30000 Training Loss: 0.04447794705629349\n",
      "Epoch 26148/30000 Training Loss: 0.0412040576338768\n",
      "Epoch 26149/30000 Training Loss: 0.044240206480026245\n",
      "Epoch 26150/30000 Training Loss: 0.05980464816093445\n",
      "Epoch 26151/30000 Training Loss: 0.03187406435608864\n",
      "Epoch 26152/30000 Training Loss: 0.05279886722564697\n",
      "Epoch 26153/30000 Training Loss: 0.0298762209713459\n",
      "Epoch 26154/30000 Training Loss: 0.05310538411140442\n",
      "Epoch 26155/30000 Training Loss: 0.0651126503944397\n",
      "Epoch 26156/30000 Training Loss: 0.03804931789636612\n",
      "Epoch 26157/30000 Training Loss: 0.04936433210968971\n",
      "Epoch 26158/30000 Training Loss: 0.039538927376270294\n",
      "Epoch 26159/30000 Training Loss: 0.0318414643406868\n",
      "Epoch 26160/30000 Training Loss: 0.042502835392951965\n",
      "Epoch 26161/30000 Training Loss: 0.04743503779172897\n",
      "Epoch 26162/30000 Training Loss: 0.026824872940778732\n",
      "Epoch 26163/30000 Training Loss: 0.03835545480251312\n",
      "Epoch 26164/30000 Training Loss: 0.05060315132141113\n",
      "Epoch 26165/30000 Training Loss: 0.04234706982970238\n",
      "Epoch 26166/30000 Training Loss: 0.04527802765369415\n",
      "Epoch 26167/30000 Training Loss: 0.038811102509498596\n",
      "Epoch 26168/30000 Training Loss: 0.04551173746585846\n",
      "Epoch 26169/30000 Training Loss: 0.04302594065666199\n",
      "Epoch 26170/30000 Training Loss: 0.04431688040494919\n",
      "Epoch 26171/30000 Training Loss: 0.0512278787791729\n",
      "Epoch 26172/30000 Training Loss: 0.05779162794351578\n",
      "Epoch 26173/30000 Training Loss: 0.04648229852318764\n",
      "Epoch 26174/30000 Training Loss: 0.03354998677968979\n",
      "Epoch 26175/30000 Training Loss: 0.0508277490735054\n",
      "Epoch 26176/30000 Training Loss: 0.03486507758498192\n",
      "Epoch 26177/30000 Training Loss: 0.04042404890060425\n",
      "Epoch 26178/30000 Training Loss: 0.03769785910844803\n",
      "Epoch 26179/30000 Training Loss: 0.029428668320178986\n",
      "Epoch 26180/30000 Training Loss: 0.04625019058585167\n",
      "Epoch 26181/30000 Training Loss: 0.043733954429626465\n",
      "Epoch 26182/30000 Training Loss: 0.04460993409156799\n",
      "Epoch 26183/30000 Training Loss: 0.04238804429769516\n",
      "Epoch 26184/30000 Training Loss: 0.05089964717626572\n",
      "Epoch 26185/30000 Training Loss: 0.03749333694577217\n",
      "Epoch 26186/30000 Training Loss: 0.036772340536117554\n",
      "Epoch 26187/30000 Training Loss: 0.03229214996099472\n",
      "Epoch 26188/30000 Training Loss: 0.04730985313653946\n",
      "Epoch 26189/30000 Training Loss: 0.03764919936656952\n",
      "Epoch 26190/30000 Training Loss: 0.051092229783535004\n",
      "Epoch 26191/30000 Training Loss: 0.03754604607820511\n",
      "Epoch 26192/30000 Training Loss: 0.03718414902687073\n",
      "Epoch 26193/30000 Training Loss: 0.034784603863954544\n",
      "Epoch 26194/30000 Training Loss: 0.05648009479045868\n",
      "Epoch 26195/30000 Training Loss: 0.03969002887606621\n",
      "Epoch 26196/30000 Training Loss: 0.03299577161669731\n",
      "Epoch 26197/30000 Training Loss: 0.03484087809920311\n",
      "Epoch 26198/30000 Training Loss: 0.03590739518404007\n",
      "Epoch 26199/30000 Training Loss: 0.046549901366233826\n",
      "Epoch 26200/30000 Training Loss: 0.05651387944817543\n",
      "Epoch 26200/30000 Validation Loss: 0.03623746335506439\n",
      "Epoch 26201/30000 Training Loss: 0.031121855601668358\n",
      "Epoch 26202/30000 Training Loss: 0.027341514825820923\n",
      "Epoch 26203/30000 Training Loss: 0.03637928515672684\n",
      "Epoch 26204/30000 Training Loss: 0.041614800691604614\n",
      "Epoch 26205/30000 Training Loss: 0.04214911162853241\n",
      "Epoch 26206/30000 Training Loss: 0.04230192303657532\n",
      "Epoch 26207/30000 Training Loss: 0.04126465320587158\n",
      "Epoch 26208/30000 Training Loss: 0.03534851223230362\n",
      "Epoch 26209/30000 Training Loss: 0.043132584542036057\n",
      "Epoch 26210/30000 Training Loss: 0.044498614966869354\n",
      "Epoch 26211/30000 Training Loss: 0.031065087765455246\n",
      "Epoch 26212/30000 Training Loss: 0.041352249681949615\n",
      "Epoch 26213/30000 Training Loss: 0.03922094404697418\n",
      "Epoch 26214/30000 Training Loss: 0.03495708853006363\n",
      "Epoch 26215/30000 Training Loss: 0.04031386971473694\n",
      "Epoch 26216/30000 Training Loss: 0.0454513318836689\n",
      "Epoch 26217/30000 Training Loss: 0.04067341238260269\n",
      "Epoch 26218/30000 Training Loss: 0.03706590458750725\n",
      "Epoch 26219/30000 Training Loss: 0.04073326289653778\n",
      "Epoch 26220/30000 Training Loss: 0.033401478081941605\n",
      "Epoch 26221/30000 Training Loss: 0.03962510824203491\n",
      "Epoch 26222/30000 Training Loss: 0.05364040285348892\n",
      "Epoch 26223/30000 Training Loss: 0.043495647609233856\n",
      "Epoch 26224/30000 Training Loss: 0.03898562490940094\n",
      "Epoch 26225/30000 Training Loss: 0.033338502049446106\n",
      "Epoch 26226/30000 Training Loss: 0.05150012671947479\n",
      "Epoch 26227/30000 Training Loss: 0.031388167291879654\n",
      "Epoch 26228/30000 Training Loss: 0.0465700626373291\n",
      "Epoch 26229/30000 Training Loss: 0.03358173742890358\n",
      "Epoch 26230/30000 Training Loss: 0.04042353108525276\n",
      "Epoch 26231/30000 Training Loss: 0.05233852192759514\n",
      "Epoch 26232/30000 Training Loss: 0.050381068140268326\n",
      "Epoch 26233/30000 Training Loss: 0.03425438702106476\n",
      "Epoch 26234/30000 Training Loss: 0.05096397548913956\n",
      "Epoch 26235/30000 Training Loss: 0.05282369628548622\n",
      "Epoch 26236/30000 Training Loss: 0.05841328203678131\n",
      "Epoch 26237/30000 Training Loss: 0.052494361996650696\n",
      "Epoch 26238/30000 Training Loss: 0.05281943082809448\n",
      "Epoch 26239/30000 Training Loss: 0.04579099267721176\n",
      "Epoch 26240/30000 Training Loss: 0.04417181760072708\n",
      "Epoch 26241/30000 Training Loss: 0.05007065087556839\n",
      "Epoch 26242/30000 Training Loss: 0.03886283561587334\n",
      "Epoch 26243/30000 Training Loss: 0.0449448823928833\n",
      "Epoch 26244/30000 Training Loss: 0.04248251020908356\n",
      "Epoch 26245/30000 Training Loss: 0.033245548605918884\n",
      "Epoch 26246/30000 Training Loss: 0.039174601435661316\n",
      "Epoch 26247/30000 Training Loss: 0.06622787564992905\n",
      "Epoch 26248/30000 Training Loss: 0.0472220778465271\n",
      "Epoch 26249/30000 Training Loss: 0.0406552329659462\n",
      "Epoch 26250/30000 Training Loss: 0.03897513449192047\n",
      "Epoch 26251/30000 Training Loss: 0.048512380570173264\n",
      "Epoch 26252/30000 Training Loss: 0.039276428520679474\n",
      "Epoch 26253/30000 Training Loss: 0.04696996510028839\n",
      "Epoch 26254/30000 Training Loss: 0.05138562619686127\n",
      "Epoch 26255/30000 Training Loss: 0.049932535737752914\n",
      "Epoch 26256/30000 Training Loss: 0.05806483328342438\n",
      "Epoch 26257/30000 Training Loss: 0.046215955168008804\n",
      "Epoch 26258/30000 Training Loss: 0.04051234573125839\n",
      "Epoch 26259/30000 Training Loss: 0.03807453438639641\n",
      "Epoch 26260/30000 Training Loss: 0.04691073298454285\n",
      "Epoch 26261/30000 Training Loss: 0.035545751452445984\n",
      "Epoch 26262/30000 Training Loss: 0.03703586012125015\n",
      "Epoch 26263/30000 Training Loss: 0.05481625720858574\n",
      "Epoch 26264/30000 Training Loss: 0.04345425218343735\n",
      "Epoch 26265/30000 Training Loss: 0.03951127454638481\n",
      "Epoch 26266/30000 Training Loss: 0.052477311342954636\n",
      "Epoch 26267/30000 Training Loss: 0.0455734021961689\n",
      "Epoch 26268/30000 Training Loss: 0.034191008657217026\n",
      "Epoch 26269/30000 Training Loss: 0.040507636964321136\n",
      "Epoch 26270/30000 Training Loss: 0.0519389808177948\n",
      "Epoch 26271/30000 Training Loss: 0.029226234182715416\n",
      "Epoch 26272/30000 Training Loss: 0.050246644765138626\n",
      "Epoch 26273/30000 Training Loss: 0.0349414199590683\n",
      "Epoch 26274/30000 Training Loss: 0.044899582862854004\n",
      "Epoch 26275/30000 Training Loss: 0.03449220955371857\n",
      "Epoch 26276/30000 Training Loss: 0.0446501150727272\n",
      "Epoch 26277/30000 Training Loss: 0.039182811975479126\n",
      "Epoch 26278/30000 Training Loss: 0.044435180723667145\n",
      "Epoch 26279/30000 Training Loss: 0.0517427995800972\n",
      "Epoch 26280/30000 Training Loss: 0.04580581188201904\n",
      "Epoch 26281/30000 Training Loss: 0.037870511412620544\n",
      "Epoch 26282/30000 Training Loss: 0.03533845394849777\n",
      "Epoch 26283/30000 Training Loss: 0.043359287083148956\n",
      "Epoch 26284/30000 Training Loss: 0.03352245315909386\n",
      "Epoch 26285/30000 Training Loss: 0.0365341454744339\n",
      "Epoch 26286/30000 Training Loss: 0.049505241215229034\n",
      "Epoch 26287/30000 Training Loss: 0.04583723843097687\n",
      "Epoch 26288/30000 Training Loss: 0.03848670423030853\n",
      "Epoch 26289/30000 Training Loss: 0.04700634256005287\n",
      "Epoch 26290/30000 Training Loss: 0.054555315524339676\n",
      "Epoch 26291/30000 Training Loss: 0.044267863035202026\n",
      "Epoch 26292/30000 Training Loss: 0.05443006753921509\n",
      "Epoch 26293/30000 Training Loss: 0.039605267345905304\n",
      "Epoch 26294/30000 Training Loss: 0.04209621250629425\n",
      "Epoch 26295/30000 Training Loss: 0.044624533504247665\n",
      "Epoch 26296/30000 Training Loss: 0.032878126949071884\n",
      "Epoch 26297/30000 Training Loss: 0.039026156067848206\n",
      "Epoch 26298/30000 Training Loss: 0.05364225432276726\n",
      "Epoch 26299/30000 Training Loss: 0.03730658069252968\n",
      "Epoch 26300/30000 Training Loss: 0.04427328333258629\n",
      "Epoch 26300/30000 Validation Loss: 0.04283036291599274\n",
      "Epoch 26301/30000 Training Loss: 0.04998983442783356\n",
      "Epoch 26302/30000 Training Loss: 0.03827531635761261\n",
      "Epoch 26303/30000 Training Loss: 0.04060324281454086\n",
      "Epoch 26304/30000 Training Loss: 0.04164017736911774\n",
      "Epoch 26305/30000 Training Loss: 0.0500137135386467\n",
      "Epoch 26306/30000 Training Loss: 0.058014776557683945\n",
      "Epoch 26307/30000 Training Loss: 0.05513394996523857\n",
      "Epoch 26308/30000 Training Loss: 0.05124098062515259\n",
      "Epoch 26309/30000 Training Loss: 0.0464453250169754\n",
      "Epoch 26310/30000 Training Loss: 0.032393306493759155\n",
      "Epoch 26311/30000 Training Loss: 0.05464260280132294\n",
      "Epoch 26312/30000 Training Loss: 0.028377674520015717\n",
      "Epoch 26313/30000 Training Loss: 0.04177284985780716\n",
      "Epoch 26314/30000 Training Loss: 0.04128719121217728\n",
      "Epoch 26315/30000 Training Loss: 0.04410445690155029\n",
      "Epoch 26316/30000 Training Loss: 0.04786352068185806\n",
      "Epoch 26317/30000 Training Loss: 0.030061911791563034\n",
      "Epoch 26318/30000 Training Loss: 0.03674020618200302\n",
      "Epoch 26319/30000 Training Loss: 0.047676343470811844\n",
      "Epoch 26320/30000 Training Loss: 0.03948362171649933\n",
      "Epoch 26321/30000 Training Loss: 0.0359952375292778\n",
      "Epoch 26322/30000 Training Loss: 0.040411774069070816\n",
      "Epoch 26323/30000 Training Loss: 0.03624977171421051\n",
      "Epoch 26324/30000 Training Loss: 0.035334136337041855\n",
      "Epoch 26325/30000 Training Loss: 0.03562944382429123\n",
      "Epoch 26326/30000 Training Loss: 0.030617941170930862\n",
      "Epoch 26327/30000 Training Loss: 0.047937214374542236\n",
      "Epoch 26328/30000 Training Loss: 0.041892729699611664\n",
      "Epoch 26329/30000 Training Loss: 0.0437915176153183\n",
      "Epoch 26330/30000 Training Loss: 0.03804764524102211\n",
      "Epoch 26331/30000 Training Loss: 0.041619088500738144\n",
      "Epoch 26332/30000 Training Loss: 0.03769547492265701\n",
      "Epoch 26333/30000 Training Loss: 0.031103264540433884\n",
      "Epoch 26334/30000 Training Loss: 0.05080753564834595\n",
      "Epoch 26335/30000 Training Loss: 0.04372018203139305\n",
      "Epoch 26336/30000 Training Loss: 0.03383847698569298\n",
      "Epoch 26337/30000 Training Loss: 0.049186475574970245\n",
      "Epoch 26338/30000 Training Loss: 0.03015856072306633\n",
      "Epoch 26339/30000 Training Loss: 0.05102379247546196\n",
      "Epoch 26340/30000 Training Loss: 0.042691394686698914\n",
      "Epoch 26341/30000 Training Loss: 0.05718855559825897\n",
      "Epoch 26342/30000 Training Loss: 0.050821755081415176\n",
      "Epoch 26343/30000 Training Loss: 0.039822354912757874\n",
      "Epoch 26344/30000 Training Loss: 0.06583663076162338\n",
      "Epoch 26345/30000 Training Loss: 0.0349358469247818\n",
      "Epoch 26346/30000 Training Loss: 0.04434685409069061\n",
      "Epoch 26347/30000 Training Loss: 0.03710070252418518\n",
      "Epoch 26348/30000 Training Loss: 0.04329838976264\n",
      "Epoch 26349/30000 Training Loss: 0.042564280331134796\n",
      "Epoch 26350/30000 Training Loss: 0.041460610926151276\n",
      "Epoch 26351/30000 Training Loss: 0.04488819092512131\n",
      "Epoch 26352/30000 Training Loss: 0.032722704112529755\n",
      "Epoch 26353/30000 Training Loss: 0.04915326088666916\n",
      "Epoch 26354/30000 Training Loss: 0.04086622595787048\n",
      "Epoch 26355/30000 Training Loss: 0.031913407146930695\n",
      "Epoch 26356/30000 Training Loss: 0.043891120702028275\n",
      "Epoch 26357/30000 Training Loss: 0.057327643036842346\n",
      "Epoch 26358/30000 Training Loss: 0.03314816951751709\n",
      "Epoch 26359/30000 Training Loss: 0.045989587903022766\n",
      "Epoch 26360/30000 Training Loss: 0.05446680635213852\n",
      "Epoch 26361/30000 Training Loss: 0.04369091987609863\n",
      "Epoch 26362/30000 Training Loss: 0.05096167325973511\n",
      "Epoch 26363/30000 Training Loss: 0.033812202513217926\n",
      "Epoch 26364/30000 Training Loss: 0.04491305723786354\n",
      "Epoch 26365/30000 Training Loss: 0.031938545405864716\n",
      "Epoch 26366/30000 Training Loss: 0.034076444804668427\n",
      "Epoch 26367/30000 Training Loss: 0.04357624053955078\n",
      "Epoch 26368/30000 Training Loss: 0.04158322513103485\n",
      "Epoch 26369/30000 Training Loss: 0.0582859180867672\n",
      "Epoch 26370/30000 Training Loss: 0.03893807530403137\n",
      "Epoch 26371/30000 Training Loss: 0.05568742752075195\n",
      "Epoch 26372/30000 Training Loss: 0.04918041080236435\n",
      "Epoch 26373/30000 Training Loss: 0.03723526746034622\n",
      "Epoch 26374/30000 Training Loss: 0.043559834361076355\n",
      "Epoch 26375/30000 Training Loss: 0.03535108640789986\n",
      "Epoch 26376/30000 Training Loss: 0.051167916506528854\n",
      "Epoch 26377/30000 Training Loss: 0.04493650421500206\n",
      "Epoch 26378/30000 Training Loss: 0.057818666100502014\n",
      "Epoch 26379/30000 Training Loss: 0.044839974492788315\n",
      "Epoch 26380/30000 Training Loss: 0.041549213230609894\n",
      "Epoch 26381/30000 Training Loss: 0.04800296574831009\n",
      "Epoch 26382/30000 Training Loss: 0.04356979578733444\n",
      "Epoch 26383/30000 Training Loss: 0.03900149092078209\n",
      "Epoch 26384/30000 Training Loss: 0.04757837951183319\n",
      "Epoch 26385/30000 Training Loss: 0.03817056491971016\n",
      "Epoch 26386/30000 Training Loss: 0.02722739428281784\n",
      "Epoch 26387/30000 Training Loss: 0.04668834060430527\n",
      "Epoch 26388/30000 Training Loss: 0.034780971705913544\n",
      "Epoch 26389/30000 Training Loss: 0.034998174756765366\n",
      "Epoch 26390/30000 Training Loss: 0.04207317903637886\n",
      "Epoch 26391/30000 Training Loss: 0.03616772219538689\n",
      "Epoch 26392/30000 Training Loss: 0.041176553815603256\n",
      "Epoch 26393/30000 Training Loss: 0.03631049394607544\n",
      "Epoch 26394/30000 Training Loss: 0.039700329303741455\n",
      "Epoch 26395/30000 Training Loss: 0.05100677162408829\n",
      "Epoch 26396/30000 Training Loss: 0.04474871978163719\n",
      "Epoch 26397/30000 Training Loss: 0.03259270265698433\n",
      "Epoch 26398/30000 Training Loss: 0.04229287803173065\n",
      "Epoch 26399/30000 Training Loss: 0.042718056589365005\n",
      "Epoch 26400/30000 Training Loss: 0.04599320888519287\n",
      "Epoch 26400/30000 Validation Loss: 0.03751333802938461\n",
      "Epoch 26401/30000 Training Loss: 0.036966294050216675\n",
      "Epoch 26402/30000 Training Loss: 0.042575933039188385\n",
      "Epoch 26403/30000 Training Loss: 0.032629504799842834\n",
      "Epoch 26404/30000 Training Loss: 0.04173382744193077\n",
      "Epoch 26405/30000 Training Loss: 0.053218964487314224\n",
      "Epoch 26406/30000 Training Loss: 0.03378983214497566\n",
      "Epoch 26407/30000 Training Loss: 0.057774629443883896\n",
      "Epoch 26408/30000 Training Loss: 0.03864101320505142\n",
      "Epoch 26409/30000 Training Loss: 0.030172327533364296\n",
      "Epoch 26410/30000 Training Loss: 0.03900495916604996\n",
      "Epoch 26411/30000 Training Loss: 0.038732681423425674\n",
      "Epoch 26412/30000 Training Loss: 0.04103843495249748\n",
      "Epoch 26413/30000 Training Loss: 0.03568386286497116\n",
      "Epoch 26414/30000 Training Loss: 0.03838437423110008\n",
      "Epoch 26415/30000 Training Loss: 0.03372621163725853\n",
      "Epoch 26416/30000 Training Loss: 0.04634084552526474\n",
      "Epoch 26417/30000 Training Loss: 0.039265550673007965\n",
      "Epoch 26418/30000 Training Loss: 0.038830794394016266\n",
      "Epoch 26419/30000 Training Loss: 0.04717584326863289\n",
      "Epoch 26420/30000 Training Loss: 0.035897064954042435\n",
      "Epoch 26421/30000 Training Loss: 0.03917819261550903\n",
      "Epoch 26422/30000 Training Loss: 0.03490607440471649\n",
      "Epoch 26423/30000 Training Loss: 0.028200112283229828\n",
      "Epoch 26424/30000 Training Loss: 0.04534056782722473\n",
      "Epoch 26425/30000 Training Loss: 0.04127441719174385\n",
      "Epoch 26426/30000 Training Loss: 0.044982731342315674\n",
      "Epoch 26427/30000 Training Loss: 0.04028679057955742\n",
      "Epoch 26428/30000 Training Loss: 0.04701904207468033\n",
      "Epoch 26429/30000 Training Loss: 0.04645702242851257\n",
      "Epoch 26430/30000 Training Loss: 0.03813020512461662\n",
      "Epoch 26431/30000 Training Loss: 0.042650192975997925\n",
      "Epoch 26432/30000 Training Loss: 0.03855415806174278\n",
      "Epoch 26433/30000 Training Loss: 0.0428561232984066\n",
      "Epoch 26434/30000 Training Loss: 0.05560354143381119\n",
      "Epoch 26435/30000 Training Loss: 0.04431553930044174\n",
      "Epoch 26436/30000 Training Loss: 0.04411912336945534\n",
      "Epoch 26437/30000 Training Loss: 0.031359050422906876\n",
      "Epoch 26438/30000 Training Loss: 0.044410884380340576\n",
      "Epoch 26439/30000 Training Loss: 0.05327432602643967\n",
      "Epoch 26440/30000 Training Loss: 0.041286516934633255\n",
      "Epoch 26441/30000 Training Loss: 0.04936530441045761\n",
      "Epoch 26442/30000 Training Loss: 0.04658912122249603\n",
      "Epoch 26443/30000 Training Loss: 0.03496932238340378\n",
      "Epoch 26444/30000 Training Loss: 0.051093101501464844\n",
      "Epoch 26445/30000 Training Loss: 0.054420098662376404\n",
      "Epoch 26446/30000 Training Loss: 0.04368366673588753\n",
      "Epoch 26447/30000 Training Loss: 0.04105188697576523\n",
      "Epoch 26448/30000 Training Loss: 0.05142717808485031\n",
      "Epoch 26449/30000 Training Loss: 0.041869137436151505\n",
      "Epoch 26450/30000 Training Loss: 0.038054224103689194\n",
      "Epoch 26451/30000 Training Loss: 0.034773651510477066\n",
      "Epoch 26452/30000 Training Loss: 0.04641655087471008\n",
      "Epoch 26453/30000 Training Loss: 0.03685269504785538\n",
      "Epoch 26454/30000 Training Loss: 0.04021213948726654\n",
      "Epoch 26455/30000 Training Loss: 0.031795140355825424\n",
      "Epoch 26456/30000 Training Loss: 0.047332633286714554\n",
      "Epoch 26457/30000 Training Loss: 0.05071209371089935\n",
      "Epoch 26458/30000 Training Loss: 0.0593457892537117\n",
      "Epoch 26459/30000 Training Loss: 0.04874745011329651\n",
      "Epoch 26460/30000 Training Loss: 0.04631407931447029\n",
      "Epoch 26461/30000 Training Loss: 0.03096070885658264\n",
      "Epoch 26462/30000 Training Loss: 0.04818977788090706\n",
      "Epoch 26463/30000 Training Loss: 0.054555099457502365\n",
      "Epoch 26464/30000 Training Loss: 0.0392727293074131\n",
      "Epoch 26465/30000 Training Loss: 0.04730062931776047\n",
      "Epoch 26466/30000 Training Loss: 0.046974532306194305\n",
      "Epoch 26467/30000 Training Loss: 0.052089083939790726\n",
      "Epoch 26468/30000 Training Loss: 0.03747953474521637\n",
      "Epoch 26469/30000 Training Loss: 0.042403124272823334\n",
      "Epoch 26470/30000 Training Loss: 0.050429150462150574\n",
      "Epoch 26471/30000 Training Loss: 0.04788823425769806\n",
      "Epoch 26472/30000 Training Loss: 0.04490434005856514\n",
      "Epoch 26473/30000 Training Loss: 0.037627797573804855\n",
      "Epoch 26474/30000 Training Loss: 0.03108331188559532\n",
      "Epoch 26475/30000 Training Loss: 0.026873022317886353\n",
      "Epoch 26476/30000 Training Loss: 0.04095636308193207\n",
      "Epoch 26477/30000 Training Loss: 0.03553108498454094\n",
      "Epoch 26478/30000 Training Loss: 0.03683708980679512\n",
      "Epoch 26479/30000 Training Loss: 0.05236009880900383\n",
      "Epoch 26480/30000 Training Loss: 0.05018967390060425\n",
      "Epoch 26481/30000 Training Loss: 0.04180130362510681\n",
      "Epoch 26482/30000 Training Loss: 0.04010365158319473\n",
      "Epoch 26483/30000 Training Loss: 0.034306518733501434\n",
      "Epoch 26484/30000 Training Loss: 0.04534173756837845\n",
      "Epoch 26485/30000 Training Loss: 0.0396704226732254\n",
      "Epoch 26486/30000 Training Loss: 0.03988722711801529\n",
      "Epoch 26487/30000 Training Loss: 0.03804199397563934\n",
      "Epoch 26488/30000 Training Loss: 0.05008004605770111\n",
      "Epoch 26489/30000 Training Loss: 0.045520734041929245\n",
      "Epoch 26490/30000 Training Loss: 0.03627154976129532\n",
      "Epoch 26491/30000 Training Loss: 0.040699511766433716\n",
      "Epoch 26492/30000 Training Loss: 0.04508814588189125\n",
      "Epoch 26493/30000 Training Loss: 0.04128587990999222\n",
      "Epoch 26494/30000 Training Loss: 0.044692181050777435\n",
      "Epoch 26495/30000 Training Loss: 0.04178924858570099\n",
      "Epoch 26496/30000 Training Loss: 0.03851103037595749\n",
      "Epoch 26497/30000 Training Loss: 0.054901834577322006\n",
      "Epoch 26498/30000 Training Loss: 0.037919219583272934\n",
      "Epoch 26499/30000 Training Loss: 0.03610730543732643\n",
      "Epoch 26500/30000 Training Loss: 0.04496292769908905\n",
      "Epoch 26500/30000 Validation Loss: 0.05094492435455322\n",
      "Epoch 26501/30000 Training Loss: 0.038306161761283875\n",
      "Epoch 26502/30000 Training Loss: 0.03348744660615921\n",
      "Epoch 26503/30000 Training Loss: 0.03696684539318085\n",
      "Epoch 26504/30000 Training Loss: 0.03511636331677437\n",
      "Epoch 26505/30000 Training Loss: 0.046278782188892365\n",
      "Epoch 26506/30000 Training Loss: 0.0497366301715374\n",
      "Epoch 26507/30000 Training Loss: 0.05056644231081009\n",
      "Epoch 26508/30000 Training Loss: 0.05218604952096939\n",
      "Epoch 26509/30000 Training Loss: 0.05523020774126053\n",
      "Epoch 26510/30000 Training Loss: 0.046224504709243774\n",
      "Epoch 26511/30000 Training Loss: 0.0337364636361599\n",
      "Epoch 26512/30000 Training Loss: 0.03322668373584747\n",
      "Epoch 26513/30000 Training Loss: 0.04776528850197792\n",
      "Epoch 26514/30000 Training Loss: 0.05572040379047394\n",
      "Epoch 26515/30000 Training Loss: 0.03910109028220177\n",
      "Epoch 26516/30000 Training Loss: 0.03506544977426529\n",
      "Epoch 26517/30000 Training Loss: 0.038270797580480576\n",
      "Epoch 26518/30000 Training Loss: 0.03792259097099304\n",
      "Epoch 26519/30000 Training Loss: 0.04269716516137123\n",
      "Epoch 26520/30000 Training Loss: 0.048296719789505005\n",
      "Epoch 26521/30000 Training Loss: 0.03561052680015564\n",
      "Epoch 26522/30000 Training Loss: 0.046072818338871\n",
      "Epoch 26523/30000 Training Loss: 0.04713714122772217\n",
      "Epoch 26524/30000 Training Loss: 0.03802495449781418\n",
      "Epoch 26525/30000 Training Loss: 0.058957964181900024\n",
      "Epoch 26526/30000 Training Loss: 0.04727829247713089\n",
      "Epoch 26527/30000 Training Loss: 0.04211648181080818\n",
      "Epoch 26528/30000 Training Loss: 0.040255025029182434\n",
      "Epoch 26529/30000 Training Loss: 0.03633996844291687\n",
      "Epoch 26530/30000 Training Loss: 0.05273596942424774\n",
      "Epoch 26531/30000 Training Loss: 0.04526205360889435\n",
      "Epoch 26532/30000 Training Loss: 0.055642880499362946\n",
      "Epoch 26533/30000 Training Loss: 0.049464575946331024\n",
      "Epoch 26534/30000 Training Loss: 0.04361493140459061\n",
      "Epoch 26535/30000 Training Loss: 0.04373275488615036\n",
      "Epoch 26536/30000 Training Loss: 0.04184339568018913\n",
      "Epoch 26537/30000 Training Loss: 0.04835868626832962\n",
      "Epoch 26538/30000 Training Loss: 0.03954578563570976\n",
      "Epoch 26539/30000 Training Loss: 0.04066276550292969\n",
      "Epoch 26540/30000 Training Loss: 0.05656644329428673\n",
      "Epoch 26541/30000 Training Loss: 0.04070976749062538\n",
      "Epoch 26542/30000 Training Loss: 0.05567365139722824\n",
      "Epoch 26543/30000 Training Loss: 0.0378284677863121\n",
      "Epoch 26544/30000 Training Loss: 0.052406344562768936\n",
      "Epoch 26545/30000 Training Loss: 0.046042732894420624\n",
      "Epoch 26546/30000 Training Loss: 0.02502281777560711\n",
      "Epoch 26547/30000 Training Loss: 0.05673358589410782\n",
      "Epoch 26548/30000 Training Loss: 0.03856147825717926\n",
      "Epoch 26549/30000 Training Loss: 0.04287371039390564\n",
      "Epoch 26550/30000 Training Loss: 0.04557874798774719\n",
      "Epoch 26551/30000 Training Loss: 0.03687485679984093\n",
      "Epoch 26552/30000 Training Loss: 0.055089764297008514\n",
      "Epoch 26553/30000 Training Loss: 0.04580901935696602\n",
      "Epoch 26554/30000 Training Loss: 0.03973105549812317\n",
      "Epoch 26555/30000 Training Loss: 0.03467859327793121\n",
      "Epoch 26556/30000 Training Loss: 0.03438953310251236\n",
      "Epoch 26557/30000 Training Loss: 0.04216103255748749\n",
      "Epoch 26558/30000 Training Loss: 0.06280425190925598\n",
      "Epoch 26559/30000 Training Loss: 0.045290134847164154\n",
      "Epoch 26560/30000 Training Loss: 0.038241688162088394\n",
      "Epoch 26561/30000 Training Loss: 0.03611476719379425\n",
      "Epoch 26562/30000 Training Loss: 0.035896651446819305\n",
      "Epoch 26563/30000 Training Loss: 0.05009008198976517\n",
      "Epoch 26564/30000 Training Loss: 0.04063784331083298\n",
      "Epoch 26565/30000 Training Loss: 0.04102228581905365\n",
      "Epoch 26566/30000 Training Loss: 0.055483072996139526\n",
      "Epoch 26567/30000 Training Loss: 0.05669202283024788\n",
      "Epoch 26568/30000 Training Loss: 0.034933775663375854\n",
      "Epoch 26569/30000 Training Loss: 0.049477607011795044\n",
      "Epoch 26570/30000 Training Loss: 0.04739263653755188\n",
      "Epoch 26571/30000 Training Loss: 0.05102193355560303\n",
      "Epoch 26572/30000 Training Loss: 0.04923740029335022\n",
      "Epoch 26573/30000 Training Loss: 0.04744572937488556\n",
      "Epoch 26574/30000 Training Loss: 0.04467141628265381\n",
      "Epoch 26575/30000 Training Loss: 0.05826430022716522\n",
      "Epoch 26576/30000 Training Loss: 0.0331149622797966\n",
      "Epoch 26577/30000 Training Loss: 0.04093027859926224\n",
      "Epoch 26578/30000 Training Loss: 0.039493486285209656\n",
      "Epoch 26579/30000 Training Loss: 0.04321695864200592\n",
      "Epoch 26580/30000 Training Loss: 0.04086538404226303\n",
      "Epoch 26581/30000 Training Loss: 0.047481827437877655\n",
      "Epoch 26582/30000 Training Loss: 0.04918842762708664\n",
      "Epoch 26583/30000 Training Loss: 0.044244054704904556\n",
      "Epoch 26584/30000 Training Loss: 0.04401996731758118\n",
      "Epoch 26585/30000 Training Loss: 0.04481565207242966\n",
      "Epoch 26586/30000 Training Loss: 0.038789961487054825\n",
      "Epoch 26587/30000 Training Loss: 0.0485958606004715\n",
      "Epoch 26588/30000 Training Loss: 0.03931393846869469\n",
      "Epoch 26589/30000 Training Loss: 0.03525776416063309\n",
      "Epoch 26590/30000 Training Loss: 0.038929734379053116\n",
      "Epoch 26591/30000 Training Loss: 0.040405161678791046\n",
      "Epoch 26592/30000 Training Loss: 0.04051268845796585\n",
      "Epoch 26593/30000 Training Loss: 0.042516350746154785\n",
      "Epoch 26594/30000 Training Loss: 0.054176539182662964\n",
      "Epoch 26595/30000 Training Loss: 0.06034807115793228\n",
      "Epoch 26596/30000 Training Loss: 0.03476247936487198\n",
      "Epoch 26597/30000 Training Loss: 0.04907390847802162\n",
      "Epoch 26598/30000 Training Loss: 0.038267090916633606\n",
      "Epoch 26599/30000 Training Loss: 0.048192523419857025\n",
      "Epoch 26600/30000 Training Loss: 0.049153219908475876\n",
      "Epoch 26600/30000 Validation Loss: 0.05508788675069809\n",
      "Epoch 26601/30000 Training Loss: 0.051089219748973846\n",
      "Epoch 26602/30000 Training Loss: 0.03463970869779587\n",
      "Epoch 26603/30000 Training Loss: 0.0404357835650444\n",
      "Epoch 26604/30000 Training Loss: 0.04045676440000534\n",
      "Epoch 26605/30000 Training Loss: 0.033329904079437256\n",
      "Epoch 26606/30000 Training Loss: 0.0457133874297142\n",
      "Epoch 26607/30000 Training Loss: 0.06033143773674965\n",
      "Epoch 26608/30000 Training Loss: 0.03459162265062332\n",
      "Epoch 26609/30000 Training Loss: 0.04141377657651901\n",
      "Epoch 26610/30000 Training Loss: 0.03931557759642601\n",
      "Epoch 26611/30000 Training Loss: 0.04907144606113434\n",
      "Epoch 26612/30000 Training Loss: 0.037745289504528046\n",
      "Epoch 26613/30000 Training Loss: 0.04138033837080002\n",
      "Epoch 26614/30000 Training Loss: 0.045663461089134216\n",
      "Epoch 26615/30000 Training Loss: 0.04256488382816315\n",
      "Epoch 26616/30000 Training Loss: 0.042357638478279114\n",
      "Epoch 26617/30000 Training Loss: 0.05012567341327667\n",
      "Epoch 26618/30000 Training Loss: 0.041548650711774826\n",
      "Epoch 26619/30000 Training Loss: 0.04683208093047142\n",
      "Epoch 26620/30000 Training Loss: 0.0433652326464653\n",
      "Epoch 26621/30000 Training Loss: 0.041214264929294586\n",
      "Epoch 26622/30000 Training Loss: 0.05141688510775566\n",
      "Epoch 26623/30000 Training Loss: 0.04494248330593109\n",
      "Epoch 26624/30000 Training Loss: 0.04463076591491699\n",
      "Epoch 26625/30000 Training Loss: 0.044078048318624496\n",
      "Epoch 26626/30000 Training Loss: 0.04160691425204277\n",
      "Epoch 26627/30000 Training Loss: 0.041559960693120956\n",
      "Epoch 26628/30000 Training Loss: 0.044384606182575226\n",
      "Epoch 26629/30000 Training Loss: 0.04466720297932625\n",
      "Epoch 26630/30000 Training Loss: 0.039056770503520966\n",
      "Epoch 26631/30000 Training Loss: 0.04918735474348068\n",
      "Epoch 26632/30000 Training Loss: 0.04889196157455444\n",
      "Epoch 26633/30000 Training Loss: 0.03386235609650612\n",
      "Epoch 26634/30000 Training Loss: 0.03872044384479523\n",
      "Epoch 26635/30000 Training Loss: 0.026597699150443077\n",
      "Epoch 26636/30000 Training Loss: 0.03847234323620796\n",
      "Epoch 26637/30000 Training Loss: 0.03112432174384594\n",
      "Epoch 26638/30000 Training Loss: 0.026297982782125473\n",
      "Epoch 26639/30000 Training Loss: 0.04734624922275543\n",
      "Epoch 26640/30000 Training Loss: 0.041183508932590485\n",
      "Epoch 26641/30000 Training Loss: 0.051580045372247696\n",
      "Epoch 26642/30000 Training Loss: 0.04427327588200569\n",
      "Epoch 26643/30000 Training Loss: 0.03250817954540253\n",
      "Epoch 26644/30000 Training Loss: 0.05095977336168289\n",
      "Epoch 26645/30000 Training Loss: 0.04081529751420021\n",
      "Epoch 26646/30000 Training Loss: 0.026830676943063736\n",
      "Epoch 26647/30000 Training Loss: 0.03646831586956978\n",
      "Epoch 26648/30000 Training Loss: 0.044290512800216675\n",
      "Epoch 26649/30000 Training Loss: 0.032474879175424576\n",
      "Epoch 26650/30000 Training Loss: 0.03531927615404129\n",
      "Epoch 26651/30000 Training Loss: 0.05049172788858414\n",
      "Epoch 26652/30000 Training Loss: 0.04323473572731018\n",
      "Epoch 26653/30000 Training Loss: 0.041491445153951645\n",
      "Epoch 26654/30000 Training Loss: 0.05040331929922104\n",
      "Epoch 26655/30000 Training Loss: 0.041709430515766144\n",
      "Epoch 26656/30000 Training Loss: 0.03889103978872299\n",
      "Epoch 26657/30000 Training Loss: 0.04390833154320717\n",
      "Epoch 26658/30000 Training Loss: 0.049879156053066254\n",
      "Epoch 26659/30000 Training Loss: 0.04110807180404663\n",
      "Epoch 26660/30000 Training Loss: 0.03920290619134903\n",
      "Epoch 26661/30000 Training Loss: 0.07007769495248795\n",
      "Epoch 26662/30000 Training Loss: 0.04960743337869644\n",
      "Epoch 26663/30000 Training Loss: 0.04047837108373642\n",
      "Epoch 26664/30000 Training Loss: 0.05124899744987488\n",
      "Epoch 26665/30000 Training Loss: 0.04615461826324463\n",
      "Epoch 26666/30000 Training Loss: 0.03755941241979599\n",
      "Epoch 26667/30000 Training Loss: 0.04536958783864975\n",
      "Epoch 26668/30000 Training Loss: 0.05258519947528839\n",
      "Epoch 26669/30000 Training Loss: 0.049930620938539505\n",
      "Epoch 26670/30000 Training Loss: 0.057983383536338806\n",
      "Epoch 26671/30000 Training Loss: 0.03760257363319397\n",
      "Epoch 26672/30000 Training Loss: 0.04421203210949898\n",
      "Epoch 26673/30000 Training Loss: 0.045850303024053574\n",
      "Epoch 26674/30000 Training Loss: 0.024847477674484253\n",
      "Epoch 26675/30000 Training Loss: 0.033196501433849335\n",
      "Epoch 26676/30000 Training Loss: 0.032167281955480576\n",
      "Epoch 26677/30000 Training Loss: 0.03468802198767662\n",
      "Epoch 26678/30000 Training Loss: 0.04199811816215515\n",
      "Epoch 26679/30000 Training Loss: 0.046591781079769135\n",
      "Epoch 26680/30000 Training Loss: 0.05873881280422211\n",
      "Epoch 26681/30000 Training Loss: 0.053404636681079865\n",
      "Epoch 26682/30000 Training Loss: 0.05100344866514206\n",
      "Epoch 26683/30000 Training Loss: 0.03357144817709923\n",
      "Epoch 26684/30000 Training Loss: 0.03826172649860382\n",
      "Epoch 26685/30000 Training Loss: 0.04338781535625458\n",
      "Epoch 26686/30000 Training Loss: 0.054562460631132126\n",
      "Epoch 26687/30000 Training Loss: 0.036722332239151\n",
      "Epoch 26688/30000 Training Loss: 0.03528009355068207\n",
      "Epoch 26689/30000 Training Loss: 0.030956130474805832\n",
      "Epoch 26690/30000 Training Loss: 0.040258586406707764\n",
      "Epoch 26691/30000 Training Loss: 0.042384590953588486\n",
      "Epoch 26692/30000 Training Loss: 0.0377110131084919\n",
      "Epoch 26693/30000 Training Loss: 0.03812772035598755\n",
      "Epoch 26694/30000 Training Loss: 0.041327059268951416\n",
      "Epoch 26695/30000 Training Loss: 0.05923993140459061\n",
      "Epoch 26696/30000 Training Loss: 0.04266178607940674\n",
      "Epoch 26697/30000 Training Loss: 0.03931143879890442\n",
      "Epoch 26698/30000 Training Loss: 0.045853391289711\n",
      "Epoch 26699/30000 Training Loss: 0.03990925848484039\n",
      "Epoch 26700/30000 Training Loss: 0.04352477192878723\n",
      "Epoch 26700/30000 Validation Loss: 0.03683026507496834\n",
      "Epoch 26701/30000 Training Loss: 0.06865880638360977\n",
      "Epoch 26702/30000 Training Loss: 0.03315328434109688\n",
      "Epoch 26703/30000 Training Loss: 0.037688612937927246\n",
      "Epoch 26704/30000 Training Loss: 0.03470055013895035\n",
      "Epoch 26705/30000 Training Loss: 0.044747479259967804\n",
      "Epoch 26706/30000 Training Loss: 0.03364773094654083\n",
      "Epoch 26707/30000 Training Loss: 0.04918546974658966\n",
      "Epoch 26708/30000 Training Loss: 0.0434022955596447\n",
      "Epoch 26709/30000 Training Loss: 0.02908206731081009\n",
      "Epoch 26710/30000 Training Loss: 0.05080818384885788\n",
      "Epoch 26711/30000 Training Loss: 0.034849394112825394\n",
      "Epoch 26712/30000 Training Loss: 0.042862169444561005\n",
      "Epoch 26713/30000 Training Loss: 0.04057304188609123\n",
      "Epoch 26714/30000 Training Loss: 0.03305889666080475\n",
      "Epoch 26715/30000 Training Loss: 0.04136580973863602\n",
      "Epoch 26716/30000 Training Loss: 0.03768991678953171\n",
      "Epoch 26717/30000 Training Loss: 0.0432891920208931\n",
      "Epoch 26718/30000 Training Loss: 0.052082359790802\n",
      "Epoch 26719/30000 Training Loss: 0.03779231011867523\n",
      "Epoch 26720/30000 Training Loss: 0.04760872572660446\n",
      "Epoch 26721/30000 Training Loss: 0.03671520575881004\n",
      "Epoch 26722/30000 Training Loss: 0.04203503206372261\n",
      "Epoch 26723/30000 Training Loss: 0.031964946538209915\n",
      "Epoch 26724/30000 Training Loss: 0.03073183447122574\n",
      "Epoch 26725/30000 Training Loss: 0.04823756217956543\n",
      "Epoch 26726/30000 Training Loss: 0.034429363906383514\n",
      "Epoch 26727/30000 Training Loss: 0.04475157707929611\n",
      "Epoch 26728/30000 Training Loss: 0.05235697329044342\n",
      "Epoch 26729/30000 Training Loss: 0.03549506515264511\n",
      "Epoch 26730/30000 Training Loss: 0.0375824049115181\n",
      "Epoch 26731/30000 Training Loss: 0.03607563301920891\n",
      "Epoch 26732/30000 Training Loss: 0.03247398883104324\n",
      "Epoch 26733/30000 Training Loss: 0.04431711882352829\n",
      "Epoch 26734/30000 Training Loss: 0.037988875061273575\n",
      "Epoch 26735/30000 Training Loss: 0.042695969343185425\n",
      "Epoch 26736/30000 Training Loss: 0.03829685598611832\n",
      "Epoch 26737/30000 Training Loss: 0.04761320352554321\n",
      "Epoch 26738/30000 Training Loss: 0.039510175585746765\n",
      "Epoch 26739/30000 Training Loss: 0.03142319992184639\n",
      "Epoch 26740/30000 Training Loss: 0.045645877718925476\n",
      "Epoch 26741/30000 Training Loss: 0.03774392604827881\n",
      "Epoch 26742/30000 Training Loss: 0.04407870024442673\n",
      "Epoch 26743/30000 Training Loss: 0.04332897812128067\n",
      "Epoch 26744/30000 Training Loss: 0.047282952815294266\n",
      "Epoch 26745/30000 Training Loss: 0.035848312079906464\n",
      "Epoch 26746/30000 Training Loss: 0.05351673811674118\n",
      "Epoch 26747/30000 Training Loss: 0.046943530440330505\n",
      "Epoch 26748/30000 Training Loss: 0.05088537186384201\n",
      "Epoch 26749/30000 Training Loss: 0.044618211686611176\n",
      "Epoch 26750/30000 Training Loss: 0.03819379210472107\n",
      "Epoch 26751/30000 Training Loss: 0.03400470316410065\n",
      "Epoch 26752/30000 Training Loss: 0.043565697968006134\n",
      "Epoch 26753/30000 Training Loss: 0.052552588284015656\n",
      "Epoch 26754/30000 Training Loss: 0.03560081124305725\n",
      "Epoch 26755/30000 Training Loss: 0.03755272552371025\n",
      "Epoch 26756/30000 Training Loss: 0.04617322236299515\n",
      "Epoch 26757/30000 Training Loss: 0.038761187344789505\n",
      "Epoch 26758/30000 Training Loss: 0.04205668717622757\n",
      "Epoch 26759/30000 Training Loss: 0.046853967010974884\n",
      "Epoch 26760/30000 Training Loss: 0.031704630702733994\n",
      "Epoch 26761/30000 Training Loss: 0.05511503666639328\n",
      "Epoch 26762/30000 Training Loss: 0.03391752392053604\n",
      "Epoch 26763/30000 Training Loss: 0.03653436899185181\n",
      "Epoch 26764/30000 Training Loss: 0.04267888516187668\n",
      "Epoch 26765/30000 Training Loss: 0.029008716344833374\n",
      "Epoch 26766/30000 Training Loss: 0.04061327874660492\n",
      "Epoch 26767/30000 Training Loss: 0.04652450978755951\n",
      "Epoch 26768/30000 Training Loss: 0.03734511137008667\n",
      "Epoch 26769/30000 Training Loss: 0.047284334897994995\n",
      "Epoch 26770/30000 Training Loss: 0.042911238968372345\n",
      "Epoch 26771/30000 Training Loss: 0.03803417086601257\n",
      "Epoch 26772/30000 Training Loss: 0.03937332332134247\n",
      "Epoch 26773/30000 Training Loss: 0.051093775779008865\n",
      "Epoch 26774/30000 Training Loss: 0.040018390864133835\n",
      "Epoch 26775/30000 Training Loss: 0.05424461513757706\n",
      "Epoch 26776/30000 Training Loss: 0.039814841002225876\n",
      "Epoch 26777/30000 Training Loss: 0.03588133305311203\n",
      "Epoch 26778/30000 Training Loss: 0.04619587957859039\n",
      "Epoch 26779/30000 Training Loss: 0.027968864887952805\n",
      "Epoch 26780/30000 Training Loss: 0.052593618631362915\n",
      "Epoch 26781/30000 Training Loss: 0.04043847694993019\n",
      "Epoch 26782/30000 Training Loss: 0.040754951536655426\n",
      "Epoch 26783/30000 Training Loss: 0.05659869313240051\n",
      "Epoch 26784/30000 Training Loss: 0.032848089933395386\n",
      "Epoch 26785/30000 Training Loss: 0.04031042754650116\n",
      "Epoch 26786/30000 Training Loss: 0.04520861431956291\n",
      "Epoch 26787/30000 Training Loss: 0.04515421390533447\n",
      "Epoch 26788/30000 Training Loss: 0.03838663548231125\n",
      "Epoch 26789/30000 Training Loss: 0.038715146481990814\n",
      "Epoch 26790/30000 Training Loss: 0.04141387343406677\n",
      "Epoch 26791/30000 Training Loss: 0.048244595527648926\n",
      "Epoch 26792/30000 Training Loss: 0.0463947057723999\n",
      "Epoch 26793/30000 Training Loss: 0.045580390840768814\n",
      "Epoch 26794/30000 Training Loss: 0.04454934969544411\n",
      "Epoch 26795/30000 Training Loss: 0.0439925491809845\n",
      "Epoch 26796/30000 Training Loss: 0.03415868058800697\n",
      "Epoch 26797/30000 Training Loss: 0.050077103078365326\n",
      "Epoch 26798/30000 Training Loss: 0.040545858442783356\n",
      "Epoch 26799/30000 Training Loss: 0.03505626320838928\n",
      "Epoch 26800/30000 Training Loss: 0.04251813516020775\n",
      "Epoch 26800/30000 Validation Loss: 0.04927438870072365\n",
      "Epoch 26801/30000 Training Loss: 0.03588223457336426\n",
      "Epoch 26802/30000 Training Loss: 0.04378986731171608\n",
      "Epoch 26803/30000 Training Loss: 0.04370689019560814\n",
      "Epoch 26804/30000 Training Loss: 0.04672479256987572\n",
      "Epoch 26805/30000 Training Loss: 0.04593954607844353\n",
      "Epoch 26806/30000 Training Loss: 0.031911496073007584\n",
      "Epoch 26807/30000 Training Loss: 0.0473199263215065\n",
      "Epoch 26808/30000 Training Loss: 0.042169999331235886\n",
      "Epoch 26809/30000 Training Loss: 0.04795076698064804\n",
      "Epoch 26810/30000 Training Loss: 0.0475652702152729\n",
      "Epoch 26811/30000 Training Loss: 0.04565111920237541\n",
      "Epoch 26812/30000 Training Loss: 0.045083608478307724\n",
      "Epoch 26813/30000 Training Loss: 0.0378292053937912\n",
      "Epoch 26814/30000 Training Loss: 0.04271608218550682\n",
      "Epoch 26815/30000 Training Loss: 0.0354120247066021\n",
      "Epoch 26816/30000 Training Loss: 0.05094485729932785\n",
      "Epoch 26817/30000 Training Loss: 0.03664771467447281\n",
      "Epoch 26818/30000 Training Loss: 0.03986454755067825\n",
      "Epoch 26819/30000 Training Loss: 0.04190916195511818\n",
      "Epoch 26820/30000 Training Loss: 0.05263935774564743\n",
      "Epoch 26821/30000 Training Loss: 0.04734700918197632\n",
      "Epoch 26822/30000 Training Loss: 0.054603129625320435\n",
      "Epoch 26823/30000 Training Loss: 0.029774433001875877\n",
      "Epoch 26824/30000 Training Loss: 0.052068181335926056\n",
      "Epoch 26825/30000 Training Loss: 0.03470078110694885\n",
      "Epoch 26826/30000 Training Loss: 0.053599342703819275\n",
      "Epoch 26827/30000 Training Loss: 0.03406112641096115\n",
      "Epoch 26828/30000 Training Loss: 0.04624047875404358\n",
      "Epoch 26829/30000 Training Loss: 0.04541015625\n",
      "Epoch 26830/30000 Training Loss: 0.04158920794725418\n",
      "Epoch 26831/30000 Training Loss: 0.04280903562903404\n",
      "Epoch 26832/30000 Training Loss: 0.05050894618034363\n",
      "Epoch 26833/30000 Training Loss: 0.0388362817466259\n",
      "Epoch 26834/30000 Training Loss: 0.04915653541684151\n",
      "Epoch 26835/30000 Training Loss: 0.04070757329463959\n",
      "Epoch 26836/30000 Training Loss: 0.04264288395643234\n",
      "Epoch 26837/30000 Training Loss: 0.0482635423541069\n",
      "Epoch 26838/30000 Training Loss: 0.05308670550584793\n",
      "Epoch 26839/30000 Training Loss: 0.04059898108243942\n",
      "Epoch 26840/30000 Training Loss: 0.037435583770275116\n",
      "Epoch 26841/30000 Training Loss: 0.051450882107019424\n",
      "Epoch 26842/30000 Training Loss: 0.032621823251247406\n",
      "Epoch 26843/30000 Training Loss: 0.05500910431146622\n",
      "Epoch 26844/30000 Training Loss: 0.04278729110956192\n",
      "Epoch 26845/30000 Training Loss: 0.04168416187167168\n",
      "Epoch 26846/30000 Training Loss: 0.04562889784574509\n",
      "Epoch 26847/30000 Training Loss: 0.05146940425038338\n",
      "Epoch 26848/30000 Training Loss: 0.031945593655109406\n",
      "Epoch 26849/30000 Training Loss: 0.038905929774045944\n",
      "Epoch 26850/30000 Training Loss: 0.05157395452260971\n",
      "Epoch 26851/30000 Training Loss: 0.05131605640053749\n",
      "Epoch 26852/30000 Training Loss: 0.04283871129155159\n",
      "Epoch 26853/30000 Training Loss: 0.0542726144194603\n",
      "Epoch 26854/30000 Training Loss: 0.03782118856906891\n",
      "Epoch 26855/30000 Training Loss: 0.04512110352516174\n",
      "Epoch 26856/30000 Training Loss: 0.04094642028212547\n",
      "Epoch 26857/30000 Training Loss: 0.0416383296251297\n",
      "Epoch 26858/30000 Training Loss: 0.057184502482414246\n",
      "Epoch 26859/30000 Training Loss: 0.04401656240224838\n",
      "Epoch 26860/30000 Training Loss: 0.04630138725042343\n",
      "Epoch 26861/30000 Training Loss: 0.040426768362522125\n",
      "Epoch 26862/30000 Training Loss: 0.03597261756658554\n",
      "Epoch 26863/30000 Training Loss: 0.038019098341464996\n",
      "Epoch 26864/30000 Training Loss: 0.04262249171733856\n",
      "Epoch 26865/30000 Training Loss: 0.04978761821985245\n",
      "Epoch 26866/30000 Training Loss: 0.03876441344618797\n",
      "Epoch 26867/30000 Training Loss: 0.044371429830789566\n",
      "Epoch 26868/30000 Training Loss: 0.03047826886177063\n",
      "Epoch 26869/30000 Training Loss: 0.03872259706258774\n",
      "Epoch 26870/30000 Training Loss: 0.04077168554067612\n",
      "Epoch 26871/30000 Training Loss: 0.046218302100896835\n",
      "Epoch 26872/30000 Training Loss: 0.04850351810455322\n",
      "Epoch 26873/30000 Training Loss: 0.0491989366710186\n",
      "Epoch 26874/30000 Training Loss: 0.038516972213983536\n",
      "Epoch 26875/30000 Training Loss: 0.06004861742258072\n",
      "Epoch 26876/30000 Training Loss: 0.04298151656985283\n",
      "Epoch 26877/30000 Training Loss: 0.04330681264400482\n",
      "Epoch 26878/30000 Training Loss: 0.03827974945306778\n",
      "Epoch 26879/30000 Training Loss: 0.04009533300995827\n",
      "Epoch 26880/30000 Training Loss: 0.03934253007173538\n",
      "Epoch 26881/30000 Training Loss: 0.0372072234749794\n",
      "Epoch 26882/30000 Training Loss: 0.053615547716617584\n",
      "Epoch 26883/30000 Training Loss: 0.035520799458026886\n",
      "Epoch 26884/30000 Training Loss: 0.04058217629790306\n",
      "Epoch 26885/30000 Training Loss: 0.04151501506567001\n",
      "Epoch 26886/30000 Training Loss: 0.049720823764801025\n",
      "Epoch 26887/30000 Training Loss: 0.03375080227851868\n",
      "Epoch 26888/30000 Training Loss: 0.050085172057151794\n",
      "Epoch 26889/30000 Training Loss: 0.03508191183209419\n",
      "Epoch 26890/30000 Training Loss: 0.0328512005507946\n",
      "Epoch 26891/30000 Training Loss: 0.04123305901885033\n",
      "Epoch 26892/30000 Training Loss: 0.04505038261413574\n",
      "Epoch 26893/30000 Training Loss: 0.03576287999749184\n",
      "Epoch 26894/30000 Training Loss: 0.042033933103084564\n",
      "Epoch 26895/30000 Training Loss: 0.04619244486093521\n",
      "Epoch 26896/30000 Training Loss: 0.03169236332178116\n",
      "Epoch 26897/30000 Training Loss: 0.035241153091192245\n",
      "Epoch 26898/30000 Training Loss: 0.0456206277012825\n",
      "Epoch 26899/30000 Training Loss: 0.042085565626621246\n",
      "Epoch 26900/30000 Training Loss: 0.04873038828372955\n",
      "Epoch 26900/30000 Validation Loss: 0.05225776508450508\n",
      "Epoch 26901/30000 Training Loss: 0.04668067395687103\n",
      "Epoch 26902/30000 Training Loss: 0.045532114803791046\n",
      "Epoch 26903/30000 Training Loss: 0.04317956417798996\n",
      "Epoch 26904/30000 Training Loss: 0.041827939450740814\n",
      "Epoch 26905/30000 Training Loss: 0.033779021352529526\n",
      "Epoch 26906/30000 Training Loss: 0.04269030690193176\n",
      "Epoch 26907/30000 Training Loss: 0.04348161816596985\n",
      "Epoch 26908/30000 Training Loss: 0.04803859815001488\n",
      "Epoch 26909/30000 Training Loss: 0.05688673257827759\n",
      "Epoch 26910/30000 Training Loss: 0.043227508664131165\n",
      "Epoch 26911/30000 Training Loss: 0.0402074009180069\n",
      "Epoch 26912/30000 Training Loss: 0.04964722320437431\n",
      "Epoch 26913/30000 Training Loss: 0.04669027402997017\n",
      "Epoch 26914/30000 Training Loss: 0.032416485249996185\n",
      "Epoch 26915/30000 Training Loss: 0.03391899913549423\n",
      "Epoch 26916/30000 Training Loss: 0.03815978020429611\n",
      "Epoch 26917/30000 Training Loss: 0.03698158264160156\n",
      "Epoch 26918/30000 Training Loss: 0.044008903205394745\n",
      "Epoch 26919/30000 Training Loss: 0.04780314117670059\n",
      "Epoch 26920/30000 Training Loss: 0.03833351284265518\n",
      "Epoch 26921/30000 Training Loss: 0.04372384399175644\n",
      "Epoch 26922/30000 Training Loss: 0.04369143396615982\n",
      "Epoch 26923/30000 Training Loss: 0.04773196950554848\n",
      "Epoch 26924/30000 Training Loss: 0.04580783471465111\n",
      "Epoch 26925/30000 Training Loss: 0.042715929448604584\n",
      "Epoch 26926/30000 Training Loss: 0.04944131523370743\n",
      "Epoch 26927/30000 Training Loss: 0.03937165439128876\n",
      "Epoch 26928/30000 Training Loss: 0.047180674970149994\n",
      "Epoch 26929/30000 Training Loss: 0.037446022033691406\n",
      "Epoch 26930/30000 Training Loss: 0.03660527616739273\n",
      "Epoch 26931/30000 Training Loss: 0.03804607316851616\n",
      "Epoch 26932/30000 Training Loss: 0.049968041479587555\n",
      "Epoch 26933/30000 Training Loss: 0.0696013867855072\n",
      "Epoch 26934/30000 Training Loss: 0.04031207785010338\n",
      "Epoch 26935/30000 Training Loss: 0.04060211405158043\n",
      "Epoch 26936/30000 Training Loss: 0.052877381443977356\n",
      "Epoch 26937/30000 Training Loss: 0.04143940657377243\n",
      "Epoch 26938/30000 Training Loss: 0.03641246259212494\n",
      "Epoch 26939/30000 Training Loss: 0.03834864869713783\n",
      "Epoch 26940/30000 Training Loss: 0.0384628064930439\n",
      "Epoch 26941/30000 Training Loss: 0.04260341078042984\n",
      "Epoch 26942/30000 Training Loss: 0.0342903733253479\n",
      "Epoch 26943/30000 Training Loss: 0.046850286424160004\n",
      "Epoch 26944/30000 Training Loss: 0.05020993575453758\n",
      "Epoch 26945/30000 Training Loss: 0.033430036157369614\n",
      "Epoch 26946/30000 Training Loss: 0.04348115995526314\n",
      "Epoch 26947/30000 Training Loss: 0.04651617258787155\n",
      "Epoch 26948/30000 Training Loss: 0.06191534548997879\n",
      "Epoch 26949/30000 Training Loss: 0.05715584009885788\n",
      "Epoch 26950/30000 Training Loss: 0.0432431623339653\n",
      "Epoch 26951/30000 Training Loss: 0.037163153290748596\n",
      "Epoch 26952/30000 Training Loss: 0.03955380246043205\n",
      "Epoch 26953/30000 Training Loss: 0.038455985486507416\n",
      "Epoch 26954/30000 Training Loss: 0.04197533428668976\n",
      "Epoch 26955/30000 Training Loss: 0.04900325834751129\n",
      "Epoch 26956/30000 Training Loss: 0.04900464415550232\n",
      "Epoch 26957/30000 Training Loss: 0.042088598012924194\n",
      "Epoch 26958/30000 Training Loss: 0.04697730392217636\n",
      "Epoch 26959/30000 Training Loss: 0.04097796231508255\n",
      "Epoch 26960/30000 Training Loss: 0.03484046459197998\n",
      "Epoch 26961/30000 Training Loss: 0.058793194591999054\n",
      "Epoch 26962/30000 Training Loss: 0.051628533750772476\n",
      "Epoch 26963/30000 Training Loss: 0.04938393831253052\n",
      "Epoch 26964/30000 Training Loss: 0.036241017282009125\n",
      "Epoch 26965/30000 Training Loss: 0.057606007903814316\n",
      "Epoch 26966/30000 Training Loss: 0.043524209409952164\n",
      "Epoch 26967/30000 Training Loss: 0.03719985485076904\n",
      "Epoch 26968/30000 Training Loss: 0.043618857860565186\n",
      "Epoch 26969/30000 Training Loss: 0.042444802820682526\n",
      "Epoch 26970/30000 Training Loss: 0.04915732890367508\n",
      "Epoch 26971/30000 Training Loss: 0.046404749155044556\n",
      "Epoch 26972/30000 Training Loss: 0.04207523167133331\n",
      "Epoch 26973/30000 Training Loss: 0.04529851675033569\n",
      "Epoch 26974/30000 Training Loss: 0.04948577284812927\n",
      "Epoch 26975/30000 Training Loss: 0.043381866067647934\n",
      "Epoch 26976/30000 Training Loss: 0.04421887546777725\n",
      "Epoch 26977/30000 Training Loss: 0.04396877437829971\n",
      "Epoch 26978/30000 Training Loss: 0.02870279923081398\n",
      "Epoch 26979/30000 Training Loss: 0.03959609568119049\n",
      "Epoch 26980/30000 Training Loss: 0.05293301120400429\n",
      "Epoch 26981/30000 Training Loss: 0.044006943702697754\n",
      "Epoch 26982/30000 Training Loss: 0.03837038204073906\n",
      "Epoch 26983/30000 Training Loss: 0.043143853545188904\n",
      "Epoch 26984/30000 Training Loss: 0.03451920673251152\n",
      "Epoch 26985/30000 Training Loss: 0.03722206503152847\n",
      "Epoch 26986/30000 Training Loss: 0.03551826253533363\n",
      "Epoch 26987/30000 Training Loss: 0.054929934442043304\n",
      "Epoch 26988/30000 Training Loss: 0.05472807213664055\n",
      "Epoch 26989/30000 Training Loss: 0.04814700037240982\n",
      "Epoch 26990/30000 Training Loss: 0.03462213650345802\n",
      "Epoch 26991/30000 Training Loss: 0.04855529963970184\n",
      "Epoch 26992/30000 Training Loss: 0.03677494823932648\n",
      "Epoch 26993/30000 Training Loss: 0.049154724925756454\n",
      "Epoch 26994/30000 Training Loss: 0.03776403144001961\n",
      "Epoch 26995/30000 Training Loss: 0.04402834177017212\n",
      "Epoch 26996/30000 Training Loss: 0.03448668122291565\n",
      "Epoch 26997/30000 Training Loss: 0.03541937097907066\n",
      "Epoch 26998/30000 Training Loss: 0.03579489141702652\n",
      "Epoch 26999/30000 Training Loss: 0.04398388788104057\n",
      "Epoch 27000/30000 Training Loss: 0.05609055608510971\n",
      "Epoch 27000/30000 Validation Loss: 0.037470560520887375\n",
      "Epoch 27001/30000 Training Loss: 0.032402440905570984\n",
      "Epoch 27002/30000 Training Loss: 0.030383743345737457\n",
      "Epoch 27003/30000 Training Loss: 0.03800639137625694\n",
      "Epoch 27004/30000 Training Loss: 0.0483437143266201\n",
      "Epoch 27005/30000 Training Loss: 0.050672419369220734\n",
      "Epoch 27006/30000 Training Loss: 0.053438249975442886\n",
      "Epoch 27007/30000 Training Loss: 0.04989967867732048\n",
      "Epoch 27008/30000 Training Loss: 0.04139431565999985\n",
      "Epoch 27009/30000 Training Loss: 0.04002828896045685\n",
      "Epoch 27010/30000 Training Loss: 0.05918281525373459\n",
      "Epoch 27011/30000 Training Loss: 0.04847852513194084\n",
      "Epoch 27012/30000 Training Loss: 0.05520672723650932\n",
      "Epoch 27013/30000 Training Loss: 0.042212456464767456\n",
      "Epoch 27014/30000 Training Loss: 0.03296438604593277\n",
      "Epoch 27015/30000 Training Loss: 0.043873440474271774\n",
      "Epoch 27016/30000 Training Loss: 0.03435921669006348\n",
      "Epoch 27017/30000 Training Loss: 0.05007603019475937\n",
      "Epoch 27018/30000 Training Loss: 0.037226296961307526\n",
      "Epoch 27019/30000 Training Loss: 0.03619329631328583\n",
      "Epoch 27020/30000 Training Loss: 0.03233785554766655\n",
      "Epoch 27021/30000 Training Loss: 0.05174899101257324\n",
      "Epoch 27022/30000 Training Loss: 0.038931041955947876\n",
      "Epoch 27023/30000 Training Loss: 0.04121880978345871\n",
      "Epoch 27024/30000 Training Loss: 0.04736918956041336\n",
      "Epoch 27025/30000 Training Loss: 0.05305273085832596\n",
      "Epoch 27026/30000 Training Loss: 0.04618337005376816\n",
      "Epoch 27027/30000 Training Loss: 0.036674149334430695\n",
      "Epoch 27028/30000 Training Loss: 0.040362440049648285\n",
      "Epoch 27029/30000 Training Loss: 0.03849545121192932\n",
      "Epoch 27030/30000 Training Loss: 0.04152776300907135\n",
      "Epoch 27031/30000 Training Loss: 0.038463249802589417\n",
      "Epoch 27032/30000 Training Loss: 0.04328732565045357\n",
      "Epoch 27033/30000 Training Loss: 0.042283788323402405\n",
      "Epoch 27034/30000 Training Loss: 0.05533924326300621\n",
      "Epoch 27035/30000 Training Loss: 0.03352389857172966\n",
      "Epoch 27036/30000 Training Loss: 0.04055626690387726\n",
      "Epoch 27037/30000 Training Loss: 0.037797458469867706\n",
      "Epoch 27038/30000 Training Loss: 0.038604866713285446\n",
      "Epoch 27039/30000 Training Loss: 0.050210997462272644\n",
      "Epoch 27040/30000 Training Loss: 0.04671720787882805\n",
      "Epoch 27041/30000 Training Loss: 0.04775043576955795\n",
      "Epoch 27042/30000 Training Loss: 0.04496230557560921\n",
      "Epoch 27043/30000 Training Loss: 0.03744309023022652\n",
      "Epoch 27044/30000 Training Loss: 0.047610536217689514\n",
      "Epoch 27045/30000 Training Loss: 0.036737799644470215\n",
      "Epoch 27046/30000 Training Loss: 0.030358754098415375\n",
      "Epoch 27047/30000 Training Loss: 0.03917968273162842\n",
      "Epoch 27048/30000 Training Loss: 0.053500283509492874\n",
      "Epoch 27049/30000 Training Loss: 0.033546313643455505\n",
      "Epoch 27050/30000 Training Loss: 0.03771946579217911\n",
      "Epoch 27051/30000 Training Loss: 0.03848607838153839\n",
      "Epoch 27052/30000 Training Loss: 0.04173281416296959\n",
      "Epoch 27053/30000 Training Loss: 0.04294738173484802\n",
      "Epoch 27054/30000 Training Loss: 0.03617282956838608\n",
      "Epoch 27055/30000 Training Loss: 0.036397941410541534\n",
      "Epoch 27056/30000 Training Loss: 0.04341493174433708\n",
      "Epoch 27057/30000 Training Loss: 0.038810499012470245\n",
      "Epoch 27058/30000 Training Loss: 0.04723218083381653\n",
      "Epoch 27059/30000 Training Loss: 0.04729197546839714\n",
      "Epoch 27060/30000 Training Loss: 0.03963180258870125\n",
      "Epoch 27061/30000 Training Loss: 0.03983508422970772\n",
      "Epoch 27062/30000 Training Loss: 0.03422841802239418\n",
      "Epoch 27063/30000 Training Loss: 0.04429274797439575\n",
      "Epoch 27064/30000 Training Loss: 0.04832330346107483\n",
      "Epoch 27065/30000 Training Loss: 0.0383155420422554\n",
      "Epoch 27066/30000 Training Loss: 0.041366323828697205\n",
      "Epoch 27067/30000 Training Loss: 0.03643149510025978\n",
      "Epoch 27068/30000 Training Loss: 0.06805539131164551\n",
      "Epoch 27069/30000 Training Loss: 0.03319600969552994\n",
      "Epoch 27070/30000 Training Loss: 0.05507029592990875\n",
      "Epoch 27071/30000 Training Loss: 0.03779107332229614\n",
      "Epoch 27072/30000 Training Loss: 0.05344659835100174\n",
      "Epoch 27073/30000 Training Loss: 0.031976789236068726\n",
      "Epoch 27074/30000 Training Loss: 0.035710468888282776\n",
      "Epoch 27075/30000 Training Loss: 0.04805934429168701\n",
      "Epoch 27076/30000 Training Loss: 0.039296574890613556\n",
      "Epoch 27077/30000 Training Loss: 0.03570524603128433\n",
      "Epoch 27078/30000 Training Loss: 0.04023371636867523\n",
      "Epoch 27079/30000 Training Loss: 0.029982801526784897\n",
      "Epoch 27080/30000 Training Loss: 0.043868862092494965\n",
      "Epoch 27081/30000 Training Loss: 0.033983562141656876\n",
      "Epoch 27082/30000 Training Loss: 0.05230295658111572\n",
      "Epoch 27083/30000 Training Loss: 0.03659634664654732\n",
      "Epoch 27084/30000 Training Loss: 0.04013533145189285\n",
      "Epoch 27085/30000 Training Loss: 0.027183718979358673\n",
      "Epoch 27086/30000 Training Loss: 0.04401350021362305\n",
      "Epoch 27087/30000 Training Loss: 0.043688494712114334\n",
      "Epoch 27088/30000 Training Loss: 0.040276527404785156\n",
      "Epoch 27089/30000 Training Loss: 0.03720344603061676\n",
      "Epoch 27090/30000 Training Loss: 0.03564552962779999\n",
      "Epoch 27091/30000 Training Loss: 0.054275430738925934\n",
      "Epoch 27092/30000 Training Loss: 0.040779247879981995\n",
      "Epoch 27093/30000 Training Loss: 0.03990684077143669\n",
      "Epoch 27094/30000 Training Loss: 0.042661793529987335\n",
      "Epoch 27095/30000 Training Loss: 0.036031387746334076\n",
      "Epoch 27096/30000 Training Loss: 0.057887010276317596\n",
      "Epoch 27097/30000 Training Loss: 0.048501938581466675\n",
      "Epoch 27098/30000 Training Loss: 0.039735473692417145\n",
      "Epoch 27099/30000 Training Loss: 0.04090665653347969\n",
      "Epoch 27100/30000 Training Loss: 0.03435353934764862\n",
      "Epoch 27100/30000 Validation Loss: 0.03796706348657608\n",
      "Epoch 27101/30000 Training Loss: 0.04652038961648941\n",
      "Epoch 27102/30000 Training Loss: 0.039381980895996094\n",
      "Epoch 27103/30000 Training Loss: 0.04693739861249924\n",
      "Epoch 27104/30000 Training Loss: 0.04284175485372543\n",
      "Epoch 27105/30000 Training Loss: 0.03141622245311737\n",
      "Epoch 27106/30000 Training Loss: 0.03492278233170509\n",
      "Epoch 27107/30000 Training Loss: 0.04043211787939072\n",
      "Epoch 27108/30000 Training Loss: 0.057566605508327484\n",
      "Epoch 27109/30000 Training Loss: 0.04887672886252403\n",
      "Epoch 27110/30000 Training Loss: 0.03376470506191254\n",
      "Epoch 27111/30000 Training Loss: 0.051079362630844116\n",
      "Epoch 27112/30000 Training Loss: 0.04514849931001663\n",
      "Epoch 27113/30000 Training Loss: 0.047519750893116\n",
      "Epoch 27114/30000 Training Loss: 0.058267414569854736\n",
      "Epoch 27115/30000 Training Loss: 0.04552521929144859\n",
      "Epoch 27116/30000 Training Loss: 0.033066585659980774\n",
      "Epoch 27117/30000 Training Loss: 0.04603434354066849\n",
      "Epoch 27118/30000 Training Loss: 0.04827968776226044\n",
      "Epoch 27119/30000 Training Loss: 0.032682694494724274\n",
      "Epoch 27120/30000 Training Loss: 0.055724240839481354\n",
      "Epoch 27121/30000 Training Loss: 0.03775101900100708\n",
      "Epoch 27122/30000 Training Loss: 0.04059387743473053\n",
      "Epoch 27123/30000 Training Loss: 0.0460759699344635\n",
      "Epoch 27124/30000 Training Loss: 0.03625641018152237\n",
      "Epoch 27125/30000 Training Loss: 0.03889654576778412\n",
      "Epoch 27126/30000 Training Loss: 0.029390763491392136\n",
      "Epoch 27127/30000 Training Loss: 0.0449889600276947\n",
      "Epoch 27128/30000 Training Loss: 0.04785028100013733\n",
      "Epoch 27129/30000 Training Loss: 0.042286209762096405\n",
      "Epoch 27130/30000 Training Loss: 0.03779013454914093\n",
      "Epoch 27131/30000 Training Loss: 0.062406718730926514\n",
      "Epoch 27132/30000 Training Loss: 0.03920682519674301\n",
      "Epoch 27133/30000 Training Loss: 0.03493325784802437\n",
      "Epoch 27134/30000 Training Loss: 0.035623520612716675\n",
      "Epoch 27135/30000 Training Loss: 0.03851770609617233\n",
      "Epoch 27136/30000 Training Loss: 0.047224342823028564\n",
      "Epoch 27137/30000 Training Loss: 0.04123522713780403\n",
      "Epoch 27138/30000 Training Loss: 0.028011539950966835\n",
      "Epoch 27139/30000 Training Loss: 0.049566615372896194\n",
      "Epoch 27140/30000 Training Loss: 0.05715087801218033\n",
      "Epoch 27141/30000 Training Loss: 0.03168201819062233\n",
      "Epoch 27142/30000 Training Loss: 0.03185031935572624\n",
      "Epoch 27143/30000 Training Loss: 0.043217502534389496\n",
      "Epoch 27144/30000 Training Loss: 0.029285291209816933\n",
      "Epoch 27145/30000 Training Loss: 0.034818559885025024\n",
      "Epoch 27146/30000 Training Loss: 0.044993966817855835\n",
      "Epoch 27147/30000 Training Loss: 0.04036364331841469\n",
      "Epoch 27148/30000 Training Loss: 0.05573950707912445\n",
      "Epoch 27149/30000 Training Loss: 0.04143952950835228\n",
      "Epoch 27150/30000 Training Loss: 0.041434671729803085\n",
      "Epoch 27151/30000 Training Loss: 0.05035994201898575\n",
      "Epoch 27152/30000 Training Loss: 0.04400533437728882\n",
      "Epoch 27153/30000 Training Loss: 0.03534848242998123\n",
      "Epoch 27154/30000 Training Loss: 0.049263305962085724\n",
      "Epoch 27155/30000 Training Loss: 0.038363970816135406\n",
      "Epoch 27156/30000 Training Loss: 0.03197220712900162\n",
      "Epoch 27157/30000 Training Loss: 0.03228031471371651\n",
      "Epoch 27158/30000 Training Loss: 0.048461899161338806\n",
      "Epoch 27159/30000 Training Loss: 0.039219051599502563\n",
      "Epoch 27160/30000 Training Loss: 0.049138009548187256\n",
      "Epoch 27161/30000 Training Loss: 0.05626195669174194\n",
      "Epoch 27162/30000 Training Loss: 0.04683062434196472\n",
      "Epoch 27163/30000 Training Loss: 0.0332486554980278\n",
      "Epoch 27164/30000 Training Loss: 0.04267441853880882\n",
      "Epoch 27165/30000 Training Loss: 0.03937536105513573\n",
      "Epoch 27166/30000 Training Loss: 0.03505849838256836\n",
      "Epoch 27167/30000 Training Loss: 0.040391210466623306\n",
      "Epoch 27168/30000 Training Loss: 0.039036817848682404\n",
      "Epoch 27169/30000 Training Loss: 0.039507944136857986\n",
      "Epoch 27170/30000 Training Loss: 0.034597959369421005\n",
      "Epoch 27171/30000 Training Loss: 0.033504121005535126\n",
      "Epoch 27172/30000 Training Loss: 0.046111732721328735\n",
      "Epoch 27173/30000 Training Loss: 0.0381525456905365\n",
      "Epoch 27174/30000 Training Loss: 0.036738909780979156\n",
      "Epoch 27175/30000 Training Loss: 0.039925724267959595\n",
      "Epoch 27176/30000 Training Loss: 0.04094550013542175\n",
      "Epoch 27177/30000 Training Loss: 0.045062556862831116\n",
      "Epoch 27178/30000 Training Loss: 0.04834941774606705\n",
      "Epoch 27179/30000 Training Loss: 0.03320112079381943\n",
      "Epoch 27180/30000 Training Loss: 0.04062340781092644\n",
      "Epoch 27181/30000 Training Loss: 0.04305695742368698\n",
      "Epoch 27182/30000 Training Loss: 0.03215903416275978\n",
      "Epoch 27183/30000 Training Loss: 0.035900503396987915\n",
      "Epoch 27184/30000 Training Loss: 0.04579272121191025\n",
      "Epoch 27185/30000 Training Loss: 0.040374673902988434\n",
      "Epoch 27186/30000 Training Loss: 0.04627968370914459\n",
      "Epoch 27187/30000 Training Loss: 0.037776052951812744\n",
      "Epoch 27188/30000 Training Loss: 0.03923904895782471\n",
      "Epoch 27189/30000 Training Loss: 0.033475134521722794\n",
      "Epoch 27190/30000 Training Loss: 0.026434030383825302\n",
      "Epoch 27191/30000 Training Loss: 0.03316599130630493\n",
      "Epoch 27192/30000 Training Loss: 0.035897910594940186\n",
      "Epoch 27193/30000 Training Loss: 0.04633445292711258\n",
      "Epoch 27194/30000 Training Loss: 0.04190109670162201\n",
      "Epoch 27195/30000 Training Loss: 0.037767983973026276\n",
      "Epoch 27196/30000 Training Loss: 0.0396486259996891\n",
      "Epoch 27197/30000 Training Loss: 0.05357510596513748\n",
      "Epoch 27198/30000 Training Loss: 0.04322182759642601\n",
      "Epoch 27199/30000 Training Loss: 0.04719679430127144\n",
      "Epoch 27200/30000 Training Loss: 0.03969661146402359\n",
      "Epoch 27200/30000 Validation Loss: 0.04426731541752815\n",
      "Epoch 27201/30000 Training Loss: 0.048455461859703064\n",
      "Epoch 27202/30000 Training Loss: 0.03888184577226639\n",
      "Epoch 27203/30000 Training Loss: 0.044657886028289795\n",
      "Epoch 27204/30000 Training Loss: 0.05092547833919525\n",
      "Epoch 27205/30000 Training Loss: 0.04476400464773178\n",
      "Epoch 27206/30000 Training Loss: 0.04019352048635483\n",
      "Epoch 27207/30000 Training Loss: 0.02955537661910057\n",
      "Epoch 27208/30000 Training Loss: 0.033933743834495544\n",
      "Epoch 27209/30000 Training Loss: 0.05371365323662758\n",
      "Epoch 27210/30000 Training Loss: 0.03410869836807251\n",
      "Epoch 27211/30000 Training Loss: 0.03208131343126297\n",
      "Epoch 27212/30000 Training Loss: 0.04665511101484299\n",
      "Epoch 27213/30000 Training Loss: 0.03639344498515129\n",
      "Epoch 27214/30000 Training Loss: 0.04114561900496483\n",
      "Epoch 27215/30000 Training Loss: 0.03227946162223816\n",
      "Epoch 27216/30000 Training Loss: 0.03907749056816101\n",
      "Epoch 27217/30000 Training Loss: 0.04310614615678787\n",
      "Epoch 27218/30000 Training Loss: 0.039378292858600616\n",
      "Epoch 27219/30000 Training Loss: 0.05734902620315552\n",
      "Epoch 27220/30000 Training Loss: 0.03363240510225296\n",
      "Epoch 27221/30000 Training Loss: 0.04089775308966637\n",
      "Epoch 27222/30000 Training Loss: 0.04331572726368904\n",
      "Epoch 27223/30000 Training Loss: 0.037607982754707336\n",
      "Epoch 27224/30000 Training Loss: 0.04815366864204407\n",
      "Epoch 27225/30000 Training Loss: 0.04043510556221008\n",
      "Epoch 27226/30000 Training Loss: 0.030577000230550766\n",
      "Epoch 27227/30000 Training Loss: 0.04793688654899597\n",
      "Epoch 27228/30000 Training Loss: 0.04565312713384628\n",
      "Epoch 27229/30000 Training Loss: 0.0385148786008358\n",
      "Epoch 27230/30000 Training Loss: 0.03955453261733055\n",
      "Epoch 27231/30000 Training Loss: 0.04138043522834778\n",
      "Epoch 27232/30000 Training Loss: 0.04389382153749466\n",
      "Epoch 27233/30000 Training Loss: 0.04072955623269081\n",
      "Epoch 27234/30000 Training Loss: 0.03824590519070625\n",
      "Epoch 27235/30000 Training Loss: 0.04937822371721268\n",
      "Epoch 27236/30000 Training Loss: 0.046559881418943405\n",
      "Epoch 27237/30000 Training Loss: 0.05148099362850189\n",
      "Epoch 27238/30000 Training Loss: 0.059294022619724274\n",
      "Epoch 27239/30000 Training Loss: 0.03336546570062637\n",
      "Epoch 27240/30000 Training Loss: 0.05274789780378342\n",
      "Epoch 27241/30000 Training Loss: 0.03390377759933472\n",
      "Epoch 27242/30000 Training Loss: 0.0379815474152565\n",
      "Epoch 27243/30000 Training Loss: 0.04175028204917908\n",
      "Epoch 27244/30000 Training Loss: 0.05114879831671715\n",
      "Epoch 27245/30000 Training Loss: 0.03270832076668739\n",
      "Epoch 27246/30000 Training Loss: 0.042634136974811554\n",
      "Epoch 27247/30000 Training Loss: 0.03343209624290466\n",
      "Epoch 27248/30000 Training Loss: 0.057655639946460724\n",
      "Epoch 27249/30000 Training Loss: 0.031205035746097565\n",
      "Epoch 27250/30000 Training Loss: 0.039032164961099625\n",
      "Epoch 27251/30000 Training Loss: 0.04316654056310654\n",
      "Epoch 27252/30000 Training Loss: 0.03742368519306183\n",
      "Epoch 27253/30000 Training Loss: 0.04495319724082947\n",
      "Epoch 27254/30000 Training Loss: 0.038973618298769\n",
      "Epoch 27255/30000 Training Loss: 0.03610646352171898\n",
      "Epoch 27256/30000 Training Loss: 0.0358041450381279\n",
      "Epoch 27257/30000 Training Loss: 0.03507755696773529\n",
      "Epoch 27258/30000 Training Loss: 0.05298584699630737\n",
      "Epoch 27259/30000 Training Loss: 0.0435200072824955\n",
      "Epoch 27260/30000 Training Loss: 0.044676028192043304\n",
      "Epoch 27261/30000 Training Loss: 0.0385216549038887\n",
      "Epoch 27262/30000 Training Loss: 0.039891116321086884\n",
      "Epoch 27263/30000 Training Loss: 0.05292956531047821\n",
      "Epoch 27264/30000 Training Loss: 0.05122467875480652\n",
      "Epoch 27265/30000 Training Loss: 0.04065193235874176\n",
      "Epoch 27266/30000 Training Loss: 0.04129289463162422\n",
      "Epoch 27267/30000 Training Loss: 0.044830407947301865\n",
      "Epoch 27268/30000 Training Loss: 0.036679357290267944\n",
      "Epoch 27269/30000 Training Loss: 0.047222889959812164\n",
      "Epoch 27270/30000 Training Loss: 0.037058405578136444\n",
      "Epoch 27271/30000 Training Loss: 0.03881927579641342\n",
      "Epoch 27272/30000 Training Loss: 0.03331425413489342\n",
      "Epoch 27273/30000 Training Loss: 0.043641045689582825\n",
      "Epoch 27274/30000 Training Loss: 0.03914839029312134\n",
      "Epoch 27275/30000 Training Loss: 0.03778104484081268\n",
      "Epoch 27276/30000 Training Loss: 0.0397823341190815\n",
      "Epoch 27277/30000 Training Loss: 0.045045286417007446\n",
      "Epoch 27278/30000 Training Loss: 0.03431473299860954\n",
      "Epoch 27279/30000 Training Loss: 0.045139580965042114\n",
      "Epoch 27280/30000 Training Loss: 0.04343365877866745\n",
      "Epoch 27281/30000 Training Loss: 0.043968476355075836\n",
      "Epoch 27282/30000 Training Loss: 0.04113560914993286\n",
      "Epoch 27283/30000 Training Loss: 0.03515446186065674\n",
      "Epoch 27284/30000 Training Loss: 0.04213578253984451\n",
      "Epoch 27285/30000 Training Loss: 0.03800104558467865\n",
      "Epoch 27286/30000 Training Loss: 0.052627939730882645\n",
      "Epoch 27287/30000 Training Loss: 0.04329801723361015\n",
      "Epoch 27288/30000 Training Loss: 0.055076345801353455\n",
      "Epoch 27289/30000 Training Loss: 0.03504326939582825\n",
      "Epoch 27290/30000 Training Loss: 0.03520280495285988\n",
      "Epoch 27291/30000 Training Loss: 0.03480830788612366\n",
      "Epoch 27292/30000 Training Loss: 0.04392792284488678\n",
      "Epoch 27293/30000 Training Loss: 0.04054960235953331\n",
      "Epoch 27294/30000 Training Loss: 0.042343005537986755\n",
      "Epoch 27295/30000 Training Loss: 0.045236214995384216\n",
      "Epoch 27296/30000 Training Loss: 0.04374650493264198\n",
      "Epoch 27297/30000 Training Loss: 0.03510759770870209\n",
      "Epoch 27298/30000 Training Loss: 0.03951900824904442\n",
      "Epoch 27299/30000 Training Loss: 0.03650480881333351\n",
      "Epoch 27300/30000 Training Loss: 0.03850775957107544\n",
      "Epoch 27300/30000 Validation Loss: 0.036827266216278076\n",
      "Epoch 27301/30000 Training Loss: 0.05760612338781357\n",
      "Epoch 27302/30000 Training Loss: 0.037361953407526016\n",
      "Epoch 27303/30000 Training Loss: 0.04005083441734314\n",
      "Epoch 27304/30000 Training Loss: 0.04341602697968483\n",
      "Epoch 27305/30000 Training Loss: 0.04147956520318985\n",
      "Epoch 27306/30000 Training Loss: 0.04424409568309784\n",
      "Epoch 27307/30000 Training Loss: 0.048262014985084534\n",
      "Epoch 27308/30000 Training Loss: 0.042484648525714874\n",
      "Epoch 27309/30000 Training Loss: 0.03467082232236862\n",
      "Epoch 27310/30000 Training Loss: 0.04811949282884598\n",
      "Epoch 27311/30000 Training Loss: 0.04186827689409256\n",
      "Epoch 27312/30000 Training Loss: 0.052070148289203644\n",
      "Epoch 27313/30000 Training Loss: 0.039706386625766754\n",
      "Epoch 27314/30000 Training Loss: 0.041476599872112274\n",
      "Epoch 27315/30000 Training Loss: 0.046720221638679504\n",
      "Epoch 27316/30000 Training Loss: 0.038323063403367996\n",
      "Epoch 27317/30000 Training Loss: 0.04077069088816643\n",
      "Epoch 27318/30000 Training Loss: 0.048876166343688965\n",
      "Epoch 27319/30000 Training Loss: 0.04810361564159393\n",
      "Epoch 27320/30000 Training Loss: 0.03758658841252327\n",
      "Epoch 27321/30000 Training Loss: 0.042537033557891846\n",
      "Epoch 27322/30000 Training Loss: 0.0596684031188488\n",
      "Epoch 27323/30000 Training Loss: 0.0359807088971138\n",
      "Epoch 27324/30000 Training Loss: 0.04548030346632004\n",
      "Epoch 27325/30000 Training Loss: 0.04341580346226692\n",
      "Epoch 27326/30000 Training Loss: 0.04993905499577522\n",
      "Epoch 27327/30000 Training Loss: 0.03871627897024155\n",
      "Epoch 27328/30000 Training Loss: 0.06122998520731926\n",
      "Epoch 27329/30000 Training Loss: 0.0534508191049099\n",
      "Epoch 27330/30000 Training Loss: 0.042087920010089874\n",
      "Epoch 27331/30000 Training Loss: 0.030147673562169075\n",
      "Epoch 27332/30000 Training Loss: 0.036367371678352356\n",
      "Epoch 27333/30000 Training Loss: 0.03371993452310562\n",
      "Epoch 27334/30000 Training Loss: 0.03656294196844101\n",
      "Epoch 27335/30000 Training Loss: 0.052556537091732025\n",
      "Epoch 27336/30000 Training Loss: 0.044986359775066376\n",
      "Epoch 27337/30000 Training Loss: 0.058974143117666245\n",
      "Epoch 27338/30000 Training Loss: 0.04715687781572342\n",
      "Epoch 27339/30000 Training Loss: 0.03853808343410492\n",
      "Epoch 27340/30000 Training Loss: 0.047006528824567795\n",
      "Epoch 27341/30000 Training Loss: 0.060350451618433\n",
      "Epoch 27342/30000 Training Loss: 0.04744924232363701\n",
      "Epoch 27343/30000 Training Loss: 0.04479251056909561\n",
      "Epoch 27344/30000 Training Loss: 0.04240722954273224\n",
      "Epoch 27345/30000 Training Loss: 0.03449805825948715\n",
      "Epoch 27346/30000 Training Loss: 0.036952171474695206\n",
      "Epoch 27347/30000 Training Loss: 0.036647699773311615\n",
      "Epoch 27348/30000 Training Loss: 0.03395964950323105\n",
      "Epoch 27349/30000 Training Loss: 0.044516731053590775\n",
      "Epoch 27350/30000 Training Loss: 0.03154994174838066\n",
      "Epoch 27351/30000 Training Loss: 0.051781222224235535\n",
      "Epoch 27352/30000 Training Loss: 0.05008760839700699\n",
      "Epoch 27353/30000 Training Loss: 0.04954417794942856\n",
      "Epoch 27354/30000 Training Loss: 0.05146728456020355\n",
      "Epoch 27355/30000 Training Loss: 0.0373099260032177\n",
      "Epoch 27356/30000 Training Loss: 0.05217110738158226\n",
      "Epoch 27357/30000 Training Loss: 0.036318909376859665\n",
      "Epoch 27358/30000 Training Loss: 0.05076516792178154\n",
      "Epoch 27359/30000 Training Loss: 0.03632520139217377\n",
      "Epoch 27360/30000 Training Loss: 0.045134324580430984\n",
      "Epoch 27361/30000 Training Loss: 0.04537283629179001\n",
      "Epoch 27362/30000 Training Loss: 0.055631086230278015\n",
      "Epoch 27363/30000 Training Loss: 0.035957835614681244\n",
      "Epoch 27364/30000 Training Loss: 0.03789392113685608\n",
      "Epoch 27365/30000 Training Loss: 0.041993752121925354\n",
      "Epoch 27366/30000 Training Loss: 0.05093245953321457\n",
      "Epoch 27367/30000 Training Loss: 0.0277753546833992\n",
      "Epoch 27368/30000 Training Loss: 0.04413018748164177\n",
      "Epoch 27369/30000 Training Loss: 0.05094045773148537\n",
      "Epoch 27370/30000 Training Loss: 0.040888093411922455\n",
      "Epoch 27371/30000 Training Loss: 0.035973720252513885\n",
      "Epoch 27372/30000 Training Loss: 0.04741582274436951\n",
      "Epoch 27373/30000 Training Loss: 0.03545810282230377\n",
      "Epoch 27374/30000 Training Loss: 0.0425124317407608\n",
      "Epoch 27375/30000 Training Loss: 0.03731774166226387\n",
      "Epoch 27376/30000 Training Loss: 0.038427144289016724\n",
      "Epoch 27377/30000 Training Loss: 0.044738467782735825\n",
      "Epoch 27378/30000 Training Loss: 0.03209291025996208\n",
      "Epoch 27379/30000 Training Loss: 0.03712444007396698\n",
      "Epoch 27380/30000 Training Loss: 0.047969914972782135\n",
      "Epoch 27381/30000 Training Loss: 0.03680102154612541\n",
      "Epoch 27382/30000 Training Loss: 0.04814253747463226\n",
      "Epoch 27383/30000 Training Loss: 0.04500182345509529\n",
      "Epoch 27384/30000 Training Loss: 0.05724768340587616\n",
      "Epoch 27385/30000 Training Loss: 0.05212219059467316\n",
      "Epoch 27386/30000 Training Loss: 0.05248458683490753\n",
      "Epoch 27387/30000 Training Loss: 0.0460883304476738\n",
      "Epoch 27388/30000 Training Loss: 0.05213869363069534\n",
      "Epoch 27389/30000 Training Loss: 0.04861355200409889\n",
      "Epoch 27390/30000 Training Loss: 0.0426003634929657\n",
      "Epoch 27391/30000 Training Loss: 0.03717076778411865\n",
      "Epoch 27392/30000 Training Loss: 0.042803019285202026\n",
      "Epoch 27393/30000 Training Loss: 0.03631403297185898\n",
      "Epoch 27394/30000 Training Loss: 0.04243893921375275\n",
      "Epoch 27395/30000 Training Loss: 0.035503681749105453\n",
      "Epoch 27396/30000 Training Loss: 0.042160000652074814\n",
      "Epoch 27397/30000 Training Loss: 0.042921580374240875\n",
      "Epoch 27398/30000 Training Loss: 0.04165519028902054\n",
      "Epoch 27399/30000 Training Loss: 0.04084709286689758\n",
      "Epoch 27400/30000 Training Loss: 0.054493118077516556\n",
      "Epoch 27400/30000 Validation Loss: 0.039025500416755676\n",
      "Epoch 27401/30000 Training Loss: 0.04291556030511856\n",
      "Epoch 27402/30000 Training Loss: 0.03677812218666077\n",
      "Epoch 27403/30000 Training Loss: 0.04474669694900513\n",
      "Epoch 27404/30000 Training Loss: 0.055717915296554565\n",
      "Epoch 27405/30000 Training Loss: 0.047735992819070816\n",
      "Epoch 27406/30000 Training Loss: 0.0328228585422039\n",
      "Epoch 27407/30000 Training Loss: 0.0415651872754097\n",
      "Epoch 27408/30000 Training Loss: 0.04439237713813782\n",
      "Epoch 27409/30000 Training Loss: 0.04535180702805519\n",
      "Epoch 27410/30000 Training Loss: 0.03835960477590561\n",
      "Epoch 27411/30000 Training Loss: 0.03728502616286278\n",
      "Epoch 27412/30000 Training Loss: 0.05628176033496857\n",
      "Epoch 27413/30000 Training Loss: 0.039734698832035065\n",
      "Epoch 27414/30000 Training Loss: 0.049561791121959686\n",
      "Epoch 27415/30000 Training Loss: 0.059838056564331055\n",
      "Epoch 27416/30000 Training Loss: 0.04113735631108284\n",
      "Epoch 27417/30000 Training Loss: 0.042640529572963715\n",
      "Epoch 27418/30000 Training Loss: 0.033116478472948074\n",
      "Epoch 27419/30000 Training Loss: 0.03627019375562668\n",
      "Epoch 27420/30000 Training Loss: 0.043959882110357285\n",
      "Epoch 27421/30000 Training Loss: 0.05059231445193291\n",
      "Epoch 27422/30000 Training Loss: 0.043979838490486145\n",
      "Epoch 27423/30000 Training Loss: 0.05283214896917343\n",
      "Epoch 27424/30000 Training Loss: 0.036474186927080154\n",
      "Epoch 27425/30000 Training Loss: 0.04389864206314087\n",
      "Epoch 27426/30000 Training Loss: 0.05779997631907463\n",
      "Epoch 27427/30000 Training Loss: 0.046919964253902435\n",
      "Epoch 27428/30000 Training Loss: 0.04405726492404938\n",
      "Epoch 27429/30000 Training Loss: 0.03907068818807602\n",
      "Epoch 27430/30000 Training Loss: 0.04019399732351303\n",
      "Epoch 27431/30000 Training Loss: 0.04361671209335327\n",
      "Epoch 27432/30000 Training Loss: 0.03332052752375603\n",
      "Epoch 27433/30000 Training Loss: 0.029586579650640488\n",
      "Epoch 27434/30000 Training Loss: 0.0589921772480011\n",
      "Epoch 27435/30000 Training Loss: 0.03967845439910889\n",
      "Epoch 27436/30000 Training Loss: 0.04071406275033951\n",
      "Epoch 27437/30000 Training Loss: 0.04338550567626953\n",
      "Epoch 27438/30000 Training Loss: 0.05725114792585373\n",
      "Epoch 27439/30000 Training Loss: 0.0401596836745739\n",
      "Epoch 27440/30000 Training Loss: 0.04252684488892555\n",
      "Epoch 27441/30000 Training Loss: 0.045908309519290924\n",
      "Epoch 27442/30000 Training Loss: 0.0465410016477108\n",
      "Epoch 27443/30000 Training Loss: 0.03367738798260689\n",
      "Epoch 27444/30000 Training Loss: 0.042848676443099976\n",
      "Epoch 27445/30000 Training Loss: 0.038663238286972046\n",
      "Epoch 27446/30000 Training Loss: 0.030380072072148323\n",
      "Epoch 27447/30000 Training Loss: 0.036253273487091064\n",
      "Epoch 27448/30000 Training Loss: 0.03524558246135712\n",
      "Epoch 27449/30000 Training Loss: 0.045827239751815796\n",
      "Epoch 27450/30000 Training Loss: 0.033991388976573944\n",
      "Epoch 27451/30000 Training Loss: 0.04877842962741852\n",
      "Epoch 27452/30000 Training Loss: 0.03298927843570709\n",
      "Epoch 27453/30000 Training Loss: 0.03781600669026375\n",
      "Epoch 27454/30000 Training Loss: 0.04267174005508423\n",
      "Epoch 27455/30000 Training Loss: 0.03503728657960892\n",
      "Epoch 27456/30000 Training Loss: 0.05062199383974075\n",
      "Epoch 27457/30000 Training Loss: 0.058039069175720215\n",
      "Epoch 27458/30000 Training Loss: 0.0559941902756691\n",
      "Epoch 27459/30000 Training Loss: 0.031769562512636185\n",
      "Epoch 27460/30000 Training Loss: 0.03702791780233383\n",
      "Epoch 27461/30000 Training Loss: 0.04435545578598976\n",
      "Epoch 27462/30000 Training Loss: 0.04752369970083237\n",
      "Epoch 27463/30000 Training Loss: 0.03179922327399254\n",
      "Epoch 27464/30000 Training Loss: 0.04577353596687317\n",
      "Epoch 27465/30000 Training Loss: 0.033162519335746765\n",
      "Epoch 27466/30000 Training Loss: 0.04071924462914467\n",
      "Epoch 27467/30000 Training Loss: 0.03601359203457832\n",
      "Epoch 27468/30000 Training Loss: 0.035811856389045715\n",
      "Epoch 27469/30000 Training Loss: 0.03745130077004433\n",
      "Epoch 27470/30000 Training Loss: 0.043050386011600494\n",
      "Epoch 27471/30000 Training Loss: 0.05349857360124588\n",
      "Epoch 27472/30000 Training Loss: 0.05622662603855133\n",
      "Epoch 27473/30000 Training Loss: 0.04136504977941513\n",
      "Epoch 27474/30000 Training Loss: 0.04300861060619354\n",
      "Epoch 27475/30000 Training Loss: 0.04223494231700897\n",
      "Epoch 27476/30000 Training Loss: 0.04661986976861954\n",
      "Epoch 27477/30000 Training Loss: 0.042690739035606384\n",
      "Epoch 27478/30000 Training Loss: 0.04337979853153229\n",
      "Epoch 27479/30000 Training Loss: 0.04593098163604736\n",
      "Epoch 27480/30000 Training Loss: 0.03747577220201492\n",
      "Epoch 27481/30000 Training Loss: 0.04667777195572853\n",
      "Epoch 27482/30000 Training Loss: 0.0335506945848465\n",
      "Epoch 27483/30000 Training Loss: 0.03635791689157486\n",
      "Epoch 27484/30000 Training Loss: 0.04004894942045212\n",
      "Epoch 27485/30000 Training Loss: 0.03159685060381889\n",
      "Epoch 27486/30000 Training Loss: 0.03225089982151985\n",
      "Epoch 27487/30000 Training Loss: 0.04739950969815254\n",
      "Epoch 27488/30000 Training Loss: 0.0408310741186142\n",
      "Epoch 27489/30000 Training Loss: 0.0433586947619915\n",
      "Epoch 27490/30000 Training Loss: 0.03273499757051468\n",
      "Epoch 27491/30000 Training Loss: 0.04472234845161438\n",
      "Epoch 27492/30000 Training Loss: 0.03831334784626961\n",
      "Epoch 27493/30000 Training Loss: 0.0349605530500412\n",
      "Epoch 27494/30000 Training Loss: 0.03984089195728302\n",
      "Epoch 27495/30000 Training Loss: 0.04627513140439987\n",
      "Epoch 27496/30000 Training Loss: 0.05593384802341461\n",
      "Epoch 27497/30000 Training Loss: 0.040172770619392395\n",
      "Epoch 27498/30000 Training Loss: 0.031612329185009\n",
      "Epoch 27499/30000 Training Loss: 0.030756786465644836\n",
      "Epoch 27500/30000 Training Loss: 0.03355562686920166\n",
      "Epoch 27500/30000 Validation Loss: 0.035861797630786896\n",
      "Epoch 27501/30000 Training Loss: 0.038052499294281006\n",
      "Epoch 27502/30000 Training Loss: 0.03743317723274231\n",
      "Epoch 27503/30000 Training Loss: 0.03643159568309784\n",
      "Epoch 27504/30000 Training Loss: 0.04052651673555374\n",
      "Epoch 27505/30000 Training Loss: 0.04879213124513626\n",
      "Epoch 27506/30000 Training Loss: 0.046033017337322235\n",
      "Epoch 27507/30000 Training Loss: 0.036440350115299225\n",
      "Epoch 27508/30000 Training Loss: 0.04335073381662369\n",
      "Epoch 27509/30000 Training Loss: 0.035343438386917114\n",
      "Epoch 27510/30000 Training Loss: 0.03538946434855461\n",
      "Epoch 27511/30000 Training Loss: 0.04442986100912094\n",
      "Epoch 27512/30000 Training Loss: 0.04478105157613754\n",
      "Epoch 27513/30000 Training Loss: 0.05234801396727562\n",
      "Epoch 27514/30000 Training Loss: 0.03877127170562744\n",
      "Epoch 27515/30000 Training Loss: 0.04821140691637993\n",
      "Epoch 27516/30000 Training Loss: 0.04486347734928131\n",
      "Epoch 27517/30000 Training Loss: 0.04534803330898285\n",
      "Epoch 27518/30000 Training Loss: 0.045588456094264984\n",
      "Epoch 27519/30000 Training Loss: 0.04488573968410492\n",
      "Epoch 27520/30000 Training Loss: 0.041205596178770065\n",
      "Epoch 27521/30000 Training Loss: 0.04263029247522354\n",
      "Epoch 27522/30000 Training Loss: 0.04498600214719772\n",
      "Epoch 27523/30000 Training Loss: 0.045500487089157104\n",
      "Epoch 27524/30000 Training Loss: 0.05233508348464966\n",
      "Epoch 27525/30000 Training Loss: 0.04579116404056549\n",
      "Epoch 27526/30000 Training Loss: 0.0462002232670784\n",
      "Epoch 27527/30000 Training Loss: 0.03845905512571335\n",
      "Epoch 27528/30000 Training Loss: 0.05040007829666138\n",
      "Epoch 27529/30000 Training Loss: 0.04816006124019623\n",
      "Epoch 27530/30000 Training Loss: 0.042551442980766296\n",
      "Epoch 27531/30000 Training Loss: 0.04293424263596535\n",
      "Epoch 27532/30000 Training Loss: 0.04884140193462372\n",
      "Epoch 27533/30000 Training Loss: 0.04779001325368881\n",
      "Epoch 27534/30000 Training Loss: 0.039099596440792084\n",
      "Epoch 27535/30000 Training Loss: 0.04866687208414078\n",
      "Epoch 27536/30000 Training Loss: 0.04030724987387657\n",
      "Epoch 27537/30000 Training Loss: 0.0528273768723011\n",
      "Epoch 27538/30000 Training Loss: 0.04936368018388748\n",
      "Epoch 27539/30000 Training Loss: 0.05911353975534439\n",
      "Epoch 27540/30000 Training Loss: 0.0526040680706501\n",
      "Epoch 27541/30000 Training Loss: 0.05063021555542946\n",
      "Epoch 27542/30000 Training Loss: 0.03919151797890663\n",
      "Epoch 27543/30000 Training Loss: 0.05011112987995148\n",
      "Epoch 27544/30000 Training Loss: 0.040332067757844925\n",
      "Epoch 27545/30000 Training Loss: 0.044867873191833496\n",
      "Epoch 27546/30000 Training Loss: 0.053378842771053314\n",
      "Epoch 27547/30000 Training Loss: 0.05111658573150635\n",
      "Epoch 27548/30000 Training Loss: 0.053313106298446655\n",
      "Epoch 27549/30000 Training Loss: 0.055958040058612823\n",
      "Epoch 27550/30000 Training Loss: 0.06140969693660736\n",
      "Epoch 27551/30000 Training Loss: 0.04081089049577713\n",
      "Epoch 27552/30000 Training Loss: 0.04950094595551491\n",
      "Epoch 27553/30000 Training Loss: 0.03751035034656525\n",
      "Epoch 27554/30000 Training Loss: 0.04192080348730087\n",
      "Epoch 27555/30000 Training Loss: 0.04442533105611801\n",
      "Epoch 27556/30000 Training Loss: 0.030467141419649124\n",
      "Epoch 27557/30000 Training Loss: 0.04309697449207306\n",
      "Epoch 27558/30000 Training Loss: 0.04232392460107803\n",
      "Epoch 27559/30000 Training Loss: 0.04730808362364769\n",
      "Epoch 27560/30000 Training Loss: 0.04783067852258682\n",
      "Epoch 27561/30000 Training Loss: 0.04036569595336914\n",
      "Epoch 27562/30000 Training Loss: 0.04174545407295227\n",
      "Epoch 27563/30000 Training Loss: 0.03942373767495155\n",
      "Epoch 27564/30000 Training Loss: 0.04653230309486389\n",
      "Epoch 27565/30000 Training Loss: 0.03907286375761032\n",
      "Epoch 27566/30000 Training Loss: 0.032720889896154404\n",
      "Epoch 27567/30000 Training Loss: 0.0467953085899353\n",
      "Epoch 27568/30000 Training Loss: 0.03464905172586441\n",
      "Epoch 27569/30000 Training Loss: 0.052500925958156586\n",
      "Epoch 27570/30000 Training Loss: 0.05754855275154114\n",
      "Epoch 27571/30000 Training Loss: 0.04857548326253891\n",
      "Epoch 27572/30000 Training Loss: 0.04196137934923172\n",
      "Epoch 27573/30000 Training Loss: 0.04026022180914879\n",
      "Epoch 27574/30000 Training Loss: 0.036891594529151917\n",
      "Epoch 27575/30000 Training Loss: 0.04214015603065491\n",
      "Epoch 27576/30000 Training Loss: 0.04129745811223984\n",
      "Epoch 27577/30000 Training Loss: 0.029264647513628006\n",
      "Epoch 27578/30000 Training Loss: 0.04669487103819847\n",
      "Epoch 27579/30000 Training Loss: 0.05259895324707031\n",
      "Epoch 27580/30000 Training Loss: 0.04437823221087456\n",
      "Epoch 27581/30000 Training Loss: 0.06279084831476212\n",
      "Epoch 27582/30000 Training Loss: 0.04310101270675659\n",
      "Epoch 27583/30000 Training Loss: 0.03349027782678604\n",
      "Epoch 27584/30000 Training Loss: 0.03142215311527252\n",
      "Epoch 27585/30000 Training Loss: 0.05126643180847168\n",
      "Epoch 27586/30000 Training Loss: 0.04497724026441574\n",
      "Epoch 27587/30000 Training Loss: 0.0473003163933754\n",
      "Epoch 27588/30000 Training Loss: 0.04477827996015549\n",
      "Epoch 27589/30000 Training Loss: 0.04179045930504799\n",
      "Epoch 27590/30000 Training Loss: 0.03321792930364609\n",
      "Epoch 27591/30000 Training Loss: 0.04458368569612503\n",
      "Epoch 27592/30000 Training Loss: 0.04693704843521118\n",
      "Epoch 27593/30000 Training Loss: 0.04345705360174179\n",
      "Epoch 27594/30000 Training Loss: 0.045170485973358154\n",
      "Epoch 27595/30000 Training Loss: 0.04054220765829086\n",
      "Epoch 27596/30000 Training Loss: 0.05432174354791641\n",
      "Epoch 27597/30000 Training Loss: 0.048570483922958374\n",
      "Epoch 27598/30000 Training Loss: 0.052551768720149994\n",
      "Epoch 27599/30000 Training Loss: 0.04752352461218834\n",
      "Epoch 27600/30000 Training Loss: 0.03537023067474365\n",
      "Epoch 27600/30000 Validation Loss: 0.03229832276701927\n",
      "Epoch 27601/30000 Training Loss: 0.0520036518573761\n",
      "Epoch 27602/30000 Training Loss: 0.0495145246386528\n",
      "Epoch 27603/30000 Training Loss: 0.0515529066324234\n",
      "Epoch 27604/30000 Training Loss: 0.03652118146419525\n",
      "Epoch 27605/30000 Training Loss: 0.04491022974252701\n",
      "Epoch 27606/30000 Training Loss: 0.029598742723464966\n",
      "Epoch 27607/30000 Training Loss: 0.03334856405854225\n",
      "Epoch 27608/30000 Training Loss: 0.0410275012254715\n",
      "Epoch 27609/30000 Training Loss: 0.04189213365316391\n",
      "Epoch 27610/30000 Training Loss: 0.04404497146606445\n",
      "Epoch 27611/30000 Training Loss: 0.03424476087093353\n",
      "Epoch 27612/30000 Training Loss: 0.040479905903339386\n",
      "Epoch 27613/30000 Training Loss: 0.03120289370417595\n",
      "Epoch 27614/30000 Training Loss: 0.02870236709713936\n",
      "Epoch 27615/30000 Training Loss: 0.04287933558225632\n",
      "Epoch 27616/30000 Training Loss: 0.036048825830221176\n",
      "Epoch 27617/30000 Training Loss: 0.03730423003435135\n",
      "Epoch 27618/30000 Training Loss: 0.038851864635944366\n",
      "Epoch 27619/30000 Training Loss: 0.0430164635181427\n",
      "Epoch 27620/30000 Training Loss: 0.044724658131599426\n",
      "Epoch 27621/30000 Training Loss: 0.03806566447019577\n",
      "Epoch 27622/30000 Training Loss: 0.04983096942305565\n",
      "Epoch 27623/30000 Training Loss: 0.052975960075855255\n",
      "Epoch 27624/30000 Training Loss: 0.04092692956328392\n",
      "Epoch 27625/30000 Training Loss: 0.048573367297649384\n",
      "Epoch 27626/30000 Training Loss: 0.03962533175945282\n",
      "Epoch 27627/30000 Training Loss: 0.033798832446336746\n",
      "Epoch 27628/30000 Training Loss: 0.039136793464422226\n",
      "Epoch 27629/30000 Training Loss: 0.043772678822278976\n",
      "Epoch 27630/30000 Training Loss: 0.04212760925292969\n",
      "Epoch 27631/30000 Training Loss: 0.05866764485836029\n",
      "Epoch 27632/30000 Training Loss: 0.03877141326665878\n",
      "Epoch 27633/30000 Training Loss: 0.03380206227302551\n",
      "Epoch 27634/30000 Training Loss: 0.04437306895852089\n",
      "Epoch 27635/30000 Training Loss: 0.03129241615533829\n",
      "Epoch 27636/30000 Training Loss: 0.04133334010839462\n",
      "Epoch 27637/30000 Training Loss: 0.03782888129353523\n",
      "Epoch 27638/30000 Training Loss: 0.05170707404613495\n",
      "Epoch 27639/30000 Training Loss: 0.04920457676053047\n",
      "Epoch 27640/30000 Training Loss: 0.03837696835398674\n",
      "Epoch 27641/30000 Training Loss: 0.0362849086523056\n",
      "Epoch 27642/30000 Training Loss: 0.03654521703720093\n",
      "Epoch 27643/30000 Training Loss: 0.04982258751988411\n",
      "Epoch 27644/30000 Training Loss: 0.053204990923404694\n",
      "Epoch 27645/30000 Training Loss: 0.04675263911485672\n",
      "Epoch 27646/30000 Training Loss: 0.04283025115728378\n",
      "Epoch 27647/30000 Training Loss: 0.032853372395038605\n",
      "Epoch 27648/30000 Training Loss: 0.05770588666200638\n",
      "Epoch 27649/30000 Training Loss: 0.04878684878349304\n",
      "Epoch 27650/30000 Training Loss: 0.03579537943005562\n",
      "Epoch 27651/30000 Training Loss: 0.03652845323085785\n",
      "Epoch 27652/30000 Training Loss: 0.03928317874670029\n",
      "Epoch 27653/30000 Training Loss: 0.03882652148604393\n",
      "Epoch 27654/30000 Training Loss: 0.0330115407705307\n",
      "Epoch 27655/30000 Training Loss: 0.03247857838869095\n",
      "Epoch 27656/30000 Training Loss: 0.03380560502409935\n",
      "Epoch 27657/30000 Training Loss: 0.038326866924762726\n",
      "Epoch 27658/30000 Training Loss: 0.027259528636932373\n",
      "Epoch 27659/30000 Training Loss: 0.04352812469005585\n",
      "Epoch 27660/30000 Training Loss: 0.031764477491378784\n",
      "Epoch 27661/30000 Training Loss: 0.05764804035425186\n",
      "Epoch 27662/30000 Training Loss: 0.033574823290109634\n",
      "Epoch 27663/30000 Training Loss: 0.04844473674893379\n",
      "Epoch 27664/30000 Training Loss: 0.04087347164750099\n",
      "Epoch 27665/30000 Training Loss: 0.05192761868238449\n",
      "Epoch 27666/30000 Training Loss: 0.05003692954778671\n",
      "Epoch 27667/30000 Training Loss: 0.04778081551194191\n",
      "Epoch 27668/30000 Training Loss: 0.04059137403964996\n",
      "Epoch 27669/30000 Training Loss: 0.04152235761284828\n",
      "Epoch 27670/30000 Training Loss: 0.038338594138622284\n",
      "Epoch 27671/30000 Training Loss: 0.044373080134391785\n",
      "Epoch 27672/30000 Training Loss: 0.04657748341560364\n",
      "Epoch 27673/30000 Training Loss: 0.045323342084884644\n",
      "Epoch 27674/30000 Training Loss: 0.0432429239153862\n",
      "Epoch 27675/30000 Training Loss: 0.04938458651304245\n",
      "Epoch 27676/30000 Training Loss: 0.042712800204753876\n",
      "Epoch 27677/30000 Training Loss: 0.051397375762462616\n",
      "Epoch 27678/30000 Training Loss: 0.04485078528523445\n",
      "Epoch 27679/30000 Training Loss: 0.04108979552984238\n",
      "Epoch 27680/30000 Training Loss: 0.04258989542722702\n",
      "Epoch 27681/30000 Training Loss: 0.05422256514430046\n",
      "Epoch 27682/30000 Training Loss: 0.03780040144920349\n",
      "Epoch 27683/30000 Training Loss: 0.040474701672792435\n",
      "Epoch 27684/30000 Training Loss: 0.045070961117744446\n",
      "Epoch 27685/30000 Training Loss: 0.05894403159618378\n",
      "Epoch 27686/30000 Training Loss: 0.030913271009922028\n",
      "Epoch 27687/30000 Training Loss: 0.03933361917734146\n",
      "Epoch 27688/30000 Training Loss: 0.034836553037166595\n",
      "Epoch 27689/30000 Training Loss: 0.03735470026731491\n",
      "Epoch 27690/30000 Training Loss: 0.02815890684723854\n",
      "Epoch 27691/30000 Training Loss: 0.03681164234876633\n",
      "Epoch 27692/30000 Training Loss: 0.04046066477894783\n",
      "Epoch 27693/30000 Training Loss: 0.04484127461910248\n",
      "Epoch 27694/30000 Training Loss: 0.0398479588329792\n",
      "Epoch 27695/30000 Training Loss: 0.045535147190093994\n",
      "Epoch 27696/30000 Training Loss: 0.06186382472515106\n",
      "Epoch 27697/30000 Training Loss: 0.06618784368038177\n",
      "Epoch 27698/30000 Training Loss: 0.046431299299001694\n",
      "Epoch 27699/30000 Training Loss: 0.05346599966287613\n",
      "Epoch 27700/30000 Training Loss: 0.034883297979831696\n",
      "Epoch 27700/30000 Validation Loss: 0.05223926901817322\n",
      "Epoch 27701/30000 Training Loss: 0.03871273994445801\n",
      "Epoch 27702/30000 Training Loss: 0.04304755479097366\n",
      "Epoch 27703/30000 Training Loss: 0.059348609298467636\n",
      "Epoch 27704/30000 Training Loss: 0.0531197264790535\n",
      "Epoch 27705/30000 Training Loss: 0.04236045107245445\n",
      "Epoch 27706/30000 Training Loss: 0.048357460647821426\n",
      "Epoch 27707/30000 Training Loss: 0.03295757621526718\n",
      "Epoch 27708/30000 Training Loss: 0.040509603917598724\n",
      "Epoch 27709/30000 Training Loss: 0.04232914000749588\n",
      "Epoch 27710/30000 Training Loss: 0.03741680830717087\n",
      "Epoch 27711/30000 Training Loss: 0.03620497137308121\n",
      "Epoch 27712/30000 Training Loss: 0.0449126660823822\n",
      "Epoch 27713/30000 Training Loss: 0.04754382371902466\n",
      "Epoch 27714/30000 Training Loss: 0.05777329206466675\n",
      "Epoch 27715/30000 Training Loss: 0.05367017537355423\n",
      "Epoch 27716/30000 Training Loss: 0.04230830818414688\n",
      "Epoch 27717/30000 Training Loss: 0.03323981165885925\n",
      "Epoch 27718/30000 Training Loss: 0.04372358322143555\n",
      "Epoch 27719/30000 Training Loss: 0.056145377457141876\n",
      "Epoch 27720/30000 Training Loss: 0.035092927515506744\n",
      "Epoch 27721/30000 Training Loss: 0.04929010570049286\n",
      "Epoch 27722/30000 Training Loss: 0.04955983906984329\n",
      "Epoch 27723/30000 Training Loss: 0.05398382246494293\n",
      "Epoch 27724/30000 Training Loss: 0.04715881496667862\n",
      "Epoch 27725/30000 Training Loss: 0.05493279546499252\n",
      "Epoch 27726/30000 Training Loss: 0.03832152858376503\n",
      "Epoch 27727/30000 Training Loss: 0.032192230224609375\n",
      "Epoch 27728/30000 Training Loss: 0.03268684074282646\n",
      "Epoch 27729/30000 Training Loss: 0.029956094920635223\n",
      "Epoch 27730/30000 Training Loss: 0.03768492490053177\n",
      "Epoch 27731/30000 Training Loss: 0.04086216166615486\n",
      "Epoch 27732/30000 Training Loss: 0.04591591656208038\n",
      "Epoch 27733/30000 Training Loss: 0.0452742725610733\n",
      "Epoch 27734/30000 Training Loss: 0.03745695948600769\n",
      "Epoch 27735/30000 Training Loss: 0.03248237073421478\n",
      "Epoch 27736/30000 Training Loss: 0.04902370274066925\n",
      "Epoch 27737/30000 Training Loss: 0.042187225073575974\n",
      "Epoch 27738/30000 Training Loss: 0.04801572114229202\n",
      "Epoch 27739/30000 Training Loss: 0.0383591465651989\n",
      "Epoch 27740/30000 Training Loss: 0.04079875349998474\n",
      "Epoch 27741/30000 Training Loss: 0.02806861326098442\n",
      "Epoch 27742/30000 Training Loss: 0.040491171181201935\n",
      "Epoch 27743/30000 Training Loss: 0.0475701242685318\n",
      "Epoch 27744/30000 Training Loss: 0.06288378685712814\n",
      "Epoch 27745/30000 Training Loss: 0.03756602853536606\n",
      "Epoch 27746/30000 Training Loss: 0.049538083374500275\n",
      "Epoch 27747/30000 Training Loss: 0.03982659429311752\n",
      "Epoch 27748/30000 Training Loss: 0.05234752967953682\n",
      "Epoch 27749/30000 Training Loss: 0.032264601439237595\n",
      "Epoch 27750/30000 Training Loss: 0.04415208101272583\n",
      "Epoch 27751/30000 Training Loss: 0.04987860471010208\n",
      "Epoch 27752/30000 Training Loss: 0.03648403286933899\n",
      "Epoch 27753/30000 Training Loss: 0.03213164955377579\n",
      "Epoch 27754/30000 Training Loss: 0.04715190827846527\n",
      "Epoch 27755/30000 Training Loss: 0.04073287546634674\n",
      "Epoch 27756/30000 Training Loss: 0.04608060047030449\n",
      "Epoch 27757/30000 Training Loss: 0.04429001361131668\n",
      "Epoch 27758/30000 Training Loss: 0.040762759745121\n",
      "Epoch 27759/30000 Training Loss: 0.038563258945941925\n",
      "Epoch 27760/30000 Training Loss: 0.04941760376095772\n",
      "Epoch 27761/30000 Training Loss: 0.05516263097524643\n",
      "Epoch 27762/30000 Training Loss: 0.040512196719646454\n",
      "Epoch 27763/30000 Training Loss: 0.0355881005525589\n",
      "Epoch 27764/30000 Training Loss: 0.04760460555553436\n",
      "Epoch 27765/30000 Training Loss: 0.040581174194812775\n",
      "Epoch 27766/30000 Training Loss: 0.025544891133904457\n",
      "Epoch 27767/30000 Training Loss: 0.043953657150268555\n",
      "Epoch 27768/30000 Training Loss: 0.03247591480612755\n",
      "Epoch 27769/30000 Training Loss: 0.03491858020424843\n",
      "Epoch 27770/30000 Training Loss: 0.043867893517017365\n",
      "Epoch 27771/30000 Training Loss: 0.04704193398356438\n",
      "Epoch 27772/30000 Training Loss: 0.03659616410732269\n",
      "Epoch 27773/30000 Training Loss: 0.04608117789030075\n",
      "Epoch 27774/30000 Training Loss: 0.045603640377521515\n",
      "Epoch 27775/30000 Training Loss: 0.04664672911167145\n",
      "Epoch 27776/30000 Training Loss: 0.04217090457677841\n",
      "Epoch 27777/30000 Training Loss: 0.04396583139896393\n",
      "Epoch 27778/30000 Training Loss: 0.04228968918323517\n",
      "Epoch 27779/30000 Training Loss: 0.040489561855793\n",
      "Epoch 27780/30000 Training Loss: 0.04554758593440056\n",
      "Epoch 27781/30000 Training Loss: 0.04734671488404274\n",
      "Epoch 27782/30000 Training Loss: 0.03817635029554367\n",
      "Epoch 27783/30000 Training Loss: 0.0324753038585186\n",
      "Epoch 27784/30000 Training Loss: 0.0391581691801548\n",
      "Epoch 27785/30000 Training Loss: 0.036263566464185715\n",
      "Epoch 27786/30000 Training Loss: 0.04923229664564133\n",
      "Epoch 27787/30000 Training Loss: 0.04297787696123123\n",
      "Epoch 27788/30000 Training Loss: 0.04504745081067085\n",
      "Epoch 27789/30000 Training Loss: 0.03949885442852974\n",
      "Epoch 27790/30000 Training Loss: 0.05385429784655571\n",
      "Epoch 27791/30000 Training Loss: 0.04429013282060623\n",
      "Epoch 27792/30000 Training Loss: 0.037892818450927734\n",
      "Epoch 27793/30000 Training Loss: 0.04365690052509308\n",
      "Epoch 27794/30000 Training Loss: 0.0462215282022953\n",
      "Epoch 27795/30000 Training Loss: 0.046328142285346985\n",
      "Epoch 27796/30000 Training Loss: 0.04918903857469559\n",
      "Epoch 27797/30000 Training Loss: 0.048673272132873535\n",
      "Epoch 27798/30000 Training Loss: 0.036501795053482056\n",
      "Epoch 27799/30000 Training Loss: 0.03405163437128067\n",
      "Epoch 27800/30000 Training Loss: 0.052235547453165054\n",
      "Epoch 27800/30000 Validation Loss: 0.03233560174703598\n",
      "Epoch 27801/30000 Training Loss: 0.03545127809047699\n",
      "Epoch 27802/30000 Training Loss: 0.04828944057226181\n",
      "Epoch 27803/30000 Training Loss: 0.0457361675798893\n",
      "Epoch 27804/30000 Training Loss: 0.03818855062127113\n",
      "Epoch 27805/30000 Training Loss: 0.044414643198251724\n",
      "Epoch 27806/30000 Training Loss: 0.04721403494477272\n",
      "Epoch 27807/30000 Training Loss: 0.049356941133737564\n",
      "Epoch 27808/30000 Training Loss: 0.03565943241119385\n",
      "Epoch 27809/30000 Training Loss: 0.04284229874610901\n",
      "Epoch 27810/30000 Training Loss: 0.04278523102402687\n",
      "Epoch 27811/30000 Training Loss: 0.042465582489967346\n",
      "Epoch 27812/30000 Training Loss: 0.039598140865564346\n",
      "Epoch 27813/30000 Training Loss: 0.04001953825354576\n",
      "Epoch 27814/30000 Training Loss: 0.0690414160490036\n",
      "Epoch 27815/30000 Training Loss: 0.03296557068824768\n",
      "Epoch 27816/30000 Training Loss: 0.040057260543107986\n",
      "Epoch 27817/30000 Training Loss: 0.0584527887403965\n",
      "Epoch 27818/30000 Training Loss: 0.04290489852428436\n",
      "Epoch 27819/30000 Training Loss: 0.03020501881837845\n",
      "Epoch 27820/30000 Training Loss: 0.04962639510631561\n",
      "Epoch 27821/30000 Training Loss: 0.035331062972545624\n",
      "Epoch 27822/30000 Training Loss: 0.03334955498576164\n",
      "Epoch 27823/30000 Training Loss: 0.034366197884082794\n",
      "Epoch 27824/30000 Training Loss: 0.03286390006542206\n",
      "Epoch 27825/30000 Training Loss: 0.03423045948147774\n",
      "Epoch 27826/30000 Training Loss: 0.048727259039878845\n",
      "Epoch 27827/30000 Training Loss: 0.04550279676914215\n",
      "Epoch 27828/30000 Training Loss: 0.07152517139911652\n",
      "Epoch 27829/30000 Training Loss: 0.04961105436086655\n",
      "Epoch 27830/30000 Training Loss: 0.04189692810177803\n",
      "Epoch 27831/30000 Training Loss: 0.040416352450847626\n",
      "Epoch 27832/30000 Training Loss: 0.045836061239242554\n",
      "Epoch 27833/30000 Training Loss: 0.047040216624736786\n",
      "Epoch 27834/30000 Training Loss: 0.04375912621617317\n",
      "Epoch 27835/30000 Training Loss: 0.05422094464302063\n",
      "Epoch 27836/30000 Training Loss: 0.03977912664413452\n",
      "Epoch 27837/30000 Training Loss: 0.059502311050891876\n",
      "Epoch 27838/30000 Training Loss: 0.04145943373441696\n",
      "Epoch 27839/30000 Training Loss: 0.05488552525639534\n",
      "Epoch 27840/30000 Training Loss: 0.04015612602233887\n",
      "Epoch 27841/30000 Training Loss: 0.046333517879247665\n",
      "Epoch 27842/30000 Training Loss: 0.0301812756806612\n",
      "Epoch 27843/30000 Training Loss: 0.055665940046310425\n",
      "Epoch 27844/30000 Training Loss: 0.04171385616064072\n",
      "Epoch 27845/30000 Training Loss: 0.04521549493074417\n",
      "Epoch 27846/30000 Training Loss: 0.0517234280705452\n",
      "Epoch 27847/30000 Training Loss: 0.05146126449108124\n",
      "Epoch 27848/30000 Training Loss: 0.044305089861154556\n",
      "Epoch 27849/30000 Training Loss: 0.04452746361494064\n",
      "Epoch 27850/30000 Training Loss: 0.04325127601623535\n",
      "Epoch 27851/30000 Training Loss: 0.04898577928543091\n",
      "Epoch 27852/30000 Training Loss: 0.04377783089876175\n",
      "Epoch 27853/30000 Training Loss: 0.049325793981552124\n",
      "Epoch 27854/30000 Training Loss: 0.04296867549419403\n",
      "Epoch 27855/30000 Training Loss: 0.03779960051178932\n",
      "Epoch 27856/30000 Training Loss: 0.04557890444993973\n",
      "Epoch 27857/30000 Training Loss: 0.038616813719272614\n",
      "Epoch 27858/30000 Training Loss: 0.05289173871278763\n",
      "Epoch 27859/30000 Training Loss: 0.05451487749814987\n",
      "Epoch 27860/30000 Training Loss: 0.03372183442115784\n",
      "Epoch 27861/30000 Training Loss: 0.03574352711439133\n",
      "Epoch 27862/30000 Training Loss: 0.05727630853652954\n",
      "Epoch 27863/30000 Training Loss: 0.043003834784030914\n",
      "Epoch 27864/30000 Training Loss: 0.04812920093536377\n",
      "Epoch 27865/30000 Training Loss: 0.04730457812547684\n",
      "Epoch 27866/30000 Training Loss: 0.034896403551101685\n",
      "Epoch 27867/30000 Training Loss: 0.0363551527261734\n",
      "Epoch 27868/30000 Training Loss: 0.03950681537389755\n",
      "Epoch 27869/30000 Training Loss: 0.04704563319683075\n",
      "Epoch 27870/30000 Training Loss: 0.047836706042289734\n",
      "Epoch 27871/30000 Training Loss: 0.04053240269422531\n",
      "Epoch 27872/30000 Training Loss: 0.03908857703208923\n",
      "Epoch 27873/30000 Training Loss: 0.03332432359457016\n",
      "Epoch 27874/30000 Training Loss: 0.03894679993391037\n",
      "Epoch 27875/30000 Training Loss: 0.0448957160115242\n",
      "Epoch 27876/30000 Training Loss: 0.04881586506962776\n",
      "Epoch 27877/30000 Training Loss: 0.042904552072286606\n",
      "Epoch 27878/30000 Training Loss: 0.05207853019237518\n",
      "Epoch 27879/30000 Training Loss: 0.05161451920866966\n",
      "Epoch 27880/30000 Training Loss: 0.05311239883303642\n",
      "Epoch 27881/30000 Training Loss: 0.03611981123685837\n",
      "Epoch 27882/30000 Training Loss: 0.039126183837652206\n",
      "Epoch 27883/30000 Training Loss: 0.035865187644958496\n",
      "Epoch 27884/30000 Training Loss: 0.048039812594652176\n",
      "Epoch 27885/30000 Training Loss: 0.04150277376174927\n",
      "Epoch 27886/30000 Training Loss: 0.05238550156354904\n",
      "Epoch 27887/30000 Training Loss: 0.0411774218082428\n",
      "Epoch 27888/30000 Training Loss: 0.053467679768800735\n",
      "Epoch 27889/30000 Training Loss: 0.04741182178258896\n",
      "Epoch 27890/30000 Training Loss: 0.05604889616370201\n",
      "Epoch 27891/30000 Training Loss: 0.03451225161552429\n",
      "Epoch 27892/30000 Training Loss: 0.03655974194407463\n",
      "Epoch 27893/30000 Training Loss: 0.05165110155940056\n",
      "Epoch 27894/30000 Training Loss: 0.047069206833839417\n",
      "Epoch 27895/30000 Training Loss: 0.037431489676237106\n",
      "Epoch 27896/30000 Training Loss: 0.04831073805689812\n",
      "Epoch 27897/30000 Training Loss: 0.04083424061536789\n",
      "Epoch 27898/30000 Training Loss: 0.036864519119262695\n",
      "Epoch 27899/30000 Training Loss: 0.03405062109231949\n",
      "Epoch 27900/30000 Training Loss: 0.0445910282433033\n",
      "Epoch 27900/30000 Validation Loss: 0.04015270248055458\n",
      "Epoch 27901/30000 Training Loss: 0.0397142618894577\n",
      "Epoch 27902/30000 Training Loss: 0.047372132539749146\n",
      "Epoch 27903/30000 Training Loss: 0.04822053387761116\n",
      "Epoch 27904/30000 Training Loss: 0.04494485259056091\n",
      "Epoch 27905/30000 Training Loss: 0.033479176461696625\n",
      "Epoch 27906/30000 Training Loss: 0.04532948136329651\n",
      "Epoch 27907/30000 Training Loss: 0.02813754417002201\n",
      "Epoch 27908/30000 Training Loss: 0.04644958674907684\n",
      "Epoch 27909/30000 Training Loss: 0.030085407197475433\n",
      "Epoch 27910/30000 Training Loss: 0.043983690440654755\n",
      "Epoch 27911/30000 Training Loss: 0.037070829421281815\n",
      "Epoch 27912/30000 Training Loss: 0.04940631985664368\n",
      "Epoch 27913/30000 Training Loss: 0.05444525182247162\n",
      "Epoch 27914/30000 Training Loss: 0.043676234781742096\n",
      "Epoch 27915/30000 Training Loss: 0.04509887471795082\n",
      "Epoch 27916/30000 Training Loss: 0.05171191692352295\n",
      "Epoch 27917/30000 Training Loss: 0.051075540482997894\n",
      "Epoch 27918/30000 Training Loss: 0.03215949982404709\n",
      "Epoch 27919/30000 Training Loss: 0.062101610004901886\n",
      "Epoch 27920/30000 Training Loss: 0.046017102897167206\n",
      "Epoch 27921/30000 Training Loss: 0.042988263070583344\n",
      "Epoch 27922/30000 Training Loss: 0.037895090878009796\n",
      "Epoch 27923/30000 Training Loss: 0.04291234910488129\n",
      "Epoch 27924/30000 Training Loss: 0.03284374251961708\n",
      "Epoch 27925/30000 Training Loss: 0.04396330565214157\n",
      "Epoch 27926/30000 Training Loss: 0.035425618290901184\n",
      "Epoch 27927/30000 Training Loss: 0.05801916494965553\n",
      "Epoch 27928/30000 Training Loss: 0.03560132905840874\n",
      "Epoch 27929/30000 Training Loss: 0.033898305147886276\n",
      "Epoch 27930/30000 Training Loss: 0.03226529061794281\n",
      "Epoch 27931/30000 Training Loss: 0.04621173441410065\n",
      "Epoch 27932/30000 Training Loss: 0.04025539383292198\n",
      "Epoch 27933/30000 Training Loss: 0.052526626735925674\n",
      "Epoch 27934/30000 Training Loss: 0.035285137593746185\n",
      "Epoch 27935/30000 Training Loss: 0.03958041965961456\n",
      "Epoch 27936/30000 Training Loss: 0.0383177250623703\n",
      "Epoch 27937/30000 Training Loss: 0.03296886384487152\n",
      "Epoch 27938/30000 Training Loss: 0.0528295561671257\n",
      "Epoch 27939/30000 Training Loss: 0.037411630153656006\n",
      "Epoch 27940/30000 Training Loss: 0.05420425534248352\n",
      "Epoch 27941/30000 Training Loss: 0.03774946182966232\n",
      "Epoch 27942/30000 Training Loss: 0.05156808719038963\n",
      "Epoch 27943/30000 Training Loss: 0.042598240077495575\n",
      "Epoch 27944/30000 Training Loss: 0.028124380856752396\n",
      "Epoch 27945/30000 Training Loss: 0.050253212451934814\n",
      "Epoch 27946/30000 Training Loss: 0.036724627017974854\n",
      "Epoch 27947/30000 Training Loss: 0.03591509908437729\n",
      "Epoch 27948/30000 Training Loss: 0.045332904905080795\n",
      "Epoch 27949/30000 Training Loss: 0.04793573543429375\n",
      "Epoch 27950/30000 Training Loss: 0.0437532402575016\n",
      "Epoch 27951/30000 Training Loss: 0.03164789825677872\n",
      "Epoch 27952/30000 Training Loss: 0.050850145518779755\n",
      "Epoch 27953/30000 Training Loss: 0.029980476945638657\n",
      "Epoch 27954/30000 Training Loss: 0.03731479123234749\n",
      "Epoch 27955/30000 Training Loss: 0.03265279158949852\n",
      "Epoch 27956/30000 Training Loss: 0.04642939940094948\n",
      "Epoch 27957/30000 Training Loss: 0.05332928150892258\n",
      "Epoch 27958/30000 Training Loss: 0.04004235193133354\n",
      "Epoch 27959/30000 Training Loss: 0.043054647743701935\n",
      "Epoch 27960/30000 Training Loss: 0.0415642149746418\n",
      "Epoch 27961/30000 Training Loss: 0.03620247542858124\n",
      "Epoch 27962/30000 Training Loss: 0.05897824093699455\n",
      "Epoch 27963/30000 Training Loss: 0.04304065555334091\n",
      "Epoch 27964/30000 Training Loss: 0.030892033129930496\n",
      "Epoch 27965/30000 Training Loss: 0.04099586606025696\n",
      "Epoch 27966/30000 Training Loss: 0.032924432307481766\n",
      "Epoch 27967/30000 Training Loss: 0.0383402481675148\n",
      "Epoch 27968/30000 Training Loss: 0.044676706194877625\n",
      "Epoch 27969/30000 Training Loss: 0.05319290608167648\n",
      "Epoch 27970/30000 Training Loss: 0.04627738893032074\n",
      "Epoch 27971/30000 Training Loss: 0.03866159915924072\n",
      "Epoch 27972/30000 Training Loss: 0.05019901320338249\n",
      "Epoch 27973/30000 Training Loss: 0.04005632549524307\n",
      "Epoch 27974/30000 Training Loss: 0.053489938378334045\n",
      "Epoch 27975/30000 Training Loss: 0.037204060703516006\n",
      "Epoch 27976/30000 Training Loss: 0.035634227097034454\n",
      "Epoch 27977/30000 Training Loss: 0.05079421028494835\n",
      "Epoch 27978/30000 Training Loss: 0.04090431332588196\n",
      "Epoch 27979/30000 Training Loss: 0.03735022619366646\n",
      "Epoch 27980/30000 Training Loss: 0.038059379905462265\n",
      "Epoch 27981/30000 Training Loss: 0.04392442852258682\n",
      "Epoch 27982/30000 Training Loss: 0.04768580198287964\n",
      "Epoch 27983/30000 Training Loss: 0.038701437413692474\n",
      "Epoch 27984/30000 Training Loss: 0.03224469721317291\n",
      "Epoch 27985/30000 Training Loss: 0.044112831354141235\n",
      "Epoch 27986/30000 Training Loss: 0.03524370864033699\n",
      "Epoch 27987/30000 Training Loss: 0.03649444878101349\n",
      "Epoch 27988/30000 Training Loss: 0.042590875178575516\n",
      "Epoch 27989/30000 Training Loss: 0.04139459505677223\n",
      "Epoch 27990/30000 Training Loss: 0.04246443137526512\n",
      "Epoch 27991/30000 Training Loss: 0.03179064393043518\n",
      "Epoch 27992/30000 Training Loss: 0.03589951992034912\n",
      "Epoch 27993/30000 Training Loss: 0.04267574101686478\n",
      "Epoch 27994/30000 Training Loss: 0.05628835782408714\n",
      "Epoch 27995/30000 Training Loss: 0.04875259846448898\n",
      "Epoch 27996/30000 Training Loss: 0.04088747501373291\n",
      "Epoch 27997/30000 Training Loss: 0.044298913329839706\n",
      "Epoch 27998/30000 Training Loss: 0.04923858121037483\n",
      "Epoch 27999/30000 Training Loss: 0.0356135293841362\n",
      "Epoch 28000/30000 Training Loss: 0.039620522409677505\n",
      "Epoch 28000/30000 Validation Loss: 0.038390398025512695\n",
      "Epoch 28001/30000 Training Loss: 0.039307851344347\n",
      "Epoch 28002/30000 Training Loss: 0.03553266078233719\n",
      "Epoch 28003/30000 Training Loss: 0.04879964143037796\n",
      "Epoch 28004/30000 Training Loss: 0.04839411377906799\n",
      "Epoch 28005/30000 Training Loss: 0.043337129056453705\n",
      "Epoch 28006/30000 Training Loss: 0.03916967660188675\n",
      "Epoch 28007/30000 Training Loss: 0.03983274847269058\n",
      "Epoch 28008/30000 Training Loss: 0.05517613887786865\n",
      "Epoch 28009/30000 Training Loss: 0.052408263087272644\n",
      "Epoch 28010/30000 Training Loss: 0.03312075510621071\n",
      "Epoch 28011/30000 Training Loss: 0.042374387383461\n",
      "Epoch 28012/30000 Training Loss: 0.05133358761668205\n",
      "Epoch 28013/30000 Training Loss: 0.04144194722175598\n",
      "Epoch 28014/30000 Training Loss: 0.04624713212251663\n",
      "Epoch 28015/30000 Training Loss: 0.039142824709415436\n",
      "Epoch 28016/30000 Training Loss: 0.03294789791107178\n",
      "Epoch 28017/30000 Training Loss: 0.05029524117708206\n",
      "Epoch 28018/30000 Training Loss: 0.038918688893318176\n",
      "Epoch 28019/30000 Training Loss: 0.03895950689911842\n",
      "Epoch 28020/30000 Training Loss: 0.05603180453181267\n",
      "Epoch 28021/30000 Training Loss: 0.04306178167462349\n",
      "Epoch 28022/30000 Training Loss: 0.04141435772180557\n",
      "Epoch 28023/30000 Training Loss: 0.038284339010715485\n",
      "Epoch 28024/30000 Training Loss: 0.04553838074207306\n",
      "Epoch 28025/30000 Training Loss: 0.0381467379629612\n",
      "Epoch 28026/30000 Training Loss: 0.03669234365224838\n",
      "Epoch 28027/30000 Training Loss: 0.038405753672122955\n",
      "Epoch 28028/30000 Training Loss: 0.04690926522016525\n",
      "Epoch 28029/30000 Training Loss: 0.0395098552107811\n",
      "Epoch 28030/30000 Training Loss: 0.03646990656852722\n",
      "Epoch 28031/30000 Training Loss: 0.03816179186105728\n",
      "Epoch 28032/30000 Training Loss: 0.03521950542926788\n",
      "Epoch 28033/30000 Training Loss: 0.04024697467684746\n",
      "Epoch 28034/30000 Training Loss: 0.03618178889155388\n",
      "Epoch 28035/30000 Training Loss: 0.03739378601312637\n",
      "Epoch 28036/30000 Training Loss: 0.04611959308385849\n",
      "Epoch 28037/30000 Training Loss: 0.04543387144804001\n",
      "Epoch 28038/30000 Training Loss: 0.03998933732509613\n",
      "Epoch 28039/30000 Training Loss: 0.03859209269285202\n",
      "Epoch 28040/30000 Training Loss: 0.06094607338309288\n",
      "Epoch 28041/30000 Training Loss: 0.03516756743192673\n",
      "Epoch 28042/30000 Training Loss: 0.03650526702404022\n",
      "Epoch 28043/30000 Training Loss: 0.03420618548989296\n",
      "Epoch 28044/30000 Training Loss: 0.03544517233967781\n",
      "Epoch 28045/30000 Training Loss: 0.046214811503887177\n",
      "Epoch 28046/30000 Training Loss: 0.06207657605409622\n",
      "Epoch 28047/30000 Training Loss: 0.040928103029727936\n",
      "Epoch 28048/30000 Training Loss: 0.04031096398830414\n",
      "Epoch 28049/30000 Training Loss: 0.04473184794187546\n",
      "Epoch 28050/30000 Training Loss: 0.036154042929410934\n",
      "Epoch 28051/30000 Training Loss: 0.04001150280237198\n",
      "Epoch 28052/30000 Training Loss: 0.04370474815368652\n",
      "Epoch 28053/30000 Training Loss: 0.04948963597416878\n",
      "Epoch 28054/30000 Training Loss: 0.034834589809179306\n",
      "Epoch 28055/30000 Training Loss: 0.04440495744347572\n",
      "Epoch 28056/30000 Training Loss: 0.04821106046438217\n",
      "Epoch 28057/30000 Training Loss: 0.05491992086172104\n",
      "Epoch 28058/30000 Training Loss: 0.043109990656375885\n",
      "Epoch 28059/30000 Training Loss: 0.05540146678686142\n",
      "Epoch 28060/30000 Training Loss: 0.04475066810846329\n",
      "Epoch 28061/30000 Training Loss: 0.03458729386329651\n",
      "Epoch 28062/30000 Training Loss: 0.04005124419927597\n",
      "Epoch 28063/30000 Training Loss: 0.04654182121157646\n",
      "Epoch 28064/30000 Training Loss: 0.04275381937623024\n",
      "Epoch 28065/30000 Training Loss: 0.03597305715084076\n",
      "Epoch 28066/30000 Training Loss: 0.040105998516082764\n",
      "Epoch 28067/30000 Training Loss: 0.045091331005096436\n",
      "Epoch 28068/30000 Training Loss: 0.03189198672771454\n",
      "Epoch 28069/30000 Training Loss: 0.03184253349900246\n",
      "Epoch 28070/30000 Training Loss: 0.04451611265540123\n",
      "Epoch 28071/30000 Training Loss: 0.035068489611148834\n",
      "Epoch 28072/30000 Training Loss: 0.03203948214650154\n",
      "Epoch 28073/30000 Training Loss: 0.04158895090222359\n",
      "Epoch 28074/30000 Training Loss: 0.03949446603655815\n",
      "Epoch 28075/30000 Training Loss: 0.042407646775245667\n",
      "Epoch 28076/30000 Training Loss: 0.04247799515724182\n",
      "Epoch 28077/30000 Training Loss: 0.047278955578804016\n",
      "Epoch 28078/30000 Training Loss: 0.03684936836361885\n",
      "Epoch 28079/30000 Training Loss: 0.04699915647506714\n",
      "Epoch 28080/30000 Training Loss: 0.03905303031206131\n",
      "Epoch 28081/30000 Training Loss: 0.041639894247055054\n",
      "Epoch 28082/30000 Training Loss: 0.03668757900595665\n",
      "Epoch 28083/30000 Training Loss: 0.047273747622966766\n",
      "Epoch 28084/30000 Training Loss: 0.046469077467918396\n",
      "Epoch 28085/30000 Training Loss: 0.04004116356372833\n",
      "Epoch 28086/30000 Training Loss: 0.04855063557624817\n",
      "Epoch 28087/30000 Training Loss: 0.04679984599351883\n",
      "Epoch 28088/30000 Training Loss: 0.03688295930624008\n",
      "Epoch 28089/30000 Training Loss: 0.040952250361442566\n",
      "Epoch 28090/30000 Training Loss: 0.04025406390428543\n",
      "Epoch 28091/30000 Training Loss: 0.04363119602203369\n",
      "Epoch 28092/30000 Training Loss: 0.058040983974933624\n",
      "Epoch 28093/30000 Training Loss: 0.044458091259002686\n",
      "Epoch 28094/30000 Training Loss: 0.05010977387428284\n",
      "Epoch 28095/30000 Training Loss: 0.04544349014759064\n",
      "Epoch 28096/30000 Training Loss: 0.05347547307610512\n",
      "Epoch 28097/30000 Training Loss: 0.0372384749352932\n",
      "Epoch 28098/30000 Training Loss: 0.051438212394714355\n",
      "Epoch 28099/30000 Training Loss: 0.058562204241752625\n",
      "Epoch 28100/30000 Training Loss: 0.04294569417834282\n",
      "Epoch 28100/30000 Validation Loss: 0.05106277018785477\n",
      "Epoch 28101/30000 Training Loss: 0.05072763189673424\n",
      "Epoch 28102/30000 Training Loss: 0.04836878925561905\n",
      "Epoch 28103/30000 Training Loss: 0.0439562126994133\n",
      "Epoch 28104/30000 Training Loss: 0.05461079627275467\n",
      "Epoch 28105/30000 Training Loss: 0.03895542398095131\n",
      "Epoch 28106/30000 Training Loss: 0.03456493094563484\n",
      "Epoch 28107/30000 Training Loss: 0.04754582419991493\n",
      "Epoch 28108/30000 Training Loss: 0.03650056570768356\n",
      "Epoch 28109/30000 Training Loss: 0.04198914021253586\n",
      "Epoch 28110/30000 Training Loss: 0.04894880950450897\n",
      "Epoch 28111/30000 Training Loss: 0.04527755454182625\n",
      "Epoch 28112/30000 Training Loss: 0.049153197556734085\n",
      "Epoch 28113/30000 Training Loss: 0.0398472398519516\n",
      "Epoch 28114/30000 Training Loss: 0.04237837716937065\n",
      "Epoch 28115/30000 Training Loss: 0.04919278249144554\n",
      "Epoch 28116/30000 Training Loss: 0.04057374224066734\n",
      "Epoch 28117/30000 Training Loss: 0.045499060302972794\n",
      "Epoch 28118/30000 Training Loss: 0.05057737976312637\n",
      "Epoch 28119/30000 Training Loss: 0.043190848082304\n",
      "Epoch 28120/30000 Training Loss: 0.05538288503885269\n",
      "Epoch 28121/30000 Training Loss: 0.04874126613140106\n",
      "Epoch 28122/30000 Training Loss: 0.0476590096950531\n",
      "Epoch 28123/30000 Training Loss: 0.041154645383358\n",
      "Epoch 28124/30000 Training Loss: 0.045716412365436554\n",
      "Epoch 28125/30000 Training Loss: 0.03599625825881958\n",
      "Epoch 28126/30000 Training Loss: 0.03227543085813522\n",
      "Epoch 28127/30000 Training Loss: 0.032890208065509796\n",
      "Epoch 28128/30000 Training Loss: 0.049283403903245926\n",
      "Epoch 28129/30000 Training Loss: 0.03455550968647003\n",
      "Epoch 28130/30000 Training Loss: 0.04211560636758804\n",
      "Epoch 28131/30000 Training Loss: 0.04133027046918869\n",
      "Epoch 28132/30000 Training Loss: 0.047845397144556046\n",
      "Epoch 28133/30000 Training Loss: 0.0487259179353714\n",
      "Epoch 28134/30000 Training Loss: 0.035308681428432465\n",
      "Epoch 28135/30000 Training Loss: 0.03167656809091568\n",
      "Epoch 28136/30000 Training Loss: 0.054317936301231384\n",
      "Epoch 28137/30000 Training Loss: 0.04909614473581314\n",
      "Epoch 28138/30000 Training Loss: 0.05145936459302902\n",
      "Epoch 28139/30000 Training Loss: 0.04308266565203667\n",
      "Epoch 28140/30000 Training Loss: 0.044088371098041534\n",
      "Epoch 28141/30000 Training Loss: 0.03609141334891319\n",
      "Epoch 28142/30000 Training Loss: 0.037507154047489166\n",
      "Epoch 28143/30000 Training Loss: 0.05603222921490669\n",
      "Epoch 28144/30000 Training Loss: 0.06148739159107208\n",
      "Epoch 28145/30000 Training Loss: 0.03996533900499344\n",
      "Epoch 28146/30000 Training Loss: 0.02772722765803337\n",
      "Epoch 28147/30000 Training Loss: 0.03956151008605957\n",
      "Epoch 28148/30000 Training Loss: 0.031722716987133026\n",
      "Epoch 28149/30000 Training Loss: 0.05499119311571121\n",
      "Epoch 28150/30000 Training Loss: 0.04518340900540352\n",
      "Epoch 28151/30000 Training Loss: 0.02916749380528927\n",
      "Epoch 28152/30000 Training Loss: 0.04976214841008186\n",
      "Epoch 28153/30000 Training Loss: 0.04635953903198242\n",
      "Epoch 28154/30000 Training Loss: 0.03506030887365341\n",
      "Epoch 28155/30000 Training Loss: 0.04263019561767578\n",
      "Epoch 28156/30000 Training Loss: 0.04038954898715019\n",
      "Epoch 28157/30000 Training Loss: 0.04877752065658569\n",
      "Epoch 28158/30000 Training Loss: 0.04267599433660507\n",
      "Epoch 28159/30000 Training Loss: 0.026845727115869522\n",
      "Epoch 28160/30000 Training Loss: 0.03482562676072121\n",
      "Epoch 28161/30000 Training Loss: 0.05138491466641426\n",
      "Epoch 28162/30000 Training Loss: 0.0473518967628479\n",
      "Epoch 28163/30000 Training Loss: 0.03541671857237816\n",
      "Epoch 28164/30000 Training Loss: 0.061868876218795776\n",
      "Epoch 28165/30000 Training Loss: 0.04048639535903931\n",
      "Epoch 28166/30000 Training Loss: 0.04909813404083252\n",
      "Epoch 28167/30000 Training Loss: 0.03151886165142059\n",
      "Epoch 28168/30000 Training Loss: 0.04257854074239731\n",
      "Epoch 28169/30000 Training Loss: 0.033822789788246155\n",
      "Epoch 28170/30000 Training Loss: 0.03674238920211792\n",
      "Epoch 28171/30000 Training Loss: 0.04822351783514023\n",
      "Epoch 28172/30000 Training Loss: 0.04500001668930054\n",
      "Epoch 28173/30000 Training Loss: 0.04539230465888977\n",
      "Epoch 28174/30000 Training Loss: 0.049606673419475555\n",
      "Epoch 28175/30000 Training Loss: 0.04559259116649628\n",
      "Epoch 28176/30000 Training Loss: 0.03090783953666687\n",
      "Epoch 28177/30000 Training Loss: 0.037881191819906235\n",
      "Epoch 28178/30000 Training Loss: 0.03938847407698631\n",
      "Epoch 28179/30000 Training Loss: 0.04693421721458435\n",
      "Epoch 28180/30000 Training Loss: 0.0430077388882637\n",
      "Epoch 28181/30000 Training Loss: 0.04022210091352463\n",
      "Epoch 28182/30000 Training Loss: 0.03939081355929375\n",
      "Epoch 28183/30000 Training Loss: 0.03530253469944\n",
      "Epoch 28184/30000 Training Loss: 0.05010326951742172\n",
      "Epoch 28185/30000 Training Loss: 0.03619684651494026\n",
      "Epoch 28186/30000 Training Loss: 0.05211380124092102\n",
      "Epoch 28187/30000 Training Loss: 0.040019016712903976\n",
      "Epoch 28188/30000 Training Loss: 0.04345407336950302\n",
      "Epoch 28189/30000 Training Loss: 0.040466248989105225\n",
      "Epoch 28190/30000 Training Loss: 0.044996634125709534\n",
      "Epoch 28191/30000 Training Loss: 0.049647748470306396\n",
      "Epoch 28192/30000 Training Loss: 0.046675119549036026\n",
      "Epoch 28193/30000 Training Loss: 0.059824250638484955\n",
      "Epoch 28194/30000 Training Loss: 0.05340007320046425\n",
      "Epoch 28195/30000 Training Loss: 0.04093542695045471\n",
      "Epoch 28196/30000 Training Loss: 0.049110256135463715\n",
      "Epoch 28197/30000 Training Loss: 0.06853616237640381\n",
      "Epoch 28198/30000 Training Loss: 0.03851982578635216\n",
      "Epoch 28199/30000 Training Loss: 0.04010361060500145\n",
      "Epoch 28200/30000 Training Loss: 0.04202917963266373\n",
      "Epoch 28200/30000 Validation Loss: 0.045494548976421356\n",
      "Epoch 28201/30000 Training Loss: 0.058104008436203\n",
      "Epoch 28202/30000 Training Loss: 0.048785500228405\n",
      "Epoch 28203/30000 Training Loss: 0.054519496858119965\n",
      "Epoch 28204/30000 Training Loss: 0.02936752326786518\n",
      "Epoch 28205/30000 Training Loss: 0.046084776520729065\n",
      "Epoch 28206/30000 Training Loss: 0.04919904097914696\n",
      "Epoch 28207/30000 Training Loss: 0.04369500279426575\n",
      "Epoch 28208/30000 Training Loss: 0.052824169397354126\n",
      "Epoch 28209/30000 Training Loss: 0.04897085577249527\n",
      "Epoch 28210/30000 Training Loss: 0.0644497275352478\n",
      "Epoch 28211/30000 Training Loss: 0.04085443168878555\n",
      "Epoch 28212/30000 Training Loss: 0.036539047956466675\n",
      "Epoch 28213/30000 Training Loss: 0.05353892594575882\n",
      "Epoch 28214/30000 Training Loss: 0.04255328327417374\n",
      "Epoch 28215/30000 Training Loss: 0.04569561406970024\n",
      "Epoch 28216/30000 Training Loss: 0.04501127451658249\n",
      "Epoch 28217/30000 Training Loss: 0.04524436220526695\n",
      "Epoch 28218/30000 Training Loss: 0.026815958321094513\n",
      "Epoch 28219/30000 Training Loss: 0.04776191711425781\n",
      "Epoch 28220/30000 Training Loss: 0.0428551584482193\n",
      "Epoch 28221/30000 Training Loss: 0.036685895174741745\n",
      "Epoch 28222/30000 Training Loss: 0.059749968349933624\n",
      "Epoch 28223/30000 Training Loss: 0.046569518744945526\n",
      "Epoch 28224/30000 Training Loss: 0.034418895840644836\n",
      "Epoch 28225/30000 Training Loss: 0.040218841284513474\n",
      "Epoch 28226/30000 Training Loss: 0.04616851359605789\n",
      "Epoch 28227/30000 Training Loss: 0.042984530329704285\n",
      "Epoch 28228/30000 Training Loss: 0.04098866879940033\n",
      "Epoch 28229/30000 Training Loss: 0.04740167409181595\n",
      "Epoch 28230/30000 Training Loss: 0.03670578449964523\n",
      "Epoch 28231/30000 Training Loss: 0.05449380725622177\n",
      "Epoch 28232/30000 Training Loss: 0.03333227336406708\n",
      "Epoch 28233/30000 Training Loss: 0.04864204674959183\n",
      "Epoch 28234/30000 Training Loss: 0.041054803878068924\n",
      "Epoch 28235/30000 Training Loss: 0.05316384881734848\n",
      "Epoch 28236/30000 Training Loss: 0.042999401688575745\n",
      "Epoch 28237/30000 Training Loss: 0.0431092269718647\n",
      "Epoch 28238/30000 Training Loss: 0.03323735296726227\n",
      "Epoch 28239/30000 Training Loss: 0.046951208263635635\n",
      "Epoch 28240/30000 Training Loss: 0.035951707512140274\n",
      "Epoch 28241/30000 Training Loss: 0.04755926504731178\n",
      "Epoch 28242/30000 Training Loss: 0.04604114592075348\n",
      "Epoch 28243/30000 Training Loss: 0.03196747228503227\n",
      "Epoch 28244/30000 Training Loss: 0.031353920698165894\n",
      "Epoch 28245/30000 Training Loss: 0.046133704483509064\n",
      "Epoch 28246/30000 Training Loss: 0.03671398013830185\n",
      "Epoch 28247/30000 Training Loss: 0.04539810121059418\n",
      "Epoch 28248/30000 Training Loss: 0.044023700058460236\n",
      "Epoch 28249/30000 Training Loss: 0.046901874244213104\n",
      "Epoch 28250/30000 Training Loss: 0.04109983518719673\n",
      "Epoch 28251/30000 Training Loss: 0.035509757697582245\n",
      "Epoch 28252/30000 Training Loss: 0.03652782738208771\n",
      "Epoch 28253/30000 Training Loss: 0.047401994466781616\n",
      "Epoch 28254/30000 Training Loss: 0.036883145570755005\n",
      "Epoch 28255/30000 Training Loss: 0.04131677374243736\n",
      "Epoch 28256/30000 Training Loss: 0.040550172328948975\n",
      "Epoch 28257/30000 Training Loss: 0.045648038387298584\n",
      "Epoch 28258/30000 Training Loss: 0.052239470183849335\n",
      "Epoch 28259/30000 Training Loss: 0.04581065475940704\n",
      "Epoch 28260/30000 Training Loss: 0.03730820119380951\n",
      "Epoch 28261/30000 Training Loss: 0.048105426132678986\n",
      "Epoch 28262/30000 Training Loss: 0.046513430774211884\n",
      "Epoch 28263/30000 Training Loss: 0.03669996187090874\n",
      "Epoch 28264/30000 Training Loss: 0.04079188406467438\n",
      "Epoch 28265/30000 Training Loss: 0.04232916980981827\n",
      "Epoch 28266/30000 Training Loss: 0.03449420630931854\n",
      "Epoch 28267/30000 Training Loss: 0.04131878912448883\n",
      "Epoch 28268/30000 Training Loss: 0.04489820450544357\n",
      "Epoch 28269/30000 Training Loss: 0.042416855692863464\n",
      "Epoch 28270/30000 Training Loss: 0.04158193618059158\n",
      "Epoch 28271/30000 Training Loss: 0.03034522384405136\n",
      "Epoch 28272/30000 Training Loss: 0.035354651510715485\n",
      "Epoch 28273/30000 Training Loss: 0.03883343189954758\n",
      "Epoch 28274/30000 Training Loss: 0.03396906703710556\n",
      "Epoch 28275/30000 Training Loss: 0.040571972727775574\n",
      "Epoch 28276/30000 Training Loss: 0.04657638818025589\n",
      "Epoch 28277/30000 Training Loss: 0.0484696626663208\n",
      "Epoch 28278/30000 Training Loss: 0.03538114205002785\n",
      "Epoch 28279/30000 Training Loss: 0.03628953546285629\n",
      "Epoch 28280/30000 Training Loss: 0.046578869223594666\n",
      "Epoch 28281/30000 Training Loss: 0.03864842653274536\n",
      "Epoch 28282/30000 Training Loss: 0.05015108361840248\n",
      "Epoch 28283/30000 Training Loss: 0.0496668815612793\n",
      "Epoch 28284/30000 Training Loss: 0.04382045567035675\n",
      "Epoch 28285/30000 Training Loss: 0.04715369641780853\n",
      "Epoch 28286/30000 Training Loss: 0.047556839883327484\n",
      "Epoch 28287/30000 Training Loss: 0.038770370185375214\n",
      "Epoch 28288/30000 Training Loss: 0.04425487667322159\n",
      "Epoch 28289/30000 Training Loss: 0.04325956478714943\n",
      "Epoch 28290/30000 Training Loss: 0.042239315807819366\n",
      "Epoch 28291/30000 Training Loss: 0.037298381328582764\n",
      "Epoch 28292/30000 Training Loss: 0.04958794265985489\n",
      "Epoch 28293/30000 Training Loss: 0.038104914128780365\n",
      "Epoch 28294/30000 Training Loss: 0.048700474202632904\n",
      "Epoch 28295/30000 Training Loss: 0.033995404839515686\n",
      "Epoch 28296/30000 Training Loss: 0.04345053806900978\n",
      "Epoch 28297/30000 Training Loss: 0.03893870860338211\n",
      "Epoch 28298/30000 Training Loss: 0.0492701381444931\n",
      "Epoch 28299/30000 Training Loss: 0.05120067298412323\n",
      "Epoch 28300/30000 Training Loss: 0.0589279904961586\n",
      "Epoch 28300/30000 Validation Loss: 0.04507017880678177\n",
      "Epoch 28301/30000 Training Loss: 0.027255505323410034\n",
      "Epoch 28302/30000 Training Loss: 0.04104325920343399\n",
      "Epoch 28303/30000 Training Loss: 0.04806920140981674\n",
      "Epoch 28304/30000 Training Loss: 0.04057122766971588\n",
      "Epoch 28305/30000 Training Loss: 0.039508212357759476\n",
      "Epoch 28306/30000 Training Loss: 0.04834667220711708\n",
      "Epoch 28307/30000 Training Loss: 0.049013517796993256\n",
      "Epoch 28308/30000 Training Loss: 0.041680023074150085\n",
      "Epoch 28309/30000 Training Loss: 0.05832621455192566\n",
      "Epoch 28310/30000 Training Loss: 0.04005024582147598\n",
      "Epoch 28311/30000 Training Loss: 0.0361657440662384\n",
      "Epoch 28312/30000 Training Loss: 0.03805501386523247\n",
      "Epoch 28313/30000 Training Loss: 0.03779361769556999\n",
      "Epoch 28314/30000 Training Loss: 0.039805326610803604\n",
      "Epoch 28315/30000 Training Loss: 0.04389296472072601\n",
      "Epoch 28316/30000 Training Loss: 0.030231032520532608\n",
      "Epoch 28317/30000 Training Loss: 0.03788944333791733\n",
      "Epoch 28318/30000 Training Loss: 0.044581204652786255\n",
      "Epoch 28319/30000 Training Loss: 0.03254587948322296\n",
      "Epoch 28320/30000 Training Loss: 0.0555625781416893\n",
      "Epoch 28321/30000 Training Loss: 0.04101380705833435\n",
      "Epoch 28322/30000 Training Loss: 0.041668158024549484\n",
      "Epoch 28323/30000 Training Loss: 0.04628922790288925\n",
      "Epoch 28324/30000 Training Loss: 0.03698046877980232\n",
      "Epoch 28325/30000 Training Loss: 0.03389722853899002\n",
      "Epoch 28326/30000 Training Loss: 0.0394623801112175\n",
      "Epoch 28327/30000 Training Loss: 0.043606650084257126\n",
      "Epoch 28328/30000 Training Loss: 0.049982406198978424\n",
      "Epoch 28329/30000 Training Loss: 0.04619923233985901\n",
      "Epoch 28330/30000 Training Loss: 0.060794755816459656\n",
      "Epoch 28331/30000 Training Loss: 0.04866740107536316\n",
      "Epoch 28332/30000 Training Loss: 0.051037393510341644\n",
      "Epoch 28333/30000 Training Loss: 0.030463211238384247\n",
      "Epoch 28334/30000 Training Loss: 0.031800005584955215\n",
      "Epoch 28335/30000 Training Loss: 0.05692727863788605\n",
      "Epoch 28336/30000 Training Loss: 0.04862202703952789\n",
      "Epoch 28337/30000 Training Loss: 0.05341742932796478\n",
      "Epoch 28338/30000 Training Loss: 0.03730536997318268\n",
      "Epoch 28339/30000 Training Loss: 0.03723592311143875\n",
      "Epoch 28340/30000 Training Loss: 0.05966569483280182\n",
      "Epoch 28341/30000 Training Loss: 0.047745026648044586\n",
      "Epoch 28342/30000 Training Loss: 0.03933381661772728\n",
      "Epoch 28343/30000 Training Loss: 0.031039483845233917\n",
      "Epoch 28344/30000 Training Loss: 0.041893307119607925\n",
      "Epoch 28345/30000 Training Loss: 0.050632886588573456\n",
      "Epoch 28346/30000 Training Loss: 0.04615502059459686\n",
      "Epoch 28347/30000 Training Loss: 0.038492605090141296\n",
      "Epoch 28348/30000 Training Loss: 0.0456090047955513\n",
      "Epoch 28349/30000 Training Loss: 0.03252298757433891\n",
      "Epoch 28350/30000 Training Loss: 0.04684711620211601\n",
      "Epoch 28351/30000 Training Loss: 0.061687082052230835\n",
      "Epoch 28352/30000 Training Loss: 0.03961914777755737\n",
      "Epoch 28353/30000 Training Loss: 0.054886989295482635\n",
      "Epoch 28354/30000 Training Loss: 0.03973948583006859\n",
      "Epoch 28355/30000 Training Loss: 0.05154208838939667\n",
      "Epoch 28356/30000 Training Loss: 0.0475274994969368\n",
      "Epoch 28357/30000 Training Loss: 0.04630925506353378\n",
      "Epoch 28358/30000 Training Loss: 0.042267341166734695\n",
      "Epoch 28359/30000 Training Loss: 0.04625626280903816\n",
      "Epoch 28360/30000 Training Loss: 0.040657103061676025\n",
      "Epoch 28361/30000 Training Loss: 0.04599720612168312\n",
      "Epoch 28362/30000 Training Loss: 0.0453999862074852\n",
      "Epoch 28363/30000 Training Loss: 0.04292387142777443\n",
      "Epoch 28364/30000 Training Loss: 0.034992970526218414\n",
      "Epoch 28365/30000 Training Loss: 0.034635528922080994\n",
      "Epoch 28366/30000 Training Loss: 0.040015727281570435\n",
      "Epoch 28367/30000 Training Loss: 0.04356861114501953\n",
      "Epoch 28368/30000 Training Loss: 0.045192211866378784\n",
      "Epoch 28369/30000 Training Loss: 0.044700272381305695\n",
      "Epoch 28370/30000 Training Loss: 0.05028636008501053\n",
      "Epoch 28371/30000 Training Loss: 0.03097587078809738\n",
      "Epoch 28372/30000 Training Loss: 0.03728719800710678\n",
      "Epoch 28373/30000 Training Loss: 0.030428791418671608\n",
      "Epoch 28374/30000 Training Loss: 0.0482998788356781\n",
      "Epoch 28375/30000 Training Loss: 0.04303441941738129\n",
      "Epoch 28376/30000 Training Loss: 0.04527746140956879\n",
      "Epoch 28377/30000 Training Loss: 0.04659389704465866\n",
      "Epoch 28378/30000 Training Loss: 0.038864873349666595\n",
      "Epoch 28379/30000 Training Loss: 0.03198419138789177\n",
      "Epoch 28380/30000 Training Loss: 0.041791513562202454\n",
      "Epoch 28381/30000 Training Loss: 0.03130906820297241\n",
      "Epoch 28382/30000 Training Loss: 0.05014493316411972\n",
      "Epoch 28383/30000 Training Loss: 0.03513356298208237\n",
      "Epoch 28384/30000 Training Loss: 0.04560170695185661\n",
      "Epoch 28385/30000 Training Loss: 0.03918719291687012\n",
      "Epoch 28386/30000 Training Loss: 0.0367426760494709\n",
      "Epoch 28387/30000 Training Loss: 0.039734140038490295\n",
      "Epoch 28388/30000 Training Loss: 0.04051171988248825\n",
      "Epoch 28389/30000 Training Loss: 0.04107603058218956\n",
      "Epoch 28390/30000 Training Loss: 0.052150048315525055\n",
      "Epoch 28391/30000 Training Loss: 0.04301859810948372\n",
      "Epoch 28392/30000 Training Loss: 0.03258783370256424\n",
      "Epoch 28393/30000 Training Loss: 0.045663733035326004\n",
      "Epoch 28394/30000 Training Loss: 0.040494367480278015\n",
      "Epoch 28395/30000 Training Loss: 0.04144391417503357\n",
      "Epoch 28396/30000 Training Loss: 0.039295636117458344\n",
      "Epoch 28397/30000 Training Loss: 0.04860258102416992\n",
      "Epoch 28398/30000 Training Loss: 0.04678620770573616\n",
      "Epoch 28399/30000 Training Loss: 0.052848562598228455\n",
      "Epoch 28400/30000 Training Loss: 0.044956788420677185\n",
      "Epoch 28400/30000 Validation Loss: 0.0550202876329422\n",
      "Epoch 28401/30000 Training Loss: 0.03865533322095871\n",
      "Epoch 28402/30000 Training Loss: 0.050505153834819794\n",
      "Epoch 28403/30000 Training Loss: 0.05061369389295578\n",
      "Epoch 28404/30000 Training Loss: 0.054872266948223114\n",
      "Epoch 28405/30000 Training Loss: 0.044469933956861496\n",
      "Epoch 28406/30000 Training Loss: 0.05112848058342934\n",
      "Epoch 28407/30000 Training Loss: 0.038246750831604004\n",
      "Epoch 28408/30000 Training Loss: 0.05243675038218498\n",
      "Epoch 28409/30000 Training Loss: 0.03948717191815376\n",
      "Epoch 28410/30000 Training Loss: 0.045953039079904556\n",
      "Epoch 28411/30000 Training Loss: 0.045954737812280655\n",
      "Epoch 28412/30000 Training Loss: 0.04048922285437584\n",
      "Epoch 28413/30000 Training Loss: 0.05187270790338516\n",
      "Epoch 28414/30000 Training Loss: 0.03289680927991867\n",
      "Epoch 28415/30000 Training Loss: 0.03608355671167374\n",
      "Epoch 28416/30000 Training Loss: 0.03169964626431465\n",
      "Epoch 28417/30000 Training Loss: 0.04529434069991112\n",
      "Epoch 28418/30000 Training Loss: 0.03989372402429581\n",
      "Epoch 28419/30000 Training Loss: 0.04866543039679527\n",
      "Epoch 28420/30000 Training Loss: 0.050351258367300034\n",
      "Epoch 28421/30000 Training Loss: 0.0294748954474926\n",
      "Epoch 28422/30000 Training Loss: 0.04488160088658333\n",
      "Epoch 28423/30000 Training Loss: 0.0519564226269722\n",
      "Epoch 28424/30000 Training Loss: 0.05919851362705231\n",
      "Epoch 28425/30000 Training Loss: 0.031274568289518356\n",
      "Epoch 28426/30000 Training Loss: 0.05323491990566254\n",
      "Epoch 28427/30000 Training Loss: 0.04503322392702103\n",
      "Epoch 28428/30000 Training Loss: 0.039936795830726624\n",
      "Epoch 28429/30000 Training Loss: 0.0371580645442009\n",
      "Epoch 28430/30000 Training Loss: 0.04112475365400314\n",
      "Epoch 28431/30000 Training Loss: 0.05371880531311035\n",
      "Epoch 28432/30000 Training Loss: 0.04408964514732361\n",
      "Epoch 28433/30000 Training Loss: 0.037953153252601624\n",
      "Epoch 28434/30000 Training Loss: 0.03980845957994461\n",
      "Epoch 28435/30000 Training Loss: 0.04960726946592331\n",
      "Epoch 28436/30000 Training Loss: 0.04902216047048569\n",
      "Epoch 28437/30000 Training Loss: 0.03814075514674187\n",
      "Epoch 28438/30000 Training Loss: 0.03924722224473953\n",
      "Epoch 28439/30000 Training Loss: 0.04447188973426819\n",
      "Epoch 28440/30000 Training Loss: 0.05228745564818382\n",
      "Epoch 28441/30000 Training Loss: 0.04381837695837021\n",
      "Epoch 28442/30000 Training Loss: 0.04453326016664505\n",
      "Epoch 28443/30000 Training Loss: 0.03904210776090622\n",
      "Epoch 28444/30000 Training Loss: 0.04603751376271248\n",
      "Epoch 28445/30000 Training Loss: 0.047696832567453384\n",
      "Epoch 28446/30000 Training Loss: 0.028336770832538605\n",
      "Epoch 28447/30000 Training Loss: 0.035668738186359406\n",
      "Epoch 28448/30000 Training Loss: 0.04357967525720596\n",
      "Epoch 28449/30000 Training Loss: 0.03722009062767029\n",
      "Epoch 28450/30000 Training Loss: 0.03368101269006729\n",
      "Epoch 28451/30000 Training Loss: 0.05602426826953888\n",
      "Epoch 28452/30000 Training Loss: 0.03215848281979561\n",
      "Epoch 28453/30000 Training Loss: 0.038915857672691345\n",
      "Epoch 28454/30000 Training Loss: 0.05714814364910126\n",
      "Epoch 28455/30000 Training Loss: 0.04817807674407959\n",
      "Epoch 28456/30000 Training Loss: 0.03898083418607712\n",
      "Epoch 28457/30000 Training Loss: 0.050276242196559906\n",
      "Epoch 28458/30000 Training Loss: 0.03584851324558258\n",
      "Epoch 28459/30000 Training Loss: 0.05106417089700699\n",
      "Epoch 28460/30000 Training Loss: 0.04326452314853668\n",
      "Epoch 28461/30000 Training Loss: 0.04086017236113548\n",
      "Epoch 28462/30000 Training Loss: 0.03163224086165428\n",
      "Epoch 28463/30000 Training Loss: 0.05160287022590637\n",
      "Epoch 28464/30000 Training Loss: 0.027959100902080536\n",
      "Epoch 28465/30000 Training Loss: 0.04474632814526558\n",
      "Epoch 28466/30000 Training Loss: 0.042753666639328\n",
      "Epoch 28467/30000 Training Loss: 0.04494232311844826\n",
      "Epoch 28468/30000 Training Loss: 0.04702755808830261\n",
      "Epoch 28469/30000 Training Loss: 0.04164043813943863\n",
      "Epoch 28470/30000 Training Loss: 0.031788744032382965\n",
      "Epoch 28471/30000 Training Loss: 0.05069682374596596\n",
      "Epoch 28472/30000 Training Loss: 0.033122770488262177\n",
      "Epoch 28473/30000 Training Loss: 0.03539680689573288\n",
      "Epoch 28474/30000 Training Loss: 0.04646046459674835\n",
      "Epoch 28475/30000 Training Loss: 0.0388982929289341\n",
      "Epoch 28476/30000 Training Loss: 0.0313444621860981\n",
      "Epoch 28477/30000 Training Loss: 0.035245150327682495\n",
      "Epoch 28478/30000 Training Loss: 0.044793158769607544\n",
      "Epoch 28479/30000 Training Loss: 0.03961348906159401\n",
      "Epoch 28480/30000 Training Loss: 0.04013098031282425\n",
      "Epoch 28481/30000 Training Loss: 0.05295795947313309\n",
      "Epoch 28482/30000 Training Loss: 0.04343800991773605\n",
      "Epoch 28483/30000 Training Loss: 0.03980497270822525\n",
      "Epoch 28484/30000 Training Loss: 0.046588361263275146\n",
      "Epoch 28485/30000 Training Loss: 0.03684767335653305\n",
      "Epoch 28486/30000 Training Loss: 0.04385319724678993\n",
      "Epoch 28487/30000 Training Loss: 0.05528941750526428\n",
      "Epoch 28488/30000 Training Loss: 0.049337346106767654\n",
      "Epoch 28489/30000 Training Loss: 0.06869786232709885\n",
      "Epoch 28490/30000 Training Loss: 0.051390327513217926\n",
      "Epoch 28491/30000 Training Loss: 0.04237305000424385\n",
      "Epoch 28492/30000 Training Loss: 0.037140727043151855\n",
      "Epoch 28493/30000 Training Loss: 0.050775136798620224\n",
      "Epoch 28494/30000 Training Loss: 0.03127223998308182\n",
      "Epoch 28495/30000 Training Loss: 0.051701903343200684\n",
      "Epoch 28496/30000 Training Loss: 0.03536736220121384\n",
      "Epoch 28497/30000 Training Loss: 0.034963302314281464\n",
      "Epoch 28498/30000 Training Loss: 0.030445190146565437\n",
      "Epoch 28499/30000 Training Loss: 0.03793269023299217\n",
      "Epoch 28500/30000 Training Loss: 0.05785714089870453\n",
      "Epoch 28500/30000 Validation Loss: 0.0349547304213047\n",
      "Epoch 28501/30000 Training Loss: 0.04988604038953781\n",
      "Epoch 28502/30000 Training Loss: 0.03837531805038452\n",
      "Epoch 28503/30000 Training Loss: 0.0538695752620697\n",
      "Epoch 28504/30000 Training Loss: 0.03899863362312317\n",
      "Epoch 28505/30000 Training Loss: 0.054507602006196976\n",
      "Epoch 28506/30000 Training Loss: 0.06601832807064056\n",
      "Epoch 28507/30000 Training Loss: 0.04069866985082626\n",
      "Epoch 28508/30000 Training Loss: 0.039147891104221344\n",
      "Epoch 28509/30000 Training Loss: 0.04759571701288223\n",
      "Epoch 28510/30000 Training Loss: 0.04162926226854324\n",
      "Epoch 28511/30000 Training Loss: 0.0335010290145874\n",
      "Epoch 28512/30000 Training Loss: 0.035267964005470276\n",
      "Epoch 28513/30000 Training Loss: 0.037585921585559845\n",
      "Epoch 28514/30000 Training Loss: 0.036096617579460144\n",
      "Epoch 28515/30000 Training Loss: 0.03779025375843048\n",
      "Epoch 28516/30000 Training Loss: 0.0440228134393692\n",
      "Epoch 28517/30000 Training Loss: 0.048835285007953644\n",
      "Epoch 28518/30000 Training Loss: 0.05121201276779175\n",
      "Epoch 28519/30000 Training Loss: 0.04603169858455658\n",
      "Epoch 28520/30000 Training Loss: 0.03493847697973251\n",
      "Epoch 28521/30000 Training Loss: 0.0480392687022686\n",
      "Epoch 28522/30000 Training Loss: 0.034900255501270294\n",
      "Epoch 28523/30000 Training Loss: 0.04335333779454231\n",
      "Epoch 28524/30000 Training Loss: 0.03895242512226105\n",
      "Epoch 28525/30000 Training Loss: 0.038309693336486816\n",
      "Epoch 28526/30000 Training Loss: 0.04781392216682434\n",
      "Epoch 28527/30000 Training Loss: 0.047009341418743134\n",
      "Epoch 28528/30000 Training Loss: 0.04569916054606438\n",
      "Epoch 28529/30000 Training Loss: 0.03729239106178284\n",
      "Epoch 28530/30000 Training Loss: 0.031510502099990845\n",
      "Epoch 28531/30000 Training Loss: 0.038218315690755844\n",
      "Epoch 28532/30000 Training Loss: 0.04909857362508774\n",
      "Epoch 28533/30000 Training Loss: 0.04878658056259155\n",
      "Epoch 28534/30000 Training Loss: 0.042750366032123566\n",
      "Epoch 28535/30000 Training Loss: 0.047292813658714294\n",
      "Epoch 28536/30000 Training Loss: 0.0483778715133667\n",
      "Epoch 28537/30000 Training Loss: 0.03976605460047722\n",
      "Epoch 28538/30000 Training Loss: 0.03583378344774246\n",
      "Epoch 28539/30000 Training Loss: 0.05115293338894844\n",
      "Epoch 28540/30000 Training Loss: 0.03857888653874397\n",
      "Epoch 28541/30000 Training Loss: 0.039495013654232025\n",
      "Epoch 28542/30000 Training Loss: 0.041170522570610046\n",
      "Epoch 28543/30000 Training Loss: 0.044957611709833145\n",
      "Epoch 28544/30000 Training Loss: 0.035338349640369415\n",
      "Epoch 28545/30000 Training Loss: 0.048025451600551605\n",
      "Epoch 28546/30000 Training Loss: 0.04321705549955368\n",
      "Epoch 28547/30000 Training Loss: 0.0660824254155159\n",
      "Epoch 28548/30000 Training Loss: 0.0468294657766819\n",
      "Epoch 28549/30000 Training Loss: 0.04932582378387451\n",
      "Epoch 28550/30000 Training Loss: 0.035189755260944366\n",
      "Epoch 28551/30000 Training Loss: 0.047549769282341\n",
      "Epoch 28552/30000 Training Loss: 0.038973405957221985\n",
      "Epoch 28553/30000 Training Loss: 0.033898819237947464\n",
      "Epoch 28554/30000 Training Loss: 0.03678368777036667\n",
      "Epoch 28555/30000 Training Loss: 0.04069442301988602\n",
      "Epoch 28556/30000 Training Loss: 0.045399680733680725\n",
      "Epoch 28557/30000 Training Loss: 0.05664768069982529\n",
      "Epoch 28558/30000 Training Loss: 0.046274684369564056\n",
      "Epoch 28559/30000 Training Loss: 0.04262975603342056\n",
      "Epoch 28560/30000 Training Loss: 0.04039759188890457\n",
      "Epoch 28561/30000 Training Loss: 0.04906092584133148\n",
      "Epoch 28562/30000 Training Loss: 0.04223210737109184\n",
      "Epoch 28563/30000 Training Loss: 0.044065557420253754\n",
      "Epoch 28564/30000 Training Loss: 0.051885008811950684\n",
      "Epoch 28565/30000 Training Loss: 0.030035877600312233\n",
      "Epoch 28566/30000 Training Loss: 0.05259539932012558\n",
      "Epoch 28567/30000 Training Loss: 0.04653586819767952\n",
      "Epoch 28568/30000 Training Loss: 0.03738739341497421\n",
      "Epoch 28569/30000 Training Loss: 0.03705630451440811\n",
      "Epoch 28570/30000 Training Loss: 0.043788667768239975\n",
      "Epoch 28571/30000 Training Loss: 0.05080661177635193\n",
      "Epoch 28572/30000 Training Loss: 0.046916406601667404\n",
      "Epoch 28573/30000 Training Loss: 0.05335737764835358\n",
      "Epoch 28574/30000 Training Loss: 0.047908902168273926\n",
      "Epoch 28575/30000 Training Loss: 0.052303001284599304\n",
      "Epoch 28576/30000 Training Loss: 0.05616704747080803\n",
      "Epoch 28577/30000 Training Loss: 0.051001839339733124\n",
      "Epoch 28578/30000 Training Loss: 0.04079286381602287\n",
      "Epoch 28579/30000 Training Loss: 0.05020146444439888\n",
      "Epoch 28580/30000 Training Loss: 0.03402536362409592\n",
      "Epoch 28581/30000 Training Loss: 0.03859500214457512\n",
      "Epoch 28582/30000 Training Loss: 0.04888761043548584\n",
      "Epoch 28583/30000 Training Loss: 0.0619233176112175\n",
      "Epoch 28584/30000 Training Loss: 0.03498290479183197\n",
      "Epoch 28585/30000 Training Loss: 0.047101039439439774\n",
      "Epoch 28586/30000 Training Loss: 0.03510900214314461\n",
      "Epoch 28587/30000 Training Loss: 0.03775317594408989\n",
      "Epoch 28588/30000 Training Loss: 0.03910930082201958\n",
      "Epoch 28589/30000 Training Loss: 0.0332716666162014\n",
      "Epoch 28590/30000 Training Loss: 0.051778510212898254\n",
      "Epoch 28591/30000 Training Loss: 0.04020964354276657\n",
      "Epoch 28592/30000 Training Loss: 0.04811640828847885\n",
      "Epoch 28593/30000 Training Loss: 0.040202796459198\n",
      "Epoch 28594/30000 Training Loss: 0.040134772658348083\n",
      "Epoch 28595/30000 Training Loss: 0.049460556358098984\n",
      "Epoch 28596/30000 Training Loss: 0.04116414487361908\n",
      "Epoch 28597/30000 Training Loss: 0.05100483447313309\n",
      "Epoch 28598/30000 Training Loss: 0.041563306003808975\n",
      "Epoch 28599/30000 Training Loss: 0.03236784413456917\n",
      "Epoch 28600/30000 Training Loss: 0.056972771883010864\n",
      "Epoch 28600/30000 Validation Loss: 0.070672407746315\n",
      "Epoch 28601/30000 Training Loss: 0.041951216757297516\n",
      "Epoch 28602/30000 Training Loss: 0.0404859259724617\n",
      "Epoch 28603/30000 Training Loss: 0.03717764839529991\n",
      "Epoch 28604/30000 Training Loss: 0.03073015809059143\n",
      "Epoch 28605/30000 Training Loss: 0.04369569569826126\n",
      "Epoch 28606/30000 Training Loss: 0.06657418608665466\n",
      "Epoch 28607/30000 Training Loss: 0.04390912503004074\n",
      "Epoch 28608/30000 Training Loss: 0.03733941912651062\n",
      "Epoch 28609/30000 Training Loss: 0.042130373418331146\n",
      "Epoch 28610/30000 Training Loss: 0.05925194174051285\n",
      "Epoch 28611/30000 Training Loss: 0.04422077164053917\n",
      "Epoch 28612/30000 Training Loss: 0.033378686755895615\n",
      "Epoch 28613/30000 Training Loss: 0.052478283643722534\n",
      "Epoch 28614/30000 Training Loss: 0.04018218070268631\n",
      "Epoch 28615/30000 Training Loss: 0.046303778886795044\n",
      "Epoch 28616/30000 Training Loss: 0.05210266262292862\n",
      "Epoch 28617/30000 Training Loss: 0.027225958183407784\n",
      "Epoch 28618/30000 Training Loss: 0.043424252420663834\n",
      "Epoch 28619/30000 Training Loss: 0.04740405082702637\n",
      "Epoch 28620/30000 Training Loss: 0.03804901987314224\n",
      "Epoch 28621/30000 Training Loss: 0.05270779877901077\n",
      "Epoch 28622/30000 Training Loss: 0.0408414788544178\n",
      "Epoch 28623/30000 Training Loss: 0.03099902905523777\n",
      "Epoch 28624/30000 Training Loss: 0.04719502478837967\n",
      "Epoch 28625/30000 Training Loss: 0.04135195538401604\n",
      "Epoch 28626/30000 Training Loss: 0.04051763564348221\n",
      "Epoch 28627/30000 Training Loss: 0.0325482040643692\n",
      "Epoch 28628/30000 Training Loss: 0.044214386492967606\n",
      "Epoch 28629/30000 Training Loss: 0.03749207779765129\n",
      "Epoch 28630/30000 Training Loss: 0.04650232940912247\n",
      "Epoch 28631/30000 Training Loss: 0.0369059182703495\n",
      "Epoch 28632/30000 Training Loss: 0.04527822881937027\n",
      "Epoch 28633/30000 Training Loss: 0.05010427534580231\n",
      "Epoch 28634/30000 Training Loss: 0.05679384618997574\n",
      "Epoch 28635/30000 Training Loss: 0.049225956201553345\n",
      "Epoch 28636/30000 Training Loss: 0.03280966356396675\n",
      "Epoch 28637/30000 Training Loss: 0.03887299448251724\n",
      "Epoch 28638/30000 Training Loss: 0.04252751171588898\n",
      "Epoch 28639/30000 Training Loss: 0.041249752044677734\n",
      "Epoch 28640/30000 Training Loss: 0.0422450453042984\n",
      "Epoch 28641/30000 Training Loss: 0.042287763208150864\n",
      "Epoch 28642/30000 Training Loss: 0.04369868338108063\n",
      "Epoch 28643/30000 Training Loss: 0.04202265292406082\n",
      "Epoch 28644/30000 Training Loss: 0.04456786438822746\n",
      "Epoch 28645/30000 Training Loss: 0.032017722725868225\n",
      "Epoch 28646/30000 Training Loss: 0.05698798596858978\n",
      "Epoch 28647/30000 Training Loss: 0.03716975077986717\n",
      "Epoch 28648/30000 Training Loss: 0.03647885471582413\n",
      "Epoch 28649/30000 Training Loss: 0.038210511207580566\n",
      "Epoch 28650/30000 Training Loss: 0.042781051248311996\n",
      "Epoch 28651/30000 Training Loss: 0.04149411618709564\n",
      "Epoch 28652/30000 Training Loss: 0.041200969368219376\n",
      "Epoch 28653/30000 Training Loss: 0.042558036744594574\n",
      "Epoch 28654/30000 Training Loss: 0.05125342309474945\n",
      "Epoch 28655/30000 Training Loss: 0.04094444960355759\n",
      "Epoch 28656/30000 Training Loss: 0.0374021902680397\n",
      "Epoch 28657/30000 Training Loss: 0.041723355650901794\n",
      "Epoch 28658/30000 Training Loss: 0.04815230891108513\n",
      "Epoch 28659/30000 Training Loss: 0.04548833519220352\n",
      "Epoch 28660/30000 Training Loss: 0.034993112087249756\n",
      "Epoch 28661/30000 Training Loss: 0.04402560740709305\n",
      "Epoch 28662/30000 Training Loss: 0.03862553462386131\n",
      "Epoch 28663/30000 Training Loss: 0.05776161700487137\n",
      "Epoch 28664/30000 Training Loss: 0.04275364428758621\n",
      "Epoch 28665/30000 Training Loss: 0.041462838649749756\n",
      "Epoch 28666/30000 Training Loss: 0.0430898517370224\n",
      "Epoch 28667/30000 Training Loss: 0.04451002925634384\n",
      "Epoch 28668/30000 Training Loss: 0.04296128451824188\n",
      "Epoch 28669/30000 Training Loss: 0.04565535858273506\n",
      "Epoch 28670/30000 Training Loss: 0.040500208735466\n",
      "Epoch 28671/30000 Training Loss: 0.044238533824682236\n",
      "Epoch 28672/30000 Training Loss: 0.03581167757511139\n",
      "Epoch 28673/30000 Training Loss: 0.04763024300336838\n",
      "Epoch 28674/30000 Training Loss: 0.04102412611246109\n",
      "Epoch 28675/30000 Training Loss: 0.03986698389053345\n",
      "Epoch 28676/30000 Training Loss: 0.048686519265174866\n",
      "Epoch 28677/30000 Training Loss: 0.024590320885181427\n",
      "Epoch 28678/30000 Training Loss: 0.04909442365169525\n",
      "Epoch 28679/30000 Training Loss: 0.03551948070526123\n",
      "Epoch 28680/30000 Training Loss: 0.036843668669462204\n",
      "Epoch 28681/30000 Training Loss: 0.03952706232666969\n",
      "Epoch 28682/30000 Training Loss: 0.0282273031771183\n",
      "Epoch 28683/30000 Training Loss: 0.05016293004155159\n",
      "Epoch 28684/30000 Training Loss: 0.03733634948730469\n",
      "Epoch 28685/30000 Training Loss: 0.04798782616853714\n",
      "Epoch 28686/30000 Training Loss: 0.04588109254837036\n",
      "Epoch 28687/30000 Training Loss: 0.04098571091890335\n",
      "Epoch 28688/30000 Training Loss: 0.04468824714422226\n",
      "Epoch 28689/30000 Training Loss: 0.052684538066387177\n",
      "Epoch 28690/30000 Training Loss: 0.04645717889070511\n",
      "Epoch 28691/30000 Training Loss: 0.04693717882037163\n",
      "Epoch 28692/30000 Training Loss: 0.05075831711292267\n",
      "Epoch 28693/30000 Training Loss: 0.04987376183271408\n",
      "Epoch 28694/30000 Training Loss: 0.0435347780585289\n",
      "Epoch 28695/30000 Training Loss: 0.04294365644454956\n",
      "Epoch 28696/30000 Training Loss: 0.04467952996492386\n",
      "Epoch 28697/30000 Training Loss: 0.03385699540376663\n",
      "Epoch 28698/30000 Training Loss: 0.05674883723258972\n",
      "Epoch 28699/30000 Training Loss: 0.0379633903503418\n",
      "Epoch 28700/30000 Training Loss: 0.0322590246796608\n",
      "Epoch 28700/30000 Validation Loss: 0.037452712655067444\n",
      "Epoch 28701/30000 Training Loss: 0.053161703050136566\n",
      "Epoch 28702/30000 Training Loss: 0.05945733189582825\n",
      "Epoch 28703/30000 Training Loss: 0.04681533947587013\n",
      "Epoch 28704/30000 Training Loss: 0.048238545656204224\n",
      "Epoch 28705/30000 Training Loss: 0.04479819908738136\n",
      "Epoch 28706/30000 Training Loss: 0.048747289925813675\n",
      "Epoch 28707/30000 Training Loss: 0.04605099558830261\n",
      "Epoch 28708/30000 Training Loss: 0.02809189073741436\n",
      "Epoch 28709/30000 Training Loss: 0.05156540125608444\n",
      "Epoch 28710/30000 Training Loss: 0.04731326177716255\n",
      "Epoch 28711/30000 Training Loss: 0.05600094795227051\n",
      "Epoch 28712/30000 Training Loss: 0.03798578679561615\n",
      "Epoch 28713/30000 Training Loss: 0.0434098094701767\n",
      "Epoch 28714/30000 Training Loss: 0.035406991839408875\n",
      "Epoch 28715/30000 Training Loss: 0.046709317713975906\n",
      "Epoch 28716/30000 Training Loss: 0.051704708486795425\n",
      "Epoch 28717/30000 Training Loss: 0.042624879628419876\n",
      "Epoch 28718/30000 Training Loss: 0.027608342468738556\n",
      "Epoch 28719/30000 Training Loss: 0.04869208112359047\n",
      "Epoch 28720/30000 Training Loss: 0.037631310522556305\n",
      "Epoch 28721/30000 Training Loss: 0.03196951746940613\n",
      "Epoch 28722/30000 Training Loss: 0.044064342975616455\n",
      "Epoch 28723/30000 Training Loss: 0.042291659861803055\n",
      "Epoch 28724/30000 Training Loss: 0.04896474629640579\n",
      "Epoch 28725/30000 Training Loss: 0.043537527322769165\n",
      "Epoch 28726/30000 Training Loss: 0.036748938262462616\n",
      "Epoch 28727/30000 Training Loss: 0.045689672231674194\n",
      "Epoch 28728/30000 Training Loss: 0.037287838757038116\n",
      "Epoch 28729/30000 Training Loss: 0.038996532559394836\n",
      "Epoch 28730/30000 Training Loss: 0.06054820865392685\n",
      "Epoch 28731/30000 Training Loss: 0.04439263418316841\n",
      "Epoch 28732/30000 Training Loss: 0.041530221700668335\n",
      "Epoch 28733/30000 Training Loss: 0.042553842067718506\n",
      "Epoch 28734/30000 Training Loss: 0.06057540327310562\n",
      "Epoch 28735/30000 Training Loss: 0.04478708654642105\n",
      "Epoch 28736/30000 Training Loss: 0.03989828750491142\n",
      "Epoch 28737/30000 Training Loss: 0.04364408925175667\n",
      "Epoch 28738/30000 Training Loss: 0.04754817485809326\n",
      "Epoch 28739/30000 Training Loss: 0.043169982731342316\n",
      "Epoch 28740/30000 Training Loss: 0.03865932673215866\n",
      "Epoch 28741/30000 Training Loss: 0.047478556632995605\n",
      "Epoch 28742/30000 Training Loss: 0.04293293505907059\n",
      "Epoch 28743/30000 Training Loss: 0.04028407484292984\n",
      "Epoch 28744/30000 Training Loss: 0.03711135685443878\n",
      "Epoch 28745/30000 Training Loss: 0.05159175395965576\n",
      "Epoch 28746/30000 Training Loss: 0.03377830609679222\n",
      "Epoch 28747/30000 Training Loss: 0.05210607498884201\n",
      "Epoch 28748/30000 Training Loss: 0.05675419792532921\n",
      "Epoch 28749/30000 Training Loss: 0.05242105573415756\n",
      "Epoch 28750/30000 Training Loss: 0.041169892996549606\n",
      "Epoch 28751/30000 Training Loss: 0.05312931537628174\n",
      "Epoch 28752/30000 Training Loss: 0.03454612195491791\n",
      "Epoch 28753/30000 Training Loss: 0.047565534710884094\n",
      "Epoch 28754/30000 Training Loss: 0.0287352092564106\n",
      "Epoch 28755/30000 Training Loss: 0.04942825436592102\n",
      "Epoch 28756/30000 Training Loss: 0.049893077462911606\n",
      "Epoch 28757/30000 Training Loss: 0.039233073592185974\n",
      "Epoch 28758/30000 Training Loss: 0.043424565345048904\n",
      "Epoch 28759/30000 Training Loss: 0.04037443920969963\n",
      "Epoch 28760/30000 Training Loss: 0.03693068400025368\n",
      "Epoch 28761/30000 Training Loss: 0.058417756110429764\n",
      "Epoch 28762/30000 Training Loss: 0.03964931517839432\n",
      "Epoch 28763/30000 Training Loss: 0.041347622871398926\n",
      "Epoch 28764/30000 Training Loss: 0.05952920392155647\n",
      "Epoch 28765/30000 Training Loss: 0.043876953423023224\n",
      "Epoch 28766/30000 Training Loss: 0.051057495176792145\n",
      "Epoch 28767/30000 Training Loss: 0.033695947378873825\n",
      "Epoch 28768/30000 Training Loss: 0.04193960502743721\n",
      "Epoch 28769/30000 Training Loss: 0.0359366238117218\n",
      "Epoch 28770/30000 Training Loss: 0.05206876993179321\n",
      "Epoch 28771/30000 Training Loss: 0.03493679687380791\n",
      "Epoch 28772/30000 Training Loss: 0.05876680463552475\n",
      "Epoch 28773/30000 Training Loss: 0.05561669170856476\n",
      "Epoch 28774/30000 Training Loss: 0.030066808685660362\n",
      "Epoch 28775/30000 Training Loss: 0.05615472421050072\n",
      "Epoch 28776/30000 Training Loss: 0.0414106510579586\n",
      "Epoch 28777/30000 Training Loss: 0.043611302971839905\n",
      "Epoch 28778/30000 Training Loss: 0.04022245854139328\n",
      "Epoch 28779/30000 Training Loss: 0.04930310696363449\n",
      "Epoch 28780/30000 Training Loss: 0.03787803277373314\n",
      "Epoch 28781/30000 Training Loss: 0.04325835034251213\n",
      "Epoch 28782/30000 Training Loss: 0.03556698560714722\n",
      "Epoch 28783/30000 Training Loss: 0.026336560025811195\n",
      "Epoch 28784/30000 Training Loss: 0.04170973226428032\n",
      "Epoch 28785/30000 Training Loss: 0.037268802523612976\n",
      "Epoch 28786/30000 Training Loss: 0.03933817520737648\n",
      "Epoch 28787/30000 Training Loss: 0.0418756902217865\n",
      "Epoch 28788/30000 Training Loss: 0.03421226143836975\n",
      "Epoch 28789/30000 Training Loss: 0.0359949991106987\n",
      "Epoch 28790/30000 Training Loss: 0.052732404321432114\n",
      "Epoch 28791/30000 Training Loss: 0.038553498685359955\n",
      "Epoch 28792/30000 Training Loss: 0.037033576518297195\n",
      "Epoch 28793/30000 Training Loss: 0.025844436138868332\n",
      "Epoch 28794/30000 Training Loss: 0.040135666728019714\n",
      "Epoch 28795/30000 Training Loss: 0.03353752940893173\n",
      "Epoch 28796/30000 Training Loss: 0.03651575744152069\n",
      "Epoch 28797/30000 Training Loss: 0.04195421561598778\n",
      "Epoch 28798/30000 Training Loss: 0.03630811348557472\n",
      "Epoch 28799/30000 Training Loss: 0.04072774946689606\n",
      "Epoch 28800/30000 Training Loss: 0.03781064972281456\n",
      "Epoch 28800/30000 Validation Loss: 0.04753326624631882\n",
      "Epoch 28801/30000 Training Loss: 0.04986560344696045\n",
      "Epoch 28802/30000 Training Loss: 0.05404621362686157\n",
      "Epoch 28803/30000 Training Loss: 0.044351011514663696\n",
      "Epoch 28804/30000 Training Loss: 0.052882902324199677\n",
      "Epoch 28805/30000 Training Loss: 0.027682140469551086\n",
      "Epoch 28806/30000 Training Loss: 0.05250538885593414\n",
      "Epoch 28807/30000 Training Loss: 0.033019594848155975\n",
      "Epoch 28808/30000 Training Loss: 0.04689047485589981\n",
      "Epoch 28809/30000 Training Loss: 0.041479386389255524\n",
      "Epoch 28810/30000 Training Loss: 0.054114870727062225\n",
      "Epoch 28811/30000 Training Loss: 0.04673386737704277\n",
      "Epoch 28812/30000 Training Loss: 0.05223090946674347\n",
      "Epoch 28813/30000 Training Loss: 0.03424690291285515\n",
      "Epoch 28814/30000 Training Loss: 0.03362279012799263\n",
      "Epoch 28815/30000 Training Loss: 0.035239145159721375\n",
      "Epoch 28816/30000 Training Loss: 0.03952620550990105\n",
      "Epoch 28817/30000 Training Loss: 0.03800028935074806\n",
      "Epoch 28818/30000 Training Loss: 0.030540654435753822\n",
      "Epoch 28819/30000 Training Loss: 0.038603026419878006\n",
      "Epoch 28820/30000 Training Loss: 0.042995478957891464\n",
      "Epoch 28821/30000 Training Loss: 0.03632458671927452\n",
      "Epoch 28822/30000 Training Loss: 0.04238565266132355\n",
      "Epoch 28823/30000 Training Loss: 0.03675180673599243\n",
      "Epoch 28824/30000 Training Loss: 0.04075086861848831\n",
      "Epoch 28825/30000 Training Loss: 0.057517558336257935\n",
      "Epoch 28826/30000 Training Loss: 0.041536226868629456\n",
      "Epoch 28827/30000 Training Loss: 0.04344535619020462\n",
      "Epoch 28828/30000 Training Loss: 0.05870916694402695\n",
      "Epoch 28829/30000 Training Loss: 0.044872693717479706\n",
      "Epoch 28830/30000 Training Loss: 0.05729261785745621\n",
      "Epoch 28831/30000 Training Loss: 0.04063854366540909\n",
      "Epoch 28832/30000 Training Loss: 0.044433631002902985\n",
      "Epoch 28833/30000 Training Loss: 0.03359144181013107\n",
      "Epoch 28834/30000 Training Loss: 0.04481981694698334\n",
      "Epoch 28835/30000 Training Loss: 0.03798423334956169\n",
      "Epoch 28836/30000 Training Loss: 0.04573618620634079\n",
      "Epoch 28837/30000 Training Loss: 0.050734542310237885\n",
      "Epoch 28838/30000 Training Loss: 0.047470465302467346\n",
      "Epoch 28839/30000 Training Loss: 0.04841381311416626\n",
      "Epoch 28840/30000 Training Loss: 0.048446036875247955\n",
      "Epoch 28841/30000 Training Loss: 0.026766914874315262\n",
      "Epoch 28842/30000 Training Loss: 0.042708903551101685\n",
      "Epoch 28843/30000 Training Loss: 0.06822443753480911\n",
      "Epoch 28844/30000 Training Loss: 0.033765073865652084\n",
      "Epoch 28845/30000 Training Loss: 0.050002746284008026\n",
      "Epoch 28846/30000 Training Loss: 0.04986072704195976\n",
      "Epoch 28847/30000 Training Loss: 0.03213860094547272\n",
      "Epoch 28848/30000 Training Loss: 0.034856997430324554\n",
      "Epoch 28849/30000 Training Loss: 0.03638707846403122\n",
      "Epoch 28850/30000 Training Loss: 0.02898789942264557\n",
      "Epoch 28851/30000 Training Loss: 0.060203611850738525\n",
      "Epoch 28852/30000 Training Loss: 0.05071685463190079\n",
      "Epoch 28853/30000 Training Loss: 0.03642308712005615\n",
      "Epoch 28854/30000 Training Loss: 0.06299757957458496\n",
      "Epoch 28855/30000 Training Loss: 0.044492289423942566\n",
      "Epoch 28856/30000 Training Loss: 0.037374477833509445\n",
      "Epoch 28857/30000 Training Loss: 0.033429987728595734\n",
      "Epoch 28858/30000 Training Loss: 0.047194287180900574\n",
      "Epoch 28859/30000 Training Loss: 0.0400872640311718\n",
      "Epoch 28860/30000 Training Loss: 0.045249056071043015\n",
      "Epoch 28861/30000 Training Loss: 0.051147911697626114\n",
      "Epoch 28862/30000 Training Loss: 0.04509582370519638\n",
      "Epoch 28863/30000 Training Loss: 0.04206736013293266\n",
      "Epoch 28864/30000 Training Loss: 0.03929802402853966\n",
      "Epoch 28865/30000 Training Loss: 0.03230713680386543\n",
      "Epoch 28866/30000 Training Loss: 0.04344162717461586\n",
      "Epoch 28867/30000 Training Loss: 0.047092411667108536\n",
      "Epoch 28868/30000 Training Loss: 0.03915979340672493\n",
      "Epoch 28869/30000 Training Loss: 0.04032393917441368\n",
      "Epoch 28870/30000 Training Loss: 0.041988953948020935\n",
      "Epoch 28871/30000 Training Loss: 0.049806661903858185\n",
      "Epoch 28872/30000 Training Loss: 0.056070491671562195\n",
      "Epoch 28873/30000 Training Loss: 0.04055628553032875\n",
      "Epoch 28874/30000 Training Loss: 0.04563664644956589\n",
      "Epoch 28875/30000 Training Loss: 0.03418252617120743\n",
      "Epoch 28876/30000 Training Loss: 0.04817074537277222\n",
      "Epoch 28877/30000 Training Loss: 0.03449104353785515\n",
      "Epoch 28878/30000 Training Loss: 0.037603892385959625\n",
      "Epoch 28879/30000 Training Loss: 0.05240655317902565\n",
      "Epoch 28880/30000 Training Loss: 0.030615024268627167\n",
      "Epoch 28881/30000 Training Loss: 0.05108587071299553\n",
      "Epoch 28882/30000 Training Loss: 0.04225114360451698\n",
      "Epoch 28883/30000 Training Loss: 0.04719609022140503\n",
      "Epoch 28884/30000 Training Loss: 0.042998045682907104\n",
      "Epoch 28885/30000 Training Loss: 0.04682331532239914\n",
      "Epoch 28886/30000 Training Loss: 0.04533416032791138\n",
      "Epoch 28887/30000 Training Loss: 0.03877297788858414\n",
      "Epoch 28888/30000 Training Loss: 0.04899756610393524\n",
      "Epoch 28889/30000 Training Loss: 0.04093252494931221\n",
      "Epoch 28890/30000 Training Loss: 0.046559181064367294\n",
      "Epoch 28891/30000 Training Loss: 0.043915752321481705\n",
      "Epoch 28892/30000 Training Loss: 0.035948872566223145\n",
      "Epoch 28893/30000 Training Loss: 0.029102712869644165\n",
      "Epoch 28894/30000 Training Loss: 0.04441000893712044\n",
      "Epoch 28895/30000 Training Loss: 0.030235201120376587\n",
      "Epoch 28896/30000 Training Loss: 0.04561498016119003\n",
      "Epoch 28897/30000 Training Loss: 0.035775020718574524\n",
      "Epoch 28898/30000 Training Loss: 0.046630002558231354\n",
      "Epoch 28899/30000 Training Loss: 0.039977818727493286\n",
      "Epoch 28900/30000 Training Loss: 0.028814755380153656\n",
      "Epoch 28900/30000 Validation Loss: 0.05263322964310646\n",
      "Epoch 28901/30000 Training Loss: 0.03952639922499657\n",
      "Epoch 28902/30000 Training Loss: 0.041166406124830246\n",
      "Epoch 28903/30000 Training Loss: 0.046346355229616165\n",
      "Epoch 28904/30000 Training Loss: 0.03879380226135254\n",
      "Epoch 28905/30000 Training Loss: 0.0409545861184597\n",
      "Epoch 28906/30000 Training Loss: 0.031090551987290382\n",
      "Epoch 28907/30000 Training Loss: 0.0404779389500618\n",
      "Epoch 28908/30000 Training Loss: 0.03876620903611183\n",
      "Epoch 28909/30000 Training Loss: 0.04484529048204422\n",
      "Epoch 28910/30000 Training Loss: 0.053276292979717255\n",
      "Epoch 28911/30000 Training Loss: 0.03775162994861603\n",
      "Epoch 28912/30000 Training Loss: 0.04220489412546158\n",
      "Epoch 28913/30000 Training Loss: 0.04416705667972565\n",
      "Epoch 28914/30000 Training Loss: 0.03626155108213425\n",
      "Epoch 28915/30000 Training Loss: 0.04091750085353851\n",
      "Epoch 28916/30000 Training Loss: 0.05532795190811157\n",
      "Epoch 28917/30000 Training Loss: 0.041896361857652664\n",
      "Epoch 28918/30000 Training Loss: 0.03611448407173157\n",
      "Epoch 28919/30000 Training Loss: 0.04459688812494278\n",
      "Epoch 28920/30000 Training Loss: 0.029529504477977753\n",
      "Epoch 28921/30000 Training Loss: 0.047201916575431824\n",
      "Epoch 28922/30000 Training Loss: 0.0347745344042778\n",
      "Epoch 28923/30000 Training Loss: 0.04839358478784561\n",
      "Epoch 28924/30000 Training Loss: 0.04261306673288345\n",
      "Epoch 28925/30000 Training Loss: 0.04377476125955582\n",
      "Epoch 28926/30000 Training Loss: 0.05248001590371132\n",
      "Epoch 28927/30000 Training Loss: 0.03250294178724289\n",
      "Epoch 28928/30000 Training Loss: 0.05290697515010834\n",
      "Epoch 28929/30000 Training Loss: 0.0426969900727272\n",
      "Epoch 28930/30000 Training Loss: 0.03290615230798721\n",
      "Epoch 28931/30000 Training Loss: 0.04987753555178642\n",
      "Epoch 28932/30000 Training Loss: 0.03135351836681366\n",
      "Epoch 28933/30000 Training Loss: 0.030905697494745255\n",
      "Epoch 28934/30000 Training Loss: 0.047975070774555206\n",
      "Epoch 28935/30000 Training Loss: 0.04397525638341904\n",
      "Epoch 28936/30000 Training Loss: 0.03948156535625458\n",
      "Epoch 28937/30000 Training Loss: 0.03526482731103897\n",
      "Epoch 28938/30000 Training Loss: 0.04649590700864792\n",
      "Epoch 28939/30000 Training Loss: 0.045950938016176224\n",
      "Epoch 28940/30000 Training Loss: 0.04019126296043396\n",
      "Epoch 28941/30000 Training Loss: 0.04266601800918579\n",
      "Epoch 28942/30000 Training Loss: 0.04556664079427719\n",
      "Epoch 28943/30000 Training Loss: 0.04161960259079933\n",
      "Epoch 28944/30000 Training Loss: 0.05038118362426758\n",
      "Epoch 28945/30000 Training Loss: 0.06154455244541168\n",
      "Epoch 28946/30000 Training Loss: 0.045981839299201965\n",
      "Epoch 28947/30000 Training Loss: 0.035155877470970154\n",
      "Epoch 28948/30000 Training Loss: 0.03533950448036194\n",
      "Epoch 28949/30000 Training Loss: 0.03821863234043121\n",
      "Epoch 28950/30000 Training Loss: 0.034389883279800415\n",
      "Epoch 28951/30000 Training Loss: 0.04716898128390312\n",
      "Epoch 28952/30000 Training Loss: 0.04356116056442261\n",
      "Epoch 28953/30000 Training Loss: 0.03876480087637901\n",
      "Epoch 28954/30000 Training Loss: 0.04495356231927872\n",
      "Epoch 28955/30000 Training Loss: 0.035607606172561646\n",
      "Epoch 28956/30000 Training Loss: 0.04086735472083092\n",
      "Epoch 28957/30000 Training Loss: 0.043365031480789185\n",
      "Epoch 28958/30000 Training Loss: 0.04271421581506729\n",
      "Epoch 28959/30000 Training Loss: 0.04597880318760872\n",
      "Epoch 28960/30000 Training Loss: 0.05021323263645172\n",
      "Epoch 28961/30000 Training Loss: 0.054206110537052155\n",
      "Epoch 28962/30000 Training Loss: 0.037068285048007965\n",
      "Epoch 28963/30000 Training Loss: 0.04155735298991203\n",
      "Epoch 28964/30000 Training Loss: 0.042017895728349686\n",
      "Epoch 28965/30000 Training Loss: 0.04362569749355316\n",
      "Epoch 28966/30000 Training Loss: 0.032111771404743195\n",
      "Epoch 28967/30000 Training Loss: 0.0340249165892601\n",
      "Epoch 28968/30000 Training Loss: 0.03832035884261131\n",
      "Epoch 28969/30000 Training Loss: 0.035449303686618805\n",
      "Epoch 28970/30000 Training Loss: 0.03905601054430008\n",
      "Epoch 28971/30000 Training Loss: 0.03141538053750992\n",
      "Epoch 28972/30000 Training Loss: 0.04026452451944351\n",
      "Epoch 28973/30000 Training Loss: 0.033870067447423935\n",
      "Epoch 28974/30000 Training Loss: 0.04392607510089874\n",
      "Epoch 28975/30000 Training Loss: 0.0377102866768837\n",
      "Epoch 28976/30000 Training Loss: 0.036884576082229614\n",
      "Epoch 28977/30000 Training Loss: 0.04056209698319435\n",
      "Epoch 28978/30000 Training Loss: 0.045676276087760925\n",
      "Epoch 28979/30000 Training Loss: 0.045956775546073914\n",
      "Epoch 28980/30000 Training Loss: 0.03376775234937668\n",
      "Epoch 28981/30000 Training Loss: 0.05960533022880554\n",
      "Epoch 28982/30000 Training Loss: 0.06014121323823929\n",
      "Epoch 28983/30000 Training Loss: 0.039543963968753815\n",
      "Epoch 28984/30000 Training Loss: 0.03292382135987282\n",
      "Epoch 28985/30000 Training Loss: 0.05242486298084259\n",
      "Epoch 28986/30000 Training Loss: 0.04427512735128403\n",
      "Epoch 28987/30000 Training Loss: 0.050178609788417816\n",
      "Epoch 28988/30000 Training Loss: 0.03974494710564613\n",
      "Epoch 28989/30000 Training Loss: 0.04018053784966469\n",
      "Epoch 28990/30000 Training Loss: 0.04019466042518616\n",
      "Epoch 28991/30000 Training Loss: 0.033975955098867416\n",
      "Epoch 28992/30000 Training Loss: 0.0354885533452034\n",
      "Epoch 28993/30000 Training Loss: 0.0438968650996685\n",
      "Epoch 28994/30000 Training Loss: 0.03618873283267021\n",
      "Epoch 28995/30000 Training Loss: 0.04572266340255737\n",
      "Epoch 28996/30000 Training Loss: 0.03371313959360123\n",
      "Epoch 28997/30000 Training Loss: 0.038823407143354416\n",
      "Epoch 28998/30000 Training Loss: 0.05353742092847824\n",
      "Epoch 28999/30000 Training Loss: 0.03937555104494095\n",
      "Epoch 29000/30000 Training Loss: 0.03848361223936081\n",
      "Epoch 29000/30000 Validation Loss: 0.03148837760090828\n",
      "Epoch 29001/30000 Training Loss: 0.06518445163965225\n",
      "Epoch 29002/30000 Training Loss: 0.03712732717394829\n",
      "Epoch 29003/30000 Training Loss: 0.044091831892728806\n",
      "Epoch 29004/30000 Training Loss: 0.042007409036159515\n",
      "Epoch 29005/30000 Training Loss: 0.04801741987466812\n",
      "Epoch 29006/30000 Training Loss: 0.040354982018470764\n",
      "Epoch 29007/30000 Training Loss: 0.06415499746799469\n",
      "Epoch 29008/30000 Training Loss: 0.03859402611851692\n",
      "Epoch 29009/30000 Training Loss: 0.047231245785951614\n",
      "Epoch 29010/30000 Training Loss: 0.044898051768541336\n",
      "Epoch 29011/30000 Training Loss: 0.0395023375749588\n",
      "Epoch 29012/30000 Training Loss: 0.028900951147079468\n",
      "Epoch 29013/30000 Training Loss: 0.060362592339515686\n",
      "Epoch 29014/30000 Training Loss: 0.03997962549328804\n",
      "Epoch 29015/30000 Training Loss: 0.040664978325366974\n",
      "Epoch 29016/30000 Training Loss: 0.060963138937950134\n",
      "Epoch 29017/30000 Training Loss: 0.050704918801784515\n",
      "Epoch 29018/30000 Training Loss: 0.034132570028305054\n",
      "Epoch 29019/30000 Training Loss: 0.03761407732963562\n",
      "Epoch 29020/30000 Training Loss: 0.04040003567934036\n",
      "Epoch 29021/30000 Training Loss: 0.0389510914683342\n",
      "Epoch 29022/30000 Training Loss: 0.05161716416478157\n",
      "Epoch 29023/30000 Training Loss: 0.03703939914703369\n",
      "Epoch 29024/30000 Training Loss: 0.0349147766828537\n",
      "Epoch 29025/30000 Training Loss: 0.04977218806743622\n",
      "Epoch 29026/30000 Training Loss: 0.03797619789838791\n",
      "Epoch 29027/30000 Training Loss: 0.050747349858284\n",
      "Epoch 29028/30000 Training Loss: 0.05838703364133835\n",
      "Epoch 29029/30000 Training Loss: 0.035410523414611816\n",
      "Epoch 29030/30000 Training Loss: 0.025861911475658417\n",
      "Epoch 29031/30000 Training Loss: 0.03927334398031235\n",
      "Epoch 29032/30000 Training Loss: 0.040904972702264786\n",
      "Epoch 29033/30000 Training Loss: 0.045414961874485016\n",
      "Epoch 29034/30000 Training Loss: 0.03718489035964012\n",
      "Epoch 29035/30000 Training Loss: 0.0426226407289505\n",
      "Epoch 29036/30000 Training Loss: 0.03393326699733734\n",
      "Epoch 29037/30000 Training Loss: 0.04307705909013748\n",
      "Epoch 29038/30000 Training Loss: 0.05519235134124756\n",
      "Epoch 29039/30000 Training Loss: 0.029195358976721764\n",
      "Epoch 29040/30000 Training Loss: 0.04388434439897537\n",
      "Epoch 29041/30000 Training Loss: 0.034853361546993256\n",
      "Epoch 29042/30000 Training Loss: 0.04345851019024849\n",
      "Epoch 29043/30000 Training Loss: 0.038886699825525284\n",
      "Epoch 29044/30000 Training Loss: 0.03970038518309593\n",
      "Epoch 29045/30000 Training Loss: 0.043725479394197464\n",
      "Epoch 29046/30000 Training Loss: 0.05877295881509781\n",
      "Epoch 29047/30000 Training Loss: 0.05466490983963013\n",
      "Epoch 29048/30000 Training Loss: 0.03938417509198189\n",
      "Epoch 29049/30000 Training Loss: 0.05536515265703201\n",
      "Epoch 29050/30000 Training Loss: 0.04640309512615204\n",
      "Epoch 29051/30000 Training Loss: 0.0467018187046051\n",
      "Epoch 29052/30000 Training Loss: 0.04653686285018921\n",
      "Epoch 29053/30000 Training Loss: 0.0468004047870636\n",
      "Epoch 29054/30000 Training Loss: 0.05171389505267143\n",
      "Epoch 29055/30000 Training Loss: 0.035879507660865784\n",
      "Epoch 29056/30000 Training Loss: 0.03877168148756027\n",
      "Epoch 29057/30000 Training Loss: 0.03204793483018875\n",
      "Epoch 29058/30000 Training Loss: 0.039213769137859344\n",
      "Epoch 29059/30000 Training Loss: 0.05066795274615288\n",
      "Epoch 29060/30000 Training Loss: 0.04175905883312225\n",
      "Epoch 29061/30000 Training Loss: 0.037602245807647705\n",
      "Epoch 29062/30000 Training Loss: 0.03527186065912247\n",
      "Epoch 29063/30000 Training Loss: 0.04977294057607651\n",
      "Epoch 29064/30000 Training Loss: 0.05047614499926567\n",
      "Epoch 29065/30000 Training Loss: 0.03648664802312851\n",
      "Epoch 29066/30000 Training Loss: 0.031930163502693176\n",
      "Epoch 29067/30000 Training Loss: 0.04007750749588013\n",
      "Epoch 29068/30000 Training Loss: 0.03408787399530411\n",
      "Epoch 29069/30000 Training Loss: 0.04800347983837128\n",
      "Epoch 29070/30000 Training Loss: 0.03975856676697731\n",
      "Epoch 29071/30000 Training Loss: 0.03293100744485855\n",
      "Epoch 29072/30000 Training Loss: 0.048540692776441574\n",
      "Epoch 29073/30000 Training Loss: 0.04424728453159332\n",
      "Epoch 29074/30000 Training Loss: 0.040726207196712494\n",
      "Epoch 29075/30000 Training Loss: 0.05007154494524002\n",
      "Epoch 29076/30000 Training Loss: 0.05110820382833481\n",
      "Epoch 29077/30000 Training Loss: 0.035414427518844604\n",
      "Epoch 29078/30000 Training Loss: 0.02792711928486824\n",
      "Epoch 29079/30000 Training Loss: 0.03594240918755531\n",
      "Epoch 29080/30000 Training Loss: 0.03503043204545975\n",
      "Epoch 29081/30000 Training Loss: 0.041848622262477875\n",
      "Epoch 29082/30000 Training Loss: 0.04989349842071533\n",
      "Epoch 29083/30000 Training Loss: 0.0469222217798233\n",
      "Epoch 29084/30000 Training Loss: 0.034809235483407974\n",
      "Epoch 29085/30000 Training Loss: 0.04143703728914261\n",
      "Epoch 29086/30000 Training Loss: 0.05494004487991333\n",
      "Epoch 29087/30000 Training Loss: 0.046508342027664185\n",
      "Epoch 29088/30000 Training Loss: 0.045902982354164124\n",
      "Epoch 29089/30000 Training Loss: 0.04143756628036499\n",
      "Epoch 29090/30000 Training Loss: 0.044479385018348694\n",
      "Epoch 29091/30000 Training Loss: 0.048238351941108704\n",
      "Epoch 29092/30000 Training Loss: 0.05122971534729004\n",
      "Epoch 29093/30000 Training Loss: 0.03656806796789169\n",
      "Epoch 29094/30000 Training Loss: 0.036408040672540665\n",
      "Epoch 29095/30000 Training Loss: 0.043486081063747406\n",
      "Epoch 29096/30000 Training Loss: 0.03941817581653595\n",
      "Epoch 29097/30000 Training Loss: 0.04742307960987091\n",
      "Epoch 29098/30000 Training Loss: 0.03999342396855354\n",
      "Epoch 29099/30000 Training Loss: 0.051226645708084106\n",
      "Epoch 29100/30000 Training Loss: 0.04337543994188309\n",
      "Epoch 29100/30000 Validation Loss: 0.04371291399002075\n",
      "Epoch 29101/30000 Training Loss: 0.04161889851093292\n",
      "Epoch 29102/30000 Training Loss: 0.04851432889699936\n",
      "Epoch 29103/30000 Training Loss: 0.041558291763067245\n",
      "Epoch 29104/30000 Training Loss: 0.0457889586687088\n",
      "Epoch 29105/30000 Training Loss: 0.03974919393658638\n",
      "Epoch 29106/30000 Training Loss: 0.031167061999440193\n",
      "Epoch 29107/30000 Training Loss: 0.039860378950834274\n",
      "Epoch 29108/30000 Training Loss: 0.054395467042922974\n",
      "Epoch 29109/30000 Training Loss: 0.03348689153790474\n",
      "Epoch 29110/30000 Training Loss: 0.03901398181915283\n",
      "Epoch 29111/30000 Training Loss: 0.043461620807647705\n",
      "Epoch 29112/30000 Training Loss: 0.04274198040366173\n",
      "Epoch 29113/30000 Training Loss: 0.03836999088525772\n",
      "Epoch 29114/30000 Training Loss: 0.05564722791314125\n",
      "Epoch 29115/30000 Training Loss: 0.037545930594205856\n",
      "Epoch 29116/30000 Training Loss: 0.031971558928489685\n",
      "Epoch 29117/30000 Training Loss: 0.04673715680837631\n",
      "Epoch 29118/30000 Training Loss: 0.05039869248867035\n",
      "Epoch 29119/30000 Training Loss: 0.038340870290994644\n",
      "Epoch 29120/30000 Training Loss: 0.0489199198782444\n",
      "Epoch 29121/30000 Training Loss: 0.04265572130680084\n",
      "Epoch 29122/30000 Training Loss: 0.044933684170246124\n",
      "Epoch 29123/30000 Training Loss: 0.031458791345357895\n",
      "Epoch 29124/30000 Training Loss: 0.04631051793694496\n",
      "Epoch 29125/30000 Training Loss: 0.051237791776657104\n",
      "Epoch 29126/30000 Training Loss: 0.03929150104522705\n",
      "Epoch 29127/30000 Training Loss: 0.04521059989929199\n",
      "Epoch 29128/30000 Training Loss: 0.045598313212394714\n",
      "Epoch 29129/30000 Training Loss: 0.04081278294324875\n",
      "Epoch 29130/30000 Training Loss: 0.03553023934364319\n",
      "Epoch 29131/30000 Training Loss: 0.03598473221063614\n",
      "Epoch 29132/30000 Training Loss: 0.04093605652451515\n",
      "Epoch 29133/30000 Training Loss: 0.042761143296957016\n",
      "Epoch 29134/30000 Training Loss: 0.04652572050690651\n",
      "Epoch 29135/30000 Training Loss: 0.03924912214279175\n",
      "Epoch 29136/30000 Training Loss: 0.047802455723285675\n",
      "Epoch 29137/30000 Training Loss: 0.053521133959293365\n",
      "Epoch 29138/30000 Training Loss: 0.03890899568796158\n",
      "Epoch 29139/30000 Training Loss: 0.0342717282474041\n",
      "Epoch 29140/30000 Training Loss: 0.03119083121418953\n",
      "Epoch 29141/30000 Training Loss: 0.038468413054943085\n",
      "Epoch 29142/30000 Training Loss: 0.05934251472353935\n",
      "Epoch 29143/30000 Training Loss: 0.04084569588303566\n",
      "Epoch 29144/30000 Training Loss: 0.03680199384689331\n",
      "Epoch 29145/30000 Training Loss: 0.05608434975147247\n",
      "Epoch 29146/30000 Training Loss: 0.04407252371311188\n",
      "Epoch 29147/30000 Training Loss: 0.04900474101305008\n",
      "Epoch 29148/30000 Training Loss: 0.040585748851299286\n",
      "Epoch 29149/30000 Training Loss: 0.03743717074394226\n",
      "Epoch 29150/30000 Training Loss: 0.04977041855454445\n",
      "Epoch 29151/30000 Training Loss: 0.04624214768409729\n",
      "Epoch 29152/30000 Training Loss: 0.04500891640782356\n",
      "Epoch 29153/30000 Training Loss: 0.03500344604253769\n",
      "Epoch 29154/30000 Training Loss: 0.046880654990673065\n",
      "Epoch 29155/30000 Training Loss: 0.03406504914164543\n",
      "Epoch 29156/30000 Training Loss: 0.04082271456718445\n",
      "Epoch 29157/30000 Training Loss: 0.036847129464149475\n",
      "Epoch 29158/30000 Training Loss: 0.0365975946187973\n",
      "Epoch 29159/30000 Training Loss: 0.04736527055501938\n",
      "Epoch 29160/30000 Training Loss: 0.051437728106975555\n",
      "Epoch 29161/30000 Training Loss: 0.04391998052597046\n",
      "Epoch 29162/30000 Training Loss: 0.040470898151397705\n",
      "Epoch 29163/30000 Training Loss: 0.04146352782845497\n",
      "Epoch 29164/30000 Training Loss: 0.04608658701181412\n",
      "Epoch 29165/30000 Training Loss: 0.03310273587703705\n",
      "Epoch 29166/30000 Training Loss: 0.040898580104112625\n",
      "Epoch 29167/30000 Training Loss: 0.05220029130578041\n",
      "Epoch 29168/30000 Training Loss: 0.036702558398246765\n",
      "Epoch 29169/30000 Training Loss: 0.027596235275268555\n",
      "Epoch 29170/30000 Training Loss: 0.04530385136604309\n",
      "Epoch 29171/30000 Training Loss: 0.04055823013186455\n",
      "Epoch 29172/30000 Training Loss: 0.0489959791302681\n",
      "Epoch 29173/30000 Training Loss: 0.040946900844573975\n",
      "Epoch 29174/30000 Training Loss: 0.0338444784283638\n",
      "Epoch 29175/30000 Training Loss: 0.031505975872278214\n",
      "Epoch 29176/30000 Training Loss: 0.041140809655189514\n",
      "Epoch 29177/30000 Training Loss: 0.03862536698579788\n",
      "Epoch 29178/30000 Training Loss: 0.035668425261974335\n",
      "Epoch 29179/30000 Training Loss: 0.03904661536216736\n",
      "Epoch 29180/30000 Training Loss: 0.043578874319791794\n",
      "Epoch 29181/30000 Training Loss: 0.03358575701713562\n",
      "Epoch 29182/30000 Training Loss: 0.04618323966860771\n",
      "Epoch 29183/30000 Training Loss: 0.03879935294389725\n",
      "Epoch 29184/30000 Training Loss: 0.05451668053865433\n",
      "Epoch 29185/30000 Training Loss: 0.04869384318590164\n",
      "Epoch 29186/30000 Training Loss: 0.04275524243712425\n",
      "Epoch 29187/30000 Training Loss: 0.04836418479681015\n",
      "Epoch 29188/30000 Training Loss: 0.04194954037666321\n",
      "Epoch 29189/30000 Training Loss: 0.035900283604860306\n",
      "Epoch 29190/30000 Training Loss: 0.05025386065244675\n",
      "Epoch 29191/30000 Training Loss: 0.030731018632650375\n",
      "Epoch 29192/30000 Training Loss: 0.03940688073635101\n",
      "Epoch 29193/30000 Training Loss: 0.04049510881304741\n",
      "Epoch 29194/30000 Training Loss: 0.06033029407262802\n",
      "Epoch 29195/30000 Training Loss: 0.04082220047712326\n",
      "Epoch 29196/30000 Training Loss: 0.036548979580402374\n",
      "Epoch 29197/30000 Training Loss: 0.038892120122909546\n",
      "Epoch 29198/30000 Training Loss: 0.031066738069057465\n",
      "Epoch 29199/30000 Training Loss: 0.031311407685279846\n",
      "Epoch 29200/30000 Training Loss: 0.0467325784265995\n",
      "Epoch 29200/30000 Validation Loss: 0.0314566008746624\n",
      "Epoch 29201/30000 Training Loss: 0.03698039427399635\n",
      "Epoch 29202/30000 Training Loss: 0.03753725439310074\n",
      "Epoch 29203/30000 Training Loss: 0.0491068996489048\n",
      "Epoch 29204/30000 Training Loss: 0.040158502757549286\n",
      "Epoch 29205/30000 Training Loss: 0.04790099337697029\n",
      "Epoch 29206/30000 Training Loss: 0.056889861822128296\n",
      "Epoch 29207/30000 Training Loss: 0.039106614887714386\n",
      "Epoch 29208/30000 Training Loss: 0.0479804128408432\n",
      "Epoch 29209/30000 Training Loss: 0.04657309502363205\n",
      "Epoch 29210/30000 Training Loss: 0.03785155713558197\n",
      "Epoch 29211/30000 Training Loss: 0.0510609969496727\n",
      "Epoch 29212/30000 Training Loss: 0.046797871589660645\n",
      "Epoch 29213/30000 Training Loss: 0.048357490450143814\n",
      "Epoch 29214/30000 Training Loss: 0.05208408832550049\n",
      "Epoch 29215/30000 Training Loss: 0.046625059098005295\n",
      "Epoch 29216/30000 Training Loss: 0.039684928953647614\n",
      "Epoch 29217/30000 Training Loss: 0.039464015513658524\n",
      "Epoch 29218/30000 Training Loss: 0.053485918790102005\n",
      "Epoch 29219/30000 Training Loss: 0.051292020827531815\n",
      "Epoch 29220/30000 Training Loss: 0.04871872812509537\n",
      "Epoch 29221/30000 Training Loss: 0.03130976855754852\n",
      "Epoch 29222/30000 Training Loss: 0.04818457365036011\n",
      "Epoch 29223/30000 Training Loss: 0.03558243811130524\n",
      "Epoch 29224/30000 Training Loss: 0.06608112901449203\n",
      "Epoch 29225/30000 Training Loss: 0.03618627414107323\n",
      "Epoch 29226/30000 Training Loss: 0.04031575843691826\n",
      "Epoch 29227/30000 Training Loss: 0.049764521420001984\n",
      "Epoch 29228/30000 Training Loss: 0.04665122181177139\n",
      "Epoch 29229/30000 Training Loss: 0.03657554090023041\n",
      "Epoch 29230/30000 Training Loss: 0.040942296385765076\n",
      "Epoch 29231/30000 Training Loss: 0.04530932009220123\n",
      "Epoch 29232/30000 Training Loss: 0.03539027273654938\n",
      "Epoch 29233/30000 Training Loss: 0.04320913553237915\n",
      "Epoch 29234/30000 Training Loss: 0.04235978424549103\n",
      "Epoch 29235/30000 Training Loss: 0.03993365913629532\n",
      "Epoch 29236/30000 Training Loss: 0.044314879924058914\n",
      "Epoch 29237/30000 Training Loss: 0.03044416755437851\n",
      "Epoch 29238/30000 Training Loss: 0.04093831032514572\n",
      "Epoch 29239/30000 Training Loss: 0.02911360189318657\n",
      "Epoch 29240/30000 Training Loss: 0.04764080047607422\n",
      "Epoch 29241/30000 Training Loss: 0.042135089635849\n",
      "Epoch 29242/30000 Training Loss: 0.033350080251693726\n",
      "Epoch 29243/30000 Training Loss: 0.05102745443582535\n",
      "Epoch 29244/30000 Training Loss: 0.034513890743255615\n",
      "Epoch 29245/30000 Training Loss: 0.043873172253370285\n",
      "Epoch 29246/30000 Training Loss: 0.04374759644269943\n",
      "Epoch 29247/30000 Training Loss: 0.04364801198244095\n",
      "Epoch 29248/30000 Training Loss: 0.045603495091199875\n",
      "Epoch 29249/30000 Training Loss: 0.042881079018116\n",
      "Epoch 29250/30000 Training Loss: 0.037895891815423965\n",
      "Epoch 29251/30000 Training Loss: 0.036971282213926315\n",
      "Epoch 29252/30000 Training Loss: 0.04302556440234184\n",
      "Epoch 29253/30000 Training Loss: 0.054335493594408035\n",
      "Epoch 29254/30000 Training Loss: 0.04006021469831467\n",
      "Epoch 29255/30000 Training Loss: 0.039264678955078125\n",
      "Epoch 29256/30000 Training Loss: 0.053613487631082535\n",
      "Epoch 29257/30000 Training Loss: 0.027167165651917458\n",
      "Epoch 29258/30000 Training Loss: 0.053587138652801514\n",
      "Epoch 29259/30000 Training Loss: 0.0346803143620491\n",
      "Epoch 29260/30000 Training Loss: 0.04482828825712204\n",
      "Epoch 29261/30000 Training Loss: 0.035210929811000824\n",
      "Epoch 29262/30000 Training Loss: 0.03209531679749489\n",
      "Epoch 29263/30000 Training Loss: 0.04609599709510803\n",
      "Epoch 29264/30000 Training Loss: 0.044800885021686554\n",
      "Epoch 29265/30000 Training Loss: 0.0630451887845993\n",
      "Epoch 29266/30000 Training Loss: 0.05227508395910263\n",
      "Epoch 29267/30000 Training Loss: 0.03770977631211281\n",
      "Epoch 29268/30000 Training Loss: 0.04118279367685318\n",
      "Epoch 29269/30000 Training Loss: 0.032584402710199356\n",
      "Epoch 29270/30000 Training Loss: 0.048100829124450684\n",
      "Epoch 29271/30000 Training Loss: 0.038466110825538635\n",
      "Epoch 29272/30000 Training Loss: 0.041310183703899384\n",
      "Epoch 29273/30000 Training Loss: 0.061153508722782135\n",
      "Epoch 29274/30000 Training Loss: 0.03780713677406311\n",
      "Epoch 29275/30000 Training Loss: 0.059519264847040176\n",
      "Epoch 29276/30000 Training Loss: 0.04791117459535599\n",
      "Epoch 29277/30000 Training Loss: 0.04454835131764412\n",
      "Epoch 29278/30000 Training Loss: 0.049442924559116364\n",
      "Epoch 29279/30000 Training Loss: 0.03666233271360397\n",
      "Epoch 29280/30000 Training Loss: 0.03951101005077362\n",
      "Epoch 29281/30000 Training Loss: 0.04435127228498459\n",
      "Epoch 29282/30000 Training Loss: 0.03489851951599121\n",
      "Epoch 29283/30000 Training Loss: 0.040586650371551514\n",
      "Epoch 29284/30000 Training Loss: 0.0331096313893795\n",
      "Epoch 29285/30000 Training Loss: 0.031974587589502335\n",
      "Epoch 29286/30000 Training Loss: 0.03775181621313095\n",
      "Epoch 29287/30000 Training Loss: 0.0398784801363945\n",
      "Epoch 29288/30000 Training Loss: 0.03892485052347183\n",
      "Epoch 29289/30000 Training Loss: 0.043392494320869446\n",
      "Epoch 29290/30000 Training Loss: 0.041715942323207855\n",
      "Epoch 29291/30000 Training Loss: 0.04543371498584747\n",
      "Epoch 29292/30000 Training Loss: 0.04245471581816673\n",
      "Epoch 29293/30000 Training Loss: 0.0376444011926651\n",
      "Epoch 29294/30000 Training Loss: 0.05009320005774498\n",
      "Epoch 29295/30000 Training Loss: 0.05066356062889099\n",
      "Epoch 29296/30000 Training Loss: 0.04015538468956947\n",
      "Epoch 29297/30000 Training Loss: 0.051062315702438354\n",
      "Epoch 29298/30000 Training Loss: 0.04894770681858063\n",
      "Epoch 29299/30000 Training Loss: 0.043741755187511444\n",
      "Epoch 29300/30000 Training Loss: 0.038423530757427216\n",
      "Epoch 29300/30000 Validation Loss: 0.0448111966252327\n",
      "Epoch 29301/30000 Training Loss: 0.04508334398269653\n",
      "Epoch 29302/30000 Training Loss: 0.0445496030151844\n",
      "Epoch 29303/30000 Training Loss: 0.04155109450221062\n",
      "Epoch 29304/30000 Training Loss: 0.034875236451625824\n",
      "Epoch 29305/30000 Training Loss: 0.03910726308822632\n",
      "Epoch 29306/30000 Training Loss: 0.048791512846946716\n",
      "Epoch 29307/30000 Training Loss: 0.03475292772054672\n",
      "Epoch 29308/30000 Training Loss: 0.0471159890294075\n",
      "Epoch 29309/30000 Training Loss: 0.04967060312628746\n",
      "Epoch 29310/30000 Training Loss: 0.04160737246274948\n",
      "Epoch 29311/30000 Training Loss: 0.044233206659555435\n",
      "Epoch 29312/30000 Training Loss: 0.04930919408798218\n",
      "Epoch 29313/30000 Training Loss: 0.04480012506246567\n",
      "Epoch 29314/30000 Training Loss: 0.04003651812672615\n",
      "Epoch 29315/30000 Training Loss: 0.031634893268346786\n",
      "Epoch 29316/30000 Training Loss: 0.04592908173799515\n",
      "Epoch 29317/30000 Training Loss: 0.04527854174375534\n",
      "Epoch 29318/30000 Training Loss: 0.03731241449713707\n",
      "Epoch 29319/30000 Training Loss: 0.028738699853420258\n",
      "Epoch 29320/30000 Training Loss: 0.06092102825641632\n",
      "Epoch 29321/30000 Training Loss: 0.055314838886260986\n",
      "Epoch 29322/30000 Training Loss: 0.04733968526124954\n",
      "Epoch 29323/30000 Training Loss: 0.024967245757579803\n",
      "Epoch 29324/30000 Training Loss: 0.05241069197654724\n",
      "Epoch 29325/30000 Training Loss: 0.037651427090168\n",
      "Epoch 29326/30000 Training Loss: 0.044153694063425064\n",
      "Epoch 29327/30000 Training Loss: 0.04431914538145065\n",
      "Epoch 29328/30000 Training Loss: 0.06055503711104393\n",
      "Epoch 29329/30000 Training Loss: 0.04385088384151459\n",
      "Epoch 29330/30000 Training Loss: 0.03924839198589325\n",
      "Epoch 29331/30000 Training Loss: 0.051013726741075516\n",
      "Epoch 29332/30000 Training Loss: 0.042872462421655655\n",
      "Epoch 29333/30000 Training Loss: 0.05202636122703552\n",
      "Epoch 29334/30000 Training Loss: 0.050103411078453064\n",
      "Epoch 29335/30000 Training Loss: 0.05308935046195984\n",
      "Epoch 29336/30000 Training Loss: 0.03782124072313309\n",
      "Epoch 29337/30000 Training Loss: 0.041767239570617676\n",
      "Epoch 29338/30000 Training Loss: 0.04294577240943909\n",
      "Epoch 29339/30000 Training Loss: 0.028034575283527374\n",
      "Epoch 29340/30000 Training Loss: 0.04427468776702881\n",
      "Epoch 29341/30000 Training Loss: 0.05217790603637695\n",
      "Epoch 29342/30000 Training Loss: 0.038867272436618805\n",
      "Epoch 29343/30000 Training Loss: 0.037857428193092346\n",
      "Epoch 29344/30000 Training Loss: 0.05245044082403183\n",
      "Epoch 29345/30000 Training Loss: 0.05343833193182945\n",
      "Epoch 29346/30000 Training Loss: 0.05617409572005272\n",
      "Epoch 29347/30000 Training Loss: 0.033430665731430054\n",
      "Epoch 29348/30000 Training Loss: 0.047204531729221344\n",
      "Epoch 29349/30000 Training Loss: 0.05125579237937927\n",
      "Epoch 29350/30000 Training Loss: 0.03270146995782852\n",
      "Epoch 29351/30000 Training Loss: 0.03999513015151024\n",
      "Epoch 29352/30000 Training Loss: 0.05019934102892876\n",
      "Epoch 29353/30000 Training Loss: 0.041081007570028305\n",
      "Epoch 29354/30000 Training Loss: 0.03756082057952881\n",
      "Epoch 29355/30000 Training Loss: 0.050351060926914215\n",
      "Epoch 29356/30000 Training Loss: 0.05390438437461853\n",
      "Epoch 29357/30000 Training Loss: 0.0428551509976387\n",
      "Epoch 29358/30000 Training Loss: 0.03700583428144455\n",
      "Epoch 29359/30000 Training Loss: 0.034956250339746475\n",
      "Epoch 29360/30000 Training Loss: 0.0519510917365551\n",
      "Epoch 29361/30000 Training Loss: 0.03423100709915161\n",
      "Epoch 29362/30000 Training Loss: 0.03978002816438675\n",
      "Epoch 29363/30000 Training Loss: 0.04162733629345894\n",
      "Epoch 29364/30000 Training Loss: 0.0493728406727314\n",
      "Epoch 29365/30000 Training Loss: 0.05028315633535385\n",
      "Epoch 29366/30000 Training Loss: 0.05040108039975166\n",
      "Epoch 29367/30000 Training Loss: 0.03674345463514328\n",
      "Epoch 29368/30000 Training Loss: 0.04213899001479149\n",
      "Epoch 29369/30000 Training Loss: 0.062262162566185\n",
      "Epoch 29370/30000 Training Loss: 0.026457125321030617\n",
      "Epoch 29371/30000 Training Loss: 0.03749489039182663\n",
      "Epoch 29372/30000 Training Loss: 0.04472737014293671\n",
      "Epoch 29373/30000 Training Loss: 0.04062533378601074\n",
      "Epoch 29374/30000 Training Loss: 0.047290585935115814\n",
      "Epoch 29375/30000 Training Loss: 0.036831632256507874\n",
      "Epoch 29376/30000 Training Loss: 0.05235963314771652\n",
      "Epoch 29377/30000 Training Loss: 0.03958427533507347\n",
      "Epoch 29378/30000 Training Loss: 0.03965801000595093\n",
      "Epoch 29379/30000 Training Loss: 0.0422341451048851\n",
      "Epoch 29380/30000 Training Loss: 0.05578026548027992\n",
      "Epoch 29381/30000 Training Loss: 0.03544280305504799\n",
      "Epoch 29382/30000 Training Loss: 0.03091142326593399\n",
      "Epoch 29383/30000 Training Loss: 0.041200146079063416\n",
      "Epoch 29384/30000 Training Loss: 0.04336123540997505\n",
      "Epoch 29385/30000 Training Loss: 0.04584847390651703\n",
      "Epoch 29386/30000 Training Loss: 0.04658247157931328\n",
      "Epoch 29387/30000 Training Loss: 0.04018084704875946\n",
      "Epoch 29388/30000 Training Loss: 0.043715398758649826\n",
      "Epoch 29389/30000 Training Loss: 0.06438648700714111\n",
      "Epoch 29390/30000 Training Loss: 0.03818334639072418\n",
      "Epoch 29391/30000 Training Loss: 0.04386921972036362\n",
      "Epoch 29392/30000 Training Loss: 0.047434255480766296\n",
      "Epoch 29393/30000 Training Loss: 0.037377022206783295\n",
      "Epoch 29394/30000 Training Loss: 0.04968702793121338\n",
      "Epoch 29395/30000 Training Loss: 0.03290150314569473\n",
      "Epoch 29396/30000 Training Loss: 0.06756940484046936\n",
      "Epoch 29397/30000 Training Loss: 0.03264452517032623\n",
      "Epoch 29398/30000 Training Loss: 0.04224708303809166\n",
      "Epoch 29399/30000 Training Loss: 0.03299996629357338\n",
      "Epoch 29400/30000 Training Loss: 0.047364380210638046\n",
      "Epoch 29400/30000 Validation Loss: 0.04050040990114212\n",
      "Epoch 29401/30000 Training Loss: 0.04954645037651062\n",
      "Epoch 29402/30000 Training Loss: 0.040134258568286896\n",
      "Epoch 29403/30000 Training Loss: 0.04056309536099434\n",
      "Epoch 29404/30000 Training Loss: 0.04742806404829025\n",
      "Epoch 29405/30000 Training Loss: 0.03419722616672516\n",
      "Epoch 29406/30000 Training Loss: 0.043897129595279694\n",
      "Epoch 29407/30000 Training Loss: 0.03783388435840607\n",
      "Epoch 29408/30000 Training Loss: 0.046723928302526474\n",
      "Epoch 29409/30000 Training Loss: 0.06342530250549316\n",
      "Epoch 29410/30000 Training Loss: 0.046105291694402695\n",
      "Epoch 29411/30000 Training Loss: 0.03953707963228226\n",
      "Epoch 29412/30000 Training Loss: 0.04089990258216858\n",
      "Epoch 29413/30000 Training Loss: 0.052625659853219986\n",
      "Epoch 29414/30000 Training Loss: 0.0413086861371994\n",
      "Epoch 29415/30000 Training Loss: 0.0424422062933445\n",
      "Epoch 29416/30000 Training Loss: 0.05678880214691162\n",
      "Epoch 29417/30000 Training Loss: 0.049697212874889374\n",
      "Epoch 29418/30000 Training Loss: 0.035435404628515244\n",
      "Epoch 29419/30000 Training Loss: 0.041098155081272125\n",
      "Epoch 29420/30000 Training Loss: 0.04208561033010483\n",
      "Epoch 29421/30000 Training Loss: 0.04693729802966118\n",
      "Epoch 29422/30000 Training Loss: 0.040056485682725906\n",
      "Epoch 29423/30000 Training Loss: 0.03513582795858383\n",
      "Epoch 29424/30000 Training Loss: 0.046712957322597504\n",
      "Epoch 29425/30000 Training Loss: 0.03734154254198074\n",
      "Epoch 29426/30000 Training Loss: 0.058033835142850876\n",
      "Epoch 29427/30000 Training Loss: 0.0422595776617527\n",
      "Epoch 29428/30000 Training Loss: 0.055559393018484116\n",
      "Epoch 29429/30000 Training Loss: 0.04637567698955536\n",
      "Epoch 29430/30000 Training Loss: 0.034091752022504807\n",
      "Epoch 29431/30000 Training Loss: 0.05362292379140854\n",
      "Epoch 29432/30000 Training Loss: 0.03462149575352669\n",
      "Epoch 29433/30000 Training Loss: 0.03747669607400894\n",
      "Epoch 29434/30000 Training Loss: 0.052201882004737854\n",
      "Epoch 29435/30000 Training Loss: 0.03878859430551529\n",
      "Epoch 29436/30000 Training Loss: 0.037237945944070816\n",
      "Epoch 29437/30000 Training Loss: 0.047958604991436005\n",
      "Epoch 29438/30000 Training Loss: 0.046641893684864044\n",
      "Epoch 29439/30000 Training Loss: 0.04733407869935036\n",
      "Epoch 29440/30000 Training Loss: 0.030327703803777695\n",
      "Epoch 29441/30000 Training Loss: 0.04513866454362869\n",
      "Epoch 29442/30000 Training Loss: 0.046455223113298416\n",
      "Epoch 29443/30000 Training Loss: 0.03902985155582428\n",
      "Epoch 29444/30000 Training Loss: 0.04555219039320946\n",
      "Epoch 29445/30000 Training Loss: 0.04674283415079117\n",
      "Epoch 29446/30000 Training Loss: 0.03225976973772049\n",
      "Epoch 29447/30000 Training Loss: 0.043580561876297\n",
      "Epoch 29448/30000 Training Loss: 0.04529400169849396\n",
      "Epoch 29449/30000 Training Loss: 0.06318575143814087\n",
      "Epoch 29450/30000 Training Loss: 0.03771800920367241\n",
      "Epoch 29451/30000 Training Loss: 0.03356940671801567\n",
      "Epoch 29452/30000 Training Loss: 0.04406847432255745\n",
      "Epoch 29453/30000 Training Loss: 0.05257360637187958\n",
      "Epoch 29454/30000 Training Loss: 0.03359462693333626\n",
      "Epoch 29455/30000 Training Loss: 0.053806327283382416\n",
      "Epoch 29456/30000 Training Loss: 0.04131564497947693\n",
      "Epoch 29457/30000 Training Loss: 0.03466106578707695\n",
      "Epoch 29458/30000 Training Loss: 0.045443907380104065\n",
      "Epoch 29459/30000 Training Loss: 0.05391059070825577\n",
      "Epoch 29460/30000 Training Loss: 0.042073704302310944\n",
      "Epoch 29461/30000 Training Loss: 0.02646779455244541\n",
      "Epoch 29462/30000 Training Loss: 0.040178436785936356\n",
      "Epoch 29463/30000 Training Loss: 0.05468258261680603\n",
      "Epoch 29464/30000 Training Loss: 0.04349392279982567\n",
      "Epoch 29465/30000 Training Loss: 0.04748821258544922\n",
      "Epoch 29466/30000 Training Loss: 0.03625581040978432\n",
      "Epoch 29467/30000 Training Loss: 0.04153183102607727\n",
      "Epoch 29468/30000 Training Loss: 0.04021601378917694\n",
      "Epoch 29469/30000 Training Loss: 0.035919614136219025\n",
      "Epoch 29470/30000 Training Loss: 0.049741748720407486\n",
      "Epoch 29471/30000 Training Loss: 0.05067834258079529\n",
      "Epoch 29472/30000 Training Loss: 0.044111378490924835\n",
      "Epoch 29473/30000 Training Loss: 0.03603515774011612\n",
      "Epoch 29474/30000 Training Loss: 0.03969854116439819\n",
      "Epoch 29475/30000 Training Loss: 0.03669378161430359\n",
      "Epoch 29476/30000 Training Loss: 0.041162289679050446\n",
      "Epoch 29477/30000 Training Loss: 0.04792970046401024\n",
      "Epoch 29478/30000 Training Loss: 0.03996086120605469\n",
      "Epoch 29479/30000 Training Loss: 0.056017644703388214\n",
      "Epoch 29480/30000 Training Loss: 0.0391496866941452\n",
      "Epoch 29481/30000 Training Loss: 0.05320703238248825\n",
      "Epoch 29482/30000 Training Loss: 0.04342447221279144\n",
      "Epoch 29483/30000 Training Loss: 0.032727304846048355\n",
      "Epoch 29484/30000 Training Loss: 0.03062639757990837\n",
      "Epoch 29485/30000 Training Loss: 0.03240292891860008\n",
      "Epoch 29486/30000 Training Loss: 0.03587152808904648\n",
      "Epoch 29487/30000 Training Loss: 0.04945950210094452\n",
      "Epoch 29488/30000 Training Loss: 0.04270287603139877\n",
      "Epoch 29489/30000 Training Loss: 0.031645309180021286\n",
      "Epoch 29490/30000 Training Loss: 0.04210996255278587\n",
      "Epoch 29491/30000 Training Loss: 0.043974943459033966\n",
      "Epoch 29492/30000 Training Loss: 0.05159986391663551\n",
      "Epoch 29493/30000 Training Loss: 0.03862569481134415\n",
      "Epoch 29494/30000 Training Loss: 0.04665631800889969\n",
      "Epoch 29495/30000 Training Loss: 0.054204076528549194\n",
      "Epoch 29496/30000 Training Loss: 0.0436771884560585\n",
      "Epoch 29497/30000 Training Loss: 0.036784712225198746\n",
      "Epoch 29498/30000 Training Loss: 0.04123632237315178\n",
      "Epoch 29499/30000 Training Loss: 0.054622288793325424\n",
      "Epoch 29500/30000 Training Loss: 0.05101698264479637\n",
      "Epoch 29500/30000 Validation Loss: 0.058441683650016785\n",
      "Epoch 29501/30000 Training Loss: 0.04141372814774513\n",
      "Epoch 29502/30000 Training Loss: 0.05239114165306091\n",
      "Epoch 29503/30000 Training Loss: 0.028196318075060844\n",
      "Epoch 29504/30000 Training Loss: 0.04086758941411972\n",
      "Epoch 29505/30000 Training Loss: 0.04090435057878494\n",
      "Epoch 29506/30000 Training Loss: 0.031313467770814896\n",
      "Epoch 29507/30000 Training Loss: 0.04085296019911766\n",
      "Epoch 29508/30000 Training Loss: 0.039776697754859924\n",
      "Epoch 29509/30000 Training Loss: 0.0512332022190094\n",
      "Epoch 29510/30000 Training Loss: 0.05645985156297684\n",
      "Epoch 29511/30000 Training Loss: 0.04221685975790024\n",
      "Epoch 29512/30000 Training Loss: 0.0390138104557991\n",
      "Epoch 29513/30000 Training Loss: 0.04424329847097397\n",
      "Epoch 29514/30000 Training Loss: 0.03637496009469032\n",
      "Epoch 29515/30000 Training Loss: 0.04596659541130066\n",
      "Epoch 29516/30000 Training Loss: 0.04292606562376022\n",
      "Epoch 29517/30000 Training Loss: 0.03898153081536293\n",
      "Epoch 29518/30000 Training Loss: 0.04026654735207558\n",
      "Epoch 29519/30000 Training Loss: 0.0625476986169815\n",
      "Epoch 29520/30000 Training Loss: 0.029621431604027748\n",
      "Epoch 29521/30000 Training Loss: 0.038913290947675705\n",
      "Epoch 29522/30000 Training Loss: 0.0404290109872818\n",
      "Epoch 29523/30000 Training Loss: 0.038366906344890594\n",
      "Epoch 29524/30000 Training Loss: 0.04218161851167679\n",
      "Epoch 29525/30000 Training Loss: 0.03552419692277908\n",
      "Epoch 29526/30000 Training Loss: 0.047651637345552444\n",
      "Epoch 29527/30000 Training Loss: 0.03704855591058731\n",
      "Epoch 29528/30000 Training Loss: 0.03864274173974991\n",
      "Epoch 29529/30000 Training Loss: 0.03144277259707451\n",
      "Epoch 29530/30000 Training Loss: 0.04933450371026993\n",
      "Epoch 29531/30000 Training Loss: 0.04171139374375343\n",
      "Epoch 29532/30000 Training Loss: 0.03971388190984726\n",
      "Epoch 29533/30000 Training Loss: 0.039021871984004974\n",
      "Epoch 29534/30000 Training Loss: 0.045473963022232056\n",
      "Epoch 29535/30000 Training Loss: 0.040836770087480545\n",
      "Epoch 29536/30000 Training Loss: 0.033731698989868164\n",
      "Epoch 29537/30000 Training Loss: 0.039037156850099564\n",
      "Epoch 29538/30000 Training Loss: 0.039421841502189636\n",
      "Epoch 29539/30000 Training Loss: 0.03407309576869011\n",
      "Epoch 29540/30000 Training Loss: 0.04078495875000954\n",
      "Epoch 29541/30000 Training Loss: 0.04437009617686272\n",
      "Epoch 29542/30000 Training Loss: 0.04193691164255142\n",
      "Epoch 29543/30000 Training Loss: 0.0564066544175148\n",
      "Epoch 29544/30000 Training Loss: 0.05014435201883316\n",
      "Epoch 29545/30000 Training Loss: 0.03646538406610489\n",
      "Epoch 29546/30000 Training Loss: 0.03397691249847412\n",
      "Epoch 29547/30000 Training Loss: 0.05405859276652336\n",
      "Epoch 29548/30000 Training Loss: 0.03178776800632477\n",
      "Epoch 29549/30000 Training Loss: 0.05338180437684059\n",
      "Epoch 29550/30000 Training Loss: 0.03779035806655884\n",
      "Epoch 29551/30000 Training Loss: 0.030447281897068024\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "        eval_data=val,\n",
    "        eval_interval=eval_interval,\n",
    "        warm_up_steps=500,\n",
    "        use_llrd=True,\n",
    "        llrd_rate=0.99\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d\")\n",
    "# best_model_fp = f'models/0221/model_best_epoch_20600_min_val_loss_0.028699999675154686.pkl'\n",
    "# with open(best_model_fp, 'rb') as handle:\n",
    "#     best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        2000,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref = sampling(model, \n",
    "                                                           diffusion, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184761",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80683b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
