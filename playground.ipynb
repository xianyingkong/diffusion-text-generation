{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a01d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_arch.run_train import create_model_and_diffusion\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from model_arch.train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from model_arch.tokenizer import load_tokenizer, load_model_emb\n",
    "from model_arch.sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=20\n",
    "microbatch=10\n",
    "epochs=30_000\n",
    "eval_interval=100\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=2000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=102\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data_dir='data'\n",
    "data_player_dir='data/with_player'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer('shakespeare', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30268, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30268"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['o hell! what have we here? a carrion death, within whose empty eye there is a written scroll!', 'and his disciples only envy at, ye blew the fire that burns ye now have at ye! enter king,'] [\"i'll read the writing. all that glitters is not gold, often have you heard that told\", 'frowning on them, takes his seat']\n",
      "RAM used: 672.36 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "RAM used: 691.26 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091c85c8ca854849bf7efb9d7d993ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "### tokenized_datasets...example [2, 37, 1300, 6, 164, 150, 133, 237, 22, 23, 7135, 432, 10, 906, 569, 3066, 756, 210, 121, 23, 4180, 7422, 6, 3]\n",
      "RAM used: 734.46 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c93d6a97b94393bc246f30874692a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 774.05 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32db0deaead64f2bb028572340804f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 48627\n",
      "}) padded dataset\n",
      "RAM used: 868.59 MB\n",
      "RAM used: 868.59 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"petruchio is my name, antonio's son, a man well known throughout all italy.\", 'the matter is to me, sir, as concerning jaquenetta. the manner of it is,'] ['i know him well you are welcome for his sake.', 'i was taken with the manner.']\n",
      "RAM used: 829.69 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "RAM used: 829.69 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bdedcb1c394547843a4a35e1c2e1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "### tokenized_datasets...example [2, 3886, 121, 105, 520, 10, 2546, 9, 41, 478, 10, 23, 211, 254, 1233, 9840, 187, 4043, 12, 3]\n",
      "RAM used: 842.63 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc386cc67abd4255aa9344b0b0b7692c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 851.73 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c06bf55db34217b42a4d4bd52b1499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 12147\n",
      "}) padded dataset\n",
      "RAM used: 874.86 MB\n",
      "RAM used: 874.86 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(30268, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=30268, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )\n",
    "\n",
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192508"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 0.0001\n",
      "name: lm_head.bias, lr: 0.0001\n",
      "name: time_embed.0.weight, lr: 0.0001\n",
      "name: time_embed.0.bias, lr: 0.0001\n",
      "name: time_embed.2.weight, lr: 0.0001\n",
      "name: time_embed.2.bias, lr: 0.0001\n",
      "name: input_up_proj.0.weight, lr: 0.0001\n",
      "name: input_up_proj.0.bias, lr: 0.0001\n",
      "name: input_up_proj.2.weight, lr: 0.0001\n",
      "name: input_up_proj.2.bias, lr: 0.0001\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 0.00010101010101010101\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 0.00010203040506070809\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 0.00010306101521283645\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 0.00010410203556852167\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 0.00010515357128133502\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 0.00010621572856700508\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 0.00010728861471414655\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 0.00010837233809509753\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.0001094670081768662\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00011057273553218808\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.00011168963185069502\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.000112817809950197\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.000112817809950197\n",
      "name: position_embeddings.weight, lr: 0.00011395738378807778\n",
      "name: LayerNorm.weight, lr: 0.00011395738378807778\n",
      "name: LayerNorm.bias, lr: 0.00011395738378807778\n",
      "name: output_down_proj.0.weight, lr: 0.00011395738378807778\n",
      "name: output_down_proj.0.bias, lr: 0.00011395738378807778\n",
      "name: output_down_proj.2.weight, lr: 0.00011395738378807778\n",
      "name: output_down_proj.2.bias, lr: 0.00011395738378807778\n",
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30000 Training Loss: 1.1961716413497925\n",
      "Epoch 2/30000 Training Loss: 1.1991360187530518\n",
      "Epoch 3/30000 Training Loss: 1.1946220397949219\n",
      "Epoch 4/30000 Training Loss: 1.1927778720855713\n",
      "Epoch 5/30000 Training Loss: 1.1906840801239014\n",
      "Epoch 6/30000 Training Loss: 1.1799726486206055\n",
      "Epoch 7/30000 Training Loss: 1.1712337732315063\n",
      "Epoch 8/30000 Training Loss: 1.164445400238037\n",
      "Epoch 9/30000 Training Loss: 1.1491776704788208\n",
      "Epoch 10/30000 Training Loss: 1.1362547874450684\n",
      "Epoch 11/30000 Training Loss: 1.1196528673171997\n",
      "Epoch 12/30000 Training Loss: 1.1070992946624756\n",
      "Epoch 13/30000 Training Loss: 1.0855627059936523\n",
      "Epoch 14/30000 Training Loss: 1.0640943050384521\n",
      "Epoch 15/30000 Training Loss: 1.0497028827667236\n",
      "Epoch 16/30000 Training Loss: 1.0301544666290283\n",
      "Epoch 17/30000 Training Loss: 1.003342866897583\n",
      "Epoch 18/30000 Training Loss: 0.9832772016525269\n",
      "Epoch 19/30000 Training Loss: 0.9728710651397705\n",
      "Epoch 20/30000 Training Loss: 0.9377175569534302\n",
      "Epoch 21/30000 Training Loss: 0.9060680866241455\n",
      "Epoch 22/30000 Training Loss: 0.895117998123169\n",
      "Epoch 23/30000 Training Loss: 0.8648985028266907\n",
      "Epoch 24/30000 Training Loss: 0.846030592918396\n",
      "Epoch 25/30000 Training Loss: 0.8302050828933716\n",
      "Epoch 26/30000 Training Loss: 0.789467453956604\n",
      "Epoch 27/30000 Training Loss: 0.7698707580566406\n",
      "Epoch 28/30000 Training Loss: 0.7596524953842163\n",
      "Epoch 29/30000 Training Loss: 0.7392066717147827\n",
      "Epoch 30/30000 Training Loss: 0.7326590418815613\n",
      "Epoch 31/30000 Training Loss: 0.7258894443511963\n",
      "Epoch 32/30000 Training Loss: 0.6889787912368774\n",
      "Epoch 33/30000 Training Loss: 0.654184103012085\n",
      "Epoch 34/30000 Training Loss: 0.6751314997673035\n",
      "Epoch 35/30000 Training Loss: 0.6558437347412109\n",
      "Epoch 36/30000 Training Loss: 0.6419634819030762\n",
      "Epoch 37/30000 Training Loss: 0.6202107667922974\n",
      "Epoch 38/30000 Training Loss: 0.6124340891838074\n",
      "Epoch 39/30000 Training Loss: 0.6280014514923096\n",
      "Epoch 40/30000 Training Loss: 0.6372414827346802\n",
      "Epoch 41/30000 Training Loss: 0.610385537147522\n",
      "Epoch 42/30000 Training Loss: 0.5685400366783142\n",
      "Epoch 43/30000 Training Loss: 0.5810267329216003\n",
      "Epoch 44/30000 Training Loss: 0.5677565336227417\n",
      "Epoch 45/30000 Training Loss: 0.5858572721481323\n",
      "Epoch 46/30000 Training Loss: 0.5801641345024109\n",
      "Epoch 47/30000 Training Loss: 0.5551271438598633\n",
      "Epoch 48/30000 Training Loss: 0.56634920835495\n",
      "Epoch 49/30000 Training Loss: 0.5496550798416138\n",
      "Epoch 50/30000 Training Loss: 0.5707198977470398\n",
      "Epoch 51/30000 Training Loss: 0.5720922350883484\n",
      "Epoch 52/30000 Training Loss: 0.5479259490966797\n",
      "Epoch 53/30000 Training Loss: 0.5557796359062195\n",
      "Epoch 54/30000 Training Loss: 0.541002631187439\n",
      "Epoch 55/30000 Training Loss: 0.5265023708343506\n",
      "Epoch 56/30000 Training Loss: 0.5470620393753052\n",
      "Epoch 57/30000 Training Loss: 0.5292007923126221\n",
      "Epoch 58/30000 Training Loss: 0.5450721979141235\n",
      "Epoch 59/30000 Training Loss: 0.5599152445793152\n",
      "Epoch 60/30000 Training Loss: 0.5341328978538513\n",
      "Epoch 61/30000 Training Loss: 0.5430200099945068\n",
      "Epoch 62/30000 Training Loss: 0.5333026647567749\n",
      "Epoch 63/30000 Training Loss: 0.5006410479545593\n",
      "Epoch 64/30000 Training Loss: 0.5390004515647888\n",
      "Epoch 65/30000 Training Loss: 0.5541539192199707\n",
      "Epoch 66/30000 Training Loss: 0.5163496732711792\n",
      "Epoch 67/30000 Training Loss: 0.5147419571876526\n",
      "Epoch 68/30000 Training Loss: 0.5250378847122192\n",
      "Epoch 69/30000 Training Loss: 0.5616581439971924\n",
      "Epoch 70/30000 Training Loss: 0.5174543857574463\n",
      "Epoch 71/30000 Training Loss: 0.49994099140167236\n",
      "Epoch 72/30000 Training Loss: 0.554364800453186\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "        eval_data=val,\n",
    "        eval_interval=eval_interval,\n",
    "        warm_up_steps=500,\n",
    "        use_llrd=True,\n",
    "        llrd_rate=0.99\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d\")\n",
    "# best_model_fp = f'models/0221/model_best_epoch_20600_min_val_loss_0.028699999675154686.pkl'\n",
    "# with open(best_model_fp, 'rb') as handle:\n",
    "#     best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        2000,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref = sampling(model, \n",
    "                                                           diffusion, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184761",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80683b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
