{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_train import create_model_and_diffusion\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from tokenizer import load_tokenizer, load_model_emb\n",
    "from sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch, os\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "batch_size=10\n",
    "microbatch=5\n",
    "epochs=20_000\n",
    "eval_interval=100\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=1_200\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=102\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_data_dir='data/commonsense'\n",
    "ss_data_dir='data/shakespeare'\n",
    "ss_small_data_dir='data/mini-shakespeare'\n",
    "combined_data_dir='data/combined'\n",
    "combined_small_data_dir='data/combined/small'\n",
    "regular_data_dir='data'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=regular_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer('shakespeare_plays', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30267, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30267"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['o hell! what have we here? a carrion death, within whose empty eye there is a written scroll!', 'and his disciples only envy at, ye blew the fire that burns ye now have at ye! enter king,'] [\"i'll read the writing. all that glitters is not gold, often have you heard that told\", 'frowning on them, takes his seat']\n",
      "RAM used: 2838.91 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "RAM used: 2868.57 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee909d1a930442d1ae264db3676003a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 48627\n",
      "})\n",
      "### tokenized_datasets...example [2, 36, 1299, 5, 163, 149, 132, 236, 21, 22, 7134, 431, 9, 905, 568, 3065, 755, 209, 120, 22, 4179, 7421, 5, 3]\n",
      "RAM used: 2917.21 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0640e10537304417a49d3f4678b5b7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 2947.92 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acc622f2fe4481b88b2ba7e5eb22172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/48627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 48627\n",
      "}) padded dataset\n",
      "RAM used: 3036.20 MB\n",
      "RAM used: 3036.20 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " [\"petruchio is my name, antonio's son, a man well known throughout all italy.\", 'the matter is to me, sir, as concerning jaquenetta. the manner of it is,'] ['i know him well you are welcome for his sake.', 'i was taken with the manner.']\n",
      "RAM used: 2999.45 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "RAM used: 2999.74 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29180c4f88d44b61a7d38c2cbb58d25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 12147\n",
      "})\n",
      "### tokenized_datasets...example [2, 3885, 120, 104, 519, 9, 2545, 8, 40, 477, 9, 22, 210, 253, 1232, 9839, 186, 4042, 11, 3]\n",
      "RAM used: 3007.48 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5278fcebc70b4e7eaa06a2d66e3b11c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 3019.97 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aae0055fe84c81933e5063a5a30c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/12147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 12147\n",
      "}) padded dataset\n",
      "RAM used: 3040.57 MB\n",
      "RAM used: 3040.57 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a49540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNetModel(\n",
       "  (word_embedding): Embedding(30267, 128)\n",
       "  (lm_head): Linear(in_features=128, out_features=30267, bias=True)\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_up_proj): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (input_transformers): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (output_down_proj): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=768, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, diffusion = create_model_and_diffusion(\n",
    "                        hidden_t_dim,\n",
    "                        hidden_dim,\n",
    "                        vocab_size,\n",
    "                        config_name,\n",
    "                        use_plm_init,\n",
    "                        dropout,\n",
    "                        diffusion_steps,\n",
    "                        noise_schedule,\n",
    "                        predict_xstart,\n",
    "                        rescale_timesteps,\n",
    "                    )\n",
    "\n",
    "model.to(dist_util.dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192379"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n",
      "Epoch 1/20000 Training Loss: 1.043076992034912\n",
      "Epoch 2/20000 Training Loss: 0.695614218711853\n",
      "Epoch 3/20000 Training Loss: 0.5766768455505371\n",
      "Epoch 4/20000 Training Loss: 0.5150974988937378\n",
      "Epoch 5/20000 Training Loss: 0.5483779907226562\n",
      "Epoch 6/20000 Training Loss: 0.47229117155075073\n",
      "Epoch 7/20000 Training Loss: 0.5219022035598755\n",
      "Epoch 8/20000 Training Loss: 0.5533789396286011\n",
      "Epoch 9/20000 Training Loss: 0.45337945222854614\n",
      "Epoch 10/20000 Training Loss: 0.49771273136138916\n",
      "Epoch 11/20000 Training Loss: 0.5337638854980469\n",
      "Epoch 12/20000 Training Loss: 0.5432272553443909\n",
      "Epoch 13/20000 Training Loss: 0.4817114472389221\n",
      "Epoch 14/20000 Training Loss: 0.5254583358764648\n",
      "Epoch 15/20000 Training Loss: 0.4739689230918884\n",
      "Epoch 16/20000 Training Loss: 0.4796767830848694\n",
      "Epoch 17/20000 Training Loss: 0.45504820346832275\n",
      "Epoch 18/20000 Training Loss: 0.4943806529045105\n",
      "Epoch 19/20000 Training Loss: 0.5340105891227722\n",
      "Epoch 20/20000 Training Loss: 0.4675132632255554\n",
      "Epoch 21/20000 Training Loss: 0.45638924837112427\n",
      "Epoch 22/20000 Training Loss: 0.5071524381637573\n",
      "Epoch 23/20000 Training Loss: 0.563582181930542\n",
      "Epoch 24/20000 Training Loss: 0.42038050293922424\n",
      "Epoch 25/20000 Training Loss: 0.5040289163589478\n",
      "Epoch 26/20000 Training Loss: 0.41777896881103516\n",
      "Epoch 27/20000 Training Loss: 0.532884418964386\n",
      "Epoch 28/20000 Training Loss: 0.42074882984161377\n",
      "Epoch 29/20000 Training Loss: 0.5273959636688232\n",
      "Epoch 30/20000 Training Loss: 0.47759369015693665\n",
      "Epoch 31/20000 Training Loss: 0.4999443590641022\n",
      "Epoch 32/20000 Training Loss: 0.5169076323509216\n",
      "Epoch 33/20000 Training Loss: 0.4375876188278198\n",
      "Epoch 34/20000 Training Loss: 0.5206550359725952\n",
      "Epoch 35/20000 Training Loss: 0.5002936124801636\n",
      "Epoch 36/20000 Training Loss: 0.5095261335372925\n",
      "Epoch 37/20000 Training Loss: 0.5084986090660095\n",
      "Epoch 38/20000 Training Loss: 0.5354944467544556\n",
      "Epoch 39/20000 Training Loss: 0.4807282090187073\n",
      "Epoch 40/20000 Training Loss: 0.5486575961112976\n",
      "Epoch 41/20000 Training Loss: 0.5344460010528564\n",
      "Epoch 42/20000 Training Loss: 0.46719491481781006\n",
      "Epoch 43/20000 Training Loss: 0.5300099849700928\n",
      "Epoch 44/20000 Training Loss: 0.5217148661613464\n",
      "Epoch 45/20000 Training Loss: 0.5375014543533325\n",
      "Epoch 46/20000 Training Loss: 0.4609178900718689\n",
      "Epoch 47/20000 Training Loss: 0.4985337257385254\n",
      "Epoch 48/20000 Training Loss: 0.5134786367416382\n",
      "Epoch 49/20000 Training Loss: 0.5750330686569214\n",
      "Epoch 50/20000 Training Loss: 0.47219526767730713\n",
      "Epoch 51/20000 Training Loss: 0.4778504967689514\n",
      "Epoch 52/20000 Training Loss: 0.488092839717865\n",
      "Epoch 53/20000 Training Loss: 0.5224374532699585\n",
      "Epoch 54/20000 Training Loss: 0.4338480234146118\n",
      "Epoch 55/20000 Training Loss: 0.5026451349258423\n",
      "Epoch 56/20000 Training Loss: 0.5014674663543701\n",
      "Epoch 57/20000 Training Loss: 0.4765205979347229\n",
      "Epoch 58/20000 Training Loss: 0.49431130290031433\n",
      "Epoch 59/20000 Training Loss: 0.5050399303436279\n",
      "Epoch 60/20000 Training Loss: 0.5265695452690125\n",
      "Epoch 61/20000 Training Loss: 0.5507595539093018\n",
      "Epoch 62/20000 Training Loss: 0.523828387260437\n",
      "Epoch 63/20000 Training Loss: 0.467781126499176\n",
      "Epoch 64/20000 Training Loss: 0.4453009366989136\n",
      "Epoch 65/20000 Training Loss: 0.3759339153766632\n",
      "Epoch 66/20000 Training Loss: 0.34404271841049194\n",
      "Epoch 67/20000 Training Loss: 0.38417893648147583\n",
      "Epoch 68/20000 Training Loss: 0.3347877264022827\n",
      "Epoch 69/20000 Training Loss: 0.35901904106140137\n",
      "Epoch 70/20000 Training Loss: 0.3158324360847473\n",
      "Epoch 71/20000 Training Loss: 0.38841816782951355\n",
      "Epoch 72/20000 Training Loss: 0.2652941048145294\n",
      "Epoch 73/20000 Training Loss: 0.3116791248321533\n",
      "Epoch 74/20000 Training Loss: 0.3241719603538513\n",
      "Epoch 75/20000 Training Loss: 0.31646794080734253\n",
      "Epoch 76/20000 Training Loss: 0.3337465524673462\n",
      "Epoch 77/20000 Training Loss: 0.336321622133255\n",
      "Epoch 78/20000 Training Loss: 0.3369356393814087\n",
      "Epoch 79/20000 Training Loss: 0.38292384147644043\n",
      "Epoch 80/20000 Training Loss: 0.3298701047897339\n",
      "Epoch 81/20000 Training Loss: 0.34135642647743225\n",
      "Epoch 82/20000 Training Loss: 0.30990976095199585\n",
      "Epoch 83/20000 Training Loss: 0.32415053248405457\n",
      "Epoch 84/20000 Training Loss: 0.2911483645439148\n",
      "Epoch 85/20000 Training Loss: 0.2746444046497345\n",
      "Epoch 86/20000 Training Loss: 0.36221036314964294\n",
      "Epoch 87/20000 Training Loss: 0.28785985708236694\n",
      "Epoch 88/20000 Training Loss: 0.3262495696544647\n",
      "Epoch 89/20000 Training Loss: 0.3439338207244873\n",
      "Epoch 90/20000 Training Loss: 0.33937960863113403\n",
      "Epoch 91/20000 Training Loss: 0.3404632806777954\n",
      "Epoch 92/20000 Training Loss: 0.31798648834228516\n",
      "Epoch 93/20000 Training Loss: 0.3106154203414917\n",
      "Epoch 94/20000 Training Loss: 0.3181154131889343\n",
      "Epoch 95/20000 Training Loss: 0.31343141198158264\n",
      "Epoch 96/20000 Training Loss: 0.3273358941078186\n",
      "Epoch 97/20000 Training Loss: 0.3000141680240631\n",
      "Epoch 98/20000 Training Loss: 0.29984012246131897\n",
      "Epoch 99/20000 Training Loss: 0.34941795468330383\n",
      "Epoch 100/20000 Training Loss: 0.3155674934387207\n",
      "Epoch 100/20000 Validation Loss: 0.3393650949001312\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.3393650949001312<=============\n",
      "Epoch 101/20000 Training Loss: 0.37449097633361816\n",
      "Epoch 102/20000 Training Loss: 0.31558167934417725\n",
      "Epoch 103/20000 Training Loss: 0.3242030739784241\n",
      "Epoch 104/20000 Training Loss: 0.32814931869506836\n",
      "Epoch 105/20000 Training Loss: 0.3838657736778259\n",
      "Epoch 106/20000 Training Loss: 0.30706632137298584\n",
      "Epoch 107/20000 Training Loss: 0.33516576886177063\n",
      "Epoch 108/20000 Training Loss: 0.33153384923934937\n",
      "Epoch 109/20000 Training Loss: 0.324340283870697\n",
      "Epoch 110/20000 Training Loss: 0.2863415777683258\n",
      "Epoch 111/20000 Training Loss: 0.3125380873680115\n",
      "Epoch 112/20000 Training Loss: 0.32461798191070557\n",
      "Epoch 113/20000 Training Loss: 0.31305232644081116\n",
      "Epoch 114/20000 Training Loss: 0.292965292930603\n",
      "Epoch 115/20000 Training Loss: 0.316499799489975\n",
      "Epoch 116/20000 Training Loss: 0.33691704273223877\n",
      "Epoch 117/20000 Training Loss: 0.3327319622039795\n",
      "Epoch 118/20000 Training Loss: 0.35165512561798096\n",
      "Epoch 119/20000 Training Loss: 0.30864018201828003\n",
      "Epoch 120/20000 Training Loss: 0.3489055931568146\n",
      "Epoch 121/20000 Training Loss: 0.3064345419406891\n",
      "Epoch 122/20000 Training Loss: 0.33623427152633667\n",
      "Epoch 123/20000 Training Loss: 0.295104444026947\n",
      "Epoch 124/20000 Training Loss: 0.33597642183303833\n",
      "Epoch 125/20000 Training Loss: 0.3030010163784027\n",
      "Epoch 126/20000 Training Loss: 0.26479634642601013\n",
      "Epoch 127/20000 Training Loss: 0.3229143023490906\n",
      "Epoch 128/20000 Training Loss: 0.3051806688308716\n",
      "Epoch 129/20000 Training Loss: 0.343197226524353\n",
      "Epoch 130/20000 Training Loss: 0.3315066695213318\n",
      "Epoch 131/20000 Training Loss: 0.2963266372680664\n",
      "Epoch 132/20000 Training Loss: 0.3191724121570587\n",
      "Epoch 133/20000 Training Loss: 0.3124625086784363\n",
      "Epoch 134/20000 Training Loss: 0.3071245551109314\n",
      "Epoch 135/20000 Training Loss: 0.3384155035018921\n",
      "Epoch 136/20000 Training Loss: 0.28589269518852234\n",
      "Epoch 137/20000 Training Loss: 0.3657659888267517\n",
      "Epoch 138/20000 Training Loss: 0.3057065010070801\n",
      "Epoch 139/20000 Training Loss: 0.2811982035636902\n",
      "Epoch 140/20000 Training Loss: 0.3159181475639343\n",
      "Epoch 141/20000 Training Loss: 0.25846680998802185\n",
      "Epoch 142/20000 Training Loss: 0.312004029750824\n",
      "Epoch 143/20000 Training Loss: 0.32023850083351135\n",
      "Epoch 144/20000 Training Loss: 0.32725197076797485\n",
      "Epoch 145/20000 Training Loss: 0.28947943449020386\n",
      "Epoch 146/20000 Training Loss: 0.28547871112823486\n",
      "Epoch 147/20000 Training Loss: 0.33215516805648804\n",
      "Epoch 148/20000 Training Loss: 0.3110365867614746\n",
      "Epoch 149/20000 Training Loss: 0.2951440215110779\n",
      "Epoch 150/20000 Training Loss: 0.28053826093673706\n",
      "Epoch 151/20000 Training Loss: 0.31233519315719604\n",
      "Epoch 152/20000 Training Loss: 0.3069029450416565\n",
      "Epoch 153/20000 Training Loss: 0.3133314251899719\n",
      "Epoch 154/20000 Training Loss: 0.29550570249557495\n",
      "Epoch 155/20000 Training Loss: 0.3000866770744324\n",
      "Epoch 156/20000 Training Loss: 0.3103352189064026\n",
      "Epoch 157/20000 Training Loss: 0.32258516550064087\n",
      "Epoch 158/20000 Training Loss: 0.3265882432460785\n",
      "Epoch 159/20000 Training Loss: 0.332148015499115\n",
      "Epoch 160/20000 Training Loss: 0.322540283203125\n",
      "Epoch 161/20000 Training Loss: 0.3344479203224182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/20000 Training Loss: 0.30627384781837463\n",
      "Epoch 163/20000 Training Loss: 0.3177127540111542\n",
      "Epoch 164/20000 Training Loss: 0.31958019733428955\n",
      "Epoch 165/20000 Training Loss: 0.3083963394165039\n",
      "Epoch 166/20000 Training Loss: 0.2914397716522217\n",
      "Epoch 167/20000 Training Loss: 0.3150452971458435\n",
      "Epoch 168/20000 Training Loss: 0.28211510181427\n",
      "Epoch 169/20000 Training Loss: 0.3398022949695587\n",
      "Epoch 170/20000 Training Loss: 0.2901287078857422\n",
      "Epoch 171/20000 Training Loss: 0.3351472318172455\n",
      "Epoch 172/20000 Training Loss: 0.303388386964798\n",
      "Epoch 173/20000 Training Loss: 0.30401650071144104\n",
      "Epoch 174/20000 Training Loss: 0.32007336616516113\n",
      "Epoch 175/20000 Training Loss: 0.27891016006469727\n",
      "Epoch 176/20000 Training Loss: 0.27343934774398804\n",
      "Epoch 177/20000 Training Loss: 0.30770373344421387\n",
      "Epoch 178/20000 Training Loss: 0.329143226146698\n",
      "Epoch 179/20000 Training Loss: 0.30214378237724304\n",
      "Epoch 180/20000 Training Loss: 0.29184016585350037\n",
      "Epoch 181/20000 Training Loss: 0.3215368390083313\n",
      "Epoch 182/20000 Training Loss: 0.2929041087627411\n",
      "Epoch 183/20000 Training Loss: 0.3338717222213745\n",
      "Epoch 184/20000 Training Loss: 0.29751884937286377\n",
      "Epoch 185/20000 Training Loss: 0.3360093832015991\n",
      "Epoch 186/20000 Training Loss: 0.3068125247955322\n",
      "Epoch 187/20000 Training Loss: 0.34302935004234314\n",
      "Epoch 188/20000 Training Loss: 0.25386127829551697\n",
      "Epoch 189/20000 Training Loss: 0.324163556098938\n",
      "Epoch 190/20000 Training Loss: 0.3534185290336609\n",
      "Epoch 191/20000 Training Loss: 0.30220067501068115\n",
      "Epoch 192/20000 Training Loss: 0.2950659394264221\n",
      "Epoch 193/20000 Training Loss: 0.31776106357574463\n",
      "Epoch 194/20000 Training Loss: 0.2733159065246582\n",
      "Epoch 195/20000 Training Loss: 0.31305620074272156\n",
      "Epoch 196/20000 Training Loss: 0.2843979001045227\n",
      "Epoch 197/20000 Training Loss: 0.32006222009658813\n",
      "Epoch 198/20000 Training Loss: 0.2780669331550598\n",
      "Epoch 199/20000 Training Loss: 0.2983289659023285\n",
      "Epoch 200/20000 Training Loss: 0.3017347753047943\n",
      "Epoch 200/20000 Validation Loss: 0.3255397081375122\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.3255397081375122<=============\n",
      "Epoch 201/20000 Training Loss: 0.2968442440032959\n",
      "Epoch 202/20000 Training Loss: 0.28518879413604736\n",
      "Epoch 203/20000 Training Loss: 0.2956094741821289\n",
      "Epoch 204/20000 Training Loss: 0.30671846866607666\n",
      "Epoch 205/20000 Training Loss: 0.27164679765701294\n",
      "Epoch 206/20000 Training Loss: 0.3029700517654419\n",
      "Epoch 207/20000 Training Loss: 0.32761165499687195\n",
      "Epoch 208/20000 Training Loss: 0.28502407670021057\n",
      "Epoch 209/20000 Training Loss: 0.3119526505470276\n",
      "Epoch 210/20000 Training Loss: 0.33411574363708496\n",
      "Epoch 211/20000 Training Loss: 0.2816835641860962\n",
      "Epoch 212/20000 Training Loss: 0.2720755338668823\n",
      "Epoch 213/20000 Training Loss: 0.29161131381988525\n",
      "Epoch 214/20000 Training Loss: 0.33414462208747864\n",
      "Epoch 215/20000 Training Loss: 0.28065139055252075\n",
      "Epoch 216/20000 Training Loss: 0.31266605854034424\n",
      "Epoch 217/20000 Training Loss: 0.28092634677886963\n",
      "Epoch 218/20000 Training Loss: 0.2570421099662781\n",
      "Epoch 219/20000 Training Loss: 0.2843232750892639\n",
      "Epoch 220/20000 Training Loss: 0.26490718126296997\n",
      "Epoch 221/20000 Training Loss: 0.30338501930236816\n",
      "Epoch 222/20000 Training Loss: 0.2641775608062744\n",
      "Epoch 223/20000 Training Loss: 0.3132653534412384\n",
      "Epoch 224/20000 Training Loss: 0.27502208948135376\n",
      "Epoch 225/20000 Training Loss: 0.2586623430252075\n",
      "Epoch 226/20000 Training Loss: 0.2972635328769684\n",
      "Epoch 227/20000 Training Loss: 0.3106515407562256\n",
      "Epoch 228/20000 Training Loss: 0.28533923625946045\n",
      "Epoch 229/20000 Training Loss: 0.3100244998931885\n",
      "Epoch 230/20000 Training Loss: 0.28520631790161133\n",
      "Epoch 231/20000 Training Loss: 0.28994154930114746\n",
      "Epoch 232/20000 Training Loss: 0.26160454750061035\n",
      "Epoch 233/20000 Training Loss: 0.29258042573928833\n",
      "Epoch 234/20000 Training Loss: 0.2560613453388214\n",
      "Epoch 235/20000 Training Loss: 0.2874535620212555\n",
      "Epoch 236/20000 Training Loss: 0.27715808153152466\n",
      "Epoch 237/20000 Training Loss: 0.2478702962398529\n",
      "Epoch 238/20000 Training Loss: 0.27559158205986023\n",
      "Epoch 239/20000 Training Loss: 0.268893837928772\n",
      "Epoch 240/20000 Training Loss: 0.22013935446739197\n",
      "Epoch 241/20000 Training Loss: 0.2883802652359009\n",
      "Epoch 242/20000 Training Loss: 0.2739780843257904\n",
      "Epoch 243/20000 Training Loss: 0.2832479476928711\n",
      "Epoch 244/20000 Training Loss: 0.28315210342407227\n",
      "Epoch 245/20000 Training Loss: 0.2728821039199829\n",
      "Epoch 246/20000 Training Loss: 0.2425948977470398\n",
      "Epoch 247/20000 Training Loss: 0.26562681794166565\n",
      "Epoch 248/20000 Training Loss: 0.2749084234237671\n",
      "Epoch 249/20000 Training Loss: 0.2339034080505371\n",
      "Epoch 250/20000 Training Loss: 0.22363558411598206\n",
      "Epoch 251/20000 Training Loss: 0.24221396446228027\n",
      "Epoch 252/20000 Training Loss: 0.2580021023750305\n",
      "Epoch 253/20000 Training Loss: 0.2876751124858856\n",
      "Epoch 254/20000 Training Loss: 0.2579886317253113\n",
      "Epoch 255/20000 Training Loss: 0.2494783252477646\n",
      "Epoch 256/20000 Training Loss: 0.25529834628105164\n",
      "Epoch 257/20000 Training Loss: 0.23755674064159393\n",
      "Epoch 258/20000 Training Loss: 0.2178158313035965\n",
      "Epoch 259/20000 Training Loss: 0.23675507307052612\n",
      "Epoch 260/20000 Training Loss: 0.2013522982597351\n",
      "Epoch 261/20000 Training Loss: 0.2595777213573456\n",
      "Epoch 262/20000 Training Loss: 0.22982025146484375\n",
      "Epoch 263/20000 Training Loss: 0.24751660227775574\n",
      "Epoch 264/20000 Training Loss: 0.26798194646835327\n",
      "Epoch 265/20000 Training Loss: 0.24788254499435425\n",
      "Epoch 266/20000 Training Loss: 0.2420637309551239\n",
      "Epoch 267/20000 Training Loss: 0.24880734086036682\n",
      "Epoch 268/20000 Training Loss: 0.26968222856521606\n",
      "Epoch 269/20000 Training Loss: 0.22401678562164307\n",
      "Epoch 270/20000 Training Loss: 0.2279447317123413\n",
      "Epoch 271/20000 Training Loss: 0.283939003944397\n",
      "Epoch 272/20000 Training Loss: 0.22010676562786102\n",
      "Epoch 273/20000 Training Loss: 0.2540662884712219\n",
      "Epoch 274/20000 Training Loss: 0.2304854691028595\n",
      "Epoch 275/20000 Training Loss: 0.23563256859779358\n",
      "Epoch 276/20000 Training Loss: 0.23162570595741272\n",
      "Epoch 277/20000 Training Loss: 0.17877289652824402\n",
      "Epoch 278/20000 Training Loss: 0.19472980499267578\n",
      "Epoch 279/20000 Training Loss: 0.25238513946533203\n",
      "Epoch 280/20000 Training Loss: 0.27016061544418335\n",
      "Epoch 281/20000 Training Loss: 0.21328461170196533\n",
      "Epoch 282/20000 Training Loss: 0.2235223948955536\n",
      "Epoch 283/20000 Training Loss: 0.24903032183647156\n",
      "Epoch 284/20000 Training Loss: 0.22398488223552704\n",
      "Epoch 285/20000 Training Loss: 0.2718760669231415\n",
      "Epoch 286/20000 Training Loss: 0.21197907626628876\n",
      "Epoch 287/20000 Training Loss: 0.23061814904212952\n",
      "Epoch 288/20000 Training Loss: 0.19816000759601593\n",
      "Epoch 289/20000 Training Loss: 0.20387713611125946\n",
      "Epoch 290/20000 Training Loss: 0.21492654085159302\n",
      "Epoch 291/20000 Training Loss: 0.236838698387146\n",
      "Epoch 292/20000 Training Loss: 0.23054587841033936\n",
      "Epoch 293/20000 Training Loss: 0.25631648302078247\n",
      "Epoch 294/20000 Training Loss: 0.23128190636634827\n",
      "Epoch 295/20000 Training Loss: 0.21709950268268585\n",
      "Epoch 296/20000 Training Loss: 0.2662285566329956\n",
      "Epoch 297/20000 Training Loss: 0.22439050674438477\n",
      "Epoch 298/20000 Training Loss: 0.20770078897476196\n",
      "Epoch 299/20000 Training Loss: 0.21075017750263214\n",
      "Epoch 300/20000 Training Loss: 0.2063060998916626\n",
      "Epoch 300/20000 Validation Loss: 0.2238890826702118\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.2238890826702118<=============\n",
      "Epoch 301/20000 Training Loss: 0.20524458587169647\n",
      "Epoch 302/20000 Training Loss: 0.25462281703948975\n",
      "Epoch 303/20000 Training Loss: 0.20378780364990234\n",
      "Epoch 304/20000 Training Loss: 0.1834360659122467\n",
      "Epoch 305/20000 Training Loss: 0.22298598289489746\n",
      "Epoch 306/20000 Training Loss: 0.20540446043014526\n",
      "Epoch 307/20000 Training Loss: 0.18750131130218506\n",
      "Epoch 308/20000 Training Loss: 0.22520112991333008\n",
      "Epoch 309/20000 Training Loss: 0.20202292501926422\n",
      "Epoch 310/20000 Training Loss: 0.19386863708496094\n",
      "Epoch 311/20000 Training Loss: 0.19938251376152039\n",
      "Epoch 312/20000 Training Loss: 0.17796272039413452\n",
      "Epoch 313/20000 Training Loss: 0.2086561918258667\n",
      "Epoch 314/20000 Training Loss: 0.2094218134880066\n",
      "Epoch 315/20000 Training Loss: 0.21688605844974518\n",
      "Epoch 316/20000 Training Loss: 0.19062647223472595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/20000 Training Loss: 0.18392035365104675\n",
      "Epoch 318/20000 Training Loss: 0.18830201029777527\n",
      "Epoch 319/20000 Training Loss: 0.21389225125312805\n",
      "Epoch 320/20000 Training Loss: 0.20672489702701569\n",
      "Epoch 321/20000 Training Loss: 0.21182310581207275\n",
      "Epoch 322/20000 Training Loss: 0.17571106553077698\n",
      "Epoch 323/20000 Training Loss: 0.2023884803056717\n",
      "Epoch 324/20000 Training Loss: 0.21646863222122192\n",
      "Epoch 325/20000 Training Loss: 0.1732669174671173\n",
      "Epoch 326/20000 Training Loss: 0.187344029545784\n",
      "Epoch 327/20000 Training Loss: 0.22465357184410095\n",
      "Epoch 328/20000 Training Loss: 0.20943115651607513\n",
      "Epoch 329/20000 Training Loss: 0.1829974204301834\n",
      "Epoch 330/20000 Training Loss: 0.1954069435596466\n",
      "Epoch 331/20000 Training Loss: 0.21412129700183868\n",
      "Epoch 332/20000 Training Loss: 0.18924666941165924\n",
      "Epoch 333/20000 Training Loss: 0.18774829804897308\n",
      "Epoch 334/20000 Training Loss: 0.2153984159231186\n",
      "Epoch 335/20000 Training Loss: 0.20108318328857422\n",
      "Epoch 336/20000 Training Loss: 0.19382083415985107\n",
      "Epoch 337/20000 Training Loss: 0.19533641636371613\n",
      "Epoch 338/20000 Training Loss: 0.20146355032920837\n",
      "Epoch 339/20000 Training Loss: 0.2036861777305603\n",
      "Epoch 340/20000 Training Loss: 0.16752150654792786\n",
      "Epoch 341/20000 Training Loss: 0.17532408237457275\n",
      "Epoch 342/20000 Training Loss: 0.20949426293373108\n",
      "Epoch 343/20000 Training Loss: 0.201216921210289\n",
      "Epoch 344/20000 Training Loss: 0.19233381748199463\n",
      "Epoch 345/20000 Training Loss: 0.15948377549648285\n",
      "Epoch 346/20000 Training Loss: 0.19170959293842316\n",
      "Epoch 347/20000 Training Loss: 0.18725460767745972\n",
      "Epoch 348/20000 Training Loss: 0.1579083353281021\n",
      "Epoch 349/20000 Training Loss: 0.17325836420059204\n",
      "Epoch 350/20000 Training Loss: 0.15727515518665314\n",
      "Epoch 351/20000 Training Loss: 0.18894988298416138\n",
      "Epoch 352/20000 Training Loss: 0.2207728922367096\n",
      "Epoch 353/20000 Training Loss: 0.1868775188922882\n",
      "Epoch 354/20000 Training Loss: 0.16822096705436707\n",
      "Epoch 355/20000 Training Loss: 0.1940644532442093\n",
      "Epoch 356/20000 Training Loss: 0.20750224590301514\n",
      "Epoch 357/20000 Training Loss: 0.19048377871513367\n",
      "Epoch 358/20000 Training Loss: 0.14483018219470978\n",
      "Epoch 359/20000 Training Loss: 0.20935627818107605\n",
      "Epoch 360/20000 Training Loss: 0.20373740792274475\n",
      "Epoch 361/20000 Training Loss: 0.21923473477363586\n",
      "Epoch 362/20000 Training Loss: 0.1695922315120697\n",
      "Epoch 363/20000 Training Loss: 0.19231414794921875\n",
      "Epoch 364/20000 Training Loss: 0.15835975110530853\n",
      "Epoch 365/20000 Training Loss: 0.20854899287223816\n",
      "Epoch 366/20000 Training Loss: 0.1796114146709442\n",
      "Epoch 367/20000 Training Loss: 0.1848643571138382\n",
      "Epoch 368/20000 Training Loss: 0.13996487855911255\n",
      "Epoch 369/20000 Training Loss: 0.1716332882642746\n",
      "Epoch 370/20000 Training Loss: 0.20595580339431763\n",
      "Epoch 371/20000 Training Loss: 0.16284066438674927\n",
      "Epoch 372/20000 Training Loss: 0.16656726598739624\n",
      "Epoch 373/20000 Training Loss: 0.1563088595867157\n",
      "Epoch 374/20000 Training Loss: 0.20262952148914337\n",
      "Epoch 375/20000 Training Loss: 0.1810041069984436\n",
      "Epoch 376/20000 Training Loss: 0.16950401663780212\n",
      "Epoch 377/20000 Training Loss: 0.18910329043865204\n",
      "Epoch 378/20000 Training Loss: 0.17194309830665588\n",
      "Epoch 379/20000 Training Loss: 0.16572704911231995\n",
      "Epoch 380/20000 Training Loss: 0.19430822134017944\n",
      "Epoch 381/20000 Training Loss: 0.19145748019218445\n",
      "Epoch 382/20000 Training Loss: 0.1252177655696869\n",
      "Epoch 383/20000 Training Loss: 0.17467045783996582\n",
      "Epoch 384/20000 Training Loss: 0.18043503165245056\n",
      "Epoch 385/20000 Training Loss: 0.15735414624214172\n",
      "Epoch 386/20000 Training Loss: 0.15283764898777008\n",
      "Epoch 387/20000 Training Loss: 0.1960766762495041\n",
      "Epoch 388/20000 Training Loss: 0.15007978677749634\n",
      "Epoch 389/20000 Training Loss: 0.1413501352071762\n",
      "Epoch 390/20000 Training Loss: 0.17800113558769226\n",
      "Epoch 391/20000 Training Loss: 0.1680096834897995\n",
      "Epoch 392/20000 Training Loss: 0.1846216917037964\n",
      "Epoch 393/20000 Training Loss: 0.16895347833633423\n",
      "Epoch 394/20000 Training Loss: 0.1500856578350067\n",
      "Epoch 395/20000 Training Loss: 0.15394848585128784\n",
      "Epoch 396/20000 Training Loss: 0.16749261319637299\n",
      "Epoch 397/20000 Training Loss: 0.1666330248117447\n",
      "Epoch 398/20000 Training Loss: 0.14726270735263824\n",
      "Epoch 399/20000 Training Loss: 0.13049030303955078\n",
      "Epoch 400/20000 Training Loss: 0.1854133903980255\n",
      "Epoch 400/20000 Validation Loss: 0.19126853346824646\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.19126853346824646<=============\n",
      "Epoch 401/20000 Training Loss: 0.13755887746810913\n",
      "Epoch 402/20000 Training Loss: 0.14565885066986084\n",
      "Epoch 403/20000 Training Loss: 0.1672709435224533\n",
      "Epoch 404/20000 Training Loss: 0.13055220246315002\n",
      "Epoch 405/20000 Training Loss: 0.13915608823299408\n",
      "Epoch 406/20000 Training Loss: 0.1477845311164856\n",
      "Epoch 407/20000 Training Loss: 0.17723678052425385\n",
      "Epoch 408/20000 Training Loss: 0.13933689892292023\n",
      "Epoch 409/20000 Training Loss: 0.1586150825023651\n",
      "Epoch 410/20000 Training Loss: 0.14940157532691956\n",
      "Epoch 411/20000 Training Loss: 0.17770998179912567\n",
      "Epoch 412/20000 Training Loss: 0.1582607924938202\n",
      "Epoch 413/20000 Training Loss: 0.1580679714679718\n",
      "Epoch 414/20000 Training Loss: 0.1625395119190216\n",
      "Epoch 415/20000 Training Loss: 0.15511231124401093\n",
      "Epoch 416/20000 Training Loss: 0.14679977297782898\n",
      "Epoch 417/20000 Training Loss: 0.14535725116729736\n",
      "Epoch 418/20000 Training Loss: 0.15113584697246552\n",
      "Epoch 419/20000 Training Loss: 0.13362860679626465\n",
      "Epoch 420/20000 Training Loss: 0.16374363005161285\n",
      "Epoch 421/20000 Training Loss: 0.13855692744255066\n",
      "Epoch 422/20000 Training Loss: 0.14725130796432495\n",
      "Epoch 423/20000 Training Loss: 0.15695345401763916\n",
      "Epoch 424/20000 Training Loss: 0.18367372453212738\n",
      "Epoch 425/20000 Training Loss: 0.16324171423912048\n",
      "Epoch 426/20000 Training Loss: 0.14733736217021942\n",
      "Epoch 427/20000 Training Loss: 0.15247514843940735\n",
      "Epoch 428/20000 Training Loss: 0.1424131691455841\n",
      "Epoch 429/20000 Training Loss: 0.13837465643882751\n",
      "Epoch 430/20000 Training Loss: 0.15959513187408447\n",
      "Epoch 431/20000 Training Loss: 0.13978970050811768\n",
      "Epoch 432/20000 Training Loss: 0.166672021150589\n",
      "Epoch 433/20000 Training Loss: 0.14355683326721191\n",
      "Epoch 434/20000 Training Loss: 0.13986825942993164\n",
      "Epoch 435/20000 Training Loss: 0.1427931785583496\n",
      "Epoch 436/20000 Training Loss: 0.14873230457305908\n",
      "Epoch 437/20000 Training Loss: 0.12779639661312103\n",
      "Epoch 438/20000 Training Loss: 0.1743224710226059\n",
      "Epoch 439/20000 Training Loss: 0.13727010786533356\n",
      "Epoch 440/20000 Training Loss: 0.14073656499385834\n",
      "Epoch 441/20000 Training Loss: 0.12926900386810303\n",
      "Epoch 442/20000 Training Loss: 0.13307100534439087\n",
      "Epoch 443/20000 Training Loss: 0.19050633907318115\n",
      "Epoch 444/20000 Training Loss: 0.16231213510036469\n",
      "Epoch 445/20000 Training Loss: 0.14064128696918488\n",
      "Epoch 446/20000 Training Loss: 0.1292780488729477\n",
      "Epoch 447/20000 Training Loss: 0.14685532450675964\n",
      "Epoch 448/20000 Training Loss: 0.14458945393562317\n",
      "Epoch 449/20000 Training Loss: 0.15687374770641327\n",
      "Epoch 450/20000 Training Loss: 0.15035367012023926\n",
      "Epoch 451/20000 Training Loss: 0.1617947667837143\n",
      "Epoch 452/20000 Training Loss: 0.12457968294620514\n",
      "Epoch 453/20000 Training Loss: 0.128321573138237\n",
      "Epoch 454/20000 Training Loss: 0.13782063126564026\n",
      "Epoch 455/20000 Training Loss: 0.15200425684452057\n",
      "Epoch 456/20000 Training Loss: 0.12031538784503937\n",
      "Epoch 457/20000 Training Loss: 0.133615642786026\n",
      "Epoch 458/20000 Training Loss: 0.14092884957790375\n",
      "Epoch 459/20000 Training Loss: 0.13965842127799988\n",
      "Epoch 460/20000 Training Loss: 0.13168396055698395\n",
      "Epoch 461/20000 Training Loss: 0.1451314091682434\n",
      "Epoch 462/20000 Training Loss: 0.1426635980606079\n",
      "Epoch 463/20000 Training Loss: 0.13338997960090637\n",
      "Epoch 464/20000 Training Loss: 0.126372829079628\n",
      "Epoch 465/20000 Training Loss: 0.15836140513420105\n",
      "Epoch 466/20000 Training Loss: 0.15869933366775513\n",
      "Epoch 467/20000 Training Loss: 0.148748978972435\n",
      "Epoch 468/20000 Training Loss: 0.13509520888328552\n",
      "Epoch 469/20000 Training Loss: 0.15403646230697632\n",
      "Epoch 470/20000 Training Loss: 0.13665413856506348\n",
      "Epoch 471/20000 Training Loss: 0.12687388062477112\n",
      "Epoch 472/20000 Training Loss: 0.11917676031589508\n",
      "Epoch 473/20000 Training Loss: 0.13307350873947144\n",
      "Epoch 474/20000 Training Loss: 0.15348854660987854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475/20000 Training Loss: 0.12968255579471588\n",
      "Epoch 476/20000 Training Loss: 0.12339361011981964\n",
      "Epoch 477/20000 Training Loss: 0.13884003460407257\n",
      "Epoch 478/20000 Training Loss: 0.1608664095401764\n",
      "Epoch 479/20000 Training Loss: 0.1358514279127121\n",
      "Epoch 480/20000 Training Loss: 0.14105093479156494\n",
      "Epoch 481/20000 Training Loss: 0.1316950023174286\n",
      "Epoch 482/20000 Training Loss: 0.14121533930301666\n",
      "Epoch 483/20000 Training Loss: 0.15275990962982178\n",
      "Epoch 484/20000 Training Loss: 0.16710670292377472\n",
      "Epoch 485/20000 Training Loss: 0.1700659543275833\n",
      "Epoch 486/20000 Training Loss: 0.12480179965496063\n",
      "Epoch 487/20000 Training Loss: 0.14827370643615723\n",
      "Epoch 488/20000 Training Loss: 0.13110977411270142\n",
      "Epoch 489/20000 Training Loss: 0.13292726874351501\n",
      "Epoch 490/20000 Training Loss: 0.17636848986148834\n",
      "Epoch 491/20000 Training Loss: 0.17391487956047058\n",
      "Epoch 492/20000 Training Loss: 0.09788326174020767\n",
      "Epoch 493/20000 Training Loss: 0.13484010100364685\n",
      "Epoch 494/20000 Training Loss: 0.14657500386238098\n",
      "Epoch 495/20000 Training Loss: 0.13714739680290222\n",
      "Epoch 496/20000 Training Loss: 0.13710199296474457\n",
      "Epoch 497/20000 Training Loss: 0.1537303626537323\n",
      "Epoch 498/20000 Training Loss: 0.1219663918018341\n",
      "Epoch 499/20000 Training Loss: 0.13907191157341003\n",
      "Epoch 500/20000 Training Loss: 0.15328079462051392\n",
      "Epoch 500/20000 Validation Loss: 0.12244270741939545\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.12244270741939545<=============\n",
      "Epoch 501/20000 Training Loss: 0.09830552339553833\n",
      "Epoch 502/20000 Training Loss: 0.13908196985721588\n",
      "Epoch 503/20000 Training Loss: 0.127997487783432\n",
      "Epoch 504/20000 Training Loss: 0.16754481196403503\n",
      "Epoch 505/20000 Training Loss: 0.14729663729667664\n",
      "Epoch 506/20000 Training Loss: 0.12138333916664124\n",
      "Epoch 507/20000 Training Loss: 0.12362222373485565\n",
      "Epoch 508/20000 Training Loss: 0.10790437459945679\n",
      "Epoch 509/20000 Training Loss: 0.11655507981777191\n",
      "Epoch 510/20000 Training Loss: 0.12064839899539948\n",
      "Epoch 511/20000 Training Loss: 0.1300496608018875\n",
      "Epoch 512/20000 Training Loss: 0.12594211101531982\n",
      "Epoch 513/20000 Training Loss: 0.11660106480121613\n",
      "Epoch 514/20000 Training Loss: 0.12099482864141464\n",
      "Epoch 515/20000 Training Loss: 0.11821972578763962\n",
      "Epoch 516/20000 Training Loss: 0.14459148049354553\n",
      "Epoch 517/20000 Training Loss: 0.11970295757055283\n",
      "Epoch 518/20000 Training Loss: 0.14545710384845734\n",
      "Epoch 519/20000 Training Loss: 0.12164320796728134\n",
      "Epoch 520/20000 Training Loss: 0.15214958786964417\n",
      "Epoch 521/20000 Training Loss: 0.12096607685089111\n",
      "Epoch 522/20000 Training Loss: 0.13662907481193542\n",
      "Epoch 523/20000 Training Loss: 0.12065428495407104\n",
      "Epoch 524/20000 Training Loss: 0.12451137602329254\n",
      "Epoch 525/20000 Training Loss: 0.13535508513450623\n",
      "Epoch 526/20000 Training Loss: 0.1292119324207306\n",
      "Epoch 527/20000 Training Loss: 0.1248500794172287\n",
      "Epoch 528/20000 Training Loss: 0.12916183471679688\n",
      "Epoch 529/20000 Training Loss: 0.1374722421169281\n",
      "Epoch 530/20000 Training Loss: 0.1304677426815033\n",
      "Epoch 531/20000 Training Loss: 0.1206914484500885\n",
      "Epoch 532/20000 Training Loss: 0.12342387437820435\n",
      "Epoch 533/20000 Training Loss: 0.0990772396326065\n",
      "Epoch 534/20000 Training Loss: 0.14901113510131836\n",
      "Epoch 535/20000 Training Loss: 0.1043301373720169\n",
      "Epoch 536/20000 Training Loss: 0.1392412781715393\n",
      "Epoch 537/20000 Training Loss: 0.13898684084415436\n",
      "Epoch 538/20000 Training Loss: 0.11686500906944275\n",
      "Epoch 539/20000 Training Loss: 0.13878017663955688\n",
      "Epoch 540/20000 Training Loss: 0.09058047086000443\n",
      "Epoch 541/20000 Training Loss: 0.14439406991004944\n",
      "Epoch 542/20000 Training Loss: 0.13002842664718628\n",
      "Epoch 543/20000 Training Loss: 0.12415695190429688\n",
      "Epoch 544/20000 Training Loss: 0.12604406476020813\n",
      "Epoch 545/20000 Training Loss: 0.11886201798915863\n",
      "Epoch 546/20000 Training Loss: 0.16003814339637756\n",
      "Epoch 547/20000 Training Loss: 0.1369779258966446\n",
      "Epoch 548/20000 Training Loss: 0.13339142501354218\n",
      "Epoch 549/20000 Training Loss: 0.14024603366851807\n",
      "Epoch 550/20000 Training Loss: 0.12851592898368835\n",
      "Epoch 551/20000 Training Loss: 0.14496538043022156\n",
      "Epoch 552/20000 Training Loss: 0.12369230389595032\n",
      "Epoch 553/20000 Training Loss: 0.12687942385673523\n",
      "Epoch 554/20000 Training Loss: 0.10883462429046631\n",
      "Epoch 555/20000 Training Loss: 0.1297665536403656\n",
      "Epoch 556/20000 Training Loss: 0.14337950944900513\n",
      "Epoch 557/20000 Training Loss: 0.10238379240036011\n",
      "Epoch 558/20000 Training Loss: 0.13132637739181519\n",
      "Epoch 559/20000 Training Loss: 0.12082351744174957\n",
      "Epoch 560/20000 Training Loss: 0.13690298795700073\n",
      "Epoch 561/20000 Training Loss: 0.12134113162755966\n",
      "Epoch 562/20000 Training Loss: 0.11710890382528305\n",
      "Epoch 563/20000 Training Loss: 0.14289316534996033\n",
      "Epoch 564/20000 Training Loss: 0.11089585721492767\n",
      "Epoch 565/20000 Training Loss: 0.12870539724826813\n",
      "Epoch 566/20000 Training Loss: 0.11463280022144318\n",
      "Epoch 567/20000 Training Loss: 0.12555630505084991\n",
      "Epoch 568/20000 Training Loss: 0.11184345185756683\n",
      "Epoch 569/20000 Training Loss: 0.11601896584033966\n",
      "Epoch 570/20000 Training Loss: 0.14954060316085815\n",
      "Epoch 571/20000 Training Loss: 0.1142587661743164\n",
      "Epoch 572/20000 Training Loss: 0.14105048775672913\n",
      "Epoch 573/20000 Training Loss: 0.1438661366701126\n",
      "Epoch 574/20000 Training Loss: 0.1430089920759201\n",
      "Epoch 575/20000 Training Loss: 0.11619563400745392\n",
      "Epoch 576/20000 Training Loss: 0.13301429152488708\n",
      "Epoch 577/20000 Training Loss: 0.11501637101173401\n",
      "Epoch 578/20000 Training Loss: 0.12396663427352905\n",
      "Epoch 579/20000 Training Loss: 0.1275044083595276\n",
      "Epoch 580/20000 Training Loss: 0.11529555916786194\n",
      "Epoch 581/20000 Training Loss: 0.1189064010977745\n",
      "Epoch 582/20000 Training Loss: 0.11878471076488495\n",
      "Epoch 583/20000 Training Loss: 0.16436003148555756\n",
      "Epoch 584/20000 Training Loss: 0.11237110197544098\n",
      "Epoch 585/20000 Training Loss: 0.09676295518875122\n",
      "Epoch 586/20000 Training Loss: 0.13247311115264893\n",
      "Epoch 587/20000 Training Loss: 0.10309972614049911\n",
      "Epoch 588/20000 Training Loss: 0.08647704124450684\n",
      "Epoch 589/20000 Training Loss: 0.12477600574493408\n",
      "Epoch 590/20000 Training Loss: 0.11792092025279999\n",
      "Epoch 591/20000 Training Loss: 0.11387157440185547\n",
      "Epoch 592/20000 Training Loss: 0.11355015635490417\n",
      "Epoch 593/20000 Training Loss: 0.11443635821342468\n",
      "Epoch 594/20000 Training Loss: 0.11631786823272705\n",
      "Epoch 595/20000 Training Loss: 0.11920563876628876\n",
      "Epoch 596/20000 Training Loss: 0.15640220046043396\n",
      "Epoch 597/20000 Training Loss: 0.14672322571277618\n",
      "Epoch 598/20000 Training Loss: 0.09337662160396576\n",
      "Epoch 599/20000 Training Loss: 0.09785845130681992\n",
      "Epoch 600/20000 Training Loss: 0.11120569705963135\n",
      "Epoch 600/20000 Validation Loss: 0.1442290097475052\n",
      "Epoch 601/20000 Training Loss: 0.12323898077011108\n",
      "Epoch 602/20000 Training Loss: 0.12365971505641937\n",
      "Epoch 603/20000 Training Loss: 0.13049793243408203\n",
      "Epoch 604/20000 Training Loss: 0.12166960537433624\n",
      "Epoch 605/20000 Training Loss: 0.12965339422225952\n",
      "Epoch 606/20000 Training Loss: 0.13413427770137787\n",
      "Epoch 607/20000 Training Loss: 0.12502384185791016\n",
      "Epoch 608/20000 Training Loss: 0.11870869994163513\n",
      "Epoch 609/20000 Training Loss: 0.09599242359399796\n",
      "Epoch 610/20000 Training Loss: 0.15611860156059265\n",
      "Epoch 611/20000 Training Loss: 0.10705789923667908\n",
      "Epoch 612/20000 Training Loss: 0.1276427060365677\n",
      "Epoch 613/20000 Training Loss: 0.118721142411232\n",
      "Epoch 614/20000 Training Loss: 0.11935235559940338\n",
      "Epoch 615/20000 Training Loss: 0.10673921555280685\n",
      "Epoch 616/20000 Training Loss: 0.09871456027030945\n",
      "Epoch 617/20000 Training Loss: 0.14724035561084747\n",
      "Epoch 618/20000 Training Loss: 0.11470389366149902\n",
      "Epoch 619/20000 Training Loss: 0.13102848827838898\n",
      "Epoch 620/20000 Training Loss: 0.09698456525802612\n",
      "Epoch 621/20000 Training Loss: 0.11766120046377182\n",
      "Epoch 622/20000 Training Loss: 0.1145772635936737\n",
      "Epoch 623/20000 Training Loss: 0.10694803297519684\n",
      "Epoch 624/20000 Training Loss: 0.11472391337156296\n",
      "Epoch 625/20000 Training Loss: 0.13755738735198975\n",
      "Epoch 626/20000 Training Loss: 0.11506679654121399\n",
      "Epoch 627/20000 Training Loss: 0.11270621418952942\n",
      "Epoch 628/20000 Training Loss: 0.11388137936592102\n",
      "Epoch 629/20000 Training Loss: 0.12230218946933746\n",
      "Epoch 630/20000 Training Loss: 0.13044346868991852\n",
      "Epoch 631/20000 Training Loss: 0.13586914539337158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 632/20000 Training Loss: 0.1010279655456543\n",
      "Epoch 633/20000 Training Loss: 0.11987681686878204\n",
      "Epoch 634/20000 Training Loss: 0.10534965991973877\n",
      "Epoch 635/20000 Training Loss: 0.14259986579418182\n",
      "Epoch 636/20000 Training Loss: 0.12641270458698273\n",
      "Epoch 637/20000 Training Loss: 0.127445250749588\n",
      "Epoch 638/20000 Training Loss: 0.11220333725214005\n",
      "Epoch 639/20000 Training Loss: 0.10971663892269135\n",
      "Epoch 640/20000 Training Loss: 0.09641699492931366\n",
      "Epoch 641/20000 Training Loss: 0.10650657117366791\n",
      "Epoch 642/20000 Training Loss: 0.11927051842212677\n",
      "Epoch 643/20000 Training Loss: 0.12435507774353027\n",
      "Epoch 644/20000 Training Loss: 0.08754831552505493\n",
      "Epoch 645/20000 Training Loss: 0.12838169932365417\n",
      "Epoch 646/20000 Training Loss: 0.1496594250202179\n",
      "Epoch 647/20000 Training Loss: 0.11192537844181061\n",
      "Epoch 648/20000 Training Loss: 0.10847268998622894\n",
      "Epoch 649/20000 Training Loss: 0.11096890270709991\n",
      "Epoch 650/20000 Training Loss: 0.11157144606113434\n",
      "Epoch 651/20000 Training Loss: 0.12822601199150085\n",
      "Epoch 652/20000 Training Loss: 0.15633828938007355\n",
      "Epoch 653/20000 Training Loss: 0.14360088109970093\n",
      "Epoch 654/20000 Training Loss: 0.0952758640050888\n",
      "Epoch 655/20000 Training Loss: 0.1083141416311264\n",
      "Epoch 656/20000 Training Loss: 0.13167092204093933\n",
      "Epoch 657/20000 Training Loss: 0.11634260416030884\n",
      "Epoch 658/20000 Training Loss: 0.134701669216156\n",
      "Epoch 659/20000 Training Loss: 0.11660953611135483\n",
      "Epoch 660/20000 Training Loss: 0.13449028134346008\n",
      "Epoch 661/20000 Training Loss: 0.12536612153053284\n",
      "Epoch 662/20000 Training Loss: 0.13208699226379395\n",
      "Epoch 663/20000 Training Loss: 0.1069868877530098\n",
      "Epoch 664/20000 Training Loss: 0.09875624626874924\n",
      "Epoch 665/20000 Training Loss: 0.10255556553602219\n",
      "Epoch 666/20000 Training Loss: 0.11672776937484741\n",
      "Epoch 667/20000 Training Loss: 0.12305986881256104\n",
      "Epoch 668/20000 Training Loss: 0.1219799816608429\n",
      "Epoch 669/20000 Training Loss: 0.11167029291391373\n",
      "Epoch 670/20000 Training Loss: 0.12893715500831604\n",
      "Epoch 671/20000 Training Loss: 0.10628862679004669\n",
      "Epoch 672/20000 Training Loss: 0.11192000657320023\n",
      "Epoch 673/20000 Training Loss: 0.12109510600566864\n",
      "Epoch 674/20000 Training Loss: 0.10020296275615692\n",
      "Epoch 675/20000 Training Loss: 0.11520997434854507\n",
      "Epoch 676/20000 Training Loss: 0.10509931296110153\n",
      "Epoch 677/20000 Training Loss: 0.12146704643964767\n",
      "Epoch 678/20000 Training Loss: 0.10985115170478821\n",
      "Epoch 679/20000 Training Loss: 0.0894131064414978\n",
      "Epoch 680/20000 Training Loss: 0.11623413860797882\n",
      "Epoch 681/20000 Training Loss: 0.11787156760692596\n",
      "Epoch 682/20000 Training Loss: 0.10995468497276306\n",
      "Epoch 683/20000 Training Loss: 0.13216543197631836\n",
      "Epoch 684/20000 Training Loss: 0.10882522910833359\n",
      "Epoch 685/20000 Training Loss: 0.11868779361248016\n",
      "Epoch 686/20000 Training Loss: 0.10693792998790741\n",
      "Epoch 687/20000 Training Loss: 0.127291738986969\n",
      "Epoch 688/20000 Training Loss: 0.12882010638713837\n",
      "Epoch 689/20000 Training Loss: 0.1044219434261322\n",
      "Epoch 690/20000 Training Loss: 0.09356558322906494\n",
      "Epoch 691/20000 Training Loss: 0.09609274566173553\n",
      "Epoch 692/20000 Training Loss: 0.12719669938087463\n",
      "Epoch 693/20000 Training Loss: 0.1201709508895874\n",
      "Epoch 694/20000 Training Loss: 0.12004834413528442\n",
      "Epoch 695/20000 Training Loss: 0.12047743797302246\n",
      "Epoch 696/20000 Training Loss: 0.12624457478523254\n",
      "Epoch 697/20000 Training Loss: 0.13049115240573883\n",
      "Epoch 698/20000 Training Loss: 0.13330374658107758\n",
      "Epoch 699/20000 Training Loss: 0.13236004114151\n",
      "Epoch 700/20000 Training Loss: 0.1099102795124054\n",
      "Epoch 700/20000 Validation Loss: 0.14100953936576843\n",
      "Epoch 701/20000 Training Loss: 0.12592527270317078\n",
      "Epoch 702/20000 Training Loss: 0.12105013430118561\n",
      "Epoch 703/20000 Training Loss: 0.13275347650051117\n",
      "Epoch 704/20000 Training Loss: 0.10654426366090775\n",
      "Epoch 705/20000 Training Loss: 0.12319900840520859\n",
      "Epoch 706/20000 Training Loss: 0.11281806975603104\n",
      "Epoch 707/20000 Training Loss: 0.127065047621727\n",
      "Epoch 708/20000 Training Loss: 0.1195092499256134\n",
      "Epoch 709/20000 Training Loss: 0.09986716508865356\n",
      "Epoch 710/20000 Training Loss: 0.12251251190900803\n",
      "Epoch 711/20000 Training Loss: 0.14082802832126617\n",
      "Epoch 712/20000 Training Loss: 0.12938304245471954\n",
      "Epoch 713/20000 Training Loss: 0.10776357352733612\n",
      "Epoch 714/20000 Training Loss: 0.12415256351232529\n",
      "Epoch 715/20000 Training Loss: 0.11496846377849579\n",
      "Epoch 716/20000 Training Loss: 0.12014402449131012\n",
      "Epoch 717/20000 Training Loss: 0.11390937119722366\n",
      "Epoch 718/20000 Training Loss: 0.09641881287097931\n",
      "Epoch 719/20000 Training Loss: 0.12788471579551697\n",
      "Epoch 720/20000 Training Loss: 0.11900696158409119\n",
      "Epoch 721/20000 Training Loss: 0.11968556046485901\n",
      "Epoch 722/20000 Training Loss: 0.11296069622039795\n",
      "Epoch 723/20000 Training Loss: 0.09647276997566223\n",
      "Epoch 724/20000 Training Loss: 0.10030114650726318\n",
      "Epoch 725/20000 Training Loss: 0.1186310425400734\n",
      "Epoch 726/20000 Training Loss: 0.12637662887573242\n",
      "Epoch 727/20000 Training Loss: 0.11732195317745209\n",
      "Epoch 728/20000 Training Loss: 0.09095537662506104\n",
      "Epoch 729/20000 Training Loss: 0.11637743562459946\n",
      "Epoch 730/20000 Training Loss: 0.12231066823005676\n",
      "Epoch 731/20000 Training Loss: 0.09284724295139313\n",
      "Epoch 732/20000 Training Loss: 0.10293751955032349\n",
      "Epoch 733/20000 Training Loss: 0.08790985494852066\n",
      "Epoch 734/20000 Training Loss: 0.13344886898994446\n",
      "Epoch 735/20000 Training Loss: 0.1003742665052414\n",
      "Epoch 736/20000 Training Loss: 0.11069744825363159\n",
      "Epoch 737/20000 Training Loss: 0.10718017816543579\n",
      "Epoch 738/20000 Training Loss: 0.10256661474704742\n",
      "Epoch 739/20000 Training Loss: 0.09420095384120941\n",
      "Epoch 740/20000 Training Loss: 0.11566302180290222\n",
      "Epoch 741/20000 Training Loss: 0.10612693428993225\n",
      "Epoch 742/20000 Training Loss: 0.09924135357141495\n",
      "Epoch 743/20000 Training Loss: 0.11082932353019714\n",
      "Epoch 744/20000 Training Loss: 0.09407538175582886\n",
      "Epoch 745/20000 Training Loss: 0.09407198429107666\n",
      "Epoch 746/20000 Training Loss: 0.10281887650489807\n",
      "Epoch 747/20000 Training Loss: 0.11490800976753235\n",
      "Epoch 748/20000 Training Loss: 0.10706140100955963\n",
      "Epoch 749/20000 Training Loss: 0.1097196489572525\n",
      "Epoch 750/20000 Training Loss: 0.0979495495557785\n",
      "Epoch 751/20000 Training Loss: 0.10863591730594635\n",
      "Epoch 752/20000 Training Loss: 0.10917818546295166\n",
      "Epoch 753/20000 Training Loss: 0.0925331860780716\n",
      "Epoch 754/20000 Training Loss: 0.09459099173545837\n",
      "Epoch 755/20000 Training Loss: 0.09447096288204193\n",
      "Epoch 756/20000 Training Loss: 0.12589728832244873\n",
      "Epoch 757/20000 Training Loss: 0.09924417734146118\n",
      "Epoch 758/20000 Training Loss: 0.1258244514465332\n",
      "Epoch 759/20000 Training Loss: 0.1573517620563507\n",
      "Epoch 760/20000 Training Loss: 0.09106805920600891\n",
      "Epoch 761/20000 Training Loss: 0.09957141429185867\n",
      "Epoch 762/20000 Training Loss: 0.10492503643035889\n",
      "Epoch 763/20000 Training Loss: 0.13698723912239075\n",
      "Epoch 764/20000 Training Loss: 0.10957305133342743\n",
      "Epoch 765/20000 Training Loss: 0.08447063714265823\n",
      "Epoch 766/20000 Training Loss: 0.10639531910419464\n",
      "Epoch 767/20000 Training Loss: 0.1360723227262497\n",
      "Epoch 768/20000 Training Loss: 0.14002133905887604\n",
      "Epoch 769/20000 Training Loss: 0.1083696186542511\n",
      "Epoch 770/20000 Training Loss: 0.11658889800310135\n",
      "Epoch 771/20000 Training Loss: 0.07369836419820786\n",
      "Epoch 772/20000 Training Loss: 0.10604529082775116\n",
      "Epoch 773/20000 Training Loss: 0.1270817667245865\n",
      "Epoch 774/20000 Training Loss: 0.1059018149971962\n",
      "Epoch 775/20000 Training Loss: 0.14410293102264404\n",
      "Epoch 776/20000 Training Loss: 0.11632020771503448\n",
      "Epoch 777/20000 Training Loss: 0.10016706585884094\n",
      "Epoch 778/20000 Training Loss: 0.0893796756863594\n",
      "Epoch 779/20000 Training Loss: 0.11621461063623428\n",
      "Epoch 780/20000 Training Loss: 0.09993384778499603\n",
      "Epoch 781/20000 Training Loss: 0.12165334820747375\n",
      "Epoch 782/20000 Training Loss: 0.1113167405128479\n",
      "Epoch 783/20000 Training Loss: 0.0847751572728157\n",
      "Epoch 784/20000 Training Loss: 0.09327556192874908\n",
      "Epoch 785/20000 Training Loss: 0.10860812664031982\n",
      "Epoch 786/20000 Training Loss: 0.11624722182750702\n",
      "Epoch 787/20000 Training Loss: 0.10357178747653961\n",
      "Epoch 788/20000 Training Loss: 0.0928918719291687\n",
      "Epoch 789/20000 Training Loss: 0.09465958923101425\n",
      "Epoch 790/20000 Training Loss: 0.09687010943889618\n",
      "Epoch 791/20000 Training Loss: 0.1121319830417633\n",
      "Epoch 792/20000 Training Loss: 0.12279219925403595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 793/20000 Training Loss: 0.08786492049694061\n",
      "Epoch 794/20000 Training Loss: 0.09161100536584854\n",
      "Epoch 795/20000 Training Loss: 0.08229983597993851\n",
      "Epoch 796/20000 Training Loss: 0.10622658580541611\n",
      "Epoch 797/20000 Training Loss: 0.10951046645641327\n",
      "Epoch 798/20000 Training Loss: 0.09384529292583466\n",
      "Epoch 799/20000 Training Loss: 0.11721418052911758\n",
      "Epoch 800/20000 Training Loss: 0.10137118399143219\n",
      "Epoch 800/20000 Validation Loss: 0.0905928686261177\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.0905928686261177<=============\n",
      "Epoch 801/20000 Training Loss: 0.09442433714866638\n",
      "Epoch 802/20000 Training Loss: 0.09516521543264389\n",
      "Epoch 803/20000 Training Loss: 0.10264230519533157\n",
      "Epoch 804/20000 Training Loss: 0.10624988377094269\n",
      "Epoch 805/20000 Training Loss: 0.09270185977220535\n",
      "Epoch 806/20000 Training Loss: 0.09365958720445633\n",
      "Epoch 807/20000 Training Loss: 0.09702375531196594\n",
      "Epoch 808/20000 Training Loss: 0.11520640552043915\n",
      "Epoch 809/20000 Training Loss: 0.08674636483192444\n",
      "Epoch 810/20000 Training Loss: 0.10523861646652222\n",
      "Epoch 811/20000 Training Loss: 0.12199656665325165\n",
      "Epoch 812/20000 Training Loss: 0.1077379584312439\n",
      "Epoch 813/20000 Training Loss: 0.09380140155553818\n",
      "Epoch 814/20000 Training Loss: 0.11174707114696503\n",
      "Epoch 815/20000 Training Loss: 0.10292893648147583\n",
      "Epoch 816/20000 Training Loss: 0.11686502397060394\n",
      "Epoch 817/20000 Training Loss: 0.11431834846735\n",
      "Epoch 818/20000 Training Loss: 0.11033141613006592\n",
      "Epoch 819/20000 Training Loss: 0.08191925287246704\n",
      "Epoch 820/20000 Training Loss: 0.10061351954936981\n",
      "Epoch 821/20000 Training Loss: 0.09171488136053085\n",
      "Epoch 822/20000 Training Loss: 0.1278834044933319\n",
      "Epoch 823/20000 Training Loss: 0.10590176284313202\n",
      "Epoch 824/20000 Training Loss: 0.0892329141497612\n",
      "Epoch 825/20000 Training Loss: 0.12919031083583832\n",
      "Epoch 826/20000 Training Loss: 0.09407536685466766\n",
      "Epoch 827/20000 Training Loss: 0.10214744508266449\n",
      "Epoch 828/20000 Training Loss: 0.08897947520017624\n",
      "Epoch 829/20000 Training Loss: 0.11892106384038925\n",
      "Epoch 830/20000 Training Loss: 0.10039648413658142\n",
      "Epoch 831/20000 Training Loss: 0.09335170686244965\n",
      "Epoch 832/20000 Training Loss: 0.09150107949972153\n",
      "Epoch 833/20000 Training Loss: 0.12674862146377563\n",
      "Epoch 834/20000 Training Loss: 0.10104702413082123\n",
      "Epoch 835/20000 Training Loss: 0.09803356975317001\n",
      "Epoch 836/20000 Training Loss: 0.10385972261428833\n",
      "Epoch 837/20000 Training Loss: 0.08568322658538818\n",
      "Epoch 838/20000 Training Loss: 0.11865995079278946\n",
      "Epoch 839/20000 Training Loss: 0.10501807928085327\n",
      "Epoch 840/20000 Training Loss: 0.11845739185810089\n",
      "Epoch 841/20000 Training Loss: 0.11586353182792664\n",
      "Epoch 842/20000 Training Loss: 0.10338196903467178\n",
      "Epoch 843/20000 Training Loss: 0.10417291522026062\n",
      "Epoch 844/20000 Training Loss: 0.09731820970773697\n",
      "Epoch 845/20000 Training Loss: 0.10835015773773193\n",
      "Epoch 846/20000 Training Loss: 0.11552570760250092\n",
      "Epoch 847/20000 Training Loss: 0.12481434643268585\n",
      "Epoch 848/20000 Training Loss: 0.11170204728841782\n",
      "Epoch 849/20000 Training Loss: 0.10004326701164246\n",
      "Epoch 850/20000 Training Loss: 0.1272856593132019\n",
      "Epoch 851/20000 Training Loss: 0.09651665389537811\n",
      "Epoch 852/20000 Training Loss: 0.10463515669107437\n",
      "Epoch 853/20000 Training Loss: 0.08743442595005035\n",
      "Epoch 854/20000 Training Loss: 0.10295901447534561\n",
      "Epoch 855/20000 Training Loss: 0.11144030094146729\n",
      "Epoch 856/20000 Training Loss: 0.0857241302728653\n",
      "Epoch 857/20000 Training Loss: 0.0987328588962555\n",
      "Epoch 858/20000 Training Loss: 0.12130649387836456\n",
      "Epoch 859/20000 Training Loss: 0.09856316447257996\n",
      "Epoch 860/20000 Training Loss: 0.11596120893955231\n",
      "Epoch 861/20000 Training Loss: 0.10132008790969849\n",
      "Epoch 862/20000 Training Loss: 0.09660295397043228\n",
      "Epoch 863/20000 Training Loss: 0.09237972646951675\n",
      "Epoch 864/20000 Training Loss: 0.08259980380535126\n",
      "Epoch 865/20000 Training Loss: 0.12343618273735046\n",
      "Epoch 866/20000 Training Loss: 0.11085852980613708\n",
      "Epoch 867/20000 Training Loss: 0.0921890065073967\n",
      "Epoch 868/20000 Training Loss: 0.12517723441123962\n",
      "Epoch 869/20000 Training Loss: 0.0975029245018959\n",
      "Epoch 870/20000 Training Loss: 0.10227186977863312\n",
      "Epoch 871/20000 Training Loss: 0.1084301769733429\n",
      "Epoch 872/20000 Training Loss: 0.09022797644138336\n",
      "Epoch 873/20000 Training Loss: 0.09286047518253326\n",
      "Epoch 874/20000 Training Loss: 0.07527972757816315\n",
      "Epoch 875/20000 Training Loss: 0.09116539359092712\n",
      "Epoch 876/20000 Training Loss: 0.08982118964195251\n",
      "Epoch 877/20000 Training Loss: 0.10306819528341293\n",
      "Epoch 878/20000 Training Loss: 0.086477130651474\n",
      "Epoch 879/20000 Training Loss: 0.08581031113862991\n",
      "Epoch 880/20000 Training Loss: 0.12505170702934265\n",
      "Epoch 881/20000 Training Loss: 0.10022613406181335\n",
      "Epoch 882/20000 Training Loss: 0.13951724767684937\n",
      "Epoch 883/20000 Training Loss: 0.09256743639707565\n",
      "Epoch 884/20000 Training Loss: 0.1094050258398056\n",
      "Epoch 885/20000 Training Loss: 0.06866908073425293\n",
      "Epoch 886/20000 Training Loss: 0.11607635021209717\n",
      "Epoch 887/20000 Training Loss: 0.0854826420545578\n",
      "Epoch 888/20000 Training Loss: 0.09445015341043472\n",
      "Epoch 889/20000 Training Loss: 0.09438613057136536\n",
      "Epoch 890/20000 Training Loss: 0.12596286833286285\n",
      "Epoch 891/20000 Training Loss: 0.08126026391983032\n",
      "Epoch 892/20000 Training Loss: 0.0912916362285614\n",
      "Epoch 893/20000 Training Loss: 0.08705821633338928\n",
      "Epoch 894/20000 Training Loss: 0.10338990390300751\n",
      "Epoch 895/20000 Training Loss: 0.10500316321849823\n",
      "Epoch 896/20000 Training Loss: 0.10600601136684418\n",
      "Epoch 897/20000 Training Loss: 0.13023380935192108\n",
      "Epoch 898/20000 Training Loss: 0.11568880081176758\n",
      "Epoch 899/20000 Training Loss: 0.09671235829591751\n",
      "Epoch 900/20000 Training Loss: 0.10826414823532104\n",
      "Epoch 900/20000 Validation Loss: 0.10701510310173035\n",
      "Epoch 901/20000 Training Loss: 0.11710715293884277\n",
      "Epoch 902/20000 Training Loss: 0.1370723992586136\n",
      "Epoch 903/20000 Training Loss: 0.11605599522590637\n",
      "Epoch 904/20000 Training Loss: 0.09071707725524902\n",
      "Epoch 905/20000 Training Loss: 0.1028958261013031\n",
      "Epoch 906/20000 Training Loss: 0.09299388527870178\n",
      "Epoch 907/20000 Training Loss: 0.10944917798042297\n",
      "Epoch 908/20000 Training Loss: 0.09003645181655884\n",
      "Epoch 909/20000 Training Loss: 0.09502007812261581\n",
      "Epoch 910/20000 Training Loss: 0.10372847318649292\n",
      "Epoch 911/20000 Training Loss: 0.08133640885353088\n",
      "Epoch 912/20000 Training Loss: 0.07638607919216156\n",
      "Epoch 913/20000 Training Loss: 0.10176832228899002\n",
      "Epoch 914/20000 Training Loss: 0.09413543343544006\n",
      "Epoch 915/20000 Training Loss: 0.10383712500333786\n",
      "Epoch 916/20000 Training Loss: 0.09124282002449036\n",
      "Epoch 917/20000 Training Loss: 0.0882968008518219\n",
      "Epoch 918/20000 Training Loss: 0.0944715291261673\n",
      "Epoch 919/20000 Training Loss: 0.10681269317865372\n",
      "Epoch 920/20000 Training Loss: 0.08603336662054062\n",
      "Epoch 921/20000 Training Loss: 0.10357828438282013\n",
      "Epoch 922/20000 Training Loss: 0.10232822597026825\n",
      "Epoch 923/20000 Training Loss: 0.10205016285181046\n",
      "Epoch 924/20000 Training Loss: 0.10875719040632248\n",
      "Epoch 925/20000 Training Loss: 0.10960584878921509\n",
      "Epoch 926/20000 Training Loss: 0.0905403196811676\n",
      "Epoch 927/20000 Training Loss: 0.11751130223274231\n",
      "Epoch 928/20000 Training Loss: 0.1173207014799118\n",
      "Epoch 929/20000 Training Loss: 0.10156027972698212\n",
      "Epoch 930/20000 Training Loss: 0.10076544433832169\n",
      "Epoch 931/20000 Training Loss: 0.09675789624452591\n",
      "Epoch 932/20000 Training Loss: 0.09584952890872955\n",
      "Epoch 933/20000 Training Loss: 0.10114403814077377\n",
      "Epoch 934/20000 Training Loss: 0.11093650013208389\n",
      "Epoch 935/20000 Training Loss: 0.11535076051950455\n",
      "Epoch 936/20000 Training Loss: 0.08268676698207855\n",
      "Epoch 937/20000 Training Loss: 0.08858036994934082\n",
      "Epoch 938/20000 Training Loss: 0.11065065115690231\n",
      "Epoch 939/20000 Training Loss: 0.12761786580085754\n",
      "Epoch 940/20000 Training Loss: 0.09385998547077179\n",
      "Epoch 941/20000 Training Loss: 0.08673505485057831\n",
      "Epoch 942/20000 Training Loss: 0.09190608561038971\n",
      "Epoch 943/20000 Training Loss: 0.09644487500190735\n",
      "Epoch 944/20000 Training Loss: 0.09879547357559204\n",
      "Epoch 945/20000 Training Loss: 0.13850320875644684\n",
      "Epoch 946/20000 Training Loss: 0.10161887854337692\n",
      "Epoch 947/20000 Training Loss: 0.08511962741613388\n",
      "Epoch 948/20000 Training Loss: 0.12378494441509247\n",
      "Epoch 949/20000 Training Loss: 0.10712100565433502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/20000 Training Loss: 0.08332863450050354\n",
      "Epoch 951/20000 Training Loss: 0.08750501275062561\n",
      "Epoch 952/20000 Training Loss: 0.0918051153421402\n",
      "Epoch 953/20000 Training Loss: 0.1018587052822113\n",
      "Epoch 954/20000 Training Loss: 0.1264859139919281\n",
      "Epoch 955/20000 Training Loss: 0.12421663850545883\n",
      "Epoch 956/20000 Training Loss: 0.10111505538225174\n",
      "Epoch 957/20000 Training Loss: 0.09336741268634796\n",
      "Epoch 958/20000 Training Loss: 0.10152625292539597\n",
      "Epoch 959/20000 Training Loss: 0.12286368757486343\n",
      "Epoch 960/20000 Training Loss: 0.11165504157543182\n",
      "Epoch 961/20000 Training Loss: 0.09047288447618484\n",
      "Epoch 962/20000 Training Loss: 0.0961267501115799\n",
      "Epoch 963/20000 Training Loss: 0.08218836039304733\n",
      "Epoch 964/20000 Training Loss: 0.10462374985218048\n",
      "Epoch 965/20000 Training Loss: 0.1401452273130417\n",
      "Epoch 966/20000 Training Loss: 0.08815424889326096\n",
      "Epoch 967/20000 Training Loss: 0.11544530093669891\n",
      "Epoch 968/20000 Training Loss: 0.09293662011623383\n",
      "Epoch 969/20000 Training Loss: 0.10273272544145584\n",
      "Epoch 970/20000 Training Loss: 0.09452717006206512\n",
      "Epoch 971/20000 Training Loss: 0.11302818357944489\n",
      "Epoch 972/20000 Training Loss: 0.09086442738771439\n",
      "Epoch 973/20000 Training Loss: 0.10370119661092758\n",
      "Epoch 974/20000 Training Loss: 0.11000452935695648\n",
      "Epoch 975/20000 Training Loss: 0.11048615723848343\n",
      "Epoch 976/20000 Training Loss: 0.07866568118333817\n",
      "Epoch 977/20000 Training Loss: 0.10830333083868027\n",
      "Epoch 978/20000 Training Loss: 0.09525539726018906\n",
      "Epoch 979/20000 Training Loss: 0.11572164297103882\n",
      "Epoch 980/20000 Training Loss: 0.09546740353107452\n",
      "Epoch 981/20000 Training Loss: 0.09604807943105698\n",
      "Epoch 982/20000 Training Loss: 0.10719047486782074\n",
      "Epoch 983/20000 Training Loss: 0.09692870080471039\n",
      "Epoch 984/20000 Training Loss: 0.1057434156537056\n",
      "Epoch 985/20000 Training Loss: 0.10738340020179749\n",
      "Epoch 986/20000 Training Loss: 0.10808978974819183\n",
      "Epoch 987/20000 Training Loss: 0.0958837941288948\n",
      "Epoch 988/20000 Training Loss: 0.12028005719184875\n",
      "Epoch 989/20000 Training Loss: 0.10244809091091156\n",
      "Epoch 990/20000 Training Loss: 0.11885668337345123\n",
      "Epoch 991/20000 Training Loss: 0.11831887811422348\n",
      "Epoch 992/20000 Training Loss: 0.11626354604959488\n",
      "Epoch 993/20000 Training Loss: 0.10108482837677002\n",
      "Epoch 994/20000 Training Loss: 0.0948871448636055\n",
      "Epoch 995/20000 Training Loss: 0.09297005832195282\n",
      "Epoch 996/20000 Training Loss: 0.09352671355009079\n",
      "Epoch 997/20000 Training Loss: 0.10647419095039368\n",
      "Epoch 998/20000 Training Loss: 0.09144119918346405\n",
      "Epoch 999/20000 Training Loss: 0.07758046686649323\n",
      "Epoch 1000/20000 Training Loss: 0.11945704370737076\n",
      "Epoch 1000/20000 Validation Loss: 0.10897542536258698\n",
      "Epoch 1001/20000 Training Loss: 0.11077897250652313\n",
      "Epoch 1002/20000 Training Loss: 0.13259436190128326\n",
      "Epoch 1003/20000 Training Loss: 0.0761408731341362\n",
      "Epoch 1004/20000 Training Loss: 0.1065749078989029\n",
      "Epoch 1005/20000 Training Loss: 0.09010037779808044\n",
      "Epoch 1006/20000 Training Loss: 0.08386824280023575\n",
      "Epoch 1007/20000 Training Loss: 0.09924662858247757\n",
      "Epoch 1008/20000 Training Loss: 0.08903978765010834\n",
      "Epoch 1009/20000 Training Loss: 0.07499010860919952\n",
      "Epoch 1010/20000 Training Loss: 0.09696480631828308\n",
      "Epoch 1011/20000 Training Loss: 0.07693863660097122\n",
      "Epoch 1012/20000 Training Loss: 0.08053711801767349\n",
      "Epoch 1013/20000 Training Loss: 0.0761326402425766\n",
      "Epoch 1014/20000 Training Loss: 0.07265591621398926\n",
      "Epoch 1015/20000 Training Loss: 0.11026094108819962\n",
      "Epoch 1016/20000 Training Loss: 0.09571182727813721\n",
      "Epoch 1017/20000 Training Loss: 0.08922334015369415\n",
      "Epoch 1018/20000 Training Loss: 0.10745538026094437\n",
      "Epoch 1019/20000 Training Loss: 0.09867796301841736\n",
      "Epoch 1020/20000 Training Loss: 0.1134420782327652\n",
      "Epoch 1021/20000 Training Loss: 0.10229380428791046\n",
      "Epoch 1022/20000 Training Loss: 0.09208045899868011\n",
      "Epoch 1023/20000 Training Loss: 0.07305382192134857\n",
      "Epoch 1024/20000 Training Loss: 0.09603169560432434\n",
      "Epoch 1025/20000 Training Loss: 0.08678068220615387\n",
      "Epoch 1026/20000 Training Loss: 0.09589756280183792\n",
      "Epoch 1027/20000 Training Loss: 0.08746762573719025\n",
      "Epoch 1028/20000 Training Loss: 0.10582828521728516\n",
      "Epoch 1029/20000 Training Loss: 0.11704865843057632\n",
      "Epoch 1030/20000 Training Loss: 0.13668212294578552\n",
      "Epoch 1031/20000 Training Loss: 0.1158694326877594\n",
      "Epoch 1032/20000 Training Loss: 0.11088095605373383\n",
      "Epoch 1033/20000 Training Loss: 0.091499924659729\n",
      "Epoch 1034/20000 Training Loss: 0.10097350925207138\n",
      "Epoch 1035/20000 Training Loss: 0.09225552529096603\n",
      "Epoch 1036/20000 Training Loss: 0.07941281795501709\n",
      "Epoch 1037/20000 Training Loss: 0.10705243051052094\n",
      "Epoch 1038/20000 Training Loss: 0.10137522220611572\n",
      "Epoch 1039/20000 Training Loss: 0.0997994914650917\n",
      "Epoch 1040/20000 Training Loss: 0.11199552565813065\n",
      "Epoch 1041/20000 Training Loss: 0.09157780557870865\n",
      "Epoch 1042/20000 Training Loss: 0.12527504563331604\n",
      "Epoch 1043/20000 Training Loss: 0.08493664115667343\n",
      "Epoch 1044/20000 Training Loss: 0.1157459020614624\n",
      "Epoch 1045/20000 Training Loss: 0.11536931991577148\n",
      "Epoch 1046/20000 Training Loss: 0.09201021492481232\n",
      "Epoch 1047/20000 Training Loss: 0.10586240887641907\n",
      "Epoch 1048/20000 Training Loss: 0.07945570349693298\n",
      "Epoch 1049/20000 Training Loss: 0.08788998425006866\n",
      "Epoch 1050/20000 Training Loss: 0.10325688123703003\n",
      "Epoch 1051/20000 Training Loss: 0.13102853298187256\n",
      "Epoch 1052/20000 Training Loss: 0.11461640894412994\n",
      "Epoch 1053/20000 Training Loss: 0.0781107246875763\n",
      "Epoch 1054/20000 Training Loss: 0.1403995156288147\n",
      "Epoch 1055/20000 Training Loss: 0.08502228558063507\n",
      "Epoch 1056/20000 Training Loss: 0.09282166510820389\n",
      "Epoch 1057/20000 Training Loss: 0.11790458858013153\n",
      "Epoch 1058/20000 Training Loss: 0.10237497091293335\n",
      "Epoch 1059/20000 Training Loss: 0.10673705488443375\n",
      "Epoch 1060/20000 Training Loss: 0.1014629453420639\n",
      "Epoch 1061/20000 Training Loss: 0.089913010597229\n",
      "Epoch 1062/20000 Training Loss: 0.12358115613460541\n",
      "Epoch 1063/20000 Training Loss: 0.088921457529068\n",
      "Epoch 1064/20000 Training Loss: 0.09325055032968521\n",
      "Epoch 1065/20000 Training Loss: 0.11297127604484558\n",
      "Epoch 1066/20000 Training Loss: 0.09749025106430054\n",
      "Epoch 1067/20000 Training Loss: 0.09790889173746109\n",
      "Epoch 1068/20000 Training Loss: 0.07236839830875397\n",
      "Epoch 1069/20000 Training Loss: 0.10830678045749664\n",
      "Epoch 1070/20000 Training Loss: 0.06956607103347778\n",
      "Epoch 1071/20000 Training Loss: 0.09644372761249542\n",
      "Epoch 1072/20000 Training Loss: 0.1036963239312172\n",
      "Epoch 1073/20000 Training Loss: 0.07917439192533493\n",
      "Epoch 1074/20000 Training Loss: 0.09483465552330017\n",
      "Epoch 1075/20000 Training Loss: 0.09643630683422089\n",
      "Epoch 1076/20000 Training Loss: 0.07889902591705322\n",
      "Epoch 1077/20000 Training Loss: 0.140315979719162\n",
      "Epoch 1078/20000 Training Loss: 0.10862331837415695\n",
      "Epoch 1079/20000 Training Loss: 0.09699535369873047\n",
      "Epoch 1080/20000 Training Loss: 0.07284033298492432\n",
      "Epoch 1081/20000 Training Loss: 0.10674244165420532\n",
      "Epoch 1082/20000 Training Loss: 0.09319348633289337\n",
      "Epoch 1083/20000 Training Loss: 0.09691727161407471\n",
      "Epoch 1084/20000 Training Loss: 0.0853850319981575\n",
      "Epoch 1085/20000 Training Loss: 0.11251586675643921\n",
      "Epoch 1086/20000 Training Loss: 0.11370478570461273\n",
      "Epoch 1087/20000 Training Loss: 0.09944157302379608\n",
      "Epoch 1088/20000 Training Loss: 0.0870010033249855\n",
      "Epoch 1089/20000 Training Loss: 0.12096700072288513\n",
      "Epoch 1090/20000 Training Loss: 0.11877020448446274\n",
      "Epoch 1091/20000 Training Loss: 0.08783457428216934\n",
      "Epoch 1092/20000 Training Loss: 0.10669109225273132\n",
      "Epoch 1093/20000 Training Loss: 0.11635319888591766\n",
      "Epoch 1094/20000 Training Loss: 0.0761079341173172\n",
      "Epoch 1095/20000 Training Loss: 0.10207748413085938\n",
      "Epoch 1096/20000 Training Loss: 0.0939209833741188\n",
      "Epoch 1097/20000 Training Loss: 0.09715718030929565\n",
      "Epoch 1098/20000 Training Loss: 0.07766778022050858\n",
      "Epoch 1099/20000 Training Loss: 0.09015506505966187\n",
      "Epoch 1100/20000 Training Loss: 0.08590847253799438\n",
      "Epoch 1100/20000 Validation Loss: 0.09701570123434067\n",
      "Epoch 1101/20000 Training Loss: 0.08562438189983368\n",
      "Epoch 1102/20000 Training Loss: 0.08633048087358475\n",
      "Epoch 1103/20000 Training Loss: 0.08555685728788376\n",
      "Epoch 1104/20000 Training Loss: 0.09857483208179474\n",
      "Epoch 1105/20000 Training Loss: 0.08668877929449081\n",
      "Epoch 1106/20000 Training Loss: 0.09932658076286316\n",
      "Epoch 1107/20000 Training Loss: 0.08166561275720596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1108/20000 Training Loss: 0.1146877110004425\n",
      "Epoch 1109/20000 Training Loss: 0.08819548785686493\n",
      "Epoch 1110/20000 Training Loss: 0.11670346558094025\n",
      "Epoch 1111/20000 Training Loss: 0.08873412013053894\n",
      "Epoch 1112/20000 Training Loss: 0.07790890336036682\n",
      "Epoch 1113/20000 Training Loss: 0.09151363372802734\n",
      "Epoch 1114/20000 Training Loss: 0.05908589065074921\n",
      "Epoch 1115/20000 Training Loss: 0.07415015250444412\n",
      "Epoch 1116/20000 Training Loss: 0.10542205721139908\n",
      "Epoch 1117/20000 Training Loss: 0.10069163888692856\n",
      "Epoch 1118/20000 Training Loss: 0.0715259313583374\n",
      "Epoch 1119/20000 Training Loss: 0.1202135682106018\n",
      "Epoch 1120/20000 Training Loss: 0.0877394825220108\n",
      "Epoch 1121/20000 Training Loss: 0.10654184967279434\n",
      "Epoch 1122/20000 Training Loss: 0.10269170254468918\n",
      "Epoch 1123/20000 Training Loss: 0.08827228844165802\n",
      "Epoch 1124/20000 Training Loss: 0.1141386330127716\n",
      "Epoch 1125/20000 Training Loss: 0.08675198256969452\n",
      "Epoch 1126/20000 Training Loss: 0.08964905142784119\n",
      "Epoch 1127/20000 Training Loss: 0.10678897798061371\n",
      "Epoch 1128/20000 Training Loss: 0.10091523826122284\n",
      "Epoch 1129/20000 Training Loss: 0.0846533477306366\n",
      "Epoch 1130/20000 Training Loss: 0.10608609020709991\n",
      "Epoch 1131/20000 Training Loss: 0.1106981635093689\n",
      "Epoch 1132/20000 Training Loss: 0.09301421791315079\n",
      "Epoch 1133/20000 Training Loss: 0.09485556185245514\n",
      "Epoch 1134/20000 Training Loss: 0.07091596722602844\n",
      "Epoch 1135/20000 Training Loss: 0.10903936624526978\n",
      "Epoch 1136/20000 Training Loss: 0.08687593042850494\n",
      "Epoch 1137/20000 Training Loss: 0.09887818992137909\n",
      "Epoch 1138/20000 Training Loss: 0.10810589045286179\n",
      "Epoch 1139/20000 Training Loss: 0.0754985511302948\n",
      "Epoch 1140/20000 Training Loss: 0.08532696962356567\n",
      "Epoch 1141/20000 Training Loss: 0.12979555130004883\n",
      "Epoch 1142/20000 Training Loss: 0.09316125512123108\n",
      "Epoch 1143/20000 Training Loss: 0.10490968823432922\n",
      "Epoch 1144/20000 Training Loss: 0.09385614097118378\n",
      "Epoch 1145/20000 Training Loss: 0.09494487941265106\n",
      "Epoch 1146/20000 Training Loss: 0.115822434425354\n",
      "Epoch 1147/20000 Training Loss: 0.09914319217205048\n",
      "Epoch 1148/20000 Training Loss: 0.07696905732154846\n",
      "Epoch 1149/20000 Training Loss: 0.08791191875934601\n",
      "Epoch 1150/20000 Training Loss: 0.07638759166002274\n",
      "Epoch 1151/20000 Training Loss: 0.08852700889110565\n",
      "Epoch 1152/20000 Training Loss: 0.08962640911340714\n",
      "Epoch 1153/20000 Training Loss: 0.07807684689760208\n",
      "Epoch 1154/20000 Training Loss: 0.14817458391189575\n",
      "Epoch 1155/20000 Training Loss: 0.09154868125915527\n",
      "Epoch 1156/20000 Training Loss: 0.06898286938667297\n",
      "Epoch 1157/20000 Training Loss: 0.08177370578050613\n",
      "Epoch 1158/20000 Training Loss: 0.07503125071525574\n",
      "Epoch 1159/20000 Training Loss: 0.09843072295188904\n",
      "Epoch 1160/20000 Training Loss: 0.06575462967157364\n",
      "Epoch 1161/20000 Training Loss: 0.09318387508392334\n",
      "Epoch 1162/20000 Training Loss: 0.09365852922201157\n",
      "Epoch 1163/20000 Training Loss: 0.09083546698093414\n",
      "Epoch 1164/20000 Training Loss: 0.10252818465232849\n",
      "Epoch 1165/20000 Training Loss: 0.08777390420436859\n",
      "Epoch 1166/20000 Training Loss: 0.08855858445167542\n",
      "Epoch 1167/20000 Training Loss: 0.08726529777050018\n",
      "Epoch 1168/20000 Training Loss: 0.09835264086723328\n",
      "Epoch 1169/20000 Training Loss: 0.10393809527158737\n",
      "Epoch 1170/20000 Training Loss: 0.08323333412408829\n",
      "Epoch 1171/20000 Training Loss: 0.05640244483947754\n",
      "Epoch 1172/20000 Training Loss: 0.08336599171161652\n",
      "Epoch 1173/20000 Training Loss: 0.06704850494861603\n",
      "Epoch 1174/20000 Training Loss: 0.07948952913284302\n",
      "Epoch 1175/20000 Training Loss: 0.08533535152673721\n",
      "Epoch 1176/20000 Training Loss: 0.08665502816438675\n",
      "Epoch 1177/20000 Training Loss: 0.10055100172758102\n",
      "Epoch 1178/20000 Training Loss: 0.1283402144908905\n",
      "Epoch 1179/20000 Training Loss: 0.08758112043142319\n",
      "Epoch 1180/20000 Training Loss: 0.07726326584815979\n",
      "Epoch 1181/20000 Training Loss: 0.09881378710269928\n",
      "Epoch 1182/20000 Training Loss: 0.09164714813232422\n",
      "Epoch 1183/20000 Training Loss: 0.10114467144012451\n",
      "Epoch 1184/20000 Training Loss: 0.0960097685456276\n",
      "Epoch 1185/20000 Training Loss: 0.0889502465724945\n",
      "Epoch 1186/20000 Training Loss: 0.09407249093055725\n",
      "Epoch 1187/20000 Training Loss: 0.06880737841129303\n",
      "Epoch 1188/20000 Training Loss: 0.07172597944736481\n",
      "Epoch 1189/20000 Training Loss: 0.11624464392662048\n",
      "Epoch 1190/20000 Training Loss: 0.08600661158561707\n",
      "Epoch 1191/20000 Training Loss: 0.08900144696235657\n",
      "Epoch 1192/20000 Training Loss: 0.12666751444339752\n",
      "Epoch 1193/20000 Training Loss: 0.0878034383058548\n",
      "Epoch 1194/20000 Training Loss: 0.07993359863758087\n",
      "Epoch 1195/20000 Training Loss: 0.06970186531543732\n",
      "Epoch 1196/20000 Training Loss: 0.10908607393503189\n",
      "Epoch 1197/20000 Training Loss: 0.08236254751682281\n",
      "Epoch 1198/20000 Training Loss: 0.08798454701900482\n",
      "Epoch 1199/20000 Training Loss: 0.08705867826938629\n",
      "Epoch 1200/20000 Training Loss: 0.09074652194976807\n",
      "Epoch 1200/20000 Validation Loss: 0.09277287125587463\n",
      "Epoch 1201/20000 Training Loss: 0.05972832441329956\n",
      "Epoch 1202/20000 Training Loss: 0.09535954892635345\n",
      "Epoch 1203/20000 Training Loss: 0.1059969812631607\n",
      "Epoch 1204/20000 Training Loss: 0.08986363559961319\n",
      "Epoch 1205/20000 Training Loss: 0.08791302144527435\n",
      "Epoch 1206/20000 Training Loss: 0.07605847716331482\n",
      "Epoch 1207/20000 Training Loss: 0.08988912403583527\n",
      "Epoch 1208/20000 Training Loss: 0.09000779688358307\n",
      "Epoch 1209/20000 Training Loss: 0.08947040140628815\n",
      "Epoch 1210/20000 Training Loss: 0.10626262426376343\n",
      "Epoch 1211/20000 Training Loss: 0.1063072606921196\n",
      "Epoch 1212/20000 Training Loss: 0.0908471941947937\n",
      "Epoch 1213/20000 Training Loss: 0.10402847081422806\n",
      "Epoch 1214/20000 Training Loss: 0.10227248817682266\n",
      "Epoch 1215/20000 Training Loss: 0.09949123859405518\n",
      "Epoch 1216/20000 Training Loss: 0.09367373585700989\n",
      "Epoch 1217/20000 Training Loss: 0.0763932466506958\n",
      "Epoch 1218/20000 Training Loss: 0.08886153995990753\n",
      "Epoch 1219/20000 Training Loss: 0.0902867391705513\n",
      "Epoch 1220/20000 Training Loss: 0.08754289895296097\n",
      "Epoch 1221/20000 Training Loss: 0.09173846244812012\n",
      "Epoch 1222/20000 Training Loss: 0.11101604998111725\n",
      "Epoch 1223/20000 Training Loss: 0.09029513597488403\n",
      "Epoch 1224/20000 Training Loss: 0.0987398698925972\n",
      "Epoch 1225/20000 Training Loss: 0.079813152551651\n",
      "Epoch 1226/20000 Training Loss: 0.08826696127653122\n",
      "Epoch 1227/20000 Training Loss: 0.0690227523446083\n",
      "Epoch 1228/20000 Training Loss: 0.10236015170812607\n",
      "Epoch 1229/20000 Training Loss: 0.0643615573644638\n",
      "Epoch 1230/20000 Training Loss: 0.08089113980531693\n",
      "Epoch 1231/20000 Training Loss: 0.09718458354473114\n",
      "Epoch 1232/20000 Training Loss: 0.09562774747610092\n",
      "Epoch 1233/20000 Training Loss: 0.08808475732803345\n",
      "Epoch 1234/20000 Training Loss: 0.09502068907022476\n",
      "Epoch 1235/20000 Training Loss: 0.09289573132991791\n",
      "Epoch 1236/20000 Training Loss: 0.07173876464366913\n",
      "Epoch 1237/20000 Training Loss: 0.08977658301591873\n",
      "Epoch 1238/20000 Training Loss: 0.08602635562419891\n",
      "Epoch 1239/20000 Training Loss: 0.08299247920513153\n",
      "Epoch 1240/20000 Training Loss: 0.07561948895454407\n",
      "Epoch 1241/20000 Training Loss: 0.0886945128440857\n",
      "Epoch 1242/20000 Training Loss: 0.08666347712278366\n",
      "Epoch 1243/20000 Training Loss: 0.07323762774467468\n",
      "Epoch 1244/20000 Training Loss: 0.08329795300960541\n",
      "Epoch 1245/20000 Training Loss: 0.10521159321069717\n",
      "Epoch 1246/20000 Training Loss: 0.08209701627492905\n",
      "Epoch 1247/20000 Training Loss: 0.08672782778739929\n",
      "Epoch 1248/20000 Training Loss: 0.08471930027008057\n",
      "Epoch 1249/20000 Training Loss: 0.08733972162008286\n",
      "Epoch 1250/20000 Training Loss: 0.09401382505893707\n",
      "Epoch 1251/20000 Training Loss: 0.08357445895671844\n",
      "Epoch 1252/20000 Training Loss: 0.06023057550191879\n",
      "Epoch 1253/20000 Training Loss: 0.1238052025437355\n",
      "Epoch 1254/20000 Training Loss: 0.08706632256507874\n",
      "Epoch 1255/20000 Training Loss: 0.08421207219362259\n",
      "Epoch 1256/20000 Training Loss: 0.09853880107402802\n",
      "Epoch 1257/20000 Training Loss: 0.07007050514221191\n",
      "Epoch 1258/20000 Training Loss: 0.09986250847578049\n",
      "Epoch 1259/20000 Training Loss: 0.1184060201048851\n",
      "Epoch 1260/20000 Training Loss: 0.10007815062999725\n",
      "Epoch 1261/20000 Training Loss: 0.08442553877830505\n",
      "Epoch 1262/20000 Training Loss: 0.08554864674806595\n",
      "Epoch 1263/20000 Training Loss: 0.09818212687969208\n",
      "Epoch 1264/20000 Training Loss: 0.11523054540157318\n",
      "Epoch 1265/20000 Training Loss: 0.10851557552814484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1266/20000 Training Loss: 0.08495499938726425\n",
      "Epoch 1267/20000 Training Loss: 0.09869742393493652\n",
      "Epoch 1268/20000 Training Loss: 0.07282255589962006\n",
      "Epoch 1269/20000 Training Loss: 0.08334369957447052\n",
      "Epoch 1270/20000 Training Loss: 0.09802514314651489\n",
      "Epoch 1271/20000 Training Loss: 0.08664926886558533\n",
      "Epoch 1272/20000 Training Loss: 0.10844458639621735\n",
      "Epoch 1273/20000 Training Loss: 0.10471165925264359\n",
      "Epoch 1274/20000 Training Loss: 0.08278769254684448\n",
      "Epoch 1275/20000 Training Loss: 0.07470972836017609\n",
      "Epoch 1276/20000 Training Loss: 0.08642743527889252\n",
      "Epoch 1277/20000 Training Loss: 0.09081956744194031\n",
      "Epoch 1278/20000 Training Loss: 0.09465885162353516\n",
      "Epoch 1279/20000 Training Loss: 0.09097204357385635\n",
      "Epoch 1280/20000 Training Loss: 0.08165188133716583\n",
      "Epoch 1281/20000 Training Loss: 0.0957711935043335\n",
      "Epoch 1282/20000 Training Loss: 0.11478348821401596\n",
      "Epoch 1283/20000 Training Loss: 0.08372842520475388\n",
      "Epoch 1284/20000 Training Loss: 0.09490227699279785\n",
      "Epoch 1285/20000 Training Loss: 0.07379138469696045\n",
      "Epoch 1286/20000 Training Loss: 0.0845746323466301\n",
      "Epoch 1287/20000 Training Loss: 0.0909833014011383\n",
      "Epoch 1288/20000 Training Loss: 0.09122639894485474\n",
      "Epoch 1289/20000 Training Loss: 0.08971764147281647\n",
      "Epoch 1290/20000 Training Loss: 0.08315692842006683\n",
      "Epoch 1291/20000 Training Loss: 0.08520367741584778\n",
      "Epoch 1292/20000 Training Loss: 0.07421442866325378\n",
      "Epoch 1293/20000 Training Loss: 0.07674786448478699\n",
      "Epoch 1294/20000 Training Loss: 0.07970203459262848\n",
      "Epoch 1295/20000 Training Loss: 0.10919952392578125\n",
      "Epoch 1296/20000 Training Loss: 0.095978744328022\n",
      "Epoch 1297/20000 Training Loss: 0.08233381062746048\n",
      "Epoch 1298/20000 Training Loss: 0.07467807084321976\n",
      "Epoch 1299/20000 Training Loss: 0.08617761731147766\n",
      "Epoch 1300/20000 Training Loss: 0.08081051707267761\n",
      "Epoch 1300/20000 Validation Loss: 0.10573489218950272\n",
      "Epoch 1301/20000 Training Loss: 0.0796225368976593\n",
      "Epoch 1302/20000 Training Loss: 0.0883062332868576\n",
      "Epoch 1303/20000 Training Loss: 0.09168741106987\n",
      "Epoch 1304/20000 Training Loss: 0.10086018592119217\n",
      "Epoch 1305/20000 Training Loss: 0.09051796793937683\n",
      "Epoch 1306/20000 Training Loss: 0.09737493842840195\n",
      "Epoch 1307/20000 Training Loss: 0.07552342861890793\n",
      "Epoch 1308/20000 Training Loss: 0.09755328297615051\n",
      "Epoch 1309/20000 Training Loss: 0.08941385895013809\n",
      "Epoch 1310/20000 Training Loss: 0.08864721655845642\n",
      "Epoch 1311/20000 Training Loss: 0.08572050929069519\n",
      "Epoch 1312/20000 Training Loss: 0.06217579543590546\n",
      "Epoch 1313/20000 Training Loss: 0.09896807372570038\n",
      "Epoch 1314/20000 Training Loss: 0.10447445511817932\n",
      "Epoch 1315/20000 Training Loss: 0.0865967869758606\n",
      "Epoch 1316/20000 Training Loss: 0.0800386443734169\n",
      "Epoch 1317/20000 Training Loss: 0.08118084073066711\n",
      "Epoch 1318/20000 Training Loss: 0.09816492348909378\n",
      "Epoch 1319/20000 Training Loss: 0.0696118101477623\n",
      "Epoch 1320/20000 Training Loss: 0.09121875464916229\n",
      "Epoch 1321/20000 Training Loss: 0.08475776761770248\n",
      "Epoch 1322/20000 Training Loss: 0.11065030097961426\n",
      "Epoch 1323/20000 Training Loss: 0.09860251843929291\n",
      "Epoch 1324/20000 Training Loss: 0.09752580523490906\n",
      "Epoch 1325/20000 Training Loss: 0.1023973673582077\n",
      "Epoch 1326/20000 Training Loss: 0.08087112754583359\n",
      "Epoch 1327/20000 Training Loss: 0.06719718128442764\n",
      "Epoch 1328/20000 Training Loss: 0.09568482637405396\n",
      "Epoch 1329/20000 Training Loss: 0.07914021611213684\n",
      "Epoch 1330/20000 Training Loss: 0.08579783141613007\n",
      "Epoch 1331/20000 Training Loss: 0.10516543686389923\n",
      "Epoch 1332/20000 Training Loss: 0.0935247391462326\n",
      "Epoch 1333/20000 Training Loss: 0.07654527574777603\n",
      "Epoch 1334/20000 Training Loss: 0.08617928624153137\n",
      "Epoch 1335/20000 Training Loss: 0.08424502611160278\n",
      "Epoch 1336/20000 Training Loss: 0.07972241938114166\n",
      "Epoch 1337/20000 Training Loss: 0.09330655634403229\n",
      "Epoch 1338/20000 Training Loss: 0.09052254259586334\n",
      "Epoch 1339/20000 Training Loss: 0.10537122189998627\n",
      "Epoch 1340/20000 Training Loss: 0.08369432389736176\n",
      "Epoch 1341/20000 Training Loss: 0.10355592519044876\n",
      "Epoch 1342/20000 Training Loss: 0.07746457308530807\n",
      "Epoch 1343/20000 Training Loss: 0.07920067012310028\n",
      "Epoch 1344/20000 Training Loss: 0.09556728601455688\n",
      "Epoch 1345/20000 Training Loss: 0.10674746334552765\n",
      "Epoch 1346/20000 Training Loss: 0.10027813911437988\n",
      "Epoch 1347/20000 Training Loss: 0.0867840051651001\n",
      "Epoch 1348/20000 Training Loss: 0.1309724748134613\n",
      "Epoch 1349/20000 Training Loss: 0.07156235724687576\n",
      "Epoch 1350/20000 Training Loss: 0.08099417388439178\n",
      "Epoch 1351/20000 Training Loss: 0.09561125189065933\n",
      "Epoch 1352/20000 Training Loss: 0.0902789831161499\n",
      "Epoch 1353/20000 Training Loss: 0.07407592982053757\n",
      "Epoch 1354/20000 Training Loss: 0.08047400414943695\n",
      "Epoch 1355/20000 Training Loss: 0.07954738289117813\n",
      "Epoch 1356/20000 Training Loss: 0.07824733108282089\n",
      "Epoch 1357/20000 Training Loss: 0.07222850620746613\n",
      "Epoch 1358/20000 Training Loss: 0.08087559044361115\n",
      "Epoch 1359/20000 Training Loss: 0.08382150530815125\n",
      "Epoch 1360/20000 Training Loss: 0.1002683937549591\n",
      "Epoch 1361/20000 Training Loss: 0.07260206341743469\n",
      "Epoch 1362/20000 Training Loss: 0.09001302719116211\n",
      "Epoch 1363/20000 Training Loss: 0.07413531839847565\n",
      "Epoch 1364/20000 Training Loss: 0.10034866631031036\n",
      "Epoch 1365/20000 Training Loss: 0.08086597919464111\n",
      "Epoch 1366/20000 Training Loss: 0.0893731415271759\n",
      "Epoch 1367/20000 Training Loss: 0.08409525454044342\n",
      "Epoch 1368/20000 Training Loss: 0.08513631671667099\n",
      "Epoch 1369/20000 Training Loss: 0.08500546216964722\n",
      "Epoch 1370/20000 Training Loss: 0.08767500519752502\n",
      "Epoch 1371/20000 Training Loss: 0.06453592330217361\n",
      "Epoch 1372/20000 Training Loss: 0.10408642888069153\n",
      "Epoch 1373/20000 Training Loss: 0.07800628244876862\n",
      "Epoch 1374/20000 Training Loss: 0.08066931366920471\n",
      "Epoch 1375/20000 Training Loss: 0.0726114958524704\n",
      "Epoch 1376/20000 Training Loss: 0.07886240631341934\n",
      "Epoch 1377/20000 Training Loss: 0.09822709858417511\n",
      "Epoch 1378/20000 Training Loss: 0.09235106408596039\n",
      "Epoch 1379/20000 Training Loss: 0.08823777735233307\n",
      "Epoch 1380/20000 Training Loss: 0.06026718020439148\n",
      "Epoch 1381/20000 Training Loss: 0.09459684044122696\n",
      "Epoch 1382/20000 Training Loss: 0.09022882580757141\n",
      "Epoch 1383/20000 Training Loss: 0.09039343893527985\n",
      "Epoch 1384/20000 Training Loss: 0.07949022203683853\n",
      "Epoch 1385/20000 Training Loss: 0.08229933679103851\n",
      "Epoch 1386/20000 Training Loss: 0.07001862674951553\n",
      "Epoch 1387/20000 Training Loss: 0.07634580880403519\n",
      "Epoch 1388/20000 Training Loss: 0.07476034760475159\n",
      "Epoch 1389/20000 Training Loss: 0.09325212240219116\n",
      "Epoch 1390/20000 Training Loss: 0.08098053932189941\n",
      "Epoch 1391/20000 Training Loss: 0.09391236305236816\n",
      "Epoch 1392/20000 Training Loss: 0.07980838418006897\n",
      "Epoch 1393/20000 Training Loss: 0.06033310294151306\n",
      "Epoch 1394/20000 Training Loss: 0.10761354863643646\n",
      "Epoch 1395/20000 Training Loss: 0.08271339535713196\n",
      "Epoch 1396/20000 Training Loss: 0.09294825047254562\n",
      "Epoch 1397/20000 Training Loss: 0.09637562930583954\n",
      "Epoch 1398/20000 Training Loss: 0.08262693881988525\n",
      "Epoch 1399/20000 Training Loss: 0.08536291122436523\n",
      "Epoch 1400/20000 Training Loss: 0.07036717981100082\n",
      "Epoch 1400/20000 Validation Loss: 0.11098809540271759\n",
      "Epoch 1401/20000 Training Loss: 0.07776527106761932\n",
      "Epoch 1402/20000 Training Loss: 0.07338850200176239\n",
      "Epoch 1403/20000 Training Loss: 0.07958196103572845\n",
      "Epoch 1404/20000 Training Loss: 0.06247591972351074\n",
      "Epoch 1405/20000 Training Loss: 0.09093070030212402\n",
      "Epoch 1406/20000 Training Loss: 0.08365742117166519\n",
      "Epoch 1407/20000 Training Loss: 0.11463950574398041\n",
      "Epoch 1408/20000 Training Loss: 0.07612442970275879\n",
      "Epoch 1409/20000 Training Loss: 0.08808969706296921\n",
      "Epoch 1410/20000 Training Loss: 0.08946305513381958\n",
      "Epoch 1411/20000 Training Loss: 0.09775377810001373\n",
      "Epoch 1412/20000 Training Loss: 0.10067100077867508\n",
      "Epoch 1413/20000 Training Loss: 0.08158248662948608\n",
      "Epoch 1414/20000 Training Loss: 0.0806741788983345\n",
      "Epoch 1415/20000 Training Loss: 0.0720551460981369\n",
      "Epoch 1416/20000 Training Loss: 0.08621231466531754\n",
      "Epoch 1417/20000 Training Loss: 0.09128789603710175\n",
      "Epoch 1418/20000 Training Loss: 0.09153395891189575\n",
      "Epoch 1419/20000 Training Loss: 0.08222807943820953\n",
      "Epoch 1420/20000 Training Loss: 0.0819690003991127\n",
      "Epoch 1421/20000 Training Loss: 0.07339860498905182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1422/20000 Training Loss: 0.07568426430225372\n",
      "Epoch 1423/20000 Training Loss: 0.1104271411895752\n",
      "Epoch 1424/20000 Training Loss: 0.08180087804794312\n",
      "Epoch 1425/20000 Training Loss: 0.08167336136102676\n",
      "Epoch 1426/20000 Training Loss: 0.09346950054168701\n",
      "Epoch 1427/20000 Training Loss: 0.09481096267700195\n",
      "Epoch 1428/20000 Training Loss: 0.07061874866485596\n",
      "Epoch 1429/20000 Training Loss: 0.062067583203315735\n",
      "Epoch 1430/20000 Training Loss: 0.07248357683420181\n",
      "Epoch 1431/20000 Training Loss: 0.09149667620658875\n",
      "Epoch 1432/20000 Training Loss: 0.08243943005800247\n",
      "Epoch 1433/20000 Training Loss: 0.0921563059091568\n",
      "Epoch 1434/20000 Training Loss: 0.08191734552383423\n",
      "Epoch 1435/20000 Training Loss: 0.07947133481502533\n",
      "Epoch 1436/20000 Training Loss: 0.08479572087526321\n",
      "Epoch 1437/20000 Training Loss: 0.07280604541301727\n",
      "Epoch 1438/20000 Training Loss: 0.10093037784099579\n",
      "Epoch 1439/20000 Training Loss: 0.055840469896793365\n",
      "Epoch 1440/20000 Training Loss: 0.10556149482727051\n",
      "Epoch 1441/20000 Training Loss: 0.09634712338447571\n",
      "Epoch 1442/20000 Training Loss: 0.08032132685184479\n",
      "Epoch 1443/20000 Training Loss: 0.08365145325660706\n",
      "Epoch 1444/20000 Training Loss: 0.07971993088722229\n",
      "Epoch 1445/20000 Training Loss: 0.09763114154338837\n",
      "Epoch 1446/20000 Training Loss: 0.11250776052474976\n",
      "Epoch 1447/20000 Training Loss: 0.07081463932991028\n",
      "Epoch 1448/20000 Training Loss: 0.08182810246944427\n",
      "Epoch 1449/20000 Training Loss: 0.07046341896057129\n",
      "Epoch 1450/20000 Training Loss: 0.07063782215118408\n",
      "Epoch 1451/20000 Training Loss: 0.06997035443782806\n",
      "Epoch 1452/20000 Training Loss: 0.06508646160364151\n",
      "Epoch 1453/20000 Training Loss: 0.07922172546386719\n",
      "Epoch 1454/20000 Training Loss: 0.09729160368442535\n",
      "Epoch 1455/20000 Training Loss: 0.0818924754858017\n",
      "Epoch 1456/20000 Training Loss: 0.07968597114086151\n",
      "Epoch 1457/20000 Training Loss: 0.08563444018363953\n",
      "Epoch 1458/20000 Training Loss: 0.10531092435121536\n",
      "Epoch 1459/20000 Training Loss: 0.07254760712385178\n",
      "Epoch 1460/20000 Training Loss: 0.10380349308252335\n",
      "Epoch 1461/20000 Training Loss: 0.08930082619190216\n",
      "Epoch 1462/20000 Training Loss: 0.09296361356973648\n",
      "Epoch 1463/20000 Training Loss: 0.06545741856098175\n",
      "Epoch 1464/20000 Training Loss: 0.07977394759654999\n",
      "Epoch 1465/20000 Training Loss: 0.10087195783853531\n",
      "Epoch 1466/20000 Training Loss: 0.07779663801193237\n",
      "Epoch 1467/20000 Training Loss: 0.0979631096124649\n",
      "Epoch 1468/20000 Training Loss: 0.09791158139705658\n",
      "Epoch 1469/20000 Training Loss: 0.08085478097200394\n",
      "Epoch 1470/20000 Training Loss: 0.08558489382266998\n",
      "Epoch 1471/20000 Training Loss: 0.062189869582653046\n",
      "Epoch 1472/20000 Training Loss: 0.06968139111995697\n",
      "Epoch 1473/20000 Training Loss: 0.07756944000720978\n",
      "Epoch 1474/20000 Training Loss: 0.10582701861858368\n",
      "Epoch 1475/20000 Training Loss: 0.07883431017398834\n",
      "Epoch 1476/20000 Training Loss: 0.0778319388628006\n",
      "Epoch 1477/20000 Training Loss: 0.09286309033632278\n",
      "Epoch 1478/20000 Training Loss: 0.07699261605739594\n",
      "Epoch 1479/20000 Training Loss: 0.08349649608135223\n",
      "Epoch 1480/20000 Training Loss: 0.07518092542886734\n",
      "Epoch 1481/20000 Training Loss: 0.09060434997081757\n",
      "Epoch 1482/20000 Training Loss: 0.0820612832903862\n",
      "Epoch 1483/20000 Training Loss: 0.09188110381364822\n",
      "Epoch 1484/20000 Training Loss: 0.1068473607301712\n",
      "Epoch 1485/20000 Training Loss: 0.12037386000156403\n",
      "Epoch 1486/20000 Training Loss: 0.07278428971767426\n",
      "Epoch 1487/20000 Training Loss: 0.09142101556062698\n",
      "Epoch 1488/20000 Training Loss: 0.10215669870376587\n",
      "Epoch 1489/20000 Training Loss: 0.0744439959526062\n",
      "Epoch 1490/20000 Training Loss: 0.0730929970741272\n",
      "Epoch 1491/20000 Training Loss: 0.054697435349226\n",
      "Epoch 1492/20000 Training Loss: 0.07819902896881104\n",
      "Epoch 1493/20000 Training Loss: 0.09091739356517792\n",
      "Epoch 1494/20000 Training Loss: 0.07377161830663681\n",
      "Epoch 1495/20000 Training Loss: 0.072836734354496\n",
      "Epoch 1496/20000 Training Loss: 0.07493608444929123\n",
      "Epoch 1497/20000 Training Loss: 0.10047932714223862\n",
      "Epoch 1498/20000 Training Loss: 0.07851836085319519\n",
      "Epoch 1499/20000 Training Loss: 0.09766671061515808\n",
      "Epoch 1500/20000 Training Loss: 0.08031157404184341\n",
      "Epoch 1500/20000 Validation Loss: 0.06317239254713058\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.06317239254713058<=============\n",
      "Epoch 1501/20000 Training Loss: 0.06569355726242065\n",
      "Epoch 1502/20000 Training Loss: 0.09562113881111145\n",
      "Epoch 1503/20000 Training Loss: 0.07950417697429657\n",
      "Epoch 1504/20000 Training Loss: 0.07998925447463989\n",
      "Epoch 1505/20000 Training Loss: 0.0747804045677185\n",
      "Epoch 1506/20000 Training Loss: 0.10224084556102753\n",
      "Epoch 1507/20000 Training Loss: 0.07045599818229675\n",
      "Epoch 1508/20000 Training Loss: 0.06810425221920013\n",
      "Epoch 1509/20000 Training Loss: 0.0751751959323883\n",
      "Epoch 1510/20000 Training Loss: 0.07342170178890228\n",
      "Epoch 1511/20000 Training Loss: 0.08894574642181396\n",
      "Epoch 1512/20000 Training Loss: 0.08168751001358032\n",
      "Epoch 1513/20000 Training Loss: 0.1141243651509285\n",
      "Epoch 1514/20000 Training Loss: 0.07924816012382507\n",
      "Epoch 1515/20000 Training Loss: 0.0692249983549118\n",
      "Epoch 1516/20000 Training Loss: 0.08533204346895218\n",
      "Epoch 1517/20000 Training Loss: 0.0803191289305687\n",
      "Epoch 1518/20000 Training Loss: 0.08091376721858978\n",
      "Epoch 1519/20000 Training Loss: 0.06831993162631989\n",
      "Epoch 1520/20000 Training Loss: 0.06112413480877876\n",
      "Epoch 1521/20000 Training Loss: 0.0911005437374115\n",
      "Epoch 1522/20000 Training Loss: 0.12128706276416779\n",
      "Epoch 1523/20000 Training Loss: 0.08825325220823288\n",
      "Epoch 1524/20000 Training Loss: 0.06794129312038422\n",
      "Epoch 1525/20000 Training Loss: 0.10951542854309082\n",
      "Epoch 1526/20000 Training Loss: 0.06692862510681152\n",
      "Epoch 1527/20000 Training Loss: 0.06283385306596756\n",
      "Epoch 1528/20000 Training Loss: 0.07081365585327148\n",
      "Epoch 1529/20000 Training Loss: 0.10625059902667999\n",
      "Epoch 1530/20000 Training Loss: 0.08707863092422485\n",
      "Epoch 1531/20000 Training Loss: 0.07339055091142654\n",
      "Epoch 1532/20000 Training Loss: 0.09008544683456421\n",
      "Epoch 1533/20000 Training Loss: 0.0935748741030693\n",
      "Epoch 1534/20000 Training Loss: 0.08914255350828171\n",
      "Epoch 1535/20000 Training Loss: 0.07641024142503738\n",
      "Epoch 1536/20000 Training Loss: 0.06103240326046944\n",
      "Epoch 1537/20000 Training Loss: 0.09522651135921478\n",
      "Epoch 1538/20000 Training Loss: 0.08019357919692993\n",
      "Epoch 1539/20000 Training Loss: 0.09302521497011185\n",
      "Epoch 1540/20000 Training Loss: 0.09068429470062256\n",
      "Epoch 1541/20000 Training Loss: 0.08931282162666321\n",
      "Epoch 1542/20000 Training Loss: 0.07036730647087097\n",
      "Epoch 1543/20000 Training Loss: 0.07151692360639572\n",
      "Epoch 1544/20000 Training Loss: 0.08854089677333832\n",
      "Epoch 1545/20000 Training Loss: 0.09373718500137329\n",
      "Epoch 1546/20000 Training Loss: 0.09610303491353989\n",
      "Epoch 1547/20000 Training Loss: 0.09184823930263519\n",
      "Epoch 1548/20000 Training Loss: 0.09455610811710358\n",
      "Epoch 1549/20000 Training Loss: 0.08158052712678909\n",
      "Epoch 1550/20000 Training Loss: 0.06910954415798187\n",
      "Epoch 1551/20000 Training Loss: 0.07824137061834335\n",
      "Epoch 1552/20000 Training Loss: 0.06332464516162872\n",
      "Epoch 1553/20000 Training Loss: 0.06501001119613647\n",
      "Epoch 1554/20000 Training Loss: 0.06486145406961441\n",
      "Epoch 1555/20000 Training Loss: 0.06444405764341354\n",
      "Epoch 1556/20000 Training Loss: 0.07594302296638489\n",
      "Epoch 1557/20000 Training Loss: 0.09667837619781494\n",
      "Epoch 1558/20000 Training Loss: 0.08463642001152039\n",
      "Epoch 1559/20000 Training Loss: 0.09678951650857925\n",
      "Epoch 1560/20000 Training Loss: 0.08222897350788116\n",
      "Epoch 1561/20000 Training Loss: 0.10436064004898071\n",
      "Epoch 1562/20000 Training Loss: 0.08173084259033203\n",
      "Epoch 1563/20000 Training Loss: 0.0848156064748764\n",
      "Epoch 1564/20000 Training Loss: 0.07729825377464294\n",
      "Epoch 1565/20000 Training Loss: 0.11106336116790771\n",
      "Epoch 1566/20000 Training Loss: 0.09347732365131378\n",
      "Epoch 1567/20000 Training Loss: 0.052805934101343155\n",
      "Epoch 1568/20000 Training Loss: 0.08525209128856659\n",
      "Epoch 1569/20000 Training Loss: 0.0840914398431778\n",
      "Epoch 1570/20000 Training Loss: 0.05741666257381439\n",
      "Epoch 1571/20000 Training Loss: 0.0680389478802681\n",
      "Epoch 1572/20000 Training Loss: 0.06760253012180328\n",
      "Epoch 1573/20000 Training Loss: 0.08059784770011902\n",
      "Epoch 1574/20000 Training Loss: 0.0718001052737236\n",
      "Epoch 1575/20000 Training Loss: 0.06792934238910675\n",
      "Epoch 1576/20000 Training Loss: 0.08504952490329742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1577/20000 Training Loss: 0.08383882790803909\n",
      "Epoch 1578/20000 Training Loss: 0.08128803968429565\n",
      "Epoch 1579/20000 Training Loss: 0.06009459123015404\n",
      "Epoch 1580/20000 Training Loss: 0.100278839468956\n",
      "Epoch 1581/20000 Training Loss: 0.06866971403360367\n",
      "Epoch 1582/20000 Training Loss: 0.06306487321853638\n",
      "Epoch 1583/20000 Training Loss: 0.11055385321378708\n",
      "Epoch 1584/20000 Training Loss: 0.09861545264720917\n",
      "Epoch 1585/20000 Training Loss: 0.0857110247015953\n",
      "Epoch 1586/20000 Training Loss: 0.09994504600763321\n",
      "Epoch 1587/20000 Training Loss: 0.07851658761501312\n",
      "Epoch 1588/20000 Training Loss: 0.07426037639379501\n",
      "Epoch 1589/20000 Training Loss: 0.08015837520360947\n",
      "Epoch 1590/20000 Training Loss: 0.07087844610214233\n",
      "Epoch 1591/20000 Training Loss: 0.07841897010803223\n",
      "Epoch 1592/20000 Training Loss: 0.0928104966878891\n",
      "Epoch 1593/20000 Training Loss: 0.06841041892766953\n",
      "Epoch 1594/20000 Training Loss: 0.08628237992525101\n",
      "Epoch 1595/20000 Training Loss: 0.10040010511875153\n",
      "Epoch 1596/20000 Training Loss: 0.11402957141399384\n",
      "Epoch 1597/20000 Training Loss: 0.09123407304286957\n",
      "Epoch 1598/20000 Training Loss: 0.07373470067977905\n",
      "Epoch 1599/20000 Training Loss: 0.10178716480731964\n",
      "Epoch 1600/20000 Training Loss: 0.09621749818325043\n",
      "Epoch 1600/20000 Validation Loss: 0.08910679817199707\n",
      "Epoch 1601/20000 Training Loss: 0.08525414764881134\n",
      "Epoch 1602/20000 Training Loss: 0.07447180896997452\n",
      "Epoch 1603/20000 Training Loss: 0.07979776710271835\n",
      "Epoch 1604/20000 Training Loss: 0.08680669218301773\n",
      "Epoch 1605/20000 Training Loss: 0.08674800395965576\n",
      "Epoch 1606/20000 Training Loss: 0.09893785417079926\n",
      "Epoch 1607/20000 Training Loss: 0.10574940592050552\n",
      "Epoch 1608/20000 Training Loss: 0.08100098371505737\n",
      "Epoch 1609/20000 Training Loss: 0.08861614018678665\n",
      "Epoch 1610/20000 Training Loss: 0.08684758841991425\n",
      "Epoch 1611/20000 Training Loss: 0.0821039229631424\n",
      "Epoch 1612/20000 Training Loss: 0.06333442777395248\n",
      "Epoch 1613/20000 Training Loss: 0.062042027711868286\n",
      "Epoch 1614/20000 Training Loss: 0.1012694463133812\n",
      "Epoch 1615/20000 Training Loss: 0.08592548966407776\n",
      "Epoch 1616/20000 Training Loss: 0.08402127027511597\n",
      "Epoch 1617/20000 Training Loss: 0.10582888126373291\n",
      "Epoch 1618/20000 Training Loss: 0.08991508185863495\n",
      "Epoch 1619/20000 Training Loss: 0.07467731088399887\n",
      "Epoch 1620/20000 Training Loss: 0.07873141765594482\n",
      "Epoch 1621/20000 Training Loss: 0.06607016175985336\n",
      "Epoch 1622/20000 Training Loss: 0.09050098806619644\n",
      "Epoch 1623/20000 Training Loss: 0.09337274730205536\n",
      "Epoch 1624/20000 Training Loss: 0.10015042871236801\n",
      "Epoch 1625/20000 Training Loss: 0.08004405349493027\n",
      "Epoch 1626/20000 Training Loss: 0.0990670919418335\n",
      "Epoch 1627/20000 Training Loss: 0.08083048462867737\n",
      "Epoch 1628/20000 Training Loss: 0.0821034237742424\n",
      "Epoch 1629/20000 Training Loss: 0.08802017569541931\n",
      "Epoch 1630/20000 Training Loss: 0.10083644092082977\n",
      "Epoch 1631/20000 Training Loss: 0.0887516587972641\n",
      "Epoch 1632/20000 Training Loss: 0.08554532378911972\n",
      "Epoch 1633/20000 Training Loss: 0.09189623594284058\n",
      "Epoch 1634/20000 Training Loss: 0.09140099585056305\n",
      "Epoch 1635/20000 Training Loss: 0.08914507925510406\n",
      "Epoch 1636/20000 Training Loss: 0.0813460648059845\n",
      "Epoch 1637/20000 Training Loss: 0.08282453566789627\n",
      "Epoch 1638/20000 Training Loss: 0.06287188827991486\n",
      "Epoch 1639/20000 Training Loss: 0.06159219890832901\n",
      "Epoch 1640/20000 Training Loss: 0.09161538630723953\n",
      "Epoch 1641/20000 Training Loss: 0.09274439513683319\n",
      "Epoch 1642/20000 Training Loss: 0.09718222916126251\n",
      "Epoch 1643/20000 Training Loss: 0.09117940068244934\n",
      "Epoch 1644/20000 Training Loss: 0.07280591875314713\n",
      "Epoch 1645/20000 Training Loss: 0.07554168999195099\n",
      "Epoch 1646/20000 Training Loss: 0.06990033388137817\n",
      "Epoch 1647/20000 Training Loss: 0.07421880960464478\n",
      "Epoch 1648/20000 Training Loss: 0.07468985766172409\n",
      "Epoch 1649/20000 Training Loss: 0.09171763062477112\n",
      "Epoch 1650/20000 Training Loss: 0.060792773962020874\n",
      "Epoch 1651/20000 Training Loss: 0.09827551245689392\n",
      "Epoch 1652/20000 Training Loss: 0.08620738983154297\n",
      "Epoch 1653/20000 Training Loss: 0.0882006287574768\n",
      "Epoch 1654/20000 Training Loss: 0.10847726464271545\n",
      "Epoch 1655/20000 Training Loss: 0.09337349236011505\n",
      "Epoch 1656/20000 Training Loss: 0.07795185595750809\n",
      "Epoch 1657/20000 Training Loss: 0.09161738306283951\n",
      "Epoch 1658/20000 Training Loss: 0.08986849337816238\n",
      "Epoch 1659/20000 Training Loss: 0.08945036679506302\n",
      "Epoch 1660/20000 Training Loss: 0.07347896695137024\n",
      "Epoch 1661/20000 Training Loss: 0.06690334528684616\n",
      "Epoch 1662/20000 Training Loss: 0.06837856024503708\n",
      "Epoch 1663/20000 Training Loss: 0.10365007072687149\n",
      "Epoch 1664/20000 Training Loss: 0.0746752992272377\n",
      "Epoch 1665/20000 Training Loss: 0.10286460816860199\n",
      "Epoch 1666/20000 Training Loss: 0.08953188359737396\n",
      "Epoch 1667/20000 Training Loss: 0.10662274807691574\n",
      "Epoch 1668/20000 Training Loss: 0.06322213262319565\n",
      "Epoch 1669/20000 Training Loss: 0.09542003273963928\n",
      "Epoch 1670/20000 Training Loss: 0.05963557958602905\n",
      "Epoch 1671/20000 Training Loss: 0.06368093192577362\n",
      "Epoch 1672/20000 Training Loss: 0.08101310580968857\n",
      "Epoch 1673/20000 Training Loss: 0.05954313278198242\n",
      "Epoch 1674/20000 Training Loss: 0.07327695935964584\n",
      "Epoch 1675/20000 Training Loss: 0.07516966015100479\n",
      "Epoch 1676/20000 Training Loss: 0.07109268754720688\n",
      "Epoch 1677/20000 Training Loss: 0.09185397624969482\n",
      "Epoch 1678/20000 Training Loss: 0.07224984467029572\n",
      "Epoch 1679/20000 Training Loss: 0.06823012977838516\n",
      "Epoch 1680/20000 Training Loss: 0.0682172030210495\n",
      "Epoch 1681/20000 Training Loss: 0.09899948537349701\n",
      "Epoch 1682/20000 Training Loss: 0.10541661828756332\n",
      "Epoch 1683/20000 Training Loss: 0.08386237919330597\n",
      "Epoch 1684/20000 Training Loss: 0.07834997773170471\n",
      "Epoch 1685/20000 Training Loss: 0.07549577951431274\n",
      "Epoch 1686/20000 Training Loss: 0.09042847901582718\n",
      "Epoch 1687/20000 Training Loss: 0.0731266438961029\n",
      "Epoch 1688/20000 Training Loss: 0.086504727602005\n",
      "Epoch 1689/20000 Training Loss: 0.0849635899066925\n",
      "Epoch 1690/20000 Training Loss: 0.08213216811418533\n",
      "Epoch 1691/20000 Training Loss: 0.08912374079227448\n",
      "Epoch 1692/20000 Training Loss: 0.059935249388217926\n",
      "Epoch 1693/20000 Training Loss: 0.11047470569610596\n",
      "Epoch 1694/20000 Training Loss: 0.06553110480308533\n",
      "Epoch 1695/20000 Training Loss: 0.06836061179637909\n",
      "Epoch 1696/20000 Training Loss: 0.06774333864450455\n",
      "Epoch 1697/20000 Training Loss: 0.086144357919693\n",
      "Epoch 1698/20000 Training Loss: 0.11630885303020477\n",
      "Epoch 1699/20000 Training Loss: 0.08836248517036438\n",
      "Epoch 1700/20000 Training Loss: 0.05907481908798218\n",
      "Epoch 1700/20000 Validation Loss: 0.09893204271793365\n",
      "Epoch 1701/20000 Training Loss: 0.07236478477716446\n",
      "Epoch 1702/20000 Training Loss: 0.06561198830604553\n",
      "Epoch 1703/20000 Training Loss: 0.08574224263429642\n",
      "Epoch 1704/20000 Training Loss: 0.08277853578329086\n",
      "Epoch 1705/20000 Training Loss: 0.0784844160079956\n",
      "Epoch 1706/20000 Training Loss: 0.0962153971195221\n",
      "Epoch 1707/20000 Training Loss: 0.09458684921264648\n",
      "Epoch 1708/20000 Training Loss: 0.09082984179258347\n",
      "Epoch 1709/20000 Training Loss: 0.0796014666557312\n",
      "Epoch 1710/20000 Training Loss: 0.10001559555530548\n",
      "Epoch 1711/20000 Training Loss: 0.06983216106891632\n",
      "Epoch 1712/20000 Training Loss: 0.09195208549499512\n",
      "Epoch 1713/20000 Training Loss: 0.08864424377679825\n",
      "Epoch 1714/20000 Training Loss: 0.09446597099304199\n",
      "Epoch 1715/20000 Training Loss: 0.10759938508272171\n",
      "Epoch 1716/20000 Training Loss: 0.06101798266172409\n",
      "Epoch 1717/20000 Training Loss: 0.07737140357494354\n",
      "Epoch 1718/20000 Training Loss: 0.07532750070095062\n",
      "Epoch 1719/20000 Training Loss: 0.0753345787525177\n",
      "Epoch 1720/20000 Training Loss: 0.06610119342803955\n",
      "Epoch 1721/20000 Training Loss: 0.0928831696510315\n",
      "Epoch 1722/20000 Training Loss: 0.0672728419303894\n",
      "Epoch 1723/20000 Training Loss: 0.06213489919900894\n",
      "Epoch 1724/20000 Training Loss: 0.053477976471185684\n",
      "Epoch 1725/20000 Training Loss: 0.08173328638076782\n",
      "Epoch 1726/20000 Training Loss: 0.08270732313394547\n",
      "Epoch 1727/20000 Training Loss: 0.0713551864027977\n",
      "Epoch 1728/20000 Training Loss: 0.07001715153455734\n",
      "Epoch 1729/20000 Training Loss: 0.08719577640295029\n",
      "Epoch 1730/20000 Training Loss: 0.06426773965358734\n",
      "Epoch 1731/20000 Training Loss: 0.09665301442146301\n",
      "Epoch 1732/20000 Training Loss: 0.08310076594352722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1733/20000 Training Loss: 0.08568872511386871\n",
      "Epoch 1734/20000 Training Loss: 0.10206127166748047\n",
      "Epoch 1735/20000 Training Loss: 0.060659635812044144\n",
      "Epoch 1736/20000 Training Loss: 0.08256913721561432\n",
      "Epoch 1737/20000 Training Loss: 0.07454149425029755\n",
      "Epoch 1738/20000 Training Loss: 0.061337150633335114\n",
      "Epoch 1739/20000 Training Loss: 0.09022233635187149\n",
      "Epoch 1740/20000 Training Loss: 0.08868386596441269\n",
      "Epoch 1741/20000 Training Loss: 0.07725746929645538\n",
      "Epoch 1742/20000 Training Loss: 0.08473315834999084\n",
      "Epoch 1743/20000 Training Loss: 0.0896615982055664\n",
      "Epoch 1744/20000 Training Loss: 0.08964432030916214\n",
      "Epoch 1745/20000 Training Loss: 0.09365454316139221\n",
      "Epoch 1746/20000 Training Loss: 0.09388140588998795\n",
      "Epoch 1747/20000 Training Loss: 0.11099602282047272\n",
      "Epoch 1748/20000 Training Loss: 0.07748281210660934\n",
      "Epoch 1749/20000 Training Loss: 0.06678780168294907\n",
      "Epoch 1750/20000 Training Loss: 0.08663241565227509\n",
      "Epoch 1751/20000 Training Loss: 0.07631140947341919\n",
      "Epoch 1752/20000 Training Loss: 0.1057167649269104\n",
      "Epoch 1753/20000 Training Loss: 0.09955809265375137\n",
      "Epoch 1754/20000 Training Loss: 0.070322185754776\n",
      "Epoch 1755/20000 Training Loss: 0.07923918962478638\n",
      "Epoch 1756/20000 Training Loss: 0.10263000428676605\n",
      "Epoch 1757/20000 Training Loss: 0.08699560165405273\n",
      "Epoch 1758/20000 Training Loss: 0.0844988152384758\n",
      "Epoch 1759/20000 Training Loss: 0.07251754403114319\n",
      "Epoch 1760/20000 Training Loss: 0.08056679368019104\n",
      "Epoch 1761/20000 Training Loss: 0.08840127289295197\n",
      "Epoch 1762/20000 Training Loss: 0.09177083522081375\n",
      "Epoch 1763/20000 Training Loss: 0.09503087401390076\n",
      "Epoch 1764/20000 Training Loss: 0.05711863562464714\n",
      "Epoch 1765/20000 Training Loss: 0.0724368691444397\n",
      "Epoch 1766/20000 Training Loss: 0.08198671042919159\n",
      "Epoch 1767/20000 Training Loss: 0.0654774010181427\n",
      "Epoch 1768/20000 Training Loss: 0.09302438795566559\n",
      "Epoch 1769/20000 Training Loss: 0.0867481455206871\n",
      "Epoch 1770/20000 Training Loss: 0.07891511917114258\n",
      "Epoch 1771/20000 Training Loss: 0.07609587907791138\n",
      "Epoch 1772/20000 Training Loss: 0.0677412822842598\n",
      "Epoch 1773/20000 Training Loss: 0.10633695870637894\n",
      "Epoch 1774/20000 Training Loss: 0.07263005524873734\n",
      "Epoch 1775/20000 Training Loss: 0.0680268183350563\n",
      "Epoch 1776/20000 Training Loss: 0.06890934705734253\n",
      "Epoch 1777/20000 Training Loss: 0.08536507189273834\n",
      "Epoch 1778/20000 Training Loss: 0.09245093166828156\n",
      "Epoch 1779/20000 Training Loss: 0.07980784773826599\n",
      "Epoch 1780/20000 Training Loss: 0.07006668299436569\n",
      "Epoch 1781/20000 Training Loss: 0.06289004534482956\n",
      "Epoch 1782/20000 Training Loss: 0.08773060142993927\n",
      "Epoch 1783/20000 Training Loss: 0.06567135453224182\n",
      "Epoch 1784/20000 Training Loss: 0.06572557240724564\n",
      "Epoch 1785/20000 Training Loss: 0.0717596560716629\n",
      "Epoch 1786/20000 Training Loss: 0.10134685039520264\n",
      "Epoch 1787/20000 Training Loss: 0.07320048660039902\n",
      "Epoch 1788/20000 Training Loss: 0.06950926780700684\n",
      "Epoch 1789/20000 Training Loss: 0.06928882747888565\n",
      "Epoch 1790/20000 Training Loss: 0.06569644063711166\n",
      "Epoch 1791/20000 Training Loss: 0.060794126242399216\n",
      "Epoch 1792/20000 Training Loss: 0.10431389510631561\n",
      "Epoch 1793/20000 Training Loss: 0.06542759388685226\n",
      "Epoch 1794/20000 Training Loss: 0.11934181302785873\n",
      "Epoch 1795/20000 Training Loss: 0.08087301254272461\n",
      "Epoch 1796/20000 Training Loss: 0.08164383471012115\n",
      "Epoch 1797/20000 Training Loss: 0.0945359617471695\n",
      "Epoch 1798/20000 Training Loss: 0.0682094395160675\n",
      "Epoch 1799/20000 Training Loss: 0.07064977288246155\n",
      "Epoch 1800/20000 Training Loss: 0.06518030911684036\n",
      "Epoch 1800/20000 Validation Loss: 0.06628745794296265\n",
      "Epoch 1801/20000 Training Loss: 0.09751135110855103\n",
      "Epoch 1802/20000 Training Loss: 0.05754656344652176\n",
      "Epoch 1803/20000 Training Loss: 0.06653264164924622\n",
      "Epoch 1804/20000 Training Loss: 0.08391636610031128\n",
      "Epoch 1805/20000 Training Loss: 0.09192992746829987\n",
      "Epoch 1806/20000 Training Loss: 0.0835220068693161\n",
      "Epoch 1807/20000 Training Loss: 0.09906738996505737\n",
      "Epoch 1808/20000 Training Loss: 0.11100631952285767\n",
      "Epoch 1809/20000 Training Loss: 0.06632576137781143\n",
      "Epoch 1810/20000 Training Loss: 0.07930918037891388\n",
      "Epoch 1811/20000 Training Loss: 0.11298520863056183\n",
      "Epoch 1812/20000 Training Loss: 0.09999282658100128\n",
      "Epoch 1813/20000 Training Loss: 0.07945852726697922\n",
      "Epoch 1814/20000 Training Loss: 0.09354332089424133\n",
      "Epoch 1815/20000 Training Loss: 0.075191430747509\n",
      "Epoch 1816/20000 Training Loss: 0.08540968596935272\n",
      "Epoch 1817/20000 Training Loss: 0.08461326360702515\n",
      "Epoch 1818/20000 Training Loss: 0.05897856503725052\n",
      "Epoch 1819/20000 Training Loss: 0.06567958742380142\n",
      "Epoch 1820/20000 Training Loss: 0.07831969857215881\n",
      "Epoch 1821/20000 Training Loss: 0.0798632949590683\n",
      "Epoch 1822/20000 Training Loss: 0.07146578282117844\n",
      "Epoch 1823/20000 Training Loss: 0.08800511062145233\n",
      "Epoch 1824/20000 Training Loss: 0.10562177747488022\n",
      "Epoch 1825/20000 Training Loss: 0.0712135061621666\n",
      "Epoch 1826/20000 Training Loss: 0.0929054543375969\n",
      "Epoch 1827/20000 Training Loss: 0.0671888142824173\n",
      "Epoch 1828/20000 Training Loss: 0.07780636847019196\n",
      "Epoch 1829/20000 Training Loss: 0.08704350888729095\n",
      "Epoch 1830/20000 Training Loss: 0.06937791407108307\n",
      "Epoch 1831/20000 Training Loss: 0.07779025286436081\n",
      "Epoch 1832/20000 Training Loss: 0.06182742118835449\n",
      "Epoch 1833/20000 Training Loss: 0.0716080516576767\n",
      "Epoch 1834/20000 Training Loss: 0.08813018351793289\n",
      "Epoch 1835/20000 Training Loss: 0.08195459097623825\n",
      "Epoch 1836/20000 Training Loss: 0.0849362164735794\n",
      "Epoch 1837/20000 Training Loss: 0.09726297855377197\n",
      "Epoch 1838/20000 Training Loss: 0.07274950295686722\n",
      "Epoch 1839/20000 Training Loss: 0.07890096306800842\n",
      "Epoch 1840/20000 Training Loss: 0.08261613547801971\n",
      "Epoch 1841/20000 Training Loss: 0.07256908714771271\n",
      "Epoch 1842/20000 Training Loss: 0.07806043326854706\n",
      "Epoch 1843/20000 Training Loss: 0.11232251673936844\n",
      "Epoch 1844/20000 Training Loss: 0.07927456498146057\n",
      "Epoch 1845/20000 Training Loss: 0.08473248779773712\n",
      "Epoch 1846/20000 Training Loss: 0.07120367884635925\n",
      "Epoch 1847/20000 Training Loss: 0.06965364515781403\n",
      "Epoch 1848/20000 Training Loss: 0.0718747228384018\n",
      "Epoch 1849/20000 Training Loss: 0.08982392400503159\n",
      "Epoch 1850/20000 Training Loss: 0.07602570950984955\n",
      "Epoch 1851/20000 Training Loss: 0.06151026487350464\n",
      "Epoch 1852/20000 Training Loss: 0.06411907076835632\n",
      "Epoch 1853/20000 Training Loss: 0.10028631240129471\n",
      "Epoch 1854/20000 Training Loss: 0.09243789315223694\n",
      "Epoch 1855/20000 Training Loss: 0.07163034379482269\n",
      "Epoch 1856/20000 Training Loss: 0.09175825119018555\n",
      "Epoch 1857/20000 Training Loss: 0.06899870932102203\n",
      "Epoch 1858/20000 Training Loss: 0.054132409393787384\n",
      "Epoch 1859/20000 Training Loss: 0.093755804002285\n",
      "Epoch 1860/20000 Training Loss: 0.06231183558702469\n",
      "Epoch 1861/20000 Training Loss: 0.07871074974536896\n",
      "Epoch 1862/20000 Training Loss: 0.08021312952041626\n",
      "Epoch 1863/20000 Training Loss: 0.07992808520793915\n",
      "Epoch 1864/20000 Training Loss: 0.09429760277271271\n",
      "Epoch 1865/20000 Training Loss: 0.071875661611557\n",
      "Epoch 1866/20000 Training Loss: 0.08367767930030823\n",
      "Epoch 1867/20000 Training Loss: 0.08552408218383789\n",
      "Epoch 1868/20000 Training Loss: 0.08912678807973862\n",
      "Epoch 1869/20000 Training Loss: 0.09720048308372498\n",
      "Epoch 1870/20000 Training Loss: 0.07859763503074646\n",
      "Epoch 1871/20000 Training Loss: 0.09168478101491928\n",
      "Epoch 1872/20000 Training Loss: 0.05743663012981415\n",
      "Epoch 1873/20000 Training Loss: 0.07186576724052429\n",
      "Epoch 1874/20000 Training Loss: 0.09385983645915985\n",
      "Epoch 1875/20000 Training Loss: 0.08083042502403259\n",
      "Epoch 1876/20000 Training Loss: 0.08524750173091888\n",
      "Epoch 1877/20000 Training Loss: 0.09074327349662781\n",
      "Epoch 1878/20000 Training Loss: 0.06580886989831924\n",
      "Epoch 1879/20000 Training Loss: 0.07665147632360458\n",
      "Epoch 1880/20000 Training Loss: 0.07564808428287506\n",
      "Epoch 1881/20000 Training Loss: 0.0803648978471756\n",
      "Epoch 1882/20000 Training Loss: 0.08279407769441605\n",
      "Epoch 1883/20000 Training Loss: 0.06962937116622925\n",
      "Epoch 1884/20000 Training Loss: 0.09039605408906937\n",
      "Epoch 1885/20000 Training Loss: 0.09107911586761475\n",
      "Epoch 1886/20000 Training Loss: 0.08882851153612137\n",
      "Epoch 1887/20000 Training Loss: 0.07298102974891663\n",
      "Epoch 1888/20000 Training Loss: 0.060983702540397644\n",
      "Epoch 1889/20000 Training Loss: 0.07050532102584839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1890/20000 Training Loss: 0.09861242771148682\n",
      "Epoch 1891/20000 Training Loss: 0.07116355746984482\n",
      "Epoch 1892/20000 Training Loss: 0.08977481722831726\n",
      "Epoch 1893/20000 Training Loss: 0.08540067076683044\n",
      "Epoch 1894/20000 Training Loss: 0.07832372933626175\n",
      "Epoch 1895/20000 Training Loss: 0.08507226407527924\n",
      "Epoch 1896/20000 Training Loss: 0.08151192963123322\n",
      "Epoch 1897/20000 Training Loss: 0.0838562548160553\n",
      "Epoch 1898/20000 Training Loss: 0.08605870604515076\n",
      "Epoch 1899/20000 Training Loss: 0.11159716546535492\n",
      "Epoch 1900/20000 Training Loss: 0.07803969830274582\n",
      "Epoch 1900/20000 Validation Loss: 0.08743682503700256\n",
      "Epoch 1901/20000 Training Loss: 0.09579437971115112\n",
      "Epoch 1902/20000 Training Loss: 0.09747449308633804\n",
      "Epoch 1903/20000 Training Loss: 0.08833235502243042\n",
      "Epoch 1904/20000 Training Loss: 0.10407476872205734\n",
      "Epoch 1905/20000 Training Loss: 0.084165558218956\n",
      "Epoch 1906/20000 Training Loss: 0.07256816327571869\n",
      "Epoch 1907/20000 Training Loss: 0.12155954539775848\n",
      "Epoch 1908/20000 Training Loss: 0.07164458185434341\n",
      "Epoch 1909/20000 Training Loss: 0.06564211845397949\n",
      "Epoch 1910/20000 Training Loss: 0.0826679915189743\n",
      "Epoch 1911/20000 Training Loss: 0.05987595021724701\n",
      "Epoch 1912/20000 Training Loss: 0.10886118561029434\n",
      "Epoch 1913/20000 Training Loss: 0.08750709146261215\n",
      "Epoch 1914/20000 Training Loss: 0.07599243521690369\n",
      "Epoch 1915/20000 Training Loss: 0.07156616449356079\n",
      "Epoch 1916/20000 Training Loss: 0.08588575571775436\n",
      "Epoch 1917/20000 Training Loss: 0.08723196387290955\n",
      "Epoch 1918/20000 Training Loss: 0.10720446705818176\n",
      "Epoch 1919/20000 Training Loss: 0.05740955471992493\n",
      "Epoch 1920/20000 Training Loss: 0.08852884918451309\n",
      "Epoch 1921/20000 Training Loss: 0.0779886469244957\n",
      "Epoch 1922/20000 Training Loss: 0.09032575786113739\n",
      "Epoch 1923/20000 Training Loss: 0.07063919305801392\n",
      "Epoch 1924/20000 Training Loss: 0.08986265957355499\n",
      "Epoch 1925/20000 Training Loss: 0.05894112586975098\n",
      "Epoch 1926/20000 Training Loss: 0.05861038714647293\n",
      "Epoch 1927/20000 Training Loss: 0.06860794872045517\n",
      "Epoch 1928/20000 Training Loss: 0.07251381874084473\n",
      "Epoch 1929/20000 Training Loss: 0.060682788491249084\n",
      "Epoch 1930/20000 Training Loss: 0.07654258608818054\n",
      "Epoch 1931/20000 Training Loss: 0.07843731343746185\n",
      "Epoch 1932/20000 Training Loss: 0.0742010846734047\n",
      "Epoch 1933/20000 Training Loss: 0.08953731507062912\n",
      "Epoch 1934/20000 Training Loss: 0.09185624867677689\n",
      "Epoch 1935/20000 Training Loss: 0.07303275913000107\n",
      "Epoch 1936/20000 Training Loss: 0.07161805033683777\n",
      "Epoch 1937/20000 Training Loss: 0.0884343832731247\n",
      "Epoch 1938/20000 Training Loss: 0.0714588612318039\n",
      "Epoch 1939/20000 Training Loss: 0.07553453743457794\n",
      "Epoch 1940/20000 Training Loss: 0.066165030002594\n",
      "Epoch 1941/20000 Training Loss: 0.0779183879494667\n",
      "Epoch 1942/20000 Training Loss: 0.07659320533275604\n",
      "Epoch 1943/20000 Training Loss: 0.10318861901760101\n",
      "Epoch 1944/20000 Training Loss: 0.08494456112384796\n",
      "Epoch 1945/20000 Training Loss: 0.07295756787061691\n",
      "Epoch 1946/20000 Training Loss: 0.0898037850856781\n",
      "Epoch 1947/20000 Training Loss: 0.061046481132507324\n",
      "Epoch 1948/20000 Training Loss: 0.06977078318595886\n",
      "Epoch 1949/20000 Training Loss: 0.09379597753286362\n",
      "Epoch 1950/20000 Training Loss: 0.06536094844341278\n",
      "Epoch 1951/20000 Training Loss: 0.07908159494400024\n",
      "Epoch 1952/20000 Training Loss: 0.08916790783405304\n",
      "Epoch 1953/20000 Training Loss: 0.07967202365398407\n",
      "Epoch 1954/20000 Training Loss: 0.06435812264680862\n",
      "Epoch 1955/20000 Training Loss: 0.08946744352579117\n",
      "Epoch 1956/20000 Training Loss: 0.09122949838638306\n",
      "Epoch 1957/20000 Training Loss: 0.07501117885112762\n",
      "Epoch 1958/20000 Training Loss: 0.09755025804042816\n",
      "Epoch 1959/20000 Training Loss: 0.07802372425794601\n",
      "Epoch 1960/20000 Training Loss: 0.0608343631029129\n",
      "Epoch 1961/20000 Training Loss: 0.08928149938583374\n",
      "Epoch 1962/20000 Training Loss: 0.0775439664721489\n",
      "Epoch 1963/20000 Training Loss: 0.06783358007669449\n",
      "Epoch 1964/20000 Training Loss: 0.10029790550470352\n",
      "Epoch 1965/20000 Training Loss: 0.0691763162612915\n",
      "Epoch 1966/20000 Training Loss: 0.06617436558008194\n",
      "Epoch 1967/20000 Training Loss: 0.06847776472568512\n",
      "Epoch 1968/20000 Training Loss: 0.07239601016044617\n",
      "Epoch 1969/20000 Training Loss: 0.06870557367801666\n",
      "Epoch 1970/20000 Training Loss: 0.09361216425895691\n",
      "Epoch 1971/20000 Training Loss: 0.08264970779418945\n",
      "Epoch 1972/20000 Training Loss: 0.08467530459165573\n",
      "Epoch 1973/20000 Training Loss: 0.07940229773521423\n",
      "Epoch 1974/20000 Training Loss: 0.06327994912862778\n",
      "Epoch 1975/20000 Training Loss: 0.0613248348236084\n",
      "Epoch 1976/20000 Training Loss: 0.07826481759548187\n",
      "Epoch 1977/20000 Training Loss: 0.08523215353488922\n",
      "Epoch 1978/20000 Training Loss: 0.060548678040504456\n",
      "Epoch 1979/20000 Training Loss: 0.07937312871217728\n",
      "Epoch 1980/20000 Training Loss: 0.06378799676895142\n",
      "Epoch 1981/20000 Training Loss: 0.10074058920145035\n",
      "Epoch 1982/20000 Training Loss: 0.09072853624820709\n",
      "Epoch 1983/20000 Training Loss: 0.0761570930480957\n",
      "Epoch 1984/20000 Training Loss: 0.09812216460704803\n",
      "Epoch 1985/20000 Training Loss: 0.07580922544002533\n",
      "Epoch 1986/20000 Training Loss: 0.07838526368141174\n",
      "Epoch 1987/20000 Training Loss: 0.10041339695453644\n",
      "Epoch 1988/20000 Training Loss: 0.09707595407962799\n",
      "Epoch 1989/20000 Training Loss: 0.08022231608629227\n",
      "Epoch 1990/20000 Training Loss: 0.09328164160251617\n",
      "Epoch 1991/20000 Training Loss: 0.0817423164844513\n",
      "Epoch 1992/20000 Training Loss: 0.07112156599760056\n",
      "Epoch 1993/20000 Training Loss: 0.0753907710313797\n",
      "Epoch 1994/20000 Training Loss: 0.0676204115152359\n",
      "Epoch 1995/20000 Training Loss: 0.07986846566200256\n",
      "Epoch 1996/20000 Training Loss: 0.07804509997367859\n",
      "Epoch 1997/20000 Training Loss: 0.10922916978597641\n",
      "Epoch 1998/20000 Training Loss: 0.07021825760602951\n",
      "Epoch 1999/20000 Training Loss: 0.09833494573831558\n",
      "Epoch 2000/20000 Training Loss: 0.08144943416118622\n",
      "Epoch 2000/20000 Validation Loss: 0.0720423236489296\n",
      "Epoch 2001/20000 Training Loss: 0.11375949531793594\n",
      "Epoch 2002/20000 Training Loss: 0.07265862822532654\n",
      "Epoch 2003/20000 Training Loss: 0.06620967388153076\n",
      "Epoch 2004/20000 Training Loss: 0.09103557467460632\n",
      "Epoch 2005/20000 Training Loss: 0.06501184403896332\n",
      "Epoch 2006/20000 Training Loss: 0.07472018152475357\n",
      "Epoch 2007/20000 Training Loss: 0.07565326243638992\n",
      "Epoch 2008/20000 Training Loss: 0.08802169561386108\n",
      "Epoch 2009/20000 Training Loss: 0.0729648619890213\n",
      "Epoch 2010/20000 Training Loss: 0.07164828479290009\n",
      "Epoch 2011/20000 Training Loss: 0.0770641416311264\n",
      "Epoch 2012/20000 Training Loss: 0.08912719786167145\n",
      "Epoch 2013/20000 Training Loss: 0.08591140806674957\n",
      "Epoch 2014/20000 Training Loss: 0.08751430362462997\n",
      "Epoch 2015/20000 Training Loss: 0.07715201377868652\n",
      "Epoch 2016/20000 Training Loss: 0.09801919758319855\n",
      "Epoch 2017/20000 Training Loss: 0.0822438895702362\n",
      "Epoch 2018/20000 Training Loss: 0.07666551321744919\n",
      "Epoch 2019/20000 Training Loss: 0.07405985891819\n",
      "Epoch 2020/20000 Training Loss: 0.08236465603113174\n",
      "Epoch 2021/20000 Training Loss: 0.0777059942483902\n",
      "Epoch 2022/20000 Training Loss: 0.0883570984005928\n",
      "Epoch 2023/20000 Training Loss: 0.11720423400402069\n",
      "Epoch 2024/20000 Training Loss: 0.09080122411251068\n",
      "Epoch 2025/20000 Training Loss: 0.0778656005859375\n",
      "Epoch 2026/20000 Training Loss: 0.08720347285270691\n",
      "Epoch 2027/20000 Training Loss: 0.058877211064100266\n",
      "Epoch 2028/20000 Training Loss: 0.06332895904779434\n",
      "Epoch 2029/20000 Training Loss: 0.08276483416557312\n",
      "Epoch 2030/20000 Training Loss: 0.07599879801273346\n",
      "Epoch 2031/20000 Training Loss: 0.06229556351900101\n",
      "Epoch 2032/20000 Training Loss: 0.10473082959651947\n",
      "Epoch 2033/20000 Training Loss: 0.07666203379631042\n",
      "Epoch 2034/20000 Training Loss: 0.09063101559877396\n",
      "Epoch 2035/20000 Training Loss: 0.059947505593299866\n",
      "Epoch 2036/20000 Training Loss: 0.0664878636598587\n",
      "Epoch 2037/20000 Training Loss: 0.07572366297245026\n",
      "Epoch 2038/20000 Training Loss: 0.10619238018989563\n",
      "Epoch 2039/20000 Training Loss: 0.06594164669513702\n",
      "Epoch 2040/20000 Training Loss: 0.050618212670087814\n",
      "Epoch 2041/20000 Training Loss: 0.07286334037780762\n",
      "Epoch 2042/20000 Training Loss: 0.072056345641613\n",
      "Epoch 2043/20000 Training Loss: 0.06513071805238724\n",
      "Epoch 2044/20000 Training Loss: 0.08624283969402313\n",
      "Epoch 2045/20000 Training Loss: 0.09697920083999634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2046/20000 Training Loss: 0.07879950106143951\n",
      "Epoch 2047/20000 Training Loss: 0.07796955108642578\n",
      "Epoch 2048/20000 Training Loss: 0.11798219382762909\n",
      "Epoch 2049/20000 Training Loss: 0.0770544558763504\n",
      "Epoch 2050/20000 Training Loss: 0.09727540612220764\n",
      "Epoch 2051/20000 Training Loss: 0.06396621465682983\n",
      "Epoch 2052/20000 Training Loss: 0.09404433518648148\n",
      "Epoch 2053/20000 Training Loss: 0.08687062561511993\n",
      "Epoch 2054/20000 Training Loss: 0.07077071070671082\n",
      "Epoch 2055/20000 Training Loss: 0.08221372961997986\n",
      "Epoch 2056/20000 Training Loss: 0.08921487629413605\n",
      "Epoch 2057/20000 Training Loss: 0.08923289179801941\n",
      "Epoch 2058/20000 Training Loss: 0.08170360326766968\n",
      "Epoch 2059/20000 Training Loss: 0.06588734686374664\n",
      "Epoch 2060/20000 Training Loss: 0.06345497816801071\n",
      "Epoch 2061/20000 Training Loss: 0.07909655570983887\n",
      "Epoch 2062/20000 Training Loss: 0.07536319643259048\n",
      "Epoch 2063/20000 Training Loss: 0.07974624633789062\n",
      "Epoch 2064/20000 Training Loss: 0.06217201054096222\n",
      "Epoch 2065/20000 Training Loss: 0.07766653597354889\n",
      "Epoch 2066/20000 Training Loss: 0.06795770674943924\n",
      "Epoch 2067/20000 Training Loss: 0.07826857268810272\n",
      "Epoch 2068/20000 Training Loss: 0.07821577787399292\n",
      "Epoch 2069/20000 Training Loss: 0.09273219853639603\n",
      "Epoch 2070/20000 Training Loss: 0.06579390168190002\n",
      "Epoch 2071/20000 Training Loss: 0.09750811755657196\n",
      "Epoch 2072/20000 Training Loss: 0.07225486636161804\n",
      "Epoch 2073/20000 Training Loss: 0.07237236201763153\n",
      "Epoch 2074/20000 Training Loss: 0.06778724491596222\n",
      "Epoch 2075/20000 Training Loss: 0.06556002795696259\n",
      "Epoch 2076/20000 Training Loss: 0.07606764882802963\n",
      "Epoch 2077/20000 Training Loss: 0.0685766264796257\n",
      "Epoch 2078/20000 Training Loss: 0.07945042848587036\n",
      "Epoch 2079/20000 Training Loss: 0.06549854576587677\n",
      "Epoch 2080/20000 Training Loss: 0.05847737938165665\n",
      "Epoch 2081/20000 Training Loss: 0.0836668312549591\n",
      "Epoch 2082/20000 Training Loss: 0.07084311544895172\n",
      "Epoch 2083/20000 Training Loss: 0.04775186628103256\n",
      "Epoch 2084/20000 Training Loss: 0.05912598967552185\n",
      "Epoch 2085/20000 Training Loss: 0.09713523089885712\n",
      "Epoch 2086/20000 Training Loss: 0.08072531968355179\n",
      "Epoch 2087/20000 Training Loss: 0.07437200844287872\n",
      "Epoch 2088/20000 Training Loss: 0.06960134208202362\n",
      "Epoch 2089/20000 Training Loss: 0.05986606329679489\n",
      "Epoch 2090/20000 Training Loss: 0.07065080106258392\n",
      "Epoch 2091/20000 Training Loss: 0.04641220346093178\n",
      "Epoch 2092/20000 Training Loss: 0.0777597427368164\n",
      "Epoch 2093/20000 Training Loss: 0.10695717483758926\n",
      "Epoch 2094/20000 Training Loss: 0.06153878569602966\n",
      "Epoch 2095/20000 Training Loss: 0.08139560371637344\n",
      "Epoch 2096/20000 Training Loss: 0.07882547378540039\n",
      "Epoch 2097/20000 Training Loss: 0.08443862199783325\n",
      "Epoch 2098/20000 Training Loss: 0.08702132105827332\n",
      "Epoch 2099/20000 Training Loss: 0.059662092477083206\n",
      "Epoch 2100/20000 Training Loss: 0.09388210624456406\n",
      "Epoch 2100/20000 Validation Loss: 0.07920205593109131\n",
      "Epoch 2101/20000 Training Loss: 0.0831083431839943\n",
      "Epoch 2102/20000 Training Loss: 0.08353189378976822\n",
      "Epoch 2103/20000 Training Loss: 0.10742077231407166\n",
      "Epoch 2104/20000 Training Loss: 0.0908890962600708\n",
      "Epoch 2105/20000 Training Loss: 0.10270355641841888\n",
      "Epoch 2106/20000 Training Loss: 0.0896814838051796\n",
      "Epoch 2107/20000 Training Loss: 0.04671989753842354\n",
      "Epoch 2108/20000 Training Loss: 0.079646997153759\n",
      "Epoch 2109/20000 Training Loss: 0.09449923038482666\n",
      "Epoch 2110/20000 Training Loss: 0.06809182465076447\n",
      "Epoch 2111/20000 Training Loss: 0.06175624579191208\n",
      "Epoch 2112/20000 Training Loss: 0.08050888031721115\n",
      "Epoch 2113/20000 Training Loss: 0.06207635998725891\n",
      "Epoch 2114/20000 Training Loss: 0.05854928493499756\n",
      "Epoch 2115/20000 Training Loss: 0.09444630146026611\n",
      "Epoch 2116/20000 Training Loss: 0.07930310070514679\n",
      "Epoch 2117/20000 Training Loss: 0.09595505893230438\n",
      "Epoch 2118/20000 Training Loss: 0.08014529943466187\n",
      "Epoch 2119/20000 Training Loss: 0.0854063630104065\n",
      "Epoch 2120/20000 Training Loss: 0.07132472097873688\n",
      "Epoch 2121/20000 Training Loss: 0.06396983563899994\n",
      "Epoch 2122/20000 Training Loss: 0.053697384893894196\n",
      "Epoch 2123/20000 Training Loss: 0.07126036286354065\n",
      "Epoch 2124/20000 Training Loss: 0.08941376209259033\n",
      "Epoch 2125/20000 Training Loss: 0.06624168157577515\n",
      "Epoch 2126/20000 Training Loss: 0.06692186743021011\n",
      "Epoch 2127/20000 Training Loss: 0.062138400971889496\n",
      "Epoch 2128/20000 Training Loss: 0.0781230479478836\n",
      "Epoch 2129/20000 Training Loss: 0.09510549902915955\n",
      "Epoch 2130/20000 Training Loss: 0.08201408386230469\n",
      "Epoch 2131/20000 Training Loss: 0.06868503987789154\n",
      "Epoch 2132/20000 Training Loss: 0.06824085861444473\n",
      "Epoch 2133/20000 Training Loss: 0.066302090883255\n",
      "Epoch 2134/20000 Training Loss: 0.06238163262605667\n",
      "Epoch 2135/20000 Training Loss: 0.08363287150859833\n",
      "Epoch 2136/20000 Training Loss: 0.07791499048471451\n",
      "Epoch 2137/20000 Training Loss: 0.09778618812561035\n",
      "Epoch 2138/20000 Training Loss: 0.060228049755096436\n",
      "Epoch 2139/20000 Training Loss: 0.05699222534894943\n",
      "Epoch 2140/20000 Training Loss: 0.08867494016885757\n",
      "Epoch 2141/20000 Training Loss: 0.06456094980239868\n",
      "Epoch 2142/20000 Training Loss: 0.07868623733520508\n",
      "Epoch 2143/20000 Training Loss: 0.08023757487535477\n",
      "Epoch 2144/20000 Training Loss: 0.06850095093250275\n",
      "Epoch 2145/20000 Training Loss: 0.09639272093772888\n",
      "Epoch 2146/20000 Training Loss: 0.06396655738353729\n",
      "Epoch 2147/20000 Training Loss: 0.07076259702444077\n",
      "Epoch 2148/20000 Training Loss: 0.06198662519454956\n",
      "Epoch 2149/20000 Training Loss: 0.0678199902176857\n",
      "Epoch 2150/20000 Training Loss: 0.09081900864839554\n",
      "Epoch 2151/20000 Training Loss: 0.122528076171875\n",
      "Epoch 2152/20000 Training Loss: 0.06327087432146072\n",
      "Epoch 2153/20000 Training Loss: 0.05611247569322586\n",
      "Epoch 2154/20000 Training Loss: 0.0858968049287796\n",
      "Epoch 2155/20000 Training Loss: 0.07662488520145416\n",
      "Epoch 2156/20000 Training Loss: 0.06873710453510284\n",
      "Epoch 2157/20000 Training Loss: 0.0639800876379013\n",
      "Epoch 2158/20000 Training Loss: 0.09167329221963882\n",
      "Epoch 2159/20000 Training Loss: 0.0647922232747078\n",
      "Epoch 2160/20000 Training Loss: 0.09233070909976959\n",
      "Epoch 2161/20000 Training Loss: 0.0714588612318039\n",
      "Epoch 2162/20000 Training Loss: 0.09431766718626022\n",
      "Epoch 2163/20000 Training Loss: 0.061304233968257904\n",
      "Epoch 2164/20000 Training Loss: 0.08081798255443573\n",
      "Epoch 2165/20000 Training Loss: 0.07607276737689972\n",
      "Epoch 2166/20000 Training Loss: 0.06697291880846024\n",
      "Epoch 2167/20000 Training Loss: 0.11069279909133911\n",
      "Epoch 2168/20000 Training Loss: 0.06651618331670761\n",
      "Epoch 2169/20000 Training Loss: 0.07785667479038239\n",
      "Epoch 2170/20000 Training Loss: 0.07283830642700195\n",
      "Epoch 2171/20000 Training Loss: 0.09749215841293335\n",
      "Epoch 2172/20000 Training Loss: 0.10126300156116486\n",
      "Epoch 2173/20000 Training Loss: 0.06671217083930969\n",
      "Epoch 2174/20000 Training Loss: 0.06772749125957489\n",
      "Epoch 2175/20000 Training Loss: 0.06925693154335022\n",
      "Epoch 2176/20000 Training Loss: 0.055824488401412964\n",
      "Epoch 2177/20000 Training Loss: 0.07457129657268524\n",
      "Epoch 2178/20000 Training Loss: 0.06628583371639252\n",
      "Epoch 2179/20000 Training Loss: 0.07050435245037079\n",
      "Epoch 2180/20000 Training Loss: 0.06314276158809662\n",
      "Epoch 2181/20000 Training Loss: 0.06910350918769836\n",
      "Epoch 2182/20000 Training Loss: 0.08240453153848648\n",
      "Epoch 2183/20000 Training Loss: 0.08448731899261475\n",
      "Epoch 2184/20000 Training Loss: 0.11729319393634796\n",
      "Epoch 2185/20000 Training Loss: 0.06986850500106812\n",
      "Epoch 2186/20000 Training Loss: 0.07775583863258362\n",
      "Epoch 2187/20000 Training Loss: 0.08859764039516449\n",
      "Epoch 2188/20000 Training Loss: 0.08164221793413162\n",
      "Epoch 2189/20000 Training Loss: 0.07822532206773758\n",
      "Epoch 2190/20000 Training Loss: 0.07056577503681183\n",
      "Epoch 2191/20000 Training Loss: 0.060686200857162476\n",
      "Epoch 2192/20000 Training Loss: 0.05644502490758896\n",
      "Epoch 2193/20000 Training Loss: 0.0543598048388958\n",
      "Epoch 2194/20000 Training Loss: 0.05919632315635681\n",
      "Epoch 2195/20000 Training Loss: 0.09357991814613342\n",
      "Epoch 2196/20000 Training Loss: 0.08750944584608078\n",
      "Epoch 2197/20000 Training Loss: 0.07269349694252014\n",
      "Epoch 2198/20000 Training Loss: 0.06140212342143059\n",
      "Epoch 2199/20000 Training Loss: 0.09918893128633499\n",
      "Epoch 2200/20000 Training Loss: 0.09190644323825836\n",
      "Epoch 2200/20000 Validation Loss: 0.06641791760921478\n",
      "Epoch 2201/20000 Training Loss: 0.07761624455451965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2202/20000 Training Loss: 0.06274010241031647\n",
      "Epoch 2203/20000 Training Loss: 0.08107832819223404\n",
      "Epoch 2204/20000 Training Loss: 0.09842483699321747\n",
      "Epoch 2205/20000 Training Loss: 0.06846629083156586\n",
      "Epoch 2206/20000 Training Loss: 0.05910788103938103\n",
      "Epoch 2207/20000 Training Loss: 0.07133172452449799\n",
      "Epoch 2208/20000 Training Loss: 0.08874179422855377\n",
      "Epoch 2209/20000 Training Loss: 0.07982142269611359\n",
      "Epoch 2210/20000 Training Loss: 0.07424063980579376\n",
      "Epoch 2211/20000 Training Loss: 0.06996425986289978\n",
      "Epoch 2212/20000 Training Loss: 0.09096123278141022\n",
      "Epoch 2213/20000 Training Loss: 0.07522755861282349\n",
      "Epoch 2214/20000 Training Loss: 0.0872848704457283\n",
      "Epoch 2215/20000 Training Loss: 0.07412390410900116\n",
      "Epoch 2216/20000 Training Loss: 0.08695347607135773\n",
      "Epoch 2217/20000 Training Loss: 0.061848923563957214\n",
      "Epoch 2218/20000 Training Loss: 0.061143264174461365\n",
      "Epoch 2219/20000 Training Loss: 0.06513939797878265\n",
      "Epoch 2220/20000 Training Loss: 0.0708424299955368\n",
      "Epoch 2221/20000 Training Loss: 0.07102935016155243\n",
      "Epoch 2222/20000 Training Loss: 0.10209566354751587\n",
      "Epoch 2223/20000 Training Loss: 0.078486368060112\n",
      "Epoch 2224/20000 Training Loss: 0.07075662910938263\n",
      "Epoch 2225/20000 Training Loss: 0.08515892922878265\n",
      "Epoch 2226/20000 Training Loss: 0.09049931913614273\n",
      "Epoch 2227/20000 Training Loss: 0.07401373982429504\n",
      "Epoch 2228/20000 Training Loss: 0.07305283844470978\n",
      "Epoch 2229/20000 Training Loss: 0.08493535220623016\n",
      "Epoch 2230/20000 Training Loss: 0.09800322353839874\n",
      "Epoch 2231/20000 Training Loss: 0.09094366431236267\n",
      "Epoch 2232/20000 Training Loss: 0.08391740918159485\n",
      "Epoch 2233/20000 Training Loss: 0.0914447158575058\n",
      "Epoch 2234/20000 Training Loss: 0.09203216433525085\n",
      "Epoch 2235/20000 Training Loss: 0.06562289595603943\n",
      "Epoch 2236/20000 Training Loss: 0.06979429721832275\n",
      "Epoch 2237/20000 Training Loss: 0.05278680473566055\n",
      "Epoch 2238/20000 Training Loss: 0.08285452425479889\n",
      "Epoch 2239/20000 Training Loss: 0.0856001228094101\n",
      "Epoch 2240/20000 Training Loss: 0.08386548608541489\n",
      "Epoch 2241/20000 Training Loss: 0.10401736199855804\n",
      "Epoch 2242/20000 Training Loss: 0.05709902197122574\n",
      "Epoch 2243/20000 Training Loss: 0.08036305010318756\n",
      "Epoch 2244/20000 Training Loss: 0.06555960327386856\n",
      "Epoch 2245/20000 Training Loss: 0.06558671593666077\n",
      "Epoch 2246/20000 Training Loss: 0.07076150923967361\n",
      "Epoch 2247/20000 Training Loss: 0.0696750208735466\n",
      "Epoch 2248/20000 Training Loss: 0.061242539435625076\n",
      "Epoch 2249/20000 Training Loss: 0.06365097314119339\n",
      "Epoch 2250/20000 Training Loss: 0.09368953108787537\n",
      "Epoch 2251/20000 Training Loss: 0.08988781273365021\n",
      "Epoch 2252/20000 Training Loss: 0.08656161278486252\n",
      "Epoch 2253/20000 Training Loss: 0.0689236968755722\n",
      "Epoch 2254/20000 Training Loss: 0.07435472309589386\n",
      "Epoch 2255/20000 Training Loss: 0.05869160592556\n",
      "Epoch 2256/20000 Training Loss: 0.06756298989057541\n",
      "Epoch 2257/20000 Training Loss: 0.10223829746246338\n",
      "Epoch 2258/20000 Training Loss: 0.0839119404554367\n",
      "Epoch 2259/20000 Training Loss: 0.08515262603759766\n",
      "Epoch 2260/20000 Training Loss: 0.07752321660518646\n",
      "Epoch 2261/20000 Training Loss: 0.09880061447620392\n",
      "Epoch 2262/20000 Training Loss: 0.07303516566753387\n",
      "Epoch 2263/20000 Training Loss: 0.07852117717266083\n",
      "Epoch 2264/20000 Training Loss: 0.07141372561454773\n",
      "Epoch 2265/20000 Training Loss: 0.10698187351226807\n",
      "Epoch 2266/20000 Training Loss: 0.10649323463439941\n",
      "Epoch 2267/20000 Training Loss: 0.09366578608751297\n",
      "Epoch 2268/20000 Training Loss: 0.08685534447431564\n",
      "Epoch 2269/20000 Training Loss: 0.07183166593313217\n",
      "Epoch 2270/20000 Training Loss: 0.058896858245134354\n",
      "Epoch 2271/20000 Training Loss: 0.0700044333934784\n",
      "Epoch 2272/20000 Training Loss: 0.06836052238941193\n",
      "Epoch 2273/20000 Training Loss: 0.08480829000473022\n",
      "Epoch 2274/20000 Training Loss: 0.0627746507525444\n",
      "Epoch 2275/20000 Training Loss: 0.0672253966331482\n",
      "Epoch 2276/20000 Training Loss: 0.0866597518324852\n",
      "Epoch 2277/20000 Training Loss: 0.08541791141033173\n",
      "Epoch 2278/20000 Training Loss: 0.08359891176223755\n",
      "Epoch 2279/20000 Training Loss: 0.07165529578924179\n",
      "Epoch 2280/20000 Training Loss: 0.05065425485372543\n",
      "Epoch 2281/20000 Training Loss: 0.09719797968864441\n",
      "Epoch 2282/20000 Training Loss: 0.06696387380361557\n",
      "Epoch 2283/20000 Training Loss: 0.08437062799930573\n",
      "Epoch 2284/20000 Training Loss: 0.08055901527404785\n",
      "Epoch 2285/20000 Training Loss: 0.08971834182739258\n",
      "Epoch 2286/20000 Training Loss: 0.09161368012428284\n",
      "Epoch 2287/20000 Training Loss: 0.06887440383434296\n",
      "Epoch 2288/20000 Training Loss: 0.07846219837665558\n",
      "Epoch 2289/20000 Training Loss: 0.060641590505838394\n",
      "Epoch 2290/20000 Training Loss: 0.09648524969816208\n",
      "Epoch 2291/20000 Training Loss: 0.07048776745796204\n",
      "Epoch 2292/20000 Training Loss: 0.05865113064646721\n",
      "Epoch 2293/20000 Training Loss: 0.09240992367267609\n",
      "Epoch 2294/20000 Training Loss: 0.07644087821245193\n",
      "Epoch 2295/20000 Training Loss: 0.054331667721271515\n",
      "Epoch 2296/20000 Training Loss: 0.0913456529378891\n",
      "Epoch 2297/20000 Training Loss: 0.06895587593317032\n",
      "Epoch 2298/20000 Training Loss: 0.07827278971672058\n",
      "Epoch 2299/20000 Training Loss: 0.07604838907718658\n",
      "Epoch 2300/20000 Training Loss: 0.07657119631767273\n",
      "Epoch 2300/20000 Validation Loss: 0.06725625693798065\n",
      "Epoch 2301/20000 Training Loss: 0.0935625433921814\n",
      "Epoch 2302/20000 Training Loss: 0.07543487101793289\n",
      "Epoch 2303/20000 Training Loss: 0.08722463995218277\n",
      "Epoch 2304/20000 Training Loss: 0.06926407665014267\n",
      "Epoch 2305/20000 Training Loss: 0.07967917621135712\n",
      "Epoch 2306/20000 Training Loss: 0.07913443446159363\n",
      "Epoch 2307/20000 Training Loss: 0.06890304386615753\n",
      "Epoch 2308/20000 Training Loss: 0.06893391907215118\n",
      "Epoch 2309/20000 Training Loss: 0.07631073892116547\n",
      "Epoch 2310/20000 Training Loss: 0.07657136023044586\n",
      "Epoch 2311/20000 Training Loss: 0.0694793164730072\n",
      "Epoch 2312/20000 Training Loss: 0.08109964430332184\n",
      "Epoch 2313/20000 Training Loss: 0.06401382386684418\n",
      "Epoch 2314/20000 Training Loss: 0.08118419349193573\n",
      "Epoch 2315/20000 Training Loss: 0.08270923793315887\n",
      "Epoch 2316/20000 Training Loss: 0.058450549840927124\n",
      "Epoch 2317/20000 Training Loss: 0.07152797281742096\n",
      "Epoch 2318/20000 Training Loss: 0.07372637093067169\n",
      "Epoch 2319/20000 Training Loss: 0.07032174617052078\n",
      "Epoch 2320/20000 Training Loss: 0.07750576734542847\n",
      "Epoch 2321/20000 Training Loss: 0.061050042510032654\n",
      "Epoch 2322/20000 Training Loss: 0.07898972183465958\n",
      "Epoch 2323/20000 Training Loss: 0.05864369496703148\n",
      "Epoch 2324/20000 Training Loss: 0.06795448064804077\n",
      "Epoch 2325/20000 Training Loss: 0.0875801220536232\n",
      "Epoch 2326/20000 Training Loss: 0.07352176308631897\n",
      "Epoch 2327/20000 Training Loss: 0.08482968807220459\n",
      "Epoch 2328/20000 Training Loss: 0.07493695616722107\n",
      "Epoch 2329/20000 Training Loss: 0.08624355494976044\n",
      "Epoch 2330/20000 Training Loss: 0.0770658403635025\n",
      "Epoch 2331/20000 Training Loss: 0.07284867763519287\n",
      "Epoch 2332/20000 Training Loss: 0.08453773707151413\n",
      "Epoch 2333/20000 Training Loss: 0.08761991560459137\n",
      "Epoch 2334/20000 Training Loss: 0.09188584238290787\n",
      "Epoch 2335/20000 Training Loss: 0.08827271312475204\n",
      "Epoch 2336/20000 Training Loss: 0.07068430632352829\n",
      "Epoch 2337/20000 Training Loss: 0.0925990641117096\n",
      "Epoch 2338/20000 Training Loss: 0.07159854471683502\n",
      "Epoch 2339/20000 Training Loss: 0.06782367825508118\n",
      "Epoch 2340/20000 Training Loss: 0.06878488510847092\n",
      "Epoch 2341/20000 Training Loss: 0.07310710847377777\n",
      "Epoch 2342/20000 Training Loss: 0.057283952832221985\n",
      "Epoch 2343/20000 Training Loss: 0.06279926747083664\n",
      "Epoch 2344/20000 Training Loss: 0.07858842611312866\n",
      "Epoch 2345/20000 Training Loss: 0.06431923061609268\n",
      "Epoch 2346/20000 Training Loss: 0.08227689564228058\n",
      "Epoch 2347/20000 Training Loss: 0.0795331746339798\n",
      "Epoch 2348/20000 Training Loss: 0.0904037356376648\n",
      "Epoch 2349/20000 Training Loss: 0.0996037945151329\n",
      "Epoch 2350/20000 Training Loss: 0.09413672983646393\n",
      "Epoch 2351/20000 Training Loss: 0.09326858818531036\n",
      "Epoch 2352/20000 Training Loss: 0.07382803410291672\n",
      "Epoch 2353/20000 Training Loss: 0.08924728631973267\n",
      "Epoch 2354/20000 Training Loss: 0.06656002998352051\n",
      "Epoch 2355/20000 Training Loss: 0.08997145295143127\n",
      "Epoch 2356/20000 Training Loss: 0.05848311632871628\n",
      "Epoch 2357/20000 Training Loss: 0.07434810698032379\n",
      "Epoch 2358/20000 Training Loss: 0.07069973647594452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2359/20000 Training Loss: 0.06865246593952179\n",
      "Epoch 2360/20000 Training Loss: 0.058037735521793365\n",
      "Epoch 2361/20000 Training Loss: 0.05827707052230835\n",
      "Epoch 2362/20000 Training Loss: 0.07726965844631195\n",
      "Epoch 2363/20000 Training Loss: 0.08287332952022552\n",
      "Epoch 2364/20000 Training Loss: 0.08331416547298431\n",
      "Epoch 2365/20000 Training Loss: 0.08781581372022629\n",
      "Epoch 2366/20000 Training Loss: 0.07761381566524506\n",
      "Epoch 2367/20000 Training Loss: 0.07030050456523895\n",
      "Epoch 2368/20000 Training Loss: 0.06683748960494995\n",
      "Epoch 2369/20000 Training Loss: 0.07945474982261658\n",
      "Epoch 2370/20000 Training Loss: 0.06853698939085007\n",
      "Epoch 2371/20000 Training Loss: 0.0620146207511425\n",
      "Epoch 2372/20000 Training Loss: 0.07593760639429092\n",
      "Epoch 2373/20000 Training Loss: 0.09449358284473419\n",
      "Epoch 2374/20000 Training Loss: 0.07456015050411224\n",
      "Epoch 2375/20000 Training Loss: 0.09256531298160553\n",
      "Epoch 2376/20000 Training Loss: 0.0784815326333046\n",
      "Epoch 2377/20000 Training Loss: 0.0673675686120987\n",
      "Epoch 2378/20000 Training Loss: 0.06389585137367249\n",
      "Epoch 2379/20000 Training Loss: 0.09027383476495743\n",
      "Epoch 2380/20000 Training Loss: 0.05656988173723221\n",
      "Epoch 2381/20000 Training Loss: 0.06997261941432953\n",
      "Epoch 2382/20000 Training Loss: 0.09456181526184082\n",
      "Epoch 2383/20000 Training Loss: 0.052202507853507996\n",
      "Epoch 2384/20000 Training Loss: 0.08003110438585281\n",
      "Epoch 2385/20000 Training Loss: 0.0828450545668602\n",
      "Epoch 2386/20000 Training Loss: 0.05999568849802017\n",
      "Epoch 2387/20000 Training Loss: 0.06558121740818024\n",
      "Epoch 2388/20000 Training Loss: 0.05645964294672012\n",
      "Epoch 2389/20000 Training Loss: 0.05684632807970047\n",
      "Epoch 2390/20000 Training Loss: 0.05230890214443207\n",
      "Epoch 2391/20000 Training Loss: 0.07079125940799713\n",
      "Epoch 2392/20000 Training Loss: 0.054521217942237854\n",
      "Epoch 2393/20000 Training Loss: 0.07318731397390366\n",
      "Epoch 2394/20000 Training Loss: 0.09044736623764038\n",
      "Epoch 2395/20000 Training Loss: 0.06640061736106873\n",
      "Epoch 2396/20000 Training Loss: 0.07807955145835876\n",
      "Epoch 2397/20000 Training Loss: 0.06590332835912704\n",
      "Epoch 2398/20000 Training Loss: 0.11493165045976639\n",
      "Epoch 2399/20000 Training Loss: 0.06851647794246674\n",
      "Epoch 2400/20000 Training Loss: 0.06703997403383255\n",
      "Epoch 2400/20000 Validation Loss: 0.059358566999435425\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.059358566999435425<=============\n",
      "Epoch 2401/20000 Training Loss: 0.09448672831058502\n",
      "Epoch 2402/20000 Training Loss: 0.066815085709095\n",
      "Epoch 2403/20000 Training Loss: 0.08752680569887161\n",
      "Epoch 2404/20000 Training Loss: 0.06919068098068237\n",
      "Epoch 2405/20000 Training Loss: 0.07521861046552658\n",
      "Epoch 2406/20000 Training Loss: 0.0826473981142044\n",
      "Epoch 2407/20000 Training Loss: 0.061866044998168945\n",
      "Epoch 2408/20000 Training Loss: 0.08026647567749023\n",
      "Epoch 2409/20000 Training Loss: 0.09335645288228989\n",
      "Epoch 2410/20000 Training Loss: 0.07433988898992538\n",
      "Epoch 2411/20000 Training Loss: 0.08747594058513641\n",
      "Epoch 2412/20000 Training Loss: 0.04889504984021187\n",
      "Epoch 2413/20000 Training Loss: 0.08390126377344131\n",
      "Epoch 2414/20000 Training Loss: 0.09633015841245651\n",
      "Epoch 2415/20000 Training Loss: 0.0786975771188736\n",
      "Epoch 2416/20000 Training Loss: 0.06802569329738617\n",
      "Epoch 2417/20000 Training Loss: 0.06869843602180481\n",
      "Epoch 2418/20000 Training Loss: 0.06586137413978577\n",
      "Epoch 2419/20000 Training Loss: 0.07735810428857803\n",
      "Epoch 2420/20000 Training Loss: 0.07477876543998718\n",
      "Epoch 2421/20000 Training Loss: 0.06534165143966675\n",
      "Epoch 2422/20000 Training Loss: 0.06405224651098251\n",
      "Epoch 2423/20000 Training Loss: 0.07773081958293915\n",
      "Epoch 2424/20000 Training Loss: 0.09268458187580109\n",
      "Epoch 2425/20000 Training Loss: 0.07254637777805328\n",
      "Epoch 2426/20000 Training Loss: 0.08092159032821655\n",
      "Epoch 2427/20000 Training Loss: 0.07013756781816483\n",
      "Epoch 2428/20000 Training Loss: 0.05363558977842331\n",
      "Epoch 2429/20000 Training Loss: 0.06225571781396866\n",
      "Epoch 2430/20000 Training Loss: 0.1042034775018692\n",
      "Epoch 2431/20000 Training Loss: 0.07999590039253235\n",
      "Epoch 2432/20000 Training Loss: 0.07752665877342224\n",
      "Epoch 2433/20000 Training Loss: 0.05880264937877655\n",
      "Epoch 2434/20000 Training Loss: 0.063911572098732\n",
      "Epoch 2435/20000 Training Loss: 0.09024547040462494\n",
      "Epoch 2436/20000 Training Loss: 0.0622728168964386\n",
      "Epoch 2437/20000 Training Loss: 0.07421205937862396\n",
      "Epoch 2438/20000 Training Loss: 0.07781262695789337\n",
      "Epoch 2439/20000 Training Loss: 0.07251927256584167\n",
      "Epoch 2440/20000 Training Loss: 0.06346450001001358\n",
      "Epoch 2441/20000 Training Loss: 0.08200770616531372\n",
      "Epoch 2442/20000 Training Loss: 0.06323464214801788\n",
      "Epoch 2443/20000 Training Loss: 0.059713516384363174\n",
      "Epoch 2444/20000 Training Loss: 0.060867439955472946\n",
      "Epoch 2445/20000 Training Loss: 0.06554560363292694\n",
      "Epoch 2446/20000 Training Loss: 0.07749711722135544\n",
      "Epoch 2447/20000 Training Loss: 0.053905684500932693\n",
      "Epoch 2448/20000 Training Loss: 0.05904090404510498\n",
      "Epoch 2449/20000 Training Loss: 0.07528777420520782\n",
      "Epoch 2450/20000 Training Loss: 0.0984499454498291\n",
      "Epoch 2451/20000 Training Loss: 0.06487757712602615\n",
      "Epoch 2452/20000 Training Loss: 0.0868735983967781\n",
      "Epoch 2453/20000 Training Loss: 0.08416120707988739\n",
      "Epoch 2454/20000 Training Loss: 0.07155119627714157\n",
      "Epoch 2455/20000 Training Loss: 0.10458482801914215\n",
      "Epoch 2456/20000 Training Loss: 0.06142716109752655\n",
      "Epoch 2457/20000 Training Loss: 0.12321878969669342\n",
      "Epoch 2458/20000 Training Loss: 0.0701712816953659\n",
      "Epoch 2459/20000 Training Loss: 0.07217375934123993\n",
      "Epoch 2460/20000 Training Loss: 0.07003501057624817\n",
      "Epoch 2461/20000 Training Loss: 0.06851641833782196\n",
      "Epoch 2462/20000 Training Loss: 0.0898057371377945\n",
      "Epoch 2463/20000 Training Loss: 0.08558499813079834\n",
      "Epoch 2464/20000 Training Loss: 0.07090810686349869\n",
      "Epoch 2465/20000 Training Loss: 0.07752754539251328\n",
      "Epoch 2466/20000 Training Loss: 0.06426925957202911\n",
      "Epoch 2467/20000 Training Loss: 0.0491287037730217\n",
      "Epoch 2468/20000 Training Loss: 0.09402097761631012\n",
      "Epoch 2469/20000 Training Loss: 0.07577678561210632\n",
      "Epoch 2470/20000 Training Loss: 0.06911678612232208\n",
      "Epoch 2471/20000 Training Loss: 0.07167989760637283\n",
      "Epoch 2472/20000 Training Loss: 0.07989449799060822\n",
      "Epoch 2473/20000 Training Loss: 0.07279166579246521\n",
      "Epoch 2474/20000 Training Loss: 0.11067437380552292\n",
      "Epoch 2475/20000 Training Loss: 0.0758134201169014\n",
      "Epoch 2476/20000 Training Loss: 0.0765460878610611\n",
      "Epoch 2477/20000 Training Loss: 0.048100754618644714\n",
      "Epoch 2478/20000 Training Loss: 0.07223068177700043\n",
      "Epoch 2479/20000 Training Loss: 0.11688379943370819\n",
      "Epoch 2480/20000 Training Loss: 0.09910289198160172\n",
      "Epoch 2481/20000 Training Loss: 0.08378104120492935\n",
      "Epoch 2482/20000 Training Loss: 0.06886784732341766\n",
      "Epoch 2483/20000 Training Loss: 0.07611700892448425\n",
      "Epoch 2484/20000 Training Loss: 0.0769011378288269\n",
      "Epoch 2485/20000 Training Loss: 0.0748567208647728\n",
      "Epoch 2486/20000 Training Loss: 0.07102397084236145\n",
      "Epoch 2487/20000 Training Loss: 0.07321429252624512\n",
      "Epoch 2488/20000 Training Loss: 0.07011330872774124\n",
      "Epoch 2489/20000 Training Loss: 0.06385014951229095\n",
      "Epoch 2490/20000 Training Loss: 0.08517684042453766\n",
      "Epoch 2491/20000 Training Loss: 0.0750068873167038\n",
      "Epoch 2492/20000 Training Loss: 0.08404210209846497\n",
      "Epoch 2493/20000 Training Loss: 0.053056687116622925\n",
      "Epoch 2494/20000 Training Loss: 0.0526551827788353\n",
      "Epoch 2495/20000 Training Loss: 0.07242478430271149\n",
      "Epoch 2496/20000 Training Loss: 0.05429195612668991\n",
      "Epoch 2497/20000 Training Loss: 0.05284357815980911\n",
      "Epoch 2498/20000 Training Loss: 0.06286507844924927\n",
      "Epoch 2499/20000 Training Loss: 0.0909874439239502\n",
      "Epoch 2500/20000 Training Loss: 0.054682739078998566\n",
      "Epoch 2500/20000 Validation Loss: 0.062186941504478455\n",
      "Epoch 2501/20000 Training Loss: 0.08313066512346268\n",
      "Epoch 2502/20000 Training Loss: 0.07453006505966187\n",
      "Epoch 2503/20000 Training Loss: 0.0758344978094101\n",
      "Epoch 2504/20000 Training Loss: 0.07352660596370697\n",
      "Epoch 2505/20000 Training Loss: 0.08904841542243958\n",
      "Epoch 2506/20000 Training Loss: 0.0580747090280056\n",
      "Epoch 2507/20000 Training Loss: 0.09280292689800262\n",
      "Epoch 2508/20000 Training Loss: 0.05489012598991394\n",
      "Epoch 2509/20000 Training Loss: 0.06095362454652786\n",
      "Epoch 2510/20000 Training Loss: 0.07908559590578079\n",
      "Epoch 2511/20000 Training Loss: 0.0707215666770935\n",
      "Epoch 2512/20000 Training Loss: 0.10266286879777908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2513/20000 Training Loss: 0.07532051205635071\n",
      "Epoch 2514/20000 Training Loss: 0.061697497963905334\n",
      "Epoch 2515/20000 Training Loss: 0.07946896553039551\n",
      "Epoch 2516/20000 Training Loss: 0.05868782103061676\n",
      "Epoch 2517/20000 Training Loss: 0.09975220263004303\n",
      "Epoch 2518/20000 Training Loss: 0.07303659617900848\n",
      "Epoch 2519/20000 Training Loss: 0.10069385915994644\n",
      "Epoch 2520/20000 Training Loss: 0.08092302083969116\n",
      "Epoch 2521/20000 Training Loss: 0.06781352311372757\n",
      "Epoch 2522/20000 Training Loss: 0.06688983738422394\n",
      "Epoch 2523/20000 Training Loss: 0.07762955129146576\n",
      "Epoch 2524/20000 Training Loss: 0.0665007159113884\n",
      "Epoch 2525/20000 Training Loss: 0.06892295181751251\n",
      "Epoch 2526/20000 Training Loss: 0.06190106272697449\n",
      "Epoch 2527/20000 Training Loss: 0.09497955441474915\n",
      "Epoch 2528/20000 Training Loss: 0.073136106133461\n",
      "Epoch 2529/20000 Training Loss: 0.053002938628196716\n",
      "Epoch 2530/20000 Training Loss: 0.06895796209573746\n",
      "Epoch 2531/20000 Training Loss: 0.06804942339658737\n",
      "Epoch 2532/20000 Training Loss: 0.07646509259939194\n",
      "Epoch 2533/20000 Training Loss: 0.08267916738986969\n",
      "Epoch 2534/20000 Training Loss: 0.08088693022727966\n",
      "Epoch 2535/20000 Training Loss: 0.06197582185268402\n",
      "Epoch 2536/20000 Training Loss: 0.06556383520364761\n",
      "Epoch 2537/20000 Training Loss: 0.0643831267952919\n",
      "Epoch 2538/20000 Training Loss: 0.07265233993530273\n",
      "Epoch 2539/20000 Training Loss: 0.06166193634271622\n",
      "Epoch 2540/20000 Training Loss: 0.06498819589614868\n",
      "Epoch 2541/20000 Training Loss: 0.07843965291976929\n",
      "Epoch 2542/20000 Training Loss: 0.076363205909729\n",
      "Epoch 2543/20000 Training Loss: 0.06366386264562607\n",
      "Epoch 2544/20000 Training Loss: 0.07817645370960236\n",
      "Epoch 2545/20000 Training Loss: 0.060414474457502365\n",
      "Epoch 2546/20000 Training Loss: 0.06517299264669418\n",
      "Epoch 2547/20000 Training Loss: 0.08652599155902863\n",
      "Epoch 2548/20000 Training Loss: 0.058911994099617004\n",
      "Epoch 2549/20000 Training Loss: 0.0879020094871521\n",
      "Epoch 2550/20000 Training Loss: 0.08434354513883591\n",
      "Epoch 2551/20000 Training Loss: 0.0585397332906723\n",
      "Epoch 2552/20000 Training Loss: 0.09452144801616669\n",
      "Epoch 2553/20000 Training Loss: 0.06917736679315567\n",
      "Epoch 2554/20000 Training Loss: 0.058493681252002716\n",
      "Epoch 2555/20000 Training Loss: 0.08355346322059631\n",
      "Epoch 2556/20000 Training Loss: 0.06865793466567993\n",
      "Epoch 2557/20000 Training Loss: 0.06829243898391724\n",
      "Epoch 2558/20000 Training Loss: 0.0851660817861557\n",
      "Epoch 2559/20000 Training Loss: 0.08517628163099289\n",
      "Epoch 2560/20000 Training Loss: 0.07482142746448517\n",
      "Epoch 2561/20000 Training Loss: 0.06583400070667267\n",
      "Epoch 2562/20000 Training Loss: 0.07787090539932251\n",
      "Epoch 2563/20000 Training Loss: 0.05320177599787712\n",
      "Epoch 2564/20000 Training Loss: 0.053775157779455185\n",
      "Epoch 2565/20000 Training Loss: 0.06205049902200699\n",
      "Epoch 2566/20000 Training Loss: 0.06496643275022507\n",
      "Epoch 2567/20000 Training Loss: 0.08758221566677094\n",
      "Epoch 2568/20000 Training Loss: 0.0804864913225174\n",
      "Epoch 2569/20000 Training Loss: 0.08972765505313873\n",
      "Epoch 2570/20000 Training Loss: 0.0781944990158081\n",
      "Epoch 2571/20000 Training Loss: 0.06694729626178741\n",
      "Epoch 2572/20000 Training Loss: 0.08303746581077576\n",
      "Epoch 2573/20000 Training Loss: 0.06829723715782166\n",
      "Epoch 2574/20000 Training Loss: 0.08234063535928726\n",
      "Epoch 2575/20000 Training Loss: 0.09973865747451782\n",
      "Epoch 2576/20000 Training Loss: 0.09130540490150452\n",
      "Epoch 2577/20000 Training Loss: 0.055032163858413696\n",
      "Epoch 2578/20000 Training Loss: 0.06959603726863861\n",
      "Epoch 2579/20000 Training Loss: 0.067710742354393\n",
      "Epoch 2580/20000 Training Loss: 0.06284093111753464\n",
      "Epoch 2581/20000 Training Loss: 0.0708523690700531\n",
      "Epoch 2582/20000 Training Loss: 0.0476745069026947\n",
      "Epoch 2583/20000 Training Loss: 0.08527056872844696\n",
      "Epoch 2584/20000 Training Loss: 0.07066220045089722\n",
      "Epoch 2585/20000 Training Loss: 0.06332144886255264\n",
      "Epoch 2586/20000 Training Loss: 0.06193379685282707\n",
      "Epoch 2587/20000 Training Loss: 0.06830180436372757\n",
      "Epoch 2588/20000 Training Loss: 0.07601133733987808\n",
      "Epoch 2589/20000 Training Loss: 0.07960506528615952\n",
      "Epoch 2590/20000 Training Loss: 0.05646003037691116\n",
      "Epoch 2591/20000 Training Loss: 0.08438326418399811\n",
      "Epoch 2592/20000 Training Loss: 0.062450408935546875\n",
      "Epoch 2593/20000 Training Loss: 0.1079457700252533\n",
      "Epoch 2594/20000 Training Loss: 0.06615576148033142\n",
      "Epoch 2595/20000 Training Loss: 0.0794561579823494\n",
      "Epoch 2596/20000 Training Loss: 0.057118792086839676\n",
      "Epoch 2597/20000 Training Loss: 0.056008562445640564\n",
      "Epoch 2598/20000 Training Loss: 0.05957409739494324\n",
      "Epoch 2599/20000 Training Loss: 0.05853120982646942\n",
      "Epoch 2600/20000 Training Loss: 0.06169464439153671\n",
      "Epoch 2600/20000 Validation Loss: 0.06052247807383537\n",
      "Epoch 2601/20000 Training Loss: 0.07564117014408112\n",
      "Epoch 2602/20000 Training Loss: 0.06477419286966324\n",
      "Epoch 2603/20000 Training Loss: 0.0652952641248703\n",
      "Epoch 2604/20000 Training Loss: 0.07641573250293732\n",
      "Epoch 2605/20000 Training Loss: 0.0597352460026741\n",
      "Epoch 2606/20000 Training Loss: 0.07349694520235062\n",
      "Epoch 2607/20000 Training Loss: 0.0709751546382904\n",
      "Epoch 2608/20000 Training Loss: 0.06799283623695374\n",
      "Epoch 2609/20000 Training Loss: 0.08353635668754578\n",
      "Epoch 2610/20000 Training Loss: 0.08930706977844238\n",
      "Epoch 2611/20000 Training Loss: 0.07146979868412018\n",
      "Epoch 2612/20000 Training Loss: 0.06964531540870667\n",
      "Epoch 2613/20000 Training Loss: 0.08435849845409393\n",
      "Epoch 2614/20000 Training Loss: 0.06879943609237671\n",
      "Epoch 2615/20000 Training Loss: 0.06547355651855469\n",
      "Epoch 2616/20000 Training Loss: 0.07647795975208282\n",
      "Epoch 2617/20000 Training Loss: 0.07434924691915512\n",
      "Epoch 2618/20000 Training Loss: 0.05753451958298683\n",
      "Epoch 2619/20000 Training Loss: 0.08115191757678986\n",
      "Epoch 2620/20000 Training Loss: 0.08892786502838135\n",
      "Epoch 2621/20000 Training Loss: 0.048185087740421295\n",
      "Epoch 2622/20000 Training Loss: 0.10610231012105942\n",
      "Epoch 2623/20000 Training Loss: 0.08301901817321777\n",
      "Epoch 2624/20000 Training Loss: 0.08098592609167099\n",
      "Epoch 2625/20000 Training Loss: 0.08143267035484314\n",
      "Epoch 2626/20000 Training Loss: 0.08488201349973679\n",
      "Epoch 2627/20000 Training Loss: 0.10826361179351807\n",
      "Epoch 2628/20000 Training Loss: 0.07669363915920258\n",
      "Epoch 2629/20000 Training Loss: 0.06240171939134598\n",
      "Epoch 2630/20000 Training Loss: 0.07618322223424911\n",
      "Epoch 2631/20000 Training Loss: 0.07521712779998779\n",
      "Epoch 2632/20000 Training Loss: 0.10110794752836227\n",
      "Epoch 2633/20000 Training Loss: 0.07748986780643463\n",
      "Epoch 2634/20000 Training Loss: 0.08034950494766235\n",
      "Epoch 2635/20000 Training Loss: 0.07767147570848465\n",
      "Epoch 2636/20000 Training Loss: 0.06425277143716812\n",
      "Epoch 2637/20000 Training Loss: 0.057904765009880066\n",
      "Epoch 2638/20000 Training Loss: 0.08088531345129013\n",
      "Epoch 2639/20000 Training Loss: 0.08406588435173035\n",
      "Epoch 2640/20000 Training Loss: 0.057840242981910706\n",
      "Epoch 2641/20000 Training Loss: 0.06867721676826477\n",
      "Epoch 2642/20000 Training Loss: 0.07411599159240723\n",
      "Epoch 2643/20000 Training Loss: 0.0726810097694397\n",
      "Epoch 2644/20000 Training Loss: 0.05348986014723778\n",
      "Epoch 2645/20000 Training Loss: 0.08044901490211487\n",
      "Epoch 2646/20000 Training Loss: 0.05212317407131195\n",
      "Epoch 2647/20000 Training Loss: 0.06735030561685562\n",
      "Epoch 2648/20000 Training Loss: 0.056919656693935394\n",
      "Epoch 2649/20000 Training Loss: 0.10230144113302231\n",
      "Epoch 2650/20000 Training Loss: 0.0867639109492302\n",
      "Epoch 2651/20000 Training Loss: 0.08269250392913818\n",
      "Epoch 2652/20000 Training Loss: 0.04994179308414459\n",
      "Epoch 2653/20000 Training Loss: 0.06501253694295883\n",
      "Epoch 2654/20000 Training Loss: 0.08539847284555435\n",
      "Epoch 2655/20000 Training Loss: 0.0719652846455574\n",
      "Epoch 2656/20000 Training Loss: 0.07213158160448074\n",
      "Epoch 2657/20000 Training Loss: 0.08555834740400314\n",
      "Epoch 2658/20000 Training Loss: 0.07302485406398773\n",
      "Epoch 2659/20000 Training Loss: 0.06078554689884186\n",
      "Epoch 2660/20000 Training Loss: 0.09236705303192139\n",
      "Epoch 2661/20000 Training Loss: 0.08776547014713287\n",
      "Epoch 2662/20000 Training Loss: 0.09721498191356659\n",
      "Epoch 2663/20000 Training Loss: 0.059893347322940826\n",
      "Epoch 2664/20000 Training Loss: 0.05869381129741669\n",
      "Epoch 2665/20000 Training Loss: 0.07706981897354126\n",
      "Epoch 2666/20000 Training Loss: 0.08433180302381516\n",
      "Epoch 2667/20000 Training Loss: 0.06619591265916824\n",
      "Epoch 2668/20000 Training Loss: 0.06196952611207962\n",
      "Epoch 2669/20000 Training Loss: 0.07051399350166321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2670/20000 Training Loss: 0.08298369497060776\n",
      "Epoch 2671/20000 Training Loss: 0.07443197071552277\n",
      "Epoch 2672/20000 Training Loss: 0.06360991299152374\n",
      "Epoch 2673/20000 Training Loss: 0.06929680705070496\n",
      "Epoch 2674/20000 Training Loss: 0.06721184402704239\n",
      "Epoch 2675/20000 Training Loss: 0.0774388462305069\n",
      "Epoch 2676/20000 Training Loss: 0.05566001683473587\n",
      "Epoch 2677/20000 Training Loss: 0.08030150830745697\n",
      "Epoch 2678/20000 Training Loss: 0.060757920145988464\n",
      "Epoch 2679/20000 Training Loss: 0.06650128960609436\n",
      "Epoch 2680/20000 Training Loss: 0.05427195876836777\n",
      "Epoch 2681/20000 Training Loss: 0.07104133069515228\n",
      "Epoch 2682/20000 Training Loss: 0.0573749840259552\n",
      "Epoch 2683/20000 Training Loss: 0.09410031139850616\n",
      "Epoch 2684/20000 Training Loss: 0.08207038789987564\n",
      "Epoch 2685/20000 Training Loss: 0.0672587975859642\n",
      "Epoch 2686/20000 Training Loss: 0.06449584662914276\n",
      "Epoch 2687/20000 Training Loss: 0.07324369251728058\n",
      "Epoch 2688/20000 Training Loss: 0.06492390483617783\n",
      "Epoch 2689/20000 Training Loss: 0.06734894961118698\n",
      "Epoch 2690/20000 Training Loss: 0.05624149739742279\n",
      "Epoch 2691/20000 Training Loss: 0.06293298304080963\n",
      "Epoch 2692/20000 Training Loss: 0.09787347167730331\n",
      "Epoch 2693/20000 Training Loss: 0.0734129548072815\n",
      "Epoch 2694/20000 Training Loss: 0.07634185254573822\n",
      "Epoch 2695/20000 Training Loss: 0.06934042274951935\n",
      "Epoch 2696/20000 Training Loss: 0.06490024924278259\n",
      "Epoch 2697/20000 Training Loss: 0.06562310457229614\n",
      "Epoch 2698/20000 Training Loss: 0.06810247898101807\n",
      "Epoch 2699/20000 Training Loss: 0.05963394045829773\n",
      "Epoch 2700/20000 Training Loss: 0.07490851730108261\n",
      "Epoch 2700/20000 Validation Loss: 0.05338314175605774\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.05338314175605774<=============\n",
      "Epoch 2701/20000 Training Loss: 0.059115998446941376\n",
      "Epoch 2702/20000 Training Loss: 0.07797496020793915\n",
      "Epoch 2703/20000 Training Loss: 0.05916041135787964\n",
      "Epoch 2704/20000 Training Loss: 0.0799998864531517\n",
      "Epoch 2705/20000 Training Loss: 0.058170437812805176\n",
      "Epoch 2706/20000 Training Loss: 0.09401652216911316\n",
      "Epoch 2707/20000 Training Loss: 0.0522187240421772\n",
      "Epoch 2708/20000 Training Loss: 0.0717947781085968\n",
      "Epoch 2709/20000 Training Loss: 0.05866573005914688\n",
      "Epoch 2710/20000 Training Loss: 0.075707346200943\n",
      "Epoch 2711/20000 Training Loss: 0.07468275725841522\n",
      "Epoch 2712/20000 Training Loss: 0.058140382170677185\n",
      "Epoch 2713/20000 Training Loss: 0.05954170972108841\n",
      "Epoch 2714/20000 Training Loss: 0.0821753665804863\n",
      "Epoch 2715/20000 Training Loss: 0.08020017296075821\n",
      "Epoch 2716/20000 Training Loss: 0.07136065512895584\n",
      "Epoch 2717/20000 Training Loss: 0.0630480945110321\n",
      "Epoch 2718/20000 Training Loss: 0.08159562945365906\n",
      "Epoch 2719/20000 Training Loss: 0.07626724988222122\n",
      "Epoch 2720/20000 Training Loss: 0.09026584774255753\n",
      "Epoch 2721/20000 Training Loss: 0.060508981347084045\n",
      "Epoch 2722/20000 Training Loss: 0.07822883129119873\n",
      "Epoch 2723/20000 Training Loss: 0.07348603010177612\n",
      "Epoch 2724/20000 Training Loss: 0.06227375566959381\n",
      "Epoch 2725/20000 Training Loss: 0.065364308655262\n",
      "Epoch 2726/20000 Training Loss: 0.0925675630569458\n",
      "Epoch 2727/20000 Training Loss: 0.10143376886844635\n",
      "Epoch 2728/20000 Training Loss: 0.08504108339548111\n",
      "Epoch 2729/20000 Training Loss: 0.08724804222583771\n",
      "Epoch 2730/20000 Training Loss: 0.08171626925468445\n",
      "Epoch 2731/20000 Training Loss: 0.08889968693256378\n",
      "Epoch 2732/20000 Training Loss: 0.06397971510887146\n",
      "Epoch 2733/20000 Training Loss: 0.07231295108795166\n",
      "Epoch 2734/20000 Training Loss: 0.10797689855098724\n",
      "Epoch 2735/20000 Training Loss: 0.09153582155704498\n",
      "Epoch 2736/20000 Training Loss: 0.05886626988649368\n",
      "Epoch 2737/20000 Training Loss: 0.0805785283446312\n",
      "Epoch 2738/20000 Training Loss: 0.09456507116556168\n",
      "Epoch 2739/20000 Training Loss: 0.10208176076412201\n",
      "Epoch 2740/20000 Training Loss: 0.06321615725755692\n",
      "Epoch 2741/20000 Training Loss: 0.0640767440199852\n",
      "Epoch 2742/20000 Training Loss: 0.08445442467927933\n",
      "Epoch 2743/20000 Training Loss: 0.10578157007694244\n",
      "Epoch 2744/20000 Training Loss: 0.07691015303134918\n",
      "Epoch 2745/20000 Training Loss: 0.07759111374616623\n",
      "Epoch 2746/20000 Training Loss: 0.06716688722372055\n",
      "Epoch 2747/20000 Training Loss: 0.06686766445636749\n",
      "Epoch 2748/20000 Training Loss: 0.076685331761837\n",
      "Epoch 2749/20000 Training Loss: 0.08532978594303131\n",
      "Epoch 2750/20000 Training Loss: 0.06545711308717728\n",
      "Epoch 2751/20000 Training Loss: 0.08165444433689117\n",
      "Epoch 2752/20000 Training Loss: 0.06927603483200073\n",
      "Epoch 2753/20000 Training Loss: 0.067207932472229\n",
      "Epoch 2754/20000 Training Loss: 0.09180258214473724\n",
      "Epoch 2755/20000 Training Loss: 0.061983995139598846\n",
      "Epoch 2756/20000 Training Loss: 0.039121177047491074\n",
      "Epoch 2757/20000 Training Loss: 0.08325913548469543\n",
      "Epoch 2758/20000 Training Loss: 0.07665614783763885\n",
      "Epoch 2759/20000 Training Loss: 0.09163957834243774\n",
      "Epoch 2760/20000 Training Loss: 0.07053965330123901\n",
      "Epoch 2761/20000 Training Loss: 0.06612631678581238\n",
      "Epoch 2762/20000 Training Loss: 0.07576492428779602\n",
      "Epoch 2763/20000 Training Loss: 0.04688147455453873\n",
      "Epoch 2764/20000 Training Loss: 0.06516298651695251\n",
      "Epoch 2765/20000 Training Loss: 0.05710487812757492\n",
      "Epoch 2766/20000 Training Loss: 0.06615472584962845\n",
      "Epoch 2767/20000 Training Loss: 0.07743871957063675\n",
      "Epoch 2768/20000 Training Loss: 0.0715128630399704\n",
      "Epoch 2769/20000 Training Loss: 0.06668027490377426\n",
      "Epoch 2770/20000 Training Loss: 0.08514958620071411\n",
      "Epoch 2771/20000 Training Loss: 0.07142198830842972\n",
      "Epoch 2772/20000 Training Loss: 0.05766374617815018\n",
      "Epoch 2773/20000 Training Loss: 0.061545610427856445\n",
      "Epoch 2774/20000 Training Loss: 0.08504581451416016\n",
      "Epoch 2775/20000 Training Loss: 0.07842711359262466\n",
      "Epoch 2776/20000 Training Loss: 0.0922616720199585\n",
      "Epoch 2777/20000 Training Loss: 0.07268059253692627\n",
      "Epoch 2778/20000 Training Loss: 0.07247011363506317\n",
      "Epoch 2779/20000 Training Loss: 0.07943275570869446\n",
      "Epoch 2780/20000 Training Loss: 0.05364803224802017\n",
      "Epoch 2781/20000 Training Loss: 0.05907580256462097\n",
      "Epoch 2782/20000 Training Loss: 0.07601027190685272\n",
      "Epoch 2783/20000 Training Loss: 0.07946784049272537\n",
      "Epoch 2784/20000 Training Loss: 0.09867524355649948\n",
      "Epoch 2785/20000 Training Loss: 0.09444321691989899\n",
      "Epoch 2786/20000 Training Loss: 0.057737283408641815\n",
      "Epoch 2787/20000 Training Loss: 0.07420700043439865\n",
      "Epoch 2788/20000 Training Loss: 0.0923738032579422\n",
      "Epoch 2789/20000 Training Loss: 0.08812041580677032\n",
      "Epoch 2790/20000 Training Loss: 0.07572885602712631\n",
      "Epoch 2791/20000 Training Loss: 0.045852359384298325\n",
      "Epoch 2792/20000 Training Loss: 0.06724488735198975\n",
      "Epoch 2793/20000 Training Loss: 0.06375645846128464\n",
      "Epoch 2794/20000 Training Loss: 0.0814085304737091\n",
      "Epoch 2795/20000 Training Loss: 0.060221776366233826\n",
      "Epoch 2796/20000 Training Loss: 0.09871542453765869\n",
      "Epoch 2797/20000 Training Loss: 0.05247253179550171\n",
      "Epoch 2798/20000 Training Loss: 0.06254253536462784\n",
      "Epoch 2799/20000 Training Loss: 0.07990417629480362\n",
      "Epoch 2800/20000 Training Loss: 0.05745109170675278\n",
      "Epoch 2800/20000 Validation Loss: 0.07545511424541473\n",
      "Epoch 2801/20000 Training Loss: 0.04533175006508827\n",
      "Epoch 2802/20000 Training Loss: 0.06607234477996826\n",
      "Epoch 2803/20000 Training Loss: 0.06080307066440582\n",
      "Epoch 2804/20000 Training Loss: 0.09147253632545471\n",
      "Epoch 2805/20000 Training Loss: 0.08883063495159149\n",
      "Epoch 2806/20000 Training Loss: 0.07588668167591095\n",
      "Epoch 2807/20000 Training Loss: 0.06166847050189972\n",
      "Epoch 2808/20000 Training Loss: 0.10488024353981018\n",
      "Epoch 2809/20000 Training Loss: 0.08232498914003372\n",
      "Epoch 2810/20000 Training Loss: 0.05720493197441101\n",
      "Epoch 2811/20000 Training Loss: 0.07056456804275513\n",
      "Epoch 2812/20000 Training Loss: 0.06605767458677292\n",
      "Epoch 2813/20000 Training Loss: 0.06997355818748474\n",
      "Epoch 2814/20000 Training Loss: 0.07938237488269806\n",
      "Epoch 2815/20000 Training Loss: 0.058476418256759644\n",
      "Epoch 2816/20000 Training Loss: 0.05736187472939491\n",
      "Epoch 2817/20000 Training Loss: 0.0726463571190834\n",
      "Epoch 2818/20000 Training Loss: 0.06892745196819305\n",
      "Epoch 2819/20000 Training Loss: 0.09079387784004211\n",
      "Epoch 2820/20000 Training Loss: 0.07523281872272491\n",
      "Epoch 2821/20000 Training Loss: 0.06640370190143585\n",
      "Epoch 2822/20000 Training Loss: 0.08212249726057053\n",
      "Epoch 2823/20000 Training Loss: 0.10286775231361389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2824/20000 Training Loss: 0.08727893233299255\n",
      "Epoch 2825/20000 Training Loss: 0.06636542081832886\n",
      "Epoch 2826/20000 Training Loss: 0.08939534425735474\n",
      "Epoch 2827/20000 Training Loss: 0.06241645663976669\n",
      "Epoch 2828/20000 Training Loss: 0.06514113396406174\n",
      "Epoch 2829/20000 Training Loss: 0.06254584342241287\n",
      "Epoch 2830/20000 Training Loss: 0.07995587587356567\n",
      "Epoch 2831/20000 Training Loss: 0.09688697755336761\n",
      "Epoch 2832/20000 Training Loss: 0.07007359713315964\n",
      "Epoch 2833/20000 Training Loss: 0.05670282244682312\n",
      "Epoch 2834/20000 Training Loss: 0.06413434445858002\n",
      "Epoch 2835/20000 Training Loss: 0.09072592854499817\n",
      "Epoch 2836/20000 Training Loss: 0.08481740951538086\n",
      "Epoch 2837/20000 Training Loss: 0.07930748164653778\n",
      "Epoch 2838/20000 Training Loss: 0.09490732848644257\n",
      "Epoch 2839/20000 Training Loss: 0.06769412755966187\n",
      "Epoch 2840/20000 Training Loss: 0.07601150870323181\n",
      "Epoch 2841/20000 Training Loss: 0.07153439521789551\n",
      "Epoch 2842/20000 Training Loss: 0.09787572920322418\n",
      "Epoch 2843/20000 Training Loss: 0.09311853349208832\n",
      "Epoch 2844/20000 Training Loss: 0.08529811352491379\n",
      "Epoch 2845/20000 Training Loss: 0.08903364837169647\n",
      "Epoch 2846/20000 Training Loss: 0.0602235421538353\n",
      "Epoch 2847/20000 Training Loss: 0.08606794476509094\n",
      "Epoch 2848/20000 Training Loss: 0.09487850964069366\n",
      "Epoch 2849/20000 Training Loss: 0.08393821120262146\n",
      "Epoch 2850/20000 Training Loss: 0.0806056335568428\n",
      "Epoch 2851/20000 Training Loss: 0.06298185884952545\n",
      "Epoch 2852/20000 Training Loss: 0.06904594600200653\n",
      "Epoch 2853/20000 Training Loss: 0.07489336282014847\n",
      "Epoch 2854/20000 Training Loss: 0.08460184931755066\n",
      "Epoch 2855/20000 Training Loss: 0.10005887597799301\n",
      "Epoch 2856/20000 Training Loss: 0.07632840424776077\n",
      "Epoch 2857/20000 Training Loss: 0.07250486314296722\n",
      "Epoch 2858/20000 Training Loss: 0.0732661634683609\n",
      "Epoch 2859/20000 Training Loss: 0.10084669291973114\n",
      "Epoch 2860/20000 Training Loss: 0.07042450457811356\n",
      "Epoch 2861/20000 Training Loss: 0.058119870722293854\n",
      "Epoch 2862/20000 Training Loss: 0.06164553016424179\n",
      "Epoch 2863/20000 Training Loss: 0.08819106221199036\n",
      "Epoch 2864/20000 Training Loss: 0.07993052154779434\n",
      "Epoch 2865/20000 Training Loss: 0.06899815052747726\n",
      "Epoch 2866/20000 Training Loss: 0.060160428285598755\n",
      "Epoch 2867/20000 Training Loss: 0.06944742053747177\n",
      "Epoch 2868/20000 Training Loss: 0.09170553088188171\n",
      "Epoch 2869/20000 Training Loss: 0.07868972420692444\n",
      "Epoch 2870/20000 Training Loss: 0.06674686074256897\n",
      "Epoch 2871/20000 Training Loss: 0.06453482061624527\n",
      "Epoch 2872/20000 Training Loss: 0.08805621415376663\n",
      "Epoch 2873/20000 Training Loss: 0.06030777096748352\n",
      "Epoch 2874/20000 Training Loss: 0.07226414978504181\n",
      "Epoch 2875/20000 Training Loss: 0.09494778513908386\n",
      "Epoch 2876/20000 Training Loss: 0.045868560671806335\n",
      "Epoch 2877/20000 Training Loss: 0.07810122519731522\n",
      "Epoch 2878/20000 Training Loss: 0.07357005774974823\n",
      "Epoch 2879/20000 Training Loss: 0.07748779654502869\n",
      "Epoch 2880/20000 Training Loss: 0.04531541466712952\n",
      "Epoch 2881/20000 Training Loss: 0.08148054778575897\n",
      "Epoch 2882/20000 Training Loss: 0.08603726327419281\n",
      "Epoch 2883/20000 Training Loss: 0.0831589549779892\n",
      "Epoch 2884/20000 Training Loss: 0.08510728180408478\n",
      "Epoch 2885/20000 Training Loss: 0.07774088531732559\n",
      "Epoch 2886/20000 Training Loss: 0.06675098091363907\n",
      "Epoch 2887/20000 Training Loss: 0.06577803194522858\n",
      "Epoch 2888/20000 Training Loss: 0.08146929740905762\n",
      "Epoch 2889/20000 Training Loss: 0.08803479373455048\n",
      "Epoch 2890/20000 Training Loss: 0.09489433467388153\n",
      "Epoch 2891/20000 Training Loss: 0.05993451923131943\n",
      "Epoch 2892/20000 Training Loss: 0.05921556428074837\n",
      "Epoch 2893/20000 Training Loss: 0.11328573524951935\n",
      "Epoch 2894/20000 Training Loss: 0.07588508725166321\n",
      "Epoch 2895/20000 Training Loss: 0.06392134726047516\n",
      "Epoch 2896/20000 Training Loss: 0.08120883256196976\n",
      "Epoch 2897/20000 Training Loss: 0.06317982077598572\n",
      "Epoch 2898/20000 Training Loss: 0.09409061819314957\n",
      "Epoch 2899/20000 Training Loss: 0.07283782213926315\n",
      "Epoch 2900/20000 Training Loss: 0.08268657326698303\n",
      "Epoch 2900/20000 Validation Loss: 0.05612342059612274\n",
      "Epoch 2901/20000 Training Loss: 0.07701128721237183\n",
      "Epoch 2902/20000 Training Loss: 0.10317505896091461\n",
      "Epoch 2903/20000 Training Loss: 0.06573616713285446\n",
      "Epoch 2904/20000 Training Loss: 0.059313684701919556\n",
      "Epoch 2905/20000 Training Loss: 0.04882499575614929\n",
      "Epoch 2906/20000 Training Loss: 0.0709758773446083\n",
      "Epoch 2907/20000 Training Loss: 0.0710417777299881\n",
      "Epoch 2908/20000 Training Loss: 0.0632397010922432\n",
      "Epoch 2909/20000 Training Loss: 0.07138877362012863\n",
      "Epoch 2910/20000 Training Loss: 0.07775911688804626\n",
      "Epoch 2911/20000 Training Loss: 0.05544380843639374\n",
      "Epoch 2912/20000 Training Loss: 0.07321329414844513\n",
      "Epoch 2913/20000 Training Loss: 0.08931711316108704\n",
      "Epoch 2914/20000 Training Loss: 0.07045698165893555\n",
      "Epoch 2915/20000 Training Loss: 0.06494122743606567\n",
      "Epoch 2916/20000 Training Loss: 0.06786297261714935\n",
      "Epoch 2917/20000 Training Loss: 0.09355222433805466\n",
      "Epoch 2918/20000 Training Loss: 0.072803795337677\n",
      "Epoch 2919/20000 Training Loss: 0.06009981781244278\n",
      "Epoch 2920/20000 Training Loss: 0.05961750075221062\n",
      "Epoch 2921/20000 Training Loss: 0.07130323350429535\n",
      "Epoch 2922/20000 Training Loss: 0.0707622542977333\n",
      "Epoch 2923/20000 Training Loss: 0.08885110914707184\n",
      "Epoch 2924/20000 Training Loss: 0.08222910761833191\n",
      "Epoch 2925/20000 Training Loss: 0.08612434566020966\n",
      "Epoch 2926/20000 Training Loss: 0.09074906259775162\n",
      "Epoch 2927/20000 Training Loss: 0.07226935774087906\n",
      "Epoch 2928/20000 Training Loss: 0.05889524519443512\n",
      "Epoch 2929/20000 Training Loss: 0.06130871921777725\n",
      "Epoch 2930/20000 Training Loss: 0.09373273700475693\n",
      "Epoch 2931/20000 Training Loss: 0.06453189253807068\n",
      "Epoch 2932/20000 Training Loss: 0.062042009085416794\n",
      "Epoch 2933/20000 Training Loss: 0.08674685657024384\n",
      "Epoch 2934/20000 Training Loss: 0.09223032742738724\n",
      "Epoch 2935/20000 Training Loss: 0.06887304782867432\n",
      "Epoch 2936/20000 Training Loss: 0.07215037941932678\n",
      "Epoch 2937/20000 Training Loss: 0.09008532762527466\n",
      "Epoch 2938/20000 Training Loss: 0.06341492384672165\n",
      "Epoch 2939/20000 Training Loss: 0.0860186517238617\n",
      "Epoch 2940/20000 Training Loss: 0.08315391838550568\n",
      "Epoch 2941/20000 Training Loss: 0.04871733859181404\n",
      "Epoch 2942/20000 Training Loss: 0.07357805967330933\n",
      "Epoch 2943/20000 Training Loss: 0.09351196885108948\n",
      "Epoch 2944/20000 Training Loss: 0.08167402446269989\n",
      "Epoch 2945/20000 Training Loss: 0.07975369691848755\n",
      "Epoch 2946/20000 Training Loss: 0.07891380786895752\n",
      "Epoch 2947/20000 Training Loss: 0.06641882658004761\n",
      "Epoch 2948/20000 Training Loss: 0.07731562852859497\n",
      "Epoch 2949/20000 Training Loss: 0.062448397278785706\n",
      "Epoch 2950/20000 Training Loss: 0.05948157608509064\n",
      "Epoch 2951/20000 Training Loss: 0.099154993891716\n",
      "Epoch 2952/20000 Training Loss: 0.07265251129865646\n",
      "Epoch 2953/20000 Training Loss: 0.06017628312110901\n",
      "Epoch 2954/20000 Training Loss: 0.07211937010288239\n",
      "Epoch 2955/20000 Training Loss: 0.05903827026486397\n",
      "Epoch 2956/20000 Training Loss: 0.09360554814338684\n",
      "Epoch 2957/20000 Training Loss: 0.06588719040155411\n",
      "Epoch 2958/20000 Training Loss: 0.0750942975282669\n",
      "Epoch 2959/20000 Training Loss: 0.06313972175121307\n",
      "Epoch 2960/20000 Training Loss: 0.07909435033798218\n",
      "Epoch 2961/20000 Training Loss: 0.07824143767356873\n",
      "Epoch 2962/20000 Training Loss: 0.05869722738862038\n",
      "Epoch 2963/20000 Training Loss: 0.0679159089922905\n",
      "Epoch 2964/20000 Training Loss: 0.07276682555675507\n",
      "Epoch 2965/20000 Training Loss: 0.07135601341724396\n",
      "Epoch 2966/20000 Training Loss: 0.08474558591842651\n",
      "Epoch 2967/20000 Training Loss: 0.07572992146015167\n",
      "Epoch 2968/20000 Training Loss: 0.07695534825325012\n",
      "Epoch 2969/20000 Training Loss: 0.08539678156375885\n",
      "Epoch 2970/20000 Training Loss: 0.06039977818727493\n",
      "Epoch 2971/20000 Training Loss: 0.04751397669315338\n",
      "Epoch 2972/20000 Training Loss: 0.09197387099266052\n",
      "Epoch 2973/20000 Training Loss: 0.07751508057117462\n",
      "Epoch 2974/20000 Training Loss: 0.0594368577003479\n",
      "Epoch 2975/20000 Training Loss: 0.08481130003929138\n",
      "Epoch 2976/20000 Training Loss: 0.07967761158943176\n",
      "Epoch 2977/20000 Training Loss: 0.07447583228349686\n",
      "Epoch 2978/20000 Training Loss: 0.07539401948451996\n",
      "Epoch 2979/20000 Training Loss: 0.08232022821903229\n",
      "Epoch 2980/20000 Training Loss: 0.05633023381233215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2981/20000 Training Loss: 0.06931187212467194\n",
      "Epoch 2982/20000 Training Loss: 0.06773526966571808\n",
      "Epoch 2983/20000 Training Loss: 0.07823944091796875\n",
      "Epoch 2984/20000 Training Loss: 0.06605172157287598\n",
      "Epoch 2985/20000 Training Loss: 0.07803372293710709\n",
      "Epoch 2986/20000 Training Loss: 0.09460270404815674\n",
      "Epoch 2987/20000 Training Loss: 0.06843529641628265\n",
      "Epoch 2988/20000 Training Loss: 0.06872037053108215\n",
      "Epoch 2989/20000 Training Loss: 0.0818481594324112\n",
      "Epoch 2990/20000 Training Loss: 0.07152757793664932\n",
      "Epoch 2991/20000 Training Loss: 0.08001013845205307\n",
      "Epoch 2992/20000 Training Loss: 0.07511716336011887\n",
      "Epoch 2993/20000 Training Loss: 0.06642137467861176\n",
      "Epoch 2994/20000 Training Loss: 0.07182809710502625\n",
      "Epoch 2995/20000 Training Loss: 0.058072321116924286\n",
      "Epoch 2996/20000 Training Loss: 0.07742352783679962\n",
      "Epoch 2997/20000 Training Loss: 0.07854194939136505\n",
      "Epoch 2998/20000 Training Loss: 0.05974597483873367\n",
      "Epoch 2999/20000 Training Loss: 0.08207251876592636\n",
      "Epoch 3000/20000 Training Loss: 0.06475065648555756\n",
      "Epoch 3000/20000 Validation Loss: 0.0837089940905571\n",
      "Epoch 3001/20000 Training Loss: 0.12583152949810028\n",
      "Epoch 3002/20000 Training Loss: 0.06732863187789917\n",
      "Epoch 3003/20000 Training Loss: 0.06750539690256119\n",
      "Epoch 3004/20000 Training Loss: 0.05297163873910904\n",
      "Epoch 3005/20000 Training Loss: 0.09826822578907013\n",
      "Epoch 3006/20000 Training Loss: 0.051665253937244415\n",
      "Epoch 3007/20000 Training Loss: 0.06948219984769821\n",
      "Epoch 3008/20000 Training Loss: 0.06799262017011642\n",
      "Epoch 3009/20000 Training Loss: 0.08464112877845764\n",
      "Epoch 3010/20000 Training Loss: 0.054072458297014236\n",
      "Epoch 3011/20000 Training Loss: 0.057761579751968384\n",
      "Epoch 3012/20000 Training Loss: 0.06739500910043716\n",
      "Epoch 3013/20000 Training Loss: 0.056005626916885376\n",
      "Epoch 3014/20000 Training Loss: 0.05288077890872955\n",
      "Epoch 3015/20000 Training Loss: 0.07371783256530762\n",
      "Epoch 3016/20000 Training Loss: 0.09431200474500656\n",
      "Epoch 3017/20000 Training Loss: 0.07076099514961243\n",
      "Epoch 3018/20000 Training Loss: 0.07049129903316498\n",
      "Epoch 3019/20000 Training Loss: 0.051040101796388626\n",
      "Epoch 3020/20000 Training Loss: 0.059659168124198914\n",
      "Epoch 3021/20000 Training Loss: 0.06645146012306213\n",
      "Epoch 3022/20000 Training Loss: 0.05154656991362572\n",
      "Epoch 3023/20000 Training Loss: 0.0717649832367897\n",
      "Epoch 3024/20000 Training Loss: 0.04971545934677124\n",
      "Epoch 3025/20000 Training Loss: 0.07725578546524048\n",
      "Epoch 3026/20000 Training Loss: 0.10089758038520813\n",
      "Epoch 3027/20000 Training Loss: 0.07880961149930954\n",
      "Epoch 3028/20000 Training Loss: 0.07252854108810425\n",
      "Epoch 3029/20000 Training Loss: 0.09466812014579773\n",
      "Epoch 3030/20000 Training Loss: 0.06493362784385681\n",
      "Epoch 3031/20000 Training Loss: 0.07509370148181915\n",
      "Epoch 3032/20000 Training Loss: 0.08490996062755585\n",
      "Epoch 3033/20000 Training Loss: 0.07201194763183594\n",
      "Epoch 3034/20000 Training Loss: 0.08364161103963852\n",
      "Epoch 3035/20000 Training Loss: 0.07728572189807892\n",
      "Epoch 3036/20000 Training Loss: 0.08850204944610596\n",
      "Epoch 3037/20000 Training Loss: 0.060535840690135956\n",
      "Epoch 3038/20000 Training Loss: 0.06771497428417206\n",
      "Epoch 3039/20000 Training Loss: 0.055263716727495193\n",
      "Epoch 3040/20000 Training Loss: 0.07934512197971344\n",
      "Epoch 3041/20000 Training Loss: 0.07882161438465118\n",
      "Epoch 3042/20000 Training Loss: 0.08108553290367126\n",
      "Epoch 3043/20000 Training Loss: 0.07619404792785645\n",
      "Epoch 3044/20000 Training Loss: 0.08494685590267181\n",
      "Epoch 3045/20000 Training Loss: 0.059035949409008026\n",
      "Epoch 3046/20000 Training Loss: 0.07660418748855591\n",
      "Epoch 3047/20000 Training Loss: 0.08333368599414825\n",
      "Epoch 3048/20000 Training Loss: 0.06411349773406982\n",
      "Epoch 3049/20000 Training Loss: 0.08968378603458405\n",
      "Epoch 3050/20000 Training Loss: 0.07446050643920898\n",
      "Epoch 3051/20000 Training Loss: 0.061680879443883896\n",
      "Epoch 3052/20000 Training Loss: 0.07992866635322571\n",
      "Epoch 3053/20000 Training Loss: 0.0994885265827179\n",
      "Epoch 3054/20000 Training Loss: 0.06367215514183044\n",
      "Epoch 3055/20000 Training Loss: 0.07602367550134659\n",
      "Epoch 3056/20000 Training Loss: 0.06247513368725777\n",
      "Epoch 3057/20000 Training Loss: 0.07804998755455017\n",
      "Epoch 3058/20000 Training Loss: 0.06491337716579437\n",
      "Epoch 3059/20000 Training Loss: 0.08726250380277634\n",
      "Epoch 3060/20000 Training Loss: 0.07891716063022614\n",
      "Epoch 3061/20000 Training Loss: 0.04525408148765564\n",
      "Epoch 3062/20000 Training Loss: 0.07660827785730362\n",
      "Epoch 3063/20000 Training Loss: 0.07479152828454971\n",
      "Epoch 3064/20000 Training Loss: 0.07266322523355484\n",
      "Epoch 3065/20000 Training Loss: 0.06269331276416779\n",
      "Epoch 3066/20000 Training Loss: 0.07939973473548889\n",
      "Epoch 3067/20000 Training Loss: 0.06744350492954254\n",
      "Epoch 3068/20000 Training Loss: 0.06697431951761246\n",
      "Epoch 3069/20000 Training Loss: 0.07088722288608551\n",
      "Epoch 3070/20000 Training Loss: 0.0715240091085434\n",
      "Epoch 3071/20000 Training Loss: 0.06664447486400604\n",
      "Epoch 3072/20000 Training Loss: 0.07352667301893234\n",
      "Epoch 3073/20000 Training Loss: 0.05492127686738968\n",
      "Epoch 3074/20000 Training Loss: 0.05452178046107292\n",
      "Epoch 3075/20000 Training Loss: 0.04671711102128029\n",
      "Epoch 3076/20000 Training Loss: 0.08720092475414276\n",
      "Epoch 3077/20000 Training Loss: 0.08195430040359497\n",
      "Epoch 3078/20000 Training Loss: 0.07620920240879059\n",
      "Epoch 3079/20000 Training Loss: 0.06724785268306732\n",
      "Epoch 3080/20000 Training Loss: 0.0728764683008194\n",
      "Epoch 3081/20000 Training Loss: 0.06818799674510956\n",
      "Epoch 3082/20000 Training Loss: 0.06281833350658417\n",
      "Epoch 3083/20000 Training Loss: 0.09014914929866791\n",
      "Epoch 3084/20000 Training Loss: 0.08578980714082718\n",
      "Epoch 3085/20000 Training Loss: 0.05771603435277939\n",
      "Epoch 3086/20000 Training Loss: 0.06578852236270905\n",
      "Epoch 3087/20000 Training Loss: 0.06638549268245697\n",
      "Epoch 3088/20000 Training Loss: 0.08568118512630463\n",
      "Epoch 3089/20000 Training Loss: 0.0684015080332756\n",
      "Epoch 3090/20000 Training Loss: 0.06305255740880966\n",
      "Epoch 3091/20000 Training Loss: 0.05508599430322647\n",
      "Epoch 3092/20000 Training Loss: 0.08660629391670227\n",
      "Epoch 3093/20000 Training Loss: 0.08387346565723419\n",
      "Epoch 3094/20000 Training Loss: 0.08283430337905884\n",
      "Epoch 3095/20000 Training Loss: 0.07048149406909943\n",
      "Epoch 3096/20000 Training Loss: 0.08115342259407043\n",
      "Epoch 3097/20000 Training Loss: 0.04899363964796066\n",
      "Epoch 3098/20000 Training Loss: 0.0629107728600502\n",
      "Epoch 3099/20000 Training Loss: 0.05771217867732048\n",
      "Epoch 3100/20000 Training Loss: 0.06238438934087753\n",
      "Epoch 3100/20000 Validation Loss: 0.06511306762695312\n",
      "Epoch 3101/20000 Training Loss: 0.06509627401828766\n",
      "Epoch 3102/20000 Training Loss: 0.06724517047405243\n",
      "Epoch 3103/20000 Training Loss: 0.07414133846759796\n",
      "Epoch 3104/20000 Training Loss: 0.07382689416408539\n",
      "Epoch 3105/20000 Training Loss: 0.07746998965740204\n",
      "Epoch 3106/20000 Training Loss: 0.0601729080080986\n",
      "Epoch 3107/20000 Training Loss: 0.04958643019199371\n",
      "Epoch 3108/20000 Training Loss: 0.09931039065122604\n",
      "Epoch 3109/20000 Training Loss: 0.07197137176990509\n",
      "Epoch 3110/20000 Training Loss: 0.054149165749549866\n",
      "Epoch 3111/20000 Training Loss: 0.053789086639881134\n",
      "Epoch 3112/20000 Training Loss: 0.06849449872970581\n",
      "Epoch 3113/20000 Training Loss: 0.1024937629699707\n",
      "Epoch 3114/20000 Training Loss: 0.06299694627523422\n",
      "Epoch 3115/20000 Training Loss: 0.04486466944217682\n",
      "Epoch 3116/20000 Training Loss: 0.08370187878608704\n",
      "Epoch 3117/20000 Training Loss: 0.06450776755809784\n",
      "Epoch 3118/20000 Training Loss: 0.07453621923923492\n",
      "Epoch 3119/20000 Training Loss: 0.07680987566709518\n",
      "Epoch 3120/20000 Training Loss: 0.049114104360342026\n",
      "Epoch 3121/20000 Training Loss: 0.04767308384180069\n",
      "Epoch 3122/20000 Training Loss: 0.05996762216091156\n",
      "Epoch 3123/20000 Training Loss: 0.06281151622533798\n",
      "Epoch 3124/20000 Training Loss: 0.07932790368795395\n",
      "Epoch 3125/20000 Training Loss: 0.05474639683961868\n",
      "Epoch 3126/20000 Training Loss: 0.08175118267536163\n",
      "Epoch 3127/20000 Training Loss: 0.06651726365089417\n",
      "Epoch 3128/20000 Training Loss: 0.08641092479228973\n",
      "Epoch 3129/20000 Training Loss: 0.07344276458024979\n",
      "Epoch 3130/20000 Training Loss: 0.05504798889160156\n",
      "Epoch 3131/20000 Training Loss: 0.0681619867682457\n",
      "Epoch 3132/20000 Training Loss: 0.07068385183811188\n",
      "Epoch 3133/20000 Training Loss: 0.07145814597606659\n",
      "Epoch 3134/20000 Training Loss: 0.07569008320569992\n",
      "Epoch 3135/20000 Training Loss: 0.06302311271429062\n",
      "Epoch 3136/20000 Training Loss: 0.06763768941164017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3137/20000 Training Loss: 0.05817835032939911\n",
      "Epoch 3138/20000 Training Loss: 0.09741011261940002\n",
      "Epoch 3139/20000 Training Loss: 0.07529369741678238\n",
      "Epoch 3140/20000 Training Loss: 0.055924512445926666\n",
      "Epoch 3141/20000 Training Loss: 0.06300758570432663\n",
      "Epoch 3142/20000 Training Loss: 0.08440344035625458\n",
      "Epoch 3143/20000 Training Loss: 0.07784033566713333\n",
      "Epoch 3144/20000 Training Loss: 0.05606796592473984\n",
      "Epoch 3145/20000 Training Loss: 0.06654930859804153\n",
      "Epoch 3146/20000 Training Loss: 0.0475783608853817\n",
      "Epoch 3147/20000 Training Loss: 0.09133085608482361\n",
      "Epoch 3148/20000 Training Loss: 0.09298364818096161\n",
      "Epoch 3149/20000 Training Loss: 0.06632695347070694\n",
      "Epoch 3150/20000 Training Loss: 0.05978942662477493\n",
      "Epoch 3151/20000 Training Loss: 0.07736018300056458\n",
      "Epoch 3152/20000 Training Loss: 0.05693785101175308\n",
      "Epoch 3153/20000 Training Loss: 0.07197047770023346\n",
      "Epoch 3154/20000 Training Loss: 0.058994218707084656\n",
      "Epoch 3155/20000 Training Loss: 0.0798799991607666\n",
      "Epoch 3156/20000 Training Loss: 0.07594740390777588\n",
      "Epoch 3157/20000 Training Loss: 0.07048565149307251\n",
      "Epoch 3158/20000 Training Loss: 0.049587659537792206\n",
      "Epoch 3159/20000 Training Loss: 0.06605842709541321\n",
      "Epoch 3160/20000 Training Loss: 0.06400670856237411\n",
      "Epoch 3161/20000 Training Loss: 0.06144321709871292\n",
      "Epoch 3162/20000 Training Loss: 0.04808242246508598\n",
      "Epoch 3163/20000 Training Loss: 0.060563746839761734\n",
      "Epoch 3164/20000 Training Loss: 0.06599314510822296\n",
      "Epoch 3165/20000 Training Loss: 0.07001374661922455\n",
      "Epoch 3166/20000 Training Loss: 0.08727149665355682\n",
      "Epoch 3167/20000 Training Loss: 0.07456494867801666\n",
      "Epoch 3168/20000 Training Loss: 0.07462736964225769\n",
      "Epoch 3169/20000 Training Loss: 0.06992536783218384\n",
      "Epoch 3170/20000 Training Loss: 0.06489384174346924\n",
      "Epoch 3171/20000 Training Loss: 0.08311808109283447\n",
      "Epoch 3172/20000 Training Loss: 0.06237222999334335\n",
      "Epoch 3173/20000 Training Loss: 0.05728696286678314\n",
      "Epoch 3174/20000 Training Loss: 0.05180366337299347\n",
      "Epoch 3175/20000 Training Loss: 0.08351371437311172\n",
      "Epoch 3176/20000 Training Loss: 0.07471460849046707\n",
      "Epoch 3177/20000 Training Loss: 0.0719882994890213\n",
      "Epoch 3178/20000 Training Loss: 0.08184439688920975\n",
      "Epoch 3179/20000 Training Loss: 0.05994870513677597\n",
      "Epoch 3180/20000 Training Loss: 0.08041965216398239\n",
      "Epoch 3181/20000 Training Loss: 0.0570979006588459\n",
      "Epoch 3182/20000 Training Loss: 0.06206208094954491\n",
      "Epoch 3183/20000 Training Loss: 0.07433930039405823\n",
      "Epoch 3184/20000 Training Loss: 0.08142393827438354\n",
      "Epoch 3185/20000 Training Loss: 0.06531873345375061\n",
      "Epoch 3186/20000 Training Loss: 0.06423290818929672\n",
      "Epoch 3187/20000 Training Loss: 0.08308107405900955\n",
      "Epoch 3188/20000 Training Loss: 0.07439017295837402\n",
      "Epoch 3189/20000 Training Loss: 0.08045589923858643\n",
      "Epoch 3190/20000 Training Loss: 0.06493628770112991\n",
      "Epoch 3191/20000 Training Loss: 0.05978839471936226\n",
      "Epoch 3192/20000 Training Loss: 0.0710098147392273\n",
      "Epoch 3193/20000 Training Loss: 0.06411875784397125\n",
      "Epoch 3194/20000 Training Loss: 0.061640553176403046\n",
      "Epoch 3195/20000 Training Loss: 0.06990429759025574\n",
      "Epoch 3196/20000 Training Loss: 0.0717354416847229\n",
      "Epoch 3197/20000 Training Loss: 0.07898957282304764\n",
      "Epoch 3198/20000 Training Loss: 0.0598803386092186\n",
      "Epoch 3199/20000 Training Loss: 0.06854870170354843\n",
      "Epoch 3200/20000 Training Loss: 0.06406798958778381\n",
      "Epoch 3200/20000 Validation Loss: 0.0594654344022274\n",
      "Epoch 3201/20000 Training Loss: 0.09973989427089691\n",
      "Epoch 3202/20000 Training Loss: 0.06460642069578171\n",
      "Epoch 3203/20000 Training Loss: 0.06454852223396301\n",
      "Epoch 3204/20000 Training Loss: 0.06381861865520477\n",
      "Epoch 3205/20000 Training Loss: 0.10625280439853668\n",
      "Epoch 3206/20000 Training Loss: 0.07242293655872345\n",
      "Epoch 3207/20000 Training Loss: 0.0656721293926239\n",
      "Epoch 3208/20000 Training Loss: 0.064066581428051\n",
      "Epoch 3209/20000 Training Loss: 0.08835164457559586\n",
      "Epoch 3210/20000 Training Loss: 0.05683005601167679\n",
      "Epoch 3211/20000 Training Loss: 0.07201889902353287\n",
      "Epoch 3212/20000 Training Loss: 0.07155877351760864\n",
      "Epoch 3213/20000 Training Loss: 0.09075769037008286\n",
      "Epoch 3214/20000 Training Loss: 0.06850621104240417\n",
      "Epoch 3215/20000 Training Loss: 0.072117879986763\n",
      "Epoch 3216/20000 Training Loss: 0.06184937804937363\n",
      "Epoch 3217/20000 Training Loss: 0.0531754270195961\n",
      "Epoch 3218/20000 Training Loss: 0.07649163901805878\n",
      "Epoch 3219/20000 Training Loss: 0.052701208740472794\n",
      "Epoch 3220/20000 Training Loss: 0.08220605552196503\n",
      "Epoch 3221/20000 Training Loss: 0.08784938603639603\n",
      "Epoch 3222/20000 Training Loss: 0.061500146985054016\n",
      "Epoch 3223/20000 Training Loss: 0.05842343717813492\n",
      "Epoch 3224/20000 Training Loss: 0.05026927590370178\n",
      "Epoch 3225/20000 Training Loss: 0.05291690677404404\n",
      "Epoch 3226/20000 Training Loss: 0.06369717419147491\n",
      "Epoch 3227/20000 Training Loss: 0.0792321264743805\n",
      "Epoch 3228/20000 Training Loss: 0.07527771592140198\n",
      "Epoch 3229/20000 Training Loss: 0.06243715435266495\n",
      "Epoch 3230/20000 Training Loss: 0.07233114540576935\n",
      "Epoch 3231/20000 Training Loss: 0.08266782760620117\n",
      "Epoch 3232/20000 Training Loss: 0.07078021764755249\n",
      "Epoch 3233/20000 Training Loss: 0.0710468515753746\n",
      "Epoch 3234/20000 Training Loss: 0.07233218848705292\n",
      "Epoch 3235/20000 Training Loss: 0.06146571785211563\n",
      "Epoch 3236/20000 Training Loss: 0.0650215595960617\n",
      "Epoch 3237/20000 Training Loss: 0.08249831944704056\n",
      "Epoch 3238/20000 Training Loss: 0.07028800249099731\n",
      "Epoch 3239/20000 Training Loss: 0.06498698890209198\n",
      "Epoch 3240/20000 Training Loss: 0.05572311952710152\n",
      "Epoch 3241/20000 Training Loss: 0.07711078971624374\n",
      "Epoch 3242/20000 Training Loss: 0.05926936864852905\n",
      "Epoch 3243/20000 Training Loss: 0.10300996899604797\n",
      "Epoch 3244/20000 Training Loss: 0.07099930942058563\n",
      "Epoch 3245/20000 Training Loss: 0.08526615798473358\n",
      "Epoch 3246/20000 Training Loss: 0.07276856899261475\n",
      "Epoch 3247/20000 Training Loss: 0.07017623633146286\n",
      "Epoch 3248/20000 Training Loss: 0.09276407957077026\n",
      "Epoch 3249/20000 Training Loss: 0.07208769768476486\n",
      "Epoch 3250/20000 Training Loss: 0.07326015830039978\n",
      "Epoch 3251/20000 Training Loss: 0.06745381653308868\n",
      "Epoch 3252/20000 Training Loss: 0.05850451439619064\n",
      "Epoch 3253/20000 Training Loss: 0.06952854245901108\n",
      "Epoch 3254/20000 Training Loss: 0.07722901552915573\n",
      "Epoch 3255/20000 Training Loss: 0.06967891752719879\n",
      "Epoch 3256/20000 Training Loss: 0.08588557690382004\n",
      "Epoch 3257/20000 Training Loss: 0.06856225430965424\n",
      "Epoch 3258/20000 Training Loss: 0.05696028843522072\n",
      "Epoch 3259/20000 Training Loss: 0.058920614421367645\n",
      "Epoch 3260/20000 Training Loss: 0.09225502610206604\n",
      "Epoch 3261/20000 Training Loss: 0.09001411497592926\n",
      "Epoch 3262/20000 Training Loss: 0.06844469159841537\n",
      "Epoch 3263/20000 Training Loss: 0.0534294918179512\n",
      "Epoch 3264/20000 Training Loss: 0.09606635570526123\n",
      "Epoch 3265/20000 Training Loss: 0.06742997467517853\n",
      "Epoch 3266/20000 Training Loss: 0.06867305934429169\n",
      "Epoch 3267/20000 Training Loss: 0.04799448698759079\n",
      "Epoch 3268/20000 Training Loss: 0.06805665791034698\n",
      "Epoch 3269/20000 Training Loss: 0.07734477519989014\n",
      "Epoch 3270/20000 Training Loss: 0.06745166331529617\n",
      "Epoch 3271/20000 Training Loss: 0.062115803360939026\n",
      "Epoch 3272/20000 Training Loss: 0.05429757758975029\n",
      "Epoch 3273/20000 Training Loss: 0.05938524752855301\n",
      "Epoch 3274/20000 Training Loss: 0.06652043014764786\n",
      "Epoch 3275/20000 Training Loss: 0.05507897585630417\n",
      "Epoch 3276/20000 Training Loss: 0.09349758923053741\n",
      "Epoch 3277/20000 Training Loss: 0.07346808910369873\n",
      "Epoch 3278/20000 Training Loss: 0.09057323634624481\n",
      "Epoch 3279/20000 Training Loss: 0.07252774387598038\n",
      "Epoch 3280/20000 Training Loss: 0.08889010548591614\n",
      "Epoch 3281/20000 Training Loss: 0.08318787068128586\n",
      "Epoch 3282/20000 Training Loss: 0.05989721417427063\n",
      "Epoch 3283/20000 Training Loss: 0.08803188055753708\n",
      "Epoch 3284/20000 Training Loss: 0.07873228192329407\n",
      "Epoch 3285/20000 Training Loss: 0.07526202499866486\n",
      "Epoch 3286/20000 Training Loss: 0.061626970767974854\n",
      "Epoch 3287/20000 Training Loss: 0.061648085713386536\n",
      "Epoch 3288/20000 Training Loss: 0.06675201654434204\n",
      "Epoch 3289/20000 Training Loss: 0.0623750165104866\n",
      "Epoch 3290/20000 Training Loss: 0.0836515799164772\n",
      "Epoch 3291/20000 Training Loss: 0.06632774323225021\n",
      "Epoch 3292/20000 Training Loss: 0.0617203414440155\n",
      "Epoch 3293/20000 Training Loss: 0.07256687432527542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3294/20000 Training Loss: 0.07834360003471375\n",
      "Epoch 3295/20000 Training Loss: 0.060788653790950775\n",
      "Epoch 3296/20000 Training Loss: 0.0884154736995697\n",
      "Epoch 3297/20000 Training Loss: 0.0839645266532898\n",
      "Epoch 3298/20000 Training Loss: 0.06538420915603638\n",
      "Epoch 3299/20000 Training Loss: 0.05391278117895126\n",
      "Epoch 3300/20000 Training Loss: 0.07726109027862549\n",
      "Epoch 3300/20000 Validation Loss: 0.04974125325679779\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04974125325679779<=============\n",
      "Epoch 3301/20000 Training Loss: 0.0648718997836113\n",
      "Epoch 3302/20000 Training Loss: 0.07167263329029083\n",
      "Epoch 3303/20000 Training Loss: 0.09131444990634918\n",
      "Epoch 3304/20000 Training Loss: 0.06988278031349182\n",
      "Epoch 3305/20000 Training Loss: 0.07338927686214447\n",
      "Epoch 3306/20000 Training Loss: 0.05490923672914505\n",
      "Epoch 3307/20000 Training Loss: 0.08390794694423676\n",
      "Epoch 3308/20000 Training Loss: 0.06469352543354034\n",
      "Epoch 3309/20000 Training Loss: 0.0751429870724678\n",
      "Epoch 3310/20000 Training Loss: 0.07375295460224152\n",
      "Epoch 3311/20000 Training Loss: 0.08548662066459656\n",
      "Epoch 3312/20000 Training Loss: 0.07305571436882019\n",
      "Epoch 3313/20000 Training Loss: 0.05144447088241577\n",
      "Epoch 3314/20000 Training Loss: 0.0617370530962944\n",
      "Epoch 3315/20000 Training Loss: 0.09076280891895294\n",
      "Epoch 3316/20000 Training Loss: 0.049098800867795944\n",
      "Epoch 3317/20000 Training Loss: 0.07062043249607086\n",
      "Epoch 3318/20000 Training Loss: 0.06721214950084686\n",
      "Epoch 3319/20000 Training Loss: 0.07143539190292358\n",
      "Epoch 3320/20000 Training Loss: 0.058383308351039886\n",
      "Epoch 3321/20000 Training Loss: 0.060491859912872314\n",
      "Epoch 3322/20000 Training Loss: 0.0802684873342514\n",
      "Epoch 3323/20000 Training Loss: 0.07942411303520203\n",
      "Epoch 3324/20000 Training Loss: 0.06523061543703079\n",
      "Epoch 3325/20000 Training Loss: 0.08382591605186462\n",
      "Epoch 3326/20000 Training Loss: 0.07928924262523651\n",
      "Epoch 3327/20000 Training Loss: 0.07437068223953247\n",
      "Epoch 3328/20000 Training Loss: 0.06108744442462921\n",
      "Epoch 3329/20000 Training Loss: 0.07626940310001373\n",
      "Epoch 3330/20000 Training Loss: 0.10226787626743317\n",
      "Epoch 3331/20000 Training Loss: 0.04742220789194107\n",
      "Epoch 3332/20000 Training Loss: 0.059594184160232544\n",
      "Epoch 3333/20000 Training Loss: 0.06687259674072266\n",
      "Epoch 3334/20000 Training Loss: 0.0928332656621933\n",
      "Epoch 3335/20000 Training Loss: 0.04920753836631775\n",
      "Epoch 3336/20000 Training Loss: 0.05737338960170746\n",
      "Epoch 3337/20000 Training Loss: 0.07768642157316208\n",
      "Epoch 3338/20000 Training Loss: 0.07859668880701065\n",
      "Epoch 3339/20000 Training Loss: 0.07877964526414871\n",
      "Epoch 3340/20000 Training Loss: 0.05180954933166504\n",
      "Epoch 3341/20000 Training Loss: 0.05866645276546478\n",
      "Epoch 3342/20000 Training Loss: 0.05117633193731308\n",
      "Epoch 3343/20000 Training Loss: 0.08782821148633957\n",
      "Epoch 3344/20000 Training Loss: 0.0728149339556694\n",
      "Epoch 3345/20000 Training Loss: 0.05120791867375374\n",
      "Epoch 3346/20000 Training Loss: 0.06979033350944519\n",
      "Epoch 3347/20000 Training Loss: 0.07556767016649246\n",
      "Epoch 3348/20000 Training Loss: 0.0783877894282341\n",
      "Epoch 3349/20000 Training Loss: 0.09308800101280212\n",
      "Epoch 3350/20000 Training Loss: 0.05659008026123047\n",
      "Epoch 3351/20000 Training Loss: 0.0674421489238739\n",
      "Epoch 3352/20000 Training Loss: 0.07711899280548096\n",
      "Epoch 3353/20000 Training Loss: 0.0639764666557312\n",
      "Epoch 3354/20000 Training Loss: 0.06464021652936935\n",
      "Epoch 3355/20000 Training Loss: 0.05201299488544464\n",
      "Epoch 3356/20000 Training Loss: 0.061417706310749054\n",
      "Epoch 3357/20000 Training Loss: 0.07124676555395126\n",
      "Epoch 3358/20000 Training Loss: 0.06606502830982208\n",
      "Epoch 3359/20000 Training Loss: 0.06047699600458145\n",
      "Epoch 3360/20000 Training Loss: 0.05416760966181755\n",
      "Epoch 3361/20000 Training Loss: 0.08160530030727386\n",
      "Epoch 3362/20000 Training Loss: 0.06506119668483734\n",
      "Epoch 3363/20000 Training Loss: 0.0527738593518734\n",
      "Epoch 3364/20000 Training Loss: 0.041340701282024384\n",
      "Epoch 3365/20000 Training Loss: 0.06926308572292328\n",
      "Epoch 3366/20000 Training Loss: 0.10289189964532852\n",
      "Epoch 3367/20000 Training Loss: 0.05941063165664673\n",
      "Epoch 3368/20000 Training Loss: 0.07639354467391968\n",
      "Epoch 3369/20000 Training Loss: 0.07273997366428375\n",
      "Epoch 3370/20000 Training Loss: 0.08173803985118866\n",
      "Epoch 3371/20000 Training Loss: 0.07087936252355576\n",
      "Epoch 3372/20000 Training Loss: 0.07623369246721268\n",
      "Epoch 3373/20000 Training Loss: 0.06065712496638298\n",
      "Epoch 3374/20000 Training Loss: 0.05863053351640701\n",
      "Epoch 3375/20000 Training Loss: 0.08125529438257217\n",
      "Epoch 3376/20000 Training Loss: 0.06334508955478668\n",
      "Epoch 3377/20000 Training Loss: 0.06343997269868851\n",
      "Epoch 3378/20000 Training Loss: 0.07088874280452728\n",
      "Epoch 3379/20000 Training Loss: 0.08217628300189972\n",
      "Epoch 3380/20000 Training Loss: 0.07305265963077545\n",
      "Epoch 3381/20000 Training Loss: 0.06615820527076721\n",
      "Epoch 3382/20000 Training Loss: 0.05963917076587677\n",
      "Epoch 3383/20000 Training Loss: 0.062315113842487335\n",
      "Epoch 3384/20000 Training Loss: 0.06756554543972015\n",
      "Epoch 3385/20000 Training Loss: 0.0788690447807312\n",
      "Epoch 3386/20000 Training Loss: 0.10531619191169739\n",
      "Epoch 3387/20000 Training Loss: 0.06845881044864655\n",
      "Epoch 3388/20000 Training Loss: 0.06681390106678009\n",
      "Epoch 3389/20000 Training Loss: 0.09265750646591187\n",
      "Epoch 3390/20000 Training Loss: 0.055895645171403885\n",
      "Epoch 3391/20000 Training Loss: 0.0671243816614151\n",
      "Epoch 3392/20000 Training Loss: 0.06768900156021118\n",
      "Epoch 3393/20000 Training Loss: 0.07632429152727127\n",
      "Epoch 3394/20000 Training Loss: 0.06371213495731354\n",
      "Epoch 3395/20000 Training Loss: 0.06106943637132645\n",
      "Epoch 3396/20000 Training Loss: 0.05005260929465294\n",
      "Epoch 3397/20000 Training Loss: 0.06853121519088745\n",
      "Epoch 3398/20000 Training Loss: 0.07096794247627258\n",
      "Epoch 3399/20000 Training Loss: 0.06685169786214828\n",
      "Epoch 3400/20000 Training Loss: 0.06758618354797363\n",
      "Epoch 3400/20000 Validation Loss: 0.07763519883155823\n",
      "Epoch 3401/20000 Training Loss: 0.059678345918655396\n",
      "Epoch 3402/20000 Training Loss: 0.0696011334657669\n",
      "Epoch 3403/20000 Training Loss: 0.06699058413505554\n",
      "Epoch 3404/20000 Training Loss: 0.0710713192820549\n",
      "Epoch 3405/20000 Training Loss: 0.06349083036184311\n",
      "Epoch 3406/20000 Training Loss: 0.0894472599029541\n",
      "Epoch 3407/20000 Training Loss: 0.05385783687233925\n",
      "Epoch 3408/20000 Training Loss: 0.0731818750500679\n",
      "Epoch 3409/20000 Training Loss: 0.07443860173225403\n",
      "Epoch 3410/20000 Training Loss: 0.08626049757003784\n",
      "Epoch 3411/20000 Training Loss: 0.10056723654270172\n",
      "Epoch 3412/20000 Training Loss: 0.05020226910710335\n",
      "Epoch 3413/20000 Training Loss: 0.057231344282627106\n",
      "Epoch 3414/20000 Training Loss: 0.06322616338729858\n",
      "Epoch 3415/20000 Training Loss: 0.09246256202459335\n",
      "Epoch 3416/20000 Training Loss: 0.06370550394058228\n",
      "Epoch 3417/20000 Training Loss: 0.08638179302215576\n",
      "Epoch 3418/20000 Training Loss: 0.06896090507507324\n",
      "Epoch 3419/20000 Training Loss: 0.06024974584579468\n",
      "Epoch 3420/20000 Training Loss: 0.04195054620504379\n",
      "Epoch 3421/20000 Training Loss: 0.08787760138511658\n",
      "Epoch 3422/20000 Training Loss: 0.06386762857437134\n",
      "Epoch 3423/20000 Training Loss: 0.06439407169818878\n",
      "Epoch 3424/20000 Training Loss: 0.0781625509262085\n",
      "Epoch 3425/20000 Training Loss: 0.08171683549880981\n",
      "Epoch 3426/20000 Training Loss: 0.08623643964529037\n",
      "Epoch 3427/20000 Training Loss: 0.05749642476439476\n",
      "Epoch 3428/20000 Training Loss: 0.07483643293380737\n",
      "Epoch 3429/20000 Training Loss: 0.059787094593048096\n",
      "Epoch 3430/20000 Training Loss: 0.05746185779571533\n",
      "Epoch 3431/20000 Training Loss: 0.07421518862247467\n",
      "Epoch 3432/20000 Training Loss: 0.07146647572517395\n",
      "Epoch 3433/20000 Training Loss: 0.056567393243312836\n",
      "Epoch 3434/20000 Training Loss: 0.06610075384378433\n",
      "Epoch 3435/20000 Training Loss: 0.05192048102617264\n",
      "Epoch 3436/20000 Training Loss: 0.07495693862438202\n",
      "Epoch 3437/20000 Training Loss: 0.07190175354480743\n",
      "Epoch 3438/20000 Training Loss: 0.08435933291912079\n",
      "Epoch 3439/20000 Training Loss: 0.04692874848842621\n",
      "Epoch 3440/20000 Training Loss: 0.07546175271272659\n",
      "Epoch 3441/20000 Training Loss: 0.045376941561698914\n",
      "Epoch 3442/20000 Training Loss: 0.07602479308843613\n",
      "Epoch 3443/20000 Training Loss: 0.07165912538766861\n",
      "Epoch 3444/20000 Training Loss: 0.06291397660970688\n",
      "Epoch 3445/20000 Training Loss: 0.06334713101387024\n",
      "Epoch 3446/20000 Training Loss: 0.05410660058259964\n",
      "Epoch 3447/20000 Training Loss: 0.05917496979236603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3448/20000 Training Loss: 0.059797272086143494\n",
      "Epoch 3449/20000 Training Loss: 0.07354944199323654\n",
      "Epoch 3450/20000 Training Loss: 0.06561142951250076\n",
      "Epoch 3451/20000 Training Loss: 0.06804556399583817\n",
      "Epoch 3452/20000 Training Loss: 0.09614618122577667\n",
      "Epoch 3453/20000 Training Loss: 0.0660032331943512\n",
      "Epoch 3454/20000 Training Loss: 0.0928645208477974\n",
      "Epoch 3455/20000 Training Loss: 0.08071582019329071\n",
      "Epoch 3456/20000 Training Loss: 0.08489927649497986\n",
      "Epoch 3457/20000 Training Loss: 0.047013044357299805\n",
      "Epoch 3458/20000 Training Loss: 0.07019171863794327\n",
      "Epoch 3459/20000 Training Loss: 0.05949132889509201\n",
      "Epoch 3460/20000 Training Loss: 0.06015308201313019\n",
      "Epoch 3461/20000 Training Loss: 0.06496188789606094\n",
      "Epoch 3462/20000 Training Loss: 0.06521962583065033\n",
      "Epoch 3463/20000 Training Loss: 0.10224416851997375\n",
      "Epoch 3464/20000 Training Loss: 0.07076326757669449\n",
      "Epoch 3465/20000 Training Loss: 0.06341177225112915\n",
      "Epoch 3466/20000 Training Loss: 0.07177804410457611\n",
      "Epoch 3467/20000 Training Loss: 0.07981507480144501\n",
      "Epoch 3468/20000 Training Loss: 0.04194016009569168\n",
      "Epoch 3469/20000 Training Loss: 0.05512849614024162\n",
      "Epoch 3470/20000 Training Loss: 0.08429588377475739\n",
      "Epoch 3471/20000 Training Loss: 0.05758734419941902\n",
      "Epoch 3472/20000 Training Loss: 0.07153502106666565\n",
      "Epoch 3473/20000 Training Loss: 0.07852163910865784\n",
      "Epoch 3474/20000 Training Loss: 0.0650135800242424\n",
      "Epoch 3475/20000 Training Loss: 0.07298870384693146\n",
      "Epoch 3476/20000 Training Loss: 0.0864415243268013\n",
      "Epoch 3477/20000 Training Loss: 0.051083825528621674\n",
      "Epoch 3478/20000 Training Loss: 0.08070617914199829\n",
      "Epoch 3479/20000 Training Loss: 0.07559162378311157\n",
      "Epoch 3480/20000 Training Loss: 0.061947502195835114\n",
      "Epoch 3481/20000 Training Loss: 0.10800737142562866\n",
      "Epoch 3482/20000 Training Loss: 0.06977222114801407\n",
      "Epoch 3483/20000 Training Loss: 0.0921429842710495\n",
      "Epoch 3484/20000 Training Loss: 0.05263020098209381\n",
      "Epoch 3485/20000 Training Loss: 0.0796785056591034\n",
      "Epoch 3486/20000 Training Loss: 0.08294651657342911\n",
      "Epoch 3487/20000 Training Loss: 0.06073968857526779\n",
      "Epoch 3488/20000 Training Loss: 0.06689233332872391\n",
      "Epoch 3489/20000 Training Loss: 0.05431290715932846\n",
      "Epoch 3490/20000 Training Loss: 0.08287590742111206\n",
      "Epoch 3491/20000 Training Loss: 0.0592522993683815\n",
      "Epoch 3492/20000 Training Loss: 0.06091172993183136\n",
      "Epoch 3493/20000 Training Loss: 0.07233801484107971\n",
      "Epoch 3494/20000 Training Loss: 0.06259271502494812\n",
      "Epoch 3495/20000 Training Loss: 0.08360506594181061\n",
      "Epoch 3496/20000 Training Loss: 0.076748326420784\n",
      "Epoch 3497/20000 Training Loss: 0.07841254770755768\n",
      "Epoch 3498/20000 Training Loss: 0.06349065154790878\n",
      "Epoch 3499/20000 Training Loss: 0.06375014781951904\n",
      "Epoch 3500/20000 Training Loss: 0.09421323984861374\n",
      "Epoch 3500/20000 Validation Loss: 0.0877496749162674\n",
      "Epoch 3501/20000 Training Loss: 0.08894043415784836\n",
      "Epoch 3502/20000 Training Loss: 0.05678311735391617\n",
      "Epoch 3503/20000 Training Loss: 0.07811981439590454\n",
      "Epoch 3504/20000 Training Loss: 0.04690184444189072\n",
      "Epoch 3505/20000 Training Loss: 0.08437882363796234\n",
      "Epoch 3506/20000 Training Loss: 0.06612229347229004\n",
      "Epoch 3507/20000 Training Loss: 0.050930798053741455\n",
      "Epoch 3508/20000 Training Loss: 0.06000160053372383\n",
      "Epoch 3509/20000 Training Loss: 0.07057861983776093\n",
      "Epoch 3510/20000 Training Loss: 0.07069984078407288\n",
      "Epoch 3511/20000 Training Loss: 0.07098062336444855\n",
      "Epoch 3512/20000 Training Loss: 0.0686972439289093\n",
      "Epoch 3513/20000 Training Loss: 0.0689864307641983\n",
      "Epoch 3514/20000 Training Loss: 0.06523744761943817\n",
      "Epoch 3515/20000 Training Loss: 0.07311347126960754\n",
      "Epoch 3516/20000 Training Loss: 0.06066727638244629\n",
      "Epoch 3517/20000 Training Loss: 0.051164187490940094\n",
      "Epoch 3518/20000 Training Loss: 0.06828489154577255\n",
      "Epoch 3519/20000 Training Loss: 0.07612539082765579\n",
      "Epoch 3520/20000 Training Loss: 0.07391101866960526\n",
      "Epoch 3521/20000 Training Loss: 0.05216316878795624\n",
      "Epoch 3522/20000 Training Loss: 0.09546627104282379\n",
      "Epoch 3523/20000 Training Loss: 0.048514436930418015\n",
      "Epoch 3524/20000 Training Loss: 0.07072032988071442\n",
      "Epoch 3525/20000 Training Loss: 0.05398684740066528\n",
      "Epoch 3526/20000 Training Loss: 0.06012392044067383\n",
      "Epoch 3527/20000 Training Loss: 0.06256283819675446\n",
      "Epoch 3528/20000 Training Loss: 0.08760534226894379\n",
      "Epoch 3529/20000 Training Loss: 0.07243728637695312\n",
      "Epoch 3530/20000 Training Loss: 0.10118594765663147\n",
      "Epoch 3531/20000 Training Loss: 0.0671500563621521\n",
      "Epoch 3532/20000 Training Loss: 0.0709514319896698\n",
      "Epoch 3533/20000 Training Loss: 0.07873554527759552\n",
      "Epoch 3534/20000 Training Loss: 0.05806643143296242\n",
      "Epoch 3535/20000 Training Loss: 0.05292204022407532\n",
      "Epoch 3536/20000 Training Loss: 0.059152744710445404\n",
      "Epoch 3537/20000 Training Loss: 0.05893772840499878\n",
      "Epoch 3538/20000 Training Loss: 0.05792143940925598\n",
      "Epoch 3539/20000 Training Loss: 0.06306441128253937\n",
      "Epoch 3540/20000 Training Loss: 0.061455436050891876\n",
      "Epoch 3541/20000 Training Loss: 0.05538090318441391\n",
      "Epoch 3542/20000 Training Loss: 0.05826850235462189\n",
      "Epoch 3543/20000 Training Loss: 0.062087997794151306\n",
      "Epoch 3544/20000 Training Loss: 0.059524573385715485\n",
      "Epoch 3545/20000 Training Loss: 0.05627024918794632\n",
      "Epoch 3546/20000 Training Loss: 0.06276126205921173\n",
      "Epoch 3547/20000 Training Loss: 0.06579574942588806\n",
      "Epoch 3548/20000 Training Loss: 0.06613518297672272\n",
      "Epoch 3549/20000 Training Loss: 0.06971035897731781\n",
      "Epoch 3550/20000 Training Loss: 0.07764525711536407\n",
      "Epoch 3551/20000 Training Loss: 0.07953410595655441\n",
      "Epoch 3552/20000 Training Loss: 0.07061570137739182\n",
      "Epoch 3553/20000 Training Loss: 0.07225415855646133\n",
      "Epoch 3554/20000 Training Loss: 0.06411014497280121\n",
      "Epoch 3555/20000 Training Loss: 0.07606607675552368\n",
      "Epoch 3556/20000 Training Loss: 0.06435265392065048\n",
      "Epoch 3557/20000 Training Loss: 0.08280482888221741\n",
      "Epoch 3558/20000 Training Loss: 0.06237589940428734\n",
      "Epoch 3559/20000 Training Loss: 0.06567990779876709\n",
      "Epoch 3560/20000 Training Loss: 0.0641975998878479\n",
      "Epoch 3561/20000 Training Loss: 0.05879353731870651\n",
      "Epoch 3562/20000 Training Loss: 0.09343010187149048\n",
      "Epoch 3563/20000 Training Loss: 0.048719968646764755\n",
      "Epoch 3564/20000 Training Loss: 0.06443842500448227\n",
      "Epoch 3565/20000 Training Loss: 0.07348040491342545\n",
      "Epoch 3566/20000 Training Loss: 0.058508194983005524\n",
      "Epoch 3567/20000 Training Loss: 0.06815950572490692\n",
      "Epoch 3568/20000 Training Loss: 0.05971365422010422\n",
      "Epoch 3569/20000 Training Loss: 0.05815935134887695\n",
      "Epoch 3570/20000 Training Loss: 0.07278813421726227\n",
      "Epoch 3571/20000 Training Loss: 0.061205655336380005\n",
      "Epoch 3572/20000 Training Loss: 0.06324316561222076\n",
      "Epoch 3573/20000 Training Loss: 0.054066844284534454\n",
      "Epoch 3574/20000 Training Loss: 0.07937850058078766\n",
      "Epoch 3575/20000 Training Loss: 0.097532257437706\n",
      "Epoch 3576/20000 Training Loss: 0.062407683581113815\n",
      "Epoch 3577/20000 Training Loss: 0.06222334876656532\n",
      "Epoch 3578/20000 Training Loss: 0.06885772943496704\n",
      "Epoch 3579/20000 Training Loss: 0.09586024284362793\n",
      "Epoch 3580/20000 Training Loss: 0.0623597726225853\n",
      "Epoch 3581/20000 Training Loss: 0.06876780837774277\n",
      "Epoch 3582/20000 Training Loss: 0.08457255363464355\n",
      "Epoch 3583/20000 Training Loss: 0.0623452365398407\n",
      "Epoch 3584/20000 Training Loss: 0.0829293429851532\n",
      "Epoch 3585/20000 Training Loss: 0.08870796859264374\n",
      "Epoch 3586/20000 Training Loss: 0.05156013369560242\n",
      "Epoch 3587/20000 Training Loss: 0.04717520624399185\n",
      "Epoch 3588/20000 Training Loss: 0.09014251828193665\n",
      "Epoch 3589/20000 Training Loss: 0.07409858703613281\n",
      "Epoch 3590/20000 Training Loss: 0.07248993217945099\n",
      "Epoch 3591/20000 Training Loss: 0.07639779895544052\n",
      "Epoch 3592/20000 Training Loss: 0.05786173790693283\n",
      "Epoch 3593/20000 Training Loss: 0.05835719779133797\n",
      "Epoch 3594/20000 Training Loss: 0.06906408816576004\n",
      "Epoch 3595/20000 Training Loss: 0.05450225621461868\n",
      "Epoch 3596/20000 Training Loss: 0.06601457297801971\n",
      "Epoch 3597/20000 Training Loss: 0.06461961567401886\n",
      "Epoch 3598/20000 Training Loss: 0.05837168172001839\n",
      "Epoch 3599/20000 Training Loss: 0.07157515734434128\n",
      "Epoch 3600/20000 Training Loss: 0.08190665394067764\n",
      "Epoch 3600/20000 Validation Loss: 0.09034554660320282\n",
      "Epoch 3601/20000 Training Loss: 0.06889607012271881\n",
      "Epoch 3602/20000 Training Loss: 0.06771376729011536\n",
      "Epoch 3603/20000 Training Loss: 0.07562355697154999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3604/20000 Training Loss: 0.06958694756031036\n",
      "Epoch 3605/20000 Training Loss: 0.0628698319196701\n",
      "Epoch 3606/20000 Training Loss: 0.055520251393318176\n",
      "Epoch 3607/20000 Training Loss: 0.06911913305521011\n",
      "Epoch 3608/20000 Training Loss: 0.07754819840192795\n",
      "Epoch 3609/20000 Training Loss: 0.03892862796783447\n",
      "Epoch 3610/20000 Training Loss: 0.06908397376537323\n",
      "Epoch 3611/20000 Training Loss: 0.06521175801753998\n",
      "Epoch 3612/20000 Training Loss: 0.0899176076054573\n",
      "Epoch 3613/20000 Training Loss: 0.05015973746776581\n",
      "Epoch 3614/20000 Training Loss: 0.07533019036054611\n",
      "Epoch 3615/20000 Training Loss: 0.06084330379962921\n",
      "Epoch 3616/20000 Training Loss: 0.0693037211894989\n",
      "Epoch 3617/20000 Training Loss: 0.07506394386291504\n",
      "Epoch 3618/20000 Training Loss: 0.06427001953125\n",
      "Epoch 3619/20000 Training Loss: 0.07019494473934174\n",
      "Epoch 3620/20000 Training Loss: 0.08232870697975159\n",
      "Epoch 3621/20000 Training Loss: 0.06128004193305969\n",
      "Epoch 3622/20000 Training Loss: 0.06890078634023666\n",
      "Epoch 3623/20000 Training Loss: 0.07895766943693161\n",
      "Epoch 3624/20000 Training Loss: 0.08949191868305206\n",
      "Epoch 3625/20000 Training Loss: 0.06745100021362305\n",
      "Epoch 3626/20000 Training Loss: 0.05417092144489288\n",
      "Epoch 3627/20000 Training Loss: 0.07050228118896484\n",
      "Epoch 3628/20000 Training Loss: 0.09377074986696243\n",
      "Epoch 3629/20000 Training Loss: 0.06910596042871475\n",
      "Epoch 3630/20000 Training Loss: 0.06683073937892914\n",
      "Epoch 3631/20000 Training Loss: 0.09383485466241837\n",
      "Epoch 3632/20000 Training Loss: 0.06427580118179321\n",
      "Epoch 3633/20000 Training Loss: 0.05328553915023804\n",
      "Epoch 3634/20000 Training Loss: 0.0670882910490036\n",
      "Epoch 3635/20000 Training Loss: 0.06181607395410538\n",
      "Epoch 3636/20000 Training Loss: 0.06430525332689285\n",
      "Epoch 3637/20000 Training Loss: 0.07648146152496338\n",
      "Epoch 3638/20000 Training Loss: 0.06949692219495773\n",
      "Epoch 3639/20000 Training Loss: 0.08076566457748413\n",
      "Epoch 3640/20000 Training Loss: 0.06061871349811554\n",
      "Epoch 3641/20000 Training Loss: 0.07789253443479538\n",
      "Epoch 3642/20000 Training Loss: 0.049498021602630615\n",
      "Epoch 3643/20000 Training Loss: 0.07259155809879303\n",
      "Epoch 3644/20000 Training Loss: 0.06767240911722183\n",
      "Epoch 3645/20000 Training Loss: 0.06449387222528458\n",
      "Epoch 3646/20000 Training Loss: 0.05895925313234329\n",
      "Epoch 3647/20000 Training Loss: 0.09185090661048889\n",
      "Epoch 3648/20000 Training Loss: 0.07280045747756958\n",
      "Epoch 3649/20000 Training Loss: 0.10598576068878174\n",
      "Epoch 3650/20000 Training Loss: 0.07044751942157745\n",
      "Epoch 3651/20000 Training Loss: 0.07614029943943024\n",
      "Epoch 3652/20000 Training Loss: 0.05809033662080765\n",
      "Epoch 3653/20000 Training Loss: 0.054021477699279785\n",
      "Epoch 3654/20000 Training Loss: 0.07772037386894226\n",
      "Epoch 3655/20000 Training Loss: 0.0762128233909607\n",
      "Epoch 3656/20000 Training Loss: 0.06263113766908646\n",
      "Epoch 3657/20000 Training Loss: 0.06325730681419373\n",
      "Epoch 3658/20000 Training Loss: 0.07008413970470428\n",
      "Epoch 3659/20000 Training Loss: 0.06259360164403915\n",
      "Epoch 3660/20000 Training Loss: 0.07688858360052109\n",
      "Epoch 3661/20000 Training Loss: 0.06093420833349228\n",
      "Epoch 3662/20000 Training Loss: 0.059659503400325775\n",
      "Epoch 3663/20000 Training Loss: 0.06768013536930084\n",
      "Epoch 3664/20000 Training Loss: 0.06246921420097351\n",
      "Epoch 3665/20000 Training Loss: 0.0602949857711792\n",
      "Epoch 3666/20000 Training Loss: 0.07731030881404877\n",
      "Epoch 3667/20000 Training Loss: 0.08794771134853363\n",
      "Epoch 3668/20000 Training Loss: 0.08175346255302429\n",
      "Epoch 3669/20000 Training Loss: 0.07575224339962006\n",
      "Epoch 3670/20000 Training Loss: 0.0683421865105629\n",
      "Epoch 3671/20000 Training Loss: 0.05320976674556732\n",
      "Epoch 3672/20000 Training Loss: 0.07352112233638763\n",
      "Epoch 3673/20000 Training Loss: 0.07209145277738571\n",
      "Epoch 3674/20000 Training Loss: 0.07009641081094742\n",
      "Epoch 3675/20000 Training Loss: 0.06102841719985008\n",
      "Epoch 3676/20000 Training Loss: 0.04783044010400772\n",
      "Epoch 3677/20000 Training Loss: 0.06100175529718399\n",
      "Epoch 3678/20000 Training Loss: 0.07471022754907608\n",
      "Epoch 3679/20000 Training Loss: 0.06478885561227798\n",
      "Epoch 3680/20000 Training Loss: 0.08399201929569244\n",
      "Epoch 3681/20000 Training Loss: 0.06814036518335342\n",
      "Epoch 3682/20000 Training Loss: 0.07318784296512604\n",
      "Epoch 3683/20000 Training Loss: 0.07515721023082733\n",
      "Epoch 3684/20000 Training Loss: 0.07060366868972778\n",
      "Epoch 3685/20000 Training Loss: 0.05746013671159744\n",
      "Epoch 3686/20000 Training Loss: 0.09592302143573761\n",
      "Epoch 3687/20000 Training Loss: 0.04829715937376022\n",
      "Epoch 3688/20000 Training Loss: 0.07333184033632278\n",
      "Epoch 3689/20000 Training Loss: 0.07271215319633484\n",
      "Epoch 3690/20000 Training Loss: 0.07110145688056946\n",
      "Epoch 3691/20000 Training Loss: 0.04943760484457016\n",
      "Epoch 3692/20000 Training Loss: 0.05729546397924423\n",
      "Epoch 3693/20000 Training Loss: 0.04878698289394379\n",
      "Epoch 3694/20000 Training Loss: 0.06672900915145874\n",
      "Epoch 3695/20000 Training Loss: 0.06494493037462234\n",
      "Epoch 3696/20000 Training Loss: 0.05731191486120224\n",
      "Epoch 3697/20000 Training Loss: 0.06362633407115936\n",
      "Epoch 3698/20000 Training Loss: 0.09091145545244217\n",
      "Epoch 3699/20000 Training Loss: 0.07057228684425354\n",
      "Epoch 3700/20000 Training Loss: 0.06737689673900604\n",
      "Epoch 3700/20000 Validation Loss: 0.09698933362960815\n",
      "Epoch 3701/20000 Training Loss: 0.058061156421899796\n",
      "Epoch 3702/20000 Training Loss: 0.06579040735960007\n",
      "Epoch 3703/20000 Training Loss: 0.06216062232851982\n",
      "Epoch 3704/20000 Training Loss: 0.08680224418640137\n",
      "Epoch 3705/20000 Training Loss: 0.07496260106563568\n",
      "Epoch 3706/20000 Training Loss: 0.06361167132854462\n",
      "Epoch 3707/20000 Training Loss: 0.07032635062932968\n",
      "Epoch 3708/20000 Training Loss: 0.06714580953121185\n",
      "Epoch 3709/20000 Training Loss: 0.09754618257284164\n",
      "Epoch 3710/20000 Training Loss: 0.05492924153804779\n",
      "Epoch 3711/20000 Training Loss: 0.08485063910484314\n",
      "Epoch 3712/20000 Training Loss: 0.051580414175987244\n",
      "Epoch 3713/20000 Training Loss: 0.0598895326256752\n",
      "Epoch 3714/20000 Training Loss: 0.07077956199645996\n",
      "Epoch 3715/20000 Training Loss: 0.06193627417087555\n",
      "Epoch 3716/20000 Training Loss: 0.09117387235164642\n",
      "Epoch 3717/20000 Training Loss: 0.0697992593050003\n",
      "Epoch 3718/20000 Training Loss: 0.06518837809562683\n",
      "Epoch 3719/20000 Training Loss: 0.07094240188598633\n",
      "Epoch 3720/20000 Training Loss: 0.059185586869716644\n",
      "Epoch 3721/20000 Training Loss: 0.07431699335575104\n",
      "Epoch 3722/20000 Training Loss: 0.06623812019824982\n",
      "Epoch 3723/20000 Training Loss: 0.06588489562273026\n",
      "Epoch 3724/20000 Training Loss: 0.05398638918995857\n",
      "Epoch 3725/20000 Training Loss: 0.06758884340524673\n",
      "Epoch 3726/20000 Training Loss: 0.07129513472318649\n",
      "Epoch 3727/20000 Training Loss: 0.0913236066699028\n",
      "Epoch 3728/20000 Training Loss: 0.07255318760871887\n",
      "Epoch 3729/20000 Training Loss: 0.06871914863586426\n",
      "Epoch 3730/20000 Training Loss: 0.07612571120262146\n",
      "Epoch 3731/20000 Training Loss: 0.06859808415174484\n",
      "Epoch 3732/20000 Training Loss: 0.07890592515468597\n",
      "Epoch 3733/20000 Training Loss: 0.044967442750930786\n",
      "Epoch 3734/20000 Training Loss: 0.09693397581577301\n",
      "Epoch 3735/20000 Training Loss: 0.06091003865003586\n",
      "Epoch 3736/20000 Training Loss: 0.07135984301567078\n",
      "Epoch 3737/20000 Training Loss: 0.06905251741409302\n",
      "Epoch 3738/20000 Training Loss: 0.08116921782493591\n",
      "Epoch 3739/20000 Training Loss: 0.06566356122493744\n",
      "Epoch 3740/20000 Training Loss: 0.06277208030223846\n",
      "Epoch 3741/20000 Training Loss: 0.047749921679496765\n",
      "Epoch 3742/20000 Training Loss: 0.07525421679019928\n",
      "Epoch 3743/20000 Training Loss: 0.05406000837683678\n",
      "Epoch 3744/20000 Training Loss: 0.05852942913770676\n",
      "Epoch 3745/20000 Training Loss: 0.07416731864213943\n",
      "Epoch 3746/20000 Training Loss: 0.07111095637083054\n",
      "Epoch 3747/20000 Training Loss: 0.06840136647224426\n",
      "Epoch 3748/20000 Training Loss: 0.08300155401229858\n",
      "Epoch 3749/20000 Training Loss: 0.07710207998752594\n",
      "Epoch 3750/20000 Training Loss: 0.06729964166879654\n",
      "Epoch 3751/20000 Training Loss: 0.06461654603481293\n",
      "Epoch 3752/20000 Training Loss: 0.06788680702447891\n",
      "Epoch 3753/20000 Training Loss: 0.09786393493413925\n",
      "Epoch 3754/20000 Training Loss: 0.056945063173770905\n",
      "Epoch 3755/20000 Training Loss: 0.05352970212697983\n",
      "Epoch 3756/20000 Training Loss: 0.09196901321411133\n",
      "Epoch 3757/20000 Training Loss: 0.07911225408315659\n",
      "Epoch 3758/20000 Training Loss: 0.059142183512449265\n",
      "Epoch 3759/20000 Training Loss: 0.07583428919315338\n",
      "Epoch 3760/20000 Training Loss: 0.08314791321754456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3761/20000 Training Loss: 0.0623926967382431\n",
      "Epoch 3762/20000 Training Loss: 0.09174452722072601\n",
      "Epoch 3763/20000 Training Loss: 0.09837212413549423\n",
      "Epoch 3764/20000 Training Loss: 0.07083550840616226\n",
      "Epoch 3765/20000 Training Loss: 0.04941398650407791\n",
      "Epoch 3766/20000 Training Loss: 0.07279811799526215\n",
      "Epoch 3767/20000 Training Loss: 0.05593222379684448\n",
      "Epoch 3768/20000 Training Loss: 0.07709196954965591\n",
      "Epoch 3769/20000 Training Loss: 0.060547955334186554\n",
      "Epoch 3770/20000 Training Loss: 0.08292153477668762\n",
      "Epoch 3771/20000 Training Loss: 0.077231265604496\n",
      "Epoch 3772/20000 Training Loss: 0.07166370749473572\n",
      "Epoch 3773/20000 Training Loss: 0.06200101226568222\n",
      "Epoch 3774/20000 Training Loss: 0.06350120902061462\n",
      "Epoch 3775/20000 Training Loss: 0.058579884469509125\n",
      "Epoch 3776/20000 Training Loss: 0.06242026761174202\n",
      "Epoch 3777/20000 Training Loss: 0.052989792078733444\n",
      "Epoch 3778/20000 Training Loss: 0.08255799859762192\n",
      "Epoch 3779/20000 Training Loss: 0.07636690884828568\n",
      "Epoch 3780/20000 Training Loss: 0.07338155806064606\n",
      "Epoch 3781/20000 Training Loss: 0.08386018127202988\n",
      "Epoch 3782/20000 Training Loss: 0.0652245357632637\n",
      "Epoch 3783/20000 Training Loss: 0.059865087270736694\n",
      "Epoch 3784/20000 Training Loss: 0.052802301943302155\n",
      "Epoch 3785/20000 Training Loss: 0.0640246719121933\n",
      "Epoch 3786/20000 Training Loss: 0.07542858272790909\n",
      "Epoch 3787/20000 Training Loss: 0.06129777431488037\n",
      "Epoch 3788/20000 Training Loss: 0.049977321177721024\n",
      "Epoch 3789/20000 Training Loss: 0.09599047154188156\n",
      "Epoch 3790/20000 Training Loss: 0.061518874019384384\n",
      "Epoch 3791/20000 Training Loss: 0.07043875008821487\n",
      "Epoch 3792/20000 Training Loss: 0.07930974662303925\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "TrainLoop(\n",
    "        model=model,\n",
    "        diffusion=diffusion,\n",
    "        data=data,\n",
    "        batch_size=batch_size,\n",
    "        microbatch=microbatch,\n",
    "        lr=lr,\n",
    "        ema_rate=ema_rate,\n",
    "        schedule_sampler=schedule_sampler,\n",
    "        weight_decay=weight_decay,\n",
    "        epochs=epochs,\n",
    "        eval_data=val,\n",
    "        eval_interval=eval_interval\n",
    "    ).run_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = datetime.now().strftime(\"%m%d\")\n",
    "# best_model_fp = f'models/{dt}/model_best_epoch_19200_min_val_loss_0.13920000195503235.pkl'\n",
    "# with open(best_model_fp, 'rb') as handle:\n",
    "#     best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref = sampling(best_model, diffusion, tokenizer, data_dir=regular_data_dir, batch_size=10, split='test_custom', seq_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9184761",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170fa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lst_ref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
