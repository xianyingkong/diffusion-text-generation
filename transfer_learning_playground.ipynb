{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a01d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25ea863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_arch.run_train import *\n",
    "from utils.step_sample import create_named_schedule_sampler\n",
    "from model_arch.train import TrainLoop\n",
    "from utils.data import load_data_text\n",
    "from model_arch.tokenizer import load_tokenizer, load_model_emb\n",
    "from model_arch.sampling import sampling\n",
    "\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast, BertTokenizerFast, set_seed\n",
    "import json, torch\n",
    "from utils import dist_util\n",
    "from functools import partial\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7cd8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_util.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fb13702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.00005\n",
    "batch_size=30\n",
    "val_batch_size=20\n",
    "microbatch=10\n",
    "epochs=20_000\n",
    "eval_interval=10\n",
    "ema_rate='0.9999' \n",
    "schedule_sampler='uniform'\n",
    "diffusion_steps=1000\n",
    "noise_schedule='sqrt'\n",
    "vocab='custom'\n",
    "use_plm_init='no' # embedding in transformer\n",
    "vocab_size=0\n",
    "config_name='bert-base-uncased'\n",
    "seq_len=128\n",
    "hidden_t_dim=128\n",
    "hidden_dim=128\n",
    "dropout=0.1\n",
    "seed=10275679\n",
    "weight_decay=0.0\n",
    "predict_xstart=True\n",
    "rescale_timesteps=True\n",
    "emb_scale_factor=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51cfcd12-4b84-4df7-827d-25c879230bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data_dir='data'\n",
    "data_player_dir='data/with_player'\n",
    "comedies_data_dir='data/comedies_only'\n",
    "\n",
    "# set the data directory\n",
    "data_dir=comedies_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffa896cd-2b1c-4ffe-8dbc-6fe4bc03ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f06dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer('shakespeare', config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3256f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight, tokenizer = load_model_emb(hidden_dim, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5366e001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30268, 128)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2542d933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30268"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## very very important to set this!!!!!\n",
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cc4920c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data/comedies_only...\n",
      "### Loading form the TRAIN set...\n",
      "### Data samples...\n",
      " ['i would not have my right rosalind of this mind, for, i protest, her frown might kill me.', 'i thought, by your readiness in the office, you had continued in it some time. you say, seven years together?'] ['by this hand, it will not kill a fly. but come, now i will be your rosalind in a more coming-on disposition, and ask me what you will. i will grant it.', 'and a half, sir.']\n",
      "RAM used: 5454.00 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 8706\n",
      "})\n",
      "RAM used: 5458.66 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f42a1f63d7d4071aa9ee26535383ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/8706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 8706\n",
      "})\n",
      "### tokenized_datasets...example [2, 31, 241, 125, 150, 105, 755, 2841, 94, 138, 727, 10, 115, 10, 31, 2566, 10, 175, 2191, 526, 803, 117, 12, 3]\n",
      "RAM used: 5459.00 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64f68b1600342ee84dde3dbc02d239c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/8706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 5471.64 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05aca54dd9ba454b902ed11620d7506e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/8706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 8706\n",
      "}) padded dataset\n",
      "RAM used: 5482.36 MB\n",
      "RAM used: 5482.36 MB\n",
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data/comedies_only...\n",
      "### Loading form the VALID set...\n",
      "### Data samples...\n",
      " ['to my petticoat, or what you will command me will i do, so well i know my duty to my elders.', \"youth, thou bear'st thy father's face, frank nature, rather curious than in haste, hath well composed thee. thy father's moral parts mayst thou inherit too! welcome to paris.\"] ['of all thy suitors, here i charge thee, tell whom thou lovest best see thou dissemble not.', \"my thanks and duty are your majesty's.\"]\n",
      "RAM used: 5475.68 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 2167\n",
      "})\n",
      "RAM used: 5475.68 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de47c4873f3441058d93120c8a4411c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=4):   0%|          | 0/2167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 2167\n",
      "})\n",
      "### tokenized_datasets...example [2, 88, 105, 10193, 10, 222, 164, 89, 159, 941, 117, 159, 31, 144, 10, 146, 254, 31, 251, 105, 1485, 88, 105, 13299, 12, 3]\n",
      "RAM used: 5471.64 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badbe35966d14581bea01900baa73ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/2167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 5475.55 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911de6ed38a34ac2b59d57df2d271354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/2167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 2167\n",
      "}) padded dataset\n",
      "RAM used: 5484.27 MB\n",
      "RAM used: 5484.41 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "        batch_size=batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        model_emb=model_weight # use model's weights as init\n",
    "    )\n",
    "\n",
    "val = load_data_text(\n",
    "        batch_size=val_batch_size,\n",
    "        seq_len=seq_len,\n",
    "        data_dir=data_dir,\n",
    "        loaded_vocab=tokenizer,\n",
    "        split='valid',\n",
    "        model_emb=model_weight, # use model's weights as init\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85a49540",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffusion = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, diffusion_steps),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")\n",
    "\n",
    "best_model_fp = 'models/results/final_model_df1000.pkl'\n",
    "with open(best_model_fp, 'rb') as handle:\n",
    "    model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9124ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91192508"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbe31a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======== Using Layer-wise Learning Rate Decay with AdamW ========\n",
      "\n",
      "\n",
      "name: word_embedding.weight, lr: 5e-05\n",
      "name: lm_head.bias, lr: 5e-05\n",
      "name: time_embed.0.weight, lr: 5e-05\n",
      "name: time_embed.0.bias, lr: 5e-05\n",
      "name: time_embed.2.weight, lr: 5e-05\n",
      "name: time_embed.2.bias, lr: 5e-05\n",
      "name: input_up_proj.0.weight, lr: 5e-05\n",
      "name: input_up_proj.0.bias, lr: 5e-05\n",
      "name: input_up_proj.2.weight, lr: 5e-05\n",
      "name: input_up_proj.2.bias, lr: 5e-05\n",
      "name: input_transformers.layer.0.attention.self.query.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.self.query.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.self.key.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.self.key.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.self.value.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.self.value.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.output.dense.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.output.dense.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.attention.output.LayerNorm.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.intermediate.dense.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.intermediate.dense.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.output.dense.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.output.dense.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.output.LayerNorm.weight, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.0.output.LayerNorm.bias, lr: 5.555555555555556e-05\n",
      "name: input_transformers.layer.1.attention.self.query.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.self.query.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.self.key.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.self.key.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.self.value.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.self.value.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.output.dense.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.output.dense.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.attention.output.LayerNorm.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.intermediate.dense.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.intermediate.dense.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.output.dense.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.output.dense.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.output.LayerNorm.weight, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.1.output.LayerNorm.bias, lr: 6.17283950617284e-05\n",
      "name: input_transformers.layer.2.attention.self.query.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.self.query.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.self.key.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.self.key.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.self.value.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.self.value.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.output.dense.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.output.dense.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.attention.output.LayerNorm.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.intermediate.dense.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.intermediate.dense.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.output.dense.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.output.dense.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.output.LayerNorm.weight, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.2.output.LayerNorm.bias, lr: 6.858710562414266e-05\n",
      "name: input_transformers.layer.3.attention.self.query.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.self.query.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.self.key.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.self.key.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.self.value.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.self.value.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.output.dense.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.output.dense.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.attention.output.LayerNorm.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.intermediate.dense.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.intermediate.dense.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.output.dense.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.output.dense.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.output.LayerNorm.weight, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.3.output.LayerNorm.bias, lr: 7.620789513793629e-05\n",
      "name: input_transformers.layer.4.attention.self.query.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.self.query.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.self.key.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.self.key.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.self.value.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.self.value.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.output.dense.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.output.dense.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.attention.output.LayerNorm.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.intermediate.dense.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.intermediate.dense.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.output.dense.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.output.dense.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.output.LayerNorm.weight, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.4.output.LayerNorm.bias, lr: 8.467543904215143e-05\n",
      "name: input_transformers.layer.5.attention.self.query.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.self.query.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.self.key.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.self.key.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.self.value.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.self.value.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.output.dense.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.output.dense.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.attention.output.LayerNorm.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.intermediate.dense.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.intermediate.dense.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.output.dense.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.output.dense.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.output.LayerNorm.weight, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.5.output.LayerNorm.bias, lr: 9.408382115794603e-05\n",
      "name: input_transformers.layer.6.attention.self.query.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.self.query.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.self.key.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.self.key.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.self.value.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.self.value.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.output.dense.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.output.dense.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.attention.output.LayerNorm.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.intermediate.dense.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.intermediate.dense.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.output.dense.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.output.dense.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.output.LayerNorm.weight, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.6.output.LayerNorm.bias, lr: 0.00010453757906438447\n",
      "name: input_transformers.layer.7.attention.self.query.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.self.query.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.self.key.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.self.key.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.self.value.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.self.value.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.output.dense.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.output.dense.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.attention.output.LayerNorm.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.intermediate.dense.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.intermediate.dense.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.output.dense.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.output.dense.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.output.LayerNorm.weight, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.7.output.LayerNorm.bias, lr: 0.00011615286562709386\n",
      "name: input_transformers.layer.8.attention.self.query.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.self.query.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.self.key.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.self.key.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.self.value.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.self.value.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.output.dense.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.output.dense.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.attention.output.LayerNorm.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.intermediate.dense.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.intermediate.dense.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.output.dense.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.output.dense.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.output.LayerNorm.weight, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.8.output.LayerNorm.bias, lr: 0.00012905873958565984\n",
      "name: input_transformers.layer.9.attention.self.query.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.self.query.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.self.key.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.self.key.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.self.value.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.self.value.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.output.dense.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.output.dense.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.attention.output.LayerNorm.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.intermediate.dense.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.intermediate.dense.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.output.dense.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.output.dense.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.output.LayerNorm.weight, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.9.output.LayerNorm.bias, lr: 0.00014339859953962203\n",
      "name: input_transformers.layer.10.attention.self.query.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.self.query.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.self.key.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.self.key.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.self.value.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.self.value.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.output.dense.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.output.dense.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.attention.output.LayerNorm.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.intermediate.dense.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.intermediate.dense.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.output.dense.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.output.dense.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.output.LayerNorm.weight, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.10.output.LayerNorm.bias, lr: 0.0001593317772662467\n",
      "name: input_transformers.layer.11.attention.self.query.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.self.query.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.self.key.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.self.key.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.self.value.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.self.value.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.output.dense.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.output.dense.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.attention.output.LayerNorm.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.intermediate.dense.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.intermediate.dense.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.output.dense.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.output.dense.bias, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.output.LayerNorm.weight, lr: 0.00017703530807360742\n",
      "name: input_transformers.layer.11.output.LayerNorm.bias, lr: 0.00017703530807360742\n",
      "name: position_embeddings.weight, lr: 0.0001967058978595638\n",
      "name: LayerNorm.weight, lr: 0.0001967058978595638\n",
      "name: LayerNorm.bias, lr: 0.0001967058978595638\n",
      "name: output_down_proj.0.weight, lr: 0.0001967058978595638\n",
      "name: output_down_proj.0.bias, lr: 0.0001967058978595638\n",
      "name: output_down_proj.2.weight, lr: 0.0001967058978595638\n",
      "name: output_down_proj.2.bias, lr: 0.0001967058978595638\n",
      "\n",
      "\n",
      "======== Training starts now ========\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20000 Training Loss: 0.046861130744218826\n",
      "Epoch 0/20000 Validation Loss: 0.04647196829319\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04647196829319<=============\n",
      "Epoch 1/20000 Training Loss: 0.06575585156679153\n",
      "Epoch 2/20000 Training Loss: 0.06334967166185379\n",
      "Epoch 3/20000 Training Loss: 0.05192740634083748\n",
      "Epoch 4/20000 Training Loss: 0.050199612975120544\n",
      "Epoch 5/20000 Training Loss: 0.07412896305322647\n",
      "Epoch 6/20000 Training Loss: 0.04815239831805229\n",
      "Epoch 7/20000 Training Loss: 0.05086776614189148\n",
      "Epoch 8/20000 Training Loss: 0.059878621250391006\n",
      "Epoch 9/20000 Training Loss: 0.0537346750497818\n",
      "Epoch 10/20000 Training Loss: 0.06251121312379837\n",
      "Epoch 10/20000 Validation Loss: 0.060411807149648666\n",
      "Epoch 11/20000 Training Loss: 0.04301084578037262\n",
      "Epoch 12/20000 Training Loss: 0.05401583015918732\n",
      "Epoch 13/20000 Training Loss: 0.06528591364622116\n",
      "Epoch 14/20000 Training Loss: 0.04747943580150604\n",
      "Epoch 15/20000 Training Loss: 0.06980641931295395\n",
      "Epoch 16/20000 Training Loss: 0.06822200864553452\n",
      "Epoch 17/20000 Training Loss: 0.07339555770158768\n",
      "Epoch 18/20000 Training Loss: 0.07370949536561966\n",
      "Epoch 19/20000 Training Loss: 0.06833896785974503\n",
      "Epoch 20/20000 Training Loss: 0.05279192700982094\n",
      "Epoch 20/20000 Validation Loss: 0.07380669564008713\n",
      "Epoch 21/20000 Training Loss: 0.05050674080848694\n",
      "Epoch 22/20000 Training Loss: 0.04401746019721031\n",
      "Epoch 23/20000 Training Loss: 0.04329923912882805\n",
      "Epoch 24/20000 Training Loss: 0.03863150626420975\n",
      "Epoch 25/20000 Training Loss: 0.06203289330005646\n",
      "Epoch 26/20000 Training Loss: 0.06111885979771614\n",
      "Epoch 27/20000 Training Loss: 0.05700324848294258\n",
      "Epoch 28/20000 Training Loss: 0.04843747988343239\n",
      "Epoch 29/20000 Training Loss: 0.06049196049571037\n",
      "Epoch 30/20000 Training Loss: 0.06320159882307053\n",
      "Epoch 30/20000 Validation Loss: 0.05204198136925697\n",
      "Epoch 31/20000 Training Loss: 0.07326250523328781\n",
      "Epoch 32/20000 Training Loss: 0.07801715284585953\n",
      "Epoch 33/20000 Training Loss: 0.05461109057068825\n",
      "Epoch 34/20000 Training Loss: 0.07751936465501785\n",
      "Epoch 35/20000 Training Loss: 0.05385909602046013\n",
      "Epoch 36/20000 Training Loss: 0.06478586047887802\n",
      "Epoch 37/20000 Training Loss: 0.055508702993392944\n",
      "Epoch 38/20000 Training Loss: 0.05002060532569885\n",
      "Epoch 39/20000 Training Loss: 0.07516490668058395\n",
      "Epoch 40/20000 Training Loss: 0.04941322281956673\n",
      "Epoch 40/20000 Validation Loss: 0.08842745423316956\n",
      "Epoch 41/20000 Training Loss: 0.05540470406413078\n",
      "Epoch 42/20000 Training Loss: 0.04592796787619591\n",
      "Epoch 43/20000 Training Loss: 0.051048215478658676\n",
      "Epoch 44/20000 Training Loss: 0.04794424772262573\n",
      "Epoch 45/20000 Training Loss: 0.05264898017048836\n",
      "Epoch 46/20000 Training Loss: 0.05944451689720154\n",
      "Epoch 47/20000 Training Loss: 0.04555949941277504\n",
      "Epoch 48/20000 Training Loss: 0.06771734356880188\n",
      "Epoch 49/20000 Training Loss: 0.07917112857103348\n",
      "Epoch 50/20000 Training Loss: 0.06369532644748688\n",
      "Epoch 50/20000 Validation Loss: 0.048417508602142334\n",
      "Epoch 51/20000 Training Loss: 0.04186468943953514\n",
      "Epoch 52/20000 Training Loss: 0.06317420303821564\n",
      "Epoch 53/20000 Training Loss: 0.056612685322761536\n",
      "Epoch 54/20000 Training Loss: 0.07602540403604507\n",
      "Epoch 55/20000 Training Loss: 0.06746984273195267\n",
      "Epoch 56/20000 Training Loss: 0.047614287585020065\n",
      "Epoch 57/20000 Training Loss: 0.06862765550613403\n",
      "Epoch 58/20000 Training Loss: 0.061896901577711105\n",
      "Epoch 59/20000 Training Loss: 0.08096092194318771\n",
      "Epoch 60/20000 Training Loss: 0.05011431500315666\n",
      "Epoch 60/20000 Validation Loss: 0.06580419838428497\n",
      "Epoch 61/20000 Training Loss: 0.054099246859550476\n",
      "Epoch 62/20000 Training Loss: 0.0673273354768753\n",
      "Epoch 63/20000 Training Loss: 0.06862584501504898\n",
      "Epoch 64/20000 Training Loss: 0.062122195959091187\n",
      "Epoch 65/20000 Training Loss: 0.06411207467317581\n",
      "Epoch 66/20000 Training Loss: 0.06479253619909286\n",
      "Epoch 67/20000 Training Loss: 0.07859188318252563\n",
      "Epoch 68/20000 Training Loss: 0.05485755205154419\n",
      "Epoch 69/20000 Training Loss: 0.051495641469955444\n",
      "Epoch 70/20000 Training Loss: 0.046605419367551804\n",
      "Epoch 70/20000 Validation Loss: 0.04586351662874222\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04586351662874222<=============\n",
      "Epoch 71/20000 Training Loss: 0.05716976150870323\n",
      "Epoch 72/20000 Training Loss: 0.07704632729291916\n",
      "Epoch 73/20000 Training Loss: 0.05809268727898598\n",
      "Epoch 74/20000 Training Loss: 0.07891879230737686\n",
      "Epoch 75/20000 Training Loss: 0.04916244372725487\n",
      "Epoch 76/20000 Training Loss: 0.052864015102386475\n",
      "Epoch 77/20000 Training Loss: 0.053279612213373184\n",
      "Epoch 78/20000 Training Loss: 0.06194611266255379\n",
      "Epoch 79/20000 Training Loss: 0.050574395805597305\n",
      "Epoch 80/20000 Training Loss: 0.08023426681756973\n",
      "Epoch 80/20000 Validation Loss: 0.05316966772079468\n",
      "Epoch 81/20000 Training Loss: 0.04004671797156334\n",
      "Epoch 82/20000 Training Loss: 0.048107076436281204\n",
      "Epoch 83/20000 Training Loss: 0.056194454431533813\n",
      "Epoch 84/20000 Training Loss: 0.04974353685975075\n",
      "Epoch 85/20000 Training Loss: 0.05047297850251198\n",
      "Epoch 86/20000 Training Loss: 0.06583002954721451\n",
      "Epoch 87/20000 Training Loss: 0.047171007841825485\n",
      "Epoch 88/20000 Training Loss: 0.06831874698400497\n",
      "Epoch 89/20000 Training Loss: 0.06444777548313141\n",
      "Epoch 90/20000 Training Loss: 0.05770668387413025\n",
      "Epoch 90/20000 Validation Loss: 0.066012904047966\n",
      "Epoch 91/20000 Training Loss: 0.05115136504173279\n",
      "Epoch 92/20000 Training Loss: 0.06537162512540817\n",
      "Epoch 93/20000 Training Loss: 0.07275870442390442\n",
      "Epoch 94/20000 Training Loss: 0.04884382709860802\n",
      "Epoch 95/20000 Training Loss: 0.05297991633415222\n",
      "Epoch 96/20000 Training Loss: 0.07895206660032272\n",
      "Epoch 97/20000 Training Loss: 0.04016649350523949\n",
      "Epoch 98/20000 Training Loss: 0.05783450976014137\n",
      "Epoch 99/20000 Training Loss: 0.05843105539679527\n",
      "Epoch 100/20000 Training Loss: 0.06904959678649902\n",
      "Epoch 100/20000 Validation Loss: 0.05416315793991089\n",
      "Epoch 101/20000 Training Loss: 0.061613839119672775\n",
      "Epoch 102/20000 Training Loss: 0.046628743410110474\n",
      "Epoch 103/20000 Training Loss: 0.044698506593704224\n",
      "Epoch 104/20000 Training Loss: 0.07050848752260208\n",
      "Epoch 105/20000 Training Loss: 0.0832703486084938\n",
      "Epoch 106/20000 Training Loss: 0.054197102785110474\n",
      "Epoch 107/20000 Training Loss: 0.06089368462562561\n",
      "Epoch 108/20000 Training Loss: 0.05795125290751457\n",
      "Epoch 109/20000 Training Loss: 0.06046891212463379\n",
      "Epoch 110/20000 Training Loss: 0.06709693372249603\n",
      "Epoch 110/20000 Validation Loss: 0.04348365217447281\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04348365217447281<=============\n",
      "Epoch 111/20000 Training Loss: 0.05128331854939461\n",
      "Epoch 112/20000 Training Loss: 0.06496679782867432\n",
      "Epoch 113/20000 Training Loss: 0.04940861091017723\n",
      "Epoch 114/20000 Training Loss: 0.08423542231321335\n",
      "Epoch 115/20000 Training Loss: 0.05134153738617897\n",
      "Epoch 116/20000 Training Loss: 0.05041931942105293\n",
      "Epoch 117/20000 Training Loss: 0.049153972417116165\n",
      "Epoch 118/20000 Training Loss: 0.042208075523376465\n",
      "Epoch 119/20000 Training Loss: 0.054459359496831894\n",
      "Epoch 120/20000 Training Loss: 0.04204815998673439\n",
      "Epoch 120/20000 Validation Loss: 0.04131321236491203\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.04131321236491203<=============\n",
      "Epoch 121/20000 Training Loss: 0.061461180448532104\n",
      "Epoch 122/20000 Training Loss: 0.06793788820505142\n",
      "Epoch 123/20000 Training Loss: 0.055900219827890396\n",
      "Epoch 124/20000 Training Loss: 0.05964670702815056\n",
      "Epoch 125/20000 Training Loss: 0.054235514253377914\n",
      "Epoch 126/20000 Training Loss: 0.051615580916404724\n",
      "Epoch 127/20000 Training Loss: 0.05996434763073921\n",
      "Epoch 128/20000 Training Loss: 0.05681470409035683\n",
      "Epoch 129/20000 Training Loss: 0.059076447039842606\n",
      "Epoch 130/20000 Training Loss: 0.06217581033706665\n",
      "Epoch 130/20000 Validation Loss: 0.06355595588684082\n",
      "Epoch 131/20000 Training Loss: 0.0701342299580574\n",
      "Epoch 132/20000 Training Loss: 0.04003918170928955\n",
      "Epoch 133/20000 Training Loss: 0.048444684594869614\n",
      "Epoch 134/20000 Training Loss: 0.06068279221653938\n",
      "Epoch 135/20000 Training Loss: 0.06436732411384583\n",
      "Epoch 136/20000 Training Loss: 0.054461196064949036\n",
      "Epoch 137/20000 Training Loss: 0.06158149614930153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/20000 Training Loss: 0.0719294548034668\n",
      "Epoch 139/20000 Training Loss: 0.0912608802318573\n",
      "Epoch 140/20000 Training Loss: 0.06126176193356514\n",
      "Epoch 140/20000 Validation Loss: 0.05715722590684891\n",
      "Epoch 141/20000 Training Loss: 0.054287731647491455\n",
      "Epoch 142/20000 Training Loss: 0.05684271082282066\n",
      "Epoch 143/20000 Training Loss: 0.06864909082651138\n",
      "Epoch 144/20000 Training Loss: 0.06541838496923447\n",
      "Epoch 145/20000 Training Loss: 0.07817097753286362\n",
      "Epoch 146/20000 Training Loss: 0.07764425873756409\n",
      "Epoch 147/20000 Training Loss: 0.0762665644288063\n",
      "Epoch 148/20000 Training Loss: 0.06457202881574631\n",
      "Epoch 149/20000 Training Loss: 0.05994972586631775\n",
      "Epoch 150/20000 Training Loss: 0.0675877258181572\n",
      "Epoch 150/20000 Validation Loss: 0.042040929198265076\n",
      "Epoch 151/20000 Training Loss: 0.055356044322252274\n",
      "Epoch 152/20000 Training Loss: 0.04870573803782463\n",
      "Epoch 153/20000 Training Loss: 0.051883865147829056\n",
      "Epoch 154/20000 Training Loss: 0.053221434354782104\n",
      "Epoch 155/20000 Training Loss: 0.06078636646270752\n",
      "Epoch 156/20000 Training Loss: 0.050668131560087204\n",
      "Epoch 157/20000 Training Loss: 0.04155247285962105\n",
      "Epoch 158/20000 Training Loss: 0.07899798452854156\n",
      "Epoch 159/20000 Training Loss: 0.06576673686504364\n",
      "Epoch 160/20000 Training Loss: 0.05381615459918976\n",
      "Epoch 160/20000 Validation Loss: 0.056449443101882935\n",
      "Epoch 161/20000 Training Loss: 0.06187731400132179\n",
      "Epoch 162/20000 Training Loss: 0.05857685208320618\n",
      "Epoch 163/20000 Training Loss: 0.05519600585103035\n",
      "Epoch 164/20000 Training Loss: 0.060632508248090744\n",
      "Epoch 165/20000 Training Loss: 0.055200014263391495\n",
      "Epoch 166/20000 Training Loss: 0.05572604015469551\n",
      "Epoch 167/20000 Training Loss: 0.049100130796432495\n",
      "Epoch 168/20000 Training Loss: 0.04667598009109497\n",
      "Epoch 169/20000 Training Loss: 0.06376288086175919\n",
      "Epoch 170/20000 Training Loss: 0.0437084324657917\n",
      "Epoch 170/20000 Validation Loss: 0.050869911909103394\n",
      "Epoch 171/20000 Training Loss: 0.06690821051597595\n",
      "Epoch 172/20000 Training Loss: 0.05891759321093559\n",
      "Epoch 173/20000 Training Loss: 0.06279855966567993\n",
      "Epoch 174/20000 Training Loss: 0.05072079226374626\n",
      "Epoch 175/20000 Training Loss: 0.05313875898718834\n",
      "Epoch 176/20000 Training Loss: 0.059437498450279236\n",
      "Epoch 177/20000 Training Loss: 0.06558830291032791\n",
      "Epoch 178/20000 Training Loss: 0.0633256733417511\n",
      "Epoch 179/20000 Training Loss: 0.04704928770661354\n",
      "Epoch 180/20000 Training Loss: 0.050109684467315674\n",
      "Epoch 180/20000 Validation Loss: 0.05887588858604431\n",
      "Epoch 181/20000 Training Loss: 0.0625288113951683\n",
      "Epoch 182/20000 Training Loss: 0.05508481338620186\n",
      "Epoch 183/20000 Training Loss: 0.05050557851791382\n",
      "Epoch 184/20000 Training Loss: 0.05230458080768585\n",
      "Epoch 185/20000 Training Loss: 0.05733084678649902\n",
      "Epoch 186/20000 Training Loss: 0.05251946672797203\n",
      "Epoch 187/20000 Training Loss: 0.07171217352151871\n",
      "Epoch 188/20000 Training Loss: 0.06774508208036423\n",
      "Epoch 189/20000 Training Loss: 0.06708616763353348\n",
      "Epoch 190/20000 Training Loss: 0.05813721939921379\n",
      "Epoch 190/20000 Validation Loss: 0.07674472033977509\n",
      "Epoch 191/20000 Training Loss: 0.07032894343137741\n",
      "Epoch 192/20000 Training Loss: 0.05114540085196495\n",
      "Epoch 193/20000 Training Loss: 0.0615697056055069\n",
      "Epoch 194/20000 Training Loss: 0.07198268175125122\n",
      "Epoch 195/20000 Training Loss: 0.05518561974167824\n",
      "Epoch 196/20000 Training Loss: 0.04202659800648689\n",
      "Epoch 197/20000 Training Loss: 0.05480906739830971\n",
      "Epoch 198/20000 Training Loss: 0.060603950172662735\n",
      "Epoch 199/20000 Training Loss: 0.05384479835629463\n",
      "Epoch 200/20000 Training Loss: 0.061240848153829575\n",
      "Epoch 200/20000 Validation Loss: 0.061168182641267776\n",
      "Epoch 201/20000 Training Loss: 0.06554145365953445\n",
      "Epoch 202/20000 Training Loss: 0.05594708397984505\n",
      "Epoch 203/20000 Training Loss: 0.06033601239323616\n",
      "Epoch 204/20000 Training Loss: 0.06860841810703278\n",
      "Epoch 205/20000 Training Loss: 0.06938520073890686\n",
      "Epoch 206/20000 Training Loss: 0.06844440847635269\n",
      "Epoch 207/20000 Training Loss: 0.0647866502404213\n",
      "Epoch 208/20000 Training Loss: 0.05377886816859245\n",
      "Epoch 209/20000 Training Loss: 0.05364493653178215\n",
      "Epoch 210/20000 Training Loss: 0.06234259903430939\n",
      "Epoch 210/20000 Validation Loss: 0.06031402200460434\n",
      "Epoch 211/20000 Training Loss: 0.06246337294578552\n",
      "Epoch 212/20000 Training Loss: 0.06417877227067947\n",
      "Epoch 213/20000 Training Loss: 0.04994149133563042\n",
      "Epoch 214/20000 Training Loss: 0.06697895377874374\n",
      "Epoch 215/20000 Training Loss: 0.04869839921593666\n",
      "Epoch 216/20000 Training Loss: 0.08048985153436661\n",
      "Epoch 217/20000 Training Loss: 0.0586232952773571\n",
      "Epoch 218/20000 Training Loss: 0.06052469089627266\n",
      "Epoch 219/20000 Training Loss: 0.06485442817211151\n",
      "Epoch 220/20000 Training Loss: 0.06116315349936485\n",
      "Epoch 220/20000 Validation Loss: 0.06761647760868073\n",
      "Epoch 221/20000 Training Loss: 0.051612794399261475\n",
      "Epoch 222/20000 Training Loss: 0.05989038571715355\n",
      "Epoch 223/20000 Training Loss: 0.06132965162396431\n",
      "Epoch 224/20000 Training Loss: 0.05970437452197075\n",
      "Epoch 225/20000 Training Loss: 0.06960274279117584\n",
      "Epoch 226/20000 Training Loss: 0.051674630492925644\n",
      "Epoch 227/20000 Training Loss: 0.07547203451395035\n",
      "Epoch 228/20000 Training Loss: 0.06555754691362381\n",
      "Epoch 229/20000 Training Loss: 0.06211741268634796\n",
      "Epoch 230/20000 Training Loss: 0.05046762898564339\n",
      "Epoch 230/20000 Validation Loss: 0.05916546285152435\n",
      "Epoch 231/20000 Training Loss: 0.046456124633550644\n",
      "Epoch 232/20000 Training Loss: 0.06809020787477493\n",
      "Epoch 233/20000 Training Loss: 0.05852685868740082\n",
      "Epoch 234/20000 Training Loss: 0.06655923277139664\n",
      "Epoch 235/20000 Training Loss: 0.061331626027822495\n",
      "Epoch 236/20000 Training Loss: 0.05264062061905861\n",
      "Epoch 237/20000 Training Loss: 0.07569337636232376\n",
      "Epoch 238/20000 Training Loss: 0.06941866129636765\n",
      "Epoch 239/20000 Training Loss: 0.0820963978767395\n",
      "Epoch 240/20000 Training Loss: 0.04733744636178017\n",
      "Epoch 240/20000 Validation Loss: 0.0531025156378746\n",
      "Epoch 241/20000 Training Loss: 0.06404511630535126\n",
      "Epoch 242/20000 Training Loss: 0.041727591305971146\n",
      "Epoch 243/20000 Training Loss: 0.05123870447278023\n",
      "Epoch 244/20000 Training Loss: 0.057008322328329086\n",
      "Epoch 245/20000 Training Loss: 0.08704090863466263\n",
      "Epoch 246/20000 Training Loss: 0.0695737972855568\n",
      "Epoch 247/20000 Training Loss: 0.05292215943336487\n",
      "Epoch 248/20000 Training Loss: 0.0510873943567276\n",
      "Epoch 249/20000 Training Loss: 0.055562183260917664\n",
      "Epoch 250/20000 Training Loss: 0.06902554631233215\n",
      "Epoch 250/20000 Validation Loss: 0.0563034787774086\n",
      "Epoch 251/20000 Training Loss: 0.06637110561132431\n",
      "Epoch 252/20000 Training Loss: 0.06976950913667679\n",
      "Epoch 253/20000 Training Loss: 0.05418849363923073\n",
      "Epoch 254/20000 Training Loss: 0.07530496269464493\n",
      "Epoch 255/20000 Training Loss: 0.051747385412454605\n",
      "Epoch 256/20000 Training Loss: 0.07480043172836304\n",
      "Epoch 257/20000 Training Loss: 0.0735749676823616\n",
      "Epoch 258/20000 Training Loss: 0.05306468531489372\n",
      "Epoch 259/20000 Training Loss: 0.05129215493798256\n",
      "Epoch 260/20000 Training Loss: 0.05199851468205452\n",
      "Epoch 260/20000 Validation Loss: 0.0773005485534668\n",
      "Epoch 261/20000 Training Loss: 0.06503396481275558\n",
      "Epoch 262/20000 Training Loss: 0.06105661019682884\n",
      "Epoch 263/20000 Training Loss: 0.04542514309287071\n",
      "Epoch 264/20000 Training Loss: 0.06227770447731018\n",
      "Epoch 265/20000 Training Loss: 0.05915991589426994\n",
      "Epoch 266/20000 Training Loss: 0.06884455680847168\n",
      "Epoch 267/20000 Training Loss: 0.05648592486977577\n",
      "Epoch 268/20000 Training Loss: 0.0620223693549633\n",
      "Epoch 269/20000 Training Loss: 0.05322036147117615\n",
      "Epoch 270/20000 Training Loss: 0.05822664499282837\n",
      "Epoch 270/20000 Validation Loss: 0.07051420211791992\n",
      "Epoch 271/20000 Training Loss: 0.06399253755807877\n",
      "Epoch 272/20000 Training Loss: 0.0390612967312336\n",
      "Epoch 273/20000 Training Loss: 0.0684669241309166\n",
      "Epoch 274/20000 Training Loss: 0.05968368053436279\n",
      "Epoch 275/20000 Training Loss: 0.055208999663591385\n",
      "Epoch 276/20000 Training Loss: 0.05109143257141113\n",
      "Epoch 277/20000 Training Loss: 0.046133000403642654\n",
      "Epoch 278/20000 Training Loss: 0.058131348341703415\n",
      "Epoch 279/20000 Training Loss: 0.04724996164441109\n",
      "Epoch 280/20000 Training Loss: 0.06593695282936096\n",
      "Epoch 280/20000 Validation Loss: 0.04493742063641548\n",
      "Epoch 281/20000 Training Loss: 0.05466833710670471\n",
      "Epoch 282/20000 Training Loss: 0.04760280251502991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 283/20000 Training Loss: 0.05138108506798744\n",
      "Epoch 284/20000 Training Loss: 0.05591554939746857\n",
      "Epoch 285/20000 Training Loss: 0.04297414422035217\n",
      "Epoch 286/20000 Training Loss: 0.05082109197974205\n",
      "Epoch 287/20000 Training Loss: 0.0678977370262146\n",
      "Epoch 288/20000 Training Loss: 0.05472026392817497\n",
      "Epoch 289/20000 Training Loss: 0.06693508476018906\n",
      "Epoch 290/20000 Training Loss: 0.05370793491601944\n",
      "Epoch 290/20000 Validation Loss: 0.05952521413564682\n",
      "Epoch 291/20000 Training Loss: 0.04837982356548309\n",
      "Epoch 292/20000 Training Loss: 0.06354865431785583\n",
      "Epoch 293/20000 Training Loss: 0.06321483105421066\n",
      "Epoch 294/20000 Training Loss: 0.06408102065324783\n",
      "Epoch 295/20000 Training Loss: 0.053368452936410904\n",
      "Epoch 296/20000 Training Loss: 0.05590392276644707\n",
      "Epoch 297/20000 Training Loss: 0.052984271198511124\n",
      "Epoch 298/20000 Training Loss: 0.05145363137125969\n",
      "Epoch 299/20000 Training Loss: 0.059255439788103104\n",
      "Epoch 300/20000 Training Loss: 0.059561748057603836\n",
      "Epoch 300/20000 Validation Loss: 0.05552549660205841\n",
      "Epoch 301/20000 Training Loss: 0.07063966989517212\n",
      "Epoch 302/20000 Training Loss: 0.07205891609191895\n",
      "Epoch 303/20000 Training Loss: 0.04976436123251915\n",
      "Epoch 304/20000 Training Loss: 0.06813063472509384\n",
      "Epoch 305/20000 Training Loss: 0.05685797333717346\n",
      "Epoch 306/20000 Training Loss: 0.04241090640425682\n",
      "Epoch 307/20000 Training Loss: 0.06912817806005478\n",
      "Epoch 308/20000 Training Loss: 0.0530792772769928\n",
      "Epoch 309/20000 Training Loss: 0.06423920392990112\n",
      "Epoch 310/20000 Training Loss: 0.04137565195560455\n",
      "Epoch 310/20000 Validation Loss: 0.0530582070350647\n",
      "Epoch 311/20000 Training Loss: 0.049683619290590286\n",
      "Epoch 312/20000 Training Loss: 0.07550408691167831\n",
      "Epoch 313/20000 Training Loss: 0.05010243132710457\n",
      "Epoch 314/20000 Training Loss: 0.05538567528128624\n",
      "Epoch 315/20000 Training Loss: 0.05510645732283592\n",
      "Epoch 316/20000 Training Loss: 0.05804822966456413\n",
      "Epoch 317/20000 Training Loss: 0.04052825644612312\n",
      "Epoch 318/20000 Training Loss: 0.06313486397266388\n",
      "Epoch 319/20000 Training Loss: 0.05999599024653435\n",
      "Epoch 320/20000 Training Loss: 0.05517039820551872\n",
      "Epoch 320/20000 Validation Loss: 0.0641772449016571\n",
      "Epoch 321/20000 Training Loss: 0.06962056457996368\n",
      "Epoch 322/20000 Training Loss: 0.08330392837524414\n",
      "Epoch 323/20000 Training Loss: 0.07095726579427719\n",
      "Epoch 324/20000 Training Loss: 0.05558424070477486\n",
      "Epoch 325/20000 Training Loss: 0.061612438410520554\n",
      "Epoch 326/20000 Training Loss: 0.05614915117621422\n",
      "Epoch 327/20000 Training Loss: 0.06314825266599655\n",
      "Epoch 328/20000 Training Loss: 0.053093284368515015\n",
      "Epoch 329/20000 Training Loss: 0.05354806408286095\n",
      "Epoch 330/20000 Training Loss: 0.05925126001238823\n",
      "Epoch 330/20000 Validation Loss: 0.057495903223752975\n",
      "Epoch 331/20000 Training Loss: 0.04915764927864075\n",
      "Epoch 332/20000 Training Loss: 0.05226558446884155\n",
      "Epoch 333/20000 Training Loss: 0.056250471621751785\n",
      "Epoch 334/20000 Training Loss: 0.04606935381889343\n",
      "Epoch 335/20000 Training Loss: 0.04613688960671425\n",
      "Epoch 336/20000 Training Loss: 0.07572708278894424\n",
      "Epoch 337/20000 Training Loss: 0.0565449483692646\n",
      "Epoch 338/20000 Training Loss: 0.05114087462425232\n",
      "Epoch 339/20000 Training Loss: 0.0692545473575592\n",
      "Epoch 340/20000 Training Loss: 0.04743116721510887\n",
      "Epoch 340/20000 Validation Loss: 0.0571012906730175\n",
      "Epoch 341/20000 Training Loss: 0.04647805169224739\n",
      "Epoch 342/20000 Training Loss: 0.064661405980587\n",
      "Epoch 343/20000 Training Loss: 0.08219951391220093\n",
      "Epoch 344/20000 Training Loss: 0.049833718687295914\n",
      "Epoch 345/20000 Training Loss: 0.056525155901908875\n",
      "Epoch 346/20000 Training Loss: 0.05802062153816223\n",
      "Epoch 347/20000 Training Loss: 0.045956116169691086\n",
      "Epoch 348/20000 Training Loss: 0.07185631990432739\n",
      "Epoch 349/20000 Training Loss: 0.050427380949258804\n",
      "Epoch 350/20000 Training Loss: 0.07797051221132278\n",
      "Epoch 350/20000 Validation Loss: 0.04617747664451599\n",
      "Epoch 351/20000 Training Loss: 0.05290014669299126\n",
      "Epoch 352/20000 Training Loss: 0.06407412886619568\n",
      "Epoch 353/20000 Training Loss: 0.059329401701688766\n",
      "Epoch 354/20000 Training Loss: 0.07225234061479568\n",
      "Epoch 355/20000 Training Loss: 0.059743236750364304\n",
      "Epoch 356/20000 Training Loss: 0.052552852779626846\n",
      "Epoch 357/20000 Training Loss: 0.05392949655652046\n",
      "Epoch 358/20000 Training Loss: 0.04161245748400688\n",
      "Epoch 359/20000 Training Loss: 0.04544825479388237\n",
      "Epoch 360/20000 Training Loss: 0.07173088192939758\n",
      "Epoch 360/20000 Validation Loss: 0.050249017775058746\n",
      "Epoch 361/20000 Training Loss: 0.03778548166155815\n",
      "Epoch 362/20000 Training Loss: 0.06023317947983742\n",
      "Epoch 363/20000 Training Loss: 0.07285197824239731\n",
      "Epoch 364/20000 Training Loss: 0.06010090187191963\n",
      "Epoch 365/20000 Training Loss: 0.05045516416430473\n",
      "Epoch 366/20000 Training Loss: 0.05790886655449867\n",
      "Epoch 367/20000 Training Loss: 0.04334352910518646\n",
      "Epoch 368/20000 Training Loss: 0.06934476643800735\n",
      "Epoch 369/20000 Training Loss: 0.05935228243470192\n",
      "Epoch 370/20000 Training Loss: 0.048725757747888565\n",
      "Epoch 370/20000 Validation Loss: 0.05683724954724312\n",
      "Epoch 371/20000 Training Loss: 0.07196293026208878\n",
      "Epoch 372/20000 Training Loss: 0.053604308515787125\n",
      "Epoch 373/20000 Training Loss: 0.04324505850672722\n",
      "Epoch 374/20000 Training Loss: 0.06480944156646729\n",
      "Epoch 375/20000 Training Loss: 0.06402873992919922\n",
      "Epoch 376/20000 Training Loss: 0.06085951253771782\n",
      "Epoch 377/20000 Training Loss: 0.056695569306612015\n",
      "Epoch 378/20000 Training Loss: 0.06853986531496048\n",
      "Epoch 379/20000 Training Loss: 0.059107035398483276\n",
      "Epoch 380/20000 Training Loss: 0.05788525938987732\n",
      "Epoch 380/20000 Validation Loss: 0.056849442422389984\n",
      "Epoch 381/20000 Training Loss: 0.06541901081800461\n",
      "Epoch 382/20000 Training Loss: 0.05697037652134895\n",
      "Epoch 383/20000 Training Loss: 0.07736002653837204\n",
      "Epoch 384/20000 Training Loss: 0.08273941278457642\n",
      "Epoch 385/20000 Training Loss: 0.06040565297007561\n",
      "Epoch 386/20000 Training Loss: 0.06910624355077744\n",
      "Epoch 387/20000 Training Loss: 0.05802316591143608\n",
      "Epoch 388/20000 Training Loss: 0.043751608580350876\n",
      "Epoch 389/20000 Training Loss: 0.06724686920642853\n",
      "Epoch 390/20000 Training Loss: 0.08838397264480591\n",
      "Epoch 390/20000 Validation Loss: 0.05256939306855202\n",
      "Epoch 391/20000 Training Loss: 0.06781595945358276\n",
      "Epoch 392/20000 Training Loss: 0.06591780483722687\n",
      "Epoch 393/20000 Training Loss: 0.044494178146123886\n",
      "Epoch 394/20000 Training Loss: 0.042417388409376144\n",
      "Epoch 395/20000 Training Loss: 0.062229324132204056\n",
      "Epoch 396/20000 Training Loss: 0.06483618170022964\n",
      "Epoch 397/20000 Training Loss: 0.05496549606323242\n",
      "Epoch 398/20000 Training Loss: 0.05893012881278992\n",
      "Epoch 399/20000 Training Loss: 0.0617939829826355\n",
      "Epoch 400/20000 Training Loss: 0.05666918680071831\n",
      "Epoch 400/20000 Validation Loss: 0.048820771276950836\n",
      "Epoch 401/20000 Training Loss: 0.06038776412606239\n",
      "Epoch 402/20000 Training Loss: 0.0654548853635788\n",
      "Epoch 403/20000 Training Loss: 0.07884357869625092\n",
      "Epoch 404/20000 Training Loss: 0.048832785338163376\n",
      "Epoch 405/20000 Training Loss: 0.0566842257976532\n",
      "Epoch 406/20000 Training Loss: 0.04824858531355858\n",
      "Epoch 407/20000 Training Loss: 0.04844105243682861\n",
      "Epoch 408/20000 Training Loss: 0.05328014865517616\n",
      "Epoch 409/20000 Training Loss: 0.06609528511762619\n",
      "Epoch 410/20000 Training Loss: 0.08282655477523804\n",
      "Epoch 410/20000 Validation Loss: 0.061506614089012146\n",
      "Epoch 411/20000 Training Loss: 0.05229467526078224\n",
      "Epoch 412/20000 Training Loss: 0.06738501787185669\n",
      "Epoch 413/20000 Training Loss: 0.04193873330950737\n",
      "Epoch 414/20000 Training Loss: 0.05140605568885803\n",
      "Epoch 415/20000 Training Loss: 0.075138621032238\n",
      "Epoch 416/20000 Training Loss: 0.08516017347574234\n",
      "Epoch 417/20000 Training Loss: 0.06355161219835281\n",
      "Epoch 418/20000 Training Loss: 0.06618069112300873\n",
      "Epoch 419/20000 Training Loss: 0.06273555010557175\n",
      "Epoch 420/20000 Training Loss: 0.04758346080780029\n",
      "Epoch 420/20000 Validation Loss: 0.049995794892311096\n",
      "Epoch 421/20000 Training Loss: 0.06282923370599747\n",
      "Epoch 422/20000 Training Loss: 0.06068414822220802\n",
      "Epoch 423/20000 Training Loss: 0.04592997953295708\n",
      "Epoch 424/20000 Training Loss: 0.040063321590423584\n",
      "Epoch 425/20000 Training Loss: 0.048712898045778275\n",
      "Epoch 426/20000 Training Loss: 0.060746997594833374\n",
      "Epoch 427/20000 Training Loss: 0.06422833353281021\n",
      "Epoch 428/20000 Training Loss: 0.051903143525123596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/20000 Training Loss: 0.05687728524208069\n",
      "Epoch 430/20000 Training Loss: 0.06097899004817009\n",
      "Epoch 430/20000 Validation Loss: 0.07599315792322159\n",
      "Epoch 431/20000 Training Loss: 0.06017455458641052\n",
      "Epoch 432/20000 Training Loss: 0.04742817580699921\n",
      "Epoch 433/20000 Training Loss: 0.04477252438664436\n",
      "Epoch 434/20000 Training Loss: 0.06840742379426956\n",
      "Epoch 435/20000 Training Loss: 0.06796757131814957\n",
      "Epoch 436/20000 Training Loss: 0.05977143719792366\n",
      "Epoch 437/20000 Training Loss: 0.06639730930328369\n",
      "Epoch 438/20000 Training Loss: 0.08336412161588669\n",
      "Epoch 439/20000 Training Loss: 0.08374893665313721\n",
      "Epoch 440/20000 Training Loss: 0.07099553197622299\n",
      "Epoch 440/20000 Validation Loss: 0.06411202251911163\n",
      "Epoch 441/20000 Training Loss: 0.06137414649128914\n",
      "Epoch 442/20000 Training Loss: 0.06273692101240158\n",
      "Epoch 443/20000 Training Loss: 0.046366069465875626\n",
      "Epoch 444/20000 Training Loss: 0.06809302419424057\n",
      "Epoch 445/20000 Training Loss: 0.05911196395754814\n",
      "Epoch 446/20000 Training Loss: 0.05036177113652229\n",
      "Epoch 447/20000 Training Loss: 0.040218207985162735\n",
      "Epoch 448/20000 Training Loss: 0.047037288546562195\n",
      "Epoch 449/20000 Training Loss: 0.06311047822237015\n",
      "Epoch 450/20000 Training Loss: 0.07203642278909683\n",
      "Epoch 450/20000 Validation Loss: 0.06795098632574081\n",
      "Epoch 451/20000 Training Loss: 0.05218048393726349\n",
      "Epoch 452/20000 Training Loss: 0.08031855523586273\n",
      "Epoch 453/20000 Training Loss: 0.08678925782442093\n",
      "Epoch 454/20000 Training Loss: 0.04686320945620537\n",
      "Epoch 455/20000 Training Loss: 0.05048908293247223\n",
      "Epoch 456/20000 Training Loss: 0.04845963418483734\n",
      "Epoch 457/20000 Training Loss: 0.07549270242452621\n",
      "Epoch 458/20000 Training Loss: 0.061037104576826096\n",
      "Epoch 459/20000 Training Loss: 0.05062501132488251\n",
      "Epoch 460/20000 Training Loss: 0.06814972311258316\n",
      "Epoch 460/20000 Validation Loss: 0.04142702743411064\n",
      "Epoch 461/20000 Training Loss: 0.053966786712408066\n",
      "Epoch 462/20000 Training Loss: 0.05306646600365639\n",
      "Epoch 463/20000 Training Loss: 0.06205807253718376\n",
      "Epoch 464/20000 Training Loss: 0.06732185930013657\n",
      "Epoch 465/20000 Training Loss: 0.0757831335067749\n",
      "Epoch 466/20000 Training Loss: 0.07920823246240616\n",
      "Epoch 467/20000 Training Loss: 0.053294647485017776\n",
      "Epoch 468/20000 Training Loss: 0.06019221618771553\n",
      "Epoch 469/20000 Training Loss: 0.05723527446389198\n",
      "Epoch 470/20000 Training Loss: 0.09282981604337692\n",
      "Epoch 470/20000 Validation Loss: 0.06013747304677963\n",
      "Epoch 471/20000 Training Loss: 0.07486537098884583\n",
      "Epoch 472/20000 Training Loss: 0.06624635308980942\n",
      "Epoch 473/20000 Training Loss: 0.05932311341166496\n",
      "Epoch 474/20000 Training Loss: 0.06785780191421509\n",
      "Epoch 475/20000 Training Loss: 0.0537855438888073\n",
      "Epoch 476/20000 Training Loss: 0.07399031519889832\n",
      "Epoch 477/20000 Training Loss: 0.05174635723233223\n",
      "Epoch 478/20000 Training Loss: 0.060811132192611694\n",
      "Epoch 479/20000 Training Loss: 0.05696500465273857\n",
      "Epoch 480/20000 Training Loss: 0.058897774666547775\n",
      "Epoch 480/20000 Validation Loss: 0.06288616359233856\n",
      "Epoch 481/20000 Training Loss: 0.05496107041835785\n",
      "Epoch 482/20000 Training Loss: 0.07066036760807037\n",
      "Epoch 483/20000 Training Loss: 0.05714549496769905\n",
      "Epoch 484/20000 Training Loss: 0.05530675873160362\n",
      "Epoch 485/20000 Training Loss: 0.06162084639072418\n",
      "Epoch 486/20000 Training Loss: 0.051754534244537354\n",
      "Epoch 487/20000 Training Loss: 0.06080205738544464\n",
      "Epoch 488/20000 Training Loss: 0.053530704230070114\n",
      "Epoch 489/20000 Training Loss: 0.06085636094212532\n",
      "Epoch 490/20000 Training Loss: 0.06280088424682617\n",
      "Epoch 490/20000 Validation Loss: 0.06698768585920334\n",
      "Epoch 491/20000 Training Loss: 0.054417192935943604\n",
      "Epoch 492/20000 Training Loss: 0.0549929141998291\n",
      "Epoch 493/20000 Training Loss: 0.07209005206823349\n",
      "Epoch 494/20000 Training Loss: 0.05948233604431152\n",
      "Epoch 495/20000 Training Loss: 0.05699409544467926\n",
      "Epoch 496/20000 Training Loss: 0.06373701244592667\n",
      "Epoch 497/20000 Training Loss: 0.061544019728899\n",
      "Epoch 498/20000 Training Loss: 0.05756859853863716\n",
      "Epoch 499/20000 Training Loss: 0.06630944460630417\n",
      "Epoch 500/20000 Training Loss: 0.04617920517921448\n",
      "Epoch 500/20000 Validation Loss: 0.040865086019039154\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.040865086019039154<=============\n",
      "Epoch 501/20000 Training Loss: 0.053411323577165604\n",
      "Epoch 502/20000 Training Loss: 0.053159911185503006\n",
      "Epoch 503/20000 Training Loss: 0.07495436817407608\n",
      "Epoch 504/20000 Training Loss: 0.045328136533498764\n",
      "Epoch 505/20000 Training Loss: 0.06570326536893845\n",
      "Epoch 506/20000 Training Loss: 0.0595029778778553\n",
      "Epoch 507/20000 Training Loss: 0.057869598269462585\n",
      "Epoch 508/20000 Training Loss: 0.05997185781598091\n",
      "Epoch 509/20000 Training Loss: 0.06602827459573746\n",
      "Epoch 510/20000 Training Loss: 0.04439844563603401\n",
      "Epoch 510/20000 Validation Loss: 0.0894588828086853\n",
      "Epoch 511/20000 Training Loss: 0.05040939524769783\n",
      "Epoch 512/20000 Training Loss: 0.05743036046624184\n",
      "Epoch 513/20000 Training Loss: 0.06093256548047066\n",
      "Epoch 514/20000 Training Loss: 0.0482570044696331\n",
      "Epoch 515/20000 Training Loss: 0.05950986221432686\n",
      "Epoch 516/20000 Training Loss: 0.06911701709032059\n",
      "Epoch 517/20000 Training Loss: 0.04766096547245979\n",
      "Epoch 518/20000 Training Loss: 0.038515355437994\n",
      "Epoch 519/20000 Training Loss: 0.06635996699333191\n",
      "Epoch 520/20000 Training Loss: 0.07106588035821915\n",
      "Epoch 520/20000 Validation Loss: 0.0637463852763176\n",
      "Epoch 521/20000 Training Loss: 0.04458532854914665\n",
      "Epoch 522/20000 Training Loss: 0.04365887865424156\n",
      "Epoch 523/20000 Training Loss: 0.07325946539640427\n",
      "Epoch 524/20000 Training Loss: 0.06496656686067581\n",
      "Epoch 525/20000 Training Loss: 0.05208773538470268\n",
      "Epoch 526/20000 Training Loss: 0.05275510624051094\n",
      "Epoch 527/20000 Training Loss: 0.07433595508337021\n",
      "Epoch 528/20000 Training Loss: 0.05961675941944122\n",
      "Epoch 529/20000 Training Loss: 0.06291293352842331\n",
      "Epoch 530/20000 Training Loss: 0.06676214188337326\n",
      "Epoch 530/20000 Validation Loss: 0.0486355796456337\n",
      "Epoch 531/20000 Training Loss: 0.06439977139234543\n",
      "Epoch 532/20000 Training Loss: 0.05856332182884216\n",
      "Epoch 533/20000 Training Loss: 0.0438266359269619\n",
      "Epoch 534/20000 Training Loss: 0.06248800829052925\n",
      "Epoch 535/20000 Training Loss: 0.04207189753651619\n",
      "Epoch 536/20000 Training Loss: 0.07343082875013351\n",
      "Epoch 537/20000 Training Loss: 0.08411461859941483\n",
      "Epoch 538/20000 Training Loss: 0.04720918834209442\n",
      "Epoch 539/20000 Training Loss: 0.06924567371606827\n",
      "Epoch 540/20000 Training Loss: 0.05172571539878845\n",
      "Epoch 540/20000 Validation Loss: 0.08236230164766312\n",
      "Epoch 541/20000 Training Loss: 0.07070958614349365\n",
      "Epoch 542/20000 Training Loss: 0.07635389268398285\n",
      "Epoch 543/20000 Training Loss: 0.04377976059913635\n",
      "Epoch 544/20000 Training Loss: 0.0636986568570137\n",
      "Epoch 545/20000 Training Loss: 0.06172063946723938\n",
      "Epoch 546/20000 Training Loss: 0.050764571875333786\n",
      "Epoch 547/20000 Training Loss: 0.060769543051719666\n",
      "Epoch 548/20000 Training Loss: 0.07207465171813965\n",
      "Epoch 549/20000 Training Loss: 0.06609577685594559\n",
      "Epoch 550/20000 Training Loss: 0.05762317776679993\n",
      "Epoch 550/20000 Validation Loss: 0.04772092401981354\n",
      "Epoch 551/20000 Training Loss: 0.06209602579474449\n",
      "Epoch 552/20000 Training Loss: 0.04749773442745209\n",
      "Epoch 553/20000 Training Loss: 0.07064250856637955\n",
      "Epoch 554/20000 Training Loss: 0.056227926164865494\n",
      "Epoch 555/20000 Training Loss: 0.05484214797616005\n",
      "Epoch 556/20000 Training Loss: 0.08098194748163223\n",
      "Epoch 557/20000 Training Loss: 0.05431600287556648\n",
      "Epoch 558/20000 Training Loss: 0.0690600574016571\n",
      "Epoch 559/20000 Training Loss: 0.05099399387836456\n",
      "Epoch 560/20000 Training Loss: 0.0708327665925026\n",
      "Epoch 560/20000 Validation Loss: 0.04482092335820198\n",
      "Epoch 561/20000 Training Loss: 0.053648144006729126\n",
      "Epoch 562/20000 Training Loss: 0.07012616842985153\n",
      "Epoch 563/20000 Training Loss: 0.05643102154135704\n",
      "Epoch 564/20000 Training Loss: 0.05065065622329712\n",
      "Epoch 565/20000 Training Loss: 0.07017654925584793\n",
      "Epoch 566/20000 Training Loss: 0.06577585637569427\n",
      "Epoch 567/20000 Training Loss: 0.06486333161592484\n",
      "Epoch 568/20000 Training Loss: 0.06174464151263237\n",
      "Epoch 569/20000 Training Loss: 0.06475818157196045\n",
      "Epoch 570/20000 Training Loss: 0.05378921702504158\n",
      "Epoch 570/20000 Validation Loss: 0.06181904301047325\n",
      "Epoch 571/20000 Training Loss: 0.05368461832404137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 572/20000 Training Loss: 0.04437221214175224\n",
      "Epoch 573/20000 Training Loss: 0.05275706574320793\n",
      "Epoch 574/20000 Training Loss: 0.052018702030181885\n",
      "Epoch 575/20000 Training Loss: 0.052246320992708206\n",
      "Epoch 576/20000 Training Loss: 0.056188762187957764\n",
      "Epoch 577/20000 Training Loss: 0.05100134015083313\n",
      "Epoch 578/20000 Training Loss: 0.06576292961835861\n",
      "Epoch 579/20000 Training Loss: 0.052438247948884964\n",
      "Epoch 580/20000 Training Loss: 0.0611291341483593\n",
      "Epoch 580/20000 Validation Loss: 0.05596209317445755\n",
      "Epoch 581/20000 Training Loss: 0.04596670717000961\n",
      "Epoch 582/20000 Training Loss: 0.05439689755439758\n",
      "Epoch 583/20000 Training Loss: 0.053175318986177444\n",
      "Epoch 584/20000 Training Loss: 0.05892421677708626\n",
      "Epoch 585/20000 Training Loss: 0.049850404262542725\n",
      "Epoch 586/20000 Training Loss: 0.0674389973282814\n",
      "Epoch 587/20000 Training Loss: 0.09323271363973618\n",
      "Epoch 588/20000 Training Loss: 0.04371761158108711\n",
      "Epoch 589/20000 Training Loss: 0.05255082622170448\n",
      "Epoch 590/20000 Training Loss: 0.058447014540433884\n",
      "Epoch 590/20000 Validation Loss: 0.07073325663805008\n",
      "Epoch 591/20000 Training Loss: 0.07853489369153976\n",
      "Epoch 592/20000 Training Loss: 0.06118343397974968\n",
      "Epoch 593/20000 Training Loss: 0.047051236033439636\n",
      "Epoch 594/20000 Training Loss: 0.057257700711488724\n",
      "Epoch 595/20000 Training Loss: 0.051137033849954605\n",
      "Epoch 596/20000 Training Loss: 0.055876318365335464\n",
      "Epoch 597/20000 Training Loss: 0.050515253096818924\n",
      "Epoch 598/20000 Training Loss: 0.06423646956682205\n",
      "Epoch 599/20000 Training Loss: 0.06192385032773018\n",
      "Epoch 600/20000 Training Loss: 0.05033883824944496\n",
      "Epoch 600/20000 Validation Loss: 0.06227631866931915\n",
      "Epoch 601/20000 Training Loss: 0.07683145999908447\n",
      "Epoch 602/20000 Training Loss: 0.06957430392503738\n",
      "Epoch 603/20000 Training Loss: 0.06033701077103615\n",
      "Epoch 604/20000 Training Loss: 0.0561373196542263\n",
      "Epoch 605/20000 Training Loss: 0.050390928983688354\n",
      "Epoch 606/20000 Training Loss: 0.04213673993945122\n",
      "Epoch 607/20000 Training Loss: 0.049663037061691284\n",
      "Epoch 608/20000 Training Loss: 0.05494287237524986\n",
      "Epoch 609/20000 Training Loss: 0.06058558449149132\n",
      "Epoch 610/20000 Training Loss: 0.07165820151567459\n",
      "Epoch 610/20000 Validation Loss: 0.07072443515062332\n",
      "Epoch 611/20000 Training Loss: 0.06047695502638817\n",
      "Epoch 612/20000 Training Loss: 0.05956264212727547\n",
      "Epoch 613/20000 Training Loss: 0.07720663398504257\n",
      "Epoch 614/20000 Training Loss: 0.06543009728193283\n",
      "Epoch 615/20000 Training Loss: 0.07383228838443756\n",
      "Epoch 616/20000 Training Loss: 0.04039548709988594\n",
      "Epoch 617/20000 Training Loss: 0.06155599653720856\n",
      "Epoch 618/20000 Training Loss: 0.0450102798640728\n",
      "Epoch 619/20000 Training Loss: 0.06269929558038712\n",
      "Epoch 620/20000 Training Loss: 0.05683712288737297\n",
      "Epoch 620/20000 Validation Loss: 0.06134095788002014\n",
      "Epoch 621/20000 Training Loss: 0.05157117173075676\n",
      "Epoch 622/20000 Training Loss: 0.04170013591647148\n",
      "Epoch 623/20000 Training Loss: 0.06146979331970215\n",
      "Epoch 624/20000 Training Loss: 0.049335774034261703\n",
      "Epoch 625/20000 Training Loss: 0.05924763157963753\n",
      "Epoch 626/20000 Training Loss: 0.046297550201416016\n",
      "Epoch 627/20000 Training Loss: 0.053830962628126144\n",
      "Epoch 628/20000 Training Loss: 0.05986329913139343\n",
      "Epoch 629/20000 Training Loss: 0.053804267197847366\n",
      "Epoch 630/20000 Training Loss: 0.05060267075896263\n",
      "Epoch 630/20000 Validation Loss: 0.03603939712047577\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03603939712047577<=============\n",
      "Epoch 631/20000 Training Loss: 0.07098294049501419\n",
      "Epoch 632/20000 Training Loss: 0.06870218366384506\n",
      "Epoch 633/20000 Training Loss: 0.060263410210609436\n",
      "Epoch 634/20000 Training Loss: 0.06185976043343544\n",
      "Epoch 635/20000 Training Loss: 0.05803036317229271\n",
      "Epoch 636/20000 Training Loss: 0.048182208091020584\n",
      "Epoch 637/20000 Training Loss: 0.06885088980197906\n",
      "Epoch 638/20000 Training Loss: 0.059532035142183304\n",
      "Epoch 639/20000 Training Loss: 0.07876062393188477\n",
      "Epoch 640/20000 Training Loss: 0.0492904968559742\n",
      "Epoch 640/20000 Validation Loss: 0.0781361535191536\n",
      "Epoch 641/20000 Training Loss: 0.07027777284383774\n",
      "Epoch 642/20000 Training Loss: 0.06555960327386856\n",
      "Epoch 643/20000 Training Loss: 0.04913460090756416\n",
      "Epoch 644/20000 Training Loss: 0.06343439966440201\n",
      "Epoch 645/20000 Training Loss: 0.06343720108270645\n",
      "Epoch 646/20000 Training Loss: 0.0738944336771965\n",
      "Epoch 647/20000 Training Loss: 0.09399542957544327\n",
      "Epoch 648/20000 Training Loss: 0.07241382449865341\n",
      "Epoch 649/20000 Training Loss: 0.06807460635900497\n",
      "Epoch 650/20000 Training Loss: 0.06507862359285355\n",
      "Epoch 650/20000 Validation Loss: 0.0441780686378479\n",
      "Epoch 651/20000 Training Loss: 0.058777838945388794\n",
      "Epoch 652/20000 Training Loss: 0.07299154996871948\n",
      "Epoch 653/20000 Training Loss: 0.05385443568229675\n",
      "Epoch 654/20000 Training Loss: 0.06652385741472244\n",
      "Epoch 655/20000 Training Loss: 0.0698753073811531\n",
      "Epoch 656/20000 Training Loss: 0.06809928268194199\n",
      "Epoch 657/20000 Training Loss: 0.05444984510540962\n",
      "Epoch 658/20000 Training Loss: 0.047784000635147095\n",
      "Epoch 659/20000 Training Loss: 0.05242421105504036\n",
      "Epoch 660/20000 Training Loss: 0.07975244522094727\n",
      "Epoch 660/20000 Validation Loss: 0.06442441791296005\n",
      "Epoch 661/20000 Training Loss: 0.04665212333202362\n",
      "Epoch 662/20000 Training Loss: 0.07725565880537033\n",
      "Epoch 663/20000 Training Loss: 0.05879877880215645\n",
      "Epoch 664/20000 Training Loss: 0.04901345074176788\n",
      "Epoch 665/20000 Training Loss: 0.048817407339811325\n",
      "Epoch 666/20000 Training Loss: 0.042642999440431595\n",
      "Epoch 667/20000 Training Loss: 0.058406978845596313\n",
      "Epoch 668/20000 Training Loss: 0.0738484337925911\n",
      "Epoch 669/20000 Training Loss: 0.0700346902012825\n",
      "Epoch 670/20000 Training Loss: 0.06673745065927505\n",
      "Epoch 670/20000 Validation Loss: 0.054743118584156036\n",
      "Epoch 671/20000 Training Loss: 0.0706697329878807\n",
      "Epoch 672/20000 Training Loss: 0.05507572367787361\n",
      "Epoch 673/20000 Training Loss: 0.046361640095710754\n",
      "Epoch 674/20000 Training Loss: 0.08013179898262024\n",
      "Epoch 675/20000 Training Loss: 0.0690089762210846\n",
      "Epoch 676/20000 Training Loss: 0.037837687879800797\n",
      "Epoch 677/20000 Training Loss: 0.055559635162353516\n",
      "Epoch 678/20000 Training Loss: 0.07113770395517349\n",
      "Epoch 679/20000 Training Loss: 0.03850913420319557\n",
      "Epoch 680/20000 Training Loss: 0.060413870960474014\n",
      "Epoch 680/20000 Validation Loss: 0.06796859204769135\n",
      "Epoch 681/20000 Training Loss: 0.07077571749687195\n",
      "Epoch 682/20000 Training Loss: 0.06763304024934769\n",
      "Epoch 683/20000 Training Loss: 0.06393153220415115\n",
      "Epoch 684/20000 Training Loss: 0.06612638384103775\n",
      "Epoch 685/20000 Training Loss: 0.05654279887676239\n",
      "Epoch 686/20000 Training Loss: 0.05558713898062706\n",
      "Epoch 687/20000 Training Loss: 0.057755809277296066\n",
      "Epoch 688/20000 Training Loss: 0.05118657648563385\n",
      "Epoch 689/20000 Training Loss: 0.0567866675555706\n",
      "Epoch 690/20000 Training Loss: 0.06444046646356583\n",
      "Epoch 690/20000 Validation Loss: 0.06990094482898712\n",
      "Epoch 691/20000 Training Loss: 0.048842281103134155\n",
      "Epoch 692/20000 Training Loss: 0.049319129437208176\n",
      "Epoch 693/20000 Training Loss: 0.06741482019424438\n",
      "Epoch 694/20000 Training Loss: 0.06806377321481705\n",
      "Epoch 695/20000 Training Loss: 0.046145882457494736\n",
      "Epoch 696/20000 Training Loss: 0.06039373204112053\n",
      "Epoch 697/20000 Training Loss: 0.059827402234077454\n",
      "Epoch 698/20000 Training Loss: 0.0802854374051094\n",
      "Epoch 699/20000 Training Loss: 0.05781017243862152\n",
      "Epoch 700/20000 Training Loss: 0.05582888796925545\n",
      "Epoch 700/20000 Validation Loss: 0.05186424404382706\n",
      "Epoch 701/20000 Training Loss: 0.054174646735191345\n",
      "Epoch 702/20000 Training Loss: 0.05208964645862579\n",
      "Epoch 703/20000 Training Loss: 0.06576671451330185\n",
      "Epoch 704/20000 Training Loss: 0.05423596873879433\n",
      "Epoch 705/20000 Training Loss: 0.05190145969390869\n",
      "Epoch 706/20000 Training Loss: 0.05615628883242607\n",
      "Epoch 707/20000 Training Loss: 0.060383740812540054\n",
      "Epoch 708/20000 Training Loss: 0.07621362060308456\n",
      "Epoch 709/20000 Training Loss: 0.06673053652048111\n",
      "Epoch 710/20000 Training Loss: 0.08209969848394394\n",
      "Epoch 710/20000 Validation Loss: 0.06937859952449799\n",
      "Epoch 711/20000 Training Loss: 0.06820788234472275\n",
      "Epoch 712/20000 Training Loss: 0.06330164521932602\n",
      "Epoch 713/20000 Training Loss: 0.06080098822712898\n",
      "Epoch 714/20000 Training Loss: 0.04510319232940674\n",
      "Epoch 715/20000 Training Loss: 0.0650448128581047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 716/20000 Training Loss: 0.05653974786400795\n",
      "Epoch 717/20000 Training Loss: 0.039458975195884705\n",
      "Epoch 718/20000 Training Loss: 0.05489277467131615\n",
      "Epoch 719/20000 Training Loss: 0.05957925319671631\n",
      "Epoch 720/20000 Training Loss: 0.06477680802345276\n",
      "Epoch 720/20000 Validation Loss: 0.05692058429121971\n",
      "Epoch 721/20000 Training Loss: 0.05989822372794151\n",
      "Epoch 722/20000 Training Loss: 0.0707993134856224\n",
      "Epoch 723/20000 Training Loss: 0.04984655976295471\n",
      "Epoch 724/20000 Training Loss: 0.06215667724609375\n",
      "Epoch 725/20000 Training Loss: 0.05573248863220215\n",
      "Epoch 726/20000 Training Loss: 0.0684329941868782\n",
      "Epoch 727/20000 Training Loss: 0.06428711861371994\n",
      "Epoch 728/20000 Training Loss: 0.07202750444412231\n",
      "Epoch 729/20000 Training Loss: 0.07615666836500168\n",
      "Epoch 730/20000 Training Loss: 0.06815305352210999\n",
      "Epoch 730/20000 Validation Loss: 0.05747171491384506\n",
      "Epoch 731/20000 Training Loss: 0.0639386996626854\n",
      "Epoch 732/20000 Training Loss: 0.05910976603627205\n",
      "Epoch 733/20000 Training Loss: 0.054619789123535156\n",
      "Epoch 734/20000 Training Loss: 0.05467359349131584\n",
      "Epoch 735/20000 Training Loss: 0.05002272129058838\n",
      "Epoch 736/20000 Training Loss: 0.055919114500284195\n",
      "Epoch 737/20000 Training Loss: 0.09380433708429337\n",
      "Epoch 738/20000 Training Loss: 0.05112980678677559\n",
      "Epoch 739/20000 Training Loss: 0.043753620237112045\n",
      "Epoch 740/20000 Training Loss: 0.04733797535300255\n",
      "Epoch 740/20000 Validation Loss: 0.06466074287891388\n",
      "Epoch 741/20000 Training Loss: 0.05085841193795204\n",
      "Epoch 742/20000 Training Loss: 0.06905612349510193\n",
      "Epoch 743/20000 Training Loss: 0.09243679791688919\n",
      "Epoch 744/20000 Training Loss: 0.058448802679777145\n",
      "Epoch 745/20000 Training Loss: 0.05680699273943901\n",
      "Epoch 746/20000 Training Loss: 0.05679647997021675\n",
      "Epoch 747/20000 Training Loss: 0.05165879800915718\n",
      "Epoch 748/20000 Training Loss: 0.0958007350564003\n",
      "Epoch 749/20000 Training Loss: 0.05546071007847786\n",
      "Epoch 750/20000 Training Loss: 0.046526480466127396\n",
      "Epoch 750/20000 Validation Loss: 0.06596590578556061\n",
      "Epoch 751/20000 Training Loss: 0.0606834776699543\n",
      "Epoch 752/20000 Training Loss: 0.04297126457095146\n",
      "Epoch 753/20000 Training Loss: 0.0539374053478241\n",
      "Epoch 754/20000 Training Loss: 0.04559536650776863\n",
      "Epoch 755/20000 Training Loss: 0.06520071625709534\n",
      "Epoch 756/20000 Training Loss: 0.04615195468068123\n",
      "Epoch 757/20000 Training Loss: 0.05702173709869385\n",
      "Epoch 758/20000 Training Loss: 0.0467950813472271\n",
      "Epoch 759/20000 Training Loss: 0.06632537394762039\n",
      "Epoch 760/20000 Training Loss: 0.05658026412129402\n",
      "Epoch 760/20000 Validation Loss: 0.0790819600224495\n",
      "Epoch 761/20000 Training Loss: 0.05459308624267578\n",
      "Epoch 762/20000 Training Loss: 0.05120357498526573\n",
      "Epoch 763/20000 Training Loss: 0.058193888515233994\n",
      "Epoch 764/20000 Training Loss: 0.06410431116819382\n",
      "Epoch 765/20000 Training Loss: 0.06387543678283691\n",
      "Epoch 766/20000 Training Loss: 0.05751274898648262\n",
      "Epoch 767/20000 Training Loss: 0.04779308661818504\n",
      "Epoch 768/20000 Training Loss: 0.05853588506579399\n",
      "Epoch 769/20000 Training Loss: 0.04950651526451111\n",
      "Epoch 770/20000 Training Loss: 0.05475678667426109\n",
      "Epoch 770/20000 Validation Loss: 0.08328250795602798\n",
      "Epoch 771/20000 Training Loss: 0.05119830369949341\n",
      "Epoch 772/20000 Training Loss: 0.0695202574133873\n",
      "Epoch 773/20000 Training Loss: 0.05918240547180176\n",
      "Epoch 774/20000 Training Loss: 0.048564013093709946\n",
      "Epoch 775/20000 Training Loss: 0.06624859571456909\n",
      "Epoch 776/20000 Training Loss: 0.0643552616238594\n",
      "Epoch 777/20000 Training Loss: 0.05841537192463875\n",
      "Epoch 778/20000 Training Loss: 0.0479515977203846\n",
      "Epoch 779/20000 Training Loss: 0.050946418195962906\n",
      "Epoch 780/20000 Training Loss: 0.06764329969882965\n",
      "Epoch 780/20000 Validation Loss: 0.05061185359954834\n",
      "Epoch 781/20000 Training Loss: 0.058618634939193726\n",
      "Epoch 782/20000 Training Loss: 0.058074742555618286\n",
      "Epoch 783/20000 Training Loss: 0.0630536898970604\n",
      "Epoch 784/20000 Training Loss: 0.06274378299713135\n",
      "Epoch 785/20000 Training Loss: 0.06559740751981735\n",
      "Epoch 786/20000 Training Loss: 0.06648697704076767\n",
      "Epoch 787/20000 Training Loss: 0.0488516241312027\n",
      "Epoch 788/20000 Training Loss: 0.05689364671707153\n",
      "Epoch 789/20000 Training Loss: 0.04744391515851021\n",
      "Epoch 790/20000 Training Loss: 0.07526252418756485\n",
      "Epoch 790/20000 Validation Loss: 0.06692806631326675\n",
      "Epoch 791/20000 Training Loss: 0.07325059175491333\n",
      "Epoch 792/20000 Training Loss: 0.047055233269929886\n",
      "Epoch 793/20000 Training Loss: 0.07289469987154007\n",
      "Epoch 794/20000 Training Loss: 0.04755334183573723\n",
      "Epoch 795/20000 Training Loss: 0.06730636209249496\n",
      "Epoch 796/20000 Training Loss: 0.05651114508509636\n",
      "Epoch 797/20000 Training Loss: 0.06872888654470444\n",
      "Epoch 798/20000 Training Loss: 0.07765630632638931\n",
      "Epoch 799/20000 Training Loss: 0.06492931395769119\n",
      "Epoch 800/20000 Training Loss: 0.048876870423555374\n",
      "Epoch 800/20000 Validation Loss: 0.05532413721084595\n",
      "Epoch 801/20000 Training Loss: 0.050397515296936035\n",
      "Epoch 802/20000 Training Loss: 0.06503655761480331\n",
      "Epoch 803/20000 Training Loss: 0.05191244184970856\n",
      "Epoch 804/20000 Training Loss: 0.05705728009343147\n",
      "Epoch 805/20000 Training Loss: 0.0498749278485775\n",
      "Epoch 806/20000 Training Loss: 0.0668618306517601\n",
      "Epoch 807/20000 Training Loss: 0.06102803722023964\n",
      "Epoch 808/20000 Training Loss: 0.0583263523876667\n",
      "Epoch 809/20000 Training Loss: 0.052612677216529846\n",
      "Epoch 810/20000 Training Loss: 0.05853345990180969\n",
      "Epoch 810/20000 Validation Loss: 0.042814746499061584\n",
      "Epoch 811/20000 Training Loss: 0.06504017114639282\n",
      "Epoch 812/20000 Training Loss: 0.05149953439831734\n",
      "Epoch 813/20000 Training Loss: 0.07435472309589386\n",
      "Epoch 814/20000 Training Loss: 0.060120031237602234\n",
      "Epoch 815/20000 Training Loss: 0.07103114575147629\n",
      "Epoch 816/20000 Training Loss: 0.058141518384218216\n",
      "Epoch 817/20000 Training Loss: 0.07193882018327713\n",
      "Epoch 818/20000 Training Loss: 0.06595111638307571\n",
      "Epoch 819/20000 Training Loss: 0.05744485929608345\n",
      "Epoch 820/20000 Training Loss: 0.08455643057823181\n",
      "Epoch 820/20000 Validation Loss: 0.06673505902290344\n",
      "Epoch 821/20000 Training Loss: 0.06362829357385635\n",
      "Epoch 822/20000 Training Loss: 0.05602346733212471\n",
      "Epoch 823/20000 Training Loss: 0.06566007435321808\n",
      "Epoch 824/20000 Training Loss: 0.06722257286310196\n",
      "Epoch 825/20000 Training Loss: 0.06278092414140701\n",
      "Epoch 826/20000 Training Loss: 0.07992821931838989\n",
      "Epoch 827/20000 Training Loss: 0.06098411977291107\n",
      "Epoch 828/20000 Training Loss: 0.07145906239748001\n",
      "Epoch 829/20000 Training Loss: 0.061489760875701904\n",
      "Epoch 830/20000 Training Loss: 0.06404077261686325\n",
      "Epoch 830/20000 Validation Loss: 0.053396422415971756\n",
      "Epoch 831/20000 Training Loss: 0.07092753797769547\n",
      "Epoch 832/20000 Training Loss: 0.06169972941279411\n",
      "Epoch 833/20000 Training Loss: 0.08501983433961868\n",
      "Epoch 834/20000 Training Loss: 0.055417727679014206\n",
      "Epoch 835/20000 Training Loss: 0.08884648233652115\n",
      "Epoch 836/20000 Training Loss: 0.07334744185209274\n",
      "Epoch 837/20000 Training Loss: 0.08007737994194031\n",
      "Epoch 838/20000 Training Loss: 0.05028850957751274\n",
      "Epoch 839/20000 Training Loss: 0.07177617400884628\n",
      "Epoch 840/20000 Training Loss: 0.05278654396533966\n",
      "Epoch 840/20000 Validation Loss: 0.040553003549575806\n",
      "Epoch 841/20000 Training Loss: 0.05012565851211548\n",
      "Epoch 842/20000 Training Loss: 0.08054929226636887\n",
      "Epoch 843/20000 Training Loss: 0.058204714208841324\n",
      "Epoch 844/20000 Training Loss: 0.06919427961111069\n",
      "Epoch 845/20000 Training Loss: 0.07665225118398666\n",
      "Epoch 846/20000 Training Loss: 0.0482746958732605\n",
      "Epoch 847/20000 Training Loss: 0.05993576720356941\n",
      "Epoch 848/20000 Training Loss: 0.07376117259263992\n",
      "Epoch 849/20000 Training Loss: 0.06902724504470825\n",
      "Epoch 850/20000 Training Loss: 0.059112489223480225\n",
      "Epoch 850/20000 Validation Loss: 0.05657058581709862\n",
      "Epoch 851/20000 Training Loss: 0.06171535328030586\n",
      "Epoch 852/20000 Training Loss: 0.05764785408973694\n",
      "Epoch 853/20000 Training Loss: 0.06833598017692566\n",
      "Epoch 854/20000 Training Loss: 0.04860568046569824\n",
      "Epoch 855/20000 Training Loss: 0.05701441690325737\n",
      "Epoch 856/20000 Training Loss: 0.08090653270483017\n",
      "Epoch 857/20000 Training Loss: 0.05741766095161438\n",
      "Epoch 858/20000 Training Loss: 0.06596466153860092\n",
      "Epoch 859/20000 Training Loss: 0.08194165676832199\n",
      "Epoch 860/20000 Training Loss: 0.05242432653903961\n",
      "Epoch 860/20000 Validation Loss: 0.10296514630317688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 861/20000 Training Loss: 0.049381595104932785\n",
      "Epoch 862/20000 Training Loss: 0.0560428760945797\n",
      "Epoch 863/20000 Training Loss: 0.054593276232481\n",
      "Epoch 864/20000 Training Loss: 0.06496907025575638\n",
      "Epoch 865/20000 Training Loss: 0.05421366170048714\n",
      "Epoch 866/20000 Training Loss: 0.0559610016644001\n",
      "Epoch 867/20000 Training Loss: 0.04450853168964386\n",
      "Epoch 868/20000 Training Loss: 0.06210927665233612\n",
      "Epoch 869/20000 Training Loss: 0.05089498683810234\n",
      "Epoch 870/20000 Training Loss: 0.06070155277848244\n",
      "Epoch 870/20000 Validation Loss: 0.06920398771762848\n",
      "Epoch 871/20000 Training Loss: 0.06763500720262527\n",
      "Epoch 872/20000 Training Loss: 0.07451780140399933\n",
      "Epoch 873/20000 Training Loss: 0.05636277794837952\n",
      "Epoch 874/20000 Training Loss: 0.055268626660108566\n",
      "Epoch 875/20000 Training Loss: 0.07025786489248276\n",
      "Epoch 876/20000 Training Loss: 0.0570591576397419\n",
      "Epoch 877/20000 Training Loss: 0.059114158153533936\n",
      "Epoch 878/20000 Training Loss: 0.07821246236562729\n",
      "Epoch 879/20000 Training Loss: 0.047357019037008286\n",
      "Epoch 880/20000 Training Loss: 0.0574059896171093\n",
      "Epoch 880/20000 Validation Loss: 0.05590806528925896\n",
      "Epoch 881/20000 Training Loss: 0.06919535994529724\n",
      "Epoch 882/20000 Training Loss: 0.05230216309428215\n",
      "Epoch 883/20000 Training Loss: 0.048131417483091354\n",
      "Epoch 884/20000 Training Loss: 0.06024767830967903\n",
      "Epoch 885/20000 Training Loss: 0.06965753436088562\n",
      "Epoch 886/20000 Training Loss: 0.06314414739608765\n",
      "Epoch 887/20000 Training Loss: 0.051109205931425095\n",
      "Epoch 888/20000 Training Loss: 0.06933612376451492\n",
      "Epoch 889/20000 Training Loss: 0.06218120455741882\n",
      "Epoch 890/20000 Training Loss: 0.06219713017344475\n",
      "Epoch 890/20000 Validation Loss: 0.05624862015247345\n",
      "Epoch 891/20000 Training Loss: 0.05623903498053551\n",
      "Epoch 892/20000 Training Loss: 0.06341450661420822\n",
      "Epoch 893/20000 Training Loss: 0.05892235040664673\n",
      "Epoch 894/20000 Training Loss: 0.05433446168899536\n",
      "Epoch 895/20000 Training Loss: 0.057234372943639755\n",
      "Epoch 896/20000 Training Loss: 0.05328591540455818\n",
      "Epoch 897/20000 Training Loss: 0.06785986572504044\n",
      "Epoch 898/20000 Training Loss: 0.061950426548719406\n",
      "Epoch 899/20000 Training Loss: 0.058393243700265884\n",
      "Epoch 900/20000 Training Loss: 0.0632077232003212\n",
      "Epoch 900/20000 Validation Loss: 0.047254715114831924\n",
      "Epoch 901/20000 Training Loss: 0.04807393252849579\n",
      "Epoch 902/20000 Training Loss: 0.05067110434174538\n",
      "Epoch 903/20000 Training Loss: 0.06141653284430504\n",
      "Epoch 904/20000 Training Loss: 0.0751023143529892\n",
      "Epoch 905/20000 Training Loss: 0.06817594915628433\n",
      "Epoch 906/20000 Training Loss: 0.05437063053250313\n",
      "Epoch 907/20000 Training Loss: 0.05563635006546974\n",
      "Epoch 908/20000 Training Loss: 0.054188087582588196\n",
      "Epoch 909/20000 Training Loss: 0.04182763770222664\n",
      "Epoch 910/20000 Training Loss: 0.07380721718072891\n",
      "Epoch 910/20000 Validation Loss: 0.05716584250330925\n",
      "Epoch 911/20000 Training Loss: 0.0735936164855957\n",
      "Epoch 912/20000 Training Loss: 0.055263739079236984\n",
      "Epoch 913/20000 Training Loss: 0.06264728307723999\n",
      "Epoch 914/20000 Training Loss: 0.041642602533102036\n",
      "Epoch 915/20000 Training Loss: 0.05824695900082588\n",
      "Epoch 916/20000 Training Loss: 0.047644078731536865\n",
      "Epoch 917/20000 Training Loss: 0.050490643829107285\n",
      "Epoch 918/20000 Training Loss: 0.05441255867481232\n",
      "Epoch 919/20000 Training Loss: 0.08167007565498352\n",
      "Epoch 920/20000 Training Loss: 0.07176726311445236\n",
      "Epoch 920/20000 Validation Loss: 0.06473137438297272\n",
      "Epoch 921/20000 Training Loss: 0.050493787974119186\n",
      "Epoch 922/20000 Training Loss: 0.057257357984781265\n",
      "Epoch 923/20000 Training Loss: 0.05689096823334694\n",
      "Epoch 924/20000 Training Loss: 0.06781313568353653\n",
      "Epoch 925/20000 Training Loss: 0.05872049927711487\n",
      "Epoch 926/20000 Training Loss: 0.06425508856773376\n",
      "Epoch 927/20000 Training Loss: 0.07605518400669098\n",
      "Epoch 928/20000 Training Loss: 0.054232072085142136\n",
      "Epoch 929/20000 Training Loss: 0.04581276699900627\n",
      "Epoch 930/20000 Training Loss: 0.06420552730560303\n",
      "Epoch 930/20000 Validation Loss: 0.05576447397470474\n",
      "Epoch 931/20000 Training Loss: 0.049072857946157455\n",
      "Epoch 932/20000 Training Loss: 0.05464230850338936\n",
      "Epoch 933/20000 Training Loss: 0.046423688530921936\n",
      "Epoch 934/20000 Training Loss: 0.06658809632062912\n",
      "Epoch 935/20000 Training Loss: 0.05991506204009056\n",
      "Epoch 936/20000 Training Loss: 0.0656517744064331\n",
      "Epoch 937/20000 Training Loss: 0.07966119050979614\n",
      "Epoch 938/20000 Training Loss: 0.07326527684926987\n",
      "Epoch 939/20000 Training Loss: 0.045223455876111984\n",
      "Epoch 940/20000 Training Loss: 0.08099231123924255\n",
      "Epoch 940/20000 Validation Loss: 0.05963984131813049\n",
      "Epoch 941/20000 Training Loss: 0.05043983459472656\n",
      "Epoch 942/20000 Training Loss: 0.07511498779058456\n",
      "Epoch 943/20000 Training Loss: 0.06445055454969406\n",
      "Epoch 944/20000 Training Loss: 0.05736403539776802\n",
      "Epoch 945/20000 Training Loss: 0.05797085538506508\n",
      "Epoch 946/20000 Training Loss: 0.060794588178396225\n",
      "Epoch 947/20000 Training Loss: 0.06123507395386696\n",
      "Epoch 948/20000 Training Loss: 0.0672464445233345\n",
      "Epoch 949/20000 Training Loss: 0.07097803801298141\n",
      "Epoch 950/20000 Training Loss: 0.07103430479764938\n",
      "Epoch 950/20000 Validation Loss: 0.0468757189810276\n",
      "Epoch 951/20000 Training Loss: 0.052063826471567154\n",
      "Epoch 952/20000 Training Loss: 0.06396353989839554\n",
      "Epoch 953/20000 Training Loss: 0.10483744740486145\n",
      "Epoch 954/20000 Training Loss: 0.03969639912247658\n",
      "Epoch 955/20000 Training Loss: 0.05576898530125618\n",
      "Epoch 956/20000 Training Loss: 0.05458277836441994\n",
      "Epoch 957/20000 Training Loss: 0.04733066260814667\n",
      "Epoch 958/20000 Training Loss: 0.062387917190790176\n",
      "Epoch 959/20000 Training Loss: 0.05700182914733887\n",
      "Epoch 960/20000 Training Loss: 0.052203934639692307\n",
      "Epoch 960/20000 Validation Loss: 0.049623046070337296\n",
      "Epoch 961/20000 Training Loss: 0.06986967474222183\n",
      "Epoch 962/20000 Training Loss: 0.07330816239118576\n",
      "Epoch 963/20000 Training Loss: 0.0689496323466301\n",
      "Epoch 964/20000 Training Loss: 0.05345550552010536\n",
      "Epoch 965/20000 Training Loss: 0.05886758491396904\n",
      "Epoch 966/20000 Training Loss: 0.06143428012728691\n",
      "Epoch 967/20000 Training Loss: 0.04827921465039253\n",
      "Epoch 968/20000 Training Loss: 0.045759957283735275\n",
      "Epoch 969/20000 Training Loss: 0.06270400434732437\n",
      "Epoch 970/20000 Training Loss: 0.047056082636117935\n",
      "Epoch 970/20000 Validation Loss: 0.05456661060452461\n",
      "Epoch 971/20000 Training Loss: 0.07514532655477524\n",
      "Epoch 972/20000 Training Loss: 0.06136040762066841\n",
      "Epoch 973/20000 Training Loss: 0.04846770688891411\n",
      "Epoch 974/20000 Training Loss: 0.07643390446901321\n",
      "Epoch 975/20000 Training Loss: 0.05825132504105568\n",
      "Epoch 976/20000 Training Loss: 0.045659516006708145\n",
      "Epoch 977/20000 Training Loss: 0.07122013717889786\n",
      "Epoch 978/20000 Training Loss: 0.05834847688674927\n",
      "Epoch 979/20000 Training Loss: 0.0666256844997406\n",
      "Epoch 980/20000 Training Loss: 0.06401413679122925\n",
      "Epoch 980/20000 Validation Loss: 0.09378966689109802\n",
      "Epoch 981/20000 Training Loss: 0.05501226708292961\n",
      "Epoch 982/20000 Training Loss: 0.05026911571621895\n",
      "Epoch 983/20000 Training Loss: 0.05344889685511589\n",
      "Epoch 984/20000 Training Loss: 0.07776334136724472\n",
      "Epoch 985/20000 Training Loss: 0.06772959977388382\n",
      "Epoch 986/20000 Training Loss: 0.05370008572936058\n",
      "Epoch 987/20000 Training Loss: 0.052044931799173355\n",
      "Epoch 988/20000 Training Loss: 0.0760887935757637\n",
      "Epoch 989/20000 Training Loss: 0.05779298022389412\n",
      "Epoch 990/20000 Training Loss: 0.07791437953710556\n",
      "Epoch 990/20000 Validation Loss: 0.0533292256295681\n",
      "Epoch 991/20000 Training Loss: 0.04816597327589989\n",
      "Epoch 992/20000 Training Loss: 0.06043944135308266\n",
      "Epoch 993/20000 Training Loss: 0.05860038101673126\n",
      "Epoch 994/20000 Training Loss: 0.059024617075920105\n",
      "Epoch 995/20000 Training Loss: 0.05939589440822601\n",
      "Epoch 996/20000 Training Loss: 0.0502140074968338\n",
      "Epoch 997/20000 Training Loss: 0.06542579084634781\n",
      "Epoch 998/20000 Training Loss: 0.049334969371557236\n",
      "Epoch 999/20000 Training Loss: 0.05175064876675606\n",
      "Epoch 1000/20000 Training Loss: 0.05312660336494446\n",
      "Epoch 1000/20000 Validation Loss: 0.08307501673698425\n",
      "Epoch 1001/20000 Training Loss: 0.061962176114320755\n",
      "Epoch 1002/20000 Training Loss: 0.05898773670196533\n",
      "Epoch 1003/20000 Training Loss: 0.04195520281791687\n",
      "Epoch 1004/20000 Training Loss: 0.05371539667248726\n",
      "Epoch 1005/20000 Training Loss: 0.04367838799953461\n",
      "Epoch 1006/20000 Training Loss: 0.05747077241539955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1007/20000 Training Loss: 0.05490689352154732\n",
      "Epoch 1008/20000 Training Loss: 0.053388580679893494\n",
      "Epoch 1009/20000 Training Loss: 0.06487096846103668\n",
      "Epoch 1010/20000 Training Loss: 0.07632269710302353\n",
      "Epoch 1010/20000 Validation Loss: 0.06238604336977005\n",
      "Epoch 1011/20000 Training Loss: 0.06681882590055466\n",
      "Epoch 1012/20000 Training Loss: 0.06027470529079437\n",
      "Epoch 1013/20000 Training Loss: 0.04795066639780998\n",
      "Epoch 1014/20000 Training Loss: 0.06206811964511871\n",
      "Epoch 1015/20000 Training Loss: 0.0696806013584137\n",
      "Epoch 1016/20000 Training Loss: 0.07347691059112549\n",
      "Epoch 1017/20000 Training Loss: 0.0625394806265831\n",
      "Epoch 1018/20000 Training Loss: 0.06243365630507469\n",
      "Epoch 1019/20000 Training Loss: 0.06835425645112991\n",
      "Epoch 1020/20000 Training Loss: 0.07471846789121628\n",
      "Epoch 1020/20000 Validation Loss: 0.04952457174658775\n",
      "Epoch 1021/20000 Training Loss: 0.06180774047970772\n",
      "Epoch 1022/20000 Training Loss: 0.04640428349375725\n",
      "Epoch 1023/20000 Training Loss: 0.07895927876234055\n",
      "Epoch 1024/20000 Training Loss: 0.06272498518228531\n",
      "Epoch 1025/20000 Training Loss: 0.04976363852620125\n",
      "Epoch 1026/20000 Training Loss: 0.056598078459501266\n",
      "Epoch 1027/20000 Training Loss: 0.047517675906419754\n",
      "Epoch 1028/20000 Training Loss: 0.07107724994421005\n",
      "Epoch 1029/20000 Training Loss: 0.04385942220687866\n",
      "Epoch 1030/20000 Training Loss: 0.0546586811542511\n",
      "Epoch 1030/20000 Validation Loss: 0.0528906024992466\n",
      "Epoch 1031/20000 Training Loss: 0.07948891073465347\n",
      "Epoch 1032/20000 Training Loss: 0.05593288317322731\n",
      "Epoch 1033/20000 Training Loss: 0.07527781277894974\n",
      "Epoch 1034/20000 Training Loss: 0.05196794867515564\n",
      "Epoch 1035/20000 Training Loss: 0.04993867874145508\n",
      "Epoch 1036/20000 Training Loss: 0.053448718041181564\n",
      "Epoch 1037/20000 Training Loss: 0.06696075946092606\n",
      "Epoch 1038/20000 Training Loss: 0.058300454169511795\n",
      "Epoch 1039/20000 Training Loss: 0.05671985074877739\n",
      "Epoch 1040/20000 Training Loss: 0.05208176001906395\n",
      "Epoch 1040/20000 Validation Loss: 0.07088729739189148\n",
      "Epoch 1041/20000 Training Loss: 0.05735987424850464\n",
      "Epoch 1042/20000 Training Loss: 0.08264871686697006\n",
      "Epoch 1043/20000 Training Loss: 0.04825921729207039\n",
      "Epoch 1044/20000 Training Loss: 0.052678778767585754\n",
      "Epoch 1045/20000 Training Loss: 0.04958212748169899\n",
      "Epoch 1046/20000 Training Loss: 0.04817604646086693\n",
      "Epoch 1047/20000 Training Loss: 0.05134166404604912\n",
      "Epoch 1048/20000 Training Loss: 0.053625527769327164\n",
      "Epoch 1049/20000 Training Loss: 0.06421307474374771\n",
      "Epoch 1050/20000 Training Loss: 0.056562747806310654\n",
      "Epoch 1050/20000 Validation Loss: 0.0863618552684784\n",
      "Epoch 1051/20000 Training Loss: 0.07413368672132492\n",
      "Epoch 1052/20000 Training Loss: 0.0572284571826458\n",
      "Epoch 1053/20000 Training Loss: 0.048036783933639526\n",
      "Epoch 1054/20000 Training Loss: 0.07243850082159042\n",
      "Epoch 1055/20000 Training Loss: 0.0662241280078888\n",
      "Epoch 1056/20000 Training Loss: 0.06319654732942581\n",
      "Epoch 1057/20000 Training Loss: 0.05198743939399719\n",
      "Epoch 1058/20000 Training Loss: 0.08125127851963043\n",
      "Epoch 1059/20000 Training Loss: 0.05581381916999817\n",
      "Epoch 1060/20000 Training Loss: 0.05447506532073021\n",
      "Epoch 1060/20000 Validation Loss: 0.06538695096969604\n",
      "Epoch 1061/20000 Training Loss: 0.057735588401556015\n",
      "Epoch 1062/20000 Training Loss: 0.06118428707122803\n",
      "Epoch 1063/20000 Training Loss: 0.057274699211120605\n",
      "Epoch 1064/20000 Training Loss: 0.05277831479907036\n",
      "Epoch 1065/20000 Training Loss: 0.055443424731492996\n",
      "Epoch 1066/20000 Training Loss: 0.06337376683950424\n",
      "Epoch 1067/20000 Training Loss: 0.059100210666656494\n",
      "Epoch 1068/20000 Training Loss: 0.0709075927734375\n",
      "Epoch 1069/20000 Training Loss: 0.051796913146972656\n",
      "Epoch 1070/20000 Training Loss: 0.06073988601565361\n",
      "Epoch 1070/20000 Validation Loss: 0.052618443965911865\n",
      "Epoch 1071/20000 Training Loss: 0.06413591653108597\n",
      "Epoch 1072/20000 Training Loss: 0.055718302726745605\n",
      "Epoch 1073/20000 Training Loss: 0.051980599761009216\n",
      "Epoch 1074/20000 Training Loss: 0.05158281326293945\n",
      "Epoch 1075/20000 Training Loss: 0.05710341036319733\n",
      "Epoch 1076/20000 Training Loss: 0.06392629444599152\n",
      "Epoch 1077/20000 Training Loss: 0.05891774594783783\n",
      "Epoch 1078/20000 Training Loss: 0.05935654044151306\n",
      "Epoch 1079/20000 Training Loss: 0.06988469511270523\n",
      "Epoch 1080/20000 Training Loss: 0.0655263289809227\n",
      "Epoch 1080/20000 Validation Loss: 0.054601799696683884\n",
      "Epoch 1081/20000 Training Loss: 0.06764912605285645\n",
      "Epoch 1082/20000 Training Loss: 0.06344037503004074\n",
      "Epoch 1083/20000 Training Loss: 0.056956518441438675\n",
      "Epoch 1084/20000 Training Loss: 0.03559263423085213\n",
      "Epoch 1085/20000 Training Loss: 0.060853272676467896\n",
      "Epoch 1086/20000 Training Loss: 0.06163829565048218\n",
      "Epoch 1087/20000 Training Loss: 0.06411519646644592\n",
      "Epoch 1088/20000 Training Loss: 0.06772515177726746\n",
      "Epoch 1089/20000 Training Loss: 0.06301882117986679\n",
      "Epoch 1090/20000 Training Loss: 0.047671232372522354\n",
      "Epoch 1090/20000 Validation Loss: 0.049196019768714905\n",
      "Epoch 1091/20000 Training Loss: 0.04633694887161255\n",
      "Epoch 1092/20000 Training Loss: 0.046192869544029236\n",
      "Epoch 1093/20000 Training Loss: 0.0740325078368187\n",
      "Epoch 1094/20000 Training Loss: 0.05087935924530029\n",
      "Epoch 1095/20000 Training Loss: 0.04281695559620857\n",
      "Epoch 1096/20000 Training Loss: 0.07066226750612259\n",
      "Epoch 1097/20000 Training Loss: 0.059776272624731064\n",
      "Epoch 1098/20000 Training Loss: 0.06585212796926498\n",
      "Epoch 1099/20000 Training Loss: 0.05474649742245674\n",
      "Epoch 1100/20000 Training Loss: 0.059745948761701584\n",
      "Epoch 1100/20000 Validation Loss: 0.05402730405330658\n",
      "Epoch 1101/20000 Training Loss: 0.06315208226442337\n",
      "Epoch 1102/20000 Training Loss: 0.04623144492506981\n",
      "Epoch 1103/20000 Training Loss: 0.05436224117875099\n",
      "Epoch 1104/20000 Training Loss: 0.058590639382600784\n",
      "Epoch 1105/20000 Training Loss: 0.0719904899597168\n",
      "Epoch 1106/20000 Training Loss: 0.06883476674556732\n",
      "Epoch 1107/20000 Training Loss: 0.06684369593858719\n",
      "Epoch 1108/20000 Training Loss: 0.06190633773803711\n",
      "Epoch 1109/20000 Training Loss: 0.060426827520132065\n",
      "Epoch 1110/20000 Training Loss: 0.053142745047807693\n",
      "Epoch 1110/20000 Validation Loss: 0.05360716953873634\n",
      "Epoch 1111/20000 Training Loss: 0.0378282256424427\n",
      "Epoch 1112/20000 Training Loss: 0.06969885528087616\n",
      "Epoch 1113/20000 Training Loss: 0.05814389884471893\n",
      "Epoch 1114/20000 Training Loss: 0.08115630596876144\n",
      "Epoch 1115/20000 Training Loss: 0.05655243992805481\n",
      "Epoch 1116/20000 Training Loss: 0.054891765117645264\n",
      "Epoch 1117/20000 Training Loss: 0.06924879550933838\n",
      "Epoch 1118/20000 Training Loss: 0.05373527482151985\n",
      "Epoch 1119/20000 Training Loss: 0.05916379764676094\n",
      "Epoch 1120/20000 Training Loss: 0.04893946647644043\n",
      "Epoch 1120/20000 Validation Loss: 0.0765850618481636\n",
      "Epoch 1121/20000 Training Loss: 0.06842309236526489\n",
      "Epoch 1122/20000 Training Loss: 0.04473491385579109\n",
      "Epoch 1123/20000 Training Loss: 0.06338255852460861\n",
      "Epoch 1124/20000 Training Loss: 0.05709031596779823\n",
      "Epoch 1125/20000 Training Loss: 0.06590024381875992\n",
      "Epoch 1126/20000 Training Loss: 0.04729068651795387\n",
      "Epoch 1127/20000 Training Loss: 0.0563431940972805\n",
      "Epoch 1128/20000 Training Loss: 0.04760369285941124\n",
      "Epoch 1129/20000 Training Loss: 0.06455060094594955\n",
      "Epoch 1130/20000 Training Loss: 0.06252812594175339\n",
      "Epoch 1130/20000 Validation Loss: 0.09344740211963654\n",
      "Epoch 1131/20000 Training Loss: 0.0415637232363224\n",
      "Epoch 1132/20000 Training Loss: 0.050519783049821854\n",
      "Epoch 1133/20000 Training Loss: 0.07197549194097519\n",
      "Epoch 1134/20000 Training Loss: 0.05035741254687309\n",
      "Epoch 1135/20000 Training Loss: 0.07350567728281021\n",
      "Epoch 1136/20000 Training Loss: 0.06854227930307388\n",
      "Epoch 1137/20000 Training Loss: 0.04661025106906891\n",
      "Epoch 1138/20000 Training Loss: 0.06339964270591736\n",
      "Epoch 1139/20000 Training Loss: 0.0651862621307373\n",
      "Epoch 1140/20000 Training Loss: 0.08432687073945999\n",
      "Epoch 1140/20000 Validation Loss: 0.08599220216274261\n",
      "Epoch 1141/20000 Training Loss: 0.0548434741795063\n",
      "Epoch 1142/20000 Training Loss: 0.0652197077870369\n",
      "Epoch 1143/20000 Training Loss: 0.053804557770490646\n",
      "Epoch 1144/20000 Training Loss: 0.05624769628047943\n",
      "Epoch 1145/20000 Training Loss: 0.0704696774482727\n",
      "Epoch 1146/20000 Training Loss: 0.06316839903593063\n",
      "Epoch 1147/20000 Training Loss: 0.08149408549070358\n",
      "Epoch 1148/20000 Training Loss: 0.0833861455321312\n",
      "Epoch 1149/20000 Training Loss: 0.07075263559818268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1150/20000 Training Loss: 0.07603230327367783\n",
      "Epoch 1150/20000 Validation Loss: 0.04551326483488083\n",
      "Epoch 1151/20000 Training Loss: 0.07823941111564636\n",
      "Epoch 1152/20000 Training Loss: 0.050608035176992416\n",
      "Epoch 1153/20000 Training Loss: 0.06390293687582016\n",
      "Epoch 1154/20000 Training Loss: 0.07074271887540817\n",
      "Epoch 1155/20000 Training Loss: 0.05021025612950325\n",
      "Epoch 1156/20000 Training Loss: 0.04945090785622597\n",
      "Epoch 1157/20000 Training Loss: 0.04997783526778221\n",
      "Epoch 1158/20000 Training Loss: 0.05068858340382576\n",
      "Epoch 1159/20000 Training Loss: 0.05716453865170479\n",
      "Epoch 1160/20000 Training Loss: 0.0495886392891407\n",
      "Epoch 1160/20000 Validation Loss: 0.043587759137153625\n",
      "Epoch 1161/20000 Training Loss: 0.03829526901245117\n",
      "Epoch 1162/20000 Training Loss: 0.05460304394364357\n",
      "Epoch 1163/20000 Training Loss: 0.027867307886481285\n",
      "Epoch 1164/20000 Training Loss: 0.05958976969122887\n",
      "Epoch 1165/20000 Training Loss: 0.06124761700630188\n",
      "Epoch 1166/20000 Training Loss: 0.06492937356233597\n",
      "Epoch 1167/20000 Training Loss: 0.04878115653991699\n",
      "Epoch 1168/20000 Training Loss: 0.04696236178278923\n",
      "Epoch 1169/20000 Training Loss: 0.072646863758564\n",
      "Epoch 1170/20000 Training Loss: 0.059506457298994064\n",
      "Epoch 1170/20000 Validation Loss: 0.07079315930604935\n",
      "Epoch 1171/20000 Training Loss: 0.0772491917014122\n",
      "Epoch 1172/20000 Training Loss: 0.06537085026502609\n",
      "Epoch 1173/20000 Training Loss: 0.06837085634469986\n",
      "Epoch 1174/20000 Training Loss: 0.0644005835056305\n",
      "Epoch 1175/20000 Training Loss: 0.046397823840379715\n",
      "Epoch 1176/20000 Training Loss: 0.05573508143424988\n",
      "Epoch 1177/20000 Training Loss: 0.05859742686152458\n",
      "Epoch 1178/20000 Training Loss: 0.06207006052136421\n",
      "Epoch 1179/20000 Training Loss: 0.05878419056534767\n",
      "Epoch 1180/20000 Training Loss: 0.07524657994508743\n",
      "Epoch 1180/20000 Validation Loss: 0.06307168304920197\n",
      "Epoch 1181/20000 Training Loss: 0.049248840659856796\n",
      "Epoch 1182/20000 Training Loss: 0.07024826854467392\n",
      "Epoch 1183/20000 Training Loss: 0.056607674807310104\n",
      "Epoch 1184/20000 Training Loss: 0.06665947288274765\n",
      "Epoch 1185/20000 Training Loss: 0.04607066884636879\n",
      "Epoch 1186/20000 Training Loss: 0.06853405386209488\n",
      "Epoch 1187/20000 Training Loss: 0.05330305173993111\n",
      "Epoch 1188/20000 Training Loss: 0.0708923190832138\n",
      "Epoch 1189/20000 Training Loss: 0.06049766764044762\n",
      "Epoch 1190/20000 Training Loss: 0.06217882037162781\n",
      "Epoch 1190/20000 Validation Loss: 0.06372024118900299\n",
      "Epoch 1191/20000 Training Loss: 0.05058258771896362\n",
      "Epoch 1192/20000 Training Loss: 0.07995077222585678\n",
      "Epoch 1193/20000 Training Loss: 0.05407940223813057\n",
      "Epoch 1194/20000 Training Loss: 0.0636826828122139\n",
      "Epoch 1195/20000 Training Loss: 0.058828938752412796\n",
      "Epoch 1196/20000 Training Loss: 0.053605277091264725\n",
      "Epoch 1197/20000 Training Loss: 0.04448948800563812\n",
      "Epoch 1198/20000 Training Loss: 0.051994044333696365\n",
      "Epoch 1199/20000 Training Loss: 0.04949526861310005\n",
      "Epoch 1200/20000 Training Loss: 0.05133819952607155\n",
      "Epoch 1200/20000 Validation Loss: 0.07787741720676422\n",
      "Epoch 1201/20000 Training Loss: 0.06606481224298477\n",
      "Epoch 1202/20000 Training Loss: 0.055333126336336136\n",
      "Epoch 1203/20000 Training Loss: 0.05572660639882088\n",
      "Epoch 1204/20000 Training Loss: 0.055635854601860046\n",
      "Epoch 1205/20000 Training Loss: 0.057268958538770676\n",
      "Epoch 1206/20000 Training Loss: 0.06302300095558167\n",
      "Epoch 1207/20000 Training Loss: 0.0766213908791542\n",
      "Epoch 1208/20000 Training Loss: 0.060977041721343994\n",
      "Epoch 1209/20000 Training Loss: 0.0802205502986908\n",
      "Epoch 1210/20000 Training Loss: 0.061650052666664124\n",
      "Epoch 1210/20000 Validation Loss: 0.06163372844457626\n",
      "Epoch 1211/20000 Training Loss: 0.059049252420663834\n",
      "Epoch 1212/20000 Training Loss: 0.0584220290184021\n",
      "Epoch 1213/20000 Training Loss: 0.06510671228170395\n",
      "Epoch 1214/20000 Training Loss: 0.054477084428071976\n",
      "Epoch 1215/20000 Training Loss: 0.06200670078396797\n",
      "Epoch 1216/20000 Training Loss: 0.07099539786577225\n",
      "Epoch 1217/20000 Training Loss: 0.06669142842292786\n",
      "Epoch 1218/20000 Training Loss: 0.05060021951794624\n",
      "Epoch 1219/20000 Training Loss: 0.05510488152503967\n",
      "Epoch 1220/20000 Training Loss: 0.05083612725138664\n",
      "Epoch 1220/20000 Validation Loss: 0.05749218910932541\n",
      "Epoch 1221/20000 Training Loss: 0.06922797113656998\n",
      "Epoch 1222/20000 Training Loss: 0.05727292597293854\n",
      "Epoch 1223/20000 Training Loss: 0.06721656024456024\n",
      "Epoch 1224/20000 Training Loss: 0.0721493735909462\n",
      "Epoch 1225/20000 Training Loss: 0.07402031123638153\n",
      "Epoch 1226/20000 Training Loss: 0.08050211519002914\n",
      "Epoch 1227/20000 Training Loss: 0.06917551159858704\n",
      "Epoch 1228/20000 Training Loss: 0.06595621258020401\n",
      "Epoch 1229/20000 Training Loss: 0.06715529412031174\n",
      "Epoch 1230/20000 Training Loss: 0.039021849632263184\n",
      "Epoch 1230/20000 Validation Loss: 0.040027569979429245\n",
      "Epoch 1231/20000 Training Loss: 0.05861919745802879\n",
      "Epoch 1232/20000 Training Loss: 0.0632944405078888\n",
      "Epoch 1233/20000 Training Loss: 0.050376251339912415\n",
      "Epoch 1234/20000 Training Loss: 0.06961911916732788\n",
      "Epoch 1235/20000 Training Loss: 0.09055348485708237\n",
      "Epoch 1236/20000 Training Loss: 0.06719908118247986\n",
      "Epoch 1237/20000 Training Loss: 0.05574621632695198\n",
      "Epoch 1238/20000 Training Loss: 0.06582546979188919\n",
      "Epoch 1239/20000 Training Loss: 0.06665574759244919\n",
      "Epoch 1240/20000 Training Loss: 0.057912345975637436\n",
      "Epoch 1240/20000 Validation Loss: 0.04822736233472824\n",
      "Epoch 1241/20000 Training Loss: 0.05524560809135437\n",
      "Epoch 1242/20000 Training Loss: 0.04364461824297905\n",
      "Epoch 1243/20000 Training Loss: 0.05438767746090889\n",
      "Epoch 1244/20000 Training Loss: 0.08312393724918365\n",
      "Epoch 1245/20000 Training Loss: 0.056326691061258316\n",
      "Epoch 1246/20000 Training Loss: 0.05714565888047218\n",
      "Epoch 1247/20000 Training Loss: 0.05077330768108368\n",
      "Epoch 1248/20000 Training Loss: 0.048815879970788956\n",
      "Epoch 1249/20000 Training Loss: 0.06970115751028061\n",
      "Epoch 1250/20000 Training Loss: 0.05469433590769768\n",
      "Epoch 1250/20000 Validation Loss: 0.07923971116542816\n",
      "Epoch 1251/20000 Training Loss: 0.05684962868690491\n",
      "Epoch 1252/20000 Training Loss: 0.059772226959466934\n",
      "Epoch 1253/20000 Training Loss: 0.0526072196662426\n",
      "Epoch 1254/20000 Training Loss: 0.056168150156736374\n",
      "Epoch 1255/20000 Training Loss: 0.0629311129450798\n",
      "Epoch 1256/20000 Training Loss: 0.08782973140478134\n",
      "Epoch 1257/20000 Training Loss: 0.09944098442792892\n",
      "Epoch 1258/20000 Training Loss: 0.05872310325503349\n",
      "Epoch 1259/20000 Training Loss: 0.05061603710055351\n",
      "Epoch 1260/20000 Training Loss: 0.05207468569278717\n",
      "Epoch 1260/20000 Validation Loss: 0.06768259406089783\n",
      "Epoch 1261/20000 Training Loss: 0.05725596472620964\n",
      "Epoch 1262/20000 Training Loss: 0.0535489022731781\n",
      "Epoch 1263/20000 Training Loss: 0.07342731207609177\n",
      "Epoch 1264/20000 Training Loss: 0.07544911652803421\n",
      "Epoch 1265/20000 Training Loss: 0.0655546560883522\n",
      "Epoch 1266/20000 Training Loss: 0.05349257215857506\n",
      "Epoch 1267/20000 Training Loss: 0.058710623532533646\n",
      "Epoch 1268/20000 Training Loss: 0.06196119263768196\n",
      "Epoch 1269/20000 Training Loss: 0.05776515230536461\n",
      "Epoch 1270/20000 Training Loss: 0.0709766075015068\n",
      "Epoch 1270/20000 Validation Loss: 0.05444376543164253\n",
      "Epoch 1271/20000 Training Loss: 0.05279548093676567\n",
      "Epoch 1272/20000 Training Loss: 0.058033693581819534\n",
      "Epoch 1273/20000 Training Loss: 0.07338256388902664\n",
      "Epoch 1274/20000 Training Loss: 0.04881248250603676\n",
      "Epoch 1275/20000 Training Loss: 0.06932836025953293\n",
      "Epoch 1276/20000 Training Loss: 0.07173632085323334\n",
      "Epoch 1277/20000 Training Loss: 0.051567185670137405\n",
      "Epoch 1278/20000 Training Loss: 0.06777258962392807\n",
      "Epoch 1279/20000 Training Loss: 0.0554276704788208\n",
      "Epoch 1280/20000 Training Loss: 0.056572794914245605\n",
      "Epoch 1280/20000 Validation Loss: 0.08934056013822556\n",
      "Epoch 1281/20000 Training Loss: 0.05866510793566704\n",
      "Epoch 1282/20000 Training Loss: 0.05683976411819458\n",
      "Epoch 1283/20000 Training Loss: 0.06174459680914879\n",
      "Epoch 1284/20000 Training Loss: 0.05281727388501167\n",
      "Epoch 1285/20000 Training Loss: 0.054321106523275375\n",
      "Epoch 1286/20000 Training Loss: 0.0551220178604126\n",
      "Epoch 1287/20000 Training Loss: 0.06129912659525871\n",
      "Epoch 1288/20000 Training Loss: 0.07558318227529526\n",
      "Epoch 1289/20000 Training Loss: 0.07804503291845322\n",
      "Epoch 1290/20000 Training Loss: 0.04828190430998802\n",
      "Epoch 1290/20000 Validation Loss: 0.057080306112766266\n",
      "Epoch 1291/20000 Training Loss: 0.06726912409067154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1292/20000 Training Loss: 0.05806499719619751\n",
      "Epoch 1293/20000 Training Loss: 0.059733808040618896\n",
      "Epoch 1294/20000 Training Loss: 0.07938598841428757\n",
      "Epoch 1295/20000 Training Loss: 0.05480560287833214\n",
      "Epoch 1296/20000 Training Loss: 0.04936308041214943\n",
      "Epoch 1297/20000 Training Loss: 0.049247920513153076\n",
      "Epoch 1298/20000 Training Loss: 0.05271083116531372\n",
      "Epoch 1299/20000 Training Loss: 0.03613540902733803\n",
      "Epoch 1300/20000 Training Loss: 0.07585635036230087\n",
      "Epoch 1300/20000 Validation Loss: 0.0686255469918251\n",
      "Epoch 1301/20000 Training Loss: 0.061500903218984604\n",
      "Epoch 1302/20000 Training Loss: 0.060677289962768555\n",
      "Epoch 1303/20000 Training Loss: 0.04689773917198181\n",
      "Epoch 1304/20000 Training Loss: 0.046566665172576904\n",
      "Epoch 1305/20000 Training Loss: 0.06243924796581268\n",
      "Epoch 1306/20000 Training Loss: 0.05335265025496483\n",
      "Epoch 1307/20000 Training Loss: 0.05949675664305687\n",
      "Epoch 1308/20000 Training Loss: 0.05586622282862663\n",
      "Epoch 1309/20000 Training Loss: 0.06081467494368553\n",
      "Epoch 1310/20000 Training Loss: 0.06125586852431297\n",
      "Epoch 1310/20000 Validation Loss: 0.0742821991443634\n",
      "Epoch 1311/20000 Training Loss: 0.06099607050418854\n",
      "Epoch 1312/20000 Training Loss: 0.06860144436359406\n",
      "Epoch 1313/20000 Training Loss: 0.05764925852417946\n",
      "Epoch 1314/20000 Training Loss: 0.05983053520321846\n",
      "Epoch 1315/20000 Training Loss: 0.0640784427523613\n",
      "Epoch 1316/20000 Training Loss: 0.056991737335920334\n",
      "Epoch 1317/20000 Training Loss: 0.05924606695771217\n",
      "Epoch 1318/20000 Training Loss: 0.04633014276623726\n",
      "Epoch 1319/20000 Training Loss: 0.061693836003541946\n",
      "Epoch 1320/20000 Training Loss: 0.042570561170578\n",
      "Epoch 1320/20000 Validation Loss: 0.0605819895863533\n",
      "Epoch 1321/20000 Training Loss: 0.04340933635830879\n",
      "Epoch 1322/20000 Training Loss: 0.05687130615115166\n",
      "Epoch 1323/20000 Training Loss: 0.060146819800138474\n",
      "Epoch 1324/20000 Training Loss: 0.050959184765815735\n",
      "Epoch 1325/20000 Training Loss: 0.06322091817855835\n",
      "Epoch 1326/20000 Training Loss: 0.062247779220342636\n",
      "Epoch 1327/20000 Training Loss: 0.0706794336438179\n",
      "Epoch 1328/20000 Training Loss: 0.056766215711832047\n",
      "Epoch 1329/20000 Training Loss: 0.05165833234786987\n",
      "Epoch 1330/20000 Training Loss: 0.06675568968057632\n",
      "Epoch 1330/20000 Validation Loss: 0.06485848873853683\n",
      "Epoch 1331/20000 Training Loss: 0.052500370889902115\n",
      "Epoch 1332/20000 Training Loss: 0.044495806097984314\n",
      "Epoch 1333/20000 Training Loss: 0.06829919666051865\n",
      "Epoch 1334/20000 Training Loss: 0.05227450653910637\n",
      "Epoch 1335/20000 Training Loss: 0.05374208092689514\n",
      "Epoch 1336/20000 Training Loss: 0.05621464550495148\n",
      "Epoch 1337/20000 Training Loss: 0.04718632996082306\n",
      "Epoch 1338/20000 Training Loss: 0.05161559209227562\n",
      "Epoch 1339/20000 Training Loss: 0.04712248221039772\n",
      "Epoch 1340/20000 Training Loss: 0.06831270456314087\n",
      "Epoch 1340/20000 Validation Loss: 0.09173574298620224\n",
      "Epoch 1341/20000 Training Loss: 0.05297868326306343\n",
      "Epoch 1342/20000 Training Loss: 0.062053073197603226\n",
      "Epoch 1343/20000 Training Loss: 0.05885341390967369\n",
      "Epoch 1344/20000 Training Loss: 0.052999358624219894\n",
      "Epoch 1345/20000 Training Loss: 0.07682646065950394\n",
      "Epoch 1346/20000 Training Loss: 0.06131467595696449\n",
      "Epoch 1347/20000 Training Loss: 0.041524726897478104\n",
      "Epoch 1348/20000 Training Loss: 0.046315524727106094\n",
      "Epoch 1349/20000 Training Loss: 0.06017998233437538\n",
      "Epoch 1350/20000 Training Loss: 0.059277016669511795\n",
      "Epoch 1350/20000 Validation Loss: 0.04747086763381958\n",
      "Epoch 1351/20000 Training Loss: 0.05184789374470711\n",
      "Epoch 1352/20000 Training Loss: 0.07790523022413254\n",
      "Epoch 1353/20000 Training Loss: 0.060748741030693054\n",
      "Epoch 1354/20000 Training Loss: 0.061828047037124634\n",
      "Epoch 1355/20000 Training Loss: 0.05267899110913277\n",
      "Epoch 1356/20000 Training Loss: 0.07085772603750229\n",
      "Epoch 1357/20000 Training Loss: 0.07795967906713486\n",
      "Epoch 1358/20000 Training Loss: 0.05597555637359619\n",
      "Epoch 1359/20000 Training Loss: 0.0767737627029419\n",
      "Epoch 1360/20000 Training Loss: 0.043598722666502\n",
      "Epoch 1360/20000 Validation Loss: 0.056428249925374985\n",
      "Epoch 1361/20000 Training Loss: 0.0509992353618145\n",
      "Epoch 1362/20000 Training Loss: 0.06140780448913574\n",
      "Epoch 1363/20000 Training Loss: 0.06101022660732269\n",
      "Epoch 1364/20000 Training Loss: 0.0426613949239254\n",
      "Epoch 1365/20000 Training Loss: 0.045348718762397766\n",
      "Epoch 1366/20000 Training Loss: 0.06223614886403084\n",
      "Epoch 1367/20000 Training Loss: 0.05322720482945442\n",
      "Epoch 1368/20000 Training Loss: 0.059440333396196365\n",
      "Epoch 1369/20000 Training Loss: 0.05105295404791832\n",
      "Epoch 1370/20000 Training Loss: 0.06257838010787964\n",
      "Epoch 1370/20000 Validation Loss: 0.06990977376699448\n",
      "Epoch 1371/20000 Training Loss: 0.057714398950338364\n",
      "Epoch 1372/20000 Training Loss: 0.05932728573679924\n",
      "Epoch 1373/20000 Training Loss: 0.037819940596818924\n",
      "Epoch 1374/20000 Training Loss: 0.05803452059626579\n",
      "Epoch 1375/20000 Training Loss: 0.06614293903112411\n",
      "Epoch 1376/20000 Training Loss: 0.05898185074329376\n",
      "Epoch 1377/20000 Training Loss: 0.06521465629339218\n",
      "Epoch 1378/20000 Training Loss: 0.05494356155395508\n",
      "Epoch 1379/20000 Training Loss: 0.06166945770382881\n",
      "Epoch 1380/20000 Training Loss: 0.06819524616003036\n",
      "Epoch 1380/20000 Validation Loss: 0.07600890845060349\n",
      "Epoch 1381/20000 Training Loss: 0.050577402114868164\n",
      "Epoch 1382/20000 Training Loss: 0.055828843265771866\n",
      "Epoch 1383/20000 Training Loss: 0.05703162029385567\n",
      "Epoch 1384/20000 Training Loss: 0.060727208852767944\n",
      "Epoch 1385/20000 Training Loss: 0.059476714581251144\n",
      "Epoch 1386/20000 Training Loss: 0.06425114721059799\n",
      "Epoch 1387/20000 Training Loss: 0.06589988619089127\n",
      "Epoch 1388/20000 Training Loss: 0.04593239352107048\n",
      "Epoch 1389/20000 Training Loss: 0.0604681558907032\n",
      "Epoch 1390/20000 Training Loss: 0.06720719486474991\n",
      "Epoch 1390/20000 Validation Loss: 0.06103312224149704\n",
      "Epoch 1391/20000 Training Loss: 0.07153346389532089\n",
      "Epoch 1392/20000 Training Loss: 0.0665440633893013\n",
      "Epoch 1393/20000 Training Loss: 0.061927977949380875\n",
      "Epoch 1394/20000 Training Loss: 0.05910306051373482\n",
      "Epoch 1395/20000 Training Loss: 0.09185966104269028\n",
      "Epoch 1396/20000 Training Loss: 0.06130462512373924\n",
      "Epoch 1397/20000 Training Loss: 0.04405495896935463\n",
      "Epoch 1398/20000 Training Loss: 0.06653448939323425\n",
      "Epoch 1399/20000 Training Loss: 0.06780585646629333\n",
      "Epoch 1400/20000 Training Loss: 0.09398739784955978\n",
      "Epoch 1400/20000 Validation Loss: 0.03541106730699539\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03541106730699539<=============\n",
      "Epoch 1401/20000 Training Loss: 0.05621723458170891\n",
      "Epoch 1402/20000 Training Loss: 0.08187080174684525\n",
      "Epoch 1403/20000 Training Loss: 0.05252032354474068\n",
      "Epoch 1404/20000 Training Loss: 0.04896495118737221\n",
      "Epoch 1405/20000 Training Loss: 0.058245692402124405\n",
      "Epoch 1406/20000 Training Loss: 0.0645853579044342\n",
      "Epoch 1407/20000 Training Loss: 0.0579732209444046\n",
      "Epoch 1408/20000 Training Loss: 0.05371912941336632\n",
      "Epoch 1409/20000 Training Loss: 0.07133174687623978\n",
      "Epoch 1410/20000 Training Loss: 0.052603527903556824\n",
      "Epoch 1410/20000 Validation Loss: 0.05738692358136177\n",
      "Epoch 1411/20000 Training Loss: 0.06605242937803268\n",
      "Epoch 1412/20000 Training Loss: 0.0452733039855957\n",
      "Epoch 1413/20000 Training Loss: 0.06506331264972687\n",
      "Epoch 1414/20000 Training Loss: 0.07862595468759537\n",
      "Epoch 1415/20000 Training Loss: 0.05689586326479912\n",
      "Epoch 1416/20000 Training Loss: 0.0509646050632\n",
      "Epoch 1417/20000 Training Loss: 0.05539965257048607\n",
      "Epoch 1418/20000 Training Loss: 0.05886315926909447\n",
      "Epoch 1419/20000 Training Loss: 0.06417196244001389\n",
      "Epoch 1420/20000 Training Loss: 0.07711465656757355\n",
      "Epoch 1420/20000 Validation Loss: 0.05919062718749046\n",
      "Epoch 1421/20000 Training Loss: 0.05691170319914818\n",
      "Epoch 1422/20000 Training Loss: 0.06949066370725632\n",
      "Epoch 1423/20000 Training Loss: 0.056037794798612595\n",
      "Epoch 1424/20000 Training Loss: 0.043439362198114395\n",
      "Epoch 1425/20000 Training Loss: 0.04113999009132385\n",
      "Epoch 1426/20000 Training Loss: 0.07090245932340622\n",
      "Epoch 1427/20000 Training Loss: 0.07355472445487976\n",
      "Epoch 1428/20000 Training Loss: 0.05204014480113983\n",
      "Epoch 1429/20000 Training Loss: 0.06379043310880661\n",
      "Epoch 1430/20000 Training Loss: 0.049341216683387756\n",
      "Epoch 1430/20000 Validation Loss: 0.03797594830393791\n",
      "Epoch 1431/20000 Training Loss: 0.060128528624773026\n",
      "Epoch 1432/20000 Training Loss: 0.0411098338663578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1433/20000 Training Loss: 0.06679928302764893\n",
      "Epoch 1434/20000 Training Loss: 0.05979567766189575\n",
      "Epoch 1435/20000 Training Loss: 0.05867113545536995\n",
      "Epoch 1436/20000 Training Loss: 0.047225505113601685\n",
      "Epoch 1437/20000 Training Loss: 0.06566379219293594\n",
      "Epoch 1438/20000 Training Loss: 0.05432937666773796\n",
      "Epoch 1439/20000 Training Loss: 0.07352494448423386\n",
      "Epoch 1440/20000 Training Loss: 0.08019904047250748\n",
      "Epoch 1440/20000 Validation Loss: 0.0443728044629097\n",
      "Epoch 1441/20000 Training Loss: 0.06128444895148277\n",
      "Epoch 1442/20000 Training Loss: 0.053840916603803635\n",
      "Epoch 1443/20000 Training Loss: 0.04859287664294243\n",
      "Epoch 1444/20000 Training Loss: 0.05535503104329109\n",
      "Epoch 1445/20000 Training Loss: 0.05317489430308342\n",
      "Epoch 1446/20000 Training Loss: 0.04714133217930794\n",
      "Epoch 1447/20000 Training Loss: 0.04768335819244385\n",
      "Epoch 1448/20000 Training Loss: 0.054882243275642395\n",
      "Epoch 1449/20000 Training Loss: 0.05434547737240791\n",
      "Epoch 1450/20000 Training Loss: 0.06683655828237534\n",
      "Epoch 1450/20000 Validation Loss: 0.08156776428222656\n",
      "Epoch 1451/20000 Training Loss: 0.06475961953401566\n",
      "Epoch 1452/20000 Training Loss: 0.05059999227523804\n",
      "Epoch 1453/20000 Training Loss: 0.04352457448840141\n",
      "Epoch 1454/20000 Training Loss: 0.03651779890060425\n",
      "Epoch 1455/20000 Training Loss: 0.06678750365972519\n",
      "Epoch 1456/20000 Training Loss: 0.049143653362989426\n",
      "Epoch 1457/20000 Training Loss: 0.04355762526392937\n",
      "Epoch 1458/20000 Training Loss: 0.0649479553103447\n",
      "Epoch 1459/20000 Training Loss: 0.05256466940045357\n",
      "Epoch 1460/20000 Training Loss: 0.12439823150634766\n",
      "Epoch 1460/20000 Validation Loss: 0.06112617254257202\n",
      "Epoch 1461/20000 Training Loss: 0.06267627328634262\n",
      "Epoch 1462/20000 Training Loss: 0.045076917856931686\n",
      "Epoch 1463/20000 Training Loss: 0.04271293804049492\n",
      "Epoch 1464/20000 Training Loss: 0.05705258250236511\n",
      "Epoch 1465/20000 Training Loss: 0.06331423670053482\n",
      "Epoch 1466/20000 Training Loss: 0.044165898114442825\n",
      "Epoch 1467/20000 Training Loss: 0.05935274064540863\n",
      "Epoch 1468/20000 Training Loss: 0.07341321557760239\n",
      "Epoch 1469/20000 Training Loss: 0.05522218719124794\n",
      "Epoch 1470/20000 Training Loss: 0.06713170558214188\n",
      "Epoch 1470/20000 Validation Loss: 0.06369976699352264\n",
      "Epoch 1471/20000 Training Loss: 0.060462385416030884\n",
      "Epoch 1472/20000 Training Loss: 0.04506504535675049\n",
      "Epoch 1473/20000 Training Loss: 0.04879550263285637\n",
      "Epoch 1474/20000 Training Loss: 0.08409125357866287\n",
      "Epoch 1475/20000 Training Loss: 0.057502347975969315\n",
      "Epoch 1476/20000 Training Loss: 0.04803748056292534\n",
      "Epoch 1477/20000 Training Loss: 0.06807649880647659\n",
      "Epoch 1478/20000 Training Loss: 0.042599912732839584\n",
      "Epoch 1479/20000 Training Loss: 0.0535728745162487\n",
      "Epoch 1480/20000 Training Loss: 0.06063796579837799\n",
      "Epoch 1480/20000 Validation Loss: 0.048913903534412384\n",
      "Epoch 1481/20000 Training Loss: 0.0641293153166771\n",
      "Epoch 1482/20000 Training Loss: 0.05347910523414612\n",
      "Epoch 1483/20000 Training Loss: 0.05263984575867653\n",
      "Epoch 1484/20000 Training Loss: 0.04969501495361328\n",
      "Epoch 1485/20000 Training Loss: 0.07947266846895218\n",
      "Epoch 1486/20000 Training Loss: 0.06329626590013504\n",
      "Epoch 1487/20000 Training Loss: 0.07115056365728378\n",
      "Epoch 1488/20000 Training Loss: 0.06854013353586197\n",
      "Epoch 1489/20000 Training Loss: 0.049620743840932846\n",
      "Epoch 1490/20000 Training Loss: 0.05824081972241402\n",
      "Epoch 1490/20000 Validation Loss: 0.04749832674860954\n",
      "Epoch 1491/20000 Training Loss: 0.058092717081308365\n",
      "Epoch 1492/20000 Training Loss: 0.07125850766897202\n",
      "Epoch 1493/20000 Training Loss: 0.06671056896448135\n",
      "Epoch 1494/20000 Training Loss: 0.05840448662638664\n",
      "Epoch 1495/20000 Training Loss: 0.05081435665488243\n",
      "Epoch 1496/20000 Training Loss: 0.06835997104644775\n",
      "Epoch 1497/20000 Training Loss: 0.05340076982975006\n",
      "Epoch 1498/20000 Training Loss: 0.055723581463098526\n",
      "Epoch 1499/20000 Training Loss: 0.051814209669828415\n",
      "Epoch 1500/20000 Training Loss: 0.052869800478219986\n",
      "Epoch 1500/20000 Validation Loss: 0.08121655881404877\n",
      "Epoch 1501/20000 Training Loss: 0.06335625797510147\n",
      "Epoch 1502/20000 Training Loss: 0.0614907406270504\n",
      "Epoch 1503/20000 Training Loss: 0.0422048419713974\n",
      "Epoch 1504/20000 Training Loss: 0.06469682604074478\n",
      "Epoch 1505/20000 Training Loss: 0.05926387384533882\n",
      "Epoch 1506/20000 Training Loss: 0.05865161120891571\n",
      "Epoch 1507/20000 Training Loss: 0.05237564444541931\n",
      "Epoch 1508/20000 Training Loss: 0.04445343092083931\n",
      "Epoch 1509/20000 Training Loss: 0.0751020535826683\n",
      "Epoch 1510/20000 Training Loss: 0.052269697189331055\n",
      "Epoch 1510/20000 Validation Loss: 0.04386443644762039\n",
      "Epoch 1511/20000 Training Loss: 0.04742543026804924\n",
      "Epoch 1512/20000 Training Loss: 0.06450272351503372\n",
      "Epoch 1513/20000 Training Loss: 0.07081878930330276\n",
      "Epoch 1514/20000 Training Loss: 0.06043059006333351\n",
      "Epoch 1515/20000 Training Loss: 0.06760932505130768\n",
      "Epoch 1516/20000 Training Loss: 0.05530381575226784\n",
      "Epoch 1517/20000 Training Loss: 0.06213994696736336\n",
      "Epoch 1518/20000 Training Loss: 0.08193427324295044\n",
      "Epoch 1519/20000 Training Loss: 0.06444358825683594\n",
      "Epoch 1520/20000 Training Loss: 0.06755533069372177\n",
      "Epoch 1520/20000 Validation Loss: 0.08079702407121658\n",
      "Epoch 1521/20000 Training Loss: 0.058192700147628784\n",
      "Epoch 1522/20000 Training Loss: 0.053253527730703354\n",
      "Epoch 1523/20000 Training Loss: 0.07215699553489685\n",
      "Epoch 1524/20000 Training Loss: 0.06757594645023346\n",
      "Epoch 1525/20000 Training Loss: 0.04200040176510811\n",
      "Epoch 1526/20000 Training Loss: 0.07921680808067322\n",
      "Epoch 1527/20000 Training Loss: 0.050040632486343384\n",
      "Epoch 1528/20000 Training Loss: 0.06043224409222603\n",
      "Epoch 1529/20000 Training Loss: 0.05778561159968376\n",
      "Epoch 1530/20000 Training Loss: 0.06754452735185623\n",
      "Epoch 1530/20000 Validation Loss: 0.07059672474861145\n",
      "Epoch 1531/20000 Training Loss: 0.05364227294921875\n",
      "Epoch 1532/20000 Training Loss: 0.060602396726608276\n",
      "Epoch 1533/20000 Training Loss: 0.05437905713915825\n",
      "Epoch 1534/20000 Training Loss: 0.0580589659512043\n",
      "Epoch 1535/20000 Training Loss: 0.07051090896129608\n",
      "Epoch 1536/20000 Training Loss: 0.059706974774599075\n",
      "Epoch 1537/20000 Training Loss: 0.0646529272198677\n",
      "Epoch 1538/20000 Training Loss: 0.054900795221328735\n",
      "Epoch 1539/20000 Training Loss: 0.062119048088788986\n",
      "Epoch 1540/20000 Training Loss: 0.06121896207332611\n",
      "Epoch 1540/20000 Validation Loss: 0.06580717861652374\n",
      "Epoch 1541/20000 Training Loss: 0.0762188732624054\n",
      "Epoch 1542/20000 Training Loss: 0.06611112505197525\n",
      "Epoch 1543/20000 Training Loss: 0.044443294405937195\n",
      "Epoch 1544/20000 Training Loss: 0.05112854018807411\n",
      "Epoch 1545/20000 Training Loss: 0.06513442099094391\n",
      "Epoch 1546/20000 Training Loss: 0.0614003986120224\n",
      "Epoch 1547/20000 Training Loss: 0.0679301917552948\n",
      "Epoch 1548/20000 Training Loss: 0.06635893136262894\n",
      "Epoch 1549/20000 Training Loss: 0.05839148536324501\n",
      "Epoch 1550/20000 Training Loss: 0.04859478399157524\n",
      "Epoch 1550/20000 Validation Loss: 0.07191269099712372\n",
      "Epoch 1551/20000 Training Loss: 0.044533565640449524\n",
      "Epoch 1552/20000 Training Loss: 0.0472501702606678\n",
      "Epoch 1553/20000 Training Loss: 0.04237769916653633\n",
      "Epoch 1554/20000 Training Loss: 0.05889928340911865\n",
      "Epoch 1555/20000 Training Loss: 0.07243959605693817\n",
      "Epoch 1556/20000 Training Loss: 0.06885277479887009\n",
      "Epoch 1557/20000 Training Loss: 0.05268039181828499\n",
      "Epoch 1558/20000 Training Loss: 0.05616919696331024\n",
      "Epoch 1559/20000 Training Loss: 0.06595130264759064\n",
      "Epoch 1560/20000 Training Loss: 0.07629529386758804\n",
      "Epoch 1560/20000 Validation Loss: 0.05796607583761215\n",
      "Epoch 1561/20000 Training Loss: 0.0631626546382904\n",
      "Epoch 1562/20000 Training Loss: 0.09114482998847961\n",
      "Epoch 1563/20000 Training Loss: 0.07000682502985\n",
      "Epoch 1564/20000 Training Loss: 0.060674700886011124\n",
      "Epoch 1565/20000 Training Loss: 0.05938415601849556\n",
      "Epoch 1566/20000 Training Loss: 0.06128762662410736\n",
      "Epoch 1567/20000 Training Loss: 0.05824647843837738\n",
      "Epoch 1568/20000 Training Loss: 0.051993656903505325\n",
      "Epoch 1569/20000 Training Loss: 0.04980545863509178\n",
      "Epoch 1570/20000 Training Loss: 0.054199397563934326\n",
      "Epoch 1570/20000 Validation Loss: 0.059003472328186035\n",
      "Epoch 1571/20000 Training Loss: 0.06615599244832993\n",
      "Epoch 1572/20000 Training Loss: 0.06383926421403885\n",
      "Epoch 1573/20000 Training Loss: 0.06404132395982742\n",
      "Epoch 1574/20000 Training Loss: 0.0794823095202446\n",
      "Epoch 1575/20000 Training Loss: 0.054677072912454605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1576/20000 Training Loss: 0.06099925562739372\n",
      "Epoch 1577/20000 Training Loss: 0.06107962504029274\n",
      "Epoch 1578/20000 Training Loss: 0.06730913370847702\n",
      "Epoch 1579/20000 Training Loss: 0.05205699801445007\n",
      "Epoch 1580/20000 Training Loss: 0.06821125000715256\n",
      "Epoch 1580/20000 Validation Loss: 0.04760609567165375\n",
      "Epoch 1581/20000 Training Loss: 0.05930685997009277\n",
      "Epoch 1582/20000 Training Loss: 0.04800254479050636\n",
      "Epoch 1583/20000 Training Loss: 0.06976873427629471\n",
      "Epoch 1584/20000 Training Loss: 0.05597566440701485\n",
      "Epoch 1585/20000 Training Loss: 0.04405953362584114\n",
      "Epoch 1586/20000 Training Loss: 0.05965544655919075\n",
      "Epoch 1587/20000 Training Loss: 0.03845040872693062\n",
      "Epoch 1588/20000 Training Loss: 0.053649988025426865\n",
      "Epoch 1589/20000 Training Loss: 0.06823209673166275\n",
      "Epoch 1590/20000 Training Loss: 0.05594201013445854\n",
      "Epoch 1590/20000 Validation Loss: 0.0542784221470356\n",
      "Epoch 1591/20000 Training Loss: 0.0635531023144722\n",
      "Epoch 1592/20000 Training Loss: 0.07007179409265518\n",
      "Epoch 1593/20000 Training Loss: 0.07300173491239548\n",
      "Epoch 1594/20000 Training Loss: 0.07868243753910065\n",
      "Epoch 1595/20000 Training Loss: 0.053641919046640396\n",
      "Epoch 1596/20000 Training Loss: 0.0545414499938488\n",
      "Epoch 1597/20000 Training Loss: 0.05894838646054268\n",
      "Epoch 1598/20000 Training Loss: 0.056943584233522415\n",
      "Epoch 1599/20000 Training Loss: 0.05305514857172966\n",
      "Epoch 1600/20000 Training Loss: 0.06348865479230881\n",
      "Epoch 1600/20000 Validation Loss: 0.04880765080451965\n",
      "Epoch 1601/20000 Training Loss: 0.06816787272691727\n",
      "Epoch 1602/20000 Training Loss: 0.07946787029504776\n",
      "Epoch 1603/20000 Training Loss: 0.06810557842254639\n",
      "Epoch 1604/20000 Training Loss: 0.0649949461221695\n",
      "Epoch 1605/20000 Training Loss: 0.05598190426826477\n",
      "Epoch 1606/20000 Training Loss: 0.06336618214845657\n",
      "Epoch 1607/20000 Training Loss: 0.05201901122927666\n",
      "Epoch 1608/20000 Training Loss: 0.07253279536962509\n",
      "Epoch 1609/20000 Training Loss: 0.04840286076068878\n",
      "Epoch 1610/20000 Training Loss: 0.059820134192705154\n",
      "Epoch 1610/20000 Validation Loss: 0.07419615238904953\n",
      "Epoch 1611/20000 Training Loss: 0.06850472837686539\n",
      "Epoch 1612/20000 Training Loss: 0.060628827661275864\n",
      "Epoch 1613/20000 Training Loss: 0.06368046998977661\n",
      "Epoch 1614/20000 Training Loss: 0.04705962538719177\n",
      "Epoch 1615/20000 Training Loss: 0.07084058970212936\n",
      "Epoch 1616/20000 Training Loss: 0.053236063569784164\n",
      "Epoch 1617/20000 Training Loss: 0.06045795604586601\n",
      "Epoch 1618/20000 Training Loss: 0.05501338467001915\n",
      "Epoch 1619/20000 Training Loss: 0.04872329533100128\n",
      "Epoch 1620/20000 Training Loss: 0.05516928434371948\n",
      "Epoch 1620/20000 Validation Loss: 0.04986012727022171\n",
      "Epoch 1621/20000 Training Loss: 0.061215590685606\n",
      "Epoch 1622/20000 Training Loss: 0.064195916056633\n",
      "Epoch 1623/20000 Training Loss: 0.07299480587244034\n",
      "Epoch 1624/20000 Training Loss: 0.07078557461500168\n",
      "Epoch 1625/20000 Training Loss: 0.04615064337849617\n",
      "Epoch 1626/20000 Training Loss: 0.05849124491214752\n",
      "Epoch 1627/20000 Training Loss: 0.05373669043183327\n",
      "Epoch 1628/20000 Training Loss: 0.06465011835098267\n",
      "Epoch 1629/20000 Training Loss: 0.05887088552117348\n",
      "Epoch 1630/20000 Training Loss: 0.05391833186149597\n",
      "Epoch 1630/20000 Validation Loss: 0.07115110754966736\n",
      "Epoch 1631/20000 Training Loss: 0.04092629626393318\n",
      "Epoch 1632/20000 Training Loss: 0.061488792300224304\n",
      "Epoch 1633/20000 Training Loss: 0.05592893436551094\n",
      "Epoch 1634/20000 Training Loss: 0.06925565749406815\n",
      "Epoch 1635/20000 Training Loss: 0.05299381539225578\n",
      "Epoch 1636/20000 Training Loss: 0.04333103820681572\n",
      "Epoch 1637/20000 Training Loss: 0.05801399052143097\n",
      "Epoch 1638/20000 Training Loss: 0.03999687358736992\n",
      "Epoch 1639/20000 Training Loss: 0.05712801218032837\n",
      "Epoch 1640/20000 Training Loss: 0.04922623932361603\n",
      "Epoch 1640/20000 Validation Loss: 0.05698518827557564\n",
      "Epoch 1641/20000 Training Loss: 0.06889300793409348\n",
      "Epoch 1642/20000 Training Loss: 0.04039054736495018\n",
      "Epoch 1643/20000 Training Loss: 0.06667567044496536\n",
      "Epoch 1644/20000 Training Loss: 0.0667768344283104\n",
      "Epoch 1645/20000 Training Loss: 0.06244799867272377\n",
      "Epoch 1646/20000 Training Loss: 0.06582669913768768\n",
      "Epoch 1647/20000 Training Loss: 0.048222023993730545\n",
      "Epoch 1648/20000 Training Loss: 0.05196410045027733\n",
      "Epoch 1649/20000 Training Loss: 0.0694960430264473\n",
      "Epoch 1650/20000 Training Loss: 0.08193708956241608\n",
      "Epoch 1650/20000 Validation Loss: 0.06618909537792206\n",
      "Epoch 1651/20000 Training Loss: 0.040232136845588684\n",
      "Epoch 1652/20000 Training Loss: 0.0659969300031662\n",
      "Epoch 1653/20000 Training Loss: 0.0649331733584404\n",
      "Epoch 1654/20000 Training Loss: 0.057722970843315125\n",
      "Epoch 1655/20000 Training Loss: 0.048614177852869034\n",
      "Epoch 1656/20000 Training Loss: 0.06771135330200195\n",
      "Epoch 1657/20000 Training Loss: 0.04428550973534584\n",
      "Epoch 1658/20000 Training Loss: 0.0608815960586071\n",
      "Epoch 1659/20000 Training Loss: 0.061538875102996826\n",
      "Epoch 1660/20000 Training Loss: 0.06126490607857704\n",
      "Epoch 1660/20000 Validation Loss: 0.0643203929066658\n",
      "Epoch 1661/20000 Training Loss: 0.05745381489396095\n",
      "Epoch 1662/20000 Training Loss: 0.059612106531858444\n",
      "Epoch 1663/20000 Training Loss: 0.06338918209075928\n",
      "Epoch 1664/20000 Training Loss: 0.057798612862825394\n",
      "Epoch 1665/20000 Training Loss: 0.08178503066301346\n",
      "Epoch 1666/20000 Training Loss: 0.06092445179820061\n",
      "Epoch 1667/20000 Training Loss: 0.05993447080254555\n",
      "Epoch 1668/20000 Training Loss: 0.06504430621862411\n",
      "Epoch 1669/20000 Training Loss: 0.06905295699834824\n",
      "Epoch 1670/20000 Training Loss: 0.057405631989240646\n",
      "Epoch 1670/20000 Validation Loss: 0.05758243054151535\n",
      "Epoch 1671/20000 Training Loss: 0.06021576747298241\n",
      "Epoch 1672/20000 Training Loss: 0.03936009481549263\n",
      "Epoch 1673/20000 Training Loss: 0.07677781581878662\n",
      "Epoch 1674/20000 Training Loss: 0.04442388191819191\n",
      "Epoch 1675/20000 Training Loss: 0.07631080597639084\n",
      "Epoch 1676/20000 Training Loss: 0.054993998259305954\n",
      "Epoch 1677/20000 Training Loss: 0.06472834199666977\n",
      "Epoch 1678/20000 Training Loss: 0.07169514894485474\n",
      "Epoch 1679/20000 Training Loss: 0.0753876194357872\n",
      "Epoch 1680/20000 Training Loss: 0.06479955464601517\n",
      "Epoch 1680/20000 Validation Loss: 0.0771137997508049\n",
      "Epoch 1681/20000 Training Loss: 0.07800246775150299\n",
      "Epoch 1682/20000 Training Loss: 0.05549769476056099\n",
      "Epoch 1683/20000 Training Loss: 0.06031915545463562\n",
      "Epoch 1684/20000 Training Loss: 0.045440927147865295\n",
      "Epoch 1685/20000 Training Loss: 0.064717136323452\n",
      "Epoch 1686/20000 Training Loss: 0.07505249977111816\n",
      "Epoch 1687/20000 Training Loss: 0.08019689470529556\n",
      "Epoch 1688/20000 Training Loss: 0.0783892348408699\n",
      "Epoch 1689/20000 Training Loss: 0.03979110345244408\n",
      "Epoch 1690/20000 Training Loss: 0.05825548246502876\n",
      "Epoch 1690/20000 Validation Loss: 0.06219068914651871\n",
      "Epoch 1691/20000 Training Loss: 0.07344009727239609\n",
      "Epoch 1692/20000 Training Loss: 0.06715809553861618\n",
      "Epoch 1693/20000 Training Loss: 0.06337102502584457\n",
      "Epoch 1694/20000 Training Loss: 0.06631294637918472\n",
      "Epoch 1695/20000 Training Loss: 0.06570269912481308\n",
      "Epoch 1696/20000 Training Loss: 0.06330177187919617\n",
      "Epoch 1697/20000 Training Loss: 0.0581403523683548\n",
      "Epoch 1698/20000 Training Loss: 0.05248212814331055\n",
      "Epoch 1699/20000 Training Loss: 0.06775330752134323\n",
      "Epoch 1700/20000 Training Loss: 0.058294445276260376\n",
      "Epoch 1700/20000 Validation Loss: 0.06928916275501251\n",
      "Epoch 1701/20000 Training Loss: 0.06734750419855118\n",
      "Epoch 1702/20000 Training Loss: 0.05681696534156799\n",
      "Epoch 1703/20000 Training Loss: 0.05499596893787384\n",
      "Epoch 1704/20000 Training Loss: 0.06571861356496811\n",
      "Epoch 1705/20000 Training Loss: 0.05939928814768791\n",
      "Epoch 1706/20000 Training Loss: 0.04982766881585121\n",
      "Epoch 1707/20000 Training Loss: 0.0588844008743763\n",
      "Epoch 1708/20000 Training Loss: 0.056885648518800735\n",
      "Epoch 1709/20000 Training Loss: 0.056683968752622604\n",
      "Epoch 1710/20000 Training Loss: 0.058914605528116226\n",
      "Epoch 1710/20000 Validation Loss: 0.06859813630580902\n",
      "Epoch 1711/20000 Training Loss: 0.05221842601895332\n",
      "Epoch 1712/20000 Training Loss: 0.07618039101362228\n",
      "Epoch 1713/20000 Training Loss: 0.05910956859588623\n",
      "Epoch 1714/20000 Training Loss: 0.06669952720403671\n",
      "Epoch 1715/20000 Training Loss: 0.04857640340924263\n",
      "Epoch 1716/20000 Training Loss: 0.05944906547665596\n",
      "Epoch 1717/20000 Training Loss: 0.04925110936164856\n",
      "Epoch 1718/20000 Training Loss: 0.05396411940455437\n",
      "Epoch 1719/20000 Training Loss: 0.05118201673030853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1720/20000 Training Loss: 0.0659511610865593\n",
      "Epoch 1720/20000 Validation Loss: 0.06346412748098373\n",
      "Epoch 1721/20000 Training Loss: 0.08389649540185928\n",
      "Epoch 1722/20000 Training Loss: 0.06386425346136093\n",
      "Epoch 1723/20000 Training Loss: 0.04600362479686737\n",
      "Epoch 1724/20000 Training Loss: 0.05343281105160713\n",
      "Epoch 1725/20000 Training Loss: 0.049925390630960464\n",
      "Epoch 1726/20000 Training Loss: 0.06065516546368599\n",
      "Epoch 1727/20000 Training Loss: 0.06777321547269821\n",
      "Epoch 1728/20000 Training Loss: 0.07104069739580154\n",
      "Epoch 1729/20000 Training Loss: 0.05455952510237694\n",
      "Epoch 1730/20000 Training Loss: 0.06332916766405106\n",
      "Epoch 1730/20000 Validation Loss: 0.05700036138296127\n",
      "Epoch 1731/20000 Training Loss: 0.04432538151741028\n",
      "Epoch 1732/20000 Training Loss: 0.07000782340765\n",
      "Epoch 1733/20000 Training Loss: 0.05844101682305336\n",
      "Epoch 1734/20000 Training Loss: 0.05656563118100166\n",
      "Epoch 1735/20000 Training Loss: 0.061682578176259995\n",
      "Epoch 1736/20000 Training Loss: 0.058741245418787\n",
      "Epoch 1737/20000 Training Loss: 0.04667970538139343\n",
      "Epoch 1738/20000 Training Loss: 0.041842710226774216\n",
      "Epoch 1739/20000 Training Loss: 0.06771180778741837\n",
      "Epoch 1740/20000 Training Loss: 0.04440733417868614\n",
      "Epoch 1740/20000 Validation Loss: 0.06891493499279022\n",
      "Epoch 1741/20000 Training Loss: 0.05981290712952614\n",
      "Epoch 1742/20000 Training Loss: 0.048631489276885986\n",
      "Epoch 1743/20000 Training Loss: 0.05723349377512932\n",
      "Epoch 1744/20000 Training Loss: 0.06989437341690063\n",
      "Epoch 1745/20000 Training Loss: 0.06892707943916321\n",
      "Epoch 1746/20000 Training Loss: 0.04629743471741676\n",
      "Epoch 1747/20000 Training Loss: 0.05708989500999451\n",
      "Epoch 1748/20000 Training Loss: 0.04876113310456276\n",
      "Epoch 1749/20000 Training Loss: 0.06402528285980225\n",
      "Epoch 1750/20000 Training Loss: 0.054416101425886154\n",
      "Epoch 1750/20000 Validation Loss: 0.07686164230108261\n",
      "Epoch 1751/20000 Training Loss: 0.0654744803905487\n",
      "Epoch 1752/20000 Training Loss: 0.05712402984499931\n",
      "Epoch 1753/20000 Training Loss: 0.05426822230219841\n",
      "Epoch 1754/20000 Training Loss: 0.05741288140416145\n",
      "Epoch 1755/20000 Training Loss: 0.05562673881649971\n",
      "Epoch 1756/20000 Training Loss: 0.0632421150803566\n",
      "Epoch 1757/20000 Training Loss: 0.059558987617492676\n",
      "Epoch 1758/20000 Training Loss: 0.04560591280460358\n",
      "Epoch 1759/20000 Training Loss: 0.06367648392915726\n",
      "Epoch 1760/20000 Training Loss: 0.04559655860066414\n",
      "Epoch 1760/20000 Validation Loss: 0.048163041472435\n",
      "Epoch 1761/20000 Training Loss: 0.06005708873271942\n",
      "Epoch 1762/20000 Training Loss: 0.05884217843413353\n",
      "Epoch 1763/20000 Training Loss: 0.06022677943110466\n",
      "Epoch 1764/20000 Training Loss: 0.05615847930312157\n",
      "Epoch 1765/20000 Training Loss: 0.056089241057634354\n",
      "Epoch 1766/20000 Training Loss: 0.058480843901634216\n",
      "Epoch 1767/20000 Training Loss: 0.06176362559199333\n",
      "Epoch 1768/20000 Training Loss: 0.07159669697284698\n",
      "Epoch 1769/20000 Training Loss: 0.07159363478422165\n",
      "Epoch 1770/20000 Training Loss: 0.06684710830450058\n",
      "Epoch 1770/20000 Validation Loss: 0.07586583495140076\n",
      "Epoch 1771/20000 Training Loss: 0.05598505213856697\n",
      "Epoch 1772/20000 Training Loss: 0.06647306680679321\n",
      "Epoch 1773/20000 Training Loss: 0.04771697521209717\n",
      "Epoch 1774/20000 Training Loss: 0.055117104202508926\n",
      "Epoch 1775/20000 Training Loss: 0.05673937126994133\n",
      "Epoch 1776/20000 Training Loss: 0.057000141590833664\n",
      "Epoch 1777/20000 Training Loss: 0.07140495628118515\n",
      "Epoch 1778/20000 Training Loss: 0.06414279341697693\n",
      "Epoch 1779/20000 Training Loss: 0.06195124611258507\n",
      "Epoch 1780/20000 Training Loss: 0.06614502519369125\n",
      "Epoch 1780/20000 Validation Loss: 0.07890315353870392\n",
      "Epoch 1781/20000 Training Loss: 0.05782374367117882\n",
      "Epoch 1782/20000 Training Loss: 0.04268983006477356\n",
      "Epoch 1783/20000 Training Loss: 0.04523387551307678\n",
      "Epoch 1784/20000 Training Loss: 0.05253574624657631\n",
      "Epoch 1785/20000 Training Loss: 0.05186815187335014\n",
      "Epoch 1786/20000 Training Loss: 0.045521125197410583\n",
      "Epoch 1787/20000 Training Loss: 0.052944958209991455\n",
      "Epoch 1788/20000 Training Loss: 0.04957175254821777\n",
      "Epoch 1789/20000 Training Loss: 0.07307231426239014\n",
      "Epoch 1790/20000 Training Loss: 0.04205009713768959\n",
      "Epoch 1790/20000 Validation Loss: 0.05527733266353607\n",
      "Epoch 1791/20000 Training Loss: 0.05741552636027336\n",
      "Epoch 1792/20000 Training Loss: 0.060375016182661057\n",
      "Epoch 1793/20000 Training Loss: 0.05090029537677765\n",
      "Epoch 1794/20000 Training Loss: 0.051381926983594894\n",
      "Epoch 1795/20000 Training Loss: 0.06550288945436478\n",
      "Epoch 1796/20000 Training Loss: 0.06547807902097702\n",
      "Epoch 1797/20000 Training Loss: 0.07553300261497498\n",
      "Epoch 1798/20000 Training Loss: 0.04897825047373772\n",
      "Epoch 1799/20000 Training Loss: 0.068069227039814\n",
      "Epoch 1800/20000 Training Loss: 0.03942425176501274\n",
      "Epoch 1800/20000 Validation Loss: 0.061499759554862976\n",
      "Epoch 1801/20000 Training Loss: 0.05475237965583801\n",
      "Epoch 1802/20000 Training Loss: 0.05950464308261871\n",
      "Epoch 1803/20000 Training Loss: 0.06461209803819656\n",
      "Epoch 1804/20000 Training Loss: 0.059706542640924454\n",
      "Epoch 1805/20000 Training Loss: 0.06389636546373367\n",
      "Epoch 1806/20000 Training Loss: 0.04634227976202965\n",
      "Epoch 1807/20000 Training Loss: 0.06239932402968407\n",
      "Epoch 1808/20000 Training Loss: 0.07211151719093323\n",
      "Epoch 1809/20000 Training Loss: 0.07128002494573593\n",
      "Epoch 1810/20000 Training Loss: 0.05726995691657066\n",
      "Epoch 1810/20000 Validation Loss: 0.0761362761259079\n",
      "Epoch 1811/20000 Training Loss: 0.06851502507925034\n",
      "Epoch 1812/20000 Training Loss: 0.05930245295166969\n",
      "Epoch 1813/20000 Training Loss: 0.06546197086572647\n",
      "Epoch 1814/20000 Training Loss: 0.06416279077529907\n",
      "Epoch 1815/20000 Training Loss: 0.07063806802034378\n",
      "Epoch 1816/20000 Training Loss: 0.046785395592451096\n",
      "Epoch 1817/20000 Training Loss: 0.07730855792760849\n",
      "Epoch 1818/20000 Training Loss: 0.05596113204956055\n",
      "Epoch 1819/20000 Training Loss: 0.05644477531313896\n",
      "Epoch 1820/20000 Training Loss: 0.054619818925857544\n",
      "Epoch 1820/20000 Validation Loss: 0.05121416598558426\n",
      "Epoch 1821/20000 Training Loss: 0.06671691685914993\n",
      "Epoch 1822/20000 Training Loss: 0.07597649842500687\n",
      "Epoch 1823/20000 Training Loss: 0.04986254498362541\n",
      "Epoch 1824/20000 Training Loss: 0.06531647592782974\n",
      "Epoch 1825/20000 Training Loss: 0.046767111867666245\n",
      "Epoch 1826/20000 Training Loss: 0.063343346118927\n",
      "Epoch 1827/20000 Training Loss: 0.03923224285244942\n",
      "Epoch 1828/20000 Training Loss: 0.06146381422877312\n",
      "Epoch 1829/20000 Training Loss: 0.06199498474597931\n",
      "Epoch 1830/20000 Training Loss: 0.05513319745659828\n",
      "Epoch 1830/20000 Validation Loss: 0.07393831014633179\n",
      "Epoch 1831/20000 Training Loss: 0.06936723738908768\n",
      "Epoch 1832/20000 Training Loss: 0.058964695781469345\n",
      "Epoch 1833/20000 Training Loss: 0.058592814952135086\n",
      "Epoch 1834/20000 Training Loss: 0.06865667551755905\n",
      "Epoch 1835/20000 Training Loss: 0.06047337129712105\n",
      "Epoch 1836/20000 Training Loss: 0.05922364071011543\n",
      "Epoch 1837/20000 Training Loss: 0.04028509184718132\n",
      "Epoch 1838/20000 Training Loss: 0.06212427839636803\n",
      "Epoch 1839/20000 Training Loss: 0.07919368147850037\n",
      "Epoch 1840/20000 Training Loss: 0.053253669291734695\n",
      "Epoch 1840/20000 Validation Loss: 0.0657522976398468\n",
      "Epoch 1841/20000 Training Loss: 0.060398366302251816\n",
      "Epoch 1842/20000 Training Loss: 0.05615554377436638\n",
      "Epoch 1843/20000 Training Loss: 0.05030057951807976\n",
      "Epoch 1844/20000 Training Loss: 0.05135431885719299\n",
      "Epoch 1845/20000 Training Loss: 0.04960373416543007\n",
      "Epoch 1846/20000 Training Loss: 0.06366120278835297\n",
      "Epoch 1847/20000 Training Loss: 0.07389282435178757\n",
      "Epoch 1848/20000 Training Loss: 0.05613303557038307\n",
      "Epoch 1849/20000 Training Loss: 0.058772046118974686\n",
      "Epoch 1850/20000 Training Loss: 0.07289722561836243\n",
      "Epoch 1850/20000 Validation Loss: 0.07294534146785736\n",
      "Epoch 1851/20000 Training Loss: 0.06850980222225189\n",
      "Epoch 1852/20000 Training Loss: 0.056188683956861496\n",
      "Epoch 1853/20000 Training Loss: 0.058918680995702744\n",
      "Epoch 1854/20000 Training Loss: 0.0564684234559536\n",
      "Epoch 1855/20000 Training Loss: 0.05129917338490486\n",
      "Epoch 1856/20000 Training Loss: 0.07346249371767044\n",
      "Epoch 1857/20000 Training Loss: 0.06619150936603546\n",
      "Epoch 1858/20000 Training Loss: 0.061608415096998215\n",
      "Epoch 1859/20000 Training Loss: 0.06375125795602798\n",
      "Epoch 1860/20000 Training Loss: 0.061913054436445236\n",
      "Epoch 1860/20000 Validation Loss: 0.07785573601722717\n",
      "Epoch 1861/20000 Training Loss: 0.04830092191696167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1862/20000 Training Loss: 0.05101287364959717\n",
      "Epoch 1863/20000 Training Loss: 0.060457926243543625\n",
      "Epoch 1864/20000 Training Loss: 0.0511336624622345\n",
      "Epoch 1865/20000 Training Loss: 0.07620856165885925\n",
      "Epoch 1866/20000 Training Loss: 0.043990153819322586\n",
      "Epoch 1867/20000 Training Loss: 0.053348492830991745\n",
      "Epoch 1868/20000 Training Loss: 0.04753671959042549\n",
      "Epoch 1869/20000 Training Loss: 0.05746239796280861\n",
      "Epoch 1870/20000 Training Loss: 0.0818897932767868\n",
      "Epoch 1870/20000 Validation Loss: 0.05916298180818558\n",
      "Epoch 1871/20000 Training Loss: 0.05986500158905983\n",
      "Epoch 1872/20000 Training Loss: 0.05633428692817688\n",
      "Epoch 1873/20000 Training Loss: 0.061021313071250916\n",
      "Epoch 1874/20000 Training Loss: 0.05206359550356865\n",
      "Epoch 1875/20000 Training Loss: 0.048233434557914734\n",
      "Epoch 1876/20000 Training Loss: 0.049913182854652405\n",
      "Epoch 1877/20000 Training Loss: 0.043521374464035034\n",
      "Epoch 1878/20000 Training Loss: 0.048676252365112305\n",
      "Epoch 1879/20000 Training Loss: 0.05034726858139038\n",
      "Epoch 1880/20000 Training Loss: 0.05519215390086174\n",
      "Epoch 1880/20000 Validation Loss: 0.0703236311674118\n",
      "Epoch 1881/20000 Training Loss: 0.061037033796310425\n",
      "Epoch 1882/20000 Training Loss: 0.047778233885765076\n",
      "Epoch 1883/20000 Training Loss: 0.06636179238557816\n",
      "Epoch 1884/20000 Training Loss: 0.0755971297621727\n",
      "Epoch 1885/20000 Training Loss: 0.05699856951832771\n",
      "Epoch 1886/20000 Training Loss: 0.06606636196374893\n",
      "Epoch 1887/20000 Training Loss: 0.07831982523202896\n",
      "Epoch 1888/20000 Training Loss: 0.05284171178936958\n",
      "Epoch 1889/20000 Training Loss: 0.06521794945001602\n",
      "Epoch 1890/20000 Training Loss: 0.06913228332996368\n",
      "Epoch 1890/20000 Validation Loss: 0.07145865261554718\n",
      "Epoch 1891/20000 Training Loss: 0.0489390604197979\n",
      "Epoch 1892/20000 Training Loss: 0.059950023889541626\n",
      "Epoch 1893/20000 Training Loss: 0.07445872575044632\n",
      "Epoch 1894/20000 Training Loss: 0.09678492695093155\n",
      "Epoch 1895/20000 Training Loss: 0.05225731059908867\n",
      "Epoch 1896/20000 Training Loss: 0.06828192621469498\n",
      "Epoch 1897/20000 Training Loss: 0.053906213492155075\n",
      "Epoch 1898/20000 Training Loss: 0.050300419330596924\n",
      "Epoch 1899/20000 Training Loss: 0.065170057117939\n",
      "Epoch 1900/20000 Training Loss: 0.0449950248003006\n",
      "Epoch 1900/20000 Validation Loss: 0.05674847960472107\n",
      "Epoch 1901/20000 Training Loss: 0.05692416429519653\n",
      "Epoch 1902/20000 Training Loss: 0.05794262886047363\n",
      "Epoch 1903/20000 Training Loss: 0.0400557778775692\n",
      "Epoch 1904/20000 Training Loss: 0.055000096559524536\n",
      "Epoch 1905/20000 Training Loss: 0.04953543841838837\n",
      "Epoch 1906/20000 Training Loss: 0.04654837027192116\n",
      "Epoch 1907/20000 Training Loss: 0.08319827169179916\n",
      "Epoch 1908/20000 Training Loss: 0.058873530477285385\n",
      "Epoch 1909/20000 Training Loss: 0.06292856484651566\n",
      "Epoch 1910/20000 Training Loss: 0.0646897628903389\n",
      "Epoch 1910/20000 Validation Loss: 0.07456763088703156\n",
      "Epoch 1911/20000 Training Loss: 0.052457693964242935\n",
      "Epoch 1912/20000 Training Loss: 0.06454838067293167\n",
      "Epoch 1913/20000 Training Loss: 0.04451759532094002\n",
      "Epoch 1914/20000 Training Loss: 0.050620004534721375\n",
      "Epoch 1915/20000 Training Loss: 0.06324150413274765\n",
      "Epoch 1916/20000 Training Loss: 0.043065816164016724\n",
      "Epoch 1917/20000 Training Loss: 0.06285583972930908\n",
      "Epoch 1918/20000 Training Loss: 0.048356637358665466\n",
      "Epoch 1919/20000 Training Loss: 0.05943779647350311\n",
      "Epoch 1920/20000 Training Loss: 0.05494880676269531\n",
      "Epoch 1920/20000 Validation Loss: 0.04966183006763458\n",
      "Epoch 1921/20000 Training Loss: 0.04630915820598602\n",
      "Epoch 1922/20000 Training Loss: 0.05918646976351738\n",
      "Epoch 1923/20000 Training Loss: 0.07300865650177002\n",
      "Epoch 1924/20000 Training Loss: 0.0528174452483654\n",
      "Epoch 1925/20000 Training Loss: 0.04758046194911003\n",
      "Epoch 1926/20000 Training Loss: 0.05006864666938782\n",
      "Epoch 1927/20000 Training Loss: 0.06239108368754387\n",
      "Epoch 1928/20000 Training Loss: 0.06335783004760742\n",
      "Epoch 1929/20000 Training Loss: 0.05454261973500252\n",
      "Epoch 1930/20000 Training Loss: 0.06641637533903122\n",
      "Epoch 1930/20000 Validation Loss: 0.040852148085832596\n",
      "Epoch 1931/20000 Training Loss: 0.0504438690841198\n",
      "Epoch 1932/20000 Training Loss: 0.06571322679519653\n",
      "Epoch 1933/20000 Training Loss: 0.04543326422572136\n",
      "Epoch 1934/20000 Training Loss: 0.07016909122467041\n",
      "Epoch 1935/20000 Training Loss: 0.06548500806093216\n",
      "Epoch 1936/20000 Training Loss: 0.06511274725198746\n",
      "Epoch 1937/20000 Training Loss: 0.04332192242145538\n",
      "Epoch 1938/20000 Training Loss: 0.05707014724612236\n",
      "Epoch 1939/20000 Training Loss: 0.07622116059064865\n",
      "Epoch 1940/20000 Training Loss: 0.059998851269483566\n",
      "Epoch 1940/20000 Validation Loss: 0.06757728010416031\n",
      "Epoch 1941/20000 Training Loss: 0.04848155379295349\n",
      "Epoch 1942/20000 Training Loss: 0.05771398916840553\n",
      "Epoch 1943/20000 Training Loss: 0.05118129774928093\n",
      "Epoch 1944/20000 Training Loss: 0.05809226632118225\n",
      "Epoch 1945/20000 Training Loss: 0.04968537390232086\n",
      "Epoch 1946/20000 Training Loss: 0.060943156480789185\n",
      "Epoch 1947/20000 Training Loss: 0.05161285772919655\n",
      "Epoch 1948/20000 Training Loss: 0.05371687188744545\n",
      "Epoch 1949/20000 Training Loss: 0.07614567130804062\n",
      "Epoch 1950/20000 Training Loss: 0.04264582693576813\n",
      "Epoch 1950/20000 Validation Loss: 0.06530735641717911\n",
      "Epoch 1951/20000 Training Loss: 0.05565083026885986\n",
      "Epoch 1952/20000 Training Loss: 0.042757581919431686\n",
      "Epoch 1953/20000 Training Loss: 0.05218641832470894\n",
      "Epoch 1954/20000 Training Loss: 0.06353932619094849\n",
      "Epoch 1955/20000 Training Loss: 0.05081337317824364\n",
      "Epoch 1956/20000 Training Loss: 0.053329769521951675\n",
      "Epoch 1957/20000 Training Loss: 0.06262482702732086\n",
      "Epoch 1958/20000 Training Loss: 0.060647543519735336\n",
      "Epoch 1959/20000 Training Loss: 0.05172617733478546\n",
      "Epoch 1960/20000 Training Loss: 0.06331977248191833\n",
      "Epoch 1960/20000 Validation Loss: 0.061009399592876434\n",
      "Epoch 1961/20000 Training Loss: 0.07105830311775208\n",
      "Epoch 1962/20000 Training Loss: 0.05709543451666832\n",
      "Epoch 1963/20000 Training Loss: 0.05765943229198456\n",
      "Epoch 1964/20000 Training Loss: 0.058711785823106766\n",
      "Epoch 1965/20000 Training Loss: 0.0345500223338604\n",
      "Epoch 1966/20000 Training Loss: 0.0681862011551857\n",
      "Epoch 1967/20000 Training Loss: 0.056372564285993576\n",
      "Epoch 1968/20000 Training Loss: 0.05508542060852051\n",
      "Epoch 1969/20000 Training Loss: 0.09200756996870041\n",
      "Epoch 1970/20000 Training Loss: 0.06059234216809273\n",
      "Epoch 1970/20000 Validation Loss: 0.05861527472734451\n",
      "Epoch 1971/20000 Training Loss: 0.06222480908036232\n",
      "Epoch 1972/20000 Training Loss: 0.054806340485811234\n",
      "Epoch 1973/20000 Training Loss: 0.04704867675900459\n",
      "Epoch 1974/20000 Training Loss: 0.054053325206041336\n",
      "Epoch 1975/20000 Training Loss: 0.07876760512590408\n",
      "Epoch 1976/20000 Training Loss: 0.062350690364837646\n",
      "Epoch 1977/20000 Training Loss: 0.06107914447784424\n",
      "Epoch 1978/20000 Training Loss: 0.04483054205775261\n",
      "Epoch 1979/20000 Training Loss: 0.08273735642433167\n",
      "Epoch 1980/20000 Training Loss: 0.056741297245025635\n",
      "Epoch 1980/20000 Validation Loss: 0.08955024182796478\n",
      "Epoch 1981/20000 Training Loss: 0.061744093894958496\n",
      "Epoch 1982/20000 Training Loss: 0.05421547219157219\n",
      "Epoch 1983/20000 Training Loss: 0.06229966878890991\n",
      "Epoch 1984/20000 Training Loss: 0.06315930932760239\n",
      "Epoch 1985/20000 Training Loss: 0.07057598233222961\n",
      "Epoch 1986/20000 Training Loss: 0.052351903170347214\n",
      "Epoch 1987/20000 Training Loss: 0.061473067849874496\n",
      "Epoch 1988/20000 Training Loss: 0.06130224093794823\n",
      "Epoch 1989/20000 Training Loss: 0.06294264644384384\n",
      "Epoch 1990/20000 Training Loss: 0.07598903775215149\n",
      "Epoch 1990/20000 Validation Loss: 0.05105484649538994\n",
      "Epoch 1991/20000 Training Loss: 0.06230154633522034\n",
      "Epoch 1992/20000 Training Loss: 0.06413675099611282\n",
      "Epoch 1993/20000 Training Loss: 0.051599759608507156\n",
      "Epoch 1994/20000 Training Loss: 0.046721119433641434\n",
      "Epoch 1995/20000 Training Loss: 0.06410533934831619\n",
      "Epoch 1996/20000 Training Loss: 0.05661432072520256\n",
      "Epoch 1997/20000 Training Loss: 0.05483945086598396\n",
      "Epoch 1998/20000 Training Loss: 0.05578377842903137\n",
      "Epoch 1999/20000 Training Loss: 0.04775847867131233\n",
      "Epoch 2000/20000 Training Loss: 0.05531485006213188\n",
      "Epoch 2000/20000 Validation Loss: 0.050900086760520935\n",
      "Epoch 2001/20000 Training Loss: 0.0576351024210453\n",
      "Epoch 2002/20000 Training Loss: 0.06422653049230576\n",
      "Epoch 2003/20000 Training Loss: 0.04948526993393898\n",
      "Epoch 2004/20000 Training Loss: 0.052901994436979294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2005/20000 Training Loss: 0.05785199627280235\n",
      "Epoch 2006/20000 Training Loss: 0.06409498304128647\n",
      "Epoch 2007/20000 Training Loss: 0.04161877557635307\n",
      "Epoch 2008/20000 Training Loss: 0.0803631916642189\n",
      "Epoch 2009/20000 Training Loss: 0.08014415949583054\n",
      "Epoch 2010/20000 Training Loss: 0.052553270012140274\n",
      "Epoch 2010/20000 Validation Loss: 0.05791262537240982\n",
      "Epoch 2011/20000 Training Loss: 0.042628198862075806\n",
      "Epoch 2012/20000 Training Loss: 0.049776289612054825\n",
      "Epoch 2013/20000 Training Loss: 0.057650696486234665\n",
      "Epoch 2014/20000 Training Loss: 0.06456097215414047\n",
      "Epoch 2015/20000 Training Loss: 0.06307470798492432\n",
      "Epoch 2016/20000 Training Loss: 0.042070209980010986\n",
      "Epoch 2017/20000 Training Loss: 0.056719690561294556\n",
      "Epoch 2018/20000 Training Loss: 0.051040228456258774\n",
      "Epoch 2019/20000 Training Loss: 0.0669308602809906\n",
      "Epoch 2020/20000 Training Loss: 0.060714736580848694\n",
      "Epoch 2020/20000 Validation Loss: 0.0621100589632988\n",
      "Epoch 2021/20000 Training Loss: 0.062487661838531494\n",
      "Epoch 2022/20000 Training Loss: 0.06016892194747925\n",
      "Epoch 2023/20000 Training Loss: 0.07000590115785599\n",
      "Epoch 2024/20000 Training Loss: 0.055121298879384995\n",
      "Epoch 2025/20000 Training Loss: 0.047420158982276917\n",
      "Epoch 2026/20000 Training Loss: 0.052136003971099854\n",
      "Epoch 2027/20000 Training Loss: 0.05308714881539345\n",
      "Epoch 2028/20000 Training Loss: 0.05259838327765465\n",
      "Epoch 2029/20000 Training Loss: 0.040097784250974655\n",
      "Epoch 2030/20000 Training Loss: 0.08280172199010849\n",
      "Epoch 2030/20000 Validation Loss: 0.060807086527347565\n",
      "Epoch 2031/20000 Training Loss: 0.040676429867744446\n",
      "Epoch 2032/20000 Training Loss: 0.05431460216641426\n",
      "Epoch 2033/20000 Training Loss: 0.053708355873823166\n",
      "Epoch 2034/20000 Training Loss: 0.05312475562095642\n",
      "Epoch 2035/20000 Training Loss: 0.06748559325933456\n",
      "Epoch 2036/20000 Training Loss: 0.03334131836891174\n",
      "Epoch 2037/20000 Training Loss: 0.0516924113035202\n",
      "Epoch 2038/20000 Training Loss: 0.046104203909635544\n",
      "Epoch 2039/20000 Training Loss: 0.05912141129374504\n",
      "Epoch 2040/20000 Training Loss: 0.05225459113717079\n",
      "Epoch 2040/20000 Validation Loss: 0.04568478465080261\n",
      "Epoch 2041/20000 Training Loss: 0.05263100937008858\n",
      "Epoch 2042/20000 Training Loss: 0.0913887545466423\n",
      "Epoch 2043/20000 Training Loss: 0.065392404794693\n",
      "Epoch 2044/20000 Training Loss: 0.05920581892132759\n",
      "Epoch 2045/20000 Training Loss: 0.053185999393463135\n",
      "Epoch 2046/20000 Training Loss: 0.07892601937055588\n",
      "Epoch 2047/20000 Training Loss: 0.08706565946340561\n",
      "Epoch 2048/20000 Training Loss: 0.06531465798616409\n",
      "Epoch 2049/20000 Training Loss: 0.05968059226870537\n",
      "Epoch 2050/20000 Training Loss: 0.06257612258195877\n",
      "Epoch 2050/20000 Validation Loss: 0.0662182867527008\n",
      "Epoch 2051/20000 Training Loss: 0.07462021708488464\n",
      "Epoch 2052/20000 Training Loss: 0.07241927087306976\n",
      "Epoch 2053/20000 Training Loss: 0.08554700016975403\n",
      "Epoch 2054/20000 Training Loss: 0.05669650062918663\n",
      "Epoch 2055/20000 Training Loss: 0.054550718516111374\n",
      "Epoch 2056/20000 Training Loss: 0.06744098663330078\n",
      "Epoch 2057/20000 Training Loss: 0.04816049337387085\n",
      "Epoch 2058/20000 Training Loss: 0.049440398812294006\n",
      "Epoch 2059/20000 Training Loss: 0.05853273347020149\n",
      "Epoch 2060/20000 Training Loss: 0.051564771682024\n",
      "Epoch 2060/20000 Validation Loss: 0.0631314367055893\n",
      "Epoch 2061/20000 Training Loss: 0.06462796777486801\n",
      "Epoch 2062/20000 Training Loss: 0.04659650847315788\n",
      "Epoch 2063/20000 Training Loss: 0.06029327213764191\n",
      "Epoch 2064/20000 Training Loss: 0.04235456511378288\n",
      "Epoch 2065/20000 Training Loss: 0.05088604614138603\n",
      "Epoch 2066/20000 Training Loss: 0.05151115730404854\n",
      "Epoch 2067/20000 Training Loss: 0.05806869640946388\n",
      "Epoch 2068/20000 Training Loss: 0.06377662718296051\n",
      "Epoch 2069/20000 Training Loss: 0.06176547333598137\n",
      "Epoch 2070/20000 Training Loss: 0.06332337111234665\n",
      "Epoch 2070/20000 Validation Loss: 0.05770563334226608\n",
      "Epoch 2071/20000 Training Loss: 0.07119966298341751\n",
      "Epoch 2072/20000 Training Loss: 0.05090322718024254\n",
      "Epoch 2073/20000 Training Loss: 0.05402187630534172\n",
      "Epoch 2074/20000 Training Loss: 0.05500195920467377\n",
      "Epoch 2075/20000 Training Loss: 0.07713575661182404\n",
      "Epoch 2076/20000 Training Loss: 0.0605769045650959\n",
      "Epoch 2077/20000 Training Loss: 0.04388539865612984\n",
      "Epoch 2078/20000 Training Loss: 0.05834446847438812\n",
      "Epoch 2079/20000 Training Loss: 0.06385546922683716\n",
      "Epoch 2080/20000 Training Loss: 0.050035733729600906\n",
      "Epoch 2080/20000 Validation Loss: 0.05022727698087692\n",
      "Epoch 2081/20000 Training Loss: 0.06149645522236824\n",
      "Epoch 2082/20000 Training Loss: 0.06734586507081985\n",
      "Epoch 2083/20000 Training Loss: 0.0630790963768959\n",
      "Epoch 2084/20000 Training Loss: 0.07120043784379959\n",
      "Epoch 2085/20000 Training Loss: 0.038868498057127\n",
      "Epoch 2086/20000 Training Loss: 0.07460522651672363\n",
      "Epoch 2087/20000 Training Loss: 0.0614967942237854\n",
      "Epoch 2088/20000 Training Loss: 0.06168346107006073\n",
      "Epoch 2089/20000 Training Loss: 0.05647013708949089\n",
      "Epoch 2090/20000 Training Loss: 0.06825356930494308\n",
      "Epoch 2090/20000 Validation Loss: 0.04626297950744629\n",
      "Epoch 2091/20000 Training Loss: 0.060378026217222214\n",
      "Epoch 2092/20000 Training Loss: 0.04315345361828804\n",
      "Epoch 2093/20000 Training Loss: 0.05173850432038307\n",
      "Epoch 2094/20000 Training Loss: 0.06244020536541939\n",
      "Epoch 2095/20000 Training Loss: 0.049754004925489426\n",
      "Epoch 2096/20000 Training Loss: 0.06512124836444855\n",
      "Epoch 2097/20000 Training Loss: 0.055042851716279984\n",
      "Epoch 2098/20000 Training Loss: 0.06247687712311745\n",
      "Epoch 2099/20000 Training Loss: 0.06459017843008041\n",
      "Epoch 2100/20000 Training Loss: 0.06457358598709106\n",
      "Epoch 2100/20000 Validation Loss: 0.06108167767524719\n",
      "Epoch 2101/20000 Training Loss: 0.06708347052335739\n",
      "Epoch 2102/20000 Training Loss: 0.11127173900604248\n",
      "Epoch 2103/20000 Training Loss: 0.05183352902531624\n",
      "Epoch 2104/20000 Training Loss: 0.054048195481300354\n",
      "Epoch 2105/20000 Training Loss: 0.07802779227495193\n",
      "Epoch 2106/20000 Training Loss: 0.06739939004182816\n",
      "Epoch 2107/20000 Training Loss: 0.0540953166782856\n",
      "Epoch 2108/20000 Training Loss: 0.08631131052970886\n",
      "Epoch 2109/20000 Training Loss: 0.058193277567625046\n",
      "Epoch 2110/20000 Training Loss: 0.08753182739019394\n",
      "Epoch 2110/20000 Validation Loss: 0.05706113204360008\n",
      "Epoch 2111/20000 Training Loss: 0.07879006117582321\n",
      "Epoch 2112/20000 Training Loss: 0.06230754032731056\n",
      "Epoch 2113/20000 Training Loss: 0.06838426738977432\n",
      "Epoch 2114/20000 Training Loss: 0.05196499451994896\n",
      "Epoch 2115/20000 Training Loss: 0.06244179233908653\n",
      "Epoch 2116/20000 Training Loss: 0.04227973148226738\n",
      "Epoch 2117/20000 Training Loss: 0.04876282811164856\n",
      "Epoch 2118/20000 Training Loss: 0.06104886531829834\n",
      "Epoch 2119/20000 Training Loss: 0.06587017327547073\n",
      "Epoch 2120/20000 Training Loss: 0.05250975862145424\n",
      "Epoch 2120/20000 Validation Loss: 0.053407102823257446\n",
      "Epoch 2121/20000 Training Loss: 0.05757172033190727\n",
      "Epoch 2122/20000 Training Loss: 0.06247152015566826\n",
      "Epoch 2123/20000 Training Loss: 0.04554569721221924\n",
      "Epoch 2124/20000 Training Loss: 0.07184287905693054\n",
      "Epoch 2125/20000 Training Loss: 0.08072913438081741\n",
      "Epoch 2126/20000 Training Loss: 0.08475890755653381\n",
      "Epoch 2127/20000 Training Loss: 0.06705359369516373\n",
      "Epoch 2128/20000 Training Loss: 0.06632662564516068\n",
      "Epoch 2129/20000 Training Loss: 0.049776043742895126\n",
      "Epoch 2130/20000 Training Loss: 0.07897324115037918\n",
      "Epoch 2130/20000 Validation Loss: 0.07558785378932953\n",
      "Epoch 2131/20000 Training Loss: 0.06437557190656662\n",
      "Epoch 2132/20000 Training Loss: 0.05498237535357475\n",
      "Epoch 2133/20000 Training Loss: 0.05010216310620308\n",
      "Epoch 2134/20000 Training Loss: 0.04999477043747902\n",
      "Epoch 2135/20000 Training Loss: 0.05873727798461914\n",
      "Epoch 2136/20000 Training Loss: 0.04686172679066658\n",
      "Epoch 2137/20000 Training Loss: 0.08052310347557068\n",
      "Epoch 2138/20000 Training Loss: 0.06342287361621857\n",
      "Epoch 2139/20000 Training Loss: 0.03684348240494728\n",
      "Epoch 2140/20000 Training Loss: 0.05829915031790733\n",
      "Epoch 2140/20000 Validation Loss: 0.04772862792015076\n",
      "Epoch 2141/20000 Training Loss: 0.06389066576957703\n",
      "Epoch 2142/20000 Training Loss: 0.060643550008535385\n",
      "Epoch 2143/20000 Training Loss: 0.08176293969154358\n",
      "Epoch 2144/20000 Training Loss: 0.06718713045120239\n",
      "Epoch 2145/20000 Training Loss: 0.048214804381132126\n",
      "Epoch 2146/20000 Training Loss: 0.058818161487579346\n",
      "Epoch 2147/20000 Training Loss: 0.07216576486825943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2148/20000 Training Loss: 0.07731089740991592\n",
      "Epoch 2149/20000 Training Loss: 0.06723081320524216\n",
      "Epoch 2150/20000 Training Loss: 0.048744019120931625\n",
      "Epoch 2150/20000 Validation Loss: 0.0689893588423729\n",
      "Epoch 2151/20000 Training Loss: 0.049014318734407425\n",
      "Epoch 2152/20000 Training Loss: 0.057489410042762756\n",
      "Epoch 2153/20000 Training Loss: 0.05884668603539467\n",
      "Epoch 2154/20000 Training Loss: 0.055082056671381\n",
      "Epoch 2155/20000 Training Loss: 0.05549629405140877\n",
      "Epoch 2156/20000 Training Loss: 0.06745139509439468\n",
      "Epoch 2157/20000 Training Loss: 0.04991742596030235\n",
      "Epoch 2158/20000 Training Loss: 0.058154184371232986\n",
      "Epoch 2159/20000 Training Loss: 0.058476269245147705\n",
      "Epoch 2160/20000 Training Loss: 0.06741919368505478\n",
      "Epoch 2160/20000 Validation Loss: 0.05338180810213089\n",
      "Epoch 2161/20000 Training Loss: 0.05217991769313812\n",
      "Epoch 2162/20000 Training Loss: 0.06872692704200745\n",
      "Epoch 2163/20000 Training Loss: 0.03936298191547394\n",
      "Epoch 2164/20000 Training Loss: 0.06280496716499329\n",
      "Epoch 2165/20000 Training Loss: 0.05331697687506676\n",
      "Epoch 2166/20000 Training Loss: 0.05232705548405647\n",
      "Epoch 2167/20000 Training Loss: 0.06606302410364151\n",
      "Epoch 2168/20000 Training Loss: 0.04719812050461769\n",
      "Epoch 2169/20000 Training Loss: 0.05302831158041954\n",
      "Epoch 2170/20000 Training Loss: 0.0469781868159771\n",
      "Epoch 2170/20000 Validation Loss: 0.06608131527900696\n",
      "Epoch 2171/20000 Training Loss: 0.06943570822477341\n",
      "Epoch 2172/20000 Training Loss: 0.06890647858381271\n",
      "Epoch 2173/20000 Training Loss: 0.06901288777589798\n",
      "Epoch 2174/20000 Training Loss: 0.06074061989784241\n",
      "Epoch 2175/20000 Training Loss: 0.07395925372838974\n",
      "Epoch 2176/20000 Training Loss: 0.07191675901412964\n",
      "Epoch 2177/20000 Training Loss: 0.04633261263370514\n",
      "Epoch 2178/20000 Training Loss: 0.06875702738761902\n",
      "Epoch 2179/20000 Training Loss: 0.05671245977282524\n",
      "Epoch 2180/20000 Training Loss: 0.051069796085357666\n",
      "Epoch 2180/20000 Validation Loss: 0.08156272768974304\n",
      "Epoch 2181/20000 Training Loss: 0.05186345800757408\n",
      "Epoch 2182/20000 Training Loss: 0.05924992635846138\n",
      "Epoch 2183/20000 Training Loss: 0.06070905551314354\n",
      "Epoch 2184/20000 Training Loss: 0.0773950144648552\n",
      "Epoch 2185/20000 Training Loss: 0.0634005144238472\n",
      "Epoch 2186/20000 Training Loss: 0.050486933439970016\n",
      "Epoch 2187/20000 Training Loss: 0.06010788679122925\n",
      "Epoch 2188/20000 Training Loss: 0.048575375229120255\n",
      "Epoch 2189/20000 Training Loss: 0.06367511302232742\n",
      "Epoch 2190/20000 Training Loss: 0.050471093505620956\n",
      "Epoch 2190/20000 Validation Loss: 0.055342622101306915\n",
      "Epoch 2191/20000 Training Loss: 0.04880839213728905\n",
      "Epoch 2192/20000 Training Loss: 0.0637134537100792\n",
      "Epoch 2193/20000 Training Loss: 0.049062054604291916\n",
      "Epoch 2194/20000 Training Loss: 0.049238499253988266\n",
      "Epoch 2195/20000 Training Loss: 0.06965789198875427\n",
      "Epoch 2196/20000 Training Loss: 0.05418609082698822\n",
      "Epoch 2197/20000 Training Loss: 0.05154351517558098\n",
      "Epoch 2198/20000 Training Loss: 0.06259740889072418\n",
      "Epoch 2199/20000 Training Loss: 0.048209089785814285\n",
      "Epoch 2200/20000 Training Loss: 0.07402665168046951\n",
      "Epoch 2200/20000 Validation Loss: 0.05039972811937332\n",
      "Epoch 2201/20000 Training Loss: 0.0567915141582489\n",
      "Epoch 2202/20000 Training Loss: 0.05424074828624725\n",
      "Epoch 2203/20000 Training Loss: 0.07484963536262512\n",
      "Epoch 2204/20000 Training Loss: 0.05489615723490715\n",
      "Epoch 2205/20000 Training Loss: 0.05614129826426506\n",
      "Epoch 2206/20000 Training Loss: 0.058999910950660706\n",
      "Epoch 2207/20000 Training Loss: 0.0418069027364254\n",
      "Epoch 2208/20000 Training Loss: 0.05931808426976204\n",
      "Epoch 2209/20000 Training Loss: 0.04155980423092842\n",
      "Epoch 2210/20000 Training Loss: 0.061322301626205444\n",
      "Epoch 2210/20000 Validation Loss: 0.0793011412024498\n",
      "Epoch 2211/20000 Training Loss: 0.05536729469895363\n",
      "Epoch 2212/20000 Training Loss: 0.0673736184835434\n",
      "Epoch 2213/20000 Training Loss: 0.07962317019701004\n",
      "Epoch 2214/20000 Training Loss: 0.06611333042383194\n",
      "Epoch 2215/20000 Training Loss: 0.05415736138820648\n",
      "Epoch 2216/20000 Training Loss: 0.04639387130737305\n",
      "Epoch 2217/20000 Training Loss: 0.047302380204200745\n",
      "Epoch 2218/20000 Training Loss: 0.060655757784843445\n",
      "Epoch 2219/20000 Training Loss: 0.06307073682546616\n",
      "Epoch 2220/20000 Training Loss: 0.06428108364343643\n",
      "Epoch 2220/20000 Validation Loss: 0.08755947649478912\n",
      "Epoch 2221/20000 Training Loss: 0.0531136654317379\n",
      "Epoch 2222/20000 Training Loss: 0.054582130163908005\n",
      "Epoch 2223/20000 Training Loss: 0.06008875370025635\n",
      "Epoch 2224/20000 Training Loss: 0.052392780780792236\n",
      "Epoch 2225/20000 Training Loss: 0.07629186660051346\n",
      "Epoch 2226/20000 Training Loss: 0.06522982567548752\n",
      "Epoch 2227/20000 Training Loss: 0.06970908492803574\n",
      "Epoch 2228/20000 Training Loss: 0.0513659231364727\n",
      "Epoch 2229/20000 Training Loss: 0.061179015785455704\n",
      "Epoch 2230/20000 Training Loss: 0.06483324617147446\n",
      "Epoch 2230/20000 Validation Loss: 0.06683178246021271\n",
      "Epoch 2231/20000 Training Loss: 0.06510434299707413\n",
      "Epoch 2232/20000 Training Loss: 0.043164629489183426\n",
      "Epoch 2233/20000 Training Loss: 0.04722842574119568\n",
      "Epoch 2234/20000 Training Loss: 0.04510675370693207\n",
      "Epoch 2235/20000 Training Loss: 0.05330770090222359\n",
      "Epoch 2236/20000 Training Loss: 0.06046465411782265\n",
      "Epoch 2237/20000 Training Loss: 0.05248681828379631\n",
      "Epoch 2238/20000 Training Loss: 0.0568026639521122\n",
      "Epoch 2239/20000 Training Loss: 0.05228877067565918\n",
      "Epoch 2240/20000 Training Loss: 0.06338900327682495\n",
      "Epoch 2240/20000 Validation Loss: 0.05359959974884987\n",
      "Epoch 2241/20000 Training Loss: 0.06525081396102905\n",
      "Epoch 2242/20000 Training Loss: 0.04702996090054512\n",
      "Epoch 2243/20000 Training Loss: 0.0628993883728981\n",
      "Epoch 2244/20000 Training Loss: 0.07761328667402267\n",
      "Epoch 2245/20000 Training Loss: 0.0688352957367897\n",
      "Epoch 2246/20000 Training Loss: 0.04358215257525444\n",
      "Epoch 2247/20000 Training Loss: 0.06423182040452957\n",
      "Epoch 2248/20000 Training Loss: 0.05158152058720589\n",
      "Epoch 2249/20000 Training Loss: 0.05486389622092247\n",
      "Epoch 2250/20000 Training Loss: 0.07783486694097519\n",
      "Epoch 2250/20000 Validation Loss: 0.062096185982227325\n",
      "Epoch 2251/20000 Training Loss: 0.059640273451805115\n",
      "Epoch 2252/20000 Training Loss: 0.051385071128606796\n",
      "Epoch 2253/20000 Training Loss: 0.07300368696451187\n",
      "Epoch 2254/20000 Training Loss: 0.06571132689714432\n",
      "Epoch 2255/20000 Training Loss: 0.062004879117012024\n",
      "Epoch 2256/20000 Training Loss: 0.04949367046356201\n",
      "Epoch 2257/20000 Training Loss: 0.04838203266263008\n",
      "Epoch 2258/20000 Training Loss: 0.04164360836148262\n",
      "Epoch 2259/20000 Training Loss: 0.05802074074745178\n",
      "Epoch 2260/20000 Training Loss: 0.06809636205434799\n",
      "Epoch 2260/20000 Validation Loss: 0.06360354274511337\n",
      "Epoch 2261/20000 Training Loss: 0.07196725159883499\n",
      "Epoch 2262/20000 Training Loss: 0.0505814291536808\n",
      "Epoch 2263/20000 Training Loss: 0.04518871009349823\n",
      "Epoch 2264/20000 Training Loss: 0.05883641913533211\n",
      "Epoch 2265/20000 Training Loss: 0.04896358773112297\n",
      "Epoch 2266/20000 Training Loss: 0.04920469596982002\n",
      "Epoch 2267/20000 Training Loss: 0.05511145293712616\n",
      "Epoch 2268/20000 Training Loss: 0.06548655033111572\n",
      "Epoch 2269/20000 Training Loss: 0.07222924381494522\n",
      "Epoch 2270/20000 Training Loss: 0.053902518004179\n",
      "Epoch 2270/20000 Validation Loss: 0.05064265429973602\n",
      "Epoch 2271/20000 Training Loss: 0.05910550430417061\n",
      "Epoch 2272/20000 Training Loss: 0.052321117371320724\n",
      "Epoch 2273/20000 Training Loss: 0.07928458601236343\n",
      "Epoch 2274/20000 Training Loss: 0.07410337030887604\n",
      "Epoch 2275/20000 Training Loss: 0.06674084812402725\n",
      "Epoch 2276/20000 Training Loss: 0.06249149516224861\n",
      "Epoch 2277/20000 Training Loss: 0.054841652512550354\n",
      "Epoch 2278/20000 Training Loss: 0.08281614631414413\n",
      "Epoch 2279/20000 Training Loss: 0.04694110155105591\n",
      "Epoch 2280/20000 Training Loss: 0.08530133217573166\n",
      "Epoch 2280/20000 Validation Loss: 0.06403350085020065\n",
      "Epoch 2281/20000 Training Loss: 0.06902366131544113\n",
      "Epoch 2282/20000 Training Loss: 0.06308433413505554\n",
      "Epoch 2283/20000 Training Loss: 0.07612907141447067\n",
      "Epoch 2284/20000 Training Loss: 0.059912826865911484\n",
      "Epoch 2285/20000 Training Loss: 0.05251110717654228\n",
      "Epoch 2286/20000 Training Loss: 0.05955274775624275\n",
      "Epoch 2287/20000 Training Loss: 0.04140086844563484\n",
      "Epoch 2288/20000 Training Loss: 0.048166725784540176\n",
      "Epoch 2289/20000 Training Loss: 0.04969019070267677\n",
      "Epoch 2290/20000 Training Loss: 0.10598483681678772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2290/20000 Validation Loss: 0.08401430398225784\n",
      "Epoch 2291/20000 Training Loss: 0.052720844745635986\n",
      "Epoch 2292/20000 Training Loss: 0.055696338415145874\n",
      "Epoch 2293/20000 Training Loss: 0.06718462705612183\n",
      "Epoch 2294/20000 Training Loss: 0.06519455462694168\n",
      "Epoch 2295/20000 Training Loss: 0.05566389858722687\n",
      "Epoch 2296/20000 Training Loss: 0.06863927096128464\n",
      "Epoch 2297/20000 Training Loss: 0.06104492023587227\n",
      "Epoch 2298/20000 Training Loss: 0.03736232593655586\n",
      "Epoch 2299/20000 Training Loss: 0.06242707371711731\n",
      "Epoch 2300/20000 Training Loss: 0.05929472669959068\n",
      "Epoch 2300/20000 Validation Loss: 0.04538670927286148\n",
      "Epoch 2301/20000 Training Loss: 0.04736773297190666\n",
      "Epoch 2302/20000 Training Loss: 0.06558608263731003\n",
      "Epoch 2303/20000 Training Loss: 0.059419047087430954\n",
      "Epoch 2304/20000 Training Loss: 0.07377894967794418\n",
      "Epoch 2305/20000 Training Loss: 0.07193276286125183\n",
      "Epoch 2306/20000 Training Loss: 0.07359250634908676\n",
      "Epoch 2307/20000 Training Loss: 0.05075496807694435\n",
      "Epoch 2308/20000 Training Loss: 0.06042252108454704\n",
      "Epoch 2309/20000 Training Loss: 0.03754565492272377\n",
      "Epoch 2310/20000 Training Loss: 0.04968641698360443\n",
      "Epoch 2310/20000 Validation Loss: 0.07047756016254425\n",
      "Epoch 2311/20000 Training Loss: 0.04933268204331398\n",
      "Epoch 2312/20000 Training Loss: 0.04666540026664734\n",
      "Epoch 2313/20000 Training Loss: 0.07009801268577576\n",
      "Epoch 2314/20000 Training Loss: 0.06849279999732971\n",
      "Epoch 2315/20000 Training Loss: 0.05914334952831268\n",
      "Epoch 2316/20000 Training Loss: 0.042126238346099854\n",
      "Epoch 2317/20000 Training Loss: 0.059989649802446365\n",
      "Epoch 2318/20000 Training Loss: 0.042386144399642944\n",
      "Epoch 2319/20000 Training Loss: 0.06381478160619736\n",
      "Epoch 2320/20000 Training Loss: 0.041595492511987686\n",
      "Epoch 2320/20000 Validation Loss: 0.04306162893772125\n",
      "Epoch 2321/20000 Training Loss: 0.07808191329240799\n",
      "Epoch 2322/20000 Training Loss: 0.05373094975948334\n",
      "Epoch 2323/20000 Training Loss: 0.06784078478813171\n",
      "Epoch 2324/20000 Training Loss: 0.053436387330293655\n",
      "Epoch 2325/20000 Training Loss: 0.057182133197784424\n",
      "Epoch 2326/20000 Training Loss: 0.06413238495588303\n",
      "Epoch 2327/20000 Training Loss: 0.08714871853590012\n",
      "Epoch 2328/20000 Training Loss: 0.04809834063053131\n",
      "Epoch 2329/20000 Training Loss: 0.061747923493385315\n",
      "Epoch 2330/20000 Training Loss: 0.049040962010622025\n",
      "Epoch 2330/20000 Validation Loss: 0.05912522226572037\n",
      "Epoch 2331/20000 Training Loss: 0.06176503002643585\n",
      "Epoch 2332/20000 Training Loss: 0.05102837085723877\n",
      "Epoch 2333/20000 Training Loss: 0.07001755386590958\n",
      "Epoch 2334/20000 Training Loss: 0.05299374461174011\n",
      "Epoch 2335/20000 Training Loss: 0.04500699043273926\n",
      "Epoch 2336/20000 Training Loss: 0.046538591384887695\n",
      "Epoch 2337/20000 Training Loss: 0.057305023074150085\n",
      "Epoch 2338/20000 Training Loss: 0.04840749502182007\n",
      "Epoch 2339/20000 Training Loss: 0.06659343838691711\n",
      "Epoch 2340/20000 Training Loss: 0.07335846871137619\n",
      "Epoch 2340/20000 Validation Loss: 0.061583224684000015\n",
      "Epoch 2341/20000 Training Loss: 0.07121608406305313\n",
      "Epoch 2342/20000 Training Loss: 0.06568390876054764\n",
      "Epoch 2343/20000 Training Loss: 0.05994008854031563\n",
      "Epoch 2344/20000 Training Loss: 0.06412350386381149\n",
      "Epoch 2345/20000 Training Loss: 0.06529039144515991\n",
      "Epoch 2346/20000 Training Loss: 0.04637554660439491\n",
      "Epoch 2347/20000 Training Loss: 0.04583600163459778\n",
      "Epoch 2348/20000 Training Loss: 0.06468600034713745\n",
      "Epoch 2349/20000 Training Loss: 0.04875833913683891\n",
      "Epoch 2350/20000 Training Loss: 0.05828116461634636\n",
      "Epoch 2350/20000 Validation Loss: 0.06541027128696442\n",
      "Epoch 2351/20000 Training Loss: 0.07299331575632095\n",
      "Epoch 2352/20000 Training Loss: 0.04524858668446541\n",
      "Epoch 2353/20000 Training Loss: 0.05640633404254913\n",
      "Epoch 2354/20000 Training Loss: 0.051491230726242065\n",
      "Epoch 2355/20000 Training Loss: 0.04536052420735359\n",
      "Epoch 2356/20000 Training Loss: 0.06372127681970596\n",
      "Epoch 2357/20000 Training Loss: 0.048695921897888184\n",
      "Epoch 2358/20000 Training Loss: 0.05744447186589241\n",
      "Epoch 2359/20000 Training Loss: 0.0645734965801239\n",
      "Epoch 2360/20000 Training Loss: 0.06616415828466415\n",
      "Epoch 2360/20000 Validation Loss: 0.0696936845779419\n",
      "Epoch 2361/20000 Training Loss: 0.05893316864967346\n",
      "Epoch 2362/20000 Training Loss: 0.059137601405382156\n",
      "Epoch 2363/20000 Training Loss: 0.046605437994003296\n",
      "Epoch 2364/20000 Training Loss: 0.05661641061306\n",
      "Epoch 2365/20000 Training Loss: 0.06489583849906921\n",
      "Epoch 2366/20000 Training Loss: 0.05433289334177971\n",
      "Epoch 2367/20000 Training Loss: 0.058140695095062256\n",
      "Epoch 2368/20000 Training Loss: 0.04563059285283089\n",
      "Epoch 2369/20000 Training Loss: 0.060754965990781784\n",
      "Epoch 2370/20000 Training Loss: 0.06098521873354912\n",
      "Epoch 2370/20000 Validation Loss: 0.04826686158776283\n",
      "Epoch 2371/20000 Training Loss: 0.05097939446568489\n",
      "Epoch 2372/20000 Training Loss: 0.04548455774784088\n",
      "Epoch 2373/20000 Training Loss: 0.07361543923616409\n",
      "Epoch 2374/20000 Training Loss: 0.04651433229446411\n",
      "Epoch 2375/20000 Training Loss: 0.06446438282728195\n",
      "Epoch 2376/20000 Training Loss: 0.06744858622550964\n",
      "Epoch 2377/20000 Training Loss: 0.08621241897344589\n",
      "Epoch 2378/20000 Training Loss: 0.05002276971936226\n",
      "Epoch 2379/20000 Training Loss: 0.05649140104651451\n",
      "Epoch 2380/20000 Training Loss: 0.05843672156333923\n",
      "Epoch 2380/20000 Validation Loss: 0.06502476334571838\n",
      "Epoch 2381/20000 Training Loss: 0.046792928129434586\n",
      "Epoch 2382/20000 Training Loss: 0.05807743966579437\n",
      "Epoch 2383/20000 Training Loss: 0.05797482654452324\n",
      "Epoch 2384/20000 Training Loss: 0.059519294649362564\n",
      "Epoch 2385/20000 Training Loss: 0.07684088498353958\n",
      "Epoch 2386/20000 Training Loss: 0.07604122906923294\n",
      "Epoch 2387/20000 Training Loss: 0.06347490102052689\n",
      "Epoch 2388/20000 Training Loss: 0.05669799819588661\n",
      "Epoch 2389/20000 Training Loss: 0.04857643321156502\n",
      "Epoch 2390/20000 Training Loss: 0.055443376302719116\n",
      "Epoch 2390/20000 Validation Loss: 0.061019062995910645\n",
      "Epoch 2391/20000 Training Loss: 0.05571260675787926\n",
      "Epoch 2392/20000 Training Loss: 0.07228779792785645\n",
      "Epoch 2393/20000 Training Loss: 0.07596925646066666\n",
      "Epoch 2394/20000 Training Loss: 0.053345635533332825\n",
      "Epoch 2395/20000 Training Loss: 0.05895352363586426\n",
      "Epoch 2396/20000 Training Loss: 0.07224751263856888\n",
      "Epoch 2397/20000 Training Loss: 0.04502240940928459\n",
      "Epoch 2398/20000 Training Loss: 0.04413871839642525\n",
      "Epoch 2399/20000 Training Loss: 0.06978482007980347\n",
      "Epoch 2400/20000 Training Loss: 0.05265653133392334\n",
      "Epoch 2400/20000 Validation Loss: 0.061832085251808167\n",
      "Epoch 2401/20000 Training Loss: 0.050499409437179565\n",
      "Epoch 2402/20000 Training Loss: 0.06418392807245255\n",
      "Epoch 2403/20000 Training Loss: 0.07320407032966614\n",
      "Epoch 2404/20000 Training Loss: 0.08128929883241653\n",
      "Epoch 2405/20000 Training Loss: 0.05672070384025574\n",
      "Epoch 2406/20000 Training Loss: 0.05519542098045349\n",
      "Epoch 2407/20000 Training Loss: 0.047119107097387314\n",
      "Epoch 2408/20000 Training Loss: 0.06154553219676018\n",
      "Epoch 2409/20000 Training Loss: 0.06230432912707329\n",
      "Epoch 2410/20000 Training Loss: 0.054847490042448044\n",
      "Epoch 2410/20000 Validation Loss: 0.06899147480726242\n",
      "Epoch 2411/20000 Training Loss: 0.058940861374139786\n",
      "Epoch 2412/20000 Training Loss: 0.05990387126803398\n",
      "Epoch 2413/20000 Training Loss: 0.06562811881303787\n",
      "Epoch 2414/20000 Training Loss: 0.058041930198669434\n",
      "Epoch 2415/20000 Training Loss: 0.07125929743051529\n",
      "Epoch 2416/20000 Training Loss: 0.051485832780599594\n",
      "Epoch 2417/20000 Training Loss: 0.05590740963816643\n",
      "Epoch 2418/20000 Training Loss: 0.05975789949297905\n",
      "Epoch 2419/20000 Training Loss: 0.058882731944322586\n",
      "Epoch 2420/20000 Training Loss: 0.061148811131715775\n",
      "Epoch 2420/20000 Validation Loss: 0.0399257056415081\n",
      "Epoch 2421/20000 Training Loss: 0.08916304260492325\n",
      "Epoch 2422/20000 Training Loss: 0.06452422589063644\n",
      "Epoch 2423/20000 Training Loss: 0.04708675667643547\n",
      "Epoch 2424/20000 Training Loss: 0.05906879901885986\n",
      "Epoch 2425/20000 Training Loss: 0.055719804018735886\n",
      "Epoch 2426/20000 Training Loss: 0.05914023518562317\n",
      "Epoch 2427/20000 Training Loss: 0.07589948922395706\n",
      "Epoch 2428/20000 Training Loss: 0.05124907195568085\n",
      "Epoch 2429/20000 Training Loss: 0.06562088429927826\n",
      "Epoch 2430/20000 Training Loss: 0.05202249065041542\n",
      "Epoch 2430/20000 Validation Loss: 0.07484009116888046\n",
      "Epoch 2431/20000 Training Loss: 0.04340394213795662\n",
      "Epoch 2432/20000 Training Loss: 0.07141246646642685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2433/20000 Training Loss: 0.07279256731271744\n",
      "Epoch 2434/20000 Training Loss: 0.05851922929286957\n",
      "Epoch 2435/20000 Training Loss: 0.05600547417998314\n",
      "Epoch 2436/20000 Training Loss: 0.057721156626939774\n",
      "Epoch 2437/20000 Training Loss: 0.05689645931124687\n",
      "Epoch 2438/20000 Training Loss: 0.07252604514360428\n",
      "Epoch 2439/20000 Training Loss: 0.05665099620819092\n",
      "Epoch 2440/20000 Training Loss: 0.049031276255846024\n",
      "Epoch 2440/20000 Validation Loss: 0.05358658730983734\n",
      "Epoch 2441/20000 Training Loss: 0.07500501722097397\n",
      "Epoch 2442/20000 Training Loss: 0.06649720668792725\n",
      "Epoch 2443/20000 Training Loss: 0.03996185585856438\n",
      "Epoch 2444/20000 Training Loss: 0.04423283413052559\n",
      "Epoch 2445/20000 Training Loss: 0.04828737676143646\n",
      "Epoch 2446/20000 Training Loss: 0.06803752481937408\n",
      "Epoch 2447/20000 Training Loss: 0.04666033387184143\n",
      "Epoch 2448/20000 Training Loss: 0.05428260937333107\n",
      "Epoch 2449/20000 Training Loss: 0.05544939637184143\n",
      "Epoch 2450/20000 Training Loss: 0.0507357120513916\n",
      "Epoch 2450/20000 Validation Loss: 0.06825168430805206\n",
      "Epoch 2451/20000 Training Loss: 0.07689028233289719\n",
      "Epoch 2452/20000 Training Loss: 0.058500926941633224\n",
      "Epoch 2453/20000 Training Loss: 0.04753565788269043\n",
      "Epoch 2454/20000 Training Loss: 0.04181097820401192\n",
      "Epoch 2455/20000 Training Loss: 0.0792461708188057\n",
      "Epoch 2456/20000 Training Loss: 0.059578850865364075\n",
      "Epoch 2457/20000 Training Loss: 0.044482823461294174\n",
      "Epoch 2458/20000 Training Loss: 0.06662075966596603\n",
      "Epoch 2459/20000 Training Loss: 0.0619853176176548\n",
      "Epoch 2460/20000 Training Loss: 0.046472858637571335\n",
      "Epoch 2460/20000 Validation Loss: 0.056570444256067276\n",
      "Epoch 2461/20000 Training Loss: 0.04461698234081268\n",
      "Epoch 2462/20000 Training Loss: 0.05284401401877403\n",
      "Epoch 2463/20000 Training Loss: 0.056300580501556396\n",
      "Epoch 2464/20000 Training Loss: 0.0741131380200386\n",
      "Epoch 2465/20000 Training Loss: 0.07000219076871872\n",
      "Epoch 2466/20000 Training Loss: 0.0617394857108593\n",
      "Epoch 2467/20000 Training Loss: 0.04476303234696388\n",
      "Epoch 2468/20000 Training Loss: 0.039319466799497604\n",
      "Epoch 2469/20000 Training Loss: 0.06191105768084526\n",
      "Epoch 2470/20000 Training Loss: 0.0597422830760479\n",
      "Epoch 2470/20000 Validation Loss: 0.07397496700286865\n",
      "Epoch 2471/20000 Training Loss: 0.06458885222673416\n",
      "Epoch 2472/20000 Training Loss: 0.042051736265420914\n",
      "Epoch 2473/20000 Training Loss: 0.06551099568605423\n",
      "Epoch 2474/20000 Training Loss: 0.0855455994606018\n",
      "Epoch 2475/20000 Training Loss: 0.06987031549215317\n",
      "Epoch 2476/20000 Training Loss: 0.06570836156606674\n",
      "Epoch 2477/20000 Training Loss: 0.048792678862810135\n",
      "Epoch 2478/20000 Training Loss: 0.07207854837179184\n",
      "Epoch 2479/20000 Training Loss: 0.06165925785899162\n",
      "Epoch 2480/20000 Training Loss: 0.04142013564705849\n",
      "Epoch 2480/20000 Validation Loss: 0.055941030383110046\n",
      "Epoch 2481/20000 Training Loss: 0.05750146508216858\n",
      "Epoch 2482/20000 Training Loss: 0.057679932564496994\n",
      "Epoch 2483/20000 Training Loss: 0.0437784381210804\n",
      "Epoch 2484/20000 Training Loss: 0.05091315507888794\n",
      "Epoch 2485/20000 Training Loss: 0.045141104608774185\n",
      "Epoch 2486/20000 Training Loss: 0.07248347252607346\n",
      "Epoch 2487/20000 Training Loss: 0.05334366858005524\n",
      "Epoch 2488/20000 Training Loss: 0.06268585473299026\n",
      "Epoch 2489/20000 Training Loss: 0.05769854411482811\n",
      "Epoch 2490/20000 Training Loss: 0.05961642041802406\n",
      "Epoch 2490/20000 Validation Loss: 0.05595415458083153\n",
      "Epoch 2491/20000 Training Loss: 0.054983120411634445\n",
      "Epoch 2492/20000 Training Loss: 0.07116351276636124\n",
      "Epoch 2493/20000 Training Loss: 0.06731811910867691\n",
      "Epoch 2494/20000 Training Loss: 0.060013074427843094\n",
      "Epoch 2495/20000 Training Loss: 0.05405765399336815\n",
      "Epoch 2496/20000 Training Loss: 0.05578848347067833\n",
      "Epoch 2497/20000 Training Loss: 0.06487508863210678\n",
      "Epoch 2498/20000 Training Loss: 0.04512697458267212\n",
      "Epoch 2499/20000 Training Loss: 0.058998480439186096\n",
      "Epoch 2500/20000 Training Loss: 0.04651544615626335\n",
      "Epoch 2500/20000 Validation Loss: 0.0635259747505188\n",
      "Epoch 2501/20000 Training Loss: 0.05280454456806183\n",
      "Epoch 2502/20000 Training Loss: 0.0665510967373848\n",
      "Epoch 2503/20000 Training Loss: 0.0469416119158268\n",
      "Epoch 2504/20000 Training Loss: 0.04415580630302429\n",
      "Epoch 2505/20000 Training Loss: 0.053987521678209305\n",
      "Epoch 2506/20000 Training Loss: 0.05590492859482765\n",
      "Epoch 2507/20000 Training Loss: 0.054659437388181686\n",
      "Epoch 2508/20000 Training Loss: 0.05984257161617279\n",
      "Epoch 2509/20000 Training Loss: 0.058700304478406906\n",
      "Epoch 2510/20000 Training Loss: 0.060629379004240036\n",
      "Epoch 2510/20000 Validation Loss: 0.0658690333366394\n",
      "Epoch 2511/20000 Training Loss: 0.06695698946714401\n",
      "Epoch 2512/20000 Training Loss: 0.05325109139084816\n",
      "Epoch 2513/20000 Training Loss: 0.05451083183288574\n",
      "Epoch 2514/20000 Training Loss: 0.05862252786755562\n",
      "Epoch 2515/20000 Training Loss: 0.05704766511917114\n",
      "Epoch 2516/20000 Training Loss: 0.05200709030032158\n",
      "Epoch 2517/20000 Training Loss: 0.06374755501747131\n",
      "Epoch 2518/20000 Training Loss: 0.043386250734329224\n",
      "Epoch 2519/20000 Training Loss: 0.06854134798049927\n",
      "Epoch 2520/20000 Training Loss: 0.051050830632448196\n",
      "Epoch 2520/20000 Validation Loss: 0.04299793764948845\n",
      "Epoch 2521/20000 Training Loss: 0.09224089980125427\n",
      "Epoch 2522/20000 Training Loss: 0.057296931743621826\n",
      "Epoch 2523/20000 Training Loss: 0.059171292930841446\n",
      "Epoch 2524/20000 Training Loss: 0.03452615439891815\n",
      "Epoch 2525/20000 Training Loss: 0.0570671372115612\n",
      "Epoch 2526/20000 Training Loss: 0.05922793969511986\n",
      "Epoch 2527/20000 Training Loss: 0.0502108633518219\n",
      "Epoch 2528/20000 Training Loss: 0.0480332225561142\n",
      "Epoch 2529/20000 Training Loss: 0.05189037695527077\n",
      "Epoch 2530/20000 Training Loss: 0.058032866567373276\n",
      "Epoch 2530/20000 Validation Loss: 0.04711167514324188\n",
      "Epoch 2531/20000 Training Loss: 0.04532922804355621\n",
      "Epoch 2532/20000 Training Loss: 0.058987587690353394\n",
      "Epoch 2533/20000 Training Loss: 0.056122004985809326\n",
      "Epoch 2534/20000 Training Loss: 0.06199781596660614\n",
      "Epoch 2535/20000 Training Loss: 0.07027392834424973\n",
      "Epoch 2536/20000 Training Loss: 0.06948501616716385\n",
      "Epoch 2537/20000 Training Loss: 0.048037488013505936\n",
      "Epoch 2538/20000 Training Loss: 0.051989153027534485\n",
      "Epoch 2539/20000 Training Loss: 0.06299907714128494\n",
      "Epoch 2540/20000 Training Loss: 0.04705573245882988\n",
      "Epoch 2540/20000 Validation Loss: 0.060816384851932526\n",
      "Epoch 2541/20000 Training Loss: 0.0573623925447464\n",
      "Epoch 2542/20000 Training Loss: 0.07945581525564194\n",
      "Epoch 2543/20000 Training Loss: 0.07594730705022812\n",
      "Epoch 2544/20000 Training Loss: 0.06507700681686401\n",
      "Epoch 2545/20000 Training Loss: 0.0740232765674591\n",
      "Epoch 2546/20000 Training Loss: 0.07624713331460953\n",
      "Epoch 2547/20000 Training Loss: 0.044851601123809814\n",
      "Epoch 2548/20000 Training Loss: 0.05033272132277489\n",
      "Epoch 2549/20000 Training Loss: 0.04587118327617645\n",
      "Epoch 2550/20000 Training Loss: 0.05561663582921028\n",
      "Epoch 2550/20000 Validation Loss: 0.06770497560501099\n",
      "Epoch 2551/20000 Training Loss: 0.05585925281047821\n",
      "Epoch 2552/20000 Training Loss: 0.06316380202770233\n",
      "Epoch 2553/20000 Training Loss: 0.08550607413053513\n",
      "Epoch 2554/20000 Training Loss: 0.04279445484280586\n",
      "Epoch 2555/20000 Training Loss: 0.040814127773046494\n",
      "Epoch 2556/20000 Training Loss: 0.0573015958070755\n",
      "Epoch 2557/20000 Training Loss: 0.056501489132642746\n",
      "Epoch 2558/20000 Training Loss: 0.055305395275354385\n",
      "Epoch 2559/20000 Training Loss: 0.05270515754818916\n",
      "Epoch 2560/20000 Training Loss: 0.06704583764076233\n",
      "Epoch 2560/20000 Validation Loss: 0.053325630724430084\n",
      "Epoch 2561/20000 Training Loss: 0.051775041967630386\n",
      "Epoch 2562/20000 Training Loss: 0.05288292095065117\n",
      "Epoch 2563/20000 Training Loss: 0.061681997030973434\n",
      "Epoch 2564/20000 Training Loss: 0.08908551931381226\n",
      "Epoch 2565/20000 Training Loss: 0.0924176275730133\n",
      "Epoch 2566/20000 Training Loss: 0.06704151630401611\n",
      "Epoch 2567/20000 Training Loss: 0.0724748969078064\n",
      "Epoch 2568/20000 Training Loss: 0.05247008800506592\n",
      "Epoch 2569/20000 Training Loss: 0.06645093113183975\n",
      "Epoch 2570/20000 Training Loss: 0.05591170862317085\n",
      "Epoch 2570/20000 Validation Loss: 0.07156512141227722\n",
      "Epoch 2571/20000 Training Loss: 0.05313965678215027\n",
      "Epoch 2572/20000 Training Loss: 0.07577336579561234\n",
      "Epoch 2573/20000 Training Loss: 0.07509057223796844\n",
      "Epoch 2574/20000 Training Loss: 0.042845193296670914\n",
      "Epoch 2575/20000 Training Loss: 0.05707327648997307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2576/20000 Training Loss: 0.05560421943664551\n",
      "Epoch 2577/20000 Training Loss: 0.06447893381118774\n",
      "Epoch 2578/20000 Training Loss: 0.06145685538649559\n",
      "Epoch 2579/20000 Training Loss: 0.06508509069681168\n",
      "Epoch 2580/20000 Training Loss: 0.06363040208816528\n",
      "Epoch 2580/20000 Validation Loss: 0.04582317918539047\n",
      "Epoch 2581/20000 Training Loss: 0.06502402573823929\n",
      "Epoch 2582/20000 Training Loss: 0.061528708785772324\n",
      "Epoch 2583/20000 Training Loss: 0.06026289984583855\n",
      "Epoch 2584/20000 Training Loss: 0.04977324977517128\n",
      "Epoch 2585/20000 Training Loss: 0.0724252387881279\n",
      "Epoch 2586/20000 Training Loss: 0.053721409291028976\n",
      "Epoch 2587/20000 Training Loss: 0.052585553377866745\n",
      "Epoch 2588/20000 Training Loss: 0.04400625824928284\n",
      "Epoch 2589/20000 Training Loss: 0.056151408702135086\n",
      "Epoch 2590/20000 Training Loss: 0.0756581649184227\n",
      "Epoch 2590/20000 Validation Loss: 0.07405757904052734\n",
      "Epoch 2591/20000 Training Loss: 0.052723586559295654\n",
      "Epoch 2592/20000 Training Loss: 0.04783632233738899\n",
      "Epoch 2593/20000 Training Loss: 0.05272211506962776\n",
      "Epoch 2594/20000 Training Loss: 0.06438054889440536\n",
      "Epoch 2595/20000 Training Loss: 0.06364908814430237\n",
      "Epoch 2596/20000 Training Loss: 0.04080783203244209\n",
      "Epoch 2597/20000 Training Loss: 0.05809846520423889\n",
      "Epoch 2598/20000 Training Loss: 0.07014405727386475\n",
      "Epoch 2599/20000 Training Loss: 0.053924400359392166\n",
      "Epoch 2600/20000 Training Loss: 0.050380971282720566\n",
      "Epoch 2600/20000 Validation Loss: 0.0622730627655983\n",
      "Epoch 2601/20000 Training Loss: 0.05266202613711357\n",
      "Epoch 2602/20000 Training Loss: 0.06301821768283844\n",
      "Epoch 2603/20000 Training Loss: 0.06469639390707016\n",
      "Epoch 2604/20000 Training Loss: 0.07007362693548203\n",
      "Epoch 2605/20000 Training Loss: 0.056869905441999435\n",
      "Epoch 2606/20000 Training Loss: 0.05371476337313652\n",
      "Epoch 2607/20000 Training Loss: 0.056380435824394226\n",
      "Epoch 2608/20000 Training Loss: 0.0606720931828022\n",
      "Epoch 2609/20000 Training Loss: 0.05825517699122429\n",
      "Epoch 2610/20000 Training Loss: 0.04237484931945801\n",
      "Epoch 2610/20000 Validation Loss: 0.0803474560379982\n",
      "Epoch 2611/20000 Training Loss: 0.05168696120381355\n",
      "Epoch 2612/20000 Training Loss: 0.06579073518514633\n",
      "Epoch 2613/20000 Training Loss: 0.04294997826218605\n",
      "Epoch 2614/20000 Training Loss: 0.06572400778532028\n",
      "Epoch 2615/20000 Training Loss: 0.060787349939346313\n",
      "Epoch 2616/20000 Training Loss: 0.05666064843535423\n",
      "Epoch 2617/20000 Training Loss: 0.05696047842502594\n",
      "Epoch 2618/20000 Training Loss: 0.03404037281870842\n",
      "Epoch 2619/20000 Training Loss: 0.056650560349226\n",
      "Epoch 2620/20000 Training Loss: 0.05657032132148743\n",
      "Epoch 2620/20000 Validation Loss: 0.059633009135723114\n",
      "Epoch 2621/20000 Training Loss: 0.04476290941238403\n",
      "Epoch 2622/20000 Training Loss: 0.05908188223838806\n",
      "Epoch 2623/20000 Training Loss: 0.053201377391815186\n",
      "Epoch 2624/20000 Training Loss: 0.08323482424020767\n",
      "Epoch 2625/20000 Training Loss: 0.06558295339345932\n",
      "Epoch 2626/20000 Training Loss: 0.05961883068084717\n",
      "Epoch 2627/20000 Training Loss: 0.058012571185827255\n",
      "Epoch 2628/20000 Training Loss: 0.06302208453416824\n",
      "Epoch 2629/20000 Training Loss: 0.07255536317825317\n",
      "Epoch 2630/20000 Training Loss: 0.059748172760009766\n",
      "Epoch 2630/20000 Validation Loss: 0.06250445544719696\n",
      "Epoch 2631/20000 Training Loss: 0.05507625639438629\n",
      "Epoch 2632/20000 Training Loss: 0.061297059059143066\n",
      "Epoch 2633/20000 Training Loss: 0.07003377377986908\n",
      "Epoch 2634/20000 Training Loss: 0.04843072593212128\n",
      "Epoch 2635/20000 Training Loss: 0.06780514866113663\n",
      "Epoch 2636/20000 Training Loss: 0.0635727271437645\n",
      "Epoch 2637/20000 Training Loss: 0.06236198917031288\n",
      "Epoch 2638/20000 Training Loss: 0.041279494762420654\n",
      "Epoch 2639/20000 Training Loss: 0.05891619250178337\n",
      "Epoch 2640/20000 Training Loss: 0.0518530048429966\n",
      "Epoch 2640/20000 Validation Loss: 0.05298364907503128\n",
      "Epoch 2641/20000 Training Loss: 0.06664250046014786\n",
      "Epoch 2642/20000 Training Loss: 0.041683632880449295\n",
      "Epoch 2643/20000 Training Loss: 0.05823597311973572\n",
      "Epoch 2644/20000 Training Loss: 0.059030234813690186\n",
      "Epoch 2645/20000 Training Loss: 0.0613572895526886\n",
      "Epoch 2646/20000 Training Loss: 0.048410844057798386\n",
      "Epoch 2647/20000 Training Loss: 0.05694873631000519\n",
      "Epoch 2648/20000 Training Loss: 0.0576288215816021\n",
      "Epoch 2649/20000 Training Loss: 0.05333326756954193\n",
      "Epoch 2650/20000 Training Loss: 0.06090899184346199\n",
      "Epoch 2650/20000 Validation Loss: 0.05147697031497955\n",
      "Epoch 2651/20000 Training Loss: 0.05877410247921944\n",
      "Epoch 2652/20000 Training Loss: 0.0434446781873703\n",
      "Epoch 2653/20000 Training Loss: 0.05800952389836311\n",
      "Epoch 2654/20000 Training Loss: 0.058437783271074295\n",
      "Epoch 2655/20000 Training Loss: 0.04642963781952858\n",
      "Epoch 2656/20000 Training Loss: 0.06532466411590576\n",
      "Epoch 2657/20000 Training Loss: 0.0652388408780098\n",
      "Epoch 2658/20000 Training Loss: 0.05188189819455147\n",
      "Epoch 2659/20000 Training Loss: 0.05077378824353218\n",
      "Epoch 2660/20000 Training Loss: 0.06867123395204544\n",
      "Epoch 2660/20000 Validation Loss: 0.05892007052898407\n",
      "Epoch 2661/20000 Training Loss: 0.07373511046171188\n",
      "Epoch 2662/20000 Training Loss: 0.05239317938685417\n",
      "Epoch 2663/20000 Training Loss: 0.041908953338861465\n",
      "Epoch 2664/20000 Training Loss: 0.06411013007164001\n",
      "Epoch 2665/20000 Training Loss: 0.07375591993331909\n",
      "Epoch 2666/20000 Training Loss: 0.062306229025125504\n",
      "Epoch 2667/20000 Training Loss: 0.06734255701303482\n",
      "Epoch 2668/20000 Training Loss: 0.07088498026132584\n",
      "Epoch 2669/20000 Training Loss: 0.05119551345705986\n",
      "Epoch 2670/20000 Training Loss: 0.05909516289830208\n",
      "Epoch 2670/20000 Validation Loss: 0.04454072564840317\n",
      "Epoch 2671/20000 Training Loss: 0.04947832599282265\n",
      "Epoch 2672/20000 Training Loss: 0.06327307224273682\n",
      "Epoch 2673/20000 Training Loss: 0.06364550441503525\n",
      "Epoch 2674/20000 Training Loss: 0.08634799718856812\n",
      "Epoch 2675/20000 Training Loss: 0.04906607046723366\n",
      "Epoch 2676/20000 Training Loss: 0.060307566076517105\n",
      "Epoch 2677/20000 Training Loss: 0.0538046658039093\n",
      "Epoch 2678/20000 Training Loss: 0.06733619421720505\n",
      "Epoch 2679/20000 Training Loss: 0.04864685609936714\n",
      "Epoch 2680/20000 Training Loss: 0.06136871501803398\n",
      "Epoch 2680/20000 Validation Loss: 0.062291815876960754\n",
      "Epoch 2681/20000 Training Loss: 0.06816715002059937\n",
      "Epoch 2682/20000 Training Loss: 0.07416781783103943\n",
      "Epoch 2683/20000 Training Loss: 0.05641919746994972\n",
      "Epoch 2684/20000 Training Loss: 0.05557006224989891\n",
      "Epoch 2685/20000 Training Loss: 0.06192678213119507\n",
      "Epoch 2686/20000 Training Loss: 0.06831013411283493\n",
      "Epoch 2687/20000 Training Loss: 0.0685577467083931\n",
      "Epoch 2688/20000 Training Loss: 0.07148682326078415\n",
      "Epoch 2689/20000 Training Loss: 0.055384356528520584\n",
      "Epoch 2690/20000 Training Loss: 0.0599728561937809\n",
      "Epoch 2690/20000 Validation Loss: 0.07280195504426956\n",
      "Epoch 2691/20000 Training Loss: 0.07937589287757874\n",
      "Epoch 2692/20000 Training Loss: 0.06103374436497688\n",
      "Epoch 2693/20000 Training Loss: 0.05072132125496864\n",
      "Epoch 2694/20000 Training Loss: 0.06070224568247795\n",
      "Epoch 2695/20000 Training Loss: 0.06942308694124222\n",
      "Epoch 2696/20000 Training Loss: 0.059528689831495285\n",
      "Epoch 2697/20000 Training Loss: 0.03993108496069908\n",
      "Epoch 2698/20000 Training Loss: 0.060308247804641724\n",
      "Epoch 2699/20000 Training Loss: 0.06735716015100479\n",
      "Epoch 2700/20000 Training Loss: 0.04072212800383568\n",
      "Epoch 2700/20000 Validation Loss: 0.0564778670668602\n",
      "Epoch 2701/20000 Training Loss: 0.06463009119033813\n",
      "Epoch 2702/20000 Training Loss: 0.053801845759153366\n",
      "Epoch 2703/20000 Training Loss: 0.06393139809370041\n",
      "Epoch 2704/20000 Training Loss: 0.07273227721452713\n",
      "Epoch 2705/20000 Training Loss: 0.06310392171144485\n",
      "Epoch 2706/20000 Training Loss: 0.05310047045350075\n",
      "Epoch 2707/20000 Training Loss: 0.06434356421232224\n",
      "Epoch 2708/20000 Training Loss: 0.05089147388935089\n",
      "Epoch 2709/20000 Training Loss: 0.04872399568557739\n",
      "Epoch 2710/20000 Training Loss: 0.050846558064222336\n",
      "Epoch 2710/20000 Validation Loss: 0.044696010649204254\n",
      "Epoch 2711/20000 Training Loss: 0.06933947652578354\n",
      "Epoch 2712/20000 Training Loss: 0.06003850698471069\n",
      "Epoch 2713/20000 Training Loss: 0.05577043071389198\n",
      "Epoch 2714/20000 Training Loss: 0.05887240171432495\n",
      "Epoch 2715/20000 Training Loss: 0.06608584523200989\n",
      "Epoch 2716/20000 Training Loss: 0.06397517025470734\n",
      "Epoch 2717/20000 Training Loss: 0.062033336609601974\n",
      "Epoch 2718/20000 Training Loss: 0.05834844335913658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2719/20000 Training Loss: 0.05664542317390442\n",
      "Epoch 2720/20000 Training Loss: 0.045246075838804245\n",
      "Epoch 2720/20000 Validation Loss: 0.08538724482059479\n",
      "Epoch 2721/20000 Training Loss: 0.06193314865231514\n",
      "Epoch 2722/20000 Training Loss: 0.051873404532670975\n",
      "Epoch 2723/20000 Training Loss: 0.06154923513531685\n",
      "Epoch 2724/20000 Training Loss: 0.06971728801727295\n",
      "Epoch 2725/20000 Training Loss: 0.048289719969034195\n",
      "Epoch 2726/20000 Training Loss: 0.07587986439466476\n",
      "Epoch 2727/20000 Training Loss: 0.06331241875886917\n",
      "Epoch 2728/20000 Training Loss: 0.050768639892339706\n",
      "Epoch 2729/20000 Training Loss: 0.0416221059858799\n",
      "Epoch 2730/20000 Training Loss: 0.05695532634854317\n",
      "Epoch 2730/20000 Validation Loss: 0.07893980294466019\n",
      "Epoch 2731/20000 Training Loss: 0.07696527987718582\n",
      "Epoch 2732/20000 Training Loss: 0.05425983667373657\n",
      "Epoch 2733/20000 Training Loss: 0.07615374773740768\n",
      "Epoch 2734/20000 Training Loss: 0.055663883686065674\n",
      "Epoch 2735/20000 Training Loss: 0.0607563741505146\n",
      "Epoch 2736/20000 Training Loss: 0.05027792230248451\n",
      "Epoch 2737/20000 Training Loss: 0.051305171102285385\n",
      "Epoch 2738/20000 Training Loss: 0.0859982967376709\n",
      "Epoch 2739/20000 Training Loss: 0.05338166281580925\n",
      "Epoch 2740/20000 Training Loss: 0.06804779171943665\n",
      "Epoch 2740/20000 Validation Loss: 0.05093277618288994\n",
      "Epoch 2741/20000 Training Loss: 0.057773422449827194\n",
      "Epoch 2742/20000 Training Loss: 0.05668541416525841\n",
      "Epoch 2743/20000 Training Loss: 0.045991525053977966\n",
      "Epoch 2744/20000 Training Loss: 0.046528276056051254\n",
      "Epoch 2745/20000 Training Loss: 0.047358911484479904\n",
      "Epoch 2746/20000 Training Loss: 0.05306713655591011\n",
      "Epoch 2747/20000 Training Loss: 0.053683195263147354\n",
      "Epoch 2748/20000 Training Loss: 0.055819135159254074\n",
      "Epoch 2749/20000 Training Loss: 0.06278592348098755\n",
      "Epoch 2750/20000 Training Loss: 0.05394656956195831\n",
      "Epoch 2750/20000 Validation Loss: 0.05060102790594101\n",
      "Epoch 2751/20000 Training Loss: 0.0473879873752594\n",
      "Epoch 2752/20000 Training Loss: 0.05218707397580147\n",
      "Epoch 2753/20000 Training Loss: 0.05024990439414978\n",
      "Epoch 2754/20000 Training Loss: 0.06009798124432564\n",
      "Epoch 2755/20000 Training Loss: 0.06511885672807693\n",
      "Epoch 2756/20000 Training Loss: 0.06457817554473877\n",
      "Epoch 2757/20000 Training Loss: 0.050022486597299576\n",
      "Epoch 2758/20000 Training Loss: 0.05869702622294426\n",
      "Epoch 2759/20000 Training Loss: 0.04684600234031677\n",
      "Epoch 2760/20000 Training Loss: 0.061400726437568665\n",
      "Epoch 2760/20000 Validation Loss: 0.038621947169303894\n",
      "Epoch 2761/20000 Training Loss: 0.06960093975067139\n",
      "Epoch 2762/20000 Training Loss: 0.051224276423454285\n",
      "Epoch 2763/20000 Training Loss: 0.0525222085416317\n",
      "Epoch 2764/20000 Training Loss: 0.07204880565404892\n",
      "Epoch 2765/20000 Training Loss: 0.06210382282733917\n",
      "Epoch 2766/20000 Training Loss: 0.07308783382177353\n",
      "Epoch 2767/20000 Training Loss: 0.06509242206811905\n",
      "Epoch 2768/20000 Training Loss: 0.06480229645967484\n",
      "Epoch 2769/20000 Training Loss: 0.06062564253807068\n",
      "Epoch 2770/20000 Training Loss: 0.06511455029249191\n",
      "Epoch 2770/20000 Validation Loss: 0.07023979723453522\n",
      "Epoch 2771/20000 Training Loss: 0.05328391119837761\n",
      "Epoch 2772/20000 Training Loss: 0.06157055124640465\n",
      "Epoch 2773/20000 Training Loss: 0.05956529453396797\n",
      "Epoch 2774/20000 Training Loss: 0.0781928226351738\n",
      "Epoch 2775/20000 Training Loss: 0.042751941829919815\n",
      "Epoch 2776/20000 Training Loss: 0.05171430483460426\n",
      "Epoch 2777/20000 Training Loss: 0.0635978952050209\n",
      "Epoch 2778/20000 Training Loss: 0.07819806784391403\n",
      "Epoch 2779/20000 Training Loss: 0.04185362532734871\n",
      "Epoch 2780/20000 Training Loss: 0.06431274116039276\n",
      "Epoch 2780/20000 Validation Loss: 0.0562906377017498\n",
      "Epoch 2781/20000 Training Loss: 0.057566795498132706\n",
      "Epoch 2782/20000 Training Loss: 0.04919172823429108\n",
      "Epoch 2783/20000 Training Loss: 0.06390023976564407\n",
      "Epoch 2784/20000 Training Loss: 0.0375310517847538\n",
      "Epoch 2785/20000 Training Loss: 0.06161646917462349\n",
      "Epoch 2786/20000 Training Loss: 0.05540112778544426\n",
      "Epoch 2787/20000 Training Loss: 0.056900233030319214\n",
      "Epoch 2788/20000 Training Loss: 0.060085415840148926\n",
      "Epoch 2789/20000 Training Loss: 0.03722601756453514\n",
      "Epoch 2790/20000 Training Loss: 0.05591195821762085\n",
      "Epoch 2790/20000 Validation Loss: 0.04928456246852875\n",
      "Epoch 2791/20000 Training Loss: 0.05956864356994629\n",
      "Epoch 2792/20000 Training Loss: 0.05707322433590889\n",
      "Epoch 2793/20000 Training Loss: 0.07451950013637543\n",
      "Epoch 2794/20000 Training Loss: 0.05969647690653801\n",
      "Epoch 2795/20000 Training Loss: 0.046418171375989914\n",
      "Epoch 2796/20000 Training Loss: 0.06507261842489243\n",
      "Epoch 2797/20000 Training Loss: 0.04468037560582161\n",
      "Epoch 2798/20000 Training Loss: 0.0594974011182785\n",
      "Epoch 2799/20000 Training Loss: 0.053952693939208984\n",
      "Epoch 2800/20000 Training Loss: 0.04540138319134712\n",
      "Epoch 2800/20000 Validation Loss: 0.04720989987254143\n",
      "Epoch 2801/20000 Training Loss: 0.06008181348443031\n",
      "Epoch 2802/20000 Training Loss: 0.04082125425338745\n",
      "Epoch 2803/20000 Training Loss: 0.06056492403149605\n",
      "Epoch 2804/20000 Training Loss: 0.05339832976460457\n",
      "Epoch 2805/20000 Training Loss: 0.03981667384505272\n",
      "Epoch 2806/20000 Training Loss: 0.04098007455468178\n",
      "Epoch 2807/20000 Training Loss: 0.05753040313720703\n",
      "Epoch 2808/20000 Training Loss: 0.05303112044930458\n",
      "Epoch 2809/20000 Training Loss: 0.055364940315485\n",
      "Epoch 2810/20000 Training Loss: 0.056642550975084305\n",
      "Epoch 2810/20000 Validation Loss: 0.04418259859085083\n",
      "Epoch 2811/20000 Training Loss: 0.058533113449811935\n",
      "Epoch 2812/20000 Training Loss: 0.05817614495754242\n",
      "Epoch 2813/20000 Training Loss: 0.07985363155603409\n",
      "Epoch 2814/20000 Training Loss: 0.06927844882011414\n",
      "Epoch 2815/20000 Training Loss: 0.03700405731797218\n",
      "Epoch 2816/20000 Training Loss: 0.04173165187239647\n",
      "Epoch 2817/20000 Training Loss: 0.06519943475723267\n",
      "Epoch 2818/20000 Training Loss: 0.0657021701335907\n",
      "Epoch 2819/20000 Training Loss: 0.08762174844741821\n",
      "Epoch 2820/20000 Training Loss: 0.07036259025335312\n",
      "Epoch 2820/20000 Validation Loss: 0.0678352415561676\n",
      "Epoch 2821/20000 Training Loss: 0.0625438466668129\n",
      "Epoch 2822/20000 Training Loss: 0.04713619127869606\n",
      "Epoch 2823/20000 Training Loss: 0.06961511820554733\n",
      "Epoch 2824/20000 Training Loss: 0.057819243520498276\n",
      "Epoch 2825/20000 Training Loss: 0.0552511103451252\n",
      "Epoch 2826/20000 Training Loss: 0.07210008054971695\n",
      "Epoch 2827/20000 Training Loss: 0.06659208983182907\n",
      "Epoch 2828/20000 Training Loss: 0.049211498349905014\n",
      "Epoch 2829/20000 Training Loss: 0.04971471428871155\n",
      "Epoch 2830/20000 Training Loss: 0.05821797251701355\n",
      "Epoch 2830/20000 Validation Loss: 0.05136074870824814\n",
      "Epoch 2831/20000 Training Loss: 0.054006412625312805\n",
      "Epoch 2832/20000 Training Loss: 0.04915619269013405\n",
      "Epoch 2833/20000 Training Loss: 0.054047808051109314\n",
      "Epoch 2834/20000 Training Loss: 0.07920225709676743\n",
      "Epoch 2835/20000 Training Loss: 0.073369599878788\n",
      "Epoch 2836/20000 Training Loss: 0.05212027207016945\n",
      "Epoch 2837/20000 Training Loss: 0.058705687522888184\n",
      "Epoch 2838/20000 Training Loss: 0.06730160117149353\n",
      "Epoch 2839/20000 Training Loss: 0.03900419548153877\n",
      "Epoch 2840/20000 Training Loss: 0.053673792630434036\n",
      "Epoch 2840/20000 Validation Loss: 0.07387369871139526\n",
      "Epoch 2841/20000 Training Loss: 0.07039984315633774\n",
      "Epoch 2842/20000 Training Loss: 0.05641882121562958\n",
      "Epoch 2843/20000 Training Loss: 0.07858654111623764\n",
      "Epoch 2844/20000 Training Loss: 0.0633407011628151\n",
      "Epoch 2845/20000 Training Loss: 0.054394226521253586\n",
      "Epoch 2846/20000 Training Loss: 0.05927487090229988\n",
      "Epoch 2847/20000 Training Loss: 0.05731033906340599\n",
      "Epoch 2848/20000 Training Loss: 0.0732908546924591\n",
      "Epoch 2849/20000 Training Loss: 0.06723912805318832\n",
      "Epoch 2850/20000 Training Loss: 0.052567366510629654\n",
      "Epoch 2850/20000 Validation Loss: 0.03739254176616669\n",
      "Epoch 2851/20000 Training Loss: 0.0705871656537056\n",
      "Epoch 2852/20000 Training Loss: 0.06793203949928284\n",
      "Epoch 2853/20000 Training Loss: 0.0514911413192749\n",
      "Epoch 2854/20000 Training Loss: 0.052906181663274765\n",
      "Epoch 2855/20000 Training Loss: 0.06761901080608368\n",
      "Epoch 2856/20000 Training Loss: 0.07558860629796982\n",
      "Epoch 2857/20000 Training Loss: 0.0776919424533844\n",
      "Epoch 2858/20000 Training Loss: 0.06297897547483444\n",
      "Epoch 2859/20000 Training Loss: 0.0540594756603241\n",
      "Epoch 2860/20000 Training Loss: 0.06400531530380249\n",
      "Epoch 2860/20000 Validation Loss: 0.05659008398652077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2861/20000 Training Loss: 0.051406826823949814\n",
      "Epoch 2862/20000 Training Loss: 0.06702908128499985\n",
      "Epoch 2863/20000 Training Loss: 0.06743770092725754\n",
      "Epoch 2864/20000 Training Loss: 0.06011706590652466\n",
      "Epoch 2865/20000 Training Loss: 0.07251831144094467\n",
      "Epoch 2866/20000 Training Loss: 0.05930369719862938\n",
      "Epoch 2867/20000 Training Loss: 0.04536410793662071\n",
      "Epoch 2868/20000 Training Loss: 0.06896765530109406\n",
      "Epoch 2869/20000 Training Loss: 0.05688988044857979\n",
      "Epoch 2870/20000 Training Loss: 0.04450901225209236\n",
      "Epoch 2870/20000 Validation Loss: 0.06399829685688019\n",
      "Epoch 2871/20000 Training Loss: 0.07259103655815125\n",
      "Epoch 2872/20000 Training Loss: 0.061892930418252945\n",
      "Epoch 2873/20000 Training Loss: 0.058984652161598206\n",
      "Epoch 2874/20000 Training Loss: 0.055041953921318054\n",
      "Epoch 2875/20000 Training Loss: 0.06789424270391464\n",
      "Epoch 2876/20000 Training Loss: 0.06847097724676132\n",
      "Epoch 2877/20000 Training Loss: 0.053602978587150574\n",
      "Epoch 2878/20000 Training Loss: 0.054680049419403076\n",
      "Epoch 2879/20000 Training Loss: 0.06277815252542496\n",
      "Epoch 2880/20000 Training Loss: 0.05463586375117302\n",
      "Epoch 2880/20000 Validation Loss: 0.050345297902822495\n",
      "Epoch 2881/20000 Training Loss: 0.07157441973686218\n",
      "Epoch 2882/20000 Training Loss: 0.06221318617463112\n",
      "Epoch 2883/20000 Training Loss: 0.05590878054499626\n",
      "Epoch 2884/20000 Training Loss: 0.06071808561682701\n",
      "Epoch 2885/20000 Training Loss: 0.0582791268825531\n",
      "Epoch 2886/20000 Training Loss: 0.06541398167610168\n",
      "Epoch 2887/20000 Training Loss: 0.06339219957590103\n",
      "Epoch 2888/20000 Training Loss: 0.0741930678486824\n",
      "Epoch 2889/20000 Training Loss: 0.0671575590968132\n",
      "Epoch 2890/20000 Training Loss: 0.054840538650751114\n",
      "Epoch 2890/20000 Validation Loss: 0.05598035454750061\n",
      "Epoch 2891/20000 Training Loss: 0.08995872735977173\n",
      "Epoch 2892/20000 Training Loss: 0.05410897731781006\n",
      "Epoch 2893/20000 Training Loss: 0.0708763524889946\n",
      "Epoch 2894/20000 Training Loss: 0.06382131576538086\n",
      "Epoch 2895/20000 Training Loss: 0.048105787485837936\n",
      "Epoch 2896/20000 Training Loss: 0.06462115049362183\n",
      "Epoch 2897/20000 Training Loss: 0.04447680711746216\n",
      "Epoch 2898/20000 Training Loss: 0.0626591369509697\n",
      "Epoch 2899/20000 Training Loss: 0.05605326220393181\n",
      "Epoch 2900/20000 Training Loss: 0.058498453348875046\n",
      "Epoch 2900/20000 Validation Loss: 0.06727840006351471\n",
      "Epoch 2901/20000 Training Loss: 0.04772105813026428\n",
      "Epoch 2902/20000 Training Loss: 0.03866597265005112\n",
      "Epoch 2903/20000 Training Loss: 0.052440181374549866\n",
      "Epoch 2904/20000 Training Loss: 0.04692235589027405\n",
      "Epoch 2905/20000 Training Loss: 0.0443873405456543\n",
      "Epoch 2906/20000 Training Loss: 0.06002418324351311\n",
      "Epoch 2907/20000 Training Loss: 0.0578158013522625\n",
      "Epoch 2908/20000 Training Loss: 0.05747658386826515\n",
      "Epoch 2909/20000 Training Loss: 0.02817697823047638\n",
      "Epoch 2910/20000 Training Loss: 0.05988210439682007\n",
      "Epoch 2910/20000 Validation Loss: 0.06835783272981644\n",
      "Epoch 2911/20000 Training Loss: 0.0551765002310276\n",
      "Epoch 2912/20000 Training Loss: 0.056992307305336\n",
      "Epoch 2913/20000 Training Loss: 0.04392334446310997\n",
      "Epoch 2914/20000 Training Loss: 0.07300052046775818\n",
      "Epoch 2915/20000 Training Loss: 0.1078021451830864\n",
      "Epoch 2916/20000 Training Loss: 0.048367131501436234\n",
      "Epoch 2917/20000 Training Loss: 0.05428531765937805\n",
      "Epoch 2918/20000 Training Loss: 0.05638192221522331\n",
      "Epoch 2919/20000 Training Loss: 0.06992007791996002\n",
      "Epoch 2920/20000 Training Loss: 0.08490852266550064\n",
      "Epoch 2920/20000 Validation Loss: 0.06669119745492935\n",
      "Epoch 2921/20000 Training Loss: 0.06774294376373291\n",
      "Epoch 2922/20000 Training Loss: 0.09159845113754272\n",
      "Epoch 2923/20000 Training Loss: 0.06327211856842041\n",
      "Epoch 2924/20000 Training Loss: 0.047995854169130325\n",
      "Epoch 2925/20000 Training Loss: 0.04761935770511627\n",
      "Epoch 2926/20000 Training Loss: 0.0569118969142437\n",
      "Epoch 2927/20000 Training Loss: 0.07684926688671112\n",
      "Epoch 2928/20000 Training Loss: 0.04978233948349953\n",
      "Epoch 2929/20000 Training Loss: 0.05518224835395813\n",
      "Epoch 2930/20000 Training Loss: 0.07172083109617233\n",
      "Epoch 2930/20000 Validation Loss: 0.060931894928216934\n",
      "Epoch 2931/20000 Training Loss: 0.05943290516734123\n",
      "Epoch 2932/20000 Training Loss: 0.057201582938432693\n",
      "Epoch 2933/20000 Training Loss: 0.050973981618881226\n",
      "Epoch 2934/20000 Training Loss: 0.06670928001403809\n",
      "Epoch 2935/20000 Training Loss: 0.07313274592161179\n",
      "Epoch 2936/20000 Training Loss: 0.06472961604595184\n",
      "Epoch 2937/20000 Training Loss: 0.05571913719177246\n",
      "Epoch 2938/20000 Training Loss: 0.05680766701698303\n",
      "Epoch 2939/20000 Training Loss: 0.05287575349211693\n",
      "Epoch 2940/20000 Training Loss: 0.0534457266330719\n",
      "Epoch 2940/20000 Validation Loss: 0.0464276447892189\n",
      "Epoch 2941/20000 Training Loss: 0.07466646283864975\n",
      "Epoch 2942/20000 Training Loss: 0.04716043174266815\n",
      "Epoch 2943/20000 Training Loss: 0.055622588843107224\n",
      "Epoch 2944/20000 Training Loss: 0.08852801471948624\n",
      "Epoch 2945/20000 Training Loss: 0.05007918179035187\n",
      "Epoch 2946/20000 Training Loss: 0.055405572056770325\n",
      "Epoch 2947/20000 Training Loss: 0.04491661861538887\n",
      "Epoch 2948/20000 Training Loss: 0.05168825760483742\n",
      "Epoch 2949/20000 Training Loss: 0.07190760970115662\n",
      "Epoch 2950/20000 Training Loss: 0.058564092963933945\n",
      "Epoch 2950/20000 Validation Loss: 0.07260219752788544\n",
      "Epoch 2951/20000 Training Loss: 0.08019599318504333\n",
      "Epoch 2952/20000 Training Loss: 0.06500031054019928\n",
      "Epoch 2953/20000 Training Loss: 0.0418052114546299\n",
      "Epoch 2954/20000 Training Loss: 0.06671684235334396\n",
      "Epoch 2955/20000 Training Loss: 0.06984978914260864\n",
      "Epoch 2956/20000 Training Loss: 0.0744716078042984\n",
      "Epoch 2957/20000 Training Loss: 0.059476640075445175\n",
      "Epoch 2958/20000 Training Loss: 0.049486737698316574\n",
      "Epoch 2959/20000 Training Loss: 0.07721974700689316\n",
      "Epoch 2960/20000 Training Loss: 0.057458337396383286\n",
      "Epoch 2960/20000 Validation Loss: 0.046047355979681015\n",
      "Epoch 2961/20000 Training Loss: 0.0754118412733078\n",
      "Epoch 2962/20000 Training Loss: 0.06519844383001328\n",
      "Epoch 2963/20000 Training Loss: 0.06977230310440063\n",
      "Epoch 2964/20000 Training Loss: 0.04176758602261543\n",
      "Epoch 2965/20000 Training Loss: 0.048511702567338943\n",
      "Epoch 2966/20000 Training Loss: 0.05554594099521637\n",
      "Epoch 2967/20000 Training Loss: 0.05777658894658089\n",
      "Epoch 2968/20000 Training Loss: 0.05016915127635002\n",
      "Epoch 2969/20000 Training Loss: 0.050140488892793655\n",
      "Epoch 2970/20000 Training Loss: 0.055341918021440506\n",
      "Epoch 2970/20000 Validation Loss: 0.05487850308418274\n",
      "Epoch 2971/20000 Training Loss: 0.0629255548119545\n",
      "Epoch 2972/20000 Training Loss: 0.06045578792691231\n",
      "Epoch 2973/20000 Training Loss: 0.07091043144464493\n",
      "Epoch 2974/20000 Training Loss: 0.05817173048853874\n",
      "Epoch 2975/20000 Training Loss: 0.0702047273516655\n",
      "Epoch 2976/20000 Training Loss: 0.061429183930158615\n",
      "Epoch 2977/20000 Training Loss: 0.06340133398771286\n",
      "Epoch 2978/20000 Training Loss: 0.059548959136009216\n",
      "Epoch 2979/20000 Training Loss: 0.055530499666929245\n",
      "Epoch 2980/20000 Training Loss: 0.05691791698336601\n",
      "Epoch 2980/20000 Validation Loss: 0.06498876214027405\n",
      "Epoch 2981/20000 Training Loss: 0.06562155485153198\n",
      "Epoch 2982/20000 Training Loss: 0.078704334795475\n",
      "Epoch 2983/20000 Training Loss: 0.06522775441408157\n",
      "Epoch 2984/20000 Training Loss: 0.060688186436891556\n",
      "Epoch 2985/20000 Training Loss: 0.06346694380044937\n",
      "Epoch 2986/20000 Training Loss: 0.060448918491601944\n",
      "Epoch 2987/20000 Training Loss: 0.06082591041922569\n",
      "Epoch 2988/20000 Training Loss: 0.057580988854169846\n",
      "Epoch 2989/20000 Training Loss: 0.04123281314969063\n",
      "Epoch 2990/20000 Training Loss: 0.05103239417076111\n",
      "Epoch 2990/20000 Validation Loss: 0.047785937786102295\n",
      "Epoch 2991/20000 Training Loss: 0.03949344530701637\n",
      "Epoch 2992/20000 Training Loss: 0.05581217631697655\n",
      "Epoch 2993/20000 Training Loss: 0.052848752588033676\n",
      "Epoch 2994/20000 Training Loss: 0.0675261914730072\n",
      "Epoch 2995/20000 Training Loss: 0.0662863552570343\n",
      "Epoch 2996/20000 Training Loss: 0.0719626396894455\n",
      "Epoch 2997/20000 Training Loss: 0.05823013186454773\n",
      "Epoch 2998/20000 Training Loss: 0.05558707192540169\n",
      "Epoch 2999/20000 Training Loss: 0.076717309653759\n",
      "Epoch 3000/20000 Training Loss: 0.06236353516578674\n",
      "Epoch 3000/20000 Validation Loss: 0.05738402158021927\n",
      "Epoch 3001/20000 Training Loss: 0.07162231206893921\n",
      "Epoch 3002/20000 Training Loss: 0.0709061399102211\n",
      "Epoch 3003/20000 Training Loss: 0.07225335389375687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3004/20000 Training Loss: 0.05495932325720787\n",
      "Epoch 3005/20000 Training Loss: 0.05261232331395149\n",
      "Epoch 3006/20000 Training Loss: 0.053492385894060135\n",
      "Epoch 3007/20000 Training Loss: 0.07323160767555237\n",
      "Epoch 3008/20000 Training Loss: 0.05381219461560249\n",
      "Epoch 3009/20000 Training Loss: 0.04788529500365257\n",
      "Epoch 3010/20000 Training Loss: 0.05939139425754547\n",
      "Epoch 3010/20000 Validation Loss: 0.03787483647465706\n",
      "Epoch 3011/20000 Training Loss: 0.05758036673069\n",
      "Epoch 3012/20000 Training Loss: 0.05199410393834114\n",
      "Epoch 3013/20000 Training Loss: 0.06474381685256958\n",
      "Epoch 3014/20000 Training Loss: 0.08739465475082397\n",
      "Epoch 3015/20000 Training Loss: 0.06564071774482727\n",
      "Epoch 3016/20000 Training Loss: 0.060722362250089645\n",
      "Epoch 3017/20000 Training Loss: 0.05794791504740715\n",
      "Epoch 3018/20000 Training Loss: 0.057764023542404175\n",
      "Epoch 3019/20000 Training Loss: 0.05531739071011543\n",
      "Epoch 3020/20000 Training Loss: 0.0625576376914978\n",
      "Epoch 3020/20000 Validation Loss: 0.05569987744092941\n",
      "Epoch 3021/20000 Training Loss: 0.06870976090431213\n",
      "Epoch 3022/20000 Training Loss: 0.0741257444024086\n",
      "Epoch 3023/20000 Training Loss: 0.063120998442173\n",
      "Epoch 3024/20000 Training Loss: 0.052392829209566116\n",
      "Epoch 3025/20000 Training Loss: 0.04264945909380913\n",
      "Epoch 3026/20000 Training Loss: 0.04728863760828972\n",
      "Epoch 3027/20000 Training Loss: 0.0805397555232048\n",
      "Epoch 3028/20000 Training Loss: 0.05624808743596077\n",
      "Epoch 3029/20000 Training Loss: 0.0764254704117775\n",
      "Epoch 3030/20000 Training Loss: 0.04213236644864082\n",
      "Epoch 3030/20000 Validation Loss: 0.05314624682068825\n",
      "Epoch 3031/20000 Training Loss: 0.0574064664542675\n",
      "Epoch 3032/20000 Training Loss: 0.03996623679995537\n",
      "Epoch 3033/20000 Training Loss: 0.0880802646279335\n",
      "Epoch 3034/20000 Training Loss: 0.05577056109905243\n",
      "Epoch 3035/20000 Training Loss: 0.057977888733148575\n",
      "Epoch 3036/20000 Training Loss: 0.04988205432891846\n",
      "Epoch 3037/20000 Training Loss: 0.07996378093957901\n",
      "Epoch 3038/20000 Training Loss: 0.07095090299844742\n",
      "Epoch 3039/20000 Training Loss: 0.05426469445228577\n",
      "Epoch 3040/20000 Training Loss: 0.048096638172864914\n",
      "Epoch 3040/20000 Validation Loss: 0.04739237204194069\n",
      "Epoch 3041/20000 Training Loss: 0.05301778018474579\n",
      "Epoch 3042/20000 Training Loss: 0.04157547280192375\n",
      "Epoch 3043/20000 Training Loss: 0.06272813677787781\n",
      "Epoch 3044/20000 Training Loss: 0.04683421552181244\n",
      "Epoch 3045/20000 Training Loss: 0.05715027078986168\n",
      "Epoch 3046/20000 Training Loss: 0.04378075525164604\n",
      "Epoch 3047/20000 Training Loss: 0.06647083163261414\n",
      "Epoch 3048/20000 Training Loss: 0.05919286608695984\n",
      "Epoch 3049/20000 Training Loss: 0.0513320155441761\n",
      "Epoch 3050/20000 Training Loss: 0.05573165416717529\n",
      "Epoch 3050/20000 Validation Loss: 0.04365124553442001\n",
      "Epoch 3051/20000 Training Loss: 0.041602443903684616\n",
      "Epoch 3052/20000 Training Loss: 0.05408390983939171\n",
      "Epoch 3053/20000 Training Loss: 0.05576656758785248\n",
      "Epoch 3054/20000 Training Loss: 0.06513100862503052\n",
      "Epoch 3055/20000 Training Loss: 0.04900875687599182\n",
      "Epoch 3056/20000 Training Loss: 0.054180603474378586\n",
      "Epoch 3057/20000 Training Loss: 0.06618409603834152\n",
      "Epoch 3058/20000 Training Loss: 0.06591407209634781\n",
      "Epoch 3059/20000 Training Loss: 0.05144990608096123\n",
      "Epoch 3060/20000 Training Loss: 0.07160214334726334\n",
      "Epoch 3060/20000 Validation Loss: 0.06717857718467712\n",
      "Epoch 3061/20000 Training Loss: 0.06325671821832657\n",
      "Epoch 3062/20000 Training Loss: 0.046374816447496414\n",
      "Epoch 3063/20000 Training Loss: 0.06179293617606163\n",
      "Epoch 3064/20000 Training Loss: 0.06694179773330688\n",
      "Epoch 3065/20000 Training Loss: 0.07156132906675339\n",
      "Epoch 3066/20000 Training Loss: 0.04242656007409096\n",
      "Epoch 3067/20000 Training Loss: 0.041481953114271164\n",
      "Epoch 3068/20000 Training Loss: 0.06911883503198624\n",
      "Epoch 3069/20000 Training Loss: 0.03881242498755455\n",
      "Epoch 3070/20000 Training Loss: 0.06614727526903152\n",
      "Epoch 3070/20000 Validation Loss: 0.053342584520578384\n",
      "Epoch 3071/20000 Training Loss: 0.06138529255986214\n",
      "Epoch 3072/20000 Training Loss: 0.062117401510477066\n",
      "Epoch 3073/20000 Training Loss: 0.045258332043886185\n",
      "Epoch 3074/20000 Training Loss: 0.05586182698607445\n",
      "Epoch 3075/20000 Training Loss: 0.052105873823165894\n",
      "Epoch 3076/20000 Training Loss: 0.0627099797129631\n",
      "Epoch 3077/20000 Training Loss: 0.06682116538286209\n",
      "Epoch 3078/20000 Training Loss: 0.04459274932742119\n",
      "Epoch 3079/20000 Training Loss: 0.06687099486589432\n",
      "Epoch 3080/20000 Training Loss: 0.05697422847151756\n",
      "Epoch 3080/20000 Validation Loss: 0.04317667335271835\n",
      "Epoch 3081/20000 Training Loss: 0.06413918733596802\n",
      "Epoch 3082/20000 Training Loss: 0.0568808950483799\n",
      "Epoch 3083/20000 Training Loss: 0.05656648799777031\n",
      "Epoch 3084/20000 Training Loss: 0.04238403961062431\n",
      "Epoch 3085/20000 Training Loss: 0.07034533470869064\n",
      "Epoch 3086/20000 Training Loss: 0.05485505238175392\n",
      "Epoch 3087/20000 Training Loss: 0.061983879655599594\n",
      "Epoch 3088/20000 Training Loss: 0.05685317516326904\n",
      "Epoch 3089/20000 Training Loss: 0.07489974051713943\n",
      "Epoch 3090/20000 Training Loss: 0.05798938870429993\n",
      "Epoch 3090/20000 Validation Loss: 0.03848862648010254\n",
      "Epoch 3091/20000 Training Loss: 0.059600699692964554\n",
      "Epoch 3092/20000 Training Loss: 0.06173967197537422\n",
      "Epoch 3093/20000 Training Loss: 0.0540272481739521\n",
      "Epoch 3094/20000 Training Loss: 0.04294946789741516\n",
      "Epoch 3095/20000 Training Loss: 0.07129054516553879\n",
      "Epoch 3096/20000 Training Loss: 0.06264753639698029\n",
      "Epoch 3097/20000 Training Loss: 0.05377526953816414\n",
      "Epoch 3098/20000 Training Loss: 0.06818104535341263\n",
      "Epoch 3099/20000 Training Loss: 0.06343287229537964\n",
      "Epoch 3100/20000 Training Loss: 0.052568357437849045\n",
      "Epoch 3100/20000 Validation Loss: 0.07083780318498611\n",
      "Epoch 3101/20000 Training Loss: 0.04503445327281952\n",
      "Epoch 3102/20000 Training Loss: 0.04779461398720741\n",
      "Epoch 3103/20000 Training Loss: 0.06478772312402725\n",
      "Epoch 3104/20000 Training Loss: 0.06258762627840042\n",
      "Epoch 3105/20000 Training Loss: 0.06010808050632477\n",
      "Epoch 3106/20000 Training Loss: 0.05968911573290825\n",
      "Epoch 3107/20000 Training Loss: 0.05576683208346367\n",
      "Epoch 3108/20000 Training Loss: 0.08156653493642807\n",
      "Epoch 3109/20000 Training Loss: 0.03930361568927765\n",
      "Epoch 3110/20000 Training Loss: 0.04476888105273247\n",
      "Epoch 3110/20000 Validation Loss: 0.043796759098768234\n",
      "Epoch 3111/20000 Training Loss: 0.06145470216870308\n",
      "Epoch 3112/20000 Training Loss: 0.06138620898127556\n",
      "Epoch 3113/20000 Training Loss: 0.052248284220695496\n",
      "Epoch 3114/20000 Training Loss: 0.049652501940727234\n",
      "Epoch 3115/20000 Training Loss: 0.060094594955444336\n",
      "Epoch 3116/20000 Training Loss: 0.04957105591893196\n",
      "Epoch 3117/20000 Training Loss: 0.07627284526824951\n",
      "Epoch 3118/20000 Training Loss: 0.05799175426363945\n",
      "Epoch 3119/20000 Training Loss: 0.0700685977935791\n",
      "Epoch 3120/20000 Training Loss: 0.050915852189064026\n",
      "Epoch 3120/20000 Validation Loss: 0.06516960263252258\n",
      "Epoch 3121/20000 Training Loss: 0.0652354285120964\n",
      "Epoch 3122/20000 Training Loss: 0.045550670474767685\n",
      "Epoch 3123/20000 Training Loss: 0.06941940635442734\n",
      "Epoch 3124/20000 Training Loss: 0.04597385600209236\n",
      "Epoch 3125/20000 Training Loss: 0.06584826856851578\n",
      "Epoch 3126/20000 Training Loss: 0.06340926885604858\n",
      "Epoch 3127/20000 Training Loss: 0.06057041883468628\n",
      "Epoch 3128/20000 Training Loss: 0.06025388836860657\n",
      "Epoch 3129/20000 Training Loss: 0.03994651511311531\n",
      "Epoch 3130/20000 Training Loss: 0.055704712867736816\n",
      "Epoch 3130/20000 Validation Loss: 0.04119409620761871\n",
      "Epoch 3131/20000 Training Loss: 0.040906041860580444\n",
      "Epoch 3132/20000 Training Loss: 0.04953557625412941\n",
      "Epoch 3133/20000 Training Loss: 0.050845105201005936\n",
      "Epoch 3134/20000 Training Loss: 0.0681963786482811\n",
      "Epoch 3135/20000 Training Loss: 0.06090707704424858\n",
      "Epoch 3136/20000 Training Loss: 0.061406057327985764\n",
      "Epoch 3137/20000 Training Loss: 0.04584842547774315\n",
      "Epoch 3138/20000 Training Loss: 0.055428583174943924\n",
      "Epoch 3139/20000 Training Loss: 0.06760415434837341\n",
      "Epoch 3140/20000 Training Loss: 0.045492637902498245\n",
      "Epoch 3140/20000 Validation Loss: 0.06103043258190155\n",
      "Epoch 3141/20000 Training Loss: 0.06866737455129623\n",
      "Epoch 3142/20000 Training Loss: 0.05683121085166931\n",
      "Epoch 3143/20000 Training Loss: 0.05852702260017395\n",
      "Epoch 3144/20000 Training Loss: 0.05549956485629082\n",
      "Epoch 3145/20000 Training Loss: 0.058826129883527756\n",
      "Epoch 3146/20000 Training Loss: 0.08315353840589523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3147/20000 Training Loss: 0.07412082701921463\n",
      "Epoch 3148/20000 Training Loss: 0.06213122606277466\n",
      "Epoch 3149/20000 Training Loss: 0.05079318955540657\n",
      "Epoch 3150/20000 Training Loss: 0.04963498190045357\n",
      "Epoch 3150/20000 Validation Loss: 0.08006329834461212\n",
      "Epoch 3151/20000 Training Loss: 0.07185254245996475\n",
      "Epoch 3152/20000 Training Loss: 0.05919456481933594\n",
      "Epoch 3153/20000 Training Loss: 0.05174754932522774\n",
      "Epoch 3154/20000 Training Loss: 0.06222766637802124\n",
      "Epoch 3155/20000 Training Loss: 0.06261720508337021\n",
      "Epoch 3156/20000 Training Loss: 0.058742448687553406\n",
      "Epoch 3157/20000 Training Loss: 0.045195769518613815\n",
      "Epoch 3158/20000 Training Loss: 0.06179491803050041\n",
      "Epoch 3159/20000 Training Loss: 0.045079782605171204\n",
      "Epoch 3160/20000 Training Loss: 0.07516957074403763\n",
      "Epoch 3160/20000 Validation Loss: 0.054797109216451645\n",
      "Epoch 3161/20000 Training Loss: 0.05102640762925148\n",
      "Epoch 3162/20000 Training Loss: 0.04817536845803261\n",
      "Epoch 3163/20000 Training Loss: 0.10007783025503159\n",
      "Epoch 3164/20000 Training Loss: 0.05081659555435181\n",
      "Epoch 3165/20000 Training Loss: 0.052799273282289505\n",
      "Epoch 3166/20000 Training Loss: 0.04666828736662865\n",
      "Epoch 3167/20000 Training Loss: 0.060271989554166794\n",
      "Epoch 3168/20000 Training Loss: 0.06973060220479965\n",
      "Epoch 3169/20000 Training Loss: 0.053344178944826126\n",
      "Epoch 3170/20000 Training Loss: 0.04803582653403282\n",
      "Epoch 3170/20000 Validation Loss: 0.047324977815151215\n",
      "Epoch 3171/20000 Training Loss: 0.05843459442257881\n",
      "Epoch 3172/20000 Training Loss: 0.0717640221118927\n",
      "Epoch 3173/20000 Training Loss: 0.0732899010181427\n",
      "Epoch 3174/20000 Training Loss: 0.048759955912828445\n",
      "Epoch 3175/20000 Training Loss: 0.04935021698474884\n",
      "Epoch 3176/20000 Training Loss: 0.0564008504152298\n",
      "Epoch 3177/20000 Training Loss: 0.06158962845802307\n",
      "Epoch 3178/20000 Training Loss: 0.0544377826154232\n",
      "Epoch 3179/20000 Training Loss: 0.05052343010902405\n",
      "Epoch 3180/20000 Training Loss: 0.05707615613937378\n",
      "Epoch 3180/20000 Validation Loss: 0.05880819261074066\n",
      "Epoch 3181/20000 Training Loss: 0.06232735514640808\n",
      "Epoch 3182/20000 Training Loss: 0.03870837017893791\n",
      "Epoch 3183/20000 Training Loss: 0.04758378863334656\n",
      "Epoch 3184/20000 Training Loss: 0.06852022558450699\n",
      "Epoch 3185/20000 Training Loss: 0.05895908176898956\n",
      "Epoch 3186/20000 Training Loss: 0.06202451512217522\n",
      "Epoch 3187/20000 Training Loss: 0.06941983848810196\n",
      "Epoch 3188/20000 Training Loss: 0.06748586148023605\n",
      "Epoch 3189/20000 Training Loss: 0.04518391564488411\n",
      "Epoch 3190/20000 Training Loss: 0.06075851619243622\n",
      "Epoch 3190/20000 Validation Loss: 0.0646238774061203\n",
      "Epoch 3191/20000 Training Loss: 0.05363386869430542\n",
      "Epoch 3192/20000 Training Loss: 0.04431808367371559\n",
      "Epoch 3193/20000 Training Loss: 0.052666764706373215\n",
      "Epoch 3194/20000 Training Loss: 0.05862049758434296\n",
      "Epoch 3195/20000 Training Loss: 0.060386285185813904\n",
      "Epoch 3196/20000 Training Loss: 0.08491619676351547\n",
      "Epoch 3197/20000 Training Loss: 0.04758955165743828\n",
      "Epoch 3198/20000 Training Loss: 0.05787810683250427\n",
      "Epoch 3199/20000 Training Loss: 0.05742141231894493\n",
      "Epoch 3200/20000 Training Loss: 0.03887341171503067\n",
      "Epoch 3200/20000 Validation Loss: 0.04070216417312622\n",
      "Epoch 3201/20000 Training Loss: 0.059933487325906754\n",
      "Epoch 3202/20000 Training Loss: 0.05347856879234314\n",
      "Epoch 3203/20000 Training Loss: 0.056302815675735474\n",
      "Epoch 3204/20000 Training Loss: 0.04439656063914299\n",
      "Epoch 3205/20000 Training Loss: 0.047921061515808105\n",
      "Epoch 3206/20000 Training Loss: 0.07439481467008591\n",
      "Epoch 3207/20000 Training Loss: 0.06810218095779419\n",
      "Epoch 3208/20000 Training Loss: 0.04685227945446968\n",
      "Epoch 3209/20000 Training Loss: 0.071487195789814\n",
      "Epoch 3210/20000 Training Loss: 0.07773192971944809\n",
      "Epoch 3210/20000 Validation Loss: 0.05419152230024338\n",
      "Epoch 3211/20000 Training Loss: 0.06424546241760254\n",
      "Epoch 3212/20000 Training Loss: 0.06113110110163689\n",
      "Epoch 3213/20000 Training Loss: 0.06470517069101334\n",
      "Epoch 3214/20000 Training Loss: 0.06675597280263901\n",
      "Epoch 3215/20000 Training Loss: 0.04902498424053192\n",
      "Epoch 3216/20000 Training Loss: 0.043958067893981934\n",
      "Epoch 3217/20000 Training Loss: 0.0544973723590374\n",
      "Epoch 3218/20000 Training Loss: 0.06225154921412468\n",
      "Epoch 3219/20000 Training Loss: 0.045154672116041183\n",
      "Epoch 3220/20000 Training Loss: 0.05709194019436836\n",
      "Epoch 3220/20000 Validation Loss: 0.04796380549669266\n",
      "Epoch 3221/20000 Training Loss: 0.06370922178030014\n",
      "Epoch 3222/20000 Training Loss: 0.04235522821545601\n",
      "Epoch 3223/20000 Training Loss: 0.0645904615521431\n",
      "Epoch 3224/20000 Training Loss: 0.059793904423713684\n",
      "Epoch 3225/20000 Training Loss: 0.056756485253572464\n",
      "Epoch 3226/20000 Training Loss: 0.04873816296458244\n",
      "Epoch 3227/20000 Training Loss: 0.06277134269475937\n",
      "Epoch 3228/20000 Training Loss: 0.07901560515165329\n",
      "Epoch 3229/20000 Training Loss: 0.08402500301599503\n",
      "Epoch 3230/20000 Training Loss: 0.06549333781003952\n",
      "Epoch 3230/20000 Validation Loss: 0.05363447219133377\n",
      "Epoch 3231/20000 Training Loss: 0.06207914277911186\n",
      "Epoch 3232/20000 Training Loss: 0.07410193234682083\n",
      "Epoch 3233/20000 Training Loss: 0.08557649701833725\n",
      "Epoch 3234/20000 Training Loss: 0.07859602570533752\n",
      "Epoch 3235/20000 Training Loss: 0.0555528961122036\n",
      "Epoch 3236/20000 Training Loss: 0.06863351911306381\n",
      "Epoch 3237/20000 Training Loss: 0.06797785311937332\n",
      "Epoch 3238/20000 Training Loss: 0.057411279529333115\n",
      "Epoch 3239/20000 Training Loss: 0.0696888193488121\n",
      "Epoch 3240/20000 Training Loss: 0.06230504810810089\n",
      "Epoch 3240/20000 Validation Loss: 0.06882093846797943\n",
      "Epoch 3241/20000 Training Loss: 0.05012821778655052\n",
      "Epoch 3242/20000 Training Loss: 0.057437315583229065\n",
      "Epoch 3243/20000 Training Loss: 0.05440261587500572\n",
      "Epoch 3244/20000 Training Loss: 0.05818290635943413\n",
      "Epoch 3245/20000 Training Loss: 0.05771549418568611\n",
      "Epoch 3246/20000 Training Loss: 0.05446894094347954\n",
      "Epoch 3247/20000 Training Loss: 0.06092241778969765\n",
      "Epoch 3248/20000 Training Loss: 0.0489344596862793\n",
      "Epoch 3249/20000 Training Loss: 0.04922136291861534\n",
      "Epoch 3250/20000 Training Loss: 0.08086288720369339\n",
      "Epoch 3250/20000 Validation Loss: 0.052077896893024445\n",
      "Epoch 3251/20000 Training Loss: 0.07427643984556198\n",
      "Epoch 3252/20000 Training Loss: 0.05887632444500923\n",
      "Epoch 3253/20000 Training Loss: 0.07478880882263184\n",
      "Epoch 3254/20000 Training Loss: 0.06398238986730576\n",
      "Epoch 3255/20000 Training Loss: 0.0681205689907074\n",
      "Epoch 3256/20000 Training Loss: 0.05550815537571907\n",
      "Epoch 3257/20000 Training Loss: 0.05630655586719513\n",
      "Epoch 3258/20000 Training Loss: 0.060331713408231735\n",
      "Epoch 3259/20000 Training Loss: 0.05320720002055168\n",
      "Epoch 3260/20000 Training Loss: 0.05482124909758568\n",
      "Epoch 3260/20000 Validation Loss: 0.05279795452952385\n",
      "Epoch 3261/20000 Training Loss: 0.04707272723317146\n",
      "Epoch 3262/20000 Training Loss: 0.046775590628385544\n",
      "Epoch 3263/20000 Training Loss: 0.055723220109939575\n",
      "Epoch 3264/20000 Training Loss: 0.0643296167254448\n",
      "Epoch 3265/20000 Training Loss: 0.06217921897768974\n",
      "Epoch 3266/20000 Training Loss: 0.06783761829137802\n",
      "Epoch 3267/20000 Training Loss: 0.049940239638090134\n",
      "Epoch 3268/20000 Training Loss: 0.056685980409383774\n",
      "Epoch 3269/20000 Training Loss: 0.047014977782964706\n",
      "Epoch 3270/20000 Training Loss: 0.0534912534058094\n",
      "Epoch 3270/20000 Validation Loss: 0.05666986107826233\n",
      "Epoch 3271/20000 Training Loss: 0.06449500471353531\n",
      "Epoch 3272/20000 Training Loss: 0.0633879005908966\n",
      "Epoch 3273/20000 Training Loss: 0.07233545929193497\n",
      "Epoch 3274/20000 Training Loss: 0.06460938602685928\n",
      "Epoch 3275/20000 Training Loss: 0.05588068068027496\n",
      "Epoch 3276/20000 Training Loss: 0.0496266670525074\n",
      "Epoch 3277/20000 Training Loss: 0.06484971195459366\n",
      "Epoch 3278/20000 Training Loss: 0.0546351782977581\n",
      "Epoch 3279/20000 Training Loss: 0.07154291123151779\n",
      "Epoch 3280/20000 Training Loss: 0.046136945486068726\n",
      "Epoch 3280/20000 Validation Loss: 0.054327890276908875\n",
      "Epoch 3281/20000 Training Loss: 0.07518855482339859\n",
      "Epoch 3282/20000 Training Loss: 0.048845645040273666\n",
      "Epoch 3283/20000 Training Loss: 0.08963823318481445\n",
      "Epoch 3284/20000 Training Loss: 0.0492074191570282\n",
      "Epoch 3285/20000 Training Loss: 0.056844767183065414\n",
      "Epoch 3286/20000 Training Loss: 0.05116678774356842\n",
      "Epoch 3287/20000 Training Loss: 0.053639162331819534\n",
      "Epoch 3288/20000 Training Loss: 0.06962919235229492\n",
      "Epoch 3289/20000 Training Loss: 0.059522490948438644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3290/20000 Training Loss: 0.07172903418540955\n",
      "Epoch 3290/20000 Validation Loss: 0.04955189302563667\n",
      "Epoch 3291/20000 Training Loss: 0.06575863063335419\n",
      "Epoch 3292/20000 Training Loss: 0.06002594158053398\n",
      "Epoch 3293/20000 Training Loss: 0.05323450639843941\n",
      "Epoch 3294/20000 Training Loss: 0.06280992180109024\n",
      "Epoch 3295/20000 Training Loss: 0.04981282353401184\n",
      "Epoch 3296/20000 Training Loss: 0.07392217963933945\n",
      "Epoch 3297/20000 Training Loss: 0.07156999409198761\n",
      "Epoch 3298/20000 Training Loss: 0.044627755880355835\n",
      "Epoch 3299/20000 Training Loss: 0.052784908562898636\n",
      "Epoch 3300/20000 Training Loss: 0.041717708110809326\n",
      "Epoch 3300/20000 Validation Loss: 0.08653111755847931\n",
      "Epoch 3301/20000 Training Loss: 0.06428496539592743\n",
      "Epoch 3302/20000 Training Loss: 0.0577876977622509\n",
      "Epoch 3303/20000 Training Loss: 0.05381596460938454\n",
      "Epoch 3304/20000 Training Loss: 0.046020686626434326\n",
      "Epoch 3305/20000 Training Loss: 0.06567732244729996\n",
      "Epoch 3306/20000 Training Loss: 0.0791425034403801\n",
      "Epoch 3307/20000 Training Loss: 0.056115686893463135\n",
      "Epoch 3308/20000 Training Loss: 0.0739312469959259\n",
      "Epoch 3309/20000 Training Loss: 0.051054179668426514\n",
      "Epoch 3310/20000 Training Loss: 0.06157638132572174\n",
      "Epoch 3310/20000 Validation Loss: 0.07749714702367783\n",
      "Epoch 3311/20000 Training Loss: 0.054525863379240036\n",
      "Epoch 3312/20000 Training Loss: 0.04169287160038948\n",
      "Epoch 3313/20000 Training Loss: 0.055189017206430435\n",
      "Epoch 3314/20000 Training Loss: 0.0533529631793499\n",
      "Epoch 3315/20000 Training Loss: 0.08180698752403259\n",
      "Epoch 3316/20000 Training Loss: 0.056359220296144485\n",
      "Epoch 3317/20000 Training Loss: 0.06575358659029007\n",
      "Epoch 3318/20000 Training Loss: 0.05129754915833473\n",
      "Epoch 3319/20000 Training Loss: 0.06542292982339859\n",
      "Epoch 3320/20000 Training Loss: 0.046317894011735916\n",
      "Epoch 3320/20000 Validation Loss: 0.07829102128744125\n",
      "Epoch 3321/20000 Training Loss: 0.06050385162234306\n",
      "Epoch 3322/20000 Training Loss: 0.07643274962902069\n",
      "Epoch 3323/20000 Training Loss: 0.0570746548473835\n",
      "Epoch 3324/20000 Training Loss: 0.04991289973258972\n",
      "Epoch 3325/20000 Training Loss: 0.0695723220705986\n",
      "Epoch 3326/20000 Training Loss: 0.05494559183716774\n",
      "Epoch 3327/20000 Training Loss: 0.04544392600655556\n",
      "Epoch 3328/20000 Training Loss: 0.049957770854234695\n",
      "Epoch 3329/20000 Training Loss: 0.057157013565301895\n",
      "Epoch 3330/20000 Training Loss: 0.036347646266222\n",
      "Epoch 3330/20000 Validation Loss: 0.08040983229875565\n",
      "Epoch 3331/20000 Training Loss: 0.06164978817105293\n",
      "Epoch 3332/20000 Training Loss: 0.04264678433537483\n",
      "Epoch 3333/20000 Training Loss: 0.04150759056210518\n",
      "Epoch 3334/20000 Training Loss: 0.045219022780656815\n",
      "Epoch 3335/20000 Training Loss: 0.05034657195210457\n",
      "Epoch 3336/20000 Training Loss: 0.04200008139014244\n",
      "Epoch 3337/20000 Training Loss: 0.055479247123003006\n",
      "Epoch 3338/20000 Training Loss: 0.06214171648025513\n",
      "Epoch 3339/20000 Training Loss: 0.06912312656641006\n",
      "Epoch 3340/20000 Training Loss: 0.07551979273557663\n",
      "Epoch 3340/20000 Validation Loss: 0.04851134866476059\n",
      "Epoch 3341/20000 Training Loss: 0.054904114454984665\n",
      "Epoch 3342/20000 Training Loss: 0.06521110981702805\n",
      "Epoch 3343/20000 Training Loss: 0.07511430978775024\n",
      "Epoch 3344/20000 Training Loss: 0.06100740656256676\n",
      "Epoch 3345/20000 Training Loss: 0.054938193410634995\n",
      "Epoch 3346/20000 Training Loss: 0.05926748737692833\n",
      "Epoch 3347/20000 Training Loss: 0.059759944677352905\n",
      "Epoch 3348/20000 Training Loss: 0.07812242954969406\n",
      "Epoch 3349/20000 Training Loss: 0.07364066690206528\n",
      "Epoch 3350/20000 Training Loss: 0.048192303627729416\n",
      "Epoch 3350/20000 Validation Loss: 0.049667201936244965\n",
      "Epoch 3351/20000 Training Loss: 0.07425051927566528\n",
      "Epoch 3352/20000 Training Loss: 0.043343085795640945\n",
      "Epoch 3353/20000 Training Loss: 0.06062574312090874\n",
      "Epoch 3354/20000 Training Loss: 0.05385228618979454\n",
      "Epoch 3355/20000 Training Loss: 0.05010806396603584\n",
      "Epoch 3356/20000 Training Loss: 0.06587889045476913\n",
      "Epoch 3357/20000 Training Loss: 0.05337272956967354\n",
      "Epoch 3358/20000 Training Loss: 0.0489945150911808\n",
      "Epoch 3359/20000 Training Loss: 0.053885847330093384\n",
      "Epoch 3360/20000 Training Loss: 0.06371133029460907\n",
      "Epoch 3360/20000 Validation Loss: 0.05966615304350853\n",
      "Epoch 3361/20000 Training Loss: 0.048218753188848495\n",
      "Epoch 3362/20000 Training Loss: 0.056594401597976685\n",
      "Epoch 3363/20000 Training Loss: 0.05998794734477997\n",
      "Epoch 3364/20000 Training Loss: 0.055246543139219284\n",
      "Epoch 3365/20000 Training Loss: 0.06579404324293137\n",
      "Epoch 3366/20000 Training Loss: 0.05111471936106682\n",
      "Epoch 3367/20000 Training Loss: 0.05446172133088112\n",
      "Epoch 3368/20000 Training Loss: 0.062348734587430954\n",
      "Epoch 3369/20000 Training Loss: 0.04871061071753502\n",
      "Epoch 3370/20000 Training Loss: 0.06091286242008209\n",
      "Epoch 3370/20000 Validation Loss: 0.0545646995306015\n",
      "Epoch 3371/20000 Training Loss: 0.03787968307733536\n",
      "Epoch 3372/20000 Training Loss: 0.06765180081129074\n",
      "Epoch 3373/20000 Training Loss: 0.0505177341401577\n",
      "Epoch 3374/20000 Training Loss: 0.05811608210206032\n",
      "Epoch 3375/20000 Training Loss: 0.052358780056238174\n",
      "Epoch 3376/20000 Training Loss: 0.046113986521959305\n",
      "Epoch 3377/20000 Training Loss: 0.055427029728889465\n",
      "Epoch 3378/20000 Training Loss: 0.05296434834599495\n",
      "Epoch 3379/20000 Training Loss: 0.050835344940423965\n",
      "Epoch 3380/20000 Training Loss: 0.06513737887144089\n",
      "Epoch 3380/20000 Validation Loss: 0.06447458267211914\n",
      "Epoch 3381/20000 Training Loss: 0.063890740275383\n",
      "Epoch 3382/20000 Training Loss: 0.05524462088942528\n",
      "Epoch 3383/20000 Training Loss: 0.06133074685931206\n",
      "Epoch 3384/20000 Training Loss: 0.05907493829727173\n",
      "Epoch 3385/20000 Training Loss: 0.049595076590776443\n",
      "Epoch 3386/20000 Training Loss: 0.049196984618902206\n",
      "Epoch 3387/20000 Training Loss: 0.06518873572349548\n",
      "Epoch 3388/20000 Training Loss: 0.06924425810575485\n",
      "Epoch 3389/20000 Training Loss: 0.052376795560121536\n",
      "Epoch 3390/20000 Training Loss: 0.06794148683547974\n",
      "Epoch 3390/20000 Validation Loss: 0.061935774981975555\n",
      "Epoch 3391/20000 Training Loss: 0.05377916991710663\n",
      "Epoch 3392/20000 Training Loss: 0.05285130813717842\n",
      "Epoch 3393/20000 Training Loss: 0.0443660132586956\n",
      "Epoch 3394/20000 Training Loss: 0.06697781383991241\n",
      "Epoch 3395/20000 Training Loss: 0.06458205729722977\n",
      "Epoch 3396/20000 Training Loss: 0.04773774743080139\n",
      "Epoch 3397/20000 Training Loss: 0.046955209225416183\n",
      "Epoch 3398/20000 Training Loss: 0.04421263560652733\n",
      "Epoch 3399/20000 Training Loss: 0.05706782266497612\n",
      "Epoch 3400/20000 Training Loss: 0.059449706226587296\n",
      "Epoch 3400/20000 Validation Loss: 0.059488698840141296\n",
      "Epoch 3401/20000 Training Loss: 0.04195943474769592\n",
      "Epoch 3402/20000 Training Loss: 0.04219183325767517\n",
      "Epoch 3403/20000 Training Loss: 0.05594636872410774\n",
      "Epoch 3404/20000 Training Loss: 0.04349061846733093\n",
      "Epoch 3405/20000 Training Loss: 0.05927574634552002\n",
      "Epoch 3406/20000 Training Loss: 0.05694219470024109\n",
      "Epoch 3407/20000 Training Loss: 0.05332276225090027\n",
      "Epoch 3408/20000 Training Loss: 0.05534942075610161\n",
      "Epoch 3409/20000 Training Loss: 0.08616680651903152\n",
      "Epoch 3410/20000 Training Loss: 0.06130967661738396\n",
      "Epoch 3410/20000 Validation Loss: 0.05079129338264465\n",
      "Epoch 3411/20000 Training Loss: 0.05329322814941406\n",
      "Epoch 3412/20000 Training Loss: 0.07015442848205566\n",
      "Epoch 3413/20000 Training Loss: 0.06940492242574692\n",
      "Epoch 3414/20000 Training Loss: 0.06974824517965317\n",
      "Epoch 3415/20000 Training Loss: 0.05914986506104469\n",
      "Epoch 3416/20000 Training Loss: 0.07099127024412155\n",
      "Epoch 3417/20000 Training Loss: 0.06414956599473953\n",
      "Epoch 3418/20000 Training Loss: 0.056022822856903076\n",
      "Epoch 3419/20000 Training Loss: 0.054675932973623276\n",
      "Epoch 3420/20000 Training Loss: 0.05688291788101196\n",
      "Epoch 3420/20000 Validation Loss: 0.04748321324586868\n",
      "Epoch 3421/20000 Training Loss: 0.050338029861450195\n",
      "Epoch 3422/20000 Training Loss: 0.054318618029356\n",
      "Epoch 3423/20000 Training Loss: 0.05299069359898567\n",
      "Epoch 3424/20000 Training Loss: 0.05725017189979553\n",
      "Epoch 3425/20000 Training Loss: 0.06766127794981003\n",
      "Epoch 3426/20000 Training Loss: 0.06410224735736847\n",
      "Epoch 3427/20000 Training Loss: 0.05782436206936836\n",
      "Epoch 3428/20000 Training Loss: 0.04924948886036873\n",
      "Epoch 3429/20000 Training Loss: 0.06798075884580612\n",
      "Epoch 3430/20000 Training Loss: 0.04841303452849388\n",
      "Epoch 3430/20000 Validation Loss: 0.09703908115625381\n",
      "Epoch 3431/20000 Training Loss: 0.057531774044036865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3432/20000 Training Loss: 0.04546884074807167\n",
      "Epoch 3433/20000 Training Loss: 0.07330196350812912\n",
      "Epoch 3434/20000 Training Loss: 0.0660887286067009\n",
      "Epoch 3435/20000 Training Loss: 0.060077980160713196\n",
      "Epoch 3436/20000 Training Loss: 0.05345180630683899\n",
      "Epoch 3437/20000 Training Loss: 0.06785022467374802\n",
      "Epoch 3438/20000 Training Loss: 0.06971050798892975\n",
      "Epoch 3439/20000 Training Loss: 0.06496662646532059\n",
      "Epoch 3440/20000 Training Loss: 0.054916709661483765\n",
      "Epoch 3440/20000 Validation Loss: 0.05536816269159317\n",
      "Epoch 3441/20000 Training Loss: 0.05305838957428932\n",
      "Epoch 3442/20000 Training Loss: 0.047851353883743286\n",
      "Epoch 3443/20000 Training Loss: 0.08541682362556458\n",
      "Epoch 3444/20000 Training Loss: 0.05131816491484642\n",
      "Epoch 3445/20000 Training Loss: 0.0634751245379448\n",
      "Epoch 3446/20000 Training Loss: 0.05117258056998253\n",
      "Epoch 3447/20000 Training Loss: 0.07203681021928787\n",
      "Epoch 3448/20000 Training Loss: 0.060101669281721115\n",
      "Epoch 3449/20000 Training Loss: 0.06276313215494156\n",
      "Epoch 3450/20000 Training Loss: 0.07386492192745209\n",
      "Epoch 3450/20000 Validation Loss: 0.07145365327596664\n",
      "Epoch 3451/20000 Training Loss: 0.06808394938707352\n",
      "Epoch 3452/20000 Training Loss: 0.05972008779644966\n",
      "Epoch 3453/20000 Training Loss: 0.04939427971839905\n",
      "Epoch 3454/20000 Training Loss: 0.05647990480065346\n",
      "Epoch 3455/20000 Training Loss: 0.05660596489906311\n",
      "Epoch 3456/20000 Training Loss: 0.06461969763040543\n",
      "Epoch 3457/20000 Training Loss: 0.04561915993690491\n",
      "Epoch 3458/20000 Training Loss: 0.05686838552355766\n",
      "Epoch 3459/20000 Training Loss: 0.0678105428814888\n",
      "Epoch 3460/20000 Training Loss: 0.05254586413502693\n",
      "Epoch 3460/20000 Validation Loss: 0.04940788820385933\n",
      "Epoch 3461/20000 Training Loss: 0.06138991191983223\n",
      "Epoch 3462/20000 Training Loss: 0.05232427641749382\n",
      "Epoch 3463/20000 Training Loss: 0.05603548511862755\n",
      "Epoch 3464/20000 Training Loss: 0.07134950906038284\n",
      "Epoch 3465/20000 Training Loss: 0.05249709263443947\n",
      "Epoch 3466/20000 Training Loss: 0.04824642464518547\n",
      "Epoch 3467/20000 Training Loss: 0.057707350701093674\n",
      "Epoch 3468/20000 Training Loss: 0.0593688078224659\n",
      "Epoch 3469/20000 Training Loss: 0.06876469403505325\n",
      "Epoch 3470/20000 Training Loss: 0.057093117386102676\n",
      "Epoch 3470/20000 Validation Loss: 0.059715986251831055\n",
      "Epoch 3471/20000 Training Loss: 0.05006008967757225\n",
      "Epoch 3472/20000 Training Loss: 0.04396935924887657\n",
      "Epoch 3473/20000 Training Loss: 0.04730899631977081\n",
      "Epoch 3474/20000 Training Loss: 0.06359544396400452\n",
      "Epoch 3475/20000 Training Loss: 0.04703444242477417\n",
      "Epoch 3476/20000 Training Loss: 0.061891619116067886\n",
      "Epoch 3477/20000 Training Loss: 0.0725841298699379\n",
      "Epoch 3478/20000 Training Loss: 0.08406919986009598\n",
      "Epoch 3479/20000 Training Loss: 0.06449747085571289\n",
      "Epoch 3480/20000 Training Loss: 0.051936548203229904\n",
      "Epoch 3480/20000 Validation Loss: 0.05284678190946579\n",
      "Epoch 3481/20000 Training Loss: 0.062011539936065674\n",
      "Epoch 3482/20000 Training Loss: 0.043904151767492294\n",
      "Epoch 3483/20000 Training Loss: 0.0458904504776001\n",
      "Epoch 3484/20000 Training Loss: 0.050692006945610046\n",
      "Epoch 3485/20000 Training Loss: 0.05200172960758209\n",
      "Epoch 3486/20000 Training Loss: 0.03901583328843117\n",
      "Epoch 3487/20000 Training Loss: 0.05221520736813545\n",
      "Epoch 3488/20000 Training Loss: 0.07172918319702148\n",
      "Epoch 3489/20000 Training Loss: 0.05080871656537056\n",
      "Epoch 3490/20000 Training Loss: 0.07194598019123077\n",
      "Epoch 3490/20000 Validation Loss: 0.06718367338180542\n",
      "Epoch 3491/20000 Training Loss: 0.029715757817029953\n",
      "Epoch 3492/20000 Training Loss: 0.0517469197511673\n",
      "Epoch 3493/20000 Training Loss: 0.05170213058590889\n",
      "Epoch 3494/20000 Training Loss: 0.06488347798585892\n",
      "Epoch 3495/20000 Training Loss: 0.06523516774177551\n",
      "Epoch 3496/20000 Training Loss: 0.0637454017996788\n",
      "Epoch 3497/20000 Training Loss: 0.07630901783704758\n",
      "Epoch 3498/20000 Training Loss: 0.050567179918289185\n",
      "Epoch 3499/20000 Training Loss: 0.0389542318880558\n",
      "Epoch 3500/20000 Training Loss: 0.047339946031570435\n",
      "Epoch 3500/20000 Validation Loss: 0.06621146202087402\n",
      "Epoch 3501/20000 Training Loss: 0.05065803602337837\n",
      "Epoch 3502/20000 Training Loss: 0.05367565155029297\n",
      "Epoch 3503/20000 Training Loss: 0.061110612004995346\n",
      "Epoch 3504/20000 Training Loss: 0.04862801730632782\n",
      "Epoch 3505/20000 Training Loss: 0.07374023646116257\n",
      "Epoch 3506/20000 Training Loss: 0.06205565109848976\n",
      "Epoch 3507/20000 Training Loss: 0.04241329804062843\n",
      "Epoch 3508/20000 Training Loss: 0.05790719762444496\n",
      "Epoch 3509/20000 Training Loss: 0.0557774156332016\n",
      "Epoch 3510/20000 Training Loss: 0.05658946558833122\n",
      "Epoch 3510/20000 Validation Loss: 0.048821598291397095\n",
      "Epoch 3511/20000 Training Loss: 0.07023680210113525\n",
      "Epoch 3512/20000 Training Loss: 0.059252411127090454\n",
      "Epoch 3513/20000 Training Loss: 0.048832815140485764\n",
      "Epoch 3514/20000 Training Loss: 0.057477712631225586\n",
      "Epoch 3515/20000 Training Loss: 0.050694193691015244\n",
      "Epoch 3516/20000 Training Loss: 0.043777093291282654\n",
      "Epoch 3517/20000 Training Loss: 0.0548238642513752\n",
      "Epoch 3518/20000 Training Loss: 0.052533432841300964\n",
      "Epoch 3519/20000 Training Loss: 0.060098085552453995\n",
      "Epoch 3520/20000 Training Loss: 0.05788755789399147\n",
      "Epoch 3520/20000 Validation Loss: 0.06268692016601562\n",
      "Epoch 3521/20000 Training Loss: 0.04558702930808067\n",
      "Epoch 3522/20000 Training Loss: 0.07565604150295258\n",
      "Epoch 3523/20000 Training Loss: 0.059332650154829025\n",
      "Epoch 3524/20000 Training Loss: 0.045937493443489075\n",
      "Epoch 3525/20000 Training Loss: 0.05953383445739746\n",
      "Epoch 3526/20000 Training Loss: 0.04698920622467995\n",
      "Epoch 3527/20000 Training Loss: 0.05181488022208214\n",
      "Epoch 3528/20000 Training Loss: 0.04652407392859459\n",
      "Epoch 3529/20000 Training Loss: 0.06449686735868454\n",
      "Epoch 3530/20000 Training Loss: 0.050405681133270264\n",
      "Epoch 3530/20000 Validation Loss: 0.05836766958236694\n",
      "Epoch 3531/20000 Training Loss: 0.05037926137447357\n",
      "Epoch 3532/20000 Training Loss: 0.0525091327726841\n",
      "Epoch 3533/20000 Training Loss: 0.06489720940589905\n",
      "Epoch 3534/20000 Training Loss: 0.058879535645246506\n",
      "Epoch 3535/20000 Training Loss: 0.061007071286439896\n",
      "Epoch 3536/20000 Training Loss: 0.06352920085191727\n",
      "Epoch 3537/20000 Training Loss: 0.061603084206581116\n",
      "Epoch 3538/20000 Training Loss: 0.07962348312139511\n",
      "Epoch 3539/20000 Training Loss: 0.04940156266093254\n",
      "Epoch 3540/20000 Training Loss: 0.05563602223992348\n",
      "Epoch 3540/20000 Validation Loss: 0.044703274965286255\n",
      "Epoch 3541/20000 Training Loss: 0.058714840561151505\n",
      "Epoch 3542/20000 Training Loss: 0.05399852991104126\n",
      "Epoch 3543/20000 Training Loss: 0.05155622959136963\n",
      "Epoch 3544/20000 Training Loss: 0.0524878166615963\n",
      "Epoch 3545/20000 Training Loss: 0.0506819486618042\n",
      "Epoch 3546/20000 Training Loss: 0.05670393630862236\n",
      "Epoch 3547/20000 Training Loss: 0.0632796362042427\n",
      "Epoch 3548/20000 Training Loss: 0.053555041551589966\n",
      "Epoch 3549/20000 Training Loss: 0.06482693552970886\n",
      "Epoch 3550/20000 Training Loss: 0.06688633561134338\n",
      "Epoch 3550/20000 Validation Loss: 0.06833061575889587\n",
      "Epoch 3551/20000 Training Loss: 0.08418479561805725\n",
      "Epoch 3552/20000 Training Loss: 0.06858140975236893\n",
      "Epoch 3553/20000 Training Loss: 0.06432496011257172\n",
      "Epoch 3554/20000 Training Loss: 0.07811341434717178\n",
      "Epoch 3555/20000 Training Loss: 0.057906460016965866\n",
      "Epoch 3556/20000 Training Loss: 0.06294546276330948\n",
      "Epoch 3557/20000 Training Loss: 0.07435782998800278\n",
      "Epoch 3558/20000 Training Loss: 0.04124808683991432\n",
      "Epoch 3559/20000 Training Loss: 0.042479258030653\n",
      "Epoch 3560/20000 Training Loss: 0.046213824301958084\n",
      "Epoch 3560/20000 Validation Loss: 0.07471120357513428\n",
      "Epoch 3561/20000 Training Loss: 0.06494846194982529\n",
      "Epoch 3562/20000 Training Loss: 0.056112140417099\n",
      "Epoch 3563/20000 Training Loss: 0.08106767386198044\n",
      "Epoch 3564/20000 Training Loss: 0.05834890529513359\n",
      "Epoch 3565/20000 Training Loss: 0.07569944113492966\n",
      "Epoch 3566/20000 Training Loss: 0.06273286789655685\n",
      "Epoch 3567/20000 Training Loss: 0.062192220240831375\n",
      "Epoch 3568/20000 Training Loss: 0.05458527430891991\n",
      "Epoch 3569/20000 Training Loss: 0.06615007668733597\n",
      "Epoch 3570/20000 Training Loss: 0.04643712937831879\n",
      "Epoch 3570/20000 Validation Loss: 0.07852636277675629\n",
      "Epoch 3571/20000 Training Loss: 0.0530080683529377\n",
      "Epoch 3572/20000 Training Loss: 0.06681904941797256\n",
      "Epoch 3573/20000 Training Loss: 0.05407188460230827\n",
      "Epoch 3574/20000 Training Loss: 0.052686531096696854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3575/20000 Training Loss: 0.0631990060210228\n",
      "Epoch 3576/20000 Training Loss: 0.06517636030912399\n",
      "Epoch 3577/20000 Training Loss: 0.06603041291236877\n",
      "Epoch 3578/20000 Training Loss: 0.05497211590409279\n",
      "Epoch 3579/20000 Training Loss: 0.061907485127449036\n",
      "Epoch 3580/20000 Training Loss: 0.07251144200563431\n",
      "Epoch 3580/20000 Validation Loss: 0.07580239325761795\n",
      "Epoch 3581/20000 Training Loss: 0.07716243714094162\n",
      "Epoch 3582/20000 Training Loss: 0.07228973507881165\n",
      "Epoch 3583/20000 Training Loss: 0.04619457200169563\n",
      "Epoch 3584/20000 Training Loss: 0.057543251663446426\n",
      "Epoch 3585/20000 Training Loss: 0.06324438005685806\n",
      "Epoch 3586/20000 Training Loss: 0.053423717617988586\n",
      "Epoch 3587/20000 Training Loss: 0.0495186485350132\n",
      "Epoch 3588/20000 Training Loss: 0.05314436927437782\n",
      "Epoch 3589/20000 Training Loss: 0.04284605383872986\n",
      "Epoch 3590/20000 Training Loss: 0.05730700492858887\n",
      "Epoch 3590/20000 Validation Loss: 0.08142377436161041\n",
      "Epoch 3591/20000 Training Loss: 0.0569436140358448\n",
      "Epoch 3592/20000 Training Loss: 0.06213410571217537\n",
      "Epoch 3593/20000 Training Loss: 0.05653952434659004\n",
      "Epoch 3594/20000 Training Loss: 0.07553374022245407\n",
      "Epoch 3595/20000 Training Loss: 0.0557909794151783\n",
      "Epoch 3596/20000 Training Loss: 0.06981200724840164\n",
      "Epoch 3597/20000 Training Loss: 0.05528305470943451\n",
      "Epoch 3598/20000 Training Loss: 0.05648788809776306\n",
      "Epoch 3599/20000 Training Loss: 0.057468950748443604\n",
      "Epoch 3600/20000 Training Loss: 0.0608777217566967\n",
      "Epoch 3600/20000 Validation Loss: 0.08157673478126526\n",
      "Epoch 3601/20000 Training Loss: 0.05704745277762413\n",
      "Epoch 3602/20000 Training Loss: 0.07013208419084549\n",
      "Epoch 3603/20000 Training Loss: 0.06890816241502762\n",
      "Epoch 3604/20000 Training Loss: 0.059111371636390686\n",
      "Epoch 3605/20000 Training Loss: 0.05578143522143364\n",
      "Epoch 3606/20000 Training Loss: 0.04890674352645874\n",
      "Epoch 3607/20000 Training Loss: 0.06124507263302803\n",
      "Epoch 3608/20000 Training Loss: 0.059085991233587265\n",
      "Epoch 3609/20000 Training Loss: 0.04906801879405975\n",
      "Epoch 3610/20000 Training Loss: 0.07070057839155197\n",
      "Epoch 3610/20000 Validation Loss: 0.03941034525632858\n",
      "Epoch 3611/20000 Training Loss: 0.057117659598588943\n",
      "Epoch 3612/20000 Training Loss: 0.052042171359062195\n",
      "Epoch 3613/20000 Training Loss: 0.06552199274301529\n",
      "Epoch 3614/20000 Training Loss: 0.06499367207288742\n",
      "Epoch 3615/20000 Training Loss: 0.048896174877882004\n",
      "Epoch 3616/20000 Training Loss: 0.044935405254364014\n",
      "Epoch 3617/20000 Training Loss: 0.052293408662080765\n",
      "Epoch 3618/20000 Training Loss: 0.04482434317469597\n",
      "Epoch 3619/20000 Training Loss: 0.04515450820326805\n",
      "Epoch 3620/20000 Training Loss: 0.08091915398836136\n",
      "Epoch 3620/20000 Validation Loss: 0.044818732887506485\n",
      "Epoch 3621/20000 Training Loss: 0.06534399837255478\n",
      "Epoch 3622/20000 Training Loss: 0.06542742997407913\n",
      "Epoch 3623/20000 Training Loss: 0.06392750144004822\n",
      "Epoch 3624/20000 Training Loss: 0.04827672615647316\n",
      "Epoch 3625/20000 Training Loss: 0.04005533456802368\n",
      "Epoch 3626/20000 Training Loss: 0.04730294644832611\n",
      "Epoch 3627/20000 Training Loss: 0.05614587292075157\n",
      "Epoch 3628/20000 Training Loss: 0.054508477449417114\n",
      "Epoch 3629/20000 Training Loss: 0.06149759888648987\n",
      "Epoch 3630/20000 Training Loss: 0.066190205514431\n",
      "Epoch 3630/20000 Validation Loss: 0.04849995672702789\n",
      "Epoch 3631/20000 Training Loss: 0.07481382042169571\n",
      "Epoch 3632/20000 Training Loss: 0.03883327916264534\n",
      "Epoch 3633/20000 Training Loss: 0.05853461101651192\n",
      "Epoch 3634/20000 Training Loss: 0.06456208974123001\n",
      "Epoch 3635/20000 Training Loss: 0.06439438462257385\n",
      "Epoch 3636/20000 Training Loss: 0.06795478612184525\n",
      "Epoch 3637/20000 Training Loss: 0.07263711094856262\n",
      "Epoch 3638/20000 Training Loss: 0.06821298599243164\n",
      "Epoch 3639/20000 Training Loss: 0.05833126977086067\n",
      "Epoch 3640/20000 Training Loss: 0.08862272650003433\n",
      "Epoch 3640/20000 Validation Loss: 0.05886663869023323\n",
      "Epoch 3641/20000 Training Loss: 0.03887394815683365\n",
      "Epoch 3642/20000 Training Loss: 0.04911414906382561\n",
      "Epoch 3643/20000 Training Loss: 0.06742562353610992\n",
      "Epoch 3644/20000 Training Loss: 0.05615950748324394\n",
      "Epoch 3645/20000 Training Loss: 0.06808993965387344\n",
      "Epoch 3646/20000 Training Loss: 0.04875722900032997\n",
      "Epoch 3647/20000 Training Loss: 0.06878574937582016\n",
      "Epoch 3648/20000 Training Loss: 0.05331631377339363\n",
      "Epoch 3649/20000 Training Loss: 0.050952449440956116\n",
      "Epoch 3650/20000 Training Loss: 0.0585796944797039\n",
      "Epoch 3650/20000 Validation Loss: 0.05407804995775223\n",
      "Epoch 3651/20000 Training Loss: 0.05777842923998833\n",
      "Epoch 3652/20000 Training Loss: 0.06671904772520065\n",
      "Epoch 3653/20000 Training Loss: 0.04556715488433838\n",
      "Epoch 3654/20000 Training Loss: 0.07713096588850021\n",
      "Epoch 3655/20000 Training Loss: 0.05194205418229103\n",
      "Epoch 3656/20000 Training Loss: 0.054301053285598755\n",
      "Epoch 3657/20000 Training Loss: 0.052022580057382584\n",
      "Epoch 3658/20000 Training Loss: 0.08488889783620834\n",
      "Epoch 3659/20000 Training Loss: 0.0633142814040184\n",
      "Epoch 3660/20000 Training Loss: 0.056204576045274734\n",
      "Epoch 3660/20000 Validation Loss: 0.05898858606815338\n",
      "Epoch 3661/20000 Training Loss: 0.07297268509864807\n",
      "Epoch 3662/20000 Training Loss: 0.055306822061538696\n",
      "Epoch 3663/20000 Training Loss: 0.056503888219594955\n",
      "Epoch 3664/20000 Training Loss: 0.06041041016578674\n",
      "Epoch 3665/20000 Training Loss: 0.06342445313930511\n",
      "Epoch 3666/20000 Training Loss: 0.05455358698964119\n",
      "Epoch 3667/20000 Training Loss: 0.03666170313954353\n",
      "Epoch 3668/20000 Training Loss: 0.050140079110860825\n",
      "Epoch 3669/20000 Training Loss: 0.0725405216217041\n",
      "Epoch 3670/20000 Training Loss: 0.07087334990501404\n",
      "Epoch 3670/20000 Validation Loss: 0.057222820818424225\n",
      "Epoch 3671/20000 Training Loss: 0.05847814679145813\n",
      "Epoch 3672/20000 Training Loss: 0.06072111055254936\n",
      "Epoch 3673/20000 Training Loss: 0.05893251299858093\n",
      "Epoch 3674/20000 Training Loss: 0.06279899924993515\n",
      "Epoch 3675/20000 Training Loss: 0.0479549765586853\n",
      "Epoch 3676/20000 Training Loss: 0.05327415466308594\n",
      "Epoch 3677/20000 Training Loss: 0.0673576220870018\n",
      "Epoch 3678/20000 Training Loss: 0.058225419372320175\n",
      "Epoch 3679/20000 Training Loss: 0.046100396662950516\n",
      "Epoch 3680/20000 Training Loss: 0.06841368973255157\n",
      "Epoch 3680/20000 Validation Loss: 0.07777045667171478\n",
      "Epoch 3681/20000 Training Loss: 0.04808290675282478\n",
      "Epoch 3682/20000 Training Loss: 0.06047149375081062\n",
      "Epoch 3683/20000 Training Loss: 0.052389245480298996\n",
      "Epoch 3684/20000 Training Loss: 0.06351340562105179\n",
      "Epoch 3685/20000 Training Loss: 0.06624654680490494\n",
      "Epoch 3686/20000 Training Loss: 0.04718086123466492\n",
      "Epoch 3687/20000 Training Loss: 0.06529676914215088\n",
      "Epoch 3688/20000 Training Loss: 0.03969914838671684\n",
      "Epoch 3689/20000 Training Loss: 0.060095299035310745\n",
      "Epoch 3690/20000 Training Loss: 0.05582967773079872\n",
      "Epoch 3690/20000 Validation Loss: 0.07586687803268433\n",
      "Epoch 3691/20000 Training Loss: 0.04496793821454048\n",
      "Epoch 3692/20000 Training Loss: 0.03738054633140564\n",
      "Epoch 3693/20000 Training Loss: 0.05056890472769737\n",
      "Epoch 3694/20000 Training Loss: 0.07001645863056183\n",
      "Epoch 3695/20000 Training Loss: 0.04566548392176628\n",
      "Epoch 3696/20000 Training Loss: 0.05949973687529564\n",
      "Epoch 3697/20000 Training Loss: 0.0516635961830616\n",
      "Epoch 3698/20000 Training Loss: 0.06303372234106064\n",
      "Epoch 3699/20000 Training Loss: 0.06284753233194351\n",
      "Epoch 3700/20000 Training Loss: 0.05780985578894615\n",
      "Epoch 3700/20000 Validation Loss: 0.056907929480075836\n",
      "Epoch 3701/20000 Training Loss: 0.05753509700298309\n",
      "Epoch 3702/20000 Training Loss: 0.06465480476617813\n",
      "Epoch 3703/20000 Training Loss: 0.06391280144453049\n",
      "Epoch 3704/20000 Training Loss: 0.07119753956794739\n",
      "Epoch 3705/20000 Training Loss: 0.06175950542092323\n",
      "Epoch 3706/20000 Training Loss: 0.05569331720471382\n",
      "Epoch 3707/20000 Training Loss: 0.06056882068514824\n",
      "Epoch 3708/20000 Training Loss: 0.06190629303455353\n",
      "Epoch 3709/20000 Training Loss: 0.059900879859924316\n",
      "Epoch 3710/20000 Training Loss: 0.05227426812052727\n",
      "Epoch 3710/20000 Validation Loss: 0.06171633303165436\n",
      "Epoch 3711/20000 Training Loss: 0.066159188747406\n",
      "Epoch 3712/20000 Training Loss: 0.06090446189045906\n",
      "Epoch 3713/20000 Training Loss: 0.05457316339015961\n",
      "Epoch 3714/20000 Training Loss: 0.04331951215863228\n",
      "Epoch 3715/20000 Training Loss: 0.06387361139059067\n",
      "Epoch 3716/20000 Training Loss: 0.04968445375561714\n",
      "Epoch 3717/20000 Training Loss: 0.06937401741743088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3718/20000 Training Loss: 0.04237644374370575\n",
      "Epoch 3719/20000 Training Loss: 0.057500749826431274\n",
      "Epoch 3720/20000 Training Loss: 0.06486710160970688\n",
      "Epoch 3720/20000 Validation Loss: 0.05540819466114044\n",
      "Epoch 3721/20000 Training Loss: 0.061684299260377884\n",
      "Epoch 3722/20000 Training Loss: 0.05050298571586609\n",
      "Epoch 3723/20000 Training Loss: 0.057482074946165085\n",
      "Epoch 3724/20000 Training Loss: 0.06184617057442665\n",
      "Epoch 3725/20000 Training Loss: 0.06378500163555145\n",
      "Epoch 3726/20000 Training Loss: 0.06191335618495941\n",
      "Epoch 3727/20000 Training Loss: 0.07012312859296799\n",
      "Epoch 3728/20000 Training Loss: 0.07702788710594177\n",
      "Epoch 3729/20000 Training Loss: 0.0746716856956482\n",
      "Epoch 3730/20000 Training Loss: 0.05245650187134743\n",
      "Epoch 3730/20000 Validation Loss: 0.07296553999185562\n",
      "Epoch 3731/20000 Training Loss: 0.05829437077045441\n",
      "Epoch 3732/20000 Training Loss: 0.05446675047278404\n",
      "Epoch 3733/20000 Training Loss: 0.06459984928369522\n",
      "Epoch 3734/20000 Training Loss: 0.04956571385264397\n",
      "Epoch 3735/20000 Training Loss: 0.07359817624092102\n",
      "Epoch 3736/20000 Training Loss: 0.06529021263122559\n",
      "Epoch 3737/20000 Training Loss: 0.05957769230008125\n",
      "Epoch 3738/20000 Training Loss: 0.06120660528540611\n",
      "Epoch 3739/20000 Training Loss: 0.052690863609313965\n",
      "Epoch 3740/20000 Training Loss: 0.061906903982162476\n",
      "Epoch 3740/20000 Validation Loss: 0.05058542639017105\n",
      "Epoch 3741/20000 Training Loss: 0.04650873318314552\n",
      "Epoch 3742/20000 Training Loss: 0.06624940037727356\n",
      "Epoch 3743/20000 Training Loss: 0.06671056896448135\n",
      "Epoch 3744/20000 Training Loss: 0.06108005717396736\n",
      "Epoch 3745/20000 Training Loss: 0.07034569978713989\n",
      "Epoch 3746/20000 Training Loss: 0.06131306290626526\n",
      "Epoch 3747/20000 Training Loss: 0.07587844133377075\n",
      "Epoch 3748/20000 Training Loss: 0.049764733761548996\n",
      "Epoch 3749/20000 Training Loss: 0.06908557564020157\n",
      "Epoch 3750/20000 Training Loss: 0.05568711832165718\n",
      "Epoch 3750/20000 Validation Loss: 0.06821729242801666\n",
      "Epoch 3751/20000 Training Loss: 0.04808114469051361\n",
      "Epoch 3752/20000 Training Loss: 0.04664092883467674\n",
      "Epoch 3753/20000 Training Loss: 0.052352193742990494\n",
      "Epoch 3754/20000 Training Loss: 0.06176535412669182\n",
      "Epoch 3755/20000 Training Loss: 0.05803242325782776\n",
      "Epoch 3756/20000 Training Loss: 0.045564714819192886\n",
      "Epoch 3757/20000 Training Loss: 0.059087902307510376\n",
      "Epoch 3758/20000 Training Loss: 0.056662410497665405\n",
      "Epoch 3759/20000 Training Loss: 0.05591122433543205\n",
      "Epoch 3760/20000 Training Loss: 0.05286707356572151\n",
      "Epoch 3760/20000 Validation Loss: 0.05985362455248833\n",
      "Epoch 3761/20000 Training Loss: 0.06011523678898811\n",
      "Epoch 3762/20000 Training Loss: 0.05620351806282997\n",
      "Epoch 3763/20000 Training Loss: 0.05955807864665985\n",
      "Epoch 3764/20000 Training Loss: 0.07508327811956406\n",
      "Epoch 3765/20000 Training Loss: 0.04949186369776726\n",
      "Epoch 3766/20000 Training Loss: 0.05251172184944153\n",
      "Epoch 3767/20000 Training Loss: 0.06886862963438034\n",
      "Epoch 3768/20000 Training Loss: 0.07860049605369568\n",
      "Epoch 3769/20000 Training Loss: 0.05286410450935364\n",
      "Epoch 3770/20000 Training Loss: 0.06653939932584763\n",
      "Epoch 3770/20000 Validation Loss: 0.054294124245643616\n",
      "Epoch 3771/20000 Training Loss: 0.058998990803956985\n",
      "Epoch 3772/20000 Training Loss: 0.06871991604566574\n",
      "Epoch 3773/20000 Training Loss: 0.06571229547262192\n",
      "Epoch 3774/20000 Training Loss: 0.05526852235198021\n",
      "Epoch 3775/20000 Training Loss: 0.057193171232938766\n",
      "Epoch 3776/20000 Training Loss: 0.04864026978611946\n",
      "Epoch 3777/20000 Training Loss: 0.05090475082397461\n",
      "Epoch 3778/20000 Training Loss: 0.06753888726234436\n",
      "Epoch 3779/20000 Training Loss: 0.06969842314720154\n",
      "Epoch 3780/20000 Training Loss: 0.04847629740834236\n",
      "Epoch 3780/20000 Validation Loss: 0.05847776308655739\n",
      "Epoch 3781/20000 Training Loss: 0.07886520028114319\n",
      "Epoch 3782/20000 Training Loss: 0.037592798471450806\n",
      "Epoch 3783/20000 Training Loss: 0.060520946979522705\n",
      "Epoch 3784/20000 Training Loss: 0.044864069670438766\n",
      "Epoch 3785/20000 Training Loss: 0.05111902952194214\n",
      "Epoch 3786/20000 Training Loss: 0.06660353392362595\n",
      "Epoch 3787/20000 Training Loss: 0.05278647318482399\n",
      "Epoch 3788/20000 Training Loss: 0.062122393399477005\n",
      "Epoch 3789/20000 Training Loss: 0.04119306430220604\n",
      "Epoch 3790/20000 Training Loss: 0.04935986176133156\n",
      "Epoch 3790/20000 Validation Loss: 0.058175284415483475\n",
      "Epoch 3791/20000 Training Loss: 0.056795984506607056\n",
      "Epoch 3792/20000 Training Loss: 0.06658172607421875\n",
      "Epoch 3793/20000 Training Loss: 0.0709366574883461\n",
      "Epoch 3794/20000 Training Loss: 0.05982506275177002\n",
      "Epoch 3795/20000 Training Loss: 0.04707801342010498\n",
      "Epoch 3796/20000 Training Loss: 0.05115132033824921\n",
      "Epoch 3797/20000 Training Loss: 0.04936251416802406\n",
      "Epoch 3798/20000 Training Loss: 0.06384336948394775\n",
      "Epoch 3799/20000 Training Loss: 0.04662002995610237\n",
      "Epoch 3800/20000 Training Loss: 0.08128715306520462\n",
      "Epoch 3800/20000 Validation Loss: 0.0443577915430069\n",
      "Epoch 3801/20000 Training Loss: 0.05485859885811806\n",
      "Epoch 3802/20000 Training Loss: 0.0684698224067688\n",
      "Epoch 3803/20000 Training Loss: 0.06380128860473633\n",
      "Epoch 3804/20000 Training Loss: 0.052046965807676315\n",
      "Epoch 3805/20000 Training Loss: 0.05324529483914375\n",
      "Epoch 3806/20000 Training Loss: 0.058245327323675156\n",
      "Epoch 3807/20000 Training Loss: 0.05637363716959953\n",
      "Epoch 3808/20000 Training Loss: 0.05050649121403694\n",
      "Epoch 3809/20000 Training Loss: 0.052778054028749466\n",
      "Epoch 3810/20000 Training Loss: 0.05161331221461296\n",
      "Epoch 3810/20000 Validation Loss: 0.06328470259904861\n",
      "Epoch 3811/20000 Training Loss: 0.05494457483291626\n",
      "Epoch 3812/20000 Training Loss: 0.05251355841755867\n",
      "Epoch 3813/20000 Training Loss: 0.054685693234205246\n",
      "Epoch 3814/20000 Training Loss: 0.052915263921022415\n",
      "Epoch 3815/20000 Training Loss: 0.04764019325375557\n",
      "Epoch 3816/20000 Training Loss: 0.07609409838914871\n",
      "Epoch 3817/20000 Training Loss: 0.06803250312805176\n",
      "Epoch 3818/20000 Training Loss: 0.05182720348238945\n",
      "Epoch 3819/20000 Training Loss: 0.07667817920446396\n",
      "Epoch 3820/20000 Training Loss: 0.0608980655670166\n",
      "Epoch 3820/20000 Validation Loss: 0.06383071094751358\n",
      "Epoch 3821/20000 Training Loss: 0.0646575465798378\n",
      "Epoch 3822/20000 Training Loss: 0.05057936906814575\n",
      "Epoch 3823/20000 Training Loss: 0.06145313382148743\n",
      "Epoch 3824/20000 Training Loss: 0.0669582411646843\n",
      "Epoch 3825/20000 Training Loss: 0.05520175024867058\n",
      "Epoch 3826/20000 Training Loss: 0.04913322255015373\n",
      "Epoch 3827/20000 Training Loss: 0.05445379018783569\n",
      "Epoch 3828/20000 Training Loss: 0.053731948137283325\n",
      "Epoch 3829/20000 Training Loss: 0.06358373910188675\n",
      "Epoch 3830/20000 Training Loss: 0.04625066742300987\n",
      "Epoch 3830/20000 Validation Loss: 0.04457785189151764\n",
      "Epoch 3831/20000 Training Loss: 0.046039458364248276\n",
      "Epoch 3832/20000 Training Loss: 0.07385944575071335\n",
      "Epoch 3833/20000 Training Loss: 0.06625854223966599\n",
      "Epoch 3834/20000 Training Loss: 0.07026495784521103\n",
      "Epoch 3835/20000 Training Loss: 0.06473305076360703\n",
      "Epoch 3836/20000 Training Loss: 0.05536596104502678\n",
      "Epoch 3837/20000 Training Loss: 0.06926486641168594\n",
      "Epoch 3838/20000 Training Loss: 0.05163532868027687\n",
      "Epoch 3839/20000 Training Loss: 0.049619078636169434\n",
      "Epoch 3840/20000 Training Loss: 0.06721114367246628\n",
      "Epoch 3840/20000 Validation Loss: 0.06021883338689804\n",
      "Epoch 3841/20000 Training Loss: 0.05966643616557121\n",
      "Epoch 3842/20000 Training Loss: 0.04876575991511345\n",
      "Epoch 3843/20000 Training Loss: 0.057048261165618896\n",
      "Epoch 3844/20000 Training Loss: 0.059891119599342346\n",
      "Epoch 3845/20000 Training Loss: 0.06636234372854233\n",
      "Epoch 3846/20000 Training Loss: 0.07317361980676651\n",
      "Epoch 3847/20000 Training Loss: 0.07901852577924728\n",
      "Epoch 3848/20000 Training Loss: 0.07153386622667313\n",
      "Epoch 3849/20000 Training Loss: 0.05396397039294243\n",
      "Epoch 3850/20000 Training Loss: 0.06518234312534332\n",
      "Epoch 3850/20000 Validation Loss: 0.05604970455169678\n",
      "Epoch 3851/20000 Training Loss: 0.05791354551911354\n",
      "Epoch 3852/20000 Training Loss: 0.05808708071708679\n",
      "Epoch 3853/20000 Training Loss: 0.048814672976732254\n",
      "Epoch 3854/20000 Training Loss: 0.06454526633024216\n",
      "Epoch 3855/20000 Training Loss: 0.07021218538284302\n",
      "Epoch 3856/20000 Training Loss: 0.07174576818943024\n",
      "Epoch 3857/20000 Training Loss: 0.047682587057352066\n",
      "Epoch 3858/20000 Training Loss: 0.047310441732406616\n",
      "Epoch 3859/20000 Training Loss: 0.07656843960285187\n",
      "Epoch 3860/20000 Training Loss: 0.06993213295936584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3860/20000 Validation Loss: 0.07722204923629761\n",
      "Epoch 3861/20000 Training Loss: 0.06450719386339188\n",
      "Epoch 3862/20000 Training Loss: 0.04667389392852783\n",
      "Epoch 3863/20000 Training Loss: 0.06888563185930252\n",
      "Epoch 3864/20000 Training Loss: 0.052500393241643906\n",
      "Epoch 3865/20000 Training Loss: 0.059245508164167404\n",
      "Epoch 3866/20000 Training Loss: 0.061724405735731125\n",
      "Epoch 3867/20000 Training Loss: 0.062675841152668\n",
      "Epoch 3868/20000 Training Loss: 0.06961149722337723\n",
      "Epoch 3869/20000 Training Loss: 0.04552365466952324\n",
      "Epoch 3870/20000 Training Loss: 0.06165677309036255\n",
      "Epoch 3870/20000 Validation Loss: 0.06934252381324768\n",
      "Epoch 3871/20000 Training Loss: 0.062139611691236496\n",
      "Epoch 3872/20000 Training Loss: 0.04968656599521637\n",
      "Epoch 3873/20000 Training Loss: 0.060377974063158035\n",
      "Epoch 3874/20000 Training Loss: 0.06430546939373016\n",
      "Epoch 3875/20000 Training Loss: 0.08136605471372604\n",
      "Epoch 3876/20000 Training Loss: 0.055346567183732986\n",
      "Epoch 3877/20000 Training Loss: 0.044617000967264175\n",
      "Epoch 3878/20000 Training Loss: 0.07032471150159836\n",
      "Epoch 3879/20000 Training Loss: 0.06498481333255768\n",
      "Epoch 3880/20000 Training Loss: 0.04971858859062195\n",
      "Epoch 3880/20000 Validation Loss: 0.04779210686683655\n",
      "Epoch 3881/20000 Training Loss: 0.049610286951065063\n",
      "Epoch 3882/20000 Training Loss: 0.06091618165373802\n",
      "Epoch 3883/20000 Training Loss: 0.04337922856211662\n",
      "Epoch 3884/20000 Training Loss: 0.06004245579242706\n",
      "Epoch 3885/20000 Training Loss: 0.04397548362612724\n",
      "Epoch 3886/20000 Training Loss: 0.06444677710533142\n",
      "Epoch 3887/20000 Training Loss: 0.06465084850788116\n",
      "Epoch 3888/20000 Training Loss: 0.054050687700510025\n",
      "Epoch 3889/20000 Training Loss: 0.04884389415383339\n",
      "Epoch 3890/20000 Training Loss: 0.05325135588645935\n",
      "Epoch 3890/20000 Validation Loss: 0.043360237032175064\n",
      "Epoch 3891/20000 Training Loss: 0.0626797154545784\n",
      "Epoch 3892/20000 Training Loss: 0.0516875796020031\n",
      "Epoch 3893/20000 Training Loss: 0.0772920474410057\n",
      "Epoch 3894/20000 Training Loss: 0.0753161609172821\n",
      "Epoch 3895/20000 Training Loss: 0.07028045505285263\n",
      "Epoch 3896/20000 Training Loss: 0.04852880537509918\n",
      "Epoch 3897/20000 Training Loss: 0.06364353746175766\n",
      "Epoch 3898/20000 Training Loss: 0.07485025376081467\n",
      "Epoch 3899/20000 Training Loss: 0.0454888790845871\n",
      "Epoch 3900/20000 Training Loss: 0.05602747201919556\n",
      "Epoch 3900/20000 Validation Loss: 0.07347802817821503\n",
      "Epoch 3901/20000 Training Loss: 0.059391215443611145\n",
      "Epoch 3902/20000 Training Loss: 0.06461253017187119\n",
      "Epoch 3903/20000 Training Loss: 0.04827604815363884\n",
      "Epoch 3904/20000 Training Loss: 0.052942488342523575\n",
      "Epoch 3905/20000 Training Loss: 0.03968757763504982\n",
      "Epoch 3906/20000 Training Loss: 0.0686497911810875\n",
      "Epoch 3907/20000 Training Loss: 0.04634486511349678\n",
      "Epoch 3908/20000 Training Loss: 0.0759611502289772\n",
      "Epoch 3909/20000 Training Loss: 0.04825293645262718\n",
      "Epoch 3910/20000 Training Loss: 0.05799328163266182\n",
      "Epoch 3910/20000 Validation Loss: 0.048952262848615646\n",
      "Epoch 3911/20000 Training Loss: 0.06467033177614212\n",
      "Epoch 3912/20000 Training Loss: 0.062623530626297\n",
      "Epoch 3913/20000 Training Loss: 0.05496455356478691\n",
      "Epoch 3914/20000 Training Loss: 0.0557682029902935\n",
      "Epoch 3915/20000 Training Loss: 0.04750405624508858\n",
      "Epoch 3916/20000 Training Loss: 0.04671698808670044\n",
      "Epoch 3917/20000 Training Loss: 0.04561368748545647\n",
      "Epoch 3918/20000 Training Loss: 0.04374595358967781\n",
      "Epoch 3919/20000 Training Loss: 0.055612172931432724\n",
      "Epoch 3920/20000 Training Loss: 0.05170061066746712\n",
      "Epoch 3920/20000 Validation Loss: 0.04919290915131569\n",
      "Epoch 3921/20000 Training Loss: 0.06171194836497307\n",
      "Epoch 3922/20000 Training Loss: 0.058103516697883606\n",
      "Epoch 3923/20000 Training Loss: 0.04215565323829651\n",
      "Epoch 3924/20000 Training Loss: 0.07573499530553818\n",
      "Epoch 3925/20000 Training Loss: 0.057396721094846725\n",
      "Epoch 3926/20000 Training Loss: 0.05610860511660576\n",
      "Epoch 3927/20000 Training Loss: 0.06849819421768188\n",
      "Epoch 3928/20000 Training Loss: 0.04809728264808655\n",
      "Epoch 3929/20000 Training Loss: 0.0839737057685852\n",
      "Epoch 3930/20000 Training Loss: 0.05439754202961922\n",
      "Epoch 3930/20000 Validation Loss: 0.0829327180981636\n",
      "Epoch 3931/20000 Training Loss: 0.06493140012025833\n",
      "Epoch 3932/20000 Training Loss: 0.06400543451309204\n",
      "Epoch 3933/20000 Training Loss: 0.05233760550618172\n",
      "Epoch 3934/20000 Training Loss: 0.06679330766201019\n",
      "Epoch 3935/20000 Training Loss: 0.057280778884887695\n",
      "Epoch 3936/20000 Training Loss: 0.055377405136823654\n",
      "Epoch 3937/20000 Training Loss: 0.04076114296913147\n",
      "Epoch 3938/20000 Training Loss: 0.04968667030334473\n",
      "Epoch 3939/20000 Training Loss: 0.0534512996673584\n",
      "Epoch 3940/20000 Training Loss: 0.04574446380138397\n",
      "Epoch 3940/20000 Validation Loss: 0.06820642948150635\n",
      "Epoch 3941/20000 Training Loss: 0.06867177039384842\n",
      "Epoch 3942/20000 Training Loss: 0.04880872368812561\n",
      "Epoch 3943/20000 Training Loss: 0.06563147902488708\n",
      "Epoch 3944/20000 Training Loss: 0.055033162236213684\n",
      "Epoch 3945/20000 Training Loss: 0.06317330151796341\n",
      "Epoch 3946/20000 Training Loss: 0.05439338460564613\n",
      "Epoch 3947/20000 Training Loss: 0.061527129262685776\n",
      "Epoch 3948/20000 Training Loss: 0.055107232183218\n",
      "Epoch 3949/20000 Training Loss: 0.059019919484853745\n",
      "Epoch 3950/20000 Training Loss: 0.06033909320831299\n",
      "Epoch 3950/20000 Validation Loss: 0.04811299219727516\n",
      "Epoch 3951/20000 Training Loss: 0.050082311034202576\n",
      "Epoch 3952/20000 Training Loss: 0.04783940687775612\n",
      "Epoch 3953/20000 Training Loss: 0.04910436272621155\n",
      "Epoch 3954/20000 Training Loss: 0.05768446996808052\n",
      "Epoch 3955/20000 Training Loss: 0.049930404871702194\n",
      "Epoch 3956/20000 Training Loss: 0.051108818501234055\n",
      "Epoch 3957/20000 Training Loss: 0.04399721324443817\n",
      "Epoch 3958/20000 Training Loss: 0.05252513289451599\n",
      "Epoch 3959/20000 Training Loss: 0.05237394943833351\n",
      "Epoch 3960/20000 Training Loss: 0.06051710620522499\n",
      "Epoch 3960/20000 Validation Loss: 0.079715795814991\n",
      "Epoch 3961/20000 Training Loss: 0.06271197646856308\n",
      "Epoch 3962/20000 Training Loss: 0.04754456505179405\n",
      "Epoch 3963/20000 Training Loss: 0.06614667922258377\n",
      "Epoch 3964/20000 Training Loss: 0.05375446006655693\n",
      "Epoch 3965/20000 Training Loss: 0.056663576513528824\n",
      "Epoch 3966/20000 Training Loss: 0.044972579926252365\n",
      "Epoch 3967/20000 Training Loss: 0.06266080588102341\n",
      "Epoch 3968/20000 Training Loss: 0.04432263597846031\n",
      "Epoch 3969/20000 Training Loss: 0.0611339695751667\n",
      "Epoch 3970/20000 Training Loss: 0.05995015427470207\n",
      "Epoch 3970/20000 Validation Loss: 0.052440278232097626\n",
      "Epoch 3971/20000 Training Loss: 0.07080787420272827\n",
      "Epoch 3972/20000 Training Loss: 0.05960918962955475\n",
      "Epoch 3973/20000 Training Loss: 0.06581058353185654\n",
      "Epoch 3974/20000 Training Loss: 0.05047066509723663\n",
      "Epoch 3975/20000 Training Loss: 0.05956044793128967\n",
      "Epoch 3976/20000 Training Loss: 0.062356460839509964\n",
      "Epoch 3977/20000 Training Loss: 0.0644664466381073\n",
      "Epoch 3978/20000 Training Loss: 0.06339120864868164\n",
      "Epoch 3979/20000 Training Loss: 0.058681290596723557\n",
      "Epoch 3980/20000 Training Loss: 0.05007367953658104\n",
      "Epoch 3980/20000 Validation Loss: 0.07024279981851578\n",
      "Epoch 3981/20000 Training Loss: 0.05078553780913353\n",
      "Epoch 3982/20000 Training Loss: 0.04688045382499695\n",
      "Epoch 3983/20000 Training Loss: 0.05396107956767082\n",
      "Epoch 3984/20000 Training Loss: 0.044123101979494095\n",
      "Epoch 3985/20000 Training Loss: 0.06567706912755966\n",
      "Epoch 3986/20000 Training Loss: 0.04424833133816719\n",
      "Epoch 3987/20000 Training Loss: 0.06113997474312782\n",
      "Epoch 3988/20000 Training Loss: 0.05519341304898262\n",
      "Epoch 3989/20000 Training Loss: 0.05774247646331787\n",
      "Epoch 3990/20000 Training Loss: 0.06643335521221161\n",
      "Epoch 3990/20000 Validation Loss: 0.06136203557252884\n",
      "Epoch 3991/20000 Training Loss: 0.07715645432472229\n",
      "Epoch 3992/20000 Training Loss: 0.03572940453886986\n",
      "Epoch 3993/20000 Training Loss: 0.07155423611402512\n",
      "Epoch 3994/20000 Training Loss: 0.043779805302619934\n",
      "Epoch 3995/20000 Training Loss: 0.0608280748128891\n",
      "Epoch 3996/20000 Training Loss: 0.07808652520179749\n",
      "Epoch 3997/20000 Training Loss: 0.04869722202420235\n",
      "Epoch 3998/20000 Training Loss: 0.04434623941779137\n",
      "Epoch 3999/20000 Training Loss: 0.08274813741445541\n",
      "Epoch 4000/20000 Training Loss: 0.05791802704334259\n",
      "Epoch 4000/20000 Validation Loss: 0.04915185272693634\n",
      "Epoch 4001/20000 Training Loss: 0.04406650364398956\n",
      "Epoch 4002/20000 Training Loss: 0.04860374331474304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4003/20000 Training Loss: 0.05901205912232399\n",
      "Epoch 4004/20000 Training Loss: 0.0705873891711235\n",
      "Epoch 4005/20000 Training Loss: 0.06537717580795288\n",
      "Epoch 4006/20000 Training Loss: 0.047338202595710754\n",
      "Epoch 4007/20000 Training Loss: 0.07380729913711548\n",
      "Epoch 4008/20000 Training Loss: 0.05941290035843849\n",
      "Epoch 4009/20000 Training Loss: 0.04786953330039978\n",
      "Epoch 4010/20000 Training Loss: 0.05897306278347969\n",
      "Epoch 4010/20000 Validation Loss: 0.04562624543905258\n",
      "Epoch 4011/20000 Training Loss: 0.0652889832854271\n",
      "Epoch 4012/20000 Training Loss: 0.0605134479701519\n",
      "Epoch 4013/20000 Training Loss: 0.05788496509194374\n",
      "Epoch 4014/20000 Training Loss: 0.04221062362194061\n",
      "Epoch 4015/20000 Training Loss: 0.05679155886173248\n",
      "Epoch 4016/20000 Training Loss: 0.06766913831233978\n",
      "Epoch 4017/20000 Training Loss: 0.06221549212932587\n",
      "Epoch 4018/20000 Training Loss: 0.07134061306715012\n",
      "Epoch 4019/20000 Training Loss: 0.07080888003110886\n",
      "Epoch 4020/20000 Training Loss: 0.06540606170892715\n",
      "Epoch 4020/20000 Validation Loss: 0.0616966113448143\n",
      "Epoch 4021/20000 Training Loss: 0.06717250496149063\n",
      "Epoch 4022/20000 Training Loss: 0.05922867730259895\n",
      "Epoch 4023/20000 Training Loss: 0.05836614966392517\n",
      "Epoch 4024/20000 Training Loss: 0.051344532519578934\n",
      "Epoch 4025/20000 Training Loss: 0.05443902686238289\n",
      "Epoch 4026/20000 Training Loss: 0.060740869492292404\n",
      "Epoch 4027/20000 Training Loss: 0.06728518009185791\n",
      "Epoch 4028/20000 Training Loss: 0.05657873675227165\n",
      "Epoch 4029/20000 Training Loss: 0.056184399873018265\n",
      "Epoch 4030/20000 Training Loss: 0.04007599130272865\n",
      "Epoch 4030/20000 Validation Loss: 0.049264125525951385\n",
      "Epoch 4031/20000 Training Loss: 0.05302564427256584\n",
      "Epoch 4032/20000 Training Loss: 0.04132441058754921\n",
      "Epoch 4033/20000 Training Loss: 0.05768870934844017\n",
      "Epoch 4034/20000 Training Loss: 0.06604941934347153\n",
      "Epoch 4035/20000 Training Loss: 0.06981942802667618\n",
      "Epoch 4036/20000 Training Loss: 0.07368836551904678\n",
      "Epoch 4037/20000 Training Loss: 0.056157175451517105\n",
      "Epoch 4038/20000 Training Loss: 0.05858007073402405\n",
      "Epoch 4039/20000 Training Loss: 0.06435544043779373\n",
      "Epoch 4040/20000 Training Loss: 0.07824388891458511\n",
      "Epoch 4040/20000 Validation Loss: 0.04480002820491791\n",
      "Epoch 4041/20000 Training Loss: 0.05632847547531128\n",
      "Epoch 4042/20000 Training Loss: 0.05109907686710358\n",
      "Epoch 4043/20000 Training Loss: 0.05772445723414421\n",
      "Epoch 4044/20000 Training Loss: 0.05594661459326744\n",
      "Epoch 4045/20000 Training Loss: 0.06248593330383301\n",
      "Epoch 4046/20000 Training Loss: 0.05084971711039543\n",
      "Epoch 4047/20000 Training Loss: 0.05852742865681648\n",
      "Epoch 4048/20000 Training Loss: 0.06796709448099136\n",
      "Epoch 4049/20000 Training Loss: 0.05615384504199028\n",
      "Epoch 4050/20000 Training Loss: 0.06492918729782104\n",
      "Epoch 4050/20000 Validation Loss: 0.054241448640823364\n",
      "Epoch 4051/20000 Training Loss: 0.048814982175827026\n",
      "Epoch 4052/20000 Training Loss: 0.05304380878806114\n",
      "Epoch 4053/20000 Training Loss: 0.05499200150370598\n",
      "Epoch 4054/20000 Training Loss: 0.052229464054107666\n",
      "Epoch 4055/20000 Training Loss: 0.06206172704696655\n",
      "Epoch 4056/20000 Training Loss: 0.06055301055312157\n",
      "Epoch 4057/20000 Training Loss: 0.0670366883277893\n",
      "Epoch 4058/20000 Training Loss: 0.05183538794517517\n",
      "Epoch 4059/20000 Training Loss: 0.07178232818841934\n",
      "Epoch 4060/20000 Training Loss: 0.07384238392114639\n",
      "Epoch 4060/20000 Validation Loss: 0.08735033869743347\n",
      "Epoch 4061/20000 Training Loss: 0.03851452097296715\n",
      "Epoch 4062/20000 Training Loss: 0.047823820263147354\n",
      "Epoch 4063/20000 Training Loss: 0.05331357195973396\n",
      "Epoch 4064/20000 Training Loss: 0.057297129184007645\n",
      "Epoch 4065/20000 Training Loss: 0.058274298906326294\n",
      "Epoch 4066/20000 Training Loss: 0.05331474542617798\n",
      "Epoch 4067/20000 Training Loss: 0.06534109264612198\n",
      "Epoch 4068/20000 Training Loss: 0.04985469952225685\n",
      "Epoch 4069/20000 Training Loss: 0.054938435554504395\n",
      "Epoch 4070/20000 Training Loss: 0.0460251122713089\n",
      "Epoch 4070/20000 Validation Loss: 0.05039510875940323\n",
      "Epoch 4071/20000 Training Loss: 0.04634198918938637\n",
      "Epoch 4072/20000 Training Loss: 0.05571847781538963\n",
      "Epoch 4073/20000 Training Loss: 0.0700233206152916\n",
      "Epoch 4074/20000 Training Loss: 0.034976113587617874\n",
      "Epoch 4075/20000 Training Loss: 0.04918728768825531\n",
      "Epoch 4076/20000 Training Loss: 0.06381307542324066\n",
      "Epoch 4077/20000 Training Loss: 0.05897444859147072\n",
      "Epoch 4078/20000 Training Loss: 0.06263785809278488\n",
      "Epoch 4079/20000 Training Loss: 0.07471644133329391\n",
      "Epoch 4080/20000 Training Loss: 0.04321227967739105\n",
      "Epoch 4080/20000 Validation Loss: 0.07430338859558105\n",
      "Epoch 4081/20000 Training Loss: 0.0494190938770771\n",
      "Epoch 4082/20000 Training Loss: 0.05380547046661377\n",
      "Epoch 4083/20000 Training Loss: 0.06265497207641602\n",
      "Epoch 4084/20000 Training Loss: 0.060339342802762985\n",
      "Epoch 4085/20000 Training Loss: 0.0629672184586525\n",
      "Epoch 4086/20000 Training Loss: 0.03932187333703041\n",
      "Epoch 4087/20000 Training Loss: 0.058271098881959915\n",
      "Epoch 4088/20000 Training Loss: 0.048890065401792526\n",
      "Epoch 4089/20000 Training Loss: 0.05965067818760872\n",
      "Epoch 4090/20000 Training Loss: 0.0776396319270134\n",
      "Epoch 4090/20000 Validation Loss: 0.07121928036212921\n",
      "Epoch 4091/20000 Training Loss: 0.05344987288117409\n",
      "Epoch 4092/20000 Training Loss: 0.06235894560813904\n",
      "Epoch 4093/20000 Training Loss: 0.05351223051548004\n",
      "Epoch 4094/20000 Training Loss: 0.07177354395389557\n",
      "Epoch 4095/20000 Training Loss: 0.054423000663518906\n",
      "Epoch 4096/20000 Training Loss: 0.061742138117551804\n",
      "Epoch 4097/20000 Training Loss: 0.053291257470846176\n",
      "Epoch 4098/20000 Training Loss: 0.06803686171770096\n",
      "Epoch 4099/20000 Training Loss: 0.06508290022611618\n",
      "Epoch 4100/20000 Training Loss: 0.05409781262278557\n",
      "Epoch 4100/20000 Validation Loss: 0.03908674046397209\n",
      "Epoch 4101/20000 Training Loss: 0.06614111363887787\n",
      "Epoch 4102/20000 Training Loss: 0.05270656943321228\n",
      "Epoch 4103/20000 Training Loss: 0.06243656575679779\n",
      "Epoch 4104/20000 Training Loss: 0.06359607726335526\n",
      "Epoch 4105/20000 Training Loss: 0.06445977091789246\n",
      "Epoch 4106/20000 Training Loss: 0.06289998441934586\n",
      "Epoch 4107/20000 Training Loss: 0.06506077200174332\n",
      "Epoch 4108/20000 Training Loss: 0.06863084435462952\n",
      "Epoch 4109/20000 Training Loss: 0.05652370676398277\n",
      "Epoch 4110/20000 Training Loss: 0.05797015503048897\n",
      "Epoch 4110/20000 Validation Loss: 0.0528399720788002\n",
      "Epoch 4111/20000 Training Loss: 0.04729287698864937\n",
      "Epoch 4112/20000 Training Loss: 0.05546320602297783\n",
      "Epoch 4113/20000 Training Loss: 0.05788666009902954\n",
      "Epoch 4114/20000 Training Loss: 0.05203287675976753\n",
      "Epoch 4115/20000 Training Loss: 0.0549607127904892\n",
      "Epoch 4116/20000 Training Loss: 0.059480708092451096\n",
      "Epoch 4117/20000 Training Loss: 0.04693571850657463\n",
      "Epoch 4118/20000 Training Loss: 0.07090546190738678\n",
      "Epoch 4119/20000 Training Loss: 0.07114133983850479\n",
      "Epoch 4120/20000 Training Loss: 0.06454437226057053\n",
      "Epoch 4120/20000 Validation Loss: 0.054908499121665955\n",
      "Epoch 4121/20000 Training Loss: 0.057194292545318604\n",
      "Epoch 4122/20000 Training Loss: 0.0648508220911026\n",
      "Epoch 4123/20000 Training Loss: 0.055182743817567825\n",
      "Epoch 4124/20000 Training Loss: 0.06604669243097305\n",
      "Epoch 4125/20000 Training Loss: 0.07019857317209244\n",
      "Epoch 4126/20000 Training Loss: 0.05934448540210724\n",
      "Epoch 4127/20000 Training Loss: 0.05724601820111275\n",
      "Epoch 4128/20000 Training Loss: 0.06541779637336731\n",
      "Epoch 4129/20000 Training Loss: 0.055824220180511475\n",
      "Epoch 4130/20000 Training Loss: 0.048340339213609695\n",
      "Epoch 4130/20000 Validation Loss: 0.06465477496385574\n",
      "Epoch 4131/20000 Training Loss: 0.08070872724056244\n",
      "Epoch 4132/20000 Training Loss: 0.05922582745552063\n",
      "Epoch 4133/20000 Training Loss: 0.0507030189037323\n",
      "Epoch 4134/20000 Training Loss: 0.06234030798077583\n",
      "Epoch 4135/20000 Training Loss: 0.049586158245801926\n",
      "Epoch 4136/20000 Training Loss: 0.06272468715906143\n",
      "Epoch 4137/20000 Training Loss: 0.08483602851629257\n",
      "Epoch 4138/20000 Training Loss: 0.05206863209605217\n",
      "Epoch 4139/20000 Training Loss: 0.05895611271262169\n",
      "Epoch 4140/20000 Training Loss: 0.05691656470298767\n",
      "Epoch 4140/20000 Validation Loss: 0.048367638140916824\n",
      "Epoch 4141/20000 Training Loss: 0.05982993170619011\n",
      "Epoch 4142/20000 Training Loss: 0.06431371718645096\n",
      "Epoch 4143/20000 Training Loss: 0.05652698874473572\n",
      "Epoch 4144/20000 Training Loss: 0.059667449444532394\n",
      "Epoch 4145/20000 Training Loss: 0.08459941297769547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4146/20000 Training Loss: 0.07005658745765686\n",
      "Epoch 4147/20000 Training Loss: 0.05795074626803398\n",
      "Epoch 4148/20000 Training Loss: 0.06109050288796425\n",
      "Epoch 4149/20000 Training Loss: 0.06403496116399765\n",
      "Epoch 4150/20000 Training Loss: 0.0503937192261219\n",
      "Epoch 4150/20000 Validation Loss: 0.05581377446651459\n",
      "Epoch 4151/20000 Training Loss: 0.051791977137327194\n",
      "Epoch 4152/20000 Training Loss: 0.06013701483607292\n",
      "Epoch 4153/20000 Training Loss: 0.06688899546861649\n",
      "Epoch 4154/20000 Training Loss: 0.06721276789903641\n",
      "Epoch 4155/20000 Training Loss: 0.05919334292411804\n",
      "Epoch 4156/20000 Training Loss: 0.04617317020893097\n",
      "Epoch 4157/20000 Training Loss: 0.06476972997188568\n",
      "Epoch 4158/20000 Training Loss: 0.05919426679611206\n",
      "Epoch 4159/20000 Training Loss: 0.040471430867910385\n",
      "Epoch 4160/20000 Training Loss: 0.06572172045707703\n",
      "Epoch 4160/20000 Validation Loss: 0.06894280761480331\n",
      "Epoch 4161/20000 Training Loss: 0.06741102784872055\n",
      "Epoch 4162/20000 Training Loss: 0.06452280282974243\n",
      "Epoch 4163/20000 Training Loss: 0.04818369820713997\n",
      "Epoch 4164/20000 Training Loss: 0.053639430552721024\n",
      "Epoch 4165/20000 Training Loss: 0.06339389085769653\n",
      "Epoch 4166/20000 Training Loss: 0.05139468237757683\n",
      "Epoch 4167/20000 Training Loss: 0.06832597404718399\n",
      "Epoch 4168/20000 Training Loss: 0.06069771572947502\n",
      "Epoch 4169/20000 Training Loss: 0.06500517576932907\n",
      "Epoch 4170/20000 Training Loss: 0.0604541152715683\n",
      "Epoch 4170/20000 Validation Loss: 0.08023810386657715\n",
      "Epoch 4171/20000 Training Loss: 0.04440829157829285\n",
      "Epoch 4172/20000 Training Loss: 0.04673706367611885\n",
      "Epoch 4173/20000 Training Loss: 0.07635005563497543\n",
      "Epoch 4174/20000 Training Loss: 0.05235058441758156\n",
      "Epoch 4175/20000 Training Loss: 0.046703893691301346\n",
      "Epoch 4176/20000 Training Loss: 0.04518221318721771\n",
      "Epoch 4177/20000 Training Loss: 0.05253217741847038\n",
      "Epoch 4178/20000 Training Loss: 0.052858009934425354\n",
      "Epoch 4179/20000 Training Loss: 0.0742979571223259\n",
      "Epoch 4180/20000 Training Loss: 0.06032469868659973\n",
      "Epoch 4180/20000 Validation Loss: 0.04835110902786255\n",
      "Epoch 4181/20000 Training Loss: 0.056029558181762695\n",
      "Epoch 4182/20000 Training Loss: 0.052230726927518845\n",
      "Epoch 4183/20000 Training Loss: 0.048294782638549805\n",
      "Epoch 4184/20000 Training Loss: 0.05649824067950249\n",
      "Epoch 4185/20000 Training Loss: 0.06020255386829376\n",
      "Epoch 4186/20000 Training Loss: 0.06062912940979004\n",
      "Epoch 4187/20000 Training Loss: 0.06255824118852615\n",
      "Epoch 4188/20000 Training Loss: 0.0669848844408989\n",
      "Epoch 4189/20000 Training Loss: 0.06262213736772537\n",
      "Epoch 4190/20000 Training Loss: 0.04274299740791321\n",
      "Epoch 4190/20000 Validation Loss: 0.04661388322710991\n",
      "Epoch 4191/20000 Training Loss: 0.057590749114751816\n",
      "Epoch 4192/20000 Training Loss: 0.05827799439430237\n",
      "Epoch 4193/20000 Training Loss: 0.07260486483573914\n",
      "Epoch 4194/20000 Training Loss: 0.04313535988330841\n",
      "Epoch 4195/20000 Training Loss: 0.059466734528541565\n",
      "Epoch 4196/20000 Training Loss: 0.05296686291694641\n",
      "Epoch 4197/20000 Training Loss: 0.06569250673055649\n",
      "Epoch 4198/20000 Training Loss: 0.05054650828242302\n",
      "Epoch 4199/20000 Training Loss: 0.05920025333762169\n",
      "Epoch 4200/20000 Training Loss: 0.05059920623898506\n",
      "Epoch 4200/20000 Validation Loss: 0.06539144366979599\n",
      "Epoch 4201/20000 Training Loss: 0.07273111492395401\n",
      "Epoch 4202/20000 Training Loss: 0.07164056599140167\n",
      "Epoch 4203/20000 Training Loss: 0.06119546666741371\n",
      "Epoch 4204/20000 Training Loss: 0.04607953503727913\n",
      "Epoch 4205/20000 Training Loss: 0.06351371109485626\n",
      "Epoch 4206/20000 Training Loss: 0.05152037739753723\n",
      "Epoch 4207/20000 Training Loss: 0.05740535259246826\n",
      "Epoch 4208/20000 Training Loss: 0.05385037884116173\n",
      "Epoch 4209/20000 Training Loss: 0.04306784272193909\n",
      "Epoch 4210/20000 Training Loss: 0.06622370332479477\n",
      "Epoch 4210/20000 Validation Loss: 0.08372174948453903\n",
      "Epoch 4211/20000 Training Loss: 0.04748980700969696\n",
      "Epoch 4212/20000 Training Loss: 0.0672714114189148\n",
      "Epoch 4213/20000 Training Loss: 0.0580732561647892\n",
      "Epoch 4214/20000 Training Loss: 0.05162249878048897\n",
      "Epoch 4215/20000 Training Loss: 0.07232555747032166\n",
      "Epoch 4216/20000 Training Loss: 0.05037354305386543\n",
      "Epoch 4217/20000 Training Loss: 0.05102706328034401\n",
      "Epoch 4218/20000 Training Loss: 0.05781538411974907\n",
      "Epoch 4219/20000 Training Loss: 0.06048239395022392\n",
      "Epoch 4220/20000 Training Loss: 0.058031272143125534\n",
      "Epoch 4220/20000 Validation Loss: 0.057746805250644684\n",
      "Epoch 4221/20000 Training Loss: 0.08207090198993683\n",
      "Epoch 4222/20000 Training Loss: 0.05908127501606941\n",
      "Epoch 4223/20000 Training Loss: 0.072880819439888\n",
      "Epoch 4224/20000 Training Loss: 0.06786796450614929\n",
      "Epoch 4225/20000 Training Loss: 0.05363696813583374\n",
      "Epoch 4226/20000 Training Loss: 0.047479357570409775\n",
      "Epoch 4227/20000 Training Loss: 0.049468230456113815\n",
      "Epoch 4228/20000 Training Loss: 0.0459432490170002\n",
      "Epoch 4229/20000 Training Loss: 0.0721038430929184\n",
      "Epoch 4230/20000 Training Loss: 0.0422242172062397\n",
      "Epoch 4230/20000 Validation Loss: 0.06706234812736511\n",
      "Epoch 4231/20000 Training Loss: 0.038332175463438034\n",
      "Epoch 4232/20000 Training Loss: 0.06775406748056412\n",
      "Epoch 4233/20000 Training Loss: 0.07363316416740417\n",
      "Epoch 4234/20000 Training Loss: 0.06839452683925629\n",
      "Epoch 4235/20000 Training Loss: 0.06459415704011917\n",
      "Epoch 4236/20000 Training Loss: 0.046663958579301834\n",
      "Epoch 4237/20000 Training Loss: 0.041420649737119675\n",
      "Epoch 4238/20000 Training Loss: 0.07035677134990692\n",
      "Epoch 4239/20000 Training Loss: 0.05793197453022003\n",
      "Epoch 4240/20000 Training Loss: 0.05691040679812431\n",
      "Epoch 4240/20000 Validation Loss: 0.06461361795663834\n",
      "Epoch 4241/20000 Training Loss: 0.051693614572286606\n",
      "Epoch 4242/20000 Training Loss: 0.0571020133793354\n",
      "Epoch 4243/20000 Training Loss: 0.05689995363354683\n",
      "Epoch 4244/20000 Training Loss: 0.05407869815826416\n",
      "Epoch 4245/20000 Training Loss: 0.0693267285823822\n",
      "Epoch 4246/20000 Training Loss: 0.035111818462610245\n",
      "Epoch 4247/20000 Training Loss: 0.055089231580495834\n",
      "Epoch 4248/20000 Training Loss: 0.05855393782258034\n",
      "Epoch 4249/20000 Training Loss: 0.05327162146568298\n",
      "Epoch 4250/20000 Training Loss: 0.04333921894431114\n",
      "Epoch 4250/20000 Validation Loss: 0.060091353952884674\n",
      "Epoch 4251/20000 Training Loss: 0.049328356981277466\n",
      "Epoch 4252/20000 Training Loss: 0.05445753037929535\n",
      "Epoch 4253/20000 Training Loss: 0.04832844436168671\n",
      "Epoch 4254/20000 Training Loss: 0.05075393244624138\n",
      "Epoch 4255/20000 Training Loss: 0.05141320452094078\n",
      "Epoch 4256/20000 Training Loss: 0.05953623726963997\n",
      "Epoch 4257/20000 Training Loss: 0.06023649498820305\n",
      "Epoch 4258/20000 Training Loss: 0.058699626475572586\n",
      "Epoch 4259/20000 Training Loss: 0.04516547918319702\n",
      "Epoch 4260/20000 Training Loss: 0.0517892986536026\n",
      "Epoch 4260/20000 Validation Loss: 0.059898585081100464\n",
      "Epoch 4261/20000 Training Loss: 0.049266960471868515\n",
      "Epoch 4262/20000 Training Loss: 0.04621356725692749\n",
      "Epoch 4263/20000 Training Loss: 0.060217779129743576\n",
      "Epoch 4264/20000 Training Loss: 0.08241171389818192\n",
      "Epoch 4265/20000 Training Loss: 0.05842588469386101\n",
      "Epoch 4266/20000 Training Loss: 0.0536336712539196\n",
      "Epoch 4267/20000 Training Loss: 0.05470147728919983\n",
      "Epoch 4268/20000 Training Loss: 0.06599753350019455\n",
      "Epoch 4269/20000 Training Loss: 0.05799989774823189\n",
      "Epoch 4270/20000 Training Loss: 0.03699984773993492\n",
      "Epoch 4270/20000 Validation Loss: 0.06480217725038528\n",
      "Epoch 4271/20000 Training Loss: 0.05735204741358757\n",
      "Epoch 4272/20000 Training Loss: 0.057999759912490845\n",
      "Epoch 4273/20000 Training Loss: 0.061122309416532516\n",
      "Epoch 4274/20000 Training Loss: 0.06762979179620743\n",
      "Epoch 4275/20000 Training Loss: 0.05862240120768547\n",
      "Epoch 4276/20000 Training Loss: 0.04545110836625099\n",
      "Epoch 4277/20000 Training Loss: 0.06155471131205559\n",
      "Epoch 4278/20000 Training Loss: 0.06287720054388046\n",
      "Epoch 4279/20000 Training Loss: 0.06106309965252876\n",
      "Epoch 4280/20000 Training Loss: 0.0592525452375412\n",
      "Epoch 4280/20000 Validation Loss: 0.05672386661171913\n",
      "Epoch 4281/20000 Training Loss: 0.0546114481985569\n",
      "Epoch 4282/20000 Training Loss: 0.06572259217500687\n",
      "Epoch 4283/20000 Training Loss: 0.04995078220963478\n",
      "Epoch 4284/20000 Training Loss: 0.07208399474620819\n",
      "Epoch 4285/20000 Training Loss: 0.07614525407552719\n",
      "Epoch 4286/20000 Training Loss: 0.07232620567083359\n",
      "Epoch 4287/20000 Training Loss: 0.06726893037557602\n",
      "Epoch 4288/20000 Training Loss: 0.06067424640059471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4289/20000 Training Loss: 0.054940562695264816\n",
      "Epoch 4290/20000 Training Loss: 0.06270008534193039\n",
      "Epoch 4290/20000 Validation Loss: 0.05929958075284958\n",
      "Epoch 4291/20000 Training Loss: 0.05874514579772949\n",
      "Epoch 4292/20000 Training Loss: 0.06189875677227974\n",
      "Epoch 4293/20000 Training Loss: 0.057074081152677536\n",
      "Epoch 4294/20000 Training Loss: 0.04328376427292824\n",
      "Epoch 4295/20000 Training Loss: 0.0578191876411438\n",
      "Epoch 4296/20000 Training Loss: 0.04799483343958855\n",
      "Epoch 4297/20000 Training Loss: 0.048578109592199326\n",
      "Epoch 4298/20000 Training Loss: 0.0701909065246582\n",
      "Epoch 4299/20000 Training Loss: 0.06398272514343262\n",
      "Epoch 4300/20000 Training Loss: 0.06831438094377518\n",
      "Epoch 4300/20000 Validation Loss: 0.05307542160153389\n",
      "Epoch 4301/20000 Training Loss: 0.04667496308684349\n",
      "Epoch 4302/20000 Training Loss: 0.05924597382545471\n",
      "Epoch 4303/20000 Training Loss: 0.06861823797225952\n",
      "Epoch 4304/20000 Training Loss: 0.057662416249513626\n",
      "Epoch 4305/20000 Training Loss: 0.07485144585371017\n",
      "Epoch 4306/20000 Training Loss: 0.057700011879205704\n",
      "Epoch 4307/20000 Training Loss: 0.04796471819281578\n",
      "Epoch 4308/20000 Training Loss: 0.07219039648771286\n",
      "Epoch 4309/20000 Training Loss: 0.06255099177360535\n",
      "Epoch 4310/20000 Training Loss: 0.07901035994291306\n",
      "Epoch 4310/20000 Validation Loss: 0.04812367632985115\n",
      "Epoch 4311/20000 Training Loss: 0.056271690875291824\n",
      "Epoch 4312/20000 Training Loss: 0.07310054451227188\n",
      "Epoch 4313/20000 Training Loss: 0.06676734238862991\n",
      "Epoch 4314/20000 Training Loss: 0.04914964735507965\n",
      "Epoch 4315/20000 Training Loss: 0.07147795706987381\n",
      "Epoch 4316/20000 Training Loss: 0.0632658377289772\n",
      "Epoch 4317/20000 Training Loss: 0.04770289361476898\n",
      "Epoch 4318/20000 Training Loss: 0.04321323335170746\n",
      "Epoch 4319/20000 Training Loss: 0.057692933827638626\n",
      "Epoch 4320/20000 Training Loss: 0.0645642802119255\n",
      "Epoch 4320/20000 Validation Loss: 0.052108559757471085\n",
      "Epoch 4321/20000 Training Loss: 0.0647301897406578\n",
      "Epoch 4322/20000 Training Loss: 0.06304990500211716\n",
      "Epoch 4323/20000 Training Loss: 0.07174289971590042\n",
      "Epoch 4324/20000 Training Loss: 0.06335937976837158\n",
      "Epoch 4325/20000 Training Loss: 0.05065352842211723\n",
      "Epoch 4326/20000 Training Loss: 0.0389893464744091\n",
      "Epoch 4327/20000 Training Loss: 0.06595038622617722\n",
      "Epoch 4328/20000 Training Loss: 0.05449967086315155\n",
      "Epoch 4329/20000 Training Loss: 0.056418124586343765\n",
      "Epoch 4330/20000 Training Loss: 0.045999858528375626\n",
      "Epoch 4330/20000 Validation Loss: 0.0512109138071537\n",
      "Epoch 4331/20000 Training Loss: 0.047407958656549454\n",
      "Epoch 4332/20000 Training Loss: 0.060294557362794876\n",
      "Epoch 4333/20000 Training Loss: 0.05117112025618553\n",
      "Epoch 4334/20000 Training Loss: 0.07293722033500671\n",
      "Epoch 4335/20000 Training Loss: 0.07177204638719559\n",
      "Epoch 4336/20000 Training Loss: 0.07031471282243729\n",
      "Epoch 4337/20000 Training Loss: 0.05724528431892395\n",
      "Epoch 4338/20000 Training Loss: 0.053442854434251785\n",
      "Epoch 4339/20000 Training Loss: 0.0620424784719944\n",
      "Epoch 4340/20000 Training Loss: 0.04636910930275917\n",
      "Epoch 4340/20000 Validation Loss: 0.04424925148487091\n",
      "Epoch 4341/20000 Training Loss: 0.06294693052768707\n",
      "Epoch 4342/20000 Training Loss: 0.06495112925767899\n",
      "Epoch 4343/20000 Training Loss: 0.05744025111198425\n",
      "Epoch 4344/20000 Training Loss: 0.04228534922003746\n",
      "Epoch 4345/20000 Training Loss: 0.08739262074232101\n",
      "Epoch 4346/20000 Training Loss: 0.0743706002831459\n",
      "Epoch 4347/20000 Training Loss: 0.04740000143647194\n",
      "Epoch 4348/20000 Training Loss: 0.0634930431842804\n",
      "Epoch 4349/20000 Training Loss: 0.05163434520363808\n",
      "Epoch 4350/20000 Training Loss: 0.06770767271518707\n",
      "Epoch 4350/20000 Validation Loss: 0.03448484465479851\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03448484465479851<=============\n",
      "Epoch 4351/20000 Training Loss: 0.0566856823861599\n",
      "Epoch 4352/20000 Training Loss: 0.050046633929014206\n",
      "Epoch 4353/20000 Training Loss: 0.0637945607304573\n",
      "Epoch 4354/20000 Training Loss: 0.0660964772105217\n",
      "Epoch 4355/20000 Training Loss: 0.05716925486922264\n",
      "Epoch 4356/20000 Training Loss: 0.037249282002449036\n",
      "Epoch 4357/20000 Training Loss: 0.04996224120259285\n",
      "Epoch 4358/20000 Training Loss: 0.038088615983724594\n",
      "Epoch 4359/20000 Training Loss: 0.04948107898235321\n",
      "Epoch 4360/20000 Training Loss: 0.057558853179216385\n",
      "Epoch 4360/20000 Validation Loss: 0.04376161843538284\n",
      "Epoch 4361/20000 Training Loss: 0.05372358486056328\n",
      "Epoch 4362/20000 Training Loss: 0.06381331384181976\n",
      "Epoch 4363/20000 Training Loss: 0.04530704393982887\n",
      "Epoch 4364/20000 Training Loss: 0.07571186125278473\n",
      "Epoch 4365/20000 Training Loss: 0.042973533272743225\n",
      "Epoch 4366/20000 Training Loss: 0.05613243952393532\n",
      "Epoch 4367/20000 Training Loss: 0.048000410199165344\n",
      "Epoch 4368/20000 Training Loss: 0.05272434279322624\n",
      "Epoch 4369/20000 Training Loss: 0.05676957964897156\n",
      "Epoch 4370/20000 Training Loss: 0.06929954141378403\n",
      "Epoch 4370/20000 Validation Loss: 0.06450142711400986\n",
      "Epoch 4371/20000 Training Loss: 0.06179184839129448\n",
      "Epoch 4372/20000 Training Loss: 0.0539071299135685\n",
      "Epoch 4373/20000 Training Loss: 0.035217974334955215\n",
      "Epoch 4374/20000 Training Loss: 0.048734575510025024\n",
      "Epoch 4375/20000 Training Loss: 0.08365020155906677\n",
      "Epoch 4376/20000 Training Loss: 0.04950104281306267\n",
      "Epoch 4377/20000 Training Loss: 0.05205618962645531\n",
      "Epoch 4378/20000 Training Loss: 0.056677356362342834\n",
      "Epoch 4379/20000 Training Loss: 0.041047077625989914\n",
      "Epoch 4380/20000 Training Loss: 0.03927352651953697\n",
      "Epoch 4380/20000 Validation Loss: 0.0679406225681305\n",
      "Epoch 4381/20000 Training Loss: 0.06293249875307083\n",
      "Epoch 4382/20000 Training Loss: 0.05730142071843147\n",
      "Epoch 4383/20000 Training Loss: 0.062385398894548416\n",
      "Epoch 4384/20000 Training Loss: 0.053147245198488235\n",
      "Epoch 4385/20000 Training Loss: 0.059710610657930374\n",
      "Epoch 4386/20000 Training Loss: 0.06342712789773941\n",
      "Epoch 4387/20000 Training Loss: 0.05422930046916008\n",
      "Epoch 4388/20000 Training Loss: 0.05697573348879814\n",
      "Epoch 4389/20000 Training Loss: 0.06283053755760193\n",
      "Epoch 4390/20000 Training Loss: 0.06051875278353691\n",
      "Epoch 4390/20000 Validation Loss: 0.05932728946208954\n",
      "Epoch 4391/20000 Training Loss: 0.04453654587268829\n",
      "Epoch 4392/20000 Training Loss: 0.05339658632874489\n",
      "Epoch 4393/20000 Training Loss: 0.05291737616062164\n",
      "Epoch 4394/20000 Training Loss: 0.04927307739853859\n",
      "Epoch 4395/20000 Training Loss: 0.07248594611883163\n",
      "Epoch 4396/20000 Training Loss: 0.0538017638027668\n",
      "Epoch 4397/20000 Training Loss: 0.04366162419319153\n",
      "Epoch 4398/20000 Training Loss: 0.048502545803785324\n",
      "Epoch 4399/20000 Training Loss: 0.04537341371178627\n",
      "Epoch 4400/20000 Training Loss: 0.07002744823694229\n",
      "Epoch 4400/20000 Validation Loss: 0.07951271533966064\n",
      "Epoch 4401/20000 Training Loss: 0.04984356835484505\n",
      "Epoch 4402/20000 Training Loss: 0.0418822318315506\n",
      "Epoch 4403/20000 Training Loss: 0.06267929822206497\n",
      "Epoch 4404/20000 Training Loss: 0.060211990028619766\n",
      "Epoch 4405/20000 Training Loss: 0.06565046310424805\n",
      "Epoch 4406/20000 Training Loss: 0.05410334840416908\n",
      "Epoch 4407/20000 Training Loss: 0.050071656703948975\n",
      "Epoch 4408/20000 Training Loss: 0.04976477101445198\n",
      "Epoch 4409/20000 Training Loss: 0.05512760207056999\n",
      "Epoch 4410/20000 Training Loss: 0.07029110938310623\n",
      "Epoch 4410/20000 Validation Loss: 0.05553959310054779\n",
      "Epoch 4411/20000 Training Loss: 0.0526706762611866\n",
      "Epoch 4412/20000 Training Loss: 0.05297918617725372\n",
      "Epoch 4413/20000 Training Loss: 0.06064939126372337\n",
      "Epoch 4414/20000 Training Loss: 0.0786544606089592\n",
      "Epoch 4415/20000 Training Loss: 0.05989687144756317\n",
      "Epoch 4416/20000 Training Loss: 0.06691375374794006\n",
      "Epoch 4417/20000 Training Loss: 0.06270811706781387\n",
      "Epoch 4418/20000 Training Loss: 0.058459341526031494\n",
      "Epoch 4419/20000 Training Loss: 0.0647859051823616\n",
      "Epoch 4420/20000 Training Loss: 0.06692436337471008\n",
      "Epoch 4420/20000 Validation Loss: 0.04713239148259163\n",
      "Epoch 4421/20000 Training Loss: 0.05150766298174858\n",
      "Epoch 4422/20000 Training Loss: 0.06015631556510925\n",
      "Epoch 4423/20000 Training Loss: 0.05850990489125252\n",
      "Epoch 4424/20000 Training Loss: 0.05628633499145508\n",
      "Epoch 4425/20000 Training Loss: 0.06372357159852982\n",
      "Epoch 4426/20000 Training Loss: 0.050405438989400864\n",
      "Epoch 4427/20000 Training Loss: 0.07812715321779251\n",
      "Epoch 4428/20000 Training Loss: 0.0681222677230835\n",
      "Epoch 4429/20000 Training Loss: 0.0550922155380249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4430/20000 Training Loss: 0.08421310782432556\n",
      "Epoch 4430/20000 Validation Loss: 0.05414462462067604\n",
      "Epoch 4431/20000 Training Loss: 0.053584516048431396\n",
      "Epoch 4432/20000 Training Loss: 0.06636492162942886\n",
      "Epoch 4433/20000 Training Loss: 0.06268303841352463\n",
      "Epoch 4434/20000 Training Loss: 0.04649883136153221\n",
      "Epoch 4435/20000 Training Loss: 0.05457846820354462\n",
      "Epoch 4436/20000 Training Loss: 0.0748990848660469\n",
      "Epoch 4437/20000 Training Loss: 0.05060119554400444\n",
      "Epoch 4438/20000 Training Loss: 0.055793363600969315\n",
      "Epoch 4439/20000 Training Loss: 0.06796461343765259\n",
      "Epoch 4440/20000 Training Loss: 0.07989455759525299\n",
      "Epoch 4440/20000 Validation Loss: 0.052404627203941345\n",
      "Epoch 4441/20000 Training Loss: 0.06605329364538193\n",
      "Epoch 4442/20000 Training Loss: 0.049327630549669266\n",
      "Epoch 4443/20000 Training Loss: 0.06310181319713593\n",
      "Epoch 4444/20000 Training Loss: 0.0406612865626812\n",
      "Epoch 4445/20000 Training Loss: 0.07357627153396606\n",
      "Epoch 4446/20000 Training Loss: 0.03673357143998146\n",
      "Epoch 4447/20000 Training Loss: 0.06539993733167648\n",
      "Epoch 4448/20000 Training Loss: 0.04958181083202362\n",
      "Epoch 4449/20000 Training Loss: 0.0634167343378067\n",
      "Epoch 4450/20000 Training Loss: 0.059228796511888504\n",
      "Epoch 4450/20000 Validation Loss: 0.03254077583551407\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.03254077583551407<=============\n",
      "Epoch 4451/20000 Training Loss: 0.06242473050951958\n",
      "Epoch 4452/20000 Training Loss: 0.07121092081069946\n",
      "Epoch 4453/20000 Training Loss: 0.06057797744870186\n",
      "Epoch 4454/20000 Training Loss: 0.064539335668087\n",
      "Epoch 4455/20000 Training Loss: 0.048426028341054916\n",
      "Epoch 4456/20000 Training Loss: 0.05744759738445282\n",
      "Epoch 4457/20000 Training Loss: 0.08517090231180191\n",
      "Epoch 4458/20000 Training Loss: 0.07252033799886703\n",
      "Epoch 4459/20000 Training Loss: 0.04817650839686394\n",
      "Epoch 4460/20000 Training Loss: 0.06810945272445679\n",
      "Epoch 4460/20000 Validation Loss: 0.04970039799809456\n",
      "Epoch 4461/20000 Training Loss: 0.05426057055592537\n",
      "Epoch 4462/20000 Training Loss: 0.0532919280230999\n",
      "Epoch 4463/20000 Training Loss: 0.05824791267514229\n",
      "Epoch 4464/20000 Training Loss: 0.0573231466114521\n",
      "Epoch 4465/20000 Training Loss: 0.0760241150856018\n",
      "Epoch 4466/20000 Training Loss: 0.052856121212244034\n",
      "Epoch 4467/20000 Training Loss: 0.04840589687228203\n",
      "Epoch 4468/20000 Training Loss: 0.06070706248283386\n",
      "Epoch 4469/20000 Training Loss: 0.06526137888431549\n",
      "Epoch 4470/20000 Training Loss: 0.07437369227409363\n",
      "Epoch 4470/20000 Validation Loss: 0.04632335901260376\n",
      "Epoch 4471/20000 Training Loss: 0.04311801865696907\n",
      "Epoch 4472/20000 Training Loss: 0.05011564493179321\n",
      "Epoch 4473/20000 Training Loss: 0.053249482065439224\n",
      "Epoch 4474/20000 Training Loss: 0.07037201523780823\n",
      "Epoch 4475/20000 Training Loss: 0.04375317692756653\n",
      "Epoch 4476/20000 Training Loss: 0.06925783306360245\n",
      "Epoch 4477/20000 Training Loss: 0.07098691910505295\n",
      "Epoch 4478/20000 Training Loss: 0.05706999823451042\n",
      "Epoch 4479/20000 Training Loss: 0.061920348554849625\n",
      "Epoch 4480/20000 Training Loss: 0.055781032890081406\n",
      "Epoch 4480/20000 Validation Loss: 0.0636899396777153\n",
      "Epoch 4481/20000 Training Loss: 0.04985235258936882\n",
      "Epoch 4482/20000 Training Loss: 0.05355843901634216\n",
      "Epoch 4483/20000 Training Loss: 0.047446805983781815\n",
      "Epoch 4484/20000 Training Loss: 0.05619678273797035\n",
      "Epoch 4485/20000 Training Loss: 0.0510287769138813\n",
      "Epoch 4486/20000 Training Loss: 0.055074919015169144\n",
      "Epoch 4487/20000 Training Loss: 0.04960206151008606\n",
      "Epoch 4488/20000 Training Loss: 0.06581491976976395\n",
      "Epoch 4489/20000 Training Loss: 0.0533500574529171\n",
      "Epoch 4490/20000 Training Loss: 0.048956360667943954\n",
      "Epoch 4490/20000 Validation Loss: 0.0538654550909996\n",
      "Epoch 4491/20000 Training Loss: 0.05161197856068611\n",
      "Epoch 4492/20000 Training Loss: 0.06600790470838547\n",
      "Epoch 4493/20000 Training Loss: 0.058758899569511414\n",
      "Epoch 4494/20000 Training Loss: 0.057266365736722946\n",
      "Epoch 4495/20000 Training Loss: 0.05564672127366066\n",
      "Epoch 4496/20000 Training Loss: 0.04676805064082146\n",
      "Epoch 4497/20000 Training Loss: 0.05913146957755089\n",
      "Epoch 4498/20000 Training Loss: 0.05529427528381348\n",
      "Epoch 4499/20000 Training Loss: 0.04671264812350273\n",
      "Epoch 4500/20000 Training Loss: 0.03886975347995758\n",
      "Epoch 4500/20000 Validation Loss: 0.05436467006802559\n",
      "Epoch 4501/20000 Training Loss: 0.05579100921750069\n",
      "Epoch 4502/20000 Training Loss: 0.05762225762009621\n",
      "Epoch 4503/20000 Training Loss: 0.05727427825331688\n",
      "Epoch 4504/20000 Training Loss: 0.056447163224220276\n",
      "Epoch 4505/20000 Training Loss: 0.04592471197247505\n",
      "Epoch 4506/20000 Training Loss: 0.057132329791784286\n",
      "Epoch 4507/20000 Training Loss: 0.06339583545923233\n",
      "Epoch 4508/20000 Training Loss: 0.045397061854600906\n",
      "Epoch 4509/20000 Training Loss: 0.07151662558317184\n",
      "Epoch 4510/20000 Training Loss: 0.06395246088504791\n",
      "Epoch 4510/20000 Validation Loss: 0.04675678908824921\n",
      "Epoch 4511/20000 Training Loss: 0.06107943877577782\n",
      "Epoch 4512/20000 Training Loss: 0.08039217442274094\n",
      "Epoch 4513/20000 Training Loss: 0.07328679412603378\n",
      "Epoch 4514/20000 Training Loss: 0.05640147626399994\n",
      "Epoch 4515/20000 Training Loss: 0.04492766037583351\n",
      "Epoch 4516/20000 Training Loss: 0.047734588384628296\n",
      "Epoch 4517/20000 Training Loss: 0.04369762912392616\n",
      "Epoch 4518/20000 Training Loss: 0.0607137493789196\n",
      "Epoch 4519/20000 Training Loss: 0.045151080936193466\n",
      "Epoch 4520/20000 Training Loss: 0.07772678136825562\n",
      "Epoch 4520/20000 Validation Loss: 0.06657055020332336\n",
      "Epoch 4521/20000 Training Loss: 0.06142013147473335\n",
      "Epoch 4522/20000 Training Loss: 0.03936927393078804\n",
      "Epoch 4523/20000 Training Loss: 0.059615861624479294\n",
      "Epoch 4524/20000 Training Loss: 0.06363225728273392\n",
      "Epoch 4525/20000 Training Loss: 0.05296248197555542\n",
      "Epoch 4526/20000 Training Loss: 0.04527515172958374\n",
      "Epoch 4527/20000 Training Loss: 0.06047778204083443\n",
      "Epoch 4528/20000 Training Loss: 0.05493917688727379\n",
      "Epoch 4529/20000 Training Loss: 0.06027355417609215\n",
      "Epoch 4530/20000 Training Loss: 0.05091935396194458\n",
      "Epoch 4530/20000 Validation Loss: 0.049797073006629944\n",
      "Epoch 4531/20000 Training Loss: 0.0631205141544342\n",
      "Epoch 4532/20000 Training Loss: 0.06831998378038406\n",
      "Epoch 4533/20000 Training Loss: 0.04939301684498787\n",
      "Epoch 4534/20000 Training Loss: 0.05586611106991768\n",
      "Epoch 4535/20000 Training Loss: 0.035724759101867676\n",
      "Epoch 4536/20000 Training Loss: 0.0595574676990509\n",
      "Epoch 4537/20000 Training Loss: 0.05128425359725952\n",
      "Epoch 4538/20000 Training Loss: 0.05408691242337227\n",
      "Epoch 4539/20000 Training Loss: 0.038937583565711975\n",
      "Epoch 4540/20000 Training Loss: 0.049683649092912674\n",
      "Epoch 4540/20000 Validation Loss: 0.07516059279441833\n",
      "Epoch 4541/20000 Training Loss: 0.04857273027300835\n",
      "Epoch 4542/20000 Training Loss: 0.053863972425460815\n",
      "Epoch 4543/20000 Training Loss: 0.04710206389427185\n",
      "Epoch 4544/20000 Training Loss: 0.053119365125894547\n",
      "Epoch 4545/20000 Training Loss: 0.05832468345761299\n",
      "Epoch 4546/20000 Training Loss: 0.0667952299118042\n",
      "Epoch 4547/20000 Training Loss: 0.060863565653562546\n",
      "Epoch 4548/20000 Training Loss: 0.06459934264421463\n",
      "Epoch 4549/20000 Training Loss: 0.04441428184509277\n",
      "Epoch 4550/20000 Training Loss: 0.06566082686185837\n",
      "Epoch 4550/20000 Validation Loss: 0.08435849100351334\n",
      "Epoch 4551/20000 Training Loss: 0.05216852203011513\n",
      "Epoch 4552/20000 Training Loss: 0.051003456115722656\n",
      "Epoch 4553/20000 Training Loss: 0.06875910609960556\n",
      "Epoch 4554/20000 Training Loss: 0.07425259798765182\n",
      "Epoch 4555/20000 Training Loss: 0.051459889858961105\n",
      "Epoch 4556/20000 Training Loss: 0.06769594550132751\n",
      "Epoch 4557/20000 Training Loss: 0.05458664894104004\n",
      "Epoch 4558/20000 Training Loss: 0.0721529945731163\n",
      "Epoch 4559/20000 Training Loss: 0.0663905218243599\n",
      "Epoch 4560/20000 Training Loss: 0.06283574551343918\n",
      "Epoch 4560/20000 Validation Loss: 0.07337593287229538\n",
      "Epoch 4561/20000 Training Loss: 0.0452977754175663\n",
      "Epoch 4562/20000 Training Loss: 0.03998658061027527\n",
      "Epoch 4563/20000 Training Loss: 0.05926264449954033\n",
      "Epoch 4564/20000 Training Loss: 0.04935627058148384\n",
      "Epoch 4565/20000 Training Loss: 0.03630497679114342\n",
      "Epoch 4566/20000 Training Loss: 0.0445905365049839\n",
      "Epoch 4567/20000 Training Loss: 0.05187622085213661\n",
      "Epoch 4568/20000 Training Loss: 0.040001604706048965\n",
      "Epoch 4569/20000 Training Loss: 0.06620362401008606\n",
      "Epoch 4570/20000 Training Loss: 0.05951898917555809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4570/20000 Validation Loss: 0.08733820915222168\n",
      "Epoch 4571/20000 Training Loss: 0.05627037212252617\n",
      "Epoch 4572/20000 Training Loss: 0.04570569097995758\n",
      "Epoch 4573/20000 Training Loss: 0.06350042670965195\n",
      "Epoch 4574/20000 Training Loss: 0.0593312568962574\n",
      "Epoch 4575/20000 Training Loss: 0.05505308508872986\n",
      "Epoch 4576/20000 Training Loss: 0.04703451320528984\n",
      "Epoch 4577/20000 Training Loss: 0.0584414005279541\n",
      "Epoch 4578/20000 Training Loss: 0.05340990051627159\n",
      "Epoch 4579/20000 Training Loss: 0.057974979281425476\n",
      "Epoch 4580/20000 Training Loss: 0.08184409886598587\n",
      "Epoch 4580/20000 Validation Loss: 0.04970096796751022\n",
      "Epoch 4581/20000 Training Loss: 0.06962421536445618\n",
      "Epoch 4582/20000 Training Loss: 0.05423659458756447\n",
      "Epoch 4583/20000 Training Loss: 0.071695476770401\n",
      "Epoch 4584/20000 Training Loss: 0.04961933568120003\n",
      "Epoch 4585/20000 Training Loss: 0.054200541228055954\n",
      "Epoch 4586/20000 Training Loss: 0.04842158779501915\n",
      "Epoch 4587/20000 Training Loss: 0.07487283647060394\n",
      "Epoch 4588/20000 Training Loss: 0.05098144710063934\n",
      "Epoch 4589/20000 Training Loss: 0.06274781376123428\n",
      "Epoch 4590/20000 Training Loss: 0.05976333096623421\n",
      "Epoch 4590/20000 Validation Loss: 0.051773831248283386\n",
      "Epoch 4591/20000 Training Loss: 0.06097196415066719\n",
      "Epoch 4592/20000 Training Loss: 0.05756719782948494\n",
      "Epoch 4593/20000 Training Loss: 0.06901615113019943\n",
      "Epoch 4594/20000 Training Loss: 0.05075649544596672\n",
      "Epoch 4595/20000 Training Loss: 0.050977203994989395\n",
      "Epoch 4596/20000 Training Loss: 0.06326364725828171\n",
      "Epoch 4597/20000 Training Loss: 0.0679529681801796\n",
      "Epoch 4598/20000 Training Loss: 0.06305717676877975\n",
      "Epoch 4599/20000 Training Loss: 0.05032948777079582\n",
      "Epoch 4600/20000 Training Loss: 0.04862767457962036\n",
      "Epoch 4600/20000 Validation Loss: 0.057096704840660095\n",
      "Epoch 4601/20000 Training Loss: 0.09143487364053726\n",
      "Epoch 4602/20000 Training Loss: 0.06523608416318893\n",
      "Epoch 4603/20000 Training Loss: 0.061803314834833145\n",
      "Epoch 4604/20000 Training Loss: 0.06396781653165817\n",
      "Epoch 4605/20000 Training Loss: 0.055364567786455154\n",
      "Epoch 4606/20000 Training Loss: 0.06701337546110153\n",
      "Epoch 4607/20000 Training Loss: 0.042674720287323\n",
      "Epoch 4608/20000 Training Loss: 0.05030088499188423\n",
      "Epoch 4609/20000 Training Loss: 0.06750492006540298\n",
      "Epoch 4610/20000 Training Loss: 0.06817661970853806\n",
      "Epoch 4610/20000 Validation Loss: 0.05827992409467697\n",
      "Epoch 4611/20000 Training Loss: 0.0652826800942421\n",
      "Epoch 4612/20000 Training Loss: 0.050332486629486084\n",
      "Epoch 4613/20000 Training Loss: 0.05354132875800133\n",
      "Epoch 4614/20000 Training Loss: 0.057972025126218796\n",
      "Epoch 4615/20000 Training Loss: 0.046553175896406174\n",
      "Epoch 4616/20000 Training Loss: 0.0629638209939003\n",
      "Epoch 4617/20000 Training Loss: 0.05976831912994385\n",
      "Epoch 4618/20000 Training Loss: 0.07156898826360703\n",
      "Epoch 4619/20000 Training Loss: 0.06333401799201965\n",
      "Epoch 4620/20000 Training Loss: 0.06286808103322983\n",
      "Epoch 4620/20000 Validation Loss: 0.04498284310102463\n",
      "Epoch 4621/20000 Training Loss: 0.05025169253349304\n",
      "Epoch 4622/20000 Training Loss: 0.06986873596906662\n",
      "Epoch 4623/20000 Training Loss: 0.05952346697449684\n",
      "Epoch 4624/20000 Training Loss: 0.059074968099594116\n",
      "Epoch 4625/20000 Training Loss: 0.038320787250995636\n",
      "Epoch 4626/20000 Training Loss: 0.03940272703766823\n",
      "Epoch 4627/20000 Training Loss: 0.05620838701725006\n",
      "Epoch 4628/20000 Training Loss: 0.05909620225429535\n",
      "Epoch 4629/20000 Training Loss: 0.052955809980630875\n",
      "Epoch 4630/20000 Training Loss: 0.05395631864666939\n",
      "Epoch 4630/20000 Validation Loss: 0.07147979736328125\n",
      "Epoch 4631/20000 Training Loss: 0.07300698757171631\n",
      "Epoch 4632/20000 Training Loss: 0.07068666070699692\n",
      "Epoch 4633/20000 Training Loss: 0.07005085796117783\n",
      "Epoch 4634/20000 Training Loss: 0.049602899700403214\n",
      "Epoch 4635/20000 Training Loss: 0.06299976259469986\n",
      "Epoch 4636/20000 Training Loss: 0.057740941643714905\n",
      "Epoch 4637/20000 Training Loss: 0.04622194170951843\n",
      "Epoch 4638/20000 Training Loss: 0.04567987844347954\n",
      "Epoch 4639/20000 Training Loss: 0.05870281159877777\n",
      "Epoch 4640/20000 Training Loss: 0.0702609047293663\n",
      "Epoch 4640/20000 Validation Loss: 0.09297581762075424\n",
      "Epoch 4641/20000 Training Loss: 0.06175554171204567\n",
      "Epoch 4642/20000 Training Loss: 0.0601508803665638\n",
      "Epoch 4643/20000 Training Loss: 0.07480684667825699\n",
      "Epoch 4644/20000 Training Loss: 0.04866693913936615\n",
      "Epoch 4645/20000 Training Loss: 0.07775753736495972\n",
      "Epoch 4646/20000 Training Loss: 0.06282683461904526\n",
      "Epoch 4647/20000 Training Loss: 0.04902660846710205\n",
      "Epoch 4648/20000 Training Loss: 0.05299786850810051\n",
      "Epoch 4649/20000 Training Loss: 0.052689313888549805\n",
      "Epoch 4650/20000 Training Loss: 0.047566160559654236\n",
      "Epoch 4650/20000 Validation Loss: 0.08507244288921356\n",
      "Epoch 4651/20000 Training Loss: 0.050731029361486435\n",
      "Epoch 4652/20000 Training Loss: 0.03483496233820915\n",
      "Epoch 4653/20000 Training Loss: 0.05399014428257942\n",
      "Epoch 4654/20000 Training Loss: 0.05590406432747841\n",
      "Epoch 4655/20000 Training Loss: 0.03559481352567673\n",
      "Epoch 4656/20000 Training Loss: 0.05471588298678398\n",
      "Epoch 4657/20000 Training Loss: 0.06205769255757332\n",
      "Epoch 4658/20000 Training Loss: 0.048460666090250015\n",
      "Epoch 4659/20000 Training Loss: 0.07463294267654419\n",
      "Epoch 4660/20000 Training Loss: 0.05286632105708122\n",
      "Epoch 4660/20000 Validation Loss: 0.07081806659698486\n",
      "Epoch 4661/20000 Training Loss: 0.06908082962036133\n",
      "Epoch 4662/20000 Training Loss: 0.056160714477300644\n",
      "Epoch 4663/20000 Training Loss: 0.051769718527793884\n",
      "Epoch 4664/20000 Training Loss: 0.07208740711212158\n",
      "Epoch 4665/20000 Training Loss: 0.05848779156804085\n",
      "Epoch 4666/20000 Training Loss: 0.0562913715839386\n",
      "Epoch 4667/20000 Training Loss: 0.07957115024328232\n",
      "Epoch 4668/20000 Training Loss: 0.04584202170372009\n",
      "Epoch 4669/20000 Training Loss: 0.06615876406431198\n",
      "Epoch 4670/20000 Training Loss: 0.054609254002571106\n",
      "Epoch 4670/20000 Validation Loss: 0.052513960748910904\n",
      "Epoch 4671/20000 Training Loss: 0.0485348254442215\n",
      "Epoch 4672/20000 Training Loss: 0.04696055129170418\n",
      "Epoch 4673/20000 Training Loss: 0.05491204187273979\n",
      "Epoch 4674/20000 Training Loss: 0.0549841970205307\n",
      "Epoch 4675/20000 Training Loss: 0.055402934551239014\n",
      "Epoch 4676/20000 Training Loss: 0.06239193305373192\n",
      "Epoch 4677/20000 Training Loss: 0.049107760190963745\n",
      "Epoch 4678/20000 Training Loss: 0.0642026886343956\n",
      "Epoch 4679/20000 Training Loss: 0.06563412398099899\n",
      "Epoch 4680/20000 Training Loss: 0.060223329812288284\n",
      "Epoch 4680/20000 Validation Loss: 0.048942338675260544\n",
      "Epoch 4681/20000 Training Loss: 0.06460031121969223\n",
      "Epoch 4682/20000 Training Loss: 0.05737657845020294\n",
      "Epoch 4683/20000 Training Loss: 0.04979808256030083\n",
      "Epoch 4684/20000 Training Loss: 0.059869974851608276\n",
      "Epoch 4685/20000 Training Loss: 0.056988079100847244\n",
      "Epoch 4686/20000 Training Loss: 0.056873854249715805\n",
      "Epoch 4687/20000 Training Loss: 0.05853257700800896\n",
      "Epoch 4688/20000 Training Loss: 0.055239077657461166\n",
      "Epoch 4689/20000 Training Loss: 0.07163942605257034\n",
      "Epoch 4690/20000 Training Loss: 0.052667971700429916\n",
      "Epoch 4690/20000 Validation Loss: 0.056998565793037415\n",
      "Epoch 4691/20000 Training Loss: 0.04519675672054291\n",
      "Epoch 4692/20000 Training Loss: 0.059909045696258545\n",
      "Epoch 4693/20000 Training Loss: 0.057497601956129074\n",
      "Epoch 4694/20000 Training Loss: 0.05909610167145729\n",
      "Epoch 4695/20000 Training Loss: 0.04831482842564583\n",
      "Epoch 4696/20000 Training Loss: 0.04903009906411171\n",
      "Epoch 4697/20000 Training Loss: 0.07222852110862732\n",
      "Epoch 4698/20000 Training Loss: 0.05592736601829529\n",
      "Epoch 4699/20000 Training Loss: 0.062388885766267776\n",
      "Epoch 4700/20000 Training Loss: 0.05357706546783447\n",
      "Epoch 4700/20000 Validation Loss: 0.0464739128947258\n",
      "Epoch 4701/20000 Training Loss: 0.06380009651184082\n",
      "Epoch 4702/20000 Training Loss: 0.07197469472885132\n",
      "Epoch 4703/20000 Training Loss: 0.058663610368967056\n",
      "Epoch 4704/20000 Training Loss: 0.04836936667561531\n",
      "Epoch 4705/20000 Training Loss: 0.04910096153616905\n",
      "Epoch 4706/20000 Training Loss: 0.06720001250505447\n",
      "Epoch 4707/20000 Training Loss: 0.06909065693616867\n",
      "Epoch 4708/20000 Training Loss: 0.07468678802251816\n",
      "Epoch 4709/20000 Training Loss: 0.04469166696071625\n",
      "Epoch 4710/20000 Training Loss: 0.07687247544527054\n",
      "Epoch 4710/20000 Validation Loss: 0.05535119026899338\n",
      "Epoch 4711/20000 Training Loss: 0.08295494318008423\n",
      "Epoch 4712/20000 Training Loss: 0.062453340739011765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4713/20000 Training Loss: 0.050483450293540955\n",
      "Epoch 4714/20000 Training Loss: 0.0639810562133789\n",
      "Epoch 4715/20000 Training Loss: 0.07252218574285507\n",
      "Epoch 4716/20000 Training Loss: 0.052313223481178284\n",
      "Epoch 4717/20000 Training Loss: 0.05737254023551941\n",
      "Epoch 4718/20000 Training Loss: 0.06793943792581558\n",
      "Epoch 4719/20000 Training Loss: 0.06892356276512146\n",
      "Epoch 4720/20000 Training Loss: 0.06850845366716385\n",
      "Epoch 4720/20000 Validation Loss: 0.06348185241222382\n",
      "Epoch 4721/20000 Training Loss: 0.0755915716290474\n",
      "Epoch 4722/20000 Training Loss: 0.05359197035431862\n",
      "Epoch 4723/20000 Training Loss: 0.05436820909380913\n",
      "Epoch 4724/20000 Training Loss: 0.0694907009601593\n",
      "Epoch 4725/20000 Training Loss: 0.05109388753771782\n",
      "Epoch 4726/20000 Training Loss: 0.05739961937069893\n",
      "Epoch 4727/20000 Training Loss: 0.07645066827535629\n",
      "Epoch 4728/20000 Training Loss: 0.07689234614372253\n",
      "Epoch 4729/20000 Training Loss: 0.06463340669870377\n",
      "Epoch 4730/20000 Training Loss: 0.0587703138589859\n",
      "Epoch 4730/20000 Validation Loss: 0.06767073273658752\n",
      "Epoch 4731/20000 Training Loss: 0.07346188277006149\n",
      "Epoch 4732/20000 Training Loss: 0.07998789101839066\n",
      "Epoch 4733/20000 Training Loss: 0.05315869674086571\n",
      "Epoch 4734/20000 Training Loss: 0.053886037319898605\n",
      "Epoch 4735/20000 Training Loss: 0.048420459032058716\n",
      "Epoch 4736/20000 Training Loss: 0.06686655431985855\n",
      "Epoch 4737/20000 Training Loss: 0.04649217054247856\n",
      "Epoch 4738/20000 Training Loss: 0.03944021090865135\n",
      "Epoch 4739/20000 Training Loss: 0.05931190773844719\n",
      "Epoch 4740/20000 Training Loss: 0.051559001207351685\n",
      "Epoch 4740/20000 Validation Loss: 0.04712234437465668\n",
      "Epoch 4741/20000 Training Loss: 0.060191620141267776\n",
      "Epoch 4742/20000 Training Loss: 0.05151742324233055\n",
      "Epoch 4743/20000 Training Loss: 0.05720103904604912\n",
      "Epoch 4744/20000 Training Loss: 0.08145507425069809\n",
      "Epoch 4745/20000 Training Loss: 0.07713053375482559\n",
      "Epoch 4746/20000 Training Loss: 0.06634466350078583\n",
      "Epoch 4747/20000 Training Loss: 0.050607312470674515\n",
      "Epoch 4748/20000 Training Loss: 0.06154760718345642\n",
      "Epoch 4749/20000 Training Loss: 0.0793253481388092\n",
      "Epoch 4750/20000 Training Loss: 0.04251299798488617\n",
      "Epoch 4750/20000 Validation Loss: 0.04109717905521393\n",
      "Epoch 4751/20000 Training Loss: 0.05824703350663185\n",
      "Epoch 4752/20000 Training Loss: 0.07518669217824936\n",
      "Epoch 4753/20000 Training Loss: 0.04852461814880371\n",
      "Epoch 4754/20000 Training Loss: 0.052390147000551224\n",
      "Epoch 4755/20000 Training Loss: 0.0467614121735096\n",
      "Epoch 4756/20000 Training Loss: 0.056411128491163254\n",
      "Epoch 4757/20000 Training Loss: 0.04535026475787163\n",
      "Epoch 4758/20000 Training Loss: 0.08566034585237503\n",
      "Epoch 4759/20000 Training Loss: 0.05153811350464821\n",
      "Epoch 4760/20000 Training Loss: 0.05575782060623169\n",
      "Epoch 4760/20000 Validation Loss: 0.06464192271232605\n",
      "Epoch 4761/20000 Training Loss: 0.06685971468687057\n",
      "Epoch 4762/20000 Training Loss: 0.05019978806376457\n",
      "Epoch 4763/20000 Training Loss: 0.06662585586309433\n",
      "Epoch 4764/20000 Training Loss: 0.06281355023384094\n",
      "Epoch 4765/20000 Training Loss: 0.060848113149404526\n",
      "Epoch 4766/20000 Training Loss: 0.07831932604312897\n",
      "Epoch 4767/20000 Training Loss: 0.06104481220245361\n",
      "Epoch 4768/20000 Training Loss: 0.052688393741846085\n",
      "Epoch 4769/20000 Training Loss: 0.05565119907259941\n",
      "Epoch 4770/20000 Training Loss: 0.06852666288614273\n",
      "Epoch 4770/20000 Validation Loss: 0.05754678696393967\n",
      "Epoch 4771/20000 Training Loss: 0.058150410652160645\n",
      "Epoch 4772/20000 Training Loss: 0.04961438104510307\n",
      "Epoch 4773/20000 Training Loss: 0.051348570734262466\n",
      "Epoch 4774/20000 Training Loss: 0.06222107633948326\n",
      "Epoch 4775/20000 Training Loss: 0.06300758570432663\n",
      "Epoch 4776/20000 Training Loss: 0.05090426281094551\n",
      "Epoch 4777/20000 Training Loss: 0.06548772007226944\n",
      "Epoch 4778/20000 Training Loss: 0.058937326073646545\n",
      "Epoch 4779/20000 Training Loss: 0.04727831482887268\n",
      "Epoch 4780/20000 Training Loss: 0.06601714342832565\n",
      "Epoch 4780/20000 Validation Loss: 0.04856906086206436\n",
      "Epoch 4781/20000 Training Loss: 0.06996232271194458\n",
      "Epoch 4782/20000 Training Loss: 0.04887036606669426\n",
      "Epoch 4783/20000 Training Loss: 0.061773061752319336\n",
      "Epoch 4784/20000 Training Loss: 0.07551965862512589\n",
      "Epoch 4785/20000 Training Loss: 0.049255531281232834\n",
      "Epoch 4786/20000 Training Loss: 0.06559555977582932\n",
      "Epoch 4787/20000 Training Loss: 0.0639239028096199\n",
      "Epoch 4788/20000 Training Loss: 0.04536512866616249\n",
      "Epoch 4789/20000 Training Loss: 0.04185876250267029\n",
      "Epoch 4790/20000 Training Loss: 0.056321095675230026\n",
      "Epoch 4790/20000 Validation Loss: 0.0454622246325016\n",
      "Epoch 4791/20000 Training Loss: 0.06057257577776909\n",
      "Epoch 4792/20000 Training Loss: 0.058609917759895325\n",
      "Epoch 4793/20000 Training Loss: 0.05141298845410347\n",
      "Epoch 4794/20000 Training Loss: 0.058893460780382156\n",
      "Epoch 4795/20000 Training Loss: 0.06270229071378708\n",
      "Epoch 4796/20000 Training Loss: 0.05803179740905762\n",
      "Epoch 4797/20000 Training Loss: 0.0712103471159935\n",
      "Epoch 4798/20000 Training Loss: 0.06179147958755493\n",
      "Epoch 4799/20000 Training Loss: 0.06497776508331299\n",
      "Epoch 4800/20000 Training Loss: 0.05038085952401161\n",
      "Epoch 4800/20000 Validation Loss: 0.07156115770339966\n",
      "Epoch 4801/20000 Training Loss: 0.05742065981030464\n",
      "Epoch 4802/20000 Training Loss: 0.06053217127919197\n",
      "Epoch 4803/20000 Training Loss: 0.08607295900583267\n",
      "Epoch 4804/20000 Training Loss: 0.08531395345926285\n",
      "Epoch 4805/20000 Training Loss: 0.05326031520962715\n",
      "Epoch 4806/20000 Training Loss: 0.050077542662620544\n",
      "Epoch 4807/20000 Training Loss: 0.05946685001254082\n",
      "Epoch 4808/20000 Training Loss: 0.05037454143166542\n",
      "Epoch 4809/20000 Training Loss: 0.04552319273352623\n",
      "Epoch 4810/20000 Training Loss: 0.05981863662600517\n",
      "Epoch 4810/20000 Validation Loss: 0.060976430773735046\n",
      "Epoch 4811/20000 Training Loss: 0.06843582540750504\n",
      "Epoch 4812/20000 Training Loss: 0.05915454030036926\n",
      "Epoch 4813/20000 Training Loss: 0.045043572783470154\n",
      "Epoch 4814/20000 Training Loss: 0.07232461124658585\n",
      "Epoch 4815/20000 Training Loss: 0.07426217943429947\n",
      "Epoch 4816/20000 Training Loss: 0.05525781214237213\n",
      "Epoch 4817/20000 Training Loss: 0.08301214128732681\n",
      "Epoch 4818/20000 Training Loss: 0.05079181119799614\n",
      "Epoch 4819/20000 Training Loss: 0.052078183740377426\n",
      "Epoch 4820/20000 Training Loss: 0.0583820678293705\n",
      "Epoch 4820/20000 Validation Loss: 0.060718901455402374\n",
      "Epoch 4821/20000 Training Loss: 0.042249854654073715\n",
      "Epoch 4822/20000 Training Loss: 0.07267151772975922\n",
      "Epoch 4823/20000 Training Loss: 0.05552763119339943\n",
      "Epoch 4824/20000 Training Loss: 0.04388992115855217\n",
      "Epoch 4825/20000 Training Loss: 0.07587087154388428\n",
      "Epoch 4826/20000 Training Loss: 0.051347773522138596\n",
      "Epoch 4827/20000 Training Loss: 0.061544179916381836\n",
      "Epoch 4828/20000 Training Loss: 0.06306568533182144\n",
      "Epoch 4829/20000 Training Loss: 0.07551691681146622\n",
      "Epoch 4830/20000 Training Loss: 0.06303218752145767\n",
      "Epoch 4830/20000 Validation Loss: 0.041284821927547455\n",
      "Epoch 4831/20000 Training Loss: 0.04419182240962982\n",
      "Epoch 4832/20000 Training Loss: 0.04994192346930504\n",
      "Epoch 4833/20000 Training Loss: 0.059976864606142044\n",
      "Epoch 4834/20000 Training Loss: 0.05393926426768303\n",
      "Epoch 4835/20000 Training Loss: 0.07313022017478943\n",
      "Epoch 4836/20000 Training Loss: 0.059009406715631485\n",
      "Epoch 4837/20000 Training Loss: 0.051980603486299515\n",
      "Epoch 4838/20000 Training Loss: 0.05872373655438423\n",
      "Epoch 4839/20000 Training Loss: 0.05137062072753906\n",
      "Epoch 4840/20000 Training Loss: 0.06298747658729553\n",
      "Epoch 4840/20000 Validation Loss: 0.0517311729490757\n",
      "Epoch 4841/20000 Training Loss: 0.04869071766734123\n",
      "Epoch 4842/20000 Training Loss: 0.0558917410671711\n",
      "Epoch 4843/20000 Training Loss: 0.06510432809591293\n",
      "Epoch 4844/20000 Training Loss: 0.052278101444244385\n",
      "Epoch 4845/20000 Training Loss: 0.06775056570768356\n",
      "Epoch 4846/20000 Training Loss: 0.05874990299344063\n",
      "Epoch 4847/20000 Training Loss: 0.03907932713627815\n",
      "Epoch 4848/20000 Training Loss: 0.05254761502146721\n",
      "Epoch 4849/20000 Training Loss: 0.05582912638783455\n",
      "Epoch 4850/20000 Training Loss: 0.06227976083755493\n",
      "Epoch 4850/20000 Validation Loss: 0.048430200666189194\n",
      "Epoch 4851/20000 Training Loss: 0.05863252654671669\n",
      "Epoch 4852/20000 Training Loss: 0.05318264290690422\n",
      "Epoch 4853/20000 Training Loss: 0.06041504815220833\n",
      "Epoch 4854/20000 Training Loss: 0.0644858106970787\n",
      "Epoch 4855/20000 Training Loss: 0.06421247869729996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4856/20000 Training Loss: 0.06706773489713669\n",
      "Epoch 4857/20000 Training Loss: 0.05808046832680702\n",
      "Epoch 4858/20000 Training Loss: 0.07695835083723068\n",
      "Epoch 4859/20000 Training Loss: 0.04890445992350578\n",
      "Epoch 4860/20000 Training Loss: 0.05439719557762146\n",
      "Epoch 4860/20000 Validation Loss: 0.042859047651290894\n",
      "Epoch 4861/20000 Training Loss: 0.04518362879753113\n",
      "Epoch 4862/20000 Training Loss: 0.081060029566288\n",
      "Epoch 4863/20000 Training Loss: 0.05091315880417824\n",
      "Epoch 4864/20000 Training Loss: 0.05738775059580803\n",
      "Epoch 4865/20000 Training Loss: 0.05676337704062462\n",
      "Epoch 4866/20000 Training Loss: 0.07094771414995193\n",
      "Epoch 4867/20000 Training Loss: 0.04859752580523491\n",
      "Epoch 4868/20000 Training Loss: 0.05396029353141785\n",
      "Epoch 4869/20000 Training Loss: 0.04936465993523598\n",
      "Epoch 4870/20000 Training Loss: 0.05297163501381874\n",
      "Epoch 4870/20000 Validation Loss: 0.03997102379798889\n",
      "Epoch 4871/20000 Training Loss: 0.046619679778814316\n",
      "Epoch 4872/20000 Training Loss: 0.0631302073597908\n",
      "Epoch 4873/20000 Training Loss: 0.05031011998653412\n",
      "Epoch 4874/20000 Training Loss: 0.06243278458714485\n",
      "Epoch 4875/20000 Training Loss: 0.048542529344558716\n",
      "Epoch 4876/20000 Training Loss: 0.03895784541964531\n",
      "Epoch 4877/20000 Training Loss: 0.06296679377555847\n",
      "Epoch 4878/20000 Training Loss: 0.07563680410385132\n",
      "Epoch 4879/20000 Training Loss: 0.04699933901429176\n",
      "Epoch 4880/20000 Training Loss: 0.03752844035625458\n",
      "Epoch 4880/20000 Validation Loss: 0.061299677938222885\n",
      "Epoch 4881/20000 Training Loss: 0.06659331172704697\n",
      "Epoch 4882/20000 Training Loss: 0.04744487628340721\n",
      "Epoch 4883/20000 Training Loss: 0.04105560481548309\n",
      "Epoch 4884/20000 Training Loss: 0.05146988853812218\n",
      "Epoch 4885/20000 Training Loss: 0.06162619963288307\n",
      "Epoch 4886/20000 Training Loss: 0.05889015272259712\n",
      "Epoch 4887/20000 Training Loss: 0.06771738082170486\n",
      "Epoch 4888/20000 Training Loss: 0.06898745894432068\n",
      "Epoch 4889/20000 Training Loss: 0.07934270054101944\n",
      "Epoch 4890/20000 Training Loss: 0.042846132069826126\n",
      "Epoch 4890/20000 Validation Loss: 0.056340523064136505\n",
      "Epoch 4891/20000 Training Loss: 0.04283538833260536\n",
      "Epoch 4892/20000 Training Loss: 0.06884660571813583\n",
      "Epoch 4893/20000 Training Loss: 0.07036740332841873\n",
      "Epoch 4894/20000 Training Loss: 0.05710916593670845\n",
      "Epoch 4895/20000 Training Loss: 0.06420886516571045\n",
      "Epoch 4896/20000 Training Loss: 0.04419879987835884\n",
      "Epoch 4897/20000 Training Loss: 0.06122204661369324\n",
      "Epoch 4898/20000 Training Loss: 0.0594460666179657\n",
      "Epoch 4899/20000 Training Loss: 0.053668200969696045\n",
      "Epoch 4900/20000 Training Loss: 0.0692395269870758\n",
      "Epoch 4900/20000 Validation Loss: 0.04753970354795456\n",
      "Epoch 4901/20000 Training Loss: 0.057324156165122986\n",
      "Epoch 4902/20000 Training Loss: 0.06592314690351486\n",
      "Epoch 4903/20000 Training Loss: 0.05406753718852997\n",
      "Epoch 4904/20000 Training Loss: 0.050055041909217834\n",
      "Epoch 4905/20000 Training Loss: 0.043715089559555054\n",
      "Epoch 4906/20000 Training Loss: 0.0471988171339035\n",
      "Epoch 4907/20000 Training Loss: 0.0538870245218277\n",
      "Epoch 4908/20000 Training Loss: 0.04448023438453674\n",
      "Epoch 4909/20000 Training Loss: 0.05120311677455902\n",
      "Epoch 4910/20000 Training Loss: 0.06596533209085464\n",
      "Epoch 4910/20000 Validation Loss: 0.058076053857803345\n",
      "Epoch 4911/20000 Training Loss: 0.06741370260715485\n",
      "Epoch 4912/20000 Training Loss: 0.053426165133714676\n",
      "Epoch 4913/20000 Training Loss: 0.07116945832967758\n",
      "Epoch 4914/20000 Training Loss: 0.06791837513446808\n",
      "Epoch 4915/20000 Training Loss: 0.0429106242954731\n",
      "Epoch 4916/20000 Training Loss: 0.053825512528419495\n",
      "Epoch 4917/20000 Training Loss: 0.058039333671331406\n",
      "Epoch 4918/20000 Training Loss: 0.05986923351883888\n",
      "Epoch 4919/20000 Training Loss: 0.04885281249880791\n",
      "Epoch 4920/20000 Training Loss: 0.03706797957420349\n",
      "Epoch 4920/20000 Validation Loss: 0.046821147203445435\n",
      "Epoch 4921/20000 Training Loss: 0.05052429810166359\n",
      "Epoch 4922/20000 Training Loss: 0.05213877931237221\n",
      "Epoch 4923/20000 Training Loss: 0.06835449486970901\n",
      "Epoch 4924/20000 Training Loss: 0.046773746609687805\n",
      "Epoch 4925/20000 Training Loss: 0.06253347545862198\n",
      "Epoch 4926/20000 Training Loss: 0.07386699318885803\n",
      "Epoch 4927/20000 Training Loss: 0.06043090298771858\n",
      "Epoch 4928/20000 Training Loss: 0.05432137846946716\n",
      "Epoch 4929/20000 Training Loss: 0.05127716436982155\n",
      "Epoch 4930/20000 Training Loss: 0.05582690238952637\n",
      "Epoch 4930/20000 Validation Loss: 0.0646931529045105\n",
      "Epoch 4931/20000 Training Loss: 0.04912503436207771\n",
      "Epoch 4932/20000 Training Loss: 0.06864560395479202\n",
      "Epoch 4933/20000 Training Loss: 0.07434162497520447\n",
      "Epoch 4934/20000 Training Loss: 0.07021120190620422\n",
      "Epoch 4935/20000 Training Loss: 0.05541868135333061\n",
      "Epoch 4936/20000 Training Loss: 0.0546606183052063\n",
      "Epoch 4937/20000 Training Loss: 0.05395190790295601\n",
      "Epoch 4938/20000 Training Loss: 0.061601001769304276\n",
      "Epoch 4939/20000 Training Loss: 0.057003945112228394\n",
      "Epoch 4940/20000 Training Loss: 0.050093408674001694\n",
      "Epoch 4940/20000 Validation Loss: 0.0673472136259079\n",
      "Epoch 4941/20000 Training Loss: 0.059334203600883484\n",
      "Epoch 4942/20000 Training Loss: 0.044672638177871704\n",
      "Epoch 4943/20000 Training Loss: 0.0352032296359539\n",
      "Epoch 4944/20000 Training Loss: 0.05139272287487984\n",
      "Epoch 4945/20000 Training Loss: 0.06189081072807312\n",
      "Epoch 4946/20000 Training Loss: 0.032802045345306396\n",
      "Epoch 4947/20000 Training Loss: 0.04096592590212822\n",
      "Epoch 4948/20000 Training Loss: 0.0477033406496048\n",
      "Epoch 4949/20000 Training Loss: 0.055675771087408066\n",
      "Epoch 4950/20000 Training Loss: 0.0499599389731884\n",
      "Epoch 4950/20000 Validation Loss: 0.04078326374292374\n",
      "Epoch 4951/20000 Training Loss: 0.0609482079744339\n",
      "Epoch 4952/20000 Training Loss: 0.07401945441961288\n",
      "Epoch 4953/20000 Training Loss: 0.05659984052181244\n",
      "Epoch 4954/20000 Training Loss: 0.04478256776928902\n",
      "Epoch 4955/20000 Training Loss: 0.05756646767258644\n",
      "Epoch 4956/20000 Training Loss: 0.060856062918901443\n",
      "Epoch 4957/20000 Training Loss: 0.06870070844888687\n",
      "Epoch 4958/20000 Training Loss: 0.05145685002207756\n",
      "Epoch 4959/20000 Training Loss: 0.0516478605568409\n",
      "Epoch 4960/20000 Training Loss: 0.05809237062931061\n",
      "Epoch 4960/20000 Validation Loss: 0.06478625535964966\n",
      "Epoch 4961/20000 Training Loss: 0.06317930668592453\n",
      "Epoch 4962/20000 Training Loss: 0.06223931908607483\n",
      "Epoch 4963/20000 Training Loss: 0.07559584826231003\n",
      "Epoch 4964/20000 Training Loss: 0.06282293796539307\n",
      "Epoch 4965/20000 Training Loss: 0.06938998401165009\n",
      "Epoch 4966/20000 Training Loss: 0.059725016355514526\n",
      "Epoch 4967/20000 Training Loss: 0.055630818009376526\n",
      "Epoch 4968/20000 Training Loss: 0.05859145522117615\n",
      "Epoch 4969/20000 Training Loss: 0.06537002325057983\n",
      "Epoch 4970/20000 Training Loss: 0.06164664030075073\n",
      "Epoch 4970/20000 Validation Loss: 0.06216104328632355\n",
      "Epoch 4971/20000 Training Loss: 0.0579460971057415\n",
      "Epoch 4972/20000 Training Loss: 0.049230318516492844\n",
      "Epoch 4973/20000 Training Loss: 0.06270018219947815\n",
      "Epoch 4974/20000 Training Loss: 0.052777018398046494\n",
      "Epoch 4975/20000 Training Loss: 0.07884018123149872\n",
      "Epoch 4976/20000 Training Loss: 0.062068257480859756\n",
      "Epoch 4977/20000 Training Loss: 0.060183361172676086\n",
      "Epoch 4978/20000 Training Loss: 0.06955862045288086\n",
      "Epoch 4979/20000 Training Loss: 0.04947327449917793\n",
      "Epoch 4980/20000 Training Loss: 0.061778873205184937\n",
      "Epoch 4980/20000 Validation Loss: 0.06515902280807495\n",
      "Epoch 4981/20000 Training Loss: 0.05399298295378685\n",
      "Epoch 4982/20000 Training Loss: 0.052194952964782715\n",
      "Epoch 4983/20000 Training Loss: 0.04585293307900429\n",
      "Epoch 4984/20000 Training Loss: 0.05694779381155968\n",
      "Epoch 4985/20000 Training Loss: 0.04740707576274872\n",
      "Epoch 4986/20000 Training Loss: 0.04671672359108925\n",
      "Epoch 4987/20000 Training Loss: 0.04856223985552788\n",
      "Epoch 4988/20000 Training Loss: 0.05336730554699898\n",
      "Epoch 4989/20000 Training Loss: 0.06086171790957451\n",
      "Epoch 4990/20000 Training Loss: 0.06830880045890808\n",
      "Epoch 4990/20000 Validation Loss: 0.06144373118877411\n",
      "Epoch 4991/20000 Training Loss: 0.04868344962596893\n",
      "Epoch 4992/20000 Training Loss: 0.046478960663080215\n",
      "Epoch 4993/20000 Training Loss: 0.052312254905700684\n",
      "Epoch 4994/20000 Training Loss: 0.06978493183851242\n",
      "Epoch 4995/20000 Training Loss: 0.047291457653045654\n",
      "Epoch 4996/20000 Training Loss: 0.06775514036417007\n",
      "Epoch 4997/20000 Training Loss: 0.0730506181716919\n",
      "Epoch 4998/20000 Training Loss: 0.06027252599596977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999/20000 Training Loss: 0.06489860266447067\n",
      "Epoch 5000/20000 Training Loss: 0.0742507353425026\n",
      "Epoch 5000/20000 Validation Loss: 0.06822624802589417\n",
      "Epoch 5001/20000 Training Loss: 0.07171359658241272\n",
      "Epoch 5002/20000 Training Loss: 0.05620458722114563\n",
      "Epoch 5003/20000 Training Loss: 0.05147963762283325\n",
      "Epoch 5004/20000 Training Loss: 0.0657556876540184\n",
      "Epoch 5005/20000 Training Loss: 0.05549607798457146\n",
      "Epoch 5006/20000 Training Loss: 0.07843022793531418\n",
      "Epoch 5007/20000 Training Loss: 0.04774438962340355\n",
      "Epoch 5008/20000 Training Loss: 0.05920957028865814\n",
      "Epoch 5009/20000 Training Loss: 0.048723045736551285\n",
      "Epoch 5010/20000 Training Loss: 0.08027392625808716\n",
      "Epoch 5010/20000 Validation Loss: 0.04742312803864479\n",
      "Epoch 5011/20000 Training Loss: 0.05598863959312439\n",
      "Epoch 5012/20000 Training Loss: 0.05423558130860329\n",
      "Epoch 5013/20000 Training Loss: 0.06731399893760681\n",
      "Epoch 5014/20000 Training Loss: 0.07010107487440109\n",
      "Epoch 5015/20000 Training Loss: 0.07623913139104843\n",
      "Epoch 5016/20000 Training Loss: 0.04947246238589287\n",
      "Epoch 5017/20000 Training Loss: 0.05767327919602394\n",
      "Epoch 5018/20000 Training Loss: 0.09603498131036758\n",
      "Epoch 5019/20000 Training Loss: 0.05716869607567787\n",
      "Epoch 5020/20000 Training Loss: 0.05939066410064697\n",
      "Epoch 5020/20000 Validation Loss: 0.05686549097299576\n",
      "Epoch 5021/20000 Training Loss: 0.05322270467877388\n",
      "Epoch 5022/20000 Training Loss: 0.06250124424695969\n",
      "Epoch 5023/20000 Training Loss: 0.048659875988960266\n",
      "Epoch 5024/20000 Training Loss: 0.06362249702215195\n",
      "Epoch 5025/20000 Training Loss: 0.054184690117836\n",
      "Epoch 5026/20000 Training Loss: 0.0519966185092926\n",
      "Epoch 5027/20000 Training Loss: 0.06575245410203934\n",
      "Epoch 5028/20000 Training Loss: 0.058920662850141525\n",
      "Epoch 5029/20000 Training Loss: 0.0872911736369133\n",
      "Epoch 5030/20000 Training Loss: 0.05396987497806549\n",
      "Epoch 5030/20000 Validation Loss: 0.05153628811240196\n",
      "Epoch 5031/20000 Training Loss: 0.06170490384101868\n",
      "Epoch 5032/20000 Training Loss: 0.04392072185873985\n",
      "Epoch 5033/20000 Training Loss: 0.06155823543667793\n",
      "Epoch 5034/20000 Training Loss: 0.07489189505577087\n",
      "Epoch 5035/20000 Training Loss: 0.06569266319274902\n",
      "Epoch 5036/20000 Training Loss: 0.06656453013420105\n",
      "Epoch 5037/20000 Training Loss: 0.07104336470365524\n",
      "Epoch 5038/20000 Training Loss: 0.05848018825054169\n",
      "Epoch 5039/20000 Training Loss: 0.059655290096998215\n",
      "Epoch 5040/20000 Training Loss: 0.06598179787397385\n",
      "Epoch 5040/20000 Validation Loss: 0.06056375056505203\n",
      "Epoch 5041/20000 Training Loss: 0.05031957849860191\n",
      "Epoch 5042/20000 Training Loss: 0.07128199189901352\n",
      "Epoch 5043/20000 Training Loss: 0.06330465525388718\n",
      "Epoch 5044/20000 Training Loss: 0.04327724501490593\n",
      "Epoch 5045/20000 Training Loss: 0.0608043372631073\n",
      "Epoch 5046/20000 Training Loss: 0.07746724039316177\n",
      "Epoch 5047/20000 Training Loss: 0.053152453154325485\n",
      "Epoch 5048/20000 Training Loss: 0.07111231237649918\n",
      "Epoch 5049/20000 Training Loss: 0.0460195355117321\n",
      "Epoch 5050/20000 Training Loss: 0.06448622792959213\n",
      "Epoch 5050/20000 Validation Loss: 0.07116576284170151\n",
      "Epoch 5051/20000 Training Loss: 0.05545609071850777\n",
      "Epoch 5052/20000 Training Loss: 0.053930189460515976\n",
      "Epoch 5053/20000 Training Loss: 0.06466379761695862\n",
      "Epoch 5054/20000 Training Loss: 0.0623425655066967\n",
      "Epoch 5055/20000 Training Loss: 0.07644670456647873\n",
      "Epoch 5056/20000 Training Loss: 0.07183068990707397\n",
      "Epoch 5057/20000 Training Loss: 0.06282491236925125\n",
      "Epoch 5058/20000 Training Loss: 0.0617225207388401\n",
      "Epoch 5059/20000 Training Loss: 0.06114142760634422\n",
      "Epoch 5060/20000 Training Loss: 0.06864198297262192\n",
      "Epoch 5060/20000 Validation Loss: 0.051184698939323425\n",
      "Epoch 5061/20000 Training Loss: 0.05420688912272453\n",
      "Epoch 5062/20000 Training Loss: 0.05018584057688713\n",
      "Epoch 5063/20000 Training Loss: 0.06483875215053558\n",
      "Epoch 5064/20000 Training Loss: 0.06288290023803711\n",
      "Epoch 5065/20000 Training Loss: 0.043902475386857986\n",
      "Epoch 5066/20000 Training Loss: 0.04969176650047302\n",
      "Epoch 5067/20000 Training Loss: 0.041860174387693405\n",
      "Epoch 5068/20000 Training Loss: 0.059591144323349\n",
      "Epoch 5069/20000 Training Loss: 0.04727676510810852\n",
      "Epoch 5070/20000 Training Loss: 0.05104980245232582\n",
      "Epoch 5070/20000 Validation Loss: 0.05638459324836731\n",
      "Epoch 5071/20000 Training Loss: 0.06402196735143661\n",
      "Epoch 5072/20000 Training Loss: 0.05581117048859596\n",
      "Epoch 5073/20000 Training Loss: 0.05350318178534508\n",
      "Epoch 5074/20000 Training Loss: 0.04812778905034065\n",
      "Epoch 5075/20000 Training Loss: 0.06044573709368706\n",
      "Epoch 5076/20000 Training Loss: 0.06375828385353088\n",
      "Epoch 5077/20000 Training Loss: 0.054290082305669785\n",
      "Epoch 5078/20000 Training Loss: 0.05827619135379791\n",
      "Epoch 5079/20000 Training Loss: 0.04495120048522949\n",
      "Epoch 5080/20000 Training Loss: 0.05916208028793335\n",
      "Epoch 5080/20000 Validation Loss: 0.08367924392223358\n",
      "Epoch 5081/20000 Training Loss: 0.04007185995578766\n",
      "Epoch 5082/20000 Training Loss: 0.05158432200551033\n",
      "Epoch 5083/20000 Training Loss: 0.03891262412071228\n",
      "Epoch 5084/20000 Training Loss: 0.0726289227604866\n",
      "Epoch 5085/20000 Training Loss: 0.05058562383055687\n",
      "Epoch 5086/20000 Training Loss: 0.0746208056807518\n",
      "Epoch 5087/20000 Training Loss: 0.05905911326408386\n",
      "Epoch 5088/20000 Training Loss: 0.06174976006150246\n",
      "Epoch 5089/20000 Training Loss: 0.05374671146273613\n",
      "Epoch 5090/20000 Training Loss: 0.05042128264904022\n",
      "Epoch 5090/20000 Validation Loss: 0.037434980273246765\n",
      "Epoch 5091/20000 Training Loss: 0.07300736755132675\n",
      "Epoch 5092/20000 Training Loss: 0.06322810798883438\n",
      "Epoch 5093/20000 Training Loss: 0.05725981667637825\n",
      "Epoch 5094/20000 Training Loss: 0.07136716693639755\n",
      "Epoch 5095/20000 Training Loss: 0.05947600305080414\n",
      "Epoch 5096/20000 Training Loss: 0.04501250386238098\n",
      "Epoch 5097/20000 Training Loss: 0.05162477865815163\n",
      "Epoch 5098/20000 Training Loss: 0.04733431339263916\n",
      "Epoch 5099/20000 Training Loss: 0.050807904452085495\n",
      "Epoch 5100/20000 Training Loss: 0.059926290065050125\n",
      "Epoch 5100/20000 Validation Loss: 0.061900265514850616\n",
      "Epoch 5101/20000 Training Loss: 0.04723617434501648\n",
      "Epoch 5102/20000 Training Loss: 0.04752372205257416\n",
      "Epoch 5103/20000 Training Loss: 0.07517696171998978\n",
      "Epoch 5104/20000 Training Loss: 0.0448155514895916\n",
      "Epoch 5105/20000 Training Loss: 0.07080379873514175\n",
      "Epoch 5106/20000 Training Loss: 0.07735059410333633\n",
      "Epoch 5107/20000 Training Loss: 0.0634802058339119\n",
      "Epoch 5108/20000 Training Loss: 0.0805669054389\n",
      "Epoch 5109/20000 Training Loss: 0.06023028492927551\n",
      "Epoch 5110/20000 Training Loss: 0.054382503032684326\n",
      "Epoch 5110/20000 Validation Loss: 0.055463265627622604\n",
      "Epoch 5111/20000 Training Loss: 0.06453181058168411\n",
      "Epoch 5112/20000 Training Loss: 0.056738805025815964\n",
      "Epoch 5113/20000 Training Loss: 0.0570082925260067\n",
      "Epoch 5114/20000 Training Loss: 0.058790866285562515\n",
      "Epoch 5115/20000 Training Loss: 0.048757147043943405\n",
      "Epoch 5116/20000 Training Loss: 0.08580774068832397\n",
      "Epoch 5117/20000 Training Loss: 0.0449291355907917\n",
      "Epoch 5118/20000 Training Loss: 0.0947456955909729\n",
      "Epoch 5119/20000 Training Loss: 0.053201962262392044\n",
      "Epoch 5120/20000 Training Loss: 0.0736423134803772\n",
      "Epoch 5120/20000 Validation Loss: 0.06023278087377548\n",
      "Epoch 5121/20000 Training Loss: 0.05214634910225868\n",
      "Epoch 5122/20000 Training Loss: 0.046009570360183716\n",
      "Epoch 5123/20000 Training Loss: 0.052947547286748886\n",
      "Epoch 5124/20000 Training Loss: 0.057981401681900024\n",
      "Epoch 5125/20000 Training Loss: 0.07407624274492264\n",
      "Epoch 5126/20000 Training Loss: 0.05017894506454468\n",
      "Epoch 5127/20000 Training Loss: 0.057065680623054504\n",
      "Epoch 5128/20000 Training Loss: 0.05930713936686516\n",
      "Epoch 5129/20000 Training Loss: 0.06204576417803764\n",
      "Epoch 5130/20000 Training Loss: 0.04827645793557167\n",
      "Epoch 5130/20000 Validation Loss: 0.10350451618432999\n",
      "Epoch 5131/20000 Training Loss: 0.04921095445752144\n",
      "Epoch 5132/20000 Training Loss: 0.04020148888230324\n",
      "Epoch 5133/20000 Training Loss: 0.07594282180070877\n",
      "Epoch 5134/20000 Training Loss: 0.04634818434715271\n",
      "Epoch 5135/20000 Training Loss: 0.06556487828493118\n",
      "Epoch 5136/20000 Training Loss: 0.06084340810775757\n",
      "Epoch 5137/20000 Training Loss: 0.07244259864091873\n",
      "Epoch 5138/20000 Training Loss: 0.04466593638062477\n",
      "Epoch 5139/20000 Training Loss: 0.05994096398353577\n",
      "Epoch 5140/20000 Training Loss: 0.06739691644906998\n",
      "Epoch 5140/20000 Validation Loss: 0.0753285214304924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5141/20000 Training Loss: 0.059862781316041946\n",
      "Epoch 5142/20000 Training Loss: 0.06151377782225609\n",
      "Epoch 5143/20000 Training Loss: 0.043106015771627426\n",
      "Epoch 5144/20000 Training Loss: 0.048920273780822754\n",
      "Epoch 5145/20000 Training Loss: 0.0789434015750885\n",
      "Epoch 5146/20000 Training Loss: 0.05980650708079338\n",
      "Epoch 5147/20000 Training Loss: 0.06447689980268478\n",
      "Epoch 5148/20000 Training Loss: 0.03975305333733559\n",
      "Epoch 5149/20000 Training Loss: 0.060019660741090775\n",
      "Epoch 5150/20000 Training Loss: 0.050769999623298645\n",
      "Epoch 5150/20000 Validation Loss: 0.0736405998468399\n",
      "Epoch 5151/20000 Training Loss: 0.04767170548439026\n",
      "Epoch 5152/20000 Training Loss: 0.047945570200681686\n",
      "Epoch 5153/20000 Training Loss: 0.05774839594960213\n",
      "Epoch 5154/20000 Training Loss: 0.0719975158572197\n",
      "Epoch 5155/20000 Training Loss: 0.0547858364880085\n",
      "Epoch 5156/20000 Training Loss: 0.05405517295002937\n",
      "Epoch 5157/20000 Training Loss: 0.061986058950424194\n",
      "Epoch 5158/20000 Training Loss: 0.054210156202316284\n",
      "Epoch 5159/20000 Training Loss: 0.07994983345270157\n",
      "Epoch 5160/20000 Training Loss: 0.053753167390823364\n",
      "Epoch 5160/20000 Validation Loss: 0.05033170059323311\n",
      "Epoch 5161/20000 Training Loss: 0.050912097096443176\n",
      "Epoch 5162/20000 Training Loss: 0.04889553785324097\n",
      "Epoch 5163/20000 Training Loss: 0.060301393270492554\n",
      "Epoch 5164/20000 Training Loss: 0.05287747457623482\n",
      "Epoch 5165/20000 Training Loss: 0.08110523223876953\n",
      "Epoch 5166/20000 Training Loss: 0.059772226959466934\n",
      "Epoch 5167/20000 Training Loss: 0.05307144299149513\n",
      "Epoch 5168/20000 Training Loss: 0.06427699327468872\n",
      "Epoch 5169/20000 Training Loss: 0.06391268223524094\n",
      "Epoch 5170/20000 Training Loss: 0.06081800162792206\n",
      "Epoch 5170/20000 Validation Loss: 0.06892673671245575\n",
      "Epoch 5171/20000 Training Loss: 0.07770708948373795\n",
      "Epoch 5172/20000 Training Loss: 0.07378976792097092\n",
      "Epoch 5173/20000 Training Loss: 0.06969387084245682\n",
      "Epoch 5174/20000 Training Loss: 0.046242404729127884\n",
      "Epoch 5175/20000 Training Loss: 0.06308407336473465\n",
      "Epoch 5176/20000 Training Loss: 0.043806303292512894\n",
      "Epoch 5177/20000 Training Loss: 0.06209694221615791\n",
      "Epoch 5178/20000 Training Loss: 0.07375917583703995\n",
      "Epoch 5179/20000 Training Loss: 0.08354447036981583\n",
      "Epoch 5180/20000 Training Loss: 0.07376018166542053\n",
      "Epoch 5180/20000 Validation Loss: 0.043008722364902496\n",
      "Epoch 5181/20000 Training Loss: 0.06376682221889496\n",
      "Epoch 5182/20000 Training Loss: 0.06394609063863754\n",
      "Epoch 5183/20000 Training Loss: 0.05251048505306244\n",
      "Epoch 5184/20000 Training Loss: 0.051602333784103394\n",
      "Epoch 5185/20000 Training Loss: 0.05371236801147461\n",
      "Epoch 5186/20000 Training Loss: 0.07448291033506393\n",
      "Epoch 5187/20000 Training Loss: 0.042939361184835434\n",
      "Epoch 5188/20000 Training Loss: 0.058837905526161194\n",
      "Epoch 5189/20000 Training Loss: 0.07144083827733994\n",
      "Epoch 5190/20000 Training Loss: 0.05710075423121452\n",
      "Epoch 5190/20000 Validation Loss: 0.03686856850981712\n",
      "Epoch 5191/20000 Training Loss: 0.051105380058288574\n",
      "Epoch 5192/20000 Training Loss: 0.05152257904410362\n",
      "Epoch 5193/20000 Training Loss: 0.0538271963596344\n",
      "Epoch 5194/20000 Training Loss: 0.04090866819024086\n",
      "Epoch 5195/20000 Training Loss: 0.06228609010577202\n",
      "Epoch 5196/20000 Training Loss: 0.05035649612545967\n",
      "Epoch 5197/20000 Training Loss: 0.06015928089618683\n",
      "Epoch 5198/20000 Training Loss: 0.057699620723724365\n",
      "Epoch 5199/20000 Training Loss: 0.05085330083966255\n",
      "Epoch 5200/20000 Training Loss: 0.06668011099100113\n",
      "Epoch 5200/20000 Validation Loss: 0.07055264711380005\n",
      "Epoch 5201/20000 Training Loss: 0.0500967912375927\n",
      "Epoch 5202/20000 Training Loss: 0.049561262130737305\n",
      "Epoch 5203/20000 Training Loss: 0.06212972104549408\n",
      "Epoch 5204/20000 Training Loss: 0.07379886507987976\n",
      "Epoch 5205/20000 Training Loss: 0.06082351878285408\n",
      "Epoch 5206/20000 Training Loss: 0.04992927238345146\n",
      "Epoch 5207/20000 Training Loss: 0.05436229705810547\n",
      "Epoch 5208/20000 Training Loss: 0.0547025240957737\n",
      "Epoch 5209/20000 Training Loss: 0.05799908563494682\n",
      "Epoch 5210/20000 Training Loss: 0.05380718410015106\n",
      "Epoch 5210/20000 Validation Loss: 0.04872455447912216\n",
      "Epoch 5211/20000 Training Loss: 0.04820563271641731\n",
      "Epoch 5212/20000 Training Loss: 0.051474858075380325\n",
      "Epoch 5213/20000 Training Loss: 0.06739798188209534\n",
      "Epoch 5214/20000 Training Loss: 0.05081182345747948\n",
      "Epoch 5215/20000 Training Loss: 0.04792802035808563\n",
      "Epoch 5216/20000 Training Loss: 0.06835377961397171\n",
      "Epoch 5217/20000 Training Loss: 0.05321748927235603\n",
      "Epoch 5218/20000 Training Loss: 0.053626980632543564\n",
      "Epoch 5219/20000 Training Loss: 0.0626099482178688\n",
      "Epoch 5220/20000 Training Loss: 0.05424605682492256\n",
      "Epoch 5220/20000 Validation Loss: 0.07693256437778473\n",
      "Epoch 5221/20000 Training Loss: 0.05015125498175621\n",
      "Epoch 5222/20000 Training Loss: 0.051190104335546494\n",
      "Epoch 5223/20000 Training Loss: 0.06223002076148987\n",
      "Epoch 5224/20000 Training Loss: 0.08139675110578537\n",
      "Epoch 5225/20000 Training Loss: 0.05761962756514549\n",
      "Epoch 5226/20000 Training Loss: 0.054355207830667496\n",
      "Epoch 5227/20000 Training Loss: 0.05778634920716286\n",
      "Epoch 5228/20000 Training Loss: 0.05197061225771904\n",
      "Epoch 5229/20000 Training Loss: 0.04739893600344658\n",
      "Epoch 5230/20000 Training Loss: 0.05042651295661926\n",
      "Epoch 5230/20000 Validation Loss: 0.04523688927292824\n",
      "Epoch 5231/20000 Training Loss: 0.06796076893806458\n",
      "Epoch 5232/20000 Training Loss: 0.05064086988568306\n",
      "Epoch 5233/20000 Training Loss: 0.047792721539735794\n",
      "Epoch 5234/20000 Training Loss: 0.06352252513170242\n",
      "Epoch 5235/20000 Training Loss: 0.05709370970726013\n",
      "Epoch 5236/20000 Training Loss: 0.06023876741528511\n",
      "Epoch 5237/20000 Training Loss: 0.04191714525222778\n",
      "Epoch 5238/20000 Training Loss: 0.052848946303129196\n",
      "Epoch 5239/20000 Training Loss: 0.054896291345357895\n",
      "Epoch 5240/20000 Training Loss: 0.046007853001356125\n",
      "Epoch 5240/20000 Validation Loss: 0.07724910974502563\n",
      "Epoch 5241/20000 Training Loss: 0.05754925683140755\n",
      "Epoch 5242/20000 Training Loss: 0.06953137367963791\n",
      "Epoch 5243/20000 Training Loss: 0.05646449327468872\n",
      "Epoch 5244/20000 Training Loss: 0.05384610965847969\n",
      "Epoch 5245/20000 Training Loss: 0.058084744960069656\n",
      "Epoch 5246/20000 Training Loss: 0.0584324486553669\n",
      "Epoch 5247/20000 Training Loss: 0.06795705109834671\n",
      "Epoch 5248/20000 Training Loss: 0.05076238140463829\n",
      "Epoch 5249/20000 Training Loss: 0.06197469308972359\n",
      "Epoch 5250/20000 Training Loss: 0.04833457991480827\n",
      "Epoch 5250/20000 Validation Loss: 0.05655651539564133\n",
      "Epoch 5251/20000 Training Loss: 0.053555939346551895\n",
      "Epoch 5252/20000 Training Loss: 0.06714845448732376\n",
      "Epoch 5253/20000 Training Loss: 0.04808573052287102\n",
      "Epoch 5254/20000 Training Loss: 0.07604098320007324\n",
      "Epoch 5255/20000 Training Loss: 0.06264642626047134\n",
      "Epoch 5256/20000 Training Loss: 0.05446697771549225\n",
      "Epoch 5257/20000 Training Loss: 0.07074267417192459\n",
      "Epoch 5258/20000 Training Loss: 0.07402119040489197\n",
      "Epoch 5259/20000 Training Loss: 0.043910399079322815\n",
      "Epoch 5260/20000 Training Loss: 0.06583750993013382\n",
      "Epoch 5260/20000 Validation Loss: 0.046095289289951324\n",
      "Epoch 5261/20000 Training Loss: 0.0504642091691494\n",
      "Epoch 5262/20000 Training Loss: 0.06420514732599258\n",
      "Epoch 5263/20000 Training Loss: 0.0744076669216156\n",
      "Epoch 5264/20000 Training Loss: 0.0559992678463459\n",
      "Epoch 5265/20000 Training Loss: 0.05662292242050171\n",
      "Epoch 5266/20000 Training Loss: 0.06757768243551254\n",
      "Epoch 5267/20000 Training Loss: 0.057335421442985535\n",
      "Epoch 5268/20000 Training Loss: 0.06706077605485916\n",
      "Epoch 5269/20000 Training Loss: 0.06261704117059708\n",
      "Epoch 5270/20000 Training Loss: 0.04978330060839653\n",
      "Epoch 5270/20000 Validation Loss: 0.04876801744103432\n",
      "Epoch 5271/20000 Training Loss: 0.05859282985329628\n",
      "Epoch 5272/20000 Training Loss: 0.06961497664451599\n",
      "Epoch 5273/20000 Training Loss: 0.04727993533015251\n",
      "Epoch 5274/20000 Training Loss: 0.054337773472070694\n",
      "Epoch 5275/20000 Training Loss: 0.04870728775858879\n",
      "Epoch 5276/20000 Training Loss: 0.05280594155192375\n",
      "Epoch 5277/20000 Training Loss: 0.054235588759183884\n",
      "Epoch 5278/20000 Training Loss: 0.06655120849609375\n",
      "Epoch 5279/20000 Training Loss: 0.06799034774303436\n",
      "Epoch 5280/20000 Training Loss: 0.057633668184280396\n",
      "Epoch 5280/20000 Validation Loss: 0.07498332113027573\n",
      "Epoch 5281/20000 Training Loss: 0.05524780973792076\n",
      "Epoch 5282/20000 Training Loss: 0.0428876094520092\n",
      "Epoch 5283/20000 Training Loss: 0.08077501505613327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5284/20000 Training Loss: 0.07483550906181335\n",
      "Epoch 5285/20000 Training Loss: 0.05962385609745979\n",
      "Epoch 5286/20000 Training Loss: 0.07496833056211472\n",
      "Epoch 5287/20000 Training Loss: 0.06202635169029236\n",
      "Epoch 5288/20000 Training Loss: 0.06752393394708633\n",
      "Epoch 5289/20000 Training Loss: 0.0532977469265461\n",
      "Epoch 5290/20000 Training Loss: 0.06675421446561813\n",
      "Epoch 5290/20000 Validation Loss: 0.061222098767757416\n",
      "Epoch 5291/20000 Training Loss: 0.06424884498119354\n",
      "Epoch 5292/20000 Training Loss: 0.05557620897889137\n",
      "Epoch 5293/20000 Training Loss: 0.04986591637134552\n",
      "Epoch 5294/20000 Training Loss: 0.04851916804909706\n",
      "Epoch 5295/20000 Training Loss: 0.058544356375932693\n",
      "Epoch 5296/20000 Training Loss: 0.07858217507600784\n",
      "Epoch 5297/20000 Training Loss: 0.06471310555934906\n",
      "Epoch 5298/20000 Training Loss: 0.08205495029687881\n",
      "Epoch 5299/20000 Training Loss: 0.055105309933423996\n",
      "Epoch 5300/20000 Training Loss: 0.06931663304567337\n",
      "Epoch 5300/20000 Validation Loss: 0.08755709230899811\n",
      "Epoch 5301/20000 Training Loss: 0.06330094486474991\n",
      "Epoch 5302/20000 Training Loss: 0.050494443625211716\n",
      "Epoch 5303/20000 Training Loss: 0.08266342431306839\n",
      "Epoch 5304/20000 Training Loss: 0.06452301889657974\n",
      "Epoch 5305/20000 Training Loss: 0.06403600424528122\n",
      "Epoch 5306/20000 Training Loss: 0.07932471483945847\n",
      "Epoch 5307/20000 Training Loss: 0.05133684352040291\n",
      "Epoch 5308/20000 Training Loss: 0.05324568971991539\n",
      "Epoch 5309/20000 Training Loss: 0.0637456551194191\n",
      "Epoch 5310/20000 Training Loss: 0.0647210106253624\n",
      "Epoch 5310/20000 Validation Loss: 0.051023513078689575\n",
      "Epoch 5311/20000 Training Loss: 0.05784013494849205\n",
      "Epoch 5312/20000 Training Loss: 0.06336686760187149\n",
      "Epoch 5313/20000 Training Loss: 0.07836694270372391\n",
      "Epoch 5314/20000 Training Loss: 0.05911214277148247\n",
      "Epoch 5315/20000 Training Loss: 0.04702090099453926\n",
      "Epoch 5316/20000 Training Loss: 0.04909078776836395\n",
      "Epoch 5317/20000 Training Loss: 0.06499025225639343\n",
      "Epoch 5318/20000 Training Loss: 0.0715271458029747\n",
      "Epoch 5319/20000 Training Loss: 0.03728955611586571\n",
      "Epoch 5320/20000 Training Loss: 0.0528700090944767\n",
      "Epoch 5320/20000 Validation Loss: 0.07423505187034607\n",
      "Epoch 5321/20000 Training Loss: 0.06227339804172516\n",
      "Epoch 5322/20000 Training Loss: 0.05721015855669975\n",
      "Epoch 5323/20000 Training Loss: 0.06336348503828049\n",
      "Epoch 5324/20000 Training Loss: 0.06470111757516861\n",
      "Epoch 5325/20000 Training Loss: 0.06807630509138107\n",
      "Epoch 5326/20000 Training Loss: 0.05674493685364723\n",
      "Epoch 5327/20000 Training Loss: 0.05573772266507149\n",
      "Epoch 5328/20000 Training Loss: 0.05367973819375038\n",
      "Epoch 5329/20000 Training Loss: 0.06813786923885345\n",
      "Epoch 5330/20000 Training Loss: 0.05517635866999626\n",
      "Epoch 5330/20000 Validation Loss: 0.05814633145928383\n",
      "Epoch 5331/20000 Training Loss: 0.05725109577178955\n",
      "Epoch 5332/20000 Training Loss: 0.047135744243860245\n",
      "Epoch 5333/20000 Training Loss: 0.04698505625128746\n",
      "Epoch 5334/20000 Training Loss: 0.05001360550522804\n",
      "Epoch 5335/20000 Training Loss: 0.05548208951950073\n",
      "Epoch 5336/20000 Training Loss: 0.04874693229794502\n",
      "Epoch 5337/20000 Training Loss: 0.059974897652864456\n",
      "Epoch 5338/20000 Training Loss: 0.05623845383524895\n",
      "Epoch 5339/20000 Training Loss: 0.06711086630821228\n",
      "Epoch 5340/20000 Training Loss: 0.06584347784519196\n",
      "Epoch 5340/20000 Validation Loss: 0.08510337769985199\n",
      "Epoch 5341/20000 Training Loss: 0.07140537351369858\n",
      "Epoch 5342/20000 Training Loss: 0.045269351452589035\n",
      "Epoch 5343/20000 Training Loss: 0.06391415745019913\n",
      "Epoch 5344/20000 Training Loss: 0.057117994874715805\n",
      "Epoch 5345/20000 Training Loss: 0.044542986899614334\n",
      "Epoch 5346/20000 Training Loss: 0.06306280940771103\n",
      "Epoch 5347/20000 Training Loss: 0.08428675681352615\n",
      "Epoch 5348/20000 Training Loss: 0.044910978525877\n",
      "Epoch 5349/20000 Training Loss: 0.07109586149454117\n",
      "Epoch 5350/20000 Training Loss: 0.05815589427947998\n",
      "Epoch 5350/20000 Validation Loss: 0.05085627734661102\n",
      "Epoch 5351/20000 Training Loss: 0.057267963886260986\n",
      "Epoch 5352/20000 Training Loss: 0.05297846719622612\n",
      "Epoch 5353/20000 Training Loss: 0.04807572439312935\n",
      "Epoch 5354/20000 Training Loss: 0.05980002507567406\n",
      "Epoch 5355/20000 Training Loss: 0.06197114661335945\n",
      "Epoch 5356/20000 Training Loss: 0.055471718311309814\n",
      "Epoch 5357/20000 Training Loss: 0.06080379709601402\n",
      "Epoch 5358/20000 Training Loss: 0.04486203193664551\n",
      "Epoch 5359/20000 Training Loss: 0.05185197666287422\n",
      "Epoch 5360/20000 Training Loss: 0.06354482471942902\n",
      "Epoch 5360/20000 Validation Loss: 0.04785585403442383\n",
      "Epoch 5361/20000 Training Loss: 0.06381168961524963\n",
      "Epoch 5362/20000 Training Loss: 0.0581529438495636\n",
      "Epoch 5363/20000 Training Loss: 0.042325884103775024\n",
      "Epoch 5364/20000 Training Loss: 0.05518927797675133\n",
      "Epoch 5365/20000 Training Loss: 0.05579974129796028\n",
      "Epoch 5366/20000 Training Loss: 0.06786513328552246\n",
      "Epoch 5367/20000 Training Loss: 0.06668654084205627\n",
      "Epoch 5368/20000 Training Loss: 0.05318867787718773\n",
      "Epoch 5369/20000 Training Loss: 0.05614851787686348\n",
      "Epoch 5370/20000 Training Loss: 0.03691266477108002\n",
      "Epoch 5370/20000 Validation Loss: 0.07372652739286423\n",
      "Epoch 5371/20000 Training Loss: 0.05153043940663338\n",
      "Epoch 5372/20000 Training Loss: 0.058744385838508606\n",
      "Epoch 5373/20000 Training Loss: 0.054845601320266724\n",
      "Epoch 5374/20000 Training Loss: 0.05142679437994957\n",
      "Epoch 5375/20000 Training Loss: 0.06342560797929764\n",
      "Epoch 5376/20000 Training Loss: 0.06558474898338318\n",
      "Epoch 5377/20000 Training Loss: 0.05420779809355736\n",
      "Epoch 5378/20000 Training Loss: 0.06521714478731155\n",
      "Epoch 5379/20000 Training Loss: 0.06752089411020279\n",
      "Epoch 5380/20000 Training Loss: 0.05363112688064575\n",
      "Epoch 5380/20000 Validation Loss: 0.04626356065273285\n",
      "Epoch 5381/20000 Training Loss: 0.06103038787841797\n",
      "Epoch 5382/20000 Training Loss: 0.04168231412768364\n",
      "Epoch 5383/20000 Training Loss: 0.06468844413757324\n",
      "Epoch 5384/20000 Training Loss: 0.04458605870604515\n",
      "Epoch 5385/20000 Training Loss: 0.05839871987700462\n",
      "Epoch 5386/20000 Training Loss: 0.068263940513134\n",
      "Epoch 5387/20000 Training Loss: 0.06459254026412964\n",
      "Epoch 5388/20000 Training Loss: 0.07872932404279709\n",
      "Epoch 5389/20000 Training Loss: 0.04896358773112297\n",
      "Epoch 5390/20000 Training Loss: 0.05717819929122925\n",
      "Epoch 5390/20000 Validation Loss: 0.058623336255550385\n",
      "Epoch 5391/20000 Training Loss: 0.07124733924865723\n",
      "Epoch 5392/20000 Training Loss: 0.039631735533475876\n",
      "Epoch 5393/20000 Training Loss: 0.06687981635332108\n",
      "Epoch 5394/20000 Training Loss: 0.056737180799245834\n",
      "Epoch 5395/20000 Training Loss: 0.04342887923121452\n",
      "Epoch 5396/20000 Training Loss: 0.06637921929359436\n",
      "Epoch 5397/20000 Training Loss: 0.053406208753585815\n",
      "Epoch 5398/20000 Training Loss: 0.056521158665418625\n",
      "Epoch 5399/20000 Training Loss: 0.0588848702609539\n",
      "Epoch 5400/20000 Training Loss: 0.041333507746458054\n",
      "Epoch 5400/20000 Validation Loss: 0.050305068492889404\n",
      "Epoch 5401/20000 Training Loss: 0.07343507558107376\n",
      "Epoch 5402/20000 Training Loss: 0.06248541548848152\n",
      "Epoch 5403/20000 Training Loss: 0.06342203170061111\n",
      "Epoch 5404/20000 Training Loss: 0.05537654086947441\n",
      "Epoch 5405/20000 Training Loss: 0.05208833888173103\n",
      "Epoch 5406/20000 Training Loss: 0.04709300771355629\n",
      "Epoch 5407/20000 Training Loss: 0.0772629901766777\n",
      "Epoch 5408/20000 Training Loss: 0.05606245994567871\n",
      "Epoch 5409/20000 Training Loss: 0.05034138634800911\n",
      "Epoch 5410/20000 Training Loss: 0.047718968242406845\n",
      "Epoch 5410/20000 Validation Loss: 0.052561938762664795\n",
      "Epoch 5411/20000 Training Loss: 0.05892832949757576\n",
      "Epoch 5412/20000 Training Loss: 0.039116743952035904\n",
      "Epoch 5413/20000 Training Loss: 0.041272807866334915\n",
      "Epoch 5414/20000 Training Loss: 0.05671263858675957\n",
      "Epoch 5415/20000 Training Loss: 0.07396238297224045\n",
      "Epoch 5416/20000 Training Loss: 0.04857657477259636\n",
      "Epoch 5417/20000 Training Loss: 0.07053165882825851\n",
      "Epoch 5418/20000 Training Loss: 0.07246462255716324\n",
      "Epoch 5419/20000 Training Loss: 0.06780105084180832\n",
      "Epoch 5420/20000 Training Loss: 0.08348398655653\n",
      "Epoch 5420/20000 Validation Loss: 0.06766132265329361\n",
      "Epoch 5421/20000 Training Loss: 0.054386842995882034\n",
      "Epoch 5422/20000 Training Loss: 0.05483129248023033\n",
      "Epoch 5423/20000 Training Loss: 0.06789736449718475\n",
      "Epoch 5424/20000 Training Loss: 0.05415419861674309\n",
      "Epoch 5425/20000 Training Loss: 0.0450122095644474\n",
      "Epoch 5426/20000 Training Loss: 0.05799204111099243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5427/20000 Training Loss: 0.05541430413722992\n",
      "Epoch 5428/20000 Training Loss: 0.06283799558877945\n",
      "Epoch 5429/20000 Training Loss: 0.056141745299100876\n",
      "Epoch 5430/20000 Training Loss: 0.049235206097364426\n",
      "Epoch 5430/20000 Validation Loss: 0.05672382563352585\n",
      "Epoch 5431/20000 Training Loss: 0.06902103871107101\n",
      "Epoch 5432/20000 Training Loss: 0.06422717124223709\n",
      "Epoch 5433/20000 Training Loss: 0.04508666694164276\n",
      "Epoch 5434/20000 Training Loss: 0.06076975166797638\n",
      "Epoch 5435/20000 Training Loss: 0.04294539615511894\n",
      "Epoch 5436/20000 Training Loss: 0.053020674735307693\n",
      "Epoch 5437/20000 Training Loss: 0.04091961681842804\n",
      "Epoch 5438/20000 Training Loss: 0.05221845209598541\n",
      "Epoch 5439/20000 Training Loss: 0.05515473708510399\n",
      "Epoch 5440/20000 Training Loss: 0.052816033363342285\n",
      "Epoch 5440/20000 Validation Loss: 0.06339909136295319\n",
      "Epoch 5441/20000 Training Loss: 0.05610498785972595\n",
      "Epoch 5442/20000 Training Loss: 0.05539461970329285\n",
      "Epoch 5443/20000 Training Loss: 0.07469058036804199\n",
      "Epoch 5444/20000 Training Loss: 0.0642433688044548\n",
      "Epoch 5445/20000 Training Loss: 0.060414254665374756\n",
      "Epoch 5446/20000 Training Loss: 0.05937016382813454\n",
      "Epoch 5447/20000 Training Loss: 0.056877221912145615\n",
      "Epoch 5448/20000 Training Loss: 0.047126248478889465\n",
      "Epoch 5449/20000 Training Loss: 0.048741698265075684\n",
      "Epoch 5450/20000 Training Loss: 0.05773839354515076\n",
      "Epoch 5450/20000 Validation Loss: 0.060372982174158096\n",
      "Epoch 5451/20000 Training Loss: 0.07268358021974564\n",
      "Epoch 5452/20000 Training Loss: 0.04782557487487793\n",
      "Epoch 5453/20000 Training Loss: 0.06546539813280106\n",
      "Epoch 5454/20000 Training Loss: 0.06580004096031189\n",
      "Epoch 5455/20000 Training Loss: 0.07386385649442673\n",
      "Epoch 5456/20000 Training Loss: 0.06238602101802826\n",
      "Epoch 5457/20000 Training Loss: 0.06953687220811844\n",
      "Epoch 5458/20000 Training Loss: 0.05262523517012596\n",
      "Epoch 5459/20000 Training Loss: 0.05416169390082359\n",
      "Epoch 5460/20000 Training Loss: 0.0552556999027729\n",
      "Epoch 5460/20000 Validation Loss: 0.06427031010389328\n",
      "Epoch 5461/20000 Training Loss: 0.06933147460222244\n",
      "Epoch 5462/20000 Training Loss: 0.06254769116640091\n",
      "Epoch 5463/20000 Training Loss: 0.04513635113835335\n",
      "Epoch 5464/20000 Training Loss: 0.04635937884449959\n",
      "Epoch 5465/20000 Training Loss: 0.05065108463168144\n",
      "Epoch 5466/20000 Training Loss: 0.07348806411027908\n",
      "Epoch 5467/20000 Training Loss: 0.07161029428243637\n",
      "Epoch 5468/20000 Training Loss: 0.04862753674387932\n",
      "Epoch 5469/20000 Training Loss: 0.06036313250660896\n",
      "Epoch 5470/20000 Training Loss: 0.07083878666162491\n",
      "Epoch 5470/20000 Validation Loss: 0.07138170301914215\n",
      "Epoch 5471/20000 Training Loss: 0.06740681082010269\n",
      "Epoch 5472/20000 Training Loss: 0.06964844465255737\n",
      "Epoch 5473/20000 Training Loss: 0.054614853113889694\n",
      "Epoch 5474/20000 Training Loss: 0.0599077008664608\n",
      "Epoch 5475/20000 Training Loss: 0.053192753344774246\n",
      "Epoch 5476/20000 Training Loss: 0.053124427795410156\n",
      "Epoch 5477/20000 Training Loss: 0.05031685531139374\n",
      "Epoch 5478/20000 Training Loss: 0.06709928065538406\n",
      "Epoch 5479/20000 Training Loss: 0.06524496525526047\n",
      "Epoch 5480/20000 Training Loss: 0.06817270815372467\n",
      "Epoch 5480/20000 Validation Loss: 0.0848592221736908\n",
      "Epoch 5481/20000 Training Loss: 0.06764981895685196\n",
      "Epoch 5482/20000 Training Loss: 0.060746852308511734\n",
      "Epoch 5483/20000 Training Loss: 0.0812607929110527\n",
      "Epoch 5484/20000 Training Loss: 0.07711857557296753\n",
      "Epoch 5485/20000 Training Loss: 0.06977949291467667\n",
      "Epoch 5486/20000 Training Loss: 0.049040064215660095\n",
      "Epoch 5487/20000 Training Loss: 0.0644163116812706\n",
      "Epoch 5488/20000 Training Loss: 0.06233135238289833\n",
      "Epoch 5489/20000 Training Loss: 0.058265361934900284\n",
      "Epoch 5490/20000 Training Loss: 0.07037527859210968\n",
      "Epoch 5490/20000 Validation Loss: 0.09163807332515717\n",
      "Epoch 5491/20000 Training Loss: 0.06626661866903305\n",
      "Epoch 5492/20000 Training Loss: 0.05492709204554558\n",
      "Epoch 5493/20000 Training Loss: 0.05674752965569496\n",
      "Epoch 5494/20000 Training Loss: 0.0579674057662487\n",
      "Epoch 5495/20000 Training Loss: 0.07249090075492859\n",
      "Epoch 5496/20000 Training Loss: 0.042219653725624084\n",
      "Epoch 5497/20000 Training Loss: 0.06348040699958801\n",
      "Epoch 5498/20000 Training Loss: 0.06037552282214165\n",
      "Epoch 5499/20000 Training Loss: 0.06438755244016647\n",
      "Epoch 5500/20000 Training Loss: 0.052325207740068436\n",
      "Epoch 5500/20000 Validation Loss: 0.10208967328071594\n",
      "Epoch 5501/20000 Training Loss: 0.07379627227783203\n",
      "Epoch 5502/20000 Training Loss: 0.051908884197473526\n",
      "Epoch 5503/20000 Training Loss: 0.042236391454935074\n",
      "Epoch 5504/20000 Training Loss: 0.053470369428396225\n",
      "Epoch 5505/20000 Training Loss: 0.07051264494657516\n",
      "Epoch 5506/20000 Training Loss: 0.057580653578042984\n",
      "Epoch 5507/20000 Training Loss: 0.07314424961805344\n",
      "Epoch 5508/20000 Training Loss: 0.04887774586677551\n",
      "Epoch 5509/20000 Training Loss: 0.05605943873524666\n",
      "Epoch 5510/20000 Training Loss: 0.05235743150115013\n",
      "Epoch 5510/20000 Validation Loss: 0.05016531050205231\n",
      "Epoch 5511/20000 Training Loss: 0.06537284702062607\n",
      "Epoch 5512/20000 Training Loss: 0.07517658919095993\n",
      "Epoch 5513/20000 Training Loss: 0.04716278240084648\n",
      "Epoch 5514/20000 Training Loss: 0.05369621142745018\n",
      "Epoch 5515/20000 Training Loss: 0.05858157202601433\n",
      "Epoch 5516/20000 Training Loss: 0.062091927975416183\n",
      "Epoch 5517/20000 Training Loss: 0.050772394984960556\n",
      "Epoch 5518/20000 Training Loss: 0.04545716568827629\n",
      "Epoch 5519/20000 Training Loss: 0.06988335400819778\n",
      "Epoch 5520/20000 Training Loss: 0.05259356275200844\n",
      "Epoch 5520/20000 Validation Loss: 0.06703297048807144\n",
      "Epoch 5521/20000 Training Loss: 0.051981121301651\n",
      "Epoch 5522/20000 Training Loss: 0.05908381938934326\n",
      "Epoch 5523/20000 Training Loss: 0.06142403557896614\n",
      "Epoch 5524/20000 Training Loss: 0.04737105593085289\n",
      "Epoch 5525/20000 Training Loss: 0.05002740025520325\n",
      "Epoch 5526/20000 Training Loss: 0.05953395366668701\n",
      "Epoch 5527/20000 Training Loss: 0.06493858993053436\n",
      "Epoch 5528/20000 Training Loss: 0.08074074983596802\n",
      "Epoch 5529/20000 Training Loss: 0.052736710757017136\n",
      "Epoch 5530/20000 Training Loss: 0.06132207438349724\n",
      "Epoch 5530/20000 Validation Loss: 0.06707446277141571\n",
      "Epoch 5531/20000 Training Loss: 0.042420949786901474\n",
      "Epoch 5532/20000 Training Loss: 0.04139680787920952\n",
      "Epoch 5533/20000 Training Loss: 0.0645650327205658\n",
      "Epoch 5534/20000 Training Loss: 0.09768521785736084\n",
      "Epoch 5535/20000 Training Loss: 0.05203370749950409\n",
      "Epoch 5536/20000 Training Loss: 0.04950467869639397\n",
      "Epoch 5537/20000 Training Loss: 0.05018347129225731\n",
      "Epoch 5538/20000 Training Loss: 0.04397183656692505\n",
      "Epoch 5539/20000 Training Loss: 0.0705471932888031\n",
      "Epoch 5540/20000 Training Loss: 0.0773298442363739\n",
      "Epoch 5540/20000 Validation Loss: 0.04893171042203903\n",
      "Epoch 5541/20000 Training Loss: 0.060932058840990067\n",
      "Epoch 5542/20000 Training Loss: 0.07104430347681046\n",
      "Epoch 5543/20000 Training Loss: 0.06122418865561485\n",
      "Epoch 5544/20000 Training Loss: 0.04863707348704338\n",
      "Epoch 5545/20000 Training Loss: 0.06069377437233925\n",
      "Epoch 5546/20000 Training Loss: 0.06406295299530029\n",
      "Epoch 5547/20000 Training Loss: 0.034741878509521484\n",
      "Epoch 5548/20000 Training Loss: 0.042992498725652695\n",
      "Epoch 5549/20000 Training Loss: 0.06052282080054283\n",
      "Epoch 5550/20000 Training Loss: 0.05155956745147705\n",
      "Epoch 5550/20000 Validation Loss: 0.043122075498104095\n",
      "Epoch 5551/20000 Training Loss: 0.06420520693063736\n",
      "Epoch 5552/20000 Training Loss: 0.06606397032737732\n",
      "Epoch 5553/20000 Training Loss: 0.06100599095225334\n",
      "Epoch 5554/20000 Training Loss: 0.05454258993268013\n",
      "Epoch 5555/20000 Training Loss: 0.05614878609776497\n",
      "Epoch 5556/20000 Training Loss: 0.047484900802373886\n",
      "Epoch 5557/20000 Training Loss: 0.06939277797937393\n",
      "Epoch 5558/20000 Training Loss: 0.06041225418448448\n",
      "Epoch 5559/20000 Training Loss: 0.049723487347364426\n",
      "Epoch 5560/20000 Training Loss: 0.05069113150238991\n",
      "Epoch 5560/20000 Validation Loss: 0.04846756160259247\n",
      "Epoch 5561/20000 Training Loss: 0.05218573287129402\n",
      "Epoch 5562/20000 Training Loss: 0.05168136954307556\n",
      "Epoch 5563/20000 Training Loss: 0.05299932137131691\n",
      "Epoch 5564/20000 Training Loss: 0.045336898416280746\n",
      "Epoch 5565/20000 Training Loss: 0.055637236684560776\n",
      "Epoch 5566/20000 Training Loss: 0.05467817559838295\n",
      "Epoch 5567/20000 Training Loss: 0.061029721051454544\n",
      "Epoch 5568/20000 Training Loss: 0.06296098977327347\n",
      "Epoch 5569/20000 Training Loss: 0.04659890756011009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5570/20000 Training Loss: 0.05146758630871773\n",
      "Epoch 5570/20000 Validation Loss: 0.05304193124175072\n",
      "Epoch 5571/20000 Training Loss: 0.06304007768630981\n",
      "Epoch 5572/20000 Training Loss: 0.04794956371188164\n",
      "Epoch 5573/20000 Training Loss: 0.05991046503186226\n",
      "Epoch 5574/20000 Training Loss: 0.08548641204833984\n",
      "Epoch 5575/20000 Training Loss: 0.047435928136110306\n",
      "Epoch 5576/20000 Training Loss: 0.05446814373135567\n",
      "Epoch 5577/20000 Training Loss: 0.05003626272082329\n",
      "Epoch 5578/20000 Training Loss: 0.07655399292707443\n",
      "Epoch 5579/20000 Training Loss: 0.06619992107152939\n",
      "Epoch 5580/20000 Training Loss: 0.06476260721683502\n",
      "Epoch 5580/20000 Validation Loss: 0.06804957240819931\n",
      "Epoch 5581/20000 Training Loss: 0.0651794821023941\n",
      "Epoch 5582/20000 Training Loss: 0.05240932106971741\n",
      "Epoch 5583/20000 Training Loss: 0.06766266375780106\n",
      "Epoch 5584/20000 Training Loss: 0.04606744647026062\n",
      "Epoch 5585/20000 Training Loss: 0.052313219755887985\n",
      "Epoch 5586/20000 Training Loss: 0.04693891108036041\n",
      "Epoch 5587/20000 Training Loss: 0.0732727125287056\n",
      "Epoch 5588/20000 Training Loss: 0.05782516300678253\n",
      "Epoch 5589/20000 Training Loss: 0.05042742192745209\n",
      "Epoch 5590/20000 Training Loss: 0.06182442978024483\n",
      "Epoch 5590/20000 Validation Loss: 0.05898510292172432\n",
      "Epoch 5591/20000 Training Loss: 0.06312621384859085\n",
      "Epoch 5592/20000 Training Loss: 0.09539571404457092\n",
      "Epoch 5593/20000 Training Loss: 0.0602608323097229\n",
      "Epoch 5594/20000 Training Loss: 0.07354568690061569\n",
      "Epoch 5595/20000 Training Loss: 0.052133966237306595\n",
      "Epoch 5596/20000 Training Loss: 0.07057385891675949\n",
      "Epoch 5597/20000 Training Loss: 0.07300498336553574\n",
      "Epoch 5598/20000 Training Loss: 0.050749074667692184\n",
      "Epoch 5599/20000 Training Loss: 0.05006087198853493\n",
      "Epoch 5600/20000 Training Loss: 0.05322791263461113\n",
      "Epoch 5600/20000 Validation Loss: 0.04672554135322571\n",
      "Epoch 5601/20000 Training Loss: 0.07562080025672913\n",
      "Epoch 5602/20000 Training Loss: 0.0649247094988823\n",
      "Epoch 5603/20000 Training Loss: 0.05717259645462036\n",
      "Epoch 5604/20000 Training Loss: 0.06491881608963013\n",
      "Epoch 5605/20000 Training Loss: 0.06123485043644905\n",
      "Epoch 5606/20000 Training Loss: 0.05163847282528877\n",
      "Epoch 5607/20000 Training Loss: 0.053035516291856766\n",
      "Epoch 5608/20000 Training Loss: 0.057432759553194046\n",
      "Epoch 5609/20000 Training Loss: 0.09932183474302292\n",
      "Epoch 5610/20000 Training Loss: 0.045370373874902725\n",
      "Epoch 5610/20000 Validation Loss: 0.06888920813798904\n",
      "Epoch 5611/20000 Training Loss: 0.044675733894109726\n",
      "Epoch 5612/20000 Training Loss: 0.06719949096441269\n",
      "Epoch 5613/20000 Training Loss: 0.06570380181074142\n",
      "Epoch 5614/20000 Training Loss: 0.059719037264585495\n",
      "Epoch 5615/20000 Training Loss: 0.04385307431221008\n",
      "Epoch 5616/20000 Training Loss: 0.06597518920898438\n",
      "Epoch 5617/20000 Training Loss: 0.06584510952234268\n",
      "Epoch 5618/20000 Training Loss: 0.050265852361917496\n",
      "Epoch 5619/20000 Training Loss: 0.07930692285299301\n",
      "Epoch 5620/20000 Training Loss: 0.057708073407411575\n",
      "Epoch 5620/20000 Validation Loss: 0.05229043960571289\n",
      "Epoch 5621/20000 Training Loss: 0.0473378449678421\n",
      "Epoch 5622/20000 Training Loss: 0.07865280658006668\n",
      "Epoch 5623/20000 Training Loss: 0.07092151045799255\n",
      "Epoch 5624/20000 Training Loss: 0.04555012658238411\n",
      "Epoch 5625/20000 Training Loss: 0.04906827211380005\n",
      "Epoch 5626/20000 Training Loss: 0.05373596027493477\n",
      "Epoch 5627/20000 Training Loss: 0.0603608638048172\n",
      "Epoch 5628/20000 Training Loss: 0.066407710313797\n",
      "Epoch 5629/20000 Training Loss: 0.04229504242539406\n",
      "Epoch 5630/20000 Training Loss: 0.06778930872678757\n",
      "Epoch 5630/20000 Validation Loss: 0.0884900614619255\n",
      "Epoch 5631/20000 Training Loss: 0.04276415705680847\n",
      "Epoch 5632/20000 Training Loss: 0.05131205543875694\n",
      "Epoch 5633/20000 Training Loss: 0.07856567949056625\n",
      "Epoch 5634/20000 Training Loss: 0.0700422152876854\n",
      "Epoch 5635/20000 Training Loss: 0.05504335090517998\n",
      "Epoch 5636/20000 Training Loss: 0.04677214100956917\n",
      "Epoch 5637/20000 Training Loss: 0.046690139919519424\n",
      "Epoch 5638/20000 Training Loss: 0.05162611976265907\n",
      "Epoch 5639/20000 Training Loss: 0.049038663506507874\n",
      "Epoch 5640/20000 Training Loss: 0.07415053993463516\n",
      "Epoch 5640/20000 Validation Loss: 0.08586320281028748\n",
      "Epoch 5641/20000 Training Loss: 0.07044956833124161\n",
      "Epoch 5642/20000 Training Loss: 0.0396125353872776\n",
      "Epoch 5643/20000 Training Loss: 0.05549106001853943\n",
      "Epoch 5644/20000 Training Loss: 0.048866722732782364\n",
      "Epoch 5645/20000 Training Loss: 0.059373047202825546\n",
      "Epoch 5646/20000 Training Loss: 0.05135675147175789\n",
      "Epoch 5647/20000 Training Loss: 0.058139871805906296\n",
      "Epoch 5648/20000 Training Loss: 0.06890185177326202\n",
      "Epoch 5649/20000 Training Loss: 0.045435477048158646\n",
      "Epoch 5650/20000 Training Loss: 0.05660158395767212\n",
      "Epoch 5650/20000 Validation Loss: 0.09220098704099655\n",
      "Epoch 5651/20000 Training Loss: 0.04196295142173767\n",
      "Epoch 5652/20000 Training Loss: 0.06284758448600769\n",
      "Epoch 5653/20000 Training Loss: 0.05685374140739441\n",
      "Epoch 5654/20000 Training Loss: 0.05372923985123634\n",
      "Epoch 5655/20000 Training Loss: 0.045483604073524475\n",
      "Epoch 5656/20000 Training Loss: 0.0635315403342247\n",
      "Epoch 5657/20000 Training Loss: 0.06225809082388878\n",
      "Epoch 5658/20000 Training Loss: 0.0747995600104332\n",
      "Epoch 5659/20000 Training Loss: 0.05227789282798767\n",
      "Epoch 5660/20000 Training Loss: 0.07001076638698578\n",
      "Epoch 5660/20000 Validation Loss: 0.05491981655359268\n",
      "Epoch 5661/20000 Training Loss: 0.040758147835731506\n",
      "Epoch 5662/20000 Training Loss: 0.054177578538656235\n",
      "Epoch 5663/20000 Training Loss: 0.05963126942515373\n",
      "Epoch 5664/20000 Training Loss: 0.04597477614879608\n",
      "Epoch 5665/20000 Training Loss: 0.05535796284675598\n",
      "Epoch 5666/20000 Training Loss: 0.05872933939099312\n",
      "Epoch 5667/20000 Training Loss: 0.06836497038602829\n",
      "Epoch 5668/20000 Training Loss: 0.0713467001914978\n",
      "Epoch 5669/20000 Training Loss: 0.06295086443424225\n",
      "Epoch 5670/20000 Training Loss: 0.05128348991274834\n",
      "Epoch 5670/20000 Validation Loss: 0.05496135726571083\n",
      "Epoch 5671/20000 Training Loss: 0.07118558883666992\n",
      "Epoch 5672/20000 Training Loss: 0.055251594632864\n",
      "Epoch 5673/20000 Training Loss: 0.06207868084311485\n",
      "Epoch 5674/20000 Training Loss: 0.06278058141469955\n",
      "Epoch 5675/20000 Training Loss: 0.06569882482290268\n",
      "Epoch 5676/20000 Training Loss: 0.06253427267074585\n",
      "Epoch 5677/20000 Training Loss: 0.0728347897529602\n",
      "Epoch 5678/20000 Training Loss: 0.05132129788398743\n",
      "Epoch 5679/20000 Training Loss: 0.056937675923109055\n",
      "Epoch 5680/20000 Training Loss: 0.08684852719306946\n",
      "Epoch 5680/20000 Validation Loss: 0.054924894124269485\n",
      "Epoch 5681/20000 Training Loss: 0.0551445372402668\n",
      "Epoch 5682/20000 Training Loss: 0.0666184350848198\n",
      "Epoch 5683/20000 Training Loss: 0.044167812913656235\n",
      "Epoch 5684/20000 Training Loss: 0.0586615689098835\n",
      "Epoch 5685/20000 Training Loss: 0.04648559167981148\n",
      "Epoch 5686/20000 Training Loss: 0.04997272789478302\n",
      "Epoch 5687/20000 Training Loss: 0.05264522507786751\n",
      "Epoch 5688/20000 Training Loss: 0.08036423474550247\n",
      "Epoch 5689/20000 Training Loss: 0.0549217127263546\n",
      "Epoch 5690/20000 Training Loss: 0.04709230363368988\n",
      "Epoch 5690/20000 Validation Loss: 0.03909071534872055\n",
      "Epoch 5691/20000 Training Loss: 0.062325701117515564\n",
      "Epoch 5692/20000 Training Loss: 0.050308357924222946\n",
      "Epoch 5693/20000 Training Loss: 0.057584989815950394\n",
      "Epoch 5694/20000 Training Loss: 0.05002937838435173\n",
      "Epoch 5695/20000 Training Loss: 0.06651481986045837\n",
      "Epoch 5696/20000 Training Loss: 0.051338452845811844\n",
      "Epoch 5697/20000 Training Loss: 0.0630318745970726\n",
      "Epoch 5698/20000 Training Loss: 0.06951309740543365\n",
      "Epoch 5699/20000 Training Loss: 0.040452923625707626\n",
      "Epoch 5700/20000 Training Loss: 0.05107317492365837\n",
      "Epoch 5700/20000 Validation Loss: 0.06466367840766907\n",
      "Epoch 5701/20000 Training Loss: 0.048524897545576096\n",
      "Epoch 5702/20000 Training Loss: 0.05471141263842583\n",
      "Epoch 5703/20000 Training Loss: 0.05519343540072441\n",
      "Epoch 5704/20000 Training Loss: 0.04473212733864784\n",
      "Epoch 5705/20000 Training Loss: 0.042552586644887924\n",
      "Epoch 5706/20000 Training Loss: 0.05568346381187439\n",
      "Epoch 5707/20000 Training Loss: 0.04812611639499664\n",
      "Epoch 5708/20000 Training Loss: 0.04914255067706108\n",
      "Epoch 5709/20000 Training Loss: 0.05461971089243889\n",
      "Epoch 5710/20000 Training Loss: 0.05359205603599548\n",
      "Epoch 5710/20000 Validation Loss: 0.0643172636628151\n",
      "Epoch 5711/20000 Training Loss: 0.07654931396245956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5712/20000 Training Loss: 0.05603930726647377\n",
      "Epoch 5713/20000 Training Loss: 0.05864362791180611\n",
      "Epoch 5714/20000 Training Loss: 0.06674356013536453\n",
      "Epoch 5715/20000 Training Loss: 0.08275038748979568\n",
      "Epoch 5716/20000 Training Loss: 0.05761147662997246\n",
      "Epoch 5717/20000 Training Loss: 0.0573219396173954\n",
      "Epoch 5718/20000 Training Loss: 0.06914684176445007\n",
      "Epoch 5719/20000 Training Loss: 0.06482336670160294\n",
      "Epoch 5720/20000 Training Loss: 0.06615845113992691\n",
      "Epoch 5720/20000 Validation Loss: 0.06461329013109207\n",
      "Epoch 5721/20000 Training Loss: 0.061035316437482834\n",
      "Epoch 5722/20000 Training Loss: 0.05249110236763954\n",
      "Epoch 5723/20000 Training Loss: 0.05306461453437805\n",
      "Epoch 5724/20000 Training Loss: 0.044372204691171646\n",
      "Epoch 5725/20000 Training Loss: 0.04801999405026436\n",
      "Epoch 5726/20000 Training Loss: 0.05471564456820488\n",
      "Epoch 5727/20000 Training Loss: 0.05796634033322334\n",
      "Epoch 5728/20000 Training Loss: 0.039669301360845566\n",
      "Epoch 5729/20000 Training Loss: 0.06070682406425476\n",
      "Epoch 5730/20000 Training Loss: 0.05757230520248413\n",
      "Epoch 5730/20000 Validation Loss: 0.056873708963394165\n",
      "Epoch 5731/20000 Training Loss: 0.050240445882081985\n",
      "Epoch 5732/20000 Training Loss: 0.051499199122190475\n",
      "Epoch 5733/20000 Training Loss: 0.0593445785343647\n",
      "Epoch 5734/20000 Training Loss: 0.05356103554368019\n",
      "Epoch 5735/20000 Training Loss: 0.05333983898162842\n",
      "Epoch 5736/20000 Training Loss: 0.059994373470544815\n",
      "Epoch 5737/20000 Training Loss: 0.058649320155382156\n",
      "Epoch 5738/20000 Training Loss: 0.047364916652441025\n",
      "Epoch 5739/20000 Training Loss: 0.06491375714540482\n",
      "Epoch 5740/20000 Training Loss: 0.0613422654569149\n",
      "Epoch 5740/20000 Validation Loss: 0.0772186741232872\n",
      "Epoch 5741/20000 Training Loss: 0.049613188952207565\n",
      "Epoch 5742/20000 Training Loss: 0.06891016662120819\n",
      "Epoch 5743/20000 Training Loss: 0.06027555838227272\n",
      "Epoch 5744/20000 Training Loss: 0.061024587601423264\n",
      "Epoch 5745/20000 Training Loss: 0.06145413592457771\n",
      "Epoch 5746/20000 Training Loss: 0.06839396804571152\n",
      "Epoch 5747/20000 Training Loss: 0.05075741186738014\n",
      "Epoch 5748/20000 Training Loss: 0.040143996477127075\n",
      "Epoch 5749/20000 Training Loss: 0.05333389714360237\n",
      "Epoch 5750/20000 Training Loss: 0.045313358306884766\n",
      "Epoch 5750/20000 Validation Loss: 0.039383403956890106\n",
      "Epoch 5751/20000 Training Loss: 0.0423920638859272\n",
      "Epoch 5752/20000 Training Loss: 0.07661524415016174\n",
      "Epoch 5753/20000 Training Loss: 0.07472836226224899\n",
      "Epoch 5754/20000 Training Loss: 0.05183577165007591\n",
      "Epoch 5755/20000 Training Loss: 0.05442221090197563\n",
      "Epoch 5756/20000 Training Loss: 0.040530625730752945\n",
      "Epoch 5757/20000 Training Loss: 0.07816771417856216\n",
      "Epoch 5758/20000 Training Loss: 0.06265360862016678\n",
      "Epoch 5759/20000 Training Loss: 0.04558538272976875\n",
      "Epoch 5760/20000 Training Loss: 0.054483890533447266\n",
      "Epoch 5760/20000 Validation Loss: 0.04909851774573326\n",
      "Epoch 5761/20000 Training Loss: 0.06084039807319641\n",
      "Epoch 5762/20000 Training Loss: 0.04894572123885155\n",
      "Epoch 5763/20000 Training Loss: 0.05498708412051201\n",
      "Epoch 5764/20000 Training Loss: 0.053743332624435425\n",
      "Epoch 5765/20000 Training Loss: 0.0554964654147625\n",
      "Epoch 5766/20000 Training Loss: 0.07761219888925552\n",
      "Epoch 5767/20000 Training Loss: 0.0654296875\n",
      "Epoch 5768/20000 Training Loss: 0.05742857977747917\n",
      "Epoch 5769/20000 Training Loss: 0.06326386332511902\n",
      "Epoch 5770/20000 Training Loss: 0.06497376412153244\n",
      "Epoch 5770/20000 Validation Loss: 0.04071658104658127\n",
      "Epoch 5771/20000 Training Loss: 0.04422314837574959\n",
      "Epoch 5772/20000 Training Loss: 0.07330051064491272\n",
      "Epoch 5773/20000 Training Loss: 0.05272086337208748\n",
      "Epoch 5774/20000 Training Loss: 0.05836660787463188\n",
      "Epoch 5775/20000 Training Loss: 0.057738468050956726\n",
      "Epoch 5776/20000 Training Loss: 0.05933874845504761\n",
      "Epoch 5777/20000 Training Loss: 0.0715600773692131\n",
      "Epoch 5778/20000 Training Loss: 0.05605484917759895\n",
      "Epoch 5779/20000 Training Loss: 0.07470021396875381\n",
      "Epoch 5780/20000 Training Loss: 0.06778856366872787\n",
      "Epoch 5780/20000 Validation Loss: 0.04140207916498184\n",
      "Epoch 5781/20000 Training Loss: 0.0532965250313282\n",
      "Epoch 5782/20000 Training Loss: 0.04242421314120293\n",
      "Epoch 5783/20000 Training Loss: 0.055460747331380844\n",
      "Epoch 5784/20000 Training Loss: 0.05710562691092491\n",
      "Epoch 5785/20000 Training Loss: 0.051010552793741226\n",
      "Epoch 5786/20000 Training Loss: 0.05172574520111084\n",
      "Epoch 5787/20000 Training Loss: 0.057883430272340775\n",
      "Epoch 5788/20000 Training Loss: 0.05760757252573967\n",
      "Epoch 5789/20000 Training Loss: 0.06249547004699707\n",
      "Epoch 5790/20000 Training Loss: 0.05058954283595085\n",
      "Epoch 5790/20000 Validation Loss: 0.03735443204641342\n",
      "Epoch 5791/20000 Training Loss: 0.061770617961883545\n",
      "Epoch 5792/20000 Training Loss: 0.07204898446798325\n",
      "Epoch 5793/20000 Training Loss: 0.033198412507772446\n",
      "Epoch 5794/20000 Training Loss: 0.055548813194036484\n",
      "Epoch 5795/20000 Training Loss: 0.048214782029390335\n",
      "Epoch 5796/20000 Training Loss: 0.05597582459449768\n",
      "Epoch 5797/20000 Training Loss: 0.04894628748297691\n",
      "Epoch 5798/20000 Training Loss: 0.07225814461708069\n",
      "Epoch 5799/20000 Training Loss: 0.04979436472058296\n",
      "Epoch 5800/20000 Training Loss: 0.04658728837966919\n",
      "Epoch 5800/20000 Validation Loss: 0.06064627319574356\n",
      "Epoch 5801/20000 Training Loss: 0.0641201063990593\n",
      "Epoch 5802/20000 Training Loss: 0.07426822185516357\n",
      "Epoch 5803/20000 Training Loss: 0.06087939441204071\n",
      "Epoch 5804/20000 Training Loss: 0.04842041805386543\n",
      "Epoch 5805/20000 Training Loss: 0.05111008882522583\n",
      "Epoch 5806/20000 Training Loss: 0.06289122253656387\n",
      "Epoch 5807/20000 Training Loss: 0.06692927330732346\n",
      "Epoch 5808/20000 Training Loss: 0.056788086891174316\n",
      "Epoch 5809/20000 Training Loss: 0.044864069670438766\n",
      "Epoch 5810/20000 Training Loss: 0.05512814223766327\n",
      "Epoch 5810/20000 Validation Loss: 0.05425741523504257\n",
      "Epoch 5811/20000 Training Loss: 0.043560177087783813\n",
      "Epoch 5812/20000 Training Loss: 0.04853729531168938\n",
      "Epoch 5813/20000 Training Loss: 0.0443260483443737\n",
      "Epoch 5814/20000 Training Loss: 0.05630385875701904\n",
      "Epoch 5815/20000 Training Loss: 0.05851145461201668\n",
      "Epoch 5816/20000 Training Loss: 0.05344805493950844\n",
      "Epoch 5817/20000 Training Loss: 0.051139310002326965\n",
      "Epoch 5818/20000 Training Loss: 0.062397051602602005\n",
      "Epoch 5819/20000 Training Loss: 0.07232332229614258\n",
      "Epoch 5820/20000 Training Loss: 0.05397171899676323\n",
      "Epoch 5820/20000 Validation Loss: 0.060481712222099304\n",
      "Epoch 5821/20000 Training Loss: 0.06045455113053322\n",
      "Epoch 5822/20000 Training Loss: 0.056188538670539856\n",
      "Epoch 5823/20000 Training Loss: 0.0579940490424633\n",
      "Epoch 5824/20000 Training Loss: 0.043664395809173584\n",
      "Epoch 5825/20000 Training Loss: 0.07831589132547379\n",
      "Epoch 5826/20000 Training Loss: 0.06418950110673904\n",
      "Epoch 5827/20000 Training Loss: 0.049987901002168655\n",
      "Epoch 5828/20000 Training Loss: 0.06627552956342697\n",
      "Epoch 5829/20000 Training Loss: 0.04541235789656639\n",
      "Epoch 5830/20000 Training Loss: 0.053067419677972794\n",
      "Epoch 5830/20000 Validation Loss: 0.06939543783664703\n",
      "Epoch 5831/20000 Training Loss: 0.052770789712667465\n",
      "Epoch 5832/20000 Training Loss: 0.05679158493876457\n",
      "Epoch 5833/20000 Training Loss: 0.05983045697212219\n",
      "Epoch 5834/20000 Training Loss: 0.0447721965610981\n",
      "Epoch 5835/20000 Training Loss: 0.0659601017832756\n",
      "Epoch 5836/20000 Training Loss: 0.06456226110458374\n",
      "Epoch 5837/20000 Training Loss: 0.04924847185611725\n",
      "Epoch 5838/20000 Training Loss: 0.07434985041618347\n",
      "Epoch 5839/20000 Training Loss: 0.0581752248108387\n",
      "Epoch 5840/20000 Training Loss: 0.07047799229621887\n",
      "Epoch 5840/20000 Validation Loss: 0.042184438556432724\n",
      "Epoch 5841/20000 Training Loss: 0.05350574478507042\n",
      "Epoch 5842/20000 Training Loss: 0.06249319016933441\n",
      "Epoch 5843/20000 Training Loss: 0.06309612840414047\n",
      "Epoch 5844/20000 Training Loss: 0.06438253074884415\n",
      "Epoch 5845/20000 Training Loss: 0.0638851597905159\n",
      "Epoch 5846/20000 Training Loss: 0.05376077815890312\n",
      "Epoch 5847/20000 Training Loss: 0.06229051947593689\n",
      "Epoch 5848/20000 Training Loss: 0.0733657106757164\n",
      "Epoch 5849/20000 Training Loss: 0.04591615870594978\n",
      "Epoch 5850/20000 Training Loss: 0.07531098276376724\n",
      "Epoch 5850/20000 Validation Loss: 0.0549297109246254\n",
      "Epoch 5851/20000 Training Loss: 0.058302730321884155\n",
      "Epoch 5852/20000 Training Loss: 0.04873140528798103\n",
      "Epoch 5853/20000 Training Loss: 0.0493626743555069\n",
      "Epoch 5854/20000 Training Loss: 0.05309763550758362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5855/20000 Training Loss: 0.037440475076436996\n",
      "Epoch 5856/20000 Training Loss: 0.047986436635255814\n",
      "Epoch 5857/20000 Training Loss: 0.05293327942490578\n",
      "Epoch 5858/20000 Training Loss: 0.048593923449516296\n",
      "Epoch 5859/20000 Training Loss: 0.05243312940001488\n",
      "Epoch 5860/20000 Training Loss: 0.052238475531339645\n",
      "Epoch 5860/20000 Validation Loss: 0.06470755487680435\n",
      "Epoch 5861/20000 Training Loss: 0.05692380294203758\n",
      "Epoch 5862/20000 Training Loss: 0.043990399688482285\n",
      "Epoch 5863/20000 Training Loss: 0.06285925954580307\n",
      "Epoch 5864/20000 Training Loss: 0.049898575991392136\n",
      "Epoch 5865/20000 Training Loss: 0.05600302293896675\n",
      "Epoch 5866/20000 Training Loss: 0.04510540887713432\n",
      "Epoch 5867/20000 Training Loss: 0.05698706582188606\n",
      "Epoch 5868/20000 Training Loss: 0.05824288725852966\n",
      "Epoch 5869/20000 Training Loss: 0.0611974261701107\n",
      "Epoch 5870/20000 Training Loss: 0.05396777763962746\n",
      "Epoch 5870/20000 Validation Loss: 0.06513077765703201\n",
      "Epoch 5871/20000 Training Loss: 0.043405041098594666\n",
      "Epoch 5872/20000 Training Loss: 0.05509714409708977\n",
      "Epoch 5873/20000 Training Loss: 0.04673536494374275\n",
      "Epoch 5874/20000 Training Loss: 0.04929778352379799\n",
      "Epoch 5875/20000 Training Loss: 0.04481097683310509\n",
      "Epoch 5876/20000 Training Loss: 0.06012660637497902\n",
      "Epoch 5877/20000 Training Loss: 0.07317196577787399\n",
      "Epoch 5878/20000 Training Loss: 0.06587906181812286\n",
      "Epoch 5879/20000 Training Loss: 0.06416498869657516\n",
      "Epoch 5880/20000 Training Loss: 0.059161290526390076\n",
      "Epoch 5880/20000 Validation Loss: 0.06646082550287247\n",
      "Epoch 5881/20000 Training Loss: 0.04441351071000099\n",
      "Epoch 5882/20000 Training Loss: 0.046681735664606094\n",
      "Epoch 5883/20000 Training Loss: 0.053374361246824265\n",
      "Epoch 5884/20000 Training Loss: 0.04219694808125496\n",
      "Epoch 5885/20000 Training Loss: 0.06563849002122879\n",
      "Epoch 5886/20000 Training Loss: 0.03913553059101105\n",
      "Epoch 5887/20000 Training Loss: 0.056665968149900436\n",
      "Epoch 5888/20000 Training Loss: 0.05053735896945\n",
      "Epoch 5889/20000 Training Loss: 0.0642787516117096\n",
      "Epoch 5890/20000 Training Loss: 0.055340368300676346\n",
      "Epoch 5890/20000 Validation Loss: 0.07140146195888519\n",
      "Epoch 5891/20000 Training Loss: 0.06800902634859085\n",
      "Epoch 5892/20000 Training Loss: 0.06633681803941727\n",
      "Epoch 5893/20000 Training Loss: 0.047174591571092606\n",
      "Epoch 5894/20000 Training Loss: 0.057736411690711975\n",
      "Epoch 5895/20000 Training Loss: 0.06416097283363342\n",
      "Epoch 5896/20000 Training Loss: 0.06589855998754501\n",
      "Epoch 5897/20000 Training Loss: 0.053668394684791565\n",
      "Epoch 5898/20000 Training Loss: 0.05749290809035301\n",
      "Epoch 5899/20000 Training Loss: 0.05730944871902466\n",
      "Epoch 5900/20000 Training Loss: 0.07197287678718567\n",
      "Epoch 5900/20000 Validation Loss: 0.04959454759955406\n",
      "Epoch 5901/20000 Training Loss: 0.040369194000959396\n",
      "Epoch 5902/20000 Training Loss: 0.0436379611492157\n",
      "Epoch 5903/20000 Training Loss: 0.04953060671687126\n",
      "Epoch 5904/20000 Training Loss: 0.05921017751097679\n",
      "Epoch 5905/20000 Training Loss: 0.04785628244280815\n",
      "Epoch 5906/20000 Training Loss: 0.052742552012205124\n",
      "Epoch 5907/20000 Training Loss: 0.0451112799346447\n",
      "Epoch 5908/20000 Training Loss: 0.06646445393562317\n",
      "Epoch 5909/20000 Training Loss: 0.06598537415266037\n",
      "Epoch 5910/20000 Training Loss: 0.05794074013829231\n",
      "Epoch 5910/20000 Validation Loss: 0.055380526930093765\n",
      "Epoch 5911/20000 Training Loss: 0.043361395597457886\n",
      "Epoch 5912/20000 Training Loss: 0.06546062976121902\n",
      "Epoch 5913/20000 Training Loss: 0.05717660114169121\n",
      "Epoch 5914/20000 Training Loss: 0.06641439348459244\n",
      "Epoch 5915/20000 Training Loss: 0.0650624930858612\n",
      "Epoch 5916/20000 Training Loss: 0.0532626211643219\n",
      "Epoch 5917/20000 Training Loss: 0.04869581386446953\n",
      "Epoch 5918/20000 Training Loss: 0.05398532375693321\n",
      "Epoch 5919/20000 Training Loss: 0.04944445192813873\n",
      "Epoch 5920/20000 Training Loss: 0.05633144453167915\n",
      "Epoch 5920/20000 Validation Loss: 0.04481661319732666\n",
      "Epoch 5921/20000 Training Loss: 0.05418229475617409\n",
      "Epoch 5922/20000 Training Loss: 0.04224798083305359\n",
      "Epoch 5923/20000 Training Loss: 0.05405346676707268\n",
      "Epoch 5924/20000 Training Loss: 0.04956166446208954\n",
      "Epoch 5925/20000 Training Loss: 0.05529361963272095\n",
      "Epoch 5926/20000 Training Loss: 0.05192951485514641\n",
      "Epoch 5927/20000 Training Loss: 0.05916239693760872\n",
      "Epoch 5928/20000 Training Loss: 0.0485374890267849\n",
      "Epoch 5929/20000 Training Loss: 0.0508762001991272\n",
      "Epoch 5930/20000 Training Loss: 0.0799379050731659\n",
      "Epoch 5930/20000 Validation Loss: 0.07205695658922195\n",
      "Epoch 5931/20000 Training Loss: 0.06294967979192734\n",
      "Epoch 5932/20000 Training Loss: 0.05997619405388832\n",
      "Epoch 5933/20000 Training Loss: 0.06076383590698242\n",
      "Epoch 5934/20000 Training Loss: 0.05757707357406616\n",
      "Epoch 5935/20000 Training Loss: 0.048557016998529434\n",
      "Epoch 5936/20000 Training Loss: 0.07011928409337997\n",
      "Epoch 5937/20000 Training Loss: 0.05479149520397186\n",
      "Epoch 5938/20000 Training Loss: 0.060251384973526\n",
      "Epoch 5939/20000 Training Loss: 0.07942003756761551\n",
      "Epoch 5940/20000 Training Loss: 0.04219299927353859\n",
      "Epoch 5940/20000 Validation Loss: 0.04153486341238022\n",
      "Epoch 5941/20000 Training Loss: 0.05256122723221779\n",
      "Epoch 5942/20000 Training Loss: 0.05481138825416565\n",
      "Epoch 5943/20000 Training Loss: 0.053130537271499634\n",
      "Epoch 5944/20000 Training Loss: 0.043283309787511826\n",
      "Epoch 5945/20000 Training Loss: 0.04172496125102043\n",
      "Epoch 5946/20000 Training Loss: 0.06118527427315712\n",
      "Epoch 5947/20000 Training Loss: 0.03995108976960182\n",
      "Epoch 5948/20000 Training Loss: 0.06063682213425636\n",
      "Epoch 5949/20000 Training Loss: 0.06366835534572601\n",
      "Epoch 5950/20000 Training Loss: 0.055373262614011765\n",
      "Epoch 5950/20000 Validation Loss: 0.04623252898454666\n",
      "Epoch 5951/20000 Training Loss: 0.07834208756685257\n",
      "Epoch 5952/20000 Training Loss: 0.045295294374227524\n",
      "Epoch 5953/20000 Training Loss: 0.04974663257598877\n",
      "Epoch 5954/20000 Training Loss: 0.059424515813589096\n",
      "Epoch 5955/20000 Training Loss: 0.03839138522744179\n",
      "Epoch 5956/20000 Training Loss: 0.04686275124549866\n",
      "Epoch 5957/20000 Training Loss: 0.05108148977160454\n",
      "Epoch 5958/20000 Training Loss: 0.05715584754943848\n",
      "Epoch 5959/20000 Training Loss: 0.06058741733431816\n",
      "Epoch 5960/20000 Training Loss: 0.05129489302635193\n",
      "Epoch 5960/20000 Validation Loss: 0.08648820221424103\n",
      "Epoch 5961/20000 Training Loss: 0.05881885811686516\n",
      "Epoch 5962/20000 Training Loss: 0.06430079787969589\n",
      "Epoch 5963/20000 Training Loss: 0.05377461388707161\n",
      "Epoch 5964/20000 Training Loss: 0.06867113709449768\n",
      "Epoch 5965/20000 Training Loss: 0.06699793040752411\n",
      "Epoch 5966/20000 Training Loss: 0.06800273805856705\n",
      "Epoch 5967/20000 Training Loss: 0.06263300776481628\n",
      "Epoch 5968/20000 Training Loss: 0.06720217317342758\n",
      "Epoch 5969/20000 Training Loss: 0.05316704884171486\n",
      "Epoch 5970/20000 Training Loss: 0.07022996246814728\n",
      "Epoch 5970/20000 Validation Loss: 0.04944527521729469\n",
      "Epoch 5971/20000 Training Loss: 0.05529448017477989\n",
      "Epoch 5972/20000 Training Loss: 0.045721635222435\n",
      "Epoch 5973/20000 Training Loss: 0.07543465495109558\n",
      "Epoch 5974/20000 Training Loss: 0.058739736676216125\n",
      "Epoch 5975/20000 Training Loss: 0.04427851364016533\n",
      "Epoch 5976/20000 Training Loss: 0.05145004019141197\n",
      "Epoch 5977/20000 Training Loss: 0.05477036535739899\n",
      "Epoch 5978/20000 Training Loss: 0.058745771646499634\n",
      "Epoch 5979/20000 Training Loss: 0.05766482651233673\n",
      "Epoch 5980/20000 Training Loss: 0.07646552473306656\n",
      "Epoch 5980/20000 Validation Loss: 0.05542550981044769\n",
      "Epoch 5981/20000 Training Loss: 0.06285685300827026\n",
      "Epoch 5982/20000 Training Loss: 0.07716899365186691\n",
      "Epoch 5983/20000 Training Loss: 0.054235681891441345\n",
      "Epoch 5984/20000 Training Loss: 0.04965826869010925\n",
      "Epoch 5985/20000 Training Loss: 0.06802911311388016\n",
      "Epoch 5986/20000 Training Loss: 0.07179614156484604\n",
      "Epoch 5987/20000 Training Loss: 0.054831285029649734\n",
      "Epoch 5988/20000 Training Loss: 0.06601765751838684\n",
      "Epoch 5989/20000 Training Loss: 0.047758374363183975\n",
      "Epoch 5990/20000 Training Loss: 0.04675475135445595\n",
      "Epoch 5990/20000 Validation Loss: 0.07289031893014908\n",
      "Epoch 5991/20000 Training Loss: 0.06414645165205002\n",
      "Epoch 5992/20000 Training Loss: 0.06594845652580261\n",
      "Epoch 5993/20000 Training Loss: 0.05137009546160698\n",
      "Epoch 5994/20000 Training Loss: 0.05083794519305229\n",
      "Epoch 5995/20000 Training Loss: 0.043474230915308\n",
      "Epoch 5996/20000 Training Loss: 0.04446202516555786\n",
      "Epoch 5997/20000 Training Loss: 0.0664912760257721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5998/20000 Training Loss: 0.04754866659641266\n",
      "Epoch 5999/20000 Training Loss: 0.05459566041827202\n",
      "Epoch 6000/20000 Training Loss: 0.05650338903069496\n",
      "Epoch 6000/20000 Validation Loss: 0.05795492231845856\n",
      "Epoch 6001/20000 Training Loss: 0.047239840030670166\n",
      "Epoch 6002/20000 Training Loss: 0.051365334540605545\n",
      "Epoch 6003/20000 Training Loss: 0.05105818435549736\n",
      "Epoch 6004/20000 Training Loss: 0.06686628609895706\n",
      "Epoch 6005/20000 Training Loss: 0.08697649091482162\n",
      "Epoch 6006/20000 Training Loss: 0.047572385519742966\n",
      "Epoch 6007/20000 Training Loss: 0.054907966405153275\n",
      "Epoch 6008/20000 Training Loss: 0.04538637027144432\n",
      "Epoch 6009/20000 Training Loss: 0.06348445266485214\n",
      "Epoch 6010/20000 Training Loss: 0.07063907384872437\n",
      "Epoch 6010/20000 Validation Loss: 0.06500421464443207\n",
      "Epoch 6011/20000 Training Loss: 0.05174916982650757\n",
      "Epoch 6012/20000 Training Loss: 0.0650496557354927\n",
      "Epoch 6013/20000 Training Loss: 0.09525857120752335\n",
      "Epoch 6014/20000 Training Loss: 0.05334523320198059\n",
      "Epoch 6015/20000 Training Loss: 0.06586549431085587\n",
      "Epoch 6016/20000 Training Loss: 0.05950595811009407\n",
      "Epoch 6017/20000 Training Loss: 0.0612349808216095\n",
      "Epoch 6018/20000 Training Loss: 0.06113428995013237\n",
      "Epoch 6019/20000 Training Loss: 0.05900649353861809\n",
      "Epoch 6020/20000 Training Loss: 0.05597406625747681\n",
      "Epoch 6020/20000 Validation Loss: 0.06027035415172577\n",
      "Epoch 6021/20000 Training Loss: 0.04073631390929222\n",
      "Epoch 6022/20000 Training Loss: 0.06971845030784607\n",
      "Epoch 6023/20000 Training Loss: 0.07676627486944199\n",
      "Epoch 6024/20000 Training Loss: 0.05371115729212761\n",
      "Epoch 6025/20000 Training Loss: 0.068732351064682\n",
      "Epoch 6026/20000 Training Loss: 0.0487934947013855\n",
      "Epoch 6027/20000 Training Loss: 0.0629262700676918\n",
      "Epoch 6028/20000 Training Loss: 0.04734324291348457\n",
      "Epoch 6029/20000 Training Loss: 0.06493517011404037\n",
      "Epoch 6030/20000 Training Loss: 0.05750283598899841\n",
      "Epoch 6030/20000 Validation Loss: 0.04541302099823952\n",
      "Epoch 6031/20000 Training Loss: 0.040121838450431824\n",
      "Epoch 6032/20000 Training Loss: 0.06433753669261932\n",
      "Epoch 6033/20000 Training Loss: 0.048178862780332565\n",
      "Epoch 6034/20000 Training Loss: 0.0701356753706932\n",
      "Epoch 6035/20000 Training Loss: 0.06115752086043358\n",
      "Epoch 6036/20000 Training Loss: 0.0685039535164833\n",
      "Epoch 6037/20000 Training Loss: 0.053494419902563095\n",
      "Epoch 6038/20000 Training Loss: 0.0653739646077156\n",
      "Epoch 6039/20000 Training Loss: 0.057546768337488174\n",
      "Epoch 6040/20000 Training Loss: 0.04960726574063301\n",
      "Epoch 6040/20000 Validation Loss: 0.0572628416121006\n",
      "Epoch 6041/20000 Training Loss: 0.062211260199546814\n",
      "Epoch 6042/20000 Training Loss: 0.08024189621210098\n",
      "Epoch 6043/20000 Training Loss: 0.049032796174287796\n",
      "Epoch 6044/20000 Training Loss: 0.07809089869260788\n",
      "Epoch 6045/20000 Training Loss: 0.07364017516374588\n",
      "Epoch 6046/20000 Training Loss: 0.06304117292165756\n",
      "Epoch 6047/20000 Training Loss: 0.052560362964868546\n",
      "Epoch 6048/20000 Training Loss: 0.06158815696835518\n",
      "Epoch 6049/20000 Training Loss: 0.05484035611152649\n",
      "Epoch 6050/20000 Training Loss: 0.05192284658551216\n",
      "Epoch 6050/20000 Validation Loss: 0.03931915760040283\n",
      "Epoch 6051/20000 Training Loss: 0.06434411555528641\n",
      "Epoch 6052/20000 Training Loss: 0.07268404960632324\n",
      "Epoch 6053/20000 Training Loss: 0.057107821106910706\n",
      "Epoch 6054/20000 Training Loss: 0.06629444658756256\n",
      "Epoch 6055/20000 Training Loss: 0.055562179535627365\n",
      "Epoch 6056/20000 Training Loss: 0.08190455287694931\n",
      "Epoch 6057/20000 Training Loss: 0.0652020052075386\n",
      "Epoch 6058/20000 Training Loss: 0.0712323784828186\n",
      "Epoch 6059/20000 Training Loss: 0.05557295307517052\n",
      "Epoch 6060/20000 Training Loss: 0.05549979209899902\n",
      "Epoch 6060/20000 Validation Loss: 0.04993494600057602\n",
      "Epoch 6061/20000 Training Loss: 0.06208398938179016\n",
      "Epoch 6062/20000 Training Loss: 0.049031827598810196\n",
      "Epoch 6063/20000 Training Loss: 0.07443531602621078\n",
      "Epoch 6064/20000 Training Loss: 0.06409486383199692\n",
      "Epoch 6065/20000 Training Loss: 0.07677999883890152\n",
      "Epoch 6066/20000 Training Loss: 0.07151897996664047\n",
      "Epoch 6067/20000 Training Loss: 0.051973696798086166\n",
      "Epoch 6068/20000 Training Loss: 0.05415058135986328\n",
      "Epoch 6069/20000 Training Loss: 0.04907101392745972\n",
      "Epoch 6070/20000 Training Loss: 0.05269034206867218\n",
      "Epoch 6070/20000 Validation Loss: 0.061177343130111694\n",
      "Epoch 6071/20000 Training Loss: 0.07549531012773514\n",
      "Epoch 6072/20000 Training Loss: 0.03811165317893028\n",
      "Epoch 6073/20000 Training Loss: 0.06955209374427795\n",
      "Epoch 6074/20000 Training Loss: 0.05561290308833122\n",
      "Epoch 6075/20000 Training Loss: 0.05849192664027214\n",
      "Epoch 6076/20000 Training Loss: 0.05791642889380455\n",
      "Epoch 6077/20000 Training Loss: 0.05618072673678398\n",
      "Epoch 6078/20000 Training Loss: 0.050231728702783585\n",
      "Epoch 6079/20000 Training Loss: 0.0466332770884037\n",
      "Epoch 6080/20000 Training Loss: 0.04937129095196724\n",
      "Epoch 6080/20000 Validation Loss: 0.048368848860263824\n",
      "Epoch 6081/20000 Training Loss: 0.07187739759683609\n",
      "Epoch 6082/20000 Training Loss: 0.062213946133852005\n",
      "Epoch 6083/20000 Training Loss: 0.06029321253299713\n",
      "Epoch 6084/20000 Training Loss: 0.05047454312443733\n",
      "Epoch 6085/20000 Training Loss: 0.06122368574142456\n",
      "Epoch 6086/20000 Training Loss: 0.06334716081619263\n",
      "Epoch 6087/20000 Training Loss: 0.06688331812620163\n",
      "Epoch 6088/20000 Training Loss: 0.04253951832652092\n",
      "Epoch 6089/20000 Training Loss: 0.06322325021028519\n",
      "Epoch 6090/20000 Training Loss: 0.05631480738520622\n",
      "Epoch 6090/20000 Validation Loss: 0.04598728194832802\n",
      "Epoch 6091/20000 Training Loss: 0.07128074020147324\n",
      "Epoch 6092/20000 Training Loss: 0.06555042415857315\n",
      "Epoch 6093/20000 Training Loss: 0.06113676354289055\n",
      "Epoch 6094/20000 Training Loss: 0.06870081275701523\n",
      "Epoch 6095/20000 Training Loss: 0.05114257335662842\n",
      "Epoch 6096/20000 Training Loss: 0.06057475879788399\n",
      "Epoch 6097/20000 Training Loss: 0.06595762819051743\n",
      "Epoch 6098/20000 Training Loss: 0.06508033722639084\n",
      "Epoch 6099/20000 Training Loss: 0.04583634063601494\n",
      "Epoch 6100/20000 Training Loss: 0.05212664604187012\n",
      "Epoch 6100/20000 Validation Loss: 0.07479086518287659\n",
      "Epoch 6101/20000 Training Loss: 0.08013112097978592\n",
      "Epoch 6102/20000 Training Loss: 0.06635794788599014\n",
      "Epoch 6103/20000 Training Loss: 0.04967309162020683\n",
      "Epoch 6104/20000 Training Loss: 0.046740978956222534\n",
      "Epoch 6105/20000 Training Loss: 0.05861007049679756\n",
      "Epoch 6106/20000 Training Loss: 0.05526602268218994\n",
      "Epoch 6107/20000 Training Loss: 0.060672927647829056\n",
      "Epoch 6108/20000 Training Loss: 0.06233689561486244\n",
      "Epoch 6109/20000 Training Loss: 0.06163191422820091\n",
      "Epoch 6110/20000 Training Loss: 0.032429203391075134\n",
      "Epoch 6110/20000 Validation Loss: 0.05740896984934807\n",
      "Epoch 6111/20000 Training Loss: 0.0488029420375824\n",
      "Epoch 6112/20000 Training Loss: 0.04020087793469429\n",
      "Epoch 6113/20000 Training Loss: 0.06568167358636856\n",
      "Epoch 6114/20000 Training Loss: 0.04507485032081604\n",
      "Epoch 6115/20000 Training Loss: 0.059928327798843384\n",
      "Epoch 6116/20000 Training Loss: 0.08942968398332596\n",
      "Epoch 6117/20000 Training Loss: 0.04251481965184212\n",
      "Epoch 6118/20000 Training Loss: 0.06109175086021423\n",
      "Epoch 6119/20000 Training Loss: 0.0757850855588913\n",
      "Epoch 6120/20000 Training Loss: 0.06443538516759872\n",
      "Epoch 6120/20000 Validation Loss: 0.05619804188609123\n",
      "Epoch 6121/20000 Training Loss: 0.05461883172392845\n",
      "Epoch 6122/20000 Training Loss: 0.049869585782289505\n",
      "Epoch 6123/20000 Training Loss: 0.06132705882191658\n",
      "Epoch 6124/20000 Training Loss: 0.06845811009407043\n",
      "Epoch 6125/20000 Training Loss: 0.05172112584114075\n",
      "Epoch 6126/20000 Training Loss: 0.06625324487686157\n",
      "Epoch 6127/20000 Training Loss: 0.07150337845087051\n",
      "Epoch 6128/20000 Training Loss: 0.04220223054289818\n",
      "Epoch 6129/20000 Training Loss: 0.05293237045407295\n",
      "Epoch 6130/20000 Training Loss: 0.0592753104865551\n",
      "Epoch 6130/20000 Validation Loss: 0.05234548822045326\n",
      "Epoch 6131/20000 Training Loss: 0.05585930123925209\n",
      "Epoch 6132/20000 Training Loss: 0.036543551832437515\n",
      "Epoch 6133/20000 Training Loss: 0.035499148070812225\n",
      "Epoch 6134/20000 Training Loss: 0.039641931653022766\n",
      "Epoch 6135/20000 Training Loss: 0.057707250118255615\n",
      "Epoch 6136/20000 Training Loss: 0.06564325094223022\n",
      "Epoch 6137/20000 Training Loss: 0.0466589629650116\n",
      "Epoch 6138/20000 Training Loss: 0.05036082863807678\n",
      "Epoch 6139/20000 Training Loss: 0.05780449137091637\n",
      "Epoch 6140/20000 Training Loss: 0.0591127835214138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6140/20000 Validation Loss: 0.07443127036094666\n",
      "Epoch 6141/20000 Training Loss: 0.047382939606904984\n",
      "Epoch 6142/20000 Training Loss: 0.045325469225645065\n",
      "Epoch 6143/20000 Training Loss: 0.061056602746248245\n",
      "Epoch 6144/20000 Training Loss: 0.0688345655798912\n",
      "Epoch 6145/20000 Training Loss: 0.06749781221151352\n",
      "Epoch 6146/20000 Training Loss: 0.06619205325841904\n",
      "Epoch 6147/20000 Training Loss: 0.043214160948991776\n",
      "Epoch 6148/20000 Training Loss: 0.05501483008265495\n",
      "Epoch 6149/20000 Training Loss: 0.060577839612960815\n",
      "Epoch 6150/20000 Training Loss: 0.053035181015729904\n",
      "Epoch 6150/20000 Validation Loss: 0.03964732587337494\n",
      "Epoch 6151/20000 Training Loss: 0.055876296013593674\n",
      "Epoch 6152/20000 Training Loss: 0.08149175345897675\n",
      "Epoch 6153/20000 Training Loss: 0.04163794592022896\n",
      "Epoch 6154/20000 Training Loss: 0.052451226860284805\n",
      "Epoch 6155/20000 Training Loss: 0.04265831410884857\n",
      "Epoch 6156/20000 Training Loss: 0.07194200158119202\n",
      "Epoch 6157/20000 Training Loss: 0.05132054165005684\n",
      "Epoch 6158/20000 Training Loss: 0.06383441388607025\n",
      "Epoch 6159/20000 Training Loss: 0.07139299064874649\n",
      "Epoch 6160/20000 Training Loss: 0.059461768716573715\n",
      "Epoch 6160/20000 Validation Loss: 0.062159255146980286\n",
      "Epoch 6161/20000 Training Loss: 0.052312660962343216\n",
      "Epoch 6162/20000 Training Loss: 0.07687286287546158\n",
      "Epoch 6163/20000 Training Loss: 0.0702582597732544\n",
      "Epoch 6164/20000 Training Loss: 0.050236452370882034\n",
      "Epoch 6165/20000 Training Loss: 0.06508158892393112\n",
      "Epoch 6166/20000 Training Loss: 0.04186263307929039\n",
      "Epoch 6167/20000 Training Loss: 0.044981449842453\n",
      "Epoch 6168/20000 Training Loss: 0.0467827133834362\n",
      "Epoch 6169/20000 Training Loss: 0.061749547719955444\n",
      "Epoch 6170/20000 Training Loss: 0.07364027947187424\n",
      "Epoch 6170/20000 Validation Loss: 0.044337958097457886\n",
      "Epoch 6171/20000 Training Loss: 0.05806201696395874\n",
      "Epoch 6172/20000 Training Loss: 0.043590743094682693\n",
      "Epoch 6173/20000 Training Loss: 0.09390419721603394\n",
      "Epoch 6174/20000 Training Loss: 0.07983609288930893\n",
      "Epoch 6175/20000 Training Loss: 0.057406071573495865\n",
      "Epoch 6176/20000 Training Loss: 0.07087298482656479\n",
      "Epoch 6177/20000 Training Loss: 0.052131038159132004\n",
      "Epoch 6178/20000 Training Loss: 0.06801601499319077\n",
      "Epoch 6179/20000 Training Loss: 0.054963529109954834\n",
      "Epoch 6180/20000 Training Loss: 0.05006490275263786\n",
      "Epoch 6180/20000 Validation Loss: 0.042159102857112885\n",
      "Epoch 6181/20000 Training Loss: 0.044668059796094894\n",
      "Epoch 6182/20000 Training Loss: 0.0978083536028862\n",
      "Epoch 6183/20000 Training Loss: 0.07735348492860794\n",
      "Epoch 6184/20000 Training Loss: 0.07532259076833725\n",
      "Epoch 6185/20000 Training Loss: 0.055398911237716675\n",
      "Epoch 6186/20000 Training Loss: 0.04734112322330475\n",
      "Epoch 6187/20000 Training Loss: 0.06447510421276093\n",
      "Epoch 6188/20000 Training Loss: 0.0525650680065155\n",
      "Epoch 6189/20000 Training Loss: 0.06532204151153564\n",
      "Epoch 6190/20000 Training Loss: 0.053311366587877274\n",
      "Epoch 6190/20000 Validation Loss: 0.051761236041784286\n",
      "Epoch 6191/20000 Training Loss: 0.07587961852550507\n",
      "Epoch 6192/20000 Training Loss: 0.04550613835453987\n",
      "Epoch 6193/20000 Training Loss: 0.07400622218847275\n",
      "Epoch 6194/20000 Training Loss: 0.06184820458292961\n",
      "Epoch 6195/20000 Training Loss: 0.05617693439126015\n",
      "Epoch 6196/20000 Training Loss: 0.07534872740507126\n",
      "Epoch 6197/20000 Training Loss: 0.059073563665151596\n",
      "Epoch 6198/20000 Training Loss: 0.06398224085569382\n",
      "Epoch 6199/20000 Training Loss: 0.07508635520935059\n",
      "Epoch 6200/20000 Training Loss: 0.05783839896321297\n",
      "Epoch 6200/20000 Validation Loss: 0.0432155579328537\n",
      "Epoch 6201/20000 Training Loss: 0.043049875646829605\n",
      "Epoch 6202/20000 Training Loss: 0.04682895913720131\n",
      "Epoch 6203/20000 Training Loss: 0.08346941322088242\n",
      "Epoch 6204/20000 Training Loss: 0.06905185431241989\n",
      "Epoch 6205/20000 Training Loss: 0.04457927867770195\n",
      "Epoch 6206/20000 Training Loss: 0.05229518190026283\n",
      "Epoch 6207/20000 Training Loss: 0.0452718548476696\n",
      "Epoch 6208/20000 Training Loss: 0.05487072467803955\n",
      "Epoch 6209/20000 Training Loss: 0.04961550235748291\n",
      "Epoch 6210/20000 Training Loss: 0.04727613925933838\n",
      "Epoch 6210/20000 Validation Loss: 0.06460869312286377\n",
      "Epoch 6211/20000 Training Loss: 0.04959043860435486\n",
      "Epoch 6212/20000 Training Loss: 0.06589476019144058\n",
      "Epoch 6213/20000 Training Loss: 0.054992031306028366\n",
      "Epoch 6214/20000 Training Loss: 0.05777253583073616\n",
      "Epoch 6215/20000 Training Loss: 0.07104730606079102\n",
      "Epoch 6216/20000 Training Loss: 0.053095221519470215\n",
      "Epoch 6217/20000 Training Loss: 0.046716660261154175\n",
      "Epoch 6218/20000 Training Loss: 0.05656944587826729\n",
      "Epoch 6219/20000 Training Loss: 0.05999782308936119\n",
      "Epoch 6220/20000 Training Loss: 0.05022535100579262\n",
      "Epoch 6220/20000 Validation Loss: 0.07952060550451279\n",
      "Epoch 6221/20000 Training Loss: 0.062461983412504196\n",
      "Epoch 6222/20000 Training Loss: 0.07462037354707718\n",
      "Epoch 6223/20000 Training Loss: 0.04294581711292267\n",
      "Epoch 6224/20000 Training Loss: 0.05837034061551094\n",
      "Epoch 6225/20000 Training Loss: 0.06864137947559357\n",
      "Epoch 6226/20000 Training Loss: 0.059210579842329025\n",
      "Epoch 6227/20000 Training Loss: 0.06448551267385483\n",
      "Epoch 6228/20000 Training Loss: 0.05611804127693176\n",
      "Epoch 6229/20000 Training Loss: 0.054744649678468704\n",
      "Epoch 6230/20000 Training Loss: 0.07528197020292282\n",
      "Epoch 6230/20000 Validation Loss: 0.06666934490203857\n",
      "Epoch 6231/20000 Training Loss: 0.03474580869078636\n",
      "Epoch 6232/20000 Training Loss: 0.055062729865312576\n",
      "Epoch 6233/20000 Training Loss: 0.049694035202264786\n",
      "Epoch 6234/20000 Training Loss: 0.05239264667034149\n",
      "Epoch 6235/20000 Training Loss: 0.04936215281486511\n",
      "Epoch 6236/20000 Training Loss: 0.048542287200689316\n",
      "Epoch 6237/20000 Training Loss: 0.04636755585670471\n",
      "Epoch 6238/20000 Training Loss: 0.05049342289566994\n",
      "Epoch 6239/20000 Training Loss: 0.07066649943590164\n",
      "Epoch 6240/20000 Training Loss: 0.05547191575169563\n",
      "Epoch 6240/20000 Validation Loss: 0.07787886261940002\n",
      "Epoch 6241/20000 Training Loss: 0.037113070487976074\n",
      "Epoch 6242/20000 Training Loss: 0.048839837312698364\n",
      "Epoch 6243/20000 Training Loss: 0.045095305889844894\n",
      "Epoch 6244/20000 Training Loss: 0.04905238747596741\n",
      "Epoch 6245/20000 Training Loss: 0.05780567601323128\n",
      "Epoch 6246/20000 Training Loss: 0.05817630514502525\n",
      "Epoch 6247/20000 Training Loss: 0.060418158769607544\n",
      "Epoch 6248/20000 Training Loss: 0.05461649224162102\n",
      "Epoch 6249/20000 Training Loss: 0.06467664241790771\n",
      "Epoch 6250/20000 Training Loss: 0.0973886027932167\n",
      "Epoch 6250/20000 Validation Loss: 0.06140777841210365\n",
      "Epoch 6251/20000 Training Loss: 0.04936210438609123\n",
      "Epoch 6252/20000 Training Loss: 0.058401722460985184\n",
      "Epoch 6253/20000 Training Loss: 0.07524972409009933\n",
      "Epoch 6254/20000 Training Loss: 0.07143410295248032\n",
      "Epoch 6255/20000 Training Loss: 0.07817883789539337\n",
      "Epoch 6256/20000 Training Loss: 0.06230313703417778\n",
      "Epoch 6257/20000 Training Loss: 0.06717870384454727\n",
      "Epoch 6258/20000 Training Loss: 0.05908627435564995\n",
      "Epoch 6259/20000 Training Loss: 0.06892067193984985\n",
      "Epoch 6260/20000 Training Loss: 0.04900279641151428\n",
      "Epoch 6260/20000 Validation Loss: 0.07229871302843094\n",
      "Epoch 6261/20000 Training Loss: 0.06960296630859375\n",
      "Epoch 6262/20000 Training Loss: 0.05368410423398018\n",
      "Epoch 6263/20000 Training Loss: 0.04323774203658104\n",
      "Epoch 6264/20000 Training Loss: 0.05094711855053902\n",
      "Epoch 6265/20000 Training Loss: 0.045872464776039124\n",
      "Epoch 6266/20000 Training Loss: 0.05554443597793579\n",
      "Epoch 6267/20000 Training Loss: 0.048417896032333374\n",
      "Epoch 6268/20000 Training Loss: 0.04536009952425957\n",
      "Epoch 6269/20000 Training Loss: 0.06971258670091629\n",
      "Epoch 6270/20000 Training Loss: 0.059631943702697754\n",
      "Epoch 6270/20000 Validation Loss: 0.05726643651723862\n",
      "Epoch 6271/20000 Training Loss: 0.06062547490000725\n",
      "Epoch 6272/20000 Training Loss: 0.06548263877630234\n",
      "Epoch 6273/20000 Training Loss: 0.039645615965127945\n",
      "Epoch 6274/20000 Training Loss: 0.05490350350737572\n",
      "Epoch 6275/20000 Training Loss: 0.056095439940690994\n",
      "Epoch 6276/20000 Training Loss: 0.05270549654960632\n",
      "Epoch 6277/20000 Training Loss: 0.0673133134841919\n",
      "Epoch 6278/20000 Training Loss: 0.04445803537964821\n",
      "Epoch 6279/20000 Training Loss: 0.0594860315322876\n",
      "Epoch 6280/20000 Training Loss: 0.06303143501281738\n",
      "Epoch 6280/20000 Validation Loss: 0.04252132400870323\n",
      "Epoch 6281/20000 Training Loss: 0.04773904010653496\n",
      "Epoch 6282/20000 Training Loss: 0.05081118643283844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6283/20000 Training Loss: 0.05022810772061348\n",
      "Epoch 6284/20000 Training Loss: 0.0755518302321434\n",
      "Epoch 6285/20000 Training Loss: 0.04479050636291504\n",
      "Epoch 6286/20000 Training Loss: 0.03966205567121506\n",
      "Epoch 6287/20000 Training Loss: 0.050964925438165665\n",
      "Epoch 6288/20000 Training Loss: 0.06827187538146973\n",
      "Epoch 6289/20000 Training Loss: 0.059635285288095474\n",
      "Epoch 6290/20000 Training Loss: 0.06588175147771835\n",
      "Epoch 6290/20000 Validation Loss: 0.04817626625299454\n",
      "Epoch 6291/20000 Training Loss: 0.05421559140086174\n",
      "Epoch 6292/20000 Training Loss: 0.05959784984588623\n",
      "Epoch 6293/20000 Training Loss: 0.055549830198287964\n",
      "Epoch 6294/20000 Training Loss: 0.054708510637283325\n",
      "Epoch 6295/20000 Training Loss: 0.05784499645233154\n",
      "Epoch 6296/20000 Training Loss: 0.03418375551700592\n",
      "Epoch 6297/20000 Training Loss: 0.06072366237640381\n",
      "Epoch 6298/20000 Training Loss: 0.04195539653301239\n",
      "Epoch 6299/20000 Training Loss: 0.05721181258559227\n",
      "Epoch 6300/20000 Training Loss: 0.06583481281995773\n",
      "Epoch 6300/20000 Validation Loss: 0.05411955714225769\n",
      "Epoch 6301/20000 Training Loss: 0.07686424255371094\n",
      "Epoch 6302/20000 Training Loss: 0.06080803647637367\n",
      "Epoch 6303/20000 Training Loss: 0.03889928385615349\n",
      "Epoch 6304/20000 Training Loss: 0.04478481411933899\n",
      "Epoch 6305/20000 Training Loss: 0.06609264761209488\n",
      "Epoch 6306/20000 Training Loss: 0.0544651560485363\n",
      "Epoch 6307/20000 Training Loss: 0.04753870889544487\n",
      "Epoch 6308/20000 Training Loss: 0.06263741850852966\n",
      "Epoch 6309/20000 Training Loss: 0.05430784821510315\n",
      "Epoch 6310/20000 Training Loss: 0.05404980853199959\n",
      "Epoch 6310/20000 Validation Loss: 0.07827012240886688\n",
      "Epoch 6311/20000 Training Loss: 0.06064031645655632\n",
      "Epoch 6312/20000 Training Loss: 0.03563703969120979\n",
      "Epoch 6313/20000 Training Loss: 0.052160367369651794\n",
      "Epoch 6314/20000 Training Loss: 0.04389311373233795\n",
      "Epoch 6315/20000 Training Loss: 0.04902857914566994\n",
      "Epoch 6316/20000 Training Loss: 0.047437842935323715\n",
      "Epoch 6317/20000 Training Loss: 0.046217601746320724\n",
      "Epoch 6318/20000 Training Loss: 0.08119085431098938\n",
      "Epoch 6319/20000 Training Loss: 0.06283365190029144\n",
      "Epoch 6320/20000 Training Loss: 0.04829391837120056\n",
      "Epoch 6320/20000 Validation Loss: 0.05573021620512009\n",
      "Epoch 6321/20000 Training Loss: 0.0519319511950016\n",
      "Epoch 6322/20000 Training Loss: 0.05766163766384125\n",
      "Epoch 6323/20000 Training Loss: 0.0668734461069107\n",
      "Epoch 6324/20000 Training Loss: 0.07661936432123184\n",
      "Epoch 6325/20000 Training Loss: 0.045245274901390076\n",
      "Epoch 6326/20000 Training Loss: 0.05750250816345215\n",
      "Epoch 6327/20000 Training Loss: 0.05785780027508736\n",
      "Epoch 6328/20000 Training Loss: 0.06970065087080002\n",
      "Epoch 6329/20000 Training Loss: 0.059015240520238876\n",
      "Epoch 6330/20000 Training Loss: 0.06675708293914795\n",
      "Epoch 6330/20000 Validation Loss: 0.08579027652740479\n",
      "Epoch 6331/20000 Training Loss: 0.053009822964668274\n",
      "Epoch 6332/20000 Training Loss: 0.06322070956230164\n",
      "Epoch 6333/20000 Training Loss: 0.05956810712814331\n",
      "Epoch 6334/20000 Training Loss: 0.06427783519029617\n",
      "Epoch 6335/20000 Training Loss: 0.07468021661043167\n",
      "Epoch 6336/20000 Training Loss: 0.0589575469493866\n",
      "Epoch 6337/20000 Training Loss: 0.06971634179353714\n",
      "Epoch 6338/20000 Training Loss: 0.03467315807938576\n",
      "Epoch 6339/20000 Training Loss: 0.06287109106779099\n",
      "Epoch 6340/20000 Training Loss: 0.06436268240213394\n",
      "Epoch 6340/20000 Validation Loss: 0.07741861045360565\n",
      "Epoch 6341/20000 Training Loss: 0.05991005897521973\n",
      "Epoch 6342/20000 Training Loss: 0.08364355564117432\n",
      "Epoch 6343/20000 Training Loss: 0.053668539971113205\n",
      "Epoch 6344/20000 Training Loss: 0.06932736188173294\n",
      "Epoch 6345/20000 Training Loss: 0.05154533311724663\n",
      "Epoch 6346/20000 Training Loss: 0.06150396540760994\n",
      "Epoch 6347/20000 Training Loss: 0.0639033243060112\n",
      "Epoch 6348/20000 Training Loss: 0.05364595726132393\n",
      "Epoch 6349/20000 Training Loss: 0.06144927814602852\n",
      "Epoch 6350/20000 Training Loss: 0.0434298999607563\n",
      "Epoch 6350/20000 Validation Loss: 0.06380947679281235\n",
      "Epoch 6351/20000 Training Loss: 0.0756298303604126\n",
      "Epoch 6352/20000 Training Loss: 0.07459638267755508\n",
      "Epoch 6353/20000 Training Loss: 0.05260265991091728\n",
      "Epoch 6354/20000 Training Loss: 0.04977808892726898\n",
      "Epoch 6355/20000 Training Loss: 0.058761510998010635\n",
      "Epoch 6356/20000 Training Loss: 0.07129409164190292\n",
      "Epoch 6357/20000 Training Loss: 0.056845974177122116\n",
      "Epoch 6358/20000 Training Loss: 0.06318647414445877\n",
      "Epoch 6359/20000 Training Loss: 0.050037238746881485\n",
      "Epoch 6360/20000 Training Loss: 0.051805537194013596\n",
      "Epoch 6360/20000 Validation Loss: 0.04608602821826935\n",
      "Epoch 6361/20000 Training Loss: 0.05180199816823006\n",
      "Epoch 6362/20000 Training Loss: 0.05662719905376434\n",
      "Epoch 6363/20000 Training Loss: 0.06329590827226639\n",
      "Epoch 6364/20000 Training Loss: 0.05255111679434776\n",
      "Epoch 6365/20000 Training Loss: 0.06495285779237747\n",
      "Epoch 6366/20000 Training Loss: 0.07971448451280594\n",
      "Epoch 6367/20000 Training Loss: 0.0714043453335762\n",
      "Epoch 6368/20000 Training Loss: 0.06647954136133194\n",
      "Epoch 6369/20000 Training Loss: 0.054522305727005005\n",
      "Epoch 6370/20000 Training Loss: 0.05944063141942024\n",
      "Epoch 6370/20000 Validation Loss: 0.07012856751680374\n",
      "Epoch 6371/20000 Training Loss: 0.056987613439559937\n",
      "Epoch 6372/20000 Training Loss: 0.05112403631210327\n",
      "Epoch 6373/20000 Training Loss: 0.06710517406463623\n",
      "Epoch 6374/20000 Training Loss: 0.05276459455490112\n",
      "Epoch 6375/20000 Training Loss: 0.05642713978886604\n",
      "Epoch 6376/20000 Training Loss: 0.06952302902936935\n",
      "Epoch 6377/20000 Training Loss: 0.05774437263607979\n",
      "Epoch 6378/20000 Training Loss: 0.0671631470322609\n",
      "Epoch 6379/20000 Training Loss: 0.07288971543312073\n",
      "Epoch 6380/20000 Training Loss: 0.045453161001205444\n",
      "Epoch 6380/20000 Validation Loss: 0.04388618469238281\n",
      "Epoch 6381/20000 Training Loss: 0.045043688267469406\n",
      "Epoch 6382/20000 Training Loss: 0.0659504309296608\n",
      "Epoch 6383/20000 Training Loss: 0.05185086652636528\n",
      "Epoch 6384/20000 Training Loss: 0.05096493288874626\n",
      "Epoch 6385/20000 Training Loss: 0.0772276297211647\n",
      "Epoch 6386/20000 Training Loss: 0.056198716163635254\n",
      "Epoch 6387/20000 Training Loss: 0.04607154801487923\n",
      "Epoch 6388/20000 Training Loss: 0.07135241478681564\n",
      "Epoch 6389/20000 Training Loss: 0.054214298725128174\n",
      "Epoch 6390/20000 Training Loss: 0.05382208153605461\n",
      "Epoch 6390/20000 Validation Loss: 0.07812511175870895\n",
      "Epoch 6391/20000 Training Loss: 0.06248162314295769\n",
      "Epoch 6392/20000 Training Loss: 0.05994406342506409\n",
      "Epoch 6393/20000 Training Loss: 0.052895914763212204\n",
      "Epoch 6394/20000 Training Loss: 0.047624990344047546\n",
      "Epoch 6395/20000 Training Loss: 0.03896030783653259\n",
      "Epoch 6396/20000 Training Loss: 0.06004170700907707\n",
      "Epoch 6397/20000 Training Loss: 0.055983077734708786\n",
      "Epoch 6398/20000 Training Loss: 0.05051340535283089\n",
      "Epoch 6399/20000 Training Loss: 0.06086321547627449\n",
      "Epoch 6400/20000 Training Loss: 0.06334979832172394\n",
      "Epoch 6400/20000 Validation Loss: 0.04039844870567322\n",
      "Epoch 6401/20000 Training Loss: 0.05445127189159393\n",
      "Epoch 6402/20000 Training Loss: 0.05140082538127899\n",
      "Epoch 6403/20000 Training Loss: 0.0737672671675682\n",
      "Epoch 6404/20000 Training Loss: 0.05332478880882263\n",
      "Epoch 6405/20000 Training Loss: 0.07319352775812149\n",
      "Epoch 6406/20000 Training Loss: 0.0631876066327095\n",
      "Epoch 6407/20000 Training Loss: 0.0671023353934288\n",
      "Epoch 6408/20000 Training Loss: 0.05356992781162262\n",
      "Epoch 6409/20000 Training Loss: 0.052510689944028854\n",
      "Epoch 6410/20000 Training Loss: 0.05336030200123787\n",
      "Epoch 6410/20000 Validation Loss: 0.0349588617682457\n",
      "Epoch 6411/20000 Training Loss: 0.06430364400148392\n",
      "Epoch 6412/20000 Training Loss: 0.05353793874382973\n",
      "Epoch 6413/20000 Training Loss: 0.08370841294527054\n",
      "Epoch 6414/20000 Training Loss: 0.05119754746556282\n",
      "Epoch 6415/20000 Training Loss: 0.0744556114077568\n",
      "Epoch 6416/20000 Training Loss: 0.054704006761312485\n",
      "Epoch 6417/20000 Training Loss: 0.04337115213274956\n",
      "Epoch 6418/20000 Training Loss: 0.05384548008441925\n",
      "Epoch 6419/20000 Training Loss: 0.06676158308982849\n",
      "Epoch 6420/20000 Training Loss: 0.07229260355234146\n",
      "Epoch 6420/20000 Validation Loss: 0.05508648604154587\n",
      "Epoch 6421/20000 Training Loss: 0.058730125427246094\n",
      "Epoch 6422/20000 Training Loss: 0.07968233525753021\n",
      "Epoch 6423/20000 Training Loss: 0.04069804027676582\n",
      "Epoch 6424/20000 Training Loss: 0.058550816029310226\n",
      "Epoch 6425/20000 Training Loss: 0.05781887099146843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6426/20000 Training Loss: 0.05308551713824272\n",
      "Epoch 6427/20000 Training Loss: 0.05749562010169029\n",
      "Epoch 6428/20000 Training Loss: 0.06587041169404984\n",
      "Epoch 6429/20000 Training Loss: 0.055205535143613815\n",
      "Epoch 6430/20000 Training Loss: 0.05948187783360481\n",
      "Epoch 6430/20000 Validation Loss: 0.06156326085329056\n",
      "Epoch 6431/20000 Training Loss: 0.050198864191770554\n",
      "Epoch 6432/20000 Training Loss: 0.04723869636654854\n",
      "Epoch 6433/20000 Training Loss: 0.06298595666885376\n",
      "Epoch 6434/20000 Training Loss: 0.0546993613243103\n",
      "Epoch 6435/20000 Training Loss: 0.07198506593704224\n",
      "Epoch 6436/20000 Training Loss: 0.0722399428486824\n",
      "Epoch 6437/20000 Training Loss: 0.058831121772527695\n",
      "Epoch 6438/20000 Training Loss: 0.04915912449359894\n",
      "Epoch 6439/20000 Training Loss: 0.04102102294564247\n",
      "Epoch 6440/20000 Training Loss: 0.05372751131653786\n",
      "Epoch 6440/20000 Validation Loss: 0.04171711951494217\n",
      "Epoch 6441/20000 Training Loss: 0.04612834379076958\n",
      "Epoch 6442/20000 Training Loss: 0.043159857392311096\n",
      "Epoch 6443/20000 Training Loss: 0.05788056179881096\n",
      "Epoch 6444/20000 Training Loss: 0.05800576135516167\n",
      "Epoch 6445/20000 Training Loss: 0.04938433691859245\n",
      "Epoch 6446/20000 Training Loss: 0.049133796244859695\n",
      "Epoch 6447/20000 Training Loss: 0.05979831516742706\n",
      "Epoch 6448/20000 Training Loss: 0.04882863163948059\n",
      "Epoch 6449/20000 Training Loss: 0.06298359483480453\n",
      "Epoch 6450/20000 Training Loss: 0.054343368858098984\n",
      "Epoch 6450/20000 Validation Loss: 0.0568266361951828\n",
      "Epoch 6451/20000 Training Loss: 0.06852734833955765\n",
      "Epoch 6452/20000 Training Loss: 0.056235846132040024\n",
      "Epoch 6453/20000 Training Loss: 0.0425473116338253\n",
      "Epoch 6454/20000 Training Loss: 0.060244593769311905\n",
      "Epoch 6455/20000 Training Loss: 0.053970638662576675\n",
      "Epoch 6456/20000 Training Loss: 0.05969977751374245\n",
      "Epoch 6457/20000 Training Loss: 0.04899036884307861\n",
      "Epoch 6458/20000 Training Loss: 0.04710221663117409\n",
      "Epoch 6459/20000 Training Loss: 0.05423982813954353\n",
      "Epoch 6460/20000 Training Loss: 0.056620437651872635\n",
      "Epoch 6460/20000 Validation Loss: 0.06083536148071289\n",
      "Epoch 6461/20000 Training Loss: 0.07307272404432297\n",
      "Epoch 6462/20000 Training Loss: 0.059976641088724136\n",
      "Epoch 6463/20000 Training Loss: 0.07234025001525879\n",
      "Epoch 6464/20000 Training Loss: 0.062255654484033585\n",
      "Epoch 6465/20000 Training Loss: 0.0534512959420681\n",
      "Epoch 6466/20000 Training Loss: 0.07446516305208206\n",
      "Epoch 6467/20000 Training Loss: 0.0833948478102684\n",
      "Epoch 6468/20000 Training Loss: 0.06520432978868484\n",
      "Epoch 6469/20000 Training Loss: 0.06053931638598442\n",
      "Epoch 6470/20000 Training Loss: 0.06011839583516121\n",
      "Epoch 6470/20000 Validation Loss: 0.042194753885269165\n",
      "Epoch 6471/20000 Training Loss: 0.06566869467496872\n",
      "Epoch 6472/20000 Training Loss: 0.051477301865816116\n",
      "Epoch 6473/20000 Training Loss: 0.06290332227945328\n",
      "Epoch 6474/20000 Training Loss: 0.05727918818593025\n",
      "Epoch 6475/20000 Training Loss: 0.06162684038281441\n",
      "Epoch 6476/20000 Training Loss: 0.06576241552829742\n",
      "Epoch 6477/20000 Training Loss: 0.04501531645655632\n",
      "Epoch 6478/20000 Training Loss: 0.04778178408741951\n",
      "Epoch 6479/20000 Training Loss: 0.05891914293169975\n",
      "Epoch 6480/20000 Training Loss: 0.057398512959480286\n",
      "Epoch 6480/20000 Validation Loss: 0.053819552063941956\n",
      "Epoch 6481/20000 Training Loss: 0.047142013907432556\n",
      "Epoch 6482/20000 Training Loss: 0.07729876786470413\n",
      "Epoch 6483/20000 Training Loss: 0.04506486654281616\n",
      "Epoch 6484/20000 Training Loss: 0.06859982758760452\n",
      "Epoch 6485/20000 Training Loss: 0.058897048234939575\n",
      "Epoch 6486/20000 Training Loss: 0.06279315054416656\n",
      "Epoch 6487/20000 Training Loss: 0.04052572324872017\n",
      "Epoch 6488/20000 Training Loss: 0.053789347410202026\n",
      "Epoch 6489/20000 Training Loss: 0.06470383703708649\n",
      "Epoch 6490/20000 Training Loss: 0.06206124648451805\n",
      "Epoch 6490/20000 Validation Loss: 0.04468756914138794\n",
      "Epoch 6491/20000 Training Loss: 0.06363480538129807\n",
      "Epoch 6492/20000 Training Loss: 0.05917962267994881\n",
      "Epoch 6493/20000 Training Loss: 0.053385693579912186\n",
      "Epoch 6494/20000 Training Loss: 0.052943289279937744\n",
      "Epoch 6495/20000 Training Loss: 0.0678754523396492\n",
      "Epoch 6496/20000 Training Loss: 0.04264451935887337\n",
      "Epoch 6497/20000 Training Loss: 0.05779609456658363\n",
      "Epoch 6498/20000 Training Loss: 0.055236201733350754\n",
      "Epoch 6499/20000 Training Loss: 0.036878447979688644\n",
      "Epoch 6500/20000 Training Loss: 0.05221325531601906\n",
      "Epoch 6500/20000 Validation Loss: 0.03925658017396927\n",
      "Epoch 6501/20000 Training Loss: 0.05933932587504387\n",
      "Epoch 6502/20000 Training Loss: 0.052968401461839676\n",
      "Epoch 6503/20000 Training Loss: 0.06732448190450668\n",
      "Epoch 6504/20000 Training Loss: 0.05414818599820137\n",
      "Epoch 6505/20000 Training Loss: 0.06716571003198624\n",
      "Epoch 6506/20000 Training Loss: 0.05743974447250366\n",
      "Epoch 6507/20000 Training Loss: 0.07946073263883591\n",
      "Epoch 6508/20000 Training Loss: 0.05246923863887787\n",
      "Epoch 6509/20000 Training Loss: 0.07351627200841904\n",
      "Epoch 6510/20000 Training Loss: 0.05950923264026642\n",
      "Epoch 6510/20000 Validation Loss: 0.08268094062805176\n",
      "Epoch 6511/20000 Training Loss: 0.04795774817466736\n",
      "Epoch 6512/20000 Training Loss: 0.08630410581827164\n",
      "Epoch 6513/20000 Training Loss: 0.060959238559007645\n",
      "Epoch 6514/20000 Training Loss: 0.06958595663309097\n",
      "Epoch 6515/20000 Training Loss: 0.06027649715542793\n",
      "Epoch 6516/20000 Training Loss: 0.06839103251695633\n",
      "Epoch 6517/20000 Training Loss: 0.054452527314424515\n",
      "Epoch 6518/20000 Training Loss: 0.04657195135951042\n",
      "Epoch 6519/20000 Training Loss: 0.04813888296484947\n",
      "Epoch 6520/20000 Training Loss: 0.06183949112892151\n",
      "Epoch 6520/20000 Validation Loss: 0.07578153163194656\n",
      "Epoch 6521/20000 Training Loss: 0.06192748248577118\n",
      "Epoch 6522/20000 Training Loss: 0.0504237562417984\n",
      "Epoch 6523/20000 Training Loss: 0.056500595062971115\n",
      "Epoch 6524/20000 Training Loss: 0.05549875274300575\n",
      "Epoch 6525/20000 Training Loss: 0.06937656551599503\n",
      "Epoch 6526/20000 Training Loss: 0.051772743463516235\n",
      "Epoch 6527/20000 Training Loss: 0.05540074408054352\n",
      "Epoch 6528/20000 Training Loss: 0.06335442513227463\n",
      "Epoch 6529/20000 Training Loss: 0.060759514570236206\n",
      "Epoch 6530/20000 Training Loss: 0.06910029798746109\n",
      "Epoch 6530/20000 Validation Loss: 0.03951475769281387\n",
      "Epoch 6531/20000 Training Loss: 0.050351087003946304\n",
      "Epoch 6532/20000 Training Loss: 0.06213490292429924\n",
      "Epoch 6533/20000 Training Loss: 0.05085579678416252\n",
      "Epoch 6534/20000 Training Loss: 0.04950587451457977\n",
      "Epoch 6535/20000 Training Loss: 0.05112546682357788\n",
      "Epoch 6536/20000 Training Loss: 0.06229398027062416\n",
      "Epoch 6537/20000 Training Loss: 0.04205961525440216\n",
      "Epoch 6538/20000 Training Loss: 0.057841818779706955\n",
      "Epoch 6539/20000 Training Loss: 0.05238790437579155\n",
      "Epoch 6540/20000 Training Loss: 0.04843733832240105\n",
      "Epoch 6540/20000 Validation Loss: 0.043937958776950836\n",
      "Epoch 6541/20000 Training Loss: 0.0707249641418457\n",
      "Epoch 6542/20000 Training Loss: 0.05491872504353523\n",
      "Epoch 6543/20000 Training Loss: 0.06140884384512901\n",
      "Epoch 6544/20000 Training Loss: 0.059214066714048386\n",
      "Epoch 6545/20000 Training Loss: 0.05494740605354309\n",
      "Epoch 6546/20000 Training Loss: 0.07051796466112137\n",
      "Epoch 6547/20000 Training Loss: 0.052061159163713455\n",
      "Epoch 6548/20000 Training Loss: 0.058400288224220276\n",
      "Epoch 6549/20000 Training Loss: 0.06681787222623825\n",
      "Epoch 6550/20000 Training Loss: 0.05685342475771904\n",
      "Epoch 6550/20000 Validation Loss: 0.04898781329393387\n",
      "Epoch 6551/20000 Training Loss: 0.062450360506772995\n",
      "Epoch 6552/20000 Training Loss: 0.07164213806390762\n",
      "Epoch 6553/20000 Training Loss: 0.05857168138027191\n",
      "Epoch 6554/20000 Training Loss: 0.03502046689391136\n",
      "Epoch 6555/20000 Training Loss: 0.051131561398506165\n",
      "Epoch 6556/20000 Training Loss: 0.05107170343399048\n",
      "Epoch 6557/20000 Training Loss: 0.07798809558153152\n",
      "Epoch 6558/20000 Training Loss: 0.062057096511125565\n",
      "Epoch 6559/20000 Training Loss: 0.038417115807533264\n",
      "Epoch 6560/20000 Training Loss: 0.06563472002744675\n",
      "Epoch 6560/20000 Validation Loss: 0.05532136559486389\n",
      "Epoch 6561/20000 Training Loss: 0.0500170923769474\n",
      "Epoch 6562/20000 Training Loss: 0.05601968243718147\n",
      "Epoch 6563/20000 Training Loss: 0.054497119039297104\n",
      "Epoch 6564/20000 Training Loss: 0.06677427887916565\n",
      "Epoch 6565/20000 Training Loss: 0.05669237673282623\n",
      "Epoch 6566/20000 Training Loss: 0.06506148725748062\n",
      "Epoch 6567/20000 Training Loss: 0.04607664421200752\n",
      "Epoch 6568/20000 Training Loss: 0.05886152386665344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6569/20000 Training Loss: 0.04179367795586586\n",
      "Epoch 6570/20000 Training Loss: 0.07036631554365158\n",
      "Epoch 6570/20000 Validation Loss: 0.07009919732809067\n",
      "Epoch 6571/20000 Training Loss: 0.08289767056703568\n",
      "Epoch 6572/20000 Training Loss: 0.05445192754268646\n",
      "Epoch 6573/20000 Training Loss: 0.06279294937849045\n",
      "Epoch 6574/20000 Training Loss: 0.041906069964170456\n",
      "Epoch 6575/20000 Training Loss: 0.053387600928545\n",
      "Epoch 6576/20000 Training Loss: 0.04867398738861084\n",
      "Epoch 6577/20000 Training Loss: 0.06472448259592056\n",
      "Epoch 6578/20000 Training Loss: 0.05351917818188667\n",
      "Epoch 6579/20000 Training Loss: 0.08221321552991867\n",
      "Epoch 6580/20000 Training Loss: 0.06613101065158844\n",
      "Epoch 6580/20000 Validation Loss: 0.05632324516773224\n",
      "Epoch 6581/20000 Training Loss: 0.04901592805981636\n",
      "Epoch 6582/20000 Training Loss: 0.05416904017329216\n",
      "Epoch 6583/20000 Training Loss: 0.04181909188628197\n",
      "Epoch 6584/20000 Training Loss: 0.08186150342226028\n",
      "Epoch 6585/20000 Training Loss: 0.05944892391562462\n",
      "Epoch 6586/20000 Training Loss: 0.062247853726148605\n",
      "Epoch 6587/20000 Training Loss: 0.054064538329839706\n",
      "Epoch 6588/20000 Training Loss: 0.046391308307647705\n",
      "Epoch 6589/20000 Training Loss: 0.045386090874671936\n",
      "Epoch 6590/20000 Training Loss: 0.07187706977128983\n",
      "Epoch 6590/20000 Validation Loss: 0.07399921119213104\n",
      "Epoch 6591/20000 Training Loss: 0.04355030879378319\n",
      "Epoch 6592/20000 Training Loss: 0.05931563302874565\n",
      "Epoch 6593/20000 Training Loss: 0.04997551813721657\n",
      "Epoch 6594/20000 Training Loss: 0.059939902275800705\n",
      "Epoch 6595/20000 Training Loss: 0.07917550951242447\n",
      "Epoch 6596/20000 Training Loss: 0.06946130841970444\n",
      "Epoch 6597/20000 Training Loss: 0.05842878296971321\n",
      "Epoch 6598/20000 Training Loss: 0.03999119624495506\n",
      "Epoch 6599/20000 Training Loss: 0.04742717742919922\n",
      "Epoch 6600/20000 Training Loss: 0.058772455900907516\n",
      "Epoch 6600/20000 Validation Loss: 0.052358657121658325\n",
      "Epoch 6601/20000 Training Loss: 0.0495738685131073\n",
      "Epoch 6602/20000 Training Loss: 0.051357824355363846\n",
      "Epoch 6603/20000 Training Loss: 0.04348655045032501\n",
      "Epoch 6604/20000 Training Loss: 0.05197193846106529\n",
      "Epoch 6605/20000 Training Loss: 0.06794463098049164\n",
      "Epoch 6606/20000 Training Loss: 0.045722465962171555\n",
      "Epoch 6607/20000 Training Loss: 0.061455171555280685\n",
      "Epoch 6608/20000 Training Loss: 0.04226701334118843\n",
      "Epoch 6609/20000 Training Loss: 0.05587909743189812\n",
      "Epoch 6610/20000 Training Loss: 0.05920231342315674\n",
      "Epoch 6610/20000 Validation Loss: 0.04643966257572174\n",
      "Epoch 6611/20000 Training Loss: 0.039016980677843094\n",
      "Epoch 6612/20000 Training Loss: 0.044580910354852676\n",
      "Epoch 6613/20000 Training Loss: 0.05860090255737305\n",
      "Epoch 6614/20000 Training Loss: 0.06217648461461067\n",
      "Epoch 6615/20000 Training Loss: 0.06632549315690994\n",
      "Epoch 6616/20000 Training Loss: 0.04572555050253868\n",
      "Epoch 6617/20000 Training Loss: 0.06757400184869766\n",
      "Epoch 6618/20000 Training Loss: 0.059434134513139725\n",
      "Epoch 6619/20000 Training Loss: 0.06908050924539566\n",
      "Epoch 6620/20000 Training Loss: 0.05434026941657066\n",
      "Epoch 6620/20000 Validation Loss: 0.04871673136949539\n",
      "Epoch 6621/20000 Training Loss: 0.05262916907668114\n",
      "Epoch 6622/20000 Training Loss: 0.04006714001297951\n",
      "Epoch 6623/20000 Training Loss: 0.04780428484082222\n",
      "Epoch 6624/20000 Training Loss: 0.055727213621139526\n",
      "Epoch 6625/20000 Training Loss: 0.04407225176692009\n",
      "Epoch 6626/20000 Training Loss: 0.05408551171422005\n",
      "Epoch 6627/20000 Training Loss: 0.05833125114440918\n",
      "Epoch 6628/20000 Training Loss: 0.05649597942829132\n",
      "Epoch 6629/20000 Training Loss: 0.045009125024080276\n",
      "Epoch 6630/20000 Training Loss: 0.06765388697385788\n",
      "Epoch 6630/20000 Validation Loss: 0.06883284449577332\n",
      "Epoch 6631/20000 Training Loss: 0.05116748809814453\n",
      "Epoch 6632/20000 Training Loss: 0.051155198365449905\n",
      "Epoch 6633/20000 Training Loss: 0.06957260519266129\n",
      "Epoch 6634/20000 Training Loss: 0.07460594922304153\n",
      "Epoch 6635/20000 Training Loss: 0.05227627977728844\n",
      "Epoch 6636/20000 Training Loss: 0.04728551581501961\n",
      "Epoch 6637/20000 Training Loss: 0.06346844881772995\n",
      "Epoch 6638/20000 Training Loss: 0.0655311793088913\n",
      "Epoch 6639/20000 Training Loss: 0.064080148935318\n",
      "Epoch 6640/20000 Training Loss: 0.0825219675898552\n",
      "Epoch 6640/20000 Validation Loss: 0.05226841941475868\n",
      "Epoch 6641/20000 Training Loss: 0.061938006430864334\n",
      "Epoch 6642/20000 Training Loss: 0.05578622221946716\n",
      "Epoch 6643/20000 Training Loss: 0.039571017026901245\n",
      "Epoch 6644/20000 Training Loss: 0.04628627374768257\n",
      "Epoch 6645/20000 Training Loss: 0.0588766448199749\n",
      "Epoch 6646/20000 Training Loss: 0.0639619529247284\n",
      "Epoch 6647/20000 Training Loss: 0.041642796248197556\n",
      "Epoch 6648/20000 Training Loss: 0.06627330929040909\n",
      "Epoch 6649/20000 Training Loss: 0.05035798251628876\n",
      "Epoch 6650/20000 Training Loss: 0.05921972915530205\n",
      "Epoch 6650/20000 Validation Loss: 0.09298036247491837\n",
      "Epoch 6651/20000 Training Loss: 0.061533402651548386\n",
      "Epoch 6652/20000 Training Loss: 0.06216563284397125\n",
      "Epoch 6653/20000 Training Loss: 0.05327503755688667\n",
      "Epoch 6654/20000 Training Loss: 0.04050338268280029\n",
      "Epoch 6655/20000 Training Loss: 0.05382947251200676\n",
      "Epoch 6656/20000 Training Loss: 0.06229565665125847\n",
      "Epoch 6657/20000 Training Loss: 0.07350817322731018\n",
      "Epoch 6658/20000 Training Loss: 0.0436692088842392\n",
      "Epoch 6659/20000 Training Loss: 0.0519137941300869\n",
      "Epoch 6660/20000 Training Loss: 0.059036631137132645\n",
      "Epoch 6660/20000 Validation Loss: 0.06623540818691254\n",
      "Epoch 6661/20000 Training Loss: 0.06225321814417839\n",
      "Epoch 6662/20000 Training Loss: 0.061233337968587875\n",
      "Epoch 6663/20000 Training Loss: 0.04317375645041466\n",
      "Epoch 6664/20000 Training Loss: 0.0601210854947567\n",
      "Epoch 6665/20000 Training Loss: 0.05793476477265358\n",
      "Epoch 6666/20000 Training Loss: 0.049677103757858276\n",
      "Epoch 6667/20000 Training Loss: 0.07287799566984177\n",
      "Epoch 6668/20000 Training Loss: 0.044678110629320145\n",
      "Epoch 6669/20000 Training Loss: 0.06002067029476166\n",
      "Epoch 6670/20000 Training Loss: 0.055467650294303894\n",
      "Epoch 6670/20000 Validation Loss: 0.06910271942615509\n",
      "Epoch 6671/20000 Training Loss: 0.06842508167028427\n",
      "Epoch 6672/20000 Training Loss: 0.05930690839886665\n",
      "Epoch 6673/20000 Training Loss: 0.07307807356119156\n",
      "Epoch 6674/20000 Training Loss: 0.08009889721870422\n",
      "Epoch 6675/20000 Training Loss: 0.04932122305035591\n",
      "Epoch 6676/20000 Training Loss: 0.06942193955183029\n",
      "Epoch 6677/20000 Training Loss: 0.04647095128893852\n",
      "Epoch 6678/20000 Training Loss: 0.06224346533417702\n",
      "Epoch 6679/20000 Training Loss: 0.05763204023241997\n",
      "Epoch 6680/20000 Training Loss: 0.06027728319168091\n",
      "Epoch 6680/20000 Validation Loss: 0.044819850474596024\n",
      "Epoch 6681/20000 Training Loss: 0.050259798765182495\n",
      "Epoch 6682/20000 Training Loss: 0.049468159675598145\n",
      "Epoch 6683/20000 Training Loss: 0.060468584299087524\n",
      "Epoch 6684/20000 Training Loss: 0.04072532430291176\n",
      "Epoch 6685/20000 Training Loss: 0.04963117837905884\n",
      "Epoch 6686/20000 Training Loss: 0.041082531213760376\n",
      "Epoch 6687/20000 Training Loss: 0.048989247530698776\n",
      "Epoch 6688/20000 Training Loss: 0.05507303774356842\n",
      "Epoch 6689/20000 Training Loss: 0.061058104038238525\n",
      "Epoch 6690/20000 Training Loss: 0.04260053113102913\n",
      "Epoch 6690/20000 Validation Loss: 0.04107086360454559\n",
      "Epoch 6691/20000 Training Loss: 0.06569486856460571\n",
      "Epoch 6692/20000 Training Loss: 0.0517784059047699\n",
      "Epoch 6693/20000 Training Loss: 0.05100627616047859\n",
      "Epoch 6694/20000 Training Loss: 0.04880271479487419\n",
      "Epoch 6695/20000 Training Loss: 0.08434177190065384\n",
      "Epoch 6696/20000 Training Loss: 0.06171305477619171\n",
      "Epoch 6697/20000 Training Loss: 0.05934819579124451\n",
      "Epoch 6698/20000 Training Loss: 0.08346748352050781\n",
      "Epoch 6699/20000 Training Loss: 0.05622270330786705\n",
      "Epoch 6700/20000 Training Loss: 0.04661446809768677\n",
      "Epoch 6700/20000 Validation Loss: 0.06213914975523949\n",
      "Epoch 6701/20000 Training Loss: 0.06706880778074265\n",
      "Epoch 6702/20000 Training Loss: 0.05862683057785034\n",
      "Epoch 6703/20000 Training Loss: 0.0575835220515728\n",
      "Epoch 6704/20000 Training Loss: 0.0613722950220108\n",
      "Epoch 6705/20000 Training Loss: 0.07397875189781189\n",
      "Epoch 6706/20000 Training Loss: 0.06073112413287163\n",
      "Epoch 6707/20000 Training Loss: 0.05544626712799072\n",
      "Epoch 6708/20000 Training Loss: 0.04338883236050606\n",
      "Epoch 6709/20000 Training Loss: 0.0609889030456543\n",
      "Epoch 6710/20000 Training Loss: 0.05479404330253601\n",
      "Epoch 6710/20000 Validation Loss: 0.05111803859472275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6711/20000 Training Loss: 0.055421024560928345\n",
      "Epoch 6712/20000 Training Loss: 0.045690108090639114\n",
      "Epoch 6713/20000 Training Loss: 0.06633193045854568\n",
      "Epoch 6714/20000 Training Loss: 0.03403618559241295\n",
      "Epoch 6715/20000 Training Loss: 0.05901573598384857\n",
      "Epoch 6716/20000 Training Loss: 0.0509859062731266\n",
      "Epoch 6717/20000 Training Loss: 0.06898245960474014\n",
      "Epoch 6718/20000 Training Loss: 0.047246504575014114\n",
      "Epoch 6719/20000 Training Loss: 0.05669954791665077\n",
      "Epoch 6720/20000 Training Loss: 0.07054278999567032\n",
      "Epoch 6720/20000 Validation Loss: 0.06742139160633087\n",
      "Epoch 6721/20000 Training Loss: 0.07752307504415512\n",
      "Epoch 6722/20000 Training Loss: 0.04460820555686951\n",
      "Epoch 6723/20000 Training Loss: 0.061116497963666916\n",
      "Epoch 6724/20000 Training Loss: 0.05607904866337776\n",
      "Epoch 6725/20000 Training Loss: 0.061457086354494095\n",
      "Epoch 6726/20000 Training Loss: 0.07621536403894424\n",
      "Epoch 6727/20000 Training Loss: 0.054307471960783005\n",
      "Epoch 6728/20000 Training Loss: 0.04772983863949776\n",
      "Epoch 6729/20000 Training Loss: 0.043452102690935135\n",
      "Epoch 6730/20000 Training Loss: 0.05293785035610199\n",
      "Epoch 6730/20000 Validation Loss: 0.07218951731920242\n",
      "Epoch 6731/20000 Training Loss: 0.06900935620069504\n",
      "Epoch 6732/20000 Training Loss: 0.05205722525715828\n",
      "Epoch 6733/20000 Training Loss: 0.046528756618499756\n",
      "Epoch 6734/20000 Training Loss: 0.07130099087953568\n",
      "Epoch 6735/20000 Training Loss: 0.04408175125718117\n",
      "Epoch 6736/20000 Training Loss: 0.056760523468256\n",
      "Epoch 6737/20000 Training Loss: 0.0672178789973259\n",
      "Epoch 6738/20000 Training Loss: 0.06259345263242722\n",
      "Epoch 6739/20000 Training Loss: 0.05759667232632637\n",
      "Epoch 6740/20000 Training Loss: 0.06329793483018875\n",
      "Epoch 6740/20000 Validation Loss: 0.06952323019504547\n",
      "Epoch 6741/20000 Training Loss: 0.057304609566926956\n",
      "Epoch 6742/20000 Training Loss: 0.05056603625416756\n",
      "Epoch 6743/20000 Training Loss: 0.04974539205431938\n",
      "Epoch 6744/20000 Training Loss: 0.06543675810098648\n",
      "Epoch 6745/20000 Training Loss: 0.047559380531311035\n",
      "Epoch 6746/20000 Training Loss: 0.040833573788404465\n",
      "Epoch 6747/20000 Training Loss: 0.045775193721055984\n",
      "Epoch 6748/20000 Training Loss: 0.04016786068677902\n",
      "Epoch 6749/20000 Training Loss: 0.05092128738760948\n",
      "Epoch 6750/20000 Training Loss: 0.06244942545890808\n",
      "Epoch 6750/20000 Validation Loss: 0.059779033064842224\n",
      "Epoch 6751/20000 Training Loss: 0.0468703918159008\n",
      "Epoch 6752/20000 Training Loss: 0.07639596611261368\n",
      "Epoch 6753/20000 Training Loss: 0.061402808874845505\n",
      "Epoch 6754/20000 Training Loss: 0.0451374351978302\n",
      "Epoch 6755/20000 Training Loss: 0.06798718124628067\n",
      "Epoch 6756/20000 Training Loss: 0.062176208943128586\n",
      "Epoch 6757/20000 Training Loss: 0.05391118302941322\n",
      "Epoch 6758/20000 Training Loss: 0.10368231683969498\n",
      "Epoch 6759/20000 Training Loss: 0.0363784022629261\n",
      "Epoch 6760/20000 Training Loss: 0.06948583573102951\n",
      "Epoch 6760/20000 Validation Loss: 0.0651102364063263\n",
      "Epoch 6761/20000 Training Loss: 0.055378902703523636\n",
      "Epoch 6762/20000 Training Loss: 0.05659956857562065\n",
      "Epoch 6763/20000 Training Loss: 0.05772770568728447\n",
      "Epoch 6764/20000 Training Loss: 0.08019677549600601\n",
      "Epoch 6765/20000 Training Loss: 0.06383802741765976\n",
      "Epoch 6766/20000 Training Loss: 0.06736976653337479\n",
      "Epoch 6767/20000 Training Loss: 0.059891242533922195\n",
      "Epoch 6768/20000 Training Loss: 0.07050084322690964\n",
      "Epoch 6769/20000 Training Loss: 0.06229386851191521\n",
      "Epoch 6770/20000 Training Loss: 0.0708145871758461\n",
      "Epoch 6770/20000 Validation Loss: 0.046402618288993835\n",
      "Epoch 6771/20000 Training Loss: 0.06856131553649902\n",
      "Epoch 6772/20000 Training Loss: 0.049829836934804916\n",
      "Epoch 6773/20000 Training Loss: 0.08399844169616699\n",
      "Epoch 6774/20000 Training Loss: 0.037585824728012085\n",
      "Epoch 6775/20000 Training Loss: 0.04639799892902374\n",
      "Epoch 6776/20000 Training Loss: 0.05926290526986122\n",
      "Epoch 6777/20000 Training Loss: 0.05598709359765053\n",
      "Epoch 6778/20000 Training Loss: 0.05483664199709892\n",
      "Epoch 6779/20000 Training Loss: 0.06559183448553085\n",
      "Epoch 6780/20000 Training Loss: 0.040871623903512955\n",
      "Epoch 6780/20000 Validation Loss: 0.07939037680625916\n",
      "Epoch 6781/20000 Training Loss: 0.04896746948361397\n",
      "Epoch 6782/20000 Training Loss: 0.05561566352844238\n",
      "Epoch 6783/20000 Training Loss: 0.05706281587481499\n",
      "Epoch 6784/20000 Training Loss: 0.07994470745325089\n",
      "Epoch 6785/20000 Training Loss: 0.062012653797864914\n",
      "Epoch 6786/20000 Training Loss: 0.08158514648675919\n",
      "Epoch 6787/20000 Training Loss: 0.05210940167307854\n",
      "Epoch 6788/20000 Training Loss: 0.0396525040268898\n",
      "Epoch 6789/20000 Training Loss: 0.059632558375597\n",
      "Epoch 6790/20000 Training Loss: 0.04839016869664192\n",
      "Epoch 6790/20000 Validation Loss: 0.08802087604999542\n",
      "Epoch 6791/20000 Training Loss: 0.05197899043560028\n",
      "Epoch 6792/20000 Training Loss: 0.06722711771726608\n",
      "Epoch 6793/20000 Training Loss: 0.05676880478858948\n",
      "Epoch 6794/20000 Training Loss: 0.05314575135707855\n",
      "Epoch 6795/20000 Training Loss: 0.06538750231266022\n",
      "Epoch 6796/20000 Training Loss: 0.05320459604263306\n",
      "Epoch 6797/20000 Training Loss: 0.06311453133821487\n",
      "Epoch 6798/20000 Training Loss: 0.05188557505607605\n",
      "Epoch 6799/20000 Training Loss: 0.046162188053131104\n",
      "Epoch 6800/20000 Training Loss: 0.06099453195929527\n",
      "Epoch 6800/20000 Validation Loss: 0.05574209988117218\n",
      "Epoch 6801/20000 Training Loss: 0.04509351775050163\n",
      "Epoch 6802/20000 Training Loss: 0.06057989224791527\n",
      "Epoch 6803/20000 Training Loss: 0.0765850767493248\n",
      "Epoch 6804/20000 Training Loss: 0.06864840537309647\n",
      "Epoch 6805/20000 Training Loss: 0.050387024879455566\n",
      "Epoch 6806/20000 Training Loss: 0.052443355321884155\n",
      "Epoch 6807/20000 Training Loss: 0.06552813202142715\n",
      "Epoch 6808/20000 Training Loss: 0.05729446932673454\n",
      "Epoch 6809/20000 Training Loss: 0.059438031166791916\n",
      "Epoch 6810/20000 Training Loss: 0.06199365481734276\n",
      "Epoch 6810/20000 Validation Loss: 0.060272786766290665\n",
      "Epoch 6811/20000 Training Loss: 0.059083759784698486\n",
      "Epoch 6812/20000 Training Loss: 0.06915213912725449\n",
      "Epoch 6813/20000 Training Loss: 0.05317525193095207\n",
      "Epoch 6814/20000 Training Loss: 0.06014300510287285\n",
      "Epoch 6815/20000 Training Loss: 0.07027389854192734\n",
      "Epoch 6816/20000 Training Loss: 0.05535166338086128\n",
      "Epoch 6817/20000 Training Loss: 0.046710893511772156\n",
      "Epoch 6818/20000 Training Loss: 0.064055435359478\n",
      "Epoch 6819/20000 Training Loss: 0.042432114481925964\n",
      "Epoch 6820/20000 Training Loss: 0.06355997174978256\n",
      "Epoch 6820/20000 Validation Loss: 0.06173166632652283\n",
      "Epoch 6821/20000 Training Loss: 0.062135349959135056\n",
      "Epoch 6822/20000 Training Loss: 0.059118807315826416\n",
      "Epoch 6823/20000 Training Loss: 0.04382467269897461\n",
      "Epoch 6824/20000 Training Loss: 0.06550774723291397\n",
      "Epoch 6825/20000 Training Loss: 0.044346731156110764\n",
      "Epoch 6826/20000 Training Loss: 0.06353513151407242\n",
      "Epoch 6827/20000 Training Loss: 0.06288569420576096\n",
      "Epoch 6828/20000 Training Loss: 0.0593532919883728\n",
      "Epoch 6829/20000 Training Loss: 0.06099337339401245\n",
      "Epoch 6830/20000 Training Loss: 0.07421877980232239\n",
      "Epoch 6830/20000 Validation Loss: 0.08615525811910629\n",
      "Epoch 6831/20000 Training Loss: 0.05783683434128761\n",
      "Epoch 6832/20000 Training Loss: 0.05872933939099312\n",
      "Epoch 6833/20000 Training Loss: 0.06993211805820465\n",
      "Epoch 6834/20000 Training Loss: 0.0534670390188694\n",
      "Epoch 6835/20000 Training Loss: 0.055407922714948654\n",
      "Epoch 6836/20000 Training Loss: 0.04899132251739502\n",
      "Epoch 6837/20000 Training Loss: 0.04859328642487526\n",
      "Epoch 6838/20000 Training Loss: 0.04178311303257942\n",
      "Epoch 6839/20000 Training Loss: 0.07270121574401855\n",
      "Epoch 6840/20000 Training Loss: 0.07281690090894699\n",
      "Epoch 6840/20000 Validation Loss: 0.0827891081571579\n",
      "Epoch 6841/20000 Training Loss: 0.06729991734027863\n",
      "Epoch 6842/20000 Training Loss: 0.05763716623187065\n",
      "Epoch 6843/20000 Training Loss: 0.07814491540193558\n",
      "Epoch 6844/20000 Training Loss: 0.05282324552536011\n",
      "Epoch 6845/20000 Training Loss: 0.057354748249053955\n",
      "Epoch 6846/20000 Training Loss: 0.05890434980392456\n",
      "Epoch 6847/20000 Training Loss: 0.04601331055164337\n",
      "Epoch 6848/20000 Training Loss: 0.0653143897652626\n",
      "Epoch 6849/20000 Training Loss: 0.053629498928785324\n",
      "Epoch 6850/20000 Training Loss: 0.03719916567206383\n",
      "Epoch 6850/20000 Validation Loss: 0.04644879698753357\n",
      "Epoch 6851/20000 Training Loss: 0.06617071479558945\n",
      "Epoch 6852/20000 Training Loss: 0.03981659933924675\n",
      "Epoch 6853/20000 Training Loss: 0.04798858240246773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6854/20000 Training Loss: 0.0542956180870533\n",
      "Epoch 6855/20000 Training Loss: 0.06111741065979004\n",
      "Epoch 6856/20000 Training Loss: 0.05744657292962074\n",
      "Epoch 6857/20000 Training Loss: 0.06681611388921738\n",
      "Epoch 6858/20000 Training Loss: 0.0472283661365509\n",
      "Epoch 6859/20000 Training Loss: 0.08330118656158447\n",
      "Epoch 6860/20000 Training Loss: 0.06607339531183243\n",
      "Epoch 6860/20000 Validation Loss: 0.05218425393104553\n",
      "Epoch 6861/20000 Training Loss: 0.055156927555799484\n",
      "Epoch 6862/20000 Training Loss: 0.07652229815721512\n",
      "Epoch 6863/20000 Training Loss: 0.05869145318865776\n",
      "Epoch 6864/20000 Training Loss: 0.055761273950338364\n",
      "Epoch 6865/20000 Training Loss: 0.05251610279083252\n",
      "Epoch 6866/20000 Training Loss: 0.06304940581321716\n",
      "Epoch 6867/20000 Training Loss: 0.05477131903171539\n",
      "Epoch 6868/20000 Training Loss: 0.06578934192657471\n",
      "Epoch 6869/20000 Training Loss: 0.06120802089571953\n",
      "Epoch 6870/20000 Training Loss: 0.07438252121210098\n",
      "Epoch 6870/20000 Validation Loss: 0.06569285690784454\n",
      "Epoch 6871/20000 Training Loss: 0.06803049892187119\n",
      "Epoch 6872/20000 Training Loss: 0.05493193864822388\n",
      "Epoch 6873/20000 Training Loss: 0.049402717500925064\n",
      "Epoch 6874/20000 Training Loss: 0.05693848058581352\n",
      "Epoch 6875/20000 Training Loss: 0.059383004903793335\n",
      "Epoch 6876/20000 Training Loss: 0.04776082932949066\n",
      "Epoch 6877/20000 Training Loss: 0.06814295798540115\n",
      "Epoch 6878/20000 Training Loss: 0.043545201420784\n",
      "Epoch 6879/20000 Training Loss: 0.05576101318001747\n",
      "Epoch 6880/20000 Training Loss: 0.06389297544956207\n",
      "Epoch 6880/20000 Validation Loss: 0.040885042399168015\n",
      "Epoch 6881/20000 Training Loss: 0.05360208824276924\n",
      "Epoch 6882/20000 Training Loss: 0.06684994697570801\n",
      "Epoch 6883/20000 Training Loss: 0.06001933291554451\n",
      "Epoch 6884/20000 Training Loss: 0.049657952040433884\n",
      "Epoch 6885/20000 Training Loss: 0.03832617402076721\n",
      "Epoch 6886/20000 Training Loss: 0.06688719242811203\n",
      "Epoch 6887/20000 Training Loss: 0.06475802510976791\n",
      "Epoch 6888/20000 Training Loss: 0.07409292459487915\n",
      "Epoch 6889/20000 Training Loss: 0.040709033608436584\n",
      "Epoch 6890/20000 Training Loss: 0.05559870973229408\n",
      "Epoch 6890/20000 Validation Loss: 0.03808087110519409\n",
      "Epoch 6891/20000 Training Loss: 0.06113770604133606\n",
      "Epoch 6892/20000 Training Loss: 0.04917259141802788\n",
      "Epoch 6893/20000 Training Loss: 0.05913625285029411\n",
      "Epoch 6894/20000 Training Loss: 0.05248970910906792\n",
      "Epoch 6895/20000 Training Loss: 0.04809832572937012\n",
      "Epoch 6896/20000 Training Loss: 0.04255881905555725\n",
      "Epoch 6897/20000 Training Loss: 0.06153959408402443\n",
      "Epoch 6898/20000 Training Loss: 0.05983543395996094\n",
      "Epoch 6899/20000 Training Loss: 0.06108703091740608\n",
      "Epoch 6900/20000 Training Loss: 0.0553385354578495\n",
      "Epoch 6900/20000 Validation Loss: 0.05540572106838226\n",
      "Epoch 6901/20000 Training Loss: 0.06819569319486618\n",
      "Epoch 6902/20000 Training Loss: 0.06520291417837143\n",
      "Epoch 6903/20000 Training Loss: 0.04562993720173836\n",
      "Epoch 6904/20000 Training Loss: 0.06474514305591583\n",
      "Epoch 6905/20000 Training Loss: 0.05716376379132271\n",
      "Epoch 6906/20000 Training Loss: 0.05537329241633415\n",
      "Epoch 6907/20000 Training Loss: 0.05742454156279564\n",
      "Epoch 6908/20000 Training Loss: 0.041051898151636124\n",
      "Epoch 6909/20000 Training Loss: 0.047823067754507065\n",
      "Epoch 6910/20000 Training Loss: 0.06069665774703026\n",
      "Epoch 6910/20000 Validation Loss: 0.05457429960370064\n",
      "Epoch 6911/20000 Training Loss: 0.05088348686695099\n",
      "Epoch 6912/20000 Training Loss: 0.05261186137795448\n",
      "Epoch 6913/20000 Training Loss: 0.06074092164635658\n",
      "Epoch 6914/20000 Training Loss: 0.06779985874891281\n",
      "Epoch 6915/20000 Training Loss: 0.05545768141746521\n",
      "Epoch 6916/20000 Training Loss: 0.0646650567650795\n",
      "Epoch 6917/20000 Training Loss: 0.07455265522003174\n",
      "Epoch 6918/20000 Training Loss: 0.052709806710481644\n",
      "Epoch 6919/20000 Training Loss: 0.051989395171403885\n",
      "Epoch 6920/20000 Training Loss: 0.04185402765870094\n",
      "Epoch 6920/20000 Validation Loss: 0.052901823073625565\n",
      "Epoch 6921/20000 Training Loss: 0.05739597603678703\n",
      "Epoch 6922/20000 Training Loss: 0.057090431451797485\n",
      "Epoch 6923/20000 Training Loss: 0.0573885440826416\n",
      "Epoch 6924/20000 Training Loss: 0.0633559301495552\n",
      "Epoch 6925/20000 Training Loss: 0.052675943821668625\n",
      "Epoch 6926/20000 Training Loss: 0.048235755413770676\n",
      "Epoch 6927/20000 Training Loss: 0.0629524514079094\n",
      "Epoch 6928/20000 Training Loss: 0.06425663828849792\n",
      "Epoch 6929/20000 Training Loss: 0.06479331105947495\n",
      "Epoch 6930/20000 Training Loss: 0.058925241231918335\n",
      "Epoch 6930/20000 Validation Loss: 0.07932332903146744\n",
      "Epoch 6931/20000 Training Loss: 0.06379751861095428\n",
      "Epoch 6932/20000 Training Loss: 0.045491915196180344\n",
      "Epoch 6933/20000 Training Loss: 0.05935259535908699\n",
      "Epoch 6934/20000 Training Loss: 0.05893947556614876\n",
      "Epoch 6935/20000 Training Loss: 0.08036460727453232\n",
      "Epoch 6936/20000 Training Loss: 0.052861616015434265\n",
      "Epoch 6937/20000 Training Loss: 0.04775778576731682\n",
      "Epoch 6938/20000 Training Loss: 0.06436289101839066\n",
      "Epoch 6939/20000 Training Loss: 0.06896653026342392\n",
      "Epoch 6940/20000 Training Loss: 0.04697352647781372\n",
      "Epoch 6940/20000 Validation Loss: 0.0884537473320961\n",
      "Epoch 6941/20000 Training Loss: 0.044393476098775864\n",
      "Epoch 6942/20000 Training Loss: 0.059005480259656906\n",
      "Epoch 6943/20000 Training Loss: 0.06627076864242554\n",
      "Epoch 6944/20000 Training Loss: 0.05012675002217293\n",
      "Epoch 6945/20000 Training Loss: 0.03821819648146629\n",
      "Epoch 6946/20000 Training Loss: 0.07017955929040909\n",
      "Epoch 6947/20000 Training Loss: 0.053909629583358765\n",
      "Epoch 6948/20000 Training Loss: 0.06244310736656189\n",
      "Epoch 6949/20000 Training Loss: 0.04670486971735954\n",
      "Epoch 6950/20000 Training Loss: 0.05826693773269653\n",
      "Epoch 6950/20000 Validation Loss: 0.05015571415424347\n",
      "Epoch 6951/20000 Training Loss: 0.055810462683439255\n",
      "Epoch 6952/20000 Training Loss: 0.04454740509390831\n",
      "Epoch 6953/20000 Training Loss: 0.07091078162193298\n",
      "Epoch 6954/20000 Training Loss: 0.04987456277012825\n",
      "Epoch 6955/20000 Training Loss: 0.0552884042263031\n",
      "Epoch 6956/20000 Training Loss: 0.06492897123098373\n",
      "Epoch 6957/20000 Training Loss: 0.04935180023312569\n",
      "Epoch 6958/20000 Training Loss: 0.06479597836732864\n",
      "Epoch 6959/20000 Training Loss: 0.05308085307478905\n",
      "Epoch 6960/20000 Training Loss: 0.061164382845163345\n",
      "Epoch 6960/20000 Validation Loss: 0.0619591660797596\n",
      "Epoch 6961/20000 Training Loss: 0.060778290033340454\n",
      "Epoch 6962/20000 Training Loss: 0.07858770340681076\n",
      "Epoch 6963/20000 Training Loss: 0.05527111887931824\n",
      "Epoch 6964/20000 Training Loss: 0.05741344392299652\n",
      "Epoch 6965/20000 Training Loss: 0.046063195914030075\n",
      "Epoch 6966/20000 Training Loss: 0.06823110580444336\n",
      "Epoch 6967/20000 Training Loss: 0.07313916832208633\n",
      "Epoch 6968/20000 Training Loss: 0.06224634870886803\n",
      "Epoch 6969/20000 Training Loss: 0.05677254870533943\n",
      "Epoch 6970/20000 Training Loss: 0.06255253404378891\n",
      "Epoch 6970/20000 Validation Loss: 0.058280862867832184\n",
      "Epoch 6971/20000 Training Loss: 0.060854654759168625\n",
      "Epoch 6972/20000 Training Loss: 0.03989604488015175\n",
      "Epoch 6973/20000 Training Loss: 0.06854157894849777\n",
      "Epoch 6974/20000 Training Loss: 0.049162715673446655\n",
      "Epoch 6975/20000 Training Loss: 0.04607371613383293\n",
      "Epoch 6976/20000 Training Loss: 0.048453543335199356\n",
      "Epoch 6977/20000 Training Loss: 0.05285356566309929\n",
      "Epoch 6978/20000 Training Loss: 0.05076715350151062\n",
      "Epoch 6979/20000 Training Loss: 0.062398701906204224\n",
      "Epoch 6980/20000 Training Loss: 0.059303492307662964\n",
      "Epoch 6980/20000 Validation Loss: 0.048462338745594025\n",
      "Epoch 6981/20000 Training Loss: 0.05620891973376274\n",
      "Epoch 6982/20000 Training Loss: 0.07495943456888199\n",
      "Epoch 6983/20000 Training Loss: 0.05005224794149399\n",
      "Epoch 6984/20000 Training Loss: 0.0409955233335495\n",
      "Epoch 6985/20000 Training Loss: 0.04570239782333374\n",
      "Epoch 6986/20000 Training Loss: 0.04930925369262695\n",
      "Epoch 6987/20000 Training Loss: 0.0473690889775753\n",
      "Epoch 6988/20000 Training Loss: 0.058718711137771606\n",
      "Epoch 6989/20000 Training Loss: 0.0824652686715126\n",
      "Epoch 6990/20000 Training Loss: 0.0555187463760376\n",
      "Epoch 6990/20000 Validation Loss: 0.08680614084005356\n",
      "Epoch 6991/20000 Training Loss: 0.04851512983441353\n",
      "Epoch 6992/20000 Training Loss: 0.06033677980303764\n",
      "Epoch 6993/20000 Training Loss: 0.08731111884117126\n",
      "Epoch 6994/20000 Training Loss: 0.07842818647623062\n",
      "Epoch 6995/20000 Training Loss: 0.057248979806900024\n",
      "Epoch 6996/20000 Training Loss: 0.044011328369379044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6997/20000 Training Loss: 0.0484030656516552\n",
      "Epoch 6998/20000 Training Loss: 0.04787152633070946\n",
      "Epoch 6999/20000 Training Loss: 0.048977743834257126\n",
      "Epoch 7000/20000 Training Loss: 0.06790328025817871\n",
      "Epoch 7000/20000 Validation Loss: 0.06023053079843521\n",
      "Epoch 7001/20000 Training Loss: 0.06976214796304703\n",
      "Epoch 7002/20000 Training Loss: 0.053816407918930054\n",
      "Epoch 7003/20000 Training Loss: 0.056121423840522766\n",
      "Epoch 7004/20000 Training Loss: 0.04364974796772003\n",
      "Epoch 7005/20000 Training Loss: 0.06335683912038803\n",
      "Epoch 7006/20000 Training Loss: 0.04176276922225952\n",
      "Epoch 7007/20000 Training Loss: 0.07477354258298874\n",
      "Epoch 7008/20000 Training Loss: 0.050549715757369995\n",
      "Epoch 7009/20000 Training Loss: 0.04956420883536339\n",
      "Epoch 7010/20000 Training Loss: 0.03949519991874695\n",
      "Epoch 7010/20000 Validation Loss: 0.08377283811569214\n",
      "Epoch 7011/20000 Training Loss: 0.04961211979389191\n",
      "Epoch 7012/20000 Training Loss: 0.060417696833610535\n",
      "Epoch 7013/20000 Training Loss: 0.044155657291412354\n",
      "Epoch 7014/20000 Training Loss: 0.0704892948269844\n",
      "Epoch 7015/20000 Training Loss: 0.06038476526737213\n",
      "Epoch 7016/20000 Training Loss: 0.04122965782880783\n",
      "Epoch 7017/20000 Training Loss: 0.06544705480337143\n",
      "Epoch 7018/20000 Training Loss: 0.052892912179231644\n",
      "Epoch 7019/20000 Training Loss: 0.050550077110528946\n",
      "Epoch 7020/20000 Training Loss: 0.05051444470882416\n",
      "Epoch 7020/20000 Validation Loss: 0.056248635053634644\n",
      "Epoch 7021/20000 Training Loss: 0.04897982254624367\n",
      "Epoch 7022/20000 Training Loss: 0.0507684201002121\n",
      "Epoch 7023/20000 Training Loss: 0.052882079035043716\n",
      "Epoch 7024/20000 Training Loss: 0.05007148161530495\n",
      "Epoch 7025/20000 Training Loss: 0.047684285789728165\n",
      "Epoch 7026/20000 Training Loss: 0.04375268146395683\n",
      "Epoch 7027/20000 Training Loss: 0.04526611045002937\n",
      "Epoch 7028/20000 Training Loss: 0.04744904860854149\n",
      "Epoch 7029/20000 Training Loss: 0.07110927253961563\n",
      "Epoch 7030/20000 Training Loss: 0.07204455137252808\n",
      "Epoch 7030/20000 Validation Loss: 0.04450505971908569\n",
      "Epoch 7031/20000 Training Loss: 0.06031078100204468\n",
      "Epoch 7032/20000 Training Loss: 0.048019248992204666\n",
      "Epoch 7033/20000 Training Loss: 0.051188159734010696\n",
      "Epoch 7034/20000 Training Loss: 0.06204207241535187\n",
      "Epoch 7035/20000 Training Loss: 0.045593831688165665\n",
      "Epoch 7036/20000 Training Loss: 0.05682481452822685\n",
      "Epoch 7037/20000 Training Loss: 0.06264079362154007\n",
      "Epoch 7038/20000 Training Loss: 0.049962714314460754\n",
      "Epoch 7039/20000 Training Loss: 0.04194221273064613\n",
      "Epoch 7040/20000 Training Loss: 0.05587349832057953\n",
      "Epoch 7040/20000 Validation Loss: 0.055519793182611465\n",
      "Epoch 7041/20000 Training Loss: 0.04661861062049866\n",
      "Epoch 7042/20000 Training Loss: 0.06555642932653427\n",
      "Epoch 7043/20000 Training Loss: 0.07164876163005829\n",
      "Epoch 7044/20000 Training Loss: 0.06177626922726631\n",
      "Epoch 7045/20000 Training Loss: 0.07916487008333206\n",
      "Epoch 7046/20000 Training Loss: 0.06790795922279358\n",
      "Epoch 7047/20000 Training Loss: 0.07536973804235458\n",
      "Epoch 7048/20000 Training Loss: 0.04709582030773163\n",
      "Epoch 7049/20000 Training Loss: 0.09305015951395035\n",
      "Epoch 7050/20000 Training Loss: 0.0510578490793705\n",
      "Epoch 7050/20000 Validation Loss: 0.06411709636449814\n",
      "Epoch 7051/20000 Training Loss: 0.056590765714645386\n",
      "Epoch 7052/20000 Training Loss: 0.07442008703947067\n",
      "Epoch 7053/20000 Training Loss: 0.04942495748400688\n",
      "Epoch 7054/20000 Training Loss: 0.06626072525978088\n",
      "Epoch 7055/20000 Training Loss: 0.05442185699939728\n",
      "Epoch 7056/20000 Training Loss: 0.06955228000879288\n",
      "Epoch 7057/20000 Training Loss: 0.05838323011994362\n",
      "Epoch 7058/20000 Training Loss: 0.04562993720173836\n",
      "Epoch 7059/20000 Training Loss: 0.05635576322674751\n",
      "Epoch 7060/20000 Training Loss: 0.08076829463243484\n",
      "Epoch 7060/20000 Validation Loss: 0.05101771280169487\n",
      "Epoch 7061/20000 Training Loss: 0.0584460012614727\n",
      "Epoch 7062/20000 Training Loss: 0.05707526206970215\n",
      "Epoch 7063/20000 Training Loss: 0.048276785761117935\n",
      "Epoch 7064/20000 Training Loss: 0.07003936171531677\n",
      "Epoch 7065/20000 Training Loss: 0.046671222895383835\n",
      "Epoch 7066/20000 Training Loss: 0.061148256063461304\n",
      "Epoch 7067/20000 Training Loss: 0.05169307067990303\n",
      "Epoch 7068/20000 Training Loss: 0.04733332619071007\n",
      "Epoch 7069/20000 Training Loss: 0.06131419166922569\n",
      "Epoch 7070/20000 Training Loss: 0.04601072892546654\n",
      "Epoch 7070/20000 Validation Loss: 0.04337421804666519\n",
      "Epoch 7071/20000 Training Loss: 0.05863109230995178\n",
      "Epoch 7072/20000 Training Loss: 0.03979036957025528\n",
      "Epoch 7073/20000 Training Loss: 0.06174340471625328\n",
      "Epoch 7074/20000 Training Loss: 0.06742934137582779\n",
      "Epoch 7075/20000 Training Loss: 0.0472637377679348\n",
      "Epoch 7076/20000 Training Loss: 0.0528511218726635\n",
      "Epoch 7077/20000 Training Loss: 0.06772519648075104\n",
      "Epoch 7078/20000 Training Loss: 0.043966710567474365\n",
      "Epoch 7079/20000 Training Loss: 0.042878102511167526\n",
      "Epoch 7080/20000 Training Loss: 0.0485960990190506\n",
      "Epoch 7080/20000 Validation Loss: 0.05392016842961311\n",
      "Epoch 7081/20000 Training Loss: 0.0465022511780262\n",
      "Epoch 7082/20000 Training Loss: 0.04057849571108818\n",
      "Epoch 7083/20000 Training Loss: 0.055832307785749435\n",
      "Epoch 7084/20000 Training Loss: 0.051193878054618835\n",
      "Epoch 7085/20000 Training Loss: 0.04310881718993187\n",
      "Epoch 7086/20000 Training Loss: 0.05023917183279991\n",
      "Epoch 7087/20000 Training Loss: 0.06292874366044998\n",
      "Epoch 7088/20000 Training Loss: 0.07358507812023163\n",
      "Epoch 7089/20000 Training Loss: 0.08148723095655441\n",
      "Epoch 7090/20000 Training Loss: 0.06654816120862961\n",
      "Epoch 7090/20000 Validation Loss: 0.04446760565042496\n",
      "Epoch 7091/20000 Training Loss: 0.04926188662648201\n",
      "Epoch 7092/20000 Training Loss: 0.06153654679656029\n",
      "Epoch 7093/20000 Training Loss: 0.048545535653829575\n",
      "Epoch 7094/20000 Training Loss: 0.06248385086655617\n",
      "Epoch 7095/20000 Training Loss: 0.06115471199154854\n",
      "Epoch 7096/20000 Training Loss: 0.050144121050834656\n",
      "Epoch 7097/20000 Training Loss: 0.045879025012254715\n",
      "Epoch 7098/20000 Training Loss: 0.06641995161771774\n",
      "Epoch 7099/20000 Training Loss: 0.06403738260269165\n",
      "Epoch 7100/20000 Training Loss: 0.05885068699717522\n",
      "Epoch 7100/20000 Validation Loss: 0.050618287175893784\n",
      "Epoch 7101/20000 Training Loss: 0.05610838532447815\n",
      "Epoch 7102/20000 Training Loss: 0.04516065493226051\n",
      "Epoch 7103/20000 Training Loss: 0.049912553280591965\n",
      "Epoch 7104/20000 Training Loss: 0.059845030307769775\n",
      "Epoch 7105/20000 Training Loss: 0.058656033128499985\n",
      "Epoch 7106/20000 Training Loss: 0.059879690408706665\n",
      "Epoch 7107/20000 Training Loss: 0.05602683499455452\n",
      "Epoch 7108/20000 Training Loss: 0.053326528519392014\n",
      "Epoch 7109/20000 Training Loss: 0.060660213232040405\n",
      "Epoch 7110/20000 Training Loss: 0.04419182240962982\n",
      "Epoch 7110/20000 Validation Loss: 0.04536183550953865\n",
      "Epoch 7111/20000 Training Loss: 0.06341750174760818\n",
      "Epoch 7112/20000 Training Loss: 0.06344971060752869\n",
      "Epoch 7113/20000 Training Loss: 0.057146307080984116\n",
      "Epoch 7114/20000 Training Loss: 0.05078313127160072\n",
      "Epoch 7115/20000 Training Loss: 0.042701054364442825\n",
      "Epoch 7116/20000 Training Loss: 0.0630401000380516\n",
      "Epoch 7117/20000 Training Loss: 0.0562555305659771\n",
      "Epoch 7118/20000 Training Loss: 0.06708425283432007\n",
      "Epoch 7119/20000 Training Loss: 0.054643597453832626\n",
      "Epoch 7120/20000 Training Loss: 0.06492369621992111\n",
      "Epoch 7120/20000 Validation Loss: 0.047152601182460785\n",
      "Epoch 7121/20000 Training Loss: 0.0639403685927391\n",
      "Epoch 7122/20000 Training Loss: 0.048043232411146164\n",
      "Epoch 7123/20000 Training Loss: 0.04623796045780182\n",
      "Epoch 7124/20000 Training Loss: 0.03527778759598732\n",
      "Epoch 7125/20000 Training Loss: 0.05238369479775429\n",
      "Epoch 7126/20000 Training Loss: 0.04180780053138733\n",
      "Epoch 7127/20000 Training Loss: 0.06487958878278732\n",
      "Epoch 7128/20000 Training Loss: 0.06977511197328568\n",
      "Epoch 7129/20000 Training Loss: 0.07009056955575943\n",
      "Epoch 7130/20000 Training Loss: 0.05833475664258003\n",
      "Epoch 7130/20000 Validation Loss: 0.06646498292684555\n",
      "Epoch 7131/20000 Training Loss: 0.058609794825315475\n",
      "Epoch 7132/20000 Training Loss: 0.05207672715187073\n",
      "Epoch 7133/20000 Training Loss: 0.06462418287992477\n",
      "Epoch 7134/20000 Training Loss: 0.058263540267944336\n",
      "Epoch 7135/20000 Training Loss: 0.0682898685336113\n",
      "Epoch 7136/20000 Training Loss: 0.04443572461605072\n",
      "Epoch 7137/20000 Training Loss: 0.050804901868104935\n",
      "Epoch 7138/20000 Training Loss: 0.046934742480516434\n",
      "Epoch 7139/20000 Training Loss: 0.0529513955116272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7140/20000 Training Loss: 0.05223827436566353\n",
      "Epoch 7140/20000 Validation Loss: 0.06933646649122238\n",
      "Epoch 7141/20000 Training Loss: 0.04524439200758934\n",
      "Epoch 7142/20000 Training Loss: 0.06124258041381836\n",
      "Epoch 7143/20000 Training Loss: 0.05984007194638252\n",
      "Epoch 7144/20000 Training Loss: 0.07007160782814026\n",
      "Epoch 7145/20000 Training Loss: 0.052596982568502426\n",
      "Epoch 7146/20000 Training Loss: 0.05541853606700897\n",
      "Epoch 7147/20000 Training Loss: 0.041731685400009155\n",
      "Epoch 7148/20000 Training Loss: 0.05844269320368767\n",
      "Epoch 7149/20000 Training Loss: 0.06227431818842888\n",
      "Epoch 7150/20000 Training Loss: 0.05713963881134987\n",
      "Epoch 7150/20000 Validation Loss: 0.042849332094192505\n",
      "Epoch 7151/20000 Training Loss: 0.06311152130365372\n",
      "Epoch 7152/20000 Training Loss: 0.06668226420879364\n",
      "Epoch 7153/20000 Training Loss: 0.06450758874416351\n",
      "Epoch 7154/20000 Training Loss: 0.04690069332718849\n",
      "Epoch 7155/20000 Training Loss: 0.06673171371221542\n",
      "Epoch 7156/20000 Training Loss: 0.05529748275876045\n",
      "Epoch 7157/20000 Training Loss: 0.04833823814988136\n",
      "Epoch 7158/20000 Training Loss: 0.06063786521553993\n",
      "Epoch 7159/20000 Training Loss: 0.0456702820956707\n",
      "Epoch 7160/20000 Training Loss: 0.04361101984977722\n",
      "Epoch 7160/20000 Validation Loss: 0.05086183920502663\n",
      "Epoch 7161/20000 Training Loss: 0.05441592261195183\n",
      "Epoch 7162/20000 Training Loss: 0.04355223849415779\n",
      "Epoch 7163/20000 Training Loss: 0.052241403609514236\n",
      "Epoch 7164/20000 Training Loss: 0.05770532786846161\n",
      "Epoch 7165/20000 Training Loss: 0.05565531551837921\n",
      "Epoch 7166/20000 Training Loss: 0.060410451143980026\n",
      "Epoch 7167/20000 Training Loss: 0.053192347288131714\n",
      "Epoch 7168/20000 Training Loss: 0.05372142419219017\n",
      "Epoch 7169/20000 Training Loss: 0.06137537956237793\n",
      "Epoch 7170/20000 Training Loss: 0.058590780943632126\n",
      "Epoch 7170/20000 Validation Loss: 0.07079958915710449\n",
      "Epoch 7171/20000 Training Loss: 0.051574673503637314\n",
      "Epoch 7172/20000 Training Loss: 0.046887706965208054\n",
      "Epoch 7173/20000 Training Loss: 0.0558907687664032\n",
      "Epoch 7174/20000 Training Loss: 0.05744810029864311\n",
      "Epoch 7175/20000 Training Loss: 0.0686953067779541\n",
      "Epoch 7176/20000 Training Loss: 0.046980131417512894\n",
      "Epoch 7177/20000 Training Loss: 0.06507235020399094\n",
      "Epoch 7178/20000 Training Loss: 0.05298956111073494\n",
      "Epoch 7179/20000 Training Loss: 0.06068513169884682\n",
      "Epoch 7180/20000 Training Loss: 0.0529719740152359\n",
      "Epoch 7180/20000 Validation Loss: 0.06586261838674545\n",
      "Epoch 7181/20000 Training Loss: 0.05569466948509216\n",
      "Epoch 7182/20000 Training Loss: 0.058195263147354126\n",
      "Epoch 7183/20000 Training Loss: 0.04543457552790642\n",
      "Epoch 7184/20000 Training Loss: 0.053483277559280396\n",
      "Epoch 7185/20000 Training Loss: 0.04268111288547516\n",
      "Epoch 7186/20000 Training Loss: 0.06554611772298813\n",
      "Epoch 7187/20000 Training Loss: 0.04407382383942604\n",
      "Epoch 7188/20000 Training Loss: 0.05241954326629639\n",
      "Epoch 7189/20000 Training Loss: 0.04240112379193306\n",
      "Epoch 7190/20000 Training Loss: 0.05028568208217621\n",
      "Epoch 7190/20000 Validation Loss: 0.05573362857103348\n",
      "Epoch 7191/20000 Training Loss: 0.05957270786166191\n",
      "Epoch 7192/20000 Training Loss: 0.06301739066839218\n",
      "Epoch 7193/20000 Training Loss: 0.06433500349521637\n",
      "Epoch 7194/20000 Training Loss: 0.0496864877641201\n",
      "Epoch 7195/20000 Training Loss: 0.043341416865587234\n",
      "Epoch 7196/20000 Training Loss: 0.06233507767319679\n",
      "Epoch 7197/20000 Training Loss: 0.06398012489080429\n",
      "Epoch 7198/20000 Training Loss: 0.050208717584609985\n",
      "Epoch 7199/20000 Training Loss: 0.06698557734489441\n",
      "Epoch 7200/20000 Training Loss: 0.056124623864889145\n",
      "Epoch 7200/20000 Validation Loss: 0.05944404751062393\n",
      "Epoch 7201/20000 Training Loss: 0.06070186197757721\n",
      "Epoch 7202/20000 Training Loss: 0.051265522837638855\n",
      "Epoch 7203/20000 Training Loss: 0.062454480677843094\n",
      "Epoch 7204/20000 Training Loss: 0.048854198306798935\n",
      "Epoch 7205/20000 Training Loss: 0.07234702259302139\n",
      "Epoch 7206/20000 Training Loss: 0.053702354431152344\n",
      "Epoch 7207/20000 Training Loss: 0.06690240651369095\n",
      "Epoch 7208/20000 Training Loss: 0.05172264203429222\n",
      "Epoch 7209/20000 Training Loss: 0.05346990004181862\n",
      "Epoch 7210/20000 Training Loss: 0.0587097704410553\n",
      "Epoch 7210/20000 Validation Loss: 0.03804276883602142\n",
      "Epoch 7211/20000 Training Loss: 0.045549798756837845\n",
      "Epoch 7212/20000 Training Loss: 0.05444558337330818\n",
      "Epoch 7213/20000 Training Loss: 0.0660916268825531\n",
      "Epoch 7214/20000 Training Loss: 0.06433884054422379\n",
      "Epoch 7215/20000 Training Loss: 0.06401827186346054\n",
      "Epoch 7216/20000 Training Loss: 0.058656077831983566\n",
      "Epoch 7217/20000 Training Loss: 0.07463306188583374\n",
      "Epoch 7218/20000 Training Loss: 0.0378972552716732\n",
      "Epoch 7219/20000 Training Loss: 0.06470075249671936\n",
      "Epoch 7220/20000 Training Loss: 0.0540076307952404\n",
      "Epoch 7220/20000 Validation Loss: 0.042290396988391876\n",
      "Epoch 7221/20000 Training Loss: 0.06506422907114029\n",
      "Epoch 7222/20000 Training Loss: 0.07178711146116257\n",
      "Epoch 7223/20000 Training Loss: 0.07045970857143402\n",
      "Epoch 7224/20000 Training Loss: 0.0433490015566349\n",
      "Epoch 7225/20000 Training Loss: 0.0546850860118866\n",
      "Epoch 7226/20000 Training Loss: 0.04828358069062233\n",
      "Epoch 7227/20000 Training Loss: 0.05046060308814049\n",
      "Epoch 7228/20000 Training Loss: 0.054227929562330246\n",
      "Epoch 7229/20000 Training Loss: 0.0845356211066246\n",
      "Epoch 7230/20000 Training Loss: 0.07969239354133606\n",
      "Epoch 7230/20000 Validation Loss: 0.057938866317272186\n",
      "Epoch 7231/20000 Training Loss: 0.05531515181064606\n",
      "Epoch 7232/20000 Training Loss: 0.05637549236416817\n",
      "Epoch 7233/20000 Training Loss: 0.07069943845272064\n",
      "Epoch 7234/20000 Training Loss: 0.058046381920576096\n",
      "Epoch 7235/20000 Training Loss: 0.05762477591633797\n",
      "Epoch 7236/20000 Training Loss: 0.059230104088783264\n",
      "Epoch 7237/20000 Training Loss: 0.07393201440572739\n",
      "Epoch 7238/20000 Training Loss: 0.04436688497662544\n",
      "Epoch 7239/20000 Training Loss: 0.06114785745739937\n",
      "Epoch 7240/20000 Training Loss: 0.05194239690899849\n",
      "Epoch 7240/20000 Validation Loss: 0.0505208745598793\n",
      "Epoch 7241/20000 Training Loss: 0.06605745106935501\n",
      "Epoch 7242/20000 Training Loss: 0.05411730334162712\n",
      "Epoch 7243/20000 Training Loss: 0.049190372228622437\n",
      "Epoch 7244/20000 Training Loss: 0.046986449509859085\n",
      "Epoch 7245/20000 Training Loss: 0.04144516959786415\n",
      "Epoch 7246/20000 Training Loss: 0.06409628689289093\n",
      "Epoch 7247/20000 Training Loss: 0.04400400444865227\n",
      "Epoch 7248/20000 Training Loss: 0.06855481117963791\n",
      "Epoch 7249/20000 Training Loss: 0.053338587284088135\n",
      "Epoch 7250/20000 Training Loss: 0.0599677711725235\n",
      "Epoch 7250/20000 Validation Loss: 0.06660964339971542\n",
      "Epoch 7251/20000 Training Loss: 0.06750154495239258\n",
      "Epoch 7252/20000 Training Loss: 0.04498149827122688\n",
      "Epoch 7253/20000 Training Loss: 0.05054374411702156\n",
      "Epoch 7254/20000 Training Loss: 0.06124979630112648\n",
      "Epoch 7255/20000 Training Loss: 0.06123752519488335\n",
      "Epoch 7256/20000 Training Loss: 0.06605493277311325\n",
      "Epoch 7257/20000 Training Loss: 0.05588972941040993\n",
      "Epoch 7258/20000 Training Loss: 0.06675965338945389\n",
      "Epoch 7259/20000 Training Loss: 0.05110710486769676\n",
      "Epoch 7260/20000 Training Loss: 0.056397516280412674\n",
      "Epoch 7260/20000 Validation Loss: 0.0760975182056427\n",
      "Epoch 7261/20000 Training Loss: 0.06754664331674576\n",
      "Epoch 7262/20000 Training Loss: 0.05933285877108574\n",
      "Epoch 7263/20000 Training Loss: 0.05827935412526131\n",
      "Epoch 7264/20000 Training Loss: 0.055433955043554306\n",
      "Epoch 7265/20000 Training Loss: 0.053251445293426514\n",
      "Epoch 7266/20000 Training Loss: 0.05942849442362785\n",
      "Epoch 7267/20000 Training Loss: 0.05973402038216591\n",
      "Epoch 7268/20000 Training Loss: 0.05369711294770241\n",
      "Epoch 7269/20000 Training Loss: 0.05262334644794464\n",
      "Epoch 7270/20000 Training Loss: 0.04307534173130989\n",
      "Epoch 7270/20000 Validation Loss: 0.0532672181725502\n",
      "Epoch 7271/20000 Training Loss: 0.056051719933748245\n",
      "Epoch 7272/20000 Training Loss: 0.05756276845932007\n",
      "Epoch 7273/20000 Training Loss: 0.05981994792819023\n",
      "Epoch 7274/20000 Training Loss: 0.04617258533835411\n",
      "Epoch 7275/20000 Training Loss: 0.06883225589990616\n",
      "Epoch 7276/20000 Training Loss: 0.04326643422245979\n",
      "Epoch 7277/20000 Training Loss: 0.06695445626974106\n",
      "Epoch 7278/20000 Training Loss: 0.04251132532954216\n",
      "Epoch 7279/20000 Training Loss: 0.05447190999984741\n",
      "Epoch 7280/20000 Training Loss: 0.05891511216759682\n",
      "Epoch 7280/20000 Validation Loss: 0.06214313954114914\n",
      "Epoch 7281/20000 Training Loss: 0.04132494330406189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7282/20000 Training Loss: 0.04657833278179169\n",
      "Epoch 7283/20000 Training Loss: 0.06039896607398987\n",
      "Epoch 7284/20000 Training Loss: 0.046301085501909256\n",
      "Epoch 7285/20000 Training Loss: 0.05575316771864891\n",
      "Epoch 7286/20000 Training Loss: 0.08113928884267807\n",
      "Epoch 7287/20000 Training Loss: 0.05734279379248619\n",
      "Epoch 7288/20000 Training Loss: 0.053321704268455505\n",
      "Epoch 7289/20000 Training Loss: 0.03995467722415924\n",
      "Epoch 7290/20000 Training Loss: 0.055579934269189835\n",
      "Epoch 7290/20000 Validation Loss: 0.06414175033569336\n",
      "Epoch 7291/20000 Training Loss: 0.07116197794675827\n",
      "Epoch 7292/20000 Training Loss: 0.05623919144272804\n",
      "Epoch 7293/20000 Training Loss: 0.05151689425110817\n",
      "Epoch 7294/20000 Training Loss: 0.04940887168049812\n",
      "Epoch 7295/20000 Training Loss: 0.05315934121608734\n",
      "Epoch 7296/20000 Training Loss: 0.0554349459707737\n",
      "Epoch 7297/20000 Training Loss: 0.05590372160077095\n",
      "Epoch 7298/20000 Training Loss: 0.05742970481514931\n",
      "Epoch 7299/20000 Training Loss: 0.05163256824016571\n",
      "Epoch 7300/20000 Training Loss: 0.056937891989946365\n",
      "Epoch 7300/20000 Validation Loss: 0.0473162978887558\n",
      "Epoch 7301/20000 Training Loss: 0.056375276297330856\n",
      "Epoch 7302/20000 Training Loss: 0.04982081055641174\n",
      "Epoch 7303/20000 Training Loss: 0.050785914063453674\n",
      "Epoch 7304/20000 Training Loss: 0.06156006455421448\n",
      "Epoch 7305/20000 Training Loss: 0.04940773919224739\n",
      "Epoch 7306/20000 Training Loss: 0.06452307850122452\n",
      "Epoch 7307/20000 Training Loss: 0.06885425001382828\n",
      "Epoch 7308/20000 Training Loss: 0.05510442331433296\n",
      "Epoch 7309/20000 Training Loss: 0.058945585042238235\n",
      "Epoch 7310/20000 Training Loss: 0.05516577884554863\n",
      "Epoch 7310/20000 Validation Loss: 0.07448417693376541\n",
      "Epoch 7311/20000 Training Loss: 0.05422263219952583\n",
      "Epoch 7312/20000 Training Loss: 0.05340999364852905\n",
      "Epoch 7313/20000 Training Loss: 0.059624046087265015\n",
      "Epoch 7314/20000 Training Loss: 0.052119482308626175\n",
      "Epoch 7315/20000 Training Loss: 0.03805086389183998\n",
      "Epoch 7316/20000 Training Loss: 0.06089773774147034\n",
      "Epoch 7317/20000 Training Loss: 0.04465348646044731\n",
      "Epoch 7318/20000 Training Loss: 0.05650229752063751\n",
      "Epoch 7319/20000 Training Loss: 0.037924349308013916\n",
      "Epoch 7320/20000 Training Loss: 0.07146421819925308\n",
      "Epoch 7320/20000 Validation Loss: 0.07344485819339752\n",
      "Epoch 7321/20000 Training Loss: 0.04161331430077553\n",
      "Epoch 7322/20000 Training Loss: 0.07188744097948074\n",
      "Epoch 7323/20000 Training Loss: 0.055803995579481125\n",
      "Epoch 7324/20000 Training Loss: 0.055783960968256\n",
      "Epoch 7325/20000 Training Loss: 0.06477051228284836\n",
      "Epoch 7326/20000 Training Loss: 0.044962283223867416\n",
      "Epoch 7327/20000 Training Loss: 0.05796291306614876\n",
      "Epoch 7328/20000 Training Loss: 0.046969521790742874\n",
      "Epoch 7329/20000 Training Loss: 0.060815513134002686\n",
      "Epoch 7330/20000 Training Loss: 0.0527840293943882\n",
      "Epoch 7330/20000 Validation Loss: 0.07118189334869385\n",
      "Epoch 7331/20000 Training Loss: 0.05301786586642265\n",
      "Epoch 7332/20000 Training Loss: 0.04665951803326607\n",
      "Epoch 7333/20000 Training Loss: 0.04451571777462959\n",
      "Epoch 7334/20000 Training Loss: 0.06814538687467575\n",
      "Epoch 7335/20000 Training Loss: 0.05374543368816376\n",
      "Epoch 7336/20000 Training Loss: 0.08272243291139603\n",
      "Epoch 7337/20000 Training Loss: 0.05622367933392525\n",
      "Epoch 7338/20000 Training Loss: 0.08675245195627213\n",
      "Epoch 7339/20000 Training Loss: 0.053683195263147354\n",
      "Epoch 7340/20000 Training Loss: 0.09026095271110535\n",
      "Epoch 7340/20000 Validation Loss: 0.07264430075883865\n",
      "Epoch 7341/20000 Training Loss: 0.04639633372426033\n",
      "Epoch 7342/20000 Training Loss: 0.06340599060058594\n",
      "Epoch 7343/20000 Training Loss: 0.05996847525238991\n",
      "Epoch 7344/20000 Training Loss: 0.05658246576786041\n",
      "Epoch 7345/20000 Training Loss: 0.04960966482758522\n",
      "Epoch 7346/20000 Training Loss: 0.06269662827253342\n",
      "Epoch 7347/20000 Training Loss: 0.07384204119443893\n",
      "Epoch 7348/20000 Training Loss: 0.05337955057621002\n",
      "Epoch 7349/20000 Training Loss: 0.0599503368139267\n",
      "Epoch 7350/20000 Training Loss: 0.06419586390256882\n",
      "Epoch 7350/20000 Validation Loss: 0.07548888772726059\n",
      "Epoch 7351/20000 Training Loss: 0.07522959262132645\n",
      "Epoch 7352/20000 Training Loss: 0.05084686353802681\n",
      "Epoch 7353/20000 Training Loss: 0.05965409800410271\n",
      "Epoch 7354/20000 Training Loss: 0.04425321891903877\n",
      "Epoch 7355/20000 Training Loss: 0.04971243068575859\n",
      "Epoch 7356/20000 Training Loss: 0.057657092809677124\n",
      "Epoch 7357/20000 Training Loss: 0.05184158310294151\n",
      "Epoch 7358/20000 Training Loss: 0.07529718428850174\n",
      "Epoch 7359/20000 Training Loss: 0.05235334113240242\n",
      "Epoch 7360/20000 Training Loss: 0.04764320328831673\n",
      "Epoch 7360/20000 Validation Loss: 0.04983731731772423\n",
      "Epoch 7361/20000 Training Loss: 0.050322990864515305\n",
      "Epoch 7362/20000 Training Loss: 0.06217602267861366\n",
      "Epoch 7363/20000 Training Loss: 0.06028091907501221\n",
      "Epoch 7364/20000 Training Loss: 0.06237400695681572\n",
      "Epoch 7365/20000 Training Loss: 0.04319697991013527\n",
      "Epoch 7366/20000 Training Loss: 0.06140772998332977\n",
      "Epoch 7367/20000 Training Loss: 0.06992556154727936\n",
      "Epoch 7368/20000 Training Loss: 0.07290598005056381\n",
      "Epoch 7369/20000 Training Loss: 0.0503229983150959\n",
      "Epoch 7370/20000 Training Loss: 0.054994065314531326\n",
      "Epoch 7370/20000 Validation Loss: 0.04640079662203789\n",
      "Epoch 7371/20000 Training Loss: 0.05791682004928589\n",
      "Epoch 7372/20000 Training Loss: 0.048906754702329636\n",
      "Epoch 7373/20000 Training Loss: 0.05193527042865753\n",
      "Epoch 7374/20000 Training Loss: 0.06302139163017273\n",
      "Epoch 7375/20000 Training Loss: 0.07052313536405563\n",
      "Epoch 7376/20000 Training Loss: 0.05160007253289223\n",
      "Epoch 7377/20000 Training Loss: 0.040762171149253845\n",
      "Epoch 7378/20000 Training Loss: 0.04995323717594147\n",
      "Epoch 7379/20000 Training Loss: 0.08298742771148682\n",
      "Epoch 7380/20000 Training Loss: 0.050455719232559204\n",
      "Epoch 7380/20000 Validation Loss: 0.035478416830301285\n",
      "Epoch 7381/20000 Training Loss: 0.05643997713923454\n",
      "Epoch 7382/20000 Training Loss: 0.055723536759614944\n",
      "Epoch 7383/20000 Training Loss: 0.05724843218922615\n",
      "Epoch 7384/20000 Training Loss: 0.0805443525314331\n",
      "Epoch 7385/20000 Training Loss: 0.06002502515912056\n",
      "Epoch 7386/20000 Training Loss: 0.09928818792104721\n",
      "Epoch 7387/20000 Training Loss: 0.05643610283732414\n",
      "Epoch 7388/20000 Training Loss: 0.06565061956644058\n",
      "Epoch 7389/20000 Training Loss: 0.06672573834657669\n",
      "Epoch 7390/20000 Training Loss: 0.05397801101207733\n",
      "Epoch 7390/20000 Validation Loss: 0.06458541750907898\n",
      "Epoch 7391/20000 Training Loss: 0.049983102828264236\n",
      "Epoch 7392/20000 Training Loss: 0.046383168548345566\n",
      "Epoch 7393/20000 Training Loss: 0.05874830484390259\n",
      "Epoch 7394/20000 Training Loss: 0.05571911856532097\n",
      "Epoch 7395/20000 Training Loss: 0.039159730076789856\n",
      "Epoch 7396/20000 Training Loss: 0.07437262684106827\n",
      "Epoch 7397/20000 Training Loss: 0.05209808424115181\n",
      "Epoch 7398/20000 Training Loss: 0.057606685906648636\n",
      "Epoch 7399/20000 Training Loss: 0.054423797875642776\n",
      "Epoch 7400/20000 Training Loss: 0.05459766462445259\n",
      "Epoch 7400/20000 Validation Loss: 0.03937852382659912\n",
      "Epoch 7401/20000 Training Loss: 0.06248636171221733\n",
      "Epoch 7402/20000 Training Loss: 0.05339274927973747\n",
      "Epoch 7403/20000 Training Loss: 0.05449629947543144\n",
      "Epoch 7404/20000 Training Loss: 0.05517875775694847\n",
      "Epoch 7405/20000 Training Loss: 0.05387592688202858\n",
      "Epoch 7406/20000 Training Loss: 0.06240515783429146\n",
      "Epoch 7407/20000 Training Loss: 0.0459502674639225\n",
      "Epoch 7408/20000 Training Loss: 0.05671592429280281\n",
      "Epoch 7409/20000 Training Loss: 0.07389609515666962\n",
      "Epoch 7410/20000 Training Loss: 0.0450570322573185\n",
      "Epoch 7410/20000 Validation Loss: 0.04994140937924385\n",
      "Epoch 7411/20000 Training Loss: 0.0691285952925682\n",
      "Epoch 7412/20000 Training Loss: 0.06144712492823601\n",
      "Epoch 7413/20000 Training Loss: 0.0677100270986557\n",
      "Epoch 7414/20000 Training Loss: 0.06940006464719772\n",
      "Epoch 7415/20000 Training Loss: 0.06086945906281471\n",
      "Epoch 7416/20000 Training Loss: 0.056410182267427444\n",
      "Epoch 7417/20000 Training Loss: 0.047952234745025635\n",
      "Epoch 7418/20000 Training Loss: 0.040769223123788834\n",
      "Epoch 7419/20000 Training Loss: 0.05637406185269356\n",
      "Epoch 7420/20000 Training Loss: 0.06467019766569138\n",
      "Epoch 7420/20000 Validation Loss: 0.053984418511390686\n",
      "Epoch 7421/20000 Training Loss: 0.06242254748940468\n",
      "Epoch 7422/20000 Training Loss: 0.04986667260527611\n",
      "Epoch 7423/20000 Training Loss: 0.053690314292907715\n",
      "Epoch 7424/20000 Training Loss: 0.06723332405090332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7425/20000 Training Loss: 0.0486128143966198\n",
      "Epoch 7426/20000 Training Loss: 0.057161420583724976\n",
      "Epoch 7427/20000 Training Loss: 0.0512014664709568\n",
      "Epoch 7428/20000 Training Loss: 0.06528804451227188\n",
      "Epoch 7429/20000 Training Loss: 0.05715961381793022\n",
      "Epoch 7430/20000 Training Loss: 0.05314226448535919\n",
      "Epoch 7430/20000 Validation Loss: 0.08976739645004272\n",
      "Epoch 7431/20000 Training Loss: 0.05201306566596031\n",
      "Epoch 7432/20000 Training Loss: 0.041163817048072815\n",
      "Epoch 7433/20000 Training Loss: 0.061543505638837814\n",
      "Epoch 7434/20000 Training Loss: 0.07007445394992828\n",
      "Epoch 7435/20000 Training Loss: 0.06498368829488754\n",
      "Epoch 7436/20000 Training Loss: 0.06650412082672119\n",
      "Epoch 7437/20000 Training Loss: 0.06192122772336006\n",
      "Epoch 7438/20000 Training Loss: 0.06816372275352478\n",
      "Epoch 7439/20000 Training Loss: 0.05623553693294525\n",
      "Epoch 7440/20000 Training Loss: 0.05154632404446602\n",
      "Epoch 7440/20000 Validation Loss: 0.08969461172819138\n",
      "Epoch 7441/20000 Training Loss: 0.07376047223806381\n",
      "Epoch 7442/20000 Training Loss: 0.058648545295000076\n",
      "Epoch 7443/20000 Training Loss: 0.041551534086465836\n",
      "Epoch 7444/20000 Training Loss: 0.061096418648958206\n",
      "Epoch 7445/20000 Training Loss: 0.041131261736154556\n",
      "Epoch 7446/20000 Training Loss: 0.07028411328792572\n",
      "Epoch 7447/20000 Training Loss: 0.05794644355773926\n",
      "Epoch 7448/20000 Training Loss: 0.06702001392841339\n",
      "Epoch 7449/20000 Training Loss: 0.06892121583223343\n",
      "Epoch 7450/20000 Training Loss: 0.04885438084602356\n",
      "Epoch 7450/20000 Validation Loss: 0.03895794600248337\n",
      "Epoch 7451/20000 Training Loss: 0.04301241412758827\n",
      "Epoch 7452/20000 Training Loss: 0.07183419167995453\n",
      "Epoch 7453/20000 Training Loss: 0.06883462518453598\n",
      "Epoch 7454/20000 Training Loss: 0.04813970997929573\n",
      "Epoch 7455/20000 Training Loss: 0.0632648766040802\n",
      "Epoch 7456/20000 Training Loss: 0.05952766537666321\n",
      "Epoch 7457/20000 Training Loss: 0.06352731585502625\n",
      "Epoch 7458/20000 Training Loss: 0.03945710137486458\n",
      "Epoch 7459/20000 Training Loss: 0.06297393888235092\n",
      "Epoch 7460/20000 Training Loss: 0.05870291590690613\n",
      "Epoch 7460/20000 Validation Loss: 0.06984984874725342\n",
      "Epoch 7461/20000 Training Loss: 0.05638200044631958\n",
      "Epoch 7462/20000 Training Loss: 0.04713502153754234\n",
      "Epoch 7463/20000 Training Loss: 0.056477054953575134\n",
      "Epoch 7464/20000 Training Loss: 0.07263802736997604\n",
      "Epoch 7465/20000 Training Loss: 0.08264712244272232\n",
      "Epoch 7466/20000 Training Loss: 0.05582400783896446\n",
      "Epoch 7467/20000 Training Loss: 0.06302599608898163\n",
      "Epoch 7468/20000 Training Loss: 0.048255518078804016\n",
      "Epoch 7469/20000 Training Loss: 0.059759896248579025\n",
      "Epoch 7470/20000 Training Loss: 0.06029428541660309\n",
      "Epoch 7470/20000 Validation Loss: 0.05850943177938461\n",
      "Epoch 7471/20000 Training Loss: 0.0512644462287426\n",
      "Epoch 7472/20000 Training Loss: 0.04707396402955055\n",
      "Epoch 7473/20000 Training Loss: 0.05894729122519493\n",
      "Epoch 7474/20000 Training Loss: 0.05224927142262459\n",
      "Epoch 7475/20000 Training Loss: 0.08527156710624695\n",
      "Epoch 7476/20000 Training Loss: 0.058768611401319504\n",
      "Epoch 7477/20000 Training Loss: 0.04528382793068886\n",
      "Epoch 7478/20000 Training Loss: 0.04765310510993004\n",
      "Epoch 7479/20000 Training Loss: 0.05361134931445122\n",
      "Epoch 7480/20000 Training Loss: 0.058593545109033585\n",
      "Epoch 7480/20000 Validation Loss: 0.04116132855415344\n",
      "Epoch 7481/20000 Training Loss: 0.04937181994318962\n",
      "Epoch 7482/20000 Training Loss: 0.045391350984573364\n",
      "Epoch 7483/20000 Training Loss: 0.07393913716077805\n",
      "Epoch 7484/20000 Training Loss: 0.05485836789011955\n",
      "Epoch 7485/20000 Training Loss: 0.04527429863810539\n",
      "Epoch 7486/20000 Training Loss: 0.05425506457686424\n",
      "Epoch 7487/20000 Training Loss: 0.06450635939836502\n",
      "Epoch 7488/20000 Training Loss: 0.05940936878323555\n",
      "Epoch 7489/20000 Training Loss: 0.05945873633027077\n",
      "Epoch 7490/20000 Training Loss: 0.04633978009223938\n",
      "Epoch 7490/20000 Validation Loss: 0.029787298291921616\n",
      "Cleared directory to save new best model.\n",
      "============>Saving current best model with min_val_loss=0.029787298291921616<=============\n",
      "Epoch 7491/20000 Training Loss: 0.08478476852178574\n",
      "Epoch 7492/20000 Training Loss: 0.05301167443394661\n",
      "Epoch 7493/20000 Training Loss: 0.07094389200210571\n",
      "Epoch 7494/20000 Training Loss: 0.044753167778253555\n",
      "Epoch 7495/20000 Training Loss: 0.061965715140104294\n",
      "Epoch 7496/20000 Training Loss: 0.06060407683253288\n",
      "Epoch 7497/20000 Training Loss: 0.060753557831048965\n",
      "Epoch 7498/20000 Training Loss: 0.05318716540932655\n",
      "Epoch 7499/20000 Training Loss: 0.06834573298692703\n",
      "Epoch 7500/20000 Training Loss: 0.06895191222429276\n",
      "Epoch 7500/20000 Validation Loss: 0.06409640610218048\n",
      "Epoch 7501/20000 Training Loss: 0.0754723772406578\n",
      "Epoch 7502/20000 Training Loss: 0.06514057517051697\n",
      "Epoch 7503/20000 Training Loss: 0.051392946392297745\n",
      "Epoch 7504/20000 Training Loss: 0.06272555142641068\n",
      "Epoch 7505/20000 Training Loss: 0.06114506348967552\n",
      "Epoch 7506/20000 Training Loss: 0.06472271680831909\n",
      "Epoch 7507/20000 Training Loss: 0.06399635225534439\n",
      "Epoch 7508/20000 Training Loss: 0.05827638506889343\n",
      "Epoch 7509/20000 Training Loss: 0.06418470293283463\n",
      "Epoch 7510/20000 Training Loss: 0.059623707085847855\n",
      "Epoch 7510/20000 Validation Loss: 0.0450301356613636\n",
      "Epoch 7511/20000 Training Loss: 0.05993492901325226\n",
      "Epoch 7512/20000 Training Loss: 0.06389989703893661\n",
      "Epoch 7513/20000 Training Loss: 0.05332191288471222\n",
      "Epoch 7514/20000 Training Loss: 0.06247202679514885\n",
      "Epoch 7515/20000 Training Loss: 0.058135777711868286\n",
      "Epoch 7516/20000 Training Loss: 0.0781063437461853\n",
      "Epoch 7517/20000 Training Loss: 0.052768439054489136\n",
      "Epoch 7518/20000 Training Loss: 0.06132929399609566\n",
      "Epoch 7519/20000 Training Loss: 0.0720856711268425\n",
      "Epoch 7520/20000 Training Loss: 0.06311500817537308\n",
      "Epoch 7520/20000 Validation Loss: 0.07329186052083969\n",
      "Epoch 7521/20000 Training Loss: 0.060036417096853256\n",
      "Epoch 7522/20000 Training Loss: 0.057731498032808304\n",
      "Epoch 7523/20000 Training Loss: 0.049849022179841995\n",
      "Epoch 7524/20000 Training Loss: 0.035635340958833694\n",
      "Epoch 7525/20000 Training Loss: 0.0683981254696846\n",
      "Epoch 7526/20000 Training Loss: 0.06176405027508736\n",
      "Epoch 7527/20000 Training Loss: 0.058450132608413696\n",
      "Epoch 7528/20000 Training Loss: 0.061730366200208664\n",
      "Epoch 7529/20000 Training Loss: 0.05511339008808136\n",
      "Epoch 7530/20000 Training Loss: 0.07791466265916824\n",
      "Epoch 7530/20000 Validation Loss: 0.060667574405670166\n",
      "Epoch 7531/20000 Training Loss: 0.064031220972538\n",
      "Epoch 7532/20000 Training Loss: 0.06654658168554306\n",
      "Epoch 7533/20000 Training Loss: 0.04026219621300697\n",
      "Epoch 7534/20000 Training Loss: 0.060760047286748886\n",
      "Epoch 7535/20000 Training Loss: 0.05283947288990021\n",
      "Epoch 7536/20000 Training Loss: 0.05061367154121399\n",
      "Epoch 7537/20000 Training Loss: 0.07483115792274475\n",
      "Epoch 7538/20000 Training Loss: 0.06423568725585938\n",
      "Epoch 7539/20000 Training Loss: 0.04350079968571663\n",
      "Epoch 7540/20000 Training Loss: 0.05312938988208771\n",
      "Epoch 7540/20000 Validation Loss: 0.046043142676353455\n",
      "Epoch 7541/20000 Training Loss: 0.0456702746450901\n",
      "Epoch 7542/20000 Training Loss: 0.04751737043261528\n",
      "Epoch 7543/20000 Training Loss: 0.045853063464164734\n",
      "Epoch 7544/20000 Training Loss: 0.07801179587841034\n",
      "Epoch 7545/20000 Training Loss: 0.04817510023713112\n",
      "Epoch 7546/20000 Training Loss: 0.04368529096245766\n",
      "Epoch 7547/20000 Training Loss: 0.048513397574424744\n",
      "Epoch 7548/20000 Training Loss: 0.05883421376347542\n",
      "Epoch 7549/20000 Training Loss: 0.059859249740839005\n",
      "Epoch 7550/20000 Training Loss: 0.066560298204422\n",
      "Epoch 7550/20000 Validation Loss: 0.06198122352361679\n",
      "Epoch 7551/20000 Training Loss: 0.04610053077340126\n",
      "Epoch 7552/20000 Training Loss: 0.06762517243623734\n",
      "Epoch 7553/20000 Training Loss: 0.05473316088318825\n",
      "Epoch 7554/20000 Training Loss: 0.05651690438389778\n",
      "Epoch 7555/20000 Training Loss: 0.05464596673846245\n",
      "Epoch 7556/20000 Training Loss: 0.06260231137275696\n",
      "Epoch 7557/20000 Training Loss: 0.03968476876616478\n",
      "Epoch 7558/20000 Training Loss: 0.03926566615700722\n",
      "Epoch 7559/20000 Training Loss: 0.06101784482598305\n",
      "Epoch 7560/20000 Training Loss: 0.05075467750430107\n",
      "Epoch 7560/20000 Validation Loss: 0.06212948262691498\n",
      "Epoch 7561/20000 Training Loss: 0.076246477663517\n",
      "Epoch 7562/20000 Training Loss: 0.06245264410972595\n",
      "Epoch 7563/20000 Training Loss: 0.050622906535863876\n",
      "Epoch 7564/20000 Training Loss: 0.06382204592227936\n",
      "Epoch 7565/20000 Training Loss: 0.030287113040685654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7566/20000 Training Loss: 0.05182209610939026\n",
      "Epoch 7567/20000 Training Loss: 0.048542190343141556\n",
      "Epoch 7568/20000 Training Loss: 0.06623110920190811\n",
      "Epoch 7569/20000 Training Loss: 0.05621631070971489\n",
      "Epoch 7570/20000 Training Loss: 0.0634923055768013\n",
      "Epoch 7570/20000 Validation Loss: 0.06094086170196533\n",
      "Epoch 7571/20000 Training Loss: 0.09154009819030762\n",
      "Epoch 7572/20000 Training Loss: 0.066930390894413\n",
      "Epoch 7573/20000 Training Loss: 0.04994425177574158\n",
      "Epoch 7574/20000 Training Loss: 0.07587864249944687\n",
      "Epoch 7575/20000 Training Loss: 0.06417211145162582\n",
      "Epoch 7576/20000 Training Loss: 0.06558001041412354\n",
      "Epoch 7577/20000 Training Loss: 0.06101296469569206\n",
      "Epoch 7578/20000 Training Loss: 0.048842903226614\n",
      "Epoch 7579/20000 Training Loss: 0.05786721780896187\n",
      "Epoch 7580/20000 Training Loss: 0.08278688788414001\n",
      "Epoch 7580/20000 Validation Loss: 0.06374992430210114\n",
      "Epoch 7581/20000 Training Loss: 0.0610804557800293\n",
      "Epoch 7582/20000 Training Loss: 0.04984070733189583\n",
      "Epoch 7583/20000 Training Loss: 0.06552784889936447\n",
      "Epoch 7584/20000 Training Loss: 0.046306174248456955\n",
      "Epoch 7585/20000 Training Loss: 0.05442170426249504\n",
      "Epoch 7586/20000 Training Loss: 0.05126667022705078\n",
      "Epoch 7587/20000 Training Loss: 0.06403961032629013\n",
      "Epoch 7588/20000 Training Loss: 0.06058351695537567\n",
      "Epoch 7589/20000 Training Loss: 0.05571332573890686\n",
      "Epoch 7590/20000 Training Loss: 0.05785299465060234\n",
      "Epoch 7590/20000 Validation Loss: 0.043074704706668854\n",
      "Epoch 7591/20000 Training Loss: 0.06237150728702545\n",
      "Epoch 7592/20000 Training Loss: 0.04385070502758026\n",
      "Epoch 7593/20000 Training Loss: 0.057250991463661194\n",
      "Epoch 7594/20000 Training Loss: 0.0626991018652916\n",
      "Epoch 7595/20000 Training Loss: 0.05394860729575157\n",
      "Epoch 7596/20000 Training Loss: 0.04543624818325043\n",
      "Epoch 7597/20000 Training Loss: 0.06971051543951035\n",
      "Epoch 7598/20000 Training Loss: 0.05890202522277832\n",
      "Epoch 7599/20000 Training Loss: 0.06670155376195908\n",
      "Epoch 7600/20000 Training Loss: 0.06802133470773697\n",
      "Epoch 7600/20000 Validation Loss: 0.04891049489378929\n",
      "Epoch 7601/20000 Training Loss: 0.05432109534740448\n",
      "Epoch 7602/20000 Training Loss: 0.04417407512664795\n",
      "Epoch 7603/20000 Training Loss: 0.06576657295227051\n",
      "Epoch 7604/20000 Training Loss: 0.05391421914100647\n",
      "Epoch 7605/20000 Training Loss: 0.04961574450135231\n",
      "Epoch 7606/20000 Training Loss: 0.058565616607666016\n",
      "Epoch 7607/20000 Training Loss: 0.05242590233683586\n",
      "Epoch 7608/20000 Training Loss: 0.050958309322595596\n",
      "Epoch 7609/20000 Training Loss: 0.03956256061792374\n",
      "Epoch 7610/20000 Training Loss: 0.0611131377518177\n",
      "Epoch 7610/20000 Validation Loss: 0.050462815910577774\n",
      "Epoch 7611/20000 Training Loss: 0.07495883852243423\n",
      "Epoch 7612/20000 Training Loss: 0.08642435073852539\n",
      "Epoch 7613/20000 Training Loss: 0.06080326810479164\n",
      "Epoch 7614/20000 Training Loss: 0.05028218403458595\n",
      "Epoch 7615/20000 Training Loss: 0.05731168016791344\n",
      "Epoch 7616/20000 Training Loss: 0.0696641206741333\n",
      "Epoch 7617/20000 Training Loss: 0.044219959527254105\n",
      "Epoch 7618/20000 Training Loss: 0.07770569622516632\n",
      "Epoch 7619/20000 Training Loss: 0.054593633860349655\n",
      "Epoch 7620/20000 Training Loss: 0.061188433319330215\n",
      "Epoch 7620/20000 Validation Loss: 0.05254168063402176\n",
      "Epoch 7621/20000 Training Loss: 0.05996950343251228\n",
      "Epoch 7622/20000 Training Loss: 0.05820571258664131\n",
      "Epoch 7623/20000 Training Loss: 0.07136132568120956\n",
      "Epoch 7624/20000 Training Loss: 0.07381460070610046\n",
      "Epoch 7625/20000 Training Loss: 0.06033746898174286\n",
      "Epoch 7626/20000 Training Loss: 0.0433650016784668\n",
      "Epoch 7627/20000 Training Loss: 0.053005535155534744\n",
      "Epoch 7628/20000 Training Loss: 0.06948604434728622\n",
      "Epoch 7629/20000 Training Loss: 0.06883060187101364\n",
      "Epoch 7630/20000 Training Loss: 0.06622082740068436\n",
      "Epoch 7630/20000 Validation Loss: 0.06222548708319664\n",
      "Epoch 7631/20000 Training Loss: 0.07278720289468765\n",
      "Epoch 7632/20000 Training Loss: 0.0586385577917099\n",
      "Epoch 7633/20000 Training Loss: 0.05628691241145134\n",
      "Epoch 7634/20000 Training Loss: 0.04300972819328308\n",
      "Epoch 7635/20000 Training Loss: 0.048153821378946304\n",
      "Epoch 7636/20000 Training Loss: 0.04998219013214111\n",
      "Epoch 7637/20000 Training Loss: 0.0548492856323719\n",
      "Epoch 7638/20000 Training Loss: 0.054816048592329025\n",
      "Epoch 7639/20000 Training Loss: 0.06503184884786606\n",
      "Epoch 7640/20000 Training Loss: 0.0621216781437397\n",
      "Epoch 7640/20000 Validation Loss: 0.08664363622665405\n",
      "Epoch 7641/20000 Training Loss: 0.05391949415206909\n",
      "Epoch 7642/20000 Training Loss: 0.0701303482055664\n",
      "Epoch 7643/20000 Training Loss: 0.06284614652395248\n",
      "Epoch 7644/20000 Training Loss: 0.048456136137247086\n",
      "Epoch 7645/20000 Training Loss: 0.046578481793403625\n",
      "Epoch 7646/20000 Training Loss: 0.0593186654150486\n",
      "Epoch 7647/20000 Training Loss: 0.05314844846725464\n",
      "Epoch 7648/20000 Training Loss: 0.0446106493473053\n",
      "Epoch 7649/20000 Training Loss: 0.060788482427597046\n",
      "Epoch 7650/20000 Training Loss: 0.062300752848386765\n",
      "Epoch 7650/20000 Validation Loss: 0.061126552522182465\n",
      "Epoch 7651/20000 Training Loss: 0.03980233892798424\n",
      "Epoch 7652/20000 Training Loss: 0.04193545877933502\n",
      "Epoch 7653/20000 Training Loss: 0.07077392190694809\n",
      "Epoch 7654/20000 Training Loss: 0.06303957104682922\n",
      "Epoch 7655/20000 Training Loss: 0.05036666616797447\n",
      "Epoch 7656/20000 Training Loss: 0.06697224825620651\n",
      "Epoch 7657/20000 Training Loss: 0.049029503017663956\n",
      "Epoch 7658/20000 Training Loss: 0.07104887813329697\n",
      "Epoch 7659/20000 Training Loss: 0.056944165378808975\n",
      "Epoch 7660/20000 Training Loss: 0.05735410749912262\n",
      "Epoch 7660/20000 Validation Loss: 0.06645102798938751\n",
      "Epoch 7661/20000 Training Loss: 0.056867580860853195\n",
      "Epoch 7662/20000 Training Loss: 0.046501170843839645\n",
      "Epoch 7663/20000 Training Loss: 0.04336400702595711\n",
      "Epoch 7664/20000 Training Loss: 0.062187712639570236\n",
      "Epoch 7665/20000 Training Loss: 0.07403429597616196\n",
      "Epoch 7666/20000 Training Loss: 0.06365103274583817\n",
      "Epoch 7667/20000 Training Loss: 0.04846922680735588\n",
      "Epoch 7668/20000 Training Loss: 0.0569208599627018\n",
      "Epoch 7669/20000 Training Loss: 0.05031021311879158\n",
      "Epoch 7670/20000 Training Loss: 0.06637152284383774\n",
      "Epoch 7670/20000 Validation Loss: 0.05621136724948883\n",
      "Epoch 7671/20000 Training Loss: 0.06570873409509659\n",
      "Epoch 7672/20000 Training Loss: 0.07011602073907852\n",
      "Epoch 7673/20000 Training Loss: 0.04363636299967766\n",
      "Epoch 7674/20000 Training Loss: 0.07309570163488388\n",
      "Epoch 7675/20000 Training Loss: 0.07278752326965332\n",
      "Epoch 7676/20000 Training Loss: 0.06324198842048645\n",
      "Epoch 7677/20000 Training Loss: 0.052306920289993286\n",
      "Epoch 7678/20000 Training Loss: 0.05239221081137657\n",
      "Epoch 7679/20000 Training Loss: 0.05253267660737038\n",
      "Epoch 7680/20000 Training Loss: 0.07726521044969559\n",
      "Epoch 7680/20000 Validation Loss: 0.08883527666330338\n",
      "Epoch 7681/20000 Training Loss: 0.05402854457497597\n",
      "Epoch 7682/20000 Training Loss: 0.05787147209048271\n",
      "Epoch 7683/20000 Training Loss: 0.058017272502183914\n",
      "Epoch 7684/20000 Training Loss: 0.04317508637905121\n",
      "Epoch 7685/20000 Training Loss: 0.055302221328020096\n",
      "Epoch 7686/20000 Training Loss: 0.0431782603263855\n",
      "Epoch 7687/20000 Training Loss: 0.051364585757255554\n",
      "Epoch 7688/20000 Training Loss: 0.04441388323903084\n",
      "Epoch 7689/20000 Training Loss: 0.06925920397043228\n",
      "Epoch 7690/20000 Training Loss: 0.05582493171095848\n",
      "Epoch 7690/20000 Validation Loss: 0.06902443617582321\n",
      "Epoch 7691/20000 Training Loss: 0.057260941714048386\n",
      "Epoch 7692/20000 Training Loss: 0.04468843340873718\n",
      "Epoch 7693/20000 Training Loss: 0.0564284473657608\n",
      "Epoch 7694/20000 Training Loss: 0.061799243092536926\n",
      "Epoch 7695/20000 Training Loss: 0.06454801559448242\n",
      "Epoch 7696/20000 Training Loss: 0.06152356043457985\n",
      "Epoch 7697/20000 Training Loss: 0.049705687910318375\n",
      "Epoch 7698/20000 Training Loss: 0.06354192644357681\n",
      "Epoch 7699/20000 Training Loss: 0.06249311938881874\n",
      "Epoch 7700/20000 Training Loss: 0.051318541169166565\n",
      "Epoch 7700/20000 Validation Loss: 0.04197799414396286\n",
      "Epoch 7701/20000 Training Loss: 0.05036889389157295\n",
      "Epoch 7702/20000 Training Loss: 0.053084567189216614\n",
      "Epoch 7703/20000 Training Loss: 0.06848401576280594\n",
      "Epoch 7704/20000 Training Loss: 0.06401287019252777\n",
      "Epoch 7705/20000 Training Loss: 0.06940024346113205\n",
      "Epoch 7706/20000 Training Loss: 0.04936623200774193\n",
      "Epoch 7707/20000 Training Loss: 0.07691672444343567\n",
      "Epoch 7708/20000 Training Loss: 0.06863778829574585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7709/20000 Training Loss: 0.05833056569099426\n",
      "Epoch 7710/20000 Training Loss: 0.06120288372039795\n",
      "Epoch 7710/20000 Validation Loss: 0.07235796749591827\n",
      "Epoch 7711/20000 Training Loss: 0.06451006233692169\n",
      "Epoch 7712/20000 Training Loss: 0.08443967252969742\n",
      "Epoch 7713/20000 Training Loss: 0.07250944525003433\n",
      "Epoch 7714/20000 Training Loss: 0.04729922115802765\n",
      "Epoch 7715/20000 Training Loss: 0.05606217309832573\n",
      "Epoch 7716/20000 Training Loss: 0.06697751581668854\n",
      "Epoch 7717/20000 Training Loss: 0.04547002911567688\n",
      "Epoch 7718/20000 Training Loss: 0.05090611055493355\n",
      "Epoch 7719/20000 Training Loss: 0.04868112504482269\n",
      "Epoch 7720/20000 Training Loss: 0.05872589349746704\n",
      "Epoch 7720/20000 Validation Loss: 0.04447373002767563\n",
      "Epoch 7721/20000 Training Loss: 0.07121270895004272\n",
      "Epoch 7722/20000 Training Loss: 0.05596347525715828\n",
      "Epoch 7723/20000 Training Loss: 0.03456854447722435\n",
      "Epoch 7724/20000 Training Loss: 0.07022304087877274\n",
      "Epoch 7725/20000 Training Loss: 0.05502454936504364\n",
      "Epoch 7726/20000 Training Loss: 0.05038457736372948\n",
      "Epoch 7727/20000 Training Loss: 0.08809901028871536\n",
      "Epoch 7728/20000 Training Loss: 0.06994736939668655\n",
      "Epoch 7729/20000 Training Loss: 0.05876306816935539\n",
      "Epoch 7730/20000 Training Loss: 0.04456866905093193\n",
      "Epoch 7730/20000 Validation Loss: 0.0631444901227951\n",
      "Epoch 7731/20000 Training Loss: 0.04121977463364601\n",
      "Epoch 7732/20000 Training Loss: 0.04573630169034004\n",
      "Epoch 7733/20000 Training Loss: 0.06212865933775902\n",
      "Epoch 7734/20000 Training Loss: 0.04588672146201134\n",
      "Epoch 7735/20000 Training Loss: 0.06494224071502686\n",
      "Epoch 7736/20000 Training Loss: 0.0548873245716095\n",
      "Epoch 7737/20000 Training Loss: 0.0657954066991806\n",
      "Epoch 7738/20000 Training Loss: 0.053275223821401596\n",
      "Epoch 7739/20000 Training Loss: 0.0637204647064209\n",
      "Epoch 7740/20000 Training Loss: 0.05200726166367531\n",
      "Epoch 7740/20000 Validation Loss: 0.05294744670391083\n",
      "Epoch 7741/20000 Training Loss: 0.056714147329330444\n",
      "Epoch 7742/20000 Training Loss: 0.054346635937690735\n",
      "Epoch 7743/20000 Training Loss: 0.047936152666807175\n",
      "Epoch 7744/20000 Training Loss: 0.05488695576786995\n",
      "Epoch 7745/20000 Training Loss: 0.03980943560600281\n",
      "Epoch 7746/20000 Training Loss: 0.04721398279070854\n",
      "Epoch 7747/20000 Training Loss: 0.06212572380900383\n",
      "Epoch 7748/20000 Training Loss: 0.06709983199834824\n",
      "Epoch 7749/20000 Training Loss: 0.05309779569506645\n",
      "Epoch 7750/20000 Training Loss: 0.059601377695798874\n",
      "Epoch 7750/20000 Validation Loss: 0.06263257563114166\n",
      "Epoch 7751/20000 Training Loss: 0.05345308780670166\n",
      "Epoch 7752/20000 Training Loss: 0.06801053136587143\n",
      "Epoch 7753/20000 Training Loss: 0.05217036232352257\n",
      "Epoch 7754/20000 Training Loss: 0.05545946955680847\n",
      "Epoch 7755/20000 Training Loss: 0.062066156417131424\n",
      "Epoch 7756/20000 Training Loss: 0.056026194244623184\n",
      "Epoch 7757/20000 Training Loss: 0.04736240580677986\n",
      "Epoch 7758/20000 Training Loss: 0.061578601598739624\n",
      "Epoch 7759/20000 Training Loss: 0.06467046588659286\n",
      "Epoch 7760/20000 Training Loss: 0.06838881224393845\n",
      "Epoch 7760/20000 Validation Loss: 0.059335023164749146\n",
      "Epoch 7761/20000 Training Loss: 0.05507391691207886\n",
      "Epoch 7762/20000 Training Loss: 0.04743781313300133\n",
      "Epoch 7763/20000 Training Loss: 0.06658684462308884\n",
      "Epoch 7764/20000 Training Loss: 0.05586402490735054\n",
      "Epoch 7765/20000 Training Loss: 0.06742388755083084\n",
      "Epoch 7766/20000 Training Loss: 0.05589042603969574\n",
      "Epoch 7767/20000 Training Loss: 0.05051998794078827\n",
      "Epoch 7768/20000 Training Loss: 0.046000462025403976\n",
      "Epoch 7769/20000 Training Loss: 0.050135288387537\n",
      "Epoch 7770/20000 Training Loss: 0.05481580272316933\n",
      "Epoch 7770/20000 Validation Loss: 0.037500228732824326\n",
      "Epoch 7771/20000 Training Loss: 0.05965157225728035\n",
      "Epoch 7772/20000 Training Loss: 0.049306321889162064\n",
      "Epoch 7773/20000 Training Loss: 0.06566237658262253\n",
      "Epoch 7774/20000 Training Loss: 0.06209440529346466\n",
      "Epoch 7775/20000 Training Loss: 0.040930647403001785\n",
      "Epoch 7776/20000 Training Loss: 0.06425473839044571\n",
      "Epoch 7777/20000 Training Loss: 0.05432352423667908\n",
      "Epoch 7778/20000 Training Loss: 0.05023420229554176\n",
      "Epoch 7779/20000 Training Loss: 0.051800962537527084\n",
      "Epoch 7780/20000 Training Loss: 0.0607101209461689\n",
      "Epoch 7780/20000 Validation Loss: 0.03934456408023834\n",
      "Epoch 7781/20000 Training Loss: 0.05317317321896553\n",
      "Epoch 7782/20000 Training Loss: 0.06802322715520859\n",
      "Epoch 7783/20000 Training Loss: 0.04108339175581932\n",
      "Epoch 7784/20000 Training Loss: 0.053970009088516235\n",
      "Epoch 7785/20000 Training Loss: 0.05012206733226776\n",
      "Epoch 7786/20000 Training Loss: 0.05917708948254585\n",
      "Epoch 7787/20000 Training Loss: 0.055489689111709595\n",
      "Epoch 7788/20000 Training Loss: 0.042865265160799026\n",
      "Epoch 7789/20000 Training Loss: 0.06395342200994492\n",
      "Epoch 7790/20000 Training Loss: 0.050222307443618774\n",
      "Epoch 7790/20000 Validation Loss: 0.05518069118261337\n",
      "Epoch 7791/20000 Training Loss: 0.06143760681152344\n",
      "Epoch 7792/20000 Training Loss: 0.06372281163930893\n",
      "Epoch 7793/20000 Training Loss: 0.05038538575172424\n",
      "Epoch 7794/20000 Training Loss: 0.06595000624656677\n",
      "Epoch 7795/20000 Training Loss: 0.05966280773282051\n",
      "Epoch 7796/20000 Training Loss: 0.056327417492866516\n",
      "Epoch 7797/20000 Training Loss: 0.03854852542281151\n",
      "Epoch 7798/20000 Training Loss: 0.08248037844896317\n",
      "Epoch 7799/20000 Training Loss: 0.0591914989054203\n",
      "Epoch 7800/20000 Training Loss: 0.05765850841999054\n",
      "Epoch 7800/20000 Validation Loss: 0.06601071357727051\n",
      "Epoch 7801/20000 Training Loss: 0.0648164227604866\n",
      "Epoch 7802/20000 Training Loss: 0.06401243805885315\n",
      "Epoch 7803/20000 Training Loss: 0.06961837410926819\n",
      "Epoch 7804/20000 Training Loss: 0.05597845837473869\n",
      "Epoch 7805/20000 Training Loss: 0.05906129255890846\n",
      "Epoch 7806/20000 Training Loss: 0.04725724831223488\n",
      "Epoch 7807/20000 Training Loss: 0.04677874967455864\n",
      "Epoch 7808/20000 Training Loss: 0.04643596336245537\n",
      "Epoch 7809/20000 Training Loss: 0.05172998085618019\n",
      "Epoch 7810/20000 Training Loss: 0.059159357100725174\n",
      "Epoch 7810/20000 Validation Loss: 0.06424374878406525\n",
      "Epoch 7811/20000 Training Loss: 0.07851593941450119\n",
      "Epoch 7812/20000 Training Loss: 0.06345266848802567\n",
      "Epoch 7813/20000 Training Loss: 0.04780418053269386\n",
      "Epoch 7814/20000 Training Loss: 0.038772061467170715\n",
      "Epoch 7815/20000 Training Loss: 0.03835048899054527\n",
      "Epoch 7816/20000 Training Loss: 0.067911796271801\n",
      "Epoch 7817/20000 Training Loss: 0.05060739442706108\n",
      "Epoch 7818/20000 Training Loss: 0.046647101640701294\n",
      "Epoch 7819/20000 Training Loss: 0.07750577479600906\n",
      "Epoch 7820/20000 Training Loss: 0.055451493710279465\n",
      "Epoch 7820/20000 Validation Loss: 0.05158095434308052\n",
      "Epoch 7821/20000 Training Loss: 0.0489824116230011\n",
      "Epoch 7822/20000 Training Loss: 0.051300108432769775\n",
      "Epoch 7823/20000 Training Loss: 0.059303030371665955\n",
      "Epoch 7824/20000 Training Loss: 0.06759054213762283\n",
      "Epoch 7825/20000 Training Loss: 0.04426681622862816\n",
      "Epoch 7826/20000 Training Loss: 0.0591609887778759\n",
      "Epoch 7827/20000 Training Loss: 0.046874869614839554\n",
      "Epoch 7828/20000 Training Loss: 0.06816922873258591\n",
      "Epoch 7829/20000 Training Loss: 0.05447034537792206\n",
      "Epoch 7830/20000 Training Loss: 0.0505484938621521\n",
      "Epoch 7830/20000 Validation Loss: 0.043440937995910645\n",
      "Epoch 7831/20000 Training Loss: 0.0689731016755104\n",
      "Epoch 7832/20000 Training Loss: 0.06122037395834923\n",
      "Epoch 7833/20000 Training Loss: 0.07239200919866562\n",
      "Epoch 7834/20000 Training Loss: 0.05268866941332817\n",
      "Epoch 7835/20000 Training Loss: 0.045401692390441895\n",
      "Epoch 7836/20000 Training Loss: 0.06722013652324677\n",
      "Epoch 7837/20000 Training Loss: 0.05528360232710838\n",
      "Epoch 7838/20000 Training Loss: 0.06994781643152237\n",
      "Epoch 7839/20000 Training Loss: 0.06496485322713852\n",
      "Epoch 7840/20000 Training Loss: 0.061566952615976334\n",
      "Epoch 7840/20000 Validation Loss: 0.06283830851316452\n",
      "Epoch 7841/20000 Training Loss: 0.06088893488049507\n",
      "Epoch 7842/20000 Training Loss: 0.06792408227920532\n",
      "Epoch 7843/20000 Training Loss: 0.05799238011240959\n",
      "Epoch 7844/20000 Training Loss: 0.059950053691864014\n",
      "Epoch 7845/20000 Training Loss: 0.04282395541667938\n",
      "Epoch 7846/20000 Training Loss: 0.058300625532865524\n",
      "Epoch 7847/20000 Training Loss: 0.04731510952115059\n",
      "Epoch 7848/20000 Training Loss: 0.04900021851062775\n",
      "Epoch 7849/20000 Training Loss: 0.05996375158429146\n",
      "Epoch 7850/20000 Training Loss: 0.06812532991170883\n",
      "Epoch 7850/20000 Validation Loss: 0.06713362038135529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7851/20000 Training Loss: 0.06640633195638657\n",
      "Epoch 7852/20000 Training Loss: 0.05221791937947273\n",
      "Epoch 7853/20000 Training Loss: 0.052530866116285324\n",
      "Epoch 7854/20000 Training Loss: 0.06590159982442856\n",
      "Epoch 7855/20000 Training Loss: 0.05630141496658325\n",
      "Epoch 7856/20000 Training Loss: 0.037594348192214966\n",
      "Epoch 7857/20000 Training Loss: 0.038863372057676315\n",
      "Epoch 7858/20000 Training Loss: 0.0560753233730793\n",
      "Epoch 7859/20000 Training Loss: 0.06002971529960632\n",
      "Epoch 7860/20000 Training Loss: 0.0630442202091217\n",
      "Epoch 7860/20000 Validation Loss: 0.06582692265510559\n",
      "Epoch 7861/20000 Training Loss: 0.05557483062148094\n",
      "Epoch 7862/20000 Training Loss: 0.06932462006807327\n",
      "Epoch 7863/20000 Training Loss: 0.05248265340924263\n",
      "Epoch 7864/20000 Training Loss: 0.05205691233277321\n",
      "Epoch 7865/20000 Training Loss: 0.07026691734790802\n",
      "Epoch 7866/20000 Training Loss: 0.0684916153550148\n",
      "Epoch 7867/20000 Training Loss: 0.07500452548265457\n",
      "Epoch 7868/20000 Training Loss: 0.0658922865986824\n",
      "Epoch 7869/20000 Training Loss: 0.05385631322860718\n",
      "Epoch 7870/20000 Training Loss: 0.06672412157058716\n",
      "Epoch 7870/20000 Validation Loss: 0.04604385793209076\n",
      "Epoch 7871/20000 Training Loss: 0.0656830444931984\n",
      "Epoch 7872/20000 Training Loss: 0.035912949591875076\n",
      "Epoch 7873/20000 Training Loss: 0.0790015161037445\n",
      "Epoch 7874/20000 Training Loss: 0.05094034597277641\n",
      "Epoch 7875/20000 Training Loss: 0.05438389256596565\n",
      "Epoch 7876/20000 Training Loss: 0.05654161795973778\n",
      "Epoch 7877/20000 Training Loss: 0.050090719014406204\n",
      "Epoch 7878/20000 Training Loss: 0.046645939350128174\n",
      "Epoch 7879/20000 Training Loss: 0.06577596068382263\n",
      "Epoch 7880/20000 Training Loss: 0.03843245282769203\n",
      "Epoch 7880/20000 Validation Loss: 0.07509767264127731\n",
      "Epoch 7881/20000 Training Loss: 0.06842862814664841\n",
      "Epoch 7882/20000 Training Loss: 0.0700734481215477\n",
      "Epoch 7883/20000 Training Loss: 0.04433373734354973\n",
      "Epoch 7884/20000 Training Loss: 0.0644945353269577\n",
      "Epoch 7885/20000 Training Loss: 0.05167800188064575\n",
      "Epoch 7886/20000 Training Loss: 0.044997889548540115\n",
      "Epoch 7887/20000 Training Loss: 0.05086694285273552\n",
      "Epoch 7888/20000 Training Loss: 0.06352514773607254\n",
      "Epoch 7889/20000 Training Loss: 0.07235687226057053\n",
      "Epoch 7890/20000 Training Loss: 0.07007564604282379\n",
      "Epoch 7890/20000 Validation Loss: 0.049338094890117645\n",
      "Epoch 7891/20000 Training Loss: 0.047213029116392136\n",
      "Epoch 7892/20000 Training Loss: 0.058831650763750076\n",
      "Epoch 7893/20000 Training Loss: 0.057950764894485474\n",
      "Epoch 7894/20000 Training Loss: 0.06264331191778183\n",
      "Epoch 7895/20000 Training Loss: 0.06122425198554993\n",
      "Epoch 7896/20000 Training Loss: 0.037586554884910583\n",
      "Epoch 7897/20000 Training Loss: 0.058914151042699814\n",
      "Epoch 7898/20000 Training Loss: 0.05034599080681801\n",
      "Epoch 7899/20000 Training Loss: 0.059739887714385986\n",
      "Epoch 7900/20000 Training Loss: 0.04315546527504921\n",
      "Epoch 7900/20000 Validation Loss: 0.060396458953619\n",
      "Epoch 7901/20000 Training Loss: 0.05662528797984123\n",
      "Epoch 7902/20000 Training Loss: 0.05641424283385277\n",
      "Epoch 7903/20000 Training Loss: 0.07135895639657974\n",
      "Epoch 7904/20000 Training Loss: 0.04012665897607803\n",
      "Epoch 7905/20000 Training Loss: 0.05459656938910484\n",
      "Epoch 7906/20000 Training Loss: 0.06441239267587662\n",
      "Epoch 7907/20000 Training Loss: 0.05636262521147728\n",
      "Epoch 7908/20000 Training Loss: 0.055757757276296616\n",
      "Epoch 7909/20000 Training Loss: 0.06719555705785751\n",
      "Epoch 7910/20000 Training Loss: 0.06284021586179733\n",
      "Epoch 7910/20000 Validation Loss: 0.05996274575591087\n",
      "Epoch 7911/20000 Training Loss: 0.06102500855922699\n",
      "Epoch 7912/20000 Training Loss: 0.038750458508729935\n",
      "Epoch 7913/20000 Training Loss: 0.043280962854623795\n",
      "Epoch 7914/20000 Training Loss: 0.03946246579289436\n",
      "Epoch 7915/20000 Training Loss: 0.04360978305339813\n",
      "Epoch 7916/20000 Training Loss: 0.07531248778104782\n",
      "Epoch 7917/20000 Training Loss: 0.054907020181417465\n",
      "Epoch 7918/20000 Training Loss: 0.05921017751097679\n",
      "Epoch 7919/20000 Training Loss: 0.05584903433918953\n",
      "Epoch 7920/20000 Training Loss: 0.05516313388943672\n",
      "Epoch 7920/20000 Validation Loss: 0.07514674216508865\n",
      "Epoch 7921/20000 Training Loss: 0.04679090902209282\n",
      "Epoch 7922/20000 Training Loss: 0.06856835633516312\n",
      "Epoch 7923/20000 Training Loss: 0.03969171643257141\n",
      "Epoch 7924/20000 Training Loss: 0.053418826311826706\n",
      "Epoch 7925/20000 Training Loss: 0.06098735332489014\n",
      "Epoch 7926/20000 Training Loss: 0.05119936540722847\n",
      "Epoch 7927/20000 Training Loss: 0.048973482102155685\n",
      "Epoch 7928/20000 Training Loss: 0.07001210004091263\n",
      "Epoch 7929/20000 Training Loss: 0.05852597951889038\n",
      "Epoch 7930/20000 Training Loss: 0.04417736455798149\n",
      "Epoch 7930/20000 Validation Loss: 0.05672232061624527\n",
      "Epoch 7931/20000 Training Loss: 0.0698598101735115\n",
      "Epoch 7932/20000 Training Loss: 0.0648818388581276\n",
      "Epoch 7933/20000 Training Loss: 0.0699770525097847\n",
      "Epoch 7934/20000 Training Loss: 0.05363035574555397\n",
      "Epoch 7935/20000 Training Loss: 0.05872434005141258\n",
      "Epoch 7936/20000 Training Loss: 0.0663096085190773\n",
      "Epoch 7937/20000 Training Loss: 0.06125780940055847\n",
      "Epoch 7938/20000 Training Loss: 0.04004433751106262\n",
      "Epoch 7939/20000 Training Loss: 0.060389965772628784\n",
      "Epoch 7940/20000 Training Loss: 0.03892771899700165\n",
      "Epoch 7940/20000 Validation Loss: 0.05825469642877579\n",
      "Epoch 7941/20000 Training Loss: 0.060643915086984634\n",
      "Epoch 7942/20000 Training Loss: 0.057528410106897354\n",
      "Epoch 7943/20000 Training Loss: 0.05036686733365059\n",
      "Epoch 7944/20000 Training Loss: 0.07046869397163391\n",
      "Epoch 7945/20000 Training Loss: 0.06010322645306587\n",
      "Epoch 7946/20000 Training Loss: 0.0805797353386879\n",
      "Epoch 7947/20000 Training Loss: 0.06201146915555\n",
      "Epoch 7948/20000 Training Loss: 0.05649489536881447\n",
      "Epoch 7949/20000 Training Loss: 0.04858620837330818\n",
      "Epoch 7950/20000 Training Loss: 0.05234566703438759\n",
      "Epoch 7950/20000 Validation Loss: 0.05754999443888664\n",
      "Epoch 7951/20000 Training Loss: 0.051083922386169434\n",
      "Epoch 7952/20000 Training Loss: 0.040891408920288086\n",
      "Epoch 7953/20000 Training Loss: 0.0504511296749115\n",
      "Epoch 7954/20000 Training Loss: 0.03719105198979378\n",
      "Epoch 7955/20000 Training Loss: 0.053129181265830994\n",
      "Epoch 7956/20000 Training Loss: 0.05583004280924797\n",
      "Epoch 7957/20000 Training Loss: 0.0603509359061718\n",
      "Epoch 7958/20000 Training Loss: 0.06414083391427994\n",
      "Epoch 7959/20000 Training Loss: 0.05698032304644585\n",
      "Epoch 7960/20000 Training Loss: 0.06301746517419815\n",
      "Epoch 7960/20000 Validation Loss: 0.07664550840854645\n",
      "Epoch 7961/20000 Training Loss: 0.04636583849787712\n",
      "Epoch 7962/20000 Training Loss: 0.06542941927909851\n",
      "Epoch 7963/20000 Training Loss: 0.04290245845913887\n",
      "Epoch 7964/20000 Training Loss: 0.06414974480867386\n",
      "Epoch 7965/20000 Training Loss: 0.060146063566207886\n",
      "Epoch 7966/20000 Training Loss: 0.05655442550778389\n",
      "Epoch 7967/20000 Training Loss: 0.05499544367194176\n",
      "Epoch 7968/20000 Training Loss: 0.07319841533899307\n",
      "Epoch 7969/20000 Training Loss: 0.06603176146745682\n",
      "Epoch 7970/20000 Training Loss: 0.043979763984680176\n",
      "Epoch 7970/20000 Validation Loss: 0.049570873379707336\n",
      "Epoch 7971/20000 Training Loss: 0.05461542680859566\n",
      "Epoch 7972/20000 Training Loss: 0.044255610555410385\n",
      "Epoch 7973/20000 Training Loss: 0.06460358947515488\n",
      "Epoch 7974/20000 Training Loss: 0.04841920733451843\n",
      "Epoch 7975/20000 Training Loss: 0.055831000208854675\n",
      "Epoch 7976/20000 Training Loss: 0.045804571360349655\n",
      "Epoch 7977/20000 Training Loss: 0.03432980924844742\n",
      "Epoch 7978/20000 Training Loss: 0.058875322341918945\n",
      "Epoch 7979/20000 Training Loss: 0.055021170526742935\n",
      "Epoch 7980/20000 Training Loss: 0.057641711086034775\n",
      "Epoch 7980/20000 Validation Loss: 0.04530449956655502\n",
      "Epoch 7981/20000 Training Loss: 0.05376410484313965\n",
      "Epoch 7982/20000 Training Loss: 0.045079704374074936\n",
      "Epoch 7983/20000 Training Loss: 0.055056262761354446\n",
      "Epoch 7984/20000 Training Loss: 0.04475526511669159\n",
      "Epoch 7985/20000 Training Loss: 0.055457595735788345\n",
      "Epoch 7986/20000 Training Loss: 0.06232066825032234\n",
      "Epoch 7987/20000 Training Loss: 0.0544009692966938\n",
      "Epoch 7988/20000 Training Loss: 0.04581513628363609\n",
      "Epoch 7989/20000 Training Loss: 0.05296798050403595\n",
      "Epoch 7990/20000 Training Loss: 0.06301232427358627\n",
      "Epoch 7990/20000 Validation Loss: 0.0637466311454773\n",
      "Epoch 7991/20000 Training Loss: 0.06596465408802032\n",
      "Epoch 7992/20000 Training Loss: 0.05546988919377327\n",
      "Epoch 7993/20000 Training Loss: 0.057096850126981735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7994/20000 Training Loss: 0.05000586807727814\n",
      "Epoch 7995/20000 Training Loss: 0.06013241037726402\n",
      "Epoch 7996/20000 Training Loss: 0.04684160277247429\n",
      "Epoch 7997/20000 Training Loss: 0.04008908569812775\n",
      "Epoch 7998/20000 Training Loss: 0.06576325744390488\n",
      "Epoch 7999/20000 Training Loss: 0.055993035435676575\n",
      "Epoch 8000/20000 Training Loss: 0.06774333119392395\n",
      "Epoch 8000/20000 Validation Loss: 0.0533476248383522\n",
      "Epoch 8001/20000 Training Loss: 0.07262706011533737\n",
      "Epoch 8002/20000 Training Loss: 0.05305280163884163\n",
      "Epoch 8003/20000 Training Loss: 0.06333911418914795\n",
      "Epoch 8004/20000 Training Loss: 0.0756722018122673\n",
      "Epoch 8005/20000 Training Loss: 0.04344971105456352\n",
      "Epoch 8006/20000 Training Loss: 0.043867453932762146\n",
      "Epoch 8007/20000 Training Loss: 0.06470992416143417\n",
      "Epoch 8008/20000 Training Loss: 0.045511990785598755\n",
      "Epoch 8009/20000 Training Loss: 0.048778753727674484\n",
      "Epoch 8010/20000 Training Loss: 0.05763178691267967\n",
      "Epoch 8010/20000 Validation Loss: 0.0463436096906662\n",
      "Epoch 8011/20000 Training Loss: 0.05064775049686432\n",
      "Epoch 8012/20000 Training Loss: 0.08229682594537735\n",
      "Epoch 8013/20000 Training Loss: 0.0686086043715477\n",
      "Epoch 8014/20000 Training Loss: 0.04536445066332817\n",
      "Epoch 8015/20000 Training Loss: 0.05025744438171387\n",
      "Epoch 8016/20000 Training Loss: 0.07425237447023392\n",
      "Epoch 8017/20000 Training Loss: 0.06461485475301743\n",
      "Epoch 8018/20000 Training Loss: 0.06633207201957703\n",
      "Epoch 8019/20000 Training Loss: 0.05430413782596588\n",
      "Epoch 8020/20000 Training Loss: 0.06143123283982277\n",
      "Epoch 8020/20000 Validation Loss: 0.03641214221715927\n",
      "Epoch 8021/20000 Training Loss: 0.06194276735186577\n",
      "Epoch 8022/20000 Training Loss: 0.04475393891334534\n",
      "Epoch 8023/20000 Training Loss: 0.04314655438065529\n",
      "Epoch 8024/20000 Training Loss: 0.06103330850601196\n",
      "Epoch 8025/20000 Training Loss: 0.04952450096607208\n",
      "Epoch 8026/20000 Training Loss: 0.05015703663229942\n",
      "Epoch 8027/20000 Training Loss: 0.042435090988874435\n",
      "Epoch 8028/20000 Training Loss: 0.06385515630245209\n",
      "Epoch 8029/20000 Training Loss: 0.06186315417289734\n",
      "Epoch 8030/20000 Training Loss: 0.08469688147306442\n",
      "Epoch 8030/20000 Validation Loss: 0.06062977388501167\n",
      "Epoch 8031/20000 Training Loss: 0.04047144576907158\n",
      "Epoch 8032/20000 Training Loss: 0.047364067286252975\n",
      "Epoch 8033/20000 Training Loss: 0.05186471343040466\n",
      "Epoch 8034/20000 Training Loss: 0.06768757849931717\n",
      "Epoch 8035/20000 Training Loss: 0.03706073760986328\n",
      "Epoch 8036/20000 Training Loss: 0.05676199495792389\n",
      "Epoch 8037/20000 Training Loss: 0.06084972992539406\n",
      "Epoch 8038/20000 Training Loss: 0.05033844709396362\n",
      "Epoch 8039/20000 Training Loss: 0.0498633086681366\n",
      "Epoch 8040/20000 Training Loss: 0.0652502030134201\n",
      "Epoch 8040/20000 Validation Loss: 0.07007906585931778\n",
      "Epoch 8041/20000 Training Loss: 0.054516881704330444\n",
      "Epoch 8042/20000 Training Loss: 0.041889775544404984\n",
      "Epoch 8043/20000 Training Loss: 0.06083894893527031\n",
      "Epoch 8044/20000 Training Loss: 0.03902572765946388\n",
      "Epoch 8045/20000 Training Loss: 0.057905156165361404\n",
      "Epoch 8046/20000 Training Loss: 0.057342588901519775\n",
      "Epoch 8047/20000 Training Loss: 0.05835422873497009\n",
      "Epoch 8048/20000 Training Loss: 0.047187406569719315\n",
      "Epoch 8049/20000 Training Loss: 0.054141849279403687\n",
      "Epoch 8050/20000 Training Loss: 0.05385463312268257\n",
      "Epoch 8050/20000 Validation Loss: 0.06451873481273651\n",
      "Epoch 8051/20000 Training Loss: 0.0665687620639801\n",
      "Epoch 8052/20000 Training Loss: 0.054602351039648056\n",
      "Epoch 8053/20000 Training Loss: 0.04283243417739868\n",
      "Epoch 8054/20000 Training Loss: 0.05614442005753517\n",
      "Epoch 8055/20000 Training Loss: 0.06638278812170029\n",
      "Epoch 8056/20000 Training Loss: 0.05875055491924286\n",
      "Epoch 8057/20000 Training Loss: 0.0559840202331543\n",
      "Epoch 8058/20000 Training Loss: 0.05337262153625488\n",
      "Epoch 8059/20000 Training Loss: 0.06001114472746849\n",
      "Epoch 8060/20000 Training Loss: 0.04889495298266411\n",
      "Epoch 8060/20000 Validation Loss: 0.08092334866523743\n",
      "Epoch 8061/20000 Training Loss: 0.05405775085091591\n",
      "Epoch 8062/20000 Training Loss: 0.05157606676220894\n",
      "Epoch 8063/20000 Training Loss: 0.054819028824567795\n",
      "Epoch 8064/20000 Training Loss: 0.07032033056020737\n",
      "Epoch 8065/20000 Training Loss: 0.057814449071884155\n",
      "Epoch 8066/20000 Training Loss: 0.05477006733417511\n",
      "Epoch 8067/20000 Training Loss: 0.06274402886629105\n",
      "Epoch 8068/20000 Training Loss: 0.06056855246424675\n",
      "Epoch 8069/20000 Training Loss: 0.06586744636297226\n",
      "Epoch 8070/20000 Training Loss: 0.054959189146757126\n",
      "Epoch 8070/20000 Validation Loss: 0.08442416787147522\n",
      "Epoch 8071/20000 Training Loss: 0.05679174140095711\n",
      "Epoch 8072/20000 Training Loss: 0.05768581107258797\n",
      "Epoch 8073/20000 Training Loss: 0.06689143925905228\n",
      "Epoch 8074/20000 Training Loss: 0.0625658631324768\n",
      "Epoch 8075/20000 Training Loss: 0.0581938736140728\n",
      "Epoch 8076/20000 Training Loss: 0.05380299314856529\n",
      "Epoch 8077/20000 Training Loss: 0.04794275760650635\n",
      "Epoch 8078/20000 Training Loss: 0.05245921015739441\n",
      "Epoch 8079/20000 Training Loss: 0.07812982052564621\n",
      "Epoch 8080/20000 Training Loss: 0.053384508937597275\n",
      "Epoch 8080/20000 Validation Loss: 0.06887142360210419\n",
      "Epoch 8081/20000 Training Loss: 0.07291688770055771\n",
      "Epoch 8082/20000 Training Loss: 0.06992346793413162\n",
      "Epoch 8083/20000 Training Loss: 0.05666184425354004\n",
      "Epoch 8084/20000 Training Loss: 0.04629071429371834\n",
      "Epoch 8085/20000 Training Loss: 0.050407614558935165\n",
      "Epoch 8086/20000 Training Loss: 0.057675883173942566\n",
      "Epoch 8087/20000 Training Loss: 0.044118255376815796\n",
      "Epoch 8088/20000 Training Loss: 0.05690145492553711\n",
      "Epoch 8089/20000 Training Loss: 0.07164592295885086\n",
      "Epoch 8090/20000 Training Loss: 0.06840699166059494\n",
      "Epoch 8090/20000 Validation Loss: 0.042722903192043304\n",
      "Epoch 8091/20000 Training Loss: 0.06849262118339539\n",
      "Epoch 8092/20000 Training Loss: 0.057320207357406616\n",
      "Epoch 8093/20000 Training Loss: 0.0811188817024231\n",
      "Epoch 8094/20000 Training Loss: 0.08579143136739731\n",
      "Epoch 8095/20000 Training Loss: 0.06990104168653488\n",
      "Epoch 8096/20000 Training Loss: 0.06601932644844055\n",
      "Epoch 8097/20000 Training Loss: 0.059710126370191574\n",
      "Epoch 8098/20000 Training Loss: 0.06798747926950455\n",
      "Epoch 8099/20000 Training Loss: 0.05507586896419525\n",
      "Epoch 8100/20000 Training Loss: 0.07815396785736084\n",
      "Epoch 8100/20000 Validation Loss: 0.050241369754076004\n",
      "Epoch 8101/20000 Training Loss: 0.06345907598733902\n",
      "Epoch 8102/20000 Training Loss: 0.07725169509649277\n",
      "Epoch 8103/20000 Training Loss: 0.07027093321084976\n",
      "Epoch 8104/20000 Training Loss: 0.03761092945933342\n",
      "Epoch 8105/20000 Training Loss: 0.05302351340651512\n",
      "Epoch 8106/20000 Training Loss: 0.05181586742401123\n",
      "Epoch 8107/20000 Training Loss: 0.058927591890096664\n",
      "Epoch 8108/20000 Training Loss: 0.0669533982872963\n",
      "Epoch 8109/20000 Training Loss: 0.05037577077746391\n",
      "Epoch 8110/20000 Training Loss: 0.06463458389043808\n",
      "Epoch 8110/20000 Validation Loss: 0.07634234428405762\n",
      "Epoch 8111/20000 Training Loss: 0.05213017761707306\n",
      "Epoch 8112/20000 Training Loss: 0.057291507720947266\n",
      "Epoch 8113/20000 Training Loss: 0.051858291029930115\n",
      "Epoch 8114/20000 Training Loss: 0.046586859971284866\n",
      "Epoch 8115/20000 Training Loss: 0.05886290594935417\n",
      "Epoch 8116/20000 Training Loss: 0.04709417000412941\n",
      "Epoch 8117/20000 Training Loss: 0.04744988679885864\n",
      "Epoch 8118/20000 Training Loss: 0.0481061153113842\n",
      "Epoch 8119/20000 Training Loss: 0.06117279455065727\n",
      "Epoch 8120/20000 Training Loss: 0.05076540634036064\n",
      "Epoch 8120/20000 Validation Loss: 0.05915609002113342\n",
      "Epoch 8121/20000 Training Loss: 0.04046046733856201\n",
      "Epoch 8122/20000 Training Loss: 0.05105781555175781\n",
      "Epoch 8123/20000 Training Loss: 0.06744108349084854\n",
      "Epoch 8124/20000 Training Loss: 0.08159328997135162\n",
      "Epoch 8125/20000 Training Loss: 0.046899616718292236\n",
      "Epoch 8126/20000 Training Loss: 0.06252708286046982\n",
      "Epoch 8127/20000 Training Loss: 0.06465647369623184\n",
      "Epoch 8128/20000 Training Loss: 0.06921286135911942\n",
      "Epoch 8129/20000 Training Loss: 0.04388028010725975\n",
      "Epoch 8130/20000 Training Loss: 0.048104021698236465\n",
      "Epoch 8130/20000 Validation Loss: 0.04131484404206276\n",
      "Epoch 8131/20000 Training Loss: 0.05869461968541145\n",
      "Epoch 8132/20000 Training Loss: 0.0664513036608696\n",
      "Epoch 8133/20000 Training Loss: 0.07421207427978516\n",
      "Epoch 8134/20000 Training Loss: 0.05431860685348511\n",
      "Epoch 8135/20000 Training Loss: 0.06635992228984833\n",
      "Epoch 8136/20000 Training Loss: 0.0729527547955513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8137/20000 Training Loss: 0.05560535192489624\n",
      "Epoch 8138/20000 Training Loss: 0.06886804103851318\n",
      "Epoch 8139/20000 Training Loss: 0.0409410297870636\n",
      "Epoch 8140/20000 Training Loss: 0.05095573887228966\n",
      "Epoch 8140/20000 Validation Loss: 0.06982602924108505\n",
      "Epoch 8141/20000 Training Loss: 0.05980886146426201\n",
      "Epoch 8142/20000 Training Loss: 0.04759501293301582\n",
      "Epoch 8143/20000 Training Loss: 0.05200428143143654\n",
      "Epoch 8144/20000 Training Loss: 0.04948101565241814\n",
      "Epoch 8145/20000 Training Loss: 0.05840429663658142\n",
      "Epoch 8146/20000 Training Loss: 0.06708315014839172\n",
      "Epoch 8147/20000 Training Loss: 0.07156675308942795\n",
      "Epoch 8148/20000 Training Loss: 0.05586189031600952\n",
      "Epoch 8149/20000 Training Loss: 0.04527611657977104\n",
      "Epoch 8150/20000 Training Loss: 0.05855390802025795\n",
      "Epoch 8150/20000 Validation Loss: 0.06559194624423981\n",
      "Epoch 8151/20000 Training Loss: 0.05678589642047882\n",
      "Epoch 8152/20000 Training Loss: 0.04567326232790947\n",
      "Epoch 8153/20000 Training Loss: 0.0682348981499672\n",
      "Epoch 8154/20000 Training Loss: 0.0479801706969738\n",
      "Epoch 8155/20000 Training Loss: 0.054011691361665726\n",
      "Epoch 8156/20000 Training Loss: 0.06434469670057297\n",
      "Epoch 8157/20000 Training Loss: 0.06419933587312698\n",
      "Epoch 8158/20000 Training Loss: 0.06402929872274399\n",
      "Epoch 8159/20000 Training Loss: 0.07703885436058044\n",
      "Epoch 8160/20000 Training Loss: 0.07189194113016129\n",
      "Epoch 8160/20000 Validation Loss: 0.03997112810611725\n",
      "Epoch 8161/20000 Training Loss: 0.06314822286367416\n",
      "Epoch 8162/20000 Training Loss: 0.05147552862763405\n",
      "Epoch 8163/20000 Training Loss: 0.06037769839167595\n",
      "Epoch 8164/20000 Training Loss: 0.0576322078704834\n",
      "Epoch 8165/20000 Training Loss: 0.05994977429509163\n",
      "Epoch 8166/20000 Training Loss: 0.04727175831794739\n",
      "Epoch 8167/20000 Training Loss: 0.04729937016963959\n",
      "Epoch 8168/20000 Training Loss: 0.05954885482788086\n",
      "Epoch 8169/20000 Training Loss: 0.04492814466357231\n",
      "Epoch 8170/20000 Training Loss: 0.05284211412072182\n",
      "Epoch 8170/20000 Validation Loss: 0.07538309693336487\n",
      "Epoch 8171/20000 Training Loss: 0.0695786327123642\n",
      "Epoch 8172/20000 Training Loss: 0.04554436728358269\n",
      "Epoch 8173/20000 Training Loss: 0.05232636630535126\n",
      "Epoch 8174/20000 Training Loss: 0.04919258877635002\n",
      "Epoch 8175/20000 Training Loss: 0.05132646858692169\n",
      "Epoch 8176/20000 Training Loss: 0.05883465334773064\n",
      "Epoch 8177/20000 Training Loss: 0.03779999539256096\n",
      "Epoch 8178/20000 Training Loss: 0.05400124192237854\n",
      "Epoch 8179/20000 Training Loss: 0.04784660413861275\n",
      "Epoch 8180/20000 Training Loss: 0.06408215314149857\n",
      "Epoch 8180/20000 Validation Loss: 0.05216449499130249\n",
      "Epoch 8181/20000 Training Loss: 0.05925903096795082\n",
      "Epoch 8182/20000 Training Loss: 0.06197410449385643\n",
      "Epoch 8183/20000 Training Loss: 0.05478094518184662\n",
      "Epoch 8184/20000 Training Loss: 0.03441954031586647\n",
      "Epoch 8185/20000 Training Loss: 0.0415961891412735\n",
      "Epoch 8186/20000 Training Loss: 0.0460907407104969\n",
      "Epoch 8187/20000 Training Loss: 0.06099000945687294\n",
      "Epoch 8188/20000 Training Loss: 0.04746564105153084\n",
      "Epoch 8189/20000 Training Loss: 0.0867733284831047\n",
      "Epoch 8190/20000 Training Loss: 0.058510977774858475\n",
      "Epoch 8190/20000 Validation Loss: 0.05860142409801483\n",
      "Epoch 8191/20000 Training Loss: 0.043846990913152695\n",
      "Epoch 8192/20000 Training Loss: 0.043087854981422424\n",
      "Epoch 8193/20000 Training Loss: 0.057065967470407486\n",
      "Epoch 8194/20000 Training Loss: 0.058682870119810104\n",
      "Epoch 8195/20000 Training Loss: 0.06095350906252861\n",
      "Epoch 8196/20000 Training Loss: 0.06330176442861557\n",
      "Epoch 8197/20000 Training Loss: 0.05322219058871269\n",
      "Epoch 8198/20000 Training Loss: 0.06694776564836502\n",
      "Epoch 8199/20000 Training Loss: 0.061523765325546265\n",
      "Epoch 8200/20000 Training Loss: 0.049520764499902725\n",
      "Epoch 8200/20000 Validation Loss: 0.06638313829898834\n",
      "Epoch 8201/20000 Training Loss: 0.057334911078214645\n",
      "Epoch 8202/20000 Training Loss: 0.045207154005765915\n",
      "Epoch 8203/20000 Training Loss: 0.05208217725157738\n",
      "Epoch 8204/20000 Training Loss: 0.05471387133002281\n",
      "Epoch 8205/20000 Training Loss: 0.05232388898730278\n",
      "Epoch 8206/20000 Training Loss: 0.05950702354311943\n",
      "Epoch 8207/20000 Training Loss: 0.06881766766309738\n",
      "Epoch 8208/20000 Training Loss: 0.05595933273434639\n",
      "Epoch 8209/20000 Training Loss: 0.05666898190975189\n",
      "Epoch 8210/20000 Training Loss: 0.05806589126586914\n",
      "Epoch 8210/20000 Validation Loss: 0.05775968357920647\n",
      "Epoch 8211/20000 Training Loss: 0.07311850786209106\n",
      "Epoch 8212/20000 Training Loss: 0.06995783001184464\n",
      "Epoch 8213/20000 Training Loss: 0.048077791929244995\n",
      "Epoch 8214/20000 Training Loss: 0.04502210021018982\n",
      "Epoch 8215/20000 Training Loss: 0.07180333137512207\n",
      "Epoch 8216/20000 Training Loss: 0.060702335089445114\n",
      "Epoch 8217/20000 Training Loss: 0.04714706540107727\n",
      "Epoch 8218/20000 Training Loss: 0.03951096907258034\n",
      "Epoch 8219/20000 Training Loss: 0.05724355950951576\n",
      "Epoch 8220/20000 Training Loss: 0.0529916025698185\n",
      "Epoch 8220/20000 Validation Loss: 0.05014397203922272\n",
      "Epoch 8221/20000 Training Loss: 0.06447780877351761\n",
      "Epoch 8222/20000 Training Loss: 0.06800716370344162\n",
      "Epoch 8223/20000 Training Loss: 0.056881267577409744\n",
      "Epoch 8224/20000 Training Loss: 0.0601508803665638\n",
      "Epoch 8225/20000 Training Loss: 0.052011508494615555\n",
      "Epoch 8226/20000 Training Loss: 0.0640454813838005\n",
      "Epoch 8227/20000 Training Loss: 0.056137967854738235\n",
      "Epoch 8228/20000 Training Loss: 0.061559345573186874\n",
      "Epoch 8229/20000 Training Loss: 0.055591877549886703\n",
      "Epoch 8230/20000 Training Loss: 0.04125628247857094\n",
      "Epoch 8230/20000 Validation Loss: 0.06976267695426941\n",
      "Epoch 8231/20000 Training Loss: 0.07507842779159546\n",
      "Epoch 8232/20000 Training Loss: 0.05504170060157776\n",
      "Epoch 8233/20000 Training Loss: 0.05233846604824066\n",
      "Epoch 8234/20000 Training Loss: 0.04268332198262215\n",
      "Epoch 8235/20000 Training Loss: 0.07700961083173752\n",
      "Epoch 8236/20000 Training Loss: 0.051518991589546204\n",
      "Epoch 8237/20000 Training Loss: 0.05740834400057793\n",
      "Epoch 8238/20000 Training Loss: 0.05257116258144379\n",
      "Epoch 8239/20000 Training Loss: 0.050723716616630554\n",
      "Epoch 8240/20000 Training Loss: 0.06748133897781372\n",
      "Epoch 8240/20000 Validation Loss: 0.06002634018659592\n",
      "Epoch 8241/20000 Training Loss: 0.06699821352958679\n",
      "Epoch 8242/20000 Training Loss: 0.04442375898361206\n",
      "Epoch 8243/20000 Training Loss: 0.05710507556796074\n",
      "Epoch 8244/20000 Training Loss: 0.04558578133583069\n",
      "Epoch 8245/20000 Training Loss: 0.047507256269454956\n",
      "Epoch 8246/20000 Training Loss: 0.055700067430734634\n",
      "Epoch 8247/20000 Training Loss: 0.059268344193696976\n",
      "Epoch 8248/20000 Training Loss: 0.06915252655744553\n",
      "Epoch 8249/20000 Training Loss: 0.07648754119873047\n",
      "Epoch 8250/20000 Training Loss: 0.047036413103342056\n",
      "Epoch 8250/20000 Validation Loss: 0.05582140013575554\n",
      "Epoch 8251/20000 Training Loss: 0.055428262799978256\n",
      "Epoch 8252/20000 Training Loss: 0.050420794636011124\n",
      "Epoch 8253/20000 Training Loss: 0.0564960278570652\n",
      "Epoch 8254/20000 Training Loss: 0.06755463033914566\n",
      "Epoch 8255/20000 Training Loss: 0.039525870233774185\n",
      "Epoch 8256/20000 Training Loss: 0.05032701417803764\n",
      "Epoch 8257/20000 Training Loss: 0.06678837537765503\n",
      "Epoch 8258/20000 Training Loss: 0.054380644112825394\n",
      "Epoch 8259/20000 Training Loss: 0.056703101843595505\n",
      "Epoch 8260/20000 Training Loss: 0.05689983442425728\n",
      "Epoch 8260/20000 Validation Loss: 0.04358494654297829\n",
      "Epoch 8261/20000 Training Loss: 0.05265949293971062\n",
      "Epoch 8262/20000 Training Loss: 0.05221673846244812\n",
      "Epoch 8263/20000 Training Loss: 0.06704375892877579\n",
      "Epoch 8264/20000 Training Loss: 0.05085905268788338\n",
      "Epoch 8265/20000 Training Loss: 0.059017639607191086\n",
      "Epoch 8266/20000 Training Loss: 0.07869306951761246\n",
      "Epoch 8267/20000 Training Loss: 0.0803852453827858\n",
      "Epoch 8268/20000 Training Loss: 0.04474101588129997\n",
      "Epoch 8269/20000 Training Loss: 0.05295741558074951\n",
      "Epoch 8270/20000 Training Loss: 0.05303804948925972\n",
      "Epoch 8270/20000 Validation Loss: 0.051161155104637146\n",
      "Epoch 8271/20000 Training Loss: 0.049667444080114365\n",
      "Epoch 8272/20000 Training Loss: 0.05341219902038574\n",
      "Epoch 8273/20000 Training Loss: 0.044732049107551575\n",
      "Epoch 8274/20000 Training Loss: 0.04943031445145607\n",
      "Epoch 8275/20000 Training Loss: 0.05397218465805054\n",
      "Epoch 8276/20000 Training Loss: 0.05524411424994469\n",
      "Epoch 8277/20000 Training Loss: 0.07242804020643234\n",
      "Epoch 8278/20000 Training Loss: 0.043361861258745193\n",
      "Epoch 8279/20000 Training Loss: 0.05772259831428528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8280/20000 Training Loss: 0.03596145287156105\n",
      "Epoch 8280/20000 Validation Loss: 0.047375939786434174\n",
      "Epoch 8281/20000 Training Loss: 0.04434672370553017\n",
      "Epoch 8282/20000 Training Loss: 0.05247358977794647\n",
      "Epoch 8283/20000 Training Loss: 0.05397631600499153\n",
      "Epoch 8284/20000 Training Loss: 0.059977516531944275\n",
      "Epoch 8285/20000 Training Loss: 0.05442604050040245\n",
      "Epoch 8286/20000 Training Loss: 0.04492787644267082\n",
      "Epoch 8287/20000 Training Loss: 0.06053103506565094\n",
      "Epoch 8288/20000 Training Loss: 0.04686780646443367\n",
      "Epoch 8289/20000 Training Loss: 0.05908669903874397\n",
      "Epoch 8290/20000 Training Loss: 0.04222724959254265\n",
      "Epoch 8290/20000 Validation Loss: 0.0571170337498188\n",
      "Epoch 8291/20000 Training Loss: 0.0671948716044426\n",
      "Epoch 8292/20000 Training Loss: 0.05929878354072571\n",
      "Epoch 8293/20000 Training Loss: 0.08078699558973312\n",
      "Epoch 8294/20000 Training Loss: 0.06247185543179512\n",
      "Epoch 8295/20000 Training Loss: 0.08421400934457779\n",
      "Epoch 8296/20000 Training Loss: 0.07483939081430435\n",
      "Epoch 8297/20000 Training Loss: 0.06853080540895462\n",
      "Epoch 8298/20000 Training Loss: 0.0692385733127594\n",
      "Epoch 8299/20000 Training Loss: 0.05915878340601921\n",
      "Epoch 8300/20000 Training Loss: 0.04437662288546562\n",
      "Epoch 8300/20000 Validation Loss: 0.05449093133211136\n",
      "Epoch 8301/20000 Training Loss: 0.05744602903723717\n",
      "Epoch 8302/20000 Training Loss: 0.056018516421318054\n",
      "Epoch 8303/20000 Training Loss: 0.044106606394052505\n",
      "Epoch 8304/20000 Training Loss: 0.046785492449998856\n",
      "Epoch 8305/20000 Training Loss: 0.036094143986701965\n",
      "Epoch 8306/20000 Training Loss: 0.05062540993094444\n",
      "Epoch 8307/20000 Training Loss: 0.04474137723445892\n",
      "Epoch 8308/20000 Training Loss: 0.05156896635890007\n",
      "Epoch 8309/20000 Training Loss: 0.05034451186656952\n",
      "Epoch 8310/20000 Training Loss: 0.049959760159254074\n",
      "Epoch 8310/20000 Validation Loss: 0.052513010799884796\n",
      "Epoch 8311/20000 Training Loss: 0.06189487874507904\n",
      "Epoch 8312/20000 Training Loss: 0.05192013084888458\n",
      "Epoch 8313/20000 Training Loss: 0.05497689172625542\n",
      "Epoch 8314/20000 Training Loss: 0.06299999356269836\n",
      "Epoch 8315/20000 Training Loss: 0.05460783839225769\n",
      "Epoch 8316/20000 Training Loss: 0.042512524873018265\n",
      "Epoch 8317/20000 Training Loss: 0.07997075468301773\n",
      "Epoch 8318/20000 Training Loss: 0.047203611582517624\n",
      "Epoch 8319/20000 Training Loss: 0.0670185312628746\n",
      "Epoch 8320/20000 Training Loss: 0.04959246143698692\n",
      "Epoch 8320/20000 Validation Loss: 0.05395997315645218\n",
      "Epoch 8321/20000 Training Loss: 0.05741995573043823\n",
      "Epoch 8322/20000 Training Loss: 0.052406370639801025\n",
      "Epoch 8323/20000 Training Loss: 0.04755508899688721\n",
      "Epoch 8324/20000 Training Loss: 0.049790505319833755\n",
      "Epoch 8325/20000 Training Loss: 0.060980457812547684\n",
      "Epoch 8326/20000 Training Loss: 0.04926634952425957\n",
      "Epoch 8327/20000 Training Loss: 0.044197678565979004\n",
      "Epoch 8328/20000 Training Loss: 0.04986356198787689\n",
      "Epoch 8329/20000 Training Loss: 0.046090107411146164\n",
      "Epoch 8330/20000 Training Loss: 0.07154098153114319\n",
      "Epoch 8330/20000 Validation Loss: 0.07884842157363892\n",
      "Epoch 8331/20000 Training Loss: 0.04615415260195732\n",
      "Epoch 8332/20000 Training Loss: 0.05569637939333916\n",
      "Epoch 8333/20000 Training Loss: 0.04878304526209831\n",
      "Epoch 8334/20000 Training Loss: 0.05216751620173454\n",
      "Epoch 8335/20000 Training Loss: 0.04684892296791077\n",
      "Epoch 8336/20000 Training Loss: 0.04266461357474327\n",
      "Epoch 8337/20000 Training Loss: 0.06698747724294662\n",
      "Epoch 8338/20000 Training Loss: 0.04266640543937683\n",
      "Epoch 8339/20000 Training Loss: 0.06978224962949753\n",
      "Epoch 8340/20000 Training Loss: 0.06438131630420685\n",
      "Epoch 8340/20000 Validation Loss: 0.054561130702495575\n",
      "Epoch 8341/20000 Training Loss: 0.060946788638830185\n",
      "Epoch 8342/20000 Training Loss: 0.05700727179646492\n",
      "Epoch 8343/20000 Training Loss: 0.07472344487905502\n",
      "Epoch 8344/20000 Training Loss: 0.04283716902136803\n",
      "Epoch 8345/20000 Training Loss: 0.043266866356134415\n",
      "Epoch 8346/20000 Training Loss: 0.06039248779416084\n",
      "Epoch 8347/20000 Training Loss: 0.07022582739591599\n",
      "Epoch 8348/20000 Training Loss: 0.048708055168390274\n",
      "Epoch 8349/20000 Training Loss: 0.04327589273452759\n",
      "Epoch 8350/20000 Training Loss: 0.045395608991384506\n",
      "Epoch 8350/20000 Validation Loss: 0.048715997487306595\n",
      "Epoch 8351/20000 Training Loss: 0.05417412146925926\n",
      "Epoch 8352/20000 Training Loss: 0.06832480430603027\n",
      "Epoch 8353/20000 Training Loss: 0.043424177914857864\n",
      "Epoch 8354/20000 Training Loss: 0.0666680708527565\n",
      "Epoch 8355/20000 Training Loss: 0.06752515584230423\n",
      "Epoch 8356/20000 Training Loss: 0.06309717148542404\n",
      "Epoch 8357/20000 Training Loss: 0.044757161289453506\n",
      "Epoch 8358/20000 Training Loss: 0.06783182173967361\n",
      "Epoch 8359/20000 Training Loss: 0.05232563614845276\n",
      "Epoch 8360/20000 Training Loss: 0.05212600156664848\n",
      "Epoch 8360/20000 Validation Loss: 0.038130126893520355\n",
      "Epoch 8361/20000 Training Loss: 0.05741769075393677\n",
      "Epoch 8362/20000 Training Loss: 0.06664609163999557\n",
      "Epoch 8363/20000 Training Loss: 0.06974495202302933\n",
      "Epoch 8364/20000 Training Loss: 0.04860838130116463\n",
      "Epoch 8365/20000 Training Loss: 0.06279642134904861\n",
      "Epoch 8366/20000 Training Loss: 0.05675725266337395\n",
      "Epoch 8367/20000 Training Loss: 0.04305919632315636\n",
      "Epoch 8368/20000 Training Loss: 0.055584728717803955\n",
      "Epoch 8369/20000 Training Loss: 0.05085821449756622\n",
      "Epoch 8370/20000 Training Loss: 0.06124049797654152\n",
      "Epoch 8370/20000 Validation Loss: 0.07607949525117874\n",
      "Epoch 8371/20000 Training Loss: 0.06422483921051025\n",
      "Epoch 8372/20000 Training Loss: 0.06510388851165771\n",
      "Epoch 8373/20000 Training Loss: 0.05650394782423973\n",
      "Epoch 8374/20000 Training Loss: 0.05785001441836357\n",
      "Epoch 8375/20000 Training Loss: 0.05284201726317406\n",
      "Epoch 8376/20000 Training Loss: 0.049247223883867264\n",
      "Epoch 8377/20000 Training Loss: 0.06971820443868637\n",
      "Epoch 8378/20000 Training Loss: 0.04990420863032341\n",
      "Epoch 8379/20000 Training Loss: 0.05824669077992439\n",
      "Epoch 8380/20000 Training Loss: 0.06507156044244766\n",
      "Epoch 8380/20000 Validation Loss: 0.03278808668255806\n",
      "Epoch 8381/20000 Training Loss: 0.05539079010486603\n",
      "Epoch 8382/20000 Training Loss: 0.06416821479797363\n",
      "Epoch 8383/20000 Training Loss: 0.07176721841096878\n",
      "Epoch 8384/20000 Training Loss: 0.06531542539596558\n",
      "Epoch 8385/20000 Training Loss: 0.050350043922662735\n",
      "Epoch 8386/20000 Training Loss: 0.08427149802446365\n",
      "Epoch 8387/20000 Training Loss: 0.05481048300862312\n",
      "Epoch 8388/20000 Training Loss: 0.0446067713201046\n",
      "Epoch 8389/20000 Training Loss: 0.0777599886059761\n",
      "Epoch 8390/20000 Training Loss: 0.05336695536971092\n",
      "Epoch 8390/20000 Validation Loss: 0.0577516034245491\n",
      "Epoch 8391/20000 Training Loss: 0.06119350716471672\n",
      "Epoch 8392/20000 Training Loss: 0.06371002644300461\n",
      "Epoch 8393/20000 Training Loss: 0.07150550931692123\n",
      "Epoch 8394/20000 Training Loss: 0.05609309300780296\n",
      "Epoch 8395/20000 Training Loss: 0.0719892829656601\n",
      "Epoch 8396/20000 Training Loss: 0.03911832347512245\n",
      "Epoch 8397/20000 Training Loss: 0.048215895891189575\n",
      "Epoch 8398/20000 Training Loss: 0.049395423382520676\n",
      "Epoch 8399/20000 Training Loss: 0.06636685132980347\n",
      "Epoch 8400/20000 Training Loss: 0.0648108422756195\n",
      "Epoch 8400/20000 Validation Loss: 0.05902223661541939\n",
      "Epoch 8401/20000 Training Loss: 0.06324896961450577\n",
      "Epoch 8402/20000 Training Loss: 0.05098024010658264\n",
      "Epoch 8403/20000 Training Loss: 0.06560003012418747\n",
      "Epoch 8404/20000 Training Loss: 0.052521150559186935\n",
      "Epoch 8405/20000 Training Loss: 0.05418868362903595\n",
      "Epoch 8406/20000 Training Loss: 0.056814391165971756\n",
      "Epoch 8407/20000 Training Loss: 0.042862359434366226\n",
      "Epoch 8408/20000 Training Loss: 0.047092076390981674\n",
      "Epoch 8409/20000 Training Loss: 0.05001433193683624\n",
      "Epoch 8410/20000 Training Loss: 0.04379715025424957\n",
      "Epoch 8410/20000 Validation Loss: 0.07947061955928802\n",
      "Epoch 8411/20000 Training Loss: 0.04675193503499031\n",
      "Epoch 8412/20000 Training Loss: 0.038621481508016586\n",
      "Epoch 8413/20000 Training Loss: 0.039742063730955124\n",
      "Epoch 8414/20000 Training Loss: 0.052745502442121506\n",
      "Epoch 8415/20000 Training Loss: 0.05905776098370552\n",
      "Epoch 8416/20000 Training Loss: 0.051839277148246765\n",
      "Epoch 8417/20000 Training Loss: 0.05237453058362007\n",
      "Epoch 8418/20000 Training Loss: 0.062250956892967224\n",
      "Epoch 8419/20000 Training Loss: 0.08657649904489517\n",
      "Epoch 8420/20000 Training Loss: 0.0503620021045208\n",
      "Epoch 8420/20000 Validation Loss: 0.05904381722211838\n",
      "Epoch 8421/20000 Training Loss: 0.0570644848048687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8422/20000 Training Loss: 0.060242462903261185\n",
      "Epoch 8423/20000 Training Loss: 0.04865546151995659\n",
      "Epoch 8424/20000 Training Loss: 0.05528150498867035\n",
      "Epoch 8425/20000 Training Loss: 0.05133609473705292\n",
      "Epoch 8426/20000 Training Loss: 0.07616927474737167\n",
      "Epoch 8427/20000 Training Loss: 0.061134349554777145\n",
      "Epoch 8428/20000 Training Loss: 0.051671940833330154\n",
      "Epoch 8429/20000 Training Loss: 0.047782499343156815\n",
      "Epoch 8430/20000 Training Loss: 0.03850099816918373\n",
      "Epoch 8430/20000 Validation Loss: 0.06405622512102127\n",
      "Epoch 8431/20000 Training Loss: 0.0400218740105629\n",
      "Epoch 8432/20000 Training Loss: 0.03781281039118767\n",
      "Epoch 8433/20000 Training Loss: 0.038604605942964554\n",
      "Epoch 8434/20000 Training Loss: 0.06618788093328476\n",
      "Epoch 8435/20000 Training Loss: 0.053095389157533646\n",
      "Epoch 8436/20000 Training Loss: 0.05337647721171379\n",
      "Epoch 8437/20000 Training Loss: 0.0604390911757946\n",
      "Epoch 8438/20000 Training Loss: 0.037156470119953156\n",
      "Epoch 8439/20000 Training Loss: 0.05328252911567688\n",
      "Epoch 8440/20000 Training Loss: 0.04451403394341469\n",
      "Epoch 8440/20000 Validation Loss: 0.04796227067708969\n",
      "Epoch 8441/20000 Training Loss: 0.07275804877281189\n",
      "Epoch 8442/20000 Training Loss: 0.04078805074095726\n",
      "Epoch 8443/20000 Training Loss: 0.06369457393884659\n",
      "Epoch 8444/20000 Training Loss: 0.060645341873168945\n",
      "Epoch 8445/20000 Training Loss: 0.04642250016331673\n",
      "Epoch 8446/20000 Training Loss: 0.03917764499783516\n",
      "Epoch 8447/20000 Training Loss: 0.06874266266822815\n",
      "Epoch 8448/20000 Training Loss: 0.05157027766108513\n",
      "Epoch 8449/20000 Training Loss: 0.04615708813071251\n",
      "Epoch 8450/20000 Training Loss: 0.054884206503629684\n",
      "Epoch 8450/20000 Validation Loss: 0.056848086416721344\n",
      "Epoch 8451/20000 Training Loss: 0.06006031855940819\n",
      "Epoch 8452/20000 Training Loss: 0.06712772697210312\n",
      "Epoch 8453/20000 Training Loss: 0.05495459958910942\n",
      "Epoch 8454/20000 Training Loss: 0.05276291072368622\n",
      "Epoch 8455/20000 Training Loss: 0.07881396263837814\n",
      "Epoch 8456/20000 Training Loss: 0.06247532740235329\n",
      "Epoch 8457/20000 Training Loss: 0.07666078209877014\n",
      "Epoch 8458/20000 Training Loss: 0.0612618625164032\n",
      "Epoch 8459/20000 Training Loss: 0.058713193982839584\n",
      "Epoch 8460/20000 Training Loss: 0.050684794783592224\n",
      "Epoch 8460/20000 Validation Loss: 0.06746432930231094\n",
      "Epoch 8461/20000 Training Loss: 0.05103218927979469\n",
      "Epoch 8462/20000 Training Loss: 0.03834935650229454\n",
      "Epoch 8463/20000 Training Loss: 0.05538468435406685\n",
      "Epoch 8464/20000 Training Loss: 0.06194516643881798\n",
      "Epoch 8465/20000 Training Loss: 0.0439099483191967\n",
      "Epoch 8466/20000 Training Loss: 0.04968705400824547\n",
      "Epoch 8467/20000 Training Loss: 0.06998846679925919\n",
      "Epoch 8468/20000 Training Loss: 0.05581095442175865\n",
      "Epoch 8469/20000 Training Loss: 0.049272552132606506\n",
      "Epoch 8470/20000 Training Loss: 0.06314592808485031\n",
      "Epoch 8470/20000 Validation Loss: 0.07237550616264343\n",
      "Epoch 8471/20000 Training Loss: 0.05686177685856819\n",
      "Epoch 8472/20000 Training Loss: 0.0582302063703537\n",
      "Epoch 8473/20000 Training Loss: 0.06046319007873535\n",
      "Epoch 8474/20000 Training Loss: 0.04457494243979454\n",
      "Epoch 8475/20000 Training Loss: 0.048394013196229935\n",
      "Epoch 8476/20000 Training Loss: 0.049791764467954636\n",
      "Epoch 8477/20000 Training Loss: 0.034039389342069626\n",
      "Epoch 8478/20000 Training Loss: 0.06538788229227066\n",
      "Epoch 8479/20000 Training Loss: 0.0567789264023304\n",
      "Epoch 8480/20000 Training Loss: 0.07166846096515656\n",
      "Epoch 8480/20000 Validation Loss: 0.06837830692529678\n",
      "Epoch 8481/20000 Training Loss: 0.05953666567802429\n",
      "Epoch 8482/20000 Training Loss: 0.04178038612008095\n",
      "Epoch 8483/20000 Training Loss: 0.03881402686238289\n",
      "Epoch 8484/20000 Training Loss: 0.050118982791900635\n",
      "Epoch 8485/20000 Training Loss: 0.06124250218272209\n",
      "Epoch 8486/20000 Training Loss: 0.050298288464546204\n",
      "Epoch 8487/20000 Training Loss: 0.04113326221704483\n",
      "Epoch 8488/20000 Training Loss: 0.058937203139066696\n",
      "Epoch 8489/20000 Training Loss: 0.06813626736402512\n",
      "Epoch 8490/20000 Training Loss: 0.07260248064994812\n",
      "Epoch 8490/20000 Validation Loss: 0.0658351331949234\n",
      "Epoch 8491/20000 Training Loss: 0.06590328365564346\n",
      "Epoch 8492/20000 Training Loss: 0.06366977840662003\n",
      "Epoch 8493/20000 Training Loss: 0.06311146914958954\n",
      "Epoch 8494/20000 Training Loss: 0.05310064181685448\n",
      "Epoch 8495/20000 Training Loss: 0.06744875758886337\n",
      "Epoch 8496/20000 Training Loss: 0.05058470740914345\n",
      "Epoch 8497/20000 Training Loss: 0.05053980275988579\n",
      "Epoch 8498/20000 Training Loss: 0.06485375761985779\n",
      "Epoch 8499/20000 Training Loss: 0.05262346193194389\n",
      "Epoch 8500/20000 Training Loss: 0.05212016776204109\n",
      "Epoch 8500/20000 Validation Loss: 0.06211421638727188\n",
      "Epoch 8501/20000 Training Loss: 0.05632977560162544\n",
      "Epoch 8502/20000 Training Loss: 0.06595692783594131\n",
      "Epoch 8503/20000 Training Loss: 0.06452483683824539\n",
      "Epoch 8504/20000 Training Loss: 0.07172834873199463\n",
      "Epoch 8505/20000 Training Loss: 0.052406828850507736\n",
      "Epoch 8506/20000 Training Loss: 0.08000478148460388\n",
      "Epoch 8507/20000 Training Loss: 0.04336069896817207\n",
      "Epoch 8508/20000 Training Loss: 0.05758436396718025\n",
      "Epoch 8509/20000 Training Loss: 0.04138581082224846\n",
      "Epoch 8510/20000 Training Loss: 0.06478644162416458\n",
      "Epoch 8510/20000 Validation Loss: 0.05670837312936783\n",
      "Epoch 8511/20000 Training Loss: 0.05455850437283516\n",
      "Epoch 8512/20000 Training Loss: 0.05382752791047096\n",
      "Epoch 8513/20000 Training Loss: 0.06509865075349808\n",
      "Epoch 8514/20000 Training Loss: 0.050207313150167465\n",
      "Epoch 8515/20000 Training Loss: 0.06185103580355644\n",
      "Epoch 8516/20000 Training Loss: 0.03757147490978241\n",
      "Epoch 8517/20000 Training Loss: 0.06082288548350334\n",
      "Epoch 8518/20000 Training Loss: 0.061123114079236984\n",
      "Epoch 8519/20000 Training Loss: 0.07398682087659836\n",
      "Epoch 8520/20000 Training Loss: 0.04408012330532074\n",
      "Epoch 8520/20000 Validation Loss: 0.0851123258471489\n",
      "Epoch 8521/20000 Training Loss: 0.05447648838162422\n",
      "Epoch 8522/20000 Training Loss: 0.053747739642858505\n",
      "Epoch 8523/20000 Training Loss: 0.05423428490757942\n",
      "Epoch 8524/20000 Training Loss: 0.0657113790512085\n",
      "Epoch 8525/20000 Training Loss: 0.050854042172431946\n",
      "Epoch 8526/20000 Training Loss: 0.06034437194466591\n",
      "Epoch 8527/20000 Training Loss: 0.0527731217443943\n",
      "Epoch 8528/20000 Training Loss: 0.0573643259704113\n",
      "Epoch 8529/20000 Training Loss: 0.051032960414886475\n",
      "Epoch 8530/20000 Training Loss: 0.04372905194759369\n",
      "Epoch 8530/20000 Validation Loss: 0.045788612216711044\n",
      "Epoch 8531/20000 Training Loss: 0.05590824782848358\n",
      "Epoch 8532/20000 Training Loss: 0.06800278276205063\n",
      "Epoch 8533/20000 Training Loss: 0.04718160629272461\n",
      "Epoch 8534/20000 Training Loss: 0.06398770213127136\n",
      "Epoch 8535/20000 Training Loss: 0.0614798478782177\n",
      "Epoch 8536/20000 Training Loss: 0.06438932567834854\n",
      "Epoch 8537/20000 Training Loss: 0.06106353923678398\n",
      "Epoch 8538/20000 Training Loss: 0.055840980261564255\n",
      "Epoch 8539/20000 Training Loss: 0.0656135156750679\n",
      "Epoch 8540/20000 Training Loss: 0.05184561014175415\n",
      "Epoch 8540/20000 Validation Loss: 0.053296733647584915\n",
      "Epoch 8541/20000 Training Loss: 0.05323975160717964\n",
      "Epoch 8542/20000 Training Loss: 0.04683505371212959\n",
      "Epoch 8543/20000 Training Loss: 0.05451981723308563\n",
      "Epoch 8544/20000 Training Loss: 0.06125715747475624\n",
      "Epoch 8545/20000 Training Loss: 0.046636153012514114\n",
      "Epoch 8546/20000 Training Loss: 0.04937206581234932\n",
      "Epoch 8547/20000 Training Loss: 0.06672024726867676\n",
      "Epoch 8548/20000 Training Loss: 0.05395488440990448\n",
      "Epoch 8549/20000 Training Loss: 0.05605242773890495\n",
      "Epoch 8550/20000 Training Loss: 0.058400679379701614\n",
      "Epoch 8550/20000 Validation Loss: 0.05995149910449982\n",
      "Epoch 8551/20000 Training Loss: 0.05214175209403038\n",
      "Epoch 8552/20000 Training Loss: 0.05651991441845894\n",
      "Epoch 8553/20000 Training Loss: 0.053754329681396484\n",
      "Epoch 8554/20000 Training Loss: 0.05619627609848976\n",
      "Epoch 8555/20000 Training Loss: 0.05730016157031059\n",
      "Epoch 8556/20000 Training Loss: 0.04861064255237579\n",
      "Epoch 8557/20000 Training Loss: 0.0567316897213459\n",
      "Epoch 8558/20000 Training Loss: 0.06636060029268265\n",
      "Epoch 8559/20000 Training Loss: 0.0468623973429203\n",
      "Epoch 8560/20000 Training Loss: 0.05799970030784607\n",
      "Epoch 8560/20000 Validation Loss: 0.055890195071697235\n",
      "Epoch 8561/20000 Training Loss: 0.030374163761734962\n",
      "Epoch 8562/20000 Training Loss: 0.06479168683290482\n",
      "Epoch 8563/20000 Training Loss: 0.05362237989902496\n",
      "Epoch 8564/20000 Training Loss: 0.06046409532427788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8565/20000 Training Loss: 0.04442288354039192\n",
      "Epoch 8566/20000 Training Loss: 0.0655461922287941\n",
      "Epoch 8567/20000 Training Loss: 0.06509435176849365\n",
      "Epoch 8568/20000 Training Loss: 0.04977911338210106\n",
      "Epoch 8569/20000 Training Loss: 0.05145617201924324\n",
      "Epoch 8570/20000 Training Loss: 0.0694630965590477\n",
      "Epoch 8570/20000 Validation Loss: 0.06485806405544281\n",
      "Epoch 8571/20000 Training Loss: 0.046304117888212204\n",
      "Epoch 8572/20000 Training Loss: 0.04780931770801544\n",
      "Epoch 8573/20000 Training Loss: 0.05633227527141571\n",
      "Epoch 8574/20000 Training Loss: 0.03097965568304062\n",
      "Epoch 8575/20000 Training Loss: 0.06836644560098648\n",
      "Epoch 8576/20000 Training Loss: 0.05374542996287346\n",
      "Epoch 8577/20000 Training Loss: 0.053862523287534714\n",
      "Epoch 8578/20000 Training Loss: 0.06395754218101501\n",
      "Epoch 8579/20000 Training Loss: 0.046733081340789795\n",
      "Epoch 8580/20000 Training Loss: 0.0515640527009964\n",
      "Epoch 8580/20000 Validation Loss: 0.04886758327484131\n",
      "Epoch 8581/20000 Training Loss: 0.05628061294555664\n",
      "Epoch 8582/20000 Training Loss: 0.05498552322387695\n",
      "Epoch 8583/20000 Training Loss: 0.07011953741312027\n",
      "Epoch 8584/20000 Training Loss: 0.06565513461828232\n",
      "Epoch 8585/20000 Training Loss: 0.0682375505566597\n",
      "Epoch 8586/20000 Training Loss: 0.06481480598449707\n",
      "Epoch 8587/20000 Training Loss: 0.05547695234417915\n",
      "Epoch 8588/20000 Training Loss: 0.04481878876686096\n",
      "Epoch 8589/20000 Training Loss: 0.060947250574827194\n",
      "Epoch 8590/20000 Training Loss: 0.059939730912446976\n",
      "Epoch 8590/20000 Validation Loss: 0.04984741657972336\n",
      "Epoch 8591/20000 Training Loss: 0.045891959220170975\n",
      "Epoch 8592/20000 Training Loss: 0.08053786307573318\n",
      "Epoch 8593/20000 Training Loss: 0.043042223900556564\n",
      "Epoch 8594/20000 Training Loss: 0.062255170196294785\n",
      "Epoch 8595/20000 Training Loss: 0.0682920590043068\n",
      "Epoch 8596/20000 Training Loss: 0.04598505422472954\n",
      "Epoch 8597/20000 Training Loss: 0.041768431663513184\n",
      "Epoch 8598/20000 Training Loss: 0.0692286565899849\n",
      "Epoch 8599/20000 Training Loss: 0.06031903624534607\n",
      "Epoch 8600/20000 Training Loss: 0.0710861012339592\n",
      "Epoch 8600/20000 Validation Loss: 0.049963898956775665\n",
      "Epoch 8601/20000 Training Loss: 0.04396234080195427\n",
      "Epoch 8602/20000 Training Loss: 0.05127677693963051\n",
      "Epoch 8603/20000 Training Loss: 0.0821821391582489\n",
      "Epoch 8604/20000 Training Loss: 0.05143032968044281\n",
      "Epoch 8605/20000 Training Loss: 0.07528094202280045\n",
      "Epoch 8606/20000 Training Loss: 0.04518721625208855\n",
      "Epoch 8607/20000 Training Loss: 0.04954482614994049\n",
      "Epoch 8608/20000 Training Loss: 0.07108578830957413\n",
      "Epoch 8609/20000 Training Loss: 0.06056293845176697\n",
      "Epoch 8610/20000 Training Loss: 0.06399042159318924\n",
      "Epoch 8610/20000 Validation Loss: 0.06660009920597076\n",
      "Epoch 8611/20000 Training Loss: 0.05685127153992653\n",
      "Epoch 8612/20000 Training Loss: 0.05140739306807518\n",
      "Epoch 8613/20000 Training Loss: 0.06517621874809265\n",
      "Epoch 8614/20000 Training Loss: 0.056233588606119156\n",
      "Epoch 8615/20000 Training Loss: 0.06491947174072266\n",
      "Epoch 8616/20000 Training Loss: 0.050928205251693726\n",
      "Epoch 8617/20000 Training Loss: 0.060202956199645996\n",
      "Epoch 8618/20000 Training Loss: 0.059359923005104065\n",
      "Epoch 8619/20000 Training Loss: 0.06018521264195442\n",
      "Epoch 8620/20000 Training Loss: 0.06394362449645996\n",
      "Epoch 8620/20000 Validation Loss: 0.05487462133169174\n",
      "Epoch 8621/20000 Training Loss: 0.05766819044947624\n",
      "Epoch 8622/20000 Training Loss: 0.05207310616970062\n",
      "Epoch 8623/20000 Training Loss: 0.040212009102106094\n",
      "Epoch 8624/20000 Training Loss: 0.04840865358710289\n",
      "Epoch 8625/20000 Training Loss: 0.056672144681215286\n",
      "Epoch 8626/20000 Training Loss: 0.058625612407922745\n",
      "Epoch 8627/20000 Training Loss: 0.04784028232097626\n",
      "Epoch 8628/20000 Training Loss: 0.06090598180890083\n",
      "Epoch 8629/20000 Training Loss: 0.06255758553743362\n",
      "Epoch 8630/20000 Training Loss: 0.058563005179166794\n",
      "Epoch 8630/20000 Validation Loss: 0.06217683106660843\n",
      "Epoch 8631/20000 Training Loss: 0.0610806830227375\n",
      "Epoch 8632/20000 Training Loss: 0.06885989755392075\n",
      "Epoch 8633/20000 Training Loss: 0.060888826847076416\n",
      "Epoch 8634/20000 Training Loss: 0.06826447695493698\n",
      "Epoch 8635/20000 Training Loss: 0.039253488183021545\n",
      "Epoch 8636/20000 Training Loss: 0.06382402032613754\n",
      "Epoch 8637/20000 Training Loss: 0.04677056148648262\n",
      "Epoch 8638/20000 Training Loss: 0.055403098464012146\n",
      "Epoch 8639/20000 Training Loss: 0.063697449862957\n",
      "Epoch 8640/20000 Training Loss: 0.05071881785988808\n",
      "Epoch 8640/20000 Validation Loss: 0.05359037220478058\n",
      "Epoch 8641/20000 Training Loss: 0.04731873795390129\n",
      "Epoch 8642/20000 Training Loss: 0.05807110667228699\n",
      "Epoch 8643/20000 Training Loss: 0.05899103358387947\n",
      "Epoch 8644/20000 Training Loss: 0.047070879489183426\n",
      "Epoch 8645/20000 Training Loss: 0.05551596358418465\n",
      "Epoch 8646/20000 Training Loss: 0.06284783035516739\n",
      "Epoch 8647/20000 Training Loss: 0.06670361757278442\n",
      "Epoch 8648/20000 Training Loss: 0.043703142553567886\n",
      "Epoch 8649/20000 Training Loss: 0.05828813835978508\n",
      "Epoch 8650/20000 Training Loss: 0.06159285083413124\n",
      "Epoch 8650/20000 Validation Loss: 0.03279317542910576\n",
      "Epoch 8651/20000 Training Loss: 0.05593249574303627\n",
      "Epoch 8652/20000 Training Loss: 0.055601462721824646\n",
      "Epoch 8653/20000 Training Loss: 0.060825228691101074\n",
      "Epoch 8654/20000 Training Loss: 0.054160743951797485\n",
      "Epoch 8655/20000 Training Loss: 0.05014774203300476\n",
      "Epoch 8656/20000 Training Loss: 0.052306145429611206\n",
      "Epoch 8657/20000 Training Loss: 0.06080305948853493\n",
      "Epoch 8658/20000 Training Loss: 0.05202638730406761\n",
      "Epoch 8659/20000 Training Loss: 0.04823264107108116\n",
      "Epoch 8660/20000 Training Loss: 0.045289184898138046\n",
      "Epoch 8660/20000 Validation Loss: 0.05393696203827858\n",
      "Epoch 8661/20000 Training Loss: 0.0560063011944294\n",
      "Epoch 8662/20000 Training Loss: 0.05430750176310539\n",
      "Epoch 8663/20000 Training Loss: 0.06640691310167313\n",
      "Epoch 8664/20000 Training Loss: 0.048373375087976456\n",
      "Epoch 8665/20000 Training Loss: 0.044619183987379074\n",
      "Epoch 8666/20000 Training Loss: 0.060417186468839645\n",
      "Epoch 8667/20000 Training Loss: 0.0676448717713356\n",
      "Epoch 8668/20000 Training Loss: 0.0642857626080513\n",
      "Epoch 8669/20000 Training Loss: 0.05966479703783989\n",
      "Epoch 8670/20000 Training Loss: 0.061876412481069565\n",
      "Epoch 8670/20000 Validation Loss: 0.052851639688014984\n",
      "Epoch 8671/20000 Training Loss: 0.05871066451072693\n",
      "Epoch 8672/20000 Training Loss: 0.06027789041399956\n",
      "Epoch 8673/20000 Training Loss: 0.059473443776369095\n",
      "Epoch 8674/20000 Training Loss: 0.06512214988470078\n",
      "Epoch 8675/20000 Training Loss: 0.07698297500610352\n",
      "Epoch 8676/20000 Training Loss: 0.061981379985809326\n",
      "Epoch 8677/20000 Training Loss: 0.04840852692723274\n",
      "Epoch 8678/20000 Training Loss: 0.05570545792579651\n",
      "Epoch 8679/20000 Training Loss: 0.04347245395183563\n",
      "Epoch 8680/20000 Training Loss: 0.05008303001523018\n",
      "Epoch 8680/20000 Validation Loss: 0.0427497923374176\n",
      "Epoch 8681/20000 Training Loss: 0.06793245673179626\n",
      "Epoch 8682/20000 Training Loss: 0.04114537313580513\n",
      "Epoch 8683/20000 Training Loss: 0.07041894644498825\n",
      "Epoch 8684/20000 Training Loss: 0.05738437548279762\n",
      "Epoch 8685/20000 Training Loss: 0.07161151617765427\n",
      "Epoch 8686/20000 Training Loss: 0.04189816117286682\n",
      "Epoch 8687/20000 Training Loss: 0.057351112365722656\n",
      "Epoch 8688/20000 Training Loss: 0.060222137719392776\n",
      "Epoch 8689/20000 Training Loss: 0.07176535576581955\n",
      "Epoch 8690/20000 Training Loss: 0.0507785864174366\n",
      "Epoch 8690/20000 Validation Loss: 0.06254427134990692\n",
      "Epoch 8691/20000 Training Loss: 0.04480794072151184\n",
      "Epoch 8692/20000 Training Loss: 0.05819869041442871\n",
      "Epoch 8693/20000 Training Loss: 0.051760781556367874\n",
      "Epoch 8694/20000 Training Loss: 0.05908626317977905\n",
      "Epoch 8695/20000 Training Loss: 0.052527591586112976\n",
      "Epoch 8696/20000 Training Loss: 0.08615854382514954\n",
      "Epoch 8697/20000 Training Loss: 0.05585089325904846\n",
      "Epoch 8698/20000 Training Loss: 0.05554860830307007\n",
      "Epoch 8699/20000 Training Loss: 0.04536372795701027\n",
      "Epoch 8700/20000 Training Loss: 0.055026039481163025\n",
      "Epoch 8700/20000 Validation Loss: 0.05193958431482315\n",
      "Epoch 8701/20000 Training Loss: 0.07117439061403275\n",
      "Epoch 8702/20000 Training Loss: 0.06769628822803497\n",
      "Epoch 8703/20000 Training Loss: 0.0488896369934082\n",
      "Epoch 8704/20000 Training Loss: 0.052111100405454636\n",
      "Epoch 8705/20000 Training Loss: 0.04748527333140373\n",
      "Epoch 8706/20000 Training Loss: 0.04022172465920448\n",
      "Epoch 8707/20000 Training Loss: 0.04500681161880493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8708/20000 Training Loss: 0.05344482138752937\n",
      "Epoch 8709/20000 Training Loss: 0.0630747452378273\n",
      "Epoch 8710/20000 Training Loss: 0.07172507792711258\n",
      "Epoch 8710/20000 Validation Loss: 0.04467364773154259\n",
      "Epoch 8711/20000 Training Loss: 0.04744858667254448\n",
      "Epoch 8712/20000 Training Loss: 0.05696730315685272\n",
      "Epoch 8713/20000 Training Loss: 0.07544972747564316\n",
      "Epoch 8714/20000 Training Loss: 0.06218663975596428\n",
      "Epoch 8715/20000 Training Loss: 0.06366130709648132\n",
      "Epoch 8716/20000 Training Loss: 0.060308653861284256\n",
      "Epoch 8717/20000 Training Loss: 0.04965106025338173\n",
      "Epoch 8718/20000 Training Loss: 0.0462518148124218\n",
      "Epoch 8719/20000 Training Loss: 0.04125591740012169\n",
      "Epoch 8720/20000 Training Loss: 0.05066089332103729\n",
      "Epoch 8720/20000 Validation Loss: 0.06422784179449081\n",
      "Epoch 8721/20000 Training Loss: 0.051219165325164795\n",
      "Epoch 8722/20000 Training Loss: 0.05551852658390999\n",
      "Epoch 8723/20000 Training Loss: 0.04864378646016121\n",
      "Epoch 8724/20000 Training Loss: 0.07237085700035095\n",
      "Epoch 8725/20000 Training Loss: 0.07177267223596573\n",
      "Epoch 8726/20000 Training Loss: 0.048296380788087845\n",
      "Epoch 8727/20000 Training Loss: 0.0504482239484787\n",
      "Epoch 8728/20000 Training Loss: 0.06057524308562279\n",
      "Epoch 8729/20000 Training Loss: 0.02906084805727005\n",
      "Epoch 8730/20000 Training Loss: 0.05245101824402809\n",
      "Epoch 8730/20000 Validation Loss: 0.05708049610257149\n",
      "Epoch 8731/20000 Training Loss: 0.05248410627245903\n",
      "Epoch 8732/20000 Training Loss: 0.062084734439849854\n",
      "Epoch 8733/20000 Training Loss: 0.049710214138031006\n",
      "Epoch 8734/20000 Training Loss: 0.056540172547101974\n",
      "Epoch 8735/20000 Training Loss: 0.05799717828631401\n",
      "Epoch 8736/20000 Training Loss: 0.04738050699234009\n",
      "Epoch 8737/20000 Training Loss: 0.05442873015999794\n",
      "Epoch 8738/20000 Training Loss: 0.059378400444984436\n",
      "Epoch 8739/20000 Training Loss: 0.05210704728960991\n",
      "Epoch 8740/20000 Training Loss: 0.0696536973118782\n",
      "Epoch 8740/20000 Validation Loss: 0.0567438006401062\n",
      "Epoch 8741/20000 Training Loss: 0.07102632522583008\n",
      "Epoch 8742/20000 Training Loss: 0.05683271586894989\n",
      "Epoch 8743/20000 Training Loss: 0.05137643590569496\n",
      "Epoch 8744/20000 Training Loss: 0.043689072132110596\n",
      "Epoch 8745/20000 Training Loss: 0.048537831753492355\n",
      "Epoch 8746/20000 Training Loss: 0.05207925662398338\n",
      "Epoch 8747/20000 Training Loss: 0.06326188892126083\n",
      "Epoch 8748/20000 Training Loss: 0.04545663669705391\n",
      "Epoch 8749/20000 Training Loss: 0.04684838280081749\n",
      "Epoch 8750/20000 Training Loss: 0.062248822301626205\n",
      "Epoch 8750/20000 Validation Loss: 0.05396854877471924\n",
      "Epoch 8751/20000 Training Loss: 0.054729435592889786\n",
      "Epoch 8752/20000 Training Loss: 0.053333580493927\n",
      "Epoch 8753/20000 Training Loss: 0.061859745532274246\n",
      "Epoch 8754/20000 Training Loss: 0.04332676902413368\n",
      "Epoch 8755/20000 Training Loss: 0.05092485249042511\n",
      "Epoch 8756/20000 Training Loss: 0.0561748743057251\n",
      "Epoch 8757/20000 Training Loss: 0.06808503717184067\n",
      "Epoch 8758/20000 Training Loss: 0.057808395475149155\n",
      "Epoch 8759/20000 Training Loss: 0.04291805997490883\n",
      "Epoch 8760/20000 Training Loss: 0.07063370198011398\n",
      "Epoch 8760/20000 Validation Loss: 0.054026953876018524\n",
      "Epoch 8761/20000 Training Loss: 0.04745296761393547\n",
      "Epoch 8762/20000 Training Loss: 0.054679106920957565\n",
      "Epoch 8763/20000 Training Loss: 0.05336512252688408\n",
      "Epoch 8764/20000 Training Loss: 0.0529799647629261\n",
      "Epoch 8765/20000 Training Loss: 0.05997288599610329\n",
      "Epoch 8766/20000 Training Loss: 0.05445224046707153\n",
      "Epoch 8767/20000 Training Loss: 0.045141588896512985\n",
      "Epoch 8768/20000 Training Loss: 0.05090877786278725\n",
      "Epoch 8769/20000 Training Loss: 0.05227034166455269\n",
      "Epoch 8770/20000 Training Loss: 0.04497295618057251\n",
      "Epoch 8770/20000 Validation Loss: 0.04618013650178909\n",
      "Epoch 8771/20000 Training Loss: 0.05688253045082092\n",
      "Epoch 8772/20000 Training Loss: 0.05521325394511223\n",
      "Epoch 8773/20000 Training Loss: 0.0523141510784626\n",
      "Epoch 8774/20000 Training Loss: 0.04789981245994568\n",
      "Epoch 8775/20000 Training Loss: 0.052733976393938065\n",
      "Epoch 8776/20000 Training Loss: 0.05908743664622307\n",
      "Epoch 8777/20000 Training Loss: 0.05701078101992607\n",
      "Epoch 8778/20000 Training Loss: 0.06345116347074509\n",
      "Epoch 8779/20000 Training Loss: 0.06109664961695671\n",
      "Epoch 8780/20000 Training Loss: 0.06934761255979538\n",
      "Epoch 8780/20000 Validation Loss: 0.053252413868904114\n",
      "Epoch 8781/20000 Training Loss: 0.05558038875460625\n",
      "Epoch 8782/20000 Training Loss: 0.06505098193883896\n",
      "Epoch 8783/20000 Training Loss: 0.04755901172757149\n",
      "Epoch 8784/20000 Training Loss: 0.06555473059415817\n",
      "Epoch 8785/20000 Training Loss: 0.05077749490737915\n",
      "Epoch 8786/20000 Training Loss: 0.048440005630254745\n",
      "Epoch 8787/20000 Training Loss: 0.06187063828110695\n",
      "Epoch 8788/20000 Training Loss: 0.05894261598587036\n",
      "Epoch 8789/20000 Training Loss: 0.05219894275069237\n",
      "Epoch 8790/20000 Training Loss: 0.06174234673380852\n",
      "Epoch 8790/20000 Validation Loss: 0.05004515498876572\n",
      "Epoch 8791/20000 Training Loss: 0.06596269458532333\n",
      "Epoch 8792/20000 Training Loss: 0.0709107369184494\n",
      "Epoch 8793/20000 Training Loss: 0.0572361946105957\n",
      "Epoch 8794/20000 Training Loss: 0.06535915285348892\n",
      "Epoch 8795/20000 Training Loss: 0.06686720252037048\n",
      "Epoch 8796/20000 Training Loss: 0.05169825628399849\n",
      "Epoch 8797/20000 Training Loss: 0.051750052720308304\n",
      "Epoch 8798/20000 Training Loss: 0.057091135531663895\n",
      "Epoch 8799/20000 Training Loss: 0.057455893605947495\n",
      "Epoch 8800/20000 Training Loss: 0.040591418743133545\n",
      "Epoch 8800/20000 Validation Loss: 0.044474463909864426\n",
      "Epoch 8801/20000 Training Loss: 0.06329669803380966\n",
      "Epoch 8802/20000 Training Loss: 0.06686992198228836\n",
      "Epoch 8803/20000 Training Loss: 0.05225922539830208\n",
      "Epoch 8804/20000 Training Loss: 0.05222156643867493\n",
      "Epoch 8805/20000 Training Loss: 0.06665220111608505\n",
      "Epoch 8806/20000 Training Loss: 0.07014445215463638\n",
      "Epoch 8807/20000 Training Loss: 0.0626104399561882\n",
      "Epoch 8808/20000 Training Loss: 0.05009572207927704\n",
      "Epoch 8809/20000 Training Loss: 0.04575809836387634\n",
      "Epoch 8810/20000 Training Loss: 0.10140080004930496\n",
      "Epoch 8810/20000 Validation Loss: 0.0633041113615036\n",
      "Epoch 8811/20000 Training Loss: 0.05165933445096016\n",
      "Epoch 8812/20000 Training Loss: 0.07635188847780228\n",
      "Epoch 8813/20000 Training Loss: 0.05293956771492958\n",
      "Epoch 8814/20000 Training Loss: 0.049677539616823196\n",
      "Epoch 8815/20000 Training Loss: 0.06377723067998886\n",
      "Epoch 8816/20000 Training Loss: 0.059183765202760696\n",
      "Epoch 8817/20000 Training Loss: 0.06222769618034363\n",
      "Epoch 8818/20000 Training Loss: 0.062379393726587296\n",
      "Epoch 8819/20000 Training Loss: 0.052754878997802734\n",
      "Epoch 8820/20000 Training Loss: 0.04949456453323364\n",
      "Epoch 8820/20000 Validation Loss: 0.04910542815923691\n",
      "Epoch 8821/20000 Training Loss: 0.0520111620426178\n",
      "Epoch 8822/20000 Training Loss: 0.05622301995754242\n",
      "Epoch 8823/20000 Training Loss: 0.06963828206062317\n",
      "Epoch 8824/20000 Training Loss: 0.05702734366059303\n",
      "Epoch 8825/20000 Training Loss: 0.05537630245089531\n",
      "Epoch 8826/20000 Training Loss: 0.0525052584707737\n",
      "Epoch 8827/20000 Training Loss: 0.04293832182884216\n",
      "Epoch 8828/20000 Training Loss: 0.049140989780426025\n",
      "Epoch 8829/20000 Training Loss: 0.0462530255317688\n",
      "Epoch 8830/20000 Training Loss: 0.051910024136304855\n",
      "Epoch 8830/20000 Validation Loss: 0.056908633559942245\n",
      "Epoch 8831/20000 Training Loss: 0.06334628164768219\n",
      "Epoch 8832/20000 Training Loss: 0.04094814881682396\n",
      "Epoch 8833/20000 Training Loss: 0.04753537476062775\n",
      "Epoch 8834/20000 Training Loss: 0.05908013507723808\n",
      "Epoch 8835/20000 Training Loss: 0.06795338541269302\n",
      "Epoch 8836/20000 Training Loss: 0.06121528521180153\n",
      "Epoch 8837/20000 Training Loss: 0.04308530315756798\n",
      "Epoch 8838/20000 Training Loss: 0.06133798882365227\n",
      "Epoch 8839/20000 Training Loss: 0.05481772497296333\n",
      "Epoch 8840/20000 Training Loss: 0.049455463886260986\n",
      "Epoch 8840/20000 Validation Loss: 0.07318127155303955\n",
      "Epoch 8841/20000 Training Loss: 0.06282761693000793\n",
      "Epoch 8842/20000 Training Loss: 0.05951962247490883\n",
      "Epoch 8843/20000 Training Loss: 0.05053190514445305\n",
      "Epoch 8844/20000 Training Loss: 0.054840538650751114\n",
      "Epoch 8845/20000 Training Loss: 0.03544072434306145\n",
      "Epoch 8846/20000 Training Loss: 0.05845402181148529\n",
      "Epoch 8847/20000 Training Loss: 0.07515532523393631\n",
      "Epoch 8848/20000 Training Loss: 0.060196664184331894\n",
      "Epoch 8849/20000 Training Loss: 0.0722946897149086\n",
      "Epoch 8850/20000 Training Loss: 0.05365435406565666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8850/20000 Validation Loss: 0.05227040499448776\n",
      "Epoch 8851/20000 Training Loss: 0.05420636758208275\n",
      "Epoch 8852/20000 Training Loss: 0.04748210683465004\n",
      "Epoch 8853/20000 Training Loss: 0.0499342642724514\n",
      "Epoch 8854/20000 Training Loss: 0.057643771171569824\n",
      "Epoch 8855/20000 Training Loss: 0.05812903121113777\n",
      "Epoch 8856/20000 Training Loss: 0.052356917411088943\n",
      "Epoch 8857/20000 Training Loss: 0.0649707093834877\n",
      "Epoch 8858/20000 Training Loss: 0.05215361341834068\n",
      "Epoch 8859/20000 Training Loss: 0.038337696343660355\n",
      "Epoch 8860/20000 Training Loss: 0.04921763017773628\n",
      "Epoch 8860/20000 Validation Loss: 0.05028456449508667\n",
      "Epoch 8861/20000 Training Loss: 0.04683757200837135\n",
      "Epoch 8862/20000 Training Loss: 0.040038418024778366\n",
      "Epoch 8863/20000 Training Loss: 0.0477880984544754\n",
      "Epoch 8864/20000 Training Loss: 0.05918285250663757\n",
      "Epoch 8865/20000 Training Loss: 0.04979347065091133\n",
      "Epoch 8866/20000 Training Loss: 0.0682070404291153\n",
      "Epoch 8867/20000 Training Loss: 0.05863991007208824\n",
      "Epoch 8868/20000 Training Loss: 0.05978195741772652\n",
      "Epoch 8869/20000 Training Loss: 0.06944140791893005\n",
      "Epoch 8870/20000 Training Loss: 0.047307733446359634\n",
      "Epoch 8870/20000 Validation Loss: 0.06914781779050827\n",
      "Epoch 8871/20000 Training Loss: 0.04296721890568733\n",
      "Epoch 8872/20000 Training Loss: 0.05680463835597038\n",
      "Epoch 8873/20000 Training Loss: 0.0512884147465229\n",
      "Epoch 8874/20000 Training Loss: 0.05915139243006706\n",
      "Epoch 8875/20000 Training Loss: 0.06726593524217606\n",
      "Epoch 8876/20000 Training Loss: 0.07717015594244003\n",
      "Epoch 8877/20000 Training Loss: 0.0762292742729187\n",
      "Epoch 8878/20000 Training Loss: 0.06911241263151169\n",
      "Epoch 8879/20000 Training Loss: 0.059297602623701096\n",
      "Epoch 8880/20000 Training Loss: 0.06617441773414612\n",
      "Epoch 8880/20000 Validation Loss: 0.06714826077222824\n",
      "Epoch 8881/20000 Training Loss: 0.06732354313135147\n",
      "Epoch 8882/20000 Training Loss: 0.04220937192440033\n",
      "Epoch 8883/20000 Training Loss: 0.0672193393111229\n",
      "Epoch 8884/20000 Training Loss: 0.04640313982963562\n",
      "Epoch 8885/20000 Training Loss: 0.06605184823274612\n",
      "Epoch 8886/20000 Training Loss: 0.04066844284534454\n",
      "Epoch 8887/20000 Training Loss: 0.04494282603263855\n",
      "Epoch 8888/20000 Training Loss: 0.0606449730694294\n",
      "Epoch 8889/20000 Training Loss: 0.06161217764019966\n",
      "Epoch 8890/20000 Training Loss: 0.04291896894574165\n",
      "Epoch 8890/20000 Validation Loss: 0.04958786815404892\n",
      "Epoch 8891/20000 Training Loss: 0.055824797600507736\n",
      "Epoch 8892/20000 Training Loss: 0.06863651424646378\n",
      "Epoch 8893/20000 Training Loss: 0.043784115463495255\n",
      "Epoch 8894/20000 Training Loss: 0.06311818957328796\n",
      "Epoch 8895/20000 Training Loss: 0.04240332543849945\n",
      "Epoch 8896/20000 Training Loss: 0.06329559534788132\n",
      "Epoch 8897/20000 Training Loss: 0.07104038447141647\n",
      "Epoch 8898/20000 Training Loss: 0.06175949051976204\n",
      "Epoch 8899/20000 Training Loss: 0.04716705158352852\n",
      "Epoch 8900/20000 Training Loss: 0.03864595666527748\n",
      "Epoch 8900/20000 Validation Loss: 0.06941786408424377\n",
      "Epoch 8901/20000 Training Loss: 0.054888766258955\n",
      "Epoch 8902/20000 Training Loss: 0.0491618812084198\n",
      "Epoch 8903/20000 Training Loss: 0.05155579745769501\n",
      "Epoch 8904/20000 Training Loss: 0.050955310463905334\n",
      "Epoch 8905/20000 Training Loss: 0.06440838426351547\n",
      "Epoch 8906/20000 Training Loss: 0.04232911393046379\n",
      "Epoch 8907/20000 Training Loss: 0.06085224822163582\n",
      "Epoch 8908/20000 Training Loss: 0.050903309136629105\n",
      "Epoch 8909/20000 Training Loss: 0.06191856786608696\n",
      "Epoch 8910/20000 Training Loss: 0.0838138684630394\n",
      "Epoch 8910/20000 Validation Loss: 0.05959052965044975\n",
      "Epoch 8911/20000 Training Loss: 0.06391912698745728\n",
      "Epoch 8912/20000 Training Loss: 0.05983954295516014\n",
      "Epoch 8913/20000 Training Loss: 0.057704437524080276\n",
      "Epoch 8914/20000 Training Loss: 0.06832083314657211\n",
      "Epoch 8915/20000 Training Loss: 0.05900159478187561\n",
      "Epoch 8916/20000 Training Loss: 0.0657675713300705\n",
      "Epoch 8917/20000 Training Loss: 0.05567137897014618\n",
      "Epoch 8918/20000 Training Loss: 0.06468508392572403\n",
      "Epoch 8919/20000 Training Loss: 0.05710804462432861\n",
      "Epoch 8920/20000 Training Loss: 0.05192318558692932\n",
      "Epoch 8920/20000 Validation Loss: 0.061792414635419846\n",
      "Epoch 8921/20000 Training Loss: 0.051641058176755905\n",
      "Epoch 8922/20000 Training Loss: 0.06034642457962036\n",
      "Epoch 8923/20000 Training Loss: 0.06787026673555374\n",
      "Epoch 8924/20000 Training Loss: 0.08626997470855713\n",
      "Epoch 8925/20000 Training Loss: 0.06155182048678398\n",
      "Epoch 8926/20000 Training Loss: 0.054515812546014786\n",
      "Epoch 8927/20000 Training Loss: 0.04431438818573952\n",
      "Epoch 8928/20000 Training Loss: 0.0585157573223114\n",
      "Epoch 8929/20000 Training Loss: 0.0573233999311924\n",
      "Epoch 8930/20000 Training Loss: 0.06167561188340187\n",
      "Epoch 8930/20000 Validation Loss: 0.05932554230093956\n",
      "Epoch 8931/20000 Training Loss: 0.05711563304066658\n",
      "Epoch 8932/20000 Training Loss: 0.07078039646148682\n",
      "Epoch 8933/20000 Training Loss: 0.045867618173360825\n",
      "Epoch 8934/20000 Training Loss: 0.06854093074798584\n",
      "Epoch 8935/20000 Training Loss: 0.053418636322021484\n",
      "Epoch 8936/20000 Training Loss: 0.04186605289578438\n",
      "Epoch 8937/20000 Training Loss: 0.051434289664030075\n",
      "Epoch 8938/20000 Training Loss: 0.06491009145975113\n",
      "Epoch 8939/20000 Training Loss: 0.06617704778909683\n",
      "Epoch 8940/20000 Training Loss: 0.061223387718200684\n",
      "Epoch 8940/20000 Validation Loss: 0.06746640801429749\n",
      "Epoch 8941/20000 Training Loss: 0.06974173337221146\n",
      "Epoch 8942/20000 Training Loss: 0.05466867610812187\n",
      "Epoch 8943/20000 Training Loss: 0.05134262517094612\n",
      "Epoch 8944/20000 Training Loss: 0.059028610587120056\n",
      "Epoch 8945/20000 Training Loss: 0.056570786982774734\n",
      "Epoch 8946/20000 Training Loss: 0.07878067344427109\n",
      "Epoch 8947/20000 Training Loss: 0.0643497034907341\n",
      "Epoch 8948/20000 Training Loss: 0.044244974851608276\n",
      "Epoch 8949/20000 Training Loss: 0.0672387108206749\n",
      "Epoch 8950/20000 Training Loss: 0.06566900759935379\n",
      "Epoch 8950/20000 Validation Loss: 0.04974372684955597\n",
      "Epoch 8951/20000 Training Loss: 0.07227600365877151\n",
      "Epoch 8952/20000 Training Loss: 0.044350516051054\n",
      "Epoch 8953/20000 Training Loss: 0.04271512106060982\n",
      "Epoch 8954/20000 Training Loss: 0.06199456378817558\n",
      "Epoch 8955/20000 Training Loss: 0.08053740859031677\n",
      "Epoch 8956/20000 Training Loss: 0.061599910259246826\n",
      "Epoch 8957/20000 Training Loss: 0.04333330690860748\n",
      "Epoch 8958/20000 Training Loss: 0.05100598931312561\n",
      "Epoch 8959/20000 Training Loss: 0.06252437829971313\n",
      "Epoch 8960/20000 Training Loss: 0.05561709776520729\n",
      "Epoch 8960/20000 Validation Loss: 0.050138942897319794\n",
      "Epoch 8961/20000 Training Loss: 0.05761215090751648\n",
      "Epoch 8962/20000 Training Loss: 0.05100863799452782\n",
      "Epoch 8963/20000 Training Loss: 0.0598757378757\n",
      "Epoch 8964/20000 Training Loss: 0.059926655143499374\n",
      "Epoch 8965/20000 Training Loss: 0.045292485505342484\n",
      "Epoch 8966/20000 Training Loss: 0.046601552516222\n",
      "Epoch 8967/20000 Training Loss: 0.04669452831149101\n",
      "Epoch 8968/20000 Training Loss: 0.053686514496803284\n",
      "Epoch 8969/20000 Training Loss: 0.046049680560827255\n",
      "Epoch 8970/20000 Training Loss: 0.048216745257377625\n",
      "Epoch 8970/20000 Validation Loss: 0.04815283045172691\n",
      "Epoch 8971/20000 Training Loss: 0.038455765694379807\n",
      "Epoch 8972/20000 Training Loss: 0.04525286331772804\n",
      "Epoch 8973/20000 Training Loss: 0.07336597144603729\n",
      "Epoch 8974/20000 Training Loss: 0.0601399801671505\n",
      "Epoch 8975/20000 Training Loss: 0.0662742629647255\n",
      "Epoch 8976/20000 Training Loss: 0.07060577720403671\n",
      "Epoch 8977/20000 Training Loss: 0.06318389624357224\n",
      "Epoch 8978/20000 Training Loss: 0.059974174946546555\n",
      "Epoch 8979/20000 Training Loss: 0.04755217954516411\n",
      "Epoch 8980/20000 Training Loss: 0.06194237247109413\n",
      "Epoch 8980/20000 Validation Loss: 0.055480532348155975\n",
      "Epoch 8981/20000 Training Loss: 0.06056184694170952\n",
      "Epoch 8982/20000 Training Loss: 0.04581871256232262\n",
      "Epoch 8983/20000 Training Loss: 0.04761457070708275\n",
      "Epoch 8984/20000 Training Loss: 0.06375160813331604\n",
      "Epoch 8985/20000 Training Loss: 0.04721886292099953\n",
      "Epoch 8986/20000 Training Loss: 0.051300887018442154\n",
      "Epoch 8987/20000 Training Loss: 0.06061730161309242\n",
      "Epoch 8988/20000 Training Loss: 0.05878250673413277\n",
      "Epoch 8989/20000 Training Loss: 0.04877614974975586\n",
      "Epoch 8990/20000 Training Loss: 0.05284154415130615\n",
      "Epoch 8990/20000 Validation Loss: 0.06902541220188141\n",
      "Epoch 8991/20000 Training Loss: 0.04514360800385475\n",
      "Epoch 8992/20000 Training Loss: 0.0646529495716095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8993/20000 Training Loss: 0.05871571600437164\n",
      "Epoch 8994/20000 Training Loss: 0.038069263100624084\n",
      "Epoch 8995/20000 Training Loss: 0.05342702567577362\n",
      "Epoch 8996/20000 Training Loss: 0.046890970319509506\n",
      "Epoch 8997/20000 Training Loss: 0.06135910749435425\n",
      "Epoch 8998/20000 Training Loss: 0.047455329447984695\n",
      "Epoch 8999/20000 Training Loss: 0.057859163731336594\n",
      "Epoch 9000/20000 Training Loss: 0.0664532408118248\n",
      "Epoch 9000/20000 Validation Loss: 0.04596113786101341\n",
      "Epoch 9001/20000 Training Loss: 0.06196814775466919\n",
      "Epoch 9002/20000 Training Loss: 0.040027860552072525\n",
      "Epoch 9003/20000 Training Loss: 0.046843022108078\n",
      "Epoch 9004/20000 Training Loss: 0.05602973699569702\n",
      "Epoch 9005/20000 Training Loss: 0.057234395295381546\n",
      "Epoch 9006/20000 Training Loss: 0.05441756919026375\n",
      "Epoch 9007/20000 Training Loss: 0.06395269185304642\n",
      "Epoch 9008/20000 Training Loss: 0.06493663042783737\n",
      "Epoch 9009/20000 Training Loss: 0.04969378933310509\n",
      "Epoch 9010/20000 Training Loss: 0.04238756373524666\n",
      "Epoch 9010/20000 Validation Loss: 0.06264863908290863\n",
      "Epoch 9011/20000 Training Loss: 0.053437694907188416\n",
      "Epoch 9012/20000 Training Loss: 0.05341948941349983\n",
      "Epoch 9013/20000 Training Loss: 0.05668959021568298\n",
      "Epoch 9014/20000 Training Loss: 0.07171100378036499\n",
      "Epoch 9015/20000 Training Loss: 0.05274263024330139\n",
      "Epoch 9016/20000 Training Loss: 0.04339848831295967\n",
      "Epoch 9017/20000 Training Loss: 0.058552056550979614\n",
      "Epoch 9018/20000 Training Loss: 0.05443925783038139\n",
      "Epoch 9019/20000 Training Loss: 0.0728020891547203\n",
      "Epoch 9020/20000 Training Loss: 0.06723769009113312\n",
      "Epoch 9020/20000 Validation Loss: 0.05596151947975159\n",
      "Epoch 9021/20000 Training Loss: 0.04332483932375908\n",
      "Epoch 9022/20000 Training Loss: 0.06682435423135757\n",
      "Epoch 9023/20000 Training Loss: 0.05534864962100983\n",
      "Epoch 9024/20000 Training Loss: 0.06707269698381424\n",
      "Epoch 9025/20000 Training Loss: 0.05367036163806915\n",
      "Epoch 9026/20000 Training Loss: 0.07289273291826248\n",
      "Epoch 9027/20000 Training Loss: 0.047288570553064346\n",
      "Epoch 9028/20000 Training Loss: 0.054157745093107224\n",
      "Epoch 9029/20000 Training Loss: 0.06205366924405098\n",
      "Epoch 9030/20000 Training Loss: 0.05807868763804436\n",
      "Epoch 9030/20000 Validation Loss: 0.052341390401124954\n",
      "Epoch 9031/20000 Training Loss: 0.0650491937994957\n",
      "Epoch 9032/20000 Training Loss: 0.07026038318872452\n",
      "Epoch 9033/20000 Training Loss: 0.04430140554904938\n",
      "Epoch 9034/20000 Training Loss: 0.05653133615851402\n",
      "Epoch 9035/20000 Training Loss: 0.07846660166978836\n",
      "Epoch 9036/20000 Training Loss: 0.07634294778108597\n",
      "Epoch 9037/20000 Training Loss: 0.044345032423734665\n",
      "Epoch 9038/20000 Training Loss: 0.054260432720184326\n",
      "Epoch 9039/20000 Training Loss: 0.060128021985292435\n",
      "Epoch 9040/20000 Training Loss: 0.05758395791053772\n",
      "Epoch 9040/20000 Validation Loss: 0.04810449481010437\n",
      "Epoch 9041/20000 Training Loss: 0.06281077116727829\n",
      "Epoch 9042/20000 Training Loss: 0.04762273654341698\n",
      "Epoch 9043/20000 Training Loss: 0.04885615035891533\n",
      "Epoch 9044/20000 Training Loss: 0.06165095791220665\n",
      "Epoch 9045/20000 Training Loss: 0.054427433758974075\n",
      "Epoch 9046/20000 Training Loss: 0.0655769556760788\n",
      "Epoch 9047/20000 Training Loss: 0.055338818579912186\n",
      "Epoch 9048/20000 Training Loss: 0.04716084524989128\n",
      "Epoch 9049/20000 Training Loss: 0.06265265494585037\n",
      "Epoch 9050/20000 Training Loss: 0.06166998669505119\n",
      "Epoch 9050/20000 Validation Loss: 0.0503227524459362\n",
      "Epoch 9051/20000 Training Loss: 0.07124950736761093\n",
      "Epoch 9052/20000 Training Loss: 0.05083800479769707\n",
      "Epoch 9053/20000 Training Loss: 0.05465792492032051\n",
      "Epoch 9054/20000 Training Loss: 0.06338483840227127\n",
      "Epoch 9055/20000 Training Loss: 0.07211591303348541\n",
      "Epoch 9056/20000 Training Loss: 0.04237530007958412\n",
      "Epoch 9057/20000 Training Loss: 0.06130841001868248\n",
      "Epoch 9058/20000 Training Loss: 0.04357147216796875\n",
      "Epoch 9059/20000 Training Loss: 0.05154796317219734\n",
      "Epoch 9060/20000 Training Loss: 0.04406819865107536\n",
      "Epoch 9060/20000 Validation Loss: 0.04401262104511261\n",
      "Epoch 9061/20000 Training Loss: 0.03931640461087227\n",
      "Epoch 9062/20000 Training Loss: 0.05047577992081642\n",
      "Epoch 9063/20000 Training Loss: 0.0533871054649353\n",
      "Epoch 9064/20000 Training Loss: 0.042922500520944595\n",
      "Epoch 9065/20000 Training Loss: 0.045669328421354294\n",
      "Epoch 9066/20000 Training Loss: 0.04874485358595848\n",
      "Epoch 9067/20000 Training Loss: 0.06760931760072708\n",
      "Epoch 9068/20000 Training Loss: 0.053182411938905716\n",
      "Epoch 9069/20000 Training Loss: 0.05750155821442604\n",
      "Epoch 9070/20000 Training Loss: 0.05480852723121643\n",
      "Epoch 9070/20000 Validation Loss: 0.048771873116493225\n",
      "Epoch 9071/20000 Training Loss: 0.05218641459941864\n",
      "Epoch 9072/20000 Training Loss: 0.06039411947131157\n",
      "Epoch 9073/20000 Training Loss: 0.08807551860809326\n",
      "Epoch 9074/20000 Training Loss: 0.045720577239990234\n",
      "Epoch 9075/20000 Training Loss: 0.0636962428689003\n",
      "Epoch 9076/20000 Training Loss: 0.03833687677979469\n",
      "Epoch 9077/20000 Training Loss: 0.04580540955066681\n",
      "Epoch 9078/20000 Training Loss: 0.05472804978489876\n",
      "Epoch 9079/20000 Training Loss: 0.06057368591427803\n",
      "Epoch 9080/20000 Training Loss: 0.056082069873809814\n",
      "Epoch 9080/20000 Validation Loss: 0.05831187963485718\n",
      "Epoch 9081/20000 Training Loss: 0.06632163375616074\n",
      "Epoch 9082/20000 Training Loss: 0.0617162324488163\n",
      "Epoch 9083/20000 Training Loss: 0.05388081073760986\n",
      "Epoch 9084/20000 Training Loss: 0.07166654616594315\n",
      "Epoch 9085/20000 Training Loss: 0.03748241439461708\n",
      "Epoch 9086/20000 Training Loss: 0.06168340519070625\n",
      "Epoch 9087/20000 Training Loss: 0.059723567217588425\n",
      "Epoch 9088/20000 Training Loss: 0.06232139468193054\n",
      "Epoch 9089/20000 Training Loss: 0.061175841838121414\n",
      "Epoch 9090/20000 Training Loss: 0.057839225977659225\n",
      "Epoch 9090/20000 Validation Loss: 0.046136729419231415\n",
      "Epoch 9091/20000 Training Loss: 0.05507851764559746\n",
      "Epoch 9092/20000 Training Loss: 0.0616096593439579\n",
      "Epoch 9093/20000 Training Loss: 0.07510089129209518\n",
      "Epoch 9094/20000 Training Loss: 0.059595197439193726\n",
      "Epoch 9095/20000 Training Loss: 0.06046655774116516\n",
      "Epoch 9096/20000 Training Loss: 0.06628140807151794\n",
      "Epoch 9097/20000 Training Loss: 0.07274319231510162\n",
      "Epoch 9098/20000 Training Loss: 0.05411538481712341\n",
      "Epoch 9099/20000 Training Loss: 0.048247065395116806\n",
      "Epoch 9100/20000 Training Loss: 0.06495017558336258\n",
      "Epoch 9100/20000 Validation Loss: 0.04823720455169678\n",
      "Epoch 9101/20000 Training Loss: 0.05605575442314148\n",
      "Epoch 9102/20000 Training Loss: 0.04731106385588646\n",
      "Epoch 9103/20000 Training Loss: 0.06433602422475815\n",
      "Epoch 9104/20000 Training Loss: 0.06841742992401123\n",
      "Epoch 9105/20000 Training Loss: 0.06098213791847229\n",
      "Epoch 9106/20000 Training Loss: 0.04385891929268837\n",
      "Epoch 9107/20000 Training Loss: 0.047083478420972824\n",
      "Epoch 9108/20000 Training Loss: 0.07020562142133713\n",
      "Epoch 9109/20000 Training Loss: 0.053753290325403214\n",
      "Epoch 9110/20000 Training Loss: 0.05951183661818504\n",
      "Epoch 9110/20000 Validation Loss: 0.054513104259967804\n",
      "Epoch 9111/20000 Training Loss: 0.04442369565367699\n",
      "Epoch 9112/20000 Training Loss: 0.04235268756747246\n",
      "Epoch 9113/20000 Training Loss: 0.07570832222700119\n",
      "Epoch 9114/20000 Training Loss: 0.06396295875310898\n",
      "Epoch 9115/20000 Training Loss: 0.05774708092212677\n",
      "Epoch 9116/20000 Training Loss: 0.0626269057393074\n",
      "Epoch 9117/20000 Training Loss: 0.052650969475507736\n",
      "Epoch 9118/20000 Training Loss: 0.05049450322985649\n",
      "Epoch 9119/20000 Training Loss: 0.06776701658964157\n",
      "Epoch 9120/20000 Training Loss: 0.060410916805267334\n",
      "Epoch 9120/20000 Validation Loss: 0.04700090363621712\n",
      "Epoch 9121/20000 Training Loss: 0.05293082073330879\n",
      "Epoch 9122/20000 Training Loss: 0.06008269265294075\n",
      "Epoch 9123/20000 Training Loss: 0.04535059258341789\n",
      "Epoch 9124/20000 Training Loss: 0.04944893345236778\n",
      "Epoch 9125/20000 Training Loss: 0.050950419157743454\n",
      "Epoch 9126/20000 Training Loss: 0.06586993485689163\n",
      "Epoch 9127/20000 Training Loss: 0.059192955493927\n",
      "Epoch 9128/20000 Training Loss: 0.07420987635850906\n",
      "Epoch 9129/20000 Training Loss: 0.048717398196458817\n",
      "Epoch 9130/20000 Training Loss: 0.056800972670316696\n",
      "Epoch 9130/20000 Validation Loss: 0.051865361630916595\n",
      "Epoch 9131/20000 Training Loss: 0.05689696967601776\n",
      "Epoch 9132/20000 Training Loss: 0.05696146562695503\n",
      "Epoch 9133/20000 Training Loss: 0.07088986784219742\n",
      "Epoch 9134/20000 Training Loss: 0.05650143325328827\n",
      "Epoch 9135/20000 Training Loss: 0.05357150733470917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9136/20000 Training Loss: 0.03776051476597786\n",
      "Epoch 9137/20000 Training Loss: 0.057037755846977234\n",
      "Epoch 9138/20000 Training Loss: 0.058017417788505554\n",
      "Epoch 9139/20000 Training Loss: 0.04915459454059601\n",
      "Epoch 9140/20000 Training Loss: 0.061230748891830444\n",
      "Epoch 9140/20000 Validation Loss: 0.035738758742809296\n",
      "Epoch 9141/20000 Training Loss: 0.04270806908607483\n",
      "Epoch 9142/20000 Training Loss: 0.06441254913806915\n",
      "Epoch 9143/20000 Training Loss: 0.056119997054338455\n",
      "Epoch 9144/20000 Training Loss: 0.060639094561338425\n",
      "Epoch 9145/20000 Training Loss: 0.04702457785606384\n",
      "Epoch 9146/20000 Training Loss: 0.05871814116835594\n",
      "Epoch 9147/20000 Training Loss: 0.04879934713244438\n",
      "Epoch 9148/20000 Training Loss: 0.06006354093551636\n",
      "Epoch 9149/20000 Training Loss: 0.05088822543621063\n",
      "Epoch 9150/20000 Training Loss: 0.05993574485182762\n",
      "Epoch 9150/20000 Validation Loss: 0.055543188005685806\n",
      "Epoch 9151/20000 Training Loss: 0.06636754423379898\n",
      "Epoch 9152/20000 Training Loss: 0.06447727233171463\n",
      "Epoch 9153/20000 Training Loss: 0.046750981360673904\n",
      "Epoch 9154/20000 Training Loss: 0.052782829850912094\n",
      "Epoch 9155/20000 Training Loss: 0.06304959207773209\n",
      "Epoch 9156/20000 Training Loss: 0.06825893372297287\n",
      "Epoch 9157/20000 Training Loss: 0.06718399375677109\n",
      "Epoch 9158/20000 Training Loss: 0.06803256273269653\n",
      "Epoch 9159/20000 Training Loss: 0.05806051567196846\n",
      "Epoch 9160/20000 Training Loss: 0.060082752257585526\n",
      "Epoch 9160/20000 Validation Loss: 0.07356218993663788\n",
      "Epoch 9161/20000 Training Loss: 0.04540836811065674\n",
      "Epoch 9162/20000 Training Loss: 0.04748521372675896\n",
      "Epoch 9163/20000 Training Loss: 0.06471335887908936\n",
      "Epoch 9164/20000 Training Loss: 0.05469382181763649\n",
      "Epoch 9165/20000 Training Loss: 0.053652163594961166\n",
      "Epoch 9166/20000 Training Loss: 0.062341928482055664\n",
      "Epoch 9167/20000 Training Loss: 0.06057898327708244\n",
      "Epoch 9168/20000 Training Loss: 0.0672658309340477\n",
      "Epoch 9169/20000 Training Loss: 0.06421057134866714\n",
      "Epoch 9170/20000 Training Loss: 0.05837077274918556\n",
      "Epoch 9170/20000 Validation Loss: 0.048948466777801514\n",
      "Epoch 9171/20000 Training Loss: 0.049522604793310165\n",
      "Epoch 9172/20000 Training Loss: 0.04847725108265877\n",
      "Epoch 9173/20000 Training Loss: 0.037558864802122116\n",
      "Epoch 9174/20000 Training Loss: 0.05358746647834778\n",
      "Epoch 9175/20000 Training Loss: 0.04053359478712082\n",
      "Epoch 9176/20000 Training Loss: 0.06326218694448471\n",
      "Epoch 9177/20000 Training Loss: 0.06269033998250961\n",
      "Epoch 9178/20000 Training Loss: 0.0427866168320179\n",
      "Epoch 9179/20000 Training Loss: 0.069989874958992\n",
      "Epoch 9180/20000 Training Loss: 0.0626242607831955\n",
      "Epoch 9180/20000 Validation Loss: 0.044627558439970016\n",
      "Epoch 9181/20000 Training Loss: 0.057498425245285034\n",
      "Epoch 9182/20000 Training Loss: 0.05855032801628113\n",
      "Epoch 9183/20000 Training Loss: 0.06859190762042999\n",
      "Epoch 9184/20000 Training Loss: 0.03994308412075043\n",
      "Epoch 9185/20000 Training Loss: 0.05868386849761009\n",
      "Epoch 9186/20000 Training Loss: 0.06929760426282883\n",
      "Epoch 9187/20000 Training Loss: 0.05250924825668335\n",
      "Epoch 9188/20000 Training Loss: 0.05652741715312004\n",
      "Epoch 9189/20000 Training Loss: 0.059167277067899704\n",
      "Epoch 9190/20000 Training Loss: 0.060276586562395096\n",
      "Epoch 9190/20000 Validation Loss: 0.05130782350897789\n",
      "Epoch 9191/20000 Training Loss: 0.04105024412274361\n",
      "Epoch 9192/20000 Training Loss: 0.0619770772755146\n",
      "Epoch 9193/20000 Training Loss: 0.04767846688628197\n",
      "Epoch 9194/20000 Training Loss: 0.06618985533714294\n",
      "Epoch 9195/20000 Training Loss: 0.047071877866983414\n",
      "Epoch 9196/20000 Training Loss: 0.04331843927502632\n",
      "Epoch 9197/20000 Training Loss: 0.054962288588285446\n",
      "Epoch 9198/20000 Training Loss: 0.05638289824128151\n",
      "Epoch 9199/20000 Training Loss: 0.0641859844326973\n",
      "Epoch 9200/20000 Training Loss: 0.04031110554933548\n",
      "Epoch 9200/20000 Validation Loss: 0.0592074878513813\n",
      "Epoch 9201/20000 Training Loss: 0.05582277104258537\n",
      "Epoch 9202/20000 Training Loss: 0.04629206657409668\n",
      "Epoch 9203/20000 Training Loss: 0.07429676502943039\n",
      "Epoch 9204/20000 Training Loss: 0.04403482377529144\n",
      "Epoch 9205/20000 Training Loss: 0.05851026251912117\n",
      "Epoch 9206/20000 Training Loss: 0.06744697690010071\n",
      "Epoch 9207/20000 Training Loss: 0.05199447274208069\n",
      "Epoch 9208/20000 Training Loss: 0.048232972621917725\n",
      "Epoch 9209/20000 Training Loss: 0.05787321925163269\n",
      "Epoch 9210/20000 Training Loss: 0.0615326352417469\n",
      "Epoch 9210/20000 Validation Loss: 0.051456112414598465\n",
      "Epoch 9211/20000 Training Loss: 0.05167840048670769\n",
      "Epoch 9212/20000 Training Loss: 0.05070051923394203\n",
      "Epoch 9213/20000 Training Loss: 0.04539639875292778\n",
      "Epoch 9214/20000 Training Loss: 0.05378907918930054\n",
      "Epoch 9215/20000 Training Loss: 0.07542864233255386\n",
      "Epoch 9216/20000 Training Loss: 0.05196349322795868\n",
      "Epoch 9217/20000 Training Loss: 0.05676168203353882\n",
      "Epoch 9218/20000 Training Loss: 0.055134132504463196\n",
      "Epoch 9219/20000 Training Loss: 0.052056875079870224\n",
      "Epoch 9220/20000 Training Loss: 0.045604780316352844\n",
      "Epoch 9220/20000 Validation Loss: 0.04431772604584694\n",
      "Epoch 9221/20000 Training Loss: 0.04809293523430824\n",
      "Epoch 9222/20000 Training Loss: 0.04706408083438873\n",
      "Epoch 9223/20000 Training Loss: 0.05832141637802124\n",
      "Epoch 9224/20000 Training Loss: 0.0491025447845459\n",
      "Epoch 9225/20000 Training Loss: 0.04887585714459419\n",
      "Epoch 9226/20000 Training Loss: 0.05734549090266228\n",
      "Epoch 9227/20000 Training Loss: 0.05729101970791817\n",
      "Epoch 9228/20000 Training Loss: 0.04359439015388489\n",
      "Epoch 9229/20000 Training Loss: 0.06701531261205673\n",
      "Epoch 9230/20000 Training Loss: 0.059204716235399246\n",
      "Epoch 9230/20000 Validation Loss: 0.05751583352684975\n",
      "Epoch 9231/20000 Training Loss: 0.060237567871809006\n",
      "Epoch 9232/20000 Training Loss: 0.04891541600227356\n",
      "Epoch 9233/20000 Training Loss: 0.0491415373980999\n",
      "Epoch 9234/20000 Training Loss: 0.08776403218507767\n",
      "Epoch 9235/20000 Training Loss: 0.05136590078473091\n",
      "Epoch 9236/20000 Training Loss: 0.04145083203911781\n",
      "Epoch 9237/20000 Training Loss: 0.06467578560113907\n",
      "Epoch 9238/20000 Training Loss: 0.058865662664175034\n",
      "Epoch 9239/20000 Training Loss: 0.054842863231897354\n",
      "Epoch 9240/20000 Training Loss: 0.039931636303663254\n",
      "Epoch 9240/20000 Validation Loss: 0.06051655113697052\n",
      "Epoch 9241/20000 Training Loss: 0.05016778036952019\n",
      "Epoch 9242/20000 Training Loss: 0.046993549913167953\n",
      "Epoch 9243/20000 Training Loss: 0.043414968997240067\n",
      "Epoch 9244/20000 Training Loss: 0.05655643343925476\n",
      "Epoch 9245/20000 Training Loss: 0.05917568877339363\n",
      "Epoch 9246/20000 Training Loss: 0.056277092546224594\n",
      "Epoch 9247/20000 Training Loss: 0.04480208829045296\n",
      "Epoch 9248/20000 Training Loss: 0.04175803065299988\n",
      "Epoch 9249/20000 Training Loss: 0.057841185480356216\n",
      "Epoch 9250/20000 Training Loss: 0.06979764997959137\n",
      "Epoch 9250/20000 Validation Loss: 0.06331650912761688\n",
      "Epoch 9251/20000 Training Loss: 0.04861654341220856\n",
      "Epoch 9252/20000 Training Loss: 0.054276928305625916\n",
      "Epoch 9253/20000 Training Loss: 0.05779401957988739\n",
      "Epoch 9254/20000 Training Loss: 0.08359632641077042\n",
      "Epoch 9255/20000 Training Loss: 0.04767638072371483\n",
      "Epoch 9256/20000 Training Loss: 0.04552352800965309\n",
      "Epoch 9257/20000 Training Loss: 0.059048473834991455\n",
      "Epoch 9258/20000 Training Loss: 0.05591214820742607\n",
      "Epoch 9259/20000 Training Loss: 0.07591023296117783\n",
      "Epoch 9260/20000 Training Loss: 0.062266793102025986\n",
      "Epoch 9260/20000 Validation Loss: 0.05912511795759201\n",
      "Epoch 9261/20000 Training Loss: 0.04722990095615387\n",
      "Epoch 9262/20000 Training Loss: 0.07004041224718094\n",
      "Epoch 9263/20000 Training Loss: 0.07855132222175598\n",
      "Epoch 9264/20000 Training Loss: 0.06560294330120087\n",
      "Epoch 9265/20000 Training Loss: 0.04119960591197014\n",
      "Epoch 9266/20000 Training Loss: 0.05242766812443733\n",
      "Epoch 9267/20000 Training Loss: 0.04709451273083687\n",
      "Epoch 9268/20000 Training Loss: 0.06261066347360611\n",
      "Epoch 9269/20000 Training Loss: 0.03493359312415123\n",
      "Epoch 9270/20000 Training Loss: 0.04850287735462189\n",
      "Epoch 9270/20000 Validation Loss: 0.05359958857297897\n",
      "Epoch 9271/20000 Training Loss: 0.049024078994989395\n",
      "Epoch 9272/20000 Training Loss: 0.06484214961528778\n",
      "Epoch 9273/20000 Training Loss: 0.04944274201989174\n",
      "Epoch 9274/20000 Training Loss: 0.056715577840805054\n",
      "Epoch 9275/20000 Training Loss: 0.0489531047642231\n",
      "Epoch 9276/20000 Training Loss: 0.0624503493309021\n",
      "Epoch 9277/20000 Training Loss: 0.05190242454409599\n",
      "Epoch 9278/20000 Training Loss: 0.06313404440879822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9279/20000 Training Loss: 0.06003584340214729\n",
      "Epoch 9280/20000 Training Loss: 0.04525617137551308\n",
      "Epoch 9280/20000 Validation Loss: 0.04256200045347214\n",
      "Epoch 9281/20000 Training Loss: 0.059553105384111404\n",
      "Epoch 9282/20000 Training Loss: 0.047118932008743286\n",
      "Epoch 9283/20000 Training Loss: 0.055363014340400696\n",
      "Epoch 9284/20000 Training Loss: 0.06644853949546814\n",
      "Epoch 9285/20000 Training Loss: 0.047738414257764816\n",
      "Epoch 9286/20000 Training Loss: 0.06060837581753731\n",
      "Epoch 9287/20000 Training Loss: 0.04848739504814148\n",
      "Epoch 9288/20000 Training Loss: 0.06550095230340958\n",
      "Epoch 9289/20000 Training Loss: 0.053349439054727554\n",
      "Epoch 9290/20000 Training Loss: 0.0749359130859375\n",
      "Epoch 9290/20000 Validation Loss: 0.08663543313741684\n",
      "Epoch 9291/20000 Training Loss: 0.06225307285785675\n",
      "Epoch 9292/20000 Training Loss: 0.07531766593456268\n",
      "Epoch 9293/20000 Training Loss: 0.0356316976249218\n",
      "Epoch 9294/20000 Training Loss: 0.06497633457183838\n",
      "Epoch 9295/20000 Training Loss: 0.055851902812719345\n",
      "Epoch 9296/20000 Training Loss: 0.05342144891619682\n",
      "Epoch 9297/20000 Training Loss: 0.03708091750741005\n",
      "Epoch 9298/20000 Training Loss: 0.06134792044758797\n",
      "Epoch 9299/20000 Training Loss: 0.05088568851351738\n",
      "Epoch 9300/20000 Training Loss: 0.05070872604846954\n",
      "Epoch 9300/20000 Validation Loss: 0.06547769904136658\n",
      "Epoch 9301/20000 Training Loss: 0.05833522975444794\n",
      "Epoch 9302/20000 Training Loss: 0.053222764283418655\n",
      "Epoch 9303/20000 Training Loss: 0.04772600531578064\n",
      "Epoch 9304/20000 Training Loss: 0.03647586330771446\n",
      "Epoch 9305/20000 Training Loss: 0.05171836540102959\n",
      "Epoch 9306/20000 Training Loss: 0.05029665306210518\n",
      "Epoch 9307/20000 Training Loss: 0.048134565353393555\n",
      "Epoch 9308/20000 Training Loss: 0.03548702970147133\n",
      "Epoch 9309/20000 Training Loss: 0.05653915926814079\n",
      "Epoch 9310/20000 Training Loss: 0.053718458861112595\n",
      "Epoch 9310/20000 Validation Loss: 0.04969482496380806\n",
      "Epoch 9311/20000 Training Loss: 0.06274259835481644\n",
      "Epoch 9312/20000 Training Loss: 0.04075433313846588\n",
      "Epoch 9313/20000 Training Loss: 0.041772499680519104\n",
      "Epoch 9314/20000 Training Loss: 0.05896274372935295\n",
      "Epoch 9315/20000 Training Loss: 0.05964719131588936\n",
      "Epoch 9316/20000 Training Loss: 0.04556937515735626\n",
      "Epoch 9317/20000 Training Loss: 0.07310940325260162\n",
      "Epoch 9318/20000 Training Loss: 0.06564261764287949\n",
      "Epoch 9319/20000 Training Loss: 0.07426140457391739\n",
      "Epoch 9320/20000 Training Loss: 0.048290278762578964\n",
      "Epoch 9320/20000 Validation Loss: 0.06160653382539749\n",
      "Epoch 9321/20000 Training Loss: 0.04683086276054382\n",
      "Epoch 9322/20000 Training Loss: 0.0642826184630394\n",
      "Epoch 9323/20000 Training Loss: 0.057691771537065506\n",
      "Epoch 9324/20000 Training Loss: 0.0451970137655735\n",
      "Epoch 9325/20000 Training Loss: 0.06538797169923782\n",
      "Epoch 9326/20000 Training Loss: 0.057135313749313354\n",
      "Epoch 9327/20000 Training Loss: 0.0466078482568264\n",
      "Epoch 9328/20000 Training Loss: 0.05343464016914368\n",
      "Epoch 9329/20000 Training Loss: 0.057779133319854736\n",
      "Epoch 9330/20000 Training Loss: 0.041465431451797485\n",
      "Epoch 9330/20000 Validation Loss: 0.052838411182165146\n",
      "Epoch 9331/20000 Training Loss: 0.049660831689834595\n",
      "Epoch 9332/20000 Training Loss: 0.05449892207980156\n",
      "Epoch 9333/20000 Training Loss: 0.05359421297907829\n",
      "Epoch 9334/20000 Training Loss: 0.060006577521562576\n",
      "Epoch 9335/20000 Training Loss: 0.05781112611293793\n",
      "Epoch 9336/20000 Training Loss: 0.056533824652433395\n",
      "Epoch 9337/20000 Training Loss: 0.04755467176437378\n",
      "Epoch 9338/20000 Training Loss: 0.04079027846455574\n",
      "Epoch 9339/20000 Training Loss: 0.062079016119241714\n",
      "Epoch 9340/20000 Training Loss: 0.04797070845961571\n",
      "Epoch 9340/20000 Validation Loss: 0.06689479947090149\n",
      "Epoch 9341/20000 Training Loss: 0.05225887894630432\n",
      "Epoch 9342/20000 Training Loss: 0.06131552532315254\n",
      "Epoch 9343/20000 Training Loss: 0.056776802986860275\n",
      "Epoch 9344/20000 Training Loss: 0.06036602333188057\n",
      "Epoch 9345/20000 Training Loss: 0.0565941296517849\n",
      "Epoch 9346/20000 Training Loss: 0.06320000439882278\n",
      "Epoch 9347/20000 Training Loss: 0.04003682732582092\n",
      "Epoch 9348/20000 Training Loss: 0.050310760736465454\n",
      "Epoch 9349/20000 Training Loss: 0.048595648258924484\n",
      "Epoch 9350/20000 Training Loss: 0.056764692068099976\n",
      "Epoch 9350/20000 Validation Loss: 0.04894857108592987\n",
      "Epoch 9351/20000 Training Loss: 0.040091902017593384\n",
      "Epoch 9352/20000 Training Loss: 0.04232698678970337\n",
      "Epoch 9353/20000 Training Loss: 0.06045059487223625\n",
      "Epoch 9354/20000 Training Loss: 0.04112212359905243\n",
      "Epoch 9355/20000 Training Loss: 0.062340084463357925\n",
      "Epoch 9356/20000 Training Loss: 0.051271531730890274\n",
      "Epoch 9357/20000 Training Loss: 0.062463242560625076\n",
      "Epoch 9358/20000 Training Loss: 0.0585281141102314\n",
      "Epoch 9359/20000 Training Loss: 0.051557838916778564\n",
      "Epoch 9360/20000 Training Loss: 0.04787135124206543\n",
      "Epoch 9360/20000 Validation Loss: 0.051694102585315704\n",
      "Epoch 9361/20000 Training Loss: 0.0713849887251854\n",
      "Epoch 9362/20000 Training Loss: 0.07018693536520004\n",
      "Epoch 9363/20000 Training Loss: 0.052635613828897476\n",
      "Epoch 9364/20000 Training Loss: 0.0696016475558281\n",
      "Epoch 9365/20000 Training Loss: 0.04783419147133827\n",
      "Epoch 9366/20000 Training Loss: 0.0737420916557312\n",
      "Epoch 9367/20000 Training Loss: 0.051621291786432266\n",
      "Epoch 9368/20000 Training Loss: 0.05754028260707855\n",
      "Epoch 9369/20000 Training Loss: 0.04756036400794983\n",
      "Epoch 9370/20000 Training Loss: 0.05316119268536568\n",
      "Epoch 9370/20000 Validation Loss: 0.06725780665874481\n",
      "Epoch 9371/20000 Training Loss: 0.048940058797597885\n",
      "Epoch 9372/20000 Training Loss: 0.04721634462475777\n",
      "Epoch 9373/20000 Training Loss: 0.04530853405594826\n",
      "Epoch 9374/20000 Training Loss: 0.07469583302736282\n",
      "Epoch 9375/20000 Training Loss: 0.08689192682504654\n",
      "Epoch 9376/20000 Training Loss: 0.053020086139440536\n",
      "Epoch 9377/20000 Training Loss: 0.06969214230775833\n",
      "Epoch 9378/20000 Training Loss: 0.05197189748287201\n",
      "Epoch 9379/20000 Training Loss: 0.06165356561541557\n",
      "Epoch 9380/20000 Training Loss: 0.05868474766612053\n",
      "Epoch 9380/20000 Validation Loss: 0.06325805932283401\n",
      "Epoch 9381/20000 Training Loss: 0.06248289346694946\n",
      "Epoch 9382/20000 Training Loss: 0.0542273074388504\n",
      "Epoch 9383/20000 Training Loss: 0.08711115270853043\n",
      "Epoch 9384/20000 Training Loss: 0.061178360134363174\n",
      "Epoch 9385/20000 Training Loss: 0.05840405449271202\n",
      "Epoch 9386/20000 Training Loss: 0.05362309142947197\n",
      "Epoch 9387/20000 Training Loss: 0.06928563863039017\n",
      "Epoch 9388/20000 Training Loss: 0.0802130177617073\n",
      "Epoch 9389/20000 Training Loss: 0.0629110261797905\n",
      "Epoch 9390/20000 Training Loss: 0.04808859899640083\n",
      "Epoch 9390/20000 Validation Loss: 0.05150150507688522\n",
      "Epoch 9391/20000 Training Loss: 0.043060317635536194\n",
      "Epoch 9392/20000 Training Loss: 0.07370825856924057\n",
      "Epoch 9393/20000 Training Loss: 0.05731559917330742\n",
      "Epoch 9394/20000 Training Loss: 0.06610266864299774\n",
      "Epoch 9395/20000 Training Loss: 0.054268285632133484\n",
      "Epoch 9396/20000 Training Loss: 0.05036221072077751\n",
      "Epoch 9397/20000 Training Loss: 0.05148618295788765\n",
      "Epoch 9398/20000 Training Loss: 0.053298961371183395\n",
      "Epoch 9399/20000 Training Loss: 0.05806689336895943\n",
      "Epoch 9400/20000 Training Loss: 0.06357920914888382\n",
      "Epoch 9400/20000 Validation Loss: 0.0818723738193512\n",
      "Epoch 9401/20000 Training Loss: 0.06321907788515091\n",
      "Epoch 9402/20000 Training Loss: 0.052407484501600266\n",
      "Epoch 9403/20000 Training Loss: 0.0476195402443409\n",
      "Epoch 9404/20000 Training Loss: 0.07757657766342163\n",
      "Epoch 9405/20000 Training Loss: 0.06699826568365097\n",
      "Epoch 9406/20000 Training Loss: 0.04046414792537689\n",
      "Epoch 9407/20000 Training Loss: 0.05329388752579689\n",
      "Epoch 9408/20000 Training Loss: 0.06802735477685928\n",
      "Epoch 9409/20000 Training Loss: 0.06569044291973114\n",
      "Epoch 9410/20000 Training Loss: 0.039544906467199326\n",
      "Epoch 9410/20000 Validation Loss: 0.04719723016023636\n",
      "Epoch 9411/20000 Training Loss: 0.07223540544509888\n",
      "Epoch 9412/20000 Training Loss: 0.06402347981929779\n",
      "Epoch 9413/20000 Training Loss: 0.06862706691026688\n",
      "Epoch 9414/20000 Training Loss: 0.055955369025468826\n",
      "Epoch 9415/20000 Training Loss: 0.050733450800180435\n",
      "Epoch 9416/20000 Training Loss: 0.04794204607605934\n",
      "Epoch 9417/20000 Training Loss: 0.056548189371824265\n",
      "Epoch 9418/20000 Training Loss: 0.061625074595212936\n",
      "Epoch 9419/20000 Training Loss: 0.057887881994247437\n",
      "Epoch 9420/20000 Training Loss: 0.060696784406900406\n",
      "Epoch 9420/20000 Validation Loss: 0.03822339326143265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9421/20000 Training Loss: 0.06881082057952881\n",
      "Epoch 9422/20000 Training Loss: 0.052176523953676224\n",
      "Epoch 9423/20000 Training Loss: 0.07622305303812027\n",
      "Epoch 9424/20000 Training Loss: 0.062405068427324295\n",
      "Epoch 9425/20000 Training Loss: 0.05319971963763237\n",
      "Epoch 9426/20000 Training Loss: 0.04562729597091675\n",
      "Epoch 9427/20000 Training Loss: 0.04849379137158394\n",
      "Epoch 9428/20000 Training Loss: 0.05216043069958687\n",
      "Epoch 9429/20000 Training Loss: 0.06452783942222595\n",
      "Epoch 9430/20000 Training Loss: 0.06373385339975357\n",
      "Epoch 9430/20000 Validation Loss: 0.08340328931808472\n",
      "Epoch 9431/20000 Training Loss: 0.059982117265462875\n",
      "Epoch 9432/20000 Training Loss: 0.05102277919650078\n",
      "Epoch 9433/20000 Training Loss: 0.058704424649477005\n",
      "Epoch 9434/20000 Training Loss: 0.05102551355957985\n",
      "Epoch 9435/20000 Training Loss: 0.05509057268500328\n",
      "Epoch 9436/20000 Training Loss: 0.056320544332265854\n",
      "Epoch 9437/20000 Training Loss: 0.06588833034038544\n",
      "Epoch 9438/20000 Training Loss: 0.04877747595310211\n",
      "Epoch 9439/20000 Training Loss: 0.06045717000961304\n",
      "Epoch 9440/20000 Training Loss: 0.07648751139640808\n",
      "Epoch 9440/20000 Validation Loss: 0.060788169503211975\n",
      "Epoch 9441/20000 Training Loss: 0.04152022674679756\n",
      "Epoch 9442/20000 Training Loss: 0.05035114288330078\n",
      "Epoch 9443/20000 Training Loss: 0.04271307960152626\n",
      "Epoch 9444/20000 Training Loss: 0.058955688029527664\n",
      "Epoch 9445/20000 Training Loss: 0.051898594945669174\n",
      "Epoch 9446/20000 Training Loss: 0.05185691639780998\n",
      "Epoch 9447/20000 Training Loss: 0.0675843134522438\n",
      "Epoch 9448/20000 Training Loss: 0.066551074385643\n",
      "Epoch 9449/20000 Training Loss: 0.06358546763658524\n",
      "Epoch 9450/20000 Training Loss: 0.07076574116945267\n",
      "Epoch 9450/20000 Validation Loss: 0.038783878087997437\n",
      "Epoch 9451/20000 Training Loss: 0.06032050773501396\n",
      "Epoch 9452/20000 Training Loss: 0.04453654959797859\n",
      "Epoch 9453/20000 Training Loss: 0.05112042650580406\n",
      "Epoch 9454/20000 Training Loss: 0.058838438242673874\n",
      "Epoch 9455/20000 Training Loss: 0.04406001791357994\n",
      "Epoch 9456/20000 Training Loss: 0.055072490125894547\n",
      "Epoch 9457/20000 Training Loss: 0.04804658889770508\n",
      "Epoch 9458/20000 Training Loss: 0.06427377462387085\n",
      "Epoch 9459/20000 Training Loss: 0.056417595595121384\n",
      "Epoch 9460/20000 Training Loss: 0.08614283055067062\n",
      "Epoch 9460/20000 Validation Loss: 0.05629988759756088\n",
      "Epoch 9461/20000 Training Loss: 0.037984699010849\n",
      "Epoch 9462/20000 Training Loss: 0.06450303643941879\n",
      "Epoch 9463/20000 Training Loss: 0.04870167374610901\n",
      "Epoch 9464/20000 Training Loss: 0.04075854271650314\n",
      "Epoch 9465/20000 Training Loss: 0.061795447021722794\n",
      "Epoch 9466/20000 Training Loss: 0.04553505778312683\n",
      "Epoch 9467/20000 Training Loss: 0.04507734254002571\n",
      "Epoch 9468/20000 Training Loss: 0.058083534240722656\n",
      "Epoch 9469/20000 Training Loss: 0.039993297308683395\n",
      "Epoch 9470/20000 Training Loss: 0.0614483468234539\n",
      "Epoch 9470/20000 Validation Loss: 0.062182746827602386\n",
      "Epoch 9471/20000 Training Loss: 0.06554841995239258\n",
      "Epoch 9472/20000 Training Loss: 0.05164294317364693\n",
      "Epoch 9473/20000 Training Loss: 0.07382699102163315\n",
      "Epoch 9474/20000 Training Loss: 0.0776466652750969\n",
      "Epoch 9475/20000 Training Loss: 0.04902933910489082\n",
      "Epoch 9476/20000 Training Loss: 0.05544992908835411\n",
      "Epoch 9477/20000 Training Loss: 0.03651459142565727\n",
      "Epoch 9478/20000 Training Loss: 0.052103687077760696\n",
      "Epoch 9479/20000 Training Loss: 0.06014925241470337\n",
      "Epoch 9480/20000 Training Loss: 0.04297274723649025\n",
      "Epoch 9480/20000 Validation Loss: 0.08103381842374802\n",
      "Epoch 9481/20000 Training Loss: 0.05887576937675476\n",
      "Epoch 9482/20000 Training Loss: 0.06017054244875908\n",
      "Epoch 9483/20000 Training Loss: 0.0682792067527771\n",
      "Epoch 9484/20000 Training Loss: 0.04869166016578674\n",
      "Epoch 9485/20000 Training Loss: 0.04868283495306969\n",
      "Epoch 9486/20000 Training Loss: 0.055470243096351624\n",
      "Epoch 9487/20000 Training Loss: 0.05305804684758186\n",
      "Epoch 9488/20000 Training Loss: 0.07389756292104721\n",
      "Epoch 9489/20000 Training Loss: 0.04445816949009895\n",
      "Epoch 9490/20000 Training Loss: 0.045734062790870667\n",
      "Epoch 9490/20000 Validation Loss: 0.06445638090372086\n",
      "Epoch 9491/20000 Training Loss: 0.03623901307582855\n",
      "Epoch 9492/20000 Training Loss: 0.04396088048815727\n",
      "Epoch 9493/20000 Training Loss: 0.05619969964027405\n",
      "Epoch 9494/20000 Training Loss: 0.04834936931729317\n",
      "Epoch 9495/20000 Training Loss: 0.053496528416872025\n",
      "Epoch 9496/20000 Training Loss: 0.050219956785440445\n",
      "Epoch 9497/20000 Training Loss: 0.04629330337047577\n",
      "Epoch 9498/20000 Training Loss: 0.0644361600279808\n",
      "Epoch 9499/20000 Training Loss: 0.03487139567732811\n",
      "Epoch 9500/20000 Training Loss: 0.05488206818699837\n",
      "Epoch 9500/20000 Validation Loss: 0.07019702345132828\n",
      "Epoch 9501/20000 Training Loss: 0.0454501174390316\n",
      "Epoch 9502/20000 Training Loss: 0.052474040538072586\n",
      "Epoch 9503/20000 Training Loss: 0.055987294763326645\n",
      "Epoch 9504/20000 Training Loss: 0.05291062965989113\n",
      "Epoch 9505/20000 Training Loss: 0.06938958168029785\n",
      "Epoch 9506/20000 Training Loss: 0.05212723836302757\n",
      "Epoch 9507/20000 Training Loss: 0.06477376818656921\n",
      "Epoch 9508/20000 Training Loss: 0.04370284080505371\n",
      "Epoch 9509/20000 Training Loss: 0.06825214624404907\n",
      "Epoch 9510/20000 Training Loss: 0.04762336611747742\n",
      "Epoch 9510/20000 Validation Loss: 0.05655226856470108\n",
      "Epoch 9511/20000 Training Loss: 0.038631048053503036\n",
      "Epoch 9512/20000 Training Loss: 0.0659651979804039\n",
      "Epoch 9513/20000 Training Loss: 0.04537894204258919\n",
      "Epoch 9514/20000 Training Loss: 0.0558801107108593\n",
      "Epoch 9515/20000 Training Loss: 0.0520457923412323\n",
      "Epoch 9516/20000 Training Loss: 0.057678237557411194\n",
      "Epoch 9517/20000 Training Loss: 0.05887719988822937\n",
      "Epoch 9518/20000 Training Loss: 0.061795543879270554\n",
      "Epoch 9519/20000 Training Loss: 0.048767175525426865\n",
      "Epoch 9520/20000 Training Loss: 0.06891285628080368\n",
      "Epoch 9520/20000 Validation Loss: 0.0735255554318428\n",
      "Epoch 9521/20000 Training Loss: 0.05127249285578728\n",
      "Epoch 9522/20000 Training Loss: 0.051206428557634354\n",
      "Epoch 9523/20000 Training Loss: 0.04566578567028046\n",
      "Epoch 9524/20000 Training Loss: 0.07908385246992111\n",
      "Epoch 9525/20000 Training Loss: 0.0567425899207592\n",
      "Epoch 9526/20000 Training Loss: 0.058019667863845825\n",
      "Epoch 9527/20000 Training Loss: 0.046642836183309555\n",
      "Epoch 9528/20000 Training Loss: 0.051639992743730545\n",
      "Epoch 9529/20000 Training Loss: 0.040607646107673645\n",
      "Epoch 9530/20000 Training Loss: 0.05235990881919861\n",
      "Epoch 9530/20000 Validation Loss: 0.053855203092098236\n",
      "Epoch 9531/20000 Training Loss: 0.05335843563079834\n",
      "Epoch 9532/20000 Training Loss: 0.04116092249751091\n",
      "Epoch 9533/20000 Training Loss: 0.05680133029818535\n",
      "Epoch 9534/20000 Training Loss: 0.0617426335811615\n",
      "Epoch 9535/20000 Training Loss: 0.04717668518424034\n",
      "Epoch 9536/20000 Training Loss: 0.05184986814856529\n",
      "Epoch 9537/20000 Training Loss: 0.06027251482009888\n",
      "Epoch 9538/20000 Training Loss: 0.054946403950452805\n",
      "Epoch 9539/20000 Training Loss: 0.04914774000644684\n",
      "Epoch 9540/20000 Training Loss: 0.04618288576602936\n",
      "Epoch 9540/20000 Validation Loss: 0.06083092838525772\n",
      "Epoch 9541/20000 Training Loss: 0.07389760762453079\n",
      "Epoch 9542/20000 Training Loss: 0.05996987596154213\n",
      "Epoch 9543/20000 Training Loss: 0.0711304172873497\n",
      "Epoch 9544/20000 Training Loss: 0.059757184237241745\n",
      "Epoch 9545/20000 Training Loss: 0.07124368101358414\n",
      "Epoch 9546/20000 Training Loss: 0.03783684968948364\n",
      "Epoch 9547/20000 Training Loss: 0.06943364441394806\n",
      "Epoch 9548/20000 Training Loss: 0.07145106047391891\n",
      "Epoch 9549/20000 Training Loss: 0.05662834644317627\n",
      "Epoch 9550/20000 Training Loss: 0.04819193109869957\n",
      "Epoch 9550/20000 Validation Loss: 0.046450138092041016\n",
      "Epoch 9551/20000 Training Loss: 0.07118882238864899\n",
      "Epoch 9552/20000 Training Loss: 0.04487599804997444\n",
      "Epoch 9553/20000 Training Loss: 0.05315854772925377\n",
      "Epoch 9554/20000 Training Loss: 0.07452341169118881\n",
      "Epoch 9555/20000 Training Loss: 0.04067830741405487\n",
      "Epoch 9556/20000 Training Loss: 0.059879425913095474\n",
      "Epoch 9557/20000 Training Loss: 0.06334226578474045\n",
      "Epoch 9558/20000 Training Loss: 0.06329312175512314\n",
      "Epoch 9559/20000 Training Loss: 0.08386390656232834\n",
      "Epoch 9560/20000 Training Loss: 0.04860908165574074\n",
      "Epoch 9560/20000 Validation Loss: 0.044846151024103165\n",
      "Epoch 9561/20000 Training Loss: 0.04952364042401314\n",
      "Epoch 9562/20000 Training Loss: 0.05167054757475853\n",
      "Epoch 9563/20000 Training Loss: 0.051052868366241455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9564/20000 Training Loss: 0.04495188593864441\n",
      "Epoch 9565/20000 Training Loss: 0.056357383728027344\n",
      "Epoch 9566/20000 Training Loss: 0.0498536117374897\n",
      "Epoch 9567/20000 Training Loss: 0.04696568474173546\n",
      "Epoch 9568/20000 Training Loss: 0.05092870071530342\n",
      "Epoch 9569/20000 Training Loss: 0.06953144073486328\n",
      "Epoch 9570/20000 Training Loss: 0.07194503396749496\n",
      "Epoch 9570/20000 Validation Loss: 0.06117403879761696\n",
      "Epoch 9571/20000 Training Loss: 0.05613356828689575\n",
      "Epoch 9572/20000 Training Loss: 0.05355025455355644\n",
      "Epoch 9573/20000 Training Loss: 0.049535658210515976\n",
      "Epoch 9574/20000 Training Loss: 0.07366377860307693\n",
      "Epoch 9575/20000 Training Loss: 0.05262986943125725\n",
      "Epoch 9576/20000 Training Loss: 0.049965281039476395\n",
      "Epoch 9577/20000 Training Loss: 0.06921876221895218\n",
      "Epoch 9578/20000 Training Loss: 0.0665687695145607\n",
      "Epoch 9579/20000 Training Loss: 0.052157554775476456\n",
      "Epoch 9580/20000 Training Loss: 0.05340932309627533\n",
      "Epoch 9580/20000 Validation Loss: 0.038979269564151764\n",
      "Epoch 9581/20000 Training Loss: 0.04654949530959129\n",
      "Epoch 9582/20000 Training Loss: 0.05370330438017845\n",
      "Epoch 9583/20000 Training Loss: 0.0589248389005661\n",
      "Epoch 9584/20000 Training Loss: 0.05370455980300903\n",
      "Epoch 9585/20000 Training Loss: 0.04535643756389618\n",
      "Epoch 9586/20000 Training Loss: 0.05758607015013695\n",
      "Epoch 9587/20000 Training Loss: 0.0447387658059597\n",
      "Epoch 9588/20000 Training Loss: 0.06834737211465836\n",
      "Epoch 9589/20000 Training Loss: 0.07004047185182571\n",
      "Epoch 9590/20000 Training Loss: 0.06589755415916443\n",
      "Epoch 9590/20000 Validation Loss: 0.04891321808099747\n",
      "Epoch 9591/20000 Training Loss: 0.05211678519845009\n",
      "Epoch 9592/20000 Training Loss: 0.05849927291274071\n",
      "Epoch 9593/20000 Training Loss: 0.058998167514801025\n",
      "Epoch 9594/20000 Training Loss: 0.04598987102508545\n",
      "Epoch 9595/20000 Training Loss: 0.034175388514995575\n",
      "Epoch 9596/20000 Training Loss: 0.04707974195480347\n",
      "Epoch 9597/20000 Training Loss: 0.07140742987394333\n",
      "Epoch 9598/20000 Training Loss: 0.05371649190783501\n",
      "Epoch 9599/20000 Training Loss: 0.06046252325177193\n",
      "Epoch 9600/20000 Training Loss: 0.049441058188676834\n",
      "Epoch 9600/20000 Validation Loss: 0.050388216972351074\n",
      "Epoch 9601/20000 Training Loss: 0.07123758643865585\n",
      "Epoch 9602/20000 Training Loss: 0.029980987310409546\n",
      "Epoch 9603/20000 Training Loss: 0.05777842923998833\n",
      "Epoch 9604/20000 Training Loss: 0.049536075443029404\n",
      "Epoch 9605/20000 Training Loss: 0.05599553510546684\n",
      "Epoch 9606/20000 Training Loss: 0.061143647879362106\n",
      "Epoch 9607/20000 Training Loss: 0.05046012997627258\n",
      "Epoch 9608/20000 Training Loss: 0.056854236871004105\n",
      "Epoch 9609/20000 Training Loss: 0.06405595690011978\n",
      "Epoch 9610/20000 Training Loss: 0.043933670967817307\n",
      "Epoch 9610/20000 Validation Loss: 0.07935772836208344\n",
      "Epoch 9611/20000 Training Loss: 0.06568897515535355\n",
      "Epoch 9612/20000 Training Loss: 0.06300318241119385\n",
      "Epoch 9613/20000 Training Loss: 0.06351879984140396\n",
      "Epoch 9614/20000 Training Loss: 0.0458814799785614\n",
      "Epoch 9615/20000 Training Loss: 0.04245682433247566\n",
      "Epoch 9616/20000 Training Loss: 0.06325257569551468\n",
      "Epoch 9617/20000 Training Loss: 0.06983765214681625\n",
      "Epoch 9618/20000 Training Loss: 0.041996683925390244\n",
      "Epoch 9619/20000 Training Loss: 0.05494637414813042\n",
      "Epoch 9620/20000 Training Loss: 0.04726756736636162\n",
      "Epoch 9620/20000 Validation Loss: 0.060514338314533234\n",
      "Epoch 9621/20000 Training Loss: 0.05343906953930855\n",
      "Epoch 9622/20000 Training Loss: 0.042593855410814285\n",
      "Epoch 9623/20000 Training Loss: 0.05579870939254761\n",
      "Epoch 9624/20000 Training Loss: 0.051115065813064575\n",
      "Epoch 9625/20000 Training Loss: 0.06538303941488266\n",
      "Epoch 9626/20000 Training Loss: 0.06260952353477478\n",
      "Epoch 9627/20000 Training Loss: 0.05549151077866554\n",
      "Epoch 9628/20000 Training Loss: 0.06007349491119385\n",
      "Epoch 9629/20000 Training Loss: 0.07277591526508331\n",
      "Epoch 9630/20000 Training Loss: 0.04197586700320244\n",
      "Epoch 9630/20000 Validation Loss: 0.04458478093147278\n",
      "Epoch 9631/20000 Training Loss: 0.07006806880235672\n",
      "Epoch 9632/20000 Training Loss: 0.06038809195160866\n",
      "Epoch 9633/20000 Training Loss: 0.06410378217697144\n",
      "Epoch 9634/20000 Training Loss: 0.06627219170331955\n",
      "Epoch 9635/20000 Training Loss: 0.04230588674545288\n",
      "Epoch 9636/20000 Training Loss: 0.0561981201171875\n",
      "Epoch 9637/20000 Training Loss: 0.04892922565340996\n",
      "Epoch 9638/20000 Training Loss: 0.05525486543774605\n",
      "Epoch 9639/20000 Training Loss: 0.05217387154698372\n",
      "Epoch 9640/20000 Training Loss: 0.061628758907318115\n",
      "Epoch 9640/20000 Validation Loss: 0.061761535704135895\n",
      "Epoch 9641/20000 Training Loss: 0.05401618406176567\n",
      "Epoch 9642/20000 Training Loss: 0.04386994242668152\n",
      "Epoch 9643/20000 Training Loss: 0.04209581017494202\n",
      "Epoch 9644/20000 Training Loss: 0.06296303868293762\n",
      "Epoch 9645/20000 Training Loss: 0.055787473917007446\n",
      "Epoch 9646/20000 Training Loss: 0.04765653610229492\n",
      "Epoch 9647/20000 Training Loss: 0.06460068374872208\n",
      "Epoch 9648/20000 Training Loss: 0.054562777280807495\n",
      "Epoch 9649/20000 Training Loss: 0.04309941828250885\n",
      "Epoch 9650/20000 Training Loss: 0.06941758841276169\n",
      "Epoch 9650/20000 Validation Loss: 0.05669713020324707\n",
      "Epoch 9651/20000 Training Loss: 0.04855731502175331\n",
      "Epoch 9652/20000 Training Loss: 0.055496856570243835\n",
      "Epoch 9653/20000 Training Loss: 0.06337827444076538\n",
      "Epoch 9654/20000 Training Loss: 0.04746332764625549\n",
      "Epoch 9655/20000 Training Loss: 0.06340011954307556\n",
      "Epoch 9656/20000 Training Loss: 0.05848648026585579\n",
      "Epoch 9657/20000 Training Loss: 0.053713858127593994\n",
      "Epoch 9658/20000 Training Loss: 0.05110590532422066\n",
      "Epoch 9659/20000 Training Loss: 0.05717973783612251\n",
      "Epoch 9660/20000 Training Loss: 0.04002757370471954\n",
      "Epoch 9660/20000 Validation Loss: 0.07216782867908478\n",
      "Epoch 9661/20000 Training Loss: 0.04809613153338432\n",
      "Epoch 9662/20000 Training Loss: 0.07611158490180969\n",
      "Epoch 9663/20000 Training Loss: 0.05513494089245796\n",
      "Epoch 9664/20000 Training Loss: 0.052152011543512344\n",
      "Epoch 9665/20000 Training Loss: 0.058577995747327805\n",
      "Epoch 9666/20000 Training Loss: 0.07677555829286575\n",
      "Epoch 9667/20000 Training Loss: 0.05941072106361389\n",
      "Epoch 9668/20000 Training Loss: 0.06198190525174141\n",
      "Epoch 9669/20000 Training Loss: 0.053004905581474304\n",
      "Epoch 9670/20000 Training Loss: 0.05323418974876404\n",
      "Epoch 9670/20000 Validation Loss: 0.05047145113348961\n",
      "Epoch 9671/20000 Training Loss: 0.04858919978141785\n",
      "Epoch 9672/20000 Training Loss: 0.04920877888798714\n",
      "Epoch 9673/20000 Training Loss: 0.05277848616242409\n",
      "Epoch 9674/20000 Training Loss: 0.04855385795235634\n",
      "Epoch 9675/20000 Training Loss: 0.06505049020051956\n",
      "Epoch 9676/20000 Training Loss: 0.0594257228076458\n",
      "Epoch 9677/20000 Training Loss: 0.06760887056589127\n",
      "Epoch 9678/20000 Training Loss: 0.05422671511769295\n",
      "Epoch 9679/20000 Training Loss: 0.05732420086860657\n",
      "Epoch 9680/20000 Training Loss: 0.0688982680439949\n",
      "Epoch 9680/20000 Validation Loss: 0.06503485143184662\n",
      "Epoch 9681/20000 Training Loss: 0.06569378823041916\n",
      "Epoch 9682/20000 Training Loss: 0.05224210396409035\n",
      "Epoch 9683/20000 Training Loss: 0.06305069476366043\n",
      "Epoch 9684/20000 Training Loss: 0.049536824226379395\n",
      "Epoch 9685/20000 Training Loss: 0.057991016656160355\n",
      "Epoch 9686/20000 Training Loss: 0.059279244393110275\n",
      "Epoch 9687/20000 Training Loss: 0.0663801059126854\n",
      "Epoch 9688/20000 Training Loss: 0.0575467087328434\n",
      "Epoch 9689/20000 Training Loss: 0.05979873239994049\n",
      "Epoch 9690/20000 Training Loss: 0.06391183286905289\n",
      "Epoch 9690/20000 Validation Loss: 0.06266965717077255\n",
      "Epoch 9691/20000 Training Loss: 0.06835023313760757\n",
      "Epoch 9692/20000 Training Loss: 0.08391067385673523\n",
      "Epoch 9693/20000 Training Loss: 0.054659415036439896\n",
      "Epoch 9694/20000 Training Loss: 0.057840242981910706\n",
      "Epoch 9695/20000 Training Loss: 0.05796641483902931\n",
      "Epoch 9696/20000 Training Loss: 0.07208169251680374\n",
      "Epoch 9697/20000 Training Loss: 0.06104602292180061\n",
      "Epoch 9698/20000 Training Loss: 0.061521172523498535\n",
      "Epoch 9699/20000 Training Loss: 0.05399861931800842\n",
      "Epoch 9700/20000 Training Loss: 0.04221991077065468\n",
      "Epoch 9700/20000 Validation Loss: 0.0717763602733612\n",
      "Epoch 9701/20000 Training Loss: 0.057357948273420334\n",
      "Epoch 9702/20000 Training Loss: 0.06589586287736893\n",
      "Epoch 9703/20000 Training Loss: 0.047111157327890396\n",
      "Epoch 9704/20000 Training Loss: 0.06245778873562813\n",
      "Epoch 9705/20000 Training Loss: 0.04756985604763031\n",
      "Epoch 9706/20000 Training Loss: 0.05615408718585968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9707/20000 Training Loss: 0.053850263357162476\n",
      "Epoch 9708/20000 Training Loss: 0.055213332176208496\n",
      "Epoch 9709/20000 Training Loss: 0.0646064504981041\n",
      "Epoch 9710/20000 Training Loss: 0.06654488295316696\n",
      "Epoch 9710/20000 Validation Loss: 0.03773491084575653\n",
      "Epoch 9711/20000 Training Loss: 0.046967923641204834\n",
      "Epoch 9712/20000 Training Loss: 0.04336147382855415\n",
      "Epoch 9713/20000 Training Loss: 0.0523344911634922\n",
      "Epoch 9714/20000 Training Loss: 0.07302003353834152\n",
      "Epoch 9715/20000 Training Loss: 0.043628185987472534\n",
      "Epoch 9716/20000 Training Loss: 0.052000705152750015\n",
      "Epoch 9717/20000 Training Loss: 0.045472919940948486\n",
      "Epoch 9718/20000 Training Loss: 0.045788705348968506\n",
      "Epoch 9719/20000 Training Loss: 0.03767664358019829\n",
      "Epoch 9720/20000 Training Loss: 0.051611751317977905\n",
      "Epoch 9720/20000 Validation Loss: 0.05144604295492172\n",
      "Epoch 9721/20000 Training Loss: 0.07431387901306152\n",
      "Epoch 9722/20000 Training Loss: 0.055069178342819214\n",
      "Epoch 9723/20000 Training Loss: 0.04379018023610115\n",
      "Epoch 9724/20000 Training Loss: 0.04799557849764824\n",
      "Epoch 9725/20000 Training Loss: 0.039838094264268875\n",
      "Epoch 9726/20000 Training Loss: 0.04110311344265938\n",
      "Epoch 9727/20000 Training Loss: 0.05026881769299507\n",
      "Epoch 9728/20000 Training Loss: 0.03843890503048897\n",
      "Epoch 9729/20000 Training Loss: 0.04663700982928276\n",
      "Epoch 9730/20000 Training Loss: 0.051629438996315\n",
      "Epoch 9730/20000 Validation Loss: 0.07157735526561737\n",
      "Epoch 9731/20000 Training Loss: 0.05946876481175423\n",
      "Epoch 9732/20000 Training Loss: 0.061749279499053955\n",
      "Epoch 9733/20000 Training Loss: 0.04434480890631676\n",
      "Epoch 9734/20000 Training Loss: 0.06747811287641525\n",
      "Epoch 9735/20000 Training Loss: 0.050945669412612915\n",
      "Epoch 9736/20000 Training Loss: 0.04338937625288963\n",
      "Epoch 9737/20000 Training Loss: 0.04719804227352142\n",
      "Epoch 9738/20000 Training Loss: 0.06041650474071503\n",
      "Epoch 9739/20000 Training Loss: 0.04331009462475777\n",
      "Epoch 9740/20000 Training Loss: 0.04391169920563698\n",
      "Epoch 9740/20000 Validation Loss: 0.03576686978340149\n",
      "Epoch 9741/20000 Training Loss: 0.04733084514737129\n",
      "Epoch 9742/20000 Training Loss: 0.05948212370276451\n",
      "Epoch 9743/20000 Training Loss: 0.04689643904566765\n",
      "Epoch 9744/20000 Training Loss: 0.0514766126871109\n",
      "Epoch 9745/20000 Training Loss: 0.07597390562295914\n",
      "Epoch 9746/20000 Training Loss: 0.05913439765572548\n",
      "Epoch 9747/20000 Training Loss: 0.05724826082587242\n",
      "Epoch 9748/20000 Training Loss: 0.038725707679986954\n",
      "Epoch 9749/20000 Training Loss: 0.06318064779043198\n",
      "Epoch 9750/20000 Training Loss: 0.06520373374223709\n",
      "Epoch 9750/20000 Validation Loss: 0.06654544174671173\n",
      "Epoch 9751/20000 Training Loss: 0.06236708164215088\n",
      "Epoch 9752/20000 Training Loss: 0.053089629858732224\n",
      "Epoch 9753/20000 Training Loss: 0.04744859039783478\n",
      "Epoch 9754/20000 Training Loss: 0.05120784416794777\n",
      "Epoch 9755/20000 Training Loss: 0.05291581153869629\n",
      "Epoch 9756/20000 Training Loss: 0.0649743527173996\n",
      "Epoch 9757/20000 Training Loss: 0.050426289439201355\n",
      "Epoch 9758/20000 Training Loss: 0.04736624285578728\n",
      "Epoch 9759/20000 Training Loss: 0.04710550978779793\n",
      "Epoch 9760/20000 Training Loss: 0.04496261477470398\n",
      "Epoch 9760/20000 Validation Loss: 0.06205102056264877\n",
      "Epoch 9761/20000 Training Loss: 0.08159434795379639\n",
      "Epoch 9762/20000 Training Loss: 0.057096391916275024\n",
      "Epoch 9763/20000 Training Loss: 0.05060840770602226\n",
      "Epoch 9764/20000 Training Loss: 0.057555850595235825\n",
      "Epoch 9765/20000 Training Loss: 0.0647616907954216\n",
      "Epoch 9766/20000 Training Loss: 0.04703986272215843\n",
      "Epoch 9767/20000 Training Loss: 0.05527133122086525\n",
      "Epoch 9768/20000 Training Loss: 0.0516071654856205\n",
      "Epoch 9769/20000 Training Loss: 0.059811610728502274\n",
      "Epoch 9770/20000 Training Loss: 0.06658057123422623\n",
      "Epoch 9770/20000 Validation Loss: 0.035404421389102936\n",
      "Epoch 9771/20000 Training Loss: 0.04997200146317482\n",
      "Epoch 9772/20000 Training Loss: 0.06827683001756668\n",
      "Epoch 9773/20000 Training Loss: 0.04928407073020935\n",
      "Epoch 9774/20000 Training Loss: 0.06388028711080551\n",
      "Epoch 9775/20000 Training Loss: 0.045470308512449265\n",
      "Epoch 9776/20000 Training Loss: 0.05581514537334442\n",
      "Epoch 9777/20000 Training Loss: 0.05883267521858215\n",
      "Epoch 9778/20000 Training Loss: 0.04506838321685791\n",
      "Epoch 9779/20000 Training Loss: 0.06199479103088379\n",
      "Epoch 9780/20000 Training Loss: 0.06337016075849533\n",
      "Epoch 9780/20000 Validation Loss: 0.0608011819422245\n",
      "Epoch 9781/20000 Training Loss: 0.04570569097995758\n",
      "Epoch 9782/20000 Training Loss: 0.05431731417775154\n",
      "Epoch 9783/20000 Training Loss: 0.058633219450712204\n",
      "Epoch 9784/20000 Training Loss: 0.05573635175824165\n",
      "Epoch 9785/20000 Training Loss: 0.06529613584280014\n",
      "Epoch 9786/20000 Training Loss: 0.05477093532681465\n",
      "Epoch 9787/20000 Training Loss: 0.046849220991134644\n",
      "Epoch 9788/20000 Training Loss: 0.051115766167640686\n",
      "Epoch 9789/20000 Training Loss: 0.0539298951625824\n",
      "Epoch 9790/20000 Training Loss: 0.046718779951334\n",
      "Epoch 9790/20000 Validation Loss: 0.05502040311694145\n",
      "Epoch 9791/20000 Training Loss: 0.057409655302762985\n",
      "Epoch 9792/20000 Training Loss: 0.06622693687677383\n",
      "Epoch 9793/20000 Training Loss: 0.059313565492630005\n",
      "Epoch 9794/20000 Training Loss: 0.05362928286194801\n",
      "Epoch 9795/20000 Training Loss: 0.047175630927085876\n",
      "Epoch 9796/20000 Training Loss: 0.06060891970992088\n",
      "Epoch 9797/20000 Training Loss: 0.07392080873250961\n",
      "Epoch 9798/20000 Training Loss: 0.06071529909968376\n",
      "Epoch 9799/20000 Training Loss: 0.044077277183532715\n",
      "Epoch 9800/20000 Training Loss: 0.05006935074925423\n",
      "Epoch 9800/20000 Validation Loss: 0.04447748139500618\n",
      "Epoch 9801/20000 Training Loss: 0.06817280501127243\n",
      "Epoch 9802/20000 Training Loss: 0.05517745018005371\n",
      "Epoch 9803/20000 Training Loss: 0.05248098075389862\n",
      "Epoch 9804/20000 Training Loss: 0.06435517966747284\n",
      "Epoch 9805/20000 Training Loss: 0.05149589106440544\n",
      "Epoch 9806/20000 Training Loss: 0.05609889328479767\n",
      "Epoch 9807/20000 Training Loss: 0.057587701827287674\n",
      "Epoch 9808/20000 Training Loss: 0.05495328828692436\n",
      "Epoch 9809/20000 Training Loss: 0.07502951472997665\n",
      "Epoch 9810/20000 Training Loss: 0.054653629660606384\n",
      "Epoch 9810/20000 Validation Loss: 0.06634534895420074\n",
      "Epoch 9811/20000 Training Loss: 0.06581858545541763\n",
      "Epoch 9812/20000 Training Loss: 0.05947611853480339\n",
      "Epoch 9813/20000 Training Loss: 0.04351990297436714\n",
      "Epoch 9814/20000 Training Loss: 0.05532419681549072\n",
      "Epoch 9815/20000 Training Loss: 0.05166225507855415\n",
      "Epoch 9816/20000 Training Loss: 0.0702337697148323\n",
      "Epoch 9817/20000 Training Loss: 0.0646619200706482\n",
      "Epoch 9818/20000 Training Loss: 0.056583222001791\n",
      "Epoch 9819/20000 Training Loss: 0.07064871490001678\n",
      "Epoch 9820/20000 Training Loss: 0.04790006950497627\n",
      "Epoch 9820/20000 Validation Loss: 0.04834996908903122\n",
      "Epoch 9821/20000 Training Loss: 0.055203843861818314\n",
      "Epoch 9822/20000 Training Loss: 0.04014185070991516\n",
      "Epoch 9823/20000 Training Loss: 0.06502125412225723\n",
      "Epoch 9824/20000 Training Loss: 0.05992816761136055\n",
      "Epoch 9825/20000 Training Loss: 0.04497218132019043\n",
      "Epoch 9826/20000 Training Loss: 0.050203725695610046\n",
      "Epoch 9827/20000 Training Loss: 0.07974345237016678\n",
      "Epoch 9828/20000 Training Loss: 0.06843781471252441\n",
      "Epoch 9829/20000 Training Loss: 0.06177958846092224\n",
      "Epoch 9830/20000 Training Loss: 0.04832860827445984\n",
      "Epoch 9830/20000 Validation Loss: 0.060530297458171844\n",
      "Epoch 9831/20000 Training Loss: 0.044539403170347214\n",
      "Epoch 9832/20000 Training Loss: 0.059734661132097244\n",
      "Epoch 9833/20000 Training Loss: 0.05482276901602745\n",
      "Epoch 9834/20000 Training Loss: 0.046163152903318405\n",
      "Epoch 9835/20000 Training Loss: 0.05102095007896423\n",
      "Epoch 9836/20000 Training Loss: 0.05109788849949837\n",
      "Epoch 9837/20000 Training Loss: 0.058444101363420486\n",
      "Epoch 9838/20000 Training Loss: 0.052791137248277664\n",
      "Epoch 9839/20000 Training Loss: 0.06854433566331863\n",
      "Epoch 9840/20000 Training Loss: 0.05993933603167534\n",
      "Epoch 9840/20000 Validation Loss: 0.06772509962320328\n",
      "Epoch 9841/20000 Training Loss: 0.06182627007365227\n",
      "Epoch 9842/20000 Training Loss: 0.04264995455741882\n",
      "Epoch 9843/20000 Training Loss: 0.05424325540661812\n",
      "Epoch 9844/20000 Training Loss: 0.04478823021054268\n",
      "Epoch 9845/20000 Training Loss: 0.06229855492711067\n",
      "Epoch 9846/20000 Training Loss: 0.05842779949307442\n",
      "Epoch 9847/20000 Training Loss: 0.05434180796146393\n",
      "Epoch 9848/20000 Training Loss: 0.0661432147026062\n",
      "Epoch 9849/20000 Training Loss: 0.051888804882764816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9850/20000 Training Loss: 0.06395386904478073\n",
      "Epoch 9850/20000 Validation Loss: 0.070502370595932\n",
      "Epoch 9851/20000 Training Loss: 0.04775358363986015\n",
      "Epoch 9852/20000 Training Loss: 0.04632498696446419\n",
      "Epoch 9853/20000 Training Loss: 0.06057867035269737\n",
      "Epoch 9854/20000 Training Loss: 0.05561843141913414\n",
      "Epoch 9855/20000 Training Loss: 0.05136415734887123\n",
      "Epoch 9856/20000 Training Loss: 0.06262090802192688\n",
      "Epoch 9857/20000 Training Loss: 0.07181238383054733\n",
      "Epoch 9858/20000 Training Loss: 0.0715576633810997\n",
      "Epoch 9859/20000 Training Loss: 0.07109334319829941\n",
      "Epoch 9860/20000 Training Loss: 0.05495031550526619\n",
      "Epoch 9860/20000 Validation Loss: 0.05500123277306557\n",
      "Epoch 9861/20000 Training Loss: 0.06330046057701111\n",
      "Epoch 9862/20000 Training Loss: 0.04563068971037865\n",
      "Epoch 9863/20000 Training Loss: 0.05071970820426941\n",
      "Epoch 9864/20000 Training Loss: 0.03242430463433266\n",
      "Epoch 9865/20000 Training Loss: 0.07683639973402023\n",
      "Epoch 9866/20000 Training Loss: 0.0442911833524704\n",
      "Epoch 9867/20000 Training Loss: 0.06595911830663681\n",
      "Epoch 9868/20000 Training Loss: 0.06415901333093643\n",
      "Epoch 9869/20000 Training Loss: 0.05673101544380188\n",
      "Epoch 9870/20000 Training Loss: 0.050756681710481644\n",
      "Epoch 9870/20000 Validation Loss: 0.0669705867767334\n",
      "Epoch 9871/20000 Training Loss: 0.04598076269030571\n",
      "Epoch 9872/20000 Training Loss: 0.05202033743262291\n",
      "Epoch 9873/20000 Training Loss: 0.05456158518791199\n",
      "Epoch 9874/20000 Training Loss: 0.048263877630233765\n",
      "Epoch 9875/20000 Training Loss: 0.054964929819107056\n",
      "Epoch 9876/20000 Training Loss: 0.05836630240082741\n",
      "Epoch 9877/20000 Training Loss: 0.056313883513212204\n",
      "Epoch 9878/20000 Training Loss: 0.07076521217823029\n",
      "Epoch 9879/20000 Training Loss: 0.048882901668548584\n",
      "Epoch 9880/20000 Training Loss: 0.04513222351670265\n",
      "Epoch 9880/20000 Validation Loss: 0.06013798713684082\n",
      "Epoch 9881/20000 Training Loss: 0.06019768491387367\n",
      "Epoch 9882/20000 Training Loss: 0.05006393417716026\n",
      "Epoch 9883/20000 Training Loss: 0.05796274542808533\n",
      "Epoch 9884/20000 Training Loss: 0.0591745488345623\n",
      "Epoch 9885/20000 Training Loss: 0.044110726565122604\n",
      "Epoch 9886/20000 Training Loss: 0.05776480957865715\n",
      "Epoch 9887/20000 Training Loss: 0.05456269904971123\n",
      "Epoch 9888/20000 Training Loss: 0.0736754834651947\n",
      "Epoch 9889/20000 Training Loss: 0.03815161809325218\n",
      "Epoch 9890/20000 Training Loss: 0.04422930255532265\n",
      "Epoch 9890/20000 Validation Loss: 0.05728304386138916\n",
      "Epoch 9891/20000 Training Loss: 0.05593293905258179\n",
      "Epoch 9892/20000 Training Loss: 0.06688306480646133\n",
      "Epoch 9893/20000 Training Loss: 0.027244776487350464\n",
      "Epoch 9894/20000 Training Loss: 0.058601196855306625\n",
      "Epoch 9895/20000 Training Loss: 0.04129102826118469\n",
      "Epoch 9896/20000 Training Loss: 0.06070099398493767\n",
      "Epoch 9897/20000 Training Loss: 0.06113361194729805\n",
      "Epoch 9898/20000 Training Loss: 0.04611687734723091\n",
      "Epoch 9899/20000 Training Loss: 0.08142608404159546\n",
      "Epoch 9900/20000 Training Loss: 0.05805693939328194\n",
      "Epoch 9900/20000 Validation Loss: 0.05192413926124573\n",
      "Epoch 9901/20000 Training Loss: 0.042425867170095444\n",
      "Epoch 9902/20000 Training Loss: 0.04453612491488457\n",
      "Epoch 9903/20000 Training Loss: 0.05954104661941528\n",
      "Epoch 9904/20000 Training Loss: 0.046097446233034134\n",
      "Epoch 9905/20000 Training Loss: 0.06284159421920776\n",
      "Epoch 9906/20000 Training Loss: 0.07273385673761368\n",
      "Epoch 9907/20000 Training Loss: 0.045552629977464676\n",
      "Epoch 9908/20000 Training Loss: 0.05255082622170448\n",
      "Epoch 9909/20000 Training Loss: 0.054496001452207565\n",
      "Epoch 9910/20000 Training Loss: 0.06888491660356522\n",
      "Epoch 9910/20000 Validation Loss: 0.0657193511724472\n",
      "Epoch 9911/20000 Training Loss: 0.06242574378848076\n",
      "Epoch 9912/20000 Training Loss: 0.06218433380126953\n",
      "Epoch 9913/20000 Training Loss: 0.04765453562140465\n",
      "Epoch 9914/20000 Training Loss: 0.05811511352658272\n",
      "Epoch 9915/20000 Training Loss: 0.045318394899368286\n",
      "Epoch 9916/20000 Training Loss: 0.0508982390165329\n",
      "Epoch 9917/20000 Training Loss: 0.05452762544155121\n",
      "Epoch 9918/20000 Training Loss: 0.053947120904922485\n",
      "Epoch 9919/20000 Training Loss: 0.04488491639494896\n",
      "Epoch 9920/20000 Training Loss: 0.03617890924215317\n",
      "Epoch 9920/20000 Validation Loss: 0.053926918655633926\n",
      "Epoch 9921/20000 Training Loss: 0.06272079795598984\n",
      "Epoch 9922/20000 Training Loss: 0.054604753851890564\n",
      "Epoch 9923/20000 Training Loss: 0.05716342851519585\n",
      "Epoch 9924/20000 Training Loss: 0.06799066066741943\n",
      "Epoch 9925/20000 Training Loss: 0.05416214466094971\n",
      "Epoch 9926/20000 Training Loss: 0.06194070354104042\n",
      "Epoch 9927/20000 Training Loss: 0.04626705124974251\n",
      "Epoch 9928/20000 Training Loss: 0.050312455743551254\n",
      "Epoch 9929/20000 Training Loss: 0.05948290601372719\n",
      "Epoch 9930/20000 Training Loss: 0.03766278922557831\n",
      "Epoch 9930/20000 Validation Loss: 0.05948346108198166\n",
      "Epoch 9931/20000 Training Loss: 0.051026683300733566\n",
      "Epoch 9932/20000 Training Loss: 0.07124128937721252\n",
      "Epoch 9933/20000 Training Loss: 0.035821396857500076\n",
      "Epoch 9934/20000 Training Loss: 0.03930143639445305\n",
      "Epoch 9935/20000 Training Loss: 0.05315159261226654\n",
      "Epoch 9936/20000 Training Loss: 0.04888841509819031\n",
      "Epoch 9937/20000 Training Loss: 0.044332440942525864\n",
      "Epoch 9938/20000 Training Loss: 0.04268249496817589\n",
      "Epoch 9939/20000 Training Loss: 0.06015809252858162\n",
      "Epoch 9940/20000 Training Loss: 0.0487813800573349\n",
      "Epoch 9940/20000 Validation Loss: 0.05224936455488205\n",
      "Epoch 9941/20000 Training Loss: 0.05602799728512764\n",
      "Epoch 9942/20000 Training Loss: 0.07446187734603882\n",
      "Epoch 9943/20000 Training Loss: 0.05143412575125694\n",
      "Epoch 9944/20000 Training Loss: 0.04956798627972603\n",
      "Epoch 9945/20000 Training Loss: 0.05290241166949272\n",
      "Epoch 9946/20000 Training Loss: 0.05561220645904541\n",
      "Epoch 9947/20000 Training Loss: 0.04792983457446098\n",
      "Epoch 9948/20000 Training Loss: 0.0567203164100647\n",
      "Epoch 9949/20000 Training Loss: 0.04974102973937988\n",
      "Epoch 9950/20000 Training Loss: 0.056415025144815445\n",
      "Epoch 9950/20000 Validation Loss: 0.03518509119749069\n",
      "Epoch 9951/20000 Training Loss: 0.0634741485118866\n",
      "Epoch 9952/20000 Training Loss: 0.05047495663166046\n",
      "Epoch 9953/20000 Training Loss: 0.047772154211997986\n",
      "Epoch 9954/20000 Training Loss: 0.04077081382274628\n",
      "Epoch 9955/20000 Training Loss: 0.03976002708077431\n",
      "Epoch 9956/20000 Training Loss: 0.05252551659941673\n",
      "Epoch 9957/20000 Training Loss: 0.06832589954137802\n",
      "Epoch 9958/20000 Training Loss: 0.05162471532821655\n",
      "Epoch 9959/20000 Training Loss: 0.0783877968788147\n",
      "Epoch 9960/20000 Training Loss: 0.04261571168899536\n",
      "Epoch 9960/20000 Validation Loss: 0.04951854422688484\n",
      "Epoch 9961/20000 Training Loss: 0.06772657483816147\n",
      "Epoch 9962/20000 Training Loss: 0.05451167747378349\n",
      "Epoch 9963/20000 Training Loss: 0.05901840329170227\n",
      "Epoch 9964/20000 Training Loss: 0.05918754264712334\n",
      "Epoch 9965/20000 Training Loss: 0.07570982724428177\n",
      "Epoch 9966/20000 Training Loss: 0.06378694623708725\n",
      "Epoch 9967/20000 Training Loss: 0.051811203360557556\n",
      "Epoch 9968/20000 Training Loss: 0.054794345051050186\n",
      "Epoch 9969/20000 Training Loss: 0.04191404581069946\n",
      "Epoch 9970/20000 Training Loss: 0.056752994656562805\n",
      "Epoch 9970/20000 Validation Loss: 0.08103407174348831\n",
      "Epoch 9971/20000 Training Loss: 0.06934512406587601\n",
      "Epoch 9972/20000 Training Loss: 0.049427226185798645\n",
      "Epoch 9973/20000 Training Loss: 0.04497766122221947\n",
      "Epoch 9974/20000 Training Loss: 0.062071871012449265\n",
      "Epoch 9975/20000 Training Loss: 0.04839671775698662\n",
      "Epoch 9976/20000 Training Loss: 0.035476796329021454\n",
      "Epoch 9977/20000 Training Loss: 0.059635017067193985\n",
      "Epoch 9978/20000 Training Loss: 0.04831072688102722\n",
      "Epoch 9979/20000 Training Loss: 0.03992975130677223\n",
      "Epoch 9980/20000 Training Loss: 0.06193395331501961\n",
      "Epoch 9980/20000 Validation Loss: 0.05751849710941315\n",
      "Epoch 9981/20000 Training Loss: 0.04872111603617668\n",
      "Epoch 9982/20000 Training Loss: 0.07056114822626114\n",
      "Epoch 9983/20000 Training Loss: 0.07094895094633102\n",
      "Epoch 9984/20000 Training Loss: 0.05177240073680878\n",
      "Epoch 9985/20000 Training Loss: 0.052448924630880356\n",
      "Epoch 9986/20000 Training Loss: 0.05853411555290222\n",
      "Epoch 9987/20000 Training Loss: 0.05238737165927887\n",
      "Epoch 9988/20000 Training Loss: 0.04428894445300102\n",
      "Epoch 9989/20000 Training Loss: 0.05496259406208992\n",
      "Epoch 9990/20000 Training Loss: 0.06808201223611832\n",
      "Epoch 9990/20000 Validation Loss: 0.059449732303619385\n",
      "Epoch 9991/20000 Training Loss: 0.041963886469602585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9992/20000 Training Loss: 0.047276318073272705\n",
      "Epoch 9993/20000 Training Loss: 0.05158733204007149\n",
      "Epoch 9994/20000 Training Loss: 0.04874235391616821\n",
      "Epoch 9995/20000 Training Loss: 0.07184723764657974\n",
      "Epoch 9996/20000 Training Loss: 0.05193255469202995\n",
      "Epoch 9997/20000 Training Loss: 0.052896905690431595\n",
      "Epoch 9998/20000 Training Loss: 0.0455755852162838\n",
      "Epoch 9999/20000 Training Loss: 0.04612037166953087\n",
      "Epoch 10000/20000 Training Loss: 0.061511192470788956\n",
      "Epoch 10000/20000 Validation Loss: 0.06984460353851318\n",
      "Epoch 10001/20000 Training Loss: 0.059606194496154785\n",
      "Epoch 10002/20000 Training Loss: 0.055767420679330826\n",
      "Epoch 10003/20000 Training Loss: 0.06567955762147903\n",
      "Epoch 10004/20000 Training Loss: 0.04612532630562782\n",
      "Epoch 10005/20000 Training Loss: 0.06779738515615463\n",
      "Epoch 10006/20000 Training Loss: 0.06408843398094177\n",
      "Epoch 10007/20000 Training Loss: 0.05642208456993103\n",
      "Epoch 10008/20000 Training Loss: 0.0710834264755249\n",
      "Epoch 10009/20000 Training Loss: 0.04825186729431152\n",
      "Epoch 10010/20000 Training Loss: 0.0443398542702198\n",
      "Epoch 10010/20000 Validation Loss: 0.06699640303850174\n",
      "Epoch 10011/20000 Training Loss: 0.04529927670955658\n",
      "Epoch 10012/20000 Training Loss: 0.055966172367334366\n",
      "Epoch 10013/20000 Training Loss: 0.048199232667684555\n",
      "Epoch 10014/20000 Training Loss: 0.05142321065068245\n",
      "Epoch 10015/20000 Training Loss: 0.044058769941329956\n",
      "Epoch 10016/20000 Training Loss: 0.060167279094457626\n",
      "Epoch 10017/20000 Training Loss: 0.048740070313215256\n",
      "Epoch 10018/20000 Training Loss: 0.05268586054444313\n",
      "Epoch 10019/20000 Training Loss: 0.060770127922296524\n",
      "Epoch 10020/20000 Training Loss: 0.04461473226547241\n",
      "Epoch 10020/20000 Validation Loss: 0.06439458578824997\n",
      "Epoch 10021/20000 Training Loss: 0.04429393634200096\n",
      "Epoch 10022/20000 Training Loss: 0.0681135430932045\n",
      "Epoch 10023/20000 Training Loss: 0.03800356760621071\n",
      "Epoch 10024/20000 Training Loss: 0.04730460047721863\n",
      "Epoch 10025/20000 Training Loss: 0.053222332149744034\n",
      "Epoch 10026/20000 Training Loss: 0.05807459354400635\n",
      "Epoch 10027/20000 Training Loss: 0.06459363549947739\n",
      "Epoch 10028/20000 Training Loss: 0.0695357397198677\n",
      "Epoch 10029/20000 Training Loss: 0.05342821776866913\n",
      "Epoch 10030/20000 Training Loss: 0.06463450193405151\n",
      "Epoch 10030/20000 Validation Loss: 0.04749077931046486\n",
      "Epoch 10031/20000 Training Loss: 0.06589621305465698\n",
      "Epoch 10032/20000 Training Loss: 0.060123126953840256\n",
      "Epoch 10033/20000 Training Loss: 0.06881674379110336\n",
      "Epoch 10034/20000 Training Loss: 0.04142019525170326\n",
      "Epoch 10035/20000 Training Loss: 0.050700634717941284\n",
      "Epoch 10036/20000 Training Loss: 0.048991356045007706\n",
      "Epoch 10037/20000 Training Loss: 0.04941822960972786\n",
      "Epoch 10038/20000 Training Loss: 0.046677205711603165\n",
      "Epoch 10039/20000 Training Loss: 0.06665680557489395\n",
      "Epoch 10040/20000 Training Loss: 0.05134395882487297\n",
      "Epoch 10040/20000 Validation Loss: 0.05442531406879425\n",
      "Epoch 10041/20000 Training Loss: 0.06840933114290237\n",
      "Epoch 10042/20000 Training Loss: 0.06940849870443344\n",
      "Epoch 10043/20000 Training Loss: 0.04491100832819939\n",
      "Epoch 10044/20000 Training Loss: 0.05503205955028534\n",
      "Epoch 10045/20000 Training Loss: 0.04825771972537041\n",
      "Epoch 10046/20000 Training Loss: 0.04387906193733215\n",
      "Epoch 10047/20000 Training Loss: 0.06516266614198685\n",
      "Epoch 10048/20000 Training Loss: 0.07369554042816162\n",
      "Epoch 10049/20000 Training Loss: 0.04389983415603638\n",
      "Epoch 10050/20000 Training Loss: 0.04373415187001228\n",
      "Epoch 10050/20000 Validation Loss: 0.04351691156625748\n",
      "Epoch 10051/20000 Training Loss: 0.04696885868906975\n",
      "Epoch 10052/20000 Training Loss: 0.057803403586149216\n",
      "Epoch 10053/20000 Training Loss: 0.0594857819378376\n",
      "Epoch 10054/20000 Training Loss: 0.04675687476992607\n",
      "Epoch 10055/20000 Training Loss: 0.0572252981364727\n",
      "Epoch 10056/20000 Training Loss: 0.04816422984004021\n",
      "Epoch 10057/20000 Training Loss: 0.04616423323750496\n",
      "Epoch 10058/20000 Training Loss: 0.05718449130654335\n",
      "Epoch 10059/20000 Training Loss: 0.05647076293826103\n",
      "Epoch 10060/20000 Training Loss: 0.06666219979524612\n",
      "Epoch 10060/20000 Validation Loss: 0.07268884778022766\n",
      "Epoch 10061/20000 Training Loss: 0.075490802526474\n",
      "Epoch 10062/20000 Training Loss: 0.051299046725034714\n",
      "Epoch 10063/20000 Training Loss: 0.06953238695859909\n",
      "Epoch 10064/20000 Training Loss: 0.050180595368146896\n",
      "Epoch 10065/20000 Training Loss: 0.06937986612319946\n",
      "Epoch 10066/20000 Training Loss: 0.04739589989185333\n",
      "Epoch 10067/20000 Training Loss: 0.07582870870828629\n",
      "Epoch 10068/20000 Training Loss: 0.04137520119547844\n",
      "Epoch 10069/20000 Training Loss: 0.0483161024749279\n",
      "Epoch 10070/20000 Training Loss: 0.055407773703336716\n",
      "Epoch 10070/20000 Validation Loss: 0.08991405367851257\n",
      "Epoch 10071/20000 Training Loss: 0.06762536615133286\n",
      "Epoch 10072/20000 Training Loss: 0.05371745303273201\n",
      "Epoch 10073/20000 Training Loss: 0.04784207418560982\n",
      "Epoch 10074/20000 Training Loss: 0.05038835480809212\n",
      "Epoch 10075/20000 Training Loss: 0.049797460436820984\n",
      "Epoch 10076/20000 Training Loss: 0.04371241107583046\n",
      "Epoch 10077/20000 Training Loss: 0.06345806270837784\n",
      "Epoch 10078/20000 Training Loss: 0.05550902709364891\n",
      "Epoch 10079/20000 Training Loss: 0.06160872057080269\n",
      "Epoch 10080/20000 Training Loss: 0.06628098338842392\n",
      "Epoch 10080/20000 Validation Loss: 0.05754382163286209\n",
      "Epoch 10081/20000 Training Loss: 0.05452260002493858\n",
      "Epoch 10082/20000 Training Loss: 0.053117621690034866\n",
      "Epoch 10083/20000 Training Loss: 0.04973145201802254\n",
      "Epoch 10084/20000 Training Loss: 0.06874430179595947\n",
      "Epoch 10085/20000 Training Loss: 0.04982643201947212\n",
      "Epoch 10086/20000 Training Loss: 0.049141377210617065\n",
      "Epoch 10087/20000 Training Loss: 0.07841984182596207\n",
      "Epoch 10088/20000 Training Loss: 0.06266259402036667\n",
      "Epoch 10089/20000 Training Loss: 0.04704728350043297\n",
      "Epoch 10090/20000 Training Loss: 0.05212131142616272\n",
      "Epoch 10090/20000 Validation Loss: 0.049157533794641495\n",
      "Epoch 10091/20000 Training Loss: 0.03488609939813614\n",
      "Epoch 10092/20000 Training Loss: 0.07521241903305054\n",
      "Epoch 10093/20000 Training Loss: 0.07076045870780945\n",
      "Epoch 10094/20000 Training Loss: 0.044179338961839676\n",
      "Epoch 10095/20000 Training Loss: 0.05158701539039612\n",
      "Epoch 10096/20000 Training Loss: 0.04667091369628906\n",
      "Epoch 10097/20000 Training Loss: 0.05063558742403984\n",
      "Epoch 10098/20000 Training Loss: 0.05349411070346832\n",
      "Epoch 10099/20000 Training Loss: 0.07018745690584183\n",
      "Epoch 10100/20000 Training Loss: 0.05291594937443733\n",
      "Epoch 10100/20000 Validation Loss: 0.06259460747241974\n",
      "Epoch 10101/20000 Training Loss: 0.06522420793771744\n",
      "Epoch 10102/20000 Training Loss: 0.06477522850036621\n",
      "Epoch 10103/20000 Training Loss: 0.04324060678482056\n",
      "Epoch 10104/20000 Training Loss: 0.0564500130712986\n",
      "Epoch 10105/20000 Training Loss: 0.05917617678642273\n",
      "Epoch 10106/20000 Training Loss: 0.05480368435382843\n",
      "Epoch 10107/20000 Training Loss: 0.0544360876083374\n",
      "Epoch 10108/20000 Training Loss: 0.06615766137838364\n",
      "Epoch 10109/20000 Training Loss: 0.08125890791416168\n",
      "Epoch 10110/20000 Training Loss: 0.060857366770505905\n",
      "Epoch 10110/20000 Validation Loss: 0.05590851604938507\n",
      "Epoch 10111/20000 Training Loss: 0.05505142733454704\n",
      "Epoch 10112/20000 Training Loss: 0.046559300273656845\n",
      "Epoch 10113/20000 Training Loss: 0.04426082968711853\n",
      "Epoch 10114/20000 Training Loss: 0.044566888362169266\n",
      "Epoch 10115/20000 Training Loss: 0.04541236162185669\n",
      "Epoch 10116/20000 Training Loss: 0.05735083296895027\n",
      "Epoch 10117/20000 Training Loss: 0.06677953153848648\n",
      "Epoch 10118/20000 Training Loss: 0.08300313353538513\n",
      "Epoch 10119/20000 Training Loss: 0.04458777979016304\n",
      "Epoch 10120/20000 Training Loss: 0.06080958619713783\n",
      "Epoch 10120/20000 Validation Loss: 0.04871024191379547\n",
      "Epoch 10121/20000 Training Loss: 0.036367375403642654\n",
      "Epoch 10122/20000 Training Loss: 0.07337852567434311\n",
      "Epoch 10123/20000 Training Loss: 0.05145367980003357\n",
      "Epoch 10124/20000 Training Loss: 0.05164271593093872\n",
      "Epoch 10125/20000 Training Loss: 0.04627467319369316\n",
      "Epoch 10126/20000 Training Loss: 0.06687605381011963\n",
      "Epoch 10127/20000 Training Loss: 0.06734587997198105\n",
      "Epoch 10128/20000 Training Loss: 0.06822942942380905\n",
      "Epoch 10129/20000 Training Loss: 0.0627211183309555\n",
      "Epoch 10130/20000 Training Loss: 0.06696053594350815\n",
      "Epoch 10130/20000 Validation Loss: 0.06057506054639816\n",
      "Epoch 10131/20000 Training Loss: 0.053405359387397766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10132/20000 Training Loss: 0.06371083110570908\n",
      "Epoch 10133/20000 Training Loss: 0.047893259674310684\n",
      "Epoch 10134/20000 Training Loss: 0.06120891496539116\n",
      "Epoch 10135/20000 Training Loss: 0.06412232667207718\n",
      "Epoch 10136/20000 Training Loss: 0.05948979780077934\n",
      "Epoch 10137/20000 Training Loss: 0.057887643575668335\n",
      "Epoch 10138/20000 Training Loss: 0.0672292485833168\n",
      "Epoch 10139/20000 Training Loss: 0.05224991962313652\n",
      "Epoch 10140/20000 Training Loss: 0.05391305685043335\n",
      "Epoch 10140/20000 Validation Loss: 0.06568795442581177\n",
      "Epoch 10141/20000 Training Loss: 0.058656539767980576\n",
      "Epoch 10142/20000 Training Loss: 0.04922473058104515\n",
      "Epoch 10143/20000 Training Loss: 0.06908055394887924\n",
      "Epoch 10144/20000 Training Loss: 0.05329899117350578\n",
      "Epoch 10145/20000 Training Loss: 0.0705791711807251\n",
      "Epoch 10146/20000 Training Loss: 0.03397013247013092\n",
      "Epoch 10147/20000 Training Loss: 0.053596287965774536\n",
      "Epoch 10148/20000 Training Loss: 0.04828159138560295\n",
      "Epoch 10149/20000 Training Loss: 0.0636623203754425\n",
      "Epoch 10150/20000 Training Loss: 0.05014419928193092\n",
      "Epoch 10150/20000 Validation Loss: 0.07321064919233322\n",
      "Epoch 10151/20000 Training Loss: 0.06312885135412216\n",
      "Epoch 10152/20000 Training Loss: 0.051857609301805496\n",
      "Epoch 10153/20000 Training Loss: 0.05096988007426262\n",
      "Epoch 10154/20000 Training Loss: 0.05333337560296059\n",
      "Epoch 10155/20000 Training Loss: 0.04514378309249878\n",
      "Epoch 10156/20000 Training Loss: 0.055362362414598465\n",
      "Epoch 10157/20000 Training Loss: 0.050686534494161606\n",
      "Epoch 10158/20000 Training Loss: 0.06388246268033981\n",
      "Epoch 10159/20000 Training Loss: 0.04593348875641823\n",
      "Epoch 10160/20000 Training Loss: 0.050138670951128006\n",
      "Epoch 10160/20000 Validation Loss: 0.046913180500268936\n",
      "Epoch 10161/20000 Training Loss: 0.06452532857656479\n",
      "Epoch 10162/20000 Training Loss: 0.04976794123649597\n",
      "Epoch 10163/20000 Training Loss: 0.051041413098573685\n",
      "Epoch 10164/20000 Training Loss: 0.05179294943809509\n",
      "Epoch 10165/20000 Training Loss: 0.05867632105946541\n",
      "Epoch 10166/20000 Training Loss: 0.06587529927492142\n",
      "Epoch 10167/20000 Training Loss: 0.06814560294151306\n",
      "Epoch 10168/20000 Training Loss: 0.0596606470644474\n",
      "Epoch 10169/20000 Training Loss: 0.05676085129380226\n",
      "Epoch 10170/20000 Training Loss: 0.07094516605138779\n",
      "Epoch 10170/20000 Validation Loss: 0.0520065538585186\n",
      "Epoch 10171/20000 Training Loss: 0.060022030025720596\n",
      "Epoch 10172/20000 Training Loss: 0.05913228914141655\n",
      "Epoch 10173/20000 Training Loss: 0.0491357184946537\n",
      "Epoch 10174/20000 Training Loss: 0.059056609869003296\n",
      "Epoch 10175/20000 Training Loss: 0.05772465839982033\n",
      "Epoch 10176/20000 Training Loss: 0.03608342632651329\n",
      "Epoch 10177/20000 Training Loss: 0.05229882523417473\n",
      "Epoch 10178/20000 Training Loss: 0.05591215193271637\n",
      "Epoch 10179/20000 Training Loss: 0.049431998282670975\n",
      "Epoch 10180/20000 Training Loss: 0.058654412627220154\n",
      "Epoch 10180/20000 Validation Loss: 0.054460734128952026\n",
      "Epoch 10181/20000 Training Loss: 0.03944748267531395\n",
      "Epoch 10182/20000 Training Loss: 0.06953566521406174\n",
      "Epoch 10183/20000 Training Loss: 0.07309889048337936\n",
      "Epoch 10184/20000 Training Loss: 0.03240928798913956\n",
      "Epoch 10185/20000 Training Loss: 0.04801490530371666\n",
      "Epoch 10186/20000 Training Loss: 0.04876004159450531\n",
      "Epoch 10187/20000 Training Loss: 0.06260021775960922\n",
      "Epoch 10188/20000 Training Loss: 0.06693895906209946\n",
      "Epoch 10189/20000 Training Loss: 0.06046203896403313\n",
      "Epoch 10190/20000 Training Loss: 0.07597292214632034\n",
      "Epoch 10190/20000 Validation Loss: 0.08271307498216629\n",
      "Epoch 10191/20000 Training Loss: 0.06967347115278244\n",
      "Epoch 10192/20000 Training Loss: 0.04569678008556366\n",
      "Epoch 10193/20000 Training Loss: 0.047695815563201904\n",
      "Epoch 10194/20000 Training Loss: 0.06541216373443604\n",
      "Epoch 10195/20000 Training Loss: 0.07165765017271042\n",
      "Epoch 10196/20000 Training Loss: 0.06103641912341118\n",
      "Epoch 10197/20000 Training Loss: 0.05773862823843956\n",
      "Epoch 10198/20000 Training Loss: 0.043029144406318665\n",
      "Epoch 10199/20000 Training Loss: 0.05854533612728119\n",
      "Epoch 10200/20000 Training Loss: 0.05568837746977806\n",
      "Epoch 10200/20000 Validation Loss: 0.05596538633108139\n",
      "Epoch 10201/20000 Training Loss: 0.04959676042199135\n",
      "Epoch 10202/20000 Training Loss: 0.04381847381591797\n",
      "Epoch 10203/20000 Training Loss: 0.059020351618528366\n",
      "Epoch 10204/20000 Training Loss: 0.040089916437864304\n",
      "Epoch 10205/20000 Training Loss: 0.04533316567540169\n",
      "Epoch 10206/20000 Training Loss: 0.06253191083669662\n",
      "Epoch 10207/20000 Training Loss: 0.05359138920903206\n",
      "Epoch 10208/20000 Training Loss: 0.06914640218019485\n",
      "Epoch 10209/20000 Training Loss: 0.0488055944442749\n",
      "Epoch 10210/20000 Training Loss: 0.04687070846557617\n",
      "Epoch 10210/20000 Validation Loss: 0.03738062083721161\n",
      "Epoch 10211/20000 Training Loss: 0.04689935967326164\n",
      "Epoch 10212/20000 Training Loss: 0.057435542345047\n",
      "Epoch 10213/20000 Training Loss: 0.07369235157966614\n",
      "Epoch 10214/20000 Training Loss: 0.0714341402053833\n",
      "Epoch 10215/20000 Training Loss: 0.05313077196478844\n",
      "Epoch 10216/20000 Training Loss: 0.05450090765953064\n",
      "Epoch 10217/20000 Training Loss: 0.06275837868452072\n",
      "Epoch 10218/20000 Training Loss: 0.0740494504570961\n",
      "Epoch 10219/20000 Training Loss: 0.07113905996084213\n",
      "Epoch 10220/20000 Training Loss: 0.05313713476061821\n",
      "Epoch 10220/20000 Validation Loss: 0.06270452588796616\n",
      "Epoch 10221/20000 Training Loss: 0.05163067206740379\n",
      "Epoch 10222/20000 Training Loss: 0.0611678808927536\n",
      "Epoch 10223/20000 Training Loss: 0.05471854284405708\n",
      "Epoch 10224/20000 Training Loss: 0.03820398449897766\n",
      "Epoch 10225/20000 Training Loss: 0.06694073230028152\n",
      "Epoch 10226/20000 Training Loss: 0.04434119537472725\n",
      "Epoch 10227/20000 Training Loss: 0.06293892115354538\n",
      "Epoch 10228/20000 Training Loss: 0.06498011201620102\n",
      "Epoch 10229/20000 Training Loss: 0.052285972982645035\n",
      "Epoch 10230/20000 Training Loss: 0.05563032627105713\n",
      "Epoch 10230/20000 Validation Loss: 0.044923268258571625\n",
      "Epoch 10231/20000 Training Loss: 0.05240236595273018\n",
      "Epoch 10232/20000 Training Loss: 0.042791664600372314\n",
      "Epoch 10233/20000 Training Loss: 0.05105699971318245\n",
      "Epoch 10234/20000 Training Loss: 0.06959476321935654\n",
      "Epoch 10235/20000 Training Loss: 0.05473161116242409\n",
      "Epoch 10236/20000 Training Loss: 0.05262105539441109\n",
      "Epoch 10237/20000 Training Loss: 0.05963854864239693\n",
      "Epoch 10238/20000 Training Loss: 0.05571239814162254\n",
      "Epoch 10239/20000 Training Loss: 0.04282735660672188\n",
      "Epoch 10240/20000 Training Loss: 0.056585218757390976\n",
      "Epoch 10240/20000 Validation Loss: 0.06513132154941559\n",
      "Epoch 10241/20000 Training Loss: 0.057363420724868774\n",
      "Epoch 10242/20000 Training Loss: 0.04086700454354286\n",
      "Epoch 10243/20000 Training Loss: 0.05312329903244972\n",
      "Epoch 10244/20000 Training Loss: 0.05987965688109398\n",
      "Epoch 10245/20000 Training Loss: 0.056574273854494095\n",
      "Epoch 10246/20000 Training Loss: 0.053518399596214294\n",
      "Epoch 10247/20000 Training Loss: 0.06689544022083282\n",
      "Epoch 10248/20000 Training Loss: 0.05960080400109291\n",
      "Epoch 10249/20000 Training Loss: 0.06892915815114975\n",
      "Epoch 10250/20000 Training Loss: 0.06916790455579758\n",
      "Epoch 10250/20000 Validation Loss: 0.058344051241874695\n",
      "Epoch 10251/20000 Training Loss: 0.03829169273376465\n",
      "Epoch 10252/20000 Training Loss: 0.04995350167155266\n",
      "Epoch 10253/20000 Training Loss: 0.0618726946413517\n",
      "Epoch 10254/20000 Training Loss: 0.06194736436009407\n",
      "Epoch 10255/20000 Training Loss: 0.06525986641645432\n",
      "Epoch 10256/20000 Training Loss: 0.0701938197016716\n",
      "Epoch 10257/20000 Training Loss: 0.06167036294937134\n",
      "Epoch 10258/20000 Training Loss: 0.06816835701465607\n",
      "Epoch 10259/20000 Training Loss: 0.04789644479751587\n",
      "Epoch 10260/20000 Training Loss: 0.0733599066734314\n",
      "Epoch 10260/20000 Validation Loss: 0.06293579936027527\n",
      "Epoch 10261/20000 Training Loss: 0.0708632841706276\n",
      "Epoch 10262/20000 Training Loss: 0.05545339360833168\n",
      "Epoch 10263/20000 Training Loss: 0.05987576022744179\n",
      "Epoch 10264/20000 Training Loss: 0.05707969889044762\n",
      "Epoch 10265/20000 Training Loss: 0.059463437646627426\n",
      "Epoch 10266/20000 Training Loss: 0.0422922782599926\n",
      "Epoch 10267/20000 Training Loss: 0.0484900176525116\n",
      "Epoch 10268/20000 Training Loss: 0.06988117843866348\n",
      "Epoch 10269/20000 Training Loss: 0.04772139713168144\n",
      "Epoch 10270/20000 Training Loss: 0.06755214929580688\n",
      "Epoch 10270/20000 Validation Loss: 0.05189257860183716\n",
      "Epoch 10271/20000 Training Loss: 0.04364866390824318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10272/20000 Training Loss: 0.05678220093250275\n",
      "Epoch 10273/20000 Training Loss: 0.06276094913482666\n",
      "Epoch 10274/20000 Training Loss: 0.05493241921067238\n",
      "Epoch 10275/20000 Training Loss: 0.05991220101714134\n",
      "Epoch 10276/20000 Training Loss: 0.06719674170017242\n",
      "Epoch 10277/20000 Training Loss: 0.06436730921268463\n",
      "Epoch 10278/20000 Training Loss: 0.0613122396171093\n",
      "Epoch 10279/20000 Training Loss: 0.0538908950984478\n",
      "Epoch 10280/20000 Training Loss: 0.05679238215088844\n",
      "Epoch 10280/20000 Validation Loss: 0.0798964872956276\n",
      "Epoch 10281/20000 Training Loss: 0.06737072020769119\n",
      "Epoch 10282/20000 Training Loss: 0.04971224442124367\n",
      "Epoch 10283/20000 Training Loss: 0.055102620273828506\n",
      "Epoch 10284/20000 Training Loss: 0.05526787042617798\n",
      "Epoch 10285/20000 Training Loss: 0.06969966739416122\n",
      "Epoch 10286/20000 Training Loss: 0.05989677086472511\n",
      "Epoch 10287/20000 Training Loss: 0.05167349800467491\n",
      "Epoch 10288/20000 Training Loss: 0.06444833427667618\n",
      "Epoch 10289/20000 Training Loss: 0.056292954832315445\n",
      "Epoch 10290/20000 Training Loss: 0.05802937224507332\n",
      "Epoch 10290/20000 Validation Loss: 0.054870475083589554\n",
      "Epoch 10291/20000 Training Loss: 0.05724973604083061\n",
      "Epoch 10292/20000 Training Loss: 0.055985305458307266\n",
      "Epoch 10293/20000 Training Loss: 0.055374909192323685\n",
      "Epoch 10294/20000 Training Loss: 0.04844546318054199\n",
      "Epoch 10295/20000 Training Loss: 0.04309617355465889\n",
      "Epoch 10296/20000 Training Loss: 0.07502280920743942\n",
      "Epoch 10297/20000 Training Loss: 0.06825107336044312\n",
      "Epoch 10298/20000 Training Loss: 0.05318569019436836\n",
      "Epoch 10299/20000 Training Loss: 0.058013539761304855\n",
      "Epoch 10300/20000 Training Loss: 0.06258941441774368\n",
      "Epoch 10300/20000 Validation Loss: 0.0514986515045166\n",
      "Epoch 10301/20000 Training Loss: 0.04266316816210747\n",
      "Epoch 10302/20000 Training Loss: 0.049858853220939636\n",
      "Epoch 10303/20000 Training Loss: 0.04780994728207588\n",
      "Epoch 10304/20000 Training Loss: 0.05447062849998474\n",
      "Epoch 10305/20000 Training Loss: 0.0424199141561985\n",
      "Epoch 10306/20000 Training Loss: 0.06573722511529922\n",
      "Epoch 10307/20000 Training Loss: 0.043805986642837524\n",
      "Epoch 10308/20000 Training Loss: 0.07462000101804733\n",
      "Epoch 10309/20000 Training Loss: 0.04814532399177551\n",
      "Epoch 10310/20000 Training Loss: 0.07488032430410385\n",
      "Epoch 10310/20000 Validation Loss: 0.039947666227817535\n",
      "Epoch 10311/20000 Training Loss: 0.05399297550320625\n",
      "Epoch 10312/20000 Training Loss: 0.050018925219774246\n",
      "Epoch 10313/20000 Training Loss: 0.056280266493558884\n",
      "Epoch 10314/20000 Training Loss: 0.05174959823489189\n",
      "Epoch 10315/20000 Training Loss: 0.06666698306798935\n",
      "Epoch 10316/20000 Training Loss: 0.059876855462789536\n",
      "Epoch 10317/20000 Training Loss: 0.048996638506650925\n",
      "Epoch 10318/20000 Training Loss: 0.04772668704390526\n",
      "Epoch 10319/20000 Training Loss: 0.05717482045292854\n",
      "Epoch 10320/20000 Training Loss: 0.04086953401565552\n",
      "Epoch 10320/20000 Validation Loss: 0.07499141991138458\n",
      "Epoch 10321/20000 Training Loss: 0.05084724724292755\n",
      "Epoch 10322/20000 Training Loss: 0.061884745955467224\n",
      "Epoch 10323/20000 Training Loss: 0.05722329020500183\n",
      "Epoch 10324/20000 Training Loss: 0.05950568988919258\n",
      "Epoch 10325/20000 Training Loss: 0.04771117866039276\n",
      "Epoch 10326/20000 Training Loss: 0.05781431123614311\n",
      "Epoch 10327/20000 Training Loss: 0.06016691401600838\n",
      "Epoch 10328/20000 Training Loss: 0.04694073274731636\n",
      "Epoch 10329/20000 Training Loss: 0.05930953100323677\n",
      "Epoch 10330/20000 Training Loss: 0.055570777505636215\n",
      "Epoch 10330/20000 Validation Loss: 0.0848880410194397\n",
      "Epoch 10331/20000 Training Loss: 0.06258738040924072\n",
      "Epoch 10332/20000 Training Loss: 0.07772263884544373\n",
      "Epoch 10333/20000 Training Loss: 0.06677776575088501\n",
      "Epoch 10334/20000 Training Loss: 0.04135550558567047\n",
      "Epoch 10335/20000 Training Loss: 0.054187919944524765\n",
      "Epoch 10336/20000 Training Loss: 0.046227917075157166\n",
      "Epoch 10337/20000 Training Loss: 0.051528822630643845\n",
      "Epoch 10338/20000 Training Loss: 0.05402287840843201\n",
      "Epoch 10339/20000 Training Loss: 0.04024935141205788\n",
      "Epoch 10340/20000 Training Loss: 0.06486159563064575\n",
      "Epoch 10340/20000 Validation Loss: 0.05849786847829819\n",
      "Epoch 10341/20000 Training Loss: 0.048084333539009094\n",
      "Epoch 10342/20000 Training Loss: 0.03833197429776192\n",
      "Epoch 10343/20000 Training Loss: 0.05283832177519798\n",
      "Epoch 10344/20000 Training Loss: 0.06686805933713913\n",
      "Epoch 10345/20000 Training Loss: 0.05904744565486908\n",
      "Epoch 10346/20000 Training Loss: 0.04824267327785492\n",
      "Epoch 10347/20000 Training Loss: 0.07289039343595505\n",
      "Epoch 10348/20000 Training Loss: 0.05672615393996239\n",
      "Epoch 10349/20000 Training Loss: 0.05705489218235016\n",
      "Epoch 10350/20000 Training Loss: 0.05429063364863396\n",
      "Epoch 10350/20000 Validation Loss: 0.060864102095365524\n",
      "Epoch 10351/20000 Training Loss: 0.05641169473528862\n",
      "Epoch 10352/20000 Training Loss: 0.05145098641514778\n",
      "Epoch 10353/20000 Training Loss: 0.03968724608421326\n",
      "Epoch 10354/20000 Training Loss: 0.06035250797867775\n",
      "Epoch 10355/20000 Training Loss: 0.06073587015271187\n",
      "Epoch 10356/20000 Training Loss: 0.05052052065730095\n",
      "Epoch 10357/20000 Training Loss: 0.04305453598499298\n",
      "Epoch 10358/20000 Training Loss: 0.05530977249145508\n",
      "Epoch 10359/20000 Training Loss: 0.048378270119428635\n",
      "Epoch 10360/20000 Training Loss: 0.04911290481686592\n",
      "Epoch 10360/20000 Validation Loss: 0.05455249920487404\n",
      "Epoch 10361/20000 Training Loss: 0.06056525930762291\n",
      "Epoch 10362/20000 Training Loss: 0.06049227714538574\n",
      "Epoch 10363/20000 Training Loss: 0.051603373140096664\n",
      "Epoch 10364/20000 Training Loss: 0.06532648950815201\n",
      "Epoch 10365/20000 Training Loss: 0.04866824671626091\n",
      "Epoch 10366/20000 Training Loss: 0.052853960543870926\n",
      "Epoch 10367/20000 Training Loss: 0.0628223717212677\n",
      "Epoch 10368/20000 Training Loss: 0.04990518465638161\n",
      "Epoch 10369/20000 Training Loss: 0.045461639761924744\n",
      "Epoch 10370/20000 Training Loss: 0.0637868195772171\n",
      "Epoch 10370/20000 Validation Loss: 0.04430537670850754\n",
      "Epoch 10371/20000 Training Loss: 0.07118408381938934\n",
      "Epoch 10372/20000 Training Loss: 0.049358125776052475\n",
      "Epoch 10373/20000 Training Loss: 0.04642019793391228\n",
      "Epoch 10374/20000 Training Loss: 0.059952262789011\n",
      "Epoch 10375/20000 Training Loss: 0.06416790932416916\n",
      "Epoch 10376/20000 Training Loss: 0.04554681479930878\n",
      "Epoch 10377/20000 Training Loss: 0.05835682153701782\n",
      "Epoch 10378/20000 Training Loss: 0.05613595247268677\n",
      "Epoch 10379/20000 Training Loss: 0.04868312552571297\n",
      "Epoch 10380/20000 Training Loss: 0.06269843131303787\n",
      "Epoch 10380/20000 Validation Loss: 0.07841655611991882\n",
      "Epoch 10381/20000 Training Loss: 0.05629868805408478\n",
      "Epoch 10382/20000 Training Loss: 0.045901596546173096\n",
      "Epoch 10383/20000 Training Loss: 0.0604608990252018\n",
      "Epoch 10384/20000 Training Loss: 0.04539995267987251\n",
      "Epoch 10385/20000 Training Loss: 0.06484862416982651\n",
      "Epoch 10386/20000 Training Loss: 0.05162203311920166\n",
      "Epoch 10387/20000 Training Loss: 0.06400065124034882\n",
      "Epoch 10388/20000 Training Loss: 0.055411774665117264\n",
      "Epoch 10389/20000 Training Loss: 0.04369434714317322\n",
      "Epoch 10390/20000 Training Loss: 0.056326817721128464\n",
      "Epoch 10390/20000 Validation Loss: 0.0796700268983841\n",
      "Epoch 10391/20000 Training Loss: 0.04157398268580437\n",
      "Epoch 10392/20000 Training Loss: 0.04962271451950073\n",
      "Epoch 10393/20000 Training Loss: 0.06252461671829224\n",
      "Epoch 10394/20000 Training Loss: 0.046363767236471176\n",
      "Epoch 10395/20000 Training Loss: 0.07997029274702072\n",
      "Epoch 10396/20000 Training Loss: 0.05506013706326485\n",
      "Epoch 10397/20000 Training Loss: 0.05115729197859764\n",
      "Epoch 10398/20000 Training Loss: 0.061236847192049026\n",
      "Epoch 10399/20000 Training Loss: 0.05536262318491936\n",
      "Epoch 10400/20000 Training Loss: 0.06141570210456848\n",
      "Epoch 10400/20000 Validation Loss: 0.0653023049235344\n",
      "Epoch 10401/20000 Training Loss: 0.06428642570972443\n",
      "Epoch 10402/20000 Training Loss: 0.047495707869529724\n",
      "Epoch 10403/20000 Training Loss: 0.07321392744779587\n",
      "Epoch 10404/20000 Training Loss: 0.04631488397717476\n",
      "Epoch 10405/20000 Training Loss: 0.060895007103681564\n",
      "Epoch 10406/20000 Training Loss: 0.06232326850295067\n",
      "Epoch 10407/20000 Training Loss: 0.05074114724993706\n",
      "Epoch 10408/20000 Training Loss: 0.0629040077328682\n",
      "Epoch 10409/20000 Training Loss: 0.05469818040728569\n",
      "Epoch 10410/20000 Training Loss: 0.06139044091105461\n",
      "Epoch 10410/20000 Validation Loss: 0.061809226870536804\n",
      "Epoch 10411/20000 Training Loss: 0.04782677814364433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10412/20000 Training Loss: 0.0452849417924881\n",
      "Epoch 10413/20000 Training Loss: 0.055480752140283585\n",
      "Epoch 10414/20000 Training Loss: 0.05772954225540161\n",
      "Epoch 10415/20000 Training Loss: 0.053932249546051025\n",
      "Epoch 10416/20000 Training Loss: 0.05917981266975403\n",
      "Epoch 10417/20000 Training Loss: 0.0643995925784111\n",
      "Epoch 10418/20000 Training Loss: 0.060561876744031906\n",
      "Epoch 10419/20000 Training Loss: 0.0448114238679409\n",
      "Epoch 10420/20000 Training Loss: 0.0641762837767601\n",
      "Epoch 10420/20000 Validation Loss: 0.048676878213882446\n",
      "Epoch 10421/20000 Training Loss: 0.06899318844079971\n",
      "Epoch 10422/20000 Training Loss: 0.060719359666109085\n",
      "Epoch 10423/20000 Training Loss: 0.057504694908857346\n",
      "Epoch 10424/20000 Training Loss: 0.06880482286214828\n",
      "Epoch 10425/20000 Training Loss: 0.05180172249674797\n",
      "Epoch 10426/20000 Training Loss: 0.07257822901010513\n",
      "Epoch 10427/20000 Training Loss: 0.05875398591160774\n",
      "Epoch 10428/20000 Training Loss: 0.04884811118245125\n",
      "Epoch 10429/20000 Training Loss: 0.045673176646232605\n",
      "Epoch 10430/20000 Training Loss: 0.07641460746526718\n",
      "Epoch 10430/20000 Validation Loss: 0.06312752515077591\n",
      "Epoch 10431/20000 Training Loss: 0.052017271518707275\n",
      "Epoch 10432/20000 Training Loss: 0.04410373792052269\n",
      "Epoch 10433/20000 Training Loss: 0.04923577979207039\n",
      "Epoch 10434/20000 Training Loss: 0.05534164980053902\n",
      "Epoch 10435/20000 Training Loss: 0.06369528919458389\n",
      "Epoch 10436/20000 Training Loss: 0.057717207819223404\n",
      "Epoch 10437/20000 Training Loss: 0.03519614040851593\n",
      "Epoch 10438/20000 Training Loss: 0.06556738168001175\n",
      "Epoch 10439/20000 Training Loss: 0.05011702701449394\n",
      "Epoch 10440/20000 Training Loss: 0.05950899422168732\n",
      "Epoch 10440/20000 Validation Loss: 0.041985392570495605\n",
      "Epoch 10441/20000 Training Loss: 0.05958033725619316\n",
      "Epoch 10442/20000 Training Loss: 0.05752163007855415\n",
      "Epoch 10443/20000 Training Loss: 0.06454101949930191\n",
      "Epoch 10444/20000 Training Loss: 0.05730879306793213\n",
      "Epoch 10445/20000 Training Loss: 0.05714568868279457\n",
      "Epoch 10446/20000 Training Loss: 0.046696487814188004\n",
      "Epoch 10447/20000 Training Loss: 0.06552371382713318\n",
      "Epoch 10448/20000 Training Loss: 0.042356938123703\n",
      "Epoch 10449/20000 Training Loss: 0.044163238257169724\n",
      "Epoch 10450/20000 Training Loss: 0.05265559256076813\n",
      "Epoch 10450/20000 Validation Loss: 0.061227500438690186\n",
      "Epoch 10451/20000 Training Loss: 0.06073436141014099\n",
      "Epoch 10452/20000 Training Loss: 0.06300431489944458\n",
      "Epoch 10453/20000 Training Loss: 0.05804646015167236\n",
      "Epoch 10454/20000 Training Loss: 0.05433529615402222\n",
      "Epoch 10455/20000 Training Loss: 0.06139953061938286\n",
      "Epoch 10456/20000 Training Loss: 0.04178129509091377\n",
      "Epoch 10457/20000 Training Loss: 0.047062769532203674\n",
      "Epoch 10458/20000 Training Loss: 0.052062828093767166\n",
      "Epoch 10459/20000 Training Loss: 0.06308652460575104\n",
      "Epoch 10460/20000 Training Loss: 0.054696813225746155\n",
      "Epoch 10460/20000 Validation Loss: 0.04807283729314804\n",
      "Epoch 10461/20000 Training Loss: 0.05878112092614174\n",
      "Epoch 10462/20000 Training Loss: 0.06007051095366478\n",
      "Epoch 10463/20000 Training Loss: 0.06093807518482208\n",
      "Epoch 10464/20000 Training Loss: 0.05920912325382233\n",
      "Epoch 10465/20000 Training Loss: 0.04505743086338043\n",
      "Epoch 10466/20000 Training Loss: 0.07158568501472473\n",
      "Epoch 10467/20000 Training Loss: 0.040414467453956604\n",
      "Epoch 10468/20000 Training Loss: 0.03647473827004433\n",
      "Epoch 10469/20000 Training Loss: 0.04625031352043152\n",
      "Epoch 10470/20000 Training Loss: 0.04871411249041557\n",
      "Epoch 10470/20000 Validation Loss: 0.07627757638692856\n",
      "Epoch 10471/20000 Training Loss: 0.053741853684186935\n",
      "Epoch 10472/20000 Training Loss: 0.04010212421417236\n",
      "Epoch 10473/20000 Training Loss: 0.04953994229435921\n",
      "Epoch 10474/20000 Training Loss: 0.05334770679473877\n",
      "Epoch 10475/20000 Training Loss: 0.08164122700691223\n",
      "Epoch 10476/20000 Training Loss: 0.051211077719926834\n",
      "Epoch 10477/20000 Training Loss: 0.04952343925833702\n",
      "Epoch 10478/20000 Training Loss: 0.03662737086415291\n",
      "Epoch 10479/20000 Training Loss: 0.0490795373916626\n",
      "Epoch 10480/20000 Training Loss: 0.05491144582629204\n",
      "Epoch 10480/20000 Validation Loss: 0.053636156022548676\n",
      "Epoch 10481/20000 Training Loss: 0.04705895856022835\n",
      "Epoch 10482/20000 Training Loss: 0.061924099922180176\n",
      "Epoch 10483/20000 Training Loss: 0.053911998867988586\n",
      "Epoch 10484/20000 Training Loss: 0.05333015322685242\n",
      "Epoch 10485/20000 Training Loss: 0.058748647570610046\n",
      "Epoch 10486/20000 Training Loss: 0.056268591433763504\n",
      "Epoch 10487/20000 Training Loss: 0.06231129169464111\n",
      "Epoch 10488/20000 Training Loss: 0.048976827412843704\n",
      "Epoch 10489/20000 Training Loss: 0.08112400025129318\n",
      "Epoch 10490/20000 Training Loss: 0.05343342944979668\n",
      "Epoch 10490/20000 Validation Loss: 0.058569006621837616\n",
      "Epoch 10491/20000 Training Loss: 0.04596370458602905\n",
      "Epoch 10492/20000 Training Loss: 0.04906044527888298\n",
      "Epoch 10493/20000 Training Loss: 0.06877420097589493\n",
      "Epoch 10494/20000 Training Loss: 0.06071050837635994\n",
      "Epoch 10495/20000 Training Loss: 0.0483740009367466\n",
      "Epoch 10496/20000 Training Loss: 0.05271763727068901\n",
      "Epoch 10497/20000 Training Loss: 0.04027954861521721\n",
      "Epoch 10498/20000 Training Loss: 0.05053569748997688\n",
      "Epoch 10499/20000 Training Loss: 0.0639755055308342\n",
      "Epoch 10500/20000 Training Loss: 0.04125216230750084\n",
      "Epoch 10500/20000 Validation Loss: 0.07285397499799728\n",
      "Epoch 10501/20000 Training Loss: 0.04360433295369148\n",
      "Epoch 10502/20000 Training Loss: 0.046332936733961105\n",
      "Epoch 10503/20000 Training Loss: 0.05595354735851288\n",
      "Epoch 10504/20000 Training Loss: 0.06925065070390701\n",
      "Epoch 10505/20000 Training Loss: 0.05443267151713371\n",
      "Epoch 10506/20000 Training Loss: 0.07048951834440231\n",
      "Epoch 10507/20000 Training Loss: 0.053464606404304504\n",
      "Epoch 10508/20000 Training Loss: 0.07497384399175644\n",
      "Epoch 10509/20000 Training Loss: 0.06490959227085114\n",
      "Epoch 10510/20000 Training Loss: 0.05009979382157326\n",
      "Epoch 10510/20000 Validation Loss: 0.04602578654885292\n",
      "Epoch 10511/20000 Training Loss: 0.05655917897820473\n",
      "Epoch 10512/20000 Training Loss: 0.0657484233379364\n",
      "Epoch 10513/20000 Training Loss: 0.05330398678779602\n",
      "Epoch 10514/20000 Training Loss: 0.057124119251966476\n",
      "Epoch 10515/20000 Training Loss: 0.04882699251174927\n",
      "Epoch 10516/20000 Training Loss: 0.047890275716781616\n",
      "Epoch 10517/20000 Training Loss: 0.053216353058815\n",
      "Epoch 10518/20000 Training Loss: 0.049429845064878464\n",
      "Epoch 10519/20000 Training Loss: 0.0536952018737793\n",
      "Epoch 10520/20000 Training Loss: 0.04456596449017525\n",
      "Epoch 10520/20000 Validation Loss: 0.08608709275722504\n",
      "Epoch 10521/20000 Training Loss: 0.06018269062042236\n",
      "Epoch 10522/20000 Training Loss: 0.06122481822967529\n",
      "Epoch 10523/20000 Training Loss: 0.050837695598602295\n",
      "Epoch 10524/20000 Training Loss: 0.049785394221544266\n",
      "Epoch 10525/20000 Training Loss: 0.05794261023402214\n",
      "Epoch 10526/20000 Training Loss: 0.08047952502965927\n",
      "Epoch 10527/20000 Training Loss: 0.057841137051582336\n",
      "Epoch 10528/20000 Training Loss: 0.06001456454396248\n",
      "Epoch 10529/20000 Training Loss: 0.07088294625282288\n",
      "Epoch 10530/20000 Training Loss: 0.06876081228256226\n",
      "Epoch 10530/20000 Validation Loss: 0.05891987681388855\n",
      "Epoch 10531/20000 Training Loss: 0.045182015746831894\n",
      "Epoch 10532/20000 Training Loss: 0.05317990481853485\n",
      "Epoch 10533/20000 Training Loss: 0.061484724283218384\n",
      "Epoch 10534/20000 Training Loss: 0.05900676175951958\n",
      "Epoch 10535/20000 Training Loss: 0.06720500439405441\n",
      "Epoch 10536/20000 Training Loss: 0.03232821449637413\n",
      "Epoch 10537/20000 Training Loss: 0.07129017263650894\n",
      "Epoch 10538/20000 Training Loss: 0.054733287543058395\n",
      "Epoch 10539/20000 Training Loss: 0.07043334096670151\n",
      "Epoch 10540/20000 Training Loss: 0.061163727194070816\n",
      "Epoch 10540/20000 Validation Loss: 0.053117893636226654\n",
      "Epoch 10541/20000 Training Loss: 0.07134508341550827\n",
      "Epoch 10542/20000 Training Loss: 0.0384281724691391\n",
      "Epoch 10543/20000 Training Loss: 0.057233601808547974\n",
      "Epoch 10544/20000 Training Loss: 0.0552855022251606\n",
      "Epoch 10545/20000 Training Loss: 0.049211423844099045\n",
      "Epoch 10546/20000 Training Loss: 0.053511470556259155\n",
      "Epoch 10547/20000 Training Loss: 0.06510794907808304\n",
      "Epoch 10548/20000 Training Loss: 0.05783321335911751\n",
      "Epoch 10549/20000 Training Loss: 0.07473556697368622\n",
      "Epoch 10550/20000 Training Loss: 0.04432639852166176\n",
      "Epoch 10550/20000 Validation Loss: 0.06242576614022255\n",
      "Epoch 10551/20000 Training Loss: 0.05878226086497307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10552/20000 Training Loss: 0.04958903789520264\n",
      "Epoch 10553/20000 Training Loss: 0.06068909168243408\n",
      "Epoch 10554/20000 Training Loss: 0.05264051631093025\n",
      "Epoch 10555/20000 Training Loss: 0.05259038880467415\n",
      "Epoch 10556/20000 Training Loss: 0.048081543296575546\n",
      "Epoch 10557/20000 Training Loss: 0.04049200937151909\n",
      "Epoch 10558/20000 Training Loss: 0.047553930431604385\n",
      "Epoch 10559/20000 Training Loss: 0.05345313623547554\n",
      "Epoch 10560/20000 Training Loss: 0.06483874469995499\n",
      "Epoch 10560/20000 Validation Loss: 0.046169571578502655\n",
      "Epoch 10561/20000 Training Loss: 0.048209477216005325\n",
      "Epoch 10562/20000 Training Loss: 0.04273737594485283\n",
      "Epoch 10563/20000 Training Loss: 0.04869287088513374\n",
      "Epoch 10564/20000 Training Loss: 0.07262498140335083\n",
      "Epoch 10565/20000 Training Loss: 0.06005699560046196\n",
      "Epoch 10566/20000 Training Loss: 0.06119013950228691\n",
      "Epoch 10567/20000 Training Loss: 0.05199502781033516\n",
      "Epoch 10568/20000 Training Loss: 0.06964185833930969\n",
      "Epoch 10569/20000 Training Loss: 0.05431492254137993\n",
      "Epoch 10570/20000 Training Loss: 0.05032195523381233\n",
      "Epoch 10570/20000 Validation Loss: 0.07146929204463959\n",
      "Epoch 10571/20000 Training Loss: 0.06917297840118408\n",
      "Epoch 10572/20000 Training Loss: 0.04155800864100456\n",
      "Epoch 10573/20000 Training Loss: 0.055401723831892014\n",
      "Epoch 10574/20000 Training Loss: 0.05350388213992119\n",
      "Epoch 10575/20000 Training Loss: 0.04709745571017265\n",
      "Epoch 10576/20000 Training Loss: 0.057551056146621704\n",
      "Epoch 10577/20000 Training Loss: 0.062456369400024414\n",
      "Epoch 10578/20000 Training Loss: 0.05225446820259094\n",
      "Epoch 10579/20000 Training Loss: 0.06042996048927307\n",
      "Epoch 10580/20000 Training Loss: 0.05153003707528114\n",
      "Epoch 10580/20000 Validation Loss: 0.0739995613694191\n",
      "Epoch 10581/20000 Training Loss: 0.08090145140886307\n",
      "Epoch 10582/20000 Training Loss: 0.049194902181625366\n",
      "Epoch 10583/20000 Training Loss: 0.041890066117048264\n",
      "Epoch 10584/20000 Training Loss: 0.04786109924316406\n",
      "Epoch 10585/20000 Training Loss: 0.04367316886782646\n",
      "Epoch 10586/20000 Training Loss: 0.04672136902809143\n",
      "Epoch 10587/20000 Training Loss: 0.059075966477394104\n",
      "Epoch 10588/20000 Training Loss: 0.057862717658281326\n",
      "Epoch 10589/20000 Training Loss: 0.054589077830314636\n",
      "Epoch 10590/20000 Training Loss: 0.057837288826704025\n",
      "Epoch 10590/20000 Validation Loss: 0.06651802361011505\n",
      "Epoch 10591/20000 Training Loss: 0.04922839626669884\n",
      "Epoch 10592/20000 Training Loss: 0.04702646657824516\n",
      "Epoch 10593/20000 Training Loss: 0.04132814332842827\n",
      "Epoch 10594/20000 Training Loss: 0.04864175245165825\n",
      "Epoch 10595/20000 Training Loss: 0.07956566661596298\n",
      "Epoch 10596/20000 Training Loss: 0.041331976652145386\n",
      "Epoch 10597/20000 Training Loss: 0.06865203380584717\n",
      "Epoch 10598/20000 Training Loss: 0.053233176469802856\n",
      "Epoch 10599/20000 Training Loss: 0.049152228981256485\n",
      "Epoch 10600/20000 Training Loss: 0.05565905570983887\n",
      "Epoch 10600/20000 Validation Loss: 0.05199997499585152\n",
      "Epoch 10601/20000 Training Loss: 0.0762946754693985\n",
      "Epoch 10602/20000 Training Loss: 0.04830431938171387\n",
      "Epoch 10603/20000 Training Loss: 0.043739769607782364\n",
      "Epoch 10604/20000 Training Loss: 0.05380895733833313\n",
      "Epoch 10605/20000 Training Loss: 0.056829970329999924\n",
      "Epoch 10606/20000 Training Loss: 0.053309276700019836\n",
      "Epoch 10607/20000 Training Loss: 0.047790687531232834\n",
      "Epoch 10608/20000 Training Loss: 0.0446750670671463\n",
      "Epoch 10609/20000 Training Loss: 0.05123168230056763\n",
      "Epoch 10610/20000 Training Loss: 0.04661056026816368\n",
      "Epoch 10610/20000 Validation Loss: 0.06857235729694366\n",
      "Epoch 10611/20000 Training Loss: 0.04636867716908455\n",
      "Epoch 10612/20000 Training Loss: 0.045086126774549484\n",
      "Epoch 10613/20000 Training Loss: 0.06127572059631348\n",
      "Epoch 10614/20000 Training Loss: 0.059019964188337326\n",
      "Epoch 10615/20000 Training Loss: 0.06133505329489708\n",
      "Epoch 10616/20000 Training Loss: 0.050885286182165146\n",
      "Epoch 10617/20000 Training Loss: 0.06351739913225174\n",
      "Epoch 10618/20000 Training Loss: 0.04142019525170326\n",
      "Epoch 10619/20000 Training Loss: 0.07091760635375977\n",
      "Epoch 10620/20000 Training Loss: 0.06452368199825287\n",
      "Epoch 10620/20000 Validation Loss: 0.05998876318335533\n",
      "Epoch 10621/20000 Training Loss: 0.0764818787574768\n",
      "Epoch 10622/20000 Training Loss: 0.059634849429130554\n",
      "Epoch 10623/20000 Training Loss: 0.06873708218336105\n",
      "Epoch 10624/20000 Training Loss: 0.06630811095237732\n",
      "Epoch 10625/20000 Training Loss: 0.054150670766830444\n",
      "Epoch 10626/20000 Training Loss: 0.05301022529602051\n",
      "Epoch 10627/20000 Training Loss: 0.05637775734066963\n",
      "Epoch 10628/20000 Training Loss: 0.04714737460017204\n",
      "Epoch 10629/20000 Training Loss: 0.04500260949134827\n",
      "Epoch 10630/20000 Training Loss: 0.053054261952638626\n",
      "Epoch 10630/20000 Validation Loss: 0.06299799680709839\n",
      "Epoch 10631/20000 Training Loss: 0.048235516995191574\n",
      "Epoch 10632/20000 Training Loss: 0.06406345218420029\n",
      "Epoch 10633/20000 Training Loss: 0.04258046671748161\n",
      "Epoch 10634/20000 Training Loss: 0.06287916749715805\n",
      "Epoch 10635/20000 Training Loss: 0.04675886034965515\n",
      "Epoch 10636/20000 Training Loss: 0.0488496758043766\n",
      "Epoch 10637/20000 Training Loss: 0.06980060786008835\n",
      "Epoch 10638/20000 Training Loss: 0.06477942317724228\n",
      "Epoch 10639/20000 Training Loss: 0.05389997735619545\n",
      "Epoch 10640/20000 Training Loss: 0.050765469670295715\n",
      "Epoch 10640/20000 Validation Loss: 0.06138722598552704\n",
      "Epoch 10641/20000 Training Loss: 0.06123974919319153\n",
      "Epoch 10642/20000 Training Loss: 0.053178850561380386\n",
      "Epoch 10643/20000 Training Loss: 0.04814201220870018\n",
      "Epoch 10644/20000 Training Loss: 0.048333924263715744\n",
      "Epoch 10645/20000 Training Loss: 0.06348150968551636\n",
      "Epoch 10646/20000 Training Loss: 0.040593329817056656\n",
      "Epoch 10647/20000 Training Loss: 0.07614700496196747\n",
      "Epoch 10648/20000 Training Loss: 0.048794180154800415\n",
      "Epoch 10649/20000 Training Loss: 0.07503879815340042\n",
      "Epoch 10650/20000 Training Loss: 0.047673773020505905\n",
      "Epoch 10650/20000 Validation Loss: 0.04422864317893982\n",
      "Epoch 10651/20000 Training Loss: 0.059639882296323776\n",
      "Epoch 10652/20000 Training Loss: 0.037546634674072266\n",
      "Epoch 10653/20000 Training Loss: 0.04891049489378929\n",
      "Epoch 10654/20000 Training Loss: 0.04707731679081917\n",
      "Epoch 10655/20000 Training Loss: 0.06052476167678833\n",
      "Epoch 10656/20000 Training Loss: 0.05633096769452095\n",
      "Epoch 10657/20000 Training Loss: 0.053284671157598495\n",
      "Epoch 10658/20000 Training Loss: 0.048308681696653366\n",
      "Epoch 10659/20000 Training Loss: 0.0543503575026989\n",
      "Epoch 10660/20000 Training Loss: 0.05248607322573662\n",
      "Epoch 10660/20000 Validation Loss: 0.054302774369716644\n",
      "Epoch 10661/20000 Training Loss: 0.04877573251724243\n",
      "Epoch 10662/20000 Training Loss: 0.06106055900454521\n",
      "Epoch 10663/20000 Training Loss: 0.04385758936405182\n",
      "Epoch 10664/20000 Training Loss: 0.07426603883504868\n",
      "Epoch 10665/20000 Training Loss: 0.05587522312998772\n",
      "Epoch 10666/20000 Training Loss: 0.06578167527914047\n",
      "Epoch 10667/20000 Training Loss: 0.057400863617658615\n",
      "Epoch 10668/20000 Training Loss: 0.045532021671533585\n",
      "Epoch 10669/20000 Training Loss: 0.057400915771722794\n",
      "Epoch 10670/20000 Training Loss: 0.056947577744722366\n",
      "Epoch 10670/20000 Validation Loss: 0.04560275375843048\n",
      "Epoch 10671/20000 Training Loss: 0.07180257886648178\n",
      "Epoch 10672/20000 Training Loss: 0.040058985352516174\n",
      "Epoch 10673/20000 Training Loss: 0.06034477427601814\n",
      "Epoch 10674/20000 Training Loss: 0.07066424936056137\n",
      "Epoch 10675/20000 Training Loss: 0.06829281896352768\n",
      "Epoch 10676/20000 Training Loss: 0.05620797351002693\n",
      "Epoch 10677/20000 Training Loss: 0.06446007639169693\n",
      "Epoch 10678/20000 Training Loss: 0.050176333636045456\n",
      "Epoch 10679/20000 Training Loss: 0.045405227690935135\n",
      "Epoch 10680/20000 Training Loss: 0.07696500420570374\n",
      "Epoch 10680/20000 Validation Loss: 0.05054463446140289\n",
      "Epoch 10681/20000 Training Loss: 0.05838366225361824\n",
      "Epoch 10682/20000 Training Loss: 0.04332692548632622\n",
      "Epoch 10683/20000 Training Loss: 0.05393223837018013\n",
      "Epoch 10684/20000 Training Loss: 0.05505258962512016\n",
      "Epoch 10685/20000 Training Loss: 0.03866857290267944\n",
      "Epoch 10686/20000 Training Loss: 0.06195738911628723\n",
      "Epoch 10687/20000 Training Loss: 0.04832634702324867\n",
      "Epoch 10688/20000 Training Loss: 0.05287570878863335\n",
      "Epoch 10689/20000 Training Loss: 0.05361062288284302\n",
      "Epoch 10690/20000 Training Loss: 0.037500277161598206\n",
      "Epoch 10690/20000 Validation Loss: 0.0453021377325058\n",
      "Epoch 10691/20000 Training Loss: 0.03713544085621834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10692/20000 Training Loss: 0.054582610726356506\n",
      "Epoch 10693/20000 Training Loss: 0.05630568787455559\n",
      "Epoch 10694/20000 Training Loss: 0.061110615730285645\n",
      "Epoch 10695/20000 Training Loss: 0.04761991277337074\n",
      "Epoch 10696/20000 Training Loss: 0.05050959065556526\n",
      "Epoch 10697/20000 Training Loss: 0.05289706960320473\n",
      "Epoch 10698/20000 Training Loss: 0.05298788473010063\n",
      "Epoch 10699/20000 Training Loss: 0.05073262378573418\n",
      "Epoch 10700/20000 Training Loss: 0.06226825714111328\n",
      "Epoch 10700/20000 Validation Loss: 0.060881197452545166\n",
      "Epoch 10701/20000 Training Loss: 0.04671632871031761\n",
      "Epoch 10702/20000 Training Loss: 0.0549333281815052\n",
      "Epoch 10703/20000 Training Loss: 0.044057250022888184\n",
      "Epoch 10704/20000 Training Loss: 0.07396256923675537\n",
      "Epoch 10705/20000 Training Loss: 0.054586973041296005\n",
      "Epoch 10706/20000 Training Loss: 0.04891893267631531\n",
      "Epoch 10707/20000 Training Loss: 0.05896594002842903\n",
      "Epoch 10708/20000 Training Loss: 0.0675664022564888\n",
      "Epoch 10709/20000 Training Loss: 0.07886876910924911\n",
      "Epoch 10710/20000 Training Loss: 0.050726715475320816\n",
      "Epoch 10710/20000 Validation Loss: 0.06595025211572647\n",
      "Epoch 10711/20000 Training Loss: 0.05181705951690674\n",
      "Epoch 10712/20000 Training Loss: 0.055878132581710815\n",
      "Epoch 10713/20000 Training Loss: 0.06398750096559525\n",
      "Epoch 10714/20000 Training Loss: 0.06020529195666313\n",
      "Epoch 10715/20000 Training Loss: 0.04452766105532646\n",
      "Epoch 10716/20000 Training Loss: 0.04790182784199715\n",
      "Epoch 10717/20000 Training Loss: 0.06645461916923523\n",
      "Epoch 10718/20000 Training Loss: 0.050582434982061386\n",
      "Epoch 10719/20000 Training Loss: 0.05794752761721611\n",
      "Epoch 10720/20000 Training Loss: 0.0711616799235344\n",
      "Epoch 10720/20000 Validation Loss: 0.05932208150625229\n",
      "Epoch 10721/20000 Training Loss: 0.07218264788389206\n",
      "Epoch 10722/20000 Training Loss: 0.05637241527438164\n",
      "Epoch 10723/20000 Training Loss: 0.04881642386317253\n",
      "Epoch 10724/20000 Training Loss: 0.06759040802717209\n",
      "Epoch 10725/20000 Training Loss: 0.06489540636539459\n",
      "Epoch 10726/20000 Training Loss: 0.051463883370161057\n",
      "Epoch 10727/20000 Training Loss: 0.05605997517704964\n",
      "Epoch 10728/20000 Training Loss: 0.05784274637699127\n",
      "Epoch 10729/20000 Training Loss: 0.07115105539560318\n",
      "Epoch 10730/20000 Training Loss: 0.0632752850651741\n",
      "Epoch 10730/20000 Validation Loss: 0.060762159526348114\n",
      "Epoch 10731/20000 Training Loss: 0.059043869376182556\n",
      "Epoch 10732/20000 Training Loss: 0.0589301623404026\n",
      "Epoch 10733/20000 Training Loss: 0.051532719284296036\n",
      "Epoch 10734/20000 Training Loss: 0.061416517943143845\n",
      "Epoch 10735/20000 Training Loss: 0.046769335865974426\n",
      "Epoch 10736/20000 Training Loss: 0.047748863697052\n",
      "Epoch 10737/20000 Training Loss: 0.05695679411292076\n",
      "Epoch 10738/20000 Training Loss: 0.06277453899383545\n",
      "Epoch 10739/20000 Training Loss: 0.07221949845552444\n",
      "Epoch 10740/20000 Training Loss: 0.051154982298612595\n",
      "Epoch 10740/20000 Validation Loss: 0.045575208961963654\n",
      "Epoch 10741/20000 Training Loss: 0.04685220494866371\n",
      "Epoch 10742/20000 Training Loss: 0.06704389303922653\n",
      "Epoch 10743/20000 Training Loss: 0.039708301424980164\n",
      "Epoch 10744/20000 Training Loss: 0.0557267963886261\n",
      "Epoch 10745/20000 Training Loss: 0.059627097100019455\n",
      "Epoch 10746/20000 Training Loss: 0.05667705461382866\n",
      "Epoch 10747/20000 Training Loss: 0.064616359770298\n",
      "Epoch 10748/20000 Training Loss: 0.054197344928979874\n",
      "Epoch 10749/20000 Training Loss: 0.043748319149017334\n",
      "Epoch 10750/20000 Training Loss: 0.0529746450483799\n",
      "Epoch 10750/20000 Validation Loss: 0.05902770161628723\n",
      "Epoch 10751/20000 Training Loss: 0.06058383360505104\n",
      "Epoch 10752/20000 Training Loss: 0.06138283386826515\n",
      "Epoch 10753/20000 Training Loss: 0.069276824593544\n",
      "Epoch 10754/20000 Training Loss: 0.04020410403609276\n",
      "Epoch 10755/20000 Training Loss: 0.05173000320792198\n",
      "Epoch 10756/20000 Training Loss: 0.04812897741794586\n",
      "Epoch 10757/20000 Training Loss: 0.05967554450035095\n",
      "Epoch 10758/20000 Training Loss: 0.056334834545850754\n",
      "Epoch 10759/20000 Training Loss: 0.044707149267196655\n",
      "Epoch 10760/20000 Training Loss: 0.05616764724254608\n",
      "Epoch 10760/20000 Validation Loss: 0.057012610137462616\n",
      "Epoch 10761/20000 Training Loss: 0.056136053055524826\n",
      "Epoch 10762/20000 Training Loss: 0.05549557879567146\n",
      "Epoch 10763/20000 Training Loss: 0.04715152457356453\n",
      "Epoch 10764/20000 Training Loss: 0.05521003529429436\n",
      "Epoch 10765/20000 Training Loss: 0.058021385222673416\n",
      "Epoch 10766/20000 Training Loss: 0.04091096669435501\n",
      "Epoch 10767/20000 Training Loss: 0.050735753029584885\n",
      "Epoch 10768/20000 Training Loss: 0.04869891703128815\n",
      "Epoch 10769/20000 Training Loss: 0.05202334001660347\n",
      "Epoch 10770/20000 Training Loss: 0.05684947967529297\n",
      "Epoch 10770/20000 Validation Loss: 0.06950660794973373\n",
      "Epoch 10771/20000 Training Loss: 0.04511469602584839\n",
      "Epoch 10772/20000 Training Loss: 0.08404302597045898\n",
      "Epoch 10773/20000 Training Loss: 0.03614357113838196\n",
      "Epoch 10774/20000 Training Loss: 0.04865827038884163\n",
      "Epoch 10775/20000 Training Loss: 0.05334046855568886\n",
      "Epoch 10776/20000 Training Loss: 0.049904223531484604\n",
      "Epoch 10777/20000 Training Loss: 0.05977349355816841\n",
      "Epoch 10778/20000 Training Loss: 0.056352149695158005\n",
      "Epoch 10779/20000 Training Loss: 0.061866361647844315\n",
      "Epoch 10780/20000 Training Loss: 0.06100615859031677\n",
      "Epoch 10780/20000 Validation Loss: 0.05383145436644554\n",
      "Epoch 10781/20000 Training Loss: 0.05553005635738373\n",
      "Epoch 10782/20000 Training Loss: 0.05212657153606415\n",
      "Epoch 10783/20000 Training Loss: 0.06287278980016708\n",
      "Epoch 10784/20000 Training Loss: 0.04925921559333801\n",
      "Epoch 10785/20000 Training Loss: 0.07066354155540466\n",
      "Epoch 10786/20000 Training Loss: 0.046526599675416946\n",
      "Epoch 10787/20000 Training Loss: 0.057378098368644714\n",
      "Epoch 10788/20000 Training Loss: 0.06373968720436096\n",
      "Epoch 10789/20000 Training Loss: 0.04919196665287018\n",
      "Epoch 10790/20000 Training Loss: 0.05595569312572479\n",
      "Epoch 10790/20000 Validation Loss: 0.0442783497273922\n",
      "Epoch 10791/20000 Training Loss: 0.060671597719192505\n",
      "Epoch 10792/20000 Training Loss: 0.05574527010321617\n",
      "Epoch 10793/20000 Training Loss: 0.04259837791323662\n",
      "Epoch 10794/20000 Training Loss: 0.05570170283317566\n",
      "Epoch 10795/20000 Training Loss: 0.053117960691452026\n",
      "Epoch 10796/20000 Training Loss: 0.044502150267362595\n",
      "Epoch 10797/20000 Training Loss: 0.045840442180633545\n",
      "Epoch 10798/20000 Training Loss: 0.061438847333192825\n",
      "Epoch 10799/20000 Training Loss: 0.05675148963928223\n",
      "Epoch 10800/20000 Training Loss: 0.06560388952493668\n",
      "Epoch 10800/20000 Validation Loss: 0.06280024349689484\n",
      "Epoch 10801/20000 Training Loss: 0.053282108157873154\n",
      "Epoch 10802/20000 Training Loss: 0.05874323844909668\n",
      "Epoch 10803/20000 Training Loss: 0.04555898532271385\n",
      "Epoch 10804/20000 Training Loss: 0.05079234018921852\n",
      "Epoch 10805/20000 Training Loss: 0.05227023735642433\n",
      "Epoch 10806/20000 Training Loss: 0.03757716715335846\n",
      "Epoch 10807/20000 Training Loss: 0.04637373611330986\n",
      "Epoch 10808/20000 Training Loss: 0.06723574548959732\n",
      "Epoch 10809/20000 Training Loss: 0.04606863483786583\n",
      "Epoch 10810/20000 Training Loss: 0.0613972544670105\n",
      "Epoch 10810/20000 Validation Loss: 0.07003265619277954\n",
      "Epoch 10811/20000 Training Loss: 0.03596628084778786\n",
      "Epoch 10812/20000 Training Loss: 0.05312070623040199\n",
      "Epoch 10813/20000 Training Loss: 0.04559339955449104\n",
      "Epoch 10814/20000 Training Loss: 0.04991401731967926\n",
      "Epoch 10815/20000 Training Loss: 0.052198365330696106\n",
      "Epoch 10816/20000 Training Loss: 0.06212775781750679\n",
      "Epoch 10817/20000 Training Loss: 0.06322512775659561\n",
      "Epoch 10818/20000 Training Loss: 0.050120700150728226\n",
      "Epoch 10819/20000 Training Loss: 0.09191358834505081\n",
      "Epoch 10820/20000 Training Loss: 0.038729745894670486\n",
      "Epoch 10820/20000 Validation Loss: 0.07321229577064514\n",
      "Epoch 10821/20000 Training Loss: 0.05629703029990196\n",
      "Epoch 10822/20000 Training Loss: 0.05733420327305794\n",
      "Epoch 10823/20000 Training Loss: 0.04456261917948723\n",
      "Epoch 10824/20000 Training Loss: 0.05617372319102287\n",
      "Epoch 10825/20000 Training Loss: 0.0539601631462574\n",
      "Epoch 10826/20000 Training Loss: 0.05803455039858818\n",
      "Epoch 10827/20000 Training Loss: 0.0640009194612503\n",
      "Epoch 10828/20000 Training Loss: 0.051197346299886703\n",
      "Epoch 10829/20000 Training Loss: 0.0739620253443718\n",
      "Epoch 10830/20000 Training Loss: 0.0858258381485939\n",
      "Epoch 10830/20000 Validation Loss: 0.04568289965391159\n",
      "Epoch 10831/20000 Training Loss: 0.04950067773461342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10832/20000 Training Loss: 0.06797610968351364\n",
      "Epoch 10833/20000 Training Loss: 0.04701736196875572\n",
      "Epoch 10834/20000 Training Loss: 0.07745835185050964\n",
      "Epoch 10835/20000 Training Loss: 0.045720651745796204\n",
      "Epoch 10836/20000 Training Loss: 0.07172814756631851\n",
      "Epoch 10837/20000 Training Loss: 0.03779866546392441\n",
      "Epoch 10838/20000 Training Loss: 0.05337013304233551\n",
      "Epoch 10839/20000 Training Loss: 0.050639163702726364\n",
      "Epoch 10840/20000 Training Loss: 0.056915100663900375\n",
      "Epoch 10840/20000 Validation Loss: 0.06425806879997253\n",
      "Epoch 10841/20000 Training Loss: 0.06072887405753136\n",
      "Epoch 10842/20000 Training Loss: 0.03982183709740639\n",
      "Epoch 10843/20000 Training Loss: 0.06287102401256561\n",
      "Epoch 10844/20000 Training Loss: 0.0635969340801239\n",
      "Epoch 10845/20000 Training Loss: 0.06175330653786659\n",
      "Epoch 10846/20000 Training Loss: 0.04187894985079765\n",
      "Epoch 10847/20000 Training Loss: 0.055401284247636795\n",
      "Epoch 10848/20000 Training Loss: 0.059052180498838425\n",
      "Epoch 10849/20000 Training Loss: 0.048501864075660706\n",
      "Epoch 10850/20000 Training Loss: 0.06679237633943558\n",
      "Epoch 10850/20000 Validation Loss: 0.058218955993652344\n",
      "Epoch 10851/20000 Training Loss: 0.05025772377848625\n",
      "Epoch 10852/20000 Training Loss: 0.049637869000434875\n",
      "Epoch 10853/20000 Training Loss: 0.061033349484205246\n",
      "Epoch 10854/20000 Training Loss: 0.04840521886944771\n",
      "Epoch 10855/20000 Training Loss: 0.07571800798177719\n",
      "Epoch 10856/20000 Training Loss: 0.045700546354055405\n",
      "Epoch 10857/20000 Training Loss: 0.053387727588415146\n",
      "Epoch 10858/20000 Training Loss: 0.054112594574689865\n",
      "Epoch 10859/20000 Training Loss: 0.0526866614818573\n",
      "Epoch 10860/20000 Training Loss: 0.050724223256111145\n",
      "Epoch 10860/20000 Validation Loss: 0.06028442829847336\n",
      "Epoch 10861/20000 Training Loss: 0.05221102014183998\n",
      "Epoch 10862/20000 Training Loss: 0.05839114263653755\n",
      "Epoch 10863/20000 Training Loss: 0.054177120327949524\n",
      "Epoch 10864/20000 Training Loss: 0.06453756242990494\n",
      "Epoch 10865/20000 Training Loss: 0.049146104604005814\n",
      "Epoch 10866/20000 Training Loss: 0.07989432662725449\n",
      "Epoch 10867/20000 Training Loss: 0.04639850929379463\n",
      "Epoch 10868/20000 Training Loss: 0.04992000386118889\n",
      "Epoch 10869/20000 Training Loss: 0.05287159979343414\n",
      "Epoch 10870/20000 Training Loss: 0.05056433752179146\n",
      "Epoch 10870/20000 Validation Loss: 0.056863974779844284\n",
      "Epoch 10871/20000 Training Loss: 0.07664549350738525\n",
      "Epoch 10872/20000 Training Loss: 0.059398677200078964\n",
      "Epoch 10873/20000 Training Loss: 0.06610827893018723\n",
      "Epoch 10874/20000 Training Loss: 0.04275844618678093\n",
      "Epoch 10875/20000 Training Loss: 0.04232616722583771\n",
      "Epoch 10876/20000 Training Loss: 0.04683328792452812\n",
      "Epoch 10877/20000 Training Loss: 0.06398576498031616\n",
      "Epoch 10878/20000 Training Loss: 0.04816187545657158\n",
      "Epoch 10879/20000 Training Loss: 0.058463748544454575\n",
      "Epoch 10880/20000 Training Loss: 0.05975782871246338\n",
      "Epoch 10880/20000 Validation Loss: 0.044775910675525665\n",
      "Epoch 10881/20000 Training Loss: 0.056567490100860596\n",
      "Epoch 10882/20000 Training Loss: 0.04706016555428505\n",
      "Epoch 10883/20000 Training Loss: 0.04649832844734192\n",
      "Epoch 10884/20000 Training Loss: 0.05909896269440651\n",
      "Epoch 10885/20000 Training Loss: 0.047392573207616806\n",
      "Epoch 10886/20000 Training Loss: 0.06721615791320801\n",
      "Epoch 10887/20000 Training Loss: 0.034226637333631516\n",
      "Epoch 10888/20000 Training Loss: 0.053433191031217575\n",
      "Epoch 10889/20000 Training Loss: 0.03483622893691063\n",
      "Epoch 10890/20000 Training Loss: 0.06614836305379868\n",
      "Epoch 10890/20000 Validation Loss: 0.061894189566373825\n",
      "Epoch 10891/20000 Training Loss: 0.06025388836860657\n",
      "Epoch 10892/20000 Training Loss: 0.06287024170160294\n",
      "Epoch 10893/20000 Training Loss: 0.05249495431780815\n",
      "Epoch 10894/20000 Training Loss: 0.0482327975332737\n",
      "Epoch 10895/20000 Training Loss: 0.058770280331373215\n",
      "Epoch 10896/20000 Training Loss: 0.060898780822753906\n",
      "Epoch 10897/20000 Training Loss: 0.05955159664154053\n",
      "Epoch 10898/20000 Training Loss: 0.06521107256412506\n",
      "Epoch 10899/20000 Training Loss: 0.04028846323490143\n",
      "Epoch 10900/20000 Training Loss: 0.04803353548049927\n",
      "Epoch 10900/20000 Validation Loss: 0.06272271275520325\n",
      "Epoch 10901/20000 Training Loss: 0.0536247193813324\n",
      "Epoch 10902/20000 Training Loss: 0.04671996831893921\n",
      "Epoch 10903/20000 Training Loss: 0.049894701689481735\n",
      "Epoch 10904/20000 Training Loss: 0.05965043231844902\n",
      "Epoch 10905/20000 Training Loss: 0.05012178421020508\n",
      "Epoch 10906/20000 Training Loss: 0.07857488840818405\n",
      "Epoch 10907/20000 Training Loss: 0.05170900747179985\n",
      "Epoch 10908/20000 Training Loss: 0.05569591000676155\n",
      "Epoch 10909/20000 Training Loss: 0.05007229372859001\n",
      "Epoch 10910/20000 Training Loss: 0.04654184356331825\n",
      "Epoch 10910/20000 Validation Loss: 0.06407110393047333\n",
      "Epoch 10911/20000 Training Loss: 0.058108121156692505\n",
      "Epoch 10912/20000 Training Loss: 0.042513083666563034\n",
      "Epoch 10913/20000 Training Loss: 0.06988366693258286\n",
      "Epoch 10914/20000 Training Loss: 0.05364394187927246\n",
      "Epoch 10915/20000 Training Loss: 0.06105909124016762\n",
      "Epoch 10916/20000 Training Loss: 0.05289207771420479\n",
      "Epoch 10917/20000 Training Loss: 0.06176294758915901\n",
      "Epoch 10918/20000 Training Loss: 0.04778914526104927\n",
      "Epoch 10919/20000 Training Loss: 0.03857281804084778\n",
      "Epoch 10920/20000 Training Loss: 0.05136112496256828\n",
      "Epoch 10920/20000 Validation Loss: 0.0620858296751976\n",
      "Epoch 10921/20000 Training Loss: 0.0348481722176075\n",
      "Epoch 10922/20000 Training Loss: 0.0621086061000824\n",
      "Epoch 10923/20000 Training Loss: 0.055532678961753845\n",
      "Epoch 10924/20000 Training Loss: 0.04066479578614235\n",
      "Epoch 10925/20000 Training Loss: 0.06897275894880295\n",
      "Epoch 10926/20000 Training Loss: 0.04585528373718262\n",
      "Epoch 10927/20000 Training Loss: 0.05546264722943306\n",
      "Epoch 10928/20000 Training Loss: 0.055303629487752914\n",
      "Epoch 10929/20000 Training Loss: 0.06609273701906204\n",
      "Epoch 10930/20000 Training Loss: 0.047391269356012344\n",
      "Epoch 10930/20000 Validation Loss: 0.061284832656383514\n",
      "Epoch 10931/20000 Training Loss: 0.05283300206065178\n",
      "Epoch 10932/20000 Training Loss: 0.0496845506131649\n",
      "Epoch 10933/20000 Training Loss: 0.05358438193798065\n",
      "Epoch 10934/20000 Training Loss: 0.04102887958288193\n",
      "Epoch 10935/20000 Training Loss: 0.055190760642290115\n",
      "Epoch 10936/20000 Training Loss: 0.07811857014894485\n",
      "Epoch 10937/20000 Training Loss: 0.0447537861764431\n",
      "Epoch 10938/20000 Training Loss: 0.05514289438724518\n",
      "Epoch 10939/20000 Training Loss: 0.0508299358189106\n",
      "Epoch 10940/20000 Training Loss: 0.04993833974003792\n",
      "Epoch 10940/20000 Validation Loss: 0.07360696792602539\n",
      "Epoch 10941/20000 Training Loss: 0.0642305240035057\n",
      "Epoch 10942/20000 Training Loss: 0.05526867136359215\n",
      "Epoch 10943/20000 Training Loss: 0.047850146889686584\n",
      "Epoch 10944/20000 Training Loss: 0.055532097816467285\n",
      "Epoch 10945/20000 Training Loss: 0.04373250901699066\n",
      "Epoch 10946/20000 Training Loss: 0.04931212589144707\n",
      "Epoch 10947/20000 Training Loss: 0.045593708753585815\n",
      "Epoch 10948/20000 Training Loss: 0.05726366117596626\n",
      "Epoch 10949/20000 Training Loss: 0.06514828652143478\n",
      "Epoch 10950/20000 Training Loss: 0.07327165454626083\n",
      "Epoch 10950/20000 Validation Loss: 0.06122232973575592\n",
      "Epoch 10951/20000 Training Loss: 0.0548044890165329\n",
      "Epoch 10952/20000 Training Loss: 0.05108211562037468\n",
      "Epoch 10953/20000 Training Loss: 0.07267012447118759\n",
      "Epoch 10954/20000 Training Loss: 0.05224631354212761\n",
      "Epoch 10955/20000 Training Loss: 0.05455963686108589\n",
      "Epoch 10956/20000 Training Loss: 0.06204201281070709\n",
      "Epoch 10957/20000 Training Loss: 0.07082003355026245\n",
      "Epoch 10958/20000 Training Loss: 0.05958594009280205\n",
      "Epoch 10959/20000 Training Loss: 0.0461106039583683\n",
      "Epoch 10960/20000 Training Loss: 0.06680002808570862\n",
      "Epoch 10960/20000 Validation Loss: 0.054481424391269684\n",
      "Epoch 10961/20000 Training Loss: 0.06397181749343872\n",
      "Epoch 10962/20000 Training Loss: 0.05018794164061546\n",
      "Epoch 10963/20000 Training Loss: 0.044549062848091125\n",
      "Epoch 10964/20000 Training Loss: 0.06559979915618896\n",
      "Epoch 10965/20000 Training Loss: 0.054080039262771606\n",
      "Epoch 10966/20000 Training Loss: 0.05958450958132744\n",
      "Epoch 10967/20000 Training Loss: 0.04828707501292229\n",
      "Epoch 10968/20000 Training Loss: 0.04457271471619606\n",
      "Epoch 10969/20000 Training Loss: 0.06029971316456795\n",
      "Epoch 10970/20000 Training Loss: 0.04147661104798317\n",
      "Epoch 10970/20000 Validation Loss: 0.05094581097364426\n",
      "Epoch 10971/20000 Training Loss: 0.06276265531778336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10972/20000 Training Loss: 0.05956320837140083\n",
      "Epoch 10973/20000 Training Loss: 0.07021913677453995\n",
      "Epoch 10974/20000 Training Loss: 0.05017444118857384\n",
      "Epoch 10975/20000 Training Loss: 0.05925126373767853\n",
      "Epoch 10976/20000 Training Loss: 0.058334629982709885\n",
      "Epoch 10977/20000 Training Loss: 0.042248744517564774\n",
      "Epoch 10978/20000 Training Loss: 0.04606099799275398\n",
      "Epoch 10979/20000 Training Loss: 0.06423676759004593\n",
      "Epoch 10980/20000 Training Loss: 0.07275637984275818\n",
      "Epoch 10980/20000 Validation Loss: 0.08134907484054565\n",
      "Epoch 10981/20000 Training Loss: 0.05582930147647858\n",
      "Epoch 10982/20000 Training Loss: 0.05355608090758324\n",
      "Epoch 10983/20000 Training Loss: 0.07287442684173584\n",
      "Epoch 10984/20000 Training Loss: 0.06117410585284233\n",
      "Epoch 10985/20000 Training Loss: 0.049893081188201904\n",
      "Epoch 10986/20000 Training Loss: 0.0644659548997879\n",
      "Epoch 10987/20000 Training Loss: 0.04547799006104469\n",
      "Epoch 10988/20000 Training Loss: 0.043561577796936035\n",
      "Epoch 10989/20000 Training Loss: 0.04439808800816536\n",
      "Epoch 10990/20000 Training Loss: 0.06069617345929146\n",
      "Epoch 10990/20000 Validation Loss: 0.051587507128715515\n",
      "Epoch 10991/20000 Training Loss: 0.06726249307394028\n",
      "Epoch 10992/20000 Training Loss: 0.03979942202568054\n",
      "Epoch 10993/20000 Training Loss: 0.06336116045713425\n",
      "Epoch 10994/20000 Training Loss: 0.04879777505993843\n",
      "Epoch 10995/20000 Training Loss: 0.0479925274848938\n",
      "Epoch 10996/20000 Training Loss: 0.07007795572280884\n",
      "Epoch 10997/20000 Training Loss: 0.05349728465080261\n",
      "Epoch 10998/20000 Training Loss: 0.05404690280556679\n",
      "Epoch 10999/20000 Training Loss: 0.04454481601715088\n",
      "Epoch 11000/20000 Training Loss: 0.05253191664814949\n",
      "Epoch 11000/20000 Validation Loss: 0.055477648973464966\n",
      "Epoch 11001/20000 Training Loss: 0.052539169788360596\n",
      "Epoch 11002/20000 Training Loss: 0.059707727283239365\n",
      "Epoch 11003/20000 Training Loss: 0.06759016960859299\n",
      "Epoch 11004/20000 Training Loss: 0.06956056505441666\n",
      "Epoch 11005/20000 Training Loss: 0.056640755385160446\n",
      "Epoch 11006/20000 Training Loss: 0.053807418793439865\n",
      "Epoch 11007/20000 Training Loss: 0.0449664480984211\n",
      "Epoch 11008/20000 Training Loss: 0.0561627633869648\n",
      "Epoch 11009/20000 Training Loss: 0.0433085672557354\n",
      "Epoch 11010/20000 Training Loss: 0.049857109785079956\n",
      "Epoch 11010/20000 Validation Loss: 0.071323923766613\n",
      "Epoch 11011/20000 Training Loss: 0.05392676219344139\n",
      "Epoch 11012/20000 Training Loss: 0.05268314480781555\n",
      "Epoch 11013/20000 Training Loss: 0.04889044165611267\n",
      "Epoch 11014/20000 Training Loss: 0.04265490174293518\n",
      "Epoch 11015/20000 Training Loss: 0.058510761708021164\n",
      "Epoch 11016/20000 Training Loss: 0.055027980357408524\n",
      "Epoch 11017/20000 Training Loss: 0.06767263263463974\n",
      "Epoch 11018/20000 Training Loss: 0.0687192752957344\n",
      "Epoch 11019/20000 Training Loss: 0.05843034014105797\n",
      "Epoch 11020/20000 Training Loss: 0.06497139483690262\n",
      "Epoch 11020/20000 Validation Loss: 0.06201014667749405\n",
      "Epoch 11021/20000 Training Loss: 0.055782388895750046\n",
      "Epoch 11022/20000 Training Loss: 0.05404594913125038\n",
      "Epoch 11023/20000 Training Loss: 0.0487738661468029\n",
      "Epoch 11024/20000 Training Loss: 0.05809327960014343\n",
      "Epoch 11025/20000 Training Loss: 0.05273423716425896\n",
      "Epoch 11026/20000 Training Loss: 0.05119030550122261\n",
      "Epoch 11027/20000 Training Loss: 0.05030621588230133\n",
      "Epoch 11028/20000 Training Loss: 0.059487033635377884\n",
      "Epoch 11029/20000 Training Loss: 0.055628400295972824\n",
      "Epoch 11030/20000 Training Loss: 0.052218008786439896\n",
      "Epoch 11030/20000 Validation Loss: 0.0595669150352478\n",
      "Epoch 11031/20000 Training Loss: 0.05421091616153717\n",
      "Epoch 11032/20000 Training Loss: 0.05470157787203789\n",
      "Epoch 11033/20000 Training Loss: 0.06815030425786972\n",
      "Epoch 11034/20000 Training Loss: 0.05068141222000122\n",
      "Epoch 11035/20000 Training Loss: 0.053548578172922134\n",
      "Epoch 11036/20000 Training Loss: 0.06583160907030106\n",
      "Epoch 11037/20000 Training Loss: 0.045392587780952454\n",
      "Epoch 11038/20000 Training Loss: 0.048131030052900314\n",
      "Epoch 11039/20000 Training Loss: 0.05739227309823036\n",
      "Epoch 11040/20000 Training Loss: 0.03995605185627937\n",
      "Epoch 11040/20000 Validation Loss: 0.06131473928689957\n",
      "Epoch 11041/20000 Training Loss: 0.06864447146654129\n",
      "Epoch 11042/20000 Training Loss: 0.05692439153790474\n",
      "Epoch 11043/20000 Training Loss: 0.062136124819517136\n",
      "Epoch 11044/20000 Training Loss: 0.04970869421958923\n",
      "Epoch 11045/20000 Training Loss: 0.06305304169654846\n",
      "Epoch 11046/20000 Training Loss: 0.057057902216911316\n",
      "Epoch 11047/20000 Training Loss: 0.05685807392001152\n",
      "Epoch 11048/20000 Training Loss: 0.04396796226501465\n",
      "Epoch 11049/20000 Training Loss: 0.033273231238126755\n",
      "Epoch 11050/20000 Training Loss: 0.04366592690348625\n",
      "Epoch 11050/20000 Validation Loss: 0.03304354101419449\n",
      "Epoch 11051/20000 Training Loss: 0.054053857922554016\n",
      "Epoch 11052/20000 Training Loss: 0.06294161081314087\n",
      "Epoch 11053/20000 Training Loss: 0.049762580543756485\n",
      "Epoch 11054/20000 Training Loss: 0.054149556905031204\n",
      "Epoch 11055/20000 Training Loss: 0.051884185522794724\n",
      "Epoch 11056/20000 Training Loss: 0.06684096902608871\n",
      "Epoch 11057/20000 Training Loss: 0.036320850253105164\n",
      "Epoch 11058/20000 Training Loss: 0.06421493738889694\n",
      "Epoch 11059/20000 Training Loss: 0.05303573235869408\n",
      "Epoch 11060/20000 Training Loss: 0.05877644941210747\n",
      "Epoch 11060/20000 Validation Loss: 0.07547745108604431\n",
      "Epoch 11061/20000 Training Loss: 0.04720224067568779\n",
      "Epoch 11062/20000 Training Loss: 0.047751665115356445\n",
      "Epoch 11063/20000 Training Loss: 0.08695006370544434\n",
      "Epoch 11064/20000 Training Loss: 0.04996366798877716\n",
      "Epoch 11065/20000 Training Loss: 0.038912583142519\n",
      "Epoch 11066/20000 Training Loss: 0.05480475351214409\n",
      "Epoch 11067/20000 Training Loss: 0.05638020113110542\n",
      "Epoch 11068/20000 Training Loss: 0.06220274791121483\n",
      "Epoch 11069/20000 Training Loss: 0.06797947734594345\n",
      "Epoch 11070/20000 Training Loss: 0.057294443249702454\n",
      "Epoch 11070/20000 Validation Loss: 0.0715872123837471\n",
      "Epoch 11071/20000 Training Loss: 0.059519294649362564\n",
      "Epoch 11072/20000 Training Loss: 0.0733242854475975\n",
      "Epoch 11073/20000 Training Loss: 0.06331908702850342\n",
      "Epoch 11074/20000 Training Loss: 0.059850502759218216\n",
      "Epoch 11075/20000 Training Loss: 0.05499155819416046\n",
      "Epoch 11076/20000 Training Loss: 0.038590360432863235\n",
      "Epoch 11077/20000 Training Loss: 0.05409228429198265\n",
      "Epoch 11078/20000 Training Loss: 0.05864649638533592\n",
      "Epoch 11079/20000 Training Loss: 0.050622668117284775\n",
      "Epoch 11080/20000 Training Loss: 0.03741670772433281\n",
      "Epoch 11080/20000 Validation Loss: 0.07760149985551834\n",
      "Epoch 11081/20000 Training Loss: 0.05484624579548836\n",
      "Epoch 11082/20000 Training Loss: 0.07177657634019852\n",
      "Epoch 11083/20000 Training Loss: 0.06752192974090576\n",
      "Epoch 11084/20000 Training Loss: 0.05222998186945915\n",
      "Epoch 11085/20000 Training Loss: 0.07263007760047913\n",
      "Epoch 11086/20000 Training Loss: 0.049587100744247437\n",
      "Epoch 11087/20000 Training Loss: 0.06025376915931702\n",
      "Epoch 11088/20000 Training Loss: 0.0730670765042305\n",
      "Epoch 11089/20000 Training Loss: 0.06897556036710739\n",
      "Epoch 11090/20000 Training Loss: 0.05362951382994652\n",
      "Epoch 11090/20000 Validation Loss: 0.06105905771255493\n",
      "Epoch 11091/20000 Training Loss: 0.0651271715760231\n",
      "Epoch 11092/20000 Training Loss: 0.06094042956829071\n",
      "Epoch 11093/20000 Training Loss: 0.04079263284802437\n",
      "Epoch 11094/20000 Training Loss: 0.03996371850371361\n",
      "Epoch 11095/20000 Training Loss: 0.04991072416305542\n",
      "Epoch 11096/20000 Training Loss: 0.04952308163046837\n",
      "Epoch 11097/20000 Training Loss: 0.04386799409985542\n",
      "Epoch 11098/20000 Training Loss: 0.05228038504719734\n",
      "Epoch 11099/20000 Training Loss: 0.050543636083602905\n",
      "Epoch 11100/20000 Training Loss: 0.07262436300516129\n",
      "Epoch 11100/20000 Validation Loss: 0.05941605567932129\n",
      "Epoch 11101/20000 Training Loss: 0.06089219078421593\n",
      "Epoch 11102/20000 Training Loss: 0.06171678379178047\n",
      "Epoch 11103/20000 Training Loss: 0.04897375777363777\n",
      "Epoch 11104/20000 Training Loss: 0.061660993844270706\n",
      "Epoch 11105/20000 Training Loss: 0.04799237474799156\n",
      "Epoch 11106/20000 Training Loss: 0.07297690212726593\n",
      "Epoch 11107/20000 Training Loss: 0.04126923158764839\n",
      "Epoch 11108/20000 Training Loss: 0.05020275339484215\n",
      "Epoch 11109/20000 Training Loss: 0.06857845932245255\n",
      "Epoch 11110/20000 Training Loss: 0.073850117623806\n",
      "Epoch 11110/20000 Validation Loss: 0.06420014798641205\n",
      "Epoch 11111/20000 Training Loss: 0.05731143057346344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11112/20000 Training Loss: 0.04708485305309296\n",
      "Epoch 11113/20000 Training Loss: 0.04531538859009743\n",
      "Epoch 11114/20000 Training Loss: 0.04835228994488716\n",
      "Epoch 11115/20000 Training Loss: 0.0631868839263916\n",
      "Epoch 11116/20000 Training Loss: 0.07180880755186081\n",
      "Epoch 11117/20000 Training Loss: 0.07321637123823166\n",
      "Epoch 11118/20000 Training Loss: 0.04770826920866966\n",
      "Epoch 11119/20000 Training Loss: 0.060170337557792664\n",
      "Epoch 11120/20000 Training Loss: 0.048170048743486404\n",
      "Epoch 11120/20000 Validation Loss: 0.05613687261939049\n",
      "Epoch 11121/20000 Training Loss: 0.07959629595279694\n",
      "Epoch 11122/20000 Training Loss: 0.06629452109336853\n",
      "Epoch 11123/20000 Training Loss: 0.06439772993326187\n",
      "Epoch 11124/20000 Training Loss: 0.06211187317967415\n",
      "Epoch 11125/20000 Training Loss: 0.04815554991364479\n",
      "Epoch 11126/20000 Training Loss: 0.045255113393068314\n",
      "Epoch 11127/20000 Training Loss: 0.060506921261548996\n",
      "Epoch 11128/20000 Training Loss: 0.04977593943476677\n",
      "Epoch 11129/20000 Training Loss: 0.07292238622903824\n",
      "Epoch 11130/20000 Training Loss: 0.07372478395700455\n",
      "Epoch 11130/20000 Validation Loss: 0.057560473680496216\n",
      "Epoch 11131/20000 Training Loss: 0.06653944402933121\n",
      "Epoch 11132/20000 Training Loss: 0.06670178472995758\n",
      "Epoch 11133/20000 Training Loss: 0.05212785676121712\n",
      "Epoch 11134/20000 Training Loss: 0.051457349210977554\n",
      "Epoch 11135/20000 Training Loss: 0.05166850611567497\n",
      "Epoch 11136/20000 Training Loss: 0.04651424288749695\n",
      "Epoch 11137/20000 Training Loss: 0.03458169102668762\n",
      "Epoch 11138/20000 Training Loss: 0.06294576078653336\n",
      "Epoch 11139/20000 Training Loss: 0.05807317793369293\n",
      "Epoch 11140/20000 Training Loss: 0.05087248608469963\n",
      "Epoch 11140/20000 Validation Loss: 0.05672416836023331\n",
      "Epoch 11141/20000 Training Loss: 0.043569598346948624\n",
      "Epoch 11142/20000 Training Loss: 0.03673955425620079\n",
      "Epoch 11143/20000 Training Loss: 0.059988170862197876\n",
      "Epoch 11144/20000 Training Loss: 0.04141855612397194\n",
      "Epoch 11145/20000 Training Loss: 0.05377912521362305\n",
      "Epoch 11146/20000 Training Loss: 0.07170314341783524\n",
      "Epoch 11147/20000 Training Loss: 0.06888392567634583\n",
      "Epoch 11148/20000 Training Loss: 0.06152212619781494\n",
      "Epoch 11149/20000 Training Loss: 0.04434673488140106\n",
      "Epoch 11150/20000 Training Loss: 0.06237794831395149\n",
      "Epoch 11150/20000 Validation Loss: 0.06586235016584396\n",
      "Epoch 11151/20000 Training Loss: 0.061683472245931625\n",
      "Epoch 11152/20000 Training Loss: 0.04267093166708946\n",
      "Epoch 11153/20000 Training Loss: 0.05506548285484314\n",
      "Epoch 11154/20000 Training Loss: 0.06287965178489685\n",
      "Epoch 11155/20000 Training Loss: 0.07071094959974289\n",
      "Epoch 11156/20000 Training Loss: 0.04489294812083244\n",
      "Epoch 11157/20000 Training Loss: 0.05711016058921814\n",
      "Epoch 11158/20000 Training Loss: 0.0601305328309536\n",
      "Epoch 11159/20000 Training Loss: 0.07255961745977402\n",
      "Epoch 11160/20000 Training Loss: 0.04535156860947609\n",
      "Epoch 11160/20000 Validation Loss: 0.047231413424015045\n",
      "Epoch 11161/20000 Training Loss: 0.042100053280591965\n",
      "Epoch 11162/20000 Training Loss: 0.05531253293156624\n",
      "Epoch 11163/20000 Training Loss: 0.06357606500387192\n",
      "Epoch 11164/20000 Training Loss: 0.05979502573609352\n",
      "Epoch 11165/20000 Training Loss: 0.06834148615598679\n",
      "Epoch 11166/20000 Training Loss: 0.053930044174194336\n",
      "Epoch 11167/20000 Training Loss: 0.0535859577357769\n",
      "Epoch 11168/20000 Training Loss: 0.044986873865127563\n",
      "Epoch 11169/20000 Training Loss: 0.06370118260383606\n",
      "Epoch 11170/20000 Training Loss: 0.05039810761809349\n",
      "Epoch 11170/20000 Validation Loss: 0.051098503172397614\n",
      "Epoch 11171/20000 Training Loss: 0.03591367229819298\n",
      "Epoch 11172/20000 Training Loss: 0.0644279196858406\n",
      "Epoch 11173/20000 Training Loss: 0.04773915931582451\n",
      "Epoch 11174/20000 Training Loss: 0.0679004117846489\n",
      "Epoch 11175/20000 Training Loss: 0.057184409350156784\n",
      "Epoch 11176/20000 Training Loss: 0.048126161098480225\n",
      "Epoch 11177/20000 Training Loss: 0.07942274212837219\n",
      "Epoch 11178/20000 Training Loss: 0.03800610080361366\n",
      "Epoch 11179/20000 Training Loss: 0.053773630410432816\n",
      "Epoch 11180/20000 Training Loss: 0.03789497911930084\n",
      "Epoch 11180/20000 Validation Loss: 0.05944535881280899\n",
      "Epoch 11181/20000 Training Loss: 0.05670688673853874\n",
      "Epoch 11182/20000 Training Loss: 0.04591962695121765\n",
      "Epoch 11183/20000 Training Loss: 0.06562928855419159\n",
      "Epoch 11184/20000 Training Loss: 0.052922263741493225\n",
      "Epoch 11185/20000 Training Loss: 0.04588651657104492\n",
      "Epoch 11186/20000 Training Loss: 0.06091831251978874\n",
      "Epoch 11187/20000 Training Loss: 0.07339484244585037\n",
      "Epoch 11188/20000 Training Loss: 0.04812037944793701\n",
      "Epoch 11189/20000 Training Loss: 0.04540127515792847\n",
      "Epoch 11190/20000 Training Loss: 0.046339333057403564\n",
      "Epoch 11190/20000 Validation Loss: 0.07808957993984222\n",
      "Epoch 11191/20000 Training Loss: 0.04983692988753319\n",
      "Epoch 11192/20000 Training Loss: 0.07245742529630661\n",
      "Epoch 11193/20000 Training Loss: 0.04500998184084892\n",
      "Epoch 11194/20000 Training Loss: 0.05266079306602478\n",
      "Epoch 11195/20000 Training Loss: 0.05247628688812256\n",
      "Epoch 11196/20000 Training Loss: 0.05220234394073486\n",
      "Epoch 11197/20000 Training Loss: 0.05217990279197693\n",
      "Epoch 11198/20000 Training Loss: 0.04241985082626343\n",
      "Epoch 11199/20000 Training Loss: 0.04667539894580841\n",
      "Epoch 11200/20000 Training Loss: 0.06698592752218246\n",
      "Epoch 11200/20000 Validation Loss: 0.053675781935453415\n",
      "Epoch 11201/20000 Training Loss: 0.059701476246118546\n",
      "Epoch 11202/20000 Training Loss: 0.06478486210107803\n",
      "Epoch 11203/20000 Training Loss: 0.05303626134991646\n",
      "Epoch 11204/20000 Training Loss: 0.060558538883924484\n",
      "Epoch 11205/20000 Training Loss: 0.06059437617659569\n",
      "Epoch 11206/20000 Training Loss: 0.06149779632687569\n",
      "Epoch 11207/20000 Training Loss: 0.051073599606752396\n",
      "Epoch 11208/20000 Training Loss: 0.08168786764144897\n",
      "Epoch 11209/20000 Training Loss: 0.04956689476966858\n",
      "Epoch 11210/20000 Training Loss: 0.0497281439602375\n",
      "Epoch 11210/20000 Validation Loss: 0.07905590534210205\n",
      "Epoch 11211/20000 Training Loss: 0.04705558344721794\n",
      "Epoch 11212/20000 Training Loss: 0.04559951648116112\n",
      "Epoch 11213/20000 Training Loss: 0.05406017601490021\n",
      "Epoch 11214/20000 Training Loss: 0.057933270931243896\n",
      "Epoch 11215/20000 Training Loss: 0.03973478451371193\n",
      "Epoch 11216/20000 Training Loss: 0.06712774187326431\n",
      "Epoch 11217/20000 Training Loss: 0.04569673538208008\n",
      "Epoch 11218/20000 Training Loss: 0.0564369298517704\n",
      "Epoch 11219/20000 Training Loss: 0.05045056343078613\n",
      "Epoch 11220/20000 Training Loss: 0.05569152161478996\n",
      "Epoch 11220/20000 Validation Loss: 0.06823496520519257\n",
      "Epoch 11221/20000 Training Loss: 0.06117897108197212\n",
      "Epoch 11222/20000 Training Loss: 0.0551278255879879\n",
      "Epoch 11223/20000 Training Loss: 0.050524938851594925\n",
      "Epoch 11224/20000 Training Loss: 0.058218780905008316\n",
      "Epoch 11225/20000 Training Loss: 0.05902819707989693\n",
      "Epoch 11226/20000 Training Loss: 0.051146578043699265\n",
      "Epoch 11227/20000 Training Loss: 0.058199476450681686\n",
      "Epoch 11228/20000 Training Loss: 0.04833303019404411\n",
      "Epoch 11229/20000 Training Loss: 0.07223791629076004\n",
      "Epoch 11230/20000 Training Loss: 0.0452691875398159\n",
      "Epoch 11230/20000 Validation Loss: 0.05614091455936432\n",
      "Epoch 11231/20000 Training Loss: 0.06920924037694931\n",
      "Epoch 11232/20000 Training Loss: 0.055316776037216187\n",
      "Epoch 11233/20000 Training Loss: 0.049797456711530685\n",
      "Epoch 11234/20000 Training Loss: 0.0471886582672596\n",
      "Epoch 11235/20000 Training Loss: 0.06469487398862839\n",
      "Epoch 11236/20000 Training Loss: 0.05396676063537598\n",
      "Epoch 11237/20000 Training Loss: 0.05044656991958618\n",
      "Epoch 11238/20000 Training Loss: 0.0515655018389225\n",
      "Epoch 11239/20000 Training Loss: 0.06873992830514908\n",
      "Epoch 11240/20000 Training Loss: 0.058131709694862366\n",
      "Epoch 11240/20000 Validation Loss: 0.05407290160655975\n",
      "Epoch 11241/20000 Training Loss: 0.06304329633712769\n",
      "Epoch 11242/20000 Training Loss: 0.05252910777926445\n",
      "Epoch 11243/20000 Training Loss: 0.054092299193143845\n",
      "Epoch 11244/20000 Training Loss: 0.0654614046216011\n",
      "Epoch 11245/20000 Training Loss: 0.046629831194877625\n",
      "Epoch 11246/20000 Training Loss: 0.0683734118938446\n",
      "Epoch 11247/20000 Training Loss: 0.05438876152038574\n",
      "Epoch 11248/20000 Training Loss: 0.05358054116368294\n",
      "Epoch 11249/20000 Training Loss: 0.06361695379018784\n",
      "Epoch 11250/20000 Training Loss: 0.046619947999715805\n",
      "Epoch 11250/20000 Validation Loss: 0.045742619782686234\n",
      "Epoch 11251/20000 Training Loss: 0.05245829001069069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11252/20000 Training Loss: 0.05790155753493309\n",
      "Epoch 11253/20000 Training Loss: 0.05858005955815315\n",
      "Epoch 11254/20000 Training Loss: 0.04955603554844856\n",
      "Epoch 11255/20000 Training Loss: 0.04260919988155365\n",
      "Epoch 11256/20000 Training Loss: 0.04663119837641716\n",
      "Epoch 11257/20000 Training Loss: 0.05612512305378914\n",
      "Epoch 11258/20000 Training Loss: 0.07438924908638\n",
      "Epoch 11259/20000 Training Loss: 0.0518900565803051\n",
      "Epoch 11260/20000 Training Loss: 0.04999970272183418\n",
      "Epoch 11260/20000 Validation Loss: 0.04278859123587608\n",
      "Epoch 11261/20000 Training Loss: 0.047144412994384766\n",
      "Epoch 11262/20000 Training Loss: 0.05072321370244026\n",
      "Epoch 11263/20000 Training Loss: 0.05169215798377991\n",
      "Epoch 11264/20000 Training Loss: 0.043543487787246704\n",
      "Epoch 11265/20000 Training Loss: 0.05281146988272667\n",
      "Epoch 11266/20000 Training Loss: 0.05438840389251709\n",
      "Epoch 11267/20000 Training Loss: 0.054239075630903244\n",
      "Epoch 11268/20000 Training Loss: 0.04627487063407898\n",
      "Epoch 11269/20000 Training Loss: 0.06713753938674927\n",
      "Epoch 11270/20000 Training Loss: 0.05043459311127663\n",
      "Epoch 11270/20000 Validation Loss: 0.042289942502975464\n",
      "Epoch 11271/20000 Training Loss: 0.06000005081295967\n",
      "Epoch 11272/20000 Training Loss: 0.06282010674476624\n",
      "Epoch 11273/20000 Training Loss: 0.048507362604141235\n",
      "Epoch 11274/20000 Training Loss: 0.05647428333759308\n",
      "Epoch 11275/20000 Training Loss: 0.04765580967068672\n",
      "Epoch 11276/20000 Training Loss: 0.05865466967225075\n",
      "Epoch 11277/20000 Training Loss: 0.052383292466402054\n",
      "Epoch 11278/20000 Training Loss: 0.04620630666613579\n",
      "Epoch 11279/20000 Training Loss: 0.047182973474264145\n",
      "Epoch 11280/20000 Training Loss: 0.051616597920656204\n",
      "Epoch 11280/20000 Validation Loss: 0.07606817781925201\n",
      "Epoch 11281/20000 Training Loss: 0.06302651017904282\n",
      "Epoch 11282/20000 Training Loss: 0.05897282436490059\n",
      "Epoch 11283/20000 Training Loss: 0.0483257919549942\n",
      "Epoch 11284/20000 Training Loss: 0.06199239194393158\n",
      "Epoch 11285/20000 Training Loss: 0.04025431349873543\n",
      "Epoch 11286/20000 Training Loss: 0.042871370911598206\n",
      "Epoch 11287/20000 Training Loss: 0.08101662248373032\n",
      "Epoch 11288/20000 Training Loss: 0.062299590557813644\n",
      "Epoch 11289/20000 Training Loss: 0.06553927809000015\n",
      "Epoch 11290/20000 Training Loss: 0.060196489095687866\n",
      "Epoch 11290/20000 Validation Loss: 0.04716511070728302\n",
      "Epoch 11291/20000 Training Loss: 0.06683442741632462\n",
      "Epoch 11292/20000 Training Loss: 0.055604372173547745\n",
      "Epoch 11293/20000 Training Loss: 0.04550549015402794\n",
      "Epoch 11294/20000 Training Loss: 0.0789329931139946\n",
      "Epoch 11295/20000 Training Loss: 0.06687857955694199\n",
      "Epoch 11296/20000 Training Loss: 0.04158115014433861\n",
      "Epoch 11297/20000 Training Loss: 0.06645869463682175\n",
      "Epoch 11298/20000 Training Loss: 0.03925996646285057\n",
      "Epoch 11299/20000 Training Loss: 0.06429444998502731\n",
      "Epoch 11300/20000 Training Loss: 0.05235425755381584\n",
      "Epoch 11300/20000 Validation Loss: 0.03829330950975418\n",
      "Epoch 11301/20000 Training Loss: 0.0482112318277359\n",
      "Epoch 11302/20000 Training Loss: 0.05705130100250244\n",
      "Epoch 11303/20000 Training Loss: 0.055177707225084305\n",
      "Epoch 11304/20000 Training Loss: 0.06337722390890121\n",
      "Epoch 11305/20000 Training Loss: 0.05902224779129028\n",
      "Epoch 11306/20000 Training Loss: 0.05089915916323662\n",
      "Epoch 11307/20000 Training Loss: 0.04339181259274483\n",
      "Epoch 11308/20000 Training Loss: 0.05522586777806282\n",
      "Epoch 11309/20000 Training Loss: 0.051017072051763535\n",
      "Epoch 11310/20000 Training Loss: 0.05448310449719429\n",
      "Epoch 11310/20000 Validation Loss: 0.0625838041305542\n",
      "Epoch 11311/20000 Training Loss: 0.05725753307342529\n",
      "Epoch 11312/20000 Training Loss: 0.056736256927251816\n",
      "Epoch 11313/20000 Training Loss: 0.06795460730791092\n",
      "Epoch 11314/20000 Training Loss: 0.04429222270846367\n",
      "Epoch 11315/20000 Training Loss: 0.05400099232792854\n",
      "Epoch 11316/20000 Training Loss: 0.05531420186161995\n",
      "Epoch 11317/20000 Training Loss: 0.056644488126039505\n",
      "Epoch 11318/20000 Training Loss: 0.04876561835408211\n",
      "Epoch 11319/20000 Training Loss: 0.052181679755449295\n",
      "Epoch 11320/20000 Training Loss: 0.04819312319159508\n",
      "Epoch 11320/20000 Validation Loss: 0.06298717856407166\n",
      "Epoch 11321/20000 Training Loss: 0.05275445058941841\n",
      "Epoch 11322/20000 Training Loss: 0.06448600441217422\n",
      "Epoch 11323/20000 Training Loss: 0.07045512646436691\n",
      "Epoch 11324/20000 Training Loss: 0.07041259109973907\n",
      "Epoch 11325/20000 Training Loss: 0.059553418308496475\n",
      "Epoch 11326/20000 Training Loss: 0.04379471763968468\n",
      "Epoch 11327/20000 Training Loss: 0.059186603873968124\n",
      "Epoch 11328/20000 Training Loss: 0.05940943956375122\n",
      "Epoch 11329/20000 Training Loss: 0.05325216427445412\n",
      "Epoch 11330/20000 Training Loss: 0.056764837354421616\n",
      "Epoch 11330/20000 Validation Loss: 0.045134346932172775\n",
      "Epoch 11331/20000 Training Loss: 0.04924201965332031\n",
      "Epoch 11332/20000 Training Loss: 0.07163554430007935\n",
      "Epoch 11333/20000 Training Loss: 0.0666339322924614\n",
      "Epoch 11334/20000 Training Loss: 0.053453896194696426\n",
      "Epoch 11335/20000 Training Loss: 0.06253795325756073\n",
      "Epoch 11336/20000 Training Loss: 0.07764453440904617\n",
      "Epoch 11337/20000 Training Loss: 0.04777149483561516\n",
      "Epoch 11338/20000 Training Loss: 0.04162035509943962\n",
      "Epoch 11339/20000 Training Loss: 0.08672863245010376\n",
      "Epoch 11340/20000 Training Loss: 0.04038994759321213\n",
      "Epoch 11340/20000 Validation Loss: 0.06367006152868271\n",
      "Epoch 11341/20000 Training Loss: 0.046922456473112106\n",
      "Epoch 11342/20000 Training Loss: 0.05744543299078941\n",
      "Epoch 11343/20000 Training Loss: 0.05415624380111694\n",
      "Epoch 11344/20000 Training Loss: 0.057021718472242355\n",
      "Epoch 11345/20000 Training Loss: 0.041646018624305725\n",
      "Epoch 11346/20000 Training Loss: 0.04501652717590332\n",
      "Epoch 11347/20000 Training Loss: 0.0504654161632061\n",
      "Epoch 11348/20000 Training Loss: 0.0579444020986557\n",
      "Epoch 11349/20000 Training Loss: 0.05852733924984932\n",
      "Epoch 11350/20000 Training Loss: 0.06635010987520218\n",
      "Epoch 11350/20000 Validation Loss: 0.04347330331802368\n",
      "Epoch 11351/20000 Training Loss: 0.06221732497215271\n",
      "Epoch 11352/20000 Training Loss: 0.07527673989534378\n",
      "Epoch 11353/20000 Training Loss: 0.06008089706301689\n",
      "Epoch 11354/20000 Training Loss: 0.06695009022951126\n",
      "Epoch 11355/20000 Training Loss: 0.0581124909222126\n",
      "Epoch 11356/20000 Training Loss: 0.0592576265335083\n",
      "Epoch 11357/20000 Training Loss: 0.06108681485056877\n",
      "Epoch 11358/20000 Training Loss: 0.0581246055662632\n",
      "Epoch 11359/20000 Training Loss: 0.06518588960170746\n",
      "Epoch 11360/20000 Training Loss: 0.0610940121114254\n",
      "Epoch 11360/20000 Validation Loss: 0.056165844202041626\n",
      "Epoch 11361/20000 Training Loss: 0.07179196178913116\n",
      "Epoch 11362/20000 Training Loss: 0.05212053656578064\n",
      "Epoch 11363/20000 Training Loss: 0.07299920171499252\n",
      "Epoch 11364/20000 Training Loss: 0.05059728026390076\n",
      "Epoch 11365/20000 Training Loss: 0.056963000446558\n",
      "Epoch 11366/20000 Training Loss: 0.05087384209036827\n",
      "Epoch 11367/20000 Training Loss: 0.03597477450966835\n",
      "Epoch 11368/20000 Training Loss: 0.0476849265396595\n",
      "Epoch 11369/20000 Training Loss: 0.04617156460881233\n",
      "Epoch 11370/20000 Training Loss: 0.06100328266620636\n",
      "Epoch 11370/20000 Validation Loss: 0.051978178322315216\n",
      "Epoch 11371/20000 Training Loss: 0.04672301188111305\n",
      "Epoch 11372/20000 Training Loss: 0.045433755964040756\n",
      "Epoch 11373/20000 Training Loss: 0.046634018421173096\n",
      "Epoch 11374/20000 Training Loss: 0.05136534571647644\n",
      "Epoch 11375/20000 Training Loss: 0.06193919852375984\n",
      "Epoch 11376/20000 Training Loss: 0.05077100172638893\n",
      "Epoch 11377/20000 Training Loss: 0.056915562599897385\n",
      "Epoch 11378/20000 Training Loss: 0.04609553888440132\n",
      "Epoch 11379/20000 Training Loss: 0.050780683755874634\n",
      "Epoch 11380/20000 Training Loss: 0.07165173441171646\n",
      "Epoch 11380/20000 Validation Loss: 0.058884888887405396\n",
      "Epoch 11381/20000 Training Loss: 0.05931559205055237\n",
      "Epoch 11382/20000 Training Loss: 0.058185502886772156\n",
      "Epoch 11383/20000 Training Loss: 0.06361639499664307\n",
      "Epoch 11384/20000 Training Loss: 0.047213759273290634\n",
      "Epoch 11385/20000 Training Loss: 0.05413417890667915\n",
      "Epoch 11386/20000 Training Loss: 0.05430004373192787\n",
      "Epoch 11387/20000 Training Loss: 0.05758265033364296\n",
      "Epoch 11388/20000 Training Loss: 0.03758144751191139\n",
      "Epoch 11389/20000 Training Loss: 0.0628405436873436\n",
      "Epoch 11390/20000 Training Loss: 0.05497783422470093\n",
      "Epoch 11390/20000 Validation Loss: 0.060952603816986084\n",
      "Epoch 11391/20000 Training Loss: 0.0629306212067604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11392/20000 Training Loss: 0.056973982602357864\n",
      "Epoch 11393/20000 Training Loss: 0.056651726365089417\n",
      "Epoch 11394/20000 Training Loss: 0.061839211732149124\n",
      "Epoch 11395/20000 Training Loss: 0.044501420110464096\n",
      "Epoch 11396/20000 Training Loss: 0.05461032688617706\n",
      "Epoch 11397/20000 Training Loss: 0.0473594069480896\n",
      "Epoch 11398/20000 Training Loss: 0.0570748895406723\n",
      "Epoch 11399/20000 Training Loss: 0.0639503002166748\n",
      "Epoch 11400/20000 Training Loss: 0.05307107791304588\n",
      "Epoch 11400/20000 Validation Loss: 0.04077012091875076\n",
      "Epoch 11401/20000 Training Loss: 0.05586954578757286\n",
      "Epoch 11402/20000 Training Loss: 0.04220477119088173\n",
      "Epoch 11403/20000 Training Loss: 0.05821514129638672\n",
      "Epoch 11404/20000 Training Loss: 0.046230241656303406\n",
      "Epoch 11405/20000 Training Loss: 0.05889219045639038\n",
      "Epoch 11406/20000 Training Loss: 0.05640881136059761\n",
      "Epoch 11407/20000 Training Loss: 0.06636685132980347\n",
      "Epoch 11408/20000 Training Loss: 0.06292745471000671\n",
      "Epoch 11409/20000 Training Loss: 0.04532508924603462\n",
      "Epoch 11410/20000 Training Loss: 0.0694413036108017\n",
      "Epoch 11410/20000 Validation Loss: 0.07116438448429108\n",
      "Epoch 11411/20000 Training Loss: 0.06649107486009598\n",
      "Epoch 11412/20000 Training Loss: 0.07181760668754578\n",
      "Epoch 11413/20000 Training Loss: 0.055378496646881104\n",
      "Epoch 11414/20000 Training Loss: 0.07562663406133652\n",
      "Epoch 11415/20000 Training Loss: 0.048378508538007736\n",
      "Epoch 11416/20000 Training Loss: 0.05420249328017235\n",
      "Epoch 11417/20000 Training Loss: 0.046507466584444046\n",
      "Epoch 11418/20000 Training Loss: 0.05857820436358452\n",
      "Epoch 11419/20000 Training Loss: 0.045310329645872116\n",
      "Epoch 11420/20000 Training Loss: 0.07357597351074219\n",
      "Epoch 11420/20000 Validation Loss: 0.06290122866630554\n",
      "Epoch 11421/20000 Training Loss: 0.055084485560655594\n",
      "Epoch 11422/20000 Training Loss: 0.051765963435173035\n",
      "Epoch 11423/20000 Training Loss: 0.08410122245550156\n",
      "Epoch 11424/20000 Training Loss: 0.07828724384307861\n",
      "Epoch 11425/20000 Training Loss: 0.05698810890316963\n",
      "Epoch 11426/20000 Training Loss: 0.055610254406929016\n",
      "Epoch 11427/20000 Training Loss: 0.056740179657936096\n",
      "Epoch 11428/20000 Training Loss: 0.04758134484291077\n",
      "Epoch 11429/20000 Training Loss: 0.049022480845451355\n",
      "Epoch 11430/20000 Training Loss: 0.04927986487746239\n",
      "Epoch 11430/20000 Validation Loss: 0.04834793880581856\n",
      "Epoch 11431/20000 Training Loss: 0.05048326775431633\n",
      "Epoch 11432/20000 Training Loss: 0.05455051362514496\n",
      "Epoch 11433/20000 Training Loss: 0.05354461073875427\n",
      "Epoch 11434/20000 Training Loss: 0.050968438386917114\n",
      "Epoch 11435/20000 Training Loss: 0.05844072625041008\n",
      "Epoch 11436/20000 Training Loss: 0.044204533100128174\n",
      "Epoch 11437/20000 Training Loss: 0.07063097506761551\n",
      "Epoch 11438/20000 Training Loss: 0.06231961026787758\n",
      "Epoch 11439/20000 Training Loss: 0.0586971640586853\n",
      "Epoch 11440/20000 Training Loss: 0.0669727697968483\n",
      "Epoch 11440/20000 Validation Loss: 0.09028895199298859\n",
      "Epoch 11441/20000 Training Loss: 0.060459911823272705\n",
      "Epoch 11442/20000 Training Loss: 0.06942954659461975\n",
      "Epoch 11443/20000 Training Loss: 0.04488210380077362\n",
      "Epoch 11444/20000 Training Loss: 0.052870381623506546\n",
      "Epoch 11445/20000 Training Loss: 0.05416388809680939\n",
      "Epoch 11446/20000 Training Loss: 0.05565531179308891\n",
      "Epoch 11447/20000 Training Loss: 0.063960500061512\n",
      "Epoch 11448/20000 Training Loss: 0.04682822898030281\n",
      "Epoch 11449/20000 Training Loss: 0.06673850864171982\n",
      "Epoch 11450/20000 Training Loss: 0.07617589086294174\n",
      "Epoch 11450/20000 Validation Loss: 0.05199214071035385\n",
      "Epoch 11451/20000 Training Loss: 0.04766741022467613\n",
      "Epoch 11452/20000 Training Loss: 0.04248441755771637\n",
      "Epoch 11453/20000 Training Loss: 0.05163068696856499\n",
      "Epoch 11454/20000 Training Loss: 0.0756891593337059\n",
      "Epoch 11455/20000 Training Loss: 0.05009706690907478\n",
      "Epoch 11456/20000 Training Loss: 0.0536486841738224\n",
      "Epoch 11457/20000 Training Loss: 0.06019489839673042\n",
      "Epoch 11458/20000 Training Loss: 0.052823472768068314\n",
      "Epoch 11459/20000 Training Loss: 0.0602458231151104\n",
      "Epoch 11460/20000 Training Loss: 0.05340077355504036\n",
      "Epoch 11460/20000 Validation Loss: 0.042126044631004333\n",
      "Epoch 11461/20000 Training Loss: 0.05500366911292076\n",
      "Epoch 11462/20000 Training Loss: 0.04840442165732384\n",
      "Epoch 11463/20000 Training Loss: 0.0536007434129715\n",
      "Epoch 11464/20000 Training Loss: 0.05648035928606987\n",
      "Epoch 11465/20000 Training Loss: 0.0476825051009655\n",
      "Epoch 11466/20000 Training Loss: 0.042505040764808655\n",
      "Epoch 11467/20000 Training Loss: 0.06358810514211655\n",
      "Epoch 11468/20000 Training Loss: 0.07742523401975632\n",
      "Epoch 11469/20000 Training Loss: 0.046712350100278854\n",
      "Epoch 11470/20000 Training Loss: 0.060456544160842896\n",
      "Epoch 11470/20000 Validation Loss: 0.0407768115401268\n",
      "Epoch 11471/20000 Training Loss: 0.05173153057694435\n",
      "Epoch 11472/20000 Training Loss: 0.05712760612368584\n",
      "Epoch 11473/20000 Training Loss: 0.04073819890618324\n",
      "Epoch 11474/20000 Training Loss: 0.06255548447370529\n",
      "Epoch 11475/20000 Training Loss: 0.07078834623098373\n",
      "Epoch 11476/20000 Training Loss: 0.07887417823076248\n",
      "Epoch 11477/20000 Training Loss: 0.06951865553855896\n",
      "Epoch 11478/20000 Training Loss: 0.05415532365441322\n",
      "Epoch 11479/20000 Training Loss: 0.050374239683151245\n",
      "Epoch 11480/20000 Training Loss: 0.05150832235813141\n",
      "Epoch 11480/20000 Validation Loss: 0.040496453642845154\n",
      "Epoch 11481/20000 Training Loss: 0.03493442386388779\n",
      "Epoch 11482/20000 Training Loss: 0.0459015853703022\n",
      "Epoch 11483/20000 Training Loss: 0.05032482370734215\n",
      "Epoch 11484/20000 Training Loss: 0.05279650166630745\n",
      "Epoch 11485/20000 Training Loss: 0.054604124277830124\n",
      "Epoch 11486/20000 Training Loss: 0.06773299723863602\n",
      "Epoch 11487/20000 Training Loss: 0.0717531070113182\n",
      "Epoch 11488/20000 Training Loss: 0.07683567702770233\n",
      "Epoch 11489/20000 Training Loss: 0.04488752409815788\n",
      "Epoch 11490/20000 Training Loss: 0.05380775034427643\n",
      "Epoch 11490/20000 Validation Loss: 0.06386084854602814\n",
      "Epoch 11491/20000 Training Loss: 0.07032108306884766\n",
      "Epoch 11492/20000 Training Loss: 0.047084420919418335\n",
      "Epoch 11493/20000 Training Loss: 0.07395658642053604\n",
      "Epoch 11494/20000 Training Loss: 0.051947593688964844\n",
      "Epoch 11495/20000 Training Loss: 0.04950065538287163\n",
      "Epoch 11496/20000 Training Loss: 0.06515097618103027\n",
      "Epoch 11497/20000 Training Loss: 0.06469020247459412\n",
      "Epoch 11498/20000 Training Loss: 0.05308172106742859\n",
      "Epoch 11499/20000 Training Loss: 0.06393884867429733\n",
      "Epoch 11500/20000 Training Loss: 0.048684012144804\n",
      "Epoch 11500/20000 Validation Loss: 0.07770179957151413\n",
      "Epoch 11501/20000 Training Loss: 0.03696107864379883\n",
      "Epoch 11502/20000 Training Loss: 0.049589116126298904\n",
      "Epoch 11503/20000 Training Loss: 0.05408923700451851\n",
      "Epoch 11504/20000 Training Loss: 0.04990018531680107\n",
      "Epoch 11505/20000 Training Loss: 0.0612378865480423\n",
      "Epoch 11506/20000 Training Loss: 0.03236972540616989\n",
      "Epoch 11507/20000 Training Loss: 0.08240842819213867\n",
      "Epoch 11508/20000 Training Loss: 0.037874698638916016\n",
      "Epoch 11509/20000 Training Loss: 0.05305277183651924\n",
      "Epoch 11510/20000 Training Loss: 0.04668125510215759\n",
      "Epoch 11510/20000 Validation Loss: 0.042958684265613556\n",
      "Epoch 11511/20000 Training Loss: 0.05411927029490471\n",
      "Epoch 11512/20000 Training Loss: 0.05269791558384895\n",
      "Epoch 11513/20000 Training Loss: 0.04611886665225029\n",
      "Epoch 11514/20000 Training Loss: 0.04504087194800377\n",
      "Epoch 11515/20000 Training Loss: 0.06737358123064041\n",
      "Epoch 11516/20000 Training Loss: 0.06764206290245056\n",
      "Epoch 11517/20000 Training Loss: 0.05856136605143547\n",
      "Epoch 11518/20000 Training Loss: 0.07108867913484573\n",
      "Epoch 11519/20000 Training Loss: 0.04292292520403862\n",
      "Epoch 11520/20000 Training Loss: 0.051289152354002\n",
      "Epoch 11520/20000 Validation Loss: 0.05887513607740402\n",
      "Epoch 11521/20000 Training Loss: 0.06405366212129593\n",
      "Epoch 11522/20000 Training Loss: 0.0526549331843853\n",
      "Epoch 11523/20000 Training Loss: 0.044783275574445724\n",
      "Epoch 11524/20000 Training Loss: 0.044659148901700974\n",
      "Epoch 11525/20000 Training Loss: 0.04708757996559143\n",
      "Epoch 11526/20000 Training Loss: 0.06521889567375183\n",
      "Epoch 11527/20000 Training Loss: 0.03994857147336006\n",
      "Epoch 11528/20000 Training Loss: 0.05360760912299156\n",
      "Epoch 11529/20000 Training Loss: 0.05737638846039772\n",
      "Epoch 11530/20000 Training Loss: 0.06462869048118591\n",
      "Epoch 11530/20000 Validation Loss: 0.046631306409835815\n",
      "Epoch 11531/20000 Training Loss: 0.04371199011802673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11532/20000 Training Loss: 0.052195172756910324\n",
      "Epoch 11533/20000 Training Loss: 0.0470343716442585\n",
      "Epoch 11534/20000 Training Loss: 0.045675475150346756\n",
      "Epoch 11535/20000 Training Loss: 0.07783720642328262\n",
      "Epoch 11536/20000 Training Loss: 0.05767315998673439\n",
      "Epoch 11537/20000 Training Loss: 0.0481574684381485\n",
      "Epoch 11538/20000 Training Loss: 0.04518986865878105\n",
      "Epoch 11539/20000 Training Loss: 0.06599859148263931\n",
      "Epoch 11540/20000 Training Loss: 0.05182035639882088\n",
      "Epoch 11540/20000 Validation Loss: 0.04992981255054474\n",
      "Epoch 11541/20000 Training Loss: 0.051099807024002075\n",
      "Epoch 11542/20000 Training Loss: 0.06957384943962097\n",
      "Epoch 11543/20000 Training Loss: 0.061003684997558594\n",
      "Epoch 11544/20000 Training Loss: 0.05006510019302368\n",
      "Epoch 11545/20000 Training Loss: 0.053365278989076614\n",
      "Epoch 11546/20000 Training Loss: 0.0618041455745697\n",
      "Epoch 11547/20000 Training Loss: 0.059401314705610275\n",
      "Epoch 11548/20000 Training Loss: 0.05192328989505768\n",
      "Epoch 11549/20000 Training Loss: 0.04576127603650093\n",
      "Epoch 11550/20000 Training Loss: 0.05194823071360588\n",
      "Epoch 11550/20000 Validation Loss: 0.04140566661953926\n",
      "Epoch 11551/20000 Training Loss: 0.04542754217982292\n",
      "Epoch 11552/20000 Training Loss: 0.050032246857881546\n",
      "Epoch 11553/20000 Training Loss: 0.06067525967955589\n",
      "Epoch 11554/20000 Training Loss: 0.04390847682952881\n",
      "Epoch 11555/20000 Training Loss: 0.06828713417053223\n",
      "Epoch 11556/20000 Training Loss: 0.052747879177331924\n",
      "Epoch 11557/20000 Training Loss: 0.05624692142009735\n",
      "Epoch 11558/20000 Training Loss: 0.04817687347531319\n",
      "Epoch 11559/20000 Training Loss: 0.048857320100069046\n",
      "Epoch 11560/20000 Training Loss: 0.04629779979586601\n",
      "Epoch 11560/20000 Validation Loss: 0.06273716688156128\n",
      "Epoch 11561/20000 Training Loss: 0.05190819874405861\n",
      "Epoch 11562/20000 Training Loss: 0.0539776049554348\n",
      "Epoch 11563/20000 Training Loss: 0.07048943638801575\n",
      "Epoch 11564/20000 Training Loss: 0.0676426887512207\n",
      "Epoch 11565/20000 Training Loss: 0.06161416694521904\n",
      "Epoch 11566/20000 Training Loss: 0.05743977054953575\n",
      "Epoch 11567/20000 Training Loss: 0.05199689790606499\n",
      "Epoch 11568/20000 Training Loss: 0.051367729902267456\n",
      "Epoch 11569/20000 Training Loss: 0.052482616156339645\n",
      "Epoch 11570/20000 Training Loss: 0.04180651530623436\n",
      "Epoch 11570/20000 Validation Loss: 0.04143727943301201\n",
      "Epoch 11571/20000 Training Loss: 0.07369719445705414\n",
      "Epoch 11572/20000 Training Loss: 0.05927758291363716\n",
      "Epoch 11573/20000 Training Loss: 0.07025245577096939\n",
      "Epoch 11574/20000 Training Loss: 0.0479133166372776\n",
      "Epoch 11575/20000 Training Loss: 0.03634827956557274\n",
      "Epoch 11576/20000 Training Loss: 0.04319528862833977\n",
      "Epoch 11577/20000 Training Loss: 0.0587935745716095\n",
      "Epoch 11578/20000 Training Loss: 0.06181247904896736\n",
      "Epoch 11579/20000 Training Loss: 0.05727461352944374\n",
      "Epoch 11580/20000 Training Loss: 0.05460474267601967\n",
      "Epoch 11580/20000 Validation Loss: 0.04388292878866196\n",
      "Epoch 11581/20000 Training Loss: 0.06625710427761078\n",
      "Epoch 11582/20000 Training Loss: 0.049086932092905045\n",
      "Epoch 11583/20000 Training Loss: 0.05856435373425484\n",
      "Epoch 11584/20000 Training Loss: 0.05498909577727318\n",
      "Epoch 11585/20000 Training Loss: 0.05724026635289192\n",
      "Epoch 11586/20000 Training Loss: 0.07089139521121979\n",
      "Epoch 11587/20000 Training Loss: 0.05933203175663948\n",
      "Epoch 11588/20000 Training Loss: 0.054738450795412064\n",
      "Epoch 11589/20000 Training Loss: 0.04407781362533569\n",
      "Epoch 11590/20000 Training Loss: 0.066319040954113\n",
      "Epoch 11590/20000 Validation Loss: 0.0697314590215683\n",
      "Epoch 11591/20000 Training Loss: 0.05683603882789612\n",
      "Epoch 11592/20000 Training Loss: 0.05490265414118767\n",
      "Epoch 11593/20000 Training Loss: 0.056312527507543564\n",
      "Epoch 11594/20000 Training Loss: 0.07186426967382431\n",
      "Epoch 11595/20000 Training Loss: 0.048497170209884644\n",
      "Epoch 11596/20000 Training Loss: 0.048777271062135696\n",
      "Epoch 11597/20000 Training Loss: 0.04751778766512871\n",
      "Epoch 11598/20000 Training Loss: 0.055385034531354904\n",
      "Epoch 11599/20000 Training Loss: 0.049498092383146286\n",
      "Epoch 11600/20000 Training Loss: 0.04716792330145836\n",
      "Epoch 11600/20000 Validation Loss: 0.06534072011709213\n",
      "Epoch 11601/20000 Training Loss: 0.03963722288608551\n",
      "Epoch 11602/20000 Training Loss: 0.07660522311925888\n",
      "Epoch 11603/20000 Training Loss: 0.06498843431472778\n",
      "Epoch 11604/20000 Training Loss: 0.04252904653549194\n",
      "Epoch 11605/20000 Training Loss: 0.03998783230781555\n",
      "Epoch 11606/20000 Training Loss: 0.06307270377874374\n",
      "Epoch 11607/20000 Training Loss: 0.04773610457777977\n",
      "Epoch 11608/20000 Training Loss: 0.05083194002509117\n",
      "Epoch 11609/20000 Training Loss: 0.05294766649603844\n",
      "Epoch 11610/20000 Training Loss: 0.04373915120959282\n",
      "Epoch 11610/20000 Validation Loss: 0.06562556326389313\n",
      "Epoch 11611/20000 Training Loss: 0.0611162930727005\n",
      "Epoch 11612/20000 Training Loss: 0.04949513077735901\n",
      "Epoch 11613/20000 Training Loss: 0.05160020664334297\n",
      "Epoch 11614/20000 Training Loss: 0.046714674681425095\n",
      "Epoch 11615/20000 Training Loss: 0.04706690087914467\n",
      "Epoch 11616/20000 Training Loss: 0.0647922083735466\n",
      "Epoch 11617/20000 Training Loss: 0.05079009011387825\n",
      "Epoch 11618/20000 Training Loss: 0.05723630264401436\n",
      "Epoch 11619/20000 Training Loss: 0.053221914917230606\n",
      "Epoch 11620/20000 Training Loss: 0.050104934722185135\n",
      "Epoch 11620/20000 Validation Loss: 0.05839552730321884\n",
      "Epoch 11621/20000 Training Loss: 0.044579628854990005\n",
      "Epoch 11622/20000 Training Loss: 0.04404987394809723\n",
      "Epoch 11623/20000 Training Loss: 0.04540593922138214\n",
      "Epoch 11624/20000 Training Loss: 0.05864667892456055\n",
      "Epoch 11625/20000 Training Loss: 0.04802149161696434\n",
      "Epoch 11626/20000 Training Loss: 0.0526396743953228\n",
      "Epoch 11627/20000 Training Loss: 0.059790994971990585\n",
      "Epoch 11628/20000 Training Loss: 0.0663488507270813\n",
      "Epoch 11629/20000 Training Loss: 0.05472003296017647\n",
      "Epoch 11630/20000 Training Loss: 0.05861757695674896\n",
      "Epoch 11630/20000 Validation Loss: 0.05706574395298958\n",
      "Epoch 11631/20000 Training Loss: 0.04651975259184837\n",
      "Epoch 11632/20000 Training Loss: 0.04396643862128258\n",
      "Epoch 11633/20000 Training Loss: 0.06334283202886581\n",
      "Epoch 11634/20000 Training Loss: 0.05125124379992485\n",
      "Epoch 11635/20000 Training Loss: 0.04723218083381653\n",
      "Epoch 11636/20000 Training Loss: 0.06975030899047852\n",
      "Epoch 11637/20000 Training Loss: 0.061071883887052536\n",
      "Epoch 11638/20000 Training Loss: 0.051088303327560425\n",
      "Epoch 11639/20000 Training Loss: 0.047847092151641846\n",
      "Epoch 11640/20000 Training Loss: 0.045977119356393814\n",
      "Epoch 11640/20000 Validation Loss: 0.053772054612636566\n",
      "Epoch 11641/20000 Training Loss: 0.03970581293106079\n",
      "Epoch 11642/20000 Training Loss: 0.044292569160461426\n",
      "Epoch 11643/20000 Training Loss: 0.058578457683324814\n",
      "Epoch 11644/20000 Training Loss: 0.056241463869810104\n",
      "Epoch 11645/20000 Training Loss: 0.06118367239832878\n",
      "Epoch 11646/20000 Training Loss: 0.04250941798090935\n",
      "Epoch 11647/20000 Training Loss: 0.045524027198553085\n",
      "Epoch 11648/20000 Training Loss: 0.04630051180720329\n",
      "Epoch 11649/20000 Training Loss: 0.04790050908923149\n",
      "Epoch 11650/20000 Training Loss: 0.06043278053402901\n",
      "Epoch 11650/20000 Validation Loss: 0.04265216737985611\n",
      "Epoch 11651/20000 Training Loss: 0.050443705171346664\n",
      "Epoch 11652/20000 Training Loss: 0.04124593734741211\n",
      "Epoch 11653/20000 Training Loss: 0.07151108980178833\n",
      "Epoch 11654/20000 Training Loss: 0.04606093466281891\n",
      "Epoch 11655/20000 Training Loss: 0.04351554438471794\n",
      "Epoch 11656/20000 Training Loss: 0.07162372022867203\n",
      "Epoch 11657/20000 Training Loss: 0.05197663977742195\n",
      "Epoch 11658/20000 Training Loss: 0.05509647727012634\n",
      "Epoch 11659/20000 Training Loss: 0.055602073669433594\n",
      "Epoch 11660/20000 Training Loss: 0.08502285927534103\n",
      "Epoch 11660/20000 Validation Loss: 0.07487985491752625\n",
      "Epoch 11661/20000 Training Loss: 0.03866110369563103\n",
      "Epoch 11662/20000 Training Loss: 0.06463619321584702\n",
      "Epoch 11663/20000 Training Loss: 0.040237341076135635\n",
      "Epoch 11664/20000 Training Loss: 0.04874330759048462\n",
      "Epoch 11665/20000 Training Loss: 0.05163492634892464\n",
      "Epoch 11666/20000 Training Loss: 0.05088190734386444\n",
      "Epoch 11667/20000 Training Loss: 0.0381234735250473\n",
      "Epoch 11668/20000 Training Loss: 0.05243586376309395\n",
      "Epoch 11669/20000 Training Loss: 0.054806578904390335\n",
      "Epoch 11670/20000 Training Loss: 0.06668143719434738\n",
      "Epoch 11670/20000 Validation Loss: 0.10087752342224121\n",
      "Epoch 11671/20000 Training Loss: 0.04979630187153816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11672/20000 Training Loss: 0.06901220232248306\n",
      "Epoch 11673/20000 Training Loss: 0.056541621685028076\n",
      "Epoch 11674/20000 Training Loss: 0.0434938482940197\n",
      "Epoch 11675/20000 Training Loss: 0.06557376682758331\n",
      "Epoch 11676/20000 Training Loss: 0.048297345638275146\n",
      "Epoch 11677/20000 Training Loss: 0.06460344046354294\n",
      "Epoch 11678/20000 Training Loss: 0.04614264890551567\n",
      "Epoch 11679/20000 Training Loss: 0.04559128358960152\n",
      "Epoch 11680/20000 Training Loss: 0.04962934926152229\n",
      "Epoch 11680/20000 Validation Loss: 0.047343909740448\n",
      "Epoch 11681/20000 Training Loss: 0.04941399022936821\n",
      "Epoch 11682/20000 Training Loss: 0.05520227178931236\n",
      "Epoch 11683/20000 Training Loss: 0.056072723120450974\n",
      "Epoch 11684/20000 Training Loss: 0.05379300191998482\n",
      "Epoch 11685/20000 Training Loss: 0.056096047163009644\n",
      "Epoch 11686/20000 Training Loss: 0.06004562973976135\n",
      "Epoch 11687/20000 Training Loss: 0.05262260511517525\n",
      "Epoch 11688/20000 Training Loss: 0.03878007456660271\n",
      "Epoch 11689/20000 Training Loss: 0.058657482266426086\n",
      "Epoch 11690/20000 Training Loss: 0.05324321612715721\n",
      "Epoch 11690/20000 Validation Loss: 0.06111155450344086\n",
      "Epoch 11691/20000 Training Loss: 0.04938478395342827\n",
      "Epoch 11692/20000 Training Loss: 0.04518389701843262\n",
      "Epoch 11693/20000 Training Loss: 0.05177679657936096\n",
      "Epoch 11694/20000 Training Loss: 0.06341549754142761\n",
      "Epoch 11695/20000 Training Loss: 0.044916048645973206\n",
      "Epoch 11696/20000 Training Loss: 0.04694126173853874\n",
      "Epoch 11697/20000 Training Loss: 0.06114314869046211\n",
      "Epoch 11698/20000 Training Loss: 0.057709041982889175\n",
      "Epoch 11699/20000 Training Loss: 0.06221204996109009\n",
      "Epoch 11700/20000 Training Loss: 0.07026845961809158\n",
      "Epoch 11700/20000 Validation Loss: 0.05223117023706436\n",
      "Epoch 11701/20000 Training Loss: 0.05351952835917473\n",
      "Epoch 11702/20000 Training Loss: 0.04801466688513756\n",
      "Epoch 11703/20000 Training Loss: 0.06221973896026611\n",
      "Epoch 11704/20000 Training Loss: 0.07884380221366882\n",
      "Epoch 11705/20000 Training Loss: 0.06434355676174164\n",
      "Epoch 11706/20000 Training Loss: 0.04549789056181908\n",
      "Epoch 11707/20000 Training Loss: 0.053097594529390335\n",
      "Epoch 11708/20000 Training Loss: 0.04999466612935066\n",
      "Epoch 11709/20000 Training Loss: 0.05101434513926506\n",
      "Epoch 11710/20000 Training Loss: 0.05080065503716469\n",
      "Epoch 11710/20000 Validation Loss: 0.06223361939191818\n",
      "Epoch 11711/20000 Training Loss: 0.06847741454839706\n",
      "Epoch 11712/20000 Training Loss: 0.07245296239852905\n",
      "Epoch 11713/20000 Training Loss: 0.07919841259717941\n",
      "Epoch 11714/20000 Training Loss: 0.0571751706302166\n",
      "Epoch 11715/20000 Training Loss: 0.05186743661761284\n",
      "Epoch 11716/20000 Training Loss: 0.04857894778251648\n",
      "Epoch 11717/20000 Training Loss: 0.05881304666399956\n",
      "Epoch 11718/20000 Training Loss: 0.06078122928738594\n",
      "Epoch 11719/20000 Training Loss: 0.053464680910110474\n",
      "Epoch 11720/20000 Training Loss: 0.06433220952749252\n",
      "Epoch 11720/20000 Validation Loss: 0.06709253042936325\n",
      "Epoch 11721/20000 Training Loss: 0.047189515084028244\n",
      "Epoch 11722/20000 Training Loss: 0.05827411636710167\n",
      "Epoch 11723/20000 Training Loss: 0.05734102800488472\n",
      "Epoch 11724/20000 Training Loss: 0.07280421257019043\n",
      "Epoch 11725/20000 Training Loss: 0.060222793370485306\n",
      "Epoch 11726/20000 Training Loss: 0.054315224289894104\n",
      "Epoch 11727/20000 Training Loss: 0.0441008023917675\n",
      "Epoch 11728/20000 Training Loss: 0.06772980093955994\n",
      "Epoch 11729/20000 Training Loss: 0.06036683917045593\n",
      "Epoch 11730/20000 Training Loss: 0.07184823602437973\n",
      "Epoch 11730/20000 Validation Loss: 0.05510706454515457\n",
      "Epoch 11731/20000 Training Loss: 0.057564735412597656\n",
      "Epoch 11732/20000 Training Loss: 0.04802834987640381\n",
      "Epoch 11733/20000 Training Loss: 0.04787290468811989\n",
      "Epoch 11734/20000 Training Loss: 0.04402850195765495\n",
      "Epoch 11735/20000 Training Loss: 0.045327913016080856\n",
      "Epoch 11736/20000 Training Loss: 0.05869703367352486\n",
      "Epoch 11737/20000 Training Loss: 0.042284656316041946\n",
      "Epoch 11738/20000 Training Loss: 0.044511109590530396\n",
      "Epoch 11739/20000 Training Loss: 0.07419709861278534\n",
      "Epoch 11740/20000 Training Loss: 0.061529383063316345\n",
      "Epoch 11740/20000 Validation Loss: 0.06432919949293137\n",
      "Epoch 11741/20000 Training Loss: 0.06804419308900833\n",
      "Epoch 11742/20000 Training Loss: 0.045162707567214966\n",
      "Epoch 11743/20000 Training Loss: 0.0469292588531971\n",
      "Epoch 11744/20000 Training Loss: 0.06164467707276344\n",
      "Epoch 11745/20000 Training Loss: 0.08819272369146347\n",
      "Epoch 11746/20000 Training Loss: 0.05110502615571022\n",
      "Epoch 11747/20000 Training Loss: 0.056050270795822144\n",
      "Epoch 11748/20000 Training Loss: 0.05396580323576927\n",
      "Epoch 11749/20000 Training Loss: 0.05141705647110939\n",
      "Epoch 11750/20000 Training Loss: 0.07187499850988388\n",
      "Epoch 11750/20000 Validation Loss: 0.06345823407173157\n",
      "Epoch 11751/20000 Training Loss: 0.06176857277750969\n",
      "Epoch 11752/20000 Training Loss: 0.0663285180926323\n",
      "Epoch 11753/20000 Training Loss: 0.0640208050608635\n",
      "Epoch 11754/20000 Training Loss: 0.07186723500490189\n",
      "Epoch 11755/20000 Training Loss: 0.05075669661164284\n",
      "Epoch 11756/20000 Training Loss: 0.04880838468670845\n",
      "Epoch 11757/20000 Training Loss: 0.06336155533790588\n",
      "Epoch 11758/20000 Training Loss: 0.06007868051528931\n",
      "Epoch 11759/20000 Training Loss: 0.0503716915845871\n",
      "Epoch 11760/20000 Training Loss: 0.03583558276295662\n",
      "Epoch 11760/20000 Validation Loss: 0.07125498354434967\n",
      "Epoch 11761/20000 Training Loss: 0.055230144411325455\n",
      "Epoch 11762/20000 Training Loss: 0.041605472564697266\n",
      "Epoch 11763/20000 Training Loss: 0.03926979377865791\n",
      "Epoch 11764/20000 Training Loss: 0.054590728133916855\n",
      "Epoch 11765/20000 Training Loss: 0.06252496689558029\n",
      "Epoch 11766/20000 Training Loss: 0.04001208022236824\n",
      "Epoch 11767/20000 Training Loss: 0.0527726374566555\n",
      "Epoch 11768/20000 Training Loss: 0.05279051885008812\n",
      "Epoch 11769/20000 Training Loss: 0.07155822962522507\n",
      "Epoch 11770/20000 Training Loss: 0.052620213478803635\n",
      "Epoch 11770/20000 Validation Loss: 0.05100321024656296\n",
      "Epoch 11771/20000 Training Loss: 0.048816412687301636\n",
      "Epoch 11772/20000 Training Loss: 0.03768062964081764\n",
      "Epoch 11773/20000 Training Loss: 0.057551246136426926\n",
      "Epoch 11774/20000 Training Loss: 0.061397138983011246\n",
      "Epoch 11775/20000 Training Loss: 0.05994755029678345\n",
      "Epoch 11776/20000 Training Loss: 0.07101446390151978\n",
      "Epoch 11777/20000 Training Loss: 0.06539886444807053\n",
      "Epoch 11778/20000 Training Loss: 0.07123979181051254\n",
      "Epoch 11779/20000 Training Loss: 0.050861191004514694\n",
      "Epoch 11780/20000 Training Loss: 0.04653926193714142\n",
      "Epoch 11780/20000 Validation Loss: 0.054890066385269165\n",
      "Epoch 11781/20000 Training Loss: 0.05396587774157524\n",
      "Epoch 11782/20000 Training Loss: 0.053740814328193665\n",
      "Epoch 11783/20000 Training Loss: 0.06823766976594925\n",
      "Epoch 11784/20000 Training Loss: 0.04294772818684578\n",
      "Epoch 11785/20000 Training Loss: 0.04415223002433777\n",
      "Epoch 11786/20000 Training Loss: 0.07072519510984421\n",
      "Epoch 11787/20000 Training Loss: 0.07580479234457016\n",
      "Epoch 11788/20000 Training Loss: 0.06710747629404068\n",
      "Epoch 11789/20000 Training Loss: 0.04759962484240532\n",
      "Epoch 11790/20000 Training Loss: 0.05316697061061859\n",
      "Epoch 11790/20000 Validation Loss: 0.06249920278787613\n",
      "Epoch 11791/20000 Training Loss: 0.04678301140666008\n",
      "Epoch 11792/20000 Training Loss: 0.041573766618967056\n",
      "Epoch 11793/20000 Training Loss: 0.05348363146185875\n",
      "Epoch 11794/20000 Training Loss: 0.059942539781332016\n",
      "Epoch 11795/20000 Training Loss: 0.06435664743185043\n",
      "Epoch 11796/20000 Training Loss: 0.04627387598156929\n",
      "Epoch 11797/20000 Training Loss: 0.05817605182528496\n",
      "Epoch 11798/20000 Training Loss: 0.05598204955458641\n",
      "Epoch 11799/20000 Training Loss: 0.06445622444152832\n",
      "Epoch 11800/20000 Training Loss: 0.04790302738547325\n",
      "Epoch 11800/20000 Validation Loss: 0.08274972438812256\n",
      "Epoch 11801/20000 Training Loss: 0.05173127353191376\n",
      "Epoch 11802/20000 Training Loss: 0.06596038490533829\n",
      "Epoch 11803/20000 Training Loss: 0.04529900848865509\n",
      "Epoch 11804/20000 Training Loss: 0.06262459605932236\n",
      "Epoch 11805/20000 Training Loss: 0.05535009503364563\n",
      "Epoch 11806/20000 Training Loss: 0.05919622257351875\n",
      "Epoch 11807/20000 Training Loss: 0.04289795830845833\n",
      "Epoch 11808/20000 Training Loss: 0.04846423491835594\n",
      "Epoch 11809/20000 Training Loss: 0.077804796397686\n",
      "Epoch 11810/20000 Training Loss: 0.049377892166376114\n",
      "Epoch 11810/20000 Validation Loss: 0.0430544838309288\n",
      "Epoch 11811/20000 Training Loss: 0.059876058250665665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11812/20000 Training Loss: 0.04995863139629364\n",
      "Epoch 11813/20000 Training Loss: 0.06877592951059341\n",
      "Epoch 11814/20000 Training Loss: 0.053900644183158875\n",
      "Epoch 11815/20000 Training Loss: 0.05450999736785889\n",
      "Epoch 11816/20000 Training Loss: 0.044127147644758224\n",
      "Epoch 11817/20000 Training Loss: 0.044269587844610214\n",
      "Epoch 11818/20000 Training Loss: 0.04491744562983513\n",
      "Epoch 11819/20000 Training Loss: 0.06361565738916397\n",
      "Epoch 11820/20000 Training Loss: 0.05474410951137543\n",
      "Epoch 11820/20000 Validation Loss: 0.06901545077562332\n",
      "Epoch 11821/20000 Training Loss: 0.06484705209732056\n",
      "Epoch 11822/20000 Training Loss: 0.043858956545591354\n",
      "Epoch 11823/20000 Training Loss: 0.04695873335003853\n",
      "Epoch 11824/20000 Training Loss: 0.048713307827711105\n",
      "Epoch 11825/20000 Training Loss: 0.05909829959273338\n",
      "Epoch 11826/20000 Training Loss: 0.07166741043329239\n",
      "Epoch 11827/20000 Training Loss: 0.04728548228740692\n",
      "Epoch 11828/20000 Training Loss: 0.06025729700922966\n",
      "Epoch 11829/20000 Training Loss: 0.05623540282249451\n",
      "Epoch 11830/20000 Training Loss: 0.05600835755467415\n",
      "Epoch 11830/20000 Validation Loss: 0.06427544355392456\n",
      "Epoch 11831/20000 Training Loss: 0.05447499081492424\n",
      "Epoch 11832/20000 Training Loss: 0.06296684592962265\n",
      "Epoch 11833/20000 Training Loss: 0.048689525574445724\n",
      "Epoch 11834/20000 Training Loss: 0.06764858216047287\n",
      "Epoch 11835/20000 Training Loss: 0.047369807958602905\n",
      "Epoch 11836/20000 Training Loss: 0.042177800089120865\n",
      "Epoch 11837/20000 Training Loss: 0.059464868158102036\n",
      "Epoch 11838/20000 Training Loss: 0.0555090457201004\n",
      "Epoch 11839/20000 Training Loss: 0.058684688061475754\n",
      "Epoch 11840/20000 Training Loss: 0.06272447854280472\n",
      "Epoch 11840/20000 Validation Loss: 0.05072023347020149\n",
      "Epoch 11841/20000 Training Loss: 0.051749955862760544\n",
      "Epoch 11842/20000 Training Loss: 0.05054342746734619\n",
      "Epoch 11843/20000 Training Loss: 0.060399770736694336\n",
      "Epoch 11844/20000 Training Loss: 0.07126584649085999\n",
      "Epoch 11845/20000 Training Loss: 0.06824774295091629\n",
      "Epoch 11846/20000 Training Loss: 0.046056997030973434\n",
      "Epoch 11847/20000 Training Loss: 0.04308154061436653\n",
      "Epoch 11848/20000 Training Loss: 0.06148134171962738\n",
      "Epoch 11849/20000 Training Loss: 0.0665774717926979\n",
      "Epoch 11850/20000 Training Loss: 0.06663426756858826\n",
      "Epoch 11850/20000 Validation Loss: 0.04231834411621094\n",
      "Epoch 11851/20000 Training Loss: 0.046929728239774704\n",
      "Epoch 11852/20000 Training Loss: 0.06295106559991837\n",
      "Epoch 11853/20000 Training Loss: 0.057643529027700424\n",
      "Epoch 11854/20000 Training Loss: 0.06015492603182793\n",
      "Epoch 11855/20000 Training Loss: 0.04552459344267845\n",
      "Epoch 11856/20000 Training Loss: 0.07104483991861343\n",
      "Epoch 11857/20000 Training Loss: 0.057886991649866104\n",
      "Epoch 11858/20000 Training Loss: 0.052700940519571304\n",
      "Epoch 11859/20000 Training Loss: 0.04437578096985817\n",
      "Epoch 11860/20000 Training Loss: 0.04666794463992119\n",
      "Epoch 11860/20000 Validation Loss: 0.04116940498352051\n",
      "Epoch 11861/20000 Training Loss: 0.06280051916837692\n",
      "Epoch 11862/20000 Training Loss: 0.05262569710612297\n",
      "Epoch 11863/20000 Training Loss: 0.05883621796965599\n",
      "Epoch 11864/20000 Training Loss: 0.062346816062927246\n",
      "Epoch 11865/20000 Training Loss: 0.049465131014585495\n",
      "Epoch 11866/20000 Training Loss: 0.05033278465270996\n",
      "Epoch 11867/20000 Training Loss: 0.04446806013584137\n",
      "Epoch 11868/20000 Training Loss: 0.059389665722846985\n",
      "Epoch 11869/20000 Training Loss: 0.05449738726019859\n",
      "Epoch 11870/20000 Training Loss: 0.058023858815431595\n",
      "Epoch 11870/20000 Validation Loss: 0.04237807169556618\n",
      "Epoch 11871/20000 Training Loss: 0.04267580807209015\n",
      "Epoch 11872/20000 Training Loss: 0.057720739394426346\n",
      "Epoch 11873/20000 Training Loss: 0.06207248568534851\n",
      "Epoch 11874/20000 Training Loss: 0.040362969040870667\n",
      "Epoch 11875/20000 Training Loss: 0.046731676906347275\n",
      "Epoch 11876/20000 Training Loss: 0.05893443897366524\n",
      "Epoch 11877/20000 Training Loss: 0.052662868052721024\n",
      "Epoch 11878/20000 Training Loss: 0.06083788350224495\n",
      "Epoch 11879/20000 Training Loss: 0.058912504464387894\n",
      "Epoch 11880/20000 Training Loss: 0.048396676778793335\n",
      "Epoch 11880/20000 Validation Loss: 0.0986512154340744\n",
      "Epoch 11881/20000 Training Loss: 0.06478189677000046\n",
      "Epoch 11882/20000 Training Loss: 0.06573428958654404\n",
      "Epoch 11883/20000 Training Loss: 0.06695282459259033\n",
      "Epoch 11884/20000 Training Loss: 0.057144645601511\n",
      "Epoch 11885/20000 Training Loss: 0.05435493588447571\n",
      "Epoch 11886/20000 Training Loss: 0.064097099006176\n",
      "Epoch 11887/20000 Training Loss: 0.04923684522509575\n",
      "Epoch 11888/20000 Training Loss: 0.05158792808651924\n",
      "Epoch 11889/20000 Training Loss: 0.05668230727314949\n",
      "Epoch 11890/20000 Training Loss: 0.060237202793359756\n",
      "Epoch 11890/20000 Validation Loss: 0.05883602797985077\n",
      "Epoch 11891/20000 Training Loss: 0.061137694865465164\n",
      "Epoch 11892/20000 Training Loss: 0.04047713801264763\n",
      "Epoch 11893/20000 Training Loss: 0.047117382287979126\n",
      "Epoch 11894/20000 Training Loss: 0.059318553656339645\n",
      "Epoch 11895/20000 Training Loss: 0.03860721364617348\n",
      "Epoch 11896/20000 Training Loss: 0.057097334414720535\n",
      "Epoch 11897/20000 Training Loss: 0.0697978138923645\n",
      "Epoch 11898/20000 Training Loss: 0.053344156593084335\n",
      "Epoch 11899/20000 Training Loss: 0.050963252782821655\n",
      "Epoch 11900/20000 Training Loss: 0.04468052461743355\n",
      "Epoch 11900/20000 Validation Loss: 0.044390320777893066\n",
      "Epoch 11901/20000 Training Loss: 0.04748031869530678\n",
      "Epoch 11902/20000 Training Loss: 0.04643829166889191\n",
      "Epoch 11903/20000 Training Loss: 0.051668327301740646\n",
      "Epoch 11904/20000 Training Loss: 0.04960758611559868\n",
      "Epoch 11905/20000 Training Loss: 0.04726986959576607\n",
      "Epoch 11906/20000 Training Loss: 0.055166393518447876\n",
      "Epoch 11907/20000 Training Loss: 0.061762988567352295\n",
      "Epoch 11908/20000 Training Loss: 0.055265817791223526\n",
      "Epoch 11909/20000 Training Loss: 0.06891042739152908\n",
      "Epoch 11910/20000 Training Loss: 0.04278438165783882\n",
      "Epoch 11910/20000 Validation Loss: 0.07769212126731873\n",
      "Epoch 11911/20000 Training Loss: 0.04599007964134216\n",
      "Epoch 11912/20000 Training Loss: 0.07067929953336716\n",
      "Epoch 11913/20000 Training Loss: 0.04373524710536003\n",
      "Epoch 11914/20000 Training Loss: 0.05968140438199043\n",
      "Epoch 11915/20000 Training Loss: 0.04387907311320305\n",
      "Epoch 11916/20000 Training Loss: 0.04795718565583229\n",
      "Epoch 11917/20000 Training Loss: 0.05714716017246246\n",
      "Epoch 11918/20000 Training Loss: 0.061588723212480545\n",
      "Epoch 11919/20000 Training Loss: 0.04933208227157593\n",
      "Epoch 11920/20000 Training Loss: 0.03680508956313133\n",
      "Epoch 11920/20000 Validation Loss: 0.04728246107697487\n",
      "Epoch 11921/20000 Training Loss: 0.055481839925050735\n",
      "Epoch 11922/20000 Training Loss: 0.04908981919288635\n",
      "Epoch 11923/20000 Training Loss: 0.04659314453601837\n",
      "Epoch 11924/20000 Training Loss: 0.06347531825304031\n",
      "Epoch 11925/20000 Training Loss: 0.06646405160427094\n",
      "Epoch 11926/20000 Training Loss: 0.04566032811999321\n",
      "Epoch 11927/20000 Training Loss: 0.04340311884880066\n",
      "Epoch 11928/20000 Training Loss: 0.06696724146604538\n",
      "Epoch 11929/20000 Training Loss: 0.059947866946458817\n",
      "Epoch 11930/20000 Training Loss: 0.04624519497156143\n",
      "Epoch 11930/20000 Validation Loss: 0.05721253901720047\n",
      "Epoch 11931/20000 Training Loss: 0.038959402590990067\n",
      "Epoch 11932/20000 Training Loss: 0.05482052266597748\n",
      "Epoch 11933/20000 Training Loss: 0.0399731881916523\n",
      "Epoch 11934/20000 Training Loss: 0.07249949127435684\n",
      "Epoch 11935/20000 Training Loss: 0.05326404049992561\n",
      "Epoch 11936/20000 Training Loss: 0.07488264888525009\n",
      "Epoch 11937/20000 Training Loss: 0.039307788014411926\n",
      "Epoch 11938/20000 Training Loss: 0.04290613532066345\n",
      "Epoch 11939/20000 Training Loss: 0.07217792421579361\n",
      "Epoch 11940/20000 Training Loss: 0.04576895758509636\n",
      "Epoch 11940/20000 Validation Loss: 0.06139860674738884\n",
      "Epoch 11941/20000 Training Loss: 0.05226397141814232\n",
      "Epoch 11942/20000 Training Loss: 0.06538213044404984\n",
      "Epoch 11943/20000 Training Loss: 0.03784557059407234\n",
      "Epoch 11944/20000 Training Loss: 0.06266067922115326\n",
      "Epoch 11945/20000 Training Loss: 0.06194670870900154\n",
      "Epoch 11946/20000 Training Loss: 0.0515054352581501\n",
      "Epoch 11947/20000 Training Loss: 0.07342302799224854\n",
      "Epoch 11948/20000 Training Loss: 0.06703886389732361\n",
      "Epoch 11949/20000 Training Loss: 0.04655756056308746\n",
      "Epoch 11950/20000 Training Loss: 0.04341399297118187\n",
      "Epoch 11950/20000 Validation Loss: 0.046299226582050323\n",
      "Epoch 11951/20000 Training Loss: 0.060529667884111404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11952/20000 Training Loss: 0.04158487543463707\n",
      "Epoch 11953/20000 Training Loss: 0.05180490016937256\n",
      "Epoch 11954/20000 Training Loss: 0.06267497688531876\n",
      "Epoch 11955/20000 Training Loss: 0.05567530170083046\n",
      "Epoch 11956/20000 Training Loss: 0.05727088078856468\n",
      "Epoch 11957/20000 Training Loss: 0.06002587080001831\n",
      "Epoch 11958/20000 Training Loss: 0.06514246016740799\n",
      "Epoch 11959/20000 Training Loss: 0.055133555084466934\n",
      "Epoch 11960/20000 Training Loss: 0.06645219773054123\n",
      "Epoch 11960/20000 Validation Loss: 0.05183245986700058\n",
      "Epoch 11961/20000 Training Loss: 0.070065438747406\n",
      "Epoch 11962/20000 Training Loss: 0.05206884443759918\n",
      "Epoch 11963/20000 Training Loss: 0.05796196684241295\n",
      "Epoch 11964/20000 Training Loss: 0.06293677538633347\n",
      "Epoch 11965/20000 Training Loss: 0.06990791112184525\n",
      "Epoch 11966/20000 Training Loss: 0.0525357611477375\n",
      "Epoch 11967/20000 Training Loss: 0.04230799153447151\n",
      "Epoch 11968/20000 Training Loss: 0.07324401289224625\n",
      "Epoch 11969/20000 Training Loss: 0.06147583946585655\n",
      "Epoch 11970/20000 Training Loss: 0.048908334225416183\n",
      "Epoch 11970/20000 Validation Loss: 0.04437539726495743\n",
      "Epoch 11971/20000 Training Loss: 0.055181581526994705\n",
      "Epoch 11972/20000 Training Loss: 0.04766845703125\n",
      "Epoch 11973/20000 Training Loss: 0.06452390551567078\n",
      "Epoch 11974/20000 Training Loss: 0.06242380663752556\n",
      "Epoch 11975/20000 Training Loss: 0.042905617505311966\n",
      "Epoch 11976/20000 Training Loss: 0.04728792980313301\n",
      "Epoch 11977/20000 Training Loss: 0.059222955256700516\n",
      "Epoch 11978/20000 Training Loss: 0.047649506479501724\n",
      "Epoch 11979/20000 Training Loss: 0.04735899344086647\n",
      "Epoch 11980/20000 Training Loss: 0.06413467973470688\n",
      "Epoch 11980/20000 Validation Loss: 0.040961071848869324\n",
      "Epoch 11981/20000 Training Loss: 0.04767676070332527\n",
      "Epoch 11982/20000 Training Loss: 0.05499586835503578\n",
      "Epoch 11983/20000 Training Loss: 0.05055563524365425\n",
      "Epoch 11984/20000 Training Loss: 0.06056955084204674\n",
      "Epoch 11985/20000 Training Loss: 0.06147919222712517\n",
      "Epoch 11986/20000 Training Loss: 0.05434742569923401\n",
      "Epoch 11987/20000 Training Loss: 0.0578058697283268\n",
      "Epoch 11988/20000 Training Loss: 0.03988784924149513\n",
      "Epoch 11989/20000 Training Loss: 0.053409915417432785\n",
      "Epoch 11990/20000 Training Loss: 0.06790021061897278\n",
      "Epoch 11990/20000 Validation Loss: 0.0556088350713253\n",
      "Epoch 11991/20000 Training Loss: 0.050812553614377975\n",
      "Epoch 11992/20000 Training Loss: 0.06030130758881569\n",
      "Epoch 11993/20000 Training Loss: 0.04609513282775879\n",
      "Epoch 11994/20000 Training Loss: 0.0595916211605072\n",
      "Epoch 11995/20000 Training Loss: 0.06025751307606697\n",
      "Epoch 11996/20000 Training Loss: 0.07238561660051346\n",
      "Epoch 11997/20000 Training Loss: 0.055966977030038834\n",
      "Epoch 11998/20000 Training Loss: 0.06810425966978073\n",
      "Epoch 11999/20000 Training Loss: 0.05573022738099098\n",
      "Epoch 12000/20000 Training Loss: 0.05744946375489235\n",
      "Epoch 12000/20000 Validation Loss: 0.051880478858947754\n",
      "Epoch 12001/20000 Training Loss: 0.05182100459933281\n",
      "Epoch 12002/20000 Training Loss: 0.07019934058189392\n",
      "Epoch 12003/20000 Training Loss: 0.0615079365670681\n",
      "Epoch 12004/20000 Training Loss: 0.05508144572377205\n",
      "Epoch 12005/20000 Training Loss: 0.05180905759334564\n",
      "Epoch 12006/20000 Training Loss: 0.055967602878808975\n",
      "Epoch 12007/20000 Training Loss: 0.062468498945236206\n",
      "Epoch 12008/20000 Training Loss: 0.06705716252326965\n",
      "Epoch 12009/20000 Training Loss: 0.04428120329976082\n",
      "Epoch 12010/20000 Training Loss: 0.04683797061443329\n",
      "Epoch 12010/20000 Validation Loss: 0.05513221025466919\n",
      "Epoch 12011/20000 Training Loss: 0.04629058763384819\n",
      "Epoch 12012/20000 Training Loss: 0.05479751154780388\n",
      "Epoch 12013/20000 Training Loss: 0.05256207287311554\n",
      "Epoch 12014/20000 Training Loss: 0.04375947639346123\n",
      "Epoch 12015/20000 Training Loss: 0.07226482033729553\n",
      "Epoch 12016/20000 Training Loss: 0.06029905006289482\n",
      "Epoch 12017/20000 Training Loss: 0.05335964262485504\n",
      "Epoch 12018/20000 Training Loss: 0.051706552505493164\n",
      "Epoch 12019/20000 Training Loss: 0.04501371085643768\n",
      "Epoch 12020/20000 Training Loss: 0.05005662143230438\n",
      "Epoch 12020/20000 Validation Loss: 0.07041244208812714\n",
      "Epoch 12021/20000 Training Loss: 0.04911031201481819\n",
      "Epoch 12022/20000 Training Loss: 0.05064459145069122\n",
      "Epoch 12023/20000 Training Loss: 0.06415458768606186\n",
      "Epoch 12024/20000 Training Loss: 0.05643057823181152\n",
      "Epoch 12025/20000 Training Loss: 0.05195961520075798\n",
      "Epoch 12026/20000 Training Loss: 0.050436291843652725\n",
      "Epoch 12027/20000 Training Loss: 0.06404215842485428\n",
      "Epoch 12028/20000 Training Loss: 0.04678290709853172\n",
      "Epoch 12029/20000 Training Loss: 0.050452232360839844\n",
      "Epoch 12030/20000 Training Loss: 0.07629168778657913\n",
      "Epoch 12030/20000 Validation Loss: 0.06978631019592285\n",
      "Epoch 12031/20000 Training Loss: 0.07059518992900848\n",
      "Epoch 12032/20000 Training Loss: 0.053477492183446884\n",
      "Epoch 12033/20000 Training Loss: 0.052849870175123215\n",
      "Epoch 12034/20000 Training Loss: 0.052677035331726074\n",
      "Epoch 12035/20000 Training Loss: 0.07290004938840866\n",
      "Epoch 12036/20000 Training Loss: 0.056439321488142014\n",
      "Epoch 12037/20000 Training Loss: 0.0579698346555233\n",
      "Epoch 12038/20000 Training Loss: 0.07289429754018784\n",
      "Epoch 12039/20000 Training Loss: 0.05806749686598778\n",
      "Epoch 12040/20000 Training Loss: 0.05573497340083122\n",
      "Epoch 12040/20000 Validation Loss: 0.05437784269452095\n",
      "Epoch 12041/20000 Training Loss: 0.056604836136102676\n",
      "Epoch 12042/20000 Training Loss: 0.07101090252399445\n",
      "Epoch 12043/20000 Training Loss: 0.059062808752059937\n",
      "Epoch 12044/20000 Training Loss: 0.04633759334683418\n",
      "Epoch 12045/20000 Training Loss: 0.042837608605623245\n",
      "Epoch 12046/20000 Training Loss: 0.04005524516105652\n",
      "Epoch 12047/20000 Training Loss: 0.05558401718735695\n",
      "Epoch 12048/20000 Training Loss: 0.05022694543004036\n",
      "Epoch 12049/20000 Training Loss: 0.04773160442709923\n",
      "Epoch 12050/20000 Training Loss: 0.06745467334985733\n",
      "Epoch 12050/20000 Validation Loss: 0.07597896456718445\n",
      "Epoch 12051/20000 Training Loss: 0.05111686885356903\n",
      "Epoch 12052/20000 Training Loss: 0.06637778133153915\n",
      "Epoch 12053/20000 Training Loss: 0.057757217437028885\n",
      "Epoch 12054/20000 Training Loss: 0.05700348690152168\n",
      "Epoch 12055/20000 Training Loss: 0.05051668360829353\n",
      "Epoch 12056/20000 Training Loss: 0.06389960646629333\n",
      "Epoch 12057/20000 Training Loss: 0.05199834704399109\n",
      "Epoch 12058/20000 Training Loss: 0.0583672858774662\n",
      "Epoch 12059/20000 Training Loss: 0.05594288185238838\n",
      "Epoch 12060/20000 Training Loss: 0.05322730541229248\n",
      "Epoch 12060/20000 Validation Loss: 0.04291887953877449\n",
      "Epoch 12061/20000 Training Loss: 0.042968686670064926\n",
      "Epoch 12062/20000 Training Loss: 0.058599699288606644\n",
      "Epoch 12063/20000 Training Loss: 0.04057851806282997\n",
      "Epoch 12064/20000 Training Loss: 0.04100993648171425\n",
      "Epoch 12065/20000 Training Loss: 0.04977046325802803\n",
      "Epoch 12066/20000 Training Loss: 0.05214859917759895\n",
      "Epoch 12067/20000 Training Loss: 0.07906875014305115\n",
      "Epoch 12068/20000 Training Loss: 0.059503499418497086\n",
      "Epoch 12069/20000 Training Loss: 0.06453132629394531\n",
      "Epoch 12070/20000 Training Loss: 0.0605594627559185\n",
      "Epoch 12070/20000 Validation Loss: 0.06370323896408081\n",
      "Epoch 12071/20000 Training Loss: 0.04457997903227806\n",
      "Epoch 12072/20000 Training Loss: 0.06126869097352028\n",
      "Epoch 12073/20000 Training Loss: 0.061161816120147705\n",
      "Epoch 12074/20000 Training Loss: 0.07736276835203171\n",
      "Epoch 12075/20000 Training Loss: 0.054554283618927\n",
      "Epoch 12076/20000 Training Loss: 0.0676347091794014\n",
      "Epoch 12077/20000 Training Loss: 0.0524018220603466\n",
      "Epoch 12078/20000 Training Loss: 0.06391897052526474\n",
      "Epoch 12079/20000 Training Loss: 0.06147393584251404\n",
      "Epoch 12080/20000 Training Loss: 0.046069029718637466\n",
      "Epoch 12080/20000 Validation Loss: 0.05428681522607803\n",
      "Epoch 12081/20000 Training Loss: 0.058909717947244644\n",
      "Epoch 12082/20000 Training Loss: 0.05584918335080147\n",
      "Epoch 12083/20000 Training Loss: 0.04245899245142937\n",
      "Epoch 12084/20000 Training Loss: 0.0714445561170578\n",
      "Epoch 12085/20000 Training Loss: 0.034896403551101685\n",
      "Epoch 12086/20000 Training Loss: 0.06614700704813004\n",
      "Epoch 12087/20000 Training Loss: 0.046770501881837845\n",
      "Epoch 12088/20000 Training Loss: 0.047319427132606506\n",
      "Epoch 12089/20000 Training Loss: 0.06069208309054375\n",
      "Epoch 12090/20000 Training Loss: 0.04923800751566887\n",
      "Epoch 12090/20000 Validation Loss: 0.05902320146560669\n",
      "Epoch 12091/20000 Training Loss: 0.05303121730685234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12092/20000 Training Loss: 0.0660712942481041\n",
      "Epoch 12093/20000 Training Loss: 0.06282327324151993\n",
      "Epoch 12094/20000 Training Loss: 0.049761030822992325\n",
      "Epoch 12095/20000 Training Loss: 0.05504591390490532\n",
      "Epoch 12096/20000 Training Loss: 0.04603502154350281\n",
      "Epoch 12097/20000 Training Loss: 0.06428925693035126\n",
      "Epoch 12098/20000 Training Loss: 0.04375655949115753\n",
      "Epoch 12099/20000 Training Loss: 0.048384908586740494\n",
      "Epoch 12100/20000 Training Loss: 0.05697619915008545\n",
      "Epoch 12100/20000 Validation Loss: 0.0611700713634491\n",
      "Epoch 12101/20000 Training Loss: 0.04228092357516289\n",
      "Epoch 12102/20000 Training Loss: 0.0712861642241478\n",
      "Epoch 12103/20000 Training Loss: 0.05892254039645195\n",
      "Epoch 12104/20000 Training Loss: 0.039496421813964844\n",
      "Epoch 12105/20000 Training Loss: 0.05346584692597389\n",
      "Epoch 12106/20000 Training Loss: 0.042468536645174026\n",
      "Epoch 12107/20000 Training Loss: 0.04115574434399605\n",
      "Epoch 12108/20000 Training Loss: 0.04888276755809784\n",
      "Epoch 12109/20000 Training Loss: 0.057603806257247925\n",
      "Epoch 12110/20000 Training Loss: 0.05560414493083954\n",
      "Epoch 12110/20000 Validation Loss: 0.05486109107732773\n",
      "Epoch 12111/20000 Training Loss: 0.05480662360787392\n",
      "Epoch 12112/20000 Training Loss: 0.05715092644095421\n",
      "Epoch 12113/20000 Training Loss: 0.04637155309319496\n",
      "Epoch 12114/20000 Training Loss: 0.04393116757273674\n",
      "Epoch 12115/20000 Training Loss: 0.04715663567185402\n",
      "Epoch 12116/20000 Training Loss: 0.05151146650314331\n",
      "Epoch 12117/20000 Training Loss: 0.05502266809344292\n",
      "Epoch 12118/20000 Training Loss: 0.045528680086135864\n",
      "Epoch 12119/20000 Training Loss: 0.052355337888002396\n",
      "Epoch 12120/20000 Training Loss: 0.06972111016511917\n",
      "Epoch 12120/20000 Validation Loss: 0.05878394842147827\n",
      "Epoch 12121/20000 Training Loss: 0.04957844316959381\n",
      "Epoch 12122/20000 Training Loss: 0.050999242812395096\n",
      "Epoch 12123/20000 Training Loss: 0.06036108732223511\n",
      "Epoch 12124/20000 Training Loss: 0.06975215673446655\n",
      "Epoch 12125/20000 Training Loss: 0.0643022432923317\n",
      "Epoch 12126/20000 Training Loss: 0.06597469002008438\n",
      "Epoch 12127/20000 Training Loss: 0.045740723609924316\n",
      "Epoch 12128/20000 Training Loss: 0.04131181910634041\n",
      "Epoch 12129/20000 Training Loss: 0.06187066435813904\n",
      "Epoch 12130/20000 Training Loss: 0.049855004996061325\n",
      "Epoch 12130/20000 Validation Loss: 0.05293719470500946\n",
      "Epoch 12131/20000 Training Loss: 0.051363151520490646\n",
      "Epoch 12132/20000 Training Loss: 0.04141634702682495\n",
      "Epoch 12133/20000 Training Loss: 0.05003632977604866\n",
      "Epoch 12134/20000 Training Loss: 0.03459939733147621\n",
      "Epoch 12135/20000 Training Loss: 0.05905963107943535\n",
      "Epoch 12136/20000 Training Loss: 0.05406293272972107\n",
      "Epoch 12137/20000 Training Loss: 0.05182409659028053\n",
      "Epoch 12138/20000 Training Loss: 0.05698714777827263\n",
      "Epoch 12139/20000 Training Loss: 0.06661361455917358\n",
      "Epoch 12140/20000 Training Loss: 0.043177783489227295\n",
      "Epoch 12140/20000 Validation Loss: 0.0788690447807312\n",
      "Epoch 12141/20000 Training Loss: 0.046681325882673264\n",
      "Epoch 12142/20000 Training Loss: 0.05174129083752632\n",
      "Epoch 12143/20000 Training Loss: 0.07533954828977585\n",
      "Epoch 12144/20000 Training Loss: 0.07041537761688232\n",
      "Epoch 12145/20000 Training Loss: 0.059728145599365234\n",
      "Epoch 12146/20000 Training Loss: 0.0698755607008934\n",
      "Epoch 12147/20000 Training Loss: 0.07817824184894562\n",
      "Epoch 12148/20000 Training Loss: 0.06257540732622147\n",
      "Epoch 12149/20000 Training Loss: 0.052427828311920166\n",
      "Epoch 12150/20000 Training Loss: 0.040245186537504196\n",
      "Epoch 12150/20000 Validation Loss: 0.05336042493581772\n",
      "Epoch 12151/20000 Training Loss: 0.05934533476829529\n",
      "Epoch 12152/20000 Training Loss: 0.04616737738251686\n",
      "Epoch 12153/20000 Training Loss: 0.049924153834581375\n",
      "Epoch 12154/20000 Training Loss: 0.05811041593551636\n",
      "Epoch 12155/20000 Training Loss: 0.0502905510365963\n",
      "Epoch 12156/20000 Training Loss: 0.06348863244056702\n",
      "Epoch 12157/20000 Training Loss: 0.04769832268357277\n",
      "Epoch 12158/20000 Training Loss: 0.04808796942234039\n",
      "Epoch 12159/20000 Training Loss: 0.05641893669962883\n",
      "Epoch 12160/20000 Training Loss: 0.053266678005456924\n",
      "Epoch 12160/20000 Validation Loss: 0.04442654550075531\n",
      "Epoch 12161/20000 Training Loss: 0.04899231716990471\n",
      "Epoch 12162/20000 Training Loss: 0.06292956322431564\n",
      "Epoch 12163/20000 Training Loss: 0.07070506364107132\n",
      "Epoch 12164/20000 Training Loss: 0.05725407600402832\n",
      "Epoch 12165/20000 Training Loss: 0.04991365596652031\n",
      "Epoch 12166/20000 Training Loss: 0.045169759541749954\n",
      "Epoch 12167/20000 Training Loss: 0.05948234722018242\n",
      "Epoch 12168/20000 Training Loss: 0.05942806228995323\n",
      "Epoch 12169/20000 Training Loss: 0.05657215043902397\n",
      "Epoch 12170/20000 Training Loss: 0.07020094990730286\n",
      "Epoch 12170/20000 Validation Loss: 0.0828971415758133\n",
      "Epoch 12171/20000 Training Loss: 0.05680673196911812\n",
      "Epoch 12172/20000 Training Loss: 0.06159154698252678\n",
      "Epoch 12173/20000 Training Loss: 0.06191660836338997\n",
      "Epoch 12174/20000 Training Loss: 0.036174476146698\n",
      "Epoch 12175/20000 Training Loss: 0.05206155776977539\n",
      "Epoch 12176/20000 Training Loss: 0.07169262319803238\n",
      "Epoch 12177/20000 Training Loss: 0.0686759352684021\n",
      "Epoch 12178/20000 Training Loss: 0.05791233852505684\n",
      "Epoch 12179/20000 Training Loss: 0.04264839366078377\n",
      "Epoch 12180/20000 Training Loss: 0.04932578280568123\n",
      "Epoch 12180/20000 Validation Loss: 0.03892563655972481\n",
      "Epoch 12181/20000 Training Loss: 0.04213723540306091\n",
      "Epoch 12182/20000 Training Loss: 0.05468182638287544\n",
      "Epoch 12183/20000 Training Loss: 0.0579674206674099\n",
      "Epoch 12184/20000 Training Loss: 0.07192183285951614\n",
      "Epoch 12185/20000 Training Loss: 0.05231583118438721\n",
      "Epoch 12186/20000 Training Loss: 0.07637164741754532\n",
      "Epoch 12187/20000 Training Loss: 0.07974448800086975\n",
      "Epoch 12188/20000 Training Loss: 0.06617579609155655\n",
      "Epoch 12189/20000 Training Loss: 0.05849655345082283\n",
      "Epoch 12190/20000 Training Loss: 0.050336871296167374\n",
      "Epoch 12190/20000 Validation Loss: 0.054595306515693665\n",
      "Epoch 12191/20000 Training Loss: 0.08160369843244553\n",
      "Epoch 12192/20000 Training Loss: 0.046555593609809875\n",
      "Epoch 12193/20000 Training Loss: 0.06297353655099869\n",
      "Epoch 12194/20000 Training Loss: 0.04588605836033821\n",
      "Epoch 12195/20000 Training Loss: 0.04135747626423836\n",
      "Epoch 12196/20000 Training Loss: 0.0604836530983448\n",
      "Epoch 12197/20000 Training Loss: 0.0454830527305603\n",
      "Epoch 12198/20000 Training Loss: 0.05834466218948364\n",
      "Epoch 12199/20000 Training Loss: 0.06656704097986221\n",
      "Epoch 12200/20000 Training Loss: 0.0601770393550396\n",
      "Epoch 12200/20000 Validation Loss: 0.05735696852207184\n",
      "Epoch 12201/20000 Training Loss: 0.0440734326839447\n",
      "Epoch 12202/20000 Training Loss: 0.07214954495429993\n",
      "Epoch 12203/20000 Training Loss: 0.06460816413164139\n",
      "Epoch 12204/20000 Training Loss: 0.04576728865504265\n",
      "Epoch 12205/20000 Training Loss: 0.0527384877204895\n",
      "Epoch 12206/20000 Training Loss: 0.05807087942957878\n",
      "Epoch 12207/20000 Training Loss: 0.07407625019550323\n",
      "Epoch 12208/20000 Training Loss: 0.05539308860898018\n",
      "Epoch 12209/20000 Training Loss: 0.05510829761624336\n",
      "Epoch 12210/20000 Training Loss: 0.07445812225341797\n",
      "Epoch 12210/20000 Validation Loss: 0.055002063512802124\n",
      "Epoch 12211/20000 Training Loss: 0.0482020229101181\n",
      "Epoch 12212/20000 Training Loss: 0.06884857267141342\n",
      "Epoch 12213/20000 Training Loss: 0.047943245619535446\n",
      "Epoch 12214/20000 Training Loss: 0.06530461460351944\n",
      "Epoch 12215/20000 Training Loss: 0.05067269876599312\n",
      "Epoch 12216/20000 Training Loss: 0.04682304337620735\n",
      "Epoch 12217/20000 Training Loss: 0.05350503697991371\n",
      "Epoch 12218/20000 Training Loss: 0.04587172344326973\n",
      "Epoch 12219/20000 Training Loss: 0.05886535346508026\n",
      "Epoch 12220/20000 Training Loss: 0.05522329732775688\n",
      "Epoch 12220/20000 Validation Loss: 0.055949676781892776\n",
      "Epoch 12221/20000 Training Loss: 0.040435388684272766\n",
      "Epoch 12222/20000 Training Loss: 0.06572275608778\n",
      "Epoch 12223/20000 Training Loss: 0.05487953498959541\n",
      "Epoch 12224/20000 Training Loss: 0.04470716416835785\n",
      "Epoch 12225/20000 Training Loss: 0.04787280037999153\n",
      "Epoch 12226/20000 Training Loss: 0.06912124156951904\n",
      "Epoch 12227/20000 Training Loss: 0.05845893546938896\n",
      "Epoch 12228/20000 Training Loss: 0.056389521807432175\n",
      "Epoch 12229/20000 Training Loss: 0.0496138371527195\n",
      "Epoch 12230/20000 Training Loss: 0.06730813533067703\n",
      "Epoch 12230/20000 Validation Loss: 0.06489799916744232\n",
      "Epoch 12231/20000 Training Loss: 0.055196989327669144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12232/20000 Training Loss: 0.04912528768181801\n",
      "Epoch 12233/20000 Training Loss: 0.052290674299001694\n",
      "Epoch 12234/20000 Training Loss: 0.04388822242617607\n",
      "Epoch 12235/20000 Training Loss: 0.06092359498143196\n",
      "Epoch 12236/20000 Training Loss: 0.05424506589770317\n",
      "Epoch 12237/20000 Training Loss: 0.03495891019701958\n",
      "Epoch 12238/20000 Training Loss: 0.07926944643259048\n",
      "Epoch 12239/20000 Training Loss: 0.056784775108098984\n",
      "Epoch 12240/20000 Training Loss: 0.05231783911585808\n",
      "Epoch 12240/20000 Validation Loss: 0.06147640198469162\n",
      "Epoch 12241/20000 Training Loss: 0.05555465817451477\n",
      "Epoch 12242/20000 Training Loss: 0.06226633861660957\n",
      "Epoch 12243/20000 Training Loss: 0.05421826243400574\n",
      "Epoch 12244/20000 Training Loss: 0.04751548171043396\n",
      "Epoch 12245/20000 Training Loss: 0.0503382571041584\n",
      "Epoch 12246/20000 Training Loss: 0.05546955391764641\n",
      "Epoch 12247/20000 Training Loss: 0.03611229732632637\n",
      "Epoch 12248/20000 Training Loss: 0.054066646844148636\n",
      "Epoch 12249/20000 Training Loss: 0.061280980706214905\n",
      "Epoch 12250/20000 Training Loss: 0.050920989364385605\n",
      "Epoch 12250/20000 Validation Loss: 0.06548094749450684\n",
      "Epoch 12251/20000 Training Loss: 0.05724896118044853\n",
      "Epoch 12252/20000 Training Loss: 0.05754039064049721\n",
      "Epoch 12253/20000 Training Loss: 0.06509861350059509\n",
      "Epoch 12254/20000 Training Loss: 0.04932231828570366\n",
      "Epoch 12255/20000 Training Loss: 0.05477123335003853\n",
      "Epoch 12256/20000 Training Loss: 0.057052407413721085\n",
      "Epoch 12257/20000 Training Loss: 0.05746970698237419\n",
      "Epoch 12258/20000 Training Loss: 0.0491994209587574\n",
      "Epoch 12259/20000 Training Loss: 0.04705295339226723\n",
      "Epoch 12260/20000 Training Loss: 0.05973703786730766\n",
      "Epoch 12260/20000 Validation Loss: 0.05750919133424759\n",
      "Epoch 12261/20000 Training Loss: 0.0432899110019207\n",
      "Epoch 12262/20000 Training Loss: 0.062456488609313965\n",
      "Epoch 12263/20000 Training Loss: 0.043659184128046036\n",
      "Epoch 12264/20000 Training Loss: 0.050777167081832886\n",
      "Epoch 12265/20000 Training Loss: 0.057757098227739334\n",
      "Epoch 12266/20000 Training Loss: 0.04641818627715111\n",
      "Epoch 12267/20000 Training Loss: 0.05212002992630005\n",
      "Epoch 12268/20000 Training Loss: 0.05332059785723686\n",
      "Epoch 12269/20000 Training Loss: 0.06543030589818954\n",
      "Epoch 12270/20000 Training Loss: 0.05015721917152405\n",
      "Epoch 12270/20000 Validation Loss: 0.053015366196632385\n",
      "Epoch 12271/20000 Training Loss: 0.06757209450006485\n",
      "Epoch 12272/20000 Training Loss: 0.051788900047540665\n",
      "Epoch 12273/20000 Training Loss: 0.052094265818595886\n",
      "Epoch 12274/20000 Training Loss: 0.04587244987487793\n",
      "Epoch 12275/20000 Training Loss: 0.05327093228697777\n",
      "Epoch 12276/20000 Training Loss: 0.051117006689310074\n",
      "Epoch 12277/20000 Training Loss: 0.04841824993491173\n",
      "Epoch 12278/20000 Training Loss: 0.0486014224588871\n",
      "Epoch 12279/20000 Training Loss: 0.0661090686917305\n",
      "Epoch 12280/20000 Training Loss: 0.04948108270764351\n",
      "Epoch 12280/20000 Validation Loss: 0.07514016330242157\n",
      "Epoch 12281/20000 Training Loss: 0.0696496069431305\n",
      "Epoch 12282/20000 Training Loss: 0.05347103253006935\n",
      "Epoch 12283/20000 Training Loss: 0.0697971060872078\n",
      "Epoch 12284/20000 Training Loss: 0.04048764705657959\n",
      "Epoch 12285/20000 Training Loss: 0.08004516363143921\n",
      "Epoch 12286/20000 Training Loss: 0.05661839246749878\n",
      "Epoch 12287/20000 Training Loss: 0.05917802453041077\n",
      "Epoch 12288/20000 Training Loss: 0.042785387486219406\n",
      "Epoch 12289/20000 Training Loss: 0.0591309480369091\n",
      "Epoch 12290/20000 Training Loss: 0.05727076530456543\n",
      "Epoch 12290/20000 Validation Loss: 0.05229005962610245\n",
      "Epoch 12291/20000 Training Loss: 0.05829613283276558\n",
      "Epoch 12292/20000 Training Loss: 0.049193788319826126\n",
      "Epoch 12293/20000 Training Loss: 0.08750420808792114\n",
      "Epoch 12294/20000 Training Loss: 0.05905957520008087\n",
      "Epoch 12295/20000 Training Loss: 0.04795757308602333\n",
      "Epoch 12296/20000 Training Loss: 0.0534786693751812\n",
      "Epoch 12297/20000 Training Loss: 0.06181354448199272\n",
      "Epoch 12298/20000 Training Loss: 0.062123458832502365\n",
      "Epoch 12299/20000 Training Loss: 0.05282747372984886\n",
      "Epoch 12300/20000 Training Loss: 0.05036451295018196\n",
      "Epoch 12300/20000 Validation Loss: 0.051987215876579285\n",
      "Epoch 12301/20000 Training Loss: 0.05042612552642822\n",
      "Epoch 12302/20000 Training Loss: 0.06607960909605026\n",
      "Epoch 12303/20000 Training Loss: 0.05776439979672432\n",
      "Epoch 12304/20000 Training Loss: 0.04671257734298706\n",
      "Epoch 12305/20000 Training Loss: 0.06434162706136703\n",
      "Epoch 12306/20000 Training Loss: 0.049168910831213\n",
      "Epoch 12307/20000 Training Loss: 0.0523357056081295\n",
      "Epoch 12308/20000 Training Loss: 0.04936834052205086\n",
      "Epoch 12309/20000 Training Loss: 0.057145874947309494\n",
      "Epoch 12310/20000 Training Loss: 0.04402058199048042\n",
      "Epoch 12310/20000 Validation Loss: 0.06956765055656433\n",
      "Epoch 12311/20000 Training Loss: 0.05164419487118721\n",
      "Epoch 12312/20000 Training Loss: 0.053294647485017776\n",
      "Epoch 12313/20000 Training Loss: 0.04794437065720558\n",
      "Epoch 12314/20000 Training Loss: 0.06314093619585037\n",
      "Epoch 12315/20000 Training Loss: 0.07194489240646362\n",
      "Epoch 12316/20000 Training Loss: 0.05335286632180214\n",
      "Epoch 12317/20000 Training Loss: 0.06198374554514885\n",
      "Epoch 12318/20000 Training Loss: 0.04474996402859688\n",
      "Epoch 12319/20000 Training Loss: 0.03582601621747017\n",
      "Epoch 12320/20000 Training Loss: 0.07282274216413498\n",
      "Epoch 12320/20000 Validation Loss: 0.0554618313908577\n",
      "Epoch 12321/20000 Training Loss: 0.06689104437828064\n",
      "Epoch 12322/20000 Training Loss: 0.05243669077754021\n",
      "Epoch 12323/20000 Training Loss: 0.05195586383342743\n",
      "Epoch 12324/20000 Training Loss: 0.046508338302373886\n",
      "Epoch 12325/20000 Training Loss: 0.04918638989329338\n",
      "Epoch 12326/20000 Training Loss: 0.06074442341923714\n",
      "Epoch 12327/20000 Training Loss: 0.046861547976732254\n",
      "Epoch 12328/20000 Training Loss: 0.05258842930197716\n",
      "Epoch 12329/20000 Training Loss: 0.06084393337368965\n",
      "Epoch 12330/20000 Training Loss: 0.06705748289823532\n",
      "Epoch 12330/20000 Validation Loss: 0.06520205736160278\n",
      "Epoch 12331/20000 Training Loss: 0.07036391645669937\n",
      "Epoch 12332/20000 Training Loss: 0.06283379346132278\n",
      "Epoch 12333/20000 Training Loss: 0.06651018559932709\n",
      "Epoch 12334/20000 Training Loss: 0.044853325933218\n",
      "Epoch 12335/20000 Training Loss: 0.05438581481575966\n",
      "Epoch 12336/20000 Training Loss: 0.04516536369919777\n",
      "Epoch 12337/20000 Training Loss: 0.04237331077456474\n",
      "Epoch 12338/20000 Training Loss: 0.04985107108950615\n",
      "Epoch 12339/20000 Training Loss: 0.08072572201490402\n",
      "Epoch 12340/20000 Training Loss: 0.05364139378070831\n",
      "Epoch 12340/20000 Validation Loss: 0.04836008697748184\n",
      "Epoch 12341/20000 Training Loss: 0.07277861982584\n",
      "Epoch 12342/20000 Training Loss: 0.03679819032549858\n",
      "Epoch 12343/20000 Training Loss: 0.056477632373571396\n",
      "Epoch 12344/20000 Training Loss: 0.06012342497706413\n",
      "Epoch 12345/20000 Training Loss: 0.0483299158513546\n",
      "Epoch 12346/20000 Training Loss: 0.06295701116323471\n",
      "Epoch 12347/20000 Training Loss: 0.04215295985341072\n",
      "Epoch 12348/20000 Training Loss: 0.051730796694755554\n",
      "Epoch 12349/20000 Training Loss: 0.05189622566103935\n",
      "Epoch 12350/20000 Training Loss: 0.05340168997645378\n",
      "Epoch 12350/20000 Validation Loss: 0.043209806084632874\n",
      "Epoch 12351/20000 Training Loss: 0.05884250998497009\n",
      "Epoch 12352/20000 Training Loss: 0.04141039028763771\n",
      "Epoch 12353/20000 Training Loss: 0.05590338632464409\n",
      "Epoch 12354/20000 Training Loss: 0.04626085236668587\n",
      "Epoch 12355/20000 Training Loss: 0.055986806750297546\n",
      "Epoch 12356/20000 Training Loss: 0.0637221410870552\n",
      "Epoch 12357/20000 Training Loss: 0.047469496726989746\n",
      "Epoch 12358/20000 Training Loss: 0.05167664587497711\n",
      "Epoch 12359/20000 Training Loss: 0.07151806354522705\n",
      "Epoch 12360/20000 Training Loss: 0.04910595342516899\n",
      "Epoch 12360/20000 Validation Loss: 0.05667411535978317\n",
      "Epoch 12361/20000 Training Loss: 0.05903209373354912\n",
      "Epoch 12362/20000 Training Loss: 0.046182215213775635\n",
      "Epoch 12363/20000 Training Loss: 0.04643446207046509\n",
      "Epoch 12364/20000 Training Loss: 0.049868080765008926\n",
      "Epoch 12365/20000 Training Loss: 0.04955184832215309\n",
      "Epoch 12366/20000 Training Loss: 0.04696177318692207\n",
      "Epoch 12367/20000 Training Loss: 0.04604901000857353\n",
      "Epoch 12368/20000 Training Loss: 0.05353141203522682\n",
      "Epoch 12369/20000 Training Loss: 0.061705660074949265\n",
      "Epoch 12370/20000 Training Loss: 0.07955633848905563\n",
      "Epoch 12370/20000 Validation Loss: 0.05416100472211838\n",
      "Epoch 12371/20000 Training Loss: 0.04617203772068024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12372/20000 Training Loss: 0.06792252510786057\n",
      "Epoch 12373/20000 Training Loss: 0.06172475591301918\n",
      "Epoch 12374/20000 Training Loss: 0.04432980716228485\n",
      "Epoch 12375/20000 Training Loss: 0.046408433467149734\n",
      "Epoch 12376/20000 Training Loss: 0.037420373409986496\n",
      "Epoch 12377/20000 Training Loss: 0.04635213688015938\n",
      "Epoch 12378/20000 Training Loss: 0.05200796201825142\n",
      "Epoch 12379/20000 Training Loss: 0.045420125126838684\n",
      "Epoch 12380/20000 Training Loss: 0.045238759368658066\n",
      "Epoch 12380/20000 Validation Loss: 0.03977740556001663\n",
      "Epoch 12381/20000 Training Loss: 0.05272316932678223\n",
      "Epoch 12382/20000 Training Loss: 0.06507367640733719\n",
      "Epoch 12383/20000 Training Loss: 0.06097051501274109\n",
      "Epoch 12384/20000 Training Loss: 0.06959239393472672\n",
      "Epoch 12385/20000 Training Loss: 0.055697958916425705\n",
      "Epoch 12386/20000 Training Loss: 0.06118142604827881\n",
      "Epoch 12387/20000 Training Loss: 0.062391798943281174\n",
      "Epoch 12388/20000 Training Loss: 0.06304973363876343\n",
      "Epoch 12389/20000 Training Loss: 0.053248852491378784\n",
      "Epoch 12390/20000 Training Loss: 0.04685690999031067\n",
      "Epoch 12390/20000 Validation Loss: 0.06947088986635208\n",
      "Epoch 12391/20000 Training Loss: 0.06641442328691483\n",
      "Epoch 12392/20000 Training Loss: 0.04928254708647728\n",
      "Epoch 12393/20000 Training Loss: 0.05000445619225502\n",
      "Epoch 12394/20000 Training Loss: 0.048388462513685226\n",
      "Epoch 12395/20000 Training Loss: 0.04362645745277405\n",
      "Epoch 12396/20000 Training Loss: 0.06276148557662964\n",
      "Epoch 12397/20000 Training Loss: 0.049547165632247925\n",
      "Epoch 12398/20000 Training Loss: 0.04918390512466431\n",
      "Epoch 12399/20000 Training Loss: 0.048378247767686844\n",
      "Epoch 12400/20000 Training Loss: 0.056806355714797974\n",
      "Epoch 12400/20000 Validation Loss: 0.05912346392869949\n",
      "Epoch 12401/20000 Training Loss: 0.06957738101482391\n",
      "Epoch 12402/20000 Training Loss: 0.05249819532036781\n",
      "Epoch 12403/20000 Training Loss: 0.060008514672517776\n",
      "Epoch 12404/20000 Training Loss: 0.060961831361055374\n",
      "Epoch 12405/20000 Training Loss: 0.052683133631944656\n",
      "Epoch 12406/20000 Training Loss: 0.04540025070309639\n",
      "Epoch 12407/20000 Training Loss: 0.05908377841114998\n",
      "Epoch 12408/20000 Training Loss: 0.048824578523635864\n",
      "Epoch 12409/20000 Training Loss: 0.04191550239920616\n",
      "Epoch 12410/20000 Training Loss: 0.05569123104214668\n",
      "Epoch 12410/20000 Validation Loss: 0.05607500672340393\n",
      "Epoch 12411/20000 Training Loss: 0.05475413799285889\n",
      "Epoch 12412/20000 Training Loss: 0.05684599280357361\n",
      "Epoch 12413/20000 Training Loss: 0.04628773406147957\n",
      "Epoch 12414/20000 Training Loss: 0.040719565004110336\n",
      "Epoch 12415/20000 Training Loss: 0.08968129754066467\n",
      "Epoch 12416/20000 Training Loss: 0.05605021119117737\n",
      "Epoch 12417/20000 Training Loss: 0.05395200476050377\n",
      "Epoch 12418/20000 Training Loss: 0.04392620921134949\n",
      "Epoch 12419/20000 Training Loss: 0.04372647404670715\n",
      "Epoch 12420/20000 Training Loss: 0.037594620138406754\n",
      "Epoch 12420/20000 Validation Loss: 0.0630035400390625\n",
      "Epoch 12421/20000 Training Loss: 0.04215532913804054\n",
      "Epoch 12422/20000 Training Loss: 0.0554344542324543\n",
      "Epoch 12423/20000 Training Loss: 0.0458933562040329\n",
      "Epoch 12424/20000 Training Loss: 0.05066032335162163\n",
      "Epoch 12425/20000 Training Loss: 0.05648450925946236\n",
      "Epoch 12426/20000 Training Loss: 0.06521604210138321\n",
      "Epoch 12427/20000 Training Loss: 0.05808468535542488\n",
      "Epoch 12428/20000 Training Loss: 0.053770750761032104\n",
      "Epoch 12429/20000 Training Loss: 0.049390215426683426\n",
      "Epoch 12430/20000 Training Loss: 0.06298879534006119\n",
      "Epoch 12430/20000 Validation Loss: 0.06308213621377945\n",
      "Epoch 12431/20000 Training Loss: 0.05794505402445793\n",
      "Epoch 12432/20000 Training Loss: 0.05910838022828102\n",
      "Epoch 12433/20000 Training Loss: 0.056624993681907654\n",
      "Epoch 12434/20000 Training Loss: 0.04775603115558624\n",
      "Epoch 12435/20000 Training Loss: 0.0457729734480381\n",
      "Epoch 12436/20000 Training Loss: 0.047655295580625534\n",
      "Epoch 12437/20000 Training Loss: 0.05222773551940918\n",
      "Epoch 12438/20000 Training Loss: 0.0806351900100708\n",
      "Epoch 12439/20000 Training Loss: 0.05607561394572258\n",
      "Epoch 12440/20000 Training Loss: 0.046484723687171936\n",
      "Epoch 12440/20000 Validation Loss: 0.047738008201122284\n",
      "Epoch 12441/20000 Training Loss: 0.053478214889764786\n",
      "Epoch 12442/20000 Training Loss: 0.06077546998858452\n",
      "Epoch 12443/20000 Training Loss: 0.046742815524339676\n",
      "Epoch 12444/20000 Training Loss: 0.04348870739340782\n",
      "Epoch 12445/20000 Training Loss: 0.05440284311771393\n",
      "Epoch 12446/20000 Training Loss: 0.053719788789749146\n",
      "Epoch 12447/20000 Training Loss: 0.06204148009419441\n",
      "Epoch 12448/20000 Training Loss: 0.0625847801566124\n",
      "Epoch 12449/20000 Training Loss: 0.036026421934366226\n",
      "Epoch 12450/20000 Training Loss: 0.06028677523136139\n",
      "Epoch 12450/20000 Validation Loss: 0.058315448462963104\n",
      "Epoch 12451/20000 Training Loss: 0.06414198130369186\n",
      "Epoch 12452/20000 Training Loss: 0.05466283857822418\n",
      "Epoch 12453/20000 Training Loss: 0.04627762734889984\n",
      "Epoch 12454/20000 Training Loss: 0.0739808976650238\n",
      "Epoch 12455/20000 Training Loss: 0.05414119362831116\n",
      "Epoch 12456/20000 Training Loss: 0.06330273300409317\n",
      "Epoch 12457/20000 Training Loss: 0.047514408826828\n",
      "Epoch 12458/20000 Training Loss: 0.07341628521680832\n",
      "Epoch 12459/20000 Training Loss: 0.06388602405786514\n",
      "Epoch 12460/20000 Training Loss: 0.07806267589330673\n",
      "Epoch 12460/20000 Validation Loss: 0.0486450120806694\n",
      "Epoch 12461/20000 Training Loss: 0.0491960346698761\n",
      "Epoch 12462/20000 Training Loss: 0.07106105238199234\n",
      "Epoch 12463/20000 Training Loss: 0.04634647071361542\n",
      "Epoch 12464/20000 Training Loss: 0.040949203073978424\n",
      "Epoch 12465/20000 Training Loss: 0.05007943511009216\n",
      "Epoch 12466/20000 Training Loss: 0.050043314695358276\n",
      "Epoch 12467/20000 Training Loss: 0.06965053826570511\n",
      "Epoch 12468/20000 Training Loss: 0.06276986747980118\n",
      "Epoch 12469/20000 Training Loss: 0.05359192565083504\n",
      "Epoch 12470/20000 Training Loss: 0.04830304905772209\n",
      "Epoch 12470/20000 Validation Loss: 0.05761396884918213\n",
      "Epoch 12471/20000 Training Loss: 0.06582912802696228\n",
      "Epoch 12472/20000 Training Loss: 0.051513269543647766\n",
      "Epoch 12473/20000 Training Loss: 0.04495217278599739\n",
      "Epoch 12474/20000 Training Loss: 0.07089348882436752\n",
      "Epoch 12475/20000 Training Loss: 0.05185919627547264\n",
      "Epoch 12476/20000 Training Loss: 0.07063533365726471\n",
      "Epoch 12477/20000 Training Loss: 0.05326420068740845\n",
      "Epoch 12478/20000 Training Loss: 0.066306933760643\n",
      "Epoch 12479/20000 Training Loss: 0.07176622748374939\n",
      "Epoch 12480/20000 Training Loss: 0.04459493234753609\n",
      "Epoch 12480/20000 Validation Loss: 0.0682728961110115\n",
      "Epoch 12481/20000 Training Loss: 0.05406487360596657\n",
      "Epoch 12482/20000 Training Loss: 0.06284881383180618\n",
      "Epoch 12483/20000 Training Loss: 0.052834879606962204\n",
      "Epoch 12484/20000 Training Loss: 0.06535318493843079\n",
      "Epoch 12485/20000 Training Loss: 0.04694901034235954\n",
      "Epoch 12486/20000 Training Loss: 0.059340160340070724\n",
      "Epoch 12487/20000 Training Loss: 0.05580900236964226\n",
      "Epoch 12488/20000 Training Loss: 0.04972095414996147\n",
      "Epoch 12489/20000 Training Loss: 0.06160089001059532\n",
      "Epoch 12490/20000 Training Loss: 0.04279441758990288\n",
      "Epoch 12490/20000 Validation Loss: 0.04255694895982742\n",
      "Epoch 12491/20000 Training Loss: 0.07284194231033325\n",
      "Epoch 12492/20000 Training Loss: 0.03965968266129494\n",
      "Epoch 12493/20000 Training Loss: 0.04750102758407593\n",
      "Epoch 12494/20000 Training Loss: 0.06808146089315414\n",
      "Epoch 12495/20000 Training Loss: 0.04734104871749878\n",
      "Epoch 12496/20000 Training Loss: 0.0524238646030426\n",
      "Epoch 12497/20000 Training Loss: 0.044867727905511856\n",
      "Epoch 12498/20000 Training Loss: 0.06844963878393173\n",
      "Epoch 12499/20000 Training Loss: 0.08111634105443954\n",
      "Epoch 12500/20000 Training Loss: 0.054622482508420944\n",
      "Epoch 12500/20000 Validation Loss: 0.06408188492059708\n",
      "Epoch 12501/20000 Training Loss: 0.04875684902071953\n",
      "Epoch 12502/20000 Training Loss: 0.03930706903338432\n",
      "Epoch 12503/20000 Training Loss: 0.03997426852583885\n",
      "Epoch 12504/20000 Training Loss: 0.04272264242172241\n",
      "Epoch 12505/20000 Training Loss: 0.047134190797805786\n",
      "Epoch 12506/20000 Training Loss: 0.05646960064768791\n",
      "Epoch 12507/20000 Training Loss: 0.0580371655523777\n",
      "Epoch 12508/20000 Training Loss: 0.05657283961772919\n",
      "Epoch 12509/20000 Training Loss: 0.04567793011665344\n",
      "Epoch 12510/20000 Training Loss: 0.045195940881967545\n",
      "Epoch 12510/20000 Validation Loss: 0.05936382710933685\n",
      "Epoch 12511/20000 Training Loss: 0.08597288280725479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12512/20000 Training Loss: 0.054385602474212646\n",
      "Epoch 12513/20000 Training Loss: 0.05104466900229454\n",
      "Epoch 12514/20000 Training Loss: 0.04557550325989723\n",
      "Epoch 12515/20000 Training Loss: 0.05130860581994057\n",
      "Epoch 12516/20000 Training Loss: 0.06782001256942749\n",
      "Epoch 12517/20000 Training Loss: 0.040987659245729446\n",
      "Epoch 12518/20000 Training Loss: 0.06161394715309143\n",
      "Epoch 12519/20000 Training Loss: 0.039621952921152115\n",
      "Epoch 12520/20000 Training Loss: 0.06270778924226761\n",
      "Epoch 12520/20000 Validation Loss: 0.04948066920042038\n",
      "Epoch 12521/20000 Training Loss: 0.06530973315238953\n",
      "Epoch 12522/20000 Training Loss: 0.05954594910144806\n",
      "Epoch 12523/20000 Training Loss: 0.06090955808758736\n",
      "Epoch 12524/20000 Training Loss: 0.06760504841804504\n",
      "Epoch 12525/20000 Training Loss: 0.04461732506752014\n",
      "Epoch 12526/20000 Training Loss: 0.07032556086778641\n",
      "Epoch 12527/20000 Training Loss: 0.052822355180978775\n",
      "Epoch 12528/20000 Training Loss: 0.04415901377797127\n",
      "Epoch 12529/20000 Training Loss: 0.0651361420750618\n",
      "Epoch 12530/20000 Training Loss: 0.05436519905924797\n",
      "Epoch 12530/20000 Validation Loss: 0.07222855091094971\n",
      "Epoch 12531/20000 Training Loss: 0.03619247302412987\n",
      "Epoch 12532/20000 Training Loss: 0.060246169567108154\n",
      "Epoch 12533/20000 Training Loss: 0.06327960640192032\n",
      "Epoch 12534/20000 Training Loss: 0.04116176441311836\n",
      "Epoch 12535/20000 Training Loss: 0.06027743220329285\n",
      "Epoch 12536/20000 Training Loss: 0.054322052747011185\n",
      "Epoch 12537/20000 Training Loss: 0.04231969639658928\n",
      "Epoch 12538/20000 Training Loss: 0.0575399249792099\n",
      "Epoch 12539/20000 Training Loss: 0.04543280974030495\n",
      "Epoch 12540/20000 Training Loss: 0.05952362343668938\n",
      "Epoch 12540/20000 Validation Loss: 0.038858234882354736\n",
      "Epoch 12541/20000 Training Loss: 0.059803660959005356\n",
      "Epoch 12542/20000 Training Loss: 0.054774343967437744\n",
      "Epoch 12543/20000 Training Loss: 0.0699479803442955\n",
      "Epoch 12544/20000 Training Loss: 0.049031198024749756\n",
      "Epoch 12545/20000 Training Loss: 0.07629237323999405\n",
      "Epoch 12546/20000 Training Loss: 0.06280069053173065\n",
      "Epoch 12547/20000 Training Loss: 0.054316092282533646\n",
      "Epoch 12548/20000 Training Loss: 0.04457421228289604\n",
      "Epoch 12549/20000 Training Loss: 0.042337287217378616\n",
      "Epoch 12550/20000 Training Loss: 0.05006273090839386\n",
      "Epoch 12550/20000 Validation Loss: 0.0628914162516594\n",
      "Epoch 12551/20000 Training Loss: 0.0650421604514122\n",
      "Epoch 12552/20000 Training Loss: 0.051480572670698166\n",
      "Epoch 12553/20000 Training Loss: 0.0399354062974453\n",
      "Epoch 12554/20000 Training Loss: 0.04838445410132408\n",
      "Epoch 12555/20000 Training Loss: 0.051786791533231735\n",
      "Epoch 12556/20000 Training Loss: 0.044726599007844925\n",
      "Epoch 12557/20000 Training Loss: 0.06815147399902344\n",
      "Epoch 12558/20000 Training Loss: 0.058909982442855835\n",
      "Epoch 12559/20000 Training Loss: 0.052014391869306564\n",
      "Epoch 12560/20000 Training Loss: 0.06041591987013817\n",
      "Epoch 12560/20000 Validation Loss: 0.05715750530362129\n",
      "Epoch 12561/20000 Training Loss: 0.05358957126736641\n",
      "Epoch 12562/20000 Training Loss: 0.06730086356401443\n",
      "Epoch 12563/20000 Training Loss: 0.060734253376722336\n",
      "Epoch 12564/20000 Training Loss: 0.05666431784629822\n",
      "Epoch 12565/20000 Training Loss: 0.04536483809351921\n",
      "Epoch 12566/20000 Training Loss: 0.05309684947133064\n",
      "Epoch 12567/20000 Training Loss: 0.057169485837221146\n",
      "Epoch 12568/20000 Training Loss: 0.06120714917778969\n",
      "Epoch 12569/20000 Training Loss: 0.05494800582528114\n",
      "Epoch 12570/20000 Training Loss: 0.04966476559638977\n",
      "Epoch 12570/20000 Validation Loss: 0.06002674251794815\n",
      "Epoch 12571/20000 Training Loss: 0.054998185485601425\n",
      "Epoch 12572/20000 Training Loss: 0.057799290865659714\n",
      "Epoch 12573/20000 Training Loss: 0.06365788727998734\n",
      "Epoch 12574/20000 Training Loss: 0.05310083553195\n",
      "Epoch 12575/20000 Training Loss: 0.05995406210422516\n",
      "Epoch 12576/20000 Training Loss: 0.06213170289993286\n",
      "Epoch 12577/20000 Training Loss: 0.06628149002790451\n",
      "Epoch 12578/20000 Training Loss: 0.08527228981256485\n",
      "Epoch 12579/20000 Training Loss: 0.057095546275377274\n",
      "Epoch 12580/20000 Training Loss: 0.05588027834892273\n",
      "Epoch 12580/20000 Validation Loss: 0.07953674346208572\n",
      "Epoch 12581/20000 Training Loss: 0.0547696016728878\n",
      "Epoch 12582/20000 Training Loss: 0.047612834721803665\n",
      "Epoch 12583/20000 Training Loss: 0.052201345562934875\n",
      "Epoch 12584/20000 Training Loss: 0.0692003145813942\n",
      "Epoch 12585/20000 Training Loss: 0.06670906394720078\n",
      "Epoch 12586/20000 Training Loss: 0.061750318855047226\n",
      "Epoch 12587/20000 Training Loss: 0.08305541425943375\n",
      "Epoch 12588/20000 Training Loss: 0.053046371787786484\n",
      "Epoch 12589/20000 Training Loss: 0.055903803557157516\n",
      "Epoch 12590/20000 Training Loss: 0.044848278164863586\n",
      "Epoch 12590/20000 Validation Loss: 0.04713787883520126\n",
      "Epoch 12591/20000 Training Loss: 0.05883277580142021\n",
      "Epoch 12592/20000 Training Loss: 0.05503750964999199\n",
      "Epoch 12593/20000 Training Loss: 0.04624536260962486\n",
      "Epoch 12594/20000 Training Loss: 0.032478246837854385\n",
      "Epoch 12595/20000 Training Loss: 0.04023370519280434\n",
      "Epoch 12596/20000 Training Loss: 0.04902828857302666\n",
      "Epoch 12597/20000 Training Loss: 0.04601592943072319\n",
      "Epoch 12598/20000 Training Loss: 0.06202824041247368\n",
      "Epoch 12599/20000 Training Loss: 0.04489883780479431\n",
      "Epoch 12600/20000 Training Loss: 0.03869897499680519\n",
      "Epoch 12600/20000 Validation Loss: 0.06223912537097931\n",
      "Epoch 12601/20000 Training Loss: 0.0716690793633461\n",
      "Epoch 12602/20000 Training Loss: 0.05814224109053612\n",
      "Epoch 12603/20000 Training Loss: 0.05278690531849861\n",
      "Epoch 12604/20000 Training Loss: 0.06913863122463226\n",
      "Epoch 12605/20000 Training Loss: 0.08064114302396774\n",
      "Epoch 12606/20000 Training Loss: 0.05675194784998894\n",
      "Epoch 12607/20000 Training Loss: 0.048579368740320206\n",
      "Epoch 12608/20000 Training Loss: 0.054846543818712234\n",
      "Epoch 12609/20000 Training Loss: 0.06441637128591537\n",
      "Epoch 12610/20000 Training Loss: 0.049335598945617676\n",
      "Epoch 12610/20000 Validation Loss: 0.056845445185899734\n",
      "Epoch 12611/20000 Training Loss: 0.06680362671613693\n",
      "Epoch 12612/20000 Training Loss: 0.06692063808441162\n",
      "Epoch 12613/20000 Training Loss: 0.06488024443387985\n",
      "Epoch 12614/20000 Training Loss: 0.061051059514284134\n",
      "Epoch 12615/20000 Training Loss: 0.05673889443278313\n",
      "Epoch 12616/20000 Training Loss: 0.05681643262505531\n",
      "Epoch 12617/20000 Training Loss: 0.043346110731363297\n",
      "Epoch 12618/20000 Training Loss: 0.06771217286586761\n",
      "Epoch 12619/20000 Training Loss: 0.056727249175310135\n",
      "Epoch 12620/20000 Training Loss: 0.07200872153043747\n",
      "Epoch 12620/20000 Validation Loss: 0.052546679973602295\n",
      "Epoch 12621/20000 Training Loss: 0.060270149260759354\n",
      "Epoch 12622/20000 Training Loss: 0.06150457635521889\n",
      "Epoch 12623/20000 Training Loss: 0.058128658682107925\n",
      "Epoch 12624/20000 Training Loss: 0.06642089039087296\n",
      "Epoch 12625/20000 Training Loss: 0.07210763543844223\n",
      "Epoch 12626/20000 Training Loss: 0.06492670625448227\n",
      "Epoch 12627/20000 Training Loss: 0.06349512189626694\n",
      "Epoch 12628/20000 Training Loss: 0.0437723807990551\n",
      "Epoch 12629/20000 Training Loss: 0.05088642239570618\n",
      "Epoch 12630/20000 Training Loss: 0.04944194480776787\n",
      "Epoch 12630/20000 Validation Loss: 0.06841956079006195\n",
      "Epoch 12631/20000 Training Loss: 0.06231670454144478\n",
      "Epoch 12632/20000 Training Loss: 0.05969610437750816\n",
      "Epoch 12633/20000 Training Loss: 0.03362322598695755\n",
      "Epoch 12634/20000 Training Loss: 0.056124597787857056\n",
      "Epoch 12635/20000 Training Loss: 0.0679851844906807\n",
      "Epoch 12636/20000 Training Loss: 0.08041082322597504\n",
      "Epoch 12637/20000 Training Loss: 0.04236125573515892\n",
      "Epoch 12638/20000 Training Loss: 0.052741631865501404\n",
      "Epoch 12639/20000 Training Loss: 0.05357932671904564\n",
      "Epoch 12640/20000 Training Loss: 0.04354807734489441\n",
      "Epoch 12640/20000 Validation Loss: 0.06663741916418076\n",
      "Epoch 12641/20000 Training Loss: 0.04758960008621216\n",
      "Epoch 12642/20000 Training Loss: 0.06584743410348892\n",
      "Epoch 12643/20000 Training Loss: 0.04126300290226936\n",
      "Epoch 12644/20000 Training Loss: 0.05350401997566223\n",
      "Epoch 12645/20000 Training Loss: 0.04940468445420265\n",
      "Epoch 12646/20000 Training Loss: 0.04845239594578743\n",
      "Epoch 12647/20000 Training Loss: 0.05763040482997894\n",
      "Epoch 12648/20000 Training Loss: 0.054464828222990036\n",
      "Epoch 12649/20000 Training Loss: 0.04663442447781563\n",
      "Epoch 12650/20000 Training Loss: 0.04677437245845795\n",
      "Epoch 12650/20000 Validation Loss: 0.054210808128118515\n",
      "Epoch 12651/20000 Training Loss: 0.0664672777056694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12652/20000 Training Loss: 0.05886681750416756\n",
      "Epoch 12653/20000 Training Loss: 0.04557099565863609\n",
      "Epoch 12654/20000 Training Loss: 0.04391925409436226\n",
      "Epoch 12655/20000 Training Loss: 0.06741369515657425\n",
      "Epoch 12656/20000 Training Loss: 0.04003262147307396\n",
      "Epoch 12657/20000 Training Loss: 0.04442257061600685\n",
      "Epoch 12658/20000 Training Loss: 0.05621906742453575\n",
      "Epoch 12659/20000 Training Loss: 0.07223296910524368\n",
      "Epoch 12660/20000 Training Loss: 0.0594954676926136\n",
      "Epoch 12660/20000 Validation Loss: 0.04978030174970627\n",
      "Epoch 12661/20000 Training Loss: 0.06420540064573288\n",
      "Epoch 12662/20000 Training Loss: 0.05750425159931183\n",
      "Epoch 12663/20000 Training Loss: 0.08451030403375626\n",
      "Epoch 12664/20000 Training Loss: 0.060918230563402176\n",
      "Epoch 12665/20000 Training Loss: 0.048784006386995316\n",
      "Epoch 12666/20000 Training Loss: 0.055914442986249924\n",
      "Epoch 12667/20000 Training Loss: 0.03832001984119415\n",
      "Epoch 12668/20000 Training Loss: 0.06359709054231644\n",
      "Epoch 12669/20000 Training Loss: 0.05769418179988861\n",
      "Epoch 12670/20000 Training Loss: 0.050310712307691574\n",
      "Epoch 12670/20000 Validation Loss: 0.06666982918977737\n",
      "Epoch 12671/20000 Training Loss: 0.05326993390917778\n",
      "Epoch 12672/20000 Training Loss: 0.05950850248336792\n",
      "Epoch 12673/20000 Training Loss: 0.04618716612458229\n",
      "Epoch 12674/20000 Training Loss: 0.03705918416380882\n",
      "Epoch 12675/20000 Training Loss: 0.06536161154508591\n",
      "Epoch 12676/20000 Training Loss: 0.049147266894578934\n",
      "Epoch 12677/20000 Training Loss: 0.04871122166514397\n",
      "Epoch 12678/20000 Training Loss: 0.05624431371688843\n",
      "Epoch 12679/20000 Training Loss: 0.05432164669036865\n",
      "Epoch 12680/20000 Training Loss: 0.0637768879532814\n",
      "Epoch 12680/20000 Validation Loss: 0.056737542152404785\n",
      "Epoch 12681/20000 Training Loss: 0.04556040093302727\n",
      "Epoch 12682/20000 Training Loss: 0.070841483771801\n",
      "Epoch 12683/20000 Training Loss: 0.04834814369678497\n",
      "Epoch 12684/20000 Training Loss: 0.05981718376278877\n",
      "Epoch 12685/20000 Training Loss: 0.06421644985675812\n",
      "Epoch 12686/20000 Training Loss: 0.058741599321365356\n",
      "Epoch 12687/20000 Training Loss: 0.056747958064079285\n",
      "Epoch 12688/20000 Training Loss: 0.04402047023177147\n",
      "Epoch 12689/20000 Training Loss: 0.04416991397738457\n",
      "Epoch 12690/20000 Training Loss: 0.055873434990644455\n",
      "Epoch 12690/20000 Validation Loss: 0.04739423841238022\n",
      "Epoch 12691/20000 Training Loss: 0.04918551445007324\n",
      "Epoch 12692/20000 Training Loss: 0.05379501357674599\n",
      "Epoch 12693/20000 Training Loss: 0.04901222512125969\n",
      "Epoch 12694/20000 Training Loss: 0.06534627825021744\n",
      "Epoch 12695/20000 Training Loss: 0.05903688073158264\n",
      "Epoch 12696/20000 Training Loss: 0.051023930311203\n",
      "Epoch 12697/20000 Training Loss: 0.0347643718123436\n",
      "Epoch 12698/20000 Training Loss: 0.04964308813214302\n",
      "Epoch 12699/20000 Training Loss: 0.07155483216047287\n",
      "Epoch 12700/20000 Training Loss: 0.03916005790233612\n",
      "Epoch 12700/20000 Validation Loss: 0.08389528840780258\n",
      "Epoch 12701/20000 Training Loss: 0.03863200172781944\n",
      "Epoch 12702/20000 Training Loss: 0.05019459128379822\n",
      "Epoch 12703/20000 Training Loss: 0.06387460976839066\n",
      "Epoch 12704/20000 Training Loss: 0.06062788888812065\n",
      "Epoch 12705/20000 Training Loss: 0.06556689739227295\n",
      "Epoch 12706/20000 Training Loss: 0.046703387051820755\n",
      "Epoch 12707/20000 Training Loss: 0.06326007097959518\n",
      "Epoch 12708/20000 Training Loss: 0.05131049081683159\n",
      "Epoch 12709/20000 Training Loss: 0.045957986265420914\n",
      "Epoch 12710/20000 Training Loss: 0.05651712790131569\n",
      "Epoch 12710/20000 Validation Loss: 0.07747142016887665\n",
      "Epoch 12711/20000 Training Loss: 0.05784602090716362\n",
      "Epoch 12712/20000 Training Loss: 0.05969274044036865\n",
      "Epoch 12713/20000 Training Loss: 0.04955180361866951\n",
      "Epoch 12714/20000 Training Loss: 0.03758649155497551\n",
      "Epoch 12715/20000 Training Loss: 0.0622522346675396\n",
      "Epoch 12716/20000 Training Loss: 0.0762968584895134\n",
      "Epoch 12717/20000 Training Loss: 0.04818810150027275\n",
      "Epoch 12718/20000 Training Loss: 0.0547548346221447\n",
      "Epoch 12719/20000 Training Loss: 0.05003119632601738\n",
      "Epoch 12720/20000 Training Loss: 0.06256461888551712\n",
      "Epoch 12720/20000 Validation Loss: 0.05015745013952255\n",
      "Epoch 12721/20000 Training Loss: 0.08879328519105911\n",
      "Epoch 12722/20000 Training Loss: 0.05360768362879753\n",
      "Epoch 12723/20000 Training Loss: 0.05767064169049263\n",
      "Epoch 12724/20000 Training Loss: 0.05752113088965416\n",
      "Epoch 12725/20000 Training Loss: 0.04316936060786247\n",
      "Epoch 12726/20000 Training Loss: 0.046438876539468765\n",
      "Epoch 12727/20000 Training Loss: 0.04989555478096008\n",
      "Epoch 12728/20000 Training Loss: 0.0628751888871193\n",
      "Epoch 12729/20000 Training Loss: 0.0614105649292469\n",
      "Epoch 12730/20000 Training Loss: 0.057405758649110794\n",
      "Epoch 12730/20000 Validation Loss: 0.0703456699848175\n",
      "Epoch 12731/20000 Training Loss: 0.04770982265472412\n",
      "Epoch 12732/20000 Training Loss: 0.04215024784207344\n",
      "Epoch 12733/20000 Training Loss: 0.06127519533038139\n",
      "Epoch 12734/20000 Training Loss: 0.05234914645552635\n",
      "Epoch 12735/20000 Training Loss: 0.05015124753117561\n",
      "Epoch 12736/20000 Training Loss: 0.04883883520960808\n",
      "Epoch 12737/20000 Training Loss: 0.0651036724448204\n",
      "Epoch 12738/20000 Training Loss: 0.052215587347745895\n",
      "Epoch 12739/20000 Training Loss: 0.042380284518003464\n",
      "Epoch 12740/20000 Training Loss: 0.04975724592804909\n",
      "Epoch 12740/20000 Validation Loss: 0.05209019035100937\n",
      "Epoch 12741/20000 Training Loss: 0.05981510877609253\n",
      "Epoch 12742/20000 Training Loss: 0.0431661494076252\n",
      "Epoch 12743/20000 Training Loss: 0.05295586213469505\n",
      "Epoch 12744/20000 Training Loss: 0.06395501643419266\n",
      "Epoch 12745/20000 Training Loss: 0.06802108138799667\n",
      "Epoch 12746/20000 Training Loss: 0.05199583247303963\n",
      "Epoch 12747/20000 Training Loss: 0.07130677253007889\n",
      "Epoch 12748/20000 Training Loss: 0.07414665818214417\n",
      "Epoch 12749/20000 Training Loss: 0.05497394874691963\n",
      "Epoch 12750/20000 Training Loss: 0.05832869932055473\n",
      "Epoch 12750/20000 Validation Loss: 0.04960808530449867\n",
      "Epoch 12751/20000 Training Loss: 0.06152696534991264\n",
      "Epoch 12752/20000 Training Loss: 0.05766860768198967\n",
      "Epoch 12753/20000 Training Loss: 0.05426992103457451\n",
      "Epoch 12754/20000 Training Loss: 0.054483454674482346\n",
      "Epoch 12755/20000 Training Loss: 0.06198563054203987\n",
      "Epoch 12756/20000 Training Loss: 0.05007649585604668\n",
      "Epoch 12757/20000 Training Loss: 0.059287179261446\n",
      "Epoch 12758/20000 Training Loss: 0.0568283349275589\n",
      "Epoch 12759/20000 Training Loss: 0.06034845486283302\n",
      "Epoch 12760/20000 Training Loss: 0.06578412652015686\n",
      "Epoch 12760/20000 Validation Loss: 0.074423648416996\n",
      "Epoch 12761/20000 Training Loss: 0.05530118569731712\n",
      "Epoch 12762/20000 Training Loss: 0.05481128394603729\n",
      "Epoch 12763/20000 Training Loss: 0.06492980569601059\n",
      "Epoch 12764/20000 Training Loss: 0.06257861107587814\n",
      "Epoch 12765/20000 Training Loss: 0.05736704543232918\n",
      "Epoch 12766/20000 Training Loss: 0.059630658477544785\n",
      "Epoch 12767/20000 Training Loss: 0.05719931423664093\n",
      "Epoch 12768/20000 Training Loss: 0.0638374388217926\n",
      "Epoch 12769/20000 Training Loss: 0.041465356945991516\n",
      "Epoch 12770/20000 Training Loss: 0.06250753998756409\n",
      "Epoch 12770/20000 Validation Loss: 0.05852323770523071\n",
      "Epoch 12771/20000 Training Loss: 0.05630208179354668\n",
      "Epoch 12772/20000 Training Loss: 0.06391751766204834\n",
      "Epoch 12773/20000 Training Loss: 0.04414088651537895\n",
      "Epoch 12774/20000 Training Loss: 0.054297205060720444\n",
      "Epoch 12775/20000 Training Loss: 0.0802328810095787\n",
      "Epoch 12776/20000 Training Loss: 0.04806065186858177\n",
      "Epoch 12777/20000 Training Loss: 0.04802991822361946\n",
      "Epoch 12778/20000 Training Loss: 0.05790768191218376\n",
      "Epoch 12779/20000 Training Loss: 0.054017405956983566\n",
      "Epoch 12780/20000 Training Loss: 0.051558468490839005\n",
      "Epoch 12780/20000 Validation Loss: 0.06069905310869217\n",
      "Epoch 12781/20000 Training Loss: 0.0681936964392662\n",
      "Epoch 12782/20000 Training Loss: 0.05484697222709656\n",
      "Epoch 12783/20000 Training Loss: 0.06384459882974625\n",
      "Epoch 12784/20000 Training Loss: 0.06382694840431213\n",
      "Epoch 12785/20000 Training Loss: 0.06014389172196388\n",
      "Epoch 12786/20000 Training Loss: 0.056946706026792526\n",
      "Epoch 12787/20000 Training Loss: 0.06596610695123672\n",
      "Epoch 12788/20000 Training Loss: 0.04902264475822449\n",
      "Epoch 12789/20000 Training Loss: 0.06362156569957733\n",
      "Epoch 12790/20000 Training Loss: 0.08621605485677719\n",
      "Epoch 12790/20000 Validation Loss: 0.04961723834276199\n",
      "Epoch 12791/20000 Training Loss: 0.0779637023806572\n",
      "Epoch 12792/20000 Training Loss: 0.046894874423742294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12793/20000 Training Loss: 0.06941793113946915\n",
      "Epoch 12794/20000 Training Loss: 0.054377373307943344\n",
      "Epoch 12795/20000 Training Loss: 0.061949294060468674\n",
      "Epoch 12796/20000 Training Loss: 0.04154876992106438\n",
      "Epoch 12797/20000 Training Loss: 0.04678243398666382\n",
      "Epoch 12798/20000 Training Loss: 0.04884468391537666\n",
      "Epoch 12799/20000 Training Loss: 0.0463898740708828\n",
      "Epoch 12800/20000 Training Loss: 0.04678801819682121\n",
      "Epoch 12800/20000 Validation Loss: 0.06628353148698807\n",
      "Epoch 12801/20000 Training Loss: 0.057575907558202744\n",
      "Epoch 12802/20000 Training Loss: 0.05644312500953674\n",
      "Epoch 12803/20000 Training Loss: 0.025179993361234665\n",
      "Epoch 12804/20000 Training Loss: 0.044826358556747437\n",
      "Epoch 12805/20000 Training Loss: 0.05508560314774513\n",
      "Epoch 12806/20000 Training Loss: 0.06590305268764496\n",
      "Epoch 12807/20000 Training Loss: 0.05847152695059776\n",
      "Epoch 12808/20000 Training Loss: 0.043498892337083817\n",
      "Epoch 12809/20000 Training Loss: 0.06062036752700806\n",
      "Epoch 12810/20000 Training Loss: 0.06431466341018677\n",
      "Epoch 12810/20000 Validation Loss: 0.08003265410661697\n",
      "Epoch 12811/20000 Training Loss: 0.055772554129362106\n",
      "Epoch 12812/20000 Training Loss: 0.05707873776555061\n",
      "Epoch 12813/20000 Training Loss: 0.056586164981126785\n",
      "Epoch 12814/20000 Training Loss: 0.07672920823097229\n",
      "Epoch 12815/20000 Training Loss: 0.07929966598749161\n",
      "Epoch 12816/20000 Training Loss: 0.057317446917295456\n",
      "Epoch 12817/20000 Training Loss: 0.050525445491075516\n",
      "Epoch 12818/20000 Training Loss: 0.054390352219343185\n",
      "Epoch 12819/20000 Training Loss: 0.0439225435256958\n",
      "Epoch 12820/20000 Training Loss: 0.059459298849105835\n",
      "Epoch 12820/20000 Validation Loss: 0.04259411245584488\n",
      "Epoch 12821/20000 Training Loss: 0.04509037733078003\n",
      "Epoch 12822/20000 Training Loss: 0.06224571168422699\n",
      "Epoch 12823/20000 Training Loss: 0.057224299758672714\n",
      "Epoch 12824/20000 Training Loss: 0.06692661345005035\n",
      "Epoch 12825/20000 Training Loss: 0.047545064240694046\n",
      "Epoch 12826/20000 Training Loss: 0.06013030186295509\n",
      "Epoch 12827/20000 Training Loss: 0.060654208064079285\n",
      "Epoch 12828/20000 Training Loss: 0.061002928763628006\n",
      "Epoch 12829/20000 Training Loss: 0.05349104478955269\n",
      "Epoch 12830/20000 Training Loss: 0.051243435591459274\n",
      "Epoch 12830/20000 Validation Loss: 0.05985262989997864\n",
      "Epoch 12831/20000 Training Loss: 0.05902145802974701\n",
      "Epoch 12832/20000 Training Loss: 0.06906234472990036\n",
      "Epoch 12833/20000 Training Loss: 0.050858382135629654\n",
      "Epoch 12834/20000 Training Loss: 0.06733372807502747\n",
      "Epoch 12835/20000 Training Loss: 0.055057093501091\n",
      "Epoch 12836/20000 Training Loss: 0.07285711914300919\n",
      "Epoch 12837/20000 Training Loss: 0.07046118378639221\n",
      "Epoch 12838/20000 Training Loss: 0.06708065420389175\n",
      "Epoch 12839/20000 Training Loss: 0.059176500886678696\n",
      "Epoch 12840/20000 Training Loss: 0.05320543050765991\n",
      "Epoch 12840/20000 Validation Loss: 0.03755737468600273\n",
      "Epoch 12841/20000 Training Loss: 0.04878659546375275\n",
      "Epoch 12842/20000 Training Loss: 0.05199125409126282\n",
      "Epoch 12843/20000 Training Loss: 0.05673665180802345\n",
      "Epoch 12844/20000 Training Loss: 0.04900561273097992\n",
      "Epoch 12845/20000 Training Loss: 0.05723124369978905\n",
      "Epoch 12846/20000 Training Loss: 0.06467913836240768\n",
      "Epoch 12847/20000 Training Loss: 0.03992326185107231\n",
      "Epoch 12848/20000 Training Loss: 0.04888490214943886\n",
      "Epoch 12849/20000 Training Loss: 0.048488128930330276\n",
      "Epoch 12850/20000 Training Loss: 0.0672159418463707\n",
      "Epoch 12850/20000 Validation Loss: 0.07372801005840302\n",
      "Epoch 12851/20000 Training Loss: 0.04766739532351494\n",
      "Epoch 12852/20000 Training Loss: 0.05126678943634033\n",
      "Epoch 12853/20000 Training Loss: 0.05867346003651619\n",
      "Epoch 12854/20000 Training Loss: 0.050708819180727005\n",
      "Epoch 12855/20000 Training Loss: 0.06839488446712494\n",
      "Epoch 12856/20000 Training Loss: 0.051408249884843826\n",
      "Epoch 12857/20000 Training Loss: 0.04384233430027962\n",
      "Epoch 12858/20000 Training Loss: 0.06031579151749611\n",
      "Epoch 12859/20000 Training Loss: 0.051540080457925797\n",
      "Epoch 12860/20000 Training Loss: 0.05531580373644829\n",
      "Epoch 12860/20000 Validation Loss: 0.04266718402504921\n",
      "Epoch 12861/20000 Training Loss: 0.07082018256187439\n",
      "Epoch 12862/20000 Training Loss: 0.06205839291214943\n",
      "Epoch 12863/20000 Training Loss: 0.05367160961031914\n",
      "Epoch 12864/20000 Training Loss: 0.046285707503557205\n",
      "Epoch 12865/20000 Training Loss: 0.06287123262882233\n",
      "Epoch 12866/20000 Training Loss: 0.0537937693297863\n",
      "Epoch 12867/20000 Training Loss: 0.0592116117477417\n",
      "Epoch 12868/20000 Training Loss: 0.060702648013830185\n",
      "Epoch 12869/20000 Training Loss: 0.07133278995752335\n",
      "Epoch 12870/20000 Training Loss: 0.05160138010978699\n",
      "Epoch 12870/20000 Validation Loss: 0.06892463564872742\n",
      "Epoch 12871/20000 Training Loss: 0.062413331121206284\n",
      "Epoch 12872/20000 Training Loss: 0.04853369668126106\n",
      "Epoch 12873/20000 Training Loss: 0.04675084352493286\n",
      "Epoch 12874/20000 Training Loss: 0.07129952311515808\n",
      "Epoch 12875/20000 Training Loss: 0.054955366998910904\n",
      "Epoch 12876/20000 Training Loss: 0.06205813214182854\n",
      "Epoch 12877/20000 Training Loss: 0.054394740611314774\n",
      "Epoch 12878/20000 Training Loss: 0.06791897863149643\n",
      "Epoch 12879/20000 Training Loss: 0.05807110294699669\n",
      "Epoch 12880/20000 Training Loss: 0.06044711545109749\n",
      "Epoch 12880/20000 Validation Loss: 0.08053714036941528\n",
      "Epoch 12881/20000 Training Loss: 0.05151120200753212\n",
      "Epoch 12882/20000 Training Loss: 0.07214286923408508\n",
      "Epoch 12883/20000 Training Loss: 0.04822913929820061\n",
      "Epoch 12884/20000 Training Loss: 0.06142832338809967\n",
      "Epoch 12885/20000 Training Loss: 0.04291076585650444\n",
      "Epoch 12886/20000 Training Loss: 0.04802703484892845\n",
      "Epoch 12887/20000 Training Loss: 0.04952402412891388\n",
      "Epoch 12888/20000 Training Loss: 0.04839743301272392\n",
      "Epoch 12889/20000 Training Loss: 0.06307753175497055\n",
      "Epoch 12890/20000 Training Loss: 0.061821099370718\n",
      "Epoch 12890/20000 Validation Loss: 0.09051041305065155\n",
      "Epoch 12891/20000 Training Loss: 0.05350587144494057\n",
      "Epoch 12892/20000 Training Loss: 0.06203805282711983\n",
      "Epoch 12893/20000 Training Loss: 0.057053808122873306\n",
      "Epoch 12894/20000 Training Loss: 0.06417423486709595\n",
      "Epoch 12895/20000 Training Loss: 0.046709705144166946\n",
      "Epoch 12896/20000 Training Loss: 0.06367906928062439\n",
      "Epoch 12897/20000 Training Loss: 0.053076598793268204\n",
      "Epoch 12898/20000 Training Loss: 0.05879386141896248\n",
      "Epoch 12899/20000 Training Loss: 0.05085188150405884\n",
      "Epoch 12900/20000 Training Loss: 0.046661268919706345\n",
      "Epoch 12900/20000 Validation Loss: 0.033541709184646606\n",
      "Epoch 12901/20000 Training Loss: 0.04576254263520241\n",
      "Epoch 12902/20000 Training Loss: 0.05960596725344658\n",
      "Epoch 12903/20000 Training Loss: 0.04872995242476463\n",
      "Epoch 12904/20000 Training Loss: 0.05964423716068268\n",
      "Epoch 12905/20000 Training Loss: 0.06284523755311966\n",
      "Epoch 12906/20000 Training Loss: 0.04071807861328125\n",
      "Epoch 12907/20000 Training Loss: 0.05979783460497856\n",
      "Epoch 12908/20000 Training Loss: 0.05554281547665596\n",
      "Epoch 12909/20000 Training Loss: 0.05235333368182182\n",
      "Epoch 12910/20000 Training Loss: 0.040451135486364365\n",
      "Epoch 12910/20000 Validation Loss: 0.05517672002315521\n",
      "Epoch 12911/20000 Training Loss: 0.07359011471271515\n",
      "Epoch 12912/20000 Training Loss: 0.0546865276992321\n",
      "Epoch 12913/20000 Training Loss: 0.05134722590446472\n",
      "Epoch 12914/20000 Training Loss: 0.05915294215083122\n",
      "Epoch 12915/20000 Training Loss: 0.061594944447278976\n",
      "Epoch 12916/20000 Training Loss: 0.05872151628136635\n",
      "Epoch 12917/20000 Training Loss: 0.053505733609199524\n",
      "Epoch 12918/20000 Training Loss: 0.06249998137354851\n",
      "Epoch 12919/20000 Training Loss: 0.04548109695315361\n",
      "Epoch 12920/20000 Training Loss: 0.06812844425439835\n",
      "Epoch 12920/20000 Validation Loss: 0.058969736099243164\n",
      "Epoch 12921/20000 Training Loss: 0.03985774889588356\n",
      "Epoch 12922/20000 Training Loss: 0.04553608596324921\n",
      "Epoch 12923/20000 Training Loss: 0.05114445090293884\n",
      "Epoch 12924/20000 Training Loss: 0.03611837700009346\n",
      "Epoch 12925/20000 Training Loss: 0.06843655556440353\n",
      "Epoch 12926/20000 Training Loss: 0.05205623805522919\n",
      "Epoch 12927/20000 Training Loss: 0.05064181610941887\n",
      "Epoch 12928/20000 Training Loss: 0.04349929094314575\n",
      "Epoch 12929/20000 Training Loss: 0.07098903506994247\n",
      "Epoch 12930/20000 Training Loss: 0.04872449114918709\n",
      "Epoch 12930/20000 Validation Loss: 0.05543055385351181\n",
      "Epoch 12931/20000 Training Loss: 0.0464787594974041\n",
      "Epoch 12932/20000 Training Loss: 0.0551009438931942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12933/20000 Training Loss: 0.05120350793004036\n",
      "Epoch 12934/20000 Training Loss: 0.050281405448913574\n",
      "Epoch 12935/20000 Training Loss: 0.047046463936567307\n",
      "Epoch 12936/20000 Training Loss: 0.061508286744356155\n",
      "Epoch 12937/20000 Training Loss: 0.04907717928290367\n",
      "Epoch 12938/20000 Training Loss: 0.04608367756009102\n",
      "Epoch 12939/20000 Training Loss: 0.053849343210458755\n",
      "Epoch 12940/20000 Training Loss: 0.05780910328030586\n",
      "Epoch 12940/20000 Validation Loss: 0.04274890571832657\n",
      "Epoch 12941/20000 Training Loss: 0.05324600636959076\n",
      "Epoch 12942/20000 Training Loss: 0.06891363114118576\n",
      "Epoch 12943/20000 Training Loss: 0.071246437728405\n",
      "Epoch 12944/20000 Training Loss: 0.06127173826098442\n",
      "Epoch 12945/20000 Training Loss: 0.05431145802140236\n",
      "Epoch 12946/20000 Training Loss: 0.054817408323287964\n",
      "Epoch 12947/20000 Training Loss: 0.055833619087934494\n",
      "Epoch 12948/20000 Training Loss: 0.061897728592157364\n",
      "Epoch 12949/20000 Training Loss: 0.06429802626371384\n",
      "Epoch 12950/20000 Training Loss: 0.06567958742380142\n",
      "Epoch 12950/20000 Validation Loss: 0.059532999992370605\n",
      "Epoch 12951/20000 Training Loss: 0.08153858035802841\n",
      "Epoch 12952/20000 Training Loss: 0.05361374095082283\n",
      "Epoch 12953/20000 Training Loss: 0.04797445610165596\n",
      "Epoch 12954/20000 Training Loss: 0.050697892904281616\n",
      "Epoch 12955/20000 Training Loss: 0.06099717319011688\n",
      "Epoch 12956/20000 Training Loss: 0.04756505414843559\n",
      "Epoch 12957/20000 Training Loss: 0.054105788469314575\n",
      "Epoch 12958/20000 Training Loss: 0.05802576616406441\n",
      "Epoch 12959/20000 Training Loss: 0.05425215885043144\n",
      "Epoch 12960/20000 Training Loss: 0.06461847573518753\n",
      "Epoch 12960/20000 Validation Loss: 0.06596371531486511\n",
      "Epoch 12961/20000 Training Loss: 0.03751656785607338\n",
      "Epoch 12962/20000 Training Loss: 0.0611640103161335\n",
      "Epoch 12963/20000 Training Loss: 0.059342462569475174\n",
      "Epoch 12964/20000 Training Loss: 0.0529768280684948\n",
      "Epoch 12965/20000 Training Loss: 0.061140965670347214\n",
      "Epoch 12966/20000 Training Loss: 0.056651998311281204\n",
      "Epoch 12967/20000 Training Loss: 0.05674009025096893\n",
      "Epoch 12968/20000 Training Loss: 0.046722691506147385\n",
      "Epoch 12969/20000 Training Loss: 0.05818946287035942\n",
      "Epoch 12970/20000 Training Loss: 0.05210628733038902\n",
      "Epoch 12970/20000 Validation Loss: 0.047357797622680664\n",
      "Epoch 12971/20000 Training Loss: 0.05183906480669975\n",
      "Epoch 12972/20000 Training Loss: 0.04743676260113716\n",
      "Epoch 12973/20000 Training Loss: 0.05589354410767555\n",
      "Epoch 12974/20000 Training Loss: 0.03491705283522606\n",
      "Epoch 12975/20000 Training Loss: 0.07489504665136337\n",
      "Epoch 12976/20000 Training Loss: 0.040959432721138\n",
      "Epoch 12977/20000 Training Loss: 0.048966873437166214\n",
      "Epoch 12978/20000 Training Loss: 0.041901398450136185\n",
      "Epoch 12979/20000 Training Loss: 0.04702988266944885\n",
      "Epoch 12980/20000 Training Loss: 0.044966697692871094\n",
      "Epoch 12980/20000 Validation Loss: 0.04147849604487419\n",
      "Epoch 12981/20000 Training Loss: 0.05674164369702339\n",
      "Epoch 12982/20000 Training Loss: 0.05519701540470123\n",
      "Epoch 12983/20000 Training Loss: 0.0567111074924469\n",
      "Epoch 12984/20000 Training Loss: 0.04647582396864891\n",
      "Epoch 12985/20000 Training Loss: 0.06144704297184944\n",
      "Epoch 12986/20000 Training Loss: 0.050522297620773315\n",
      "Epoch 12987/20000 Training Loss: 0.05422784760594368\n",
      "Epoch 12988/20000 Training Loss: 0.05027120187878609\n",
      "Epoch 12989/20000 Training Loss: 0.05448661744594574\n",
      "Epoch 12990/20000 Training Loss: 0.05362410843372345\n",
      "Epoch 12990/20000 Validation Loss: 0.06114766746759415\n",
      "Epoch 12991/20000 Training Loss: 0.05598435923457146\n",
      "Epoch 12992/20000 Training Loss: 0.05680881813168526\n",
      "Epoch 12993/20000 Training Loss: 0.053136441856622696\n",
      "Epoch 12994/20000 Training Loss: 0.056987833231687546\n",
      "Epoch 12995/20000 Training Loss: 0.04468584060668945\n",
      "Epoch 12996/20000 Training Loss: 0.043554022908210754\n",
      "Epoch 12997/20000 Training Loss: 0.05578390881419182\n",
      "Epoch 12998/20000 Training Loss: 0.058879852294921875\n",
      "Epoch 12999/20000 Training Loss: 0.043327972292900085\n",
      "Epoch 13000/20000 Training Loss: 0.039103370159864426\n",
      "Epoch 13000/20000 Validation Loss: 0.0600256472826004\n",
      "Epoch 13001/20000 Training Loss: 0.06790398806333542\n",
      "Epoch 13002/20000 Training Loss: 0.0643029436469078\n",
      "Epoch 13003/20000 Training Loss: 0.04885762929916382\n",
      "Epoch 13004/20000 Training Loss: 0.05682704970240593\n",
      "Epoch 13005/20000 Training Loss: 0.045369867235422134\n",
      "Epoch 13006/20000 Training Loss: 0.046854984015226364\n",
      "Epoch 13007/20000 Training Loss: 0.05530490353703499\n",
      "Epoch 13008/20000 Training Loss: 0.05422617495059967\n",
      "Epoch 13009/20000 Training Loss: 0.07514241337776184\n",
      "Epoch 13010/20000 Training Loss: 0.048902954906225204\n",
      "Epoch 13010/20000 Validation Loss: 0.05443577468395233\n",
      "Epoch 13011/20000 Training Loss: 0.044623687863349915\n",
      "Epoch 13012/20000 Training Loss: 0.06218909099698067\n",
      "Epoch 13013/20000 Training Loss: 0.055386852473020554\n",
      "Epoch 13014/20000 Training Loss: 0.04369017481803894\n",
      "Epoch 13015/20000 Training Loss: 0.055232685059309006\n",
      "Epoch 13016/20000 Training Loss: 0.051492128521203995\n",
      "Epoch 13017/20000 Training Loss: 0.06742172688245773\n",
      "Epoch 13018/20000 Training Loss: 0.04975144937634468\n",
      "Epoch 13019/20000 Training Loss: 0.0552956722676754\n",
      "Epoch 13020/20000 Training Loss: 0.060440849512815475\n",
      "Epoch 13020/20000 Validation Loss: 0.049854811280965805\n",
      "Epoch 13021/20000 Training Loss: 0.05820012092590332\n",
      "Epoch 13022/20000 Training Loss: 0.05745105445384979\n",
      "Epoch 13023/20000 Training Loss: 0.0484459362924099\n",
      "Epoch 13024/20000 Training Loss: 0.06441668421030045\n",
      "Epoch 13025/20000 Training Loss: 0.08614867925643921\n",
      "Epoch 13026/20000 Training Loss: 0.05241157487034798\n",
      "Epoch 13027/20000 Training Loss: 0.04106520488858223\n",
      "Epoch 13028/20000 Training Loss: 0.05575968697667122\n",
      "Epoch 13029/20000 Training Loss: 0.07484004646539688\n",
      "Epoch 13030/20000 Training Loss: 0.04502895474433899\n",
      "Epoch 13030/20000 Validation Loss: 0.05576448142528534\n",
      "Epoch 13031/20000 Training Loss: 0.041063327342271805\n",
      "Epoch 13032/20000 Training Loss: 0.04575641453266144\n",
      "Epoch 13033/20000 Training Loss: 0.07515943795442581\n",
      "Epoch 13034/20000 Training Loss: 0.04090839624404907\n",
      "Epoch 13035/20000 Training Loss: 0.050449203699827194\n",
      "Epoch 13036/20000 Training Loss: 0.06158354505896568\n",
      "Epoch 13037/20000 Training Loss: 0.054063230752944946\n",
      "Epoch 13038/20000 Training Loss: 0.047693829983472824\n",
      "Epoch 13039/20000 Training Loss: 0.05741814896464348\n",
      "Epoch 13040/20000 Training Loss: 0.0811220332980156\n",
      "Epoch 13040/20000 Validation Loss: 0.07557022571563721\n",
      "Epoch 13041/20000 Training Loss: 0.04539324343204498\n",
      "Epoch 13042/20000 Training Loss: 0.045178014785051346\n",
      "Epoch 13043/20000 Training Loss: 0.06699272990226746\n",
      "Epoch 13044/20000 Training Loss: 0.053290221840143204\n",
      "Epoch 13045/20000 Training Loss: 0.06649837642908096\n",
      "Epoch 13046/20000 Training Loss: 0.04705369472503662\n",
      "Epoch 13047/20000 Training Loss: 0.03237683326005936\n",
      "Epoch 13048/20000 Training Loss: 0.03583662584424019\n",
      "Epoch 13049/20000 Training Loss: 0.0516275130212307\n",
      "Epoch 13050/20000 Training Loss: 0.07238893955945969\n",
      "Epoch 13050/20000 Validation Loss: 0.061001621186733246\n",
      "Epoch 13051/20000 Training Loss: 0.04768659174442291\n",
      "Epoch 13052/20000 Training Loss: 0.050985679030418396\n",
      "Epoch 13053/20000 Training Loss: 0.052202705293893814\n",
      "Epoch 13054/20000 Training Loss: 0.045130398124456406\n",
      "Epoch 13055/20000 Training Loss: 0.07642056792974472\n",
      "Epoch 13056/20000 Training Loss: 0.05274486169219017\n",
      "Epoch 13057/20000 Training Loss: 0.0780569314956665\n",
      "Epoch 13058/20000 Training Loss: 0.03821716085076332\n",
      "Epoch 13059/20000 Training Loss: 0.0478789359331131\n",
      "Epoch 13060/20000 Training Loss: 0.05676333233714104\n",
      "Epoch 13060/20000 Validation Loss: 0.04315103217959404\n",
      "Epoch 13061/20000 Training Loss: 0.07871410995721817\n",
      "Epoch 13062/20000 Training Loss: 0.06499425321817398\n",
      "Epoch 13063/20000 Training Loss: 0.05521691218018532\n",
      "Epoch 13064/20000 Training Loss: 0.05417090281844139\n",
      "Epoch 13065/20000 Training Loss: 0.055821601301431656\n",
      "Epoch 13066/20000 Training Loss: 0.07744460552930832\n",
      "Epoch 13067/20000 Training Loss: 0.04565517231822014\n",
      "Epoch 13068/20000 Training Loss: 0.04824705794453621\n",
      "Epoch 13069/20000 Training Loss: 0.07191456854343414\n",
      "Epoch 13070/20000 Training Loss: 0.04428635910153389\n",
      "Epoch 13070/20000 Validation Loss: 0.06428035348653793\n",
      "Epoch 13071/20000 Training Loss: 0.05596039816737175\n",
      "Epoch 13072/20000 Training Loss: 0.05515560135245323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13073/20000 Training Loss: 0.048592180013656616\n",
      "Epoch 13074/20000 Training Loss: 0.05340567231178284\n",
      "Epoch 13075/20000 Training Loss: 0.06010439991950989\n",
      "Epoch 13076/20000 Training Loss: 0.045888226479291916\n",
      "Epoch 13077/20000 Training Loss: 0.04322249814867973\n",
      "Epoch 13078/20000 Training Loss: 0.0510985441505909\n",
      "Epoch 13079/20000 Training Loss: 0.0468134768307209\n",
      "Epoch 13080/20000 Training Loss: 0.04310586676001549\n",
      "Epoch 13080/20000 Validation Loss: 0.05523552745580673\n",
      "Epoch 13081/20000 Training Loss: 0.07492202520370483\n",
      "Epoch 13082/20000 Training Loss: 0.05488479137420654\n",
      "Epoch 13083/20000 Training Loss: 0.044142067432403564\n",
      "Epoch 13084/20000 Training Loss: 0.06010958552360535\n",
      "Epoch 13085/20000 Training Loss: 0.0595724880695343\n",
      "Epoch 13086/20000 Training Loss: 0.039217978715896606\n",
      "Epoch 13087/20000 Training Loss: 0.03654046356678009\n",
      "Epoch 13088/20000 Training Loss: 0.06525187939405441\n",
      "Epoch 13089/20000 Training Loss: 0.051084499806165695\n",
      "Epoch 13090/20000 Training Loss: 0.04551359638571739\n",
      "Epoch 13090/20000 Validation Loss: 0.05200678110122681\n",
      "Epoch 13091/20000 Training Loss: 0.053738612681627274\n",
      "Epoch 13092/20000 Training Loss: 0.051788825541734695\n",
      "Epoch 13093/20000 Training Loss: 0.05144144222140312\n",
      "Epoch 13094/20000 Training Loss: 0.037148505449295044\n",
      "Epoch 13095/20000 Training Loss: 0.054356276988983154\n",
      "Epoch 13096/20000 Training Loss: 0.05294985696673393\n",
      "Epoch 13097/20000 Training Loss: 0.06714895367622375\n",
      "Epoch 13098/20000 Training Loss: 0.055241506546735764\n",
      "Epoch 13099/20000 Training Loss: 0.04570861533284187\n",
      "Epoch 13100/20000 Training Loss: 0.05628247559070587\n",
      "Epoch 13100/20000 Validation Loss: 0.067840576171875\n",
      "Epoch 13101/20000 Training Loss: 0.04650850221514702\n",
      "Epoch 13102/20000 Training Loss: 0.05494098365306854\n",
      "Epoch 13103/20000 Training Loss: 0.04997577145695686\n",
      "Epoch 13104/20000 Training Loss: 0.05890116095542908\n",
      "Epoch 13105/20000 Training Loss: 0.060836683958768845\n",
      "Epoch 13106/20000 Training Loss: 0.061417389661073685\n",
      "Epoch 13107/20000 Training Loss: 0.06553139537572861\n",
      "Epoch 13108/20000 Training Loss: 0.052494365721940994\n",
      "Epoch 13109/20000 Training Loss: 0.03967893496155739\n",
      "Epoch 13110/20000 Training Loss: 0.05478529632091522\n",
      "Epoch 13110/20000 Validation Loss: 0.0723441019654274\n",
      "Epoch 13111/20000 Training Loss: 0.06620781868696213\n",
      "Epoch 13112/20000 Training Loss: 0.040075257420539856\n",
      "Epoch 13113/20000 Training Loss: 0.040533844381570816\n",
      "Epoch 13114/20000 Training Loss: 0.04967743530869484\n",
      "Epoch 13115/20000 Training Loss: 0.05253392457962036\n",
      "Epoch 13116/20000 Training Loss: 0.04691856727004051\n",
      "Epoch 13117/20000 Training Loss: 0.06647377461194992\n",
      "Epoch 13118/20000 Training Loss: 0.05944202467799187\n",
      "Epoch 13119/20000 Training Loss: 0.049728136509656906\n",
      "Epoch 13120/20000 Training Loss: 0.051491040736436844\n",
      "Epoch 13120/20000 Validation Loss: 0.07003526389598846\n",
      "Epoch 13121/20000 Training Loss: 0.06166429445147514\n",
      "Epoch 13122/20000 Training Loss: 0.05509035661816597\n",
      "Epoch 13123/20000 Training Loss: 0.06288512796163559\n",
      "Epoch 13124/20000 Training Loss: 0.04477514699101448\n",
      "Epoch 13125/20000 Training Loss: 0.06906033307313919\n",
      "Epoch 13126/20000 Training Loss: 0.05452491343021393\n",
      "Epoch 13127/20000 Training Loss: 0.07728227227926254\n",
      "Epoch 13128/20000 Training Loss: 0.059357453137636185\n",
      "Epoch 13129/20000 Training Loss: 0.035410325974226\n",
      "Epoch 13130/20000 Training Loss: 0.04739778861403465\n",
      "Epoch 13130/20000 Validation Loss: 0.056691974401474\n",
      "Epoch 13131/20000 Training Loss: 0.04276850447058678\n",
      "Epoch 13132/20000 Training Loss: 0.06333605200052261\n",
      "Epoch 13133/20000 Training Loss: 0.03713233396410942\n",
      "Epoch 13134/20000 Training Loss: 0.05983980372548103\n",
      "Epoch 13135/20000 Training Loss: 0.04687398672103882\n",
      "Epoch 13136/20000 Training Loss: 0.06351140141487122\n",
      "Epoch 13137/20000 Training Loss: 0.05702489614486694\n",
      "Epoch 13138/20000 Training Loss: 0.0424761138856411\n",
      "Epoch 13139/20000 Training Loss: 0.058799345046281815\n",
      "Epoch 13140/20000 Training Loss: 0.07541989535093307\n",
      "Epoch 13140/20000 Validation Loss: 0.05151426047086716\n",
      "Epoch 13141/20000 Training Loss: 0.04709756001830101\n",
      "Epoch 13142/20000 Training Loss: 0.03872554376721382\n",
      "Epoch 13143/20000 Training Loss: 0.041371677070856094\n",
      "Epoch 13144/20000 Training Loss: 0.05831214785575867\n",
      "Epoch 13145/20000 Training Loss: 0.06377481669187546\n",
      "Epoch 13146/20000 Training Loss: 0.044746991246938705\n",
      "Epoch 13147/20000 Training Loss: 0.06896849721670151\n",
      "Epoch 13148/20000 Training Loss: 0.058052908629179\n",
      "Epoch 13149/20000 Training Loss: 0.055508047342300415\n",
      "Epoch 13150/20000 Training Loss: 0.06724744290113449\n",
      "Epoch 13150/20000 Validation Loss: 0.05842726305127144\n",
      "Epoch 13151/20000 Training Loss: 0.05135856568813324\n",
      "Epoch 13152/20000 Training Loss: 0.056472066789865494\n",
      "Epoch 13153/20000 Training Loss: 0.0475054532289505\n",
      "Epoch 13154/20000 Training Loss: 0.04621850326657295\n",
      "Epoch 13155/20000 Training Loss: 0.03486761823296547\n",
      "Epoch 13156/20000 Training Loss: 0.03624808415770531\n",
      "Epoch 13157/20000 Training Loss: 0.05205923691391945\n",
      "Epoch 13158/20000 Training Loss: 0.05811333283782005\n",
      "Epoch 13159/20000 Training Loss: 0.06688109785318375\n",
      "Epoch 13160/20000 Training Loss: 0.04792298004031181\n",
      "Epoch 13160/20000 Validation Loss: 0.06770537793636322\n",
      "Epoch 13161/20000 Training Loss: 0.04789453372359276\n",
      "Epoch 13162/20000 Training Loss: 0.049620505422353745\n",
      "Epoch 13163/20000 Training Loss: 0.0450269877910614\n",
      "Epoch 13164/20000 Training Loss: 0.0653245747089386\n",
      "Epoch 13165/20000 Training Loss: 0.03666478022933006\n",
      "Epoch 13166/20000 Training Loss: 0.060356367379426956\n",
      "Epoch 13167/20000 Training Loss: 0.07164887338876724\n",
      "Epoch 13168/20000 Training Loss: 0.06116539612412453\n",
      "Epoch 13169/20000 Training Loss: 0.05787717178463936\n",
      "Epoch 13170/20000 Training Loss: 0.06974097341299057\n",
      "Epoch 13170/20000 Validation Loss: 0.07220898568630219\n",
      "Epoch 13171/20000 Training Loss: 0.061249375343322754\n",
      "Epoch 13172/20000 Training Loss: 0.0694735124707222\n",
      "Epoch 13173/20000 Training Loss: 0.0530875027179718\n",
      "Epoch 13174/20000 Training Loss: 0.04349696263670921\n",
      "Epoch 13175/20000 Training Loss: 0.07792378216981888\n",
      "Epoch 13176/20000 Training Loss: 0.033002015203237534\n",
      "Epoch 13177/20000 Training Loss: 0.04994777962565422\n",
      "Epoch 13178/20000 Training Loss: 0.058773498982191086\n",
      "Epoch 13179/20000 Training Loss: 0.06431299448013306\n",
      "Epoch 13180/20000 Training Loss: 0.04219459369778633\n",
      "Epoch 13180/20000 Validation Loss: 0.04116477817296982\n",
      "Epoch 13181/20000 Training Loss: 0.05831995606422424\n",
      "Epoch 13182/20000 Training Loss: 0.06584762781858444\n",
      "Epoch 13183/20000 Training Loss: 0.052979011088609695\n",
      "Epoch 13184/20000 Training Loss: 0.0614885576069355\n",
      "Epoch 13185/20000 Training Loss: 0.04991215094923973\n",
      "Epoch 13186/20000 Training Loss: 0.03823074698448181\n",
      "Epoch 13187/20000 Training Loss: 0.07492861896753311\n",
      "Epoch 13188/20000 Training Loss: 0.05939774587750435\n",
      "Epoch 13189/20000 Training Loss: 0.05519266426563263\n",
      "Epoch 13190/20000 Training Loss: 0.0533653162419796\n",
      "Epoch 13190/20000 Validation Loss: 0.05183439329266548\n",
      "Epoch 13191/20000 Training Loss: 0.0612625926733017\n",
      "Epoch 13192/20000 Training Loss: 0.03647809848189354\n",
      "Epoch 13193/20000 Training Loss: 0.03831480070948601\n",
      "Epoch 13194/20000 Training Loss: 0.06512518972158432\n",
      "Epoch 13195/20000 Training Loss: 0.06599441170692444\n",
      "Epoch 13196/20000 Training Loss: 0.07322859019041061\n",
      "Epoch 13197/20000 Training Loss: 0.054842229932546616\n",
      "Epoch 13198/20000 Training Loss: 0.055414214730262756\n",
      "Epoch 13199/20000 Training Loss: 0.05172942951321602\n",
      "Epoch 13200/20000 Training Loss: 0.051726650446653366\n",
      "Epoch 13200/20000 Validation Loss: 0.06177014112472534\n",
      "Epoch 13201/20000 Training Loss: 0.057834092527627945\n",
      "Epoch 13202/20000 Training Loss: 0.06362955272197723\n",
      "Epoch 13203/20000 Training Loss: 0.04257030785083771\n",
      "Epoch 13204/20000 Training Loss: 0.07236393541097641\n",
      "Epoch 13205/20000 Training Loss: 0.06261739879846573\n",
      "Epoch 13206/20000 Training Loss: 0.06028240546584129\n",
      "Epoch 13207/20000 Training Loss: 0.043674975633621216\n",
      "Epoch 13208/20000 Training Loss: 0.054179489612579346\n",
      "Epoch 13209/20000 Training Loss: 0.05587800219655037\n",
      "Epoch 13210/20000 Training Loss: 0.04254532977938652\n",
      "Epoch 13210/20000 Validation Loss: 0.06724067032337189\n",
      "Epoch 13211/20000 Training Loss: 0.051313284784555435\n",
      "Epoch 13212/20000 Training Loss: 0.05102213844656944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13213/20000 Training Loss: 0.04321718215942383\n",
      "Epoch 13214/20000 Training Loss: 0.05486631393432617\n",
      "Epoch 13215/20000 Training Loss: 0.037992533296346664\n",
      "Epoch 13216/20000 Training Loss: 0.055564697831869125\n",
      "Epoch 13217/20000 Training Loss: 0.05118627846240997\n",
      "Epoch 13218/20000 Training Loss: 0.03903063014149666\n",
      "Epoch 13219/20000 Training Loss: 0.043475717306137085\n",
      "Epoch 13220/20000 Training Loss: 0.05012228712439537\n",
      "Epoch 13220/20000 Validation Loss: 0.057323798537254333\n",
      "Epoch 13221/20000 Training Loss: 0.04662419483065605\n",
      "Epoch 13222/20000 Training Loss: 0.0482991486787796\n",
      "Epoch 13223/20000 Training Loss: 0.05093933269381523\n",
      "Epoch 13224/20000 Training Loss: 0.050380829721689224\n",
      "Epoch 13225/20000 Training Loss: 0.04666369780898094\n",
      "Epoch 13226/20000 Training Loss: 0.05353511497378349\n",
      "Epoch 13227/20000 Training Loss: 0.04161509871482849\n",
      "Epoch 13228/20000 Training Loss: 0.05191381648182869\n",
      "Epoch 13229/20000 Training Loss: 0.06538351625204086\n",
      "Epoch 13230/20000 Training Loss: 0.05129753425717354\n",
      "Epoch 13230/20000 Validation Loss: 0.04847659170627594\n",
      "Epoch 13231/20000 Training Loss: 0.04869299754500389\n",
      "Epoch 13232/20000 Training Loss: 0.06054510548710823\n",
      "Epoch 13233/20000 Training Loss: 0.0617755651473999\n",
      "Epoch 13234/20000 Training Loss: 0.06521599739789963\n",
      "Epoch 13235/20000 Training Loss: 0.05643276870250702\n",
      "Epoch 13236/20000 Training Loss: 0.05389004945755005\n",
      "Epoch 13237/20000 Training Loss: 0.05051824077963829\n",
      "Epoch 13238/20000 Training Loss: 0.06873805820941925\n",
      "Epoch 13239/20000 Training Loss: 0.06672555208206177\n",
      "Epoch 13240/20000 Training Loss: 0.0648423433303833\n",
      "Epoch 13240/20000 Validation Loss: 0.07547295838594437\n",
      "Epoch 13241/20000 Training Loss: 0.046649519354104996\n",
      "Epoch 13242/20000 Training Loss: 0.10052412748336792\n",
      "Epoch 13243/20000 Training Loss: 0.05958530679345131\n",
      "Epoch 13244/20000 Training Loss: 0.04620755836367607\n",
      "Epoch 13245/20000 Training Loss: 0.05034181475639343\n",
      "Epoch 13246/20000 Training Loss: 0.054993342608213425\n",
      "Epoch 13247/20000 Training Loss: 0.04322221502661705\n",
      "Epoch 13248/20000 Training Loss: 0.04543207585811615\n",
      "Epoch 13249/20000 Training Loss: 0.04894595965743065\n",
      "Epoch 13250/20000 Training Loss: 0.06489207595586777\n",
      "Epoch 13250/20000 Validation Loss: 0.05971861630678177\n",
      "Epoch 13251/20000 Training Loss: 0.08260960131883621\n",
      "Epoch 13252/20000 Training Loss: 0.044687669724226\n",
      "Epoch 13253/20000 Training Loss: 0.05220910534262657\n",
      "Epoch 13254/20000 Training Loss: 0.05333391949534416\n",
      "Epoch 13255/20000 Training Loss: 0.058401405811309814\n",
      "Epoch 13256/20000 Training Loss: 0.04608641564846039\n",
      "Epoch 13257/20000 Training Loss: 0.06333158165216446\n",
      "Epoch 13258/20000 Training Loss: 0.04876892641186714\n",
      "Epoch 13259/20000 Training Loss: 0.05917077139019966\n",
      "Epoch 13260/20000 Training Loss: 0.05266505107283592\n",
      "Epoch 13260/20000 Validation Loss: 0.07285735011100769\n",
      "Epoch 13261/20000 Training Loss: 0.04942471906542778\n",
      "Epoch 13262/20000 Training Loss: 0.045681972056627274\n",
      "Epoch 13263/20000 Training Loss: 0.06001655384898186\n",
      "Epoch 13264/20000 Training Loss: 0.04761761426925659\n",
      "Epoch 13265/20000 Training Loss: 0.05738236382603645\n",
      "Epoch 13266/20000 Training Loss: 0.06604814529418945\n",
      "Epoch 13267/20000 Training Loss: 0.05682697892189026\n",
      "Epoch 13268/20000 Training Loss: 0.06853244453668594\n",
      "Epoch 13269/20000 Training Loss: 0.058327317237854004\n",
      "Epoch 13270/20000 Training Loss: 0.04679149016737938\n",
      "Epoch 13270/20000 Validation Loss: 0.052773140370845795\n",
      "Epoch 13271/20000 Training Loss: 0.04865537956357002\n",
      "Epoch 13272/20000 Training Loss: 0.052836835384368896\n",
      "Epoch 13273/20000 Training Loss: 0.058420728892087936\n",
      "Epoch 13274/20000 Training Loss: 0.041596874594688416\n",
      "Epoch 13275/20000 Training Loss: 0.04830851033329964\n",
      "Epoch 13276/20000 Training Loss: 0.06482982635498047\n",
      "Epoch 13277/20000 Training Loss: 0.060287147760391235\n",
      "Epoch 13278/20000 Training Loss: 0.04984035715460777\n",
      "Epoch 13279/20000 Training Loss: 0.05032752454280853\n",
      "Epoch 13280/20000 Training Loss: 0.057728320360183716\n",
      "Epoch 13280/20000 Validation Loss: 0.05840364098548889\n",
      "Epoch 13281/20000 Training Loss: 0.05195027217268944\n",
      "Epoch 13282/20000 Training Loss: 0.039963915944099426\n",
      "Epoch 13283/20000 Training Loss: 0.04525841400027275\n",
      "Epoch 13284/20000 Training Loss: 0.07124602794647217\n",
      "Epoch 13285/20000 Training Loss: 0.057014286518096924\n",
      "Epoch 13286/20000 Training Loss: 0.055872004479169846\n",
      "Epoch 13287/20000 Training Loss: 0.04443902149796486\n",
      "Epoch 13288/20000 Training Loss: 0.06934300065040588\n",
      "Epoch 13289/20000 Training Loss: 0.054042916744947433\n",
      "Epoch 13290/20000 Training Loss: 0.05590301752090454\n",
      "Epoch 13290/20000 Validation Loss: 0.054098084568977356\n",
      "Epoch 13291/20000 Training Loss: 0.04774552583694458\n",
      "Epoch 13292/20000 Training Loss: 0.042833805084228516\n",
      "Epoch 13293/20000 Training Loss: 0.04336561635136604\n",
      "Epoch 13294/20000 Training Loss: 0.04305364191532135\n",
      "Epoch 13295/20000 Training Loss: 0.035544466227293015\n",
      "Epoch 13296/20000 Training Loss: 0.040751371532678604\n",
      "Epoch 13297/20000 Training Loss: 0.05807332322001457\n",
      "Epoch 13298/20000 Training Loss: 0.054956670850515366\n",
      "Epoch 13299/20000 Training Loss: 0.037975095212459564\n",
      "Epoch 13300/20000 Training Loss: 0.04615012928843498\n",
      "Epoch 13300/20000 Validation Loss: 0.07744064182043076\n",
      "Epoch 13301/20000 Training Loss: 0.051181990653276443\n",
      "Epoch 13302/20000 Training Loss: 0.053618740290403366\n",
      "Epoch 13303/20000 Training Loss: 0.05444652959704399\n",
      "Epoch 13304/20000 Training Loss: 0.04674570634961128\n",
      "Epoch 13305/20000 Training Loss: 0.05505921319127083\n",
      "Epoch 13306/20000 Training Loss: 0.05242682620882988\n",
      "Epoch 13307/20000 Training Loss: 0.05321112275123596\n",
      "Epoch 13308/20000 Training Loss: 0.0658964291214943\n",
      "Epoch 13309/20000 Training Loss: 0.049374837428331375\n",
      "Epoch 13310/20000 Training Loss: 0.06094907224178314\n",
      "Epoch 13310/20000 Validation Loss: 0.07526009529829025\n",
      "Epoch 13311/20000 Training Loss: 0.08553028851747513\n",
      "Epoch 13312/20000 Training Loss: 0.05622485280036926\n",
      "Epoch 13313/20000 Training Loss: 0.06108219549059868\n",
      "Epoch 13314/20000 Training Loss: 0.05288664624094963\n",
      "Epoch 13315/20000 Training Loss: 0.040233638137578964\n",
      "Epoch 13316/20000 Training Loss: 0.04977529123425484\n",
      "Epoch 13317/20000 Training Loss: 0.042752236127853394\n",
      "Epoch 13318/20000 Training Loss: 0.05523131787776947\n",
      "Epoch 13319/20000 Training Loss: 0.06315393000841141\n",
      "Epoch 13320/20000 Training Loss: 0.05621443688869476\n",
      "Epoch 13320/20000 Validation Loss: 0.05639175325632095\n",
      "Epoch 13321/20000 Training Loss: 0.06233477592468262\n",
      "Epoch 13322/20000 Training Loss: 0.04669337347149849\n",
      "Epoch 13323/20000 Training Loss: 0.059373099356889725\n",
      "Epoch 13324/20000 Training Loss: 0.0677344799041748\n",
      "Epoch 13325/20000 Training Loss: 0.07024037837982178\n",
      "Epoch 13326/20000 Training Loss: 0.051134224981069565\n",
      "Epoch 13327/20000 Training Loss: 0.05190190300345421\n",
      "Epoch 13328/20000 Training Loss: 0.05654573068022728\n",
      "Epoch 13329/20000 Training Loss: 0.05345736816525459\n",
      "Epoch 13330/20000 Training Loss: 0.05965711548924446\n",
      "Epoch 13330/20000 Validation Loss: 0.058114998042583466\n",
      "Epoch 13331/20000 Training Loss: 0.05331714078783989\n",
      "Epoch 13332/20000 Training Loss: 0.045427266508340836\n",
      "Epoch 13333/20000 Training Loss: 0.06525163352489471\n",
      "Epoch 13334/20000 Training Loss: 0.061268676072359085\n",
      "Epoch 13335/20000 Training Loss: 0.04268162325024605\n",
      "Epoch 13336/20000 Training Loss: 0.051863301545381546\n",
      "Epoch 13337/20000 Training Loss: 0.0470740906894207\n",
      "Epoch 13338/20000 Training Loss: 0.05431531369686127\n",
      "Epoch 13339/20000 Training Loss: 0.06319857388734818\n",
      "Epoch 13340/20000 Training Loss: 0.055251967161893845\n",
      "Epoch 13340/20000 Validation Loss: 0.04413643479347229\n",
      "Epoch 13341/20000 Training Loss: 0.06936507672071457\n",
      "Epoch 13342/20000 Training Loss: 0.05310703441500664\n",
      "Epoch 13343/20000 Training Loss: 0.050449978560209274\n",
      "Epoch 13344/20000 Training Loss: 0.05277480185031891\n",
      "Epoch 13345/20000 Training Loss: 0.058273982256650925\n",
      "Epoch 13346/20000 Training Loss: 0.05721554160118103\n",
      "Epoch 13347/20000 Training Loss: 0.04096921905875206\n",
      "Epoch 13348/20000 Training Loss: 0.05197317525744438\n",
      "Epoch 13349/20000 Training Loss: 0.048744942992925644\n",
      "Epoch 13350/20000 Training Loss: 0.0623333603143692\n",
      "Epoch 13350/20000 Validation Loss: 0.06892237067222595\n",
      "Epoch 13351/20000 Training Loss: 0.06141933426260948\n",
      "Epoch 13352/20000 Training Loss: 0.06569754332304001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13353/20000 Training Loss: 0.04964270815253258\n",
      "Epoch 13354/20000 Training Loss: 0.04489612206816673\n",
      "Epoch 13355/20000 Training Loss: 0.084430992603302\n",
      "Epoch 13356/20000 Training Loss: 0.043593499809503555\n",
      "Epoch 13357/20000 Training Loss: 0.06827154755592346\n",
      "Epoch 13358/20000 Training Loss: 0.04750411584973335\n",
      "Epoch 13359/20000 Training Loss: 0.04557978734374046\n",
      "Epoch 13360/20000 Training Loss: 0.04365710914134979\n",
      "Epoch 13360/20000 Validation Loss: 0.05038754642009735\n",
      "Epoch 13361/20000 Training Loss: 0.05084862932562828\n",
      "Epoch 13362/20000 Training Loss: 0.05548533797264099\n",
      "Epoch 13363/20000 Training Loss: 0.04941856488585472\n",
      "Epoch 13364/20000 Training Loss: 0.05866802856326103\n",
      "Epoch 13365/20000 Training Loss: 0.050094377249479294\n",
      "Epoch 13366/20000 Training Loss: 0.06243184581398964\n",
      "Epoch 13367/20000 Training Loss: 0.048839617520570755\n",
      "Epoch 13368/20000 Training Loss: 0.04749098792672157\n",
      "Epoch 13369/20000 Training Loss: 0.06230393424630165\n",
      "Epoch 13370/20000 Training Loss: 0.04409359395503998\n",
      "Epoch 13370/20000 Validation Loss: 0.04941326007246971\n",
      "Epoch 13371/20000 Training Loss: 0.04393501952290535\n",
      "Epoch 13372/20000 Training Loss: 0.052285850048065186\n",
      "Epoch 13373/20000 Training Loss: 0.056528568267822266\n",
      "Epoch 13374/20000 Training Loss: 0.053140465170145035\n",
      "Epoch 13375/20000 Training Loss: 0.055177658796310425\n",
      "Epoch 13376/20000 Training Loss: 0.05279713496565819\n",
      "Epoch 13377/20000 Training Loss: 0.0424569733440876\n",
      "Epoch 13378/20000 Training Loss: 0.05513036251068115\n",
      "Epoch 13379/20000 Training Loss: 0.0589233934879303\n",
      "Epoch 13380/20000 Training Loss: 0.06191779300570488\n",
      "Epoch 13380/20000 Validation Loss: 0.09553469717502594\n",
      "Epoch 13381/20000 Training Loss: 0.05129295587539673\n",
      "Epoch 13382/20000 Training Loss: 0.047786444425582886\n",
      "Epoch 13383/20000 Training Loss: 0.055319786071777344\n",
      "Epoch 13384/20000 Training Loss: 0.05769583210349083\n",
      "Epoch 13385/20000 Training Loss: 0.048772215843200684\n",
      "Epoch 13386/20000 Training Loss: 0.044541727751493454\n",
      "Epoch 13387/20000 Training Loss: 0.04874318838119507\n",
      "Epoch 13388/20000 Training Loss: 0.05579875037074089\n",
      "Epoch 13389/20000 Training Loss: 0.06311187893152237\n",
      "Epoch 13390/20000 Training Loss: 0.05927262082695961\n",
      "Epoch 13390/20000 Validation Loss: 0.0629979819059372\n",
      "Epoch 13391/20000 Training Loss: 0.06881092488765717\n",
      "Epoch 13392/20000 Training Loss: 0.05197827145457268\n",
      "Epoch 13393/20000 Training Loss: 0.041252266615629196\n",
      "Epoch 13394/20000 Training Loss: 0.05560429394245148\n",
      "Epoch 13395/20000 Training Loss: 0.04570329189300537\n",
      "Epoch 13396/20000 Training Loss: 0.04163043200969696\n",
      "Epoch 13397/20000 Training Loss: 0.050699423998594284\n",
      "Epoch 13398/20000 Training Loss: 0.048820555210113525\n",
      "Epoch 13399/20000 Training Loss: 0.048926834017038345\n",
      "Epoch 13400/20000 Training Loss: 0.06168925762176514\n",
      "Epoch 13400/20000 Validation Loss: 0.04135345667600632\n",
      "Epoch 13401/20000 Training Loss: 0.04343516752123833\n",
      "Epoch 13402/20000 Training Loss: 0.04894179105758667\n",
      "Epoch 13403/20000 Training Loss: 0.05163965001702309\n",
      "Epoch 13404/20000 Training Loss: 0.06613368541002274\n",
      "Epoch 13405/20000 Training Loss: 0.045781780034303665\n",
      "Epoch 13406/20000 Training Loss: 0.05531281232833862\n",
      "Epoch 13407/20000 Training Loss: 0.040850330144166946\n",
      "Epoch 13408/20000 Training Loss: 0.06697337329387665\n",
      "Epoch 13409/20000 Training Loss: 0.04457388445734978\n",
      "Epoch 13410/20000 Training Loss: 0.060245875269174576\n",
      "Epoch 13410/20000 Validation Loss: 0.053988248109817505\n",
      "Epoch 13411/20000 Training Loss: 0.048934075981378555\n",
      "Epoch 13412/20000 Training Loss: 0.046415477991104126\n",
      "Epoch 13413/20000 Training Loss: 0.03965158760547638\n",
      "Epoch 13414/20000 Training Loss: 0.06809636950492859\n",
      "Epoch 13415/20000 Training Loss: 0.044950585812330246\n",
      "Epoch 13416/20000 Training Loss: 0.05846549943089485\n",
      "Epoch 13417/20000 Training Loss: 0.059965360909700394\n",
      "Epoch 13418/20000 Training Loss: 0.05188142880797386\n",
      "Epoch 13419/20000 Training Loss: 0.05427350476384163\n",
      "Epoch 13420/20000 Training Loss: 0.06048424169421196\n",
      "Epoch 13420/20000 Validation Loss: 0.05309700965881348\n",
      "Epoch 13421/20000 Training Loss: 0.04887385666370392\n",
      "Epoch 13422/20000 Training Loss: 0.04834393784403801\n",
      "Epoch 13423/20000 Training Loss: 0.057139281183481216\n",
      "Epoch 13424/20000 Training Loss: 0.049410924315452576\n",
      "Epoch 13425/20000 Training Loss: 0.040912896394729614\n",
      "Epoch 13426/20000 Training Loss: 0.05695342645049095\n",
      "Epoch 13427/20000 Training Loss: 0.04782574251294136\n",
      "Epoch 13428/20000 Training Loss: 0.045024484395980835\n",
      "Epoch 13429/20000 Training Loss: 0.036552201956510544\n",
      "Epoch 13430/20000 Training Loss: 0.044683653861284256\n",
      "Epoch 13430/20000 Validation Loss: 0.059975333511829376\n",
      "Epoch 13431/20000 Training Loss: 0.06577076762914658\n",
      "Epoch 13432/20000 Training Loss: 0.061438173055648804\n",
      "Epoch 13433/20000 Training Loss: 0.0699796974658966\n",
      "Epoch 13434/20000 Training Loss: 0.06096323952078819\n",
      "Epoch 13435/20000 Training Loss: 0.05160624906420708\n",
      "Epoch 13436/20000 Training Loss: 0.051426779478788376\n",
      "Epoch 13437/20000 Training Loss: 0.06184592843055725\n",
      "Epoch 13438/20000 Training Loss: 0.0714680477976799\n",
      "Epoch 13439/20000 Training Loss: 0.0505053885281086\n",
      "Epoch 13440/20000 Training Loss: 0.0616990365087986\n",
      "Epoch 13440/20000 Validation Loss: 0.06514682620763779\n",
      "Epoch 13441/20000 Training Loss: 0.04357529804110527\n",
      "Epoch 13442/20000 Training Loss: 0.05260347202420235\n",
      "Epoch 13443/20000 Training Loss: 0.04414362087845802\n",
      "Epoch 13444/20000 Training Loss: 0.04480041563510895\n",
      "Epoch 13445/20000 Training Loss: 0.07990541309118271\n",
      "Epoch 13446/20000 Training Loss: 0.0488012433052063\n",
      "Epoch 13447/20000 Training Loss: 0.05222121253609657\n",
      "Epoch 13448/20000 Training Loss: 0.04643074795603752\n",
      "Epoch 13449/20000 Training Loss: 0.04791175201535225\n",
      "Epoch 13450/20000 Training Loss: 0.04439949616789818\n",
      "Epoch 13450/20000 Validation Loss: 0.06350214779376984\n",
      "Epoch 13451/20000 Training Loss: 0.0663783848285675\n",
      "Epoch 13452/20000 Training Loss: 0.04739749804139137\n",
      "Epoch 13453/20000 Training Loss: 0.057612095028162\n",
      "Epoch 13454/20000 Training Loss: 0.06521592289209366\n",
      "Epoch 13455/20000 Training Loss: 0.04976002499461174\n",
      "Epoch 13456/20000 Training Loss: 0.04347251355648041\n",
      "Epoch 13457/20000 Training Loss: 0.05875787138938904\n",
      "Epoch 13458/20000 Training Loss: 0.06146945431828499\n",
      "Epoch 13459/20000 Training Loss: 0.07105685770511627\n",
      "Epoch 13460/20000 Training Loss: 0.048822831362485886\n",
      "Epoch 13460/20000 Validation Loss: 0.048546988517045975\n",
      "Epoch 13461/20000 Training Loss: 0.06634248048067093\n",
      "Epoch 13462/20000 Training Loss: 0.06259709596633911\n",
      "Epoch 13463/20000 Training Loss: 0.04759245738387108\n",
      "Epoch 13464/20000 Training Loss: 0.0409938208758831\n",
      "Epoch 13465/20000 Training Loss: 0.0535559244453907\n",
      "Epoch 13466/20000 Training Loss: 0.04805077612400055\n",
      "Epoch 13467/20000 Training Loss: 0.05115723982453346\n",
      "Epoch 13468/20000 Training Loss: 0.053093891590833664\n",
      "Epoch 13469/20000 Training Loss: 0.040351077914237976\n",
      "Epoch 13470/20000 Training Loss: 0.05711505934596062\n",
      "Epoch 13470/20000 Validation Loss: 0.06353026628494263\n",
      "Epoch 13471/20000 Training Loss: 0.04771088436245918\n",
      "Epoch 13472/20000 Training Loss: 0.04583567753434181\n",
      "Epoch 13473/20000 Training Loss: 0.05580909177660942\n",
      "Epoch 13474/20000 Training Loss: 0.04563393071293831\n",
      "Epoch 13475/20000 Training Loss: 0.06544458121061325\n",
      "Epoch 13476/20000 Training Loss: 0.07318232208490372\n",
      "Epoch 13477/20000 Training Loss: 0.05185167118906975\n",
      "Epoch 13478/20000 Training Loss: 0.06270409375429153\n",
      "Epoch 13479/20000 Training Loss: 0.07705116271972656\n",
      "Epoch 13480/20000 Training Loss: 0.04210910201072693\n",
      "Epoch 13480/20000 Validation Loss: 0.05700527876615524\n",
      "Epoch 13481/20000 Training Loss: 0.06367521733045578\n",
      "Epoch 13482/20000 Training Loss: 0.06006162241101265\n",
      "Epoch 13483/20000 Training Loss: 0.06374820321798325\n",
      "Epoch 13484/20000 Training Loss: 0.04340633377432823\n",
      "Epoch 13485/20000 Training Loss: 0.05727535858750343\n",
      "Epoch 13486/20000 Training Loss: 0.05157526209950447\n",
      "Epoch 13487/20000 Training Loss: 0.05365350842475891\n",
      "Epoch 13488/20000 Training Loss: 0.03984329104423523\n",
      "Epoch 13489/20000 Training Loss: 0.05852450057864189\n",
      "Epoch 13490/20000 Training Loss: 0.04758356884121895\n",
      "Epoch 13490/20000 Validation Loss: 0.05842757225036621\n",
      "Epoch 13491/20000 Training Loss: 0.07829894870519638\n",
      "Epoch 13492/20000 Training Loss: 0.05877332016825676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13493/20000 Training Loss: 0.05639037489891052\n",
      "Epoch 13494/20000 Training Loss: 0.06192915514111519\n",
      "Epoch 13495/20000 Training Loss: 0.04926738515496254\n",
      "Epoch 13496/20000 Training Loss: 0.07498369365930557\n",
      "Epoch 13497/20000 Training Loss: 0.07324052602052689\n",
      "Epoch 13498/20000 Training Loss: 0.06705004721879959\n",
      "Epoch 13499/20000 Training Loss: 0.04545557498931885\n",
      "Epoch 13500/20000 Training Loss: 0.04335696995258331\n",
      "Epoch 13500/20000 Validation Loss: 0.03973007947206497\n",
      "Epoch 13501/20000 Training Loss: 0.04786698892712593\n",
      "Epoch 13502/20000 Training Loss: 0.062149692326784134\n",
      "Epoch 13503/20000 Training Loss: 0.0674341693520546\n",
      "Epoch 13504/20000 Training Loss: 0.048528071492910385\n",
      "Epoch 13505/20000 Training Loss: 0.05738937854766846\n",
      "Epoch 13506/20000 Training Loss: 0.04189795255661011\n",
      "Epoch 13507/20000 Training Loss: 0.04933517053723335\n",
      "Epoch 13508/20000 Training Loss: 0.048001259565353394\n",
      "Epoch 13509/20000 Training Loss: 0.05877324938774109\n",
      "Epoch 13510/20000 Training Loss: 0.049374744296073914\n",
      "Epoch 13510/20000 Validation Loss: 0.0816972628235817\n",
      "Epoch 13511/20000 Training Loss: 0.03770068660378456\n",
      "Epoch 13512/20000 Training Loss: 0.04910130426287651\n",
      "Epoch 13513/20000 Training Loss: 0.04480454698204994\n",
      "Epoch 13514/20000 Training Loss: 0.0875043049454689\n",
      "Epoch 13515/20000 Training Loss: 0.04685476794838905\n",
      "Epoch 13516/20000 Training Loss: 0.03995436802506447\n",
      "Epoch 13517/20000 Training Loss: 0.048365477472543716\n",
      "Epoch 13518/20000 Training Loss: 0.035658106207847595\n",
      "Epoch 13519/20000 Training Loss: 0.044950928539037704\n",
      "Epoch 13520/20000 Training Loss: 0.05134173855185509\n",
      "Epoch 13520/20000 Validation Loss: 0.07813306152820587\n",
      "Epoch 13521/20000 Training Loss: 0.053811196237802505\n",
      "Epoch 13522/20000 Training Loss: 0.04697105288505554\n",
      "Epoch 13523/20000 Training Loss: 0.04562608525156975\n",
      "Epoch 13524/20000 Training Loss: 0.06049894168972969\n",
      "Epoch 13525/20000 Training Loss: 0.06959641724824905\n",
      "Epoch 13526/20000 Training Loss: 0.048237260431051254\n",
      "Epoch 13527/20000 Training Loss: 0.05988683924078941\n",
      "Epoch 13528/20000 Training Loss: 0.056460361927747726\n",
      "Epoch 13529/20000 Training Loss: 0.04101729765534401\n",
      "Epoch 13530/20000 Training Loss: 0.07784116268157959\n",
      "Epoch 13530/20000 Validation Loss: 0.044505566358566284\n",
      "Epoch 13531/20000 Training Loss: 0.061475563794374466\n",
      "Epoch 13532/20000 Training Loss: 0.06282070279121399\n",
      "Epoch 13533/20000 Training Loss: 0.0619337260723114\n",
      "Epoch 13534/20000 Training Loss: 0.04804086685180664\n",
      "Epoch 13535/20000 Training Loss: 0.04907427728176117\n",
      "Epoch 13536/20000 Training Loss: 0.05950179696083069\n",
      "Epoch 13537/20000 Training Loss: 0.06234129145741463\n",
      "Epoch 13538/20000 Training Loss: 0.04391218721866608\n",
      "Epoch 13539/20000 Training Loss: 0.05570271238684654\n",
      "Epoch 13540/20000 Training Loss: 0.04131753742694855\n",
      "Epoch 13540/20000 Validation Loss: 0.03908189386129379\n",
      "Epoch 13541/20000 Training Loss: 0.036651451140642166\n",
      "Epoch 13542/20000 Training Loss: 0.06407627463340759\n",
      "Epoch 13543/20000 Training Loss: 0.06209950149059296\n",
      "Epoch 13544/20000 Training Loss: 0.06277523934841156\n",
      "Epoch 13545/20000 Training Loss: 0.053312066942453384\n",
      "Epoch 13546/20000 Training Loss: 0.0493239164352417\n",
      "Epoch 13547/20000 Training Loss: 0.058744773268699646\n",
      "Epoch 13548/20000 Training Loss: 0.0542273223400116\n",
      "Epoch 13549/20000 Training Loss: 0.06705497950315475\n",
      "Epoch 13550/20000 Training Loss: 0.05431394279003143\n",
      "Epoch 13550/20000 Validation Loss: 0.053270719945430756\n",
      "Epoch 13551/20000 Training Loss: 0.04302928224205971\n",
      "Epoch 13552/20000 Training Loss: 0.05149450525641441\n",
      "Epoch 13553/20000 Training Loss: 0.04277745261788368\n",
      "Epoch 13554/20000 Training Loss: 0.04664510488510132\n",
      "Epoch 13555/20000 Training Loss: 0.054811906069517136\n",
      "Epoch 13556/20000 Training Loss: 0.05004182830452919\n",
      "Epoch 13557/20000 Training Loss: 0.0767078623175621\n",
      "Epoch 13558/20000 Training Loss: 0.04471808299422264\n",
      "Epoch 13559/20000 Training Loss: 0.046682704240083694\n",
      "Epoch 13560/20000 Training Loss: 0.04828331992030144\n",
      "Epoch 13560/20000 Validation Loss: 0.04932327941060066\n",
      "Epoch 13561/20000 Training Loss: 0.0450592078268528\n",
      "Epoch 13562/20000 Training Loss: 0.062187135219573975\n",
      "Epoch 13563/20000 Training Loss: 0.047889310866594315\n",
      "Epoch 13564/20000 Training Loss: 0.05140736699104309\n",
      "Epoch 13565/20000 Training Loss: 0.060514483600854874\n",
      "Epoch 13566/20000 Training Loss: 0.06529761850833893\n",
      "Epoch 13567/20000 Training Loss: 0.055248577147722244\n",
      "Epoch 13568/20000 Training Loss: 0.0575702078640461\n",
      "Epoch 13569/20000 Training Loss: 0.039564695209264755\n",
      "Epoch 13570/20000 Training Loss: 0.05869019031524658\n",
      "Epoch 13570/20000 Validation Loss: 0.0608687624335289\n",
      "Epoch 13571/20000 Training Loss: 0.03789359703660011\n",
      "Epoch 13572/20000 Training Loss: 0.060532182455062866\n",
      "Epoch 13573/20000 Training Loss: 0.05503600835800171\n",
      "Epoch 13574/20000 Training Loss: 0.05138738825917244\n",
      "Epoch 13575/20000 Training Loss: 0.04331604763865471\n",
      "Epoch 13576/20000 Training Loss: 0.053543101996183395\n",
      "Epoch 13577/20000 Training Loss: 0.05380925536155701\n",
      "Epoch 13578/20000 Training Loss: 0.06009380891919136\n",
      "Epoch 13579/20000 Training Loss: 0.05940984562039375\n",
      "Epoch 13580/20000 Training Loss: 0.04586639627814293\n",
      "Epoch 13580/20000 Validation Loss: 0.04536646604537964\n",
      "Epoch 13581/20000 Training Loss: 0.04645472392439842\n",
      "Epoch 13582/20000 Training Loss: 0.04279514029622078\n",
      "Epoch 13583/20000 Training Loss: 0.06328392028808594\n",
      "Epoch 13584/20000 Training Loss: 0.056943293660879135\n",
      "Epoch 13585/20000 Training Loss: 0.060903698205947876\n",
      "Epoch 13586/20000 Training Loss: 0.05330316722393036\n",
      "Epoch 13587/20000 Training Loss: 0.05087769031524658\n",
      "Epoch 13588/20000 Training Loss: 0.04637004807591438\n",
      "Epoch 13589/20000 Training Loss: 0.04392525926232338\n",
      "Epoch 13590/20000 Training Loss: 0.049758944660425186\n",
      "Epoch 13590/20000 Validation Loss: 0.05345600098371506\n",
      "Epoch 13591/20000 Training Loss: 0.04878978058695793\n",
      "Epoch 13592/20000 Training Loss: 0.044010866433382034\n",
      "Epoch 13593/20000 Training Loss: 0.0631876215338707\n",
      "Epoch 13594/20000 Training Loss: 0.06900494545698166\n",
      "Epoch 13595/20000 Training Loss: 0.053105685859918594\n",
      "Epoch 13596/20000 Training Loss: 0.0472823791205883\n",
      "Epoch 13597/20000 Training Loss: 0.06434217840433121\n",
      "Epoch 13598/20000 Training Loss: 0.04776085540652275\n",
      "Epoch 13599/20000 Training Loss: 0.05291904881596565\n",
      "Epoch 13600/20000 Training Loss: 0.05272400379180908\n",
      "Epoch 13600/20000 Validation Loss: 0.045067884027957916\n",
      "Epoch 13601/20000 Training Loss: 0.05790706351399422\n",
      "Epoch 13602/20000 Training Loss: 0.060605715960264206\n",
      "Epoch 13603/20000 Training Loss: 0.04072101414203644\n",
      "Epoch 13604/20000 Training Loss: 0.05798357352614403\n",
      "Epoch 13605/20000 Training Loss: 0.062233757227659225\n",
      "Epoch 13606/20000 Training Loss: 0.060429174453020096\n",
      "Epoch 13607/20000 Training Loss: 0.06036854162812233\n",
      "Epoch 13608/20000 Training Loss: 0.04090357944369316\n",
      "Epoch 13609/20000 Training Loss: 0.06091034412384033\n",
      "Epoch 13610/20000 Training Loss: 0.07022254914045334\n",
      "Epoch 13610/20000 Validation Loss: 0.048802636563777924\n",
      "Epoch 13611/20000 Training Loss: 0.05698003992438316\n",
      "Epoch 13612/20000 Training Loss: 0.0780251994729042\n",
      "Epoch 13613/20000 Training Loss: 0.05352199077606201\n",
      "Epoch 13614/20000 Training Loss: 0.05672840401530266\n",
      "Epoch 13615/20000 Training Loss: 0.059927474707365036\n",
      "Epoch 13616/20000 Training Loss: 0.06147363781929016\n",
      "Epoch 13617/20000 Training Loss: 0.06950483471155167\n",
      "Epoch 13618/20000 Training Loss: 0.06396496295928955\n",
      "Epoch 13619/20000 Training Loss: 0.053704261779785156\n",
      "Epoch 13620/20000 Training Loss: 0.04586835205554962\n",
      "Epoch 13620/20000 Validation Loss: 0.06149443984031677\n",
      "Epoch 13621/20000 Training Loss: 0.050431590527296066\n",
      "Epoch 13622/20000 Training Loss: 0.05405249074101448\n",
      "Epoch 13623/20000 Training Loss: 0.06109847500920296\n",
      "Epoch 13624/20000 Training Loss: 0.07179468125104904\n",
      "Epoch 13625/20000 Training Loss: 0.08285080641508102\n",
      "Epoch 13626/20000 Training Loss: 0.04877405986189842\n",
      "Epoch 13627/20000 Training Loss: 0.05393700301647186\n",
      "Epoch 13628/20000 Training Loss: 0.05201463773846626\n",
      "Epoch 13629/20000 Training Loss: 0.0507386140525341\n",
      "Epoch 13630/20000 Training Loss: 0.05784652754664421\n",
      "Epoch 13630/20000 Validation Loss: 0.06761253625154495\n",
      "Epoch 13631/20000 Training Loss: 0.06611021608114243\n",
      "Epoch 13632/20000 Training Loss: 0.06608492881059647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13633/20000 Training Loss: 0.05835801362991333\n",
      "Epoch 13634/20000 Training Loss: 0.05747996270656586\n",
      "Epoch 13635/20000 Training Loss: 0.05363258719444275\n",
      "Epoch 13636/20000 Training Loss: 0.04153529927134514\n",
      "Epoch 13637/20000 Training Loss: 0.060714852064847946\n",
      "Epoch 13638/20000 Training Loss: 0.04536672309041023\n",
      "Epoch 13639/20000 Training Loss: 0.07033288478851318\n",
      "Epoch 13640/20000 Training Loss: 0.055855900049209595\n",
      "Epoch 13640/20000 Validation Loss: 0.05056941881775856\n",
      "Epoch 13641/20000 Training Loss: 0.05935699865221977\n",
      "Epoch 13642/20000 Training Loss: 0.04079967364668846\n",
      "Epoch 13643/20000 Training Loss: 0.059308093041181564\n",
      "Epoch 13644/20000 Training Loss: 0.04305034503340721\n",
      "Epoch 13645/20000 Training Loss: 0.05245563015341759\n",
      "Epoch 13646/20000 Training Loss: 0.05841746926307678\n",
      "Epoch 13647/20000 Training Loss: 0.05395028367638588\n",
      "Epoch 13648/20000 Training Loss: 0.0698462426662445\n",
      "Epoch 13649/20000 Training Loss: 0.04111656919121742\n",
      "Epoch 13650/20000 Training Loss: 0.05947497487068176\n",
      "Epoch 13650/20000 Validation Loss: 0.040397390723228455\n",
      "Epoch 13651/20000 Training Loss: 0.05221201851963997\n",
      "Epoch 13652/20000 Training Loss: 0.04249739274382591\n",
      "Epoch 13653/20000 Training Loss: 0.051628708839416504\n",
      "Epoch 13654/20000 Training Loss: 0.05184909701347351\n",
      "Epoch 13655/20000 Training Loss: 0.06930205225944519\n",
      "Epoch 13656/20000 Training Loss: 0.059335868805646896\n",
      "Epoch 13657/20000 Training Loss: 0.062152598053216934\n",
      "Epoch 13658/20000 Training Loss: 0.057279009371995926\n",
      "Epoch 13659/20000 Training Loss: 0.04543079063296318\n",
      "Epoch 13660/20000 Training Loss: 0.07242123782634735\n",
      "Epoch 13660/20000 Validation Loss: 0.04764081910252571\n",
      "Epoch 13661/20000 Training Loss: 0.05848381295800209\n",
      "Epoch 13662/20000 Training Loss: 0.06056772544980049\n",
      "Epoch 13663/20000 Training Loss: 0.0568644255399704\n",
      "Epoch 13664/20000 Training Loss: 0.050246745347976685\n",
      "Epoch 13665/20000 Training Loss: 0.0489245243370533\n",
      "Epoch 13666/20000 Training Loss: 0.055097270756959915\n",
      "Epoch 13667/20000 Training Loss: 0.058330316096544266\n",
      "Epoch 13668/20000 Training Loss: 0.04140624403953552\n",
      "Epoch 13669/20000 Training Loss: 0.04521485045552254\n",
      "Epoch 13670/20000 Training Loss: 0.04322604462504387\n",
      "Epoch 13670/20000 Validation Loss: 0.04755806922912598\n",
      "Epoch 13671/20000 Training Loss: 0.043732285499572754\n",
      "Epoch 13672/20000 Training Loss: 0.05011361837387085\n",
      "Epoch 13673/20000 Training Loss: 0.057452112436294556\n",
      "Epoch 13674/20000 Training Loss: 0.045935217291116714\n",
      "Epoch 13675/20000 Training Loss: 0.048557382076978683\n",
      "Epoch 13676/20000 Training Loss: 0.051117345690727234\n",
      "Epoch 13677/20000 Training Loss: 0.057389695197343826\n",
      "Epoch 13678/20000 Training Loss: 0.04877273365855217\n",
      "Epoch 13679/20000 Training Loss: 0.05212107300758362\n",
      "Epoch 13680/20000 Training Loss: 0.058353498578071594\n",
      "Epoch 13680/20000 Validation Loss: 0.044640615582466125\n",
      "Epoch 13681/20000 Training Loss: 0.05158652737736702\n",
      "Epoch 13682/20000 Training Loss: 0.07599035650491714\n",
      "Epoch 13683/20000 Training Loss: 0.05246122553944588\n",
      "Epoch 13684/20000 Training Loss: 0.052204664796590805\n",
      "Epoch 13685/20000 Training Loss: 0.059808399528265\n",
      "Epoch 13686/20000 Training Loss: 0.04557875171303749\n",
      "Epoch 13687/20000 Training Loss: 0.0583256334066391\n",
      "Epoch 13688/20000 Training Loss: 0.06444195657968521\n",
      "Epoch 13689/20000 Training Loss: 0.04849836602807045\n",
      "Epoch 13690/20000 Training Loss: 0.05167565867304802\n",
      "Epoch 13690/20000 Validation Loss: 0.047714464366436005\n",
      "Epoch 13691/20000 Training Loss: 0.0663149431347847\n",
      "Epoch 13692/20000 Training Loss: 0.07053723186254501\n",
      "Epoch 13693/20000 Training Loss: 0.040945034474134445\n",
      "Epoch 13694/20000 Training Loss: 0.057207703590393066\n",
      "Epoch 13695/20000 Training Loss: 0.05116976425051689\n",
      "Epoch 13696/20000 Training Loss: 0.05012517794966698\n",
      "Epoch 13697/20000 Training Loss: 0.0534115694463253\n",
      "Epoch 13698/20000 Training Loss: 0.03964167460799217\n",
      "Epoch 13699/20000 Training Loss: 0.057478051632642746\n",
      "Epoch 13700/20000 Training Loss: 0.060999687761068344\n",
      "Epoch 13700/20000 Validation Loss: 0.04626099020242691\n",
      "Epoch 13701/20000 Training Loss: 0.05721293389797211\n",
      "Epoch 13702/20000 Training Loss: 0.07050109654664993\n",
      "Epoch 13703/20000 Training Loss: 0.05157517269253731\n",
      "Epoch 13704/20000 Training Loss: 0.04480768367648125\n",
      "Epoch 13705/20000 Training Loss: 0.05802541971206665\n",
      "Epoch 13706/20000 Training Loss: 0.051428575068712234\n",
      "Epoch 13707/20000 Training Loss: 0.0416564904153347\n",
      "Epoch 13708/20000 Training Loss: 0.060966406017541885\n",
      "Epoch 13709/20000 Training Loss: 0.061799049377441406\n",
      "Epoch 13710/20000 Training Loss: 0.0685931146144867\n",
      "Epoch 13710/20000 Validation Loss: 0.042631007730960846\n",
      "Epoch 13711/20000 Training Loss: 0.05626415088772774\n",
      "Epoch 13712/20000 Training Loss: 0.06494327634572983\n",
      "Epoch 13713/20000 Training Loss: 0.03653581440448761\n",
      "Epoch 13714/20000 Training Loss: 0.04310857877135277\n",
      "Epoch 13715/20000 Training Loss: 0.05970779061317444\n",
      "Epoch 13716/20000 Training Loss: 0.04807344079017639\n",
      "Epoch 13717/20000 Training Loss: 0.05278840661048889\n",
      "Epoch 13718/20000 Training Loss: 0.06467030197381973\n",
      "Epoch 13719/20000 Training Loss: 0.04730615019798279\n",
      "Epoch 13720/20000 Training Loss: 0.0630078837275505\n",
      "Epoch 13720/20000 Validation Loss: 0.051980093121528625\n",
      "Epoch 13721/20000 Training Loss: 0.05212290957570076\n",
      "Epoch 13722/20000 Training Loss: 0.05510290339589119\n",
      "Epoch 13723/20000 Training Loss: 0.05992433428764343\n",
      "Epoch 13724/20000 Training Loss: 0.06075320765376091\n",
      "Epoch 13725/20000 Training Loss: 0.047553837299346924\n",
      "Epoch 13726/20000 Training Loss: 0.05072455480694771\n",
      "Epoch 13727/20000 Training Loss: 0.06646234542131424\n",
      "Epoch 13728/20000 Training Loss: 0.07670287042856216\n",
      "Epoch 13729/20000 Training Loss: 0.054299455136060715\n",
      "Epoch 13730/20000 Training Loss: 0.04690469428896904\n",
      "Epoch 13730/20000 Validation Loss: 0.06466788798570633\n",
      "Epoch 13731/20000 Training Loss: 0.05915624275803566\n",
      "Epoch 13732/20000 Training Loss: 0.048593997955322266\n",
      "Epoch 13733/20000 Training Loss: 0.060968052595853806\n",
      "Epoch 13734/20000 Training Loss: 0.03902985155582428\n",
      "Epoch 13735/20000 Training Loss: 0.053454603999853134\n",
      "Epoch 13736/20000 Training Loss: 0.060657307505607605\n",
      "Epoch 13737/20000 Training Loss: 0.06694328039884567\n",
      "Epoch 13738/20000 Training Loss: 0.05648333951830864\n",
      "Epoch 13739/20000 Training Loss: 0.0664832666516304\n",
      "Epoch 13740/20000 Training Loss: 0.057379648089408875\n",
      "Epoch 13740/20000 Validation Loss: 0.0675329640507698\n",
      "Epoch 13741/20000 Training Loss: 0.06637707352638245\n",
      "Epoch 13742/20000 Training Loss: 0.06976740807294846\n",
      "Epoch 13743/20000 Training Loss: 0.04825723543763161\n",
      "Epoch 13744/20000 Training Loss: 0.05907426401972771\n",
      "Epoch 13745/20000 Training Loss: 0.04812696576118469\n",
      "Epoch 13746/20000 Training Loss: 0.04772844538092613\n",
      "Epoch 13747/20000 Training Loss: 0.040463637560606\n",
      "Epoch 13748/20000 Training Loss: 0.08271152526140213\n",
      "Epoch 13749/20000 Training Loss: 0.05298449471592903\n",
      "Epoch 13750/20000 Training Loss: 0.06502214819192886\n",
      "Epoch 13750/20000 Validation Loss: 0.043305832892656326\n",
      "Epoch 13751/20000 Training Loss: 0.054216012358665466\n",
      "Epoch 13752/20000 Training Loss: 0.05265267565846443\n",
      "Epoch 13753/20000 Training Loss: 0.041973043233156204\n",
      "Epoch 13754/20000 Training Loss: 0.05747932195663452\n",
      "Epoch 13755/20000 Training Loss: 0.0717940405011177\n",
      "Epoch 13756/20000 Training Loss: 0.059135910123586655\n",
      "Epoch 13757/20000 Training Loss: 0.06564036011695862\n",
      "Epoch 13758/20000 Training Loss: 0.041437115520238876\n",
      "Epoch 13759/20000 Training Loss: 0.04533671215176582\n",
      "Epoch 13760/20000 Training Loss: 0.05701411888003349\n",
      "Epoch 13760/20000 Validation Loss: 0.04663664847612381\n",
      "Epoch 13761/20000 Training Loss: 0.05486772581934929\n",
      "Epoch 13762/20000 Training Loss: 0.04645933583378792\n",
      "Epoch 13763/20000 Training Loss: 0.06716769188642502\n",
      "Epoch 13764/20000 Training Loss: 0.0646684318780899\n",
      "Epoch 13765/20000 Training Loss: 0.0594935417175293\n",
      "Epoch 13766/20000 Training Loss: 0.06909549236297607\n",
      "Epoch 13767/20000 Training Loss: 0.06072576716542244\n",
      "Epoch 13768/20000 Training Loss: 0.0369127094745636\n",
      "Epoch 13769/20000 Training Loss: 0.049616456031799316\n",
      "Epoch 13770/20000 Training Loss: 0.0594806969165802\n",
      "Epoch 13770/20000 Validation Loss: 0.054546162486076355\n",
      "Epoch 13771/20000 Training Loss: 0.055959898978471756\n",
      "Epoch 13772/20000 Training Loss: 0.07018844038248062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13773/20000 Training Loss: 0.04934665560722351\n",
      "Epoch 13774/20000 Training Loss: 0.039515476673841476\n",
      "Epoch 13775/20000 Training Loss: 0.05749271437525749\n",
      "Epoch 13776/20000 Training Loss: 0.041147101670503616\n",
      "Epoch 13777/20000 Training Loss: 0.050051990896463394\n",
      "Epoch 13778/20000 Training Loss: 0.05075584724545479\n",
      "Epoch 13779/20000 Training Loss: 0.039365917444229126\n",
      "Epoch 13780/20000 Training Loss: 0.05739623308181763\n",
      "Epoch 13780/20000 Validation Loss: 0.056682486087083817\n",
      "Epoch 13781/20000 Training Loss: 0.0685737207531929\n",
      "Epoch 13782/20000 Training Loss: 0.058659013360738754\n",
      "Epoch 13783/20000 Training Loss: 0.07198483496904373\n",
      "Epoch 13784/20000 Training Loss: 0.06357783079147339\n",
      "Epoch 13785/20000 Training Loss: 0.0629490315914154\n",
      "Epoch 13786/20000 Training Loss: 0.07479660958051682\n",
      "Epoch 13787/20000 Training Loss: 0.05213971436023712\n",
      "Epoch 13788/20000 Training Loss: 0.05790208652615547\n",
      "Epoch 13789/20000 Training Loss: 0.05392998084425926\n",
      "Epoch 13790/20000 Training Loss: 0.04579121246933937\n",
      "Epoch 13790/20000 Validation Loss: 0.06282735615968704\n",
      "Epoch 13791/20000 Training Loss: 0.057451386004686356\n",
      "Epoch 13792/20000 Training Loss: 0.04981781914830208\n",
      "Epoch 13793/20000 Training Loss: 0.05250197649002075\n",
      "Epoch 13794/20000 Training Loss: 0.05716298520565033\n",
      "Epoch 13795/20000 Training Loss: 0.043554406613111496\n",
      "Epoch 13796/20000 Training Loss: 0.07069394737482071\n",
      "Epoch 13797/20000 Training Loss: 0.055866945534944534\n",
      "Epoch 13798/20000 Training Loss: 0.05648238584399223\n",
      "Epoch 13799/20000 Training Loss: 0.05856354534626007\n",
      "Epoch 13800/20000 Training Loss: 0.04945668950676918\n",
      "Epoch 13800/20000 Validation Loss: 0.0527491495013237\n",
      "Epoch 13801/20000 Training Loss: 0.060657892376184464\n",
      "Epoch 13802/20000 Training Loss: 0.06880155950784683\n",
      "Epoch 13803/20000 Training Loss: 0.04648905619978905\n",
      "Epoch 13804/20000 Training Loss: 0.04367029666900635\n",
      "Epoch 13805/20000 Training Loss: 0.06335978955030441\n",
      "Epoch 13806/20000 Training Loss: 0.04812303185462952\n",
      "Epoch 13807/20000 Training Loss: 0.04539826512336731\n",
      "Epoch 13808/20000 Training Loss: 0.03818536549806595\n",
      "Epoch 13809/20000 Training Loss: 0.05167374387383461\n",
      "Epoch 13810/20000 Training Loss: 0.04115665704011917\n",
      "Epoch 13810/20000 Validation Loss: 0.03519158065319061\n",
      "Epoch 13811/20000 Training Loss: 0.060434263199567795\n",
      "Epoch 13812/20000 Training Loss: 0.050437912344932556\n",
      "Epoch 13813/20000 Training Loss: 0.0532911978662014\n",
      "Epoch 13814/20000 Training Loss: 0.061497047543525696\n",
      "Epoch 13815/20000 Training Loss: 0.05936506763100624\n",
      "Epoch 13816/20000 Training Loss: 0.06098382547497749\n",
      "Epoch 13817/20000 Training Loss: 0.056192900985479355\n",
      "Epoch 13818/20000 Training Loss: 0.05637415125966072\n",
      "Epoch 13819/20000 Training Loss: 0.05424962565302849\n",
      "Epoch 13820/20000 Training Loss: 0.0475393682718277\n",
      "Epoch 13820/20000 Validation Loss: 0.06959275901317596\n",
      "Epoch 13821/20000 Training Loss: 0.05084560438990593\n",
      "Epoch 13822/20000 Training Loss: 0.04344630241394043\n",
      "Epoch 13823/20000 Training Loss: 0.04982556030154228\n",
      "Epoch 13824/20000 Training Loss: 0.06863599270582199\n",
      "Epoch 13825/20000 Training Loss: 0.050356701016426086\n",
      "Epoch 13826/20000 Training Loss: 0.05543509125709534\n",
      "Epoch 13827/20000 Training Loss: 0.0797763392329216\n",
      "Epoch 13828/20000 Training Loss: 0.05114997923374176\n",
      "Epoch 13829/20000 Training Loss: 0.0414663702249527\n",
      "Epoch 13830/20000 Training Loss: 0.05962587520480156\n",
      "Epoch 13830/20000 Validation Loss: 0.055822499096393585\n",
      "Epoch 13831/20000 Training Loss: 0.04259991645812988\n",
      "Epoch 13832/20000 Training Loss: 0.05431189015507698\n",
      "Epoch 13833/20000 Training Loss: 0.052991196513175964\n",
      "Epoch 13834/20000 Training Loss: 0.05184661224484444\n",
      "Epoch 13835/20000 Training Loss: 0.07142776995897293\n",
      "Epoch 13836/20000 Training Loss: 0.06127610430121422\n",
      "Epoch 13837/20000 Training Loss: 0.04274189472198486\n",
      "Epoch 13838/20000 Training Loss: 0.08219250291585922\n",
      "Epoch 13839/20000 Training Loss: 0.05080500617623329\n",
      "Epoch 13840/20000 Training Loss: 0.05932549014687538\n",
      "Epoch 13840/20000 Validation Loss: 0.08365637063980103\n",
      "Epoch 13841/20000 Training Loss: 0.04509945586323738\n",
      "Epoch 13842/20000 Training Loss: 0.037278320640325546\n",
      "Epoch 13843/20000 Training Loss: 0.06265976279973984\n",
      "Epoch 13844/20000 Training Loss: 0.0507146380841732\n",
      "Epoch 13845/20000 Training Loss: 0.06329513341188431\n",
      "Epoch 13846/20000 Training Loss: 0.0555509515106678\n",
      "Epoch 13847/20000 Training Loss: 0.04351043701171875\n",
      "Epoch 13848/20000 Training Loss: 0.061079103499650955\n",
      "Epoch 13849/20000 Training Loss: 0.050154801458120346\n",
      "Epoch 13850/20000 Training Loss: 0.06709811836481094\n",
      "Epoch 13850/20000 Validation Loss: 0.057374507188797\n",
      "Epoch 13851/20000 Training Loss: 0.051372408866882324\n",
      "Epoch 13852/20000 Training Loss: 0.03466497361660004\n",
      "Epoch 13853/20000 Training Loss: 0.035600993782281876\n",
      "Epoch 13854/20000 Training Loss: 0.043946776539087296\n",
      "Epoch 13855/20000 Training Loss: 0.04862840473651886\n",
      "Epoch 13856/20000 Training Loss: 0.05337119102478027\n",
      "Epoch 13857/20000 Training Loss: 0.05170544609427452\n",
      "Epoch 13858/20000 Training Loss: 0.0655827596783638\n",
      "Epoch 13859/20000 Training Loss: 0.06391303986310959\n",
      "Epoch 13860/20000 Training Loss: 0.06176517903804779\n",
      "Epoch 13860/20000 Validation Loss: 0.05911271274089813\n",
      "Epoch 13861/20000 Training Loss: 0.04270952567458153\n",
      "Epoch 13862/20000 Training Loss: 0.05325029790401459\n",
      "Epoch 13863/20000 Training Loss: 0.0468917191028595\n",
      "Epoch 13864/20000 Training Loss: 0.05458461865782738\n",
      "Epoch 13865/20000 Training Loss: 0.04912402853369713\n",
      "Epoch 13866/20000 Training Loss: 0.058006320148706436\n",
      "Epoch 13867/20000 Training Loss: 0.06525948643684387\n",
      "Epoch 13868/20000 Training Loss: 0.06427906453609467\n",
      "Epoch 13869/20000 Training Loss: 0.04562155529856682\n",
      "Epoch 13870/20000 Training Loss: 0.06076207756996155\n",
      "Epoch 13870/20000 Validation Loss: 0.0658365935087204\n",
      "Epoch 13871/20000 Training Loss: 0.06638000905513763\n",
      "Epoch 13872/20000 Training Loss: 0.05289648845791817\n",
      "Epoch 13873/20000 Training Loss: 0.04673195257782936\n",
      "Epoch 13874/20000 Training Loss: 0.054249268025159836\n",
      "Epoch 13875/20000 Training Loss: 0.07647690922021866\n",
      "Epoch 13876/20000 Training Loss: 0.04840971529483795\n",
      "Epoch 13877/20000 Training Loss: 0.045324984937906265\n",
      "Epoch 13878/20000 Training Loss: 0.047130364924669266\n",
      "Epoch 13879/20000 Training Loss: 0.048975080251693726\n",
      "Epoch 13880/20000 Training Loss: 0.04585680365562439\n",
      "Epoch 13880/20000 Validation Loss: 0.05380159616470337\n",
      "Epoch 13881/20000 Training Loss: 0.060051798820495605\n",
      "Epoch 13882/20000 Training Loss: 0.05396558344364166\n",
      "Epoch 13883/20000 Training Loss: 0.07077433913946152\n",
      "Epoch 13884/20000 Training Loss: 0.060567647218704224\n",
      "Epoch 13885/20000 Training Loss: 0.05024689435958862\n",
      "Epoch 13886/20000 Training Loss: 0.04495551064610481\n",
      "Epoch 13887/20000 Training Loss: 0.057654108852148056\n",
      "Epoch 13888/20000 Training Loss: 0.039109375327825546\n",
      "Epoch 13889/20000 Training Loss: 0.04386431351304054\n",
      "Epoch 13890/20000 Training Loss: 0.06663580983877182\n",
      "Epoch 13890/20000 Validation Loss: 0.06873055547475815\n",
      "Epoch 13891/20000 Training Loss: 0.048842739313840866\n",
      "Epoch 13892/20000 Training Loss: 0.04678143188357353\n",
      "Epoch 13893/20000 Training Loss: 0.059304606169462204\n",
      "Epoch 13894/20000 Training Loss: 0.05674558877944946\n",
      "Epoch 13895/20000 Training Loss: 0.04623701050877571\n",
      "Epoch 13896/20000 Training Loss: 0.042273204773664474\n",
      "Epoch 13897/20000 Training Loss: 0.0547247938811779\n",
      "Epoch 13898/20000 Training Loss: 0.04261235520243645\n",
      "Epoch 13899/20000 Training Loss: 0.05126567184925079\n",
      "Epoch 13900/20000 Training Loss: 0.05302202329039574\n",
      "Epoch 13900/20000 Validation Loss: 0.04323216527700424\n",
      "Epoch 13901/20000 Training Loss: 0.06179956719279289\n",
      "Epoch 13902/20000 Training Loss: 0.0679277777671814\n",
      "Epoch 13903/20000 Training Loss: 0.04455597326159477\n",
      "Epoch 13904/20000 Training Loss: 0.04057597741484642\n",
      "Epoch 13905/20000 Training Loss: 0.047428857535123825\n",
      "Epoch 13906/20000 Training Loss: 0.06113751605153084\n",
      "Epoch 13907/20000 Training Loss: 0.05117211118340492\n",
      "Epoch 13908/20000 Training Loss: 0.053214605897665024\n",
      "Epoch 13909/20000 Training Loss: 0.05263923108577728\n",
      "Epoch 13910/20000 Training Loss: 0.05478103086352348\n",
      "Epoch 13910/20000 Validation Loss: 0.04914659261703491\n",
      "Epoch 13911/20000 Training Loss: 0.05632983148097992\n",
      "Epoch 13912/20000 Training Loss: 0.041478510946035385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13913/20000 Training Loss: 0.05502156540751457\n",
      "Epoch 13914/20000 Training Loss: 0.05830089747905731\n",
      "Epoch 13915/20000 Training Loss: 0.052054986357688904\n",
      "Epoch 13916/20000 Training Loss: 0.07269007712602615\n",
      "Epoch 13917/20000 Training Loss: 0.05257387086749077\n",
      "Epoch 13918/20000 Training Loss: 0.04773314669728279\n",
      "Epoch 13919/20000 Training Loss: 0.04771055281162262\n",
      "Epoch 13920/20000 Training Loss: 0.06629224866628647\n",
      "Epoch 13920/20000 Validation Loss: 0.049955934286117554\n",
      "Epoch 13921/20000 Training Loss: 0.05491259694099426\n",
      "Epoch 13922/20000 Training Loss: 0.061505842953920364\n",
      "Epoch 13923/20000 Training Loss: 0.06831742078065872\n",
      "Epoch 13924/20000 Training Loss: 0.05281256511807442\n",
      "Epoch 13925/20000 Training Loss: 0.04055345058441162\n",
      "Epoch 13926/20000 Training Loss: 0.05759138986468315\n",
      "Epoch 13927/20000 Training Loss: 0.07820096611976624\n",
      "Epoch 13928/20000 Training Loss: 0.05914086475968361\n",
      "Epoch 13929/20000 Training Loss: 0.04619264975190163\n",
      "Epoch 13930/20000 Training Loss: 0.0563775897026062\n",
      "Epoch 13930/20000 Validation Loss: 0.050932902842760086\n",
      "Epoch 13931/20000 Training Loss: 0.05322788655757904\n",
      "Epoch 13932/20000 Training Loss: 0.04505342245101929\n",
      "Epoch 13933/20000 Training Loss: 0.04667017236351967\n",
      "Epoch 13934/20000 Training Loss: 0.059007178992033005\n",
      "Epoch 13935/20000 Training Loss: 0.05604450777173042\n",
      "Epoch 13936/20000 Training Loss: 0.04254031181335449\n",
      "Epoch 13937/20000 Training Loss: 0.049984950572252274\n",
      "Epoch 13938/20000 Training Loss: 0.05005994811654091\n",
      "Epoch 13939/20000 Training Loss: 0.05325191095471382\n",
      "Epoch 13940/20000 Training Loss: 0.05601709708571434\n",
      "Epoch 13940/20000 Validation Loss: 0.07360167801380157\n",
      "Epoch 13941/20000 Training Loss: 0.042065054178237915\n",
      "Epoch 13942/20000 Training Loss: 0.039708420634269714\n",
      "Epoch 13943/20000 Training Loss: 0.051264286041259766\n",
      "Epoch 13944/20000 Training Loss: 0.042668361216783524\n",
      "Epoch 13945/20000 Training Loss: 0.05005420744419098\n",
      "Epoch 13946/20000 Training Loss: 0.06923189759254456\n",
      "Epoch 13947/20000 Training Loss: 0.055282846093177795\n",
      "Epoch 13948/20000 Training Loss: 0.06006177142262459\n",
      "Epoch 13949/20000 Training Loss: 0.050943683832883835\n",
      "Epoch 13950/20000 Training Loss: 0.06798822432756424\n",
      "Epoch 13950/20000 Validation Loss: 0.04946325719356537\n",
      "Epoch 13951/20000 Training Loss: 0.06176904961466789\n",
      "Epoch 13952/20000 Training Loss: 0.05410230532288551\n",
      "Epoch 13953/20000 Training Loss: 0.059821199625730515\n",
      "Epoch 13954/20000 Training Loss: 0.05869367718696594\n",
      "Epoch 13955/20000 Training Loss: 0.06176529452204704\n",
      "Epoch 13956/20000 Training Loss: 0.05101892724633217\n",
      "Epoch 13957/20000 Training Loss: 0.03150061145424843\n",
      "Epoch 13958/20000 Training Loss: 0.0584808774292469\n",
      "Epoch 13959/20000 Training Loss: 0.03577469661831856\n",
      "Epoch 13960/20000 Training Loss: 0.03958949074149132\n",
      "Epoch 13960/20000 Validation Loss: 0.06358825415372849\n",
      "Epoch 13961/20000 Training Loss: 0.05224715545773506\n",
      "Epoch 13962/20000 Training Loss: 0.056416068226099014\n",
      "Epoch 13963/20000 Training Loss: 0.05083199962973595\n",
      "Epoch 13964/20000 Training Loss: 0.07772431522607803\n",
      "Epoch 13965/20000 Training Loss: 0.04823833703994751\n",
      "Epoch 13966/20000 Training Loss: 0.0644209012389183\n",
      "Epoch 13967/20000 Training Loss: 0.05719161033630371\n",
      "Epoch 13968/20000 Training Loss: 0.042261380702257156\n",
      "Epoch 13969/20000 Training Loss: 0.042032331228256226\n",
      "Epoch 13970/20000 Training Loss: 0.05416850373148918\n",
      "Epoch 13970/20000 Validation Loss: 0.07532109320163727\n",
      "Epoch 13971/20000 Training Loss: 0.06582259386777878\n",
      "Epoch 13972/20000 Training Loss: 0.06441812962293625\n",
      "Epoch 13973/20000 Training Loss: 0.06861258298158646\n",
      "Epoch 13974/20000 Training Loss: 0.054594773799180984\n",
      "Epoch 13975/20000 Training Loss: 0.04915369674563408\n",
      "Epoch 13976/20000 Training Loss: 0.05088389292359352\n",
      "Epoch 13977/20000 Training Loss: 0.07300855964422226\n",
      "Epoch 13978/20000 Training Loss: 0.05380262807011604\n",
      "Epoch 13979/20000 Training Loss: 0.06312195211648941\n",
      "Epoch 13980/20000 Training Loss: 0.05980009213089943\n",
      "Epoch 13980/20000 Validation Loss: 0.08212584257125854\n",
      "Epoch 13981/20000 Training Loss: 0.046578988432884216\n",
      "Epoch 13982/20000 Training Loss: 0.04424777254462242\n",
      "Epoch 13983/20000 Training Loss: 0.059576813131570816\n",
      "Epoch 13984/20000 Training Loss: 0.04711275175213814\n",
      "Epoch 13985/20000 Training Loss: 0.06739387661218643\n",
      "Epoch 13986/20000 Training Loss: 0.05246816202998161\n",
      "Epoch 13987/20000 Training Loss: 0.05776497721672058\n",
      "Epoch 13988/20000 Training Loss: 0.04925740882754326\n",
      "Epoch 13989/20000 Training Loss: 0.052085865288972855\n",
      "Epoch 13990/20000 Training Loss: 0.05276551470160484\n",
      "Epoch 13990/20000 Validation Loss: 0.04917223006486893\n",
      "Epoch 13991/20000 Training Loss: 0.062272775918245316\n",
      "Epoch 13992/20000 Training Loss: 0.04960644617676735\n",
      "Epoch 13993/20000 Training Loss: 0.04248841479420662\n",
      "Epoch 13994/20000 Training Loss: 0.06354828178882599\n",
      "Epoch 13995/20000 Training Loss: 0.04826571047306061\n",
      "Epoch 13996/20000 Training Loss: 0.04315681755542755\n",
      "Epoch 13997/20000 Training Loss: 0.05338257923722267\n",
      "Epoch 13998/20000 Training Loss: 0.05209672451019287\n",
      "Epoch 13999/20000 Training Loss: 0.07543927431106567\n",
      "Epoch 14000/20000 Training Loss: 0.0501956008374691\n",
      "Epoch 14000/20000 Validation Loss: 0.061024755239486694\n",
      "Epoch 14001/20000 Training Loss: 0.0630539283156395\n",
      "Epoch 14002/20000 Training Loss: 0.0615183524787426\n",
      "Epoch 14003/20000 Training Loss: 0.05664266273379326\n",
      "Epoch 14004/20000 Training Loss: 0.036670148372650146\n",
      "Epoch 14005/20000 Training Loss: 0.04214505851268768\n",
      "Epoch 14006/20000 Training Loss: 0.06534742563962936\n",
      "Epoch 14007/20000 Training Loss: 0.05527336895465851\n",
      "Epoch 14008/20000 Training Loss: 0.046566352248191833\n",
      "Epoch 14009/20000 Training Loss: 0.06306099146604538\n",
      "Epoch 14010/20000 Training Loss: 0.05254153534770012\n",
      "Epoch 14010/20000 Validation Loss: 0.05611548572778702\n",
      "Epoch 14011/20000 Training Loss: 0.04439888522028923\n",
      "Epoch 14012/20000 Training Loss: 0.05169364809989929\n",
      "Epoch 14013/20000 Training Loss: 0.04063218832015991\n",
      "Epoch 14014/20000 Training Loss: 0.05227116122841835\n",
      "Epoch 14015/20000 Training Loss: 0.0565958134829998\n",
      "Epoch 14016/20000 Training Loss: 0.05107329413294792\n",
      "Epoch 14017/20000 Training Loss: 0.07312563806772232\n",
      "Epoch 14018/20000 Training Loss: 0.054268330335617065\n",
      "Epoch 14019/20000 Training Loss: 0.06486286967992783\n",
      "Epoch 14020/20000 Training Loss: 0.0714634358882904\n",
      "Epoch 14020/20000 Validation Loss: 0.06566517800092697\n",
      "Epoch 14021/20000 Training Loss: 0.05360075831413269\n",
      "Epoch 14022/20000 Training Loss: 0.04336440935730934\n",
      "Epoch 14023/20000 Training Loss: 0.05426715686917305\n",
      "Epoch 14024/20000 Training Loss: 0.04439307376742363\n",
      "Epoch 14025/20000 Training Loss: 0.06347668170928955\n",
      "Epoch 14026/20000 Training Loss: 0.07585956901311874\n",
      "Epoch 14027/20000 Training Loss: 0.037539493292570114\n",
      "Epoch 14028/20000 Training Loss: 0.06402555108070374\n",
      "Epoch 14029/20000 Training Loss: 0.052897367626428604\n",
      "Epoch 14030/20000 Training Loss: 0.0512264184653759\n",
      "Epoch 14030/20000 Validation Loss: 0.0529097244143486\n",
      "Epoch 14031/20000 Training Loss: 0.07261939346790314\n",
      "Epoch 14032/20000 Training Loss: 0.05218056961894035\n",
      "Epoch 14033/20000 Training Loss: 0.07926243543624878\n",
      "Epoch 14034/20000 Training Loss: 0.03716437146067619\n",
      "Epoch 14035/20000 Training Loss: 0.06331389397382736\n",
      "Epoch 14036/20000 Training Loss: 0.06420021504163742\n",
      "Epoch 14037/20000 Training Loss: 0.050289954990148544\n",
      "Epoch 14038/20000 Training Loss: 0.07451478391885757\n",
      "Epoch 14039/20000 Training Loss: 0.05024613067507744\n",
      "Epoch 14040/20000 Training Loss: 0.06031022593379021\n",
      "Epoch 14040/20000 Validation Loss: 0.053786635398864746\n",
      "Epoch 14041/20000 Training Loss: 0.07380944490432739\n",
      "Epoch 14042/20000 Training Loss: 0.05127149447798729\n",
      "Epoch 14043/20000 Training Loss: 0.06029777228832245\n",
      "Epoch 14044/20000 Training Loss: 0.054249703884124756\n",
      "Epoch 14045/20000 Training Loss: 0.058399755507707596\n",
      "Epoch 14046/20000 Training Loss: 0.05170932412147522\n",
      "Epoch 14047/20000 Training Loss: 0.04825848340988159\n",
      "Epoch 14048/20000 Training Loss: 0.05262799561023712\n",
      "Epoch 14049/20000 Training Loss: 0.046469371765851974\n",
      "Epoch 14050/20000 Training Loss: 0.057308223098516464\n",
      "Epoch 14050/20000 Validation Loss: 0.05722101777791977\n",
      "Epoch 14051/20000 Training Loss: 0.05912875756621361\n",
      "Epoch 14052/20000 Training Loss: 0.056488990783691406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14053/20000 Training Loss: 0.043968793004751205\n",
      "Epoch 14054/20000 Training Loss: 0.055693116039037704\n",
      "Epoch 14055/20000 Training Loss: 0.04210515320301056\n",
      "Epoch 14056/20000 Training Loss: 0.04643106460571289\n",
      "Epoch 14057/20000 Training Loss: 0.049375321716070175\n",
      "Epoch 14058/20000 Training Loss: 0.04301154986023903\n",
      "Epoch 14059/20000 Training Loss: 0.05843908712267876\n",
      "Epoch 14060/20000 Training Loss: 0.06106358394026756\n",
      "Epoch 14060/20000 Validation Loss: 0.062093984335660934\n",
      "Epoch 14061/20000 Training Loss: 0.0675019845366478\n",
      "Epoch 14062/20000 Training Loss: 0.04565521702170372\n",
      "Epoch 14063/20000 Training Loss: 0.04067648574709892\n",
      "Epoch 14064/20000 Training Loss: 0.06507425755262375\n",
      "Epoch 14065/20000 Training Loss: 0.04332569241523743\n",
      "Epoch 14066/20000 Training Loss: 0.06035436689853668\n",
      "Epoch 14067/20000 Training Loss: 0.05299133062362671\n",
      "Epoch 14068/20000 Training Loss: 0.049743086099624634\n",
      "Epoch 14069/20000 Training Loss: 0.0523192398250103\n",
      "Epoch 14070/20000 Training Loss: 0.052888210862874985\n",
      "Epoch 14070/20000 Validation Loss: 0.05039237439632416\n",
      "Epoch 14071/20000 Training Loss: 0.04144176468253136\n",
      "Epoch 14072/20000 Training Loss: 0.058886151760816574\n",
      "Epoch 14073/20000 Training Loss: 0.05690239369869232\n",
      "Epoch 14074/20000 Training Loss: 0.06933993846178055\n",
      "Epoch 14075/20000 Training Loss: 0.05912013724446297\n",
      "Epoch 14076/20000 Training Loss: 0.04706902801990509\n",
      "Epoch 14077/20000 Training Loss: 0.04941587522625923\n",
      "Epoch 14078/20000 Training Loss: 0.04008403792977333\n",
      "Epoch 14079/20000 Training Loss: 0.05461025610566139\n",
      "Epoch 14080/20000 Training Loss: 0.051482170820236206\n",
      "Epoch 14080/20000 Validation Loss: 0.05600827932357788\n",
      "Epoch 14081/20000 Training Loss: 0.04514126852154732\n",
      "Epoch 14082/20000 Training Loss: 0.059036895632743835\n",
      "Epoch 14083/20000 Training Loss: 0.051914963871240616\n",
      "Epoch 14084/20000 Training Loss: 0.05180874466896057\n",
      "Epoch 14085/20000 Training Loss: 0.04751545190811157\n",
      "Epoch 14086/20000 Training Loss: 0.051946815103292465\n",
      "Epoch 14087/20000 Training Loss: 0.0699581727385521\n",
      "Epoch 14088/20000 Training Loss: 0.048685472458601\n",
      "Epoch 14089/20000 Training Loss: 0.05683555081486702\n",
      "Epoch 14090/20000 Training Loss: 0.052375782281160355\n",
      "Epoch 14090/20000 Validation Loss: 0.04385889694094658\n",
      "Epoch 14091/20000 Training Loss: 0.0589662604033947\n",
      "Epoch 14092/20000 Training Loss: 0.06316790729761124\n",
      "Epoch 14093/20000 Training Loss: 0.06790920346975327\n",
      "Epoch 14094/20000 Training Loss: 0.05006467178463936\n",
      "Epoch 14095/20000 Training Loss: 0.07135161012411118\n",
      "Epoch 14096/20000 Training Loss: 0.05517527833580971\n",
      "Epoch 14097/20000 Training Loss: 0.05886334180831909\n",
      "Epoch 14098/20000 Training Loss: 0.05710217356681824\n",
      "Epoch 14099/20000 Training Loss: 0.04735475778579712\n",
      "Epoch 14100/20000 Training Loss: 0.05546119809150696\n",
      "Epoch 14100/20000 Validation Loss: 0.06254948675632477\n",
      "Epoch 14101/20000 Training Loss: 0.04558253288269043\n",
      "Epoch 14102/20000 Training Loss: 0.054955583065748215\n",
      "Epoch 14103/20000 Training Loss: 0.05736823007464409\n",
      "Epoch 14104/20000 Training Loss: 0.04214000701904297\n",
      "Epoch 14105/20000 Training Loss: 0.06201542541384697\n",
      "Epoch 14106/20000 Training Loss: 0.05899139121174812\n",
      "Epoch 14107/20000 Training Loss: 0.0632208064198494\n",
      "Epoch 14108/20000 Training Loss: 0.04556591436266899\n",
      "Epoch 14109/20000 Training Loss: 0.053545791655778885\n",
      "Epoch 14110/20000 Training Loss: 0.06310391426086426\n",
      "Epoch 14110/20000 Validation Loss: 0.04282296821475029\n",
      "Epoch 14111/20000 Training Loss: 0.062407176941633224\n",
      "Epoch 14112/20000 Training Loss: 0.05307094380259514\n",
      "Epoch 14113/20000 Training Loss: 0.04730696976184845\n",
      "Epoch 14114/20000 Training Loss: 0.058535199612379074\n",
      "Epoch 14115/20000 Training Loss: 0.06387932598590851\n",
      "Epoch 14116/20000 Training Loss: 0.06409161537885666\n",
      "Epoch 14117/20000 Training Loss: 0.04745159670710564\n",
      "Epoch 14118/20000 Training Loss: 0.05465969815850258\n",
      "Epoch 14119/20000 Training Loss: 0.05710483714938164\n",
      "Epoch 14120/20000 Training Loss: 0.039272889494895935\n",
      "Epoch 14120/20000 Validation Loss: 0.05751137435436249\n",
      "Epoch 14121/20000 Training Loss: 0.04368196427822113\n",
      "Epoch 14122/20000 Training Loss: 0.04228321835398674\n",
      "Epoch 14123/20000 Training Loss: 0.062456876039505005\n",
      "Epoch 14124/20000 Training Loss: 0.05219288542866707\n",
      "Epoch 14125/20000 Training Loss: 0.038810547441244125\n",
      "Epoch 14126/20000 Training Loss: 0.05282830074429512\n",
      "Epoch 14127/20000 Training Loss: 0.0360015369951725\n",
      "Epoch 14128/20000 Training Loss: 0.04861266911029816\n",
      "Epoch 14129/20000 Training Loss: 0.0771249458193779\n",
      "Epoch 14130/20000 Training Loss: 0.05178031325340271\n",
      "Epoch 14130/20000 Validation Loss: 0.04552464932203293\n",
      "Epoch 14131/20000 Training Loss: 0.041968394070863724\n",
      "Epoch 14132/20000 Training Loss: 0.0628068745136261\n",
      "Epoch 14133/20000 Training Loss: 0.052562166005373\n",
      "Epoch 14134/20000 Training Loss: 0.05314363166689873\n",
      "Epoch 14135/20000 Training Loss: 0.038635674864053726\n",
      "Epoch 14136/20000 Training Loss: 0.05333276465535164\n",
      "Epoch 14137/20000 Training Loss: 0.050288498401641846\n",
      "Epoch 14138/20000 Training Loss: 0.04022209346294403\n",
      "Epoch 14139/20000 Training Loss: 0.054366424679756165\n",
      "Epoch 14140/20000 Training Loss: 0.04514307901263237\n",
      "Epoch 14140/20000 Validation Loss: 0.06341084837913513\n",
      "Epoch 14141/20000 Training Loss: 0.04787452518939972\n",
      "Epoch 14142/20000 Training Loss: 0.04840433970093727\n",
      "Epoch 14143/20000 Training Loss: 0.035674359649419785\n",
      "Epoch 14144/20000 Training Loss: 0.06257038563489914\n",
      "Epoch 14145/20000 Training Loss: 0.05196413770318031\n",
      "Epoch 14146/20000 Training Loss: 0.06526398658752441\n",
      "Epoch 14147/20000 Training Loss: 0.060386985540390015\n",
      "Epoch 14148/20000 Training Loss: 0.06035466492176056\n",
      "Epoch 14149/20000 Training Loss: 0.047182295471429825\n",
      "Epoch 14150/20000 Training Loss: 0.06522578001022339\n",
      "Epoch 14150/20000 Validation Loss: 0.04666167497634888\n",
      "Epoch 14151/20000 Training Loss: 0.06388159841299057\n",
      "Epoch 14152/20000 Training Loss: 0.04895596206188202\n",
      "Epoch 14153/20000 Training Loss: 0.06414761394262314\n",
      "Epoch 14154/20000 Training Loss: 0.04575525224208832\n",
      "Epoch 14155/20000 Training Loss: 0.04792070388793945\n",
      "Epoch 14156/20000 Training Loss: 0.04947994276881218\n",
      "Epoch 14157/20000 Training Loss: 0.05535407364368439\n",
      "Epoch 14158/20000 Training Loss: 0.05950295552611351\n",
      "Epoch 14159/20000 Training Loss: 0.04874541237950325\n",
      "Epoch 14160/20000 Training Loss: 0.06504974514245987\n",
      "Epoch 14160/20000 Validation Loss: 0.036945343017578125\n",
      "Epoch 14161/20000 Training Loss: 0.060282718390226364\n",
      "Epoch 14162/20000 Training Loss: 0.056772131472826004\n",
      "Epoch 14163/20000 Training Loss: 0.039724063128232956\n",
      "Epoch 14164/20000 Training Loss: 0.053049486130476\n",
      "Epoch 14165/20000 Training Loss: 0.05509508028626442\n",
      "Epoch 14166/20000 Training Loss: 0.05950121954083443\n",
      "Epoch 14167/20000 Training Loss: 0.0704125165939331\n",
      "Epoch 14168/20000 Training Loss: 0.05557822063565254\n",
      "Epoch 14169/20000 Training Loss: 0.04636624455451965\n",
      "Epoch 14170/20000 Training Loss: 0.04228444769978523\n",
      "Epoch 14170/20000 Validation Loss: 0.051419682800769806\n",
      "Epoch 14171/20000 Training Loss: 0.04305429384112358\n",
      "Epoch 14172/20000 Training Loss: 0.05445605516433716\n",
      "Epoch 14173/20000 Training Loss: 0.06681203842163086\n",
      "Epoch 14174/20000 Training Loss: 0.06093941256403923\n",
      "Epoch 14175/20000 Training Loss: 0.06712470203638077\n",
      "Epoch 14176/20000 Training Loss: 0.0490996390581131\n",
      "Epoch 14177/20000 Training Loss: 0.03847628831863403\n",
      "Epoch 14178/20000 Training Loss: 0.04581587389111519\n",
      "Epoch 14179/20000 Training Loss: 0.04896943271160126\n",
      "Epoch 14180/20000 Training Loss: 0.06038455292582512\n",
      "Epoch 14180/20000 Validation Loss: 0.07381759583950043\n",
      "Epoch 14181/20000 Training Loss: 0.07636145502328873\n",
      "Epoch 14182/20000 Training Loss: 0.054002951830625534\n",
      "Epoch 14183/20000 Training Loss: 0.059418659657239914\n",
      "Epoch 14184/20000 Training Loss: 0.06275543570518494\n",
      "Epoch 14185/20000 Training Loss: 0.05283541604876518\n",
      "Epoch 14186/20000 Training Loss: 0.047525838017463684\n",
      "Epoch 14187/20000 Training Loss: 0.04215048626065254\n",
      "Epoch 14188/20000 Training Loss: 0.05779147520661354\n",
      "Epoch 14189/20000 Training Loss: 0.05707608163356781\n",
      "Epoch 14190/20000 Training Loss: 0.048716794699430466\n",
      "Epoch 14190/20000 Validation Loss: 0.05281651020050049\n",
      "Epoch 14191/20000 Training Loss: 0.06025128439068794\n",
      "Epoch 14192/20000 Training Loss: 0.04477505758404732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14193/20000 Training Loss: 0.04880101606249809\n",
      "Epoch 14194/20000 Training Loss: 0.049791306257247925\n",
      "Epoch 14195/20000 Training Loss: 0.05439228191971779\n",
      "Epoch 14196/20000 Training Loss: 0.04464608430862427\n",
      "Epoch 14197/20000 Training Loss: 0.06154121831059456\n",
      "Epoch 14198/20000 Training Loss: 0.053231775760650635\n",
      "Epoch 14199/20000 Training Loss: 0.045108016580343246\n",
      "Epoch 14200/20000 Training Loss: 0.053303610533475876\n",
      "Epoch 14200/20000 Validation Loss: 0.060023047029972076\n",
      "Epoch 14201/20000 Training Loss: 0.06378612667322159\n",
      "Epoch 14202/20000 Training Loss: 0.07320711761713028\n",
      "Epoch 14203/20000 Training Loss: 0.056840214878320694\n",
      "Epoch 14204/20000 Training Loss: 0.06102249398827553\n",
      "Epoch 14205/20000 Training Loss: 0.07335741817951202\n",
      "Epoch 14206/20000 Training Loss: 0.05193758010864258\n",
      "Epoch 14207/20000 Training Loss: 0.053314853459596634\n",
      "Epoch 14208/20000 Training Loss: 0.05017217993736267\n",
      "Epoch 14209/20000 Training Loss: 0.04926547780632973\n",
      "Epoch 14210/20000 Training Loss: 0.04478013142943382\n",
      "Epoch 14210/20000 Validation Loss: 0.05625060945749283\n",
      "Epoch 14211/20000 Training Loss: 0.05193435773253441\n",
      "Epoch 14212/20000 Training Loss: 0.05289709195494652\n",
      "Epoch 14213/20000 Training Loss: 0.05801818147301674\n",
      "Epoch 14214/20000 Training Loss: 0.07433874160051346\n",
      "Epoch 14215/20000 Training Loss: 0.055186931043863297\n",
      "Epoch 14216/20000 Training Loss: 0.03623935952782631\n",
      "Epoch 14217/20000 Training Loss: 0.05363864079117775\n",
      "Epoch 14218/20000 Training Loss: 0.04737752676010132\n",
      "Epoch 14219/20000 Training Loss: 0.07441600412130356\n",
      "Epoch 14220/20000 Training Loss: 0.045389484614133835\n",
      "Epoch 14220/20000 Validation Loss: 0.055698513984680176\n",
      "Epoch 14221/20000 Training Loss: 0.06246109679341316\n",
      "Epoch 14222/20000 Training Loss: 0.058049511164426804\n",
      "Epoch 14223/20000 Training Loss: 0.060533612966537476\n",
      "Epoch 14224/20000 Training Loss: 0.06254462152719498\n",
      "Epoch 14225/20000 Training Loss: 0.05701090767979622\n",
      "Epoch 14226/20000 Training Loss: 0.052697647362947464\n",
      "Epoch 14227/20000 Training Loss: 0.046231165528297424\n",
      "Epoch 14228/20000 Training Loss: 0.06352114677429199\n",
      "Epoch 14229/20000 Training Loss: 0.03833360970020294\n",
      "Epoch 14230/20000 Training Loss: 0.07275835424661636\n",
      "Epoch 14230/20000 Validation Loss: 0.06211274489760399\n",
      "Epoch 14231/20000 Training Loss: 0.05203958973288536\n",
      "Epoch 14232/20000 Training Loss: 0.04233086109161377\n",
      "Epoch 14233/20000 Training Loss: 0.05609389767050743\n",
      "Epoch 14234/20000 Training Loss: 0.06604225188493729\n",
      "Epoch 14235/20000 Training Loss: 0.06407177448272705\n",
      "Epoch 14236/20000 Training Loss: 0.05061992630362511\n",
      "Epoch 14237/20000 Training Loss: 0.06195199489593506\n",
      "Epoch 14238/20000 Training Loss: 0.0621892511844635\n",
      "Epoch 14239/20000 Training Loss: 0.049003105610609055\n",
      "Epoch 14240/20000 Training Loss: 0.04799870774149895\n",
      "Epoch 14240/20000 Validation Loss: 0.05837078392505646\n",
      "Epoch 14241/20000 Training Loss: 0.05222829058766365\n",
      "Epoch 14242/20000 Training Loss: 0.04939129948616028\n",
      "Epoch 14243/20000 Training Loss: 0.0670950785279274\n",
      "Epoch 14244/20000 Training Loss: 0.06282085925340652\n",
      "Epoch 14245/20000 Training Loss: 0.07083799690008163\n",
      "Epoch 14246/20000 Training Loss: 0.061400845646858215\n",
      "Epoch 14247/20000 Training Loss: 0.048181891441345215\n",
      "Epoch 14248/20000 Training Loss: 0.04196181520819664\n",
      "Epoch 14249/20000 Training Loss: 0.055337220430374146\n",
      "Epoch 14250/20000 Training Loss: 0.049854785203933716\n",
      "Epoch 14250/20000 Validation Loss: 0.06431979686021805\n",
      "Epoch 14251/20000 Training Loss: 0.04861511290073395\n",
      "Epoch 14252/20000 Training Loss: 0.06112292781472206\n",
      "Epoch 14253/20000 Training Loss: 0.04158545285463333\n",
      "Epoch 14254/20000 Training Loss: 0.05893697217106819\n",
      "Epoch 14255/20000 Training Loss: 0.05176662281155586\n",
      "Epoch 14256/20000 Training Loss: 0.04740917310118675\n",
      "Epoch 14257/20000 Training Loss: 0.06865031272172928\n",
      "Epoch 14258/20000 Training Loss: 0.06414817273616791\n",
      "Epoch 14259/20000 Training Loss: 0.041362881660461426\n",
      "Epoch 14260/20000 Training Loss: 0.05231836438179016\n",
      "Epoch 14260/20000 Validation Loss: 0.050315290689468384\n",
      "Epoch 14261/20000 Training Loss: 0.04556598886847496\n",
      "Epoch 14262/20000 Training Loss: 0.06523334980010986\n",
      "Epoch 14263/20000 Training Loss: 0.044515904039144516\n",
      "Epoch 14264/20000 Training Loss: 0.06589175015687943\n",
      "Epoch 14265/20000 Training Loss: 0.051139187067747116\n",
      "Epoch 14266/20000 Training Loss: 0.05326518788933754\n",
      "Epoch 14267/20000 Training Loss: 0.05245901271700859\n",
      "Epoch 14268/20000 Training Loss: 0.05742885544896126\n",
      "Epoch 14269/20000 Training Loss: 0.04379264637827873\n",
      "Epoch 14270/20000 Training Loss: 0.05173547565937042\n",
      "Epoch 14270/20000 Validation Loss: 0.060051627457141876\n",
      "Epoch 14271/20000 Training Loss: 0.0637383759021759\n",
      "Epoch 14272/20000 Training Loss: 0.057792048901319504\n",
      "Epoch 14273/20000 Training Loss: 0.04780590161681175\n",
      "Epoch 14274/20000 Training Loss: 0.0564362108707428\n",
      "Epoch 14275/20000 Training Loss: 0.05102131888270378\n",
      "Epoch 14276/20000 Training Loss: 0.04978048801422119\n",
      "Epoch 14277/20000 Training Loss: 0.05908311903476715\n",
      "Epoch 14278/20000 Training Loss: 0.04519141837954521\n",
      "Epoch 14279/20000 Training Loss: 0.06814942508935928\n",
      "Epoch 14280/20000 Training Loss: 0.04344240948557854\n",
      "Epoch 14280/20000 Validation Loss: 0.07800976186990738\n",
      "Epoch 14281/20000 Training Loss: 0.04316764697432518\n",
      "Epoch 14282/20000 Training Loss: 0.054471928626298904\n",
      "Epoch 14283/20000 Training Loss: 0.04259313642978668\n",
      "Epoch 14284/20000 Training Loss: 0.0486217625439167\n",
      "Epoch 14285/20000 Training Loss: 0.06722793728113174\n",
      "Epoch 14286/20000 Training Loss: 0.04274808242917061\n",
      "Epoch 14287/20000 Training Loss: 0.06149178743362427\n",
      "Epoch 14288/20000 Training Loss: 0.049257636070251465\n",
      "Epoch 14289/20000 Training Loss: 0.06510288268327713\n",
      "Epoch 14290/20000 Training Loss: 0.06738609075546265\n",
      "Epoch 14290/20000 Validation Loss: 0.049400728195905685\n",
      "Epoch 14291/20000 Training Loss: 0.06017453595995903\n",
      "Epoch 14292/20000 Training Loss: 0.05467888340353966\n",
      "Epoch 14293/20000 Training Loss: 0.05572532117366791\n",
      "Epoch 14294/20000 Training Loss: 0.048438575118780136\n",
      "Epoch 14295/20000 Training Loss: 0.03365194797515869\n",
      "Epoch 14296/20000 Training Loss: 0.045886266976594925\n",
      "Epoch 14297/20000 Training Loss: 0.05357831344008446\n",
      "Epoch 14298/20000 Training Loss: 0.05278882011771202\n",
      "Epoch 14299/20000 Training Loss: 0.03875526413321495\n",
      "Epoch 14300/20000 Training Loss: 0.05829654633998871\n",
      "Epoch 14300/20000 Validation Loss: 0.05375546216964722\n",
      "Epoch 14301/20000 Training Loss: 0.06279769539833069\n",
      "Epoch 14302/20000 Training Loss: 0.04661346971988678\n",
      "Epoch 14303/20000 Training Loss: 0.04180963709950447\n",
      "Epoch 14304/20000 Training Loss: 0.061615586280822754\n",
      "Epoch 14305/20000 Training Loss: 0.056887075304985046\n",
      "Epoch 14306/20000 Training Loss: 0.051549822092056274\n",
      "Epoch 14307/20000 Training Loss: 0.049348365515470505\n",
      "Epoch 14308/20000 Training Loss: 0.0638941302895546\n",
      "Epoch 14309/20000 Training Loss: 0.05249524116516113\n",
      "Epoch 14310/20000 Training Loss: 0.05110834166407585\n",
      "Epoch 14310/20000 Validation Loss: 0.041498810052871704\n",
      "Epoch 14311/20000 Training Loss: 0.06061738729476929\n",
      "Epoch 14312/20000 Training Loss: 0.048924047499895096\n",
      "Epoch 14313/20000 Training Loss: 0.047855984419584274\n",
      "Epoch 14314/20000 Training Loss: 0.05740170553326607\n",
      "Epoch 14315/20000 Training Loss: 0.05679881200194359\n",
      "Epoch 14316/20000 Training Loss: 0.06293464452028275\n",
      "Epoch 14317/20000 Training Loss: 0.06317427009344101\n",
      "Epoch 14318/20000 Training Loss: 0.07049200683832169\n",
      "Epoch 14319/20000 Training Loss: 0.06610608845949173\n",
      "Epoch 14320/20000 Training Loss: 0.06143723428249359\n",
      "Epoch 14320/20000 Validation Loss: 0.048747316002845764\n",
      "Epoch 14321/20000 Training Loss: 0.060054581612348557\n",
      "Epoch 14322/20000 Training Loss: 0.04966927692294121\n",
      "Epoch 14323/20000 Training Loss: 0.056059885770082474\n",
      "Epoch 14324/20000 Training Loss: 0.060967933386564255\n",
      "Epoch 14325/20000 Training Loss: 0.05967262387275696\n",
      "Epoch 14326/20000 Training Loss: 0.06531081348657608\n",
      "Epoch 14327/20000 Training Loss: 0.04087134823203087\n",
      "Epoch 14328/20000 Training Loss: 0.060177844017744064\n",
      "Epoch 14329/20000 Training Loss: 0.0668845847249031\n",
      "Epoch 14330/20000 Training Loss: 0.075605608522892\n",
      "Epoch 14330/20000 Validation Loss: 0.05384231358766556\n",
      "Epoch 14331/20000 Training Loss: 0.05663071945309639\n",
      "Epoch 14332/20000 Training Loss: 0.04828549921512604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14333/20000 Training Loss: 0.060731738805770874\n",
      "Epoch 14334/20000 Training Loss: 0.05945435166358948\n",
      "Epoch 14335/20000 Training Loss: 0.05423704907298088\n",
      "Epoch 14336/20000 Training Loss: 0.05160028859972954\n",
      "Epoch 14337/20000 Training Loss: 0.06737985461950302\n",
      "Epoch 14338/20000 Training Loss: 0.04451724514365196\n",
      "Epoch 14339/20000 Training Loss: 0.06321319937705994\n",
      "Epoch 14340/20000 Training Loss: 0.050083328038454056\n",
      "Epoch 14340/20000 Validation Loss: 0.0414106547832489\n",
      "Epoch 14341/20000 Training Loss: 0.07472925633192062\n",
      "Epoch 14342/20000 Training Loss: 0.07879691570997238\n",
      "Epoch 14343/20000 Training Loss: 0.05988796055316925\n",
      "Epoch 14344/20000 Training Loss: 0.04993966594338417\n",
      "Epoch 14345/20000 Training Loss: 0.04607297107577324\n",
      "Epoch 14346/20000 Training Loss: 0.05938947573304176\n",
      "Epoch 14347/20000 Training Loss: 0.054120197892189026\n",
      "Epoch 14348/20000 Training Loss: 0.07323705404996872\n",
      "Epoch 14349/20000 Training Loss: 0.0513811893761158\n",
      "Epoch 14350/20000 Training Loss: 0.05822734534740448\n",
      "Epoch 14350/20000 Validation Loss: 0.08446970582008362\n",
      "Epoch 14351/20000 Training Loss: 0.05147525295615196\n",
      "Epoch 14352/20000 Training Loss: 0.06255729496479034\n",
      "Epoch 14353/20000 Training Loss: 0.051673974841833115\n",
      "Epoch 14354/20000 Training Loss: 0.04044268652796745\n",
      "Epoch 14355/20000 Training Loss: 0.05252261832356453\n",
      "Epoch 14356/20000 Training Loss: 0.035055190324783325\n",
      "Epoch 14357/20000 Training Loss: 0.07001647353172302\n",
      "Epoch 14358/20000 Training Loss: 0.06118813529610634\n",
      "Epoch 14359/20000 Training Loss: 0.05798700824379921\n",
      "Epoch 14360/20000 Training Loss: 0.053039729595184326\n",
      "Epoch 14360/20000 Validation Loss: 0.06762722879648209\n",
      "Epoch 14361/20000 Training Loss: 0.05070182681083679\n",
      "Epoch 14362/20000 Training Loss: 0.04737556353211403\n",
      "Epoch 14363/20000 Training Loss: 0.04840964078903198\n",
      "Epoch 14364/20000 Training Loss: 0.06131776049733162\n",
      "Epoch 14365/20000 Training Loss: 0.059626996517181396\n",
      "Epoch 14366/20000 Training Loss: 0.05502213165163994\n",
      "Epoch 14367/20000 Training Loss: 0.05545112490653992\n",
      "Epoch 14368/20000 Training Loss: 0.05323980376124382\n",
      "Epoch 14369/20000 Training Loss: 0.04784691706299782\n",
      "Epoch 14370/20000 Training Loss: 0.07567958533763885\n",
      "Epoch 14370/20000 Validation Loss: 0.055319152772426605\n",
      "Epoch 14371/20000 Training Loss: 0.06027116999030113\n",
      "Epoch 14372/20000 Training Loss: 0.05876306816935539\n",
      "Epoch 14373/20000 Training Loss: 0.04446549713611603\n",
      "Epoch 14374/20000 Training Loss: 0.045029785484075546\n",
      "Epoch 14375/20000 Training Loss: 0.048779457807540894\n",
      "Epoch 14376/20000 Training Loss: 0.05688795447349548\n",
      "Epoch 14377/20000 Training Loss: 0.0579020120203495\n",
      "Epoch 14378/20000 Training Loss: 0.07067680358886719\n",
      "Epoch 14379/20000 Training Loss: 0.039622217416763306\n",
      "Epoch 14380/20000 Training Loss: 0.057507485151290894\n",
      "Epoch 14380/20000 Validation Loss: 0.048089392483234406\n",
      "Epoch 14381/20000 Training Loss: 0.06952110677957535\n",
      "Epoch 14382/20000 Training Loss: 0.0711338073015213\n",
      "Epoch 14383/20000 Training Loss: 0.061935778707265854\n",
      "Epoch 14384/20000 Training Loss: 0.05525079742074013\n",
      "Epoch 14385/20000 Training Loss: 0.05503373220562935\n",
      "Epoch 14386/20000 Training Loss: 0.05233175680041313\n",
      "Epoch 14387/20000 Training Loss: 0.08454853296279907\n",
      "Epoch 14388/20000 Training Loss: 0.059488099068403244\n",
      "Epoch 14389/20000 Training Loss: 0.05715420842170715\n",
      "Epoch 14390/20000 Training Loss: 0.05277205631136894\n",
      "Epoch 14390/20000 Validation Loss: 0.08831978589296341\n",
      "Epoch 14391/20000 Training Loss: 0.04376335069537163\n",
      "Epoch 14392/20000 Training Loss: 0.062422603368759155\n",
      "Epoch 14393/20000 Training Loss: 0.046363502740859985\n",
      "Epoch 14394/20000 Training Loss: 0.04232165589928627\n",
      "Epoch 14395/20000 Training Loss: 0.07107152789831161\n",
      "Epoch 14396/20000 Training Loss: 0.05286143347620964\n",
      "Epoch 14397/20000 Training Loss: 0.04099714756011963\n",
      "Epoch 14398/20000 Training Loss: 0.05903465673327446\n",
      "Epoch 14399/20000 Training Loss: 0.05616002902388573\n",
      "Epoch 14400/20000 Training Loss: 0.060709644109010696\n",
      "Epoch 14400/20000 Validation Loss: 0.05349944531917572\n",
      "Epoch 14401/20000 Training Loss: 0.05209408327937126\n",
      "Epoch 14402/20000 Training Loss: 0.056492384523153305\n",
      "Epoch 14403/20000 Training Loss: 0.05810504034161568\n",
      "Epoch 14404/20000 Training Loss: 0.04454263672232628\n",
      "Epoch 14405/20000 Training Loss: 0.05657908320426941\n",
      "Epoch 14406/20000 Training Loss: 0.058307480067014694\n",
      "Epoch 14407/20000 Training Loss: 0.06233711540699005\n",
      "Epoch 14408/20000 Training Loss: 0.058005839586257935\n",
      "Epoch 14409/20000 Training Loss: 0.05636715888977051\n",
      "Epoch 14410/20000 Training Loss: 0.05148138105869293\n",
      "Epoch 14410/20000 Validation Loss: 0.06808675080537796\n",
      "Epoch 14411/20000 Training Loss: 0.038804564625024796\n",
      "Epoch 14412/20000 Training Loss: 0.05039632320404053\n",
      "Epoch 14413/20000 Training Loss: 0.0496293306350708\n",
      "Epoch 14414/20000 Training Loss: 0.06166638806462288\n",
      "Epoch 14415/20000 Training Loss: 0.04532122239470482\n",
      "Epoch 14416/20000 Training Loss: 0.043205294758081436\n",
      "Epoch 14417/20000 Training Loss: 0.08005240559577942\n",
      "Epoch 14418/20000 Training Loss: 0.055145133286714554\n",
      "Epoch 14419/20000 Training Loss: 0.056029532104730606\n",
      "Epoch 14420/20000 Training Loss: 0.045759424567222595\n",
      "Epoch 14420/20000 Validation Loss: 0.05383510887622833\n",
      "Epoch 14421/20000 Training Loss: 0.048778582364320755\n",
      "Epoch 14422/20000 Training Loss: 0.03582458570599556\n",
      "Epoch 14423/20000 Training Loss: 0.05792272090911865\n",
      "Epoch 14424/20000 Training Loss: 0.041579388082027435\n",
      "Epoch 14425/20000 Training Loss: 0.05482739582657814\n",
      "Epoch 14426/20000 Training Loss: 0.04246716573834419\n",
      "Epoch 14427/20000 Training Loss: 0.06635506451129913\n",
      "Epoch 14428/20000 Training Loss: 0.05401502922177315\n",
      "Epoch 14429/20000 Training Loss: 0.03499342128634453\n",
      "Epoch 14430/20000 Training Loss: 0.06398074328899384\n",
      "Epoch 14430/20000 Validation Loss: 0.05039513111114502\n",
      "Epoch 14431/20000 Training Loss: 0.05859820172190666\n",
      "Epoch 14432/20000 Training Loss: 0.048398543149232864\n",
      "Epoch 14433/20000 Training Loss: 0.04806322976946831\n",
      "Epoch 14434/20000 Training Loss: 0.051303934305906296\n",
      "Epoch 14435/20000 Training Loss: 0.040584880858659744\n",
      "Epoch 14436/20000 Training Loss: 0.0492648147046566\n",
      "Epoch 14437/20000 Training Loss: 0.051463376730680466\n",
      "Epoch 14438/20000 Training Loss: 0.045118894428014755\n",
      "Epoch 14439/20000 Training Loss: 0.059078216552734375\n",
      "Epoch 14440/20000 Training Loss: 0.05164201557636261\n",
      "Epoch 14440/20000 Validation Loss: 0.046072378754615784\n",
      "Epoch 14441/20000 Training Loss: 0.04587514325976372\n",
      "Epoch 14442/20000 Training Loss: 0.043525174260139465\n",
      "Epoch 14443/20000 Training Loss: 0.05118195340037346\n",
      "Epoch 14444/20000 Training Loss: 0.04289543256163597\n",
      "Epoch 14445/20000 Training Loss: 0.06535586714744568\n",
      "Epoch 14446/20000 Training Loss: 0.05393010377883911\n",
      "Epoch 14447/20000 Training Loss: 0.06412387639284134\n",
      "Epoch 14448/20000 Training Loss: 0.05670197680592537\n",
      "Epoch 14449/20000 Training Loss: 0.058369915932416916\n",
      "Epoch 14450/20000 Training Loss: 0.04708261787891388\n",
      "Epoch 14450/20000 Validation Loss: 0.05735138803720474\n",
      "Epoch 14451/20000 Training Loss: 0.05630039796233177\n",
      "Epoch 14452/20000 Training Loss: 0.07361317425966263\n",
      "Epoch 14453/20000 Training Loss: 0.05007092282176018\n",
      "Epoch 14454/20000 Training Loss: 0.044808417558670044\n",
      "Epoch 14455/20000 Training Loss: 0.047308336943387985\n",
      "Epoch 14456/20000 Training Loss: 0.04513044282793999\n",
      "Epoch 14457/20000 Training Loss: 0.05773353576660156\n",
      "Epoch 14458/20000 Training Loss: 0.046076495200395584\n",
      "Epoch 14459/20000 Training Loss: 0.07565963268280029\n",
      "Epoch 14460/20000 Training Loss: 0.06815539300441742\n",
      "Epoch 14460/20000 Validation Loss: 0.08811456710100174\n",
      "Epoch 14461/20000 Training Loss: 0.06527665257453918\n",
      "Epoch 14462/20000 Training Loss: 0.04002281650900841\n",
      "Epoch 14463/20000 Training Loss: 0.04947024583816528\n",
      "Epoch 14464/20000 Training Loss: 0.05794902518391609\n",
      "Epoch 14465/20000 Training Loss: 0.05965046212077141\n",
      "Epoch 14466/20000 Training Loss: 0.05290776491165161\n",
      "Epoch 14467/20000 Training Loss: 0.054167985916137695\n",
      "Epoch 14468/20000 Training Loss: 0.04648837819695473\n",
      "Epoch 14469/20000 Training Loss: 0.05877143144607544\n",
      "Epoch 14470/20000 Training Loss: 0.050668101757764816\n",
      "Epoch 14470/20000 Validation Loss: 0.05207005888223648\n",
      "Epoch 14471/20000 Training Loss: 0.0492837131023407\n",
      "Epoch 14472/20000 Training Loss: 0.048016030341386795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14473/20000 Training Loss: 0.0567520447075367\n",
      "Epoch 14474/20000 Training Loss: 0.05667649582028389\n",
      "Epoch 14475/20000 Training Loss: 0.061393264681100845\n",
      "Epoch 14476/20000 Training Loss: 0.04904569312930107\n",
      "Epoch 14477/20000 Training Loss: 0.053270015865564346\n",
      "Epoch 14478/20000 Training Loss: 0.057060301303863525\n",
      "Epoch 14479/20000 Training Loss: 0.04921853169798851\n",
      "Epoch 14480/20000 Training Loss: 0.054388418793678284\n",
      "Epoch 14480/20000 Validation Loss: 0.06573687493801117\n",
      "Epoch 14481/20000 Training Loss: 0.0421772263944149\n",
      "Epoch 14482/20000 Training Loss: 0.045016441494226456\n",
      "Epoch 14483/20000 Training Loss: 0.061657726764678955\n",
      "Epoch 14484/20000 Training Loss: 0.04750525578856468\n",
      "Epoch 14485/20000 Training Loss: 0.0590248741209507\n",
      "Epoch 14486/20000 Training Loss: 0.054442211985588074\n",
      "Epoch 14487/20000 Training Loss: 0.03739696368575096\n",
      "Epoch 14488/20000 Training Loss: 0.04508264735341072\n",
      "Epoch 14489/20000 Training Loss: 0.05636033043265343\n",
      "Epoch 14490/20000 Training Loss: 0.06499757617712021\n",
      "Epoch 14490/20000 Validation Loss: 0.06492841243743896\n",
      "Epoch 14491/20000 Training Loss: 0.054302554577589035\n",
      "Epoch 14492/20000 Training Loss: 0.04611901566386223\n",
      "Epoch 14493/20000 Training Loss: 0.051813945174217224\n",
      "Epoch 14494/20000 Training Loss: 0.05694374442100525\n",
      "Epoch 14495/20000 Training Loss: 0.051967743784189224\n",
      "Epoch 14496/20000 Training Loss: 0.04678258299827576\n",
      "Epoch 14497/20000 Training Loss: 0.06551727652549744\n",
      "Epoch 14498/20000 Training Loss: 0.07261787354946136\n",
      "Epoch 14499/20000 Training Loss: 0.06214522942900658\n",
      "Epoch 14500/20000 Training Loss: 0.05419619381427765\n",
      "Epoch 14500/20000 Validation Loss: 0.05763150751590729\n",
      "Epoch 14501/20000 Training Loss: 0.0610082745552063\n",
      "Epoch 14502/20000 Training Loss: 0.04531269893050194\n",
      "Epoch 14503/20000 Training Loss: 0.04597705602645874\n",
      "Epoch 14504/20000 Training Loss: 0.06970325857400894\n",
      "Epoch 14505/20000 Training Loss: 0.05617504194378853\n",
      "Epoch 14506/20000 Training Loss: 0.049768541008234024\n",
      "Epoch 14507/20000 Training Loss: 0.043068572878837585\n",
      "Epoch 14508/20000 Training Loss: 0.05495036020874977\n",
      "Epoch 14509/20000 Training Loss: 0.05017869547009468\n",
      "Epoch 14510/20000 Training Loss: 0.07724221795797348\n",
      "Epoch 14510/20000 Validation Loss: 0.046858303248882294\n",
      "Epoch 14511/20000 Training Loss: 0.0511438250541687\n",
      "Epoch 14512/20000 Training Loss: 0.06360233575105667\n",
      "Epoch 14513/20000 Training Loss: 0.042796436697244644\n",
      "Epoch 14514/20000 Training Loss: 0.06341997534036636\n",
      "Epoch 14515/20000 Training Loss: 0.06449474394321442\n",
      "Epoch 14516/20000 Training Loss: 0.055446892976760864\n",
      "Epoch 14517/20000 Training Loss: 0.04973481595516205\n",
      "Epoch 14518/20000 Training Loss: 0.05362049862742424\n",
      "Epoch 14519/20000 Training Loss: 0.05175286531448364\n",
      "Epoch 14520/20000 Training Loss: 0.046154867857694626\n",
      "Epoch 14520/20000 Validation Loss: 0.06449893116950989\n",
      "Epoch 14521/20000 Training Loss: 0.07557463645935059\n",
      "Epoch 14522/20000 Training Loss: 0.04893326759338379\n",
      "Epoch 14523/20000 Training Loss: 0.044327449053525925\n",
      "Epoch 14524/20000 Training Loss: 0.058782998472452164\n",
      "Epoch 14525/20000 Training Loss: 0.05819053575396538\n",
      "Epoch 14526/20000 Training Loss: 0.05128061771392822\n",
      "Epoch 14527/20000 Training Loss: 0.04703840613365173\n",
      "Epoch 14528/20000 Training Loss: 0.048376116901636124\n",
      "Epoch 14529/20000 Training Loss: 0.04514226317405701\n",
      "Epoch 14530/20000 Training Loss: 0.0714581236243248\n",
      "Epoch 14530/20000 Validation Loss: 0.04411168396472931\n",
      "Epoch 14531/20000 Training Loss: 0.04946296289563179\n",
      "Epoch 14532/20000 Training Loss: 0.04581255838274956\n",
      "Epoch 14533/20000 Training Loss: 0.05414145067334175\n",
      "Epoch 14534/20000 Training Loss: 0.04050188511610031\n",
      "Epoch 14535/20000 Training Loss: 0.05808702111244202\n",
      "Epoch 14536/20000 Training Loss: 0.07951503247022629\n",
      "Epoch 14537/20000 Training Loss: 0.05060523748397827\n",
      "Epoch 14538/20000 Training Loss: 0.06251339614391327\n",
      "Epoch 14539/20000 Training Loss: 0.05387211963534355\n",
      "Epoch 14540/20000 Training Loss: 0.043760016560554504\n",
      "Epoch 14540/20000 Validation Loss: 0.049578212201595306\n",
      "Epoch 14541/20000 Training Loss: 0.046038269996643066\n",
      "Epoch 14542/20000 Training Loss: 0.046237897127866745\n",
      "Epoch 14543/20000 Training Loss: 0.03909558430314064\n",
      "Epoch 14544/20000 Training Loss: 0.05319709703326225\n",
      "Epoch 14545/20000 Training Loss: 0.051666710525751114\n",
      "Epoch 14546/20000 Training Loss: 0.048235934227705\n",
      "Epoch 14547/20000 Training Loss: 0.047338616102933884\n",
      "Epoch 14548/20000 Training Loss: 0.046983879059553146\n",
      "Epoch 14549/20000 Training Loss: 0.035750728100538254\n",
      "Epoch 14550/20000 Training Loss: 0.04907744750380516\n",
      "Epoch 14550/20000 Validation Loss: 0.05341608077287674\n",
      "Epoch 14551/20000 Training Loss: 0.054560597985982895\n",
      "Epoch 14552/20000 Training Loss: 0.04926609992980957\n",
      "Epoch 14553/20000 Training Loss: 0.05942953750491142\n",
      "Epoch 14554/20000 Training Loss: 0.07449374347925186\n",
      "Epoch 14555/20000 Training Loss: 0.06931506842374802\n",
      "Epoch 14556/20000 Training Loss: 0.05639970302581787\n",
      "Epoch 14557/20000 Training Loss: 0.05260934308171272\n",
      "Epoch 14558/20000 Training Loss: 0.052622634917497635\n",
      "Epoch 14559/20000 Training Loss: 0.05040256306529045\n",
      "Epoch 14560/20000 Training Loss: 0.058407124131917953\n",
      "Epoch 14560/20000 Validation Loss: 0.04890426993370056\n",
      "Epoch 14561/20000 Training Loss: 0.04779006168246269\n",
      "Epoch 14562/20000 Training Loss: 0.03774834796786308\n",
      "Epoch 14563/20000 Training Loss: 0.06135310232639313\n",
      "Epoch 14564/20000 Training Loss: 0.06723430752754211\n",
      "Epoch 14565/20000 Training Loss: 0.05324220284819603\n",
      "Epoch 14566/20000 Training Loss: 0.06217844411730766\n",
      "Epoch 14567/20000 Training Loss: 0.0426434725522995\n",
      "Epoch 14568/20000 Training Loss: 0.03947576880455017\n",
      "Epoch 14569/20000 Training Loss: 0.05603012070059776\n",
      "Epoch 14570/20000 Training Loss: 0.07006589323282242\n",
      "Epoch 14570/20000 Validation Loss: 0.06657212972640991\n",
      "Epoch 14571/20000 Training Loss: 0.0380760096013546\n",
      "Epoch 14572/20000 Training Loss: 0.058083560317754745\n",
      "Epoch 14573/20000 Training Loss: 0.04260224103927612\n",
      "Epoch 14574/20000 Training Loss: 0.055412422865629196\n",
      "Epoch 14575/20000 Training Loss: 0.05504269525408745\n",
      "Epoch 14576/20000 Training Loss: 0.048726886510849\n",
      "Epoch 14577/20000 Training Loss: 0.048060640692710876\n",
      "Epoch 14578/20000 Training Loss: 0.05244077369570732\n",
      "Epoch 14579/20000 Training Loss: 0.040894512087106705\n",
      "Epoch 14580/20000 Training Loss: 0.07488106936216354\n",
      "Epoch 14580/20000 Validation Loss: 0.061367474496364594\n",
      "Epoch 14581/20000 Training Loss: 0.06119425967335701\n",
      "Epoch 14582/20000 Training Loss: 0.06403005123138428\n",
      "Epoch 14583/20000 Training Loss: 0.06318048387765884\n",
      "Epoch 14584/20000 Training Loss: 0.061913520097732544\n",
      "Epoch 14585/20000 Training Loss: 0.05919533967971802\n",
      "Epoch 14586/20000 Training Loss: 0.05932892486453056\n",
      "Epoch 14587/20000 Training Loss: 0.07469170540571213\n",
      "Epoch 14588/20000 Training Loss: 0.05316254496574402\n",
      "Epoch 14589/20000 Training Loss: 0.04311467334628105\n",
      "Epoch 14590/20000 Training Loss: 0.04773198068141937\n",
      "Epoch 14590/20000 Validation Loss: 0.05679736286401749\n",
      "Epoch 14591/20000 Training Loss: 0.05134415626525879\n",
      "Epoch 14592/20000 Training Loss: 0.08096200227737427\n",
      "Epoch 14593/20000 Training Loss: 0.061908502131700516\n",
      "Epoch 14594/20000 Training Loss: 0.047987427562475204\n",
      "Epoch 14595/20000 Training Loss: 0.0579938180744648\n",
      "Epoch 14596/20000 Training Loss: 0.055462103337049484\n",
      "Epoch 14597/20000 Training Loss: 0.04727930203080177\n",
      "Epoch 14598/20000 Training Loss: 0.04798303544521332\n",
      "Epoch 14599/20000 Training Loss: 0.06243498995900154\n",
      "Epoch 14600/20000 Training Loss: 0.06080247461795807\n",
      "Epoch 14600/20000 Validation Loss: 0.058951638638973236\n",
      "Epoch 14601/20000 Training Loss: 0.04543787240982056\n",
      "Epoch 14602/20000 Training Loss: 0.05667586997151375\n",
      "Epoch 14603/20000 Training Loss: 0.057987481355667114\n",
      "Epoch 14604/20000 Training Loss: 0.057357292622327805\n",
      "Epoch 14605/20000 Training Loss: 0.04229028522968292\n",
      "Epoch 14606/20000 Training Loss: 0.04563131928443909\n",
      "Epoch 14607/20000 Training Loss: 0.0508340559899807\n",
      "Epoch 14608/20000 Training Loss: 0.06649025529623032\n",
      "Epoch 14609/20000 Training Loss: 0.05304403230547905\n",
      "Epoch 14610/20000 Training Loss: 0.05067026987671852\n",
      "Epoch 14610/20000 Validation Loss: 0.06090052053332329\n",
      "Epoch 14611/20000 Training Loss: 0.05317085608839989\n",
      "Epoch 14612/20000 Training Loss: 0.07451558858156204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14613/20000 Training Loss: 0.07409662008285522\n",
      "Epoch 14614/20000 Training Loss: 0.061955541372299194\n",
      "Epoch 14615/20000 Training Loss: 0.05537596344947815\n",
      "Epoch 14616/20000 Training Loss: 0.056331876665353775\n",
      "Epoch 14617/20000 Training Loss: 0.04350673034787178\n",
      "Epoch 14618/20000 Training Loss: 0.04269760847091675\n",
      "Epoch 14619/20000 Training Loss: 0.047114551067352295\n",
      "Epoch 14620/20000 Training Loss: 0.051947902888059616\n",
      "Epoch 14620/20000 Validation Loss: 0.08476647734642029\n",
      "Epoch 14621/20000 Training Loss: 0.0628686249256134\n",
      "Epoch 14622/20000 Training Loss: 0.05211761221289635\n",
      "Epoch 14623/20000 Training Loss: 0.05363793298602104\n",
      "Epoch 14624/20000 Training Loss: 0.05971536040306091\n",
      "Epoch 14625/20000 Training Loss: 0.04978279396891594\n",
      "Epoch 14626/20000 Training Loss: 0.05271626636385918\n",
      "Epoch 14627/20000 Training Loss: 0.054198652505874634\n",
      "Epoch 14628/20000 Training Loss: 0.045867279171943665\n",
      "Epoch 14629/20000 Training Loss: 0.03556129336357117\n",
      "Epoch 14630/20000 Training Loss: 0.057826895266771317\n",
      "Epoch 14630/20000 Validation Loss: 0.062385208904743195\n",
      "Epoch 14631/20000 Training Loss: 0.053629156202077866\n",
      "Epoch 14632/20000 Training Loss: 0.04945921525359154\n",
      "Epoch 14633/20000 Training Loss: 0.052727002650499344\n",
      "Epoch 14634/20000 Training Loss: 0.058481018990278244\n",
      "Epoch 14635/20000 Training Loss: 0.06364871561527252\n",
      "Epoch 14636/20000 Training Loss: 0.05175888538360596\n",
      "Epoch 14637/20000 Training Loss: 0.06855709105730057\n",
      "Epoch 14638/20000 Training Loss: 0.07963506132364273\n",
      "Epoch 14639/20000 Training Loss: 0.05340952053666115\n",
      "Epoch 14640/20000 Training Loss: 0.061421703547239304\n",
      "Epoch 14640/20000 Validation Loss: 0.06151120364665985\n",
      "Epoch 14641/20000 Training Loss: 0.05547076463699341\n",
      "Epoch 14642/20000 Training Loss: 0.04222665727138519\n",
      "Epoch 14643/20000 Training Loss: 0.08466505259275436\n",
      "Epoch 14644/20000 Training Loss: 0.049399007111787796\n",
      "Epoch 14645/20000 Training Loss: 0.056740835309028625\n",
      "Epoch 14646/20000 Training Loss: 0.0631459429860115\n",
      "Epoch 14647/20000 Training Loss: 0.052794843912124634\n",
      "Epoch 14648/20000 Training Loss: 0.0429375022649765\n",
      "Epoch 14649/20000 Training Loss: 0.05354757606983185\n",
      "Epoch 14650/20000 Training Loss: 0.05202217400074005\n",
      "Epoch 14650/20000 Validation Loss: 0.06641025096178055\n",
      "Epoch 14651/20000 Training Loss: 0.044063370674848557\n",
      "Epoch 14652/20000 Training Loss: 0.05400201305747032\n",
      "Epoch 14653/20000 Training Loss: 0.045932602137327194\n",
      "Epoch 14654/20000 Training Loss: 0.05507872626185417\n",
      "Epoch 14655/20000 Training Loss: 0.0641218051314354\n",
      "Epoch 14656/20000 Training Loss: 0.04597271978855133\n",
      "Epoch 14657/20000 Training Loss: 0.05392466112971306\n",
      "Epoch 14658/20000 Training Loss: 0.05058910325169563\n",
      "Epoch 14659/20000 Training Loss: 0.07127010077238083\n",
      "Epoch 14660/20000 Training Loss: 0.06489671021699905\n",
      "Epoch 14660/20000 Validation Loss: 0.06628676503896713\n",
      "Epoch 14661/20000 Training Loss: 0.06800905615091324\n",
      "Epoch 14662/20000 Training Loss: 0.06482530385255814\n",
      "Epoch 14663/20000 Training Loss: 0.0491274856030941\n",
      "Epoch 14664/20000 Training Loss: 0.046127039939165115\n",
      "Epoch 14665/20000 Training Loss: 0.044054772704839706\n",
      "Epoch 14666/20000 Training Loss: 0.05798722431063652\n",
      "Epoch 14667/20000 Training Loss: 0.06516201049089432\n",
      "Epoch 14668/20000 Training Loss: 0.0454954095184803\n",
      "Epoch 14669/20000 Training Loss: 0.07570625841617584\n",
      "Epoch 14670/20000 Training Loss: 0.051477815955877304\n",
      "Epoch 14670/20000 Validation Loss: 0.057120952755212784\n",
      "Epoch 14671/20000 Training Loss: 0.06314444541931152\n",
      "Epoch 14672/20000 Training Loss: 0.052782583981752396\n",
      "Epoch 14673/20000 Training Loss: 0.05074566975235939\n",
      "Epoch 14674/20000 Training Loss: 0.0491669587790966\n",
      "Epoch 14675/20000 Training Loss: 0.053863923996686935\n",
      "Epoch 14676/20000 Training Loss: 0.045645128935575485\n",
      "Epoch 14677/20000 Training Loss: 0.043671831488609314\n",
      "Epoch 14678/20000 Training Loss: 0.0454195998609066\n",
      "Epoch 14679/20000 Training Loss: 0.0521228052675724\n",
      "Epoch 14680/20000 Training Loss: 0.04398049786686897\n",
      "Epoch 14680/20000 Validation Loss: 0.05321422219276428\n",
      "Epoch 14681/20000 Training Loss: 0.04868534579873085\n",
      "Epoch 14682/20000 Training Loss: 0.04989059641957283\n",
      "Epoch 14683/20000 Training Loss: 0.05629666522145271\n",
      "Epoch 14684/20000 Training Loss: 0.05214814469218254\n",
      "Epoch 14685/20000 Training Loss: 0.052581965923309326\n",
      "Epoch 14686/20000 Training Loss: 0.045049045234918594\n",
      "Epoch 14687/20000 Training Loss: 0.055335987359285355\n",
      "Epoch 14688/20000 Training Loss: 0.049032289534807205\n",
      "Epoch 14689/20000 Training Loss: 0.049949873238801956\n",
      "Epoch 14690/20000 Training Loss: 0.036321550607681274\n",
      "Epoch 14690/20000 Validation Loss: 0.06806827336549759\n",
      "Epoch 14691/20000 Training Loss: 0.059026703238487244\n",
      "Epoch 14692/20000 Training Loss: 0.04899643734097481\n",
      "Epoch 14693/20000 Training Loss: 0.043962281197309494\n",
      "Epoch 14694/20000 Training Loss: 0.058193620294332504\n",
      "Epoch 14695/20000 Training Loss: 0.06136442348361015\n",
      "Epoch 14696/20000 Training Loss: 0.060197386890649796\n",
      "Epoch 14697/20000 Training Loss: 0.04663799703121185\n",
      "Epoch 14698/20000 Training Loss: 0.0588628351688385\n",
      "Epoch 14699/20000 Training Loss: 0.06353368610143661\n",
      "Epoch 14700/20000 Training Loss: 0.06511671096086502\n",
      "Epoch 14700/20000 Validation Loss: 0.042382970452308655\n",
      "Epoch 14701/20000 Training Loss: 0.04893893375992775\n",
      "Epoch 14702/20000 Training Loss: 0.05516405031085014\n",
      "Epoch 14703/20000 Training Loss: 0.05623845383524895\n",
      "Epoch 14704/20000 Training Loss: 0.04118645563721657\n",
      "Epoch 14705/20000 Training Loss: 0.0690818727016449\n",
      "Epoch 14706/20000 Training Loss: 0.05134391784667969\n",
      "Epoch 14707/20000 Training Loss: 0.043271470814943314\n",
      "Epoch 14708/20000 Training Loss: 0.06997948884963989\n",
      "Epoch 14709/20000 Training Loss: 0.050494223833084106\n",
      "Epoch 14710/20000 Training Loss: 0.06696844100952148\n",
      "Epoch 14710/20000 Validation Loss: 0.053992293775081635\n",
      "Epoch 14711/20000 Training Loss: 0.044016171246767044\n",
      "Epoch 14712/20000 Training Loss: 0.06031492352485657\n",
      "Epoch 14713/20000 Training Loss: 0.051656316965818405\n",
      "Epoch 14714/20000 Training Loss: 0.058825600892305374\n",
      "Epoch 14715/20000 Training Loss: 0.059255924075841904\n",
      "Epoch 14716/20000 Training Loss: 0.06245455518364906\n",
      "Epoch 14717/20000 Training Loss: 0.05439389869570732\n",
      "Epoch 14718/20000 Training Loss: 0.056890975683927536\n",
      "Epoch 14719/20000 Training Loss: 0.05968122556805611\n",
      "Epoch 14720/20000 Training Loss: 0.04524184763431549\n",
      "Epoch 14720/20000 Validation Loss: 0.06461453437805176\n",
      "Epoch 14721/20000 Training Loss: 0.06536515802145004\n",
      "Epoch 14722/20000 Training Loss: 0.05149368569254875\n",
      "Epoch 14723/20000 Training Loss: 0.057956527918577194\n",
      "Epoch 14724/20000 Training Loss: 0.05791735649108887\n",
      "Epoch 14725/20000 Training Loss: 0.04559146240353584\n",
      "Epoch 14726/20000 Training Loss: 0.0443364717066288\n",
      "Epoch 14727/20000 Training Loss: 0.06477009505033493\n",
      "Epoch 14728/20000 Training Loss: 0.049735333770513535\n",
      "Epoch 14729/20000 Training Loss: 0.037689581513404846\n",
      "Epoch 14730/20000 Training Loss: 0.04325856268405914\n",
      "Epoch 14730/20000 Validation Loss: 0.07138437777757645\n",
      "Epoch 14731/20000 Training Loss: 0.057698991149663925\n",
      "Epoch 14732/20000 Training Loss: 0.06769570708274841\n",
      "Epoch 14733/20000 Training Loss: 0.04503655433654785\n",
      "Epoch 14734/20000 Training Loss: 0.042216088622808456\n",
      "Epoch 14735/20000 Training Loss: 0.054855022579431534\n",
      "Epoch 14736/20000 Training Loss: 0.052244339138269424\n",
      "Epoch 14737/20000 Training Loss: 0.04627262055873871\n",
      "Epoch 14738/20000 Training Loss: 0.054417628794908524\n",
      "Epoch 14739/20000 Training Loss: 0.04978736862540245\n",
      "Epoch 14740/20000 Training Loss: 0.05115213990211487\n",
      "Epoch 14740/20000 Validation Loss: 0.048079103231430054\n",
      "Epoch 14741/20000 Training Loss: 0.05618599057197571\n",
      "Epoch 14742/20000 Training Loss: 0.04692324623465538\n",
      "Epoch 14743/20000 Training Loss: 0.04327214136719704\n",
      "Epoch 14744/20000 Training Loss: 0.045097094029188156\n",
      "Epoch 14745/20000 Training Loss: 0.04352111741900444\n",
      "Epoch 14746/20000 Training Loss: 0.04724466800689697\n",
      "Epoch 14747/20000 Training Loss: 0.04499430954456329\n",
      "Epoch 14748/20000 Training Loss: 0.05911541357636452\n",
      "Epoch 14749/20000 Training Loss: 0.044882774353027344\n",
      "Epoch 14750/20000 Training Loss: 0.051709745079278946\n",
      "Epoch 14750/20000 Validation Loss: 0.05301879718899727\n",
      "Epoch 14751/20000 Training Loss: 0.045747388154268265\n",
      "Epoch 14752/20000 Training Loss: 0.051873285323381424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14753/20000 Training Loss: 0.046251554042100906\n",
      "Epoch 14754/20000 Training Loss: 0.06004483625292778\n",
      "Epoch 14755/20000 Training Loss: 0.04180268570780754\n",
      "Epoch 14756/20000 Training Loss: 0.040720079094171524\n",
      "Epoch 14757/20000 Training Loss: 0.054160118103027344\n",
      "Epoch 14758/20000 Training Loss: 0.06642013043165207\n",
      "Epoch 14759/20000 Training Loss: 0.056371886283159256\n",
      "Epoch 14760/20000 Training Loss: 0.043209198862314224\n",
      "Epoch 14760/20000 Validation Loss: 0.05355504900217056\n",
      "Epoch 14761/20000 Training Loss: 0.060235440731048584\n",
      "Epoch 14762/20000 Training Loss: 0.04647475481033325\n",
      "Epoch 14763/20000 Training Loss: 0.06751158088445663\n",
      "Epoch 14764/20000 Training Loss: 0.037784308195114136\n",
      "Epoch 14765/20000 Training Loss: 0.05162707343697548\n",
      "Epoch 14766/20000 Training Loss: 0.069947250187397\n",
      "Epoch 14767/20000 Training Loss: 0.048553284257650375\n",
      "Epoch 14768/20000 Training Loss: 0.041193146258592606\n",
      "Epoch 14769/20000 Training Loss: 0.06183461472392082\n",
      "Epoch 14770/20000 Training Loss: 0.04197901487350464\n",
      "Epoch 14770/20000 Validation Loss: 0.04692566767334938\n",
      "Epoch 14771/20000 Training Loss: 0.0610373318195343\n",
      "Epoch 14772/20000 Training Loss: 0.04468774423003197\n",
      "Epoch 14773/20000 Training Loss: 0.060792189091444016\n",
      "Epoch 14774/20000 Training Loss: 0.04470125958323479\n",
      "Epoch 14775/20000 Training Loss: 0.05032284930348396\n",
      "Epoch 14776/20000 Training Loss: 0.0600065253674984\n",
      "Epoch 14777/20000 Training Loss: 0.03850792348384857\n",
      "Epoch 14778/20000 Training Loss: 0.055735986679792404\n",
      "Epoch 14779/20000 Training Loss: 0.04841986671090126\n",
      "Epoch 14780/20000 Training Loss: 0.05598258972167969\n",
      "Epoch 14780/20000 Validation Loss: 0.06703773140907288\n",
      "Epoch 14781/20000 Training Loss: 0.04948795959353447\n",
      "Epoch 14782/20000 Training Loss: 0.06131509318947792\n",
      "Epoch 14783/20000 Training Loss: 0.04745829105377197\n",
      "Epoch 14784/20000 Training Loss: 0.05778980627655983\n",
      "Epoch 14785/20000 Training Loss: 0.04547201097011566\n",
      "Epoch 14786/20000 Training Loss: 0.04949549213051796\n",
      "Epoch 14787/20000 Training Loss: 0.0654025450348854\n",
      "Epoch 14788/20000 Training Loss: 0.07245390862226486\n",
      "Epoch 14789/20000 Training Loss: 0.048035621643066406\n",
      "Epoch 14790/20000 Training Loss: 0.05365821719169617\n",
      "Epoch 14790/20000 Validation Loss: 0.05458321422338486\n",
      "Epoch 14791/20000 Training Loss: 0.06281336396932602\n",
      "Epoch 14792/20000 Training Loss: 0.04546205326914787\n",
      "Epoch 14793/20000 Training Loss: 0.04330110177397728\n",
      "Epoch 14794/20000 Training Loss: 0.05455806851387024\n",
      "Epoch 14795/20000 Training Loss: 0.05091716721653938\n",
      "Epoch 14796/20000 Training Loss: 0.06418575346469879\n",
      "Epoch 14797/20000 Training Loss: 0.05445556342601776\n",
      "Epoch 14798/20000 Training Loss: 0.044762756675481796\n",
      "Epoch 14799/20000 Training Loss: 0.057555001229047775\n",
      "Epoch 14800/20000 Training Loss: 0.056170981377363205\n",
      "Epoch 14800/20000 Validation Loss: 0.04250402748584747\n",
      "Epoch 14801/20000 Training Loss: 0.04750528931617737\n",
      "Epoch 14802/20000 Training Loss: 0.0569634735584259\n",
      "Epoch 14803/20000 Training Loss: 0.04934407398104668\n",
      "Epoch 14804/20000 Training Loss: 0.06181344389915466\n",
      "Epoch 14805/20000 Training Loss: 0.057820141315460205\n",
      "Epoch 14806/20000 Training Loss: 0.06538549065589905\n",
      "Epoch 14807/20000 Training Loss: 0.0567023791372776\n",
      "Epoch 14808/20000 Training Loss: 0.05184013769030571\n",
      "Epoch 14809/20000 Training Loss: 0.04559165611863136\n",
      "Epoch 14810/20000 Training Loss: 0.05834111571311951\n",
      "Epoch 14810/20000 Validation Loss: 0.051650021225214005\n",
      "Epoch 14811/20000 Training Loss: 0.052210066467523575\n",
      "Epoch 14812/20000 Training Loss: 0.05476628616452217\n",
      "Epoch 14813/20000 Training Loss: 0.05249817296862602\n",
      "Epoch 14814/20000 Training Loss: 0.04993661120533943\n",
      "Epoch 14815/20000 Training Loss: 0.04861297085881233\n",
      "Epoch 14816/20000 Training Loss: 0.04468092322349548\n",
      "Epoch 14817/20000 Training Loss: 0.05284909904003143\n",
      "Epoch 14818/20000 Training Loss: 0.04221370443701744\n",
      "Epoch 14819/20000 Training Loss: 0.04670117422938347\n",
      "Epoch 14820/20000 Training Loss: 0.06371324509382248\n",
      "Epoch 14820/20000 Validation Loss: 0.04543487727642059\n",
      "Epoch 14821/20000 Training Loss: 0.05742185190320015\n",
      "Epoch 14822/20000 Training Loss: 0.06040702387690544\n",
      "Epoch 14823/20000 Training Loss: 0.0651746615767479\n",
      "Epoch 14824/20000 Training Loss: 0.05300411581993103\n",
      "Epoch 14825/20000 Training Loss: 0.05843890830874443\n",
      "Epoch 14826/20000 Training Loss: 0.057179201394319534\n",
      "Epoch 14827/20000 Training Loss: 0.06012877821922302\n",
      "Epoch 14828/20000 Training Loss: 0.05277833342552185\n",
      "Epoch 14829/20000 Training Loss: 0.05494703724980354\n",
      "Epoch 14830/20000 Training Loss: 0.054799020290374756\n",
      "Epoch 14830/20000 Validation Loss: 0.06282695382833481\n",
      "Epoch 14831/20000 Training Loss: 0.07595493644475937\n",
      "Epoch 14832/20000 Training Loss: 0.06884052604436874\n",
      "Epoch 14833/20000 Training Loss: 0.0564250648021698\n",
      "Epoch 14834/20000 Training Loss: 0.04970141127705574\n",
      "Epoch 14835/20000 Training Loss: 0.04106716811656952\n",
      "Epoch 14836/20000 Training Loss: 0.0486893467605114\n",
      "Epoch 14837/20000 Training Loss: 0.048414260149002075\n",
      "Epoch 14838/20000 Training Loss: 0.05474676564335823\n",
      "Epoch 14839/20000 Training Loss: 0.05491691827774048\n",
      "Epoch 14840/20000 Training Loss: 0.02958577871322632\n",
      "Epoch 14840/20000 Validation Loss: 0.07531332969665527\n",
      "Epoch 14841/20000 Training Loss: 0.04976735636591911\n",
      "Epoch 14842/20000 Training Loss: 0.05938485264778137\n",
      "Epoch 14843/20000 Training Loss: 0.04348805174231529\n",
      "Epoch 14844/20000 Training Loss: 0.045765284448862076\n",
      "Epoch 14845/20000 Training Loss: 0.05618599057197571\n",
      "Epoch 14846/20000 Training Loss: 0.06129543110728264\n",
      "Epoch 14847/20000 Training Loss: 0.04239680990576744\n",
      "Epoch 14848/20000 Training Loss: 0.04967185854911804\n",
      "Epoch 14849/20000 Training Loss: 0.05769692733883858\n",
      "Epoch 14850/20000 Training Loss: 0.05569439008831978\n",
      "Epoch 14850/20000 Validation Loss: 0.04432113841176033\n",
      "Epoch 14851/20000 Training Loss: 0.070592500269413\n",
      "Epoch 14852/20000 Training Loss: 0.045959796756505966\n",
      "Epoch 14853/20000 Training Loss: 0.04615940526127815\n",
      "Epoch 14854/20000 Training Loss: 0.06485258787870407\n",
      "Epoch 14855/20000 Training Loss: 0.05357407033443451\n",
      "Epoch 14856/20000 Training Loss: 0.04209372028708458\n",
      "Epoch 14857/20000 Training Loss: 0.06894166022539139\n",
      "Epoch 14858/20000 Training Loss: 0.057389840483665466\n",
      "Epoch 14859/20000 Training Loss: 0.052974194288253784\n",
      "Epoch 14860/20000 Training Loss: 0.05428969860076904\n",
      "Epoch 14860/20000 Validation Loss: 0.04845689609646797\n",
      "Epoch 14861/20000 Training Loss: 0.05939699336886406\n",
      "Epoch 14862/20000 Training Loss: 0.0509055070579052\n",
      "Epoch 14863/20000 Training Loss: 0.04665128514170647\n",
      "Epoch 14864/20000 Training Loss: 0.052418600767850876\n",
      "Epoch 14865/20000 Training Loss: 0.04440131410956383\n",
      "Epoch 14866/20000 Training Loss: 0.05154809728264809\n",
      "Epoch 14867/20000 Training Loss: 0.05227961763739586\n",
      "Epoch 14868/20000 Training Loss: 0.04876687005162239\n",
      "Epoch 14869/20000 Training Loss: 0.05897925794124603\n",
      "Epoch 14870/20000 Training Loss: 0.056139569729566574\n",
      "Epoch 14870/20000 Validation Loss: 0.047544486820697784\n",
      "Epoch 14871/20000 Training Loss: 0.04406558349728584\n",
      "Epoch 14872/20000 Training Loss: 0.07542547583580017\n",
      "Epoch 14873/20000 Training Loss: 0.06293833255767822\n",
      "Epoch 14874/20000 Training Loss: 0.0638054832816124\n",
      "Epoch 14875/20000 Training Loss: 0.05168459936976433\n",
      "Epoch 14876/20000 Training Loss: 0.05433770641684532\n",
      "Epoch 14877/20000 Training Loss: 0.04267299175262451\n",
      "Epoch 14878/20000 Training Loss: 0.05982023477554321\n",
      "Epoch 14879/20000 Training Loss: 0.0614035427570343\n",
      "Epoch 14880/20000 Training Loss: 0.057900115847587585\n",
      "Epoch 14880/20000 Validation Loss: 0.06780935823917389\n",
      "Epoch 14881/20000 Training Loss: 0.045276761054992676\n",
      "Epoch 14882/20000 Training Loss: 0.052412357181310654\n",
      "Epoch 14883/20000 Training Loss: 0.05139151215553284\n",
      "Epoch 14884/20000 Training Loss: 0.04549553617835045\n",
      "Epoch 14885/20000 Training Loss: 0.049754004925489426\n",
      "Epoch 14886/20000 Training Loss: 0.05836519971489906\n",
      "Epoch 14887/20000 Training Loss: 0.06837350875139236\n",
      "Epoch 14888/20000 Training Loss: 0.07137449830770493\n",
      "Epoch 14889/20000 Training Loss: 0.050878703594207764\n",
      "Epoch 14890/20000 Training Loss: 0.05838784575462341\n",
      "Epoch 14890/20000 Validation Loss: 0.059203557670116425\n",
      "Epoch 14891/20000 Training Loss: 0.0742931067943573\n",
      "Epoch 14892/20000 Training Loss: 0.04967501387000084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14893/20000 Training Loss: 0.047275688499212265\n",
      "Epoch 14894/20000 Training Loss: 0.06354062259197235\n",
      "Epoch 14895/20000 Training Loss: 0.05547240749001503\n",
      "Epoch 14896/20000 Training Loss: 0.045701876282691956\n",
      "Epoch 14897/20000 Training Loss: 0.05013464018702507\n",
      "Epoch 14898/20000 Training Loss: 0.056023139506578445\n",
      "Epoch 14899/20000 Training Loss: 0.04718717560172081\n",
      "Epoch 14900/20000 Training Loss: 0.059117089956998825\n",
      "Epoch 14900/20000 Validation Loss: 0.04435446858406067\n",
      "Epoch 14901/20000 Training Loss: 0.040503498166799545\n",
      "Epoch 14902/20000 Training Loss: 0.05173995718359947\n",
      "Epoch 14903/20000 Training Loss: 0.057186078280210495\n",
      "Epoch 14904/20000 Training Loss: 0.054087717086076736\n",
      "Epoch 14905/20000 Training Loss: 0.03938752040266991\n",
      "Epoch 14906/20000 Training Loss: 0.07834095507860184\n",
      "Epoch 14907/20000 Training Loss: 0.04821145534515381\n",
      "Epoch 14908/20000 Training Loss: 0.05808370187878609\n",
      "Epoch 14909/20000 Training Loss: 0.07218289375305176\n",
      "Epoch 14910/20000 Training Loss: 0.057069092988967896\n",
      "Epoch 14910/20000 Validation Loss: 0.04756108298897743\n",
      "Epoch 14911/20000 Training Loss: 0.06006792187690735\n",
      "Epoch 14912/20000 Training Loss: 0.05113159492611885\n",
      "Epoch 14913/20000 Training Loss: 0.05019409582018852\n",
      "Epoch 14914/20000 Training Loss: 0.054615091532468796\n",
      "Epoch 14915/20000 Training Loss: 0.07623792439699173\n",
      "Epoch 14916/20000 Training Loss: 0.04284919425845146\n",
      "Epoch 14917/20000 Training Loss: 0.06325750797986984\n",
      "Epoch 14918/20000 Training Loss: 0.05647411569952965\n",
      "Epoch 14919/20000 Training Loss: 0.047424107789993286\n",
      "Epoch 14920/20000 Training Loss: 0.03836403414607048\n",
      "Epoch 14920/20000 Validation Loss: 0.04150622338056564\n",
      "Epoch 14921/20000 Training Loss: 0.06267581135034561\n",
      "Epoch 14922/20000 Training Loss: 0.05611125007271767\n",
      "Epoch 14923/20000 Training Loss: 0.052654098719358444\n",
      "Epoch 14924/20000 Training Loss: 0.059069711714982986\n",
      "Epoch 14925/20000 Training Loss: 0.05115492269396782\n",
      "Epoch 14926/20000 Training Loss: 0.04871005937457085\n",
      "Epoch 14927/20000 Training Loss: 0.045607682317495346\n",
      "Epoch 14928/20000 Training Loss: 0.062979556620121\n",
      "Epoch 14929/20000 Training Loss: 0.06115151569247246\n",
      "Epoch 14930/20000 Training Loss: 0.04706576466560364\n",
      "Epoch 14930/20000 Validation Loss: 0.07549471408128738\n",
      "Epoch 14931/20000 Training Loss: 0.06145364046096802\n",
      "Epoch 14932/20000 Training Loss: 0.04802970960736275\n",
      "Epoch 14933/20000 Training Loss: 0.06090311333537102\n",
      "Epoch 14934/20000 Training Loss: 0.0642031878232956\n",
      "Epoch 14935/20000 Training Loss: 0.05232599377632141\n",
      "Epoch 14936/20000 Training Loss: 0.06407671421766281\n",
      "Epoch 14937/20000 Training Loss: 0.05859208106994629\n",
      "Epoch 14938/20000 Training Loss: 0.04997923597693443\n",
      "Epoch 14939/20000 Training Loss: 0.0513286329805851\n",
      "Epoch 14940/20000 Training Loss: 0.055189117789268494\n",
      "Epoch 14940/20000 Validation Loss: 0.0793631374835968\n",
      "Epoch 14941/20000 Training Loss: 0.060970425605773926\n",
      "Epoch 14942/20000 Training Loss: 0.05016763135790825\n",
      "Epoch 14943/20000 Training Loss: 0.05897970125079155\n",
      "Epoch 14944/20000 Training Loss: 0.06189040467143059\n",
      "Epoch 14945/20000 Training Loss: 0.04126421734690666\n",
      "Epoch 14946/20000 Training Loss: 0.06995978206396103\n",
      "Epoch 14947/20000 Training Loss: 0.04718497768044472\n",
      "Epoch 14948/20000 Training Loss: 0.05081167444586754\n",
      "Epoch 14949/20000 Training Loss: 0.06314148753881454\n",
      "Epoch 14950/20000 Training Loss: 0.04265998303890228\n",
      "Epoch 14950/20000 Validation Loss: 0.0777735710144043\n",
      "Epoch 14951/20000 Training Loss: 0.06800790876150131\n",
      "Epoch 14952/20000 Training Loss: 0.061694104224443436\n",
      "Epoch 14953/20000 Training Loss: 0.07578130811452866\n",
      "Epoch 14954/20000 Training Loss: 0.046936627477407455\n",
      "Epoch 14955/20000 Training Loss: 0.0560249425470829\n",
      "Epoch 14956/20000 Training Loss: 0.045365795493125916\n",
      "Epoch 14957/20000 Training Loss: 0.056963998824357986\n",
      "Epoch 14958/20000 Training Loss: 0.054028287529945374\n",
      "Epoch 14959/20000 Training Loss: 0.06324701756238937\n",
      "Epoch 14960/20000 Training Loss: 0.038326457142829895\n",
      "Epoch 14960/20000 Validation Loss: 0.06539108604192734\n",
      "Epoch 14961/20000 Training Loss: 0.042250171303749084\n",
      "Epoch 14962/20000 Training Loss: 0.054190244525671005\n",
      "Epoch 14963/20000 Training Loss: 0.05272611975669861\n",
      "Epoch 14964/20000 Training Loss: 0.05751991644501686\n",
      "Epoch 14965/20000 Training Loss: 0.04889384284615517\n",
      "Epoch 14966/20000 Training Loss: 0.050338130444288254\n",
      "Epoch 14967/20000 Training Loss: 0.04639659449458122\n",
      "Epoch 14968/20000 Training Loss: 0.06272074580192566\n",
      "Epoch 14969/20000 Training Loss: 0.04886995628476143\n",
      "Epoch 14970/20000 Training Loss: 0.059366464614868164\n",
      "Epoch 14970/20000 Validation Loss: 0.06789208948612213\n",
      "Epoch 14971/20000 Training Loss: 0.0442587174475193\n",
      "Epoch 14972/20000 Training Loss: 0.05241991579532623\n",
      "Epoch 14973/20000 Training Loss: 0.055251460522413254\n",
      "Epoch 14974/20000 Training Loss: 0.04154888167977333\n",
      "Epoch 14975/20000 Training Loss: 0.06068119779229164\n",
      "Epoch 14976/20000 Training Loss: 0.05165717005729675\n",
      "Epoch 14977/20000 Training Loss: 0.042926859110593796\n",
      "Epoch 14978/20000 Training Loss: 0.07169584929943085\n",
      "Epoch 14979/20000 Training Loss: 0.06737460941076279\n",
      "Epoch 14980/20000 Training Loss: 0.06148526445031166\n",
      "Epoch 14980/20000 Validation Loss: 0.06247619539499283\n",
      "Epoch 14981/20000 Training Loss: 0.04183868691325188\n",
      "Epoch 14982/20000 Training Loss: 0.04469148814678192\n",
      "Epoch 14983/20000 Training Loss: 0.0477469228208065\n",
      "Epoch 14984/20000 Training Loss: 0.039622776210308075\n",
      "Epoch 14985/20000 Training Loss: 0.05503020063042641\n",
      "Epoch 14986/20000 Training Loss: 0.04287436231970787\n",
      "Epoch 14987/20000 Training Loss: 0.057860057801008224\n",
      "Epoch 14988/20000 Training Loss: 0.07406166195869446\n",
      "Epoch 14989/20000 Training Loss: 0.04161861166357994\n",
      "Epoch 14990/20000 Training Loss: 0.06247120723128319\n",
      "Epoch 14990/20000 Validation Loss: 0.07171821594238281\n",
      "Epoch 14991/20000 Training Loss: 0.057164210826158524\n",
      "Epoch 14992/20000 Training Loss: 0.052111487835645676\n",
      "Epoch 14993/20000 Training Loss: 0.04903090372681618\n",
      "Epoch 14994/20000 Training Loss: 0.07086209207773209\n",
      "Epoch 14995/20000 Training Loss: 0.04509957507252693\n",
      "Epoch 14996/20000 Training Loss: 0.055511463433504105\n",
      "Epoch 14997/20000 Training Loss: 0.04513034597039223\n",
      "Epoch 14998/20000 Training Loss: 0.045281365513801575\n",
      "Epoch 14999/20000 Training Loss: 0.05130033567547798\n",
      "Epoch 15000/20000 Training Loss: 0.06140404939651489\n",
      "Epoch 15000/20000 Validation Loss: 0.0494454987347126\n",
      "Epoch 15001/20000 Training Loss: 0.06041918322443962\n",
      "Epoch 15002/20000 Training Loss: 0.09025051444768906\n",
      "Epoch 15003/20000 Training Loss: 0.048528436571359634\n",
      "Epoch 15004/20000 Training Loss: 0.05817331001162529\n",
      "Epoch 15005/20000 Training Loss: 0.050269562751054764\n",
      "Epoch 15006/20000 Training Loss: 0.044992756098508835\n",
      "Epoch 15007/20000 Training Loss: 0.04976319149136543\n",
      "Epoch 15008/20000 Training Loss: 0.04565404728055\n",
      "Epoch 15009/20000 Training Loss: 0.04937633499503136\n",
      "Epoch 15010/20000 Training Loss: 0.06635443866252899\n",
      "Epoch 15010/20000 Validation Loss: 0.06663623452186584\n",
      "Epoch 15011/20000 Training Loss: 0.04478880763053894\n",
      "Epoch 15012/20000 Training Loss: 0.057663191109895706\n",
      "Epoch 15013/20000 Training Loss: 0.05136169493198395\n",
      "Epoch 15014/20000 Training Loss: 0.07095056772232056\n",
      "Epoch 15015/20000 Training Loss: 0.046276334673166275\n",
      "Epoch 15016/20000 Training Loss: 0.05334441736340523\n",
      "Epoch 15017/20000 Training Loss: 0.0480685718357563\n",
      "Epoch 15018/20000 Training Loss: 0.061703938990831375\n",
      "Epoch 15019/20000 Training Loss: 0.05069845914840698\n",
      "Epoch 15020/20000 Training Loss: 0.0485449880361557\n",
      "Epoch 15020/20000 Validation Loss: 0.05255447328090668\n",
      "Epoch 15021/20000 Training Loss: 0.044287193566560745\n",
      "Epoch 15022/20000 Training Loss: 0.05119778588414192\n",
      "Epoch 15023/20000 Training Loss: 0.06333360821008682\n",
      "Epoch 15024/20000 Training Loss: 0.0452149324119091\n",
      "Epoch 15025/20000 Training Loss: 0.04791264235973358\n",
      "Epoch 15026/20000 Training Loss: 0.051081109791994095\n",
      "Epoch 15027/20000 Training Loss: 0.04963960871100426\n",
      "Epoch 15028/20000 Training Loss: 0.04526449739933014\n",
      "Epoch 15029/20000 Training Loss: 0.05189083516597748\n",
      "Epoch 15030/20000 Training Loss: 0.06112952157855034\n",
      "Epoch 15030/20000 Validation Loss: 0.04796804487705231\n",
      "Epoch 15031/20000 Training Loss: 0.05268241837620735\n",
      "Epoch 15032/20000 Training Loss: 0.05065840482711792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15033/20000 Training Loss: 0.05972319841384888\n",
      "Epoch 15034/20000 Training Loss: 0.04546647146344185\n",
      "Epoch 15035/20000 Training Loss: 0.048285722732543945\n",
      "Epoch 15036/20000 Training Loss: 0.07696074992418289\n",
      "Epoch 15037/20000 Training Loss: 0.056865643709897995\n",
      "Epoch 15038/20000 Training Loss: 0.046016473323106766\n",
      "Epoch 15039/20000 Training Loss: 0.06097177788615227\n",
      "Epoch 15040/20000 Training Loss: 0.05027972161769867\n",
      "Epoch 15040/20000 Validation Loss: 0.04956716299057007\n",
      "Epoch 15041/20000 Training Loss: 0.05865051969885826\n",
      "Epoch 15042/20000 Training Loss: 0.044916022568941116\n",
      "Epoch 15043/20000 Training Loss: 0.04951893165707588\n",
      "Epoch 15044/20000 Training Loss: 0.0569458045065403\n",
      "Epoch 15045/20000 Training Loss: 0.05322400853037834\n",
      "Epoch 15046/20000 Training Loss: 0.05813392996788025\n",
      "Epoch 15047/20000 Training Loss: 0.06753969192504883\n",
      "Epoch 15048/20000 Training Loss: 0.05048366263508797\n",
      "Epoch 15049/20000 Training Loss: 0.0535186231136322\n",
      "Epoch 15050/20000 Training Loss: 0.04454757645726204\n",
      "Epoch 15050/20000 Validation Loss: 0.052135612815618515\n",
      "Epoch 15051/20000 Training Loss: 0.061126261949539185\n",
      "Epoch 15052/20000 Training Loss: 0.05746806040406227\n",
      "Epoch 15053/20000 Training Loss: 0.06215281784534454\n",
      "Epoch 15054/20000 Training Loss: 0.05562828853726387\n",
      "Epoch 15055/20000 Training Loss: 0.04913860559463501\n",
      "Epoch 15056/20000 Training Loss: 0.04557482525706291\n",
      "Epoch 15057/20000 Training Loss: 0.06048282980918884\n",
      "Epoch 15058/20000 Training Loss: 0.05567917227745056\n",
      "Epoch 15059/20000 Training Loss: 0.048505377024412155\n",
      "Epoch 15060/20000 Training Loss: 0.0539126880466938\n",
      "Epoch 15060/20000 Validation Loss: 0.06556035578250885\n",
      "Epoch 15061/20000 Training Loss: 0.050117503851652145\n",
      "Epoch 15062/20000 Training Loss: 0.07388170063495636\n",
      "Epoch 15063/20000 Training Loss: 0.042525824159383774\n",
      "Epoch 15064/20000 Training Loss: 0.04313234984874725\n",
      "Epoch 15065/20000 Training Loss: 0.04156799241900444\n",
      "Epoch 15066/20000 Training Loss: 0.04382602870464325\n",
      "Epoch 15067/20000 Training Loss: 0.040760647505521774\n",
      "Epoch 15068/20000 Training Loss: 0.05850622430443764\n",
      "Epoch 15069/20000 Training Loss: 0.04319655895233154\n",
      "Epoch 15070/20000 Training Loss: 0.04930543899536133\n",
      "Epoch 15070/20000 Validation Loss: 0.050410397350788116\n",
      "Epoch 15071/20000 Training Loss: 0.05502859875559807\n",
      "Epoch 15072/20000 Training Loss: 0.04482855275273323\n",
      "Epoch 15073/20000 Training Loss: 0.06379880011081696\n",
      "Epoch 15074/20000 Training Loss: 0.060977816581726074\n",
      "Epoch 15075/20000 Training Loss: 0.04637959972023964\n",
      "Epoch 15076/20000 Training Loss: 0.05261029675602913\n",
      "Epoch 15077/20000 Training Loss: 0.06884270161390305\n",
      "Epoch 15078/20000 Training Loss: 0.044267166405916214\n",
      "Epoch 15079/20000 Training Loss: 0.05112920328974724\n",
      "Epoch 15080/20000 Training Loss: 0.05127974972128868\n",
      "Epoch 15080/20000 Validation Loss: 0.05656392499804497\n",
      "Epoch 15081/20000 Training Loss: 0.043322402983903885\n",
      "Epoch 15082/20000 Training Loss: 0.050602346658706665\n",
      "Epoch 15083/20000 Training Loss: 0.06925905495882034\n",
      "Epoch 15084/20000 Training Loss: 0.04459249973297119\n",
      "Epoch 15085/20000 Training Loss: 0.06249621510505676\n",
      "Epoch 15086/20000 Training Loss: 0.054563552141189575\n",
      "Epoch 15087/20000 Training Loss: 0.0704500675201416\n",
      "Epoch 15088/20000 Training Loss: 0.03781624883413315\n",
      "Epoch 15089/20000 Training Loss: 0.05275088921189308\n",
      "Epoch 15090/20000 Training Loss: 0.04908261075615883\n",
      "Epoch 15090/20000 Validation Loss: 0.07689888030290604\n",
      "Epoch 15091/20000 Training Loss: 0.0512709803879261\n",
      "Epoch 15092/20000 Training Loss: 0.04833654686808586\n",
      "Epoch 15093/20000 Training Loss: 0.034286826848983765\n",
      "Epoch 15094/20000 Training Loss: 0.0590038001537323\n",
      "Epoch 15095/20000 Training Loss: 0.05907091870903969\n",
      "Epoch 15096/20000 Training Loss: 0.05063015595078468\n",
      "Epoch 15097/20000 Training Loss: 0.05666683241724968\n",
      "Epoch 15098/20000 Training Loss: 0.05438188835978508\n",
      "Epoch 15099/20000 Training Loss: 0.0535762719810009\n",
      "Epoch 15100/20000 Training Loss: 0.05271148309111595\n",
      "Epoch 15100/20000 Validation Loss: 0.06980399042367935\n",
      "Epoch 15101/20000 Training Loss: 0.04879032447934151\n",
      "Epoch 15102/20000 Training Loss: 0.04345734417438507\n",
      "Epoch 15103/20000 Training Loss: 0.05671968683600426\n",
      "Epoch 15104/20000 Training Loss: 0.04890497401356697\n",
      "Epoch 15105/20000 Training Loss: 0.05315270647406578\n",
      "Epoch 15106/20000 Training Loss: 0.04113706573843956\n",
      "Epoch 15107/20000 Training Loss: 0.049494668841362\n",
      "Epoch 15108/20000 Training Loss: 0.05754726752638817\n",
      "Epoch 15109/20000 Training Loss: 0.04845929145812988\n",
      "Epoch 15110/20000 Training Loss: 0.05459558591246605\n",
      "Epoch 15110/20000 Validation Loss: 0.07397300004959106\n",
      "Epoch 15111/20000 Training Loss: 0.05624094977974892\n",
      "Epoch 15112/20000 Training Loss: 0.04865971580147743\n",
      "Epoch 15113/20000 Training Loss: 0.04657530412077904\n",
      "Epoch 15114/20000 Training Loss: 0.05804575979709625\n",
      "Epoch 15115/20000 Training Loss: 0.062336623668670654\n",
      "Epoch 15116/20000 Training Loss: 0.050683990120887756\n",
      "Epoch 15117/20000 Training Loss: 0.06613083183765411\n",
      "Epoch 15118/20000 Training Loss: 0.07463312894105911\n",
      "Epoch 15119/20000 Training Loss: 0.05573016405105591\n",
      "Epoch 15120/20000 Training Loss: 0.04856092855334282\n",
      "Epoch 15120/20000 Validation Loss: 0.04568085819482803\n",
      "Epoch 15121/20000 Training Loss: 0.04240503907203674\n",
      "Epoch 15122/20000 Training Loss: 0.0513763390481472\n",
      "Epoch 15123/20000 Training Loss: 0.038017213344573975\n",
      "Epoch 15124/20000 Training Loss: 0.045224230736494064\n",
      "Epoch 15125/20000 Training Loss: 0.05826246738433838\n",
      "Epoch 15126/20000 Training Loss: 0.06768158823251724\n",
      "Epoch 15127/20000 Training Loss: 0.06966155022382736\n",
      "Epoch 15128/20000 Training Loss: 0.04937378689646721\n",
      "Epoch 15129/20000 Training Loss: 0.04712708666920662\n",
      "Epoch 15130/20000 Training Loss: 0.05559238791465759\n",
      "Epoch 15130/20000 Validation Loss: 0.042953867465257645\n",
      "Epoch 15131/20000 Training Loss: 0.02598433941602707\n",
      "Epoch 15132/20000 Training Loss: 0.054713573306798935\n",
      "Epoch 15133/20000 Training Loss: 0.05951857194304466\n",
      "Epoch 15134/20000 Training Loss: 0.042648810893297195\n",
      "Epoch 15135/20000 Training Loss: 0.06016531214118004\n",
      "Epoch 15136/20000 Training Loss: 0.03625128045678139\n",
      "Epoch 15137/20000 Training Loss: 0.07647644728422165\n",
      "Epoch 15138/20000 Training Loss: 0.0391237735748291\n",
      "Epoch 15139/20000 Training Loss: 0.04225906729698181\n",
      "Epoch 15140/20000 Training Loss: 0.07569334656000137\n",
      "Epoch 15140/20000 Validation Loss: 0.06693446636199951\n",
      "Epoch 15141/20000 Training Loss: 0.06040515378117561\n",
      "Epoch 15142/20000 Training Loss: 0.06870333105325699\n",
      "Epoch 15143/20000 Training Loss: 0.04575395584106445\n",
      "Epoch 15144/20000 Training Loss: 0.05033025145530701\n",
      "Epoch 15145/20000 Training Loss: 0.05557894706726074\n",
      "Epoch 15146/20000 Training Loss: 0.044377297163009644\n",
      "Epoch 15147/20000 Training Loss: 0.044337108731269836\n",
      "Epoch 15148/20000 Training Loss: 0.06490319967269897\n",
      "Epoch 15149/20000 Training Loss: 0.049336597323417664\n",
      "Epoch 15150/20000 Training Loss: 0.06951747834682465\n",
      "Epoch 15150/20000 Validation Loss: 0.0744093507528305\n",
      "Epoch 15151/20000 Training Loss: 0.044137436896562576\n",
      "Epoch 15152/20000 Training Loss: 0.06884118169546127\n",
      "Epoch 15153/20000 Training Loss: 0.040708597749471664\n",
      "Epoch 15154/20000 Training Loss: 0.05831320956349373\n",
      "Epoch 15155/20000 Training Loss: 0.04877272620797157\n",
      "Epoch 15156/20000 Training Loss: 0.048585548996925354\n",
      "Epoch 15157/20000 Training Loss: 0.05416221544146538\n",
      "Epoch 15158/20000 Training Loss: 0.06656026095151901\n",
      "Epoch 15159/20000 Training Loss: 0.061366382986307144\n",
      "Epoch 15160/20000 Training Loss: 0.04459180310368538\n",
      "Epoch 15160/20000 Validation Loss: 0.05849269777536392\n",
      "Epoch 15161/20000 Training Loss: 0.047126900404691696\n",
      "Epoch 15162/20000 Training Loss: 0.049638379365205765\n",
      "Epoch 15163/20000 Training Loss: 0.056269269436597824\n",
      "Epoch 15164/20000 Training Loss: 0.06292323023080826\n",
      "Epoch 15165/20000 Training Loss: 0.06643905490636826\n",
      "Epoch 15166/20000 Training Loss: 0.04564521089196205\n",
      "Epoch 15167/20000 Training Loss: 0.05769945681095123\n",
      "Epoch 15168/20000 Training Loss: 0.05349954590201378\n",
      "Epoch 15169/20000 Training Loss: 0.04956873133778572\n",
      "Epoch 15170/20000 Training Loss: 0.04980910196900368\n",
      "Epoch 15170/20000 Validation Loss: 0.06074164807796478\n",
      "Epoch 15171/20000 Training Loss: 0.04462669417262077\n",
      "Epoch 15172/20000 Training Loss: 0.04206695780158043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15173/20000 Training Loss: 0.06203272566199303\n",
      "Epoch 15174/20000 Training Loss: 0.055733274668455124\n",
      "Epoch 15175/20000 Training Loss: 0.0461033433675766\n",
      "Epoch 15176/20000 Training Loss: 0.05257756635546684\n",
      "Epoch 15177/20000 Training Loss: 0.0486593060195446\n",
      "Epoch 15178/20000 Training Loss: 0.05731223151087761\n",
      "Epoch 15179/20000 Training Loss: 0.048183709383010864\n",
      "Epoch 15180/20000 Training Loss: 0.05616988614201546\n",
      "Epoch 15180/20000 Validation Loss: 0.05174688994884491\n",
      "Epoch 15181/20000 Training Loss: 0.056013286113739014\n",
      "Epoch 15182/20000 Training Loss: 0.057645533233881\n",
      "Epoch 15183/20000 Training Loss: 0.07228202372789383\n",
      "Epoch 15184/20000 Training Loss: 0.06850466877222061\n",
      "Epoch 15185/20000 Training Loss: 0.04809579253196716\n",
      "Epoch 15186/20000 Training Loss: 0.05991785600781441\n",
      "Epoch 15187/20000 Training Loss: 0.048009783029556274\n",
      "Epoch 15188/20000 Training Loss: 0.05848349630832672\n",
      "Epoch 15189/20000 Training Loss: 0.05405014753341675\n",
      "Epoch 15190/20000 Training Loss: 0.06395387649536133\n",
      "Epoch 15190/20000 Validation Loss: 0.05166146159172058\n",
      "Epoch 15191/20000 Training Loss: 0.04483030363917351\n",
      "Epoch 15192/20000 Training Loss: 0.07642524689435959\n",
      "Epoch 15193/20000 Training Loss: 0.05130552127957344\n",
      "Epoch 15194/20000 Training Loss: 0.05355491116642952\n",
      "Epoch 15195/20000 Training Loss: 0.06015009805560112\n",
      "Epoch 15196/20000 Training Loss: 0.05524830147624016\n",
      "Epoch 15197/20000 Training Loss: 0.06417972594499588\n",
      "Epoch 15198/20000 Training Loss: 0.06223047152161598\n",
      "Epoch 15199/20000 Training Loss: 0.04968251660466194\n",
      "Epoch 15200/20000 Training Loss: 0.04791449010372162\n",
      "Epoch 15200/20000 Validation Loss: 0.040360067039728165\n",
      "Epoch 15201/20000 Training Loss: 0.049589503556489944\n",
      "Epoch 15202/20000 Training Loss: 0.0503762923181057\n",
      "Epoch 15203/20000 Training Loss: 0.05775722488760948\n",
      "Epoch 15204/20000 Training Loss: 0.07570170611143112\n",
      "Epoch 15205/20000 Training Loss: 0.04183465242385864\n",
      "Epoch 15206/20000 Training Loss: 0.05421615019440651\n",
      "Epoch 15207/20000 Training Loss: 0.05881128087639809\n",
      "Epoch 15208/20000 Training Loss: 0.05676570534706116\n",
      "Epoch 15209/20000 Training Loss: 0.07848531752824783\n",
      "Epoch 15210/20000 Training Loss: 0.04733123257756233\n",
      "Epoch 15210/20000 Validation Loss: 0.04246661812067032\n",
      "Epoch 15211/20000 Training Loss: 0.041492611169815063\n",
      "Epoch 15212/20000 Training Loss: 0.06291289627552032\n",
      "Epoch 15213/20000 Training Loss: 0.04894360899925232\n",
      "Epoch 15214/20000 Training Loss: 0.05050419270992279\n",
      "Epoch 15215/20000 Training Loss: 0.06346699595451355\n",
      "Epoch 15216/20000 Training Loss: 0.06537928432226181\n",
      "Epoch 15217/20000 Training Loss: 0.0561077781021595\n",
      "Epoch 15218/20000 Training Loss: 0.05733536183834076\n",
      "Epoch 15219/20000 Training Loss: 0.06355420500040054\n",
      "Epoch 15220/20000 Training Loss: 0.052143801003694534\n",
      "Epoch 15220/20000 Validation Loss: 0.05273125320672989\n",
      "Epoch 15221/20000 Training Loss: 0.061314716935157776\n",
      "Epoch 15222/20000 Training Loss: 0.051850687712430954\n",
      "Epoch 15223/20000 Training Loss: 0.06369877606630325\n",
      "Epoch 15224/20000 Training Loss: 0.06088244915008545\n",
      "Epoch 15225/20000 Training Loss: 0.053111959248781204\n",
      "Epoch 15226/20000 Training Loss: 0.0659765750169754\n",
      "Epoch 15227/20000 Training Loss: 0.039339330047369\n",
      "Epoch 15228/20000 Training Loss: 0.05581694841384888\n",
      "Epoch 15229/20000 Training Loss: 0.040197718888521194\n",
      "Epoch 15230/20000 Training Loss: 0.041620511561632156\n",
      "Epoch 15230/20000 Validation Loss: 0.04915902018547058\n",
      "Epoch 15231/20000 Training Loss: 0.05511517450213432\n",
      "Epoch 15232/20000 Training Loss: 0.04645584151148796\n",
      "Epoch 15233/20000 Training Loss: 0.04619450494647026\n",
      "Epoch 15234/20000 Training Loss: 0.04618633911013603\n",
      "Epoch 15235/20000 Training Loss: 0.04817827418446541\n",
      "Epoch 15236/20000 Training Loss: 0.06800392270088196\n",
      "Epoch 15237/20000 Training Loss: 0.04370618984103203\n",
      "Epoch 15238/20000 Training Loss: 0.04163011536002159\n",
      "Epoch 15239/20000 Training Loss: 0.06152541562914848\n",
      "Epoch 15240/20000 Training Loss: 0.04771273210644722\n",
      "Epoch 15240/20000 Validation Loss: 0.0680006891489029\n",
      "Epoch 15241/20000 Training Loss: 0.04742717742919922\n",
      "Epoch 15242/20000 Training Loss: 0.04257689788937569\n",
      "Epoch 15243/20000 Training Loss: 0.06641607731580734\n",
      "Epoch 15244/20000 Training Loss: 0.056663621217012405\n",
      "Epoch 15245/20000 Training Loss: 0.045793671160936356\n",
      "Epoch 15246/20000 Training Loss: 0.04742169380187988\n",
      "Epoch 15247/20000 Training Loss: 0.04405071213841438\n",
      "Epoch 15248/20000 Training Loss: 0.05815132334828377\n",
      "Epoch 15249/20000 Training Loss: 0.040892843157052994\n",
      "Epoch 15250/20000 Training Loss: 0.06060696020722389\n",
      "Epoch 15250/20000 Validation Loss: 0.06357195973396301\n",
      "Epoch 15251/20000 Training Loss: 0.05602044239640236\n",
      "Epoch 15252/20000 Training Loss: 0.041954923421144485\n",
      "Epoch 15253/20000 Training Loss: 0.06995662301778793\n",
      "Epoch 15254/20000 Training Loss: 0.04478057846426964\n",
      "Epoch 15255/20000 Training Loss: 0.042328596115112305\n",
      "Epoch 15256/20000 Training Loss: 0.04622507095336914\n",
      "Epoch 15257/20000 Training Loss: 0.06576308608055115\n",
      "Epoch 15258/20000 Training Loss: 0.03402818366885185\n",
      "Epoch 15259/20000 Training Loss: 0.0750286653637886\n",
      "Epoch 15260/20000 Training Loss: 0.04734817147254944\n",
      "Epoch 15260/20000 Validation Loss: 0.059076640754938126\n",
      "Epoch 15261/20000 Training Loss: 0.034379277378320694\n",
      "Epoch 15262/20000 Training Loss: 0.04275663569569588\n",
      "Epoch 15263/20000 Training Loss: 0.06329192221164703\n",
      "Epoch 15264/20000 Training Loss: 0.03960402309894562\n",
      "Epoch 15265/20000 Training Loss: 0.05311496928334236\n",
      "Epoch 15266/20000 Training Loss: 0.040897730737924576\n",
      "Epoch 15267/20000 Training Loss: 0.05114845559000969\n",
      "Epoch 15268/20000 Training Loss: 0.06022757291793823\n",
      "Epoch 15269/20000 Training Loss: 0.062350451946258545\n",
      "Epoch 15270/20000 Training Loss: 0.0638936385512352\n",
      "Epoch 15270/20000 Validation Loss: 0.06379488855600357\n",
      "Epoch 15271/20000 Training Loss: 0.049074381589889526\n",
      "Epoch 15272/20000 Training Loss: 0.04619861766695976\n",
      "Epoch 15273/20000 Training Loss: 0.05144607648253441\n",
      "Epoch 15274/20000 Training Loss: 0.05457736551761627\n",
      "Epoch 15275/20000 Training Loss: 0.07215370982885361\n",
      "Epoch 15276/20000 Training Loss: 0.053159695118665695\n",
      "Epoch 15277/20000 Training Loss: 0.05625160038471222\n",
      "Epoch 15278/20000 Training Loss: 0.06931302696466446\n",
      "Epoch 15279/20000 Training Loss: 0.0605628676712513\n",
      "Epoch 15280/20000 Training Loss: 0.04704487696290016\n",
      "Epoch 15280/20000 Validation Loss: 0.06084079295396805\n",
      "Epoch 15281/20000 Training Loss: 0.046301763504743576\n",
      "Epoch 15282/20000 Training Loss: 0.0552615262567997\n",
      "Epoch 15283/20000 Training Loss: 0.05716956779360771\n",
      "Epoch 15284/20000 Training Loss: 0.0496460385620594\n",
      "Epoch 15285/20000 Training Loss: 0.05814666673541069\n",
      "Epoch 15286/20000 Training Loss: 0.047902122139930725\n",
      "Epoch 15287/20000 Training Loss: 0.04511119797825813\n",
      "Epoch 15288/20000 Training Loss: 0.05659125745296478\n",
      "Epoch 15289/20000 Training Loss: 0.04902258515357971\n",
      "Epoch 15290/20000 Training Loss: 0.06872088462114334\n",
      "Epoch 15290/20000 Validation Loss: 0.05829499661922455\n",
      "Epoch 15291/20000 Training Loss: 0.03832899406552315\n",
      "Epoch 15292/20000 Training Loss: 0.04342637583613396\n",
      "Epoch 15293/20000 Training Loss: 0.059588443487882614\n",
      "Epoch 15294/20000 Training Loss: 0.05963294580578804\n",
      "Epoch 15295/20000 Training Loss: 0.0683722272515297\n",
      "Epoch 15296/20000 Training Loss: 0.057646799832582474\n",
      "Epoch 15297/20000 Training Loss: 0.048982638865709305\n",
      "Epoch 15298/20000 Training Loss: 0.05149706080555916\n",
      "Epoch 15299/20000 Training Loss: 0.04943893849849701\n",
      "Epoch 15300/20000 Training Loss: 0.055472057312726974\n",
      "Epoch 15300/20000 Validation Loss: 0.05892034247517586\n",
      "Epoch 15301/20000 Training Loss: 0.05729673430323601\n",
      "Epoch 15302/20000 Training Loss: 0.05187033489346504\n",
      "Epoch 15303/20000 Training Loss: 0.061713505536317825\n",
      "Epoch 15304/20000 Training Loss: 0.04378407821059227\n",
      "Epoch 15305/20000 Training Loss: 0.03907485678792\n",
      "Epoch 15306/20000 Training Loss: 0.05512414500117302\n",
      "Epoch 15307/20000 Training Loss: 0.053621381521224976\n",
      "Epoch 15308/20000 Training Loss: 0.05797581747174263\n",
      "Epoch 15309/20000 Training Loss: 0.05872263386845589\n",
      "Epoch 15310/20000 Training Loss: 0.05222413316369057\n",
      "Epoch 15310/20000 Validation Loss: 0.06784046441316605\n",
      "Epoch 15311/20000 Training Loss: 0.0507529191672802\n",
      "Epoch 15312/20000 Training Loss: 0.059280022978782654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15313/20000 Training Loss: 0.050987958908081055\n",
      "Epoch 15314/20000 Training Loss: 0.049472495913505554\n",
      "Epoch 15315/20000 Training Loss: 0.050823602825403214\n",
      "Epoch 15316/20000 Training Loss: 0.05885156989097595\n",
      "Epoch 15317/20000 Training Loss: 0.052872419357299805\n",
      "Epoch 15318/20000 Training Loss: 0.047409504652023315\n",
      "Epoch 15319/20000 Training Loss: 0.055559173226356506\n",
      "Epoch 15320/20000 Training Loss: 0.05478358268737793\n",
      "Epoch 15320/20000 Validation Loss: 0.05882703512907028\n",
      "Epoch 15321/20000 Training Loss: 0.05815690755844116\n",
      "Epoch 15322/20000 Training Loss: 0.047852788120508194\n",
      "Epoch 15323/20000 Training Loss: 0.053121838718652725\n",
      "Epoch 15324/20000 Training Loss: 0.06660258024930954\n",
      "Epoch 15325/20000 Training Loss: 0.0777973160147667\n",
      "Epoch 15326/20000 Training Loss: 0.05604981258511543\n",
      "Epoch 15327/20000 Training Loss: 0.05754578113555908\n",
      "Epoch 15328/20000 Training Loss: 0.0358659066259861\n",
      "Epoch 15329/20000 Training Loss: 0.041371505707502365\n",
      "Epoch 15330/20000 Training Loss: 0.05388866737484932\n",
      "Epoch 15330/20000 Validation Loss: 0.042150385677814484\n",
      "Epoch 15331/20000 Training Loss: 0.05572099983692169\n",
      "Epoch 15332/20000 Training Loss: 0.04097756743431091\n",
      "Epoch 15333/20000 Training Loss: 0.04633147642016411\n",
      "Epoch 15334/20000 Training Loss: 0.04371116682887077\n",
      "Epoch 15335/20000 Training Loss: 0.0508890338242054\n",
      "Epoch 15336/20000 Training Loss: 0.06479120254516602\n",
      "Epoch 15337/20000 Training Loss: 0.034018050879240036\n",
      "Epoch 15338/20000 Training Loss: 0.03956165909767151\n",
      "Epoch 15339/20000 Training Loss: 0.06229037418961525\n",
      "Epoch 15340/20000 Training Loss: 0.048909977078437805\n",
      "Epoch 15340/20000 Validation Loss: 0.05022741109132767\n",
      "Epoch 15341/20000 Training Loss: 0.052691731601953506\n",
      "Epoch 15342/20000 Training Loss: 0.04542485252022743\n",
      "Epoch 15343/20000 Training Loss: 0.06423356384038925\n",
      "Epoch 15344/20000 Training Loss: 0.05418212339282036\n",
      "Epoch 15345/20000 Training Loss: 0.0552251972258091\n",
      "Epoch 15346/20000 Training Loss: 0.0561676062643528\n",
      "Epoch 15347/20000 Training Loss: 0.05927544832229614\n",
      "Epoch 15348/20000 Training Loss: 0.05502952262759209\n",
      "Epoch 15349/20000 Training Loss: 0.05023545026779175\n",
      "Epoch 15350/20000 Training Loss: 0.05035291239619255\n",
      "Epoch 15350/20000 Validation Loss: 0.04781198874115944\n",
      "Epoch 15351/20000 Training Loss: 0.04985927417874336\n",
      "Epoch 15352/20000 Training Loss: 0.06193771958351135\n",
      "Epoch 15353/20000 Training Loss: 0.06010422483086586\n",
      "Epoch 15354/20000 Training Loss: 0.04414745047688484\n",
      "Epoch 15355/20000 Training Loss: 0.049323033541440964\n",
      "Epoch 15356/20000 Training Loss: 0.05643497407436371\n",
      "Epoch 15357/20000 Training Loss: 0.05053173378109932\n",
      "Epoch 15358/20000 Training Loss: 0.04806436225771904\n",
      "Epoch 15359/20000 Training Loss: 0.045855920761823654\n",
      "Epoch 15360/20000 Training Loss: 0.05583192780613899\n",
      "Epoch 15360/20000 Validation Loss: 0.05843949317932129\n",
      "Epoch 15361/20000 Training Loss: 0.05090149864554405\n",
      "Epoch 15362/20000 Training Loss: 0.06240396201610565\n",
      "Epoch 15363/20000 Training Loss: 0.052827898412942886\n",
      "Epoch 15364/20000 Training Loss: 0.059405356645584106\n",
      "Epoch 15365/20000 Training Loss: 0.059045787900686264\n",
      "Epoch 15366/20000 Training Loss: 0.046694230288267136\n",
      "Epoch 15367/20000 Training Loss: 0.05189913883805275\n",
      "Epoch 15368/20000 Training Loss: 0.05491672828793526\n",
      "Epoch 15369/20000 Training Loss: 0.05492928624153137\n",
      "Epoch 15370/20000 Training Loss: 0.0495634563267231\n",
      "Epoch 15370/20000 Validation Loss: 0.0495036318898201\n",
      "Epoch 15371/20000 Training Loss: 0.052336644381284714\n",
      "Epoch 15372/20000 Training Loss: 0.04866143688559532\n",
      "Epoch 15373/20000 Training Loss: 0.05385676026344299\n",
      "Epoch 15374/20000 Training Loss: 0.06210145354270935\n",
      "Epoch 15375/20000 Training Loss: 0.03235388919711113\n",
      "Epoch 15376/20000 Training Loss: 0.04932032898068428\n",
      "Epoch 15377/20000 Training Loss: 0.05518491938710213\n",
      "Epoch 15378/20000 Training Loss: 0.08033397048711777\n",
      "Epoch 15379/20000 Training Loss: 0.054173022508621216\n",
      "Epoch 15380/20000 Training Loss: 0.05034780129790306\n",
      "Epoch 15380/20000 Validation Loss: 0.039435748010873795\n",
      "Epoch 15381/20000 Training Loss: 0.053750935941934586\n",
      "Epoch 15382/20000 Training Loss: 0.056552331894636154\n",
      "Epoch 15383/20000 Training Loss: 0.058123111724853516\n",
      "Epoch 15384/20000 Training Loss: 0.05279092863202095\n",
      "Epoch 15385/20000 Training Loss: 0.06311080604791641\n",
      "Epoch 15386/20000 Training Loss: 0.07126150280237198\n",
      "Epoch 15387/20000 Training Loss: 0.05427753925323486\n",
      "Epoch 15388/20000 Training Loss: 0.062277425080537796\n",
      "Epoch 15389/20000 Training Loss: 0.04646070674061775\n",
      "Epoch 15390/20000 Training Loss: 0.04596355929970741\n",
      "Epoch 15390/20000 Validation Loss: 0.06229693442583084\n",
      "Epoch 15391/20000 Training Loss: 0.06177164614200592\n",
      "Epoch 15392/20000 Training Loss: 0.056326333433389664\n",
      "Epoch 15393/20000 Training Loss: 0.05675791576504707\n",
      "Epoch 15394/20000 Training Loss: 0.06581071019172668\n",
      "Epoch 15395/20000 Training Loss: 0.050506848841905594\n",
      "Epoch 15396/20000 Training Loss: 0.06334061175584793\n",
      "Epoch 15397/20000 Training Loss: 0.05213674530386925\n",
      "Epoch 15398/20000 Training Loss: 0.050725001841783524\n",
      "Epoch 15399/20000 Training Loss: 0.06633328646421432\n",
      "Epoch 15400/20000 Training Loss: 0.05089156702160835\n",
      "Epoch 15400/20000 Validation Loss: 0.043389759957790375\n",
      "Epoch 15401/20000 Training Loss: 0.05965809524059296\n",
      "Epoch 15402/20000 Training Loss: 0.06118026003241539\n",
      "Epoch 15403/20000 Training Loss: 0.04929570481181145\n",
      "Epoch 15404/20000 Training Loss: 0.06095755100250244\n",
      "Epoch 15405/20000 Training Loss: 0.06395434588193893\n",
      "Epoch 15406/20000 Training Loss: 0.07772040367126465\n",
      "Epoch 15407/20000 Training Loss: 0.06297178566455841\n",
      "Epoch 15408/20000 Training Loss: 0.05769439414143562\n",
      "Epoch 15409/20000 Training Loss: 0.057780906558036804\n",
      "Epoch 15410/20000 Training Loss: 0.047984350472688675\n",
      "Epoch 15410/20000 Validation Loss: 0.04040171205997467\n",
      "Epoch 15411/20000 Training Loss: 0.048774879425764084\n",
      "Epoch 15412/20000 Training Loss: 0.05410207435488701\n",
      "Epoch 15413/20000 Training Loss: 0.05318723991513252\n",
      "Epoch 15414/20000 Training Loss: 0.05592148005962372\n",
      "Epoch 15415/20000 Training Loss: 0.05712798610329628\n",
      "Epoch 15416/20000 Training Loss: 0.04988120496273041\n",
      "Epoch 15417/20000 Training Loss: 0.041491247713565826\n",
      "Epoch 15418/20000 Training Loss: 0.05921924114227295\n",
      "Epoch 15419/20000 Training Loss: 0.04499061778187752\n",
      "Epoch 15420/20000 Training Loss: 0.05011756718158722\n",
      "Epoch 15420/20000 Validation Loss: 0.07244066148996353\n",
      "Epoch 15421/20000 Training Loss: 0.053253378719091415\n",
      "Epoch 15422/20000 Training Loss: 0.04740861803293228\n",
      "Epoch 15423/20000 Training Loss: 0.0522182397544384\n",
      "Epoch 15424/20000 Training Loss: 0.05297574773430824\n",
      "Epoch 15425/20000 Training Loss: 0.059010207653045654\n",
      "Epoch 15426/20000 Training Loss: 0.06833819299936295\n",
      "Epoch 15427/20000 Training Loss: 0.06198636814951897\n",
      "Epoch 15428/20000 Training Loss: 0.05355192348361015\n",
      "Epoch 15429/20000 Training Loss: 0.053622305393218994\n",
      "Epoch 15430/20000 Training Loss: 0.050392746925354004\n",
      "Epoch 15430/20000 Validation Loss: 0.06729011237621307\n",
      "Epoch 15431/20000 Training Loss: 0.06102545186877251\n",
      "Epoch 15432/20000 Training Loss: 0.06264598667621613\n",
      "Epoch 15433/20000 Training Loss: 0.05605920031666756\n",
      "Epoch 15434/20000 Training Loss: 0.07334375381469727\n",
      "Epoch 15435/20000 Training Loss: 0.0485476553440094\n",
      "Epoch 15436/20000 Training Loss: 0.05371544882655144\n",
      "Epoch 15437/20000 Training Loss: 0.053696829825639725\n",
      "Epoch 15438/20000 Training Loss: 0.046078938990831375\n",
      "Epoch 15439/20000 Training Loss: 0.04018789157271385\n",
      "Epoch 15440/20000 Training Loss: 0.05288165807723999\n",
      "Epoch 15440/20000 Validation Loss: 0.07434151321649551\n",
      "Epoch 15441/20000 Training Loss: 0.04180430248379707\n",
      "Epoch 15442/20000 Training Loss: 0.04695730283856392\n",
      "Epoch 15443/20000 Training Loss: 0.05858124420046806\n",
      "Epoch 15444/20000 Training Loss: 0.04330543801188469\n",
      "Epoch 15445/20000 Training Loss: 0.050061360001564026\n",
      "Epoch 15446/20000 Training Loss: 0.056095536798238754\n",
      "Epoch 15447/20000 Training Loss: 0.057436734437942505\n",
      "Epoch 15448/20000 Training Loss: 0.0457121916115284\n",
      "Epoch 15449/20000 Training Loss: 0.053598787635564804\n",
      "Epoch 15450/20000 Training Loss: 0.044129516929388046\n",
      "Epoch 15450/20000 Validation Loss: 0.07281195372343063\n",
      "Epoch 15451/20000 Training Loss: 0.05718092992901802\n",
      "Epoch 15452/20000 Training Loss: 0.05451497435569763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15453/20000 Training Loss: 0.04668720066547394\n",
      "Epoch 15454/20000 Training Loss: 0.06034556031227112\n",
      "Epoch 15455/20000 Training Loss: 0.04745746776461601\n",
      "Epoch 15456/20000 Training Loss: 0.05469176545739174\n",
      "Epoch 15457/20000 Training Loss: 0.055348824709653854\n",
      "Epoch 15458/20000 Training Loss: 0.05693495646119118\n",
      "Epoch 15459/20000 Training Loss: 0.0418354831635952\n",
      "Epoch 15460/20000 Training Loss: 0.04887546971440315\n",
      "Epoch 15460/20000 Validation Loss: 0.049609191715717316\n",
      "Epoch 15461/20000 Training Loss: 0.058585286140441895\n",
      "Epoch 15462/20000 Training Loss: 0.05275638774037361\n",
      "Epoch 15463/20000 Training Loss: 0.041966453194618225\n",
      "Epoch 15464/20000 Training Loss: 0.050027698278427124\n",
      "Epoch 15465/20000 Training Loss: 0.056731898337602615\n",
      "Epoch 15466/20000 Training Loss: 0.04518289864063263\n",
      "Epoch 15467/20000 Training Loss: 0.040444400161504745\n",
      "Epoch 15468/20000 Training Loss: 0.04506582021713257\n",
      "Epoch 15469/20000 Training Loss: 0.07003196328878403\n",
      "Epoch 15470/20000 Training Loss: 0.041778143495321274\n",
      "Epoch 15470/20000 Validation Loss: 0.05613961070775986\n",
      "Epoch 15471/20000 Training Loss: 0.04018199443817139\n",
      "Epoch 15472/20000 Training Loss: 0.0642932653427124\n",
      "Epoch 15473/20000 Training Loss: 0.06515644490718842\n",
      "Epoch 15474/20000 Training Loss: 0.05956443026661873\n",
      "Epoch 15475/20000 Training Loss: 0.06061838939785957\n",
      "Epoch 15476/20000 Training Loss: 0.057755161076784134\n",
      "Epoch 15477/20000 Training Loss: 0.05842958018183708\n",
      "Epoch 15478/20000 Training Loss: 0.049541402608156204\n",
      "Epoch 15479/20000 Training Loss: 0.037652697414159775\n",
      "Epoch 15480/20000 Training Loss: 0.05393935739994049\n",
      "Epoch 15480/20000 Validation Loss: 0.0802859365940094\n",
      "Epoch 15481/20000 Training Loss: 0.04536035656929016\n",
      "Epoch 15482/20000 Training Loss: 0.06541816890239716\n",
      "Epoch 15483/20000 Training Loss: 0.04125082865357399\n",
      "Epoch 15484/20000 Training Loss: 0.05738991126418114\n",
      "Epoch 15485/20000 Training Loss: 0.07025368511676788\n",
      "Epoch 15486/20000 Training Loss: 0.06901194900274277\n",
      "Epoch 15487/20000 Training Loss: 0.05788632109761238\n",
      "Epoch 15488/20000 Training Loss: 0.07567497342824936\n",
      "Epoch 15489/20000 Training Loss: 0.04645127058029175\n",
      "Epoch 15490/20000 Training Loss: 0.06183871254324913\n",
      "Epoch 15490/20000 Validation Loss: 0.06002625823020935\n",
      "Epoch 15491/20000 Training Loss: 0.060200706124305725\n",
      "Epoch 15492/20000 Training Loss: 0.05821962282061577\n",
      "Epoch 15493/20000 Training Loss: 0.07200304418802261\n",
      "Epoch 15494/20000 Training Loss: 0.07463563233613968\n",
      "Epoch 15495/20000 Training Loss: 0.07232128828763962\n",
      "Epoch 15496/20000 Training Loss: 0.06908244639635086\n",
      "Epoch 15497/20000 Training Loss: 0.0628327950835228\n",
      "Epoch 15498/20000 Training Loss: 0.038663867861032486\n",
      "Epoch 15499/20000 Training Loss: 0.07280612736940384\n",
      "Epoch 15500/20000 Training Loss: 0.05678604170680046\n",
      "Epoch 15500/20000 Validation Loss: 0.05578052997589111\n",
      "Epoch 15501/20000 Training Loss: 0.057186856865882874\n",
      "Epoch 15502/20000 Training Loss: 0.05186738446354866\n",
      "Epoch 15503/20000 Training Loss: 0.06088246405124664\n",
      "Epoch 15504/20000 Training Loss: 0.04578647017478943\n",
      "Epoch 15505/20000 Training Loss: 0.06039510294795036\n",
      "Epoch 15506/20000 Training Loss: 0.05519269034266472\n",
      "Epoch 15507/20000 Training Loss: 0.06501393020153046\n",
      "Epoch 15508/20000 Training Loss: 0.061327189207077026\n",
      "Epoch 15509/20000 Training Loss: 0.04687308892607689\n",
      "Epoch 15510/20000 Training Loss: 0.04586723446846008\n",
      "Epoch 15510/20000 Validation Loss: 0.0618383064866066\n",
      "Epoch 15511/20000 Training Loss: 0.07611219584941864\n",
      "Epoch 15512/20000 Training Loss: 0.05064545199275017\n",
      "Epoch 15513/20000 Training Loss: 0.0515577495098114\n",
      "Epoch 15514/20000 Training Loss: 0.04865552484989166\n",
      "Epoch 15515/20000 Training Loss: 0.070681631565094\n",
      "Epoch 15516/20000 Training Loss: 0.061467211693525314\n",
      "Epoch 15517/20000 Training Loss: 0.04141702130436897\n",
      "Epoch 15518/20000 Training Loss: 0.06852724403142929\n",
      "Epoch 15519/20000 Training Loss: 0.06815243512392044\n",
      "Epoch 15520/20000 Training Loss: 0.04415959119796753\n",
      "Epoch 15520/20000 Validation Loss: 0.08420031517744064\n",
      "Epoch 15521/20000 Training Loss: 0.04999660328030586\n",
      "Epoch 15522/20000 Training Loss: 0.05855752155184746\n",
      "Epoch 15523/20000 Training Loss: 0.053875017911195755\n",
      "Epoch 15524/20000 Training Loss: 0.07896721363067627\n",
      "Epoch 15525/20000 Training Loss: 0.048403333872556686\n",
      "Epoch 15526/20000 Training Loss: 0.06787420064210892\n",
      "Epoch 15527/20000 Training Loss: 0.05742327868938446\n",
      "Epoch 15528/20000 Training Loss: 0.05041730776429176\n",
      "Epoch 15529/20000 Training Loss: 0.05014212056994438\n",
      "Epoch 15530/20000 Training Loss: 0.0623893141746521\n",
      "Epoch 15530/20000 Validation Loss: 0.0729031190276146\n",
      "Epoch 15531/20000 Training Loss: 0.05843454226851463\n",
      "Epoch 15532/20000 Training Loss: 0.06063966825604439\n",
      "Epoch 15533/20000 Training Loss: 0.056184861809015274\n",
      "Epoch 15534/20000 Training Loss: 0.07038185000419617\n",
      "Epoch 15535/20000 Training Loss: 0.05367573723196983\n",
      "Epoch 15536/20000 Training Loss: 0.0553438775241375\n",
      "Epoch 15537/20000 Training Loss: 0.06474464386701584\n",
      "Epoch 15538/20000 Training Loss: 0.05976179614663124\n",
      "Epoch 15539/20000 Training Loss: 0.04282202944159508\n",
      "Epoch 15540/20000 Training Loss: 0.061797160655260086\n",
      "Epoch 15540/20000 Validation Loss: 0.05515562370419502\n",
      "Epoch 15541/20000 Training Loss: 0.04414751008152962\n",
      "Epoch 15542/20000 Training Loss: 0.06615539640188217\n",
      "Epoch 15543/20000 Training Loss: 0.035801392048597336\n",
      "Epoch 15544/20000 Training Loss: 0.05803220346570015\n",
      "Epoch 15545/20000 Training Loss: 0.04030025377869606\n",
      "Epoch 15546/20000 Training Loss: 0.04332796111702919\n",
      "Epoch 15547/20000 Training Loss: 0.046868402510881424\n",
      "Epoch 15548/20000 Training Loss: 0.061205700039863586\n",
      "Epoch 15549/20000 Training Loss: 0.04812680184841156\n",
      "Epoch 15550/20000 Training Loss: 0.05665777623653412\n",
      "Epoch 15550/20000 Validation Loss: 0.07333490997552872\n",
      "Epoch 15551/20000 Training Loss: 0.060008421540260315\n",
      "Epoch 15552/20000 Training Loss: 0.048829421401023865\n",
      "Epoch 15553/20000 Training Loss: 0.05388863757252693\n",
      "Epoch 15554/20000 Training Loss: 0.045735638588666916\n",
      "Epoch 15555/20000 Training Loss: 0.038450729101896286\n",
      "Epoch 15556/20000 Training Loss: 0.05737491324543953\n",
      "Epoch 15557/20000 Training Loss: 0.06179914250969887\n",
      "Epoch 15558/20000 Training Loss: 0.055925726890563965\n",
      "Epoch 15559/20000 Training Loss: 0.06444811820983887\n",
      "Epoch 15560/20000 Training Loss: 0.06608905643224716\n",
      "Epoch 15560/20000 Validation Loss: 0.05963999032974243\n",
      "Epoch 15561/20000 Training Loss: 0.05940982699394226\n",
      "Epoch 15562/20000 Training Loss: 0.08412826061248779\n",
      "Epoch 15563/20000 Training Loss: 0.055292751640081406\n",
      "Epoch 15564/20000 Training Loss: 0.05251125991344452\n",
      "Epoch 15565/20000 Training Loss: 0.05055834725499153\n",
      "Epoch 15566/20000 Training Loss: 0.05288836359977722\n",
      "Epoch 15567/20000 Training Loss: 0.046196337789297104\n",
      "Epoch 15568/20000 Training Loss: 0.07204685360193253\n",
      "Epoch 15569/20000 Training Loss: 0.056501444429159164\n",
      "Epoch 15570/20000 Training Loss: 0.04340166971087456\n",
      "Epoch 15570/20000 Validation Loss: 0.10007613152265549\n",
      "Epoch 15571/20000 Training Loss: 0.05715220794081688\n",
      "Epoch 15572/20000 Training Loss: 0.04984085634350777\n",
      "Epoch 15573/20000 Training Loss: 0.06154834106564522\n",
      "Epoch 15574/20000 Training Loss: 0.06681973487138748\n",
      "Epoch 15575/20000 Training Loss: 0.04534092918038368\n",
      "Epoch 15576/20000 Training Loss: 0.0631321370601654\n",
      "Epoch 15577/20000 Training Loss: 0.04429503157734871\n",
      "Epoch 15578/20000 Training Loss: 0.0630507841706276\n",
      "Epoch 15579/20000 Training Loss: 0.044366199523210526\n",
      "Epoch 15580/20000 Training Loss: 0.053042177110910416\n",
      "Epoch 15580/20000 Validation Loss: 0.05910666286945343\n",
      "Epoch 15581/20000 Training Loss: 0.06331174820661545\n",
      "Epoch 15582/20000 Training Loss: 0.05162215605378151\n",
      "Epoch 15583/20000 Training Loss: 0.0721512958407402\n",
      "Epoch 15584/20000 Training Loss: 0.04963534697890282\n",
      "Epoch 15585/20000 Training Loss: 0.07205723226070404\n",
      "Epoch 15586/20000 Training Loss: 0.042924243956804276\n",
      "Epoch 15587/20000 Training Loss: 0.049089569598436356\n",
      "Epoch 15588/20000 Training Loss: 0.05282916501164436\n",
      "Epoch 15589/20000 Training Loss: 0.06987877935171127\n",
      "Epoch 15590/20000 Training Loss: 0.052489180117845535\n",
      "Epoch 15590/20000 Validation Loss: 0.06629569083452225\n",
      "Epoch 15591/20000 Training Loss: 0.057671383023262024\n",
      "Epoch 15592/20000 Training Loss: 0.0661558210849762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15593/20000 Training Loss: 0.045773476362228394\n",
      "Epoch 15594/20000 Training Loss: 0.07564657181501389\n",
      "Epoch 15595/20000 Training Loss: 0.06101125851273537\n",
      "Epoch 15596/20000 Training Loss: 0.0644196942448616\n",
      "Epoch 15597/20000 Training Loss: 0.042772889137268066\n",
      "Epoch 15598/20000 Training Loss: 0.05030288174748421\n",
      "Epoch 15599/20000 Training Loss: 0.059091150760650635\n",
      "Epoch 15600/20000 Training Loss: 0.053616851568222046\n",
      "Epoch 15600/20000 Validation Loss: 0.06642462313175201\n",
      "Epoch 15601/20000 Training Loss: 0.061120253056287766\n",
      "Epoch 15602/20000 Training Loss: 0.05667475238442421\n",
      "Epoch 15603/20000 Training Loss: 0.056866854429244995\n",
      "Epoch 15604/20000 Training Loss: 0.05624663829803467\n",
      "Epoch 15605/20000 Training Loss: 0.049830254167318344\n",
      "Epoch 15606/20000 Training Loss: 0.040373776108026505\n",
      "Epoch 15607/20000 Training Loss: 0.04569808021187782\n",
      "Epoch 15608/20000 Training Loss: 0.050652001053094864\n",
      "Epoch 15609/20000 Training Loss: 0.05999695882201195\n",
      "Epoch 15610/20000 Training Loss: 0.05711328983306885\n",
      "Epoch 15610/20000 Validation Loss: 0.03734805807471275\n",
      "Epoch 15611/20000 Training Loss: 0.04824914410710335\n",
      "Epoch 15612/20000 Training Loss: 0.050176817923784256\n",
      "Epoch 15613/20000 Training Loss: 0.0495249442756176\n",
      "Epoch 15614/20000 Training Loss: 0.05497026443481445\n",
      "Epoch 15615/20000 Training Loss: 0.04876298829913139\n",
      "Epoch 15616/20000 Training Loss: 0.057311270385980606\n",
      "Epoch 15617/20000 Training Loss: 0.06178872287273407\n",
      "Epoch 15618/20000 Training Loss: 0.05756575986742973\n",
      "Epoch 15619/20000 Training Loss: 0.04663318395614624\n",
      "Epoch 15620/20000 Training Loss: 0.04986436292529106\n",
      "Epoch 15620/20000 Validation Loss: 0.057641252875328064\n",
      "Epoch 15621/20000 Training Loss: 0.053378213196992874\n",
      "Epoch 15622/20000 Training Loss: 0.050367508083581924\n",
      "Epoch 15623/20000 Training Loss: 0.05217602476477623\n",
      "Epoch 15624/20000 Training Loss: 0.05091126263141632\n",
      "Epoch 15625/20000 Training Loss: 0.047163646668195724\n",
      "Epoch 15626/20000 Training Loss: 0.05568068102002144\n",
      "Epoch 15627/20000 Training Loss: 0.05364561453461647\n",
      "Epoch 15628/20000 Training Loss: 0.055818360298871994\n",
      "Epoch 15629/20000 Training Loss: 0.045755188912153244\n",
      "Epoch 15630/20000 Training Loss: 0.05585870519280434\n",
      "Epoch 15630/20000 Validation Loss: 0.05884188786149025\n",
      "Epoch 15631/20000 Training Loss: 0.05016351118683815\n",
      "Epoch 15632/20000 Training Loss: 0.04943162202835083\n",
      "Epoch 15633/20000 Training Loss: 0.059307415038347244\n",
      "Epoch 15634/20000 Training Loss: 0.05436381697654724\n",
      "Epoch 15635/20000 Training Loss: 0.050008375197649\n",
      "Epoch 15636/20000 Training Loss: 0.06362248212099075\n",
      "Epoch 15637/20000 Training Loss: 0.04848289489746094\n",
      "Epoch 15638/20000 Training Loss: 0.045788053423166275\n",
      "Epoch 15639/20000 Training Loss: 0.04753045365214348\n",
      "Epoch 15640/20000 Training Loss: 0.060872483998537064\n",
      "Epoch 15640/20000 Validation Loss: 0.034716323018074036\n",
      "Epoch 15641/20000 Training Loss: 0.0620168037712574\n",
      "Epoch 15642/20000 Training Loss: 0.04995105043053627\n",
      "Epoch 15643/20000 Training Loss: 0.0625692829489708\n",
      "Epoch 15644/20000 Training Loss: 0.037485212087631226\n",
      "Epoch 15645/20000 Training Loss: 0.043788135051727295\n",
      "Epoch 15646/20000 Training Loss: 0.04944493994116783\n",
      "Epoch 15647/20000 Training Loss: 0.06151769682765007\n",
      "Epoch 15648/20000 Training Loss: 0.06085127592086792\n",
      "Epoch 15649/20000 Training Loss: 0.056800175458192825\n",
      "Epoch 15650/20000 Training Loss: 0.04481736943125725\n",
      "Epoch 15650/20000 Validation Loss: 0.05549069494009018\n",
      "Epoch 15651/20000 Training Loss: 0.04940101504325867\n",
      "Epoch 15652/20000 Training Loss: 0.0628708228468895\n",
      "Epoch 15653/20000 Training Loss: 0.04850611090660095\n",
      "Epoch 15654/20000 Training Loss: 0.056433796882629395\n",
      "Epoch 15655/20000 Training Loss: 0.047721702605485916\n",
      "Epoch 15656/20000 Training Loss: 0.051802683621644974\n",
      "Epoch 15657/20000 Training Loss: 0.03740648552775383\n",
      "Epoch 15658/20000 Training Loss: 0.04900488257408142\n",
      "Epoch 15659/20000 Training Loss: 0.07021171599626541\n",
      "Epoch 15660/20000 Training Loss: 0.06661058962345123\n",
      "Epoch 15660/20000 Validation Loss: 0.05671743303537369\n",
      "Epoch 15661/20000 Training Loss: 0.06031101569533348\n",
      "Epoch 15662/20000 Training Loss: 0.05903441831469536\n",
      "Epoch 15663/20000 Training Loss: 0.05394681170582771\n",
      "Epoch 15664/20000 Training Loss: 0.06209179759025574\n",
      "Epoch 15665/20000 Training Loss: 0.04101258143782616\n",
      "Epoch 15666/20000 Training Loss: 0.04887308552861214\n",
      "Epoch 15667/20000 Training Loss: 0.04385128617286682\n",
      "Epoch 15668/20000 Training Loss: 0.04380128160119057\n",
      "Epoch 15669/20000 Training Loss: 0.04538186267018318\n",
      "Epoch 15670/20000 Training Loss: 0.052944254130125046\n",
      "Epoch 15670/20000 Validation Loss: 0.0683128759264946\n",
      "Epoch 15671/20000 Training Loss: 0.05265572667121887\n",
      "Epoch 15672/20000 Training Loss: 0.05299009010195732\n",
      "Epoch 15673/20000 Training Loss: 0.0529797337949276\n",
      "Epoch 15674/20000 Training Loss: 0.053767573088407516\n",
      "Epoch 15675/20000 Training Loss: 0.05630682781338692\n",
      "Epoch 15676/20000 Training Loss: 0.07113223522901535\n",
      "Epoch 15677/20000 Training Loss: 0.06285973638296127\n",
      "Epoch 15678/20000 Training Loss: 0.04515177011489868\n",
      "Epoch 15679/20000 Training Loss: 0.053062960505485535\n",
      "Epoch 15680/20000 Training Loss: 0.049173157662153244\n",
      "Epoch 15680/20000 Validation Loss: 0.04492293670773506\n",
      "Epoch 15681/20000 Training Loss: 0.06402555108070374\n",
      "Epoch 15682/20000 Training Loss: 0.05182655155658722\n",
      "Epoch 15683/20000 Training Loss: 0.05189456045627594\n",
      "Epoch 15684/20000 Training Loss: 0.04748595133423805\n",
      "Epoch 15685/20000 Training Loss: 0.08108600229024887\n",
      "Epoch 15686/20000 Training Loss: 0.049890678375959396\n",
      "Epoch 15687/20000 Training Loss: 0.05514591932296753\n",
      "Epoch 15688/20000 Training Loss: 0.06649952381849289\n",
      "Epoch 15689/20000 Training Loss: 0.0845940038561821\n",
      "Epoch 15690/20000 Training Loss: 0.060936927795410156\n",
      "Epoch 15690/20000 Validation Loss: 0.06473049521446228\n",
      "Epoch 15691/20000 Training Loss: 0.040960222482681274\n",
      "Epoch 15692/20000 Training Loss: 0.05563164874911308\n",
      "Epoch 15693/20000 Training Loss: 0.042360853403806686\n",
      "Epoch 15694/20000 Training Loss: 0.07191826403141022\n",
      "Epoch 15695/20000 Training Loss: 0.059651296585798264\n",
      "Epoch 15696/20000 Training Loss: 0.05195505917072296\n",
      "Epoch 15697/20000 Training Loss: 0.05858125910162926\n",
      "Epoch 15698/20000 Training Loss: 0.05979040265083313\n",
      "Epoch 15699/20000 Training Loss: 0.060468077659606934\n",
      "Epoch 15700/20000 Training Loss: 0.05577807500958443\n",
      "Epoch 15700/20000 Validation Loss: 0.05525019019842148\n",
      "Epoch 15701/20000 Training Loss: 0.06442829221487045\n",
      "Epoch 15702/20000 Training Loss: 0.05909654498100281\n",
      "Epoch 15703/20000 Training Loss: 0.04069335758686066\n",
      "Epoch 15704/20000 Training Loss: 0.059786658734083176\n",
      "Epoch 15705/20000 Training Loss: 0.03865533694624901\n",
      "Epoch 15706/20000 Training Loss: 0.04470342770218849\n",
      "Epoch 15707/20000 Training Loss: 0.051167044788599014\n",
      "Epoch 15708/20000 Training Loss: 0.057262033224105835\n",
      "Epoch 15709/20000 Training Loss: 0.04873493313789368\n",
      "Epoch 15710/20000 Training Loss: 0.049108829349279404\n",
      "Epoch 15710/20000 Validation Loss: 0.04153822362422943\n",
      "Epoch 15711/20000 Training Loss: 0.05593672767281532\n",
      "Epoch 15712/20000 Training Loss: 0.04819975793361664\n",
      "Epoch 15713/20000 Training Loss: 0.03269900381565094\n",
      "Epoch 15714/20000 Training Loss: 0.04237015172839165\n",
      "Epoch 15715/20000 Training Loss: 0.04601835086941719\n",
      "Epoch 15716/20000 Training Loss: 0.05184447765350342\n",
      "Epoch 15717/20000 Training Loss: 0.047051142901182175\n",
      "Epoch 15718/20000 Training Loss: 0.05493800714612007\n",
      "Epoch 15719/20000 Training Loss: 0.07601583003997803\n",
      "Epoch 15720/20000 Training Loss: 0.04809117317199707\n",
      "Epoch 15720/20000 Validation Loss: 0.05163244903087616\n",
      "Epoch 15721/20000 Training Loss: 0.04888511821627617\n",
      "Epoch 15722/20000 Training Loss: 0.05929512903094292\n",
      "Epoch 15723/20000 Training Loss: 0.06636428833007812\n",
      "Epoch 15724/20000 Training Loss: 0.04784809425473213\n",
      "Epoch 15725/20000 Training Loss: 0.05158928036689758\n",
      "Epoch 15726/20000 Training Loss: 0.05165167525410652\n",
      "Epoch 15727/20000 Training Loss: 0.05851976200938225\n",
      "Epoch 15728/20000 Training Loss: 0.05066980794072151\n",
      "Epoch 15729/20000 Training Loss: 0.050797343254089355\n",
      "Epoch 15730/20000 Training Loss: 0.04311785101890564\n",
      "Epoch 15730/20000 Validation Loss: 0.05004558712244034\n",
      "Epoch 15731/20000 Training Loss: 0.05565652251243591\n",
      "Epoch 15732/20000 Training Loss: 0.04712888225913048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15733/20000 Training Loss: 0.058431435376405716\n",
      "Epoch 15734/20000 Training Loss: 0.06077958270907402\n",
      "Epoch 15735/20000 Training Loss: 0.055543918162584305\n",
      "Epoch 15736/20000 Training Loss: 0.044075917452573776\n",
      "Epoch 15737/20000 Training Loss: 0.05267518758773804\n",
      "Epoch 15738/20000 Training Loss: 0.0497448556125164\n",
      "Epoch 15739/20000 Training Loss: 0.06758313626050949\n",
      "Epoch 15740/20000 Training Loss: 0.04959350451827049\n",
      "Epoch 15740/20000 Validation Loss: 0.055921316146850586\n",
      "Epoch 15741/20000 Training Loss: 0.05261882022023201\n",
      "Epoch 15742/20000 Training Loss: 0.06724586337804794\n",
      "Epoch 15743/20000 Training Loss: 0.06545842438936234\n",
      "Epoch 15744/20000 Training Loss: 0.08240587264299393\n",
      "Epoch 15745/20000 Training Loss: 0.06783503293991089\n",
      "Epoch 15746/20000 Training Loss: 0.06003640219569206\n",
      "Epoch 15747/20000 Training Loss: 0.05457490682601929\n",
      "Epoch 15748/20000 Training Loss: 0.04933596029877663\n",
      "Epoch 15749/20000 Training Loss: 0.04434343799948692\n",
      "Epoch 15750/20000 Training Loss: 0.04579852148890495\n",
      "Epoch 15750/20000 Validation Loss: 0.051506102085113525\n",
      "Epoch 15751/20000 Training Loss: 0.043435681611299515\n",
      "Epoch 15752/20000 Training Loss: 0.05918882414698601\n",
      "Epoch 15753/20000 Training Loss: 0.04846332594752312\n",
      "Epoch 15754/20000 Training Loss: 0.058641303330659866\n",
      "Epoch 15755/20000 Training Loss: 0.07018665224313736\n",
      "Epoch 15756/20000 Training Loss: 0.047533124685287476\n",
      "Epoch 15757/20000 Training Loss: 0.0396907739341259\n",
      "Epoch 15758/20000 Training Loss: 0.06147259473800659\n",
      "Epoch 15759/20000 Training Loss: 0.05010993406176567\n",
      "Epoch 15760/20000 Training Loss: 0.06831813603639603\n",
      "Epoch 15760/20000 Validation Loss: 0.04840945452451706\n",
      "Epoch 15761/20000 Training Loss: 0.046862173825502396\n",
      "Epoch 15762/20000 Training Loss: 0.06377854943275452\n",
      "Epoch 15763/20000 Training Loss: 0.05763610079884529\n",
      "Epoch 15764/20000 Training Loss: 0.0549062043428421\n",
      "Epoch 15765/20000 Training Loss: 0.05622117221355438\n",
      "Epoch 15766/20000 Training Loss: 0.04201449826359749\n",
      "Epoch 15767/20000 Training Loss: 0.04886351153254509\n",
      "Epoch 15768/20000 Training Loss: 0.04499809443950653\n",
      "Epoch 15769/20000 Training Loss: 0.04734151065349579\n",
      "Epoch 15770/20000 Training Loss: 0.03934380039572716\n",
      "Epoch 15770/20000 Validation Loss: 0.08388325572013855\n",
      "Epoch 15771/20000 Training Loss: 0.04759800806641579\n",
      "Epoch 15772/20000 Training Loss: 0.06892216950654984\n",
      "Epoch 15773/20000 Training Loss: 0.0668843537569046\n",
      "Epoch 15774/20000 Training Loss: 0.055990368127822876\n",
      "Epoch 15775/20000 Training Loss: 0.06082853674888611\n",
      "Epoch 15776/20000 Training Loss: 0.05070476233959198\n",
      "Epoch 15777/20000 Training Loss: 0.06782970577478409\n",
      "Epoch 15778/20000 Training Loss: 0.049864232540130615\n",
      "Epoch 15779/20000 Training Loss: 0.0602298229932785\n",
      "Epoch 15780/20000 Training Loss: 0.050440382212400436\n",
      "Epoch 15780/20000 Validation Loss: 0.05616389214992523\n",
      "Epoch 15781/20000 Training Loss: 0.0501437671482563\n",
      "Epoch 15782/20000 Training Loss: 0.05940357223153114\n",
      "Epoch 15783/20000 Training Loss: 0.04738166928291321\n",
      "Epoch 15784/20000 Training Loss: 0.052532538771629333\n",
      "Epoch 15785/20000 Training Loss: 0.042397063225507736\n",
      "Epoch 15786/20000 Training Loss: 0.05274505540728569\n",
      "Epoch 15787/20000 Training Loss: 0.06878738850355148\n",
      "Epoch 15788/20000 Training Loss: 0.04413900151848793\n",
      "Epoch 15789/20000 Training Loss: 0.055334437638521194\n",
      "Epoch 15790/20000 Training Loss: 0.047993943095207214\n",
      "Epoch 15790/20000 Validation Loss: 0.059099528938531876\n",
      "Epoch 15791/20000 Training Loss: 0.05473244562745094\n",
      "Epoch 15792/20000 Training Loss: 0.04691701754927635\n",
      "Epoch 15793/20000 Training Loss: 0.041419580578804016\n",
      "Epoch 15794/20000 Training Loss: 0.05253002047538757\n",
      "Epoch 15795/20000 Training Loss: 0.03772880509495735\n",
      "Epoch 15796/20000 Training Loss: 0.06951756775379181\n",
      "Epoch 15797/20000 Training Loss: 0.05722282826900482\n",
      "Epoch 15798/20000 Training Loss: 0.057815905660390854\n",
      "Epoch 15799/20000 Training Loss: 0.0479617565870285\n",
      "Epoch 15800/20000 Training Loss: 0.06032772734761238\n",
      "Epoch 15800/20000 Validation Loss: 0.04983706772327423\n",
      "Epoch 15801/20000 Training Loss: 0.050575125962495804\n",
      "Epoch 15802/20000 Training Loss: 0.07707402110099792\n",
      "Epoch 15803/20000 Training Loss: 0.07456482201814651\n",
      "Epoch 15804/20000 Training Loss: 0.05518878623843193\n",
      "Epoch 15805/20000 Training Loss: 0.05696208402514458\n",
      "Epoch 15806/20000 Training Loss: 0.0635475143790245\n",
      "Epoch 15807/20000 Training Loss: 0.05321064218878746\n",
      "Epoch 15808/20000 Training Loss: 0.048235997557640076\n",
      "Epoch 15809/20000 Training Loss: 0.04946218803524971\n",
      "Epoch 15810/20000 Training Loss: 0.05809957906603813\n",
      "Epoch 15810/20000 Validation Loss: 0.04053029790520668\n",
      "Epoch 15811/20000 Training Loss: 0.05098925903439522\n",
      "Epoch 15812/20000 Training Loss: 0.05753472447395325\n",
      "Epoch 15813/20000 Training Loss: 0.06671464443206787\n",
      "Epoch 15814/20000 Training Loss: 0.048537809401750565\n",
      "Epoch 15815/20000 Training Loss: 0.05861837789416313\n",
      "Epoch 15816/20000 Training Loss: 0.0560888797044754\n",
      "Epoch 15817/20000 Training Loss: 0.07118594646453857\n",
      "Epoch 15818/20000 Training Loss: 0.05143865942955017\n",
      "Epoch 15819/20000 Training Loss: 0.05163881182670593\n",
      "Epoch 15820/20000 Training Loss: 0.039703089743852615\n",
      "Epoch 15820/20000 Validation Loss: 0.04924195259809494\n",
      "Epoch 15821/20000 Training Loss: 0.06191731616854668\n",
      "Epoch 15822/20000 Training Loss: 0.06294428557157516\n",
      "Epoch 15823/20000 Training Loss: 0.0571286678314209\n",
      "Epoch 15824/20000 Training Loss: 0.061882536858320236\n",
      "Epoch 15825/20000 Training Loss: 0.07093552500009537\n",
      "Epoch 15826/20000 Training Loss: 0.0616389624774456\n",
      "Epoch 15827/20000 Training Loss: 0.0592472217977047\n",
      "Epoch 15828/20000 Training Loss: 0.055062372237443924\n",
      "Epoch 15829/20000 Training Loss: 0.04839291051030159\n",
      "Epoch 15830/20000 Training Loss: 0.04728459194302559\n",
      "Epoch 15830/20000 Validation Loss: 0.04993610456585884\n",
      "Epoch 15831/20000 Training Loss: 0.0659945011138916\n",
      "Epoch 15832/20000 Training Loss: 0.04972890019416809\n",
      "Epoch 15833/20000 Training Loss: 0.05662361904978752\n",
      "Epoch 15834/20000 Training Loss: 0.04504747316241264\n",
      "Epoch 15835/20000 Training Loss: 0.062460750341415405\n",
      "Epoch 15836/20000 Training Loss: 0.04685573652386665\n",
      "Epoch 15837/20000 Training Loss: 0.04703015089035034\n",
      "Epoch 15838/20000 Training Loss: 0.05465254187583923\n",
      "Epoch 15839/20000 Training Loss: 0.05502904951572418\n",
      "Epoch 15840/20000 Training Loss: 0.040539246052503586\n",
      "Epoch 15840/20000 Validation Loss: 0.05408145859837532\n",
      "Epoch 15841/20000 Training Loss: 0.05908383056521416\n",
      "Epoch 15842/20000 Training Loss: 0.07187580317258835\n",
      "Epoch 15843/20000 Training Loss: 0.05842720344662666\n",
      "Epoch 15844/20000 Training Loss: 0.051098983734846115\n",
      "Epoch 15845/20000 Training Loss: 0.06810726970434189\n",
      "Epoch 15846/20000 Training Loss: 0.03834204003214836\n",
      "Epoch 15847/20000 Training Loss: 0.05948026850819588\n",
      "Epoch 15848/20000 Training Loss: 0.05101044103503227\n",
      "Epoch 15849/20000 Training Loss: 0.039030011743307114\n",
      "Epoch 15850/20000 Training Loss: 0.04954126104712486\n",
      "Epoch 15850/20000 Validation Loss: 0.04361789673566818\n",
      "Epoch 15851/20000 Training Loss: 0.0514216423034668\n",
      "Epoch 15852/20000 Training Loss: 0.0475558303296566\n",
      "Epoch 15853/20000 Training Loss: 0.06038646027445793\n",
      "Epoch 15854/20000 Training Loss: 0.046342071145772934\n",
      "Epoch 15855/20000 Training Loss: 0.055595193058252335\n",
      "Epoch 15856/20000 Training Loss: 0.06381084024906158\n",
      "Epoch 15857/20000 Training Loss: 0.04418496415019035\n",
      "Epoch 15858/20000 Training Loss: 0.0533297061920166\n",
      "Epoch 15859/20000 Training Loss: 0.05291004851460457\n",
      "Epoch 15860/20000 Training Loss: 0.06982938200235367\n",
      "Epoch 15860/20000 Validation Loss: 0.049900636076927185\n",
      "Epoch 15861/20000 Training Loss: 0.057581577450037\n",
      "Epoch 15862/20000 Training Loss: 0.051486313343048096\n",
      "Epoch 15863/20000 Training Loss: 0.04614589735865593\n",
      "Epoch 15864/20000 Training Loss: 0.06251444667577744\n",
      "Epoch 15865/20000 Training Loss: 0.058215945959091187\n",
      "Epoch 15866/20000 Training Loss: 0.05133017897605896\n",
      "Epoch 15867/20000 Training Loss: 0.050068650394678116\n",
      "Epoch 15868/20000 Training Loss: 0.05427047610282898\n",
      "Epoch 15869/20000 Training Loss: 0.0532132089138031\n",
      "Epoch 15870/20000 Training Loss: 0.058788564056158066\n",
      "Epoch 15870/20000 Validation Loss: 0.04381366819143295\n",
      "Epoch 15871/20000 Training Loss: 0.0402163527905941\n",
      "Epoch 15872/20000 Training Loss: 0.046706151217222214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15873/20000 Training Loss: 0.053577858954668045\n",
      "Epoch 15874/20000 Training Loss: 0.06447050720453262\n",
      "Epoch 15875/20000 Training Loss: 0.05978650972247124\n",
      "Epoch 15876/20000 Training Loss: 0.06086452305316925\n",
      "Epoch 15877/20000 Training Loss: 0.06496919691562653\n",
      "Epoch 15878/20000 Training Loss: 0.06412112712860107\n",
      "Epoch 15879/20000 Training Loss: 0.04650062695145607\n",
      "Epoch 15880/20000 Training Loss: 0.07597070932388306\n",
      "Epoch 15880/20000 Validation Loss: 0.04898989200592041\n",
      "Epoch 15881/20000 Training Loss: 0.05432458594441414\n",
      "Epoch 15882/20000 Training Loss: 0.062032002955675125\n",
      "Epoch 15883/20000 Training Loss: 0.05578743293881416\n",
      "Epoch 15884/20000 Training Loss: 0.04924458637833595\n",
      "Epoch 15885/20000 Training Loss: 0.06887101382017136\n",
      "Epoch 15886/20000 Training Loss: 0.04570223018527031\n",
      "Epoch 15887/20000 Training Loss: 0.049635257571935654\n",
      "Epoch 15888/20000 Training Loss: 0.057965945452451706\n",
      "Epoch 15889/20000 Training Loss: 0.05723206326365471\n",
      "Epoch 15890/20000 Training Loss: 0.0516672320663929\n",
      "Epoch 15890/20000 Validation Loss: 0.05374757945537567\n",
      "Epoch 15891/20000 Training Loss: 0.057905446738004684\n",
      "Epoch 15892/20000 Training Loss: 0.058473777025938034\n",
      "Epoch 15893/20000 Training Loss: 0.03970588371157646\n",
      "Epoch 15894/20000 Training Loss: 0.06678649038076401\n",
      "Epoch 15895/20000 Training Loss: 0.045101359486579895\n",
      "Epoch 15896/20000 Training Loss: 0.04899754747748375\n",
      "Epoch 15897/20000 Training Loss: 0.05582960322499275\n",
      "Epoch 15898/20000 Training Loss: 0.05570995807647705\n",
      "Epoch 15899/20000 Training Loss: 0.05443604663014412\n",
      "Epoch 15900/20000 Training Loss: 0.04601174220442772\n",
      "Epoch 15900/20000 Validation Loss: 0.0564810186624527\n",
      "Epoch 15901/20000 Training Loss: 0.05085253715515137\n",
      "Epoch 15902/20000 Training Loss: 0.061576858162879944\n",
      "Epoch 15903/20000 Training Loss: 0.058751657605171204\n",
      "Epoch 15904/20000 Training Loss: 0.05263189598917961\n",
      "Epoch 15905/20000 Training Loss: 0.038959190249443054\n",
      "Epoch 15906/20000 Training Loss: 0.048078637570142746\n",
      "Epoch 15907/20000 Training Loss: 0.07462024688720703\n",
      "Epoch 15908/20000 Training Loss: 0.06195783615112305\n",
      "Epoch 15909/20000 Training Loss: 0.03965943679213524\n",
      "Epoch 15910/20000 Training Loss: 0.05293845012784004\n",
      "Epoch 15910/20000 Validation Loss: 0.06836153566837311\n",
      "Epoch 15911/20000 Training Loss: 0.04352612420916557\n",
      "Epoch 15912/20000 Training Loss: 0.0606105737388134\n",
      "Epoch 15913/20000 Training Loss: 0.04932832717895508\n",
      "Epoch 15914/20000 Training Loss: 0.046771664172410965\n",
      "Epoch 15915/20000 Training Loss: 0.04990001022815704\n",
      "Epoch 15916/20000 Training Loss: 0.04983583092689514\n",
      "Epoch 15917/20000 Training Loss: 0.04862545058131218\n",
      "Epoch 15918/20000 Training Loss: 0.09693169593811035\n",
      "Epoch 15919/20000 Training Loss: 0.07083944231271744\n",
      "Epoch 15920/20000 Training Loss: 0.05795179307460785\n",
      "Epoch 15920/20000 Validation Loss: 0.06898216903209686\n",
      "Epoch 15921/20000 Training Loss: 0.056595925241708755\n",
      "Epoch 15922/20000 Training Loss: 0.05345560982823372\n",
      "Epoch 15923/20000 Training Loss: 0.06388265639543533\n",
      "Epoch 15924/20000 Training Loss: 0.05824177339673042\n",
      "Epoch 15925/20000 Training Loss: 0.052330780774354935\n",
      "Epoch 15926/20000 Training Loss: 0.06282693892717361\n",
      "Epoch 15927/20000 Training Loss: 0.06994292140007019\n",
      "Epoch 15928/20000 Training Loss: 0.049590688198804855\n",
      "Epoch 15929/20000 Training Loss: 0.04788348451256752\n",
      "Epoch 15930/20000 Training Loss: 0.05312399938702583\n",
      "Epoch 15930/20000 Validation Loss: 0.06254051625728607\n",
      "Epoch 15931/20000 Training Loss: 0.0489467978477478\n",
      "Epoch 15932/20000 Training Loss: 0.060882944613695145\n",
      "Epoch 15933/20000 Training Loss: 0.03962334617972374\n",
      "Epoch 15934/20000 Training Loss: 0.058693915605545044\n",
      "Epoch 15935/20000 Training Loss: 0.04246324673295021\n",
      "Epoch 15936/20000 Training Loss: 0.042339909821748734\n",
      "Epoch 15937/20000 Training Loss: 0.0402713380753994\n",
      "Epoch 15938/20000 Training Loss: 0.062307823449373245\n",
      "Epoch 15939/20000 Training Loss: 0.05495430901646614\n",
      "Epoch 15940/20000 Training Loss: 0.04694413021206856\n",
      "Epoch 15940/20000 Validation Loss: 0.05567499250173569\n",
      "Epoch 15941/20000 Training Loss: 0.06482701748609543\n",
      "Epoch 15942/20000 Training Loss: 0.05145496502518654\n",
      "Epoch 15943/20000 Training Loss: 0.05912125110626221\n",
      "Epoch 15944/20000 Training Loss: 0.049160975962877274\n",
      "Epoch 15945/20000 Training Loss: 0.06339451670646667\n",
      "Epoch 15946/20000 Training Loss: 0.06536158174276352\n",
      "Epoch 15947/20000 Training Loss: 0.06486108154058456\n",
      "Epoch 15948/20000 Training Loss: 0.053928207606077194\n",
      "Epoch 15949/20000 Training Loss: 0.0473574735224247\n",
      "Epoch 15950/20000 Training Loss: 0.06720204651355743\n",
      "Epoch 15950/20000 Validation Loss: 0.048725519329309464\n",
      "Epoch 15951/20000 Training Loss: 0.058794114738702774\n",
      "Epoch 15952/20000 Training Loss: 0.07304049283266068\n",
      "Epoch 15953/20000 Training Loss: 0.059738148003816605\n",
      "Epoch 15954/20000 Training Loss: 0.039886076003313065\n",
      "Epoch 15955/20000 Training Loss: 0.05706204101443291\n",
      "Epoch 15956/20000 Training Loss: 0.052937135100364685\n",
      "Epoch 15957/20000 Training Loss: 0.05916516110301018\n",
      "Epoch 15958/20000 Training Loss: 0.05858612433075905\n",
      "Epoch 15959/20000 Training Loss: 0.05799254775047302\n",
      "Epoch 15960/20000 Training Loss: 0.061332423239946365\n",
      "Epoch 15960/20000 Validation Loss: 0.05498228222131729\n",
      "Epoch 15961/20000 Training Loss: 0.033322837203741074\n",
      "Epoch 15962/20000 Training Loss: 0.05263767018914223\n",
      "Epoch 15963/20000 Training Loss: 0.03898036107420921\n",
      "Epoch 15964/20000 Training Loss: 0.05640830472111702\n",
      "Epoch 15965/20000 Training Loss: 0.06463822722434998\n",
      "Epoch 15966/20000 Training Loss: 0.04794635996222496\n",
      "Epoch 15967/20000 Training Loss: 0.058893244713544846\n",
      "Epoch 15968/20000 Training Loss: 0.06166573241353035\n",
      "Epoch 15969/20000 Training Loss: 0.05680367350578308\n",
      "Epoch 15970/20000 Training Loss: 0.04749632999300957\n",
      "Epoch 15970/20000 Validation Loss: 0.07024264335632324\n",
      "Epoch 15971/20000 Training Loss: 0.06009705364704132\n",
      "Epoch 15972/20000 Training Loss: 0.05053358152508736\n",
      "Epoch 15973/20000 Training Loss: 0.0578189492225647\n",
      "Epoch 15974/20000 Training Loss: 0.052858609706163406\n",
      "Epoch 15975/20000 Training Loss: 0.03617938235402107\n",
      "Epoch 15976/20000 Training Loss: 0.06140217185020447\n",
      "Epoch 15977/20000 Training Loss: 0.059898603707551956\n",
      "Epoch 15978/20000 Training Loss: 0.04335659369826317\n",
      "Epoch 15979/20000 Training Loss: 0.044875819236040115\n",
      "Epoch 15980/20000 Training Loss: 0.04726414009928703\n",
      "Epoch 15980/20000 Validation Loss: 0.05788315832614899\n",
      "Epoch 15981/20000 Training Loss: 0.05446849390864372\n",
      "Epoch 15982/20000 Training Loss: 0.04034215584397316\n",
      "Epoch 15983/20000 Training Loss: 0.04329897463321686\n",
      "Epoch 15984/20000 Training Loss: 0.049537599086761475\n",
      "Epoch 15985/20000 Training Loss: 0.0521683543920517\n",
      "Epoch 15986/20000 Training Loss: 0.05043412372469902\n",
      "Epoch 15987/20000 Training Loss: 0.0547315888106823\n",
      "Epoch 15988/20000 Training Loss: 0.0667743980884552\n",
      "Epoch 15989/20000 Training Loss: 0.06918696314096451\n",
      "Epoch 15990/20000 Training Loss: 0.0740058645606041\n",
      "Epoch 15990/20000 Validation Loss: 0.058132849633693695\n",
      "Epoch 15991/20000 Training Loss: 0.052285339683294296\n",
      "Epoch 15992/20000 Training Loss: 0.05280967429280281\n",
      "Epoch 15993/20000 Training Loss: 0.0550643615424633\n",
      "Epoch 15994/20000 Training Loss: 0.06165625527501106\n",
      "Epoch 15995/20000 Training Loss: 0.05475806072354317\n",
      "Epoch 15996/20000 Training Loss: 0.045222941786050797\n",
      "Epoch 15997/20000 Training Loss: 0.05444846674799919\n",
      "Epoch 15998/20000 Training Loss: 0.04400387406349182\n",
      "Epoch 15999/20000 Training Loss: 0.0489359050989151\n",
      "Epoch 16000/20000 Training Loss: 0.05125229060649872\n",
      "Epoch 16000/20000 Validation Loss: 0.053547345101833344\n",
      "Epoch 16001/20000 Training Loss: 0.05692404881119728\n",
      "Epoch 16002/20000 Training Loss: 0.04917246475815773\n",
      "Epoch 16003/20000 Training Loss: 0.07427815347909927\n",
      "Epoch 16004/20000 Training Loss: 0.037211205810308456\n",
      "Epoch 16005/20000 Training Loss: 0.05782337859272957\n",
      "Epoch 16006/20000 Training Loss: 0.05541497841477394\n",
      "Epoch 16007/20000 Training Loss: 0.03790915384888649\n",
      "Epoch 16008/20000 Training Loss: 0.0539289154112339\n",
      "Epoch 16009/20000 Training Loss: 0.03694923222064972\n",
      "Epoch 16010/20000 Training Loss: 0.07949966192245483\n",
      "Epoch 16010/20000 Validation Loss: 0.056509122252464294\n",
      "Epoch 16011/20000 Training Loss: 0.05284178629517555\n",
      "Epoch 16012/20000 Training Loss: 0.059839148074388504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16013/20000 Training Loss: 0.06454384326934814\n",
      "Epoch 16014/20000 Training Loss: 0.058848459273576736\n",
      "Epoch 16015/20000 Training Loss: 0.05437518283724785\n",
      "Epoch 16016/20000 Training Loss: 0.05738363787531853\n",
      "Epoch 16017/20000 Training Loss: 0.05282006785273552\n",
      "Epoch 16018/20000 Training Loss: 0.04591110348701477\n",
      "Epoch 16019/20000 Training Loss: 0.05853733792901039\n",
      "Epoch 16020/20000 Training Loss: 0.06619490683078766\n",
      "Epoch 16020/20000 Validation Loss: 0.04851488769054413\n",
      "Epoch 16021/20000 Training Loss: 0.06424129754304886\n",
      "Epoch 16022/20000 Training Loss: 0.04004691541194916\n",
      "Epoch 16023/20000 Training Loss: 0.04509354010224342\n",
      "Epoch 16024/20000 Training Loss: 0.058580875396728516\n",
      "Epoch 16025/20000 Training Loss: 0.05522904917597771\n",
      "Epoch 16026/20000 Training Loss: 0.037868861109018326\n",
      "Epoch 16027/20000 Training Loss: 0.052569206804037094\n",
      "Epoch 16028/20000 Training Loss: 0.05768100917339325\n",
      "Epoch 16029/20000 Training Loss: 0.0729074701666832\n",
      "Epoch 16030/20000 Training Loss: 0.06030074879527092\n",
      "Epoch 16030/20000 Validation Loss: 0.0567665621638298\n",
      "Epoch 16031/20000 Training Loss: 0.04703832045197487\n",
      "Epoch 16032/20000 Training Loss: 0.052776262164115906\n",
      "Epoch 16033/20000 Training Loss: 0.042841117829084396\n",
      "Epoch 16034/20000 Training Loss: 0.04091387614607811\n",
      "Epoch 16035/20000 Training Loss: 0.058474015444517136\n",
      "Epoch 16036/20000 Training Loss: 0.04728841781616211\n",
      "Epoch 16037/20000 Training Loss: 0.055875301361083984\n",
      "Epoch 16038/20000 Training Loss: 0.05488796904683113\n",
      "Epoch 16039/20000 Training Loss: 0.07144232839345932\n",
      "Epoch 16040/20000 Training Loss: 0.03519931063055992\n",
      "Epoch 16040/20000 Validation Loss: 0.0569014847278595\n",
      "Epoch 16041/20000 Training Loss: 0.05264700576663017\n",
      "Epoch 16042/20000 Training Loss: 0.05460047721862793\n",
      "Epoch 16043/20000 Training Loss: 0.053942009806632996\n",
      "Epoch 16044/20000 Training Loss: 0.04333295300602913\n",
      "Epoch 16045/20000 Training Loss: 0.04094821214675903\n",
      "Epoch 16046/20000 Training Loss: 0.043533727526664734\n",
      "Epoch 16047/20000 Training Loss: 0.05678955093026161\n",
      "Epoch 16048/20000 Training Loss: 0.05525493249297142\n",
      "Epoch 16049/20000 Training Loss: 0.03534497693181038\n",
      "Epoch 16050/20000 Training Loss: 0.0646500289440155\n",
      "Epoch 16050/20000 Validation Loss: 0.053775355219841\n",
      "Epoch 16051/20000 Training Loss: 0.06454229354858398\n",
      "Epoch 16052/20000 Training Loss: 0.0497809499502182\n",
      "Epoch 16053/20000 Training Loss: 0.04898667335510254\n",
      "Epoch 16054/20000 Training Loss: 0.04141058400273323\n",
      "Epoch 16055/20000 Training Loss: 0.04935505613684654\n",
      "Epoch 16056/20000 Training Loss: 0.05325988307595253\n",
      "Epoch 16057/20000 Training Loss: 0.05817210674285889\n",
      "Epoch 16058/20000 Training Loss: 0.05696757137775421\n",
      "Epoch 16059/20000 Training Loss: 0.05351294204592705\n",
      "Epoch 16060/20000 Training Loss: 0.048127446323633194\n",
      "Epoch 16060/20000 Validation Loss: 0.043550681322813034\n",
      "Epoch 16061/20000 Training Loss: 0.05548560619354248\n",
      "Epoch 16062/20000 Training Loss: 0.05183153226971626\n",
      "Epoch 16063/20000 Training Loss: 0.06255295127630234\n",
      "Epoch 16064/20000 Training Loss: 0.04890149459242821\n",
      "Epoch 16065/20000 Training Loss: 0.06327024847269058\n",
      "Epoch 16066/20000 Training Loss: 0.04793098568916321\n",
      "Epoch 16067/20000 Training Loss: 0.04420844838023186\n",
      "Epoch 16068/20000 Training Loss: 0.07501918822526932\n",
      "Epoch 16069/20000 Training Loss: 0.05536048486828804\n",
      "Epoch 16070/20000 Training Loss: 0.07117191702127457\n",
      "Epoch 16070/20000 Validation Loss: 0.05777865648269653\n",
      "Epoch 16071/20000 Training Loss: 0.07006626576185226\n",
      "Epoch 16072/20000 Training Loss: 0.06326166540384293\n",
      "Epoch 16073/20000 Training Loss: 0.042687639594078064\n",
      "Epoch 16074/20000 Training Loss: 0.04324382543563843\n",
      "Epoch 16075/20000 Training Loss: 0.0713522732257843\n",
      "Epoch 16076/20000 Training Loss: 0.04522223770618439\n",
      "Epoch 16077/20000 Training Loss: 0.0581117607653141\n",
      "Epoch 16078/20000 Training Loss: 0.06307481974363327\n",
      "Epoch 16079/20000 Training Loss: 0.06296246498823166\n",
      "Epoch 16080/20000 Training Loss: 0.0685717910528183\n",
      "Epoch 16080/20000 Validation Loss: 0.057008806616067886\n",
      "Epoch 16081/20000 Training Loss: 0.0676518902182579\n",
      "Epoch 16082/20000 Training Loss: 0.04605895280838013\n",
      "Epoch 16083/20000 Training Loss: 0.05627799034118652\n",
      "Epoch 16084/20000 Training Loss: 0.07135165482759476\n",
      "Epoch 16085/20000 Training Loss: 0.05846250429749489\n",
      "Epoch 16086/20000 Training Loss: 0.03882850334048271\n",
      "Epoch 16087/20000 Training Loss: 0.050007183104753494\n",
      "Epoch 16088/20000 Training Loss: 0.05218155309557915\n",
      "Epoch 16089/20000 Training Loss: 0.0475267618894577\n",
      "Epoch 16090/20000 Training Loss: 0.04552416875958443\n",
      "Epoch 16090/20000 Validation Loss: 0.03375571593642235\n",
      "Epoch 16091/20000 Training Loss: 0.056233230978250504\n",
      "Epoch 16092/20000 Training Loss: 0.04870617389678955\n",
      "Epoch 16093/20000 Training Loss: 0.05322945490479469\n",
      "Epoch 16094/20000 Training Loss: 0.0598655641078949\n",
      "Epoch 16095/20000 Training Loss: 0.0409788116812706\n",
      "Epoch 16096/20000 Training Loss: 0.064020074903965\n",
      "Epoch 16097/20000 Training Loss: 0.06546013802289963\n",
      "Epoch 16098/20000 Training Loss: 0.0778420940041542\n",
      "Epoch 16099/20000 Training Loss: 0.0467190183699131\n",
      "Epoch 16100/20000 Training Loss: 0.056929707527160645\n",
      "Epoch 16100/20000 Validation Loss: 0.03700496256351471\n",
      "Epoch 16101/20000 Training Loss: 0.06548061221837997\n",
      "Epoch 16102/20000 Training Loss: 0.038851115852594376\n",
      "Epoch 16103/20000 Training Loss: 0.04343850538134575\n",
      "Epoch 16104/20000 Training Loss: 0.04647856950759888\n",
      "Epoch 16105/20000 Training Loss: 0.05501696467399597\n",
      "Epoch 16106/20000 Training Loss: 0.042512744665145874\n",
      "Epoch 16107/20000 Training Loss: 0.03987477347254753\n",
      "Epoch 16108/20000 Training Loss: 0.055255185812711716\n",
      "Epoch 16109/20000 Training Loss: 0.04711679741740227\n",
      "Epoch 16110/20000 Training Loss: 0.05043299123644829\n",
      "Epoch 16110/20000 Validation Loss: 0.04712057113647461\n",
      "Epoch 16111/20000 Training Loss: 0.05921204388141632\n",
      "Epoch 16112/20000 Training Loss: 0.03999561443924904\n",
      "Epoch 16113/20000 Training Loss: 0.05049709603190422\n",
      "Epoch 16114/20000 Training Loss: 0.06792879849672318\n",
      "Epoch 16115/20000 Training Loss: 0.06158588454127312\n",
      "Epoch 16116/20000 Training Loss: 0.04648355022072792\n",
      "Epoch 16117/20000 Training Loss: 0.0564480721950531\n",
      "Epoch 16118/20000 Training Loss: 0.05801498889923096\n",
      "Epoch 16119/20000 Training Loss: 0.06737770885229111\n",
      "Epoch 16120/20000 Training Loss: 0.057514626532793045\n",
      "Epoch 16120/20000 Validation Loss: 0.03501654043793678\n",
      "Epoch 16121/20000 Training Loss: 0.052147697657346725\n",
      "Epoch 16122/20000 Training Loss: 0.04824388399720192\n",
      "Epoch 16123/20000 Training Loss: 0.05065440014004707\n",
      "Epoch 16124/20000 Training Loss: 0.06331267207860947\n",
      "Epoch 16125/20000 Training Loss: 0.03713264688849449\n",
      "Epoch 16126/20000 Training Loss: 0.03850797191262245\n",
      "Epoch 16127/20000 Training Loss: 0.049868181347846985\n",
      "Epoch 16128/20000 Training Loss: 0.05558612942695618\n",
      "Epoch 16129/20000 Training Loss: 0.07580164074897766\n",
      "Epoch 16130/20000 Training Loss: 0.06258872896432877\n",
      "Epoch 16130/20000 Validation Loss: 0.04750378429889679\n",
      "Epoch 16131/20000 Training Loss: 0.051029548048973083\n",
      "Epoch 16132/20000 Training Loss: 0.05906342342495918\n",
      "Epoch 16133/20000 Training Loss: 0.06292527168989182\n",
      "Epoch 16134/20000 Training Loss: 0.049553561955690384\n",
      "Epoch 16135/20000 Training Loss: 0.063372902572155\n",
      "Epoch 16136/20000 Training Loss: 0.05444677546620369\n",
      "Epoch 16137/20000 Training Loss: 0.037621546536684036\n",
      "Epoch 16138/20000 Training Loss: 0.05781931057572365\n",
      "Epoch 16139/20000 Training Loss: 0.06490763276815414\n",
      "Epoch 16140/20000 Training Loss: 0.04897766932845116\n",
      "Epoch 16140/20000 Validation Loss: 0.07153242081403732\n",
      "Epoch 16141/20000 Training Loss: 0.0603453665971756\n",
      "Epoch 16142/20000 Training Loss: 0.04542672261595726\n",
      "Epoch 16143/20000 Training Loss: 0.05266903340816498\n",
      "Epoch 16144/20000 Training Loss: 0.06055689975619316\n",
      "Epoch 16145/20000 Training Loss: 0.046322088688611984\n",
      "Epoch 16146/20000 Training Loss: 0.0479976087808609\n",
      "Epoch 16147/20000 Training Loss: 0.0421256385743618\n",
      "Epoch 16148/20000 Training Loss: 0.06372193247079849\n",
      "Epoch 16149/20000 Training Loss: 0.05819888785481453\n",
      "Epoch 16150/20000 Training Loss: 0.05892520770430565\n",
      "Epoch 16150/20000 Validation Loss: 0.07935728132724762\n",
      "Epoch 16151/20000 Training Loss: 0.062369268387556076\n",
      "Epoch 16152/20000 Training Loss: 0.059844955801963806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16153/20000 Training Loss: 0.0740339532494545\n",
      "Epoch 16154/20000 Training Loss: 0.04597513750195503\n",
      "Epoch 16155/20000 Training Loss: 0.07155156880617142\n",
      "Epoch 16156/20000 Training Loss: 0.055113863199949265\n",
      "Epoch 16157/20000 Training Loss: 0.03956826031208038\n",
      "Epoch 16158/20000 Training Loss: 0.04550521448254585\n",
      "Epoch 16159/20000 Training Loss: 0.050663501024246216\n",
      "Epoch 16160/20000 Training Loss: 0.059375036507844925\n",
      "Epoch 16160/20000 Validation Loss: 0.08294688910245895\n",
      "Epoch 16161/20000 Training Loss: 0.05611085891723633\n",
      "Epoch 16162/20000 Training Loss: 0.04399794340133667\n",
      "Epoch 16163/20000 Training Loss: 0.056789297610521317\n",
      "Epoch 16164/20000 Training Loss: 0.043357476592063904\n",
      "Epoch 16165/20000 Training Loss: 0.04765264317393303\n",
      "Epoch 16166/20000 Training Loss: 0.07976862043142319\n",
      "Epoch 16167/20000 Training Loss: 0.04025327041745186\n",
      "Epoch 16168/20000 Training Loss: 0.0428803525865078\n",
      "Epoch 16169/20000 Training Loss: 0.04918403550982475\n",
      "Epoch 16170/20000 Training Loss: 0.046771977096796036\n",
      "Epoch 16170/20000 Validation Loss: 0.0477764755487442\n",
      "Epoch 16171/20000 Training Loss: 0.060951460152864456\n",
      "Epoch 16172/20000 Training Loss: 0.05272728204727173\n",
      "Epoch 16173/20000 Training Loss: 0.07028799504041672\n",
      "Epoch 16174/20000 Training Loss: 0.06090495362877846\n",
      "Epoch 16175/20000 Training Loss: 0.04383505508303642\n",
      "Epoch 16176/20000 Training Loss: 0.057434260845184326\n",
      "Epoch 16177/20000 Training Loss: 0.04320433363318443\n",
      "Epoch 16178/20000 Training Loss: 0.06830143183469772\n",
      "Epoch 16179/20000 Training Loss: 0.049256082624197006\n",
      "Epoch 16180/20000 Training Loss: 0.03783582150936127\n",
      "Epoch 16180/20000 Validation Loss: 0.09440465271472931\n",
      "Epoch 16181/20000 Training Loss: 0.04846234992146492\n",
      "Epoch 16182/20000 Training Loss: 0.06279168277978897\n",
      "Epoch 16183/20000 Training Loss: 0.04790760204195976\n",
      "Epoch 16184/20000 Training Loss: 0.06196783483028412\n",
      "Epoch 16185/20000 Training Loss: 0.06523259729146957\n",
      "Epoch 16186/20000 Training Loss: 0.044543344527482986\n",
      "Epoch 16187/20000 Training Loss: 0.0566575787961483\n",
      "Epoch 16188/20000 Training Loss: 0.04499883949756622\n",
      "Epoch 16189/20000 Training Loss: 0.0616459958255291\n",
      "Epoch 16190/20000 Training Loss: 0.0467706173658371\n",
      "Epoch 16190/20000 Validation Loss: 0.044128429144620895\n",
      "Epoch 16191/20000 Training Loss: 0.053823407739400864\n",
      "Epoch 16192/20000 Training Loss: 0.04116220772266388\n",
      "Epoch 16193/20000 Training Loss: 0.05334174633026123\n",
      "Epoch 16194/20000 Training Loss: 0.06846529245376587\n",
      "Epoch 16195/20000 Training Loss: 0.048224691301584244\n",
      "Epoch 16196/20000 Training Loss: 0.05170520767569542\n",
      "Epoch 16197/20000 Training Loss: 0.04282471537590027\n",
      "Epoch 16198/20000 Training Loss: 0.06347294896841049\n",
      "Epoch 16199/20000 Training Loss: 0.07721216231584549\n",
      "Epoch 16200/20000 Training Loss: 0.04966836795210838\n",
      "Epoch 16200/20000 Validation Loss: 0.050814189016819\n",
      "Epoch 16201/20000 Training Loss: 0.0435008704662323\n",
      "Epoch 16202/20000 Training Loss: 0.05549582839012146\n",
      "Epoch 16203/20000 Training Loss: 0.05800280347466469\n",
      "Epoch 16204/20000 Training Loss: 0.0681665912270546\n",
      "Epoch 16205/20000 Training Loss: 0.043448250740766525\n",
      "Epoch 16206/20000 Training Loss: 0.052947670221328735\n",
      "Epoch 16207/20000 Training Loss: 0.04399691894650459\n",
      "Epoch 16208/20000 Training Loss: 0.05303643271327019\n",
      "Epoch 16209/20000 Training Loss: 0.06649845838546753\n",
      "Epoch 16210/20000 Training Loss: 0.06122398376464844\n",
      "Epoch 16210/20000 Validation Loss: 0.04616948589682579\n",
      "Epoch 16211/20000 Training Loss: 0.06633532047271729\n",
      "Epoch 16212/20000 Training Loss: 0.0587475560605526\n",
      "Epoch 16213/20000 Training Loss: 0.05965835973620415\n",
      "Epoch 16214/20000 Training Loss: 0.04288835451006889\n",
      "Epoch 16215/20000 Training Loss: 0.054236214607954025\n",
      "Epoch 16216/20000 Training Loss: 0.05460260435938835\n",
      "Epoch 16217/20000 Training Loss: 0.049991726875305176\n",
      "Epoch 16218/20000 Training Loss: 0.055695999413728714\n",
      "Epoch 16219/20000 Training Loss: 0.05456135794520378\n",
      "Epoch 16220/20000 Training Loss: 0.05950380861759186\n",
      "Epoch 16220/20000 Validation Loss: 0.03951616212725639\n",
      "Epoch 16221/20000 Training Loss: 0.0685458779335022\n",
      "Epoch 16222/20000 Training Loss: 0.039546333253383636\n",
      "Epoch 16223/20000 Training Loss: 0.07087740302085876\n",
      "Epoch 16224/20000 Training Loss: 0.04946991801261902\n",
      "Epoch 16225/20000 Training Loss: 0.04754526913166046\n",
      "Epoch 16226/20000 Training Loss: 0.04870319738984108\n",
      "Epoch 16227/20000 Training Loss: 0.04883280023932457\n",
      "Epoch 16228/20000 Training Loss: 0.0539763979613781\n",
      "Epoch 16229/20000 Training Loss: 0.06751944869756699\n",
      "Epoch 16230/20000 Training Loss: 0.05048099532723427\n",
      "Epoch 16230/20000 Validation Loss: 0.07078839093446732\n",
      "Epoch 16231/20000 Training Loss: 0.053447429090738297\n",
      "Epoch 16232/20000 Training Loss: 0.037970781326293945\n",
      "Epoch 16233/20000 Training Loss: 0.07014582306146622\n",
      "Epoch 16234/20000 Training Loss: 0.061792388558387756\n",
      "Epoch 16235/20000 Training Loss: 0.057914119213819504\n",
      "Epoch 16236/20000 Training Loss: 0.045343562960624695\n",
      "Epoch 16237/20000 Training Loss: 0.048132676631212234\n",
      "Epoch 16238/20000 Training Loss: 0.04808443784713745\n",
      "Epoch 16239/20000 Training Loss: 0.047214243561029434\n",
      "Epoch 16240/20000 Training Loss: 0.05793848633766174\n",
      "Epoch 16240/20000 Validation Loss: 0.08235225081443787\n",
      "Epoch 16241/20000 Training Loss: 0.06568592041730881\n",
      "Epoch 16242/20000 Training Loss: 0.05500245466828346\n",
      "Epoch 16243/20000 Training Loss: 0.06734529137611389\n",
      "Epoch 16244/20000 Training Loss: 0.06615200638771057\n",
      "Epoch 16245/20000 Training Loss: 0.05392351746559143\n",
      "Epoch 16246/20000 Training Loss: 0.054804038256406784\n",
      "Epoch 16247/20000 Training Loss: 0.06709105521440506\n",
      "Epoch 16248/20000 Training Loss: 0.051797930151224136\n",
      "Epoch 16249/20000 Training Loss: 0.05741700157523155\n",
      "Epoch 16250/20000 Training Loss: 0.06655196100473404\n",
      "Epoch 16250/20000 Validation Loss: 0.05121203511953354\n",
      "Epoch 16251/20000 Training Loss: 0.0612088106572628\n",
      "Epoch 16252/20000 Training Loss: 0.05256335437297821\n",
      "Epoch 16253/20000 Training Loss: 0.03921027109026909\n",
      "Epoch 16254/20000 Training Loss: 0.05303051695227623\n",
      "Epoch 16255/20000 Training Loss: 0.06073068454861641\n",
      "Epoch 16256/20000 Training Loss: 0.05390777811408043\n",
      "Epoch 16257/20000 Training Loss: 0.056206539273262024\n",
      "Epoch 16258/20000 Training Loss: 0.06477764248847961\n",
      "Epoch 16259/20000 Training Loss: 0.04409876465797424\n",
      "Epoch 16260/20000 Training Loss: 0.06893425434827805\n",
      "Epoch 16260/20000 Validation Loss: 0.06631051748991013\n",
      "Epoch 16261/20000 Training Loss: 0.04835410788655281\n",
      "Epoch 16262/20000 Training Loss: 0.06551707535982132\n",
      "Epoch 16263/20000 Training Loss: 0.04157967492938042\n",
      "Epoch 16264/20000 Training Loss: 0.05596909299492836\n",
      "Epoch 16265/20000 Training Loss: 0.05384337902069092\n",
      "Epoch 16266/20000 Training Loss: 0.04751312732696533\n",
      "Epoch 16267/20000 Training Loss: 0.06756766885519028\n",
      "Epoch 16268/20000 Training Loss: 0.0514155738055706\n",
      "Epoch 16269/20000 Training Loss: 0.05822025611996651\n",
      "Epoch 16270/20000 Training Loss: 0.06528585404157639\n",
      "Epoch 16270/20000 Validation Loss: 0.040620286017656326\n",
      "Epoch 16271/20000 Training Loss: 0.054710790514945984\n",
      "Epoch 16272/20000 Training Loss: 0.05971856787800789\n",
      "Epoch 16273/20000 Training Loss: 0.035920143127441406\n",
      "Epoch 16274/20000 Training Loss: 0.05210323631763458\n",
      "Epoch 16275/20000 Training Loss: 0.039588939398527145\n",
      "Epoch 16276/20000 Training Loss: 0.05326347425580025\n",
      "Epoch 16277/20000 Training Loss: 0.058917056769132614\n",
      "Epoch 16278/20000 Training Loss: 0.05081019923090935\n",
      "Epoch 16279/20000 Training Loss: 0.07384505867958069\n",
      "Epoch 16280/20000 Training Loss: 0.04702458903193474\n",
      "Epoch 16280/20000 Validation Loss: 0.04100348800420761\n",
      "Epoch 16281/20000 Training Loss: 0.06899062544107437\n",
      "Epoch 16282/20000 Training Loss: 0.0634659081697464\n",
      "Epoch 16283/20000 Training Loss: 0.048603612929582596\n",
      "Epoch 16284/20000 Training Loss: 0.042175937443971634\n",
      "Epoch 16285/20000 Training Loss: 0.0474722795188427\n",
      "Epoch 16286/20000 Training Loss: 0.04728490486741066\n",
      "Epoch 16287/20000 Training Loss: 0.05021113529801369\n",
      "Epoch 16288/20000 Training Loss: 0.04361643269658089\n",
      "Epoch 16289/20000 Training Loss: 0.05830322578549385\n",
      "Epoch 16290/20000 Training Loss: 0.053449273109436035\n",
      "Epoch 16290/20000 Validation Loss: 0.05389472097158432\n",
      "Epoch 16291/20000 Training Loss: 0.059489037841558456\n",
      "Epoch 16292/20000 Training Loss: 0.04434504732489586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16293/20000 Training Loss: 0.061232611536979675\n",
      "Epoch 16294/20000 Training Loss: 0.051983144134283066\n",
      "Epoch 16295/20000 Training Loss: 0.09560062736272812\n",
      "Epoch 16296/20000 Training Loss: 0.06003672257065773\n",
      "Epoch 16297/20000 Training Loss: 0.04654723405838013\n",
      "Epoch 16298/20000 Training Loss: 0.042339861392974854\n",
      "Epoch 16299/20000 Training Loss: 0.04425857961177826\n",
      "Epoch 16300/20000 Training Loss: 0.04869337007403374\n",
      "Epoch 16300/20000 Validation Loss: 0.0557110458612442\n",
      "Epoch 16301/20000 Training Loss: 0.04971585050225258\n",
      "Epoch 16302/20000 Training Loss: 0.05598745867609978\n",
      "Epoch 16303/20000 Training Loss: 0.049329619854688644\n",
      "Epoch 16304/20000 Training Loss: 0.06338221579790115\n",
      "Epoch 16305/20000 Training Loss: 0.04816238209605217\n",
      "Epoch 16306/20000 Training Loss: 0.04856650158762932\n",
      "Epoch 16307/20000 Training Loss: 0.05993877351284027\n",
      "Epoch 16308/20000 Training Loss: 0.055361051112413406\n",
      "Epoch 16309/20000 Training Loss: 0.048342034220695496\n",
      "Epoch 16310/20000 Training Loss: 0.07055256515741348\n",
      "Epoch 16310/20000 Validation Loss: 0.03867451846599579\n",
      "Epoch 16311/20000 Training Loss: 0.050935178995132446\n",
      "Epoch 16312/20000 Training Loss: 0.047824084758758545\n",
      "Epoch 16313/20000 Training Loss: 0.05539877340197563\n",
      "Epoch 16314/20000 Training Loss: 0.05470287799835205\n",
      "Epoch 16315/20000 Training Loss: 0.0516054593026638\n",
      "Epoch 16316/20000 Training Loss: 0.05635958909988403\n",
      "Epoch 16317/20000 Training Loss: 0.04948095604777336\n",
      "Epoch 16318/20000 Training Loss: 0.06202417239546776\n",
      "Epoch 16319/20000 Training Loss: 0.05589577183127403\n",
      "Epoch 16320/20000 Training Loss: 0.06366237252950668\n",
      "Epoch 16320/20000 Validation Loss: 0.07452668249607086\n",
      "Epoch 16321/20000 Training Loss: 0.06098230183124542\n",
      "Epoch 16322/20000 Training Loss: 0.04511214420199394\n",
      "Epoch 16323/20000 Training Loss: 0.048648908734321594\n",
      "Epoch 16324/20000 Training Loss: 0.047769855707883835\n",
      "Epoch 16325/20000 Training Loss: 0.05138444900512695\n",
      "Epoch 16326/20000 Training Loss: 0.05764772370457649\n",
      "Epoch 16327/20000 Training Loss: 0.05015910044312477\n",
      "Epoch 16328/20000 Training Loss: 0.05359600856900215\n",
      "Epoch 16329/20000 Training Loss: 0.04655221104621887\n",
      "Epoch 16330/20000 Training Loss: 0.049484629184007645\n",
      "Epoch 16330/20000 Validation Loss: 0.06685543060302734\n",
      "Epoch 16331/20000 Training Loss: 0.0369504876434803\n",
      "Epoch 16332/20000 Training Loss: 0.0441729836165905\n",
      "Epoch 16333/20000 Training Loss: 0.06689588725566864\n",
      "Epoch 16334/20000 Training Loss: 0.044788017868995667\n",
      "Epoch 16335/20000 Training Loss: 0.04179412126541138\n",
      "Epoch 16336/20000 Training Loss: 0.04647204652428627\n",
      "Epoch 16337/20000 Training Loss: 0.05607367679476738\n",
      "Epoch 16338/20000 Training Loss: 0.05555063858628273\n",
      "Epoch 16339/20000 Training Loss: 0.05120064690709114\n",
      "Epoch 16340/20000 Training Loss: 0.048845041543245316\n",
      "Epoch 16340/20000 Validation Loss: 0.06191559508442879\n",
      "Epoch 16341/20000 Training Loss: 0.058853209018707275\n",
      "Epoch 16342/20000 Training Loss: 0.044447947293519974\n",
      "Epoch 16343/20000 Training Loss: 0.05514645576477051\n",
      "Epoch 16344/20000 Training Loss: 0.04567686840891838\n",
      "Epoch 16345/20000 Training Loss: 0.058697398751974106\n",
      "Epoch 16346/20000 Training Loss: 0.051577985286712646\n",
      "Epoch 16347/20000 Training Loss: 0.061162132769823074\n",
      "Epoch 16348/20000 Training Loss: 0.045731525868177414\n",
      "Epoch 16349/20000 Training Loss: 0.05354760214686394\n",
      "Epoch 16350/20000 Training Loss: 0.062077272683382034\n",
      "Epoch 16350/20000 Validation Loss: 0.06937003880739212\n",
      "Epoch 16351/20000 Training Loss: 0.05807134136557579\n",
      "Epoch 16352/20000 Training Loss: 0.05093669518828392\n",
      "Epoch 16353/20000 Training Loss: 0.05665867403149605\n",
      "Epoch 16354/20000 Training Loss: 0.05211004987359047\n",
      "Epoch 16355/20000 Training Loss: 0.05690282583236694\n",
      "Epoch 16356/20000 Training Loss: 0.05976635217666626\n",
      "Epoch 16357/20000 Training Loss: 0.05553055927157402\n",
      "Epoch 16358/20000 Training Loss: 0.065729059278965\n",
      "Epoch 16359/20000 Training Loss: 0.06417468935251236\n",
      "Epoch 16360/20000 Training Loss: 0.03728744760155678\n",
      "Epoch 16360/20000 Validation Loss: 0.08513963967561722\n",
      "Epoch 16361/20000 Training Loss: 0.05930681154131889\n",
      "Epoch 16362/20000 Training Loss: 0.051707834005355835\n",
      "Epoch 16363/20000 Training Loss: 0.05638381838798523\n",
      "Epoch 16364/20000 Training Loss: 0.05023748055100441\n",
      "Epoch 16365/20000 Training Loss: 0.053994763642549515\n",
      "Epoch 16366/20000 Training Loss: 0.05354824662208557\n",
      "Epoch 16367/20000 Training Loss: 0.0718228742480278\n",
      "Epoch 16368/20000 Training Loss: 0.06177988648414612\n",
      "Epoch 16369/20000 Training Loss: 0.05414462089538574\n",
      "Epoch 16370/20000 Training Loss: 0.06068902835249901\n",
      "Epoch 16370/20000 Validation Loss: 0.05285291373729706\n",
      "Epoch 16371/20000 Training Loss: 0.058299720287323\n",
      "Epoch 16372/20000 Training Loss: 0.06400076299905777\n",
      "Epoch 16373/20000 Training Loss: 0.06428740173578262\n",
      "Epoch 16374/20000 Training Loss: 0.0547083355486393\n",
      "Epoch 16375/20000 Training Loss: 0.04995347186923027\n",
      "Epoch 16376/20000 Training Loss: 0.0731971487402916\n",
      "Epoch 16377/20000 Training Loss: 0.052066754549741745\n",
      "Epoch 16378/20000 Training Loss: 0.055760353803634644\n",
      "Epoch 16379/20000 Training Loss: 0.054915785789489746\n",
      "Epoch 16380/20000 Training Loss: 0.051175806671381\n",
      "Epoch 16380/20000 Validation Loss: 0.08601163327693939\n",
      "Epoch 16381/20000 Training Loss: 0.05878777429461479\n",
      "Epoch 16382/20000 Training Loss: 0.04974595829844475\n",
      "Epoch 16383/20000 Training Loss: 0.05160476267337799\n",
      "Epoch 16384/20000 Training Loss: 0.06445259600877762\n",
      "Epoch 16385/20000 Training Loss: 0.06054019555449486\n",
      "Epoch 16386/20000 Training Loss: 0.07327700406312943\n",
      "Epoch 16387/20000 Training Loss: 0.05411103367805481\n",
      "Epoch 16388/20000 Training Loss: 0.06011324003338814\n",
      "Epoch 16389/20000 Training Loss: 0.06513628363609314\n",
      "Epoch 16390/20000 Training Loss: 0.06271759420633316\n",
      "Epoch 16390/20000 Validation Loss: 0.04472307860851288\n",
      "Epoch 16391/20000 Training Loss: 0.05500630661845207\n",
      "Epoch 16392/20000 Training Loss: 0.060529496520757675\n",
      "Epoch 16393/20000 Training Loss: 0.036705438047647476\n",
      "Epoch 16394/20000 Training Loss: 0.05302179977297783\n",
      "Epoch 16395/20000 Training Loss: 0.048153847455978394\n",
      "Epoch 16396/20000 Training Loss: 0.056102752685546875\n",
      "Epoch 16397/20000 Training Loss: 0.05789951607584953\n",
      "Epoch 16398/20000 Training Loss: 0.06190104782581329\n",
      "Epoch 16399/20000 Training Loss: 0.042442407459020615\n",
      "Epoch 16400/20000 Training Loss: 0.046715300530195236\n",
      "Epoch 16400/20000 Validation Loss: 0.06151636689901352\n",
      "Epoch 16401/20000 Training Loss: 0.06708309054374695\n",
      "Epoch 16402/20000 Training Loss: 0.04914155229926109\n",
      "Epoch 16403/20000 Training Loss: 0.05719659850001335\n",
      "Epoch 16404/20000 Training Loss: 0.07482852786779404\n",
      "Epoch 16405/20000 Training Loss: 0.06087459996342659\n",
      "Epoch 16406/20000 Training Loss: 0.05548570677638054\n",
      "Epoch 16407/20000 Training Loss: 0.05021106079220772\n",
      "Epoch 16408/20000 Training Loss: 0.04874208942055702\n",
      "Epoch 16409/20000 Training Loss: 0.05098444223403931\n",
      "Epoch 16410/20000 Training Loss: 0.052160393446683884\n",
      "Epoch 16410/20000 Validation Loss: 0.061764977872371674\n",
      "Epoch 16411/20000 Training Loss: 0.046245794743299484\n",
      "Epoch 16412/20000 Training Loss: 0.041528597474098206\n",
      "Epoch 16413/20000 Training Loss: 0.053286295384168625\n",
      "Epoch 16414/20000 Training Loss: 0.06344322115182877\n",
      "Epoch 16415/20000 Training Loss: 0.06446322798728943\n",
      "Epoch 16416/20000 Training Loss: 0.03764478489756584\n",
      "Epoch 16417/20000 Training Loss: 0.04991859570145607\n",
      "Epoch 16418/20000 Training Loss: 0.03900744393467903\n",
      "Epoch 16419/20000 Training Loss: 0.07084512710571289\n",
      "Epoch 16420/20000 Training Loss: 0.05187647417187691\n",
      "Epoch 16420/20000 Validation Loss: 0.055984269827604294\n",
      "Epoch 16421/20000 Training Loss: 0.04885631799697876\n",
      "Epoch 16422/20000 Training Loss: 0.04254944622516632\n",
      "Epoch 16423/20000 Training Loss: 0.06973832100629807\n",
      "Epoch 16424/20000 Training Loss: 0.038483429700136185\n",
      "Epoch 16425/20000 Training Loss: 0.041964396834373474\n",
      "Epoch 16426/20000 Training Loss: 0.05292646214365959\n",
      "Epoch 16427/20000 Training Loss: 0.046663615852594376\n",
      "Epoch 16428/20000 Training Loss: 0.062444280833005905\n",
      "Epoch 16429/20000 Training Loss: 0.046292152255773544\n",
      "Epoch 16430/20000 Training Loss: 0.041610460728406906\n",
      "Epoch 16430/20000 Validation Loss: 0.056667260825634\n",
      "Epoch 16431/20000 Training Loss: 0.040361903607845306\n",
      "Epoch 16432/20000 Training Loss: 0.05923471972346306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16433/20000 Training Loss: 0.051411647349596024\n",
      "Epoch 16434/20000 Training Loss: 0.06343605369329453\n",
      "Epoch 16435/20000 Training Loss: 0.07669607549905777\n",
      "Epoch 16436/20000 Training Loss: 0.045212239027023315\n",
      "Epoch 16437/20000 Training Loss: 0.04521803557872772\n",
      "Epoch 16438/20000 Training Loss: 0.061484962701797485\n",
      "Epoch 16439/20000 Training Loss: 0.039981190115213394\n",
      "Epoch 16440/20000 Training Loss: 0.051585596054792404\n",
      "Epoch 16440/20000 Validation Loss: 0.07969404011964798\n",
      "Epoch 16441/20000 Training Loss: 0.05883358046412468\n",
      "Epoch 16442/20000 Training Loss: 0.049516160041093826\n",
      "Epoch 16443/20000 Training Loss: 0.0684959813952446\n",
      "Epoch 16444/20000 Training Loss: 0.05949528142809868\n",
      "Epoch 16445/20000 Training Loss: 0.05484204366803169\n",
      "Epoch 16446/20000 Training Loss: 0.06540051847696304\n",
      "Epoch 16447/20000 Training Loss: 0.05087864398956299\n",
      "Epoch 16448/20000 Training Loss: 0.03963944688439369\n",
      "Epoch 16449/20000 Training Loss: 0.050087977200746536\n",
      "Epoch 16450/20000 Training Loss: 0.04831648990511894\n",
      "Epoch 16450/20000 Validation Loss: 0.06010136753320694\n",
      "Epoch 16451/20000 Training Loss: 0.05721865966916084\n",
      "Epoch 16452/20000 Training Loss: 0.05397957190871239\n",
      "Epoch 16453/20000 Training Loss: 0.050590645521879196\n",
      "Epoch 16454/20000 Training Loss: 0.057172250002622604\n",
      "Epoch 16455/20000 Training Loss: 0.05807054042816162\n",
      "Epoch 16456/20000 Training Loss: 0.05713241174817085\n",
      "Epoch 16457/20000 Training Loss: 0.0641075000166893\n",
      "Epoch 16458/20000 Training Loss: 0.05148202180862427\n",
      "Epoch 16459/20000 Training Loss: 0.04487147927284241\n",
      "Epoch 16460/20000 Training Loss: 0.043575916439294815\n",
      "Epoch 16460/20000 Validation Loss: 0.04454215243458748\n",
      "Epoch 16461/20000 Training Loss: 0.06676702946424484\n",
      "Epoch 16462/20000 Training Loss: 0.0641159787774086\n",
      "Epoch 16463/20000 Training Loss: 0.03584332391619682\n",
      "Epoch 16464/20000 Training Loss: 0.05483412370085716\n",
      "Epoch 16465/20000 Training Loss: 0.05291685834527016\n",
      "Epoch 16466/20000 Training Loss: 0.03568718582391739\n",
      "Epoch 16467/20000 Training Loss: 0.05689145252108574\n",
      "Epoch 16468/20000 Training Loss: 0.04578277841210365\n",
      "Epoch 16469/20000 Training Loss: 0.05105079337954521\n",
      "Epoch 16470/20000 Training Loss: 0.04436282441020012\n",
      "Epoch 16470/20000 Validation Loss: 0.060338884592056274\n",
      "Epoch 16471/20000 Training Loss: 0.05224308371543884\n",
      "Epoch 16472/20000 Training Loss: 0.043056588619947433\n",
      "Epoch 16473/20000 Training Loss: 0.055956657975912094\n",
      "Epoch 16474/20000 Training Loss: 0.05166265740990639\n",
      "Epoch 16475/20000 Training Loss: 0.05489518120884895\n",
      "Epoch 16476/20000 Training Loss: 0.03858298063278198\n",
      "Epoch 16477/20000 Training Loss: 0.050599802285432816\n",
      "Epoch 16478/20000 Training Loss: 0.05632569268345833\n",
      "Epoch 16479/20000 Training Loss: 0.04165775328874588\n",
      "Epoch 16480/20000 Training Loss: 0.052521001547575\n",
      "Epoch 16480/20000 Validation Loss: 0.05491693317890167\n",
      "Epoch 16481/20000 Training Loss: 0.05307392776012421\n",
      "Epoch 16482/20000 Training Loss: 0.05214301124215126\n",
      "Epoch 16483/20000 Training Loss: 0.05119715631008148\n",
      "Epoch 16484/20000 Training Loss: 0.05592617392539978\n",
      "Epoch 16485/20000 Training Loss: 0.05744943022727966\n",
      "Epoch 16486/20000 Training Loss: 0.05109645798802376\n",
      "Epoch 16487/20000 Training Loss: 0.05633166432380676\n",
      "Epoch 16488/20000 Training Loss: 0.0529593825340271\n",
      "Epoch 16489/20000 Training Loss: 0.04694128409028053\n",
      "Epoch 16490/20000 Training Loss: 0.058898746967315674\n",
      "Epoch 16490/20000 Validation Loss: 0.053562723100185394\n",
      "Epoch 16491/20000 Training Loss: 0.055342841893434525\n",
      "Epoch 16492/20000 Training Loss: 0.041721731424331665\n",
      "Epoch 16493/20000 Training Loss: 0.04908761382102966\n",
      "Epoch 16494/20000 Training Loss: 0.047392163425683975\n",
      "Epoch 16495/20000 Training Loss: 0.048412516713142395\n",
      "Epoch 16496/20000 Training Loss: 0.043715670704841614\n",
      "Epoch 16497/20000 Training Loss: 0.043213099241256714\n",
      "Epoch 16498/20000 Training Loss: 0.045903876423835754\n",
      "Epoch 16499/20000 Training Loss: 0.04783920571208\n",
      "Epoch 16500/20000 Training Loss: 0.0541394017636776\n",
      "Epoch 16500/20000 Validation Loss: 0.057415466755628586\n",
      "Epoch 16501/20000 Training Loss: 0.04443422332406044\n",
      "Epoch 16502/20000 Training Loss: 0.045895595103502274\n",
      "Epoch 16503/20000 Training Loss: 0.04050484299659729\n",
      "Epoch 16504/20000 Training Loss: 0.04147384688258171\n",
      "Epoch 16505/20000 Training Loss: 0.05303817614912987\n",
      "Epoch 16506/20000 Training Loss: 0.05138752982020378\n",
      "Epoch 16507/20000 Training Loss: 0.05455778166651726\n",
      "Epoch 16508/20000 Training Loss: 0.06989546865224838\n",
      "Epoch 16509/20000 Training Loss: 0.06379278749227524\n",
      "Epoch 16510/20000 Training Loss: 0.04190761223435402\n",
      "Epoch 16510/20000 Validation Loss: 0.06888087093830109\n",
      "Epoch 16511/20000 Training Loss: 0.08158319443464279\n",
      "Epoch 16512/20000 Training Loss: 0.06376703828573227\n",
      "Epoch 16513/20000 Training Loss: 0.050160959362983704\n",
      "Epoch 16514/20000 Training Loss: 0.054165031760931015\n",
      "Epoch 16515/20000 Training Loss: 0.0469575934112072\n",
      "Epoch 16516/20000 Training Loss: 0.05671321973204613\n",
      "Epoch 16517/20000 Training Loss: 0.047331880778074265\n",
      "Epoch 16518/20000 Training Loss: 0.03535190224647522\n",
      "Epoch 16519/20000 Training Loss: 0.05720028653740883\n",
      "Epoch 16520/20000 Training Loss: 0.04137072339653969\n",
      "Epoch 16520/20000 Validation Loss: 0.07209892570972443\n",
      "Epoch 16521/20000 Training Loss: 0.05474749207496643\n",
      "Epoch 16522/20000 Training Loss: 0.06122317910194397\n",
      "Epoch 16523/20000 Training Loss: 0.034898240119218826\n",
      "Epoch 16524/20000 Training Loss: 0.0704035833477974\n",
      "Epoch 16525/20000 Training Loss: 0.06778661161661148\n",
      "Epoch 16526/20000 Training Loss: 0.04541819170117378\n",
      "Epoch 16527/20000 Training Loss: 0.04607240483164787\n",
      "Epoch 16528/20000 Training Loss: 0.06841062754392624\n",
      "Epoch 16529/20000 Training Loss: 0.05806019529700279\n",
      "Epoch 16530/20000 Training Loss: 0.0588531494140625\n",
      "Epoch 16530/20000 Validation Loss: 0.08890056610107422\n",
      "Epoch 16531/20000 Training Loss: 0.06447088718414307\n",
      "Epoch 16532/20000 Training Loss: 0.05112866684794426\n",
      "Epoch 16533/20000 Training Loss: 0.07059482485055923\n",
      "Epoch 16534/20000 Training Loss: 0.045514579862356186\n",
      "Epoch 16535/20000 Training Loss: 0.06820821762084961\n",
      "Epoch 16536/20000 Training Loss: 0.03453521430492401\n",
      "Epoch 16537/20000 Training Loss: 0.04811382666230202\n",
      "Epoch 16538/20000 Training Loss: 0.084952712059021\n",
      "Epoch 16539/20000 Training Loss: 0.049823034554719925\n",
      "Epoch 16540/20000 Training Loss: 0.054699838161468506\n",
      "Epoch 16540/20000 Validation Loss: 0.05005335807800293\n",
      "Epoch 16541/20000 Training Loss: 0.05904420092701912\n",
      "Epoch 16542/20000 Training Loss: 0.04901786521077156\n",
      "Epoch 16543/20000 Training Loss: 0.047374289482831955\n",
      "Epoch 16544/20000 Training Loss: 0.04396873712539673\n",
      "Epoch 16545/20000 Training Loss: 0.052636757493019104\n",
      "Epoch 16546/20000 Training Loss: 0.039749931544065475\n",
      "Epoch 16547/20000 Training Loss: 0.047259632498025894\n",
      "Epoch 16548/20000 Training Loss: 0.05732757970690727\n",
      "Epoch 16549/20000 Training Loss: 0.06293348222970963\n",
      "Epoch 16550/20000 Training Loss: 0.046754468232393265\n",
      "Epoch 16550/20000 Validation Loss: 0.05513465031981468\n",
      "Epoch 16551/20000 Training Loss: 0.05401696637272835\n",
      "Epoch 16552/20000 Training Loss: 0.055173859000205994\n",
      "Epoch 16553/20000 Training Loss: 0.08961763232946396\n",
      "Epoch 16554/20000 Training Loss: 0.06548897922039032\n",
      "Epoch 16555/20000 Training Loss: 0.046102285385131836\n",
      "Epoch 16556/20000 Training Loss: 0.06331773847341537\n",
      "Epoch 16557/20000 Training Loss: 0.05962352827191353\n",
      "Epoch 16558/20000 Training Loss: 0.05928157642483711\n",
      "Epoch 16559/20000 Training Loss: 0.06586111336946487\n",
      "Epoch 16560/20000 Training Loss: 0.057368457317352295\n",
      "Epoch 16560/20000 Validation Loss: 0.04952308535575867\n",
      "Epoch 16561/20000 Training Loss: 0.05068454518914223\n",
      "Epoch 16562/20000 Training Loss: 0.07348281145095825\n",
      "Epoch 16563/20000 Training Loss: 0.06359798461198807\n",
      "Epoch 16564/20000 Training Loss: 0.04888676479458809\n",
      "Epoch 16565/20000 Training Loss: 0.05770474299788475\n",
      "Epoch 16566/20000 Training Loss: 0.05394677445292473\n",
      "Epoch 16567/20000 Training Loss: 0.0651656910777092\n",
      "Epoch 16568/20000 Training Loss: 0.04207664728164673\n",
      "Epoch 16569/20000 Training Loss: 0.04992780089378357\n",
      "Epoch 16570/20000 Training Loss: 0.05533467233181\n",
      "Epoch 16570/20000 Validation Loss: 0.0527045913040638\n",
      "Epoch 16571/20000 Training Loss: 0.05839397385716438\n",
      "Epoch 16572/20000 Training Loss: 0.05270886421203613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16573/20000 Training Loss: 0.0543021522462368\n",
      "Epoch 16574/20000 Training Loss: 0.05102819576859474\n",
      "Epoch 16575/20000 Training Loss: 0.04141838103532791\n",
      "Epoch 16576/20000 Training Loss: 0.04969494417309761\n",
      "Epoch 16577/20000 Training Loss: 0.05086107924580574\n",
      "Epoch 16578/20000 Training Loss: 0.03323082998394966\n",
      "Epoch 16579/20000 Training Loss: 0.04756234213709831\n",
      "Epoch 16580/20000 Training Loss: 0.06444761157035828\n",
      "Epoch 16580/20000 Validation Loss: 0.057036951184272766\n",
      "Epoch 16581/20000 Training Loss: 0.044444818049669266\n",
      "Epoch 16582/20000 Training Loss: 0.06072351709008217\n",
      "Epoch 16583/20000 Training Loss: 0.04686763882637024\n",
      "Epoch 16584/20000 Training Loss: 0.0507667250931263\n",
      "Epoch 16585/20000 Training Loss: 0.046980369836091995\n",
      "Epoch 16586/20000 Training Loss: 0.044284921139478683\n",
      "Epoch 16587/20000 Training Loss: 0.0506698302924633\n",
      "Epoch 16588/20000 Training Loss: 0.06979921460151672\n",
      "Epoch 16589/20000 Training Loss: 0.04544884338974953\n",
      "Epoch 16590/20000 Training Loss: 0.04915633797645569\n",
      "Epoch 16590/20000 Validation Loss: 0.0670999139547348\n",
      "Epoch 16591/20000 Training Loss: 0.04336315765976906\n",
      "Epoch 16592/20000 Training Loss: 0.0781952515244484\n",
      "Epoch 16593/20000 Training Loss: 0.036685798317193985\n",
      "Epoch 16594/20000 Training Loss: 0.044869404286146164\n",
      "Epoch 16595/20000 Training Loss: 0.05294843390583992\n",
      "Epoch 16596/20000 Training Loss: 0.058366190642118454\n",
      "Epoch 16597/20000 Training Loss: 0.07306093722581863\n",
      "Epoch 16598/20000 Training Loss: 0.050475168973207474\n",
      "Epoch 16599/20000 Training Loss: 0.04992290213704109\n",
      "Epoch 16600/20000 Training Loss: 0.060175154358148575\n",
      "Epoch 16600/20000 Validation Loss: 0.06779089570045471\n",
      "Epoch 16601/20000 Training Loss: 0.04052184522151947\n",
      "Epoch 16602/20000 Training Loss: 0.038656752556562424\n",
      "Epoch 16603/20000 Training Loss: 0.05967824533581734\n",
      "Epoch 16604/20000 Training Loss: 0.0430402010679245\n",
      "Epoch 16605/20000 Training Loss: 0.06397923082113266\n",
      "Epoch 16606/20000 Training Loss: 0.04220674932003021\n",
      "Epoch 16607/20000 Training Loss: 0.07263707369565964\n",
      "Epoch 16608/20000 Training Loss: 0.05822838470339775\n",
      "Epoch 16609/20000 Training Loss: 0.051308974623680115\n",
      "Epoch 16610/20000 Training Loss: 0.06197928264737129\n",
      "Epoch 16610/20000 Validation Loss: 0.06307440251111984\n",
      "Epoch 16611/20000 Training Loss: 0.05528106912970543\n",
      "Epoch 16612/20000 Training Loss: 0.06622634083032608\n",
      "Epoch 16613/20000 Training Loss: 0.04210671782493591\n",
      "Epoch 16614/20000 Training Loss: 0.042515844106674194\n",
      "Epoch 16615/20000 Training Loss: 0.04972825571894646\n",
      "Epoch 16616/20000 Training Loss: 0.04871203005313873\n",
      "Epoch 16617/20000 Training Loss: 0.056791048496961594\n",
      "Epoch 16618/20000 Training Loss: 0.050726816058158875\n",
      "Epoch 16619/20000 Training Loss: 0.05591928958892822\n",
      "Epoch 16620/20000 Training Loss: 0.05366687849164009\n",
      "Epoch 16620/20000 Validation Loss: 0.051162250339984894\n",
      "Epoch 16621/20000 Training Loss: 0.054357245564460754\n",
      "Epoch 16622/20000 Training Loss: 0.0540216863155365\n",
      "Epoch 16623/20000 Training Loss: 0.04797295108437538\n",
      "Epoch 16624/20000 Training Loss: 0.06005973741412163\n",
      "Epoch 16625/20000 Training Loss: 0.05111759528517723\n",
      "Epoch 16626/20000 Training Loss: 0.05316033586859703\n",
      "Epoch 16627/20000 Training Loss: 0.05008899047970772\n",
      "Epoch 16628/20000 Training Loss: 0.055852532386779785\n",
      "Epoch 16629/20000 Training Loss: 0.06506172567605972\n",
      "Epoch 16630/20000 Training Loss: 0.06215489283204079\n",
      "Epoch 16630/20000 Validation Loss: 0.07049413025379181\n",
      "Epoch 16631/20000 Training Loss: 0.038842421025037766\n",
      "Epoch 16632/20000 Training Loss: 0.061179738491773605\n",
      "Epoch 16633/20000 Training Loss: 0.04494207724928856\n",
      "Epoch 16634/20000 Training Loss: 0.06227407976984978\n",
      "Epoch 16635/20000 Training Loss: 0.05582861974835396\n",
      "Epoch 16636/20000 Training Loss: 0.05723709985613823\n",
      "Epoch 16637/20000 Training Loss: 0.05336882546544075\n",
      "Epoch 16638/20000 Training Loss: 0.05935819447040558\n",
      "Epoch 16639/20000 Training Loss: 0.055058207362890244\n",
      "Epoch 16640/20000 Training Loss: 0.04936239495873451\n",
      "Epoch 16640/20000 Validation Loss: 0.059812046587467194\n",
      "Epoch 16641/20000 Training Loss: 0.060659077018499374\n",
      "Epoch 16642/20000 Training Loss: 0.05482110753655434\n",
      "Epoch 16643/20000 Training Loss: 0.044125404208898544\n",
      "Epoch 16644/20000 Training Loss: 0.04627549648284912\n",
      "Epoch 16645/20000 Training Loss: 0.0653504729270935\n",
      "Epoch 16646/20000 Training Loss: 0.06906596571207047\n",
      "Epoch 16647/20000 Training Loss: 0.05525721237063408\n",
      "Epoch 16648/20000 Training Loss: 0.04679866135120392\n",
      "Epoch 16649/20000 Training Loss: 0.057295169681310654\n",
      "Epoch 16650/20000 Training Loss: 0.06429162621498108\n",
      "Epoch 16650/20000 Validation Loss: 0.07056077569723129\n",
      "Epoch 16651/20000 Training Loss: 0.052411604672670364\n",
      "Epoch 16652/20000 Training Loss: 0.05752982571721077\n",
      "Epoch 16653/20000 Training Loss: 0.06049355864524841\n",
      "Epoch 16654/20000 Training Loss: 0.07346022874116898\n",
      "Epoch 16655/20000 Training Loss: 0.03600098565220833\n",
      "Epoch 16656/20000 Training Loss: 0.03617881238460541\n",
      "Epoch 16657/20000 Training Loss: 0.044341932982206345\n",
      "Epoch 16658/20000 Training Loss: 0.0640420913696289\n",
      "Epoch 16659/20000 Training Loss: 0.048509035259485245\n",
      "Epoch 16660/20000 Training Loss: 0.07004749029874802\n",
      "Epoch 16660/20000 Validation Loss: 0.07379012554883957\n",
      "Epoch 16661/20000 Training Loss: 0.05500514805316925\n",
      "Epoch 16662/20000 Training Loss: 0.06044710800051689\n",
      "Epoch 16663/20000 Training Loss: 0.06412524729967117\n",
      "Epoch 16664/20000 Training Loss: 0.05605390667915344\n",
      "Epoch 16665/20000 Training Loss: 0.03976383060216904\n",
      "Epoch 16666/20000 Training Loss: 0.05907425284385681\n",
      "Epoch 16667/20000 Training Loss: 0.07290825992822647\n",
      "Epoch 16668/20000 Training Loss: 0.05107300356030464\n",
      "Epoch 16669/20000 Training Loss: 0.05982375144958496\n",
      "Epoch 16670/20000 Training Loss: 0.038034602999687195\n",
      "Epoch 16670/20000 Validation Loss: 0.056871309876441956\n",
      "Epoch 16671/20000 Training Loss: 0.06238747760653496\n",
      "Epoch 16672/20000 Training Loss: 0.05336228013038635\n",
      "Epoch 16673/20000 Training Loss: 0.0577264241874218\n",
      "Epoch 16674/20000 Training Loss: 0.05404286086559296\n",
      "Epoch 16675/20000 Training Loss: 0.05958586931228638\n",
      "Epoch 16676/20000 Training Loss: 0.06242211535573006\n",
      "Epoch 16677/20000 Training Loss: 0.05286828801035881\n",
      "Epoch 16678/20000 Training Loss: 0.060579780489206314\n",
      "Epoch 16679/20000 Training Loss: 0.06006546691060066\n",
      "Epoch 16680/20000 Training Loss: 0.048486996442079544\n",
      "Epoch 16680/20000 Validation Loss: 0.046368151903152466\n",
      "Epoch 16681/20000 Training Loss: 0.040018629282712936\n",
      "Epoch 16682/20000 Training Loss: 0.06939303874969482\n",
      "Epoch 16683/20000 Training Loss: 0.049356698989868164\n",
      "Epoch 16684/20000 Training Loss: 0.0438973493874073\n",
      "Epoch 16685/20000 Training Loss: 0.034007709473371506\n",
      "Epoch 16686/20000 Training Loss: 0.05085138976573944\n",
      "Epoch 16687/20000 Training Loss: 0.052599985152482986\n",
      "Epoch 16688/20000 Training Loss: 0.06045004725456238\n",
      "Epoch 16689/20000 Training Loss: 0.04647885635495186\n",
      "Epoch 16690/20000 Training Loss: 0.04724467918276787\n",
      "Epoch 16690/20000 Validation Loss: 0.06379204243421555\n",
      "Epoch 16691/20000 Training Loss: 0.06391999125480652\n",
      "Epoch 16692/20000 Training Loss: 0.07231748849153519\n",
      "Epoch 16693/20000 Training Loss: 0.04905704781413078\n",
      "Epoch 16694/20000 Training Loss: 0.061717767268419266\n",
      "Epoch 16695/20000 Training Loss: 0.06360733509063721\n",
      "Epoch 16696/20000 Training Loss: 0.055119838565588\n",
      "Epoch 16697/20000 Training Loss: 0.04363347962498665\n",
      "Epoch 16698/20000 Training Loss: 0.054055485874414444\n",
      "Epoch 16699/20000 Training Loss: 0.057114794850349426\n",
      "Epoch 16700/20000 Training Loss: 0.06552378088235855\n",
      "Epoch 16700/20000 Validation Loss: 0.04289767146110535\n",
      "Epoch 16701/20000 Training Loss: 0.05018969252705574\n",
      "Epoch 16702/20000 Training Loss: 0.047352079302072525\n",
      "Epoch 16703/20000 Training Loss: 0.054188262671232224\n",
      "Epoch 16704/20000 Training Loss: 0.05081559717655182\n",
      "Epoch 16705/20000 Training Loss: 0.05997806414961815\n",
      "Epoch 16706/20000 Training Loss: 0.05336923524737358\n",
      "Epoch 16707/20000 Training Loss: 0.05239258334040642\n",
      "Epoch 16708/20000 Training Loss: 0.06383254379034042\n",
      "Epoch 16709/20000 Training Loss: 0.050767745822668076\n",
      "Epoch 16710/20000 Training Loss: 0.04763172194361687\n",
      "Epoch 16710/20000 Validation Loss: 0.03837550804018974\n",
      "Epoch 16711/20000 Training Loss: 0.051262546330690384\n",
      "Epoch 16712/20000 Training Loss: 0.047136660665273666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16713/20000 Training Loss: 0.04702809453010559\n",
      "Epoch 16714/20000 Training Loss: 0.06265690177679062\n",
      "Epoch 16715/20000 Training Loss: 0.056090760976076126\n",
      "Epoch 16716/20000 Training Loss: 0.06122233346104622\n",
      "Epoch 16717/20000 Training Loss: 0.051512211561203\n",
      "Epoch 16718/20000 Training Loss: 0.06109548732638359\n",
      "Epoch 16719/20000 Training Loss: 0.05183982849121094\n",
      "Epoch 16720/20000 Training Loss: 0.057160958647727966\n",
      "Epoch 16720/20000 Validation Loss: 0.03645147383213043\n",
      "Epoch 16721/20000 Training Loss: 0.05701281502842903\n",
      "Epoch 16722/20000 Training Loss: 0.03837314248085022\n",
      "Epoch 16723/20000 Training Loss: 0.05342056229710579\n",
      "Epoch 16724/20000 Training Loss: 0.05962447449564934\n",
      "Epoch 16725/20000 Training Loss: 0.06479229032993317\n",
      "Epoch 16726/20000 Training Loss: 0.04809457063674927\n",
      "Epoch 16727/20000 Training Loss: 0.05091319978237152\n",
      "Epoch 16728/20000 Training Loss: 0.054894741624593735\n",
      "Epoch 16729/20000 Training Loss: 0.060711801052093506\n",
      "Epoch 16730/20000 Training Loss: 0.05046658217906952\n",
      "Epoch 16730/20000 Validation Loss: 0.046490781009197235\n",
      "Epoch 16731/20000 Training Loss: 0.058270905166864395\n",
      "Epoch 16732/20000 Training Loss: 0.061039041727781296\n",
      "Epoch 16733/20000 Training Loss: 0.051128972321748734\n",
      "Epoch 16734/20000 Training Loss: 0.07028184086084366\n",
      "Epoch 16735/20000 Training Loss: 0.04833201691508293\n",
      "Epoch 16736/20000 Training Loss: 0.06509954482316971\n",
      "Epoch 16737/20000 Training Loss: 0.07294432073831558\n",
      "Epoch 16738/20000 Training Loss: 0.0492401085793972\n",
      "Epoch 16739/20000 Training Loss: 0.04078103229403496\n",
      "Epoch 16740/20000 Training Loss: 0.06422434747219086\n",
      "Epoch 16740/20000 Validation Loss: 0.058413319289684296\n",
      "Epoch 16741/20000 Training Loss: 0.03431829810142517\n",
      "Epoch 16742/20000 Training Loss: 0.05101554095745087\n",
      "Epoch 16743/20000 Training Loss: 0.0494568832218647\n",
      "Epoch 16744/20000 Training Loss: 0.05292805656790733\n",
      "Epoch 16745/20000 Training Loss: 0.05164913460612297\n",
      "Epoch 16746/20000 Training Loss: 0.04469640925526619\n",
      "Epoch 16747/20000 Training Loss: 0.059822335839271545\n",
      "Epoch 16748/20000 Training Loss: 0.058862119913101196\n",
      "Epoch 16749/20000 Training Loss: 0.07722284644842148\n",
      "Epoch 16750/20000 Training Loss: 0.06317427009344101\n",
      "Epoch 16750/20000 Validation Loss: 0.05766785517334938\n",
      "Epoch 16751/20000 Training Loss: 0.04914924502372742\n",
      "Epoch 16752/20000 Training Loss: 0.05889764428138733\n",
      "Epoch 16753/20000 Training Loss: 0.05205177143216133\n",
      "Epoch 16754/20000 Training Loss: 0.044261034578084946\n",
      "Epoch 16755/20000 Training Loss: 0.05894755199551582\n",
      "Epoch 16756/20000 Training Loss: 0.04689167067408562\n",
      "Epoch 16757/20000 Training Loss: 0.049639273434877396\n",
      "Epoch 16758/20000 Training Loss: 0.05794936418533325\n",
      "Epoch 16759/20000 Training Loss: 0.03611493483185768\n",
      "Epoch 16760/20000 Training Loss: 0.0582708977162838\n",
      "Epoch 16760/20000 Validation Loss: 0.05175617337226868\n",
      "Epoch 16761/20000 Training Loss: 0.05069463327527046\n",
      "Epoch 16762/20000 Training Loss: 0.05458964407444\n",
      "Epoch 16763/20000 Training Loss: 0.05693269893527031\n",
      "Epoch 16764/20000 Training Loss: 0.05337749794125557\n",
      "Epoch 16765/20000 Training Loss: 0.057031840085983276\n",
      "Epoch 16766/20000 Training Loss: 0.0568668395280838\n",
      "Epoch 16767/20000 Training Loss: 0.06047403812408447\n",
      "Epoch 16768/20000 Training Loss: 0.05896163359284401\n",
      "Epoch 16769/20000 Training Loss: 0.05746912956237793\n",
      "Epoch 16770/20000 Training Loss: 0.05577588081359863\n",
      "Epoch 16770/20000 Validation Loss: 0.05638788640499115\n",
      "Epoch 16771/20000 Training Loss: 0.03785603120923042\n",
      "Epoch 16772/20000 Training Loss: 0.050052251666784286\n",
      "Epoch 16773/20000 Training Loss: 0.05419735610485077\n",
      "Epoch 16774/20000 Training Loss: 0.055674124509096146\n",
      "Epoch 16775/20000 Training Loss: 0.05154120549559593\n",
      "Epoch 16776/20000 Training Loss: 0.060221221297979355\n",
      "Epoch 16777/20000 Training Loss: 0.05321541428565979\n",
      "Epoch 16778/20000 Training Loss: 0.049134671688079834\n",
      "Epoch 16779/20000 Training Loss: 0.03955613076686859\n",
      "Epoch 16780/20000 Training Loss: 0.052730005234479904\n",
      "Epoch 16780/20000 Validation Loss: 0.061658963561058044\n",
      "Epoch 16781/20000 Training Loss: 0.0587712787091732\n",
      "Epoch 16782/20000 Training Loss: 0.04761483147740364\n",
      "Epoch 16783/20000 Training Loss: 0.038732510060071945\n",
      "Epoch 16784/20000 Training Loss: 0.04888265207409859\n",
      "Epoch 16785/20000 Training Loss: 0.06174539402127266\n",
      "Epoch 16786/20000 Training Loss: 0.06780115514993668\n",
      "Epoch 16787/20000 Training Loss: 0.05528071150183678\n",
      "Epoch 16788/20000 Training Loss: 0.053795117884874344\n",
      "Epoch 16789/20000 Training Loss: 0.057046085596084595\n",
      "Epoch 16790/20000 Training Loss: 0.03694372996687889\n",
      "Epoch 16790/20000 Validation Loss: 0.0605035126209259\n",
      "Epoch 16791/20000 Training Loss: 0.0495678186416626\n",
      "Epoch 16792/20000 Training Loss: 0.04235482215881348\n",
      "Epoch 16793/20000 Training Loss: 0.05834956094622612\n",
      "Epoch 16794/20000 Training Loss: 0.06329435855150223\n",
      "Epoch 16795/20000 Training Loss: 0.05375512316823006\n",
      "Epoch 16796/20000 Training Loss: 0.05789640545845032\n",
      "Epoch 16797/20000 Training Loss: 0.054024506360292435\n",
      "Epoch 16798/20000 Training Loss: 0.05845515802502632\n",
      "Epoch 16799/20000 Training Loss: 0.05656644329428673\n",
      "Epoch 16800/20000 Training Loss: 0.053049612790346146\n",
      "Epoch 16800/20000 Validation Loss: 0.06419713795185089\n",
      "Epoch 16801/20000 Training Loss: 0.04033401980996132\n",
      "Epoch 16802/20000 Training Loss: 0.04356906935572624\n",
      "Epoch 16803/20000 Training Loss: 0.05262727662920952\n",
      "Epoch 16804/20000 Training Loss: 0.047664791345596313\n",
      "Epoch 16805/20000 Training Loss: 0.05335557833313942\n",
      "Epoch 16806/20000 Training Loss: 0.056004881858825684\n",
      "Epoch 16807/20000 Training Loss: 0.05647522583603859\n",
      "Epoch 16808/20000 Training Loss: 0.06571123003959656\n",
      "Epoch 16809/20000 Training Loss: 0.043486595153808594\n",
      "Epoch 16810/20000 Training Loss: 0.054315607994794846\n",
      "Epoch 16810/20000 Validation Loss: 0.060580283403396606\n",
      "Epoch 16811/20000 Training Loss: 0.06215493753552437\n",
      "Epoch 16812/20000 Training Loss: 0.07201138138771057\n",
      "Epoch 16813/20000 Training Loss: 0.04398103430867195\n",
      "Epoch 16814/20000 Training Loss: 0.05600064620375633\n",
      "Epoch 16815/20000 Training Loss: 0.04186469316482544\n",
      "Epoch 16816/20000 Training Loss: 0.05714995786547661\n",
      "Epoch 16817/20000 Training Loss: 0.04881875589489937\n",
      "Epoch 16818/20000 Training Loss: 0.06595400720834732\n",
      "Epoch 16819/20000 Training Loss: 0.05195366218686104\n",
      "Epoch 16820/20000 Training Loss: 0.06137412413954735\n",
      "Epoch 16820/20000 Validation Loss: 0.04291991889476776\n",
      "Epoch 16821/20000 Training Loss: 0.05583849549293518\n",
      "Epoch 16822/20000 Training Loss: 0.06054767593741417\n",
      "Epoch 16823/20000 Training Loss: 0.0606483519077301\n",
      "Epoch 16824/20000 Training Loss: 0.05820341780781746\n",
      "Epoch 16825/20000 Training Loss: 0.08085661381483078\n",
      "Epoch 16826/20000 Training Loss: 0.06856059283018112\n",
      "Epoch 16827/20000 Training Loss: 0.05543289706110954\n",
      "Epoch 16828/20000 Training Loss: 0.044007450342178345\n",
      "Epoch 16829/20000 Training Loss: 0.04537586495280266\n",
      "Epoch 16830/20000 Training Loss: 0.04858371615409851\n",
      "Epoch 16830/20000 Validation Loss: 0.05809633433818817\n",
      "Epoch 16831/20000 Training Loss: 0.050070662051439285\n",
      "Epoch 16832/20000 Training Loss: 0.05371665582060814\n",
      "Epoch 16833/20000 Training Loss: 0.06771108508110046\n",
      "Epoch 16834/20000 Training Loss: 0.047141700983047485\n",
      "Epoch 16835/20000 Training Loss: 0.05473815277218819\n",
      "Epoch 16836/20000 Training Loss: 0.05691109970211983\n",
      "Epoch 16837/20000 Training Loss: 0.0438404381275177\n",
      "Epoch 16838/20000 Training Loss: 0.06064249575138092\n",
      "Epoch 16839/20000 Training Loss: 0.04922609031200409\n",
      "Epoch 16840/20000 Training Loss: 0.07380229234695435\n",
      "Epoch 16840/20000 Validation Loss: 0.05825628712773323\n",
      "Epoch 16841/20000 Training Loss: 0.05784090980887413\n",
      "Epoch 16842/20000 Training Loss: 0.045856375247240067\n",
      "Epoch 16843/20000 Training Loss: 0.05324351787567139\n",
      "Epoch 16844/20000 Training Loss: 0.054453104734420776\n",
      "Epoch 16845/20000 Training Loss: 0.06092424318194389\n",
      "Epoch 16846/20000 Training Loss: 0.04659859836101532\n",
      "Epoch 16847/20000 Training Loss: 0.057063471525907516\n",
      "Epoch 16848/20000 Training Loss: 0.04814672842621803\n",
      "Epoch 16849/20000 Training Loss: 0.07120123505592346\n",
      "Epoch 16850/20000 Training Loss: 0.049372244626283646\n",
      "Epoch 16850/20000 Validation Loss: 0.06195371598005295\n",
      "Epoch 16851/20000 Training Loss: 0.05145040154457092\n",
      "Epoch 16852/20000 Training Loss: 0.056540485471487045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16853/20000 Training Loss: 0.058869656175374985\n",
      "Epoch 16854/20000 Training Loss: 0.04666604474186897\n",
      "Epoch 16855/20000 Training Loss: 0.04761756956577301\n",
      "Epoch 16856/20000 Training Loss: 0.05821004882454872\n",
      "Epoch 16857/20000 Training Loss: 0.05958304926753044\n",
      "Epoch 16858/20000 Training Loss: 0.049758415669202805\n",
      "Epoch 16859/20000 Training Loss: 0.04664953425526619\n",
      "Epoch 16860/20000 Training Loss: 0.05030962452292442\n",
      "Epoch 16860/20000 Validation Loss: 0.05873873084783554\n",
      "Epoch 16861/20000 Training Loss: 0.051131170243024826\n",
      "Epoch 16862/20000 Training Loss: 0.0391860269010067\n",
      "Epoch 16863/20000 Training Loss: 0.060548435896635056\n",
      "Epoch 16864/20000 Training Loss: 0.056536946445703506\n",
      "Epoch 16865/20000 Training Loss: 0.0551450215280056\n",
      "Epoch 16866/20000 Training Loss: 0.045204829424619675\n",
      "Epoch 16867/20000 Training Loss: 0.03985020890831947\n",
      "Epoch 16868/20000 Training Loss: 0.04568932577967644\n",
      "Epoch 16869/20000 Training Loss: 0.04105866327881813\n",
      "Epoch 16870/20000 Training Loss: 0.037487104535102844\n",
      "Epoch 16870/20000 Validation Loss: 0.05971230939030647\n",
      "Epoch 16871/20000 Training Loss: 0.053364645689725876\n",
      "Epoch 16872/20000 Training Loss: 0.05835958942770958\n",
      "Epoch 16873/20000 Training Loss: 0.0315057598054409\n",
      "Epoch 16874/20000 Training Loss: 0.04916880652308464\n",
      "Epoch 16875/20000 Training Loss: 0.044129952788352966\n",
      "Epoch 16876/20000 Training Loss: 0.05465617775917053\n",
      "Epoch 16877/20000 Training Loss: 0.025502007454633713\n",
      "Epoch 16878/20000 Training Loss: 0.05227270722389221\n",
      "Epoch 16879/20000 Training Loss: 0.06080541014671326\n",
      "Epoch 16880/20000 Training Loss: 0.043577563017606735\n",
      "Epoch 16880/20000 Validation Loss: 0.034484170377254486\n",
      "Epoch 16881/20000 Training Loss: 0.04696491360664368\n",
      "Epoch 16882/20000 Training Loss: 0.049030035734176636\n",
      "Epoch 16883/20000 Training Loss: 0.049719419330358505\n",
      "Epoch 16884/20000 Training Loss: 0.05829111859202385\n",
      "Epoch 16885/20000 Training Loss: 0.044892460107803345\n",
      "Epoch 16886/20000 Training Loss: 0.05875786021351814\n",
      "Epoch 16887/20000 Training Loss: 0.07498321682214737\n",
      "Epoch 16888/20000 Training Loss: 0.06685598939657211\n",
      "Epoch 16889/20000 Training Loss: 0.043132174760103226\n",
      "Epoch 16890/20000 Training Loss: 0.05260062590241432\n",
      "Epoch 16890/20000 Validation Loss: 0.07884147763252258\n",
      "Epoch 16891/20000 Training Loss: 0.06022900715470314\n",
      "Epoch 16892/20000 Training Loss: 0.04761083424091339\n",
      "Epoch 16893/20000 Training Loss: 0.05147489905357361\n",
      "Epoch 16894/20000 Training Loss: 0.05438203737139702\n",
      "Epoch 16895/20000 Training Loss: 0.047678153961896896\n",
      "Epoch 16896/20000 Training Loss: 0.05363953113555908\n",
      "Epoch 16897/20000 Training Loss: 0.047198619693517685\n",
      "Epoch 16898/20000 Training Loss: 0.04929782822728157\n",
      "Epoch 16899/20000 Training Loss: 0.043694645166397095\n",
      "Epoch 16900/20000 Training Loss: 0.058533504605293274\n",
      "Epoch 16900/20000 Validation Loss: 0.03585910424590111\n",
      "Epoch 16901/20000 Training Loss: 0.056738581508398056\n",
      "Epoch 16902/20000 Training Loss: 0.055246490985155106\n",
      "Epoch 16903/20000 Training Loss: 0.06396103650331497\n",
      "Epoch 16904/20000 Training Loss: 0.05341525748372078\n",
      "Epoch 16905/20000 Training Loss: 0.04107765853404999\n",
      "Epoch 16906/20000 Training Loss: 0.04897995665669441\n",
      "Epoch 16907/20000 Training Loss: 0.050142064690589905\n",
      "Epoch 16908/20000 Training Loss: 0.049445539712905884\n",
      "Epoch 16909/20000 Training Loss: 0.04896420240402222\n",
      "Epoch 16910/20000 Training Loss: 0.04533376172184944\n",
      "Epoch 16910/20000 Validation Loss: 0.04937098175287247\n",
      "Epoch 16911/20000 Training Loss: 0.04854006692767143\n",
      "Epoch 16912/20000 Training Loss: 0.048853982239961624\n",
      "Epoch 16913/20000 Training Loss: 0.0629778578877449\n",
      "Epoch 16914/20000 Training Loss: 0.039255291223526\n",
      "Epoch 16915/20000 Training Loss: 0.048345897346735\n",
      "Epoch 16916/20000 Training Loss: 0.0686122328042984\n",
      "Epoch 16917/20000 Training Loss: 0.043581608682870865\n",
      "Epoch 16918/20000 Training Loss: 0.05382964015007019\n",
      "Epoch 16919/20000 Training Loss: 0.0680290088057518\n",
      "Epoch 16920/20000 Training Loss: 0.05260680243372917\n",
      "Epoch 16920/20000 Validation Loss: 0.03870466724038124\n",
      "Epoch 16921/20000 Training Loss: 0.04947230592370033\n",
      "Epoch 16922/20000 Training Loss: 0.052807461470365524\n",
      "Epoch 16923/20000 Training Loss: 0.04531889781355858\n",
      "Epoch 16924/20000 Training Loss: 0.04736281931400299\n",
      "Epoch 16925/20000 Training Loss: 0.05528455972671509\n",
      "Epoch 16926/20000 Training Loss: 0.05421094968914986\n",
      "Epoch 16927/20000 Training Loss: 0.049800191074609756\n",
      "Epoch 16928/20000 Training Loss: 0.0690959095954895\n",
      "Epoch 16929/20000 Training Loss: 0.07146569341421127\n",
      "Epoch 16930/20000 Training Loss: 0.06266976147890091\n",
      "Epoch 16930/20000 Validation Loss: 0.058143213391304016\n",
      "Epoch 16931/20000 Training Loss: 0.06408113241195679\n",
      "Epoch 16932/20000 Training Loss: 0.05070050433278084\n",
      "Epoch 16933/20000 Training Loss: 0.03658458590507507\n",
      "Epoch 16934/20000 Training Loss: 0.06263091415166855\n",
      "Epoch 16935/20000 Training Loss: 0.04536932334303856\n",
      "Epoch 16936/20000 Training Loss: 0.039238911122083664\n",
      "Epoch 16937/20000 Training Loss: 0.06764473021030426\n",
      "Epoch 16938/20000 Training Loss: 0.05336039140820503\n",
      "Epoch 16939/20000 Training Loss: 0.055418480187654495\n",
      "Epoch 16940/20000 Training Loss: 0.05525621399283409\n",
      "Epoch 16940/20000 Validation Loss: 0.06609322130680084\n",
      "Epoch 16941/20000 Training Loss: 0.06144784018397331\n",
      "Epoch 16942/20000 Training Loss: 0.048813074827194214\n",
      "Epoch 16943/20000 Training Loss: 0.06865200400352478\n",
      "Epoch 16944/20000 Training Loss: 0.05286382511258125\n",
      "Epoch 16945/20000 Training Loss: 0.05160440877079964\n",
      "Epoch 16946/20000 Training Loss: 0.065908282995224\n",
      "Epoch 16947/20000 Training Loss: 0.05371155962347984\n",
      "Epoch 16948/20000 Training Loss: 0.04060729220509529\n",
      "Epoch 16949/20000 Training Loss: 0.0746568962931633\n",
      "Epoch 16950/20000 Training Loss: 0.051738422363996506\n",
      "Epoch 16950/20000 Validation Loss: 0.06218571215867996\n",
      "Epoch 16951/20000 Training Loss: 0.04755884408950806\n",
      "Epoch 16952/20000 Training Loss: 0.06212856248021126\n",
      "Epoch 16953/20000 Training Loss: 0.0567750446498394\n",
      "Epoch 16954/20000 Training Loss: 0.05581372603774071\n",
      "Epoch 16955/20000 Training Loss: 0.04833792522549629\n",
      "Epoch 16956/20000 Training Loss: 0.04504219815135002\n",
      "Epoch 16957/20000 Training Loss: 0.038001250475645065\n",
      "Epoch 16958/20000 Training Loss: 0.07036405056715012\n",
      "Epoch 16959/20000 Training Loss: 0.04604007676243782\n",
      "Epoch 16960/20000 Training Loss: 0.05902360379695892\n",
      "Epoch 16960/20000 Validation Loss: 0.05109956115484238\n",
      "Epoch 16961/20000 Training Loss: 0.044056933373212814\n",
      "Epoch 16962/20000 Training Loss: 0.0774322897195816\n",
      "Epoch 16963/20000 Training Loss: 0.04755531623959541\n",
      "Epoch 16964/20000 Training Loss: 0.04208119586110115\n",
      "Epoch 16965/20000 Training Loss: 0.06293004751205444\n",
      "Epoch 16966/20000 Training Loss: 0.04821828007698059\n",
      "Epoch 16967/20000 Training Loss: 0.05584757402539253\n",
      "Epoch 16968/20000 Training Loss: 0.050359081476926804\n",
      "Epoch 16969/20000 Training Loss: 0.050115521997213364\n",
      "Epoch 16970/20000 Training Loss: 0.06829360127449036\n",
      "Epoch 16970/20000 Validation Loss: 0.056696679443120956\n",
      "Epoch 16971/20000 Training Loss: 0.05621920898556709\n",
      "Epoch 16972/20000 Training Loss: 0.058559443801641464\n",
      "Epoch 16973/20000 Training Loss: 0.03881611302495003\n",
      "Epoch 16974/20000 Training Loss: 0.0468057245016098\n",
      "Epoch 16975/20000 Training Loss: 0.035605933517217636\n",
      "Epoch 16976/20000 Training Loss: 0.05691513791680336\n",
      "Epoch 16977/20000 Training Loss: 0.045833032578229904\n",
      "Epoch 16978/20000 Training Loss: 0.0541125126183033\n",
      "Epoch 16979/20000 Training Loss: 0.06447783857584\n",
      "Epoch 16980/20000 Training Loss: 0.055425312370061874\n",
      "Epoch 16980/20000 Validation Loss: 0.0693889707326889\n",
      "Epoch 16981/20000 Training Loss: 0.047783706337213516\n",
      "Epoch 16982/20000 Training Loss: 0.06267253309488297\n",
      "Epoch 16983/20000 Training Loss: 0.061545480042696\n",
      "Epoch 16984/20000 Training Loss: 0.045680880546569824\n",
      "Epoch 16985/20000 Training Loss: 0.05795000120997429\n",
      "Epoch 16986/20000 Training Loss: 0.059769514948129654\n",
      "Epoch 16987/20000 Training Loss: 0.04225926473736763\n",
      "Epoch 16988/20000 Training Loss: 0.050082478672266006\n",
      "Epoch 16989/20000 Training Loss: 0.05087621882557869\n",
      "Epoch 16990/20000 Training Loss: 0.059005703777074814\n",
      "Epoch 16990/20000 Validation Loss: 0.06452623754739761\n",
      "Epoch 16991/20000 Training Loss: 0.0627247616648674\n",
      "Epoch 16992/20000 Training Loss: 0.05214565992355347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16993/20000 Training Loss: 0.04997105523943901\n",
      "Epoch 16994/20000 Training Loss: 0.05378735065460205\n",
      "Epoch 16995/20000 Training Loss: 0.06744813919067383\n",
      "Epoch 16996/20000 Training Loss: 0.06507444381713867\n",
      "Epoch 16997/20000 Training Loss: 0.05871649459004402\n",
      "Epoch 16998/20000 Training Loss: 0.04276524484157562\n",
      "Epoch 16999/20000 Training Loss: 0.05725951865315437\n",
      "Epoch 17000/20000 Training Loss: 0.07184786349534988\n",
      "Epoch 17000/20000 Validation Loss: 0.05271391570568085\n",
      "Epoch 17001/20000 Training Loss: 0.06009898707270622\n",
      "Epoch 17002/20000 Training Loss: 0.05374935269355774\n",
      "Epoch 17003/20000 Training Loss: 0.07275829464197159\n",
      "Epoch 17004/20000 Training Loss: 0.05163766071200371\n",
      "Epoch 17005/20000 Training Loss: 0.04954658821225166\n",
      "Epoch 17006/20000 Training Loss: 0.04403872787952423\n",
      "Epoch 17007/20000 Training Loss: 0.08279647678136826\n",
      "Epoch 17008/20000 Training Loss: 0.043376874178647995\n",
      "Epoch 17009/20000 Training Loss: 0.05351201817393303\n",
      "Epoch 17010/20000 Training Loss: 0.03925788030028343\n",
      "Epoch 17010/20000 Validation Loss: 0.05453934147953987\n",
      "Epoch 17011/20000 Training Loss: 0.04053926840424538\n",
      "Epoch 17012/20000 Training Loss: 0.05959638953208923\n",
      "Epoch 17013/20000 Training Loss: 0.0380837507545948\n",
      "Epoch 17014/20000 Training Loss: 0.05790812894701958\n",
      "Epoch 17015/20000 Training Loss: 0.05260556563735008\n",
      "Epoch 17016/20000 Training Loss: 0.04499724879860878\n",
      "Epoch 17017/20000 Training Loss: 0.057574063539505005\n",
      "Epoch 17018/20000 Training Loss: 0.045165758579969406\n",
      "Epoch 17019/20000 Training Loss: 0.04489812254905701\n",
      "Epoch 17020/20000 Training Loss: 0.0484532006084919\n",
      "Epoch 17020/20000 Validation Loss: 0.04521368443965912\n",
      "Epoch 17021/20000 Training Loss: 0.04314795136451721\n",
      "Epoch 17022/20000 Training Loss: 0.05451413616538048\n",
      "Epoch 17023/20000 Training Loss: 0.06876769661903381\n",
      "Epoch 17024/20000 Training Loss: 0.05725502967834473\n",
      "Epoch 17025/20000 Training Loss: 0.06418802589178085\n",
      "Epoch 17026/20000 Training Loss: 0.06182720139622688\n",
      "Epoch 17027/20000 Training Loss: 0.04877875745296478\n",
      "Epoch 17028/20000 Training Loss: 0.057725001126527786\n",
      "Epoch 17029/20000 Training Loss: 0.0459333099424839\n",
      "Epoch 17030/20000 Training Loss: 0.04390650615096092\n",
      "Epoch 17030/20000 Validation Loss: 0.0688391774892807\n",
      "Epoch 17031/20000 Training Loss: 0.0501067228615284\n",
      "Epoch 17032/20000 Training Loss: 0.05202298238873482\n",
      "Epoch 17033/20000 Training Loss: 0.0696641206741333\n",
      "Epoch 17034/20000 Training Loss: 0.046639297157526016\n",
      "Epoch 17035/20000 Training Loss: 0.05032828450202942\n",
      "Epoch 17036/20000 Training Loss: 0.05546162649989128\n",
      "Epoch 17037/20000 Training Loss: 0.06562996655702591\n",
      "Epoch 17038/20000 Training Loss: 0.06439635902643204\n",
      "Epoch 17039/20000 Training Loss: 0.060957249253988266\n",
      "Epoch 17040/20000 Training Loss: 0.056511566042900085\n",
      "Epoch 17040/20000 Validation Loss: 0.06738006323575974\n",
      "Epoch 17041/20000 Training Loss: 0.04958394169807434\n",
      "Epoch 17042/20000 Training Loss: 0.05568631365895271\n",
      "Epoch 17043/20000 Training Loss: 0.03970379754900932\n",
      "Epoch 17044/20000 Training Loss: 0.0766647532582283\n",
      "Epoch 17045/20000 Training Loss: 0.047639112919569016\n",
      "Epoch 17046/20000 Training Loss: 0.0451536662876606\n",
      "Epoch 17047/20000 Training Loss: 0.06115498021245003\n",
      "Epoch 17048/20000 Training Loss: 0.049669474363327026\n",
      "Epoch 17049/20000 Training Loss: 0.04494548216462135\n",
      "Epoch 17050/20000 Training Loss: 0.05200354382395744\n",
      "Epoch 17050/20000 Validation Loss: 0.052805930376052856\n",
      "Epoch 17051/20000 Training Loss: 0.05234788358211517\n",
      "Epoch 17052/20000 Training Loss: 0.05300245061516762\n",
      "Epoch 17053/20000 Training Loss: 0.047191694378852844\n",
      "Epoch 17054/20000 Training Loss: 0.04803141579031944\n",
      "Epoch 17055/20000 Training Loss: 0.05784517899155617\n",
      "Epoch 17056/20000 Training Loss: 0.050000209361314774\n",
      "Epoch 17057/20000 Training Loss: 0.05041396617889404\n",
      "Epoch 17058/20000 Training Loss: 0.06157761439681053\n",
      "Epoch 17059/20000 Training Loss: 0.04129810258746147\n",
      "Epoch 17060/20000 Training Loss: 0.06485124677419662\n",
      "Epoch 17060/20000 Validation Loss: 0.054418791085481644\n",
      "Epoch 17061/20000 Training Loss: 0.05585743859410286\n",
      "Epoch 17062/20000 Training Loss: 0.043463084846735\n",
      "Epoch 17063/20000 Training Loss: 0.05166351795196533\n",
      "Epoch 17064/20000 Training Loss: 0.05509158968925476\n",
      "Epoch 17065/20000 Training Loss: 0.041582223027944565\n",
      "Epoch 17066/20000 Training Loss: 0.047001976519823074\n",
      "Epoch 17067/20000 Training Loss: 0.05736518278717995\n",
      "Epoch 17068/20000 Training Loss: 0.07945849746465683\n",
      "Epoch 17069/20000 Training Loss: 0.041963767260313034\n",
      "Epoch 17070/20000 Training Loss: 0.05088798701763153\n",
      "Epoch 17070/20000 Validation Loss: 0.0506862998008728\n",
      "Epoch 17071/20000 Training Loss: 0.0641740933060646\n",
      "Epoch 17072/20000 Training Loss: 0.05982029065489769\n",
      "Epoch 17073/20000 Training Loss: 0.06640294194221497\n",
      "Epoch 17074/20000 Training Loss: 0.045824047178030014\n",
      "Epoch 17075/20000 Training Loss: 0.06398776918649673\n",
      "Epoch 17076/20000 Training Loss: 0.052773624658584595\n",
      "Epoch 17077/20000 Training Loss: 0.04603191092610359\n",
      "Epoch 17078/20000 Training Loss: 0.0448421947658062\n",
      "Epoch 17079/20000 Training Loss: 0.041585005819797516\n",
      "Epoch 17080/20000 Training Loss: 0.047779615968465805\n",
      "Epoch 17080/20000 Validation Loss: 0.062287069857120514\n",
      "Epoch 17081/20000 Training Loss: 0.0409160852432251\n",
      "Epoch 17082/20000 Training Loss: 0.06046310067176819\n",
      "Epoch 17083/20000 Training Loss: 0.04988616704940796\n",
      "Epoch 17084/20000 Training Loss: 0.046122509986162186\n",
      "Epoch 17085/20000 Training Loss: 0.05543622747063637\n",
      "Epoch 17086/20000 Training Loss: 0.05956969037652016\n",
      "Epoch 17087/20000 Training Loss: 0.04880771040916443\n",
      "Epoch 17088/20000 Training Loss: 0.04997342452406883\n",
      "Epoch 17089/20000 Training Loss: 0.054034680128097534\n",
      "Epoch 17090/20000 Training Loss: 0.04996499791741371\n",
      "Epoch 17090/20000 Validation Loss: 0.05958352982997894\n",
      "Epoch 17091/20000 Training Loss: 0.05416017398238182\n",
      "Epoch 17092/20000 Training Loss: 0.04825020208954811\n",
      "Epoch 17093/20000 Training Loss: 0.06723945587873459\n",
      "Epoch 17094/20000 Training Loss: 0.05982913449406624\n",
      "Epoch 17095/20000 Training Loss: 0.049233708530664444\n",
      "Epoch 17096/20000 Training Loss: 0.05095808580517769\n",
      "Epoch 17097/20000 Training Loss: 0.04923709109425545\n",
      "Epoch 17098/20000 Training Loss: 0.06091393530368805\n",
      "Epoch 17099/20000 Training Loss: 0.049059461802244186\n",
      "Epoch 17100/20000 Training Loss: 0.04929696023464203\n",
      "Epoch 17100/20000 Validation Loss: 0.03832298889756203\n",
      "Epoch 17101/20000 Training Loss: 0.04348291829228401\n",
      "Epoch 17102/20000 Training Loss: 0.06294868141412735\n",
      "Epoch 17103/20000 Training Loss: 0.03963084891438484\n",
      "Epoch 17104/20000 Training Loss: 0.040214765816926956\n",
      "Epoch 17105/20000 Training Loss: 0.04829570651054382\n",
      "Epoch 17106/20000 Training Loss: 0.06668203324079514\n",
      "Epoch 17107/20000 Training Loss: 0.06518623977899551\n",
      "Epoch 17108/20000 Training Loss: 0.06893368810415268\n",
      "Epoch 17109/20000 Training Loss: 0.04601225256919861\n",
      "Epoch 17110/20000 Training Loss: 0.06540798395872116\n",
      "Epoch 17110/20000 Validation Loss: 0.0657767727971077\n",
      "Epoch 17111/20000 Training Loss: 0.06690939515829086\n",
      "Epoch 17112/20000 Training Loss: 0.04572269693017006\n",
      "Epoch 17113/20000 Training Loss: 0.05144180729985237\n",
      "Epoch 17114/20000 Training Loss: 0.06152452528476715\n",
      "Epoch 17115/20000 Training Loss: 0.061183810234069824\n",
      "Epoch 17116/20000 Training Loss: 0.06651713699102402\n",
      "Epoch 17117/20000 Training Loss: 0.05439618229866028\n",
      "Epoch 17118/20000 Training Loss: 0.07054010033607483\n",
      "Epoch 17119/20000 Training Loss: 0.05005398392677307\n",
      "Epoch 17120/20000 Training Loss: 0.06498665362596512\n",
      "Epoch 17120/20000 Validation Loss: 0.04791831597685814\n",
      "Epoch 17121/20000 Training Loss: 0.04504288733005524\n",
      "Epoch 17122/20000 Training Loss: 0.06572628021240234\n",
      "Epoch 17123/20000 Training Loss: 0.058282870799303055\n",
      "Epoch 17124/20000 Training Loss: 0.050671715289354324\n",
      "Epoch 17125/20000 Training Loss: 0.042301345616579056\n",
      "Epoch 17126/20000 Training Loss: 0.03214617446064949\n",
      "Epoch 17127/20000 Training Loss: 0.07080475240945816\n",
      "Epoch 17128/20000 Training Loss: 0.05273513123393059\n",
      "Epoch 17129/20000 Training Loss: 0.05456450581550598\n",
      "Epoch 17130/20000 Training Loss: 0.05295449495315552\n",
      "Epoch 17130/20000 Validation Loss: 0.06663525849580765\n",
      "Epoch 17131/20000 Training Loss: 0.0699690505862236\n",
      "Epoch 17132/20000 Training Loss: 0.03471658006310463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17133/20000 Training Loss: 0.04467862844467163\n",
      "Epoch 17134/20000 Training Loss: 0.04618888720870018\n",
      "Epoch 17135/20000 Training Loss: 0.05398450419306755\n",
      "Epoch 17136/20000 Training Loss: 0.07117599248886108\n",
      "Epoch 17137/20000 Training Loss: 0.05412052944302559\n",
      "Epoch 17138/20000 Training Loss: 0.049890901893377304\n",
      "Epoch 17139/20000 Training Loss: 0.041100602596998215\n",
      "Epoch 17140/20000 Training Loss: 0.05286666750907898\n",
      "Epoch 17140/20000 Validation Loss: 0.05069781839847565\n",
      "Epoch 17141/20000 Training Loss: 0.05468440055847168\n",
      "Epoch 17142/20000 Training Loss: 0.052594829350709915\n",
      "Epoch 17143/20000 Training Loss: 0.09627475589513779\n",
      "Epoch 17144/20000 Training Loss: 0.055487632751464844\n",
      "Epoch 17145/20000 Training Loss: 0.07820195704698563\n",
      "Epoch 17146/20000 Training Loss: 0.07014533132314682\n",
      "Epoch 17147/20000 Training Loss: 0.06411527842283249\n",
      "Epoch 17148/20000 Training Loss: 0.048897888511419296\n",
      "Epoch 17149/20000 Training Loss: 0.05663251876831055\n",
      "Epoch 17150/20000 Training Loss: 0.05667528510093689\n",
      "Epoch 17150/20000 Validation Loss: 0.056757908314466476\n",
      "Epoch 17151/20000 Training Loss: 0.04257354140281677\n",
      "Epoch 17152/20000 Training Loss: 0.059088822454214096\n",
      "Epoch 17153/20000 Training Loss: 0.06403456628322601\n",
      "Epoch 17154/20000 Training Loss: 0.057934701442718506\n",
      "Epoch 17155/20000 Training Loss: 0.04713784530758858\n",
      "Epoch 17156/20000 Training Loss: 0.05212656036019325\n",
      "Epoch 17157/20000 Training Loss: 0.047322481870651245\n",
      "Epoch 17158/20000 Training Loss: 0.04602835699915886\n",
      "Epoch 17159/20000 Training Loss: 0.046186793595552444\n",
      "Epoch 17160/20000 Training Loss: 0.03794081509113312\n",
      "Epoch 17160/20000 Validation Loss: 0.05399908125400543\n",
      "Epoch 17161/20000 Training Loss: 0.03666529059410095\n",
      "Epoch 17162/20000 Training Loss: 0.036996398121118546\n",
      "Epoch 17163/20000 Training Loss: 0.03639822453260422\n",
      "Epoch 17164/20000 Training Loss: 0.05010984465479851\n",
      "Epoch 17165/20000 Training Loss: 0.05369086563587189\n",
      "Epoch 17166/20000 Training Loss: 0.04269940033555031\n",
      "Epoch 17167/20000 Training Loss: 0.04484141990542412\n",
      "Epoch 17168/20000 Training Loss: 0.044369280338287354\n",
      "Epoch 17169/20000 Training Loss: 0.0504189096391201\n",
      "Epoch 17170/20000 Training Loss: 0.055856723338365555\n",
      "Epoch 17170/20000 Validation Loss: 0.04497230798006058\n",
      "Epoch 17171/20000 Training Loss: 0.0522146113216877\n",
      "Epoch 17172/20000 Training Loss: 0.05526166036725044\n",
      "Epoch 17173/20000 Training Loss: 0.038943517953157425\n",
      "Epoch 17174/20000 Training Loss: 0.07025384157896042\n",
      "Epoch 17175/20000 Training Loss: 0.06360430270433426\n",
      "Epoch 17176/20000 Training Loss: 0.055791180580854416\n",
      "Epoch 17177/20000 Training Loss: 0.05435211956501007\n",
      "Epoch 17178/20000 Training Loss: 0.0551944375038147\n",
      "Epoch 17179/20000 Training Loss: 0.040265925228595734\n",
      "Epoch 17180/20000 Training Loss: 0.05876021087169647\n",
      "Epoch 17180/20000 Validation Loss: 0.055062390863895416\n",
      "Epoch 17181/20000 Training Loss: 0.04771934822201729\n",
      "Epoch 17182/20000 Training Loss: 0.047860682010650635\n",
      "Epoch 17183/20000 Training Loss: 0.05370582267642021\n",
      "Epoch 17184/20000 Training Loss: 0.0478350967168808\n",
      "Epoch 17185/20000 Training Loss: 0.04329347610473633\n",
      "Epoch 17186/20000 Training Loss: 0.05992436408996582\n",
      "Epoch 17187/20000 Training Loss: 0.05949587747454643\n",
      "Epoch 17188/20000 Training Loss: 0.05377082899212837\n",
      "Epoch 17189/20000 Training Loss: 0.03985535725951195\n",
      "Epoch 17190/20000 Training Loss: 0.04151556268334389\n",
      "Epoch 17190/20000 Validation Loss: 0.06127895414829254\n",
      "Epoch 17191/20000 Training Loss: 0.05323301628232002\n",
      "Epoch 17192/20000 Training Loss: 0.04816354438662529\n",
      "Epoch 17193/20000 Training Loss: 0.04833534359931946\n",
      "Epoch 17194/20000 Training Loss: 0.04657672345638275\n",
      "Epoch 17195/20000 Training Loss: 0.03913198783993721\n",
      "Epoch 17196/20000 Training Loss: 0.05191373825073242\n",
      "Epoch 17197/20000 Training Loss: 0.049422383308410645\n",
      "Epoch 17198/20000 Training Loss: 0.045135755091905594\n",
      "Epoch 17199/20000 Training Loss: 0.058632660657167435\n",
      "Epoch 17200/20000 Training Loss: 0.040807999670505524\n",
      "Epoch 17200/20000 Validation Loss: 0.04731035977602005\n",
      "Epoch 17201/20000 Training Loss: 0.051008135080337524\n",
      "Epoch 17202/20000 Training Loss: 0.04405847191810608\n",
      "Epoch 17203/20000 Training Loss: 0.05708984658122063\n",
      "Epoch 17204/20000 Training Loss: 0.045736029744148254\n",
      "Epoch 17205/20000 Training Loss: 0.051942359656095505\n",
      "Epoch 17206/20000 Training Loss: 0.05547047778964043\n",
      "Epoch 17207/20000 Training Loss: 0.05294167995452881\n",
      "Epoch 17208/20000 Training Loss: 0.03786907717585564\n",
      "Epoch 17209/20000 Training Loss: 0.0497128926217556\n",
      "Epoch 17210/20000 Training Loss: 0.05562064051628113\n",
      "Epoch 17210/20000 Validation Loss: 0.08961154520511627\n",
      "Epoch 17211/20000 Training Loss: 0.05863174423575401\n",
      "Epoch 17212/20000 Training Loss: 0.040646567940711975\n",
      "Epoch 17213/20000 Training Loss: 0.050986796617507935\n",
      "Epoch 17214/20000 Training Loss: 0.07346127182245255\n",
      "Epoch 17215/20000 Training Loss: 0.05440788343548775\n",
      "Epoch 17216/20000 Training Loss: 0.046553026884794235\n",
      "Epoch 17217/20000 Training Loss: 0.0588846355676651\n",
      "Epoch 17218/20000 Training Loss: 0.046703826636075974\n",
      "Epoch 17219/20000 Training Loss: 0.053559910506010056\n",
      "Epoch 17220/20000 Training Loss: 0.05927078798413277\n",
      "Epoch 17220/20000 Validation Loss: 0.0717768669128418\n",
      "Epoch 17221/20000 Training Loss: 0.05603216215968132\n",
      "Epoch 17222/20000 Training Loss: 0.05851828679442406\n",
      "Epoch 17223/20000 Training Loss: 0.046870384365320206\n",
      "Epoch 17224/20000 Training Loss: 0.0499870628118515\n",
      "Epoch 17225/20000 Training Loss: 0.04473453760147095\n",
      "Epoch 17226/20000 Training Loss: 0.05159265920519829\n",
      "Epoch 17227/20000 Training Loss: 0.051494013518095016\n",
      "Epoch 17228/20000 Training Loss: 0.06420577317476273\n",
      "Epoch 17229/20000 Training Loss: 0.054213881492614746\n",
      "Epoch 17230/20000 Training Loss: 0.06099170446395874\n",
      "Epoch 17230/20000 Validation Loss: 0.04890674352645874\n",
      "Epoch 17231/20000 Training Loss: 0.057838767766952515\n",
      "Epoch 17232/20000 Training Loss: 0.06968650221824646\n",
      "Epoch 17233/20000 Training Loss: 0.06002384424209595\n",
      "Epoch 17234/20000 Training Loss: 0.07366780936717987\n",
      "Epoch 17235/20000 Training Loss: 0.041032370179891586\n",
      "Epoch 17236/20000 Training Loss: 0.04535544291138649\n",
      "Epoch 17237/20000 Training Loss: 0.059062499552965164\n",
      "Epoch 17238/20000 Training Loss: 0.06921806931495667\n",
      "Epoch 17239/20000 Training Loss: 0.04803309217095375\n",
      "Epoch 17240/20000 Training Loss: 0.05656086280941963\n",
      "Epoch 17240/20000 Validation Loss: 0.049448296427726746\n",
      "Epoch 17241/20000 Training Loss: 0.051054585725069046\n",
      "Epoch 17242/20000 Training Loss: 0.05853040888905525\n",
      "Epoch 17243/20000 Training Loss: 0.056749820709228516\n",
      "Epoch 17244/20000 Training Loss: 0.052913665771484375\n",
      "Epoch 17245/20000 Training Loss: 0.05138125643134117\n",
      "Epoch 17246/20000 Training Loss: 0.05636715888977051\n",
      "Epoch 17247/20000 Training Loss: 0.038012392818927765\n",
      "Epoch 17248/20000 Training Loss: 0.042837854474782944\n",
      "Epoch 17249/20000 Training Loss: 0.05686315894126892\n",
      "Epoch 17250/20000 Training Loss: 0.038827452808618546\n",
      "Epoch 17250/20000 Validation Loss: 0.06687089800834656\n",
      "Epoch 17251/20000 Training Loss: 0.04907258227467537\n",
      "Epoch 17252/20000 Training Loss: 0.06193845346570015\n",
      "Epoch 17253/20000 Training Loss: 0.053963273763656616\n",
      "Epoch 17254/20000 Training Loss: 0.04647570475935936\n",
      "Epoch 17255/20000 Training Loss: 0.04586370289325714\n",
      "Epoch 17256/20000 Training Loss: 0.05591580644249916\n",
      "Epoch 17257/20000 Training Loss: 0.04982450604438782\n",
      "Epoch 17258/20000 Training Loss: 0.0584503598511219\n",
      "Epoch 17259/20000 Training Loss: 0.04606205224990845\n",
      "Epoch 17260/20000 Training Loss: 0.04990081861615181\n",
      "Epoch 17260/20000 Validation Loss: 0.047101687639951706\n",
      "Epoch 17261/20000 Training Loss: 0.05081702396273613\n",
      "Epoch 17262/20000 Training Loss: 0.06533525139093399\n",
      "Epoch 17263/20000 Training Loss: 0.03408408537507057\n",
      "Epoch 17264/20000 Training Loss: 0.063033327460289\n",
      "Epoch 17265/20000 Training Loss: 0.07175024598836899\n",
      "Epoch 17266/20000 Training Loss: 0.052144717425107956\n",
      "Epoch 17267/20000 Training Loss: 0.06105323135852814\n",
      "Epoch 17268/20000 Training Loss: 0.05537385120987892\n",
      "Epoch 17269/20000 Training Loss: 0.044428933411836624\n",
      "Epoch 17270/20000 Training Loss: 0.06294187158346176\n",
      "Epoch 17270/20000 Validation Loss: 0.06535688042640686\n",
      "Epoch 17271/20000 Training Loss: 0.06055997312068939\n",
      "Epoch 17272/20000 Training Loss: 0.052749618887901306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17273/20000 Training Loss: 0.05078780651092529\n",
      "Epoch 17274/20000 Training Loss: 0.052485715597867966\n",
      "Epoch 17275/20000 Training Loss: 0.06102949008345604\n",
      "Epoch 17276/20000 Training Loss: 0.06816691905260086\n",
      "Epoch 17277/20000 Training Loss: 0.06086662784218788\n",
      "Epoch 17278/20000 Training Loss: 0.037748247385025024\n",
      "Epoch 17279/20000 Training Loss: 0.05707782879471779\n",
      "Epoch 17280/20000 Training Loss: 0.06353636831045151\n",
      "Epoch 17280/20000 Validation Loss: 0.0473654568195343\n",
      "Epoch 17281/20000 Training Loss: 0.05949137732386589\n",
      "Epoch 17282/20000 Training Loss: 0.05835371091961861\n",
      "Epoch 17283/20000 Training Loss: 0.07569819688796997\n",
      "Epoch 17284/20000 Training Loss: 0.050701189786195755\n",
      "Epoch 17285/20000 Training Loss: 0.05234761908650398\n",
      "Epoch 17286/20000 Training Loss: 0.06008436158299446\n",
      "Epoch 17287/20000 Training Loss: 0.05102415755391121\n",
      "Epoch 17288/20000 Training Loss: 0.07306715846061707\n",
      "Epoch 17289/20000 Training Loss: 0.044963907450437546\n",
      "Epoch 17290/20000 Training Loss: 0.05911346897482872\n",
      "Epoch 17290/20000 Validation Loss: 0.06462733447551727\n",
      "Epoch 17291/20000 Training Loss: 0.048578813672065735\n",
      "Epoch 17292/20000 Training Loss: 0.05381399020552635\n",
      "Epoch 17293/20000 Training Loss: 0.05716032162308693\n",
      "Epoch 17294/20000 Training Loss: 0.05628015100955963\n",
      "Epoch 17295/20000 Training Loss: 0.05058149993419647\n",
      "Epoch 17296/20000 Training Loss: 0.05311067774891853\n",
      "Epoch 17297/20000 Training Loss: 0.05359905958175659\n",
      "Epoch 17298/20000 Training Loss: 0.05182184278964996\n",
      "Epoch 17299/20000 Training Loss: 0.043655816465616226\n",
      "Epoch 17300/20000 Training Loss: 0.05807038024067879\n",
      "Epoch 17300/20000 Validation Loss: 0.0588500052690506\n",
      "Epoch 17301/20000 Training Loss: 0.04246729612350464\n",
      "Epoch 17302/20000 Training Loss: 0.052976999431848526\n",
      "Epoch 17303/20000 Training Loss: 0.057959992438554764\n",
      "Epoch 17304/20000 Training Loss: 0.04029129818081856\n",
      "Epoch 17305/20000 Training Loss: 0.05633004382252693\n",
      "Epoch 17306/20000 Training Loss: 0.04798536375164986\n",
      "Epoch 17307/20000 Training Loss: 0.05176481604576111\n",
      "Epoch 17308/20000 Training Loss: 0.07251792401075363\n",
      "Epoch 17309/20000 Training Loss: 0.045465756207704544\n",
      "Epoch 17310/20000 Training Loss: 0.04604480043053627\n",
      "Epoch 17310/20000 Validation Loss: 0.05274254083633423\n",
      "Epoch 17311/20000 Training Loss: 0.04947328567504883\n",
      "Epoch 17312/20000 Training Loss: 0.05028776451945305\n",
      "Epoch 17313/20000 Training Loss: 0.0490126796066761\n",
      "Epoch 17314/20000 Training Loss: 0.046496838331222534\n",
      "Epoch 17315/20000 Training Loss: 0.06095967814326286\n",
      "Epoch 17316/20000 Training Loss: 0.08096703141927719\n",
      "Epoch 17317/20000 Training Loss: 0.059829920530319214\n",
      "Epoch 17318/20000 Training Loss: 0.050227995961904526\n",
      "Epoch 17319/20000 Training Loss: 0.050237566232681274\n",
      "Epoch 17320/20000 Training Loss: 0.05059367045760155\n",
      "Epoch 17320/20000 Validation Loss: 0.058767467737197876\n",
      "Epoch 17321/20000 Training Loss: 0.035458825528621674\n",
      "Epoch 17322/20000 Training Loss: 0.046842124313116074\n",
      "Epoch 17323/20000 Training Loss: 0.0447712205350399\n",
      "Epoch 17324/20000 Training Loss: 0.06051252409815788\n",
      "Epoch 17325/20000 Training Loss: 0.05677391588687897\n",
      "Epoch 17326/20000 Training Loss: 0.04566572606563568\n",
      "Epoch 17327/20000 Training Loss: 0.05748434737324715\n",
      "Epoch 17328/20000 Training Loss: 0.042117226868867874\n",
      "Epoch 17329/20000 Training Loss: 0.038639847189188004\n",
      "Epoch 17330/20000 Training Loss: 0.06038254499435425\n",
      "Epoch 17330/20000 Validation Loss: 0.056666284799575806\n",
      "Epoch 17331/20000 Training Loss: 0.059025198221206665\n",
      "Epoch 17332/20000 Training Loss: 0.04442782700061798\n",
      "Epoch 17333/20000 Training Loss: 0.07309146970510483\n",
      "Epoch 17334/20000 Training Loss: 0.054378341883420944\n",
      "Epoch 17335/20000 Training Loss: 0.05772150680422783\n",
      "Epoch 17336/20000 Training Loss: 0.047319937497377396\n",
      "Epoch 17337/20000 Training Loss: 0.08107154816389084\n",
      "Epoch 17338/20000 Training Loss: 0.05018015578389168\n",
      "Epoch 17339/20000 Training Loss: 0.04163029044866562\n",
      "Epoch 17340/20000 Training Loss: 0.034158769994974136\n",
      "Epoch 17340/20000 Validation Loss: 0.06437061727046967\n",
      "Epoch 17341/20000 Training Loss: 0.03253168240189552\n",
      "Epoch 17342/20000 Training Loss: 0.05091562494635582\n",
      "Epoch 17343/20000 Training Loss: 0.0516989529132843\n",
      "Epoch 17344/20000 Training Loss: 0.05690585449337959\n",
      "Epoch 17345/20000 Training Loss: 0.04529254510998726\n",
      "Epoch 17346/20000 Training Loss: 0.05774432420730591\n",
      "Epoch 17347/20000 Training Loss: 0.043955881148576736\n",
      "Epoch 17348/20000 Training Loss: 0.052423980087041855\n",
      "Epoch 17349/20000 Training Loss: 0.056741874665021896\n",
      "Epoch 17350/20000 Training Loss: 0.06754853576421738\n",
      "Epoch 17350/20000 Validation Loss: 0.06622176617383957\n",
      "Epoch 17351/20000 Training Loss: 0.05170249566435814\n",
      "Epoch 17352/20000 Training Loss: 0.04989616200327873\n",
      "Epoch 17353/20000 Training Loss: 0.04691929742693901\n",
      "Epoch 17354/20000 Training Loss: 0.056662846356630325\n",
      "Epoch 17355/20000 Training Loss: 0.06277763098478317\n",
      "Epoch 17356/20000 Training Loss: 0.04622778668999672\n",
      "Epoch 17357/20000 Training Loss: 0.0632966160774231\n",
      "Epoch 17358/20000 Training Loss: 0.06995094567537308\n",
      "Epoch 17359/20000 Training Loss: 0.06221555545926094\n",
      "Epoch 17360/20000 Training Loss: 0.05539742112159729\n",
      "Epoch 17360/20000 Validation Loss: 0.058707185089588165\n",
      "Epoch 17361/20000 Training Loss: 0.05218157172203064\n",
      "Epoch 17362/20000 Training Loss: 0.05098779499530792\n",
      "Epoch 17363/20000 Training Loss: 0.06575741618871689\n",
      "Epoch 17364/20000 Training Loss: 0.06496275961399078\n",
      "Epoch 17365/20000 Training Loss: 0.04534812644124031\n",
      "Epoch 17366/20000 Training Loss: 0.04188501834869385\n",
      "Epoch 17367/20000 Training Loss: 0.06612134724855423\n",
      "Epoch 17368/20000 Training Loss: 0.059714656323194504\n",
      "Epoch 17369/20000 Training Loss: 0.062470387667417526\n",
      "Epoch 17370/20000 Training Loss: 0.04496986046433449\n",
      "Epoch 17370/20000 Validation Loss: 0.03478260710835457\n",
      "Epoch 17371/20000 Training Loss: 0.05362524464726448\n",
      "Epoch 17372/20000 Training Loss: 0.041429802775382996\n",
      "Epoch 17373/20000 Training Loss: 0.06919771432876587\n",
      "Epoch 17374/20000 Training Loss: 0.03943510726094246\n",
      "Epoch 17375/20000 Training Loss: 0.049240779131650925\n",
      "Epoch 17376/20000 Training Loss: 0.05533492937684059\n",
      "Epoch 17377/20000 Training Loss: 0.05179361626505852\n",
      "Epoch 17378/20000 Training Loss: 0.05225299298763275\n",
      "Epoch 17379/20000 Training Loss: 0.05611582472920418\n",
      "Epoch 17380/20000 Training Loss: 0.044616054743528366\n",
      "Epoch 17380/20000 Validation Loss: 0.06829419732093811\n",
      "Epoch 17381/20000 Training Loss: 0.05285319685935974\n",
      "Epoch 17382/20000 Training Loss: 0.04999271407723427\n",
      "Epoch 17383/20000 Training Loss: 0.05510196089744568\n",
      "Epoch 17384/20000 Training Loss: 0.058184098452329636\n",
      "Epoch 17385/20000 Training Loss: 0.0643140897154808\n",
      "Epoch 17386/20000 Training Loss: 0.06063959002494812\n",
      "Epoch 17387/20000 Training Loss: 0.04986780881881714\n",
      "Epoch 17388/20000 Training Loss: 0.062397513538599014\n",
      "Epoch 17389/20000 Training Loss: 0.060429275035858154\n",
      "Epoch 17390/20000 Training Loss: 0.046700507402420044\n",
      "Epoch 17390/20000 Validation Loss: 0.04374406486749649\n",
      "Epoch 17391/20000 Training Loss: 0.04843292757868767\n",
      "Epoch 17392/20000 Training Loss: 0.04115764796733856\n",
      "Epoch 17393/20000 Training Loss: 0.06434555351734161\n",
      "Epoch 17394/20000 Training Loss: 0.05120369419455528\n",
      "Epoch 17395/20000 Training Loss: 0.054350655525922775\n",
      "Epoch 17396/20000 Training Loss: 0.03707238659262657\n",
      "Epoch 17397/20000 Training Loss: 0.04336659982800484\n",
      "Epoch 17398/20000 Training Loss: 0.04415755346417427\n",
      "Epoch 17399/20000 Training Loss: 0.05890396237373352\n",
      "Epoch 17400/20000 Training Loss: 0.06402311474084854\n",
      "Epoch 17400/20000 Validation Loss: 0.06042403727769852\n",
      "Epoch 17401/20000 Training Loss: 0.06047618389129639\n",
      "Epoch 17402/20000 Training Loss: 0.05494244396686554\n",
      "Epoch 17403/20000 Training Loss: 0.04957671836018562\n",
      "Epoch 17404/20000 Training Loss: 0.06019757688045502\n",
      "Epoch 17405/20000 Training Loss: 0.06533157080411911\n",
      "Epoch 17406/20000 Training Loss: 0.05278222635388374\n",
      "Epoch 17407/20000 Training Loss: 0.0616416297852993\n",
      "Epoch 17408/20000 Training Loss: 0.06425882130861282\n",
      "Epoch 17409/20000 Training Loss: 0.059568505734205246\n",
      "Epoch 17410/20000 Training Loss: 0.05803126096725464\n",
      "Epoch 17410/20000 Validation Loss: 0.08034264296293259\n",
      "Epoch 17411/20000 Training Loss: 0.05648024007678032\n",
      "Epoch 17412/20000 Training Loss: 0.062124501913785934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17413/20000 Training Loss: 0.07253345847129822\n",
      "Epoch 17414/20000 Training Loss: 0.06736885756254196\n",
      "Epoch 17415/20000 Training Loss: 0.06415050476789474\n",
      "Epoch 17416/20000 Training Loss: 0.04789768159389496\n",
      "Epoch 17417/20000 Training Loss: 0.0450504869222641\n",
      "Epoch 17418/20000 Training Loss: 0.057695332914590836\n",
      "Epoch 17419/20000 Training Loss: 0.05153161287307739\n",
      "Epoch 17420/20000 Training Loss: 0.05335638299584389\n",
      "Epoch 17420/20000 Validation Loss: 0.04243519902229309\n",
      "Epoch 17421/20000 Training Loss: 0.06224847957491875\n",
      "Epoch 17422/20000 Training Loss: 0.06292915344238281\n",
      "Epoch 17423/20000 Training Loss: 0.05609467625617981\n",
      "Epoch 17424/20000 Training Loss: 0.05467960238456726\n",
      "Epoch 17425/20000 Training Loss: 0.05025944486260414\n",
      "Epoch 17426/20000 Training Loss: 0.051444705575704575\n",
      "Epoch 17427/20000 Training Loss: 0.059202890843153\n",
      "Epoch 17428/20000 Training Loss: 0.04071624204516411\n",
      "Epoch 17429/20000 Training Loss: 0.052009791135787964\n",
      "Epoch 17430/20000 Training Loss: 0.049772944301366806\n",
      "Epoch 17430/20000 Validation Loss: 0.08313094079494476\n",
      "Epoch 17431/20000 Training Loss: 0.0646696612238884\n",
      "Epoch 17432/20000 Training Loss: 0.059880781918764114\n",
      "Epoch 17433/20000 Training Loss: 0.054716091603040695\n",
      "Epoch 17434/20000 Training Loss: 0.041395530104637146\n",
      "Epoch 17435/20000 Training Loss: 0.06174325570464134\n",
      "Epoch 17436/20000 Training Loss: 0.05038072541356087\n",
      "Epoch 17437/20000 Training Loss: 0.05452549457550049\n",
      "Epoch 17438/20000 Training Loss: 0.0583137683570385\n",
      "Epoch 17439/20000 Training Loss: 0.06538984179496765\n",
      "Epoch 17440/20000 Training Loss: 0.0547134168446064\n",
      "Epoch 17440/20000 Validation Loss: 0.05038263648748398\n",
      "Epoch 17441/20000 Training Loss: 0.051357295364141464\n",
      "Epoch 17442/20000 Training Loss: 0.05403020605444908\n",
      "Epoch 17443/20000 Training Loss: 0.05657992884516716\n",
      "Epoch 17444/20000 Training Loss: 0.048181962221860886\n",
      "Epoch 17445/20000 Training Loss: 0.07652892917394638\n",
      "Epoch 17446/20000 Training Loss: 0.061717141419649124\n",
      "Epoch 17447/20000 Training Loss: 0.054391127079725266\n",
      "Epoch 17448/20000 Training Loss: 0.053893208503723145\n",
      "Epoch 17449/20000 Training Loss: 0.058268964290618896\n",
      "Epoch 17450/20000 Training Loss: 0.05884900316596031\n",
      "Epoch 17450/20000 Validation Loss: 0.04846781864762306\n",
      "Epoch 17451/20000 Training Loss: 0.03916236758232117\n",
      "Epoch 17452/20000 Training Loss: 0.04042072594165802\n",
      "Epoch 17453/20000 Training Loss: 0.05735589191317558\n",
      "Epoch 17454/20000 Training Loss: 0.04487079381942749\n",
      "Epoch 17455/20000 Training Loss: 0.06775808334350586\n",
      "Epoch 17456/20000 Training Loss: 0.04831739142537117\n",
      "Epoch 17457/20000 Training Loss: 0.048802632838487625\n",
      "Epoch 17458/20000 Training Loss: 0.04692232981324196\n",
      "Epoch 17459/20000 Training Loss: 0.03074636124074459\n",
      "Epoch 17460/20000 Training Loss: 0.049875494092702866\n",
      "Epoch 17460/20000 Validation Loss: 0.052489519119262695\n",
      "Epoch 17461/20000 Training Loss: 0.042691245675086975\n",
      "Epoch 17462/20000 Training Loss: 0.0656520202755928\n",
      "Epoch 17463/20000 Training Loss: 0.045041635632514954\n",
      "Epoch 17464/20000 Training Loss: 0.05342442914843559\n",
      "Epoch 17465/20000 Training Loss: 0.0667647197842598\n",
      "Epoch 17466/20000 Training Loss: 0.053793128579854965\n",
      "Epoch 17467/20000 Training Loss: 0.04949028789997101\n",
      "Epoch 17468/20000 Training Loss: 0.04085342213511467\n",
      "Epoch 17469/20000 Training Loss: 0.04603385552763939\n",
      "Epoch 17470/20000 Training Loss: 0.06108151748776436\n",
      "Epoch 17470/20000 Validation Loss: 0.060713864862918854\n",
      "Epoch 17471/20000 Training Loss: 0.059219300746917725\n",
      "Epoch 17472/20000 Training Loss: 0.05158048868179321\n",
      "Epoch 17473/20000 Training Loss: 0.0467386357486248\n",
      "Epoch 17474/20000 Training Loss: 0.042907580733299255\n",
      "Epoch 17475/20000 Training Loss: 0.04678913950920105\n",
      "Epoch 17476/20000 Training Loss: 0.04980606213212013\n",
      "Epoch 17477/20000 Training Loss: 0.0525839626789093\n",
      "Epoch 17478/20000 Training Loss: 0.05132340267300606\n",
      "Epoch 17479/20000 Training Loss: 0.04996401071548462\n",
      "Epoch 17480/20000 Training Loss: 0.05137068033218384\n",
      "Epoch 17480/20000 Validation Loss: 0.06281372159719467\n",
      "Epoch 17481/20000 Training Loss: 0.04052233323454857\n",
      "Epoch 17482/20000 Training Loss: 0.05715436115860939\n",
      "Epoch 17483/20000 Training Loss: 0.05053703486919403\n",
      "Epoch 17484/20000 Training Loss: 0.049231965094804764\n",
      "Epoch 17485/20000 Training Loss: 0.0471527986228466\n",
      "Epoch 17486/20000 Training Loss: 0.04952762648463249\n",
      "Epoch 17487/20000 Training Loss: 0.053559910506010056\n",
      "Epoch 17488/20000 Training Loss: 0.0607757605612278\n",
      "Epoch 17489/20000 Training Loss: 0.05041666328907013\n",
      "Epoch 17490/20000 Training Loss: 0.05337462201714516\n",
      "Epoch 17490/20000 Validation Loss: 0.07373732328414917\n",
      "Epoch 17491/20000 Training Loss: 0.060597170144319534\n",
      "Epoch 17492/20000 Training Loss: 0.05287795886397362\n",
      "Epoch 17493/20000 Training Loss: 0.04837473854422569\n",
      "Epoch 17494/20000 Training Loss: 0.06118765473365784\n",
      "Epoch 17495/20000 Training Loss: 0.0464429073035717\n",
      "Epoch 17496/20000 Training Loss: 0.04639263078570366\n",
      "Epoch 17497/20000 Training Loss: 0.04299746826291084\n",
      "Epoch 17498/20000 Training Loss: 0.04990914836525917\n",
      "Epoch 17499/20000 Training Loss: 0.04556846246123314\n",
      "Epoch 17500/20000 Training Loss: 0.04245256260037422\n",
      "Epoch 17500/20000 Validation Loss: 0.06506186723709106\n",
      "Epoch 17501/20000 Training Loss: 0.059858087450265884\n",
      "Epoch 17502/20000 Training Loss: 0.049874600023031235\n",
      "Epoch 17503/20000 Training Loss: 0.043960507959127426\n",
      "Epoch 17504/20000 Training Loss: 0.06219443678855896\n",
      "Epoch 17505/20000 Training Loss: 0.059334445744752884\n",
      "Epoch 17506/20000 Training Loss: 0.06330414861440659\n",
      "Epoch 17507/20000 Training Loss: 0.06153279170393944\n",
      "Epoch 17508/20000 Training Loss: 0.0460500605404377\n",
      "Epoch 17509/20000 Training Loss: 0.05439702048897743\n",
      "Epoch 17510/20000 Training Loss: 0.051116377115249634\n",
      "Epoch 17510/20000 Validation Loss: 0.056793682277202606\n",
      "Epoch 17511/20000 Training Loss: 0.07119586318731308\n",
      "Epoch 17512/20000 Training Loss: 0.05906018614768982\n",
      "Epoch 17513/20000 Training Loss: 0.04052913561463356\n",
      "Epoch 17514/20000 Training Loss: 0.07277114689350128\n",
      "Epoch 17515/20000 Training Loss: 0.05355040356516838\n",
      "Epoch 17516/20000 Training Loss: 0.06469523906707764\n",
      "Epoch 17517/20000 Training Loss: 0.06008957698941231\n",
      "Epoch 17518/20000 Training Loss: 0.05585099384188652\n",
      "Epoch 17519/20000 Training Loss: 0.03923777863383293\n",
      "Epoch 17520/20000 Training Loss: 0.04418039694428444\n",
      "Epoch 17520/20000 Validation Loss: 0.05795656889677048\n",
      "Epoch 17521/20000 Training Loss: 0.06218878924846649\n",
      "Epoch 17522/20000 Training Loss: 0.05525140091776848\n",
      "Epoch 17523/20000 Training Loss: 0.07015460729598999\n",
      "Epoch 17524/20000 Training Loss: 0.07104813307523727\n",
      "Epoch 17525/20000 Training Loss: 0.07486208528280258\n",
      "Epoch 17526/20000 Training Loss: 0.03672768548130989\n",
      "Epoch 17527/20000 Training Loss: 0.0523228757083416\n",
      "Epoch 17528/20000 Training Loss: 0.06963304430246353\n",
      "Epoch 17529/20000 Training Loss: 0.05272609367966652\n",
      "Epoch 17530/20000 Training Loss: 0.04726405814290047\n",
      "Epoch 17530/20000 Validation Loss: 0.07828012853860855\n",
      "Epoch 17531/20000 Training Loss: 0.045937877148389816\n",
      "Epoch 17532/20000 Training Loss: 0.055030208081007004\n",
      "Epoch 17533/20000 Training Loss: 0.04052107408642769\n",
      "Epoch 17534/20000 Training Loss: 0.04926026985049248\n",
      "Epoch 17535/20000 Training Loss: 0.04915255680680275\n",
      "Epoch 17536/20000 Training Loss: 0.06597523391246796\n",
      "Epoch 17537/20000 Training Loss: 0.054399192333221436\n",
      "Epoch 17538/20000 Training Loss: 0.044657155871391296\n",
      "Epoch 17539/20000 Training Loss: 0.04154219105839729\n",
      "Epoch 17540/20000 Training Loss: 0.06777834892272949\n",
      "Epoch 17540/20000 Validation Loss: 0.06142232567071915\n",
      "Epoch 17541/20000 Training Loss: 0.0416739247739315\n",
      "Epoch 17542/20000 Training Loss: 0.059234511107206345\n",
      "Epoch 17543/20000 Training Loss: 0.06581607460975647\n",
      "Epoch 17544/20000 Training Loss: 0.04924657568335533\n",
      "Epoch 17545/20000 Training Loss: 0.057547677308321\n",
      "Epoch 17546/20000 Training Loss: 0.042751431465148926\n",
      "Epoch 17547/20000 Training Loss: 0.05428189039230347\n",
      "Epoch 17548/20000 Training Loss: 0.04806775972247124\n",
      "Epoch 17549/20000 Training Loss: 0.05427629128098488\n",
      "Epoch 17550/20000 Training Loss: 0.050371985882520676\n",
      "Epoch 17550/20000 Validation Loss: 0.07879611849784851\n",
      "Epoch 17551/20000 Training Loss: 0.05066560208797455\n",
      "Epoch 17552/20000 Training Loss: 0.06044800952076912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17553/20000 Training Loss: 0.07118012011051178\n",
      "Epoch 17554/20000 Training Loss: 0.04132260009646416\n",
      "Epoch 17555/20000 Training Loss: 0.06238261237740517\n",
      "Epoch 17556/20000 Training Loss: 0.057740747928619385\n",
      "Epoch 17557/20000 Training Loss: 0.05338962748646736\n",
      "Epoch 17558/20000 Training Loss: 0.038646962493658066\n",
      "Epoch 17559/20000 Training Loss: 0.054237693548202515\n",
      "Epoch 17560/20000 Training Loss: 0.0658334419131279\n",
      "Epoch 17560/20000 Validation Loss: 0.05181235820055008\n",
      "Epoch 17561/20000 Training Loss: 0.059891652315855026\n",
      "Epoch 17562/20000 Training Loss: 0.04626984894275665\n",
      "Epoch 17563/20000 Training Loss: 0.06471925228834152\n",
      "Epoch 17564/20000 Training Loss: 0.043285299092531204\n",
      "Epoch 17565/20000 Training Loss: 0.05728829279541969\n",
      "Epoch 17566/20000 Training Loss: 0.04666751250624657\n",
      "Epoch 17567/20000 Training Loss: 0.06618241220712662\n",
      "Epoch 17568/20000 Training Loss: 0.05406532809138298\n",
      "Epoch 17569/20000 Training Loss: 0.04239291325211525\n",
      "Epoch 17570/20000 Training Loss: 0.04333195462822914\n",
      "Epoch 17570/20000 Validation Loss: 0.054536640644073486\n",
      "Epoch 17571/20000 Training Loss: 0.06819774955511093\n",
      "Epoch 17572/20000 Training Loss: 0.05698506161570549\n",
      "Epoch 17573/20000 Training Loss: 0.051991358399391174\n",
      "Epoch 17574/20000 Training Loss: 0.059851568192243576\n",
      "Epoch 17575/20000 Training Loss: 0.055258605629205704\n",
      "Epoch 17576/20000 Training Loss: 0.061020154505968094\n",
      "Epoch 17577/20000 Training Loss: 0.060464128851890564\n",
      "Epoch 17578/20000 Training Loss: 0.06269841641187668\n",
      "Epoch 17579/20000 Training Loss: 0.05151018500328064\n",
      "Epoch 17580/20000 Training Loss: 0.045246422290802\n",
      "Epoch 17580/20000 Validation Loss: 0.039135582745075226\n",
      "Epoch 17581/20000 Training Loss: 0.06916804611682892\n",
      "Epoch 17582/20000 Training Loss: 0.04820060729980469\n",
      "Epoch 17583/20000 Training Loss: 0.0552532933652401\n",
      "Epoch 17584/20000 Training Loss: 0.04854051396250725\n",
      "Epoch 17585/20000 Training Loss: 0.04443788528442383\n",
      "Epoch 17586/20000 Training Loss: 0.033977802842855453\n",
      "Epoch 17587/20000 Training Loss: 0.047965217381715775\n",
      "Epoch 17588/20000 Training Loss: 0.0511607863008976\n",
      "Epoch 17589/20000 Training Loss: 0.04585106298327446\n",
      "Epoch 17590/20000 Training Loss: 0.05057967081665993\n",
      "Epoch 17590/20000 Validation Loss: 0.05336352810263634\n",
      "Epoch 17591/20000 Training Loss: 0.043391745537519455\n",
      "Epoch 17592/20000 Training Loss: 0.03804192319512367\n",
      "Epoch 17593/20000 Training Loss: 0.0613667368888855\n",
      "Epoch 17594/20000 Training Loss: 0.06349506229162216\n",
      "Epoch 17595/20000 Training Loss: 0.04753924906253815\n",
      "Epoch 17596/20000 Training Loss: 0.07355435937643051\n",
      "Epoch 17597/20000 Training Loss: 0.0665118470788002\n",
      "Epoch 17598/20000 Training Loss: 0.05267342925071716\n",
      "Epoch 17599/20000 Training Loss: 0.047645483165979385\n",
      "Epoch 17600/20000 Training Loss: 0.07098653167486191\n",
      "Epoch 17600/20000 Validation Loss: 0.06710264831781387\n",
      "Epoch 17601/20000 Training Loss: 0.05513870716094971\n",
      "Epoch 17602/20000 Training Loss: 0.07942322641611099\n",
      "Epoch 17603/20000 Training Loss: 0.06247653439640999\n",
      "Epoch 17604/20000 Training Loss: 0.06088687852025032\n",
      "Epoch 17605/20000 Training Loss: 0.07924097031354904\n",
      "Epoch 17606/20000 Training Loss: 0.056224290281534195\n",
      "Epoch 17607/20000 Training Loss: 0.0489065982401371\n",
      "Epoch 17608/20000 Training Loss: 0.05021638050675392\n",
      "Epoch 17609/20000 Training Loss: 0.046407584100961685\n",
      "Epoch 17610/20000 Training Loss: 0.04488418623805046\n",
      "Epoch 17610/20000 Validation Loss: 0.07033580541610718\n",
      "Epoch 17611/20000 Training Loss: 0.07785070687532425\n",
      "Epoch 17612/20000 Training Loss: 0.04162423685193062\n",
      "Epoch 17613/20000 Training Loss: 0.05756762996315956\n",
      "Epoch 17614/20000 Training Loss: 0.04316621646285057\n",
      "Epoch 17615/20000 Training Loss: 0.05370563268661499\n",
      "Epoch 17616/20000 Training Loss: 0.0635378509759903\n",
      "Epoch 17617/20000 Training Loss: 0.041716862469911575\n",
      "Epoch 17618/20000 Training Loss: 0.06707344204187393\n",
      "Epoch 17619/20000 Training Loss: 0.053966838866472244\n",
      "Epoch 17620/20000 Training Loss: 0.04801574721932411\n",
      "Epoch 17620/20000 Validation Loss: 0.07548831403255463\n",
      "Epoch 17621/20000 Training Loss: 0.06003553792834282\n",
      "Epoch 17622/20000 Training Loss: 0.04685528203845024\n",
      "Epoch 17623/20000 Training Loss: 0.053379472345113754\n",
      "Epoch 17624/20000 Training Loss: 0.042408961802721024\n",
      "Epoch 17625/20000 Training Loss: 0.044014543294906616\n",
      "Epoch 17626/20000 Training Loss: 0.049326106905937195\n",
      "Epoch 17627/20000 Training Loss: 0.04879757761955261\n",
      "Epoch 17628/20000 Training Loss: 0.04518107697367668\n",
      "Epoch 17629/20000 Training Loss: 0.07391192764043808\n",
      "Epoch 17630/20000 Training Loss: 0.05246291682124138\n",
      "Epoch 17630/20000 Validation Loss: 0.059682294726371765\n",
      "Epoch 17631/20000 Training Loss: 0.05178149417042732\n",
      "Epoch 17632/20000 Training Loss: 0.055043745785951614\n",
      "Epoch 17633/20000 Training Loss: 0.05333063006401062\n",
      "Epoch 17634/20000 Training Loss: 0.06364493072032928\n",
      "Epoch 17635/20000 Training Loss: 0.05293257534503937\n",
      "Epoch 17636/20000 Training Loss: 0.04132510721683502\n",
      "Epoch 17637/20000 Training Loss: 0.05529967322945595\n",
      "Epoch 17638/20000 Training Loss: 0.04789304360747337\n",
      "Epoch 17639/20000 Training Loss: 0.05534109100699425\n",
      "Epoch 17640/20000 Training Loss: 0.06885978579521179\n",
      "Epoch 17640/20000 Validation Loss: 0.044014595448970795\n",
      "Epoch 17641/20000 Training Loss: 0.05393672361969948\n",
      "Epoch 17642/20000 Training Loss: 0.053543150424957275\n",
      "Epoch 17643/20000 Training Loss: 0.058059513568878174\n",
      "Epoch 17644/20000 Training Loss: 0.049478109925985336\n",
      "Epoch 17645/20000 Training Loss: 0.0632607638835907\n",
      "Epoch 17646/20000 Training Loss: 0.042596448212862015\n",
      "Epoch 17647/20000 Training Loss: 0.04636888578534126\n",
      "Epoch 17648/20000 Training Loss: 0.05992181971669197\n",
      "Epoch 17649/20000 Training Loss: 0.05069240927696228\n",
      "Epoch 17650/20000 Training Loss: 0.05200735852122307\n",
      "Epoch 17650/20000 Validation Loss: 0.04686986654996872\n",
      "Epoch 17651/20000 Training Loss: 0.0337500125169754\n",
      "Epoch 17652/20000 Training Loss: 0.059196192771196365\n",
      "Epoch 17653/20000 Training Loss: 0.051257673650979996\n",
      "Epoch 17654/20000 Training Loss: 0.04934573173522949\n",
      "Epoch 17655/20000 Training Loss: 0.05123987793922424\n",
      "Epoch 17656/20000 Training Loss: 0.043141335248947144\n",
      "Epoch 17657/20000 Training Loss: 0.04122896492481232\n",
      "Epoch 17658/20000 Training Loss: 0.06130271777510643\n",
      "Epoch 17659/20000 Training Loss: 0.05029042437672615\n",
      "Epoch 17660/20000 Training Loss: 0.05439426004886627\n",
      "Epoch 17660/20000 Validation Loss: 0.050545524805784225\n",
      "Epoch 17661/20000 Training Loss: 0.04847428575158119\n",
      "Epoch 17662/20000 Training Loss: 0.04896144941449165\n",
      "Epoch 17663/20000 Training Loss: 0.05740346014499664\n",
      "Epoch 17664/20000 Training Loss: 0.051297497004270554\n",
      "Epoch 17665/20000 Training Loss: 0.07465458661317825\n",
      "Epoch 17666/20000 Training Loss: 0.053278546780347824\n",
      "Epoch 17667/20000 Training Loss: 0.05035187676548958\n",
      "Epoch 17668/20000 Training Loss: 0.05683198571205139\n",
      "Epoch 17669/20000 Training Loss: 0.07507538050413132\n",
      "Epoch 17670/20000 Training Loss: 0.05590367689728737\n",
      "Epoch 17670/20000 Validation Loss: 0.04353378340601921\n",
      "Epoch 17671/20000 Training Loss: 0.04937076196074486\n",
      "Epoch 17672/20000 Training Loss: 0.0627611055970192\n",
      "Epoch 17673/20000 Training Loss: 0.04820401966571808\n",
      "Epoch 17674/20000 Training Loss: 0.049650341272354126\n",
      "Epoch 17675/20000 Training Loss: 0.05143710970878601\n",
      "Epoch 17676/20000 Training Loss: 0.042784690856933594\n",
      "Epoch 17677/20000 Training Loss: 0.03651314973831177\n",
      "Epoch 17678/20000 Training Loss: 0.04675982519984245\n",
      "Epoch 17679/20000 Training Loss: 0.05583837255835533\n",
      "Epoch 17680/20000 Training Loss: 0.04937058314681053\n",
      "Epoch 17680/20000 Validation Loss: 0.04911824315786362\n",
      "Epoch 17681/20000 Training Loss: 0.04400284215807915\n",
      "Epoch 17682/20000 Training Loss: 0.044934794306755066\n",
      "Epoch 17683/20000 Training Loss: 0.03752542659640312\n",
      "Epoch 17684/20000 Training Loss: 0.05915537104010582\n",
      "Epoch 17685/20000 Training Loss: 0.04787709191441536\n",
      "Epoch 17686/20000 Training Loss: 0.06452857702970505\n",
      "Epoch 17687/20000 Training Loss: 0.061923909932374954\n",
      "Epoch 17688/20000 Training Loss: 0.05969538167119026\n",
      "Epoch 17689/20000 Training Loss: 0.047125112265348434\n",
      "Epoch 17690/20000 Training Loss: 0.06975439935922623\n",
      "Epoch 17690/20000 Validation Loss: 0.06118706613779068\n",
      "Epoch 17691/20000 Training Loss: 0.05260628089308739\n",
      "Epoch 17692/20000 Training Loss: 0.05174422636628151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17693/20000 Training Loss: 0.057318855077028275\n",
      "Epoch 17694/20000 Training Loss: 0.03758835420012474\n",
      "Epoch 17695/20000 Training Loss: 0.04777194559574127\n",
      "Epoch 17696/20000 Training Loss: 0.051590804010629654\n",
      "Epoch 17697/20000 Training Loss: 0.05629422143101692\n",
      "Epoch 17698/20000 Training Loss: 0.05571363493800163\n",
      "Epoch 17699/20000 Training Loss: 0.062310267239809036\n",
      "Epoch 17700/20000 Training Loss: 0.06331344693899155\n",
      "Epoch 17700/20000 Validation Loss: 0.053283944725990295\n",
      "Epoch 17701/20000 Training Loss: 0.03289660066366196\n",
      "Epoch 17702/20000 Training Loss: 0.06939329952001572\n",
      "Epoch 17703/20000 Training Loss: 0.061941634863615036\n",
      "Epoch 17704/20000 Training Loss: 0.05090310052037239\n",
      "Epoch 17705/20000 Training Loss: 0.06554426997900009\n",
      "Epoch 17706/20000 Training Loss: 0.06489995121955872\n",
      "Epoch 17707/20000 Training Loss: 0.06842716783285141\n",
      "Epoch 17708/20000 Training Loss: 0.04429720342159271\n",
      "Epoch 17709/20000 Training Loss: 0.04340709373354912\n",
      "Epoch 17710/20000 Training Loss: 0.04859473928809166\n",
      "Epoch 17710/20000 Validation Loss: 0.04563385993242264\n",
      "Epoch 17711/20000 Training Loss: 0.050006914883852005\n",
      "Epoch 17712/20000 Training Loss: 0.04972792789340019\n",
      "Epoch 17713/20000 Training Loss: 0.08127966523170471\n",
      "Epoch 17714/20000 Training Loss: 0.05254139378666878\n",
      "Epoch 17715/20000 Training Loss: 0.06512957066297531\n",
      "Epoch 17716/20000 Training Loss: 0.06830313801765442\n",
      "Epoch 17717/20000 Training Loss: 0.07445015758275986\n",
      "Epoch 17718/20000 Training Loss: 0.050738345831632614\n",
      "Epoch 17719/20000 Training Loss: 0.058006394654512405\n",
      "Epoch 17720/20000 Training Loss: 0.056079257279634476\n",
      "Epoch 17720/20000 Validation Loss: 0.06861473619937897\n",
      "Epoch 17721/20000 Training Loss: 0.0455365926027298\n",
      "Epoch 17722/20000 Training Loss: 0.05286182835698128\n",
      "Epoch 17723/20000 Training Loss: 0.059964168816804886\n",
      "Epoch 17724/20000 Training Loss: 0.04312862083315849\n",
      "Epoch 17725/20000 Training Loss: 0.04814193770289421\n",
      "Epoch 17726/20000 Training Loss: 0.044572293758392334\n",
      "Epoch 17727/20000 Training Loss: 0.05439792945981026\n",
      "Epoch 17728/20000 Training Loss: 0.05446846783161163\n",
      "Epoch 17729/20000 Training Loss: 0.06167213246226311\n",
      "Epoch 17730/20000 Training Loss: 0.04487406089901924\n",
      "Epoch 17730/20000 Validation Loss: 0.07441829144954681\n",
      "Epoch 17731/20000 Training Loss: 0.06320025771856308\n",
      "Epoch 17732/20000 Training Loss: 0.044460177421569824\n",
      "Epoch 17733/20000 Training Loss: 0.04545919969677925\n",
      "Epoch 17734/20000 Training Loss: 0.05800609290599823\n",
      "Epoch 17735/20000 Training Loss: 0.044965196400880814\n",
      "Epoch 17736/20000 Training Loss: 0.06410547345876694\n",
      "Epoch 17737/20000 Training Loss: 0.06454689055681229\n",
      "Epoch 17738/20000 Training Loss: 0.05410156771540642\n",
      "Epoch 17739/20000 Training Loss: 0.04900515079498291\n",
      "Epoch 17740/20000 Training Loss: 0.04437025263905525\n",
      "Epoch 17740/20000 Validation Loss: 0.0650123804807663\n",
      "Epoch 17741/20000 Training Loss: 0.042590390890836716\n",
      "Epoch 17742/20000 Training Loss: 0.039541881531476974\n",
      "Epoch 17743/20000 Training Loss: 0.0434972308576107\n",
      "Epoch 17744/20000 Training Loss: 0.05591660737991333\n",
      "Epoch 17745/20000 Training Loss: 0.04857301712036133\n",
      "Epoch 17746/20000 Training Loss: 0.04904841259121895\n",
      "Epoch 17747/20000 Training Loss: 0.055606525391340256\n",
      "Epoch 17748/20000 Training Loss: 0.04382301867008209\n",
      "Epoch 17749/20000 Training Loss: 0.0555412657558918\n",
      "Epoch 17750/20000 Training Loss: 0.03132132440805435\n",
      "Epoch 17750/20000 Validation Loss: 0.06137414276599884\n",
      "Epoch 17751/20000 Training Loss: 0.038766879588365555\n",
      "Epoch 17752/20000 Training Loss: 0.05863242223858833\n",
      "Epoch 17753/20000 Training Loss: 0.05404162406921387\n",
      "Epoch 17754/20000 Training Loss: 0.06936827301979065\n",
      "Epoch 17755/20000 Training Loss: 0.05308955907821655\n",
      "Epoch 17756/20000 Training Loss: 0.08212032914161682\n",
      "Epoch 17757/20000 Training Loss: 0.04779988154768944\n",
      "Epoch 17758/20000 Training Loss: 0.0418945848941803\n",
      "Epoch 17759/20000 Training Loss: 0.06484071165323257\n",
      "Epoch 17760/20000 Training Loss: 0.0652044489979744\n",
      "Epoch 17760/20000 Validation Loss: 0.05222994089126587\n",
      "Epoch 17761/20000 Training Loss: 0.06836581975221634\n",
      "Epoch 17762/20000 Training Loss: 0.051023077219724655\n",
      "Epoch 17763/20000 Training Loss: 0.04053488001227379\n",
      "Epoch 17764/20000 Training Loss: 0.060511816293001175\n",
      "Epoch 17765/20000 Training Loss: 0.062046606093645096\n",
      "Epoch 17766/20000 Training Loss: 0.055179670453071594\n",
      "Epoch 17767/20000 Training Loss: 0.04851658642292023\n",
      "Epoch 17768/20000 Training Loss: 0.06147947907447815\n",
      "Epoch 17769/20000 Training Loss: 0.05118035152554512\n",
      "Epoch 17770/20000 Training Loss: 0.043324511498212814\n",
      "Epoch 17770/20000 Validation Loss: 0.05253644660115242\n",
      "Epoch 17771/20000 Training Loss: 0.05221739411354065\n",
      "Epoch 17772/20000 Training Loss: 0.05319410562515259\n",
      "Epoch 17773/20000 Training Loss: 0.05390527844429016\n",
      "Epoch 17774/20000 Training Loss: 0.042570650577545166\n",
      "Epoch 17775/20000 Training Loss: 0.0510844923555851\n",
      "Epoch 17776/20000 Training Loss: 0.051312971860170364\n",
      "Epoch 17777/20000 Training Loss: 0.03808409720659256\n",
      "Epoch 17778/20000 Training Loss: 0.043791696429252625\n",
      "Epoch 17779/20000 Training Loss: 0.048875898122787476\n",
      "Epoch 17780/20000 Training Loss: 0.060543786734342575\n",
      "Epoch 17780/20000 Validation Loss: 0.04930264502763748\n",
      "Epoch 17781/20000 Training Loss: 0.06651649624109268\n",
      "Epoch 17782/20000 Training Loss: 0.05424150824546814\n",
      "Epoch 17783/20000 Training Loss: 0.0543459951877594\n",
      "Epoch 17784/20000 Training Loss: 0.054578304290771484\n",
      "Epoch 17785/20000 Training Loss: 0.059697020798921585\n",
      "Epoch 17786/20000 Training Loss: 0.04219989851117134\n",
      "Epoch 17787/20000 Training Loss: 0.04792224243283272\n",
      "Epoch 17788/20000 Training Loss: 0.045778363943099976\n",
      "Epoch 17789/20000 Training Loss: 0.043771401047706604\n",
      "Epoch 17790/20000 Training Loss: 0.05024735629558563\n",
      "Epoch 17790/20000 Validation Loss: 0.05980398505926132\n",
      "Epoch 17791/20000 Training Loss: 0.0571175180375576\n",
      "Epoch 17792/20000 Training Loss: 0.054590463638305664\n",
      "Epoch 17793/20000 Training Loss: 0.05242791771888733\n",
      "Epoch 17794/20000 Training Loss: 0.049121346324682236\n",
      "Epoch 17795/20000 Training Loss: 0.04097580909729004\n",
      "Epoch 17796/20000 Training Loss: 0.05117383599281311\n",
      "Epoch 17797/20000 Training Loss: 0.06892502307891846\n",
      "Epoch 17798/20000 Training Loss: 0.05824838578701019\n",
      "Epoch 17799/20000 Training Loss: 0.060230761766433716\n",
      "Epoch 17800/20000 Training Loss: 0.06441745907068253\n",
      "Epoch 17800/20000 Validation Loss: 0.0490848645567894\n",
      "Epoch 17801/20000 Training Loss: 0.0649549663066864\n",
      "Epoch 17802/20000 Training Loss: 0.04820580407977104\n",
      "Epoch 17803/20000 Training Loss: 0.03634704649448395\n",
      "Epoch 17804/20000 Training Loss: 0.05108928307890892\n",
      "Epoch 17805/20000 Training Loss: 0.057783495634794235\n",
      "Epoch 17806/20000 Training Loss: 0.059228215366601944\n",
      "Epoch 17807/20000 Training Loss: 0.05502225086092949\n",
      "Epoch 17808/20000 Training Loss: 0.039070453494787216\n",
      "Epoch 17809/20000 Training Loss: 0.04834645986557007\n",
      "Epoch 17810/20000 Training Loss: 0.0606117807328701\n",
      "Epoch 17810/20000 Validation Loss: 0.06013525277376175\n",
      "Epoch 17811/20000 Training Loss: 0.04947800934314728\n",
      "Epoch 17812/20000 Training Loss: 0.05673618242144585\n",
      "Epoch 17813/20000 Training Loss: 0.03757176175713539\n",
      "Epoch 17814/20000 Training Loss: 0.049605872482061386\n",
      "Epoch 17815/20000 Training Loss: 0.04809263348579407\n",
      "Epoch 17816/20000 Training Loss: 0.06618190556764603\n",
      "Epoch 17817/20000 Training Loss: 0.051327209919691086\n",
      "Epoch 17818/20000 Training Loss: 0.04600357636809349\n",
      "Epoch 17819/20000 Training Loss: 0.08658071607351303\n",
      "Epoch 17820/20000 Training Loss: 0.04732556641101837\n",
      "Epoch 17820/20000 Validation Loss: 0.06504175066947937\n",
      "Epoch 17821/20000 Training Loss: 0.061906274408102036\n",
      "Epoch 17822/20000 Training Loss: 0.06492473930120468\n",
      "Epoch 17823/20000 Training Loss: 0.07191579788923264\n",
      "Epoch 17824/20000 Training Loss: 0.06008027121424675\n",
      "Epoch 17825/20000 Training Loss: 0.0730690136551857\n",
      "Epoch 17826/20000 Training Loss: 0.06405513733625412\n",
      "Epoch 17827/20000 Training Loss: 0.05717508867383003\n",
      "Epoch 17828/20000 Training Loss: 0.04886813089251518\n",
      "Epoch 17829/20000 Training Loss: 0.057888153940439224\n",
      "Epoch 17830/20000 Training Loss: 0.04813733696937561\n",
      "Epoch 17830/20000 Validation Loss: 0.05092429369688034\n",
      "Epoch 17831/20000 Training Loss: 0.06623219698667526\n",
      "Epoch 17832/20000 Training Loss: 0.03805163502693176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17833/20000 Training Loss: 0.04198871925473213\n",
      "Epoch 17834/20000 Training Loss: 0.057872164994478226\n",
      "Epoch 17835/20000 Training Loss: 0.056283678859472275\n",
      "Epoch 17836/20000 Training Loss: 0.04568449780344963\n",
      "Epoch 17837/20000 Training Loss: 0.05347320809960365\n",
      "Epoch 17838/20000 Training Loss: 0.0734291598200798\n",
      "Epoch 17839/20000 Training Loss: 0.05022386834025383\n",
      "Epoch 17840/20000 Training Loss: 0.06426920741796494\n",
      "Epoch 17840/20000 Validation Loss: 0.04410819709300995\n",
      "Epoch 17841/20000 Training Loss: 0.06158773973584175\n",
      "Epoch 17842/20000 Training Loss: 0.053284287452697754\n",
      "Epoch 17843/20000 Training Loss: 0.046852245926856995\n",
      "Epoch 17844/20000 Training Loss: 0.0614771842956543\n",
      "Epoch 17845/20000 Training Loss: 0.055104658007621765\n",
      "Epoch 17846/20000 Training Loss: 0.044906675815582275\n",
      "Epoch 17847/20000 Training Loss: 0.059119801968336105\n",
      "Epoch 17848/20000 Training Loss: 0.05425317957997322\n",
      "Epoch 17849/20000 Training Loss: 0.03640424460172653\n",
      "Epoch 17850/20000 Training Loss: 0.05795343592762947\n",
      "Epoch 17850/20000 Validation Loss: 0.07159441709518433\n",
      "Epoch 17851/20000 Training Loss: 0.05365167185664177\n",
      "Epoch 17852/20000 Training Loss: 0.053839217871427536\n",
      "Epoch 17853/20000 Training Loss: 0.06116948649287224\n",
      "Epoch 17854/20000 Training Loss: 0.06054035946726799\n",
      "Epoch 17855/20000 Training Loss: 0.048391085118055344\n",
      "Epoch 17856/20000 Training Loss: 0.05234473571181297\n",
      "Epoch 17857/20000 Training Loss: 0.06296972185373306\n",
      "Epoch 17858/20000 Training Loss: 0.05180232599377632\n",
      "Epoch 17859/20000 Training Loss: 0.045693475753068924\n",
      "Epoch 17860/20000 Training Loss: 0.049392327666282654\n",
      "Epoch 17860/20000 Validation Loss: 0.04454582929611206\n",
      "Epoch 17861/20000 Training Loss: 0.03722362965345383\n",
      "Epoch 17862/20000 Training Loss: 0.06291179358959198\n",
      "Epoch 17863/20000 Training Loss: 0.059104178100824356\n",
      "Epoch 17864/20000 Training Loss: 0.052749037742614746\n",
      "Epoch 17865/20000 Training Loss: 0.06102227792143822\n",
      "Epoch 17866/20000 Training Loss: 0.04849286004900932\n",
      "Epoch 17867/20000 Training Loss: 0.04959068074822426\n",
      "Epoch 17868/20000 Training Loss: 0.055937349796295166\n",
      "Epoch 17869/20000 Training Loss: 0.04729392006993294\n",
      "Epoch 17870/20000 Training Loss: 0.06656662374734879\n",
      "Epoch 17870/20000 Validation Loss: 0.05292806774377823\n",
      "Epoch 17871/20000 Training Loss: 0.050998881459236145\n",
      "Epoch 17872/20000 Training Loss: 0.06510384380817413\n",
      "Epoch 17873/20000 Training Loss: 0.057201892137527466\n",
      "Epoch 17874/20000 Training Loss: 0.05020575225353241\n",
      "Epoch 17875/20000 Training Loss: 0.0525275357067585\n",
      "Epoch 17876/20000 Training Loss: 0.06518295407295227\n",
      "Epoch 17877/20000 Training Loss: 0.044068071991205215\n",
      "Epoch 17878/20000 Training Loss: 0.05263352394104004\n",
      "Epoch 17879/20000 Training Loss: 0.055205345153808594\n",
      "Epoch 17880/20000 Training Loss: 0.06104668974876404\n",
      "Epoch 17880/20000 Validation Loss: 0.057134680449962616\n",
      "Epoch 17881/20000 Training Loss: 0.05248301103711128\n",
      "Epoch 17882/20000 Training Loss: 0.05364738404750824\n",
      "Epoch 17883/20000 Training Loss: 0.04053759202361107\n",
      "Epoch 17884/20000 Training Loss: 0.047476500272750854\n",
      "Epoch 17885/20000 Training Loss: 0.04318958520889282\n",
      "Epoch 17886/20000 Training Loss: 0.0632040798664093\n",
      "Epoch 17887/20000 Training Loss: 0.04990528151392937\n",
      "Epoch 17888/20000 Training Loss: 0.05865609645843506\n",
      "Epoch 17889/20000 Training Loss: 0.0652615875005722\n",
      "Epoch 17890/20000 Training Loss: 0.0539788119494915\n",
      "Epoch 17890/20000 Validation Loss: 0.08225105702877045\n",
      "Epoch 17891/20000 Training Loss: 0.04377036169171333\n",
      "Epoch 17892/20000 Training Loss: 0.05968509986996651\n",
      "Epoch 17893/20000 Training Loss: 0.07387488335371017\n",
      "Epoch 17894/20000 Training Loss: 0.04595272243022919\n",
      "Epoch 17895/20000 Training Loss: 0.04701492562890053\n",
      "Epoch 17896/20000 Training Loss: 0.05187379941344261\n",
      "Epoch 17897/20000 Training Loss: 0.06726039946079254\n",
      "Epoch 17898/20000 Training Loss: 0.0545516200363636\n",
      "Epoch 17899/20000 Training Loss: 0.07669287174940109\n",
      "Epoch 17900/20000 Training Loss: 0.041405465453863144\n",
      "Epoch 17900/20000 Validation Loss: 0.04858839884400368\n",
      "Epoch 17901/20000 Training Loss: 0.060076549649238586\n",
      "Epoch 17902/20000 Training Loss: 0.04533563181757927\n",
      "Epoch 17903/20000 Training Loss: 0.04710302874445915\n",
      "Epoch 17904/20000 Training Loss: 0.055525824427604675\n",
      "Epoch 17905/20000 Training Loss: 0.046866998076438904\n",
      "Epoch 17906/20000 Training Loss: 0.03725169971585274\n",
      "Epoch 17907/20000 Training Loss: 0.06400757282972336\n",
      "Epoch 17908/20000 Training Loss: 0.05341873690485954\n",
      "Epoch 17909/20000 Training Loss: 0.06317427009344101\n",
      "Epoch 17910/20000 Training Loss: 0.05996983125805855\n",
      "Epoch 17910/20000 Validation Loss: 0.05804355815052986\n",
      "Epoch 17911/20000 Training Loss: 0.07214126735925674\n",
      "Epoch 17912/20000 Training Loss: 0.06650220602750778\n",
      "Epoch 17913/20000 Training Loss: 0.06303461641073227\n",
      "Epoch 17914/20000 Training Loss: 0.06946920603513718\n",
      "Epoch 17915/20000 Training Loss: 0.06161072477698326\n",
      "Epoch 17916/20000 Training Loss: 0.0441870242357254\n",
      "Epoch 17917/20000 Training Loss: 0.05562451854348183\n",
      "Epoch 17918/20000 Training Loss: 0.04943843185901642\n",
      "Epoch 17919/20000 Training Loss: 0.05023181065917015\n",
      "Epoch 17920/20000 Training Loss: 0.05120443180203438\n",
      "Epoch 17920/20000 Validation Loss: 0.045052669942379\n",
      "Epoch 17921/20000 Training Loss: 0.049744367599487305\n",
      "Epoch 17922/20000 Training Loss: 0.0498289130628109\n",
      "Epoch 17923/20000 Training Loss: 0.04765164852142334\n",
      "Epoch 17924/20000 Training Loss: 0.05091997608542442\n",
      "Epoch 17925/20000 Training Loss: 0.05849955603480339\n",
      "Epoch 17926/20000 Training Loss: 0.04625816270709038\n",
      "Epoch 17927/20000 Training Loss: 0.042891811579465866\n",
      "Epoch 17928/20000 Training Loss: 0.0757794976234436\n",
      "Epoch 17929/20000 Training Loss: 0.05254615470767021\n",
      "Epoch 17930/20000 Training Loss: 0.053485557436943054\n",
      "Epoch 17930/20000 Validation Loss: 0.057687900960445404\n",
      "Epoch 17931/20000 Training Loss: 0.05627240613102913\n",
      "Epoch 17932/20000 Training Loss: 0.06834709644317627\n",
      "Epoch 17933/20000 Training Loss: 0.050835028290748596\n",
      "Epoch 17934/20000 Training Loss: 0.055609118193387985\n",
      "Epoch 17935/20000 Training Loss: 0.056934718042612076\n",
      "Epoch 17936/20000 Training Loss: 0.06510262936353683\n",
      "Epoch 17937/20000 Training Loss: 0.041411370038986206\n",
      "Epoch 17938/20000 Training Loss: 0.04428964853286743\n",
      "Epoch 17939/20000 Training Loss: 0.05050750449299812\n",
      "Epoch 17940/20000 Training Loss: 0.0538877435028553\n",
      "Epoch 17940/20000 Validation Loss: 0.04793963581323624\n",
      "Epoch 17941/20000 Training Loss: 0.05887943506240845\n",
      "Epoch 17942/20000 Training Loss: 0.05609561875462532\n",
      "Epoch 17943/20000 Training Loss: 0.04037558659911156\n",
      "Epoch 17944/20000 Training Loss: 0.05248095095157623\n",
      "Epoch 17945/20000 Training Loss: 0.05227179452776909\n",
      "Epoch 17946/20000 Training Loss: 0.053124796599149704\n",
      "Epoch 17947/20000 Training Loss: 0.047420646995306015\n",
      "Epoch 17948/20000 Training Loss: 0.05164477229118347\n",
      "Epoch 17949/20000 Training Loss: 0.06409893184900284\n",
      "Epoch 17950/20000 Training Loss: 0.06702645868062973\n",
      "Epoch 17950/20000 Validation Loss: 0.043858982622623444\n",
      "Epoch 17951/20000 Training Loss: 0.06730178743600845\n",
      "Epoch 17952/20000 Training Loss: 0.05139670893549919\n",
      "Epoch 17953/20000 Training Loss: 0.06372084468603134\n",
      "Epoch 17954/20000 Training Loss: 0.06270291656255722\n",
      "Epoch 17955/20000 Training Loss: 0.05889606475830078\n",
      "Epoch 17956/20000 Training Loss: 0.05087152495980263\n",
      "Epoch 17957/20000 Training Loss: 0.050731953233480453\n",
      "Epoch 17958/20000 Training Loss: 0.05856376886367798\n",
      "Epoch 17959/20000 Training Loss: 0.0639072135090828\n",
      "Epoch 17960/20000 Training Loss: 0.06090109422802925\n",
      "Epoch 17960/20000 Validation Loss: 0.054204512387514114\n",
      "Epoch 17961/20000 Training Loss: 0.05013604834675789\n",
      "Epoch 17962/20000 Training Loss: 0.046096596866846085\n",
      "Epoch 17963/20000 Training Loss: 0.06779659539461136\n",
      "Epoch 17964/20000 Training Loss: 0.057954758405685425\n",
      "Epoch 17965/20000 Training Loss: 0.0396227203309536\n",
      "Epoch 17966/20000 Training Loss: 0.059322208166122437\n",
      "Epoch 17967/20000 Training Loss: 0.07790915668010712\n",
      "Epoch 17968/20000 Training Loss: 0.044990379363298416\n",
      "Epoch 17969/20000 Training Loss: 0.05437276139855385\n",
      "Epoch 17970/20000 Training Loss: 0.05425478145480156\n",
      "Epoch 17970/20000 Validation Loss: 0.061263248324394226\n",
      "Epoch 17971/20000 Training Loss: 0.058817435055971146\n",
      "Epoch 17972/20000 Training Loss: 0.06026312708854675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17973/20000 Training Loss: 0.05422667786478996\n",
      "Epoch 17974/20000 Training Loss: 0.055599942803382874\n",
      "Epoch 17975/20000 Training Loss: 0.05341999605298042\n",
      "Epoch 17976/20000 Training Loss: 0.05799486115574837\n",
      "Epoch 17977/20000 Training Loss: 0.051123619079589844\n",
      "Epoch 17978/20000 Training Loss: 0.04338288679718971\n",
      "Epoch 17979/20000 Training Loss: 0.05974981561303139\n",
      "Epoch 17980/20000 Training Loss: 0.054669033735990524\n",
      "Epoch 17980/20000 Validation Loss: 0.06718006730079651\n",
      "Epoch 17981/20000 Training Loss: 0.05862625315785408\n",
      "Epoch 17982/20000 Training Loss: 0.05954824015498161\n",
      "Epoch 17983/20000 Training Loss: 0.08352134376764297\n",
      "Epoch 17984/20000 Training Loss: 0.062389571219682693\n",
      "Epoch 17985/20000 Training Loss: 0.04189634323120117\n",
      "Epoch 17986/20000 Training Loss: 0.05384351685643196\n",
      "Epoch 17987/20000 Training Loss: 0.04720460996031761\n",
      "Epoch 17988/20000 Training Loss: 0.05200241133570671\n",
      "Epoch 17989/20000 Training Loss: 0.04715064540505409\n",
      "Epoch 17990/20000 Training Loss: 0.06016223505139351\n",
      "Epoch 17990/20000 Validation Loss: 0.04904421791434288\n",
      "Epoch 17991/20000 Training Loss: 0.04745877906680107\n",
      "Epoch 17992/20000 Training Loss: 0.04778091609477997\n",
      "Epoch 17993/20000 Training Loss: 0.051444392651319504\n",
      "Epoch 17994/20000 Training Loss: 0.04799222946166992\n",
      "Epoch 17995/20000 Training Loss: 0.05447189509868622\n",
      "Epoch 17996/20000 Training Loss: 0.06524060666561127\n",
      "Epoch 17997/20000 Training Loss: 0.06755568832159042\n",
      "Epoch 17998/20000 Training Loss: 0.03511759638786316\n",
      "Epoch 17999/20000 Training Loss: 0.039435211569070816\n",
      "Epoch 18000/20000 Training Loss: 0.05448351427912712\n",
      "Epoch 18000/20000 Validation Loss: 0.05982071906328201\n",
      "Epoch 18001/20000 Training Loss: 0.04322533309459686\n",
      "Epoch 18002/20000 Training Loss: 0.06911491602659225\n",
      "Epoch 18003/20000 Training Loss: 0.054555315524339676\n",
      "Epoch 18004/20000 Training Loss: 0.06889709830284119\n",
      "Epoch 18005/20000 Training Loss: 0.05536551773548126\n",
      "Epoch 18006/20000 Training Loss: 0.054859112948179245\n",
      "Epoch 18007/20000 Training Loss: 0.0466880202293396\n",
      "Epoch 18008/20000 Training Loss: 0.06112683191895485\n",
      "Epoch 18009/20000 Training Loss: 0.04595448449254036\n",
      "Epoch 18010/20000 Training Loss: 0.05885746702551842\n",
      "Epoch 18010/20000 Validation Loss: 0.05192189663648605\n",
      "Epoch 18011/20000 Training Loss: 0.05232146754860878\n",
      "Epoch 18012/20000 Training Loss: 0.05314779281616211\n",
      "Epoch 18013/20000 Training Loss: 0.06085138022899628\n",
      "Epoch 18014/20000 Training Loss: 0.05826764181256294\n",
      "Epoch 18015/20000 Training Loss: 0.04892794415354729\n",
      "Epoch 18016/20000 Training Loss: 0.04454195126891136\n",
      "Epoch 18017/20000 Training Loss: 0.055932674556970596\n",
      "Epoch 18018/20000 Training Loss: 0.05426657572388649\n",
      "Epoch 18019/20000 Training Loss: 0.05530701205134392\n",
      "Epoch 18020/20000 Training Loss: 0.05421494320034981\n",
      "Epoch 18020/20000 Validation Loss: 0.047440581023693085\n",
      "Epoch 18021/20000 Training Loss: 0.04821896553039551\n",
      "Epoch 18022/20000 Training Loss: 0.04623007774353027\n",
      "Epoch 18023/20000 Training Loss: 0.0471864677965641\n",
      "Epoch 18024/20000 Training Loss: 0.05929572880268097\n",
      "Epoch 18025/20000 Training Loss: 0.056577812880277634\n",
      "Epoch 18026/20000 Training Loss: 0.046800851821899414\n",
      "Epoch 18027/20000 Training Loss: 0.04667562618851662\n",
      "Epoch 18028/20000 Training Loss: 0.046621810644865036\n",
      "Epoch 18029/20000 Training Loss: 0.045815642923116684\n",
      "Epoch 18030/20000 Training Loss: 0.03898700699210167\n",
      "Epoch 18030/20000 Validation Loss: 0.0606999471783638\n",
      "Epoch 18031/20000 Training Loss: 0.044266313314437866\n",
      "Epoch 18032/20000 Training Loss: 0.06350641697645187\n",
      "Epoch 18033/20000 Training Loss: 0.04239291325211525\n",
      "Epoch 18034/20000 Training Loss: 0.03982368856668472\n",
      "Epoch 18035/20000 Training Loss: 0.048877567052841187\n",
      "Epoch 18036/20000 Training Loss: 0.04598899558186531\n",
      "Epoch 18037/20000 Training Loss: 0.051203083246946335\n",
      "Epoch 18038/20000 Training Loss: 0.055476024746894836\n",
      "Epoch 18039/20000 Training Loss: 0.04389257729053497\n",
      "Epoch 18040/20000 Training Loss: 0.06019577383995056\n",
      "Epoch 18040/20000 Validation Loss: 0.04878952354192734\n",
      "Epoch 18041/20000 Training Loss: 0.05588502436876297\n",
      "Epoch 18042/20000 Training Loss: 0.05063043534755707\n",
      "Epoch 18043/20000 Training Loss: 0.05396069958806038\n",
      "Epoch 18044/20000 Training Loss: 0.042360663414001465\n",
      "Epoch 18045/20000 Training Loss: 0.07200371474027634\n",
      "Epoch 18046/20000 Training Loss: 0.03949734941124916\n",
      "Epoch 18047/20000 Training Loss: 0.06529742479324341\n",
      "Epoch 18048/20000 Training Loss: 0.05897161737084389\n",
      "Epoch 18049/20000 Training Loss: 0.05008045956492424\n",
      "Epoch 18050/20000 Training Loss: 0.06810855865478516\n",
      "Epoch 18050/20000 Validation Loss: 0.048457272350788116\n",
      "Epoch 18051/20000 Training Loss: 0.049741119146347046\n",
      "Epoch 18052/20000 Training Loss: 0.06401994079351425\n",
      "Epoch 18053/20000 Training Loss: 0.06884870678186417\n",
      "Epoch 18054/20000 Training Loss: 0.04423059895634651\n",
      "Epoch 18055/20000 Training Loss: 0.04295024275779724\n",
      "Epoch 18056/20000 Training Loss: 0.05744706466794014\n",
      "Epoch 18057/20000 Training Loss: 0.04335661605000496\n",
      "Epoch 18058/20000 Training Loss: 0.044781286269426346\n",
      "Epoch 18059/20000 Training Loss: 0.043823178857564926\n",
      "Epoch 18060/20000 Training Loss: 0.039396923035383224\n",
      "Epoch 18060/20000 Validation Loss: 0.045269355177879333\n",
      "Epoch 18061/20000 Training Loss: 0.06269706040620804\n",
      "Epoch 18062/20000 Training Loss: 0.05844692513346672\n",
      "Epoch 18063/20000 Training Loss: 0.05355352163314819\n",
      "Epoch 18064/20000 Training Loss: 0.06879512220621109\n",
      "Epoch 18065/20000 Training Loss: 0.04826712980866432\n",
      "Epoch 18066/20000 Training Loss: 0.05338728055357933\n",
      "Epoch 18067/20000 Training Loss: 0.04560847207903862\n",
      "Epoch 18068/20000 Training Loss: 0.05400073528289795\n",
      "Epoch 18069/20000 Training Loss: 0.03846501186490059\n",
      "Epoch 18070/20000 Training Loss: 0.057682812213897705\n",
      "Epoch 18070/20000 Validation Loss: 0.0647282674908638\n",
      "Epoch 18071/20000 Training Loss: 0.06315438449382782\n",
      "Epoch 18072/20000 Training Loss: 0.05246106907725334\n",
      "Epoch 18073/20000 Training Loss: 0.05421733856201172\n",
      "Epoch 18074/20000 Training Loss: 0.06418246775865555\n",
      "Epoch 18075/20000 Training Loss: 0.05781202390789986\n",
      "Epoch 18076/20000 Training Loss: 0.05764402076601982\n",
      "Epoch 18077/20000 Training Loss: 0.05337681993842125\n",
      "Epoch 18078/20000 Training Loss: 0.036770883947610855\n",
      "Epoch 18079/20000 Training Loss: 0.05579842999577522\n",
      "Epoch 18080/20000 Training Loss: 0.05193132162094116\n",
      "Epoch 18080/20000 Validation Loss: 0.0510893352329731\n",
      "Epoch 18081/20000 Training Loss: 0.0706433430314064\n",
      "Epoch 18082/20000 Training Loss: 0.038009513169527054\n",
      "Epoch 18083/20000 Training Loss: 0.06513607501983643\n",
      "Epoch 18084/20000 Training Loss: 0.039807748049497604\n",
      "Epoch 18085/20000 Training Loss: 0.03843971714377403\n",
      "Epoch 18086/20000 Training Loss: 0.041429776698350906\n",
      "Epoch 18087/20000 Training Loss: 0.058022093027830124\n",
      "Epoch 18088/20000 Training Loss: 0.039572905749082565\n",
      "Epoch 18089/20000 Training Loss: 0.060223087668418884\n",
      "Epoch 18090/20000 Training Loss: 0.058083366602659225\n",
      "Epoch 18090/20000 Validation Loss: 0.05990048497915268\n",
      "Epoch 18091/20000 Training Loss: 0.06568325310945511\n",
      "Epoch 18092/20000 Training Loss: 0.044971924275159836\n",
      "Epoch 18093/20000 Training Loss: 0.07412650436162949\n",
      "Epoch 18094/20000 Training Loss: 0.05840550735592842\n",
      "Epoch 18095/20000 Training Loss: 0.04696870222687721\n",
      "Epoch 18096/20000 Training Loss: 0.06180736422538757\n",
      "Epoch 18097/20000 Training Loss: 0.040785256773233414\n",
      "Epoch 18098/20000 Training Loss: 0.058981847018003464\n",
      "Epoch 18099/20000 Training Loss: 0.04307471215724945\n",
      "Epoch 18100/20000 Training Loss: 0.05401613190770149\n",
      "Epoch 18100/20000 Validation Loss: 0.05845503881573677\n",
      "Epoch 18101/20000 Training Loss: 0.05484108626842499\n",
      "Epoch 18102/20000 Training Loss: 0.04935995861887932\n",
      "Epoch 18103/20000 Training Loss: 0.056098878383636475\n",
      "Epoch 18104/20000 Training Loss: 0.07050789147615433\n",
      "Epoch 18105/20000 Training Loss: 0.062458138912916183\n",
      "Epoch 18106/20000 Training Loss: 0.07047871500253677\n",
      "Epoch 18107/20000 Training Loss: 0.07399522513151169\n",
      "Epoch 18108/20000 Training Loss: 0.03652200102806091\n",
      "Epoch 18109/20000 Training Loss: 0.06322766095399857\n",
      "Epoch 18110/20000 Training Loss: 0.07024580240249634\n",
      "Epoch 18110/20000 Validation Loss: 0.09850390255451202\n",
      "Epoch 18111/20000 Training Loss: 0.05210535600781441\n",
      "Epoch 18112/20000 Training Loss: 0.049616217613220215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18113/20000 Training Loss: 0.04068906977772713\n",
      "Epoch 18114/20000 Training Loss: 0.05524309352040291\n",
      "Epoch 18115/20000 Training Loss: 0.06804975867271423\n",
      "Epoch 18116/20000 Training Loss: 0.0726473331451416\n",
      "Epoch 18117/20000 Training Loss: 0.05050363019108772\n",
      "Epoch 18118/20000 Training Loss: 0.04135957732796669\n",
      "Epoch 18119/20000 Training Loss: 0.04881848767399788\n",
      "Epoch 18120/20000 Training Loss: 0.04454806447029114\n",
      "Epoch 18120/20000 Validation Loss: 0.048425137996673584\n",
      "Epoch 18121/20000 Training Loss: 0.06719885021448135\n",
      "Epoch 18122/20000 Training Loss: 0.0633363351225853\n",
      "Epoch 18123/20000 Training Loss: 0.06484817713499069\n",
      "Epoch 18124/20000 Training Loss: 0.05269024893641472\n",
      "Epoch 18125/20000 Training Loss: 0.06575964391231537\n",
      "Epoch 18126/20000 Training Loss: 0.05388025939464569\n",
      "Epoch 18127/20000 Training Loss: 0.06380408257246017\n",
      "Epoch 18128/20000 Training Loss: 0.05067680403590202\n",
      "Epoch 18129/20000 Training Loss: 0.05187353864312172\n",
      "Epoch 18130/20000 Training Loss: 0.046581853181123734\n",
      "Epoch 18130/20000 Validation Loss: 0.05235569551587105\n",
      "Epoch 18131/20000 Training Loss: 0.0413832813501358\n",
      "Epoch 18132/20000 Training Loss: 0.055804941803216934\n",
      "Epoch 18133/20000 Training Loss: 0.04676809534430504\n",
      "Epoch 18134/20000 Training Loss: 0.04321816936135292\n",
      "Epoch 18135/20000 Training Loss: 0.056000519543886185\n",
      "Epoch 18136/20000 Training Loss: 0.05452733114361763\n",
      "Epoch 18137/20000 Training Loss: 0.06684618443250656\n",
      "Epoch 18138/20000 Training Loss: 0.046747613698244095\n",
      "Epoch 18139/20000 Training Loss: 0.04183207452297211\n",
      "Epoch 18140/20000 Training Loss: 0.06173878535628319\n",
      "Epoch 18140/20000 Validation Loss: 0.052559271454811096\n",
      "Epoch 18141/20000 Training Loss: 0.05215403065085411\n",
      "Epoch 18142/20000 Training Loss: 0.05645656958222389\n",
      "Epoch 18143/20000 Training Loss: 0.05088887736201286\n",
      "Epoch 18144/20000 Training Loss: 0.048438120633363724\n",
      "Epoch 18145/20000 Training Loss: 0.03947613760828972\n",
      "Epoch 18146/20000 Training Loss: 0.04766608402132988\n",
      "Epoch 18147/20000 Training Loss: 0.06108609214425087\n",
      "Epoch 18148/20000 Training Loss: 0.041907187551259995\n",
      "Epoch 18149/20000 Training Loss: 0.06363534182310104\n",
      "Epoch 18150/20000 Training Loss: 0.06562846899032593\n",
      "Epoch 18150/20000 Validation Loss: 0.05696697533130646\n",
      "Epoch 18151/20000 Training Loss: 0.05915207043290138\n",
      "Epoch 18152/20000 Training Loss: 0.05694477632641792\n",
      "Epoch 18153/20000 Training Loss: 0.05739709362387657\n",
      "Epoch 18154/20000 Training Loss: 0.054332684725522995\n",
      "Epoch 18155/20000 Training Loss: 0.06663557142019272\n",
      "Epoch 18156/20000 Training Loss: 0.05145779252052307\n",
      "Epoch 18157/20000 Training Loss: 0.05279168486595154\n",
      "Epoch 18158/20000 Training Loss: 0.045411333441734314\n",
      "Epoch 18159/20000 Training Loss: 0.04227614775300026\n",
      "Epoch 18160/20000 Training Loss: 0.05505317449569702\n",
      "Epoch 18160/20000 Validation Loss: 0.05083721876144409\n",
      "Epoch 18161/20000 Training Loss: 0.06039504334330559\n",
      "Epoch 18162/20000 Training Loss: 0.05401136353611946\n",
      "Epoch 18163/20000 Training Loss: 0.05861331894993782\n",
      "Epoch 18164/20000 Training Loss: 0.06045461818575859\n",
      "Epoch 18165/20000 Training Loss: 0.05178223177790642\n",
      "Epoch 18166/20000 Training Loss: 0.06198452040553093\n",
      "Epoch 18167/20000 Training Loss: 0.06918081641197205\n",
      "Epoch 18168/20000 Training Loss: 0.05947745218873024\n",
      "Epoch 18169/20000 Training Loss: 0.049824777990579605\n",
      "Epoch 18170/20000 Training Loss: 0.06747512519359589\n",
      "Epoch 18170/20000 Validation Loss: 0.05679812654852867\n",
      "Epoch 18171/20000 Training Loss: 0.051657188683748245\n",
      "Epoch 18172/20000 Training Loss: 0.043956439942121506\n",
      "Epoch 18173/20000 Training Loss: 0.04848612844944\n",
      "Epoch 18174/20000 Training Loss: 0.0453876294195652\n",
      "Epoch 18175/20000 Training Loss: 0.04230384901165962\n",
      "Epoch 18176/20000 Training Loss: 0.04993118718266487\n",
      "Epoch 18177/20000 Training Loss: 0.053295910358428955\n",
      "Epoch 18178/20000 Training Loss: 0.04894861578941345\n",
      "Epoch 18179/20000 Training Loss: 0.06307957321405411\n",
      "Epoch 18180/20000 Training Loss: 0.056231867522001266\n",
      "Epoch 18180/20000 Validation Loss: 0.05817131698131561\n",
      "Epoch 18181/20000 Training Loss: 0.04341572895646095\n",
      "Epoch 18182/20000 Training Loss: 0.0639612153172493\n",
      "Epoch 18183/20000 Training Loss: 0.05350412428379059\n",
      "Epoch 18184/20000 Training Loss: 0.051972389221191406\n",
      "Epoch 18185/20000 Training Loss: 0.06651570647954941\n",
      "Epoch 18186/20000 Training Loss: 0.058137934654951096\n",
      "Epoch 18187/20000 Training Loss: 0.04118639603257179\n",
      "Epoch 18188/20000 Training Loss: 0.06912436336278915\n",
      "Epoch 18189/20000 Training Loss: 0.05978545546531677\n",
      "Epoch 18190/20000 Training Loss: 0.04982635751366615\n",
      "Epoch 18190/20000 Validation Loss: 0.05648283287882805\n",
      "Epoch 18191/20000 Training Loss: 0.04984882101416588\n",
      "Epoch 18192/20000 Training Loss: 0.04765819013118744\n",
      "Epoch 18193/20000 Training Loss: 0.060848671942949295\n",
      "Epoch 18194/20000 Training Loss: 0.048561882227659225\n",
      "Epoch 18195/20000 Training Loss: 0.05620619282126427\n",
      "Epoch 18196/20000 Training Loss: 0.037470608949661255\n",
      "Epoch 18197/20000 Training Loss: 0.0646028146147728\n",
      "Epoch 18198/20000 Training Loss: 0.049007315188646317\n",
      "Epoch 18199/20000 Training Loss: 0.04326794669032097\n",
      "Epoch 18200/20000 Training Loss: 0.05216403678059578\n",
      "Epoch 18200/20000 Validation Loss: 0.06374305486679077\n",
      "Epoch 18201/20000 Training Loss: 0.04795471206307411\n",
      "Epoch 18202/20000 Training Loss: 0.06969661265611649\n",
      "Epoch 18203/20000 Training Loss: 0.05739634856581688\n",
      "Epoch 18204/20000 Training Loss: 0.07375665754079819\n",
      "Epoch 18205/20000 Training Loss: 0.05157950520515442\n",
      "Epoch 18206/20000 Training Loss: 0.058862242847681046\n",
      "Epoch 18207/20000 Training Loss: 0.04792102053761482\n",
      "Epoch 18208/20000 Training Loss: 0.06334524601697922\n",
      "Epoch 18209/20000 Training Loss: 0.041390687227249146\n",
      "Epoch 18210/20000 Training Loss: 0.04733414575457573\n",
      "Epoch 18210/20000 Validation Loss: 0.07616838812828064\n",
      "Epoch 18211/20000 Training Loss: 0.06156399846076965\n",
      "Epoch 18212/20000 Training Loss: 0.04888259246945381\n",
      "Epoch 18213/20000 Training Loss: 0.052590399980545044\n",
      "Epoch 18214/20000 Training Loss: 0.05923384055495262\n",
      "Epoch 18215/20000 Training Loss: 0.05200077220797539\n",
      "Epoch 18216/20000 Training Loss: 0.06965752691030502\n",
      "Epoch 18217/20000 Training Loss: 0.050732310861349106\n",
      "Epoch 18218/20000 Training Loss: 0.04794606566429138\n",
      "Epoch 18219/20000 Training Loss: 0.06842362880706787\n",
      "Epoch 18220/20000 Training Loss: 0.04753392934799194\n",
      "Epoch 18220/20000 Validation Loss: 0.07439108937978745\n",
      "Epoch 18221/20000 Training Loss: 0.05407304689288139\n",
      "Epoch 18222/20000 Training Loss: 0.052676837891340256\n",
      "Epoch 18223/20000 Training Loss: 0.06435167044401169\n",
      "Epoch 18224/20000 Training Loss: 0.04780006781220436\n",
      "Epoch 18225/20000 Training Loss: 0.06732857972383499\n",
      "Epoch 18226/20000 Training Loss: 0.07706920057535172\n",
      "Epoch 18227/20000 Training Loss: 0.056039128452539444\n",
      "Epoch 18228/20000 Training Loss: 0.05533059313893318\n",
      "Epoch 18229/20000 Training Loss: 0.053806889802217484\n",
      "Epoch 18230/20000 Training Loss: 0.05935446918010712\n",
      "Epoch 18230/20000 Validation Loss: 0.0660271868109703\n",
      "Epoch 18231/20000 Training Loss: 0.06005318835377693\n",
      "Epoch 18232/20000 Training Loss: 0.06510723382234573\n",
      "Epoch 18233/20000 Training Loss: 0.056903231889009476\n",
      "Epoch 18234/20000 Training Loss: 0.06084100902080536\n",
      "Epoch 18235/20000 Training Loss: 0.06367788463830948\n",
      "Epoch 18236/20000 Training Loss: 0.047481194138526917\n",
      "Epoch 18237/20000 Training Loss: 0.042674824595451355\n",
      "Epoch 18238/20000 Training Loss: 0.04993833974003792\n",
      "Epoch 18239/20000 Training Loss: 0.06763651221990585\n",
      "Epoch 18240/20000 Training Loss: 0.059457793831825256\n",
      "Epoch 18240/20000 Validation Loss: 0.06530730426311493\n",
      "Epoch 18241/20000 Training Loss: 0.049673039466142654\n",
      "Epoch 18242/20000 Training Loss: 0.064246267080307\n",
      "Epoch 18243/20000 Training Loss: 0.06001842021942139\n",
      "Epoch 18244/20000 Training Loss: 0.04998873546719551\n",
      "Epoch 18245/20000 Training Loss: 0.049234166741371155\n",
      "Epoch 18246/20000 Training Loss: 0.05702763795852661\n",
      "Epoch 18247/20000 Training Loss: 0.047787752002477646\n",
      "Epoch 18248/20000 Training Loss: 0.056421469897031784\n",
      "Epoch 18249/20000 Training Loss: 0.043918635696172714\n",
      "Epoch 18250/20000 Training Loss: 0.06661277264356613\n",
      "Epoch 18250/20000 Validation Loss: 0.05505557358264923\n",
      "Epoch 18251/20000 Training Loss: 0.04480139538645744\n",
      "Epoch 18252/20000 Training Loss: 0.05271657183766365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18253/20000 Training Loss: 0.045192401856184006\n",
      "Epoch 18254/20000 Training Loss: 0.04845348000526428\n",
      "Epoch 18255/20000 Training Loss: 0.05248541012406349\n",
      "Epoch 18256/20000 Training Loss: 0.051813770085573196\n",
      "Epoch 18257/20000 Training Loss: 0.05360668525099754\n",
      "Epoch 18258/20000 Training Loss: 0.056675199419260025\n",
      "Epoch 18259/20000 Training Loss: 0.06574124097824097\n",
      "Epoch 18260/20000 Training Loss: 0.05320360139012337\n",
      "Epoch 18260/20000 Validation Loss: 0.06622761487960815\n",
      "Epoch 18261/20000 Training Loss: 0.04114880412817001\n",
      "Epoch 18262/20000 Training Loss: 0.056530844420194626\n",
      "Epoch 18263/20000 Training Loss: 0.04073318839073181\n",
      "Epoch 18264/20000 Training Loss: 0.05370256304740906\n",
      "Epoch 18265/20000 Training Loss: 0.045190174132585526\n",
      "Epoch 18266/20000 Training Loss: 0.0527510941028595\n",
      "Epoch 18267/20000 Training Loss: 0.04705245792865753\n",
      "Epoch 18268/20000 Training Loss: 0.04778463765978813\n",
      "Epoch 18269/20000 Training Loss: 0.05721414461731911\n",
      "Epoch 18270/20000 Training Loss: 0.05476858839392662\n",
      "Epoch 18270/20000 Validation Loss: 0.04332366958260536\n",
      "Epoch 18271/20000 Training Loss: 0.06357590109109879\n",
      "Epoch 18272/20000 Training Loss: 0.056829050183296204\n",
      "Epoch 18273/20000 Training Loss: 0.056928593665361404\n",
      "Epoch 18274/20000 Training Loss: 0.06614047288894653\n",
      "Epoch 18275/20000 Training Loss: 0.0627141073346138\n",
      "Epoch 18276/20000 Training Loss: 0.048864495009183884\n",
      "Epoch 18277/20000 Training Loss: 0.049796316772699356\n",
      "Epoch 18278/20000 Training Loss: 0.057300735265016556\n",
      "Epoch 18279/20000 Training Loss: 0.06306135654449463\n",
      "Epoch 18280/20000 Training Loss: 0.05226222053170204\n",
      "Epoch 18280/20000 Validation Loss: 0.04214721918106079\n",
      "Epoch 18281/20000 Training Loss: 0.060661543160676956\n",
      "Epoch 18282/20000 Training Loss: 0.05372123420238495\n",
      "Epoch 18283/20000 Training Loss: 0.042191412299871445\n",
      "Epoch 18284/20000 Training Loss: 0.051308561116456985\n",
      "Epoch 18285/20000 Training Loss: 0.05313640460371971\n",
      "Epoch 18286/20000 Training Loss: 0.060772042721509933\n",
      "Epoch 18287/20000 Training Loss: 0.08285966515541077\n",
      "Epoch 18288/20000 Training Loss: 0.04821974039077759\n",
      "Epoch 18289/20000 Training Loss: 0.037652891129255295\n",
      "Epoch 18290/20000 Training Loss: 0.0413716621696949\n",
      "Epoch 18290/20000 Validation Loss: 0.04137289524078369\n",
      "Epoch 18291/20000 Training Loss: 0.062082383781671524\n",
      "Epoch 18292/20000 Training Loss: 0.04930360987782478\n",
      "Epoch 18293/20000 Training Loss: 0.05888107419013977\n",
      "Epoch 18294/20000 Training Loss: 0.03830229863524437\n",
      "Epoch 18295/20000 Training Loss: 0.07191898673772812\n",
      "Epoch 18296/20000 Training Loss: 0.05213899910449982\n",
      "Epoch 18297/20000 Training Loss: 0.05256141349673271\n",
      "Epoch 18298/20000 Training Loss: 0.0480264276266098\n",
      "Epoch 18299/20000 Training Loss: 0.0599810965359211\n",
      "Epoch 18300/20000 Training Loss: 0.05465547367930412\n",
      "Epoch 18300/20000 Validation Loss: 0.0538439080119133\n",
      "Epoch 18301/20000 Training Loss: 0.04821614548563957\n",
      "Epoch 18302/20000 Training Loss: 0.05847249925136566\n",
      "Epoch 18303/20000 Training Loss: 0.057128071784973145\n",
      "Epoch 18304/20000 Training Loss: 0.05600656941533089\n",
      "Epoch 18305/20000 Training Loss: 0.05961224436759949\n",
      "Epoch 18306/20000 Training Loss: 0.05487997457385063\n",
      "Epoch 18307/20000 Training Loss: 0.06112822890281677\n",
      "Epoch 18308/20000 Training Loss: 0.06810038536787033\n",
      "Epoch 18309/20000 Training Loss: 0.05125179514288902\n",
      "Epoch 18310/20000 Training Loss: 0.062367986887693405\n",
      "Epoch 18310/20000 Validation Loss: 0.050859082490205765\n",
      "Epoch 18311/20000 Training Loss: 0.05601843073964119\n",
      "Epoch 18312/20000 Training Loss: 0.04454360529780388\n",
      "Epoch 18313/20000 Training Loss: 0.05135755613446236\n",
      "Epoch 18314/20000 Training Loss: 0.07786038517951965\n",
      "Epoch 18315/20000 Training Loss: 0.04704102873802185\n",
      "Epoch 18316/20000 Training Loss: 0.06162555515766144\n",
      "Epoch 18317/20000 Training Loss: 0.047436993569135666\n",
      "Epoch 18318/20000 Training Loss: 0.04588024318218231\n",
      "Epoch 18319/20000 Training Loss: 0.06286703050136566\n",
      "Epoch 18320/20000 Training Loss: 0.05469765141606331\n",
      "Epoch 18320/20000 Validation Loss: 0.052179478108882904\n",
      "Epoch 18321/20000 Training Loss: 0.040570467710494995\n",
      "Epoch 18322/20000 Training Loss: 0.07083135843276978\n",
      "Epoch 18323/20000 Training Loss: 0.05070855841040611\n",
      "Epoch 18324/20000 Training Loss: 0.06012646481394768\n",
      "Epoch 18325/20000 Training Loss: 0.04981895163655281\n",
      "Epoch 18326/20000 Training Loss: 0.04863578453660011\n",
      "Epoch 18327/20000 Training Loss: 0.04748419299721718\n",
      "Epoch 18328/20000 Training Loss: 0.05646084249019623\n",
      "Epoch 18329/20000 Training Loss: 0.04023626074194908\n",
      "Epoch 18330/20000 Training Loss: 0.044714659452438354\n",
      "Epoch 18330/20000 Validation Loss: 0.05757938697934151\n",
      "Epoch 18331/20000 Training Loss: 0.053726762533187866\n",
      "Epoch 18332/20000 Training Loss: 0.043338652700185776\n",
      "Epoch 18333/20000 Training Loss: 0.04411597177386284\n",
      "Epoch 18334/20000 Training Loss: 0.0461738258600235\n",
      "Epoch 18335/20000 Training Loss: 0.05222608149051666\n",
      "Epoch 18336/20000 Training Loss: 0.05700504407286644\n",
      "Epoch 18337/20000 Training Loss: 0.06391645222902298\n",
      "Epoch 18338/20000 Training Loss: 0.08085394650697708\n",
      "Epoch 18339/20000 Training Loss: 0.054827481508255005\n",
      "Epoch 18340/20000 Training Loss: 0.05681757256388664\n",
      "Epoch 18340/20000 Validation Loss: 0.06113014370203018\n",
      "Epoch 18341/20000 Training Loss: 0.04168948531150818\n",
      "Epoch 18342/20000 Training Loss: 0.04469833895564079\n",
      "Epoch 18343/20000 Training Loss: 0.07687666267156601\n",
      "Epoch 18344/20000 Training Loss: 0.0461769737303257\n",
      "Epoch 18345/20000 Training Loss: 0.05452677235007286\n",
      "Epoch 18346/20000 Training Loss: 0.04458418861031532\n",
      "Epoch 18347/20000 Training Loss: 0.054194461554288864\n",
      "Epoch 18348/20000 Training Loss: 0.05217355489730835\n",
      "Epoch 18349/20000 Training Loss: 0.06035657599568367\n",
      "Epoch 18350/20000 Training Loss: 0.046638503670692444\n",
      "Epoch 18350/20000 Validation Loss: 0.0692901536822319\n",
      "Epoch 18351/20000 Training Loss: 0.05899488553404808\n",
      "Epoch 18352/20000 Training Loss: 0.04778636619448662\n",
      "Epoch 18353/20000 Training Loss: 0.06106482446193695\n",
      "Epoch 18354/20000 Training Loss: 0.04747868701815605\n",
      "Epoch 18355/20000 Training Loss: 0.04195919260382652\n",
      "Epoch 18356/20000 Training Loss: 0.03815709054470062\n",
      "Epoch 18357/20000 Training Loss: 0.05943206325173378\n",
      "Epoch 18358/20000 Training Loss: 0.055826906114816666\n",
      "Epoch 18359/20000 Training Loss: 0.054295506328344345\n",
      "Epoch 18360/20000 Training Loss: 0.05732085183262825\n",
      "Epoch 18360/20000 Validation Loss: 0.045272454619407654\n",
      "Epoch 18361/20000 Training Loss: 0.0714929923415184\n",
      "Epoch 18362/20000 Training Loss: 0.05076846480369568\n",
      "Epoch 18363/20000 Training Loss: 0.04830992594361305\n",
      "Epoch 18364/20000 Training Loss: 0.06154404953122139\n",
      "Epoch 18365/20000 Training Loss: 0.0560944564640522\n",
      "Epoch 18366/20000 Training Loss: 0.043569739907979965\n",
      "Epoch 18367/20000 Training Loss: 0.0683768093585968\n",
      "Epoch 18368/20000 Training Loss: 0.05049458518624306\n",
      "Epoch 18369/20000 Training Loss: 0.043361395597457886\n",
      "Epoch 18370/20000 Training Loss: 0.053557489067316055\n",
      "Epoch 18370/20000 Validation Loss: 0.06386454403400421\n",
      "Epoch 18371/20000 Training Loss: 0.07299110293388367\n",
      "Epoch 18372/20000 Training Loss: 0.0447637140750885\n",
      "Epoch 18373/20000 Training Loss: 0.04872504249215126\n",
      "Epoch 18374/20000 Training Loss: 0.044993892312049866\n",
      "Epoch 18375/20000 Training Loss: 0.06432009488344193\n",
      "Epoch 18376/20000 Training Loss: 0.04872450605034828\n",
      "Epoch 18377/20000 Training Loss: 0.05161922797560692\n",
      "Epoch 18378/20000 Training Loss: 0.04914515092968941\n",
      "Epoch 18379/20000 Training Loss: 0.07607846707105637\n",
      "Epoch 18380/20000 Training Loss: 0.06195889785885811\n",
      "Epoch 18380/20000 Validation Loss: 0.07113310694694519\n",
      "Epoch 18381/20000 Training Loss: 0.053201545029878616\n",
      "Epoch 18382/20000 Training Loss: 0.05535249412059784\n",
      "Epoch 18383/20000 Training Loss: 0.06246289238333702\n",
      "Epoch 18384/20000 Training Loss: 0.068096823990345\n",
      "Epoch 18385/20000 Training Loss: 0.04915458336472511\n",
      "Epoch 18386/20000 Training Loss: 0.05936123803257942\n",
      "Epoch 18387/20000 Training Loss: 0.059287648648023605\n",
      "Epoch 18388/20000 Training Loss: 0.06802720576524734\n",
      "Epoch 18389/20000 Training Loss: 0.04473331943154335\n",
      "Epoch 18390/20000 Training Loss: 0.05544106289744377\n",
      "Epoch 18390/20000 Validation Loss: 0.04658486694097519\n",
      "Epoch 18391/20000 Training Loss: 0.04405605420470238\n",
      "Epoch 18392/20000 Training Loss: 0.055240705609321594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18393/20000 Training Loss: 0.05478368699550629\n",
      "Epoch 18394/20000 Training Loss: 0.044176921248435974\n",
      "Epoch 18395/20000 Training Loss: 0.05863014981150627\n",
      "Epoch 18396/20000 Training Loss: 0.05654841288924217\n",
      "Epoch 18397/20000 Training Loss: 0.06039753556251526\n",
      "Epoch 18398/20000 Training Loss: 0.07408281415700912\n",
      "Epoch 18399/20000 Training Loss: 0.05203045904636383\n",
      "Epoch 18400/20000 Training Loss: 0.06997349113225937\n",
      "Epoch 18400/20000 Validation Loss: 0.05217888206243515\n",
      "Epoch 18401/20000 Training Loss: 0.046228598803281784\n",
      "Epoch 18402/20000 Training Loss: 0.06346700340509415\n",
      "Epoch 18403/20000 Training Loss: 0.04700992628931999\n",
      "Epoch 18404/20000 Training Loss: 0.06357366591691971\n",
      "Epoch 18405/20000 Training Loss: 0.04600271210074425\n",
      "Epoch 18406/20000 Training Loss: 0.07154816389083862\n",
      "Epoch 18407/20000 Training Loss: 0.059528619050979614\n",
      "Epoch 18408/20000 Training Loss: 0.06001962721347809\n",
      "Epoch 18409/20000 Training Loss: 0.06559687107801437\n",
      "Epoch 18410/20000 Training Loss: 0.06398852914571762\n",
      "Epoch 18410/20000 Validation Loss: 0.05806560069322586\n",
      "Epoch 18411/20000 Training Loss: 0.04586637020111084\n",
      "Epoch 18412/20000 Training Loss: 0.05061401054263115\n",
      "Epoch 18413/20000 Training Loss: 0.052887141704559326\n",
      "Epoch 18414/20000 Training Loss: 0.050174277275800705\n",
      "Epoch 18415/20000 Training Loss: 0.04750340059399605\n",
      "Epoch 18416/20000 Training Loss: 0.04692591726779938\n",
      "Epoch 18417/20000 Training Loss: 0.05407049134373665\n",
      "Epoch 18418/20000 Training Loss: 0.07042550295591354\n",
      "Epoch 18419/20000 Training Loss: 0.04968436062335968\n",
      "Epoch 18420/20000 Training Loss: 0.07362265884876251\n",
      "Epoch 18420/20000 Validation Loss: 0.05275551974773407\n",
      "Epoch 18421/20000 Training Loss: 0.046383216977119446\n",
      "Epoch 18422/20000 Training Loss: 0.04321792721748352\n",
      "Epoch 18423/20000 Training Loss: 0.05151136592030525\n",
      "Epoch 18424/20000 Training Loss: 0.04238234832882881\n",
      "Epoch 18425/20000 Training Loss: 0.04590649530291557\n",
      "Epoch 18426/20000 Training Loss: 0.05541135370731354\n",
      "Epoch 18427/20000 Training Loss: 0.05033670738339424\n",
      "Epoch 18428/20000 Training Loss: 0.04485788568854332\n",
      "Epoch 18429/20000 Training Loss: 0.06099245324730873\n",
      "Epoch 18430/20000 Training Loss: 0.044688206166028976\n",
      "Epoch 18430/20000 Validation Loss: 0.05256209895014763\n",
      "Epoch 18431/20000 Training Loss: 0.04982282593846321\n",
      "Epoch 18432/20000 Training Loss: 0.04942747950553894\n",
      "Epoch 18433/20000 Training Loss: 0.0368860699236393\n",
      "Epoch 18434/20000 Training Loss: 0.051217127591371536\n",
      "Epoch 18435/20000 Training Loss: 0.047384340316057205\n",
      "Epoch 18436/20000 Training Loss: 0.038827162235975266\n",
      "Epoch 18437/20000 Training Loss: 0.05149209126830101\n",
      "Epoch 18438/20000 Training Loss: 0.052449941635131836\n",
      "Epoch 18439/20000 Training Loss: 0.0594918429851532\n",
      "Epoch 18440/20000 Training Loss: 0.04964950308203697\n",
      "Epoch 18440/20000 Validation Loss: 0.050728023052215576\n",
      "Epoch 18441/20000 Training Loss: 0.062285225838422775\n",
      "Epoch 18442/20000 Training Loss: 0.06818541884422302\n",
      "Epoch 18443/20000 Training Loss: 0.05990007892251015\n",
      "Epoch 18444/20000 Training Loss: 0.0705215260386467\n",
      "Epoch 18445/20000 Training Loss: 0.052883923053741455\n",
      "Epoch 18446/20000 Training Loss: 0.041903939098119736\n",
      "Epoch 18447/20000 Training Loss: 0.05794602259993553\n",
      "Epoch 18448/20000 Training Loss: 0.05460916459560394\n",
      "Epoch 18449/20000 Training Loss: 0.06923441588878632\n",
      "Epoch 18450/20000 Training Loss: 0.047834139317274094\n",
      "Epoch 18450/20000 Validation Loss: 0.06534606218338013\n",
      "Epoch 18451/20000 Training Loss: 0.04746916517615318\n",
      "Epoch 18452/20000 Training Loss: 0.06883710622787476\n",
      "Epoch 18453/20000 Training Loss: 0.04804695025086403\n",
      "Epoch 18454/20000 Training Loss: 0.045120835304260254\n",
      "Epoch 18455/20000 Training Loss: 0.050999339669942856\n",
      "Epoch 18456/20000 Training Loss: 0.06639403104782104\n",
      "Epoch 18457/20000 Training Loss: 0.061354368925094604\n",
      "Epoch 18458/20000 Training Loss: 0.06188109144568443\n",
      "Epoch 18459/20000 Training Loss: 0.050631508231163025\n",
      "Epoch 18460/20000 Training Loss: 0.07115516066551208\n",
      "Epoch 18460/20000 Validation Loss: 0.059178076684474945\n",
      "Epoch 18461/20000 Training Loss: 0.05557350441813469\n",
      "Epoch 18462/20000 Training Loss: 0.04303281009197235\n",
      "Epoch 18463/20000 Training Loss: 0.04888473078608513\n",
      "Epoch 18464/20000 Training Loss: 0.03674980625510216\n",
      "Epoch 18465/20000 Training Loss: 0.04224659129977226\n",
      "Epoch 18466/20000 Training Loss: 0.052341263741254807\n",
      "Epoch 18467/20000 Training Loss: 0.03572100028395653\n",
      "Epoch 18468/20000 Training Loss: 0.04768969491124153\n",
      "Epoch 18469/20000 Training Loss: 0.060385581105947495\n",
      "Epoch 18470/20000 Training Loss: 0.04919964075088501\n",
      "Epoch 18470/20000 Validation Loss: 0.052630506455898285\n",
      "Epoch 18471/20000 Training Loss: 0.04934495687484741\n",
      "Epoch 18472/20000 Training Loss: 0.06135985255241394\n",
      "Epoch 18473/20000 Training Loss: 0.04703114554286003\n",
      "Epoch 18474/20000 Training Loss: 0.04472121223807335\n",
      "Epoch 18475/20000 Training Loss: 0.05138293281197548\n",
      "Epoch 18476/20000 Training Loss: 0.048340052366256714\n",
      "Epoch 18477/20000 Training Loss: 0.05236348882317543\n",
      "Epoch 18478/20000 Training Loss: 0.04206693544983864\n",
      "Epoch 18479/20000 Training Loss: 0.05019146576523781\n",
      "Epoch 18480/20000 Training Loss: 0.0641857385635376\n",
      "Epoch 18480/20000 Validation Loss: 0.050543271005153656\n",
      "Epoch 18481/20000 Training Loss: 0.06518090516328812\n",
      "Epoch 18482/20000 Training Loss: 0.07630494236946106\n",
      "Epoch 18483/20000 Training Loss: 0.05490237846970558\n",
      "Epoch 18484/20000 Training Loss: 0.05219333991408348\n",
      "Epoch 18485/20000 Training Loss: 0.04987499117851257\n",
      "Epoch 18486/20000 Training Loss: 0.048201803117990494\n",
      "Epoch 18487/20000 Training Loss: 0.0421307273209095\n",
      "Epoch 18488/20000 Training Loss: 0.03703341260552406\n",
      "Epoch 18489/20000 Training Loss: 0.04382900893688202\n",
      "Epoch 18490/20000 Training Loss: 0.04790429398417473\n",
      "Epoch 18490/20000 Validation Loss: 0.06045110523700714\n",
      "Epoch 18491/20000 Training Loss: 0.04836711660027504\n",
      "Epoch 18492/20000 Training Loss: 0.053743988275527954\n",
      "Epoch 18493/20000 Training Loss: 0.054570525884628296\n",
      "Epoch 18494/20000 Training Loss: 0.059100907295942307\n",
      "Epoch 18495/20000 Training Loss: 0.06099626421928406\n",
      "Epoch 18496/20000 Training Loss: 0.0540151484310627\n",
      "Epoch 18497/20000 Training Loss: 0.037934426218271255\n",
      "Epoch 18498/20000 Training Loss: 0.03761664032936096\n",
      "Epoch 18499/20000 Training Loss: 0.05809294059872627\n",
      "Epoch 18500/20000 Training Loss: 0.04952356219291687\n",
      "Epoch 18500/20000 Validation Loss: 0.07929341495037079\n",
      "Epoch 18501/20000 Training Loss: 0.0439266711473465\n",
      "Epoch 18502/20000 Training Loss: 0.04412876442074776\n",
      "Epoch 18503/20000 Training Loss: 0.03957199677824974\n",
      "Epoch 18504/20000 Training Loss: 0.06429164856672287\n",
      "Epoch 18505/20000 Training Loss: 0.04312831163406372\n",
      "Epoch 18506/20000 Training Loss: 0.05813242122530937\n",
      "Epoch 18507/20000 Training Loss: 0.05904257297515869\n",
      "Epoch 18508/20000 Training Loss: 0.04131196066737175\n",
      "Epoch 18509/20000 Training Loss: 0.05368040129542351\n",
      "Epoch 18510/20000 Training Loss: 0.06191852316260338\n",
      "Epoch 18510/20000 Validation Loss: 0.057082220911979675\n",
      "Epoch 18511/20000 Training Loss: 0.052906643599271774\n",
      "Epoch 18512/20000 Training Loss: 0.0478290356695652\n",
      "Epoch 18513/20000 Training Loss: 0.04903428629040718\n",
      "Epoch 18514/20000 Training Loss: 0.04460940882563591\n",
      "Epoch 18515/20000 Training Loss: 0.0542156882584095\n",
      "Epoch 18516/20000 Training Loss: 0.0672953799366951\n",
      "Epoch 18517/20000 Training Loss: 0.05375910922884941\n",
      "Epoch 18518/20000 Training Loss: 0.048995241522789\n",
      "Epoch 18519/20000 Training Loss: 0.047212641686201096\n",
      "Epoch 18520/20000 Training Loss: 0.04951103404164314\n",
      "Epoch 18520/20000 Validation Loss: 0.09086212515830994\n",
      "Epoch 18521/20000 Training Loss: 0.05402718856930733\n",
      "Epoch 18522/20000 Training Loss: 0.05464928224682808\n",
      "Epoch 18523/20000 Training Loss: 0.049009084701538086\n",
      "Epoch 18524/20000 Training Loss: 0.04263816401362419\n",
      "Epoch 18525/20000 Training Loss: 0.05564960837364197\n",
      "Epoch 18526/20000 Training Loss: 0.06794930249452591\n",
      "Epoch 18527/20000 Training Loss: 0.0523659884929657\n",
      "Epoch 18528/20000 Training Loss: 0.04948066547513008\n",
      "Epoch 18529/20000 Training Loss: 0.04389461502432823\n",
      "Epoch 18530/20000 Training Loss: 0.05756428837776184\n",
      "Epoch 18530/20000 Validation Loss: 0.06702107191085815\n",
      "Epoch 18531/20000 Training Loss: 0.058116670697927475\n",
      "Epoch 18532/20000 Training Loss: 0.05482928454875946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18533/20000 Training Loss: 0.0517609529197216\n",
      "Epoch 18534/20000 Training Loss: 0.047814782708883286\n",
      "Epoch 18535/20000 Training Loss: 0.05595678091049194\n",
      "Epoch 18536/20000 Training Loss: 0.057766497135162354\n",
      "Epoch 18537/20000 Training Loss: 0.061878710985183716\n",
      "Epoch 18538/20000 Training Loss: 0.05064262077212334\n",
      "Epoch 18539/20000 Training Loss: 0.06275343149900436\n",
      "Epoch 18540/20000 Training Loss: 0.06306225061416626\n",
      "Epoch 18540/20000 Validation Loss: 0.09149989485740662\n",
      "Epoch 18541/20000 Training Loss: 0.0426117479801178\n",
      "Epoch 18542/20000 Training Loss: 0.0437035970389843\n",
      "Epoch 18543/20000 Training Loss: 0.05250607803463936\n",
      "Epoch 18544/20000 Training Loss: 0.05377909541130066\n",
      "Epoch 18545/20000 Training Loss: 0.06596392393112183\n",
      "Epoch 18546/20000 Training Loss: 0.05083765089511871\n",
      "Epoch 18547/20000 Training Loss: 0.0457429476082325\n",
      "Epoch 18548/20000 Training Loss: 0.06049266457557678\n",
      "Epoch 18549/20000 Training Loss: 0.06202244386076927\n",
      "Epoch 18550/20000 Training Loss: 0.04679948091506958\n",
      "Epoch 18550/20000 Validation Loss: 0.05160775035619736\n",
      "Epoch 18551/20000 Training Loss: 0.057943712919950485\n",
      "Epoch 18552/20000 Training Loss: 0.05665465071797371\n",
      "Epoch 18553/20000 Training Loss: 0.058427903801202774\n",
      "Epoch 18554/20000 Training Loss: 0.04006956145167351\n",
      "Epoch 18555/20000 Training Loss: 0.05466097220778465\n",
      "Epoch 18556/20000 Training Loss: 0.039826344698667526\n",
      "Epoch 18557/20000 Training Loss: 0.06819044798612595\n",
      "Epoch 18558/20000 Training Loss: 0.05281757935881615\n",
      "Epoch 18559/20000 Training Loss: 0.04747726395726204\n",
      "Epoch 18560/20000 Training Loss: 0.04668528959155083\n",
      "Epoch 18560/20000 Validation Loss: 0.07553144544363022\n",
      "Epoch 18561/20000 Training Loss: 0.048561323434114456\n",
      "Epoch 18562/20000 Training Loss: 0.04784606769680977\n",
      "Epoch 18563/20000 Training Loss: 0.04335619881749153\n",
      "Epoch 18564/20000 Training Loss: 0.054314035922288895\n",
      "Epoch 18565/20000 Training Loss: 0.07086563855409622\n",
      "Epoch 18566/20000 Training Loss: 0.04876023158431053\n",
      "Epoch 18567/20000 Training Loss: 0.043845295906066895\n",
      "Epoch 18568/20000 Training Loss: 0.03661022707819939\n",
      "Epoch 18569/20000 Training Loss: 0.06596571952104568\n",
      "Epoch 18570/20000 Training Loss: 0.05632880702614784\n",
      "Epoch 18570/20000 Validation Loss: 0.06724129617214203\n",
      "Epoch 18571/20000 Training Loss: 0.07353099435567856\n",
      "Epoch 18572/20000 Training Loss: 0.045503031462430954\n",
      "Epoch 18573/20000 Training Loss: 0.05521367862820625\n",
      "Epoch 18574/20000 Training Loss: 0.07499628514051437\n",
      "Epoch 18575/20000 Training Loss: 0.04953284189105034\n",
      "Epoch 18576/20000 Training Loss: 0.04703868553042412\n",
      "Epoch 18577/20000 Training Loss: 0.06993740797042847\n",
      "Epoch 18578/20000 Training Loss: 0.0618838407099247\n",
      "Epoch 18579/20000 Training Loss: 0.06326787918806076\n",
      "Epoch 18580/20000 Training Loss: 0.04753566160798073\n",
      "Epoch 18580/20000 Validation Loss: 0.05675036460161209\n",
      "Epoch 18581/20000 Training Loss: 0.05401066318154335\n",
      "Epoch 18582/20000 Training Loss: 0.05073003098368645\n",
      "Epoch 18583/20000 Training Loss: 0.06348850578069687\n",
      "Epoch 18584/20000 Training Loss: 0.06759929656982422\n",
      "Epoch 18585/20000 Training Loss: 0.03172667697072029\n",
      "Epoch 18586/20000 Training Loss: 0.06008252501487732\n",
      "Epoch 18587/20000 Training Loss: 0.0774136558175087\n",
      "Epoch 18588/20000 Training Loss: 0.07427734136581421\n",
      "Epoch 18589/20000 Training Loss: 0.05479332432150841\n",
      "Epoch 18590/20000 Training Loss: 0.05761398375034332\n",
      "Epoch 18590/20000 Validation Loss: 0.06549803912639618\n",
      "Epoch 18591/20000 Training Loss: 0.03882860019803047\n",
      "Epoch 18592/20000 Training Loss: 0.04567709565162659\n",
      "Epoch 18593/20000 Training Loss: 0.046496033668518066\n",
      "Epoch 18594/20000 Training Loss: 0.05083395913243294\n",
      "Epoch 18595/20000 Training Loss: 0.07226894795894623\n",
      "Epoch 18596/20000 Training Loss: 0.06419558078050613\n",
      "Epoch 18597/20000 Training Loss: 0.04583794251084328\n",
      "Epoch 18598/20000 Training Loss: 0.05243201181292534\n",
      "Epoch 18599/20000 Training Loss: 0.045880209654569626\n",
      "Epoch 18600/20000 Training Loss: 0.06364300101995468\n",
      "Epoch 18600/20000 Validation Loss: 0.046904340386390686\n",
      "Epoch 18601/20000 Training Loss: 0.047419507056474686\n",
      "Epoch 18602/20000 Training Loss: 0.06395107507705688\n",
      "Epoch 18603/20000 Training Loss: 0.052103400230407715\n",
      "Epoch 18604/20000 Training Loss: 0.07264000177383423\n",
      "Epoch 18605/20000 Training Loss: 0.03937153145670891\n",
      "Epoch 18606/20000 Training Loss: 0.0508907325565815\n",
      "Epoch 18607/20000 Training Loss: 0.06377451866865158\n",
      "Epoch 18608/20000 Training Loss: 0.052910104393959045\n",
      "Epoch 18609/20000 Training Loss: 0.069914810359478\n",
      "Epoch 18610/20000 Training Loss: 0.06843913346529007\n",
      "Epoch 18610/20000 Validation Loss: 0.05617278814315796\n",
      "Epoch 18611/20000 Training Loss: 0.06006056070327759\n",
      "Epoch 18612/20000 Training Loss: 0.05254465714097023\n",
      "Epoch 18613/20000 Training Loss: 0.040924493223428726\n",
      "Epoch 18614/20000 Training Loss: 0.04558681324124336\n",
      "Epoch 18615/20000 Training Loss: 0.0440698079764843\n",
      "Epoch 18616/20000 Training Loss: 0.051286280155181885\n",
      "Epoch 18617/20000 Training Loss: 0.05661699175834656\n",
      "Epoch 18618/20000 Training Loss: 0.04505380988121033\n",
      "Epoch 18619/20000 Training Loss: 0.06966978311538696\n",
      "Epoch 18620/20000 Training Loss: 0.05059032514691353\n",
      "Epoch 18620/20000 Validation Loss: 0.05334456264972687\n",
      "Epoch 18621/20000 Training Loss: 0.05073435604572296\n",
      "Epoch 18622/20000 Training Loss: 0.05328001454472542\n",
      "Epoch 18623/20000 Training Loss: 0.07861602306365967\n",
      "Epoch 18624/20000 Training Loss: 0.042479176074266434\n",
      "Epoch 18625/20000 Training Loss: 0.04726601764559746\n",
      "Epoch 18626/20000 Training Loss: 0.054915428161621094\n",
      "Epoch 18627/20000 Training Loss: 0.054302435368299484\n",
      "Epoch 18628/20000 Training Loss: 0.04816371574997902\n",
      "Epoch 18629/20000 Training Loss: 0.0661400631070137\n",
      "Epoch 18630/20000 Training Loss: 0.05791941657662392\n",
      "Epoch 18630/20000 Validation Loss: 0.054305534809827805\n",
      "Epoch 18631/20000 Training Loss: 0.040103983134031296\n",
      "Epoch 18632/20000 Training Loss: 0.05597013607621193\n",
      "Epoch 18633/20000 Training Loss: 0.05438963696360588\n",
      "Epoch 18634/20000 Training Loss: 0.04802162945270538\n",
      "Epoch 18635/20000 Training Loss: 0.058234069496393204\n",
      "Epoch 18636/20000 Training Loss: 0.04738098755478859\n",
      "Epoch 18637/20000 Training Loss: 0.05733175203204155\n",
      "Epoch 18638/20000 Training Loss: 0.054644446820020676\n",
      "Epoch 18639/20000 Training Loss: 0.0491633415222168\n",
      "Epoch 18640/20000 Training Loss: 0.049947697669267654\n",
      "Epoch 18640/20000 Validation Loss: 0.05778459459543228\n",
      "Epoch 18641/20000 Training Loss: 0.05275747552514076\n",
      "Epoch 18642/20000 Training Loss: 0.05730995163321495\n",
      "Epoch 18643/20000 Training Loss: 0.05191851779818535\n",
      "Epoch 18644/20000 Training Loss: 0.06349404156208038\n",
      "Epoch 18645/20000 Training Loss: 0.03870775178074837\n",
      "Epoch 18646/20000 Training Loss: 0.04052804782986641\n",
      "Epoch 18647/20000 Training Loss: 0.05211573839187622\n",
      "Epoch 18648/20000 Training Loss: 0.04141164943575859\n",
      "Epoch 18649/20000 Training Loss: 0.041387349367141724\n",
      "Epoch 18650/20000 Training Loss: 0.060567423701286316\n",
      "Epoch 18650/20000 Validation Loss: 0.0543515682220459\n",
      "Epoch 18651/20000 Training Loss: 0.04508569464087486\n",
      "Epoch 18652/20000 Training Loss: 0.05701039358973503\n",
      "Epoch 18653/20000 Training Loss: 0.07038948684930801\n",
      "Epoch 18654/20000 Training Loss: 0.05321505293250084\n",
      "Epoch 18655/20000 Training Loss: 0.06659422069787979\n",
      "Epoch 18656/20000 Training Loss: 0.06986650824546814\n",
      "Epoch 18657/20000 Training Loss: 0.057899314910173416\n",
      "Epoch 18658/20000 Training Loss: 0.06884793937206268\n",
      "Epoch 18659/20000 Training Loss: 0.03947989642620087\n",
      "Epoch 18660/20000 Training Loss: 0.055948931723833084\n",
      "Epoch 18660/20000 Validation Loss: 0.05659488961100578\n",
      "Epoch 18661/20000 Training Loss: 0.060759831219911575\n",
      "Epoch 18662/20000 Training Loss: 0.05266338586807251\n",
      "Epoch 18663/20000 Training Loss: 0.04871770739555359\n",
      "Epoch 18664/20000 Training Loss: 0.05501442030072212\n",
      "Epoch 18665/20000 Training Loss: 0.06474945694208145\n",
      "Epoch 18666/20000 Training Loss: 0.04372505471110344\n",
      "Epoch 18667/20000 Training Loss: 0.04148423671722412\n",
      "Epoch 18668/20000 Training Loss: 0.047398362308740616\n",
      "Epoch 18669/20000 Training Loss: 0.059921134263277054\n",
      "Epoch 18670/20000 Training Loss: 0.060475390404462814\n",
      "Epoch 18670/20000 Validation Loss: 0.05300581082701683\n",
      "Epoch 18671/20000 Training Loss: 0.05982482433319092\n",
      "Epoch 18672/20000 Training Loss: 0.05124390125274658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18673/20000 Training Loss: 0.04843682050704956\n",
      "Epoch 18674/20000 Training Loss: 0.06511380523443222\n",
      "Epoch 18675/20000 Training Loss: 0.048382360488176346\n",
      "Epoch 18676/20000 Training Loss: 0.07409416884183884\n",
      "Epoch 18677/20000 Training Loss: 0.07471340149641037\n",
      "Epoch 18678/20000 Training Loss: 0.0666530430316925\n",
      "Epoch 18679/20000 Training Loss: 0.06616126745939255\n",
      "Epoch 18680/20000 Training Loss: 0.05237278342247009\n",
      "Epoch 18680/20000 Validation Loss: 0.03297143056988716\n",
      "Epoch 18681/20000 Training Loss: 0.06186645105481148\n",
      "Epoch 18682/20000 Training Loss: 0.04472031816840172\n",
      "Epoch 18683/20000 Training Loss: 0.0702204704284668\n",
      "Epoch 18684/20000 Training Loss: 0.0520927794277668\n",
      "Epoch 18685/20000 Training Loss: 0.0577310286462307\n",
      "Epoch 18686/20000 Training Loss: 0.06546381115913391\n",
      "Epoch 18687/20000 Training Loss: 0.06235622987151146\n",
      "Epoch 18688/20000 Training Loss: 0.05386774241924286\n",
      "Epoch 18689/20000 Training Loss: 0.06926850974559784\n",
      "Epoch 18690/20000 Training Loss: 0.04355138540267944\n",
      "Epoch 18690/20000 Validation Loss: 0.06487639993429184\n",
      "Epoch 18691/20000 Training Loss: 0.06686490029096603\n",
      "Epoch 18692/20000 Training Loss: 0.06230756640434265\n",
      "Epoch 18693/20000 Training Loss: 0.04702368751168251\n",
      "Epoch 18694/20000 Training Loss: 0.05090254917740822\n",
      "Epoch 18695/20000 Training Loss: 0.06484215706586838\n",
      "Epoch 18696/20000 Training Loss: 0.061736539006233215\n",
      "Epoch 18697/20000 Training Loss: 0.07297982275485992\n",
      "Epoch 18698/20000 Training Loss: 0.04883744940161705\n",
      "Epoch 18699/20000 Training Loss: 0.06086587905883789\n",
      "Epoch 18700/20000 Training Loss: 0.055285438895225525\n",
      "Epoch 18700/20000 Validation Loss: 0.06090159714221954\n",
      "Epoch 18701/20000 Training Loss: 0.05214190110564232\n",
      "Epoch 18702/20000 Training Loss: 0.05415724590420723\n",
      "Epoch 18703/20000 Training Loss: 0.045189231634140015\n",
      "Epoch 18704/20000 Training Loss: 0.0972626581788063\n",
      "Epoch 18705/20000 Training Loss: 0.03506898134946823\n",
      "Epoch 18706/20000 Training Loss: 0.05987397953867912\n",
      "Epoch 18707/20000 Training Loss: 0.04250948503613472\n",
      "Epoch 18708/20000 Training Loss: 0.05975094065070152\n",
      "Epoch 18709/20000 Training Loss: 0.0435304194688797\n",
      "Epoch 18710/20000 Training Loss: 0.05765710771083832\n",
      "Epoch 18710/20000 Validation Loss: 0.05788199231028557\n",
      "Epoch 18711/20000 Training Loss: 0.05511588230729103\n",
      "Epoch 18712/20000 Training Loss: 0.05279519781470299\n",
      "Epoch 18713/20000 Training Loss: 0.04402431845664978\n",
      "Epoch 18714/20000 Training Loss: 0.05809156224131584\n",
      "Epoch 18715/20000 Training Loss: 0.058746304363012314\n",
      "Epoch 18716/20000 Training Loss: 0.06710508465766907\n",
      "Epoch 18717/20000 Training Loss: 0.05597309395670891\n",
      "Epoch 18718/20000 Training Loss: 0.04708588495850563\n",
      "Epoch 18719/20000 Training Loss: 0.04504409432411194\n",
      "Epoch 18720/20000 Training Loss: 0.042316511273384094\n",
      "Epoch 18720/20000 Validation Loss: 0.08649545162916183\n",
      "Epoch 18721/20000 Training Loss: 0.04113864526152611\n",
      "Epoch 18722/20000 Training Loss: 0.048560768365859985\n",
      "Epoch 18723/20000 Training Loss: 0.058483537286520004\n",
      "Epoch 18724/20000 Training Loss: 0.04568064212799072\n",
      "Epoch 18725/20000 Training Loss: 0.07081811875104904\n",
      "Epoch 18726/20000 Training Loss: 0.058745261281728745\n",
      "Epoch 18727/20000 Training Loss: 0.04812656715512276\n",
      "Epoch 18728/20000 Training Loss: 0.05286969617009163\n",
      "Epoch 18729/20000 Training Loss: 0.05248764157295227\n",
      "Epoch 18730/20000 Training Loss: 0.061365436762571335\n",
      "Epoch 18730/20000 Validation Loss: 0.07476302981376648\n",
      "Epoch 18731/20000 Training Loss: 0.05633426085114479\n",
      "Epoch 18732/20000 Training Loss: 0.062207888811826706\n",
      "Epoch 18733/20000 Training Loss: 0.054517313838005066\n",
      "Epoch 18734/20000 Training Loss: 0.056482624262571335\n",
      "Epoch 18735/20000 Training Loss: 0.06728426367044449\n",
      "Epoch 18736/20000 Training Loss: 0.05961569771170616\n",
      "Epoch 18737/20000 Training Loss: 0.05306846275925636\n",
      "Epoch 18738/20000 Training Loss: 0.053387001156806946\n",
      "Epoch 18739/20000 Training Loss: 0.051645826548337936\n",
      "Epoch 18740/20000 Training Loss: 0.06026972457766533\n",
      "Epoch 18740/20000 Validation Loss: 0.05968606844544411\n",
      "Epoch 18741/20000 Training Loss: 0.040830422192811966\n",
      "Epoch 18742/20000 Training Loss: 0.052341610193252563\n",
      "Epoch 18743/20000 Training Loss: 0.05848252400755882\n",
      "Epoch 18744/20000 Training Loss: 0.047123003751039505\n",
      "Epoch 18745/20000 Training Loss: 0.06092696264386177\n",
      "Epoch 18746/20000 Training Loss: 0.049608051776885986\n",
      "Epoch 18747/20000 Training Loss: 0.0547964833676815\n",
      "Epoch 18748/20000 Training Loss: 0.053104519844055176\n",
      "Epoch 18749/20000 Training Loss: 0.06626658886671066\n",
      "Epoch 18750/20000 Training Loss: 0.04894397780299187\n",
      "Epoch 18750/20000 Validation Loss: 0.07363827526569366\n",
      "Epoch 18751/20000 Training Loss: 0.07653234899044037\n",
      "Epoch 18752/20000 Training Loss: 0.05353883281350136\n",
      "Epoch 18753/20000 Training Loss: 0.05252251401543617\n",
      "Epoch 18754/20000 Training Loss: 0.0533522330224514\n",
      "Epoch 18755/20000 Training Loss: 0.053164344280958176\n",
      "Epoch 18756/20000 Training Loss: 0.052876655012369156\n",
      "Epoch 18757/20000 Training Loss: 0.05422106385231018\n",
      "Epoch 18758/20000 Training Loss: 0.051847025752067566\n",
      "Epoch 18759/20000 Training Loss: 0.04917147755622864\n",
      "Epoch 18760/20000 Training Loss: 0.0489933043718338\n",
      "Epoch 18760/20000 Validation Loss: 0.05984770134091377\n",
      "Epoch 18761/20000 Training Loss: 0.05481414869427681\n",
      "Epoch 18762/20000 Training Loss: 0.04121169075369835\n",
      "Epoch 18763/20000 Training Loss: 0.06893166899681091\n",
      "Epoch 18764/20000 Training Loss: 0.043313413858413696\n",
      "Epoch 18765/20000 Training Loss: 0.04292893782258034\n",
      "Epoch 18766/20000 Training Loss: 0.04881112277507782\n",
      "Epoch 18767/20000 Training Loss: 0.06681391596794128\n",
      "Epoch 18768/20000 Training Loss: 0.0561421699821949\n",
      "Epoch 18769/20000 Training Loss: 0.0719742700457573\n",
      "Epoch 18770/20000 Training Loss: 0.051020603626966476\n",
      "Epoch 18770/20000 Validation Loss: 0.048626407980918884\n",
      "Epoch 18771/20000 Training Loss: 0.043001145124435425\n",
      "Epoch 18772/20000 Training Loss: 0.07633227109909058\n",
      "Epoch 18773/20000 Training Loss: 0.06034253165125847\n",
      "Epoch 18774/20000 Training Loss: 0.061284586787223816\n",
      "Epoch 18775/20000 Training Loss: 0.05411308631300926\n",
      "Epoch 18776/20000 Training Loss: 0.05220630764961243\n",
      "Epoch 18777/20000 Training Loss: 0.05728290602564812\n",
      "Epoch 18778/20000 Training Loss: 0.04649137333035469\n",
      "Epoch 18779/20000 Training Loss: 0.05814065411686897\n",
      "Epoch 18780/20000 Training Loss: 0.05535445734858513\n",
      "Epoch 18780/20000 Validation Loss: 0.052027203142642975\n",
      "Epoch 18781/20000 Training Loss: 0.04698851704597473\n",
      "Epoch 18782/20000 Training Loss: 0.06074027344584465\n",
      "Epoch 18783/20000 Training Loss: 0.038886554539203644\n",
      "Epoch 18784/20000 Training Loss: 0.04596666619181633\n",
      "Epoch 18785/20000 Training Loss: 0.061870869249105453\n",
      "Epoch 18786/20000 Training Loss: 0.044374238699674606\n",
      "Epoch 18787/20000 Training Loss: 0.0533171184360981\n",
      "Epoch 18788/20000 Training Loss: 0.046028707176446915\n",
      "Epoch 18789/20000 Training Loss: 0.05516178905963898\n",
      "Epoch 18790/20000 Training Loss: 0.048588480800390244\n",
      "Epoch 18790/20000 Validation Loss: 0.06911016255617142\n",
      "Epoch 18791/20000 Training Loss: 0.048582810908555984\n",
      "Epoch 18792/20000 Training Loss: 0.06194404140114784\n",
      "Epoch 18793/20000 Training Loss: 0.05041642487049103\n",
      "Epoch 18794/20000 Training Loss: 0.04123732075095177\n",
      "Epoch 18795/20000 Training Loss: 0.06364846229553223\n",
      "Epoch 18796/20000 Training Loss: 0.04030907154083252\n",
      "Epoch 18797/20000 Training Loss: 0.05867299810051918\n",
      "Epoch 18798/20000 Training Loss: 0.04565498232841492\n",
      "Epoch 18799/20000 Training Loss: 0.07143334299325943\n",
      "Epoch 18800/20000 Training Loss: 0.04942184314131737\n",
      "Epoch 18800/20000 Validation Loss: 0.05869752913713455\n",
      "Epoch 18801/20000 Training Loss: 0.06335646659135818\n",
      "Epoch 18802/20000 Training Loss: 0.04603278264403343\n",
      "Epoch 18803/20000 Training Loss: 0.0392543263733387\n",
      "Epoch 18804/20000 Training Loss: 0.058050960302352905\n",
      "Epoch 18805/20000 Training Loss: 0.053785938769578934\n",
      "Epoch 18806/20000 Training Loss: 0.05373317375779152\n",
      "Epoch 18807/20000 Training Loss: 0.06552832573652267\n",
      "Epoch 18808/20000 Training Loss: 0.0495779924094677\n",
      "Epoch 18809/20000 Training Loss: 0.05847032740712166\n",
      "Epoch 18810/20000 Training Loss: 0.04570312425494194\n",
      "Epoch 18810/20000 Validation Loss: 0.06539222598075867\n",
      "Epoch 18811/20000 Training Loss: 0.050219908356666565\n",
      "Epoch 18812/20000 Training Loss: 0.05698797479271889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18813/20000 Training Loss: 0.06558255106210709\n",
      "Epoch 18814/20000 Training Loss: 0.08197411149740219\n",
      "Epoch 18815/20000 Training Loss: 0.054149970412254333\n",
      "Epoch 18816/20000 Training Loss: 0.04682771489024162\n",
      "Epoch 18817/20000 Training Loss: 0.05871181562542915\n",
      "Epoch 18818/20000 Training Loss: 0.04664387181401253\n",
      "Epoch 18819/20000 Training Loss: 0.048833638429641724\n",
      "Epoch 18820/20000 Training Loss: 0.04485559090971947\n",
      "Epoch 18820/20000 Validation Loss: 0.055277034640312195\n",
      "Epoch 18821/20000 Training Loss: 0.051516953855752945\n",
      "Epoch 18822/20000 Training Loss: 0.07680075615644455\n",
      "Epoch 18823/20000 Training Loss: 0.04739658534526825\n",
      "Epoch 18824/20000 Training Loss: 0.06207531690597534\n",
      "Epoch 18825/20000 Training Loss: 0.048270225524902344\n",
      "Epoch 18826/20000 Training Loss: 0.048932794481515884\n",
      "Epoch 18827/20000 Training Loss: 0.05730806291103363\n",
      "Epoch 18828/20000 Training Loss: 0.050024811178445816\n",
      "Epoch 18829/20000 Training Loss: 0.04160964488983154\n",
      "Epoch 18830/20000 Training Loss: 0.058654170483350754\n",
      "Epoch 18830/20000 Validation Loss: 0.06589453667402267\n",
      "Epoch 18831/20000 Training Loss: 0.05687990412116051\n",
      "Epoch 18832/20000 Training Loss: 0.053608238697052\n",
      "Epoch 18833/20000 Training Loss: 0.0404549278318882\n",
      "Epoch 18834/20000 Training Loss: 0.062306661158800125\n",
      "Epoch 18835/20000 Training Loss: 0.0594826303422451\n",
      "Epoch 18836/20000 Training Loss: 0.05936974659562111\n",
      "Epoch 18837/20000 Training Loss: 0.07273184508085251\n",
      "Epoch 18838/20000 Training Loss: 0.055254776030778885\n",
      "Epoch 18839/20000 Training Loss: 0.07336887717247009\n",
      "Epoch 18840/20000 Training Loss: 0.05619088187813759\n",
      "Epoch 18840/20000 Validation Loss: 0.05840511620044708\n",
      "Epoch 18841/20000 Training Loss: 0.05293337628245354\n",
      "Epoch 18842/20000 Training Loss: 0.06553594768047333\n",
      "Epoch 18843/20000 Training Loss: 0.06539026647806168\n",
      "Epoch 18844/20000 Training Loss: 0.06035088375210762\n",
      "Epoch 18845/20000 Training Loss: 0.0525498241186142\n",
      "Epoch 18846/20000 Training Loss: 0.05497888848185539\n",
      "Epoch 18847/20000 Training Loss: 0.048349786549806595\n",
      "Epoch 18848/20000 Training Loss: 0.0626402422785759\n",
      "Epoch 18849/20000 Training Loss: 0.05995883047580719\n",
      "Epoch 18850/20000 Training Loss: 0.048042889684438705\n",
      "Epoch 18850/20000 Validation Loss: 0.0525711253285408\n",
      "Epoch 18851/20000 Training Loss: 0.03962042182683945\n",
      "Epoch 18852/20000 Training Loss: 0.04154781997203827\n",
      "Epoch 18853/20000 Training Loss: 0.05032198131084442\n",
      "Epoch 18854/20000 Training Loss: 0.04338441416621208\n",
      "Epoch 18855/20000 Training Loss: 0.051762327551841736\n",
      "Epoch 18856/20000 Training Loss: 0.05841952562332153\n",
      "Epoch 18857/20000 Training Loss: 0.06351732462644577\n",
      "Epoch 18858/20000 Training Loss: 0.05526871979236603\n",
      "Epoch 18859/20000 Training Loss: 0.075179323554039\n",
      "Epoch 18860/20000 Training Loss: 0.054929494857788086\n",
      "Epoch 18860/20000 Validation Loss: 0.05391690507531166\n",
      "Epoch 18861/20000 Training Loss: 0.06552585959434509\n",
      "Epoch 18862/20000 Training Loss: 0.0594453327357769\n",
      "Epoch 18863/20000 Training Loss: 0.05002623423933983\n",
      "Epoch 18864/20000 Training Loss: 0.041896093636751175\n",
      "Epoch 18865/20000 Training Loss: 0.05318136885762215\n",
      "Epoch 18866/20000 Training Loss: 0.04120960831642151\n",
      "Epoch 18867/20000 Training Loss: 0.05083449184894562\n",
      "Epoch 18868/20000 Training Loss: 0.07233919948339462\n",
      "Epoch 18869/20000 Training Loss: 0.05284364894032478\n",
      "Epoch 18870/20000 Training Loss: 0.06663528829813004\n",
      "Epoch 18870/20000 Validation Loss: 0.057701341807842255\n",
      "Epoch 18871/20000 Training Loss: 0.039191342890262604\n",
      "Epoch 18872/20000 Training Loss: 0.05130802094936371\n",
      "Epoch 18873/20000 Training Loss: 0.06745938956737518\n",
      "Epoch 18874/20000 Training Loss: 0.04696641489863396\n",
      "Epoch 18875/20000 Training Loss: 0.06652162969112396\n",
      "Epoch 18876/20000 Training Loss: 0.0440242774784565\n",
      "Epoch 18877/20000 Training Loss: 0.054035719484090805\n",
      "Epoch 18878/20000 Training Loss: 0.046195369213819504\n",
      "Epoch 18879/20000 Training Loss: 0.059866875410079956\n",
      "Epoch 18880/20000 Training Loss: 0.0578487403690815\n",
      "Epoch 18880/20000 Validation Loss: 0.057786859571933746\n",
      "Epoch 18881/20000 Training Loss: 0.0684187188744545\n",
      "Epoch 18882/20000 Training Loss: 0.04866446554660797\n",
      "Epoch 18883/20000 Training Loss: 0.05111036077141762\n",
      "Epoch 18884/20000 Training Loss: 0.05374124273657799\n",
      "Epoch 18885/20000 Training Loss: 0.053585220128297806\n",
      "Epoch 18886/20000 Training Loss: 0.039352599531412125\n",
      "Epoch 18887/20000 Training Loss: 0.06378062814474106\n",
      "Epoch 18888/20000 Training Loss: 0.06002625450491905\n",
      "Epoch 18889/20000 Training Loss: 0.06260529905557632\n",
      "Epoch 18890/20000 Training Loss: 0.052339788526296616\n",
      "Epoch 18890/20000 Validation Loss: 0.04510495811700821\n",
      "Epoch 18891/20000 Training Loss: 0.07568509131669998\n",
      "Epoch 18892/20000 Training Loss: 0.05654493346810341\n",
      "Epoch 18893/20000 Training Loss: 0.07247429341077805\n",
      "Epoch 18894/20000 Training Loss: 0.04601317644119263\n",
      "Epoch 18895/20000 Training Loss: 0.05425475537776947\n",
      "Epoch 18896/20000 Training Loss: 0.04964256286621094\n",
      "Epoch 18897/20000 Training Loss: 0.05409887805581093\n",
      "Epoch 18898/20000 Training Loss: 0.04748588800430298\n",
      "Epoch 18899/20000 Training Loss: 0.04765288904309273\n",
      "Epoch 18900/20000 Training Loss: 0.05870242044329643\n",
      "Epoch 18900/20000 Validation Loss: 0.06359495222568512\n",
      "Epoch 18901/20000 Training Loss: 0.056086465716362\n",
      "Epoch 18902/20000 Training Loss: 0.050291866064071655\n",
      "Epoch 18903/20000 Training Loss: 0.046598102897405624\n",
      "Epoch 18904/20000 Training Loss: 0.05181948468089104\n",
      "Epoch 18905/20000 Training Loss: 0.04207979142665863\n",
      "Epoch 18906/20000 Training Loss: 0.04443658888339996\n",
      "Epoch 18907/20000 Training Loss: 0.044060081243515015\n",
      "Epoch 18908/20000 Training Loss: 0.05429830029606819\n",
      "Epoch 18909/20000 Training Loss: 0.044516634196043015\n",
      "Epoch 18910/20000 Training Loss: 0.06857920438051224\n",
      "Epoch 18910/20000 Validation Loss: 0.052169546484947205\n",
      "Epoch 18911/20000 Training Loss: 0.044825587421655655\n",
      "Epoch 18912/20000 Training Loss: 0.05392158403992653\n",
      "Epoch 18913/20000 Training Loss: 0.057539135217666626\n",
      "Epoch 18914/20000 Training Loss: 0.042484432458877563\n",
      "Epoch 18915/20000 Training Loss: 0.04128025099635124\n",
      "Epoch 18916/20000 Training Loss: 0.05162763595581055\n",
      "Epoch 18917/20000 Training Loss: 0.055421266704797745\n",
      "Epoch 18918/20000 Training Loss: 0.06656482070684433\n",
      "Epoch 18919/20000 Training Loss: 0.06402234733104706\n",
      "Epoch 18920/20000 Training Loss: 0.05734565854072571\n",
      "Epoch 18920/20000 Validation Loss: 0.058633435517549515\n",
      "Epoch 18921/20000 Training Loss: 0.05566265061497688\n",
      "Epoch 18922/20000 Training Loss: 0.03385559841990471\n",
      "Epoch 18923/20000 Training Loss: 0.052287567406892776\n",
      "Epoch 18924/20000 Training Loss: 0.045221805572509766\n",
      "Epoch 18925/20000 Training Loss: 0.04808221384882927\n",
      "Epoch 18926/20000 Training Loss: 0.0442679263651371\n",
      "Epoch 18927/20000 Training Loss: 0.039110876619815826\n",
      "Epoch 18928/20000 Training Loss: 0.05049257352948189\n",
      "Epoch 18929/20000 Training Loss: 0.04690786078572273\n",
      "Epoch 18930/20000 Training Loss: 0.05362014099955559\n",
      "Epoch 18930/20000 Validation Loss: 0.048745907843112946\n",
      "Epoch 18931/20000 Training Loss: 0.056369949132204056\n",
      "Epoch 18932/20000 Training Loss: 0.06346989423036575\n",
      "Epoch 18933/20000 Training Loss: 0.04154976084828377\n",
      "Epoch 18934/20000 Training Loss: 0.06627217680215836\n",
      "Epoch 18935/20000 Training Loss: 0.048845306038856506\n",
      "Epoch 18936/20000 Training Loss: 0.06500809639692307\n",
      "Epoch 18937/20000 Training Loss: 0.05744064971804619\n",
      "Epoch 18938/20000 Training Loss: 0.044172465801239014\n",
      "Epoch 18939/20000 Training Loss: 0.05355791375041008\n",
      "Epoch 18940/20000 Training Loss: 0.053899869322776794\n",
      "Epoch 18940/20000 Validation Loss: 0.07103818655014038\n",
      "Epoch 18941/20000 Training Loss: 0.056697238236665726\n",
      "Epoch 18942/20000 Training Loss: 0.03959796950221062\n",
      "Epoch 18943/20000 Training Loss: 0.06351237744092941\n",
      "Epoch 18944/20000 Training Loss: 0.04339759424328804\n",
      "Epoch 18945/20000 Training Loss: 0.04623867943882942\n",
      "Epoch 18946/20000 Training Loss: 0.06997004896402359\n",
      "Epoch 18947/20000 Training Loss: 0.056762248277664185\n",
      "Epoch 18948/20000 Training Loss: 0.04870733618736267\n",
      "Epoch 18949/20000 Training Loss: 0.05761964991688728\n",
      "Epoch 18950/20000 Training Loss: 0.04829069972038269\n",
      "Epoch 18950/20000 Validation Loss: 0.05949828028678894\n",
      "Epoch 18951/20000 Training Loss: 0.05164581537246704\n",
      "Epoch 18952/20000 Training Loss: 0.056122709065675735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18953/20000 Training Loss: 0.057702288031578064\n",
      "Epoch 18954/20000 Training Loss: 0.0451960414648056\n",
      "Epoch 18955/20000 Training Loss: 0.05388571694493294\n",
      "Epoch 18956/20000 Training Loss: 0.04258854314684868\n",
      "Epoch 18957/20000 Training Loss: 0.06263750046491623\n",
      "Epoch 18958/20000 Training Loss: 0.042065124958753586\n",
      "Epoch 18959/20000 Training Loss: 0.05251510813832283\n",
      "Epoch 18960/20000 Training Loss: 0.06951548904180527\n",
      "Epoch 18960/20000 Validation Loss: 0.051588792353868484\n",
      "Epoch 18961/20000 Training Loss: 0.04577961936593056\n",
      "Epoch 18962/20000 Training Loss: 0.04398210346698761\n",
      "Epoch 18963/20000 Training Loss: 0.0472097210586071\n",
      "Epoch 18964/20000 Training Loss: 0.05134405568242073\n",
      "Epoch 18965/20000 Training Loss: 0.06170206889510155\n",
      "Epoch 18966/20000 Training Loss: 0.05719119310379028\n",
      "Epoch 18967/20000 Training Loss: 0.07277228683233261\n",
      "Epoch 18968/20000 Training Loss: 0.04835069179534912\n",
      "Epoch 18969/20000 Training Loss: 0.058765191584825516\n",
      "Epoch 18970/20000 Training Loss: 0.058949071913957596\n",
      "Epoch 18970/20000 Validation Loss: 0.07530874758958817\n",
      "Epoch 18971/20000 Training Loss: 0.053159069269895554\n",
      "Epoch 18972/20000 Training Loss: 0.0561625175178051\n",
      "Epoch 18973/20000 Training Loss: 0.061495453119277954\n",
      "Epoch 18974/20000 Training Loss: 0.061511535197496414\n",
      "Epoch 18975/20000 Training Loss: 0.043351635336875916\n",
      "Epoch 18976/20000 Training Loss: 0.04731267690658569\n",
      "Epoch 18977/20000 Training Loss: 0.05834043025970459\n",
      "Epoch 18978/20000 Training Loss: 0.04457806050777435\n",
      "Epoch 18979/20000 Training Loss: 0.05400612950325012\n",
      "Epoch 18980/20000 Training Loss: 0.08042557537555695\n",
      "Epoch 18980/20000 Validation Loss: 0.0685473382472992\n",
      "Epoch 18981/20000 Training Loss: 0.04911724105477333\n",
      "Epoch 18982/20000 Training Loss: 0.06855728477239609\n",
      "Epoch 18983/20000 Training Loss: 0.049324970692396164\n",
      "Epoch 18984/20000 Training Loss: 0.05173158645629883\n",
      "Epoch 18985/20000 Training Loss: 0.055729448795318604\n",
      "Epoch 18986/20000 Training Loss: 0.05281580612063408\n",
      "Epoch 18987/20000 Training Loss: 0.08885116130113602\n",
      "Epoch 18988/20000 Training Loss: 0.06772498041391373\n",
      "Epoch 18989/20000 Training Loss: 0.04770338535308838\n",
      "Epoch 18990/20000 Training Loss: 0.04880126938223839\n",
      "Epoch 18990/20000 Validation Loss: 0.03672860935330391\n",
      "Epoch 18991/20000 Training Loss: 0.04992543160915375\n",
      "Epoch 18992/20000 Training Loss: 0.0435166172683239\n",
      "Epoch 18993/20000 Training Loss: 0.06854182481765747\n",
      "Epoch 18994/20000 Training Loss: 0.04413529112935066\n",
      "Epoch 18995/20000 Training Loss: 0.052068036049604416\n",
      "Epoch 18996/20000 Training Loss: 0.03788517788052559\n",
      "Epoch 18997/20000 Training Loss: 0.045142438262701035\n",
      "Epoch 18998/20000 Training Loss: 0.06009054556488991\n",
      "Epoch 18999/20000 Training Loss: 0.05349037051200867\n",
      "Epoch 19000/20000 Training Loss: 0.05612534284591675\n",
      "Epoch 19000/20000 Validation Loss: 0.07347841560840607\n",
      "Epoch 19001/20000 Training Loss: 0.04073052108287811\n",
      "Epoch 19002/20000 Training Loss: 0.056259170174598694\n",
      "Epoch 19003/20000 Training Loss: 0.04886405169963837\n",
      "Epoch 19004/20000 Training Loss: 0.05808402970433235\n",
      "Epoch 19005/20000 Training Loss: 0.05157453939318657\n",
      "Epoch 19006/20000 Training Loss: 0.053577348589897156\n",
      "Epoch 19007/20000 Training Loss: 0.08311482518911362\n",
      "Epoch 19008/20000 Training Loss: 0.06044856086373329\n",
      "Epoch 19009/20000 Training Loss: 0.040045883506536484\n",
      "Epoch 19010/20000 Training Loss: 0.059116486459970474\n",
      "Epoch 19010/20000 Validation Loss: 0.045237425714731216\n",
      "Epoch 19011/20000 Training Loss: 0.05331873893737793\n",
      "Epoch 19012/20000 Training Loss: 0.041684821248054504\n",
      "Epoch 19013/20000 Training Loss: 0.04167655110359192\n",
      "Epoch 19014/20000 Training Loss: 0.0395711325109005\n",
      "Epoch 19015/20000 Training Loss: 0.06273134797811508\n",
      "Epoch 19016/20000 Training Loss: 0.058114100247621536\n",
      "Epoch 19017/20000 Training Loss: 0.050483014434576035\n",
      "Epoch 19018/20000 Training Loss: 0.04996110126376152\n",
      "Epoch 19019/20000 Training Loss: 0.06302489340305328\n",
      "Epoch 19020/20000 Training Loss: 0.061064016073942184\n",
      "Epoch 19020/20000 Validation Loss: 0.05305343493819237\n",
      "Epoch 19021/20000 Training Loss: 0.05333366617560387\n",
      "Epoch 19022/20000 Training Loss: 0.049733009189367294\n",
      "Epoch 19023/20000 Training Loss: 0.07828115671873093\n",
      "Epoch 19024/20000 Training Loss: 0.07112281769514084\n",
      "Epoch 19025/20000 Training Loss: 0.05269191786646843\n",
      "Epoch 19026/20000 Training Loss: 0.06537514179944992\n",
      "Epoch 19027/20000 Training Loss: 0.04646630585193634\n",
      "Epoch 19028/20000 Training Loss: 0.03814369812607765\n",
      "Epoch 19029/20000 Training Loss: 0.052698809653520584\n",
      "Epoch 19030/20000 Training Loss: 0.044978197664022446\n",
      "Epoch 19030/20000 Validation Loss: 0.04947448521852493\n",
      "Epoch 19031/20000 Training Loss: 0.07138829678297043\n",
      "Epoch 19032/20000 Training Loss: 0.05348717048764229\n",
      "Epoch 19033/20000 Training Loss: 0.055311817675828934\n",
      "Epoch 19034/20000 Training Loss: 0.0494721420109272\n",
      "Epoch 19035/20000 Training Loss: 0.041875820606946945\n",
      "Epoch 19036/20000 Training Loss: 0.04774830862879753\n",
      "Epoch 19037/20000 Training Loss: 0.06040680781006813\n",
      "Epoch 19038/20000 Training Loss: 0.05614205077290535\n",
      "Epoch 19039/20000 Training Loss: 0.05773027241230011\n",
      "Epoch 19040/20000 Training Loss: 0.05640149489045143\n",
      "Epoch 19040/20000 Validation Loss: 0.06055578589439392\n",
      "Epoch 19041/20000 Training Loss: 0.049750592559576035\n",
      "Epoch 19042/20000 Training Loss: 0.051174938678741455\n",
      "Epoch 19043/20000 Training Loss: 0.049612462520599365\n",
      "Epoch 19044/20000 Training Loss: 0.046209871768951416\n",
      "Epoch 19045/20000 Training Loss: 0.04925745725631714\n",
      "Epoch 19046/20000 Training Loss: 0.055611174553632736\n",
      "Epoch 19047/20000 Training Loss: 0.03502273932099342\n",
      "Epoch 19048/20000 Training Loss: 0.043714433908462524\n",
      "Epoch 19049/20000 Training Loss: 0.04738380014896393\n",
      "Epoch 19050/20000 Training Loss: 0.04256972298026085\n",
      "Epoch 19050/20000 Validation Loss: 0.06644449383020401\n",
      "Epoch 19051/20000 Training Loss: 0.04394780099391937\n",
      "Epoch 19052/20000 Training Loss: 0.053823038935661316\n",
      "Epoch 19053/20000 Training Loss: 0.06678720563650131\n",
      "Epoch 19054/20000 Training Loss: 0.05656816437840462\n",
      "Epoch 19055/20000 Training Loss: 0.050352681428194046\n",
      "Epoch 19056/20000 Training Loss: 0.06899259239435196\n",
      "Epoch 19057/20000 Training Loss: 0.04385372996330261\n",
      "Epoch 19058/20000 Training Loss: 0.05531569942831993\n",
      "Epoch 19059/20000 Training Loss: 0.05823197960853577\n",
      "Epoch 19060/20000 Training Loss: 0.04225282743573189\n",
      "Epoch 19060/20000 Validation Loss: 0.06272199749946594\n",
      "Epoch 19061/20000 Training Loss: 0.058998364955186844\n",
      "Epoch 19062/20000 Training Loss: 0.06599064916372299\n",
      "Epoch 19063/20000 Training Loss: 0.05872902646660805\n",
      "Epoch 19064/20000 Training Loss: 0.05127343535423279\n",
      "Epoch 19065/20000 Training Loss: 0.051779091358184814\n",
      "Epoch 19066/20000 Training Loss: 0.05766350403428078\n",
      "Epoch 19067/20000 Training Loss: 0.051948342472314835\n",
      "Epoch 19068/20000 Training Loss: 0.0547199547290802\n",
      "Epoch 19069/20000 Training Loss: 0.045569028705358505\n",
      "Epoch 19070/20000 Training Loss: 0.041182562708854675\n",
      "Epoch 19070/20000 Validation Loss: 0.05753698945045471\n",
      "Epoch 19071/20000 Training Loss: 0.04731791839003563\n",
      "Epoch 19072/20000 Training Loss: 0.0471821129322052\n",
      "Epoch 19073/20000 Training Loss: 0.056315645575523376\n",
      "Epoch 19074/20000 Training Loss: 0.051100343465805054\n",
      "Epoch 19075/20000 Training Loss: 0.06796663254499435\n",
      "Epoch 19076/20000 Training Loss: 0.05313391610980034\n",
      "Epoch 19077/20000 Training Loss: 0.07476309686899185\n",
      "Epoch 19078/20000 Training Loss: 0.050329964607954025\n",
      "Epoch 19079/20000 Training Loss: 0.04852241277694702\n",
      "Epoch 19080/20000 Training Loss: 0.04339027404785156\n",
      "Epoch 19080/20000 Validation Loss: 0.0551648810505867\n",
      "Epoch 19081/20000 Training Loss: 0.06808438897132874\n",
      "Epoch 19082/20000 Training Loss: 0.051180604845285416\n",
      "Epoch 19083/20000 Training Loss: 0.04106977581977844\n",
      "Epoch 19084/20000 Training Loss: 0.05609489604830742\n",
      "Epoch 19085/20000 Training Loss: 0.04379719868302345\n",
      "Epoch 19086/20000 Training Loss: 0.05401748791337013\n",
      "Epoch 19087/20000 Training Loss: 0.04448336735367775\n",
      "Epoch 19088/20000 Training Loss: 0.05083128809928894\n",
      "Epoch 19089/20000 Training Loss: 0.04668332636356354\n",
      "Epoch 19090/20000 Training Loss: 0.037014689296483994\n",
      "Epoch 19090/20000 Validation Loss: 0.06312904506921768\n",
      "Epoch 19091/20000 Training Loss: 0.05479094758629799\n",
      "Epoch 19092/20000 Training Loss: 0.06356262415647507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19093/20000 Training Loss: 0.05012606084346771\n",
      "Epoch 19094/20000 Training Loss: 0.06831682473421097\n",
      "Epoch 19095/20000 Training Loss: 0.04653940722346306\n",
      "Epoch 19096/20000 Training Loss: 0.05324712023139\n",
      "Epoch 19097/20000 Training Loss: 0.05839742347598076\n",
      "Epoch 19098/20000 Training Loss: 0.037551093846559525\n",
      "Epoch 19099/20000 Training Loss: 0.048553455621004105\n",
      "Epoch 19100/20000 Training Loss: 0.056295040994882584\n",
      "Epoch 19100/20000 Validation Loss: 0.07177148759365082\n",
      "Epoch 19101/20000 Training Loss: 0.06096832826733589\n",
      "Epoch 19102/20000 Training Loss: 0.04511640593409538\n",
      "Epoch 19103/20000 Training Loss: 0.048811476677656174\n",
      "Epoch 19104/20000 Training Loss: 0.04813459888100624\n",
      "Epoch 19105/20000 Training Loss: 0.06616968661546707\n",
      "Epoch 19106/20000 Training Loss: 0.04235149547457695\n",
      "Epoch 19107/20000 Training Loss: 0.04458771273493767\n",
      "Epoch 19108/20000 Training Loss: 0.061976343393325806\n",
      "Epoch 19109/20000 Training Loss: 0.04864504933357239\n",
      "Epoch 19110/20000 Training Loss: 0.04197244346141815\n",
      "Epoch 19110/20000 Validation Loss: 0.07166188955307007\n",
      "Epoch 19111/20000 Training Loss: 0.0515170581638813\n",
      "Epoch 19112/20000 Training Loss: 0.0453328974545002\n",
      "Epoch 19113/20000 Training Loss: 0.03820725157856941\n",
      "Epoch 19114/20000 Training Loss: 0.05068976804614067\n",
      "Epoch 19115/20000 Training Loss: 0.04829459264874458\n",
      "Epoch 19116/20000 Training Loss: 0.04337364807724953\n",
      "Epoch 19117/20000 Training Loss: 0.05738087370991707\n",
      "Epoch 19118/20000 Training Loss: 0.051702797412872314\n",
      "Epoch 19119/20000 Training Loss: 0.05009947344660759\n",
      "Epoch 19120/20000 Training Loss: 0.05394415184855461\n",
      "Epoch 19120/20000 Validation Loss: 0.05302853137254715\n",
      "Epoch 19121/20000 Training Loss: 0.0423743911087513\n",
      "Epoch 19122/20000 Training Loss: 0.050769779831171036\n",
      "Epoch 19123/20000 Training Loss: 0.053857188671827316\n",
      "Epoch 19124/20000 Training Loss: 0.04529040679335594\n",
      "Epoch 19125/20000 Training Loss: 0.05826802924275398\n",
      "Epoch 19126/20000 Training Loss: 0.046147558838129044\n",
      "Epoch 19127/20000 Training Loss: 0.06017351150512695\n",
      "Epoch 19128/20000 Training Loss: 0.05139303207397461\n",
      "Epoch 19129/20000 Training Loss: 0.04362495243549347\n",
      "Epoch 19130/20000 Training Loss: 0.04848508909344673\n",
      "Epoch 19130/20000 Validation Loss: 0.08886238932609558\n",
      "Epoch 19131/20000 Training Loss: 0.061747848987579346\n",
      "Epoch 19132/20000 Training Loss: 0.06887191534042358\n",
      "Epoch 19133/20000 Training Loss: 0.04629107937216759\n",
      "Epoch 19134/20000 Training Loss: 0.05613868311047554\n",
      "Epoch 19135/20000 Training Loss: 0.06589043140411377\n",
      "Epoch 19136/20000 Training Loss: 0.04000851511955261\n",
      "Epoch 19137/20000 Training Loss: 0.0568370558321476\n",
      "Epoch 19138/20000 Training Loss: 0.04805958271026611\n",
      "Epoch 19139/20000 Training Loss: 0.06284492462873459\n",
      "Epoch 19140/20000 Training Loss: 0.05231578275561333\n",
      "Epoch 19140/20000 Validation Loss: 0.06371360272169113\n",
      "Epoch 19141/20000 Training Loss: 0.0380081981420517\n",
      "Epoch 19142/20000 Training Loss: 0.06443163007497787\n",
      "Epoch 19143/20000 Training Loss: 0.05228489637374878\n",
      "Epoch 19144/20000 Training Loss: 0.06444959342479706\n",
      "Epoch 19145/20000 Training Loss: 0.05411427840590477\n",
      "Epoch 19146/20000 Training Loss: 0.054023995995521545\n",
      "Epoch 19147/20000 Training Loss: 0.04710794985294342\n",
      "Epoch 19148/20000 Training Loss: 0.049135640263557434\n",
      "Epoch 19149/20000 Training Loss: 0.04742327332496643\n",
      "Epoch 19150/20000 Training Loss: 0.040440384298563004\n",
      "Epoch 19150/20000 Validation Loss: 0.05804184824228287\n",
      "Epoch 19151/20000 Training Loss: 0.0724567398428917\n",
      "Epoch 19152/20000 Training Loss: 0.06632352620363235\n",
      "Epoch 19153/20000 Training Loss: 0.04971543326973915\n",
      "Epoch 19154/20000 Training Loss: 0.04856153205037117\n",
      "Epoch 19155/20000 Training Loss: 0.04538826271891594\n",
      "Epoch 19156/20000 Training Loss: 0.07665470242500305\n",
      "Epoch 19157/20000 Training Loss: 0.03657612204551697\n",
      "Epoch 19158/20000 Training Loss: 0.049892593175172806\n",
      "Epoch 19159/20000 Training Loss: 0.07816629856824875\n",
      "Epoch 19160/20000 Training Loss: 0.06569555401802063\n",
      "Epoch 19160/20000 Validation Loss: 0.048977166414260864\n",
      "Epoch 19161/20000 Training Loss: 0.08403763175010681\n",
      "Epoch 19162/20000 Training Loss: 0.05788552761077881\n",
      "Epoch 19163/20000 Training Loss: 0.04646701738238335\n",
      "Epoch 19164/20000 Training Loss: 0.05667979642748833\n",
      "Epoch 19165/20000 Training Loss: 0.06782042980194092\n",
      "Epoch 19166/20000 Training Loss: 0.06323494762182236\n",
      "Epoch 19167/20000 Training Loss: 0.03871815279126167\n",
      "Epoch 19168/20000 Training Loss: 0.058954209089279175\n",
      "Epoch 19169/20000 Training Loss: 0.059705764055252075\n",
      "Epoch 19170/20000 Training Loss: 0.05740973353385925\n",
      "Epoch 19170/20000 Validation Loss: 0.08456914126873016\n",
      "Epoch 19171/20000 Training Loss: 0.054398030042648315\n",
      "Epoch 19172/20000 Training Loss: 0.05493985489010811\n",
      "Epoch 19173/20000 Training Loss: 0.06381557136774063\n",
      "Epoch 19174/20000 Training Loss: 0.042191341519355774\n",
      "Epoch 19175/20000 Training Loss: 0.05078240856528282\n",
      "Epoch 19176/20000 Training Loss: 0.05151146650314331\n",
      "Epoch 19177/20000 Training Loss: 0.06270512193441391\n",
      "Epoch 19178/20000 Training Loss: 0.04444712772965431\n",
      "Epoch 19179/20000 Training Loss: 0.05825285241007805\n",
      "Epoch 19180/20000 Training Loss: 0.05713745579123497\n",
      "Epoch 19180/20000 Validation Loss: 0.049269258975982666\n",
      "Epoch 19181/20000 Training Loss: 0.05919300392270088\n",
      "Epoch 19182/20000 Training Loss: 0.055421650409698486\n",
      "Epoch 19183/20000 Training Loss: 0.04193846881389618\n",
      "Epoch 19184/20000 Training Loss: 0.04552517831325531\n",
      "Epoch 19185/20000 Training Loss: 0.041581522673368454\n",
      "Epoch 19186/20000 Training Loss: 0.05006075277924538\n",
      "Epoch 19187/20000 Training Loss: 0.04665790870785713\n",
      "Epoch 19188/20000 Training Loss: 0.04683999344706535\n",
      "Epoch 19189/20000 Training Loss: 0.07251311093568802\n",
      "Epoch 19190/20000 Training Loss: 0.04533000662922859\n",
      "Epoch 19190/20000 Validation Loss: 0.061045899987220764\n",
      "Epoch 19191/20000 Training Loss: 0.059002164751291275\n",
      "Epoch 19192/20000 Training Loss: 0.05340820550918579\n",
      "Epoch 19193/20000 Training Loss: 0.05739038065075874\n",
      "Epoch 19194/20000 Training Loss: 0.0552256815135479\n",
      "Epoch 19195/20000 Training Loss: 0.04886741563677788\n",
      "Epoch 19196/20000 Training Loss: 0.057532910257577896\n",
      "Epoch 19197/20000 Training Loss: 0.0449383519589901\n",
      "Epoch 19198/20000 Training Loss: 0.04002148285508156\n",
      "Epoch 19199/20000 Training Loss: 0.04871629551053047\n",
      "Epoch 19200/20000 Training Loss: 0.0538802333176136\n",
      "Epoch 19200/20000 Validation Loss: 0.05412350222468376\n",
      "Epoch 19201/20000 Training Loss: 0.05238017067313194\n",
      "Epoch 19202/20000 Training Loss: 0.05785650387406349\n",
      "Epoch 19203/20000 Training Loss: 0.07095668464899063\n",
      "Epoch 19204/20000 Training Loss: 0.06499659270048141\n",
      "Epoch 19205/20000 Training Loss: 0.03517713397741318\n",
      "Epoch 19206/20000 Training Loss: 0.045520227402448654\n",
      "Epoch 19207/20000 Training Loss: 0.04795081913471222\n",
      "Epoch 19208/20000 Training Loss: 0.04912635684013367\n",
      "Epoch 19209/20000 Training Loss: 0.051152121275663376\n",
      "Epoch 19210/20000 Training Loss: 0.049332037568092346\n",
      "Epoch 19210/20000 Validation Loss: 0.040468163788318634\n",
      "Epoch 19211/20000 Training Loss: 0.050142887979745865\n",
      "Epoch 19212/20000 Training Loss: 0.04994937777519226\n",
      "Epoch 19213/20000 Training Loss: 0.06005668640136719\n",
      "Epoch 19214/20000 Training Loss: 0.04679961875081062\n",
      "Epoch 19215/20000 Training Loss: 0.05643929913640022\n",
      "Epoch 19216/20000 Training Loss: 0.05669260025024414\n",
      "Epoch 19217/20000 Training Loss: 0.04772120341658592\n",
      "Epoch 19218/20000 Training Loss: 0.0606650747358799\n",
      "Epoch 19219/20000 Training Loss: 0.04762917757034302\n",
      "Epoch 19220/20000 Training Loss: 0.06455578655004501\n",
      "Epoch 19220/20000 Validation Loss: 0.06353934109210968\n",
      "Epoch 19221/20000 Training Loss: 0.05621090158820152\n",
      "Epoch 19222/20000 Training Loss: 0.04664405807852745\n",
      "Epoch 19223/20000 Training Loss: 0.05206957459449768\n",
      "Epoch 19224/20000 Training Loss: 0.04814091697335243\n",
      "Epoch 19225/20000 Training Loss: 0.05746970698237419\n",
      "Epoch 19226/20000 Training Loss: 0.049428630620241165\n",
      "Epoch 19227/20000 Training Loss: 0.04831783473491669\n",
      "Epoch 19228/20000 Training Loss: 0.0605102963745594\n",
      "Epoch 19229/20000 Training Loss: 0.05359170213341713\n",
      "Epoch 19230/20000 Training Loss: 0.050383199006319046\n",
      "Epoch 19230/20000 Validation Loss: 0.04492092877626419\n",
      "Epoch 19231/20000 Training Loss: 0.04741722345352173\n",
      "Epoch 19232/20000 Training Loss: 0.058711934834718704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19233/20000 Training Loss: 0.05585340037941933\n",
      "Epoch 19234/20000 Training Loss: 0.07181765884160995\n",
      "Epoch 19235/20000 Training Loss: 0.0559857152402401\n",
      "Epoch 19236/20000 Training Loss: 0.05462896451354027\n",
      "Epoch 19237/20000 Training Loss: 0.050863225013017654\n",
      "Epoch 19238/20000 Training Loss: 0.0667085349559784\n",
      "Epoch 19239/20000 Training Loss: 0.05193747952580452\n",
      "Epoch 19240/20000 Training Loss: 0.061481837183237076\n",
      "Epoch 19240/20000 Validation Loss: 0.06993986666202545\n",
      "Epoch 19241/20000 Training Loss: 0.03179681673645973\n",
      "Epoch 19242/20000 Training Loss: 0.05280255898833275\n",
      "Epoch 19243/20000 Training Loss: 0.04695956036448479\n",
      "Epoch 19244/20000 Training Loss: 0.0639290139079094\n",
      "Epoch 19245/20000 Training Loss: 0.04194168746471405\n",
      "Epoch 19246/20000 Training Loss: 0.04730724170804024\n",
      "Epoch 19247/20000 Training Loss: 0.05289558693766594\n",
      "Epoch 19248/20000 Training Loss: 0.04624982550740242\n",
      "Epoch 19249/20000 Training Loss: 0.04882049188017845\n",
      "Epoch 19250/20000 Training Loss: 0.06162336468696594\n",
      "Epoch 19250/20000 Validation Loss: 0.06873772293329239\n",
      "Epoch 19251/20000 Training Loss: 0.04916346073150635\n",
      "Epoch 19252/20000 Training Loss: 0.051461879163980484\n",
      "Epoch 19253/20000 Training Loss: 0.06778492778539658\n",
      "Epoch 19254/20000 Training Loss: 0.05228891968727112\n",
      "Epoch 19255/20000 Training Loss: 0.07891526073217392\n",
      "Epoch 19256/20000 Training Loss: 0.05862880125641823\n",
      "Epoch 19257/20000 Training Loss: 0.05851924046874046\n",
      "Epoch 19258/20000 Training Loss: 0.05551527068018913\n",
      "Epoch 19259/20000 Training Loss: 0.04390581324696541\n",
      "Epoch 19260/20000 Training Loss: 0.04816524311900139\n",
      "Epoch 19260/20000 Validation Loss: 0.06639702618122101\n",
      "Epoch 19261/20000 Training Loss: 0.04786589369177818\n",
      "Epoch 19262/20000 Training Loss: 0.04015607759356499\n",
      "Epoch 19263/20000 Training Loss: 0.051418762654066086\n",
      "Epoch 19264/20000 Training Loss: 0.0538509227335453\n",
      "Epoch 19265/20000 Training Loss: 0.049672652035951614\n",
      "Epoch 19266/20000 Training Loss: 0.06303469091653824\n",
      "Epoch 19267/20000 Training Loss: 0.04789351299405098\n",
      "Epoch 19268/20000 Training Loss: 0.05044092610478401\n",
      "Epoch 19269/20000 Training Loss: 0.06871851533651352\n",
      "Epoch 19270/20000 Training Loss: 0.050176575779914856\n",
      "Epoch 19270/20000 Validation Loss: 0.062431126832962036\n",
      "Epoch 19271/20000 Training Loss: 0.06965994089841843\n",
      "Epoch 19272/20000 Training Loss: 0.058941200375556946\n",
      "Epoch 19273/20000 Training Loss: 0.04567578807473183\n",
      "Epoch 19274/20000 Training Loss: 0.05442800745368004\n",
      "Epoch 19275/20000 Training Loss: 0.046994734555482864\n",
      "Epoch 19276/20000 Training Loss: 0.05842776969075203\n",
      "Epoch 19277/20000 Training Loss: 0.053692013025283813\n",
      "Epoch 19278/20000 Training Loss: 0.054195698350667953\n",
      "Epoch 19279/20000 Training Loss: 0.061819661408662796\n",
      "Epoch 19280/20000 Training Loss: 0.0687992125749588\n",
      "Epoch 19280/20000 Validation Loss: 0.04802525043487549\n",
      "Epoch 19281/20000 Training Loss: 0.06380428373813629\n",
      "Epoch 19282/20000 Training Loss: 0.06598915159702301\n",
      "Epoch 19283/20000 Training Loss: 0.05264781042933464\n",
      "Epoch 19284/20000 Training Loss: 0.04556260630488396\n",
      "Epoch 19285/20000 Training Loss: 0.052256349474191666\n",
      "Epoch 19286/20000 Training Loss: 0.06197618320584297\n",
      "Epoch 19287/20000 Training Loss: 0.045455772429704666\n",
      "Epoch 19288/20000 Training Loss: 0.058824535459280014\n",
      "Epoch 19289/20000 Training Loss: 0.04909015819430351\n",
      "Epoch 19290/20000 Training Loss: 0.05294748768210411\n",
      "Epoch 19290/20000 Validation Loss: 0.09030251950025558\n",
      "Epoch 19291/20000 Training Loss: 0.059440478682518005\n",
      "Epoch 19292/20000 Training Loss: 0.05260680615901947\n",
      "Epoch 19293/20000 Training Loss: 0.04491788148880005\n",
      "Epoch 19294/20000 Training Loss: 0.0634564459323883\n",
      "Epoch 19295/20000 Training Loss: 0.0724804624915123\n",
      "Epoch 19296/20000 Training Loss: 0.04222827032208443\n",
      "Epoch 19297/20000 Training Loss: 0.0532207190990448\n",
      "Epoch 19298/20000 Training Loss: 0.060281213372945786\n",
      "Epoch 19299/20000 Training Loss: 0.05855521187186241\n",
      "Epoch 19300/20000 Training Loss: 0.05370016396045685\n",
      "Epoch 19300/20000 Validation Loss: 0.04955492168664932\n",
      "Epoch 19301/20000 Training Loss: 0.056665558367967606\n",
      "Epoch 19302/20000 Training Loss: 0.05625545606017113\n",
      "Epoch 19303/20000 Training Loss: 0.0538371317088604\n",
      "Epoch 19304/20000 Training Loss: 0.05160994455218315\n",
      "Epoch 19305/20000 Training Loss: 0.0602358914911747\n",
      "Epoch 19306/20000 Training Loss: 0.060687437653541565\n",
      "Epoch 19307/20000 Training Loss: 0.05775769427418709\n",
      "Epoch 19308/20000 Training Loss: 0.04503770172595978\n",
      "Epoch 19309/20000 Training Loss: 0.05662212893366814\n",
      "Epoch 19310/20000 Training Loss: 0.061337780207395554\n",
      "Epoch 19310/20000 Validation Loss: 0.040077514946460724\n",
      "Epoch 19311/20000 Training Loss: 0.06102438643574715\n",
      "Epoch 19312/20000 Training Loss: 0.053510118275880814\n",
      "Epoch 19313/20000 Training Loss: 0.07281091064214706\n",
      "Epoch 19314/20000 Training Loss: 0.04981975257396698\n",
      "Epoch 19315/20000 Training Loss: 0.04418191686272621\n",
      "Epoch 19316/20000 Training Loss: 0.05568864941596985\n",
      "Epoch 19317/20000 Training Loss: 0.06643731892108917\n",
      "Epoch 19318/20000 Training Loss: 0.06900715082883835\n",
      "Epoch 19319/20000 Training Loss: 0.05178442224860191\n",
      "Epoch 19320/20000 Training Loss: 0.04969059303402901\n",
      "Epoch 19320/20000 Validation Loss: 0.06194503605365753\n",
      "Epoch 19321/20000 Training Loss: 0.04141685739159584\n",
      "Epoch 19322/20000 Training Loss: 0.06480442732572556\n",
      "Epoch 19323/20000 Training Loss: 0.06946439296007156\n",
      "Epoch 19324/20000 Training Loss: 0.05367720127105713\n",
      "Epoch 19325/20000 Training Loss: 0.0641956627368927\n",
      "Epoch 19326/20000 Training Loss: 0.04700227454304695\n",
      "Epoch 19327/20000 Training Loss: 0.05173544958233833\n",
      "Epoch 19328/20000 Training Loss: 0.05615870654582977\n",
      "Epoch 19329/20000 Training Loss: 0.0551779568195343\n",
      "Epoch 19330/20000 Training Loss: 0.04605169966816902\n",
      "Epoch 19330/20000 Validation Loss: 0.048685066401958466\n",
      "Epoch 19331/20000 Training Loss: 0.0691390335559845\n",
      "Epoch 19332/20000 Training Loss: 0.0535929910838604\n",
      "Epoch 19333/20000 Training Loss: 0.056354958564043045\n",
      "Epoch 19334/20000 Training Loss: 0.04842015728354454\n",
      "Epoch 19335/20000 Training Loss: 0.05616840720176697\n",
      "Epoch 19336/20000 Training Loss: 0.041321586817502975\n",
      "Epoch 19337/20000 Training Loss: 0.04845889285206795\n",
      "Epoch 19338/20000 Training Loss: 0.039747338742017746\n",
      "Epoch 19339/20000 Training Loss: 0.04110604524612427\n",
      "Epoch 19340/20000 Training Loss: 0.04276532307267189\n",
      "Epoch 19340/20000 Validation Loss: 0.06466349959373474\n",
      "Epoch 19341/20000 Training Loss: 0.062064409255981445\n",
      "Epoch 19342/20000 Training Loss: 0.04212954267859459\n",
      "Epoch 19343/20000 Training Loss: 0.0421316958963871\n",
      "Epoch 19344/20000 Training Loss: 0.057654742151498795\n",
      "Epoch 19345/20000 Training Loss: 0.06015514209866524\n",
      "Epoch 19346/20000 Training Loss: 0.03678512200713158\n",
      "Epoch 19347/20000 Training Loss: 0.05088995769619942\n",
      "Epoch 19348/20000 Training Loss: 0.04374833032488823\n",
      "Epoch 19349/20000 Training Loss: 0.051846276968717575\n",
      "Epoch 19350/20000 Training Loss: 0.04972892627120018\n",
      "Epoch 19350/20000 Validation Loss: 0.045576442033052444\n",
      "Epoch 19351/20000 Training Loss: 0.052761662751436234\n",
      "Epoch 19352/20000 Training Loss: 0.057032037526369095\n",
      "Epoch 19353/20000 Training Loss: 0.060492709279060364\n",
      "Epoch 19354/20000 Training Loss: 0.05333186686038971\n",
      "Epoch 19355/20000 Training Loss: 0.05787445232272148\n",
      "Epoch 19356/20000 Training Loss: 0.05491359531879425\n",
      "Epoch 19357/20000 Training Loss: 0.046353746205568314\n",
      "Epoch 19358/20000 Training Loss: 0.0402911901473999\n",
      "Epoch 19359/20000 Training Loss: 0.05585538223385811\n",
      "Epoch 19360/20000 Training Loss: 0.055928975343704224\n",
      "Epoch 19360/20000 Validation Loss: 0.06101784110069275\n",
      "Epoch 19361/20000 Training Loss: 0.055794671177864075\n",
      "Epoch 19362/20000 Training Loss: 0.046558793634176254\n",
      "Epoch 19363/20000 Training Loss: 0.04299965500831604\n",
      "Epoch 19364/20000 Training Loss: 0.051040589809417725\n",
      "Epoch 19365/20000 Training Loss: 0.04541682079434395\n",
      "Epoch 19366/20000 Training Loss: 0.0669512003660202\n",
      "Epoch 19367/20000 Training Loss: 0.06772026419639587\n",
      "Epoch 19368/20000 Training Loss: 0.05755851790308952\n",
      "Epoch 19369/20000 Training Loss: 0.05099685862660408\n",
      "Epoch 19370/20000 Training Loss: 0.047779571264982224\n",
      "Epoch 19370/20000 Validation Loss: 0.0498940572142601\n",
      "Epoch 19371/20000 Training Loss: 0.0507027767598629\n",
      "Epoch 19372/20000 Training Loss: 0.045738041400909424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19373/20000 Training Loss: 0.05858529731631279\n",
      "Epoch 19374/20000 Training Loss: 0.04808008670806885\n",
      "Epoch 19375/20000 Training Loss: 0.0631568506360054\n",
      "Epoch 19376/20000 Training Loss: 0.046521883457899094\n",
      "Epoch 19377/20000 Training Loss: 0.04549521580338478\n",
      "Epoch 19378/20000 Training Loss: 0.05166604742407799\n",
      "Epoch 19379/20000 Training Loss: 0.049005065113306046\n",
      "Epoch 19380/20000 Training Loss: 0.044495489448308945\n",
      "Epoch 19380/20000 Validation Loss: 0.0511292889714241\n",
      "Epoch 19381/20000 Training Loss: 0.0557762049138546\n",
      "Epoch 19382/20000 Training Loss: 0.03693823516368866\n",
      "Epoch 19383/20000 Training Loss: 0.058072347193956375\n",
      "Epoch 19384/20000 Training Loss: 0.049296215176582336\n",
      "Epoch 19385/20000 Training Loss: 0.07018435001373291\n",
      "Epoch 19386/20000 Training Loss: 0.049209654331207275\n",
      "Epoch 19387/20000 Training Loss: 0.04019713029265404\n",
      "Epoch 19388/20000 Training Loss: 0.05760771036148071\n",
      "Epoch 19389/20000 Training Loss: 0.057311952114105225\n",
      "Epoch 19390/20000 Training Loss: 0.048340510576963425\n",
      "Epoch 19390/20000 Validation Loss: 0.045781757682561874\n",
      "Epoch 19391/20000 Training Loss: 0.05628618597984314\n",
      "Epoch 19392/20000 Training Loss: 0.046128999441862106\n",
      "Epoch 19393/20000 Training Loss: 0.056736212223768234\n",
      "Epoch 19394/20000 Training Loss: 0.05547522380948067\n",
      "Epoch 19395/20000 Training Loss: 0.054790470749139786\n",
      "Epoch 19396/20000 Training Loss: 0.03866524621844292\n",
      "Epoch 19397/20000 Training Loss: 0.042761798948049545\n",
      "Epoch 19398/20000 Training Loss: 0.05856099724769592\n",
      "Epoch 19399/20000 Training Loss: 0.06876498460769653\n",
      "Epoch 19400/20000 Training Loss: 0.053604770451784134\n",
      "Epoch 19400/20000 Validation Loss: 0.05414312705397606\n",
      "Epoch 19401/20000 Training Loss: 0.061615753918886185\n",
      "Epoch 19402/20000 Training Loss: 0.037700727581977844\n",
      "Epoch 19403/20000 Training Loss: 0.047193605452775955\n",
      "Epoch 19404/20000 Training Loss: 0.06242414191365242\n",
      "Epoch 19405/20000 Training Loss: 0.03866793215274811\n",
      "Epoch 19406/20000 Training Loss: 0.05099038407206535\n",
      "Epoch 19407/20000 Training Loss: 0.060914624482393265\n",
      "Epoch 19408/20000 Training Loss: 0.056268077343702316\n",
      "Epoch 19409/20000 Training Loss: 0.049688056111335754\n",
      "Epoch 19410/20000 Training Loss: 0.04593147337436676\n",
      "Epoch 19410/20000 Validation Loss: 0.038097552955150604\n",
      "Epoch 19411/20000 Training Loss: 0.06748432666063309\n",
      "Epoch 19412/20000 Training Loss: 0.04433677718043327\n",
      "Epoch 19413/20000 Training Loss: 0.0626714676618576\n",
      "Epoch 19414/20000 Training Loss: 0.057659704238176346\n",
      "Epoch 19415/20000 Training Loss: 0.050313983112573624\n",
      "Epoch 19416/20000 Training Loss: 0.07081985473632812\n",
      "Epoch 19417/20000 Training Loss: 0.07234547287225723\n",
      "Epoch 19418/20000 Training Loss: 0.05938177928328514\n",
      "Epoch 19419/20000 Training Loss: 0.05217532441020012\n",
      "Epoch 19420/20000 Training Loss: 0.05308357998728752\n",
      "Epoch 19420/20000 Validation Loss: 0.07629771530628204\n",
      "Epoch 19421/20000 Training Loss: 0.065947987139225\n",
      "Epoch 19422/20000 Training Loss: 0.0680498406291008\n",
      "Epoch 19423/20000 Training Loss: 0.04826582595705986\n",
      "Epoch 19424/20000 Training Loss: 0.05972933769226074\n",
      "Epoch 19425/20000 Training Loss: 0.05049596354365349\n",
      "Epoch 19426/20000 Training Loss: 0.055635854601860046\n",
      "Epoch 19427/20000 Training Loss: 0.04060608148574829\n",
      "Epoch 19428/20000 Training Loss: 0.05247822776436806\n",
      "Epoch 19429/20000 Training Loss: 0.06348427385091782\n",
      "Epoch 19430/20000 Training Loss: 0.06450167298316956\n",
      "Epoch 19430/20000 Validation Loss: 0.054654598236083984\n",
      "Epoch 19431/20000 Training Loss: 0.06344833225011826\n",
      "Epoch 19432/20000 Training Loss: 0.05206531286239624\n",
      "Epoch 19433/20000 Training Loss: 0.044346313923597336\n",
      "Epoch 19434/20000 Training Loss: 0.05711362883448601\n",
      "Epoch 19435/20000 Training Loss: 0.038811974227428436\n",
      "Epoch 19436/20000 Training Loss: 0.04792509973049164\n",
      "Epoch 19437/20000 Training Loss: 0.05048387870192528\n",
      "Epoch 19438/20000 Training Loss: 0.059026241302490234\n",
      "Epoch 19439/20000 Training Loss: 0.04817584156990051\n",
      "Epoch 19440/20000 Training Loss: 0.05266662314534187\n",
      "Epoch 19440/20000 Validation Loss: 0.04694533348083496\n",
      "Epoch 19441/20000 Training Loss: 0.05860511586070061\n",
      "Epoch 19442/20000 Training Loss: 0.049947340041399\n",
      "Epoch 19443/20000 Training Loss: 0.06742491573095322\n",
      "Epoch 19444/20000 Training Loss: 0.05857737734913826\n",
      "Epoch 19445/20000 Training Loss: 0.07101235538721085\n",
      "Epoch 19446/20000 Training Loss: 0.04196961224079132\n",
      "Epoch 19447/20000 Training Loss: 0.061295945197343826\n",
      "Epoch 19448/20000 Training Loss: 0.051146965473890305\n",
      "Epoch 19449/20000 Training Loss: 0.05162778124213219\n",
      "Epoch 19450/20000 Training Loss: 0.07956916838884354\n",
      "Epoch 19450/20000 Validation Loss: 0.040216922760009766\n",
      "Epoch 19451/20000 Training Loss: 0.06288120150566101\n",
      "Epoch 19452/20000 Training Loss: 0.051454562693834305\n",
      "Epoch 19453/20000 Training Loss: 0.05067002400755882\n",
      "Epoch 19454/20000 Training Loss: 0.049170780926942825\n",
      "Epoch 19455/20000 Training Loss: 0.052551835775375366\n",
      "Epoch 19456/20000 Training Loss: 0.05493863299489021\n",
      "Epoch 19457/20000 Training Loss: 0.0510304719209671\n",
      "Epoch 19458/20000 Training Loss: 0.05032013729214668\n",
      "Epoch 19459/20000 Training Loss: 0.060325343161821365\n",
      "Epoch 19460/20000 Training Loss: 0.04761278256773949\n",
      "Epoch 19460/20000 Validation Loss: 0.052630603313446045\n",
      "Epoch 19461/20000 Training Loss: 0.04985353350639343\n",
      "Epoch 19462/20000 Training Loss: 0.05763090029358864\n",
      "Epoch 19463/20000 Training Loss: 0.07067910581827164\n",
      "Epoch 19464/20000 Training Loss: 0.05071696639060974\n",
      "Epoch 19465/20000 Training Loss: 0.05662001296877861\n",
      "Epoch 19466/20000 Training Loss: 0.04493942856788635\n",
      "Epoch 19467/20000 Training Loss: 0.06968683749437332\n",
      "Epoch 19468/20000 Training Loss: 0.08477228134870529\n",
      "Epoch 19469/20000 Training Loss: 0.052831169217824936\n",
      "Epoch 19470/20000 Training Loss: 0.04984544217586517\n",
      "Epoch 19470/20000 Validation Loss: 0.04466750845313072\n",
      "Epoch 19471/20000 Training Loss: 0.0500171072781086\n",
      "Epoch 19472/20000 Training Loss: 0.045746099203825\n",
      "Epoch 19473/20000 Training Loss: 0.060746241360902786\n",
      "Epoch 19474/20000 Training Loss: 0.05937793850898743\n",
      "Epoch 19475/20000 Training Loss: 0.04740205407142639\n",
      "Epoch 19476/20000 Training Loss: 0.056744277477264404\n",
      "Epoch 19477/20000 Training Loss: 0.05439012125134468\n",
      "Epoch 19478/20000 Training Loss: 0.051289889961481094\n",
      "Epoch 19479/20000 Training Loss: 0.05553345009684563\n",
      "Epoch 19480/20000 Training Loss: 0.048728134483098984\n",
      "Epoch 19480/20000 Validation Loss: 0.038967426866292953\n",
      "Epoch 19481/20000 Training Loss: 0.05193685367703438\n",
      "Epoch 19482/20000 Training Loss: 0.06992486119270325\n",
      "Epoch 19483/20000 Training Loss: 0.067219577729702\n",
      "Epoch 19484/20000 Training Loss: 0.06208363175392151\n",
      "Epoch 19485/20000 Training Loss: 0.04976421594619751\n",
      "Epoch 19486/20000 Training Loss: 0.055939722806215286\n",
      "Epoch 19487/20000 Training Loss: 0.05306757614016533\n",
      "Epoch 19488/20000 Training Loss: 0.04745763540267944\n",
      "Epoch 19489/20000 Training Loss: 0.04589257761836052\n",
      "Epoch 19490/20000 Training Loss: 0.04327226057648659\n",
      "Epoch 19490/20000 Validation Loss: 0.053283967077732086\n",
      "Epoch 19491/20000 Training Loss: 0.06373230367898941\n",
      "Epoch 19492/20000 Training Loss: 0.04273739457130432\n",
      "Epoch 19493/20000 Training Loss: 0.056052010506391525\n",
      "Epoch 19494/20000 Training Loss: 0.058861732482910156\n",
      "Epoch 19495/20000 Training Loss: 0.06030464544892311\n",
      "Epoch 19496/20000 Training Loss: 0.0383620485663414\n",
      "Epoch 19497/20000 Training Loss: 0.043588075786828995\n",
      "Epoch 19498/20000 Training Loss: 0.04747820273041725\n",
      "Epoch 19499/20000 Training Loss: 0.06024528667330742\n",
      "Epoch 19500/20000 Training Loss: 0.08038026839494705\n",
      "Epoch 19500/20000 Validation Loss: 0.04634450376033783\n",
      "Epoch 19501/20000 Training Loss: 0.05105190351605415\n",
      "Epoch 19502/20000 Training Loss: 0.07744599133729935\n",
      "Epoch 19503/20000 Training Loss: 0.04172957316040993\n",
      "Epoch 19504/20000 Training Loss: 0.055311501026153564\n",
      "Epoch 19505/20000 Training Loss: 0.05332982912659645\n",
      "Epoch 19506/20000 Training Loss: 0.05483077093958855\n",
      "Epoch 19507/20000 Training Loss: 0.04596396163105965\n",
      "Epoch 19508/20000 Training Loss: 0.057605501264333725\n",
      "Epoch 19509/20000 Training Loss: 0.061519768089056015\n",
      "Epoch 19510/20000 Training Loss: 0.057045649737119675\n",
      "Epoch 19510/20000 Validation Loss: 0.0629524365067482\n",
      "Epoch 19511/20000 Training Loss: 0.06765934824943542\n",
      "Epoch 19512/20000 Training Loss: 0.04270051792263985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19513/20000 Training Loss: 0.05640801787376404\n",
      "Epoch 19514/20000 Training Loss: 0.06311284750699997\n",
      "Epoch 19515/20000 Training Loss: 0.05255197361111641\n",
      "Epoch 19516/20000 Training Loss: 0.0512322336435318\n",
      "Epoch 19517/20000 Training Loss: 0.0620715469121933\n",
      "Epoch 19518/20000 Training Loss: 0.04116389527916908\n",
      "Epoch 19519/20000 Training Loss: 0.05378140136599541\n",
      "Epoch 19520/20000 Training Loss: 0.046021003276109695\n",
      "Epoch 19520/20000 Validation Loss: 0.04212082549929619\n",
      "Epoch 19521/20000 Training Loss: 0.04954395070672035\n",
      "Epoch 19522/20000 Training Loss: 0.053962502628564835\n",
      "Epoch 19523/20000 Training Loss: 0.05720418691635132\n",
      "Epoch 19524/20000 Training Loss: 0.046745166182518005\n",
      "Epoch 19525/20000 Training Loss: 0.05976935848593712\n",
      "Epoch 19526/20000 Training Loss: 0.045290905982255936\n",
      "Epoch 19527/20000 Training Loss: 0.054555606096982956\n",
      "Epoch 19528/20000 Training Loss: 0.05915170535445213\n",
      "Epoch 19529/20000 Training Loss: 0.05481760576367378\n",
      "Epoch 19530/20000 Training Loss: 0.05481720343232155\n",
      "Epoch 19530/20000 Validation Loss: 0.06368301063776016\n",
      "Epoch 19531/20000 Training Loss: 0.06301849335432053\n",
      "Epoch 19532/20000 Training Loss: 0.03972850367426872\n",
      "Epoch 19533/20000 Training Loss: 0.06517709791660309\n",
      "Epoch 19534/20000 Training Loss: 0.050916384905576706\n",
      "Epoch 19535/20000 Training Loss: 0.06219451501965523\n",
      "Epoch 19536/20000 Training Loss: 0.05294913053512573\n",
      "Epoch 19537/20000 Training Loss: 0.04308512434363365\n",
      "Epoch 19538/20000 Training Loss: 0.0569109208881855\n",
      "Epoch 19539/20000 Training Loss: 0.05741424858570099\n",
      "Epoch 19540/20000 Training Loss: 0.045128434896469116\n",
      "Epoch 19540/20000 Validation Loss: 0.06507396697998047\n",
      "Epoch 19541/20000 Training Loss: 0.04722411558032036\n",
      "Epoch 19542/20000 Training Loss: 0.04068304970860481\n",
      "Epoch 19543/20000 Training Loss: 0.05646141245961189\n",
      "Epoch 19544/20000 Training Loss: 0.06128911301493645\n",
      "Epoch 19545/20000 Training Loss: 0.04477490484714508\n",
      "Epoch 19546/20000 Training Loss: 0.053923506289720535\n",
      "Epoch 19547/20000 Training Loss: 0.06462442129850388\n",
      "Epoch 19548/20000 Training Loss: 0.057043951004743576\n",
      "Epoch 19549/20000 Training Loss: 0.061395008116960526\n",
      "Epoch 19550/20000 Training Loss: 0.07024874538183212\n",
      "Epoch 19550/20000 Validation Loss: 0.053482964634895325\n",
      "Epoch 19551/20000 Training Loss: 0.05098806694149971\n",
      "Epoch 19552/20000 Training Loss: 0.052124809473752975\n",
      "Epoch 19553/20000 Training Loss: 0.04775664582848549\n",
      "Epoch 19554/20000 Training Loss: 0.053816840052604675\n",
      "Epoch 19555/20000 Training Loss: 0.053007349371910095\n",
      "Epoch 19556/20000 Training Loss: 0.0675237849354744\n",
      "Epoch 19557/20000 Training Loss: 0.06737442314624786\n",
      "Epoch 19558/20000 Training Loss: 0.05425405502319336\n",
      "Epoch 19559/20000 Training Loss: 0.0648026391863823\n",
      "Epoch 19560/20000 Training Loss: 0.06336211413145065\n",
      "Epoch 19560/20000 Validation Loss: 0.04685279726982117\n",
      "Epoch 19561/20000 Training Loss: 0.06636761128902435\n",
      "Epoch 19562/20000 Training Loss: 0.06372687965631485\n",
      "Epoch 19563/20000 Training Loss: 0.05684002861380577\n",
      "Epoch 19564/20000 Training Loss: 0.05673709511756897\n",
      "Epoch 19565/20000 Training Loss: 0.04918452724814415\n",
      "Epoch 19566/20000 Training Loss: 0.058718692511320114\n",
      "Epoch 19567/20000 Training Loss: 0.04453454911708832\n",
      "Epoch 19568/20000 Training Loss: 0.07377491891384125\n",
      "Epoch 19569/20000 Training Loss: 0.04830586910247803\n",
      "Epoch 19570/20000 Training Loss: 0.05320758745074272\n",
      "Epoch 19570/20000 Validation Loss: 0.045104656368494034\n",
      "Epoch 19571/20000 Training Loss: 0.05885713919997215\n",
      "Epoch 19572/20000 Training Loss: 0.06642430275678635\n",
      "Epoch 19573/20000 Training Loss: 0.057990748435258865\n",
      "Epoch 19574/20000 Training Loss: 0.05110473558306694\n",
      "Epoch 19575/20000 Training Loss: 0.052417781203985214\n",
      "Epoch 19576/20000 Training Loss: 0.06263846158981323\n",
      "Epoch 19577/20000 Training Loss: 0.06437595188617706\n",
      "Epoch 19578/20000 Training Loss: 0.04464505612850189\n",
      "Epoch 19579/20000 Training Loss: 0.05184060335159302\n",
      "Epoch 19580/20000 Training Loss: 0.056254953145980835\n",
      "Epoch 19580/20000 Validation Loss: 0.060290563851594925\n",
      "Epoch 19581/20000 Training Loss: 0.0618150532245636\n",
      "Epoch 19582/20000 Training Loss: 0.05689672753214836\n",
      "Epoch 19583/20000 Training Loss: 0.04562323912978172\n",
      "Epoch 19584/20000 Training Loss: 0.05628271773457527\n",
      "Epoch 19585/20000 Training Loss: 0.0637095645070076\n",
      "Epoch 19586/20000 Training Loss: 0.051640987396240234\n",
      "Epoch 19587/20000 Training Loss: 0.04044186696410179\n",
      "Epoch 19588/20000 Training Loss: 0.03527547046542168\n",
      "Epoch 19589/20000 Training Loss: 0.07058029621839523\n",
      "Epoch 19590/20000 Training Loss: 0.05152890458703041\n",
      "Epoch 19590/20000 Validation Loss: 0.059845782816410065\n",
      "Epoch 19591/20000 Training Loss: 0.05910925194621086\n",
      "Epoch 19592/20000 Training Loss: 0.04352094605565071\n",
      "Epoch 19593/20000 Training Loss: 0.05155168101191521\n",
      "Epoch 19594/20000 Training Loss: 0.037794530391693115\n",
      "Epoch 19595/20000 Training Loss: 0.05360030010342598\n",
      "Epoch 19596/20000 Training Loss: 0.04708560183644295\n",
      "Epoch 19597/20000 Training Loss: 0.05902374908328056\n",
      "Epoch 19598/20000 Training Loss: 0.06299737095832825\n",
      "Epoch 19599/20000 Training Loss: 0.06573449820280075\n",
      "Epoch 19600/20000 Training Loss: 0.042853403836488724\n",
      "Epoch 19600/20000 Validation Loss: 0.038627706468105316\n",
      "Epoch 19601/20000 Training Loss: 0.04504357650876045\n",
      "Epoch 19602/20000 Training Loss: 0.058085907250642776\n",
      "Epoch 19603/20000 Training Loss: 0.049813318997621536\n",
      "Epoch 19604/20000 Training Loss: 0.057427484542131424\n",
      "Epoch 19605/20000 Training Loss: 0.0535617433488369\n",
      "Epoch 19606/20000 Training Loss: 0.059875354170799255\n",
      "Epoch 19607/20000 Training Loss: 0.07090798020362854\n",
      "Epoch 19608/20000 Training Loss: 0.06550093740224838\n",
      "Epoch 19609/20000 Training Loss: 0.050664257258176804\n",
      "Epoch 19610/20000 Training Loss: 0.049830395728349686\n",
      "Epoch 19610/20000 Validation Loss: 0.09365405142307281\n",
      "Epoch 19611/20000 Training Loss: 0.044259872287511826\n",
      "Epoch 19612/20000 Training Loss: 0.048434752970933914\n",
      "Epoch 19613/20000 Training Loss: 0.058126434683799744\n",
      "Epoch 19614/20000 Training Loss: 0.05230497196316719\n",
      "Epoch 19615/20000 Training Loss: 0.046552080661058426\n",
      "Epoch 19616/20000 Training Loss: 0.07914294302463531\n",
      "Epoch 19617/20000 Training Loss: 0.044247012585401535\n",
      "Epoch 19618/20000 Training Loss: 0.04885758459568024\n",
      "Epoch 19619/20000 Training Loss: 0.037098631262779236\n",
      "Epoch 19620/20000 Training Loss: 0.07030629366636276\n",
      "Epoch 19620/20000 Validation Loss: 0.0551239438354969\n",
      "Epoch 19621/20000 Training Loss: 0.03528018668293953\n",
      "Epoch 19622/20000 Training Loss: 0.046016354113817215\n",
      "Epoch 19623/20000 Training Loss: 0.04167303442955017\n",
      "Epoch 19624/20000 Training Loss: 0.060762446373701096\n",
      "Epoch 19625/20000 Training Loss: 0.0848298892378807\n",
      "Epoch 19626/20000 Training Loss: 0.05229230597615242\n",
      "Epoch 19627/20000 Training Loss: 0.05459757521748543\n",
      "Epoch 19628/20000 Training Loss: 0.04932825639843941\n",
      "Epoch 19629/20000 Training Loss: 0.047292206436395645\n",
      "Epoch 19630/20000 Training Loss: 0.050238821655511856\n",
      "Epoch 19630/20000 Validation Loss: 0.050012387335300446\n",
      "Epoch 19631/20000 Training Loss: 0.049799177795648575\n",
      "Epoch 19632/20000 Training Loss: 0.04787953570485115\n",
      "Epoch 19633/20000 Training Loss: 0.04162497818470001\n",
      "Epoch 19634/20000 Training Loss: 0.0502496063709259\n",
      "Epoch 19635/20000 Training Loss: 0.05367050692439079\n",
      "Epoch 19636/20000 Training Loss: 0.06061817333102226\n",
      "Epoch 19637/20000 Training Loss: 0.04844525456428528\n",
      "Epoch 19638/20000 Training Loss: 0.05165949463844299\n",
      "Epoch 19639/20000 Training Loss: 0.05304129794239998\n",
      "Epoch 19640/20000 Training Loss: 0.04867008700966835\n",
      "Epoch 19640/20000 Validation Loss: 0.04993989318609238\n",
      "Epoch 19641/20000 Training Loss: 0.06790604442358017\n",
      "Epoch 19642/20000 Training Loss: 0.04189347103238106\n",
      "Epoch 19643/20000 Training Loss: 0.053223494440317154\n",
      "Epoch 19644/20000 Training Loss: 0.07005307078361511\n",
      "Epoch 19645/20000 Training Loss: 0.04609803855419159\n",
      "Epoch 19646/20000 Training Loss: 0.04871702194213867\n",
      "Epoch 19647/20000 Training Loss: 0.05288607254624367\n",
      "Epoch 19648/20000 Training Loss: 0.05362434312701225\n",
      "Epoch 19649/20000 Training Loss: 0.0522950179874897\n",
      "Epoch 19650/20000 Training Loss: 0.05113515257835388\n",
      "Epoch 19650/20000 Validation Loss: 0.07906670868396759\n",
      "Epoch 19651/20000 Training Loss: 0.04885260388255119\n",
      "Epoch 19652/20000 Training Loss: 0.05246555805206299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19653/20000 Training Loss: 0.056324247270822525\n",
      "Epoch 19654/20000 Training Loss: 0.03472547605633736\n",
      "Epoch 19655/20000 Training Loss: 0.07060442119836807\n",
      "Epoch 19656/20000 Training Loss: 0.05643632635474205\n",
      "Epoch 19657/20000 Training Loss: 0.06211816146969795\n",
      "Epoch 19658/20000 Training Loss: 0.055952038615942\n",
      "Epoch 19659/20000 Training Loss: 0.045102447271347046\n",
      "Epoch 19660/20000 Training Loss: 0.04069503769278526\n",
      "Epoch 19660/20000 Validation Loss: 0.051585908979177475\n",
      "Epoch 19661/20000 Training Loss: 0.05085625872015953\n",
      "Epoch 19662/20000 Training Loss: 0.03243439644575119\n",
      "Epoch 19663/20000 Training Loss: 0.05878196284174919\n",
      "Epoch 19664/20000 Training Loss: 0.047049056738615036\n",
      "Epoch 19665/20000 Training Loss: 0.05123193562030792\n",
      "Epoch 19666/20000 Training Loss: 0.06370700895786285\n",
      "Epoch 19667/20000 Training Loss: 0.047142621129751205\n",
      "Epoch 19668/20000 Training Loss: 0.04465031623840332\n",
      "Epoch 19669/20000 Training Loss: 0.04451678693294525\n",
      "Epoch 19670/20000 Training Loss: 0.05490683391690254\n",
      "Epoch 19670/20000 Validation Loss: 0.04519382864236832\n",
      "Epoch 19671/20000 Training Loss: 0.04429854452610016\n",
      "Epoch 19672/20000 Training Loss: 0.04512758553028107\n",
      "Epoch 19673/20000 Training Loss: 0.06031220033764839\n",
      "Epoch 19674/20000 Training Loss: 0.045707136392593384\n",
      "Epoch 19675/20000 Training Loss: 0.04752354696393013\n",
      "Epoch 19676/20000 Training Loss: 0.036420416086912155\n",
      "Epoch 19677/20000 Training Loss: 0.05141075327992439\n",
      "Epoch 19678/20000 Training Loss: 0.053915124386548996\n",
      "Epoch 19679/20000 Training Loss: 0.061281468719244\n",
      "Epoch 19680/20000 Training Loss: 0.04762611910700798\n",
      "Epoch 19680/20000 Validation Loss: 0.08886656910181046\n",
      "Epoch 19681/20000 Training Loss: 0.05549350380897522\n",
      "Epoch 19682/20000 Training Loss: 0.05075882747769356\n",
      "Epoch 19683/20000 Training Loss: 0.05114974081516266\n",
      "Epoch 19684/20000 Training Loss: 0.05633483827114105\n",
      "Epoch 19685/20000 Training Loss: 0.05876375362277031\n",
      "Epoch 19686/20000 Training Loss: 0.044574957340955734\n",
      "Epoch 19687/20000 Training Loss: 0.06698492169380188\n",
      "Epoch 19688/20000 Training Loss: 0.04822133854031563\n",
      "Epoch 19689/20000 Training Loss: 0.05752679705619812\n",
      "Epoch 19690/20000 Training Loss: 0.05441102012991905\n",
      "Epoch 19690/20000 Validation Loss: 0.039643898606300354\n",
      "Epoch 19691/20000 Training Loss: 0.04594508185982704\n",
      "Epoch 19692/20000 Training Loss: 0.07363267987966537\n",
      "Epoch 19693/20000 Training Loss: 0.0476359985768795\n",
      "Epoch 19694/20000 Training Loss: 0.05589652061462402\n",
      "Epoch 19695/20000 Training Loss: 0.0652259811758995\n",
      "Epoch 19696/20000 Training Loss: 0.03873065486550331\n",
      "Epoch 19697/20000 Training Loss: 0.04051714017987251\n",
      "Epoch 19698/20000 Training Loss: 0.05495557561516762\n",
      "Epoch 19699/20000 Training Loss: 0.06175505742430687\n",
      "Epoch 19700/20000 Training Loss: 0.045064400881528854\n",
      "Epoch 19700/20000 Validation Loss: 0.06099604442715645\n",
      "Epoch 19701/20000 Training Loss: 0.05106769874691963\n",
      "Epoch 19702/20000 Training Loss: 0.0621052086353302\n",
      "Epoch 19703/20000 Training Loss: 0.04347771406173706\n",
      "Epoch 19704/20000 Training Loss: 0.07420384883880615\n",
      "Epoch 19705/20000 Training Loss: 0.064340740442276\n",
      "Epoch 19706/20000 Training Loss: 0.0517660416662693\n",
      "Epoch 19707/20000 Training Loss: 0.054671600461006165\n",
      "Epoch 19708/20000 Training Loss: 0.06281539797782898\n",
      "Epoch 19709/20000 Training Loss: 0.05202852189540863\n",
      "Epoch 19710/20000 Training Loss: 0.060515712946653366\n",
      "Epoch 19710/20000 Validation Loss: 0.05178003013134003\n",
      "Epoch 19711/20000 Training Loss: 0.05761329457163811\n",
      "Epoch 19712/20000 Training Loss: 0.040909379720687866\n",
      "Epoch 19713/20000 Training Loss: 0.05626990273594856\n",
      "Epoch 19714/20000 Training Loss: 0.06218394637107849\n",
      "Epoch 19715/20000 Training Loss: 0.055539149791002274\n",
      "Epoch 19716/20000 Training Loss: 0.03946535661816597\n",
      "Epoch 19717/20000 Training Loss: 0.047682542353868484\n",
      "Epoch 19718/20000 Training Loss: 0.0506732203066349\n",
      "Epoch 19719/20000 Training Loss: 0.054154831916093826\n",
      "Epoch 19720/20000 Training Loss: 0.06723958998918533\n",
      "Epoch 19720/20000 Validation Loss: 0.07294748723506927\n",
      "Epoch 19721/20000 Training Loss: 0.0598890483379364\n",
      "Epoch 19722/20000 Training Loss: 0.04783576726913452\n",
      "Epoch 19723/20000 Training Loss: 0.05011391639709473\n",
      "Epoch 19724/20000 Training Loss: 0.03774043545126915\n",
      "Epoch 19725/20000 Training Loss: 0.05578151345252991\n",
      "Epoch 19726/20000 Training Loss: 0.06507786363363266\n",
      "Epoch 19727/20000 Training Loss: 0.05575448274612427\n",
      "Epoch 19728/20000 Training Loss: 0.0502459891140461\n",
      "Epoch 19729/20000 Training Loss: 0.0581255741417408\n",
      "Epoch 19730/20000 Training Loss: 0.038427118211984634\n",
      "Epoch 19730/20000 Validation Loss: 0.0487484447658062\n",
      "Epoch 19731/20000 Training Loss: 0.052943792194128036\n",
      "Epoch 19732/20000 Training Loss: 0.04895016551017761\n",
      "Epoch 19733/20000 Training Loss: 0.06082103028893471\n",
      "Epoch 19734/20000 Training Loss: 0.053787317126989365\n",
      "Epoch 19735/20000 Training Loss: 0.06703680008649826\n",
      "Epoch 19736/20000 Training Loss: 0.058869313448667526\n",
      "Epoch 19737/20000 Training Loss: 0.04646070674061775\n",
      "Epoch 19738/20000 Training Loss: 0.06278660893440247\n",
      "Epoch 19739/20000 Training Loss: 0.05487566068768501\n",
      "Epoch 19740/20000 Training Loss: 0.06476865708827972\n",
      "Epoch 19740/20000 Validation Loss: 0.06844770163297653\n",
      "Epoch 19741/20000 Training Loss: 0.05357355251908302\n",
      "Epoch 19742/20000 Training Loss: 0.06877324730157852\n",
      "Epoch 19743/20000 Training Loss: 0.06275441497564316\n",
      "Epoch 19744/20000 Training Loss: 0.05857817828655243\n",
      "Epoch 19745/20000 Training Loss: 0.04709961637854576\n",
      "Epoch 19746/20000 Training Loss: 0.05965525284409523\n",
      "Epoch 19747/20000 Training Loss: 0.053771570324897766\n",
      "Epoch 19748/20000 Training Loss: 0.06250172108411789\n",
      "Epoch 19749/20000 Training Loss: 0.05848037078976631\n",
      "Epoch 19750/20000 Training Loss: 0.04425675794482231\n",
      "Epoch 19750/20000 Validation Loss: 0.0438518226146698\n",
      "Epoch 19751/20000 Training Loss: 0.04525938630104065\n",
      "Epoch 19752/20000 Training Loss: 0.061681196093559265\n",
      "Epoch 19753/20000 Training Loss: 0.04886540770530701\n",
      "Epoch 19754/20000 Training Loss: 0.046435803174972534\n",
      "Epoch 19755/20000 Training Loss: 0.041348520666360855\n",
      "Epoch 19756/20000 Training Loss: 0.050513673573732376\n",
      "Epoch 19757/20000 Training Loss: 0.04944898560643196\n",
      "Epoch 19758/20000 Training Loss: 0.043816059827804565\n",
      "Epoch 19759/20000 Training Loss: 0.07519293576478958\n",
      "Epoch 19760/20000 Training Loss: 0.05214466527104378\n",
      "Epoch 19760/20000 Validation Loss: 0.07002698630094528\n",
      "Epoch 19761/20000 Training Loss: 0.053760770708322525\n",
      "Epoch 19762/20000 Training Loss: 0.06619922071695328\n",
      "Epoch 19763/20000 Training Loss: 0.04237284138798714\n",
      "Epoch 19764/20000 Training Loss: 0.08324689418077469\n",
      "Epoch 19765/20000 Training Loss: 0.04251858964562416\n",
      "Epoch 19766/20000 Training Loss: 0.05026440694928169\n",
      "Epoch 19767/20000 Training Loss: 0.040017906576395035\n",
      "Epoch 19768/20000 Training Loss: 0.047572653740644455\n",
      "Epoch 19769/20000 Training Loss: 0.057355135679244995\n",
      "Epoch 19770/20000 Training Loss: 0.055372223258018494\n",
      "Epoch 19770/20000 Validation Loss: 0.044905975461006165\n",
      "Epoch 19771/20000 Training Loss: 0.07176019996404648\n",
      "Epoch 19772/20000 Training Loss: 0.04894964024424553\n",
      "Epoch 19773/20000 Training Loss: 0.05646524950861931\n",
      "Epoch 19774/20000 Training Loss: 0.046191588044166565\n",
      "Epoch 19775/20000 Training Loss: 0.05482957884669304\n",
      "Epoch 19776/20000 Training Loss: 0.05797381326556206\n",
      "Epoch 19777/20000 Training Loss: 0.0467127300798893\n",
      "Epoch 19778/20000 Training Loss: 0.037239234894514084\n",
      "Epoch 19779/20000 Training Loss: 0.06064102426171303\n",
      "Epoch 19780/20000 Training Loss: 0.04077226296067238\n",
      "Epoch 19780/20000 Validation Loss: 0.06870786845684052\n",
      "Epoch 19781/20000 Training Loss: 0.04961851239204407\n",
      "Epoch 19782/20000 Training Loss: 0.06354867666959763\n",
      "Epoch 19783/20000 Training Loss: 0.056080520153045654\n",
      "Epoch 19784/20000 Training Loss: 0.055161092430353165\n",
      "Epoch 19785/20000 Training Loss: 0.04943549633026123\n",
      "Epoch 19786/20000 Training Loss: 0.04539474844932556\n",
      "Epoch 19787/20000 Training Loss: 0.06976034492254257\n",
      "Epoch 19788/20000 Training Loss: 0.04107442870736122\n",
      "Epoch 19789/20000 Training Loss: 0.04868284985423088\n",
      "Epoch 19790/20000 Training Loss: 0.04316115006804466\n",
      "Epoch 19790/20000 Validation Loss: 0.05439004302024841\n",
      "Epoch 19791/20000 Training Loss: 0.06459320336580276\n",
      "Epoch 19792/20000 Training Loss: 0.047447916120290756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19793/20000 Training Loss: 0.05084219574928284\n",
      "Epoch 19794/20000 Training Loss: 0.054430484771728516\n",
      "Epoch 19795/20000 Training Loss: 0.042294543236494064\n",
      "Epoch 19796/20000 Training Loss: 0.0658591166138649\n",
      "Epoch 19797/20000 Training Loss: 0.04864148423075676\n",
      "Epoch 19798/20000 Training Loss: 0.05957349017262459\n",
      "Epoch 19799/20000 Training Loss: 0.041492052376270294\n",
      "Epoch 19800/20000 Training Loss: 0.062048688530921936\n",
      "Epoch 19800/20000 Validation Loss: 0.07272925972938538\n",
      "Epoch 19801/20000 Training Loss: 0.06296298652887344\n",
      "Epoch 19802/20000 Training Loss: 0.05441347137093544\n",
      "Epoch 19803/20000 Training Loss: 0.05729318782687187\n",
      "Epoch 19804/20000 Training Loss: 0.046053480356931686\n",
      "Epoch 19805/20000 Training Loss: 0.0426378957927227\n",
      "Epoch 19806/20000 Training Loss: 0.05313101410865784\n",
      "Epoch 19807/20000 Training Loss: 0.054423198103904724\n",
      "Epoch 19808/20000 Training Loss: 0.05623513460159302\n",
      "Epoch 19809/20000 Training Loss: 0.042432863265275955\n",
      "Epoch 19810/20000 Training Loss: 0.04944385215640068\n",
      "Epoch 19810/20000 Validation Loss: 0.07393333315849304\n",
      "Epoch 19811/20000 Training Loss: 0.04281659796833992\n",
      "Epoch 19812/20000 Training Loss: 0.04966730996966362\n",
      "Epoch 19813/20000 Training Loss: 0.053740665316581726\n",
      "Epoch 19814/20000 Training Loss: 0.034577637910842896\n",
      "Epoch 19815/20000 Training Loss: 0.04925752803683281\n",
      "Epoch 19816/20000 Training Loss: 0.05291258171200752\n",
      "Epoch 19817/20000 Training Loss: 0.04997403547167778\n",
      "Epoch 19818/20000 Training Loss: 0.053339626640081406\n",
      "Epoch 19819/20000 Training Loss: 0.048415083438158035\n",
      "Epoch 19820/20000 Training Loss: 0.05064362287521362\n",
      "Epoch 19820/20000 Validation Loss: 0.06205901876091957\n",
      "Epoch 19821/20000 Training Loss: 0.04728870093822479\n",
      "Epoch 19822/20000 Training Loss: 0.04584190621972084\n",
      "Epoch 19823/20000 Training Loss: 0.04547877237200737\n",
      "Epoch 19824/20000 Training Loss: 0.04193982481956482\n",
      "Epoch 19825/20000 Training Loss: 0.06181357428431511\n",
      "Epoch 19826/20000 Training Loss: 0.0505157895386219\n",
      "Epoch 19827/20000 Training Loss: 0.04258636757731438\n",
      "Epoch 19828/20000 Training Loss: 0.04748016595840454\n",
      "Epoch 19829/20000 Training Loss: 0.0504223071038723\n",
      "Epoch 19830/20000 Training Loss: 0.05986659228801727\n",
      "Epoch 19830/20000 Validation Loss: 0.06124826520681381\n",
      "Epoch 19831/20000 Training Loss: 0.05377374589443207\n",
      "Epoch 19832/20000 Training Loss: 0.04108617082238197\n",
      "Epoch 19833/20000 Training Loss: 0.05540437996387482\n",
      "Epoch 19834/20000 Training Loss: 0.05865563824772835\n",
      "Epoch 19835/20000 Training Loss: 0.043276410549879074\n",
      "Epoch 19836/20000 Training Loss: 0.06580686569213867\n",
      "Epoch 19837/20000 Training Loss: 0.054820090532302856\n",
      "Epoch 19838/20000 Training Loss: 0.04943495988845825\n",
      "Epoch 19839/20000 Training Loss: 0.043181851506233215\n",
      "Epoch 19840/20000 Training Loss: 0.06778416037559509\n",
      "Epoch 19840/20000 Validation Loss: 0.058202698826789856\n",
      "Epoch 19841/20000 Training Loss: 0.05864148214459419\n",
      "Epoch 19842/20000 Training Loss: 0.04838709533214569\n",
      "Epoch 19843/20000 Training Loss: 0.050316035747528076\n",
      "Epoch 19844/20000 Training Loss: 0.043643444776535034\n",
      "Epoch 19845/20000 Training Loss: 0.060786884278059006\n",
      "Epoch 19846/20000 Training Loss: 0.07170195877552032\n",
      "Epoch 19847/20000 Training Loss: 0.06090431287884712\n",
      "Epoch 19848/20000 Training Loss: 0.058786649256944656\n",
      "Epoch 19849/20000 Training Loss: 0.059751566499471664\n",
      "Epoch 19850/20000 Training Loss: 0.06254223734140396\n",
      "Epoch 19850/20000 Validation Loss: 0.057605840265750885\n",
      "Epoch 19851/20000 Training Loss: 0.0686148852109909\n",
      "Epoch 19852/20000 Training Loss: 0.058530986309051514\n",
      "Epoch 19853/20000 Training Loss: 0.04700767621397972\n",
      "Epoch 19854/20000 Training Loss: 0.03642817214131355\n",
      "Epoch 19855/20000 Training Loss: 0.044382285326719284\n",
      "Epoch 19856/20000 Training Loss: 0.04528060182929039\n",
      "Epoch 19857/20000 Training Loss: 0.04489634558558464\n",
      "Epoch 19858/20000 Training Loss: 0.046702489256858826\n",
      "Epoch 19859/20000 Training Loss: 0.05628539249300957\n",
      "Epoch 19860/20000 Training Loss: 0.05780036374926567\n",
      "Epoch 19860/20000 Validation Loss: 0.0582578219473362\n",
      "Epoch 19861/20000 Training Loss: 0.05189349129796028\n",
      "Epoch 19862/20000 Training Loss: 0.04881567135453224\n",
      "Epoch 19863/20000 Training Loss: 0.05889704450964928\n",
      "Epoch 19864/20000 Training Loss: 0.048561036586761475\n",
      "Epoch 19865/20000 Training Loss: 0.0751558467745781\n",
      "Epoch 19866/20000 Training Loss: 0.045690570026636124\n",
      "Epoch 19867/20000 Training Loss: 0.053855348378419876\n",
      "Epoch 19868/20000 Training Loss: 0.06970302760601044\n",
      "Epoch 19869/20000 Training Loss: 0.042866915464401245\n",
      "Epoch 19870/20000 Training Loss: 0.05448533594608307\n",
      "Epoch 19870/20000 Validation Loss: 0.07780566811561584\n",
      "Epoch 19871/20000 Training Loss: 0.05411667749285698\n",
      "Epoch 19872/20000 Training Loss: 0.05419944226741791\n",
      "Epoch 19873/20000 Training Loss: 0.051919106394052505\n",
      "Epoch 19874/20000 Training Loss: 0.05649971961975098\n",
      "Epoch 19875/20000 Training Loss: 0.05787661671638489\n",
      "Epoch 19876/20000 Training Loss: 0.05074450373649597\n",
      "Epoch 19877/20000 Training Loss: 0.04900337755680084\n",
      "Epoch 19878/20000 Training Loss: 0.04596314951777458\n",
      "Epoch 19879/20000 Training Loss: 0.057385582476854324\n",
      "Epoch 19880/20000 Training Loss: 0.04554426670074463\n",
      "Epoch 19880/20000 Validation Loss: 0.08552692830562592\n",
      "Epoch 19881/20000 Training Loss: 0.07351946830749512\n",
      "Epoch 19882/20000 Training Loss: 0.047124266624450684\n",
      "Epoch 19883/20000 Training Loss: 0.04822855815291405\n",
      "Epoch 19884/20000 Training Loss: 0.07227274030447006\n",
      "Epoch 19885/20000 Training Loss: 0.04200855270028114\n",
      "Epoch 19886/20000 Training Loss: 0.05093744397163391\n",
      "Epoch 19887/20000 Training Loss: 0.06282120943069458\n",
      "Epoch 19888/20000 Training Loss: 0.05785578861832619\n",
      "Epoch 19889/20000 Training Loss: 0.05074448511004448\n",
      "Epoch 19890/20000 Training Loss: 0.0545060969889164\n",
      "Epoch 19890/20000 Validation Loss: 0.07526279240846634\n",
      "Epoch 19891/20000 Training Loss: 0.054694514721632004\n",
      "Epoch 19892/20000 Training Loss: 0.05179927870631218\n",
      "Epoch 19893/20000 Training Loss: 0.04403981566429138\n",
      "Epoch 19894/20000 Training Loss: 0.059860825538635254\n",
      "Epoch 19895/20000 Training Loss: 0.05597199499607086\n",
      "Epoch 19896/20000 Training Loss: 0.04774321988224983\n",
      "Epoch 19897/20000 Training Loss: 0.0600254125893116\n",
      "Epoch 19898/20000 Training Loss: 0.047492966055870056\n",
      "Epoch 19899/20000 Training Loss: 0.05929786339402199\n",
      "Epoch 19900/20000 Training Loss: 0.07588551193475723\n",
      "Epoch 19900/20000 Validation Loss: 0.06397464126348495\n",
      "Epoch 19901/20000 Training Loss: 0.04729914292693138\n",
      "Epoch 19902/20000 Training Loss: 0.04146096110343933\n",
      "Epoch 19903/20000 Training Loss: 0.04815668985247612\n",
      "Epoch 19904/20000 Training Loss: 0.07245496660470963\n",
      "Epoch 19905/20000 Training Loss: 0.056762099266052246\n",
      "Epoch 19906/20000 Training Loss: 0.06349994987249374\n",
      "Epoch 19907/20000 Training Loss: 0.0604422502219677\n",
      "Epoch 19908/20000 Training Loss: 0.045650165528059006\n",
      "Epoch 19909/20000 Training Loss: 0.08107899129390717\n",
      "Epoch 19910/20000 Training Loss: 0.04111355543136597\n",
      "Epoch 19910/20000 Validation Loss: 0.05887303128838539\n",
      "Epoch 19911/20000 Training Loss: 0.062064021825790405\n",
      "Epoch 19912/20000 Training Loss: 0.04433702304959297\n",
      "Epoch 19913/20000 Training Loss: 0.05323958769440651\n",
      "Epoch 19914/20000 Training Loss: 0.05193011835217476\n",
      "Epoch 19915/20000 Training Loss: 0.07577844709157944\n",
      "Epoch 19916/20000 Training Loss: 0.05713999271392822\n",
      "Epoch 19917/20000 Training Loss: 0.058792948722839355\n",
      "Epoch 19918/20000 Training Loss: 0.042640384286642075\n",
      "Epoch 19919/20000 Training Loss: 0.0673273429274559\n",
      "Epoch 19920/20000 Training Loss: 0.04800710454583168\n",
      "Epoch 19920/20000 Validation Loss: 0.05597904324531555\n",
      "Epoch 19921/20000 Training Loss: 0.05107086896896362\n",
      "Epoch 19922/20000 Training Loss: 0.047796580940485\n",
      "Epoch 19923/20000 Training Loss: 0.05385000631213188\n",
      "Epoch 19924/20000 Training Loss: 0.048199672251939774\n",
      "Epoch 19925/20000 Training Loss: 0.06280247122049332\n",
      "Epoch 19926/20000 Training Loss: 0.07842734456062317\n",
      "Epoch 19927/20000 Training Loss: 0.06936164945363998\n",
      "Epoch 19928/20000 Training Loss: 0.03672388568520546\n",
      "Epoch 19929/20000 Training Loss: 0.040974799543619156\n",
      "Epoch 19930/20000 Training Loss: 0.04600111022591591\n",
      "Epoch 19930/20000 Validation Loss: 0.04402519017457962\n",
      "Epoch 19931/20000 Training Loss: 0.056455422192811966\n",
      "Epoch 19932/20000 Training Loss: 0.05883563682436943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19933/20000 Training Loss: 0.04936782643198967\n",
      "Epoch 19934/20000 Training Loss: 0.057489026337862015\n",
      "Epoch 19935/20000 Training Loss: 0.0613127239048481\n",
      "Epoch 19936/20000 Training Loss: 0.05819815397262573\n",
      "Epoch 19937/20000 Training Loss: 0.05381517484784126\n",
      "Epoch 19938/20000 Training Loss: 0.05978461727499962\n",
      "Epoch 19939/20000 Training Loss: 0.052602630108594894\n",
      "Epoch 19940/20000 Training Loss: 0.04567664861679077\n",
      "Epoch 19940/20000 Validation Loss: 0.06594572961330414\n",
      "Epoch 19941/20000 Training Loss: 0.05016736686229706\n",
      "Epoch 19942/20000 Training Loss: 0.06132359802722931\n",
      "Epoch 19943/20000 Training Loss: 0.05086119845509529\n",
      "Epoch 19944/20000 Training Loss: 0.0458657331764698\n",
      "Epoch 19945/20000 Training Loss: 0.04054535925388336\n",
      "Epoch 19946/20000 Training Loss: 0.07431098818778992\n",
      "Epoch 19947/20000 Training Loss: 0.04856088384985924\n",
      "Epoch 19948/20000 Training Loss: 0.048321183770895004\n",
      "Epoch 19949/20000 Training Loss: 0.05694011226296425\n",
      "Epoch 19950/20000 Training Loss: 0.05255967378616333\n",
      "Epoch 19950/20000 Validation Loss: 0.0552191287279129\n",
      "Epoch 19951/20000 Training Loss: 0.046630337834358215\n",
      "Epoch 19952/20000 Training Loss: 0.05762895941734314\n",
      "Epoch 19953/20000 Training Loss: 0.059524569660425186\n",
      "Epoch 19954/20000 Training Loss: 0.06243881210684776\n",
      "Epoch 19955/20000 Training Loss: 0.06491883844137192\n",
      "Epoch 19956/20000 Training Loss: 0.05564117804169655\n",
      "Epoch 19957/20000 Training Loss: 0.05159755423665047\n",
      "Epoch 19958/20000 Training Loss: 0.038783926516771317\n",
      "Epoch 19959/20000 Training Loss: 0.05853022634983063\n",
      "Epoch 19960/20000 Training Loss: 0.03846555948257446\n",
      "Epoch 19960/20000 Validation Loss: 0.05542387440800667\n",
      "Epoch 19961/20000 Training Loss: 0.06072504445910454\n",
      "Epoch 19962/20000 Training Loss: 0.06938135623931885\n",
      "Epoch 19963/20000 Training Loss: 0.03953126445412636\n",
      "Epoch 19964/20000 Training Loss: 0.05257245898246765\n",
      "Epoch 19965/20000 Training Loss: 0.045419469475746155\n",
      "Epoch 19966/20000 Training Loss: 0.05361330136656761\n",
      "Epoch 19967/20000 Training Loss: 0.05074591562151909\n",
      "Epoch 19968/20000 Training Loss: 0.05625493451952934\n",
      "Epoch 19969/20000 Training Loss: 0.06682933121919632\n",
      "Epoch 19970/20000 Training Loss: 0.06669221073389053\n",
      "Epoch 19970/20000 Validation Loss: 0.04773024469614029\n",
      "Epoch 19971/20000 Training Loss: 0.055331338196992874\n",
      "Epoch 19972/20000 Training Loss: 0.047463249415159225\n",
      "Epoch 19973/20000 Training Loss: 0.05987866595387459\n",
      "Epoch 19974/20000 Training Loss: 0.06245487555861473\n",
      "Epoch 19975/20000 Training Loss: 0.04119221493601799\n",
      "Epoch 19976/20000 Training Loss: 0.050244834274053574\n",
      "Epoch 19977/20000 Training Loss: 0.058738697320222855\n",
      "Epoch 19978/20000 Training Loss: 0.054761290550231934\n",
      "Epoch 19979/20000 Training Loss: 0.05254741385579109\n",
      "Epoch 19980/20000 Training Loss: 0.052701979875564575\n",
      "Epoch 19980/20000 Validation Loss: 0.04594023525714874\n",
      "Epoch 19981/20000 Training Loss: 0.058228641748428345\n",
      "Epoch 19982/20000 Training Loss: 0.05639069899916649\n",
      "Epoch 19983/20000 Training Loss: 0.062447186559438705\n",
      "Epoch 19984/20000 Training Loss: 0.04515494033694267\n",
      "Epoch 19985/20000 Training Loss: 0.05427795648574829\n",
      "Epoch 19986/20000 Training Loss: 0.05380857363343239\n",
      "Epoch 19987/20000 Training Loss: 0.05205226317048073\n",
      "Epoch 19988/20000 Training Loss: 0.057988088577985764\n",
      "Epoch 19989/20000 Training Loss: 0.06263239681720734\n",
      "Epoch 19990/20000 Training Loss: 0.041540712118148804\n",
      "Epoch 19990/20000 Validation Loss: 0.049376726150512695\n",
      "Epoch 19991/20000 Training Loss: 0.041336774826049805\n",
      "Epoch 19992/20000 Training Loss: 0.0521080307662487\n",
      "Epoch 19993/20000 Training Loss: 0.04148891568183899\n",
      "Epoch 19994/20000 Training Loss: 0.04937243089079857\n",
      "Epoch 19995/20000 Training Loss: 0.05500171706080437\n",
      "Epoch 19996/20000 Training Loss: 0.06359972059726715\n",
      "Epoch 19997/20000 Training Loss: 0.04325238987803459\n",
      "Epoch 19998/20000 Training Loss: 0.05141958221793175\n",
      "Epoch 19999/20000 Training Loss: 0.03662492707371712\n",
      "Epoch 20000/20000 Validation Loss: 0.05170821771025658\n"
     ]
    }
   ],
   "source": [
    "schedule_sampler = create_named_schedule_sampler('uniform', diffusion)\n",
    "\n",
    "model.to(dist_util.dev())\n",
    "\n",
    "train_loss, val_loss = TrainLoop(\n",
    "                            model=model,\n",
    "                            diffusion=diffusion,\n",
    "                            data=data,\n",
    "                            batch_size=batch_size,\n",
    "                            microbatch=microbatch,\n",
    "                            lr=lr,\n",
    "                            ema_rate=ema_rate,\n",
    "                            schedule_sampler=schedule_sampler,\n",
    "                            weight_decay=weight_decay,\n",
    "                            epochs=epochs,\n",
    "                            eval_data=val,\n",
    "                            eval_interval=eval_interval,\n",
    "                            warm_up_steps=50,\n",
    "                            use_llrd=True,\n",
    "                            llrd_rate=0.9\n",
    "                        ).run_loop()\n",
    "\n",
    "dt = datetime.now().strftime(\"%m%d\")\n",
    "pickle.dump(model, open(f\"models/{dt}/final_model_df{diffusion_steps}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063fa9c7-84c8-48bd-b368-318df3f0275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2649f-12d5-4248-9de7-e647d242578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_names = []\n",
    "# for i, (name, param) in enumerate(model.named_parameters()):\n",
    "#     param_names.append(name)\n",
    "#     print(f'{i}: {name} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad7e2-8b77-4cb1-b18b-64de7518214c",
   "metadata": {},
   "source": [
    "# Generating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2881734-9e97-4947-9b4b-4b4766248ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_fp = f'models/0311/final_model_df1000.pkl'\n",
    "with open(best_model_fp, 'rb') as handle:\n",
    "    best_model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb8bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = SpacedDiffusion(\n",
    "    betas=get_named_beta_schedule(noise_schedule, 1000),\n",
    "    rescale_timesteps=rescale_timesteps,\n",
    "    predict_xstart=predict_xstart,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98e82b32-6aba-47fe-8daf-07dcaa4fc938",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset from data...\n",
      "### Loading from the custom TEST set...\n",
      "### Data samples...\n",
      " ['call her forth to me', 'no, sir, i live by the church.'] ['', 'art thou a churchman?']\n",
      "RAM used: 5540.30 MB\n",
      "This is raw_datasets:  Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 2\n",
      "})\n",
      "RAM used: 5540.30 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d19ad6fd884c3ab75c424506e32e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 2\n",
      "})\n",
      "### tokenized_datasets...example [2, 439, 175, 769, 88, 117, 3]\n",
      "RAM used: 5541.81 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb4ce66282848648a47c5d5b1e0834a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 5541.81 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a034788c8c0843cb9507a4ad1957d196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 2\n",
      "}) padded dataset\n",
      "RAM used: 5541.81 MB\n",
      "RAM used: 5541.81 MB\n",
      "### End of reading iteration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29c5bcca4b74ca78399227fcde99331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_lst_source, word_lst_recover, word_lst_ref, inter_lst_recover = sampling(best_model, \n",
    "                                                           diffusion, \n",
    "                                                           tokenizer, \n",
    "                                                           data_dir=regular_data_dir, \n",
    "                                                           batch_size=10, \n",
    "                                                           split='test_custom', \n",
    "                                                           seq_len=64, \n",
    "                                                           show_intermediate_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed69bc0-8481-4287-bd2b-2e3188d1ff22",
   "metadata": {},
   "source": [
    "Generating 20 sentences takes 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a7a4756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] call her forth to me [SEP] [SEP]',\n",
       " '[CLS] no, sir, i live by the church. [SEP] [SEP]']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9184761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] i beseech love to soul. [SEP]',\n",
       " '[CLS] what? is one, i pray but here of the duke. [SEP]']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lst_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20621c94-f0f4-485f-a262-90071d0082e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
