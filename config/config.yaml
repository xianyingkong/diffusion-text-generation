# Tokenizer to tokenize words into embeddings
tokenizer: 'shakespeare' # Change to either bert, shakespeare, or custom and modify custom_vocab_fp
custom_vocab_fp: 'shakespeare-tokenizer-bert/plays/vocab.txt' # Change this for tokenizer vocabulary

# Model hyperparameters
lr: 0.0001 # Weight learning rate
batch_size: 10 # Examples per data batch
microbatch: 5
epochs: 10 # Number of epochs to run
eval_interval: 1000
ema_rate: '0.9999' 
schedule_sampler: 'uniform' 
diffusion_steps: 1000 # Number of timesteps/diffusion steps for the diffusion mode
noise_schedule: 'sqrt'
use_plm_init: 'no' # embedding in transformer
transformer: 'bert-base-uncased'

seq_len: 128 # Amount of tokens that can be processed at once
hidden_t_dim: 128 # Dimension for the timestep embedding
hidden_dim: 128 # Hidden dimension for the transformer
dropout: 0.1 # Dropout rate for better learning
seed: 102
weight_decay: 0.0
predict_xstart: True
rescale_timesteps: True
emb_scale_factor: 1.0
data_dir: 'data/mini-shakespeare'

# sampling hyperparameters
sampling_step: 1000
clip_denoised: False
top_p: 0
clamp_step: 0
sampling_batch_size: 10
