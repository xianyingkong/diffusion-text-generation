lr: 0.0001
batch_size: 10
microbatch: 5
epochs: 10
eval_interval: 1000
ema_rate: '0.9999' 
schedule_sampler: 'uniform'
diffusion_steps: 1000
noise_schedule: 'sqrt'
vocab: 'custom'
use_plm_init: 'no' # embedding in transformer
tokenizer: 'shakespeare'
transformer: 'bert-base-uncased'
seq_len: 128
hidden_t_dim: 128
hidden_dim: 128
dropout: 0.1
seed: 102
weight_decay: 0.0
predict_xstart: True
rescale_timesteps: True
emb_scale_factor: 1.0
data_dir: 'data/mini-shakespeare'

# sampling hyperparameters
sampling_step: 1000
clip_denoised: False
top_p: 0
clamp_step: 0
sampling_batch_size: 10